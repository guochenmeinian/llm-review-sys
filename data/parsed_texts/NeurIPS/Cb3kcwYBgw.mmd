# Spatio-Spectral Graph Neural Networks

 Simon Geisler\({}^{\dagger}\), Arthur Kosmala\({}^{\dagger}\), Daniel Herbst, and Stephan Gunnemann

Department of Computer Science & Munich Data Science Institute

Technical University of Munich

{s.geisler, a.kosmala, d.herbst, s.guennemann}@tum.de

###### Abstract

Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of \(\ell\)-step MPGNNs are that their "receptive field" is typically limited to the \(\ell\)-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose _Spatio-Spectral Graph Neural Networks (S\({}^{2}\)GNNs)_ - a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S\({}^{2}\)GNNs's 'nquish' over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S\({}^{2}\)GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Leman (WL) test. Moreover, to obtain general-purpose S\({}^{2}\)GNNs, we propose spectrally parametrized filters for directed graphs. S\({}^{2}\)GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S\({}^{2}\)GNNs scale to millions of nodes.

## 1 Introduction

_Spatial_ Message-Passing Graph Neural Networks (MPGNNs) ushered in various recent breakthroughs. For example, MPGNNs are able to predict the weather with unprecedented precision (Lam et al., 2023), can be composed as a foundation model for a rich set of tasks on knowledge graphs (Galkin et al., 2023), and are a key component in the discovery of millions of AI-generated crystal structures (Merchant et al., 2023). Despite this success, MPGNNs produce node-level signals solely considering _limited-size neighborhoods_, effectively bounding their expressivity. Even with a large number of message-passing steps, MPGNNs are limited in their capability of propagating information to distant nodes due to _over-squashing_. As evident by the success of global models like transformers (Vaswani et al., 2017), modeling long-range interactions can be pivotal and an important step towards foundation models that understand graphs.

**We propose _Spatio-Spectral Graph Neural Networks (S\({}^{2}\)GNNs)_**,** a new modeling paradigm for tackling the aforementioned limitations, that synergistically combine _message passing_ with _spectral filters_, explicitly parametrized in the spectral domain. _Spectral filters_ are virtually ignored by prior work but go beyond stacks of message-passing layers or polynomial parametrizations. Due to _message passing's_ finite number

Figure 1: S\({}^{2}\)GNN principle.

of propagation steps, it comes with a distance cutoff \(p_{\text{cut}}\) (# hops, see Fig. 1). Conversely, _spectral filters_ act globally (\(p_{\max}\)), even on a truncated frequency spectrum \(\lambda_{\text{cut}}\). Truncating the frequency spectrum for _spectral filters_ is required for efficiency, yet _message passing_ has access to the entire spectrum (right plots in Fig. 1). The combination of _message passing and spectral filters_ provably leverages the strengths of each parametrization. Utilizing this combination, S\({}^{2}\)GNNs generalize the concept of "virtual nodes" and distill many important properties of hierarchical message-passing schemes, graph-rewritings, and pooling into a single GNN (see Fig. 3). Outside of GNNs, a similar composition is at the core of some State Space Models (SSM) models (Poli et al., 2023), that deliver transformer-like properties with superior scalability on sequences - as do S\({}^{2}\)GNNs on graphs.

**Our analysis of S\({}^{2}\)GNNs** (SS 3.1) validates their capability for modeling long-range interactions. We prove in SS 3.1.1 that combining spectral and spatial filters alleviates the over-squashing phenomenon (Alon and Yahav, 2020; Di Giovanni et al., 2023a, b), a necessity for effective information-exchange among distant nodes. Our approximation-theoretic analysis goes one step further and proves strictly tighter error bounds in terms of approximation of the target idealized GNN (SS 3.1.2).

**Design space of S\({}^{2}\)GNNs** (SS 3.2).

**Superpersitic composition of**

**spectral filters & spatial message passing**

**Vanquiesies over-squashing** (SS 3.1.1)

**Supervisor approximation capabilities** (SS 3.1.2)

**Spatial message passing** (see literature)

**Spectral filter parametrization** (SS 3.2.1)

**Spectral-domain MLP** (SS 3.2.2)

**Spectral filter for directed graphs** (SS 3.2.3)

**Dual use of partial _Electrocomposition_ (EVD):

_free-of-cost_ **Positional Encodings (PE) (& 3.2.1).**

We propose the first permutation-equivariance-preserving _neural network in the spectral domain_ (SS 3.2.2) and generalize _spectral filters to directed graphs_ (SS 3.2.3). The dual use of the partial eigendecomposition, required for spectral filters, allows us to propose "free-of-cost" _positional encodings_ (SS 3.2.4), that are permutation-equivariant, stable, and increase expressivity strictly beyond the 1-Weisfeiler-Leman (WL) test.

**S\({}^{2}\)GNNs are effective and practical.** We empirically verify the shortcomings of MPGNNs and how S\({}^{2}\)GNNs overcome them (SS 4). E.g., we set a new state-of-the-art on peptides-func (Dwivedi et al., 2022) with \(\approx\) 35% fewer parameters, outperforming MPGNNs and graph transformers. Although sequences are just a subdomain of (directed) graphs, we also study how S\({}^{2}\)GNNs compare to specialized sequence models like transformers (Vaswani et al., 2017) or Hyena (Poli et al., 2023). We find that S\({}^{2}\)GNNs are highly competitive even though they operate on a much more general domain (un-/directed graphs). Last, the runtime and space complexity of S\({}^{2}\)GNNs is equivalent to MPGNNs and, with vanilla full-graph training, S\({}^{2}\)GNNs can handle millions of nodes with a 40 GB GPU.

## 2 Background

We study graphs \(\mathcal{G}(\bm{A},\bm{X})\) with adjacency matrix \(\bm{A}\in\{0,1\}^{n\times n}\) (or \(\bm{A}\in\mathbb{R}_{\geq 0}^{n\times n}\) if weighted), node features \(\bm{X}\in\mathbb{R}^{n\times d}\) and edge count \(m\). \(\bm{A}\) is symmetric for undirected graphs and, thus, has eigendecomposition \(\bm{\lambda},\bm{V}=\operatorname{EVD}(\bm{A})\) with eigenvalues \(\bm{\lambda}\in\mathbb{R}^{n}\) and eigenvectors \(\bm{V}\in\mathbb{R}^{n\times n}\): \(\bm{A}=\bm{V}\bm{\Lambda}\bm{V}^{\top}\) using \(\bm{\Lambda}=\operatorname{diag}(\bm{\lambda})\). Instead of \(\bm{A}\), we decompose the _Laplacian_\(\bm{L}\coloneqq\bm{I}-\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2}\), with diagonal degree matrix \(\bm{D}=\operatorname{diag}(\bm{A}\overline{\bm{1}})\), since its ordered eigenvalues \(0=\lambda_{1}\leq\lambda_{2}\leq\cdots\leq\lambda_{n}\leq 2\) are similar to frequencies (e.g., low eigenvalues relate to low frequencies, see Fig. 4). Likewise, one could use, e.g., \(\bm{L}=\bm{I}-\bm{D}^{-1}\bm{A}\) or more general variants (Yang et al., 2023); however, we focus our explanations on the most common choice \(\bm{L}\coloneqq\bm{I}-\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2}\). We choose the matrix of eigenvectors \(\bm{V}\in\mathbb{R}^{n\times n}\) to be orthogonal \(\bm{V}\bm{V}^{\top}=\bm{I}\). We refer to \(\bm{V}\) as the Fourier basis of the graph, with Graph Fourier Transformation (GFT) \(\hat{\bm{X}}=\bm{V}^{\top}\bm{X}\) and its inverse \(\bm{X}=\bm{V}\hat{\bm{X}}\). To provide an overview, Table 5 lists the symbols used in this work.

**Spectral graph filters.** Many GNNs implement a graph convolution, where node signal \(\bm{X}\in\mathbb{R}^{n\times d}\) is convolved \(\bm{g}\ast_{\mathcal{G}}\bm{X}\) for every \(d\) with a scalar filter \(\bm{g}\in\mathbb{R}^{n}\). The graph convolution (Hammond et al., 2011) is defined in the spectral domain as \(\bm{g}\ast_{\mathcal{G}}\bm{X}\coloneqq\bm{V}([\bm{V}^{\top}\bm{g}]\odot[\bm {V}^{\top}\bm{X}])\), with element-wise product \(\odot\) and broadcast of \(\bm{V}^{\top}\bm{g}\) to match shapes. Instead of spatial \(\bm{g}\), spectral graph filters parametrize \(\hat{g}:[0,2]\to\mathbb{R}\) explicitly and yield \(\bm{V}^{\top}\bm{g}:=\hat{g}(\bm{\lambda})\in\mathbb{R}^{n}\) as a function of the eigenvalues.

Figure 2: S\({}^{2}\)GNN framework with adjacency matrix \(\bm{A}\), node features \(\bm{X}\), and Laplacian \(\bm{L}\) (function of \(\bm{A}\)).

**Message Passing Graph Neural Networks (MPGNNs)** circumvent the \(\mathrm{EVD}\) via polynomial \(\hat{g}(\bm{\lambda})_{u}=\sum_{p=0}^{p}\gamma_{j}\lambda_{u}^{j}\) since \(\bm{V}[\sum_{j=0}^{p}\gamma_{j}\operatorname{diag}(\bm{\lambda})^{j}]\bm{V}^{ \top}\bm{X}=\sum_{j=0}^{p}\gamma_{j}\bm{L}^{j}\bm{X}\). In practice, many MPGNNs use \(p=1\): \(\bm{H}^{(l)}=(\gamma_{0}\bm{I}+\gamma_{1}\bm{L})\bm{H}^{(l-1)}\) with \(\bm{H}^{(0)}=\bm{X}\), and stack \(1\leq l\leq\ell\) layers interleaved with node-wise transformations and activations \(\sigma\). We refer to Balcilar et al. (2021b) for similar interpretations of MPGNNs like GAT Velickovic et al. (2018) or GIN Xu et al. (2019).

## 3 Method

_S\({}^{2}\)GNNs_ symbiotically pair spatial \(\mathrm{Spatial}(\bm{H}^{(l-1)};\bm{A})\) MPGNNs and \(\mathrm{Spectral}(\bm{H}^{(l-1)};\bm{V},\bm{\lambda})\) filters, using a partial eigendecomposition. Even though the spectral filter operates on a truncated eigendecomposition (**spectrally bounded**), it is **spatially unbounded**. Conversely, spatial MPGNNs are **spatially bounded** yet **spectrally unbounded** (see Fig. 1).

A spectrally bounded filter is sensible for modeling global pair-wise interactions, considering its **message-passing interpretation** of Fig. 3. Conceptually, a spectral filter consists of three steps: 1 Gather: The multiplication of the node signal with the eigenvectors \(\bm{v}_{u}^{\top}\bm{X}\) (GFT) is a weighted and signed aggregation over all nodes; 2 Apply: the "Fourier coefficients" are weighted; and 3 Scatter broadcasts the signal \(\bm{v}_{u}\hat{\bm{X}}\) back to the nodes (inverse GFT). The first eigenvector (there for \(\bm{L}=\bm{D}-\bm{A}\)) acts like a "virtual node" Gilmer et al. (2017) (see also SS E). That is, it calculates the average embedding and then distributes this information, potentially interlayered with neural networks. Importantly, the other eigenvectors effectively allow messages to be passed within or between clusters. As we show for exemplary graphs in Fig. 4, low frequencies/eigenvalues capture coarse structures, while high(er) frequencies/eigenvalues capture details. For example, the second eigenvector in Fig. 4b contrasts the inner with the outer rectangle, while the third eigenspace models both symmetries up/down and left/right. In conclusion, S\({}^{2}\)GNNs augment spatial message-passing with a _graph-adaptive hierarchy_ (spectral filter). Thus, **S\({}^{2}\)GNNs distill many important properties of hierarchical message-passing schemes**Bodnar et al. (2021), **graph-rewirings**Di Giovanni et al. (2023a), **pooling**Lee et al. (2019) etc. See SS J.1 for more examples.

**S\({}^{2}\)GNN's composition.** We study (1) an _additive combination_ for its simpler approximation-theoretic interpretation (SS 3.1.2), or (2) an _arbitrary sequence of filters_ due to its flexibility. In both cases, residual connections may be desirable (see SS J.2).

\[\bm{H}^{(l)}=\mathrm{Spectral}^{(l)}(\bm{H}^{(l-1)};\bm{V},\bm{ \lambda})+\mathrm{Spatial}^{(l)}(\bm{H}^{(l-1)};\bm{A})\] (1) \[\bm{H}^{(\ell)}=(h^{(\ell)}\circ h^{(\ell-1)}\circ\cdots\circ h^{ (1)})(\bm{H}^{(0)})\quad\text{with }h^{(j)}\in\{\mathrm{Spectral},\mathrm{Spatial}\}\] (2)

**Spectral Filter.** The building block that turns a spatial MPGNN into an S\({}^{2}\)GNN is the spectral filter:

\[\mathrm{Spectral}^{(l)}(\bm{H}^{(l-1)};\bm{V},\bm{\lambda})=\bm{V}\Big{(}\hat{ g}_{\vartheta}^{(l)}(\bm{\lambda})\odot\big{[}\bm{V}^{\top}f_{\theta}^{(l)}( \bm{H}^{(l-1)})\big{]}\Big{)}\] (3)

with a point-wise transformation \(f_{\theta}^{(l)}:\mathbb{R}^{n\times d^{(l-1)}}\to\mathbb{R}^{n\times d^{(l)}}\), a learnable spectral filter \(\hat{g}_{\vartheta}^{(l)}(\bm{\lambda})\in\mathbb{R}^{k\times d^{(l)}}\) parameterized element-wise as \(\hat{g}_{\vartheta}^{(l)}(\bm{\lambda})_{u,v}\coloneqq\hat{g}_{v}^{(l)}(\lambda _{u};\vartheta_{v})\) (see SS 3.2.1), and truncated \(\bm{V}\in\mathbb{R}^{n\times k},\bm{\lambda}\in\mathbb{R}^{k}\). Due to the combination of message passing with spectral filters, S\({}^{2}\)GNNs' hypothesis class goes beyond (finite-order) polynomials of the Laplacian \(\bm{L}\) (or stacks message passing layers), unlocking a larger class of filters. In Algo. 1, we provide pseudo code for S\({}^{2}\)GNNs (Eq. 1).

**Truncated spectrum.** We omit extra notation for the truncated eigendecompositon \(\mathrm{EVD}(\bm{L},k)\) since it is equivalent to define \(\hat{g}(\lambda_{j})=0\) for \(j>k\). However, truncating after the \(k\)-th eigenvector requires care with the last eigenspace to maintain permutation equivariance. Due to the

Figure 4: Exemplary (lowest) eigenspaces.

Figure 3: **Message-passing interpretation of \(\bm{V}(\hat{g}_{\vartheta}(\bm{\lambda})\odot[\bm{V}^{\top}\bm{X}])\) (spectral filter): via the Fourier coefficients they may **exchange information globally** and allow **intra-cluster message passing**. Edge width/color denotes the magnitude/sign of \(\bm{V}\).

ambiguity of eigenvectors in the presence of repeated eigenvalues, we must ensure that we only include eigenspaces in their entirety. That is, we only include \(\{\lambda_{j}\,|\,j\leq k\wedge\lambda_{j}\neq\lambda_{k+1}\}\). Thus, \(\operatorname{Spectral}(\boldsymbol{H}^{(l-1)};\operatorname{EVD}(\boldsymbol{L},k))\) is permutation equivariant nonetheless. We defer all proofs to SS H.

**Theorem 1**.: \(\operatorname{Spectral}(\boldsymbol{H}^{(l-1)};\operatorname{EVD}(\boldsymbol{L},k))\) _of Eq. 3 is equivariant to all \(n\times n\) permutation matrices \(\boldsymbol{P}\in\mathcal{P}\): \(\operatorname{Spectral}(\boldsymbol{P}\boldsymbol{H}^{(l-1)};\operatorname{EVD }(\boldsymbol{P}\boldsymbol{L}\boldsymbol{P}^{\top},k))=\boldsymbol{P} \operatorname{Spectral}(\boldsymbol{H}^{(l-1)};\operatorname{EVD}(\boldsymbol{ L},k))\)._

**Complementary high-resolution filters.** Our _Spectral filters are highly discriminative between the frequencies_ and, e.g., can readily access a single eigenspace. Yet, for efficiency, we limit the spectral filter to a specific frequency band. _Due to the combination with message passing, this choice of band does not decide on, say, low-pass behavior; it solely determines where to increase the spectral selectivity._ While \(\operatorname{S}^{2}\)GNNs with subsequent guarantees adapt to domain-specific choices for the spectral filter's frequency band, a sensible default is to focus on the low frequencies. The two main reasons for this are (see SS J.3 for an extensive list): (1) Low frequencies model the smoothest global signals w.r.t. the graph structure (see Fig. 3 & 4). (2) Under a relative perturbation model (perturbation budget proportional to degree), stability implies \(C\)-integral-Lipschitzness (\(\exists C>0\): \(|\lambda^{\bar{d}\bar{g}/\underline{d}\lambda}|\leq C\)), i.e., the filter can vary strongly around zero but must level out for large \(\lambda\) (see Gama et al. (2020)).

### Theoretical Analysis

We show that how \(\operatorname{S}^{2}\)GNNs alleviate oversquashing in SS 3.1.1. Next, SS 3.1.2 makes the approximation-theoretic advantages precise.

#### 3.1.1 \(\operatorname{S}^{2}\)GNNs Vanquish Over-Squashing

Alon & Yahav (2020) pointed out that MPGNNs must pass information through bottlenecks that connect different communities using fixed-size embedding vectors. Topping et al. (2022) and Di Giovanni et al. (2023a) formalize this via an \(L^{1}\)-norm Jacobian sensitivity analysis: \(\|\partial\mathbf{h}_{v}^{(\ell)}/\partial\mathbf{h}_{u}^{(0)}\|_{L^{1}}\) models the output's \(\mathbf{h}_{v}^{(\ell)}\) change if altering input \(\mathbf{h}_{u}^{(0)}\). MPGNNs' Jacobian sensitivity typically decays \(\mathcal{O}\left(\exp\left(-r\right)\right)\) with node distance \(r\) if the number of walks between the two nodes is small. See SS F for results of Di Giovanni et al. (2023a).

\(\operatorname{S}^{2}\)GNNs are not prone to such an exponential sensitivity decay due to their global message scheme. We formalize this in Theorem 2, refer to Fig. 4 for intuition and Fig. 5 for empirical verification. All theoretical guarantees hold if a \(\theta\) exists such that \(f_{\theta}=\boldsymbol{I}\).

**Theorem 2**.: _An \(\ell\)-layer \(\operatorname{S}^{2}\)GNN can be parametrized s.t. output \(\mathbf{h}_{v}^{(\ell)}\)has a uniformly lower-bounded Jacobian sensitivity on a connected graph: \(\|\partial\mathbf{h}_{v}^{(\ell)}/\partial\mathbf{h}_{u}^{(0)}\|_{L^{1}}\geq C _{\theta}d/m\) with rows \(\tilde{\boldsymbol{h}}_{u}^{(0)}\), \(\boldsymbol{h}_{v}^{(\ell)}\) of \(\boldsymbol{H}^{(0)}\), \(\boldsymbol{H}^{(\ell)}\) for nodes \(u\), \(v\in\mathcal{G}\), a parameter-dependent \(C_{\theta}\), network width \(d\) and edge count \(m\)._

In contrast to Di Giovanni et al. (2023a), we prove a lower bound for \(\operatorname{S}^{2}\)GNNs, guaranteeing a minimum "influence" for any \(u\) on \(v\). This is true since \(\operatorname{S}^{2}\)GNNs contain a virtual node as a special case with \(\hat{g}_{\theta}^{(l)}(\boldsymbol{\lambda})=\mathds{1}_{\{0\}}\), with \(\mathds{1}_{\mathcal{S}}\) denoting the indicator function of a set \(\mathcal{S}\) (see also SS E). However, we find that a virtual node is insufficient for some long-range tasks, including our long-range clustering (LR-CLUSTER) of Fig. 9(b). Hence, the exponential sensitivity decay of spatial MPGNNs only shows their inadequacy in long-range settings. Proving its absence is not sufficient to quantify long-range modeling capabilities, noting that the lower bound is not tight for \(\operatorname{S}^{2}\)GNNs on many graphs. We close this gap with our subsequent analysis rooted in polynomial approximation theory.

#### 3.1.2 Approximation Theory: Superior Error Bounds Despite Spectral Cutoff

To demonstrate how \(\operatorname{S}^{2}\)GNNs can express a more general hypothesis class than MPGNNs, we study how well an "idealized" GNN (IGNN) can be approximated. Each IGNN layer \(l\) can express convolution operators \(g^{(l)}\) of _any_ spectral form \(\hat{g}^{(l)}\): \([0,2]\to\mathbb{R}\). We approximate IGNNs with \(\operatorname{S}^{2}\)GNNs from Eq. 1, with a spectral filter as in Eq. 3 and a spatial part parametrized by a polynomial. While we assume here that the \(\operatorname{S}^{2}\)GNN spectral filter is bandlimited to and a universal approximator on the interval \([0,\lambda_{\text{max}}]\), the findings generalize to, e.g., a high-pass interval. In the main body, we focus on the key insights for architectures without nonlinear activations. Wang & Zhang (2022) prove that even linear IGNNs can produce any one-dimensional output under certain regularity assumptions on the graph and input signal. Thus, we solely need to consider a single layer. In SS H.4, we cover the generic setting including nonlinearities, where multiple layers are helpful.

Figure 5: Spectral filters do not exhibit oversquashing on “Clique Path” graphs (Di Giovanni et al., 2023a).

**Locality relates to spectral smoothness.** The locality of the true/ideal filter \(g\) is related to the smoothness of its Fourier transform \(\hat{g}\). For instance, if \(g\) is a low-order polynomial of \(\bm{L}\), it is localized to a few-hop neighborhood, and \(\hat{g}\) is regularized to vary slowly (Fig. 5(a) w/o discontinuity). The other extreme is a discontinuous spectral filter \(\hat{g}\), such as the entirely non-local virtual node filter, \(\hat{g}=\mathds{1}_{\{0\}}\) (discontinuity in Fig. 5(a), details in SS E). This viewpoint of spectral smoothness illuminates the limitations of finite-hop message passing from an angle that complements spatial analyses in the over-squashing picture. It informs a lower bound on the error, which shows that spatial message passing, i.e, order-\(p\) polynomial graph filters \(g_{\bm{\gamma}_{p}}\) with \(p+1\) coefficients \(\bm{\gamma}_{p}\in\mathbb{R}^{p+1}\), can converge exceedingly slowly - slower than any inverse root (!) of \(p\) - to a discontinuous ground truth in the Frobenius-induced operator norm:

**Theorem 3**.: _Let \(\hat{g}\) be a discontinuous spectral filter. For any approximating sequence \(\big{(}g_{\bm{\gamma}_{p}}\big{)}_{p\in\mathbb{N}}\) of polynomial filters, an adversarial sequence \((\mathcal{G}_{p})_{p\in\mathbb{N}}\) of input graphs exists such that_

\[\hat{\exists}\alpha\in\mathbb{R}_{>0}\colon\sup_{0\neq\bm{X}\in\mathbb{R}^{| \mathcal{G}_{p}|\times d}}\frac{\|(g_{\bm{\gamma}_{p}}-g)\ast_{\mathcal{G}_{p} }\bm{X}\|_{\mathrm{F}}}{\|\bm{X}\|_{\mathrm{F}}}=\mathcal{O}\left(p^{-\alpha}\right)\]

**Superior S\({}^{2}\)GNN error bound.** A spatio-spectral convolution wins over a purely spatial filter when the sharpest irregularities of the ground truth \(\hat{g}\) are within reach of its expressive spectral part. The spatial part, which can "focus" on learning the remaining, smoother part outside of this window, now needs much fewer hops to give a faithful approximation. We illustrate this principle in Fig. 6 where we approximate an additive combination of an order-three polynomial filter with discontinuous low-pass. Only the S\({}^{2}\) filter is faithfully approximating this filter. Formally, we find:

**Theorem 4**.: _Assume \(\hat{g}\big{|}_{[\lambda_{\text{cut}},2]}\) is \(r\)-times continuously differentiable on \([\lambda_{\text{cut}},2]\), and a bound \(K_{r}(\hat{g},\lambda_{\text{cut}})\geq 0\) such that \(\big{|}\frac{d^{2}}{d\lambda^{2}}\hat{g}(\lambda)\big{|}\leq K_{r}(\hat{g}, \lambda_{\text{cut}})\ \forall\lambda\in[\lambda_{\text{cut}},2]\). An approximating S\({}^{2}\)GNN sequence with parameters \(\big{(}\hat{g}^{*}_{p},\bm{\gamma}^{*}_{p}\big{)}_{p\in\mathbb{N}}\) exists such that, for arbitrary graph sequences \((\mathcal{G}_{p})_{p\in\mathbb{N}}\),_

\[\sup_{0\neq\bm{X}\in\mathbb{R}^{|\mathcal{G}_{p}|\times d}}\frac{\|(g_{\bm{ \gamma}^{*}_{p}}+g_{\bm{\gamma}^{*}_{p}}-g)\ast_{\mathcal{G}_{p}}\bm{X}\|_{ \mathrm{F}}}{\|\bm{X}\|_{\mathrm{F}}}=\mathcal{O}\left(K_{r}(\hat{g},\lambda_ {\text{cut}})p^{-r}\right)\]

_with a scaling constant that depends only on \(r\), not on \(\hat{g}\) or \((\mathcal{G}_{p})_{p\in\mathbb{N}}\)._

The above bound extends to purely spatial convolutions in terms of \(K_{r}(\hat{g},0)\) if \(\hat{g}\) is \(r\)-times continuously differentiable on the full interval \([0,2]\). The S\({}^{2}\)GNN bound of Theorem 4 is then still strictly tighter if \(K_{r}(\hat{g},\lambda_{\text{cut}})<K_{r}(\hat{g},0)\). In particular, taking the limit \(K_{1}(\hat{g},0)\to\infty\) towards discontinuity makes the purely spatial upper bound arbitrarily loose, whereas a benign filter might still admit a small \(K_{1}(\hat{g},\lambda_{\text{cut}})\) for some \(\lambda_{\text{cut}}>0\). Theorem 3 suggests that this is not an artifact of a loose upper bound but that there is an inherent difficulty in approximating unsmooth filters with polynomials.

We conclude the analysis by instantiating the bounds: assuming \(\hat{g}\) is \(C\)-integral-Lipschitz for stability reasons (see Gama et al. (2020) and the paragraph before SS 3.1.1) yields \(K_{1}(\hat{g},\lambda_{\text{cut}})=C/\lambda_{\text{cut}}\), whereas for the electrostatics example \(\hat{g}_{\sigma}\) in SS G, we find upper bounds \(K_{r}(\hat{g}_{\sigma},\lambda_{\text{cut}})=\nicefrac{{r}}{{\lambda_{\text{ cut}}^{r+1}}}\). In both cases, the pure spatial bound diverges as smoothness around \(0\) remains unconstrained.

### Design Space

As shown in Fig. 2, we identify three major, yet unexplored, directions in S\({}^{2}\)GNNs' design space. In SS 3.2.1, we discuss how we parametrize the spectral filter. In SS 3.2.2, we propose the first neural network for the spectral domain. That is, we allow transformations and non-linearities in the "Fourier" domain. In SS 3.2.3, we are the first to instantiate spectral filters for directed graphs. Additionally,

Figure 6: S\({}^{2}\) filter perfectly approximates true filter (a) with a discontinuity at \(\lambda=0\), while polynomial (“Spa.”) and spectral (“Spec.”) alone do not. (b) shows responses on a path graph.

due to the availability of the partial eigendecomposition, positional encodings may dual use them to improve expressivity at negligible cost. In SS 3.2.4, we propose the first permutation equivariant, stable and efficient positional encodings that provably admit an expressivity beyond 1-WL. SS 3.2.4 provides further details and considerations, like some remarks on batching (SS 3.7). For the (sub-) design space of spatial message passing (You et al., 2020), we refer to its rich literature.

#### 3.2.1 Parametrizing Spectral Filters

For spectral filter function \(\hat{g}_{\vartheta}(\bm{\lambda})\) of Eq. 3, we learn a channel-wise linear combination of translated Gaussian basis functions (see "Gaussian smearing" used by Schutt et al. (2017)), as depicted in Fig. 7. This choice (1) may represent any possible \(\hat{g}_{\vartheta}(\bm{\lambda})\) with sufficient resolution (assumption in SS 3.1.2); (2) avoids overfitting towards numerical inaccuracies of the eigenvalue calculation; (3) limits the discrimination of almost repeated eigenvalues and, in turn, should yield stability (similar to SS 3.2.4). Strategies to cope with a variable \(\lambda_{\text{cut}}\) and \(k\) (e.g., using attention similar to SpecFormer (Bo et al., 2023)) did usually not yield superior experimental results.

**Window.** We multiply the learned combinations of Gaussians by an envelope function (we choose a Tukey window) that decays smoothly to zero around cutoff \(\lambda_{\text{cut}}\). This counteracts the so-called "Gibbs phenomenon" (aka "ringing"): as visualized for a path graph/sequence of 100 nodes in Fig. 8, trying to approximate a spatially-discontinuous target signal using an ideal low-pass range of frequency components results in an overshooting oscillatory behavior near the spatial discontinuity. Dampening the frequencies near \(\lambda_{\text{cut}}\) by a smooth envelope/window function alleviates this behavior. We note that the learned filter may, in principle, overrule the windowing at the cost of a higher weight decay penalty. See Algo. 2 for \(\hat{g}_{\vartheta}(\bm{\lambda})\)'s algorithmic description.

**Depth-wise separable convolution**(Sifre, 2014; Howard et al., 2017): Applying different filters for each dimension is computationally convenient for spectral filters. While "full" convolutions are also possible, we find that such a construction is more prone to over-fitting. In practice, we even use parameter sharing and apply fewer filters than dimensions to counteract over-fitting. We argue that sharing filters among dimensions is similar to the heads in a transformer (Vaswani et al., 2017).

**Feature transformations \(f_{\theta}^{(l)}\).** As sketched in Fig. 3 & 4, all nodes participate in the global data transfer. While this global message-passing scheme is _graph-adaptive_, it does not adjust to the inputs. For adaptivity, we typically consider non-linear feature transformations \(f_{\theta}^{(l-1)}(\bm{H}^{(l-1)})\), like gating mechanism \(f_{\theta}^{(l-1)}(\bm{H}^{(l-1)})=\bm{H}^{(l-1)}\odot\sigma^{\prime}(\bm{H}^ {(l-1)}\bm{W}_{G}^{(l)}+\overline{1}\bm{b}^{\top}))\) with element-wise multiplication \(\odot\), SiLU function \(\sigma^{\prime}\), learnable weight \(\bm{W}\), and bias \(\bm{b}\). A linear transformation \(f_{\theta}^{(l)}(\bm{H}^{(l-1)})=\bm{H}^{(l-1)}\bm{W}^{(l)}\) is another interesting case since we may first apply the GFT and then the transformation: \((\bm{V}^{\top}\bm{H}^{(l-1)})\bm{W}^{(l)}\). Next, we extend this linear transformation to a neural network in the spectral domain by adding multiple transformations and nonlinearities.

#### 3.2.2 Neural Network for the Spectral Domain

Applying a neural network \(s_{\zeta}\) in the spectral domain is highly desirable due to its negligible computational cost if \(k\ll n\). Moreover, \(s_{\zeta}\) allows the spectral filter to become data-dependent and may mix between channels. Data-dependent filtering is one of the properties that is hypothesized to make transformers powerful Fu et al. (2023). We propose the first neural network for the spectral domain of graph filters \(s_{\zeta}^{(l)}:\mathbb{R}^{k\times d^{(l)}}\rightarrow\mathbb{R}^{k\times d^{ (l)}}\) that is designed to preserve permutation equivariance.

\[\bm{H}^{(l)}=\mathrm{Spectral}^{(l)}(\bm{H}^{(l-1)};\bm{V},\bm{\lambda})=\bm{ V}s_{\zeta}^{(l)}\Big{(}\hat{g}_{\vartheta}^{(l)}(\bm{\lambda})\odot\big{[} \bm{V}^{\top}f_{\theta}^{(l)}(\bm{H}^{(l-1)})\big{]}\Big{)}\] (4)

We achieve permutation equivariance via sign equivariance \(s_{\zeta}(\bm{S}\odot\bm{X})=\bm{S}\odot s_{\zeta}(\bm{X})\,,\,\forall\bm{S} \in\{-1,1\}^{k\times d^{(l)}}\), combined with a permutation equivariance \(s_{\zeta}(\bm{P}\bm{X})=\bm{P}s_{\zeta}(\bm{X})\,,\,\bm{P}\in\mathcal{P}_{k}\), where \(\mathcal{P}_{k}\) is the set of all \(k\times k\) permutation matrices. Specifically, we stack linear mappings \(\bm{W}_{s}\in\mathbb{R}^{d^{(l)}\times d^{(l)}}\) (_without bias_) with a gated nonlinearity \(\phi(\hat{\bm{H}})=\hat{\bm{H}}\odot\sigma(\overline{1}\big{[}\bm{m}^{\top}\bm {W}_{a}+\bm{b}_{a}^{\top}])\) with sigmoid \(\sigma\), column-wise norm \(m_{j}=\|\hat{\bm{H}}_{:,j}\|\), and learnable \(\bm{W}_{a}\in\mathbb{R}^{d^{(l)}\times d^{(l)}}\) as well as \(\bm{b}_{a}\in\mathbb{R}^{d^{(l)}}\).

Figure 8: Ringing of ideal low pass filter on path graph.

#### 3.2.3 Directed Graphs

Directed graphs are an important topic that did not discuss so far. For S\({}^{2}\)GNNs to generalize the capabilities of non-local sequence models like transformers (Vaswani et al., 2017) or SSMs (Poli et al., 2023; Gu and Dao, 2023) it is vital to support direction, e.g., for distinguishing source/beginning and sink/end. However, all discussion before assumed the existence of the eigenvalue _decomposition_ of \(\bm{L}\). This was the case for symmetric \(\bm{L}\); however, for directed graphs, \(\bm{L}\) may be asymmetric.

To guarantee \(\bm{L}\) is diagonalizable with real eigenvalues, we use the Magnetic Laplacian (Forman, 1993; Shubin, 1994; De Verdiere, 2013) which is Hermitian and models direction in the complex domain: \(\bm{L}_{q}=\bm{I}-(\bm{D}_{s}^{-1/2}\bm{A}_{s}\bm{D}_{s}^{-1/2})\odot\exp[i2 \pi q(\bm{A}-\bm{A}^{\top})]\) with symmetrized adjacency/degrees \(\bm{A}_{s}\)/\(\bm{D}_{s}\), potential \(q\in[0,2\pi]\), element-wise exponential \(\exp\), and imaginary unit \(i^{2}=-1\). While other parametrizations of a Hermitian matrix are also possible, with \(\bm{A}\in\{0,1\}^{n\times n}\) and appropriate choice of \(q\), \(\bm{L}_{q}:\{0,1\}^{n\times n}\to\mathbb{C}^{n\times n}\) is _injective_. In other words, every possible asymmetric \(\bm{A}\) maps to exactly one \(\bm{L}_{q}\) and, thus, this representation is lossless. Moreover, for sufficiently small potential \(q\), the order of eigenvalues is well-behaved (Furutani et al., 2020). In contrast to Koke and Cremers (2024), a Hermitian parametrization of spectral filters does not require a dedicated propagation for forward and backward information flow. For simplicity we choose \(q<\nicefrac{{1}}{{n_{\max}}}\) with maximal number of nodes \(n_{\max}\) (with binary \(\bm{A}\)). This choice ensures that the first eigenvector suffices to obtain, e.g., the topological sorts of a Directed Acyclic Graph (DAG). Due to the real eigenvalues of a Hermitian matrix, the presented content generalizes with minor adjustments. Most notably, we use a feature transformation \(f_{\theta}^{(l)}:\mathbb{R}^{n\times d^{(l-\bm{R})}}\to\mathbb{C}^{n\times d^{ (l)}}\) and map back into the real domain after the spectral convolution. We give more implementation details in SS J.6 and provide additional background on directed graphs in SS C.

#### 3.2.4 Efficient Yet Stable and Expressive Positional Encodings

The availability of the partial eigendecomposition allows for their _dual use_ for positional encodings at negligible cost. Motivated by this, we propose the first efficient (\(\mathcal{O}(km)\)) and (fully) permutation equivariant spectral Positional Encodings \(\mathrm{PE}\) that provably increase the expressivity strictly beyond the 1-Weisfeiler-Leman (1-WL) test (Xu et al., 2019; Morris et al., 2019). In contrast to the Laplacian encodings of Dwivedi and Bresson (2021), our \(\mathrm{PE}\) do not require augmenting eigenvectors w.r.t. their sign and maintain permutation equivariance also in the presence of repeated eigenvalues. In comparison to Huang et al. (2024), our \(\mathrm{PE}\) come with drastically lower computational cost and have no learnable parameters. Due to the absence of learnable parameters, we need to calculate our \(\mathrm{PE}\) only once.

We construct our \(k\)-dimensional positional encodings \(\mathrm{PE}(\bm{V},\bm{\lambda})\in\mathbb{R}^{n\times k}\) as

\[\mathrm{PE}(\bm{V},\bm{\lambda})=||_{j=1}^{k}[(\bm{V}\hat{h}_{j}(\bm{\lambda}) \bm{V}^{\top})\odot\bm{A}]\cdot\vec{1}\] (5)

with concatenation \(||\) and binary adjacency \(\bm{A}\in\{0,1\}^{n\times n}\). We use a Radial Basis Function (RBF) filter with normalization around each eigenvalue \(\hat{h}_{j}(\bm{\lambda})=\mathrm{softmax}(\nicefrac{{(\lambda_{j}-\bm{ \lambda})\odot(\lambda_{j}-\bm{\lambda})}}{{\sigma^{2}}})\) with small width \(\sigma\in\mathbb{R}_{>0}\). This parametrization is not only permutation equivariant but also stable according to the subsequent definition via the Holder continuity. Note that \(C\) depends on the eigengap between \(\nicefrac{{1}}{{(\lambda_{k+1}-\lambda_{k})}}\) at the frequency cutoff (for exact constant \(C\) see proof in SS H.5).

**Definition 1** (Stable \(\mathrm{PE}\)).: _(Huang et al., 2024) A PE method \(\mathrm{PE}:\mathbb{R}^{n\times k}\times\mathbb{R}^{k}\to\mathbb{R}^{n\times k}\) is called stable, if there exist constants \(c,C>0\), such that for any Laplacian \(\bm{L}\), \(\bm{L}^{\prime}\), and \(\bm{P}_{*}=\arg\min_{\bm{P}}\left\|\bm{L}-\bm{P}\bm{L}^{\prime}\bm{P}^{\top} \right\|_{\mathrm{F}}\)_

\[\left\|\mathrm{PE}(\mathrm{EVD}(\bm{L}))-\bm{P}_{*}\,\mathrm{PE}\left(\mathrm{ EVD}\left(\bm{L}^{\prime}\right)\right)\right\|_{\mathrm{F}}\leq C\cdot \left\|\bm{L}-\bm{P}_{*}\bm{L}^{\prime}\bm{P}_{*}^{\top}\right\|_{\mathrm{F}}^ {c}.\] (6)

**Theorem 5**.: _The Positional Encodings \(\mathrm{PE}\) in Eq. 5 are stable according to Definition 1._

Next to their stability, our \(\mathrm{PE}\) can discriminate certain degree-regular graphs (e.g., Fig. 9). Since degree-regular graphs cannot be distinguished by 1-WL, our \(\mathrm{PE}\) makes the equipped GNN (as expressive as 1-WL) strictly more expressive than 1-WL. See SS I for continued expressivity analyses.

**Theorem 6**.: \(S^{2}\)_GNNs are strictly more expressive than 1-WL with the \(\mathrm{PE}\) of Eq. 5._

Figure 9: \(\mathrm{PE}\) discriminates the depicted degree-regular degree-regular graphs, except for (a) vs. (c).

## 4 Empirical Results

With state-of-the-art performance on the peptides-func task of the long-range benchmark (Dwivedi et al., 2022), plus strong results on further benchmarks, we demonstrate that _S\({}^{2}\)GCN, a GCN paired with spectral filters_, is highly capable of **modeling long-range interactions (SS 4.1)**. We assess S\({}^{2}\)GNNs' **long sequence performance (SS 4.2)** (mechanistic in-context learning) and show that S\({}^{2}\)GCN, a graph machine learning method, can achieve competitive results to state-of-the-art sequence models, including H3, Hyena, and transformers. We exemplify S\({}^{2}\)GNNs' practicality and competitiveness at scale on **large-scale benchmarks (SS 4.3)** like TPUGraphs (Phothilimthana et al., 2023), PCQM4Mv2 (Hu et al., 2021), and Open Graph Benchmark (OGB) Products (Hu et al., 2020). Further, in SS 8.7, we report state-of-the-art performance on the heterophilic arXiv-year (Lim et al., 2021) and,in SS 4.4, we study combinations of spatial and spectral filters beyond Eq. 1 & 2.

**Setup.** We pair different MPGNNs with spectral filters and name the composition S\({}^{2}\)<base>. For example, a S\({}^{2}\)GNN with GAT as base will be called S\({}^{2}\)GAT. We typically perform 3 to 10 random reruns and report the mean \(\pm\) standard deviation. The experiments of SS 4.1 require <11 GB (e.g. Nvidia GTX 1080Ti); for the experiments in SS 4.2 & 4.3 we use a 40 GB A100. We usually optimize weights with AdamW (Loshchilov and Hutter, 2019) and cosine annealing scheduler (Loshchilov and Hutter, 2017). We use early stopping based on the validation loss/score. See SS 4.1 for more details and https://www.cs.cit.tum.de/daml/s2gnn for code as well as supplementary material.

### Long-Range Interactions

**Finding (I): S\({}^{2}\)GCN outperforms state-of-the-art graph transformers, MPGNNs, and graph rewirings** on the peptides-func long-range benchmarks (Dwivedi et al., 2022) by a substantial margin. Simultaneously, we remain approximately 35% below the 500k parameter threshold and. On peptides-struct we are only outperformed by NBA-GIN (Park et al., 2023). We extend the best configuration for a GCN of Tonshoff et al. (2023) (see GCN in Table 1), lower the number of message passing steps from six to three, and interleave spatial and spectral filters (Eq. 2) with \(\lambda_{\text{cut}}=0.7\).

**Dataset contribution: Clustering**, given a single seed node per cluster, measures the ability (1) to spread information within the cluster and (2) to discriminate between the clusters. We complement the semi-supervised task CLUSTER from Dwivedi et al. (2023) with **(our)** LR-CLUSTER dataset, a scaled-upon version with long-range interactions (1). We closely follow Dwivedi et al. (2023), but instead of using graphs sampled from Stochastic Block Models (SBMs), we sample coordinates from a Gaussian Mixture Model (GMM) and then connect nearby nodes. CLUSTER has 117 nodes on average, while ours has 896. LR-CLUSTER has an average diameter of \(\approx 33\) and often contain hub nodes that cause over-squashing. For full details on the dataset construction, see SS 4.1.

**Dataset contribution: Distance regression** is a task with long-range interactions used in prior work (Geisler et al., 2023; Lim et al., 2023). Here, the regression targets are the shortest path distances to the only root node (in-degree 0). We generate random trees/DAGs with \(\approx\)750 # of nodes on average (details are in SS 4.7). The target distances often exceed 30 hops. We evaluate on similarly sized graphs as in the training data, i.e., in-distribution (**ID**) samples, and out-of-distribution (**OOD**) samples that consist of slightly larger graphs. Details on the dataset construction are in SS 4.7.

**Finding (II): spatial MPGNNs are less effective as S\({}^{2}\)GNNs, for long-range interactions. This is evident for peptides Table 1, clustering Fig. 10, distance regression Fig. 11, and over-squashing Fig. 12. Specifically, if the task requires long-range interactions beyond the receptive field of MPGNNs, they return crude estimates. E.g., in Fig. 11, the MPGNN predicts (approx.) constantly 20 for all distances beyond its receptive field - roughly the mean in the training data. Moreover,

\begin{table}
\begin{tabular}{c c c c} \hline  & **Model** & **peptides-func (\(\uparrow\))** & **peptides-struct (\(\downarrow\))** \\ \hline \multirow{7}{*}{**S\({}^{2}\)GNN**} & TGT (Cohi et al., 2024) & \(0.679\pm 0.0074\) & \(0.2458\pm 0.0015\) \\  & MGT+WPE (Ngo et al., 2023) & \(0.8617\pm 0.0064\) & \(0.2453\pm 0.0025\) \\  & MGLMAP (He et al., 2023) & \(0.6921\pm 0.0054\) & \(0.2475\pm 0.0015\) \\  & Graph (VV He et al., 2023) & \(0.6842\pm 0.0075\) & \(0.2449\pm 0.0016\) \\  & GRIF (Mai et al., 2023) & \(0.6988\pm 0.0082\) & \(0.2460\pm 0.0012\) \\  & GPS+MSP+GIN (Jung et al., 2024) & \(0.7156\pm 0.0078\) & \(0.2457\pm 0.0013\) \\ \hline
**Rewiring: DRes-GCN** & & \(0.7150\pm 0.0044\) & \(0.2536\pm 0.0015\) \\  & **Static Speed Models:** Graph Manna & \(0.7071\pm 0.0083\) & \(0.2473\pm 0.0025\) \\  & (Behov & Hashemi, 2024) & \(0.7133\pm 0.0011\) & \(0.2455\pm 0.0013\) \\  & GRIF (Behov & Hashemi, 2024) & \(0.6186\pm 0.0026\) & \(0.2453\pm 0.0032\) \\ \multirow{7}{*}{**S\({}^{2}\)GCN** (Jung et al., 2023) & \(0.6569\pm 0.0117\) & \(0.2523\pm 0.0013\) \\  & MNR-GIN (Park et al., 2023) & \(0.7071\pm 0.0067\) & \(0.2424\pm 0.0010\) \\ \cline{1-1}  & GCN (Toshoff et al., 2023) & \(0.6860\pm 0.0050\) & \(0.2460\pm 0.0007\) \\ \cline{1-1}  & \(S^{2}\)GCN (**Ours**) & \(0.7275\pm 0.0066\) & \(0.2467\pm 0.0019\) \\ \cline{1-1}  & \(\star\) PE (**ours**) & \(\mathbf{0.7311\pm 0.0066}\) & \(0.24417\pm 0.0032\) \\ \hline \end{tabular}
\end{table}
Table 1: Long-range benchmark. Our S\({}^{2}\)GNN uses \(\approx 35\%\) fewer parameters than the other models. AP is Peptides-func’s and MAE peptides-struct’s target metric. The best/second best is bold/unlored.

S\({}^{2}\)GNNs may converge faster (see Fig. 25 in SS M.6.2) and are more parameter-efficient, as we show on PCQM4Mv2 Hu et al. (2021) in SS M.9.

**Finding (III): virtual nodes are insufficient.** We frequently find that including more than a single eigenvector (\(k>1\)) yields substantial gains. We make this explicit in Fig. 11(a), where we append a single spectral layer and sweep over the number of eigenvectors \(k\). We complement these findings with an ablation for the frequency cutoff \(\lambda_{\text{cut}}\) on peptides-func in SS M.5.

**Finding (IV): our Positional Encodings \(\mathrm{PE}\) consistently help**, when concatenated to the node features. While this finding is true throughout our evaluation, the differences are more pronounced in certain situations. For example, on LR-CLUSTER in Fig. 11, the \(\mathrm{PE}\) help with spectral filter and a small \(k\) or without spectral filter and many message passing steps.

**Finding (V): spectral filters align with clusters**, as we illustrate in Fig. 14 for four arbitrary spectral filters learned on LR-CLUSTER. We observe that (a) the spectral filters reflect the true clustering structure, (b) some filters are smooth while others contain details, and (c) they model coarser or finer cluster structures (e.g., first vs. third filter).

### Sequence Modelling: Mechanistic In-Context Learning

Following the evaluation of Hyena (Poli et al., 2023) and H3 Fu et al. (2023), we benchmark S\({}^{2}\)GCN with sequence models on the _associative recall_ in-context learning task, stemming from mechanistic interpretability Elhage et al. (2021); Power et al. (2022); Zhang et al. (2023); Olsson et al. (2022). In associative recall, the model is asked to retrieve the value for a key given in a sequence. For example, in the sequence a,0,e,b,z,9,h,2,=>,z, the target is the value for key z, which is 9 since it follows z in its prior occurrences. We create a sequence/path graph with a node for each "token" (separated by "," in the example above) and label the target node with its value. We assess the performance of S\({}^{2}\)GCN on graphs that vary in size by almost two orders of magnitude and follow Poli et al. (2023) with a vocabulary of 30 tokens. Moreover, we finetune our S\({}^{2}\)GCN on up to 30k nodes.

**Finding (VI): our spectral filter for directed are effective** and may improve generalization, as we find in Fig. 14 (and Table 13 of SS M.7).

**Finding (VII): S\({}^{2}\)GCN a state-of-the-art sequence model**, as it performs on par with Hyena and, here, outperforms transformers (Table 2).

\begin{table}
\begin{tabular}{l c} \hline \hline
**Model** & **Accuracy (\(\uparrow\))** \\ \hline Transformer & _OOM_ \\ Vaswani et al. (2017) & _OOM_ \\ w/ FlashAttention & 0.324 \\ Dong et al. (2022) & 0.84 \\ Hyena & **1.000** \\ \(S^{2}\)GCN (ours)** & \(0.97\pm 0.05\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: 30k token associative recall.

Figure 14: S\({}^{2}\)GCN solves associative recall for sequences varying in size by two orders of magnitude. Grey area marks **ID**.

Figure 13: 4 filters on LR-CLUSTER. Large/small entries are yellow/blue, white lines mark clusters.

### Large-Scale Benchmarks

**Finding (VIII): S\({}^{2}\)GNNs is practical and scalable.** We demonstrate this on the OGB Products graph (2.5 mio. nodes, Table 3) and the (directed) 10 million graphs dataset TPUGraphs (average number of nodes \(\approx\)10,000, Table 4). In both cases, we find full-graph training (without segment training (Cao et al., 2023)) using 3 (Dir-) GCN layers interlayered with spectral filters, a reasonable configuration on a 40 GB A100. However, for OGB Products, we find that batching is superior, presumably because the training nodes are drawn from a "small" region of the graph (see SS K).

**The cost of partial \(\mathrm{EVD}\)** for each dataset (excluding TPUGraphs and distance regression) is between 1 to 30 minutes on CPUs. We report the detailed costs of \(\mathrm{EVD}\) and experiments in SS M.3.

## 5 Related Work

**Combining spatial and spectral filters** has recently attracted attention outside of the graph domain in models like Hyena (Poli et al., 2023), Spectral State Space Models (Agarwal et al., 2024), etc. with different flavors of parametrizing the global/FFT convolution. Nevertheless, the properties of spatial and spectral filter parametrization (e.g., local vs. global) are well-established in classical signal processing. A combination of spectral and spatial filters was applied to (periodic) molecular point clouds (Kosmala et al., 2023). For GNNs, Stachenfeld et al. (2020) compose a spatial and spectral message passing but do not handle the ambiguity of the eigendecomposition and, thus, do not maintain permutation equivariance. Moreover, Beaini et al. (2021) use the \(\mathrm{EVD}\) for localized anisotropic graph filters; Liao et al. (2019) propose an approach that combines spatial and spectral convolution via the Lanczos algorithm; and Huang et al. (2022) augment message passing with power iterations. Behrouz and Hashemi (2024) apply a Mambo-like state space model to graphs via arbitrarily ordering the nodes and, thus, sacrifice permutation equivariance.

**Long-range interactions on graphs.** Works that model long-range interactions can be categorized into: (a) MPGNNs on rewired graphs (Gasteiger et al., 2019, 2019; Gutteridge et al., 2023); (b) higher-order GNNs (Fey et al., 2020; Wollschlager et al., 2024) that, e.g., may pass information to distant nodes through hierarchical message passing schemes; and (c) message passing adaptations to facilitate long-range interactions. For example, Park et al. (2023) propose "non-backtracking" message passing, Errica et al. (2024) adaptively choose the numbers of message passing steps, and Ding et al. (2024) use linear RNNs to aggregate over each node's neighborhoods. While approaches (a-c) can increase the receptive field of GNNs, they are typically still spatially bounded. In contrast, (d) alternative architectures, like graph transformers (Ma et al., 2023; Dwivedi and Bresson, 2021; Kreuzer et al., 2021; Rampasek et al., 2022; Geisler et al., 2023; Deng et al., 2024) with global attention, may model all possible \(n\times n\) interactions. We provide notes on the limitations of graph transformers with absolute positional endodings in \(\lx@sectionsign\) D, which highlights the importance of capturing the relative relationships between nodes, as S\({}^{2}\)GNNs do. Moreover, in a recent/contemporary non-attention model for all pair-wise interactions, Batatia et al. (2024) use a resolvent parametrization of matrix functions relying on the LDL factorization of a matrix, but do not characterize their approximation-theoretic properties, over-squashing, expressivity on graphs, nor how to deal with directed graphs.

In SS B, we discuss additional related work w.r.t. expressivity and directed graphs.

## 6 Discussion

We propose S\({}^{2}\)GNNs, adept at efficiently modeling complex long-range interactions via the synergistic composition of spatially and spectrally parametrized filters (SS 3). We show that S\({}^{2}\)GNNs share many properties with graph rewirings, pooling, and hierarchical message passing schemes (Fig. 3 & 4). S\({}^{2}\)GNNs outperform the aforementioned techniques with a substantial margin on the peptides long-range benchmark (SS 4.1), and we show that S\({}^{2}\)GNNs are also strong sequence models, performing on par or outperforming state-of-the-art like Hyena or H3 in our evaluation (SS 4.2). Even though we find global graph models, like S\({}^{2}\)GNNs, more prone to overfitting (see SS K/L for further limitations/impact), moving to global models aligns with the trend for other deep learning domains.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Split** & **Model** & **Accuracy (\(\uparrow\))** & **F1 (\(\uparrow\))** \\ \hline \multirow{2}{*}{Train} & GAT & **0.86\(\pm\)**0.001 & 0.381\(\pm\)**0.001 \\  & S\({}^{2}\)GAT & **0.902\(\pm\)**0.000** & **0.472\(\pm\)**0.006** \\ \hline \multirow{2}{*}{Val} & GAT & 0.907\(\pm\)**0.001 & 0.508\(\pm\)**0.002** \\  & S\({}^{2}\)GAT & **0.913\(\pm\)**0.002** & **0.582\(\pm\)**0.014** \\ \hline \multirow{2}{*}{Test} & GAT & 0.798\(\pm\)**0.003 & 0.347\(\pm\)**0.004** \\  & S\({}^{2}\)GAT & **0.811\(\pm\)**0.007** & **0.381\(\pm\)**0.009** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Graph ranking on TPUGraphs “layout”.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Split** & **Model** & **Accuracy (\(\uparrow\))** & **F1 (\(\uparrow\))** \\ \hline \multirow{2}{*}{Train} & GAT & **0.866\(\pm\)**0.001 & 0.381\(\pm\)**0.001 \\  & S\({}^{2}\)GAT & **0.902\(\pm\)**0.000** & **0.472\(\pm\)**0.006** \\ \hline \multirow{2}{*}{Val} & GAT & 0.907\(\pm\)**0.001 & 0.508\(\pm\)**0.002** \\  & S\({}^{2}\)GAT & **0.913\(\pm\)**0.002** & **0.582\(\pm\)**0.014** \\ \hline \multirow{2}{*}{Test} & GAT & 0.798\(\pm\)**0.003 & 0.347\(\pm\)**0.004** \\  & S\({}^{2}\)GAT & **0.811\(\pm\)**0.007** & **0.381\(\pm\)**0.009** \\ \hline \hline \end{tabular}
\end{table}
Table 3: OGB Products.

## Acknowledgments and Disclosure of Funding

We want to express our gratitude to Nicholas Gao for his feedback and the discussions about modeling choices. Moreover, we thank Leo Schwinn and Tim Beyer for their helpful and on-point feedback and suggestions.

This research was supported by the Helmholtz Association under the joint research school "Munich School for Data Science - MUDS", as well as by the Munich Data Science Institute (MDSI) via the Linde/MDSI Doctoral Fellowship program and the MDSI Seed Fund.

## References

* Agarwal et al. (2024) Naman Agarwal, Daniel Suo, Xinyi Chen, and Elad Hazan. Spectral State Space Models, arXiv, 2024.
* Alon and Yahav (2020) Uri Alon and Eran Yahav. On the Bottleneck of Graph Neural Networks and its Practical Implications. In _International Conference on Learning Representations, ICLR_, 2020.
* Balcilar et al. (2021a) Muhammet Balcilar, Pierre Heroux, Benoit Gauzere, Pascal Vasseur, Sebastien Adam, and Paul Honeine. Breaking the Limits of Message Passing Graph Neural Networks. In _International Conference on Machine Learning, ICML_, 2021a.
* Balcilar et al. (2021b) Muhammet Balcilar, Guillaume Renton, and Pierre Heroux. Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective. In _International Conference on Learning Representations, ICLR_, 2021b.
* Batatia et al. (2024) Ilyes Batatia, Lars L Schaaf, Gabor Csanyi, Christoph Ortner, and Felix A Faber. Equivariant Matrix Function Neural Networks. In _International Conference on Learning Representations, ICLR_, 2024.
* Battaglia et al. (2018) Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks, arXiv, 2018.
* Beaini et al. (2021) Dominique Beaini, Saro Passaro, Vincent Letourneau, William L. Hamilton, Gabriele Corso, and Pietro Lio. Directional Graph Networks. In _International Conference on Machine Learning, ICML_, 2021.
* Behrouz and Hashemi (2024) Ali Behrouz and Farnoosh Hashemi. Graph Mamba: Towards Learning on Graphs with State Space Models, arXiv, 2024.
* Bo et al. (2023a) Deyu Bo, Chuan Shi, Lele Wang, and Renjie Liao. Specformer: Spectral Graph Neural Networks Meet Transformers. In _International Conference on Learning Representations, ICLR_, 2023a.
* Bo et al. (2023b) Deyu Bo, Xiao Wang, Yang Liu, Yuan Fang, Yawen Li, and Chuan Shi. A Survey on Spectral Graph Neural Networks, arXiv, 2023b.
* Bodnar et al. (2021) Cristian Bodnar, Catalina Cangea, and Pietro Lio. Deep Graph Mapper: Seeing Graphs Through the Neural Lens. _Frontiers in Big Data_, 4:38, 2021.
* Bresson and Laurent (2018) Xavier Bresson and Thomas Laurent. Residual Gated Graph ConvNets, arXiv, 2018.
* Bronstein et al. (2021) Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. _Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges_. arXiv, 2021.
* Bruna et al. (2014) Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral Networks and Locally Connected Networks on Graphs, arXiv, 2014.
* Cai et al. (2023) Chen Cai, Truong Son Hy, Rose Yu, and Yusu Wang. On the Connection Between MPNN and Graph Transformer. In _International Conference on Machine Learning, ICML_. arXiv, 2023.
* Cai et al. (2021)Shaofei Cai, Liang Li, Xinzhe Han, Jiebo Luo, Zheng-Jun Zha, and Qingming Huang. Automatic Relation-Aware Graph Network Proliferation. In _Conference on Computer Vision and Pattern Recognition, CVPR_, 2022.
* Cao et al. (2023) Kaidi Cao, Phitchaya Mangbo Phothilimthana, Sami Abu-El-Haija, Dustin Zelle, Yanqi Zhou, Charith Mendis, Jure Leskovec, and Bryan Perozzi. Learning Large Graph Property Prediction via Graph Segment Training. In _Neural Information Processing Systems, NeruIPS_. arXiv, 2023.
* Chen et al. (2023) Zhe Chen, Hao Tan, Tao Wang, Tianrun Shen, Tong Lu, Qiuying Peng, Cheng Cheng, and Yue Qi. Graph Propagation Transformer for Graph Representation Learning. In _International Joint Conference on Artificial Intelligence, IJCAI_, 2023.
* Chien et al. (2021) Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive Universal Generalized PageRank Graph Neural Network. In _International Conference on Learning Representations, {ICLR}_, 2021.
* Choi et al. (2024) Yun Young Choi, Sun Woo Park, Minho Lee, and Youngho Woo. Topology-Informed Graph Transformer, arXiv, 2024.
* Dao et al. (2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In _Neural Information Processing Systems, NeruIPS_. arXiv, 2022.
* De Verdiere (2013) Yves Colin De Verdiere. Magnetic interpretation of the nodal defect on graphs. _Analysis & PDE_, 6(5):1235-1242, 2013.
* Defferrard et al. (2017) Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In _Neural Information Processing Systems, NeurIPS_, 2017.
* Deng et al. (2024) Chenhui Deng, Zichao Yue, and Zhiru Zhang. Polynormer: Polynomial-Expressive Graph Transformer in Linear Time. In _International Conference on Learning Representations, ICLR_, 2024.
* Di Giovanni et al. (2023a) Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael Bronstein. On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology. In _International Conference on Machine Learning, ICML_. arXiv, 2023a.
* Di Giovanni et al. (2023b) Francesco Di Giovanni, T. Konstantin Rusch, Michael M. Bronstein, Andreea Deac, Marc Lackenby, Siddhartha Mishra, and Petar Velickovic. How does over-squashing affect the power of GNNs?, arXiv, 2023b.
* Ding et al. (2024) Yuhui Ding, Antonio Orvieto, Bobby He, and Thomas Hofmann. Recurrent Distance Filtering for Graph Representation Learning. In _International Conference on Machine Learning, ICML_, 2024.
* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In _International Conference on Learning Representations, ICLR_, 2021.
* Dwivedi and Bresson (2021) Vijay Prakash Dwivedi and Xavier Bresson. A Generalization of Transformer Networks to Graphs. _Deep Learning on Graphs at AAAI Conference on Artificial Intelligence_, 2021.
* Dwivedi et al. (2022) Vijay Prakash Dwivedi, Ladislav Rampasek, Mikhail Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long Range Graph Benchmark. In _Neural Information Processing Systems, NeruIPS_. arXiv, 2022.
* Dwivedi et al. (2023) Vijay Prakash Dwivedi, Chaitanya K. Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking Graph Neural Networks. _Journal of Machine Learning Research, JMLR_, 2023.
* Elhage et al. (2021) Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, and et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021.
* Errica et al. (2024) Federico Errica, Henrik Christiansen, Viktor Zaverkin, Takashi Maruyama, Mathias Niepert, and Francesco Alesiani. Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching, arXiv, 2024.
* Errica et al. (2021)* Fey and Lenssen (2019) Matthias Fey and Jan Eric Lenssen. Fast Graph Representation Learning with PyTorch Geometric, arXiv, 2019.
* Fey et al. (2020) Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical Inter-Message Passing for Learning on Molecular Graphs. In _Graph Representation Learning and Beyond (GRL+) Workshop at ICML 2020_. arXiv, 2020.
* Forman (1993) Robin Forman. Determinants of Laplacians on graphs. _Topology_, 32(1):35-46, 1993.
* Fu et al. (2023) Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Re. Hungry Hupgros: Towards Language Modeling with State Space Models. In _International Conference on Learning Representations, ICLR_, 2023.
* Furutani et al. (2020) Satoshi Furutani, Toshiki Shibahara, Mitsuaki Akiyama, Kunio Hato, and Masaki Aida. Graph Signal Processing for Directed Graphs Based on the Hermitian Laplacian. In _European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD_, 2020.
* Galkin et al. (2023) Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, and Zhaocheng Zhu. Towards Foundation Models for Knowledge Graph Reasoning, arXiv, 2023.
* Gama et al. (2020) Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability Properties of Graph Neural Networks. _IEEE Transactions on Signal Processing_, 68:5680-5695, 2020.
* Gasteiger et al. (2019a) Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph neural networks meet personalized PageRank. _International Conference on Learning Representations, ICLR_, 2019a.
* Gasteiger et al. (2019b) Johannes Gasteiger, Stefan Weissenberger, and Stephan Gunnemann. Diffusion Improves Graph Learning. _Neural Information Processing Systems, NeurIPS_, 2019b.
* Geerts (2021) Floris Geerts. On the Expressive Power of Linear Algebra on Graphs. _Theory Comput. Syst._, 65(1):179-239, 2021.
* Geisler et al. (2023) Simon Geisler, Yujia Li, Daniel Mankowitz, Ali Taylan Cemgil, Stephan Gunnemann, and Cosmin Paduraru. Transformers Meet Directed Graphs. In _International Conference on Machine Learning, ICML_, 2023.
* Gilmer et al. (2017) Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In _International Conference on Machine Learning, ICML_, 2017.
* Giusti et al. (2023) Lorenzo Giusti, Teodora Reu, Francesco Ceccarelli, Cristian Bodnar, and Pietro Lio. CIN++: Enhancing Topological Message Passing, arXiv, 2023.
* Grohe et al. (2021) Martin Grohe, Kristian Kersting, Martin Mladenov, and Pascal Schweitzer. Color Refinement and Its Applications. In Guy Van Den Broeck, Kristian Kersting, Sriraam Natarajan, and David Poole (eds.), _An Introduction to Lifted Probabilistic Inference_, pp. 349-372. The MIT Press, 2021.
* Gu and Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, arXiv, 2023.
* Guo and Wei (2023) Yuhe Guo and Zhewei Wei. Graph Neural Networks with Learnable and Optimal Polynomial Bases. In _International Conference on Machine Learning, ICML_. arXiv, 2023.
* Gutteridge et al. (2023) Benjamin Gutteridge, Xiaowen Dong, Michael Bronstein, and Francesco Di Giovanni. DRew: Dynamically Rewired Message Passing with Delay. In _International Conference on Learning Representations, ICLR_, 2023.
* Hammond et al. (2011) David K. Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via spectral graph theory. _Applied and Computational Harmonic Analysis_, 30(2):129-150, 2011.
* He et al. (2021) Mingguo He, Zhewei Wei, Zengfeng Huang, and Hongteng Xu. BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation. In _Neural Information Processing Systems, NeruIPS_, 2021.
* He et al. (2019)Mingguo He, Zhewei Wei, and Ji-Rong Wen. Convolutional Neural Networks on Graphs with Chebyshev Approximation, Revisited. In _Neural Information Processing Systems, NeurIPS_, 2022.
* He et al. (2023) Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A Generalization of ViT/MLP-Mixer to Graphs. In _International Conference on Machine Learning, ICML_, 2023.
* Hendrycks and Gimpel (2016) Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. _CoRR_, abs/1606.08415, 2016.
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and J Urgen Schmidhuber. Long Short-term Memory. _Neural Computation_, 9(8):1735 1780-1735 1780, 1997.
* Howard et al. (2017) Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, arXiv, 2017.
* Hu et al. (2020) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In _Neural Information Processing Systems, NeurIPS_, 2020.
* Hu et al. (2021) Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs, arXiv, 2021.
* Huang et al. (2022) Ningyuan Huang, Soledad Villar, Carey E. Priebe, Da Zheng, Chengyue Huang, Lin Yang, and Vladimir Braverman. From Local to Global: Spectral-Inspired Graph Neural Networks. In _New Frontiers in Graph Learning at NeurIPS_. arXiv, 2022.
* Huang et al. (2024) Yinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, and Pan Li. On the Stability of Expressive Positional Encodings for Graph Neural Networks. In _International Conference on Learning Representations, ICLR_, 2024.
* Hussain et al. (2022) Md Shamim Hussain, Mohammed J. Zaki, and Dharmashankar Subramanian. Global Self-Attention as a Replacement for Graph Convolution. In _International Conference on Knowledge Discovery and Data Mining, KDD_, pp. 655-665, 2022.
* Hussain et al. (2024) Md Shamim Hussain, Mohammed J. Zaki, and Dharmashankar Subramanian. Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers, arXiv, 2024.
* Kipf and Welling (2017) Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _International Conference on Learning Representations, ICLR_, 2017.
* Koke and Cremers (2024) Christian Koke and Daniel Cremers. HoloNets: Spectral Convolutions do extend to Directed Graphs. In _International Conference on Learning Representations, ICLR_, 2024.
* Kosmala et al. (2023) Arthur Kosmala, Johannes Gasteiger, Nicholas Gao, and Stephan Gunnemann. Ewald-based Long-Range Message Passing for Molecular Graphs. In _International Conference on Machine Learning, ICML_, 2023.
* Kreuzer et al. (2021) Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent Letourneau, and Prudencio Tossou. Rethinking Graph Transformers with Spectral Attention. In _Neural Information Processing Systems, NeurIPS_, 2021.
* Lam et al. (2023) Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, and Peter Battaglia. Learning skillful medium-range global weather forecasting. _Science_, 382(6677):1416-1421, 2023.
* LeCun et al. (1989) Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation Applied to Handwritten Zip Code Recognition. _Neural Computation_, 1(4):541-551, 1989.
* LeCun et al. (2017)Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In _International Conference on Machine Learning, ICML_, 2019.
* Lehoucq et al. (1998) R. B. Lehoucq, D. C. Sorensen, and C. Yang. _ARPACK Users' Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods_. Society for Industrial and Applied Mathematics, 1998.
* Li and Leskovec (2022) Pan Li and Jure Leskovec. The Expressive Power of Graph Neural Networks. In Lingfei Wu, Peng Cui, Jian Pei, and Liang Zhao (eds.), _Graph Neural Networks: Foundations, Frontiers, and Applications_, pp. 63-98. Springer Nature Singapore, Singapore, 2022.
* Li et al. (2020) Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance Encoding: Design Provably More Powerful Neural Networks for Graph Representation Learning. In _Neural Information Processing Systems, NeurIPS_, 2020.
* Liao et al. (2019) Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S. Zemel. LanczosNet: Multi-Scale Deep Graph Convolutional Networks. In _International Conference on Learning Representations, ICLR_. arXiv, 2019.
* Lim et al. (2021) Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim. Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods. In _Advances in Neural Information Processing Systems_, 2021.
* Lim et al. (2023) Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie Jegelka. Sign and Basis Invariant Networks for Spectral Graph Representation Learning. In _International Conference on Learning Representations, ICLR_, 2023.
* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In _International Conference on Learning Representations, ICLR_, 2017.
* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. _International Conference on Learning Representations, ICLR_, 2019.
* Luo et al. (2024) Yuankai Luo, Hongkang Li, Lei Shi, and Xiao-Ming Wu. Enhancing Graph Transformers with Hierarchical Distance Structural Encoding, arXiv, 2024.
* Ma et al. (2023) Liheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet K. Dokania, Mark Coates, Philip Torr, and Ser-Nam Lim. Graph Inductive Biases in Transformers without Message Passing. In _International Conference on Machine Learning, ICML_, 2023.
* Merchant et al. (2023) Amil Merchant, Simon Batzner, Samuel S. Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. _Nature_, 624(7990):80-85, 2023.
* Michel et al. (2023) Gaspard Michel, Giannis Nikolentzos, Johannes Lutzeyer, and Michalis Vazirgiannis. Path Neural Networks: Expressive and Accurate Graph Neural Networks. In _International Conference on Machine Learning, ICML_, 2023.
* Morris et al. (2019) Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gauvra Rattan, and Martin Grohe. Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks. _AAAI Conference on Artificial Intelligence_, 33:4602-4609, 2019.
* Natanson (1964) I. P. Natanson. Constructive function theory. Vol. I: Uniform approximation. Translated by Alexis N. Obolensky. New York: Frederick Ungar Publishing Co. IX, 232 p. (1964)., 1964.
* Ngo et al. (2023) Nhat Khang Ngo, Truong Son Hy, and Risi Kondor. Multiresolution Graph Transformers and Wavelet Positional Encoding for Learning Hierarchical Structures. _The Journal of Chemical Physics_, 159(3):034109, 2023.
* Olsson et al. (2022) Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context Learning and Induction Heads, arXiv, 2022.
* Olsson et al. (2019)Seonghyun Park, Narae Ryu, Gahee Kim, Dongyeop Woo, Se-Young Yun, and Sungsoo Ahn. Non-backtracking Graph Neural Networks, arXiv, 2023.
* [Phothilimthana et al.2023] Phitchaya Mangpo Phothilimthana, Sami Abu-El-Haija, Kaidi Cao, Bahare Fatemi, Charith Mendis, and Bryan Perozzi. TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs. In _Neural Information Processing Systems, NeruIPS_, 2023.
* Poli et al. [2023] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena Hierarchy: Towards Larger Convolutional Language Models. In _International Conference on Machine Learning, ICML_. arXiv, 2023.
* Power et al. [2022] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets, arXiv, 2022.
* Rampasek et al. [2022] Ladislav Rampasek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a General, Powerful, Scalable Graph Transformer. In _Neural Information Processing Systems, NeurIPS_, 2022.
* Rossi et al. [2023] Emanuele Rossi, Bertrand Charpentier, Francesco Di Giovanni, Fabrizio Frasca, Stephan Gunnemann, and Michael Bronstein. Edge Directionality Improves Learning on Heterophilic Graphs. In _Learning on Graphs Conference, LoG_, 2023.
* Schutt et al. [2017] Kristof T. Schutt, Pieter-Jan Kindermans, Huziel E. Sauceda, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Muller. SchNet: A continuous-filter convolutional neural network for modeling quantum interactions. In _Neural Information Processing Systems, NeruIPS_. arXiv, 2017.
* Shirzad et al. [2023] Hamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J. Sutherland, and Ali Kemal Sinop. Exphormer: Sparse Transformers for Graphs. In _International Conference on Machine Learning, ICML_, 2023.
* Shubin [1994] M. A. Shubin. Discrete Magnetic Laplacian. _Communications in Mathematical Physics_, 164(2):259-275, 1994.
* Sifre [2014] Laurent Sifre. _Rigid-Motion Scattering For Image Classification_. PhD thesis, Ecole Polytechnique, CMAP, 2014.
* Stachenfeld et al. [2020] Kimberly Stachenfeld, Jonathan Godwin, and Peter Battaglia. Graph Networks with Spectral Message Passing, arXiv, 2020.
* Tong et al. [2020] Zekun Tong, Yuxuan Liang, Changsheng Sun, David S. Rosenblum, and Andrew Lim. Directed Graph Convolutional Network, arXiv, 2020.
* Topping et al. [2022] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M. Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. In _International Conference on Learning Representations, ICLR_, 2022.
* Tonshoff et al. [2023] Jan Tonshoff, Martin Ritzert, Eran Rosenbluth, and Martin Grohe. Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark. In _Learning on Graphs Conference_, 2023.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Neural Information Processing Systems, NeurIPS_, 2017.
* Velickovic et al. [2018] Petar Velickovic, Arantxa Casanova, Pietro Lio, Guillem Cucurull, Adriana Romero, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations, ICLR_, 2018.
* van Luxburg [2007] Ulrike von Luxburg. A tutorial on spectral clustering. _Statistics and Computing_, 17(4):395-416, 2007.
* Wang et al. [2022] Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks. In _International Conference on Learning Representations, ICLR_, 2022.
* Wang et al. [2020]* Wang and Zhang (2022) Xiyuan Wang and Muhan Zhang. How Powerful are Spectral Graph Neural Networks. In _International Conference on Machine Learning, ICML_. arXiv, 2022.
* Wollschlager et al. (2024) Tom Wollschlager, Niklas Kemper, Leon Hetzel, Johanna Sommer, and Stephan Gunnemann. Expressivity and Generalization: Fragment Biases for Molecular GNNs, arXiv, 2024.
* Xu et al. (2018) Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken Ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In _International Conference on Machine Learning, ICML_, 2018.
* Xu et al. (2019) Keyulu Xu, Stefanie Jegelka, Weihua Hu, and Jure Leskovec. How powerful are graph neural networks? In _International Conference on Learning Representations, ICLR_, 2019.
* Yang et al. (2023) Mingqi Yang, Wenjie Feng, Yanming Shen, and Bryan Hooi. Towards Better Graph Representation Learning with Parameterized Decomposition & Filtering. In _International Conference on Machine Learning, ICML_, 2023.
* You et al. (2020) Jiaxuan You, Rex Ying, and Jure Leskovec. Design Space for Graph Neural Networks. pp. 13, 2020.
* Zaheer et al. (2017) Manzil Zaheer, Satwik Kottur, Siamak Ravanbhakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander J. Smola. Deep sets. In _Neural Information Processing Systems, NerulPS_, 2017.
* Zhang et al. (2021) Xitong Zhang, Yixuan He, Nathan Brugnone, Michael Perlmutter, and Matthew Hirn. MagNet: A Neural Network for Directed Graphs. In _Neural Information Processing Systems, NeurIPS_, 2021.
* Zhang et al. (2023) Yi Zhang, Arturs Backurs, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling Transformers with LEGO: a synthetic reasoning task, arXiv, 2023.

## Appendix

* 1 Notation
* 2 Related Work for Expressivity and Directed Graphs
* 3 Background for Directed Graphs
* 4 Limitations of Graph Transformers Using Absolute Positional Encodings
* 5 \(\mathbf{S^{2}}\)GNN Generalizes a Virtual Node
* 6 Existing Results on Over-Squashing
* 7 Construction of an explicit ground truth filter
* 8 Proofs
* 9 Proof of Theorem 1
* 10 Proof of Theorem 2
* 11 Proof of Theorem 3
* 12 Proof of Theorem 4
* 13 Proof of Theorem 5
* 14 Proof of Theorem 6
* 11 Expressivity of Spectral Filters and Spectrally Designed Spatial Filters
* 12 Further Remarks on \(\mathbf{S^{2}}\)GNNs
* 13 Visualization of Spectral Filters
* 13 Composition of Filters
* 14 Exhaustive Reasons Why Low Frequencies Are Sensible
* 15 Scaling to Graphs of Different Magnitude
* 16 Spectral Normalization
* 17 Adjusting \(\mathbf{S^{2}}\)GNNs to Directed Graphs
* 18 Computational Remarks
* 19 Limitations
* 20 A broader Impact
* 21 Experimental Results
* 22 Facilitations
* 23 Facilitations
* 24 Facilitations
* 25 A Computational Cost
* 26 S\({}^{2}\)GNN Aggregation Ablation
* 27 Number of Eigenvectors Ablation on Peptides-Func
* 28 Clustering Tasks
* 29 Distance Regression
* 21 Heterophilic arXiv-year (Lim et al., 2021)
* 22 Large-Scale PCQM4Mv2 (Hu et al., 2021)
* 23 TUPGraphs Graph Construction
* 24

## Appendix A Notation

## Appendix B Related Work for Expressivity and Directed Graphs

**Expressivity.** Laplacian eigenvectors have been used previously to construct positional encodings that improve the expressivity of GNNs or Transformers (Lim et al., 2023; Wang et al., 2022; Geisler et al., 2023; Huang et al., 2024). Our positional encodings are similar to the preprocessing of Balcilar et al. (2021), where the authors design an edge-level encoding/mask to surpass 1-WL. The hierarchy of Weisfeiler-Leman (WL) tests is a common way to categorize the expressivity of GNNs (Grohe

\begin{table}
\begin{tabular}{l l} \hline \hline \(b\) & Scalar \\ \(\bm{b}\) & (column) Vector \\ \(\bm{B}\) & Matrix \\ \(\mathbb{B}\) & Set \\ \(\bm{B}^{\top}\) & Transpose of matrix \(\bm{B}\) \\ \(\bm{B}^{\mathrm{H}}\) & Conjugate transpose of matrix \(\bm{B}\) \\ \(i,i^{2}=-1\) & Complex number \\ \(\odot\) & Element-wise multiplication \\ \(\circ\) & Function composition, i.e., \(f_{2}\circ f_{1}=f_{1}(f_{2}(\cdot))\) \\ \(\|\cdot\|_{F}\) & Frobenius norm \\ \(\mathcal{O}\) & “Big O” notation for asymptotic growth of function \\ \(\bm{P}\in\mathbb{P}\) & Permutation matrix \\ *\(\mathcal{G}\) & Graph convolution (see § 2) \\ \((\mathcal{G}_{p})_{p\in\mathbb{N}}\) & Graph sequence (cf. Theorem 3) \\ \(\mathcal{G}(\bm{A},\bm{X})\) & Graph \\ \(\bm{A}\in\mathbb{R}_{2}^{n\times n}\) & Adjacency matrix \\ \(\bm{X}\in\mathbb{R}_{2}^{n\times d}\) & Node features \\ \(n\) & Number of nodes \\ \(m\) & Number of edges \\ \(d\) & Number of attributes \\ \(\bm{L}_{q}\in\mathbb{C}^{n\times n}\) & Graph/combinatorial/random walk (Magnetic) Laplacian \\ \(q\in\mathbb{R}_{\geq 0}\) & Potential (see Eq. 7) \\ \(\bm{L}_{q=0}=\bm{L}\in\mathbb{R}^{n\times n}\) & Real-valued Laplacian (\(q=0\)) \\ \(\bm{\lambda},\bm{V}=\mathrm{EVD}(\bm{L})\) & Eigendecomposition s.t. \(\bm{L}=\bm{V}\operatorname{diag}(\bm{\lambda})\bm{L}^{\mathrm{H}}\) \\ \(\bm{\lambda},\bm{V}=\mathrm{EVD}(\bm{L},k)\) & Partial eigendecomposition containing \(\lambda_{1}\leq\lambda_{2}\leq\cdots\leq\lambda_{k}\quad(\leq\lambda_{k+1})\) \\ \(k\) & Number of eigenvectors \\ \(p\) & Polynomial order \\ \(p_{\text{cut}}\) & Receptive field size (number hops) of polynomial filter \\ \(p_{\text{max}}\) & Maximal diameter of graph \\ \(\lambda_{\text{cut}}\) & Eigenvalue threshold considered in spectral filter \\ \(\lambda_{\text{max}}\) & Maximal eigenvalue of graph \\ \(\ell\) & Number of layers \\ \(K_{r}(\hat{g},\lambda_{\text{cut}})\) & Bound on \(r\)-th derivative of spectral filter on interval \([\lambda_{\text{cut}},2]\) \\ Spectral\({}^{(l)}(\bm{H}^{(l-1)};\bm{V},\bm{\lambda})\) & Spectral(ly parametrized) filter \\ Spatial\({}^{(l)}(\bm{H}^{(l-1)};\bm{A})\) & Spatial message passing \\ \(g\) & Filter in graph convolution \\ \(g_{\bm{\gamma}_{p}}\) & Order-\(p\) polynomial filter in graph convolution, with coefficients \(\bm{\gamma}_{p}\) \\ \(\hat{g}\) & Filter in graph convolution, in spectral domain \\ \(\hat{g}(\bm{\lambda})\) & as function evaluated at eigenvalues \\ \(\hat{g}_{\theta}(\bm{\lambda})\) & as parametrized function evaluated at eigenvalues \\ \(\hat{g}_{\phi}^{(l)}(\bm{\lambda})\) & as parametrized function at layer \(l\), evaluated at eigenvalues \\ \(\mathrm{PE}(\bm{V},\bm{\lambda})\) & Positional encodings \\ \(f_{\theta}\) & Feature transformation in spatial domain \\ \(s_{\zeta}\) & Feature transformation in spectral domain \\ \hline \hline \end{tabular}
\end{table}
Table 5: List of most important symbols used in this work (with the most general domain).

et al., 2021). Xu et al. (2019) showed that most MPGNNs are bound by or as strong as the 1-WL test. Lim et al. (2023) point out that spectral GNNs suffer from similar limitations as MPGNNs w.r.t. their expressivity. Generally, the development of expressive GNNs is an active research direction, and we refer to Li and Leskovec (2022) for a broad overview.

**Directed graphs.** Rossi et al. (2023) also extend the WL test to directed graphs and propose an MPGNN for directed graphs. How to model direction in graphs is also still an open question and various approaches were proposed (Battaglia et al., 2018; Tong et al., 2020; Zhang et al., 2021; Rossi et al., 2023; Koke and Cremers, 2024). We utilize a Hermitian Laplacian for direction awareness, namely the Magnetic Laplacian, which was also used by Zhang et al. (2021) for an MPGNN and Geisler et al. (2023) for positional encodings.

## Appendix C Background for Directed Graphs

**Undirected vs. directed graphs.** For spatial filtering, it is straightforward to plausibly extend the message passing (e.g. Battaglia et al. (2018); Rossi et al. (2023)). However, the spectral motivation and spectral filter on directed graphs require more care. The eigendecomposition is guaranteed to exist for real symmetric matrices. Real symmetric matrices are always diagonalizable, and the eigenvectors will then span a complete orthogonal basis to represent all possible signals \(\bm{X}\in\mathbb{R}^{n\times d}\). Note that some non-symmetric square matrices are also diagonalizable and, thus, also have an eigendecomposition, albeit the eigenvectors may not be orthogonal. Thus, further consideration is required to generalize the graph Laplacian to general directed graphs.

**Magnetic Laplacian.** For the spectral filter on directed graphs, we build upon a direction-aware generalization, called Magnetic Laplacian (Forman, 1993; Shubin, 1994; De Verdiere, 2013; Furutani et al., 2020; Geisler et al., 2023)

\[\bm{L}_{q}=\bm{I}-(\bm{D}_{s}^{-\nicefrac{{1}}{{2}}}\bm{A}_{s}\bm{D}_{s}^{- \nicefrac{{1}}{{2}}})\odot\exp[i2\pi q(\bm{A}-\bm{A}^{\top})]\] (7)

where \(\bm{A}_{s}=\bm{A}\vee\bm{A}^{\top}\) is the symmetrized graph with diagonal degree matrix \(\bm{D}_{s}\). \(\odot\) denotes the element-wise product, \(\exp\) the element-wise exponential, \(i=\sqrt{-1}\) an imaginary number, and \(q\) the potential (hyperparameter). By construction \(\bm{L}_{q}\) is a Hermitian matrix \(\bm{L}_{q}=\bm{L}_{q}^{\mathrm{H}}\) where the conjugate transpose is equal to \(\bm{L}_{q}\) itself. Importantly, Hermitian matrices naturally generalize real symmetric matrices and have a well-defined eigendecomposition \(\bm{L}_{q}=\bm{V}\bm{\Lambda}\bm{V}^{\mathrm{H}}\) with real eigenvalues \(\bm{\Lambda}\) and unitary eigenvectors \(\bm{V}\bm{V}^{\mathrm{H}}=I\). For appropriate choices of the potential \(q\), the order of eigenvalues is well-behaved (Furutani et al., 2020). Recently Geisler et al. (2023) demonstrated the efficacy of these eigenvectors for positional encodings for transformers. Moreover, the Magnetic Laplacian was used for a spectrally designed spatial MPGNN (Zhang et al., 2021), extending Defferrard et al. (2017). Due to the real eigenvalues, one could, in principle, also apply a monomial basis (Chien et al., 2021), or different polynomial bases stemming from approximation theory (He et al., 2021; Wang and Zhang, 2022; He et al., 2022; Guo and Wei, 2023).

To see why Eq. 7 describes an injection for appropriate choices of \(q\), consider that the sparsity pattern of \(\bm{L}_{q}\) matches \(\bm{A}\) up to the main diagonal. If \(\bm{A}\) contains a self-loop the main diagonal will have a 0 instead of 1 entry at the self-loop location. \(\bm{A}-\bm{A}^{\top}\) can be directly inferred from the phase \(\exp[i2\pi q(\bm{A}-\bm{A}^{\top})]\), assuming that \(q<\nicefrac{{1}}{{(2\max_{u,v}A_{u,v})}}\). Thus, it is solely left to obtain \(\bm{A}_{s}\) from \(\bm{I}-\bm{D}_{s}^{-\nicefrac{{1}}{{2}}}\bm{A}_{s}\bm{D}_{s}^{-\nicefrac{{1} }{{2}}}\), which is trivial for a binary adjacency but more involved for real-valued weights. Determining if and when \(\bm{L}_{q}\) is injective for real-valued \(\bm{A}\) is left for future work.

**Properties of the eigendecomposition.** The eigendecomposition is not unique, and thus, one should consider the result of the eigensolver arbitrary in that regard. One ambiguity becomes apparent from the definition of an eigenvalue itself \(\bm{L}\bm{v}=\lambda\bm{v}\) since one can multiply both sides of the equation with a scalar \(c\in\mathbb{C}\setminus\{0\}\): \(\bm{L}(c\bm{v})=\lambda(c\bm{v})\). We already implicitly normalized the magnitude of the eigenvectors \(\bm{V}\) by choosing them to be orthogonal (\(\bm{V}\bm{V}^{\top}=\bm{I}\)) or unitary (\(\bm{V}\bm{V}^{\mathrm{H}}=\bm{I}\)). Thus, after this normalization, \(c\) only represents an arbitrary sign for real-valued eigenvectors or a rotation on the unit circle in the complex case. Another reason for ambiguity occurs in the case of repeated / multiple eigenvalues (e.g., \(\lambda_{u}=\lambda_{v}\) for \(u\neq v\)). In this case, the eigensolver may return an arbitrary set of orthogonal eigenvectors chosen from the corresponding eigenspace.

Limitations of Graph Transformers Using Absolute Positional Encodings

Here, we consider a vanilla graph transformer \(f(\bm{X})\) that solely becomes structure-aware due to the addition (or concatenation) of positional encodings: \(f(\bm{X}+\operatorname{PE}(\bm{A}))\). The main point we are going to demonstrate is that a vanilla transformer with such absolute positional encodings \(\operatorname{PE}(\bm{A})\in\mathbb{R}^{n\times d}\) will be limited in its expressivity if the positional encodings are permutation equivariant \(\bm{P}\operatorname{PE}(\bm{A})=\operatorname{PE}(\bm{P}\bm{A}\bm{P}^{\top})\) w.r.t. any \(n\times n\) permutation matrix \(\bm{P}\in\mathcal{P}\).

The limitation particularly arises in the presence of automorphisms \(\bm{P}_{a}\bm{A}\bm{P}_{a}^{\top}=\bm{A}\) with specifically chosen permutation \(\bm{P}_{a}\). To be more specific, assume that nodes \(u\) and \(v\) are automorphic to each other. That is, there exists a \(\bm{P}_{a}\) that will swap the order of \(u\) and \(v\) (among other nodes) \(\operatorname{s.t.}\)\(\bm{P}_{a}\bm{A}\bm{P}_{a}^{\top}=\bm{A}\). By permutation equivariance, we know \(\bm{P}_{a}\operatorname{PE}(\bm{A})=\operatorname{PE}(\bm{P}_{a}\bm{A}\bm{P}_ {a}^{\top})=\operatorname{PE}(\bm{A})\) and, hence, \(\operatorname{PE}(\bm{A})_{u}=\operatorname{PE}(\bm{A})_{v}\).

We have just shown that automorphic nodes will have the same positional encodings \(\operatorname{PE}\) if the positional encodings are permutation equivariant. This implies that permutation equivariant positional encodings \(\operatorname{PE}\) are not even able to capture simple neighboring relationships. For example, consider an undirected sequence/path graph o-o-o-o with five nodes. Here, the two end nodes, which we also all first and last node, are automorphic. So are the second and second-last nodes. Assuming the second and second last nodes have different node features (e.g., A-B-C-D-A), that breaks the symmetry, it is still not possible for a transformer with absolute positional encodings to tell the first and last node apart. In other words, in the example, the transformer cannot tell apart the end node with neighboring feature B from the end node with neighboring feature D. This shows a severe limitation of architectures without additional components capturing the relative distances (e.g., as S\({}^{2}\)GNNs can). This concern is not as critical for architectures where the positional encodings are not entirely permutation equivariant (Dwivedi and Bresson, 2021; Kreuzer et al., 2021), with relative positional encodings (Ma et al., 2023), and might also be of lesser concern for directed graphs (Geisler et al., 2023).

## Appendix E S\({}^{2}\)GNN Generalizes a Virtual Node

Adding a fully connected virtual node (Gilmer et al., 2017) is among the simplest ways to add the ability for long-range information exchange. An equivalent method was proposed as a simple over-squashing remedy in the seminal work by Alon and Yahav (2020). A single \(\operatorname{Spectral}\) layer amounts to a type of virtual nodes in the special case of \(f_{\theta}=\bm{I}\) and

\[\hat{g}^{(l)}(\lambda)=\begin{cases}1\text{ for }\lambda=0,\\ 0\text{ for }\lambda>0,\end{cases}\] (8)

Assuming a simply-connected graph \(\mathcal{G}\), the unique normalized zero-eigenvector \(\bm{v}\) of the symmetrically-normalized graph Laplacian \(\bm{L}=\bm{I}-\bm{D}^{-\nicefrac{{1}}{{2}}}\bm{A}\bm{D}^{-\nicefrac{{1}}{{2}}}\) has components \(\bm{v}_{u}=\sqrt{\frac{d_{u}}{2|E|}}\), where \(d_{u}\) denotes the degree of node \(u\in\mathcal{G}\), and \(|E|\) the number of edges in the graph. At node \(u\in\mathcal{G}\), we therefore find

\[\operatorname{Spectral}_{u}^{(l)}(\bm{H}^{(l-1)};\bm{V},\bm{\lambda})=\frac{ \sqrt{d_{u}}}{2|E|}\sum_{v\in\mathcal{G}}\sqrt{d_{v}}\bm{h}_{v}^{(l-1)}\] (9)

with \(\bm{h}_{v}^{(l-1)}\) denoting the row of \(\bm{H}^{(l-1)}\) corresponding to node \(v\in\mathcal{G}\). In other words, filtering out the zero-frequency component of the signal means scattering a global, degree-weighted embedding average to all nodes of the graph. For the unnormalized graph Laplacian, Eq. 9 instead becomes an unweighted average, which is consistent with the usual definition of a virtual node. We refer to Fig. 3 & 4 for additional intuition.

## Appendix F Existing Results on Over-Squashing

We restate two key results from Di Giovanni et al. (2023) using our notation. They imply the existance of a regime in which 1-hop MPNN architectures suffer from exponentially decaying Jacobian sensitivity. Meanwhile, S\({}^{2}\)GNNs can easily learn a signal of constantly lower-bounded sensitivity, as shown by invoking its trivial subcase of a virtual node in Theorem 2.

**Theorem 7** (Adapted from Di Giovanni et al. (2023a)).: _In an \(\ell\)-layer spatial MPGNN with message-passing matrix \(\bm{S}=c_{r}\bm{I}+c_{a}\bm{A}\) (\(c_{r},\ c_{a}\in\mathbb{R}^{+}\)) and a Lipschitz nonlinearity \(\sigma\),_

\[\bm{H}^{(l)}=\mathrm{Spatial}^{(l)}(\bm{H}^{(l-1)};\bm{A})=\sigma\left(\bm{SH }^{(l-1)}\bm{W}^{(l-1)}\right),\ 1\leq l\leq\ell\] (10)

_the Jacobian sensitivity satisfies the following upper bound:_

\[\left\|\frac{\partial\mathbf{h}_{v}^{(\ell)}}{\partial\mathbf{h}_{u}^{(0)}} \right\|_{L^{1}}\leq\left(c_{\sigma}wd\right)^{\ell}\left(\bm{S}^{\ell}\right) _{vu},\] (11)

_with \(\bm{h}_{u}^{(0)}\), \(\bm{h}_{v}^{(\ell)}\) denoting the rows of \(\bm{H}^{(0)}\), \(\bm{H}^{(\ell)}\) corresponding to the nodes \(v\), \(u\in\mathcal{G}\), \(c_{\sigma}\) the Lipschitz constant of the nonlinearity, \(w\) the maximum entry value over all weight matrices \(\bm{W}^{(l)}\), and \(d\) the network width._

The dependence of the upper bound on the matrix power \(\left(\bm{S}^{\ell}\right)_{vu}\) - not generally present for S\({}^{2}\)GNN by Theorem 2 - leads to a topology-dependence which becomes explicit in the following theorem. It concerns the typical shallow-diameter regime, in which the number \(\ell\) of MPGNN layers is comparable to the graph diameter.

**Theorem 8** (Adapted from Di Giovanni et al. (2023a)).: _Given an MPNN as in Eq. 10, with \(c_{\mathrm{a}}\leq 1\), let \(v,u\in\mathcal{G}\) be at distance \(r\). Let \(c_{\sigma}\) be the Lipschitz constant of \(\sigma,w\) the maximal entry-value overall weight matrices, \(d_{\min}\) the minimal degree of \(\mathcal{G}\), and \(\gamma_{\ell}(v,u)\) the number of walks from \(v\) to \(u\) of maximal length \(\ell\). For any \(0\leq k<r\), there exists \(C_{k}>0\)independent of \(r\) and of the graph, such that_

\[\left\|\frac{\partial\mathbf{h}_{v}^{(r+k)}}{\partial\mathbf{h}_{u}^{(0)}} \right\|_{L^{1}}\leq C_{k}\gamma_{r+k}(v,u)\left(\frac{2c_{\sigma}wd}{d_{ \min}}\right)^{r}.\]

For 1-hop MPGNNs with \(2c_{\sigma}wd<d_{\min}\), we therefore identify an exponential decay of sensitivity with node distance \(r\) in the weak-connectivity limit for which \(\gamma_{r+k}(v,u)\) increases sub-exponentially with \(r\). As Di Giovanni et al. (2023a) point out, sharper bounds can be derived under graph-specific information about \(\left(\bm{S}^{r}\right)_{vu}\).

## Appendix G Construction of an explicit ground truth filter

We express the electric potential along a periodic sequence of screened 1D charges as a convolution of a corresponding graph signal with a consistently defined graph filter. This closed-form example underscores our default use of a low-pass window for the spectral part of S\({}^{2}\)GNNs by showing how a continuous problem with a convolutional structure and quickly flattening spectral response (typical for pair interactions in physics and chemistry) discretizes into a graph problem with similar features.

The approach exploits the surjective mapping of Fourier modes on \([0,n]\) onto the Laplacian eigenvectors of a cycle graph \(C_{n}\). We consider two corresponding representations of the same problem:

\[\rho(x)=\sum_{l=0}^{n-1}q_{l}\Delta_{n}\left(x-l\right),\ \Delta_{m}(x)=\sum_{m\in\mathbb{Z}}\delta(x-mn),\ V(x)=(\phi_{\sigma}*_{ \mathbb{R}}\rho)(x),\] (12) \[\phi_{\sigma}(x)=\left(x\text{erf}\left(\frac{x}{\sqrt{2}\sigma} \right)+\sigma\sqrt{\frac{2}{\pi}}\exp\left(-\frac{x^{2}}{2\sigma^{2}}\right) \right)-|x|,\quad\sigma>0\]

\[\left[V(l)=(\phi_{\sigma}*_{\mathbb{R}}\rho)(l)\overset{!}{=}(g_{\sigma}*_{ \mathcal{G}}q)_{l},\,0\leq k\leq n-1,\,\forall\bm{q}\in\mathbb{R}^{n}\right] \qquad\right]\] (13)

\[\mathcal{G}=\mathcal{C}_{n},\quad\bm{q}=(q_{1},\ldots,q_{n})^{\top}\] (14)

* A continuous representation (Eq. 12) in terms of a 1D distribution \(\rho\) of \(n\) point charges \(q_{1},\ldots,q_{n}\) and their periodically repeating image charges, written as a sum of Dirac combs at equidistant offsets \(l\) with \(0\leq l\leq n-1\), interacting via the potential profile \(\phi_{\sigma}\) obtained from solving in Gauss' law of electrostatics for a 1D point charge screened by a Gaussian cloud of opposite background charge with standard deviation \(\sigma\). The screening ensuresconvergence to a finite potential and its exact form is insignificant (we choose the Gaussian-type screening due to its analytical tractability). Note that \(\phi_{\sigma}(x)\simeq\text{const.}-|x|\) for \(x\to 0\) (the unscreened 1D potential in the direction normal to an infinitely wide capacitor plate), while the screening guarantees an exponential dropoff to zero as \(x\to\infty\),
* A graph representation (Eq. 14) by placing the \(n\) charges \(q_{1},\ldots,q_{n}\) onto a cycle graph \(\mathcal{C}_{n}\).

We derive the graph filter \(g_{\sigma}\) from a consistency condition (Eq. 13) between both representations: the graph convolution \((g_{\sigma}*_{\mathcal{G}}\boldsymbol{q})\) has to yield the electric potential \(V\) sampled at the charge loci if we want \(g_{\sigma}\) to act like the continuous convolution kernel \(\phi_{\sigma}\) in the discrete graph picture.

The Fourier transform of \(\phi_{\sigma}\) (in the convention without integral prefactor and with a \(2\pi\) frequency prefactor) reads \(\hat{\phi}_{\sigma}(\kappa)=\frac{1}{\pi\kappa^{2}}\left(1-\exp\left(-\frac{1} {2}\sigma^{2}\kappa^{2}\right)\right)\). For the density, the Poisson sum formula gives \(\hat{\rho}(\kappa)=\sum_{k=0}^{n-1}\frac{1}{\sqrt{n}}\hat{q}_{k}\Delta_{1}( \kappa-\frac{k}{n})\) with \(\hat{q}_{k}=\frac{1}{\sqrt{n}}\sum_{j=0}^{n-1}q_{i}\exp\left(-i2\pi\frac{k}{ n}j\right)\). The coefficients \(\hat{q}_{k}\) are precisely the components of the graph Fourier transform of \(\boldsymbol{q}\) (physically, they amount for the structure factor). By the convolution theorem, \(\hat{V}(\kappa)=\hat{\phi}_{\sigma}(\kappa)\hat{\rho}(\kappa)\). By noting that all integer-shifted frequencies in the Dirac combs \(\Delta_{1}\left(\,\cdot-\frac{k}{n}\right)\) (or all Brillouin zones, in physics terminology) yield the same phase \(\exp\left(i2\pi\frac{k}{n}l\right)\) if we only sample \(V(x)\) at the charge loci \(x=l\), \(0\leq l\leq n-1\), we can write \(V(l)=\frac{1}{2\pi}\sum_{k=0}^{n-1}\hat{\boldsymbol{q}}_{k}\left(\sum_{m\in \mathbb{Z}}\hat{\phi}_{\sigma}\left(\frac{k}{n}+m\right)\right)\frac{1}{\sqrt{ n}}\exp\left(i2\pi\frac{k}{n}l\right)\). Through pattern-matching with the consistency condition of Eq. 13, we can therefore identify that the graph filter is a sum over Brillouin zones, \((\hat{g}_{\sigma})(\lambda_{k})=\frac{1}{2\pi}\sum_{m\in\mathbb{Z}}\hat{\phi} _{\sigma}\left(\frac{k}{n}+m\right),\) where \(\lambda_{k}\) denotes the eigenvalues of the normalized \(\mathcal{C}_{n}\) graph Laplacian, \(\lambda_{k}=1-\cos\left(\frac{2\pi k}{n}\right)\). To fulfill this relation for all \(n\), \(k\) we set

\[\hat{g}_{\sigma}(\lambda)=\frac{1}{2\pi}\sum_{m\in\mathbb{Z}}\hat{\phi}_{ \sigma}\left(\frac{1}{2\pi}\arccos(1-\lambda)+m\right)\]

We claim now (and prove in a later paragraph) that for \(\lambda>\lambda_{0}>0\) and a sufficiently large choice \(\sigma>\sigma(r,\lambda_{0})\), the absolute \(r\)-th derivative satisfies the upper bound \(|\frac{d^{r}}{d\lambda^{r}}\hat{g}_{\sigma}(\lambda)|\leq|\frac{d^{r}}{d \lambda^{r}}\hat{g}_{\infty}(\lambda)|\), where we can think of \(\hat{g}_{\infty}\) as the limit of taking \(\sigma\to\infty\) (i.e., a constant background charge):

\[\hat{g}_{\infty}(\lambda)=\frac{1}{2\pi}\sum_{m\in\mathbb{Z}}\hat{\phi}_{ \infty}\left(\frac{1}{2\pi}\arccos(1-\lambda)+m\right),\quad\hat{\phi}_{ \infty}(\kappa)=\frac{1}{\pi\kappa^{2}}\]

The merit of this is that unlike the screened \(\hat{g}_{\sigma}(\lambda)\), \(\hat{g}_{\infty}(\lambda)\) can be solved analytically to find closed-form bounds on the absolute derivatives \(|\frac{d^{r}}{d\lambda^{r}}\hat{g}_{\sigma}(\lambda_{0})|\). By invoking the sum expansion form of the trigamma function \(\Psi_{1}(z)=\sum_{m=0}^{\infty}\frac{1}{(z+m)^{2}}\), the reflection identity \(\psi_{1}(1-z)+\psi_{1}(z)=\frac{\pi^{2}}{\sin^{2}\pi z}\), and the half-angle formula \(\sin^{2}\left(\frac{x}{2}\right)=\frac{1-\cos(x)}{2}\), we find

\[\hat{g}_{\infty}(\lambda) =\frac{1}{2\pi^{2}}\left(\Psi_{1}\left(\frac{1}{2\pi}\arccos(1- \lambda)\right)+\Psi_{1}\left(1-\frac{1}{2\pi}\arccos(1-\lambda)\right)\right)\] \[=\frac{1}{2\sin^{2}\left(\frac{1}{2}\arccos(1-\lambda)\right)}= \frac{1}{\lambda},\]

a remarkably simple result. We can now readily evaluate \(|\frac{d^{r}}{d\lambda^{r}}\hat{g}_{\infty}(\lambda)|=\frac{r^{1}}{\lambda^{r+1}}\), but it remains to prove that this upper-bounds \(|\frac{d^{r}}{d\lambda^{r}}\hat{g}_{\sigma}(\lambda)|\) for any \(\lambda>\lambda_{0}>0\) and sufficiently large \(\sigma>\sigma(r,\lambda_{0})\). For compactness, define the expressions \(z(\lambda):=\frac{1}{2\pi}\arccos(1-\lambda)\in\left[0,\frac{1}{2}\right]\) (strictly increasing in \(\lambda\), \(y_{\sigma}(z):=\exp\left(-\frac{1}{2}\sigma^{2}z^{2}\right)\), and \(\tilde{z}=1-z\geq z\). Consider the series of "term-by-term" derivatives

\[\frac{d}{dz}\hat{g}_{\sigma}(\lambda(z)) =-\frac{1}{\pi^{2}}\sum_{m=0}^{\infty}\left(\frac{1}{(z+n)^{3}}(1- y_{\sigma}(z+m))-\frac{1}{(\tilde{z}+n)^{3}}(1-y_{\sigma}(\tilde{z}+m))\right)\] \[+\sum_{m=0}^{\infty}\mathcal{O}\left(y_{\sigma}(m)\right)\] \[\frac{d^{2}}{dz^{2}}\hat{g}_{\sigma}(\lambda(z)) =\frac{3}{\pi^{2}}\sum_{m=0}^{\infty}\left(\frac{1}{(z+n)^{4}}(1- y_{\sigma}(z+m))+\frac{1}{(\tilde{z}+n)^{4}}(1-y_{\sigma}(\tilde{z}+m))\right)\] \[+\sum_{m=0}^{\infty}\mathcal{O}\left(y_{\sigma}(m)\right)\] \[\vdots\]

They converge uniformly on \(\left[0,\frac{1}{2}\right]\) as they clearly are Cauchy sequences under uniform bound (moreover, well-definedness in \(z=0\) follows by applying l'Hospital's rule - physically, this is the merit provided by including Gaussian screening in our model). Therefore, they indeed converge to the respective derivatives \(\frac{d^{r}}{dz^{r}}\hat{g}_{\sigma}(\lambda(z))\) (justifying the above notation). The same holds for the corresponding series for \(\frac{d^{r}}{dz^{r}}\hat{g}_{\infty}(\lambda(z))\): they are not defined in \(z=0\), but otherwise still converge as they match the known series expansion of the polygamma function. Given \(\lambda_{0}>0\) and thus \(z(\lambda_{0})>0\), taking \(\sigma\) larger than some \(\sigma(r,\lambda_{0})\) guarantees that \(\frac{d^{r}}{dz^{r}}\hat{g}_{\sigma}(\lambda(z))\) and \(\frac{d^{r}}{dz^{r}}\hat{g}_{\infty}(\lambda(z))\) are of the same sign for \(\lambda>\lambda_{0}\) (\(z(\lambda)>z(\lambda_{0})\)). This holds for all orders \(r\in\mathbb{N}\) since we see by induction that the product rule always yields one term analogous to the first respective terms above, and otherwise only terms of \(\mathcal{O}\left(y_{\sigma}(m)\right))\). Then, observing that \(0\leq 1-y_{\sigma}(x)<1\;\forall\;x\geq 0\) and \(\tilde{z}\geq z\) implies \(|\frac{d^{r}}{dz^{r}}\hat{g}_{\sigma}(\lambda_{0}(z_{0}))|\leq|\frac{d^{r}}{dz ^{r}}\hat{g}_{\infty}(\lambda_{0}(z_{0}))|\). The same must hold for the \(\lambda\)-derivatives by the chain rule.

One interesting question is whether \(\hat{g}_{\sigma}\) is also \(C\)-integral-Lipschitz for some constant \(C>0\). We discuss this stability-informed criterion (Gama et al., 2020) in the main body as a domain-agnostic prior assumption about the "ideal" graph filter if no other ground truth knowledge informing additional smoothness bounds (such as here) is available. While the above bound is too loose to certify this directly (\(|\frac{d}{d\lambda}\hat{g}_{\sigma}(\lambda)|\leq C\lambda^{-1}\) would be needed), integral-Lipschitzness under some constant follows from the fact that \(|\frac{d}{d\lambda}\hat{g}_{\sigma}(\lambda)|\) is bounded on \([0,2]\): by the uniform convergence of the term-by-term derivative series, it is continuous. Well-definedness of the product \(\frac{d}{dz}\hat{g}_{\sigma}\frac{dz}{d\lambda}\) has to be checked in \(\lambda=0\), where it follows by continuous extension using l'Hospital's rule. As a continuous function defined on a compact interval, \(|\frac{d}{d\lambda}\hat{g}_{\sigma}|\) assumes a maximum.

## Appendix H Proofs

### Proof of Theorem 1

We next prove the permutation equivariance of the spectral filter in Eq. 3:

**Theorem 1**.: \(\mathrm{Spectral}(\bm{H}^{(l-1)};\mathrm{EVD}(\bm{L},k))\) _of Eq. 3 is equivariant to all \(n\times n\) permutation matrices \(\bm{P}\in\mathcal{P}\): \(\mathrm{Spectral}(\bm{P}\bm{H}^{(l-1)};\mathrm{EVD}(\bm{P}\bm{L}\bm{P}^{\top},k ))=\bm{P}\,\mathrm{Spectral}(\bm{H}^{(l-1)};\mathrm{EVD}(\bm{L},k))\)._

for the general case of parametrizing a Hermitian "Laplacian" \(\bm{L}\in\mathbb{C}^{n\times n},\bm{L}^{\mathrm{H}}=\bm{L}\). Note that this proof does not rely in any means on the specifics of \(\bm{L}\), solely that the eigendecomposition exists \(\bm{L}=\bm{V}\bm{\Lambda}\bm{V}^{\mathrm{H}}\) with unitary eigenvectors \(\bm{V}\bm{V}^{\mathrm{H}}=\bm{I}\). For practical reasons, it is suitable to define \(\bm{L}(\bm{A})\) as a function of \(\bm{A}\). A similar proof for real-valued eigenvectors is given by (Lim et al., 2023). The specific spectral filter we consider is

\[\mathrm{Spectral}(\bm{H}^{(l-1)};\bm{V},\bm{\lambda})=h\left[\bm{V}\Big{(} \hat{g}(\bm{\lambda})\odot\big{[}\bm{V}^{\mathrm{H}}f(\bm{H}^{(l-1)})\big{]} \Big{)}\right]\] (15)

with arbitrary \(f:\mathbb{C}^{d_{1}}\to\mathbb{C}^{d_{2}}\), applied row-wise to \(\bm{H}^{(l-1)}\in\mathbb{C}^{n\times d_{1}}\). Analogously, \(h:\mathbb{C}^{d_{2}}\to\mathbb{C}^{d_{3}}\) is applied row-wise. We choose complex functions to emphasize generality, although we restrict \(\mathrm{Spectral}\) to real in- and outputs in all experiments. The graph filter is defined as element-wise function \(\hat{g}_{u,v}(\bm{\lambda})\coloneqq\hat{g}_{v}(\lambda_{u},\{\lambda_{1}, \lambda_{2},\ldots,\lambda_{k}\})\) that depends on the specific eigenvalue \(\lambda\) and potentially the set of eigenvalues \(\{\lambda_{1},\lambda_{2},\ldots,\lambda_{k}\}\) (or its vector representation \(\bm{\lambda}\)) of the partial eigendecomposition.

We need to make sure that the partial decomposition includes all eigenvalues of the same magnitude, i.e., \(\lambda_{u}\neq\lambda_{u^{\prime}},\forall u\in\{1,2,\ldots,k\},u^{\prime} \in\{k+1,k+2,\ldots,n\}\). In practice, this is achieved by choosing large enough \(k\) to accommodate all eigenvalues \(\lambda_{\text{cut}}<\lambda_{k+1}\), or by dropping trailing eigenvalues where \(\lambda_{j}=\lambda_{k+1}\) for \(j\in\{1,2,\ldots,k\}\). Generally, it is also not important that we consider the \(k\)_smallest_ eigenvalues in the spectral filter. We only need to ensure that the spectral filter is either calculated on all or no eigenvalues/-vectors of an eigenspace.

Proof.: Assuming functions \(\phi(\bm{X})\) and \(\psi(\bm{X})\) are permutation equivariant, then \(\phi(\psi(\bm{X}))\) is permutation equivariant \(\phi(\psi(\bm{P}\bm{X}))=\phi(\bm{P}\psi(\bm{X}))=\bm{P}\phi(\psi(\bm{X}))\) for any \(n\times n\) permutation \(\bm{P}\in\mathcal{P}\). Thus, it sufficies to prove permutation equivariance for \(h,f,\bm{V}(\hat{g}(\bm{\lambda})\odot[\bm{V}^{\mathrm{H}}\bm{X}])\) independently, where \(\bm{X}\in\mathbb{C}^{n\times d_{2}}\).

Regardless of the complex image and domain of \(h\) and \(f\), they are permutation equivariant since they are applied row-wise

\[f(\bm{X})=[f(\bm{X}_{1})\quad f(\bm{X}_{2})\quad\ldots\quad f(\bm{X}_{n})]^{ \mathrm{H}}\]

and reordering the rows in \(\bm{X}\in\mathbb{C}^{n\times d_{1}}\) also reorders the outputs: \(f(\bm{P}\bm{X})=\bm{P}f(\bm{X})\).

For finalizing the proof of permutation equivariance, we first rearrange \(\bm{Y}=\bm{V}(\hat{g}(\bm{\lambda})\odot[\bm{V}^{\mathrm{H}}\bm{X}])=\sum_{u =1}^{k}\bm{v}_{u}(\hat{g}_{u,:}(\lambda_{u})\odot[\bm{v}_{u}^{\mathrm{H}}\bm{ X}])\) and \(\bm{Y}_{:,v}=\sum_{u=1}^{k}\hat{g}_{u,v}(\lambda_{u})\bm{v}_{u}\bm{v}_{u}^{ \mathrm{H}}\bm{X}_{:,v}\).

This construction (a) is invariant to the ambiguity that every eigenvector \(\bm{v}_{u}\) can be arbitrarily rotated \(c_{u}\bm{v}_{u}\) by \(\{c_{u}\in\mathbb{C}\,|\,|c_{u}|=1\}\). That is, \((c_{u}\bm{v}_{u})(c_{u}\bm{v}_{u})^{\mathrm{H}}=c_{u}\bar{c}_{u}\bm{v}_{u}\bm{ v}_{u}^{\mathrm{H}}=\bm{v}_{u}\bm{v}_{u}^{\mathrm{H}}\).

Moreover, (b) in the case of \(j\) repeated eigenvalues \(\{s+1,s+2,\ldots,s+j\}\) where \(\lambda_{s+1}=\lambda_{s+2}=\cdots=\lambda_{s+j}\), we can choose a set of orthogonal eigenvectors arbitrarily rotated/reflected from the \(j\)-dimensional eigenspace (basis symmetry). The given set of eigenvectors can be arbitrarily transformed \(\bm{V}_{:,s+1:s+j}\bm{\Gamma}_{j}\) by a matrix chosen from the unitary group \(\bm{\Gamma}_{j}\in U(j)\). Since

\[\sum_{u=s}^{s+j}\hat{g}_{u,v}(\lambda_{u})\bm{v}_{u}\bm{v}_{u}^{\mathrm{H}}\bm {X}_{:,v}=\hat{g}_{s,v}(\lambda_{s})\left[\sum_{u=s}^{s+j}\bm{v}_{u}\bm{v}_{u}^ {\mathrm{H}}\right]\bm{X}_{:,v}=\hat{g}_{s,v}(\lambda_{s})\left[\bm{V}_{:,s+1:s +j}\bm{V}_{:,s+1:s+j}^{\mathrm{H}}\right]\bm{X}_{:,v}\]

we simply need to show that the expression is invariant to a transformation by \(\bm{\Gamma}_{j}\):

\[\bm{V}_{:,s+1:s+j}\bm{\Gamma}_{j}(\bm{V}_{:,s+1:s+j}\bm{\Gamma}_{j})^{\mathrm{H }}=\bm{V}_{:,s+1:s+j}\bm{\Gamma}_{j}\bm{\Gamma}_{j}^{\mathrm{H}}\bm{V}_{:,s+1:s +j}^{\mathrm{H}}=\bm{V}_{:,s+1:s+j}\bm{V}_{:,s+1:s+j}^{\mathrm{H}}\]

To see why \(\bm{\Gamma}_{j}\in U(j)\) is a sufficient choice in the light of repeated/multiple eigenvalues, consider the defintion of eigenvalues/vectors

\[\bm{L}\bm{V}_{:,s+1:s+j} =\bm{L}\begin{bmatrix}|&|&&|\\ \bm{v}_{s+1}&\bm{v}_{s+2}&\ldots&\bm{v}_{s+j}\\ |&&|&&|\end{bmatrix}\] \[=\begin{bmatrix}|&|&&|\\ \bm{v}_{s+1}&\bm{v}_{s+2}&\ldots&\bm{v}_{s+j}\\ |&|&&|\end{bmatrix}\begin{bmatrix}\lambda_{s+1}&0&\ldots&0\\ 0&\lambda_{s+2}&\ldots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\ldots&\lambda_{s+j}\end{bmatrix}\] \[=\lambda_{s+1}\begin{bmatrix}|&|&&|\\ \bm{v}_{s+1}&\bm{v}_{s+2}&\ldots&\bm{v}_{s+j}\\ |&|&&|\end{bmatrix}\] \[=\lambda_{s+1}\bm{V}_{:,s+1:s+j}\]

we can now multiply both sides from the right with an arbitrary matrix \(\bm{B}\in\mathbb{C}^{j\times j}\). To preserve the unitary property \(\bm{V}_{:,s+1:s+j}\bm{V}_{:,s+1:s+j}^{\mathrm{H}}=\bm{I}\), we require \((\bm{V}_{:,s+1:s+j}\bm{B})(\bm{V}_{:,s+1:s+j}\bm{B})^{\mathrm{H}}=\bm{I}\). Thus, the eigenvectors can be arbitrarily transformed by \(\bm{\Gamma}_{j}\in U(j)\) instead of \(\bm{B}\in\mathbb{C}^{j\times j}\).

This concludes the proof.

### Proof of Theorem 2

We create Theorem 2 in more detail and also considering graphs that contain multiple connected components. The unchanged bottom line is that S\({}^{2}\)GNNs can express signals lower-bounded by a constant that is unaffected by local properties of the graph topology, instead of suffering from exponential sensitivity decay like spatial MPGNNs.

**Theorem** (Theorem 2, formal).: _Consider an \(\ell\)-layer S\({}^{2}\)GNN of the form Eq. 1. Let \((\tilde{\vartheta},\vartheta,\theta)\) be parameters of the spatial GNN, spectral filters \(\hat{g}_{\vartheta}^{(l)}\), and feature transformation \(f_{\theta}\). Assume the existence of parameters \(\tilde{\vartheta}\) such that \(\mathrm{Spatial}^{(l)}(\bm{H}^{(l-1)};\bm{A},\tilde{\vartheta})=0\ \forall 1\leq l\leq\ell\) and \(\theta\) such that \(f_{\theta}=\bm{I}\). Then, a filter choice \(\vartheta\) exists such that the \(\ell\)-layer S\({}^{2}\)GNN of the additive form Eq. 1 can express a signal \(\mathbf{h}_{v}^{(\ell)}(\bm{H}^{(0)};\tilde{\vartheta},\vartheta,\theta)\) with uniformly lower-bounded Jacobian sensitivity,_

\[\left\|\frac{\partial\mathbf{h}_{v}^{(\ell)}(\bm{H}^{(0)};\tilde{\vartheta}, \vartheta,\theta)}{\partial\mathbf{h}_{u}^{(0)}}\right\|_{L^{1}}\geq\begin{cases} \frac{dK_{\vartheta}^{\ell}}{2|E_{\mathcal{C}}|}\text{ if }u,v\text{ connected,}\\ 0\text{ otherwise,}\end{cases}\] (16)

_with \(\bm{h}_{u}^{(0)}\), \(\bm{h}_{u}^{(\ell)}\) denoting the rows of \(\bm{H}^{(0)}\), \(\bm{H}^{(\ell)}\) corresponding to the nodes \(u\neq v\in\mathcal{G}\), connected component \(\mathcal{C}\subset\mathcal{G}\) containing \(|E_{\mathcal{C}}|\) edges, network width \(d\) and parameter-dependent constant \(K_{\vartheta}\)._

Proof.: Choose \(\tilde{\vartheta}\) such that \(\mathrm{Spatial}^{(l)}(\bm{H}^{(l-1)};\bm{A})=0\ \forall 1\leq l\leq\ell\) (typically by setting all weights and biases to zero), \(\theta\) such that \(f_{\theta}=\bm{I}\), and set \(\vartheta\) such that

\[\hat{g}_{k}^{(l)}(\lambda;\vartheta)=K_{\vartheta}\begin{cases}1\text{ for }\lambda=0,\\ 0\text{ for }\lambda>0,\end{cases}\qquad\forall 1\leq l\leq\ell,\,1\leq k\leq d\] (17)

for some \(K_{\vartheta}>0\). This choice of filter parameters \(\vartheta\) lets \(\mathrm{Spectral}\) act like a type of virtual node across all hidden dimensions \(k\): In the standard orthonormal basis of the \(0\)-eigenspace given by

\[\left(\bm{v}^{(\mathcal{C})}\right)_{u}=\sqrt{\frac{d_{u}}{2|E_{\mathcal{C}}| }}\begin{cases}1\text{ for }u\in\mathcal{C},\\ 0\text{ else,}\end{cases}\] (18)

where \(\mathcal{C}\) enumerates all connected components, and \(d_{u}\) denotes the degree of node \(u\), we find

\[\begin{split}\mathbf{h}_{v}^{(\ell)}(\bm{H}^{(0)};\tilde{\vartheta },\vartheta,\theta)&=\left(\mathrm{Spectral}^{(\ell)}\circ\cdots\circ \mathrm{Spectral}^{(0)}\right)_{v}(\bm{H}^{(0)};\bm{V},\bm{\lambda})\\ &=\frac{K_{\vartheta}^{\ell}\sqrt{d_{v}}}{2|E_{\mathcal{C}^{(v)}}|} \sum_{u\in\mathcal{C}^{(v)}}\sqrt{d_{u}}\mathbf{h}_{u}^{(0)},\end{split}\] (19)

with \(\mathcal{C}^{(v)}\) denoting the connected component containing \(v\). Particularly, note that applying the spectral layer more than once does not affect the result since the projector onto an eigenvector is idempotent (up to \(K_{\vartheta}\)). The result must also hold in any other orthonormal basis of the \(0\)-eigenspace due to the invariance of \(\mathrm{Spectral}\) under orthogonal eigenbasis transformations. Differentiating with respect to \(\mathbf{h}_{u}^{(0)}\), taking the \(L^{1}\) norm and using \(\sqrt{d_{u}d_{v}}\geq 1\) shows the statement. 

### Proof of Theorem 3

**Theorem 3**.: _Let \(\hat{g}\) be a discontinuous spectral filter. For any approximating sequence \(\left(g_{\mathcal{T}_{p}}\right)_{p\in\mathbb{N}}\) of polynomial filters, an adversarial sequence \(\left(\mathcal{G}_{p}\right)_{p\in\mathbb{N}}\) of input graphs exists such that_

\[\nexists\alpha\in\mathbb{R}_{>0}\colon\sup_{0\neq\bm{X}\in\mathbb{R}^{| \mathcal{G}_{p}|\times d}}\frac{\|(g_{\mathcal{T}_{p}}-g)\ast_{\mathcal{G}_{p }}\bm{X}\|_{\mathrm{F}}}{\|\bm{X}\|_{\mathrm{F}}}=\mathcal{O}\left(p^{-\alpha}\right)\]

The proof makes use of a result by S. Bernstein (Natanson, 1964):

**Theorem 9** (Bernstein).: _Let \(f\colon[0,2\pi]\to\mathbb{C}\) be a \(2\pi\)-periodic function. Then \(f\) is \(\alpha\)-Holder continuous for some \(\alpha\in(0,1)\) if, for every \(p\in\mathbb{N}\), there exists a degree-\(p\) trigonometric polynomial \(T_{p}(x)=a_{0}+\sum_{j=1}^{p}a_{j}\cos(jx)+\sum_{j=1}^{p}b_{j}\sin(jx)\) with coefficients \(a_{j},b_{j}\in\mathbb{C}\), such that_

\[\sup_{0\leq x\leq 2\pi}|f(x)-T_{p}(x)|\leq\frac{C(f)}{p^{\alpha}}\]

_where C(f) is a positive number depending on \(f\)._Proof.: Given a discontinuous filter \(\hat{g}\colon[0,2]\to\mathbb{R}\), construct the function \(f\colon[0,2\pi]\to\mathbb{C}\) fulfilling the prerequisites of Theorem 9 by pre-composing \(f:=\hat{g}\circ(\cos(\cdot)+1)\). We proceed via contradiction. Suppose that there is an \(\alpha\in(0,1)\) and a sequence of degree-\(p\) polynomial filters, \(\hat{g}_{\boldsymbol{\gamma}_{p}}(\lambda)=\sum_{j=0}^{p}\gamma_{j}\lambda^{j}\), \(\boldsymbol{\gamma}=(\gamma_{0},\ldots,\gamma_{p})^{\top}\in\mathbb{R}^{p+1}\), such that \(\|\hat{g}_{\boldsymbol{\gamma}_{p}}-\hat{g}\|_{\infty}=\mathcal{O}(p^{-\alpha})\). Then, the sequence of trigonometric polynomials \(T_{p}:=\hat{g}_{\boldsymbol{\gamma}_{p}}\circ(\cos(\cdot)+1)\) fulfills the condition of Theorem 9. This would imply that \(f=\hat{g}\circ(\cos(\cdot)+1)\) is \(\alpha\)-Holder continuous, meaning that a constant \(K>0\) exists such that

\[|\hat{g}(\cos(x)+1)-\hat{g}(\cos(y)+1)|\leq K|x-y|^{\alpha}\;\forall\,x,y\in[0,2\pi]\]

Considering \(\lambda_{0}\in[0,2]\), \(\lambda\to\lambda_{0}\) and \(x=\arccos(\lambda_{0}-1)\), \(y=\arccos(\lambda-1)\) (using the \(\arccos\) branch in which both \(\lambda_{0}\), \(\lambda\) eventually end up) shows a contradiction to the assumed discontinuity of \(\hat{g}\). Therefore, no polynomial filter sequence \(\big{(}\hat{g}_{\boldsymbol{\gamma}_{p}}\big{)}_{p\in\mathbb{N}}\) together with an \(\alpha\in(0,1)\) exist such that \(\|\hat{g}_{\boldsymbol{\gamma}_{p}}-\hat{g}\|_{\infty}=\mathcal{O}(p^{-\alpha})\). In particular, for any sequence \(\big{(}\hat{g}_{\boldsymbol{\gamma}_{p}}\big{)}_{p\in\mathbb{N}}\), a sequence of adversarial values \((\lambda_{p})_{p\in\mathbb{N}}\), \(\lambda_{p}\in[0,2]\) exists such that

\[\nexists\alpha\in(0,1)\colon\;|\hat{g}_{\boldsymbol{\gamma}_{p}}(\lambda_{p} )-\hat{g}(\lambda_{p})|=\mathcal{O}(p^{-\alpha})\]

The proof is finished if we can find a sequence of graphs \((\mathcal{G}_{p})\) such that the symmetrically-normalized graph Laplacian \(\boldsymbol{L}_{p}\) of \(\mathcal{G}_{p}\) contains \(\lambda_{p}\) as an eigenvalue. In this case, we can construct adversarial input signals \(\boldsymbol{X}_{p}\) on the graphs \(\mathcal{G}_{p}\) by setting the first embedding channel to an eigenvector corresponding to \(\lambda_{p}\), and the remaining channels to zero, such that \(\big{(}g_{\boldsymbol{\gamma}_{p}}-g\big{)}\ast_{\mathcal{G}_{p}}\boldsymbol{X }_{p}=|\hat{g}_{\boldsymbol{\gamma}_{p}}(\lambda_{p})-\hat{g}(\lambda_{p})| \boldsymbol{X}_{p}\). In particular, it then holds that

\[\nexists\alpha\in\mathbb{R}^{+}\colon\sup_{0\neq\boldsymbol{X}\in\mathbb{R}^ {|\mathcal{G}_{p}|\times d}}\frac{\|(g_{\boldsymbol{\gamma}_{p}}-g)\ast_{ \mathcal{G}_{p}}\boldsymbol{X}\|_{\mathrm{F}}}{\|\boldsymbol{X}\|_{\mathrm{F }}}=\mathcal{O}\left(p^{-\alpha}\right)\]

If we assume only simple graphs, such a construction is unfortunately not possible since the set of all simple graphs and therefore the set of all realizable eigenvalues is countable, whereas the adversarial values \(\lambda_{p}\) could lie anywhere in the uncountable set \([0,2]\). We can, however realize arbitrary eigenvalues by using weighted graphs with three nodes. Consider a cyclic graph structure and tune the weight of edge \((1,2)\) to \(\sin^{2}(\theta_{p})\) and the weight of edges \((2,3)\) and \((3,1)\) to \(\cos^{2}(\theta_{p})\) with \(\theta_{p}\in\big{[}0,\frac{\pi}{2}\big{]}\). The symmetrically-normalized graph Laplacian,

\[\boldsymbol{L}_{p}=\begin{pmatrix}1&-\cos^{2}(\theta_{p})&-\sin^{2}(\theta_{p })\\ -\cos^{2}(\theta_{p})&1&-\sin^{2}(\theta_{p})\\ -\sin^{2}(\theta_{p})&-\sin^{2}(\theta_{p})&1\end{pmatrix},\]

has eigenvalues \(\lambda_{p}^{(1)}=1,\,\lambda_{p}^{(2)}=\sin^{2}(\theta_{p}),\,\lambda_{p}^{( 3)}=2-\sin^{2}(\theta_{p})\). \(\lambda_{p}^{(2)}\) can assume all values \(\lambda_{p}\in[0,1]\), whereas \(\lambda_{p}^{(3)}\) can assume all values \(\lambda_{p}\in[1,2]\). This finishes the proof. 

_Remark_.: If one wishes to restrict the set of possible adversarial graph sequences \(\left(\mathcal{G}_{p}\right)_{p\in\mathbb{N}}\) to include only simple graphs, a version of Theorem 3 still holds where we restrict the assumption to filters \(\hat{g}\) which are piecewise-continuous with discontinuities on a finite set of points \(\mathcal{D}\subset\mathcal{S}\), where \(\mathcal{S}\subset[0,2]\) denotes the countable set of eigenvalues realizable by simple graphs. This still covers a large class of filters to which order-\(p\) polynomial filters can provably converge slower than any inverse root of \(p\) in the operator norm, and includes the virtual node filter (discontinuous only in \(\lambda=0\)) presented as an example in the main body. The proof is fully analogous up to the point of constructing \(\lambda_{p}\). If \(\lambda_{p}\in\mathcal{D}\), we can find a graph that realizes it exactly. Now assume \(\lambda_{p}\notin\mathcal{D}\). We note that the set \(\mathcal{S}\) is dense in \([0,2]\) (clear from considering, e.g., the cyclic graphs \(\mathcal{C}_{n}\) with symmetrically-normalized Laplacian eigenvalues \(\lambda_{k}=1-\cos\big{(}\frac{2\pi k}{n}\big{)}\)). Since we assume that \(\hat{g}\) and therefore also \(|\hat{g}_{\boldsymbol{\gamma}_{p}}-\hat{g}|\) is piecewise-continuous anywhere but on \(\mathcal{D}\subset\mathcal{S}\) and \(\mathcal{D}\) is finite, we can find an open neighborhood \(\mathcal{N}(\lambda_{p})\) for any \(\lambda_{p}\notin\mathcal{D}\) on which \(\hat{g}\) is continuous. Using that \(\mathcal{S}\) is dense in \([0,2]\), we find a graph sequence \(\left(\tilde{\mathcal{G}}_{p}^{(l)}\right)_{l\in\mathbb{N}}\) with eigenvalues \(\tilde{\lambda}_{p}^{(l)}\in\mathcal{N}(\lambda_{p})\;\forall l\in\mathbb{N}, \left(\tilde{\lambda}_{p}^{(l)}\right)_{l\in\mathbb{N}}\to\lambda_{p}\) for which \(\|\hat{g}_{\boldsymbol{\gamma}_{p}}(\tilde{\lambda}_{p}^{(l)})-\hat{g}(\tilde{ \lambda}_{p}^{(l)})\|\to\|\hat{g}_{\boldsymbol{\gamma}_{p}}(\lambda_{p})-\hat{g}( \lambda_{p})\|\). Therefore, by the same reasoning as in the proof of Theorem 3, we find that there can be no \(\alpha\in(0,1)\) for which \(\sup_{0\neq\boldsymbol{X}\in\mathbb{R}^{|\mathcal{G}_{p}|\times d}}\frac{\|(g_{ \boldsymbol{\gamma}_{p}}-g)\ast_{\mathcal{G}_{p}}\boldsymbol{X}\|_{\mathrm{F}}}{ \|\boldsymbol{X}\|_{\mathrm{F}}}\) is of \(\mathcal{O}\left(p^{-\alpha}\right)\).

### Proof of Theorem 4

We first introduce the setting and notation to state Theorem 4 in its general version. We study how well S\({}^{2}\)GNNs can approximate "idealized" GNNs (_IGNNs_) containing \(L\) graph convolution layers \(1\leq l\leq L\), each of which can express a convolution operator \(g\) with _any_ spectral representation \(\hat{g}^{(l)}\colon\,[0,2]\to\mathbb{R}^{d^{(l)}}\). An IGNN layer therefore has the structure

\[\bm{\mathcal{H}}^{(l)}=\sigma\left(g^{(l)}\ast_{\mathcal{G}}[\bm{\mathcal{H}}^{ (l-1)}\bm{W}^{(l)}]\right)=\sigma\left(\bm{V}\hat{g}^{(l)}(\bm{\lambda}) \odot[\bm{V}^{\top}\bm{\mathcal{H}}^{(l-1)}\bm{W}^{(l)}]\right)\] (20)

with \(\bm{\mathcal{H}}^{(l)}\in\mathbb{R}^{n\times D^{(l)}}\), \(\bm{W}^{(l)}\in\mathbb{R}^{D^{(l)}\times D^{(l-1)}}\) and \(\bm{V}\in\mathbb{R}^{n\times n}\).

We compare this to S\({}^{2}\)GNNs with \(\ell=(m+1)L\) layers for \(m\geq 1\), in the additive form of Eq. 1,

\[\bm{H}^{(l)}=\mathrm{Spectral}^{(l)}(\bm{H}^{(l-1)};\bm{V},\bm{\lambda})+ \mathrm{Spatial}^{(l)}(\bm{H}^{(l-1)};\bm{A})\] (21)

Each layer \(1\leq l\leq\ell\) parametrizes a spatio-spectral convolution. The spectral part satisfies Eq. 3,

\[\mathrm{Spectral}^{(l)}(\bm{H}^{(l-1)};\bm{V},\bm{\lambda})=\bm{V}\Big{(}\hat {g}^{(l)}_{\vartheta}(\bm{\lambda})\odot\big{[}\bm{V}^{\top}\bm{H}^{(l-1)}\bm {W}^{(l)}_{\text{spec}}\big{]}\Big{)}\] (22)

with embeddings \(\bm{H}^{(l)}\in\mathbb{R}^{n\times\bar{d}^{(l)}}\), linear feature transforms \(f^{(l)}_{\theta}:=\bm{W}^{(l)}_{\text{spec}}\in\mathbb{R}^{d^{(l)}\times d^{(l -1)}}\) and a spectral filter \(\hat{g}^{(l)}_{\vartheta}:[0,2]\to\mathbb{R}\) that is fully supported and a universal approximator on \([0,\lambda_{\text{cut}}]\). Note we assume here that in every layer, there is only one spectral filter which gets reshaped as to act on every hidden component, whereas in practice, we relax this assumption to different filters per component, which can only be more expressive. The spatial part is a polynomial filter of the form

\[\mathrm{Spatial}^{(l)}(\bm{H}^{(l-1)};\bm{A}) =\sigma\left(\left[\sum_{j=0}^{p}\gamma^{(l)}_{j}\bm{L}^{j}\right] \bm{H}^{(l-1)}\bm{W}^{(l)}_{\text{spat}}\right)\] \[=\sigma\left(\bm{V}\Big{(}\hat{g}^{(l)}_{\bm{\gamma}}(\bm{ \lambda})\odot\big{[}\bm{V}^{\top}\bm{H}^{(l-1)}\bm{W}^{(l)}_{\text{spat}} \big{]}\Big{)}\right)\]

with \(\bm{W}^{(l)}_{\text{spat}}\in\mathbb{R}^{d^{(l)}\times d^{(l-1)}}\), polynomial order \(p\) (fixed across layers), and a spectral representation \(\hat{g}^{(l)}_{\bm{\gamma}}(\bm{\lambda})=\sum_{j=0}^{p}\gamma^{(l)}_{j} \lambda^{j}\) with coefficients \(\bm{\gamma}^{(l)}=(\gamma^{(l)}_{0},\dots,\gamma^{(l)}_{p})^{\top}\in\mathbb{ R}^{p+1}\). We note that theorem 4 extends immediately to the case of directed graphs if the spatial part is instead a polynomial of the magnetic Laplacian (see section 3.2.3) over complex-valued embeddings like in Zhang et al. (2021).

Note that the layer-wise hidden dimensions \(D^{(l)}\) vs. \(d^{(l)}\) of the IGNN vs. S\({}^{2}\)GNN do not have to agree except at the input layer, \(d^{(0)}=D^{(0)}\) (of course, both networks receive the same input \(\bm{\mathcal{H}}^{(0)}=\bm{H}^{(0)}=\bm{X}\)), and at the output layer, \(d^{(\ell)}=D^{(L)}\). We now state the general version of Theorem 4.

**Theorem** (Theorem 4, general).: _Assume an \(L\)-layer IGNN with filters \(\hat{g}^{(l)}\) such that \(\hat{g}^{(l)}\big{|}_{\lfloor\lambda_{\text{cut}},2\rceil}\in C^{r}[\lambda_{ \text{cut}},2]\) and \(\left\|\frac{d^{r}}{d\lambda^{r}}\hat{g}^{(l)}\big{|}_{\lfloor\lambda_{ \text{cut}},2\rceil}\right\|_{\infty}\leq K^{\text{max}}_{r}\left(\lambda_{ \text{cut}}\right)\) for all \(1\leq l\leq L\). Let \(\|\hat{g}^{(l)}\|_{\infty}\leq\|\hat{g}\|_{\infty}^{\text{max}}\) and \(\|\bm{W}^{(l)}\|_{2}\leq\|\bm{W}\|_{2}^{\text{max}}\) for all \(1\leq l\leq L\). Assume that \(\sigma=\lceil\cdot\rceil_{\geq}\) is the ReLu function. Then,_

_(1) For a fixed polynomial order \(p\geq 2\), an approximating sequence \(\big{(}\text{S}^{2}\text{GNN}_{m}\big{)}_{m\in\mathbb{N}}\) of \([(m+1)L]\)-layer S\({}^{2}\)GNNs exists such that, for arbitrary graph sequences \((\mathcal{G}_{m})_{m\in\mathbb{N}}\),_

\[\sup_{0\neq\bm{X}\in\mathbb{R}^{|\mathcal{G}_{p}|\times d}}\frac{ \|\big{[}\big{(}\text{S}^{2}\text{GNN}_{m}\big{)}_{\mathcal{G}_{m}}-(\text{IGNN} )_{\mathcal{G}_{m}}\big{]}\,(\bm{X})\|_{F}}{\|\bm{X}\|_{F}}\] \[= \,\mathcal{O}\Big{(}C_{L}(\|\hat{g}\|_{\infty}^{\text{max}},\|\bm{ W}\|_{2}^{\text{max}})\ K^{\text{max}}_{r}\left(\lambda_{\text{cut}}\right)\ (pm)^{-r}\Big{)},\] \[C_{L}(\|\hat{g}\|_{\infty}^{\text{max}},\|\bm{W}\|_{2}^{\text{ max}})=\|\bm{W}\|_{2}^{\text{max}}\prod_{l=1}^{L-1}\big{[}\|\hat{g}\|_{\infty}^{ \text{max}}\|\bm{W}\|_{2}^{\text{max}}+(\|\hat{g}\|_{\infty}^{\text{max}}\| \bm{W}\|_{2}^{\text{max}})^{l}\big{]}\]

_with a leading-order scaling constant that depends only on \(r\). Here, \((\,\cdot\,)_{\mathcal{G}_{m}}\) denotes the instantiation of all model filters on the eigenvalues of an input graph \(\mathcal{G}_{m}\), which maps both models onto a \(\mathcal{G}_{m}\)-dependent function \(\mathbb{R}^{D^{(0)}}\to\mathbb{R}^{D^{(L)}}\)._

_(2) For fixed \(m\geq 1\), an approximating sequence \(\big{(}\text{S}^{2}\text{GNN}_{p}\big{)}_{p\in\mathbb{N}}\) of \([(m+1)L]\)-layer S\({}^{2}\)GNNs with increasing layer-wise polynomial order \(p\) exists such that, for all \((\mathcal{G}_{p})_{p\in\mathbb{N}}\), the same bound holds._Proof.: We first prove the following lemma, narrowing down the previous theorem to a single layer.

**Lemma 1**.: _Let \(IGNN^{(l)}\) denote a single IGNN layer as in Eq. 20, with a filter \(\hat{g}^{(l)}\) such that \(\hat{g}^{(l)}|_{[\lambda_{\text{cut}},2]}\) is \(r\)-times continuously differentiable on \([\lambda_{\text{cut}},2]\) and satisfies a bound \(K_{r}\left(\hat{g}^{(l)},\lambda_{\text{cut}}\right)\geq 0\), \(|\frac{d^{r}}{d\lambda^{r}}\hat{g}^{(l)}(\lambda)|\leq K_{r}\left(\hat{g}^{(l)},\lambda_{\text{cut}}\right)\ \forall\lambda\in[\lambda_{\text{cut}},2]\). Let \(\sigma=[\,\cdot\,]_{\geq}\) be the ReLu function, and let \(\|\bm{W}^{(l)}\|_{2}\) denote the spectral norm of \(\bm{W}^{(l)}\). Then,_

_(1) For fixed polynomial order_ \(p\geq 2\)_, an approximating sequence_ \(\left(\text{S}^{2}\text{GNN}^{(l)}_{m}\right)_{m\in\mathbb{N}}\) _of_ \((m+1)\)_-layer_ \(\text{S}^{2}\text{GNNs}\) _exists such that, for arbitrary graph sequences_ \(\left(\mathcal{G}_{m}\right)_{m\in\mathbb{N}}\)__

\[\sup_{0\neq\bm{X}\in\mathbb{R}^{|\mathcal{G}_{p}|\times d}}\frac{\left\|\left[ \left(\text{S}^{2}\text{GNN}^{(l)}_{m}\right)_{\mathcal{G}_{m}}-\left(\text{ IGNN}^{(l)}\right)_{\mathcal{G}_{m}}\right]\left(\bm{X}\right)\right\|_{F}}{\| \bm{X}\|_{F}}=\mathcal{O}\left(\|\bm{W}^{(l)}\|_{2}K_{r}(\hat{g},\lambda_{\text {cut}})](pm)^{-r}\right)\]

_with a scaling constant that depends only on_ \(r\)_. Here,_ \((\,\cdot\,)_{\mathcal{G}_{m}}\) _denotes the instantiation of all model filters on the eigenvalues of an input graph_ \(\mathcal{G}_{m}\)_, which maps both models onto a_ \(\mathcal{G}_{m}\)_-dependent function_ \(\mathbb{R}^{D^{(l-1)}}\rightarrow\mathbb{R}^{D^{(l)}}\)_._

_(2) For fixed_ \(m\geq 1\)_, an approximating sequence_ \(\left(\text{S}^{2}\text{GNN}^{(l)}_{p}\right)_{p\in\mathbb{N}}\) _of_ \((m+1)\)_-layer_ \(\text{S}^{2}\text{GNNs}\) _with increasing layer-wise polynomial order_ \(p\) _exists such that, for all_ \((\mathcal{G}_{p})_{p\in\mathbb{N}}\)_, the same bound holds._

_Remark_.: The proof of the simplified Theorem 4 used in the main body is analogous to the proof of Lemma 1 just without the nonlinearity, which has the following consequences:

* The final layer \(m+1\) which we only need to apply one last nonlinearity to the output (since the spectral part of all layers, including the previous layer \(m\), has none) becomes obsolete, so the final layer instead becomes \(m\),
* The two limits (1) and (2) are equivalent by the reduction to an \(mp\)-order polynomial filter,
* We do not need the dimension-doubling "trick" outlined below to get rid of the nonlinearity in the proof and instead set all feature transform matrices in layers \(1\) through \(m-1\) to the identity and the final ones to \(\bm{W}^{*(m)}_{\text{spec}}=\bm{W}^{(l)}\), \(\bm{W}^{*(m)}_{\text{spat}}=\bm{W}^{(l)}\).

Proof of Lemma 1.: We first note that \(m\) S\({}^{2}\)GNN spatial parts, each of order \(p\), would act like an \((mp)\)-order polynomial filter (factorized into \(m\) order-\(p\) polynomials), were it not for the nonlinearities in between. However, using the fact that \(\sigma\) is the ReLu function, we can choose intermediate hidden dimensions twice the size of the input dimension and then use the linear transforms to store a positive and a negative copy of the embeddings, add them back together after applying each ReLu, just to split the result back into a positive and negative copy for the next layer. This essentially gets us rid of \(\sigma\). Throughout the proof, we use a star superscript to denote the specific parameters that will ultimately satisfy our bound, whereas we put no star above parameters that are yet to be fixed in a later part of the proof.

For \(m\geq 2\), the trick discussed above works if we set

\[\bm{W}^{*(1)}_{\text{spec}}=\frac{1}{2}\left(\bm{I}\quad-\bm{I} \right)\in\mathbb{R}^{D^{(l-1)}\times 2D^{(l-1)}},\] \[\bm{W}^{*(2)}_{\text{spec}},\ldots,\bm{W}^{*(m-1)}_{\text{spec}}= \frac{1}{2}\begin{pmatrix}\bm{I}\\ -\bm{I}\end{pmatrix}(\bm{I}\quad-\bm{I})\in\mathbb{R}^{2D^{(l-1)}\times 2D^{(l-1)}},\] \[\bm{W}^{*(m+1)}_{\text{spec}}=\bm{W}^{(l)}\begin{pmatrix}\bm{I} \\ -\bm{I}\end{pmatrix}\in\mathbb{R}^{2D^{(l-1)}\times D^{(l)}},\] \[\bm{W}^{*(1)}_{\text{spat}}=(\bm{I}\quad-\bm{I})\in\mathbb{R}^{2D^ {(l-1)}\times D^{(l-1)}},\] \[\bm{W}^{*(2)}_{\text{spat}},\ldots,\bm{W}^{*(m-1)}_{\text{spat}}= \begin{pmatrix}\bm{I}\\ -\bm{I}\end{pmatrix}(\bm{I}\quad-\bm{I})\in\mathbb{R}^{2D^{(l-1)}\times 2D^{(l-1)}},\] \[\bm{W}^{*(m+1)}_{\text{spat}}=\bm{W}^{(l)}\begin{pmatrix}\bm{I} \\ -\bm{I}\end{pmatrix}\in\mathbb{R}^{2D^{(l-1)}\times D^{(l)}}.\]In the case \(m=1\), pick the matrices \(\bm{W}_{\text{spec}}^{*(1)},\bm{W}_{\text{spat}}^{*(1)}\) from above for the first, and the matrices \(\bm{W}_{\text{spec}}^{*(m+1)},\bm{W}_{\text{spat}}^{*(m+1)}\) from above for the second layer.

Set \(\hat{g}_{\bm{\gamma}}^{*(m+1)}(\lambda)=1\) and \(\hat{g}_{\vartheta}^{*(m+1)}(\lambda)=0\). Given these choices and a graph \(\mathcal{G}\) with eigenvalues \(\bm{\lambda}\),

\[(\text{S}^{2}\text{GNN}^{(l)})_{\mathcal{G}}(\bm{X})=\sigma\Big{(}\bm{V}\left( \hat{g}_{\text{spsp}}(\bm{\lambda})\odot\big{[}\bm{V}^{\top}\bm{H}^{(l-1)}\bm{ W}^{(l)}\big{]}\right)\Big{)},\quad\hat{g}_{\text{spsp}}=\prod_{j=1}^{m}\left(\hat{g}_{ \vartheta}^{(j)}+\hat{g}_{\bm{\gamma}}^{(j)}\right)\]

We see that \(\hat{g}_{\text{spsp}}\big{|}_{[\lambda_{\text{max}},2]}=\prod_{j=1}^{m}\hat{g} _{\vartheta}^{(j)}\) since \(\hat{g}_{\vartheta}^{(j)}\big{|}_{[\lambda_{\text{max}},2]}=0\) for \(1\leq j\leq m\). This can express any polynomial up to order \(mp\) on \([\lambda_{\text{max}},2]\), since we assumed a layer-wise \(p\geq 2\) and any polynomial with real coefficients factorizes into real-coefficient polynomials of degree less or equal to \(2\) by the fundamental theorem of algebra. On the interval \([0,\lambda_{\text{max}}]\), on the other hand, the filter \(\hat{g}_{\text{spsp}}\big{|}_{[0,\lambda_{\text{max}}]}\) can express any IGNN filter \(\hat{g}^{(l)}\big{|}_{[0,\lambda_{\text{max}}]}\). For \(m=1\), this is immediately clear. Else, set \(\hat{g}_{\vartheta}^{(j)}\) to constants \(C_{j}\in\mathbb{R}_{\geq}\), \(1\leq j\leq m-1\) large enough that none of the polynomials \(\left(C_{j}+\hat{g}_{\bm{\gamma}}^{(j)}\right)\), \(1\leq j\leq m-1\), has a zero in \([0,\lambda_{\text{max}}]\). Defining \(\hat{g}_{\vartheta}^{(m)}=\frac{\hat{g}^{(l)}\big{|}_{[0,\lambda_{\text{max}} ]}}{\prod_{j=1}^{m}\left(C_{j}+\hat{g}_{\bm{\gamma}}^{(j)}\right)}-\hat{g}_{ \bm{\gamma}}^{(m)}\big{|}_{[0,\lambda_{\text{max}}]}\) gives the desired function.

We proceed by making use of a result by D. Jackson (Natanson, 1964), which is essentially a converse to Theorem 9 which we used to prove Theorem 3:

**Theorem** (Jackson's theorem on an interval).: _Let \(a<b\in\mathbb{R}\), \(k,r\in\mathbb{N}\) with \(k\geq r-1\geq 0\), \(f\in C^{r}[a,b]\). Then, a polynomial \(p_{k}\) of degree less or equal to \(k\) exists such that_

\[\|p_{k}-f\|_{\infty}\leq\frac{b-a}{2}\left(\frac{\pi}{2}\right)^{r}\frac{1}{(k +1)k\ldots(k-r+2)}\left\|\frac{d^{r}}{dx^{r}}f\right\|_{\infty}\]

Since \(\hat{g}_{\text{spsp}}\) can express any polynomial up to order \(mp\) on \([\lambda_{\text{max}},2]\) and, for any such polynomial, find parameters for the spectral parts that match the ideal filter \(\hat{g}^{(l)}\big{|}_{[0,\lambda_{\text{max}}]}\) exactly (not contributing to the supremum error), we can directly transfer this theorem to our case. Define \(\text{S}^{2}\text{GNN}_{m}^{(l)}\) from the lemma by setting the linear feature transforms and final-layer filters as above. For the filters in layers \(1\) through \(m\), define \(\gamma^{*(1)},\ldots,\gamma^{*(m)}\) such that \(\prod_{j=1}^{m}\hat{g}_{\bm{\gamma}}^{*(j)}\) factorizes into into the polynomial from Jackson's theorem on \([\lambda_{\text{max}},2]\), and \(\vartheta^{*(1)},\ldots,\vartheta^{*(m)}\) to match \(\hat{g}^{(l)}\) on \([0,\lambda_{\text{max}}]\). This defines a filter \(\hat{g}_{\text{spsp}}^{(l)}\). We then find, for \(mp\geq r-1\geq 0\),

\[\|\hat{g}_{\text{spsp}}^{(l)}-\hat{g}^{(l)}\|_{\infty}\leq\frac{2-\lambda_{ \text{max}}}{2}\left(\frac{\pi}{2}\right)^{r}\frac{1}{(mp+1)\,mp\,\ldots\,(mp- r+2)}\left\|\frac{d^{r}}{d\lambda^{r}}\hat{g}^{(l)}\big{|}_{[0,\lambda_{\text{max}}]} \right\|_{\infty}\]

Therefore, \(\|\hat{g}_{\text{spsp}}^{(l)}-\hat{g}^{(l)}\|_{\infty}\) is of \(\mathcal{O}\left(K_{r}(\hat{g},\lambda_{\text{cut}})(mp)^{-r}\right)\) and we can find a scaling constant that depends only on \(r\). Since the Lipschitz constant of \(\sigma\) is \(1\), we find for _any_ graph \(\mathcal{G}\) with eigenvalues \(\bm{\lambda}\) and any graph signal \(0\neq\bm{X}\in\mathbb{R}^{|\mathcal{G}|}\),

\[\frac{\left\|\left[(\text{S}^{2}\text{GNN}_{m}^{(l)})_{\mathcal{G }}-(\text{IGNN}^{(l)})_{\mathcal{G}}\right](\bm{X})\right\|_{F}}{\|\bm{X}\|_{F}} \leq\frac{\left\|\bm{V}\left(\hat{g}_{\text{spsp}}^{(l)}-\hat{g}^{(l)} \right)(\bm{\lambda})\odot\big{[}\bm{V}^{\top}\bm{X}\bm{W}^{(l)}\big{]}\right\|_ {F}}{\|\bm{X}\|_{F}}\] \[\leq\frac{\|\hat{g}_{\text{spsp}}^{(l)}-\hat{g}^{(l)}\|_{\infty} \left\|(\bm{V}\bm{V}^{\top})\bm{X}\bm{W}^{(l)}\right\|_{F}}{\|\bm{X}\|_{F}} \leq\|\hat{g}_{\text{spsp}}^{(l)}-\hat{g}^{(l)}\|_{\infty}\|\bm{W}^{(l)}\|_{2}\] \[=\mathcal{O}\left(\|\bm{W}^{(l)}\|_{2}K_{r}(\hat{g},\lambda_{ \text{cut}})](mp)^{-r}\right)\]

with a scaling constant that depends only on \(r\). Exactly the same procedure and bounds hold if we instead keep \(m\) fixed and increase \(p\). This finishes the proof of Lemma 1. 

We can now prove the main theorem by induction. Lemma 1 gives the initial step. Now, assume the theorem holds for \(L\) IGNN layers. We can then choose \(\left(\text{S}^{2}\text{GNN}_{m}^{(L+1)}\circ\text{S}^{2}\text{GNN}_{m}^{(L_{0} \cdots 01)}\right)_{m\in\mathbb{N}}\), where \(\text{S}^{2}\text{GNN}_{m}^{(L+1)}\) are the approximating models fulfilling Lemma 1, while \(\text{S}^{2}\text{GNN}_{m}^{(L_{0}\cdots 01)}\) fulfill the induction assumption. We assume fixed \(p\) and increasing \(m\), but the proof is fully analogous in the other case. Applying the same decomposition to \((\text{IGNN}_{m})_{m\in\mathbb{N}}\) lets us express the error on a graph sequence \((\mathcal{G}_{m})_{m\in\mathbb{N}}\) as

\[\frac{\left\|\left[\left(\text{S}^{2}\text{GNN}_{m}\right)_{ \mathcal{G}_{m}}-\left(\text{IGNN}\right)_{\mathcal{G}_{m}}\right](\bm{X}) \right\|_{F}}{\left\|\bm{X}\right\|_{F}}\] \[= \frac{\left\|\left[\left(\text{S}^{2}\text{GNN}_{m}^{(L+1)} \circ\text{S}^{2}\text{GNN}_{m}^{(L_{0}\cdots 01)}\right)_{\mathcal{G}_{m}}- \left(\text{IGNN}_{m}^{(L+1)}\circ\text{IGNN}_{m}^{(L_{0}\cdots 01)}\right)_{ \mathcal{G}_{m}}\right](\bm{X})\right\|_{F}}{\left\|\bm{X}\right\|_{F}}\] \[\leq \left(\left\|\bm{X}\right\|_{F}\right)^{-1}\left\|\left[\left( \text{S}^{2}\text{GNN}_{m}^{(L+1)}\circ\text{S}^{2}\text{GNN}_{m}^{(L_{0} \cdots 01)}\right)_{\mathcal{G}_{m}}-\left(\text{S}^{2}\text{GNN}_{m}^{(L+1)} \circ\text{IGNN}_{m}^{(L_{0}\cdots 01)}\right)_{\mathcal{G}_{m}}\right](\bm{X}) \right\|_{F}\] \[+ \left(\left\|\bm{X}\right\|_{F}\right)^{-1}\left\|\left[\left( \text{S}^{2}\text{GNN}_{m}^{(L+1)}\circ\text{IGNN}_{m}^{(L_{0}\cdots 01)}\right)_{ \mathcal{G}_{m}}-\left(\text{IGNN}_{m}^{(L+1)}\circ\text{IGNN}_{m}^{(L_{0} \cdots 01)}\right)_{\mathcal{G}_{m}}\right](\bm{X})\right\|_{F}\] \[\leq \left[\|\hat{g}\|_{\infty}^{\text{max}}\|\bm{W}\|_{2}^{\text{max} }+\mathcal{O}(K_{r}^{\text{max}}\left(\lambda_{\text{cut}}\right)(pm)^{-r}) \right]\mathcal{O}\Big{(}C_{L}(\|\hat{g}\|_{\infty}^{\text{max}},\|\bm{W}\|_{2 }^{\text{max}})\;K_{r}^{\text{max}}\left(\lambda_{\text{cut}}\right)\;(pm)^{- r}\Big{)}\] \[+ \mathcal{O}(K_{r}^{\text{max}}\left(\lambda_{\text{cut}}\right)(pm )^{-r})(\|\hat{g}\|_{\infty}^{\text{max}}\|\bm{W}\|_{2}^{\text{max}})^{L}\] \[= \mathcal{O}\Big{(}C_{L+1}(\|\hat{g}\|_{\infty}^{\text{max}},\|\bm {W}\|_{2}^{\text{max}})\;K_{r}^{\text{max}}\left(\lambda_{\text{cut}}\right) \;(pm)^{-r}\Big{)}.\]

We first used the triangle inequality in line 3 to split the difference into two terms. Next, we bound the first term using the induction assumption, as well as the Lipschitz constant of \(\text{S}^{2}\text{GNN}_{m}^{(L+1)}\), which in turn, by Lemma 1, is the Lipschitz constant of \(\text{IGNN}_{m}^{(L+1)}\) up to a term of \(\mathcal{O}(K_{r}^{\text{max}}\left(\lambda_{\text{cut}}\right)(pm)^{-r})\). We moreover bound the second term using the Lipschitz constant of \(\text{IGNN}_{m}^{(L_{0}\cdots 01)}\), as well as Lemma 1 to arrive at the final result. 

### Proof of Theorem 5

We next prove the stability of our positional encodings:

**Theorem 5**.: _The Positional Encodings \(\mathrm{PE}\) in Eq. 5 are stable according to Definition 1._

Recall the definition of stability via Holder continuity:

**Definition 1** (Stable \(\mathrm{PE}\)).: _(Huang et al., 2024) A PE method \(\mathrm{PE}:\mathbb{R}^{n\times k}\times\mathbb{R}^{k}\rightarrow\mathbb{R}^{n \times k}\) is called stable, if there exist constants \(c,C>0\), such that for any Laplacian \(\bm{L},\bm{L}^{\prime}\), and \(\bm{P}_{*}=\arg\min_{\bm{P}}\|\bm{L}-\bm{P}\bm{L}^{\prime}\bm{P}^{\top}\|_{ \mathrm{F}}\)_

\[\left\|\mathrm{PE}(\mathrm{EVD}(\bm{L}))-\bm{P}_{*}\,\mathrm{PE}\left(\mathrm{ EVD}\left(\bm{L}^{\prime}\right)\right)\right\|_{\mathrm{F}}\leq C\cdot\left\|\bm{L}-\bm{P}_{*}\bm{L}^{\prime}\bm{P}_{*}^{ \top}\right\|_{\mathrm{F}}^{c}.\] (6)

For this proof, we build on the work of Huang et al. (2024) where the authors show that under the assumptions of Definition 2, and some minor adjustments, a positional encoding of the following form Eq. 23 is stable (Theorem 10).

\[\mathrm{SPE}(\bm{V},\bm{\lambda})=\rho\left(\bm{V}\,\mathrm{diag}\left(\phi_{1}( \bm{\lambda})\right)\bm{V}^{\top},\bm{V}\,\mathrm{diag}\left(\phi_{2}(\bm{ \lambda})\right)\bm{V}^{\top},\ldots,\bm{V}\,\mathrm{diag}\left(\phi_{k}(\bm{ \lambda})\right)\bm{V}^{\top}\right)\] (7)

**Definition 2**.: _The key assumptions for SPE are as follows:_

* \(\phi_{\ell}\) _and_ \(\rho\) _are permutation equivariant._
* \(\phi_{\ell}\) _is_ \(K_{\ell}\)_-Lipschitz continuous: for any_ \(\bm{\lambda},\bm{\lambda}^{\prime}\in\mathbb{R}^{k}\)_,_ \(\|\phi_{\ell}(\bm{\lambda})-\phi_{\ell}\left(\bm{\lambda}^{\prime}\right)\|_{ \mathrm{F}}\leq K_{\ell}\left\|\bm{\lambda}-\bm{\lambda}^{\prime}\right\|\)_._
* \(\rho\) _is J-Lipschitz continuous: for any_ \([\bm{B}_{1},\bm{B}_{2},\ldots,\bm{B}_{k}]\in\mathbb{R}^{n\times n\times k}\) _and_ \([\bm{B}_{1}^{\prime},\bm{B}_{2}^{\prime},\ldots,\bm{B}_{k}^{\prime}]\in\mathbb{R}^{ n\times n\times k},\|\rho\left(\bm{B}_{1},\bm{B}_{2},\ldots,\bm{B}_{k}\right)-\rho \left(\bm{B}_{1}^{\prime},\bm{B}_{2}^{\prime},\ldots,\bm{B}_{k}^{\prime}\right)\|_{ \mathrm{F}}\leq J\sum_{l=1}^{k}\|\bm{B}_{\ell}-\bm{B}_{\ell}^{\prime}\|_{ \mathrm{F}}\)_._

**Theorem 10** (Stability of Eq. 23 by Huang et al. (2024)).: _Under Definition 2, \(\mathrm{SPE}\) (Eq. 23) is stable with respect to the input Laplacian: for Laplacians \(\bm{L},\bm{L}^{\prime}\),_

\[\left\|\mathrm{SPE}(\mathrm{EVD}(\bm{L}))-\bm{P}_{*}\,\mathrm{SPE} \left(\mathrm{EVD}\left(\bm{L}^{\prime}\right)\right)\right\|_{\mathrm{F}}\leq \left(\alpha_{1}+\alpha_{2}\right)k^{5/4}\sqrt{\left\|\bm{L}-\bm{P}_{*} \bm{L}\bm{P}_{*}^{\top}\right\|_{\mathrm{F}}}\] (8) \[+\left(\alpha_{2}\frac{k}{\gamma}+\alpha_{3}\right)\left\|\bm{L}-\bm {P}_{*}\bm{L}\bm{P}_{*}^{\top}\right\|_{\mathrm{F}},\]_where the constants are \(\alpha_{1}=2J\sum_{k=1}^{k}K_{\ell},\alpha_{2}=4\sqrt{2}J\sum_{l=1}^{k}M_{\ell}\), and \(\alpha_{3}=J\sum_{l=1}^{k}K_{\ell}\). Here \(M_{\ell}=\sup_{\bm{\lambda}\in[0,2]^{k}}\left\|\phi_{\ell}(\bm{\lambda})\right\|\) and again \(\bm{P}_{*}=\arg\min_{\bm{P}\in\Pi(n)}\left\|\bm{L}-\bm{P}_{*}\bm{L}\bm{P}_{*}^{ \top}\right\|_{\mathrm{F}}\). The eigengap \(\gamma=\lambda_{k+1}-\lambda_{k}\) is the difference between the \((k+1)\)-th and \(k\)-th smallest eigenvalues, and \(\gamma=+\infty\) if \(k=n\)._

We prove a similar bound for general weighted adjacency matrices \(\bm{A}\in\mathbb{R}_{\geq 0}^{n\times n}\) (note that such a stability result would be trivial if we restrict \(\bm{A}\in\{0,1\}^{n\times n}\), since any function on a finite set is Lipschitz continuous). To achieve this, we need a technical assumption in order to ensure that the function values do not blow up and degree normalization is indeed a Lipschitz continuous function: We assume that the domain of \(\bm{A}\) is restricted to (symmetric) matrices whose degrees are uniformly bounded by some constants \(0<\tilde{D}_{\min}<\tilde{D}_{\max}\):

\[d_{u}\,:=\,\sum_{v}A_{u,v}\,\in\,[\tilde{D}_{\min},\tilde{D}_{\max}]\qquad \forall u\in\{1,\ldots,n\}.\] (25)

To decompose the proof into smaller pieces we commonly use the well-known fact that the composition of Lipschitz continuous functions \(f_{1}\circ f_{2}\), with constants \(C_{1}\) and \(C_{2}\), is also Lipschitz continuous \(\|f_{1}(f_{2}(y))-f_{1}(f_{2}(x))\|\leq C_{1}C_{2}\left\|y-x\right\|\) with constant \(C_{1}C_{2}\).

Proof.: Our proposed encoding (Eq. 5) matches roughly Eq. 23. Specifically, \(\phi_{\ell}(\bm{\lambda})=\mathrm{softmax}((\lambda_{j}-\bm{\lambda})\odot( \lambda_{j}-\bm{\lambda})/\sigma^{2})\) with \(\sigma\in\mathbb{R}_{>0}\). However, \(\rho_{\ell}(\bm{B}_{1},\bm{B}_{2},\ldots,\bm{B}_{k})\) does not directly match \(||_{j=1}^{k}[\bm{B}_{j}\odot\bm{A}]\cdot\vec{1}\), since it is also a function of the adjacency \(\bm{A}\). Nevertheless, we show that \(\phi_{\ell}\) is \(K_{\ell}\)-Lipschitz continuous and \(\rho\) is \(J\)-Lipschitz continuous, where we also bound the change of \(\bm{A}\).

We will start with \(\phi_{\ell}(\bm{\lambda})=\mathrm{softmax}((\lambda_{j}-\bm{\lambda})\odot( \lambda_{j}-\bm{\lambda})/\sigma^{2})\). The \(\mathrm{softmax}\) is well-known to be of Lipschitz constant 1 w.r.t. the \(L^{2}\) vector norm/Frobenius norm. \(-\nicefrac{{\bm{x}}}{{\sigma}}\) has a Lipschitz constant of \(1/\sigma\). This leaves us with the quadratic term \(\psi_{u}(\bm{\lambda})=(\lambda_{u}-\bm{\lambda})\odot(\lambda_{u}-\bm{\lambda})\) where we bound the norm of the Jacobian

\[J_{\psi_{u}}=\begin{bmatrix}-2(\lambda_{u}-\lambda_{1})&0&\ldots&0&\ldots&0\\ 0&-2(\lambda_{u}-\lambda_{2})&\ldots&0&\ldots&0\\ \vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\ 0&0&\ldots&0&\ldots&0\\ \vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\ 0&0&\ldots&0&\ldots&-2(\lambda_{u}-\lambda_{k})\end{bmatrix}\] (26)

that is zero everywhere except for the diagonal entries, excluding its \(u\)-th entry. Thus, \(\|J_{\psi_{u}}\|_{\mathrm{F}}\leq 2k\max_{v\in\{1,2,\ldots,k\}}(\lambda_{v}- \lambda_{u})\leq 2k(\lambda_{k}-\lambda_{1})\leq 4k\), as \(0=\lambda_{1}\leq\lambda_{k}\leq 2\). We can therefore use \(K_{\ell}:=4k/\sigma\).

Now we continue with \(\tilde{\rho}_{\ell}(\bm{A},\bm{B}_{1},\bm{B}_{2},\ldots,\bm{B}_{k})\). For \(f(\bm{A},\bm{B})=(\bm{B}\odot\bm{A})\cdot\vec{1}\) with a general weighted adjacency \(\bm{A}\in\mathbb{R}^{n\times n}\), we consider

\[\|(\bm{B}\odot\bm{A})\cdot\vec{1}-(\bm{B}^{\prime}\odot\bm{A}^{ \prime})\cdot\vec{1}\|_{\mathrm{F}} \underset{(A)}{\leq}\|\vec{1}\|_{2}\|\bm{B}\odot\bm{A}-\bm{B}^{ \prime}\odot\bm{A}^{\prime}\|_{\mathrm{F}}\] (27) \[= \sqrt{n}\|\bm{B}\odot\bm{A}-\bm{B}^{\prime}\odot\bm{A}^{\prime}\| _{\mathrm{F}}\] \[= \sqrt{n}\|\bm{B}\odot\bm{A}-\bm{B}^{\prime}\odot\bm{A}+\bm{B}^{ \prime}\odot\bm{A}-\bm{B}^{\prime}\odot\bm{A}^{\prime}\|_{\mathrm{F}}\] \[\underset{(B)}{\leq}\sqrt{n}\|\bm{B}\odot\bm{A}-\bm{B}^{\prime} \odot\bm{A}\|_{\mathrm{F}}+\sqrt{n}\|\bm{B}^{\prime}\odot\bm{A}-\bm{B}^{ \prime}\odot\bm{A}^{\prime}\|_{\mathrm{F}}\] \[= \sqrt{n}\|(\bm{B}-\bm{B}^{\prime})\odot\bm{A}\|_{\mathrm{F}}+ \sqrt{n}\|\bm{B}^{\prime}\odot(\bm{A}-\bm{A}^{\prime})\|_{\mathrm{F}}\] \[\underset{(C)}{\leq}\sqrt{n}\underset{(D)}{\max}\underset{(D)}{ \max}\underset{(E)}{\leq}\|\bm{B}-\bm{B}^{\prime}\|_{\mathrm{F}}+\underset{(E)}{ \underset{(E)}{\leq}1}\sqrt{n}\,\underset{(E)}{\leq}\sqrt{n}\,\underset{(E)}{ \leq}\sqrt{n}\,\underset{(E)}{\underbrace{\|\bm{A}-\bm{A}^{\prime}\|_{\mathrm{F}}}} \,.\]

(A) holds by Cauchy-Schwarz, (B) by triangle inequality, (C) by Cauchy-Schwarz, (D) follows from the domain of \(\bm{A}\), and (E) is true since the largest eigenvalue of \(\bm{B}=\bm{V}\phi_{\ell}(\bm{\lambda})\bm{V}^{\top}\) is \(1\) because \(\phi_{\ell}(\bm{\lambda})_{j}\leq 1,\forall 1\leq j\leq k\).

To further bound (F), i.e., note that \(\|\bm{L}-\bm{L}^{\prime}\|_{\mathrm{F}}=\|\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2}-\bm{D} ^{\prime-1/2}\bm{A}^{\prime}\bm{D}^{\prime-1/2}\|_{\mathrm{F}}\). For \(g(\bm{A}):=\bm{D}^{1/2}\bm{A}\bm{D}^{1/2}\), our initial assumption from Eq. 25 yields the existence of a Lipschitz constant \(C_{\tilde{D}_{\min},\tilde{D}_{\max}}\) for \(g\), which can be verified by computing the partial derivatives of \(g\). Thus, we can bound

\[\|\bm{A}-\bm{A}^{\prime}\|_{\mathrm{F}} =\|g(\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2})-g(\bm{D}^{\prime-1/2}\bm{A }^{\prime}\bm{D}^{\prime-1/2})\|_{\mathrm{F}}\] (28) \[\leq C_{\tilde{D}_{\min},\tilde{D}_{\max}}\|\bm{D}^{-1/2}\bm{A} \bm{D}^{-1/2}-\bm{D}^{\prime-1/2}\bm{A}^{\prime}\bm{D}^{\prime-1/2}\|_{ \mathrm{F}}\] \[=C_{\tilde{D}_{\min},\tilde{D}_{\max}}\|\bm{L}-\bm{L}^{\prime}\| _{\mathrm{F}}\ =:\ \alpha_{4}\|\bm{L}-\bm{L}^{\prime}\|_{\mathrm{F}}.\]

As concatenation of \(k\) vectors \(|\big{|}_{j=1}^{k}\bm{x}\) has a Lipschitz constant of 1, we have \(J=\sqrt{n}\tilde{D}_{\max}\). Moreover, we have an additional term for the RHS of Eq. 24 with constant \(\alpha_{4}\sqrt{n}k\), coming from (F) and Eq. 28.

To finalize the proof, we restate the beginning of the proof of Huang et al. (2024) and incorporate the additional \(\bm{A}\)-dependency of \(\tilde{\rho}_{\ell}(\bm{A},\bm{B}_{1},\bm{B}_{2},\ldots,\bm{B}_{k})\) with \(\bm{B}_{j}=\bm{V}\operatorname{diag}\left(\phi_{j}(\bm{\lambda})\right)\bm{V} ^{\top}\) for \(1\leq j\leq k\).

\[\|\mathrm{SPE}(\mathrm{EVD}(\bm{L}),\bm{L})-\bm{P}_{*}\,\mathrm{ SPE}\left(\mathrm{EVD}\left(\bm{L}^{\prime}\right),\bm{L}\right)\|_{\mathrm{F}}\] (29) \[=\|\tilde{\rho}_{\ell}(\bm{A},\bm{B}_{1},\bm{B}_{2},\ldots,\bm{B} _{k})-\bm{P}_{*}\tilde{\rho}_{\ell}(\bm{A}^{\prime},\bm{B}_{1}^{\prime},\bm{B }_{2}^{\prime},\ldots,\bm{B}_{k}^{\prime})\|_{\mathrm{F}}\] \[=\|\tilde{\rho}_{\ell}(\bm{A},\bm{B}_{1},\bm{B}_{2},\ldots,\bm{B} _{k})-\tilde{\rho}_{\ell}(\bm{P}_{*}\bm{A}^{\prime}\bm{P}_{*}^{\top},\bm{P}_{* }\bm{B}_{1}^{\prime}\bm{P}_{*}^{\top},\bm{P}_{*}\bm{B}_{2}^{\prime}\bm{P}_{*} ^{\top},\ldots,\bm{P}_{*}\bm{B}_{k}^{\prime}\bm{P}_{*}^{\top})\|_{\mathrm{F}}\] \[\leq\underbrace{\left[J\sum_{l=1}^{k}\left\|\bm{B}_{l}-\bm{P}_{* }\bm{B}_{l}^{\prime}\bm{P}_{*}^{\top}\right\|_{\mathrm{F}}\right]}_{\text{ subject of Huang et al. (2024)}}+\alpha_{4}\sqrt{n}k\left\|\bm{L}-\bm{P}_{*}\bm{L}_{l}^{\prime}\bm{P}_{*}^{ \top}\right\|_{\mathrm{F}}.\]

Including the extra term stemming from our \(\bm{A}\)-dependent \(\tilde{\rho}_{\ell}(\bm{A},\bm{B}_{1},\bm{B}_{2},\ldots,\bm{B}_{k})\), the stability guarantee reads

\[\|\mathrm{SPE}(\mathrm{EVD}(\bm{L}),\bm{L})-\bm{P}_{*}\,\mathrm{ SPE}\left(\mathrm{EVD}\left(\bm{L}^{\prime}\right),\bm{L}\right)\|_{\mathrm{F}}\leq \left(\alpha_{1}+\alpha_{2}\right)k^{5/4}\sqrt{\left\|\bm{L}- \bm{P}_{*}\bm{L}\bm{P}_{*}^{\top}\right\|_{\mathrm{F}}}\] (30) \[+\left(\alpha_{2}\frac{k}{\gamma}+\alpha_{3}+\alpha_{4}\sqrt{n}k \right)\left\|\bm{L}-\bm{P}_{*}\bm{L}\bm{P}_{*}^{\top}\right\|_{\mathrm{F}}\]

with the newly introduced \(\alpha_{4}\) arising as Lipschitz constant of (inverse) degree normalization. The proof is complete. 

**Windowing for "eigengap" independent bounds.** Note that \(C\) depends on the eigengap between \(\nicefrac{{1}}{{\lambda_{k+1}-\lambda_{k}}}\) at the frequency cutoff. One should be able to improve upon this bound with windowing (see Fig. 7)), effectively lowering the Lipschitz constant of \(\hat{h}_{j}(\bm{\lambda})\) around \(\lambda_{k}\). We leave a formal treatment of this insight to future work.

### Proof of Theorem 6

We next prove the expressivity of a GNN/S\({}^{2}\)GNN in combination with our positional encodings:

**Theorem 6**.: \(S^{2}\)_GNNs are strictly more expressive than 1-WL with the \(\mathrm{PE}\) of Eq. 5._

For this, we assume that the positional encodings are the only node attributes, subsuming a constant feature or that there is a linear transformation on the raw features. We require that the choice of spatial MPGNN / spectral filter is at least as expressive as the 1-WL test, which is the case, e.g., for GIN. Moreover, we assume that the node-level embeddings are aggregated to the graph level using summation.

Proof.: To show that \(\mathrm{GNN}(\mathrm{PE}(\bm{V},\lambda))\) is strictly more expressive as 1-WL. For all graphs that 1-WL can distinguish, the GNN may learn to ignore the \(\mathrm{PE}\). Thus, we only need to prove that the positional encodings/node features of \(\mathrm{PE}(\bm{V},\lambda)\) suffice to distinguish some graphs that 1-WL could not distinguish. For all graphs that 1-WL can distinguish we know, by assumption, that the \(\mathrm{GNN}\) can distinguish the graphs.

As Li et al. (2020) point out, 1-WL (and MPGNN that are as capable as 1-WL) cannot distinguish degree-regular graphs with the same number of nodes and degrees. A degree regular graph is a graph where each node has the same degree. This is closely related to Theorem 11.

We next show that our \(\mathrm{PE}\) alone distinguishes certain degree-regular graphs. In this construction, we consider all 3-regular graphs with \(n=8\) nodes for this (see Fig. 15). The encodings \(\mathrm{\tilde{I}}\,\mathrm{PE}\) result in the following values with \(\sigma=0.001\) and rounded to max 2 decimal places:

\[\begin{split}\mathrm{\tilde{I}}^{\top}\,\mathrm{PE}(\mathrm{EVD} (\boldsymbol{L}_{1}))&=[3\;\;1.73\;\;1\;\;0.41\;\;-1\;\;-1\;\;-1.73 \;\;\;-2.41]\\ \mathrm{\tilde{I}}^{\top}\,\mathrm{PE}(\mathrm{EVD}(\boldsymbol{L}_ {2}))&=[3\;\;1.56\;\;\;0.62\;\;\;0.62\;\;\;0\;\;-1.62\;\;\;-1.62\; \;\;-2.41]\\ \mathrm{\tilde{I}}^{\top}\,\mathrm{PE}(\mathrm{EVD}(\boldsymbol{L}_ {3}))&=[3\;\;1.73\;\;1\;\;0.41\;\;-1\;\;-1\;\;-1.73\;\;\;-2.41]\\ \mathrm{\tilde{I}}^{\top}\,\mathrm{PE}(\mathrm{EVD}(\boldsymbol{L}_ {4}))&=[3\;\;1\;\;1\;\;0.41\;\;0.41\;\;-1\;\;-2.41\;\;\;-2.41]\\ \mathrm{\tilde{I}}^{\top}\,\mathrm{PE}(\mathrm{EVD}(\boldsymbol{L}_ {5}))&=[3\;\;1\;\;1\;\;1\;\;-1\;\;-1\;\;-1\;\;-3]\end{split}\] (31)

By constructing examples, this shows that our \(\mathrm{PE}\) can distinguish 4 out of the 5 3-regular graphs with 8 nodes. Thus, our \(\mathrm{PE}\) may distinguish at least some graphs that 1-WL cannot. This concludes the proof.

## Appendix I Expressivity of Spectral Filters and Spectrally Designed Spatial Filters

While it is well-known that common spatial MPGNNs are at most as expressive as 1-WL and that spectrally designed GNNs can be more expressive than 1-WL (Theorem 2 of Balcilar et al. (2021a)), we show that spectral GNNs are not able to distinguish degree-regular graphs. This upper bound was

Figure 15: Our positional encodings \(\mathrm{PE}\) Eq. 5 illustrated in the node colors and sizes. We plot all 5 (rows) 3-regular graphs with 8 nodes and all possible dimensions of the encoding (columns). We use \(\sigma=0.001\). Color denotes the sign, and size encodes the absolute value. We hypothesize that the visual “smoothness” between graphs and dimensions is due to our \(\mathrm{PE}\)’s stability (Theorem 5).

not known/formalized prior to our work (Bo et al., 2023b). Fortunately, our \(\mathrm{PE}\) largely mitigates the limitation. The improved expressivity of our positional encodings, along with their efficiency, stems from the element-wise product with \(\bm{A}\) (see also Geerts (2021)).

**Theorem 11**.: _Spectral filters \(\bm{V}\,\operatorname{diag}(\hat{g}(\bm{\lambda}))\bm{V}^{\top}\bar{\mathbb{I}}\) are strictly less expressive than 3-WL with Laplacian \(\bm{L}=\bm{D}-\bm{A}\), \(\bm{L}=\bm{I}-\bm{D}^{-1}\bm{A}\), or \(\bm{L}=\bm{I}-\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2}\)._

Proof.: The proof relies on properties of the eigenvectors for the different choices \(\bm{L}_{\text{u}}=\bm{D}-\bm{A}\), \(\bm{L}_{\text{rw}}=\bm{I}-\bm{D}^{-1}\bm{A}\), or \(\bm{L}_{\text{s}}=\bm{I}-\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2}\). For \(\bm{L}_{\text{u}}\bar{\mathbb{I}}=\lambda_{\text{0}}\bar{\mathbb{I}}=0\) and \(\bm{L}_{\text{rw}}\bar{\mathbb{I}}=\lambda_{\text{0}}\bar{\mathbb{I}}=0\) the first eigenvector is constant. The first eigenvector of \(\bm{L}_{\text{s}}\) is \(\bm{D}^{1/2}\bar{\mathbb{I}}\) (ignoring normalization). Thus, for degree-regular graphs, the first eigenvector of \(\bm{L}_{\text{s}}\) is also constant.

By the orthogonality of eigenvectors, \(\bm{v}_{\text{u}}\perp\bm{v}_{\text{v}}\) if \(u\neq v\), we know that all other eigenvectors are orthogonal to constant node features. Consequently, the "Fourier transformed" node features are \(\bm{V}^{\top}\bar{\mathbb{I}}=\begin{bmatrix}\sqrt{n}&0&\ldots&0\end{bmatrix}\) for all three choices \(\bm{L}_{\text{u}}\), \(\bm{L}_{\text{rw}}\), and \(\bm{L}_{\text{s}}\). Since this is true for all degree-regular graphs, spectral GNNs cannot distinguish degree-regular graphs with the same number of nodes.

Since the 3-WL test can distinguish some degree-regular graphs, 3-WL is strictly more expressive than a spectral GNN. 

**Corollary 1**.: _"Spectrally designed" MPGNNs that use a polynomial parametrization of filter \(\operatorname{diag}(\hat{g}(\bm{\lambda}))\) are strictly less expressive than 3-WL with the same choices for \(\bm{L}\)._

Proof.: With a polynomial parametrization of the spectral filter \(\hat{g}(\bm{\lambda})\), we know \(\bm{V}(\hat{g}(\bm{\lambda})\odot[\bm{V}^{\top}\bm{x}])=\bm{V}\operatorname{ diag}(\hat{g}(\bm{\lambda}))\bm{V}^{\top}\bm{x}=\hat{g}(\bm{L})\bm{x}=\sum_{j=0}^{p} \gamma_{j}\bm{L}^{j}\bm{x}\) (see SS 2). Due to this equivalence between a spectral and spatial filter and the constant node features \(\bm{x}=\bar{\mathbb{I}}\), any polynomial filter \(\sum_{j=0}^{p}\gamma_{j}\bm{L}^{j}\bar{\mathbb{I}}\) cannot distinguish degree-regular graphs. This argument also holds if the polynomial filter is normalized by the maximum eigenvalue as done by ChebNet (Defferrard et al., 2017). 

## Appendix J Further Remarks on S\({}^{2}\)GNNs

We next provide insights, details, and remarks on the details and variants of S\({}^{2}\)GNNs, accompanying the main section SS 3. The structure roughly follows the main body.

Next to the overview in Fig. 2 and the method description of the main part, we provide pseudo-code in Algo. 1 for a Spatio-Spectral Graph Neural Network on a graph with node attributes, and in Algo. 2 for a spectral filter.

```
1:Input: Adjacency \(\bm{A}\in\mathbb{R}_{\geq 0}^{n\times n}\), node attributes \(\bm{X}\in\mathbb{R}^{n\times d^{(0)}}\), number of eigenvectors \(k\)
2:\(\bm{V},\bm{\lambda}\leftarrow\mathrm{EVD}(\bm{L}(\bm{A}),k)\)
3:\(\bm{H}^{(0)}\leftarrow\bm{X}+\mathrm{PE}(\bm{V},\bm{\lambda})\)
4:for\(l\in\{1,2,\dots,\ell\}\)do
5:\(\bm{H}^{(l)}\leftarrow\mathrm{Spectral}^{(l)}(\bm{H}^{(l-1)};\bm{V},\bm{\lambda} )+\mathrm{Spatial}^{(l)}(\bm{H}^{(l-1)};\bm{A})\)
6:Return\(\bm{H}^{(l)}\) ```

**Algorithm 1** Spatio-Spectral Graph Neural Network (S\({}^{2}\)GNN), implementing Eq. 1

### Visualization of Spectral Filters

In Fig. 16, we provide further examples of hierarchies/eigenspaces spectral filters have access to, complementing Fig. 3 & 4. Here and in the main part, we use the main diagonal of \(\sum_{j\,\text{s.t.}\,\lambda_{j}=\lambda_{u}}\boldsymbol{v}_{j}\boldsymbol{v}_ {j}^{\top}\) for deciding on the edge weights of the graph structures, potentially summing over multiple eigenvectors with identical values \(\lambda_{j}=\lambda_{u}\). We take the product \(\prod_{j\,\text{s.t.}\,\lambda_{j}=\lambda_{u}}\operatorname{sign}( \boldsymbol{v}_{j})\) for visualizing the sign of the \(n\) edges for the global aggregation.

For all graphs, the first eigenvector denotes the constant signal (for \(\boldsymbol{L}=\boldsymbol{D}-\boldsymbol{A}\)). For (a-d), we observe that the second eigenspace roughly describes a half oscillation, i.e., the left vs. right part of the graph. The third eigenspace separates the middle parts. For (a), the fourth eigenspace models the interactions between the extremal nodes. For (b-d), the frequency increments again, effectively clustering the graph in four roughly equal pieces. For (e), the eigenspaces model the interplay between (automorphic) inner and outer structures, as well as the vertical and horizontal symmetry.

### Composition of Filters

Composing a _residual connection_ with a graph filter \(\bm{G}=\operatorname{diag}(\hat{g}(\bm{\lambda}))\in\mathbb{R}^{n\times n}\) yields \(\bm{Y}=\bm{V}\bm{G}\bm{V}^{\top}\bm{H}+\bm{H}=\bm{V}(\bm{G}+\bm{I})\bm{V}^{\top} \bm{H}\), _chaining multiple filters_ (without nonlinearities) results in \(\bm{V}\bm{G}_{2}\bm{V}^{\top}\bm{V}\bm{G}_{1}\bm{V}^{\top}\bm{H}=\bm{V}\bm{G}_ {2}\bm{G}_{1}\bm{V}^{\top}\bm{H}\). Chaining and residual connections resolve to \(\bm{V}(\bm{G}_{2}\bm{G}_{1}+\bm{G}_{2}+\bm{G}_{1}+\bm{I})\bm{V}^{\top}\bm{H}\). Hence, an arbitrary sequence of graph filters (Eq. 2) can be more flexible due to the interactions between filters. Note that this composition is only true in the absence of nonlinearities. Nevertheless, the main intuition about how filters interact remains approximately the same also in the light of nonlinearities.

### Exhaustive Reasons Why Low Frequencies Are Sensible

A sensible default is to focus on the low frequencies. We specifically identify the following six reasons: (1) Low frequencies model the smoothest global signals w.r.t. the high-level graph structure (see Fig. 3 & 4). (2) Gama et al. (2020) find that, under a relative perturbation model (perturbation budget proportional to connectivity), stability implies \(C\)-integral-Lipschitzness (\(\exists C>0\colon\ |\lambda^{d\beta}\!/_{d\lambda}|\leq C\)), i.e., the filter can vary arbitrarily around zero but must level out towards larger \(\lambda\). Stability to graph perturbations is a strong domain-agnostic prior. (3) Many physical long-range interactions are power laws with a flattening frequency response. For example, we construct an explicit graph filter modeling the electric potential of charges in a 1D "ion crystal" (SS G) and find that a low-pass window is optimal. (4) Sequence models like Hyena (Poli et al., 2023) apply global low-pass filters through their exponential windows. (5) Cai et al. (2023) prove that an MPGNN plus virtual node (see SS E) can emulate DeepSets (Zaheer et al., 2017) and, thus, approximate self-attention to any precision. Nonetheless, we find that a virtual node alone does not necessarily yield good generalization (SS 3.1.1 & 4.1). (6) Nonlinearities "spill" features between frequency bands (Gama et al., 2020). This includes spillage from higher frequencies to the band of the spectral filter. Gama et al. (2020) argue that this spillage makes it possible to learn stable yet expressive graph filters and is also a feature of stable message passing models.

### Scaling to Graphs of Different Magnitude

For scaling a single to graphs of different orders of magnitude, it can be beneficial to rescale the eigenvalues before learning the filter \(\hat{g}_{\vartheta}(\bm{\lambda})\). That is, we use \(\hat{g}_{\vartheta}^{(l)}(\tilde{\bm{\lambda}})\) with rescaled \(\tilde{\bm{\lambda}}\).

For example, the eigenvalues for a path/sequence are \(\lambda_{j}\approx(1-\cos(\nicefrac{{\pi j}}{{n}}))\). Thus, the resolution is poor, especially for the eigenvalues close to zero since \(\cos\) approaches slope 0. For this reason, we consider rescaling the eigenvalues with

\[\tilde{\lambda}_{j}=\nicefrac{{1}}{{\pi}}\cos^{-1}(1-\lambda_{j})\] (32)

or

\[\tilde{\lambda}_{j}=\nicefrac{{n}}{{\pi}}\cos^{-1}(1-\lambda_{j})\] (33)

The latter is, e.g., convenient for identifying the second lowest eigenvalue regardless of \(n\). Due to the poor numerical properties of these relations, we evaluate \(\cos^{-1}(1-\lambda_{j})=\tan^{-1}(\nicefrac{{\sqrt{2\lambda_{j}-\lambda_{j} ^{2}}}}{{1-\lambda_{j}}})\) instead.

### Spectral Normalization

While the GFT and its inverse preserve the norm of the input (e.g., \(\|\hat{\bm{x}}\|_{2}=\|\bm{V}^{\top}\bm{x}\|_{2}=\|\bm{x}\|_{2}\)), this is not true if operating on a truncated frequency spectrum or if the filter \(\hat{g}_{\vartheta}(\bm{\lambda})\) suppresses certain frequencies. For example, in the example of a virtual node (for simplicity here with \(\bm{L}=\bm{D}-\bm{A}\)), a signal \(\bm{x}\) that is zero at every node but one at a single node, then the signal will be equally scattered to every frequency. Then, suppressing all frequencies but \(\lambda=0\), yields \(\|\bm{V}\bm{1}_{\{0\}}\bm{V}^{\top}\bm{x}\|_{2}=\nicefrac{{1}}{{\sqrt{n}}}\).

Motivated by this unfortunate scaling, we also consider normalization in the spectral domain. Specifically, we normalize \(\hat{\bm{H}}=\hat{g}_{\vartheta}(\bm{\lambda})\odot\left[\bm{V}^{\top}f_{ \vartheta}(\bm{H})\right]\in\mathbb{R}^{k\times d}\) s.t. \(\hat{\bm{H}}_{j}\leftarrow(1-a_{j})\hat{\bm{H}}_{j}+a_{j}\hat{\bm{H}}_{j} \nicefrac{{H}}{{\|\hat{H}_{j}\|_{2}}}\) with learnable \(\bm{a}\in[0,1]^{d}\). This allows, e.g., broadcasting a signal from one node without impacting its scale. However, we empirically find that this normalization only helps only marginally in the over-smoothing experiment (Di Giovanni et al., 2023) and otherwise can destabilize training. We also consider variants where the norm in the spectral domain is scaled with the norm of the signalin the spatial domain with more or less identical results. We hypothesize that such normalization is counter-productive for, e.g., a bandpass filter if the signal does not contain the corresponding frequencies.

### Adjusting S\({}^{2}\)GNNs to Directed Graphs

For the spectral filter of Eq. 3, we use \(f_{\theta}^{(l)}(\hat{\bm{H}}^{(l)})=\bm{H}^{(l)}\odot[\sigma(\bm{H}^{(l)}\bm{W} _{G,\Re}^{(l)})+i\cdot\sigma(\bm{H}^{(l)}\bm{W}_{G,\Im}^{(l)})]\) and subsequently map the result of \(\mathrm{Spectral}\) back the real domain, e.g., using \(\bm{w}_{\Re}^{(l)}\Re(\mathrm{Spectral}^{(l)}(\bm{H}^{(l-1)}))+\bm{w}_{\Im}^{ (l)}\Im(\mathrm{Spectral}^{(l)}(\bm{H}^{(l-1)}))\), with learnable weights \(\bm{w}_{\Re}^{(l)},\bm{w}_{\Im}^{(l)}\in\mathbb{R}^{d}\) and real \(\Re(\cdot)\) as well as imaginary component \(\Im(\cdot)\). For the positional encodings \(\mathrm{PE}(\bm{V},\bm{\lambda})\) of SS 3.2.4, we use \(\bm{A}_{s}\) in Eq. 5 and concatenate real as well as imaginary components. The neural network for the spectral domain \(s_{\zeta}\) of SS 3.2.2 generalizes without adjustment. Similar to Koke and Cremers (2024), one could also employ complex weights; however, we do not.

### Computational Remarks

We use readily available eigensolvers (scipy) and, thus, use a fixed number of eigenvectors (typically \(k\ll n\)) instead of determining \(k\) based on \(\lambda_{\mathrm{cut}}\). The partial eigendecomposition is of complexity \(\mathcal{O}(km)\) for \(m\) edges, while the spectral filter has complexity \(\mathcal{O}(kdn)\). On a different remark, we batch multiple graphs using block diagonal matrices (Fig. 17).

**Spectral graph-level readouts.** The key insight is that frequencies are a global concept, and hence, the GFT can be used for global readouts in graph-level tasks. With \(k\ll n\), such a readout is practically free in the presence of intermediate spectral layers and of \(\mathcal{O}(kn)\) otherwise. Thus, there is the opportunity for a computationally convenient aggregation of global information, including a sort of graph-level "jumping knowledge" (Xu et al., 2018). The only caveat is that the Fourier coefficients are not unique due to the ambiguity in the eigendecomposition. To maintain permutation equivariance, we take the absolute value and aggregate over dimension \(k\) in Eq. 4 instead of the multiplication with \(\bm{V}\). We observe that such intermediate readout can improve performance slightly, e.g., on TPUGraphs. However, we leave a systematic evaluation of its benefits for future work.

**Linear bottle necks.** To circumvent overfitting, we commonly replace the linear transformations \(\bm{W}\bm{X}\) in \(f_{\theta}^{(l)}(\hat{\bm{H}}^{(l)})\) and \(\hat{g}_{\theta}(\bm{\lambda})\) with low-rank bottlenecks \(\bm{W}_{2}\bm{W}_{1}\bm{X}\), s.t. \(\bm{W}\in\mathbb{R}^{d\times d}\), \(\bm{W}_{2}\in\mathbb{R}^{d\times d^{\prime}}\), \(\bm{W}_{1}\in\mathbb{R}^{d^{\prime}\times d}\), and \(d^{\prime}<d\).

## Appendix K Limitations

We expect that many common graph benchmarks do not have or only insignificant long-range interactions. We observe that MPGNNs are less likely to overfit, perhaps since locality is a good inductive bias in many circumstances (Bronstein et al., 2021). Moreover, we observe that the spectral filter (SS 3.2.1) may converge slowly and get stuck in local optima. We find that a sufficient amount of randomly initialized filters mitigates this issue to a large extent. Further, one can introduce inductive biases via windowing functions (Fig. 7), like the exponential window used by Hyena (Poli et al., 2023).

Even if the true causal model generating the target consists of long-range interactions, it might be sufficient to model the training data solely using (potentially spurious) local interactions. This might be especially true if the training nodes are samples from a "small" vicinity of the graph (e.g., OGB Products (Hu et al., 2020)).

Closely related to the previous point is the amount of available training data. We hypothesize that S\({}^{2}\)GNNs are more _data-hungry_ than their purely spatial counterpart. That is, to reliably detect (non-spurious) long-range interactions in the training data, a sufficient amount of data is required. Similar findings have been made, e.g., in the image domain (Dosovitskiy et al., 2021).

Figure 17: Block diagonal batching for spatial and spectral filters.

Except for heterophilic graphs, direction plays a small role in graph machine learning even though many benchmark tasks actually consist of directed graphs (Rossi et al., 2023). Moreover, there is a lack of benchmarks involving directed graphs, which require long-range interactions. Note that most of the theoretical findings generalize to directed graphs under appropriate modeling decisions/assumptions. However, we do not make this discussion explicit since MPGNNs for directed graphs are still actively researched.

Since a lot of the previous points hover around the insufficiency of the available benchmarks, we propose two new tasks SS 4.1 and derive further datasets, e.g., for associative recall.

While we demonstrate the practicality of S\({}^{2}\)GNNs in SS 4.3 on large-scale benchmarks, the partial eigendecomposition \(\mathrm{EVD}\) starts to become costly on the largest graphs we use for evaluation. Even though we did not experiment with lowering the requested precision, etc., we expect that for scaling further, naive approaches might not be sufficient. One direction could be to utilize GPUs instead of CPUs or to adapt concepts, e.g., from spectral clustering (von Luxburg, 2007).

Even though there are many important reasons why we should utilize a spectral filter on the low end of the spectrum, there might be tasks for which this choice is suboptimal. One way to estimate the frequency band to which one should apply a spectral filter is via a polynomial regression and then determine where the derivative is maximal. Note that it is efficient to calculate the eigenvectors around an arbitrary location of the spectrum, e.g., with the "shift-invert mode" of scipy/ARPACK(Lehoucq et al., 1998).

Due to the many possible design decisions of spectrally parametrized filters, the neglect of spectral filters in prior work, and the lack of appropriate benchmarks, it was not possible to ablate all the details. We expect that future work will discuss the specific building blocks in greater detail.

## Appendix L Broader Impact

We expect that S\({}^{2}\)GNNs will have similar societal implications as other model developments like Convolutional Neural Networks (CNNs) (LeCun et al., 1989), LSTMs (Hochreiter & Urgen Schmidhuber, 1997), transformers (Vaswani et al., 2017), or modern Graph Neural Networks (Gilmer et al., 2017). Since such models may be used as building blocks in architectures for predictive tasks, generative modeling, etc., they have a wide range of positive and negative implications. Nevertheless, we expect that S\({}^{2}\)GNNs will not have more negative implications than other machine learning model innovations.

## Appendix M Experimental Results

This section provides further details on the experimental setup (SS M.1), the computational cost (SS M.3), and graph constructions with additional experimental results for the clustering tasks (SS M.6); likewise we provide details for the distance regression (SS M.7), arXiv-year (SS M.8), and provide nodes on the graph construction in TPUGraphs (SS M.10). Note that the sections on clustering (SS M.6) and distance regression (SS M.7) also contain ablations and further insights.

### Experimental Details

**Implementation.** The code base is derived from Cao et al. (2023), which on the other hand derive the code of Rampasek et al. (2022). The implementation heavily relies on PyTorch geometric (Fey & Lenssen, 2019).

**Datasets.** We collect the main statistics, including licenses, for the datasets in Table 6. The provided code will download all datasets along with the experiment execution, except for TPUGraphs, where one should follow the official instructions. Due to the high variation in results, we merge all "layout" datasets and present the results on this joint dataset. We use the fixed public splits for all experiments and proceed accordingly for our datasets (see SS M.6 and SS M.7).

**Hyperparameters.** While we provide full parameters for all experiments and models in our code, we gather an overview of the used S\({}^{2}\)GNNs variants here. The parameters were determined through cascades of random search throughout the development of the method. We list the most important parameters in Table 7.

**Usage of external results.** The performance of baselines is commonly taken from leaderboards and the respective accompanying papers. This specifically includes the results in Table 1, Table 2, and Table 11.

**Setup.** For clustering (SS M.6), distance regression (SS M.7), and arXiv-year (SS M.8) we report the detailed setup in the respective sections. For the other tasks, the relevant details are:

* **Peptides:** We follow the setup and implementation of Rampasek et al. (2022). That is, we train for 250 epochs with a batch size of 200. We rerun experiments on 10 random seeds.
* **Over-squashing:** We derive the setup from Di Giovanni et al. (2023a). In the main part (Fig. 5), for the GCN, we report the numbers of their Figure 3 for a GCN on "Clique Path" graphs. For the spectral filter, we actually consider the more challenging setting where we do not train one model per graph size. Instead, we train one model for all sequence lengths. The task is to retrieve the correct of five possible classes on the other end of the graph. In the extended experiment of Fig. 12, we compose the dataset of "Clique Path" and "Ring" graphs (see Di Giovanni et al. (2023a)). To avoid \(m=\mathcal{O}(n^{2})\), we limit the fully connected clique to 15 nodes. For training and validation, we enumerate all graphs with even \(n\in\{4,6,\ldots,50\}\) and train for 500 epochs. For test, we enumerate the graphs with even \(n\in\{52,54,\ldots,100\}\). We rerun experiments on 10 random seeds.
* **Associative recall:** We construct one dataset consisting of key-value sequences of length 20 to 999. As Poli et al. (2023), we use a vocabulary of 30. We sample 25,000/500 random graphs for train/validation. For the test set, we randomly generate 500 graphs for the sequence lengths of 1,000 to 1,199. We train for 200 epochs. In the experiment with validation/test sequence length 30k (Table 2), we generate 10,000 training graphs of length 29,500 to 30,499 and finetune S\({}^{2}\)GNNsGCN from the smaller setup. We rerun experiments on 10 random seeds.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Dataset** & **\# 3D** & **\# 90-** & **\# spec. filters** & **\# eigenvectors \(k/\)** & **Spectral** & **Train** & \(\mathrm{EVD}\)** **time** & **GPU** & **Notes** \\  & **layers** & **layers** & **per layer** & **frequency cutoff\({}_{\mathrm{max}}\)** & **NN** & **time** & \(\mathrm{EVD}\)** **time** & **GPU** & **Notes** \\ \hline Peptide-Func & 3 & 3 & 224 & 128 & \(\lambda_{m}=0.7\) & ✗ & 1 h & 2 min & 1080/H \\ Peptide-Struct. Struct. Struct. & 3 & 1 & 260 & 200 & \(\lambda_{m}=0.7\) & ✗ & 1 h & 2 min & 1080/H \\ CLUSTER & 18 & 17 & 64 & 32 & \(\lambda_{m}=1.3\) & ✗ & 1.2 h & 4 min & 1080/H \\ NL- CLUSTER (ours) & 4 & 1 & 128 & 128 & \(\lambda=10.3\) & ✗ & 20 min & 4 min & 1080/H \\ Distance regression (ours) & 5 & 4 & 236 & \(k=50,\lambda_{m}=0.1\) & ✗ & 3 h & 1.5 h & 4100/H \\ Comparative & 0 & 1 & 16 & \(k=20,\lambda_{m}=0.05\) & ✗ & 3 min & 3 s & 1080/H \\  & & & & & & & & \\ Associatedine recall & 3 & 3 & 224 & 128 & \(k=10,\hat{\lambda}_{m}=10\) & ✓ & 3 h & closed form & 1080/H \\ AV-Voter & 4 & 2 & 256 & 256 & \(k=100,\lambda_{m}=0.05\) & ✓ & 1 h & 5 min & 1080/H \\
**Open Graph** & 6 & 2 & 256 & 164 & \(k=100\times\lambda_{m}=0.056\) & ✓ & 11 h & 26 min & A100/H \\ Benchmark Products & 1 & 128 & 64 & \(k=100,\lambda_{m}=0.05\) & ✓ & 40 h & 4 h & A100/H \\ \hline \hline \end{tabular}
\end{table}
Table 7: Important S\({}^{2}\)GNNs specific hyperparameters and runtimes. The times for the \(\mathrm{EVD}\) cover the respective dataset entirely.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Name** & **\# of graphs** & **Average \# of nodes** & **Average \# of edges** & **Task** & **License** \\ \hline Peptides func (Dwivedi et al., 2022) & 15,535 & 150.9 & 307.3 & graph multi-label classification & CC BY-NC 4.0 \\ Peptides extract (Dwivedi et al., 2022) & 15,535 & 150.9 & 307.3 & graph regression & CC BY-NC 4.0 \\ CLUSTER (Dwivedi et al., 2023) & 12,000 & 117.2 & 4,301.7 & node classification & CC BY 4.0 \\ LR-CLUSTER (ours) & 12,000 & 896.9 & 6,195.1 & node classification & CC-BY 4.0 \\ Tree Distance regression (ours) & 55,000 & 749.2 & 748.2 & node regression & CC-BY 4.0 \\ DAG Distance regression (ours) & 55,000 & 748.6 & 821.8 & node regression & CC-BY 4.0 \\ Oversquashing extended & 730 & 43.8 & 231.9 & node classification & CC-BY 4.0 \\ (derived from Di Giovanni et al., 2023a) & & & & & \\ Associative recall small & 26,000 & 524.7 & 523.7 & node classification & CC-BY 4.0 \\ (derived from Poli et al., 2023) & & & & & & \\ Associative recall 30k & 11,000 & 30,003.8 & 30,002.8 & node classification & CC-BY 4.0 \\ (derived from Poli et al., 2023) & & & & & & \\ OGB arXiv (Hu et al., 2020) & & 1 & 169,343 & 1,166,243 & node classification & MIT \\ OGB Products (Hu et al., 2020) & & 1 & 2,449,029 & 61,859,140 & node classification & MIT \\ TPUGraphs (Phouldingham et al., 2023) & \(\approx\)31,000,000 & \(\approx\)6,100 & NA & graph ranking & Apache License \\ \hline \hline \end{tabular}
\end{table}
Table 6: Dataset statistics and licenses.

* **OGB Products:** Even though full-graph training with 3 layers GCN plus one spectral layer fits into a 40 GB A100 GPU, we find that batched training works better. We randomly divide the graph during training into 16 parts and train a 6-layer S\({}^{2}\)GAT with spectral layers after the second and last message passing step. Inference is performed on the entire graph at once. We rerun experiments on 5 random seeds.
* **TPUGraphs:** This is the only dataset where we use a transformer to model \(\hat{g}\) instead of the procedure detailed in SS 3.2.1. We fix the number of eigenvectors to \(k=100\) and do not apply any windowing. Due to the large variation of results, we merge all "layout" tasks into a single dataset. Since the default graph construction is not able to express all relevant information, we adapt it as detailed in SS M.10, however, the empirical impact was small. TPUGraphs "layout" consists of a few hundred distinct graph structures with a large variation on the node-level configuration/features. We sample 10,000 configurations for each graph structure of each "layout" sub-split. Here, we introduce two batch dimensions: (1) batching over multiple graphs and (2) batching over the configurations. In each training step of the 1,000 epochs, we sample a small subset of configurations per graph structure and apply a pairwise hinge loss to rank the configurations. We do not perform random reruns due to the computational cost.

### Qualitative Experiments

In Fig. 6 and Fig. 8, we provide qualitative insights about the approximation of filters and ringing.

In Fig. 6, we construct a true filter by adding a discontinuous filter at \(\lambda=0\) and a polynomial filter of order 3. For the spectral part, we use the true filter values and fit a Chebyshev polynomial on the remaining part. We then plot the response of the true filter and its approximations on a path graph with 21 nodes and \(\bm{L}=\bm{I}-\bm{D}^{-1}\bm{A}\).

Similarly, in Fig. 8, we use a path graph with 100 nodes and \(\bm{L}=\bm{I}-\bm{D}^{-1}\bm{A}\). We then construct a perfect low pass (\(k=25\)) and approximate a rectangular wave.

### Computational Cost

We report the computational cost for the experiments in Table 7 for a single random seed. On top of the pure cost of reproducing our numbers, we conducted hyperparameter searches using random search. Partially, we required 100s of runs to determine good parameter ranges. A generally well-working approach was first to reproduce the results of the best available MPGNN in prior work. Thereafter, we needed to assess how likely additional capacity would lead to overfitting. Usually, we reduced the number of message-passing steps, added the spectral filter, and determined appropriate values for the number of eigenvectors \(k\). In the light of overfitting, it is a good idea to lower the number of Gaussians in the smearing of the filter parametrization (SS 3.2.1), introduce bottle-neck layers (SS J), and use fewer spectral filters than hidden dimensions.

**Runtime with precalculated eigenvectors.** In Fig. 18, we contrast the runtime cost of a spectral convolution with spatial messages passing on ogb-arXiv (170k nodes) of Hu et al. (2020), using an Nvidia GTX 1080Ti. This essentially compares a sparse matrix multiplication (adjacency matrix)

Figure 18: Runtime comparison on arXiv (w/o EVD) using \(k=2,500\) for the spectral filter.

with matrix multiplications on dense "tall and skinny" matrices (GFT). we find that one GCN-layer here is as costly as a spectral filter with approx. \(k=2,500\) eigenvectors.

**Large-scale benchmarks.** On the large-scale datasets OGB Products and TPUGraphs, we perform full-graph training (without, e.g., segment training (Cao et al., 2023)) using 3 DirGCN layers interspersed with spectral filters targeting a pair-wise hinge loss. The spectral GNN uses the Magnetic Laplacian to incorporate direction. The spatial MPGNN closely resembles the model of Rossi et al. (2023), except that we half the dimension for the forward and backward message passing and concatenate the result. We shrink the dimensions to model the direction at a very low cost. We conclude that S\({}^{2}\)GNNs can be very practical even if applied at scale and can effectively model long-range interactions also on large graphs.

**Eigendecompositon.** We show the computational cost for the eigendecomposition of a random Erdos Renyi graph (every edge has equal likelihood to be drawn). We use scipy (CPU) and PyTorch (GPU) with default arguments. For the sparse decomposition with PyTorch, we use the svd_lowrank method. Note that the default parameters for PyTorch are usually leading to large numerical errors. Fig. 19 demonstrates that the cost of the eigendecomposition is manageable. For large graphs like ogbn-products (2.5 mio. nodes), the EVD takes around 30 minutes with \(k=100\) on 6 CPU cores of an AMD EPYC 7542. Note that the default parameters of the eigensolver allow for 1000s of iterations or until the error in the 32-bit float representation achieves machine precision.

### S\({}^{2}\)GNN Aggregation Ablation

In the main body we present two ways to combine a spatial and spectral filter: An _additive combination_ (Eq. 1) and an arbitrary sequence of filters (Eq. 2). In this section, we perform an ablation analysis on the peptides-func benchmark and report the results in Table 8.

Instead of summation of the spatial and spectral parts, **concatenation** is another possible option. Getting input features \(\bm{H}^{(l-1)}\in\mathbb{R}^{n\times d^{(l-1)}}\), we design \(\mathrm{Spectral}^{(l)}\) and \(\mathrm{Spatial}^{(l)}\) to map to \(\mathbb{R}^{n\times d^{(l)}/2}\) and update the embeddings as

\[\bm{H}^{(l)}\,=\,\mathrm{Spectral}^{(l)}(\bm{H}^{(l-1)};\bm{V},\bm{\lambda}) \,||\,\,\mathrm{Spatial}^{(l)}(\bm{H}^{(l-1)};\bm{A}).\] (34)

Additionally, we consider **normalization** of the addends at the end of each embedding update, dividing by \(1/\sqrt{2}\) (concatenation w/ residual) and \(1/\sqrt{3}\) (summation w/ residual) as an attempt to keep the variance constant.

Inspired by recent advancements in state space models like Mamba (Gu & Dao, 2023), we also consider modeling an update step in a similar way, identifying the convolutional part with \(\mathrm{Spatial}^{(l)}\) and the SSM part with \(\mathrm{Spectral}^{(l)}\).

The following table shows results for the different design choices to combine the spatial and spectral parts, with all hyperparameters being precisely the ones reported in Table 7 for peptides-func.

Figure 19: Runtime of partial eigendecomposition \(k=25\) of Erdős Rényi graph with average degree 5. Dashed mark directed/Hermitian Laplacian.

[MISSING_PAGE_FAIL:43]

Nevertheless, we find that the spectral filter is well aligned with the cluster structure in these tasks. We plot this some exemplary filter in Fig. 23. The findings match the explanations of SS 4.1 also for CLUSTER.

In the remainder of the section, we provide full details of the experiment setups. Moreover, we provide additional results not presented in the main text, including ablations.

#### d.6.1 GMM Clustering LR-cluster

**Setup.** To sample an input graph, we start by generating \(C=6\)\(p\)-dimensional cluster centers \(\mu_{c}\sim U[0,10]^{p}\) for \(c\in\{0,\dots,C-1\}\) (we use \(p=2\)). Next, we draw \(n_{c}\in\{100,\dots,199\}\) points \(x_{ic}\sim\mathcal{N}(\mu_{c},4I_{p})\) which will represent the nodes of the graph. Subsequently, we update the class memberships such that every point is in its most likely class according to the underlying probabilistic model. Finally, we connect each node \(v\) to its \(e_{v}\sim U(\{1,\dots,10\})\) closest neighbors by Euclidean distance \(\|\cdot\|_{2}\). This whole procedure is repeated until the generated graph is connected. We then discard the location information and only keep the graph structure. In this way, we generate graphs of an average diameter of \(\approx 33\). See Fig. 21 for depictions of example graphs.

Apart from the graph generation procedure, we adhere closely to Dwivedi et al. (2023): We introduce input features in \(\{0,1,2,\dots,C\}\), where a feature value of \(c=1,\dots,C\) corresponds to the node

Figure 23: SBM-based (a), visualized in Fig. 21a, and _our_ GMM-based (b), visualized in Fig. 22a, graphs along with four learned filters. Large entries are yellow, small are blue, and white lines denote clusters.

Figure 22: Examples of generated graphs for the CLUSTER task (SBM). Labeled nodes are marked red. Edges within clusters are highlighted.

being in class \(c-1\) and a feature value of \(0\) means that the class is unknown and has to be inferred by the model. Only one node \(v_{c}\) per class is randomly chosen to be labeled and all remaining node features are set to \(0\). The output labels are defined as the class labels. We use weighted cross entropy loss for training and class-size-weighted accuracy as a target metric. We generate 10,000 training and 1,000 val/test graphs each and report the average \(\pm\) standard deviation over 3 random reruns.

**Models.** As an underlying spatial model baseline, we use a vanilla GCN (Kipf & Welling, 2017). We compare this to S\({}^{2}\)GCN, only applying one spectral convolution immediately before the last spatial layer. We investigate the influence of the number \(k\in\{0,1,\dots,10\}\) of eigenvectors to be taken into account with 4 spatial layers, with \(k=0\) indicating the absence of a spectral layer (see Fig. 9(a), and Table 9 for the underlying data). We also vary the number of spatial MP layers from 2 to 10 and compare the performance of a purely spatial GCN to the corresponding S\({}^{2}\)GCN with one spectral convolution (see Fig. 9(b), and Table 10 for the underlying data).

Throughout all evaluations, we maintain a consistent hyperparameter configuration: Specifically, we use an inner dimension of 128, GELU (Hendrycks & Gimpel, 2016) as an activation function, no dropout, and residual connections for all spatial and spectral layers. For the spectral layer, we implement the gating mechanism \(f_{\theta}^{(l)}\), but abstain from a neural network in the spectral domain (SS 3.2.2), bottlenecks, or parameter sharing. We train for \(50\) epochs with a batch size of \(50\), using the AdamW optimizer (Loshchilov & Hutter, 2019) with a base learning rate of \(0.003\), a weight decay of \(0.0001\), a cosine scheduler and \(5\) warmup epochs.

**Further discussion.** The clustering task comes naturally to S\({}^{2}\)GCN, as a spectral layer can simulate certain variations of spectral clustering (von Luxburg, 2007): Suppose \(\bm{H}^{(l-1)}\in\mathbb{R}^{n\times C}\) is a one-hot encoding of the cluster labels, i.e. \(\bm{H}^{(l-1)}_{v,c}=\delta_{v,v_{c}}\), with \(c\in\{1,\dots,C\}\) and \(v_{c}\) being the unique labeled node per class. In its simplest form, taking \(\hat{g}_{\theta}^{(l)}(\bm{\lambda})\equiv 1\) and \(f_{\theta}^{(l)}\equiv\mathrm{id}\), the spectral layer \(\mathrm{Spectral}^{(l)}\) from Eq. 3 turns into \(\bm{H}^{(l)}=\bm{V}\bm{V}^{\top}\bm{H}^{(l-1)}\). Hence, \(\bm{H}^{(l)}_{v,c}=\bm{V}^{\top}_{v,:}\bm{V}_{v_{c}:}\) encodes a notion of similarity between a node \(v\) and each labeled node \(v_{c}\). This relates to the Euclidean distance \(\|\bm{V}_{v,:}-\bm{V}_{v_{c},:}\|_{2}=\sqrt{\|\bm{V}_{v,:}\|_{2}^{2}+\|\bm{V}_ {v_{c},:}\|_{2}^{2}-2\bm{V}^{\top}_{v,:}\bm{V}_{v_{c},:}}\) which is more typically used for spectral clustering.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline  & \multicolumn{2}{c}{**2**} & \multicolumn{1}{c}{**3**} & \multicolumn{1}{c}{**4**} & \multicolumn{1}{c}{**5**} \\ \hline GCN & \(0.2700\pm 0.0002\) & \(0.3557\pm 0.0000\) & \(0.4544\pm 0.0003\) & \(0.5521\pm 0.0001\) \\ GCN (\(+\operatorname{PE}\)) & \(0.2684\pm 0.0005\) & \(0.3552\pm 0.0015\) & \(0.4550\pm 0.0004\) & \(0.5526\pm 0.0006\) \\ \hline S\({}^{2}\)GCN & \(0.8517\pm 0.0003\) & \(0.8520\pm 0.0008\) & \(0.8518\pm 0.0005\) & \(0.8512\pm 0.0002\) \\ S\({}^{2}\)GCN (\(+\operatorname{PE}\)) & \(0.8547\pm 0.0007\) & \(0.8550\pm 0.0010\) & \(0.8552\pm 0.0015\) & \(0.8539\pm 0.0010\) \\ \hline \hline
**6** & **7** & **8** & **9** & **10** \\ \hline \(0.6367\pm 0.0001\) & \(0.7013\pm 0.0001\) & \(0.7448\pm 0.0003\) & \(0.7708\pm 0.0007\) & \(0.7860\pm 0.0004\) \\ \(0.6387\pm 0.0012\) & \(0.7104\pm 0.0011\) & \(0.7609\pm 0.0009\) & \(0.7931\pm 0.0005\) & \(0.8135\pm 0.0007\) \\ \hline \(0.8512\pm 0.0008\) & \(0.8509\pm 0.0008\) & \(0.8511\pm 0.0003\) & \(0.8504\pm 0.0009\) & \(0.8509\pm 0.0006\) \\ \(0.8552\pm 0.0008\) & \(0.8542\pm 0.0004\) & \(0.8545\pm 0.0008\) & \(0.8536\pm 0.0013\) & \(0.8542\pm 0.0008\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Accuracy on the GMM clustering task for varying number of MP layers, while comparing a purely spatial GCN model to S\({}^{2}\)GCN with one spectral layer added in the end.

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline \(k\) & **0 (MPCNN)** & **1 (Virtual Node)** & **2** & **3** & **4** \\ \hline S\({}^{2}\)GCN & \(0.4564\pm 0.0002\) & \(0.4646\pm 0.0001\) & \(0.6786\pm 0.0010\) & \(0.7429\pm 0.0026\) & \(0.7971\pm 0.0008\) \\ S\({}^{2}\)GCN (\(+\operatorname{PE}\)) & \(0.4564\pm 0.0002\) & \(0.4642\pm 0.0007\) & \(0.7221\pm 0.0008\) & \(0.7860\pm 0.0005\) & \(0.8202\pm 0.0011\) \\ \hline \hline
**5** & **6** & **7** & **8** & **9** & **10** \\ \hline \(0.8322\pm 0.0004\) & \(0.8511\pm 0.0008\) & \(0.8510\pm 0.0008\) & \(0.8519\pm 0.0005\) & \(0.8517\pm 0.0006\) & \(0.8513\pm 0.0018\) \\ \(0.8440\pm 0.0006\) & \(0.8538\pm 0.0012\) & \(0.8548\pm 0.0011\) & \(0.8546\pm 0.0002\) & \(0.8545\pm 0.0005\) & \(0.8554\pm 0.0005\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Accuracy on the GMM clustering task for varying number of eigenvectors \(k\), using 4 GCN layers and one spectral layer in the end.

#### d.6.2 SBM Clustering CLUSTER (Dwivedi et al., 2023)

**Setup.** We conduct an ablation study on the original CLUSTER task (Dwivedi et al., 2023), which uses a similar setup to our GMM clustering task, however drawing from a SBM instead: For each cluster, \(n_{c}\in\{5,\ldots,35\}\) nodes are sampled. Nodes in the same community are connected with a probability of \(p=0.55\), while nodes in different communities are connected with a probability of \(q=0.25\). While there is no need for long-range interactions in this task, considering that the average diameter of the graphs is just \(\approx 2.17\), separating the clusters is much harder than in the GMM clustering task (see Fig. 23 for example adjacency matrices from the SBM and GMM models). We use weighted cross entropy loss for training and class-size-weighted accuracy as a target metric. We report the average \(\pm\) standard deviation over 3 random reruns.

**Models.** In our ablation study, we consider GCN (Kipf and Welling, 2017), GAT (Velickovic et al., 2018), and GatedGCN (Bresson and Laurent, 2018) as MPGNN baselines, following a setup similar to Dwivedi et al. (2023). We consider models with 4 layers (roughly 100k parameters) and 16 layers (roughly 500k parameters), while keeping most hyperparameters the same as in the benchmark, including inner dimension, dropout, and the number of heads for GAT. However, our reported baseline results and parameter counts differ slightly as we are using a different post-MP head, where we maintain a constant dimension until the last layer, in contrast to Dwivedi et al. (2023) who progressively shrink the inner dimension. We construct the corresponding S\({}^{2}\)GNNs by modifying each baseline model, replacing the \(3^{\mathrm{rd}}\) and the \(5^{\mathrm{th}}/15^{\mathrm{th}}\) layers with spectral layers, ensuring a roughly equivalent parameter count. Additionally, each model is optionally supplemented by our positional encodings \(\mathrm{PE}\) (SS 3.2.4).

We further conduct a hyperparameter search on the most promising base MPGNN candidate, GatedGCN, which leads to an optimized version of S\({}^{2}\)GNN. This optimized model has 18 spatial MPGNN layers, spectral layers between all spatial layers, and additional RWSE encodings. The inner dimension is adjusted to keep the model well below a parameter budget of 500k. Finally, we also evaluate S\({}^{2}\)GCN and S\({}^{2}\)GAT using these hyperparameter settings.

Throughout all evaluations, we use GELU (Hendrycks and Gimpel, 2016) as an activation function, residual connections for all spatial and spectral layers, and implement the gating mechanism \(f^{(l)}_{\theta}\) without employing a neural network in the spectral domain. We use a batch size of 128 for training the 4-layer models and 64 for all other models. For the spectral layer, we use the partial eigendecomposition corresponding to the lowest \(k=50\) eigenvalues (\(k=100\) for the optimized S\({}^{2}\)GNN versions), spectral normalization, and \(\lambda_{\text{cut}}=1.3\). For the optimized models, we employ parameter sharing with 128 heads, and a bottleneck of 0.25 in feature gating. We use 8 attention heads for all GAT versions in accordance with Dwivedi et al. (2023) (in

\begin{table}
\begin{tabular}{l l l} \hline \hline  & **Model** & **Accuracy (\(\uparrow\))** \\ \hline \multirow{6}{*}{**SBM**} & ARGNP Cai et al. (2022) & \(0.7735\pm 0.0005\) \\  & GPS Rampásek et al. (2022) & \(0.7802\pm 0.0018\) \\  & ITGI Choi et al. (2024) & \(0.7803\pm 0.0022\) \\  & GPTTrans-Nano Chen et al. (2023) & \(0.7807\pm 0.0015\) \\  & Exphormer (Shirzad et al., 2023) & \(0.7807\pm 0.0004\) \\  & EGT Hussain et al. (2022) & \(0.7923\pm 0.0035\) \\  & GRIT Ma et al. (2023) & \(0.8003\pm 0.0028\) \\ \hline \multirow{2}{*}{**SBM**} & GatedGCN & \(0.7608\pm 0.0020\) \\  & \(S^{2}\)_GatedGCN (**ours**)_ & \(0.7808\pm 0.0005\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Results on the CLUSTER task (Dwivedi et al., 2023). Transformer models that outperform our S\({}^{2}\)GatedGCN are underlined.

Figure 24: Effects of the spectral part on SBM clustering performance for different base architectures.

ner dimension is not expanded but split up), except for the optimized version, which uses 4 heads. For the purely spatial models, we use \(p=0.0\) as dropout (similar to Dwivedi et al. (2023)). We observe this to lead to overfitting for models with spectral layers, for which we set \(p\in\{0.1,0.2\}\). Hyperparameters differing between the compared models are listed in Table 12. We train for 100 epochs using the AdamW optimizer (Loshchilov and Hutter, 2019) with a base learning rate of 0.001, no weight decay, and a cosine scheduler with 5 warmup epochs.

**Results.** Results for the CLUSTER task are presented in Table 12, Table 11 and Fig. 24. Introducing a spectral layer significantly enhances performance on the 4-layer architectures, both with and without positional encodings. The effect is most pronounced on GCN, where replacing just a single GCN layer by a spectral layer boosts accuracy from \(0.504\) to \(0.655\). Notably, introducing two spectral layers still has a consistent positive effect on all 16-layer architectures.

### Distance Regression

**Setup.** We generate directed random trees with one source by sampling trees with \(n\in\{500,\ldots,999\}\) nodes, picking one node at random to declare as a source and introducing edge directions accordingly. To construct random DAGs with long distances, we start from such directed random trees and proceed by adding \(\lfloor n/10\rfloor\) edges at random, choosing each edge direction such that the resulting graph is still a DAG. Additionally, we mark the source node with a node feature. Besides evaluating all models in an in-distribution regime, we also assess the generalization power of the methods by drawing out-of-distribution val/test splits from slightly larger graphs of \(n\in\{1000,\ldots,1099\}\) and \(n\in\{1100,\ldots,1199\}\) nodes each. We use \(L^{2}\) loss for training and \(R^{2}\) as a target metric. We sample 50,000 training and 2,500 val/test graphs each and report the average \(\pm\) standard deviation over 3 random reruns.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline MPGNN & \# total & Inner & Spec. & Pos. & Dropout & \# params & Train accuracy (\(\uparrow\)) & Test accuracy (\(\uparrow\)) \\ base & layers & dim. & filters & enc. & & & & \\ \hline \multirow{6}{*}{\begin{tabular}{c} \end{tabular} } & \multirow{4}{*}{4} & \multirow{4}{*}{146} & ✗ & ✗ & 0.0 & 109k & \(0.5059\pm 0.0018\) & \(0.5037\pm 0.0023\) \\  & & & ✗ & ✓ & 0.0 & 117k & \(0.5053\pm 0.0010\) & \(0.5026\pm 0.0006\) \\  & & & 1 & ✗ & 0.1 & 117k & \(0.6492\pm 0.0009\) & \(0.6545\pm 0.0013\) \\  & & & 1 & ✓ & 0.1 & 125k & \(0.6663\pm 0.0020\) & \(0.6640\pm 0.0021\) \\ \cline{2-10}  & & & ✗ & ✗ & 0.0 & 508k & \(0.7354\pm 0.0009\) & \(0.7190\pm 0.0010\) \\  & & & ✗ & ✓ & 0.0 & 517k & \(0.7378\pm 0.0017\) & \(0.7194\pm 0.0010\) \\  & & & 2 & ✗ & 0.1 & 527k & \(0.7535\pm 0.0011\) & \(0.7359\pm 0.0017\) \\  & & & 2 & ✓ & 0.1 & 536k & \(0.7526\pm 0.0021\) & \(0.7269\pm 0.0011\) \\ \cline{2-10}  & & 18 & 124 & 17 & PE+RWSE & 0.2 & 491k & \(0.8022\pm 0.0147\) & \(0.7711\pm 0.0020\) \\ \hline \multirow{6}{*}{\begin{tabular}{c} \end{tabular} } & \multirow{4}{*}{4} & \multirow{4}{*}{152} & ✗ & 0.0 & 120k & \(0.6705\pm 0.0008\) & \(0.6525\pm 0.0010\) \\  & & & ✗ & ✓ & 0.0 & 128k & \(0.7167\pm 0.0001\) & \(0.6680\pm 0.0020\) \\  & & & 1 & ✗ & 0.1 & 128k & \(0.7093\pm 0.0007\) & \(0.6960\pm 0.0010\) \\  & & & 1 & ✓ & 0.1 & 136k & \(0.7398\pm 0.0006\) & \(0.7065\pm 0.0007\) \\ \cline{2-10}  & & & ✗ & ✗ & 0.0 & 541k & \(0.8537\pm 0.0025\) & \(0.7126\pm 0.0014\) \\  & & & & ✓ & 0.0 & 549k & \(0.8740\pm 0.0014\) & \(0.7139\pm 0.0022\) \\  & & & 2 & ✗ & 0.1 & 558k & \(0.8723\pm 0.0013\) & \(0.7277\pm 0.0005\) \\  & & & 2 & ✓ & 0.1 & 567k & \(0.8836\pm 0.0005\) & \(0.7232\pm 0.0010\) \\ \cline{2-10}  & & 18 & 120 & 17 & PE+RWSE & 0.1 & 469k & \(0.8071\pm 0.0262\) & \(0.7681\pm 0.0003\) \\ \hline \multirow{6}{*}{
\begin{tabular}{c} \end{tabular} } & \multirow{4}{*}{4} & \multirow{4}{*}{70} & ✗ & 0.0 & 106k & \(0.6181\pm 0.0020\) & \(0.6039\pm 0.0019\) \\  & & & ✗ & ✓ & 0.0 & 110k & \(0.7292\pm 0.0031\) & \(0.6889\pm 0.0027\) \\ \cline{1-1}  & & & 1 & ✗ & 0.1 & 90k & \(0.6933\pm 0.0003\) & \(0.7050\pm 0.0001\) \\ \cline{1-1}  & & & 1 & ✓ & 0.1 & 94k & \(0.7245\pm 0.0002\) & \(0.7217\pm 0.0018\) \\ \cline{1-1} \cline{2-10}  & & & ✗ & ✗ & 0.0 & 505k & \(0.8667\pm 0.0019\) & \(0.7369\pm 0.0011\) \\ \cline{1-1}  & & & ✗ & ✓ & 0.0 & 509k & \(0.8753\pm 0.0257\) & \(0.7314\pm 0.0058\) \\ \cline{1-1} \cline{2-10}  & & & 2 & ✗ & 0.1 & 464k & \(0.8086\pm 0.0016\) & \(0.7627\pm 0.0010\) \\ \cline{1-1} \cline{2-10}  & & & 2 & ✓ & 0.1 & 468k & \(0.8302\pm 0.0011\) & \(0.7659\pm 0.0003\) \\ \cline{1-1} \cline{2-10}  & & 18 & 64 & 17 & PE+RWSE & 0.2 & 460k & \(0.8202\pm 0.0024\) & \(\mathbf{0.7808\pm 0.0005}\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Ablation results on the SBM clustering task (Dwivedi et al., 2023). The best mean test accuracy is bold, second is underlined.

Models.** As a MPGNN baseline, we use a five-layer directed version of GCN, DirGCN (Rossi et al., 2023), with three post-message-passing layers, and concatenating instead of averaging over the source-to-target and target-to-source parts. We compare these baselines to S\({}^{2}\)DirGCN of the form Eq. 2 with four spectral layers, alternating spatial and spectral convolutions and employing residual connections. We benchmark versions of S\({}^{2}\)DirGCN that ignore edge direction in the spectral convolution against directed versions in which we set \(q=0.001\). In all cases, we use the partial eigendecomposition corresponding to the \(k=50\) lowest eigenvalues. All models are optionally enriched by the positional encodings from SS 3.2.4. Throughout all evaluations, we use an inner dimension of 236, GELU (Hendrycks & Gimpel, 2016) as an activation function, and dropout \(p=0.05\). For the spectral layers, we utilize the gating mechanism \(f_{\theta}^{(l)}\), not employing a neural network in the spectral domain, we use spectral normalization, \(\lambda_{\text{cut}}=0.1\), and a bottleneck of \(0.03\) in the spectral layer. We train for 50 epochs, using a batch size of 36 and the AdamW optimizer (Loshchilov & Hutter, 2019) with a base learning rate of 0.001, a weight decay of 0.008, and a cosine scheduler with 5 warmup epochs.

Results.In Table 13, we show the performance of the different models on DAGs and trees. We observe that the simple MPGNNs are notably surpassed by all versions of S\({}^{2}\)DirGCN. While S\({}^{2}\)DirGCN achieves nearly perfect predictions on the tree tasks in both the directed and undirected case, the undirected version is outperformed by the directed version on the DAG tasks. Here, performance also reduces slightly in the out-of-distribution regime. The great performance on the tree

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & +Spec. & PE & \multicolumn{2}{c}{**in-distribution**} & \multicolumn{2}{c}{**out-of-distribution**} \\  & filter & \(MAE\) (\(\mathrm{\SIUnitSymbolMicro}\)) & \(RMSE\) (\(\mathrm{\SIUnitSymbolMicro}\)) & \(R^{2}\) (\(\mathrm{\SIUnitSymbolMicro}\)) & \(MAE\) (\(\mathrm{\SIUnitSymbolMicro}\)) & \(RMSE\) (\(\mathrm{\SIUnitSymbolMicro}\)) & \(R^{2}\) (\(\mathrm{\SIUnitSymbolMicro}\)) \\ \hline \multirow{7}{*}{**Baselines**} & ✗ & \(7.0283\pm 0.0033\) & \(9.9650\pm 0.0005\) & \(0.1915\pm 0.0001\) & \(8.1381\pm 0.0368\) & \(10.7735\pm 0.0102\) & \(0.1214\pm 0.0066\) \\  & ✗ & \(6.8252\pm 0.0008\) & \(8.8636\pm 0.0024\) & \(0.2322\pm 0.0004\) & \(8.0018\pm 0.0018\) & \(0.4432\pm 0.0017\) & \(0.1745\pm 0.0003\) \\  & under. & ✗ & \(1.9248\pm 0.0116\) & \(3.2687\pm 0.0100\) & \(0.8965\pm 0.0006\) & \(3.0471\pm 0.0192\) & \(4.9467\pm 0.0263\) & \(0.8148\pm 0.0020\) \\  &ault. & ✓ & \(1.7834\pm 0.0039\) & \(2.5934\pm 0.0046\) & \(0.9124\pm 0.0003\) & \(2.7950\pm 0.0041\) & \(4.5834\pm 0.0117\) & \(0.8410\pm 0.0008\) \\  & direc. & ✗ & \(1.2401\pm 0.0173\) & \(2.1603\pm 0.0340\) & \(0.9544\pm 0.0014\) & \(2.1824\pm 0.0787\) & \(3.7604\pm 0.0170\) & \(0.8924\pm 0.0040\) \\  & direc. & ✓ & \(1.1676\pm 0.0032\) & \(2.0428\pm 0.0066\) & \(0.9529\pm 0.0003\) & \(2.0565\pm 0.0326\) & \(3.5877\pm 0.0143\) & \(0.9025\pm 0.0024\) \\ \hline \multirow{7}{*}{**Baselines**} & ✗ & \(13.4717\pm 0.0478\) & \(17.3502\pm 0.0277\) & \(0.0958\pm 0.0292\) & \(16.554\pm 0.0559\) & \(21.645\pm 0.1394\) & \(0.0141\pm 0.0127\) \\  & ✗ & ✓ & \(11.6316\pm 0.0370\) & \(10.5123\pm 0.0249\) & \(0.3262\pm 0.0022\) & \(11.9847\pm 0.0501\) & \(10.3659\pm 0.05610\) & \(0.2110\pm 0.0030\) \\  & under. & ✗ & \(1.0268\pm 0.0408\) & \(1.7951\pm 0.1956\) & \(0.9092\pm 0.0020\) & \(1.5981\pm 0.2211\) & \(2.7377\pm 0.4786\) & \(0.9839\pm 0.0033\) \\  & under. & ✓ & \(1.2287\pm 0.1195\) & \(2.0065\pm 0.2638\) & \(0.9878\pm 0.0031\) & \(1.7184\pm 0.3288\) & \(2.5791\pm 0.5372\) & \(0.9856\pm 0.0055\) \\  & direc. & ✗ & \(0.8166\pm 0.5012\) & \(1.2224\pm 0.7600\) & \(0.9944\pm 0.0060\) & \(1.5280\pm 0.4539\) & \(2.2942\pm 0.7592\) & \(0.9881\pm 0.0059\) \\  & direc. & ✓ & \(0.7674\pm 0.3306\) & \(1.1512\pm 0.5839\) & \(0.954\pm 0.0041\) & \(0.9911\pm 0.6911\) & \(1.5064\pm 1.0206\) & \(0.9938\pm 0.0077\) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Results on the distance task, with DirGCN as base. The best mean score is bold, second is underlined.

Figure 25: Test accuracy curves for the SBM clustering task. Curves are shown for the models from Table 12 with \(\mathrm{PE}\), with the MPGNN baseline and the respective S\({}^{2}\)GNN.

task is due to the fact that trees are _collision-free_ graphs (Geisler et al., 2023), where the phase of each eigenvector is \(\exp(i2\pi q(d_{v}+c))\) for each node \(v\), with \(d_{v}\) representing the distance to the source node and \(c\in\mathbb{R}\) being an arbitrary constant (due to phase invariance of the eigenvector). It is noteworthy that a simple MPGNN with positional encodings, despite having the distances (shifted by \(c\)) readily available, fails the task, as the information about the phase of the source node cannot be effectively shared among all nodes. In Fig. 26, we compare the distance predictions by the different models. While the prediction of all models is close to perfect below a distance of 5, the spatial MPGNNs are almost unable to distinguish higher distances. By contrast, S\({}^{2}\)DirGCN predicts reasonable distances regardless of the ground truth, with the absolute error only increasing slowly.

### Heterophilic arXiv-year (Lim et al., 2021)

**Setup.** We evaluate S\({}^{2}\)GNN on a large-scale heterophilic dataset, namely _arXiv-year_. _arXiv-year_ is based on OGB arXiv (Hu et al., 2020), but instead of paper subject areas, the year of publication (divided into 5 classes) is the prediction target. While there are no long-range interactions in this dataset, preliminary experiments indicated that the phase of the Magnetic Laplacian eigenvectors on its own can also be predictive of the class label. We report average \(\pm\) standard deviation over 5 reruns with the splits from Lim et al. (2021), using a different random seed for each run.

**Models.** We use DirGCN (Rossi et al., 2023) as a baseline and largely follow the original setup. However, we observe that using 4 layers (instead of 6) and introducing a dropout of \(p=0.5\) improves baseline performance. Furthermore, we drop the jumping knowledge used by Rossi et al. (2023). We compare this baseline to S\({}^{2}\)DirGCN with two spectral layers (after the second and third spatial layers) and apply residual connections only for the spectral layers. For the spectral layers, we set \(q=0.0001\) and use the partial eigendecomposition with \(k=100\), a NN in the spectral domain SS 3.2.2, no feature gating, and a bottleneck of \(0.05\). All other parameters are kept similar to the DirGCN base of Rossi et al. (2023). We train for 2000 epochs using the AdamW optimizer (Loshchilov and Hutter, 2019) with a base learing rate of 0.005, no weight decay, and a cosine scheduler with 50 warmup epochs.

**Results.** We report the results in Table 14. Notably, our S\({}^{2}\)DirGCN outperforms both our baseline DirGCN as well as the recent FaberNet (Koke and Cremers, 2024), albeit by a very tight margin. However, we found hyperparameter optimizations to be quite noisy, and as such, the resulting performance metrics should be interpreted cautiously. A more comprehensive evaluation of S\({}^{2}\)GNN's power on heterophilic datasets, potentially with long-range interactions, is left for future work.

Figure 26: RMSE and 90% prediction intervals for distance predictions by ground truth.

### Large-Scale PCQM4MV2 (Hu et al., 2021)

We show that S\({}^{2}\)GNNs are very parameter efficient. Even though we only conduct a very rudimentary hyperparameter search, S\({}^{2}\)GNNs keep up with state of the art approaches. Specifically, we adapt the hyperparameters from peptides-func and achieve comparable performance to the state of the art (excluding external data) with about 3-20% of the number of parameters.

### TPUGraphs Graph Construction

The "XLA" collections of TPUGraphs contain many constructs that are most certainly suboptimal for a machine-learning-based runtime prediction. However, in our preliminary experiments, we could not show that our graph construction yielded better results in a statistically significant manner. Nevertheless, we include this discussion since it might be insightful.

To understand the challenges with the default graph construction, note that in the TPUGraphs dataset each node represents an operation in the compuational graph of Accelerated Linear Algebra (XLA). Its incoming edges are the respective operands, and the outgoing edges signal where the operation's result is used. Thus, the graph describes how the tensors are being transformed. An (perhaps unnecessary) challenge for machine learning models arises from using tuple, which represents a sequence of tensors of arbitrary shapes. In this case, the model needs to reason how the tuple is constructed, converted, and unpacked again. Moreover, directly adjacent tensors/operations can be very far away in the graphs of TPUGraphs.

We identified and manually "fixed" three cases to eliminate this problem largely in the TPUGraphs dataset: Tuple-GetTupleElement, While, and Conditional. Since we could not access the configurations in the HLO protobuf files and C++ XLA extraction code, we decided to perform these optimizations ourselves. However, it might be a better strategy to utilize the existing XLA compiler etc.

Additionally, to the subsequently described graph structure changes, we extract the order of operands from the HLO protobuf files. Outgoing edges are assumed to be unordered except for the GetTupleElement operation, where the tuple index is used as order. Moreover, we extracted all features masked in the C++ code and then excluded constant features.

#### h.10.1 Tuple-GetTupleElement

The dataset contains aggregations via the XLA Tuple operation that are often directly followed by a GetTupleElement operation. To a large extent, these constructs are required for the subsequently discussed While and Conditional operations. Importantly, the model could not extract the relationships through a tuple aggregation since the tuple_index was not included in the default features. Moreover, the resulting tuple hub nodes severely impact the Fourier basis of the graphs (see SS 2). We illustrate the graph simplification in Fig. 27 and denote the edge order of incoming edges from top to bottom. The edge order represents the order of operands.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Method** & **MAE** (\(\downarrow\)) & **\# Parameters** & **Notes** \\ \hline EGT (Hussain et al., 2022) & 0.0857 & 89.3 mio. & 16 layers \\ GRIT (Ma et al., 2023) & 0.0859 & 16.6 mio. & 16 layers \\ GPS (Rampakk et al., 2022) & 0.0852 & 13.8 mio. & 16 layers \\ TGT-At Hussain et al. (2024) & 0.0671 & 203.9 mio. & 32 layers, pretraining on \\
**our S\({}^{2}\)**GNN** & 0.0870 & **2.8 mio.** & 5 layers, hyperparameters \\  & & & adapted from peptides-func \\ \hline \hline \end{tabular}
\end{table}
Table 15: Results on PCQM4MV2 (Hu et al., 2021) (validation).

We propose dropping immediate Tuple-GetTupleElement constructs and directly connecting predecessors and successors. For this, we generate a graph solely consisting of direct connections and then resolve multiple consecutive Tuple-GetTupleElement constructs via a graph traversal (depth-first search).

We perform the Tuple-GetTupleElement simplification after dealing with While and Conditionals. However, for the sake of simplicity, we will avoid using tuples in the subsequent explanations for While and Conditional. In other words, the subsequent explanations extend to functions with multiple arguments via the use of tuples.

#### m.10.2 While Operation

The While operation has the signature While(condition, body, init) where condition is a function given the init or output of the body function. Note that in the TPUGraph construction, body as well as condition only represent the outputs of the respective function and their operands need to be extracted from HLO.

To avoid hub nodes and to retain the dataflow between operations (important for decisions about the layout), operands and outputs are connected directly. Technically, we am modeling a do-while construct because the condition is not connected to the inputs. Since the successors of the while are of type GetTupleElement, they relabeled to a new node type, signaling the end of a while loop. To support nested while loops, each node in the body is assigned a new node feature signaling the number of while body statements it is part of.

Figure 28: Instead of aggregating everything into a hub node, we propose to connect respective inputs and outputs.

Figure 27: Tuple-GetTupleElement simplification: the Tuple aggregates the output of multiple predecessors/operations and then the GetTupleElement extracts the tensor according to its index (number in respective nodes). We propose dropping immediate Tuple-GetTupleElement constructs and connecting predecessors and successors.

#### d.1.0.3 Conditional Operation

Conditional(branch_index, branch_computations, branch_operands) is the most common signature of the Conditional operation, where the integer-valued branch_index selects which branch_computations is executed with the respective input in branch_operands. Similarly to the While operation, we introduce new node types for the inputs of computations and the successors (they are GetTupleElement operations).

Figure 29: Instead of aggregating everything into a hub node, we propose to connect respective inputs and outputs. Here as an example with two conditional computations.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We discuss the shortcomings of prior MPGNNs and that our new framework S\({}^{2}\)GNN takes a new approach to overcome the limitations. We detail our theoretical analysis/justification, outline the yet largely unexplored design space of S\({}^{2}\)GNN, and summarize the experimental evaluation. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We provide approximation-theoretic error bound in SS 3.1.2 that also gives light on the limitations. Moreover, we discuss important considerations of S\({}^{2}\)GNNs throughout the method section SS 3. Additionally, we elaborately list and summarize the limitations in SS K. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

[MISSING_PAGE_FAIL:54]

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will open source fully automized scripts for all datasets that download and preprocess the data prior to the execution of the configured experiment (except TPUGraphs (Phothilimthana et al., 2023) due to its dedicated access). This includes our own datasets on clustering, distance prediction, and associative recall. We provide code at https://www.cs.cit.tum.de/daml/s2gnn. Moreover, we provide reasonable details for reproduction in SS M. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide the setup and details to a reasonable detail in SS 4 and SS M.1. For additional details, we refer to the provided code, including configuration, at https://www.cs.cit.tum.de/daml/s2gnn. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report mean, standard deviation, and details on randomization for all relevant results on benchmark datasets. For qualitative insights and the large-scale TPUGraphs dataset, we solely provide mean estimates. The collective experimental results underline the claims and are strong evidence against the statistical insignificance of our results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide a reasonable summary of used resources in SS M.1 and specifically Table 7. Here, we list the hardware accelerator used and the runtime of the experiment. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: [Yes]Guidelines: The proposed framework S\({}^{2}\)GNN does not pose an unusual risk on top of the common risks for research on machine learning architectures. Our evaluation does not include human subjects, participants, or data. We discuss the broader impact in SS L. Thus, in summary, we conform to the ethics guidelines in every aspect.

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

**Broader Impacts**

Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?

Answer: [Yes]

Justification: We discuss the broader impact in SS L. It should be noted that the proposed framework S\({}^{2}\)GNN does not pose an unusual risk on top of the common risks for research on machine learning architectures. We discuss the broader impact in SS L.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

**Safeguards**

Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?

Answer: [NA]

Justification: The proposed framework S\({}^{2}\)GNN does not pose an unusual risk on top of the common risks for research on machine learning architectures. Our evaluation does not include human subjects, participants, or data.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.

* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all origins of the datasets, even if we derive the benchmark from someone else. We provide an overview of the datasets, including the licenses in Table 6. We provide code, including configuration, at https://www.cs.cit.tum.de/daml/s2gnn. It also contains an (anonymized) license for its usage and discloses the provenance of the project setup. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code and configuration for the dataset on GMM clustering, distance regression, and associative recall will be provided. The generation of all resources takes around one hour and uses solely CPUs. We provide code, including configuration, at https://www.cs.cit.tum.de/daml/s2gnn. The datasets are described in SS M, SS M.6, and SS M.7. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: Our work neither involves crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work neither involves crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper neither involves crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.