# Learning World Models with Identifiable Factorization

 Yu-Ren Liu\({}^{*,1,4}\), Biwei Huang \({}^{*,2}\), Zhengmao Zhu\({}^{1,4}\), Honglong Tian\({}^{1}\),

&Mingming Gong\({}^{5,4}\), Yang Yu\({}^{7,1,6,7}\), Kun Zhang\({}^{7,3,4}\)

\({}^{1}\) National Key Laboratory for Novel Software Technology, Nanjing University, China

\({}^{2}\) University of California San Diego, USA

\({}^{3}\) Carnegie Mellon University, USA

\({}^{4}\) Mohamed bin Zayed University of Artificial Intelligence, UAE

\({}^{5}\) University of Melbourne, Australia

\({}^{6}\) Polisir.ai, China

\({}^{7}\) Peng Cheng Laboratory, China

{liuyr,zhuzm, tianhl, yuy}@lamda.nju.edu.cn bih007@ucsd.edu

mingmingming.gong@unimelb.edu.au kunz1@cmu.edu

###### Abstract

Extracting a stable and compact representation of the environment is crucial for efficient reinforcement learning in high-dimensional, noisy, and non-stationary environments. Different categories of information coexist in such environments - how to effectively extract and disentangle the information remains a challenging problem. In this paper, we propose IFactor, a general framework to model four distinct categories of latent state variables that capture various aspects of information within the RL system, based on their interactions with actions and rewards. Our analysis establishes block-wise identifiability of these latent variables, which not only provides a stable and compact representation but also discloses that all reward-relevant factors are significant for policy learning. We further present a practical approach to learning the world model with identifiable blocks, ensuring the removal of redundancies but retaining minimal and sufficient information for policy optimization. Experiments in synthetic worlds demonstrate that our method accurately identifies the ground-truth latent variables, substantiating our theoretical findings. Moreover, experiments in variants of the DeepMind Control Suite and RoboDesk showcase the superior performance of our approach over baselines.

## 1 Introduction

Humans excel at extracting various categories of information from complex environments [1, 2]. By effectively distinguishing between task-relevant information and noise, humans can learn efficiently and avoid distractions. Similarly, in the context of reinforcement learning, it's crucial for an agent to precisely extract information from high-dimensional, noisy, and non-stationary environments.

World models [3, 4] tackle this challenge by learning compact representations from images and modeling dynamics using low-dimensional features. Recent research has demonstrated that learning policies through latent imagination in world models significantly enhances sample efficiency [5, 6, 7, 8]. However, these approaches often treat all information as an undifferentiated whole, leaving policies susceptible to irrelevant distractions [9] and lacking transparency in decision-making [10].

This paper investigates the extraction and disentanglement of diverse information types within an environment. To address this, we tackle two fundamental questions: 1) How can we establish acomprehensive classification system for different information categories in diverse decision scenarios? 2) Can the latent state variables, classified according to this system, be accurately identified? Prior research has made progress in answering the first question. Task Informed Abstractions [11] partition the state space into reward-relevant and reward-irrelevant features, assuming independent latent processes for each category. Iso-Dream [12] learns controllable and noncontrollable sources of spatiotemporal changes on isolated state transition branches. Denoised MDP [9] further decomposes reward-relevant states into controllable and uncontrollable components, also assuming independent latent processes. However, the assumption of independent latent processes is overly restrictive and may lead to decomposition degradation in many scenarios. Moreover, none of these approaches guarantee the identifiability of representations, potentially leading to inaccurate recovery of the underlying latent variables. While discussions on the identifiability of representations in reinforcement learning (RL) exist under linear assumptions [13], the question of identifiability remains unexplored for general nonlinear cases.

In this paper, we present IFactor, a general framework to model four distinct categories of latent state variables within the RL system. These variables capture different aspects of information based on their interactions with actions and rewards, providing transparent representations of the following aspects: (i) reward-relevant and controllable parts, (ii) reward-irrelevant but uncontrollable parts, (iii) controllable but reward-irrelevant parts, and (iv) unrelated noise (see Section 2). Diverging from prior methods, our approach employs a general factorization for the latent state variables, allowing for causally-related latent processes. We theoretically establish the block-wise identifiability of these four variable categories in general nonlinear cases under weak and realistic assumptions. Our findings challenge the conclusion drawn in the Denoised MDP [9] that only controllable and reward-relevant variables are required for policy optimization. We emphasize the necessity of considering states that directly or indirectly influence the reward during the decision-making process, irrespective of their controllability. To learn these four categories of latent variables, we propose a principled approach that involves optimizing an evidence lower bound and integrating multiple novel mutual information constraints. Through simulations on synthetic data, we demonstrate the accurate identification of true latent variables, validating our theoretical findings. Furthermore, our method achieves state-of-the-art performance on variants of RoboDesk and DeepMind Control Suite.

## 2 Four Categories of (Latent) State Variables in RL

For generality, we consider tasks in the form of Partially Observed Markov Decision Process (POMDP) [15], which is described as \(\mathcal{M}\triangleq(\mathcal{S},\mathcal{A},\Omega,R,T,O,\gamma)\), where \(\mathcal{S}\) is the latent state space, \(\mathcal{A}\) is the action space, \(\Omega\) is the observation space, \(R:S\times A\rightarrow\mathbb{R}\) defines the reward function, \(T:S\times A\rightarrow\Delta(S)\) is the transition dynamics, \(O:S\times A\rightarrow\Delta(\Omega)\) is the observation function, \(\gamma\in[0,1]\) is the discount factor. We use \(\Delta(S)\) to denote the set of all distributions over \(S\). The agent can interact with the environment to get sequences of observations \(\{\langle o_{i},a_{i},r_{i}\rangle\}_{i=1}^{T}\). The objective is to find a policy acting based on history observations, that maximizes the expected cumulative (discounted) reward.

Figure 1: (a) An illustrative example of the car-driving task. (b) Four categories of latent state variables in the car driving task. (c) The structure of our world model. Grey nodes denote observed variables and other nodes are unobserved. We allow causally-related latent processes for four types of latent variables and prove that they are respectively identifiable up to block-wise invertible transformations. We show that both \(s^{\mu\nu}_{t}\) and \(s^{\mu\nu}_{t}\) are essential for policy optimization. The inclusion of the gray dashed line from (\(s^{\mu\nu}_{t}\), \(s^{\mu\nu}_{t}\)) to \(a_{i}\) signifies that the action could be determined by reward-relevant variables. (d) The structure of Denoised MDP [9]. It assumes the latent processes of \(x_{t}\) and \(y_{t}\) are independent and uses only \(x_{t}\) for policy optimization. The gray dashed line from \(x_{t-1}\) to \(a_{t-1}\) shows that the action could be determined only based on the controllable and reward-relevant latent variables. Further, the existence of instantaneous causal effect from \(x_{t}\) and \(y_{t}\) to \(z_{t}\) renders the latent process unidentifiable without extra intervention on latent states [14].

In the context of POMDPs, extracting a low-dimensional state representation from high-dimensional observations is crucial. Considering that action and reward information is fundamental to decision-making across various scenarios, we disentangle the latent variables in the environment into four distinct categories, represented as \(\mathbf{s_{t}}=\{s_{t}^{ar},s_{t}^{ar},s_{t}^{ar}\}\), \(s_{t}^{ar}\), \(s_{t}^{ar}\), based on their relationships with action and reward (see Figure 1(c) as a graphical illustration):

* Type 1: \(s_{t}^{ar}\) has an incident edge from \(a_{t-1}\), and there is a directed path from \(s_{t}^{ar}\) to \(r_{t}\).
* Type 2: \(s_{t}^{ar}\) has no incident edge from \(a_{t-1}\), and there is a directed path from \(s_{t}^{ar}\) to \(r_{t}\).
* Type 3: \(s_{t}^{ar}\) has an incident edge from \(a_{t-1}\), and there is no directed path from \(s_{t}^{ar}\) to \(r_{t}\).
* Type 4: \(s_{t}^{ar}\) has no incident edge from \(a_{t-1}\), and there is no directed path from \(s_{t}^{ar}\) to \(r_{t}\).

\(s_{t}^{ar}\) represents controllable and reward-relevant state variables that are essential in various scenarios. Taking the example of a car driving context, \(s_{t}^{ar}\) encompasses driving-related states like the current speed, position, and direction of the car. These latent variables play a critical role in determining the driving policy since actions such as steering and braking directly influence these states, which, in turn, have a direct impact on the reward received.

\(s_{t}^{ar}\) refers to reward-relevant state variables that are beyond our control. Despite not being directly controllable, these variables are still necessary for policy learning. In the context of car driving, \(s_{t}^{ar}\) includes factors like surrounding vehicles and weather conditions. Although we cannot directly control other cars, our car's status influences their behavior: if we attempt to cut in line unethically, the surrounding vehicles must decide whether to yield or block our behavior. As a result, we need to adjust our actions based on their reactions. Similarly, the driver must adapt their driving behavior to accommodate various weather conditions, even though the weather itself is uncontrollable.

The state variables denoted as \(s_{t}^{ar}\) consist of controllable but reward-irrelevant factors. Examples of \(s_{t}^{ar}\) could be the choice of music being played or the positioning of ormanns within the car. On the other hand, \(s_{t}^{ar}\) represents uncontrollable and reward-irrelevant latent variables such as the remote scenery. Both \(s_{t}^{ar}\) and \(s_{t}^{ar}\) do not have any impact on current or future rewards and are unrelated to policy optimization.

We next build a connection between the graph structure and statistical dependence between the variables in the RL system, so that the different types of state variables can be characterized from the data. To simplify symbol notation, we define \(s_{t}^{r}:=(s_{t}^{ar},\,s_{t}^{ar})\), \(s_{t}^{r}:=(s_{t}^{ar},\,s_{t}^{ar})\), \(s_{t}^{ar}:=(s_{t}^{ar},\,s_{t}^{ar})\) and \(s_{t}^{ar}:=(s_{t}^{ar},\,s_{t}^{ar})\). Specifically, the following proposition shows that \(s_{t}^{r}\) that has directed paths to \(r_{t+r}\) (for \(\tau>0\)), is minimally sufficient for policy learning that aims to maximize the future reward and can be characterized by conditional dependence with the cumulative reward variable \(R_{t}=\sum_{t^{\prime}=t}\gamma^{t^{\prime}-t}r_{t^{\prime}}\), which has also been shown in [13].

**Proposition 1**.: _Under the assumption that the graphical representation, corresponding to the environment model, is Markov and faithful to the measured data, \(s_{t}^{r}\subseteq\mathbf{s_{t}}\) is a minimal subset of state dimensions that are sufficient for policy learning, and \(s_{i,t}\in s_{t}^{r}\) if and only if \(s_{i,t}\not\perp R_{i}|a_{t-1:t}\), \(s_{t-1}^{r}\)._

Moreover, the proposition below shows that \(s_{t}^{a}\), that has a directed edge from \(a_{t-1}\), can be directly controlled by actions and can be characterized by conditional dependence with the action variable.

**Proposition 2**.: _Under the assumption that the graphical representation, corresponding to the environment model, is Markov and faithful to the measured data, \(s_{t}^{a}\subseteq\mathbf{s_{t}}\) is a minimal subset of state dimensions that are sufficient for direct control, and \(s_{i,t}\in s_{t}^{a}\) if and only if \(s_{i,t}\not\perp a_{t-1}|\mathbf{s_{t-1}}\)._

Furthermore, based on Proposition 1 and Proposition 2, we can further differentiate \(s_{t}^{ar},s_{t}^{ar}\), \(s_{t}^{ar}\) from \(s_{t}^{r}\) and \(s_{t}^{a}\), which is given in the following proposition.

**Proposition 3**.: _Under the assumption that the graphical representation, corresponding to the environment model, is Markov and faithful to the measured data, we can build a connection between the graph structure and statistical independence of causal variables in the RL system, with (1) \(s_{i,t}\in s_{t}^{ar}\) if and only if \(s_{i,t}\not\perp R_{i}|a_{t-1:t}\), \(s_{t-1}^{r}\) and \(s_{i,t}\not\perp a_{t-1}|\mathbf{s_{t-1}}\), (2) \(s_{i,t}\in s_{t}^{ar}\) if and only if \(s_{i,t}\not\perp R_{i}|a_{t-1:t}\), \(s_{t-1}^{r}\), and \(s_{i,t}\perp\Delta_{t-1}|\mathbf{s_{t-1}}\), (3) \(s_{i,t}\in s_{t}^{ar}\) if and only if \(s_{i,t}\perp R_{i}|a_{t-1:t}\), \(s_{t-1}^{r}\) and \(s_{i,t}\perp\Delta_{t-1}|\mathbf{s_{t-1}}\), and (4) \(s_{i,t}\in s_{t}^{ar}\) if and only if \(s_{i,t}\perp R_{i}|a_{t-1:t}\), \(s_{t-1}^{r}\) and \(s_{i,t}\perp\Delta_{t-1}|\mathbf{s_{t-1}}\)._

By identifying these four categories of latent state variables, we can achieve interpretable input for policies, including (i)reward-relevant and directly controllable parts, (ii) reward-relevant but not controllable parts, (iii) controllable but non-reward-relevant parts, and (iv) unrelated noise. Furthermore, policy training will become more sample efficient and robust to task-irrelevant changes by utilizing only \(s_{t}^{ar}\) and \(s_{t}^{ar}\) for policy learning. We show the block-wise identifiability of the four categories of variables in the next section.

Identifiability Theory

Identifying causally-related latent variables from observations is particularly challenging, as latent variables are generally not uniquely recoverable [16; 17]. Previous work in causal representation learning and nonlinear-ICA has established identifiability conditions for non-parametric temporally latent causal processes [18; 19]. However, these conditions are often too stringent in reality, and none of these methods have been applied to complex control tasks. In contrast to component-wise identifiability, our focus lies on the block-wise identifiability of the four categories of latent state variables. From a policy optimization standpoint, this is sufficient because we do not need the latent variables to be recovered up to permutation and component-wise invertible nonlinearities. This relaxation makes the conditions more likely to hold true and applicable to a wide range of RL tasks. We provide proof of the block-wise identifiability of the four types of latent variables. To the best of our knowledge, we are the first to prove the identifiability of disentangled latent state variables in general nonlinear cases for RL tasks.

According to the causal process in the RL system (as described in Eq.1 in [13]), we can build the following mapping from latent state variables \(\mathbf{s}_{t}\) to observed variables \(o_{t}\) and future cumulative reward \(R_{t}\):

\[[o_{t},R_{t}]=f(s^{r}_{t},s^{\bar{r}}_{t},\eta_{t}),\] (1)

where

\[\begin{array}{rcl}o_{t}&=&f_{1}(s^{r}_{t},s^{r}_{t}),\\ R_{t}&=&f_{2}(s^{r}_{t},\eta_{t}).\end{array}\] (2)

Here, note that to recover \(s^{r}_{t}\), it is essential to take into account all future rewards \(r_{t:T}\), because any state dimension \(s_{t:t}\in\mathbf{s}_{t}\) that has a directed path to the future reward \(r_{t+\tau}\), for \(\tau>0\), is involved in \(s^{r}_{t}\). Hence, we consider the mapping from \(s^{r}_{t}\) to the future cumulative reward \(R_{t}\), and \(\eta_{t}\) represents residuals, except \(s^{r}_{t}\), that have an effect to \(R_{t}\).

Below, we first provide the definition of blockwise identifiability and relevant notations, related to [20; 21; 22; 23].

**Definition 1** (Blockwise Identifiability).: _A latent variable \(\mathbf{s}_{t}\) is blockwise identifiable if there exists a one-to-one mapping \(h(\cdot)\) between \(\mathbf{s}_{t}\) and the estimated \(\hat{\mathbf{s}}_{t}\), i.e., \(\hat{\mathbf{s}}_{t}=h(\mathbf{s}_{t})\)._

Notations.We denote by \(\tilde{\mathbf{s}}_{t}:=(s^{r}_{t},s^{\bar{r}}_{t},\eta_{t})\) and by \(|s|\) the dimension of a variable \(s\). We further denote \(d_{s_{t}}:=|s^{r}_{t}|\), \(d_{s_{t}}:=|s^{r}_{t}|\), \(d_{s}:=|\mathbf{s}_{t}|\), \(d_{o}:=|o_{t}|\), and \(d_{R}:=|R_{t}|\). We denote by \(\mathcal{F}\) the support of Jacobian \(\mathbf{J}_{j}(\mathbf{s}_{t})\), by \(\hat{\mathcal{F}}\) the support of \(\mathbf{J}_{j}(\mathbf{s}_{t})\), and by \(\mathcal{T}\) the support of \(\mathbf{T}(\mathbf{s}_{t})\) with \(\mathbf{J}_{j}(\mathbf{s}_{t})=\mathbf{J}_{j}(\mathbf{s}_{t})\mathbf{T}( \mathbf{s})\). We also denote \(T\) as a matrix with the same support as \(\mathcal{T}\).

In addition, given a subset \(\mathcal{D}\subseteq\{1,\cdots,d_{i}\}\), the subspace \(\mathbb{R}_{\mathcal{D}}^{d_{i}}\) is defined as \(\mathbb{R}_{\mathcal{D}}^{d_{i}}:=\{z\in\mathbb{R}^{d_{i}}|i\notin\mathcal{D} \Longrightarrow z_{i}=0\}\). In other words, \(\mathbb{R}_{\mathcal{D}}^{d_{i}}\) refers to the subspace of \(\mathbb{R}^{d_{i}}\) indicated by an index set \(\mathcal{D}\).

We next show that the different types of states \(s^{ar}_{t}\), \(s^{\bar{ar}}_{t}\), \(s^{\bar{ar}}_{t}\), and \(s^{\bar{ar}}_{t}\) are blockwise identifiable from observed image variable \(o_{t}\), reward variable \(r_{t}\), and action variable \(a_{t}\), under reasonable and weak assumptions, which is partly inspired by [20; 21; 22; 23].

**Theorem 1**.: _Suppose that the causal process in the RL system and the four categories of latent state variables can be described as that in Section 2 and illustrated in Figure 1(c). Under the following assumptions_

1. _The mapping_ \(f\) _in Eq._ 1 _is smooth and invertible with smooth inverse._
2. _For all_ \(i\in\{1,\ldots,d_{o}+d_{R}\}\) _and_ \(j\in\mathcal{F}_{i,\cdot}\)_, there exist_ \(\{\mathbf{s}_{t}^{(i)}\}_{t=1}^{|\mathcal{F}_{i,\cdot}|}\)_, so that_ \(\text{span}\{\mathbf{J}_{j}(\mathbf{s}_{t}^{(i)})_{i,\cdot}\}_{t=1}^{|\mathcal{F }_{i,\cdot}|}=\mathbb{R}_{\mathcal{F}_{i,\cdot}}^{d_{i}}\)_, and there exists a matrix_ \(T\) _with its support identical to that of_ \(\mathbf{J}_{j}^{-1}(\hat{\mathbf{s}}_{t})\mathbf{J}_{j}(\mathbf{s}_{t})\)_, so that_ \(\left[\mathbf{J}_{j}(\mathbf{s}_{t}^{(l)})T\right]_{j,\cdot}\in\mathbb{R}_{ \mathcal{F}_{i,\cdot}}^{d_{i}}\)_._

_Then, reward-relevant and controllable states \(s^{ar}_{t}\), reward-relevant but not controllable states \(s^{\bar{ar}}_{t}\), reward-irrelevant but controllable states \(s^{\bar{ar}}_{t}\), and noise \(s^{\bar{ar}}_{t}\), are blockwise identifiable._

In the theorem presented above, Assumption A1 only assumes the invertibility of function \(f\), while functions \(f_{1}\) and \(f_{2}\) are considered general and not necessarily invertible, as that in [23]. Since the function \(f\) is the mapping from all (latent) variables, including noise factors, that influence the observed variables, the invertibility assumption holds reasonably. However, note that it is not reasonable to assume the invertibility of the function \(f_{2}\) since usually, the reward function is not invertible. Intuitively, Assumption A2 requires that the Jacobian varies "enough" so that it cannot be contained in a proper subspace of \(\mathbb{R}_{\mathcal{F}_{i}}^{d_{\mathcal{F}_{i}}}\). This requirement is necessary to avoid undesirable situations where the problem becomes ill-posed and is essential for identifiability. A special case when this property does not hold is when the function \(f\) is linear, as the Jacobian remains constant in such cases. Some proof techniques of Theorem 1 follow from [20; 22; 23], with the detailed proof given in Appendix A.4.

## 4 World Model with Disentangled Latent Dynamics

Based on the four categories of latent state variables, we formulate a world model with disentangled latent dynamics. Each component of the world model is described as follows:

\[\left\{\begin{array}{ll}\textit{Observation Model:}&p_{\theta}\left(o_{t} \mid\mathbf{s_{t}}\right)\\ \textit{Reward Model:}&p_{\theta}\left(r_{t}\mid s_{t}^{\prime}\right)\\ \textit{Transition Model:}&p_{\gamma}\left(\mathbf{s_{t}}\mid\mathbf{s_{t-1}},a_{ t-1}\right)\\ \textit{Representation Model:}&q_{\phi}\left(\mathbf{s_{t}}\mid o_{t},\mathbf{s_{t-1}},a_{ t-1}\right)\end{array}\right.\] (3)

Specifically, the world model includes three generative models: an observation model, a reward function, and a transition model (prior), and a representation model (posterior). The observation model and reward model are parameterized by \(\theta\). The transition model is parameterized by \(\gamma\) and the representation model is parameterized by \(\phi\). We assume noises are i.i.d for all models. The action \(a_{t-1}\) has a direct effect on the latent states \(\mathbf{s_{t}}\), but not on the perceived signals \(o_{t}\). The perceived signals, \(o_{t}\), are generated from the underlying states \(\mathbf{s_{t}}\), while signals \(r_{t}\) are generated only from reward-relevant latent variables \(s_{t}^{\prime}\).

According to the graphical model depicted in Figure 1(c), the transition model and the representation model can be further decomposed into four distinct sub-models, according to the four categories of latent state variables, as shown in equation 4.

\[\left\{\begin{array}{ll}\textit{Disentangled Transition Model:}&\textit{ Disentangled Representation Model:}\\ \left\{\begin{array}{ll}p_{\gamma_{1}}(s_{t}^{\prime\prime}\mid s_{t-1}^{\prime},a_{ t-1})\\ p_{\gamma_{2}}(s_{t}^{\prime\prime}\mid s_{t-1}^{\prime})\\ p_{\gamma_{3}}(s_{t}^{\prime\prime}\mid\mathbf{s_{t-1}},a_{t-1})\\ p_{\gamma_{4}}(s_{t}^{\prime\prime}\mid\mathbf{s_{t-1}})\end{array}\right. \left\{\begin{array}{ll}q_{\phi_{1}}(s_{t}^{\prime\prime}\mid o_{t},s_{t-1}^ {\prime},a_{t-1})\\ q_{\phi_{2}}(s_{t}^{\prime\prime}\mid o_{t},s_{t-1}^{\prime})\\ q_{\phi_{3}}(s_{t}^{\prime\prime}\mid o_{t},s_{t-1})\\ q_{\phi_{4}}(s_{t}^{\prime\prime}\mid o_{t},s_{t-1})\end{array}\right.\] (4)

Specifically, we have \(p_{\gamma}=p_{\gamma_{1}}\cdot p_{\gamma_{2}}\cdot p_{\gamma_{3}}\cdot p_{ \gamma_{4}}\) and \(q_{\phi}=q_{\phi_{1}}\cdot q_{\phi_{2}}\cdot q_{\phi_{3}}\cdot q_{\phi_{4}}\). Note this factorization differs from previous works [11; 9] that assume independent latent processes. Instead, we only assume conditional independence among the four categories of latent variables given \(\mathbf{s_{t-1}}\), which provides a more general factorization. In particular, the dynamics of \(s_{t}^{\prime\prime}\) and \(s_{t}^{\prime\prime}\) are dependent on \(s_{t-1}^{r}\), ensuring that they are unaffected by any reward-irrelevant latent variables present in \(s_{t-1}^{j}\). On the other hand, \(s_{t}^{\prime}\) may be influenced by all the latent variables from the previous time step. Note that the connections between \(\mathbf{s_{t-1}}\) and \(s_{t}^{\prime}\) can be adjusted based on the concrete problem. Since \(s_{t}^{\prime\prime}\) and \(s_{t}^{\prime\prime}\) are controllable variables, their determination also relies on \(a_{t-1}\).

### World Model Estimation

The optimization of the world model involves joint optimization of its four components to maximize the variational lower bound [24] or, more generally, the variational information bottleneck [25; 26]. The bound encompasses reconstruction terms for both observations and rewards, along with a KL regularizer:

\[\mathcal{J}_{\mathrm{O}}^{t}=\ln p_{\theta}\left(o_{t}\mid\mathbf{s_{t}} \right)\quad\mathcal{J}_{\mathrm{R}}^{t}=\ln p_{\theta}\left(r_{t}\mid s_{t}^{ \prime}\right)\quad\mathcal{J}_{\mathrm{D}}^{t}=-\mathrm{KL}\left(q_{\phi} \|p_{\gamma}\right).\] (5)

Further, the KL regularizer \(\mathcal{J}_{\mathrm{D}}^{t}\) can be decomposed into four components based on our factorization of the state variables. We introduce additional hyperparameters to regulate the amount of information contained within each category of variables:

\[\mathcal{J}_{\mathrm{D}}^{t}=-\beta_{1}\cdot\mathrm{KL}\left(q_{\phi_{1}}\|p_{ \gamma_{1}}\right)-\beta_{2}\cdot\mathrm{KL}\left(q_{\phi_{2}}\|p_{\gamma_{2} }\right)-\beta_{3}\cdot\mathrm{KL}\left(q_{\phi_{1}}\|p_{\gamma_{1}}\right)- \beta_{4}\cdot\mathrm{KL}\left(q_{\phi_{4}}\|p_{\gamma_{4}}\right).\] (6)

Additionally, we introduce two supplementary objectives to explicitly capture the distinctive characteristics of the four distinct representation categories. Specifically, we characterize the reward relevant representations by measuring the dependence between \(s^{r}_{t}\) and \(R_{t}\), given \(a_{t-1:t}\) and \(s^{r}_{t-1}\), that is \(I(s^{r}_{t},R_{t}\mid a_{t-1:t},s^{r}_{t-1})\). To ensure that \(s^{r}_{t}\) are minimally sufficient for policy training, we maximize \(I(s^{r}_{t},R_{t}\mid a_{t-1:t},s^{r}_{t-1})\) while minimizing \(I(s^{r}_{t},R_{t}\mid a_{t-1:t},s^{r}_{t-1})\) to discourage the inclusion of redundant information in \(s^{r}_{t}\) concerning the rewards:

\[I(s^{r}_{t};R_{t}\mid a_{t-1:t},s^{r}_{t-1})-I(s^{r}_{t};R_{t}\mid a_{t-1:t},s^ {r}_{t-1}).\] (7)

The conditional mutual information can be expressed as the disparity between two mutual information.

\[\begin{split}& I(s^{r}_{t};R_{t}\mid a_{t-1:t},s^{r}_{t-1})=I(R_{t} ;s^{r}_{t},a_{t-1:t},s^{r}_{t-1})-I(R_{t};a_{t-1:t},s^{r}_{t-1}),\\ & I(s^{r}_{t};R_{t}\mid a_{t-1:t},s^{r}_{t-1})=I(R_{t};s^{r}_{t}, a_{t-1:t},s^{r}_{t-1})-I(R_{t};a_{t-1:t},s^{r}_{t-1}).\end{split}\] (8)

After removing the common term, we leverage mutual information neural estimation [27] to approximate the value of mutual information. Thus, we reframe the objective in Eq.7 as follows:

\[\mathcal{J}^{t}_{\text{RS}}=\lambda_{1}\cdot\{I_{a_{1}}(R_{t};s^{r}_{t},a_{t -1:t},\mathbf{sg}(s^{r}_{t-1}))-I_{a_{2}}(R_{t};s^{r}_{t},a_{t-1:t},\mathbf{ sg}(s^{r}_{t-1}))\}.\] (9)

We employ additional neural networks, parameterized by \(\alpha\), to estimate the mutual information. To incorporate the conditions from the original objective, we apply the stop_gradient operation \(\mathbf{sg}\) to the variable \(s^{r}_{t-1}\). Similarly, to ensure that the representations \(s^{a}_{t}\) are directly controllable by actions, while \(s^{\tilde{n}}_{t}\) are not, we maximize the following objective:

\[\mathcal{J}^{t}_{\text{AS}}=\lambda_{2}\cdot[I_{a_{3}}(a_{t-1};s^{a}_{t}, \mathbf{sg}(\mathbf{s_{t-1}}))-I_{a_{4}}(a_{t-1};s^{\tilde{n}}_{t},\mathbf{ sg}(\mathbf{s_{t-1}}))].\] (10)

Intuitively, these two objective functions ensure that \(s^{r}_{t}\) is predictive of the reward, while \(s^{r}_{t}\) is not; similarly, \(s^{a}_{t}\) can be predicted by the action, whereas \(s^{\tilde{n}}_{t}\) cannot. The total objective for learning the world model is summarized as:

\[\mathcal{J}_{\text{TOTAL}}=\text{E}_{q_{\text{e}}}\left(\sum_{t}\left( \mathcal{J}^{t}_{\text{O}}+\mathcal{J}^{t}_{\text{R}}+\mathcal{J}^{t}_{\text{ D}}+\mathcal{J}^{t}_{\text{RS}}+\mathcal{J}^{t}_{\text{AS}}\right)\right)+\text{ const}\.\] (11)

The expectation is computed over the dataset and the representation model. Throughout the model learning process, the objectives for estimating mutual information and learning the world model are alternately optimized. A thorough derivation and discussion of the objective function are given in Appendix B.

It is important to note that our approach to learning world models with identifiable factorization is independent of the specific policy optimization algorithm employed. In this work, we choose to build upon the state-of-the-art method Dreamer [6; 7], which iteratively performs exploration, model-fitting, and policy optimization. As illustrated in Figure 2, the learning process for the dynamics involves the joint training of the four categories of latent variables. However, only the variables of \(s^{r}_{t}\) are utilized for policy learning. We provide the pseudocode for the IFactor algorithm in Appendix C.

Figure 2: (a) The agent learns the disentangled latent dynamics from prior experiences. The yellow arrow represents a one-one mapping from \(h^{*}_{t}\) to \(s^{*}_{t}\) with the same superscript. (b) Within the latent space, state values and actions are forecasted to maximize future value predictions by backpropagating gradients through imagined trajectories. Only \(s^{r}_{t}\) (reward-relevant) are used for policy optimization.

Experiments

In this section, we begin by evaluating the identifiability of our method using simulated datasets. Subsequently, we visualize the learned representations of the four categories in a cartpole environment that includes distractors. Further, we assess the advantages of our factored world model in policy learning by conducting experiments on variants of Robodesk and DeepMind Control Suite. The reported results are aggregated over 5 runs for the Robodesk experiments and over 3 runs for others. A detailed introduction to each environment is provided in Appendix D, while experiment details and hyperparameters can be found in Appendix E. The source code is available at https://github.com/AlexLiuyuren/IFactor

### Latent State Identification Evaluation

#### 5.1.1 Synthetic environments

Evaluation Metrics and BaselinesWe generate synthetic datasets that satisfy the identifiability conditions outlined in the theorems (see Appendix D.1). To evaluate the block-wise identifiability of the four categories of representations, we follow the experimental methodology of [28] and compute the coefficient of determination \(R^{2}\) from \(s^{*}_{t}\) to \(\hat{s}^{*}_{t}\), as well as from \(\hat{s}^{*}_{t}\) to \(s^{*}_{t}\), for \(*\in[ar,\bar{a}r,a\bar{r},\bar{a}\bar{r}]\). \(R^{2}\) can be viewed as the identifiability score. \(R^{2}=1\) in both directions suggests a one-to-one mapping between the true latent variables and the recovered ones. We compare our latent representation learning method with Denoised MDP [9]. We also include baselines along the line of nonlinear ICA: BetaVAE [29] and FactorVAE [30] which do not consider temporal dependencies; SlowVAE [31] and PCL[32] which leverage temporal constraints but assume independent sources; and TDRL [19] which incorporates temporal constraints and non-stationary noise but does not utilize the action and reward information in the Markov Decision Process.

ResultsFigure 9 demonstrates the effectiveness of our method in accurately recovering the four categories of latent state variables, as evidenced by high \(R^{2}\) values (\(>0.9\)). In contrast, the baseline methods exhibit distortions in the identification results. Particularly, Denoised MDP assumes independent latent process for \(x_{t},y_{t}\), and the existence of instantaneous causal effect from \(x_{t},y_{t}\) to \(z_{t}\) (see Figure 1(d)), leading to unidentifiable latent variables without further intervention. BetaVAE, FactorVAE, SlowVAE, and PCL, which assume independent sources, show subbar performance. Although TDRL allows for causally-related processes under conditional independence assumptions, it fails to explicitly leverage the distinct transition structures of the four variable categories. More results on the identifiability scores of baselines are provided in Appendix E.1.

#### 5.1.2 Modified Cartpole with distractors

We have introduced a variant of the original Cartpole environment by incorporating two distractors. The first distractor is an uncontrollable Cartpole located in the upper portion of the image, which is irrelevant to the rewards. The second distractor is a controllable but reward-irrelevant green light positioned below the reward-relevant Cartpole in the lower part of the image.

Figure 3: Simulation results: (a) The coefficient of determination (\(R^{2}\)) obtained by using kernel ridge regression to regress estimated latents on true latents.(b) The \(R^{2}\) obtained by using kernel ridge regression [33] to regress true latents on estimated latents. (c) The average \(R^{2}\) over four types of representations during training (True latents \(\rightarrow\) Estimated latents).(d) The average \(R^{2}\) over four types of representations during training (Estimated latents \(\rightarrow\) True latents).

After estimating the representation model, we utilize latent traversal to visualize the four categories of representations in the modified Cartpole environment (see Figure 4). Specifically, the recovered \(s^{ar}_{t}\) corresponds to the position of the cart, while \(s^{ar}_{t}\) corresponds to the angle of the pole. This demonstrates the ability of our method to automatically factorize reward-relevant representations based on (one-step) controllability: the cart experiences a force at the current time step, whereas the pole angle is influenced in the subsequent time step. Note that during the latent traversal of \(s^{ar}_{t}\), the position of the cart remains fixed, even though the pole originally moves with the cart in the video. Additionally, \(s^{ar}_{t}\) corresponds to greenness of the light, and \(s^{ar}_{t}\) corresponds to the uncontrollable and reward-irrelevant cartpole. These findings highlight the capability of our representation learning method to disentangle and accurately recover the ground-true latent variables from videos. The identifiability scores of four categories of representations are provided in Appendix E.2.

### Policy Optimization Evaluation

To assess the effectiveness of our method in enhancing policy learning by utilizing minimal yet sufficient representations for control, we conducted experiments on more complex control tasks. One of these tasks is a variant of Robodesk [34], which includes realistic noise elements such as flickering lights, shaky cameras, and a dynamic video background. In this task, the objective for the agent is to change the hue of a TV screen to green using a button press. We also consider variants of DeepMind Control Suite (DMC) [35; 9], where distractors such as a dynamic video background, noisy sensor readings, and a jittering camera are introduced to the original DMC environment. These additional elements aim to challenge the agent's ability to focus on the relevant aspects of the task while filtering out irrelevant distractions. Baseline results except for DreamerPro [36] are derived from the Denoised MDP paper. We have omitted the standard deviation of their performance for clarity.

Evaluation Metrics and BaselinesWe evaluate the performance of the policy at intervals of every 10,000 environment steps and compare our method with both model-based and model-free approaches. Among the model-based methods, we include Denoised MDP [9], which is the state-of-the-art method for variants of Robodesk and DMC. We also include DreamerPro [36], TIA [11] and Dreamer [6] as additional model-based baselines. For the model-free methods, we include DBC [37], CURL[38], and PI-SAC [39]. To ensure a fair comparison, we have aligned all common hyperparameters and neural network structures with those used in the Denoised MDP [9].

#### 5.2.1 RoboDesk with Various Noise Distractors

In Figure 5, the left image demonstrates that our model IFactor achieves comparable performance to Denoised MDP while outperforming other baselines. The results of baselines except for DreamerPro are directly copied from the paper of Denoised MDP [9] (its error bar cannot be directly copied), and the replication of Denoised MDP results is given in Appendix E.3. Furthermore, to investigate whether \(s^{r}_{t}\) serves as minimal and sufficient representations for policy learning, we retrain policies using the Soft Actor-Critic algorithm [40] with different combinations of the four learned categories as input. Remarkably, the policy trained using \(s^{r}_{t}\) exhibits the best performance, while the performance of other policies degrades due to insufficient information (e.g., \(s^{ar}_{t}\) and \(\tilde{s^{r}_{t}}\)) or redundant information

Figure 4: Latent traversal for four types of representations in the modified Cartpole environment.

(e.g., \(s_{t}^{r}+s_{t}^{a^{\prime}}\) and \(\mathbf{s_{t}}\)). Moreover, we conduct latent traversal on the learned representations to elucidate their original meaning in the observation. Interestingly, our model identifies the greenness of the screen as \(s_{t}^{a^{\prime}}\), which may initially appear counter-intuitive. However, this phenomenon arises from the robot arm pressing the green button, resulting in the screen turning green. Consequently, the screen turning green becomes independent of rewards conditioned on the robot arm pressing the green button, aligning with the definition of \(s_{t}^{a^{\prime}}\). This empirically confirms the minimality of \(s_{t}^{r}\) for policy optimization.

#### 5.2.2 DeepMind Control Suite (DMC)

Figure 6 demonstrates the consistent superiority of our method over baselines in the Deepmind Control Suite (DMC) across various distractor scenarios. Our approach exhibits robust performance in both noiseless and noisy environments, demonstrating the effectiveness of our factored world model in eliminating distractors while preserving essential information for effective and efficient control in policy learning. The ablation study of IFactor's objective function terms is presented in Appendix B.1 and Appendix E.3, which shows that the inclusion of mutual information constraints is essential to promote disentanglement and enhance policy performance. The policy performance that includes the standard error is provided in Appendix E.4. Visualization of the learned representations is also provided in Appendix E.5.

## 6 Related Work

World Model Learning in RL.Image reconstruction [41; 42] and contrastive learning [43; 44; 45; 46; 47] are wildly used to learn representations in RL. Dreamer and its subsequent extensions [5; 6; 7; 8] adopt a world model-based learning approach, where the policy is learned solely through dreaming in the world model. However, the agents trained by these techniques tend to underperform in noisy environments [9]. To address this issue, numerous approaches have been introduced to enhance robustness to distractions. Task Informed Abstractions (TIA) [11] explicitly partition the latent state space into reward-relevant and reward-irrelevant features. Denoised MDP [9] takes a step further and decomposes the reward-relevant states into controllable and uncontrollable components. InfoPower [48] prioritizes information that is correlated with action based on mutual information. Iso-Dream [12] learns controllable and noncontrollable sources of spatiotemporal changes on isolated state transition branches. Our method extends these work by accommodating a more general factorization with block-wise identifiablity. Recent research also explores reconstruction-free representation learning methods [36; 49; 50; 51]. DreamerPro [36] combines Dreamer with prototypes, which distills temporal structures from past observations and actions. Temporal Predictive Coding [50] encodes elements in the environment that can be predicted across time. Contratively-trained Structured World Models [51] utilize graph neural networks and a contrastive approach for representation learning in environments with compositional structure. Latent states learned by these methods are not identifiable due to the reconstruction-free property, where the mixing function is not invertible anymore. A detailed comparison between IFactor and related work is also given in Appendix F.

Identifiability in Causal Representation Learning.Temporal structure and nonstationarities were recently used to achieve identifiability in causal representation learning [52]. Methods such as TCL

Figure 5: Results on Robodesk. We have omitted the standard deviation of the performance of baselines for clarity. The left image shows that our method IFactor achieves comparable performance with Denoised Mdp, outperforming other baselines. The middle image shows the policy optimization process by SAC [40] using representations learned by IFactor, where policies that use \(s_{t}^{r}\) as input achieve the best performance. The right image shows the latent traversal of the representations.

[53] and PCL [32] leverage nonstationarity in temporal data through contrastive learning to identify independent sources. CITRIS [54] and iCITRIS [55] takes advantage of observed intervention targets to identify causal factors. LEAP [18] introduces identifiability conditions for both parametric and nonparametric latent processes. TDRL [19] explores identifiability of latent processes under stationary environments and distribution shifts. ASRs [13] establish the identifiability of the representations in the linear-gaussian case in the RL setting. Our work extends the theoretical results in ASRs to enable block-wise identifiability of four categories of latent variables in general nonlinear cases.

## 7 Conclusion

In this paper, we present a general framework to model four distinct categories of latent state variables within the RL system, based on their interactions with actions and rewards. We establish the block-wise identifiability of these latent categories in general nonlinear cases, under weak and realistic assumptions. Accordingly, we propose IFactor to extract four types of representations from raw observations and use reward-relevant representations for policy optimization. Experiments verify our theoretical results and show that our method achieves state-of-the-art performance in variants of the DeepMind Control Suite and RoboDesk. The basic limitation of this work is that the underlying latent processes are assumed to have no instantaneous causal relations but only time-delayed influences. This assumption does not hold true if the resolution of the time series is much lower than the causal frequency. We leave the identifiability of the latent dynamics in the presence of instantaneous causal and the extension of our method to heterogeneous environments to future work.

## 8 Acknowledgements

This work is supported by National Key Research and Development Program of China (2020AAA0107200), the Major Key Project of PCL (PCL2021A12), China Scholarship Council, NSF Grant 2229881, the National Institutes of Health (NIH) under Contract R01HL159805, a grant from Apple Inc., a grant from KDDI Research Inc., and generous gifts from Salesforce Inc., Microsoft Research, and Amazon Research. We thank Guangyi Chen, Weiran Yao and the anonymous reviewers for their support and helpful discussions on improving the paper.

Figure 6: Policy optimization on variants of DMC with various distractors.

## References

* [1] Le Chang and Doris Y Tsao. The code for facial identity in the primate brain. _Cell_, 169(6):1013-1028, 2017.
* [2] R Quian Quiroga, Leila Reddy, Gabriel Kreiman, Christof Koch, and Itzhak Fried. Invariant visual representation by single neurons in the human brain. _Nature_, 435(7045):1102-1107, 2005.
* [3] Jurgen Schmidhuber. On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models. _CoRR_, abs/1511.09249, 2015.
* [4] David Ha and Jurgen Schmidhuber. World models. _CoRR_, abs/1803.10122, 2018.
* [5] Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In _Proceedings of the 36th International Conference on Machine Learning_, pages 2555-2565, Long Beach, California, 2019.
* [6] Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In _Proceedings of the 8th International Conference on Learning Representations_, Addis Ababa, Ethiopia, 2020.
* [7] Danijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In _Proceedings of the 9th International Conference on Learning Representations_, Virtual Event, Austria, 2021.
* [8] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy P. Lillicrap. Mastering diverse domains through world models. _CoRR_, abs/2301.04104, 2023.
* [9] Tongzhou Wang, Simon Du, Antonio Torralba, Phillip Isola, Amy Zhang, and Yuandong Tian. Denoised mdps: Learning world models better than the world itself. In _Proceedings of the 39th International Conference on Machine Learning_, pages 22591-22612, Baltimore, Maryland, 2022.
* [10] Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, and Wulong Liu. A survey on interpretable reinforcement learning. _CoRR_, abs/2112.13112, 2021.
* [11] Xiang Fu, Ge Yang, Pulkit Agrawal, and Tommi S. Jaakkola. Learning task informed abstractions. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139, pages 3480-3491, Virtual Event, 2021.
* [12] Minting Pan, Xiangming Zhu, Yunbo Wang, and Xiaokang Yang. Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models. In _Advances in neural information processing systems_, volume 35, pages 23178-23191, New Orleans, Louisiana, 2022.
* [13] Biwei Huang, Chaochao Lu, Liu Leqi, Jose Miguel Hernandez-Lobato, Clark Glymour, Bernhard Scholkopf, and Kun Zhang. Action-sufficient state representation learning for control with structural constraints. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 9260-9279, Baltimore, Maryland, 2022.
* [14] Aapo Hyvarinen, Shohei Shimizu, and Patrik O. Hoyer. Causal modelling combining instantaneous and lagged effects: an identifiable model based on non-gaussianity. In _Proceedings of the 25th International Conference on Machine Learning_, volume 307, pages 424-431, Helsinki, Finland, 2008.
* [15] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. _Artificial intelligence_, 101(1-2):99-134, 1998.
* [16] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Ratsch, Sylvain Gelly, Bernhard Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97, pages 4114-4124, Long Beach, California, 2019.

* [17] Aapo Hyvarinen and Petterri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. _Neural networks_, 12(3):429-439, 1999.
* [18] Weiran Yao, Yuewen Sun, Alex Ho, Changyin Sun, and Kun Zhang. Learning temporally causal latent processes from general temporal data. In _Proceedings of the 10th International Conference on Learning Representations_, Virtual Event, 2022.
* [19] Weiran Yao, Guangyi Chen, and Kun Zhang. Temporally disentangled representation learning. In _Advances in Neural Information Processing Systems 35_, New Orleans, Louisiana, 2022.
* [20] Sebastien Lachapelle, Pau Rodriguez, Yash Sharma, Katie Everett, Remi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In _Proceedings of the 1st Conference on Causal Learning and Reasoning_, volume 177, pages 428-484, Eureka, CA, 2022.
* [21] Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng, Guangyi Chen, Petar Stojanov, Victor Akinwande, and Kun Zhang. Partial disentanglement for domain adaptation. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 11455-11472, Baltimore, Maryland, 2022.
* [22] Yujia Zheng, Ignavier Ng, and Kun Zhang. On the identifiability of nonlinear ica: Sparsity and beyond. In _Advances in Neural Information Processing Systems 35_, New Orleans, Louisiana, 2022.
* [23] Lingjing Kong, Biwei Huang, Feng Xie, Eric Xing, Yuejie Chi, and Kun Zhang. Identification of nonlinear latent hierarchical models. _CoRR_, abs/2306.07916, 2023.
* [24] Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. _Machine learning_, 37:183-233, 1999.
* [25] Naftali Tishby, Fernando C. N. Pereira, and William Bialek. The information bottleneck method. _CoRR_, physics/0004057, 2000.
* [26] Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information bottleneck. In _Proceedings of the 5th International Conference on Learning Representations_, Toulon, France, 2017.
* [27] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, R. Devon Hjelm, and Aaron C. Courville. Mutual information neural estimation. In _Proceedings of the 35th International Conference on Machine Learning_, volume 80, pages 530-539, Stockholm, Sweden, 2018.
* [28] Julius von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. In _Advances in Neural Information Processing Systems 34_, pages 16451-16467, Virtual Event, 2021.
* [29] Irina Higgins, Loic Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In _Proceedings of the 5th International Conference on Learning Representations_, Toulon, France, 2017.
* [30] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In _Proceedings of the 35th International Conference on Machine Learning_, volume 80, pages 2654-2663, Stockholm, Sweden, 2018.
* [31] David A. Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan M. Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. In _Proceedings of the 9th International Conference on Learning Representations_, Virtual Event, Austria, 2021.
* [32] Aapo Hyvarinen and Hiroshi Morioka. Nonlinear ICA of temporally dependent stationary sources. In _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics_, volume 54, pages 460-469, Fort Lauderdale, FL, 2017.

- Festschrift in Honor of Vladimir N. Vapnik_, pages 105-116, 2013.
* Wang [2022] Tongzhou Wang. Robodesk with a diverse set of distractors. https://github.com/SsnL/robodesk, 2022.
* Tassa et al. [2018] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller. Deepmind control suite. _CoRR_, abs/1801.00690, 2018.
* Deng et al. [2022] Fei Deng, Ingook Jang, and Sungjin Ahn. Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 4956-4975, Baltimore, Maryland, 2022.
* Zhang et al. [2021] Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In _Proceedings of the 9th International Conference on Learning Representations_, Virtual Event, Austria, 2021.
* Laskin et al. [2020] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: contrastive unsupervised representations for reinforcement learning. In _Proceedings of the 37th International Conference on Machine Learning_, volume 119, pages 5639-5650, Virtual Event, 2020.
* Lee et al. [2020] Kuang-Huei Lee, Ian Fischer, Anthony Z. Liu, Yijie Guo, Honglak Lee, John F. Canny, and Sergio Guadarrama. Predictive information accelerates learning in RL. In _Advances in Neural Information Processing Systems 33_, Virtual Event, 2020.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _Proceedings of the 35th International conference on machine learning_, pages 1861-1870, Stockholm, Sweden, 2018.
* Watter et al. [2015] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin A. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In _Advances in Neural Information Processing Systems 28_, pages 2746-2754, Quebec, Canada, 2015.
* Wahlstrom et al. [2015] Niklas Wahlstrom, Thomas B. Schon, and Marc Peter Deisenroth. From pixels to torques: Policy learning with deep dynamical models. _CoRR_, abs/1502.02251, 2015.
* Sermanet et al. [2018] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Time-contrastive networks: Self-supervised learning from video. In _Proceedings of the 35th International Conference on Robotics and Automation_, pages 1134-1141, Brisbane, Australia, 2018.
* van den Oord et al. [2018] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _CoRR_, abs/1807.03748, 2018.
* Yan et al. [2021] Wilson Yan, Ashwin Vangipuram, Pieter Abbeel, and Lerrel Pinto. Learning predictive representations for deformable objects using contrastive estimation. In _Proceedings of the 5th Conference on Robot Learning_, pages 564-574, 2021.
* Mazoure et al. [2020] Bogdan Mazoure, Remi Tachet des Combes, Thang Doan, Philip Bachman, and R. Devon Hjelm. Deep reinforcement and infomax learning. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33_, pages 3686-3698, Virtual Event, 2020.
* Zhu et al. [2022] Zheng-Mao Zhu, Shengyi Jiang, Yu-Ren Liu, Yang Yu, and Kun Zhang. Invariant action effect model for reinforcement learning. In _Proceedings of the 36th AAAI Conference on Artificial Intelligence_, pages 9260-9268, Virtual Event, 2022.
* Bharadhwaj et al. [2022] Homanga Bharadhwaj, Mohammad Babaeizadeh, Dumitru Erhan, and Sergey Levine. Information prioritization through empowerment in visual model-based RL. In _Proceedings of the Tenth International Conference on Learning Representations_, Virtual Event, 2022.

* [49] Arnav Kumar Jain, Shivakanth Sujit, Shruti Joshi, Vincent Michalski, Danijar Hafner, and Samira Ebrahimi Kahou. Learning robust dynamics through variational sparse gating. In _Advances in neural information processing systems_, volume 35, pages 1612-1626, New Orleans, Louisiana, 2022.
* [50] Tung D. Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal predictive coding for model-based planning in latent space. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139, pages 8130-8139, Virtual Event, 2021.
* [51] Thomas N. Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models. In _Proceedings of the 8th International Conference on Learning Representations_, Addis Ababa, Ethiopia, 2020.
* [52] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. _Proc. IEEE_, 109(5):612-634, 2021.
* [53] Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ICA. In _Advances in Neural Information Processing Systems 29_, pages 3765-3773, Barcelona, Spain, 2016.
* [54] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M. Asano, Taco Cohen, and Stratis Gavves. CITRIS: causal identifiability from temporal intervened sequences. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 13557-13603, Baltimore, Maryland, 2022.
* [55] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M Asano, Taco Cohen, and Efstratios Gavves. Causal representation learning for instantaneous and temporal effects in interactive systems. In _Proceedings of the 11th International Conference on Learning Representations_, 2023.
* [56] P. Spirtes, C. Glymour, and R. Scheines. _Causation, Prediction, and Search_. Spring-Verlag Lectures in Statistics, 1993.
* [57] J. Pearl. _Causality: Models, Reasoning, and Inference_. Cambridge University Press, Cambridge, 2000.
* [58] Julius von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems_, pages 16451-16467, Virtual Event, 2021.
* [59] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. _Journal of Machine Learning Research_, 22:268:1-268:8, 2021.

Theoretical Proofs

### Proof of Proposition 1

Proposition 1 shows that \(s^{\prime}_{t}\), which has directed paths to \(r_{t+\tau}\) (for \(\tau\geq 0\)), is minimally sufficient for policy learning that aims to maximize the future reward and can be characterized by conditional dependence with the cumulative reward variable \(R_{t}\).

**Proposition 1**.: _Under the assumption that the graphical representation, corresponding to the environment model, is Markov and faithful to the measured data, \(s^{\prime}_{t}\subseteq\mathbf{s_{t}}\) is a minimal subset of state dimensions that are sufficient for policy learning, and \(s_{i,t}\in s^{\prime}_{t}\) if and only if \(s_{i,t}\not\perp R_{t}|a_{t-1:t},s^{\prime}_{t-1}\)._

We first give the definitions of the Markov condition and the faithfulness assumption [56; 57], which will be used in the proof.

**Definition 1** (Global Markov Condition).: _The distribution \(p\) over a set of variables \(\mathbf{V}\) satisfies the global Markov property on graph \(G\) if for any partition \((A,B,C)\) such that if \(B\)\(d\)-separates \(A\) from \(C\), then \(p(A,C|B)=p(A|B)p(C|B)\)._

**Definition 2** (Faithfulness Assumption).: _There are no independencies between variables that are not entailed by the Markov Condition in the graph._

Below, we give the proof of Proposition 1.

Proof.: The proof contains the following three steps.

* In step 1, we show that a state dimension \(s_{i,t}\) is in \(s^{\prime}_{t}\), that is, it has a directed path to \(r_{t+\tau}\), if and only if \(s_{i,t}\not\perp R_{t}|a_{t-1:t},\mathbf{s}_{t-1}\).
* In step 2, we show that for \(s_{i,t}\) with \(s_{i,t}\not\perp R_{t}|a_{t-1:t},\mathbf{s}_{t-1}\), if and only if \(s_{i,t}\not\perp R_{t}|a_{t-1:t},s^{\prime}_{t-1}\).
* In step 3, we show that \(s^{\prime}_{t}\) are minimally sufficient for policy learning.

Step 1:We first show that if a state dimension \(s_{i,t}\) is in \(s^{\prime}_{t}\), then \(s_{i,t}\not\perp R_{t}|a_{t-1:t},\mathbf{s}_{t-1}\).

We prove it by contradiction. Suppose that \(s_{i,t}\) is independent of \(R_{t}\) given \(a_{t-1:t}\) and \(\mathbf{s}_{t-1}\). Then according to the faithfulness assumption, we can see from the graph that \(s_{i,t}\) does not have a directed path to \(r_{t+\tau}\), which contradicts the assumption, because, otherwise, \(a_{t-1:t}\) and \(\mathbf{s}_{t-1}\) cannot break the paths between \(s_{i,t}\) and \(R_{t}\) which leads to the dependence.

We next show that if \(s_{i,t}\not\perp R_{t}|a_{t-1:t},\mathbf{s}_{t-1}\), then \(s_{i,t}\in s^{\prime}_{t}\).

Similarly, by contradiction suppose that \(s_{i,t}\) does not have a directed path to \(r_{t+\tau}\). From the graph, it is easy to see that \(a_{t-1:t}\) and \(\mathbf{s}_{t-1}\) must d-separate the path between \(s_{i,t}\) and \(R_{t}\). According to the Markov assumption, \(s_{i,t}\) is independent of \(R_{t}\) given \(a_{t-1:t}\) and \(\mathbf{s}_{t-1}\), which contradicts to the assumption. Since we have a contradiction, it must be that \(s_{i,t}\) has a directed path to \(r_{t+\tau}\), i.e. \(s_{i,t}\in s^{\prime}_{t}\).

Step 2:In step 1, we have shown that \(s_{i,t}\not\perp R_{t}|a_{t-1:t},\mathbf{s}_{t-1}\), if and only if it has a directed path to \(r_{t+\tau}\). From the graph, it is easy to see that for those state dimensions which have a directed path to \(r_{t+\tau}\), \(a_{t-1:t}\) and \(\mathbf{s}_{t-1}\) cannot break the path between \(s_{i,t}\) and \(R_{t}\). Moreover, for those state dimensions which do not have a directed path to \(r_{t+\tau}\), \(a_{t-1:t}\) and \(s^{\prime}_{t-1}\) are enough to break the path between \(s_{i,t}\) and \(R_{t}\).

Therefore, for \(s_{i,t}\), \(s_{i,t}\not\perp R_{t}|a_{t-1:t},\mathbf{s}_{t-1}\), if and only if \(s_{i,t}\not\perp R_{t+1}|a_{t-1:t},s^{\prime}_{t-1}\).

Step 3:In the previous steps, it has been shown that if a state dimension \(s_{i,t}\) is in \(s^{\prime}_{t}\), then \(s_{i,t}\not\perp R_{t}|a_{t-1:t},s^{\prime}_{t-1}\), and if a state dimension \(s_{i,t}\) is not in \(s^{\prime}_{t}\), then \(s_{i,t}\perp R_{t}|a_{t-1:t},s^{\prime}_{t-1}\). This implies that \(s^{\prime}_{t}\) are minimally sufficient for policy learning to maximize the future reward. 

### Proof of Proposition 2

Moreover, the proposition below shows that \(s^{a}_{t}\), which receives an edge from \(a_{t-1}\), can be directly controlled by actions and can be characterized by conditional dependence with the action variable.

**Proposition 2**.: _Under the assumption that the graphical representation, corresponding to the environment model, is Markov and faithful to the measured data, \(s^{\prime}_{t}\subseteq\mathbf{s_{t}}\) is a minimal subset of state dimensions that are sufficient for direct control, and \(s_{i,t}\in s^{a}_{t}\) if and only if \(s_{i,t}\not\perp a_{t-1}|\mathbf{s_{t-1}}\)._Below, we give the proof of Proposition 2.

Proof.: The proof contains the following two steps.

* In step 1, we show that a state dimension \(s_{i,t}\) is in \(s_{t}^{a}\), that is, it receives an edge from \(a_{t-1}\), if and only if \(s_{i,t}\not\perp a_{t-1}|\mathbf{s_{t-1}}\).
* In step 2, we show that \(s_{t}^{a}\) contains a minimally sufficient subset of state dimensions that can be directly controlled by actions.

Step 1:We first show that if a state dimension \(s_{i,t}\) is in \(s_{t}^{a}\), then \(s_{i,t}\not\perp a_{t-1}|\mathbf{s_{t-1}}\).

We prove it by contradiction. Suppose that \(s_{i,t}\) is independent of \(a_{t-1}\) given \(\mathbf{s_{t-1}}\). Then according to the faithfulness assumption, we can see from the graph that \(s_{i,t}\) does not receive an edge from \(a_{t-1}\), which contradicts the assumption, because, otherwise, \(\mathbf{s_{t-1}}\) cannot break the paths between \(s_{i,t}\) and \(a_{t-1}\) which leads to the dependence.

We next show that if \(s_{i,t}\not\perp a_{t-1}|\mathbf{s_{t-1}}\), then \(s_{i,t}\in s_{t}^{a}\).

Similarly, by contradiction suppose that \(s_{i,t}\) does not receive an edge from \(a_{t-1}\). From the graph, it is easy to see that \(\mathbf{s_{t-1}}\) must break the path between \(s_{i,t}\) and \(a_{t-1}\). According to the Markov assumption, \(s_{i,t}\) is independent of \(a_{t-1}\) given \(\mathbf{s_{t-1}}\), which contradicts to the assumption. Since we have a contradiction, it must be that \(s_{i,t}\) has an edge from \(a_{t-1}\).

Step 2:In the previous steps, it has been shown that if a state dimension \(s_{i,t}\) is in \(s_{t}^{a}\), then \(s_{i,t}\not\perp a_{t-1}|\mathbf{s_{t-1}}\), and if a state dimension \(s_{i,t}\) is not in \(s_{t}^{a}\), then \(s_{i,t}\perp a_{t-1}|\mathbf{s_{t-1}}\). This implies that \(s_{t}^{a}\) is minimally sufficient for one-step direct control. 

### Proof of Proposition 3

Furthermore, based on Proposition 1 and Proposition 2, we can further differentiate \(s_{t}^{ar},s_{t}^{ar},s_{t}^{a^{\bar{r}}}\) from \(s_{t}^{r}\) and \(s_{t}^{a}\), which is given in the following proposition.

**Proposition 3**.: _Under the assumption that the graphical representation, corresponding to the environment model, is Markov and faithful to the measured data, we can build a connection between the graph structure and statistical independence of causal variables in the RL system, with (1) \(s_{i,t}\in s_{t}^{ar}\) if and only if \(s_{i,t}\not\perp R_{i}|a_{t-1},s_{t-1}^{r}\) and \(s_{i,t}\not\perp a_{t-1}|\mathbf{s_{t-1}}\), (2) \(s_{i,t}\in s_{t}^{ir}\) if and only if \(s_{i,t}\not\perp R_{i}|a_{t-1},s_{t-1}^{r}\) and \(s_{i,t}\perp a_{t-1}|\mathbf{s_{t-1}}\), (3) \(s_{i,t}\in s_{t}^{a^{\bar{r}}}\) if and only if \(s_{i,t}\perp R_{i}|a_{t-1},s_{t-1}^{r}\) and \(s_{i,t}\not\perp a_{t-1}|\mathbf{s_{t-1}}\), and (4) \(s_{i,t}\in s_{t}^{a^{\bar{r}}}\) if and only if \(s_{i,t}\perp R_{i}|a_{t-1},s_{t-1}^{r}\) and \(s_{i,t}\perp a_{t-1}|\mathbf{s_{t-1}}\)._

Proof.: This proposition can be easily proved by levering the results from Propositions 1 and 2. 

### Proof of Theorem 1

According to the causal process in the RL system (as described in Eq.1 in [13]), we can build the following mapping from latent state variables \(\mathbf{s}_{t}\) to observed variables \(o_{t}\) and future cumulative reward \(R_{t}\):

\[[o_{t},R_{t}]=f(s_{t}^{r},s_{t}^{r},\eta_{t}),\] (12)

where

\[\begin{array}{rcl}o_{t}&=&f_{1}(s_{t}^{r},s_{t}^{r}),\\ R_{t}&=&f_{2}(s_{t}^{r},\eta_{t}).\end{array}\] (13)

Here, note that to recover \(s_{t}^{r}\), it is essential to take into account all future rewards \(r_{t:T}\), because any state dimension \(s_{i,t}\in\mathbf{s}_{t}\) that has a directed path to the future reward \(r_{t+\tau}\), for \(\tau>0\), is involved in \(s_{t}^{r}\). Hence, we consider the mapping from \(s_{t}^{r}\) to the future cumulative reward \(R_{t}\), and \(\eta_{t}\) represents residuals, except \(s_{t}^{r}\), that have an effect to \(R_{t}\).

The following theorem shows that the different types of states \(s_{t}^{ar}\), \(s_{t}^{ir}\), \(s_{t}^{a^{\bar{r}}}\), and \(s_{t}^{a^{\bar{r}}}\) are blockwise identifiable from observed image variable \(o_{t}\), reward variable \(r_{t}\), and action variable \(a_{t}\), under reasonable and weak assumptions.

**Theorem 1**.: _Suppose that the causal process in the RL system and the four categories of latent state variables can be described as that in Section 2 and illustrated in Figure 1(c). Under the following assumptions1. _The mapping_ \(f\) _in Eq._ 12 _is smooth and invertible with smooth inverse._
2. _For all_ \(i\in\{1,\ldots,d_{o}+d_{R}\}\) _and_ \(j\in\mathcal{F}_{i,\cdot}\)_, there exist_ \(\{\bar{\mathbf{s}}_{i}^{(l)}\}_{l=1}^{|\mathcal{F}_{i,\cdot}|}\)_, so that_ \(\text{span}\{\mathbf{J}_{f}(\bar{\mathbf{s}}_{i}^{(l)})_{i:}\}_{l=1}^{| \mathcal{F}_{i,\cdot}|}=\mathbb{R}_{\mathcal{F}_{i,\cdot}^{\prime}}^{d_{i}}\)_, and there exists a matrix_ \(T\) _with its support identical to that of_ \(\mathbf{J}_{f}^{-1}(\hat{\mathbf{s}}_{i})\mathbf{J}_{f}(\bar{\mathbf{s}}_{i})\)_, so that_ \([\mathbf{J}_{f}(\bar{\mathbf{s}}_{i}^{(l)})T]_{j:}\in\mathbb{R}_{\mathcal{F}_ {i,\cdot}^{\prime}}^{d_{j}}\)_._

_Then, reward-relevant and controllable states \(s_{t}^{ar}\), reward-relevant but not controllable states \(s_{t}^{\bar{a}r}\), reward-irrelevant but controllable states \(s_{t}^{a\bar{a}}\), and noise \(s_{t}^{\bar{a}\bar{r}}\), are blockwise identifiable._

In the theorem presented above, Assumption \(A1\) only assumes the invertibility of function \(f\), while functions \(f_{1}\) and \(f_{2}\) are considered general and not necessarily invertible, as that in [23]. Since the function \(f\) is the mapping from all (latent) variables, including noise factors, that influence the observed variables, the invertibility assumption holds reasonably. However, note that it is not reasonable to assume the invertibility of the function \(f_{2}\) since usually, the reward function is not invertible. Assumption \(A2\), which is also given in [20, 22, 23], aims to establish a more generic condition that rules out certain sets of parameters to prevent ill-posed conditions. Specifically, it ensures that the Jacobian is not partially constant. This condition is typically satisfied asymptotically, and it is necessary to avoid undesirable situations where the problem becomes ill-posed.

Proof.: The proof consists of four steps.

1. In step 1, we show that \(s_{t}^{a}=s_{t}^{ar}\cup s_{t}^{a\bar{a}}\) is blockwise identifiable, by using the characterization that the action variable \(a_{t}\) only directly influences \(s_{t}^{ar}\) and \(s_{t}^{a\bar{a}}\).
2. In step 2, we show that \(s_{t}^{r}=s_{t}^{ar}\cup s_{t}^{\bar{a}\bar{r}}\) is blockwise identifiable, by using the characterization that the future cumulative reward \(R_{t}\) is only influenced by \(s_{t}^{ar}\) and \(s_{t}^{\bar{a}r}\).
3. In step 3, we show that \(s_{t}^{ar}\) is blockwise identifiable, by using the identifiability of \(s_{t}^{ar}\cup s_{t}^{a\bar{r}}\) and \(s_{t}^{a\bar{r}}\cup s_{t}^{a\bar{r}}\).
4. In step 4, we further show the blockwise identifiability of \(s_{t}^{\bar{a}r}\), \(s_{t}^{a\bar{r}}\), and \(s_{t}^{\bar{a}\bar{r}}\).

#### Step 1: prove the block identifiability of \(s_{t}^{a}\).

For simplicity of notation, below, we omit the subscript \(t\).

Let \(h:=f^{-1}\circ\hat{f}\). We have

\[\hat{\mathbf{s}}=h(\mathbf{s}),\] (14)

where \(h=f^{-1}\circ\hat{f}\) is the transformation between the true latent variable and the estimated one, and \(\hat{f}:\mathcal{S}\rightarrow\mathcal{X}\) denotes the estimated invertible generating function. Note that as both \(f^{-1}\) and \(\hat{f}\) are smooth and invertible, \(h\) and \(h^{-1}\) is smooth and invertible.

Since \(h(\cdot)\) is smooth over \(\mathcal{S}\), its Jacobian can be written as follows:

\[J_{h^{-1}}=\begin{bmatrix}A:=\frac{\partial\hat{x}^{a}}{\partial\hat{x}^{a}}& B:=\frac{\partial\hat{x}^{a}}{\partial\hat{x}^{a}}\\ C:=\frac{\partial\hat{x}^{a}}{\partial\hat{x}^{a}}&D:=\frac{\partial\hat{x}^{a}}{ \partial\hat{x}^{a}}\end{bmatrix}\] (15)

The invertibility of \(h^{-1}\) implies that \(J_{h^{-1}}\) is full rank. Since \(s^{a}\) has changing distributions over the action variable \(a\) while \(s^{a}\) has invariant distributions over different values of \(a\), we can derive that \(C=0\). Furthermore, because \(J_{h^{-1}}\) is full rank and \(C\) is a zero matrix, \(D\) must be of full rank, which implies \(h_{a}^{-1}\) is invertible, where \(h_{a}^{-1}\) denotes the first derivative of \(h_{a}^{-1}\). Therefore, \(s^{a}\) is blockwise identifiable up to invertible transformations.

#### Step 2: prove the blockwise identifiability of \(s_{t}^{r}\).

Recall that we have the following mapping:

\[[o_{t},R_{t}]=f(s_{t}^{r},s_{t}^{j},\eta_{t}),\]

where

\[\begin{array}{rcl}o_{t}&=&f_{1}(s_{t}^{r},s_{t}^{j}),\\ R_{t}&=&f_{2}(s_{t}^{r},\eta_{t}).\end{array}\]Note that here to recover \(s^{\prime}_{t}\), we need to take into account all future rewards, because \(s^{\prime}_{t}\) contains all those state dimensions that have a directed path to future rewards \(r_{t+1:T}\). \(\eta_{t}\) represents all other factors, except \(s^{\prime}_{t}\), that influence \(R_{t}\) at time instance \(t\). Further note here we assume the invertibility of \(f\), while \(f_{1}\) and \(f_{2}\) are general functions not necessarily invertible.

We denote by \(\mathbf{\hat{s}}=(s^{\prime},s^{\prime},\eta)\). We further denote by the dimension of \(s^{\prime}\) by \(d_{s^{\prime}}\), the dimension of \(s^{\prime}\) by \(d_{s^{\prime}}\), the dimension of \(\mathbf{\hat{s}}\) by \(d_{s}\), the dimension of \(o\) by \(d_{o}\), and the dimension of \(R_{t}\) by \(d_{R}\).

We denote by \(\mathcal{F}\) the support of \(\mathbf{J}_{j}(\mathbf{s})\), by \(\hat{\mathcal{F}}\) the support of \(\mathbf{J}_{j}(\mathbf{\hat{s}})\), and by \(\mathcal{T}\) the support of \(\mathbf{T}(\mathbf{s})\). We also denote \(T\) as a matrix with the same support as \(\mathcal{T}\). The proof technique mainly follows [23], as well as the required assumptions, and is also related to [20, 22].

Since \(h:=\hat{f}^{-1}\circ f\), we have \(\hat{f}=f\circ h^{-1}(\mathbf{\hat{s}})\). By applying the chain rule repeatedly, we have

\[\mathbf{J}_{j}(\mathbf{\hat{s}})=\mathbf{J}_{f}(\mathbf{\hat{s}})\cdot \mathbf{J}_{h^{-1}}(h(\mathbf{\hat{s}})).\] (16)

With Assumption A2, for any \(i\in\{1,\ldots,d_{o}+d_{R}\}\), there exists \(\{\mathbf{\hat{s}}^{(0)}\}_{i=1}^{|\mathcal{F}_{i,i}|}\), s.t. \(\text{span}(\{\mathbf{J}_{j}(\mathbf{\hat{s}}^{(0)})_{i:}\}_{i=1}^{|\mathcal{F }_{i,i}|})=\mathbb{R}_{\mathcal{F}_{i,i}}^{d_{R}}\). Since \(\{\mathbf{J}_{j}(\mathbf{\hat{s}}^{(0)})_{i:}\}_{i=1}^{|\mathcal{F}_{i,i}|}\) forms a basis of \(\mathbb{R}_{\mathcal{F}_{i,i}}^{d_{j}}\), for any \(j_{0}\in\mathcal{F}_{i,i}\), we can write canonical basis vector \(e_{j_{0}}\in\mathbb{R}_{\mathcal{F}_{i,i}}^{d_{j}}\) as:

\[e_{j_{0}}=\sum_{l\in\mathcal{F}_{i,i}}\alpha_{l}\cdot\mathbf{J}_{\mathbf{ \hat{s}}}(\mathbf{\hat{s}}^{(l)})_{i:},\] (17)

where \(\alpha_{l}\in\mathbb{R}\) is a coefficient.

Then, following Assumption A2, there exists a deterministic matrix \(T\) such that

\[T_{j_{0}:}=e_{j_{0}}^{\intercal}T=\sum_{i\in\mathcal{F}_{i,i}} \alpha_{l}\cdot\mathbf{J}_{\mathbf{\hat{s}}}(\mathbf{\hat{s}}^{(l)})_{i:}T \in\mathbb{R}_{\mathcal{F}_{i,i}}^{d_{j}},\] (18)

where \(\in\) is due to that each element in the summation belongs to \(\mathbb{R}_{\mathcal{F}_{i,i}}^{d_{j}}\).

Therefore,

\[\forall j\in\mathcal{F}_{i:,i},T_{j_{i}:}\in\mathbb{R}_{\mathcal{F}_{i,i}}^{d _{R}}.\]

Equivalently, we have:

\[\forall(i,j)\in\mathcal{F},\quad\{i\}\times T_{j_{i}:}\subset \hat{\mathcal{F}}.\] (19)

We would like to show that \(\hat{s}^{\prime}\) does not depend on \(s^{\prime}\) and \(\eta\), that is, \(T_{i,j}=0\) for \(i\in\{1,\ldots,d_{s^{\prime}}\}\) and \(j\in\{d_{s^{\prime}}+1,\ldots,d_{s}\}\).

We prove it by contradiction. Suppose that \(\hat{s}^{\prime}\) had dependence on \(s^{\prime}\), that is, \(\exists(j_{s^{\prime}},j_{s^{\prime}})\in\mathcal{T}\) with \(j_{s^{\prime}}\in\{1,\ldots,d_{s^{\prime}}\}\) and \(j_{s^{\prime}}\in\{d_{s^{\prime}}+1,\ldots,d_{s^{\prime}}+d_{s^{\prime}}\}\).

Hence, there must exist \(i_{r}\in\{d_{o}+1,\ldots,d_{o}+d_{R}\}\), such that, \((i_{r},j_{s^{\prime}})\in\mathcal{F}\).

It follows from Equation 19 that:

\[\{i_{r}\}\times\mathcal{T}_{j_{s^{\prime}}:}\in\hat{\mathcal{F}} \implies(i_{r},j_{s^{\prime}})\in\hat{\mathcal{F}}.\] (20)

However, due to the structure of \(\hat{f}_{2}\), \([\mathbf{J}_{\hat{f}_{2}}]_{i_{r}:,j_{s^{\prime}}}=0\), which results in a contradiction. Therefore, such \((i_{r},j_{s^{\prime}})\) does not exist and \(\hat{s}^{\prime}\) does not depend on \(s^{\prime}\). The same reasoning implies that \(\hat{s}^{\prime}\) does not dependent on \(\eta\). Thus, \(\hat{s}_{r}\) does not depend on \((s^{\prime},\eta)\). In conclusion, \(\hat{s}^{\prime}\) does not contain extra information beyond \(s^{\prime}\).

Similarly, we can show that \((\hat{s}^{\prime},\hat{\eta})\) does not contain information of \(s_{r}\).

Therefore, there is a one-to-one mapping between \(s^{\prime}\) and \(\hat{s}^{\prime}\).

**Step 3: prove the blockwise identifiability of \(s^{\prime\prime}_{t}\).**In Step 1 and Step 2, we have shown that both \(s^{a}\) and \(s^{\prime}\) are blockwise identifiable. That is,

\[\begin{array}{rcl}\hat{s}^{\prime}&=&h_{r}(s^{\prime}),\\ \hat{s}^{a}&=&h_{a}(s^{a}),\end{array}\] (21)

where \(h_{a}\) and \(h_{r}\) are invertible functions.

According to the invariance relation of \(s^{\prime a}\), We have the following relations:

\[\hat{s}^{\prime a}=h_{r}(s^{\prime})_{1:d,\nu^{a}}=h_{a}(s^{a})_{1:d,\nu^{a}}.\] (22)

It remains to show that both \(\tilde{h}_{r}:=h_{r}(\cdot)_{1:d,\nu^{a}}\) and \(\tilde{h}_{a}:=h_{a}(\cdot)_{1:d,\nu^{a}}\) do not depend on \(s^{\tilde{a}r}\) and \(s^{\tilde{a}\tilde{r}}\) in their arguments.

We will prove this by contradiction, following the proof technique in [58]. Without loss of generality, we suppose \(\exists l\in\{1,\cdots,d_{s^{\prime a}}\}\), \(s^{\prime*}\in\mathcal{S}^{r}\), s.t., \(\frac{\delta\tilde{h}_{r}}{\delta\tilde{s}^{\prime\tilde{a}}}(s^{\prime*})\neq 0\). As \(h\) is smooth, it has continuous partial derivatives. Thus, \(\frac{\delta\tilde{h}_{r}}{\delta\tilde{h}_{r}^{\prime\tilde{a}}}\neq 0\) holds true in a neighbourhood of \(s^{\prime*}\), i.e.,

\[\exists l\eta>0,\,s.t.,\,s^{\tilde{a}\tilde{r}}_{l}\rightarrow\tilde{h}_{r}( s^{\prime*},(s^{\tilde{a}\tilde{r}}_{-l},s^{\tilde{a}\tilde{r}}_{l}))\text{ is strictly monotonic on }(s^{\tilde{a}\tilde{r}*}_{l}-\eta,\,s^{\tilde{a}\tilde{r}*}_{l}+\eta),\] (23)

where \(s^{\tilde{a}\tilde{r}}_{-l}\) denotes variable \(s^{\tilde{a}\tilde{r}}\) excluding the dimension \(l\).

We further define an auxiliary function \(\psi:\mathcal{S}^{ar}\times\mathcal{S}^{\tilde{a}r}\times\mathcal{S}^{ \tilde{a}\tilde{r}}\rightarrow\mathbb{R}_{\geq 0}\) as follows:

\[\psi(s^{\prime a},s^{\tilde{a}r},s^{\tilde{a}\tilde{r}}):=|\tilde{h}_{r}(s^{ \prime})-\tilde{h}_{a}(s^{a})|.\] (24)

To obtain the contradiction to the invariance, it remains to show that \(\psi>0\) with a probability greater than zero w.r.t. the true generating process.

There are two situations at \((s^{\prime\prime a}\,,s^{\tilde{a}\tilde{r}*},s^{\prime\prime\prime})\) where \(s^{\tilde{a}\tilde{r}*}\) is an arbitrary point in \(\mathcal{S}^{\tilde{a}\tilde{r}}\):

* situation 1: \(\psi(s^{\prime\prime a}\,,s^{\tilde{a}\tilde{r}*},s^{\tilde{a}\tilde{r}})>0\);
* situation 2: \(\psi(s^{\prime\prime a}\,,s^{\tilde{a}\tilde{r}*},s^{\tilde{a}\tilde{r}})=0\).

In situation 1, we have identified a specific point \(\psi(s^{\prime\prime a}\,,s^{\tilde{a}\tilde{r}*},s^{\tilde{a}\tilde{r}*})\) that makes \(\psi>0\).

In situation 2, Eq. 23 implies that \(\forall s^{\tilde{a}\tilde{r}}_{l}\in(s^{\prime\prime a}_{l},s^{\prime\prime a }_{l}+\eta)\), \(\psi(s^{\prime\prime a}\,,(s^{\tilde{a}\tilde{r}\prime}_{-l},s^{\tilde{a} \tilde{r}}_{l}),s^{\tilde{a}\tilde{r}*})>0\).

Thus, in both situations, we can locate a point \((s^{\prime\prime a}\,,s^{\tilde{a}\tilde{r}^{\prime}},s^{\tilde{a}\tilde{r} *})\) such that \(\psi(s^{\prime\prime a}\,,s^{\tilde{a}\tilde{r}^{\prime}},s^{\tilde{a}\tilde{r} *})>0\), where \(s^{\tilde{a}\tilde{r}^{\prime}*}=s^{\tilde{a}\tilde{r}*}\) in situation 1 and \(s^{\tilde{a}\tilde{r}^{\prime}*^{\prime}}_{l}\in(s^{\prime\prime a}_{l},s^{ \prime\prime a}_{l}+\eta),s^{\prime\prime a}_{-l}=s^{\prime\prime a}_{-l}\) in situation 2.

Since \(\psi\) is a composition of continuous functions, it is continuous. As pre-image of open sets are always open for continuous functions, the open set \(\mathbb{R}_{>0}\) has an open set \(\mathcal{U}\in\mathcal{S}^{ar}\times\mathcal{S}^{\tilde{a}\tilde{r}}\times \mathcal{S}^{\tilde{a}\tilde{r}}\) as its preimage. Due to \((s^{\prime\prime a}\,,s^{\tilde{a}\tilde{r}^{\prime}},s^{\tilde{a}\tilde{r} *})\in\mathcal{U}\), \(\mathcal{U}\) is nonempty. Further, as \(\mathcal{U}\) is nonempty and open, \(\mathcal{U}\) has a Lebesgue measure of greater than zero.

As we assume that \(p_{s^{\prime\prime},s^{\prime\prime},s^{\prime\prime}}\) is fully supported over the entire domain \(\mathcal{S}^{ar}\times\mathcal{S}^{\tilde{a}\tilde{r}}\times\mathcal{S}^{ \tilde{a}\tilde{r}}\), we can deduce that \(\mathbb{P}_{p}[\mathcal{U}]>0\). That is, \(\psi>0\) with a probability greater than zero, which contradicts the invariance condition, Therefore, we can show that \(\hat{h}_{r}(s^{\prime})\) does not depend on \(s^{\tilde{a}\tilde{r}}\).

Similarly, we can show that \(\hat{h}_{a}(s^{\prime\prime})\) does not depend on \(s^{\tilde{a}\tilde{r}}\).

Finally, the smoothness and invertibility of \(\hat{h}_{r}\) and \(\hat{h}_{a}\) follow from the smoothness and invertibility of \(h_{r}\) and \(h_{a}\) over the entire domain.

Therefore, \(h_{r}(h_{a})\) is a smooth invertible mapping between \(s^{\prime\prime a}\) and \(\hat{s}^{\prime\prime a}\). That is, \(s^{\prime\prime\prime}\) is blockwise invertible.

**Step 4: prove the blockwise identifiability of \(s^{\tilde{a}\tilde{r}}_{t}\), \(s^{\tilde{a}\tilde{r}}_{t}\), and \(s^{\tilde{a}\tilde{r}}_{t}\).**

We can use the same technique in Step 3 to show the identifiability of \(s^{\tilde{a}\tilde{r}}\) and \(s^{\tilde{a}\tilde{r}}\). Specifically, since \(s_{t}\) and \(s^{\prime\prime a}\) are identifiable, we can show that \(s^{\tilde{a}\tilde{r}}\) is identifiable. Similarly, since \(s^{a}\) and \(s^{\prime\prime\prime}\) are identifiable, we can show that \(s^{\tilde{a}\tilde{r}}\) is identifiable. Furthermore, since \(s^{\prime\prime\prime}\), \(s^{\tilde{a}\tilde{r}}\), and \(s^{\tilde{a}\tilde{r}}\) are identifiable, we can show that \(s^{\tilde{a}\tilde{r}}\) is identifiable Derivation of the Objective Function

We start by defining the components of the world mode as follows:

\[\left\{\begin{array}{ll}\textit{Observation Model:}&p_{\phi}(o_{t}\mid\mathbf{s_{t}})\\ \textit{Reward Model:}&p_{\theta}(r_{t}\mid s_{t}^{r})\\ \textit{Transition Model:}&p_{\gamma}(\mathbf{s_{t}}\mid\mathbf{s_{t-1}},a_{t-1}) \\ \textit{Representation Model:}&q_{\phi}(\mathbf{s_{t}}\mid o_{t},\mathbf{s_{t-1}},a_{t-1}) \end{array}\right.\] (25)

The latent dynamics can be disentangled into four categories:

\[\begin{array}{ll}\textit{Disentangled Transition Model:}&\textit{ Disentangled Representation Model:}\\ \left\{\begin{array}{ll}p_{\gamma_{1}}(s_{t}^{\prime\prime}\mid s_{t-1}^{r},a_{t-1}) \\ p_{\gamma_{2}}(s_{t}^{\prime\prime}\mid s_{t-1}^{r})\\ p_{\gamma_{3}}(s_{t}^{\prime\prime}\mid\mathbf{s_{t-1}},a_{t-1})\\ p_{\gamma_{4}}(s_{t}^{\prime\prime}\mid\mathbf{s_{t-1}})\end{array}\right.& \left(\begin{array}{ll}q_{\phi_{1}}(s_{t}^{\prime\prime}\mid o_{t},s_{t-1}^{r },a_{t-1})\\ q_{\phi_{2}}(s_{t}^{\prime\prime}\mid o_{t},s_{t-1}^{r})\\ q_{\phi_{3}}(s_{t}^{\prime\prime}\mid o_{t},\mathbf{s_{t-1}},a_{t-1})\\ q_{\phi_{4}}(s_{t}^{\prime\prime}\mid o_{t},\mathbf{s_{t-1}})\end{array}\right. \end{array}\] (26)

We follow the derivation framework in Dreamer [6] and define the information bottleneck objective for latent dynamics models [25]

\[\max I\left(\mathbf{s_{1:T}};(o_{1:T},r_{1:T})\mid a_{1:T}\right)-\beta\ \cdot\ I \left(\mathbf{s_{1:T}},i_{1:T}\mid a_{1:T}\right),\] (27)

where \(\beta\) is scalar and \(i_{t}\) are dataset indices that determine the observations \(p(o_{t}\mid i_{t})=\delta(o_{t}-\bar{o}_{t})\) as in [26]. Maximizing the objective leads to model states that can predict the sequence of observations and rewards while limiting the amount of information extracted at each time step. We derive the lower bound of the first term in Equation 27:

\[\begin{array}{ll}&I\left(\mathbf{s_{1:T}};(o_{1:T},r_{1:T})\mid a_{1:T}\right) \\ =&\mathrm{E}_{q(o_{1:T},r_{1:T},s_{1:T},a_{1:T})}\left(\sum_{t}\ln p \left(o_{1:T},r_{1:T}\mid\mathbf{s_{1:T}},a_{1:T}\right)-\frac{\ln p\left(o_{ 1:T},r_{1:T}\mid a_{1:T}\right)\right)}{\mathrm{const}}\right)\\ \pm&\mathrm{E}\left(\sum_{t}\ln p\left(o_{1:T},r_{1:T}\mid\mathbf{s_{1:T}},a_{ 1:T}\right)\right)\\ \geq&\mathrm{E}\left(\sum_{t}\ln p\left(o_{1:T},r_{1:T}\mid\mathbf{s_{1:T}},a_ {1:T}\right)\right)-\mathrm{KL}\left(p\left(o_{1:T},r_{1:T}\mid\mathbf{s_{1:T} },a_{1:T}\right)\parallel\prod_{t}p_{\theta}\left(o_{t}\mid s_{t}\right)p_{ \theta}\left(r_{t}\mid s_{t}^{r}\right)\right)\\ =&\mathrm{E}\left(\sum_{t}\ln p_{\theta}\left(o_{t}\mid\mathbf{s_{t}}\right)+ \ln p_{\theta}\left(r_{t}\mid s_{t}^{r}\right)\right).\end{array}\] (28)

Thus, we obtain the objective function:

\[\mathcal{J}_{\mathrm{O}}^{t}=\ln p_{\theta}\left(o_{t}\mid\mathbf{s_{t}}\right) \quad\mathcal{J}_{\mathrm{R}}^{t}=\ln p_{\theta}\left(r_{t}\mid s_{t}^{r}\right)\] (29)

For the second term in Equation 27, we use the non-negativity of the KL divergence to obtain an upper bound,

\[\begin{array}{ll}&I\left(\mathbf{s_{1:T}};i_{1:T}\mid a_{1:T}\right)\\ =&\mathrm{E}_{q(o_{1:T},r_{1:T},s_{1:T},a_{1:T},i_{1:T})}\left(\sum_{t}\ln q \left(\mathbf{s_{t}}\mid\mathbf{s_{t-1}},a_{t-1},i_{t}\right)-\ln p\left( \mathbf{s_{t}}\mid\mathbf{s_{t-1}},a_{t-1}\right)\right)\\ =&\mathrm{E}\left(\sum_{t}\ln q_{\phi}\left(\mathbf{s_{t}}\mid\mathbf{s_{t-1}}, a_{t-1},o_{t}\right)-\ln p\left(\mathbf{s_{t}}\mid\mathbf{s_{t-1}},a_{t-1}\right)\right)\\ \leq&\mathrm{E}\left(\sum_{t}\ln q_{\phi}\left(\mathbf{s_{t}}\mid\mathbf{s_{t-1} },a_{t-1},o_{t}\right)-\ln p_{\gamma}\left(\mathbf{s_{t}}\mid\mathbf{s_{t-1}},a _{t-1}\right)\right)\\ =&\mathrm{E}\left(\sum_{t}\mathrm{KL}\left(q_{\phi}\left(\mathbf{s_{t}}\mid \mathbf{s_{t-1}},a_{t-1},o_{t}\right)\parallel p_{\gamma}\left(\mathbf{s_{t}} \mid\mathbf{s_{t-1}},a_{t-1}\right)\right)\right).\end{array}\] (30)According to equation 26, we have \(p_{\gamma}=p_{\gamma_{1}}\cdot p_{\gamma_{2}}\cdot p_{\gamma_{3}}\cdot p_{\gamma_{4}}\) and \(q_{\phi}=q_{\phi_{1}}\cdot q_{\phi_{2}}\cdot q_{\phi_{3}}\cdot q_{\phi_{4}}\).

\[\begin{split}\text{KL}(q_{\phi}\parallel p_{\gamma})& =\text{KL}(q_{\phi_{1}}\cdot q_{\phi_{2}}\cdot q_{\phi_{3}}\cdot q_{\phi_{4}} \parallel p_{\gamma_{1}}\cdot p_{\gamma_{2}}\cdot p_{\gamma_{3}}\cdot p_{\gamma _{4}})\\ &=E_{q_{\phi}}\left(\ln\frac{q_{\phi_{1}}}{p_{\gamma_{1}}}+ln\frac{ q_{\phi_{2}}}{p_{\gamma_{2}}}+ln\frac{q_{\phi_{3}}}{p_{\gamma_{3}}}+ln\frac{q_{\phi_{4}}}{p_{ \gamma_{4}}}\right)\\ &=\text{KL}(q_{\phi_{1}}\parallel p_{\gamma_{1}})+\text{KL}(q_{ \phi_{2}}\parallel p_{\gamma_{2}})+\text{KL}(q_{\phi_{3}}\parallel p_{\gamma_{ 3}})+\text{KL}(q_{\phi_{4}}\parallel p_{\gamma_{4}})\end{split}\] (31)

We introduce additional hyperparameters to regulate the amount of information contained within each category of variables:

\[\mathcal{J}_{\text{D}}^{t}=-\beta_{1}\cdot\text{KL}\left(q_{\phi_{1}} \parallel p_{\gamma_{1}}\right)-\beta_{2}\cdot\text{KL}\left(q_{\phi_{2}} \parallel p_{\gamma_{2}}\right)-\beta_{3}\cdot\text{KL}\left(q_{\phi_{3}} \parallel p_{\gamma_{3}}\right)-\beta_{4}\cdot\text{KL}\left(q_{\phi_{4}} \parallel p_{\gamma_{4}}\right).\] (32)

Additionally, we introduce two supplementary objectives to explicitly capture the distinctive characteristics of the four distinct representation categories. Specifically, we characterize the reward-relevant representations by measuring the dependence between \(s_{t}^{r}\) and \(R_{t}\), given \(a_{t-1:t}\) and \(s_{t-1}^{r}\), that is \(I(s_{t}^{r},R_{t}\mid a_{t-1:t},s_{t-1}^{r})\) (see Figure 15(a). Note that if the action \(a_{t}\) is not dependent on \(s_{t}^{r}\), such as when it is randomly chosen. \(a_{t}\) does not need to be conditioned. In this case, the mutual information turns into \(I(s_{t}^{r},R_{t}\mid a_{t-1},s_{t-1}^{r})\). To ensure that \(s_{t}^{r}\) are minimally sufficient for policy training, we maximize \(I(s_{t}^{r},R_{t}\mid a_{t-1:t},s_{t-1}^{r})\) while minimizing \(I(s_{t}^{r},R_{t}\mid a_{t-1:t},s_{t-1}^{r})\) to discourage the inclusion of redundant information in \(s_{t}^{r}\) concerning the rewards:

\[I(s_{t}^{r};R_{t}\mid a_{t-1:t},s_{t-1}^{r})-I(s_{t}^{r};R_{t}\mid a_{t-1:t},s _{t-1}^{r}).\] (33)

The conditional mutual information can be expressed as the disparity between two mutual information values.

\[\begin{split} I(s_{t}^{r};R_{t}\mid a_{t-1:t},s_{t-1}^{r})& =I(R_{t};s_{t}^{r},a_{t-1:t},s_{t-1}^{r})-I(R_{t};a_{t-1:t},s_{t-1} ^{r}),\\ I(s_{t}^{r};R_{t}\mid a_{t-1:t},s_{t-1}^{r})&=I(R _{t};s_{t}^{r},a_{t-1:t},s_{t-1}^{r})-I(R_{t};a_{t-1:t},s_{t-1}^{r}).\end{split}\] (34)

Combining the above two equations, we eliminated the identical terms, ultimately yielding the following formula

\[I(R_{t};s_{t}^{r},a_{t-1:t},s_{t-1}^{r})-I(R_{t};s_{t}^{r},a_{t-1:t},s_{t-1}^{ r}).\] (35)

We use the Donsker-Varadhan representation to express mutual information as a supremum over functions,

\[\begin{split} I(X;Y)&=D_{KL}(p(x,y)\parallel p(x )p(y))\\ &=\sup_{T\in\mathcal{T}}\mathbb{E}_{p(x,y)}[T(x,y)]-\log\mathbb{E }_{p(x)p(y)}[\epsilon^{T(x,y)}].\end{split}\] (36)

We employ mutual information neural estimation [27] to approximate the mutual information value. We represent the function \(T\) using a neural network that accepts variables \((x,y)\) as inputs and is parameterized by \(\alpha\). The neural network is optimized through stochastic gradient ascent to find the supremum. Substituting \(x\) and \(y\) with variables defined in Equation 35, our objective is reformulated as follows:

\[\mathcal{J}_{\text{RS}}^{t}=\lambda_{1}\cdot\{I_{\alpha_{1}}(R_{t};s_{t}^{r},a _{t-1:t},\mathbf{sg}(s_{t-1}^{r}))-I_{\alpha_{2}}(R_{t};s_{t}^{r},a_{t-1:t}, \mathbf{sg}(s_{t-1}^{r}))\}.\] (37)

To incorporate the conditions from the original objective, we apply the stop_gradient operation to the variable \(s_{t-1}^{r}\). Similarly, to ensure that the representations \(s_{t}^{s}\) are directly controllable by actions, while \(s_{t}^{\tilde{a}}\) are not, we maximize the following objective:

\[I(s_{t}^{a};a_{t-1}\mid\mathbf{s_{t-1}})-I(s_{t}^{\tilde{a}},a_{t-1}\mid \mathbf{s_{t-1}}),\] (38)

By splitting the conditional mutual information and eliminating identical terms, we obtain the following objective function:

\[\mathcal{J}_{\text{AS}}^{t}=\lambda_{2}\cdot\{I_{\alpha_{3}}(a_{t-1};s_{t}^{a}, \mathbf{sg}(\mathbf{s_{t-1}}))-I_{\alpha_{4}}(a_{t-1};s_{t}^{\tilde{a}}, \mathbf{sg}(\mathbf{s_{t-1}}))\}.\] (39)

where \(\alpha_{1},\alpha_{2},\alpha_{3},\alpha_{4}\) can be obtained by maximizing Equation 36. Intuitively, these two objective functions ensure that \(s_{t}^{r}\) is predictive of the reward, while \(s_{t}^{r}\) is not; similarly, \(s_{t}^{a}\) can be predicted by the action, whereas \(s_{t}^{\tilde{a}}\) cannot.

Combine the equation 29, equation 32, equation 37 and equation 39, the total objective function is:

\[\begin{split}\mathcal{J}_{\text{TOTAL}}&=\max_{\theta, \theta,\gamma,\alpha_{1,\alpha_{2}}}\min_{\alpha_{2}\alpha_{4}}\mathds{E}_{q_{ \theta}}\left(\sum_{t}\left(\mathcal{J}_{\text{O}}^{t}+\mathcal{J}_{\text{R} }^{t}+\mathcal{J}_{\text{D}}^{t}+\mathcal{J}_{\text{RS}}^{t}+\mathcal{J}_{ \text{AS}}^{t}\right)\right)+\text{ const}\\ &=\max_{\theta,\gamma,\sigma,\alpha_{1,\alpha_{2}}}\min_{\alpha_{ 2},\alpha_{4}}\mathbb{E}_{q_{\theta}}[\,\log p_{\theta}(o_{t}\,|\,\mathbf{s}_{ t})+\log p_{\theta}(r_{t}\,|\,s_{t}^{r})\\ &\qquad-\sum_{i=1}^{4}\beta_{i}\cdot\text{KL}(q_{\phi_{i}}\,\|\,p_ {\gamma_{i}})+\lambda_{1}\cdot(I_{\alpha_{1}}-I_{\alpha_{2}})+\lambda_{2}\cdot( I_{\alpha_{3}}-I_{\alpha_{4}})\}+\text{ const}\.\end{split}\] (40)

The expectation is computed over the dataset and the representation model. Throughout the model learning process, the objectives for estimating mutual information and learning the world model are alternately optimized.

### Discussions

In this subsection, we examine the mutual information constraints in equation 37 and equation 39 and their relationship with other objectives. Our findings reveal that while other objectives partially fulfill the desired functionality of the mutual information constraints, incorporating both mutual information objectives is essential for certain environments.

The objective functions can be summarized as follows::

\[\begin{split}\mathcal{J}_{\text{O}}^{t}&=\ln p_{ \theta}\left(o_{t}\,|\,\mathbf{s}_{t}\right),\quad\mathcal{J}_{\text{R}}^{t}= \ln p_{\theta}\left(r_{t}\,|\,s_{t}^{r}\right),\quad\mathcal{J}_{\text{D}}^{t} =-\text{KL}\left(q_{\phi}\,\|\,p_{\gamma}\right),\\ \mathcal{J}_{\text{RS}}^{t}&=\lambda_{1}\cdot[I_{ \alpha_{1}}(R_{t};s_{t}^{r},a_{t-1:\pi},\mathbf{sg}(s_{t-1}^{r}))-I_{\alpha_{ 2}}(R_{t};\,\tilde{s}_{t}^{r},a_{t-1:\pi},\mathbf{sg}(s_{t-1}^{r}))],\\ \mathcal{J}_{\text{AS}}^{t}&=\lambda_{2}\cdot[I_{ \alpha_{3}}(a_{t-1:},s_{t}^{r},\mathbf{sg}(\mathbf{s_{t-1}}))-I_{\alpha_{4}}(a _{t-1:},\tilde{s}_{t}^{\bar{a}},\mathbf{sg}(\mathbf{s_{t-1}}))],\end{split}\] (41)

and the KL divergence term can be further decomposed into 4 components:

\[\begin{split}\mathcal{J}_{D_{1}}^{t}&=-\beta_{1} \cdot\text{KL}(q_{\phi_{1}}\left(s_{t}^{\pi r}\,|\,\,o_{t},s_{t-1}^{r},a_{t-1} \right)\,\|\,p_{\gamma_{1}}\left(s_{t}^{\pi r}\,|\,\,s_{t-1}^{r},a_{t-1} \right))\\ \mathcal{J}_{D_{2}}^{t}&=-\beta_{2}\cdot\text{KL}(q _{\phi_{2}}\left(\tilde{s}_{t}^{\bar{a}r}\,|\,\,o_{t},s_{t-1}^{r}\right)\,\|\,p_ {\gamma_{2}}\left(\tilde{s}_{t}^{\bar{a}r}\,|\,\,s_{t-1}^{r}\right))\\ \mathcal{J}_{D_{3}}^{t}&=-\beta_{3}\cdot\text{KL}(q _{\phi_{3}}\left(s_{t}^{\bar{a}r}\,|\,\,o_{t},\mathbf{s_{t-1}}\right)\,\|\,p_ {\gamma_{3}}\left(s_{t}^{\bar{a}r}\,|\,\,\mathbf{s_{t-1}},a_{t-1}\right))\\ \mathcal{J}_{D_{4}}^{t}&=-\beta_{4}\cdot\text{KL}(q _{\phi_{4}}\left(\tilde{s}_{t}^{\bar{a}r}\,|\,\,o,\mathbf{s_{t-1}}\right)\,\|\,p_ {\gamma_{4}}\left(\tilde{s}_{t}^{\bar{a}r}\,|\,\,\mathbf{s_{t-1}}\right)). \end{split}\] (42)

Specifically, maximizing \(I_{\alpha_{1}}\) in \(\mathcal{J}_{\text{RS}}^{t}\) enhances the predictability of \(R_{t}\) based on the current state \(s_{t-1}^{r}\) conditioning on \((s_{t-1}^{r},a_{t-1:\pi})\). However, notice that this objective can be partially accomplished by optimizing \(\mathcal{J}_{\text{R}}^{t}\). When learning the world model, both the transition function and the reward function are trained: the reward function predicts the current reward \(r_{t}\) using \(s_{t}^{r}\), while the transition model predicts the next state. These combined predictions contribute to the overall prediction of \(R_{t}\).

Minimizing \(I_{\alpha_{2}}\) in \(\mathcal{J}_{\text{RS}}^{t}\) eliminates extraneous reward-related information present in \(s_{t}^{r}\). According to our formulation, \(s_{t}^{r}\) can still be predictive of \(R_{t}\) as long as it does not introduce additional predictability beyond what is already captured by \((s_{t-1}^{r},a_{t-1:\pi})\). This is because we only assume that \(s_{t}^{r}\) is **conditionally** independent from \(R_{t}\) when conditioning on \((s_{t-1}^{r},a_{t-1:\pi})\). If we don't condition on \((s_{t-1}^{r},a_{t-1:\pi})\), it introduces \(s_{t-1}^{r}\) as the confounding factor between \(s_{t}^{r}\) and \(R_{t}\), establishing association between \(s_{t}^{r}\) and \(R_{t}\) (refer to Figure 15(a)). Note that the KL divergence constraints govern the information amount within each state category. By amplifying the weight of the KL constraints on \(s_{t}^{r}\), the value of \(I_{\alpha_{2}}\) can indirectly be diminished.

By maximizing \(I_{\alpha_{3}}\) and minimizing \(I_{\alpha_{4}}\) in \(\mathcal{J}_{\text{AS}}^{t}\), we ensure that \(s_{t}^{a}\) can be predicted based on \((a_{t-1},\mathbf{s_{t-1}})\) while \(s_{t}^{\bar{a}}\) cannot. The KL constraints on \(s_{t}^{\pi r}\) and \(s_{t}^{\bar{a}r}\) incorporate the action \(a_{t-1}\) into the prior and posterior, implicitly requiring that \(s_{t}^{a}\) should be predictable given \(a_{t-1}\). Conversely, the KL constraints on \(s_{t}^{\bar{a}r}\) and \(s_{t}^{\bar{a}r}\) do not include the action \(a_{t-1}\) in the prior and posterior, implicitly requiring that \(s_{t}^{a}\) should not be predictable based on \(a_{t-1}\). However, relying solely on indirect constraints can sometimes be ineffective, as it may lead to entangled representations that negatively impact policy performance (see Figure 13).

Ablation of the mutual information constraints.The inclusion of both \(\mathcal{J}_{\text{RS}}^{t}\) and \(\mathcal{J}_{\text{AS}}^{t}\) is essential in certain environments to promote disentanglement and enhance policy performance, despite sharingsome common objectives. We have observed improved training stability in the variant of Robodesk environment (see Figure 12) and significant performance gains in the Reacher environment with video background and camera jittering (see Figure 7). When two mutual information objectives are removed, we notice that entangled representations emerged in these environments, as depicted in Figure 13. We assign values of 0.1 to \(\lambda_{1}\) and \(\lambda_{2}\) in the environment of modified Cartpole, variant of Robodesk and Reacher with video background and jittering camera. Empirically, a value of 0.1 has been found to be preferable for both \(\lambda_{1}\) and \(\lambda_{2}\). Using a higher value for regularization might negatively impact the learning of representation and transition model. In other DMC environments, the ELBO loss alone has proven effective due to the inherent structure of our disentangled latent dynamics. The choice of hyperparameters \((\beta_{1},\beta_{2},\beta_{3},\beta_{4})\) depends on the specific goals of representation learning and the extent of noise interference in the task. If the objective is to accurately recover the true latent variables for understanding the environment, it is often effective to assign equal weights to the four KL divergence terms (for experiments on synthetic data and modified cartpole). When the aim is to enhance policy training stability by mitigating noise, it is recommended to set the values of \(\beta_{1}\) and \(\beta_{2}\) higher than \(\beta_{3}\) and \(\beta_{4}\) (for experiments on variants of Robodesk and DMC). Moreover, in environments with higher levels of noise, it is advisable to increase the discrepancy in values between the hyperparameters.

## Appendix C Algorithm

```
0: Representation model: \(q_{\phi}(s_{r}\mid s_{r-1},a_{r-1},o_{s})=q_{\phi_{1}}\cdot q_{\phi_{2}}\cdot q _{\phi_{3}}\cdot q_{\phi_{4}}\);  Transition model: \(p_{\gamma}(s_{r}\mid s_{r-1},a_{r-1})=p_{\gamma_{1}}\cdot p_{\gamma_{2}}\cdot p _{\gamma_{3}}\cdot p_{\gamma_{4}}\);  Observation Model: \(p_{\theta}(o_{r}\mid s_{r})\); Reward model: \(p_{\theta}(r_{r}\mid s_{r}^{\prime})\);  Policy Model: \(\pi_{\phi}(a_{r}\mid s_{r}^{\prime})\);  Value model: \(\psi_{\phi}(s_{r}^{\prime})\);  Mutual information estimator: \(I_{a_{1}},I_{a_{2}},I_{a_{3}},I_{a_{4}}\);  Policy optimization algorithm Pi-Opt, which is in default the same as that used in Dreamer.
0: Transition Model \(p_{\gamma}\). Representation Model \(q_{\phi}\). Policy Model \(\pi_{\phi}\).
1:while training do
2: // Exploration
3: Collect new trajectories \(D^{\prime}=\{(o_{r},a_{r},r_{i})\}\), with \(\pi\) acting on \(q_{\phi}\) encoded outputs
4: Add experience to the replay buffer \(D=D\cup D^{\prime}\)
5: // Model learning
6: Sample a batch of data sequences \(\{(o_{r},a_{r},r_{r})\}\), from the reply buffer \(D\)
7: Obtain \(s_{r}\) by the representation model and estimate mutual information terms \(I_{a_{1}},I_{a_{2}},I_{a_{3}},I_{a_{4}}\)
8:for\(i=1\) to \(n\)do
9: Train mutual information neural estimators by maximizing \(I_{a_{1}},I_{a_{2}},I_{a_{3}},I_{a_{4}}\)
10:endfor
11: Freeze the parameters in \(I_{a_{1}},I_{a_{2}},I_{a_{3}},I_{a_{4}}\) and Train \(q_{\phi}\), \(p_{\gamma}\) and \(p_{\theta}\) with Equation 11
12: // Policy learning by dreaming
13: Imagine trajectories of \(s_{r}^{\prime}\) using the learned world model.
14: Train \(\pi_{\phi}\) and \(v_{\phi}\) by running Pi-Opt
15:endwhile ```

**Algorithm 1**IFactor

Figure 7: Ablation of the mutual information constraints in the Reacher environment with video background and jittering camera. The dashed brown line illustrates the policy performance of Denoised MDP after 1 million environment steps.

Environment Descriptions

### Synthetic Data

For the sake of simplicity, we consider one lag for the latent processes in Section 4. Our identifiability proof can actually be applied for arbitrary lags directly because the identifiability does not rely on the number of previous states. We extend the latent dynamics of the synthetic environment to incorporate a general time-delayed causal effect with \(\tau\geq 1\) in the synthetic environment. When \(\tau=1\), it reduces to a common MDP. The ground-truth generative model of the environment is as follows::

\[\left\{\begin{array}{ll}\textit{Observation Model:}&p_{\theta}\left(o_{t} \mid\mathbf{s_{t}}\right)\\ \textit{Reward Model:}&p_{\theta}\left(r_{t}\mid s_{t}^{\prime}\right)\\ \textit{Transition Model:}&p_{\gamma}\left(\mathbf{s_{t}}\mid\mathbf{s_{t-\tau \mathbf{t-1}}},a_{t-\tau\mathbf{t-1}}\right)\end{array}\right.\quad\textit{Transition :}\left\{\begin{array}{ll}p_{\gamma_{t}}\left(s_{t}^{\prime\prime}\mid s_{t- \tau t-1}^{\prime},a_{t-\tau t-1}^{\prime}\right)\\ p_{\gamma_{t}}\left(s_{t}^{\prime\prime}\mid s_{t-\tau\mathbf{t-1}}^{\prime},a_ {t-\tau t-1}\right)\\ p_{\gamma_{t}}\left(s_{t}^{\prime\prime}\mid\mathbf{s_{t-\tau\mathbf{t-1}}},a _{t-\tau t-1}\right)\\ p_{\gamma_{t}}\left(s_{t}^{\prime\prime}\mid\mathbf{s_{t-\tau\mathbf{t-1}}} \right)\\ \end{array}\right.\] (43)

Data GenerationWe generate synthetic datasets with 100, 000 data points according to the generating process in Equation 43, which satisfies the identifiability conditions stated in Theorem 1. The latent variables \(\mathbf{s_{t}}\) have 8 dimensions, where \(s_{t}^{\prime\prime}=s_{t}^{\prime\prime}=s_{t}^{\prime\prime\prime}=s_{t}^{ \prime\prime\prime}=2\). At each timestep, a one-hot action of dimension 5, denoted as \(a_{t}\), is taken. The lag number of the process is set to \(\tau=2\). The observation model \(p_{\theta}\left(o_{t},\mid\mathbf{s_{t}}\right)\) is implemented using a random three-layer MLPs with LeakyReLU units. The reward model \(p_{\theta}\left(r_{r},\mid s_{t}^{\prime}\right)\) is represented by a random one-layer MLP. It's worth noting that the reward model is not invertible due to the scalar nature of \(r_{t}\). Four distinct transition functions, namely \(p_{\gamma_{t}}\), \(p_{\gamma_{t}}\), \(p_{\gamma_{t}}\), and \(p_{\gamma_{t}}\), are employed and modeled using random one-layer MLP with LeakyReLU units. The process noise is sampled from an i.i.d. Gaussian distribution with a standard deviation of \(\sigma=0.1\). To simulate nonstationary noise for various latent variables in RL, the process noise terms are coupled with the historical information by multiplying them with the average value of all the time-lagged latent variables, as suggested in [19].

### Modified Cartpole

We have modified the original Cartpole environment by introducing two distractors. The first distractor is an uncontrollable Cartpole located in the upper portion of the image, which does not affect the rewards. The second distractor is a controllable green light positioned below the reward-relevant Cartpole in the lower part of the image, but it is not associated with any rewards. The task-irrelevant cartpole undergoes random actions at each time step and stops moving when its angle exceeds 45 degrees or goes beyond the screen boundaries. The action space consists of three independent degrees of freedom: direction (left or right), force magnitude (10N or 20N), and green light intensity (lighter or darker). This results in an 8-dimensional one-hot vector. The objective of this variant is to maintain balance for the reward-relevant cartpole by applying suitable forces.

### Variant of Robodesk

The RoboDesk environment with noise distractors [9] is a control task designed to simulate realistic sources of noise, such as flickering lights and shaky cameras. Within the environment, there is a large TV that displays natural RGB videos. On the desk, there is a green button that controls both the hue of the TV and a light. The agent's objective is to manipulate this button in order to change the TV's hue to green. The agent's reward is determined based on the greenness of the TV image. In this environment, all four types of information are present (see Table 1).

Figure 8: Visualization of the environments used in our experiments.

### Variants of DeepMind Control Suite

Four variants [9] are introduced for each DMC task:

* **Noiseless**: Original environment without distractors.
* **Video Background**: Replacing noiseless background with natural videos [37] (\(\overline{\mathbf{Ctrl}}+\overline{\mathbf{Rew}}\)).
* **Video Background + Sensor Noise**: Imperfect sensors sensitive to intensity of a background patch (\(\overline{\mathbf{Ctrl}}\) + \(\mathbf{Rew}\)).
* **Video Background + Camera Jittering**: Shifting the observation by a smooth random walk (\(\overline{\mathbf{Ctrl}}\) + \(\overline{\mathbf{Rew}}\)).

The video background in the environment incorporates grayscale videos from Kinetics-400, where pixels with high blue channel values are replaced. Camera jittering is introduced through a smooth random walk shift using Gaussian-perturbing acceleration, velocity decay, and pulling force. Sensor noise is added by perturbing a specific sensor based on the intensity of a patch in the background video. The perturbation involves adding the average patch value minus 0.5. Different sensors are perturbed for different environments. These sensor values undergo non-linear transformations, primarily piece-wise linear, to compute rewards. While the additive reward noise model may not capture sensor behavior perfectly, it is generally sufficient as long as the values remain within moderate ranges and stay within one linear region. (Note: the variants of Robodesk and DMC are not the contributions of this paper. We kindly refer readers to the paper of Denoised MDP [9] for a more detailed introduction.)

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \(\mathbf{Ctrl}\) + \(\mathbf{Rew}\) & \(\overline{\mathbf{Ctrl}}\) + \(\overline{\mathbf{Rew}}\) & \(\overline{\mathbf{Ctrl}}\) + \(\overline{\mathbf{Rew}}\) \\ \hline Modified Cartpole & Agent & Agent & Green Light & Distractor cartpole \\ \hline Robodesk & Agent, Button, Light on desk & TV content, Button sensor noise & Blocks on desk, Handle on desk, Other movable objects (Green hue of TV) & Jittering and flickering \\  & & & & \\ \hline DMC & **Noiseless** & Agent & **(Agent)** & — & — \\  & **Video Background** & Agent & **(Agent)** & — & Background \\  & **Video Background** & Agent & Background & — & — \\  & **+ Noisy Sensor** & Agent & Background & — & — \\  & **Video Background** & & & & Background \\  & **+ Camera Jittering** & Agent & **(Agent)** & — & Jittering camera \\ \hline \hline \end{tabular}
\end{table}
Table 1: Categorization of various types of information in the environments we evaluated. We use the red color to emphasize the categorization difference between IFactor and Denoised MDP. Unlike Denoised MDP that assumes independent latent processes, IFactor allows for causally-related processes. Therefore, in this paper, the term ”controllable” refers specifically to one-step controllability, while ”reward-relevant” is characterized by the conditional dependence between \(s_{t}^{*}\) and the cumulative reward variable \(R_{t}\) when conditioning on \((s_{l-1},a_{t-1:t})\). Following this categorization, certain agent information can be classified as (one-step) uncontrollable (including indirectly controllable and uncontrollable factors) but reward-relevant factors, such as some position information determined by the position and velocity in the previous time-step rather than the action. On the other hand, the green hue of TV in Robodesk is classified as controllable but reward-irrelevant factors, as they are independent of the reward given the state of the robot arm and green button, aligning with the definition of \(s_{t}^{a\bar{t}}\).

Experimental Details

Computing HardwareWe used a machine with the following CPU specifications: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz; 32 CPUs, eight physical cores per CPU, a total of 256 logical CPU units. The machine has two GeForce RTX 2080 Ti GPUs with 11GB GPU memory.

ReproducibilityWe've included the code for the framework and all experiments in the supplement. We plan to release our code under the MIT License after the paper review period.

### Synthetic Dataset

Hyperparameter Selection and Network StructureWe adopt a similar experimental setup to TDRL [19], while extending it by decomposing the dynamics into four causally related latent processes proposed in this paper (refer to Equation 43). For all experiments, we assign \(\beta_{1}=\beta_{2}=\beta_{3}=\beta_{4}=0.003\) as the weights for the KL divergence terms. In this particular experiment, we set \(\lambda_{1}\) and \(\lambda_{2}\) to \(0\) because the utilization of the ELBO loss alone has effectively maximized \(J^{t}_{\text{RS}}\) and \(J^{t}_{\text{AS}}\), as illustrated in Figure 10. Here, \(J^{t}_{\text{RS}}\) represents \(I_{\alpha_{1}}-I_{\alpha_{2}}\), and \(J^{t}_{\text{AS}}\) represents \(I_{\alpha_{3}}-I_{\alpha_{4}}\). The network structure employed in this experiment is presented in Table 2.

Training DetailsThe models are implemented in PyTorch 1.13.1. The VAE network is trained using AdamW optimizer for 100 epochs. A learning rate of 0.001 and a mini-batch size of 64 are used. We have used three random seeds in each experiment and reported the mean performance with standard deviation averaged across random seeds.

#### e.1.1 The identifiability scores for baselines

Figure 9: The identifiability scores for baselines in the experiments on synthetic data set. It can be observed that all baselines do not enjoy the property of block-wise identifiability.

#### e.1.2 Extra Results.

During the training process, we record the estimation value of four mutual information (MI) terms. The corresponding results are presented in Figure 10. Despite not being explicitly incorporated into the objective function, the terms \(I_{\alpha_{1}}-I_{\alpha_{2}}\) and \(I_{\alpha_{3}}-I_{\alpha_{4}}\) exhibit significant maximization. Furthermore, the estimation values of \(I_{\alpha_{2}}\) and \(I_{\alpha_{4}}\) are found to be close to 0. These findings indicate that the state variable \(s_{t}^{z}\) contains little information about the reward, and the predictability of \(\hat{s}_{t}^{\hat{u}}\) by the action is also low.

### Modified Cartpole

In the modified Cartpole environment, we configure the values as follows: \(\beta_{1}=\beta_{2}=\beta_{3}=\beta_{4}=0.1\) and \(\lambda_{1}=\lambda_{2}=0.1\). Recurrent State Space Model (RSSM) uses a deterministic part and a stochastic part to represent latent variables. The deterministic state size for four dynamics are set to be (15, 15, 15, 15), and the stochastic state size are set to be (2, 2, 1, 4). The architecture of the encoder and decoder for observation is shown in Table 3 and Table 4 (\(64\times 64\) resolution). Reward model uses 3-layer MLPs with hidden size to be 100 and four mutual information neural estimators are 4-layer MLPs with hidden size to be 128. Unlike the synthetic dataset, where there are clear categories

[MISSING_PAGE_EMPTY:28]

### Variant of Robodesk

In the variant of Robodesk, we conduct experiments with the following hyperparameter settings: \(\beta_{1}=\beta_{2}=2,\beta_{3}=\beta_{4}=0.25\), and \(\lambda_{1}=\lambda_{2}=0.1\). For the four dynamics, we set the deterministic state sizes to (120, 40, 40, 40), and the stochastic state sizes to (30, 10, 10, 10). Denoised MDP utilizes two latent processes with deterministic state sizes [120, 120] and stochastic state sizes [10, 10]. For the mutual information neural estimators, we employ 4-layer MLPs with a hidden size of 128. To ensure a fair comparison, we align the remaining hyperparameters and network structure with those in the Denoised MDP. We reproduce the results of the Denoised MDP using their released code, maintaining consistency with their paper by employing the default hyperparameters. In order to evaluate the impact of the Mutual Information (MI) constraints, we conduct an ablation study. The results are shown is Figure 12. The constraints \(\mathcal{J}^{t}_{RS}\) and \(\mathcal{J}^{t}_{AS}\) are observed to stabilize the training process of IFactor. The results of IFactor are areaveraged over 5 runs, while the results of Denoised MDP and IFactor without MI are averaged over three runs.

Policy learning based on the learned representations by IFactorWe retrain policies using the Soft Actor-Critic algorithm [40] with various combinations of the four learned latent categories as input. We wrap the original environment with visual output using our representation model to obtain compact features. In this process, both deterministic states and stochastic states are utilized to form the feature. For instance, when referring to \(s^{t}_{r}\), we use both the deterministic states and stochastic states of \(s^{t}_{r}\). The implementation of SAC algorithm is based on Stableb-Baselines3[59], with a learning rate of 0.0002. Both the policy network and Q network consist of 4-layer MLPs with a hidden size of 256. We use the default hyperparameter settings in Stable-Baselines3 for other parameters.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Operator & \begin{tabular}{c} Input \\ Shape \\ \end{tabular} & 
\begin{tabular}{c} Kernel \\ Size \\ \end{tabular} & Stride & Padding \\ \hline Input & \([\)input\_size\(]\) & — & — & — \\ FC + ReLU + Reshape & \([1024,1,1]\) & — & — & — \\ Conv. Transpose + ReLU * & \([128,3,3]\) & 3 & 1 & 0 \\ Conv. Transpose + ReLU & \([128,9,9]\) & 5 & 2 & 0 \\ Conv. Transpose + ReLU & \([64,21,21]\) & 5 & 2 & 0 \\ Conv. Transpose + ReLU & \([32,46,46]\) & 6 & 2 & 0 \\ Conv. Transpose + ReLU & \([3,96,96]\) & 6 & 2 & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The decoder architecture designed for (\(96\times 96\))-resolution observation. For (\(64\times 64\))-resolution observation, the first transpose convolutional layer(*) is removed.

Figure 12: Comparison between IFactor and Denoised MDP in the variant of Robodesk environment.

### Variants of Deep Mind Control Suite

In the noiseless DMC environments, we set \(\beta_{1}=\beta_{2}=\beta_{3}=\beta_{4}=1\). For the DMC environments with video background, we set \(\beta_{1}=\beta_{2}=1\) and \(\beta_{3}=\beta_{4}=0.25\). In the DMC environments with video background and noisy sensor, we set \(\beta_{1}=\beta_{2}=2\) and \(\beta_{3}=\beta_{4}=0.25\). Lastly, for the DMC environments with video background and jittering camera, we set \(\beta_{1}=\beta_{2}=1\) and \(\beta_{3}=\beta_{4}=0.25\). Regarding the Reacher environment with video background and jittering camera, we set \(\lambda_{1}=\lambda_{2}=0.1\) for our experiments. For the other environments, we set \(\lambda_{1}=\lambda_{2}=0\). The deterministic state sizes for the four dynamics are set to (120, 120, 60, 60), while the stochastic state sizes are set to (20, 20, 10, 10). The four mutual information neural estimators utilize a 4-layer MLPs with a hidden size of 128. We align the other hyperparameters and network structure with those used in the Denoised MDP for a fair comparison.

#### e.4.1 Policy optimization results on variants of DMC

We present detailed evaluation results in Table 6, showcasing both the mean values and the corresponding standard deviations for the final policy performance across each task. Results are averaged across three seeds. Denoised MDP performs well across all four variants with distinct noise types.

#### e.4.2 Mutual information

Figure 7 demonstrates the notable improvement in policy performance in the Reacher environment with video background and jittering camera due to the inclusion of the constraints \(J^{r}_{RS}\) and \(J^{r}_{A\bar{S}}\). To further investigate how they affects the model learning, we record the estimation values of four Mutual Information terms throughout the training process, as depicted in Figure 13. The results indicate that both \(I_{\alpha_{1}}-I_{\alpha_{2}}\) and \(I_{\alpha_{3}}-\tilde{I}_{\alpha_{4}}\) are maximized for both IFactor and IFactor without MI. However, IFactor exhibits a significantly higher rate of maximizing \(I_{\alpha_{1}}-I_{\alpha_{4}}\) compared to IFactor without MI. This increased maximization leads to greater predictability of \(s^{\bar{r}}_{t}\) by the action, ultimately contributing to the observed performance gain.

### Visualization for DMC

In this experiment, we investigate five types of representations, which can be derived from the combination of four original disentangled representation categories. Specifically, \(s^{\pi}_{t}\) is the controllable and reward relevant representation. \(s^{r}_{t}=(s^{\pi r}_{t},s^{\bar{r}\bar{r}}_{t})\) is the reward-relevant representation. \(s^{\bar{r}\bar{r}}_{t}\) is the controllable but reward-irrelevant representation. \(s^{\bar{r}\bar{r}}_{t}\) is the uncontrollable and reward-irrelevant representation (noise). \(s^{\bar{r}}_{t}=(s^{\pi\bar{r}}_{t},s^{\bar{r}\bar{r}}_{t})\) is the reward-irrelevant representation. Only representations of \(s^{r}_{t}\) are used for policy optimization. We retrain 5 extra observation decoders to reconstruct the original image, which can precisely characterize what kind of information each type of representation contains, surpassing the limitations of the original decoder that is used in latent traversal. The visualization results are shown in Figure 14. It can be observed that \(s^{\prime r}_{t}\) captures the movement of the agent partially but not well enough; \(s^{\prime}_{t}\) captures the movement of the agent precisely but \(s^{\bar{r}}_{t}\) fails (Reacher and Cheetah) or captures extra information of the background (Walker). This finding

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & Environment & Action & Train & Collection & Batch & Sequence & Horizon \\  & Steps & Repeat & Every & Intervals & Size & Length & \\ \hline Modified Cartpole & 200,000 & 1 & 5 & 5 & 20 & 30 & 8 \\ Robodesk & 1,000,000 & 2 & 1000 & 100 & 50 & 50 & 15 \\ DMC & 1,000,000 & 2 & 1000 & 100 & 25 & 50 & 12 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Some hyperparameters of our method in the environment of Modified Cartpole, Robodesk and DMC. _Environment Steps_ represents the number of interactions between the agent and the environment. _Action Repeat_ determines how many times an agent repeats an action in a step. _Train Every_ specifies the environment step between adjacent training iterations. _Collection Intervals_ defines the number of times the model is trained in each training iteration (including world models, policy networks and value networks). _Batch Size_ refers to the number of trajectories in each mini-batch. _Sequence Length_ denotes the length of the chuck used in training the world models. _Horizon_ determines the length of dreaming when training the policy using the world model. Hyperparameters are aligned with those used in the Denoised MDP for fair comparison.

suggests that \(s_{i}^{\prime}\) contains sufficient information within the original noisy observation for effective control, while effectively excluding other sources of noise.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Cheetah} & IFactor & Dreamer & Denoised & \multirow{2}{*}{Dreamer} & \multirow{2}{*}{TIA} & \multirow{2}{*}{DBC} & \multirow{2}{*}{CURL} & \multirow{2}{*}{PI-SAC} \\  & (Ours) & Pro & MDP & & & & & \\ \hline Noiseless & **874\(\pm\)39** & 803\(\pm\)75 & 771\(\pm\)60 & 714\(\pm\)348 & 766\(\pm\)29 & 182\(\pm\)50 & 171\(\pm\)34 & 369\(\pm\)28 \\ Video Background & **573\(\pm\)193** & 366\(\pm\)96 & 431\(\pm\)111 & 171\(\pm\)45 & 388\(\pm\)297 & 248\(\pm\)71 & 70\(\pm\)48 & 112\(\pm\)31 \\ Video Background & **456\(\pm\)88** & 134\(\pm\)41 & 400\(\pm\)190 & 166\(\pm\)42 & 227\(\pm\)13 & 141\(\pm\)13 & 199\(\pm\)7 & 151\(\pm\)14 \\ Video Background + Camera Jittering & **418\(\pm\)22** & 150\(\pm\)104 & 294\(\pm\)100 & 160\(\pm\)32 & 202\(\pm\)93 & 141\(\pm\)35 & 169\(\pm\)22 & 156\(\pm\)20 \\ \hline \hline \multirow{2}{*}{Walker} & IFactor & Dreamer & Denoised & \multirow{2}{*}{Dreamer} & \multirow{2}{*}{TIA} & \multirow{2}{*}{DBC} & \multirow{2}{*}{CURL} & \multirow{2}{*}{PI-SAC} \\  & (Ours) & Pro & MDP & & & & & \\ \hline Noiseless & 966\(\pm\)5 & **941\(\pm\)16** & 947\(\pm\)13 & 955\(\pm\)6 & 955\(\pm\)5 & 614\(\pm\)111 & 417\(\pm\)296 & 203\(\pm\)92 \\ Video Background & **917\(\pm\)52** & 909\(\pm\)48 & 790\(\pm\)113 & 247\(\pm\)135 & 685\(\pm\)337 & 199\(\pm\)67 & 608\(\pm\)100 & 200\(\pm\)18 \\ Video Background + Noisy Sensor & **701\(\pm\)174** & 242\(\pm\)65 & 661\(\pm\)120 & 279\(\pm\)145 & 425\(\pm\)281 & 95\(\pm\)54 & 338\(\pm\)92 & 222\(\pm\)21 \\ Video Background + Camera Jittering & **524\(\pm\)194** & 368\(\pm\)301 & 291\(\pm\)104 & 106\(\pm\)22 & 230\(\pm\)332 & 62\(\pm\)17 & 448\(\pm\)70 & 116\(\pm\)6 \\ \hline \hline \multirow{2}{*}{Reacher} & IFactor & Dreamer & Denoised & \multirow{2}{*}{TIA} & \multirow{2}{*}{DBC} & \multirow{2}{*}{CURL} & \multirow{2}{*}{PI-SAC} \\  & (Ours) & Pro & MDP & & & & & \\ \hline Noiseless & **924\(\pm\)37** & 924\(\pm\)61 & 686\(\pm\)216 & 876\(\pm\)57 & 587\(\pm\)256 & 95\(\pm\)58 & 663\(\pm\)221 & 166\(\pm\)235 \\ Video Background & **963\(\pm\)10** & 555\(\pm\)92 & 544\(\pm\)121 & 253\(\pm\)127 & 123\(\pm\)21 & 102\(\pm\)58 & 751\(\pm\)189 & 76\(\pm\)35 \\ Video Background + Noisy Sensor & **839\(\pm\)51** & 675\(\pm\)137 & 561\(\pm\)182 & 202\(\pm\)82 & 264\(\pm\)280 & 97\(\pm\)39 & 607\(\pm\)260 & 85\(\pm\)5 \\ Video Background + Camera Jittering & **736\(\pm\)53** & 675\(\pm\)82 & 213\(\pm\)106 & 109\(\pm\)19 & 89\(\pm\)26 & 87\(\pm\)51 & 632\(\pm\)96 & 84\(\pm\)13 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Policy optimization evaluation on variants of DMC with various distractors. IFactor consistently performs well across all four variants with distinct noise types. Bold numbers show the best model-learning result for specific policy learning choices. Results are averaged over 3 runs.

Figure 14: Visualization of the DMC variants and the factorization learned by IFactor.

Figure 13: Estimation of the value of four mutual information terms and their differences in the Reacher Easy environment with video background and jittering camera.

Comparison between IFactor and Related Work

### Comparison between IFactor and Denoised MDP

While both IFactor and Denoised MDP share the common aspect of factorizing latent variables based on controllability and reward relevance, it is crucial to recognize the numerous fundamental distinctions between them.

Firstly, Denoised MDP is limited to only factoring out additive rewards \(r_{y_{t}}\) from \(r_{s_{t}}\), disregarding the possibility of non-additive effects in many uncontrollable yet reward-relevant factors. In contrast, our method embraces the inclusion of non-additive effects of \(s_{t}^{ur}\) on the reward, which is more general. Thirdly, Denoised MDP uses only controllable and reward-relevant latent variables \(x_{t}\) for policy optimization, which we show in the theoretical analysis that it is generally insufficient. In contrast, our method utilize both controllable and uncontrollable reward-relevant factors for policy training.

Finally, Denoised MDP makes the assumption of an instantaneous causal effect from \(x_{t}\) to \(z_{t}\) and from \(y_{t}\) to \(z_{t}\), which is inherently unidentifiable without further intervention. It is worth noting that imposing interventions on the latent states is unrealistic in most control tasks, as agents can only choose actions at specific states and cannot directly intervene on the state itself. In contrast, our method assumes that there exists no instantaneous causal effect for latent variables. In conjunction with several weak assumptions, we provide a proof of block-wise identifiability for our four categories of latent variables. This property serves two important purposes: (1) it ensures the removal of reward-irrelevant factors and the utilization of minimal and sufficient reward-relevant variables for policy optimization, and (2) it provides a potential means for humans to comprehend the learned representations within the reinforcement learning (RL) framework. Through latent traversal of the four types of latent variables, humans can gain insights into the specific kind of information that each category of representation contains within the image.

From the perspective of model structure, it is worth highlighting that the architecture of both the transition model (prior) and the representation model (posterior) in IFactor differs from that of Denoised MDP. The structure of prior and posterior of IFactor is shown as follows:

\[\begin{array}{ll}\text{Prior:}&\text{Posterior:}\\ \left\{\begin{array}{l}p_{\gamma_{1}}\left(s_{t}^{ur}\mid s_{t-1}^{r},a_{t-1 }\right)\\ p_{\gamma_{2}}\left(s_{t}^{ur}\mid s_{t-1}^{r}\right)\\ p_{\gamma_{3}}\left(s_{t}^{ur}\mid\mathbf{s_{t-1}},a_{t-1}\right)\\ p_{\gamma_{4}}\left(s_{t}^{ur}\mid\mathbf{s_{t-1}}\right)\end{array}\right\}& \left\{\begin{array}{l}q_{\phi_{1}}\left(s_{t}^{ur}\mid o_{t},s_{t-1}^{r},a_ {t-1}\right)\\ q_{\phi_{2}}\left(s_{t}^{ur}\mid o_{t},s_{t-1}^{r}\right)\\ q_{\phi_{3}}\left(s_{t}^{ur}\mid o_{t},\mathbf{s_{t-1}},a_{t-1}\right)\\ q_{\phi_{4}}\left(s_{t}^{ur}\mid o_{t},\mathbf{s_{t-1}}\right)\end{array}\right. \end{array}\] (44)

While Denoised MDP has the following prior and posterior:

Figure 15: Graphical illustration of our world model and Denoised MDP.

Prior:

(45)

A notable distinction can be observed between Denoised MDP and IFactor in terms of the assumptions made for the prior and posterior structures. Denoised MDP assumes independent priors for \(x_{t}\) and \(y_{t}\), whereas IFactor only incorporates conditional independence, utilizing \(s^{\prime}_{t-1}\) as input for the transition of both \(s^{\prime\prime}_{t}\) and \(s^{\prime\prime}_{t}\). Moreover, the posterior of \(y_{t}\) receives \(a_{t-1}\) as input, potentially implying controllability. Similarly, the posterior of \(x_{t}\) incorporates \(z_{t-1}\) as input, which may introduce noise from \(z_{t-1}\) into \(x_{t}\). These implementation details can deviate from the original concept. In contrast, our implementation ensures consistency between the prior and posterior, facilitating a clean disentanglement in our factored model.

From the perspective of the objective function, IFactor incorporates two supplementary mutual information constraints, namely \(\mathcal{J}^{t}_{\text{RS}}\) and \(\mathcal{J}^{t}_{\text{AS}}\), to promote disentanglement and improve policy performance.

### Comparison between IFactor, InfoPower and IsoDream

IFactor learns different categories of state representations according to their relation with action and reward, which is different from InfoPower [48] and IsoDream [12], and moreover, IFactor emphasizes block-wise identifiability for the four categories of representations while InfoPower and Iso-Dream do not. Specifically, InfoPower learns 3 types of latent variables, including \(s^{\prime\prime}_{t},s^{\prime\prime}_{t}\) and \(s^{\prime\prime}_{t}\). IsoDream uses three branches for latent dynamics, distinguishing controllable, noncontrollable, and static parts.

**Other differences with InfoPower:**

* **Reconstruction Basis**: InfoPower is reconstruction-free, while IFactor is reconstruction-based.
* **Objective Functions**: InfoPower prioritizes mutual information and empowerment, while IFactor utilizes reconstruction, KL constraints, and mutual information constraints for disentanglement. InfoPower formulates policy using task reward value estimates and the empowerment objective, while IFactor learns policy by maximizing the estimated Q value and dynamics backpropagating.

**Other differences with IsoDream:**

* **Objective Functions and Dynamics Modeling**: IsoDream models controllable transitions using inverse dynamics, while IFactor disentangles with multiple mutual information and KL divergence constraints. IsoDream learns policy using a future-state attention mechanism rooted in present controllable and future uncontrollable states. In contrast, IFactor focuses on reward-relevant states, ensuring \(s^{\prime}_{t}\) variables are optimal for policy optimization.