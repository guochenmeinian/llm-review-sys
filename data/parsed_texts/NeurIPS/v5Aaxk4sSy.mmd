# Improving Adversarial Robustness via

Information Bottleneck Distillation

 Huafeng Kuang\({}^{1}\), Hong Liu\({}^{2}\), YongJian Wu\({}^{3}\), Shin'ichi Satoh\({}^{2}\), Rongrong Ji\({}^{1}\)

\({}^{1}\)Key Laboratory of Multimedia Trusted Perception and Efficient Computing,

Ministry of Education of China, Xiamen University, 361005, P.R. China

\({}^{2}\)National Institute of Informatics, Tokyo, 101-8430, Japan

\({}^{3}\)Youtu Laboratory, Tencent, Shanghai, 200233, China

skykuang@stu.xmu.edu.cn, lynnliu.xmu@gmail.com

littlekenwu@tencent.com, satoh@nii.ac.jp, rrji@xmu.edu.cn

Project leadCorresponding author

###### Abstract

Previous studies have shown that optimizing the information bottleneck can significantly improve the robustness of deep neural networks. Our study closely examines the information bottleneck principle and proposes an Information Bottleneck Distillation approach. This specially designed, robust distillation technique utilizes prior knowledge obtained from a robust pre-trained model to boost information bottlenecks. Specifically, we propose two distillation strategies that align with the two optimization processes of the information bottleneck. Firstly, we use a robust soft-label distillation method to increase the mutual information between latent features and output prediction. Secondly, we introduce an adaptive feature distillation method that automatically transfers relevant knowledge from the teacher model to the student model, thereby reducing the mutual information between the input and latent features. We conduct extensive experiments to evaluate our approach's robustness against state-of-the-art adversarial attackers such as PGD-attack and AutoAttack. Our experimental results demonstrate the effectiveness of our approach in significantly improving adversarial robustness. Our code is available at https://github.com/SkyKuang/IBD.

## 1 Introduction

Numerous works have shown that deep neural networks (DNNs) are easily attacked by adversarial examples [56, 41, 4, 14], which involve adding imperceptible noise to inputs and causing incorrect outputs. This vulnerability of DNNs raises significant security concerns when deploying DNNs in safety-critical applications. To address this potential threat, various adversarial defense strategies have been proposed [44, 41, 72, 43, 72]. Among these defenses, adversarial training (AT) [41, 71] is a general solution for defending against adversarial attacks by incorporating adversarial examples generated by an adversarial attack into the training process. While many AT techniques can defend against sophisticated attacks, such as AutoAttack [14], a big robust generalization gap still exists between the training data and testing data [12].

Recent years have witnessed the growing popularity of the studies of the use of **I**nformation **B**ottleneck (IB) [57] in training robust DNNs [54, 2, 64, 68]. IB involves finding a trade-off in intermediate features \(Z\) between relevant information for the prediction \(Y\) and nuisance information about input \(X\). The overall objective of IB is formulated as follows:

\[\max I(Z;Y)-\beta I(X;Z),\] (1)where \(I\) denotes mutual information and \(\beta\) controls the trade-off between the two terms. Studies in [54; 64] show that IB can produce concise representations leading to better generalization, and can make intermediate features of DNNs less sensitive to input perturbations. On the other hand, variational information bottleneck (VIB) has also been found to significantly improve the robustness of models [2; 19]. In addition, [40] replaces mutual information with the Hilbert Schmidt Independence Criterion (HSIC) and calls this the HSIC Bottleneck, and [64] further investigates the HSIC bottleneck as a regularization to enhance the robustness of DNNs.

The crucial step to optimizing the IB is to calculate mutual information in an efficient and effective manner. An approach in [19] proposes a conditional entropy bottleneck (CEB) (see Eq.(6)), which enhances IB by introducing label priors in variational inference. This approach employs a backward encoder to approximate the true distribution \(p(z|y)\). However, DNN processing can form a Markov chain \((X\to Z\to Y)\) and obey the data processing inequality: \(H(X)\geq H(Z)\geq H(Y)\), and the label \(Y\) often cannot satisfy the intrinsic dimension requirement [37]. As a result, directly using label information to approximate \(p(z|y)\) is imprecise. Furthermore, the backward encoder usually provides uniform priors for inputs with the same label and cannot provide customized priors based on different inputs, particularly for high-dimensional image data.

Fortunately, prior research has shown that robust pre-trained models can provide robust prior information to enhance model robustness and certainty [27; 53; 21]. Motivated by this, we revisit VIB through the lens of robustness distillation and further introduce a new IB objective: Information **B**ottleneck **D**istillation (IBD) (see Eq.(7)). To make IBD practical, we put forward two distillation strategies meant to target the two optimization processes of the information bottleneck respectively. Firstly, we utilize soft-label distillation to maximize the mutual information between intermediate features and output prediction. Secondly, we use adaptive feature distillation to restrict the mutual information between the input and intermediate features, which facilitates the transfer of appropriate knowledge from the teacher model to the student model, ultimately resulting in a more accurate estimation of the distribution of student features. A schematic diagram illustrating the IBD is shown in Figure 1.

Overall, we make the following contributions:

* **Theoretically**, we utilize conditional variational inference to construct a lower bound to estimate the mutual information and reformat the IB principle by using the adversarial robustness as the prior for learning features, which is termed Information Bottleneck Distillation (IBD).
* **Algorithmically**, to realize IBD, we propose two distillation strategies: robust soft-label and adaptive feature distillation, to match the two optimization processes of the information bottleneck, respectively.

Figure 1: Schematic of successive extraction of relevant information in IBD. The inputs are passed through a robust teacher and a student to get intermediate features (Z). These features are then passed through an adaptive feature distillation module, where an attention-based model determines similarities between the teacher and student features. Robust information from each teacher feature is transferred to the student with the attention values. The student model also minimizes cross-entropy (CE) loss using the teacherâ€™s soft output label. Consult the section 3.2 for more details.

* **Experimentally**, we conducted extensive experiments on various benchmark datasets such as CIFAR and ImageNet. The results show the effectiveness of our IBD in improving the robustness of DNNs against most attacks (e.g., PGD-attack and AutoAttack), and our IBD behaves more robustly than state-of-the-art methods.

## 2 Background & Related Work

**Adversarial Attacks.** Adversarial examples are observed when an attacker adds human-imperceptible perturbation to the inputs, which misleads DNN's output [56]. And a series of attacking algorithms are proposed [22; 4; 41; 18; 14; 38]. One of the most popular attacks is the Project Gradient Descent (PGD) attack [41], which uses multi-step projected gradient descent to generate adversarial examples:

\[x_{k}=\Pi\Big{(}x_{k-1}+\alpha\cdot\text{sign}\big{(}\nabla_{x}\mathcal{L}_{ \mathcal{CE}}\big{(}h_{\theta}(x_{k-1}),y\big{)}\big{)}\Big{)},\] (2)

where \(\alpha\) is the step size, \(\Pi\) is the project function, and \(x_{k}\) is the adversarial example at the \(k\)-th step, \(h_{\theta}\) is a DNN model with weight \(\theta\), and \(\mathcal{L}_{CE}\) is cross-entropy function. Another widely-used adversarial attack method is the C&W attack [4], which applies a rectifier function regularizer to generate adversarial examples near the original input. AutoAttack (AA) [14], an ensemble of different parameter-free attacks, is currently regarded as the most reliable evaluation of the adversarial robustness of the model. Black-box attacks, which do not require full knowledge of DNNs, are more practical, and previous works [67; 18] have shown that adversarial examples generated by one model can attack other models with a high probability. Query-based black-box attacks [3; 9] update the perturbation iteratively according to the attack objective. In this work, we test the robust performance under both white- and black-box attacks to verify the robustness of our proposed method.

**Adversarial Robustness.** To counter the threat of adversarial attacks, researchers have proposed various defense methods, including defensive distillation [44], manifold-projection [51], pre-processing [24; 69], provable defenses [45; 50], and adversarial training [11; 22; 41]. Among these, adversarial training **(AT)**[41] is a highly effective and widely-used solution. It involves adding adversarial examples, generated by the adversarial attack scheme, to the training data, which can be formulated by a min-max optimization problem:

\[\min_{\theta}\mathbb{E}\bigg{[}\max_{x_{\text{adv}}}\mathcal{L}_{CE}\Big{(}h_ {\theta}\big{(}x_{\text{adv}}\big{)},y\Big{)}\bigg{]}.\] (3)

where \(x_{\text{adv}}\) is adversarial example generated by a special attack algorithm. Many variants of AT have been proposed. [71] proposed the TRADES, which characterizes the trade-off between accuracy and robustness. [66] proposed adversarial weight perturbation (AWP), which uses weight perturbation to increase the robustness of the model. In addition, some new efforts have also been devoted from different aspects including designing new training strategies [43; 33], adversarial regularization [42; 48; 39], robustness architecture search [8; 25; 29; 30], and data augmentation [5; 46; 63].

**Robustness Distillation.** Knowledge distillation (KD) [28] is an effective method, wherein a well-trained teacher model is used to supervise the learning of the student model. The overall objective of KD is formulated as:

\[\min_{\theta_{z}}\mathbb{E}\bigg{[}\mathcal{L}_{task}\big{(}h_{s}(x),y\big{)} +\alpha\cdot L_{Dist}\big{(}h_{s}(x),h_{t}(x)\big{)}\bigg{]},\] (4)

where \(\mathcal{L}_{task}\) is the task-specific loss function for the target task and \(\mathcal{L}_{Dist}\) is the loss function that penalizes the difference between the teacher and the student. Recent studies show that KD can also be used to enhance the adversarial robustness of a student network [44]. Then, the adversarially robust distillation (ARD) method, as introduced in [21], combines adversarial training with KD to further increase the robustness of the student model under the supervision of an adversarially robust teacher network. Introspective Adversarial Distillation (IAD) [73] propose proposes a method to automatically realize the intuition of the previous reliable guidance during the adversarial distillation. And, RSLAD [74] improves the performance of ARD by using robust soft labels generated by a pre-trained teacher network to replace hard labels used in [21].

Information Bottleneck Distillation

In this section, we revisit the concept of information bottleneck from the perspective of robust distillation and establish the IBD objective. We further propose two distillation strategies to perform the two optimization processes of the information bottleneck, respectively. Finally, we combine IBD with AT to learn a robust model.

### A Distillation View of Information Bottleneck

The information bottleneck theory has been used to explain deep models [58], compress models [16], and learn robust models [2]. To make the IB practical, a major challenge is to accurately estimate the mutual information. In [2], Variational Information Bottleneck (VIB)1 utilizes variational inference to construct the lower bound to estimate the mutual information. VIB is formulated as:

Footnote 1: For more details of the bound, we refer the reader to [2], or see Appendix A.

\[I(Z;Y)-\beta I(X;Z)\geq\operatorname*{\mathbb{E}}_{p(x,y)p(z|x)}\left[\log q( y|z)-\beta\log\frac{p(z|x)}{q(z)}\right],\] (5)

where \(p(z|x)\) is feature distribution, \(q(y|z)\) is a variational approximation to the true distribution \(p(y|z)\), and \(q(z)\) is the variational approximation to the true distribution \(p(z)\). To optimize Eq.(5), VIB uses neural networks to parameterize the Gaussian densities \(p(z|x)\) and \(q(y|z)\), and treats \(q(z)\) as a fixed \(K\)-dimensional spherical Gaussian distribution \(q(z)=\mathcal{N}(0,I)\) (where \(K\) is the size of the bottleneck). Then, the reparameterization trick and Monte Carlo sampling are used to get an unbiased estimate of the gradient. This process thus allows DNNs to handle high-dimensional data, such as images, avoiding previous limitations on discrete or Gaussian distribution cases.

Next, how to determine the approximate \(q(z)\) is important in variational inference. VIB empirically treats \(q(z)\) as a fixed Gaussian distribution. For a better approximation of \(q(z)\), [19] proposes the Conditional Entropy Bottleneck (CEB), which introduces label prior information to approximate \(q(z)\). The CEB can be formulated as:

\[I(Z;Y)-\beta I(X;Z|Y)\geq\operatorname*{\mathbb{E}}_{p(x,y)p(z|x)}\left[\log q (y|z)-\beta\log\frac{p(z|x)}{q(z|y)}\right].\] (6)

In detail, CEB utilizes a linear mapping layer to parameterize \(q(z|y)\) that takes a one-hot label \(y\) as input and outputs a vector \(\mu_{y}\) as the mean of the Gaussian distribution \(q(z|y)=\mathcal{N}(\mu_{y},I)\). CEB also uses an identity matrix \(I\) for the variance of both \(q(z|y)\) and \(p(z|x)\). Note that CEB and VIB differ in terms of the presence of a class conditional or unconditional variational marginal, and whether \(q(z|y)\) and \(p(z|x)\) have a fixed variance. Thus, by introducing such a class prior, CBE can learn an approximate distribution better. However, using the one-hot label alone to approximate high-dimensional distribution \(q(z)\) is inaccurate enough due to data processing inequality and intrinsic dimension [37], particularly for high-dimensional image data.

To address this challenge, we propose leveraging adversarial robustness distillation [21] to build the information bottleneck objective. This approach involves providing robust prior information obtained from a robust pre-trained model, which aims to improve model robustness and reduce model uncertainty [27; 53]. Specifically, we replace conventional one-hot label prior information with robust prior information derived from such a robust pre-trained model. We call this method **I**nformation **B**otleneck **D**istillation (IBD). To implement IBD, we approximate \(q(z)\) by utilizing the intermediate features extracted by an adversarially pre-trained model, and our final objective is formulated as:

\[I(Z;Y)-\beta I(X;Z|T)\geq\operatorname*{\mathbb{E}}_{p(x,y)p(z|x)}\left[\log q (y|z)-\beta\log\frac{p(z|x)}{q(z|t)}\right],\] (7)

where \(T\) is the random variable of intermediate features extracted by the adversarially pre-trained teacher model. We employ a Gaussian distribution \(\mathcal{N}(\mu_{y},\delta)\) with mean \(\mu(.)\) and variance \(\delta\) as the variational distribution \(q(z|t)\). Here, the mean \(\mu(.)\) is a function of the intermediate feature from the robust pre-trained model, and the variance \(\delta\) is set to an identity matrix as the same CEB. Eq. (7) provides a variational lower bound on IBD, and a more detailed derivation is available in Appendix B.

### Optimization for IBD

In a standard classification task, we denote \(h_{\theta}=(g\circ f)\) as a deep neural network with parameter \(\theta\). Here, \(f:\mathbb{R}^{d_{X}}\rightarrow\mathbb{R}^{d_{Z}}\) maps the inputs \(X\) to the intermediate features \(Z\), and \(g:\mathbb{R}^{d_{Z}}\rightarrow\mathbb{R}^{d_{Y}}\) further maps the intermediate features \(Z\) to the final outputs \(Y\), so that \(Z=f(X)\) and \(g(Z)=h_{\theta}(X)\). Then, we denote the target student network as \(h_{s}=(g_{s}\circ f_{s})\), and the adversarially pre-trained teacher network as \(h_{t}=(g_{t}\circ f_{t})\). Furthermore, a set of intermediate features from different layers of the student model is denoted as \(Z_{s}=f_{s}(X)=\{z_{t}^{1},z_{s}^{2},\ldots,z_{t}^{m}\}\), and a set of intermediate features from different layers of the teacher model is denoted as \(Z_{t}=f_{t}(X)=\{z_{t}^{1},z_{t}^{2},\ldots,z_{t}^{n}\}\). Here, \(n\) and \(m\) represent the number of layers of the student and teacher models, respectively. Each feature has its own feature map size and channel dimension, denoted as \(z\in\mathbb{R}^{C\times H\times W}\), where \(C,H\) and \(W\) represent the channel numbers, feature height, and width, respectively.

**1) Maximizing \(I(Z,Y)\) via Soft Label Distillation.** To optimize Eq.(7) using the gradient descent algorithm, we simplify the first term on the right-hand side of Eq.(7) to the expected log-likelihood in the form of cross-entropy, which can be formulated as follows:

\[\begin{split}\mathop{\mathbb{E}}_{p(x,y)p(z|x)}\Big{[}\log q(y|z )\Big{]}&=\int\,p(y,z)\log q(y|z)dydz\\ &=\int\,p(z)p(y|z)\log q(y|z)dydz\\ &=\int\,p(z,x)p(y|z)\log q(y|z)dydzdx\\ &=\mathop{\mathbb{E}}_{p(x)p(z|x)}\Big{[}\int\,p(y|z)\log q(y|z) dyd\bigg{]},\end{split}\] (8)

where \(p(y|z)\) indicates the true likelihood considered as a target label \(y\) corresponding to \(z\). The \(q(y|z)\) is modeled by the classifier of the student model \(h_{s}\). During the training phase, we can use the output probability of the robust teacher model \(h_{t}\) as the soft label \(y_{t}=h_{t}(x_{\text{nat}})\) to approximate the distribution of \(p(y|z)\), despite knowing the label \(y\). According to [74], using robust soft-labels is crucial in enhancing robustness. Thus, we rewrite the first term as:

\[\mathop{\mathbb{E}}_{p(x,y)p(z|x)}\Big{[}\log q(y|z)\Big{]}=\mathop{\mathbb{E }}_{p(x)p(z|x)}\left[\int\,p(y|z)\log q(y|z)dy\right]=\mathop{\mathbb{E}}_{p( x)}\left[-\mathcal{L}_{CE}\big{(}h_{s}(x),y_{t}\big{)}\right].\] (9)

**2) Maximizing \(-I(Z,X|T)\) via Adaptive Feature Distillation.** To optimize the second term on the right-hand side of Eq.(7), the formulation can be written as:

\[\begin{split}\mathop{\mathbb{E}}_{p(x)p(z|x)}\left[\log\frac{p( z|x)}{q(z|t)}\right]&=\int\,p(z,x)\log\frac{p(z|x)}{q(z|t)}dzdx\\ &=\int\,p(x)p(z|x)\log\frac{p(z|x)}{q(z|t)}dzdx\\ &=\mathop{\mathbb{E}}_{p(x)}\left[\int\,p(z|x)\log\frac{p(z|x)}{q( z|t)}dz\right].\end{split}\] (10)

Thus, we optimize the \(\mathrm{KL}\) divergence between the feature likelihood \(p(z|x)\) and the appropriate feature probability \(q(z|t)\). Here, we parameterize Gaussian densities \(p(z|x)\) and \(q(z|t)\) using neural networks, where the mean of \(p(z|x)\) and \(q(z|t)\) are the intermediate features of \(f_{s}\) and \(f_{t}\), respectively. Both variances are set to an identity matrix. As a result, the second term can be calculated by:

\[\mathop{\mathbb{E}}_{p(x,y)p(z|x)}\left[\log\frac{p(z|x)}{q(z|t)}\right]= \mathop{\mathbb{E}}_{p(x)p(z|x)}\left[\mathrm{KL}\big{(}p(z|x)\|q(z|t)\big{)} \right]=\mathop{\mathbb{E}}_{p(x)}\left[\big{(}f_{t}(x)-f_{s}(x)\big{)}^{2}+ \mathrm{c}\right],\] (11)

where \(c\) is a constant. When optimizing DNNs using Eq.(11) as the objective, a challenge arises in selecting suitable intermediate features from models to calculate the loss. This is due to the fact that different intermediate features tend to have different information, especially when the student and teacher models have different architectures. Motivated by [6; 32], we leverage an attention-based feature distillation strategy to achieve cross-layer information transfer.

Given two sets of the intermediate features, \(Z_{t}=f_{t}(x)=\{z_{t}^{1},z_{t}^{2},\ldots,z_{t}^{n}\}\) and \(Z_{s}=f_{s}(x)=\{z_{s}^{1},z_{s}^{2},\ldots,z_{s}^{m}\}\), our aim is to identify similarities for all possible combinations and transfer relevant information from the teacher to the student. We compare the features of both teacher and student models by using two different pooling methods - global average pooling and channel-wise pooling. The similarity determined by two globally pooled features is used as the weight for transferring information over the distance defined by the channel-wisely averaged features. In order to identify the similarity between \(Z_{t}\) and \(Z_{s}\), we adopt a query-key concept of the attention mechanism [61]. In detail, each teacher features generate a query set \(Q_{t}=\{q_{t}^{1},q_{t}^{2},\ldots,q_{t}^{n}\}\), and each student feature generate a key set \(K_{s}=\{k_{s}^{1},k_{s}^{2},\ldots,k_{s}^{m}\}\), where the \(q_{t}^{n}\) and \(k_{s}^{m}\) are calculated as:

\[\begin{split}\mathbf{q}_{t}^{n}=& Re\big{(}W_{t}^{n }\cdot\text{GAP}\left(z_{t}^{n}\right)\big{)},\\ \mathbf{k}_{s}^{m}=& Re\big{(}W_{s}^{m}\cdot\text{GAP }\left(z_{s}^{m}\right)\big{)},\end{split}\] (12)

where GAP denotes the global average pooling, \(Re\) is Relu activation function, \(W_{t}^{n}\) and \(W_{s}^{m}\) are linear transition parameters. It should be noted that these features possess varying transition weights, as they convey different levels of information.

By utilizing the queries and keys, attention values that represent the relation between teacher and student features are calculated with a softmax function:

\[Attn=\text{softmax}\left(\frac{Q_{T}W^{Q-k}K_{S}^{\top}}{\sqrt{d}}\right),\] (13)

where \(W^{Q-k}\in\mathbb{R}^{d\times d}\) is a bilinear weight. The bilinear weighting is utilized to generalize the attention values derived from different source ranks, as queries and keys are identified within features of distinct dimensions [34]. \(Attn_{i,j}\) indicates the attention weight that captures the relation between the \(i\)-th teacher feature and the \(j\)-th student feature. Therefore, \(Attn\) can make the teacher feature \(z_{t}^{n}\) transmit its corresponding information selectively and adaptively to different student features. Finally, the second term on the right-hand side of Eq. (7) can be written as:

\[\underset{p(x,y)p(z|x)}{\mathbb{E}}\Bigg{[}\log\frac{p(z|x)}{q(z|t)}\Bigg{]}= \underset{p(x)}{\mathbb{E}}\Bigg{[}\sum_{i}^{n}\sum_{j}^{m}Attn_{i,j}\big{(} \mathbf{T}_{t}^{i}(z_{t}^{i})-\mathbf{T}_{s}^{j}(z_{s}^{j})\big{)}^{2}\Bigg{]},\] (14)

where \(\mathbf{T}\) is a transform function with a feature map size alignment (up-sampled or down-sampled), and then apply a channel-wise average pooling to get an average feature map for loss computation.

Combing Eq.9 and Eq.14, the final objective of IBD can be defined as follows:

\[L_{IBD}=\min\underset{p(x)}{\mathbb{E}}\Bigg{[}\mathcal{L}_{CE}\big{(}h_{s}(x ),y_{t}\big{)}+\beta\sum_{i}^{n}\sum_{j}^{m}Attn_{i,j}\big{(}\mathbf{T}_{t}^{i }(z_{t}^{i})-\mathbf{T}_{s}^{j}(z_{s}^{j})\big{)}^{2}\Bigg{]}.\] (15)

### Applying IBD to Robust Learning

IBD can be naturally applied in combination with adversarial training. The final objective function is formulated as follows:

\[\begin{split} L_{obj}=\min\underset{p(x)}{\mathbb{E}}\Bigg{[}(1- \alpha)\mathcal{L}_{CE}\big{(}h_{s}(x_{\text{nat}}),y_{t}\big{)}+\alpha \mathcal{L}_{CE}\big{(}h_{s}(x_{\text{adv}}),y_{t}\big{)}\\ +\beta\sum_{i}^{n}\sum_{j}^{m}Attn_{i,j}\big{(}\mathbf{T}_{t}^{i}( z_{t}^{i})-\mathbf{T}_{s}^{j}(z_{s}^{j})\big{)}^{2}\Bigg{]},\end{split}\] (16)

where \(\alpha\) and \(\beta\) are two trade-off hyper-parameters. We generate adversarial examples using the same method introduced by [71], where given a natural input \(x_{\text{nat}}\), generated the adversarial example \(x_{\text{adv}}\) by maximizing KL-divergence term. Note that the intermediate features \(Z_{t}\) and \(Z_{s}\) are extracted from \(x_{\text{adv}}\). Finally, we utilize this new loss function to train a robust model.

### Discussion

The objective of IBD is comparable to certain conventional KD techniques [6; 10; 32]. However, the major difference is, that in our IBD, the teacher model must be an adversarially pre-trained model that can provide robust information. Additionally, we design the objective function following the information bottleneck principle, which is theoretically proven to be a lower bound of the information bottleneck. Notably, previous adversarial distillation methods [21; 44; 74] only consider utilizing information from the final prediction output, while ignoring the information from intermediate features. From another perspective, motivated by [31], IBD can explore intermediate features to capture the robust information and leverage an adaptive feature distillation strategy to automatically transfer appropriate features from the teacher model to the target student model. Our experiments indicate that making full use of the intermediate features of the DNNs can improve the model's robustness effectively.

## 4 Experiments

In this section, we first describe the experimental setting and implementation details. We evaluate the robustness and accuracy of various widely used benchmark datasets. Finally, we conduct a wealth of ablation experiments to provide a comprehensive understanding of the proposed IBD.

### Experiments Settings

**Baselines Setup.** We conduct our experiments on three benchmark datasets including CIFAR-10, CIFAR-100 [36] and ImageNet [17]. We consider three types of baseline model: 1) classical adversarial training method, including standard Standard AT[41] and TRADES [71]; 2) adversarial robustness distillation, including ARD [21], IAD [73] and RSLAD [74]; 3) the models trained using variants IB, include CEB [19], HBaR [64] and InfoAT [68].

**Evaluation Attack.** We evaluate the defense under different white- and black-box attacks including FGSM [22], PGD [41], CW [4], and AutoAttack [14]. The black-box attacks include query-based

\begin{table}
\begin{tabular}{l||c c c c|c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c|}{Best Checkpoint} & \multicolumn{4}{c}{Last Checkpoint} \\  & Clean & FGSM & PGD & CW & AA & Clean & FGSM & PGD & CW & AA \\ \hline
**CIFAR-10** - \(l_{\inf}-\epsilon=8/255\) & & & & & & & & & \\ \hline SAT [41] & 82.97 & 57.77 & 50.85 & 50.09 & 47.73 & 85.16 & 53.97 & 43.03 & 43.71 & 41.58 \\ TRADES [71] & 83.74 & 59.54 & 52.73 & 50.94 & 49.58 & 84.11 & 58.72 & 49.97 & 49.05 & 47.02 \\ CEB [19] & 82.87 & 58.61 & 52.94 & 50.22 & 48.87 & 83.39 & 57.74 & 48.79 & 47.67 & 45.85 \\ HBaR [64] & **84.13** & 58.95 & 53.35 & 51.47 & 49.77 & **85.04** & 58.33 & 53.19 & 51.21 & 48.64 \\ InfoAT [68] & 83.17 & 60.52 & 54.29 & 51.62 & 49.92 & 83.23 & 60.40 & 53.94 & 51.17 & 49.67 \\ ARD [21] & 83.94 & 59.32 & 52.16 & 51.21 & 49.17 & 84.32 & 59.35 & 51.41 & 51.22 & 48.89 \\ IAD [73] & 83.24 & 59.34 & 54.24 & 51.92 & 50.63 & 83.76 & 59.17 & 53.84 & 51.60 & 50.17 \\ RSLAD [74] & 83.38 & 60.08 & 54.27 & 53.19 & 51.52 & 83.88 & 59.98 & 54.01 & 53.08 & 51.36 \\ \hline
**IBD** (Ours) & 83.17 & **60.75** & **55.13** & **53.62** & **52.11** & 82.95 & **60.94** & **55.04** & **53.49** & **52.05** \\ \hline \hline
**CIFAR-100** - \(l_{\inf}-\epsilon=8/255\) & & & & & & & & & \\ \hline SAT [41] & 57.75 & 32.78 & 29.27 & 27.52 & 24.01 & 58.17 & 27.02 & 21.02 & 21.54 & 19.86 \\ TRADES [71] & 58.57 & 32.84 & 29.88 & 26.29 & 25.27 & 57.11 & 32.05 & 28.12 & 25.47 & 24.52 \\ CEB [19] & 55.17 & 32.36 & 30.12 & 26.35 & 25.36 & 55.74 & 30.84 & 26.59 & 24.51 & 23.16 \\ HBaR [64] & 59.53 & 34.46 & 31.82 & 27.42 & 26.62 & 58.26 & 32.41 & 28.23 & 25.78 & 25.43 \\ InfoAT [68] & 58.23 & 34.46 & 31.39 & 28.68 & 26.76 & 58.42 & 33.13 & 30.53 & 27.52 & 26.21 \\ ARD [21] & **60.58** & 33.43 & 29.07 & 27.54 & 25.62 & **60.79** & 32.67 & 28.11 & 26.76 & 24.62 \\ IAD [73] & 57.08 & 34.65 & 30.60 & 27.24 & 25.84 & 57.52 & 33.71 & 29.23 & 27.35 & 25.36 \\ RSLAD [74] & 57.72 & 34.23 & 31.01 & 28.27 & 26.73 & 57.83 & 34.09 & 30.55 & 28.07 & 26.41 \\ \hline
**IBD** (Ours) & 58.10 & **36.37** & **33.59** & **31.16** & **29.21** & 58.32 & **36.17** & **33.40** & **30.87** & **28.74** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Robustness comparison of the proposed approach and baseline models under different attack methods under the \(\ell_{\infty}\) norm with \(\epsilon=8/255\) on different datasets. All the models are based on pre-activation ResNet-18 architecture. We choose the best checkpoint according to the highest robust accuracy on the test set under PGD-10. The best results are **b****lodfaced**.

attacks [1] and transfer-based attacks [59]. The maximum pertubation is set to \(\epsilon=8/255\) for CIFAR-10 and CIFAR-100.

**Implementation Details.** For the robust pre-trained teacher models on the CIFAR dataset, we use the ResNet-18 [26] and WideResNet-34x10 [70] models trained with a way of TRADES [71] and AWP [66]. For the ImageNet dataset, the pre-trained teacher model (a ResNet-50) is provided by [49]. The results of the pre-trained teacher models under different attacks are shown in Table 8. The initial learning rate is 0.1 with a piece-wise schedule which is divided by 10 at epochs 100 and 150 for a total number of 200 training epochs, similar to [47]. We train all models with the SGD optimizer with a momentum of 0.9, weight decay of 0.0005, and a batch size of 128. We use ResNet-18 as the student model for most experiments by default, unless otherwise stated. We adopt the common setting that the \(\ell_{\infty}\) threat model with radius \(8/255\), with the PGD attack taking 10 steps of size \(2/255\). In addition, we performed standard data augmentation, including random crops and random horizontal flips during training. For the hyper-parameter, we set \(\alpha=0.9\) and \(\beta=0.8\) based on our ablation studies. For more details please refer to our open source code. Our implementation is based on PyTorch and the code to reproduce our results is available at https://github.com/SkyKuang/IBD.

### Adversarial Robustness Evaluation

**White-box Robustness.** To verify the impact of the IBD on model robustness, we first train a natural model (without adversarial training), We evaluate the performance of IBD on CIFAR-10 under ResNet-18, and IBD achieves \(25.49\%\) robust accuracy against standard AA. However, other IB-based methods (CEB and HBaR) without adversarial training can not defend AA (that means the robust accuracy is 0). We further combine IBD with adversarial training and evaluate the robustness of all baseline models and our IBD against various types of attacks. Following in [47], we report the results at both the best checkpoint and the last checkpoint. The best checkpoint is selected based on the performance under the PGD-10 attack. The results are shown in Table 1. Our IBD method shows better robustness of both CIFAR-10 and CIFAR-100 against all attacks at either the best or the last checkpoints. In particular, our IBD improves the robustness by \(0.59\%\) and \(2.48\%\) on CIFAR-10 and CIFAR-100 respectively, compared to previous state-of-the-art methods under AutoAttack.

**Black-box Robustness.** To further verify the robustness of our method under black-box attacks, we evaluate our IBD and baseline methods against transfer-based attacks and query-based attacks. For transfer-based attacks, we choose a robust surrogate model, which is trained by standard adversarial training, to generate the adversarial examples. All the models are trained using the same setting

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline Method & WRN & Clean & AA \\ \hline SAT [41] & 34-10 & 84.92 & 53.08 \\ LBGAT [15] & 34-20 & 88.70 & 53.57 \\ TRADES [43] & 34-20 & 86.18 & 54.39 \\ LTD [7] & 34-10 & 85.02 & 54.45 \\
**IBD** & 34-10 & 83.33 & **55.65** \\ \hline TRADES + AWP [66] & 34-10 & 85.26 & 56.17 \\ LASAT + AWP [33] & 34-10 & 84.98 & 56.26 \\ LTD + AWP [7] & 34-10 & 86.28 & 56.94 \\
**IBD** + AWP & 34-10 & 85.21 & **57.18** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Robustness comparison of the proposed IBD and several state-of-the-art models under standard AutoAttack.

\begin{table}
\begin{tabular}{l|c|c c c c c} \hline \hline Dataset & Architecture & Natural Acc & FGSM & PGD-100 & CW-100 & AutoAttack \\ \hline CIFAR-10 & WideResNet-34-10 & 84.92 & 60.87 & 55.33 & 53.98 & 53.08 \\ CIFAR-100 & WideResNet-34-10 & 57.16 & 33.58 & 30.61 & 27.74 & 26.78 \\ ImageNet-1K & ResNet-50 & 64.02 & - & 38.46 & - & 34.96 \\ \hline \hline \end{tabular}
\end{table}
Table 2: The performance of the pre-trained teacher models under different attacks.

\begin{table}
\begin{tabular}{l|c c|c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Trans-based} & \multicolumn{2}{c}{Query-based} \\ \cline{2-5}  & FGSM & PGD-100 & SPSA & Square \\ \hline SAT[41] & 61.34 & 59.83 & 66.35 & 54.16 \\ TRADES[71] & 63.14 & 60.31 & 67.56 & 54.74 \\ CEB[19] & 62.75 & 61.93 & 66.71 & 55.28 \\ HBaR[64] & 63.61 & 60.93 & 68.07 & 56.62 \\ Infcat [68] & 64.23 & 62.42 & 68.73 & 57.35 \\ ARD [21] & 63.27 & 61.44 & 67.89 & 55.85 \\ IAD [73] & 63.53 & 62.13 & 68.42 & 56.21 \\ RSLAD[74] & 64.78 & 62.13 & 68.78 & 56.97 \\
**IBD** & **65.54** & **63.98** & **69.63** & **58.34** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Black-box robustness results in CIFAR-10 under the \(\ell_{\infty}\) norm with \(\epsilon=8/255\). We evaluate against transfer-based and query-based attacks.

(referring to Section 4.1). We generate adversarial examples using both FGSM and PGD-100 attacks with attack budget \(\epsilon=8/255\) on CIFAR-10. For query-based attacks, we evaluate the robustness under SPSA attack [60] and Square attack [13]. SPSA attack can make a full gradient evaluation by drawing random samples and obtaining the corresponding loss values. We set the number of random samples \(q\) as 128 for every iteration and \(\delta=0.01\) and set iteration as 10. The square attack uses random search and does not exploit any gradient approximation to produce adversarial noise, and we set the maximum number of queries as 5000. We evaluate both transfer-based and query-based attacks on the best checkpoints. The results are shown in Table 3. We observe that the IBD-trained model is robust to different types of black-box attacks and surpasses all baseline methods. The results under the black-box attack demonstrate the robustness improvement of IBD is not caused by the obfuscated gradients.

**Robustness Evaluation on WideResNet.** Many works have demonstrated larger model capacity can usually lead to better adversarial robustness [23; 41; 43]. Therefore, we employ the large-capacity network, _e.g._, WideResNet (WRN) [70], as the student model. Table 4 reports the robustness results against AA on the CIFAR-10. We compare several state-of-the-art adversarial trained models on robust benchmark [12]. We can observe that the proposed IBD indeed improves the adversarial robustness by \(\sim 1.2\%\). Furthermore, when combined with AWP [66], our IBD also surpasses the previously state-of-the-art models reported by the benchmark. where every small margin of improvement is significant. **Note that**, our method does not use any additional datasets.

**Evaluation on Large-scale ImageNet.** To further verify the generalization of our method, we consider a high-resolution, large-scale ImageNet dataset[17], which includes 1,000 classes and more than 1M training samples. Maintaining robustness for this dataset is particularly challenging. We use the fast adversarial training framework [65] to train all robust models. Here, the robust budgets are set as \(\epsilon=2/255\) and \(\epsilon=4/255\). The results of our method for the ResNet-50 (student model) are shown in Table 5. Our IBD outperforms both Fast-AT [62] and RSLAD [74], significantly.

### Ablation Studies

**The importance of robust soft label.**[52] proposes leveraging label smoothing during adversarial training, and [74] also determines the robust soft label is an important factor in robustness enhancement. We empirically verify the importance of robust soft labels in our IBD, by comparing the performance of models trained using different label modification schemes: a) true label (hard label); b) smoothing label crafted by label smoothing on the true label [55]; c) natural soft label that output by non-robust pre-trained model; 4) robust soft label that output by robust pre-trained model. The results are summarized in Table 6, which demonstrates that the robust soft label is beneficial to improving the model's robustness.

**The role of features matching.** We further investigate the role of features distillation strategy in section 3.2, we consider two features matching strategies: a) a hand-crafted feature distillation method, which uses the same level of features to guide the student model learning; b) an adaptive feature distillation method, which is based on attention mechanism to guide the student model learning. We conduct experiments with these two strategies on different network architectures ResNet-18 (Res) and WideResNet-34x10 (WRN). The results are reported in Table 7. Our adaptive feature

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline Method & Epsilon & Clean & PGD-100 & AutoAttack \\ \hline FAST-AT [65] & \(\epsilon=2\) & **65.95** & 37.52 & 35.22 \\ RSLAD [74] & \(\epsilon=2\) & 63.64 & 40.49 & 38.35 \\ FAST-IBD & \(\epsilon=2\) & 62.03 & **44.31** & **40.94** \\ \hline FAST-AT [65] & \(\epsilon=4\) & 60.16 & 27.46 & 24.82 \\ RSLAD [74] & \(\epsilon=4\) & **60.64** & 30.19 & 26.53 \\ FAST-IBD & \(\epsilon=4\) & 59.10 & **31.52** & **27.74** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Robustness comparison of the proposed approach and different fast adversarial training under different attack methods at \(\epsilon=2/255\) and \(\epsilon=4/255\) on ImageNet-1k. All the models are based on ResNet-50 architecture.

distillation performs better than hand-crafted feature distillation in both the same and different network architectures.

**The impact of the teacher.** We conducted an ablation experiment by using different teacher models to verify the impact of the teacher's robustness on the performance of the student model. We conduct this experiment on CIFAR-10 with two student models: ResNet-18 and WideResNet-34-10, and five different teacher models which have different robustness. The results are shown in Table 8 in the Appendix. We can observe that different robust teacher models have a significant positive benefit on the student model. For the ResNet-18 student model, we find that the robustness of the student does not increase monotonically with that of the teacher. As the teacher model (WideResNet-34-20) becomes more complex, the robustness of the student model decreases, compared to WideResNet-34-10. This may be due to the large gap in the architecture of the teacher model and the student model. This phenomenon is called robust saturation [74]. For the WideResNet-34-10 student model, we found that in most cases, the student's robustness can surpass that of the teacher model. We think there are two reasons for this, one is that the performance of the teacher model is not very strong. The other is that the teacher model provides robust soft labels to alleviate overfitting and improve performance. Therefore, in most cases, it is expected that the student model exceeds the teacher model, but when the teacher model is strong enough, it is not easy for the student model to surpass the teacher model (e.g., WideResNet-76-10).

**The impact of \(\alpha\) and \(\beta\).** The \(\alpha\) is a trade-off the adversarial robustness and natural accuracy. We conduct ablation experiments to verify the trade-off. The results are shown in Figure 2(a) in the Appendix. When we set \(\alpha=0.9\), our method can achieve the best adversarial robustness. In the IB principle, the hyperparameters \(\beta\) are important to control the trade-off. Therefore, we finally choose different values of \(\beta\) to train the IBD, from which we choose the optimal value of \(\beta\). The experimental results are shown in Figure 2(b) in the Appendix, where the best results are achieved when setting \(\beta=0.8\).

## 5 Conclusions

In this paper, we revisited the information bottleneck principle from the perspective of robustness distillation and then presented a new IB objective, called Information Bottleneck Distillation (IBD). IBD can be thought of as a tighter variational approximation to the IB objective than VIB. To optimize IBD effectively, we proposed two adaptive distillation strategies. Experimentally, we empirically demonstrated the advantage of IBD over existing methods, IB-based methods, and adversarial distillation methods on benchmark datasets.

## 6 Acknowledgments

This work was supported by National Key R&D Program of China (No.2022ZD0118202), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No.2021J01002, No.2022J06001).

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline Method & Clean & PGD & CW & AA \\ \hline Hard label & 85.25 & 51.64 & 50.49 & 48.35 \\ Label smooth & 85.43 & 51.92 & 50.56 & 48.71 \\ Natural soft label & 86.38 & 48.08 & 45.70 & 43.79 \\ Robust soft label & 83.17 & 55.13 & 53.62 & 52.11 \\ \hline RSLAD [74] & 83.38 & 54.27 & 53.19 & 51.52 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Robustness of our IBD model trained using different types of labels on CIFAR-10 under different attacks.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline Architecture & Schedule & Clean & AA \\ \hline Res \(\rightarrow\) Res & No matching & 83.25 & 50.63 \\ WRN \(\rightarrow\) Res & No matching & 83.37 & 51.48 \\ \hline Res \(\rightarrow\) Res & Hand-crafted & 82.99 & 51.12 \\ WRN \(\rightarrow\) Res & Hand-crafted & 83.10 & 51.61 \\ \hline Res \(\rightarrow\) Res & Adaptive & 83.24 & 51.46 \\ WRN \(\rightarrow\) Res & Adaptive & 83.17 & 52.11 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Robustness of our IBD model trained using different features matching schedule on CIFAR-10 under the AutoAttack.

## References

* [1]N. Akhtar and A. Mian (2018) Threat of adversarial attacks on deep learning in computer vision: a survey. IEEE Access6, pp. 14410-14430. Cited by: SS1.
* [2]A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy (2016) Deep variational information bottleneck. arXiv preprint arXiv:1612.00410. Cited by: SS1.
* [3]M. Andriushchenko, F. Croce, N. Flammarion, and M. Hein (2019) Square attack: a query-efficient black-box adversarial attack via random search. arXiv preprint arXiv:1912.00049. Cited by: SS1.
* [4]N. Carlini and D. Wagner (2017) Towards evaluating the robustness of neural networks. In Symposium on Security and Privacy (SP), pp. 39-57. Cited by: SS1.
* [5]Y. Carmon, A. Raghunathan, L. Schmidt, J. C. Duchi, and P. S. Liang (2019) Unlabeled data improves adversarial robustness. In NeurIPS, pp. 11192-11203. Cited by: SS1.
* [6]D. Chen, J. Mei, Y. Zhang, C. Wang, Z. Wang, Y. Feng, and C. Chen (2021) Cross-layer distillation with semantic calibration. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35, pp. 7028-7036. Cited by: SS1.
* [7]E. Chen and C. Lee (2021) Ltd: low temperature distillation for robust adversarial training. arXiv preprint arXiv:2111.02331. Cited by: SS1.
* [8]H. Chen, B. Zhang, S. Xue, X. Gong, H. Liu, R. Ji, and D. Doermann (2020) Anti-bandit neural architecture search for model defense. In ECCV, pp. 70-85. Cited by: SS1.
* [9]J. Chen, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [10]P. Chen, S. Liu, H. Zhao, and J. Jia (2021) Distilling knowledge via knowledge review. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5008-5017. Cited by: SS1.
* [11]Z. Craneko, A. K. Menon, R. Nock, C. S. Ong, Z. Shi, and C. Walder (2019) Monge blunts bayes: hardness results for adversarial training. In ICML, Cited by: SS1.
* [12]F. Croce, M. Andriushchenko, V. Sethwag, E. Debenedetti, N. Flammarion, M. Chiang, P. Mittal, and M. Hein (2020) Robustbench: a standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670. Cited by: SS1.
* [13]F. Croce and M. Hein (2019) Minimally distorted adversarial examples with a fast adaptive boundary attack. arXiv preprint arXiv:1907.02044. Cited by: SS1.
* [14]F. Croce and M. Hein (2020) Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In ICML, Cited by: SS1.
* [15]J. Cui, S. Liu, L. Wang, and J. Jia (2021) Learnable boundary guided adversarial training. In ICCV, pp. 15721-15730. Cited by: SS1.
* [16]B. Dai, C. Zhu, B. Guo, and D. Wipf (2018) Compressing neural networks using the variational information bottleneck. In International Conference on Machine Learning, pp. 1135-1144. Cited by: SS1.
* [17]J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009) Imagenet: a large-scale hierarchical image database. In CVPR, pp. 248-255. Cited by: SS1.
* [18]Y. Dong, T. Pang, H. Su, and J. Zhu (2019) Evading defenses to transferable adversarial examples by translation-invariant attacks. In CVPR, pp. 4312-4321. Cited by: SS1.
* [19]I. Fischer (2020) The conditional entropy bottleneck. Entropy22 (9), pp. 999. Cited by: SS1.
* [20]J. Chen, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [21]J. Chen, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [22]J. Chen, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [23]J. Chen, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [24]J. Chen, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [25]J. Chen, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [26]J. Chen, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [27]J. Chen, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [28]J. Chen, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [29]J. Chen, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [30]J. Chen, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [31]J. Chen, M. I. Jordan, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [32]J. Chen, M. I. Jordan, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [33]J. Chen, M. I. Jordan, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [34]J. Chen, M. I. Jordan, M. I. Jordan, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [35]J. Chen, M. I. Jordan, M. I. Jordan, M. I. Jordan, M. I. Jordan, M. I. Jordan, and M. J. Wainwright (2020) Hopskipjupmattack: a query-efficient decision-based attack. In IEEE Symposium on Security and Privacy (SP), pp. 1277-1294. Cited by: SS1.
* [36]J. Chen, M. I.

* Fischer and Alemi [2020] Ian Fischer and Alexander A Alemi. Ceb improves model robustness. _Entropy_, 22(10):1081, 2020.
* Goldblum et al. [2020] Micah Goldblum, Liam Fowl, Soheil Feizi, and Tom Goldstein. Adversarially robust distillation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 3996-4003, 2020.
* Goodfellow et al. [2015] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In _ICLR_, 2015.
* Gowal et al. [2020] Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering the limits of adversarial training against norm-bounded adversarial examples. _arXiv preprint arXiv:2010.03593_, 2020.
* Guo et al. [2018] Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial images using input transformations. In _ICLR_, 2018.
* Guo et al. [2020] Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, and Dahua Lin. When nas meets robustness: In search of robust architectures against adversarial attacks. In _CVPR_, pages 631-640, 2020.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.
* Hendrycks et al. [2019] Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness and uncertainty. In _ICML_, 2019.
* Hinton et al. [2015] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2(7), 2015.
* Hosseini et al. [2021] Ramtin Hosseini, Xingyi Yang, and Pengtao Xie. Dsrna: Differentiable search of robust neural architectures. In _CVPR_, pages 6196-6205, 2021.
* Huang et al. [2023] Shihua Huang, Zhichao Lu, Kalyanmoy Deb, and Vishnu Naresh Boddeti. Revisiting residual networks for adversarial robustness: An architectural perspective. _CVPR_, 2023.
* Ilyas et al. [2019] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In _NeurIPS_, pages 125-136, 2019.
* Ji et al. [2021] Mingi Ji, Byeongho Heo, and Sungrae Park. Show, attend and distill: Knowledge distillation via attention-based feature matching. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 7945-7952, 2021.
* Jia et al. [2022] Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. Las-at: Adversarial training with learnable attack strategy. In _CVPR_, pages 13398-13408, 2022.
* Kim et al. [2018] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks. _Advances in Neural Information Processing Systems_, 31, 2018.
* Kim et al. [2021] Junho Kim, Byung-Kwan Lee, and Yong Man Ro. Distilling robust and non-robust features in adversarial examples by information bottleneck. _Advances in Neural Information Processing Systems_, 34, 2021.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
* Levina and Bickel [2004] Elizaveta Levina and Peter Bickel. Maximum likelihood estimation of intrinsic dimension. _Advances in neural information processing systems_, 17, 2004.
* Li et al. [2021] Jie Li, Rongrong Ji, Peixian Chen, Baochang Zhang, Xiaopeng Hong, Ruixin Zhang, Shaoxin Li, Jilin Li, Feiyue Huang, and Yongjian Wu. Aha! adaptive history-driven attack for decision-based black-box models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 16168-16177, 2021.

* Liu et al. [2023] Hong Liu, Zhun Zhong, Nicu Sebe, and Shin'ichi Satoh. Mitigating robust overfitting via self-residual-calibration regularization. _Artificial Intelligence_, 317:103877, 2023.
* Ma et al. [2020] Wan-Duo Kurt Ma, JP Lewis, and W Bastiaan Kleijn. The hsic bottleneck: Deep learning without back-propagation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5085-5092, 2020.
* Madry et al. [2018] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _ICLR_, 2018.
* Mao et al. [2019] Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, and Baishakhi Ray. Metric learning for adversarial robustness. In _NeurIPS_, pages 480-491, 2019.
* Pang et al. [2021] Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial training. In _ICLR_, 2021.
* Papernot et al. [2016] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In _IEEE Symposium on Security and Privacy (SP)_, pages 582-597, 2016.
* Raghunathan et al. [2018] Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying robustness to adversarial examples. In _NeurIPS_, pages 10877-10887, 2018.
* Rebuffi et al. [2021] Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Timothy Mann. Fixing data augmentation to improve adversarial robustness. _arXiv preprint arXiv:2103.01946_, 2021.
* Rice et al. [2020] Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In _ICML_, pages 8093-8104. PMLR, 2020.
* Roth et al. [2019] Kevin Roth, Yannic Kilcher, and Thomas Hofmann. Adversarial training generalizes data-dependent spectral norm regularization. _arXiv preprint arXiv:1906.01527_, 2019.
* Salman et al. [2020] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially robust imagenet models transfer better? _Advances in Neural Information Processing Systems_, 33:3533-3545, 2020.
* Salman et al. [2019] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In _NeurIPS_, pages 11289-11300, 2019.
* Samangouei et al. [2018] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against adversarial attacks using generative models. In _ICLR_, 2018.
* Shafahi et al. [2019] Ali Shafahi, Amin Ghiasi, Furong Huang, and Tom Goldstein. Label smoothing and logit squeezing: a replacement for adversarial training? _arXiv preprint arXiv:1910.11585_, 2019.
* Shafahi et al. [2019] Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David Jacobs, and Tom Goldstein. Adversarially robust transfer learning. _arXiv preprint arXiv:1905.08232_, 2019.
* Shamir et al. [2010] Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information bottleneck. _Theoretical Computer Science_, 411(29-30):2696-2711, 2010.
* Szegedy et al. [2016] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2818-2826, 2016.
* Szegedy et al. [2014] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _ICLR_, 2014.
* Tishby et al. [2000] Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. _arXiv preprint physics/0004057_, 2000.
* Tishby and Zaslavsky [2015] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In _2015 ieee information theory workshop (itw)_, pages 1-5. IEEE, 2015.

* Tramer et al. [2018] Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In _ICLR_, 2018.
* Uesato et al. [2018] Jonathan Uesato, Brendan O'donoghue, Pushmeet Kohli, and Aaron Oord. Adversarial risk and the dangers of evaluating against weak attacks. In _ICML_, pages 5025-5034. PMLR, 2018.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Wang et al. [2018] Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings and knowledge graphs. In _CVPR_, pages 6857-6866, 2018.
* Wang et al. [2023] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. _ICML_, 2023.
* Wang et al. [2021] Zifeng Wang, Tong Jian, Aria Masoomi, Stratis Ioannidis, and Jennifer Dy. Revisiting hilbert-schmidt information bottleneck for adversarial robustness. _Advances in Neural Information Processing Systems_, 34, 2021.
* Wong et al. [2020] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training. In _ICLR_, 2020.
* Wu et al. [2020] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In _NeurIPS_, 2020.
* Wu et al. [2018] Lei Wu, Zhanxing Zhu, Cheng Tai, et al. Understanding and enhancing the transferability of adversarial examples. _arXiv preprint arXiv:1802.09707_, 2018.
* Xu et al. [2022] Mengting Xu, Tao Zhang, Zhongnian Li, and Daoqiang Zhang. Infoat: Improving adversarial training using the information bottleneck principle. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* Yang et al. [2019] Yuzhe Yang, Guo Zhang, Dina Katabi, and Zhi Xu. Me-net: Towards effective adversarial robustness with matrix estimation. In _ICML_, 2019.
* Zagoruyko and Komodakis [2016] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In _BMVC_, 2016.
* Zhang et al. [2019] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan. Theoretically principled trade-off between robustness and accuracy. In _ICML_, 2019.
* Zhang et al. [2020] Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In _ICML_, pages 11278-11287. PMLR, 2020.
* Zhu et al. [2021] Jianing Zhu, Jiangchao Yao, Bo Han, Jingfeng Zhang, Tongliang Liu, Gang Niu, Jingren Zhou, Jianliang Xu, and Hongxia Yang. Reliable adversarial distillation with unreliable teachers. _arXiv preprint arXiv:2106.04928_, 2021.
* Zi et al. [2021] Bojia Zi, Shihao Zhao, Xingjun Ma, and Yu-Gang Jiang. Revisiting adversarial robustness distillation: Robust soft labels make student better. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 16443-16452, 2021.

Variational Information Bottleneck

We show the derivation process of Variational Information Bottleneck (VIB), which is provided by [2, 35]. The information bottleneck (IB) [57] expresses a trade-off in intermediate features \(Z\) between information useful for output prediction \(Y\) and information retained about the input \(X\). The objective of IB can be formulated as follows:

\[IB(\theta)=\max I(Z;Y)-\beta I(Z;X),\] (17)

where \(I\) denotes mutual information and \(\beta\) controls the trade-off between the two terms.

The first term can be expressed as follows:

\[\begin{split} I(Z;Y)&=\int\,p(y,z)\log\frac{p(y, z)}{p(y)p(z)}dydz\\ &=\int\,p(y,z)\log\frac{p(y|z)}{p(y)}dydz\end{split}\] (18)

To apply it to a deep neural network \(h_{\theta}=(g*f)\), The \(q(y|z)\) is modelled by the classifier \(g\). Based on variational inference, it is a closed form for a true likelihood \(p(y|z)\). This approximation is formulated by KL divergence and then the following inequality is constructed as follows:

\[KL\big{(}p(Y|Z)\|q(Y|Z)\big{)}\geq 0\quad\Rightarrow\quad\int\,p(y|z)\log p(y|z) dy\geq\int\,p(y|z)\log q(y|z)dy\] (19)

which helps to get the objective of IB to be tractable. With this equality, the first term in Eq. (17) can be represented to a lower bound as:

\[\begin{split} I(Z;Y)&\geq\int\,p(y,z)\log\frac{q( y|z)}{p(y)}dydz\\ &=\int\,p(y,z)\log q(y|z)dydz-\int\,p(y,z)\log p(y)dydz\\ &=\int\,p(y,z)\log q(y|z)dydz-\int\,p(y)\log p(y)dy\\ &=\int\,p(y,z)\log q(y|z)dydz+H(Y)\\ &\geq\int\,p(y,z)\log q(y|z)dydz=\mathbb{E}_{p(x,y)p(z|x)}\left[ \log q(y|z)\right]\end{split}\] (20)

where a positive constant \(H(Y)\) denotes the Shannon entropy of target labels.

The second term in Eq. (17) is described as :

\[\begin{split} I(Z;X)&=\int\,p(z,x)\log\frac{p(z,x)} {p(x)p(z)}dzdx\\ &=\int\,p(z,x)\log\frac{p(z\,|\,x)}{p(z)}dzdx\end{split}\] (21)

where a dataset probability \(p(x)\) is erased on the fraction. Here, an approximate feature probability \(q(Z)\) is introduced to appropriate the true feature probability \(p(Z)\). As similar to Eq.(19), the relationship between \(q(Z)\) and \(p(Z)\) can be written and then it builds the following equality:

\[KL\big{(}p(Z)\|q(Z)\big{)}\geq 0\quad\Rightarrow\quad\int\,p(z)\log p(z)dz \geq\int\,p(z)\log q(z)dz\] (22)

By using it, the second term is constructed with an upper bound as follows:

\[\begin{split} I(Z;X)&\leq\int\,p(z,x)\log\frac{p(z \,|\,x)}{q(z)}dzdx\\ &=\int\,p(x)p(z\,|\,x)\log\frac{p(z\,|\,x)}{q(z)}dzdx\\ &=\mathbb{E}_{p(x,y)p(z\,|x)}\left[\log\frac{p(z|x)}{q(z)}\right] \end{split}\] (23)

where a feature likelihood is denoted by \(p(z|x)\). To sum it up, the objective of IB can be re-formulated with a lower bound as follows:

\[I(Z;Y)-\beta I(Z;X)\geq\mathbb{E}_{p(x,y)p(z|x)}\left[\log q(y|z)-\beta\log \frac{p(z|x)}{q(z)}\right],\] (24)

[MISSING_PAGE_FAIL:16]

### The impact of \(\alpha\) and \(\beta\).

### Different attack budgets and steps

We plot the results of testing robust accuracy over epochs and evaluate adversarial accuracy against PGD attacks under different attack budgets with a fixed attack step of 10, and we also conduct experiments using PGD attacks with different attack iterations with a fixed attack budget of 8/255. The results are shown in Figure 3. Our IBD is better than standard AT, TRADES and RSLAD at larger budgets, besides, our IBD is stable against large iterations attacks, _e.g.,_ PGD attack with 500 step iterations. Therefore, the results demonstrate the effectiveness of our proposed IBD.

### Visualization of attention matrix

We visualize the attention matrices when distilling between different network architectures, as shown in Figure 4. We can see that models of the same architecture can achieve mutual correspondence between feature layers, but for models of different architectures, the shallow layers of the student model will receive a lot of attention, and the high-level features will correspond to each other.

## Appendix D Limitations.

One limitation of our method is that the accuracy of our method on natural samples does not improve significantly, This may be because there exists a trade-off between robustness and natural accuracy [71]. Despite this, our natural accuracy is still over 83+% on CIFAR10, we think this is acceptable. Another limitation is that our method requires a robust pre-trained teacher model, which may increase training costs and training time. At present, our IBD has only verified its effect on the adversarial

Figure 3: The test robust accuracy under PGD attack with different attack budgets and attack iterations, respectively. All these experiments were conducted on CIFAR-10 using ResNet-18 architecture. (Best view in color)robustness of DNNs. It is not yet known whether it can achieve the same effect in other applications of IB. We are further studying its generalization ability.

## Appendix E Broader Impacts.

We propose an adversarial defense method to enhance the robustness of the model, but we still need to be aware of the potential negative societal impacts it might result in. For example, the attacker can get our model and design a special attack algorithm for it. At present, we can not guarantee that our model can defend against stronger attack algorithms that may appear in the future. Thus, we encourage our machine learning community to further establish more reliable adversarial robustness checking routines for machine learning models deployed in safety-critical applications.

Figure 4: The visualization of attention matrix.