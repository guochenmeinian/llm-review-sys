# MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning

Bin-Bin Gao

Tencent YouTu Lab, Shenzhen, China

csgaobb@gmail.com

Code and Models: https://github.com/gaobb/MetaUAS

###### Abstract

Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods.

## 1 Introduction

Visual anomaly classification (AC) and segmentation (AS) aims to group images and pixels into two different semantics, normal and anomalous, facilitating many applications such as industrial defect inspection in manufacturing [4; 39; 3; 74], medical image diagnosis [28; 65], and video surveillance [55; 40; 18], etc. Generally, AS is performed first, and then AC is obtained based on post-processing of the segmentation results. Therefore, AS is more essential than AC. AS can be viewed as binary semantic segmentation assuming that pixel-level annotated images are available. Unfortunately, it is usually difficult to collect anomaly images due to their scarcity in practical applications, and pixel-level annotations also involve labor costs. Based on available training images, e.g., only normal, normal with noise, few-shot normal or anomaly, and multi-class normal, have led to different types of AS tasks, such as unsupervised [47; 31; 72; 12; 37; 29], fully-unsupervised [9; 27; 38], few-shot [24; 14; 66; 69; 15], and unified AS [70; 17], etc. _These methods achieve excellent performance on seen objects but often perform poorly on unseen objects._

To address this fragmentation, recent works, such as WinCLIP [26] and AnomalyCLIP [76], have attempted to design universal models, that are capable of recognizing anomalies for unseen objects. They typically build on vision-language models (i.e., CLIP [42]), benefiting from strong generalizationability. However, WinCLIP [26] still struggles with handcrafted text prompts about defects. Further, AnomalyCLIP [76] learns general prompt embedding through developing specialized modules but requiring fine-tuning on an auxiliary domain dataset with pixel-level annotations. In addition to being flexible, these vision-language-based methods are still weak in anomaly segmentation. Accurate anomaly segmentation is crucial in real-world applications, such as industrial inspection, as anomalies often relate to the area, shape, and location where they occur. On the other hand, we know that visual representations are not dependent on language in the animal world [16]. In particular, the principles of visual perception in non-human primates are very similar to humans [58]. Although there is room for universal AS based on visual-language models and worthwhile further to pursue, in this paper, _we want to explore how far we can go with a pure visual model without any guidance from language._

To explore a more general AS framework, lets first review how the visual system perceives anomalies. Generally, humans can perceive anomalies when an input significantly deviates from those normal patterns stored in our brains. There is evidence to support this point in neuroscience. For example, predictive coding theory [43] postulates that the brain constantly generates and updates a "mental model". The mental model compares its expectations (or predictions) with the actual inputs from the visual cortex. This process allows the brain to perceive anomalies. In fact, PatchCore [47] captures normal local patch features, stores them in a memory bank, and recognizes anomalies by comparing input features with the memory bank. In addition, some distribution-based methods [12] learn a multivariate Gaussian distribution from normal local features and then utilize a distance metric to measure anomalies. However, these memory- and distribution-based methods usually require a certain number of normal images and thus are limited in universal (i.e., open-world) scenarios.

Actually, we can build similar concepts in AS. First, given one normal image prompt for each class, we take it as the expected output. Then, the actual input could be any query images from the same class of the normal prompt. Last but not least, how to construct a "mental model" to compare between a given normal image prompt and any query images. Despite these challenges, we can imagine that the "mental model" should satisfy several basic principles. First, it should have a strong generalization ability to perceive anomalies facing unseen objects or textures. Second, it can perform pixel-level anomaly segmentation only given one normal image prompt. Third, its training does not depend on target domain distribution or any guidance from language.

To obtain the "metal model", we rethink AS tasks and find they can be transformed into change segmentation between one normal image prompt and query images. From this novel perspective, we are capable of leveraging a larger number of synthetic image pairs that exhibit appearance changes based on available image datasets. We assume these synthesized image pairs carry mask annotations indicating change regions. Inspired by the "mental model" in predictive coding theory [43], we propose a simple but effective framework that learns the "metal model" in a one-prompt meta-learning manner. The meta-learning ensures strong generalization [8] when applying the model for segment unseen anomalies. Our contributions are summarized as follows:

* We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs with object-level and local region changes, thereby overcoming the long-standing challenge of lacking large-scale anomaly segmentation datasets.
* We propose a one-prompt meta-learning framework training on synthesized images and generalizing well on real-world scenarios. To handle geometrical variations between prompt and query images, we proposed a soft feature alignment module that builds a bridge between paired-image change perception and singe-image semantic segmentation.
* We provide a pure visual foundation model for universal anomaly segmentation that can serve as an alternative to widely used vision-language models. Our method, which requires only a single normal image prompt and no additional training, effectively and efficiently segments any visual anomalies. On three industrial anomaly benchmarks, our approach achieves state-of-the-art performance, while also enjoying faster speed and requiring fewer parameters.

## 2 Related Work

**Unsupervised AS** aims to segment anomaly pixels for both normal and anomaly testing images only given full normal training images. Unsupervised AS can be categorized into two learning paradigms, separated and unified models. Most AS methods focus on training separated models for different objects or textures. However, this separated paradigm may be impractical, as it requires high memory consumption and storage burden, especially with the number of classes increasing. In contrast, the unified models attempt to detect anomalies for all categories using a single model. Compared to the separated mode, the unified paradigm is more challenging as it requires handling more complex data distributions.

From a modeling perspective, AS methods can be roughly grouped into three groups, embedding, discriminator, and reconstruction. Embedding-based methods, such as PaDiM [12], MDND [44], PatchCore [47], CS-Flow [49] and PyramidFlow [29], assume that offline features extracted from a pre-trained model preserve discriminative information and thus help to separate anomalies from normal samples. They usually model normal features to a normal distribution or store them in a memory bank. Then, anomaly scores are calculated by comparing testing features and the modeled distribution or the memory bank. Discriminator-based methods, such as CutPaste [31], DRAEM [72], [10], and SimpleNet [37], typically convert unsupervised AS to supervised ones by introducing pseudo (synthesized) anomaly samples. The pseudo-anomaly samples are generated by pasting random patches or adding Gaussian noise to normal images or features. Naturally, a binary anomaly classifier or segmentation model can be trained on normal and pseudo-anomaly samples in a supervised manner. Reconstruction-based AS, such as autoencoder [59; 2; 20; 23], generative adversarial networks [41; 67; 71] and reconstruction networks [73; 45; 36], assume that anomalous regions should not be able to be properly reconstructed and thus result in high reconstruction errors since they do not exist in normal training samples. These methods tend to be computationally expensive because they involve reconstruction in image space. The recent knowledge distillation [5; 62; 61; 52; 13] or feature reconstruction methods [70; 75; 68] train a student or reconstruction network to match a fixed pre-trained teacher network and achieve a good balance between effectiveness and efficiency. However, all these methods are limited to recognizing anomalies in a close set as the same as the training set but often perform poorly on unseen classes in open-world scenarios.

**Few-shot AS** pays attention to learning with only a limited number of normal samples. TDG [54] proposes a multi-scale hierarchical generative model, which jointly learns self-supervised discriminator and generator in an adversarial training manner. DifferNet [48] detects defects utilizing a normalizing-flow-based density estimation from a few normal image features. RegAD [24] learns the category-agnostic feature registration, enabling the model to detect anomalies in new categories given a few normal images without fine-tuning. GraphCore [66] utilizes graph representation and provides a visual isometric invariant feature. FastRecon [15] utilizes a few normal samples as a reference to reconstruct a normal version for a query sample with distribution regularization, where the final anomaly detection can be achieved by sample alignment. Some works [14; 69] consider another few-shot setting where a limited number of samples is given from the anomalous classes. Instead of learning few-shot models with a few normal or anomaly images, we push it to a new extreme only using one normal image as a visual prompt at the inference stage, not involving model training.

**Zero- and Few-shot AS** mainly utilizes large pre-trained vision-language models, e.g., CLIP [42], have shown unprecedented generality, and achieved impressive performance. WinCLIP [26] firstly utilizes multiple handcrafted textual prompts on a powerful CLIP model that can yield excellent zero- and few-shot AS performance. AnomalyCLIP [76] learns object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. InCtrl [77] detects residual CLIP visual features between test images and in-context few-shot normal samples. However, optimizing AnomalyCLIP [76] and InCtrl [77] requires an auxiliary domain dataset including normal and anomaly images. PromptAD [32] introduces the concept of explicit anomaly margin, which mitigates the training challenge caused by the absence of anomaly images. Furthermore, AnomalyGPT [21] incorporates a visual-language model and a large language model applying multi-turn dialogues. It not only indicates the presence and location of the anomaly but also provides a detailed description of anomalies. Instead of visual-language models, ACR [30] and MuSc [33] perform zero-shot AS only requiring information from batch- and full-level testing images, but they may be limited in privacy protection scenarios. Overall, existing most methods primarily use textual prompts based on visual-language models to identify anomalies. Different from these methods, we explore universal AS using one normal image as a visual prompt without guidance from language or information from testing images.

## 3 Method

### Rethinking Anomaly Segmentation

Unlike traditional image segmentation, since anomaly appearance has various ways, it is hard to exhaustively pre-define and collect enough anomaly samples to train an AS model. Most unsupervised AS methods have to use normal samples for building models. However, these unsupervised models are limited to recognizing anomalies in a close set but often perform poorly on unseen classes in open-world scenarios. In contrast, humans can quickly learn novel concepts from a few training examples. To this end, few-shot AS aims to adapt novel classes by only providing a few normal images. Unfortunately, existing few-shot AS models are still far behind unsupervised ones.

Zero- and few-shot AS methods rely on powerful vision-language models that are capable of handling unseen anomalies, benefiting from their strong generalization. For further boosting zero-shot performance, some recent works attempt to optimize models by an auxiliary domain dataset including normal and anomaly images with pixel-level annotations. Although they can generalize to different domains, training models with normal and anomaly images conflict with the original intentions of anomaly segmentation to some extent. In addition, visual representations are not dependent on language prompts. Given this perspective, a natural question emerges: _can an image prompt visual model replace textual prompts vision-language approaches for universal anomaly segmentation? And can such a one-prompt vision model be trained on a non-anomaly segmentation dataset?_

This paper explores and attempts to answer the above questions. Generally, anomalies mainly include "appearance", "disappearance", and "exchange", which are very similar to the types of changes in change segmentation [60]. Change segmentation aims to identify changes that occur on a pair of images captured at different times. Therefore, anomaly segmentation will be absorbed into change segmentation if we regard normal prompt and query as a pair of images captured at different times. Indeed, one can imagine that if a model is capable of perceiving changes, it would naturally generalize to anomaly segmentation. This simple transformation allows us to achieve universal AS with visual modality alone. This reason is change segmentation does not require anomaly images, as they can be trained using pairs of images containing any changes, which are easily synthesized by available image datasets.

### One-Prompt Meta-Learning for Universal Anomaly Segmentation

Given a change segmentation dataset \(D_{base}=\{X_{i}^{p},X_{i}^{q},Y_{i}\}_{i}\), where \((X_{i}^{p},X_{i}^{q})\) denotes the \(i\)-th image pair and \(Y_{i}\) is its corresponding change mask. MetaUAS trains a meta-model based on the base set \(D_{base}\) and then segments query images \(\{X_{i}^{q}\}_{i=1}^{N}\) with the corresponding normal image prompt \(X_{i}^{p}\) from a novel set \(D_{novel}\), where \(X_{i}^{p}\) and \(X_{i}^{q}\) belongs to the same class. Note that the base and novel set are non-overlapping (i.e., \(D_{base}\cap D_{novel}\)=\(\emptyset\)).

**Overview.** As shown in Fig. 1, our MetaUAS framework is mainly composed of an encoder, a feature alignment module, and a decoder. The encoder extracts hierarchical features using a pre-trained model, while the decoder integrates query and prompt features to predict change heatmap. The feature alignment module is a bridge between the encoder and the decoder. It aligns the query and normal prompt features to address the geometric variation in spatial position. Concretely, for a query image \(X^{q}\) and its corresponding prompt image \(X^{p}\) (\(X^{p}\) and \(X^{q}\in\mathcal{R}^{H\times W\times 3}\)), we parallelly extract

Figure 1: The proposed MetaUAS consists of an encoder, a feature alignment module (FAM), and a decoder. It is trained on a synthesized dataset in a one-prompt meta-learning manner for change segmentation tasks. Once trained, it can segment any anomalies providing only one normal image prompt.

muti-scale offline features, \(\{F_{l}^{q}\}_{l=1}^{5}\) and \(\{F_{l}^{p}\}_{l=1}^{5}\in\mathcal{R}^{h_{1}\times w_{l}\times c_{l}}\) from the encoder, where \(l\) denote the \(l\)-th stage of the encoder. Then, the feature alignment module independently processes \(F_{l}^{q}\) and \(F_{l}^{p}\) for aligning query and prompt in feature space at each scale. Next, these aligned features are contacted and fed into the decoder for predicting change heatmaps. The MetaUAS is trained in a meta-learning manner. At inference, given a query image and its normal image prompt, the anomaly mask can be directly predicted. Next, we elaborately introduce them in this section.

**Encoder.** MetaUAS is compatible with any hierarchical architecture. Considering efficiency, we use the standard convolution-based EfficientNet-b4 [57] as our encoder following UniAD [70]. Given a query image \(X^{q}\) and its prompt \(X^{p}\), we extract multi-scale features \(F_{l}^{q}\) and \(F_{l}^{p}\) from the \(l\)-\(th\) (\(l=1,2,\cdots,5\)) stage of a pre-trained encoder \(\mathcal{F}(\cdot;\theta_{e})\), that is

\[F_{l}^{q}=\mathcal{F}(X^{q};\theta_{e}),F_{l}^{p}=\mathcal{F}(X^{p};\theta_{e }),\] (1)

where \(\theta_{e}\) is frozen to sufficiently utilize its generalization because it is pre-trained on a large-scale ImageNet.

**Feature Alignment Module.** We expect to learn a comparison between query and prompt features (\(F_{l}^{q}\) and \(F_{l}^{p}\)) for improving change segmentation. A simple and naive manner is to directly contact them along the channel dimension, that is

\[F_{l}=\mathrm{Concat}(F_{l}^{q},F_{l}^{p}).\] (2)

Then, the fused features \(F_{l}\) are fed into a decoder to perform change segmentation. This simple fusion manner may only work when the query and its prompt are aligned in pixel space. However, this usually does not hold in practical applications because it is hard to refrain from geometric variations between query and prompt. Therefore, we have to align query and prompt features for better change segmentation. We propose two alignment strategies, hard and soft alignment, for enabling the interaction between query and prompt features.

**Hard Alignment** aims to search the most similar prompt feature at a spatial dimension for each query feature. Here, we take cosine similarity \(\langle\cdot\rangle\) as a distance measure. Formally, for any query feature \(F_{l}^{p}(i,j)\in\mathbb{R}^{c_{l}}\), the most similar prompt feature is

\[F_{l}^{p}(i,j)\gets F_{l}^{p}\big{(}\operatorname*{argmin}_{k,l}\left\langle F _{l}^{q}(i,j),F_{l}^{p}(k,l)\right\rangle\big{)},\] (3)

where \((i,j)\) and \((k,l)\) denote spatial locations. Considering computational efficiency, we use a 1\(\times\)1 convolution layer \(\mathrm{Conv}(\cdot;\theta_{a})\) with shared parameters \(\theta_{a}\) to reduce the dimension of the channel before computing the cosine similarity, that is

\[F_{l}^{q^{\prime}}\leftarrow\mathrm{Conv}(F_{l}^{q};\theta_{a}),F_{l}^{p^{ \prime}}\leftarrow\mathrm{Conv}(F_{l}^{p};\theta_{a}).\] (4)

**Soft Alignment** is different from the hard alignment, which aligns each query feature with a weighted combination on the prompt feature. Similar to the hard alignment, we first apply Eq. 4 to reduce computation. The weighted probability is computed with the softmax function on a cross-similarity between the query and prompt features, that is

\[W_{ijkl}=\mathrm{Softmax}\left(F_{l}^{q}(i,j)(F_{l}^{p}(k,l))^{T}\right),\] (5)

where the \(\mathrm{Softmax}\) operation is applied to the last two dimensions, and thus \(\sum_{k}\sum_{l}W_{ijkl}=1\). Finally, the aligned prompt feature can be obtained by the weight and original prompt feature, that is

\[F_{l}^{p}(i,j)\leftarrow\sum_{k}\sum_{l}W_{ijkl}F_{l}^{p}(k,l).\] (6)

The hard and soft alignment can adaptively align prompt features with query features, and thus handing geometric variation between query and prompt images to some extent. Appling Eqs. 3 or 6, we obtain an aligned prompt feature and then replace the original prompt feature \(F_{l}^{p}\) in Eq. 2 with it.

**Decoder.** Considering efficiency, we apply the feature alignment module to three high-level features of prompt and query, and thus three fusion features \(\{F_{l}\}_{l=3}^{5}\) are derived. Change segmentation needs to predict each pixel to determine whether it is changed. Specifically, we utilize UNet [46]\(\mathcal{G}(\cdot;\theta_{g})\) as our decoder because it is better suited for tasks requiring high precision and the preservation of fine-grained details. The UNet integrates all three fused features and two low-level original features and produces a final feature at the original image resolution. Finally, a segmentation head transforms the final feature to generate pixel-level change prediction \(\hat{Y}\). The segmentation head is implemented by a simple 1\(\times\)1 convolution layer, \(\mathrm{Conv}(\cdot;\theta_{h})\), following a sigmoid activation.

### Synthesizing Change Segmentation Images

Remote sensing [7] and street scenes [1] are two main scenarios in change segmentation. They mainly focus on semantic change, and various noises are included in unchanged background regions. Furthermore, the dataset scale is small and the diversity is insufficient. Therefore, it is not suitable for universal change segmentation. Recent works [64; 63; 22] have exploited generative diffusion models to create synthetic datasets and presented a promising performance on real datasets. However, it is hard to guarantee generative annotations are accurate. In contrast, some works have shown that simple synthesis, such as copy-paste, also brings strong performance for instance segmentation [19; 51] and anomaly segmentation [31; 72]. Similar to these works, we want to leverage a synthetic change segmentation dataset with accurate change masks. As early discussed, there are three main change types, "appearance", "disappearance", and "exchange", where "appearance" and "disappearance" are a pair of opposite concepts, and they can be transformed into each other by swapping paired images. Therefore, we only need to synthesize two change types to simulate all three ones.

**Object-Level Change.** In the famous MSCOCO [35], instances are annotated with polygons, and thus their foreground masks are available. Following CYWS [50], given a random instance and its binary mask from an image, we could make it disappear from the image by inpainting the mask region [56]. In this simple manner, we can simulate the "disappearance" change. Meanwhile, the change mask is freely available. The "appearance" change can be easily obtained by swapping original and unpainted images. It is challenging to synthesize the "exchange" change because two different instances usually mean different masks. But we can randomly paste an or multiple instances to a given image for rough simulation.

**Local-Region Change.** Object-level changes are generated only by inpainting mask regions or randomly pasting objects as shown in Fig 2. However, anomaly changes are usually diverse, and they may be whole objects or local regions. The local "disappearance" and "appearance" may be failed by inpainting because local regions are easily restored by context. Following DRAEM [72], we first generate a binary change mask with Perlin noise and then synthesize a new image by filling the mask region with another image pixels. This synthesis can simulate local changes since the binary mask is generated randomly. Given a changed image pair, we apply various data augmentations, such as scale transformations, translation, rotation, and color jittering to enhance the diversity of changes during training phase.

### Training and Inference

**Traning.** We train MetaUAS in a meta-learning manner, and each meta-task \(\{X_{i}^{p},X_{i}^{q},Y_{i}\}\) is one prompt-query pair. The binary cross-entropy loss is adopted to optimize the learnable parameters (\(\theta_{a}\), \(\theta_{g}\), and \(\theta_{h}\)) of MetaUAS, that is

\[\mathcal{L}=-\sum_{i}\left(Y_{i}\cdot\log(\hat{Y}_{i})+(1-Y_{i})\cdot\log(1- \hat{Y}_{i})\right).\] (7)

**Inference.** For a class-specific query image \(X^{q}\), we first randomly select a normal image prompt \(X^{p}\) from the corresponding normal training set and then process prompt-query pairs online to perform anomaly segmentation. For a class-agnostic query image, we need to first construct a class-aware prompt pool \(\{P_{i}\}_{i=1}^{C}\) via extracting offline features of all normal prompts \(\{X_{i}^{p}\}_{i=1}^{C}\) in a total of \(C\) classes, and then derive the best matching prompt by computing the cosine similarity between the query feature \(F\) and the prompt pool. Here, the query and prompt features are obtained by using a global average pooling on the last stage feature from the encoder.

Figure 2: Selected synthesizing image pairs and their change masks. (a) and (b) simulate “appearance” and “disappearance” synthesizing with mask inpainting [56], and [50], (c) simulate “exchange” synthesizing with random pasting, and (d) simulate local region changes synthesizing with DRAEM [72].

Given a query image \(X^{q}\) and the corresponding prompt \(X^{p}\), we successively feed them into the encoder, the feature alignment module, the decoder, and the segmentation head, and finally obtain a predicted anomaly map \(\hat{Y}\). Following previous works, we take the maximum of \(\hat{Y}\) as the image-level anomaly score, without additional post-processing.

## 4 Experiment

### Experimental Setup

Following previous works, we comprehensively evaluate MetaUAS on three industrial anomaly segmentation benchmarks, MVTec [4], VisA [78] and Goods [74]. We train a universal change segmentation model on a synthetic dataset. To demonstrate cross-domain generalization ability, we directly test it on three industry anomaly detection benchmarks without fine-tuning

**Competing Methods.** We compare **MetaUAS** and its two variants (**MetaUAS\(\star\)** and **MetaUAS\(\star\)+**) with diverse state-of-the-art anomaly segmentation methods including zero-shot CLIP [42], WinCLIP [26], AnomalyCLIP [76], and one-shot PatchCore [47], WinCLIP+ [26], and full-shot UniAD [70]. **MetaUAS** segments any anomalies with only one normal image prompt. Here, the one normal prompt is randomly sampled from normal training images for each class. The results are mean and standard deviation based on 5 independent repeated tests with different random seeds. **MetaUAS\(\star\)** takes the best-matched normal image from the normal training set as the prompt of query image. Here, the matching degree is computed using the cosine similarity between the query image and all normal training images in feature space. **MetaUAS\(\star\)+** builds on MetaUAS\(\star\). Following WinCLIP+ [26], we also add the visual prior knowledge from the image encoder of CLIP model to our MetaUAS\(\star\) for a fair comparison. The visual prior knowledge used in MetaUAS\(\star\)+ is kept exactly the same as WinCLIP+.

**Evaluation Metrics.** Following previous works, we use ROC, PR, and F1max metrics for image-level anomaly classification. Similar to the anomaly classification evaluation, we use the same metrics and additionally report Per-Region Overlap (PRO) for pixel-level anomaly segmentation. We argue that the PR and F1max metrics are better for anomaly segmentation, where the imbalance issue is very extreme between normal and anomaly pixels [11; 78].

### Comparison with Previous Works

Tables 1 and 2 present the comparison results of MetaUAS with the above-mentioned competing methods in generalization and efficiency, respectively.

Figure 3: Qualitative comparisons with state-of-the-art methods on MVTec, VisA and Goods. In both two sub-figures (left and right), (b) and (g) represent query images and their anomaly masks, while (a) represent the corresponding normal image prompts. The predicted anomaly maps are shown using different methods, including (c) WinCLIP+ [26], (d) AnomalyCLIP [76], (e) UniAD [70] and (f) our MetaUAS. Best viewed in color and zoom-in.

**Generalization.** First, MetaUAS with one normal image prompt achieves competitive performance among all zero-, few- and full-shot methods both on MVTec and VisA. This suggests that it is possible to boost anomaly classification and segmentation performance only with visual information alone. But on Goods dataset, MetaUAS seems to perform similarly to other methods. Different from MVTec and VisA, Goods consists of six groups and each group contains dozens or even hundreds of subcategories (484 in total). In fact, it is challenging to address multi-classes with a single model, and the state-of-the-art UniAD is not good even using all normal training images. In contrast, MetaUAS only uses one normal image prompt for each group, which means that the prompt image does not match most query images from multiple subcategories. Furthermore, MetaUAS\(\star\) significantly outperforms almost all competing models when the best-matched normal image is used to take as the normal prompt of each query image. In addition, WinCLIP+ boosts few-shot AS with dense similarity between few-shot prompts and query images. For a fair comparison, we also add the visual prior knowledge from the image encoder of CLIP model to our MetaUAS\(\star\) (denoting as MetaUAS\(\star\)). We can see that the performance can be further improved when introducing the visual prior of CLIP models to MetaUAS\(\star\).

**Efficiency.** We measure complexity and efficiency with the number of parameters and forward inference times. The evaluation is performed on one V100 GPU with batch size 32. The number of parameters of WinCIIP+ and AnomalyCLIP is 10\(\times\) and 20\(\times\) of MetaUAS due to the large vision-language backbone. Compared to state-of-the-art, our MetaUAS\(\star\)+ achieves the best performance using the single model with half of the parameters and faster inference time. What is more, our MetaUAS\(\star\) has 10\(\times\) fewer parameters and 100\(\times\) speed improvement compared to WinCLIP+, which still performs better.

**Qualitative Comparisons.** Figs. A1 and 4 show some selected visualizations from MVTec, VisA, and Goods testing images using the state-of-arts and our MetaUAS. Generally, MetaUAS segments anomalies more accurately and produces fewer false positives. MetaUAS is robust to different image prompts from the same category, especially for those categories with large geometric variations, such as screw. We believe that better performance can be derived if the objects or textures of the prompt image and query images can be roughly aligned.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Datasets Methods} & \multirow{2}{*}{Venue} & \multirow{2}{*}{Shot} & \multirow{2}{*}{Auxiliary} & \multicolumn{3}{c}{Anomaly Classification} & \multicolumn{3}{c}{Anomaly Segmentation} \\ \cline{5-10}  & & & & & I-ROC & I-PR & \(\text{I-F1}_{\text{max}}\) & P-ROC & P-PR & \(\text{P-F1}_{\text{max}}\) & P-PRO \\ \hline \multirow{5}{*}{**Deep**} & CLIP [42] & ICML 21 & 0 & ✗ & 74.4 & 89.3 & 88.7 & 62.0 & 6.5 & 11.2 & 21.4 \\  & PatchCore [47] & CVPR 22 & 1 & ✗ & 79.0\(\pm\)0.8 & 89.6\(\pm\)1.1 & 88.9\(\pm\)0.3 & 93.1\(\pm\)0.2 & 37.1\(\pm\)0.9 & 42.2\(\pm\)0.8 & 82.7\(\pm\)0.5 \\  & WinCLIP [26] & CVPR 23 & 0 & ✗ & 90.4 & 95.6 & 92.7 & 82.3 & 18.2 & 24.8 & 61.9 \\  & WinCLIP+ [26] & CVPR 23 & 1 & ✗ & 92.8\(\pm\)1.2 & 96.4\(\pm\)0.7 & 93.8\(\pm\)0.5 & 93.5\(\pm\)0.2 & 38.4\(\pm\)1.2 & 42.5\(\pm\)1.0 & 83.9\(\pm\)0.4 \\  & AnomalyCLIP [76] & ICLR 24 & 0 & ✓ & 91.5 & 96.3 & 92.7 & 91.1 & 34.5 & 39.1 & 81.4 \\  & UniAD [70] & NeurIPS 22 & full & ✗ & 96.7 & 98.9 & 96.7 & 96.8 & 44.7 & 50.4 & 90.0 \\ \cline{2-10}  & \multirow{2}{*}{**MetaUAS**} & **MetaUAS** & 1 & ✗ & 90.7\(\pm\)0.7 & 95.7\(\pm\)0.6 & 92.5\(\pm\)0.3 & 94.6\(\pm\)0.2 & 59.3\(\pm\)1.4 & 57.5\(\pm\)1.1 & 82.6\(\pm\)0.6 \\  & & **MetaUAS\(\star\)** & 1 & ✗ & 94.2 & 97.6 & 93.9 & 95.3 & 63.7 & 61.6 & 83.1 \\  & & **MetaUAS\(\star\)+** & 1 & ✗ & 95.3 & 97.9 & 94.6 & 97.6 & 67.0 & 62.9 & 92.5 \\ \hline \hline \multirow{5}{*}{**Deep**} & CLIP [42] & ICML 21 & 0 & ✗ & 59.1 & 67.4 & 74.5 & 56.5 & 1.8 & 3.6 & 22.4 \\  & PatchCore [47] & CVPR 22 & 1 & ✗ & 64.2\(\pm\)1.0 & 66.0\(\pm\)0.7 & 75.5\(\pm\)0.5 & 95.3\(\pm\)0.3 & 16.5\(\pm\)1.7 & 26.0\(\pm\)1.5 & 84.6\(\pm\)0.5 \\  & WinCLIP [26] & CVPR 23 & 0 & ✗ & 75.5 & 78.7 & 78.2 & 73.2 & 5.4 & 9.0 & 51.0 \\  & WinCLIP+ [26] & CVPR 23 & 1 & ✗ & 80.5\(\pm\)2.6 & 82.1\(\pm\)2.7 & 81.3\(\pm\)1.0 & 94.4\(\pm\)1.5 & 19.0\(\pm\)2.2 & 23.2\(\pm\)0.4 & 79.3\(\pm\)0.3 \\  & AnomalyCLIP [76] & ICLR 24 & 0 & ✓ & 81.9 & 85.4 & 80.7 & 95.5 & 21.3 & 28.3 & 86.8 \\  & UniAD [70] & NeurIPS 22 & full & ✗ & 90.8 & 93.2 & 87.8 & 98.5 & 34.3 & 39.1 & 84.8 \\ \cline{2-10}  & \multirow{2}{*}{**MetaUAS**} & **MetaUAS** & 1 & ✗ & 81.2\(\pm\)1.7 & 84.5\(\pm\)1.4 & 80.2\(\pm\)0.7 & 92.2\(\pm\)0.7 & 42.7\(\pm\)0.8 & 44.7\(\pm\)0.6 & 60.4\(\pm\)1.5 \\  & & **MetaUAS\(\star\)** & 1 & ✗ & 83.4 & 85.7 & 81.3 & 92.0 & 43.9 & 45.6 & 57.3 \\  & & **MetaUAS\(\star\)+** & 1 & ✗ & 85.1 & 87.2 & 82.3 & 98.0 & 48.1 & 48.6 & 85.5 \\ \hline \hline \multirow{5}{*}{**Deep**} & CLIP [42] & ICML 21 & 0 & ✗ & 51.8 & 57.3 & 71.3 & 55.3 & 4.3 & 2.0 & 16.4 \\  & PatchCore [47] & CVPR 22 & 1 & ✗ & 48.3\(\pm\)1.0 & 54.2\(\pm\)0.5 & 71.3\(\pm\)0.1 & 84.3\(\pm\)0.5 & 4.5\(\pm\)0.2 & 9.3\(\pm\)0.3 & 55.6\(\pm\)1.0 \\  & WinCLIP [26] & CVPR 23 & 0 & ✗ & 52.2 & 58.2 & 71.4 & 73.0 & 5.0 & 10.2 & 44.5 \\  & WinCLIP+ [26] & CVPR 23 & 1 & ✗ & 53.5\(\pm\)0.2 & 58.6\(\pm\)0.2 & 71.5\(\pm\)0.1 & 85.5\(\pm\)0.6 & 5.7\(\pm\)0.4 & 11.3\(\pm\)0.5 & 56.6\(\pm\)1.2 \\  & AnomalyCLIP [76] & ICLR 24 & 0 & ✓ & 57.2 & 63.3 & 71.4 & 83.5 & 16.9 & 24.0 & 63.3 \\  & UniAD [70] & NeurIPS 22 & full & ✗ & 67.5 & 72.1 & 74.6 & 90.4 & 15.0 & 20.6 & 66.1 \\ \cline{2-10}  & \multirow{2}{*}{**MetaUAS**} & **MetaUAS** & 1 & ✗ & 54.5\(\pm\)1.0 & 58.5\(\pm\)0.4 & 71.5\(\pm\)0.1 & 88.5\(\pm\)0.6 & 8.6\(\pm\)0.7 & 14.0\(\pm\)0.7 & 59.0\(\pm\)1.3 \\ \cline{2-10}  & \multirow{2}{*}{**MetaUAS**} & **MetaUAS** & 1 & ✗ & 90.1 & 91.7 & 85.7 & 97.4 & 53.7 & 55.5 & 70.8 \\ \cline{2-10}  & \multirow{2}{*}{**MetaUAS++**} & **MetaUAS** & 1 & ✗ & 89.9 & 89.9 & 86.2 & 97.9 & 49.0 & 55.8

### Ablation Study

We perform component-wise analysis on MVTec with 256\(\times\)256 inputs.

**The influence of feature alignment module.** As reported in Tab. 3a, we conduct experiments with combinations of different align strategies and feature fusions. The experimental results demonstrate that using soft alignment is better than the other two ones (no alignment and hard alignment) in the comprehensive results of AC and AS. Moreover, we compare the feature concatenation (Concat) with element-wised addition (Add) and absolute difference (AbsDiff) for aggregating prompt and query features. The Concat operation is the best one. The Add is not suitable to fuse two types of features as it fails to depict the image changes, and it may lead to confusion between these two features, as its results are the worst. The AbsDiff is widely used in the change segmentation field. However, this method may result in the loss of contextual information. In contrast, direct concatenation preserves all information and allows the network to adaptively learn the fusion, yielding the best results.

**The effects of change types.** The diversity of synthetic data is critical for good generalization. We use object-level changes and local region changes to ensure this diversity. Experiments show that object-level changes contribute more performance than local changes in Tab. 3c. This is natural because the object-level change is implemented by inpainting object regions and randomly pasting

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Methods & Backbone & \#All Params(\#Leanable) & Input Size & Times (ms) & I-ROC & P-ROC & P-PR \\ \hline CLIP [42] & ViT-B-16+240 & 208.4 (0.0) & 240\(\times\)240 & 13.7 & 74.4 & 62.0 & 6.5 \\ PatchCore [47] & E-b4 & 17.5 (0.0) & 256\(\times\)256 & 36.4 & 79.0\(\pm\)0.8 & 93.1\(\pm\)0.2 & 37.1\(\pm\)0.9 \\  & & & 512\(\times\)512 & 145.1 & 79.1\(\pm\)0.7 & 93.1\(\pm\)0.2 & 37.5\(\pm\)1.2 \\ WinCLIP [26] & ViT-B-16+240 & 208.4 (0.0) & 240\(\times\)240 & 201.3 & 90.4 & 82.3 & 18.2 \\ WinCLIP [26] & ViT-L/14@336px & 433.5 (5.6) & 518\(\times\)518 & 154.9 & 91.5 & 91.1 & 34.5 \\ UniAD [70] & Eb4 & 27.1 (7.7) & 224\(\times\)224 & 5.0 & 96.7 & 96.8 & 44.7 \\ \hline
**MetaUAS** & Eb4 & 22.1 (4.6) & & & & 90.7\(\pm\)0.7 & 94.6\(\pm\)0.2 & 59.3\(\pm\)1.4 \\
**MetaUAS**+ & Eb4+ViT-B-16+240 & 139.3 (4.6) & & & & 94.2 & 95.3 & 63.7 \\  & & & & & 204.8 & 95.3 & 97.6 & 67.0 \\ \hline
**MetaUAS** & Eb4 & 22.1 (4.6) & & & & 90.4\(\pm\)1.8 & 92.9\(\pm\)0.4 & 57.2\(\pm\)1.9 \\
**MetaUAS**+ & Eb4+ViT-B-16+240 & 139.3 (4.6) & & & & 93.2 & 93.3 & 59.8 \\  & & & & & 213.0 & 94.8 & 97.1 & 65.8 \\ \hline \hline \end{tabular}
\end{table}
Table 2: The complexity and efficiency comparisons. The performance of anomaly classification and segmentation is reported on **MVTec**.

Figure 4: Anomaly segmentation for query images with different normal image prompts including 5 random prompts and the optimal prompt (denoting as prompt\(\star\)). The anomaly segmentation maps are generated with MetaUAS, MetaUAS\(\star\) and MetaUAS\(\star\)+.

objects, which has a larger space than local region synthesis. Undoubtedly, combining them further enhances the diversity of synthetic changes, thus further improving performance.

**Learn or freeze encoder.** In Tab. 2(b), if we train the encoder like other modules, the performance drops on both AC and AS. We speculate that it makes the network overfit change segmentation dataset, which will degenerate generalization. We also evaluate other backbones, such as EfficientNet-b6 [57], EfficientViT-b3 [6], and MobileNetV2 [53]. The EfficientNet-b4 performs better than others. The reason might be that shallow networks cannot extract discriminative features, while deep networks focus more on semantic features. AS requires more structural and texture features.

**The effects of decoder module.** In the decoder, we compare UNet [46] with FPN [34]. U-Net has been widely used and validated in many segmentation tasks due to its effectiveness and efficiency, while FPN is a type of network designed for object detection that requires recognizing objects at various scales. As reported in Tab. 2(c), both different types of FPN are worse than UNet.

**The effects on training samples scale.** To investigate the influence of training samples scale on model performance, we conduct experiments with different training subsets where each one is generated by randomly sampling the original training set at various rates, such as \(\{10\%,30\%,50\%,70\%,95\%\}\). The performance of each model on the MVTec testing set is reported in Tab. 2(d). It can be seen that MetaUAS still works when the number of training images is small scale (e.g., 50%), and the performance can further improve when increasing the number of training samples.

## 5 Conclusion

This paper is the first study to focus on universal anomaly segmentation using pure visual information, enabling the segmenting of unseen anomalies without any training on target anomaly datasets or reliance on language guidance. First, we rethink anomaly segmentation tasks and find they can be unified into change segmentation. This paradigm shift allows us to break away from the persistent challenge of lacking large-scale anomaly segmentation datasets. Naturally, we are capable of leveraging large-scale synthetic image pairs with object-level and local region changes derived from available image datasets. Second, we propose a simple but effective universal anomaly segmentation framework, i.e., MetaUAS. We train MetaUAS in a one-prompt meta-learning manner on this synthesized dataset. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change segmentation and singe-image semantic segmentation. This makes it possible to use sophisticated semantic segmentation modules for change segmentation. MetaUAS achieves a superior generalization using only one normal image prompt on three industrial datasets. Meanwhile, MetaUAS enjoys faster inference speed and fewer parameters. We believe MetaUAS will serve as an alternative to widely used vision-language models for universal anomaly segmentation. **Limitation.** The performance of MetaUAS can be affected by using inappropriate normal image prompts. In this study, we leverage cosine similarity to identify the most suitable prompts when the category of the query image is unknown. In scenarios involving fine-grained objects, it may be essential to train a classification model to accurately predict the categories of the query images.

\begin{table}

\end{table}
Table 3: **Ablation studies on MVTec. Default settings are marked in blue.**

## References

* [1]P. F. Alcantarilla, S. Stent, G. Ros, R. Arroyo, and R. Gherardi (2018) Street-view change detection with deconvolutional networks. Autonomous Robots42. Cited by: SS1.
* [2]Y. Bengio, L. Yao, G. Alain, and P. Vincent (2013) Generalized denoising auto-encoders as generative models. In NeurIPS, Cited by: SS1.
* [3]P. Bergmann, K. Batzner, M. Fauser, D. Sattlegger, and C. Steger (2022) Beyond dents and scratches: logical constraints in unsupervised anomaly detection and localization. IJCV130 (4). Cited by: SS1.
* [4]P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger (2019) MWTec-AD: a comprehensive real-world dataset for unsupervised anomaly detection. In CVPR, Cited by: SS1.
* [5]P. Bergmann, K. Batzner, M. Fauser, D. Sattlegger, and C. Steger (2022) Beyond dents and scratches: logical constraints in unsupervised anomaly detection and localization. IJCV130 (4). Cited by: SS1.
* [6]H. Cai, J. Li, M. Hu, C. Gan, and S. Han (2023) EfficientVit: lightweight multi-scale attention for high-resolution dense prediction. In ICCV, Cited by: SS1.
* [7]H. Chen and Z. Shi (2020) A spatial-temporal attention-based method and a new dataset for remote sensing image change detection. Remote Sensing12 (10). Cited by: SS1.
* [8]J. Chen, X. Wu, Y. Li, Q. Li, L. Zhan, and F. Chung (2020) A closer look at the training strategy for modern meta-learning. In NeurIPS, Cited by: SS1.
* [9]Y. Chen, Y. Tian, G. Pang, and G. Carneiro (2022) Deep one-class classification via interpolated gaussian descriptor. In AAAI, Cited by: SS1.
* [10]L. Chiu and S. Lai (2023) Self-supervised normalizing flows for image anomaly detection and localization. In ICCV, Cited by: SS1.
* [11]J. Davis and M. Goadrich (2006) The relationship between precision-recall and roc curves. In ICML, Cited by: SS1.
* [12]T. Defard, A. Setkov, A. Loesch, and R. Audigier (2021) PaDiM: a patch distribution modeling framework for anomaly detection and localization. In ICPR, Cited by: SS1.
* [13]H. Deng and X. Li (2022) Anomaly detection via reverse distillation from one-class embedding. In CVPR, Cited by: SS1.
* [14]C. Ding, G. Pang, and C. Shen (2022) Catching both gray and black swans: open-set supervised anomaly detection. In CVPR, Cited by: SS1.
* [15]Z. Fang, X. Wang, H. Li, J. Liu, Q. Hu, and J. Xiao (2023) FastRecon: few-shot industrial anomaly detection via fast feature reconstruction. In ICCV, Cited by: SS1.
* [16]D. J. Felleman and D. C. Van Essen (1991) Distributed hierarchical processing in the primate cerebral cortex. Cerebral cortex1 (1). Cited by: SS1.
* [17]B. Gao (2024) Learning to detect multi-class anomalies with just one normal image prompt. In ECCV, Cited by: SS1.
* [18]M. Georgescu, A. Barbalau, R. T. Ionescu, F. Shahbaz Khan, M. Popescu, and M. Shah (2021) Anomaly detection in video via self-supervised and multi-task learning. In CVPR, Cited by: SS1.
* [19]G. Ghiasi, Y. Cui, A. Srinivas, R. Qian, T. Lin, E. D. Cubuk, Q. V. Le, and B. Zoph (2021) Simple copy-paste is a strong data augmentation method for instance segmentation. In CVPR, Cited by: SS1.
* [20]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [21]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [22]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [23]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [24]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [25]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [26]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [27]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [28]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [29]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [30]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [31]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [32]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [33]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [34]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [35]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [36]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [37]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [38]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [39]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [40]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [41]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [42]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [43]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [44]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [45]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [46]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [47]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [48]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [49]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel (2019) Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, Cited by: SS1.
* [50]D. Gong, L. Liu, V. Le, B. Saha, M. Reda Mansour, S. Venkatesh, and A. van den Hengel* [21]Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, and Jinqiao Wang. AnomalyGPT: Detecting industrial anomalies using large vision-language models. In _AAAI_, 2024.
* [22] Guan Gui, Bin-Bin Gao, Jun Liu, Chengjie Wang, and Yunsheng Wu. Few-shot anomaly-driven generation for anomaly classification and segmentation. In _ECCV_, 2024.
* [23] Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, and Hong Zhou. Divide-and-Assemble: Learning block-wise memory for unsupervised anomaly detection. In _ICCV_, 2021.
* [24] Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, and Yan-Feng Wang. Registration based few-shot anomaly detection. In _ECCV_, 2022.
* [25] Loshchilov Ilya and Hutter Frank. Decoupled weight decay regularization. In _ICLR_, 2019.
* [26] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer. WinCLIP: Zero-/few-shot anomaly classification and segmentation. In _CVPR_, 2023.
* [27] Xi Jiang, Jianlin Liu, Jinbao Wang, Qiang Nie, Kai WU, Yong Liu, Chengjie Wang, and Feng Zheng. SoftPatch: Unsupervised anomaly detection with noisy data. In _NeurIPS_, 2022.
* [28] Dong-Yun Kim, Soo Jin Lee, Eun-Kyu Kim, Eunyoung Kang, Chan Yeong Heo, Jae Hoon Jeong, Yujin Myung, In Ah Kim, and Bum-Sup Jang. Feasibility of anomaly score detected with deep learning in irradiated breast cancer patients with reconstruction. _npj Digit. Med._, 5(1), 2022.
* [29] Jiarui Lei, Xiaobo Hu, Yue Wang, and Dong Liu. PyramidFlow: High-resolution defect contrastive localization using pyramid normalizing flow. In _CVPR_, 2023.
* [30] Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, and Stephan Mandt. Zero-shot anomaly detection via batch normalization. In _NeurIPS_, 2023.
* [31] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. CutPaste: Self-supervised learning for anomaly detection and localization. In _CVPR_, 2021.
* [32] Xiaofan Li, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yuan Xie, and Lizhuang Ma. PromptAD: Learning prompts with only normal samples for few-shot anomaly detection. In _CVPR_, 2024.
* [33] Xurui Li, Ziming Huang, Feng Xue, and Yu Zhou. MuSc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images. In _ICLR_, 2024.
* [34] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In _CVPR_, 2017.
* [35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In _ECCV_, 2014.
* [36] Wenrui Liu, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Diversity-measurable anomaly detection. In _CVPR_, 2023.
* [37] Zhikang Liu, Yiming Zhou, Yuansheng Xu, and Zilei Wang. SimpleNet: A simple network for image anomaly detection and localization. In _CVPR_, 2023.
* [38] Declan McIntosh and Alexandra Branzan Albu. Inter-realization channels: Unsupervised anomaly detection beyond one-class classification. In _ICCV_, 2023.
* [39] Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti. VT-ADL: A vision transformer network for image anomaly detection and localization. In _ISIE_, 2021.
* [40] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning memory-guided normality for anomaly detection. In _CVPR_, 2020.

* [41] Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. OCGAN: One-class novelty detection using GANs with constrained latent representations. In _CVPR_, 2019.
* [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.
* [43] Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. _Nature Neuroscience_, 2(1), 1999.
* [44] Oliver Rippel, Patrick Mertens, and Dorit Merhof. Modeling the distribution of normal data in pretrained deep features for anomaly detection. In _ICPR_, 2021.
* [45] Nicolae-Catalin Ristea, Neelu Madan, Radu Tudor Ionescu, Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B Moeslund, and Mubarak Shah. Self-supervised predictive convolutional attentive block for anomaly detection. In _CVPR_, 2022.
* [46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In _MICCAI_, 2015.
* [47] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Scholkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. In _CVPR_, 2022.
* [48] Marco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same same but differnet: Semi-supervised defect detection with normalizing flows. In _WACV_, 2021.
* [49] Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bastian Wandt. Fully convolutional cross-scale-flows for image-based defect detection. In _WACV_, 2022.
* [50] Ragav Sachdeva and Andrew Zisserman. The change you want to see. In _WACV_, 2023.
* [51] Kuniaki Saito, Ping Hu, Trevor Darrell, and Kate Saenko. Learning to detect everything in an open world. In _ECCV_, 2022.
* [52] Mohammadraza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, and Hamid R Rabiee. Multiresolution knowledge distillation for anomaly detection. In _CVPR_, 2021.
* [53] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted residuals and linear bottlenecks. In _CVPR_, 2018.
* [54] Shelly Sheynin, Sagie Benaim, and Lior Wolf. A hierarchical transformation-discriminating generative model for few shot anomaly detection. In _ICCV_, 2021.
* [55] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In _CVPR_, 2018.
* [56] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In _WACV_, 2022.
* [57] Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural networks. In _ICML_, 2019.
* [58] Keiji Tanaka. Inferotemporal cortex and object vision. _Annual review of neuroscience_, 19(1), 1996.
* [59] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In _ICML_, 2008.
* [60] Guo-Hua Wang, Bin-Bin Gao, and Chengjie Wang. How to reduce change detection to semantic segmentation. _PR_, 2023.
* [61] Guodong Wang, Shumin Han, Errui Ding, and Di Huang. Student-teacher feature pyramid matching for anomaly detection. _BMVC_, 2021.

* [62] Shenzhi Wang, Liwei Wu, Lei Cui, and Yujun Shen. Glancing at the patch: Anomaly localization with global and local feature comparison. In _CVPR_, 2021.
* [63] Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, and Chunhua Shen. DatasetDM: Synthesizing data with perception annotations using diffusion models. In _NeurIPS_, 2023.
* [64] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. DiffuMask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models. In _ICCV_, 2023.
* [65] Tiange Xiang, Yongyi Lu, Alan L Yuille, Chaoyi Zhang, Weidong Cai, and Zongwei Zhou. In-painting radiography images for unsupervised anomaly detection. In _CVPR_, 2023.
* [66] Guoyang Xie, Jingbao Wang, Jiaqi Liu, Feng Zheng, and Yaochu Jin. Pushing the limits of fewshot anomaly detection in industry vision: Graphcore. In _ICLR_, 2023.
* [67] Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, and Pheng-Ann Heng. Learning semantic context from normal samples for unsupervised anomaly detection. In _AAAI_, 2021.
* [68] Xincheng Yao, Ruoqi Li, Zefeng Qian, Yan Luo, and Chongyang Zhang. Focus the Discrepancy: Intra-and inter-correlation learning for image anomaly detection. In _ICCV_, 2023.
* [69] Xincheng Yao, Ruoqi Li, Jing Zhang, Jun Sun, and Chongyang Zhang. Explicit boundary guided semi-push-pull contrastive learning for supervised anomaly detection. In _CVPR_, 2023.
* [70] Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, and Xinyi Le. A unified model for multi-class anomaly detection. In _NeurIPS_, 2022.
* [71] Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, and Seung-Ik Lee. Old is Gold: Redefining the adversarially learned one-class classifier training paradigm. In _CVPR_, 2020.
* [72] Vitjan Zavrtanik, Matej Kristan, and Danijel Skocaj. DRAEM: A discriminatively trained reconstruction embedding for surface anomaly detection. In _ICCV_, 2021.
* [73] Vitjan Zavrtanik, Matej Kristan, and Danijel Skocaj. Reconstruction by inpainting for visual anomaly detection. _PR_, 112, 2021.
* [74] Jian Zhang, Runwei Ding, Miaoju Ban, and Linhui Dai. PKU-GoodsAD: A supermarket goods dataset for unsupervised anomaly detection and segmentation. _RA-L_, 9(3), 2024.
* [75] Ying Zhao. OmniAL: A unified cnn framework for unsupervised anomaly localization. In _CVPR_, 2023.
* [76] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and Jiming Chen. AnomalyCLIP: Object-agnostic prompt learning for zero-shot anomaly detection. In _ICLR_, 2024.
* [77] Jiawen Zhu and Guansong Pang. Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts. In _CVPR_, 2024.
* [78] Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. In _ECCV_, 2022.

Figure A1: Qualitative comparisons with state-of-the-art methods on MVTec, VisA and Goods. In both two sub-figures (left and right), (b) and (g) represent query images and their anomaly masks, while (a) represent the corresponding normal image prompts. The predicted anomaly maps are shown using different methods, including (c) WinCLIP+ [26], (d) AnomalyCLIP [76], (e) UniAD [70] and (f) our MetaUAS. Best viewed in color and zoom-in.

## Appendix A Implementation Details.

Following UniAD [70], we extract multi-scale features from all 5 stages of EfficientNet-b4 [57] encoder. In the feature alignment module, the three highest-level features are used to perform query-prompt alignment, and the channel number is reduced to half of one of the original channels before calculating the similarity between query and prompt. Therefore, we derive three aligned features of query and prompt using the feature alignment module. Finally, these three aligned features and two original low-level query features from the first and second stages are fed into the decoder and segmentation head for change segmentation. The model is trained with 30 epochs on 8 Tesla V100 GPUs with batch size 128. We freeze the encoder and optimize the feature alignment module, thedecoder, and the segmentation head with AdamW [25] using weight decay 0.0005 and learning rate 0.0001. We conduct experiments based on the open-source framework PyTorch.

We follow CYWS [50] and use the same procedure for synthesizing the change segmentation dataset. Specifically, given a labeled image from an existing instance segmentation dataset, i.e., MS-COCO, we randomly selected one or several instances and then could make it disappear from the image by inpainting the mask region [56]. It is worth noting that the binary change mask between the inpainted and original images can be freely available because these selected instances have been manually annotated at the pixel level. We keep the dataset setup as similar to CYWS [50] as possible. Specifically, the change segmentation dataset is synthesized using the randomly selected 60,000 images from the MS-COCO training set. For each image, a synthesized image is generated by inpainting a union mask of a random set of labeled instances. Then, all these 60,000 samples are divided into training and validation sets with a ratio of 0.95:0.05. During training, we randomly employ object-level change and local-region change with a probability of 0.5.

## Appendix B Competing Methods.

To demonstrate the superiority of MetaUAS, we compare MetaUAS and its variants (MetaUAS+ and MetaUAS++) with diverse state-of-the-art methods. Implementation and reproduction details are summarized as follows:

**CLIP**[42] is a powerful vision-language model, and it has a strong zero-shot generalization ability. Following previous works, we use two classes of text prompt templates, "A photo of a normal [cls]" and "A photo of an anomalous [cls]", where "cls" denotes the target class name. The anomaly score is computed by cosine similarity between textual features and the class token of a query image. For anomaly segmentation, we extend the above computation from class tokens to local patch tokens.

**WinCLIP**[26] is a zero-shot anomaly segmentation method based on CLIP. A large set of hand-crafted textual prompts is designed for anomaly classification. A window scaling strategy is used to obtain better anomaly segmentation. We keep all parameters the same as in their paper. Note that no official implementation of WinCLIP is available, our results are based on an unofficial implementation 1.

Footnote 1: https://github.com/zqhang/Accurate-WinCLIP-pytorch

**WinCLIP+**[26] combines the complementary prediction from both language-guided and visual-based for better anomaly classification and segmentation. The language-guided prediction is the same as WinCLIP. For visual-based prediction, it first simply stores multi-scale features for given few-shot normal images and retrieves the memory features based on the cosine similarity. The final anomaly score is derived by averaging these two scores.

**AnomalyCLIP**[76] learns object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. But AnomalyCLIP requires fine-tuning on an auxiliary domain dataset including normal and anomaly images. AnomalyCLIP is a zero-shot anomaly classification and segmentation method, and it is capable of recognizing any anomalies. We use the official model to report performance for anomaly classification and segmentation.

**UniAD**[70] is a unified unsupervised anomaly segmentation method for addressing multi-classes anomalies with a single model. Different from most zero-/few-shot anomaly segmentation models, UniAD learns feature reconstruction with a transformer-based encoder-decoder architecture on all normal training images. We use the official code to train the specific model for each dataset.

**PatchCore**[47] is a popular unsupervised anomaly classification method that enjoys training-free. For a fair comparison, we modify the official implementation in two folds. First, we replace the original WideResNet-50 backbone with EfficientNet-b4. Second, the memory-bank construction is limited to only one normal image for each class.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: This paper is the first study to focus on universal anomaly segmentation with visual modality alone in identifying open-word anomalies without any training on target or domain datasets or any guidance from language. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of our work is discussed in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]  Justification: Our work does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [No] Justification: We will release the code and models at https://github.com/gaobb/MetaUAS to ensure strict reproducibility. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We will release the code and models at https://github.com/gaobb/MetaUAS to ensure strict reproducibility. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We give implementation details in Section A and reproduction details for competing methods in Section B in the Supplemental Material. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our method may be affected by different image prompts. In our experiment, we randomly sample one normal image for each class from the normal training set. The final results are mean and standard deviation based on 5 independent repeated tests with different random seeds. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We measure the complexity and efficiency with the number of parameters and forward inference times. The evaluation is performed on one V100 GPU. Please refer to detailed comparisons in Tab. 2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have carefully read the NeurIPS Code of Ethics. Our research conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our work will facilitate many applications, such as industrial defect inspection in manufacturing and medical image diagnosis, which is discussed in Section 1.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: In our paper, the change segmentation dataset is synthesized using the available MS-COCO dataset. The annotations in the MS-COCO dataset are licensed under a Creative Commons Attribution 4.0 License. And we use the open-source PyTorch (BSD-style license) framework for conducting experiments. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our work does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.