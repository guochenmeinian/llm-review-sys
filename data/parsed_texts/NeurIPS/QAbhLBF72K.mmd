# What makes unlearning hard and what to do about it

 Kairan Zhao

University of Warwick

&Meghdad Kurmanji

University of Cambridge

&George-Octavian Barbulescu

University of Warwick

&Eleni Triantafillou

Google DeepMind

&Peter Triantafillou

University of Warwick

Correspondence to Kairan.Zhao@warwick.ac.uk

Equal senior contribution

Code is available at: https://github.com/kairanzhao/RUM

###### Abstract

Machine unlearning is the problem of removing the effect of a subset of training data (the "forget set") from a trained model e.g. to comply with users' requests to delete their data, or remove mislabeled, poisoned or otherwise problematic data. With unlearning research still being at its infancy, many fundamental open questions exist: Are there _interpretable_ characteristics of forget sets that substantially affect the difficulty of the problem? How do these characteristics affect different state-of-the-art algorithms? We present the first investigation into these questions. We identify two key factors affecting unlearning difficulty and the performance of unlearning algorithms. Our evaluation on forget sets that isolate these identified factors reveals previously-unknown behaviours of state-of-the-art algorithms that don't materialize on random forget sets. Based on our insights, we develop a framework coined Refined-Unlearning Meta-algorithm (RUM) that encompasses: (i) _refining_ the forget set into homogenized subsets, according to different characteristics; and (ii) a _meta-algorithm_ that employs existing algorithms to unlearn each subset and finally delivers a model that has unlearned the overall forget set. RUM substantially improves top-performing unlearning algorithms. Overall, we view our work as an important step in deepening our scientific understanding of unlearning and revealing new pathways to improving the state-of-the-art. ++

Footnote â€ : Code is available at: https://github.com/kairanzhao/RUM

## 1 Introduction

Deep learning models have generated impressive success stories recently by leveraging increasingly large and data-hungry neural networks that are also increasingly expensive to train. This trend has led to reusing previously-trained models for a wide range of tasks more than ever before. However, the heavy reliance of deep models on training data, together with the difficulty of removing data from trained models after-the-fact, has exacerbated concerns on perpetuating harmful or outdated information, violating user privacy and other issues. Specifically, deep networks are highly non-convex, making it difficult to trace (and thus attempt to remove) the effect of a given subset of training data on the model weights. We are therefore faced with important technical challenges when it comes to building machine learning pipelines that are performant while efficiently supporting deletion requests. Machine unlearning [29] is a growing field that aims to address this important issue.

While unlearning is receiving increasing attention [34, 33], it is still a young area of research and the factors affecting the success of different approaches remain poorly-understood. Understandingwhat makes an unlearning problem easy or hard is crucial for several reasons. First, knowledge of behaviours of unlearning algorithms on different types of forgetting requests may inform which unlearning method to choose for a given request. In fact, for some requests it may be that all current methods are inadequate, suggesting that one should pay the cost of retraining from scratch rather than opting for "approximate unlearning" that imperfectly removes information after-the-fact. Further, deepening our understanding of unlearning can illuminate pathways for improving both unlearning algorithms as well as evaluation protocols by focusing on relevant factors that affect difficulty.

To this end, we present the first investigation into different factors that characterize the difficulty of an unlearning problem. We find that the unlearning problem becomes harder i) the more entangled the retain and forget sets are and ii) the more memorized the forget set examples are. Our investigation reveals that different unlearning algorithms suffer disproportionately as the difficulty level increases and surfaces previously-unknown behaviours and failure modes of state-of-the-art unlearning algorithms. Inspired by our findings, we propose a Refined-Unlearning Meta-algorithm (RUM) for improving unlearning pipelines. RUM contains two steps: i) a refinement procedure that divides the given forget set into subsets that are homogeneous with respect to relevant factors that influence algorithms' behaviours, and ii) a meta-algorithm that dictates how to unlearn each of those subsets and compose the resulting models to arrive at one that has unlearned the entire forget set. Our thorough investigation shows that RUM boosts unlearning performance of several state-of-the-art algorithms and addresses issues that our investigation of unlearning difficulty has uncovered.

## 2 Preliminaries

### Unlearning problem formulation

Let \(\theta^{o}=\mathcal{A}(\mathcal{D}_{train})\) be the weights obtained by applying a training algorithm \(\mathcal{A}\) on a training dataset \(\mathcal{D}_{train}\). We will refer to \(\theta^{o}\) as the "original model". Further, let \(\mathcal{S}\subseteq\mathcal{D}_{train}\) denote a subset of the training data referred to as the "forget set". For convenience, we will refer to its complement as the "retain set" \(\mathcal{R}=\mathcal{D}_{train}\setminus\mathcal{S}\). Informally, the goal of an unlearning algorithm \(\mathcal{U}\) is to utilize \(\theta^{o}\), \(\mathcal{S}\) and \(\mathcal{R}\) to produce an unlearned \(\theta^{u}=\mathcal{U}(\theta^{o},\mathcal{S},\mathcal{R})\) from which the influence of \(\mathcal{S}\) is removed.

This idea has been formalized by considering the distributional similarity between the model \(\theta^{u}\) produced by \(\mathcal{U}\) and the model \(\theta^{r}\) produced by the optimal unlearning approach: retraining from scratch on an adjusted training dataset that excludes the forget set: \(\theta^{r}=\mathcal{A}(\mathcal{D}_{train}\setminus S)\). Note that we refer to distributions here since rerunning \(\mathcal{U}\) and \(\mathcal{A}\) with different random seeds (that control e.g. the initialization and the order of mini-batches) will lead to slightly different weights. The ideal unlearning algorithm then, according to this viewpoint, is one that yields the same distribution of weights as retraining from scratch. Of course, for an unlearning algorithm to be practical, we would additionally desire it to be significantly more computationally efficient than retraining the model.

The following definition, borrowed from [33] (and similar to [14]) formalizes this idea in a framework inspired by differential privacy [8].

**Definition 2.1**.: **Unlearning.** An unlearning algorithm \(\mathcal{U}\) is an \((\epsilon,\delta)\)_-unlearner_ (for \(\mathcal{A}\), \(\mathcal{D}_{train}\) and \(\mathcal{S}\)) if the distributions of \(\mathcal{A}(\mathcal{D}_{train}\setminus S)\) and \(\mathcal{U}(\theta^{o},\mathcal{S},\mathcal{D}_{train}\setminus S)\) are \((\epsilon,\delta)\)-close.

where we say two distributions \(\mu,\nu\) are \((\epsilon,\delta)\)_-close_ if \(\mu(B)\leq e^{\epsilon}\nu(B)+\delta\) and \(\nu(B)\leq e^{\epsilon}\mu(B)+\delta\) for all measurable events \(B\).

According to the above definition, an unlearning algorithm is said to be _exact unlearning_ if it satisfies the above definition for \(\epsilon=\delta=0\), i.e., it yields a distribution of models identical to that of retraining from scratch. For neural networks, the only known exact solutions involve retraining, either naively, or in the context of mixtures where one can retrain only a subset of models affected by the deletion request [3]. These approaches unfortunately are inefficient; in the worst-case, even clever schemes suffer inefficiency similar to naive retraining, and may also yield poorer performance. To address this, a plethora of _approximate unlearning_ algorithms have been recently proposed, whose \(\epsilon\) and \(\delta\) values aren't known in general, but are substantially more efficient and may have higher utility.

Evaluating approximate unlearning.Since the success of (most) approximate unlearning algorithms cannot be proved within tight \((\epsilon,\delta)\) bounds, the community has considered various empirical measurements of success, guided by three desiderata: i) good forgetting quality, 2) high utility, and 3) efficiency. An unlearning algorithm thus is faced with a complex balancing act, as there are well-known trade-offs both between forgetting quality and utility, as well as forgetting quality and efficiency, and a good unlearning metric should capture these nuances.

Utility and efficiency are straightforward to measure, and, in the context of classifiers, can be represented by the accuracy on the retain and test sets, and time in seconds, respectively. Measuring forgetting quality, on the other hand, is more complex and several proxies have been proposed. The simplest one is to inspect the accuracy on the forget set, with the goal of matching the accuracy on the forget set that would have been obtained by retraining from scratch. Alternatively, inspired from privacy literature [4; 28], _Membership Inference Attacks (MIAs)_ have been adopted by the unlearning community [25; 26; 20] to measure forgetting quality. In essence, an MIA is designed to infer from the model's characteristics (e.g. loss, confidence) whether a data point has been used in training, and then unlearned, versus was never trained on in the first place. Intuitively, the failure of an attacker to tell apart unlearned examples from never-seen examples marks a success for the unlearning algorithm in terms of this metric. We will consider both of these proxies in our experimental investigation.

To holistically evaluate an unlearning algorithm, we desire a single metric that captures both forgetting quality and utility. We will later introduce a "tug-of-war" metric for this purpose, inspired by [33].

### Memorization

Deep neural networks are known to "memorize" (a subset of) their training data, with a recent theory showing that label memorization is in fact necessary for achieving close-to-optimal generalization error in classifiers [11] when the data distribution is long-tailed.

**Definition 2.2**.: **Memorization score [11].** The _memorization score_ for an example \(i\in\mathcal{D}\), with respect to a training dataset \(\mathcal{D}\) and training algorithm \(\mathcal{A}\) is

\[\text{mem}(\mathcal{A},\mathcal{D},i)=\Pr_{f\sim\mathcal{A}(\mathcal{D})}[f( x_{i})=y_{i}]\ -\Pr_{f\sim\mathcal{A}(\mathcal{D}\setminus i)}[f(x_{i})=y_{i}]\] (1)

where \(x_{i}\) and \(y_{i}\) are the feature and label, respectively, of example \(i\).

The first term in the above equation considers models trained on all of \(\mathcal{D}\) whereas the second term considers models trained on \(\mathcal{D}\) excluding example \(i\). Intuitively, the memorization score for an example \(i\) is high if including it in training yields a different distribution of predictions on that example than excluding it from training would have. Recent works [11; 12; 23] identify atypical examples or outliers of the data distribution as examples that are more highly memorized: if an example has a noisy or incorrect label, the model is required to memorize it in order to predict it correctly.

## 3 Related Work

Approximate unlearning algorithms.A plethora of algorithms have been proposed that aim to identify effective data scrubbing procedures post-training. We now describe representative methods.

**Fine-tune**[36; 15] relies on catastrophic forgetting to diminish the confidence of the original model \(\theta^{o}\) on \(\mathcal{S}\). Catastrophic forgetting is induced by simply fine-tuning on the retain set \(\mathcal{D}_{train}\setminus\mathcal{S}\). On the other hand, **NegGrad**[15; 17; 31] instead directly maximizes the loss on \(\mathcal{S}\). This approach has been found empirically to cause a large drop in the utility of the model. To address this, **NegGrad+**[25] combines fine-tuning and gradient ascent, by jointly minimizing the loss function on the retain set, and maximizing the loss function with respect to the forget set. **SCRUB**, proposed by the same authors as NegGrad+, extends the contrastive learning behind NegGrad+ by framing it as a student-teacher problem. Concretely, SCRUB is a bi-optimization algorithm, where the student aims to mimic the teacher's behaviour on \(\mathcal{R}\) and to disobey the teacher's output with respect to \(\mathcal{S}\). **L1-sparse**[26] infuses weight "sparsity" into the unlearning algorithm by fine-tuning on the retain set with an L1-penalty, drawing inspiration from the model pruning literature [13; 27]. **Influence Unlearning**[21; 24] arrives at the important model's weights by estimating how removing a data point affects \(\theta^{o}\) via influence functions [7], and draws connections to \((\epsilon,\delta)\)_-forgetting_[35; 19].

A different line of work is "relabelling-based" methods that trick the model to learn new labels for \(S\). This can be achieved by finetuning the model with respect to a dataset \(\mathcal{D}_{relabel}=(X_{\mathcal{S}},Y)\), where \(X_{\mathcal{S}}\) are the features and labels \(Y\) are sampled from a prior distribution of the label space. **Saliency Unlearning (SalUn)**[10] learns \(\mathcal{D}_{relabel}\) by optimising only the _salient_ parameters of the model.

Concretely, the authors argue that the model's weights \(\theta^{o}\) can be decomposed into salient weights and "intact" model weights, by investigating the weight space with respect to the forget set \(S\) ala [30; 1].

Difficulty of Unlearning.The closest research to ours is the contemporaneous work of [9], where the authors study _adversarial unlearning_ cases, i.e. "worst-case" forget sets. [9] arrives at difficult forget sets by solving a bi-level optimization based on fine-tuning (i.e. catastrophic forgetting). Instead, we arrive at difficult partitions through the lens of _interpretable_ factors: the degree of entanglement between the retain and forget set and memorization. While the primary aim of [9] is to construct more pessimistic evaluation benchmarks, our primary aim is to deepen our understanding of unlearning problems and of the behaviour of state-of-the-art algorithms when operating on forget set of different identified characteristics, ultimately improving unlearning pipelines.

Catastrophic forgetting, atypical examples and privacy.[22] and [32] study catastrophic forgetting during training. [22] finds that, when training on large datasets, examples that were only seen early in training may enjoy better privacy, in terms of MIAs and extraction attacks, compared to examples seen recently. [32] investigate "forgetting events", where an example that was previously correctly predicted becomes incorrectly predicted later in training. They find that examples with noisy labels witness a larger number of these forgetting events. Further, [5] find that models trained with Differential Privacy (DP) find it primarily hard to correctly predict atypical examples. We build on this literature by studying the difficulty of unlearning after-the-fact, rather than (passive) forgetting during training and draw connections to memorization, a notion closely related to atypicality in the data distribution.

## 4 What Makes Unlearning Hard?

In this section, we identify and empirically examine two factors that affect the difficulty of unlearning. Before diving in, we first define a simple proxy for unlearning difficulty that we will use in this section. Our goal is to capture the difficulty of performing the "balancing act" of forgetting \(\mathcal{S}\) while retaining the ability to perform well on \(\mathcal{R}\) and generalize well to the test set. We propose a metric to capture this "tug-of-war" (ToW) using the relative difference between the accuracies of the unlearned and the retrained model on the forget, retain and test sets, in a manner inspired by [33].

\[\text{ToW}(\theta^{u},\theta^{r},\mathcal{S},\mathcal{R},\mathcal{D}_{test}) =(1-\text{da}(\theta^{u},\theta^{r},\mathcal{S}))\cdot(1-\text{da}( \theta^{u},\theta^{r},\mathcal{R}))\cdot(1-\text{da}(\theta^{u},\theta^{r}, \mathcal{D}_{test}))\]

where \(\text{a}(\theta,\mathcal{D})=\frac{1}{|\mathcal{D}|}\sum_{(x,y)\in\mathcal{D} }[f(x;\theta)=y]\) is the accuracy on \(\mathcal{D}\) of a model \(f\) parameterized by \(\theta\) and \(\text{da}(\theta^{u},\theta^{r},\mathcal{D})=|\text{a}(\theta^{u},\mathcal{D })-\text{a}(\theta^{r},\mathcal{D})|\) is the absolute difference between the accuracy of models \(\theta^{u}\) and \(\theta^{r}\) on \(\mathcal{D}\). Therefore, ToW rewards unlearned models that match the accuracy of the retrained-from-scratch model, on each of the forget, retain, and test sets. ToW ranges from 0 to 1, with higher values associated with better unlearning.

### The more entangled the forget and retain sets are, the harder unlearning becomes

Prior research (e.g., by Feldman et al [11] and Carlini et al [5]) has tried to identify prototypical (or atypical) examples and their impact on learning. Primarily, this depended on the position of _examples_ within the overall _data space distribution_. In contrast, here we focus on the _embedding space_, as unlearning depends heavily on how the model has learned to represent training data. Furthermore, instead of looking at isolated examples, we delve into how "entangled" the retain and forget sets are in embedding space. We hypothesize that higher "entanglement" leads to harder unlearning: if the two sets are highly entangled, attempting to erase \(\mathcal{S}\) will cause accidentally erasing \(\mathcal{R}\) too.

We propose to measure entanglement between the retain and forget sets via the below _Entanglement Score_ (ES), inspired by a measure previously introduced in [16] to study learned representations.

\[\text{ES}(\mathcal{R},\mathcal{S};\theta^{o})=\frac{\frac{1}{|\mathcal{R}|} \sum_{i\in\mathcal{R}}(\phi_{i}-\mu_{\mathcal{R}})^{2}+\frac{1}{|\mathcal{S}|} \sum_{j\in\mathcal{S}}(\phi_{j}-\mu_{\mathcal{S}})^{2}}{\frac{1}{2}\big{(}( \mu_{\mathcal{R}}-\mu)^{2}+(\mu_{\mathcal{S}}-\mu)^{2}\big{)}}\] (2)

where \(\phi_{i}=g(x_{i};\theta^{o})\) is the embedding of example \(x_{i}\) according to the "original model" \(f\), parameterized by \(\theta^{o}\); where \(g\) denotes the forward pass through \(f\) up till the penultimate layer, i.e. excluding the classifier layer. Further, \(\mu_{\mathcal{R}}=\frac{1}{|\mathcal{R}|}\sum_{i\in\mathcal{R}}\phi_{i}\) is the mean embedding of the retain set, and analogously, \(\mu_{\mathcal{S}}\) the mean embedding of the forget set, while \(\mu\) is the mean embedding over all of \(\mathcal{D}_{train}=\mathcal{R}\cup\mathcal{S}\).

Intuitively, ES measures entanglement between \(\mathcal{S}\) and \(\mathcal{R}\) in the embedding space of the original model (before unlearning begins). The numerator measures intra-class variance, capturing the tightness of each of those two sets, independently, while the denominator measures inter-class variance between those two sets. Higher ES score corresponds to higher entanglement in the embedding space.

Our investigation hinges on generating three different forget/retain partitions with different degrees of entanglement: low, medium, and high. But, Equation (2) does not directly suggest a procedure for generating retain/forget partitions with a desired ES score. Hence, we achieved this indirectly using a proxy. Specifically, let \(d(i,\mu;\theta^{o})=||\phi_{i}-\mu||^{2}\) denote the l2-distance in the original model's embedding space between example \(i\) and centroid \(\mu\), as defined above. We compute this distance for each example in \(\mathcal{D}_{train}\) and sort those examples according to their \(d\)-values. We then form each forget set to contain a contiguous subset of examples from different ranges of that sorted list. We find that this procedure allows us to construct retain/forget partitions of varying ES values. The ES values for our low, medium and high partitions are 309.94\(\pm\)98.56, 1076.99\(\pm\)78.64, 1612.21\(\pm\)110.82 for CIFAR-10, and 963.82\(\pm\)113.53, 2831.24\(\pm\)558.63, and 3876.90\(\pm\)426.92 for CIFAR-100. We include details of this procedure in the Section A.3, along with visualizations and Maximum Mean Discrepancy (MMD) analysis to further validate ES, confirming that the degree of retain/forget entanglement aligns with the computed scores. We experiment with four dataset/architecture settings: CIFAR-10/ResNet-18, CIFAR-100/ResNet-50, Tiny-ImageNet/ResNet-18 and Tiny-ImageNet/VGG-16, using \(|S|=3000\). Refer to Section A.2 for implementation details and Section A.8 for more detailed results on all datasets.

We observe from Figure 0(a) that it is harder to unlearn when the retain and forget sets are more entangled: all unlearning algorithms have poorer performance for highly-entangled vs lower-entangled settings. Further, we notice that different unlearning algorithms suffer disproportionately as the entanglement increases. Notably, methods based on relabelling (SalUn and Random-label) perform very poorly when the entanglement is high. We hypothesize that this is because, if two examples \(i\) and \(j\) are close neighbours in embedding space, with \(i\) in the forget set and \(j\) in the retain set, forcing example \(i\) to be confidently predicted as an incorrect class (as relabelling algorithms do) will also cause \(j\) to be predicted as that incorrect class, too, thus causing a drop in retain accuracy, which is captured by ToW. This effect will be less pronounced if \(i\) and \(j\) are far from each other.

### The more memorized the forget examples are, the harder unlearning becomes

Feldman et al [11] have already established that models must memorize some atypical examples in order to perform well. Further, prior literature has also established that noisy examples (that are more likely to be memorized) witness more "forgetting events" during training (their predicted label flips to an incorrect one) [32] and that models trained with Differential Privacy (DP), a procedure where noise is added to the gradients (making it harder to memorize), find it primarily hard to correctly predict atypical examples [5]. In this section, we build upon these prior insights by investigating the connection between the degree of memorization of the forget set and difficulty of unlearning.

Figure 1: Uncovering two factors that affect unlearning difficulty according to ToW (where higher is better). **Left:** the more entangled the retain and forget sets are in the embedding space, the harder it is to unlearn. **Right:** the less memorized a forget set is (thus having influenced the model less), the easier it is to unlearn (for most algorithms). Error bars correspond to 95% confidence intervals from running each algorithm 3 times (6 times for relabelling-based that had higher variance).

Let's begin by inspecting Definition 2.2: if an example is not really memorized, the predictions of the model on that example will not change much whether the example was included in training or not. This implies that even the original model (no unlearning) is similar to retrain-from-scratch in terms of predictions on those examples, making unlearning unnecessary or trivial. On the other hand, for highly-memorized examples, the predictions between the original and retrained models will differ significantly, implying that an unlearning algorithm has "more work" to do to turn the original model into one that resembles the retrained one. We now investigate how the level of memorization of the forget set affects the behaviour of state-of-the-art unlearning algorithms. We hypothesize, based on our above intuition, that unlearning is easier when the forget set contains less-memorized examples.

To investigate this, we first compute the memorization score \(\text{mem}(\mathcal{A},\mathcal{D}_{train},i)\) of each example \(i\in\mathcal{D}_{train}\) and we sort all examples according to their scores. We then use that sorted list to create three different forget sets, corresponding to the lowest \(N\) scores ("low-mem"), the highest \(N\) ("high-mem"), and the \(N\) that are nearest to 0.5, i.e. the midpoint of the range of memorization scores ("medium-mem"), where \(N=3000\). We then apply different unlearning algorithms on each of these forget sets and compute ToW. We perform this experiment on CIFAR-10 using ResNet-18 and on CIFAR-100 using ResNet-50. Refer to Section A.2 for implementation details.

We first emphasize two key sets of conclusions. First, in terms of ToW, Figure 0(b) shows that, indeed, for most algorithms, the lower the memorization level of the forget set, the easier the problem is. In line with our prior discussion, even the original model performs well on "low-mem", but performs very poorly on "high-mem". Interestingly though, the two relabelling-based algorithms (SalUn and Random-label) follow an inverse trend: they perform better for higher-memorized forget sets. Second, breaking down ToW into its parts, we find interesting trends in terms of the forget set accuracy, in Figure 12. Specifically, for several unlearning algorithms, the forget accuracy for "low-mem" is still very high after unlearning them, as the model can infer the correct labels for such examples even when they weren't included in training; this follows directly by Definition 2.2 if unlearning is done by retraining, and is shown here for the first time for approximate unlearning algorithms. On the other hand, we find that, for "high-mem", different unlearning algorithms can (to varying degrees) cause the forget set accuracy to drop substantially; this is consistent with both [32] and [5], but shown here for the first time for approximate unlearning algorithms. Notably, we find that relabelling-based algorithms cause a larger drop in the accuracy of the forget set, relative to other approaches. This benefits ToW in the case of "high-mem" forget sets, where retraining has poor accuracy on this set (so they get rewarded by matching it), but it hurts on "low-mem", since it causes a large discrepancy from retraining, which has high accuracy on this set (since it makes similar predictions to the original model on this set, by definition, and the original model has high accuracy on all of \(\mathcal{D}_{train}\)).

Overall, we have presented the first investigation into the behaviour of unlearning algorithms applied on forget sets of different degrees of memorization. A key finding is that different algorithms outshine others for different forget sets. Most notable is the failure of relabelling-based algorithms on the "low-mem" forget set, which is easy for other algorithms and, in fact, even no unlearning in that case might be an acceptable solution. We intuit that this is due to their aggressive unlearning strategy yielding "over forgetting" (producing a forget set accuracy that is lower than that of retraining from scratch) as discussed above. Furthermore, we observe that different unlearning algorithms work best for the "low-mem", "medium-mem" and "high-mem" forget sets. Concretely, from Figure 0(b) we note that Finetune is best for "medium-mem", SalUn is best for "high-mem", and a number of algorithms are top-performers for "low-mem" (including no unlearning). This reveals a possible pathway for improving unlearning based on using different algorithms for different forget sets. So, how can one build on these insights to further improve unlearning algorithms performance?

## 5 Refined-Unlearning Meta-algorithm (RUM) for Improved Unlearning

Previously, we observed that unlearning algorithms have different behaviours on forget sets with different properties. For example, while "low mem" forget sets are almost trivial to unlearn (and even doing nothing may be acceptable), SalUn and Random-label perform poorly on them. On the other hand, SalUn and Random-label evidently outperform other unlearning algorithms on "high mem". These observations suggest that the optimal unlearning algorithm to use is dependent on the properties of the forget set. One could therefore pick the best unlearning algorithm for each unlearning request, based on these factors. However, in practical scenarios, forget sets may be distributed differently than in our preliminary experiments, that were designed to cleanly separate different factors of interest.

Indeed, real-world forget sets would likely contain a mixture of examples from different modes of the data distribution, some rare or highly-memorized while others common and not memorized at all. So, what can be done about these expected heterogeneous forget sets? How can our insights above be leveraged to improve unlearning for such cases?

To address this, we first propose a _refinement procedure_ that divides forget sets into homogeneous subsets (with respect to the factors that we have found to affect the difficulty of unlearning and behaviours of existing algorithms). Second, we propose to utilize a pool of state-of-the-art algorithms to unlearn different subsets. Put together, we propose a Refined-Unlearning Meta-algorithm (RUM), comprised of two steps: 1) Refinement and 2) Meta-unlearning. Figure 2 overviews RUM.

Step 1: Refinement.We introduce a function \(\mathcal{F}\) that partitions the forget set \(S\) into \(K\) subsets: \(\{S_{i}\}_{i=1}^{K}=\mathcal{F}(\mathcal{S})\) such that each forget set example appears in exactly one such subset. The intention of \(\mathcal{F}\) is to generate homogeneous subsets w.r.t factor(s) that affect difficulty / algorithm's behaviours.

Step 2: Meta-Unlearning.Having obtained the subsets \(\{S_{i}\}_{i=1}^{K}\) of \(\mathcal{S}\), we now require a "meta-algorithm" \(\mathcal{M}\) that dictates how to perform the individual unlearning requests and how to compose the resulting unlearned models to arrive to a model that has unlearned all of \(\mathcal{S}\). In this work, we focus on meta-algorithms that tackle unlearning of subsets in a sequence, leaving other designs for future work. It therefore remains for our "meta-algorithm" to decide: i) what unlearning algorithm to apply for each subset, ii) what order should the unlearning requests be executed in.

More concretely, we assume access to a pool of existing unlearning algorithms \(\mathcal{U}_{1}\ldots\mathcal{U}_{N}\), like the ones described in related work, for instance. Let \(\mathcal{M}^{\mathcal{U}}(S_{i})\) denote a procedure that takes as input a subset \(S_{i}\) and returns an unlearning algorithm \(\mathcal{U}\in\{\mathcal{U}_{1}\ldots\mathcal{U}_{N}\}\) that will be used for that subset. This selection can be done by leveraging insights such as those in Section 4. Further, let \(\mathcal{M}^{\mathcal{O}}\) denote a procedure that takes as input the \(K\) subsets of \(\mathcal{S}\) and returns a sorted list \(S^{\prime}=\mathcal{M}^{\mathcal{O}}(\mathcal{F}(\mathcal{S}))\) containing the K subsets in the desired order of execution.

Given the above ingredients, RUM proceeds by executing \(K\) unlearning requests in a sequence, with step \(i\) of that sequence corresponding to unlearning subset \(S^{\prime}[i]\) by applying \(\mathcal{U}_{i}(\theta^{o},S^{\prime}[i],\mathcal{R}_{i})=\theta^{u}_{i}\), where \(\theta^{u}_{i}\) denotes the unlearned model up to step \(i\) and \(\mathcal{U}_{i}=\mathcal{M}^{\mathcal{U}}(S^{\prime}[i])\) and \(\mathcal{R}_{i}=\mathcal{R}\cup\{S^{\prime}[i+1],\ldots S^{\prime}[K]\}\) is the retain set for step \(i\), containing \(\mathcal{R}\) as well as all other subsets of \(\mathcal{S}\) that have not yet been unlearned in the sequence so far. We finally return the unlearned model of the last step \(\theta^{u}_{K}\).

Our RUM framework is meant as an analysis framework, surfacing new problems to be solved and offering new pathways into future state-of-the-art algorithms. Nonetheless, we contribute below specific top-performing RUM instantiations, with specific choices for \(\mathcal{F}\) and for \(\mathcal{M}\).

## 6 Experimenting with RUM flavours

We now present RUM instantiations using a refinement strategy based on memorization scores and experimental evaluations answering the following questions: **Q1**: How useful is refinement alone? That is, for a given unlearning algorithm \(\mathcal{U}\), does applying \(\mathcal{U}\) sequentially on the \(K\) homogeneous subsets of \(\mathcal{S}\) outperform applying \(\mathcal{U}\) once on all of \(\mathcal{S}\)? **Q2**: Can we obtain further gains by additionally selecting the best-performing unlearning algorithm for each forget set subset? **Q3**: Are there interpetable factors behind the boost obtained by sequential unlearning of homogeneous subsets?

Experimental setupWe experiment with a refinement strategy based on memorization scores where \(K\) = 3. Specifically, we study unlearning a forget set \(\mathcal{S}\) that is the union of the three sets containing the \(N\) lowest, the \(N\) closest to 0.5, and the \(N\) highest memorized examples in the dataset,

Figure 2: Overview of RUM.

[MISSING_PAGE_FAIL:8]

mem", "medium-mem" and "high-mem" scenarios. On CIFAR-10, this corresponds to doing nothing for low-mem (i.e. using the original model directly), using Fine-tune for medium-mem and SalUn for high-mem (based on Figure 0(b)). From Figure 2(b) (and Table 0(a)), we observe that we get the overall best results by far, both in terms of ToW and MIA, by applying RUM with "nothing \(\rightarrow\) Fine-tune \(\rightarrow\) SalUn", demonstrating the value of incorporating our insights into unlearning pipelines. In fact, lets revisit our previous observation that SalUn and Random-Label perform uncharacteristically poorly on the "low-mem" forget set. In line with our hypothesis, we notice that "nothing \(\rightarrow\) SalUn \(\rightarrow\) SalUn" outperforms applying SalUn on all three subsets (a.k.a. SalUn \(\text{RUM}^{\mathcal{F}}\)). Note that, for CIFAR-100, the best algorithm for all subsets is NegGrad+, so full RUM corresponds to NegGrad+ \(\text{RUM}^{\mathcal{F}}\).

Can we obtain similar performance boosts with compute-efficient proxies?Given the computational cost of calculating memorization scores, we aim to find a more efficient proxy to enable practical deployment of RUM. We discuss a confidence-based memorization metric, termed "C-proxy", as a proxy for memorization [23, 32]. Section A.6 provides a detailed explanation of the proxy and the complete results across various datasets and architectures. Figure 3(a) shows that the observed unlearning difficulty pattern--specifically, that forget examples with higher memorization scores (i.e., lower C-proxy values, as they are negatively correlated; see Table 4) are harder to unlearn--remains consistent when using C-proxy, paralleling the results in Figure 0(b). This pattern is further confirmed on Tiny-ImageNet with both ResNet-18 and the VGG-16 architectures, as shown in Figure 9. We then examine whether using C-proxy achieves similar performance gains within RUM as the original memorization score. Figure 3(b) presents results on CIFAR-10 with ResNet-18, using C-proxy in place of memorization score during the refinement step. This analysis is also extended to Tiny-ImageNet with both ResNet-18 and VGG-16 architectures, as shown in Figure 10 and Table 17. Together, these results suggest that C-proxy is a practical and compute-efficient alternative, delivering significant performance gains comparable to those achieved with the memorization score. Detailed analysis of the use and appropriateness of various memorization proxies and their effect on RUM can be found in [37].

Analysis of sequence dynamicsWe report ToW and MIA with different orderings in Tables 0(a) and 0(b). We find that, while ToW is similar for different orderings, MIA can vary. To better understand these dynamics, we inspect the accuracies on \(\mathcal{S}\) and its subsets after each step in Figure 5 for SalUn\({}^{\mathcal{F}}\) on CIFAR-10 and in Figure 13 for NegGrad+\({}^{\mathcal{F}}\) CIFAR-100. The former reveals why SalUn\({}^{\mathcal{F}}\) with low \(\rightarrow\) med \(\rightarrow\) high order greatly outperforms vanilla SalUn. Recall that we identified in Section 4.2 that SalUn "overforgets" low-mem examples (its forget accuracy is lower than that of retraining). We observe from Figure 5 that future steps of the sequence neutralize that overforgetting effect on low-mem, leading to better ToW (see Figure 3). Interestingly, in line with our previous insights (Section 4.2), we find (from both Figures 5 and 13) that it is hard to cause the "low-mem" accuracy to become low and stay low. NegGrad+ does not drop it for any order of execution; SalUn drops it in the first ordering, but that drop is later reversed. We leave it to future work to further study these sequential dynamics and their helpful or harmful effects on Tow and MIA. The fact that MIA results differ based on the sequence may tie in with the recently-identified "privacy onion effect" [6].

Figure 4: Replacing mem scores with the efficient C-proxy yields similar trends and performance gains, carving a path for practical deployment of RUM. **Left:** forget sets with lower C-proxy values (i.e., higher mem scores, since C-proxy and memorization are negatively correlated) are harder to unlearn, consistent with the trend in Figure 0(b). **Right:**\(\text{RUM}^{\mathcal{F}}\) using C-proxy in the refinement step enhances unlearning performance across algorithms, comparable to using the memorization score in Figure 2(a). Error bars correspond to 95% confidence intervals, with each algorithm run 3 times.

## 7 Discussion and conclusion

We presented the first investigation into interpretable factors that affect the difficulty of unlearning. We found that unlearning gets harder i) the more entangled the forget and retain sets are in embedding space and ii) the more memorized the forget set is. Our investigation led into uncovering previously-unknown behaviours of state-of-the-art algorithms that were not surfaced when considering random forget sets: when do they fail, when do they excel and when do they exhibit different trends from each other. Notably, we discovered that relabelling-based methods suffer disproportionately as the embedding-space entanglement increases, and exhibit a reverse trend compared to other methods in their behaviour for different memorization levels. Armed with these insights, we then proposed the RUM framework surfacing new (sub)problems within unlearning, whose solution may lead to greater performance. Finally, we derived specific instantiations of RUM and analyzed how its different components can improve performance. We found that sequential unlearning of homogenized forget set subsets improves all considered state-of-the-art unlearning algorithms and investigated the dynamics of sequential unlearning to glean insights as to why that is. We also found that we can further boost performance by selecting the best unlearning algorithm per subset.

EfficiencyHow is this important aspect affected in our sequential framework? We remark that it depends on the unlearning algorithm. For instance, applying Fine-tune three times is much more expensive than applying it once, because Fine-tune performs (at least) one epoch over the entire retain set. But for other algorithms the overall cost does not increase significantly. Further, a key observation from our results is that we can do well by actually _doing nothing_ on a subset of the forget set, which can really boost efficiency (especially since the vast majority of examples are "low mem"). Additionally, our results with the C-proxy demonstrate that significant performance improvements in unlearning can be achieved with minimal computational cost, avoiding the heavy expense of computing memorization scores.

Data-space vs embedding-space outliersHow does the embedding space entanglement interact with the level of memorization of the forget set? We analyzed this and found in Table 3 that all of our memorization buckets have relatively high ES, indicating that separating out (data-space) outliers in the forget set doesn't lead to lower entanglement between the two sets in the embedding space. We leave it to future work to study the interaction of these factors.

Limitations and future workWe hope future work explores other refinement strategies (e.g. for a notion that captures embedding-space entanglement) and investigates privacy implications of sequential RUM, e.g. in terms of the "privacy onion effect" [6]. We also hope to see how our RUM framework can be adopted and adapted for unlearning in LLMs, especially given the findings from the contemporaneous paper [2] where unlearning is performed only on the highest-memorized examples in the forget set (albeit, memorization is defined differently for LLMs). We hope our framework continues to enable progress in understanding and improving unlearning and that our identified factors of difficulty and associated behaviours of existing algorithms continue to improve the state-of-the-art and inform the development of strong evaluation metrics that consider forget sets that vary in terms of relevant identified characteristics.

Acknowledgements

We thank Vincent Dumoulin and Fabian Pedregosa for valuable conversations and feedback at various stages of the project.

## References

* [1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. _Advances in neural information processing systems_, 31, 2018.
* [2] George-Octavian Barbulescu and Peter Triantafillou. To each (textual sequence) its own: Improving memorized-data unlearning in large language models. In _International conference on machine learning_, 2024.
* [3] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In _2021 IEEE Symposium on Security and Privacy (SP)_, pages 141-159. IEEE, 2021.
* [4] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 1897-1914. IEEE, 2022.
* [5] Nicholas Carlini, Ulfar Erlingsson, and Nicolas Papernot. Distribution density, tails, and outliers in machine learning: Metrics and applications. _arXiv preprint arXiv:1910.13427_, 2019.
* [6] Nicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, and Florian Tramer. The privacy onion effect: Memorization is relative. _Advances in Neural Information Processing Systems_, 35:13263-13276, 2022.
* [7] R Dennis Cook and Sanford Weisberg. Criticism and influence analysis in regression. _Sociological methodology_, 13:313-361, 1982.
* [8] Cynthia Dwork. Differential privacy. In _International colloquium on automata, languages, and programming_, pages 1-12. Springer, 2006.
* [9] Chongyu Fan, Jiancheng Liu, Alfred Hero, and Sijia Liu. Challenging forgets: Unveiling the worst-case forget sets in machine unlearning. _arXiv preprint arXiv:2403.07362_, 2024.
* [10] Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation. _arXiv preprint arXiv:2310.12508_, 2023.
* [11] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In _Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing_, pages 954-959, 2020.
* [12] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. _Advances in Neural Information Processing Systems_, 33:2881-2891, 2020.
* [13] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. _arXiv preprint arXiv:1803.03635_, 2018.
* [14] Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making ai forget you: Data deletion in machine learning. _Advances in neural information processing systems_, 32, 2019.
* [15] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9304-9312, 2020.
* [16] Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriiia Cherepanova, and Tom Goldstein. Unraveling meta-learning: Understanding feature representations for few-shot tasks. In _International Conference on Machine Learning_, pages 3607-3616. PMLR, 2020.

* [17] Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 11516-11524, 2021.
* [18] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _The Journal of Machine Learning Research_, 13(1):723-773, 2012.
* [19] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal from machine learning models. _arXiv preprint arXiv:1911.03030_, 2019.
* [20] Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, and Nicolas Papernot. Inexact unlearning needs more careful evaluations to avoid a false sense of privacy. _arXiv preprint arXiv:2403.01218_, 2024.
* [21] Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approximate data deletion from machine learning models. In _International Conference on Artificial Intelligence and Statistics_, pages 2008-2016. PMLR, 2021.
* [22] Matthew Jagielski, Om Thakkar, Florian Tramer, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, et al. Measuring forgetting of memorized training examples. _arXiv preprint arXiv:2207.00099_, 2022.
* [23] Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regularities of labeled data in overparameterized models. _arXiv preprint arXiv:2002.03206_, 2020.
* [24] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In _International conference on machine learning_, pages 1885-1894. PMLR, 2017.
* [25] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [26] Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, PRANAY SHARMA, Sijia Liu, et al. Model sparsity can simplify machine unlearning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [27] Xiaolong Ma, Geng Yuan, Xuan Shen, Tianlong Chen, Xuxi Chen, Xiaohan Chen, Ning Liu, Minghai Qin, Sijia Liu, Zhangyang Wang, et al. Sanity checks for lottery tickets: Does your winning ticket really win the jackpot? _Advances in Neural Information Processing Systems_, 34:12749-12760, 2021.
* [28] Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Scholkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neighbourhood comparison. _arXiv preprint arXiv:2305.18462_, 2023.
* [29] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. _arXiv preprint arXiv:2209.02299_, 2022.
* [30] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. _arXiv preprint arXiv:1706.03825_, 2017.
* [31] Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling sgd: Understanding factors influencing machine unlearning. In _2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P)_, pages 303-319. IEEE, 2022.
* [32] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. _arXiv preprint arXiv:1812.05159_, 2018.
* [33] Eleni Triantafillou, Peter Kairouz, Fabian Pedregosa, Jamie Hayes, Meghdad Kurmanji, Kairan Zhao, Vincent Dumoulin, Julio Jacques Junior, Ioannis Mitliagkas, Jun Wan, Lisheng Sun Hosoya, Sergio Escalera, Gintare Karolina Dziugaite, Peter Triantafillou, and Isabelle Guyon. Are we making progress in unlearning? findings from the first neurips unlearning competition. _arXiv preprint arXiv:2406.09073_, 2024.

- machine unlearning, 2023.
* [35] Junxiao Wang, Song Guo, Xin Xie, and Heng Qi. Federated unlearning via class-discriminative pruning. In _Proceedings of the ACM Web Conference 2022_, pages 622-632, 2022.
* [36] Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features and labels. _arXiv preprint arXiv:2108.11577_, 2021.
* [37] Kairan Zhao and Peter Triantafillou. Scalability of memorization-based machine unlearning. In _NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability_.

Appendix / supplemental material

### Broader impact

Unlearning research can have profound broader impact in allowing users to request their data to be deleted from models or making models safer or more accurate by removing harmful or outdated information. Our work is exploratory: we identify factors affecting difficulty that are interpretable and can drive improvements to unlearning pipelines. As such, we don't see any direct harmful impact of our work.

### Implementation details

Datasets and modelsWe use the CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets for our evaluation. CIFAR-10 consists of 60,000 32x32 color images across 10 classes, with 6,000 images per class. CIFAR-100 contains 100 classes, each with 600 32x32 color images. Tiny-ImageNet includes 100,000 64x64 color images across 200 classes, with each class containing 500 training images, 50 validation images, and 50 test images (for a total of 100,000 training, 10,000 validation, and 10,000 test images). For CIFAR-10 and CIFAR-100, the train, validation, and test set sizes are 45,000, 5,000, and 10,000 images, respectively. We use ResNet-18, ResNet-50, and VGG-16 as model architectures. Specifically, we use ResNet-18 for CIFAR-10, ResNet-50 for CIFAR-100, and both ResNet-18 and VGG-16 for Tiny-ImageNet.

Training details for original modelsWe trained four models across different datasets and architectures: ResNet-18 on CIFAR-10, ResNet-50 on CIFAR-100, ResNet-18 on Tiny-ImageNet, and VGG-16 on Tiny-ImageNet. For CIFAR-10, the ResNet-18 model was trained for 30 epochs using the SGD optimizer. The learning rate was initialized at 0.1 and scheduled with cosine decay. For CIFAR-100, the ResNet-50 model was trained for 150 epochs using the SGD optimizer, with the learning rate initialized at 0.1 and decayed by a factor of 0.2 at epochs 60 and 120. For Tiny-ImageNet, the ResNet-18 model was trained for 80 epochs, and the VGG-16 model was trained for 100 epochs, both using the SGD optimizer with an initial learning rate of 0.1 and cosine decay. All models were trained with a weight decay of 0.0005, a momentum of 0.9, and a batch size of 256. Additionally, data augmentations, including random cropping and random horizontal flipping, were applied during training on CIFAR-100 and Tiny-ImageNet. All training was conducted on Nvidia RTX A5000 GPUs.

Training details for machine unlearningTo ensure optimal performance, we carefully set the hyperparameters for each unlearning method across different datasets and architectures. For retrain-from-scratch, we followed the exact same training procedure as the original model, but only trained on the retain set, excluding the forget set. For Fine-tune, we trained the model for 10 epochs with a learning rate in the range [0.01, 0.1]. L1-sparse was run for 10 epochs with a learning rate in the range [0.005, 0.1] and a sparsity-promoting regularization parameter \(\gamma\) in the range [\(10^{-6}\), \(10^{-4}\)]. NegGrad was also trained for 10 epochs, with a learning rate in the range [\(10^{-4}\), 0.1]. For NegGrad+, we used a \(\beta\) value in the range [0.85, 0.99] as a weighting factor that balances two components of the loss, training for 5 epochs with a learning rate in the range [0.01, 0.05]. Influence unlearning involved tuning the parameter \(\alpha\) for the woodisher Hessian Inverse approximation within the range [1, 100]. SalUn was trained for 5-10 epochs with a learning rate in the range [0.005, 0.1] and sparsity ratios in the range [0.3, 0.7]. Random-label was trained for 10 epochs with a learning rate in the range [0.01, 0.1].

In our experiments with forget / retain set partitions at varied levels of memorization or ES, we tuned the hyperparameters to achieve the best ToW performance for each unlearning algorithm. The results are reported as averages with 95% confidence intervals over 3 runs, except for relabeling-based methods, which had higher variance and were therefore run 6 times. For the RUM experiment, we adjusted the hyperparameters at each step to ensure that the accuracy after each step closely matched the accuracy obtained by retraining from scratch. This procedure was repeated for all algorithms, with results reported as averages over 3 runs with 95% confidence intervals.

### Procedure for creating retain / forget partitions with varying ES

To create retain / forget partitions with varied levels of ES, we followed a systematic procedure. Initially, we trained the original model \(\theta^{o}\) on the entire training dataset \(\mathcal{D}_{train}\). Using \(\theta^{o}\), we then extracted embeddings for each data point in \(\mathcal{D}_{train}\). The global centroid for \(\mathcal{D}_{train}\), denoted as \(\mu\), was determined by calculating the mean of all example embeddings. For each example \(i\) in \(\mathcal{D}_{train}\), we then computed its \(l2\)-distance from the global centroid \(\mu\) in the original model's embedding space as follows:

\[d(i,\mu;\theta^{o})=||\phi_{i}-\mu||^{2}\]

We ranked these distances for all data examples in \(\mathcal{D}_{train}\) and selected the 3000 examples with the highest distances to form the low ES bucket. Subsequently, we moved further down the ranked list, selecting 3000 examples with progressively lower distances to form the medium and high ES buckets, until we achieved the desired levels of ES variation. This approach allowed us to form forget sets, each with a size of 3000, categorized into low, medium, and high ES levels. The rationale behind this selection is that examples with high distances from the global centroid are considered "distant" from the overall data distribution in the embedding space and are therefore less entangled with the rest of the dataset, i.e., the retain set. Various unlearning algorithms were then deployed on \(\theta^{o}\) across different forget / retain partitions. Their performance was measured using ToW along with forget, retain, and test accuracy, as well as MIA.

This procedure enabled us to create retain / forget partitions with varying ES values. The ES values for our low, medium, and high ES partitions are shown in Table 2. As observed from the table, the ES values increase from low to high ES partitions for both CIFAR-10 and CIFAR-100, confirming the effectiveness of our procedure. We also use Maximum Mean Discrepancy (MMD) [18], a widely-used metric, to further validate ES. The MMD is computed as follows: Given a pre-trained model \(\theta^{o}\) (the original model in our case) trained on \(\mathcal{D}_{train}\), we first extract features for the forget set \(\mathcal{S}\) and the retain set \(\mathcal{R}\) using \(\theta^{o}\), denoted as \(\phi(\mathcal{S})\) and \(\phi(\mathcal{R})\), respectively. We then apply an RBF kernel to map these features to another space \(\mathcal{H}\) such that \(\phi^{\prime}(\mathcal{S}),\phi^{\prime}(\mathcal{R})\in\mathcal{H}\). The MMD between \(\phi(\mathcal{S})\) and \(\phi(\mathcal{R})\) is then calculated using the following formula:

\[\text{MMD}^{2}(\phi(\mathcal{S}),\phi(\mathcal{R}))=\left\|\frac{1}{|\mathcal{ S}|}\sum_{i\in\mathcal{S}}\phi^{\prime}_{i}-\frac{1}{|\mathcal{R}|}\sum_{j\in \mathcal{R}}\phi^{\prime}_{j}\right\|^{2}_{\mathcal{H}}\]

Our experiments reveal a negative correlation between ES and MMD scores, as reported in Table 2. For CIFAR-10/ResNet-18 and CIFAR-100/ResNet-50, low (high) ES values correspond to high (low) MMD values, supporting the consistency of the ES.

Additionally, Figure 6 presents the data representation of low, medium, and high ES partitions, confirming that the degree of entanglement between the retain and forget sets aligns with the computed ES values. As we move from low to high ES partitions, the forget set (yellow) and the retain set (blue) become increasingly entangled. This indicates that higher ES partitions reflect greater complexity in distinguishing between the two sets.

### Description of MIA and ToW-MIA

We adopted a MIA based on prediction confidence, following the procedure described by [26]. To conduct this attack, we first sampled equal-sized data from the retain set and the test set, using these to train a binary classifier as the MIA model. This model is designed to distinguish whether a data example was involved in the training stage or not.

Next, we applied this attack model to the forget set to evaluate unlearning performance during the testing phase, after an unlearning method was implemented. Intuitively, for successful unlearning, we want forget set examples to be classified as "non-training" data. We define "training" data as the positive class and "non-training" data as the negative class, and measured the performance of MIA by calculating the ratio of true negatives (i.e., the number of the forgetting samples predicted asnon-training examples) predicted by the MIA model to the total size of the forget set, as shown in (3), following the same procedure as prior work [26].

\[\text{MIA Performance}=\frac{TN_{\mathcal{S}}}{|\mathcal{S}|},\] (3)

In this context, \(\mathcal{S}\subseteq\mathcal{D}_{train}\) represents the forget set and \(|\mathcal{S}|\) is the total size of this set. The term \(TN_{\mathcal{S}}\) denotes the number of true negatives predicted by the MIA model, indicating examples identified as "non-training" data. This metric ranges from 0 to 1, with higher values signifying a larger portion of the forget set predicted as "non-training" data. The ideal MIA score, for an unlearning algorithm, is one that matches the MIA score of retraining-from-scratch. Note that, even if applying this MIA on retrain-from-scratch, some portion of the forget set will be classified as "training data" due to heavily resembling the retain set (e.g. examples that are not really memorized may have similar confidences regardless on whether or not they were trained on). And we want the model confidences of the unlearned model to resemble as closely as possible those of retrain-from-scratch. For this reason, we additionally report the "MIA gap" (the absolute differentce between the MIA score for unlearning compared to that of retrain-from-scratch) as a easier-to-interpret metric, where lower is better, and the ideal score there is 0.

Building on the MIA setup above, we introduce another metric "ToW-MIA" to evaluate unlearning performance, similar to ToW described in Section 4. The metric is defined as follows:

\[\text{ToW-MIA}(\theta^{u},\theta^{r},\mathcal{S},\mathcal{R},\mathcal{D}_{ test})=(1-\text{dm}(\theta^{u},\theta^{r},\mathcal{S}))\cdot(1-\text{da}( \theta^{u},\theta^{r},\mathcal{R}))\cdot(1-\text{da}(\theta^{u},\theta^{r}, \mathcal{D}_{test}))\]

where \(\text{m}(\theta,\mathcal{D})\) represents the MIA performance of the model \(\theta\) trained on \(\mathcal{D}\), as defined in Equation (3). The first term, \(\text{dm}(\theta_{u},\theta_{r},\mathcal{S})=|\text{m}(\theta_{u},\mathcal{S}) -\text{m}(\theta_{r},\mathcal{S})|\), denotes the absolute difference in MIA

\begin{table}
\begin{tabular}{c c c c} \hline  & Low ES & Medium ES & High ES \\ \hline ES value & 309.94\(\pm\)98.56 & 1076.99\(\pm\)78.64 & 1612.210\(\pm\)110.82 \\ MMD value & (5.15\(\pm\)0.26)\(\times 10^{-2}\) & (4.07\(\pm\)0.59)\(\times 10^{-2}\) & (2.84\(\pm\)0.75)\(\times 10^{-2}\) \\ \hline \end{tabular} (a) CIFAR-10

\begin{tabular}{c c c c} \hline  & Low ES & Medium ES & High ES \\ \hline ES value & 963.82\(\pm\)113.53 & 2831.24\(\pm\)558.63 & 3876.90\(\pm\)426.92 \\ MMD value & (1892.69\(\pm\)0.07)\(\times 10^{-5}\) & (1892.13\(\pm\)0.05)\(\times 10^{-5}\) & (1891.44\(\pm\)0.18)\(\times 10^{-5}\) \\ \hline \end{tabular} (b) CIFAR-100

\end{table}
Table 2: ES and MMD scores for the low, medium and high forget / retain partitions for CIFAR-10 and CIFAR-100.

Figure 6: Data representation visualization for forget / retain partitions with low, medium, and high ES. We used PCA to reduce the dimensionality for visualization. In each figure, the data examples from the forget set are shown in yellow, while those from the retain set are in blue. The global centroid is marked in red at the center of the figures.

performance on \(\mathcal{S}\) between the unlearned model \(\theta_{u}\) and the retrained-from-scratch model \(\theta_{r}\). This term is the key distinction between ToW and ToW-MIA, where "forget quality" is measured by MIA (ToW-MIA) rather than accuracy (ToW). The other two terms remain identical to those in ToW (see the equation in Section 4), representing the absolute difference in accuracy between \(\theta^{u}\) and \(\theta^{r}\) on \(\mathcal{R}\) and \(\mathcal{D}_{test}\), respectively, which reflect "model utility". Like ToW, ToW-MIA rewards unlearned models that approximate the performance of the retrained model across the forget, retain, and test sets. ToW-MIA ranges from 0 to 1, with higher values indicating better unlearning performance. Figure 7 show the same trends with ToW-MIA as for ToW, both for unlearning difficulty analysis and RUM. More detailed results using ToW-MIA can be found in Table 14.

### Data-space vs embedding-space outliers

Building on our discussion in Section 4, we identify two factors that affect the difficulty of unlearning: the entanglement between the forget and retain sets, and the memorization level of the forget examples. This raises the question: how do these two factors interact with each other? Are the outliers in data space the same as those in embedding space? Our findings in this section suggest that these factors are indeed distinct.

Memorization within ES-based partitionsWe analyzed the memorization scores within each forget set categorized by different ES values. Figure 8 shows the distribution and average memorization scores in the forget sets of low, medium, and high ES categories. It can be seen that each ES category's forget set contains a mix of memorization levels. Specifically, while the mean memorization score of forget examples increases from low to high ES categories, the memorization scores still span the entire range from 0 to 1.

Entanglement within memorization-based partitionsWe examined the ES values for each memorization category to understand the entanglement between the forget and retain sets in the embedding space. Table 3 presents the ES values corresponding to low, medium, and high memorization buckets. The findings reveal that all the memorization bucket exhibits relatively high ES, suggesting that separating outliers in the data space does not reduce the entanglement between the forget and retain sets in the embedding space.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & Low memorization & Medium memorization & High memorization \\ \hline ES value & 21134.127 & 32785.711 & 14736.591 \\ \hline \hline \end{tabular}
\end{table}
Table 3: ES values for forget / retain partitions across varied memorization levels for CIFAR-10 and CIFAR-100.

Figure 7: Evaluating performance with ToW-MIA (higher is better) for unlearning difficulty analysis (a, b) and \(\text{RUM}^{\mathcal{F}}\) (c). Error bars represent 95% confidence intervals from 3 runs per algorithm.

These findings demonstrate that embedding space entanglement and the memorization level of the forget set are distinct concepts, not merely different aspects of the same phenomenon.

### Confidence-based proxy for memorization

To improve the efficiency of calculating memorization scores, we leverage insights from previous works [23; 32] and introduce a model confidence-based metric, referred to as the **C-proxy**, which serves as a proxy for the memorization score. The computation of the C-proxy proceeds as follows: For each data point \((x_{i},y_{i})\in\mathcal{D}\), we track the softmax probability of the model's prediction \(\theta(x_{i})\) for the ground-truth label \(y_{i}\) across all training epochs, as the model \(\theta\) is trained on \(\mathcal{D}\) using algorithm \(\mathcal{A}\). These probabilities are then averaged over all epochs at the end of training to capture the training dynamics.

Table 4 presents the fidelity (measured by Spearman correlation) and efficiency (measured by additional computational cost) of the C-proxy in relation to memorization. This evaluation is conducted on both CIFAR-10 and CIFAR-100 datasets. As shown, the C-proxy demonstrates a strong negative Spearman correlation with memorization scores while significantly reducing computational overhead compared to both computing memorization scores and retraining the model from scratch. This suggests that the C-proxy is an effective proxy for memorization while also offering substantial efficiency gains.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset & Spearman corr. (mem) & Comp. time \% (mem) & Comp. time \% (retrain) \\ \hline CIFAR-10 & -0.80 & 0.018\% & 17.123\% \\ CIFAR-100 & -0.91 & 0.002\% & 8.175\% \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of C-proxy based on Spearman correlation with memorization and relative computation time percentages in comparison to memorization computation / retraining the model from scratch, evaluated on CIFAR-10 and CIFAR-100 using ResNet-18 and ResNet-50, respectively.

Figure 8: Memorization distribution for low, medium, high ES forget / retain partitions. The mean memorization score for low, medium, high ES partitions are 0.084\(\pm\)0.203, 0.134\(\pm\)0.235, 0.390\(\pm\)0.326 for CIFAR-10, and 0.159\(\pm\)0.283, 0.222\(\pm\)0.329, and 0.317\(\pm\)0.364 for CIFAR-100.

Figure 3(a) and Figure 9 present results across different datasets and model architectures, demonstrating that the pattern identified in Section 4.2 remains consistent when using the C-proxy instead of the memorization score. Specifically, the lower the C-proxy value, the harder it is to unlearn (note that the C-proxy is negatively correlated with memorization). Furthermore, Figure 3(b) and Figure 10 show the results of \(\text{RUM}^{\mathcal{F}}\) across various datasets and model architectures, using the C-proxy in place of memorization for the refinement step described in Section 5. More detailed results, including accuracy and MIA, are available in Table 17. These results demonstrate that \(\text{RUM}^{\mathcal{F}}\) still achieves a significant unlearning performance gain when C-proxy is used, comparable to those obtained with the memorization score. A detailed analysis of various memorization proxies, their suitability, and their impact on RUM can be found in [37].

### Per-example differences between the unlearned and retrained models

While accuracy is a commonly used metric in the unlearning literature [29], it reflects an average over a data distribution and may overlook finer details. To gain a more granular understanding of the similarities/differences between the unlearned model and the retrained model, we examine their behavior at the level of individual examples. Specifically, we collect predictions from both models for each example and count the examples where their predictions differ. Table 5 presents these results for a representative setting: CIFAR-10 dataset with ResNet-18 architecture.

We observe that, in all cases, the percentage of disagreements between the unlearned and retrained models remains low. For highly memorized data (which typically makes unlearning harder), the disagreement rate increases but stays modest, ranging between ca. [7%, 15%]. For low-memorized data, the disagreement rate is lower, within ca. [3%, 5%].

Interestingly, for the vast majority of unlearning algorithms, these per-example disagreement trends align with those observed using ToW: low-memorized examples are generally easier to unlearn, as indicated by both ToW and the lower disagreement rate, while high-memorized examples are harder to unlearn, with higher disagreement rates. A similar trend is observed across different ES levels as well. This consistency between average accuracies (ToW) and per-example disagreements supports the robustness of our findings and underscores the validity of ToW as a measure of unlearning difficulty.

Figure 10: Impact of C-proxy on unlearning performance on Tiny-ImageNet with ResNet-18 and VGG-16. Each unlearning algorithm is applied in three ways: \(\text{RUM}^{\mathcal{F}}\), as well as vanilla and shuffle as comparison. Error bars represent 95% confidence intervals from 3 runs per algorithm.

Figure 9: Impact of C-proxy on unlearning difficulty: C-proxy vs ToW on Tiny-ImageNet across low, medium, and high C-proxy levels, using ResNet-18 and VGG-16. Error bars represent 95% confidence intervals from 3 runs per algorithm.

[MISSING_PAGE_FAIL:20]

Tables 16 provide detailed results when applying RUM to different unlearning algorithms for CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. We selected the top-performing algorithms from previous experiments (i.e., Fine-tune, L1-sparse, NegGrad+, SalUn) and applied the refinement strategy to each algorithm in three ways: i) "vanilla": unlearning \(\mathcal{S}\) in one go, ii)"shuffle": sequentially applying the algorithm on 3 equal-sized random subsets, serving as a control experiment, iii)"RUM\({}^{\mathcal{F}}\)": sequentially applying the algorithm on 3 subsets of \(\mathcal{S}\) in low \(\rightarrow\) med \(\rightarrow\) high memorization order. Each experiment was conducted 3 times, with average values and 95% confidence intervals reported. Our results indicate that RUM\({}^{\mathcal{F}}\) improves the performance of each unlearning algorithm, and the full RUM approach, which uses the best algorithm for each subset, achieves the overall best results.

Furthermore, to gain a deeper understanding of the dynamics involved in applying RUM, we plotted the sequence dynamics for SalUn RUM\({}^{\mathcal{F}}\) on CIFAR-10 (Figure 5) and NegGrad+ RUM\({}^{\mathcal{F}}\) on CIFAR-100 (Figure 13). These plots show the accuracy of the entire forget set, the retain set, the test set, and subsets of the forget set after each step. They demonstrate that while both orderings (low \(\rightarrow\) med \(\rightarrow\) high and high \(\rightarrow\) med \(\rightarrow\) low memorization) yield similar ToW according to Table 16, their sequence dynamics during the unlearning phase are different. This phenomenon is discussed in Section 6 in the main paper, specifically in the **Analysis of Sequence Dynamics** paragraph.

Figure 11: Forget, retain, test accuracy and MIA performance for forget / retain partitions with varied ES, for CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. As the ES increases, there is a notable decline in forget accuracy and a significant rise in MIA performance. This trend indicates that as the forget and retain sets become more entangled, more information from the forget set is effectively removed after unlearning.

Figure 12: Forget, retain, test accuracy and MIA performance for forget / retain partitions with varying levels of memorization, for CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. As the memorization level of the forget sets increases, forget accuracy significantly decreases while MIA performance increases. This trend indicates that as the forget examples become more memorized by the model, unlearning becomes more effective in removing the effect of these examples.

Figure 13: Analysis of sequence dynamics for two different orderings. We apply NegGrad+ RUM on CIFAR-100 dataset using ResNet-50 and show the accuracy on overall forget set (and each of its subsets), the retain set and test set after each step in the RUM sequence.

\begin{table}

\end{table}
Table 6: ToW for different unlearning algorithms applied to forget / retain sets with varying ES, evaluated on various datasets and model architectures. Across all algorithms, including the baseline without any unlearning performed (denoted as â€œOriginalâ€), we observe that ToW decreases from low to high ES partitions, indicating that unlearning becomes harder as the forget and retain sets become more entangled.

\begin{table}

\end{table}
Table 7: Accuracy and MIA performance for different unlearning algorithms applied to forget / retain sets with varying ES for CIFAR-10 using ResNet-18.

\begin{table}

\end{table}
Table 8: Accuracy and MIA performance of different unlearning algorithms on forget / retain sets with varying ES for CIFAR-100 using ResNet-50.

\begin{table}

\end{table}
Table 9: Accuracy and MIA performance of different unlearning algorithms on forget / retain sets with varying ES for Tiny-ImageNet using ResNet-18.

\begin{table}

\end{table}
Table 10: Accuracy and MIA performance of different unlearning algorithms on forget / retain sets with varying ES for Tiny-ImageNet using VGG-16.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Low memorization & Medium memorization & High memorization \\ \hline Retrain & 1.000 \(\pm\) 0.000 & 1.000 \(\pm\) 0.000 & 1.000 \(\pm\) 0.000 \\ Original & 0.988 \(\pm\) 0.007 & 0.723 \(\pm\) 0.053 & 0.231 \(\pm\) 0.058 \\ Fine-tune & 0.933 \(\pm\) 0.052 & 0.884 \(\pm\) 0.019 & 0.760 \(\pm\) 0.065 \\ L1-sparse & 0.914 \(\pm\) 0.061 & 0.816 \(\pm\) 0.011 & 0.629 \(\pm\) 0.087 \\ NegGrad & 0.938 \(\pm\) 0.028 & 0.738 \(\pm\) 0.005 & 0.325 \(\pm\) 0.098 \\ NegGrad+ & 0.965 \(\pm\) 0.020 & 0.831 \(\pm\) 0.032 & 0.661 \(\pm\) 0.082 \\ SCRUB & 0.988 \(\pm\) 0.010 & 0.923 \(\pm\) 0.033 & 0.780 \(\pm\) 0.101 \\ Influence unlearning & 0.986 \(\pm\) 0.031 & 0.738 \(\pm\) 0.051 & 0.381 \(\pm\) 0.037 \\ Salun & 0.774 \(\pm\) 0.043 & 0.758 \(\pm\) 0.053 & 0.886 \(\pm\) 0.082 \\ Random-label & 0.709 \(\pm\) 0.029 & 0.548 \(\pm\) 0.093 & 0.877 \(\pm\) 0.064 \\ \hline \hline \end{tabular}
\end{table}
Table 11: ToW for different unlearning algorithms applied to forget sets with varying levels of memorization, for CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. As the memorization level of the forget examples increases, the ToW significantly decreases for most algorithms, indicating that unlearning becomes harder when the forget examples are more memorized by the model.

\begin{table}

\end{table}
Table 12: Accuracy and MIA results for various unlearning algorithms applied on forget / retain sets with different memorization levels for CIFAR10 using ResNet-18.

\begin{table}

\end{table}
Table 13: Accuracy and MIA performance for different unlearning algorithms applied on forget / retain sets of varying levels of memorization for CIFAR100 using ResNet-50.

\begin{table}

\end{table}
Table 14: Unlearning performance evaluated by ToW-MIA, with ToW results included for comparison. **Subtables (a, b)**: ToW-MIA for different unlearning algorithms applied to forget / retain sets with varying ES or memorization levels on CIFAR-10 using ResNet-18, with ToW results from Tables 6 and 11 for comparison. **Subtable (c)**: RUM\({}^{\mathcal{F}}\) and RUM results evaluated by ToW-MIA on CIFAR-10 using ResNet-18, with ToW results from Table 16 for comparison.

\begin{table}

\end{table}
Table 15: Class distribution of the forget set in the RUM experiment for CIFAR-10 and CIFAR-100, with 3000 examples in the forget set for each dataset. The forget set includes examples from all classes in both datasets.

[MISSING_PAGE_FAIL:33]

\begin{table}

\end{table}
Table 17: RUM results using C-proxy for the refinement step, evaluated by ToW on CIFAR-10 and Tiny-ImageNet datasets. Each algorithm is applied in three ways: \(\text{RUM}^{\mathcal{F}}\), as well as vanilla and shuffle as comparison. Results are averaged over 3 runs with 95% confidence intervals.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims in the abstract and introduction accurately outline the paper's contributions, aligning well with the scope and findings presented in the study (we study interpretable factors affecting unlearning difficulty, uncover previously-unknown behaviours of state-of-the-art algorithms and propose a framework that leads to improvements of unlearning performance). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper includes a dedicated "Discussion and conclusion" section that discuss the limitations of the work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This paper does not include any theoretical results, hence no assumptions or proofs are required. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper provides detailed descriptions of the experimental setup, including the datasets used, model architectures, hyperparameters, and evaluation metrics. This ensures that the main experimental results can be reproduced accurately, supporting the paper's claims and conclusions. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

* We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
* **Open access to data and code*
* Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have included a GitHub repository containing our code and detailed instructions to enable other researchers to faithfully reproduce the main experimental results. Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details*
* Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental setting and details are described in Section 6 in the main paper and in Section A.2. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports the results with 95% confidence intervals and provides error bars, which appropriately reflect the statistical significance and variability of the experimental findings. This ensures that the reliability and significance of the results are clearly communicated.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper provides detailed information on the computational resources used for the experiments in A.2. This ensures that other researchers can accurately reproduce the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research adheres to the NeurIPS Code of Ethics, ensuring compliance with ethical standards in terms of data usage, experiment conduct, and the overall research process. Anonymity and confidentiality have been maintained where required, and all ethical considerations have been duly addressed. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss this is Section A.1. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no significant risks for misuse of data or models. Therefore, specific safeguards are not necessary. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]Justification: The paper properly credits the creators and original owners of all assets used, including code, data, and models. The licenses and terms of use are explicitly mentioned and adhered to, ensuring compliance with the original creators' requirements. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.