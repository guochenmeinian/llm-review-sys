# Improving robustness to corruptions with multiplicative weight perturbations

 Trung Trinh\({}^{1}\)   Markus Heinonen\({}^{1}\)   Luigi Acerbi\({}^{2}\)   Samuel Kaski\({}^{1,3}\)

\({}^{1}\)Department of Computer Science, Aalto University, Finland

\({}^{2}\)Department of Computer Science, University of Helsinki, Finland

\({}^{3}\)Department of Computer Science, University of Manchester, United Kingdom

{trung.trinh, markus.o.heinonen, samuel.kaski}@aalto.fi,

luigi.acerbi@helsinki.fi

###### Abstract

Deep neural networks (DNNs) excel on clean images but struggle with corrupted ones. Incorporating specific corruptions into the data augmentation pipeline can improve robustness to those corruptions but may harm performance on clean images and other types of distortion. In this paper, we introduce an alternative approach that improves the robustness of DNNs to a wide range of corruptions without compromising accuracy on clean images. We first demonstrate that input perturbations can be mimicked by multiplicative perturbations in the weight space. Leveraging this, we propose Data Augmentation via Multiplicative Perturbation (DAMP), a training method that optimizes DNNs under random multiplicative weight perturbations. We also examine the recently proposed Adaptive Sharpness-Aware Minimization (ASAM) and show that it optimizes DNNs under adversarial multiplicative weight perturbations. Experiments on image classification datasets (CIFAR-10/100, Tiny-ImageNet and ImageNet) and neural network architectures (ResNet50, ViT-S/16, ViT-B/16) show that DAMP enhances model generalization performance in the presence of corruptions across different settings. Notably, DAMP is able to train a ViT-S/16 on ImageNet from scratch, reaching the top-1 error of \(23.7\%\) which is comparable to ResNet50 without extensive data augmentations.1

Footnote 1: Our code is available at https://github.com/trungtrinh44/DAMP

## 1 Introduction

Deep neural networks (DNNs) demonstrate impressive accuracy in computer vision tasks when evaluated on carefully curated and clean datasets. However, their performance significantly declines when test images are affected by natural distortions such as camera noise, changes in lighting and weather conditions, or image compression algorithms (Hendrycks and Dietterich, 2019). This drop in performance is problematic in production settings, where models inevitably encounter such perturbed inputs. Therefore, it is crucial to develop methods that produce reliable DNNs robust to common image corruptions, particularly for deployment in safety-critical systems (Amodei et al., 2016).

To enhance robustness against a specific corruption, one could simply include it in the data augmentation pipeline during training. However, this approach can diminish performance on clean images and reduce robustness to other types of corruptions (Geirhos et al., 2018). More advanced data augmentation techniques (Cubuk et al., 2018; Hendrycks et al., 2019; Lopes et al., 2019) have been developed which effectively enhance corruption robustness without compromising accuracy on clean images. Nonetheless, a recent study by Mintun et al. (2021) has identified a new set of image corruptions to which models trained with these techniques remain vulnerable. Besides dataaugmentation, ensemble methods such as Deep ensembles and Bayesian neural networks have also been shown to improve generalization in the presence of corruptions (Lakshminarayanan et al., 2017; Ovadia et al., 2019; Dusenberry et al., 2020; Trinh et al., 2022). However, the training and inference costs of these methods increase linearly with the number of ensemble members, rendering them less suitable for very large DNNs.

ContributionsIn this work, we show that simply perturbing weights with multiplicative random variables during training can significantly improve robustness to a wide range of corruptions. Our contributions are as follows:

* We show in Section 2 and Fig. 1 that the effects of input corruptions can be simulated during training via multiplicative weight perturbations.
* From this insight, we propose a new training algorithm called Data Augmentation via Multiplicative Perturbations (DAMP) which perturbs weights using multiplicative Gaussian random variables during training while having the same training cost as standard SGD.
* In Section 3, we show a connection between adversarial multiplicative weight perturbations and Adaptive Sharpness-Aware Minimization (ASAM) (Kwon et al., 2021).
* Through a rigorous empirical study in Section 4, we demonstrate that DAMP consistently improves generalization ability of DNNs under corruptions across different image classification datasets and model architectures.
* Notably, we demonstrate that DAMP can train a Vision Transformer (ViT) (Dosovitskiy et al., 2021) from scratch on ImageNet, achieving similar accuracy to a ResNet50 (He et al., 2016) in 200 epochs with only basic Inception-style preprocessing (Szegedy et al., 2016). This is significant as ViT typically requires advanced training methods or sophisticated data augmentation to match ResNet50's performance when being trained on ImageNet from scratch (Chen et al., 2022; Beyer et al., 2022). We also show that DAMP can be combined with modern augmentation techniques such as MixUp (Zhang et al., 2018) and RandAugment (Cubuk et al., 2020) to further improve robustness of neural networks.

## 2 Data Augmentation via Multiplicative Perturbations

In this section, we demonstrate the equivalence between input corruptions and multiplicative weight perturbations (MWPs), as shown in Fig. 1, motivating the use of MWPs for data augmentation.

Figure 1: **Depictions of a pre-activation neuron \(z=\mathbf{w}^{\top}\mathbf{x}\) in the presence of (a) covariate shift \(\epsilon\), (b) a multiplicative weight perturbation (MWP) equivalent to \(\epsilon\), and (c) random MWPs \(\xi\). \(\circ\) denotes the Hadamard product. Figs. (a) and (b) show that for a covariate shift \(\epsilon\), one can always find an equivalent MWP. From this intuition, we propose to inject random MWPs \(\xi\) to the forward pass during training as shown in Fig. (c) to robustify a DNN to covariate shift.**

### Problem setting

Given a training data set \(\mathcal{S}=\{(\mathbf{x}_{k},y_{k})\}_{k=1}^{N}\subseteq\mathcal{X}\times\mathcal{Y}\) drawn i.i.d. from the data distribution \(\mathcal{D}\), we seek to learn a model that generalizes well on both clean and corrupted inputs. We denote \(\mathcal{G}\) as a set of functions whose each member \(\mathbf{g}:\mathcal{X}\rightarrow\mathcal{X}\) represents an input corruption. That is, for each \(\mathbf{x}\in\mathcal{X}\), \(\mathbf{g}(\mathbf{x})\) is a corrupted version of \(\mathbf{x}\).2 We define \(\mathbf{g}(\mathcal{S}):=\{(\mathbf{g}(\mathbf{x}_{k}),y_{k})\}_{k=1}^{N}\) as the training set corrupted by \(\mathbf{g}\). We consider a DNN \(\mathbf{f}:\mathcal{X}\rightarrow\mathcal{Y}\) parameterized by \(\bm{\omega}\in\mathcal{W}\). Given a per-sample loss \(\ell:\mathcal{W}\times\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}_{+}\), the training loss is defined as the average loss over the samples \(\mathcal{L}(\bm{\omega};\mathcal{S}):=\frac{1}{N}\sum_{k=1}^{N}\ell(\bm{ \omega},\mathbf{x}_{k},y_{k})\). Our goal is to find \(\bm{\omega}\) which minimizes:

Footnote 2: For instance, if \(\mathbf{x}\) is a clean image then \(\mathbf{g}(\mathbf{x})\) could be \(\mathbf{x}\) corrupted by _Gaussian noise_.

\[\mathcal{L}(\bm{\omega};\mathcal{G}(\mathcal{S})):=\mathbb{E}_{\mathbf{g} \sim\mathcal{G}}[\mathcal{L}(\bm{\omega};\mathbf{g}(\mathcal{S}))]\] (1)

without knowing exactly the types of corruption contained in \(\mathcal{G}\). This problem is crucial for the reliable deployment of DNNs, especially in safety-critical systems, since it is difficult to anticipate all potential types of corruption the model might encounter in production.

### Multiplicative weight perturbations simulate input corruptions

To address the problem above, we make two key assumptions about the corruptions in \(\mathcal{G}\):

**Assumption 1** (Bounded corruption).: _For each corruption function \(\mathbf{g}:\mathcal{X}\rightarrow\mathcal{X}\) in \(\mathcal{G}\), there exists a constant \(M>0\) such that \(\|\mathbf{g}(\mathbf{x})-\mathbf{x}\|_{2}\leq M\) for all \(\mathbf{x}\in\mathcal{X}\)._

**Assumption 2** (Transferable robustness).: _A model's robustness to corruptions in \(\mathcal{G}\) can be indirectly enhanced by improving its resilience to a more easily simulated set of input perturbations._

Assumption 1 implies that the corrupted versions of an input \(\mathbf{x}\) must be constrained within a bounded neighborhood of \(\mathbf{x}\) in the input space. Assumption 2 is corroborated by Rusak et al. (2020), who demonstrated that distorting training images with Gaussian noise improves a DNN's performance against various types of corruption. We further validate this observation for corruptions beyond Gaussian noise in Section 4.1. However, Section 4.1 also reveals that using corruptions as data augmentation degrades model performance on clean images. Consequently, we need to identify a method that efficiently simulates diverse input corruptions during training, thereby robustifying a DNN against a wide range of corruptions without compromising its performance on clean inputs.

One such method involves injecting random multiplicative weight perturbations (MWPs) into the forward pass of DNNs during training. The intuition behind this approach is illustrated in Fig. 1. Essentially, for a pre-activated neuron \(z=\mathbf{w}^{\top}\mathbf{x}\) in a DNN, given a corruption causing a covariate shift \(\bm{\epsilon}\) in the input \(\mathbf{x}\), Figs. 0(a) and 0(b) show that one can always find an equivalent MWP \(\bm{\xi}(\bm{\epsilon},\mathbf{x})\):

\[z=\mathbf{w}^{\top}(\mathbf{x}+\bm{\epsilon})=(\mathbf{w}\circ\bm{\xi}(\bm{ \epsilon},\mathbf{x}))^{\top}\mathbf{x},\quad\bm{\xi}(\bm{\epsilon},\mathbf{x} )=1+\bm{\epsilon}/\mathbf{x}\] (2)

where \(\circ\) denotes the Hadamard product. This observation suggests that input corruptions can be simulated during training by injecting random MWPs into the forward pass, as depicted in Fig. 0(c), resulting in a model more robust to corruption. We thus move the problem of simulating corruptions from the input space to the weight space.

Here we provide theoretical arguments supporting the usage of MWPs to robustify DNNs. To this end, we study how corruption affects training loss. We consider a feedforward neural network \(\mathbf{f}(\mathbf{x};\bm{\omega})\)

Figure 2: **Depiction of how a corruption \(\mathbf{g}\) affects the output of a DNN.** Here \(\mathbf{x_{g}}=\mathbf{g}(\mathbf{x})\). The corruption \(\mathbf{g}\) creates a shift \(\bm{\delta_{\mathbf{g}}\mathbf{x}^{(0)}}=\mathbf{x_{g}}-\mathbf{x}\) in the input \(\mathbf{x}\), which propagates into shifts \(\bm{\delta_{\mathbf{g}}\mathbf{x}^{(h)}}\) in the output of each layer. This will eventually cause a shift in the loss \(\bm{\delta_{\mathbf{g}}\ell}\). This figure explains why the model performance tends to degrade under corruption.

of depth \(H\) parameterized by \(\bm{\omega}=\{\mathbf{W}^{(h)}\}_{h=1}^{H}\in\mathcal{W}\), which we define recursively as follows:

\[\mathbf{f}^{(0)}(\mathbf{x}):=\mathbf{x},\quad\mathbf{z}^{(h)}(\mathbf{x}):= \mathbf{W}^{(h)}\mathbf{f}^{(h-1)}(\mathbf{x}),\quad\mathbf{f}^{(h)}(\mathbf{x }):=\bm{\sigma}^{(h)}(\mathbf{z}^{(h)}(\mathbf{x})),\quad\forall h=1,\dots,H\] (3)

where \(\mathbf{f}(\mathbf{x};\bm{\omega}):=\mathbf{f}^{(H)}(\mathbf{x})\) and \(\bm{\sigma}^{(h)}\) is the non-linear activation of layer \(h\). For brevity, we use \(\mathbf{x}^{(h)}\) and \(\mathbf{x}^{(h)}_{\mathbf{g}}\) as shorthand notations for \(\mathbf{f}^{(h)}(\mathbf{x})\) and \(\mathbf{f}^{(h)}(\mathbf{g}(\mathbf{x}))\) respectively. Given a corruption function \(\mathbf{g}\), Fig. 2 shows that \(\mathbf{g}\) creates a covariate shift \(\bm{\delta}_{\mathbf{g}}\mathbf{x}^{(0)}:=\mathbf{x}^{(0)}_{\mathbf{g}}- \mathbf{x}^{(0)}\) in the input \(\mathbf{x}\) leading to shifts \(\bm{\delta}_{\mathbf{g}}\mathbf{x}^{(h)}:=\mathbf{x}^{(h)}_{\mathbf{g}}- \mathbf{x}^{(h)}\) in the output of each layer. This will eventually cause a shift in the per-sample loss \(\bm{\delta}_{\mathbf{g}}\ell(\omega,\mathbf{x},y):=\ell(\bm{\omega},\mathbf{x }_{\mathbf{g}},y)-\ell(\bm{\omega},\mathbf{x},y)\). The following lemma characterizes the connection between \(\bm{\delta}_{\mathbf{g}}\ell(\bm{\omega},\mathbf{x},y)\) and \(\bm{\delta}_{\mathbf{g}}\mathbf{x}^{(h)}\):

**Lemma 1**.: _For all \(h=1,\dots,H\) and for all \(\mathbf{x}\in\mathcal{X}\), there exists a scalar \(C_{\mathbf{g}}^{(h)}(\mathbf{x})>0\) such that:_

\[\delta_{\mathbf{g}}\ell(\bm{\omega},\mathbf{x},y)\leq\Big{\langle}\nabla_{ \mathbf{z}^{(h+1)}}\ell(\bm{\omega},\mathbf{x},y)\otimes\delta_{\mathbf{g}} \mathbf{x}^{(h)},\mathbf{W}^{(h+1)}\Big{\rangle}_{F}+\frac{C_{\mathbf{g}}^{(h) }(\mathbf{x})}{2}\|\mathbf{W}^{(h)}\|_{F}^{2}\] (4)

Here \(\otimes\) denotes the outer product of two vectors, \(\langle\cdot,\cdot\rangle_{F}\) denotes the Frobenius inner product of two matrices of the same dimension, \(\|\cdot\|_{F}\) is the Frobenius norm, and \(\nabla_{\mathbf{z}^{(h)}}\ell(\bm{\omega},\mathbf{x},y)\) is the Jacobian of the per-sample loss with respect to the pre-activation output \(\mathbf{z}^{(h)}(\mathbf{x})\) at layer \(h\). To prove Lemma 1, we use Assumption 1 and the following assumption about the loss function:

**Assumption 3** (Lipschitz-continuous objective input gradients).: _The input gradient of the per-sample loss \(\nabla_{\mathbf{x}}\ell(\bm{\omega},\mathbf{x},y)\) is Lipschitz continuous._

Assumption 3 allows us to define a quadratic bound of the loss function using a second-order Taylor expansion. The proof of Lemma 1 is provided in Appendix A. Using Lemma 1, we prove Theorem 1, which bounds the training loss in the presence of corruptions using the training loss under multiplicative perturbations in the weight space:

**Theorem 1**.: _For a function \(\mathbf{g}:\mathcal{X}\to\mathcal{X}\) satisfying Assumption 1 and a loss function \(\mathcal{L}\) satisfying Assumption 3, there exists \(\bm{\xi}_{\mathbf{g}}\in\mathcal{W}\) and \(C_{\mathbf{g}}>0\) such that:_

\[\mathcal{L}(\omega;\mathbf{g}(\mathcal{S}))\leq\mathcal{L}(\bm{\omega}\circ \bm{\xi}_{\mathbf{g}};\mathcal{S})+\frac{C_{\mathbf{g}}}{2}\|\bm{\omega}\|_{F} ^{2}\] (5)

We provide the proof of Theorem 1 in Appendix B. This theorem establishes an upper bound for the target loss in Eq. (1):

\[\mathcal{L}(\omega;\mathcal{G}(\mathcal{S}))\leq\mathbb{E}_{\mathbf{g}\sim \mathcal{G}}\left[\mathcal{L}(\bm{\omega}\circ\bm{\xi}_{\mathbf{g}};\mathcal{S}) +\frac{C_{\mathbf{g}}}{2}\|\bm{\omega}\|_{F}^{2}\right]\] (6)

This bound implies that training a DNN using the following loss function:

\[\mathcal{L}_{\bm{\Xi}}(\bm{\omega};\mathcal{S}):=\mathbb{E}_{\bm{\xi}\sim\bm{ \Xi}}\left[\mathcal{L}(\bm{\omega}\circ\bm{\xi};\mathcal{S})\right]+\frac{ \lambda}{2}\|\bm{\omega}\|_{F}^{2}\] (7)

where the expected loss is taken with respect to a distribution \(\bm{\Xi}\) of random MWPs \(\bm{\xi}\), will minimize the upper bound of the loss \(\mathcal{L}(\omega;\widehat{\mathcal{G}}(\mathcal{S}))\) of a hypothetical set of corruptions \(\mathcal{\hat{G}}\) simulated by \(\bm{\xi}\sim\bm{\Xi}\). This approach results in a model robust to these simulated corruptions, which, according to Assumption 2, could indirectly improve robustness to corruptions in \(\mathcal{G}\).

We note that the second term in Eq. (7) is the \(L_{2}\)-regularization commonly used in optimizing DNNs. Based on this proxy loss, we propose Algorithm 1 which minimizes the objective function in Eq. (7) when \(\bm{\Xi}\) is an isotropic Gaussian distribution \(\mathcal{N}(\mathbf{1},\sigma^{2}\mathbf{I})\). We call this algorithm Data Augmentation via Multiplicative Perturbations (DAMP), as it uses random MWPs during training to simulate input corruptions, which can be viewed as data augmentations.

RemarkThe standard method to calculate the expected loss in Eq. (7), which lacks a closed-form solution, is the Monte Carlo (MC) approximation. However, the training cost of this approach scales linearly with the number of MC samples. To match the training cost of standard SGD, Algorithm 1 divides each data batch into \(M\) equal-sized sub-batches (Line 6) and calculates the loss on each sub-batch with different multiplicative noises from the noise distribution \(\bm{\Xi}\) (Lines 7-9). The final gradient is obtained by averaging the sub-batch gradients (Line 11). Algorithm 1 is thus suitable for data parallelism in multi-GPU training, where the data batch is evenly distributed across \(M>1\) GPUs. Compared to SGD, Algorithm 1 requires only two additional operations: generating Gaussian samples and point-wise multiplication, both of which have negligible computational costs. In our experiments, we found that both SGD and DAMP had similar training times.

Adaptive Sharpness-Aware Minimization optimizes DNNs under adversarial multiplicative weight perturbations

In this section, we demonstrate that optimizing DNNs with adversarial MWPs follows a similar update rule to Adaptive Sharpness-Aware Minimization (ASAM) (Kwon et al., 2021). We first provide a brief description of ASAM and its predecessor Sharpness-Aware Minimization (SAM) (Foret et al., 2021):

SamMotivated by previous findings that wide optima tend to generalize better than sharp ones (Keskar et al., 2017; Jiang et al., 2020), SAM regularizes the sharpness of an optimum by solving the following minimax optimization:

\[\min_{\bm{\omega}}\max_{\|\bm{\xi}\|_{2}\leq\rho}\mathcal{L}(\bm{\omega}+\bm{ \xi};\mathcal{S})+\frac{\lambda}{2}\|\bm{\omega}\|_{F}^{2}\] (8)

which can be interpreted as optimizing DNNs under adversarial additive weight perturbations. To efficiently solve this problem, Foret et al. (2021) devise a two-step procedure for each iteration \(t\):

\[\bm{\xi}^{(t)}=\rho\frac{\nabla_{\bm{\omega}}\mathcal{L}(\bm{\omega}^{(t)}; \mathcal{S})}{\big{|}\big{|}\nabla_{\bm{\omega}}\mathcal{L}(\bm{\omega}^{(t)}; \mathcal{S})\big{|}\big{|}_{2}},\qquad\bm{\omega}^{(t+1)}=\bm{\omega}^{(t)}- \eta_{t}\left(\nabla_{\bm{\omega}}\mathcal{L}(\bm{\omega}^{(t)}+\bm{\xi}^{(t) };\mathcal{S})+\lambda\bm{\omega}^{(t)}\right)\] (9)

where \(\eta_{t}\) is the learning rate. Each iteration of SAM thus takes twice as long to run than SGD.

AsamKwon et al. (2021) note that SAM attempts to minimize the maximum loss over a rigid sphere of radius \(\rho\) around an optimum, which is not suitable for ReLU networks since their parameters can be freely re-scaled without affecting the outputs. The authors thus propose ASAM as an alternative optimization problem to SAM which regularizes the _adaptive sharpness_ of an optimum:

\[\min_{\bm{\omega}}\max_{\|T_{\bm{\omega}}^{-1}\bm{\xi}\|_{2}\leq\rho}\mathcal{ L}(\bm{\omega}+\bm{\xi};\mathcal{S})+\frac{\lambda}{2}\|\bm{\omega}\|_{F}^{2}\] (10)

where \(T_{\bm{\omega}}\) is an invertible linear operator used to reshape the perturbation region (so that it is not necessarily a sphere as in SAM). Kwon et al. (2021) found that \(T_{\bm{\omega}}=|\bm{\omega}|\) produced the best results. Solving Eq. (10) in this case leads to the following two-step procedure for each iteration \(t\):

\[\widehat{\bm{\xi}}^{(t)}=\rho\frac{\big{(}\bm{\omega}^{(t)}\big{)}^{2}\circ \nabla_{\bm{\omega}}\mathcal{L}(\bm{\omega}^{(t)};\mathcal{S})}{\big{|}\big{|} \bm{\omega}^{(t)}\circ\nabla_{\bm{\omega}}\mathcal{L}(\bm{\omega}^{(t)}; \mathcal{S})\big{|}\big{|}_{2}},\quad\bm{\omega}^{(t+1)}=\bm{\omega}^{(t)}- \eta_{t}\left(\nabla_{\bm{\omega}}\mathcal{L}(\bm{\omega}^{(t)}+\widehat{\bm{ \xi}}^{(t)};\mathcal{S})+\lambda\bm{\omega}^{(t)}\right)\] (11)

Similar to SAM, each iteration of ASAM also takes twice as long to run than SGD.

ASAM and adversarial multiplicative perturbationsAlgorithm 1 minimizes the expected loss in Eq. (7). Instead, we could minimize the loss under the adversarial MWP:

\[\mathcal{L}_{\max}(\bm{\omega};\mathcal{S}):=\max_{\|\bm{\xi}\|_{2}\leq\rho} \mathcal{L}(\bm{\omega}+\bm{\omega}\circ\bm{\xi};\mathcal{S})+\frac{\lambda}{2} \|\bm{\omega}\|_{F}^{2}\] (12)

Following Foret et al. (2021), we solve this optimization problem by using a first-order Taylor expansion of \(\mathcal{L}(\bm{\omega}+\bm{\omega}\circ\bm{\xi};\mathcal{S})\) to find an approximate solution of the inner maximization:

\[\operatorname*{arg\,max}_{\|\bm{\xi}\|_{2}\leq\rho}\mathcal{L}(\bm{\omega}+\bm {\omega}\circ\bm{\xi};\mathcal{S})\approx\operatorname*{arg\,max}_{\|\bm{\xi} \|_{2}\leq\rho}\mathcal{L}(\bm{\omega};\mathcal{S})+\langle\bm{\omega}\circ\bm {\xi},\nabla_{\bm{\omega}}\mathcal{L}(\bm{\omega};\mathcal{S})\rangle\] (13)

The maximizer of the Taylor expansion is:

\[\widehat{\bm{\xi}}(\bm{\omega})=\rho\frac{\bm{\omega}\circ\nabla_{\bm{\omega} }\mathcal{L}(\bm{\omega};\mathcal{S})}{\|\bm{\omega}\circ\nabla_{\bm{\omega} }\mathcal{L}(\bm{\omega};\mathcal{S})\|_{2}}\] (14)

Substituting back into Eq. (12) and differentiating, we get:

\[\nabla_{\omega}\mathcal{L}_{\max}(\bm{\omega};\mathcal{S}) \approx\nabla_{\omega}\mathcal{L}(\widehat{\bm{\omega}};\mathcal{ S})+\lambda\bm{\omega}=\nabla_{\omega}\widehat{\bm{\omega}}\cdot\nabla_{\widehat{ \bm{\omega}}}\mathcal{L}(\widehat{\bm{\omega}};\mathcal{S})+\lambda\bm{\omega}\] (15) \[=\nabla_{\widehat{\bm{\omega}}}\mathcal{L}(\widehat{\bm{\omega}} ;\mathcal{S})+\nabla_{\omega}\left(\omega\circ\widehat{\bm{\xi}}(\omega) \right)\cdot\nabla_{\widehat{\bm{\omega}}}\mathcal{L}(\widehat{\bm{\omega}}; \mathcal{S})+\lambda\bm{\omega}\] (16)

where \(\widehat{\bm{\omega}}\) is the perturbed weight:

\[\widehat{\bm{\omega}}=\bm{\omega}+\bm{\omega}\circ\widehat{\bm{\xi}}(\bm{ \omega})=\bm{\omega}+\rho\frac{\bm{\omega}^{2}\circ\nabla_{\omega}\mathcal{L} (\bm{\omega};\mathcal{S})}{\|\bm{\omega}\circ\nabla_{\bm{\omega}}\mathcal{L} (\bm{\omega};\mathcal{S})\|_{2}}\] (17)

Similar to Foret et al. (2021), we omit the second summand in Eq. (16) for efficiency, as it requires calculating the Hessian of the loss. We then arrive at the gradient formula in the update rule of ASAM in Eq. (11). We have thus established a connection between ASAM and adversarial MWPs.

## 4 Empirical evaluation

In this section, we assess the corruption robustness of DAMP and ASAM in image classification tasks. We conduct experiments using the CIFAR-10/100 (Krizhevsky, 2009), TinyImageNet (Le and Yang, 2015), and ImageNet (Deng et al., 2009) datasets. For evaluation on corrupted images, we utilize the CIFAR-10/100-C, TinyImageNet-C, and ImageNet-C datasets provided by Hendrycks and Dietterich (2019), as well as ImageNet-\(\overline{\text{C}}\)(Mitnun et al., 2021), ImageNet-D (Zhang et al., 2024), ImageNet-A (Hendrycks et al., 2021), ImageNet-Sketch (Wang et al., 2019), ImageNet-{Drawing, Cartoon} (Salvador and Oberman, 2022), and ImageNet-Hard (Taesiri et al., 2023) datasets, which encapsulate a wide range of corruptions. Detail descriptions of these datasets are provided in Appendix E. We further evaluate the models on adversarial examples generated by the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014). In terms of architectures, we use ResNet18 (He et al., 2016) for CIFAR-10/100, PreActResNet18 (He et al., 2016) for TinyImageNet, ResNet50 (He et al., 2016), ViT-S/16, and ViT-B/16 (Dosovitskiy et al., 2021) for ImageNet. We ran all experiments on a single machine with 8 Nvidia V100 GPUs. Appendix F includes detailed information for each experiment.

### Comparing DAMP to directly using corruptions as augmentations

In this section, we compare the corruption robustness of DNNs trained using DAMP with those trained directly on corrupted images. To train models on corrupted images, we utilize Algorithm 2 described in the Appendix. For a given target corruption \(\mathbf{g}\), Algorithm 2 randomly selects half the images in each training batch and applies \(\mathbf{g}\) to them. This random selection process enhances the final model's robustness to the target corruption while maintaining its accuracy on clean images. We use the imagecorruptions library (Michaelis et al., 2019) to apply the corruptions during training.

Evaluation metricWe use the corruption error \(\operatorname{CE}_{c}^{f}\)(Hendrycks and Dietterich, 2019) which measures the predictive error of classifier \(f\) in the presence of corruption \(c\). Denote \(E_{s,c}^{f}\) as the error of classifier \(f\) under corruption \(c\) with corruption severity \(s\), the corruption error \(\operatorname{CE}_{c}^{f}\) is defined as:

\[\operatorname{CE}_{c}^{f}=\left(\sum_{s=1}^{5}E_{s,c}^{f}\right)\bigg{/} \bigg{(}\sum_{s=1}^{5}E_{s,c}^{f_{\text{feature}}}\bigg{)}\] (18)

[MISSING_PAGE_FAIL:7]

most corruption scenarios, even though SAM takes twice as long to train and has higher accuracy on clean images. Additionally, DAMP improves accuracy on clean images over Dropout on CIFAR-100 and TinyImageNet. Finally, ASAM consistently surpasses other methods on both clean and corrupted images, as it employs adversarial MWPs (Section 3). However, like SAM, each ASAM experiment takes twice as long as DAMP given the same epoch counts.

ResNet50 / ImageNetTable 1 presents the predictive errors for the ResNet50 / ImageNet setting on a variety of corruption test sets. It shows that DAMP consistently outperforms the baselines in most corruption scenarios and on average, despite having half the training cost of SAM and ASAM.

ViT-S16 / ImageNet / Basic augmentationsTable 2 presents the predictive errors for the ViT-S16 / ImageNet setting, using the training setup from Beyer et al. (2022) but with only basic Inception-style preprocessing (Szegedy et al., 2016). Remarkably, DAMP can train ViT-S16 from scratch in 200 epochs to match ResNet50's accuracy without advanced data augmentation. This is significant as ViT typically requires either extensive pretraining (Dosovitskiy et al., 2021), comprehensive data augmentation (Beyer et al., 2022), sophisticated training techniques (Chen et al., 2022), or modifications to the original architecture (Yuan et al., 2021) to perform well on ImageNet. Additionally, DAMP consistently ranks in the top 2 for corruption robustness across various test settings and has the best corruption robustness on average (last column). Comparing Tables 1 and 2 reveals that ViT-S16 is more robust to corruptions than ResNet50 when both have similar performance on clean images.

ViT / ImageNet / Advanced augmentationsTable 3 presents the predictive errors of ViT-S16 and ViT-B16 on ImageNet with MixUp (Zhang et al., 2018) and RandAugment (Cubuk et al., 2020). These results indicate that DAMP can be combined with modern augmentation techniques to further improve robustness. Furthermore, using DAMP to train a larger model (ViT-B16) yields better results than using SAM/ASAM to train a smaller model (ViT-S16), given the same amount of training time.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c}{Clean} & \multicolumn{3}{c}{Corrupted Error (\%) \(\downarrow\)} \\  & Error (\%) & \multicolumn{1}{c}{FGSM} & \multicolumn{1}{c}{A} & \multicolumn{1}{c}{C} & \multicolumn{1}{c}{\(\overline{\mathbf{c}}\)} & \multicolumn{1}{c}{Cartoon} & \multicolumn{1}{c}{D} & \multicolumn{1}{c}{Drawing} & \multicolumn{1}{c}{Sketch} & \multicolumn{1}{c}{Hard} & \multicolumn{1}{c}{Avg} \\ \hline Dropout & \(23.6_{\pm 0.2}\) & \(90.7_{\pm 0.1}\) & \(95.7_{\pm 0.1}\) & \(61.7_{\pm 0.2}\) & \(61.6_{\pm 0.1}\) & \(49.6_{\pm 0.2}\) & \(88.9_{\pm 0.1}\) & \(77.4_{\pm 0.3}\) & \(78.3_{\pm 0.3}\) & \(85.8_{\pm 0.1}\) & \(76.6\) \\ DAMP & \(23.8_{\pm 0.1}\) & \(\mathbf{88.3_{\pm 0.1}}\) & \(96.2_{\pm 0.1}\) & \(\mathbf{58.6_{\pm 0.1}}\) & \(\mathbf{58.7_{\pm 0.1}}\) & \(\mathbf{44.4_{\pm 0.1}}\) & \(88.7_{\pm 0.1}\) & \(\mathbf{71.1_{\pm 0.5}}\) & \(\mathbf{76.3_{\pm 0.2}}\) & \(85.3_{\pm 0.2}\) & \(\mathbf{74.2}\) \\ SAM & \(23.2_{\pm 0.1}\) & \(90.4_{\pm 0.2}\) & \(96.6_{\pm 0.1}\) & \(60.2_{\pm 0.2}\) & \(60.7_{\pm 0.1}\) & \(47.6_{\pm 0.1}\) & \(\mathbf{88.3_{\pm 0.1}}\) & \(74.8_{\pm 0.1}\) & \(77.5_{\pm 0.1}\) & \(85.8_{\pm 0.3}\) & \(75.8\) \\ ASAM & \(\mathbf{22.8}_{\pm 0.1}\) & \(89.7_{\pm 0.2}\) & \(96.8_{\pm 0.1}\) & \(58.9_{\pm 0.1}\) & \(59.2_{\pm 0.1}\) & \(45.5_{\pm 0.1}\) & \(88.7_{\pm 0.1}\) & \(72.3_{\pm 0.1}\) & \(76.4_{\pm 0.2}\) & \(\mathbf{85.2_{\pm 0.1}}\) & \(74.7\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **DAMP surpasses the baselines on corrupted images in most cases and on average. We report the predictive errors (lower is better) averaged over 3 seeds for the ResNet50 / ImageNet experiments. Subscript numbers represent standard deviations. We evaluate the models on IN-{C, \(\overline{\mathbf{c}}\), A, D, Sketch, Drawing, Cartoon, Hard}, and adversarial examples generated by FGSM. For FGSM, we use \(\epsilon=2/224\). For IN-{C, \(\overline{\mathbf{c}}\)}, we report the results averaged over all corruption types and severity levels. We use 90 epochs and the basic Inception-style preprocessing for all experiments.**

Figure 4: **DAMP surpasses SAM on corrupted images in most cases, despite requiring only half the training cost. We report the predictive errors (lower is better) averaged over 5 seeds. A severity level of \(0\) indicates no corruption. We use the same number of epochs for all methods.**

## 5 Related works

DropoutPerhaps most relevant to our method is Dropout (Srivastava et al., 2014) and its many variants, such as DropConnect (Wan et al., 2013) and Variational Dropout (Kingma et al., 2015). These methods can be viewed as DAMP where the noise distribution \(\bm{\Xi}\) is a structured multivariate Bernoulli distribution. For instance, Dropout multiplies all the weights connecting to a node with a binary random variable \(p\sim\text{Bernoulli}(\rho)\). While the main motivation of these Dropout methods is to prevent co-adapitations of neurons to improve generalization on clean data, the motivation of DAMP is to improve robustness to input corruptions without harming accuracy on clean data. Nonetheless, our experiments show that DAMP can improve generalization on clean data in certain scenarios, such as PreActResNet18/TinyImageNet and ViT-S16/ImageNet.

Ensemble methodsEnsemble methods, such as Deep ensembles (Lakshminarayanan et al., 2017) and Bayesian neural networks (BNNs) (Graves, 2011; Blundell et al., 2015; Gal and Ghahramani, 2016; Louizos and Welling, 2017; Izmailov et al., 2021; Trinh et al., 2022), have been explored as effective defenses against corruptions. Ovadia et al. (2019) benchmarked some of these methods, demonstrating that they are more robust to corruptions compared to a single model. However, the training and inference costs of these methods increase linearly with the number of ensemble members, making them inefficient for use with very large DNNs.

Data augmentationData augmentations aim at enhancing robustness include AugMix (Hendrycks et al., 2019), which combines common image transformations; Patch Gaussian (Lopes et al., 2019), which applies Gaussian noise to square patches; ANT (Rusak et al., 2020), which uses adversarially learned noise distributions for augmentation; and AutoAugment (Cubuk et al., 2018), which learns

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{\#Epochs} & \multirow{2}{*}{Runtime} & \multicolumn{4}{c}{Clean} & \multicolumn{4}{c}{Corrupted Error (\%) \(\downarrow\)} \\  & & & Error (\%) \(\downarrow\) & FGSM & A & C & \(\overline{C}\) & Cartoon & D & Drawing & Sketch & Hard & Avg \\ \hline \multirow{2}{*}{Dropout} & 100 & 20.6h & 28.55 & 93.47 & 93.44 & 65.87 & 64.52 & 50.37 & 91.15 & 79.62 & 88.06 & 87.19 & 79.30 \\  & 200 & 41.1h & 28.74 & 90.95 & 93.33 & 66.90 & 64.83 & 51.23 & 92.56 & 81.24 & 87.99 & 87.60 & 79.63 \\ \hline \multirow{2}{*}{DAMP} & 100 & 20.7h & 25.50 & 92.76 & 92.92 & 57.85 & 57.02 & 44.78 & 88.79 & 69.92 & 83.16 & 85.65 & 74.76 \\  & 200 & 41.1h & **23.75** & **84.33** & **90.56** & 55.58 & **55.58** & 41.06 & **87.87** & 68.36 & 81.82 & **84.18** & **72.15** \\ \hline SAM & 100 & 41h & 23.91 & 87.61 & 93.96 & 55.56 & 55.93 & 42.53 & 88.23 & 69.53 & 81.86 & 85.54 & 73.42 \\ ASAM & 100 & 41.1h & 24.01 & 85.85 & 92.99 & **55.13** & 55.64 & **40.74** & 89.03 & **67.80** & **81.47** & 84.31 & 72.55 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **ViT-S16 / ImageNet (IN) with basic Inception-style data augmentations**. Due to the high training cost, we report the predictive error (lower is better) of a single run. We evaluate corruption robustness of the models using IN-{C, \(\overline{C}\), A, D, Sketch, Drawing, Cartoon, Hard}, and adversarial examples generated by FGSM. For IN-{C, \(\overline{C}\)}, we report the results averaged over all corruption types and severity levels. For FGSM, we use \(\epsilon=2/224\). We also report the runtime of each experiment, showing that SAM and ASAM take twice as long to run than DAMP and AdamW given the same number of epochs. DAMP produces the most robust model on average.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Method} & \multirow{2}{*}{\#Epochs} & \multirow{2}{*}{Runtime} & \multicolumn{4}{c}{Clean} & \multicolumn{4}{c}{Corrupted Error (\%) \(\downarrow\)} \\  & & & Error (\%) \(\downarrow\) & FGSM & \(\overline{C}\) & A & C & Cartoon & D & Drawing & Sketch & Hard & Avg \\ \hline \multirow{2}{*}{ViT} & Dropout & 500 & 111h & 20.25 & 62.45 & 40.85 & 84.29 & 44.72 & 34.35 & 86.59 & 56.31 & 71.03 & 80.87 & 62.38 \\  & DAMP & 500 & 111h & **20.09** & 59.87 & **39.30** & **81.21** & **43.18** & 34.01 & **84.74** & **54.16** & **68.03** & **80.05** & **60.72** \\ \multirow{2}{*}{S16} & SAM & 300 & 123h & 20.17 & 59.92 & 40.05 & 83.91 & 44.04 & 34.34 & 85.99 & 55.63 & 70.85 & 80.18 & 61.66 \\  & ASAM & 300 & 123h & 20.38 & **59.38** & 39.44 & 83.64 & 43.41 & **33.82** & 85.41 & 54.43 & 69.13 & 80.50 & 61.02 \\ \hline \multirow{2}{*}{ViT} & Dropout & 275 & 123h & 20.41 & 56.43 & 39.14 & 82.85 & 43.82 & 33.13 & 87.72 & 56.15 & 71.36 & 79.13 & 61.08 \\  & DAMP & 275 & 124h & **19.36** & **55.20** & 37.87 & **80.49** & 41.67 & 31.63 & **87.06** & 52.32 & **67.91** & **78.69** & **91.99** \\ \multirow{2}{*}{B16} & SAM & 150 & 135h & 19.84 & 61.85 & 39.09 & 82.69 & 43.53 & 32.95 & 88.38 & 55.33 & 71.22 & 79.48 & 61.61 \\ \multirow{2}{*}{ASAM} & 150 & 136h & 19.40 & 58.87 & **37.41** & 82.21 & **41.18** & **30.76** & 88.03 & **51.84** & 69.54 & 78.83 & 59.85 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **ViT / ImageNet (IN) with MixUp and RandAugment**. We train ViT-S16 and ViT-B16 on ImageNet from scratch with advanced data augmentations (DAs). We evaluate the models on IN-{C, \(\overline{C}\), A, D, Sketch, Drawing, Cartoon, Hard}, and adversarial examples generated by FGSM. For FGSM, we use \(\epsilon=2/224\). For IN-{C, \(\overline{C}\)}, we report the results averaged over all corruption types and severity levels. These results indicate that: (i) DAMP can be combined with modern DA techniques to further enhance robustness; (ii) DAMP is capable of training large models like ViT-B16; (iii) given the same amount of training time, it is better to train a large model (ViT-B16) using DAMP than to train a smaller model (ViT-S16) using SAM/ASAM.

augmentation policies directly from the training data. These methods have been demonstrated to improve robustness to the corruptions in ImageNet-C (Hendrycks and Dietterich, 2019). Mintun et al. (2021) attribute the success of these methods to the fact that they generate augmented images perceptually similar to the corruptions in ImageNet-C and propose ImageNet-\(\overline{\text{C}}\), a test set of 10 new corruptions that are challenging to models trained by these augmentation methods.

Test-time adaptations via BatchNormOne effective approach to using unlabelled data to improve corruption robustness is to keep BatchNorm (Ioffe and Szegedy, 2015) on at test-time to adapt the batch statistics to the corrupted test data (Li et al., 2016; Nado et al., 2020; Schneider et al., 2020; Benz et al., 2021). A major drawback is that this approach cannot be used with BatchNorm-free architectures, such as Vision Transformer (Dosovitskiy et al., 2021).

## 6 Conclusion

In this work, we demonstrate that MWPs improve robustness of DNNs to a wide range of input corruptions. We introduce DAMP, a simple training algorithm that perturbs weights during training with random multiplicative noise while maintaining the same training cost as standard SGD. We further show that ASAM (Kwon et al., 2021) can be viewed as optimizing DNNs under adversarial MWPs. Our experiments show that both DAMP and ASAM indeed produce models that are robust to corruptions. DAMP is also shown to improve sample efficiency of Vision Transformer, allowing it to achieve comparable performance to ResNet50 on medium size datasets such as ImageNet without extensive data augmentations. Additionally, DAMP can be used in conjunction with modern augmentation techniques such as MixUp and RandAugment to further boost robustness. As DAMP is domain-agnostic, one future direction is to explore its effectiveness in domains other than computer vision, such as natural language processing and reinforcement learning. Another direction is to explore alternative noise distributions to the Gaussian distribution used in our work.

LimitationsHere we outline some limitations of this work. First, the proof of Theorem 1 assumes a simple feedforward neural network, thus it does not take into accounts modern DNN's components such as normalization layers and attentions. Second, we only explored random Gaussian multiplicative perturbations, and there are likely more sophisticated noise distributions that could further boost corruption robustness.

## Broader Impacts

Our paper introduces a new training method for neural networks that improves their robustness to input corruptions. Therefore, we believe that our work contributes towards making deep leading models safer and more reliable to use in real-world applications, especially those that are safety-critical. However, as with other methods that improve robustness, our method could also be improperly used in applications that negatively impact society, such as making mass surveillance systems more accurate and harder to fool. To this end, we hope that practitioners carefully consider issues regarding fairness, bias and other potentially harmful societal impacts when designing deep learning applications.

## Acknowledgments

This work was supported by the Research Council of Finland (Flagship programme: Finnish Center for Artificial Intelligence FCAI and decision no. 359567, 345604 and 341763), ELISE Networks of Excellence Centres (EU Horizon: 2020 grant agreement 951847) and UKRI Turing AI World-Leading Researcher Fellowship (EP/W002973/1). We acknowledge the computational resources provided by Aalto Science-IT project and CSC-IT Center for Science, Finland.

## References

* Hendrycks and Dietterich (2019) Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _arXiv preprint arXiv:1903.12261_, 2019.
* Amodei et al. (2016) Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in ai safety. _arXiv preprint arXiv:1606.06565_, 2016.
* Amodei et al. (2016)Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schutt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. _Advances in neural information processing systems_, 31, 2018.
* Cubuk et al. (2018) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. _arXiv preprint arXiv:1805.09501_, 2018.
* Hendrycks et al. (2019) Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. _arXiv preprint arXiv:1912.02781_, 2019.
* Lopes et al. (2019) Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin D Cubuk. Improving robustness without sacrificing accuracy with patch gaussian augmentation. _arXiv preprint arXiv:1906.02611_, 2019.
* Mintun et al. (2021) Eric Mintun, Alexander Kirillov, and Saining Xie. On interaction between augmentations and corruptions in natural corruption robustness. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=LOHyqjfyra.
* Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.
* Ovadia et al. (2019) Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf.
* Dusenberry et al. (2020) Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek, Katherine Heller, Balaji Lakshminarayanan, and Dustin Tran. Efficient and scalable Bayesian neural nets with rank-1 factors. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 2782-2792. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/dusenberry20a.html.
* Trinh et al. (2022) Trung Trinh, Markus Heinonen, Luigi Acerbi, and Samuel Kaski. Tackling covariate shift with node-based Bayesian neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 21751-21775. PMLR, 17-23 Jul 2022.
* Kwon et al. (2021) Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In _International Conference on Machine Learning_, pages 5905-5914. PMLR, 2021.
* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=YicbFdNTTY.
* He et al. (2016a) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE conference on Computer Vision and Pattern Recognition_, 2016a.
* Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2818-2826, 2016.
* Chen et al. (2022) Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets without pre-training or strong data augmentations. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=LtKcMgG0eLt.

Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Better plain vit baselines for imagenet-1k. _arXiv preprint arXiv:2205.01580_, 2022.
* Zhang et al. (2018) Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=r1Ddp1-Rb.
* Cubuk et al. (2020) Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_, pages 702-703, 2020.
* Rusak et al. (2020) Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. _arXiv preprint arXiv:2001.06057_, 2020.
* Foret et al. (2021) Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=6TmlpmoslrM.
* Keskar et al. (2017) Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=HloyRlYgg.
* Jiang et al. (2020) Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=SJgIPJBFvH.
* Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
* Le and Yang (2015) Ya Le and Xuan S. Yang. Tiny ImageNet visual recognition challenge. 2015.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _IEEE Conference on Computer Vision and Pattern Recognition_, pages 248-255, 2009. doi: 10.1109/CVPR.2009.5206848.
* Zhang et al. (2024) Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, and Chengzhi Mao. Imagenet-d: Benchmarking neural network robustness on diffusion synthetic object. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 21752-21762, June 2024.
* Hendrycks et al. (2021) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 15262-15271, June 2021.
* Wang et al. (2019) Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/3eefceb8087e964f89c2d59e8a249915-Paper.pdf.
* Salvador and Oberman (2022) Tiago Salvador and Adam M Oberman. Imagenet-cartoon and imagenet-drawing: two domain shift datasets for imagenet. In _ICML 2022 Shift Happens Workshop_, 2022. URL https://openreview.net/forum?id=YlAUXhjwaQt.
* Taesiri et al. (2023) Mohammad Reza Taesiri, Giang Nguyen, Sarra Habchi, Cor-Paul Bezemer, and Anh Nguyen. Imagenet-hard: The hardest images remaining from a study of the power of zoom and spatial biases in image classification. 2023.
* Goodfellow et al. (2014) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In _European Conference on Computer Vision_, 2016b.
* He et al. (2017)Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S. Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking robustness in object detection: Autonomous driving when winter is coming. _arXiv preprint arXiv:1907.07484_, 2019.
* Srivastava et al. (2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. _Journal of Machine Learning Research_, 15(56):1929-1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html.
* Yuan et al. (2021) Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 558-567, 2021.
* Wan et al. (2013) Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Sanjoy Dasgupta and David McAllester, editors, _Proceedings of the 30th International Conference on Machine Learning_, volume 28 of _Proceedings of Machine Learning Research_, pages 1058-1066, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL https://proceedings.mlr.press/v28/wan13.html.
* Kingma et al. (2015) Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. _Advances in neural information processing systems_, 28, 2015.
* Graves (2011) Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 24. Curran Associates, Inc., 2011. URL https://proceedings.neurips.cc/paper_files/paper/2011/file/7eb3c8be3d411e8ebfab08eb45f49632-Paper.pdf.
* Blundell et al. (2015) Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In _International conference on machine learning_, pages 1613-1622. PMLR, 2015.
* Gal and Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 1050-1059, New York, New York, USA, 20-22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/gal16.html.
* Louizos and Welling (2017) Christos Louizos and Max Welling. Multiplicative normalizing flows for variational bayesian neural networks. In _International Conference on Machine Learning_, pages 2218-2227. PMLR, 2017.
* Izmailov et al. (2021) Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, and Andrew G Wilson. Dangers of bayesian model averaging under covariate shift. _Advances in Neural Information Processing Systems_, 34:3309-3322, 2021.
* Volume 37_, ICML'15, page 448-456. JMLR.org, 2015.
* Li et al. (2016) Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. _arXiv preprint arXiv:1603.04779_, 2016.
* Nado et al. (2020) Zachary Nado, Shreyas Padhy, D Sculley, Alexander D'Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. _arXiv preprint arXiv:2006.10963_, 2020.
* Schneider et al. (2020) Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. _Advances in neural information processing systems_, 33:11539-11551, 2020.
* Benz et al. (2021) Philipp Benz, Chaoning Zhang, Adil Karjauv, and In So Kweon. Revisiting batch normalization for improving corruption robustness. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pages 494-503, 2021.
* Bohning and Lindsay (1988) Dankmar Bohning and Bruce G Lindsay. Monotonicity of quadratic-approximation algorithms. _Annals of the Institute of Statistical Mathematics_, 40(4):641-663, 1988.
* Bringmann et al. (2015)Proof of Lemma 1

Proof.: Here we note that:

\[\mathbf{x}^{(h)} :=\mathbf{f}^{(h)}(\mathbf{x})\] (19) \[\mathbf{x}^{(h)}_{\mathbf{g}} :=\mathbf{f}^{(h)}(\mathbf{g}(\mathbf{x}))\] (20) \[\boldsymbol{\delta}_{\mathbf{g}}\ell(\boldsymbol{\omega},\mathbf{x },y) :=\ell(\boldsymbol{\omega},\mathbf{x}_{\mathbf{g}},y)-\ell( \boldsymbol{\omega},\mathbf{x},y)\] (21) \[\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(h)} :=\mathbf{x}^{(h)}_{\mathbf{g}}-\mathbf{x}^{(h)}\] (22)

We first notice that the per-sample loss \(\ell(\boldsymbol{\omega},\mathbf{x},y)\) can be viewed as a function of the intermediate activation \(\mathbf{x}^{(h)}\) of layer \(h\) (see Fig. 2). From Assumption 3, there exists a constant \(L_{h}>0\) such that:

\[\|\nabla_{\mathbf{x}^{(h)}_{\mathbf{g}}}\ell(\boldsymbol{\omega},\mathbf{x}_{ \mathbf{g}},y)-\nabla_{\mathbf{x}^{(h)}}\ell(\boldsymbol{\omega},\mathbf{x},y )\|_{2}\leq L_{h}\|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(h)}\|_{2}\] (23)

which gives us the following quadratic bound:

\[\ell(\boldsymbol{\omega},\mathbf{x}_{\mathbf{g}},y)\leq\ell(\boldsymbol{ \omega},\mathbf{x},y)+\left\langle\nabla_{\mathbf{x}^{(h)}}\ell(\boldsymbol{ \omega},\mathbf{x},y),\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(h)}\right \rangle+\frac{L_{h}}{2}\|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(h)}\|_{2} ^{2}\] (24)

where \(\langle\cdot,\cdot\rangle\) denotes the dot product between two vectors. The results in the equation above have been proven in Bohning and Lindsay (1988). Subtracting \(\ell(\boldsymbol{\omega},\mathbf{x},y)\) from both side of Eq. (24) gives us:

\[\boldsymbol{\delta}_{\mathbf{g}}\ell(\boldsymbol{\omega},\mathbf{x},y)\leq \left\langle\nabla_{\mathbf{x}^{(h)}}\ell(\boldsymbol{\omega},\mathbf{x},y), \boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(h)}\right\rangle+\frac{L_{h}}{2} \|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(h)}\|_{2}^{2}\] (25)

Since the pre-activation output of layer \(h+1\) is \(\mathbf{z}^{(h+1)}(\mathbf{x})=\mathbf{W}^{(h+1)}\mathbf{f}^{(h)}(\mathbf{x}) =\mathbf{W}^{(h+1)}\mathbf{x}^{(h)}\), we can rewrite the inequality above as:

\[\boldsymbol{\delta}_{\mathbf{g}}\ell(\boldsymbol{\omega},\mathbf{x},y)\leq \left\langle\nabla_{\mathbf{z}^{(h+1)}}\ell(\boldsymbol{\omega},\mathbf{x},y) \otimes\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(h)},\mathbf{W}^{(h+1)} \right\rangle_{F}+\frac{L_{h}}{2}\|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{ (h)}\|_{2}^{2}\] (26)

where \(\otimes\) denotes the outer product of two vectors and \(\langle\cdot,\cdot\rangle_{F}\) denotes the Frobenius inner product of two matrices of similar dimension.

From Assumption 1, we have that there exists a constant \(M>0\) such that:

\[\|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(0)}\|_{2}^{2}=\|\mathbf{x}^{(0 )}_{\mathbf{g}}-\mathbf{x}^{(0)}\|_{2}^{2}=\|\mathbf{g}(\mathbf{x})-\mathbf{x }\|_{2}^{2}\leq M\] (27)

Given that \(\mathbf{x}^{(1)}=\boldsymbol{\sigma}^{(1)}\left(\mathbf{W}^{(1)}\mathbf{x}^{(0 )}\right)\), we have:

\[\|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(1)}\|_{2}^{2}=\|\mathbf{x}^{(1 )}_{\mathbf{g}}-\mathbf{x}^{(1)}\|_{2}^{2}\leq\|\mathbf{W}^{(1)}\boldsymbol{ \delta}_{\mathbf{g}}\mathbf{x}^{(0)}\|_{2}^{2}\] (28)

Here we assume that the activate \(\boldsymbol{\sigma}\) satisfies \(\|\boldsymbol{\sigma}(\mathbf{x})-\boldsymbol{\sigma}(\mathbf{y})\|_{2}\leq\| \mathbf{x}-\mathbf{y}\|_{2}\), which is true for modern activation functions such as ReLU. Since \(\|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(0)}\|_{2}^{2}\) is bounded, there exists a constant \(\hat{C}^{(1)}_{\mathbf{g}}(\mathbf{x})\) such that:

\[\|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(1)}\|_{2}^{2}=\|\mathbf{x}^{(1 )}_{\mathbf{g}}-\mathbf{x}^{(1)}\|_{2}^{2}\leq\|\mathbf{W}^{(1)}\boldsymbol{ \delta}_{\mathbf{g}}\mathbf{x}^{(0)}\|_{2}^{2}\leq\frac{\hat{C}^{(1)}_{ \mathbf{g}}(\mathbf{x})}{2}\|\mathbf{W}^{(1)}\|_{F}^{2}\] (29)

where \(\|\cdot\|_{F}\) denotes the Frobenius norm. Similarly, as we have proven that \(\|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(1)}\|_{2}^{2}\) is bounded, there exists a constant \(\hat{C}^{(2)}_{\mathbf{g}}(\mathbf{x})\) such that:

\[\|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(2)}\|_{2}^{2}=\|\mathbf{x}^{(2 )}_{\mathbf{g}}-\mathbf{x}^{(2)}\|_{2}^{2}\leq\|\mathbf{W}^{(2)}\boldsymbol{ \delta}_{\mathbf{g}}\mathbf{x}^{(1)}\|_{2}^{2}\leq\frac{\hat{C}^{(2)}_{ \mathbf{g}}(\mathbf{x})}{2}\|\mathbf{W}^{(2)}\|_{F}^{2}\] (30)

Thus we have proven that for all \(h=1,\ldots,H\), there exists a constant \(\hat{C}^{(h)}_{\mathbf{g}}(\mathbf{x})\) such that:

\[\|\boldsymbol{\delta}_{\mathbf{g}}\mathbf{x}^{(h)}\|_{2}^{2}\leq\frac{\hat{C}^ {(h)}_{\mathbf{g}}(\mathbf{x})}{2}\|\mathbf{W}^{(h)}\|_{F}^{2}\] (31)

By combining Eqs. (26) and (31) and setting \(C^{(h)}_{\mathbf{g}}(\mathbf{x})=L_{h}\hat{C}^{(h)}_{\mathbf{g}}(\mathbf{x})\), we arrive at Eq. (4).

## Appendix B Proof of Theorem 1

Proof.: From Lemma 1, we have for all \(h=0,\ldots,H-1\):

\[\mathcal{L}(\bm{\omega};\mathbf{g}(\mathcal{S}))=\frac{1}{N}\sum_{k =1}^{N}\ell(\bm{\omega},\mathbf{g}(\mathbf{x}_{k}),y_{k})=\frac{1}{N}\sum_{k=1} ^{N}\left(\ell(\bm{\omega},\mathbf{x}_{k},y_{k})+\bm{\delta}_{\mathbf{g}}\ell( \bm{\omega},\mathbf{x}_{k},y_{k})\right)\] (32) \[\leq\mathcal{L}(\bm{\omega};\mathcal{S})+\frac{1}{N}\sum_{k=1}^{ N}\left\langle\nabla_{\mathbf{z}^{(h+1)}}\ell(\bm{\omega},\mathbf{x}_{k},y_{k}) \otimes\bm{\delta}_{\mathbf{g}}\mathbf{x}_{k}^{(h)},\mathbf{W}^{(h+1)} \right\rangle_{F}+\frac{\hat{C}_{\mathbf{g}}^{(h)}}{2}\|\mathbf{W}^{(h)}\|_{F} ^{2}\] (33)

where \(\hat{C}_{\mathbf{g}}^{(h)}=\max_{\mathbf{x}\in\mathcal{S}}C_{\mathbf{g}}^{(h) }(\mathbf{x})\). Since this bound is true for all \(h\), we can take the average:

\[\mathcal{L}(\bm{\omega};\mathbf{g}(\mathcal{S}))\leq\mathcal{L}( \bm{\omega};\mathcal{S})+\frac{1}{H}\sum_{h=1}^{H}\frac{1}{N}\sum_{k=1}^{N} \left\langle\nabla_{\mathbf{z}^{(h)}}\ell(\bm{\omega},\mathbf{x}_{k},y_{k}) \otimes\bm{\delta}_{\mathbf{g}}\mathbf{x}_{k}^{(h-1)},\mathbf{W}^{(h)} \right\rangle_{F}\\ +\frac{C_{\mathbf{g}}}{2}\|\bm{\omega}\|_{F}^{2}\] (34)

where \(C_{\mathbf{g}}=\frac{1}{H}\sum_{h=1}^{H}\hat{C}_{\mathbf{g}}^{(h)}\). The right-hand side of Eq. (34) can be written as:

\[\mathcal{L}(\bm{\omega};\mathcal{S})+\frac{1}{H}\sum_{h=1}^{H} \left\langle\frac{1}{N}\sum_{k=1}^{N}\nabla_{\mathbf{z}^{(h)}}\ell(\bm{\omega },\mathbf{x}_{k},y_{k})\otimes\bm{\delta}_{\mathbf{g}}\mathbf{x}_{k}^{(h-1)}, \mathbf{W}^{(h)}\right\rangle_{F}+\frac{C_{\mathbf{g}}}{2}\|\bm{\omega}\|_{F} ^{2}\] (35) \[=\mathcal{L}(\bm{\omega};\mathcal{S})+\sum_{h=1}^{H}\left\langle \nabla_{\mathbf{W}^{(h)}}\mathcal{L}(\bm{\omega};\mathcal{S}),\mathbf{W}^{(h)} \circ\bm{\xi}^{(h)}(\mathbf{g})\right\rangle_{F}+\frac{C_{\mathbf{g}}}{2}\| \bm{\omega}\|_{F}^{2}\] (36) \[\leq\mathcal{L}(\bm{\omega}+\bm{\omega}\circ\bm{\xi}(\mathbf{g}); \mathcal{S})+\frac{C_{\mathbf{g}}}{2}\|\bm{\omega}\|_{F}^{2}=\mathcal{L}(\bm{ \omega}\circ(1+\bm{\xi}(\mathbf{g}))\,;\mathcal{S})+\frac{C_{\mathbf{g}}}{2} \|\bm{\omega}\|_{F}^{2}\] (37)

where \(\bm{\xi}^{(h)}(\mathbf{g})\) is a matrix of the same dimension as \(\mathbf{W}^{(h)}\) whose each entry is defined as:

\[\left[\bm{\xi}^{(h)}(\mathbf{g})\right]_{i,j}=\frac{1}{H}\frac{\left[\sum_{k= 1}^{N}\nabla_{\mathbf{z}^{(h)}}\ell(\bm{\omega},\mathbf{x}_{k},y_{k})\otimes \bm{\delta}_{\mathbf{g}}\mathbf{x}_{k}^{(h-1)}\right]_{i,j}}{\left[\sum_{k=1}^ {N}\nabla_{\mathbf{z}^{(h)}}\ell(\bm{\omega},\mathbf{x}_{k},y_{k})\otimes \mathbf{x}_{k}^{(h-1)}\right]_{i,j}}\] (38)

The inequality in Eq. (37) is due to the first-order Taylor expansion and the assumption that the training loss is locally convex at \(\bm{\omega}\). This assumption is expected to hold for the final solution but does not necessarily hold for any \(\bm{\omega}\). Eq. (5) is obtained by combining Eq. (34) and Eq. (37). 

## Appendix C Training with corruption

Here we present Algorithm 2 which uses corruptions as data augmentation during training, as well as the experiment results of Section 4.1 for ResNet18/CIFAR-10 and PreActResNet18/TinyImageNet settings in Figs. 5 and 6.

## Appendix D Training with random additive weight perturbations

Here, we present Algorithm 3 used in Section 4.2 which trains DNNs under random additive weight perturbations and Fig. 7 comparing performance between DAMP and DAAP.

## Appendix E Corruption datasets

CIFAR-10/100-C (Hendrycks and Dietterich, 2019)These datasets contain the corrupted versions of the CIFAR-10/100 test sets. They contain 19 types of corruption, each divided into 5 levels of severity.

TinyImageNet-C (Hendrycks and Dietterich, 2019)This dataset contains the corrupted versions of the TinyImageNet test set. It contains 19 types of corruption, each divided into 5 levels of severity.

```
1:Input: training data \(\mathcal{S}=\{(\mathbf{x}_{k},y_{k})\}_{k=1}^{N}\), a neural network \(\mathbf{f}(\cdot;\bm{\omega})\) parameterized by \(\bm{\omega}\in\mathbb{R}^{P}\), number of iterations \(T\), step sizes \(\{\eta_{t}\}_{t=1}^{T}\), batch size \(B\), a corruption \(\mathbf{g}\) such as Gaussian noise, weight decay coefficient \(\lambda\), a loss function \(\mathcal{L}:\mathbb{R}^{P}\rightarrow\mathbb{R}_{+}\).
2:Output: Optimized parameter \(\bm{\omega}^{(T)}\).
3: Initialize parameter \(\bm{\omega}^{(0)}\).
4:for\(t=1\)to\(T\)do
5: Draw a mini-batch \(\mathcal{B}=\{(\mathbf{x}_{k},y_{k})\}_{k=1}^{B}\sim\mathcal{S}\).
6: Divide the mini-batch into two disjoint sub-batches of equal size \(\mathcal{B}_{1}\) and \(\mathcal{B}_{2}\).
7: Apply the corruption \(\mathbf{g}\) to all samples in \(\mathcal{B}_{1}\): \(\mathbf{g}(\mathcal{B}_{1})=\{(\mathbf{g}(\mathbf{x}),y)\}_{(\mathbf{x},y)\in \mathcal{B}_{1}}\).
8: Compute the gradient \(\mathbf{g}=\nabla_{\bm{\omega}}\mathcal{L}(\bm{\omega};\mathbf{g}(\mathcal{B}_ {1})\cup\mathcal{B}_{2})\).
9: Update the weights: \(\bm{\omega}^{(t+1)}=\bm{\omega}^{(t)}-\eta_{t}\left(\mathbf{g}+\lambda\bm{ \omega}^{(t)}\right)\).
10:endfor ```

**Algorithm 2** Training with corruption

ImageNet-C (Hendrycks and Dietterich, 2019)This dataset contains the corrupted versions of the ImageNet validation set, as the labels of the true ImageNet test set was never released. It contains 15 types of corruption, each divided into 5 levels of severity.

Figure 5: **DAMP improves robustness to all corruptions while preserving accuracy on clean images. Results of ResNet18/CIFAR-10 experiments averaged over 3 seeds. The heatmap shows \(\mathrm{CE}_{c}^{f}\) described in Eq. (18), where each row corresponds to a tuple of of training (method, corruption), while each column corresponds to the test corruption. The Avg column shows the average of the results of the previous columns. none indicates no corruption. We use the models trained under the SGD/none setting (first row) as baselines to calculate the \(\mathrm{CE}_{c}^{f}\). The last five rows are the 5 best training corruptions ranked by the results in the Avg column.**

Figure 6: **DAMP improves robustness to all corruptions while preserving accuracy on clean images. Results of PreActResNet18/TinyImageNet experiments averaged over 3 seeds. The heatmap shows \(\mathrm{CE}_{c}^{f}\) described in Eq. (18), where each row corresponds to a tuple of training (method, corruption), while each column corresponds to the test corruption. The Avg column shows the average of the results of the previous columns. none indicates no corruption. We use the models trained under the SGD/none setting (first row) as baselines to calculate the \(\mathrm{CE}_{c}^{f}\). The last five rows are the 5 best training corruptions ranked by the results in the Avg column.**

ImageNet-\(\overline{\mathbf{C}}\)(Mintun et al., 2021)This dataset contains the corrupted versions of the ImageNet validation set, as the labels of the true ImageNet test set was never released. It contains 10 types of corruption, each divided into 5 levels of severity. The types of corruption in ImageNet-\(\overline{\mathbf{C}}\) differ from those in ImageNet-C.

ImageNet-A (Hendrycks et al., 2021)This dataset contains natural adversarial examples, which are real-world, unmodified, and naturally occurring examples that cause machine learning model performance to significantly degrade. The images contain in this dataset, while differ from those in the ImageNet validation set, stills belong to the same set of classes.

ImageNet-D (Zhang et al., 2024)This dataset contains images belong to the classes of ImageNet but they are modified by diffusion models to change the background, material, and texture.

ImageNet-Cartoon and ImageNet-Drawing (Salvador and Oberman, 2022)This dataset contains the drawing and cartoon versions of the images in the ImageNet validation set.

ImageNet-Sketch (Wang et al., 2019)This dataset contains sketch images belonging to the classes of the ImageNet dataset.

ImageNet-Hard (Taesiri et al., 2023)This dataset comprises an array of challenging images, curated from several validation datasets of ImageNet.

Figure 7: **DAMP has better corruption robustness than DAAP. We report the predictive errors (lower is better) averaged over 5 seeds. None indicates no corruption. Mild includes severity levels 1, 2 and 3. Severe includes severity levels 4 and 5. We evaluate DAMP and DAAP under different noise standard deviations \(\sigma\). These results imply that the multiplicative weight perturbations of DAMP are more effective than the additive perturbations of DAAP in improving robustness to corruptions.**

Training details

For each method and each setting, we tune the important hyperparameters (\(\sigma\) for DAMP, \(\rho\) for SAM and ASAM) using \(10\%\) of the training set as validation set.

Cifar-10/100For each setting, we train a ResNet18 for 300 epochs. We use a batch size of 128. We use a learning rate of \(0.1\) and a weight decay coefficient of \(5\times 10^{-4}\). We use SGD with Nesterov momentum as the optimizer with a momentum coefficient of \(0.9\). The learning rate is kept at \(0.1\) until epoch 150, then is linearly annealed to \(0.001\) from epoch 150 to epoch 270, then kept at \(0.001\) for the rest of the training. We use basic data preprocessing, which includes channel-wise normalization, random cropping after padding and random horizontal flipping. On CIFAR-10, we set \(\sigma=0.2\) for DAMP, \(\rho=0.045\) for SAM and \(\rho=1.0\) for ASAM. On CIFAR-100, we set \(\sigma=0.1\) for DAMP, \(\rho=0.06\) for SAM and \(\rho=2.0\) for ASAM. Each method is trained on a single host with 8 Nvidia V100 GPUs where the data batch is evenly distributed among the GPUs at each iteration (data parallelism). This means we use the number of sub-batches \(M=8\) for DAMP.

TinyImageNetFor each setting, we train a PreActResNet18 for 150 epochs. We use a batch size of 128. We use a learning rate of \(0.1\) and a weight decay coefficient of \(2.5\times 10^{-4}\). We use SGD with Nesterov momentum as the optimizer with a momentum coefficient of \(0.9\). The learning rate is kept at \(0.1\) until epoch 75, then is linearly annealed to \(0.001\) from epoch 75 to epoch 135, then kept at \(0.001\) for the rest of the training. We use basic data preprocessing, which includes channel-wise normalization, random cropping after padding and random horizontal flipping. We set \(\sigma=0.2\) for DAMP, \(\rho=0.2\) for SAM and \(\rho=3.0\) for ASAM. Each method is trained on a single host with 8 Nvidia V100 GPUs where the data batch is evenly distributed among the GPUs at each iteration (data parallelism). This means we use the number of sub-batches \(M=8\) for DAMP.

ResNet50 / ImageNetWe train each experiment for 90 epochs. We use a batch size of 2048. We use a weight decay coefficient of \(1\times 10^{-4}\). We use SGD with Nesterov momentum as the optimizer with a momentum coefficient of \(0.9\). We use basic Inception-style data preprocessing, which includes random cropping, resizing to the resolution of \(224\times 224\), random horizontal flipping and channel-wise normalization. We increase the learning rate linearly from \(8\times 10^{-4}\) to \(0.8\) for the first 5 epochs then decrease the learning rate from \(0.8\) to \(8\times 10^{-4}\) using a cosine schedule for the remaining epochs. All experiments were run on a single host with 8 Nvidia V100 GPUs and we set \(M=8\) for DAMP. We use \(p=0.05\) for Dropout, \(\sigma=0.1\) for DAMP, \(\rho=0.05\) for SAM, and \(\rho=1.5\) for ASAM. We also use the image resolution of \(224\times 224\) during evaluation.

Vit-S16 / ImageNet / Basic augmentationsWe follow the training setup of Beyer et al. (2022) with one difference is that we only use basic Inception-style data processing similar to the ResNet50/ImageNet experiments. We use AdamW as the optimizer with \(\beta_{1}=0.9\), \(\beta_{2}=0.999\) and \(\epsilon=10^{-8}\). We clip the gradient norm to \(1.0\). We use a weight decay coefficient of \(0.1\). We use a batch size of \(1024\). We increase the learning rate linearly from \(10^{-6}\) to \(10^{-3}\) for the first 10000 iterations, then we anneal the learning rate from \(10^{-3}\) to \(0\) using a cosine schedule for the remaining iterations. We use the image resolution of \(224\times 224\) for both training and testing. Following Beyer et al. (2022), we make 2 minor modifications to the original ViT-S16 architecture: (1) We change the position embedding layer from learnable to sincos2d; (2) We change the input of the final classification layer from the embedding of the [cls] token to global average-pooling. All experiments were run on a single host with 8 Nvidia V100 GPUs and we set \(M=8\) for DAMP. We use \(p=0.10\) for Dropout, \(\sigma=0.25\) for DAMP, \(\rho=0.6\) for SAM, and \(\rho=3.0\) for ASAM.

Vit-S16 and B16 / ImageNet / MixUp and RandAugmentMost of the hyperparameters are identical to the ViT-S16 / ImageNet / Basic augmentations setting. With ViT-S16, we use \(p=0.1\) for Dropout, \(\sigma=0.10\) for DAMP, \(\rho=0.015\) for SAM, and \(\rho=0.4\) for ASAM. With ViT-B16, we use \(p=0.1\) for Dropout, \(\sigma=0.15\) for DAMP, \(\rho=0.025\) for SAM, and \(\rho=0.6\) for ASAM.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We clearly state our contributions and claims in the abstract and introduction and back these claims and contributions with theoretical justifications and experimental results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in Section 6, which outlines the shortcomings of our theoretical proofs and our experiments. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide full set of assumptions and complete proofs of the theoretical result in Section 2 and Appendix A. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all the details regarding our experiments in Appendix F. We also describe the new algorithm that we propose in detail in Algorithm 1. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide our code on a public GitHub repository with instruction on how to reproduce the experimental results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the details regarding our experiments in Appendix F and specify sufficient details to understand the results in the main paper. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For Figs. 4 and 7, the error bars display \(95\%\) confidence intervals. For Table 1, we report the standard deviation. For Figs. 3, 5 and 6, we cannot display the error bars since these figures show heatmaps. For Tables 2 and 3, we does not report the error bars since we ran each experiment once due to the high training cost and our limit in computing resources. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We state clearly in the paper that we run all experiments on a single machine with 8 Nvidia V100 GPUs. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the NeurIPS Code of Ethics and confirm that the research in this paper conforms with these guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss at the end of the paper the possible societal impact of the work in this paper. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not release any new datasets or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly cite all the papers that produced the datasets and model architectures used in this work. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide our code in a public GitHub repository with documentation on how to run the code. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing nor human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve crowdsourcing nor human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.