# Optimal Learners for Realizable Regression:

PAC Learning and Online Learning

 Idan Attias

Ben-Gurion University of the Negev

idanatti@post.bgu.ac.il

&Steve Hanneke

Purdue University

steve.hanneke@gmail.com

&Alkis Kalavasis

Yale University

alvertos.kalavasis@yale.edu

&Amin Karbasi

Yale University, Google Research

amin.karbasi@yale.edu

&Grigoris Velegkas

Yale University

grigoris.velegkas@yale.edu

###### Abstract

In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.

Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in this context.

Additionally, in the context of online learning we provide a dimension that characterizes the minimax instance optimal cumulative loss up to a constant factor and design an optimal online learner for realizable regression, thus resolving an open question raised by Daskalakis and Golowich in STOC '22.

## 1 Introduction

Real-valued regression is one of the most fundamental and well-studied problems in statistics and data science [25, 1, 1], with numerous applications in domains such as economics and medicine [1]. However, despite its significance and applicability, theoretical understanding of the statistical complexity of real-valued regression is still lacking.

Perhaps surprisingly, in the fundamental **realizable** Probably Approximately Correct (PAC) setting [24] and the **realizable** online setting [15, 1], we do not know of any characterizing dimension or optimal learners for the regression task. This comes in sharp contrast with binary and multiclass classification, both in the offline and the online settings, where the situation is much more clear [15, 14, 1, 1, 2, 3, 4, 5, 6]. Our goal in this work is to make progress regarding the following important question:

[MISSING_PAGE_EMPTY:2]

by [1]. The study of multiclass online classification was initiated by [1], who provided an optimal algorithm for the realizable setting and an algorithm that is suboptimal by a factor of \(\log k\cdot\log T\) in the agnostic setting, where \(k\) is the total number of labels. Recently, [13] shaved off the \(\log k\) factor. The problem of online regression differs significantly from that of online classification, since the loss function is not binary. The agnostic online regression setting has received a lot of attention and there is a series of works that provides optimal minimax guarantees [13, 14, 13, 14, 15, 16].

To the best of our knowledge, the realizable setting has received much less attention. A notable exception is the work of [10] that focuses on realizable online regression using _proper_1 learners. They provide an optimal regret bound with respect to the _sequential fat-shattering dimension_. However, as they mention in their work (cf. Examples 1 and 2), this dimension does _not_ characterize the optimal cumulative loss bound.

Footnote 1: A learner is proper when the predictions \(\widehat{y}_{t}\) can be realized by some function \(h_{t}\in\mathcal{H}\).

Interestingly, Daskalakis and Golowich [10] leave the question of providing a dimension that characterizes online realizable regression open. In our work, we resolve this question by providing upper and lower bounds for the cumulative loss of the learner that are tight up to a constant factor of \(2\) using a novel combinatorial dimension that is related to (scaled) Littlestone trees (cf. Definition 13). Formally, the setting of realizable online regression is defined as follows:

**Definition 2** (Online Realizable Regression).: _Let \(\ell:[0,1]^{2}\to\mathbb{R}_{\geq 0}\) be a loss function. Consider a class \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) for some domain \(\mathcal{X}\). The realizable online regression setting over \(T\) rounds consists of the following interaction between the learner and the adversary:_

* _The adversary presents_ \(x_{t}\in\mathcal{X}\)_._
* _The learner predicts_ \(\widehat{y}_{t}\in[0,1]\)_, possibly using randomization._
* _The adversary reveals the true label_ \(y_{t}^{\star}\in[0,1]\) _with the constraint that_ \(\exists h_{t}^{\star}\in\mathcal{H},\forall\tau\leq t,h(x_{\tau})=y_{\tau}^{\star}\)_._
* _The learner suffers loss_ \(\ell(\widehat{y}_{t},y_{t}^{\star})\)_._

_The goal of the learner is to minimize its expected cumulative loss \(\mathfrak{C}_{T}=\mathbf{E}\left[\sum_{t\in[T]}\ell(\widehat{y}_{t},y_{t}^{ \star})\right]\)._

We remark that in the definition of the cumulative loss \(\mathfrak{C}_{T}\), the expectation is over the randomness of the algorithm (which is the only stochastic aspect of the online setting). As we explained before, the main question we study in this setting is the following:

Figure 1: Landscape of Realizable PAC Regression: the “deleted” arrows mean that the implication is _not_ true. The equivalence between finite fat-shattering dimension and the uniform convergence property is known even in the realizable case (see [12]) and the fact that PAC learnability requires finite scaled Natarajan dimension is proved in [13]. The properties of the other three dimensions (scaled Graph dimension, scaled One-Inclusion-Graph (OIG) dimension, and scaled Daniely-Shalev Shwartz (DS) dimension) are shown in this work. We further conjecture that finite scaled Natajaran dimension is not sufficient for PAC learning, while finite scaled DS does suffice. Interestingly, we observe that the notions of uniform convergence, learnability by any ERM and PAC learnability are separated in realizable regression.

[MISSING_PAGE_FAIL:4]

### \(\gamma\)-Fat Shattering Dimension

Perhaps the most well-known dimension in the real-valued learning setting is the fat shattering dimension that was introduced in [10]. Its definition is inspired by the pseudo-dimension [11] and it is, essentially, a scaled version of it.

**Definition 4** (\(\gamma\)-Fat Shattering Dimension [10]).: _Let \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\). We say that a sample \(S\in\mathcal{X}^{n}\) is \(\gamma\)-fat shattered by \(\mathcal{H}\) if there exist \(s_{1},\ldots,s_{n}\in[0,1]^{n}\) such that for all \(b\in\{0,1\}^{n}\) there exists \(h_{b}\in\mathcal{H}\) such that:_

* \(h_{b}(x_{i})\geq s_{i}+\gamma,\forall i\in[n]\) _such that_ \(b_{i}=1\)_._
* \(h_{b}(x_{i})\leq s_{i}-\gamma,\forall i\in[n]\) _such that_ \(b_{i}=0\)_._

_The \(\gamma\)-fat shattering dimension \(\mathbb{D}_{\gamma}^{\mathrm{fat}}\) is defined to be the maximum size of a \(\gamma\)-fat shattered set._

In the realizable setting, finiteness of the fat-shattering dimension (at all scales) is sufficient for learnability and it is equivalent to uniform convergence2[13]. However, the next example shows that it is **not** a necessary condition for learnability. This comes in contrast to the agnostic case for real-valued functions, in which learnability and uniform convergence are equivalent for all \(\mathcal{H}\).

Footnote 2: Informally, uniform convergence means that for all distributions \(\mathcal{D}\), with high probability over the sample, the error of all \(h\in\mathcal{H}\) on the sample is close to their true population error.

**Example 1** (Realizable Learnability \(\#\) Finite Fat-Shattering Dimension, see Section 6 in [13]).: _Consider a class \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) where each hypothesis is uniquely identifiable by a single example, i.e., for any \(x\in\mathcal{X}\) and any \(f,g\in\mathcal{H}\) we have that \(f(x)\neq g(x)\), unless \(f\equiv g\). Concretely, for every \(d\in\mathbb{N}\), let \(\left\{S_{j}\right\}_{0\leq j\leq d-1}\) be a partition of \(\mathcal{X}\) and define \(\mathcal{H}_{d}=\left\{h_{b_{0},...,b_{d-1}}:b_{i}\in\{0,1\},0\leq i\leq d-1 \right\}\), where_

\[h_{b_{0},...,b_{d-1}}(x)=\frac{3}{4}\sum_{j=0}^{d-1}\mathbbm{1}_{S_{j}}(x)b_{ j}+\frac{1}{8}\sum_{k=0}^{d-1}b_{k}2^{-k}\,.\]

_For any \(\gamma\leq 1/4,\mathbb{D}_{\gamma}^{\mathrm{fat}}(\mathcal{H}_{d})=d\), since for a set of points \(x_{0},\ldots,x_{d-1}\in\mathcal{X}\) such that each \(x_{j}\) belongs to \(S_{j}\), \(0\leq j\leq d-1\), \(\mathcal{H}_{d}\) contains all possible patterns of values above \(3/4\) and below \(1/4\). Indeed, it is not hard to verify that for any \(j\in\{0,\ldots,d-1\}\) if we consider any \(h^{\prime}:=h_{b_{0},...,b_{d-1}}\in\mathcal{H}_{d}\) with \(b_{j}=1\) we have \(h^{\prime}(x_{j})\geq 3/4\). Similarly, if \(b_{j}=0\) then it holds that \(h^{\prime}(x_{j})\leq 1/4\). Hence, \(\mathbb{D}_{\gamma}^{\mathrm{fat}}\left(\cup_{d\in\mathcal{H}}\mathcal{H}_{d }\right)=\infty\). Nevertheless, by just observing one example \((x,h^{\star}(x))\) any ERM learner finds the exact labeling function \(h^{\star}\)._

We remark that this example already shows that the PAC learnability landscape of regression is quite different from that of multiclass classification, where agnostic learning and realizable learning are characterized by the same dimension [1].

To summarize this subsection, the fat-shattering dimension is a natural way to quantify how well the function class can interpolate (with gap \(\gamma\)) some fixed function. Crucially, this interpolation contains only inequalities (see Definition4) and hence (at least intuitively) cannot be tight for the realizable setting, where there exists some function that exactly labels the features. Example1 gives a natural example of a class with infinite fat-shattering dimension that can, nevertheless, be learned with a single sample in the realizable setting.

### \(\gamma\)-Natarajan Dimension

The \(\gamma\)-Natarajan dimension was introduced by [14] and is inspired by the Natarajan dimension [15], which has been used to derive bounds in the multiclass classification setting. Before explaining the \(\gamma\)-Natarajan dimension, let us begin by reminding to the reader the standard Natarajan dimension. We say that a set \(S=\{x_{1},...,x_{n}\}\) of size \(n\) is Natarajan-shattered by a concept class \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\) if there exist two functions \(f,g:S\rightarrow\mathcal{Y}\) so that \(f(x_{i})\neq g(x_{i})\) for all \(i\in[n]\), and for all \(b\in\{0,1\}^{n}\) there exists \(h\in\mathcal{H}\) such that \(h(x_{i})=f(x_{i})\) if \(b_{i}=1\) and \(h(x_{i})=g(x_{i})\) if \(b_{i}=0\). Note that here we have equalities instead of inequalities (recall the fat-shattering case Definition4).

From a geometric perspective (see [1]), this means that the space \(\mathcal{H}\) projected on the set \(S\) contains the set \(\{f(x_{1}),g(x_{1})\}\times...\times\{f(x_{n}),g(x_{n})\}\). This set is "isomorphic" to the Booleanhypercube of size \(n\) by mapping \(f(x_{i})\) to 1 and \(g(x_{i})\) to 0 for all \(i\in[n]\). This means that the Natarajan dimension is essentially the size of the largest Boolean cube contained in \(\mathcal{H}\).

[Sim97] defines the scaled analogue of the above dimension as follows.

**Definition 5** (\(\gamma\)-Natarajan Dimension [Sim97]).: _Let \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\). We say that a set \(S\in\mathcal{X}^{n}\) is \(\gamma\)-Natarajan-shattered by \(\mathcal{H}\) if there exist \(f,g:[n]\to[0,1]\) such that for every \(i\in[n]\) we have \(\ell(f(i),g(i))\geq 2\gamma,\) and_

\[\mathcal{H}|_{S}\supseteq\{f(1),g(1)\}\times\ldots\times\{f(n),g(n)\}\,.\]

_The \(\gamma\)-Natarajan dimension \(\mathbb{D}_{\gamma}^{\mathrm{Nat}}\) is defined to be the maximum size of a \(\gamma\)-Natarajan-shattered set._

Intuitively, one should think of the \(\gamma\)-Natarajan dimension as indicating the size of the largest Boolean cube that is contained in \(\mathcal{H}\). Essentially, every coordinate \(i\in[n]\) gets its own translation of the \(0,1\) labels of the Boolean cube, with the requirement that these two labels are at least \(2\gamma\) far from each other. [Sim97] showed the following result, which states that finiteness of the Natarajan dimension at all scales is a necessary condition for realizable PAC regression:

**Informal Theorem 1** (Theorem 3.1 in [Sim97]).: \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) _is PAC learnable in the realizable regression setting only if \(\mathbb{D}_{\gamma}^{\mathrm{Nat}}(\mathcal{H})<\infty\) for all \(\gamma\in(0,1).\)_

Concluding these two subsections, we have explained the main known general results about realizable offline regression: (i) finiteness of fat-shattering is sufficient but not necessary for PAC learning and (ii) finiteness of scaled Natarajan is necessary for PAC learning.

### \(\gamma\)-Graph Dimension

We are now ready to introduce the \(\gamma\)-graph dimension, which is a relaxation of the definition of the \(\gamma\)-Natarajan dimension. To the best of our knowledge, it has not appeared in the literature before. Its definition is inspired by its non-scaled analogue in multiclass classification [Nat89, DSS14].

**Definition 6** (\(\gamma\)-Graph Dimension).: _Let \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}},\ell:\mathbb{R}^{2}\to[0,1]\). We say that a sample \(S\in\mathcal{X}^{n}\) is \(\gamma\)-graph shattered by \(\mathcal{H}\) if there exists \(f:[n]\mapsto[0,1]\) such that for all \(b\in\{0,1\}^{n}\) there exists \(h_{b}\in\mathcal{H}\) such that:_

* \(h_{b}(x_{i})=f(i),\forall i\in[n]\) _such that_ \(b_{i}=0\)_._
* \(\ell(h_{b}(x_{i}),f(i))>\gamma,\forall i\in[n]\) _such that_ \(b_{i}=1\)_._

_The \(\gamma\)-graph dimension \(\mathbb{D}_{\gamma}^{\mathrm{G}}\) is defined to be the maximum size of a \(\gamma\)-graph shattered set._

We mention that the asymmetry in the above definition is crucial. In particular, replacing the equality \(h_{b}(x_{i})=f(i)\) with \(\ell(h_{b}(x_{i}),f(i))\leq\gamma\) fails to capture the properties of the graph dimension. Intuitively, the equality in the definition reflects the assumption of realizability, i.e., the guarantee that there exists a hypothesis \(h^{\star}\) that exactly fits the labels. Before stating our main result, we can collect some useful observations about this new combinatorial measure. In particular, we provide examples inspired by [DSS14, DSSBOSS15] which show (i) that there exist gaps between different ERM learners (see Example 3) and (ii) that any learning algorithm with a close to optimal sample complexity must be improper (see Example 4).

Our first main result relates the scaled graph dimension with the learnability of any class \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) using a (worst case) ERM learner. This result is the scaled analogue of known multiclass results [DSS14, DSSBOSS15] but its proof for the upper bound follows a different path. For the formal statement of our result and its full proof, we refer the reader to Appendix B.

**Informal Theorem 2** (Informal, see Theorem 1).: _Any \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) is PAC learnable in the realizable regression setting by a worst-case ERM learner if and only if \(\mathbb{D}_{\gamma}^{\mathrm{G}}(\mathcal{H})<\infty\) for all \(\gamma\in(0,1)\)._

**Proof Sketch.** The proof of the lower bound follows in a similar way as the lower bound regarding learnability of binary hypothesis classes that have infinite VC dimension [VC71, BEHW89]. If \(\mathcal{H}\) has infinite \(\gamma\)-graph dimension for some \(\gamma\in(0,1)\), then for any \(n\in\mathbb{N}\) we can find a sequence of \(n\) points \(x_{1},\ldots,x_{n}\) that are \(\gamma\)-graph shattered by \(\mathcal{H}\). Then, we can define a distribution \(\mathcal{D}\) that puts most of its mass on \(x_{1}\), so if the learner observes \(n\) samples then with high probability it will not observe at least half of the shattered points. By the definition of the \(\gamma\)-graph dimension this shows

[MISSING_PAGE_FAIL:7]

**Definition 8** (Orientation and Scaled Out-Degree).: _Let \(\gamma\in[0,1],n\in\mathbb{N},\mathcal{H}\subseteq[0,1]^{[n]}\). An orientation of the one-inclusion graph \(G_{\mathcal{H}}^{\mathrm{OIG}}=(V,E)\) is a mapping \(\sigma:E\to V\) so that \(\sigma(e)\in e\) for any \(e\in E\). Let \(\sigma_{i}(e)\in[0,1]\) denote the \(i\)-th entry of the orientation._

_For a vertex \(v\in V\), corresponding to some hypothesis \(h\in\mathcal{H}\) (see Definition 7), let \(v_{i}\) be the \(i\)-th entry of \(v\), which corresponds to \(h(i)\). The (scaled) out-degree of a vertex \(v\) under \(\sigma\) is \(\mathrm{outdeg}(v;\sigma,\gamma)=|\{i\in[n]:\ell(\sigma_{i}(e_{i,v}),v_{i})>\gamma\}|\). The maximum (scaled) out-degree of \(\sigma\) is \(\mathrm{outdeg}(\sigma,\gamma)=\max_{v\in V}\mathrm{outdeg}(v;\sigma,\gamma)\)._

Finally, we introduce the following novel dimension in the context of real-valued regression. An analogous dimension was proposed in the context of learning under adversarial robustness [14].

**Definition 9** (\(\gamma\)-Oig Dimension).: _Consider a class \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) and let \(\gamma\in[0,1]\). We define the \(\gamma\)-one-inclusion graph dimension \(\mathbb{D}_{\gamma}^{\mathrm{OIG}}\) of \(\mathcal{H}\) as follows:_

\[\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})=\sup\{n\in \mathbb{N}:\exists S\in\mathcal{X}^{n}\text{ such that }\exists\text{ finite subgraph }G=(V,E)\text{ of }G_{\mathcal{H}|_{S}}^{\mathrm{OIG}}=(V_{n},E_{n})\] \[\text{ such that }\forall\text{ orientations }\sigma,\exists v\in V,\text{ where }\mathrm{outdeg}(v;\sigma,\gamma)>n/3\}\,.\]

_We define the dimension to be infinite if the supremum is not attained by a finite \(n\)._

In words, it is the largest \(n\in\mathbb{N}\) (potentially \(\infty\)) such that there exists an (unlabeled) sequence \(S\) of length \(n\) with the property that no matter how one orients some finite subgraph of the one-inclusion graph, there is always some vertex for which at least \(1/3\) of its coordinates are \(\gamma\)-different from the labels of the edges that are attached to this vertex. We remark that the hypothesis class in Example 1 has \(\gamma\)-OIG dimension equal to \(O(1)\). Moreover, a finite fat-shattering dimension of hypothesis class implies a finite OIG dimension of roughly the same size (see Appendix C). We also mention that the above dimension satisfies the "finite character" property and the remaining criteria that dimensions should satisfy according to [1] (see Appendix F). As our main result in this section, we show that any class \(\mathcal{H}\) is learnable if and only if this dimension is finite and we design an (almost) optimal learner for it.

**Informal Theorem 3** (Informal, see Theorem 2).: _Any \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) is PAC learnable in the realizable regression setting if and only if \(\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})<\infty\) for all \(\gamma\in(0,1)\)._

The formal statement and its full proof are postponed to Appendix C.

**Proof Sketch.** We start with the lower bound. As we explained before, orientations of the one-inclusion graph are, in some sense, equivalent to learning algorithms. Therefore, if this dimension is infinite for some \(\gamma>0\), then for any \(n\in\mathbb{N}\), there are no orientations with small maximum out-degree. Thus, for _any_ learner we can construct _some_ distribution \(\mathcal{D}\) under which it makes a prediction that is \(\gamma\)-far from the correct one with constant probability, which means that \(\mathcal{H}\) is not PAC learnable.

Let us now describe the proof of the converse direction, which consists of several steps. First, notice that the finiteness of this dimension provides good orientations for _finite_ subgraphs of the one-inclusion graph. Using the compactness theorem of first-order logic, we can extend them to a good orientation of the whole, potentially infinite, one-inclusion graph. This step gives us a weak learner with the following property: for any given \(\gamma\) there is some \(n_{0}\in\mathbb{N}\) so that, with high probability over the training set, when it is given \(n_{0}\) examples as its training set it makes mistakes that are of order at least \(\gamma\) on a randomly drawn point from \(\mathcal{D}\) with probability at most \(1/3\). The next step is to boost the performance of this weak learner. This is done using the "median-boosting" technique [13] (cf. Algorithm 2) which guarantees that after a small number of iterations we can create an ensemble of weak learners such that, a prediction rule according to their (weighted) median will not make any \(\gamma\)- mistakes on the training set. However, this is not sufficient to prove that the ensemble of these learners has small loss on the distribution \(\mathcal{D}\). This is done by establishing _sample compression_ schemes that have small length. Essentially, such schemes consist of a _compression_ function \(\kappa\) which takes as input a training set and outputs a subset of it, and a reconstruction function \(\rho\) which takes as input the output of \(\kappa\) and returns a predictor whose error on every point of the training set \(S\) is at most \(\gamma\). Extending the arguments of [10] from the binary setting to the real-valued setting we show that the existence of such a scheme whose compression function returns a set of "small" cardinality implies generalization properties of the underlying learning rule. Finally, we show that our weak learner combined with the boosting procedure admit such a sample compression scheme.

Before proceeding to the next section, one could naturally ask whether there is a natural property of the concept class that implies finiteness of the scaled OIG dimension. The work of [15] providesa sufficient and natural condition that implies finiteness of our complexity measure. In particular, Mendelson shows that classes that contain functions with bounded oscillation (as defined in [13]) have finite fat-shattering dimension. This implies that the class is learnable in the agnostic setting and hence is also learnable in the realizable setting. As a result, the OIG-based dimension is also finite. So, bounded oscillations are a general property that guarantees that the finiteness of OIG-based dimension and fat-shattering dimension coincide. We also mention that deriving bounds for the OIG-dimension for interesting families of functions is an important yet non-trivial question.

### \(\gamma\)-DS Dimension

So far we have identified a dimension (cf. Definition 9) that characterizes the PAC learnability of realizable regression. However, it might not be easy to calculate it in some settings. Our goal in this section is to introduce a relaxation of this definition which we conjecture that also characterizes learnability in this context. This new dimension is inspired by the DS dimension, a combinatorial dimension defined by Daniely and Shalev-Shwartz in [14]. In a recent breakthrough result, [1] showed that the DS dimension characterizes multiclass learnability (with a possibly unbounded number of labels and the 0-1 loss). We introduce a scaled version of the DS dimension. To this end, we first define the notion of a scaled pseudo-cube.

**Definition 10** (Scaled Pseudo-Cube).: _Let \(\gamma\in[0,1]\). A class \(\mathcal{H}\subseteq[0,1]^{d}\) is called a \(\gamma\)**-pseudo-cube** of dimension \(d\) if it is non-empty, finite and, for any \(f\in\mathcal{H}\) and direction \(i\in[d]\), the hyper-edge \(e_{i,f}=\{g\in\mathcal{H}:g(j)=f(j)\;\;\forall j\in[d],i\neq j\}\) satisfies \(|e_{i,f}|>1\) and \(\ell(g_{1}(i),g_{2}(i))>\gamma\) for any \(g_{1},g_{2}\in e_{i,f},g_{1}\neq g_{2}\)._

Pseudo-cubes can be seen as a relaxation of the notion of a Boolean cube (which should be intuitively related with the Natarajan dimension) and were a crucial tool in the proof of [1]. In our setting, scaled pseudo-cubes will give us the following combinatorial dimension.

**Definition 11** (\(\gamma\)-Ds Dimension).: _Let \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\). A set \(S\in\mathcal{X}^{n}\) is \(\gamma\)-\(\mathrm{DS}\) shattered if \(\mathcal{H}|_{S}\) contains an \(n\)-dimensional \(\gamma\)-pseudo-cube. The \(\gamma\)-\(\mathrm{DS}\) dimension\(\mathbb{D}_{\gamma}^{\mathrm{DS}}\) is the maximum size of a \(\gamma\)-\(\mathrm{DS}\)-shattered set._

Extending the ideas from the multiclass classification setting, we show that the scaled-DS dimension is necessary for realizable PAC regression. Simon (Section 6, [15]) left as an open direction to "obtain supplementary lower bounds [for realizable regression] (perhaps completely unrelated to the combinatorial or Natarajan dimension)". Our next result is a novel contribution to this direction.

**Informal Theorem 4** (Informal, see Theorem 3).: _Any \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) is PAC learnable in the realizable regression setting only if \(\mathbb{D}_{\gamma}^{\mathrm{DS}}(\mathcal{H})<\infty\) for all \(\gamma\in(0,1)\)._

The proof is postponed to Appendix D. We believe that finiteness of \(\mathbb{D}_{\gamma}^{\mathrm{DS}}(\mathcal{H})\) is also a _sufficient_ condition for realizable regression. However, the approach of [1] that establishes a similar result in the setting of multiclass classification does not extend trivially to the regression setting.

**Conjecture 1** (Finite \(\gamma\)-DS is Sufficient).: _Let \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\). If \(\mathbb{D}_{\gamma}^{\mathrm{DS}}(\mathcal{H})<\infty\) for all \(\gamma\in(0,1)\), then \(\mathcal{H}\) is PAC learnable in the realizable regression setting._

## 3 Online Learnability for Realizable Regression

In this section we will provide our main result regarding realizable online regression. Littlestone trees have been the workhorse of online classification problems [10]. First, we provide a definition for scaled Littlestone trees.

**Definition 12** (Scaled Littlestone Tree).: _A scaled Littlestone tree of depth \(d\leq\infty\) is a complete binary tree of depth \(d\) defined as a collection of nodes_

\[\bigcup_{0\leq\ell<d}\left\{x_{u}\in\mathcal{X}:u\in\{0,1\}^{\ell}\right\}= \left\{x_{\emptyset}\right\}\cup\left\{x_{0},x_{1}\right\}\cup\left\{x_{00},x _{01},x_{10},x_{11}\right\}\cup...\]

_and real-valued gaps_

\[\bigcup_{0\leq\ell<d}\left\{\gamma_{u}\in[0,1]:u\in\{0,1\}^{\ell}\right\}= \left\{\gamma_{\emptyset}\right\}\cup\left\{\gamma_{0},\gamma_{1}\right\}\cup \left\{\gamma_{00},\gamma_{01},\gamma_{10},\gamma_{11}\right\}\cup...\]_such that for every path \(\bm{y}\in\{0,1\}^{d}\) and finite \(n<d\), there exists \(h\in\mathcal{H}\) so that \(h(x_{\bm{y}\leq\ell})=s_{\bm{y}\leq\ell+1}\) for \(0\leq\ell\leq n\), where \(s_{\bm{y}_{\leq\ell+1}}\) is the label of the edge connecting the nodes \(x_{\bm{y}_{\leq\ell}}\) and \(x_{\bm{y}_{\leq\ell+1}}\) and \(\ell\left(s_{\bm{y}\leq\ell,0},s_{\bm{y}\leq\ell,1}\right)=\gamma_{\bm{y}\leq \ell}\)._

In words, scaled Littlestone trees are complete binary trees whose nodes are points of \(\mathcal{X}\) and the two edges attached to every node are its potential classifications. An important quantity is the _gap_ between the two values of the edges. We define the online dimension \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})\) as follows.

**Definition 13** (Online Dimension).: _Let \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\). Let \(\mathcal{T}_{d}\) be the space of all scaled Littlestone trees of depth \(d\) (cf. Definition12) and \(\mathcal{T}=\bigcup_{d=0}^{\infty}\mathcal{T}_{d}\). For any scaled tree \(T=\bigcup_{0\leq\ell\leq\mathrm{dep}(T)}\left\{(x_{u},\gamma_{u})\in(\mathcal{ X},[0,1]):u\in\{0,1\}^{\ell}\right\}\,,\) let \(\mathcal{P}(T)=\{y=(y_{0},...,y_{\mathrm{dep}(T)}):y_{i}\in\{0,1\}^{i}\}\}\) be the set of all paths in \(T\). The dimension \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})\) is_

\[\mathbb{D}^{\mathrm{onl}}(\mathcal{H})=\sup_{T\in\mathcal{T}}\inf_{y\in \mathcal{P}(T)}\sum_{i=0}^{\mathrm{dep}(T)}\gamma_{y_{i}}\,.\] (1)

In words, this dimension considers the tree that has the maximum sum of label gaps over its path with the smallest such sum, among all the trees of arbitrary depth. Note that we are taking the supremum (infimum) in case there is no tree (path) that achieves the optimal value. Providing a characterization and a learner with optimal cumulative loss \(\mathfrak{C}_{T}\) for realizable online regression was left as an open problem by [10]. We resolve this question (up to a factor of 2) by showing the following result.

**Informal Theorem 5** (Informal, see Theorem4).: _For any \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) and \(\varepsilon>0\), there exists a deterministic learner with \(\mathfrak{C}_{\infty}\leq\mathbb{D}^{\mathrm{onl}}(\mathcal{H})+\varepsilon\) and any, potentially randomized, learner satisfies \(\mathfrak{C}_{\infty}\geq\mathbb{D}^{\mathrm{onl}}(\mathcal{H})/2-\varepsilon\)._

The formal statement and the full proof of our results can be found in AppendixE. First, we underline that this result holds when there is no bound on the number of rounds that the learner and the adversary interact. This follows the same spirit as the results in the realizable binary and multiclass classification settings [14, 15]. The dimension that characterizes the minimax optimal cumulative loss for any given \(T\) follows by taking the supremum in Definition13 over trees whose depth is at most \(T\) and the proof follows in an identical way (note that even with finite fixed depth \(T\) the supremum is over infinitely many trees). Let us now give a sketch of our proofs.

**Proof Sketch.** The lower bound follows using similar arguments as in the classification setting: for any \(\varepsilon>0\), the adversary can create a scaled Littlestone tree \(T\) that achieves the \(\sup\inf\) bound, up to an additive \(\varepsilon\). In the first round, the adversary present the root of the tree \(x_{\emptyset}\). Then, no matter what the learner picks the adversary can force error at least \(\gamma_{\emptyset}/2\). The game is repeated on the new subtree. The proof of the upper bound presents the main technical challenge to establish Theorem4. The strategy of the learner can be found in Algorithm3. The key insight in the proof is that, due to realizability, we can show that in every round \(t\) there is some \(\widehat{y}_{t}\in[0,1]\) the learner can predict so that, no matter what the adversary picks as the true label \(y_{t}^{*}\), the online dimension of the class under the extra restriction that \(h(x_{t})=y_{t}^{*}\), i.e, the updated _version space_\(V=\{h\in\mathcal{H}:h(x_{\tau})=y_{\tau}^{*},1\leq\tau\leq t\}\), will decrease by \(\ell(y_{\tau}^{*},\widehat{y}_{t})\). Thus, under this strategy of the learner, the adversary can only distribute up to \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})\) across all the rounds of the interaction. We explain how the learner can find such a \(\widehat{y}_{t}\) and we handle technical issues that arise due to the fact that we are dealing with \(\sup\inf\) instead of \(\max\min\) in the formal proof (cf. AppendixE).

## 4 Conclusion

In this work, we developed optimal learners for realizable regression in PAC learning and online learning. Moreover, we identified combinatorial dimensions that characterize learnability in these settings. We hope that our work can lead to simplified characterizations for these problems. We believe that the main limitation of our work is that the OIG-based dimension we propose is more complicated than the dimensions that have been proposed in the past, like the fat-shattering dimension (which, as we explain, does not characterize learnability in the realizable regression setting). Nevertheless, despite its complexity, this is the first dimension that characterizes learnability in the realizable regression setting. More to that, our work leaves as an important next step to prove (or disprove) the conjecture that the (combinatorial and simpler) \(\gamma\)-DS dimension is qualitatively equivalent to the \(\gamma\)-OIG dimension. Another future direction, that is not directly related to this conjecture, is to better understand the gap between the fat-shattering dimension and the OIG-based dimension.

## Acknowledgements

Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032), ONR (N00014- 19-1-2406), and the AI Institute for Learning-Enabled Optimization at Scale (TILOS). Grigoris Velegkas is supported by TILOS, the Onassis Foundation, and the Bodossaki Foundation. This work was done in part while some of the authors were visiting Archimedes AI Research Center.

## References

* [AACSZ22] Ishaq Aden-Ali, Yeshwanth Cherapanamjeri, Abhishek Shetty, and Nikita Zhivotovskiy. The one-inclusion graph algorithm is not always optimal. _arXiv preprint arXiv:2212.09270_, 2022.
* [AACSZ23] Ishaq Aden-Ali, Yeshwanth Cherapanamjeri, Abhishek Shetty, and Nikita Zhivotovskiy. Optimal pac bounds without uniform convergence. _arXiv preprint arXiv:2304.09167_, 2023.
* [AB99] Martin Anthony and Peter L Bartlett. _Neural network learning: Theoretical foundations_, volume 9. Cambridge university press Cambridge, 1999.
* [ABDCBH97] Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. _Journal of the ACM (JACM)_, 44(4):615-631, 1997.
* [ABDH\({}^{+}\)20] Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian mixtures via compression schemes. _Journal of the ACM (JACM)_, 67(6):1-42, 2020.
* [ABED\({}^{+}\)21] Noga Alon, Omri Ben-Eliezer, Yuval Dagan, Shay Moran, Moni Naor, and Eylon Yogev. Adversarial laws of large numbers and optimal regret in online classification. In _Proceedings of the 53rd annual ACM SIGACT symposium on theory of computing_, pages 447-455, 2021.
* [AH22] Idan Attias and Steve Hanneke. Adversarially robust learning of real-valued functions. _arXiv preprint arXiv:2206.12977_, 2022.
* [AHHM22] Noga Alon, Steve Hanneke, Ron Holzman, and Shay Moran. A theory of pac learnability of partial concept classes. In _2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 658-671. IEEE, 2022.
* [AHM22] Idan Attias, Steve Hanneke, and Yishay Mansour. A characterization of semi-supervised adversarially-robust pac learnability. _arXiv preprint arXiv:2202.05420_, 2022.
* [Bac21] Francis Bach. Learning theory from first principles. _Online version_, 2021.
* [BBM05] Peter Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. _Annals of Statistics_, 33(4):1497-1537, 2005.
* [BCD\({}^{+}\)22] Nataly Brukhim, Daniel Carmon, Irit Dinur, Shay Moran, and Amir Yehudayoff. A characterization of multiclass learnability. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 943-955. IEEE, 2022.
* [BDGR22] Adam Block, Yuval Dagan, Noah Golowich, and Alexander Rakhlin. Smoothed online learning is as easy as statistical learning. In _Conference on Learning Theory_, pages 1716-1786. PMLR, 2022.
* [BDHM\({}^{+}\)19] Shai Ben-David, Pavel Hrubes, Shay Moran, Amir Shpilka, and Amir Yehudayoff. Learnability can be undecidable. _Nature Machine Intelligence_, 1(1):44-48, 2019.

* [BDPSS09] Shai Ben-David, David Pal, and Shai Shalev-Shwartz. Agnostic online learning. In _COLT_, volume 3, page 1, 2009.
* [BDR21] Adam Block, Yuval Dagan, and Alexander Rakhlin. Majorizing measures, sequential complexities, and online learning. In _Conference on Learning Theory_, pages 587-590. PMLR, 2021.
* [BEHW89] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and the vapnik-chervonenkis dimension. _Journal of the ACM (JACM)_, 36(4):929-965, 1989.
* [BGH\({}^{+}\)23] Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann Pitassi, Satchit Sivakumar, and Jessica Sorrell. Stability is stable: Connections between replicability, privacy, and adaptive generalization. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 520-527, 2023.
* [BHM\({}^{+}\)21] Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon Van Handel, and Amir Yehudayoff. A theory of universal learning. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 532-541, 2021.
* [BHMZ20] Olivier Bousquet, Steve Hanneke, Shay Moran, and Nikita Zhivotovskiy. Proper learning, helly number, and an optimal svm bound. In _Conference on Learning Theory_, pages 582-609. PMLR, 2020.
* [BL98] Peter L Bartlett and Philip M Long. Prediction, learning, uniform convergence, and scale-sensitive dimensions. _Journal of Computer and System Sciences_, 56(2):174-190, 1998.
* [BLM20] Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. In _2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)_, pages 389-402. IEEE, 2020.
* [BLW94] Peter L Bartlett, Philip M Long, and Robert C Williamson. Fat-shattering and the learnability of real-valued functions. In _Proceedings of the seventh annual conference on Computational learning theory_, pages 299-310, 1994.
* [CKW08] Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple sources. _Journal of Machine Learning Research_, 9(8), 2008.
* [CP22] Moses Charikar and Chirag Pabbaraju. A characterization of list learnability. _arXiv preprint arXiv:2211.04956_, 2022.
* [DG17] Dheeru Dua and Casey Graff. UCI machine learning repository. 2017.
* [DG22] Constantinos Daskalakis and Noah Golowich. Fast rates for nonparametric online learning: from realizability to learning in games. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 846-859, 2022.
* [DKLD84] Richard M Dudley, H Kunita, F Ledrappier, and RM Dudley. A course on empirical processes. In _Ecole d'ete de probabilites de Saint-Flour XII-1982_, pages 1-142. Springer, 1984.
* [DMY16] Ofir David, Shay Moran, and Amir Yehudayoff. Supervised learning through the lens of compression. _Advances in Neural Information Processing Systems_, 29, 2016.
* [DSBDSS15] Amit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz. Multiclass learnability and the erm principle. _J. Mach. Learn. Res._, 16(1):2377-2404, 2015.
* [DSS14] Amit Daniely and Shai Shalev-Shwartz. Optimal learners for multiclass problems. In _Conference on Learning Theory_, pages 287-316. PMLR, 2014.

* [FHMM23] Yuval Filmus, Steve Hanneke, Idan Mehalel, and Shay Moran. Optimal prediction using expert advice and randomized littlestone dimension. _arXiv preprint arXiv:2302.13849_, 2023.
* [FR20] Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. In _International Conference on Machine Learning_, pages 3199-3210. PMLR, 2020.
* [FW95] Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the vapnik-chervonenkis dimension. _Machine learning_, 21(3):269-304, 1995.
* [GBC16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep learning_. MIT press, 2016.
* [GHST05] Thore Graepel, Ralf Herbrich, and John Shawe-Taylor. Pac-bayesian compression bounds on the prediction error of learning algorithms for classification. _Machine Learning_, 59:55-76, 2005.
* [GKN14] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Near-optimal sample compression for nearest neighbors. _Advances in Neural Information Processing Systems_, 27, 2014.
* [Gol21] Noah Golowich. Differentially private nonparametric regression under a growth condition. In _Conference on Learning Theory_, pages 2149-2192. PMLR, 2021.
* [Han16] Steve Hanneke. The optimal sample complexity of pac learning. _The Journal of Machine Learning Research_, 17(1):1319-1333, 2016.
* [HKLM22] Max Hopkins, Daniel M Kane, Shachar Lovett, and Gaurav Mahajan. Realizable learning is all you need. In _Conference on Learning Theory_, pages 3015-3069. PMLR, 2022.
* [HKS18] Steve Hanneke, Aryeh Kontorovich, and Menachem Sadigurshi. Agnostic sample compression for linear regression. _arXiv preprint arXiv:1810.01864_, 2018.
* [HKS19] Steve Hanneke, Aryeh Kontorovich, and Menachem Sadigurshi. Sample compression for real-valued learners. In _Algorithmic Learning Theory_, pages 466-488. PMLR, 2019.
* [HLW94] David Haussler, Nick Littlestone, and Manfred K Warmuth. Predicting {0, 1}-functions on randomly drawn points. _Information and Computation_, 115(2):248-292, 1994.
* [JS03] Tadeusz Januszkiewicz and Jacek Swiatkowski. Hyperbolic coxeter groups of large dimension. _Commentarii Mathematici Helvetici_, 78(3):555-583, 2003.
* [Keg03] Balazs Kegl. Robust regression by boosting the median. In _Learning Theory and Kernel Machines_, pages 258-272. Springer, 2003.
* [KKMV23] Alkis Kalavasis, Amin Karbasi, Shay Moran, and Grigoris Velegkas. Statistical indistinguishability of learning algorithms. _arXiv preprint arXiv:2305.14311_, 2023.
* [KS94] Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of probabilistic concepts. _Journal of Computer and System Sciences_, 48(3):464-497, 1994.
* [KS23] Pieter Kleer and Hans Simon. Primal and dual combinatorial dimensions. _Discrete Applied Mathematics_, 327:185-196, 2023.
* [KSW17] Aryeh Kontorovich, Sivan Sabato, and Roi Weiss. Nearest-neighbor sample compression: Efficiency, consistency, infinite dimensions. _Advances in Neural Information Processing Systems_, 30, 2017.

* [KVK22] Alkis Kalavasis, Grigoris Velegkas, and Amin Karbasi. Multiclass learnability beyond the pac framework: Universal rates and partial concept classes. _arXiv preprint arXiv:2210.02297_, 2022.
* [Lit88] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. _Machine learning_, 2:285-318, 1988.
* [LW86] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. 1986.
* [Men02] Shahar Mendelson. Improving the sample complexity using global data. _IEEE transactions on Information Theory_, 48(7):1977-1991, 2002.
* [MHS19] Omar Montasser, Steve Hanneke, and Nathan Srebro. Vc classes are adversarially robustly learnable, but only improperly. In _Conference on Learning Theory_, pages 2512-2530. PMLR, 2019.
* [MHS20] Omar Montasser, Steve Hanneke, and Nati Srebro. Reducing adversarially robust learning to non-robust pac learning. _Advances in Neural Information Processing Systems_, 33:14626-14637, 2020.
* [MHS21] Omar Montasser, Steve Hanneke, and Nathan Srebro. Adversarially robust learning with unknown perturbation sets. In _Conference on Learning Theory_, pages 3452-3482. PMLR, 2021.
* [MHS22] Omar Montasser, Steve Hanneke, and Nati Srebro. Adversarially robust learning: A generic minimax optimal learner and characterization. _Advances in Neural Information Processing Systems_, 35:37458-37470, 2022.
* [MKF122] Jason Milionis, Alkis Kalavasis, Dimitris Fotakis, and Stratis Ioannidis. Differentially private regression with unbounded covariates. In _International Conference on Artificial Intelligence and Statistics_, pages 3242-3273. PMLR, 2022.
* [MY16] Shay Moran and Amir Yehudayoff. Sample compression schemes for vc classes. _Journal of the ACM (JACM)_, 63(3):1-10, 2016.
* [Nat89] Balas K Natarajan. On learning sets and functions. _Machine Learning_, 4(1):67-97, 1989.
* [Osa13] Damian L Osajda. A construction of hyperbolic coxeter groups. _Commentarii Mathematici Helvetici_, 88(2):353-367, 2013.
* [Pol90] David Pollard. Empirical processes: theory and applications. Ims, 1990.
* [Pol12] David Pollard. _Convergence of stochastic processes_. Springer Science & Business Media, 2012.
* [RBR09] Benjamin IP Rubinstein, Peter L Bartlett, and J Hyam Rubinstein. Shifting: One-inclusion mistake bounds and sample compression. _Journal of Computer and System Sciences_, 75(1):37-59, 2009.
* [RS14] Alexander Rakhlin and Karthik Sridharan. Online non-parametric regression. In _Conference on Learning Theory_, pages 1232-1264. PMLR, 2014.
* [RST10] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. _Advances in Neural Information Processing Systems_, 23, 2010.
* [RST15a] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning via sequential complexities. _J. Mach. Learn. Res._, 16(1):155-186, 2015.
* [RST15b] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform martingale laws of large numbers. _Probability theory and related fields_, 161:111-153, 2015.

* [RST17] Alexander Rakhlin, Karthik Sridharan, and Alexandre B Tsybakov. Empirical entropy, minimax regret and minimax risk. 2017.
* [RST23] Vinod Raman, Unique Subedi, and Ambuj Tewari. A characterization of online multiclass learnability. _arXiv preprint arXiv:2303.17716_, 2023.
* [She17] Or Sheffet. Differentially private ordinary least squares. In _International Conference on Machine Learning_, pages 3105-3114. PMLR, 2017.
* [Sim97] Hans Ulrich Simon. Bounds on the number of examples needed for learning functions. _SIAM Journal on Computing_, 26(3):751-763, 1997.
* [SLX22] David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. _Mathematics of Operations Research_, 47(3):1904-1931, 2022.
* [SMB22] Han Shao, Omar Montasser, and Avrim Blum. A theory of pac learnability under transformation invariances. _arXiv preprint arXiv:2202.07552_, 2022.
* [SSSSS10] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. _The Journal of Machine Learning Research_, 11:2635-2670, 2010.
* [Val84] Leslie G Valiant. A theory of the learnable. _Communications of the ACM_, 27(11):1134-1142, 1984.
* [Vap99] Vladimir N Vapnik. An overview of statistical learning theory. _IEEE transactions on neural networks_, 10(5):988-999, 1999.
* [VC71] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. _Theory of Probability and its Applications_, 16(2):264-280, 1971.
* [WHEY15] Yair Wiener, Steve Hanneke, and Ran El-Yaniv. A compression technique for analyzing disagreement-based active learning. _J. Mach. Learn. Res._, 16:713-745, 2015.

Notation and Definitions

We first overview standard definitions about sample complexity in PAC learning.

PAC Sample Complexity.The realizable PAC sample complexity \(\mathcal{M}(\mathcal{H};\varepsilon,\delta)\) of \(\mathcal{H}\) is defined as

\[\mathcal{M}(\mathcal{H};\varepsilon,\delta)=\inf_{A\in\mathcal{A}}\mathcal{M}_{ A}(\mathcal{H};\varepsilon,\delta)\,,\] (1)

where the infimum is over all possible learning algorithms and \(\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta)\) is the minimal integer such that for any \(m\geq\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta)\), every distribution \(\mathcal{D}_{\mathcal{X}}\) on \(\mathcal{X}\), and, true target \(h^{\star}\in\mathcal{H}\), the expected loss \(\mathbf{E}_{x\sim\mathcal{D}_{\mathcal{X}}}[\ell(A(T)(x),h^{\star}(x))]\) of \(A\) is at most \(\varepsilon\) with probability \(1-\delta\) over the training set \(T=\{(x,h^{\star}(x)):x\in S\}\), \(S\sim\mathcal{D}_{\mathcal{X}}^{\mathcal{D}_{\mathcal{X}}}\).

PAC Cut-Off Sample Complexity.We slightly overload the notation of the sample complexity and we define

\[\mathcal{M}(\mathcal{H};\varepsilon,\delta,\gamma)=\inf_{A\in\mathcal{A}} \mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta,\gamma)\,,\] (2)

where the infimum is over all possible learning algorithms and \(\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta,\gamma)\) is the minimal integer such that for any \(m\geq\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta,\gamma)\), every distribution \(\mathcal{D}_{\mathcal{X}}\) on \(\mathcal{X}\), and, true target \(h^{\star}\in\mathcal{H}\), the expected cut-off loss \(\mathbf{Pr}_{x\sim\mathcal{D}_{\mathcal{X}}}[\ell(A(T)(x),h^{\star}(x))>\gamma]\) of \(A\) is at most \(\varepsilon\) with probability \(1-\delta\) over the training set \(T=\{(x,h^{\star}(x)):x\in S\}\), \(S\sim\mathcal{D}_{\mathcal{X}}^{\mathcal{D}_{\mathcal{X}}}\).

**Lemma 1** (Equivalence Between Sample complexities).: _For every \(\varepsilon,\delta\in(0,1)^{2}\) and every \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\), where \(\mathcal{X}\) is the input domain, it holds that_

\[\mathcal{M}(\mathcal{H};\sqrt{\varepsilon},\delta,\sqrt{\varepsilon})\leq \mathcal{M}(\mathcal{H};\varepsilon,\delta)\leq\mathcal{M}(\mathcal{H}; \varepsilon/2,\delta,\varepsilon/2)\]

Proof.: Let \(A\) be a learning algorithm. We will prove the statement for each fixed \(A\) and for each data-generating distribution \(\mathcal{D}\), so the result follows by taking the infimum over the learning algorithms.

Assume that the cut-off sample complexity of \(A\) is \(\mathcal{M}_{A}(\mathcal{H};\varepsilon/2,\delta,\varepsilon/2)\). Then, with probability \(1-\delta\) over the training sample \(S\sim\mathcal{D}\), for its expected loss it holds that

\[\mathop{\mathbf{E}}_{x\sim\mathcal{D}_{\mathcal{X}}}[\ell(A(S;x),h^{\star}(x) )]\leq\frac{\varepsilon}{2}+\left(1-\frac{\varepsilon}{2}\right)\cdot\frac{ \varepsilon}{2}\leq\varepsilon\,,\]

thus, \(\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta)\leq\mathcal{M}_{A}(\mathcal{ H};\varepsilon/2,\delta,\varepsilon/2)\).

The other direction follows by using Markov's inequality. In particular, if we have that with probability at least \(1-\delta\) over \(S\sim\mathcal{D}\) it holds that

\[\mathop{\mathbf{E}}_{x\sim\mathcal{D}_{\mathcal{X}}}[\ell(A(S;x),h^{\star}(x) )]\leq\varepsilon\,,\]

then Markov's inequality gives us that

\[\mathop{\mathbf{Pr}}_{x\sim\mathcal{D}_{\mathcal{X}}}[\ell(A(S;x),h^{\star}( x))\geq\sqrt{\varepsilon}]\leq\sqrt{\varepsilon}\,,\]

which shows that \(\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta)\geq\mathcal{M}_{A}(\mathcal{H} ;\sqrt{\varepsilon},\delta,\sqrt{\varepsilon})\). 

ERM Sample Complexity.In the special case where \(\mathcal{A}\) is the class \(\mathrm{ERM}\) of all possible ERM algorithms, i.e., algorithms that return a hypothesis whose sample error is exactly 0, we define the ERM sample complexity as the number of samples required by the worst-case ERM algorithm, i.e.,

\[\mathcal{M}_{\mathrm{ERM}}(\mathcal{H};\varepsilon,\delta)=\sup_{A\in\mathrm{ ERM}}\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta)\,,\] (3)

and its cut-off analogue as

\[\mathcal{M}_{\mathrm{ERM}}(\mathcal{H};\varepsilon,\delta,\gamma)=\sup_{A\in \mathrm{ERM}}\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta,\gamma)\,,\] (4)

## Appendix B \(\gamma\)-Graph Dimension and ERM Learnability

In this section, we show that \(\gamma\)-graph dimension determines the learnability of \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) using _any_ ERM learner. We first revisit the notion of partial concept classes which will be useful for deriving our algorithms.

### Partial Concept Classes and A Naive Approach that Fails

[1] proposed an extension of the binary PAC model to handle _partial concept classes_, where \(\mathcal{H}\subseteq\{0,1,\star\}^{\mathcal{X}}\), for some input domain \(\mathcal{X}\), where \(h(x)=\star\) should be thought of as \(h\) not knowing the label of \(x\in\mathcal{X}\). The main motivation behind their work is that partial classes allow one to conveniently express _data-dependent_ assumptions. As an intuitive example, a halfspace with margin is a partial function that is undefined inside the forbidden margin and is a well-defined halfspace outside the margin boundaries. Instead of dealing with concept classes \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\) where each concept \(h\in\mathcal{H}\) is a **total function**\(h:\mathcal{X}\rightarrow\mathcal{Y}\), we study **partial concept classes**\(\mathcal{H}\subseteq(\mathcal{Y}\cup\{\star\})^{\mathcal{X}}\), where each concept \(h\) is now a **partial function** and \(h(x)=\star\) means that the function \(h\) is **undefined** at \(x\). We define the support of \(h\) as the set \(\operatorname{supp}(h)=\{x\in\mathcal{X}:h(x)\neq\star\}\). Similarly as in the case of total classes, we say that a finite sequence \(S=(x_{1},y_{1},\ldots,x_{n},y_{n})\) is realizable with respect to \(\mathcal{H}\) if there exists some \(h^{*}\in\mathcal{H}\) such that \(h^{*}(x_{i})=y_{i},\forall i\in[n]\).

An important notion related to partial concept classes is that of _disambiguation_.

**Definition 14** (Disambiguation of Partial Concept Class [1]).: _Let \(\mathcal{X}\) be an input domain. A total concept class \(\overline{\mathcal{H}}\subseteq\{0,1\}^{\mathcal{X}}\) is a special type of a partial concept. Given some partial concept class \(\mathcal{H}\subseteq\{0,1,\star\}^{\mathcal{X}}\) we say that \(\overline{H}\) is a disambiguation of \(\mathcal{H}\) if for any finite sequence \(S\in(\mathcal{X}\times\{0,1\})^{*}\) if \(S\) is realizable with respect to \(\mathcal{H}\), then \(S\) is realizable with respect to \(\overline{\mathcal{H}}\)._

Intuitively, by disambiguating a partial concept class we convert it to a total concept class without reducing its "expressivity".

Let us first describe an approach to prove the upper bound, i.e., that if the scaled-graph dimension is finite for all scales then the class is ERM learnable, that does not work. We could perform the following transformation, inspired by the multiclass setting [13]: for any \(h\in\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\), let us consider the function \(\widetilde{h}:\mathcal{X}\times[0,1]\rightarrow\{0,1\}\) with \(\widetilde{h}(x,y)=1\) if and only if \(h(x)=y\), \(\widetilde{h}(x,y)=0\) if and only if \(\ell(h(x),y)>\varepsilon\) and \(\widetilde{h}(x,y)=\star\) otherwise. This induces a new binary _partial_ hypothesis class \(\widetilde{\mathcal{H}}=\{\widetilde{h}:h\in\mathcal{H}\}\subseteq\{0,1, \star\}^{\mathcal{X}}\). We note that \(\mathbb{D}_{\varepsilon}^{\mathrm{G}}(\mathcal{H})=\mathrm{VC}(\widetilde{ \mathcal{H}})\). However, we cannot use ERM for the partial concept class since in general this approach fails. In particular, a sufficient condition for applying ERM is that \(\mathrm{VC}(\{\operatorname{supp}(\widetilde{h}):\widetilde{h}\in\widetilde{ \mathcal{H}}\})<\infty\).

**Remark 1**.: _Predicting \(\star\) in [1] implies a mistake for the setting of partial concept classes. However, in our regression setting, \(\star\) is interpreted differently and corresponds to loss at most \(\gamma\) which is desirable. In particular, the hard instance for proper learners in the partial concepts paper (see Proposition 4 in [1]) is good in settings where predicting \(\star\) does not count as a mistake, as in our regression case._

### Main Result

We are now ready to state the main result of this section. We will prove the next statement.

**Theorem 1**.: _Let \(\ell\) be the absolute loss function. For every class \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) and for any \(\varepsilon,\delta,\gamma\in(0,1)^{3},\) the sample complexity bound for realizable PAC regression by any ERM satisfies_

\[\Omega\left(\frac{\mathbb{D}_{\gamma}^{\mathrm{G}}(\mathcal{H})+\log(1/\delta) }{\varepsilon}\right)\leq\mathcal{M}_{\mathrm{ERM}}(\mathcal{H};\varepsilon, \delta,\gamma)\leq O\left(\frac{\mathbb{D}_{\gamma}^{\mathrm{G}}(\mathcal{H}) \log(1/\varepsilon)+\log(1/\delta)}{\varepsilon}\right)\,.\]

_In particular, any ERM algorithm \(\mathbb{A}\) achieves_

\[\mathop{\mathbf{E}}_{x\sim\mathcal{D}_{\mathcal{X}}}[\ell(A(S;x),h^{\star}(x) ]\leq\inf_{\gamma\in[0,1]}\gamma+\widetilde{\Theta}\left(\frac{\mathbb{D}_{ \gamma}^{\mathrm{G}}(\mathcal{H})+\log(1/\delta)}{n}\right)\,,\]

_with probability at least \(1-\delta\) over \(S\) of size \(n\)._

Proof.: We prove the upper bound and the lower bound of the statement separately.

Upper Bound for the ERM learner.We deal with the cut-off loss problem with parameters \((\varepsilon,\delta,\gamma)\in(0,1)^{3}\). Our proof is based on a technique that uses a "ghost" sample to establish generalization guarantees of the algorithm. Let us denote

\[\operatorname{er}_{\mathcal{D},\gamma}(h)\triangleq\mathop{\mathbf{Pr}}_{x \sim\mathcal{D}_{\mathcal{X}}}[\ell(h(x),h^{\star}(x))>\gamma]\,,\] (1)and for a dataset \(z\in(\mathcal{X}\times[0,1])^{n}\),

\[\widehat{\mathrm{er}}_{z,\gamma}(h)\triangleq\frac{1}{|z|}\sum_{(x,y)\in z} \mathbb{I}\{\ell(h(x),y)>\gamma\}\,.\] (2)

We will start by showing the next symmetrization lemma in our setting. Essentially, it bounds the probability that there exists a bad ERM learner by the probability that there exists an ERM learner on a sample \(r\) whose performance on a hidden sample \(s\) is bad. For a similar result, see Lemma 4.4 in [1].

**Lemma 2** (Symmetrization).: _Let \(\varepsilon,\gamma\in(0,1)^{2},n>0\). Fix \(Z=\mathcal{X}\times[0,1]\). Let_

\[Q_{\varepsilon,\gamma}=\{z\in Z^{n}:\exists h\in\mathcal{H}:\widehat{\mathrm{ er}}_{z,\gamma}(h)=0,\;\mathrm{er}_{\mathcal{D},\gamma}(h)>\varepsilon\}\] (3)

_and_

\[R_{\varepsilon,\gamma}=\{(r,s)\in Z^{n}\times Z^{n}:\exists h\in\mathcal{H}: \mathrm{er}_{\mathcal{D},\gamma}(h)>\varepsilon,\;\widehat{\mathrm{er}}_{r, \gamma}(h)=0,\;\widehat{\mathrm{er}}_{s,\gamma}(h)\geq\varepsilon/2\}\,.\] (4)

_Then, for \(n\geq c/\varepsilon\), where \(c\) is some absolute constant, we have that_

\[\mathcal{D}^{n}(Q_{\varepsilon,\gamma})\leq 2\mathcal{D}^{2n}(R_{\varepsilon, \gamma})\,.\]

Proof.: We will show that \(\mathcal{D}^{2n}(R_{\varepsilon,\gamma})\geq\frac{\mathcal{D}^{n}(Q_{ \varepsilon,\gamma})}{2}\). By the definition of \(R_{\varepsilon,\gamma}\) we can write

\[\mathcal{D}^{2n}(R_{\varepsilon,\gamma})=\int_{Q_{\varepsilon,\gamma}} \mathcal{D}^{n}(s:\exists h\in\mathcal{H},\mathrm{er}_{\mathcal{D},\gamma}(h)> \varepsilon,\widehat{\mathrm{er}}_{r,\gamma}(h)=0,\widehat{\mathrm{er}}_{s, \gamma}(h)\geq\varepsilon/2)d\mathcal{D}^{n}(r)\,.\]

For \(r\in Q_{\varepsilon,\gamma}\), fix \(h_{r}\in\mathcal{H}\) that satisfies \(\widehat{\mathrm{er}}_{r,\gamma}(h_{r})=0,\;\mathrm{er}_{\mathcal{D},\gamma}(h )>\varepsilon\). It suffices to show that for \(h_{r}\)

\[\mathcal{D}^{n}(s:\widehat{\mathrm{er}}_{s,\gamma}(h_{r})\geq\varepsilon/2) \geq 1/2\,.\]

Then, the proof of the lemma follows immediately. Since \(\mathrm{er}_{\mathcal{D},\gamma}(h_{r})>\varepsilon\), we know that \(n\cdot\widehat{\mathrm{er}}_{s,\gamma}(h)\) follows a binomial distribution with probability of success on every try at least \(\varepsilon\) and \(n\) number of tries. Thus, the multiplicative version of Chernoff's bound gives us

\[\mathcal{D}^{n}(s:\widehat{\mathrm{er}}_{s,\gamma}(h_{r})<\varepsilon/2)\leq e ^{-\frac{n\cdot\varepsilon}{8}}\,.\]

Thus, if \(n=c/\varepsilon\), for some appropriate absolute constant \(c\) we see that

\[\mathcal{D}^{n}(s:\widehat{\mathrm{er}}_{s,\gamma}(h_{r})<\varepsilon/2)<1/2\,,\]

which concludes the proof. 

Next, we can use a random swap argument to upper bound \(\mathcal{D}^{2n}(R_{\varepsilon,\gamma})\) with a quantity that involves a set of permutations over the sample of length \(2n\). The main idea behind the proof is to try to leverage the fact that each of the labeled examples is as likely to occur among the first \(n\) examples or the last \(n\) examples.

Following [1], we denote by \(\Gamma_{n}\) the set of all permutations on \(\{1,\ldots,2n\}\) that swap \(i\) and \(n+i\), for all \(i\) that belongs to \(\{1,\ldots,n\}\). In other words, for all \(\sigma\in\Gamma_{n}\) and \(i\in\{1,\ldots,n\}\) either \(\sigma(i)=i,\sigma(n+i)=n+i\) or \(\sigma(i)=n+i,\sigma(n+i)=i\). Thus, we can think of \(\sigma\) as acting on coordinates where it (potentially) swaps one element from the first half of the sample with the corresponding element on the second half of the sample. For some \(z\in Z^{2n}\) we overload the notation and denote \(\sigma(z)\) the effect of applying \(\sigma\) to the sample \(z\).

We are now ready to state the bound. Importantly, it shows that by (uniformly) randomly choosing a permutation \(\sigma\in\Gamma_{n}\) we can bound the probability that a sample falls into the bad set \(R_{\varepsilon,\gamma}\) by a quantity that does not depend on the distribution \(\mathcal{D}\).

**Lemma 3** (Random Swaps; Adaptation of Lemma 4.5 in [1]).: _Fix \(Z=\mathcal{X}\times[0,1]\). Let \(R_{\varepsilon,\gamma}\) be any subset of \(Z^{2n}\) and \(\mathcal{D}\) any probability distribution on \(Z\). Then_

\[\mathcal{D}^{2n}(R_{\varepsilon,\gamma})=\underset{z\sim\mathcal{D}^{2n}}{ \mathbf{E}}\underset{\sigma\sim\mathbb{U}(\Gamma_{n})}{\mathbf{Pr}}[\sigma(z) \in R_{\varepsilon,\gamma}]\leq\underset{z\in Z^{2n}}{\mathbf{Pr}}\underset{ \sigma\sim\mathbb{U}(\Gamma_{n})}{\mathbf{Pr}}[\sigma(z)\in R_{\varepsilon, \gamma}]\,,\]

_where \(\mathbb{U}(\Gamma_{n})\) is the uniform distribution over the set of swapping permutations \(\Gamma_{n}\)._Proof.: First, notice that the bound

\[\operatorname*{\mathbf{E}}_{z\sim D^{2n}}\operatorname*{\mathbf{Pr}}_{\sigma\sim \operatorname{U}[\Gamma_{n}]}[\sigma(z)\in R_{\varepsilon,\gamma}]\leq\max_{z \in Z^{2n}}\operatorname*{\mathbf{Pr}}_{\sigma\sim\operatorname{U}(\Gamma_{n}) }[\sigma(z)\in R_{\varepsilon,\gamma}]\,,\]

follows trivially and the maximum exists since \(\operatorname*{\mathbf{Pr}}_{\sigma\sim\operatorname{U}(\Gamma_{n})}[\sigma(z) \in R_{\varepsilon,\gamma}]\) takes finitely many values for any finite \(n\) and all \(z\in Z^{2n}\). Thus, the bulk of the proof is to show that

\[\mathcal{D}^{2n}(R_{\varepsilon,\gamma})=\operatorname*{\mathbf{E}}_{z\sim D^{ 2n}}\operatorname*{\mathbf{Pr}}_{\sigma\sim\operatorname{U}(\Gamma_{n})}[ \sigma(z)\in R_{\varepsilon,\gamma}]\,.\]

First, notice that since example is drawn i.i.d., for any swapping permutation \(\sigma\in\Gamma_{n}\) we have that

\[\mathcal{D}^{2n}(R_{\varepsilon,\gamma})=\mathcal{D}^{2n}\left(\left\{z\in Z ^{2n}:\sigma(z)\in R_{\varepsilon,\gamma}\right\}\right)\,.\] (5)

Thus, the following holds

\[\mathcal{D}^{2n}(R_{\varepsilon,\gamma}) =\int_{Z^{2n}}\mathbb{I}\{z\in R_{\varepsilon,\gamma}\}\,d \mathcal{D}^{2n}(z)\] \[=\frac{1}{|\Gamma_{n}|}\sum_{\sigma\in\Gamma_{n}}\int_{Z^{2n}} \mathbb{I}\{\sigma(z)\in R_{\varepsilon,\gamma}\}\,d\mathcal{D}^{2n}(z)\] \[=\int_{Z^{2n}}\left(\frac{1}{|\Gamma_{n}|}\sum_{\sigma\in\Gamma_{ n}}\mathbb{I}\{\sigma(z)\in R_{\varepsilon,\gamma}\}\right)d\mathcal{D}^{2n}(z)\] \[=\operatorname*{\mathbf{E}}_{z\sim\mathcal{D}^{2n}}\operatorname* {\mathbf{Pr}}_{\sigma\sim\operatorname{U}(\Gamma_{n})}[\sigma(z)\in R_{ \varepsilon,\gamma}]\,,\]

where the first equation follows by definition, the second by Equation5, the third because the number of terms in the summation is finite, and the last one by definition. 

As a last step we can bound the above RHS by using all possible patterns when \(\mathcal{H}\) is (roughly speaking) projected in the sample \(rs\in Z^{2n}\).

**Lemma 4** (Bounding the Bad Event).: _Fix \(Z=\mathcal{X}\times[0,1]\). Let \(R_{\varepsilon,\gamma}\subseteq Z^{2n}\) be the set_

\[R_{\varepsilon,\gamma}=\{(r,s)\in Z^{n}\times Z^{n}:\exists h\in\mathcal{H}: \operatorname*{\widehat{\operatorname*{e}}}_{r,\gamma}(h)=0,\,\operatorname*{ \widehat{\operatorname*{e}}}_{s,\gamma}(h)\geq\varepsilon/2\}\,.\]

_Then_

\[\operatorname*{\mathbf{Pr}}_{\sigma\sim\operatorname{U}(\Gamma_{n})}[\sigma(z )\in R_{\varepsilon,\gamma}]\leq(2n)^{O(\mathbb{D}_{\gamma}^{\operatorname{G }}(\mathcal{H})\log(2n))}2^{-n\varepsilon/2}\,.\]

Proof.: Throughout the proof we fix \(z=(z_{1},...,z_{2n})\in Z^{2n}\), where \(z_{i}=(x_{i},y_{i})=(x_{i},h^{\star}(x_{i}))\) and let \(S=\{x_{1},...,x_{2n}\}\). Consider the projection set \(\mathcal{H}|_{S}\). We define a _partial binary_ concept class \(\mathcal{H}^{\prime}\subseteq\{0,1,\star\}^{2n}\) as follows:

\[\mathcal{H}^{\prime}:=\left\{h^{\prime}\in\{0,1,\star\}^{2n}:\exists h\in \mathcal{H}|_{S}:\forall i\in[2n]\begin{cases}h(x_{i})=y_{i},&h^{\prime}(i)=0\\ \ell(h(x_{i}),y_{i})>\gamma,&h^{\prime}(i)=1\\ 0<\ell(h(x_{i}),y_{i})\leq\gamma,&h^{\prime}(i)=\star\end{cases}\right\}\,.\]

Importantly, we note that, by definition, \(\operatorname{VC}(\mathcal{H}^{\prime})\subseteq\mathbb{D}_{\gamma}^{ \operatorname{G}}(\mathcal{H})\).

Currently, we have a partial binary concept class \(\mathcal{H}^{\prime}\). As a next step, we would like to replace the \(\star\) symbols and essentially reduce the problem to a total concept class. This procedure is called disambiguation (cf. Definition14). The next key lemma shows that there exists a compact (in terms of cardinality) disambiguation of a VC partial concept class for finite instance domains.

**Lemma 5** (Compact Disambiguations, see [1]).: _Let \(\mathcal{H}\) be a partial concept class on a finite instance domain \(\mathcal{X}\) with \(\operatorname{VC}(\mathcal{H})=d\). Then there exists a disambiguation \(\overline{\mathcal{H}}\) of \(\mathcal{H}\) with size \(|\overline{\mathcal{H}}|=|\mathcal{X}|^{O(d\log|\mathcal{X}|)}\)._

This means that there exists a disambiguation \(\overline{\mathcal{H}^{\prime}}\) of \(\mathcal{H}^{\prime}\) of size at most

\[(2n)^{O(\mathbb{D}_{\gamma}^{\operatorname{G}}(\mathcal{H})\log(2n))}\,.\]Since this (total) binary concept class is finite, we can apply the following union bound argument. We have that \(\sigma(z)\in R\) if and only if some \(h\in\mathcal{H}\) satisfies

\[\frac{\sum_{i=1}^{n}\mathbb{I}\{\ell(h(x_{\sigma(i)}),y_{\sigma(i)})>\gamma\}} {n}=0,\;\;\;\frac{\sum_{i=1}^{n}\mathbb{I}\{\ell(h(x_{\sigma(n+i)}),y_{\sigma(n +i)})>\gamma\}}{n}\geq\varepsilon/2\,.\]

We can relate this event with an event about the disambiguated partial concept class \(\overline{\mathcal{H}^{\prime}}\) since the number of 1's can only increase. In particular, for any swapping permutation \(\sigma\) of the \(2n\) points, if there exists a function in \(\mathcal{H}\) that is correct on the first \(n\) points and is off by at least \(\gamma\) on at least \(\varepsilon n/2\) of the remaining \(n\) points, then there is a function in the disambiguation \(\overline{\mathcal{H}^{\prime}}\) that is \(0\) on the first \(n\) points and is \(1\) on those same \(\epsilon n/2\) of the remaining points.

If we fix some \(\sigma\in\Gamma_{n}\), and some \(\overline{h^{\prime}}\in\overline{\mathcal{H}^{\prime}}\) then \(\overline{h^{\prime}}\) is a witness that \(\sigma(z)\in R_{\varepsilon,\gamma}\) only if \(\forall i\in[n]\) we do not have that \(\overline{h^{\prime}}(i)=1,\overline{h^{\prime}}(i+n)=1\). Thus at least one of \(\overline{h^{\prime}}(i),\overline{h^{\prime}}(n+i)\) must be zero. Moreover, at least \(n\varepsilon/2\) entries must be non-zero. Thus, when we draw random swapping permutation the probability that all the non-zero entries land on the second half of the sample sample is at most \(2^{-n\varepsilon/2}\).

Crucially since the number of possible functions is at most \(|\overline{\mathcal{H}^{\prime}}|\leq(2n)^{O(\mathbb{D}_{\gamma}^{\mathbb{G}} (\mathcal{H})\log(2n))}\) a union bound gives us that

\[\underset{\sigma\sim\cup(\Gamma_{n})}{\mathbf{Pr}}[\sigma(z)\in R_{ \varepsilon,\gamma}]\leq(2n)^{O(\mathbb{D}_{\gamma}^{\mathbb{G}}(\mathcal{H}) \log(2n))}\cdot 2^{-n\varepsilon/2}\,.\]

Thus, since \(z\in Z^{2n}\) was arbitrary we have that

\[\max_{z\in Z^{2n}}\underset{\sigma\in(\Gamma_{n})}{\mathbf{Pr}}[\sigma(z)\in R _{\varepsilon,\gamma}]\leq(2n)^{O(\mathbb{D}_{\gamma}^{\mathbb{G}}(\mathcal{H })\log(2n))}\cdot 2^{-n\varepsilon/2}\,.\]

This concludes the proof. 

Lower Bound for the ERM learner.Our next goal is to show that

\[\mathcal{M}_{\mathrm{ERM}}(\mathcal{H};\varepsilon,\delta,\gamma)\geq C_{0} \cdot\frac{\mathbb{D}_{\gamma}^{\mathbb{G}}(\mathcal{H})+\log(1/\delta)}{ \varepsilon}\,.\]

To this end, we will show that there exists an ERM learner satisfying this lower bound. This will establish that the finiteness of \(\gamma\)-graph dimension for any \(\gamma\in(0,1)\), is necessary for PAC learnability using _a worst-case_ ERM. It suffices to show that there exists a bad ERM algorithm that requires at least \(C_{0}\frac{\mathbb{D}_{\gamma}^{\mathbb{G}}(\mathcal{H})+\log(1/\delta)}{\varepsilon}\) samples to cut-off PAC learn \(\mathcal{H}\). First let us consider the case where \(d=\mathbb{D}_{\gamma}^{\mathbb{G}}(\mathcal{H})<\infty\) and let \(S=\{x_{1},....,x_{d}\}\) be a \(\gamma\)-graph-shattered set by \(\mathcal{H}\) with witness \(f_{0}\). Consider the ERM learner \(\mathcal{A}\) that works as follows: Upon seeing a sample \(T\subseteq S\) consistent with \(f_{0}\), \(\mathcal{A}\) returns a function \(\mathcal{A}(T)\) that is equal to \(f_{0}\) on elements of \(T\) and \(\gamma\)-far from \(f_{0}\) on \(S\setminus T\). Such a function exists since \(S\) is \(\gamma\)-graph-shattered with witness \(f_{0}\). Let us take \(\delta<1/100\) and \(\varepsilon<1/12\). Define a distribution over \(S\subseteq\mathcal{X}\) such that

\[\mathbf{Pr}[x_{1}]=1-2\varepsilon,\;\;\;\mathbf{Pr}[x_{i}]=2\varepsilon/(d-1 ),\;\forall i\in\{2,...,d\}\,.\]

Let us set \(h^{\star}=f_{0}\) and consider \(m\) samples \(\{(z_{i},f_{0}(z_{i})\}_{i\in[m]}\). Since we work in the scaled PAC model, \(\mathcal{A}\) will make a \(\gamma\)-error on all examples from \(S\) which are not in the sample (since in that case the output will be \(\gamma\)-far from the true label). Let us take \(m\leq\frac{d-1}{6\varepsilon}\). Then, the sample will include at most \((d-1)/2\) examples which are not \(x_{1}\) with probability \(1/100\), using Chernoff's bound. Conditioned on that event, this implies that the ERM learner will make a \(\gamma\)-error with probability at least \(\frac{2\varepsilon}{d-1}\cdot(d-1-\frac{d-1}{2})=\varepsilon\), over the random draw of the test point. Thus, \(\mathcal{M}_{\mathcal{A}}(\mathcal{H};\varepsilon,\delta,\gamma)=\Omega(\frac {d-1}{\varepsilon})\). Moreover, the probability that the sample will only contain \(x_{1}\) is \((1-2\varepsilon)^{m}\geq e^{-4\varepsilon m}\) which is greater that \(\delta\) whenever \(m\leq\log(1/\delta)/(4\varepsilon)\). This implies that the \(\gamma\)-cut-off ERM sample complexity is lower bounded by

\[\max\left\{\frac{d-1}{6\varepsilon},\frac{\log(1/\delta)}{2\varepsilon} \right\}=C_{0}\cdot\frac{\mathbb{D}_{\gamma}^{\mathbb{G}}(\mathcal{H})+\log(1/ \delta)}{\varepsilon}\,.\]

Thus \(\mathcal{M}_{\mathrm{ERM}}(\mathcal{H};\varepsilon,\delta,\gamma)\), satisfies the desired bound when the dimension is finite. Finally, it remains to claim about the case where \(\mathbb{D}_{\gamma}^{\mathbb{G}}(\mathcal{H})=\infty\) for the given \(\gamma\). We consider a sequence of \(\gamma\)-graph-shattered sets \(S_{n}\) with \(|S_{n}|=n\) and repeat the claim for the finite case. This will yield that for any \(n\) the cut-off ERM sample complexity is lower bounded by \(\Omega((n+\log(1/\delta))/\varepsilon)\) and this yields that \(\mathcal{M}_{\mathrm{ERM}}(\mathcal{H};\varepsilon,\delta,\gamma)=\infty\).

However, as Example 4 shows, the optimal learner cannot be proper and as a result, this dimension does not characterize PAC learnability for real-valued regression (there exist classes whose \(\gamma\)-graph dimension is infinite but are PAC learnable in the realizable regression setting).

## Appendix C \(\gamma\)-OIG Dimension and Learnability

In this section we identify a dimension characterizing qualitatively and quantitatively what classes of predictors \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) are PAC learnable and we provide PAC learners that achieve (almost) optimal sample complexity. In particular, we show the following result.

**Theorem 2**.: _Let \(\ell\) be the absolute loss function. For every class \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) and for any \(\varepsilon,\delta,\gamma\in(0,1)^{3},\) the sample complexity bound for realizable PAC regression satisfies_

\[\Omega\left(\frac{\mathbb{D}_{2\gamma}^{\mathrm{OIG}}(\mathcal{H})}{ \varepsilon}\right)\leq\mathcal{M}(\mathcal{H};\varepsilon,\delta,\gamma) \leq O\!\left(\frac{\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})}{ \varepsilon}\log^{2}\frac{\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})}{ \varepsilon}+\frac{1}{\varepsilon}\log\frac{1}{\delta}\right)\;.\]

_In particular, there exists an algorithm \(A\) such that_

\[\mathop{\mathbf{E}}_{x\sim\mathcal{D}_{\mathcal{X}}}[\ell(A(S;x),h^{\star}(x) ]\leq\inf_{\gamma\in[0,1]}\gamma+\widetilde{\Theta}\left(\frac{\mathbb{D}_{ \gamma}^{\mathrm{OIG}}(\mathcal{H})+\log(1/\delta)}{n}\right)\;,\]

_with probability at least \(1-\delta\) over \(S\sim\mathcal{D}^{n}\)._

A finite fat-shattering dimension implies a finite OIG dimension.Let \(\mathcal{F}\subseteq[0,1]^{\mathcal{X}}\) be a function class with finite \(\gamma\)-fat shattering dimension for any \(\gamma>0\). We show that \(\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{F})\) is upper bounded (up to constants and log factors) by \(\mathbb{D}_{c\gamma}^{\mathrm{f}at}(\mathcal{F})\), for some \(c>0\), where the OIG dimension is defined with respect to the \(\ell_{1}\) loss. Note that the opposite direction does not hold. Example 1 exhibits a function class with an infinite fat-shattering dimension that can be learned with a single example, and as a result, the OIG dimension has to be finite. On the one hand, we have an upper bound on the sample complexity of \(O\left(\frac{1}{\epsilon}\left(\mathbb{D}_{\varepsilon x}^{\mathrm{f}at}( \mathcal{F})\log^{2}\frac{1}{\epsilon}+\log\frac{1}{\delta}\right)\right)\), for any \(\varepsilon,\delta\in(0,1)\). See sections 19.6 and 20.4 about the restricted model in [1]. On the other hand, we prove in Lemma 6 a lower bound on the sample complexity of \(\Omega\left(\frac{\mathbb{D}_{2\gamma}^{\mathrm{OIG}}(\mathcal{F})}{ \varepsilon}\right)\), for any \(\varepsilon\in(0,1)\), and so \(\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{F})\) is upper bounded by \(\mathbb{D}_{c\gamma}^{\mathrm{f}at}(\mathcal{F})\) up to constants and log factors.

### Proof of the Lower Bound

**Lemma 6**.: _[Lower Bound of PAC Regression] Let \(A\) be any learning algorithm and \(\varepsilon,\delta,\gamma\in(0,1)^{3}\) such that \(\delta<\varepsilon\). Then,_

\[\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta,\gamma)\geq\Omega\left(\frac{ \mathbb{D}_{2\gamma}^{\mathrm{OIG}}(\mathcal{H})}{\varepsilon}\right)\;.\]

Proof.: Let \(n_{0}=\mathbb{D}_{2\gamma}^{\mathrm{OIG}}(\mathcal{H})\). Let \(n\in\mathbb{N},1<n\leq n_{0}\). We know that for each such \(n\) there exists some \(S\in\mathcal{X}^{n}\) such that the one-inclusion graph of \(\mathcal{H}|_{S}\) has the property that: there exists a finite subgraph \(G=(V,E)\) of \(G_{\mathcal{H}|_{S}}^{\mathrm{OIG}}\) such that for any orientation \(\sigma:E\to V\) of the subgraph, there exists a vertex \(v\in V\) with \(\mathrm{outdeg}(v;\sigma,2\gamma)>\mathbb{D}_{2\gamma}^{\mathrm{OIG}}(\mathcal{ H})/3\).

Given the learning algorithm \(A:(\mathcal{X}\times[0,1])^{\star}\times\mathcal{X}\to[0,1]\), we can describe an orientation \(\sigma_{\mathcal{A}}\) of the edges in \(E\). For any vertex \(v=(v_{1},\ldots,v_{n})\in V\) let \(P_{v}\) be the distribution over \((x_{1},v_{1}),....,(x_{n},v_{n})\) defined as

\[P_{v}((x_{1},v_{1}))=1-\varepsilon,\;P_{v}((x_{t},v_{t}))=\frac{\varepsilon}{ n-1},\;t\in\{2,\ldots,n\}\;.\]

Let \(m=n/(2\varepsilon)\). For each vertex \(v\in V\) and direction \(t\in[n]\), consider the hyperedge \(e_{t,v}\). For each \(u\in e_{t,v}\) we define

\[p_{t}(u)=\mathop{\mathbf{Pr}}_{S\sim P_{u}^{m}}[\ell(A(S;x_{t}),u_{t})>\gamma |(x_{t},u_{t})\notin S]\,,\]

and let \(C_{e_{t,v}}=\{u\in e_{t,v}:p_{t}(u)<1/2\}\). If \(C_{e_{t,v}}=\emptyset\), we orient the edge \(e_{t,v}\) arbitrarily. Since for all \(u,v\in e_{t,v}\) the distributions \(P_{u}^{m},P_{v}^{m}\) conditioned on the event that \((x_{t},u_{t}),(x_{t},v_{t})\) respectively are not in \(S\) are the same, we can see that \(\forall u,v\in C_{e_{t,v}}\) it holds that \(\ell(u_{t},v_{t})\leq 2\gamma\). We orient the edge \(e_{t,v}\) using an arbitrary element of \(C_{e_{t,v}}\).

Because of the previous discussion, we can bound from above the out-degree of all vertices \(v\in V\) with respect to the orientation \(\sigma_{A}\) as follows:

\[\mathrm{outdeg}(v;\sigma_{A},2\gamma)\leq\sum_{t}\mathbb{I}\{p_{t}(v)\geq 1/2 \}\leq 1+2\sum_{t=2}^{n}\underset{S\sim P_{v}^{m}}{\mathbf{Pr}}[\ell(A(S,x_{t}), y_{t})>\gamma|(x_{t},y_{t})\notin S]\,.\]

Notice that

\[\underset{S\sim P_{v}^{m}}{\mathbf{Pr}}[\ell(A(S,x_{t}),y_{t})>\gamma|(x_{t}, y_{t})\notin S]=\frac{\mathbf{Pr}_{S\sim P_{v}^{m}}[\{\ell(A(S,x_{t}),y_{t})> \gamma\}\wedge\{(x_{t},y_{t})\notin S\}]}{\mathbf{Pr}_{S\sim P_{v}^{m}}[(x_{t },y_{t})\notin S]}\,,\]

and by the definition of \(P_{v}\), we have that

\[\underset{S\sim P_{v}^{m}}{\mathbf{Pr}}[(x_{t},y_{t})\notin S]=\left(1-\frac {\varepsilon}{n-1}\right)^{m}\geq 1-\frac{n}{2(n-1)}\,,\]

since \(m=n/(2\varepsilon)\). Combining the above, we get that

\[\mathrm{outdeg}(v;\sigma_{A},2\gamma)\leq 1+2\left(1-\frac{n}{2(n-1)}\right) \sum_{t=2}^{n}\underset{S\sim P_{v}^{m}}{\mathbf{E}}[\mathbb{I}\{\ell(A(S,x_ {t}),y_{t})>\gamma\}\cdot\mathbb{I}\{(x_{t},y_{t})\notin S\}]\,,\]

and so

\[\mathrm{outdeg}(v;\sigma_{A},2\gamma) \leq 1+2\left(1-\frac{n}{2(n-1)}\right)\underset{S\sim P_{v}^{m}} {\mathbf{E}}\left[\sum_{t=2}^{n}\mathbb{I}\{\ell(A(S,x_{t}),y_{t})>\gamma\}\right]\] \[=1+2\left(1-\frac{n}{2(n-1)}\right)\frac{n-1}{\varepsilon} \underset{S\sim P_{v}^{m}}{\mathbf{E}}\left[\frac{\varepsilon}{n-1}\sum_{t=2 }^{n}\mathbb{I}\{\ell(A(S,x_{t}),y_{t})>\gamma\}\right]\] \[\leq 1+2\left(1-\frac{n}{2(n-1)}\right)\frac{n-1}{\varepsilon} \underset{S\sim P_{v}^{m}}{\mathbf{E}}\left[\underset{(x,y)\sim P_{v}}{ \mathbf{E}}[\mathbb{I}\{\ell(A(S;x),y)>\gamma\}]\right]\] \[=1+2\left(1-\frac{n}{2(n-1)}\right)\frac{n-1}{\varepsilon} \underset{S\sim P_{v}^{m}}{\mathbf{E}}\left[\underset{(x,y)\sim P_{v}}{ \mathbf{Pr}}[\ell(A(S;x),y)>\gamma]\right]\] \[\leq 1+\frac{n-2}{\varepsilon}\underset{S\sim P_{v}^{m}}{\mathbf{ E}}\left[\underset{(x,y)\sim P_{v}}{\mathbf{Pr}}[\ell(A(S;x),y)>\gamma]\right]\]

By picking "hard" distribution \(\mathcal{D}=P_{v^{\star}}\), where \(v^{\star}\in\arg\max_{v^{\prime}\in V}\mathrm{outdeg}(v^{\prime};\sigma_{A},2\gamma)\) we get that

\[\underset{S\sim P_{v}^{m}}{\mathbf{E}}\left[\underset{(x,y)\sim P _{v^{\star}}}{\mathbf{Pr}}[\ell(A(S;x),y)>\gamma]\right] \geq(\mathrm{outdeg}(v^{\star};\sigma_{A},2\gamma)-1)\cdot\frac{ \varepsilon}{n-2}\] \[\geq\frac{\varepsilon}{6}\,,\]

since \(\mathrm{outdeg}(v^{\star};\sigma_{A},2\gamma)>n/3\). By picking \(n=n_{0}\) we see that when the learner uses \(m=n_{0}/\varepsilon\) samples then its expected error is at least \(\varepsilon/6\). Notice that when the learner uses \(m^{\prime}=\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta,\gamma)\) samples we have that

\[\underset{S\sim P_{v}^{m^{\prime}}}{\mathbf{E}}\left[\underset{(x,y)\sim P_ {v^{\star}}}{\mathbf{Pr}}[\ell(A(S;x),y)>\gamma]\right]\leq\delta+(1-\delta) \varepsilon\leq\delta+\varepsilon\leq 2\varepsilon\,.\]

Thus, we see that for any algorithm \(A\)

\[\mathcal{M}_{A}(\mathcal{H};\varepsilon,\delta,\gamma)\geq\Omega\left(\frac{ \mathbb{D}_{2\gamma}^{\mathrm{OIG}}(\mathcal{H})}{\varepsilon}\right)\,,\]

hence

\[\mathcal{M}(\mathcal{H};\varepsilon,\delta,\gamma)\geq\Omega\left(\frac{ \mathbb{D}_{2\gamma}^{\mathrm{OIG}}(\mathcal{H})}{\varepsilon}\right)\,.\]

### Proof of the Upper Bound

Let us present the upper bound. For this proof, we need three tools: we will provide a weak learner based on the scaled one-inclusion graph, a boosting algorithm for real-valued functions, and consistent sample compression schemes for real-valued functions.

To this end, we introduce the one-inclusion graph (OIG) algorithm \(\mathbb{A}_{\gamma}^{\mathrm{OIG}}\) for realizable regression at scale \(\gamma\).

#### c.2.1 Scaled One-Inclusion Graph Algorithm and Weak Learning

First, we show that every scaled orientation \(\sigma\) of the one-inclusion graph gives rise to a learner \(\mathbb{A}_{\sigma}\) whose expected absolute loss is upper bounded by the maximum out-degree induced by \(\sigma\).

**Lemma 7** (From Orientations to Learners).: _Let \(\mathcal{D}_{\mathcal{X}}\) be a distribution over \(\mathcal{X}\) and \(h^{\star}\in\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\), let \(n\in\mathbb{N}\) and \(\gamma\in(0,1)\). Then, for any orientation \(\sigma:E_{n}\to V_{n}\) of the scaled-one-inclusion graph \(G_{\mathcal{H}}^{\mathrm{OIG}}=(V_{n},E_{n})\), there exists a learner \(\mathbb{A}_{\sigma}:(\mathcal{X}\times[0,1])^{n-1}\to[0,1]^{\mathcal{X}}\), such that_

\[\mathop{\mathbf{E}}_{S\sim\mathcal{D}_{\mathcal{X}}^{n-1}}\left[\mathop{ \mathbf{Pr}}_{x\sim\mathcal{D}_{\mathcal{X}}}\left[\ell(\mathbb{A}_{\sigma}(x ),h^{\star}(x))>\gamma\right]\right]\leq\frac{\max_{v\in V_{n}}\mathrm{outdeg} (v;\sigma,\gamma)}{n}\,,\]

_where \(\mathbb{A}_{\sigma}\) is trained using a sample \(S\) of size \(n-1\) realized by \(h^{\star}\)._

**Input:** An \(\mathcal{H}\)-realizable sample \(\{(x_{i},y_{i})\}_{i=1}^{n-1}\) and a test point \(x\in\mathcal{X}\), \(\gamma\in(0,1)\).

**Output:** A prediction \(\mathbb{A}_{\sigma}(x)\).

1. Create the one-inclusion graph \(G_{\mathcal{H}|(x_{1},\ldots,x_{n-1,x})}^{\mathrm{OIG}}\).
2. Consider the edge in direction \(n\) defined by the realizable sample \(\{(x_{i},y_{i})\}_{i=1}^{n-1}\); let \[e=\left\{h\in\mathcal{H}|_{(x_{1},\ldots,x_{n-1,x})}:\forall i\in[n-1]\;h(i)=y _{i}\right\}.\]
3. Return \(\mathbb{A}_{\sigma}(x)=\sigma(e)(n)\).

**Algorithm 1** From orientation \(\sigma\) to learner \(\mathbb{A}_{\sigma}\)

Proof.: By the classical leave-one-out argument, we have that

\[\mathop{\mathbf{E}}_{S\sim\mathcal{D}_{\mathcal{X}}^{n-1}}\left[\mathop{ \mathbf{Pr}}_{x\sim\mathcal{D}_{\mathcal{X}}}\left[\ell(\mathbb{A}_{\sigma}(x ),h^{\star}(x))>\gamma\right]\right]=\mathop{\mathbf{E}}_{S(x,y)\sim\mathcal{ D}^{n}}\left[\mathbb{I}\{\ell(h_{S}(x),y)>\gamma\}\right]=\mathop{\mathbf{E}}_{S^{ \prime}\sim\mathcal{D}^{n},I\sim\mathbb{U}([n])}\left[\mathbb{I}\{\ell(h_{S^{ \prime}_{-I}}(x^{\prime}_{I}),y^{\prime}_{I})>\gamma\}\right],\]

where \(h_{S}\) is the predictor \(\mathbb{A}_{\sigma}\) using the examples \(S\), and \(\mathbb{U}([n])\) is the uniform distribution on \(\{1,\ldots,n\}\). Now for every fixed \(S^{\prime}\) we have that

\[\mathop{\mathbf{E}}_{I\sim\mathbb{U}([n])}\left[\mathbb{I}\{\ell(h_{S^{ \prime}_{-I}}(x^{\prime}_{I}),y^{\prime}_{I})>\gamma\}\right]=\frac{1}{n}\sum_ {i\in[n]}\mathbb{I}\{\ell(\sigma(e_{i})(i),y^{\prime}_{i})>\gamma\}=\frac{ \mathrm{outdeg}(y^{\prime};\sigma,\gamma)}{n}\,,\]

where \(y^{\prime}\) is the node of the scaled OIG that corresponds to the true labeling of \(S^{\prime}\). By taking expectation over \(S^{\prime}\sim\mathcal{D}^{n}\) we get that

\[\mathop{\mathbf{E}}_{S^{\prime}\sim\mathcal{D}^{n},I\sim\mathbb{U}([n])}\left[ \mathbb{I}\{\ell(h_{S^{\prime}_{-I}}(x^{\prime}_{I}),y^{\prime}_{I})>\gamma\} \right]\leq\mathop{\mathbf{E}}_{S^{\prime}\sim\mathcal{D}^{n}}\left[\frac{ \mathrm{outdeg}(y^{\prime};\sigma,\gamma)}{n}\right]\leq\frac{\max_{v\in V_{n}} \mathrm{outdeg}(v;\sigma,\gamma)}{n}\,.\]

Equipped with the previous result, we are now ready to show that when the learner gets at least \(\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})\) samples as its training set, then its expected \(\gamma\)-cutoff loss is bounded away from \(1/2\).

**Lemma 8** (Scaled OIG Guarantee (Weak Learner)).: _Let \(\mathcal{D}_{\mathcal{X}}\) be a distribution over \(\mathcal{X}\) and \(h^{\star}\in\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\), and \(\gamma\in(0,1)\). Then, for all \(n>\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})\) there exists an orientation \(\sigma^{\star}\) such that for the prediction error of the one-inclusion graph algorithm \(\mathbb{A}_{\sigma^{\star}}^{\mathrm{OIG}}:(\mathcal{X}\times[0,1])^{n-1}\times \mathcal{X}\to[0,1]\), it holds that_

\[\mathop{\mathbf{E}}_{S\sim\mathcal{D}_{\mathcal{X}}^{n-1}}\left[\mathop{ \mathbf{Pr}}_{x\sim\mathcal{D}_{\mathcal{X}}}\left[\ell(\mathbb{A}_{\sigma^{ \star}}^{\mathrm{OIG}}(x),h^{\star}(x))>\gamma\right]\right]\leq 1/3\,.\]Proof.: Fix \(\gamma\in(0,1)\). Assume that \(n>\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})\) and let \(G^{\mathrm{OIG}}_{\mathcal{H}_{(S,s)}}=(V_{n},E_{n})\) be the possibly infinite scaled one-inclusion graph. By the definition of the \(\gamma\)-OIG dimension (see Definition 9), for every finite subgraph \(G=(V,E)\) of \(G^{\mathrm{OIG}}_{\mathcal{H}_{|S}}\) there exists an orientation \(\sigma:E\to V\) such that for every vertex in \(G\) the out-degree is at most \(n/3\), i.e.,

\[\forall S\in\mathcal{X}^{n},\;\forall\text{ finite }G=(V,E)\text{ of }G^{\mathrm{OIG}}_{\mathcal{H}_{|S}},\,\exists \text{ orientation }\sigma_{E}\text{ s.t. }\forall v\in V,\text{ it holds }\mathrm{outdeg}(v;\sigma_{E},\gamma)\leq n/3\,.\]

First, we need to create an orientation of the whole (potentially infinite) one-inclusion graph.

We will create this orientation using the compactness theorem of first-order logic which states that a set of formulas \(\Phi\) is satisfiable if and only if it is finitely satisfiable, i.e., every finite subset \(\Phi^{\prime}\subseteq\Phi\) is satisfiable. Let \(G^{\mathrm{OIG}}_{\mathcal{H}_{|S}}=(V_{n},E_{n})\) be the (potentially infinite) one-inclusion graph of \(\mathcal{H}|_{S}\). Let \(\mathcal{Z}\) be the set of pairs \(z=(v,e)\in V_{n}\times E_{n}\) so that \(v\in e\). Our goal is to assign binary values to each \(z\in\mathcal{Z}\). We define the following sets of formulas:

* For each \(e\in E_{n}\) we let \(\Phi_{e}\coloneqq\exists\) exactly one \(v\in e:z(v,e)=1\).
* For each \(v\in V_{n}\) we let \(\Phi_{v}\coloneqq\exists\text{ at most }n/3\text{ different }e_{i,f}\in E_{n}\ :\ v\in e_{i,f} \wedge(\exists v^{\prime}\in e_{i,f}:(z(v^{\prime},e)=1\wedge\ell(v^{\prime}_{ i},v_{i})>\gamma))\)

It is not hard to see that each \(\Phi_{e},\Phi_{v}\) can be expressed in first-order logic. Then, we define

\[\Phi\coloneqq\left(\bigcap_{e\in E_{n}}\Phi_{e}\right)\cap\left(\bigcap_{v\in V _{n}}\Phi_{v}\right)\,.\]

Notice that an orientation of the edges of \(G^{\mathrm{OIG}}_{\mathcal{H}_{|S}}\) is equivalent to picking an assignment of the elements of \(\mathcal{Z}\) that satisfies all the \(\Phi_{e}\). Moreover, notice that for such an assignment, if all the \(\Phi_{v}\) are satisfied then then maximum \(\gamma\)-scaled out-degree of \(G^{\mathrm{OIG}}_{\mathcal{H}_{|S}}\) is at most \(n/3\).

We will now show that \(\Phi\) is finitely satisfiable. Let \(\Phi^{\prime}\) be a finite subset of \(\Phi\) and let \(E^{\prime}\subseteq E_{n},V^{\prime}\subseteq V_{n}\), be the set of edges, vertices that appear in \(\Phi^{\prime}\), respectively. If \(V^{\prime}=\emptyset\), then we can orient the edges in \(E^{\prime}\) arbitrarily and satisfy \(\Phi^{\prime}\). Similarly, if \(E^{\prime}=\emptyset\) we can let all the \(z(e,v)=0\) and satisfy all the \(\Phi_{v},v\in V^{\prime}\). Thus, assume that both sets are non-empty. Consider the finite subgraph of \(G^{\mathrm{OIG}}_{\mathcal{H}_{|S}}\) that is induced by \(V^{\prime}\) and let \(E^{\prime\prime}\) be the set of edges of this subgraph. For every edge \(e\in E^{\prime}\setminus E^{\prime\prime}\), pick an arbitrary orientation, i.e, for exactly one \(v\in e\) set \(z(e,v)=1\) and for the remaining \(v^{\prime}\in e\) set \(z(e,v^{\prime})=0\). By the definition of \(\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})\) there is an orientation \(\sigma_{E^{\prime\prime}}\) of the edges in \(E^{\prime\prime}\) such that \(\forall v\in V^{\prime}\mathrm{outdeg}(v;\sigma_{E^{\prime\prime}},\gamma) \leq n/3\). For every \(e\in E^{\prime\prime}\) pick the assignment of all the \(z(v,e),v\in e\), according to the orientation \(\sigma_{E^{\prime\prime}}\). Thus, because of the maximum out-degree property of \(\sigma_{E^{\prime\prime}}\) we described before, we can also see that all the \(\Phi_{v},v\in V^{\prime}\), are satisfied. Hence, we have shown that \(\Phi\) is finitely satisfiable, so it is satisfiable. This assignment on \(z(v,e)\) induces an orientation \(\sigma^{\star}\) under which all the vertices of the one-inclusion graph have out-degree at most \(n/3\).

We will next use the orientation \(\sigma^{\star}\) of \(G^{\mathrm{OIG}}_{\mathcal{H}_{|S}}=(V_{n},E_{n})\) to design a learner \(\mathbb{A}_{\sigma^{\star}}^{\mathrm{OIG}}:(\mathcal{X}\times[0,1])^{n-1} \times\mathcal{X}\to[0,1]\), invoking Lemma 7. In particular, we get that, from Lemma 7 with the chosen orientation,

\[\underset{S\sim\mathcal{D}_{\mathcal{X}}^{n-1}}{\mathbf{E}}\left[\underset{x \sim\mathcal{D}_{\mathcal{X}}}{\mathbf{Pr}}\left[\ell(\mathbb{A}_{\sigma^{ \star}}^{\mathrm{OIG}}(x),h^{\star}(x))>\gamma\right]\right]\leq\frac{\max_{ v\in V_{n}}\mathrm{outdeg}(v;\sigma^{\star},\gamma)}{n}\leq 1/3\,,\]

which concludes the proof. 

#### c.2.2 Boosting Real-Valued Functions

**Definition 15** (Weak Real-Valued Learner).: _Let \(\ell\) be a loss function. Let \(\gamma\in[0,1]\), \(\beta\in(0,\frac{1}{2})\), and \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\). For a distribution \(\mathcal{D}_{\mathcal{X}}\) over \(\mathcal{X}\) and true target function \(h^{\star}\in\mathcal{H}\), we say that \(f:\mathcal{X}\to[0,1]\) is \((\gamma,\beta)\)-weak learner with respect to \(\mathcal{D}_{\mathcal{X}}\) and \(h^{\star}\), if_

\[\underset{x\sim\mathcal{D}_{\mathcal{X}}}{\mathbf{Pr}}\left[\ell(f(x),h^{\star} (x))>\gamma\right]<\frac{1}{2}-\beta.\]Following [11], we define the weighted median as

\[\mathrm{Median}(y_{1},\ldots,y_{T};\alpha_{1},\ldots,\alpha_{T})=\min\Biggl{\{}y _{j}:\frac{\sum_{t=1}^{T}\alpha_{t}\mathbb{I}[y_{j}<y_{t}]}{\sum_{t=1}^{T} \alpha_{t}}<\frac{1}{2}\Biggr{\}}\;,\]

and the weighted quantiles, for \(\theta\in[0,1/2]\), as

\[Q_{\theta}^{+}(y_{1},\ldots,y_{T};\alpha_{1},\ldots,\alpha_{T})= \min\Biggl{\{}y_{j}:\frac{\sum_{t=1}^{T}\alpha_{t}\mathbb{I}[y_{j}<y_{t}]}{ \sum_{t=1}^{T}\alpha_{t}}<\frac{1}{2}-\theta\Biggr{\}}\] \[Q_{\theta}^{-}(y_{1},\ldots,y_{T};\alpha_{1},\ldots,\alpha_{T})= \max\Biggl{\{}y_{j}:\frac{\sum_{t=1}^{T}\alpha_{t}\mathbb{I}[y_{j}>y_{t}]}{ \sum_{t=1}^{T}\alpha_{t}}<\frac{1}{2}-\theta\Biggr{\}}\;,\]

and we let \(Q_{\theta}^{+}(x)=Q_{\theta}^{+}(h_{1}(x),\ldots,h_{T}(x);\alpha_{1},\ldots, \alpha_{T}),Q_{\theta}^{-}(x)=Q_{\theta}^{-}(h_{1}(x),\ldots,h_{T}(x);\alpha_ {1},\ldots,\alpha_{T})\), where \(h_{1},\ldots,h_{T},\alpha_{1},\ldots,\alpha_{T}\) are the values returned by Algorithm 2. The following guarantee holds for this procedure.

**Lemma 9** (\(\mathrm{MedBoost}\) guarantee [10]).: _Let \(\ell\) be the absolute loss and \(S=\{(x_{i},y_{i})\}_{i=1}^{m}\), \(T=O\bigl{(}\frac{1}{\theta^{2}}\log(m)\bigr{)}\). Let \(h_{1},\ldots,h_{T}\) and \(\alpha_{1},\ldots,\alpha_{T}\) be the functions and coefficients returned from \(\mathrm{MedBoost}\). For any \(i\in\{1,\ldots,m\}\) it holds that_

\[\max\Bigl{\{}\ell\Bigl{(}Q_{\theta/2}^{+}(x_{i}),y_{i}\Bigr{)}\,,\ell\Bigl{(} Q_{\theta/2}^{-}(x_{i}),y_{i}\Bigr{)}\Bigr{\}}\leq\gamma.\]

**Input:**\(S=\{(x_{i},y_{i})\}_{i=1}^{m}\).

**Parameters:**\(\gamma,\beta,T\).

**Initialize**\(\mathcal{P}_{1}\) = Uniform(\(S\)).

For \(t=1,\ldots,T\):

1. Find a \((\gamma,\beta)\)-weak learner \(h_{t}\) with respect to \((x_{i},y_{i})\sim\mathcal{P}_{t}\), using a subset \(S_{t}\subseteq S\).
2. For \(i=1,\ldots,m\): 1. Set \(w_{i}^{(t)}=1-2\mathbb{I}\bigl{[}\ell(h_{t}(x_{i}),y_{i})>\gamma\bigr{]}\). 2. Set \(\alpha_{t}=\frac{1}{2}\log\biggl{(}\frac{(1-\gamma)\sum_{i=1}^{n}\mathcal{P}_{ t}(x_{i},y_{i})\mathbb{I}\left[w_{i}^{(t)}=1\right]}{(1+\gamma)\sum_{i=1}^{n} \mathcal{P}_{t}(x_{i},y_{i})\mathbb{I}\left[w_{i}^{(t)}=-1\right]}\biggr{)}\). 3. * If \(\alpha_{t}=\infty\): return \(T\) copies of \(h_{t}\), \((\alpha_{1}=1,\ldots,\alpha_{T}=1)\), and \(S_{t}\). * Else: \(P_{t+1}(x_{i},y_{i})=P_{t}(x_{i},y_{i})\exp(-\alpha_{t}w_{i}^{t})/Z_{t}\), where \(Z_{t}=\sum_{j=1}^{n}\mathcal{P}_{t}(x_{j},y_{j})\exp\bigl{(}-\alpha_{t}w_{j}^ {t}\bigr{)}\).

**Output:** Functions \(h_{1},\ldots,h_{T}\), coefficients \(\alpha_{1},\ldots,\alpha_{T}\) and sets \(S_{1},\ldots,S_{T}\).

**Algorithm 2**\(\mathrm{MedBoost}\)[10]

#### c.2.3 Generalization via Sample Compression Schemes

Sample compression scheme is a classic technique for proving generalization bounds, introduced by [12, 13]. These bounds proved to be useful in numerous learning settings, such as binary classification [1, 14, 15], multiclass classification [1, 16, 17, 18], regression [19, 18], active learning [14], density estimation [1], adversarially robust learning [19, 18, 17, 16, 15], adversary to Rural learning with partial concepts [1, 18, 19], and showing Bayes-consistency for nearest-neighbor methods [15, 16]. As a matter of fact, compressibility and learnability are known to be equivalent for general learning problems [17]. Another remarkable result by [19] showed that VC classes enjoy a sample compression that is independent of the sample size.

We start with a formal definition of a sample compression scheme.

**Definition 16** (Sample compression scheme).: _A pair of functions \((\kappa,\rho)\) is a sample compression scheme of size \(\ell\) for class \(\mathcal{H}\) if for any \(n\in\mathbb{N}\), \(h\in\mathcal{H}\) and sample \(S=\{(x_{i},h(x_{i}))\}_{i=1}^{n}\), it holds for the compression function that \(\kappa\left(S\right)\subseteq S\) and \(\left|\kappa\left(S\right)\right|\leq\ell\), and the reconstruction function \(\rho\left(\kappa\left(S\right)\right)=\hat{h}\) satisfies \(\hat{h}(x_{i})=h(x_{i})\) for any \(i\in[n]\)._We show a generalization bound that scales with the sample compression size. The proof follows from [10].

**Lemma 10** (Sample compression scheme generalization bound).: _Fix a margin \(\gamma\in[0,1]\). For any \(k\in\mathbb{N}\) and fixed function \(\phi:(\mathcal{X}\times[0,1]^{k}\to[0,1]^{\mathcal{X}}\), for any distribution \(\mathcal{D}\) over \(\mathcal{X}\times[0,1]\) and any \(m\in\mathbb{N}\), for \(S=\{(x_{i},y_{i})\}_{i\in[m]}\) i.i.d. \(\mathcal{D}\)-distributed random variables, if there exist indices \(i_{1},...,i_{k}\in[m]\) such that \(\sum_{(x,y)\in S}\mathbb{I}\{\ell(\phi((x_{i_{1}},y_{i_{1}}),...,(x_{i_{k}},y_ {i_{k}}))(x),y)>\gamma\}=0\), then_

\[\underset{(x,y)\sim\mathcal{D}}{\mathbf{E}}\left[\mathbb{I}\{\ell(\phi((x_{i_{ 1}},y_{i_{1}}),...,(x_{i_{k}},y_{i_{k}}))(x),y)>\gamma\}\right]\leq\frac{1}{m-k }(k\log m+\log(1/\delta))\,.\]

_with probability at least \(1-\delta\) over \(S\)._

Proof.: Let us define \(\widehat{\ell}_{\gamma}(h;S)=\frac{1}{|S|}\sum_{(x,y)\in S}\mathbb{I}\{\ell(h( x),y)>\gamma\}\) and \(\ell_{\gamma}(h;\mathcal{D})=\mathbf{E}_{(x,y)\sim\mathcal{D}}\left[\mathbb{I} \{\ell(h(x),y)>\gamma\}\right]\). For any indices \(i_{1},...,i_{k}\in[m]\), the probability of the bad event

\[\underset{S\sim\mathcal{D}^{m}}{\mathbf{Pr}}[\widehat{\ell}_{\gamma}(\phi((x_ {i_{1}},y_{i_{1}}),...,(x_{i_{k}},y_{i_{k}}));S)=0\wedge\ell_{\gamma}(\phi((x_ {i_{1}},y_{i_{1}}),...,(x_{i_{k}},y_{i_{k}}));\mathcal{D})>\varepsilon]\]

is at most

\[\mathbf{E}\left[\mathbb{I}\{\ell_{\gamma}(\phi(\{(x_{i_{j}},y_{i_ {j}})\}_{j\in[k]});\mathcal{D})>\varepsilon\}\,\mathbf{Pr}[\widehat{\ell}_{ \gamma}(\phi(\{(x_{i_{j}},y_{i_{j}})\}_{j\in[k]});S\setminus\{(x_{i_{j}},y_{i_{ j}})\}_{j\in[k]})=0|\{(x_{i_{j}},y_{i_{j}})\}_{j\in[k]}]\right]\] \[<(1-\varepsilon)^{m-k}\]

where the expectation is over \((x_{i_{1}},y_{i_{1}}),...,(x_{i_{k}},y_{i_{k}})\) and the inner probability is over \(S\setminus(x_{i_{1}},y_{i_{1}}),...,(x_{i_{k}},y_{i_{k}})\). Taking a union bound over all \(m^{k}\) possible choices for the \(k\) indices, we get that the bad event occurs with probability at most

\[m^{k}\exp(-\varepsilon(m-k))\leq\delta\Rightarrow\varepsilon=\frac{1}{m-k}(k \log m+\log(1/\delta))\,.\]

### Putting it Together

We now have all the necessary ingredients in place to prove the upper bound of Theorem2. First, we use Lemma8 on a sample of size \(n_{0}=\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})\) to obtain a learner which makes \(\gamma\)-errors with probability at most \(1/3\)5. Then, we use the boosting algorithm we described (see Algorithm2) to obtain a learner that does not make any \(\gamma\)-mistakes on the training set. Notice that the boosting algorithm on its own does not provide any guarantees about the generalization error of the procedure. This is obtained through the sample compression result we described in SectionC.2.3. Since we run the boosting algorithm for a few rounds on a sample whose size is small, we can provide a sample compression scheme following the approach of [11, 13].

Footnote 5: In expectation over the training set.

**Lemma 11** (Upper Bound of PAC Regression).: _Let \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) and \(\varepsilon,\delta,\gamma\in(0,1)^{3}\). Then,_

\[\mathcal{M}(\mathcal{H};\varepsilon,\delta,\gamma)\leq O\left(\frac{\mathbb{D }_{\gamma}^{\mathrm{OIG}}(\mathcal{H})}{\varepsilon}\log^{2}\frac{\mathbb{D}_ {\gamma}^{\mathrm{OIG}}(\mathcal{H})}{\varepsilon}+\frac{1}{\varepsilon}\log \frac{1}{\delta}\right)\,.\]

Proof.: Let \(n\) be the number of samples \(S=((x_{1},y_{1}),\ldots,(x_{n},y_{n}))\) that are available to the learner, \(n_{0}=\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})\) and let \(A\) be the algorithm obtained from Lemma8. We have that

\[\underset{S\sim\mathcal{D}_{\mathcal{X}}^{n_{0}-1}}{\mathbf{E}}\left[ \underset{x\sim\mathcal{D}_{\mathcal{X}}}{\mathbf{Pr}}\left[\ell(A(S;x),h^{ \star}(x))>\gamma\right]\right]\leq 1/3\,.\]

This means that, for any distribution \(\mathcal{D}_{\mathcal{X}}\) and any labeling function \(h^{\star}\in\mathcal{H}\) we can draw a sample \(S^{\star}=((x_{1},y_{1}),\ldots,(x_{n_{0}-1},y_{n_{0}-1}))\) with non-zero probability such that

\[\underset{x\sim\mathcal{D}_{\mathcal{X}}}{\mathbf{Pr}}\left[\ell(A(S^{\star};x),h^{\star}(x))>\gamma\right]\leq\frac{1}{3}\,.\]Notice that such a classifier is a \((\gamma,1/6)\)-weak learner (see Definition15). Thus, by executing the \(\mathrm{MedBoost}\) algorithm (see Algorithm2) for \(T=O(\log n)\) rounds we obtain a classifier \(\hat{h}:\mathcal{X}\to\mathbb{R}\) such that, \(\ell(\hat{h}(x_{i}),y_{i})\leq\gamma,\forall i\in[n]\). We underline that the subset \(S_{t}\) that is used in line1 of Algorithm2 has size at most \(n_{0}\), for all rounds \(t\in[T]\). Thus, the total number of samples that is used by \(\mathrm{MedBoost}\) is at most \(O(n_{0}\log n)\). Hence, following the approach of [13] we can encode the classifiers produced by \(\mathrm{MedBoost}\) as a compression set that consists of \(k=O(n_{0}\log n)\) samples that were used to train the classifiers along with \(k\log k\) extra bits that indicate their order. Thus, using generalization based on sample compression scheme as in Lemma10, we have that with probability at least \(1-\delta\) over \(S\sim\mathcal{D}^{n}\),

\[\mathop{\mathbf{E}}_{(x,y)\sim\mathcal{D}}\left[\mathbb{I}\Big{\{}\ell(\hat{h} (x),y)>\gamma\Big{\}}\right]\leq\frac{C}{n-n_{0}\log(n)}\left(n_{0}\log^{2}n+ \log(1/\delta)\right)\,,\]

which means that for large enough \(n\),

\[\mathop{\mathbf{E}}_{(x,y)\sim\mathcal{D}}\left[\mathbb{I}\Big{\{}\ell(\hat{h }(x),y)>\gamma\Big{\}}\right]\leq O\left(\frac{n_{0}\log^{2}n}{n}+\frac{\log(1 /\delta)}{n}\right)\,.\]

Thus,

\[\mathop{\mathbf{Pr}}_{(x,y)\sim\mathcal{D}}\left[\ell(\hat{h}(x),y)>\gamma \right]\leq O\left(\frac{n_{0}\log^{2}n}{n}+\frac{\log(1/\delta)}{n}\right)\,.\]

Hence, we can see that

\[\mathcal{M}(\mathcal{H};\varepsilon,\delta,\gamma)\leq O\Bigg{(}\frac{ \mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})}{\varepsilon}\log^{2}\frac{ \mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})}{\varepsilon}+\frac{1}{ \varepsilon}\log\frac{1}{\delta}\Bigg{)}\,.\]

## Appendix D \(\gamma\)-DS Dimension and Learnability

In this section, we will show that finiteness of \(\gamma\)-DS dimension is necessary for PAC learning in the realizable case.

**Theorem 3**.: _Let \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}},\varepsilon,\delta,\gamma\in(0,1)^{3}\). Then,_

\[\mathcal{M}(\mathcal{H};\varepsilon,\delta,\gamma)\geq\Omega\left(\frac{ \mathbb{D}_{2\gamma}^{\mathrm{DS}}(\mathcal{H})+\log(1/\delta)}{\varepsilon} \right)\,.\]

Proof.: Let \(d=\mathbb{D}_{2\gamma}^{\mathrm{OIG}}(\mathcal{H})\). Then, there exists some \(S=(x_{1},\ldots,x_{d})\in\mathcal{X}^{d}\) such that \(\mathcal{H}|_{S}\) contains a \(2\gamma\)-pseudo-cube, which we call \(\mathcal{H}^{\prime}\). By the definition of the scaled pseudo-cube, \(\forall h\in\mathcal{H}^{\prime},i\in[d]\), there is exactly one \(h^{\prime}\in\mathcal{H}^{\prime}\) such that \(\hat{h}(x_{j})=h^{\prime}(x_{j}),j\neq i\), and \(\ell(h(x_{i}),h^{\prime}(x_{i}))>2\gamma\). We pick the target function \(h^{*}\) uniformly at random among the hypotheses of \(\mathcal{H}^{\prime}\) and we set the marginal distribution \(\mathcal{D}_{\mathcal{X}}\) of \(\mathcal{D}\) as follows

\[\mathop{\mathbf{Pr}}[x_{1}]=1-2\varepsilon,\ \ \mathop{\mathbf{Pr}}[x_{i}]=2 \varepsilon/(d-1),\ \forall i\in\{2,...,d\}\,.\]

Consider \(m\) samples \(\{(z_{i},h^{*}(z_{i})\}_{i\in[m]}\) drawn i.i.d. from \(\mathcal{D}\). Let us take \(m\leq\frac{d-1}{6\varepsilon}\). Then, the sample will include at most \((d-1)/2\) examples which are not \(x_{1}\) with probability \(1/100\), using Chernoff's bound. Let us call this event \(E\). Conditioned on \(E\), the posterior distribution of the unobserved points is uniform among the vertices of the \(d/2\)-dimensional \(2\gamma\)-pseudo-cube. Thus, if the test point \(x\) falls among the unobserved points, the learner will make a \(\gamma\)-mistake with probability at least \(1/2\). To see that, let \(\widehat{y}\) be the prediction of the learner on \(x\). Since every hyperedge has size at least \(2\) and all the vertices that are on the hyperedge differ by at least \(2\gamma\) in the direction of \(x\), no matter what \(\widehat{y}\) is the correct label \(y^{*}\) is at least \(\gamma\)-far from it. Since \(\mathop{\mathbf{Pr}}[E]\geq 1/100\), we can see that \(\mathcal{M}_{\mathcal{A}}(\mathcal{H};\varepsilon,\delta,\gamma)=\Omega(\frac{ d}{\varepsilon})\). Moreover, by the law of total probability there must exist a deterministic choice of the target function \(h^{\star}\), that could depend on \(\mathcal{A}\), which satisfies the lower bound. For the other part of the lower bound, notices the probability that the sample will only contain \(x_{1}\) is \((1-2\varepsilon)^{m}\geq e^{-4\varepsilon m}\) which is greater that \(\delta\) whenever \(m\leq\log(1/\delta)/(4\varepsilon)\). This implies that the \(\gamma\)-cut-off sample complexity is lower bounded by

\[\max\left\{C_{1}\cdot\frac{d}{\varepsilon},C_{2}\cdot\frac{\log(1/\delta)}{ \varepsilon}\right\}=C_{0}\cdot\frac{\mathbb{D}_{2\gamma}^{\mathrm{DS}}( \mathcal{H})+\log(1/\delta)}{\varepsilon}\,.\]

Thus \(\mathcal{M}_{\mathcal{A}}(\mathcal{H};\varepsilon,\delta,\gamma)\), satisfies the desired bound when the dimension is finite. Finally, it remains to claim about the case where \(\mathbb{D}_{2\gamma}^{\mathrm{DS}}(\mathcal{H})=\infty\) for the given \(\gamma\). We consider a sequence of \(2\gamma\)-DS shattered sets \(S_{n}\) with \(|S_{n}|=n\) and repeat the claim for the finite case. This will yield that for any \(n\) the \(\gamma\)-cut-off sample complexity is lower bounded by \(\Omega((n+\log(1/\delta))/\varepsilon)\) and this yields that \(\mathcal{M}(\mathcal{H};\varepsilon,\delta,\gamma)=\infty\). 

We further conjecture that this dimension is also sufficient for PAC learning.

**Conjecture 2**.: _A class \(\mathcal{H}\subseteq(0,1)^{\mathcal{X}}\) is PAC learnable in the realizable regression setting with respect to the absolute loss function if and only if \(\mathbb{D}_{\gamma}^{\mathrm{DS}}(\mathcal{H})<\infty\) for any \(\gamma\in(0,1)\)._

We believe that there must exist a modification of the approach of [1] that will be helpful in settling the above conjecture.

**Conjecture 3**.: _There exists \(\mathcal{H}\subseteq(0,1)^{\mathcal{X}}\) for which \(\mathbb{D}_{\gamma}^{\mathrm{Nat}}(\mathcal{H})=1\) for all \(\gamma\in(0,1)\) but \(\mathbb{D}_{\gamma}^{\mathrm{DS}}(\mathcal{H})<\infty\) for some \(\gamma\in(0,1)\)._

In particular, we believe that one can extend the construction of [1] (which uses various tools from algebraic topology as a black-box) and obtain a hypothesis class \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) that has \(\gamma\)-Natarajan dimension 1 but is not PAC learnable (it will have infinite \(\gamma\)-DS dimension). This construction though is not immediate and requires new ideas related to the works of [13, 14]

## Appendix E Online Realizable Regression

In this section, we present our results regarding online realizable regression. The next result resolves an open question of [10]. It provides an online learner with optimal (off by a factor of 2) cumulative loss in realizable regression.

**Theorem 4** (Optimal Cumulative Loss).: _Let \(\mathcal{H}\subseteq[0,1]^{\mathcal{X}}\) and \(\varepsilon>0\). Then, there exists a deterministic algorithm (Algorithm 3) whose cumulative loss in the realizable setting is bounded by \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})+\varepsilon\). Conversely, for any \(\varepsilon>0\), every deterministic algorithm in the realizable setting incurs loss at least \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})/2-\varepsilon\)._

``` Parameters:\(\left\{\varepsilon_{t}\right\}_{t\in\mathbb{N}}\). Initialize \(V^{(1)}=\mathcal{H}\). For \(t=1,\ldots\):

1. Receive \(x_{t}\in\mathcal{X}\).
2. For every \(y\in[0,1]\), let \(V^{(t)}_{(x_{t},y)}=\left\{h\in V^{(t)}:h(x_{t})=y\right\}\).
3. Let \(\widehat{y}_{t}\) be an arbitrary label such that \[\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},\widehat{y}_{t})}\right)\geq \sup_{y^{\prime}}\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},y^{\prime})} \right)-\varepsilon_{t}\,.\]
4. Predict \(\widehat{y}_{t}\).
5. Receive the true label \(y_{t}^{\star}\) and incur loss \(\ell(\widehat{y}_{t},y_{t}^{\star})\).
6. Update \(V^{(t+1)}=\left\{h\in V^{(t)}:h(x_{t})=y_{t}^{\star}\right\}.\) ```

**Algorithm 3** Scaled SOA

Proof.: Let us begin with the upper bound. Assume that \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})<\infty\). Suppose we are predicting on the \(t\)-th point in the sequence and let \(V^{(t)}\) be the version space so far, i.e.,\(\{h\in\mathcal{H}:\forall\tau\in[t-1],h(x_{\tau})=y_{\tau}\}\). Let \(x_{t}\) be the next point to predict on. For each label \(y\in\mathbb{R}\), let \(V^{(t)}_{(x_{t},y)}=\{h\in V^{(t)}:h(x_{t})=y\}\). From the definition of the dimension \(\mathbb{D}^{\mathrm{onl}}\), we know that for all \(y,y^{\prime}\in\mathbb{R}\) such that \(V^{(t)}_{(x_{t},y)},V^{(t)}_{(x_{t},y^{\prime})}\neq\emptyset\),

\[\mathbb{D}^{\mathrm{onl}}(V^{t})\geq\ell(y,y^{\prime})+\min\Bigl{\{}\mathbb{D }^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},y)}\right),\mathbb{D}^{\mathrm{onl}} \left(V^{(t)}_{(x_{t},y^{\prime})}\right)\Bigr{\}}\.\]

Let \(\widehat{y}_{t}\) be an arbitrary label with \(\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},\widehat{y}_{t})}\right)\geq \sup_{y^{\prime}}\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},y^{\prime})} \right)-\varepsilon_{t}\), where \(\varepsilon_{t}\) is some sequence shrinking arbitrarily quickly in the number of rounds \(t\). The learner predicts \(\widehat{y}_{t}\). Assume that the adversary picks \(y^{\star}_{t}\) as the true label and, so, the learner incurs loss \(\ell(\widehat{y}_{t},y^{\star}_{t})\) at round \(t\). Then, the updated version space \(V^{(t)}_{(x_{t},y^{\star}_{t})}\) has

\[\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},y^{\star}_{t})}\right)\leq \sup_{y^{\prime}}\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},y^{\prime})} \right)\leq\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},\widehat{y}_{t})} \right)+\varepsilon_{t}\,,\]

which implies

\[\min\Bigl{\{}\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},\widehat{y}_{t})} \right),\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},y^{\star}_{t})} \right)\Bigr{\}}\geq\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},y^{\star}_ {t})}\right)-\varepsilon_{t}\,.\]

This gives that

\[\mathbb{D}^{\mathrm{onl}}(V^{(t)}) \geq\ell(\widehat{y}_{t},y^{\star}_{t})+\min\Bigl{\{}\mathbb{D}^{ \mathrm{onl}}\left(V^{(t)}_{(x_{t},\widehat{y}_{t})}\right),\mathbb{D}^{ \mathrm{onl}}\left(V^{(t)}_{(x_{t},y^{\star}_{t})}\right)\Bigr{\}}\] \[\geq\ell(\widehat{y}_{t},y^{\star}_{t})+\mathbb{D}^{\mathrm{onl}} \left(V^{(t)}_{(x_{t},y^{\star}_{t})}\right)-\varepsilon_{t}\,,\]

and, by re-arranging,

\[\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{(x_{t},y^{\star}_{t})}\right)\leq \mathbb{D}^{\mathrm{onl}}(V^{(t)})-\ell(\widehat{y}_{t},y^{\star}_{t})+ \varepsilon_{t}\,.\] (1)

So every round reduces the dimension by at least the magnitude of the loss (minus \(\varepsilon_{t}\)). Notice that \(\mathbb{D}^{\mathrm{onl}}(V^{(t+1)})=\mathbb{D}^{\mathrm{onl}}\left(V^{(t)}_{ (x_{t},y^{\star}_{t})}\right)\). Thus, by choosing the \(\left\{\varepsilon_{t}\right\}_{t\in\mathbb{N}}\) sequence such that

\[\sum_{t}\varepsilon_{t}\leq\varepsilon^{\prime}\,,\]

and summing up Equation1 over all \(t\in\mathbb{N}\), we get a cumulative loss bound

\[\sum_{t}\ell(\widehat{y}_{t},y^{\star}_{t})\leq\mathbb{D}^{\mathrm{onl}}( \mathcal{H})+\varepsilon^{\prime}\,.\]

Hence, we see that by taking the limit as \(\varepsilon^{\prime}\) goes to 0 shows that the cumulative loss is upper bounded by \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})\). This analysis shows that Algorithm3 achieves the cumulative loss bound \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})+\varepsilon^{\prime}\), for arbitrarily small \(\varepsilon^{\prime}>0\).

Let us continue with the lower bound. For any \(\varepsilon>0\), we are going to prove that any deterministic learner must incur cumulative loss at least \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})/2-\varepsilon\). By the definition of \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})\), for any \(\varepsilon>0\), there exists a tree \(T_{\varepsilon}\) such that, for every path \(y\),

\[\sum_{i=1}^{\infty}\gamma_{y\leq i}\geq\mathbb{D}^{\mathrm{onl}}(\mathcal{H})-2 \varepsilon\,,\]

i.e., the sum of the gaps across the path is at least \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})-2\varepsilon\). The strategy of the adversary is the following: in the first round, she presents the learner with the instance \(x_{1}=x_{0}\). Then, no matter what label \(\hat{y}_{1}\) the learner picks, the adversary can choose the label \(y^{\star}_{1}\) so that \(|\hat{y}_{1}-y^{\star}_{1}|\geq\gamma_{\emptyset}/2\). The adversary can keep picking the instances \(x_{t}\) based on the induced path of the choices of the true labels \(\left\{y^{\star}_{\tau}\right\}_{\tau<t}\) and the loss of the learner in every round \(t\) is at least \(\gamma_{y\leq t}/2\). Thus, summing up over all the rounds as \(T\to\infty\), we see that the total loss of the learner is at least

\[\frac{\mathbb{D}^{\mathrm{onl}}(\mathcal{H})}{2}-\varepsilon\,.\]

[MISSING_PAGE_EMPTY:30]

The construction that illustrates the poor performance of the first learner is exactly the same as in the proof of the lower bound of Theorem1. The second part of the example is formally shown in Claim2, which follows.

**Claim 2** (Good ERM Learner).: _Let \(\varepsilon,\delta\in(0,1)^{2}\). Then, the good ERM learner of Example3 has sample complexity \(\mathcal{M}(\varepsilon,\delta)=\frac{1}{\varepsilon}\log\left(\frac{1}{ \delta}\right)\)._

Proof.: Let \(d\in\mathbb{N},\mathcal{D}_{\mathcal{X}_{d}}\) be a distribution over \(\mathcal{X}_{d}\) and \(h_{A_{d}^{\star}}\) be the labeling function. Consider a sample \(S\) of length \(m\). If the learner observes a value that is different from \(0\) among the labels in \(S\), then it will be able to infer \(h_{A_{d}^{\star}}\) and incur \(0\) error. On the other hand, if the learner returns the all zero function its error can be bounded as

\[\mathop{\mathbf{E}}_{x\sim\mathcal{D}_{\mathcal{X}_{d}}}[\ell(h_{\emptyset}(x ),h_{A_{d}^{\star}}(x))]\leq\mathop{\mathbf{Pr}}_{x\sim\mathcal{D}_{\mathcal{ X}_{d}}}[x\in A_{d}^{\star}]\,.\]

Since in all the \(m=\frac{1}{\varepsilon}\log\left(\frac{1}{\delta}\right)\) draws of the training set \(S\) there were no elements from \(A_{d}^{\star}\) we can see that, with probability at least \(1-\delta\) over the draws of \(S\) it holds that

\[\mathop{\mathbf{Pr}}_{x\sim\mathcal{D}_{\mathcal{X}_{d}}}[x\in A_{d}^{\star}] \leq\varepsilon\,.\]

Thus, the algorithm satisfies the desired guarantees. 

The next example shows that no proper algorithm can be optimal in the realizable regression setting.

**Example 4** (No Optimal PAC Learner Can be Proper).: _Let \(\mathcal{X}_{d}\) contain \(d\) elements and let \(\gamma\in(0,1)\). Consider the subclass of the scaled first Cantor class (see Example3) with \(\mathcal{H}^{\prime}_{d,\gamma}=\{h_{A}:A\in P(\mathcal{X}_{d}),|A|=\lfloor d/2\rfloor\}\). First, since this class is contained in the scaled first Cantor class, we can employ the good ERM and learn it. However, this learner is improper since \(h_{\emptyset}\notin\mathcal{H}^{\prime}_{d,\gamma}\). Then, no proper algorithm is able to PAC learn \(\mathcal{H}^{\prime}_{d,\gamma}\) using \(o(d)\) examples._

Proof.: Suppose that an adversary chooses \(h_{A}\in\mathcal{H}^{\prime}_{d,\gamma}\) uniformly at random and consider the distribution on \(\mathcal{X}_{d}\) which is uniform on the complement of \(A\), where \(|A|=O(d)\). Note that the error of every hypothesis \(h_{B}\in\mathcal{H}^{\prime}_{d,\gamma}\) is at least \(\gamma|B\setminus A|/d\). Therefore, to return a hypothesis with small error, the algorithm must recover a set that is almost disjoint from \(A\) and so recover \(A\). However the size of \(A\) implies that it cannot be done with \(o(d)\) examples.

Formally, fix \(x_{0}\in\mathcal{X}_{d}\) and \(\varepsilon\in(0,1)\). Let \(A\subseteq\mathcal{X}_{d}\setminus\{x_{0}\}\) of size \(d/2\). Let \(\mathcal{D}_{A}\) be a distribution with mass \(\mathcal{D}_{A}((x_{0},h_{A}(x_{0})))=1-16\varepsilon\) and is uniform on the points \(\{(x,h_{A}(x)):x\in A^{c}\}\), where \(A^{c}\) is the complement of \(A\) (without \(x_{0}\)).

Consider a proper learning algorithm \(\mathcal{A}\). We will show that there is some algorithm-dependent set \(A\), so that when \(\mathcal{A}\) is run on \(\mathcal{D}_{A}\) with \(m=O(d/\varepsilon)\), it outputs a hypothesis with error at least \(\gamma\) with constant probability.

Pick \(A\) uniformly at random from all sets of size \(d/2\) of \(\mathcal{X}_{d}\setminus\{x_{0}\}\). Let \(Z\) be the random variable that counts the number of samples in the \(m\) draws from \(\mathcal{D}_{A}\) that are not \((x_{0},h_{A}(x_{0}))\). Standard concentration bounds imply that with probability at least \(1/2\), the number of points from \((\mathcal{X}_{d}\setminus\{x_{0}\})\setminus A\) is at most \(d/4\). Conditioning on this event, \(A\) is a uniformly chosen random set of size \(d/2\) that is chosen uniformly from all subsets of a set \(\mathcal{X}^{\prime}\subset\mathcal{X}_{d}\) with \(|\mathcal{X}^{\prime}|\geq 3d/4\) (these points are not present in the sample). Now assume that the learner returns a hypothesis \(h_{B}\), where \(B\) is a subset of size \(d/2\). Note that \(\mathop{\mathbf{E}}[|B\setminus A|]\geq d/6\). Hence there exists a set \(A\) such that with probability \(1/2\), it holds that \(|B\setminus A|\geq d/6\). This means that \(\mathcal{A}\) incurs a loss of at least \(\gamma\) on all points in \(B\setminus A\) and the mass of each such point is \(\Omega(\varepsilon/d)\). Hence, in total, the learner will incur a loss of order \(\gamma\cdot\varepsilon\). 

## Appendix H Extension to More General Loss Functions

Our results can be extended to loss functions that satisfy approximate pseudo-metric axioms (see e.g., [14, 15]). The main difference from metric losses is that we allow an approximate triangle inequality instead of a strict inequality. Many natural loss functions are captured by this definition, such as the well-studied \(\ell_{p}\) losses for the regression setting. Abstractly, in this context, the label space7 is an abstract non-empty set \(\mathcal{Y}\), equipped with a general loss function \(\ell:\mathcal{Y}^{2}\to\mathbb{R}_{\geq 0}\) satisfying the following property.

Footnote 7: We would like to mention that, in general, we do not require that the label space is bounded. In contrast, we have to assume that the loss function takes values in a bounded space. This is actually necessary since having an unbounded loss in the regression task would potentially make the learning task impossible. For instance, having some fixed accuracy goal, one could construct a learning instance (distribution over labeled examples) that would make estimation with that level of accuracy trivially impossible.

**Definition 18** (Approximate Pseudo-Metric).: _For \(c\geq 1\), a loss function \(\ell:\mathcal{Y}^{2}\to\mathbb{R}_{\geq 0}\) is \(c\)-approximate pseudo-metric if (i) \(\ell(x,x)=0\) for any \(x\in\mathcal{Y}\), (ii) \(\ell(x,y)=\ell(y,x)\) for any \(x,y\in\mathcal{Y}\), and, (iii) \(\ell\) satisfies a \(c\)-approximate triangle inequality; for any \(y_{1},y_{2},y_{3}\in\mathcal{Y}\), it holds that \(\ell(y_{1},y_{2})\leq c(\ell(y_{1},y_{3})+\ell(y_{2},y_{3}))\)._

Furthermore, note that all dimensions for \(\mathcal{H}\), \(\mathbb{D}_{\gamma}^{\mathrm{G}}(\mathcal{H})\), \(\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})\), \(\mathbb{D}_{\gamma}^{\mathrm{DS}}(\mathcal{H})\), and \(\mathbb{D}_{\gamma}^{\mathrm{onl}}(\mathcal{H})\) are defined for loss functions satisfying Definition18.

Next, we provide extensions of our main results for approximate pseudo-metric losses and provide proof sketches for the extensions.

ERM Learnability for Approximate Pseudo-Metrics.For ERM learnability and losses satisfying Definition18, we can obtain the next result.

**Theorem 5**.: _Let \(\ell\) be a loss function satisfying Definition18. Then for every class \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\), \(\mathcal{H}\) is learnable by any ERM in the realizable PAC regression setting under \(\ell\) if and only if \(\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})<\infty\) for all \(\gamma\in(0,1)\)._

The proof of the upper bound and the lower bound follow in the exact same way as with the absolute loss.

PAC Learnability for Approximate Pseudo-Metrics.As for PAC learning with approximate pseudo-metric losses, we can derive the next statement.

**Theorem 6**.: _Let \(\ell\) be a loss function satisfying Definition18. Then every class \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\) is PAC learnable in the realizable PAC regression setting under \(\ell\) if and only if \(\mathbb{D}_{\gamma}^{\mathrm{OIG}}(\mathcal{H})<\infty\) for any \(\gamma\in(0,1)\)._

Proof Sketch.We can generalize the upper bound in Theorem2 for the scaled OIG dimension as follows. One of the ingredients of the proof for the absolute loss is to construct a sample compression scheme through the median boosting algorithm (cf. Algorithm2). While the multiplicative update rule is defined for any loss function, the median aggregation is no longer the right aggregation for arbitrary (approximate) pseudo-metrics. However, for each such loss function, there exists an aggregation such that the output value of the ensemble is within some cutoff value from the true label for each example in the training set, which means that we have a sample compression scheme for some cutoff loss. In particular, we show that by using weak learners with cutoff parameter \(\gamma/(2c)\), where \(c\) is the approximation level of the triangle inequality, the aggregation of the base learners can be expressed as a sample compression scheme for cutoff loss with parameter \(\gamma\).

Indeed, running the boosting algorithm with \((\gamma/(2c),1/6)\)-weak learners yields a set \(h_{1},\ldots,h_{N}\) of weak predictors, with the property that for each training example \((x,y)\), at least \(2/3\) of the functions \(h_{i}\) (as weighted by coefficients \(\alpha_{i}\)), \(1\leq i\leq N\), satisfy \(\ell(h_{i}(x),y)\leq\gamma/(2c)\). For any \(x\), let \(\hat{h}(x)\) be a value in \(\mathcal{Y}\) such that at least \(2/3\) of \(h_{i}\) (as weighted by \(\alpha_{i}\)), \(1\leq i\leq N\), satisfy \(\ell(h_{i}(x),\hat{h}(x))\leq\gamma/(2c)\), if such a value exists, and otherwise \(\hat{h}(x)\) is an arbitrary value in \(\mathcal{Y}\). In particular, note that on the training examples \((x,y)\), the label \(y\) satisfies this property, and hence \(\hat{h}(x)\) is defined by the first case. Thus, for any training example, there exists \(h_{i}\) (indeed, at least \(2/3\) of them) such that _both_\(\ell(h_{i}(x),y)\leq\gamma/(2c)\) and \(\ell(h_{i}(x),\hat{h}(x))\leq\gamma/(2c)\) are satisfied, and therefore we have

\[\ell(\hat{h}(x),y)\leq c(\ell(\hat{h}(x),h_{i}(x))+\ell(h_{i}(x),y))\leq\gamma.\]

This function \(\hat{h}\) can be expressed as a sample compression scheme of size \(O\Big{(}\mathbb{D}_{\gamma/(2c)}^{\mathrm{OIG}}(\mathcal{H})\log(m)\Big{)}\) for cutoff loss with parameter \(\gamma\): namely, it is purely defined by the \(h_{i}\) functions, where each \(h_{i}\) is specified by \(O\Big{(}\mathbb{D}_{\gamma/(2c)}^{\mathrm{OIG}}(\mathcal{H})\Big{)}\) training examples, and we have \(N=O(\log(m))\) such functions, and \(\hat{h}\) satisfies \(\ell(\hat{h}(x),y)\leq\gamma\) for all \(m\) training examples \((x,y)\). Thus, by standard generalization bounds for sample compression, we get an upper bound that scales with \(\tilde{O}\Big{(}\mathbb{D}_{\gamma/(2c)}^{\mathrm{OIG}}(\mathcal{H})\frac{1}{m} \Big{)}\) for the cutoff loss with parameter \(\gamma\), and hence by Markov's inequality, an upper bound

\[\mathbf{E}[\ell(\hat{h}(x),y)]=\tilde{O}\Big{(}\mathbb{D}_{\gamma/(2c)}^{ \mathrm{OIG}}(\mathcal{H})\frac{1}{m\gamma}\Big{)}\,.\]

We next deal with the lower bound. For the absolute loss, we scale the dimension by \(2\gamma\) instead of \(\gamma\) since for any two possible labels \(y_{1},y_{2}\) the learner can predict some intermediate point, and we want to make sure that the prediction will be either \(\gamma\) far from \(y_{1}\) or \(y_{2}\). For an approximate pseudo-metric, we should take instead \(2c\gamma\) in order to ensure that the prediction is \(\gamma\) far, which means that the lower bounds in Theorem2 hold with a scale of \(2c\gamma\). 

Online Learnability for Approximate Pseudo-Metrics.Finally, we present the more general statement for online learning.

**Theorem 7**.: _Let \(\ell\) be a loss function satisfying Definition18 with parameter \(c\geq 1\). Let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\) and \(\varepsilon>0\). Then, there exists a deterministic algorithm whose cumulative loss in the realizable setting is bounded by \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})+\varepsilon\). Conversely, for any \(\varepsilon>0\), every deterministic algorithm in the realizable setting incurs loss at least \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})/(2c)-\varepsilon\)._

Proof Sketch.: The upper bound of Theorem4 works for any loss function. Recall the proof idea; in every round \(t\) there is some \(\widehat{y}_{t}\in\mathcal{Y}\) the learner can predict such that no matter what the adversary picks as the true label \(y_{t}^{\star}\), the online dimension of the version space at round \(t\), i.e, \(V=\{h\in\mathcal{H}:h(x_{\tau})=y_{\tau}^{\star},1\leq\tau\leq t\}\), decreases by \(\ell(y_{t}^{\star},\widehat{y}_{t})\), minus some shrinking number \(\epsilon_{t}\) that we can choose as a parameter. Therefore we get that the sum of losses is bounded by the online dimension and the sum of \(\epsilon_{t}\) that we can choose to be arbitrarily small.

The lower bound for online learning in Theorem4 would be \(\mathbb{D}^{\mathrm{onl}}(\mathcal{H})/(2c)-\varepsilon\), for any \(\epsilon>0\), since the adversary can force a loss of \(\gamma_{\boldsymbol{y}\leq t}/(2c)\) in every round \(t\), where \(\gamma_{\boldsymbol{y}\leq t}\) is the sum of the gaps across the path \(\boldsymbol{y}\).