# An active learning framework for multi-group mean estimation

 Abdellah Aznag Rachel Cummings Adam N. Elmachtoub

Department of Industrial Engineering and Operational Research

Columbia University

{aa4683, rac2239, ae2516} @columbia.edu

###### Abstract

We consider a problem with multiple groups whose data distributions are unknown, and an analyst would like to learn the mean of each group. We consider an active learning framework to sequentially collect \(T\) samples with bandit feedback, each period observing a sample from a chosen group. After observing a sample, the analyst may update their estimate of the mean and variance of that group and choose the next group accordingly. The analyst's objective is to dynamically collect samples to minimize the \(p\)-norm of the vector of variances of the mean estimators after \(T\) rounds. We propose an algorithm, Variance-UCB, that selects groups according to a an upper bound on the variance estimate adjusted to the \(p\)-norm chosen. We show that the regret of Variance-UCB is \(\tilde{O}(T^{-2})\) for finite \(p\), and prove that no algorithm can do better. When \(p\) is infinite, we recover the \(\tilde{O}(T^{-1.5})\) regret bound obtained in [4, 13] with improved dependence on the remaining parameters and provide a new lower bound showing that no algorithm can do better.

## 1 Introduction

Obtaining accurate estimates from limited labeled data is a fundamental challenge. To tackle this issue, active learning has emerged as a promising solution framework where a decision-maker strategically selects one sample at a time [35, 15, 22, 40, 24]. The estimation task becomes more complex when faced with a large population of different groups, where it is important that all groups are represented in the estimation procedure. If this allocation is not done correctly, the decision might incur structural bias against some groups [32, 21, 33, 29]. If allowed to adjust the sampling allocation strategy, the analyst can address their biases and re-allocate their sampling resources. A key challenge is to dynamically collect data from different groups while maintaining a reasonable representation of all the groups, which is the problem we tackle in this paper. We provide an active learning framework for dynamic data collection with bandit feedback for estimating the means of multiple groups in a representative manner.

We consider a population partitioned into multiple groups, each with data points drawn from an unknown data distribution, and an analyst would like to learn the mean of each group. Each group's distribution has an unknown mean and unknown variance. At each time period, the analyst collects one sample from a group of their choice and observes a sample from that group. Since only one group is observed, this is also known as bandit feedback. Upon observation, the analyst exploits their new knowledge to optimize their choice of the next group to observe. At the end of the time horizon \(T\), the analyst forms a mean estimate for each group. The objective of the analyst is to return a vector of mean estimates with smallest \(p\)-norm of the variance vector of the mean estimates. The choice of \(p\)-norm is motivated by its ability to capture different aspects of multi-group estimation performance [10, 23, 28].

By considering various values of \(p\), we gain insights into different dimensions of estimation accuracy, ranging from the overall spread of the estimates (\(p=2\), Euclidean norm) to the worst case deviation from the true mean (\(p=+\infty\), infinity norm), a case that is studied in [4; 13]. This approach allows us to assess the estimation quality from multiple perspectives, providing a more nuanced understanding of how the diverse population is represented in the final estimation of the group means. The analyst can choose \(p\) in a way to capture notions of fairness in the data collection process.

### Summary of contributions

In this work, we first present an active learning framework that captures the trade-off between representation and accuracy in the task of multi-group estimation. Our framework encapsulates a large class of norms (\(p\)-norms), which bring various insights into different dimensions of estimation accuracy. In Section 3, we present Variance-UCB, an algorithm that selects groups according to an upper bound on the variance estimate adjusted to the \(p\)-norm.

We provide a norm-dependent regret bound on Variance-UCB under the two assumptions sub-Gaussian feedback and positive variances. Specifically, we show that the regret is \(\tilde{O}(T^{-2})\) for finite \(p\) norms (Theorem 1). For the case of the infinite norm, we show that the regret is \(\tilde{O}(T^{-1.5})\) (Theorem 3). While bounds at this rate are already established in [4; 13], our bound provides tighter dependencies in the problem parameters \(G\) and \(\sigma_{\min}\). In the case of the infinite norm, we improve the coefficient of the \(T^{-1.5}\) term in the regret from \(\sigma_{\min}^{-2}G^{2.5}\) to \(G^{1.5}\). This improvement not only tightens the dependency in the number of groups, but it also partially solves one of the open questions left in [13] regarding the role of \(\sigma_{\min}\) in the regret bound. While we prove that \(\sigma_{\min}\) can still impact the regret, it appears only in lower order terms with respect to \(T\).

The analysis and proofs of these upper bounds is presented in Section 4. Our proof technique differs from proofs of similar upper bounds in the related literature. Instead of studying the regret function directly, we first consider its Taylor expansion, and focus our analysis on its dominant term. This technique has two advantages. First, the dominant term is much simpler to analyze (linear in the case of the infinite norm, and quadratic in the case of finite \(p\) norms) than the full regret function. Second, the resulting upper bound is tighter in its dominant term, in that it does not suffer from large numerical constants, as is common in the existing literature on regret analysis of bandit algorithms (e.g., [4; 13]).

We also provide new matching lower bounds in \(T\) for both finite (Theorem 2) and infinite \(p\)-norms (Theorem 4), establishing that Variance-UCB achieves the best possible regret in both regimes. These bounds are summarized in Table 1. Prior to our work and to the best of our knowledge, no lower bounds were known for this problem. The analysis of these results is presented in Section 5.

We empirically validate our findings by numerical experiments presented in Section 6, which show that our theoretical regret bounds match empirical convergence rates in both cases of finite and infinite \(p\)-norms. We also provide examples showing that for finite \(p\)-norms, the smallest variance affects the regret, even when the feedback is Gaussian. This is in contrast to the case of the infinite norm, where it is proven [13] that under Gaussian feedback, the algorithm is not affected by the smallest variance.

### Related work

Our motivation stems from growing attention to data collection methods [34; 2; 18; 20]. We focus on the problem of mean estimation and dynamically collecting data to achieve this goal. While there is a substantial body of literature on data acquisition [9; 26], specifically in the presence of privacy concerns and associated costs [19; 30; 31; 12; 17; 16], our approach differs as we do not consider the costs of sharing data. Instead, we concentrate on the data collection process itself, rather than the costs to data providers.

\begin{table}
\begin{tabular}{c||c|c}
**Norm** & **Variance-UCB Regret** & **Lower Bound on Regret for Any Policy** \\ \hline \(p<\infty\) & \(\tilde{O}\left(T^{-2}\right)\) (Theorem 1) & \(\Omega\left(T^{-2}\right)\) (Theorem 2) \\ \hline \(p=\infty\) & \(\tilde{O}(T^{-1.5})\) (Theorem 3) & \(\Omega(T^{-1.5})\) (Theorem 4) \\ \end{tabular}
\end{table}
Table 1: Summary of main resultsOur work is related to multi-armed bandits problems [5; 37], in the sense that each group can be seen as an arm, and choosing a group to sample from at each time step corresponds to choosing which arm to pull. However, the performance criterion for multi-armed bandits is measured by the difference between the mean of the chosen arm and the best arm [3; 14; 38]. In our framework, the means of the chosen arms do not impact the performance. It is their variances that matter in the optimal solution, as we measure the performance by considering the \(p\)-norm of the variance of the estimator, which can be non-convex. Because of this, and to the best of our knowledge, usual bandits algorithms and proof techniques [25] do not apply. Instead, we propose Variance-UCB, an algorithm that uses high probability upper bounds on the variance estimates adjusted to the chosen norm.

Considering data acquisition from the perspective of active learning is a natural approach [6; 7; 27], and in that sense our work is also related to active learning [15; 22; 24; 40]. In [1], the authors address the optimal data acquisition problem under the assumption of additive objective functions. They formulate the problem as an online learning problem and leverage well-understood tools from online convex optimization [8; 11; 36]. However, their ideas do not apply to our setting due to the non-additive nature of our decisions over time.

The case where the chosen norm is \(\|\cdot\|_{\infty}\) is introduced in [4], where the authors devise the _GAFS-MAX_ algorithm and show that for bounded feedback with known upper bounds, it achieves regret \(\tilde{O}(T^{-1.5})\). This case is also studied in [13], where the authors extend the feedback to sub-Gaussian, and devise the _B-AS_ algorithm, which also has \(\tilde{O}(T^{-1.5})\) regret. One property that emerges in their study is a regret bound deteriorates with \(\sigma_{\min}^{-1}\), where \(\sigma_{\min}^{2}\) is the smallest variance across groups. This is counter-intuitive, as smaller variances in general make the learning simpler. The authors pose as an open question whether any algorithm can have performance independent of \(\sigma_{\min}\) and show that in the special case of exactly Gaussian feedback, one can derive a \(\sigma_{\min}\) free bound. We partially answer this question, by showing that the leading term of the regret bound (with respect to \(T\)) does not depend on \(\sigma_{\min}\). In other words, while \(\sigma_{\min}\) can still impact the regret, its effect appears only in lower order terms. Our regret analysis of Variance-UCB for \(\|\cdot\|_{\infty}\) recovers the same dependence in \(T\) as in [13], but also has improved dependence in the number of groups \(G\) and the variance vector \(\bm{\sigma}\). While [4; 13] serve as a starting point, most of their findings are only applicable to the special case \(p=+\infty\). In particular, they cannot be expanded to other choices of norm. We show that for finite \(p\) norms, we prove the regret bound is \(\tilde{O}(T^{-2})\), which is fundamentally different than the previous \(\tilde{O}(T^{-1.5})\) bound.

## 2 Model

We consider a population partitioned into \(G\) disjoint groups. Each group is represented by an index \(g\) from \([G]:=\{1,\ldots,G\}\). Each individual in the population holds a real-valued data point; data from each group \(g\in[G]\) are distributed according to an unknown distribution \(\mathcal{D}_{g}\), with unknown mean \(\mu_{g}\) and unknown variance \(\sigma_{g}^{2}\). We denote \(\sigma_{\min}:=\min_{g\in[G]}\sigma_{g}\) and let \(\bm{\mathcal{D}}:=\mathcal{D}_{1}\times\ldots\times\mathcal{D}_{G}\). The analyst wishes to compute an unbiased estimate of the population mean for each group over \(T\) times of data collection, sampling only one group at a time. The set of feasible policies is defined as

\[\Pi:=\left\{\bm{\pi}=\{\pi_{t}\}_{t\in[T]}\ |\ \pi_{t}\in G^{t-1}\times \mathbb{R}^{t-1}\rightarrow\Delta(G),\ \forall t\in[T]\right\},\]

where \(\Delta(G)\) is the set of measures supported on \([G]\). Let \(n_{g,T}\) denote the number of collected samples from group \(g\) at the end of time \(T\), and let \(\hat{\mu}_{g,T}\) be the sample mean estimator of \(\mu_{g}\) for \(n_{g,T}\) collected samples. Once all data have been collected at the end of the time horizon \(T\), the analyst will compute the sample mean of each group, i.e.,

\[\hat{\mu}_{g,T}=\frac{1}{\sum_{t=1}^{T}\mathbbm{1}_{X_{t}=g}}\sum_{t=1}^{T} \mathbbm{1}_{X_{t}=g}Y_{t}.\]

Note that the vector \(\hat{\bm{\mu}}_{T}\) is always an unbiased estimator of the vector \(\bm{\mu}\), as long as the policy \(\bm{\pi}\) samples at least once from each group.

The variance of \(\hat{\mu}_{g,T}\) is \(\frac{\sigma_{g}^{2}}{n_{g,T}}\). The \(p-\)norm of the vector of variances of \(n_{g,T}\) is denoted as

\[R_{p}(\bm{n}_{T}):=\left\|\left\{\frac{\sigma_{g}^{2}}{n_{g,T}}\right\}_{g=1}^{ G}\right\|_{p}.\] (1)The analyst wishes to minimize \(\mathbb{E}[R_{p}(\bm{n}_{T})]\).

When choosing a policy, the analyst does not have access to the true standard deviation vector \(\bm{\sigma}:=(\sigma_{1},\ldots,\sigma_{G})\), which is needed to compute the value \(R_{p}(\bm{n}_{T})\). Therefore the analyst must learn \(\bm{\sigma}\) through their decisions. We benchmark the performance of a policy against the best possible performance in a complete information setting where \(\bm{\sigma}\) is known, that is,

\[\min_{\bm{n}\in\mathbb{N}^{G}}R_{p}(\bm{n})\quad s.t.\quad\sum_{g\in[G]}n_{g}=T.\] (2)

The optimization program (2) can be difficult to solve and analyze due to the integer constraints. Instead of using the solution to (2) as a benchmark, we use the solution to its continuous relaxation (a lower bound on (2)), which we denote as,

\[R_{p}^{*}:=\quad\min_{\bm{n}\in\mathbb{R}_{\uparrow}^{G}}R_{p}(\bm{n})\quad s.t.\quad\sum_{g\in[G]}n_{g}=T.\] (3)

Thus, we can define the regret of a policy as

\[\text{Regret}_{p,T}(\bm{\pi},\bm{\mathcal{D}}):=\mathbb{E}_{\pi}[R_{p}(\bm{n} _{T})]-R_{p}^{*}.\] (4)

For our analysis, we assume that the distributions \(\mathcal{D}_{g}\) are sub-Gaussian, as stated in Assumption 1, with corresponding constants \(c_{1},c_{2}\) known to the analyst.[1]

**Assumption 1** (Sub-Gaussianity).: _For each \(g\in[G]\), \(\mathcal{D}_{g}\) is sub-Gaussian. That is, there exist universal constants \(c_{1},c_{2}>0\) such that for any \(\epsilon>0\),_

\[\mathbb{P}_{Y\sim\mathcal{D}_{g}}\left(|Y-\mu_{g}|\geq\epsilon\right)\leq c_{ 1}\exp\left(-\epsilon^{2}/c_{2}\right).\]

_We assume that such \(c_{1},c_{2}\) are known to the analyst._

Moreover, we assume that all groups have some variation in their data, as stated in Assumption 2.

**Assumption 2** (Positive Variance).: _The minimum group variance is positive; i.e., \(\sigma_{\min}=\min_{g\in[G]}\sigma_{g}>0\)._

## 3 Our algorithm: Variance-UCB

Our algorithm, Variance-UCB, builds an increasingly accurate upper confidence bound for each \(\sigma_{g}\), which we denote \(\text{UCB}_{t}(\sigma_{g})\). Recall that \(n_{g,t}\) denotes the number of collected samples from group \(g\) through time \(t\). At each time \(t\), \(\sigma_{g}\) can be estimated via the sample standard deviation

\[\hat{\sigma}_{g,t}:=\sqrt{\frac{1}{n_{g,t}-1}\sum_{s\leq t:X_{s}=g}\left(Y_{s }-\hat{\mu}_{g,t}\right)^{2}}.\]

For convention, we set \(\hat{\sigma}_{g,0}=\hat{\sigma}_{g,1}=+\infty\), indicating that at least two samples are required to obtain a meaningful estimate of \(\sigma_{g}\). We can then define

\[\text{UCB}_{t}(\sigma_{g}):=\hat{\sigma}_{g,t}+\frac{C_{T}}{\sqrt{n_{g,t}}},\] (5)

where

\[C_{T}:=2\sqrt{2\log\left(2T^{4}\right)c_{1}\log(c_{2}T^{4})}+\frac{2\sqrt{c_ {1}\log\left(2T^{4}\right)\left(1+c_{2}+\log(c_{2}T^{4})\right)}}{\left(1-T^{ -4}\right)\sqrt{2\log(2T^{4})}}T^{-2}.\] (6)

\(C_{T}\) was introduced in [13], and captures the trade-off between accuracy of the upper confidence bound and confidence in the estimate, and is a polylogarithmic factor in \(T\). In particular, \(C_{T}\) can be constructed by the analyst since it depends only on \(c_{1},c_{2}\), and \(T\), which are known.

At time \(t+1\), the algorithm chooses the next group \(X_{t+1}\) using the rule:

\[X_{t+1}=\arg\max_{g\in[G]}\frac{\text{UCB}_{t}\left(\sigma_{g}\right)^{\frac{2 p}{p+1}}}{n_{g,t}},\]where where ties in the argmax can be broken arbitrarily. The analyst then observes a new sample \(Y_{t+1}\) from the chosen group \(X_{t+1}\) and updates the upper confidence bounds accordingly. Variance-UCB is presented formally in Algorithm 1.

```
0: norm parameter \(p\), time horizon \(T\), number of groups \(G\) and subgaussian parameters \(c_{1},c_{2}\).
1: Initialize \(n_{g,0}=0,\hat{\sigma}_{g,0}=\hat{\sigma}_{g,1}=+\infty\), \(\forall g\in[G]\)
2: Compute \(C_{T}\) according to (6).
3:for\(t=0,\ldots,T-1\)do
4: Compute \(\text{UCB}_{t}(\sigma_{g})\): \(\text{UCB}_{t}(\sigma_{g})=\hat{\sigma}_{g,t}+\frac{C_{T}}{\sqrt{n_{g,t}}}, \quad\forall g\in[G]\)
5: Select group \(X_{t+1}=\text{argmax}_{g}\frac{\text{UCB}_{t}(\sigma_{g})^{\frac{2p}{p+1}}}{n_ {g,t}}\)
6: Observe feedback \(Y_{t+1}\sim\mathcal{D}_{X_{t+1}}\)
7: Update the number of samples: \(n_{g,t+1}=n_{g,t}+\mathbbm{1}_{X_{t+1}=g},\quad\forall g\in[G]\)
8: Update the mean estimates: \(\hat{\mu}_{g,t+1}=\frac{1}{n_{g,t+1}}\sum_{s=1}^{t+1}\mathbbm{1}_{X_{s}=g}Y_ {s},\quad\forall g\in[G]\)
9: Update the standard deviation estimates: \(\hat{\sigma}_{g,t+1}:=\sqrt{\frac{1}{n_{g,t+1}-1}\sum_{s\leq t+1:X_{s}=g}\left( Y_{s}-\hat{\mu}_{g,t+1}\right)^{2}},\quad\forall g\in[G]\)
10:endfor
11:\(\hat{\mu}_{g,T}=\frac{1}{n_{g,T}}\sum_{s=1}^{T}\mathbbm{1}_{X_{s}=g}Y_{s},\quad \forall g\in[G]\) ```

**Algorithm 1** Variance-UCB \((p,T,G,c_{1},c_{2})\)

Variance-UCB takes as input the time horizon \(T\), the norm \(p\), the groups \([G]\), and the sub-Gaussian parameters \(c_{1},c_{2}\). The algorithm initializes the upper confidence bound for every group to be infinity, which reflects the absence of knowledge of \(\{\sigma_{g}\}\).

In the special case \(p=+\infty\), Variance-UCB (instantiated with a different choice of \(C_{T}\)) coincides with the _B-AS_ algorithm of [13].2

Footnote 2: Our results still hold under this modified parameter setting for any \(C_{T}\) satisfying Lemma 2.

### Regret guarantees

Our first main result gives theoretical bounds on the performance of Algorithm 1 for finite \(p\in[1,\infty)\). We show in Theorem 1 that when \(p\) is finite, Variance-UCB incurs regret of \(\tilde{O}(T^{-2})\).

**Theorem 1**.: _For any \(\boldsymbol{\mathcal{D}}\) that satisfies Assumptions 1 and 2 and for any finite \(p\), the regret of Variance-UCB is at most \(\tilde{O}(T^{-2})\). That is,_

\[\text{Regret}_{p,T}(\text{Variance-UCB},\boldsymbol{\mathcal{D}})=\tilde{O}(T ^{-2}).\]

Our second main result, Theorem 2, provides a matching lower bound for our problem. Thus showing that the performance of Variance-UCB and its analysis is the best possible in terms of \(T\).

**Theorem 2**.: _Let \(p\) be finite and \(\kappa\) be a universal constant. For any online policy \(\boldsymbol{\pi}\), there exists an instance \(\boldsymbol{\mathcal{D}}_{\boldsymbol{\pi}}\) such that for any \(T\geq 1\),_

\[\text{Regret}_{p,T}(\boldsymbol{\pi},\boldsymbol{\mathcal{D}}_{\boldsymbol{ \pi}})\geq\kappa(p+1)T^{-2}+O\left(T^{-2.5}\right)=\Theta(T^{-2}).\]

Our analysis and proof techniques can be extended naturally to the case where \(p=+\infty\). However, the \(\tilde{O}(T^{-2})\) regret no longer holds, and the convergence rate jumps to \(\tilde{O}(T^{-3/2})\).

**Theorem 3**.: _Let \(\Sigma_{\infty}:=\sum_{g\in[G]}\sigma_{g}^{2}\). For any \(\boldsymbol{\mathcal{D}}\) that satisfies Assumptions 1 and 2,_

\[\text{Regret}_{\infty,T}(\text{Variance-UCB},\boldsymbol{\mathcal{D}})\leq \left(C_{T}\sqrt{\Sigma_{\infty}}+C_{T}^{2}\right)G^{1.5}T^{-1.5}+o(T^{-1.5})= \tilde{O}(T^{-1.5}).\]

Note that a similar bound for Theorem 3 has already been established in [13]:

\[\text{Regret}_{\infty,T}(\text{B-AS},\boldsymbol{\mathcal{D}})\leq\frac{28230(c _{1}((c_{2}+2)^{2}+1)\log^{2}(T)\Sigma_{\infty}G^{2.5}}{\sigma_{\min}^{2}T^{1.5 }}+o\left(T^{-1.5}\right).\]Our result in Theorem 3 improves the existing regret in all the problem parameters, i.e., \(\Sigma_{\infty}\) to \(\sqrt{\Sigma_{\infty}}\), and from \(G^{2.5}\) to \(G^{1.5}\)). The most significant improvement lies in removing the dependence on \(\sigma_{\min}\) in the main term of the regret. While \(\sigma_{\min}\) still appears in the negligible term \(o(T^{-1.5})\), this term will be asymptotically dominated for even moderate \(T\). This improved bound gives a better understanding on how \(\sigma_{\min}\) impacts the performance of the algorithm, which was left as an open question in [13].

Finally, we give a matching lower bound for the case when \(p=\infty\), showing that the analysis of Variance-UCB is tight in \(T\) when \(p=+\infty\). To the best of our knowledge, no lower bound for this problem was previously known.

**Theorem 4**.: _For any online policy \(\boldsymbol{\pi}\), there exists an instance \(\boldsymbol{\mathcal{D}_{\boldsymbol{\pi}}}\) such that for any \(T\geq 1\),_

\[\text{Regret}_{\infty,T}(\boldsymbol{\pi},\boldsymbol{\mathcal{D}_{\boldsymbol {\pi}}})\geq\frac{1}{2}G^{1.5}T^{-1.5}.\]

In Sections 4 and 5, we give outlines of the proofs of Theorems 1 and 2, respectively. While we briefly mention why both results change at \(p=+\infty\), the full proofs for Theorems 3 and 4 is deferred to Appendix C.

## 4 Deriving the upper bounds

In this section, we give an overview of the main steps to prove Theorem 1. A complete proof is given in Appendix A. We first show in Lemma 1 that the optimal \(R_{p}^{*}(\boldsymbol{\sigma})\) is achieved for an optimal static policy \(\boldsymbol{n}_{T}^{*}(p)=\{n_{g,T}^{*}(p)\}_{g\in[G]}\) that assumes knowledge of \(\boldsymbol{\sigma}\) and samples each group \(g\) proportionally to \(\sigma_{g}^{\frac{2p}{p+1}}\).

**Lemma 1**.: _[Benchmark analysis] For each \(t\in\mathbb{N}^{*}\) and \(p\in[1,+\infty]\), let \(n_{g,t}^{*}=\frac{\sigma_{g}^{\frac{2p}{p+1}}t}{\sum_{h\in[G]}\sigma_{h}^{ \frac{2p}{p+1}}}\). Then,_

\[R_{p}^{*}(\boldsymbol{\sigma})=R_{p}(\boldsymbol{n}_{T}^{*})=\frac{1}{T}R_{p}( \boldsymbol{n}_{1}^{*}).\]

The proof of Lemma 1 utilizes the KKT conditions of the optimization program (3) that defines \(R_{p}^{*}\). For convenience, we introduce3\(\Sigma_{p}:=\sum_{g\in[G]}\sigma_{g}^{\frac{2p}{p+1}}\) for each \(p\in[1,+\infty]\) so that we can simplify \(n_{g,t}^{*}=\frac{t}{\Sigma_{p}}\sigma_{g}^{\frac{2p}{p+1}}\). Using Equation (4), the expression of regret can be simplified to

Footnote 3: Note that the definition of \(\Sigma_{p}\) is consistent with the definition of \(\Sigma_{\infty}\) introduced in Theorem 3

\[\text{Regret}_{p,T}(\boldsymbol{\pi},\boldsymbol{\mathcal{D}})=\mathbb{E}_{ \boldsymbol{\pi}}\left[R_{p}(\boldsymbol{n}_{T})-R_{p}(\boldsymbol{n}_{T}^{*} )\right].\] (7)

From Eq. (7), we can understand the behavior of the regret by answering the following questions.

1. How close is the random variable \(\boldsymbol{n}_{T}\) to the optimal sampling vector \(\boldsymbol{n}_{T}^{*}\)?
2. How does the curvature of \(R_{p}(.)\) affect the difference \(R_{p}(\boldsymbol{n}_{T})-R_{p}(\boldsymbol{n}_{T}^{*})\)?

Upper bounding the error \(\boldsymbol{n}_{T}-\boldsymbol{n}_{T}^{*}\):Before we answer the first question, we show in Lemma 2 that \(\text{UCB}_{t}(\sigma_{g})\) is an increasingly accurate upper bound for \(\sigma_{g}\).

**Lemma 2**.: _For all \(g\in[G]\), with probability at least \(1-\tilde{O}(T^{-2})\),_

\[0\leq\text{UCB}_{t}(\sigma_{g})^{\frac{2p}{p+1}}-\sigma_{g}^{\frac{2p}{p+1}} \leq\frac{4C_{T}}{\sqrt{n_{g,t}}}\frac{p}{p+1}\left(\sigma_{g}+\frac{2C_{T}}{ \sqrt{n_{g,t}}}\right)^{\frac{p-1}{p+1}}.\]

The proof of Lemma 2 utilizes Assumption 1 and the choice of \(\text{UCB}_{t}(\sigma_{g})\)4, as defined in Eq. (5).

Footnote 4: Lemma 2 holds for all choices of \(p\in[1,+\infty]\). In particular, it is still true in the case where \(p=+\infty\), as it can be understood by taking the limit \(p\to+\infty\) in the inequalities: \(0\leq\text{UCB}_{t}(\sigma_{g})^{2}-\sigma_{g}^{2}\leq\frac{4C_{T}}{\sqrt{n_{g, t}}}\left(\sigma_{g}+\frac{2C_{T}}{\sqrt{n_{g,t}}}\right).\)

Using Lemma 2, the difference between the number of samples Variance-UCB collects and the optimal number of samples can be bounded with high probability, which we state in Lemma 3.

**Lemma 3**.: _Variance-UCB collects a vector of samples \(\bm{n}\) such that for all \(g\in[G]\), with probability at least \(1-\tilde{O}(T^{-2})\),_

\[n_{g,T}-n_{g,T}^{*}\leq 3+\frac{4C_{T}p}{\Sigma_{p}(p+1)}\left(\sigma_{g}+\frac{ 2C_{T}}{\sqrt{n_{g,T}^{*}}}\right)^{\frac{p-1}{p+1}}\sqrt{n_{g,T}^{*}}=\Theta( \sqrt{T}).\]

To understand the meaning of Lemma 3, we note that the feedback-dependent structure of the algorithm makes the numbers of samples \(n_{g,T}\) correlated across groups and over time, since the algorithm attempts to sample regularly from all groups. A key technical challenge is to decouple \(n_{g,t}\) across groups and derive an instance dependent upper bound on \(\bm{n}_{T}-\bm{n}_{T}^{*}\). This challenge does not arise in the classic multi-armed bandits setting, where the decision-maker's goal is to repeatedly pull only the best arm.

Curvature properties of \(R_{p}\) around the optimal value \(\bm{n}_{T}^{*}\):To answer the second question, we exploit the smoothness of \(R_{p}(\cdot)\) around \(\bm{n}_{T}^{*}\) to approximate it with a polynomial function of \(\bm{n}\). Since \(\bm{n}_{T}^{*}\) is the minimizer of \(R_{p}(\cdot)\) (subject to \(\sum_{g\in[G]}n_{g}=T\)) and \(R_{p}(\cdot)\) is differentiable around \(\bm{n}_{T}^{*}\), the first order of the Taylor approximation of \(R_{p}\) vanishes as \(T\) grows large, formalized in Lemma 4.

**Lemma 4**.: _Let \(p<+\infty\) and \(\bm{n}^{\prime}\in\mathbb{R}_{+}^{G}\) such that \(\sum_{g\in[G]}n_{g}^{\prime}=T\). Then,_

\[\left|\frac{R_{p}(\bm{n}^{\prime})-R_{p}(\bm{n}_{T}^{*})}{R_{p}(\bm{n}^{*})}- \frac{p+1}{2}\sum_{g\in[G]}\frac{(n_{g}^{\prime}-n_{g,T}^{*})^{2}}{Tn_{g,T}^{* }}\right|\leq\frac{7(p+2)^{2}\Sigma_{p}^{2}}{\sigma_{\min}^{2}}\max_{g}\left( \frac{n_{g,T}^{*}}{n_{g}^{\prime}}\right)^{3p+3}\frac{\|\bm{n}^{\prime}-\bm{n }_{T}^{*}\|_{\infty}^{3}}{T^{3}}.\]

We note that the bound in Lemma 4 holds regardless of the choice of the vector \(\bm{n}^{\prime}\), including those generated by Variance-UCB. One can interpret the upper bound in Lemma 4 as follows: the term \(\frac{p+1}{2}\sum_{g\in[G]}\frac{(n_{g}^{\prime}-n_{g,T}^{*})^{2}}{Tn_{g,T}^{ *}}\) represents the exact Taylor first-order approximation of \(\frac{R_{p}(\bm{n}^{\prime})-R_{p}(\bm{n}_{T}^{*})}{R_{p}(\bm{n}^{*})}\), and the right hand side represents an upper bound on this approximation. In particular, assuming an error \(\bm{\epsilon}=\bm{n}^{\prime}-\bm{n}^{*}\), Lemma 4 can be restated as,

\[\frac{R_{p}(\bm{n}^{\prime})-R_{p}(\bm{n}_{T}^{*})}{R_{p}(\bm{n}^{*})}=\Theta \left(\frac{\|\bm{\epsilon}\|^{2}}{T^{2}}\right)+o\left(\frac{\|\bm{\epsilon} \|^{2}}{T^{2}}\right).\]

Putting everything together:The rest of the proof consists of applying Lemmas 3 and 4 to the regret expression in Eq. (7). By Lemma 3, with high probability, \(\|\bm{n}_{T}-\bm{n}_{T}^{*}\|_{\infty}=\tilde{O}\left(\sqrt{T}\right)\). Applying this to Lemma 4 gives that \(R_{p}(\bm{n}_{T})-R_{p}(\bm{n}_{T}^{*})=\|\bm{n}_{T}-\bm{n}_{T}^{*}\|_{\infty} ^{2}R_{p}^{*}\cdot O\left(T^{-2}\right)\). By Lemma 1, \(R_{p}^{*}=\Theta(T^{-1})\), and therefore with high probability, \(R_{p}(\bm{n}_{T})-R_{p}(\bm{n}_{T}^{*})=\tilde{O}(T^{-2})\). With additional work, we show that the equality also holds in expectation, which implies from Eq. (7) that \(\text{Regret}_{p,T}(\text{Variance-UCB},\bm{\mathcal{D}})=\mathbb{E}_{\pi}[R_{p} (\bm{n})-R_{p}(\bm{n}_{T}^{*})]=\tilde{O}(T^{-2})\).

### The case \(p=+\infty\)

Even though Lemmas 1, 2, and 3 hold for \(p=+\infty\), the approximation guarantee given in Lemma 4 does not hold because \(R_{\infty}\) is not differentiable at \(\bm{n}_{T}^{*}\). As an alternative, we derive a first order upper bound, which we state in Lemma 5.

**Lemma 5**.: _Let \(\bm{\sigma}\in\mathbb{R}_{+}^{G}\) and \(\bm{n}^{\prime}\in\mathbb{R}_{+}^{G}\) such that \(\sum_{g\in[G]}n_{g}^{\prime}=T\). Then,_

\[\frac{R_{\infty}(\bm{n}_{T}^{\prime})-R_{\infty}(\bm{n}_{T}^{*})}{R_{\infty}(\bm {n}_{T}^{*})}\leq-\min_{g}\left(\frac{n_{g}^{\prime}}{n_{g,T}^{*}}-1\right)+ \frac{1}{4}\max_{g}\left(\frac{n_{g}^{\prime}}{n_{g,T}^{*}}-1\right)^{2}\max_{g }\left(\frac{n_{g,T}^{*}}{n_{g}^{\prime}}\right)^{3}\]

Similar to Lemma 4, we interpret the upper bound above by decomposing it into a main term, \(-\min_{g}\left(\frac{n_{g}^{\prime}}{n_{g,T}^{*}}-1\right)\), and an error term, \(\frac{1}{4}\max_{g}\left(\frac{n_{g}^{\prime}}{n_{g,T}^{*}}-1\right)^{2}\max_{g }\left(\frac{n_{g,T}^{*}}{n_{g}^{\prime}}\right)^{3}\). As opposed to the case where \(p<+\infty\), \(R_{\infty}\) is not differentiable in \(\bm{n}^{*}\), thus the inequality above does not stem from a Taylor approximation. Adapting the bound in Lemma 5 in the same steps as in **Step 3** gives a regret bound of \(\tilde{O}(T^{-1.5})\).

Deriving the lower bounds

In this section, we provide the key ideas behind the proof of Theorem 2, with the full proof given in Appendix B. Given any policy \(\bm{\pi}\), the idea is to pick \(\bm{\mathcal{D}}_{\bm{\pi}}\) from two constructed instances \(\bm{\mathcal{D}}^{a}=\mathcal{D}_{1}^{a}\times\cdots\times\mathcal{D}_{G}^{a}\) and \(\bm{\mathcal{D}}^{b}=\mathcal{D}_{1}^{b}\times\cdots\times\mathcal{D}_{G}^{b}\). The interaction of \(\bm{\mathcal{D}}^{a}\) (resp. \(\bm{\mathcal{D}}^{b}\)) with \(\bm{\pi}\) yields a random vector of the number of collected samples, denoted by \(\bm{n}^{a}\) (resp. \(\bm{n}^{b}\)). \(\bm{\mathcal{D}}^{a}\) and \(\bm{\mathcal{D}}^{b}\) must satisfy the following two conflicting properties:

1. \(\bm{\mathcal{D}}^{a}\) and \(\bm{\mathcal{D}}^{b}\) are sufficiently similar, in the sense that the distributions of \(\bm{n}^{a}\) and \(\bm{n}^{b}\) are close, so that \(\bm{\pi}\) would have a hard time distinguishing between \(\bm{\mathcal{D}}^{a}\) and \(\bm{\mathcal{D}}^{b}\). This notion of similarity is captured by the KL-divergence of \(\bm{\mathcal{D}}^{a}\) and \(\bm{\mathcal{D}}^{b}\).
2. \(\bm{\mathcal{D}}^{a}\) and \(\bm{\mathcal{D}}^{b}\) are also sufficiently dissimilar in the sense that they induce distinct optimal allocation rules \(\bm{n}_{a}^{*}\) and \(\bm{n}_{b}^{*}\). Specifically, any allocation \(\bm{n}\) cannot be simultaneously close to both \(\bm{n}_{a}^{*}\) and \(\bm{n}_{b}^{*}\), and \(\bm{n}\) incurs a high regret under at least one of the two instances. This notion of dissimilarity is captured below.

Let \(\bm{\sigma}^{a}\in\mathbb{R}_{+}^{G}\) be the vector of standard deviations of all groups under \(\bm{\mathcal{D}}^{a}\), and let \(S_{\epsilon}^{a}\) be the set of allocations \(\bm{n}\) such that \(R_{p}(\bm{n};\bm{\sigma}^{a})\) is within \(\epsilon\) of \(R_{p}^{*}(\bm{\sigma}^{a})\) for any \(\bm{n}\in S_{\epsilon}^{a}\). Formally,

\[S_{\epsilon}^{a}:=\left\{\bm{n}\in\mathbb{N}^{G}\middle|\sum_{g}n_{g}=T,\quad R _{p}(\bm{n},\bm{\sigma}^{a})-R_{p}^{*}(\bm{\sigma}^{a})\leq\epsilon\right\}.\]

Let \(\bm{\sigma}^{b}\) be a second vector and define \(S_{\epsilon}^{b}\) similarly. We define the dissimilarity \(d(\bm{\sigma}^{a},\bm{\sigma}^{b}):=\inf_{\epsilon\geq 0}\{S_{\epsilon}^{a} \cap S_{\epsilon}^{b}\neq\emptyset\}\), which is the smallest \(\epsilon\) so that these sets of allocations are disjoint. There is a tension between the two requirements on \(\bm{\mathcal{D}}^{a}\) and \(\bm{\mathcal{D}}^{b}\), which is formalized next in Lemma 6.

**Lemma 6**.: _Let \(\bm{\pi}\) be a fixed policy and \(\bm{\mathcal{D}}^{a}\), \(\bm{\mathcal{D}}^{b}\) be two instances with standard deviation vectors \(\bm{\sigma}^{a},\bm{\sigma}^{b}\), respectively. Then,_

\[\max\{\text{Regret}_{p,T}(\bm{\pi},\bm{\mathcal{D}}^{a}),\text{Regret}_{p,T}( \bm{\pi},\bm{\mathcal{D}}^{b})\}\geq d(\bm{\sigma}^{a},\bm{\sigma}^{b})\exp \left(-\sum_{g\in[G]}\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{g,t}]KL( \mathcal{D}_{g}^{a}||\mathcal{D}_{g}^{b})\right).\]

The proof follows from an adapted version of LeCam's method [39]. To understand the implication of Lemma 6, we examine the two terms involved in the right hand side of the inequality. The first term, \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})\), increases the further \(\bm{\sigma}^{a}\) and \(\bm{\sigma}^{b}\) are from each other. The second term has the opposite monotonicity, as it decreases with the \(KL\) divergence of the two instances. These conflicting terms capture the trade-off of loss minimization versus information gain, and deriving the lower bound will come from constructing two instances for which this trade-off is maximised.

Specifically, \(\bm{\mathcal{D}}^{a}\) is chosen such that all groups have data distributed according to \(\mathcal{D}_{g}^{a}\sim\mathcal{N}\left(0,\sigma^{2}\right)\); we denote \(\bm{\sigma}^{a}:=\sigma(1,\dots,1)\) as the vector of standard deviations of each group. Even though instance \(a\) has all identical groups, the policy \(\bm{\pi}\) may treat groups differently, and the interaction of \(\bm{\pi}\) with \(\bm{\mathcal{D}}^{a}\) may yield give different expected numbers of samples from each group. We pick \(h\in[G]\) such that \(\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{h,T}]\) is minimized over \([G]\), and construct \(\bm{\mathcal{D}}^{b}\) as follows:

\[\bm{\mathcal{D}}^{b}:\begin{cases}\mathcal{D}_{h}^{b}&\sim\mathcal{N}\left(0, \sigma^{2}(1+\sqrt{\frac{G}{T}})\right)\\ \mathcal{D}_{g}^{b}&\sim\mathcal{N}\left(0,\sigma^{2}\right),\quad\forall g \neq h\end{cases}.\]

On the one hand, notice that \(\bm{\mathcal{D}}^{a}\) and \(\bm{\mathcal{D}}^{b}\) only differ in their coordinate \(h\), so that \(KL(\mathcal{D}_{g}^{a}||\mathcal{D}_{g}^{b})=\mathds{1}(g=h)\Theta(GT^{-1})\), and by minimality of \(h\), \(\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{h,t}]\leq\frac{T}{G}\), so that

\[\sum_{g\in[G]}\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{g,t}]KL(\mathcal{D }_{g}^{a}||\mathcal{D}_{g}^{b})=\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{h,t}]KL(\mathcal{D}_{h}^{a}||\mathcal{D}_{h}^{b})\leq\frac{T}{G}\Theta(GT^{-1})= \Theta(1).\]

Second, we show that \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})=\Theta(T^{-2})\). We do this by measuring the distance between the sets \(S_{\epsilon}^{a}\) and \(S_{\epsilon}^{b}\) for a fixed \(\epsilon\), and then estimate the smallest \(\epsilon\) for which this distance is \(0\). This smallest \(\epsilon\) corresponds to \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})\). Combining both in Lemma 6 yields \(\max\{\text{Regret}_{p,T}(\bm{\pi},\bm{\mathcal{D}}^{a}),\text{Regret}_{p,T}( \bm{\pi},\bm{\mathcal{D}}^{b})\}\geq\Theta(T^{-2})\).

**Remark 1**.: _For \(p=+\infty\), the proof remains the same except that in this case \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})=\Theta(T^{-3/2})\), resulting in an overall lower bound of \(\Theta(T^{-3/2})\). See Appendix C.3 for the complete proof._Numerical study

In this section, we present experimental results on the empirical performance of Variance-UCB. We explore the impact of varying each important parameter of the problem: the time horizon \(T\), norm parameter \(p\), number of groups \(G\), distributions \(\mathcal{D}_{g}\), and the sub-Gaussianity parameters \(c_{1},c_{2}\). In all the experiments \(\mathcal{D}_{g}\) follow Gaussian distributions. Except where they are varied, the default parameter settings are \(T=10^{5}\), \(p=2\), \(G=2\), with the respective data distributions of groups 1 and 2 as \(\mathcal{N}(1,1)\) and \(\mathcal{N}(2,2.5)\), satisfying \(c_{1}=c_{2}=5\). For each parameter setting evaluated, we run Variance-UCB 500 times and report average regret over all 500 runs. For convenience, time and regret are presented on logarithmic scales.

First, we vary the choice of the parameter \(p\in\{1,2,10,25,+\infty\}\) and observe its effect on the regret. Figure 1 shows that the convergence rates for each \(p\) precisely match those predicted by Theorems 1 and 3: a slope of -2 for the finite values of \(p\) (\(\{1,2,10,25\}\)), and a slope of -1.5 for \(p=\infty\).

Next, we study how mis-specifying the sub-Gaussianity parameters \(c_{1}\) and \(c_{2}\) affect the regret of Algorithm 1. The parameter values (\(c_{1},c_{2}\)) only affect the algorithm's behavior through \(C_{T}\), so it is more natural to directly study the impact of errors in \(C_{T}\). For the parameters used in our experimental setup, the value of \(C_{T}\) prescribed in Equation (6) is \(\sim 5\); we consider both underestimating and overestimating \(C_{T}\), and evaluate regret when instead plugging in values of \(C_{T}\in\{0.001,5,1000\}\) in the UCB update step defined in Equation (5). Figure 2 shows that choosing an overestimate of \(C_{T}\) incurs an increased regret. This is not surprising since choosing a larger \(C_{T}\) decreases the confidence of the algorithm in its estimates, and forces over-exploration. Choosing a low \(C_{T}\) also incurs a higher regret which can more severely impact the performance, as the algorithm under-explores and can get stuck in sub-optimal behavior. However, with a long enough time horizon, the algorithm will eventually estimate \(\sigma\) accurately, regardless of the small choice of \(C_{T}\). As observed in Figure 2, the smallest value of \(C_{T}=0.001\) initially has the highest regret, corresponding to incorrect estimates and insufficient exploration, and then finally converges very late to have the lowest regret. Even for very large \(T\), this curve has higher variance due to the noise in the estimates.

Figure 1: Impact of the \(p\)-norm on the regret.

Figure 2: Impact of mis-estimating \(C_{T}\) on the regret

Next, we study the effect of varying the lowest variance \(\sigma_{\min}^{2}\in\{0.05,0.1,0.5,1\}\) while holding all other variances fixed, for each \(p\in\{1,2,+\infty\}\). First, Figure 3 shows that varying \(\sigma_{\min}\) has no effect on the regret when \(p=+\infty\). This matches a result of [13], where they prove that when \(p=+\infty\), their UCB-style algorithm (very similar to Variance-UCB) is not affected by the lowest variance when the feedback is Gaussian. However, our experimental results show that this phenomenon does not persist when \(p\) is finite, as illustrated in Figure 3, where we observe that regret decreases when the lowest variance \(\sigma_{\min}\) increases. This is surprising since increasing the lowest variance makes the feedback more volatile, and one would expect an increase in regret as a result.

Finally, we vary the number of groups \(G\in\{2,10,50\}\). For the additional groups, we generate their data from Gaussian \(\mathcal{D}_{g}\), with means \(\mu_{g}\sim\mathcal{U}([-1,1])\) and standard deviations \(\sigma_{g}\sim\mathcal{U}([2,4])\) independently for each group. From Figure 4, we observe that the regret increases in the number of groups, as expected. When the number of groups is small (\(G=2\)), Variance-UCB quickly enters a regime where regret decreases quickly. However, as the number of groups grows (\(G\in\{10,50\}\)), the necessary time to enter the decay regime increases (\(\sim 30,000\) for \(G=10\) and \(\sim 90,000\) for \(G=50\)). Initially the algorithm samples (on average) uniformly across all groups due to the UCB term outweighing the sample variance estimates, and each group must wait to be sampled enough times for the algorithm to estimate its optimal sampling rate. This delay will naturally increase with the number of groups. Once the confidence bounds are small enough, the algorithm samples the highest variances first. This causes abrupt variation (especially in the case where \(p\) is small) because the objective function is very sensitive to changes in one coordinate.

### Acknowledgements

The authors gratefully acknowledge the support of National Science Foundation grants IIS-2147361. The authors would also like to thank the reviewers for their valuable feedback which helped improve the paper.

Figure 4: Impact of the number of groups \(G\) on regret

Figure 3: The impact of \(\sigma_{\min}\) on the regret

## References

* Abernethy et al. [2015] Jacob Abernethy, Yiling Chen, Chien-Ju Ho, and Bo Waggoner. Low-cost learning via active data procurement. In _Proceedings of the Sixteenth ACM Conference on Economics and Computation_, EC '15, page 619-636, New York, NY, USA, 2015. Association for Computing Machinery.
* Acemoglu et al. [2019] Daron Acemoglu, Ali Makhdoumi, Azarakhsh Malekian, and Asuman Ozdaglar. Too much data: Prices and inefficiencies in data markets. Working Paper 26296, National Bureau of Economic Research, September 2019.
* Agrawal and Devanur [2014] Shipra Agrawal and Nikhil R Devanur. Bandits with concave rewards and convex knapsacks. In _Proceedings of the fifteenth ACM conference on Economics and computation_, pages 989-1006, 2014.
* Antos et al. [2008] Andras Antos, Varun Grover, and Csaba Szepesvari. Active learning in multi-armed bandits. pages 287-302, 10 2008.
* Auer et al. [2002] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47:235-256, 2002.
* Balcan et al. [2009] Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. _Journal of Computer and System Sciences_, 75(1):78-89, 2009. Learning Theory 2006.
* Balcan et al. [2010] Maria-Florina Balcan, Steve Hanneke, and Jennifer Wortman Vaughan. The true sample complexity of active learning. _Machine learning_, 80:111-139, 2010.
* Berthet and Perchet [2017] Quentin Berthet and Vianney Perchet. Fast rates for bandit optimization with upper-confidence frank-wolfe. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Bhat et al. [2020] Nikhil Bhat, Vivek F. Farias, Ciamac C. Moallemi, and Deeksha Sinha. Near-optimal a-b testing. _Management Science_, 66(10):4477-4495, 2020.
* Brimberg and Love [1993] Jack Brimberg and Robert F Love. General considerations on the use of the weighted lp norm as an empirical distance measure. _Transportation Science_, 27(4):341-349, 1993.
* Bubeck and Cesa-Bianchi [2012] Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 5(1):1-122, 2012.
* Cai et al. [2015] Yang Cai, Constantinos Daskalakis, and Christos Papadimitriou. Optimum statistical estimation with strategic data sources. In Peter Grunwald, Elad Hazan, and Satyen Kale, editors, _Proceedings of The 28th Conference on Learning Theory_, volume 40 of _Proceedings of Machine Learning Research_, pages 280-296, Paris, France, 03-06 Jul 2015. PMLR.
* Carpentier et al. [2011] Alexandra Carpentier, Alessandro Lazaric, Mohammad Ghavamzadeh, Remi Munos, and Peter Auer. Upper-confidence-bound algorithms for active learning in multi-armed bandits. In _Algorithmic Learning Theory: 22nd International Conference, ALT 2011, Espoo, Finland, October 5-7, 2011. Proceedings 22_, pages 189-203. Springer, 2011.
* Cella et al. [2020] Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear bandits. In _International Conference on Machine Learning_, pages 1360-1370. PMLR, 2020.
* Cohn et al. [1996] David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. _Journal of artificial intelligence research_, 4:129-145, 1996.
* Cummings et al. [2023] Rachel Cummings, Hadi Elzayn, Vasilis Gkatzelis, Emmanouil Pountourakis, and Juba Ziani. Optimal data acquisition with privacy-aware agents. In _Proceedings of the IEEE Conference on Secure and Trustworthy Machine Learning_, SaTML '23, 2023.

- 22_, pages 29139-29151, 2022.
* [18] Edith D De Leeuw. Choosing the method of data collection. 2008.
* [19] Arpita Ghosh and Aaron Roth. Selling privacy at auction. In _Proceedings of the 12th ACM Conference on Electronic Commerce_, EC '11, page 199-208, New York, NY, USA, 2011. Association for Computing Machinery.
* [20] Stephen J Grove and Raymond P Fisk. Observational data collection methods for services marketing: An overview. _Journal of the Academy of Marketing Science_, 20:217-224, 1992.
* [21] Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daume III, Miro Dudik, and Hanna Wallach. Improving fairness in machine learning systems: What do industry practitioners need? In _Proceedings of the 2019 CHI conference on human factors in computing systems_, pages 1-16, 2019.
* [22] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A. Al Sallab, Senthil Yogamani, and Patrick Perez. Deep reinforcement learning for autonomous driving: A survey. _IEEE Transactions on Intelligent Transportation Systems_, 23(6):4909-4926, 2022.
* [23] Marius Kloft, Ulf Brefeld, Soren Sonnenburg, and Alexander Zien. Lp-norm multiple kernel learning. _The Journal of Machine Learning Research_, 12:953-997, 2011.
* [24] Yoshiaki Kuwata, Justin Teo, Gaston Fiore, Sertac Karaman, Emilio Frazzoli, and Jonathan P How. Real-time motion planning with applications to autonomous urban driving. _IEEE Transactions on control systems technology_, 17(5):1105-1118, 2009.
* [25] Tor Lattimore and Csaba Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020.
* [26] Shengyuan Liu, Yuxuan Zhao, Zhenzhi Lin, Yi Ding, Yong Yan, Li Yang, Qin Wang, Hao Zhou, and Hongwei Wu. Data-driven condition monitoring of data acquisition for consumers' transformers in actual distribution systems using t-statistics. _IEEE Transactions on Power Delivery_, 34(4):1578-1587, 2019.
* [27] Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg, and Le Song. Iterative machine teaching. In _International Conference on Machine Learning_, pages 2149-2158. PMLR, 2017.
* [28] Wenbo Liu, Shengnan Liang, and Xiwen Qin. Weighted p-norm distance t kernel svm classification algorithm based on improved polarization. _Scientific reports_, 12(1):1-16, 2022.
* [29] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. _ACM Computing Surveys (CSUR)_, 54(6):1-35, 2021.
* [30] Kobbi Nissim, Claudio Orlandi, and Rann Smorodinsky. Privacy-aware mechanism design. In _Proceedings of the 13th ACM Conference on Electronic Commerce_, EC '12, page 774-789, New York, NY, USA, 2012. Association for Computing Machinery.
* [31] Kobbi Nissim, Salil Vadhan, and David Xiao. Redrawing the boundaries on purchasing data from privacy-sensitive individuals. In _Proceedings of the 5th Conference on Innovations in Theoretical Computer Science_, ITCS '14, page 411-422, New York, NY, USA, 2014. Association for Computing Machinery.
* [32] Dana Pessach and Erez Shmueli. A review on fairness in machine learning. _ACM Computing Surveys (CSUR)_, 55(3):1-44, 2022.
* [33] Alvin Rajkomar, Michaela Hardt, Michael D Howell, Greg Corrado, and Marshall H Chin. Ensuring fairness in machine learning to advance health equity. _Annals of internal medicine_, 169(12):866-872, 2018.

* [34] Mitchell Nicholas Sarkies, K-A Bowles, EH Skinner, D Mitchell, Romi Haas, M Ho, Karen Salter, Kerry May, Donna Markham, L O'Brien, et al. Data collection methods in health services research. _Applied clinical informatics_, 6(01):96-109, 2015.
* [35] Burr Settles. Active learning literature survey. 2009.
* [36] Aleksandrs Slivkins. Contextual bandits with similarity information. In Sham M. Kakade and Ulrike von Luxburg, editors, _Proceedings of the 24th Annual Conference on Learning Theory_, volume 19 of _Proceedings of Machine Learning Research_, pages 679-702, Budapest, Hungary, 09-11 Jun 2011. PMLR.
* [37] Aleksandrs Slivkins et al. Introduction to multi-armed bandits. _Foundations and Trends(r) in Machine Learning_, 12(1-2):1-286, 2019.
* [38] Marta Soare, Alessandro Lazaric, and Remi Munos. Best-arm identification in linear bandits. _Advances in Neural Information Processing Systems_, 27, 2014.
* [39] Bin Yu. _Assouad, Fano, and Le Cam_, pages 423-435. Springer New York, New York, NY, 1997.
* [40] Liang Yu, Shuqi Qin, Meng Zhang, Chao Shen, Tao Jiang, and Xiaohong Guan. A review of deep reinforcement learning for smart building energy management. _IEEE Internet of Things Journal_, 8(15):12046-12063, 2021.

Proof of Theorem 1

### Proof of Lemma 1

The proof of Lemma 1 consists of using optimality conditions on \(r_{p}\):

**Lemma 1**.: _[Benchmark analysis] For each \(t\in\mathbb{N}^{*}\) and \(p\in[1,+\infty]\), let \(n_{g,t}^{*}=\frac{\sigma_{g}^{\frac{2p}{p+1}}t}{\sum_{h\in[G]}\sigma_{h}^{\frac {2p}{p+1}}}\). Then,_

\[R_{p}^{*}(\bm{\sigma})=R_{p}(\bm{n}_{T}^{*})=\frac{1}{T}R_{p}(\bm{n}_{1}^{*}).\]

Proof.: The case of \(p=+\infty\) is discussed in [4, 13]. We focus on when \(p<+\infty\). Recall the definition of \(R_{p}^{*}\) from Eq. (3):

\[R_{p}^{*}:=\quad\min_{\bm{n}\in\mathbb{R}_{+}^{G}}R_{p}(\bm{n})\quad s.t.\quad \sum_{g\in[G]}n_{g}=T.\]

Since the function \(x\to x^{p}\) is increasing in \(\mathbb{R}_{+}\), we can replace the objective by \([R_{p}(\bm{n})]^{p}=\sum_{g\in[G]}\frac{\sigma_{g}^{2p}}{n_{g}^{p}}\). Any feasible point \(\bm{n}\) with a zero coordinate would have \(R_{p}(\bm{n})=+\infty\), therefore any argmin to the optimization program \(\bm{n}^{*}\) above must have positive coordinates, and satisfies the KKT conditions

\[\forall g\in[G],\quad\frac{\partial}{\partial n_{g}}R_{p}(\bm{n})^{p}-\frac{ \partial}{\partial n_{g}}\lambda\left(\sum_{h\in[G]}n_{h}-T\right)=0,\]

\[\lambda\in\mathbb{R}.\]

For each \(g\in[G]\), the first line of the system above is equivalent to \(-p\frac{\sigma_{g}^{2p}}{n_{g}^{p+1}}-\lambda=0\). Therefore the KKT conditions imply that

\[\frac{\sigma_{1}^{2p}}{n_{1}^{p+1}}=\ldots=\frac{\sigma_{G}^{2p}}{n_{G}^{p+1} },\quad\text{or equivalently,}\quad\frac{\sigma_{1}^{\frac{2p}{p+1}}}{n_{1}}= \ldots=\frac{\sigma_{G}^{\frac{2p}{p+1}}}{n_{G}}=\frac{\sum_{g\in[G]}\sigma_{ g}^{\frac{2p}{p+1}}}{\sum_{g\in[G]}n_{g}}=\frac{\sum_{g\in[G]}\sigma_{g}^{\frac{2p}{p+1} }}{T}.\]

Hence the unique minimizer is the vector

\[\bm{n}_{T}^{*}=\frac{T}{\sum_{g\in[G]}\sigma_{g}^{\frac{2p}{p+1}}}\bm{\sigma} ^{\frac{2p}{p+1}}.\]

Therefore, \(R_{p}^{*}=R_{p}(\bm{n}_{T}^{*})\). Moreover, \(R_{p}(\bm{n}_{T}^{*})=R_{p}(T\bm{n}_{1}^{*})=\frac{1}{T}R_{p}(\bm{n}_{1}^{*})\), where the last equality follows from the homogeneity of the norm. 

### Proof of Lemma 2

In this section, we establish a key property of \(\text{UCB}_{t}\left(\sigma_{g}\right)\), which relies on Assumption 1.

**Lemma 2**.: _For all \(g\in[G]\), with probability at least \(1-\tilde{O}(T^{-2})\),_

\[0\leq\text{UCB}_{t}(\sigma_{g})^{\frac{2p}{p+1}}-\sigma_{g}^{\frac{2p}{p+1}} \leq\frac{4C_{T}}{\sqrt{n_{g,t}}}\frac{p}{p+1}\left(\sigma_{g}+\frac{2C_{T}}{ \sqrt{n_{g,t}}}\right)^{\frac{p-1}{p+1}}.\]

Proof.: Following the notations of Section 3, we introduce the following event:

\[\mathcal{A}_{T}:=\bigcap_{g\in[G],2\leq t\leq T}\left\{|\hat{\sigma}_{g,t}- \sigma_{g}|\leq\frac{C_{T}}{\sqrt{t}}\right\}.\]

Based on corollary 1 of [13], we have

1. \(\mathbb{P}_{\pi}(\mathcal{A}_{T})\geq 1-2GT^{-2.5}\),* conditionally on \(\mathcal{A}_{T}\), \[\forall g\in[G],t\geq 2G,\quad|\hat{\sigma}_{g,t}-\sigma_{g}|\leq\frac{C_{T}}{ \sqrt{n_{g,t}}},\] so that conditionally on \(\mathcal{A}_{T}\), we have: \[\text{UCB}_{t}\left(\sigma_{g}\right) =\hat{\sigma}_{g,t}+\frac{C_{T}}{\sqrt{n_{g,t}}}\] \[=\sigma_{g}+\frac{C_{T}}{\sqrt{n_{g,t}}}+(\hat{\sigma}_{g,t}- \sigma_{g})\] \[\in\sigma_{g}+\frac{C_{T}}{\sqrt{n_{g,t}}}+\left[-\frac{C_{T}}{ \sqrt{n_{g,t}}},\frac{C_{T}}{\sqrt{n_{g,t}}}\right]\] \[=\sigma_{g}+\left[0,\frac{2C_{T}}{\sqrt{n_{g,t}}}\right],\] where the first Eq. is due to the definition of \(\text{UCB}_{t}\) introduced in (5), and the bounding is due to the definition of \(\mathcal{A}_{T}\). Next, notice that the function \(x\rightarrow\left(\sigma_{g}+x\right)^{\frac{2p}{p+1}}\) is increasing, which implies that \[\sigma_{g}^{\frac{2p}{p+1}}\leq\text{UCB}_{t}\left(\sigma_{g}\right)^{\frac{ 2p}{p+1}}\leq\left(\sigma_{g}+\frac{2C_{T}}{\sqrt{n_{g,t}}}\right)^{\frac{2p}{ p+1}},\] (8) which proves the leftmost inequality in Lemma 2. To prove the rightmost inequality, notice that \(x\rightarrow\left(\sigma_{g}+x\right)^{\frac{2p}{p+1}}\) is also convex, therefore by Jensen's inequality, \[\left(\sigma_{g}+\frac{2C_{T}}{\sqrt{n_{g,t}}}\right)^{\frac{2p}{p+1}}-\sigma _{g}^{\frac{2p}{p+1}}\leq\frac{2C_{T}}{\sqrt{n_{g,t}}}\frac{2p}{p+1}\left( \sigma_{g}+\frac{2C_{T}}{\sqrt{n_{g,t}}}\right)^{\frac{2p}{p+1}-1}=\frac{4C_ {T}}{\sqrt{n_{g,t}}}\frac{p}{p+1}\left(\sigma_{g}+\frac{2C_{T}}{\sqrt{n_{g,t}} }\right)^{\frac{p-1}{p+1}},\] which concludes the proof. 

### Proof of Lemma 3

The goal of this section is to prove Lemma 3, which consists of bounding with high probability \(\bm{n}-\bm{n}^{*}\). To do so, we will design an alternative sequence \(\bm{\tilde{n}}\), that is simultaneously easy to analyze, and upper bounds \(\bm{n}\) with high probability. The motivation for the choice of \(\bm{\tilde{n}}\) comes from the relaxation of the choice made at every time step by Variance-UCB.

We assume through the whole section what the event \(\mathcal{A}_{T}\) is realized. For convenience, we view the right hand side of Lemma 2 as a quantity of its own, and introduce the _width_ function

\[w_{g}:x>0\to w_{g}(x):=\frac{4C_{T}}{\Sigma_{p}\sqrt{x}}\frac{p}{p+1} \left(\sigma_{g}+\frac{2C_{T}}{\sqrt{x}}\right)^{\frac{p-1}{p+1}},\]

so that the inequality in Lemma 2 can be rewritten as

\[0\leq\text{UCB}_{t}\left(\sigma_{g}\right)^{\frac{2p}{p+1}}-\sigma_{g}^{ \frac{2p}{p+1}}\leq\Sigma_{p}w_{g}(n_{g,t}).\] (9)

We start by proving the following lemma, which follows from the decisions Variance-UCB makes:

**Lemma 7**.: _For \(t\geq 2G\), we have:_

\[n_{X_{t+1,t}}-tw_{X_{t+1}}(n_{X_{t+1,t}})\leq n_{X_{t+1,t}}^{*}\]

Proof.: The proof exploits the greedy property of the algorithm. A group needs to be sampled exactly twice to have a finite UCB. Therefore, for \(t=1,\ldots,2G\), Variance-UCB samples every group twice (in an arbitrary order), so that at \(n_{1,2G}=\ldots=n_{G,2G}=2\) and \(\text{UCB}_{2G}(\sigma_{1})^{\frac{2p}{p+1}},\ldots,\text{UCB}_{2G}(\sigma_{G} )^{\frac{2p}{p+1}}<+\infty\). By choice of \(X_{t+1}\), the following inequality holds:

\[\forall g\in[G],\quad\frac{\text{UCB}_{t}(\sigma_{g})^{\frac{2p}{p+1}}}{n_{g, t}}\leq\frac{\text{UCB}_{t}(\sigma_{X_{t+1}})^{\frac{2p}{p+1}}}{n_{X_{t+1,t}}}.\] (10)On the one hand, from the leftmost inequality in (9),

\[\forall g\in[G],\quad\frac{\sigma_{g}^{\frac{2p}{p+1}}}{n_{g,t}}\leq\frac{ \text{UCB}_{t}(\sigma_{g})^{\frac{2p}{p+1}}}{n_{g,t}}.\] (11)

On the other hand, from the rightmost inequality in (9),

\[\frac{\text{UCB}_{t}(\sigma_{X_{t+1}})^{\frac{2p}{p+1}}}{n_{X_{t+1},t}}\leq \frac{\sigma_{X_{t+1}}^{\frac{2p}{p+1}}+\Sigma_{p}w_{X_{t+1}}(n_{X_{t+1},t})}{ n_{X_{t+1},t}}.\] (12)

Therefore by combining both Inequalities (11) and (12) in Inequality (10),

\[\forall g\in[G],\quad\frac{\sigma_{g}^{\frac{2p}{p+1}}}{n_{g,t}}\leq\frac{ \sigma_{X_{t+1}}^{\frac{2p}{p+1}}+\Sigma_{p}w_{X_{t+1}}(n_{X_{t+1},t})}{n_{X_{t+ 1},t}},\]

which implies, after multiplying both sides by \(n_{g,t}n_{X_{t+1},t}\) and summing over \(g\in[G]\),

\[n_{X_{t+1},t}\underbrace{\sum_{g\in[G]}\sigma_{g}^{\frac{2p}{p+1}}}_{=\Sigma_ {p}}\leq\left(\sigma_{X_{t+1}}^{\frac{2p}{p+1}}+\Sigma_{p}w_{X_{t+1}}(n_{X_{t+ 1},t})\right)\underbrace{\sum_{g\in[G]}n_{g,t}}_{=t}.\]

Dividing both sides by \(\Sigma_{p}>0\), and using the formula \(n_{g,t}^{*}=\frac{\sigma_{g}^{\frac{2p}{p+1}}}{\Sigma_{p}}t\) (see Lemma 1) implies

\[n_{X_{t+1},t}\leq n_{X_{t+1},t}^{*}+tw_{X_{t+1}}(n_{X_{t+1},t}).\]

Lemma 7 follows by substracting \(tw_{X_{t+1}}(n_{X_{t+1},t})\) from both sides. 

Lemma 7 states that the possible excess between the number of samples output by the algorithm and the optimal number of samples is not too big, and can be controlled by the width \(w\). Since the width decreases in the number of samples, the function

\[x\to x-tw_{g}(x)\]

must be increasing and has therefore an inverse function that is also increasing, which we denote \(W_{g}^{t}(x)\). We introduce the following sequence, which mimics the behavior stated in Lemma 7:

\[\tilde{n}_{g,t} =n_{g,t} \text{For }t=1,\ldots,2G\] \[\tilde{n}_{g,t+1} =\tilde{n}_{g,t}+\mathbbm{1}\left(\tilde{n}_{g,t}\leq W_{g}^{t} \left(n_{g,t}^{*}\right)\right) \text{For }t\geq 2G\]

The sequence is easier to analyze and upper bounds the true number of samples \(n\):

**Lemma 8**.: \(\tilde{n}\geq\bm{n}\)_._

Proof.: By construction, the result holds for \(t=1,\ldots,2G\). Assume for the sake of contradiction that the result does not hold for a \(g\in[G]\) and \(t+1>2G\), and take such a \(t\) minimal. For such a pair \((g,t)\):

\[1\geq\mathbbm{1}(X_{t+1}=g) =n_{g,t+1}-n_{g,t}\] \[>\tilde{n}_{g,t+1}-\tilde{n}_{g,t}\] \[=\mathbbm{1}(\tilde{n}_{g,t}\leq W_{g}^{t}(n_{g,t}^{*}))\] \[=\mathbbm{1}(n_{g,t}\leq W_{g}^{t}(n_{g,t}^{*}))\geq 0,\]

where the first step follows from the definition of \(n\). By minimality of \(t\), \(n_{g,t+1}>\tilde{n}_{g,t+1}\) and \(n_{g,t}=\tilde{n}_{g,t}\), which induces the second step. The third step follows from the definition of \(\tilde{n}\), and the last step follows once again from the minimality of \(t\).

The strict inequality in the chain of inequalities implies that \(\mathbbm{1}(X_{t+1}=g)=1\) and \(\mathbbm{1}(n_{g,t}\leq W_{g}^{t}(n_{g,t}^{*}))=0\), so that

\[n_{X_{t+1},t}>W_{g}^{t}(n_{X_{t+1},t}^{*}).\]

By taking the inverse of the increasing function \(W_{g}^{t}\) on both sides, the previous inequality can be rewritten as

\[n_{X_{t+1},t}-tw_{X_{t+1}}(n_{X_{t+1},t})>n_{X_{t+1},t}^{*},\]

contradicting Lemma 7. Therefore the assumption is wrong and \(\tilde{n}\geq n\), which completes the proof.

**Lemma 9**.: _For a fixed \(g\),the sequence \(\{W_{g}^{t}(n_{g,t}^{*})\}_{t\geq 1}\) is increasing. Consequently,_

\[\tilde{n}_{g,t}\leq W_{g}^{t}(n_{g,t}^{*})^{+}+2\]

Proof.: For a fixed \(x>0\) and \(t\geq 1\),

\[(x-(t+1)w_{g}(x))-(x-tw_{g}(x))=-w_{g}(x)<0,\]

therefore the sequence of functions \(\{x\to x-tw_{g}(x)\}_{t\geq 1}\) is decreasing in \(t\). Consequentely, the sequence of its inverse functions \(\{x\to W_{g}^{t}(x)\}_{t\geq 1}\) is increasing in \(t\):

\[W_{g}^{t}(n_{g,t+1}^{*})\geq W_{g}^{t}(n_{g,t+1}^{*}).\] (13)

Moreover, the function \(W_{g}^{t}\) is increasing in \(\mathbb{R}_{+}\):

\[W_{g}^{t}(n_{g,t+1}^{*})\geq W_{g}^{t}(n_{g,t}^{*}).\] (14)

By combining Equations (13) and (14), we obtain

\[W_{g}^{t+1}(n_{g,t+1}^{*})\geq W_{g}^{t}(n_{g,t}^{*}),\]

which proves that the sequence \(\{W_{g}^{t}(n_{g,t}^{*})\}_{t\geq 1}\) is increasing, thus completing the proof for the first part of Lemma 9.

The second part follows by induction on \(t\geq 1\). For \(t\leq 2G\), the result holds immediately as \(\tilde{n}_{g,t}=n_{g,t}\) and \(n_{g,t}\leq 2\). We assume the result holds for a \(t\geq 2G\). We distinguish two cases:

* \(\tilde{n}_{g,t}\leq W_{g}^{t}\left(n_{g,t}^{*}\right)\): This implies that: \[\tilde{n}_{g,t+1} =\tilde{n}_{g,t}+1\] \[\leq W_{g}^{t}\left(n_{g,t}^{*}\right)+1\] \[\leq W_{p}^{t+1,g}\left(n_{g,t+1}^{*}\right)^{+}+2,\] where the first step stems from the definition of \(\tilde{n}\), the second step stems from the assumption \(\tilde{n}_{g,t}\leq W_{g}^{t}\left(n_{g,t}^{*}\right)\), and the third step stems from the first part of the proof.
* \(\tilde{n}_{g,t}>W_{g}^{t}\left(n_{g,t}^{*}\right)\): This implies that: \[\tilde{n}_{g,t+1} =\tilde{n}_{g,t}\] \[\leq W_{g}^{t}\left(n_{g,t}^{*}\right)^{+}+2\] \[\leq W_{p}^{t+1,g}\left(n_{g,t+1}^{*}\right)^{+}+2,\] where the first step stems from the definition of \(\tilde{n}\), the second step stems from the induction hypothesis, and the third step stems from the first part of the proof.

Studying both cases concludes the induction and proves the inequality for all \(t\geq 1\). This concludes the proof of Lemma 9. 

Next, we derive an upper bound on \(W_{g}^{t}\):

**Lemma 10**.: _For \(x>0\), \(W_{g}^{t}(x)\leq\frac{x}{1-t\frac{w_{g}(x)}{x}}\)._

Proof.: Let \(x,y>0\) with \(0\leq y-tw_{g}(y)=x\), so that \(y=W_{g}^{t}(x)\) by definition of \(W_{g}^{t}\). we have:

\[\frac{x}{1-t\frac{w_{g}(x)}{x}} =\frac{y-tw_{g}(y)}{1-t\frac{w_{g}(y-tw_{g}(y))}{x-tw_{g}(x)}}\] \[=y\frac{1-t\frac{w_{g}(y)}{y}}{1-t\frac{w_{g}(y-tw_{g}(y))}{y-tw_ {g}(y)}}\] \[\geq y=W_{g}^{t}(x),\]

where the last step follows from the function \(y\to\frac{w_{g}(y)}{y}\) being decreasing, which concludes the proof.

We are now ready to prove Lemma 3.

**Lemma 3**.: _Variance-UCB collects a vector of samples \(\bm{n}\) such that for all \(g\in[G]\), with probability at least \(1-\tilde{O}(T^{-2})\),_

\[n_{g,T}-n_{g,T}^{*}\leq 3+\frac{4C_{T}p}{\Sigma_{p}(p+1)}\left(\sigma_{g}+ \frac{2C_{T}}{\sqrt{n_{g,T}^{*}}}\right)^{\frac{p-1}{p+1}}\sqrt{n_{g,T}^{*}}= \Theta(\sqrt{T}).\]

Proof.: Conditionally on \(\mathcal{A}_{T}\), all the previous lemmas stated in this section hold. Consequently,

\[n_{g,T} \leq\tilde{n}_{g,T}\] \[\leq 2+W_{g}^{t}(n_{g,T}^{*})^{+}\] \[\leq 2+\frac{n_{g,T}^{*}}{1-T\frac{w_{g}(n_{g,T}^{*})}{n_{g,T}^{* }}}\] \[\leq 2+n_{g,T}^{*}+1+Tw_{g}(n_{g,T}^{*})\] \[=3+n_{g,T}^{*}+\frac{4C_{T}p}{\Sigma_{p}(p+1)}\left(\sigma_{g}+ \frac{2C_{T}}{\sqrt{n_{g,T}^{*}}}\right)^{\frac{p-1}{p+1}}\sqrt{n_{g,T}^{*}},\]

where the first step stems from Lemma 8, the second step stems from Lemma 9, the third step stems from first order approximations and the last step stems from Lemma 10. From Assumption 2, \(\bm{\sigma}>0\), therefore \(\bm{n}^{*}=\Theta(T)\) and the right hand side is \(\Theta(\sqrt{T})\). 

### Proof of Lemma 4

The goal of this section is to prove Lemma 4. We will do so by combining the optimality of \(\bm{n}^{*}\) and the curvature properties of \(R_{p}\).

**Lemma 4**.: _Let \(p<+\infty\) and \(\bm{n}^{\prime}\in\mathbb{R}_{+}^{G}\) such that \(\sum_{g\in[G]}n_{g}^{\prime}=T\). Then,_

\[\left|\frac{R_{p}(\bm{n}^{\prime})-R_{p}(\bm{n}^{*}_{T})}{R_{p}(\bm{n}^{*})}- \frac{p+1}{2}\sum_{g\in[G]}\frac{(n_{g}^{\prime}-n_{g,T}^{*})^{2}}{Tn_{g,T}^{ *}}\right|\leq\frac{7(p+2)^{2}\Sigma_{p}^{2}}{\sigma_{\min}^{2}}\max_{g}\left( \frac{n_{g,T}^{*}}{n_{g}^{\prime}}\right)^{3p+3}\frac{\|\bm{n}^{\prime}-\bm{n} _{T}^{*}\|_{\infty}^{3}}{T^{3}}.\]

Proof.: Since \(R_{p}\) is defined over the constrained set \(\{\bm{n}\in\mathbb{R}_{+}^{G}|\sum_{g}n_{g}=T\}\) with empty interior, Taylor inequality can't be directly applied. To overcome this, we construct an alternative function that has the same curvature as \(R_{p}\) while being defined on a non-empty interior domain. Define

\[\mathcal{K}:=\left\{\bm{\lambda}\in[0,1]^{G-1}\Big{|}\sum_{g}\lambda_{g}\leq 1 \right\}.\] (15)

For each vector \(\bm{n}\in\mathbb{R}_{+}^{G}\) with \(\sum_{t=1}^{T}n_{g}=T\), we set for each \(g\in[G-1]\)\(\lambda_{g}:=\frac{n_{g}}{T}\). We have \(\bm{\lambda}\in\mathcal{K}\). Moreover, we have from (1):

\[R_{p}(\bm{n})=\left\|\left\{\frac{\sigma_{g}^{2}}{n_{g}}\right\}_{g=1}^{G} \right\|_{p}=\frac{1}{T}\left\|\left\{\frac{\sigma_{1}^{2}}{\lambda_{1}}, \ldots,\frac{\sigma_{G-1}^{2}}{\lambda_{G-1}},\frac{\sigma_{G}^{2}}{1-\lambda_ {1}-\ldots-\lambda_{G-1}}\right\}\right\|_{p}.\] (16)

where the last equality follows from the homogeneity property of the \(p\)-norm. Eq. (16) motivates the introduction of the re-scaled function \(r\):

\[r_{p}:\bm{\lambda}\in\mathcal{K}\to r_{p}(\bm{\lambda}):=\left\|\left\{ \frac{\sigma_{1}^{2}}{\lambda_{1}},\ldots,\frac{\sigma_{G-1}^{2}}{\lambda_{G-1 }},\frac{\sigma_{G}^{2}}{1-\lambda_{1}-\ldots-\lambda_{G-1}}\right\}\right\|_ {p},\] (17)

so that Eq. (16) can be written as

\[R_{p}(\bm{n})=\frac{1}{T}r_{p}(\bm{\lambda}).\] (18)From the definition of \(\mathcal{K}\) in (15), the interior of \(\mathcal{K}\), denoted \(\mathcal{K}^{\circ}\), is non-empty and is equal to

\[\mathcal{K}^{\circ}=\left\{\boldsymbol{\lambda}\in\mathcal{K}|\forall g\in[G-1 ],\lambda_{g}>0,\quad\sum_{g\in[G-1]}\lambda_{g}<1\right\}.\]

Moreover, the function \(r_{p}\) is \(\mathcal{C}^{3}\) in \(\mathcal{K}^{\circ}\). From Lemma 1, \(\boldsymbol{\lambda}^{*}\in\mathcal{K}^{\circ}\). Therefore from Taylor's theorem, we have for \(\boldsymbol{\lambda}\in\mathcal{K}^{\circ}\):

\[\left|r_{p}(\boldsymbol{\lambda})-r_{p}(\boldsymbol{\lambda}^{*})-( \boldsymbol{\lambda}-\boldsymbol{\lambda}^{*})\nabla r_{p}(\boldsymbol{ \lambda}^{*})-\frac{1}{2}\langle\mathcal{H}(\boldsymbol{\lambda}^{*})( \boldsymbol{\lambda}-\boldsymbol{\lambda}^{*}),\boldsymbol{\lambda}- \boldsymbol{\lambda}^{*}\rangle\right|\leq\|\boldsymbol{\lambda}-\boldsymbol {\lambda}^{*}\|_{\infty}^{3}\sup_{\begin{subarray}{c}\boldsymbol{u}\in[ \boldsymbol{\lambda},\boldsymbol{\lambda}^{*}]\\ x\neq y\neq 0\\ x\neq y\neq 0\end{subarray}}\left|\frac{1}{x^{\gamma}y!x!}\frac{\partial^{3}r_{p}( \boldsymbol{u})}{\partial\lambda_{g}\partial\lambda_{h}\partial\lambda_{i}} \right|,\] (19)

where \(\nabla\) is the gradient operator and \(\mathcal{H}\) is the hessian operator. We exploit the optimality of \(\boldsymbol{\lambda}^{*}\) to derive simple expressions for \(\nabla r_{p}(\boldsymbol{\lambda}^{*})\), \(\mathcal{H}(\boldsymbol{\lambda}^{*})\), and \(\frac{\partial^{3}r_{p}(\boldsymbol{u})}{\partial\lambda_{g}\partial\lambda_{ h}\partial\lambda_{i}}\). First, since \(\boldsymbol{\lambda}^{*}\) is optimal and an interior point of \(\mathcal{K}\), the following equality holds:

\[\nabla r_{p}(\boldsymbol{\lambda}^{*})=0_{G}.\] (20)

In particular, \(\langle\boldsymbol{\lambda}-\boldsymbol{\lambda}^{*},\nabla r_{p}(\boldsymbol {\lambda}^{*})\rangle=0\). \(\langle\mathcal{H}(\boldsymbol{\lambda}^{*})(\boldsymbol{\lambda}-\boldsymbol {\lambda}^{*}),\boldsymbol{\lambda}-\boldsymbol{\lambda}^{*}\rangle\) is simpified in the following lemma, which proof is deferred to A.6:

**Lemma 11**.: \(\langle\mathcal{H}(\boldsymbol{\lambda}^{*})(\boldsymbol{\lambda}-\boldsymbol {\lambda}^{*}),\boldsymbol{\lambda}-\boldsymbol{\lambda}^{*}\rangle=(p+1)r_{ p}(\boldsymbol{\lambda}^{*})\sum_{g\in[G]}\frac{(\lambda_{g}-\lambda_{g}^{*})^{2}}{ \lambda_{g}^{*}}\)_._

so that Inequality (19) is rewritten as

\[\left|r_{p}(\boldsymbol{\lambda})-r_{p}(\boldsymbol{\lambda}^{*})-\frac{1}{2 }(p+1)r_{p}(\boldsymbol{\lambda}^{*})\sum_{g\in[G]}\frac{(\lambda_{g}-\lambda _{g}^{*})^{2}}{\lambda_{g}^{*}}\right|\leq\|\boldsymbol{\lambda}-\boldsymbol {\lambda}^{*}\|_{\infty}^{3}\sup_{\begin{subarray}{c}\boldsymbol{u}\in[ \boldsymbol{\lambda},\boldsymbol{\lambda}^{\prime}]\\ x,y,z\in\mathbb{N}\\ x\neq y+z=3\\ \{g,h,i\}\subset[G-1]\end{subarray}}\left|\frac{1}{x!y!z!}\frac{\partial^{3}r_{p}( \boldsymbol{u})}{\partial\lambda_{g}\partial\lambda_{h}\partial\lambda_{i}} \right|.\] (21)

It remains to bound \(\frac{1}{x!y!z!}\frac{\partial^{3}r_{p}(\boldsymbol{u})}{\partial\lambda_{g} \partial\lambda_{h}\partial\lambda_{i}}\). To do this, the following lemma is applied (the proof is also deferred to A.6):

**Lemma 12**.: _The following inequality holds:_

\[\sup_{\begin{subarray}{c}\boldsymbol{u}\in[\boldsymbol{\lambda},\boldsymbol{ \lambda}^{\prime}]\\ x,y,z\in\mathbb{N}\\ x\neq y\neq z=3\\ \{g,h,i\}\subset[G-1]\end{subarray}}\left|\frac{1}{x!y!z!}\frac{\partial^{3}r_{ p}(\boldsymbol{u})}{\partial\lambda_{g}\partial\lambda_{h}\partial\lambda_{i}} \right|\leq\frac{7(p+2)^{2}}{(\min_{g}\lambda_{g}^{*})^{2}}\max_{g}\left(\frac {\lambda_{g}^{*}}{\lambda_{g}}\right)^{3p+3}r_{p}(\boldsymbol{\lambda}^{*}).\]

which implies after dividing both sides by \(r_{p}(\boldsymbol{\lambda})\)

\[\left|\frac{r_{p}(\boldsymbol{\lambda})-r_{p}(\boldsymbol{\lambda}^{*})}{r_ {p}(\boldsymbol{\lambda}^{*})}-\frac{p+1}{2}\sum_{g\in[G]}\frac{(\lambda_{g}- \lambda_{g}^{*})^{2}}{\lambda_{g}^{*}}\right|\leq\frac{7(p+2)^{2}}{(\min_{g }\lambda_{g}^{*})^{2}}\max_{g}\left(\frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^ {3p+3}\|\boldsymbol{\lambda}-\boldsymbol{\lambda}^{*}\|_{\infty}^{3}.\] (22)

By using the change of variable from \(\boldsymbol{\lambda}\) to \(\boldsymbol{n}^{\prime}\), we have

\[R_{p}(\boldsymbol{n}^{\prime})=\frac{1}{T}r_{p}(\boldsymbol{\lambda}),\quad R _{p}(\boldsymbol{n}^{*})=\frac{1}{T}r_{p}(\boldsymbol{\lambda}^{*}),\quad \|\boldsymbol{\lambda}-\boldsymbol{\lambda}^{*}\|_{\infty}\leq\frac{\|\boldsymbol {n}^{\prime}-\boldsymbol{n}^{*}\|_{\infty}}{T}\]

which implies

\[\frac{R_{p}(\boldsymbol{n}^{\prime})-R_{p}(\boldsymbol{n}_{T}^{*})}{R_{p}( \boldsymbol{n}^{*})}\leq\frac{p+1}{2}\sum_{g\in[G]}\frac{(n_{g}^{\prime}-n_{g,T}^{*})^{2}}{Tn_{g,T}^{*}}+\frac{7(p+2)^{2}\Sigma_{p}^{2}}{\sigma_{\min}^{2}} \max_{g}\left(\frac{n_{g,T}^{*}}{n_{g}^{\prime}}\right)^{3p+3}\frac{\| \boldsymbol{n}^{\prime}-\boldsymbol{n}_{T}^{*}\|_{\infty}^{3}}{T^{3}},\]

hence proving Lemma 5.

### Putting everything together

We are now ready to complete the proof of Theorem 1.

**Theorem 1**.: _For any \(\bm{\mathcal{D}}\) that satisfies Assumptions 1 and 2 and for any finite \(p\), the regret of Variance-UCB is at most \(\tilde{O}(T^{-2})\). That is,_

\[\text{Regret}_{p,T}(\text{Variance-UCB},\bm{\mathcal{D}})=\tilde{O}(T^{-2}).\]

Proof.: First, notice that

\[\text{Regret}_{p,T}(\text{Variance-UCB}) =\mathbb{E}_{\pi}[R_{p}(\bm{n})-R_{p}^{*}]\] \[=\mathbb{E}[R_{p}(\bm{n})-R_{p}^{*}|\mathcal{A}_{T}]\mathbb{P}_{ \pi}(\mathcal{A}_{T})+\mathbb{E}[R_{p}(\bm{n})-R_{p}^{*}|\mathcal{A}_{T}^{c}] \mathbb{P}_{\pi}(\mathcal{A}_{T}^{c})\] \[\leq\mathbb{E}[R_{p}(\bm{n})-R_{p}^{*}|\mathcal{A}_{T}]+\|\bm{ \sigma}^{2}\|_{p}\mathbb{P}_{\pi}(\mathcal{A}_{T}^{c}),\]

where the first step stems from the definition of regret introduced in Eq. (4), the second step stems from the law of total expectation, and the third step stems from both \(\mathbb{P}(\mathcal{A}_{T})\leq 1\) and \(R_{p}(\bm{n})-R_{p}^{*}\leq\|\bm{\sigma}^{2}\|_{p}\). It remains to show that each term in the rightmost side is in \(\tilde{O}(T^{-2})\). First, \(\mathbb{P}_{\pi}(\mathcal{A}_{T}^{c})\leq 2GT^{-2.5}=\tilde{O}(T^{-2})\). Next, we have conditionally on \(\mathcal{A}_{T}\):

\[\frac{R_{p}(\bm{n})-R_{p}(\bm{n}^{*})}{R_{p}(\bm{n}^{*})} \leq\frac{p+1}{2}\sum_{g\in[G]}\frac{(n_{g}-n_{g,T}^{*})^{2}}{Tn_ {g,T}^{*}}+\frac{7(p+2)^{2}\Sigma_{p}^{2}}{\sigma_{\min}^{2}}\max_{g}\left( \frac{n_{g,T}^{*}}{n_{g}}\right)^{3p+3}\frac{\|\bm{n}-\bm{n}^{*}\|_{\infty}^{3} }{T^{3}}\] \[\leq\frac{p+1}{2}\frac{G\|\bm{n}-\bm{n}^{*}\|_{\infty}^{2}}{T\min _{g}n_{g,T}^{*}}+\frac{7(p+2)^{2}\Sigma_{p}^{2}}{\sigma_{\min}^{2}}\max_{g} \left(\frac{n_{g,T}^{*}}{n_{g}}\right)^{3p+3}\frac{\|n-n^{*}\|_{\infty}^{3}}{T ^{3}},\]

where the first inequality stems from Lemma 4, and the second inequality stems from \((n_{g,T}-n_{g,T}^{*})^{2}\leq\|\bm{n}-\bm{n}^{*}\|_{\infty}^{2}\). Since \(\sum_{g}n_{g,T}=T\), from Lemma 3,

\[n_{g,T}-n_{g,T}^{*} =-\sum_{h\neq g}n_{h,T}-n_{h,T}^{*}\] \[\geq-3(G-1)-\sum_{h\neq g}\frac{4C_{T}p}{\Sigma_{p}(p+1)}\left( \sigma_{g}+\frac{2C_{T}}{\sqrt{n_{h,T}^{*}}}\right)^{\frac{p-1}{p+1}}\sqrt{n_ {h,T}^{*}}\] \[\geq-3G-\frac{4GC_{T}p}{\Sigma_{p}(p+1)}\left(\min\sigma_{g}+ \frac{2C_{T}}{\sqrt{\min_{h}n_{h,T}^{*}}}\right)^{\frac{p-1}{p+1}}\sqrt{\min_ {h}n_{h,T}^{*}},\]

where the first step stems from \(\sum_{h}n_{h,T}-n_{h,T}^{*}=\sum_{h}n_{h,T}-\sum_{h}n_{h,T}^{*}=T-T=0\), the second step stems from Lemma 3, and the last steps stem from taking the max over the sum. The last inequality implies

\[\|\bm{n}-\bm{n}^{*}\|_{\infty} \leq 3G+\frac{4GC_{T}p}{\Sigma_{p}(p+1)}\left(\min\sigma_{g}+ \frac{2C_{T}}{\sqrt{\min_{h}n_{h,T}^{*}}}\right)^{\frac{p-1}{p+1}}\sqrt{\min_ {h}n_{h,T}^{*}}.\] (23)

In particular, \(\|\bm{n}-\bm{n}^{*}\|_{\infty}=\tilde{O}(\sqrt{\min_{h}n_{h,t}^{*}})=\tilde{O} (\sqrt{T})\) and

\[\max_{g}\frac{n_{g,T}^{*}}{n_{g,T}}\leq\frac{1}{1-\frac{\|\bm{n}-\bm{n}^{*}\|_{ \infty}}{\min_{h}n^{*}}}=\frac{1}{1-\tilde{O}(T^{-0.5})}=\tilde{O}(1).\]

Therefore,

\[\frac{p+1}{2}\frac{G\|n-n^{*}\|_{\infty}^{2}}{T\min_{g}n_{g,T}^{* }}+\frac{7(p+2)^{2}\Sigma_{p}^{2}}{\sigma_{\min}^{2}}\max_{g}\left(\frac{n_{g, T}^{*}}{n_{g}}\right)^{3p+3}\frac{\|\bm{n}-\bm{n}^{*}\|_{\infty}^{3}}{T^{3}} =\frac{p+1}{2}\frac{G\tilde{O}(T)}{T\Theta(T)}+\frac{7(p+2)^{2} \Sigma_{p}^{2}}{\sigma_{\min}^{2}}\tilde{O}(1)\frac{\tilde{O}(T^{1.5})}{T^{3}}\] \[=\tilde{O}(T^{-1}).\]Recall from Lemma 1 that \(R_{p}^{*}=R_{p}(\bm{n}^{*})=\Theta(T^{-1})\). Thus by taking the conditional expectation on \(\mathcal{A}_{T}\), we have

\[\mathbb{E}[R_{p}(\bm{n})-R_{p}^{*}|\mathcal{A}_{T}]\leq R_{p}^{*}\tilde{O}(T^{-1 })=\tilde{O}(T^{-2}),\]

which concludes the proof of Theorem 1. 

### Proof of auxiliary lemmas

In this section, we prove Lemmas 11 and 12. To do so, simple expressions for the derivatives of \(r_{p}\) are needed. These are established in the following lemma:

**Lemma 13**.: _For \(g,h,i\in[G-1]\), we introduce the following functions in \(\mathcal{K}^{\circ}\):_

\[H_{g}:\bm{\lambda}\in\mathcal{K} \to\frac{\sigma_{G}^{2p}}{(1-\lambda_{1}-\ldots-\lambda_{G-1})^{p +1}}-\frac{\sigma_{g}^{2p}}{\lambda_{g}^{p+1}}\] \[G_{h,g}:=\frac{1}{p+1}\frac{\partial}{\partial\lambda_{g}}H_{h},\] \[I_{g,h,i}:=\frac{1}{p+2}\frac{\partial}{\partial\lambda_{i}}G_{ g,h}.\]

_The following holds:_

1. \(\nabla r_{p}=r_{p}^{1-p}\bm{H}\)_._
2. \(\mathcal{H}_{g,h}=(1-p)r_{p}^{1-2p}H_{g}H_{h}+(p+1)r_{p}^{1-p}G_{g,h}\)_._
3. \[\frac{\partial^{3}r_{p}}{\partial\lambda_{g}\partial\lambda_{h} \partial_{i}} =(1-p)(1-2p)H_{g}H_{h}H_{i}r_{p}^{1-3p}\] \[+(1-2p)(1+p)\left(H_{g}G_{h,i}+H_{h}G_{i,g}+H_{g}G_{h,i}\right)\] \[+r_{p}^{1-2p}+(p+1)(p+2)I_{g,h,i}r_{p}^{1-p}.\]

Proof.: Fix a \(\lambda\in\mathcal{K}^{\circ}\) and \(g,h,i\in[G-1]\).

**Expression of the gradient:** On the one hand,the definition of \(r_{p}\) in (17) implies

\[r_{p}^{p}(\bm{\lambda})=\left\|\left\{\frac{\sigma_{1}^{2}}{\lambda_{1}}, \ldots,\frac{\sigma_{G-1}^{2}}{\lambda_{G-1}},\frac{\sigma_{G}^{2}}{1-\lambda_ {1}-\ldots-\lambda_{G-1}}\right\}\right\|_{p}^{p}=\frac{\sigma_{G}^{2p}}{(1- \lambda_{1}-\ldots-\lambda_{G-1})^{p}}+\sum_{h\leq G-1}\frac{\sigma_{h}^{2p}} {\lambda_{h}^{p}},\]

so that

\[\frac{\partial}{\partial\lambda_{g}}(r_{p}^{p})(\bm{\lambda})=p\left[\frac{ \sigma_{G}^{2p}}{(1-\lambda_{1}-\ldots-\lambda_{G-1})^{p+1}}-\frac{\sigma_{g} ^{2p}}{\lambda_{g}^{p+1}}\right]=pH_{g}(\bm{\lambda}).\] (24)

On the other hand, from the formula \((f^{p})^{\prime}=pf^{\prime}f^{p-1}\),

\[\frac{\partial}{\partial\lambda_{g}}(r_{p}^{p})(\bm{\lambda})=pr_{p}^{p-1}( \lambda)\frac{\partial}{\partial\lambda_{g}}r_{p}(\bm{\lambda}),\] (25)

Combining Equations (24) and (25) yields

\[\frac{\partial}{\partial\lambda_{g}}r_{p}^{p}=pH_{g}=pr_{p}^{p-1}\frac{ \partial}{\partial\lambda_{g}}r_{p},\]

so that

\[\frac{\partial}{\partial\lambda_{g}}r_{p}=r_{p}^{1-p}H_{g}.\] (26)

Therefore \(\nabla r_{p}=\left(\frac{\partial}{\partial\lambda_{1}}r_{p},\ldots,\frac{ \partial}{\partial\lambda_{G-1}}r_{p}\right)=r_{p}^{1-p}(H_{1},\ldots,H_{G-1}) =r_{p}^{1-p}\bm{H}\), which proves the first Eq. of Lemma 13.

**Expression of the Hessian:** We have

\[\mathcal{H}_{g,h} =\frac{\partial^{2}}{\partial\lambda_{g}\partial\lambda_{h}}r_{p}\] \[=\frac{\partial}{\partial\lambda_{g}}\left(r_{p}^{1-p}H_{h}\right)\] \[=(1-p)r_{p}^{-p}\left(\frac{\partial}{\partial\lambda_{g}}r_{p} \right)H_{h}+r_{p}^{1-p}\frac{\partial}{\partial\lambda_{g}}H_{h}\] \[=(1-p)r_{p}^{1-2p}H_{g}H_{h}+(p+1)r_{p}^{1-p}G_{g,h}.\]

where the first equality is due to the definition of the Hessian, the second equality is due to the expression of the gradient from Eq. (26), the third equality applies the product rule to the derivative, and the fourth equality applies the definition of \(G_{g,h}\) in Lemma 13. This proves the second Equality of Lemma 13.

**Third derivatives.**

\[\frac{\partial^{3}}{\partial\lambda_{g}\partial\lambda_{h}\partial _{i}}r_{p} =\frac{\partial}{\partial\lambda_{i}}\mathcal{H}_{g,h}\] \[=\frac{\partial}{\partial\lambda_{i}}\left\{(1-p)r_{p}^{1-2p}H_ {g}H_{h}+(p+1)r_{p}^{1-p}G_{g,h}\right\}\] \[=(1-p)\frac{\partial}{\partial\lambda_{i}}\left\{r_{p}^{1-2p}H_ {g}H_{h}\right\}+(p+1)\frac{\partial}{\partial\lambda_{i}}\left\{r_{p}^{1-p}G _{g,h}\right\},\]

where the first equality is due to the definition of the Hessian, the second inequality is due to the Hessian expression established previously, and the third equality is due to the linearity of derivation. In a similar fashion to the previous case, we deviate each of the products \(r_{p}^{1-2p}H_{g}H_{h}\) and \(r_{p}^{1-p}G_{g,h}\) separately. On the one hand,

\[\frac{\partial}{\partial\lambda_{i}}\{r_{p}^{1-2p}H_{g}H_{h}\} =H_{g}H_{h}\frac{\partial}{\partial\lambda_{i}}\{r_{p}^{1-2p}\} +r_{p}^{1-2p}\left(H_{g}\frac{\partial}{\partial\lambda_{i}}H_{h}+H_{h}\frac {\partial}{\partial\lambda_{i}}H_{g}\right)\] \[=H_{g}H_{h}(1-2p)r_{p}^{-2p}r_{p}^{1-p}H_{i}+r_{p}^{1-2p}\left(H_ {g}(p+1)G_{h,i}+H_{h}(p+1)G_{g,i}\right)\] \[=(1-2p)r_{p}^{1-3p}H_{g}H_{h}H_{i}+(p+1)r_{p}^{1-2p}\left(H_{g}G _{h,i}+H_{h}G_{g,i}\right),\]

where the first equality is due to the derivation product rule, the second equality is due to the definition of \(G_{g,h}\) introduced in Lemma 13, and the third equality is due to a reordering of the terms. On the other hand, by following the exact same steps

\[\frac{\partial}{\partial\lambda_{i}}\left\{r_{p}^{1-p}G_{g,h}\right\} =(1-p)r_{p}^{-p}\frac{\partial}{\partial\lambda_{i}}r_{p}+r_{p}^{ 1-p}\frac{\partial}{\partial\lambda_{i}}G_{g,h}\] \[=(1-p)r_{p}^{-p}r_{p}^{1-p}H_{i}G_{g,h}+(p+2)r_{p}^{1-p}I_{g,h,i}\] \[=(1-p)r_{p}^{1-2p}H_{i}G_{g,h}+(p+2)r_{p}^{1-p}I_{g,h,i}.\]

Replacing the previous two expressions in the formula for \(\frac{\partial^{3}}{\partial\lambda_{g}\partial\lambda_{h}\partial_{i}}r_{p}\) yields

\[\frac{\partial^{3}}{\partial\lambda_{g}\partial\lambda_{h}\partial _{i}}r_{p} =(1-p)(1-2p)H_{g}H_{h}H_{i}r_{p}^{1-3p}\] \[+(1-2p)(1+p)\left(H_{g}G_{h,i}+H_{h}G_{i,g}+H_{g}G_{h,i}\right)\] \[+r_{p}^{1-2p}+(p+1)(p+2)I_{g,h,i}r_{p}^{1-p},\]

where the symmetry of \(G_{g,h}=G_{h,g}\) is used. This derives the third Eq. of Lemma 13 and concludes the proof. 

We are now ready to establish the proof of Lemma 11.

**Lemma 11**.: \(\langle\mathcal{H}(\bm{\lambda}^{*})(\bm{\lambda}-\bm{\lambda}^{*}),\bm{ \lambda}-\bm{\lambda}^{*}\rangle=(p+1)r_{p}(\bm{\lambda}^{*})\sum_{g\in[G]} \frac{(\lambda_{g}-\lambda_{g}^{*})^{2}}{\lambda_{g}^{*}}\)_._Proof.: Setting the value \(\bm{\lambda}^{*}\) in the value of the Hessian established in Lemma 13 implies that for \(g,h\in[G]\):

\[\mathcal{H}_{g,h}(\bm{\lambda}^{*}) =(1-p)r_{p}^{1-2p}H_{g}H_{h}(\bm{\lambda}^{*})+(p+1)r_{p}^{1-p}( \bm{\lambda}^{*})G_{g,h}(\bm{\lambda}^{*})\] \[=(p+1)r_{p}^{1-p}(\bm{\lambda}^{*})G_{g,h}(\bm{\lambda}^{*})\] \[=(p+1)r_{p}^{1-p}(\bm{\lambda}^{*})\Sigma_{p}^{p+1}\left(\frac{1} {\lambda_{G}^{*}}+\frac{1}{\lambda_{g}^{*}}\mathbbm{1}(g=h)\right)\] \[=(p+1)r_{p}(\bm{\lambda}^{*})\left(\frac{1}{\lambda_{G}^{*}}+ \frac{1}{\lambda_{g}^{*}}\mathbbm{1}(g=h)\right)\]

where the first equality stems from the definition of the Hessian, the second equality stems from \(\bm{H}(\bm{\lambda}^{*})=0\) due to the optimality of \(\bm{\lambda}^{*}\), the third equality stems from the definition of \(G_{g,h}\), and the fourth equality stems from \(r_{p}(\bm{\lambda}^{*})=\Sigma_{p}^{\frac{1}{p}-1}\). As a consequence,

\[\langle\mathcal{H}(\bm{\lambda}^{*})(\bm{\lambda}-\bm{\lambda}^{ *}),\bm{\lambda}-\bm{\lambda}^{*}\rangle =\sum_{g,h\in[G-1]}\mathcal{H}_{g,h}(\lambda-\lambda^{*})_{g}( \lambda-\lambda^{*})_{h}\] \[=(p+1)r_{p}(\bm{\lambda}^{*})\sum_{g,h\in[G-1]}\left(\frac{1}{ \lambda_{G}^{*}}+\frac{1}{\lambda_{g}^{*}}\mathbbm{1}(g=h)\right)(\lambda- \lambda^{*})_{g}(\lambda-\lambda^{*})_{h}\] \[=(p+1)r_{p}(\bm{\lambda}^{*})\left\{\frac{\sum_{g,h\in[G-1]}( \lambda-\lambda^{*})_{g}(\lambda-\lambda^{*})_{h}}{\lambda_{G}^{*}}+\sum_{g\in [G-1]}\frac{(\lambda-\lambda^{*})_{g}^{2}}{\lambda_{g}^{*}}\right\}\] \[=(p+1)r_{p}(\bm{\lambda}^{*})\left\{\frac{\left(\sum_{g\in[G-1]} (\lambda-\lambda^{*})_{g}\right)^{2}}{\lambda_{G}^{*}}+\sum_{g\in[G-1]}\frac{ (\lambda-\lambda^{*})_{g}^{2}}{\lambda_{g}^{*}}\right\}\] \[=(p+1)r_{p}(\bm{\lambda}^{*})\sum_{g\in[G]}\frac{(\lambda_{g}- \lambda_{g}^{*})^{2}}{\lambda_{g}^{*}},\]

where the first step follows from the definition of the scalar product, and the second step stems from the expression of \(\mathcal{H}(\bm{\lambda}^{*})\) derived previously in the proof. In the third step, the sum is distributed over the terms \(\frac{1}{\lambda_{G}^{*}}\) and \(\frac{1}{\lambda_{g}^{*}}\mathbbm{1}(g=h)\). In the fourth step, the first sum is factorized, and in the fifth step, the equality \(\sum_{g\in[G-1]}(\lambda-\lambda^{*})_{g}=(1-\lambda_{G})-(1-\lambda_{G}^{*})= \lambda_{G}^{*}-\lambda_{G}\) is used. This completes the proof of Lemma 11. 

Next, we establish the proof of Lemma 12.

**Lemma 12**.: _The following inequality holds:_

\[\sup_{\begin{subarray}{c}\bm{u}\in[\bm{\lambda},\bm{\lambda}^{*}]\\ x,y,z\in\mathbb{N}\\ x+y+z=3\\ \{g,h,i\}\subset[G-1]\end{subarray}}\left|\frac{1}{x!y!z!}\frac{\partial^{3}r_ {p}(\bm{u})}{\partial\lambda_{g}\partial\lambda_{h}\partial\lambda_{i}} \right|\leq\frac{7(p+2)^{2}}{(\min_{g}\lambda_{g}^{*})^{2}}\max_{g}\left(\frac {\lambda_{g}^{*}}{\lambda_{g}}\right)^{3p+3}r_{p}(\bm{\lambda}^{*}).\]

Proof.: From Lemma 1, the following equality holds for each \(g\in[G]\):

\[\sigma_{g}^{2p}=\left(\sum_{h\in[G]}\sigma_{h}^{\frac{2p}{p+1}}\right)^{p+1}( \lambda_{g}^{*})^{p+1}=\Sigma_{p}^{p+1}(\lambda_{g}^{*})^{p+1}.\]

For a fixed \(\bm{u}\in[\bm{\lambda},\bm{\lambda}^{*}]\), \(\sum_{g}u_{g}=\sum_{g}\lambda_{g}=\sum_{g}\lambda_{g}^{*}=1\), thus the coordinate \(g_{0}\) achieving the maximal \(\frac{\lambda_{g}^{*}}{u_{g}}\) must have \(u_{g_{0}}\leq\lambda_{g_{0}}^{*}\). Since \(\bm{u}\in[\bm{\lambda},\bm{\lambda}^{*}]\) this implies that \(\lambda_{g_{0}}\leq u_{g_{0}}\leq\lambda_{g_{0}}^{*}\) and consequently \(\max_{g}\frac{\lambda_{g}^{*}}{u_{g}}\leq\max_{g}\frac{\lambda_{g}^{*}}{\lambda_{g}}\). The following upper bounds follow:

\[|H_{g}(\bm{u})| =\Sigma_{p}^{p+1}\left|\left(\frac{\lambda_{G}^{*}}{u_{G}}\right)^ {p+1}-\left(\frac{\lambda_{g}^{*}}{u_{g}}\right)^{p+1}\right| \leq\Sigma_{p}^{p+1}\left(\max_{g}\frac{\lambda_{g}^{*}}{\lambda_{g} }\right)^{p+1},\] \[|G_{g,h}(\bm{u})| =\Sigma_{p}^{p+1}\left|\frac{(\lambda_{G}^{*})^{p+1}}{(u_{G})^{p +2}}+\frac{(\lambda_{g}^{*})^{p+1}}{(u_{g})^{p+2}}\mathbbm{1}(g=h)\right| \leq\frac{2\Sigma_{p}^{p+1}}{\min_{g}\lambda_{g}^{*}}\left(\max_{g} \frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^{p+2},\] \[|I_{g,h,i}(\bm{u})| =\Sigma_{p}^{p+1}\left|\frac{(\lambda_{G}^{*})^{p+1}}{(u_{G})^{p +3}}-\frac{(\lambda_{g}^{*})^{p+1}}{(u_{g})^{p+3}}\mathbbm{1}(g=h=i)\right| \leq\frac{\Sigma_{p}^{p+1}}{(\min_{g}\lambda_{g}^{*})^{2}}\left( \max_{g}\frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^{p+3}.\]

Moreover, since \(p\geq 1\), each \(j\in\{1-p,1-2p,1-3p\}\) is non-positive, and by minimality of \(\bm{\lambda}^{*}\):

\[r_{p}(\bm{u})^{j}\leq r_{p}(\bm{\lambda}^{*})^{j},\]

so that:

\[|H_{g}H_{h}H_{i}r_{p}^{1-3p}(\bm{u})| \leq\left(\max_{g}\frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^{3p+ 3}\Sigma_{p}^{3p+3}r_{p}(\bm{\lambda}^{*})^{1-3p},\] \[|\left(H_{g}G_{h,i}+H_{h}G_{i,g}+H_{g}G_{h,i}\right)r_{p}^{1-2p}( \bm{u})| \leq 3\frac{2\Sigma_{p}^{p+1}}{\min_{g}\lambda_{g}^{*}}\left( \max_{g}\frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^{p+2}\left(\Sigma_{p}^{p+1} \left(\max_{g}\frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^{p+1}\right),\] \[|I_{g,h,i}r_{p}^{1-p}(\bm{u})| \leq\frac{\Sigma_{p}^{p+1}}{(\min_{g}\lambda_{g}^{*})^{2}}\left( \max_{g}\frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^{p+3}r_{p}^{1-p}(\bm{ \lambda}^{*}).\]

Each of the previous three expressions can be simplified by using \(r_{p}(\bm{\lambda}^{*})=\Sigma_{p}^{\frac{1}{2}-1}\):

\[|H_{g}H_{h}H_{i}r_{p}^{1-3p}(\bm{u})| \leq\left(\max_{g}\frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^{3p+ 3}r_{p}(\bm{\lambda}^{*}),\] \[|\left(H_{g}G_{h,i}+H_{h}G_{i,g}+H_{g}G_{h,i}\right)r_{p}^{1-2p}( \bm{u})| \leq\frac{6}{\min_{g}\lambda_{g}^{*}}\left(\max_{g}\frac{\lambda_ {g}^{*}}{\lambda_{g}}\right)^{2p+3}r_{p}(\bm{\lambda}^{*}),\] \[|I_{g,h,i}r_{p}^{1-p}(\bm{u})| \leq\frac{1}{(\min_{g}\lambda_{g}^{*})^{2}}\left(\max_{g}\frac{ \lambda_{g}^{*}}{\lambda_{g}}\right)^{p+3}r_{p}(\bm{\lambda}^{*}).\]

Hence by using the expression of the third derivatives established in Lemma 13:

\[\left|\frac{\partial^{3}r_{p}(\bm{u})}{\partial\lambda_{g}\partial \lambda_{h}\partial\lambda_{i}}\right| \leq\left|(1-p)(1-2p)H_{g}H_{h}H_{i}r_{p}^{1-3p}(\bm{u})\right|\] \[+\left|(1-2p)(1+p)\left(H_{g}G_{h,i}+H_{h}G_{i,g}+H_{g}G_{h,i} \right)r_{p}^{1-2p(\bm{u})}\right|\] \[+\left|(p+1)(p+2)I_{g,h,i}r_{p}^{1-p}(\bm{u})\right|\] \[\leq 2p^{2}\left(\max_{g}\frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^ {3p+3}r_{p}(\bm{\lambda}^{*})+\frac{12(p+1)^{2}}{\min_{g}\lambda_{g}^{*}} \left(\max_{g}\frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^{2p+3}r_{p}(\bm{ \lambda}^{*})\] \[+\frac{(p+2)^{2}}{(\min_{g}\lambda_{g}^{*})^{2}}\left(\max_{g} \frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^{p+3}r_{p}(\bm{\lambda}^{*})\] \[\leq\frac{7(p+2)^{2}}{(\min_{g}\lambda_{g}^{*})^{2}}\left(\max_{g }\frac{\lambda_{g}^{*}}{\lambda_{g}}\right)^{3p+3}r_{p}(\bm{\lambda}^{*}).\]

As a consequence, by taking the sup over \(u,g,h,i,x,y,z\),

\[\sup_{\begin{subarray}{c}u\in[\lambda,\lambda^{*}]\\ x,y,z\in\mathbb{N}\\ x+y+z=3\\ \{g,h,i\}\subset[G-1]\end{subarray}}\left|\frac{1}{x!y!z!}\frac{\partial^{3}r_ {p}(\bm{u})}{\partial\lambda_{g}\partial\lambda_{h}\partial\lambda_{i}}\right| \leq\frac{7(p+2)^{2}}{(\min_{g}\lambda_{g}^{*})^{2}}\max_{g}\left(\frac{ \lambda_{g}^{*}}{\lambda_{g}}\right)^{3p+3}r_{p}(\bm{\lambda}^{*}),\] (27)

which completes the proof of Lemma 12. \(\square\)Proof of Theorem 2

In this section, we prove Theorem 2. First, we establish an initial lower bound that captures the trade-off between how hard it is to distinguish two instances and how hard it is to optimize both under the same action (See Appendix B.1). Next, we provide a specific counter example which regret is at least \(\Theta(T^{-2})\) (See Appendix B.2).

### Proof of Lemma 6

**Lemma 6**.: _Let \(\bm{\pi}\) be a fixed policy and \(\bm{\mathcal{D}}^{a}\), \(\bm{\mathcal{D}}^{b}\) be two instances with standard deviation vectors \(\bm{\sigma}^{a},\bm{\sigma}^{b}\), respectively. Then,_

\[\max\{\text{Regret}_{p,T}(\bm{\pi},\bm{\mathcal{D}}^{a}),\text{Regret}_{p,T} (\bm{\pi},\bm{\mathcal{D}}^{b})\}\geq d(\bm{\sigma}^{a},\bm{\sigma}^{b})\exp \left(-\sum_{g\in[G]}\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{g,t}]KL( \mathcal{D}_{g}^{a}||\mathcal{D}_{g}^{b})\right).\]

Proof.: Let \(\bm{\mathcal{D}}^{a}\) and \(\bm{\mathcal{D}}^{b}\) be two instances with standard deviations \(\bm{\sigma}^{a}\) and \(\bm{\sigma}^{b}\), and let \(\delta\geq d(\sigma^{a},\sigma^{b})\).

Let \(X\) be a random variable over the set \(\{a,b\}\). The following inequalities hold

\[\max(\text{Regret}_{p,T}(\bm{\pi},\bm{\mathcal{D}}^{a}),\text{ Regret}_{p,T}(\bm{\pi},\bm{\mathcal{D}}^{b})) \geq\mathbb{E}_{X}[\text{Regret}_{p}(\bm{\pi},\bm{\mathcal{D}}^{ X})]\] \[\geq\mathbb{E}[R_{p}(\bm{n};\bm{\sigma}^{X})-R_{p}^{*}(\bm{ \sigma}^{X})|R_{p}(\bm{n};\bm{\sigma}^{X})-R_{p}^{*}(\bm{\sigma}^{X})>\delta]\] \[\times\mathbb{P}_{\bm{\pi},X}\left(R_{p}(\bm{n};\bm{\sigma}^{X}) -R_{p}^{*}(\bm{\sigma}^{X})>\delta\right)\] \[\geq\delta\mathbb{P}_{X,\bm{\pi}}\left(R_{p}(\bm{n};\bm{\sigma}^{ X})-R_{p}^{*}(\bm{\sigma}^{X})>\delta\right),\]

where the first step stems from the support of \(X\) being \(\{a,b\}\), and the second step stems from the law of total expectation. Let \(\hat{x}\) be the following (random) classifier:

\[\hat{x}:=\begin{cases}a&\text{If }R_{p}(\bm{n};\bm{\sigma}^{a})-R_{p}^{*}( \bm{\sigma}^{a})\leq\delta\\ b&\text{If }R_{p}(\bm{n},\bm{\sigma}^{b})-R_{p}^{*}(\bm{\sigma}^{b})\leq\delta\\ \text{Indifferent}&\text{Otherwise}\end{cases}\]

Since \(\delta\geq d(\sigma^{a},\sigma^{b})\), \(\hat{x}\) is well defined. Moreover, \(\mathbb{P}_{\bm{\pi},X}\left(R_{p}(\bm{n};\bm{\sigma}^{X})-R_{p}^{*}(\bm{ \sigma}^{X})>\delta\right)\geq\mathbb{P}_{X}(\hat{x}\neq X)\geq\inf_{\hat{x}} \mathbb{P}_{X}(\hat{x}\neq X)\), where the infinimum is taken over all the classifiers of \(\{a,b\}\). Moreover, by Pinsker's inequality, \(\inf_{\hat{x}}\mathbb{P}_{X}(\hat{x}\neq X)\geq\exp\left(-KL(\mathcal{D}^{a }||\mathcal{D}^{b})\right)\). Moreover, from bandits feedback divergence properties (see [25]), \(KL(\mathcal{D}^{a}||\mathcal{D}^{b})=\sum_{g\in[G]}\mathbb{E}_{\bm{\pi},\bm{ \mathcal{D}}^{a}}[n_{g,t}]KL(\mathcal{D}_{g}^{a}||\mathcal{D}_{g}^{b})\), which concludes the proof of Lemma 6. 

### The counter-examples

We introduce the following instances for each \(g\in[G]\),

\[\bm{\mathcal{D}}^{g}:\begin{cases}\mathcal{D}_{1}^{a}\sim\mathcal{N}\left(0, 1+\frac{1}{\sqrt{T}}\right)\\ \mathcal{D}_{g}^{a}\sim\mathcal{N}\left(0,1\right),\quad\forall g\neq 1 \end{cases}\qquad\qquad\bm{\mathcal{D}}^{b}:\begin{cases}\mathcal{D}_{2}^{b} \sim\mathcal{N}\left(0,1+\frac{1}{\sqrt{T}}\right)\\ \mathcal{D}_{g}^{a}\sim\mathcal{N}\left(0,1\right),\quad\forall g\neq 2 \end{cases}\]

We start by upper bounding the \(KL\)-divergence between the two instances:

**Lemma 14**.: _The following inequality holds: \(\sum_{g\in[G]}\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{g,T}]KL(\mathcal{D }_{g}^{a}||\mathcal{D}_{g}^{b})\leq\frac{1}{2}\)_

Proof.: For convenience, we set \(\nu:=\sqrt{\frac{1}{T}}<1\). The formula for the \(KL-\)divergence of two univariate normal distributions of zero mean implies

\[KL(\mathcal{D}_{h}^{a}||\mathcal{D}_{h}^{b})=\frac{1}{2}\left(\log\left(\frac {\sigma^{2}(1+\nu)}{\sigma^{2}}\right)+\frac{\sigma^{2}-(\sigma^{2}(1+\nu))}{ \sigma^{2}(1+\nu)}\right).\]The taylor expansion of the expression above can be derived by combining the expansions of both the functions \(x\to\log(1+x)\) and \(x\to\frac{1}{1+x}\) in the domain \((0,1)\):

\[\frac{1}{2}\left(\log\left(\frac{\sigma^{2}(1+\nu)}{\sigma^{2}} \right)+\frac{\sigma^{2}-(\sigma^{2}(1+\nu))}{\sigma^{2}(1+\nu)}\right) =\frac{1}{2}\left(-\sum_{k\geq 1}\frac{(-1)^{k}}{k}\nu^{k}-\nu \sum_{k\geq 0}(-1)^{k}\nu^{k}\right)\] \[=\frac{1}{2}\sum_{k\geq 1}(-1)^{k}\nu^{k}\left(1-\frac{1}{k}\right)\] \[=\frac{\nu^{2}}{2}\sum_{k\geq 0}(-1)^{k}\nu^{k}\left(1-\frac{1}{k+2}\right)\] \[\leq\frac{\nu^{2}}{2},\]

Summing over all coordinates implies

\[\sum_{g\in[G]}\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{g,T}]KL(\mathcal{ D}_{g}^{a}||\mathcal{D}_{g}^{b})=\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{h,T} ]KL(\mathcal{D}_{h}^{a}||\mathcal{D}_{h}^{b})\leq\frac{T\nu^{2}}{2}=\frac{1}{2},\]

where the inequality follows from \(\bm{n}_{T}\leq T\). This concludes the proof. 

Next, we derive a simpler form for \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})\). The simplification exploits the symmetries in \(\bm{\sigma}^{a},\bm{\sigma}^{b}\):

**Lemma 15**.: _Let **u** denote the unit vector \((1,\dots,1)^{T}\). The following equality holds:_

\[d(\bm{\sigma}^{a},\bm{\sigma}^{b})=r_{p}\left(\frac{1}{G}\bm{u};\bm{\sigma}^{ a}\right)-r_{p}^{*}(\bm{\sigma}^{a})\]

_where the function \(r_{p}\) is introduced in Appendix A._

Proof.: For \(x\in\{a,b\}\), let \(\mathcal{S}_{\epsilon}^{x}:=\{\epsilon>0|r_{p}(\bm{\lambda};\sigma^{x})-r_{p}^ {*}(\bm{\sigma}^{x})\leq\epsilon\}\). By definition of \(d\),

\[d(\bm{\sigma}^{a},\bm{\sigma}^{b})=\inf\{\delta\geq 0|\mathcal{S}_{\epsilon}^{a} \cap\mathcal{S}_{\epsilon}^{b}\neq\emptyset\}.\]

We prove \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})=r_{p}\left(\frac{1}{G}\bm{u};\bm{\sigma}^{ a}\right)-r_{p}^{*}(\bm{\sigma}^{a})\) by proving each of the inequalities \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})\leq r_{p}\left(\frac{1}{G}\bm{u};\bm{\sigma }^{a}\right)-r_{p}^{*}(\bm{\sigma}^{a})\) and \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})\geq r_{p}\left(\frac{1}{G}\bm{u};\bm{\sigma }^{a}\right)-r_{p}^{*}(\bm{\sigma}^{a})\).

First, we prove \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})\leq r_{p}\left(\frac{1}{G}\bm{u};\bm{ \sigma}^{a}\right)-r_{p}^{*}(\bm{\sigma}^{a})\). Since \(\bm{\sigma}^{a}\) can be obtained by swapping the first two coordinates of \(\bm{\sigma}^{b}\), the symmetry of \(r_{p}\) implies \(r_{p}^{*}(\bm{\sigma}^{a})=r_{p}^{*}(\bm{\sigma}^{b})\). Moreover, for each \((\lambda_{1},\lambda_{2},\bm{\lambda}^{\prime})\in\mathcal{K}\), \(r_{p}((\lambda_{1},\lambda_{2},\bm{\lambda}_{3:G});\bm{\sigma}^{a})=r_{p}(( \lambda_{2},\lambda_{1},\bm{\lambda}_{3:G});\bm{\sigma}^{b})\). As a consequence, \(r_{p}\left(\frac{1}{G}\bm{u};\bm{\sigma}^{a}\right)-r_{p}^{*}(\bm{\sigma}^{a })=r_{p}\left(\frac{1}{G}\bm{u};\bm{\sigma}^{b}\right)-r_{p}^{*}(\bm{\sigma}^ {b})\), and any \(\epsilon>0\) satisfying \(\epsilon\geq r_{p}\left(\frac{1}{G}\bm{u};\bm{\sigma}^{a}\right)-r_{p}^{*}( \bm{\sigma}^{a})\) must also satisfy \(\mathcal{S}_{\epsilon}^{a}\cap\mathcal{S}_{\epsilon}^{b}\neq\emptyset\). In particular, \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})\leq r_{p}\left(\frac{1}{G}\bm{u};\bm{ \sigma}^{a}\right)-r_{p}^{*}(\bm{\sigma}^{a})\).

To derive \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})\geq r_{p}\left(\frac{1}{G}\bm{u};\bm{ \sigma}^{a}\right)-r_{p}^{*}(\bm{\sigma}^{a})\), we use the following lemma, which proof is deferred later in the section:

**Lemma 16**.: _If \(\bm{\lambda}=(\lambda_{1},\dots,\lambda_{G})\in\mathcal{S}_{\epsilon}^{a} \cap\mathcal{S}_{\epsilon}^{b}\), and \(\tau\) a permutation of \([G]\). \(\bm{\lambda}_{\tau}:=(\lambda_{\tau(1)},\dots,\lambda_{\tau(G)})\) is also in \(\mathcal{S}_{\epsilon}^{a}\cap\mathcal{S}_{\epsilon}^{b}\)._

Let \(\epsilon\geq 0\) satisfying \(\mathcal{S}_{\epsilon}^{a}\cap\mathcal{S}_{\epsilon}^{b}\neq\emptyset\), and let \(\bm{\lambda}\in\mathcal{S}_{\epsilon}^{a}\cap\mathcal{S}_{\epsilon}^{b}\). For each permutation \(\tau\), \(\bm{\lambda}_{\tau}\) is also in \(\mathcal{S}_{\epsilon}^{a}\cap\mathcal{S}_{\epsilon}^{b}\). Each of \(\mathcal{S}_{\epsilon}^{a}\) and \(\mathcal{S}_{\epsilon}^{b}\) is convex and therefore the intersection \(\mathcal{S}_{\epsilon}^{a}\cap\mathcal{S}_{\epsilon}^{b}\) is also convex, which implies that

\[\frac{1}{G}\bm{u}=\frac{1}{G!}\sum_{\tau\text{permutation}}\bm{\lambda}_{\tau} \in\mathcal{S}_{\epsilon}^{a}\cap\mathcal{S}_{\epsilon}^{b},\]

which in turn implies that \(\epsilon\geq r_{p}\left(\frac{1}{G}\bm{u},\bm{\sigma}^{a}\right)-r^{*}(\sigma^{a})\). By taking the \(\inf\) we get \(d_{p}(\bm{\sigma}^{a},\bm{\sigma}^{b})\geq r_{p}\left(\frac{1}{G}\bm{u},\bm{ \sigma}^{a}\right)-r_{p}^{*}(\bm{\sigma}^{a})\), which completes the proof. 

We now state the proof of Lemma 16:Proof.: Let \(\bm{\lambda}=(\lambda_{1},\ldots,\lambda_{G})\in\mathcal{S}_{\epsilon}^{a}\cap \mathcal{S}_{\epsilon}^{b}\). Since every permutation can be written as a composition of transpositions (2-cycles), it suffices to prove the result for transpositions. Let \(\tau=(g,h)\) with \(g\neq h\). We distinguish 3 cases:

1. \(g,h\geq 3\): Since \(\bm{\sigma}_{3:G}^{a}=\bm{\sigma}_{3:G}^{b}\), \(r_{p}(\bm{\lambda};\bm{\sigma}^{x})=r_{p}(\bm{\lambda}_{\tau};\bm{\sigma}^{x})\) for each \(x\in\{a,b\}\). The implication \(\bm{\lambda}_{\tau}\in\mathcal{S}_{\epsilon}^{a}\cap\mathcal{S}_{\epsilon}^{b}\) follows.
2. \(g,h\leq 2\): By symmetry of the problem, \(\bm{\sigma}_{1}^{a}=\bm{\sigma}_{2}^{b}=\bm{\sigma}_{\tau(1)}^{b}\). Similarly, \(\bm{\sigma}_{2}^{a}=\bm{\sigma}_{\tau(2)}^{b}\). Therefore, \(\bm{\lambda}\in\mathcal{S}_{\epsilon}^{a}\) implies \(\bm{\lambda}_{\tau}\in\mathcal{S}_{\epsilon}^{b}\). Similarly, \(\bm{\lambda}\in\mathcal{S}_{\epsilon}^{b}\) implies \(\bm{\lambda}_{\tau}\in\mathcal{S}_{\epsilon}^{a}\). The implication \(\bm{\lambda}_{\tau}\in\mathcal{S}_{\epsilon}^{a}\cap\mathcal{S}_{\epsilon}^{b}\) follows.
3. \(g\in\{1,2\}\) and \(h\geq 3\). Without loss of generality, assume that \(g=1\) and \(h=3\). Since \(\mathcal{D}_{1}^{b}\sim\mathcal{D}_{3}^{b}\), the equality \(\sigma_{1}^{b}=\sigma_{3}^{b}\) holds and therefore \(r_{p}(\bm{\lambda};\sigma^{b})=r_{p}(\bm{\lambda}_{\tau};\bm{\sigma}^{b})\), therefore \(\bm{\lambda}_{\tau}\in\mathcal{S}_{\epsilon}^{b}\). Moreover, \(r_{p}(\bm{\lambda}_{\tau};\bm{\sigma}^{a})=r_{p}(\bm{\lambda};\sigma^{b})\), therefore \(\bm{\lambda}_{\tau}\in\mathcal{S}_{\epsilon}^{a}\). The implication \(\bm{\lambda}_{\tau}\in\mathcal{S}_{\epsilon}^{a}\cap\mathcal{S}_{\epsilon}^{b}\) follows.

The three cases cover all possible 2-cycles, which completes the proof. 

We are now ready to state the proof of Theorem 2. It remains to show that \(d_{p}(\bm{\sigma}^{a},\bm{\sigma}^{b})=r_{p}\left(\frac{1}{2}\mathbf{u},\sigma ^{a}\right)-r_{p}^{*}(\sigma^{a})=\Theta(T^{-2})\). We are ready to state the proof of Theorem 2.

**Theorem 2**.: _Let \(p\) be finite and \(\kappa\) be a universal constant. For any online policy \(\bm{\pi}\), there exists an instance \(\bm{\mathcal{D}_{\bm{\pi}}}\) such that for any \(T\geq 1\),_

\[\text{Regret}_{p,T}(\bm{\pi},\bm{\mathcal{D}_{\bm{\pi}}})\geq\kappa(p+1)T^{-2} +O\left(T^{-2.5}\right)=\Theta(T^{-2}).\]

Proof.: We set \(\bm{\mathcal{D}_{\bm{\pi}}}:=\text{argmax}\left\{\text{Regret}_{p}\left(\bm{ \pi},\bm{\mathcal{D}^{a}}\right),\text{Regret}_{p}\left(\bm{\pi},\bm{\mathcal{ D}^{b}}\right)\right\}\). The following inequalities hold:

\[\text{Regret}_{p}\left(\bm{\pi},\bm{\mathcal{D}_{\bm{\pi}}}\right) \geq\max\left\{\text{Regret}_{p}\left(\bm{\pi},\bm{\mathcal{D}^{a }}\right),\text{Regret}_{p}\left(\bm{\pi},\bm{\mathcal{D}^{b}}\right)\right\}\] \[\geq d(\bm{\sigma}^{a},\bm{\sigma}^{b})\exp\left(-\sum_{g\in[G]} \mathbb{E}_{\bm{\pi},\bm{\mathcal{D}^{a}}}[n_{g,t}]KL(\mathcal{D}_{g}^{a}|| \mathcal{D}_{g}^{b})\right)\] \[\geq\left(r_{p}\left(\frac{1}{G}\bm{u};\bm{\sigma}^{a}\right)-r_{ p}^{*}(\bm{\sigma}^{a})\right)\exp\left(\frac{-1}{2}\right)\]

where the first step stems from the definition of \(\bm{\mathcal{D}_{\bm{\pi}}}\), the second step stems from Lemma 6, and the third step stems from a combination of both Lemma 19 and Lemma 15. The rest of the proof consists in deriving a lower bound on \(r_{p}\left(\frac{1}{G}\bm{u};\bm{\sigma}^{a}\right)-r_{p}^{*}(\bm{\sigma}^{a})\). Since we will now solely focus on \(\bm{\sigma}^{a}\), we can drop the dependencies in \(\bm{\sigma}\). A direct consequence of Inequality (19) is

\[\frac{r_{p}\left(\frac{1}{G}\bm{u}\right)}{r_{p}^{*}}-1\geq\frac{p+1}{2}\sum_{ g\in[G]}\frac{\left(\frac{1}{G}-\lambda_{g}^{*}\right)^{2}}{\lambda_{g}^{*}}- \frac{7(p+2)^{2}}{(\min_{g}\lambda_{g}^{*})^{2}}\max_{g}\left(\frac{\lambda_{g} ^{*}}{1/G}\right)^{3p+3}\|\bm{u}/G-\bm{\lambda}^{*}\|_{\infty}^{3}.\] (28)

We will lower bound each of the terms \(\frac{p+1}{2}\sum_{g\in[G]}\frac{\left(\frac{1}{G}-\lambda_{g}^{*}\right)^{2}}{ \lambda_{g}^{*}}\) and \(-\frac{7(p+2)^{2}}{(\min_{g}\lambda_{g}^{*})^{2}}\max_{g}\left(\frac{\lambda_{g }^{*}}{1/G}\right)^{3p+3}\|\bm{u}/G-\bm{\lambda}^{*}\|_{\infty}^{3}\). For convenience, we set

\[f_{T}:=\lambda_{1}^{*}-\frac{1}{G}.\]

Since the first group has the highest variance, \(\lambda_{1}^{*}\) should also be the highest. In particular, \(\lambda_{1}^{*}\geq\frac{1}{G}\) and \(f_{T}\geq 0\). By symmetry of \(\bm{\sigma}^{a}\), \(\lambda_{2}^{*}=\ldots=\lambda_{G}^{*}=\frac{1}{G}-\frac{f_{T}}{G-1}\). Therefore,

\[\sum_{g\in[G]}\frac{\left(\frac{1}{G}-\lambda_{g}^{*}\right)^{2}}{\lambda_{g}^{* }}=\frac{f_{T}^{2}}{\frac{1}{G}+f_{T}}+\frac{(G-1)\left(\frac{f_{T}}{G-1}\right)^ {2}}{\frac{1}{G}-\frac{f_{T}}{G-1}}=Gf_{T}^{2}\left(\frac{1+Gf_{T}}{\geq 1-Gf_{T}}+ \underbrace{\frac{1}{G-1}\frac{1}{1-\frac{Gf_{T}}{G-1}}}_{\geq 0}\right)\geq Gf_{T}^{2}-G^{2}f_{T}^{3}.\]Next, notice that \(\bm{\lambda}^{*}\leq 1\), which implies that \(\max_{g}\left(\frac{\lambda_{g}^{2}}{1/G}\right)^{3p+3}\leq G^{3p+3}\), and that \(\|\bm{u}/G-\bm{\lambda}^{*}\|_{\infty}=\max(f_{T},f_{T}/(G-1))=f_{T}\), and \(\min_{g}\lambda_{g}^{*}=\frac{1}{G}-\frac{f_{T}}{G-1}\). Therefore,

\[\frac{r_{p}\left(\frac{1}{G}\bm{u}\right)}{r_{p}^{*}}-1\geq\frac{p+1}{2}\left( Gf_{T}^{2}-G^{2}f_{T}^{3}\right)-\frac{7(p+2)^{2}G^{3p+3}f_{T}^{3}}{\left( \frac{1}{G}-\frac{f_{T}}{G-1}\right)^{2}}=\frac{p+1}{2}\left(Gf_{T}^{2}-G^{2}f _{T}^{3}\right)-\frac{7(p+2)^{2}G^{3p+5}f_{T}^{3}}{\left(1-\frac{G}{G-1}f_{T} \right)^{2}}.\] (29)

It remains to bound \(f_{T}\). We do so by deriving the first terms of its taylor expansion in and \(T\):

\[f_{T} =\frac{\left(1+\frac{1}{\sqrt{T}}\right)^{\frac{2p}{p+1}}}{G-1+ \left(1+\frac{1}{\sqrt{T}}\right)^{\frac{2p}{p+1}}}-\frac{1}{G}\] \[=\frac{1+\frac{2p}{p+1}\frac{1}{\sqrt{T}}+o\left(\frac{1}{\sqrt{T }}\right)}{G+\frac{2p}{p+1}\frac{1}{\sqrt{T}}+o\left(\frac{1}{\sqrt{T}}\right) }-\frac{1}{G}\] \[=\frac{1}{G}\left(\left(1+\frac{2p}{p+1}\frac{1}{\sqrt{T}}+o \left(\frac{1}{\sqrt{T}}\right)\right)\left(1-\frac{2p}{p+1}\frac{1}{G\sqrt{T }}+o\left(\frac{1}{\sqrt{T}}\right)\right)-1\right)\] \[=\frac{2p}{p+1}\frac{1}{G}\left(1-\frac{1}{G}\right)\frac{1}{ \sqrt{T}}+o\left(\frac{1}{\sqrt{T}}\right),\]

where the first equality stems from Lemma 1, the second equality stems from the binomial Taylor expansion, and the third equality stems from the Taylor expansion of \(x\to\frac{1}{1-x}\). The last step implies that there exists universal constants \(q_{1},q_{2}\) such that for \(T\geq 1\),

\[\frac{1}{G}\left(1-\frac{1}{G}\right)\frac{q_{1}}{\sqrt{T}}\leq f_{T}\leq \frac{1}{G}\left(1-\frac{1}{G}\right)\frac{q_{2}}{\sqrt{T}}.\]

Replacing the previous bounding of \(f_{T}\) in Inequality (29) yields

\[\frac{r_{p}\left(\frac{1}{G}\bm{u}\right)}{r_{p}^{*}}-1\geq\frac{2q_{1}^{2}}{ G}\left(1-\frac{1}{G}\right)^{2}\frac{p+1}{T}+O\left(\frac{1}{T\sqrt{T}}\right),\]

which in turn implies

\[r_{p}\left(\frac{1}{G}\bm{u}\right)-r_{p}^{*}\geq\frac{q_{1}^{2}(p+1)}{4} \frac{r_{p}^{*}}{GT}+O\left(\frac{1}{T^{2}\sqrt{T}}\right)\geq\frac{q_{1}^{2} }{4}\frac{p+1}{T^{2}}+O\left(\frac{1}{T^{2}\sqrt{T}}\right).\]

where we used \(r_{p}^{*}=\Theta(T^{-1})\) and from Lemma 1 that

\[r_{p}^{*}=\left\|\frac{\bm{\sigma}^{2}}{\bm{n}_{T}^{*}}\right\|_{p}=\frac{1}{ T}\Sigma_{p}^{\frac{p+1}{p}}=\frac{\left((G-1)+\left(1+\frac{1}{\sqrt{T}} \right)^{\frac{2p}{p+1}}\right)^{\frac{p+1}{p}}}{T}\geq\frac{G}{T}.\]

By setting \(\kappa=\frac{q_{1}^{2}}{4}\), we get \(\text{Regret}_{T}(\bm{\pi},\bm{\mathcal{D}_{\pi}})\geq\kappa\frac{p+1}{T^{2}}+O \left(\frac{1}{T^{2}\sqrt{T}}\right)\), which completes the proof of Theorem 4. 

## Appendix C Upper and lower bounds when \(p=\infty\)

The proof for Theorem 3 (upper bound when \(p=\infty\)) follows the same high-level steps as the proof of Theorem 1 (upper bound when \(p\in\mathbb{R}\)). However, some adjustments of the proofs are necessary. Table 2 summarizes the changes that are required.

In Appendix C.1, we introduce and prove Lemma 5, the replacement for Lemma 4. In Appendix C.3, we prove Theorem 4.

### Curvature of \(R_{\infty}\)

The upper bound Lemma 4 goes to \(+\infty\) as \(p=+\infty\) and is no longer insightful. In this section, we provide a suitable bound:

**Lemma 5**.: _Let \(\bm{\sigma}\in\mathbb{R}_{+}^{G}\) and \(\bm{n}^{\prime}\in\mathbb{R}_{+}^{G}\) such that \(\sum_{g\in[G]}n^{\prime}_{g}=T\). Then,_

\[\frac{R_{\infty}(\bm{n}^{\prime}_{T})-R_{\infty}(\bm{n}^{*}_{T})}{R_{\infty}( \bm{n}^{*}_{T})}\leq-\min_{g}\left(\frac{n^{\prime}_{g}}{n^{*}_{g,T}}-1\right) +\frac{1}{4}\max_{g}\left(\frac{n^{\prime}_{g}}{n^{*}_{g,T}}-1\right)^{2}\max _{g}\left(\frac{n^{*}_{g,T}}{n^{\prime}_{g}}\right)^{3}\]

Proof.: 

From Lemma 1,

\[\frac{\sigma_{1}^{2}}{n^{*}_{1,T}}=\ldots=\frac{\sigma_{G}^{2}}{n^{*}_{G,T}}= R_{\infty}(\bm{n}^{*}_{T})=R^{*}_{\infty},\]

so that for \(g\in[G]\) and \(\bm{n}^{\prime}\),

\[\frac{\sigma_{g}^{2}}{n^{\prime}_{g}} =R^{*}_{\infty}+\sigma_{g}^{2}\left(\frac{1}{n^{\prime}_{g}}- \frac{1}{n^{*}_{g,T}}\right)\] \[\leq R^{*}_{\infty}+\sigma_{g}^{2}\left(\frac{-1}{(n^{*}_{g,T})^ {2}}(n^{\prime}_{g}-n^{*}_{g,T})+\frac{1}{2}(n^{\prime}_{g}-n^{*}_{g,T})^{2} \sup_{x\in\{n^{\prime}_{g},n^{*}_{g,T}\}}\frac{1}{2x^{3}}\right)\] \[=R^{*}_{\infty}\left(1-\left(\frac{n^{\prime}_{g}}{n^{*}_{g,T}}-1 \right)+\frac{1}{4}\left(\frac{n^{\prime}_{g}}{n^{*}_{g,T}}-1\right)^{2}\max \left(1,\left(\frac{n^{*}_{g,T}}{n^{\prime}_{g}}\right)^{3}\right)\right)\]

where the first step follows from \(R^{*}_{\infty}=\frac{\sigma_{g}^{2}}{n^{*}_{g,T}}\), the second step follows from Taylor's inequality applied on the function \(x\to\frac{1}{x}\) on \(n^{*}_{g,T}\), and the first step follows from the definition of the infinite norm. By dividing both sides by \(R^{*}_{\infty}\), rearranging the terms, and taking the max, we obtain

\[\frac{R_{\infty}(\bm{n}^{\prime}_{T})-R_{\infty}(\bm{n}^{*}_{T})} {R_{\infty}(\bm{n}^{*}_{T})} \leq\max_{g}\left(-\left(\frac{n^{\prime}_{g}}{n^{*}_{g,T}}-1 \right)+\frac{1}{4}\left(\frac{n^{\prime}_{g}}{n^{*}_{g,T}}-1\right)^{2}\max \left(1,\left(\frac{n^{*}_{g,T}}{n^{\prime}_{g}}\right)^{3}\right)\right)\] \[\leq-\min_{g}\left(\frac{n^{\prime}_{g}}{n^{*}_{g,T}}-1\right)+ \frac{1}{4}\max_{g}\left(\frac{n^{\prime}_{g}}{n^{*}_{g,T}}-1\right)^{2}\max _{g}\left(\frac{n^{*}_{g,T}}{n^{\prime}_{g}}\right)^{3}.\]

which completes the proof of Lemma 1.

### Proof of Theorem 3

**Theorem 3**.: _Let \(\Sigma_{\infty}:=\sum_{g\in[G]}\sigma_{g}^{2}\). For any \(\bm{\mathcal{D}}\) that satisfies Assumptions 1 and 2,_

\[\text{Regret}_{\infty,T}(\text{Variance-UCB},\bm{\mathcal{D}})\leq\left(C_{T} \sqrt{\Sigma_{\infty}}+C_{T}^{2}\right)G^{1.5}T^{-1.5}+o(T^{-1.5})=\tilde{O}(T ^{-1.5}).\]

\begin{table}
\begin{tabular}{c||c} \multicolumn{2}{c}{**Result**} & \multicolumn{2}{c}{**Does it hold for \(p=+\infty\)?**} \\ \hline Lemma 1 & Yes \\ \hline Lemmas 2, 3 & Yes \\ \hline Lemma 4 & No, replaced by Lemma 5 \\ \hline Lemmas 6, 19, 15 & Yes \\ \hline \(d(\sigma^{a},\sigma^{b})=\Theta(T^{-2})\) & No, replaced by \(d(\sigma^{a},\sigma^{b})=\Theta(T^{-1.5})\) \\ \end{tabular}
\end{table}
Table 2: Summary of the possible extensions to \(p=+\infty\)Proof.: Similarly to Appendix A.5, the total-expectation bounding still holds:

\[\text{Regret}_{\infty,T}(\text{Variance-UCB},\bm{\mathcal{D}})\leq\mathbb{E}[R_{ \infty}(\bm{n})-R_{\infty}^{*}|\mathcal{A}_{T}]+\|\bm{\sigma}^{2}\|_{\infty} \mathbb{P}_{\pi}(\mathcal{A}_{T}^{c}).\] (30)

We now upper bound each of the two terms in (30). First, similarly to the case where \(p<+\infty\) presented in Appendix A, \(\mathbb{P}_{\pi}(\mathcal{A}_{T}^{c})\leq 2GT^{-2.5}\), so that

\[\|\bm{\sigma}^{2}\|_{\infty}\mathbb{P}_{\pi}(\mathcal{A}_{T}^{c})\leq 2G\|\bm{ \sigma}^{2}\|_{\infty}T^{-2.5}=o(T^{-1.5}).\] (31)

Next, we apply the final lower bound derived in Section B.2 from [13]:

**Lemma 17**.: _Conditionally on \(\mathcal{A}_{T}\),_

\[\frac{n_{g,T}}{n_{g,T}^{*}}-1\geq\frac{-G\sqrt{G}}{\sqrt{T}}\left(\frac{C_{T} }{\sqrt{\Sigma_{\infty}}}\left(1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\right) +8\sqrt{2}G^{1/4}\left(\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\right)^{3/4}\sqrt {1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}}T^{-3/4}\right).\]

By taking the min over \(g\in[G]\) in Lemma 17, the following inequality follows conditionally on \(\mathcal{A}_{T}\):

\[-\min_{g}\left(\frac{n_{g,T}}{n_{g,T}^{*}}-1\right)\leq\frac{G\sqrt{G}}{\sqrt{ T}}\left(\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\left(1+\frac{C_{T}}{\sqrt{ \Sigma_{\infty}}}\right)+8\sqrt{2}G^{1/4}\left(\frac{C_{T}}{\sqrt{\Sigma_{ \infty}}}\right)^{3/4}\sqrt{1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}}T^{-3/4} \right).\] (32)

Moreover, since \(C_{T}=\tilde{O}(1)\), the right hand side of the inequality above is in \(\tilde{O}(\sqrt{T})\). Next, from Inequality (23), and from \(\min_{g}n_{g,T}^{*}=T\frac{\min_{g}\sigma_{g}^{2}}{\Sigma_{\infty}}\),

\[\max_{g}\left(\frac{n_{g,T}}{n_{g,T}^{*}}-1\right)^{2} \leq\frac{1}{(\min_{h}n_{h,T}^{*})^{2}}\left(3G+\frac{4GC_{T}}{ \Sigma_{\infty}}\left(\min_{g}\sigma_{g}+\frac{2C_{T}}{\sqrt{\min_{h}n_{h,T}^ {*}}}\right)\sqrt{\min_{h}n_{h,T}^{*}}\right)^{2}\] \[\leq\frac{18G^{2}}{(\min_{h}n_{h,T}^{*})^{2}}+\frac{32G^{2}C_{T} ^{2}}{\Sigma_{\infty}\min_{h}n_{h,T}^{*}}\left(2(\min_{g}\sigma_{g})^{2}+4C_{T }^{2}\right)\] \[=\frac{64G^{2}C_{T}^{2}}{T}\left(1+\frac{2C_{T}^{2}}{\min_{g} \sigma_{g}^{2}}\right)+\frac{18G^{2}\Sigma_{\infty}^{2}}{T^{2}\min_{g}\sigma_ {g}^{4}}\] \[=\tilde{O}(T^{-1}),\]

and

\[\max_{g}\frac{n_{g,T}^{*}}{n_{g,T}} =\frac{1}{\min_{g,T}}=\frac{1}{1+\min_{g}\left(\frac{n_{g,T}}{n_{g,T}^{*}}-1\right)}\] \[\leq\frac{1}{\left(1-\frac{G\sqrt{G}}{\sqrt{T}}\left(\frac{C_{T} }{\sqrt{\Sigma_{\infty}}}\left(1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\right) +8\sqrt{2}G^{1/4}\left(\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\right)^{3/4} \sqrt{1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}}T^{-3/4}\right)\right)^{+}}\] \[=O(1).\]

Combining the previous two inequalities yields

\[\frac{1}{4}\max_{g}\left(\frac{n_{g,T}}{n_{g,T}^{*}}-1\right)^{2}\max_{g} \left(\frac{n_{g,T}^{*}}{n_{g,T}^{*}}\right)^{3}\leq\frac{\frac{64G^{2}C_{T}^{2 }}{T}\left(1+\frac{2C_{T}^{2}}{\min_{g}\sigma_{g}^{2}}\right)+\frac{18G^{2} \Sigma_{T}^{2}}{T^{2}\min_{g}\sigma_{g}^{2}}}{\left[\left(1-\frac{G\sqrt{G}}{ \sqrt{T}}\left(\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\left(1+\frac{C_{T}}{ \sqrt{\Sigma_{\infty}}}\right)+8\sqrt{2}G^{1/4}\left(\frac{C_{T}}{\sqrt{ \Sigma_{\infty}}}\right)^{3/4}\sqrt{1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}}T^{- 3/4}\right)\right)^{+}}\right]^{3}}.\] (33)Recall that \(R_{\infty}^{*}=\frac{\Sigma_{\infty}}{T}\). Applying Lemma 5 on \(n_{g,T}\) and using the upper bounds (32) and (33) implies that conditionally on \(\mathcal{A}_{T}\)

\[R_{\infty}(\bm{n}_{T})-R_{\infty}(\bm{n}_{T}^{*}) \leq\frac{G^{3/2}\Sigma_{\infty}}{T^{3/2}}\frac{C_{T}}{\sqrt{ \Sigma_{\infty}}}\left(1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\right)\] \[+\frac{G^{7/4}\Sigma_{\infty}}{T^{9/4}}\left(\frac{C_{T}}{\sqrt{ \Sigma_{\infty}}}\right)^{3/4}\sqrt{1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}}\] \[+\frac{\frac{64G^{2}\Sigma_{\infty}C_{T}^{2}}{T^{2}}\left(1+ \frac{2C_{T}^{2}}{\min_{g}\sigma_{g}^{2}}\right)}{\left[\left(1-\frac{G\sqrt {G}}{\sqrt{T}}\left(\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\left(1+\frac{C_{T}}{ \sqrt{\Sigma_{\infty}}}\right)+8\sqrt{2}G^{1/4}\left(\frac{C_{T}}{\sqrt{\Sigma_ {\infty}}}\right)^{3/4}\sqrt{1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}T^{-3/4}} \right)\right)^{+}\right]^{3}}\] \[+\frac{\frac{18G^{2}\Sigma_{\infty}^{3}}{T^{3}\min_{g}\sigma_{g }^{2}}}{\left[\left(1-\frac{G\sqrt{G}}{\sqrt{T}}\left(\frac{C_{T}}{\sqrt{ \Sigma_{\infty}}}\left(1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\right)+8\sqrt{2} G^{1/4}\left(\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\right)^{3/4}\sqrt{1+\frac{C_{T}}{ \sqrt{\Sigma_{\infty}}}T^{-3/4}}\right)\right)^{+}\right]^{3}}.\]

Since the right hand side is deterministic, the inequality above extends by taking the conditional expected value on \(\mathcal{A}_{T}\). Moreover, the second, third, and fourth term are all in \(o(T^{-1.5})\), so that:

\[\mathbb{E}[R_{\infty}(\bm{n})-R_{\infty}^{*}|\mathcal{A}_{T}]\leq C_{T}\sqrt{ \Sigma_{\infty}}\left(1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\right)G^{1.5}T^{- 1.5}+o(T^{-1.5}).\] (34)

Finally, combining both inequalities (31) and (34) in Inequality (30) yields

\[\text{Regret}_{\infty,T}(\text{Variance-UCB},\bm{\mathcal{D}})\leq C_{T}\sqrt{ \Sigma_{\infty}}\left(1+\frac{C_{T}}{\sqrt{\Sigma_{\infty}}}\right)G^{1.5}T^{- 1.5}+o(T^{-1.5}),\]

which completes the proof of Theorem 3. 

### Proof of Theorem 4

**Lemma 18**.: _Let \(\bm{\sigma}^{a},\bm{\sigma}^{b}\) be two vectors with \(\Sigma_{\infty}^{a}:=\sum_{g}(\sigma_{g}^{a})^{2}\), \(\Sigma_{\infty}^{b}:=\sum_{g}(\sigma_{g}^{b})^{2}\) and \(\Sigma_{\infty}^{a}\geq\Sigma_{\infty}^{b}\). There exists a \(\Sigma_{\infty}^{a,b}\in[\Sigma_{\infty}^{b},\Sigma_{\infty}^{a}]\) such that_

\[d(\bm{\sigma}^{a},\bm{\sigma}^{b})=\frac{\Sigma_{\infty}^{a,b}}{2T}\|\bm{ \lambda}^{*}(\bm{\sigma}^{a})-\bm{\lambda}^{*}(\bm{\sigma}^{b})\|_{1}\]

Proof.: Let \(\bm{\sigma}^{a},\bm{\sigma}^{b}\) be two vectors with \(\Sigma_{\infty}^{a}:=\sum_{g}(\sigma_{g}^{a})^{2}\) and \(\Sigma_{\infty}^{b}:=\sum_{g}(\sigma_{g}^{b})^{2}\), \(\epsilon>0\), and \(\bm{\lambda}\in[0,1]^{G}\) such that \(\sum_{g}\lambda_{g}=1\). We have:

\[R_{\infty}(T\bm{\lambda};\bm{\sigma}^{a})-R_{\infty}^{*}(\bm{\sigma}^{a})=\frac {1}{T}\left(\max_{g}\frac{\sigma_{g}^{a}}{\lambda_{g}}-\Sigma_{\infty}^{a} \right)=\frac{\Sigma_{\infty}^{a}}{T}\left(\max_{g}\frac{\lambda_{g}^{*}(\bm{ \sigma}^{a})}{\lambda_{g}}-1\right),\]

therefore,

\[T\bm{\lambda}\in S_{\epsilon}^{a} \Longleftrightarrow R_{\infty}(T\bm{\lambda};\bm{\sigma}^{a})-R_{\infty}^{*}(\bm{ \sigma}^{a})\leq\epsilon\] \[\Longleftrightarrow \frac{\Sigma_{\infty}^{a}}{T}\left(\max_{g}\frac{\lambda_{g}^{*} (\bm{\sigma}^{a})}{\lambda_{g}}-1\right)\leq\epsilon\] \[\Longleftrightarrow \forall g\in[G],\quad\frac{\lambda_{g}^{*}(\bm{\sigma}^{a})}{ \frac{T\epsilon}{\Sigma_{\infty}^{a}}+1}\leq\lambda_{g}.\]

Hence, the allocation \(T\bm{\lambda}\) is simultaneously in \(S_{\epsilon}^{a}\) and \(S_{\epsilon}^{b}\) if and only if

\[\forall g\in[G],\quad\max\left(\frac{\lambda_{g}^{*}(\bm{\sigma}^{a})}{\frac {T\epsilon}{\Sigma_{\infty}^{a}}+1},\frac{\lambda_{g}^{*}(\bm{\sigma}^{b})}{ \frac{T\epsilon}{\Sigma_{\infty}^{a}}+1}\right)\leq\lambda_{g},\]In particular, \(S_{\epsilon}^{a}\cap S_{\epsilon}^{b}\) is the polytope

\[\left\{T\bm{\lambda}|\bm{\lambda}\in[0,1]^{G},\sum_{g}\lambda_{g}=1,\quad\forall g \in[G],\quad\max\left(\frac{\lambda_{g}^{*}(\bm{\sigma^{a}})}{\frac{T\epsilon}{ \Sigma_{\infty}^{a}}+1},\frac{\lambda_{g}^{*}(\bm{\sigma^{b}})}{\frac{T \epsilon}{\Sigma_{\infty}^{c}}+1}\right)\leq\lambda_{g}\right\}\]

which is non-empty if and only if

\[\sum_{g}\max\left(\frac{\lambda_{g}^{*}(\bm{\sigma^{a}})}{\frac{T\epsilon}{ \Sigma_{\infty}^{a}}+1},\frac{\lambda_{g}^{*}(\bm{\sigma^{b}})}{\frac{T \epsilon}{\Sigma_{\infty}^{c}}+1}\right)\leq 1.\]

so that \(d(\bm{\sigma}^{a},\bm{\sigma}^{b})\) is the (unique) solution to the equation \(\sum_{g}\max\left(\frac{\lambda_{g}^{*}(\bm{\sigma^{a}})}{\frac{T\epsilon}{ \Sigma_{\infty}^{a}}+1},\frac{\lambda_{g}^{*}(\bm{\sigma^{b}})}{\frac{T \epsilon}{\Sigma_{\infty}^{c}}+1}\right)=1\). To better understand the form of this solution, we introduce the following three decreasing functions:

\[f:\epsilon>0 \rightarrow\sum_{g}\max\left(\frac{\lambda_{g}^{*}(\bm{\sigma^{a }})}{\frac{T\epsilon}{\Sigma_{\infty}^{a}}+1},\frac{\lambda_{g}^{*}(\bm{\sigma ^{b}})}{\frac{T\epsilon}{\Sigma_{\infty}^{c}}+1}\right)\] \[f_{a}:\epsilon>0 \rightarrow\sum_{g}\max\left(\frac{\lambda_{g}^{*}(\bm{\sigma^{a }})}{\frac{T\epsilon}{\Sigma_{\infty}^{a}}+1},\frac{\lambda_{g}^{*}(\bm{\sigma ^{b}})}{\frac{T\epsilon}{\Sigma_{\infty}^{a}}+1}\right)\] \[f_{a}:\epsilon>0 \rightarrow\sum_{g}\max\left(\frac{\lambda_{g}^{*}(\bm{\sigma^{a }})}{\frac{T\epsilon}{\Sigma_{\infty}^{a}}+1},\frac{\lambda_{g}^{*}(\bm{\sigma ^{b}})}{\frac{T\epsilon}{\Sigma_{\infty}^{a}}+1}\right)\]

First, notice that \(d(\bm{\sigma}^{a},\bm{\sigma^{b}})=f^{-1}(1)\). Next, by assuming (WLOG) that \(\Sigma_{\infty}^{a}\geq\Sigma_{\infty}^{b}\), we have \(f^{a}\geq f\geq f^{b}\), so that \(d(\bm{\sigma}^{a},\bm{\sigma^{b}})=f^{-1}(1)\in[(f^{b})^{-1}(1),(f^{a})^{-1}( 1)]\). Moreover,

\[f_{a}(\epsilon) =\sum_{g}\max\left(\frac{\lambda_{g}^{*}(\bm{\sigma^{a}})}{\frac {T\epsilon}{\Sigma_{\infty}^{a}}+1},\frac{\lambda_{g}^{*}(\bm{\sigma^{b}})}{ \frac{T\epsilon}{\Sigma_{\infty}^{a}}+1}\right)\] \[=\frac{\sum_{g}\max\left(\lambda_{g}^{*}(\bm{\sigma^{a}}), \lambda_{g}^{*}(\bm{\sigma^{b}})\right)}{\frac{T\epsilon}{\Sigma_{\infty}^{c }}+1}\] \[=\frac{\frac{1}{2}\sum_{g}\lambda_{g}^{*}(\bm{\sigma^{a}})+ \lambda_{g}^{*}(\bm{\sigma^{b}})+|\lambda_{g}^{*}(\bm{\sigma^{a}})-\lambda_{g} ^{*}(\bm{\sigma^{b}})|}{\frac{T\epsilon}{\Sigma_{\infty}^{a}}+1}\] \[=\frac{1+\frac{1}{2}\|\bm{\lambda}^{*}(\bm{\sigma^{a}})-\bm{ \lambda}^{*}(\bm{\sigma^{b}})\|_{1}}{\frac{T\epsilon}{\Sigma_{\infty}^{a}}+1},\]

where the first step stems from the definition of \(f_{a}\), the second step stems from \(\max(x,y)=\frac{x+y+|x-y|}{2}\), the third step stems from \(\sum_{g}\lambda_{g}^{*}=1\), and the fourth step stems from the defintion of the \(1-\)norm. The last equation implies

\[f_{a}^{-1}(1)=\frac{\Sigma_{\infty}^{a}}{2T}\|\bm{\lambda}^{*}(\bm{\sigma^{a}}) -\bm{\lambda}^{*}(\bm{\sigma^{b}})\|_{1}.\]

Similarly, \(f_{b}^{-1}(1)=\frac{\Sigma_{\infty}^{b}}{2T}\|\bm{\lambda}^{*}(\bm{\sigma^{a} })-\bm{\lambda}^{*}(\bm{\sigma^{b}})\|_{1}\). Therefore:

\[\frac{\Sigma_{\infty}^{b}}{2T}\|\bm{\lambda}^{*}(\bm{\sigma^{a}})-\bm{\lambda} ^{*}(\bm{\sigma^{b}})\|_{1}\leq d(\bm{\sigma^{a}},\bm{\sigma^{b}})\leq\frac {\Sigma_{\infty}^{a}}{2T}\|\bm{\lambda}^{*}(\bm{\sigma^{a}})-\bm{\lambda}^{*}( \bm{\sigma^{b}})\|_{1},\]

which concludes the proof. 

**Theorem 4**.: _For any online policy \(\bm{\pi}\), there exists an instance \(\bm{\mathcal{D}}_{\bm{\pi}}\) such that for any \(T\geq 1\),_

\[\text{Regret}_{\infty,T}(\bm{\pi},\bm{\mathcal{D}}_{\bm{\pi}})\geq\frac{1}{2}G ^{1.5}T^{-1.5}.\]

Proof.: The proof consists of constructing two instances that optimize the trade-off stated in Lemma 6. The first instance, which is denoted \(\bm{\mathcal{D}}^{a}\), consists of \(G\) groups of standard normal distributions. Next, select a group \(h\) such that \(\mathbb{E}_{\pi,\bm{\mathcal{D}}^{a}}[n_{g,T}]\) is minimal. Notice that by minimality of \(\mathbb{E}_{\pi,\bm{\mathcal{D}}^{a}}[n_{h,T}]\)\[\mathbb{E}_{\pi,\bm{\mathcal{D}}^{a}}[n_{h,T}]\leq\frac{1}{G}\sum_{g\in[G]}\mathbb{E }_{\pi,\bm{\mathcal{D}}^{a}}[n_{g,T}]=\frac{1}{G}\mathbb{E}_{\pi,\bm{\mathcal{D} }^{a}}\left[\sum_{g\in[G]}n_{g,T}\right]=\frac{T}{G}.\] (35)

The second instance, which is denotes \(\bm{\mathcal{D}}^{b}\), is defined as follows:

\[\mathcal{D}^{b}_{g}=\begin{cases}\mathcal{N}(0,1+\nu)&\text{For }g=h\\ \mathcal{N}(0,1)&\text{Otherwise}\end{cases}\]

We have

\[\begin{cases}\bm{\sigma}^{a}&=(1,\ldots,1)\\ \bm{\sigma}^{b}&=\bm{\sigma}^{a}+\nu\bm{e}_{h}\end{cases}\]

so that \(\Sigma^{a}_{\infty}=G\), \(\Sigma^{b}_{\infty}=G+\nu\) and

\[\bm{\lambda}^{*}(\bm{\sigma}^{a}) =\frac{1}{G}(1,\ldots,1)\] \[\forall g\in[G],\quad\lambda^{*}_{g}(\bm{\sigma}^{b}) =\begin{cases}\frac{1}{G+\nu}&\text{For }g\neq h\\ \frac{1+\nu}{G+\nu}&\text{For }g=h\end{cases}\]

We introduce the following Lemma, which proof is deferred to later in this section.

**Lemma 19**.: _The following inequality holds: \(\sum_{g\in[G]}\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{g,T}]KL(\mathcal{ D}^{a}_{g}||\mathcal{D}^{b}_{g})\leq\frac{T\nu^{2}}{2G}\)._

By combining both Lemmas 18 and 19 in the inequality stated in Lemma 6, we obtain

\[\max\left\{\text{Regret}_{\infty}(\bm{\pi},\bm{\mathcal{D}}^{a}),\text{Regret}_{\infty}(\bm{\pi},\bm{\mathcal{D}}^{b})\right\} \geq\frac{\min(\Sigma^{a}_{\infty},\Sigma^{b}_{\infty})}{2T}\| \bm{\lambda}^{*}(\bm{\sigma}^{a})-\bm{\lambda}^{*}(\bm{\sigma}^{b})\|_{1}\exp \left(-\frac{T\nu^{2}}{2G}\right)\] \[=e^{-\frac{T\nu^{2}}{2G}}\frac{G}{2T}\left(\frac{1}{G}-\frac{1}{ G+\nu}+(G-1)\left(\frac{1+\nu}{G+\nu}-\frac{1}{G}\right)\right)\] \[=\frac{e^{-\frac{T\nu^{2}}{2G}}}{2}\frac{G}{T}\times\frac{(G+\nu )-G+(G-1)(G+G\nu-G-\nu)}{G(G+\nu)}\] \[=\frac{e^{-\frac{T\nu^{2}}{2G}}}{2}\frac{\nu G}{T}\times\frac{1+ (G-1)^{2}}{G(G+\nu)}.\]

By setting \(\nu=\sqrt{\frac{G}{T}}\leq 1\) and \(\bm{\mathcal{D}}_{\bm{\pi}}:=\text{argmax}\left\{\text{Regret}_{\infty}(\bm{ \pi},\bm{\mathcal{D}}^{a}),\text{Regret}_{\infty}(\bm{\pi},\bm{\mathcal{D}}^{ b})\right\}\), the previous inequality implies

\[\text{Regret}_{\infty}(\bm{\pi},\bm{\mathcal{D}}_{\bm{\pi}}) =\max\left\{\text{Regret}_{\infty}(\bm{\pi},\bm{\mathcal{D}}^{a}),\text{Regret}_{\infty}(\bm{\pi},\bm{\mathcal{D}}^{b})\right\}\geq\frac{e^{-1 /2}}{2}\frac{G^{1.5}}{T^{1.5}}\frac{1+(G-1)^{2}}{G(G+1)}\geq\frac{1}{2}G^{1.5}T^ {-1.5}\]

which completes the proof of Theorem 4,.. \(\Box\)

We now prove Lemma 19.

**Lemma 19**.: _The following inequality holds: \(\sum_{g\in[G]}\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{g,T}]KL(\mathcal{ D}^{a}_{g}||\mathcal{D}^{b}_{g})\leq\frac{T\nu^{2}}{2G}\)._

Proof.: For convenience, we set \(\nu:=\sqrt{\frac{G}{T}}<1\). The formula for the \(KL-\)divergence of two univariate normal distributions of zero mean implies

\[KL(\mathcal{D}^{a}_{h}||\mathcal{D}^{b}_{h})=\frac{1}{2}\left(\log\left(\frac {\sigma^{2}(1+\nu)}{\sigma^{2}}\right)+\frac{\sigma^{2}-(\sigma^{2}(1+\nu))}{ \sigma^{2}(1+\nu)}\right).\]The Taylor expansion of the expression above can be derived by combining the expansions of both the functions \(x\to\log(1+x)\) and \(x\to\frac{1}{1+x}\) in the domain \((0,1)\):

\[\frac{1}{2}\left(\log\left(\frac{\sigma^{2}(1+\nu)}{\sigma^{2}} \right)+\frac{\sigma^{2}-(\sigma^{2}(1+\nu))}{\sigma^{2}(1+\nu)}\right) =\frac{1}{2}\left(-\sum_{k\geq 1}\frac{(-1)^{k}}{k}\nu^{k}-\nu \sum_{k\geq 0}(-1)^{k}\nu^{k}\right)\] \[=\frac{1}{2}\sum_{k\geq 1}(-1)^{k}\nu^{k}\left(1-\frac{1}{k}\right)\] \[=\frac{\nu^{2}}{2}\sum_{k\geq 0}(-1)^{k}\nu^{k}\left(1-\frac{1}{k +2}\right)\] \[\leq\frac{\nu^{2}}{2},\]

where the first step stems from the inequality \(\bm{n}_{T}\leq T\), the second step stems from using the previous two equalities, the third step stems from simplifying the infinite sum term-wise, the fourth step stems from the expansion of a geometric serie, and the fifth step stems from \(\mu=\frac{1}{\sqrt{T}}\geq 0\). Since \(\bm{\mathcal{D}}^{a}\) and \(\bm{\mathcal{D}}^{b}\) have the same distribution at all coordinates except coordinate \(h\), we have:

\[\sum_{g\in[G]}\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{g,T}]KL(\mathcal{D} _{g}^{a}||\mathcal{D}_{g}^{b})=\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{ h,T}]KL(\mathcal{D}_{h}^{a}||\mathcal{D}_{h}^{b})\leq\frac{T\nu^{2}}{2G}\]

where the inequality follows from the minimality of \(\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{h,T}]\) and \(\sum_{g}\mathbb{E}_{\bm{\pi},\bm{\mathcal{D}}^{a}}[n_{g,T}]=T\). This concludes the proof.