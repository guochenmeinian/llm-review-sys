# Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound

 Reuben Adams

Department of Computer Science

University College London

reuben.adams.20@ucl.ac.uk

&John Shawe-Taylor

Department of Computer Science

University College London

j.shawe-taylor@ucl.ac.uk

Benjamin Guedj

Department of Computer Science, University College London and Inria

b.guedj@ucl.ac.uk

###### Abstract

Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis-classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of \(M\) error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided weighting of the error types. In contrast our bound implicitly controls all uncountably many weightings simultaneously.

## 1 Introduction

Generalisation bounds are a core component of the theoretical understanding of machine learning algorithms. For over two decades now, PAC-Bayesian theory has been at the core of studies on generalisation abilities of machine learning algorithms. PAC-Bayes originated in the seminal work of McAllester (1998, 1999) and was further developed by Catoni (2003, 2004, 2007), among other authors--we refer to the surveys Guedj (2019) and Alquier (2021) for an introduction to the field. The outstanding empirical success of deep neural networks in the past decade calls for better theoretical understanding of deep learning, and PAC-Bayes has emerged as one of the few frameworks that can be used to derive meaningful (and non-vacuous) generalisation bounds for neural networks: the pioneering work of Dziugaite and Roy (2017) has been followed by a number of contributions, including Neyshabur et al. (2018); Zhou et al. (2019); Letarte et al. (2019); Perez-Ortiz et al. (2021); Perez-Ortiz et al. (2021); Biggs and Guedj (2022a,b), to name but a few.

Much of the PAC-Bayes literature focuses on the case of binary classification, or of multiclass classification where one only distinguishes whether each classification is correct or incorrect. This is in stark contrast to the complexity of contemporary real-world learning problems, such as medical diagnosis where the severity of Type I and Type II errors may be crucial and context-dependent. This work aims to bridge this gap by deriving a generalisation bound that provides information-rich measures of performance at test time by controlling the probabilities of errors of any finite number of user-specified types. More precisely, we bound the KL-divergence between the empirical and truedistributions over the different error types. From this single bound one can then derive bounds on arbitrary linear combinations of these error probabilities, which will all hold simultaneously with the same probability as the original bound. In addition, these bounds are guaranteed to be non-vacuous (this follows since the KL-divergence blows up on the boundary of the simplex).

As a concrete example, if the severity of Type I and Type II errors of a medical test are context-dependent, one would want to be able to bound arbitrary linear combinations of these error probabilities. Existing bounds could only bound finitely many pre-specified weightings by employing a union bound, which would also degrade the bound. In contrast, by constraining the KL-divergence between the true and empirical error probabilities, our bound constrains all uncountably many weightings of the error probabilities simultaneously.

The usual setting of PAC-Bayes bounds is that of binary classification, namely an input set \(\mathcal{X}\), output set \(\mathcal{Y}=\{-1,1\}\), hypothesis space \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\) and a sample \(S\in(\mathcal{X}\times\mathcal{Y})^{m}\) drawn i.i.d. from a data-generating distribution \(D\). A number of PAC-Bayes bounds in this setting (e.g. Maurer (2004)) have been unified by a single general bound found in Begin et al. (2016). Briefly, Begin et al. (2016) prove a bound on the discrepancy \(d(R_{S}(Q),R_{D}(Q))\) between the error probability \(R_{D}(Q)\) of a stochastic classifier \(Q\) (a distribution over \(\mathcal{H}\) which classifies by first drawing \(h\sim Q\) and then classifying according to \(h\)) and its empirical counterpart \(R_{S}(Q)\) (the fraction of the sample \(Q\) misclassifies). The bound holds with high probability for all \(Q\) simultaneously. The bound in Begin et al. (2016) is binary in the sense that \(\mathcal{Y}\) contains two elements, but a more subtle way to look at this is that only two cases are distinguished--correct classification and incorrect classification. While it can be applied to multiclassification provided one maintains the second binary characteristic by only distinguishing correct and incorrect classifications. It is this heavy restriction that our result lifts, by considering the new framework of _error types_.

A new framework of errors typesWe consider a user-specified partition of the space \(\mathcal{Y}\times\mathcal{Y}\) of prediction-truth label-pairs into a finite partition of error types \(E_{1},\ldots,E_{M}\). Our bound then simultaneously constrains the probability with which errors of each type occur. In multiclass classification for example, one can choose the error types to be the set of all different possible mis-classifications, in which case our bound will control the entire confusion matrix, bounding how far the true confusion matrix (i.e. expected over the data-generating distribution) can diverge from the empirical one (i.e. on the training set). From this one can then derive bounds on the probabilities with which each mis-classification may be made, and arbitrary linear combinations of these error probabilities, and all of these will hold simultaneously with the same probability as the original bound. Our bound therefore paints a far richer picture of the performance of the final learned model than can be provided by any existing PAC-Bayes bound.

Formally, we let \(\bigcup_{j=1}^{M}E_{j}\) be a user-specified disjoint partition of \(\mathcal{Y}^{2}\) into a finite number of \(M\)_error types_, where we say that a hypothesis \(h\in\mathcal{H}\) makes an error of type \(j\) on datapoint \((x,y)\) if \((h(x),y)\in E_{j}\) (by convention, every pair \((\hat{y},y)\in\mathcal{Y}^{2}\) is interpreted as a predicted value \(\hat{y}\) followed by a true value \(y\), in that order). It should be stressed that not all of the \(E_{j}\) need correspond to mislabellings--indeed, some of the \(E_{j}\) may distinguish different correct labellings.

Relation to previous resultsOur framework of a finite number of user-specified "error types" includes multiclass classification as a particular case, and it is in this field that one finds the work most closely related to ours. Little is known of multiclass classification from a theoretical perspective and, to the best of our knowledge, only a handful of relevant strategies or generalisation bounds can be compared to the present paper.

The closest is the work of Morvant et al. (2012), which establishes a PAC-Bayes bound on the spectral norm of the difference between the true and empirical confusion matrices. Our bound differs from theirs in two respects. First, they consider the confusion matrix, whereas ours applies to the more general setting of a finite number of error types, which can be the set of all mis-classifications, or some partition thereof. Second, they deal with the spectral norm, whereas we employ the KL-divergence. Since the KL-divergence follows a simple formula, this means we can much more easily infer bounds on the individual error probabilities, which would be very challenging for the spectral norm. The follow-up work Koco and Capponi (2013) shows how a proxy of the spectral norm bound can be used as a training objective that may deal with imbalanced classes. In the present work, we show how our bound can be used as a differentiable training objective directly (without the need of a proxy) and that it can more sensitively deal with imbalanced classes, or errors of different severity, by assigning each error type a user-specified loss value.

Laviolette et al. (2017) extend the celebrated \(\mathcal{C}\)-bound in PAC-Bayes to ensembles, obtaining a bound on the risk of the majority vote classifier in the case of multiclass classification. In this context, our bound is able to distinguish different mis-classifications and control them, whereas they bound the scalar risk which lumps all mis-classifications together. The \(\mathcal{C}\)-bound has alternately been generalised by Lacasse et al. (2006) (see also Germain et al. (2015)) to simultaneously control three metrics, namely the so-called _expected disagreement, expected joint success and expected joint error_ of the posterior. While they restricted themselves to the ternary case, some of their proof techniques share similarities with ours. In cases where one has exactly three error types, for example the \(\{-1,0,1\}\)-valued _excess loss_, the work of Wu and Seldin (2022) is applicable; they construct so-called'split-kl' inequalities (both classical and PAC-Bayesian) which deftly handle this specific scenario.

Pires et al. (2013) present a comprehensive analysis of convex surrogate losses in cost-sensitive multiclass classification, providing conditions for consistency, bounding the excess loss of a predictor, and extending the analysis to the "Simplex Coding" scheme. We are considering the generalisation gap rather than the excess loss. Lei et al. (2019) study data-dependent bounds for multiclass classification. Their analysis is restricted to SVMs however, whereas ours applies to arbitrary hypothesis spaces.

Outline.We fix notation in Section 2. Theorem 1 in Section 3 is our main result--a PAC-Bayes bound on the KL-divergence between the true and empirical error distributions. For multiclass classification with a fully refined partition this becomes a bound on the KL-divergence between the true and empirical confusion matrices. Proposition 1 then bounds the individual error probabilities. Our second main result, Theorem 2 in Section 4, allows us to use bounds on _linear combinations_ of error probabilities as training objectives. We prove Theorem 1 in Section 5 via Proposition 4, which bounds the distribution of errors via a general convex function \(d\) and may be of independent interest. Section 6 outlines positive empirical results1 from using our bound as a training objective for neural networks and Section 7 gives perspectives for follow-up work..

Footnote 1: Code available here: https://github.com/reubenadams/PAC-Bayes-Control

## 2 Notation

For any set \(A\), let \(\mathcal{M}(A)\) be the set of probability measures on \(A\). Let \(\mathcal{X}\) and \(\mathcal{Y}\) be arbitrary input (_e.g._, feature) and output (_e.g._, label) sets respectively, and \(D\in\mathcal{M}(\mathcal{X}\times\mathcal{Y})\) be a data-generating distribution. For any sample \(S\sim D^{m}\) drawn i.i.d. from \(D\), let \(\hat{D}(S)\in\mathcal{M}(\mathcal{X}\times\mathcal{Y})\) denote the empirical distribution \(\hat{D}(S):=\frac{1}{m}\sum_{(x,y)\in S}\delta_{(x,y)}\). We consider the setting where the user has specified a partition \(\{E_{1},\ldots,E_{M}\}\) of \(\mathcal{Y}^{2}\) into \(M\)_error types_.

We are interested in _simple_ hypotheses \(h:\mathcal{X}\to\mathcal{Y}\) and _soft_ hypotheses \(H:\mathcal{X}\to\mathcal{M}(\mathcal{Y})\). For example, a neural network outputting scores (logits) in \(\mathbb{R}^{\mathcal{Y}}\) is converted to a simple or soft hypothesis, respectively, by passing the scores through the argmax or softmax function, respectively. For any \(A\subseteq\mathcal{Y}\), \(H(x)(A)\) can be interpreted as the probability according to \(H\) that the label of \(x\) is in \(A\). We will see in Section 4 that soft hypotheses permit more flexible training procedures and a more fine-grained analysis. Note that while soft hypotheses output distributions, they do so deterministically, always returning the same distribution for the same input \(x\), and so are distinct from the stochastic classifiers introduced shortly.

For a simple hypothesis \(h:\mathcal{X}\to\mathcal{Y}\) and \(j\in[M]\), define the _\(j\)-risk_ of \(h\) to be \(R^{j}_{D}(h):=\mathbb{P}_{(x,y)\sim D}((h(x),y)\in E_{j})\), namely the probability that \(h\) makes an error of type \(E_{j}\) for a randomly sampled \((x,y)\sim D\). For a soft hypothesis \(H:\mathcal{X}\to\mathcal{M}(\mathcal{Y})\) define the _\(j\)-risk_ of \(H\) to be \(R^{j}_{D}(H):=\mathbb{P}_{(x,y)\sim D,\hat{y}\sim H(x)}((\hat{y},y)\in E_{j})\), namely the probability that one would make an error of type \(E_{j}\) on a randomly sampled \((x,y)\sim D\) if one predicted by sampling \(\hat{y}\) from the distribution \(H(x)\). From now until Section 4 it will not matter whether we are dealing with simple or soft hypotheses. So, unless stated explicitly, we will refer to both simply as hypotheses, denote both by lowercase \(h\), and refer to the hypothesis class \(\mathcal{H}\), whether it is a subset of \(\mathcal{Y}^{\mathcal{X}}\) or \(\mathcal{M}(\mathcal{Y})^{\mathcal{X}}\).

Our goal is to control the _risk vector_\(\bm{R}_{D}(h):=(R^{1}_{D}(h),\ldots,R^{M}_{D}(h))\), since controlling this vector controls all linear combinations of \(j\)-risks. Since this is unobservable, we will control it by bounding how far it diverges from its empirical counterpart \(\bm{R}_{S}(h):=\bm{R}_{\hat{D}(S)}(h)\), which we term the _empirical risk vector_. Note that \(\mathbb{E}_{S\sim D_{m}}\bm{R}_{S}(h)=\bm{R}_{D}(h)\), and that, for a simple hypothesis \(h\in\mathcal{Y}^{\mathcal{X}}\), \(\bm{R}_{S}(h)\) is the vector of proportions of the sample on which \(h\) makes an error of type \(E_{j}\)2. Since the \(E_{j}\) partition \(\mathcal{Y}^{2}\), \(\bm{R}_{D}(h)\) and \(\bm{R}_{S}(h)\) are elements of the \(M\)-dimensional simplex \(\bigtriangleup_{M}:=\{\bm{u}\in[0,1]^{M}:u_{1}+\cdots+u_{M}=1\}\). Thus we can choose our divergence measure to be \(\text{kl}(\bm{R}_{S}(h)\|\bm{R}_{D}(Q))\), where for \(\bm{q},\bm{p}\in\bigtriangleup_{M}\) we define \(\text{kl}(\bm{q}\|\bm{p}):=\sum_{j=1}^{M}q_{j}\ln\frac{q_{j}}{p_{j}}\).3 When \(M=2\) we abbreviate \(\text{kl}((q,1-q)\|(p,1-p))\) to \(\text{kl}(q\|p)\), which is then the conventional definition of \(\text{kl}(\cdot\|\cdot)\) found in the PAC-Bayes literature (as in Seeger, 2002, for example). We define the _risk_ and _empirical risk_ of \(Q\) as \(\bm{R}_{D}(Q):=\mathbb{E}_{h\sim Q}\bm{R}_{D}(h)\) and \(\bm{R}_{S}(Q):=\mathbb{E}_{h\sim Q}\bm{R}_{S}(h)\), respectively, and seek a bound on \(\text{kl}(\bm{R}_{S}(Q)\|\bm{R}_{D}(Q))\). Note we still have \(\mathbb{E}_{S}[\bm{R}_{S}(Q)]=\bm{R}_{D}(Q)\), this time using Fubini. Moreover, for a sample \(S\) of size \(m\), we have that \(\bm{R}_{S}(Q)=\bm{K}/m\) where \(\bm{K}\sim\text{Mult}(m,M,\bm{R}_{D}(Q))\). Recall that for \(m,M\in\mathbb{N}\) and \(\bm{r}\in\bigtriangleup_{M}\), the multinomial distribution \(\text{Mult}(m,M,\bm{r})\) has probability mass function \(\text{Mult}(\bm{k};m,M,\bm{r}):=\big{(}\begin{smallmatrix}k_{1}&k_{2}&m\\ k_{1}&k_{2}&\cdots&k_{M}\end{smallmatrix}\big{)}\prod_{j=1}^{M}r_{j}^{k_{j}}\), where \(\quad\big{(}\begin{smallmatrix}k_{1}&k_{2}&m\\ k_{1}&k_{2}&\cdots&k_{M}\end{smallmatrix}\big{)}:=\frac{m!}{\prod_{j=1}^{M}k_ {j}!}\) for \(\bm{k}\in S_{m,M}:=\big{\{}(k_{1},\ldots,k_{M})\in\mathbb{N}_{0}^{M}:k_{1}+ \cdots+k_{M}=m\big{\}}\), and zero otherwise. As a final piece of notation, we let \(\bigtriangleup_{M}^{>0}:=\bigtriangleup_{M}\cap(0,1)^{M}\) and \(S_{m,M}^{>0}:=S_{m,M}\cap\mathbb{N}^{M}\) denote the vector elements of \(\bigtriangleup_{M}\) and \(S_{m,M}\), respectively, that have no zero components.

Footnote 2: \((\bm{R}_{S}(h))_{j}=R_{\hat{D}}^{j}(h)=\mathbb{P}_{(x,y)\sim\hat{D}}((h(x),y) \in E_{j})=\frac{1}{m}\sum_{(x,y)\in S}\mathbbm{1}[(h(x),y)\in E_{j}]\).

Footnote 3: We follow the usual convention that \(0\ln\frac{q}{x}=0\) for \(x\geq 0\) and \(x\ln\frac{x}{0}=\infty\) for \(x>0\).

## 3 Main result

We now state our main result, which bounds the KL-divergence between the true and empirical risk vectors \(\bm{R}_{D}(Q)\) and \(\bm{R}_{S}(Q)\), interpreted as probability distributions. As is conventional in the PAC-Bayes literature, we refer to sample independent and dependent distributions on \(\mathcal{M}(\mathcal{H})\) (_i.e._ stochastic hypotheses) as priors (denoted \(P\)) and _posteriors_ (denoted \(Q\)) respectively, even if they are not related by Bayes' theorem.

**Theorem 1**.: _Let \(\mathcal{X}\) and \(\mathcal{Y}\) be arbitrary sets and \(\bigcup_{j=1}^{M}E_{j}\) be a disjoint partition of \(\mathcal{Y}^{2}\) into \(M\) error types. Let \(D\in\mathcal{M}(\mathcal{X}\times\mathcal{Y})\) be a data-generating distribution and \(\mathcal{H}\) be a simple (\(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\)) or soft (\(\mathcal{H}\subseteq\mathcal{M}(\mathcal{Y})^{\mathcal{X}}\)) hypothesis class. For any prior \(P\in\mathcal{M}(\mathcal{H})\), \(\delta\in(0,1]\) and sample size \(m\geq M\), with probability at least \(1-\delta\) over the random draw \(S\sim D^{m}\), we have that simultaneously for all posteriors \(Q\in\mathcal{M}(\mathcal{H})\), the divergence \(\text{kl}\big{(}\bm{R}_{S}(Q)\|\bm{R}_{D}(Q)\big{)}\) is upper bounded by_

\[\frac{1}{m}\left[\text{KL}(Q\|P)+\ln\frac{\xi(m,M)}{\delta}\right],\quad\text{ where}\] (1)

\(\xi(m,M):=\sqrt{\pi}e^{1/(12m)}\left(\frac{m}{2}\right)^{\frac{M-1}{2}}\sum_{z=0 }^{M-1}\binom{M}{z}\left(\frac{2}{z}\right)^{z/2}\Gamma\left(\frac{M-z}{2} \right)^{-1}\in\mathcal{O}\left((mM)^{M}\right).\)

The fact that the logarithmic term is of order \(\mathcal{O}(M\ln(mM/\delta))\) means the bound is linear in \(M\) up to logarithmic terms, while this may seem excessive, one should note that the quantity that our theorem bounds also depends on \(M\). Further, the bound has been successfully used in by Biggs and Guedj (2023) to improve on state of the art PAC-Bayes bounds.

To see how our bound compares to existing PAC-Bayes bounds for binary classification, take \(\mathcal{Y}=\{-1,1\}\), \(M=2\), \(E_{1}=\{(-y,y):y\in\mathcal{Y}\}\) and \(E_{2}=\{(y,y):y\in\mathcal{Y}\}\). The argument of the logarithm then reduces to \(\frac{1}{\delta}e^{1/(12m)}\left(2+\sqrt{\frac{\pi m}{2}}\right)\leq 1.25\sqrt{m}\) when \(m\) is large. The corresponding term in Maurer (2004) is \(2\sqrt{m}\), which is only larger because he relaxes the term for aesthetics. Therefore our bound gracefully reduces to Maurer's in the case of binary classification.

Suppose after a use of Theorem 1 we have a bound of the form \(\text{kl}(\bm{R}_{S}(Q)\|\bm{R}_{D}(Q))\leq B\). We can then derive bounds on the individual \(j\)-risks \(R_{D}^{j}(Q)\) or, more generally, on linear combinations thereof. While one could obtain such bounds perhaps more directly with existing PAC-Bayes bounds, the significance of our bound is that _all_ such derived bounds hold with high probability _simultaneously_. Existing PAC-Bayes bounds would require the use of a union bound in order to bound multiple combinations simultaneously, whereas ours bounds all uncountably many combinations simultaneously, as a package. As for the individual \(j\)-risksks \(R_{D}^{j}(Q)\), the following proposition then yields the bounds \(L_{j}\leq R_{D}^{j}(Q)\leq U_{j}\), where \(L_{j}:=\inf\{p\in[0,1]:\text{kl}(R_{S}^{j}(Q)\|p)\leq B\}\) and \(U_{j}:=\sup\{p\in[0,1]:\text{kl}(R_{S}^{j}(Q)\|p)\leq B\}\). Moreover, since in the worst case we have \(\text{kl}(\bm{R}_{S}(Q)\|\bm{R}_{D}(Q))=B\), the proposition shows that the lower and upper bounds \(L_{j}\) and \(U_{j}\) are the tightest possible, since if \(R_{D}^{j}(Q)\not\in[L_{j},U_{j}]\) then \(\text{kl}(R_{S}^{j}(Q)\|R_{D}^{j}(Q))>B\) implying \(\text{kl}(\bm{R}_{S}(Q)\|\bm{R}_{D}(Q))>B\). For a more precise version of this argument and a proof of Proposition 1, see Appendix C.4.

**Proposition 1**.: _Let \(\bm{q},\bm{p}\in\triangle_{M}\). Then \(\text{kl}(q_{j}\|p_{j})\leq\text{kl}(\bm{q}\|\bm{p})\) for all \(j\in[M]\), with equality when \(p_{i}=\frac{1-p_{j}}{1-q_{j}}q_{i}\). for all \(i\neq j\)._

More generally, suppose we can quantify how costly an error of each type is by means of a loss vector \(\bm{\ell}\in[0,\infty)^{M}\), where \(\ell_{j}\) is the loss we attribute to an error of type \(E_{j}\). We may then be interested in bounding the _total risk_\(R_{D}^{T}(Q):=\bm{\ell}\cdot\bm{R}_{D}(Q)\). Then, given a bound \(\text{kl}(\bm{R}_{S}(Q)\|\bm{R}_{D}(Q))\leq B\) from Theorem 1, we can deduce

\[R_{D}^{T}(Q)\leq\sup\left\{\bm{\ell}\cdot\bm{r}:\bm{r}\in\triangle_{M},\text{ kl}(\bm{R}_{S}(Q)\|\bm{r})\leq B\right\}=\bm{\ell}\cdot\text{kl}_{\bm{\ell}}^{-1}( \bm{R}_{S}(Q)|B),\] (2)

where we define \(\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\in\triangle_{M}\) as follows. To see that it is indeed well-defined (at least when \(\bm{u}\in\triangle_{M}^{>0}\)), see the discussion at the beginning of Appendix C.5.

**Definition 1**.: _For \(\bm{u}\in\triangle_{M},c\in[0,\infty)\) and \(\bm{\ell}\in[0,\infty)^{M}\), define \(\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) to be an element \(\bm{v}\in\triangle_{M}\) solving the constrained optimisation problem_

\[\text{Maximise:}\quad f_{\bm{\ell}}(\bm{v}):=\bm{\ell}\cdot\bm{v},\] (3) \[\text{Subject to:}\quad\text{kl}(\bm{u}\|\bm{v})\leq c.\] (4)

This motivates the following training procedure: search for a posterior \(Q\) for which the bound \(\bm{\ell}\cdot\text{kl}_{\bm{\ell}}^{-1}(\bm{R}_{S}(Q)|B)\) on the total risk \(R_{D}^{T}(Q)\) is minimised. While this requires a particular choice of loss vector \(\bm{\ell}\), we emphasise that at the end of training, Theorem 1 bounds \(\text{kl}(\bm{R}_{S}(Q)\|\bm{R}_{D}(Q))\) and so can be used to bound _any_ linear combination of the \(j\)-risks, not just the one given the loss vector \(\bm{\ell}\) chosen for training. It is this flexibility which is the main advantage of our bound; changes in the severity of different error types over time do not require union bounds or retraining.

In the next section we provide a theorem for calculating \(\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) and its derivatives so that the training procedure can be executed.

## 4 Construction of a Differentiable Training Objective

We now state and prove Theorem 2, which provides a speedy method for approximating \(\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) and its derivatives to arbitrary precision, provided \(c>0\) and \(\forall j\ u_{j}>0\). The only approximation step required is that of approximating the unique root of a continuous and strictly increasing scalar function. Thus, provided the \(u_{j}\) themselves are differentiable, Theorem 1 combined with Theorem 2 shows that the upper bound on the total risk can be used as a tractable and fully differentiable training objective. See Appendix A for more details, including a pseudocode algorithm and an implementation. Since the proof of Theorem 2 is rather long and technical, we defer it to Appendix C.5. The requirement that the \(\ell_{j}\) are not all equal only rules out trivial cases where \(R_{D}^{T}(Q)\) is independent of \(\bm{R}_{D}(Q)\).

**Theorem 2**.: _Fix \(\bm{\ell}\in[0,\infty)^{M}\) such that not all \(\ell_{j}\) are equal, and define \(f_{\bm{\ell}}:\triangle_{M}\to[0,\infty)\) by \(f_{\bm{\ell}}(\bm{v}):=\sum_{j=1}^{M}\ell_{j}v_{j}\). For all \(\tilde{\bm{u}}=(\bm{u},c)\in\triangle_{M}^{>0}\times(0,\infty)\), define \(\bm{v}^{*}(\tilde{\bm{u}}):=\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\in\triangle_{M}\) and let \(\mu^{*}(\tilde{\bm{u}})\in(-\infty,-\max_{j}\ell_{j})\) be the unique solution to \(c=\phi_{\bm{\ell}}(\mu)\), where \(\phi_{\bm{\ell}}:(-\infty,-\max_{j}\ell_{j})\to\mathbb{R}\) is given by \(\phi_{\bm{\ell}}(\mu):=\ln(-\sum_{j=1}^{M}\frac{u_{j}}{\mu+\ell_{j}})+\sum_{j= 1}^{M}u_{j}\ln(-(\mu+\ell_{j}))\), which is continuous and strictly increasing. Then \(\bm{v}^{*}(\tilde{\bm{u}})=\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) is given by_

\[\bm{v}^{*}(\tilde{\bm{u}})_{j}=\frac{\lambda^{*}(\tilde{\bm{u}})u_{j}}{\mu^{*}( \tilde{\bm{u}})+\ell_{j}}\quad\text{for }j\in[M],\ \ \text{where}\quad\lambda^{*}(\tilde{\bm{u}})=\left(\sum_{j=1}^{M}\frac{u_{j}}{\mu^{* }(\tilde{\bm{u}})+\ell_{j}}\right)^{-1}.\] (5)

_Further, defining \(f_{\bm{\ell}}^{*}:\triangle_{M}^{>0}\times(0,\infty)\to[0,\infty)\) by \(f_{\bm{\ell}}^{*}(\tilde{\bm{u}}):=f_{\bm{\ell}}(\bm{v}^{*}(\tilde{\bm{u}}))\), we have that_

\[\frac{\partial f_{\bm{\ell}}^{*}}{\partial u_{j}}(\tilde{\bm{u}})=\lambda^{*}( \tilde{\bm{u}})\left(1+\ln\frac{u_{j}}{\bm{v}^{*}(\tilde{\bm{u}})_{j}}\right) \qquad\text{and}\qquad\frac{\partial f_{\bm{\ell}}^{*}}{\partial c}(\tilde{\bm{u} })=-\lambda^{*}(\tilde{\bm{u}}).\] (6)A final wrinkle in evaluating our bound is that while the empirical risk vector \(\bm{R}_{S}(Q)=\mathbb{E}_{h\sim Q}\bm{R}_{S}(h)\) does not depend on the data-generating distribution \(D\), the expectation over \(Q\) may still be intractable. This would be the default case when \(Q\) is a Gaussian over the weights of a multi-layer perceptron, for example. In such cases, we can estimate \(\bm{R}_{S}(Q)\) via a Monte Carlo sample \(\bm{R}_{S}(\hat{Q}):=\frac{1}{N}\sum_{n=1}^{N}\bm{R}_{S}(h_{n})\) (where the \(h_{n}\) are drawn i.i.d. from \(Q\)) and use the following two results. Proposition 2 shows that the \(\text{kl}(R_{S}^{j}(\hat{Q})\|R_{D}^{j}(Q))\) can be simultaneously bounded, whence Proposition 3 can be used to obtain a bound on \(\text{kl}(\bm{R}_{S}(\hat{Q})\|\bm{R}_{D}(Q))\).

**Proposition 2**.: _Let \(\bm{X}\sim\text{Multinomial}(N,M,\bm{p})\). Then for any \(\delta\in(0,1)\), with probability at least \(1-\delta\) we have that for all \(j\in[M]\) simultaneously \(\text{kl}\left(\frac{1}{N}X_{j}\big{\|}p_{j}\right)\leq\frac{\ln\frac{2M}{N}}{N}\)._

Proof.: Each bound holds separately with probability at least \(1-\delta/M\) by Theorem 2.5 in Langford and Caruana (2001). They then hold simultaneously by application of a union bound. 

**Proposition 3**.: _Suppose \(\bm{q},\bm{p},\hat{\bm{q}}\in\triangle_{M}\) are such that \(\text{kl}(\hat{q}\|\bm{p})\leq B_{1}\) and \(\text{kl}(\hat{q}_{j}\|q_{j})\leq B_{2}\) for all \(j\in[M]\). For each \(j\), define \(\underline{q}_{j}=\inf\{r\in[0,1]:\text{kl}(\hat{q}_{j}\|r)\leq B_{2}\}\). Then_

\[\text{kl}(\hat{\bm{q}}\|\bm{p})\leq MB_{2}-\sum_{j=1}^{M}(1-\hat{q}_{j})\ln \frac{1-\hat{q}_{j}}{1-\underline{q}_{j}}+B_{1}\max_{j}\frac{\hat{q}_{j}}{ \underline{q}_{j}}\to B_{1}\quad\text{as}\quad B_{2}\to 0.\] (7)

Proof.: Deferred to C.1. 

The fact that the bound on \(\text{kl}(\hat{\bm{q}}\|\bm{p})\to B_{1}\) as \(B_{2}\to 0\) ensures that as we increase the size of our Monte Carlo sample for estimating \(\bm{R}_{S}(Q)\) the bound on \(\text{kl}(\bm{R}_{S}(\hat{Q})\|\bm{R}_{D}(Q))\) approaches that of \(\text{kl}(\bm{R}_{S}(Q)\|\bm{R}_{D}(\hat{Q}))\), meaning in the limit we pay an arbitrarily small price in the bound for the approximation.

## 5 Proof of the main bound

We split the proof of Theorem 1 into three parts. First, we prove Proposition 4, a bound on \(d(\bm{R}_{S}(Q),\bm{R}_{D}(Q))\) for an arbitrary convex function \(d\), which may be of independent interest. Second, we prove Corollary 1 by specialising Proposition 4 to the case \(d(\cdot,\cdot)=\text{kl}(\cdot\|\cdot)\). Finally, we show that the bound in Theorem 1 is a loosened version of the bound in Corollary 1.

**Proposition 4**.: _Let \(d:\triangle_{M}^{2}\to\mathbb{R}\) be jointly convex. In the setting of Theorem 1,_

\[d\big{(}\bm{R}_{S}(Q),\bm{R}_{D}(Q)\big{)}\leq\frac{1}{\beta}\left[\text{KL} (Q\|P)+\ln\frac{\mathcal{I}_{d}(m,\beta)}{\delta}\right],\quad\text{where}\] (8)

\[\mathcal{I}_{d}(m,\beta):=\sup_{\bm{r}\in\triangle_{M}}\left[\sum_{\bm{k}\in S _{m,M}}\text{Mult}(\bm{k};m,M,\bm{r})\exp\left(\!\beta d\left(\tfrac{\bm{k}}{ m},\bm{r}\right)\!\right)\right].\]

This is a generalisation of the unifying PAC-Bayes bound given in Begin et al. (2016) where we replace the scalar risk quantities \(R_{S}(Q)\) and \(R_{D}(Q)\) with their vector counterparts \(\bm{R}_{S}(Q)\) and \(\bm{R}_{D}(Q)\). To see this, note that we can recover it by setting \(\mathcal{Y}=\{-1,1\}\), \(M=2\), \(E_{1}=\{(-y,y):y\in\mathcal{Y}\}\) and \(E_{2}=\{(y,y):y\in\mathcal{Y}\}\). Then, for any convex function \(d:[0,1]^{2}\to\mathbb{R}\), apply Theorem 4 with the convex function \(d^{\prime}:\triangle_{M}^{2}\to\mathbb{R}\) defined by \(d^{\prime}((u_{1},u_{2}),(v_{1},v_{2})):=d(u_{1},v_{1})\) so that Theorem 4 bounds \(d^{\prime}\big{(}\bm{R}_{S}(Q),\bm{R}_{D}(Q)\big{)}=d\big{(}R_{S}^{1}(Q),R_{D} ^{1}(Q)\big{)}\) which equals \(d(R_{S}(Q),R_{D}(Q))\) in the notation of Begin et al. (2016). Further, \(\sum_{\bm{k}\in S_{m,2}}\text{Mult}(\bm{k};m,2,\bm{r})\exp\left(\beta d^{ \prime}\big{(}\tfrac{\bm{k}}{m},\bm{r}\big{)}\,\right)=\sum_{k=0}^{m}\text{ Bin}(k;m,r_{1})\exp\left(\beta d\left(\tfrac{\bm{k}}{m},r_{1}\right)\right)\), so that the supremum over \(r_{1}\in[0,1]\) of the right hand side equals the supremum over \(\bm{r}\in\triangle_{2}\) of the left hand side, which, when substituted into (8), yields the bound given in Begin et al. (2016).

To prove Proposition 4 we require the following two lemmas. The first is the well-known change of measure in equality (Csiszar, 1975; Donsker and Varadhan, 1975). The second is a generalisation from Binomial to Multinomial distributions of a result found in Maurer (2004), the proof of which we defer to Appendix C.2.

**Lemma 1**.: _For any set \(\mathcal{H}\), any \(P,Q\in\mathcal{M}(\mathcal{H})\) and any measurable function \(\phi:\mathcal{H}\rightarrow\mathbb{R}\), \(\underset{h\sim Q}{\mathbb{E}}\phi(h)\leq\text{\rm KL}(Q\|P)+\ln\underset{h\sim P }{\mathbb{E}}\exp(\phi(h))\)._

**Lemma 2**.: _Let \(\bm{X}_{1},\ldots,\bm{X}_{m}\) be i.i.d \(\triangle_{M}\)-valued random vectors with mean \(\bm{\mu}\) and suppose that \(f:\triangle_{M}^{m}\rightarrow\mathbb{R}\) is convex. If \(\bm{X}^{\prime}_{1},\ldots,\bm{X}^{\prime}_{m}\) are i.i.d. \(\text{\rm Mult}(1,M,\bm{\mu})\) random vectors, then \(\mathbb{E}[f(\bm{X}_{1},\ldots,\bm{X}_{m})]\leq\mathbb{E}[f(\bm{X}^{\prime}_{ 1},\ldots,\bm{X}^{\prime}_{m})]\)._

The consequence of Lemma 2 is that the worst case (in terms of bounding \(d(\bm{R}_{S}(Q),\bm{R}_{D}(Q))\)) occurs when \(\bm{R}_{\{(x,y)\}}(h)\) is a one-hot vector for all \((x,y)\in S\) and \(h\in\mathcal{H}\), namely when \(\mathcal{H}\subseteq\mathcal{M}(\mathcal{Y})^{\mathcal{X}}\) only contains hypotheses that, when labelling \(S\), put all their mass on elements \(\hat{y}\in\mathcal{Y}\) that incur the same error type4. In particular, this is the case for hypotheses that put all their mass on a single element of \(\mathcal{Y}\), equivalent to the simpler case \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\) as discussed in Section 2. Thus, Lemma 2 shows that the bound given in Proposition 4 cannot be made tighter only by restricting to such hypotheses.

Footnote 4: More precisely, when \(\forall h\in\mathcal{H}\ \ \forall(x,y)\in S\ \exists j\in[M]\) such that \(h(x)[\{\hat{y}\in\mathcal{Y}:(\hat{y},y)\in E_{j}\}]=1\).

Proof.: (of Proposition 4) The case \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\) follows directly from the more general case by taking \(\mathcal{H}^{\prime}:=\{h^{\prime}\in\mathcal{M}(\mathcal{Y})^{\mathcal{X}}: \exists h\in\mathcal{H}\) such that \(\forall x\in\mathcal{X}\ \ h^{\prime}(x)=\delta_{h(x)}\}\), where \(\delta_{h(x)}\in\mathcal{M}(\mathcal{Y})\) denotes a point mass on \(h(x)\). For the general case \(\mathcal{H}\subseteq\mathcal{M}(\mathcal{Y})^{\mathcal{X}}\), using Jensen's inequality with the convex function \(d(\cdot,\cdot)\) and Lemma 1 with \(\phi(h)=\beta d\big{(}\bm{R}_{S}(h),\bm{R}_{D}(h)\big{)}\), we see that for all \(Q\in\mathcal{M}(\mathcal{H})\)

\[\beta d\big{(}\bm{R}_{S}(Q),\bm{R}_{D}(Q)\big{)} =\beta d\left(\underset{h\sim Q}{\mathbb{E}}\bm{R}_{S}(h), \underset{h\sim Q}{\mathbb{E}}\bm{R}_{D}(h)\right)\] \[\leq\underset{h\sim Q}{\mathbb{E}}\beta d\big{(}\bm{R}_{S}(h), \bm{R}_{D}(h)\big{)}\] \[\leq\text{\rm KL}(Q\|P)+\ln\left(\underset{h\sim P}{\mathbb{E}} \exp\Big{(}\beta d\big{(}\bm{R}_{S}(h),\bm{R}_{D}(h)\big{)}\Big{)}\right)\] \[=\text{\rm KL}(Q\|P)+\ln(Z_{P}(S)),\]

where \(Z_{P}(S):=\mathbb{E}_{h\sim P}\exp\big{(}\beta d(\bm{R}_{S}(h),\bm{R}_{D}(h)) \big{)}\). Note that \(Z_{P}(S)\) is a non-negative random variable, so that by Markov's inequality \(\underset{S\sim D^{m}}{\mathbb{P}}\Big{(}Z_{P}(S)\leq\frac{\mathbb{E}_{S^{ \prime}\sim D}Z_{P}(S^{\prime})}{\delta}\Big{)}\geq 1-\delta\). Thus, since \(\ln(\cdot)\) is strictly increasing, with probability at least \(1-\delta\) over \(S\sim D^{m}\), we have that simultaneously for all \(Q\in\mathcal{M}(\mathcal{H})\)

\[\beta d\big{(}\bm{R}_{S}(Q),\bm{R}_{D}(Q)\big{)}\leq\text{\rm KL}(Q\|P)+\ln \frac{\underset{S^{\prime}\sim D^{m}}{\mathbb{E}}Z_{P}(S^{\prime})}{\delta}.\] (9)

To bound \(\mathbb{E}_{S^{\prime}\sim D^{m}}Z_{P}(S^{\prime})\), let \(\bm{X}_{i}:=\bm{R}_{\{(x_{i},y_{i})^{\prime}\}}(h)\in\triangle_{M}\) for \(i\in[m]\), where \((x_{i},y_{i})^{\prime}\) is the \(i\)'th element of the dummy sample \(S^{\prime}\). Noting that each \(\bm{X}_{i}\) has mean \(\bm{R}_{D}(h)\), define the random vectors \(\bm{X}^{\prime}_{i}\sim\text{\rm Mult}(1,M,\bm{R}_{D}(h))\) and \(\bm{Y}:=\sum_{i=1}^{m}\bm{X}^{\prime}_{i}\sim\text{\rm Mult}(m,M,\bm{R}_{D}(h))\). Finally let \(f:\triangle_{M}^{m}\rightarrow\mathbb{R}\) be defined by \(f(x_{1},\ldots,x_{m}):=\exp\big{(}\beta d\left(\frac{1}{m}\sum_{i=1}^{m}x_{i}, \bm{R}_{D}(h)\right)\big{)}\), which is convex since the average is linear, \(d\) is convex and the exponential is non-decreasing and convex. Then, by swapping expectations (which is permitted by Fubini's theorem since the argument is non-negative) and applying Lemma 2, we have that \(\mathbb{E}_{S^{\prime}\sim D^{m}}Z_{P}(S^{\prime})\) can be written as

\[\mathbb{E}_{S^{\prime}\sim D^{m}}Z_{P}(S^{\prime}) =\underset{S^{\prime}\sim D^{m}}{\mathbb{E}}\ \underset{h\sim P}{\mathbb{E}}\exp\Big{(}\beta d\big{(}\bm{R}_{S^{\prime}}(h), \bm{R}_{D}(h)\big{)}\Big{)}\] \[=\underset{h\sim P}{\mathbb{E}}\ \underset{\bm{X}_{1},\ldots,\bm{X}_{m}}{ \mathbb{E}}\exp\left(\beta d\left(\frac{1}{m}\sum_{i=1}^{m}\bm{X}_{i},\bm{R}_{D}( h)\right)\right)\] \[\leq\underset{h\sim P}{\mathbb{E}}\ \underset{\bm{X}^{\prime}_{1},\ldots,\bm{X}^{ \prime}_{m}}{\mathbb{E}}\exp\left(\beta d\left(\frac{1}{m}\sum_{i=1}^{m}\bm{X}^{ \prime}_{i},\bm{R}_{D}(h)\right)\right)\] \[=\underset{h\sim P}{\mathbb{E}}\ \underset{Y}{\mathbb{E}}\exp\left(\beta d \left(\frac{1}{m}\bm{Y},\bm{R}_{D}(h)\right)\right)\]\[=\mathop{\mathbb{E}}_{h\sim P}\ \sum_{\bm{k}\in S_{m,M}}\text{Mult} \big{(}\bm{k};m,M,\bm{R}_{D}(h)\big{)}\exp\left(\beta d\big{(}\frac{\bm{k}}{m}, \bm{R}_{D}(h)\big{)}\right)\] \[\leq\sup_{\bm{r}\in\triangle_{M}}\left[\sum_{\bm{k}\in S_{m,M}} \text{Mult}\big{(}\bm{k};m,M,\bm{r}\big{)}\exp\left(\beta d\big{(}\frac{\bm{k}} {m},\bm{r}\big{)}\right)\right],\]

which is the definition of \(\mathcal{I}_{d}(m,\beta)\). Inequality (8) then follows by substituting this bound on \(\mathbb{E}_{S^{\prime}\sim D^{m}}Z_{P}(S^{\prime})\) into (9) and dividing by \(\beta\). 

We now specialise Proposition 4 to the case \(d(\cdot,\cdot)=\text{kl}(\cdot\|\cdot)\) to obtain Corollary 1.

**Corollary 1**.: _In the setting of Theorem 1,_

\[\text{kl}\big{(}\bm{R}_{S}(Q)\|\bm{R}_{D}(Q)\big{)} \leq\frac{1}{m}\left[\text{KL}(Q\|P)+\ln\frac{\eta(m,M)}{\delta} \right],\quad\text{where}\] (10) \[\eta(m,M) :=\frac{m!}{m^{m}}\sum_{\bm{k}\in S_{m,M}}\prod_{j=1}^{M}\frac{k _{j}^{k_{j}}}{k_{j}!}.\] (11)

Proof.: Applying Proposition 4 with \(d(\cdot,\cdot)=\text{kl}(\cdot\|\cdot)\) and \(\beta=m\) gives that with probability at least \(1-\delta\) over \(S\sim D^{m}\), simultaneously for all posteriors \(Q\in\mathcal{M}(\mathcal{H})\),

\[\text{kl}\big{(}\bm{R}_{S}(Q)\|\bm{R}_{D}(Q)\big{)}\leq\frac{1}{m}\left[\text {KL}(Q\|P)+\ln\frac{\mathcal{I}_{\text{kl}}(m,m)}{\delta}\right],\]

where \(\mathcal{I}_{\text{kl}}(m,m):=\sup_{\bm{r}\in\triangle_{M}}[\sum_{\bm{k}\in S_ {m,M}}\text{Mult}(\bm{k};m,M,\bm{r})\exp\left(m\text{kl}(\frac{\bm{k}}{m},\bm {r})\right)]\). Thus it suffices to show that \(\mathcal{I}_{\text{kl}}(m,m)\leq\eta(m,M)\).

To prove this, for each fixed \(\bm{r}=(r_{1},\ldots,r_{M})\in\triangle_{M}\) let \(J_{\bm{r}}=\{j\in[M]:r_{j}=0\}\). Then \(\text{Mult}(\bm{k};m,M,\bm{r})=0\) for any \(\bm{k}\in S_{m,M}\) such that \(k_{j}\neq 0\) for some \(j\in J_{\bm{r}}\). For the other \(\bm{k}\in S_{m,M}\), namely those such that \(k_{j}=0\) for all \(j\in J_{\bm{r}}\), the probability term can be written as \(\text{Mult}(\bm{k};m,M,\bm{r})=\frac{m!}{\prod_{j=1}^{m}k_{j}!}\prod_{j=1}^{M }r_{j}^{k_{j}}=\frac{m!}{\prod_{j\not\in J_{\bm{r}}}^{k_{j}}!}\prod_{j\not\in J _{\bm{r}}}r_{j}^{k_{j}},\) and (recalling the convention that \(0\ln\frac{0}{0}=0\)) the term \(\exp(m\text{kl}(\frac{\bm{k}}{m},\bm{r}))\) can be written as

\[\exp\left(m\sum_{j=1}^{M}\frac{k_{j}}{m}\ln\frac{\frac{k_{j}}{m}}{r_{j}} \right)=\exp\left(\sum_{j\not\in J_{\bm{r}}}k_{j}\ln\frac{k_{j}}{mr_{j}}\right) =\prod_{j\not\in J_{\bm{r}}}\left(\frac{k_{j}}{mr_{j}}\right)^{k_{j}}=\frac{1} {m^{m}}\prod_{j\not\in J_{\bm{r}}}\left(\frac{k_{j}}{r_{j}}\right)^{k_{j}},\]

where the last equality is obtained by recalling that the \(k_{j}\) sum to \(m\). Substituting these two expressions into the definition of \(\mathcal{I}_{\text{kl}}(m,m)\) and only summing over those \(\bm{k}\in S_{m,M}\) with non-zero probability, we obtain

\[\sum_{\bm{k}\in S_{m,M}}\text{Mult}(\bm{k};m,M,\bm{r})\exp\big{(} m\text{kl}\left(\frac{\bm{k}}{m},\bm{r}\right)\big{)} =\sum_{\begin{subarray}{c}\bm{k}\in S_{m,M}:\\ \forall j\in J_{\bm{r}}k_{j}=0\end{subarray}}\text{Mult}(\bm{k};m,M,\bm{r}) \exp\left(m\text{kl}\left(\frac{\bm{k}}{m},\bm{r}\right)\right)\] \[=\sum_{\begin{subarray}{c}\bm{k}\in S_{m,M}:\\ \forall j\in J_{\bm{r}}k_{j}=0\end{subarray}}\frac{m!}{\prod_{j\not\in J_{\bm{r} }}k_{j}!}\prod_{j\not\in J_{\bm{r}}}r_{j}^{k_{j}}\frac{1}{m^{m}}\prod_{j\not\in J _{\bm{r}}}\left(\frac{k_{j}}{r_{j}}\right)^{k_{j}}\] \[=\frac{m!}{m^{m}}\sum_{\begin{subarray}{c}\bm{k}\in S_{m,M}:\\ \forall j\in J_{\bm{r}}k_{j}=0\end{subarray}}\prod_{j\not\in J_{\bm{r}}}\frac{k _{j}^{k_{j}}}{k_{j}!}\] \[=\frac{m!}{m^{m}}\sum_{\begin{subarray}{c}\bm{k}\in S_{m,M}:\\ \forall j\in J_{\bm{r}}k_{j}=0\end{subarray}}\prod_{j=1}^{M}\frac{k_{j}^{k_{j}}}{ k_{j}!}\qquad\text{(because $\frac{0^{0}}{0!}=1$)}\] \[\leq\frac{m!}{m^{m}}\sum_{\bm{k}\in S_{m,M}}\prod_{j=1}^{M}\frac{ k_{j}^{k_{j}}}{k_{j}!},\]which is \(\eta(m,M)\). Since this is independent of \(\bm{r}\), it also holds after taking the supremum over \(\bm{r}\in\triangle_{M}\) of the left hand side, showing that \(\mathcal{I}_{\mathbb{A}\mathbb{I}}(m,m)\leq\eta(m,M)\). 

The final step in obtaining Theorem 1 is to loosen the bound given in Corollary 1 (which is intractable when \(m\) is large) to the tractable form given in Theorem 1. For this we require the following technical lemma, the proof of which we defer to Appendix C.3.

**Lemma 3**.: _For integers \(M\geq 1\) and \(m\geq M\), \(\sum_{\bm{k}\in S_{m,M}^{>0}}\frac{1}{\prod_{j=1}^{M}\sqrt{k_{j}}}\leq\frac{ \pi^{\frac{M}{2}}m^{\frac{M-2}{2}}}{\Gamma(\frac{M}{2})}\)._

Proof.: (Of Theorem 1) It suffices to show that for all \(m\geq M\geq 1\) we have \(\eta(m,M)\leq\xi(m,M)\). We achieve this by applying Stirling's approximation \(\sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}<n!<\sqrt{2\pi n}\left(\frac{n}{e} \right)^{n}e^{\frac{1}{12n}}\) (valid for \(n\geq 1\)) to the factorials in \(\eta(m,M)\) and then using Lemma 3.

Since Stirling's approximation requires that all the \(k_{j}\) are at least one, we partition the sum in \(\eta(m,M)\) according to the number of coordinates of \(\bm{k}\) at which \(k_{j}=0\). Let \(z\) index the number of such coordinates. Defining \(f:\bigcup_{M=2}^{\infty}S_{m,M}\to\mathbb{R}\) by \(f(\bm{k})=\prod_{j=1}^{|\bm{k}|}k_{j}^{k_{j}}/k_{j}!\) and noting that \(f\) is symmetric under permutations of its arguments, we then have

\[\eta(m,M)=\frac{m!}{m^{m}}\sum_{\bm{k}\in S_{m,M}}f(\bm{k})=\frac{m!}{m^{m}} \sum_{z=0}^{M-1}\binom{M}{z}\sum_{\bm{k}\in S_{m,M-z}^{>0}}f(\bm{k}).\] (12)

Stirling's approximation can now be applied to each \(\bm{k}\in S_{m,M}^{>0}\)\(f(\bm{k})\leq\prod_{j=1}^{M}\frac{k_{j}^{k_{j}}}{\sqrt{2\pi k_{j}}\left(\frac{k_{j}}{e} \right)^{k_{j}}}=\prod_{j=1}^{M}\frac{e^{k_{j}}}{\sqrt{2\pi k_{j}}}=\frac{e^{ m}}{(2\pi)^{M/2}}\prod_{j=1}^{M}\frac{1}{\sqrt{k_{j}}}\). An application of Lemma 3 now gives

\[\sum_{\bm{k}\in S_{m,M-z}^{>0}}f(\bm{k})\leq\sum_{\bm{k}\in S_{m,M-z}^{>0}} \frac{e^{m}}{(2\pi)^{\frac{M-z}{2}}}\prod_{j=1}^{M-z}\frac{1}{\sqrt{k_{j}}} \leq\frac{e^{m}}{(2\pi)^{\frac{M-z}{2}}}\frac{\pi^{\frac{M-z}{2}}m^{\frac{M-z -2}{2}}}{\Gamma\left(\frac{M-z}{2}\right)}=\frac{e^{m}m^{\frac{M-z-2}{2}}}{2^ {\frac{M-z}{2}}\Gamma\left(\frac{M-z}{2}\right)}.\]

Substituting this into equation (12) and bounding \(m!\) using Stirling's approximation, we have \(\eta(m,M)\leq\frac{\sqrt{2\pi m}e^{1/(12m)}}{e^{m}}\sum_{z=0}^{M-1}\binom{M}{ z}\frac{e^{m}m^{\frac{M-z-2}{2}}}{2^{\frac{M-z}{2}}\Gamma\left(\frac{M-z}{2} \right)}=\xi(m,M),\) which completes the proof of the bound. As for the order of the bound, it is sufficient to bound \(\ln\xi(m,M)\) using the crude approximations \(\binom{M}{z}\leq M^{M}\), \((2/m)^{z/2}\leq 1\) and \(\Gamma((M-z)/2)\geq 1\).

## 6 Numerical experiments

We use binarised versions of MNIST, and HAM10000 Tschandl (2018). In both cases we partition \(\mathcal{Y}^{2}\) into \(E_{0}=\{(0,0),(1,1)\}\), \(E_{1}=\{(0,1)\}\) and \(E_{2}=\{(1,0)\}\), and take \(\bm{\ell}=(0,1,3)\). Each dataset is split into prior and certification sets. We take \(\mathcal{H}\) to be two-layer MLPs. As is common in the PAC-Bayes literature, we restrict \(P\) and \(Q\) to be isotropic and diagonal Gaussian distributions over the parameter space, respectively. The means of \(P\) and \(Q\) are set to the parameters of an MLP trained on the prior set. The mean and variances of \(Q\) and the variance of \(P\) are tuned via Theorem 2 to minimize the bound on the total risk \(R_{D}^{T}(Q)\). See Appendix A for pseudocode, Appendix B for full experimental details and https://github.com/reubenadams/PAC-Bayes-Control for code. The results for MNIST can be seen in Figure 1.

We estimate \(\bm{R}_{S}(Q)\) with a Monte Carlo and obtain a PAC-Bayes bound on \(R_{D}^{T}(Q)\) by combining Proposition 2 (with \(\delta=0.01\) and \(N=100000\)) and Proposition 3. We obtain \(R_{D}^{T}(Q)\leq 0.2640\) for MNIST and \(R_{D}^{T}(Q)\leq 0.8379\) for HAM10000, where both bounds hold with probability at least \(1-0.05-0.01=0.94\). While these bounds are far from vacuous--the maximum possible value of \(R_{D}^{T}(Q)\) is \(3\) for our choice of \(\bm{\ell}\)--one might wonder whether one can do better by bounding each error probability individually using Maurer's inequality Maurer (2004), and then unioning these bounds. As with our Theorem 1, this would also constrain the entire distribution of error types since for any \(\bm{\ell}\), one could then calculate the maximimum value of \(R_{D}^{T}(Q)\) that satisfies all of these constraints. Bothmethods constrain the region of the simplex in which \(\bm{R}_{D}(Q)\) can lie (with high probability), and a reasonable metric by which to compare them is the volumes of these regions. This can be estimated via a MC sample by uniformly sampling points \(\bm{r}\) from \(\triangle_{M}\) and counting how samples are legal values of \(\bm{R}_{D}(Q)\) according to each method. The 95% confidence intervals for the volumes of the two regions are given in Table 1. A more comprehensive table for synthetic values of \(\bm{R}_{S}(Q)\) can be found in Appendix B.

## 7 Perspectives

We introduce the framework of error types, considering the vectors \(\bm{R}_{S}(Q)\) and \(\bm{R}_{D}(Q)\) of empirical and true probabilities of errors of different types. We prove a PAC-Bayes bound (Theorem 1) on \(\text{kl}(\bm{R}_{S}(Q)\|\bm{R}_{D}(Q))\) which controls the entire distribution of error probabilities, and hence can be used to derive bounds on arbitrary linear combinations of the error probabilities, all of which hold simultaneously with high probability; this cannot be achieved with any existing PAC-Bayes bound.

We construct a differential training objective based on our bound by introducing the the vectorised kl inverse, providing a recipe for quickly computing its value and derivatives (Theorem 2). Our framework is flexible enough to encompass multiclass classification or discretised regression, but also structured output prediction, multi-task learning and learning-to-learn.

Another potential application of our work is to the excess risk, since under a misclassification loss there are three different error types, corresponding to excess losses of \(\{-1,0,1\}\). Biggs and Guedj (2023) adapted Theorems 1 and 2 to this setting, leading to an empirically tighter PAC-Bayes bound for certain classification tasks.

We require i.i.d. data, which in practice is frequently not the case or is hard to verify. Further, the number of error types \(M\) must be finite. In continuous scenarios it would be preferable to be able to control the entire distribution of loss values without having to discretise into finitely many error types. We leave this direction to future work.

\begin{table}
\begin{tabular}{|c|c|c|} \hline
**Dataset** & **Volume Our Region** & **Volume Maurer Region** \\ \hline MNIST & **0.0025** (0.002498, 0.002504) & 0.0028 (0.002793, 0.002800) \\ \hline HAM10000 & 0.0012 (0.001207, 0.001211) & **0.0011** (0.001142, 0.001146) \\ \hline \end{tabular}
\end{table}
Table 1: Point estimates and 95% confidence intervals for the volumes of the confidence regions for \(\bm{R}_{D}(Q)\) given by Theorem 1 and a union over \(M\) individual Maurer bounds, respectively. Our method is superior for MNIST and inferior for HAM100000.

Figure 1: Experimental results for binarised MNIST. (a) The PAC-Bayes bound on the total risk decreases when tuning the posterior via Theorem 2. (b) This is achieved by a shift in the empirical error probabilities. (c) The bound on \(\text{kl}(\bm{R}_{S}(Q)\|\bm{R}_{D}(Q))\) is not substantially increased, meaning we still retain good control of \(\bm{R}_{D}(Q)\) after optimizing \(Q\) for this particular choice of \(\bm{\ell}\).

## Acknowledgments and Disclosure of Funding

We warmly thank reviewers and the Area Chair who provided insightful comments and suggestions which greatly helped us improve our manuscript. R.A. was supported by the UKRI grant number EP/S021566/1 and gratefully thanks Felix Biggs for his insights. J.S-T gratefully acknowledges the European Union's Horizon 2020 Research and Innovation Program through the grant numbers 951847 (European learning and intelligent systems excellence, ELISE) and 952026 (Human-centred artificial intelligence, HumanE-AI-Net). B.G. acknowledges partial support by the U.S. Army Research Laboratory and the U.S. Army Research Office, and by the U.K. Ministry of Defence and the U.K. Engineering and Physical Sciences Research Council (EPSRC) under grant number EP/R013616/1. B.G. acknowledges partial support from the French National Agency for Research, through grants ANR-18-CE40-0016-01 and ANR-18- CE23-0015-02, and through the programme "France 2030" and PEPR IA on grant SHARP ANR-23-PEIA-0008.

## References

* Alquier [2021] Pierre Alquier. User-friendly introduction to PAC-Bayes bounds. _arXiv preprint arXiv:2110.11216_, 2021.
* Ambroladze et al. [2006] Amiran Ambroladze, Emilio Parrado-Hernandez, and John Shawe-Taylor. Tighter PAC-Bayes bounds. In Bernhard Scholkopf, John C. Platt, and Thomas Hofmann, editors, _Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006_, pages 9-16. MIT Press, 2006. URL https://proceedings.neurips.cc/paper/2006/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
* Biggs and Guedj [2022a] Felix Biggs and Benjamin Guedj. On margins and derandomisation in pac-bayes. In _International Conference on Artificial Intelligence and Statistics_, pages 3709-3731. PMLR, 2022a.
* Biggs and Guedj [2022b] Felix Biggs and Benjamin Guedj. Non-vacuous generalisation bounds for shallow neural networks. In _International Conference on Machine Learning_, pages 1963-1981. PMLR, 2022b.
* Biggs and Guedj [2023] Felix Biggs and Benjamin Guedj. Tighter pac-bayes generalisation bounds by leveraging example difficulty. In _International Conference on Artificial Intelligence and Statistics_, pages 8165-8182. PMLR, 2023.
* Begin et al. [2016] Luc Begin, Pascal Germain, Francois Laviolette, and Jean-Francis Roy. PAC-Bayesian Bounds based on the Renyi Divergence. In Arthur Gretton and Christian C. Robert, editors, _Proceedings of the 19th International Conference on Artificial Intelligence and Statistics_, volume 51 of _Proceedings of Machine Learning Research_, pages 435-444, Cadiz, Spain, 09-11 May 2016. PMLR. URL https://proceedings.mlr.press/v51/begin16.html.
* Catoni [2003] Olivier Catoni. A PAC-Bayesian approach to adaptive classification. _preprint_, 840, 2003.
* Catoni [2004] Olivier Catoni. _Statistical Learning Theory and Stochastic Optimization: Ecole d'Ete de Probabilites de Saint-Flour XXXI-2001_. Springer, 2004.
* Monograph Series_. Institute of Mathematical Statistics, 2007. ISBN 9780940600720. URL https://books.google.fr/books?id=acnaAAAAAAJ.
* Clerico et al. [2022] Eugenio Clerico, George Deligiannidis, and Arnaud Doucet. Conditionally gaussian pac-bayes. In _International Conference on Artificial Intelligence and Statistics_, pages 2311-2329. PMLR, 2022.
* Csiszar [1975] Imre Csiszar. I-divergence geometry of probability distributions and minimization problems. _The Annals of Probability_, pages 146-158, 1975.
* Donsker and Varadhan [1975] MD Donsker and SRS Varadhan. Large deviations for Markov processes and the asymptotic evaluation of certain markov process expectations for large times. In _Probabilistic Methods in Differential Equations_, pages 82-88. Springer, 1975.
* Donsker and Varadhan [1976]Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In _Conference on Uncertainty in Artificial Intelligence [UAI]_, 2017.
* Dziugaite and Roy (2018) Gintare Karolina Dziugaite and Daniel M. Roy. Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of entropy-SGD and data-dependent priors. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 1376-1385. PMLR, 2018. URL http://proceedings.mlr.press/v80/dziugaite18a.html.
* Dziugaite et al. (2021) Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and Daniel Roy. On the role of data in pac-bayes bounds. In _International Conference on Artificial Intelligence and Statistics_, pages 604-612. PMLR, 2021.
* Germain et al. (2015) Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario Marchand, and Jean-Francis Roy. Risk bounds for the majority vote: From a pac-bayesian analysis to a learning algorithm. _arXiv preprint arXiv:1503.08329_, 2015.
* Guedj (2019) Benjamin Guedj. A Primer on PAC-Bayesian Learning. In _Proceedings of the second congress of the French Mathematical Society_, 2019. URL https://arxiv.org/abs/1901.05353.
* Koco and Capponi (2013) Sokol Koco and Cecile Capponi. On multi-class classification through the minimization of the confusion matrix norm. In _Asian Conference on Machine Learning_, pages 277-292. PMLR, 2013.
* Lacasse et al. (2006) Alexandre Lacasse, Francois Laviolette, Mario Marchand, Pascal Germain, and Nicolas Usunier. Pac-bayes bounds for the risk of the majority vote and the variance of the gibbs classifier. _Advances in Neural information processing systems_, 19, 2006.
* Langford and Caruana (2001) John Langford and Rich Caruana. (not) bounding the true error. _Advances in Neural Information Processing Systems_, 14, 2001.
* Laviolette et al. (2017) Francois Laviolette, Emilie Morvant, Liva Ralaivola, and Jean-Francis Roy. Risk upper bounds for general ensemble methods with an application to multiclass classification. _Neurocomputing_, 219:15-25, 2017. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2016.09.016. URL https://www.sciencedirect.com/science/article/pii/S0925231216310177.
* Lei et al. (2019) Yunwen Lei, Urun Dogan, Ding-Xuan Zhou, and Marius Kloft. Data-dependent generalization bounds for multi-class classification. _IEEE Transactions on Information Theory_, 65(5):2995-3021, 2019. doi: 10.1109/TIT.2019.2893916.
* Letarte et al. (2019) Gael Letarte, Pascal Germain, Benjamin Guedj, and Francois Laviolette. Dichotomize and generalize: PAC-Bayesian binary activated deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32_, pages 6872-6882. Curran Associates, Inc., 2019.
* Lever et al. (2010) Guy Lever, Francois Laviolette, and John Shawe-Taylor. Distribution-dependent PAC-Bayes priors. In _International Conference on Algorithmic Learning Theory_, pages 119-133. Springer, 2010.
* Lever et al. (2013) Guy Lever, Francois Laviolette, and John Shawe-Taylor. Tighter PAC-Bayes bounds through distribution-dependent priors. _Theoretical Computer Science_, 473:4-28, February 2013. ISSN 0304-3975. doi: 10.1016/j.tcs.2012.10.013. URL https://linkinghub.elsevier.com/retrieve/pii/S0304397512009346.
* Maurer (2004) Andreas Maurer. A note on the PAC-Bayesian theorem. _arXiv preprint cs/0411099_, 2004.
* McAllester (1998) David A McAllester. Some PAC-Bayesian theorems. In _Proceedings of the eleventh annual conference on Computational Learning Theory_, pages 230-234. ACM, 1998.
* McAllester (1999) David A McAllester. PAC-Bayesian model averaging. In _Proceedings of the twelfth annual conference on Computational Learning Theory_, pages 164-170. ACM, 1999.
* Mohamed et al. (2020) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. _J. Mach. Learn. Res._, 21(132):1-62, 2020.
* Mo et al. (2018)Emilie Morvant, Sokol Koco, and Liva Ralaivola. PAC-Bayesian generalization bound on confusion matrix for multi-class classification. In _Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012_. icml.cc / Omnipress, 2012. URL http://icml.cc/2012/papers/434.pdf.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL https://openreview.net/forum?id=Skz_WfbCZ.
* Parrado-Hernandez et al. (2012) Emilio Parrado-Hernandez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-Bayes bounds with data dependent priors. _J. Mach. Learn. Res._, 13:3507-3531, 2012. URL http://dl.acm.org/citation.cfm?id=2503353.
* Perez-Ortiz et al. (2021) Maria Perez-Ortiz, Omar Rivasplata, Benjamin Guedj, Matthew Gleeson, Jingyu Zhang, John Shawe-Taylor, Miroslaw Bober, and Josef Kittler. Learning pac-bayes priors for probabilistic neural networks. _arXiv preprint arXiv:2109.10304_, 2021.
* Perez-Ortiz et al. (2021) Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvari. Tighter risk certificates for neural networks. _Journal of Machine Learning Research_, 22(227):1-40, 2021. URL http://jmlr.org/papers/v22/20-879.html.
* Pires et al. (2013) Bernardo Avila Pires, Csaba Szepesvari, and Mohammad Ghavamzadeh. Cost-sensitive multiclass classification risk bounds. In _International Conference on Machine Learning_, pages 1391-1399. PMLR, 2013.
* Rivasplata et al. (2018) Omar Rivasplata, Csaba Szepesvari, John Shawe-Taylor, Emilio Parrado-Hernandez, and Shiliang Sun. PAC-Bayes bounds for stable algorithms with instance-dependent priors. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 9234-9244, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/386854131f58a556343e056f03626e00-Abstract.html.
* Seeger (2002) Matthias Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classification. _Journal of Machine Learning Research_, 3(Oct):233-269, 2002.
* Takayama and Akira (1985) Akira Takayama and Takayama Akira. _Mathematical economics_. Cambridge university press, 1985.
* Tschandl (2018) Philipp Tschandl. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions, 2018. URL https://doi.org/10.7910/DVN/DBW86T.
* Wu and Seldin (2022) Yi-Shan Wu and Yevgeny Seldin. Split-kl and pac-bayes-split-kl inequalities for ternary random variables. _Advances in Neural Information Processing Systems_, 35:11369-11381, 2022.
* Zhou et al. (2019) Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Non-vacuous generalization bounds at the ImageNet scale: a PAC-Bayesian compression approach. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=BJgqqsAct7.

Recipe for implementing Theorems 1 and 2

We here outline more explicitly how Theorem 1 and Theorem 2 may be used to formulate a fully differentiable objective by which a model may be trained.

First, if one wishes to make hard labels, namely \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\), it will first be necessary to use a surrogate class of soft hypotheses \(\mathcal{H}^{\prime}\subseteq\mathcal{M}(\mathcal{Y})^{\mathcal{X}}\) during training, before reverting to hard labels for example by taking the mean label or the one with highest probability. Using soft hypotheses during training is necessary to ensure that the empirical \(j\)-risks \(R^{j}_{S}(Q)\) are differentiable with respect to the model parameters. Since how one chooses to do this will depend on the specific use case, we restrict our attention here to the case of soft hypotheses. Specifically, we consider a class of soft hypotheses \(\mathcal{H}=\{h_{\theta}:\theta\in\mathbb{R}^{N}\}\subseteq\mathcal{M}( \mathcal{Y})^{\mathcal{X}}\) parameterised by the weights \(\theta\in\mathbb{R}^{N}\) of some neural network of a given architecture with \(N\) parameters in such a way that the \(R^{j}_{S}(h_{\theta})\) are differentiable in \(\theta\). A concrete example would be multiclass classification using a fully connected neural network with output being softmax probabilities on the classes so that the \(R^{j}_{S}(h_{\theta})\) are differentiable.

Second, it is necessary to restrict the prior and posterior \(P,Q\in\mathcal{M}(\mathcal{H})\) to a parameterised subset of \(\mathcal{M}(\mathcal{H})\) in which \(\text{KL}(Q\|P)\) has a closed form which is differentiable in the parameterisation. A simple choice for our case of a neural network with \(N\) parameters is \(P,Q\in\{\mathcal{N}(\bm{w},\text{diag}(\bm{s})):\bm{w}\in\mathbb{R}^{N},\bm{s} \in\mathbb{R}^{N}_{>0}\}\). For prior a \(P_{\bm{v},\bm{r}}=\mathcal{N}(\bm{v},\text{diag}(\bm{r}))\) and posterior \(Q_{\bm{w},\bm{s}}=\mathcal{N}(\bm{w},\text{diag}(\bm{s}))\) we have the closed form

\[\text{KL}(Q_{\bm{w},\bm{s}}\|P_{\bm{v},\bm{r}})=\frac{1}{2}\left[\sum_{n=1}^{ N}\left(\frac{s_{n}}{r_{n}}+\frac{(w_{n}-v_{n})^{2}}{r_{n}}+\ln\frac{r_{n}}{s_{n}} \right)-N\right],\]

which is indeed differentiable in \(\bm{v},\bm{r},\bm{w}\) and \(\bm{s}\). While \(Q_{\bm{w},\bm{s}}\) and \(P_{\bm{v},\bm{r}}\) are technically distributions on \(\mathbb{R}^{D}\) rather than \(\mathcal{H}\), the KL-divergence between the distributions they induce on \(\mathcal{H}\) will be at most as large as the expression above. Thus, substituting the expression above into the bounds we prove in Section 3 can only increase the value of the bounds, meaning the enlarged bounds certainly still hold with probability at least \(1-\delta\).

Third, in all but the simplest cases \(R^{j}_{S}(Q_{\bm{w},\bm{s}})\) will not have a closed form, much less one that is differentiable in \(\bm{w}\) and \(\bm{s}\). A common solution to this is to use the so-called pathwise gradient estimator. In our case, this corresponds to drawing \(\bm{\epsilon}\sim\mathcal{N}(\bm{0},\mathbb{I})\), where \(\mathbb{I}\) is the \(N\times N\) identity matrix, and estimating

\[\nabla_{\bm{w},\bm{s}}R^{j}_{S}(Q_{\bm{w},\bm{s}})=\nabla_{\bm{w},\bm{s}} \left[\mathbb{E}_{\bm{\epsilon}^{\prime}\sim\mathcal{N}(\bm{0},\mathbb{I})}R^ {j}_{S}(h_{\bm{w}+\bm{\epsilon}^{\prime}\odot\sqrt{\bm{s}}})\right]\approx \nabla_{\bm{w},\bm{s}}R^{j}_{S}(h_{\bm{w}+\bm{\epsilon}\odot\sqrt{\bm{s}}}),\]

where \(h_{\bm{w}}\) denotes the function expressed by the neural network with parameters \(\bm{w}\). For a proof that this is an unbiased estimator, and for other methods for estimating the gradients of expectations, see the survey Mohamed et al. (2020).

Fourth, one must choose the prior. Designing priors which are optimal in some sense (_i.e._, minimising the Kullback-Leibler term in the right-hand side of generalisation bounds) has been at the core of an active line of work in the PAC-Bayesian literature. For the sake of simplicity, and since it is out of the scope of our contributions, we assume here that the prior is given beforehand, although we stress that practitioners should pay great attention to its tuning. For our purposes, it suffices to say that if one is using a data-dependent prior then it is necessary to partition the sample into \(S=S_{\text{Prior}}\cup S_{\text{Bound}}\), where \(S_{\text{Prior}}\) is used to train the prior and \(S_{\text{Bound}}\) is used to evaluate the bound. Since our bound holds uniformly over posteriors \(Q\in\mathcal{M}(\mathcal{H})\), the entire sample \(S\) is free to be used to train the posterior \(Q\). For a more in-depth discussion on the choice of prior, we refer to the following body of work: Ambroladze et al. (2006); Lever et al. (2010, 2013); Parrado-Hernandez et al. (2012); Dziugaite and Roy (2017, 2018); Rivasplata et al. (2018); Letarte et al. (2019); Perez-Ortiz et al. (2021); Dziugaite et al. (2021); Biggs and Guedj (2022, 2022).

Finally, given a confidence level \(\delta\in(0,1]\), one may use Algorithm 1 to obtain a posterior \(Q_{\bm{w},\bm{s}}\) with minimal upper bound on the total risk. Note we take the pointwise logarithm of the variances \(\bm{r}\) and \(\bm{s}\) to obtain unbounded parameters on which to perform stochastic gradient descent or some other minimisation algorithm. We use \(\oplus\) to denote vector concatenation. The algorithm can be straightforwardly adapted to permit mini-batches by, for each epoch, sequentially repeating the steps with \(S\) equal to each mini-batch.

**Input:**

\(\mathcal{X},\mathcal{Y}\) /* Arbitrary input and output spaces */ \(\bigcup_{j=1}^{M}E_{j}=\mathcal{Y}^{2}\) /* A finite partition into error types */ \(\boldsymbol{\ell}\in[0,\infty)^{M}\) /* A vector of losses, not all equal */ \(S=S_{\text{Prior}}\cup S_{\text{Bound}}\in(\mathcal{X}\times\mathcal{Y})^{m}\) /* A partitioned i.i.d. sample */ \(N\in\mathbb{N}\) /* The number of model parameters */ \(P_{\boldsymbol{v},\boldsymbol{r}}\), \(\boldsymbol{v}(S_{\text{Prior}})\in\mathbb{R}^{N},\boldsymbol{r}(S_{\text{ Prior}})\in\mathbb{R}_{\geq 0}^{N}\) /* A (data-dependent) prior */ \(Q_{\boldsymbol{w}_{0},\boldsymbol{s}_{0}}\), \(\boldsymbol{w}_{0}\in\mathbb{R}^{N},\boldsymbol{s}_{0}\in\mathbb{R}_{\geq 0}^{ N}\) /* An initial posterior */ \(\delta\in(0,1]\) /* A confidence level */ \(\lambda>0\) /* A learning rate */ \(T\) /* The number of epochs to train for */ Output: \(Q_{\boldsymbol{w},\boldsymbol{s}}\), \(\boldsymbol{w}\in\mathbb{R}^{N},\boldsymbol{s}\in\mathbb{R}_{\geq 0}^{N}\) /* A trained posterior */

**Procedure:**

\(\zeta_{0}\leftarrow\log s_{0}\) /* Transform to unbounded scale parameters */ \(\boldsymbol{p}\leftarrow\boldsymbol{w}_{0}\oplus\zeta_{0}\) /* Collect mean and scale parameters */

**for \(t\gets 1\) to \(T\) do**

 Draw \(\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbb{0},\mathbb{I})\)

\(\boldsymbol{u}\leftarrow\boldsymbol{R}_{S}\left(h_{\boldsymbol{w}\gets \boldsymbol{\epsilon}\odot\sqrt{\exp(\boldsymbol{\zeta})}}\right)\)

\(B\leftarrow\frac{1}{m}\bigg{[}\text{KL}\left(Q_{\boldsymbol{w},\exp(\boldsymbol{ \zeta})}\big{\|}P_{\boldsymbol{v},\boldsymbol{r}}\right)+\ln\left(\frac{1}{ \delta}\sqrt{\pi}e^{1/12m}\left(\frac{m}{2}\right)^{\frac{M-1}{2}}\sum_{z=0}^{ M-1}\binom{M}{z}\frac{1}{(\pi m)^{z/2}\Gamma\left(\frac{M-z}{2}\right)}\right)\bigg{]}\)

\(\tilde{\boldsymbol{u}}\leftarrow(u_{1},\ldots,u_{M},B)\)

\(\boldsymbol{G}\leftarrow\boldsymbol{0}_{2N\times(M+1)}\) /* Initialise gradient matrix */ \(\boldsymbol{F}\leftarrow\boldsymbol{0}_{M+1}\) /* Initialise gradient vector */ for \(j\gets 1\) to \(M+1\) do \(\boldsymbol{F}_{j}\leftarrow\frac{\partial f_{j}}{\partial\tilde{u}_{j}}( \tilde{\boldsymbol{u}})\) /* Gradients of total loss from Theorem 2 */ for \(i\gets 1\) to \(2N\) do \(\boldsymbol{G}_{i,j}\leftarrow\frac{\partial\tilde{u}_{j}}{\partial p_{i}}( \boldsymbol{p})\) /* Gradients of empirical risks and bound */

**end**

**end**

\(\boldsymbol{H}\leftarrow\boldsymbol{GF}\) /* Gradients of total loss w.r.t. parameters */ \(\boldsymbol{p}\leftarrow\boldsymbol{p}-\lambda\boldsymbol{H}\) /* Gradient step */

**end**

\(\boldsymbol{w}=(p_{1},\ldots,p_{N})\)

\(\boldsymbol{s}=(\exp(p_{N+1}),\ldots,\exp(p_{2N}))\)

**return \(\boldsymbol{w},\boldsymbol{s}\)**

**Algorithm 1** Calculating a posterior with minimal bound on the total risk.

## Appendix B Additional Experimental Details

For MNIST we map labels \(\{0,1,2,3,4\}\) to \(0\) and \(\{5,6,7,8,9\}\) to \(1\). For HAM10000 we map the cancerous or pre-cancerous labels \(\{\texttt{Melanoma},\texttt{Basal Cell Carcinoma},\texttt{Actinic Keratosis}\}\) to \(1\) and the other labels to \(0\). In both cases we partition \(\mathcal{Y}^{2}\) into \(E_{0}=\{(0,0),(1,1)\}\), \(E_{1}=\{(0,1)\}\) and \(E_{2}=\{(1,0)\}\), and take \(\boldsymbol{\ell}=(0,1,3)\). For HAM10000, \(E_{1}\) and \(E_{2}\) then refer to Type I and Type II errors, respectively, and \(\boldsymbol{\ell}\) reflects the greater severity of false negatives.

Each dataset is split into prior and certification sets \(S_{\text{Prior}}\) and \(S_{\text{Bound}}\), respectively. For MNIST, we use the conventional training set of size \(60000\) as the prior set, and the conventional test set of size \(10000\) as the certification set. For HAM10000 we pool the conventional train, validation and test sets together and then split 50-50 to obtain prior and certification sets each of size \(5860\). For HAM10000we resize the images to \((28,28)\) and use just the first channel so that the data dimension is the same for both datasets.

We take \(\mathcal{H}\) to be two-layer MLPs with 784, 100 and 2 units in the input, hidden and output layers, respectively. As is common in the PAC-Bayes literature, we restrict \(P\) to be an isotropic Gaussian \(N(\bm{v},\lambda\bm{I})\) and \(Q\) to be a diagonal Gaussian \(N(\bm{w},\text{diag}(\bm{s}))\). Further, as in Dziugaite and Roy (2017), we restrict \(\lambda\) to be of the form \(\lambda_{j}=c\exp(-j/b)\) for some \(j\in\mathbb{N}\), taking \(c=0.1\) and \(b=100\). Since, at the end of training, we will then have one prior \(P_{j}\) for each \(j\in\mathbb{N}\), we can choose the \(j\) that minimizes the PAC-Bayes bound provided we take a union over all of them, taking \(\delta_{j}=\frac{6\delta}{\pi^{2}j^{2}}\) so that \(\sum_{j}\delta_{j}=1\) and all the bounds hold simultaneously with probability at least \(1-\delta\). After applying Algorithm 1 we round \(\lambda\) to a discrete \(\lambda_{j}\), either up or down depending on which gives the smaller bound.

For both datasets we set the prior mean \(\bm{v}\) to be the parameters of an MLP trained on the prior set. In both cases we use SGD with learning rate \(0.01\) to minimise the cross-entropy loss, using a portion of the prior set as a validation set. For MNIST we train the MLP for 20 epochs to get an error rate of 14%, for HAM10000 we train the MLP for 5 epochs to get an error rate of 22%. We then apply Algorithm 1. By combining Proposition 2 (with \(\delta=0.01\) and \(N=100000\)) and Proposition 3. We obtain \(\bm{R}_{S}(\hat{Q})=(0.8879,0.0919,0.0203)\) and \(R_{D}^{T}(Q)\leq 0.2640\) for MNIST and \(\bm{R}_{S}(\hat{Q})=(0.7860,0.0146,0.1995)\) and \(R_{D}^{T}(Q)\leq 0.8379\) for HAM10000, where both bounds hold with probability at least \(1-0.05-0.01=0.94\).

The full results are shown in Figure 2. Figures 1(a), 1(c) and 1(c), and are repeated here for easier comparison with the HAM10000 results. Figure 1(b) shows that Algorithm 1 has failed to reduce the bound on the total risk beyond the initialisation of \(Q\) to \(P\), with the small variation being explained by different MC samples being drawn from \(Q\) during training rather than \(Q\) changing substantially. Indeed, Figure 1(h) shows that \(Q\) does not appreciably move from its initialisation at \(P\)--\(\text{KL}(Q\|P)\) remains below \(0.1\) whereas in the MNNIST experiment, which has the same number of parameters, exceeds \(30\). It is therefore unsurprising that Figures 1(d) and 1(f) show negligible change in the empirical error probabilities and the bound on \(\text{kl}(\bm{R}_{S}(Q)\|\bm{R}_{D}(Q))\), respectively. The divergence in the results is likely due to the difference in sample size; the certification set for the MNIST experiment contains \(10000\) samples, whereas for the HAM10000 dataset there are only \(5000\), which, all else equal, makes an increase in \(\text{KL}(Q\|P)\) twice as expensive.

Recall from Section 6 that while \(\bm{R}_{D}(Q)\) can be effectively constrained to a sub-region of the simple \(\triangle_{M}\) using our Theorem 1, this can also be achieved by unioning \(M\) Maurer bounds, one for each error probability. Table 1 gave the 95% confidence intervals for the volumes of the confidence regions in which \(\bm{R}_{D}(Q)\) was likely to lie for experiments on MNIST and HAM10000, but neither region was uniformly smaller, making it unclear which method should be preferred.

Table 2 provides additional data by taking synthetic values for \(\bm{R}_{S}(Q)\) and \(\text{KL}(Q\|P)\), for different values of \(m\) (the size of the certification set) and \(M\) (the number of error types). 'Individual' denotes unioning individual Maurer bounds, 'Ours' is our method, 'Intersection' is the intersection of the confidence regions given by the previous two methods (but loosened so that they now both hold simultaneously with probability at least 0.95), and 'Morv.' is the confidence region produced by Morvant's bound Morvant et al. (2012). The 95% confidence intervals for the volumes of all the regions have been produced by Monte Carlo samples. We see that our confidence region is tighter than the individual one in 4/9 cases (green), worse in 3/9 cases (red) and ties in 2/9 cases (orange). Interestingly, union bounding the naive CR and our CR and intersecting often beats both of these (**bold**). Morvant's result is either not applicable or their confidence region is much larger than ours and essentially takes up the entire simplex, hence the volume estimate of \(1.000\). The reason their bound is sometimes inapplicable is because it requires every class to contain at least \(8L\) instances, where \(L\) is the number of labels--in the \(L=5,M=25,m=100\) case this would require each class to contain at least \(5\times 8=40\) instances which is impossible with \(m=100\) samples.

Figure 2: MNIST (first column) and HAM10000 (second column) experiments.

## Appendix C Proofs

### Proof of Proposition 3

Write \(\text{kl}(\hat{\bm{q}}\|\bm{p})\) as

\[\sum_{j=1}^{M}\hat{q}_{j}\ln\frac{\hat{q}_{j}}{q_{j}}+\sum_{j=1}^{M}\hat{q}_{j} \ln\frac{q_{j}}{p_{j}}.\] (13)

The result then follows by bounding the two sums by

\[\sum_{j=1}^{M}\hat{q}_{j}\ln\frac{\hat{q}_{j}}{q_{j}}=\sum_{j=1}^{M}\text{kl}( \hat{q}_{j}\|q_{j})-(1-\hat{q}_{j})\ln\frac{1-\hat{q}_{j}}{1-q_{j}}\leq MB_{2} -\sum_{j=1}^{M}(1-\hat{q}_{j})\ln\frac{1-\hat{q}_{j}}{1-\underline{q}_{j}}\] (14)

and

\[\sum_{j=1}^{M}\hat{q}_{j}\ln\frac{q_{j}}{p_{j}}=\sum_{j=1}^{M}\frac{\hat{q}_{ j}}{q_{j}}q_{j}\ln\frac{q_{j}}{p_{j}}\leq\max_{j}\frac{\hat{q}_{j}}{ \underline{q}_{j}}\sum_{j=1}^{M}q_{j}\ln\frac{q_{j}}{p_{j}}\leq B_{1}\max_{j} \frac{\hat{q}_{j}}{\underline{q}_{j}}.\] (15)

Putting these together we obtain the bound on \(\text{kl}(\hat{\bm{q}}\|\bm{p})\). The limit follows because each \(\underline{q}_{j}\to\hat{q}_{j}\) as \(B_{2}\to 0\).

### Proof of Lemma 2

Let \(\bm{E}_{M}:=\{\bm{e}_{1},\ldots,\bm{e}_{M}\}\), namely the set of \(M\)-dimensional basis vectors. We will denote a typical element of \(\bm{E}_{M}^{m}\) by \(\bm{\eta}^{(m)}=(\bm{\eta}_{1},\ldots,\bm{\eta}_{m})\). For any \(\bm{x}^{(m)}=(\bm{x}_{1},\ldots,\bm{x}_{m})\in\triangle_{M}^{m}\), a straightforward induction on \(m\) yields

\[\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\left(\prod_{i=1}^{m}\bm{x}_{i}\cdot \bm{\eta}_{i}\right)=1.\] (16)

To see this, for \(m=1\) we have \(\bm{E}_{M}^{1}=\{(\bm{e}_{1},),\ldots,(\bm{e}_{M},)\}\), where we have been pedantic in using \(1\)-tuples to maintain consistency with larger values of \(m\). Thus, for any \(\bm{x}^{(1)}=(\bm{x}_{1},)\in\triangle_{M}^{1}\), the left hand side of equation (16) can be written as

\[\sum_{j=1}^{M}\bm{x}_{1}\cdot\bm{e}_{j}=\sum_{j=1}^{M}(\bm{x}_{1})_{j}=1.\]Now suppose that equation (16) holds for any \(\bm{x}^{(m)}\in\triangle_{M}^{m}\) and let \(\bm{x}^{(m+1)}=(\bm{x}_{1},\ldots,\bm{x}_{m+1})\in\triangle_{M}^{m+1}\). Then the left hand side of equation (16) can be written as

\[\sum_{\bm{\eta}^{(m+1)}\in\bm{E}_{M}^{m+1}}\left(\prod_{i=1}^{m+1} \bm{x}_{i}\cdot\bm{\eta}_{i}\right) =\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\sum_{j=1}^{M}\left(\prod _{i=1}^{m}\bm{x}_{i}\cdot\bm{\eta}_{i}\right)(\bm{x}_{m+1}\cdot\bm{e}_{j})\] \[=\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\left(\prod_{i=1}^{m}\bm {x}_{i}\cdot\bm{\eta}_{i}\right)\sum_{j=1}^{M}\left(\bm{x}_{m+1}\cdot\bm{e}_{ j}\right)=1.\]

We now show that any \(\bm{x}^{(m)}=(\bm{x}_{1},\ldots,\bm{x}_{m})\in\triangle_{M}^{m}\) can be written as a convex combination of the elements of \(\bm{E}_{M}^{m}\) in the following way

\[\bm{x}^{(m)}=\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\left(\prod_{i=1}^{m}\bm{ x}_{i}\cdot\bm{\eta}_{i}\right)\bm{\eta}^{(m)}.\] (17)

We have already shown that the weights sum to one, and they are clearly elements of \([0,1]\), so the right hand side of equation (17) is indeed a convex combination of the elements of \(\bm{E}_{M}^{m}\). We now show that equation (17) holds, again by induction.

For \(m=1\) and any \(\bm{x}^{(1)}=(\bm{x}_{1},)\in\triangle_{M}^{1}\), the right hand side of equation (17) can be written as

\[\sum_{j=1}^{M}(\bm{x}_{1}\cdot\bm{e}_{j})(\bm{e}_{j},)=(\bm{x}_{1},)=\bm{x}.\]

For the inductive hypothesis, suppose equation (17) holds for some arbitrary \(m\geq 1\), and denote elements of \(\bm{E}_{M}^{m+1}\) by \(\bm{\eta}^{(m)}\oplus(\bm{e},)\) for some \(\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}\) and \(\bm{e}\in\bm{E}_{M}\), where \(\oplus\) denotes vector concatenation. Then for any \(\bm{x}^{(m+1)}=\bm{x}^{(m)}\oplus(\bm{x}_{m+1},)=(\bm{x}_{1},\ldots,\bm{x}_{m +1})\in\triangle_{M}^{m+1}\), the right hand side of equation (17) can be written as

\[\sum_{\bm{\eta}^{(m+1)}\in\bm{E}_{M}^{m+1}}\left(\prod_{i=1}^{m+1 }\bm{x}_{i}\cdot\bm{\eta}_{i}\right)\bm{\eta}^{(m+1)} =\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\sum_{j=1}^{M}\left(\prod _{i=1}^{m}\bm{x}_{i}\cdot\bm{\eta}_{i}\right)(\bm{x}_{m+1}\cdot\bm{e}_{j})\bm {\eta}^{(m)}\oplus(\bm{e}_{j},)\] \[=\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\sum_{j=1}^{M}\left(\prod _{i=1}^{m}\bm{x}_{i}\cdot\bm{\eta}_{i}\right)(\bm{x}_{m+1}\cdot\bm{e}_{j})\bm {\eta}^{(m)}\] \[\oplus\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\sum_{j=1}^{M}\left( \prod_{i=1}^{m}\bm{x}_{i}\cdot\bm{\eta}_{i}\right)(\bm{x}_{m+1}\cdot\bm{e}_{j} )(\bm{e}_{j},)\] \[=\sum_{j=1}^{M}(\bm{x}_{m+1}\cdot\bm{e}_{j})\sum_{\bm{\eta}^{(m)} \in\bm{E}_{M}^{m}}\left(\prod_{i=1}^{m}\bm{x}_{i}\cdot\bm{\eta}_{i}\right)\bm {\eta}^{(m)}\] \[\oplus\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\left(\prod_{i=1}^{m} \bm{x}_{i}\cdot\bm{\eta}_{i}\right)\sum_{j=1}^{M}(\bm{x}_{m+1}\cdot\bm{e}_{j} )(\bm{e}_{j},)\] \[=1\cdot\bm{x}^{(m)}\oplus 1\cdot(\bm{x}_{m+1},)=\bm{x}^{(m+1)},\]

where in the penultimate equality we have used the inductive hypothesis and (twice) the result of the previous induction.

We can now prove the statement of the Lemma. Applying Jensen's inequality to equation (17) with the convex function \(f\), we have that

\[f(\bm{x}_{1},\ldots,\bm{x}_{m}) =f\left(\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\left(\prod_{i=1}^{m} \bm{x}_{i}\cdot\bm{\eta}_{i}\right)\bm{\eta}^{(m)}\right)\] \[\leq\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\left(\prod_{i=1}^{m} \bm{x}_{i}\cdot\bm{\eta}_{i}\right)f\left(\bm{\eta}^{(m)}\right).\]Let \(\bm{\mu}=\mathbb{E}[\bm{X}_{1}]\) denote the mean of the i.i.d. random vectors \(X_{i}\). Then the above inequality implies

\[\mathbb{E}[f(\bm{X}_{1},\ldots,\bm{X}_{m})] \leq\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\left(\prod_{i=1}^{m} \bm{\mu}\cdot\bm{\eta}_{i}\right)f\left(\bm{\eta}^{(m)}\right)\] \[=\sum_{\bm{\eta}^{(m)}\in\bm{E}_{M}^{m}}\left(\prod_{i=1}^{m} \mathbb{P}(\bm{X}_{i}^{\prime}=\bm{\eta}_{i})\right)f\left(\bm{\eta}^{(m)}\right)\] \[=\mathbb{E}[f(\bm{X}_{1}^{\prime},\ldots,\bm{X}_{m}^{\prime})].\]

### Proof of Lemma 3

The proof of Lemma 3 itself requires two technical helping lemmas which we now state and prove.

**Lemma 4**.: _For any integers \(n\geq 2\) and \(p\geq-1\),_

\[\sum_{k=1}^{n-1}\frac{(n-k)^{p/2}}{\sqrt{k}}\leq n^{\frac{p+1}{2}}\int_{0}^{1 }\frac{(1-x)^{p/2}}{\sqrt{x}}dx.\]

Proof.: The case of \(p=-1\), namely

\[\sum_{k=1}^{n-1}\frac{1}{\sqrt{k(n-k)}}\leq\int_{0}^{1}\frac{1}{\sqrt{x(1-x)}}dx,\]

has already been demonstrated in Maurer (2004). For \(p>-1\), let

\[f_{p}(x):=\frac{(1-x)^{p/2}}{\sqrt{x}}.\]

We will show that each \(f_{p}(\cdot)\) is monotonically decreasing on \((0,1)\). Indeed,

\[\frac{df_{p}}{dx}(x)=-\frac{(1-x)^{\frac{p}{2}-1}(px+1-x)}{2x^{3/2}}\leq-\frac {(1-x)^{p/2}}{2x^{3/2}}<0,\]

where for the inequalities we have used the fact that \(p>-1\) and \(x\in(0,1)\). We therefore see that

\[\sum_{k=1}^{n-1}\frac{(n-k)^{p/2}}{\sqrt{k}} =\sum_{k=1}^{n-1}\frac{n^{p/2}(1-\frac{k}{n})^{p/2}}{\sqrt{n} \sqrt{\frac{k}{n}}}\] \[=n^{\frac{p+1}{2}}\sum_{k=1}^{n-1}\frac{1}{n}\frac{(1-\frac{k}{n })^{p/2}}{\sqrt{\frac{k}{n}}}\] \[=n^{\frac{p+1}{2}}\sum_{k=1}^{n-1}\frac{1}{n}f_{p}\left(\frac{k} {n}\right)\] \[\leq n^{\frac{p+1}{2}}\sum_{k=1}^{n-1}\int_{\frac{k-1}{n}}^{\frac {k}{n}}f_{p}(x)dx\] \[=n^{\frac{p+1}{2}}\int_{0}^{1-\frac{1}{n}}f_{p}(x)dx\] \[\leq n^{\frac{p+1}{2}}\int_{0}^{1}f_{p}(x)dx.\]

Intuitively, the proof of the above lemma works by bounding the integral below by a Riemann sum. In the following lemma we actually calculate this integral, yielding a more explicit bound on the sum in Lemma 4. We found it is easier to calculate a slightly more general integral, where the \(1\) in the limit and the integrand is replaced by a positive constant \(a\).

**Lemma 5**.: _For any real number \(a>0\) and integer \(n\geq-1\),_

\[\int_{0}^{a}\frac{(a-x)^{n/2}}{\sqrt{x}}dx=\sqrt{\pi}\frac{\Gamma(\frac{n+2}{2})} {\Gamma(\frac{n+3}{2})}a^{\frac{n+1}{2}}.\]

Proof.: Define

\[\text{I}_{n}(a):=\int_{0}^{a}\frac{(a-x)^{n/2}}{\sqrt{x}}dx\qquad\text{and} \qquad f_{n}(a):=\sqrt{\pi}\frac{\Gamma(\frac{n+2}{2})}{\Gamma(\frac{n+3}{2})} a^{\frac{n+1}{2}}.\]

We proceed by induction, increasing \(n\) by \(2\) each time. This means we need two base cases. First, for \(n=-1\), we have

\[\text{I}_{-1}(a)=\int_{0}^{a}\frac{1}{\sqrt{x(a-x)}}dx=\left[2\arcsin\sqrt{ \frac{x}{a}}\right]_{0}^{a}=\pi=f_{-1}(a),\]

since \(\Gamma(\frac{1}{2})=\sqrt{\pi}\) and \(\Gamma(1)=1\). Second, for \(n=0\),

\[\text{I}_{0}(a)=\int_{0}^{a}\frac{1}{\sqrt{x}}dx=\left[2\sqrt{x}\,\right]_{0 }^{a}=2\sqrt{a}=f_{0}(a),\]

since \(\Gamma(\frac{3}{2})=\frac{\sqrt{\pi}}{2}\). Now, by the Leibniz integral rule, we have

\[\frac{d}{da}\text{I}_{n+2}(a)=\int_{0}^{a}\frac{\partial}{\partial a}\frac{( a-x)^{\frac{n+2}{2}}}{\sqrt{x}}dx=\frac{n+2}{2}\int_{0}^{a}\frac{(a-x)^{\frac{n}{2} }}{\sqrt{x}}dx=\frac{n+2}{2}\text{I}_{n}(a).\]

Thus

\[\text{I}_{n+2}(a)=\frac{n+2}{2}\left[\int_{0}^{a}\text{I}_{n}(t)dt+\text{I}_{ n}(0)\right]=\frac{n+2}{2}\int_{0}^{a}\text{I}_{n}(t)dt,\]

since \(\text{I}_{n}(0)=0\).

Now, for the inductive step, suppose \(\text{I}_{n}(a)=f_{n}(a)\) for some \(n\geq-1\). Then, using the previous calculation, we have

\[\text{I}_{n+2}(a) =\frac{n+2}{2}\int_{0}^{a}f_{n}(t)dt\] \[=\frac{n+2}{2}\int_{0}^{a}\sqrt{\pi}\frac{\Gamma(\frac{n+2}{2})} {\Gamma(\frac{n+3}{2})}t^{\frac{n+1}{2}}dt\] \[=\sqrt{\pi}\frac{\frac{n+2}{2}\Gamma(\frac{n+2}{2})}{\frac{n+3}{ 2}\Gamma(\frac{n+3}{2})}a^{\frac{n+3}{2}}\] \[=\sqrt{\pi}\frac{\Gamma(\frac{n+2}{2}+1)}{\Gamma(\frac{n+3}{2}+1) }a^{\frac{n+3}{2}}\] \[=\sqrt{\pi}\frac{\Gamma\left(\frac{(n+2)+2}{2}\right)}{\Gamma \left(\frac{(n+2)+3}{2}\right)}a^{\frac{(n+2)+1}{2}}\] \[=f_{n+2}(a).\]

This completes the proof. 

We are now ready to prove Lemma 3 which, for ease of reference, we restate here. For integers \(M\geq 1\) and \(m\geq M\),

\[\sum_{\bm{k}\in S_{m,M}^{\otimes 0}}\frac{1}{\prod_{j=1}^{M}\sqrt{k_{j}}}\leq \frac{\pi^{\frac{M}{2}}m^{\frac{M-2}{2}}}{\Gamma(\frac{M}{2})}.\]

[MISSING_PAGE_EMPTY:22]

The final inequality holds since \(\text{kl}(\tilde{\bm{q}}\|\tilde{\bm{p}})\geq 0\). Further, note that we have equality if and only if \(\tilde{\bm{q}}=\tilde{\bm{p}}\), which, by their definitions, translates to

\[p_{i}=\frac{1-p_{j}}{1-q_{j}}q_{i}\]

for all \(i\neq j\). If we now add \(q_{j}\log\frac{q_{j}}{p_{j}}\) to both sides, we obtain

\[\text{kl}(\bm{q}\|\bm{p})\geq(1-q_{j})\log\frac{1-q_{j}}{1-p_{j}}+q_{j}\log \frac{q_{j}}{p_{j}}=\text{kl}(q_{j}\|p_{j}),\]

with the same condition for equality. 

The following proposition makes more precise the argument found at the beginning of Section 4 for how Proposition 1 can be used to derive the tightest possible lower and upper bounds on each \(R_{D}^{j}(Q)\).

**Proposition 5**.: _Suppose that \(\bm{q},\bm{p}\in\triangle_{M}\) are such that \(\text{kl}(\bm{q}\|\bm{p})\leq B\), where \(\bm{q}\) is known and \(\bm{p}\) is unknown. Then, in the absence of any further information, the tightest bound that can be obtained on each \(p_{j}\) is_

\[p_{j}\leq\text{kl}^{-1}(q_{j},B).\]

Proof.: Suppose \(p_{j}>\text{kl}^{-1}(q_{j},B)\). Then, by definition of \(\text{kl}^{-1}\), we have that \(\text{kl}(q_{j}\|p_{j})>B\). By Proposition 1, this would then imply \(\text{kl}(\bm{q}\|\bm{p})>B\), contradicting our assumption. Therefore \(p_{j}\leq\text{kl}^{-1}(q_{j},B)\). Now, with the information we have, we cannot rule out that

\[p_{i}=\frac{1-p_{j}}{1-q_{j}}q_{i}\]

for all \(i\neq j\) and thus, by Proposition 1, that \(\text{kl}(q_{j}\|p_{j})=\text{kl}(\bm{q}\|\bm{p})\). Further, we cannot rule out that \(\text{kl}(\bm{q}\|\bm{p})=B\). Thus, it is possible that \(\text{kl}(q_{j}\|p_{j})=B\), in which case \(p_{j}=\text{kl}^{-1}(q_{j},B)\). We therefore see that \(\text{kl}^{-1}(q_{j},B)\) is the tightest possible upper bound on \(p_{j}\), for each \(j\in[M]\). 

### Proof of Theorem 2

Before proving the proposition, we first argue that \(\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) given by Definition 1 is well-defined. First, note that \(A_{\bm{u}}:=\{\bm{v}\in\triangle_{M}:\text{kl}(\bm{u}\|\bm{v})\leq c\}\) is compact (boundedness is clear and it is closed because it is the preimage of the closed set \([0,c]\) under the continuous map \(\bm{v}\mapsto\text{kl}(\bm{u}\|\bm{v})\)) and so the continuous function \(f_{\bm{\ell}}\) achieves its supremum on \(A_{\bm{u}}\). Further, note that \(A_{\bm{u}}\) is a convex subset of \(\triangle_{M}\) (because the map \(\bm{v}\mapsto\text{kl}(\bm{u}\|\bm{v})\) is convex) and \(f_{\bm{\ell}}\) is linear, so the supremum of \(f_{\bm{\ell}}\) over \(A_{\bm{u}}\) is achieved and is located on the boundary of \(A_{\bm{u}}\). This means we can replace the inequality constraint \(\text{kl}(\bm{u}\|\bm{v})\leq c\) in Definition 1 with the equality constraint \(\text{kl}(\bm{u}\|\bm{v})=c\). Finally, if \(\bm{u}\in\triangle_{M}^{>0}\) then \(A_{\bm{u}}\) is a _strictly_ convex subset of \(\triangle_{M}\) (because the map \(\bm{v}\mapsto\text{kl}(\bm{u}\|\bm{v})\) is then _strictly_ convex) and so the supremum of \(f_{\bm{\ell}}\) occurs at a _unique_ point on the boundary of \(A_{\bm{u}}\). In other words, if \(\bm{u}\in\triangle_{M}^{>0}\) then \(\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) is defined _uniquely_.

We now prove Theorem 2. While our proof technique is somewhat analogous to the technique used in Clerico et al. (2022) to obtain derivatives of the one-dimensional kl-inverse, our theorem directly yields derivatives on the total risk by (implicitly) employing the envelope theorem (see for example Takayama and Akira, 1985).

Proof Outline:We first derive the expression given for \(\bm{v}^{*}(\tilde{\bm{u}})=\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) given on line (5) of the theorem using the method of Lagrange multipliers. Since we are working on the simplex, we make things easier for ourselves by first making the substitution \(t_{j}=\ln v_{j}\) to make the \(v_{j}>0\) constraints unnecessary. The method of Lagrange multipliers yields both the maximum and the minimum (recall that \(\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) is defined as the location of a maximum) for the two values of the Lagrange multiplier \(\mu\). We show that exactly one of these values lies in the interval \(\mu\in(-\infty,-\max_{j}\ell_{j})\) and that this one corresponds to the maximum. This shows that the value \(\mu^{*}\) Theorem 2 instructs us to find indeed yields \(\bm{v}^{*}(\tilde{\bm{u}})=\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\). Finally, we derive the partial derivatives of \(\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) with respect the \(\tilde{u}_{j}\) to obtain the second part of the theorem, namely line (6) by employing the envelope theorem.

Proof.: (of Theorem 2) We start by deriving the implicit expression for \(\bm{v}^{*}(\tilde{\bm{u}})=\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) given in the proposition by solving a transformed version of the optimisation problem given by Definition 1 using the method of Lagrange multipliers. We obtain two solutions to the Lagrangian equations, which must correspond to the maximum and minimum total risk over the set \(A_{\bm{u}}:=\{\bm{v}\in\triangle_{M}:\text{kl}(\bm{u}\|\bm{v})\leq c\}\) because, as argued in the main text (see the discussion after Definition 1), \(A_{\bm{u}}\) is compact and so the linear total risk \(f_{\bm{\ell}}(\bm{v})\) attains its maximum and minimum on \(A_{\bm{u}}\).

By definition of \(\bm{v}^{*}(\tilde{\bm{u}})=\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\), we know that \(\text{kl}(\bm{v}^{*}(\tilde{\bm{u}})\|\bm{u})\leq c\). Since, by assumption, \(u_{j}>0\) for all \(j\), we see that \(\bm{v}^{*}(\tilde{\bm{u}})_{j}>0\) for all \(j\), otherwise we would have \(\text{kl}(\bm{v}^{*}(\tilde{\bm{u}})\|\bm{u})=\infty\), a contradiction. Thus \(\bm{v}^{*}(\tilde{\bm{u}})\in\triangle_{M}^{>0}\) and we are permitted to instead optimise over the unbounded variable \(\bm{t}\in\mathbb{R}^{M}\), where \(t_{j}:=\ln v_{j}\). With this transformation, the constraint \(\bm{v}\in\triangle_{M}\) can be replaced simply by \(\sum_{j}e^{t_{j}}=1\) and the optimisation problem becomes

Maximise: \[F(\bm{t}):=\sum_{j=1}^{M}\ell_{j}e^{t_{j}}\] Subject to: \[g(\bm{t};\bm{u},c):=\text{kl}(\bm{u}\|e^{\bm{t}})-c=0,\] \[h(\bm{t}):=\sum_{j=1}^{M}e^{t_{j}}-1=0,\]

where \(e^{\bm{t}}\in\mathbb{R}^{M}\) is defined by \(\left(e^{\bm{t}}\right)_{j}:=e^{t_{j}}\). Note that \(F(\bm{t})=f_{\bm{\ell}}(e^{\bm{t}})\). Following the terminology of mathematical economics, we call the \(t_{j}\) the _optimisation variables_, and the \(\tilde{u}_{j}\) (namely the \(u_{j}\) and \(c\)) the _choice variables_. The vector \(\bm{\ell}\) is considered fixed--we neither want to optimise over it nor differentiate with respect to it--which is why we occasionally suppress it from the notation henceforth.

For each \(\tilde{\bm{u}}\), let \(\bm{v}^{*}(\tilde{\bm{u}})\) and \(\bm{t}^{*}(\tilde{\bm{u}})\) be the solutions to the original and transformed optimisation problems respectively. Since the map \(\bm{v}=e^{\bm{t}}\) is one-to-one, it is clear that since \(\bm{v}^{*}(\tilde{\bm{u}})\) exists uniquely, so does \(\bm{t}^{*}(\tilde{\bm{u}})\), and that they are related by \(\bm{v}^{*}(\tilde{\bm{u}})=e^{\bm{t}^{*}(\tilde{\bm{u}})}\). We therefore have the identity

\[f_{\bm{\ell}}(\bm{v}^{*}(\tilde{\bm{u}}))\equiv F(\bm{t}^{*}(\tilde{\bm{u}})).\]

Recalling that \(f_{\bm{\ell}}^{*}(\tilde{\bm{u}}):=f_{\bm{\ell}}(\bm{v}^{*}(\tilde{\bm{u}}))\), we see that

\[\nabla_{\tilde{\bm{u}}}f_{\bm{\ell}}^{*}(\tilde{\bm{u}})\equiv\nabla_{\tilde{ \bm{u}}}F(\bm{t}^{*}(\tilde{\bm{u}})).\] (18)

the derivatives of \(f_{\bm{\ell}}(\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c))\) with respect to \(\bm{u}\) and \(c\) are given by \(\nabla_{\tilde{\bm{u}}}F(\bm{t}^{*}(\tilde{\bm{u}}))\).

Using the method of Lagrange multipliers, there exist real numbers \(\lambda^{*}=\lambda^{*}(\tilde{\bm{u}})\) and \(\mu^{*}=\mu^{*}(\tilde{\bm{u}})\) such that \((\bm{t}^{*},\lambda^{*},\mu^{*})\) is a stationary point (with respect to \(\bm{t},\lambda\) and \(\mu\)) of the Lagrangian function

\[\mathcal{L}(\bm{t},\lambda,\mu;\tilde{\bm{u}}):=F(\bm{t})+\lambda g(\bm{t}; \tilde{\bm{u}})+\mu h(\bm{t}).\]

Let \(F_{\bm{t}}(\cdot)\) and \(h_{\bm{t}}(\cdot)\) denote the gradient vectors of \(F\) and \(h\) respectively, and let \(g_{\bm{t}}(\cdot\ ;\tilde{\bm{u}})\) and \(g_{\tilde{\bm{u}}}(\bm{t};\cdot\ )\) denote the gradient vectors of \(g\) with respect to \(\bm{t}\) only and \(\tilde{\bm{u}}\) only, respectively. Simple calculation yields

\[g_{\bm{t}}(\bm{t};\tilde{\bm{u}})=\bigg{(}\frac{\partial g}{ \partial t_{1}}(\bm{t};\tilde{\bm{u}}),\ldots,\frac{\partial g}{\partial t_{M} }(\bm{t};\tilde{\bm{u}})\bigg{)}=-\bm{u}\quad\text{and}\] (19) \[g_{\tilde{\bm{u}}}(\bm{t};\tilde{\bm{u}})=\bigg{(}\frac{\partial g }{\partial\tilde{u}_{1}}(\bm{t};\tilde{\bm{u}}),\ldots,\frac{\partial g}{ \partial\tilde{u}_{M+1}}(\bm{t};\tilde{\bm{u}})\bigg{)}=\Big{(}1-t_{1}+\log u _{1},\ldots,1-t_{M}+\log u_{M},-1\Big{)}.\]

Then, taking the partial derivatives of \(\mathcal{L}\) with respect to \(\lambda,\mu\) and the \(t_{j}\), we have that \((\bm{t},\lambda,\mu)=(\bm{t}^{*}(\tilde{\bm{u}}),\lambda^{*}(\tilde{\bm{u}}),\mu ^{*}(\tilde{\bm{u}}))\) solves the simultaneous equations

\[F_{\bm{t}}(\bm{t})+\lambda g_{\bm{t}}(\bm{t};\tilde{\bm{u}})+\mu h_{\bm{t}}(\bm{ t})=\bm{0},\] (20)

\[g(\bm{t};\tilde{\bm{u}})=0,\quad\text{and}\] \[h(\bm{t})=0,\]

where the last two equations recover the constraints. Substituting the gradients \(F_{\bm{t}},g_{\bm{t}}\) and \(h_{\bm{t}}\), the first equation reduces to

\[\bm{\ell}\odot e^{\bm{t}}-\lambda\bm{u}+\mu e^{\bm{t}}=\bm{0},\]which implies that for all \(j\in[M]\)

\[e^{t_{j}}=\frac{\lambda u_{j}}{\mu+\ell_{j}}.\] (21)

Substituting this into the constraints \(g=h=0\) yields the following simultaneous equations in \(\lambda\) and \(\mu\)

\[c=\text{kl}(\bm{u}\|e^{\bm{t}})=\sum_{j=1}^{M}u_{j}\log\frac{u_{j}}{e^{t_{j}}}= \sum_{j=1}^{M}u_{j}\log\frac{\mu+\ell_{j}}{\lambda}\quad\text{and}\quad\lambda \sum_{j=1}^{M}\frac{u_{j}}{\mu+\ell_{j}}=1.\]

Substituting the second into the first and rearranging the second, this is equivalent to solving

\[c=\sum_{j=1}^{M}u_{j}\log\left((\mu+\ell_{j})\sum_{k=1}^{M}\frac{u_{k}}{\mu+ \ell_{k}}\right)\quad\text{and}\quad\lambda=\left(\sum_{j=1}^{M}\frac{u_{j}}{ \mu+\ell_{j}}\right)^{-1}.\] (22)

It has already been established in the discussion after Definition 1 that \(f_{\bm{\ell}}(\bm{v})\) attains its maximum on the set \(A_{\bm{u}}:=\{\bm{v}\in\triangle_{M}:\text{kl}(\bm{u}\|\bm{v})\leq c\}\). Therefore \(F(\bm{t})\) also attains its maximum on \(\mathbb{R}^{M}\) and one of the solutions to these simultaneous equations corresponds to this maximum. We first show that there is a single solution to the first equation in the set \((-\infty,-\max_{j}\ell_{j})\), referred to as \(\mu^{*}(\tilde{\bm{u}})\) in the proposition. Second, we show that any other solution corresponds to a smaller total risk, so that \(\mu^{*}(\tilde{\bm{u}})\) corresponds to the maximum total risk and yields \(\bm{v}^{*}(\tilde{\bm{u}})=\text{kl}_{\bm{\ell}}^{-1}(\bm{u}|c)\) when \(\mu^{*}(\tilde{\bm{u}})\) and the associated \(\lambda^{*}(\tilde{\bm{u}})\) are substituted into Equation 21.

For the first step, note that since the \(e^{t_{j}}\) are probabilities, we see from Equation 21 that either \(\mu+\ell_{j}>0\) for all \(j\) (in the case that \(\lambda>0\)), or \(\mu+\ell_{j}<0\) for all \(j\) (in the case that \(\lambda<0\)). Thus any solutions \(\mu\) to the first equation must be in \(\left(-\infty,-\max_{j}\ell_{j}\right)\) or \((-\min_{j}\ell_{j},\infty)\). If \(\mu\in(-\infty,-\max_{j}\ell_{j})\) then the first equation can be written as \(c=\phi_{\bm{\ell}}(\mu)\), with \(\phi_{\bm{\ell}}\) as defined in the statement of the proposition. We now show that \(\phi_{\bm{\ell}}\) is strictly increasing in \(\mu\), and that \(\phi_{\bm{\ell}}(\mu)\to 0\) as \(\mu\to-\infty\) and \(\phi_{\bm{\ell}}(\mu)\to\infty\) as \(\mu\to-\max_{j}\ell_{j}\), so that \(c=\phi_{\bm{\ell}}(\mu)\) does indeed have a single solution in the set \((-\infty,-\max_{j}\ell_{j})\). Straightforward differentiation and algebra shows that

\[\phi^{\prime}_{\bm{\ell}}(\mu) =\sum_{j=1}^{M}\frac{u_{j}}{(\mu+\ell_{j})\sum_{k=1}^{M}\frac{u_{ k^{\prime}}}{\mu+\ell_{k}}}\left(\sum_{k^{\prime}=1}^{M}\frac{u_{k^{\prime}}}{ \mu+\ell_{k^{\prime}}}-(\mu+\ell_{j})\sum_{k^{\prime}=1}^{M}\frac{u_{k^{ \prime}}}{(\mu+\ell_{k^{\prime}})^{2}}\right)\] \[=\frac{\left(\sum_{j=1}^{M}\frac{u_{j}}{\mu+\ell_{j}}\right)^{2} -\sum_{j=1}^{M}\frac{u_{j}}{(\mu+\ell_{j})^{2}}}{\sum_{k=1}^{M}\frac{u_{k}}{ \mu+\ell_{k}}}.\]

Jensen's inequality demonstrates that the numerator is strictly negative, where strictness is due to the assumption that the \(\ell_{j}\) are not all equal. Further, since the denominator is strictly negative (since we are dealing with the case where \(\mu\in(-\infty,-\max_{j}\ell_{j})\)), we see that \(\phi_{\bm{\ell}}\) is strictly increasing for \(\mu\in(-\infty,-\max_{j}\ell_{j})\).5 Turning to the limits, we first show that \(\phi_{\bm{\ell}}(\mu)\to\infty\) as \(\mu\to-\max_{j}\ell_{j}\).

Footnote 5: Incidentally, this argument also shows that there is at most one solution to the first equation in (22) in the range \((-\min_{j}\ell_{j},\infty)\). There indeed exists a unique solution, which corresponds to the minimum total risk, but we do not prove this.

We now determine the left hand limit. Define \(J=\{j\in[M]:\ell_{j}=\max_{k}\ell_{k}\}\), noting that this is a strict subset of \([M]\) since by assumption the \(\ell_{j}\) are not all equal. We then have that for \(\mu\in(-\infty,\max_{j}\ell_{j})\)

\[e^{\phi_{\bm{\ell}}(\mu)} =\left(-\sum_{j=1}^{M}\frac{u_{j}}{\mu+\ell_{j}}\right)\left(\prod_ {k=1}^{M}\big{(}-(\mu+\ell_{k})\big{)}^{u_{k}}\right)\] \[=\left(-\sum_{j\in J}\frac{u_{j}}{\mu+\ell_{j}}-\sum_{j^{\prime} \not\in J}\frac{u_{j^{\prime}}}{\mu+\ell_{j^{\prime}}}\right)\prod_{k\in J} \big{(}-(\mu+\ell_{k})\big{)}^{u_{k}}\prod_{k^{\prime}\not\in J}\big{(}-(\mu+ \ell_{k^{\prime}})\big{)}^{u_{k^{\prime}}}\] \[\geq\left(-\sum_{j\in J}\frac{u_{j}}{\mu+\ell_{j}}\right)\prod_{k \in J}\big{(}-(\mu+\ell_{k})\big{)}^{u_{k}}\prod_{k^{\prime}\not\in J}\big{(} -(\mu+\ell_{k^{\prime}})\big{)}^{u_{k^{\prime}}}\] \[=\frac{\left(\sum_{j\in J}u_{j}\right)\left(\prod_{k^{\prime}\not \in J}\big{(}-(\mu+\ell_{k^{\prime}})\big{)}^{u_{k^{\prime}}}\right)}{\big{(} -(\mu+\max_{j}\ell_{j})\big{)}^{1-\sum_{k\in J}u_{k}}}.\]

The first term in the numerator is a positive constant, independent of \(\mu\). The second term in the numerator tends to a finite positive limit as \(\mu\uparrow-\max_{j}\ell_{j}\). Since \([M]\setminus J\) is non-empty, the power in the denominator is positive and the term in the outer brackets is positive and tends to zero as \(\mu\uparrow-\max_{j}\ell_{j}\). Thus \(e^{\phi_{\bm{\ell}}(\mu)}\to\infty\) as \(\mu\uparrow-\max_{j}\ell_{j}\) and, by the continuity of the logarithm, \(\phi_{\bm{\ell}}(\mu)\) as \(\mu\uparrow-\max_{j}\ell_{j}\).

We now determine \(\lim_{\mu\to-\infty}\phi_{\bm{\ell}}(\mu)\) by sandwiching \(\phi(\mu)\) between two functions that both tend to zero as \(\mu\to-\infty\). First, since \(\ell_{j}\geq 0\) for all \(j\), for \(\mu\in(-\infty,-\max_{j}\ell_{j})\) we have

\[\log\left(-\sum_{j=1}^{M}\frac{u_{j}}{\mu+\ell_{j}}\right)\geq\log\left(-\sum _{j=1}^{M}\frac{u_{j}}{\mu}\right)=-\log(-\mu)=-\sum_{j=1}^{M}u_{j}\log(-\mu),\]

and so

\[\phi_{\bm{\ell}}(\mu)\geq-\sum_{j=1}^{M}u_{j}\log(-\mu)+\sum_{j=1}^{M}u_{j} \log\big{(}-(\mu+\ell_{j})\big{)}=\sum_{j=1}^{M}u_{j}\log\left(1+\frac{\ell_{j }}{\mu}\right)\to 0\quad\text{as}\quad\mu\to-\infty.\]

Similarly,

\[\sum_{j=1}^{M}u_{j}\log\big{(}-(\mu+\ell_{j})\big{)}\leq\sum_{j=1}^{M}u_{j} \log(-\mu)=\log(-\mu),\]

and so

\[\phi_{\bm{\ell}}(\mu)\leq\log\left(\mu\sum_{j=1}^{M}\frac{u_{j}}{\mu+\ell_{j }}\right)=\log\left(\sum_{j=1}^{M}\frac{u_{j}}{1+\frac{\ell_{j}}{\mu}}\right) \to 0\quad\text{as}\quad\mu\to-\infty.\]

This completes the first step, namely showing that there does indeed exist a unique solution \(\mu^{*}(\tilde{\bm{u}})\) in the set \((-\ell_{1},\infty)\) to the first equation in line (22).

We now turn to the second step, namely showing that this solution corresponds to the maximum total risk. Given a value of the Lagrange multiplier \(\mu\), substitution into Equation 21 gives

\[e^{t_{j}}(\mu)=\frac{\frac{u_{j}}{\mu+\ell_{j}}}{\sum_{k=1}^{M}\frac{u_{k}}{\mu +\ell_{k}}}\]

and therefore total risk

\[R(\mu)=\frac{\sum_{j=1}^{M}\frac{u_{j}\ell_{j}}{\mu+\ell_{j}}}{\sum_{k=1}^{M} \frac{u_{k}}{\mu+\ell_{k}}}.\]

To prove that the solution \(\mu^{*}(\tilde{\bm{u}})\in(-\infty,-\max_{j}\ell_{j})\) is the solution to the first equation in line (22) that maximises \(R\), it suffices to show that \(R(\mu)\to\sum_{j=1}^{M}u_{j}\ell_{j}\) as \(|\mu|\to\infty\) and \(R^{\prime}(\mu)\geq 0\) for all \(\mu\in(-\infty,-\max_{j}\ell_{j})\cup(-\min_{j}\ell_{j},\infty)\), so that

\[\inf_{\mu\in(-\infty,-\max_{j}\ell_{j})}R(\mu)\geq\sup_{\mu\in(-\min_{j}\ell_ {j},\infty)}R(\mu).\]This suffices as we have already proved that \(\mu^{*}(\tilde{\bm{u}})\) is the only solution in \((-\infty,-\max_{j}\ell_{j})\) to the first equation in line (22), and that no solutions exists in the set \([-\max_{j}\ell_{j},-\min_{j}\ell_{j}]\).

The limit can be easily evaluated by first rewriting \(R(\mu)\) and then taking the limit as \(|\mu|\to\infty\) as follows

\[R(\mu)=\frac{\sum_{j=1}^{M}\frac{u_{j}\ell_{j}}{1+\frac{\ell_{j}}{\mu}}}{\sum_ {k=1}^{M}\frac{u_{k}}{1+\frac{\ell_{k}}{\mu}}}\to\frac{\sum_{j=1}^{M}u_{j} \ell_{j}}{\sum_{k=1}^{M}u_{k}}=\sum_{j=1}^{M}u_{j}\ell_{j}.\]

To show that \(R^{\prime}(\mu)\geq 0\), let \(\ell_{(j)}\) denote the \(j\)'th smallest component of \(\bm{\ell}\) (breaking ties arbitrarily), so that \(\ell_{(1)}\leq\cdots\leq\ell_{(M)}\), and use the quotient rule to see that

\[R^{\prime}(\mu)\geq 0 \iff\frac{\left(\sum_{k=1}^{M}\frac{u_{k}}{\mu+\ell_{k}}\right) \left(\sum_{j=1}^{M}\frac{-u_{j}\ell_{j}}{(\mu+\ell_{j})^{2}}\right)-\left( \sum_{j=1}^{M}\frac{u_{j}\ell_{j}}{\mu+\ell_{j}}\right)\left(\sum_{k=1}^{M} \frac{-u_{k}}{(\mu+\ell_{k})^{2}}\right)}{\left(\sum_{p=1}^{M}\frac{u_{p}}{ \mu+\ell_{p}}\right)^{2}}\geq 0\] \[\iff\sum_{j=1}^{M}\sum_{k=1}^{M}\frac{u_{j}u_{k}\ell_{j}}{(\mu+ \ell_{j})(\mu+\ell_{k})}\left(\frac{1}{\mu+\ell_{k}}-\frac{1}{\mu+\ell_{j}} \right)\geq 0\] \[\iff\sum_{\begin{subarray}{c}j,k\in[M]\\ k<j\end{subarray}}\frac{u_{j}u_{k}\ell_{(j)}}{(\mu+\ell_{(j)})(\mu+\ell_{(k)}) }\left(\frac{1}{\mu+\ell_{(k)}}-\frac{1}{\mu+\ell_{(j)}}\right)\] \[\quad+\sum_{\begin{subarray}{c}j,k\in[M]\\ k>j\end{subarray}}\frac{u_{j}u_{k}\ell_{(j)}}{(\mu+\ell_{(j)})(\mu+\ell_{(k)}) }\left(\frac{1}{\mu+\ell_{(k)}}-\frac{1}{\mu+\ell_{(j)}}\right)\geq 0,\]

where in the final line we have dropped the summands where \(k=j\) since they equal zero as the terms in the bracket cancel. This final inequality holds since the first sum can be bounded below by the negative of the second sum as follows

\[\sum_{\begin{subarray}{c}j,k\in[M]\\ k<j\end{subarray}}\frac{u_{j}u_{k}\ell_{(j)}}{(\mu+\ell_{(j)})(\mu+\ell_{(k)}) }\left(\frac{1}{\mu+\ell_{(k)}}-\frac{1}{\mu+\ell_{(j)}}\right)\] \[\geq\sum_{\begin{subarray}{c}j,k\in[M]\\ k<j\end{subarray}}\frac{u_{j}u_{k}\ell_{(k)}}{(\mu+\ell_{(j)})(\mu+\ell_{(k)}) }\left(\frac{1}{\mu+\ell_{(k)}}-\frac{1}{\mu+\ell_{(j)}}\right)\quad\text{(since $\ell_{(k)}\leq \ell_{(j)}$ for $k<j$)}\] \[=\sum_{\begin{subarray}{c}j,k\in[M]\\ k>j\end{subarray}}\frac{u_{k}u_{j}\ell_{(j)}}{(\mu+\ell_{(k)})(\mu+\ell_{(j)}) }\left(\frac{1}{\mu+\ell_{(j)}}-\frac{1}{\mu+\ell_{(k)}}\right)\quad\text{( swapping dummy variables $j,k$)}.\]

We now turn to finding the partial derivatives of \(F(\bm{t}^{*}(\tilde{\bm{u}}))\) with respect the \(\tilde{u}_{j}\), which in turn will allow us to find the partial derivatives of \(\text{kl}_{\bm{t}}^{-1}(\bm{u}|c)\). Let \(\nabla_{\tilde{\bm{u}}}\) denote the gradient operator with respect to \(\tilde{\bm{u}}\). Then the quantity we are after is \(\nabla_{\tilde{\bm{u}}}F(\bm{t}^{*}(\tilde{\bm{u}}))\in\mathbb{R}^{M+1}\), the \(j\)'th component of which is

\[\left(\nabla_{\tilde{\bm{u}}}F(\bm{t}^{*}(\tilde{\bm{u}}))\right)_{j}=\sum_{k= 1}^{M+1}\frac{\partial F}{\partial t_{k}}(\bm{t}^{*}(\tilde{\bm{u}}))\frac{ \partial t_{k}^{*}}{\partial\tilde{u}_{j}}(\tilde{\bm{u}})=F_{\bm{t}}(\bm{t}^ {*}(\tilde{\bm{u}}))\cdot\frac{\partial\bm{t}^{*}}{\partial\tilde{u}_{j}}( \tilde{\bm{u}})\in\mathbb{R}.\]

Thus the full gradient vector is

\[\nabla_{\tilde{\bm{u}}}F(\bm{t}^{*}(\tilde{\bm{u}}))=F_{\bm{t}}(\bm{t}^{*}( \tilde{\bm{u}}))\nabla_{\tilde{\bm{u}}}\bm{t}^{*}(\tilde{\bm{u}}),\] (23)

where \(\nabla_{\tilde{\bm{u}}}\bm{t}^{*}(\tilde{\bm{u}})\) is the \(M\times(M+1)\) matrix given by

\[\left(\nabla_{\tilde{\bm{u}}}\bm{t}^{*}(\tilde{\bm{u}})\right)_{j,k}=\frac{ \partial t_{k}^{*}}{\partial\tilde{u}_{j}}(\tilde{\bm{u}}).\]

Finding an expression for this matrix is difficult. Fortunately we can avoid needing to by using a trick from mathematical economics referred to as the envelope theorem, as we now show.

[MISSING_PAGE_FAIL:28]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our Theorem 1 is a PAC-Bayes bound of the form we claim to prove in the abstract. Further, Proposition 2 is a recipe for the differentiable training objective we claim to derive in the abstract.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss these in Section 7.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The proof of our main result Theorem 1 is found in Section 5. Proposition 1 is proved in Appendix C.4. Our second main result, Theorem 2 is proved in Appendix C.5. Proposition 2 is proved directly after its statement. Proposition 3 is proved in Appendix C.1. Proposition 4 is proved in Section 5. Lemma 1 does not require proof as it is the classic change of measure inequality (Csiszar, 1975, Donsker and Varadhan, 1975). Lemma 2 is proved in Appendix C.2. Finally, Corollary 1 and Lemma 3 are proved directly after their statement.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Appendix A is devoted to a detailed explanation of how to implement our training regime, and Appendix B gives the details of the specific experiments we run.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: URL is provided.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: This is outlined in Appendix B.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper concerns bounds, meaning the results themselves are confidence regions.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: The compute resources required are not stated as they are negligible.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: There are no data-related concerns or societal impact concerns.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The introduction describes a class of concrete real-world problems for which our method may have positive impact.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper uses only the MNIST dataset.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: None required.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: None required.