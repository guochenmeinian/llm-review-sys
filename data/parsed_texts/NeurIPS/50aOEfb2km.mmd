# Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing

Siyi Chen\({}^{1*}\) Huijie Zhang\({}^{1*}\) Minzhe Guo\({}^{1}\) Yifu Lu\({}^{1}\) Peng Wang\({}^{1}\) Qing Qu\({}^{1}\)

\({}^{1}\)University of Michigan

{siyich,huijiezh,vincegma,yifulu,pengwa,qingqu}@umich.edu

###### Abstract

Recently, diffusion models have emerged as a powerful class of generative models. Despite their success, there is still limited understanding of their semantic spaces. This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way. In this work, we improve the understanding of their semantic spaces from intriguing observations: among a certain range of noise levels, (1) the learned posterior mean predictor (PMP) in the diffusion model is locally linear, and (2) the singular vectors of its Jacobian lie in low-dimensional semantic subspaces. We provide a solid theoretical basis to justify the linearity and low-rankness in the PMP. These insights allow us to propose an unsupervised, single-step, training-free **LO**w-rank **CO**ntrollable image editing (LOCO Edit) method for precise local editing in diffusion models. LOCO Edit identified editing directions with nice properties: homogeneity, transferability, composability, and linearity. These properties of LOCO Edit benefit greatly from the low-dimensional semantic subspace. Our method can further be extended to unsupervised or text-supervised editing in various text-to-image diffusion models (T-LOCO Edit). Finally, extensive empirical experiments demonstrate the effectiveness and efficiency of LOCO Edit. The code and the arXiv version can be found on the project website.1

Footnote 1: https://chicychen.github.io/LOCO

## 1 Introduction

Recently, diffusion models have emerged as a powerful new family of deep generative models with remarkable performance in many applications such as image generation across various domains [1, 2, 3, 4, 5, 6], audio synthesis [7, 8], solving inverse problem [9, 10, 11, 12, 13, 14], and video generation [15, 16, 17]. For example, recent advances in AI-based image generation, revolutionized by diffusion models such as Dalle-2 [18], Imagen [19], and stable diffusion [4], have taken the world of "AI Art generation", enabling the generation of images directly from descriptive text inputs. These models corrupt images by adding noise through multiple steps of forward process and then generate samples by progressive denoising through multiple steps of the reverse generative process.

Although modern diffusion models are capable of generating photorealistic images from text prompts, manipulating the generated content by diffusion models in practice has remaining challenges. Unlike generative adversarial networks [20], the understanding of semantic spaces in diffusion models is still limited. Thus, achieving disentangled and localized control over content generation by direct manipulation of the semantic spaces remains a difficult task for diffusion models. Although effective, some existing editing methods in diffusion models often demand additional training procedures and are limited to global control of content generation [21, 22, 23]. Some methods are training-free or localized but are still based upon heuristics, lacking clear mathematical interpretations, or for text-supervised editing only [24, 25, 26, 27, 28]. Others provide analysis in diffusion models [29, 30, 31, 32, 33], but also have difficulty in local edits such as hair color.

In this study, we address the above problem by studying the low-rank semantic subspaces in diffusion models and proposing the LOw-rank COntrollable edit (LOCO Edit) approach. LOCO is the first local editing method that is single-step, training-free, requiring no text supervision, and having other intriguing properties (see Figure 1 for an illustration). Our method is highly intuitive and theoretically grounded, originating from a simple while intriguing observation in the learned _posterior mean predictor_ (PMP) in diffusion models: for a large portion of denoising time steps,

_The PMP is a locally linear mapping between the noise image and the estimated clean image, and the singular vectors of its Jacobian reside within low-dimensional subspaces._

The empirical evidence in Figure 2 consistently shows that this phenomenon occurs when training diffusion models using different network architectures on a range of real-world image datasets. Theoretically, we validated this observation by assuming a mixture of low-rank Gaussian distributions for the data. We then prove the local linearity of the PMP, the low-rank nature of its Jacobian, and that the singular vectors of the Jacobian span the low-dimensional subspaces.

By utilizing the linearity of the PMP, we can edit within the singular vector subspace of its Jacobian to achieve linear control of the image content with no label or text supervision. The editing direction can be efficiently computed using the generalized power method (GPM) [30, 34]. Furthermore, we can manipulate specific regions of interest in the image along a disentangled direction through efficient nullspace projection, taking advantage of the low-rank properties of the Jacobian.

Benefits of LOCO Edit.Compared to existing editing methods (e.g., [29, 35, 23, 24]) based on diffusion models, the proposed LOCO Edit offers several benefits that we highlight below:

* **Precise, single-step, training-free, and unsupervised editing.** LOCO enables precise _localized_ editing (Figure 0(a)) in a single timestep without any training. Further, it requires no text supervision based on CLIP [36], thus integrating no intrinsic biases or flaws from CLIP [37]. LOCO is applicable to various diffusion models and datasets (Figure 5).
* **Linear, transferable, and composable editing directions.** The identified editing direction is linear, meaning that changes along this direction produce proportional changes in a semantic feature in the image space (Figure 0(d)). These editing directions are homogeneous and can be transferred across various images and noise levels (Figure 0(b)). Moreover, combining disentangled editing directions leads to simultaneous semantic changes in the respective region, while maintaining consistency in other areas (Figure 0(c)).
* **An intuitive and theoretically grounded approach.** Unlike previous works, by leveraging the local linearity of the PMP and the low-rankness of its Jacobian, our method is highly interpretable. The identified properties are well supported by both our empirical observation (Figure 2) and theoretical justifications in Section 4.

Moreover, LOCO Edit is generalizable to T-LOCO Edit for T2I diffusion models including DeepFloyd IF [19], Stable Diffusion [4], and Latent Consistency Models [38], with or without text supervision (Figure 4). A more detailed discussion on the relationship with prior arts can be found in Appendix B.

Figure 1: **LOCO Edit. (a) The proposed method can perform precise localized editing in the region of interest. The editing direction is (b) homogeneous, (c) composable, and (d) linear.**

Notations.Throughout the paper, we use \(\mathcal{X}_{t}\subseteq\mathbb{R}^{d}\) to denote the noise-corrupted image space at the time-step \(t\in[0,1]\). In particular, \(\mathcal{X}_{0}\) denotes the clean image space with distribution \(p_{\text{data}}(\bm{x})\), and \(\bm{x}_{0}\in\mathcal{X}_{0}\) denote an image. \(\mathcal{X}_{0,t}\) denote the posterior mean space at time-step \(t\in(0,1]\). Here, \(\mathbb{S}^{d-1}\) denotes a unit hypersphere in \(\mathbb{R}^{d}\), and \(\mathrm{St}(d,r):=\{\bm{Z}\in\mathbb{R}^{d\times r}\mid\bm{Z}^{\top}\bm{Z}=\bm {I}_{r}\}\) denotes the Stiefel manifold. \(\widehat{\text{rank}}(\bm{A})\) denotes the numerical rank of \(\bm{A}\). \(\mathbb{E}_{\bm{x}_{0}\sim p_{\text{data}}(\bm{x})}[\bm{x}_{0}|\bm{x}_{t}]\) denotes the posterior mean and is written as \(\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\). \(\mathrm{range}(\mathrm{A})\) denotes the span of the columns of \(\bm{A}\). \(\mathrm{null}(\bm{A})\) denotes the set of solutions to \(\bm{A}\bm{x}=0\). \(\mathrm{proj}_{\mathrm{null}\,\bm{A}}(\bm{x})\) denotes the projection of \(\bm{x}\) onto \(\mathrm{null}(\bm{A})\).

## 2 Preliminaries on Diffusion Models

In this section, we start by reviewing the basics of diffusion models [1; 2; 39], followed by several key techniques that will be used in our approach, such as Denoising Diffusion Implicit Models (DDIM) [3] and its inversion [40], T2I diffusion model, and classifier-free guidance [41].

Basics of Diffusion Models.In general, diffusion models consist of two processes:

* _The forward diffusion process._ The forward process progressively perturbs the original data \(\bm{x}_{0}\) to a noisy sample \(\bm{x}_{t}\) for \(t\in[0,1]\) with the Gaussian noise. As in [1], this can be characterized by a conditional Gaussian distribution \(p_{t}(\bm{x}_{t}|\bm{x}_{0})=\mathcal{N}(\bm{x}_{t};\sqrt{\alpha_{t}}\bm{x}_{0 },(1-\alpha_{t})\mathbf{I}_{d})\). Particularly, parameters \(\{\alpha_{t}\}_{t=0}^{1}\) sastify: (_i_) \(\alpha_{0}=1\), and thus \(p_{0}=p_{\text{data}}\), and (_ii_) \(\alpha_{1}=0\), and thus \(p_{1}=\mathcal{N}(\bm{0},\mathbf{I}_{d})\).
* _The reverse sampling process._ To generate a new sample, previous works [1; 3; 42; 43] have proposed various methods to approximate the reverse process of diffusion models. Typically, these methods involve estimating the noise \(\bm{\epsilon}_{t}\) and removing the estimated noise from \(\bm{x}_{t}\) recursively to obtain an estimate of \(\bm{x}_{0}\). Specifically, the sampling step from \(\bm{x}_{t}\) to \(\bm{x}_{t-\Delta t}\) with a small \(\Delta t>0\) can be described as: \[\bm{x}_{t-\Delta t}=\sqrt{\alpha_{t-\Delta t}}\left(\frac{\bm{x}_{t}-\sqrt{1- \alpha_{t}}\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t)}{\sqrt{\alpha_{t}}} \right)+\sqrt{1-\alpha_{t-\Delta t}}\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t),\] (1) where \(\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t)\) is parameterized by a neural network and trained to predict the noise at time \(t\).

Denoiser and Posterior Mean Predictor (PMP).According to [1], the denoiser \(\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t)\) is optimized by solving the following problem:

\[\min_{\bm{\theta}}\ell(\bm{\theta}):=\mathbb{E}_{t\sim[0,1],\bm{x}_{t}\sim p _{t}(\bm{x}_{t}|\bm{x}_{0}),\bm{\epsilon}\sim\mathcal{N}(\bm{0},\mathbf{I})} \left[\|\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t)-\bm{\epsilon}\|_{2}^{2} \right],\]

where \(\bm{\theta}\) denotes the network parameters of the denoiser. Once \(\bm{\epsilon}_{\bm{\theta}}\) is well trained, recent studies [44; 45] show that the posterior mean \(\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\), i.e., predicted clean image at time \(t\), can be estimated as follows:

\[\hat{\bm{x}}_{0,t}=\bm{f}_{\bm{\theta},t}(\bm{x}_{t};t)\coloneqq\frac{\bm{x} _{t}-\sqrt{1-\alpha_{t}}\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t)}{\sqrt{ \alpha_{t}}},\] (2)

Here, \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t};t)\) denotes the _posterior mean predictor_ (PMP) [45; 44], and \(\hat{\bm{x}}_{0,t}\in\mathcal{X}_{0,t}\) denotes the estimated posterior mean output from PMP given \(\bm{x}_{t}\) and \(t\) as the input. For simplicity, we denote \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t};t)\) as \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\).

DDIM and DDIM Inversion.Given a noisy sample \(\bm{x}_{t}\) at time \(t\), DDIM [3] can generate clean images by multiple denoising steps. Given a clean sample \(\bm{x}_{0}\), DDIM inversion [3] can generate a noisy \(\bm{x}_{t}\) at time \(t\) by adding multiple steps of noise following the reversed trajectory of DDIM. DDIM inversion has been widely in image editing methods [40; 46; 29; 35; 47; 26] to obtain \(\bm{x}_{t}\) given the original \(\bm{x}_{0}\) and then performing editing starting from \(\bm{x}_{t}\). In our work, after getting \(\bm{x}_{t}\) given \(\bm{x}_{0}\) via DDIM inversion, we edit \(\bm{x}_{t}\) to \(\bm{x}_{t}^{\prime}\) only at the single time step \(t\) with the help of PMP, and then utilize DDIM to generate the edited image \(\bm{x}_{0}^{\prime}\).

For ease of exposition, for any \(t_{1}\) and \(t_{2}\) with \(t_{2}>t_{1}\), we denote DDIM operator and its inversion as \(\bm{x}_{t_{1}}=\texttt{DDIM}(\bm{x}_{t_{2}},t_{1})\quad\text{and}\quad\bm{x}_{t _{2}}=\texttt{DDIM-Inv}(\bm{x}_{t_{1}},t_{2})\).

Text-to-image (T2I) Diffusion Models & Classifier-Free Guidance.So far, our discussion has only focused on unconditional diffusion models. Moreover, our approach can be generalized from unconditional diffusion models to T2I diffusion models [38; 4; 48; 19], where the latter enables controllable image generation \(\bm{x}_{0}\) guided by a text prompt \(c\). In more detail, when training T2I diffusion models, we optimize a conditional denoising function \(\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t,c)\). For sampling, we employ a technique called _classifier-free guidance_[41], which substitutes the unconditional denoiser \(\bm{\epsilon_{\theta}}(\bm{x}_{t},t)\) in Equation (1) with its conditional counterpart \(\bm{\bar{\epsilon}_{\theta}}(\bm{x}_{t},t,c)\) that can be described as follows:

\[\bm{\bar{\epsilon}_{\theta}}(\bm{x}_{t},t,c)=\bm{\epsilon_{\theta}}(\bm{x}_{t}, t,\varnothing)+\eta(\bm{\epsilon_{\theta}}(\bm{x}_{t},t,c)-\bm{\epsilon_{ \theta}}(\bm{x}_{t},t,\varnothing)).\] (3)

Here, \(\varnothing\) denotes the empty prompt and \(\eta>0\) denotes the strength for the classifier-free guidance.

## 3 Exploring Linearity & Low-Dimensionality for Image Editing

In this section, we formally introduce the identified low-rank subspace in diffusion models and the proposed LOCO Edit method with the underlying intuitions. In Section 3.1, we present the benign properties in PMP that our method utilizes. Followed by this, in Section 3.3 we provide a detailed description of our method.

### Local Linearity and Intrinsic Low-Dimensionality in PMP

First, let us delve into the key intuitions behind the proposed LOCO Edit method, which lie in the benign properties of the PMP \(f_{\bm{\theta},t}(\bm{x}_{t})\). At one given timestep \(t\in[0,1]\), let us consider the first-order Taylor expansion of \(\bm{f_{\bm{\theta},t}}(\bm{x}_{t}+\lambda\Delta\bm{x})\) at the point \(\bm{x}_{t}\):

\[\boxed{\bm{l_{\theta}}(\bm{x}_{t};\lambda\Delta\bm{x})\ :=\ \bm{f_{\bm{ \theta},t}}(\bm{x}_{t})+\lambda\bm{J_{\theta,t}}(\bm{x}_{t})\cdot\Delta\bm{x},}\] (4)

where \(\Delta\bm{x}\in\mathbb{S}^{d-1}\) is a perturbation direction with unit length, \(\lambda\in\mathbb{R}\) is the perturbation strength, and \(\bm{J_{\theta,t}}(\bm{x}_{t})=\nabla_{\bm{x}_{t}}\bm{f_{\bm{\theta},t}}(\bm{ x}_{t})\) is the Jacobian of \(\bm{f_{\bm{\theta},t}}(\bm{x}_{t})\). Interestingly, we discovered that within a certain range of noise levels, the learned PMP \(\bm{f_{\bm{\theta},t}}\) exhibits local linearity, and the singular subspace of its Jacobian \(\bm{J_{\theta,t}}\) is low rank. Notably, these properties are universal across various network architectures (e.g., UNet and Transformers) and datasets.

We measure the low-rankness with rank ratio and the local linearity with norm ratio and cosine similarity. Specifically, (_i_) _rank ratio_ is the ratio of \(\widehat{\operatorname{rank}}(\bm{J_{\theta,t}}(\bm{x}_{t}))\) and the ambient dimension \(d\); (_ii_) _norm ratio_ is the ratio of \(\|\bm{f_{\bm{\theta},t}}(\bm{x}_{t}+\lambda\Delta\bm{x})\|_{2}\) and \(\|\bm{l_{\theta}}(\bm{x}_{t};\lambda\Delta\bm{x})\|_{2}\); (_iii_) _cosine similarity_ is between \(\bm{f_{\bm{\theta},t}}(\bm{x}_{t}+\lambda\Delta\bm{x})\) and \(\bm{l_{\theta}}(\bm{x}_{t};\lambda\Delta\bm{x})\). The detailed experiment settings are provided in Appendix D.1, and results are illustrated in Figure 2, from which we observe:

* **Low-rankness of the Jacobian \(\bm{J_{\theta,t}}(\bm{x}_{t})\).** As shown in Figure 2(a), the _rank ratio_ for \(t\in[0,1]\)_consistently_ displays a U-shaped pattern across various network architectures and datasets: (_i_) it is close to \(1\) near either the pure noise \(t=1\) or the clean image \(t=0\), (_ii_) \(\bm{J_{\theta,t}}(\bm{x}_{t})\) is low-rank (i.e., rank ratio less than \(10^{-1}\)) for all diffusion models within the range \(t\in[0.2,0.7]\), (_iii_) it achieves the lowest value around mid-to-late timestep, slightly differs depending on architecture and dataset.
* **Local linearity of the PMP \(\bm{f_{\bm{\theta},t}}(\bm{x}_{t})\).** Moreover, the mapping \(\bm{f_{\bm{\theta},t}}(\bm{x}_{t})\) exhibits strong linearity across a large portion of the timesteps; see Figure 2(b) and Figure 10. Specifically, in Figure 2(b), we evaluate the linearity of \(\bm{f_{\bm{\theta},t}}(\bm{x}_{t})\) at \(t=0.7\) where the rank ratio is close to the lowest value. We can see that \(\bm{f_{\bm{\theta},t}}(\bm{x}_{t}+\lambda\Delta\bm{x})\approx\bm{l_{\theta}}( \bm{x}_{t};\lambda\Delta\bm{x})\) even when \(\lambda=40\), which is consistently true among different architectures trained on different datasets.

Figure 2: **Low-rankness of the Jacobian \(\bm{J_{\theta,t}}(\bm{x}_{t})\) and Local linearity of the PMP \(\bm{f_{\bm{\theta},t}}(\bm{x}_{t})\).** We evaluated DDPM (U-Net [49]) on CIFAR-10 dataset [50], U-ViT [51] (Transformer) on CelebA [52], ImageNet [53] datasets and DeepFloy IF [19] trained on LAION-5B [54] dataset. (a) The rank ratio of \(\bm{J_{\theta,t}}(\bm{x}_{t})\) against timestep \(t\). (b) The norm ratio (Top) and cosine similarity (Bottom) between \(\bm{f_{\theta,t}}(\bm{x}_{t}+\lambda\Delta\bm{x})\) and \(\bm{l_{\theta}}(\bm{x}_{t};\lambda\Delta\bm{x})\) against step size \(\lambda\) at timestep \(t=0.7\).

In addition to comprehensive experimental studies, we will also demonstrate in Section 4 that both properties can be theoretically justified.

### Key Intuitions for Our Image Editing Method

The two benign properties offer valuable insights for image editing with precise control. Here, we first present the high-level intuitions behind our method, with further details postponed to Section 3.3. Specifically, for any given time-step \(t\in[0,1]\), let us denote the compact singular value decomposition (SVD) of the Jacobian \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\) as

\[\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\ =\ \bm{U}\bm{\Sigma}\bm{V}^{\top}\ =\ \sum_{i=1}^{r} \sigma_{i}\bm{u}_{i}\bm{v}_{i}^{\top},\] (5)

where \(r\) is the rank of \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\), \(\bm{U}=[\bm{u}_{1}\quad\cdots\quad\bm{u}_{r}]\in\mathrm{St}(d,r)\) and \(\bm{V}=[\bm{v}_{1}\quad\cdots\quad\bm{v}_{r}]\in\mathrm{St}(d,r)\) denote the left and right singular vectors, and \(\bm{\Sigma}=\mathrm{diag}(\sigma_{1},\cdots,\sigma_{r})\) denote the singular values. We write \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})=\bm{J}_{\bm{\theta},t}\) in short for a specific \(\bm{x}_{t}\), and denote \(\mathrm{range}(\bm{J}_{\bm{\theta},t}^{\top})=\mathrm{span}(\bm{V})\) and \(\mathrm{null}(\bm{J}_{\bm{\theta},t})=\{\bm{w}\ |\ \bm{J}_{\bm{\theta},t}\bm{w}=0\}\).

* **Local linearity of PMP for one-step, training-free, and supervision-free editing.** Given the PMP \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) is locally linear at the \(t\)-th timestep, if we perturb \(\bm{x}_{t}\) by \(\Delta\bm{x}=\lambda\bm{v}_{i}\), using one right singular vector \(\bm{v}_{i}\) of \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\) as an example editing direction, then by orthogonality \[\bm{f}_{\bm{\theta},t}(\bm{x}_{t}+\lambda\bm{v}_{i})\ \approx\ \bm{f}_{\bm{\theta},t}(\bm{x}_{t})+\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\bm{v}_ {i}\ =\ \bm{f}_{\bm{\theta},t}(\bm{x}_{t})+\lambda\sigma_{i}\bm{u}_{i}\ =\ \bm{\hat{x}}_{0,t}+\rho_{i}\bm{u}_{i}.\] (6) This implies we can achieve _one-step editing_ along the semantic direction \(\bm{u}_{i}\). Notably, the method is training-free and supervision-free since \(\bm{v}_{i}\) can be simply found via the SVD of \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\).
* **Local linearity of PMP for linear, homogeneous, and composable image editing.** (_i_) First, the editing direction \(\bm{v}=\bm{v}_{i}\) is _linear_, where any linear \(\lambda\in\mathbb{R}\) change along \(\bm{v}_{i}\) results in a linear change \(\rho_{i}=\lambda\sigma_{i}\) along \(\bm{u}_{i}\) for the edited image. (_ii_) Second, the editing direction \(\bm{v}=\bm{v}_{i}\) is _homogeneous_ due to its independence of \(\bm{\hat{x}}_{0,t}\), where it could be applied on any images from the same data distribution and results in the same semantic editing. (_iii_) Third, editing directions are _composable_. Any linearly combined editing direction \(\bm{v}=\sum_{i\in\mathcal{I}}\lambda_{i}\bm{v}_{i}\in\mathrm{range}\left(\bm{ J}_{\bm{\theta},t}^{\top}\right)\) is a valid editing direction which would result in a composable change \(\sum_{i\in\mathcal{I}}\rho_{i}\bm{u}_{i}\) in the edited image. On the contrary, \(\bm{w}\in\mathrm{null}\left(\bm{J}_{\bm{\theta},t}\right)\) results in no editing since \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t}+\lambda\bm{w})\approx\bm{f}_{\bm{\theta},t} (\bm{x}_{t})\).
* **Low-rankness of Jacobian for localized and efficient editing.**\(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\) is for the entire predicted clean image, thus \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\) finds editing directions in the entire image. Denote \(\bm{\tilde{J}}_{\bm{\theta},t}\) the Jacobian only for a certain region of interest (ROI), and \(\bm{\tilde{J}}_{\bm{\theta},t}\) the Jacobian for regions outside ROI. Similarly, \(\bm{v}\in\mathrm{range}\left(\bm{\tilde{J}}_{\bm{\theta},t}^{\top}\right)\) can edit mainly regions within the ROI, and \(\mathrm{null}\left(\bm{\tilde{J}}_{\bm{\theta},t}^{\top}\right)\) contain directions that do not edit regions outside of ROI. Further projection of \(\bm{v}\) onto \(\mathrm{null}\left(\bm{\tilde{J}}_{\bm{\theta},t}^{\top}\right)\) can result in a more localized editing direction for ROI. To perform such nullspace projection, computing the full SVD can be very expensive. But we can highly reduce the computation by the low-rank estimation of Jacobians with rank \(r^{\prime}\ll d\). The estimation is efficient yet effective with \(t\in[0.5,0.7]\) when the rank of the Jacobian achieves the lowest value.

### Low-rank Controllable Image Editing Method with Nullspace Projection

In this subsection, we provide a detailed introduction to LOCO Edit, expanding on the discussion in Section 3.1. We first introduce the supervision-free LOCO Edit, where we further enable localized image editing through nullspace projection with masks. Second, we generalize to T-LOCO Edit for T2I diffusion models w/wo text-supervision to define the semantic editing directions.

LOCO Edit.We first introduce the general pipeline of LOCO Edit. As illustrated in Figure 3, given an original image \(\bm{x}_{0}\), we first use \(\bm{x}_{t}=\texttt{DDIM-Inv}(\bm{x}_{0},t)\) to generate a noisy image \(\bm{x}_{t}\). In particular, we choose \(t\in[0.5,0.7]\) so that the PMP \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) is locally linear and its Jacobian \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\) is close to its lowest rank. From Section 3.1, we know that we can edit the image by changing \(\bm{x}_{t}^{\prime}=\bm{x}_{t}+\lambda\bm{v}_{p}\), where \(\bm{v}_{p}\) is the identified editing direction. After editing \(\bm{x}_{t}\) to \(\bm{x}_{t}^{\prime}\), we use \(\bm{x}_{0}^{\prime}=\texttt{DDIM}(\bm{x}_{t}^{\prime},0)\) to generate the edited image.

In many practical applications, we often need to edit only specific _local_ regions of an image while leaving the rest unchanged. As discussed in Section 3.2, we can achieve this task by finding a precise local editing direction with localized Jacobians and nullspace projection. Overall, the complete method is in Algorithm 1. We describe the key details as follows.

* **Finding localized Jacobians via masking.** To enable local editing, we use a mask \(\Omega\) (i.e., an index set of pixels) to select the region of interest,2 with \(\mathcal{P}_{\Omega}(\cdot)\) denoting the projection onto the index set \(\Omega\). For picking a local editing direction, we calculate the Jacobian of \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) restricted to the region of interest, \(\bm{\bar{J}}_{\bm{\theta},t}=\nabla_{\bm{x}_{t}}P_{\Omega}(\bm{f}_{\bm{\theta},t}(\bm{x}_{t}))=\bm{\bar{U}}\bm{\bar{\Sigma}}\bm{\bar{V}}^{\top}\), and select the localized editing direction \(\bm{v}\) from the top-\(r\) singular vectors of \(\bm{\bar{V}}\) (e.g., \(\bm{v}=\bm{\bar{V}}[:,k]\in\operatorname{range}\bm{\bar{J}}_{\bm{\theta},t}^{\top}\) for some index \(k\in[r]\)). In practice, a top-\(r\) rank estimation for \(\bm{\bar{V}}\) is calculated through the generalized power method (GPM) Algorithm 2 with \(r=5\) to improve efficiency. Footnote 2: For datasets that have predefined masks, we can use them directly. For other datasets that lack predefined masks as well as generate images, we can utilize Segment Anything (SAM) to generate masks [55].
* **Better semantic disentanglement via nullspace projection.** However, the projection \(\mathcal{P}_{\Omega}(\cdot)\) introduces extra _nonlinearity_ into the mapping \(P_{\Omega}(\bm{f}_{\bm{\theta},t}(\bm{x}_{t}))\), causing the identified direction to have semantic entanglements with the area \(\Omega^{C}\) outside of the mask. Here, \(\Omega^{C}\) denotes the complimentary set of \(\Omega\). To address this issue, we can use the nullspace projection method [56, 57]. Specifically, given \(\bm{\bar{J}}_{\bm{\theta},t}=\nabla_{\bm{x}_{t}}P_{\Omega^{C}}(\bm{f}_{\bm{ \theta},t}(\bm{x}_{t}))=\bm{\bar{U}}\bm{\bar{\Sigma}}\bm{\bar{V}}^{\top}\), nullspace projection projects \(\bm{v}\) onto \(\operatorname{null}\left(\bm{\bar{J}}_{\bm{\theta},t}^{\top}\right)\). The projection can be computed as \(\bm{v}_{p}=\operatorname{proj}_{\operatorname{null}\left(\bm{J}_{\bm{\theta},t}\right)}(\bm{v})=(\bm{I}-\bm{\bar{V}}\bm{\bar{V}}^{\top})\bm{v}\) so that the modified \(\bm{v}_{p}\) does not change the image in \(\Omega^{C}\). In practice, we calculate a top-\(r^{\prime}\) rank estimation for \(\bm{\bar{V}}\) through the generalized power method (GPM) Algorithm 2 with \(r^{\prime}=5\).

T-LOCO Edit.The unsupervised edit method can be seamlessly applied to T2I diffusion models with classifier-free guidance (3) (Algorithm 3). Besides, we can further enable text-supervised image editing with an editing prompt (Algorithm 4). See results in Figure 4(a). This is useful because the additional text prompt allows us to enforce a specified editing direction that _cannot_ be found easily in the semantic subspace of the vanilla Jacobian \(\bm{J}_{\bm{\theta},t}\). As illustrated in Figure 4(b), this includes adding glasses or changing the curly hair of a human face. For simplicity, we introduce the key ideas of text-supervised T-LOCO Edit based upon DeepFloyd IF [19]. Similar procedures are also generalized to Stable Diffusion and Latent Consistency Models with an additional decoding step [4, 38]. We discuss the key intuition below, see Appendix E.2 and Appendix E.3 for method details.

Figure 3: **Illustration of the unsupervised LOCO Edit for unconditional diffusion models.** Given an image \(\bm{x}_{0}\), we perform DDIM-Inv until time \(t\) to get \(\bm{x}_{t}\), and estimate \(\bm{\hat{x}}_{\bm{0},t}\) from \(\bm{x}_{t}\). After masking to get the region of interest (ROI) \(\bm{\tilde{x}}_{0,t}\) and its counterparts \(\bm{\bar{x}}_{0,t}\), we find the edit direction \(\bm{v}_{p}\) via SVD and nullspace projection based on their Jacobians (Algorithm 1). By denoising \(\bm{x}_{t}+\lambda\bm{v}_{p}\), an image \(\bm{x}_{0}^{\prime}\) with localized editing is generated. _In this paper, the variables and notions related to ROI, nullspace, and final direction are respectively highlighted by green, blue, and red colors.

We first introduce some notations. Let \(c_{o}\) denote the original prompt, and \(c_{e}\) denote the editing prompt. For example, in Figure 4(b), \(c_{o}\) can be "portrait of a man", while \(c_{e}\) can be "portrait of a man with glasses". Correspondingly, given the noisy image \(\bm{x}_{t}\) for the clean image \(\bm{x}_{0}\) generated with \(c_{o}\), let \(\bm{f}^{o}_{\theta,t}(\bm{x}_{t})\) and \(\bm{J}^{o}_{\theta,t}(\bm{x}_{t})\) be the estimated posterior mean and its Jacobian conditioned on the original prompt \(c_{o}\), and let \(\bm{f}^{e}_{\theta,t}(\bm{x}_{t})\) and \(\bm{J}^{o}_{\theta,t}(\bm{x}_{t})\) be the estimated posterior mean and its Jacobian conditioned on both the editing prompt \(c_{e}\) and \(c_{o}\).

According to the classifier-free guidance (3), we can estimate the _difference_ of estimated posterior means caused by the editing prompt as \(\bm{d}=\bm{f}^{e}_{\theta,t}(\bm{x}_{t})-\bm{f}^{o}_{\theta,t}(\bm{x}_{t})\), and then set \(\bm{v}=\bm{J}^{e}_{\theta,t}(\bm{x}_{t})^{\top}\bm{d}\) as an initial estimator of the editing direction.3 Based upon this, to enable localized editing, similar to the unsupervised case, we can apply masks \(\Omega\) to select ROI in \(\bm{d}\) and calculate localized Jacobian to get \(\bm{v}\). After that, similarly, we can perform nullspace projection of \(\bm{v}\) for better disentanglement to get the final editing direction \(\bm{v}_{p}\).

Footnote 3: The idea is to identify the editing direction in the \(\mathcal{X}_{t}\) space based on changes in the estimated posterior mean caused by the editing prompt. More details are provided in Appendix E.3.

## 4 Justification of Local Linearity, Low-rankness, & Semantic Direction

In this section, we provide theoretical justification for the benign properties in Section 3.1. First, we assume that the image distribution \(p_{\text{data}}\) follows _mixture of low-rank Gaussians_ defined as follows.

**Assumption 1**.: _The data \(\bm{x}_{0}\in\mathbb{R}^{d}\) generated distribution \(p_{\text{data}}\) lies on a union of \(K\) subspaces. The basis of each subspace \(\left\{\bm{M}_{k}\in\operatorname{St}(d,r_{k})\right\}_{k=1}^{K}\) are orthogonal to each other with \(\bm{M}_{i}^{\top}\bm{M}_{j}=\bm{0}\) for all \(1\leq i\neq j\leq K\), and the subspace dimension \(r_{k}\) is much smaller than the ambient dimension \(d\). Moreover, for each \(k\in[K]\), \(\bm{x}_{0}\) follows degenerated Gaussian with \(\mathbb{P}\left(\bm{x}_{0}=\bm{M}_{k}\bm{a}_{k}\right)=1/K,\bm{a}_{k}\sim \mathcal{N}(\bm{0},\bm{I}_{r_{k}})\). Without loss of generality, suppose \(\bm{x}_{t}\) is from the \(h\)-th class, that is \(\bm{x}_{t}=\sqrt{\alpha_{t}}\bm{x}_{0}+\sqrt{1-\alpha_{t}}\bm{\epsilon}\) where \(\bm{x}_{0}\in\operatorname{range}(\bm{M}_{h})\), i.e. \(\bm{x}_{0}=\bm{M}_{h}\bm{a}_{h}\). Both \(||\bm{x}_{0}||_{2},||\bm{\epsilon}||_{2}\) is bounded._

Our data assumption is motivated by the intrinsic low-dimensionality of real-world image dataset [58].Additionally, Wang et al. [59] demonstrated that images generated by an analytical score function derived from a mixture of Gaussians distribution exhibit conceptual similarities to those produced by practically trained diffusion models. Given that \(\bm{f}_{\theta,t}(\bm{x}_{t})\) is an estimator of the posterior mean \(\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\), we show that the posterior mean \(\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\) can analytically derived as follows.

**Lemma 1**.: _Under Assumption 1, for \(t\in(0,1]\), the posterior mean is_

\[\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]=\sqrt{\alpha_{t}}\frac{\sum_{k=1 }^{K}\exp\left(\frac{\alpha_{t}}{2\left(1-\alpha_{t}\right)}\|\bm{M}_{k}^{ \top}\bm{x}_{t}\|^{2}\right)\bm{M}_{k}\bm{M}_{k}^{\top}\bm{x}_{t}}{\sum_{k=1}^ {K}\exp\left(\frac{\alpha_{t}}{2\left(1-\alpha_{t}\right)}\|\bm{M}_{k}^{\top} \bm{x}_{t}\|^{2}\right)}.\] (7)

Lemma 1 shows that the posterior mean \(\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]\) could be viewed as a convex combination of \(\bm{M}_{k}\bm{M}_{k}^{\top}\bm{x}_{t}\), i.e. \(\bm{x}_{t}\) projected onto each subspace \(\bm{M}_{k}\). This lemma leads to the following theorem:

Figure 4: **T-LOCO Edit on T2I diffusion models.** (a) Unsupervised editing direction is found only via the given mask without editing prompt. (b) Text-supervised editing direction is found with both a mask and an editing prompt such as ”with glasses”. Experiment details can be found in Appendix G.3.

**Theorem 1**.: _Based upon Assumption 1, we can show the following three properties for the posterior mean \(\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\):_

* _The Jacobian of posterior mean satisfies_ \(\operatorname{rank}\left(\nabla_{\bm{x}_{t}}\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}] \right)\leq r:=\sum\limits_{k=1}^{K}r_{k}\) _for all_ \(t\in(0,1]\)_._
* _The posterior mean_ \(\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\) _has local linearity such that_ \[\left\|\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}+\lambda\Delta\bm{x}\right]- \mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]-\lambda\nabla_{\bm{x}_{t}} \mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\cdot\Delta\bm{x}\right\|=\lambda\frac{\alpha _{t}}{(1-\alpha_{t})}\mathcal{O}(\lambda),\] (8) _where_ \(\Delta\bm{x}\in\mathbb{S}^{d-1}\) _and_ \(\lambda\in\mathbb{R}\) _is the step size._
* \(\nabla_{\bm{x}_{t}}\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\) _is symmetric and the full SVD of_ \(\nabla_{\bm{x}_{t}}\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\) _could be written as_ \(\nabla_{\bm{x}_{t}}\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]=\bm{U}_{t}\bm{\Sigma}_{ t}\bm{V}_{t}^{\top}\)_, where_ \(\bm{U}_{t}=[\bm{u}_{t,1},\bm{u}_{t,2},\ldots,\bm{u}_{t,d}]\in\operatorname{St} (d,d)\)_,_ \(\bm{\Sigma}_{t}=\operatorname{diag}(\sigma_{t,1},\ldots,\sigma_{t,r},\ldots,0)\) _with_ \(\sigma_{t,1}\geq\cdots\geq\sigma_{t,r}\geq 0\) _and_ \(\bm{V}_{t}=[\bm{v}_{t,1},\bm{v}_{t,2},\ldots,\bm{v}_{t,d}]\in\operatorname{St }(d,d)\)_. Let_ \(\bm{U}_{t,1}:=[\bm{u}_{t,1},\bm{u}_{t,2},\ldots,\bm{u}_{t,r}]\) _and_ \(\bm{M}:=[\bm{M}_{1},\bm{M}_{2},\ldots,\bm{M}_{K}]\)_. It holds that_ \(\lim_{t\to 1}\left\|\left(\bm{I}_{d}-\bm{U}_{t,1}\bm{U}_{t,1}^{\top}\right) \bm{M}\right\|_{F}=0\)_._

The proof is deferred to Appendix F. Admittedly, there are gap between our theory and practice, such as the approximation error between \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) and \(\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\), assumptions about the data distribution, and the high rankness of \(\bm{J}_{\bm{\theta},t}\) for \(t<0.2\) and \(t>0.9\) in Figure 2. Nonetheless, Theorem 1 largely supports our empirical observation in Section 3 that we discuss below:

* **Low-rankness of the Jacobian.** The first property in Theorem 1 demonstrates that the rank of \(\nabla_{\bm{x}_{t}}\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\) is always no greater than the intrinsic dimension of the data distribution. Given that the intrinsic dimension of the real data distribution is usually much lower than the ambient dimension [58], the rank of \(\bm{J}_{\bm{\theta},t}\) on the real dataset should also be low. The results align with our empirical observations in Figure 2 when \(t\in[0.2,0.7]\).
* **Linearity of the posterior mean.** The second property in Theorem 1 shows that the linear approximation error is within the order of \(\lambda\alpha_{t}/(1-\alpha_{t})\cdot\mathcal{O}(\lambda)\). This implies that when \(t\) approaches 1, \(\alpha_{t}/(1-\alpha_{t})\) becomes small, resulting in a small approximation error even for large \(\lambda\). Empirically, Figure 2 shows that the linear approximation error of \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) is small when \(t=0.7\) and \(\lambda=40\), whereas Figure 10 shows a much larger error for \(t=0.0\) under the same \(\lambda\). These observations align well with our theory.
* **Low-dimensional semantic subspace.** The third property in Theorem 1 shows that, when \(t\) is close to 1, left singular vectors associated with the top-\(r\) singular values form the basis of the image distribution. Since the editing direction consists of basis, the edited image remains within the image distribution. This explains why \(\bm{u}_{i}\) found in Equation (6) is a semantic direction for image editing.

## 5 Experiments

In this section, we perform extensive experiments to demonstrate the effectiveness and efficiency of LOCO Edit. We first showcase LOCO Edit has strong _localized_ editing ability across a variety of datasets in Section 5.1. Moreover, we conduct comprehensive comparisons with other methods to show the superiority of the LOCO Edit method in Section 5.2. Besides, we provide ablation studies on multiple components in our method in Appendix C.1, and analyze the editing directions in Appendix C.2, with extra experimental details postponed to Appendix G.

### Demonstration on Localized Editing and Other Benign Properties

First, we demonstrate benign properties of LOCO Edit in Algorithm 1 on a variety of datasets, including LSUN-Church [60], Flower [61], AFHQ [62], CelebA-HQ [52], and FFHQ [63].

As shown in Figure 5 and Figure 0(a), our method enables editing specific localized regions such as eye size/focus, hair curvature, length/amount, and architecture, while preserving the consistency of other regions. Besides the ability of precise local editing, Figure 1 demonstrates the benign properties of the identified editing directions and verify our analysis in Section 4:

* **Linearity.** As shown Figure 0(d), the semantic editing can be strengthened through larger editing scales and can be flipped by negating the scale.
* **Homogeneity and transferability.** As shown Figure 0(b), the discovered editing direction can be transferred across samples and timesteps in \(\mathcal{X}_{t}\).

* **Composability.** As shown Figure 1(c), the identified disentangled editing directions in the low-rank subspace allow direct composition without influencing each other.

### Comprehensive Comparison with Other Image Editing Methods

We compare LOCO Edit with several notable and recent image editing techniques, including Asyrp [29], Pullback [30], NoiseCLR [23], and BlendedDifusion [24]. We also compare with an unexplored method using the Jacobians \(\frac{\partial\epsilon_{t}}{\partial x_{t}}\) to find the editing direction, named as \(\frac{\partial\epsilon_{t}}{\partial x_{t}}\).

Metrics.We evaluate our method using the below metrics and summarize the results in Table 1. Besides the image generation quality, we also compared other attributes such as the local edit ability, efficiency, the requirement for supervision, and theoretical justifications.

* _Local Edit Success Rate_ evaluates whether the editing successfully changes the target semantics and preserves unrelated regions by human evaluators.
* _LPIPS_[64] and _SSIM_[65] measure the consistency between edited and original images.
* _Transfer Success Rate_ measures whether the editing transferred to other images successfully changes the target semantics and preserves unrelated regions by human evaluators.
* _Learning time_ to measure the time required to identify the edit directions.
* _Transfer Edit Time_ to measure the time required to transfer the editing to other images directly.
* _#Images for Learning_ measures the number of images used to find the editing directions.
* _One-step Edit_, _No Additional Supervision_, _Theoretically Grounded_, and _Localized Edit_ are attributes of the editing methods, where each of them measures a specific property for the method.

Moreover, we visualize the editing results on non-cherry-picked images in Figure 6. The detailed evaluation settings are provided in Appendix G.2.

Benefits of Our Method.Based upon the qualitative and quantitative comparisons, our method shows several clear advantages that we summarize as follows.

* **Superior local edit ability with one-step edit.** Table 1 shows LOCO Edit achieves the best Local Edit Success Rate. Such local edit ability only requires one-step edit at a specific time \(t\). For LPIPS and SSIM, our method performs better than most methods but worse than BlendedDiffusion. However, BlendedDiffusion sometimes fails the edit within the masks (as visualized in Figure 6, rows 1, 3, 4, and 5). Other methods find semantic direction more globally, leading to worse performance in Local Edit Success Rate, LPIPS, and SSIM for localized edits.

Figure 5: **Benchmarking LOCO Edit across various datasets. For each group of three images, in the center is the original image, and on the left and right are edited images along the negative and the positive directions accordingly.**

* **Transferability and efficiency.** First, LOCO Edit requires less learning time than most of the other methods and requires learning only for a single time step with a single image. Moreover, LOCO Edit is highly transferable, having the highest Transfer Success Rate in Table A. In contrast, BlendedDiffusion cannot transfer and requires optimization for each individual image. NoiseCLR has the second-best yet lower transfer success rate, while other methods exhibit worse transferability.
* **Theoretically-grounded and supervision-free.** LOCO Edit is theoretically grounded. Besides, it is supervision-free, thus integrating no biases from other modules such as CLIP [36]. [37] shows CLIP sometimes can't capture detailed semantics such as color. We can observe failures in capturing detailed semantics for methods that utilize CLIP guidance such as BlendedDiffusion and Asyrp in Figure 6, where there are no edits or wrong edits.

## 6 Conclusion

We proposed a new low-rank controllable image editing method, LOCO Edit, which enables precise, one-step, localized editing using diffusion models. Our approach stems from the discovery of the locally linear posterior mean estimator in diffusion models and the identification of a low-dimensional semantic subspace in its Jacobian, theoretically verified under certain data assumptions. The identified editing directions possess several beneficial properties, such as linearity, homogeneity, and composability. Additionally, our method is versatile across different datasets and models and is applicable to text-supervised editing in T2I diffusion models. Through various experiments, we demonstrate the superiority of our method compared to existing approaches.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Method Name & Pullback & \(\partial\epsilon_{t}/\partial\bm{x}_{t}\) & NoiseCLR & Asyrp & BlendedDiffusion & **LOCO (Ours)** \\ \hline Local Edit Success Rate\(\uparrow\) & 0.32 & 0.37 & 0.32 & 0.47 & **0.55** & **0.80** \\ LPIPS\(\downarrow\) & 0.16 & 0.13 & 0.14 & 0.22 & **0.03** & **0.08** \\ SSIM\(\uparrow\) & 0.60 & 0.66 & 0.68 & 0.68 & **0.94** & **0.71** \\ Transfer Success Rate\(\uparrow\) & 0.14 & 0.24 & **0.66** & 0.58 & Can’t Transfer & **0.91** \\ Transfer Edit Time\(\downarrow\) & 4s & **2s** & 5s & 3s & Can’t Transfer & **2s** \\ \#Images for Learning & **1** & **1** & 100 & 100 & **1** & **1** \\ Learning Time\(\downarrow\) & **88** & **44s** & 1 day & 475s & 120s & 79s \\ One-step Edit? & ✓ & ✓ & ✗ & ✗ & ✗ & ✓ \\ No Additional Supervision? & ✓ & ✓ & ✓ & ✗ & ✗ & ✓ \\ Theoretically Grounded? & ✗ & ✗ & ✗ & ✗ & ✗ & ✓ \\ Localized Edit? & ✗ & ✗ & ✗ & ✗ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparisons with existing methods. Our LOCO Edit excels in localized editing, transferability and efficiency, with other intriguing properties such as one-step edit, supervision-free, and theoretically grounded.**

Figure 6: **Compare local edit ability with other works on non-cherry-picked images. LOCO has consistent and accurate local edit ability, while other methods have wrong, global, or no edits.**

## Acknowledgement

We acknowledge support from NSF CAREER CCF-2143904, NSF CCF-2212066, NSF CCF-2212326, NSF IIS 2312842, NSF IIS 2402950, ONR N00014-22-1-2529, a gift grant from KLA, an Amazon AWS AI Award, MICDE Catalyst Grant. The authors acknowledge valuable discussions with Mr. Zekai Zhang (U. Michigan), Dr. Ismail R. Alkhouri (U. Michigan and MSU), Mr. Jinfan Zhou (U. Michigan), and Mr. Xiao Li (U. Michigan).

## References

* [1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [2] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021.
* [3] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2021.
* [4] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [5] Huijie Zhang, Yifu Lu, Ismail Alkhouri, Saiprasad Ravishankar, Dogyoon Song, and Qing Qu. Improving training efficiency of diffusion models via multi-stage framework and tailored multi-decoder architectures. In _Conference on Computer Vision and Pattern Recognition 2024_, 2024.
* [6] Ismail Alkhouri, Shijun Liang, Rongrong Wang, Qing Qu, and Saiprasad Ravishankar. Diffusion-based adversarial purification for robust deep mri reconstruction. _ArXiv preprint arXiv:2309.05794_, 2023.
* [7] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In _International Conference on Learning Representations_, 2021.
* [8] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In _International Conference on Learning Representations_, 2021.
* [9] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [10] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In _International Conference on Learning Representations_, 2023.
* [11] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In _The Eleventh International Conference on Learning Representations_, 2023.
* [12] Xiang Li, Soo Min Kwon, Ismail R Alkhouri, Saiprasad Ravishanka, and Qing Qu. Decoupled data consistency with diffusion purification for image restoration. _ArXiv preprint arXiv:2403.06054_, 2024.
* [13] Ismail Alkhouri, Shijun Liang, Rongrong Wang, Qing Qu, and Saiprasad Ravishankar. Robust physics-based deep mri reconstruction via diffusion purification. In _Conference on Parsimony and Learning (Recent Spotlight Track)_, 2023.
* [14] Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse problems with latent diffusion models via hard data consistency. In _The Twelfth International Conference on Learning Representations_, 2024.
* [15] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18456-18466, 2023.
* [16] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. _ArXiv preprint arXiv:2311.15127_, 2023.

* [17] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2Video-zero: Text-to-image diffusion models are zero-shot video generators. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 15954-15964, 2023.
* [18] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _ArXiv preprint arXiv:2204.06125_, 2022.
* [19] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* [20] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4396-4405, 2018.
* [21] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.
* [22] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.
* [23] Yusuf Dalva and Pinar Yanardag. Noiseclr: A contrastive learning approach for unsupervised discovery of interpretable directions in diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 24209-24218, 2024.
* [24] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 18208-18218, June 2022.
* [25] Theodoros Kouzelis, Manos Pitisis, Mihalis A. Nicolaou, and Yannis Panagakis. Enabling local editing in diffusion models by joint and individual component analysis, 2024.
* [26] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. In _The Eleventh International Conference on Learning Representations_, 2023.
* [27] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. SEGA: Instructing text-to-image models using semantic guidance. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [28] Quicheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in text-to-image diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1900-1910, 2023.
* [29] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent space. In _The Eleventh International Conference on Learning Representations_, 2023.
* [30] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the latent space of diffusion models through the lens of riemannian geometry. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [31] Ye Zhu, Yu Wu, Zhiwei Deng, Olga Russakovsky, and Yan Yan. Boundary guided learning-free semantic control with diffusion models. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* [32] Hila Manor and Tomer Michaeli. Zero-shot unsupervised and text-based audio editing using DDPM inversion. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pages 34603-34629. PMLR, 21-27 Jul 2024.
* [33] Hila Manor and Tomer Michaeli. On the posterior distribution in denoising: Application to uncertainty quantification. In _The Twelfth International Conference on Learning Representations_, 2024.
* [34] Yousef Saad. _Numerical methods for large eigenvalue problems: revised edition_. SIAM, 2011.

* [35] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the latent space of diffusion models through the lens of riemannian geometry. _Advances in Neural Information Processing Systems_, 36:24129-24142, 2023.
* [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* [37] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal lms. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9568-9578, 2024.
* [38] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. _ArXiv preprint arXiv:2310.04378_, 2023.
* [39] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.
* [40] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6038-6047, 2023.
* [41] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications_, 2021.
* [42] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _Advances in Neural Information Processing Systems_, 35:5775-5787, 2022.
* [43] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.
* [44] Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Peng Wang, Liyue Shen, and Qing Qu. The emergence of reproducibility and consistency in diffusion models. In _Forty-first International Conference on Machine Learning_, 2024.
* [45] Calvin Luo. Understanding diffusion models: A unified perspective. _ArXiv preprint arXiv:2208.11970_, 2022.
* [46] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2426-2435, 2022.
* [47] Rene Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, and Tomer Michaeli. Discovering interpretable directions in the semantic latent space of diffusion models. _International Conference on Automatic Face and Gesture Recognition_, abs/2303.11073, 2024.
* [48] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In _Forty-first International Conference on Machine Learning_, 2024.
* [49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-assisted Intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18_, pages 234-241. Springer, 2015.
* [50] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [51] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22669-22679, 2023.
* [52] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 3730-3738, 2015.
* [53] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pages 248-255. Ieee, 2009.

* [54] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.
* [55] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 4015-4026, October 2023.
* [56] S. Banerjee and A. Roy. _Linear Algebra and Matrix Analysis for Statistics_. Chapman & Hall/CRC Texts in Statistical Science. CRC Press, 2014.
* [57] Jiapeng Zhu, Ruili Feng, Yujun Shen, Deli Zhao, Zhengjun Zha, Jingren Zhou, and Qifeng Chen. Low-rank subspaces in gans. In _Neural Information Processing Systems_, 2021.
* [58] Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. In _International Conference on Learning Representations_, 2021.
* [59] Binku Wang and John J Vastola. The hidden linear structure in score-based models and its application. _ArXiv preprint arXiv:2311.10892_, 2023.
* [60] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. _ArXiv_, abs/1506.03365, 2015.
* [61] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing_, pages 722-729, 2008.
* [62] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8188-8197, 2020.
* [63] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4401-4410, 2019.
* [64] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 586-595, 2018.
* [65] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE Transactions on Image Processing_, 13(4):600-612, 2004.
* [66] Peng Wang, Huikang Liu, Druv Pai, Yaodong Yu, Zhihui Zhu, Qing Qu, and Yi Ma. A global geometric analysis of maximal coding rate reduction. In _Forty-first International Conference on Machine Learning_, 2024.
* [67] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022.
* [68] Can Yaras, Peng Wang, Laura Balzano, and Qing Qu. Compressible dynamics in deep overparameterized low-rank learning & adaptation. In _Forty-first International Conference on Machine Learning_, 2024.
* [69] Michael Fuest, Pingchuan Ma, Ming Gui, Johannes S Fischer, Vincent Tao Hu, and Bjorn Ommer. Diffusion models and representation learning: A survey. _ArXiv preprint arXiv:2407.00783_, 2024.
* [70] Xiang Li, Yixiang Dai, and Qing Qu. Understanding generalizability of diffusion models requires rethinking the hidden gaussian structure. 2024.
* [71] Peng Wang, Xiao Li, Yaras Can, Zhihui Zhu, Laura Balzano, Wei Hu, and Qing Qu. Understanding deep representation learning via layerwise feature compression and discrimination. _ArXiv preprint arXiv:2311.02960_, 2023.
* [72] Siyi Chen, Minkyu Choi, Zesen Zhao, Kuan Han, Qing Qu, and Zhongming Liu. Unfolding videos dynamics via taylor expansion, 2024.

* [73] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2023.
* [74] Shengyi Qian, Linyi Jin, Chris Rockwell, Siyi Chen, and David F. Fouhey. Understanding 3d object articulation in internet videos. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1589-1599, 2022.
* [75] Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, and Rene Vidal. Pace: Parsimonious concept engineering for large language models. _ArXiv preprint arXiv:2406.04331_, 2024.
* [76] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In _Neural Information Processing Systems_, 2014.
* [77] Rene Haas, Inbar Huberman-Spiegelas, Rotem Mulayoff, and Tomer Michaeli. Discovering interpretable directions in the semantic latent space of diffusion models. _International Conference on Automatic Face and Gesture Recognition_, abs/2303.11073, 2024.
* [78] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In _The Eleventh International Conference on Learning Representations_, 2023.
* [79] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. Instructedit: Improving automatic masks for diffusion-based image editing with user instructions. _ArXiv_, abs/2305.18047, 2023.
* [80] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 18392-18402, 2022.
* [81] Shanglin Li, Bo-Wen Zeng, Yutang Feng, Sicheng Gao, Xuhui Liu, Jiaming Liu, Li Lin, Xu Tang, Yao Hu, Jianzhuang Liu, and Baochang Zhang. Zone: Zero-shot instruction-guided local editing. _2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2024.
* [82] Bradley Efron. Tweedie's formula and selection bias. _Journal of the American Statistical Association_, 106(496):1602-1614, 2011.
* [83] A Woodbury Max. Inverting modified matrices. In _Memorandum Rept. 42, Statistical Research Group_, page 4. Princeton Univ., 1950.
* [84] Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. _SIAM Journal on Numerical Analysis_, 7(1):1-46, 1970.
* [85] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS '20, Red Hook, NY, USA, 2020. Curran Associates Inc.
* [86] Jooyoung Choi, Jungboom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo J. Kim, and Sung-Hoon Yoon. Perception prioritized training of diffusion models. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11462-11471, 2022.
* [87] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.

**Appendix**

## Appendix A Future Direction

We identify several future directions and limitations of the current work. The current theoretical framework explains mainly the unsupervised image editing part. A more solid and thorough analysis of text-supervised image editing is of significant importance in understanding T2I diffusion models, which is yet a difficult open problem in the field. For example, there is still a lack of geometric analysis of the relationship between subspaces under different text-prompt conditions [4, 19, 38, 66]. Based on such understandings, it may be possible to further discover benign properties of editing directions in T2I diffusion models, or design more efficient fine-tuning [67, 68] accordingly. Besides, the current method has the potential to be extended for combining coarse to fine editing across different time steps. Furthermore, it is worth exploring the direct manipulation of semantic spaces in flow-matching diffusion models and transformer-architecture diffusion models. Lastly, it is possible to connect the current finding to image or video representation learning in diffusion models [69, 70, 71, 72], extend to 3D editing of pose or shape [73, 74], or utilize the low-rank structures to build dictionaries [75].

## Appendix B Discussion on Related Works

Study of Latent Semantic Space in Generative Models.Although diffusion models have demonstrated their strengths in state-of-the-art image synthesis, the understanding of diffusion models is still far behind the other generative models such as Generative Adversarial Networks (GAN) [76, 57], the understanding of which can provide tools as well as inspiration for the understanding of diffusion models. Some recent works have identified such gaps, discovered latent semantic spaces in diffusion models [29], and further studied the properties of the latent space from a geometrical perspective [30]. These prior arts deepen our understanding of the latent semantic space in diffusion models, and inspire later works to study the structures of information represented in diffusion models from various angles. However, their semantic space is constrained to diffusion models using UNet architecture, and can not represent localized semantics. Our work explores an alternative space to study the semantic expression in diffusion models, inspired by our observation of the low-rank and locally linear Jacobian of the denoiser over the noisy images. We provide a theoretical framework for demonstrating and understanding such properties, which can deepen the interpretation of the learned data distribution in diffusion models.

Image Editing in Unconditional Diffusion Models.Recent research has significantly improved the understanding of latent semantic spaces in diffusion models, enabling global image editing through either training-free methods [29, 30, 31] or by incorporating an additional lightweight model [30, 77]. However, these methods result in poor performance for localized edit. In contrast, our approach achieves localized editing without requiring supervised training. For localized edits, [25] builds on [30], enabling local edits by altering the intermediate layers of UNet. However, these approaches are restricted to UNet-based architectures in diffusion models and have largely ignored intrinsic properties like linearity and low-rankness. In comparison, our work provides a rigorous theoretical analysis of low-rankness and local linearity in diffusion models, and we are the first to offer a principled justification of the semantic significance of the basis used for editing. Moreover, our method is independent of specific network architectures.

Other recent works, such as [32], introduce training-free global audio and image editing based on a theoretical understanding of the posterior covariance matrix [33], also independent of UNet architectures. However, our approach offers a distinct perspective, providing complementary insights and new findings. We explore the low-rank nature and local linearity in PMP, offering rigorous theoretical analyses. Based on this, our proposed LOCO Edit method allows unsupervised and localized editing, which enables several advantageous properties including transferability, composability, and linearity - benign features that have not been explored in prior work. Further, we extend the method to unsupervised and text-supervised editing in various text-to-image models. Additionally, while [24] supports localized editing, it requires supervision from CLIP, lacks a theoretical basis, and is time-consuming for editing each image. In contrast, our method is more efficient, theoretically grounded, and free from failures or biases in CLIP. The CLIP-supervised may also exhibit a biastoward the CLIP score, leading to suboptimal editing results, as shown in Figure 6. In comparison, our method consistently enables high-quality edits without such bias.

Image Editing in T2I Diffusion Models.T2I image editing usually requires much more complicated sampling and training procedures, such as providing certainly learned guidance in the reverse sampling process [11], training an extra neural network [21], or fine-tuning the models for certain attributes [22]. Although effective, these methods often require extra training or even human intervention. Some other T2I image editing methods are training-free [46, 27, 28], and further enable editing with identifying masks [46], or optimizing the soft combination of text prompts [28]. These methods involve a continuous injection of the edit prompt during the generation process to gradually refine the generated image to have the target semantics. Though effective, all of the above methods (either training-free or not) as well as instruction-guided ones [78, 79, 80, 81] lack clear mathematical interpretations and requires text supervision. [23] discovers editing directions in T2I diffusion models through contrastive learning without text supervision, but is not generalizable to editing with text supervision. [30] has some theoretical basis and extends to an editing approach in T2I diffusion models with text supervision, but such supervision is only for unconditional sampling. In contrast, our extended T-LOCO Edit, which originated from the understanding of diffusion models, is the first method exploring single-step editing with or without text supervision for conditional sampling.

## Appendix C More Experiment Results on LOCO-Edit

### Ablation Studies

We conduct several important ablation studies on noise levels, the rank of nullspace projection, and editing strength, which demonstrates the robustness of our method.

* **Noise levels (i.e., editing time step \(t\)).** We conducted an ablation study on different noise levels, with representative examples shown in Figure 6(a). The key observations are summarized as follows: (a) Larger noise levels (i.e., edit on \(x_{t}\) with larger \(t\)) perform more coarse edit while small noise levels perform finer edit; (b) LOCO Edit is applicable to a generally large range of noise levels ([0.2T, 0.7T]) for precise edit.
* **Rank of nullspace projection \(r^{\prime}\).** Ablation study on nullspace projection is in Figure 6(b) (definition of \(r^{\prime}\) is in Algorithm 1). We present the key observations: (a) the local edit ability with no nullspace projection is weaker than that with nullspace projection; (b) when conducting nullspace projection, an effective low-rank estimation with \(r^{\prime}=5\) can already achieve good local edit results.
* **Editing strength \(\lambda\).** The linearity with respect to editing strengths is visualized in Figure 6(c), with the key observations in addition to linearity: LOCO Edit is applicable to a generally wide range of editing strengths ([-15, 15]) to achieve localized edit.

Figure 7: **Ablation Study. (a) Effects of one-step edit time. (b)Effects of using nullspace projection and rank. (c)Effects of editing strengths.**

### Visualization and Analysis of Editing Directions

We visualize the identified editing direction \(\bm{v}_{p}\) (see Algorithm 1) in Figure 8. The editing directions are semantically meaningful to the region of interest for editing. For example, the editing directions for eyes, lips, nose, etc., have similar shapes to eyes, lips, nose, etc.

Further, since the objects in datasets Flower, AFHQ, CelebA-HQ, and FFHQ are usually positioned at the center, the identified editing directions also tend to be at the center. Besides, objects could have different shapes, and semantics in some images do not exist in other images. To further study the robustness of transferability for the editing directions, we transfer editing directions to images with objects at different positions, from different datasets, with different shapes, and with no corresponding semantics. We present the results in Figure 9, with key observations that: (a) the edit directions are generally robust to gender differences, shape differences, moderate position differences, and dataset differences, illustrated in the first five rows of Figure 9 (b) transferring editing direction to images without corresponding semantics results in almost no editing (shown in the last row of Figure 9). Therefore, in practical applications, meaningful transfer editing scenarios for LOCO Edit occur when the transferred editing directions correspond to existing semantics in the target image (e.g., transferring the editing direction of "eyes" is effective only if the target image also contains eyes).

Figure 8: **Visualizing edit directions identified via LOCO Edit. The edit directions are semantically meaningful.**

Figure 9: **Analyzing transferability of edit directions to objects with different positions and shapes, images from different datasets, or images with no corresponding semantics.**

## Appendix D More Empirical Study on Low-rankness & Local Linearity

### Experiment Setup for Section 3.1

We evaluate the numerical rank of the denoiser function \(\bm{x}_{\bm{\theta}}(\bm{x}_{t},t)\) for DDPM (U-Net [49] architecture) on CIFAR-10 dataset [50] (\(d=32\times 32\times 3\)), U-ViT [51] (Transformer based networks) on CelebA [52] (\(d=64\times 64\times 3\)), ImageNet [53] datasets (\(d=64\times 64\times 3\)) and DeepFloy IF [19] trained on LAION-5B [54] dataset (\(d=64\times 64\times 3\)). Notably, U-ViT architecture uses the autoencoder to compress the image \(\bm{x}_{0}\) to embedding vector \(\bm{z}_{0}=\texttt{Encoder}(\bm{x}_{0})\), and adding noise to \(\bm{z}_{t}\) for the diffusion forward process; and the reverse process replaces \(\bm{x}_{t},\bm{x}_{t-\Delta t}\) with \(\bm{z}_{t},\bm{z}_{t-\Delta t}\) in Equation (1). And the generated image \(\bm{x}_{0}=\texttt{Decoder}(\bm{z}_{0})\). The PMP defined for U-ViT is:

\[\bm{\hat{x}}_{0,t}=f_{\bm{\theta},t}(\bm{z}_{t};t)\coloneqq\texttt{Decoder} \left(\frac{\bm{z}_{t}-\sqrt{1-\alpha_{t}}\bm{\epsilon_{\theta}}(\bm{z}_{t},t )}{\sqrt{\alpha_{t}}}\right).\] (9)

The \(\bm{J}_{\bm{\theta},t}(\bm{z}_{t};t)=\nabla_{\bm{z}_{t}}f_{\bm{\theta},t}(\bm {z}_{t};t)\) for \(f_{\bm{\theta},t}(\bm{z}_{t};t)\) defined above. For DeepFloy IF, there are three diffusion models, one for generation and the other two for super-resolution. Here we only evaluate \(\bm{J}_{\bm{\theta},t}(\bm{z}_{t};t)\) for diffusion generating the images.

Given a random initial noise \(\bm{x}_{T}\), diffusion model \(\bm{x}_{\bm{\theta}}\) generate image sequence \(\{\bm{x}_{t}\}\) follows reverse sampler Equation (1). Along the sampling trajectory \(\{\bm{x}_{t}\}\), for each \(\bm{x}_{t}\), we calculate \(\bm{J}_{\bm{\theta},t}(\bm{z}_{t};t)\) and compute its numerical rank via

\[\widetilde{\texttt{rank}}(\bm{J}_{\bm{\theta},t}(\bm{x}_{t}))=\arg\min_{r} \left\{r:\frac{\sum_{i=1}^{r}\sigma_{i}^{2}\left(\bm{J}_{\bm{\theta},t}(\bm{x }_{t};t)\right)}{\sum_{i=1}^{n}\sigma_{i}^{2}\left(\bm{J}_{\bm{\theta},t}(\bm{ x}_{t};t)\right)}>\eta^{2}\right\},\] (10)

where \(\sigma_{i}(\bm{A})\) denotes the \(i\)th largest singular value of \(\bm{A}\). In our experiments, we set \(\eta=0.99\). We random generate \(15\) initialize noise \(\bm{x}_{t}\) (\(\bm{z}_{t}\) for U-ViT). We only use one prompt for DeepFloyd IF. We use DDIM with 100 steps for DDPM and DeepFloyd IF, DPM-Solver with 20 steps for U-ViT, and select some of the steps to calculate \(\texttt{rank}(\bm{J}_{\bm{\theta},t}(\bm{x}_{t};t))\), reported the averaged rank in Figure 2. To report the norm ratio and cosine similarity, we select the closest \(t\) to 0.7 along the sampling trajectory and reported in Figure 2, i.e. \(t=0.71\) for DDPM, \(t=0.66\) for U-ViT and \(t=0.69\) for DeepFloyd IF. The norm ratio and cosine similarity are also averaged over 15 samples.

### More Experiments for Section 3.1

We illustrated the norm ratio and cosine similarity for more timesteps in Figure 10, more text prompts, and flow-matching-based diffusion model in Figure 11. More specifically, for the plot of \(t=0.0\), we exactly use \(t=0.04\) for DDPM, \(t=0.005\) for U-ViT and \(t=0.09\) for DeepFloyd IF; for the plot of \(t=0.5\), we exactly use \(t=0.49\) for DDPM, \(t=0.50\) for U-ViT and \(t=0.49\) for DeepFloyd IF. The results aligned with our results in Theorem 1 that when \(t\) is closer the 1, the linearity of \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t},t)\) is better.

Figure 10: **More results on the linearity of \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t},t)\).**

### Comparison for Low-rankness & Local Linearity for Different Manifold

This section is an extension of Section 3.1. We study the low rankness and local linearity of more mappings between spaces of diffusion models. The sampling process of diffusion model involved the following space: \(\bm{x}_{t}\in\mathcal{X}_{t}\), \(\hat{\bm{x}}_{0,t}\in\mathcal{X}_{0,t}\), \(\bm{h}_{t}\in\mathcal{H}_{t}\), \(\bm{\epsilon}_{t}\in\mathcal{E}_{t}\), where \(\mathcal{H}_{t}\) is the h-space of U-Net's bottleneck feature space [29] and \(\mathcal{E}_{t}\) is the predict noise space. First, we explore the rank ratio of Jacobian \(\bm{J}_{\bm{\theta},t}\) and Frobenius norm \(||\bm{J}_{\bm{\theta},t}||_{F}\) for: \(\dfrac{\partial\bm{h}_{t}}{\partial\bm{x}_{t}}\dfrac{\partial\bm{\epsilon}_{ t}}{\partial\bm{h}_{t}},\dfrac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{h}_{t}}, \dfrac{\partial\bm{\epsilon}_{t}}{\partial\bm{x}_{t}},\dfrac{\partial\hat{\bm {x}}_{0,t}}{\partial\bm{x}_{t}}\). We use DDPM with U-Net architecture, trained on CIFAR-10 dataset, and other experiment settings are the same as Appendix D.1, results are shown in Figure 12. The conclusion could be summarized as :

* \(\dfrac{\partial\bm{h}_{t}}{\partial\bm{x}_{t}},\dfrac{\partial\bm{\epsilon}_{ t}}{\partial\bm{h}_{t}},\dfrac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{h}_{t}}, \dfrac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{x}_{t}}\) _are low rank jacobian when_ \(t\in[0.2,0.7]\)_. As shown in the left of Figure_ 12_, rank ratio for_ \(\dfrac{\partial\bm{h}_{t}}{\partial\bm{x}_{t}},\dfrac{\partial\bm{\epsilon}_{ t}}{\partial\bm{h}_{t}},\dfrac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{h}_{t}}, \dfrac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{x}_{t}}\) _is less than 0.1. It should be noted that:_
* \(\widetilde{\text{rank}}(\dfrac{\partial\bm{\epsilon}_{t}}{\partial\bm{x}_{t}}) \geq d-\widetilde{\text{rank}}(\dfrac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm {x}_{t}})\)_. This is because_ \[\widetilde{\text{rank}}(\dfrac{\sqrt{1-\alpha_{t}}}{\sqrt{\alpha_{t}}}\dfrac{ \partial\bm{\epsilon}_{t}}{\partial\bm{x}_{t}})\geq\widetilde{\text{rank}}( \dfrac{1}{\sqrt{\alpha_{t}}}\bm{I}_{d})-\widetilde{\text{rank}}(\dfrac{ \partial\hat{\bm{x}}_{0,t}}{\partial\bm{x}_{t}}).\] _Therefore,_ \(\dfrac{\partial\bm{\epsilon}_{t}}{\partial\bm{x}_{t}}\) _is high rank when_ \(\dfrac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{x}_{t}}\) _is low rank._

Figure 11: **More empirical study on low-rankness and local linearity on more prompts and models trained with flow-matching objectives.**

Figure 12: (Left) **Numerical rank of different jacobian \(\bm{J}\) at different timestep \(t\).** (Right) **Frobenius norm of different jacobian \(\bm{J}\) at different timestep \(t\)*** \(\widetilde{\mathsf{rank}}(\frac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{h}_{t}})= \widetilde{\mathsf{rank}}(\frac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{x}_{t}})\) This is because \(\hat{\bm{x}}_{0,t}=\frac{\bm{x}_{t}-\sqrt{1-\alpha_{t}}\bm{\epsilon}_{\bm{ \theta}}(\bm{x}_{t},t)}{\sqrt{\alpha_{t}}}\) and \(\frac{\partial\bm{x}_{t}}{\partial\bm{h}_{t}}=0\)
* _When_ \(\bm{x}_{t}\) _fixed,_ \(\hat{\bm{x}}_{0,t},\bm{\epsilon}_{t}\) _will change little when changing_ \(\bm{h}_{t}\)_. As shown in the right of Figure_ 12_,_ \(||\frac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{h}_{t}}||_{F}\ll||\frac{ \partial\hat{\bm{x}}_{0,t}}{\partial\bm{x}_{t}}||_{F}\) _and_ \(\frac{\partial\bm{\epsilon}_{t}}{\partial\bm{h}_{t}}\ll\frac{\partial\bm{ \epsilon}_{t}}{\partial\bm{x}_{t}}\)_. This means when_ \(\bm{x}_{t}\) _fixed,_ \(\hat{\bm{x}}_{0,t},\bm{\epsilon}_{t}\) _will change little when changing_ \(\bm{h}_{t}\)_._

_Then, we also study the linearity of_ \(\bm{h}_{t}\) _and_ \(\hat{\bm{x}}_{0,t}\) _given_ \(\bm{x}_{t}\)_, using DDPM with U-Net architecture trained on CIFAR-10 dataset. We change the step size_ \(\lambda\) _defined in Equation (_4_). Results are shown in Figure_ 13_, both_ \(\bm{h}_{t}\) _and_ \(\hat{\bm{x}}_{0,t}\) _have good linearity with respect to_ \(\bm{x}_{t}\)_._

_In Theorem_ 1_, the jacobian_ \(\nabla_{\bm{x}_{t}}\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\) _is a symmetric matrix. Therefore, we also verify the symmetry of the jacobian over the PMP_ \(\bm{J}_{\bm{\theta},t}\)_. We use DDPM with U-Net architecture trained on CIFAR-10 dataset. At different timestep_ \(t\)_, we measure_ \(||\bm{J}_{\bm{\theta},t}-\bm{J}_{\bm{\theta},t}^{\top}||_{F}\)_. Results are shown on the right of Figure_ 13_._ \(\bm{J}_{\bm{\theta},t}\) _has good symmetric property when_ \(t<0.1\) _and_ \(t\in[0.6,0.7]\)_. Additionally,_ \(\bm{J}_{\bm{\theta},t}\) _is low rank when_ \(t\in[0.6,0.7]\)_. So_ \(\bm{J}_{\bm{\theta},t}\) _aligned with Theorem_ 1__\(t\in[0.6,0.7]\)_._

_To the end, we want to based on the experiments in Figure_ 12 _and Figure_ 13 _to select the best space for out image editing method._ \(\frac{\partial\bm{\epsilon}_{t}}{\partial\bm{x}_{t}}\) _is the high-rank matrix, not suitable for efficiently estimate the nullspace;_ \(\frac{\partial\bm{\epsilon}_{t}}{\partial\bm{h}_{t}}\) _and_ \(\frac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{h}_{t}}\) _has too small Frobenius norm to edit the image. Therefore, only_ \(\frac{\partial\bm{h}_{t}}{\partial\bm{x}_{t}}\) _and_ \(\frac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{x}_{t}}\) _are low-rank and linear for image editing. What's more,_ \(h_{t}\) _space is restricted to UNet architecture, but the property of the_ \(\frac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{x}_{t}}\) _does not depend on the UNet architecture and is verified in diffusion models using transformer architectures. Additionally, we could only apply masks on_ \(\hat{\bm{x}}_{0,t}\) _but cannot on_ \(\bm{h}_{t}\)_._ **Therefore, the PMP_ \(\bm{f}_{\bm{\theta},t}\) _is the best mapping for image editing.**__

## Appendix E Extra Details of LOCO Edit and T-LOCO Edit

### Generalized Power Method

The Generalized Power Method [34, 30] for calculating the op-\(t\) singular vectors of the Jacobian is summarized in Algorithm 2. It efficiently computes the top-\(k\) singular values and singular vectors of the Jacobian with a randomly initialized orthonormal \(\bm{V}\in\mathbb{R}^{d\times k}\).

### Unsupervised T-LOCO Edit

The overall method for DeepFloyd is summarized in Algorithm 3. For T2I diffusion models in the latent space such as Stable Diffusion and Latent Consistency Model, at time \(t\), we additionally decode \(\hat{\bm{z}}_{0}\) into the image space \(\hat{\bm{x}}_{0}\) to enable masking and nullspace projection. The editing is still in the space of \(\bm{z}_{t}\).

Figure 13: (Left, Middle) **Cosine similarity and norm ration of different mappings with respect to \(\lambda\). (Right) Symmetric property of \(\frac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{x}_{t}}\) with respect to timestep \(t\).**

### Text-suprised T-LOCO Edit

Before introducing the algorithm, we define:

\[\bm{f}^{o}_{\bm{\theta},t}(\bm{x}_{t})=\frac{\bm{x}_{t}-\alpha_{t} \sigma_{t}(\bm{\epsilon_{\theta}}(\bm{x}_{t},t,c_{n})+s(\bm{\epsilon_{\theta}}( \bm{x}_{t},t,c_{o})-\bm{\epsilon_{\theta}}(\bm{x}_{t},t,c_{n})))}{\alpha_{t}},\] (11)

and

\[\bm{f}^{e}_{\bm{\theta},t}(\bm{x}_{t})=\bm{f}^{o}_{\bm{\theta},t} (\bm{x}_{t})+\frac{m(\bm{\epsilon_{\theta}}(\bm{x}_{t},t,c_{o})-\bm{\epsilon_{ \theta}}(\bm{x}_{t},t,c_{n})))}{\alpha_{t}},\] (12)

to be the posterior mean predictors when using classifier-free guidance on the original prompt \(c_{o}\), and both the original prompt \(c_{o}\) and the edit prompt \(c_{e}\) accordingly.

Algorithm.The overall method for DeepFloyd is summarized in Algorithm 4. For T2I diffusion models in the latent space such as Stable Diffusion and Latent Consistency Model, at time \(t\), we additionally decode \(\bm{\hat{z}}_{0}\) into the image space \(\bm{\hat{x}}_{0}\) to enable masking and nullspace projection. The editing is in the space of \(\bm{z}_{t}\) for Stable Diffusion and Latent Consistency Model. The proposed method is not proposed as an approach beating other T2I editing methods, but as a way to both understand semantic correspondences in the low-rank subspaces of T2I diffusion models and utilize subspaces for semantic control in a more interpretable way. We hope to inspire and open up directions in understanding T2I diffusion models and utilize the understanding in versatile applications.

Here, we want to find a specific change direction \(\bm{v}_{p}\) in the \(\bm{x}_{t}\) space that can provide target edited images in the space of \(\bm{x}_{0}\) by directly moving \(\bm{x}_{t}\) along \(\bm{v}_{p}\): the whole generation is not conditioned on \(c_{e}\) at all, except that we utilize \(c_{e}\) in finding the editing direction \(\bm{v}_{p}\). This is in contrast to the method proposed in [30], where additional semantic information is injected via indirect x-space guidance conditioned on the edit prompt at time \(t\). We hope to discover an editing direction that is expressive enough by itself to perform semantic editing.

```
1:Input: Random noise \(\bm{x}_{T}\), the mask \(\Omega\),, edit timestep \(t\), pretrained diffusion model \(\bm{\epsilon_{\theta}}\), editing scale \(\lambda\), noise scheduler \(\alpha_{t},\sigma_{t}\), selected semantic index \(k\), nullspace approximate rank \(r\), original prompt \(c_{o}\), edit prompt \(c_{e}\), null prompt \(c_{n}\), classifier free guidance scale \(s\).
2:Output: Edited image \(\bm{x}^{o}_{0}\),
3:\(\bm{x}_{t}\leftarrow\texttt{DIM}(\bm{x}_{T},1,t,\bm{\epsilon_{\theta}}(\bm{x }_{T},t,c_{n})+s(\bm{\epsilon_{\theta}}(\bm{x}_{T},t,c_{o})-\bm{\epsilon_{ \theta}}(\bm{x}_{T},t,c_{n})))\)
4:\(\bm{\hat{x}}^{o}_{0,t}\leftarrow\bm{f}^{o}_{\theta,t}(\bm{x}_{t})\)
5:\(\bm{\hat{x}}^{o}_{0,t}\leftarrow\bm{f}^{o}_{\theta,t}(\bm{x}_{t})\)
6:\(d\leftarrow\mathcal{P}_{\Omega}\left(\bm{\hat{x}}^{o}_{0,t}-\bm{\hat{x}}^{o}_{0,t}\right)\)
7:\(\bm{\hat{x}}_{0,t}\leftarrow\mathcal{P}_{\Omega}(\bm{\hat{x}}^{o}_{0,t})\)
8:\(\bm{v}\leftarrow\frac{\partial(\bm{d}^{\top}\bm{\hat{x}}_{0,t})}{\partial\bm{x }_{t}}\)\(\triangleright\) Get text-supervised editing direction within the mask
9:\(\bm{\hat{x}}_{0,t}\leftarrow\bm{\hat{x}}^{o}_{0,t}-\mathcal{P}_{\Omega}(\bm{ \hat{x}}^{o}_{0,t})\)
10: The top-\(r\) SVD \((\bm{\tilde{U}}_{t,r},\bm{\hat{x}}_{1,r},\bm{\tilde{V}}_{t,r})\) of \(\bm{\tilde{J}}_{\bm{\theta},t}=\frac{\partial\bm{\bar{x}}_{0,t}}{\partial\bm{x }_{t}}\)\(\triangleright\) Efficiently computed via generalized power method
11:\(\bm{v}_{p}\leftarrow(\bm{I}-\bm{\tilde{V}}_{t,r},\bm{\tilde{V}}^{\top}_{t,r})\)\(\triangleright\) nullspace projection for editing within the mask
12:\(\bm{v}_{p}\leftarrow\frac{\bm{v}_{p}}{\|\bm{v}_{p}\|^{2}_{2}}\)\(\triangleright\) Normalize the editing direction
13:\(\bm{x}^{\prime}_{t}\leftarrow\bm{x}_{t}+\lambda\bm{v}_{p}\)
14:\(\bm{x}^{\prime}_{0}\leftarrow\texttt{DIM}(\bm{x}^{\prime}_{t},t,0,\bm{\epsilon_ {\theta}}(\bm{x}_{t},t,c_{n})+s(\bm{\epsilon_{\theta}}(\bm{x}_{t},t,c_{o})- \bm{\epsilon_{\theta}}(\bm{x}_{t},t,c_{n})))\) ```

**Algorithm 4** Text-supervised T-LOCO Edit for T2I diffusion models

Intuition.Let \(\bm{\hat{x}}^{o}_{0,t}\) be the estimated posterior mean conditioned on the original prompt \(c_{o}\), and \(\bm{\hat{x}}^{e}_{0,t}\) be the estimated posterior mean conditioned on both the original prompt \(c_{o}\) and the edit prompt \(c_{e}\). Let \(\bm{J}^{o}_{\bm{\theta},t}\) and \(\bm{J}^{e}_{\bm{\theta},t}\) be their Jacobian over the noisy image \(\bm{x}_{t}\) accordingly. The key intuition inspired by the unconditional cases are: i) the target editing direction \(\bm{v}\) in the \(\bm{x}_{t}\) space is homogeneous between the subspaces in \(\bm{J}^{o}_{\bm{\theta},t}\) and \(\bm{J}^{e}_{\bm{\theta},t}\); ii) the founded editing direction \(\bm{v}\) can effectively reside in the direction of a right singular vector for both \(\bm{J}^{o}_{\bm{\theta},t}\) and \(\bm{J}^{e}_{\bm{\theta},t}\); iii) \(\bm{\hat{x}}^{e}_{0,t}\) and \(\bm{\hat{x}}^{o}_{0,t}\) are locally linear.

Define \(\bm{\hat{x}}^{e}_{0,t}-\bm{\hat{x}}^{o}_{0,t}=\bm{d}\) as the change of estimated posterior mean conditioned on the original prompt \(c_{o}\), Let \(\bm{J}^{e}_{\bm{\theta},t}=\bm{U}^{e}_{t}\bm{S}^{e}_{t}\bm{V}^{e^{T}}_{t}\), then \(\bm{v}=\pm\bm{v}^{e}_{t}\) for some \(i\). Besides, we have \(\bm{\hat{x}}^{e}_{0,t}=\bm{\hat{x}}^{o}_{0,t}+\lambda^{o}\bm{J}^{o}_{\bm{ \theta},t}\bm{v}\) and \(\bm{\hat{x}}^{o}_{0,t}=\bm{\hat{x}}^{o}_{0,t}+\lambda^{e}\bm{J}^{e}_{\bm{ \theta},t}\bm{v}\) due to homogeneity and linearity. Hence, \(\bm{d}=-\lambda^{e}\bm{J}^{e}_{\bm{\theta},t}\bm{v}=\pm\lambda^{e}s^{e}_{t}\bm {u}^{e}_{t}\) and then \(\bm{J}^{e}_{\bm{\theta},t}\bm{d}=\pm\lambda^{e}s^{e}_{i}s^{e}_{i}\bm{v}^{e}_{i}= \pm\lambda^{e}s^{e}_{i}s^{e}_{i}\bm{v}\), which is along the desired direction \(\bm{v}\). And this \(\bm{v}\) identified through the subspace in \(\bm{J}^{e}_{\bm{\theta},t}\) can be effectively transferred in \(\bm{J}^{o}_{\bm{\theta},t}\) for controlling the editing of target semantics. We further apply nullspace projection based on \(\bm{J}^{o}_{\bm{\theta},t}\) to obtain the final editing direction \(\bm{v}_{p}\).

## Appendix F Proofs in Section 4

### Proofs of Lemma 1

Proof of Lemma 1.: Under the Assumption 1, we could calculate the noised distribution \(p_{t}(\bm{x}_{t})\) at any timestep \(t\),

\[p_{t}(\bm{x}_{t}) =\frac{1}{K}\sum_{k=1}^{K}p_{t}(\bm{x}_{t}|^{\top}\bm{x}_{0}\text{ belongs to class }k")\] \[=\frac{1}{K}\sum_{k=1}^{K}\int p_{t}(\bm{x}_{t}|\bm{x}_{0}=\bm{M}_ {k}\bm{a}_{k},"\bm{x}_{0}\text{ belongs to class }k")\mathcal{N}(\bm{a}_{k};\bm{0},\bm{I}_{r_{k}})d\bm{a}_{k}.\]Because \(a_{k}\sim\mathcal{N}(\mathbf{0},\bm{I}_{r_{k}})\), \(p_{t}(\bm{x}_{t}|\bm{x}_{0}=\bm{M}_{k}\bm{a}_{k},"\bm{x}_{0}\) belongs to class \(k")\sim\mathcal{N}(\sqrt{\alpha_{t}}\bm{M}_{k}\bm{a}_{k},(1-\alpha_{t})\bm{I} _{d})\). From the relationship between conditional Gaussian distribution and marginal Gaussian distribution, it is easy to show that \(p_{t}(\bm{x}_{t}|^{\top}\bm{x}_{0}\) belongs to class \(k")\sim\mathcal{N}(\mathbf{0},\alpha_{t}\bm{M}_{k}\bm{M}_{k}^{\top}+(1- \alpha_{t})\bm{I}_{d})\) Then, we have

\[p_{t}(\bm{x}_{t})=\frac{1}{K}\sum_{k=1}^{K}\mathcal{N}(\mathbf{0},\alpha_{t} \bm{M}_{k}\bm{M}_{k}^{\top}+(1-\alpha_{t})\bm{I}_{d}).\]

Next, we compute the score function as follows:

\[\nabla_{\bm{x}_{t}}\text{log}p_{t}(\bm{x}_{t}) =\frac{\nabla_{\bm{x}_{t}}p_{t}(\bm{x}_{t})}{p_{t}(\bm{x}_{t})}\] \[=\frac{\sum_{k=1}^{K}\mathcal{N}(\mathbf{0},\alpha_{t}\bm{M}_{k} \bm{M}_{k}^{\top}+(1-\alpha_{t})\bm{I}_{d})\left(-\frac{1}{1-\alpha_{t}}\bm{x} _{t}+\frac{\alpha_{t}}{1-\alpha_{t}}\bm{M}_{k}\bm{M}_{k}^{\top}\bm{x}_{t} \right)}{\sum_{k=1}^{K}\mathcal{N}(\mathbf{0},\alpha_{t}\bm{M}_{k}\bm{M}_{k}^{ \top}+(1-\alpha_{t})\bm{I}_{d})}\] \[=-\frac{1}{1-\alpha_{t}}\bm{x}_{t}+\frac{\alpha_{t}}{1-\alpha_{t} }\frac{\sum_{k=1}^{K}\mathcal{N}(\mathbf{0},\alpha_{t}\bm{M}_{k}\bm{M}_{k}^{ \top}+(1-\alpha_{t})\bm{I}_{d})\bm{M}_{k}\bm{M}_{k}^{\top}\bm{x}_{t}}{\sum_{k=1 }^{K}\mathcal{N}(\mathbf{0},\alpha_{t}\bm{M}_{k}\bm{M}_{k}^{\top}+(1-\alpha_{t })\bm{I}_{d})}.\]

Based on Tweedie's formula [45, 82], the relationship between the score function and posterior is

\[\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]=\frac{\bm{x}_{t}+(1-\alpha_{t})\nabla_{\bm{ x}_{t}}\text{log}p_{t}(\bm{x}_{t})}{\sqrt{\alpha_{t}}}.\] (13)

Therefore, the posterior mean is

\[\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}] =\sqrt{\alpha_{t}}\frac{\sum_{k=1}^{K}\mathcal{N}(\mathbf{0}, \alpha_{t}\bm{M}_{k}\bm{M}_{k}^{\top}+(1-\alpha_{t})\bm{I}_{d})\bm{M}_{k}\bm{ M}_{k}^{\top}\bm{x}_{t}}{\sum_{k=1}^{K}\mathcal{N}(\mathbf{0},\alpha_{t}\bm{M}_{k} \bm{M}_{k}^{\top}+(1-\alpha_{t})\bm{I}_{d})}\] \[=\sqrt{\alpha_{t}}\frac{\sum_{k=1}^{K}\exp\left(-\frac{1}{2}\bm{x }_{t}^{\top}\left(\alpha_{t}\bm{M}_{k}\bm{M}_{k}^{\top}+(1-\alpha_{t})\bm{I}_ {d}\right)^{-1}\bm{x}_{t}\right)\bm{M}_{k}\bm{M}_{k}^{\top}\bm{x}_{t}}{\sum_{k =1}^{K}\exp\left(-\frac{1}{2}\bm{x}_{t}^{\top}\left(\alpha_{t}\bm{M}_{k}\bm{M}_ {k}^{\top}+(1-\alpha_{t})\bm{I}_{d}\right)^{-1}\bm{x}_{t}\right)}\] \[=\sqrt{\alpha_{t}}\frac{\sum_{k=1}^{K}\exp\left(-\frac{1}{2(1- \alpha_{t})}\left(\|\bm{x}_{t}\|^{2}-\alpha_{t}\|\bm{M}_{k}^{\top}\bm{x}_{t}\| ^{2}\right)\right)\bm{M}_{k}\bm{M}_{k}^{\top}\bm{x}_{t}}{\sum_{k=1}^{K}\exp \left(-\frac{1}{2(1-\alpha_{t})}\left(\|\bm{x}_{t}\|^{2}-\alpha_{t}\|\bm{M}_{k}^ {\top}\bm{x}_{t}\|^{2}\right)\right)}\] \[=\sqrt{\alpha_{t}}\frac{\sum_{k=1}^{K}\exp\left(\frac{\alpha_{t}}{ 2(1-\alpha_{t})}\|\bm{M}_{k}^{\top}\bm{x}\|^{2}\right)\bm{M}_{k}\bm{M}_{k}^{ \top}\bm{x}_{t}}{\sum_{k=1}^{K}\exp\left(\frac{\alpha_{t}}{2(1-\alpha_{t})}\| \bm{M}_{k}^{\top}\bm{x}\|^{2}\right)},\]

where the third equation is obtained by Woodbury formula [83]\((\alpha_{t}\bm{M}_{k}\bm{M}_{k}^{\top}+(1-\alpha_{t})\bm{I}_{d})^{-1}=\frac{1}{ 1-\alpha_{t}}\left(\bm{I}_{d}-\alpha_{t}\bm{M}_{k}\bm{M}_{k}^{\top}\right)\).

### Proofs of Theorem 1

**Lemma 2**.: _The jacobian of the poster mean is_

\[\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right] =\sqrt{\alpha_{t}}\underbrace{\sum_{k=1}^{K}\omega_{k}(\bm{x}_{t} )\bm{M}_{k}\bm{M}_{k}^{\top}}_{\bm{A}:=}\] (14) \[+\frac{\alpha_{t}\sqrt{\alpha_{t}}}{(1-\alpha_{t})}\underbrace{ \sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k}\bm{M}_{k}^{\top}\bm{x}_{t}\bm{ x}_{t}^{\top}\bm{M}_{k}\bm{M}_{k}^{\top}}_{\bm{B}:=}\] \[-\frac{\alpha_{t}\sqrt{\alpha_{t}}}{(1-\alpha_{t})}\underbrace{ \left(\sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k}\bm{M}_{k}^{\top}\right) \bm{x}_{t}\bm{x}_{t}^{\top}\left(\sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k }\bm{M}_{k}^{\top}\right)^{\top}}_{\bm{C}:=},\]

_where \(\omega_{k}(\bm{x}_{t})\coloneqq\frac{\exp\left(\frac{\alpha_{t}}{2 \left(1-\alpha_{t}\right)}\|\bm{M}_{k}^{\top}\bm{x}_{t}\|^{2}\right)}{\sum_{l= 1}^{K}\exp\left(\frac{\alpha_{t}}{2(1-\alpha_{t})}\|\bm{M}_{l}^{\top}\bm{x}\|^ {2}\right)}\)_

Proof of Lemma 2.: Let \(\omega_{k}(\bm{x}_{t})\coloneqq\frac{\exp\left(\frac{\alpha_{t}}{2 \left(1-\alpha_{t}\right)}\|\bm{M}_{k}^{\top}\bm{x}_{t}\|^{2}\right)}{\sum_{l= 1}^{K}\exp\left(\frac{\alpha_{t}}{2(1-\alpha_{t})}\|\bm{M}_{l}^{\top}\bm{x}\|^ {2}\right)}\), so we have:

\[\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right] =\sqrt{\alpha_{t}}\sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k} \bm{M}_{k}^{\top}\bm{x}_{t}\] \[\nabla_{\bm{x}_{t}}\omega_{k}(\bm{x}_{t}) =\frac{\alpha_{t}}{(1-\alpha_{t})}\omega_{k}(\bm{x}_{t})\left[ \bm{M}_{k}\bm{M}_{k}^{\top}\bm{x}_{t}-\sum_{l=1}^{K}\omega_{l}(\bm{x}_{t})\bm {M}_{l}\bm{M}_{l}^{\top}\bm{x}_{t}\right]\]

So:

\[\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right] =\sqrt{\alpha_{t}}\sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k} \bm{M}_{k}^{\top}+\sqrt{\alpha_{t}}\sum_{k=1}^{K}\nabla_{\bm{x}_{t}}\omega_{k} (\bm{x}_{t})\bm{x}_{t}^{\top}\bm{M}_{k}\bm{M}_{k}^{\top}\] \[=\sqrt{\alpha_{t}}\sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k} \bm{M}_{k}^{\top}\] \[+\frac{\alpha_{t}\sqrt{\alpha_{t}}}{(1-\alpha_{t})}\sum_{k=1}^{K} \omega_{k}(\bm{x}_{t})\bm{M}_{k}\bm{M}_{k}^{\top}\bm{x}_{t}\bm{x}_{t}^{\top} \bm{M}_{k}\bm{M}_{k}^{\top}\] \[-\frac{\alpha_{t}\sqrt{\alpha_{t}}}{(1-\alpha_{t})}\left(\sum_{k=1 }^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k}\bm{M}_{k}^{\top}\right)\bm{x}_{t}\bm{x}_ {t}^{\top}\left(\sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k}\bm{M}_{k}^{ \top}\right)^{\top}.\]

**Lemma 3**.: _Assume second-order partial derivatives of \(p_{t}(\bm{x}_{t})\) exist for any \(\bm{x}_{t}\), then the posterior mean \(\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]\) satisfied \(\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]=\nabla_{\bm{x} _{t}}\mathbb{E}^{\top}\left[\bm{x}_{0}|\bm{x}_{t}\right]\)._

Proof of Lemma 3.: By taking the gradient of Equation (13) with respect to \(\bm{x}_{t}\) for both side, because the second-order partial derivatives of \(p_{t}(\bm{x}_{t})\) exist for any \(\bm{x}_{t}\), we have:

\[\nabla_{\bm{x}_{t}}\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]=\frac{\bm{I}+(1-\alpha_{t} )\nabla_{\bm{x}_{t}}^{2}\text{log}p_{t}(\bm{x}_{t})}{\sqrt{\alpha_{t}}}.\]The hessian of log\(p_{t}(\bm{x}_{t})\) is symmetric, so we have:

\[\nabla_{\bm{x}_{t}}\mathbb{E}^{\top}[\bm{x}_{0}|\bm{x}_{t}]=\frac{\bm{I}+(1- \alpha_{t})\left(\nabla_{\bm{x}_{t}}^{2}\text{log}p_{t}(\bm{x}_{t})\right)^{ \top}}{\sqrt{\alpha_{t}}}=\frac{\bm{I}+(1-\alpha_{t})\nabla_{\bm{x}}^{2}\text {log}p_{t}(\bm{x}_{t})}{\sqrt{\alpha_{t}}}=\nabla_{\bm{x}_{t}}\mathbb{E}[\bm{ x}_{0}|\bm{x}_{t}].\]

Notably, the symmetric of \(\nabla_{\bm{x}_{t}}\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\) holds without the Assumption 1. 

Proof of Theorem 1.: **First, let's prove the low-rankness of the posterior mean**. From Lemma 2,

\[\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right] =\sqrt{\alpha_{t}}\bm{A}+\frac{\alpha_{t}\sqrt{\alpha_{t}}}{(1- \alpha_{t})}\bm{B}-\frac{\alpha_{t}\sqrt{\alpha_{t}}}{(1-\alpha_{t})}\bm{C}\] \[=\sum_{k=1}^{K}\bm{M}_{k}\bm{M}_{k}^{\top}\left(\sqrt{\alpha_{t}} \bm{A}+\frac{\alpha_{t}\sqrt{\alpha_{t}}}{(1-\alpha_{t})}\bm{B}-\frac{\alpha_ {t}\sqrt{\alpha_{t}}}{(1-\alpha_{t})}\bm{C}\right),\]

where the second equation is obtained due to the fact that \(\sum_{k=1}^{K}\bm{M}_{k}\bm{M}_{k}^{\top}\bm{A}=\bm{A},\sum_{k=1}^{K}\bm{M}_{k }\bm{M}_{k}^{\top}\bm{B}=\bm{B},\sum_{k=1}^{K}\bm{M}_{k}\bm{M}_{k}^{\top}\bm{C }=\bm{C}\). Therefore, we have:

\[\begin{split} rank\left(\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_ {0}|\bm{x}_{t}\right]\right)&=rank\left(\sum_{k=1}^{K}\bm{M}_{k} \bm{M}_{k}^{\top}\left(\sqrt{\alpha_{t}}\bm{A}+\frac{\alpha_{t}\sqrt{\alpha_{ t}}}{(1-\alpha_{t})}\bm{B}-\frac{\alpha_{t}\sqrt{\alpha_{t}}}{(1-\alpha_{t})}\bm{C} \right)\right)\\ &\leq rank\left(\sum_{k=1}^{K}\bm{M}_{k}\bm{M}_{k}^{\top}\right) =\sum_{k=1}^{K}r_{k}\end{split}\] (15)

**Then, we prove the linearity**:

\[\bigcirc:\|\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}+\lambda\Delta \bm{x}\right]-\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]-\lambda\nabla_{\bm {x}_{t}}\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\Delta\bm{x}\|_{2}\] \[= ||\sqrt{\alpha_{t}}\sum_{k=1}^{K}\left(\omega_{k}(\bm{x}_{t}+ \lambda\Delta\bm{x})-\omega_{k}(\bm{x}_{t})\right)\bm{M}_{k}\bm{M}_{k}^{\top} \left(\bm{x}_{t}+\lambda\Delta\bm{x}\right)-\lambda\sum_{k=1}^{K}\nabla_{\bm{ x}_{t}}\omega_{k}(\bm{x}_{t})\bm{x}_{t}^{\top}\bm{M}_{k}\bm{M}_{k}^{\top} \Delta\bm{x}||_{2}\] \[= ||\sqrt{\alpha_{t}}\sum_{k=1}^{K}\left(\lambda\nabla_{\bm{x}_{t}} ^{\top}\omega_{k}(\bm{x}_{t}+\lambda_{1}\Delta\bm{x})\Delta\bm{x}\right)\bm{M} _{k}\bm{M}_{k}^{\top}\left(\bm{x}_{t}+\lambda\Delta\bm{x}\right)-\lambda\sum_{ k=1}^{K}\nabla_{\bm{x}_{t}}\omega_{k}(\bm{x}_{t})\bm{x}_{t}^{\top}\bm{M}_{k}\bm{M}_{k}^{ \top}\Delta\bm{x}||_{2}\] \[\leq \lambda\left(\sum_{k=1}^{K}\sqrt{\alpha_{t}}\nabla_{\bm{x}_{t}}^{ \top}\omega_{k}(\bm{x}_{t}+\lambda_{1}\Delta\bm{x})\Delta\bm{x}||\bm{M}_{k}^{ \top}\left(\bm{x}_{t}+\lambda\Delta\bm{x}\right)||_{2}+\bm{x}_{t}^{\top}\bm{M}_ {k}\bm{M}_{k}^{\top}\Delta\bm{x}||\nabla_{\bm{x}_{t}}^{\top}\omega_{k}(\bm{x}_ {t})||_{2}\right)\] \[\leq \lambda\sum_{k=1}^{K}\left(\sqrt{\alpha_{t}}||\nabla_{\bm{x}_{t}} \omega_{k}(\bm{x}_{t}+\lambda_{1}\Delta\bm{x})||_{2}||\bm{M}_{k}^{\top}\left( \bm{x}_{t}+\lambda\Delta\bm{x}\right)||_{2}+||\nabla_{\bm{x}_{t}}\omega_{k}( \bm{x}_{t})||_{2}||\bm{M}_{k}^{\top}\bm{x}_{t}||_{2}\right)\]

where the first equation plug in the formula of \(\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]=\sqrt{\alpha_{t}} \sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k}\bm{M}_{k}^{\top}+\sqrt{\alpha_{t}} \sum_{k=1}^{K}\nabla_{\bm{x}_{t}}\omega_{k}(\bm{x}_{t})\bm{x}_{t}^{\top}\bm{M}_ {k}\bm{M}_{k}^{\top}\) and the second equation use the mean value theorem \(\omega_{k}(\bm{x}_{t}+\lambda\Delta\bm{x})-\omega_{k}(\bm{x}_{t})=\lambda\nabla_{ \bm{x}_{t}}^{\top}\omega_{k}(\bm{x}_{t}+\lambda_{1}\Delta\bm{x})\Delta\bm{x}, \lambda_{1}\in(0,\lambda)\).

\[\text{\textcircled{2}}:||\nabla_{\bm{x}_{t}}\omega_{k}(\bm{x}_{t}+ \lambda_{1}\Delta\bm{x})||_{2}\] \[= \frac{\alpha_{t}}{(1-\alpha_{t})}\omega_{k}||\bm{M}_{k}\bm{M}_{k}^{ \top}(\bm{x}_{t}+\lambda_{1}\Delta\bm{x})-\sum_{l=1}^{K}\omega_{l}\bm{M}_{l} \bm{M}_{l}^{\top}(\bm{x}_{t}+\lambda_{1}\Delta\bm{x})||_{2}\] \[\leq \frac{\alpha_{t}}{(1-\alpha_{t})}\omega_{k}\left(||\bm{M}_{k}^{ \top}\bm{x}_{t}||_{2}+\sum_{l=1}^{K}\omega_{l}||\bm{M}_{l}^{\top}\bm{x}_{t}|| _{2}+\lambda_{1}||\bm{M}_{k}^{\top}\Delta\bm{x}||_{2}+\lambda_{1}\sum_{l=1}^{ K}\omega_{l}||\bm{M}_{l}^{\top}\Delta\bm{x}||_{2}\right)\] \[\leq \frac{\alpha_{t}}{(1-\alpha_{t})}\omega_{k}\left(||\bm{M}_{k}^{ \top}||_{F}||\bm{x}_{t}||_{2}+\sum_{l=1}^{K}\omega_{l}||\bm{M}_{l}^{\top}||_{ F}||\bm{x}_{t}||_{2}+\lambda_{1}||\bm{M}_{k}^{\top}||_{F}+\lambda_{1}\sum_{l=1}^{ K}\omega_{l}||\bm{M}_{l}^{\top}||_{F}\right)\] \[\leq \frac{\alpha_{t}}{(1-\alpha_{t})}\omega_{k}\left(r_{k}+\sum_{l=1 }^{K}\omega_{l}r_{l}\right)\Big{(}\sqrt{2}\max\{||\bm{x}_{0}||_{2},||\bm{ \epsilon}||_{2}\}+\lambda_{1}\Big{)}\] \[\leq \frac{\alpha_{t}}{(1-\alpha_{t})}\omega_{k}(\bm{x}_{t}+\lambda_{1 }\Delta\bm{x})\underbrace{\cdot 2\cdot\max_{k}r_{k}\cdot\Big{(}\sqrt{2}\max\{||\bm{x}_{0}|| _{2},||\bm{\epsilon}||_{2}\}+\lambda_{1}\Big{)}}_{C_{1}=},\]

where the third inequality use the fact that \(||\bm{x}_{t}||_{2}=||\sqrt{\alpha_{t}}\bm{x}_{0}+\sqrt{1-\alpha_{t}}\bm{ \epsilon}||_{2}\leq||\sqrt{\alpha_{t}}\bm{x}_{0}||_{2}+||\sqrt{1-\alpha_{t}} \bm{\epsilon}||_{2}\leq\sqrt{2}\max\{||\bm{x}_{0}||_{2},||\bm{\epsilon}||_{2}\}\), we simplified \(\omega_{k}(\bm{x}_{t}+\lambda_{1}\Delta\bm{x})\) as \(\omega_{k}\) in this prove, and \(C_{1}\) defined in the last inequality is independent of \(t\). Similarly, we could prove that:

\[\textcircled{3}:||\bm{M}_{k}\bm{M}_{k}^{\top}\left(\bm{x}_{t}+ \lambda\Delta\bm{x}\right)||_{2}\leq\underbrace{\max_{k}r_{k}\cdot\Big{(} \sqrt{2}\max\{||\bm{x}_{0}||_{2},||\bm{\epsilon}||_{2}\}+\lambda\Big{)}}_{C_{2 }=},\] \[\textcircled{4}:||\nabla_{\bm{x}_{t}}\omega_{k}(\bm{x}_{t})||_{2} \leq\frac{\alpha_{t}}{(1-\alpha_{t})}\omega_{k}(\bm{x}_{t})\underbrace{ 2\sqrt{2}\cdot\max_{k}r_{k}\cdot\max\{||\bm{x}_{0}||_{2},||\bm{\epsilon}||_{2}\} }_{C_{3}:=},\] \[\textcircled{5}:||\bm{M}_{k}\bm{M}_{k}^{\top}\bm{x}_{t}||_{2} \leq\underbrace{\sqrt{2}\max_{k}r_{k}\cdot\max\{||\bm{x}_{0}||_{2},||\bm{\epsilon}||_{2}\}}_{C_{4}=}.\]

Here, \(C_{1}=\mathcal{O}(\lambda),C_{2}=\mathcal{O}(\lambda),C_{3}=\mathcal{O}( \lambda),C_{4}=\mathcal{O}(\lambda)\). After plugin \(\textcircled{2},\textcircled{3},\textcircled{4},\textcircled{5}\) to \(\textcircled{1}\), we could obtain:

\[||\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}+\lambda\Delta\bm{x}\right] -\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]-\lambda\nabla_{\bm{x}_{t}} \mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\Delta\bm{x}||_{2}\] \[\leq \lambda\sqrt{\alpha_{t}}\sum_{k=1}^{K}\frac{\alpha_{t}}{(1-\alpha _{t})}\omega_{k}(\bm{x}_{t}+\lambda_{1}\Delta\bm{x})C_{1}C_{2}+\lambda\sum_{k= 1}^{K}\frac{\alpha_{t}}{(1-\alpha_{t})}\omega_{k}(\bm{x}_{t})C_{3}C_{4}\] \[= \lambda\frac{\alpha_{t}}{(1-\alpha_{t})}\mathcal{O}(\lambda)\]

**Finally, let's prove the property of the left singular vector of \(\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]\)**:

From Lemma 3, the eigenvalue decomposition of \(\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]\) could be written as \(\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]=\bm{U}_{t} \Lambda_{t}\bm{U}_{t}^{\top}\), where \(\Lambda_{t}=\operatorname{diag}(\lambda_{t,1},\ldots,\lambda_{t,r},\ldots,0)\), and the relation between eigenvalue decomposition and singular value decomposition of \(\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]\) could be summarized as for all \(i\in[r]\):

\[\sigma_{t,i}=|\lambda_{t,i}|,\ \ \bm{v}_{i}=\text{sign}\left(\lambda_{t,i}\right) \bm{u}_{i},\]

where \(\text{sign}\left(\cdot\right)\) is the sign function. Therefore, we have:

\[\bm{U}_{t,1}\bm{U}_{t,1}^{\top}=\bm{V}_{t,1}\bm{V}_{t,1}^{\top},\] (16)

given \(\bm{V}_{t,1}:=[\bm{v}_{t,1},\bm{v}_{t,2},\ldots,\bm{v}_{t,r}]\). From Lemma 2, we define:

\[\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]= \sqrt{\alpha_{t}}\sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k}\bm{M}_{k}^{ \top}+\underbrace{\sqrt{\alpha_{t}}\sum_{k=1}^{K}\nabla_{\bm{x}_{t}}\omega_{k}( \bm{x}_{t})\bm{x}_{t}^{\top}\bm{M}_{k}\bm{M}_{k}^{\top}}_{\bm{\Delta}_{t}:=}.\]From the full singular value decomposition of \(\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]\) and \(\sqrt{\alpha_{t}}\sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k}\bm{M}_{k}^{\top}\):

\[\nabla_{\bm{x}_{t}}\mathbb{E}\left[\bm{x}_{0}|\bm{x}_{t}\right]= \left[\bm{U}_{t,1}\quad\bm{U}_{t,2}\right]\begin{bmatrix}\bm{\Sigma}_{t,1}&\bm {0}\\ \bm{0}&\bm{\Sigma}_{t,2}\end{bmatrix}\begin{bmatrix}\bm{V}_{t,1}\\ \bm{V}_{t,2}\end{bmatrix}^{\top},\] \[\sqrt{\alpha_{t}}\sum_{k=1}^{K}\omega_{k}(\bm{x}_{t})\bm{M}_{k} \bm{M}_{k}^{\top}=\begin{bmatrix}\hat{\bm{U}}_{t,1}&\hat{\bm{U}}_{t,2}\end{bmatrix} \begin{bmatrix}\hat{\bm{\Sigma}}_{t,1}&\bm{0}\\ \bm{0}&\hat{\bm{\Sigma}}_{t,2}\end{bmatrix}\begin{bmatrix}\hat{\bm{V}}_{t,1} \\ \hat{\bm{V}}_{t,2}\end{bmatrix}^{\top}.\]

where:

\[\bm{\Sigma}_{t,1}=\begin{bmatrix}\sigma_{t,1}&&\\ &\ddots&\\ &&\sigma_{t,r}\end{bmatrix},\bm{\Sigma}_{t,2}=\begin{bmatrix}\sigma_{t,r+1}&&\\ &\ddots&\\ &&\sigma_{t,n}\end{bmatrix},\] \[\hat{\bm{\Sigma}}_{t,1}=\begin{bmatrix}\hat{\sigma}_{t,1}&&\\ &\ddots&\\ &&\hat{\sigma}_{t,r}\end{bmatrix},\hat{\bm{\Sigma}}_{t,2}=\begin{bmatrix}\hat{ \sigma}_{t,r+1}&&\\ &\ddots&\\ &&\hat{\sigma}_{t,n}\end{bmatrix}\] \[\sigma_{t,1}\geq\sigma_{t,2}\geq\ldots\geq\sigma_{t,r}\geq\ldots \geq\sigma_{t,d},\quad\hat{\sigma}_{t,1}\geq\hat{\sigma}_{t,2}\geq\ldots\geq \hat{\sigma}_{t,r}\geq\ldots\geq\hat{\sigma}_{t,d},\,\,\,r=\sum_{k=1}^{K}r_{k}.\]

From Equation (15), we know that \(\sigma_{t,r+1}=\ldots=\sigma_{t,d}=0\). It is easy to show that:

\[\bm{M}\coloneqq\hat{V}_{t,1}=\left[\bm{M}_{s_{1}}\quad\bm{M}_{s_{2}}\quad \ldots\quad\bm{M}_{s_{K}}\right],\]

where \(\{s_{1},s_{2},\ldots,s_{K}\}=\{1,2,\ldots,K\}\) satisfied \(\omega_{s_{1}}(\bm{x}_{t})\geq\omega_{s_{2}}(\bm{x}_{t})\geq\ldots\geq\omega_{s _{K}}(\bm{x}_{t})\). And \(\hat{\sigma}_{t,r}=\sqrt{\alpha_{t}}\omega_{s_{K}}(\bm{x}_{t})=\sqrt{\alpha_{t} }\min_{k}\omega_{k}(\bm{x}_{t})\). Based on the Davis-Kahan theorem [84], we have:

\[||\left(\bm{I}_{d}-\bm{V}_{t,1}\bm{V}_{t,1}^{\top}\right)\bm{M}||_ {F} \leq\frac{||\bm{\Delta}_{t}||_{F}}{\min_{1\leq i\leq r,r+1\leq j \leq d}|\hat{\sigma}_{t,i}-\sigma_{t,j}|}\] \[=\frac{||\sqrt{\alpha_{t}}\sum_{k=1}^{K}\nabla_{\bm{x}_{t}}\omega_ {k}(\bm{x}_{t})\bm{x}_{t}^{\top}\bm{M}_{k}\bm{M}_{k}^{\top}||_{F}}{\sqrt{ \alpha_{t}}\min_{k}\omega_{k}(\bm{x}_{t})}\] \[\leq\frac{\sum_{k=1}^{K}||\nabla_{\bm{x}_{t}}\omega_{k}(\bm{x}_{t} )||_{F}||\bm{x}_{t}^{\top}\bm{M}_{k}\bm{M}_{k}^{\top}||_{F}}{\min_{k}\omega_{k }(\bm{x}_{t})}\] \[=\frac{\alpha_{t}}{1-\alpha_{t}}\frac{C_{3}C_{4}}{\min_{k}\omega_{ k}(\bm{x}_{t})}\]

Because \(\lim_{t\to 1}\min_{k}\omega_{k}(\bm{x}_{t})=\frac{1}{K}\), \(\lim_{t\to 1}\frac{\alpha_{t}}{1-\alpha_{t}}=0\), so:

\[\lim_{t\to 1}||\left(\bm{I}_{d}-\bm{V}_{t,1}\bm{V}_{t,1}^{\top} \right)\bm{M}||_{F}=0.\]

And from Equation (16), we have:

\[\lim_{t\to 1}||\left(\bm{I}_{d}-\bm{U}_{t,1}\bm{U}_{t,1}^{\top}\right)\bm{M}||_{F}=0.\]

## Appendix G Image Editing and Evaluation Experiment Details

All the experiments can be conducted with a single A40 GPU having 48G memory.

### Editing in Unconditional Diffusion Models of Different Datasets

Datasets.We demonstrate the unconditional editing method in various dataset: FFHQ [63], CelebaA-HQ [52], AFHQ [62], Flowers [61], MetFace [85], and LSUN-church [60].

Models.Following [30], we use DDPM [1] for CelebaA-HQ and LSUN-church, and DDPM trained with P2 weighting [86] for FFHQ, AFHQ, Flowers, and MetFaces. We download the official pre-trained checkpoints of resolution \(256\times 256\), and keep all model parameters frozen. We use the same linear schedule including 100 DDIM inversion steps [3] as [30]. Further, we apply quanlity boosting after \(t=0.2\) as proposed in [87].

Edit Time Steps.We empirically choose the edit time step \(t\) for different datasets in the range \([0.5,0.8]\). In practice, we found time steps within the above range give similar editing results. In most of the experiments, the edit time steps chosen are: \(0.5\) for FFHQ, \(0.6\) for CelebaA-HQ and LSUN-church, \(0.7\) for AFHQ, Flowers, and MetFace.

Editing Strength.In the empirical study of local linearity, we observed that the local linearity is well-preserved even with a strength of \(300\). In practice, we choose the edit strength \(\lambda\) in the range of \([-15.0,15.0]\), where a larger \(\alpha\) leads to stronger semantic editing and a negative \(\alpha\) leads to the change of semantics in the opposite direction.

### Comparing with Alternative Manifolds and Methods

Existing MethodsWe compare with four existing methods: NoiseCLR [23], BlendedDiffusion [24], Pullback [30], and Asyrp [29].

Alternative Manifolds.There are two alternative manifolds where similar training-free approaches can be applied, and each of the alternative involves evaluation of the Jacobians \(\dfrac{\partial\bm{\epsilon}_{t}}{\partial\bm{h}_{t}}\) (equivalently \(\dfrac{\partial\bm{\hat{x}}_{0}}{\partial\bm{h}_{t}}\)), and \(\dfrac{\partial\bm{\epsilon}_{t}}{\partial\bm{x}_{t}}\) accordingly.

* \(\dfrac{\partial\bm{\epsilon}_{t}}{\partial\bm{h}_{t}}\) (or equivalently \(\dfrac{\partial\hat{\bm{x}}_{0,t}}{\partial\bm{h}_{t}}\) up to a scale) calculates the Jacobian of the noise residual \(\bm{\epsilon}_{t}\) with respect to the bottleneck feature of \(\bm{x}_{t}\).
* \(\dfrac{\partial\bm{\epsilon}_{t}}{\partial\bm{x}_{t}}\) calculates the Jacobian of the noise residual \(\bm{\epsilon}_{t}\) with respect to the input \(\bm{x}_{t}\).

Notably, \(\dfrac{\partial\bm{\epsilon}_{t}}{\partial\bm{h}_{t}}\) has hardly notable editing results on images, and hence we present the editing results of \(\dfrac{\partial\bm{\epsilon}_{t}}{\partial\bm{x}_{t}}\). Besides, with masking and nullspace projection, \(\dfrac{\partial\bm{\epsilon}_{t}}{\partial\bm{x}_{t}}\) also leads to hardly notable changes on images, thus the final comparison is without masking and nullspace projection.

Evaluation Dataset Setup.In human evaluation, for each method, we randomly select \(15\) editing direction on \(15\) images. Each direction is transferred to \(3\) other images along both the negative and positive directions, in total \(90\) transferability testing cases. Learning time and transfer edit time are averaged over 100 examples. LPIPS [64] and SSIM [65] are calculated over \(400\) images for each method.

Human Evaluation Metrics.We measure both Local Edit Success Rate and Transfer Success Rate via human evaluation on CelebA-HQ. i) Local Edit Success Rate: The subject will be given the source image with the edited one, if the subject judges only one major feature among {"eyes", "nose", "hair", "skin", "mouth", "views", "Eyebrows"} are edited, the subject will respond a success, otherwise a failure. ii) Transfer Success Rate: The subject will be given the source image with the edited one, and another image with the edited one via transferring the editing direction from the source image. The subject will respond a success if the two edited images have the same features changed, otherwise a failure. We calculate the average success rate among all subjects for both Local Edit Success Rate and Transfer Success Rate. Lastly, we have ensured no harmful contents are generated and presented to the human subjects.

Learning Time.Learning time is a measure of the time it takes to compute local basis(training free approaches), to train an implicit function, or to optimize certain variables that help achieve editing for a specific edit method.

### Editing in T2I Diffusion Models

Models.We generalize our method to three types of T2I diffusion models: DeepFloyd [19], Stable Diffusion [4], and Latent Consistency Model [38]. We download the official checkpoints and keep all model parameters frozen. The same scheduling as that in the unconditional models is applied to DeepFloyd and Stable Diffusion, except that no quality boosting is applied. We follow the original schedule for Latent Consistency Model [38] with the number of inference steps set as \(4\).

Edit Time Steps.We empirically choose the the edit time step \(t\) as \(0.75\) for DeepFloyd and \(0.7\) for Stable Difffusin. As for Latent Consistency Model, image editing is performed at the second inference step.

Editing Strength.For unsupervised image editing, we choose \(\lambda\in[-5.0,5.0]\) in Stable Diffusion, \(\lambda\in[-15.0,15.0]\) in DeepFloyd, and \(\lambda\in[-5.0,5.0]\) in Latent Consistency Model. For text-supervised image editing, we choose \(\lambda\in[-10.0,10.0]\) in Stable Diffusion, \(\lambda\in[-50.0,50.0]\) in DeepFloyd, and \(\lambda\in[-10.0,10.0]\) in Latent Consistency Model.

## Appendix H Social Impacts and Safeguards

The paper originally presents a new image manipulation method, with a theoretical framework to deepen the understanding of diffusion models. However, there exist potential social impacts that the proposed methods can be misused in generating and manipulating harmful content. Therefore, we will release our code and models with license and ethics commitments in the future. Besides, methods for identifying and preventing such harmful behaviors are of great significance in generative models.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We present the empirical observation in Section 3.1, image edit method in Section 3, theoretical proof in Section 4 and Appendix F, and experiment results in Section 5 in details in the paper and appendix.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations and future direction of our works in Appendix A.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: For the theory presented in Section 4, we provide detailed proofs in Appendix F.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide detailed experiment setup in Appendix G for reproducing our result.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide codes and documentations at https://github.com/ChicyChen/LCOO-Edit.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide detailed experiment setup, evaluation setup, and metrics setup in Appendix G for better interpretation of our results.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Our generation experiments are conducted randomly for hundreds of times across different dataset and models.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide computation resources information in Appendix G.

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the paper's impacts in introduction and related works, as well as potential misuse and our commitment in preventing harmful behaviors in Appendix H.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We discuss potential misuse and our commitment in preventing harmful behaviors in Appendix H.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have properly credited all existing models and datasets that are related to the paper.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide detailed descriptions and implementation details for the proposed new method. We have also released codes for reproducibility.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: We provide details on human evaluation for the generated images in details in Appendix G.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [Yes]