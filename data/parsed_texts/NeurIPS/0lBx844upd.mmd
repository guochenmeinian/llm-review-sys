# _Alps_: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models

 Xiang Meng

Operations Research Center

Massachusetts Institute of Technology

mengx@mit.edu &Kayhan Behdin

Operations Research Center

Massachusetts Institute of Technology

behdink@mit.edu &Haoyue Wang

Operations Research Center

Massachusetts Institute of Technology

haoyuew@mit.edu &Rahul Mazumder

Operations Research Center

Massachusetts Institute of Technology

rahulmaz@mit.edu

###### Abstract

The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce _ALPS_, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. _ALPS_ outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the LLaMA3-8B model with 70% sparsity, _ALPS_ achieves a 29% reduction in test perplexity on the WikiText dataset and a 8% improvement in zero-shot benchmark performance compared to existing methods. Our code is available at https://github.com/mazumder-lab/ALPS.

## 1 Introduction

Large Language Models (LLMs) have revolutionized the field of natural language processing, demonstrating remarkable performance across a wide spectrum of tasks, from question answering and text generation to sentiment analysis and named entity recognition [22, 23, 19]. The success of LLMs can in part be attributed to their massive scale--state-of-the-art models like OPT-175B [10] and LLaMA3 [18] have hundreds of billions of parameters. However, this enormous size comes at a steep cost in terms of storage and computational resources. For instance, the OPT-175B model requires at least 320 GB of memory to store its parameters in half-precision (FP16) format, necessitating the use of multiple high-end GPUs for inference [14]. To make LLMs more accessible and efficient, considerable efforts have been made to compress these models, with a particular emphasis on model quantization techniques [15, 17, 18].

Network pruning [13, 14, 15], a complementary approach to quantization, has received comparatively less attention in the realm of LLMs. Pruningaims to reduce the model size by identifying and removing redundant or less important weights, resulting in a sparser and more efficient network. Traditional pruning methods rely on iterative retraining to recover accuracy after each pruning stage [14, 17, 15, 16], which can be computationally expensive and time-consuming. To address this, recent research has focused on one-shot pruning methods [13, 15] that compress a pre-trained model using only a small amount of data (e.g., a few thousand samples)--the key idea here is to perform pruning while retaining model accuracy as much as possible without expensive fine-tuning/retraining on the entire dataset. Many prior works [14, 15, 16] on one-shot pruning address such pruning-accuracy tradeoffs using optimization based approaches.

Despite the progress made in one-shot pruning, the massive scale of LLMs poses additional challenges, as many one-shot pruning methods designed for vision models cannot be directly applied due to their large model sizes. To overcome this, existing LLM pruning methods often rely on heuristic approaches to prune instead of solving optimization problems. For instance, SparseGPT [14] approximates the OBS [17] algorithm by employing partial weight updates and adaptive mask selection to reduce costly Hessian computation. Similarly, Wanda [15] prunes weights based on the product of their magnitudes and corresponding input activations. Zhang et al. [2023] propose to iteratively grow and prune the weight mask according to the change in reconstruction error achieved by each update. While these heuristics enable pruning at scale, they may lead to suboptimal compression (and hence, suboptimal compression-accuracy tradeoffs) compared to advanced optimization-based approaches, as we show in this paper.

In this paper, we propose _ALPS1_, an optimization-based framework for one-shot LLM pruning. _ALPS_ consists of two key components. First, it formulates pruning LLMs as an \(\ell_{0}\)-constrained optimization problem and solves it directly using the operator splitting technique (i.e., ADMM) [15, 16] without any simplification. The proposed algorithm simultaneously finds the support2 of the weights and updates them. After the support stabilizes, _ALPS_ fixes the support and employs preconditioned conjugate gradient (PCG) [18, Section 5] to compute the optimal weights on the support. Our modified PCG leverages sparse matrix structure (arising from pruning) and GPU computation to solve large systems efficiently, providing a significant speed advantage over direct matrix inversion. An outline of our proposed optimization-based framework _ALPS_ is given in Figure 1. Compared to previous heuristics, _ALPS_ offers higher quality supports and weights, as demonstrated in Section 4.1. This improvement translates to a better performance of the pruned model compared to existing methods, particularly in the challenging high-sparsity regime.

Footnote 1: ADMM-based **LLM Pr**uning in one-Shot

Footnote 2: The “support” of the weights refers to the set of indices corresponding to non-zero weights within a layer.

**Contributions.** Our technical contributions are:

1. We introduce _ALPS_, a novel one-shot LLM pruning framework that formulates an \(\ell_{0}\)-constrained optimization problem with a layer-wise reconstruction objective. By extending the operator splitting technique (i.e., ADMM) to this non-convex, non-continuous problem, _ALPS_ simultaneously

Figure 1: Overview of the proposed _ALPS_ algorithm. (**Left**) The pruning problem with a layerwise reconstruction objective and an \(\ell_{0}\) constraint on the weights (Section 3.1). (**Middle**) ADMM with a \(\rho\)-update scheme (Algorithm 1) is employed to determine high-quality support for the weight matrix \(\mathbf{W}\) (Section 3.2). (**Right**) The optimization problem is restricted to the obtained support, and a modified PCG method (Algorithm 2) is used to solve for the optimal weight values within the support (Section 3.3).

finds a high-quality support and updates the weights on the support. This approach leads to improvements over state-of-the-art heuristics in terms of the pruning objective. Furthermore, we provide theoretical convergence guarantees for our proposed algorithm, which, to the best of our knowledge, is a novel convergence result for \(\ell_{0}\)-constrained problems.
2. We further enhance the performance of _ALPS_ through two techniques. First, we design a novel penalty parameter updating scheme that enables _ALPS_ to find better support and accelerates its convergence. Second, we propose a post-processing technique to further improve the performance of the pruned models--we fix the support determined by ADMM and optimally solve the resulting quadratic problem using the PCG method. We utilize vectorization to solve the problem in a single pass and leverage GPU parallelism to further accelerate PCG. Our proposed method achieves a 20x-200x speedup compared to the vanilla backsolve approach.
3. _ALPS_ substantially improves upon state-of-the-art methods for one-shot unstructured pruning of LLMs. For the LLMaMA3-8B model with 70% sparsity, _ALPS_ achieves a 29% reduction in test perplexity on the WikiText dataset and a 4%-13% improvement in performance on zero-shot benchmark evaluations. We also adapt _ALPS_ to the popular N:M sparsity format (Zhou et al., 2021) and observe a 3%-10% higher performance compared to existing methods. Our code is publicly available at: https://github.com/mazumder-lab/ALPS.

## 2 Related Work

Network pruning.Network pruning is a well-established technique for reducing the complexity of deep neural networks by removing redundant weights (LeCun et al., 1989; Han et al., 2015). Pruning methods can be classified based on the structure of the resulting sparse network and the training requirements. In terms of structure, pruning can be categorized into unstructured pruning, which removes individual weights (Han et al., 2015; Guo et al., 2016), and structured pruning, which removes entire structures such as channels, filters, or attention heads (Lebedev and Lempitsky, 2016; Wen et al., 2016; Voita et al., 2019; El Halabi et al., 2022). Unstructured pruning offers better flexibility and higher sparsity levels but requires specialized hardware for acceleration, while structured pruning is more hardware-friendly but may suffer from larger performance loss. Based on the training requirements, pruning methods can be classified into three categories: (i) one-shot pruning, which directly removes weights from a pre-trained model without further training (Gale et al., 2019; Frantar and Alistarh, 2022; Meng et al., 2024, 2024), (ii) gradual pruning, which begins with a pre-trained model but alternates between pruning and fine-tuning via SGD to recover performance (Molchanov et al., 2016; Zhu and Gupta, 2017; Blalock et al., 2020; Kurtic et al., 2022), and (iii) training from scratch, where the model is trained from randomly initialized weights, and the sparse network structure is either determined before training or evolves during the training process, (Mocanu et al., 2018; Dettmers and Zettlemoyer, 2019; Evci et al., 2020; Kusupati et al., 2020; Chen et al., 2021). In this paper, we focus on one-shot unstructured pruning.

Post-training unstructured pruning.Based on their pruning objectives, there are three types of post-training unstructured pruning methods: (i) Importance-based methods, which assign a score to each weight (e.g., its absolute value) to assess its significance and decide whether it should be eliminated (Han et al., 2015; Lee et al., 2018; Molchanov et al., 2019; Sun et al., 2023). (ii) Second-order techniques, which consider a local quadratic approximation of the loss function around the pre-trained model and remove weights based on their influence on the loss (Hassibi and Stork, 1992; Singh and Alistarh, 2020; Yu et al., 2022; Benbaki et al., 2023). These approaches employ the empirical Fisher information matrix to estimate the Hessian matrix efficiently. (iii) Layer-wise pruning algorithms, which adapt OBS (Hassibi and Stork, 1992) framework to the layer-wise reconstruction objective (Dong et al., 2017; Frantar and Alistarh, 2022, 2023). These methods prune each layer separately to address the computational challenge of calculating the full Hessian required in OBS. This work considers layer-wise reconstruction error as the pruning objective.

Unstructured pruning in LLMs.While pruning algorithms designed for convolutional networks (Singh and Alistarh, 2020; Chen et al., 2020; Frantar and Alistarh, 2022) can be readily adapted to moderate-sized language models like BERT (Vaswani et al., 2017), pruning LLMs with billions of parameters presents distinct challenges. The immense model size and extensive datasets associated with LLMs render traditional pruning methods computationally infeasible (Ma et al., 2023). SparseGPT (Frantar and Alistarh, 2023) utilizes partial weight updates and adaptive mask selection to mitigate the expensive Hessian computation, while Wanda (Sun et al., 2023) directly obtains a sparse LLM model using a criterion that considers the product of the absolute values of weights and their activations. DSnoT (Zhang et al., 2023) iteratively grow and prune the weight mask according to the change in reconstruction error achieved by each update. (Boza, 2024) introduces an efficient approach to determine the optimal weights on a given support and extends this technique to develop heuristics for updating the support.

**ADMM in network pruning.** The operator-splitting technique (Boyd et al., 2011; Davis and Yin, 2016) (also known as Alternating Direction Method of Multipliers, ADMM) is a well-known approach for solving composite optimization or optimization problems with coupled variables (or constraints), and has been used earlier in network pruning. Ye et al. (2018) applied ADMM to solve the original loss function under sparsity constraint, and Boza (2024) used ADMM to solve a convex pruning problem with fixed support. Moreover, Zhang et al. (2018) employed ADMM to train deep neural networks under sparsity constraints, while Ye et al. (2019) utilized ADMM to perform concurrent adversarial training and weight pruning. Our proposed method differs significantly from previous methods in two key aspects: (i) _ALPS_ solves the pruning problem with an \(\ell_{0}\) constraint at LLM scale, simultaneously optimizes over the weights and the sparsity pattern. (ii) We introduce a novel penalty parameter update scheme that ensures convergence both practically and theoretically.

## 3 _Alps_: Effective LLM pruning in One-shot

### Problem formulation

A common approach in post-training unstructured pruning of LLMs is to decompose the full-model compression problem into layer-wise subproblems. The quality of the solution for each subproblem is assessed by measuring the \(\ell_{2}\) error between the output of the dense layer and that of the pruned one, given a set of input activations.

Formally, let \(\widehat{\mathbf{W}}\in\mathbb{R}^{N_{in}\times N_{out}}\) denote the (dense) weight matrix of layer \(\ell\), where \(N_{in}\) and \(N_{out}\) denote the input and output dimension of the layer, respectively. Given a set of \(N\) calibration samples, the input activations can be represented as \(\mathbf{X}\in\mathbb{R}^{NL\times N_{in}}\), where \(L\) is the sequence length. The goal of pruning is to find a sparse weight matrix \(\mathbf{W}\) that minimizes the reconstruction error between the original and pruned layer outputs, while satisfying a target sparsity constraint. In addition, we add a ridge term that penalizes the distance between \(\mathbf{W}\) and \(\widehat{\mathbf{W}}\), preventing \(\mathbf{W}\) from diverging too far from the original weights. This layer-wise pruning problem can be formulated as an \(\ell_{0}\)-constrained optimization problem:

\[\min_{\mathbf{W}\in\mathbb{R}^{N_{in}\times N_{out}}}~{}\|\mathbf{X}\widehat{ \mathbf{W}}-\mathbf{X}\mathbf{W}\|_{F}^{2}+\lambda_{2}\|\widehat{\mathbf{W}}- \mathbf{W}\|_{F}^{2}~{}~{}~{}\text{s.t.}~{}~{}~{}~{}\|\mathbf{W}\|_{0}\leq k,\] (1)

where \(\lambda_{2}\geq 0\) and \(\|\cdot\|_{0}\) denotes the \(\ell_{0}\)-(pseudo)norm, which counts the number of non-zero elements.

### Operator-splitting for layer-wise pruning

Optimization of Problem (1) is quite challenging: we need to simultaneously find a support of \(\mathbf{W}\) and a corresponding set of optimal weights (that minimize the objective restricted to the support). Notably, \(\mathbf{W}\) may contain over 100 million parameters in the LLM setting, making (1) even more computationally demanding. To address this, we employ an operator-splitting technique (Boyd et al., 2011; Davis and Yin, 2016) (also known as ADMM), which decomposes the problem into two computationally 'friendlier' subproblems. Specifically, we reformulate problem (1) by introducing a copy \(\mathbf{D}\) of weight matrix \(\mathbf{W}\):

\[\min_{\mathbf{W},\mathbf{D}\in\mathbb{R}^{N_{in}\times N_{out}}}~{}~{}\| \mathbf{X}\widehat{\mathbf{W}}-\mathbf{X}\mathbf{W}\|_{F}^{2}+\lambda_{2}\| \widehat{\mathbf{W}}-\mathbf{W}\|_{F}^{2}+\infty\cdot\mathbf{1}_{\|\mathbf{D} \|_{0}>k}~{}~{}~{}\text{s.t.}~{}~{}~{}~{}\mathbf{W}=\mathbf{D},\] (2)

where the penalty function "\(\infty\cdot\mathbf{1}_{\|\mathbf{D}\|_{0}>k}\)" imposes the \(\ell_{0}\) constraint \(\|\mathbf{D}\|_{0}\leq k\) by assigning a value of zero when this condition is met and infinity otherwise. This reformulation separates the objective function into two independent parts while coupling the variables \(\mathbf{W}\) and \(\mathbf{D}\) through the linear constraint \(\mathbf{W}=\mathbf{D}\). We consider the augmented Lagrangian function of this problem:

\[L_{\rho}(\mathbf{W},\mathbf{D},\mathbf{V})=\|\mathbf{X}\widehat{\mathbf{W}}- \mathbf{X}\mathbf{W}\|_{F}^{2}+\lambda_{2}\|\widehat{\mathbf{W}}-\mathbf{W}\|_ {F}^{2}+\infty\cdot\mathbf{1}_{\|\mathbf{D}\|_{0}>k}+\langle\mathbf{V}, \mathbf{W}-\mathbf{D}\rangle+\frac{\rho}{2}\|\mathbf{W}-\mathbf{D}\|_{F}^{2},\] (3)where \(\rho>0\) is the quadratic penalty parameter. We minimize the augmented Lagrangian with respect to \(\mathbf{W}\) and \(\mathbf{D}\) alternatively, followed by a dual update. We get the following update at iteration \(t\):

\[\mathbf{W}^{(t+1)} =\arg\min_{\mathbf{W}}L_{\rho}(\mathbf{W},\mathbf{D}^{(t)}, \mathbf{V}^{(t)})=\left(\mathbf{H}+\rho\mathbf{I}\right)^{-1}\Big{(}\mathbf{H} \mathbf{\widehat{W}}-\mathbf{V}^{(t)}+\rho\mathbf{D}^{(t)}\Big{)}\,,\] (4) \[\mathbf{D}^{(t+1)} =\arg\min_{\mathbf{D}}L_{\rho}(\mathbf{W}^{(t+1)},\mathbf{D}, \mathbf{V}^{(t)})=P_{k}\big{(}\mathbf{W}^{(t+1)}+\mathbf{V}^{(t)}/\rho\big{)},\] \[\mathbf{V}^{(t+1)} =\mathbf{V}^{(t)}+\rho(\mathbf{W}^{(t+1)}-\mathbf{D}^{(t+1)}),\]

where \(\mathbf{H}=\mathbf{X}^{\top}\mathbf{X}+\lambda_{2}\mathbf{I}\). Here, the \(\mathbf{W}\)-update aims to minimize the objective by solving a system of equations, while the \(\mathbf{D}\) update enforces sparsity by using the projection operator \(P_{k}(\cdot)\), which projects an input matrix onto the set of matrices with at most \(k\) non-zero elements. The dual update on matrix \(\mathbf{V}\) ensures consistency between \(\mathbf{W}\) and \(\mathbf{D}\). As the iterations (4) progress, our proposed method concurrently identifies the support of the weight matrix and updates the weights on the determined support.

\(\rho\) **update scheme.** In practice, we observe that the sequence of updates (4) and the resulting solution can be sensitive to the choice of the penalty parameter \(\rho\). A small \(\rho\) leads to slow convergence due to large changes in the support of \(\mathbf{D}\) across iterations, while a large \(\rho\) may compromise solution quality though the support stabilizes early on. To balance support quality and convergence speed, we introduce a novel penalty parameter update scheme. Starting with a small \(\rho\), we gradually increase it every few iterations, with the increase rate proportional to the change in the support of \(\mathbf{D}\). The detailed \(\rho\) update scheme is provided in Appendix B.1. This scheme allows our algorithm to explore and find a good support when \(\rho\) is small and to converge rapidly as \(\rho\) grows, as demonstrated experimentally in Appendix B.2.1.

Algorithm 1 outlines the proposed operator-splitting technique with the \(\rho\) update scheme. The convergence of Algorithm 1 is guaranteed by the following theorem, with its proof provided in Appendix A. We note that existing convergence results for operator-splitting type methods (e.g., ADMM) focus on convex or continuous problems (Hong et al., 2016; Wang et al., 2019). However, our result guarantees the convergence on a non-convex, non-continuous \(\ell_{0}\)-constrained problem, which, to the best of our knowledge, is a novel convergence result for such a problem.

**Theorem 1**.: _Let \(\big{\{}\mathbf{D}^{(t)}\big{\}}_{t=0}^{\infty}\) and \(\big{\{}\mathbf{W}^{(t)}\big{\}}_{t=0}^{\infty}\) be the sequences generated in Algorithm 1. Suppose the penalty parameter \(\{\rho_{t}\}_{t=1}^{\infty}\) chosen in Algorithm 1 satisfies \(\sum_{t=1}^{\infty}1/\rho_{t}<\infty\). It then holds_

\[\max\Big{\{}\|\mathbf{D}^{(t+1)}-\mathbf{D}^{(t)}\|_{F},\|\mathbf{W}^{(t+1)}- \mathbf{D}^{(t+1)}\|_{F}\Big{\}}\leq C/\rho_{t},\] (5)

_where \(C\) is a constant depending on \(\mathbf{X}\), \(\mathbf{\widehat{W}}\), \(\lambda_{2}\), and \(\sum_{t=1}^{\infty}1/\rho_{t}\). In particular, there exists a matrix \(\mathbf{\bar{D}}\) such that \(\mathbf{D}^{(t)}\rightarrow\mathbf{\bar{D}}\) and \(\mathbf{W}^{(t)}\rightarrow\mathbf{\bar{D}}\) as \(t\rightarrow\infty\)._

```
0: Initial penalty \(\rho_{0}\).
1: Initialize \(\mathbf{V}^{(0)}=\mathbf{0}_{N_{in}\times N_{out}}\) and \(\mathbf{D}^{(0)}=\mathbf{W}^{(0)}=\mathbf{\widehat{W}}\)
2:for\(t=0,1,\cdots\)do
3: Update \(\mathbf{W}^{(t+1)},\mathbf{D}^{(t+1)}\) and \(\mathbf{V}^{(t+1)}\) according to (4) with \(\rho=\rho_{t}\).
4: Increase \(\rho_{t}\) to get \(\rho_{t+1}\) based on the change in the support of \(\mathrm{Supp}\left(\mathbf{D}^{(t)}\right)\).
5:endfor ```

**Algorithm 1** ADMM for layer-wise pruning with \(\ell_{0}\) constraint

Computational cost.The primary computational cost of Algorithm 1 arises from the \(\mathbf{W}\)-update step in display (4), which involves solving a system of linear equations. In the update, the inverse of \(\mathbf{H}+\rho\mathbf{I}\) can be reused across iterations and needs to be updated when \(\rho\) changes. To avoid re-computing the inverse, we store the eigenvalue decomposition \(\mathbf{H}=\mathbf{Q}\mathbf{M}\mathbf{Q}^{\top}\). For varying \(\rho\) values, the inverse can be efficiently calculated as \((\mathbf{H}+\rho\mathbf{I})^{-1}=\mathbf{Q}(\mathbf{M}+\rho\mathbf{I})^{-1} \mathbf{Q}^{\top}\), requiring only a single matrix-matrix multiplication. Additionally, the term \(\mathbf{H}\mathbf{\widehat{W}}\) in \(\mathbf{W}\)-update remains constant across iterations and can be pre-computed and stored. Thus, each iteration of update (4) requires at most two matrix-matrix multiplications, leading to a time complexity of \(O(N_{in}^{2}N_{out})\).

Extension to other sparsity patterns.Algorithm 1 can be extended to support \(N:M\) sparsity (Zhou et al., 2021; Hubara et al., 2021), a pattern in which a neural network has at most \(N\) non-zeroweights in each group of \(M\) consecutive weights. This sparsity pattern enables inference time acceleration on specialized hardware like NVIDIA A100 GPUs. To accommodate \(N:M\) sparsity, we modify the \(\mathbf{D}\)-update step in (4) by replacing the projection operator \(P_{k}(\cdot)\) with a projection onto the set of matrices satisfying \(N:M\) sparsity. This modification can be easily implemented by applying magnitude pruning (Zhou et al., 2021) to \(\mathbf{W}^{(t+1)}+\mathbf{V}^{(t)}/\rho\). Our approach can be further generalized to handle other structured sparsity patterns, such as block sparsity (Gray et al., 2017) and row sparsity (Meng et al., 2024). Similar to the \(N:M\) sparsity adaptation, this is achieved by modifying the \(\mathbf{D}\)-update step to project onto the set of matrices satisfying the desired sparsity pattern. Importantly, our convergence results and the refining procedure introduced in Section 3.3 remain applicable to these various sparsity patterns.

### Efficiently refining weights after support stabilization

Our proposed \(\rho\)-update technique enables Algorithm 1 to search for a high-quality support when \(\rho\) is small, and the support stabilizes quickly as \(\rho\) increases. However, once the support stabilizes, the convergence rate of Algorithm 1 becomes slow in practice. To accelerate the optimization process on the support, we employ a Preconditioned Conjugate Gradient (PCG) (Nocedal and Wright, 1999, Section 5) method with GPU parallelism and vectorization for efficient computation.

Formally, we introduce a post-processing technique that fixes the support \(\mathcal{S}\) of the current solution \(\mathbf{W}\) and refines the solution within this support, leading to the following problem:

\[\min_{\mathbf{W}\in\mathbb{R}^{N_{in}\times N_{out}}}\ \|\mathbf{X} \widehat{\mathbf{W}}-\mathbf{X}\mathbf{W}\|_{F}^{2}+\lambda_{2}\|\widehat{ \mathbf{W}}-\mathbf{W}\|_{F}^{2}\quad\text{s.t.}\ \ \mathrm{Supp}(\mathbf{W})\subset \mathcal{S}.\] (6)

(6) decomposes into separate least squares problems across the columns of \(\mathbf{W}\). However, as illustrated in Figure 1 (Middle), the supports of the columns of \(\mathbf{W}\) are different. Using direct matrix inversion (backsolve) to solve these problems would involve solving \(N_{out}\) linear equations, each requiring the inversion of a submatrix of \(\mathbf{H}=\mathbf{X}^{\top}\mathbf{X}+\lambda_{2}\mathbf{I}\). Since the submatrices under consideration vary across different columns, parallelization is not straightforward, and we must solve \(N_{out}\) different linear equations, each with size \(O(N_{in})\). In LLMs, where \(N_{out}\) and \(N_{in}\) are of the order \(10^{4}\), this would result in a significant computational expense. We present a workaround as discussed next.

To efficiently solve problem (6), we propose using the Preconditioned Conjugate Gradient (PCG) method, a high-performance numerical linear algebra technique for approximately solving systems of equations through repeated matrix-matrix multiplications. We further enhance PCG's performance by introducing two novel acceleration strategies. First, instead of solving for each column of \(\mathbf{W}\) separately, we solve the entire problem in a single pass by directly solving the linear equation \(\mathbf{H}\mathbf{W}=\mathbf{H}\widehat{\mathbf{W}}\) using PCG and projecting \(\mathbf{W}\) onto the given support \(\mathcal{S}\) in each iteration (Algorithm 2, line 8). We leverage vectorization to significantly enhance the speed. Second, we perform the matrix-matrix multiplications involved in PCG on the GPU, further utilizing GPU acceleration to expedite the computations. Algorithm 2 provides the detailed steps for the PCG method, and Figure1 offers an overview of our proposed _ALPS_ method.

```
1:Support \(\mathcal{S}\), pre-conditioner \(\mathbf{M}=\mathrm{Diag}(\mathbf{H})\), initial solution \(\mathbf{W_{0}}\)
2:Set \(\mathbf{R}_{0}:=\mathbf{H}(\widehat{\mathbf{W}}-\mathbf{W}_{0})\)
3:Project \(\mathbf{R}_{0}\) onto the support \(\mathcal{S}\) by setting all elements outside the support to zero.
4:Set \(\mathbf{Z}_{0}=\mathbf{M}^{-1}\mathbf{R}_{0}\) and \(\mathbf{P}_{0}=\mathbf{Z}_{0}\)
5:for\(t=0,1,\ldots\)do
6:\(\alpha_{t}=\mathrm{Tr}(\mathbf{R}_{t}^{\top}\mathbf{Z}_{t})/\,\mathrm{Tr}( \mathbf{P}_{t}^{\top}\mathbf{H}\mathbf{P}_{t})\)
7:\(\mathbf{W}_{t+1}=\mathbf{W}_{t}+\alpha_{t}\mathbf{P}_{t}\)
8:\(\mathbf{R}_{t+1}=\mathbf{R}_{t}-\alpha_{t}\mathbf{H}\mathbf{P}_{t}\)
9:Project \(\mathbf{R}_{t+1}\) onto the support \(\mathcal{S}\) by setting all elements outside the support to zero.
10:\(\mathbf{Z}_{t+1}=\mathbf{M}^{-1}\mathbf{R}_{t+1}\)
11:if\(\mathbf{R}_{t+1}\) is sufficiently small then
12:break
13:endif
14:\(\beta_{t}=\mathrm{Tr}(\mathbf{R}_{t+1}^{\top}\mathbf{Z}_{t+1})/\,\mathrm{Tr}( \mathbf{R}_{t}^{\top}\mathbf{Z}_{t})\)
15:\(\mathbf{P}_{t+1}:=\mathbf{Z}_{t+1}+\beta_{t}\mathbf{P}_{t}\)
16:endfor ```

**Algorithm 2** PCG with vectorization for solving problem (6)Experimental Results

This section compares our proposed framework, _ALPS_, with state-of-the-art unstructured pruning methods for LLMs. Detailed information on the experimental setup and reproducibility is provided in Appendix B.1, while additional results are presented in Appendix B.2.

**Models and datasets.** We evaluate the performance of _ALPS_ on the OPT model family (Zhang et al., 2022) with sizes ranging from 1.3 billion to 30 billion parameters, the LLaMA2 model family (Touvron et al., 2023) with 7 billion and 13 billion parameters, and the LLaMA3 model (Dubey et al., 2024) with 8 billion parameters. Following the approach of Frantar and Alistarh, 2023, we use 128 segments of 2048 tokens each, randomly selected from the first shard of the C4 dataset (Raffel et al., 2020), as calibration data. We assess the performance using perplexity and zero-shot evaluation benchmarks, with perplexity calculated according to the procedure described by HuggingFace (Per, 2022), using full stride. The test sets of raw-WikiText2 (Merity et al., 2017), PTB (Marcus et al., 1994), and a subset of the C4 validation data, which are popular benchmarks in LLM pruning literature (Yao et al., 2022; Xiao et al., 2023; Meng et al., 2024), are used for evaluation. Additionally, we consider five zero-shot tasks: MMLU (Hendrycks et al., 2021), PIQA (Bisk et al., 2020), LAMBADA (Paperno et al., 2016), ARC-Easy and ARC-Challenge (Clark et al., 2018).

**Competing methods.** We compare _ALPS_ with several one-shot pruning methods for LLMs, including (i) Magnitude Pruning (MP, (Han et al., 2015)), (ii) SparseGPT (Frantar and Alistarh, 2023), (iii) Wanda Sun et al. (2023), and (iv) DSnoT (Zhang et al., 2023).

### Reconstruction error on a single layer

We first evaluate the performance of our proposed _ALPS_ framework on a single layer. Specifically, we prune a linear layer in the OPT-13B model with input and output dimensions of \(5120\) to various sparsity levels and compute the relative reconstruction error of the pruned weight \(\mathbf{W}\) using \(\|\mathbf{X}\widehat{\mathbf{W}}-\mathbf{X}\mathbf{W}\|_{F}^{2}/\|\mathbf{X} \widehat{\mathbf{W}}\|_{F}^{2}\). The results are shown in Figure 2. As demonstrated, _ALPS_ achieves significantly lower reconstruction errors compared to other methods, especially at high sparsity levels. For instance, at a sparsity level of \(0.8\), _ALPS_ yields a \(7.6\%\) relative reconstruction error, while SparseGPT shows a 12% error, and other methods exceed \(20\%\). As demonstrated in Sections 4.2 and 4.3, our method's superior ability to approximate the dense model's output at each layer translates to much better performance in the pruned model.

We attribute the superior performance of _ALPS_ in solving the reconstruction problem at each layer to two key aspects: (i) Algorithm 1 obtains a high-quality support by directly optimizing for an optimal subset of weights that contribute the most to recovering the dense model's output (ii) The PCG method in Algorithm 2 efficiently solves the reconstruction problem on a fixed support, further reducing the reconstruction error. To verify these claims, we conducted the following two ablation studies.

Firstly, we compare the quality of the support determined by various pruning methods. For each method, we prune the layer to different sparsity levels and fix the support of the weights matrix provided by the method. We then solve the post-processing problem (6) with this support to optimality and compute the relative reconstruction error of the resulting weights. This approach ensures that the reconstruction error depends solely on the quality of the support. Table 1 (left) presents the performance of each method. As shown, the support determined by _ALPS_ yields \(20\%\sim 40\%\) lower reconstruction error compared to other methods, demonstrating its effectiveness in finding high-quality supports.

Figure 2: Performance analysis of pruning the “self_attn_k_proj” layer in the first block of the OPT-13B model at various sparsity levels. The plot shows the relative reconstruction error of pruned weights, comparing different pruning methods.

[MISSING_PAGE_FAIL:8]

performance of the pruned LLaMA3-8B model at different sparsity levels on the WikiText2 and PIQA datasets is presented in Figure 3. Table 2 showcases the performance of OPT models with 70% sparsity on various datasets. Additional results on different models, sparsity levels, and datasets are provided in Appendix B.2.5.

Figure 3 demonstrates that _ALPS_ outperforms other competitors when sparsity levels exceed 50%, and the performance gap between _ALPS_ and other methods widens as the sparsity level increases. For instance, _ALPS_ achieves a \(60\%\) perplexity reduction on the WikiText2 dataset compared to other methods at \(80\%\) sparsity level. This observation aligns with our findings in Section 4.1, confirming that _ALPS_'s highly advanced optimization method in solving layer-wise reconstruction problems enables it to better preserve performance at medium-to-high sparsity levels compared to other methods. Table 2 further validates this fact, showing that _ALPS_ outperforms other methods by a large margin across all models on all criteria. This suggests the superiority of _ALPS_ in pruning models at medium-to-high sparsity levels.

### N:M sparsity

We further assess _ALPS_'s performance on \(N:M\) sparsity patterns, with Table 3 listing the results for pruning OPT-30B and LLaMA2-13B models at 2:4 and 4:8 sparse patterns (see Appendix B.2.5 for other models). _ALPS_ outperforms other methods on most datasets, achieving larger performance improvements in \(N:M\) pruning compared to unstructured pruning at the same sparsity level. This is due to the higher complexity of the \(N:M\) sparsity pruning problem, which _ALPS_, as a highly advanced optimization algorithm, can handle more effectively than competing heuristics.

## 5 Conclusion

We present _ALPS_, an efficient optimization-based framework for one-shot unstructured LLM pruning. _ALPS_ employs the operator splitting technique to effectively solve the \(\ell_{0}\)-constrained layer-wise pruning problem. To enhance the performance of our algorithm, we introduce a novel penalty parameter updating scheme and a post-processing procedure using PCG with vectorization/GPU parallelism that takes into account problem-structure. We also establish novel convergence guarantees for our algorithm. _ALPS_ can efficiently perform high-quality pruning of LLMs at scale. Our experiments confirm that _ALPS_ outperforms existing pruning methods in terms of both the pruning objective and the performance of the pruned model. Future work will consider extending _ALPS_ to incorporate structured pruning constraints and quantization to get a better understanding of the strengths and scope of our optimization-based approach.

Figure 3: Performance analysis for one-shot unstructured pruning of LLaMA3-8B model at various sparsity levels on two datasets: WikiText2 (**Left**) and PIQA (**Right**). We run each method five times and plot the shaded region as the area between the mean (solid line) and two standard deviations above and below the mean.

[MISSING_PAGE_FAIL:10]

## References

* Perplexity of fixed-length models (2022) Perplexity of fixed-length models, 2022. URL https://huggingface.co/docs/transformers/perplexity.
* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Behdin et al. (2023) Kayhan Behdin, Ayan Acharya, Aman Gupta, Sathiya Keerthi, and Rahul Mazumder. Quantease: Optimization-based quantization for language models-an efficient and intuitive algorithm. _arXiv preprint arXiv:2309.01885_, 2023.
* Benbaki et al. (2023) Riade Benbaki, Wenyu Chen, Xiang Meng, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, and Rahul Mazumder. Fast as chita: Neural network pruning with combinatorial optimization. _arXiv preprint arXiv:2302.14623_, 2023.
* Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 7432-7439, 2020.
* Blalock et al. (2020) Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural network pruning? _Proceedings of machine learning and systems_, 2:129-146, 2020.
* Boyd et al. (2011) Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers. _Foundations and Trends(r) in Machine learning_, 3(1):1-122, 2011.
* Boza (2024) Vladimir Boza. Fast and optimal weight update for pruned large language models. _arXiv preprint arXiv:2401.02938_, 2024.
* Bubeck et al. (2023) Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* Chen et al. (2020) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. _Advances in neural information processing systems_, 33:15834-15846, 2020.
* Chen et al. (2021) Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng Yi, and Xiao Tu. Only train once: A one-shot neural network training and pruning framework. _Advances in Neural Information Processing Systems_, 34:19637-19651, 2021.
* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* Davis and Yin (2016) Damek Davis and Wotao Yin. Convergence rate analysis of several splitting schemes. _Splitting methods in communication, imaging, science, and engineering_, pages 115-163, 2016.
* Dettmers and Zettlemoyer (2019) Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. _arXiv preprint arXiv:1907.04840_, 2019.
* Dettmers et al. (2023) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuzmedelev, Elias Frantz, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression. _arXiv preprint arXiv:2306.03078_, 2023.
* Dong et al. (2017) Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. _Advances in neural information processing systems_, 30, 2017.
* Dubey et al. (2024) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. _arXiv preprint arXiv:2407.21783_, 2024.
* D'Alessio et al. (2018)* El Halabi et al. (2022) Marwa El Halabi, Suraj Srinivas, and Simon Lacoste-Julien. Data-efficient structured pruning via submodular optimization. _Advances in Neural Information Processing Systems_, 35:36613-36626, 2022.
* Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In _International conference on machine learning_, pages 2943-2952. PMLR, 2020.
* Frantar and Alistarh (2022) Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning. _Advances in Neural Information Processing Systems_, 35:4475-4488, 2022.
* Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In _International Conference on Machine Learning_, pages 10323-10337. PMLR, 2023.
* Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. _arXiv preprint arXiv:1902.09574_, 2019.
* Gray et al. (2017) Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. _arXiv preprint arXiv:1711.09224_, 3(2):2, 2017.
* Guo et al. (2016) Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. _Advances in neural information processing systems_, 29, 2016.
* Han et al. (2015) Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. _Advances in neural information processing systems_, 28, 2015.
* Hassibi and Stork (1992) Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. _Advances in neural information processing systems_, 5, 1992.
* He et al. (2017) Y. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. In _2017 IEEE International Conference on Computer Vision (ICCV)_, pages 1398-1406, Los Alamitos, CA, USA, oct 2017. IEEE Computer Society. doi: 10.1109/ICCV.2017.155. URL https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.155.
* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021.
* Hong et al. (2016) Mingyi Hong, Zhi-Quan Luo, and Meisam Razaviyayn. Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems. _SIAM Journal on Optimization_, 26(1):337-364, 2016.
* Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* Hubara et al. (2021) Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel Soudry. Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks. _Advances in neural information processing systems_, 34:21099-21111, 2021.
* Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert surgeon: Scalable and accurate second-order pruning for large language models. _arXiv preprint arXiv:2203.07259_, 2022.
* Kusupati et al. (2020) Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In _International Conference on Machine Learning_, pages 5544-5555. PMLR, 2020.
* Lebedev and Lempitsky (2016) Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2554-2564, 2016.
* LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. _Advances in neural information processing systems_, 2, 1989.
* LeCun et al. (2017)* Lee et al. (2018) Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. _arXiv preprint arXiv:1810.02340_, 2018.
* Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. _arXiv preprint arXiv:2306.00978_, 2023.
* Liu et al. (2018) Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. _arXiv preprint arXiv:1810.05270_, 2018.
* Luo et al. (2017) Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In _IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017_, pages 5068-5076. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017.541.
* Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. _Advances in neural information processing systems_, 36:21702-21720, 2023.
* Marcus et al. (1994) Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate argument structure. In _Proceedings of the Workshop on Human Language Technology_, HLT '94, page 114-119, USA, 1994. Association for Computational Linguistics. ISBN 1558603573. doi: 10.3115/1075812.1075835. URL https://doi.org/10.3115/1075812.1075835.
* Meng et al. (2024a) Xiang Meng, Wenyu Chen, Riade Benbaki, and Rahul Mazumder. Falcon: Flop-aware combinatorial optimization for neural network pruning. In _International Conference on Artificial Intelligence and Statistics_, pages 4384-4392. PMLR, 2024a.
* Meng et al. (2024b) Xiang Meng, Shibal Ibrahim, Kayhan Behdin, Hussein Hazimeh, Natalia Ponomareva, and Rahul Mazumder. Osscar: One-shot structured pruning in vision and language models with combinatorial optimization. _arXiv preprint arXiv:2403.12983_, 2024b.
* Merity et al. (2017) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=Byj72udxe.
* Mocanu et al. (2018) Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. _Nature communications_, 9(1):2383, 2018.
* Molchanov et al. (2016) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. _arXiv preprint arXiv:1611.06440_, 2016.
* Molchanov et al. (2019) Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11264-11272, 2019.
* Nocedal and Wright (1999) Jorge Nocedal and Stephen J Wright. _Numerical optimization_. Springer, 1999.
* Paperno et al. (2016) Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambda dataset: Word prediction requiring a broad discourse context. _arXiv preprint arXiv:1606.06031_, 2016.
* Paszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21(1), jan 2020. ISSN 1532-4435.
* Sahoo et al. (2024) Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. A systematic survey of prompt engineering in large language models: Techniques and applications. _arXiv preprint arXiv:2402.07927_, 2024.
* Snoek et al. (2018)Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient second-order approximation for neural network compression. _Advances in Neural Information Processing Systems_, 33:18098-18109, 2020.
* Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. _arXiv preprint arXiv:2306.11695_, 2023.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. _arXiv preprint arXiv:1905.09418_, 2019.
* Wang et al. (2019) Yu Wang, Wotao Yin, and Jinshan Zeng. Global convergence of admm in nonconvex nonsmooth optimization. _Journal of Scientific Computing_, 78:29-63, 2019.
* Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* Wen et al. (2016) Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. _Advances in neural information processing systems_, 29, 2016.
* Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate and efficient post-training quantization for large language models. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 27168-27183. Curran Associates, Inc., 2022.
* Ye et al. (2018) Shaokai Ye, Tianyun Zhang, Kaiqi Zhang, Jiayu Li, Kaidi Xu, Yunfei Yang, Fuxun Yu, Jian Tang, Makan Fardad, Sijia Liu, et al. Progressive weight pruning of deep neural networks using admm. _arXiv preprint arXiv:1810.07378_, 2018.
* Ye et al. (2019) Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, and Xue Lin. Adversarial robustness vs. model compression, or both? In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 111-120, 2019.
* Yu et al. (2022) Xin Yu, Thiago Serra, Srikumar Ramalingam, and Shandian Zhe. The combinatorial brain surgeon: pruning weights that cancel one another in neural networks. In _International Conference on Machine Learning_, pages 25668-25683. PMLR, 2022.
* Zhang et al. (2022a) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022a.
* Zhang et al. (2022b) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022b.
* Zhang et al. (2018) Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, and Yanzhi Wang. A systematic dnn weight pruning framework using alternating direction method of multipliers. In _Proceedings of the European conference on computer vision (ECCV)_, pages 184-199, 2018.

Yuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. Dynamic sparse no training: Training-free fine-tuning for sparse llms. _arXiv preprint arXiv:2310.08915_, 2023.
* Zhou et al. [2021] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning n: m fine-grained structured sparse neural networks from scratch. _arXiv preprint arXiv:2102.04010_, 2021.
* Zhu and Gupta [2017] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. _arXiv preprint arXiv:1710.01878_, 2017.

Proofs of Theorem 1

Proof.: For the sake of conciseness, throughout the proof, we denote \(\mathbf{H}=\mathbf{X}^{\top}\mathbf{X}+\lambda_{2}\mathbf{I}\) and \(\mathbf{G}=\left(\mathbf{X}^{\top}\mathbf{X}+\lambda_{2}\mathbf{I}\right) \widetilde{\mathbf{W}}\). To establish the theorem, we first present the following two lemmas. The proofs of these two lemmas are given in Section A.1 and A.2, respectively.

**Lemma 1**.: _Let \(\left\{\mathbf{D}^{(t)}\right\}_{t=0}^{\infty}\) and \(\left\{\mathbf{V}^{(t)}\right\}_{t=0}^{\infty}\) be the sequence generated in Algorithm 1. Then for any \(t\geq 0\), it holds_

\[\|\mathbf{V}^{(t+1)}\|_{F} \leq\|\mathbf{G}-\mathbf{H}\mathbf{D}^{(t)}\|_{F}+\frac{\|\mathbf{ H}\mathbf{V}^{(t)}\|_{F}}{\rho_{t}}\] (7)

_and_

\[\|\mathbf{D}^{(t+1)}-\mathbf{D}^{(t)}\|_{F} \leq\frac{2}{\rho_{t}}\left(\|\mathbf{G}-\mathbf{H}\mathbf{D}^{( t)}\|_{F}+\frac{\|\mathbf{H}\mathbf{V}^{(t)}\|_{F}}{\rho_{t}}\right).\] (8)

**Lemma 2**.: _Let \(\left\{\mathbf{D}^{(t)}\right\}_{t=0}^{\infty}\), \(\left\{\mathbf{W}^{(t)}\right\}_{t=0}^{\infty}\) and \(\left\{\mathbf{V}^{(t)}\right\}_{t=0}^{\infty}\) be the sequence generated in Algorithm 1. Suppose \(\left\{\rho_{t}\right\}_{t=0}^{\infty}\) is non-decreasing. Then for any \(t\geq 0\), it holds_

\[\|\mathbf{D}^{(t)}\|_{F}+\frac{\|\mathbf{V}^{(t)}\|_{F}}{\rho_{t}} \leq\left[\prod_{s=0}^{t-1}\left(1+\frac{3\|\mathbf{H}\|_{2}}{ \rho_{s}}\right)\right]\cdot\left(\|\mathbf{D}^{(0)}\|_{F}+\frac{\|\mathbf{V} ^{(0)}\|_{F}}{\rho_{0}}+\sum_{s=0}^{t-1}\frac{3\|\mathbf{G}\|_{F}}{\rho_{s}}\right)\] (9)

Returning to the proof of the main theorem, combining Lemma 2 with the initialization of Algorithm 1 gives

\[\|\mathbf{D}^{(t)}\|_{F}+\frac{\|\mathbf{V}^{(t)}\|_{F}}{\rho_{t}} \leq\left[\prod_{s=0}^{t-1}\left(1+\frac{3\|\mathbf{H}\|_{2}}{ \rho_{s}}\right)\right]\cdot\left(\|\mathbf{D}^{(0)}\|_{F}+\frac{\|\mathbf{V} ^{(0)}\|_{F}}{\rho_{0}}+\sum_{s=0}^{t-1}\frac{3\|\mathbf{G}\|_{F}}{\rho_{s}}\right)\] (10) \[\leq\exp\left(3\|\mathbf{H}\|_{2}\sum_{s=0}^{\infty}\frac{1}{ \rho_{s}}\right)\cdot\left(\|\mathbf{G}\|_{F}+3\|\mathbf{G}\|_{F}\sum_{s=0}^{ \infty}\frac{1}{\rho_{s}}\right)\]

Let

\[C(\mathbf{X},\widehat{\mathbf{W}},\rho_{0},t_{u},\hat{\tau}):=2\|\mathbf{G}\| _{F}+2\|\mathbf{H}\|_{2}\left(\exp\left(3\|\mathbf{H}\|_{2}\sum_{s=0}^{\infty} \frac{1}{\rho_{s}}\right)\cdot\left(\|\mathbf{G}\|_{F}+3\|\mathbf{G}\|_{F} \sum_{s=0}^{\infty}\frac{1}{\rho_{s}}\right)\right)\] (11)

be the constant depending on \(\mathbf{X}\), \(\widehat{\mathbf{W}}\) and \(\sum_{s=0}^{\infty}1/\rho_{s}\). Lemma 1 together with (10) leads to

\[\|\mathbf{V}^{(t+1)}\|_{F} \leq\|\mathbf{G}-\mathbf{H}\mathbf{D}^{(t)}\|_{F}+\frac{\|\mathbf{ H}\mathbf{V}^{(t)}\|_{F}}{\rho_{t}}\] (12) \[\leq\|\mathbf{G}\|_{F}+\|\mathbf{H}\|_{2}\left(\|\mathbf{D}^{(t)} \|_{F}+\frac{\|\mathbf{V}^{(t)}\|_{F}}{\rho_{t}}\right)\leq\frac{1}{2}C( \mathbf{X},\widehat{\mathbf{W}},\rho_{0},t_{u},\hat{\tau})\]

and

\[\|\mathbf{D}^{(t+1)}-\mathbf{D}^{(t)}\|_{F} \leq\frac{2}{\rho_{t}}\left(\|\mathbf{G}-\mathbf{H}\mathbf{D}^{( t)}\|_{F}+\frac{\|\mathbf{H}\mathbf{V}^{(t)}\|_{F}}{\rho_{t}}\right)\leq\frac{C( \mathbf{X},\widehat{\mathbf{W}},\rho_{0},t_{u},\hat{\tau})}{\rho_{t}}.\] (13)

It then follows from \(\mathbf{W}^{(t+1)}-\mathbf{D}^{(t+1)}=(\mathbf{V}^{(t+1)}-\mathbf{V}^{(t)})/ \rho_{t}\) that

\[\|\mathbf{W}^{(t+1)}-\mathbf{D}^{(t+1)}\|_{F} \leq\frac{\|\mathbf{V}^{(t+1)}\|_{F}+\|\mathbf{V}^{(t)}\|_{F}}{ \rho_{t}}\leq\frac{C(\mathbf{X},\widehat{\mathbf{W}},\rho_{0},t_{u},\hat{\tau}) }{\rho_{t}}.\] (14)

Therefore, we prove the desired inequality. Since \(\sum_{s=0}^{\infty}1/\rho_{s}<\infty\), \(\left\{\mathbf{D}\right\}_{t=0}^{\infty}\) is a Cauchy sequence, and therefore there exists a matrix \(\widetilde{\mathbf{D}}\) such that \(\mathbf{D}^{(t)}\rightarrow\widetilde{\mathbf{D}}\). It follows from \(\|\mathbf{W}^{(t+1)}-\mathbf{D}^{(t+1)}\|_{F}\to 0\) that \(\mathbf{W}^{(t)}\rightarrow\widetilde{\mathbf{D}}\). The proof is completed.

### Proof of Lemma 1

Proof.: According to the \(\mathbf{W}-\)update rule in (4), it holds

\[\mathbf{W}^{(t+1)}-\mathbf{D}^{(t)}+\frac{\mathbf{V}^{(t)}}{\rho^{(t) }} =(\mathbf{H}+\rho_{t}\mathbf{I})^{-1}(\mathbf{G}-\mathbf{V}^{(t)}+ \rho_{t}\mathbf{D}^{(t)})-\mathbf{D}^{(t)}+\frac{\mathbf{V}^{(t)}}{\rho^{(t)}}\] \[=\left((\mathbf{H}+\rho_{t}\mathbf{I})^{-1}\rho_{t}-\mathbf{I} \right)\mathbf{D}^{(t)}+(\mathbf{H}+\rho_{t}\mathbf{I})^{-1}(\mathbf{G}- \mathbf{V}^{(t)})+\frac{\mathbf{V}^{(t)}}{\rho_{t}}\] \[=-\frac{1}{\rho_{t}}\left(\mathbf{I}+\frac{\mathbf{H}}{\rho_{t}} \right)^{-1}\mathbf{H}\mathbf{D}^{(t)}+\frac{1}{\rho_{t}}\left(\mathbf{I}+ \frac{\mathbf{H}}{\rho_{t}}\right)^{-1}(\mathbf{G}-\mathbf{V}^{(t)})+\frac{ \mathbf{V}^{(t)}}{\rho_{t}}\] \[=\frac{1}{\rho_{t}}\left(\mathbf{I}+\frac{\mathbf{H}}{\rho_{t}} \right)^{-1}(\mathbf{G}-\mathbf{H}\mathbf{D}^{(t)})+\frac{1}{\rho_{t}}\left[ \mathbf{I}-\left(\mathbf{I}+\frac{\mathbf{H}}{\rho_{t}}\right)^{-1}\right] \mathbf{V}^{(t)}\] \[=\frac{1}{\rho_{t}}\left(\mathbf{I}+\frac{\mathbf{H}}{\rho_{t}} \right)^{-1}\left(\mathbf{G}-\mathbf{H}\mathbf{D}^{(t)}+\frac{\mathbf{H} \mathbf{V}^{(t)}}{\rho_{t}}\right)\] (15)

Therefore, we obtain

\[\left\|\mathbf{W}^{(t+1)}-\mathbf{D}^{(t)}+\frac{\mathbf{V}^{(t) }}{\rho^{(t)}}\right\|_{F} \leq\frac{1}{\rho_{t}}\left\|\left(\mathbf{I}+\frac{\mathbf{H}}{ \rho_{t}}\right)^{-1}\right\|_{2}\left\|\mathbf{G}-\mathbf{H}\mathbf{D}^{(t)} +\frac{\mathbf{H}\mathbf{V}^{(t)}}{\rho_{t}}\right\|_{F}\] (16) \[\leq\frac{1}{\rho_{t}}\left\|\mathbf{G}-\mathbf{H}\mathbf{D}^{(t) }+\frac{\mathbf{H}\mathbf{V}^{(t)}}{\rho_{t}}\right\|_{F}\] \[\leq\frac{1}{\rho_{t}}\left(\|\mathbf{G}-\mathbf{H}\mathbf{D}^{(t )}\|_{F}+\frac{\|\mathbf{H}\mathbf{V}^{(t)}\|}{\rho_{t}}\right).\]

Denote \(\widetilde{\mathcal{I}}:=\{(i,j)\in[N_{in}]\times[N_{out}]\mid\mathbf{D}^{(t)}_ {ij}=0\}\). It follows from the \(\mathbf{D}-\)update rule and the definition of the projection operator that

\[\left\|\mathbf{D}^{(t+1)}-\mathbf{W}^{(t+1)}-\frac{\mathbf{V}^{(t )}}{\rho_{t}}\right\|_{F}^{2}=\min_{\begin{subarray}{c}\mathcal{I}\subseteq[N _{in}]\times[N_{out}]\\ |\mathcal{I}|=N_{in}N_{out}-k\end{subarray}}\sum_{(i,j)\in\mathcal{I}}\left( \mathbf{W}^{(t+1)}+\frac{\mathbf{V}^{(t)}}{\rho_{t}}\right)_{i,j}^{2}\] (17) \[\leq\left\|\mathbf{W}^{(t+1)}-\mathbf{D}^{(t)}+\frac{\mathbf{V}^{ (t)}}{\rho_{t}}\right\|_{F}^{2}\]

Together with (16), we get

\[\left\|\mathbf{D}^{(t+1)}-\mathbf{W}^{(t+1)}-\frac{\mathbf{V}^{(t)}}{\rho_{t}} \right\|_{F}\leq\frac{1}{\rho_{t}}\left(\|\mathbf{G}-\mathbf{H}\mathbf{D}^{(t )}\|_{F}+\frac{\|\mathbf{H}\mathbf{V}^{(t)}\|}{\rho_{t}}\right).\] (18)

It then follows from the \(\mathbf{V}-\)update rule that

\[\frac{\|\mathbf{V}^{(t+1)}\|_{F}}{\rho_{t}}=\left\|\mathbf{D}^{(t+1)}-\mathbf{ W}^{(t+1)}-\frac{\mathbf{V}^{(t)}}{\rho_{t}}\right\|_{F}\leq\frac{1}{\rho_{t}} \left(\|\mathbf{G}-\mathbf{H}\mathbf{D}^{(t)}\|_{F}+\frac{\|\mathbf{H} \mathbf{V}^{(t)}\|}{\rho_{t}}\right)\] (19)

This establishes the inequality (7). Furthermore, by summing up (16) and (18) and applying the triangle inequality, we verify the inequality (8). 

### Proof of Lemma 2

Proof.: It follows from Lemma 1 that

\[\|\mathbf{V}^{(t+1)}\|_{F} \leq\|\mathbf{G}-\mathbf{H}\mathbf{D}^{(t)}\|_{F}+\frac{\|\mathbf{ H}\mathbf{V}^{(t)}\|_{F}}{\rho_{t}}\] (20) \[\leq\|\mathbf{H}\|_{2}\|\mathbf{D}^{(t)}\|_{F}+\|\mathbf{G}\|_{F}+ \frac{\|\mathbf{H}\|_{2}\|\mathbf{V}^{(t)}\|_{F}}{\rho_{t}}\]\[\|\mathbf{D}^{(t+1)}-\mathbf{D}^{(t)}\|_{F} \leq\frac{2}{\rho_{t}}\left(\|\mathbf{G}-\mathbf{H}\mathbf{D}^{(t) }\|_{F}+\frac{\|\mathbf{H}\mathbf{V}^{(t)}\|_{F}}{\rho_{t}}\right)\] (21) \[\leq\frac{2}{\rho_{t}}\left(\|\mathbf{H}\|_{2}\|\mathbf{D}^{(t)} \|_{F}+\|\mathbf{G}\|_{F}+\frac{\|\mathbf{H}\|_{2}\|\mathbf{V}^{(t)}\|_{F}}{ \rho_{t}}\right).\]

This further implies

\[\|\mathbf{D}^{(t+1)}\|_{F}\leq\left(1+\frac{2\|\mathbf{H}\|_{2}}{\rho_{t}} \right)\|\mathbf{D}^{(t)}\|_{F}+\frac{2\|\mathbf{G}\|_{F}}{\rho_{t}}+\frac{2 \|\mathbf{H}\|_{2}\|\mathbf{V}^{(t)}\|_{F}}{\rho_{t}^{2}}\] (22)

Combining inequalities (20) and (22) yields

\[\|\mathbf{D}^{(t+1)}\|_{F}+\frac{\|\mathbf{V}^{(t+1)}\|_{F}}{ \rho_{t+1}} \leq\|\mathbf{D}^{(t+1)}\|_{F}+\frac{\|\mathbf{V}^{(t+1)}\|_{F}}{ \rho_{t}}\] (23) \[\leq\left(1+\frac{3\|\mathbf{H}\|_{2}}{\rho_{t}}\right)\|\mathbf{ D}^{(t)}\|_{F}+\frac{3\|\mathbf{G}\|_{F}}{\rho_{t}}+\frac{3\|\mathbf{H}\|_{2}\| \mathbf{V}^{(t)}\|_{F}}{\rho_{t}^{2}}\] \[\leq\left(1+\frac{3\|\mathbf{H}\|_{2}}{\rho_{t}}\right)\left(\| \mathbf{D}^{(t)}\|_{F}+\frac{\|\mathbf{V}^{(t)}\|_{F}}{\rho_{t}}\right)+\frac{ 3\|\mathbf{G}\|_{F}}{\rho_{t}}\]

Denote \(a_{t}:=\|\mathbf{D}^{(t)}\|_{F}+\|\mathbf{V}^{(t)}\|_{F}/\rho_{t}\), then the above inequality can be rewritten as

\[a_{t+1}\leq\left(1+\frac{3\|\mathbf{H}\|_{2}}{\rho_{t}}\right)a_{t}+\frac{3\| \mathbf{G}\|_{F}}{\rho_{t}}\] (24)

Therefore,

\[\frac{a_{t+1}}{\prod_{s=0}^{t}(1+3\|\mathbf{H}\|_{2}/\rho_{k})} \leq\frac{a_{t}}{\prod_{s=0}^{t-1}(1+3\|\mathbf{H}\|_{2}/\rho_{k} )}+\frac{3\|\mathbf{G}\|_{F}}{\rho_{t}\prod_{s=0}^{t}(1+3\|\mathbf{H}\|_{2}/ \rho_{k})}\] (25) \[\leq\frac{a_{t}}{\prod_{s=0}^{t-1}(1+3\|\mathbf{H}\|_{2}/\rho_{k} )}+\frac{3\|\mathbf{G}\|_{F}}{\rho_{t}}\]

It then follows from telescoping that

\[\frac{a_{t}}{\prod_{s=0}^{t-1}(1+3\|\mathbf{H}\|_{2}/\rho_{k})}\leq a_{0}+ \sum_{s=0}^{t-1}\frac{3\|\mathbf{G}\|_{F}}{\rho_{t}}\] (26)

Recalling the definition of \(a_{t}\) completes the proof. 

## Appendix B Experimental Details

### Experimental setup

We performed all experiments on a computing cluster using an Intel Xeon Gold 6248 machine with 20 CPUs and a single NVIDIA V100 GPU, which is equipped with 192GB of CPU RAM and 32GB of CUDA memory. The PyTorch library Paszke et al. (2017) was used to implement all language models and pruning methods for our experiments.

**Pruning problem setup.** For a given sparsity \(s\), we set the \(\ell_{0}\) constraint \(k\) in the pruning problem (1) to \(\lfloor N_{in}N_{out}s\rfloor\). Following the pruning framework proposed by Frantar and Alistarh (2023); Meng et al. (2024), we solve the LLM pruning problem sequentially, layer by layer. For layer \(\ell\), the input activation matrix \(\mathbf{X}\) in (1) is set as the output of the previous \(\ell-1\) pruned layers on \(N\) calibration samples.

**Data pre-processing.** Let \(\mathbf{E}=\mathrm{Diag}(\mathbf{X}^{\top}\mathbf{X}+\lambda_{2}\mathbf{I})^{- 1/2}\). To achieve better scaling, we define \(\mathbf{W}^{\prime}=\mathbf{E}^{-1}\mathbf{W}\) and reformulate Equation (1) into the following equivalent form:

\[\min_{\mathbf{W}^{\prime}\in\mathbb{R}^{N_{in}\times N_{out}}}\ \|\mathbf{X} \mathbf{\widetilde{W}}-\mathbf{X}\mathbf{E}\mathbf{W}^{\prime}\|_{F}^{2}+ \lambda_{2}\|\mathbf{\widetilde{W}}-\mathbf{E}\mathbf{W}^{\prime}\|_{F}^{2} \quad\text{s.t.}\ \|\mathbf{W}^{\prime}\|_{0}\leq k\] (27)

Practically, we apply Algorithm 1 to solve Equation (27) and recover the solution to the original problem by setting \(\mathbf{W}=\mathbf{E}\mathbf{W}^{\prime}\). It is important to note that this pre-processing step does not alter the procedure of Algorithm 1 or affect the convergence analysis. It only modifies the updates within Algorithm 1 and can lead to a better convergence rate in practice.

**Hyperparamter choice.** We choose \(\lambda_{2}=0.01\operatorname{Tr}(\mathbf{X}^{\top}\mathbf{X})\). In Algorithm 1, we set \(\rho_{0}=0.1\). And we update \(\rho\) every \(3\) iteration based on a step function that depends on the current value of \(\rho_{t}\) and \(s_{t}:=|\operatorname{Supp}\left(\mathbf{D}^{(t)}\right)\Delta\operatorname{ Supp}\left(\mathbf{D}^{(t-3)}\right)|\), which represents the number of elements in the symmetric difference between \(\operatorname{Supp}\left(\mathbf{D}^{(t)}\right)\) and \(\operatorname{Supp}\left(\mathbf{D}^{(t-3)}\right)\). Specifically, we set

\[\rho_{t+1}=\left\{\begin{array}{ll}1.3\rho_{t}&\text{if }s_{t}\geq 0.1k,\\ 1.2\rho_{t}&\text{if }s_{t}\geq 0.005k,\\ 1.1\rho_{t}&\text{if }s_{t}\geq 1.\end{array}\right.\] (28)

If \(s_{t}=0\), it indicates that \(\rho\) is sufficiently large and the support has stabilized. In this case, we terminate Algorithm 1, set \(\mathcal{S}\) as the support of \(\mathbf{W}\), and apply Algorithm 2 with \(10\) iterations to solve problem (6) with support \(\mathcal{S}\).

**Implementation details.** Below are the configuration and implementation details for the competing methods and our proposed framework _ALPS_.

* MP: For each layer in the LLM, we perform magnitude pruning by sorting the absolute values of all entries of the dense weight \(\widehat{\mathbf{W}}\) in descending order, keeping the top \(k\) entries unchanged, and setting the remaining entries to zero.
* SparseGPT: We utilize the authors' implementation (codes available on GitHub) with default hyperparameter settings to perform one-shot unstructured LLM pruning.
* Wanda: We utilize the authors' implementation (codes available on GitHub) with default hyperparameter settings to perform one-shot unstructured LLM pruning.
* DSnoT: We utilize the authors' implementation (codes available on GitHub) with default hyperparameter settings to perform one-shot unstructured LLM pruning.

### Additional experimental results

#### b.2.1 The importance of the \(\rho\) update scheme

Our proposed \(\rho\) update scheme, theoretically supported by Theorem 1, ensures that _ALPS_ converges rapidly while finding high quality solutions. In contrast, ADMM with a fixed \(\rho\) may fail to converge when applied to \(\ell_{0}\) constrained least squares problems. To provide empirical evidence for this claim, we compare _ALPS_ with ADMM using fixed \(\rho\) values. We examined two key metrics: reconstruction loss (objective) and the rate of change of the support (of weights) between consecutive iterations (this measures the convergence speed of the algorithm).Tables 4 and 5 present the results for these metrics, respectively. Our findings show that ADMM with a large \(\rho(=3)\) converges quickly but yields poor solutions, while a small \(\rho(=0.3)\) fails to converge. _ALPS_, utilizing our \(\rho\) update scheme, achieves both rapid convergence and high-quality solutions.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Supp change / Iter & \(5\) & \(10\) & \(20\) & \(30\) & \(50\) & \(100\) \\ \hline _ALPS_ & 1.63e-1 & 1.28e-1 & 5.95e-2 & 5.32e-2 & 5.31e-2 & 5.31e-2 \\ \hline ADMM(\(\rho=0.3\)) & 7.83e-2 & 7.55e-2 & 7.50e-2 & 7.47e-2 & 7.47e-2 & 7.45e-2 \\ \hline ADMM(\(\rho=0.3\)) & 9.32e-2 & 8.18e-2 & 7.64e-2 & 7.53e-2 & 7.45e-2 & 7.42e-2 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The relative reconstruction error \(\|\mathbf{X}\widehat{\mathbf{W}}-\mathbf{X}\mathbf{W}\|_{F}^{2}/\|\mathbf{X} \widehat{\mathbf{W}}\|_{F}^{2}\) over iterations, comparing _ALPS_ with ADMM using a fixed penalty parameter \(\rho\).

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Supp change / Iter & \(5\) & \(10\) & \(20\) & \(30\) & \(50\) & \(100\) \\ \hline _ALPS_ & \(20.2\%\) & \(17.0\%\) & \(2.8\%\) & \(0.0\%\) & \(0.0\%\) & \(0.0\%\) \\ \hline ADMM(\(\rho=0.3\)) & \(6.4\%\) & \(7.0\%\) & \(7.0\%\) & \(7.0\%\) & \(6.9\%\) & \(6.9\%\) \\ \hline ADMM(\(\rho=0.3\)) & \(0.2\%\) & \(<0.1\%\) & \(<0.1\%\) & \(<0.1\%\) & \(<0.1\%\) & \(<0.1\%\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: The rate of change of the support (of weights) between consecutive iterations, comparing _ALPS_ with ADMM using a fixed penalty parameter \(\rho\).

#### b.2.2 Runtime comparison

We compare the runtime of _ALPS_ with other methods in Table 6. _ALPS_ employs an advanced optimization method to solve the layerwise reconstruction problem, which results in longer running times compared to other algorithms. However, it's important to note that _ALPS_'s runtime is still negligible when compared to fine-tuning methods for LLMs, e.g., LoRA (Hu et al., 2021).

#### b.2.3 Comparison of _ALPS_ and ADMM-Grad

We discuss the difference between _ALPS_ and ADMM-Grad (Boza, 2024), another interesting work using ADMM for LLM unstructured pruning. (Boza, 2024) selects the sparsity mask via iterative magnitude pruning and applies ADMM to solve problem (6) with the selected sparsity mask. _ALPS_, in contrast, is an end-to-end approach that directly targets the \(\ell_{0}\) constrained least squares problem (1). By simultaneously optimizing both weights and sparsity patterns, _ALPS_ achieves lower layerwise reconstruction loss compared to ADMM-Grad, as demonstrated in Table 7.

Additionally, since (Boza, 2024) employs ADMM for solving problem (6), we compare it with our proposed PCG procedure. We tested both approaches for solving problem (6) with a given support. Results presented in Table 8 show that PCG outperforms ADMM-Grad in both computational time and objective value. The time advantage of PCG stems from its ability to backsolve without explicitly computing matrix inverses.

#### b.2.4 Pruned model performance on MMLU benchmark

We evaluated the one-shot unstructured pruning performance of MP, SparseGPT, Wanda, DSnoT, and _ALPS_ on LLaMA3-8B using the MMLU benchmark to give a comprehensive assessment of each method's effectiveness. Table 1 shows the mean accuracy across all MMLU categories. The results demonstrate that _ALPS_ outperforms other methods, further validating its effectiveness in producing high-performance pruned models. We also observe significant performance degradation on MMLU

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline Algorithm & OPT-1.3B & OPT-2.7B & OPT-6.7B & OPT-13B & OPT-30B & LLaMA2-7B & LLaMA2-13B & LLaMA3-8B \\ \hline MP & 4.7 & 8.9 & 23 & 47 & 120 & 25 & 46 & 24 \\ Wanda & 99 & 161 & 280 & 502 & 1027 & 214 & 407 & 1118 \\ SparseGPT & 363 & 728 & 1621 & 2980 & 6662 & 1263 & 2319 & 2392 \\ DSnoT & 125 & 213 & 417 & 758 & 1528 & 347 & 651 & 1176 \\ _ALPS_ & 963 & 2360 & 6069 & 14323 & 48366 & 3043 & 7145 & 6735 \\ \hline \end{tabular}
\end{table}
Table 6: Runtime (in seconds) comparison for one-shot unstructured pruning of OPT models and LLaMA models. Here, runtime includes input activation generation and model pruning.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Sparsity & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\ \hline _ALPS_ & 3.55e-3 & 7.56e-3 & 1.47e-2 & 2.77e-2 & 5.32e-2 & 1.13e-1 \\ \hline ADMM-Grad & 4.47e-3 & 9.53e-3 & 1.83e-2 & 3.36e-2 & 6.19e-2 & 1.25e-1 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Relative reconstruction error \(\|\mathbf{X\widehat{W}}-\mathbf{XW}\|_{F}^{2}/\|\mathbf{X\widehat{W}}\|_{F}^{2}\) comparison between _ALPS_ and ADMM-Grad across different sparsity levels.

at high sparsity levels, suggesting that one-shot pruning should be combined with fine-tuning [Hu et al., 2021] or prompting [Sahoo et al., 2024] techniques to maintain model performance in practical applications.

#### b.2.5 Comprehensive model performance across sparsity levels

We compare the one-shot unstructured pruning performance of MP, SparseGPT, Wanda, DSnoT, and _ALPS_ on OPT-1.3B-30B models and LLAMA2-7B, LLAMA2-13B, LLAMA3-8B models in Tables 10-17. The models are pruned at unstructured sparsity levels of \(40\%\), \(50\%\), \(60\%\), \(70\%\), \(80\%\), and \(90\%\), as well as at 2:4 and 4:8 sparsity patterns. We evaluate the perplexity of the pruned models on WikiText2, PTB, and C4 datasets. Additionally, we assess the accuracy of the pruned models on PIQA, LAMBADA, ARC-Easy, and ARC-Challenge. However, LAMBADA accuracy results for the LLAMA model have been omitted since LLaMA models have unsatisfactory performance on this dataset without further modifications. For each combination of model, sparsity, and pruning approach, we run each method five times and report the mean and standard deviation of each performance criterion.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline Sparsity & MP & Wanda & SparseGPT & DSnoT & _ALPS_ \\ \hline
0.4 & 47.30 & 53.48 & 56.74 & 54.19 & **57.42** \\ \hline
0.5 & 36.11 & 42.64 & 50.87 & 44.49 & **51.01** \\ \hline
0.6 & 25.35 & 29.50 & 37.54 & 28.96 & **40.40** \\ \hline
0.7 & 23.10 & 24.78 & 27.99 & 24.73 & **28.71** \\ \hline
2:4 & 25.30 & 28.24 & **34.27** & 29.25 & 33.98 \\ \hline
4:8 & 28.70 & 32.64 & 38.37 & 34.59 & **41.21** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Performance analysis for one-shot unstructured pruning of LLaMA-3 8B models at various sparsity levels using MMLU benchmark.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline Model & Algorithm & WallTex21 & PTB \(\downarrow\) & C4 \(\downarrow\) & LAMBADA \(\uparrow\) & PDA \(\downarrow\) & ABC-Case \(\uparrow\) & ABC-Collabor \(\uparrow\) \\ \hline \multirow{4}{*}{OPT-1.3B} & MP & 18.6\(\pm\)0.03 & 26.6\(\pm\)0.03 & 18.7\(\pm\)0.14 & 14.1\(\pm\)0.05 & 68.9\(\pm\)0.36 & 32.7\(\pm\)0.05 & 24.6\(\pm\)0.22 \\  & Sparse(PT-1.3B) & Sparse(PT-1.09) & 22.5\(\pm\)0.02 & 17.5\(\pm\)0.23 & 24.8\(\pm\)0.00 & 68.9\(\pm\)0.46 & 48.5\(\pm\)0.47 & 2.5\(\pm\)0.27 \\  & DSsort & 19.2\(\pm\)0.15 & 26.1\(\pm\)0.03 & 19.5\(\pm\)0.03 & 19.1\(\pm\)0.03 & 68.4\(\pm\)0.19 & 58.4\(\pm\)0.39 & 2.5\(\pm\)0.39 \\  & DSsort & 18.2\(\pm\)0.12 & 24.0\(\pm\)0.24 & **19.6\(\pm\)**0.15 & **47.5\(\pm\)**1.00 & **76.3\(\pm\)**1.05 & **54.7\(\pm\)**0.27 & **23.3\(\pm\)**0.40 \\ \hline \multirow{4}{*}{OPT-2.7B} & MP & 11.9\(\pm\)0.19 & 18.2\(\pm\)0.09 & 55.5\(\pm\)0.00 & 17.9\(\pm\)0.09 & 68.6\(\pm\)0.03 & 65.5\(\pm\)0.00 & 22.7\(\pm\)0.00 \\  & Variable & 12.0\(\pm\)0.25 & 21.7\(\pm\)0.05 & 19.5\(\pm\)0.00 & 39.8\(\pm\)0.05 & 73.8\(\pm\)0.35 & 55.0\(\pm\)0.00 & 24.4\(\pm\)0.34 \\  & DSsort & 13.4\(\pm\)0.09 & 18.5\(\pm\)0.04 & 19.5\(\pm\)0.00 & 53.8\(\pm\)0.00 & 77.5\(\pm\)0.01 & 55.4\(\pm\)0.00 & 24.4\(\pm\)0.34 \\  & DSsort & 13.4\(\pm\)0.13 & **17.6\(\pm\)**0.05 & **24.1\(\pm\)**0.10 & 52.3\(\pm\)**0.07 & 71.8\(\pm\)0.29 & 53.6\(\pm\)0.31 & 3.2\(\pm\)0.25 \\ \hline \multirow{4}{*}{OPT-3B} & MP & 53.2\(\pm\)0.02 & 22.2\(\pm\)0.09 & 22.2\(\pm\)0.09 & 22.9\(\pm\)0.09 & 77.4\(\pm\)0.04 & 47.1\(\pm\)0.00 & 23.5\(\pm\)0.00 \\  & Wands & 12.0\(\pm\)0.11 & 17.1\(\pm\)0.00 & 13.1\(\pm\)0.00 & 66.0\(\pm\)0.05 & 74.4\(\pm\)0.18 & 63.4\(\pm\)0.33 & 23.7\(\pm\)0.43 \\  & DSsort & 11.9\(\pm\)0.12 & 15.5\(\pm\)0.19 & 10.9\(\pm\)0.03 & 68.9\(\pm\)0.05 & 73.9\(\pm\)0.05 & 25.4\(\pm\)0.36 & 27.6\(\pm\)0.27 \\  & DSsort & 11.4\(\pm\)0.09 & **15.2\(\pm\)**0.00 & 22.8\(\pm\)**0.00 & **63.3\(\pm\)**0.05 & **74.8\(\pm\)**0.15 & **64.4\(\pm\)**0.35 & **29.1\(\pm\)**0.25 \\  & MPP & 25.0\(\pm\)0.07 & 19.2\(\pm\)0.09 & 23.4\(\pm\)0.00 & 69.9\(\pm\)0.01 & 73.7\(\pm\)0.01 & 34.7\(\pm\)0.00 & 23.0\(\pm\)0.00 \\  & Wands & 11.9\(\pm\)0.09 & 15.9\(\pm\)0.02 & 12.5\(\pm\)0.00 & 69.9\(\pm\)0.01 & 74.7\(\pm\)0.19 & 63.3\(\pm\)0.33 & 30.07\(\pm\)0.18 \\  & DSsort & 11.1\(\pm\)0.09 & 13.9\(\pm\)0.01 & 19.9\(\pm\)0.01 & 63.2\(\pm\)0.01 & 74.8\(\pm\)0.12 & 63.5\(\pm\)0.00 & 23.0\(\pm\)0.17 \\  & DSsort & 11.5\(\pm\)0.07 & 14.4\(\pm\)0.10 & 12.0\(\pm\)0.03 & 63.6\(\pm\)0.00 & 74.4\(\pm\)0.16 & 63.4\(\pm\)0.36 & 30.0\(\pm\)0.09 \\  & DSsort & **10.7\(\pm\)0.07 & **13.8\(\pm\)**0.05 & **11.7\(\pm\)**0.05 & **62.0\(\pm\)**0.04 & **75.1\(\pm\)**0.01 & **66.0\(\pm\)**0.20 & **31.5\(\pm\)**0.79 \\ \hline \multirow{4}{*}{OPT-3B} & MP & 10.7\(\pm\)0.06 & 25.0\(\pm\)0.09 & 69.0\(\pm\)0.09 & 69.0\(\pm\)0.09 & 63.0\(\pm\)0.09 & 73.6\(\pm\)0.09 & 35.0\(\pm\)0.00 \\  & DSsort & 12.8\(\pm\)0.09 & 13.7\(\pm\)0.09 & 14.8\(\pm\)0.09 & 63.7\(\pm\)0.09 & 73.2\(\pm\)0.15 & 59.1\(\pm\)0.25 & 26.1\(\pm\)0.36 \\  & \(\downarrow\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\

\begin{table}
\begin{tabular}{c|c|c c c c c c c c} \hline \hline Model & Algorithm & WaTreat2 & \(\uparrow\)FIB \(\downarrow\) & C4 \(\downarrow\) & LAMBADA & FOB \(\downarrow\) & ALacA\(\downarrow\) & ALacA\(\downarrow\) & ALacChange \(\uparrow\) \\ \hline \multirow{4}{*}{OPT-13B} & MP & 9649\(\pm\)0.010 & 6699\(\pm\)0.010 & 5852\(\pm\)0.00 & 6095\(\pm\)0.000 & 55.121\(\pm\)0.000 & 26.18\(\pm\)0.000 & 20.99\(\pm\)0.000 \\  & Words & 520.10\(\pm\)3.13 & 128.01\(\pm\)1.78 & 785.23\(\pm\)9.74 & 977.46\(\pm\)0.000 & 59.38\(\pm\)0.46 & 97.37\(\pm\)0.03 & 18.12\(\pm\)0.36 \\  & Sparse(PT & 520.12\(\pm\)3.229 & 107.01\(\pm\)3.310 & 53.71\(\pm\)0.178 & 77.21\(\pm\)1.04 & 63.58\(\pm\)0.000 & 49.70\(\pm\)0.58 & 19.69\(\pm\)0.78 \\  & SparseT & 367.51\(\pm\)0.196 & 370.32\(\pm\)0.36 & 295.46\(\pm\)1.31 & 8.621\(\pm\)0.23 & 56.39\(\pm\)0.37 & 33.22\(\pm\)0.47 & 17.48\(\pm\)0.57 \\  & AJPS & **95.85\(\pm\)0.23** & **94.08\(\pm\)**1.11 & **83.52\(\pm\)**1.29 & **2.11\(\pm\)**1.17 & **6.44\(\pm\)**0.37 & **6.49\(\pm\)**0.12 & **12.88\(\pm\)**0.32 \\ \hline \multirow{4}{*}{OPT-27B} & MP & 122.00\(\pm\)0.000 & 9709\(\pm\)0.000 & 9966\(\pm\)0.000 & 59.00\(\pm\)0.000 & 59.00\(\pm\)0.000 & 25.52\(\pm\)0.000 \\  & Sparse(PT & 289.31\(\pm\)0.139 & 28.40\(\pm\)0.19 & 21.06\(\pm\)0.000 & 34.91\(\pm\)0.000 & 59.00\(\pm\)0.000 & 26.53\(\pm\)0.000 & 17.16\(\pm\)0.20 \\  & Sparse(PT & 289.31\(\pm\)0.139 & 28.40\(\pm\)0.19 & 21.06\(\pm\)0.000 & 34.91\(\pm\)0.000 & 59.00\(\pm\)0.000 & 26.53\(\pm\)0.000 \\  & Sparse(PT & 114.87\(\pm\)0.196 & 28.40\(\pm\)0.19 & 28.40\(\pm\)0.19 & 28.40\(\pm\)0.19 & 28.40\(\pm\)0.19 & 28.40\(\pm\)0.19 \\  & Sparse(PT & 28.07\(\pm\)0.259 & 33.01\(\pm\)0.30 & 20.32\(\pm\)0.16 & 33.07\(\pm\)0.19 & 46.58\(\pm\)0.24 & 50.66\(\pm\)0.49 & 21.76\(\pm\)0.42 \\  & _AllPS_ & **18.54\(\pm\)0.20** & **22.66\(\pm\)**0.26 & **18.50\(\pm\)**0.26 & **35.43\(\pm\)**1.91 & **17.34\(\pm\)**0.19 & **6.59\(\pm\)**0.19 & **28.41\(\pm\)**0.55 \\ \hline \multirow{4}{*}{OPT-3B} & MP & 9643\(\pm\)0.49 & 4791\(\pm\)0.50 & 5876\(\pm\)0.00 & 0.00\(\pm\)0.000 & 53.86\(\pm\)0.000 & 30.50\(\pm\)0.000 & 26.53\(\pm\)0.000 \\  & Sparse(PT & 15.51\(\pm\)0.196 & 24.91\(\pm\)0.18 & 17.10\(\pm\)0.19 & 14.19\(\pm\)0.19 & 25.92\(\pm\)0.18 & 58.27\(\pm\)0.18 & 25.26\(\pm\)0.25 \\  & Sparse(PT & 13.02\(\pm\)0.199 & 23.01\(\pm\)0.19 & 15.00\(\pm\)0.13 & 55.51\(\pm\)0.12 & 79.16\(\pm\)0.12 & 57.64\(\pm\)0.19 & 24.85\(\pm\)0.19 \\  & Sparse(PT & 13.03\(\pm\)0.199 & 23.01\(\pm\)0.21 & 16.00\(\pm\)0.13 & 55.51\(\pm\)0.12 & 79.16\(\pm\)0.12 & 57.64\(\pm\)0.19 & 24.85\(\pm\)0.19 \\  & _AllPS_ & **13.03\(\pm\)**1.47 & **17.36\(\pm\)**0.25 & **15.52\(\pm\)**0.23 & **64.00\(\pm\)**1.48 & **4.80\(\pm\)**0.29 & **6.44\(\pm\)**0.40 & **27.24\(\pm\)**0.80 \\ \hline \multirow{4}{*}{ILAMA-1B} & MP & 1171\(\pm\)0.010 & 2408\(\pm\)0.000 & 1490\(\pm\)0.00 & 903\(\pm\)0.000 & 52.51\(\pm\)0.000 & 25.18\(\pm\)0.000 \\  & Words & 1608\(\pm\)0.003 & 16.42\(\pm\)0.15 & 15.10\(\pm\)0.19 & 35.11\(\pm\)0.19 & 71.56\(\pm\)0.12 & 57.68\(\pm\)0.11 & 24.85\(\pm\)0.19 \\  & Sparse(PT & 13.04\(\pm\)0.000 & 23.03\(\pm\)0.000 & 13.11\(\pm\)0.16 & 61.90\(\pm\)0.16 & 72.14\(\pm\)0.14 & 36.11\(\pm\)0.19 & 27.91\(\pm\)0.000 \\  & Sparse(PT & 11.01\(\pm\)0.144 & 20.30\(\pm\)0.19 & 14.10\(\pm\)0.19 & 61.90\(\pm\)0.14 & 73.44\(\pm\)0.19 & 36.11\(\pm\)0.19 & 27.91\(\pm\)0.000 \\  & Sparse(PT & 11.11\(\pm\)0.144 & 20.30\(\pm\)0.19 & 14.10\(\pm\)0.19 & 14.05\(\pm\)0.19 & 74.56\(\pm\)0.12 & 36.90\(\pm\)0.19 & 27.33\(\pm\)0.18 \\ \hline \multirow{4}{*}{ILAMA-1B} & MP & 1232\(\pm\)0.000 & **15.57\(\pm\)**0.16 & **28.50\(\pm\)**0.16 & **48.54\(\pm\)**1.24 & **74.19\(\pm\)**0.19 & **36.44\(\pm\)**0.19 & 27.91\(\pm\)**0.00 \\  & Sparse(PT & 13.03\(\pm\)0.199 & 23.01\(\pm\)0.19 & 15.00\(\pm\)0.13 & 6.90\(\pm\)0.13 & 7.56\(\pm\)0.12 & 36.89\(\pm\)0.15 & 29.60\(\pm\)0.19 \\  & Sparse(PT & 14.12\(\pm\)0.199 & 23.01\(\pm\)0.19 & 13.11\(\pm\)0.19 & 6.91\(\pm\)0.16 & 72.14\(\pm\)0.19 & 36.11\(\pm\)0.19 & 27.91\(\pm\)0.00 \\  & Sparse(PT & 11.11\(\pm\)0.144 & 20.30\(\pm\)0.19 & 14.10\(\pm\)0.19 & 14.05\(\pm\)0.19 & 74.56\(\pm\)0.12 & 36.90\(\pm\)0.19 & 27.33\(\pm\)0.18 \\ \hline \multirow{4}{*}{ILAMA-1B} & MP

\begin{table}
\begin{tabular}{c|c|c c c|c c c|c c} \hline \hline Model & Algorithm & WallTcz1 & PTB \(\downarrow\) & C\(\downarrow\) & LAMBADA \(\uparrow\) & PPOA \(\uparrow\) & ARC-Easy \(\uparrow\) & ARC-Callorig \(\uparrow\) \\ \hline \multirow{4}{*}{OPT-13B} & MP & 20886(6.0) & 11334(6.0) & 12708(6.0) & 0.000(0.00) & 529.00(0.00) & 283.50(0.00) & 1782.80(0.00) \\  & Swath & 11290(2.0) & 3272(2.74) & 3736(4.25) & 0.000(0.00) & 539.00(0.00) & 283.40(0.00) & **283.15**(0.00) & 184.65(0.00) \\  & Swath & 3372(4.1) & 1783(4.2) & 1311(2.74) & 591(4.0) & 43.50(0.00) & 559.00(0.00) & **283.70**(0.00) & 184.65(0.00) \\  & Swath & 11375(5.4) & **1280(6.2)** & **1428(6.2)** & **0.000(0.00)** & 529.00(0.00) & 256.50(0.00) & 183.95(0.00) \\ \hline \multirow{4}{*}{OPT-13B} & MP & 18937(6.0) & 11320(2.0) & 1585(1.0) & 0.000(0.00) & 283.40(0.00) & 235.00(0.00) & **283.00**(0.00) \\  & Swath & 5008(1.0) & 1225(1.76) & 1791(1.99) & 409(4.5) & 0.000(0.00) & **529.00**(0.00) & 283.00(0.00) \\  & Swath & 1125(1.2) & 1125(1.17) & 1198(1.99) & 409(4.5) & 0.000(0.00) & **522.00**(0.00) & 223.00(0.00) \\ \hline \multirow{4}{*}{LMAM2-13B} & MP & 12252(1.6) & **1125(1.1)** & **1000(1.0)** & **555.00(0.00)** & **0.000(0.00)** & 252.00(0.00) & 23.00(0.00) \\  & Swath & 11290(1.1) & 1134(6.0) & 1105(1.0) & 0.000(0.00) & 252.00(0.00) & 23.00(0.00) & 193.95(0.00) \\  & Swath & 1125(1.2) & 1125(1.1) & **1000(1.0)** & 100(1.0) & 252.00(0.00) & 23.00(0.00) & 193.95(0.00) \\ \hline \multirow{4}{*}{OPT-27.28} & MP & 1225(1.6) & **1135(1.0)** & **1000(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) & 23.00(0.00) \\  & Swath & 1138(1.0) & 1141(1.2) & **1100(1.0)** & 100(1.0) & 252.00(0.00) & 23.00(0.00) & 193.95(0.00) \\ \hline \multirow{4}{*}{LMAM3-13B} & MP & 1225(1.6) & **1135(1.0)** & **100(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) & 193.95(0.00) \\  & Swath & 1126(1.0) & **1141(1.2)** & **1100(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) & 193.95(0.00) \\ \hline \multirow{4}{*}{LMAM2-13B} & MP & 1249(1.0) & **125(1.1)** & **1000(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) & 193.95(0.00) \\  & Swath & 1140(1.0) & **125(1.1)** & **100(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) & 193.95(0.00) \\ \hline \multirow{4}{*}{LMAM2-13B} & MP & 1250(1.0) & **125(1.1)** & **1000(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) & 193.95(0.00) \\  & Swath & 1126(1.0) & **115(1.0)** & **100(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) & 193.95(0.00) \\ \hline \multirow{4}{*}{LMAM3-13B} & MP & 1250(1.0) & **125(1.1)** & **1000(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) & 193.95(0.00) \\  & Swath & 1138(1.0) & **1141(1.2)** & **1100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** \\ \hline \multirow{4}{*}{LMAM3-13B} & MP & 6016(1.0) & **125(1.1)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) \\  & Swath & 1140(1.0) & **125(1.1)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** \\ \hline \multirow{4}{*}{LMAM2-13B} & MP & 6016(1.0) & **125(1.1)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) \\  & Swath & 1146(1.0) & **125(1.1)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) \\  & Swath & 1149(1.0) & **125(1.1)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & 252.00(0.00) & 23.00(0.00) \\  & Swath & 1126(1.0) & **115(1.1)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** \\ \hline \multirow{4}{*}{LMAM3-13B} & MP & 6016(1.0) & **125(1.1)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** \\  & Swath & 1138(1.0) & **1178(1.0)** & **1100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** \\ \hline \multirow{4}{*}{LMAM3-13B} & MP & 6016(1.0) & **125(1.1)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** \\  & Swath & 1146(1.0) & **125(1.1)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** & **100(1.0)** \\ \hline \multirow{4}{*}{LMAM2-13B} & MP & 1250(1.0) & **125(1.1)** & **125(1.1)** & **100(1.0)** & **100(1.

[MISSING_PAGE_EMPTY:25]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: At the end of the introduction, we include a paragraph that clearly states the contributions and scope of our paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We briefly discuss the limitation of our work in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We clearly state all assumptions in Theorem 1 and provide a formal proof in Appendix A. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We thoroughly describe our proposed pruning approach in Algorithms 1 and 2, and we include all other details necessary for reproducing the results in Appendix B.1. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our code is available at https://github.com/mazumder-lab/ALPS. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide detailed settings of our proposed pruning framework and its competitors used in our experiments in Appendix B.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In our experiments, for each combination of model, pruning method, and sparsity level, we ran 5 different seeds and reported the mean and standard deviation of the results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details of the computational resources used for our experiments in Appendix B.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and ensured that the research conducted in this paper fully complies with it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: To the best of our knowledge, our work has no societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: To the best of our knowledge, our work poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: At the beginning of Section 4, we cite all the datasets and models used in our experiments. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.