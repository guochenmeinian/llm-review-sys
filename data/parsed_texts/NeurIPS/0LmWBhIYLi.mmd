# Universal Prompt Tuning for Graph Neural Networks

 Taoran Fang\({}^{1}\), Yunchao Zhang\({}^{1}\), Yang Yang\({}^{1}\), Chunping Wang\({}^{2}\), Lei Chen\({}^{2}\)

\({}^{1}\)Zhejiang University, \({}^{2}\)FinVolution Group

{fangtr,3190105622,yangx}@zju.edu.cn,

{wangchunping02,chenlei04}@xinye.com

Corresponding author.

###### Abstract

In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called _Graph Prompt Feature (GPF)_ for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to obtain the prompted graph for the downstream task in an adaptive manner. We provide rigorous derivations to demonstrate the universality of GPF and make guarantee of its effectiveness. The experimental results under various pre-training strategies indicate that our method performs better than fine-tuning, with an average improvement of about \(1.4\%\) in full-shot scenarios and about \(3.2\%\) in few-shot scenarios. Moreover, our method significantly outperforms existing specialized prompt-based tuning methods when applied to models utilizing the pre-training strategy they specialize in. These numerous advantages position our method as a compelling alternative to fine-tuning for downstream adaptations. Our code is available at: https://github.com/zjunet/GPF.

## 1 Introduction

Graph neural networks (GNNs) have garnered significant attention from researchers due to their remarkable success in graph representation learning (Kipf and Welling, 2017; Hamilton et al., 2017; Xu et al., 2019). However, two fundamental challenges hinder the large-scale practical applications of GNNs. One is the scarcity of labeled data in the real world (Zitnik et al., 2018), and the other is the low out-of-distribution generalization ability of the trained models (Hu et al., 2020; Knyazev et al., 2019; Yehudai et al., 2021; Morris et al., 2019). To overcome these challenges, researchers have made substantial efforts in designing pre-trained GNN models (Xia et al., 2022; Hu et al., 2020; Lu et al., 2021) in recent years. Similar to the pre-trained models in the language field, pre-trained GNN models undergo training on extensive pre-training datasets and are subsequently adapted to downstream tasks. Most existing pre-trained GNN models obey the "pre-train, fine-tune" learning strategy (Xu et al., 2021). Specifically, we train a GNN model with a massive corpus of pre-training graphs, then we utilize the pre-trained GNN model as initialization and fine-tune the model parameters based on the specific downstream task.

However, the "pre-train, fine-tune" framework of pre-trained GNN models also presents several critical issues [17]. First, there is a misalignment between the objectives of pre-training tasks and downstream tasks [16]. Most existing pre-trained models employ self-supervised tasks [16] such as edge prediction and attribute masking as the training targets during pre-training, while the downstream tasks involve graph or node classification. This disparity in objectives leads to sub-optimal performance [16]. Additionally, ensuring that the model retains its generalization ability is challenging. Pre-trained models may suffer from catastrophic forgetting [15, 16] during downstream adaptation. This issue becomes particularly acute when the downstream data is small in scale, approaching the few-shot scenarios [14]. The pre-trained model tends to over-fit the downstream data in such cases, rendering the pre-training process ineffective.

_"If your question isn't getting the desired response, try rephrasing it."_ In recent years, a novel approach called prompt tuning has emerged as a powerful method for downstream adaptation, addressing the aforementioned challenges. This technique has achieved significant success in Natural Language Processing [16, 17, 18, 16] and Computer Vision [15, 16]. Prompt tuning provides an alternative method for adapting pre-trained models to specific downstream tasks: it freezes the parameters of the pre-trained model and modifies the input data. Unlike fine-tuning, prompt tuning diverges from tuning the parameters of the pre-trained model and instead focuses on adapting the data space by transforming the input.

Despite that, applying prompt tuning on pre-trained GNN models poses significant challenges and is far from straightforward. First, the diverse pre-training strategies employed on graphs make it difficult to design suitable prompting functions. Previous research [16] suggests that the prompting function should be closely aligned with the pre-training strategy. For pre-trained language models, the typical pre-training tasks involve masked sentence completion [14]. In order to align with this task, we may modify a sentence like "I received a gift" to "I received a gift, and I feel [Mask]" to make it closer to the task of sentence completion. However, in the case of graph pre-training, there is no unified pre-training task, making it challenging to design feasible prompting functions. Some pioneering studies [21, 16] have applied prompt-based tuning methods to models pre-trained by edge prediction [13]. They introduce virtual class-prototype nodes/graphs with learnable links into the original graph, making the adaptation process more akin to edge prediction. However, these methods have limited applicability and are only compatible with specific models. When it comes to more intricate pre-training strategies, it becomes challenging to design manual prompting functions in the same manner as employed for link prediction. Consequently, no prompt-based tuning method is available for models pre-trained using alternative strategies, such as attribute masking [15]. Furthermore, existing prompt-based tuning methods for GNN models are predominantly designed based on intuition, lacking theoretical guarantees for their effectiveness.

In this paper, we address the aforementioned issues for graph prompt tuning. To deal with the diversity of graph pre-training strategies, we propose a universal prompt-based tuning method that can be

Figure 1: **Comparison of universal graph prompt tuning and existing approaches. (a) Fine-tuning updates the parameters of the pre-trained GNN model. (b) Existing specialized prompt-based tuning methods generate manual graph templates to adapt the models under certain pre-training strategies. (c) Our universal graph prompt tuning works on the feature space of the input graph. It can achieve an equivalent effect to any form of prompting function and be applied to any pre-trained GNN model.**

applied to the pre-trained GNN models that employ any pre-training strategy. Figure 1 illustrates the distinction between our universal prompt-based tuning method and existing approaches. Our solution, called Graph Prompt Feature (GPF), operates on the input graph's feature space and involves adding a shared learnable vector to all node features in the graph. This approach is easily applicable to any GNN architecture. We rigorously demonstrate that GPF can achieve comparable results to any form of prompting function when applied to arbitrary pre-trained GNN models. Consequently, instead of explicitly illustrating the prompting function corresponding to each pre-training strategy, we adopt GPF to dynamically obtain the prompted graph for downstream tasks. We also introduce a theoretically stronger variant of GPF, named GPF-plus, for practical application, which incorporates different prompted features for different nodes in the graph. To guarantee the effectiveness of our proposed GPF and GPF-plus, we provide theoretical analyses to prove that GPF and GPF-plus are not weaker than full fine-tuning and can obtain better theoretical tuning results in some cases. Furthermore, we conduct extensive experiments to validate the efficacy of our methods. Despite using a significantly smaller number of tunable parameters than fine-tuning, GPF and GPF-plus achieve better results across all pre-training strategies. For models pre-trained using edge prediction, GPF and GPF-plus exhibit a substantial performance advantage over existing specialized prompt-based tuning methods. Overall, the contributions of our work can be summarized as follows:

* To the best of our knowledge, we present the first investigation of universal prompt-based tuning methods for existing pre-trained GNN models. We propose GPF and its variant, GPF-plus, as novel approaches for universal graph prompt tuning. Our methods can be applied to the pre-trained GNN models that employ any pre-training strategy.
* We provide theoretical guarantees for the effectiveness of GPF and GPF-plus. We demonstrate that GPF and GPF-plus can achieve an equivalent effect to any prompting function and can obtain better tuning results in some cases compared to fine-tuning.
* We conduct extensive experiments (both full-shot and few-shot scenarios) to validate the effectiveness of GPF and GPF-plus. The experimental results indicate that GPF and GPF-plus can perform better than fine-tuning, with an average improvement of about \(1.4\%\) in full-shot scenarios and about \(3.2\%\) in few-shot scenarios. Furthermore, GPF and GPF-plus significantly outperform existing prompt-based tuning methods when applied to models that utilize the pre-training strategy they specialize in.

## 2 Related work

Pre-trained GNN ModelsInspired by the remarkable achievements of pre-trained models in Natural Language Processing (Qiu et al., 2020) and Computer Vision (Long et al., 2022), substantial efforts have been dedicated to pre-trained GNN models (PGMs) (Xia et al., 2022) in recent years. These methods utilize self-supervised strategies (Jin et al., 2020) to acquire meaningful representations from extensive pre-training graphs. GAE (Kipf and Welling, 2016) first uses edge prediction as the objective task to train graph representations. Deep Graph Infomax (DGI) (Velickovic et al., 2019) and InfoGraph (Sun et al., 2019) are proposed to garner nodes or graph representations by maximizing the mutual information between graph-level and substructure-level representations of different granularity. Hu et al. (2020) employ attribute masking and context prediction as pre-training tasks to predict molecular properties and protein functions. Both GROVER (Rong et al., 2020) and MGSSL (Zhang et al., 2021) propose to predict the presence of the motifs or generate them with the consideration that rich domain knowledge of molecules hides in the motifs. Graph Contrastive Learning (GCL) is another widely adopted pre-training strategy for GNN models. GraphCL (You et al., 2020) and JOAO (You et al., 2021) propose various augmentation strategies to generate different augmented views for contrastive learning. In summary, there exists a diverse range of pre-training strategies for GNN models, each characterized by unique objectives.

Prompt-based Tuning MethodsPrompt-based tuning methods, originating from Natural Language Processing, have been widely used to facilitate the adaptation of pre-trained language models to various downstream tasks (Liu et al., 2021). Research has also explored the design of soft prompts to achieve optimal performance (Lester et al., 2021, Liu et al., 2021). These methods freeze the parameters of the pre-train models and introduce additional learnable components in the input space, thereby enhancing the compatibility between inputs and pre-trained models. Aside from the success of prompts in the language field, the prompting methods are utilized in other areas. Jia et al. (2022) and Bahng et al. (2022) investigate the efficacy of adapting large-scale models in the vision field by modifying input images at the pixel level. In the realm of graph neural networks, the exploration of prompt-based tuning methods is still limited. Some pioneering work (Sun et al., 2022; Liu et al., 2023) applies prompt-based tuning methods on the models pre-trained by edge prediction (Kipf and Welling, 2016). These methods introduce virtual class-prototype nodes/graphs with learnable links into the input graph, making the downstream adaptation more closely resemble edge prediction. However, these methods are specialized for models pre-trained using edge prediction and cannot be applied to models trained with other strategies. We are the first to investigate the universal prompt-based tuning methods that can be applied to the GNN models under any pre-training strategy.

## 3 Methodology

We introduce _graph prompt tuning_ for adapting pre-trained GNN models to downstream tasks. It is important to note that there are several types of downstream tasks in graph analysis, including node classification, link prediction, and graph classification. We first concentrate on the graph classification task and then extend our method to node-wise tasks. We define the notations in Section 3.1, then illustrate the process of graph prompt tuning in Section 3.2. We introduce our universal graph prompt tuning method in Section 3.3 and provide theoretical analyses in Section 3.4. Finally, we present the extension of our method to node-wise tasks (node classification and link prediction) in the appendix.

### Preliminaries

Let \(\mathcal{G}=(\mathcal{V},\mathcal{E})\in\mathbb{G}\) represents a graph, where \(\mathcal{V}=\{v_{1},v_{2},\ldots,v_{N}\}\), \(\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}\) denote the node set and edge set respectively. The node features can be denoted as a matrix \(\mathbf{X}=\{x_{1},x_{2},\ldots,x_{N}\}\in\mathbb{R}^{N\times F}\), where \(x_{i}\in\mathbb{R}^{F}\) is the feature of the node \(v_{i}\), and \(F\) is the dimensionality of node features. \(\mathbf{A}\in\{0,1\}^{N\times N}\) denotes the adjacency matrix, where \(\mathbf{A}_{ij}=1\) if \((v_{i},v_{j})\in\mathcal{E}\).

Fine-Tuning Pre-trained Models.Given a pre-trained GNN model \(f\), a learnable projection head \(\theta\) and a downstream task dataset \(\mathcal{D}=\{(\mathcal{G}_{1},y_{1}),\ldots,(\mathcal{G}_{m},y_{m})\}\), we adjust the parameters of the pre-trained model \(f\) and the projection head \(\theta\) to maximize the likelihood of predicting the correct labels \(y\) of the downstream graph \(\mathcal{G}\):

\[\max_{f,\theta}P_{f,\theta}(y|\mathcal{G})\] (1)

### Graph Prompt Tuning

Overall Process.Our proposed _graph prompt tuning_ works on the input space by drawing on the design of the prompt tuning in the language field (Liu et al., 2022). Given a frozen pre-trained GNN model \(f\), a learnable projection head \(\theta\), and a downstream task dataset \(\mathcal{D}=\{(\mathcal{G}_{1},y_{1}),\ldots,(\mathcal{G}_{m},y_{m})\}\), our target is to obtain a task-specific _graph prompt_\(g_{\phi}\colon\mathbb{G}\to\mathbb{G}\) parameterized by \(\phi\). The _graph prompt_\(g_{\phi}(\cdot)\) transforms the input graph \(\mathcal{G}\) into a specific _prompted graph_\(g_{\phi}(\mathcal{G})\). And then \(g_{\phi}(\mathcal{G})\) will replace \(\mathcal{G}\) as input to the pre-trained GNN model \(f\). During the downstream task training, we select the optimal parameters of \(\phi\) and \(\theta\) that maximize the likelihood of predicting the correct labels \(y\) without tuning the pre-trained model \(f\), which can be formulated as:

\[\max_{\phi,\theta}P_{f,\theta}(y|g_{\phi}(\mathcal{G}))\] (2)

During the evaluation stage, the test graph \(\mathcal{G}_{\text{test}}\) is first transformed by _graph prompt_\(g_{\phi}(\cdot)\), and the resulting prompted graph \(g_{\phi}(\mathcal{G}_{\text{test}})\) is processed through the frozen GNN model \(f\).

Practical Usage.In this part, we provide a detailed description of the refined process of _graph prompt tuning_, which comprises two fundamental steps: _template design_ and _prompt optimization_.

A. Template Design.Given an input graph \(\mathcal{G}\), we first generate a _graph template_\(\mathcal{G}^{*}\), which includes learnable components in its adjacency matrix \(\mathbf{A}^{*}\) and feature matrix \(\mathbf{X}^{*}\). Previous research has attributed the success of prompt tuning to bridging the gap between pre-training tasks and downstream tasks (Liu et al., 2022). Consequently, it implies that the specific form of the graph template is influenced by the pre-training strategy employed by the model. For a specific pre-training task \(t\in\mathbb{T}\) and an input graph \(\mathcal{G}\), the graph template \(\mathcal{G}^{*}\) can be expressed as:

\[\mathcal{G}^{*}\colon(\mathbf{A}^{*},\mathbf{X}^{*})=\psi_{t}(\mathcal{G})\] (3)where the graph template \(\mathcal{G}^{*}\) may contain learnable parameters (_i.e._, tunable links or node features) in its adjacency matrix or feature matrix (similar to the inclusion of learnable soft prompts in a sentence), the candidate space for \(\mathbf{A}^{*}\) is \(\mathbb{A}\), and the candidate space for \(\mathbf{X}^{*}\) is \(\mathbb{X}\).

_B. Prompt Optimization._ Once we have obtained the graph template \(\mathcal{G}^{*}\), our next step is to search for the optimal \(\mathbf{\hat{A}}\) and \(\mathbf{\hat{X}}\) within their respective candidate spaces \(\mathbb{A}\) and \(\mathbb{X}\) that maximize the likelihood of correctly predicting the labels \(y\) using the pre-trained model \(f\) and a learnable projection head \(\theta\). This process can be expressed as:

\[\max_{\mathbf{\hat{A}}\in\mathbb{A},\mathbf{\hat{X}}\in\mathbb{X},\theta}P_{f,\theta}(y|\mathcal{G}^{*})\] (4)

The graph \(\hat{\mathcal{G}}\) composed of \(\mathbf{\hat{A}}\) and \(\mathbf{\hat{X}}\) can be considered as the the _prompted graph_\(g_{\phi}(\mathcal{G})\) mentioned in Formula 2.

**Practical Challenges.** The specific form of the graph template is closely tied to the pre-training task \(t\) employed by the model \(f\). However, designing the prompting function \(\psi_{t}(\cdot)\) is challenging and varies for different pre-training tasks. Pioneering works (Sun et al., 2022, Liu et al., 2023) have proposed corresponding prompting functions \(\psi_{t}(\cdot)\) for a specific pre-training strategy, with a focus on models pre-trained using edge prediction. However, many other pre-training strategies (Hu et al., 2020, Xia et al., 2022), such as attribute masking and context prediction, are widely utilized in existing pre-trained GNN models, yet no research has been conducted on designing prompting functions for these strategies. Furthermore, existing prompting functions are all intuitively designed, and these manual prompting functions lack a guarantee of effectiveness. It raises a natural question: _Can we design a universal prompting method that can be applied to any pre-trained model, regardless of the underlying pre-training strategy?_

### Universal Graph Prompt Design

In this section, we introduce a universal prompting method and its variant. Drawing inspiration from the success of pixel-level Visual Prompt (VP) techniques (Bahng et al., 2022, Wu et al., 2022, Xing et al., 2022) in Computer Vision, our methods introduce learnable components to the feature space of the input graph. In Section 3.4, we will demonstrate that these prompting methods can theoretically achieve an equivalent effect as any prompting function \(\psi_{t}(\cdot)\).

**Graph Prompt Feature (GPF).** GPF focuses on incorporating additional learnable parameters into the feature space of the input graph. Specifically, the learnable component \(p\) is a vector of dimension \(F\), where \(F\) corresponds to the dimensionality of the node features. It can be denoted as:

\[p\in\mathbb{R}^{F}\] (5)

The learnable vector \(p\) is added to the graph features \(\mathbf{X}\) to generate the prompted features \(\mathbf{X}^{*}\), which can be expressed as:

\[\mathbf{X}=\{x_{1},x_{2},\ldots,x_{N}\}\quad\mathbf{X}^{*}=\{x_{1}+p,x_{2}+p, \ldots,x_{N}+p\}\] (6)

The prompted features \(\mathbf{X}^{*}\) replace the initial features \(\mathbf{X}\) and are processed by the pre-trained model.

**Graph Prompt Feature-Plus (GPF-plus).** Building upon GPF, we introduce a variant called GPF-plus, which assigns an independent learnable vector \(p_{i}\) to each node \(v_{i}\) in the graph. It can be expressed as:

\[p_{1},p_{2},\ldots p_{N}\in\mathbb{R}^{F}\] (7)

\[\mathbf{X}=\{x_{1},x_{2},\ldots,x_{N}\}\quad\mathbf{X}^{*}=\{x_{1}+p_{1},x_{2} +p_{2},\ldots,x_{N}+p_{N}\}\] (8)

Similarly to GPF, the prompted features \(\mathbf{X}^{*}\) replace the initial features \(\mathbf{X}\) and are processed by the pre-trained model. However, such a design is not universally suitable for all scenarios. For instance, when training graphs have different scales (i.e., varying node numbers), it is challenging to train such a series of \(p_{i}\). Additionally, when dealing with large-scale input graphs, such design requires a substantial amount of storage resources due to its \(O(N)\) learnable parameters. To address these issues, we introduce an attention mechanism in the generation of \(p_{i}\), making GPF-plus more parameter-efficient and capable of handling graphs with different scales. In practice, we train only \(k\) independent basis vectors \(p^{b}\), which can be expressed as:

\[p_{1}^{b},p_{2}^{b},\ldots p_{k}^{b}\in\mathbb{R}^{F}\] (9)where \(k\) is a hyper-parameter that can be adjusted based on the downstream dataset. To obtain \(p_{i}\) for node \(v_{i}\), we utilize attentive aggregation of these basis vectors with the assistance of \(k\) learnable linear projections \(a\). The calculation process can be expressed as:

\[p_{i}=\sum_{j}^{k}\alpha_{i,j}p_{j}^{b}\qquad\alpha_{i,j}=\frac{\exp(a_{j}^{T}x_ {i})}{\sum_{l}^{k}\exp(a_{l}^{T}x_{i})}\] (10)

Subsequently, \(p_{i}\) is used to generate the prompted feature \(\mathbf{X}^{*}\) as described in Formula 8.

### Theoretical Analysis

In this section, we provide theoretical analyses for our proposed GPF and GPF-plus. Our analyses are divided into two parts. First, we certify the universality of our methods. We demonstrate that our approaches can theoretically achieve results equivalent to any prompting function \(\psi_{t}(\cdot)\). It confirms the versatility and applicability of our methods across different pre-training strategies. Then, we make guarantee of the effectiveness of our proposed methods. Specifically, we demonstrate that our proposed graph prompt tuning is not weaker than full fine-tuning, which means that in certain scenarios, GPF and GPF-plus can achieve superior tuning results compared to fine-tuning. It is important to note that our derivations in the following sections are based on GPF, which adds a global extra vector \(p\) to all nodes in the graph. GPF-plus, being a more powerful version, can be seen as an extension of GPF and degenerates to GPF when the hyperparameter \(k\) is set to \(1\). Therefore, the analyses discussed for GPF are also applicable to GPF-plus.

Before we illustrate our conclusions, we first provide some preliminaries. For a given pre-training task \(t\in\mathbb{T}\) and an input graph \(\mathcal{G}\colon(\mathbf{A},\mathbf{X})\), we assume the existence of a prompting function \(\psi_{t}(\cdot)\) that generates a graph template \(\mathcal{G}^{*}\colon(\mathbf{A}^{*},\mathbf{X}^{*})=\psi_{t}(\mathcal{G})\). The candidate space for \(\mathbf{A}^{*}\) and \(\mathbf{X}^{*}\) is denoted as \(\mathbb{A}\) and \(\mathbb{X}\), respectively.

**Theorem 1**.: _(Universal Capability of GPF) Given a pre-trained GNN model \(f\), an input graph \(\mathcal{G}\colon(\mathbf{A},\mathbf{X})\), an arbitrary prompting function \(\psi_{t}(\cdot)\), for any prompted graph \(\hat{\mathcal{G}}\colon(\hat{\mathbf{A}}\in\mathbb{A},\hat{\mathbf{X}}\in \mathbb{X})\) in the candidate space of the graph template \(\mathcal{G}^{*}=\psi_{t}(\mathcal{G})\), there exists a GPF extra feature vector \(\hat{p}\) that satisfies:_

\[f(\mathbf{A},\mathbf{X}+\hat{p})=f(\hat{\mathbf{A}},\hat{\mathbf{X}})\] (11)

The complete proof of Theorem 1 can be found in the appendix. Theorem 1 implies that GPF can achieve the theoretical performance upper bound of any prompting function described in Formula 3 and 4. Specifically, if optimizing the graph template \(\mathcal{G}^{*}\) generated by a certain prompting function \(\psi_{t}(\cdot)\) can yield satisfactory graph representations, then theoretically, optimizing the vector \(p\) of GPF can also achieve the exact graph representations. This conclusion may initially appear counter-intuitive since GPF only adds learnable components to node features without explicitly modifying the graph structure. The key lies in understanding that the feature matrix \(\mathbf{X}\) and the adjacency matrix \(\mathbf{A}\) are not entirely independent during the processing. The impact of graph structure modifications on the final graph representations can also be obtained through appropriate modifications to the node features. Therefore, GPF and GPF-plus, by avoiding the explicit illustration of the prompting function \(\psi_{t}(\cdot)\), adopt a simple yet effective architecture that enables them to possess universal capabilities in dealing with pre-trained GNN models under various pre-training strategies.

Next, we make guarantee of the effectiveness of GPF and demonstrate that GPF is not weaker than fine-tuning, which means GPF can achieve better theoretical tuning results in certain situations compared to fine-tuning. In Natural Language Processing, the results obtained from fine-tuning are generally considered the upper bound for prompt tuning results (Lester et al., 2021; Liu et al., 2021; Ding et al., 2022). It is intuitive to believe that fine-tuning, which allows for more flexible and comprehensive parameter adjustments in the pre-trained model, can lead to better theoretical results during downstream adaptation. However, in the graph domain, the architecture of graph neural networks magnifies the impact of input space transformation on the final representations to some extent. To further illustrate this point, following previous work (Kumar et al., 2022; Tian et al., 2023; Wei et al., 2021), we assume that the downstream task utilizes the squared regression loss \(l=\sum_{i}(\hat{y}_{i}-y_{i})^{2}\).

**Theorem 2**.: _(Effectiveness Guarantee of GPF) For a pre-trained GNN model \(f\), a series of graphs \(\mathcal{D}=\{(\mathcal{G}_{1}\colon(\mathbf{A}_{1},\mathbf{X}_{1}),y_{1}), \ldots,(\mathcal{G}_{m}\colon(\mathbf{A}_{m},\mathbf{X}_{m}),y_{m})\}\) under the non-degeneracy condition, and a linear projection head \(\theta\), there exists \(\mathcal{Y}^{\prime}=\{y^{\prime}_{1},\ldots,y^{\prime}_{m}\}\) for \(y_{1}=y^{\prime}_{1},\ldots,y_{m}=y^{\prime}_{m}\) that satisfies:_

\[l_{\mathrm{GPF}}=\min_{p,\theta}\sum_{i}^{m}(f(\mathbf{A}_{i},\mathbf{X}_{i}+p )\cdot\theta-y_{i})^{2}<l_{\mathrm{FT}}=\min_{f,\theta}\sum_{i}^{m}(f( \mathbf{A}_{i},\mathbf{X}_{i})\cdot\theta-y_{i})^{2}\] (12)

The detailed proof of Theorem 2 and the description of the degeneracy condition can be found in the appendix. Theorem 2 indicates that GPF obtains a lower minimum loss compared to fine-tuning in certain scenarios, demonstrating its ability to achieve better theoretical tuning results.

## 4 Experiments

### Experiment Setup

**Model Architecture and Datasets.** We adopt the widely used 5-layer GIN [Xu et al., 2019] as the underlying architecture for our models, which aligns with the majority of existing pre-trained GNN models [Xia et al., 2022b, Hu et al., 2020a, Qiu et al., 2020a, You et al., 2020, Suresh et al., 2021, Xu et al., 2021b, Zhang et al., 2021b, You et al., 2022, Xia et al., 2022a]. As for the benchmark datasets, we employ the chemistry and biology datasets published by Hu et al. [2020a]. A comprehensive description of these datasets can be found in the appendix.

**Pre-training Strategies.** We employ five widely used strategies (tasks) to pre-train the GNN models, including Deep Graph Infomax (denoted by Infomax) [Velickovic et al., 2019a], Edge Prediction (denoted by EdgePred) [Kipf and Welling, 2016a], Attribute Masking (denoted by AttrMasking) [Hu et al., 2020a], Context Prediction (denoted by ContextPred) [Hu et al., 2020a] and Graph Contrastive Learning (denoted by GCL) [You et al., 2020]. A detailed description of these pre-training strategies can be found in the appendix.

**Tuning Strategies.** We adopt the pre-trained models to downstream tasks with different tuning strategies. Given a pre-trained GNN model \(f\), a task-specific projection head \(\theta\),

* _Fine Tuning_ (denoted as FT). We tune the parameters of the pre-trained GNN model \(f\) and the projection head \(\theta\) simultaneously during the downstream training stage.
* _Graph Prompt Feature_ (denoted as GPF). We freeze the parameters of the pre-trained model \(f\) and introduce an extra learnable feature vector \(p\) into the feature space of the input graph described as Formula 6. We tune the parameters of the projection head \(\theta\) and feature vector \(p\) during the downstream training stage.
* _Graph Prompt Feature-Plus_ (denoted as GPF-plus). We freeze the parameters of pre-trained model \(f\) and introduce \(k\) learnable basis vectors \(p_{1}^{b},\ldots,p_{k}^{b}\) with \(k\) learnable linear projections \(a_{1},\ldots,a_{k}\) to calculate the node-wise \(p_{i}\) as Formula 10. We tune the parameters of the projection head \(\theta\), basis vectors \(p_{1}^{b},\ldots,p_{k}^{b}\), and linear projections \(a_{1},\ldots,a_{k}\) during the downstream training stage.

**Implementation.** We perform five rounds of experiments with different random seeds for each experimental setting and report the average results. The projection head \(\theta\) is selected from a range of [1, 2, 3]-layer MLPs with equal widths. The hyper-parameter \(k\) of GPF-plus is chosen from the range [5,10,20]. Further details on the hyper-parameter settings can be found in the appendix.

### Main Results

We compare the downstream performance of models trained using different pre-training and tuning strategies, and the overall results are summarized in Table 1. Our systematic study suggests the following observations:

_1. Our graph prompt tuning outperforms fine-tuning in most cases._ Based on the results presented in Table 1, it is evident that GPF and GPF-plus achieve superior performance compared to fine-tuning in the majority of cases. Specifically, GPF outperforms fine-tuning in \(28/36\) experiments, while GPF-plus outperforms fine-tuning in \(29/36\) experiments. It is worth noting that the tunable parameters in GPF and GPF-plus are significantly fewer in magnitude than those in fine-tuning (details can be found in the appendix). These experimental findings highlight the efficacy of our methods and demonstrate their capability to unleash the power of the pre-trained models.

_2. GPF and GPF-plus exhibit universal capability across various pre-training strategies._ GPF and GPF-plus present favorable tuning performance across all pre-training strategies examined in our experiments, consistently surpassing the average results obtained from fine-tuning. Specifically, GPF achieves an average improvement of \(1.14\%\), while GPF-plus achieves an average improvement of \(1.60\%\). These results signify the universal capability of GPF and GPF-plus, enabling their application to models trained with any pre-training strategy.

_3. GPF-plus marginally outperforms GPF._ Among the two graph prompt tuning methods, GPF-plus performs better than GPF in the majority of experiments (\(26/36\)). As discussed in Section 3.3, GPF-plus offers greater flexibility and expressiveness compared to GPF. The results further affirm that GPF-plus is an enhanced version of graph prompt tuning, aligning with the theoretical analysis.

### Comparison with Existing Graph Prompt-based Methods

We also conducted a comparative analysis between our proposed methods, GPF and GPF-plus, and existing graph prompt-based tuning approaches (Sun et al., 2022, Liu et al., 2023). Both of them

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline Pre-training Strategy & Tuning Strategy & BBBP & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE & PPI & **Avg.** \\ \hline \multirow{4}{*}{Infomax} & FT & **67.55** & 78.57 & 65.16 & 63.34 & 70.06 & **81.42** & 77.71 & 81.32 & 71.29 & \multirow{4}{*}{72.93} \\  & & \(\pm 2.06\) & \(\pm 0.51\) & \(\pm 0.53\) & \(\pm 0.45\) & \(\pm 1.45\) & \(\pm 2.65\) & \(\pm 0.45\) & \(\pm 1.25\) & \(\pm 1.79\) \\  & GPF & **66.83** & 79.09 & 66.10 & **66.17** & 73.56 & 80.43 & 76.49 & 83.60 & 77.02 & \multirow{4}{*}{74.36} \\  & & \(\pm 0.86\) & \(\pm 0.25\) & \(\pm 0.53\) & \(\pm 0.81\) & \(\pm 3.94\) & \(\pm 0.53\) & \(\pm 0.18\) & \(\pm 1.00\) & \(\pm 0.42\) & \\  & GPF-plus & **67.17** & **79.13** & **66.35** & **65.62** & **75.12** & **81.33** & **77.73** & **83.67** & **77.03** & \multirow{4}{*}{74.79} \\  & & \(\pm 0.36\) & \(\pm 0.70\) & \(\pm 0.37\) & \(\pm 0.74\) & \(\pm 2.45\) & \(\pm 1.52\) & \(\pm 1.14\) & \(\pm 1.08\) & \(\pm 0.32\) & \\ \hline \multirow{4}{*}{AttrMasking} & FT & 66.33 & **78.28** & 65.34 & 66.77 & **74.46** & **81.78** & **77.90** & 80.94 & 73.93 & \multirow{4}{*}{73.97} \\  & & \(\pm 0.55\) & \(\pm 0.05\) & \(\pm 0.30\) & \(\pm 0.13\) & \(\pm 2.82\) & \(\pm 1.95\) & \(\pm 0.18\) & \(\pm 1.99\) & \(\pm 1.17\) & \\  & GPF & **68.09** & **79.04** & 66.32 & **69.13** & 75.06 & **82.17** & **78.86** & **84.33** & **78.91** & \multirow{4}{*}{75.76} \\  & & \(\pm 0.38\) & \(\pm 0.90\) & \(\pm 0.42\) & \(\pm 1.16\) & \(\pm 1.02\) & \(\pm 0.65\) & \(\pm 1.42\) & \(\pm 0.54\) & \(\pm 0.25\) & \\  & GPF-plus & 67.71 & **78.87** & **66.58** & **68.65** & **76.17** & **81.12** & **78.13** & **85.76** & **78.90** & \multirow{4}{*}{**75.76**} \\  & & \(\pm 0.64\) & \(\pm 0.31\) & \(\pm 0.13\) & \(\pm 0.72\) & \(\pm 9.98\) & \(\pm 1.32\) & \(\pm 1.12\) & \(\pm 0.36\) & \(\pm 0.11\) & \\ \hline \multirow{4}{*}{ContextPred} & FT & **69.65** & 78.29 & 66.36 & 64.45 & 73.71 & 82.36 & **79.20** & 84.66 & 72.10 & \multirow{4}{*}{74.53} \\  & & \(\pm 0.87\) & \(\pm 0.44\) & \(\pm 0.57\) & \(\pm 0.6\) & \(\pm 1.57\) & \(\pm 1.22\) & \(\pm 0.51\) & \(\pm 0.84\) & \(\pm 1.94\) & \\  & GPF & **68.48** & **79.99** & **67.92** & **66.18** & **74.51** & **84.34** & **78.62** & **85.32** & **77.42** & \multirow{4}{*}{75.86} \\  & & \(\pm 0.88\) & \(\pm 0.24\) & \(\pm 0.35\) & \(\pm 0.46\) & \(\pm 2.72\) & \(\pm 0.25\) & \(\pm 1.46\) & \(\pm 0.41\) & \(\pm 0.07\) & \\  & GPF-plus & **69.15** & **80.05** & **67.58** & **66.94** & **75.25** & **84.48** & **78.40** & **85.81** & **77.71** & \\  & & \(\pm 0.82\) & \(\pm 0.46\) & \(\pm 0.54\) & \(\pm 0.95\) & \(\pm 1.88\) & \(\pm 0.78\) & \(\pm 0.16\) & \(\pm 0.43\) & \(\pm 0.21\) & **76.15** \\ \hline \multirow{4}{*}{GCL} & FT & **69.49** & **73.35** & **62.54** & **60.63** & **75.17** & **69.78** & **78.26** & **75.51** & 67.76 & \multirow{4}{*}{70.27} \\  & & \(\pm 0.35\) & \(\pm 0.70\) & \(\pm 0.26\) & \(\pm 1.26\) & \(\pm 2.14\) & \(\pm 1.44\) & \(\pm 0.73\) & \(\pm 2.01\) & \(\pm 0.78\) & \\  & GPF & **71.11** & **73.64** & **62.70** & **61.26** & **70.09** & **75.52** & **78.55** & **67.60** & & \\  & & \(\pm 1.20\) & \(\pm 0.25\) & \(\pm 0.46\) & \(\pm 0.53\) & \(\pm 2.98\) & \(\pm 0.67\) & \(\pm 1.09\) & \(\pm 0.56\) & \(\pm 0.57\) & \(\phantom{0.28}\) \\  & GPF-plus & **72.18** & 73.35** & **62.76** & **62.37** & 73.90 & **72.94** & **77.51** & **79.61** & **67.89** & **71.39** \\  & & \(\pm 0.93\) & \(\pm 0.43\) & \(\pm 0.75\) & \(\pm 0.38\) & \(\pm 2.47\) & \(\pm 1.87\) & \(\pm 0.82\) & \(\pm 2.06\) & \(\pm 0.69\) & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test ROC-AUC (%) performance on molecular prediction benchmarks and protein function prediction benchmarks with different pre-training strategies and different tuning strategies.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline Pre-training Strategy & Tuning Strategy & BBBP & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE & PPI & **Avg.** \\ \hline \multirow{4}{*}{EdgePred} & FT & 66.56 & 78.67 & **66.29** & 64.35 & 69.07 & 79.67 & 77.44 & 80.90specialize in tuning the models pre-trained by Edge Prediction (also known as Link Prediction). We apply GPPT (Sun et al., 2022), GPPT without orthogonal prompt constraint loss (denoted as GPPT (w/o ol)) (Sun et al., 2022), GraphPrompt (Liu et al., 2023) to the models pre-trained using Edge Prediction, and the results are summarized in Table 2. It is worth mentioning that GPPT is originally designed for node classification tasks. Therefore, we make minor modifications by substituting class-prototype nodes with class-prototype graphs to adapt it for graph classification tasks. The experimental results indicate that our proposed GPF and GPF-plus outperform existing graph prompt-based tuning methods by a significant margin. On the chemistry and biology benchmarks, GPF and GPF-plus achieve average improvements of \(12\%\), \(3\%\), and \(13\%\) over GPPT, GPPT (w/o ol), and GraphPrompt, respectively. These results showcase the ability of GPF and GPF-plus to achieve superior results compared to existing graph prompt-based tuning methods designed specifically for the pre-training strategy. Furthermore, it is worth highlighting that GPF and GPF-plus are the only two graph prompt-based tuning methods that surpass the performance of fine-tuning.

### Additional Experiments

Few-shot graph classification.Prompt tuning has also been recognized for its effectiveness in addressing few-shot downstream tasks (Brown et al., 2020; Schick and Schutze, 2020, 2020; Liu et al., 2021; Liu et al., 2021; 2023). We evaluate the efficacy of our proposed methods in handling few-shot scenarios. To conduct few-shot graph classification on the chemistry and biology datasets, we limit the number of training samples in the downstream tasks to 50 (compared to the original range of 1.2k to 72k training samples). The results are summarized in Table 4 of the appendix. Compared to the full-shot scenarios, our proposed graph prompt tuning demonstrates even more remarkable performance improvement (an average improvement of \(2.95\%\) for GPF and \(3.42\%\) for GPF-plus) over fine-tuning in the few-shot scenarios. This finding indicates that our solutions retain a higher degree of generalization ability in pre-trained models during few-shot downstream adaptations compared to fine-tuning.

Training process analysis.We conducted an analysis of the training process using different tuning methods on the biology datasets with the GNN models that employ Attribute Masking and Context Prediction as their pre-training tasks (Hu et al., 2020). Figure 2 presents the training and test curves during the adaptation stage. From Figure 2 (a), it can be observed that the ROC-AUC scores of the training set consistently increase during the adaptation stage for both our proposed methods and fine-tuning. However, from Figure 2 (b), we can find that their behavior on the test set is quite distinct. For fine-tuning, the ROC-AUC scores on the test set exhibit fluctuations and continuously decrease after an initial increase. On the other hand, when applying GPF or GPF-plus to adapt pre-trained models, the ROC-AUC scores on the test set continue to grow and remain consistently high. These results indicate that fully fine-tuning a pre-trained GNN model on a downstream task may lose the model's generalization ability. In contrast, employing our proposed graph prompt tuning methods can significantly alleviate this issue and maintain superior performance on the test set.

## 5 Conclusion

In this paper, we introduce a universal prompt-based tuning method for pre-trained GNN models. Our method GPF and its variant GPF-plus operate on the feature space of the downstream input graph. GPF and GPF-plus can theoretically achieve an equivalent effect to any form of prompting function, meaning we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we can adaptively use GPF to obtain the prompted graph for downstream task adaptation. Compared to fine-tuning, the superiority of our method is demonstrated both theoretically and empirically, making it a compelling alternative for downstream adaptations.

Figure 2: Training and test curves of different tuning methods.

Acknowledgements

This work was partially supported by Zhejiang NSF (LR22F020005), the National Key Research and Development Project of China (2018AAA0101900), and the Fundamental Research Funds for the Central Universities.

## References

* Bahng et al. (2022) Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual prompts for adapting large-scale models. 2022.
* Bevilacqua et al. (2022) Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, G. Balamurugan, Michael M. Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. _ICLR_, 2022.
* Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. _NeurIPS_, 2020.
* Cotta et al. (2021) Leonardo Cotta, Christopher Morris, and Bruno Ribeiro. Reconstruction for powerful graph representations. _NeurIPS_, 2021.
* Ding et al. (2022) Ning Ding, Yujia Qin, Guang Yang, Fu Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weizee Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Haitao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juan Li, and Maosong Sun. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models. _ArXiv_, abs/2203.06904, 2022.
* Du et al. (2020) Simon Shaolei Du, Wei Hu, Sham M. Kakade, J. Lee, and Qi Lei. Few-shot learning via learning the representation, provably. _ICLR_, 2020.
* Frasca et al. (2022) Fabrizio Frasca, Beatrice Bevilacqua, Michael Bronstein, and Haggai Maron. Understanding and extending subgraph gnns by rethinking their symmetries. _NeurIPS_, 2022.
* D1107, 2012.
* Hamilton et al. (2017) Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _NeurIPS_, pages 1024-1034, 2017.
* He et al. (2021) Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll'ar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. _ArXiv_, abs/2111.06377, 2021.
* Hu et al. (2020a) Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. _ICLR_, 2020a.
* Hu et al. (2020b) Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-gnn: Generative pre-training of graph neural networks. _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, 2020b.
* Jia et al. (2022) Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. _ECCV_, 2022.
* Jin et al. (2020) Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang Tang. Self-supervised learning on graphs: Deep insights and new direction. _ArXiv_, abs/2006.10141, 2020.
* Kipf and Welling (2016a) Thomas Kipf and Max Welling. Variational graph auto-encoders. _ArXiv_, abs/1611.07308, 2016a.
* Kipf and Welling (2016b) Thomas N Kipf and Max Welling. Variational graph auto-encoders. In _ArXiv_, volume abs/1611.07308, 2016b.
* Kipf et al. (2016c)Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017.
* Klicpera et al. [2019] Johannes Klicpera, Stefan Weissenberger, and Stephan Gunnemann. Diffusion improves graph learning. In _Neural Information Processing Systems_, 2019.
* Knyazev et al. [2019] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In _NeurIPS_, 2019.
* Kumar et al. [2022] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. _ICLR_, 2022.
* Lester et al. [2021a] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. _EMNLP_, 2021a.
* Lester et al. [2021b] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. _arXiv preprint arXiv:2104.08691_, 2021b.
* Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, abs/2101.00190, 2021.
* Liu et al. [2020] Huihui Liu, Yiding Yang, and Xinchao Wang. Overcoming catastrophic forgetting in graph neural networks. In _AAAI Conference on Artificial Intelligence_, 2020.
* Liu et al. [2021a] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _arXiv preprint arXiv:2107.13586_, 2021a.
* Liu et al. [2022a] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys (CSUR)_, 2022a.
* Liu et al. [2021b] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. _ArXiv_, abs/2110.07602, 2021b.
* Liu et al. [2021c] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. _arXiv preprint arXiv:2110.07602_, 2021c.
* Liu et al. [2021d] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. _ArXiv_, abs/2103.10385, 2021d.
* Liu et al. [2022b] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In _ACL_, 2022b.
* Liu et al. [2021e] Yixin Liu, Shirui Pan, Ming Jin, Chuan Zhou, Feng Xia, and Philip S. Yu. Graph self-supervised learning: A survey. _IEEE Transactions on Knowledge and Data Engineering_, 35:5879-5900, 2021e.
* Liu et al. [2023] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. _Proceedings of the ACM Web Conference 2023_, 2023.
* Long et al. [2022] Siqu Long, Feiqi Cao, Soyeon Caren Han, and Haiqing Yang. Vision-and-language pretrained models: A survey. _IJCAI_, 2022.
* Lu et al. [2021] Yuanfu Lu, Xunqiang Jiang, Yuan Fang, and Chuan Shi. Learning to pre-train graph neural networks. In _AAAI_, 2021.
* Liu et al. [2021b]Andreas Mayr, Gunter Klambauer, Thomas Unterthiner, Marvin N. Steijaert, Jorg Kurt Wegner, Hugo Ceulemans, Djork-Arne Clevert, and Sepp Hochreiter. Large-scale comparison of machine learning methods for drug target prediction on chembl\(\dagger\)\(\dagger\)electronic supplementary information (esi) available: Overview, data collection and clustering, methods, results, appendix. see doi: 10.1039/c8sc00148k. _Chemical Science_, 9:5441 - 5451, 2018.
* Mesquita et al. (2020) Diego Mesquita, Amauri H. de Souza, and Samuel Kaski. Rethinking pooling in graph neural networks. _ArXiv_, abs/2010.11418, 2020.
* Morris et al. (2019) Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. _AAAI_, 2019.
* Qiu et al. (2020a) Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, 2020a.
* 1897, 2020b.
* Rong et al. (2020) Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. _Advances in Neural Information Processing Systems_, 33:12559-12571, 2020.
* Schick and Schutze (2020a) Timo Schick and Hinrich Schutze. Exploiting cloze-questions for few-shot text classification and natural language inference. In _Conference of the European Chapter of the Association for Computational Linguistics_, 2020a.
* Schick and Schutze (2020b) Timo Schick and Hinrich Schutze. It's not just size that matters: Small language models are also few-shot learners. _NAACL_, 2020b.
* ligand discovery for everyone. _Journal of Chemical Information and Modeling_, 55:2324
- 2337, 2015.
* Sun et al. (2019) Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. _arXiv preprint arXiv:1908.01000_, 2019.
* Sun et al. (2022) Mingchen Sun, Kaixiong Zhou, Xingbo He, Ying Wang, and Xin Wang. Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, 2022.
* Suresh et al. (2021) Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. In _Neural Information Processing Systems_, 2021.
* Tian et al. (2023) Junjiao Tian, Xiaoliang Dai, Chih-Yao Ma, Zecheng He, Yen-Cheng Liu, and Zsolt Kira. Trainable projected gradient method for robust fine-tuning. _ArXiv_, abs/2303.10720, 2023.
* Tripuraneni et al. (2020) Nilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. _NeurIPS_, 2020.
* Velickovic et al. (2019a) Petar Velickovic, William Fedus, William L. Hamilton, Pietro Lio', Yoshua Bengio, and R. Devon Hjelm. Deep graph infomax. _ICLR_, 2019a.
* Velickovic et al. (2019b) Petar Velickovic, William Fedus, William L. Hamilton, Pietro Lio', Yoshua Bengio, and R. Devon Hjelm. Deep graph infomax. _ICLR_, 2019b.
* Wei et al. (2021) Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning. _ArXiv_, abs/2106.09226, 2021.
* Wu et al. (2022) Junyang Wu, Xianhang Li, Chen Wei, Huiyu Wang, Alan Loddon Yuille, Yuyin Zhou, and Cihang Xie. Unleashing the power of visual prompting at the pixel level. _ArXiv_, abs/2212.10556, 2022.

Sen Wu, Hongyang Zhang, and Christopher Re. Understanding and improving information transfer in multi-task learning. _ICLR_, 2020.
* Wu et al. (2017) Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay S. Pande. Moleculenet: A benchmark for molecular machine learning. _arXiv: Learning_, 2017.
* Xia et al. (2022) Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z. Li. Simgrace: A simple framework for graph contrastive learning without data augmentation. _Proceedings of the ACM Web Conference 2022_, 2022a.
* Xia et al. (2022b) Jun Xia, Yanqiao Zhu, Yuanqi Du, and Stan Z Li. A survey of pretraining on graphs: Taxonomy, methods, and applications. _arXiv preprint arXiv:2202.07893_, 2022b.
* Xing et al. (2022) Yinghui Xing, Qirui Wu, De Cheng, Shizhou Zhang, Guoqiang Liang, and Yanning Zhang. Class-aware visual prompt tuning for vision-language pre-trained model. _ArXiv_, abs/2208.08340, 2022.
* under the pre-train and fine-tune paradigm. _NAACL_, 2021a.
* Xu et al. (2019) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _ICLR_, 2019.
* Xu et al. (2021b) Minghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. Self-supervised graph-level representation learning with local and global structure. In _International Conference on Machine Learning_, 2021b.
* Yanardag and Vishwanathan (2015) Pinar Yanardag and S. V. N. Vishwanathan. Deep graph kernels. _Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, 2015.
* Yehudai et al. (2021) Gilad Yehudai, Ethan Fetaya, Eli A. Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In _ICML_, 2021.
* You et al. (2020) Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. _NeurIPS_, 2020.
* You et al. (2021) Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In _International Conference on Machine Learning_, pages 12121-12132. PMLR, 2021.
* You et al. (2022) Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Bringing your own view: Graph contrastive learning without prefabricated data augmentations. _Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining_, 2022.
* Zhang et al. (2022) Chuxu Zhang, Kaize Ding, Jundong Li, Xiangliang Zhang, Yanfang Ye, N. Chawla, and Huan Liu. Few-shot learning on graphs. In _International Joint Conference on Artificial Intelligence_, 2022.
* Zhang and Li (2021) Muhan Zhang and Pan Li. Nested graph neural networks. _NeurIPS_, 2021.
* Zhang et al. (2016) Richard Zhang, Phillip Isola, and Alexei A. Efros. Colorful image colorization. In _ECCV_, 2016.
* Zhang et al. (2021a) Z. Zhang, Q. Liu, H. Wang, C. Lu, and C. K. Lee. Motif-based graph self-supervised learning for molecular property prediction. 2021a.
* Zhang et al. (2021b) Zaixin Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph self-supervised learning for molecular property prediction. In _Neural Information Processing Systems_, 2021b.
* Zhao et al. (2022) Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any gnn with local structure awareness. _ICLR_, 2022.
* Zhou and Cao (2021) Fan Zhou and Chengtai Cao. Overcoming catastrophic forgetting in graph neural networks with experience replay. In _AAAI Conference on Artificial Intelligence_, 2021.
* Zitnik et al. (2018) Marinka Zitnik, Rok Sosi, and Jure Leskovec. Prioritizing network communities. _Nature Communications_, 9, 2018.
* Zhang et al. (2021)Extra Materials for Section 3

### Extension to node-wise tasks

In this section, we illustrate the process of graph prompt tuning for node-wise tasks (node classification and link prediction). These tasks differ from graph classification, requiring node-level representations instead of graph-level representations. To bridge this gap, we employ _Subgraph GNNs_(Cotta et al., 2021; Zhang and Li, 2021; Bevilacqua et al., 2022; Zhao et al., 2022) to capture the graph-level and node-level representations. Subgraph GNN models utilize MPNNs on sets of subgraphs extracted from the original input graph. They subsequently aggregate the resulting representations (Frasca et al., 2022). As a result, the node representations can also be interpreted as graph representations of the induced subgraphs. Specifically, the node representation \(h_{i}\) for node \(v_{i}\) can be calculated as:

\[h_{i}=f(\mathcal{G}_{i})\] (13)

where \(f\) is a Subgraph GNN model, and \(\mathcal{G}_{i}\) is the induced subgraph for node \(v_{i}\). To obtain the node representations through graph prompt tuning, we freeze the model \(f\) and introduce a learnable graph prompt \(g_{\phi}\colon\mathbb{G}\to\mathbb{G}\), parameterized by \(\phi\), as indicated in Formula 2, which can be expressed as:

\[h_{i}=f(g_{\phi}(\mathcal{G}_{i}))\] (14)

Once we acquire the node representations, we can seamlessly proceed with downstream node classification or link prediction tasks. Our proposed methods, GPF and GPF-plus, enable the effective execution of node-level tasks through the approach described in Formula 14.

### Proof for Theorem 1

**Theorem 1**.: _(Universal Capability of GPF) Given a pre-trained GNN model \(f\), an input graph \(\mathcal{G}\colon(\mathbf{A},\mathbf{X})\), an arbitrary prompting function \(\psi_{t}(\cdot)\), for any prompted graph \(\hat{\mathcal{G}}\colon(\hat{\mathbf{A}}\in\mathbb{A},\hat{\mathbf{X}}\in \mathbb{X})\) in the candidate space of the graph template \(\mathcal{G}^{*}=\psi_{t}(\mathcal{G})\), there exists a GPF extra feature vector \(\hat{p}\) that satisfies:_

\[f(\mathbf{A},\mathbf{X}+\hat{p})=f(\hat{\mathbf{A}},\hat{\mathbf{X}})\]

_where \(\psi_{t}(\mathcal{G})=\mathcal{G}^{*}\colon(\mathbf{A}^{*},\mathbf{X}^{*})\), \(\mathbb{A}\) and \(\mathbb{X}\) are the candidate space for \(\mathbf{A}^{*}\) and \(\mathbf{X}^{*}\) respectively._

To illustrate Theorem 1, we provide the specific architecture of the pre-trained GNN model \(f\). For analytical simplicity, we initially assume that \(f\) is a single-layer GIN (Xu et al., 2019) with a linear transformation. Subsequently, we extend our conclusions to multi-layer models utilizing various transition matrices (Klicpera et al., 2019). During the generation of graph representations, we first obtain node representations and then employ a readout function to calculate the final graph representations. Existing pre-trained GNN models commonly employ _sum_ or _mean_ pooling for this purpose. Previous research (Mesquita et al., 2020) suggests that complex pooling mechanisms are unnecessary, as simple readout functions can yield superior performance. It is worth mentioning that the subsequent derivations still hold when we use any other weighted aggregation readout functions, such as average pooling, min/max pooling, and hierarchical pooling. Hence, we assume that the concrete architecture of the pre-trained GNN model \(f\) can be expressed as:

\[\mathbf{H}=(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot\mathbf{X}\cdot \mathbf{W}\] (15)

\[h_{\mathcal{G}}=\sum_{v_{i}\in\mathcal{V}}h_{i}\] (16)

where \(\mathbf{W}\) is a linear projection. The parameters \(\epsilon\) and \(\mathbf{W}\) have been pre-trained in advance and remain fixed during downstream adaptation. Next, we proceed with the derivation of Theorem 1. In Theorem 1, we impose no constraints on the form of the prompting function \(\psi_{t}(\cdot)\), allowing \(\hat{\mathbf{A}}\) and \(\hat{\mathbf{X}}\) to represent the adjacency matrix and feature matrix of any graph. We define the graph-level transformation \(g\colon\mathbb{G}\to\mathbb{G}\), which satisfies \((\hat{\mathbf{A}},\hat{\mathbf{X}})=g(\mathbf{A},\mathbf{X})\). Consequently, Theorem 1 is equivalent to Proposition 1.

**Proposition 1**.: _Given a pre-trained GNN model \(f\), an input graph \(\mathcal{G}\colon(\mathbf{A},\mathbf{X})\), for any graph-level transformation \(g\colon\mathbb{G}\to\mathbb{G}\), there exists a GPF extra feature vector \(\hat{p}\) that satisfies:_

\[f(\mathbf{A},\mathbf{X}+\hat{p})=f(g(\mathbf{A},\mathbf{X}))\] (17)To further illustrate the graph-level transformation \(g(\cdot)\), we decompose it into several specific transformations.

**Proposition 2**.: _Given an input graph \(\mathcal{G}=\{\mathbf{A},\mathbf{X}\}\), an arbitrary graph-level transformation \(g\colon\mathbb{G}\to\mathbb{G}\) can be decoupled to a series of following transformations:_

* _Feature transformations._ _Modifying the node features and generating the new feature matrix_ \(\mathbf{X}^{\prime}=g_{ft}(\mathbf{X})\)_._
* _Link transformations._ _Adding or removing edges and generating the new adjacency matrix_ \(\mathbf{A}^{\prime}=g_{tt}(\mathbf{A})\)_._
* _Isolated component transformations._ _Adding or removing isolated components (sub-graphs) and generating the new adjacency matrix and feature matrix_ \(\mathbf{A}^{\prime},\mathbf{X}^{\prime}=g_{ict}(\mathbf{A},\mathbf{X})\)_._

Here, the word "isolated" refers to a component (sub-graph) that does not link with the rest of the graph. Proposition 2 suggests that an arbitrary graph-level transformation is a combination of the three transformations mentioned above. For instance, the deletion of a node in the initial graph can be decomposed into two steps: removing its connected edges and then removing the isolated node.

**Proposition 3**.: _Given a pre-trained GNN model \(f\), an input graph \(\mathcal{G}\colon(\mathbf{A},\mathbf{X})\), for any feature transformation \(g_{ft}\colon g_{ft}(\mathbf{X})=\mathbf{X}^{\prime}\), there exists a GPF extra feature vector \(\hat{p}\) that satisfies:_

\[f(\mathbf{A},\mathbf{X}+\hat{p})=f(\mathbf{A},\mathbf{X}^{\prime})\] (18)

Proof.: We set \(\Delta\mathbf{X}=\mathbf{X}^{\prime}-\mathbf{X}\). Then, we have:

\[\mathbf{H}^{\prime} =(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot\mathbf{X}^{\prime }\cdot\mathbf{W}\] (19) \[=(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot(\mathbf{X}+\Delta \mathbf{X})\cdot\mathbf{W}\] (20) \[=(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot\mathbf{X}\cdot \mathbf{W}+(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot\Delta\mathbf{X}\cdot \mathbf{W}\] (21) \[=\mathbf{H}+(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot\Delta \mathbf{X}\cdot\mathbf{W}\] (22)

For GPF \(p=[\alpha_{1},\cdots,\alpha_{F}]\in\mathbb{R}^{1\times F}\), we can perform a similar split:

\[\mathbf{H}_{p} =(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot(\mathbf{X}+[1]^{N }\cdot p)\cdot\mathbf{W}\] (23) \[=(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot\mathbf{X}\cdot \mathbf{W}+(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot[1]^{N}\cdot p\cdot \mathbf{W}\] (24) \[=\mathbf{H}+(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot[1]^{N }\cdot p\cdot\mathbf{W}\] (25) \[=\mathbf{H}+[d_{i}+1+\epsilon]^{N}\cdot p\cdot\mathbf{W}\] (26)

where \([1]^{N}\in\mathbb{R}^{N\times 1}\) denotes a column vector with \(N\)\(1\)s, \([d_{i}+1+\epsilon]^{N}\in\mathbb{R}^{N\times 1}\) denotes a column vector with the value of \(i\)-th row is \(d_{i}+1+\epsilon\) and \(d_{i}\) represents the degree number of \(v_{i}\). To obtain the same graph representation \(h_{\mathcal{G}}\), we have:

\[h_{\mathcal{G},ft}=h_{\mathcal{G},p}\rightarrow\text{Sum}(\mathbf{H}^{\prime} )=\text{Sum}(\mathbf{H}_{p})\] (27)

where \(\text{Sum}(\mathbf{M})=\sum_{i}m_{i}\) denotes the operation that calculates the sum vector for each row in the matrix. We can further simplify the above equation as:

\[h_{\mathcal{G},ft}=h_{\mathcal{G},p}\] (28) \[\rightarrow\text{Sum}(\mathbf{H}^{\prime})=\text{Sum}(\mathbf{H} _{p})\] (29) \[\rightarrow\text{Sum}(\mathbf{H}+(\mathbf{A}+(1+\epsilon)\cdot \mathbf{I})\cdot\Delta\mathbf{X}\cdot\mathbf{W})=\text{Sum}(\mathbf{H}+[d_{i} +1+\epsilon]^{N}\cdot p\cdot\mathbf{W})\] (30) \[\rightarrow\text{Sum}((\mathbf{A}+(1+\epsilon)\cdot\mathbf{I}) \cdot\Delta\mathbf{X}\cdot\mathbf{W})=\text{Sum}([d_{i}+1+\epsilon]^{N} \cdot p\cdot\mathbf{W})\] (31)

where the results of \(((\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot\Delta\mathbf{X})\in\mathbb{R}^{ N\times F}\),and the frozen linear transformation \(\mathbf{W}\in\mathbb{R}^{F\times F^{\prime}}\). We first calculate \(\Delta h_{\mathcal{G},p}=\text{Sum}([d_{i}+1+\epsilon]^{N}\cdot p\cdot \mathbf{W})\in\mathbb{R}^{F^{\prime}}\). We can obtain that:

\[\Delta h_{\mathcal{G},p}^{i} =\sum_{j=1}^{F}\sum_{k=1}^{N}(d_{k}+1+\epsilon)\cdot\alpha_{j} \cdot\mathbf{W}_{j,i}\] (32) \[=\sum_{j=1}^{F}(D+N+N\cdot\epsilon)\cdot\alpha_{j}\cdot\mathbf{W}_ {j,i}\] (33)where \(h^{i}_{\mathcal{G},p}\) denotes the value of the \(i\)-th dimension in \(h_{\mathcal{G},p}\), \(D=\sum_{k=1}^{N}d_{k}\) denotes the total degree of all nodes in the whole graph, \(\alpha_{j},j\in[1,F]\) denotes the \(j\)-th learnable parameter in GPF \(p\), and \(\mathbf{W}_{j,i},j\in[1,F],i\in[1,F^{\prime}]\) denotes the frozen parameter in \(\mathbf{W}\). As for \(\Delta h_{\mathcal{G},ft}=\text{Sum}((\mathbf{A}+(1+\epsilon)\cdot\mathbf{I} )\cdot\Delta\mathbf{X}\cdot\mathbf{W})\), we assume \((\mathbf{A}+(1+\epsilon)\cdot\mathbf{I})\cdot\Delta\mathbf{X}=\mathbf{B}\in \mathbb{R}^{N\times F}\). Then we have:

\[\Delta h^{i}_{\mathcal{G},ft}=\sum_{j=1}^{F}(\sum_{k=1}^{N}\beta_{k,j})\cdot \mathbf{W}_{j,i}\] (34)

where \(\beta_{k,j},k\in[1,N],j\in[1,F]\) denotes the learnable parameter in \(\mathbf{B}\). According to above analysis, to obtain a same graph representation \(h_{\mathcal{G},\hat{p}}\) with a certain \(h_{\mathcal{G},ft}\), we have:

\[h^{i}_{\mathcal{G},\hat{p}}=h^{i}_{\mathcal{G},ft}\;,\;\text{for every}\;i\in[1,F^{ \prime}]\] (35) \[\rightarrow\Delta h^{i}_{\mathcal{G},\hat{p}}=\Delta h^{i}_{ \mathcal{G},ft}\] (36) \[\rightarrow\alpha_{j}=\frac{\sum_{k=1}^{N}\beta_{k,j}}{D+N+N\cdot \epsilon}\;,\;j\in[1,F]\] (37)

Therefore, for an arbitrary feature transformation \(g_{ft}\), there exists a GPF \(\hat{p}\) that satisfies the above conditions and can obtain the exact graph representation for the pre-trained GNN model \(f\). 

Proposition 3 demonstrates the comprehensive coverage of our proposed GPF for all graph-level feature transformations. GPF introduces a uniform feature modification \(p\in\mathbb{R}^{F}\) to each node in the graph. However, it can achieve an equivalent effect to adding independent feature modifications to each node individually under the pre-trained GNN model described above.

**Proposition 4**.: _Given a pre-trained GNN model \(f\), an input graph \(\mathcal{G}\colon(\mathbf{A},\mathbf{X})\), for any link transformation \(g_{tt}\colon g_{tt}(\mathbf{A})=\mathbf{A}^{\prime}\), there exists a GPF extra feature vector \(\hat{p}\) that satisfies:_

\[f(\mathbf{A},\mathbf{X}+\hat{p})=f(\mathbf{A}^{\prime},\mathbf{X})\] (38)

Proof.: The proof of Proposition 4 is similar to that of Proposition 3. We set \(\Delta\mathbf{A}=\mathbf{A}^{\prime}-\mathbf{A}\). It is worth mentioning that \(\mathbf{A}\), \(\mathbf{A}^{\prime}\in\{0,1\}^{N\times N}\) and \(\Delta\mathbf{A}\in\{-1,0,1\}^{N\times N}\), which means they are of the same size, the values of \(\mathbf{A}\), \(\mathbf{A}^{\prime}\) can only be \(0\) or \(1\), and the values of \(\Delta\mathbf{A}\) can be \(-1\), \(0\) or \(1\). We have:

\[\mathbf{H}^{\prime} =(\mathbf{A}^{\prime}+(1+\epsilon)\cdot\mathbf{I})\cdot\mathbf{X }\cdot\mathbf{W}\] (39) \[=((\mathbf{A}+\Delta\mathbf{A})+(1+\epsilon)\cdot\mathbf{I})\cdot \mathbf{X}\cdot\mathbf{W}\] (40) \[=\mathbf{H}+\Delta\mathbf{A}\cdot\mathbf{X}\cdot\mathbf{W}\] (41)

From the proof of Proposition 3, we obtain:

\[\mathbf{H}_{p}=\mathbf{H}+\left[d_{i}+1+\epsilon\right]^{N}\cdot p\cdot\mathbf{W}\] (42)

where \(p=[\alpha_{1},\cdots,\alpha_{F}]\in\mathbb{R}^{1\times F}\) denotes our learnable GPF, \([d_{i}+1+\epsilon]^{N}\in\mathbb{R}^{N\times 1}\) denotes a column vector with the value of \(i\)-th line is \(d_{i}+1+\epsilon\) and \(d_{i}\) represents the degree number of \(v_{i}\). With \(\Delta h_{\mathcal{G},p}=\text{Sum}([d_{i}+1+\epsilon]^{N}\cdot p\cdot\mathbf{ W})\in\mathbb{R}^{F^{\prime}}\), we can obtain:

\[\Delta h^{i}_{\mathcal{G},p} =\sum_{j=1}^{F}\sum_{k=1}^{N}(d_{k}+1+\epsilon)\cdot\alpha_{j} \cdot\mathbf{W}_{j,i}\] (43) \[=\sum_{j=1}^{F}(D+N+N\cdot\epsilon)\cdot\alpha_{j}\cdot\mathbf{W} _{j,i}\] (44)

where \(h^{i}_{\mathcal{G},p}\) denotes the value of the \(i\)-th dimension in \(h_{\mathcal{G},p}\), \(D=\sum_{k=1}^{N}d_{k}\) denotes the total degree of all nodes in the whole graph, \(\alpha_{j},j\in[1,F]\) denotes the \(j\)-th learnable parameter in GPF \(p\), and \(\mathbf{W}_{j,i},j\in[1,F],i\in[1,F^{\prime}]\) denotes the frozen parameter in \(\mathbf{W}\). As for \(\Delta h_{\mathcal{G},lt}=\text{Sum}(\Delta\mathbf{A}\cdot\mathbf{X}\cdot \mathbf{W})\), we have:

\[\Delta h^{i}_{\mathcal{G},lt}=\sum_{j=1}^{F}\sum_{(k,l)\in N\times N}(\Delta a _{k,l}\cdot x_{l,j})\cdot\mathbf{W}_{j,i}\] (45)where \(\Delta a_{k,l},k\in[1,N],l\in[1,N]\) denotes the element of \(\Delta\mathbf{A}\), and \(x_{l,j},l\in[1,N],j\in[1,F]\) denotes the element of \(\mathbf{X}\). To obtain a same graph representation \(h_{\mathcal{G},\hat{p}}\) with a certain \(h_{\mathcal{G},lt}\), we have:

\[h^{i}_{\mathcal{G},\hat{p}}=h^{i}_{\mathcal{G},lt}\,\text{ for every }i \in[1,F^{\prime}]\] (46) \[\rightarrow\Delta h^{i}_{\mathcal{G},\hat{p}}=\Delta h^{i}_{ \mathcal{G},lt}\] (47) \[\rightarrow\alpha_{j}=\frac{\sum_{(k,l)\in N\times N}\Delta a_{k, l}\cdot x_{l,j}}{D+N+N\cdot\epsilon}\,\ j\in[1,F]\] (48)

Therefore, for an arbitrary link transformation \(g_{lt}\), there exists a GPF \(\hat{p}\) that satisfies above conditions and can obtain the same graph representation for pre-trained GNN model \(f\). 

Proposition 4 demonstrates that GPF can also encompass all link transformations. Intuitively, link transformations are closely associated with changes in the adjacency matrix, which are independent of node features. However, our findings reveal that, within the architecture of most existing GNN models, modifications in the feature space and modifications in the structural space can produce equivalent effects.

**Proposition 5**.: _Given a pre-trained GNN model \(f\), an input graph \(\mathcal{G}\colon(\mathbf{A},\mathbf{X})\), for any isolated component transformation \(g_{ict}\colon g_{ict}(\mathbf{A},\mathbf{X})=\mathbf{A}^{\prime},\mathbf{X}^{\prime}\), there exists a GPF extra feature vector \(\hat{p}\) that satisfies:_

\[f(\mathbf{A},\mathbf{X}+\hat{p})=f(\mathbf{A}^{\prime},\mathbf{X}^{\prime})\] (49)

Proof.: Unlike feature transformations and linear transformations, isolated component transformations will change the number of nodes in the graph, which means the scale of modified \(\mathbf{A}^{\prime}\) and \(\mathbf{X}^{\prime}\) is uncertain. We first express the isolated component transformation in more details. The adjacency matrix \(\mathbf{A}\) and feature matrix \(\mathbf{X}\) can be divided into several isolated components, which can be expressed as:

\[\mathbf{A}=\begin{pmatrix}\mathbf{A}_{1}&0&\cdots&0\\ 0&\mathbf{A}_{2}&\cdots&0\\ \vdots&\vdots&&\vdots\\ 0&0&\cdots&\mathbf{A}_{m}\end{pmatrix}\qquad\mathbf{X}=\begin{pmatrix} \mathbf{X}_{1}\\ \mathbf{X}_{2}\\ \vdots\\ \mathbf{X}_{m}\end{pmatrix}\] (50)

Removing an isolated component \(\mathcal{C}_{k}=\{\mathbf{A}_{k},\mathbf{X}_{k}\},k\in[1,m]\) means removing both \(\mathbf{A}_{k}\) in the adjacency matrix and corresponding \(\mathbf{X}_{k}\) in the feature matrix. Adding a new isolated component \(\mathcal{C}_{m+l}=\{\mathbf{A}_{m+l},\mathbf{X}_{m+l}\},l\geq 1\) means adding \(\mathbf{A}_{m+l}\) to the adjacency matrix \(\mathbf{A}\), and adding \(\mathbf{X}_{m+l}\) to the corresponding position of \(\mathbf{X}\). Then we have:

\[h_{\mathcal{G},ist}=\sum_{k}\text{Sum}((\mathbf{A}_{k}+(1+\epsilon)\cdot \mathbf{I})\cdot\mathbf{X}_{k}\cdot\mathbf{W})\] (51)

To align with the proofs of Proposition 3 and Proposition 4, we set \(\Delta h_{\mathcal{G},ist}=h_{\mathcal{G},ist}-\text{Sum}((\mathbf{A}+(1+ \epsilon)\cdot\mathbf{I})\cdot\mathbf{X}\cdot\mathbf{W})\), and it can be expressed as:

\[\Delta h_{\mathcal{G},ist}=\sum_{k}I_{k}\cdot\text{Sum}((\mathbf{A}_{k}+(1+ \epsilon)\cdot\mathbf{I})\cdot\mathbf{X}_{k}\cdot\mathbf{W})\] (52)

where \(I_{k}\) is an indicator that satisfies:

\[I_{k}=\begin{cases}\begin{array}{rl}0&\text{if }\mathcal{C}_{k}\text{ has no change}\\ 1&\text{if }\mathcal{C}_{k}\text{ is an additional component}\\ -1&\text{if }\mathcal{C}_{k}\text{ is a removed component}\end{array}\end{cases}\] (53)

From the proof of Proposition 3, we have following conclusions:

\[\mathbf{H}_{p}=\mathbf{H}+[d_{i}+1+\epsilon]^{N}\cdot p\cdot\mathbf{W}\] (54)

where \(p=[\alpha_{1},\cdots,\alpha_{F}]\in\mathbb{R}^{1\times F}\) denotes our learnable GPF, \([d_{i}+1+\epsilon]^{N}\in\mathbb{R}^{N\times 1}\) denotes a column vector with the value of \(i\)-th line is \(d_{i}+1+\epsilon\) and \(d_{i}\) represents the degree number of \(v_{i}\)With \(\Delta h_{\mathcal{G},p}=\text{Sum}([d_{i}+1+\epsilon]^{N}\cdot p\cdot\mathbf{W}) \in\mathbb{R}^{F^{\prime}}\), we can obtain:

\[\Delta h^{i}_{\mathcal{G},p} =\sum_{j=1}^{F}\sum_{k=1}^{N}(d_{k}+1+\epsilon)\cdot\alpha_{j} \cdot\mathbf{W}_{j,i}\] (55) \[=\sum_{j=1}^{F}(D+N+N\cdot\epsilon)\cdot\alpha_{j}\cdot\mathbf{W }_{j,i}\] (56)

where \(h^{i}_{\mathcal{G},p}\) denotes the value of the \(i\)-th dimension in \(h_{\mathcal{G},p}\), \(D=\sum_{k=1}^{N}d_{k}\) denotes the total degree of all nodes in the whole graph, \(\alpha_{j},j\in[1,F]\) denotes the \(j\)-th learnable parameter in GPF \(p\), and \(\mathbf{W}_{j,i},j\in[1,F],i\in[1,F^{\prime}]\) denotes the frozen parameter in \(\mathbf{W}\). To obtain a same graph representation \(h_{\mathcal{G},\hat{p}}\) with a certain \(h_{\mathcal{G},ist}\), we have:

\[h^{i}_{\mathcal{G},\hat{p}}=h^{i}_{\mathcal{G},ist}\;,\;\text{ for every }i\in[1,F^{\prime}]\] (57) \[\rightarrow\Delta h^{i}_{\mathcal{G},\hat{p}}=\Delta h^{i}_{ \mathcal{G},ist}\] (58) \[\rightarrow\alpha_{j}=\frac{\sum_{k}I_{k}\cdot\text{Sum}(( \mathbf{A}_{k}+(1+\epsilon)\cdot\mathbf{I})\cdot\mathbf{X}^{j}_{k})}{D+N+N \cdot\epsilon}\;,\;j\in[1,F]\] (59)

where \(\mathbf{X}^{j}\) denotes the \(j\)-th column of the matrix \(\mathbf{X}\). Therefore, for an arbitrary isolated component transformation \(g_{ict}\), there exists a GPF \(\hat{p}\) that satisfies above conditions and can obtain the same graph representation for pre-trained GNN model \(f\). 

The isolated component transformation possesses the capability to alter the scale of a graph, and previous studies have paid limited attention to this type of graph-level transformation.

**Proposition 6**.: _Given a pre-trained GNN model \(f\), an input graph \(\mathcal{G}\colon(\mathbf{A},\mathbf{X})\), for a series of transformations \(\mathbf{g}=\{g_{1},g_{2},\cdots,g_{k}\}\) composed of \(g_{ft}\), \(g_{lt}\) and \(g_{ist}\), there exists a GPF extra feature vector \(\hat{p}\) that satisfies:_

\[f(\mathbf{A},\mathbf{X}+\hat{p})=f(\mathbf{g}(\mathbf{A},\mathbf{X}))\] (60)

Proof.: Without loss of generality, we consider \(\mathbf{g}=\{g_{1},g_{2}\}\) with two transformations described in Proposition 2. Now we prove there exists a GPF \(\hat{p}\) that satisfies:

\[f(\mathbf{A},\mathbf{X}+p)=f(g_{2}(g_{1}(\mathbf{A},\mathbf{X})))\] (61)

We assume \(g_{1}(\mathbf{A},\mathbf{X})=\mathbf{A}^{\prime},\mathbf{X}^{\prime}\). According to Proposition 3, 4, 5, there exists a GPF \(\hat{p}_{1}\) that satisfies:

\[f(\mathbf{A},\mathbf{X}+\hat{p}_{1})=f(g_{1}(\mathbf{A},\mathbf{X}))\] (62)

and a \(\hat{p}_{2}\) that satisfies:

\[f(\mathbf{A},\mathbf{X}+\hat{p}_{2})=f(g_{2}(\mathbf{A}^{\prime},\mathbf{X}^{ \prime}))\] (63)

Therefore, there is a \(\hat{p}=\hat{p}_{1}+\hat{p}_{2}\) that satisfies:

\[f(\mathbf{A},\mathbf{X}+\hat{p})=f(g_{2}(g_{1}(\mathbf{A},\mathbf{X})))\] (64)

Based on the preceding analysis, we have established that the GPF can replicate the effects of any graph-level transformation on the pre-trained GNN model defined by Formula 15 and 16. Hence, we have successfully demonstrated Proposition 1 and Theorem 1 within the framework of the simple model architecture. Next, we aim to generalize our findings to more intricate scenarios.

**Extension to other GNN backbones.** We use GIN Xu et al. (2019) as the default backbone model in our previous derivation. When replacing GIN with another GNN model, only slight modifications are required to ensure that all propositions remain valid. As is described in Klicpera et al. (2019), various GNN models can be represented as:

\[\mathbf{H}=\mathbf{S}\cdot\mathbf{X}\cdot\mathbf{W}\] (65)where \(\mathbf{S}\in\mathbb{R}^{N\times N}\) denotes the diffusion matrix (_e.g._, \(\mathbf{A}+(1+\epsilon)\cdot\mathbf{I}\) is the diffusion matrix for GIN), and \(\mathbf{W}\) denotes the linear projection. In this case, we modify the Formula 26, 42 and 54 as follows:

\[\mathbf{H}_{p}=\mathbf{H}+\left[d_{i}+1+\epsilon\right]^{N}\cdot p \cdot\mathbf{W}\] (66) \[\rightarrow\mathbf{H}_{p}=\mathbf{H}+[\sum_{j}s_{i,j}]^{N}\cdot p \cdot\mathbf{W}\] (67)

where \(s_{i,j}\) is the element of the diffusion matrix \(\mathbf{S}\). With these modifications, Propositions 3, 4, and 5 remain valid.

**Extension to multi-layer models.** For analytical simplification, similar to many previous works, we consider multi-layer GNN models without non-linear activation functions between layers, which can be expressed as:

\[\mathbf{H}_{(0)}=\mathbf{X}\] (68) \[\mathbf{H}_{(k)}=\mathbf{S}_{(k)}\cdot\mathbf{H}_{(k-1)}\cdot \mathbf{W}_{(k)}\] (69)

where \(\mathbf{S}_{(k)}\) is the diffusion matrix described as Formula 65 of the \(k\)-th layer, and \(\mathbf{W}_{(k)}\) is the linear projection of the \(k\)-th layer. With such architecture, the node representations of the \(k\)-th layer \(\mathbf{H}_{(k)}\) can also be obtained as:

\[\mathbf{H}_{(k)}=(\prod_{i=1}^{k}\mathbf{S}_{(i)})\cdot\mathbf{X}\cdot(\prod_ {i=1}^{k}\mathbf{W}_{(i)})=\mathbf{S}^{\prime}\cdot\mathbf{X}\cdot\mathbf{W}^ {\prime}\] (70)

where \(\mathbf{S}^{\prime}=\prod_{i=1}^{k}\mathbf{S}_{(i)}\) and \(\mathbf{W}^{\prime}=\prod_{i=1}^{k}\mathbf{W}_{(i)}\). By substituting \(\mathbf{S}=\mathbf{S}^{\prime}\) and \(\mathbf{W}=\mathbf{W}^{\prime}\) in Formula 65, we can find that Proposition 3, 4 and 5 still hold true.

### Proof for Theorem 2

**Theorem 2**.: _(Effectiveness Guarantee of GPF) For a pre-trained GNN model \(f\), a series of graphs \(\mathcal{D}=\{(\mathcal{G}_{1}\colon(\mathbf{A}_{1},\mathbf{X}_{1}),y_{1}), \ldots,(\mathcal{G}_{m}\colon(\mathbf{A}_{m},\mathbf{X}_{m}),y_{m})\}\) under the non-degeneracy condition, and a linear projection head \(\theta\), there exists \(\mathcal{Y}^{\prime}=\{y^{\prime}_{1},\ldots,y^{\prime}_{m}\}\) for \(y_{1}=y^{\prime}_{1},\ldots,y_{m}=y^{\prime}_{m}\) that satisfies:_

\[l_{\mathrm{GPF}}=\min_{p,\theta}\sum_{i}^{m}(f(\mathbf{A}_{i},\mathbf{X}_{i}+p )\cdot\theta-y_{i})^{2}<l_{\mathrm{FT}}=\min_{f,\theta}\sum_{i}^{m}(f(\mathbf{ A}_{i},\mathbf{X}_{i})\cdot\theta-y_{i})^{2}\]

To demonstrate Theorem 2, we need to provide a more detailed description of the architecture of the GNN model \(f\). We assume that the graph representations \(h_{\mathcal{G}_{i}}\) for \(\mathcal{G}_{i}\) are obtained through the following process:

\[\mathbf{H}_{i}=\mathbf{S}_{i}\cdot\mathbf{X}_{i}\cdot\mathbf{W}\] (71) \[h_{\mathcal{G}_{i}}=\text{Sum}(\mathbf{H}_{i})\] (72)

where \(\mathbf{S}_{i}\in\mathbb{R}^{N_{i}\times N_{i}}\) denotes the diffusion matrix of \(\mathcal{G}_{i}\) as Formula 65, \(N_{i}\) denotes the node number of the graph \(\mathcal{G}_{i}\), \(\mathbf{W}\in\mathbb{R}^{F\times F^{\prime}}\) denotes the linear projection, \(F\) is the dimension of node features, and \(\text{Sum}(\mathbf{M})=\sum_{i}m_{i}\) denotes the operation that calculates the sum vector for each row in the matrix \(\mathbf{M}\). When we employ our proposed GPF in the above model \(f\), the graph representations \(h^{\text{GPF}}_{\mathcal{G}_{i}}\) are calculated as:

\[\mathbf{H}^{\text{GPF}}_{i}= \mathbf{S}_{i}\cdot(\mathbf{X}_{i}+[1]^{\mathsf{T}}\cdot p)\cdot \mathbf{W}\] (73) \[h^{\text{GPF}}_{\mathcal{G}_{i}}=\text{Sum}(\mathbf{H}^{\text{ GPF}}_{i})\] (74)

where \([1]^{\mathsf{T}}\) is a column vector with all 1's, and \(p\in\mathbb{R}^{1\times F}\) is the extra learnable vector of GPF. The squared regression loss \(l\) can be expressed as:

\[l=\sum_{i}^{m}(h_{\mathcal{G}_{i}}\cdot\theta-y_{i})^{2}\] (75)where \(\theta\in\mathbb{R}^{F^{\times 1}}\) is a linear projection head. In such case, the optimal tuning loss of fine-tuning \(l_{\text{FT}}\) and GPF \(l_{\text{GPF}}\) can be expressed as:

\[l_{\text{FT}}=\min_{\mathbf{W},\theta}\sum_{i}^{m}(\text{Sum}(\mathbf{S}_{i} \cdot\mathbf{X}_{i}\cdot\mathbf{W})\cdot\theta-y_{i})^{2}\] (76)

\[l_{\text{GPF}}=\min_{p,\theta}\sum_{i}^{m}(\text{Sum}(\mathbf{S}_{i}\cdot( \mathbf{X}_{i}+[1]^{\text{T}}\cdot p)\cdot\mathbf{W})\cdot\theta-y_{i})^{2}\] (77)

Before we prove Theorem 2, we first illustrate the following proposition.

**Proposition 7**.: _Given a series of graphs \(\mathcal{D}=\{\mathcal{G}_{1}\colon(\mathbf{A}_{1},\mathbf{X}_{1}),\ldots, \mathcal{G}_{m}\colon(\mathbf{A}_{m},\mathbf{X}_{m})\}\) and a linear projection \(\hat{\mathbf{W}}\), there exist \(\hat{p}\in\mathbb{R}^{1\times F}\), \(\hat{\theta}\in\mathbb{R}^{F^{\prime}\times 1}\), and \(\delta>0\) that satisfy:_

\[\sum_{i}^{m}\|\mathrm{Sum}(\mathbf{S}_{i}\cdot(\mathbf{X}_{i}+[1]^{\text{T}} \cdot\hat{p})\cdot\hat{\mathbf{W}})\cdot\hat{\theta}-\mathrm{Sum}(\mathbf{S} _{i}\cdot\mathbf{X}_{i}\cdot\mathbf{W}^{\prime})\cdot\theta^{\prime}\|_{2}>\delta\] (78)

_for any \(\mathbf{W}^{\prime}\in\mathbb{R}^{F\times F^{\prime}}\), \(\theta^{\prime}\in\mathbb{R}^{F^{\prime}\times 1}\)._

For analytical simplification, we gather all the unique node features in \(\mathcal{D}\) into a set \(\mathcal{S}_{\mathbf{X}}=\{x_{1},\cdots,x_{l}\}\). Consequently, the node feature matrix \(\mathbf{X}_{i}\) of any graph \(\mathcal{G}_{i}\) can be constructed from elements in the set \(\mathcal{S}\mathbf{X}\). Next, we proceed to expand the function \(\text{Sum}(\cdot)\) as:

\[h_{\mathcal{G}_{i}}= \text{Sum}(\mathbf{S}_{i}\cdot\mathbf{X}_{i}\cdot\mathbf{W})= \sum_{k}^{N_{i}}\sum_{j}^{N_{i}}\mathbf{S}_{i,(j,k)}\cdot\mathbf{X}_{i,(k,:)}\cdot \mathbf{W}\] (79) \[h_{\mathcal{G}_{i}}^{\text{GPF}}= \text{Sum}(\mathbf{S}_{i}\cdot(\mathbf{X}_{i}+[1]^{\text{T}} \cdot\hat{p})\cdot\mathbf{W})\] \[= \sum_{k}^{N_{i}}\sum_{j}^{N_{i}}\mathbf{S}_{i,(j,k)}\cdot\mathbf{ X}_{i,(k,:)}\cdot\mathbf{W}+\sum_{k}^{N_{i}}\sum_{j}^{N_{i}}\mathbf{S}_{i,(j,k)} \cdot p\cdot\mathbf{W}\] (80)

where \(\mathbf{S}_{i,(j,k)}\) denotes the element in the \(j\)-th row and \(k\)-th column of \(\mathbf{S}_{i}\), and \(\mathbf{X}_{i,(k,:)}\) denotes the \(k\)-th row vector of \(\mathbf{X}_{i}\). In order to express the representations of different graphs in a unified form, we rewrite the above formulas to calculate the graph representations \(h_{\mathcal{G}_{i}}\) and \(h_{\mathcal{G}_{i}}^{\text{GPF}}\) from the node representations in \(\mathcal{S}_{\mathbf{X}}\):

\[h_{\mathcal{G}_{i}}= \sum_{k}^{N_{i}}\sum_{j}^{N_{i}}\mathbf{S}_{i,(j,k)}\cdot\mathbf{ X}_{i,(k,:)}\cdot\mathbf{W}=\sum_{j}^{l}c_{i,j}\cdot x_{j}\cdot\mathbf{W}\] (81) \[h_{\mathcal{G}_{i}}^{\text{GPF}}= \sum_{k}^{N_{i}}\sum_{j}^{N_{i}}\mathbf{S}_{i,(j,k)}\cdot\mathbf{ X}_{i,(k,:)}\cdot\mathbf{W}+\sum_{k}^{N_{i}}\sum_{j}^{N_{i}}\mathbf{S}_{i,(j,k)} \cdot p\cdot\mathbf{W}\] \[= \sum_{j}^{l}c_{i,j}\cdot x_{j}\cdot\mathbf{W}+\sum_{j}^{l}c_{i,j} \cdot p\cdot\mathbf{W}\] (82)

where \(c_{i,j}\) is the coefficient of \(x_{j}\) for the graph \(\mathcal{G}_{i}\), which is calculated as \(c_{i,j}=\sum_{k}\sum_{j^{\prime}}^{N_{i}}\mathbf{S}_{i,(j^{\prime},k)}\) for all \(k\)'s that satisfy \(\mathbf{X}_{i,(k,:)}=x_{j}\). We can also get that \(\sum_{j}^{l}c_{i,j}=\sum_{k}^{N_{i}}\sum_{j^{\prime}}^{N_{i}}\mathbf{S}_{i,(j^{ \prime},k)}\). Then, we rewrite Formula 78 into a matrix form as follows:

\[\|(\mathbf{C}\cdot\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}+[\sum_{j}\mathbf{C}_{(i,j)}]^{\text{T}}\cdot\hat{p})\cdot\hat{\mathbf{W}}\cdot\hat{\theta}-\mathbf{C} \cdot\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}\cdot\mathbf{W}^{\prime}\cdot\theta^{ \prime}\|_{2}>\delta\] (83)

where \(\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}\in\mathbb{R}^{l\times F}\) denotes the feature matrix for \(\mathcal{S}_{\mathbf{X}}\) that satisfies \(\mathbf{X}_{\mathcal{S}_{\mathbf{X}},(j,:)}=x_{j}\), \(\mathbf{C}\in\mathbb{R}^{m\times l}\) denotes the coefficient matrix and the element in the \(i\)-th row and \(k\)-th column \(\mathbf{C}_{(i,j)}\) is equal to \(c_{i,j}\), \([\sum_{j}\mathbf{C}_{(i,j)}]^{\text{T}}\) denotes a column vector with the value of \(i\)-th row is \(\sum_{j}\mathbf{C}_{(i,j)}\). We represent \(\mathbf{W}^{\prime}\cdot\theta^{\prime}=\hat{\mathbf{W}}\cdot\hat{\theta}+ \Delta\mathbf{W}\cdot\theta\), then we have:

\[\|(\mathbf{C}\cdot\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}+[\sum_{j} \mathbf{C}_{(i,j)}]^{\text{T}}\cdot\hat{p})\cdot\hat{\mathbf{W}}\cdot\hat{ \theta}-\mathbf{C}\cdot\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}\cdot(\hat{\mathbf{W}} \cdot\hat{\theta}+\Delta\mathbf{W}\cdot\theta)\|_{2}\] (84) \[= \|[\sum_{j}\mathbf{C}_{(i,j)}]^{\text{T}}\cdot\hat{p}\cdot\hat{ \mathbf{W}}\cdot\hat{\theta}-\mathbf{C}\cdot\mathbf{X}_{\mathcal{S}_{\mathbf{X}}} \cdot\Delta\mathbf{W}\cdot\theta\|_{2}\] (85)As described by the condition of Proposition 7, \(\mathbf{W}^{\prime}\) and \(\theta^{\prime}\) can be chosen arbitrarily, which means \(\Delta\mathbf{W}\cdot\theta\) can be equal to any \(v\in\mathbb{R}^{F\times 1}\). Therefore, Proposition 7 can be reformed as below.

_Given a series of graphs \(\mathcal{D}=\{\mathcal{G}_{1}\colon(\mathbf{A}_{1},\mathbf{X}_{1}),\ldots, \mathcal{G}_{m}\colon(\mathbf{A}_{m},\mathbf{X}_{m})\}\) and a linear projection \(\hat{\mathbf{W}}\), there exist \(\hat{p}\in\mathbb{R}^{1\times F}\), \(\hat{\theta}\in\mathbb{R}^{F^{\prime}\times 1}\), and \(\delta>0\) that satisfy:_

\[\|[\sum_{j}\mathbf{C}_{(i,j)}]^{\mathsf{T}}\cdot\hat{p}\cdot\hat{\mathbf{W}} \cdot\hat{\theta}-\mathbf{C}\cdot\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}\cdot v \|_{2}>\delta\] (86)

_for any \(v\in\mathbb{R}^{F\times 1}\)._

We make the assumption that \(\mathbf{C}\) is a column full-rank matrix, which implies that there is no uniform feature distribution shared among different graphs, aligning with real-world scenarios. It is important to note that \(\mathbf{W}\) is pre-trained beforehand, and for any \(\hat{p}\) and \(\hat{\theta}\) satisfying \(\hat{p}\cdot\hat{\mathbf{W}}\cdot\hat{\theta}\neq 0\), the non-degeneracy condition for Formula 86 is as follows:

\[\|[\sum_{j}\mathbf{C}_{(i,j)}]^{\mathsf{T}}\cdot\hat{p}\cdot\hat{\mathbf{W}} \cdot\hat{\theta}-\mathbf{C}\cdot\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}\cdot v \|_{2}\neq 0\] (87)

\[\rightarrow\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}\cdot v^{\prime}=[1]^{ \mathsf{T}}\text{ has no solution }v^{\prime}\] (88)

where \([1]^{\mathsf{T}}\in\mathbb{R}^{m\times 1}\) is a column vector with all 1's. Therefore, Formula 86 and Proposition 7 hold for all \(\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}\) for which \(\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}\cdot v^{\prime}=[1]^{\mathsf{T}}\) has no solution \(v^{\prime}\).

Finally, we revisit Theorem 2. Given a series of graphs under the non-degeneracy condition, a pre-trained linear projection \(\hat{\mathbf{W}}\), \(\hat{p}\) and \(\hat{\theta}\) that satisfy \(\hat{p}\cdot\hat{\mathbf{W}}\cdot\hat{\theta}\neq 0\), we can construct \(\mathcal{Y}^{\prime}=\{y^{\prime}_{1},\ldots,y^{\prime}_{m}\}\) as:

\[y^{\prime}_{i}=\mathbf{C}_{(i,:)}\cdot\mathbf{X}_{\mathcal{S}_{\mathbf{X}}} \cdot\hat{\mathbf{W}}\cdot\hat{\theta}+\sum_{j}\mathbf{C}_{(i,j)}\cdot\hat{p }\cdot\hat{\mathbf{W}}\cdot\hat{\theta}\] (89)

Under these conditions, the optimal theoretical tuning results for GPF and fine-tuning can be expressed as::

\[l_{\text{GPF}}=\min_{p,\theta}\|(\mathbf{C}\cdot\mathbf{X}_{\mathcal{S}_{ \mathbf{X}}}+[\sum_{j}\mathbf{C}_{(i,j)}]^{\mathsf{T}}\cdot p)\cdot\hat{ \mathbf{W}}\cdot\theta-(\mathbf{C}\cdot\mathbf{X}_{\mathcal{S}_{\mathbf{X}}}+[ \sum_{j}\mathbf{C}_{(i,j)}]^{\mathsf{T}}\cdot\hat{p})\cdot\hat{\mathbf{W}} \cdot\hat{\theta}\|_{2}=0\] (90)

\[l_{\text{FT}}=\min_{\hat{\mathbf{W}},\theta}\|\mathbf{C}\cdot\mathbf{X}_{ \mathcal{S}_{\mathbf{X}}}\cdot\mathbf{W}\cdot\theta-(\mathbf{C}\cdot\mathbf{X }_{\mathcal{S}_{\mathbf{X}}}+[\sum_{j}\mathbf{C}_{(i,j)}]^{\mathsf{T}}\cdot \hat{p})\cdot\hat{\mathbf{W}}\cdot\hat{\theta}\|_{2}>0\] (91)

Here, \(l_{\text{GPF}}\) can be achieved with \(p=\hat{p}\) and \(\theta=\hat{\theta}\), while \(l_{\text{FT}}\) consistently remains greater than 0, as stated in Proposition 7. Therefore, we can conclude that \(l_{\text{GPF}}<l_{\text{FT}}\), thus establishing the proof of Theorem 2.

More Information on Experiments

### Details of the datasets

**Dataset overview** We utilize the datasets provided by Hu et al. (2020) for our pre-training datasets. These datasets consist of two domains: chemistry and biology. The chemistry domain dataset comprises 2 million unlabeled molecules sampled from the ZINC15 database (Sterling and Irwin, 2015), along with 256K labeled molecules obtained from the preprocessed ChEMBL dataset (Mayr et al., 2018; Gaulton et al., 2012). On the other hand, the biology domain dataset includes 395K unlabeled protein ego-networks and 88K labeled protein ego-networks extracted from PPI networks. For the models pre-trained on the chemistry dataset, we employ eight binary graph classification datasets available in MoleculeNet (Wu et al., 2017) as downstream tasks. As for the models pre-trained on the biology dataset, we apply the pre-trained models to 40 binary classification tasks, with each task involving the prediction of a specific fine-grained biological function.

**Pre-training datasets.** The datasets provided by Hu et al. (2020) consist of two distinct datasets: Biology and Chemistry, corresponding to the biology domain and chemistry domain, respectively. The Biology dataset contains 395K unlabeled protein ego-networks obtained from PPI networks of 50 species. These networks are used for node-level self-supervised pre-training. Additionally, 88K labeled protein ego networks serve as the training data for predicting 5000 coarse-grained biological functions. This graph-level multi-task supervised pre-training aims to predict these functions jointly. Regarding the Chemistry dataset, it comprises 2 million unlabeled molecules sampled from the ZINC15 database (Sterling and Irwin, 2015). These molecules are utilized for node-level self-supervised pre-training. For graph-level multi-task supervised pre-training, a preprocessed ChEMBL dataset (Mayr et al., 2018; Gaulton et al., 2012) is employed. This dataset contains 456K molecules and covers 1310 different biochemical assays.

**Downstream datasets.** The statistics of the downstream datasets utilized for the models pre-trained on Biology and Chemistry are presented in Table 3.

### Details of pre-training strategies

We adopt five widely used strategies (tasks) to pre-train the GNN models, which are listed as below:

* _Deep Graph Infomax_ (denoted by Infomax). It is first proposed by Velickovic et al. (2019). Deep graph infomax obtains expressive representations for graphs or nodes via maximizing the mutual information between graph-level representations and substructure-level representations of different granularity.
* _Edge Prediction_ (denoted by EdgePred). It is a regular graph reconstruction task used by many models, such as GAE (Kipf and Welling, 2016). The prediction target is the existence of edge between a pair of nodes.
* _Attribute Masking_ (denoted by AttrMasking). It is proposed by Hu et al. (2020). It masks node/edge attributes and then let GNNs predict those attributes based on neighboring structure.
* _Context Prediction_ (denoted by ContextPred). It is also proposed by Hu et al. (2020). Context prediction uses subgraphs to predict their surrounding graph structures, and aims to mapping nodes appearing in similar structural contexts to nearby embeddings.
* _Graph Contrastive Learning_ (denoted by GCL). It embeds augmented versions of the anchor close to each other (positive samples) and pushes the embeddings of other samples (negatives) apart. We use the augmentation strategies proposed in You et al. (2020) for generating the positive and negative samples.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline
**Dataset** & **BBBP** & **Tox21** & **ToxCast** & **SIDER** & **ClinTox** & **MUV** & **HIV** & **BACE** & **PPI** \\ \hline \(\#\) Proteins / Molecules & 2039 & 7831 & 8575 & 1427 & 1478 & 93087 & 41127 & 1513 & 88K \\ \(\#\) Binary prediction tasks & 1 & 12 & 617 & 27 & 2 & 17 & 1 & 1 & 40 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Statistics of datasets for downstream tasks.

To pre-train our models, we follow the training steps outlined in Hu et al. (2020) for Infomax, EdgePred, AttrMasking, and ContextPred tasks. We then perform supervised graph-level property prediction to enhance the performance of the pre-trained models further. For models pre-trained using GCL, we follow the training steps detailed in You et al. (2020).

### Results of few-shot graph classification

The results for 50-shot scenarios.Table 4 summarizes the results for 50-shot graph classification.

The results for 100-shot scenarios.We also conducted experiments where the number of training samples in the downstream task was limited to 100 for both the chemistry and biology datasets. The summary of the overall results can be found in Table 5. The experimental findings align with those observed in the 50-shot scenarios. Our proposed graph prompt tuning method achieves the best results in 42 out of 45 cases (14 out of 45 for GPF and 28 out of 45 for GPF-plus). Additionally, the average results of both GPF and GPF-plus surpass the average results of fine-tuning across all pre-training strategies, showcasing the superiority of our proposed graph prompt tuning approach.

### Parameter efficiency analysis

We have computed the number of tunable parameters for full fine-tuning, GPF, and GPF-plus (excluding the task-specific projection head \(\theta\)) on the chemistry and biology datasets. The statistics are presented in Table 6. The results indicate that the number of tunable parameters in GPF and GPF-plus is several orders of magnitude smaller than that of fine-tuning. Specifically, GPF utilizes no more than \(0.02\%\) of the tunable parameters used in fine-tuning, while GPF-plus utilizes no more than \(0.7\%\) of the tunable parameters used in fine-tuning. Our proposed graph prompt tuning methods

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline Pre-training & Tuning & \multirow{2}{*}{BBBP} & \multirow{2}{*}{Tox21} & \multirow{2}{*}{ToxCast} & SIDER & ClinTox & MUV & HIV & BACE & PPI & **Avg.** \\ Strategy & & & & & & & & & & & \\ \hline \multirow{4}{*}{Infomax} & FT & 53.81 & 61.42 & 53.93 & 50.77 & 58.6 & 66.12 & 65.09 & 52.64 & 48.79 & \multirow{4}{*}{56.79} \\  & & \(\pm\)3.35 & \(\pm\)1.19 & \(\pm\)0.59 & \(\pm\)2.27 & \(\pm\)3.48 & \(\pm\)0.63 & \(\pm\)1.17 & \(\pm\)2.64 & \(\pm\)1.32 & \\  & & 55.52 & 65.56 & 56.76 & 50.29 & 62.44 & **68.00** & **67.68** & 54.49 & **54.03** & \\  & & \(\pm\)1.84 & \(\pm\)0.64 & \(\pm\)0.54 & \(\pm\)1.61 & \(\pm\)4.11 & \(\pm\)0.61 & \(\pm\)1.09 & \(\pm\)2.54 & \(\pm\)0.34 & \\  & & GPF-plus & **58.09** & **65.71** & **57.13** & **51.33** & **62.96** & **67.88** & **66.00** & **56.56** & **53.78** & \\ \hline \multirow{4}{*}{EdgePred} & FT & 48.88 & 60.95 & 55.73 & 51.30 & 57.78 & 66.88 & 64.22 & 61.27 & 47.62 & \multirow{4}{*}{57.18} \\  & & \(\pm\)0.68 & \(\pm\)1.46 & \(\pm\)0.43 & \(\pm\)2.21 & \(\pm\)4.03 & \(\pm\)0.53 & \(\pm\)1.57 & \(\pm\)6.10 & \(\pm\)1.50 & \\  & & GPF & 50.53 & **64.46** & **57.33** & **51.35** & **68.74** & **68.08** & **66.22** & **68.25** & **52.81** & \\  & & \(\pm\)1.35 & \(\pm\)0.93 & \(\pm\)0.65 & \(\pm\)0.76 & \(\pm\)0.63 & \(\pm\)0.39 & \(\pm\)1.90 & \(\pm\)5.91 & \(\pm\)0.38 & **60.26** \\  & & GPF-plus & **54.49** & **64.99** & **57.69** & 51.30 & 66.64 & **68.16** & 62.05 & 62.60 & **53.30** & \\  & & \(\pm\)4.60 & \(\pm\)0.53 & \(\pm\)0.61 & \(\pm\)1.18 & \(\pm\)2.40 & \(\pm\)0.48 & \(\pm\)3.39 & \(\pm\)2.48 & \(\pm\)0.34 & \\ \hline \multirow{4}{*}{AttrMasking} & FT & 51.26 & 60.28 & 53.47 & 50.11 & 61.51 & 59.35 & 67.18 & 55.62 & 48.17 & \multirow{4}{*}{56.32} \\  & & \(\pm\)2.33 & \(\pm\)1.73 & \(\pm\)0.46 & \(\pm\)1.63 & \(\pm\)1.45 & \(\pm\)1.31 & \(\pm\)1.59 & \(\pm\)5.04 & \(\pm\)2.45 & \\  & & GPF & **54.24** & **64.24** & **56.84** & **50.62** & **65.34** & 61.34 & **67.94** & **57.31** & **51.26** & \\  & & \(\pm\)0.74 & \(\pm\)0.40 & \(\pm\)0.28 & \(\pm\)0.88 & \(\pm\)1.93 & \(\pm\)0.60 & \(\pm\)0.48 & \(\pm\)6.71 & +0.32 & \\  & & GPF-plus & **58.10** & **64.39** & 56.78 & 50.30 & 63.34 & **63.84** & **68.05** & **57.29** & **51.35** & \\  & & \(\pm\)1.92 & \(\pm\)0.30 & \(\pm\)0.25 & \(\pm\)0.78 & \(\pm\)0.85 & \(\pm\)1.13 & \(\pm\)0.97 & \(\pm\)4.46 & \(\pm\)0.32 & **59.27** \\ \hline \multirow{4}{*}{ContextPred} & FT & 49.45 & 58.77 & 54.46 & 49.89 & 48.60 & **56.14** & **60.91** & **56.37** & **46.33** & 53.43 \\  & & \(\pm\)5.74 & \(\pm\)0.70 & \(\pm\)0.54 & \(\pm\)1.16 & \(\pm\)3.40 & \(\pm\)8.22 & \(\pm\)1.84 & \(\pm\)1.90 & \(\pm\)1.76 & \\  & & GPF & 52.55 & **59.73** & 55.70 & **50.54** & **53.03** & **61.93** & **60.59** & **59.91** & **50.14** & \\  & & \(\pm\)1.24 & \(\pm\)0.86 & \(\pm\)0.29 & \(\pm\)0.91 & \(\pm\)5.98 & \(\pm\)5.84 & \(\pm\)1.28 & \(\pm\)6.31 & \(\pm\)0.33 & \\  & & **53.76** & **60.59** & **55.91** & **51.44** & **52.37** & **64.51** & **60.84** & **64.21** & **50.52** & **57.12** \\  & & \(\pm\)4.47 & \(\pm\)0.51 & \(\pm\)0.22 & \(\pm\)1.70 & \(\pm\)4.30 & \(\pm\)4.38 & \(\pm\)1.11 & \(\pm\)7.30 & \(\pm\)0.41 & \\ \hline \multirow{4}{*}{GCL} & FT & 54.40 & 48.35 & 50.29 & 53.23 & **54.05** & **46.73** & **60.05** & **49.87** & 49.94 & \multirow{4}{*}{51.62} \\  & & \(\pm\)2.87 & \(\pm\)1.67 & \(\pm\)0.19 & \(\pm\)0.87 & \(\pm\)0.16 & \(\pm\)1.88 & \(\pm\)3.80 & \(\pm\)1.78 & \(\pm\)1.77 & 51.62 \\ \cline{1-1}  & & GPF & 53.87 & **50.58** & 52.64 & **53.86** & **64.44** & **47.22** & **64.86** & **67.56** & **50.40** & \\ \cline{1-1}  & & \(\pm\)2.17 & \(\pm\)0.49 & \(exhibit significant advantages in terms of parameter efficiency compared to fine-tuning. It leads to reduced training time and storage space required for downstream adaptations.

### Comparison with linear probing

In the field of Natural Language Processing and Computer Vision, _linear probing_(Kumar et al., 2022; Wu et al., 2020; Tripuraneni et al., 2020; Du et al., 2020) is a widely employed method for adapting pre-trained models to downstream tasks. This approach involves freezing the parameters of the pre-trained model \(f\) and solely optimizing the linear projection head \(\theta\). To evaluate the effectiveness of linear probing, we conducted experiments on the chemistry datasets Toxcast and SIDER, and the results are summarized in Table 7. It is evident from the results that linear probing exhibits a significant performance decline compared to fine-tuning and our proposed graph prompt tuning. The primary distinction between linear probing and our proposed graph prompt tuning lies in the incorporation of an additional learnable graph prompt \(g_{\phi}(\cdot)\) in the input space. The substantial

\begin{table}
\begin{tabular}{c c c c} \hline \hline Dataset & Tuning Strategy & Tunable Parameters & Relative Ratio (\%) \\ \hline \multirow{3}{*}{Chemistry} & FT & \(\sim 1.8\)M & 100 \\  & GPF & \(\sim 0.3\)K & 0.02 \\  & GPF-plus & \(\sim 3\)-\(12\)K & 0.17-0.68 \\ \hline \multirow{3}{*}{Biology} & FT & \(\sim 2.7\)M & 100 \\  & GPF & \(\sim 0.3\)K & 0.01 \\  & GPF-plus & \(\sim 3\)-\(12\)K & 0.11-0.44 \\ \hline \hline \end{tabular}
\end{table}
Table 6: The number of tunable parameters for different tuning strategies.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline Pre-training & \begin{tabular}{c} Tuning \\ Strategy \\ \end{tabular} & 
\begin{tabular}{c} BBBP \\ Strategy \\ \end{tabular} & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE & PPI & **Avg.** \\ \hline \multirow{4}{*}{Infomax} & FT & 56.29 & 60.46 & 55.34 & 50.49 & 50.90 & 65.88 & 65.81 & 57.35 & 49.74 & 56.91 \\  & & \(\pm 1.65\) & \(\pm 0.66\) & \(\pm 0.20\) & \(\pm 1.29\) & \(\pm 4.92\) & \(\pm 1.76\) & \(\pm 1.43\) & \(\pm 2.67\) & \(\pm 0.72\) & 56.91 \\  & GPF & \(\bm{56.38}\) & 61.54 & 57.31 & 54.49 & 56.49 & 66.52 & **68.02** & 61.67 & 54.57 & 59.66 \\  & & \(\bm{\pm 2.84}\) & \(\bm{\pm 0.28}\) & \(\bm{\pm 0.35}\) & \(\pm 0.72\) & \(\pm 1.98\) & \(\pm 0.67\) & \(\pm 1.22\) & \(\pm 2.76\) & \(\pm 0.51\) & 59.66 \\  & GPF-plus & \(\bm{56.97}\) & \(\bm{62.48}\) & \(\bm{\pm 57.64}\) & \(\bm{54.86}\) & \(\bm{57.68}\) & \(\bm{67.00}\) & \(\bm{67.66}\) & \(\bm{56.16}\) & \(\bm{54.66}\) & \(\bm{54.62}\) & **60.07** \\ \hline \multirow{4}{*}{EdgePred} & FT & 51.27 & 61.48 & 58.28 & 52.23 & **58.50** & **64.32** & **59.82** & **58.06** & **48.06** & \\  & & \(\pm 3.89\) & \(\pm 1.21\) & \(\pm 0.81\) & \(\pm 1.27\) & \(\pm 2.54\) & \(\pm 2.48\) & \(\pm 1.47\) & \(\pm 0.85\) & \(\pm 2.00\) & 56.09 \\  & GPF & \(\bm{55.13}\) & 63.35 & 59.09 & 52.30 & **65.02** & **65.47** & **63.19** & 48.64 & 52.52 & 58.30 \\  & GPF & \(\bm{54.20}\) & \(\bm{\pm 0.94}\) & \(\bm{\pm 0.55}\) & \(\pm 0.54\) & \(\pm 4.13\) & \(\pm 0.31\) & \(\pm 1.49\) & \(\pm 1.70\) & \(\pm 0.46\) & 58.30 \\  & GPF-plus & \(\bm{54.20}\) & \(\bm{\pm 0.58}\) & \(\bm{\pm 0.23}\) & \(\bm{\pm 0.64}\) & \(\pm 3.54\) & \(\pm 0.37\) & \(\pm 1.28\) & \(\pm 55.65\) & \(\pm 0.44\) & **58.35** \\ \hline \multirow{4}{*}{AttrMasking} & FT & \(\bm{54.56}\) & 60.95 & 55.84 & 50.64 & 61.16 & 64.90 & 61.65 & 59.03 & 47.29 & \multirow{4}{*}{57.33} \\  & & \(\pm 4.82\) & \(\pm 1.28\) & \(\pm 0.40\) & \(\pm 1.16\) & \(\pm 1.19\) & \(\pm 1.43\) & \(\pm 3.31\) & \(\pm 2.89\) & \(\pm 1.43\) & 57.33 \\  & GPF & \(\bm{55.23}\) & \(\bm{63.36}\) & \(\bm{57.66}\) & 50.08 & **63.05** & **65.58** & **69.79** & **59.37** & **52.31** & **59.60** \\  & GPF & \(\bm{53.58}\) & \(\bm{63.89}\) & \(\bm{57.72}\) & \(\bm{51.70}\) & \(\bm{62.68}\) & \(\bm{66.47}\) & \(\bm{69.35}\) & \(\bm{58.50}\) & \(\bm{52.28}\) & \\  & GPF-plus & \(\bm{\pm 2.19}\) & \(\bm{\pm 0.58}\) & \(\bm{\pm 0.37}\) & \(\bm{\pm 0.61}\) & \(\pm 2.50\) & \(\pm 0.43\) & \(\pm 1.58\) & \(\pm 2.36\) & \(\pm 0.89\) & 59.57 \\ \hline \multirow{4}{*}{ContextPred} & FT & 50.42 & 60.74 & **56.00** & 51.81 & **51.48** & **64.87** & **59.82** & **50.43** & **45.39** \\  & & \(\pm 0.57\) & \(\pm 0.88\) & \(\bm{\pm 0.29}\) & \(\pm 1.77\) & \(\pm 2.86\) & \(\pm 2.30\) & \(\pm 2.00\) & \(\pm 0.37\) & \(\pm 0.42\) & 54.55 \\  & GPF & \(\bm{52.33}\) & \(\bm{63.91}\) & \(\bm{57.32}\) & \(\bm{53.55}\) & **54.31** & **65.80** & **68.51** & **54.70** & **50.44** & \\  & GPF-plus & \(\bm{\pm 5.07}\) & \(\bm{\pm 0.82}\) & \(\bm{\pm 0.30}\) & \(\bm{\pm 0.88}\) & \(\bm{\pm 2.58}\) & \(\bm{\pm 0.45}\) & \(\bm{\pm 2.23}\) & \(\bm{\pm 5.89}\) & \(\pm 0.64\) & 57.87 \\  & GPF-plus & \(\bm{53.62}\) & \(\bm{64.89}\) & \(\bm{58.02}\) & \(\bm{\pm 0.41}\) & \(\bm{\pm 1.5}\) & \(\bm{\pm 2.38}\) & \(\bm{\pm 0.54}\) & \(\bm{\pm 3.80}\) & \(\bm{\pm 5.85}\) & \(\bm{\pm 0.50}\) & \(\bm{58.28}\) \\ \hline \multirow{4}{*}{GCL} & FT & 44.06 & 48.47 & 51.91 & **56.10** & 48.13 & 53.93 & **32.63** & **55.41** & **49.44** & \\  & & \(\pm 2.55\) & \(\pm 1.63\) & \(\bm{\pm 0.3performance gap observed between these approaches underscores the importance of integrating a graph prompt for the effective adaptation of pre-trained models.

### Comparison with other tuning methods

We also compare our proposed graph prompt tuning with other tuning methods described as follows:

* PARTIAL-\(k\): We tune the last \(k\) layers of the pre-trained model \(f\) with a projection head \(\theta\) and freeze other parts, which is utilized in Zhang et al. (2016); He et al. (2021); Jia et al. (2022).
* MLP-\(k\): We freeze the pre-trained model \(f\) and utilize a multi-layer perceptron (MLP) with \(k\) layers as the projection head to perform the classification.

We conduct the experiments on the biology datasets (PPI), and Table 8 summarizes the results. The experimental results indicate that our methods outperform other tuning methods in all cases.

### Extra results on GCC

Another popular pre-training strategy for graph contrastive learning involves following the training steps outlined in Qiu et al. (2020). First, we introduce the datasets utilized for pre-training GCC. The self-supervised pre-training task of GCC is conducted on six graph datasets, and Table 9 provides detailed statistics for each dataset. For the downstream tasks of the pre-trained GCC, we employ

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Pre-training & Tuning & ToxCast & SIDER & **Avg.** \\ Strategy & Strategy & & 65.16 & 63.34 & \\ \multirow{4}{*}{Infomax} & \multirow{4}{*}{Linear Probing} & 63.84 & 59.62 & \multirow{4}{*}{61.73} \\  & & \(\pm 0.10\) & \(\pm 0.73\) & \\  & & GPF & \(\pm 0.53\) & \(\pm 0.81\) & \\  & & GPF-plus & **66.35** & 65.62 & \multirow{4}{*}{65.98} \\  & & \(\pm 0.37\) & \(\pm 0.74\) & \\ \hline \multirow{4}{*}{EdgePred} & \multirow{4}{*}{Linear Probing} & 66.29 & 64.35 & \multirow{4}{*}{63.36} \\  & & \(\pm 0.45\) & \(\pm 0.78\) & \\ \multirow{4}{*}{GPF-plus} & \multirow{4}{*}{Linear Probing} & 65.25 & 61.47 & \multirow{4}{*}{63.36} \\  & & \(\pm 0.09\) & & \\ \multirow{4}{*}{GPF} & \multirow{4}{*}{GPF} & 65.65 & 67.20 & \multirow{4}{*}{66.42} \\  & & \(\pm 0.30\) & & \\ \multirow{4}{*}{AttrMasking} & \multirow{4}{*}{Linear Probing} & 65.94 & **67.51** & \multirow{4}{*}{66.77} \\  & & \(\pm 0.31\) & & \\ \multirow{4}{*}{AttrMasking} & \multirow{4}{*}{Linear Probing} & 64.75 & **62.60** & \multirow{4}{*}{63.67} \\  & & \(\pm 0.07\) & & \\ \multirow{4}{*}{GPF} & \multirow{4}{*}{GPF} & 66.32 & **69.13** & **67.72** \\  & & \(\pm 0.42\) & \(\pm 1.16\) & \\ \multirow{4}{*}{GPF-plus} & \multirow{4}{*}{GPF-plus} & **66.58** & **68.65** & \multirow{4}{*}{67.61} \\  & & \(\pm 0.13\) & & \\ \hline \multirow{4}{*}{ContextPred} & \multirow{4}{*}{Linear Probing} & 66.39 & 64.45 & \multirow{4}{*}{65.42} \\  & & \(\pm 0.57\) & & \\ \multirow{4}{*}{GPF} & \multirow{4}{*}{Linear Probing} & 65.35 & **61.28** & \multirow{4}{*}{63.31} \\  & & \(\pm 0.09\) & & \\ \multirow{4}{*}{GPF} & \multirow{4}{*}{GPF} & **67.92** & 66.18 & \multirow{4}{*}{67.05} \\  & & \(\pm 0.35\) & & \\ \multirow{4}{*}{GPF-plus} & \multirow{4}{*}{GPF-plus} & **67.58** & **66.94** & \multirow{4}{*}{**67.26**} \\  & & \(\pm 0.54\) & & \\ \multirow{4}{*}{GCL} & \multirow{4}{*}{Linear Probing} & 62.54 & **60.63** & \multirow{4}{*}{61.58} \\  & & \(\pm 0.26\) & & \\ \multirow{4}{*}{GCL} & \multirow{4}{*}{Linear Probing} & 50.92 & **52.91** & \multirow{4}{*}{51.91} \\  & & \(\pm 0.22\) & & \\ \multirow{4}{*}{GPF} & \multirow{4}{*}{GPF} & 62.70 & **61.26** & \multirow{4}{*}{61.98} \\  & & \(\pm 0.46\) & & \\ \multirow{4}{*}{GPF-plus} & \multirow{4}{*}{GPF-plus} & **62.76** & **62.37** & \multirow{4}{*}{**62.56**} \\  & & \(\pm 0.75\) & & \\ \hline \hline \end{tabular}
\end{table}
Table 7: Test ROC-AUC (%) performance on ToxCast and SIDER with the linear probing.

IMDB-BINARY and IMDB-MULTI datasets (Yanardag and Vishwanathan, 2015). Each dataset consists of a collection of graphs associated with specific target labels. We evaluate GPF on these datasets, and the results are presented in Table 10. The experimental findings consistently demonstrate that GPF outperforms fine-tuning when adapting models pre-trained using the GCC strategy.

### Hyper-parameter settings

This section presents the hyper-parameters used during the adaptation stage of pre-trained GNN models on downstream tasks for our proposed graph prompt tuning. Table 11 summarizes the hyper-parameter settings. You can also visit our code repository to obtain the specific commands for reproducing the experimental results.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Pre-training Strategy & Tuning Strategy & IMDB-B & IMDB-M & **Avg.** \\ \hline \multirow{3}{*}{GCC (E2E)} & FT & 72.60 \(\pm\)4.72 & 49.07 \(\pm\)3.59 & 60.83 \\  & GPF & **73.40**\(\pm\)3.80 & **49.17**\(\pm\)3.12 & **61.28** \\ \hline \multirow{3}{*}{GCC (MoCo)} & FT & 71.70 \(\pm\)4.98 & 48.07 \(\pm\)2.91 & 59.88 \\  & GPF & **72.50**\(\pm\)3.20 & **49.33**\(\pm\)3.93 & **60.91** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Test accuracy (%) performance of GCC on graph classification benchmarks.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Pre-training Strategy & FT & MLP-3 & Partial-1 & Partial-3 & GPF & GPF-plus \\ \hline \multirow{3}{*}{Infomax} & 71.29 \(\pm\)1.79 & 74.68 \(\pm\)0.56 & 74.36 \(\pm\)0.92 & 73.28 \(\pm\)0.18 & 77.02 \(\pm\)0.42 & **77.03**\(\pm\)0.32 \\  & EdgePred & 71.54 \(\pm\)0.85 & 74.60 \(\pm\)0.88 & 73.24 \(\pm\)0.68 & 73.35 \(\pm\)0.77 & 76.98 \(\pm\)0.20 & **77.00**\(\pm\)0.12 \\ \cline{1-1}  & AttrMasking & 73.93 \(\pm\)1.17 & 77.99 \(\pm\)0.42 & 75.91 \(\pm\)0.10 & 74.02 \(\pm\)0.37 & **78.91**\(\pm\)0.25 & 78.90 \(\pm\)0.11 \\ \cline{1-1}  & ContextPred & 72.10 \(\pm\)1.94 & 76.01 \(\pm\)0.68 & 76.62 \(\pm\)0.92 & 74.86 \(\pm\)0.79 & 77.42 \(\pm\)0.07 & **77.71**\(\pm\)0.21 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Test ROC-AUC (%) performance on protein function prediction benchmarks with different tuning methods.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Pre-training Strategy & Tuning Strategy & IMDB-B & IMDB-M & **Avg.** \\ \hline \multirow{3}{*}{GCC (E2E)} & FT & 72.60 \(\pm\)4.72 & 49.07 \(\pm\)3.59 & 60.83 \\  & GPF & **73.40**\(\pm\)3.80 & **49.17**\(\pm\)3.12 & **61.28** \\ \hline \multirow{3}{*}{GCC (MoCo)} & FT & 71.70 \(\pm\)4.98 & 48.07 \(\pm\)2.91 & 59.88 \\  & GPF & **72.50**\(\pm\)3.20 & **49.33**\(\pm\)3.93 & **60.91** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Test accuracy (%) performance of GCC on graph classification benchmarks.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Dataset** & **Academia** & **DBLP(SNALP)** & **DBLP(NetRep)** & **IMDB** & **Facebook** & **LiveJournal** \\ \hline \(|V|\) & 137,969 & 317,080 & 540,486 & 896,305 & 3,097,165 & 4,843,953 \\ \(|E|\) & 739,984 & 2,099,732 & 30,491,158 & 7,564,894 & 47,334,788 & 85,691,368 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Statistics of datasets for pre-training