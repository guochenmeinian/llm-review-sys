# MAC Advice for Facility Location Mechanism Design

Zohar Barak

School of Computer Science

Tel Aviv University

zoharbarak@mail.tau.ac.il

Anupam Gupta

New York University and Google Research

anupam.g@nyu.edu

Inbal Talgam-Cohen

School of Computer Science

Tel Aviv University

inbaltalgam@gmail.com

###### Abstract

Algorithms with predictions are gaining traction across various domains, as a way to surpass traditional worst-case bounds through (machine-learned) advice. We study the canonical problem of \(k\)-facility location mechanism design, where the \(n\) agents are strategic and might misreport their locations. We receive a prediction for each agent's location, and these predictions are crucially allowed to be only "mostly" and "approximately" correct (Mac for short): a \(\delta\)-fraction of the predicted locations are allowed to be arbitrarily incorrect, and the remainder of the predictions are required to be correct up to an \(\varepsilon\)-error. Moreover, we make no assumption on the independence of the errors. Can such "flawed" predictions allow us to beat the current best bounds for strategyproof facility location?

We show how natural robustness of the \(1\)-median (also known as the geometric median) of a set of points leads to an algorithm for single-facility location with MAC predictions. We extend our results to a natural "balanced" variant of the \(k\)-facility case, and show that without balancedness, robustness completely breaks down even for \(k=2\) facilities on a line. As our main result, for this "unbalanced" setting we devise a truthful random mechanism, which outperforms the best known mechanism (with no predictions) by Lu et al. (2010). En route, we introduce the problem of "second" facility location, in which the first facility location is already fixed. Our robustness findings may be of independent interest, as quantitative versions of classic breakdown-point results in robust statistics.

## 1 Introduction

Algorithms with predictions is a popular field of study in recent years within the paradigm of beyond the worst case analysis of algorithms -- see Mitzenmacher and Vassilvitskii (2022) for a comprehensive survey. Motivated by developments in machine learning, this area assumes that algorithms can access predictions regarding the input or solution, thus leveraging predictability in typical computational problems. Recently, Agrawal et al. (2022) and Xu and Lu (2022) proposed to study predictions in the context of mechanism design, where they have the potential to transform existing designs by adding information about agents' private knowledge (see Balkanski et al. (2023)).

Our focus in this work is on the canonical problem of facility location. The (utilitarian) \(k\)-facility location problem is as follows: Consider a multi-set of \(n\) points \(X\subset\mathbb{R}^{d}\) and \(k\) facility locations; the _cost_ of point \(x\in X\) is the minimum distance between \(x\) and any of the \(k\) locations. The goal is to compute \(k\) locations that minimize the utilitarian cost, which is the total cost of points in \(X\). In the context of mechanism design, the points \(X\) are the privately-known true locations of strategic agents,and the mechanism receives location reports (see Chan et al. (2021) for a recent survey). Facility location mechanism design with predictions has been studied by Agrawal et al. (2022); Xu and Lu (2022).1 We diverge from previous work by assuming a prediction for each point -- that is, the advice \(X^{\prime}\) consists of a prediction \(x^{\prime}_{i}\) for every \(x_{i}\in X\). Importantly, our measure of prediction error allows for very large errors (outliers) for a \(\delta\)-fraction of points (hence we call it the _"mostly_ approximately correct" model). Our work shows how to get improved bounds despite these large errors, and can thus be of interest beyond facility location.

Footnote 1: Predictions have also been studied for the online non-strategic version of facility location — see Appendix A.

**Standard worst-case prediction error.** In the majority of the literature, the goal of (algorithm or mechanism) design with predictions centers around two seemingly-conflicting properties: _robustness_ to erroneous predictions, and _consistency_ with predictions that are non-erroneous. A fairly common way of achieving both consistency and robustness is interpolating between the approaches of completely following the predictions, and applying the best worst-case algorithm while disregarding the predictions. An ideal design is one that gracefully transitions -- as the prediction error increases -- from optimal performance when the prediction is correct, to the best-known worst-case performance (Lykouris and Vassilvitskii, 2021; Purohit et al., 2018; Agrawal et al., 2022). By definition, achieving such graceful degradation hinges on how the prediction error is measured.

The most common measure of prediction error is arguably the distance between the predicted and actual values. If \(X=\{x_{1},\ldots,x_{n}\}\) contains the actual values and \(X^{\prime}=\{x^{\prime}_{1},\ldots,x^{\prime}_{n}\}\) is the prediction, the error is defined as \(\eta=\ell_{p}(X-X^{\prime})\), where \(\ell_{p}\) represents either the \(\ell_{1}\) norm (\(\ell_{1}(t)=\sum_{i}|t_{i}|\)) or the \(\ell_{\infty}\) norm (\(\ell_{\infty}(t)=\max_{i}|t_{i}|\)). This is a "worst case" measure of prediction error, since a single deviation in one of the \(n\) entries can significantly inflate the error. Prediction errors measured in this way appear in the context of ski rental Purohit et al. (2018), online caching Lykouris and Vassilvitskii (2021); Rohatgi (2020), the secretary problem Antoniadis et al. (2020), single-item auctions Xu and Lu (2022), and many other problems. In what follows, we address this type of prediction error as the "worst case" error model.

**Motivation for an alternative measure.** We demonstrate the need for an alternative measure of prediction error combined with new algorithms through the following \(k\)-facility location instance, in which a single error causes the optimal solution for prediction \(X^{\prime}\) to perform badly for the actual dataset \(X\).2

Footnote 2: Similarly, bad performance persists for any algorithm that does not get location reports from the agents.

**Example 1** (Sensitivity of \(2\)-facility location on a line).: _Let \(k=2\), and assume the \(n\) agents' actual locations \(X\) are divided evenly between 0 and 1 on the real line. Let \(X^{\prime}\) be the prediction, with all coordinates predicted accurately except for a single coordinate \(i\), which is predicted erroneously to be located at \(x^{\prime}_{i}=M\gg n\). While the optimal solution for \(X\) is to place the \(k=2\) facilities at \(0\) and \(1\) for a total cost of zero, in the optimal solution for \(X^{\prime}\) one of the facilities moves to \(M\). This solution performs badly for \(X\) -- the approximation ratio compared to the optimal solution is unbounded._

In many ML contexts, it is very possible that a small fraction of points have a large "worst case" prediction error -- even excellent predictors may err on a certain fraction of the dataset. This motivates alternative measures of prediction accuracy. Concretely, we ask:

**Question 1**.: _For \(2\)-facility location on a line (as in Example 1), can we design a mechanism that uses agent reports to get a good solution despite a large "worst case" prediction error?_

We answer this question positively, proving that, using agent reports, it is possible to design resilient mechanisms against adversarial "worst case" errors. The main take-away is that predictions can still

Figure 1: Illustration of Example 1; the predictions \(X^{\prime}\) are on the left side and the real points \(X\) are on the right side. The triangles are the optimal facility locations for the points. A single bad prediction translates into an unbounded “worst case” error and approximation ratio.

be valuable despite high "worst case" prediction error, if the fraction of such grossly-erroneous points is small. In other words, a high "worst case" prediction error may not always justify "throwing out" the predictions and settling for the best worst-case guarantee (although it is currently standard in design with predictions). Our new error model and results thus suggest a new pathway for effectively utilizing predictive advice in strategic settings, despite significant uncertainties in the predictions.

## 2 Overview of Contributions

### Our Model

We introduce \(\operatorname{\textsc{Mac}}\) predictions where \(\operatorname{\textsc{Mac}}\) stands for _Mostly Approximately Correct_. This formulation captures the essential features of some closely related previous models, e.g., the model (Azar et al., 2022) formulated for online metric problems, and the PMAC model of (Balcan and Harvey, 2018) formulated in the context of learning submodular functions. A vector of predictions \(X^{\prime}\) is \(\operatorname{\textsc{Mac}}(\varepsilon,\delta)\) with respect to dataset \(X\) if at least a \((1-\delta)\)-fraction (i.e., most) of the predictions are approximately correct up to an additive \(\varepsilon\)-error. Intuitively, if we do not think it is likely that most of our predictions will be close to being correct --we would typically refrain from relying on them in decision-making processes. Throughout, \(\delta\in[0,0.5)\) is assumed to be small (but can still be a constant). Crucially, when a prediction is not approximately correct (i.e., it belongs to the \(\delta\)-fraction of incorrect predictions), the error is allowed to be arbitrarily large. Also, the \(\delta\)-fraction of wrong predictions is allowed to be arbitrary among all possible subsets of that size (there is no assumption of independence). We show that despite their adversarial nature, and the fact that the prediction error is unbounded, such predictions can be used, in combination with strategic reports, to produce a solution with better cost compared to the best no-prediction solution.

The \(\operatorname{\textsc{Mac}}\) prediction model is suitable for capturing errors from a variety of sources including ML-generated predictions, data that accommodates changes over time, data corrupted by a malicious user, and expert advice (see Appendix B for more details). The accuracy parameters \(\varepsilon,\delta\) of the \(\operatorname{\textsc{Mac}}\) model can also be viewed as _confidence parameters_. The use of confidence parameters is prevalent in the algorithms with predictions literature (see, e.g., Agrawal et al. (2022)). Arguably, \(\varepsilon,\delta\) are highly _explainable_ compared to most confidence measures in the literature, and it is also quite natural to estimate them from data.

### Our Results

We design both deterministic and randomized strategyproof anonymous mechanisms for facility location problems, with \(\operatorname{\textsc{Mac}}\) predictions of the \(n\) agent locations. A preliminary simplifying observation is that in the context of facility location, it suffices to consider mostly-correct (i.e., \(\operatorname{\textsc{Mac}}(0,\delta)\)) predictions, since handling a nonzero approximation parameter \(\varepsilon\) follows directly (see Appendix C).

**Randomized mechanism design.** As our main technical result, for \(2\)_-facility location on a line_ we design a randomized mechanism that guarantees an expected approximation ratio of \(3.6+O(\delta)\) (see Algorithm 4 and Theorem 9). For sufficiently small (but still constant) \(\delta\), this improves upon the best-known no-prediction expected guarantee of \(4\) by Lu et al. (2010). (The mechanism of Lu et al. (2010) works for any metric space.) This result provides a positive answer to Question 1, justifying the take-away that predictions with high "worst case" error are useful. We give a brief description of the mechanism in Section 2.3. Note that \(2\)-facility location on a line was also studied by Procaccia and Tennenholtz (2013), and they provided a _deterministic_ mechanism with an \((n-2)\) approximation ratio, which is tight (Lu et al., 2010; Potakis and Tzamos, 2014).

**Deterministic mechanism design.** For _single-facility location in \(\mathbb{R}^{d}\)_, we design a mechanism that guarantees an approximation ratio of \(\min\{1+\frac{4\delta}{1-2\delta},\sqrt{d}\}\) (see Algorithm 1 and Theorem 5). For sufficiently small (but still constant) \(\delta\), this improves upon the no-prediction guarantee of \(\sqrt{d}\) by Meir (2019), which is tight for \(d=2\). For \(\beta\)_-balanced \(k\)-facility location_ with constant \(k\) (in any metric space), which is a natural extension of facility location where at least a \(\beta\)-fraction of the \(n\) points must be assigned to each of the \(k\) facilities (for cost-sharing or load-balancing reasons), we design a simple strategyproof mechanism which guarantees an approximation ratio of \(c+O(\frac{\delta}{\beta})\), where \(c\) is a constant (see Algorithm 7 and Theorem 13). For a sufficiently small (but still constant) \(\delta\), this greatly improves upon the best-known no-prediction guarantee of \(\nicefrac{{n}}{{2}}-1\) by Aziz et al. (2020) (we remark that Aziz et al. (2020) study a variant called _capacitated_, to which our result applies).3

Footnote 3: \(\beta\)-balanced means every cluster is of size at least \(\beta n\), and hence at most \(n-(k-1)\beta n\).

While we have not optimized the constants, our conceptual contribution is in showing that even if the worst case error of the predictions is unbounded, it can still provide useful information for improving the performance of facility location mechanisms when coupled with strategic reports. In other words, the Mac model allows achieving robustness to outliers while still beating the no-prediction performance. Our single-facility result demonstrates how the accuracy parameter of the model, \(\delta\), can serve as a trust parameter which has natural explainability. Another interesting feature of our results is that utilizing Mac predictions seems to suggest a richer family of mechanisms than standard interpolations of "no-predictions" and "complete trust" in the predictions.

Below we give a summary of the results in tables.

### Our Techniques and Roadmap

We break the problem into two natural conceptual steps. First, we establish what can be done by just using the predictions - this requires us to show some stability properties of algorithms for k-medians. For some problems, such as the single facility location, we show that using only the predictions is enough to get a good approximation ratio. For other problems, such as \(2\)-facility location, it is not enough, and thus we need to include the input in the computation. For this purpose, we show how to choose the first facility by just using the predictions, and a way to choose the second facility by taking the first facility and the input into account. Then, we show how the interplay between these parts allows us to get a better approximation. Let us now give a few more details for these steps.

**Robust statistics and the robustness of the geometric median and \(k\)-medians.** We first develop quantitative versions of classic results in robust statistics on _breakdown points_, which may be of possible independent interest. Consider a _location estimator_, a function that gets \(n\) points in a metric space and returns \(k\) locations. Its breakdown point is the proportion \(\delta\) of adversarially-modified points it can handle before returning an arbitrarily far result from the original estimator. It is well-known that the mean has a breakdown point of \(\delta=\nicefrac{{1}}{{n}}\) (consider changing one point to an arbitrarily large value) and that the median is robust with a breakdown point of \(\delta=\nicefrac{{1}}{{2}}\). However, the notion of breakdown point is qualitative, and does not reveal the relation between the fraction \(\delta\) of modified points, and how much the estimator changes. In Section 5 we fill this gap by defining measures of robustness (_distance robustness_ (Definition 9) and _approximation robustness_ (Definition 10)). Next, we show robustness properties of the median and its generalizations, showing that the cost of the geometric median

\begin{table}
\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \hline \multicolumn{3}{|c|}{**Deterministic Mechanism Design**} \\ \hline
**Problem** & **Best known ”no predictions” approximation ratio** & **Approximation ratio obtained using MAC predictions** \\ \hline Single facility location in \(\mathbb{R}^{d}\) & \(\sqrt{d}\) & \(1+O(\delta)\) \\ \hline \(\beta\)-balanced \(k\) facilities in \(\mathbb{R}^{d}\) & Linear (O(n))* & A constant depending on \(\delta,\beta\) \\ \hline \end{tabular}
\end{table}
Table 1: Deterministic Mechanism Design.

*The linear approximation ratio is of Aziz et al. (2020) for the capacitated facility location variant where there is an additional constraint of a maximum cluster size. The \(\beta\)-balanced variant of the problem was not studied without predictions, but is comparable to the capacitated variant, since a minimum cluster size (as in the balanced variant) implies a maximum cluster size (and for \(k=2\), a maximum cluster size implies a minimum cluster size as well).

\begin{table}
\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \hline \multicolumn{3}{|c|}{**Random Mechanism Design**} \\ \hline
**Problem** & **Best known ”no predictions” approximation ratio** & **Approximation ratio obtained using MAC predictions** \\ \hline
2 facility location on a line & 4 & \(3.6+O(\delta)\) \\ \hline \end{tabular}
\end{table}
Table 2: Random Mechanism Designbehaves smoothly as the fraction of modified points increases. These smooth robustness results prove crucial to obtaining improved mechanisms for facility locations using predictions. Moreover, the robustness guarantees have interesting interpretations in the context of robust statistics and robust clustering in machine learning, and might also be of independent interest as explained in Appendix D.

**Second facility location.** In Section 6.2.1 we introduce the second facility location problem. It is often the case that facilities are created incrementally and not all at once. Hence, it is natural to ask how can we design a mechanism for choosing the location of a second facility (say a second hospital) given an existing first one. We show that the random proportional choice (second step) used in the _proportional mechanism_ by Lu et al. (2010) achieves a (tight) approximation ratio of \(3\) for the line metric, improving over the factor of \(4\) in the general case for the second facility location problem.

**The Robust Half Technique: Beyond interpolation.** We generate half of the solution (the first facility location) from the predictions, and use the result together with the input (agent reports) to generate the other half of the solution (the second facility location). To generate the first half of the solution, we define and analyze a robust location estimator we name Big-Cluster-Center (which turns out to be a non-trivial task); Hence the name: "Robust Half". To generate the second half of the solution from the input (given the first half) we define and analyze the second facility location on the line problem. Finally, to combine the results we consider two cases: the input either induces a balanced clustering or an unbalanced clustering. We utilize the robustness results (both for balanced k-medians and for Big-Cluster-Center) to show that in both cases we get a good approximation ratio. Our mechanism differs from many existing designs which interpolate between the two approaches of trusting the predictions and using the no-predictions algorithm.

## 3 Related Work

**Facility location mechanism design with predictions.** In the "worst case" error model, Agrawal et al. (2022) show a mechanism for _single-facility location_ on the real plane \(\mathbb{R}^{2}\). Their prediction consists of a single value representing advice for where to locate the facility. Given a trust parameter \(c\in[0,1]\), their mechanism guarantees an approximation ratio of at most \(\frac{\sqrt{2c+2}}{1+c}\) if the prediction is perfect, and \(\frac{\sqrt{2c+2}}{1-c}\) in the worst case. In theory, we could reduce the Mac model to the one studied by Agrawal et al. (2022), by setting their single prediction to be the geometric median of the Mac predictions. Then \(c\) can be determined according to \(\delta\). However, knowing \(\delta\) and the \(\delta\)-robustness of \(1\)-\(\mathrm{Median}\), we can directly choose either the no-predictions mechanism or the predictions' \(1\)-\(\mathrm{Median}\), rather than interpolating between these two solutions.

Mechanism design for the \(2\)_-facility location_ on the line with predictions is studied by Xu and Lu (2022), where the predictions are for the locations of the agents. While they do not define an error parameter, their deterministic mechanism is in line with the "worst case" prediction error model: it achieves an approximation ratio of \(\frac{n}{2}\) if the predictions are perfect, and \(2n\) if they are arbitrarily bad.

**Two-facility location on a line mechanism design without predictions.** In the setting of no-predictions, the best strategyproof deterministic anonymous mechanism achieves a tight approximation ratio of \(n-2\)Procaccia and Tennenholtz (2013); Fotakis and Tzamos (2014). In comparison, in our Mac model under the additional assumption that the number of agents assigned to each facility in the optimal solution is of minimum size \(\Omega(\delta n)\), the deterministic mechanism in Algorithm 7 achieves a constant approximation ratio. Under no further assumption, our randomized mechanism in Algorithm 4 achieves a constant expected approximation ratio for small \(\delta\).

Additional related work is deferred to Appendix A.

## 4 Formal Definitions

Let \((V,d)\) be a metric space and \(k,n\in\mathbb{N}\). The input to the facility location problem is a multi-set \(X=\{x_{1},\ldots,x_{n}\}\in V^{n}\). We are also given a _prediction_, which is another multi-set \(X^{\prime}=\{x^{\prime}_{1},\ldots,x^{\prime}_{n}\}\in V^{n}\). The distance between a point \(u\) and a multi-set \(W\) is \(d(u,W)=\min_{w\in W}d(u,w)\). The _Hausdorff distance_ between multi-sets \(U,W\in V^{n}\) is

\[d_{H}(U,W):=\max\{\max_{u\in U}d(u,W),\max_{w\in W}d(w,U)\},\]

that is, the maximum distance between a point in one multi-set and the other multi-set.

**Definition 1**.: _Fix \(\varepsilon\geq 0\). For \(x,x^{\prime}\in V\), we say \(x^{\prime}\) is an \(\varepsilon\)-correct prediction of \(x\) if \(d(x,x^{\prime})\leq\varepsilon\), otherwise we say it is \(\varepsilon\)-incorrect._

**Definition 2**.: _The \((\varepsilon,r)\)-neighborhood of multi-set \(X\in V^{n}\) is defined to be all predictions \(X^{\prime}\in V^{n}\) such that \(|\{i\in[n]\mid x^{\prime}_{i}\text{ is an $\varepsilon$-incorrect prediction of }x_{i}\}|\leq r\)._

The \((0,r)\)-neighborhood of \(X\) is often just referred to as the \(r\)_-neighborhood_ of \(X\).

**Definition 3** (Mostly Approximately Correct (Mac) Predictions).: _Fix \(\varepsilon\geq 0\), \(\delta\in[0,0.5)\). The point set \(X^{\prime}\) is an \((\varepsilon,\delta)\)-Mac prediction for \(X\) if \(X^{\prime}\) belongs to the \((\varepsilon,\delta|X|)\)-neighborhood of \(X\)._

**Definition 4** (Location Estimator).: _For \(n,k\in\mathbb{N}\), a function \(f(X):V^{n}\to V^{k}\) is called a location estimator._

Some common examples of location estimators with \(k=1\) for points on the real line are the minimum, maximum, mean, and median; for general metric spaces, \(k\)-means and \(k\)-medians are well-studied examples (\(k\)-means is similar to \(k\)-medians with squared distances). For \(k=1\) and \(V=\mathbb{R}^{d}\), the \(k\)-medians solution is also called the Geometric-Median which is a generalization of the median to higher dimensions. A different such median generalization is the Coordinate-Wise-Median:

**Definition 5**.: _For a multi-set of \(n\) points \(X\subseteq\mathbb{R}^{d}\): Coordinate-Wise-Median\((X):=(l_{1},\ldots,l_{d})\) where where \(l_{j}\) is the median of the multi-set of the \(j\)'th coordinates of \(X\) for all \(j\in[d]\)._

**Definition 6** (k-medians cost function).: _Given multi-sets \(X\in V^{n},F\in V^{k}\), the \(k\)-medians cost function is_

\[\operatorname{med}_{k}(X,F):=\sum_{x\in X}d(x,F).\] (1)

_In the context of the facility location problem, this function is also known as the social cost function or the utilitarian goal function._

The \(k\)_-medians problem_ takes as input a multi-set \(X\in V^{n}\) and outputs a multi-set \(F\) that minimizes this cost function. The minimizer \(F^{*}\in V^{k}\) the problem is called the k-Medians _solution_. k-Medians can be viewed as a _location estimator_ of \(X\) that optimizes the \(k\)-medians objective function:

**Definition 7** (Center-Induced Partitions).: _A collection of centers \(F=\{f_{1},\ldots,f_{k}\}\subseteq V^{k}\) induces a partition \(\mathscr{C}=\{C_{1},\ldots,C_{k}\}\) of the dataset \(X\) if \(C_{i}\subseteq\{x\in X\mid d(x,f_{i})\leq d(x,f_{j})\,\forall j\}\). This partition is called the clustering of \(X\) induced by \(F\). This partition (or clustering) is \(\beta\)-balanced if \(|C_{i}|\geq\beta|X|\) for all \(i\in[k]\)._

Note that a collection \(\mathscr{C}\) need not be unique, since points in \(X\) can "choose" between any of their closest centers in \(F\).

**Definition 8** (\(\beta\)-balanced \(k\)-median).: _The \(\beta\)-balanced \(k\)-medians problem takes a dataset \(X\in V^{n}\) as input, and outputs centers \(F\in V^{k}\) along with an induced \(\beta\)-balanced partition \(\mathscr{C}\) of \(X\); the goal is to minimize the cost function \(\operatorname{med}_{k}(X,F)=\sum_{x\in X}d(x,F)\)._

This problem is similar to the _lower-bounded \(k\)-median_ problem where we want to output \(F=\{f_{1},\ldots,f_{k}\}\in V^{k}\) as well as a partition \(\{C_{1},\ldots,C_{k}\}\) of \(X\) such that \(|C_{i}|\geq\beta|X|\), in order to minimize \(\sum_{x\in C_{i}}d(x,f_{i})\) (see Han et al. (2020), Wu et al. (2022)). A significant difference between that definition and ours is that we require the partition to be induced by \(F\) (in the sense of Definition 7), which is not required by lower-bounded \(k\)-median.

## 5 \(\delta\)-Robustness of Location Estimators

In this section we define notions of robustness with respect to changing a \(\delta\)-fraction of the points (hence referred to as \(\delta\)-robustness) and analyze the \(\delta\)-robustness of \(1\)-Median and k-Medians. These results will be very useful for the mechanism design for facility location with predictions. We consider two robustness notions for location estimators: The first notion measures the movement in the solution (as measured by the Hausdorff distance), and the second measures the change in an objective function applied to the solution.

**Definition 9** (Distance Robustness).: _Let \(\rho\geq 0\), \(\delta\in[0,0.5)\). For location estimators \(f,\widehat{f}:V^{n}\to V^{k}\), we say that \(\widehat{f}\) is \((\rho,\delta)\)-distance-robust with respect to \(f\) if for any \(X\in V^{n}\) and any \(X^{\prime}\) in the \(\delta|X|\)-neighborhood of \(X\),_

\[d_{H}\Big{(}f(X),\widehat{f}(X^{\prime})\Big{)}\leq\rho.\]_If \(\widehat{f}=f\), we say that \(f\) is \((\rho,\delta)\)-distance-robust._

In the next definition, cost function \(F\) is evaluated for the same dataset \(X\in V^{n}\) with respect to two different solutions \(f(X)\) and \(\widehat{f}(X^{\prime})\).

**Definition 10** (Approximation Robustness).: _Let \(\gamma\geq 1\), \(\delta\in[0,0.5)\) and let \(F:V^{n}\times V^{k}\to\mathbb{R}_{\geq 0}\) be a cost measure. For location estimators \(f,\widehat{f}:V^{n}\to V^{k}\), we say that \(\widehat{f}\) is a \((\gamma,\delta)\)-approximation-robust solution for cost function \(F\) with respect to \(f\) if for any \(X\in V^{n}\) and any \(X^{\prime}\) in the \(\delta|X|\)-neighborhood of \(X\),_

\[F(X,\widehat{f}(X^{\prime}))\leq\gamma\cdot F(X,f(X)).\] (2)

_If \(\widehat{f}=f\), we say that \(f\) is a \((\gamma,\delta)\)-approximation-robust solution for \(F\). If Eq. (2) only holds for datasets \(X\in\mathscr{Y}\) for some \(\mathscr{Y}\subseteq V^{n}\), we say that \(f\) is a \((\gamma,\delta)\)-approximation-robust solution for \(F\) restricted to instances \(\mathscr{Y}\)._

We give the the following \(\delta\)-robustness results. For the \(1\)-Median:

**Theorem 2**.: _For \(\delta<\nicefrac{{1}}{{2}}\), the \(1\)-Median location estimator is \(\left(\frac{2}{(1-2\delta)}\cdot\frac{\operatorname{med}_{1}(X)}{|X|},\delta\right)\)-distance-robust, where \(\operatorname{med}_{1}(X)\) is the optimal cost of the geometric median, that is: \(\operatorname{med}_{1}(X)=\sum_{x_{i}\in X}d(x_{i},\textsc{Geometric-Median}(X))\)._

**Corollary 3**.: _For \(\delta<\nicefrac{{1}}{{2}}\), the \(1\)-Median estimator is \((1+\frac{4\delta}{1-2\delta},\delta)\)-approximation-robust for \(\operatorname{med}_{1}\)._

Recall Example 1 from Section 1. In this example, no choice of \(k=2\) centers simultaneously achieves a good (bounded) approximation for the \(2\)-medians problem on both \(X\) and \(X^{\prime}\). Given this negative example, we turn to a _balanced_ version of \(k\)-medians for which we are able to give a robustness guarantee. The following theorem shows that computing the best "slightly less balanced" solution on the predictions has a cost (on the original dataset \(X\)) that is close to the optimal one.

**Theorem 4**.: _Let \(b>2k+2\). Consider the algorithm \(\mathcal{B}\) that computes the optimal \((b-1)\delta\)-balanced \(k\)-medians solution on its input. For any instance \(X\in V^{n}\) of the \(b\delta\)-balanced \(k\)-medians problem with optimal solution \(G\), let \(X^{\prime}\in V^{n}\) belong to the \(\delta\)-neighborhood of \(X\). The algorithm \(\mathcal{B}\), when given \(X^{\prime}\), returns a solution \(H\) such that_

\[d_{H}(G,H)\leq\frac{2k}{\delta|X|\cdot(b-2-2k)}\cdot\sum_{x\in X}d(x,G).\]

_Moreover, the \(k\)-medians cost \(\operatorname{med}_{k}(X,H)\leq(1+\frac{4k}{b-2-2k})\operatorname{med}_{k}(X,G)\), and \(H\) induces a \((b-2)\delta\)-balanced partition \(\mathscr{C}_{H}\) of \(X\)._

For a fixed value of \(k\), this theorem gives a solution \(H\) that is \((b-2)\delta\)-balanced and also \((1+O(\nicefrac{{1}}{{b}}),\delta)\)-approximation-robust with respect to the best \(b\delta\)-balanced solution.

Missing proofs are provided in Appendix E. The intuition behind the proofs in this section is that since most points (\(1-\delta\)) are shared between the actual instances and the predictions, any cost-minimizing solution for the predictions will be close (in terms of Hausdorff distance) to the cost-minimizing solution for the real points, thus resulting in a similar cost as implied by Lemma 10 in Appendix E.

## 6 Mechanism Design for Facility Location with Mac Predictions

### Warmup: Deterministic Mechanism Design for Facility Location

In this section we show how to design deterministic mechanisms for facility location problems that utilize the Mac predictions to get better approximation guarantees. We do so by utilizing the \(\delta\)-robustness results from Section 5. In the mechanism design setting, there are \(n\) agents with (true) locations \(X=\{x_{i}\mid i\in[n]\}\in\mathbb{R}^{d}\), where \(d\) is the dimension of the facility location problem. The mechanism has access to location reports by the strategic agents, and also to \(X^{\prime}=\{x^{\prime}_{1},\ldots,x^{\prime}_{n}\}\), which are the \(\textsc{Mac}(\varepsilon,\delta)\) predictions of the true locations \(X\). The mechanism only has access to the reported locations (\(\tilde{X}\)) and the predictions (\(X^{\prime}\)). For any strategyproof mechanism, the reported locations will be the same as real ones (\(X=\tilde{X}\)) as illustrated in Fig. 2.

As explained in Appendix C, for the sake of the analysis we assume that \(\varepsilon=0\). All proofs can be found at Appendix F.

**For the single facility location problem \((k=1)\):** In this setting we assume \(\delta\) (or an upper bound value) is known and thus we can simply check in advance if \(1+\frac{4\delta}{1-2\delta}\leq\sqrt{d}\) or not, and use the algorithm that guarantees us the best approximation ratio.

``` if\((1+\frac{4\delta}{1-2\delta}\leq\sqrt{d})\) then return Geometric-Median\((X^{\prime})\) else return Coordinate-Wise-Median\((X)\); ```

**Algorithm 1**Best-Choice-Single-Facility-Loc\((X,X^{\prime},\delta)\)

See Appendix F.2 for a clarification on the computation of the geometric median.

**Theorem 5**.: _Algorithm 1 is strategyproof and gets an approximation ratio of \(min\Big{(}1+\frac{4\delta}{1-2\delta},\sqrt{d}\Big{)}\)._

**Supporting the weaker setting of high probability (only w.p. 1 - \(o(\nicefrac{{1}}{{n}})\)) Mac predictions**: In Appendix F.3 we show how to achieve the same approximation ratio as in Theorem 5.

**For the \(k\) facility location problem:** In a similar way to the single facility, we can also get a deterministic mechanism for the \(k\) facility location problem with predictions, under some balancedness condition (see Appendix G). The balancedness condition roughly translates to a minimum cluster size in the optimal solution. The resulting approximation ratio is bounded by \(1+\frac{4k}{b-2-2k}\) where \(b\) is a constant that depends on the minimum cluster size and \(\delta\).

### Randomized Mechanism Design for \(2\)-Facility Location on a Line

We now consider the \(2\)-facility-location problem, where the metric is given by the real line \(\mathbb{R}\). Formally, our metric is over the point set \(V=\mathbb{R}\), with \(d(x,y)=|x-y|\). There are \(n\) agents with locations on the real line \(X=\{x_{1},\ldots,x_{n}\}\), and the cost of a solution \(G\in V^{2}\) is \(\operatorname{med}_{k}(X,G)=\sum_{x_{i}\in X}d(x_{i},G)\). In the mechanism-design setting, the agent locations \(X\) are unknown, but we are given (a) Mac predictions \(X^{\prime}\) for these locations, as well as (b) the agents' reports for their locations. Unlike in the \(\beta\)-balanced problem variation, we do not assume the balancedness of the optimal solution, but handle the unconstrained case where the optimal solution might induce either a balanced or an unbalanced clustering of \(X\). In this section, we present a strategy-proof random mechanism with a small (expected) approximation ratio.

Before doing so, we solve two other problems. After we solve those in Sections 6.2.1 and 6.2.2, we use the solutions in our mechanism for the \(2\)-facility-location on a line problem in Section 6.2.3.

#### 6.2.1 The Second Facility Location Problem

First, we define another independent mechanism design problem (the _second facility location_) for which we show a randomized mechanism. We later use to solve the \(2\)-facility-location problem.

**Definition 11** (Second Facility Location).: _Given a metric space \((V,d)\), a dataset \(X\in V^{n}\), and a single facility \(h_{S}\in V\), the goal is to find another facility \(h_{T}\) to minimize the \(2\)-medians cost \(\operatorname{med}_{2}(X,\{h_{S},h_{T}\})\)._

Figure 2: There are three sets of points: The real locations \(X\) (black), the reported locations \(\tilde{X}\) (yellow), and the predicted locations \(X^{\prime}\)(blue). The algorithm can only access \(\tilde{X},X^{\prime}\). For any strategyproof mechanism, the yellow and black points overlap.

We consider the following Second-Proportional-Mechanism, which is the second step of the random mechanism for the 2 facility location on the line problem proposed by Lu et al. (2010).

\(a_{i}\gets d(x_{i},h_{S})\);

Pick \(h\in X\) such that \(Pr[h=x_{i}]=\nicefrac{{a_{i}}}{{\sum_{j\in[n]}a_{j}}}\);

**Lemma 6**.: Second-Proportional-Mechanism _is strategyproof for any metric space._

**Theorem 7** (Second Facility Results).: _Let \(V=\mathbb{R}\). Fix any dataset \(X\) and first facility \(\{g_{S}\}\). For any second facility \(g_{T}^{*}\in V\); let \((S,T)\) be a partition of the dataset induced by \(F^{*}=\{g_{S},g_{T}^{*}\}\). The expected cost of Second-Proportional-Mechanism given \(X\) and \(g_{S}\) is:_

\[\mathbb{E}[\operatorname{med}_{2}(X,\{g_{S},g_{T}\})]\leq 2\operatorname{med }_{1}(S,\{g_{S}\})+3\operatorname{med}_{1}(T,\{g_{T}^{*}\}),\] (3)

_where \(g_{T}\) is the second facility location chosen by Second-Proportional-Mechanism._

We defer the proofs of Lemma 6 and Theorem 7 to Appendix I. See Remark 29 for a tight example.

The algorithm prioritizes farther locations (belonging to agents who pay more if their location is not chosen) over agents that are near the first facility. The intuition for the strategyproofness is that if the agent deviates to a different location, it might be chosen with a higher probability, but the agent will also pay more. The largest technical difference in the analysis from Lu et al. (2010) is our introducing of a more delicate analysis for the line metric. We stress, however, that the better approximation ratio of \(3\) (rather than the ratio of \(4\) of Lu et al. (2010)) is due to the different nature of the problem (rather then the more delicate analysis) as both algorithms are tight for the line metric. One intuitive explanation for why the similar approach works better for the second facility problem is that the first facility is fixed and so we do not have to pay the cost of "guessing it" as done in Lu et al. (2010).

#### 6.2.2 The \(\delta\)-Robustness of the Big Cluster Center

Our randomized mechanism uses the predictions to estimate one facility, and randomly chooses the second facility. For this purpose, we come up with an algorithm to "estimate" the first facility location. We formally define what it means to "estimate" one of the two facilities, and then we quantify the distance and approximation robustness of the estimator. This is the last piece of the puzzle that we need for our randomized mechanism. The idea here is that even though Example 1 shows we cannot simply compute the optimal solution, most of the predictions are correct and contain useful information: We can still get a good estimation for one of the two centers of the optimal solution. In this section, we show an algorithm (location estimator), Big-Cluster-Center, that "estimates" of the center of the bigger4 cluster center out of the two clusters induced by \(2\)-Medians.

Footnote 4: For any clustering induced by any \(2\) points in space, there is always one bigger cluster (that is, the cluster that contains at least half the points).

**Definition 12** (Big-Cluster-Cost).: _For any \(X=\{x_{1},\ldots,x_{n}\}\in V^{n}\), let \(F\) be its optimal \(2\)-median, and let \(\mathscr{C}=\mathscr{C}(X,F)\) be the induced clustering. Let Big-Cluster\((X)\) be the cluster in clustering \(\mathscr{C}\) with at least \(\nicefrac{{|X|}}{{2}}\) points (if both clusters have the same size \(\textsc{Big-Cluster}(X)\) can be the one containing \(x_{1}\)). For \(t\in V\), define_

\[\textsc{Big-Cluster-Cost}(X,t):=\operatorname{med}_{1}(\textsc{Big-Cluster}( X),t).\]

**Definition 13** (Big Cluster Location Problem).: _Given \(n\) points \(X\in V^{n}\), the objective is to find a point \(t\in V\) to minimize the cost function Big-Cluster-Cost\((X,t)\)._

We specifically focus on the line metric, and only consider \(X\in\mathbb{R}^{n}\) instances where the clustering induced by \(2\)-Medians\((X)\) are \(b\delta\)-unbalanced for some (some constants) \(b>1\), \(\delta\in[0,\nicefrac{{1}}{{2}})\).

\((h_{L},h_{R})=(b-1)\delta\)-Balanced \(2\)-Median\((X)\) ;

\(L^{\prime}=\{x_{i}\in X\mid d(x_{i},h_{L})\leq d(x_{i},h_{R})\}\);

\(R^{\prime}=X\setminus L^{\prime}\);

Return \(h_{L}\) if \(|L^{\prime}|\geq|R^{\prime}|\) and \(h_{R}\) otherwise;

**Theorem 8**.: _Let \(\mathscr{Y}\) be the collection of all datasets in \(\mathbb{R}\) for which the optimal \(2\)-medians induces no \(b\delta\)-balanced clusterings. Then for a small enough (constant) \(\delta\), Algorithm Big-Cluster-Center is \((1.8+O(\delta),\delta)\)-approximation-robust for the cost function Big-Cluster-Cost restricted to instances in \(\mathscr{Y}\)._

The intuition for the proof of Theorem 8 is that one cluster has a big portion of the points, and most of them also appear in the predictions. Thus, in the cost minimizing solution computed on the predictions, there must be a big center "near" these points (since any solution far away from these points would result in a higher cost). The analysis is close to tight (see Remark 25 in Appendix H).

#### 6.2.3 The \(2\)-Facility Location Algorithm

Our randomized algorithm below first uses the Big-Cluster-Center procedure on the predictions to approximate the "big" cluster center, and then uses the Second-Proportional-Mechanism to approximate the second cluster center using only the agents' reports. (We set \(b\in\mathbb{R}_{>1}\) such that the approximation robustness in Theorem 4 equals 1.2.) The proof is at Appendix J.

\(h_{1}=\)Big-Cluster-Center\((X^{\prime},b,\delta)\);

\(h_{2}=\)Second-Proportional-Mechanism\((X,h_{1})\);

Return \(H=(h_{1},h_{2})\);

**Theorem 9**.: _For a small enough (but still constant) \(\delta\): Algorithm 4 is strategyproof and has an expected approximation ratio of \(3.6+O(\delta)\)._

The proof uses most of the tools we came up with so far, and it considers two main scenarios: when clustering is \(b\delta\)-balanced and when it is unbalanced. In both cases, by leveraging the \(\delta\)-robustness of 1-median, 2-medians, big-cluster-center and the (randomized) strategic placement of the second facility, the mechanism ensures that the solution approaches the optimal cost.

## 7 Conclusion and Future Directions

We explore strategyproof facility location within our introduced Mac predictions error model. Our model integrates the trust level into the error model, handles outliers, and leads to new mechanism designs (that do not seem to be an interpolation between completely trusting the predictions and resorting to a "no-predictions" method). To analyze our designs, we utilize distance and approximation robustness notions established in Section 5.

An immediate question arising from our results is whether there is a _deterministic_ mechanism with Mac predictions for \(2\)-facility location on a line that yields a constant approximation. Another avenue for future research is improving the approximation ratio by randomized mechanisms for this problem. One promising strategy entails refining the selection process for the first facility in Algorithm 4, which can lead to a superior approximation ratio. Our current mechanisms treat the multi-sets of predictions and reported values as independent entities; an intriguing direction for further investigation is to devise mechanisms that capitalize on the matching between predictions and agent-reported values, potentially yielding a more accurate approximation ratio. Lastly, exploring the application of the Mac model to problems beyond mechanism design for facility location may yield insights into the efficacy of such predictions in diverse contexts.

## Acknowledgements

We are grateful to Batya Berzack for comments on an earlier draft of the paper, and to Haim Kaplan, Michal Feldman, Amos Fiat and Yossi Azar for discussions about a previous version of the model. We would like to thank the NeurIPS 2024 anonymous reviewers for their comments on this paper. This work is funded by the European Union (ERC grant No. 101077862, project: ALGORCNTACT, PI: Inbal Talgam-Cohen), a Google Research Scholar award, the Israel Science Foundation (ISF grant No. 3331/24), the Binational Science Foundation (BSF grant No. 2021680), and the National Science Foundation (NSF grant Nos. CCF-1955785 and CCF-2006953).

## References

* Aggarwal et al. (2010) Gagan Aggarwal, Rina Panigrahy, Tomas Feder, Dilys Thomas, Krishnaram Kenthapadi, Samir Khuller, and An Zhu. 2010. Achieving anonymity via clustering. _ACM Transactions on Algorithms (TALG)_ 6, 3 (2010), 1-19.
* Agrawal et al. (2023) Akanksha Agrawal, Tanmay Inamdar, Saket Saurabh, and Jie Xue. 2023. Clustering what matters: Optimal approximation for clustering with outliers. _Journal of Artificial Intelligence Research_ 78 (2023), 143-166.
* Agrawal et al. (2022) Priyank Agrawal, Eric Balkanski, Vasilis Gkatzelis, Tingting Ou, and Xizhi Tan. 2022. Learning-augmented mechanism design: Leveraging predictions for facility location. In _Proceedings of the 23rd ACM Conference on Economics and Computation_. 497-528.
* Almanza et al. (2021) Matteo Almanza, Flavio Chierichetti, Silvio Lattanzi, Alessandro Panconesi, and Giuseppe Re. 2021. Online facility location with multiple advice. _Advances in Neural Information Processing Systems_ 34 (2021), 4661-4673.
* Amir et al. (2020) Idan Amir, Idan Attias, Tomer Koren, Yishay Mansour, and Roi Livni. 2020. Prediction with corrupted expert advice. _Advances in Neural Information Processing Systems_ 33 (2020), 14315-14325.
* Anand et al. (2021) Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. 2021. A regression approach to learning-augmented online algorithms. _Advances in Neural Information Processing Systems_ 34 (2021), 30504-30517.
* Anand et al. (2022) Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. 2022. Online algorithms with multiple predictions. In _International Conference on Machine Learning_. PMLR, 582-598.
* Anand et al. (2020) Keerti Anand, Rong Ge, and Debmalya Panigrahi. 2020. Customizing ML predictions for online algorithms. In _International Conference on Machine Learning_. PMLR, 303-313.
* Antoniadis et al. (2020) Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. 2020. Secretary and online matching problems with machine learned advice. _Advances in Neural Information Processing Systems_ 33 (2020), 7933-7944.
* Azar et al. (2022) Yossi Azar, Debmalya Panigrahi, and Noam Touitou. 2022. Online graph algorithms with predictions. In _Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_. SIAM, 35-66.
* Azar et al. (2023) Yossi Azar, Debmalya Panigrahi, and Noam Touitou. 2023. Discrete-Smoothness in Online Algorithms with Predictions. In _Thirty-seventh Conference on Neural Information Processing Systems_. https://openreview.net/forum?id=DDMH3H78iJ
* Aziz et al. (2020) Haris Aziz, Hau Chan, Barton Lee, Bo Li, and Toby Walsh. 2020. Facility location problem with capacity constraints: Algorithmic and mechanism design perspectives. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 34. 1806-1813.
* Balcan et al. (2023) Maria-Florina Balcan, Siddharth Prasad, and Tuomas Sandholm. 2023. Bicriteria Multidimensional Mechanism Design with Side Information. _CoRR_ abs/2302.14234 (2023). https://doi.org/10.48550/ARXIV.2302.14234 arXiv:2302.14234
* Balcan and Harvey (2018) Maria-Florina Balcan and Nicholas JA Harvey. 2018. Submodular functions: Learnability, structure, and optimization. _SIAM J. Comput._ 47, 3 (2018), 703-754.
* Balkanski et al. (2023a) Eric Balkanski, Vasilis Gkatzelis, and Xizhi Tan. 2023a. Mechanism Design with Predictions: An Annotated Reading List. _SIGecom Exchanges_ 21, 1 (2023), 54-57.
* Leibniz-Zentrum fur Informatik, 11:1-11:22. https://doi.org/10.4230/LIPICS.ITCS.2023.11
Eric Balkanski, Vasilis Gkatzelis, Xizhi Tan, and Cherlin Zhu. 2023c. Online Mechanism Design with Predictions. _CoRR_ abs/2310.02879 (2023). https://doi.org/10.48550/ARXIV.2310.02879 arXiv:2310.02879
* Beck and Sabach (2015) Amir Beck and Shoham Sabach. 2015. Weiszfeld's method: Old and new results. _Journal of Optimization Theory and Applications_ 164 (2015), 1-40.
* Berger et al. (2023) Ben Berger, Michal Feldman, Vasilis Gkatzelis, and Xizhi Tan. 2023. Optimal Metric Distortion with Predictions. _CoRR_ abs/2307.07495 (2023). https://doi.org/10.48550/ARXIV.2307.07495 arXiv:2307.07495
* Bernardini et al. (2022) Giulia Bernardini, Alexander Lindermayr, Alberto Marchetti-Spaccamela, Nicole Megow, Leen Stougie, and Michelle Sweering. 2022. A universal error measure for input predictions applied to online graph problems. _Advances in Neural Information Processing Systems_ 35 (2022), 3178-3190.
* Bhattacharya et al. (2020) Anup Bhattacharya, Dishant Goyal, and Ragesh Jaiswal. 2020. Hardness of Approximation of Euclidean \(k\)-Median. _arXiv preprint arXiv:2011.04221_ (2020).
* Chan et al. (2021) Hau Chan, Aris Filos-Ratsikas, Bo Li, Minming Li, and Chenhao Wang. 2021. Mechanism Design for Facility Location Problems: A Survey. In _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21_, Zhi-Hua Zhou (Ed.). International Joint Conferences on Artificial Intelligence Organization, 4356-4365. https://doi.org/10.24963/ijcai.2021/596 Survey Track.
* Charikar et al. (2001) Moses Charikar, Samir Khuller, David M Mount, and Giri Narasimhan. 2001. Algorithms for facility location problems with outliers. In _SODA_, Vol. 1. Citeseer, 642-651.
* Cohen et al. (2016) Michael B Cohen, Yin Tat Lee, Gary Miller, Jakub Pachocki, and Aaron Sidford. 2016. Geometric median in nearly linear time. In _Proceedings of the forty-eighth annual ACM symposium on Theory of Computing_. 9-21.
* Diakonikolas et al. (2021) Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Ali Vakilian, and Nikos Zarifis. 2021. Learning online algorithms with distributional advice. In _International Conference on Machine Learning_. PMLR, 2687-2696.
* Dutting et al. (2021) Paul Dutting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. 2021. Secretaries with advice. In _Proceedings of the 22nd ACM Conference on Economics and Computation_. 409-429.
* Eckhardt (1980) Ulrich Eckhardt. 1980. Weber's problem and Weiszfeld's algorithm in general spaces. _Mathematical Programming_ 18 (1980), 186-196.
* Emek et al. (2023) Yuval Emek, Yuval Gil, Maciej Pacut, and Stefan Schmid. 2023. Online Algorithms with Randomly Infused Advice. _arXiv preprint arXiv:2302.05366_ (2023).
* Fotakis et al. (2021) Dimitris Fotakis, Evangelia Gergatsouli, Themis Gouleakis, and Nikolas Patris. 2021. Learning augmented online facility location. _arXiv preprint arXiv:2107.08277_ (2021).
* Fotakis and Tzamos (2014) Dimitris Fotakis and Christos Tzamos. 2014. On the power of deterministic mechanisms for facility location games. _ACM Transactions on Economics and Computation (TEAC)_ 2, 4 (2014), 1-37.
* Gkatzelis et al. (2022) Vasilis Gkatzelis, Kostas Kollias, Alkmini Sgouritsa, and Xizhi Tan. 2022. Improved Price of Anarchy via Predictions. In _EC '22: The 23rd ACM Conference on Economics and Computation_. 529-557.
* Goel and Hann-Caruthers (2023) Sumit Goel and Wade Hann-Caruthers. 2023. Optimality of the coordinate-wise median mechanism for strategyproof facility location in two dimensions. _Social Choice and Welfare_ 61, 1 (2023), 11-34.
* Gupta et al. (2022) Anupam Gupta, Debmalya Panigrahi, Bernardo Subercaseaux, and Kevin Sun. 2022. Augmenting Online Algorithms with epsilon Accurate Predictions. _Advances in neural information processing systems_ (2022).
* Han et al. (2020) Lu Han, Chunlin Hao, Chenchen Wu, and Zhenning Zhang. 2020. Approximation algorithms for the lower-bounded k-median and its generalizations. In _International Computing and Combinatorics Conference_. Springer, 627-639.
* Huang et al. (2021)Gabriel Istrate and Cosmin Bonchis. 2022. Mechanism Design With Predictions for Obnoxious Facility Location. _CoRR_ abs/2212.09521 (2022). https://doi.org/10.48550/ARKIV.2212.09521 arXiv:2212.09521
* Jiang et al. (2021a) Shaofeng H-C Jiang, Erzhi Liu, You Lyu, Zhihao Gavin Tang, and Yubo Zhang. 2021a. Online facility location with predictions. _arXiv preprint arXiv:2110.08840_ (2021).
* Jiang et al. (2021b) Zhihao Jiang, Pinyan Lu, Zhihao Gavin Tang, and Yuhao Zhang. 2021b. Online selection problems against constrained adversary. In _International Conference on Machine Learning_. PMLR, 5002-5012.
* Khodak et al. (2022) Misha Khodak, Maria-Florina F Balcan, Ameet Talwalkar, and Sergei Vassilvitskii. 2022. Learning predictions for algorithms with predictions. _Advances in Neural Information Processing Systems_ 35 (2022), 3542-3555.
* Krishnamurthy et al. (2020) Akshay Krishnamurthy, Thodoris Lykouris, and Chara Podimata. 2020. Corrupted Multidimensional Binary Search: Learning in the Presence of Irrational Agents. _CoRR_ abs/2002.11650 (2020). arXiv:2002.11650 https://arxiv.org/abs/2002.11650
* Krishnaswamy et al. (2018) Ravishankar Krishnaswamy, Shi Li, and Sai Sandeep. 2018. Constant approximation for k-median and k-means with outliers via iterative rounding. In _Proceedings of the 50th annual ACM SIGACT symposium on theory of computing_. 646-659.
* Lai et al. (2016) Kevin A. Lai, Anup B. Rao, and Santosh Vempala. 2016. Agnostic Estimation of Mean and Covariance. arXiv:1604.06968 [cs.DS]
* Lavastida et al. (2020) Thomas Lavastida, Benjamin Moseley, R Ravi, and Chenyang Xu. 2020. Learnable and instance-robust predictions for online matching, flows and load balancing. _arXiv preprint arXiv:2011.11743_ (2020).
* Lopuhaa and Rousseeuw (1991) Hendrik P Lopuhaa and Peter J Rousseeuw. 1991. Breakdown points of affine equivariant estimators of multivariate location and covariance matrices. _The Annals of Statistics_ (1991), 229-248.
* Lu et al. (2010) Pinyan Lu, Xiaorui Sun, Yajun Wang, and Zeyuan Allen Zhu. 2010. Asymptotically optimal strategy-proof mechanisms for two-facility games. In _Proceedings of the 11th ACM conference on Electronic commerce_. 315-324.
* Lu et al. (2009) Pinyan Lu, Yajun Wang, and Yuan Zhou. 2009. Tighter bounds for facility games. In _Internet and Network Economics: 5th International Workshop, WINE 2009, Rome, Italy, December 14-18, 2009. Proceedings 5_. Springer, 137-148.
* Lykouris et al. (2018) Thodoris Lykouris, Vahab S. Mirrokni, and Renato Paes Leme. 2018. Stochastic bandits robust to adversarial corruptions. _CoRR_ abs/1803.09353 (2018). arXiv:1803.09353 http://arxiv.org/abs/1803.09353
* Lykouris and Vassilvitskii (2021) Thodoris Lykouris and Sergei Vassilvitskii. 2021. Competitive caching with machine learned advice. _Journal of the ACM (JACM)_ 68, 4 (2021), 1-25.
* Megiddo and Supowit (1984) Nimrod Megiddo and Kenneth J Supowit. 1984. On the complexity of some common geometric location problems. _SIAM journal on computing_ 13, 1 (1984), 182-196.
* Meir (2019) Reshef Meir. 2019. Strategyproof facility location for three agents on a circle. In _International symposium on algorithmic game theory_. Springer, 18-33.
* Mitzenmacher and Vassilvitskii (2022) Michael Mitzenmacher and Sergei Vassilvitskii. 2022. Algorithms with predictions. _Commun. ACM_ 65, 7 (2022), 33-35. https://doi.org/10.1145/3528087
* Procaccia and Tennenholtz (2013) Ariel D Procaccia and Moshe Tennenholtz. 2013. Approximate mechanism design without money. _ACM Transactions on Economics and Computation (TEAC)_ 1, 4 (2013), 1-26.
* Purohit et al. (2018) Manish Purohit, Zoya Svitkina, and Ravi Kumar. 2018. Improving online algorithms via ML predictions. _Advances in Neural Information Processing Systems_ 31 (2018).
* Purohit et al. (2018)Dhruv Rohatgi. 2020. Near-optimal bounds for online caching with machine learned advice. In _Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms_. SIAM, 1834-1845.
* Sun et al. (2023) Bo Sun, Jerry Huang, Nicolas Christianson, Mohammad Hajiesmaili, and Adam Wierman. 2023. Online Algorithms with Uncertainty-Quantified Predictions. _arXiv preprint arXiv:2310.11558_ (2023).
* Wu et al. (2022) Xiaoliang Wu, Feng Shi, Yutian Guo, Zhen Zhang, Junyu Huang, and Jianxin Wang. 2022. An approximation algorithm for lower-bounded k-median with constant factor. _Science China Information Sciences_ 65, 4 (2022), 140601.
* Xu and Lu (2022) Chenyang Xu and Pinyan Lu. 2022. Mechanism Design with Predictions. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_, Lud De Raedt (Ed.). International Joint Conferences on Artificial Intelligence Organization, 571-577. https://doi.org/10.24963/ijcai.2022/81 Main Track.
* Xu and Moseley (2022) Chenyang Xu and Benjamin Moseley. 2022. Learning-augmented algorithms for online steiner tree. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 36. 8744-8752.
* Zampetakis and Zhang (2023) Emmanouil Zampetakis and Fred Zhang. 2023. Bayesian Strategy-Proof Facility Location via Robust Estimation. In _International Conference on Artificial Intelligence and Statistics_. PMLR, 4196-4208.

### Organization of the Appendices

Appendix A contains more related work.

Appendix B contains more examples for sources of Mac predictions.

Appendix C contains the explanation for the effect of \(\varepsilon>0\).

Appendix D contains an interpretation of the established robust statistics \(\delta\)-robustness results.

Appendix E contains the proofs for Section 5.

Appendix F and Appendix G contain proofs and supplementary material for Section 6.1.

Appendix H, Appendix I and Appendix J contain the missing proofs for section Section 6.2 (for the analysis of Algorithm 4 - "Robust Half" mechanism). Namely:

* Appendix I contains the proof for Algorithm 2 properties: Lemma 6, Theorem 7.
* Appendix H contains the proof of Theorem 8.
* Appendix J contains the proof of Theorem 9.

Additional Related Work

Models of prediction accuracy.Azar et al. (2022) study online graph algorithms with a notion of error they call _metric error with outliers_, applied to problems where the predictions are locations in a metric space. The error parameters of this model are \(D\) and \(\Delta\)--roughly, \(D\) is the minimum cost matching between the predictions and the actual locations, where \(\Delta\) outliers may be excluded from the matching. These parameters are similar to the \((\varepsilon,\delta)\) parameters of the Mac model. They then give a general framework for solving online graph problems in their model. Interestingly, their matching cost \(D\) captures a notion of average error, whereas we focus on individual error; while our results can be extended to fit within their model, it would be interesting to investigate a version of Mac where \(\varepsilon\) captures some notion of average error.

Gkatzelis et al. (2022) generalize the outlier model of Azar et al. (2022) and apply it to improve the price of anarchy of cost-sharing mechanisms. Xu and Moseley (2022) study online Steiner tree problems with predictions where the error parameter is the number of erroneous predictions. Bernardini et al. (2022) introduce a model named the "cover error model" that generalizes the model of Azar et al. (2022) to achieve more precision for online problems.

Gupta et al. (2022) study predictions that are incorrect independently with probability \(\delta\). Their work focuses on online algorithms rather than mechanism design. Our model is related to their model since independent errors imply Mac predictions w.h.p. by Chernoff. The Mac model is more general in the sense that it does not assume independence of erroneous predictions. Emek et al. (2023) also consider a similar model to the one of Gupta et al. (2022), assuming access to predictions that are correct with some probability or contain random bits otherwise. Dutting et al. (2021) consider a signaling scheme advice model for the secretary, which for binary ML-advice roughly translates into measures of accuracy of the signals (named recall and specificity).

Jiang et al. (2021) consider a prediction guaranteed to be inside a prediction interval. While their error model is closer to the "worst case" error model, their motivation is confidence intervals from statistics, where it is reasonable to assume that a confidence interval is achieved with probability \(1-\delta\) for some \(\delta\in[0,1]\). Sun et al. (2023) consider an approach for "uncertainty quantified" prediction. One class of predictions they focus on, the "probabilistic interval prediction", can also be seen as a confidence interval, as in the motivation of Jiang et al. (2021). Like our Mac model, they "model in" the amount of trust in the advice. Sun et al. (2023) focus on settings where the algorithm receives a single prediction (such as the ski rental problem). They use online learning for tuning the trust parameters, and introduce a regret minimization analysis for learning the trust parameter online (over many instances of the same problem). The Mac model addresses problems that necessitate the quantification of uncertainty for multiple predictions simultaneously within the same input instance, such as in the \(k\)-facility location problem.

Mechanism design for facility location problems without predictions.**

Mechanism design for the single facility location problem.In the single facility location in \(\mathbb{R}^{d}\) problem, the task is to return a single facility minimizing the social cost function. For the case of no-predictions, Meir (2019) shows a deterministic strategyproof mechanism that gets an approximation ratio of \(\sqrt{d}\) which is optimal for \(d=2\)(Goel and Hann-Caruthers (2023)). The optimal deterministic strategyproof mechanism is computing the Coordinate-Wise-Median of the points. In short, the \(\sqrt{d}\) ratio follows from the fact that Coordinate-Wise-Median is the optimal solution is the optimal solution w.r.t the \(l1\) norm, which is at most \(sqrt{d}(d)\) times \(l2\) norm. The goal of introducing predictions is to get something better than \(\sqrt{d}\) in this context.

Mechanism design for the two facility location on the line.In this variant the goal is to return two facilities to minimize the social cost, where all points lie on the real line \(\mathbb{R}\). We want to find a strategyproof mechanism that given the agent reported locations returns the two locations for facilities in \(\mathbb{R}\), s.t. the social cost is minimal. A deterministic \(n-2\) approximation ratio mechanism (called the "Two Extremes" mechanism) was given by Procaccia and Tennenholtz (2013), and a lower bound of \(\nicefrac{{n}}{{2}}-1\) was given by Lu et al. (2010) which was later improved to (the tight) lower bound of \(n-2\) by Fotakis and Tzamos (2014). A randomized strategyproof mechanism with an expected approximation ratio of \(4\) was given by Lu et al. (2010) (which also works for any metric space), while the currently best known lower bound for random mechanisms (Lu et al. (2009)) is \(1.045\).

\(k\)-facility location. For \(k>2\), Fotakis and Tzamos (2014) show that there is no deterministic anonymous strategyproof mechanism with a bounded approximation ratio for \(k\)-facility location on the line for the general case (not just balanced) for any \(k\geq 3\), even for simple instances with \(k+1\) agents. Moreover, they show that there do not exist any deterministic strategyproof mechanisms with a bounded approximation ratio for 2-Facility Location on more general metric spaces, which is true even for simple instances with 3 agents located in a star.

_Mechanism design for the capacitated facility location_ is a variant of the problem studied by Aziz et al. (2020) where each facility has a maximum capacity, limiting the number of points that can be assigned to it. For the utilitarian cost function, they have shown a \(\nicefrac{{n}}{{2}}-1\) approximation ratio.

**Robust \(k\) medians and facility location.** The robustness of the (offline non-strategic) \(k\) medians and \(k\) facility location has been studied under different variations. As Example 1 demonstrates, it is not possible to get any bounded approximation ratio for the optimal solution. The approach of Charikar et al. (2001) is to look at different variants of the problems with less restrictive objectives. In one variant they consider, the problem is to place facilities so as to minimize the service cost to any subset of facilities of size at least \(p\) for some parameter \(p\). Another variant they consider allows denial of service for some of the clients with the additional cost of some penalty for each such denied client. For work in these kind of models see Krishnaswamy et al. (2018); Agrawal et al. (2023).

**Other models of advice/predictions.** In the recent literature of algorithms with predictions there are existing other models for algorithms with predictions which are different from the models we already discussed. We now mention some of them.

Some other ML advice is to focus on values that are learned via classical PAC learnability and focus on the learnability and analysis of sample complexity bounds. The assumption is that there is some distribution generating the input for the algorithm. Then the approach is to learn the distribution in terms of classic supervised learning. This kind of modeling is detailed in Anand et al. (2020), Lavastida et al. (2020), Anand et al. (2021).

Diakonikolas et al. (2021) assumes that the advice is not given deterministically, but that they can access the distributions of the inputs and sample from these: they consider access to the instances of interest, and the goal is to learn a competitive algorithm given access to i.i.d. samples. They provide sample complexity bounds for the underlying learning tasks.

Online variants of the facility location problem with different notions of prediction were studied e.g. by Almanza et al. (2021); Fotakis et al. (2021); Jiang et al. (2021); Azar et al. (2022); Gupta et al. (2022); Azar et al. (2023); Anand et al. (2022). The online setting of the problem differs significantly from our mechanism design setting: the real points arrive in sequence where each time an irrevocable decision must be made by the online algorithm (unlike the mechanism design setting where all the input is given to the mechanism at once, but is reported by strategic agents).

**Learning the trust parameter.** In the context of online algorithms with prediction, Khodak et al. (2022), show that the confidence parameter can be estimated under certain conditions via online learning. Sun et al. (2023) also have online learning analysis to estimate their different type of confidence parameter.

**Other algorithmic game theory with prediction work**Istrate and Bonchis (2022) study mechanism design with predictions for obnoxious facility location where agents prefer to be as far as possible from the facilities. Research in algorithmic game theory incorporating predictions, beyond the previously discussed work, has been explored by Balkanski et al. (2023, 2023); Berger et al. (2023); Balcan et al. (2023).

**Models outside of algorithms with predictions literature.** One way to view the Mac model is to view the predictions as data with with corruptions. Designing algorithms to try and handle the corruptions was studied before. Multi-Arm-Bandit with corruptions is such a setting (Lykouris et al. (2018); Krishnamurthy et al. (2020); Amir et al. (2020)).

Zampetakis and Zhang (2023) propose the use of robust statistical estimators for strategyproof mechanism design in a _Bayesian_ setting. They show how to use a location estimator which is robust to corrupting \(\delta\) fraction of the data drawn from some known distribution, to get a strategyproof mechanism for the same location problem. This is conceptually very similar to our result for the single facility case. Since there is no robust estimator for the two-facility problem, their approach cannot apply without making Bayesian assumptions; nonetheless, in our work we show how to use predictions and agent reports to get improvements on the worst-case approximation guarantees.

## Appendix B Mac Prediction Sources

The MAC prediction model is suitable for capturing errors from a variety of sources, mainly:

* _Machine learning_. In practice, useful ML models produce predictions which are mostly accurate, and the accuracy parameters \(\epsilon,\delta\) can be estimated through testing. For example, we often have a predictor with the guarantee that each prediction (independently) will be \(\varepsilon\)-correct with probability at least \(1-\delta\). Thus, through concentration bounds such as Chernoff, with high probability the number of predictions which are \(\varepsilon\)-incorrect is at most \(\approx\delta n\). This was the motivation in Balcan and Harvey (2018). Gupta et al. (2022) also consider independent predictions, each correct w.p. \(\delta\). Note that unlike this example, the Mac model does not require that the predictor's errors are independent (and in that sense is a generalization, having a weaker assumption).
* _Data with corruptions_. In a setting in which the algorithm's input itself might contain errors, we can treat the corrupted input as a prediction of the true input and apply the Mac model. Corruptions can be a result of adversarial changes to the algorithm's input, e.g., in a malicious attempt to affect the output -- in which case the adversary may change only a \(\delta\)-fraction of the input to avoid getting caught. Corruptions can also capture _outliers_ in the data, e.g., as a result of rare but arbitrary measurement errors. This was the motivation in Azar et al. (2022). As another example, the input data may have been collected at a certain point in time, while the ground truth continues to evolve -- in the context of facility location, a population census may form the prediction, with errors occurring since a small fraction of the population has relocated since being surveyed.
* _Experts advice_. The data can also originate from expert advice, where experts are usually correct, but can be significantly inaccurate when they are wrong.

## Appendix C Supporting Small \(\varepsilon>0\) Values

Throughout the analysis we assume that \(\varepsilon=0\), meaning each prediction is either wrong, or exactly correct. As we predict values in space (such as real values) it is hard to expect exact precision in the predictions. Thus, we can weaken this assumption by allowing a prediction to be "correct" if it is \(\varepsilon\)-accurate for some small \(\varepsilon>0\) value. Consider, for example, the scenario where we want to locate a hospital close to where people live where the locations are the house addresses. An "approximately correct" prediction that equals to the location of the house next door to the actual house of a person would be a reasonable prediction but would result in a small \(\varepsilon>0\). The prediction (and the cost) is almost the same, but it is not exactly right.

Consider, for example, the case of the single facility location problem. The effect of \(\varepsilon\) on the approximation ratio is bounded by an additive term of at most \(\varepsilon n\), simply because each of the \(n\) points "contributes" another additive \(\varepsilon\) term. If \(\varepsilon n\) is small in comparison to \(OPT\), or equivalently, if \(\varepsilon\) is small compared to \(\nicefrac{{OPT}}{{n}}\) then the effect on the approximation ratio will be small; also, it simply means that every agent pays at most \(\varepsilon\) more on average. On the other hand, if \(\varepsilon\) is big in comparison to \(\nicefrac{{OPT}}{{n}}\), then we have no chance to compute a good solution. Indeed, \(\nicefrac{{OPT}}{{n}}\) is the _average deviation_ of the points from their true geometric median, which can be viewed as a notion of the variance of the input; this is a measure of the "scale" of the cost of the problem, and a noise level above this value swamps the signal in the data.

We drop the \(\varepsilon\) from the calculations to avoid "dragging" the additive \(\varepsilon\)-dependent term along each result.

## Appendix D Robust Statistics and Clustering Results Interpretation

In Section 2.3 we've mentioned how the \(\delta\)-robustness notion generalizes the breakdown point notion from robust statistics. The existing literature does have a quantitative approach, but mostly for specific distributions or for distributions under some constraints, and mostly for mean and covariance robust estimation. For example, Lai et al. (2016) shows that the distance between the mean of a Gaussian distribution in \(\mathbb{R}^{d}\) from the geometric median is \(\Omega(\delta\sqrt{d})\). In section Section 5 we give the definitions of \(\delta\)-robustness, to consider the robust estimation of general location estimators (such as the mean, median and their generalizations) for data chosen adversarially (thus fitting any distribution, with no further assumption). Then, we use these to study the robustness of the median and its generalizations to adversarial corruptions.

In Appendix E.1, we show that \(1\)-\(\mathrm{Median}\) (also known as the geometric median) has a distance robustness of \(O_{\delta}(1)\cdot\mathrm{MAD}(X)\) where \(O_{\delta}(1)\) is a small \(\delta\)-dependent constant, and \(MAD\) stands for Mean Average Deviation, a known notion of variance defined as \(\mathrm{MAD}(X):=\frac{1}{n}\sum_{x_{i}\in X}d(x_{i},\textsc{Geometric-Median}(X))\). An interpretation of our result is that given a small \(\delta\), the robustness of the geometric median depends on the \(\mathrm{MAD}\) of the points, and the quantification of this relation is given by Theorem 2 and Corollary 3. When we calculate the (geometric) median we often want a single point representing the dataset. The geometric median represents the dataset in the sense that it is the point closest to the dataset points. Intuitively, for data with high \(\mathrm{MAD}\) (or variance), there is no single point that represents it in a good way. Consider the examples in Fig. 3:

On the left example (high \(\mathrm{MAD}\) example) half the points at \(0\) and half the points at \(1\): no single point can represent the data in a good way -- any point returned would misrepresent half the data. In the second example (low \(\mathrm{MAD}\) example) all points are co-located at \(0\). In this case \(\mathrm{MAD}=0\) and so the geometric median (at \(0\)) represents the dataset in the best possible way -- it equals to every point in the dataset. So the interpretation of the results here is that the geometric median is robust in the sense that it still returns a good representation of the dataset: a point that gets a representation of the dataset, which is almost as good as the real "best" representation of the dataset.

In Appendix E.2, we show that for \(k>1\) there is no hope for robustness of \(k\)-medians. We thus turn to \(\beta\)-balanced \(k\)-medians, which are \(k\) medians that induce clusters of size at least \(\beta n\), where \(n\) is the total number of points (see Definition 7). Clustering with the additional constraint of minimum cluster size has applications such as data privacy (Aggarwal et al. (2010)), or clustering with a minimum/maximum cluster size (capacitated variation). The robustness of \(\beta\)-balanced \(k\)-medians implies that balanced clustering algorithms can be robust to adversarial corruptions, outliers or missing values.

In our context, we show the usefulness of these \(\delta\)-robustness results for all of the mechanisms for the Mac predictions settings.

## Appendix E \(\delta\)-Robustness of the Median and Its Generalizations: Proofs and More Details

First, we show a lemma connecting the distance robustness to the approximation robustness notions, in the context of \(\mathrm{k}\)-\(\mathrm{Medians}\).

**Lemma 10**.: _Consider location estimators \(f,\widehat{f}:V^{n}\to V^{k}\) that satisfy the following two properties:_

1. _For any_ \(X\in V^{n}\) _and_ \(X^{\prime}\) _in the_ \(\delta|X|\)_-neighborhood of_ \(X\)_,_ \[\mathrm{med}_{k}(X^{\prime},\widehat{f}(X^{\prime}))\leq\mathrm{med}_{k}(X^{ \prime},f(X));\]
2. \(\widehat{f}\) _is_ \((\rho,\delta)\)_-distance-robust with respect to_ \(f\)_._

_Then \(\widehat{f}\) is a \(\left(1+\frac{2\delta|X|\rho}{\mathrm{med}_{k}(X,f(X))},\delta\right)\)-approximation-robust solution for \(F=\mathrm{med}_{k}\) with respect to \(f\)._

Figure 3: Illustration of instances with different \(\mathrm{MAD}\); In the LHS example, \(\mathrm{MAD}\) is very high unlike RHS where \(\mathrm{MAD}=0\).

Proof.: Let \(n=|X|\), let \(A:=\{i\in[n]\mid x_{i}=x_{i}^{\prime}\}\) be the indices where \(X,X^{\prime}\) are the same, and \(B:=[n]\setminus A\) be the remaining indices. Define \(G:=f(X)\) and \(H:=\widehat{f}(X^{\prime})\). Using this notation we can write \(\operatorname{med}_{k}(X,\widehat{f}(X^{\prime}))\) as

\[\sum_{i}d(x_{i},H) =\sum_{i}d(x_{i}^{\prime},H)+\sum_{i\in B}(d(x_{i},H)-d(x_{i}^{ \prime},H))\] \[\leq\sum_{i}d(x_{i}^{\prime},G)+\sum_{i\in B}(d(x_{i},H)-d(x_{i}^{ \prime},H))\] (4) \[=\sum_{i}d(x_{i},G)+\sum_{i\in B}(d(x_{i}^{\prime},G)-d(x_{i},G))+ \sum_{i\in B}(d(x_{i},H)-d(x_{i}^{\prime},H))\] \[=\operatorname{med}_{k}(X,f(X))+\sum_{i\in B}(d(x_{i}^{\prime},G) -d(x_{i}^{\prime},H))+\sum_{i\in B}(d(x_{i},H)-d(x_{i},G)),\] (5)

where (4) uses the first assumption of the theorem.

We claim that each of the differences in Eq. (5) is at most \(d_{H}(G,H)\). Indeed, for any \(x\in V\), let its closest points in \(G\) and \(H\) be \(g\) and \(h\) respectively. Then \(d(x,g)-d(x,h)\leq d(g,h)\) by the triangle inequality, and this is at most the Hausdorff distance; the case of \(d(x,h)-d(x,g)\) is identical. Therefore,

\[\operatorname{med}_{k}(X,\widehat{f}(X^{\prime}))\leq\operatorname{med}_{k}(X,f(X))+2|B|\cdot d_{H}(G,H).\]

Finally, using the definition of \((\rho,\delta)\)-distance-robustness implies that \(d_{H}(G,H)\) is at most \(\rho\). The fact that \(|B|\leq\delta|X|\) completes the proof. 

Now, we move onto proving the robustness of \(1\)-\(\operatorname{Median}\) (which is Geometric-Median for a Euclidean space).

### The \(\delta\)-Robustness of \(1\)-Median

In this section, we quantify the distance robustness (and hence approximation robustness, by Lemma 10) of the \(1\)-median location estimator. We show that changing any \(\delta\)-fraction of a dataset \(X\) where \(\delta<\nicefrac{{1}}{{2}}\) can move the \(1\)-\(\operatorname{Median}\) by only \(\frac{2}{1-2\delta}\) times the average cost of a point in the optimal solution of \(X\). Hence, this changes the total \(1\)-\(\operatorname{Median}\) cost by only a \(1+O(\frac{\delta}{1-2\delta})\) factor.

This is captured by Theorem 2, Corollary 3.

#### e.1.1 Proof of Theorem 2

Proof.: Let \(X=\{x_{1},\ldots,x_{n}\},X^{\prime}=\{x_{1}^{\prime},\ldots,x_{n}^{\prime}\} \in V^{n}\) where \(X^{\prime}\) is in the \(\delta|X|\)-neighborhood of \(X\). Let \(n=|X|\), let \(A:=\{i\in[n]\mid x_{i}=x_{i}^{\prime}\}\) be the indices where \(X,X^{\prime}\) are the same, and \(B:=[n]\setminus A\) be the remaining indices. Let \(m:=\operatorname*{arg\,min}_{v\in V}\operatorname{med}_{1}(X,\{v\})\) and \(m^{\prime}:=\operatorname*{arg\,min}_{v\in V}\operatorname{med}_{1}(X^{\prime },\{v\})\). It follows that \(\sum_{i\in[n]}d(x_{i}^{\prime},m^{\prime})\leq\sum_{i\in[n]}d(x_{i}^{\prime},m)\). Using the triangle inequality and the fact that \(x_{i}^{\prime}=x_{i}\) for all \(i\in A\), we get that

\[\sum_{i\in A}(d(m^{\prime},m)-d(m,x_{i}))+\sum_{i\in B}(d(x_{i}^{ \prime},m)-d(m^{\prime},m))\leq\sum_{i\in A}d(x_{i},m)+\sum_{i\in B}d(x_{i}^{ \prime},m)\] \[\implies d(m^{\prime},m)\cdot(|A|-|B|)\leq 2\sum_{i\in A}d(x_{i},m) \leq 2\sum_{i\in[n]}d(x_{i},m)=2\ \operatorname{med}_{1}(X).\]

Since \(|B|\leq\delta n\) and \(|A|+|B|=n\), we get \(d(m^{\prime},m)\leq\frac{2}{(1-2\delta)n}\operatorname{med}_{1}(X)\). 

The quantity \(\frac{\operatorname{med}_{1}(X)}{|X|}\) is called the _mean absolute deviation_ and can be viewed as a notion of variance. Hence, Theorem 2 essentially says that since \(\delta<\nicefrac{{1}}{{2}}\), the \(1\)-\(\operatorname{Median}\) only changes by \(O_{\delta}(1)\) (a constant depending on \(\delta\)) times the mean absolute deviation.

To get Corollary 3 we simply substituting the distance-robustness parameter of \(\rho=\frac{2}{1-2\delta}\frac{\operatorname{med}_{1}(X)}{|X|}\) into Lemma 10.

Corollary 3 says that if \(X^{\prime}\) is obtained by perturbing a \(\delta\)-fraction of the points in \(X\), then the geometric median of \(X^{\prime}\) is a \((1+\frac{4\delta}{1-2\delta})\)-approximately optimal solution for the original point set \(X\) (with respect to the \(1\)-\(\mathrm{Median}\) cost objective). This result can be viewed as a quantitative version of a classical result of Lopuhaa and Rousseeuw (1991), which shows that the _breakdown point_ for the geometric median is \(\nicefrac{{1}}{{2}}\). This gives us an understanding not only of the fraction of perturbations required to "break" the estimator (i.e., make it arbitrarily far from the real location), but also of the deterioration in the estimation before breaking it. The quality of estimation as a function of \(\delta\) is captured by our result of \(\rho=\frac{2}{1-2\delta}\frac{\mathrm{med}_{1}(X)}{|X|}\), showing that a smaller \(\delta\) corresponds to better estimation.

We conclude the discussion of \(\delta\)-robustness of the \(1\)-\(\mathrm{Median}\) with two remarks:

1. Results similar to Theorem 2 hold even for the case where \(X^{\prime}\) is obtained by adding or removing points; in fact, the approximation factors can be improved slightly (the proof is almost the same).
2. The quantitative bounds are tight; consider the example where \(X\) comprises of \((0.5-\delta)n+1\) points at \(0\), and \((0.5+\delta)n-1\) points at \(1\) and \(X^{\prime}\) is obtained from \(X\) by moving \(\delta n\) points from \(1\) to \(0\). A calculation shows that in this case the approximation robustness of the geometric median is \((1+\frac{4\delta}{1-2\delta}-\Theta(\frac{1}{n}),\delta)\).

### The \(\delta\)-Robustness of \(\beta\)-Balanced \(k\)-Medians

Proof of Theorem 4:

Proof.: The claimed algorithm \(\mathcal{B}\) is simple: compute the optimal \((b-1)\delta\)-balanced \(k\)-medians solution on \(X^{\prime}\). Since \(X\) admits a \(b\delta\)-balanced solution \(G=\{g_{1},\dots,g_{k}\}\), and \(X^{\prime}\) differs from \(X\) in only \(\delta|X|\) points, this solution \(G\) is \((b-1)\delta\)-balanced for \(X^{\prime}\). Now let \(H=\{h_{1},\dots,h_{k}\}\) be a \((b-1)\delta\)-balanced \(k\)-medians solution of least cost for \(X^{\prime}\) computed by \(\mathcal{B}\): this means

\[\sum_{x^{\prime}\in X^{\prime}}d(x^{\prime},H)\leq\sum_{x^{\prime}\in X^{ \prime}}d(x^{\prime},G).\]

Let \(\mathscr{C}^{\prime}_{H}=\{C^{\prime}_{1},\dots,C^{\prime}_{k}\}\) be the balanced partition of \(X^{\prime}\) induced by \(H\). Similarly, since \(X\) is in the \(\delta\)-neighborhood of \(X^{\prime}\), the centers in \(H\) induce a \((b-2)\delta\)-balanced partition of the original dataset \(X\); call this \(\mathscr{C}_{H}=\{C_{1},\dots,C_{k}\}\). Finally, let \(\mathscr{C}_{G}=\{C^{*}_{1},\dots,C^{*}_{k}\}\) be the \(b\delta\)-balanced partition of \(X\) induced by \(G\). Recall that \(n=|X|\); define

\[OPT :=\mathrm{med}_{k}(X,G)=\sum_{x\in X}d(x,G)=\sum_{i=1}^{k}\sum_{ x\in C^{*}_{i}}d(x,g_{i})\quad\text{and}\] \[ALG :=\mathrm{med}_{k}(X,H)=\sum_{x\in X}d(x,H)=\sum_{j=1}^{k}\sum_{ x\in C_{j}}d(x,h_{j}).\]

We now want to prove that \(\max_{i}d(g_{i},H)\) and \(\max_{j}d(h_{j},G)\) are both small. To show the former, fix any \(i\) and let \(C^{*}_{i}\) be the cluster corresponding to \(g_{i}\). Since the clustering is \(b\delta\)-balanced, \(|C^{*}_{i}|\geq b\delta n\). By averaging, there exists some cluster \(C_{j}\in\mathscr{C}_{H}\) such that \(|C^{*}_{i}\cap C_{j}|\geq b\delta n/k\); choose this \(j\), and consider the corresponding center \(h_{j}\in H\). Now

\[d(g_{i},H)\leq d(g_{i},h_{j})\stackrel{{(\star)}}{{\leq}}\frac{1 }{|C^{*}_{i}\cap C_{j}|}\sum_{x\in C^{*}_{i}\cap C_{j}}\left[d(x,g_{i})+d(x,h_ {j})\right]\leq\frac{ALG+OPT}{b\delta n/k},\]

where \((\star)\) uses the triangle inequality. A similar argument shows:

\[d(h_{j},G)\leq\frac{ALG+OPT}{(b-2)\delta n/k}.\]

Hence the Hausdorff distance between the two solutions is \(d_{H}(G,H)\leq\frac{ALG+OPT}{(b-2)\delta n/k}\). Using Lemma 10 with this bound on the Hausdorff distance, we get

\[ALG\leq OPT+2\delta n\cdot d_{H}(G,H)\leq OPT+2k\cdot\frac{ALG+OPT}{(b-2)}.\]Simplifying, we get \(ALG\leq(1+\frac{4k}{b-2-2k})OPT\), and that \(d_{H}(G,H)\leq\frac{2k}{b-2-2k}\cdot\frac{OPT}{\delta n}\), as claimed. 

## Appendix F Proofs and Additional Analysis for Section 6.1

### Proof of Theorem 5

Proof.: By Corollary 3, computing the \(1\)-\(\operatorname{Median}\) on \(X^{\prime}\) results in a \(1+\frac{4\delta}{1-2\delta}\)-approximation ratio: this is because the metric space is \(\mathbb{R}^{d}\), and hence the \(1\)-\(\operatorname{Median}\) is in fact the geometric median (which is also known as the spatial median, \(L_{1}\) median or Fermat point). Since the geometric median computation depends solely on the predictions it is strategyproof. Thus, by the the strategyproofness of Coordinate-Wise-Median, and by the fact that the decision of which mechanism to use does not depend on the reported locations, Algorithm 1 is strategyproof. Due to the \(\sqrt{d}\) approximation ratio of Coordinate-Wise-Median (Meir (2019)), we get a \(\min\Bigl{(}1+\frac{4\delta}{1-2\delta},\sqrt{d}\Bigr{)}\) approximation ratio for Algorithm 1. 

### Remark on Computing the Geometric Median

For general dimensions \(d\) and number of points \(n\), there is no known formula or algorithm to find the geometric median exactly. However, since the problem is a convex optimization problem, there are optimization algorithms that find solutions with arbitrary precision efficiently. By using such an optimization algorithm we get an additional factor of \(\varepsilon^{\prime}>0\) where \(\varepsilon^{\prime}\) is arbitrarily small. For more information on efficiently computing the geometric median, see Eckhardt (1980), Beck and Sabach (2015) Cohen et al. (2016).

### Combined Strategy for High Probability \(\textsc{Mac}(\varepsilon,\delta)\)

In this section we show how we can use both the predictions and the reports of the agents to gain good performance even in the case of \(\textsc{Mac}(\varepsilon,\delta)\) only in high probability.

The \(\textsc{Mac}(\varepsilon,\delta)\) model requires that at most a \(\delta\) fraction of the points have error more than \(\varepsilon\); one can extend this to the setting where this requirement holds only with high probability. For instance, consider the setting where the number of prediction errors can be larger than a \(\delta\) fraction with probability at most \(o(\nicefrac{{1}}{{n}})\). In this case the computed geometric median might be arbitrarily far away with this small probability, and hence the expected approximation ratio for our mechanism would be unbounded. To avoid this problem, we can use the agent reports. Intuitively the agents have the incentive to make sure that the returned facility location is not infinitely far away from them.

One way to do this is by using the Min-Bounding-Box mechanism, introduced by (Agrawal et al., 2022, Mechanism 2) for the egalitarian cost function for \(d=2\); The mechanism naturally extends to any \(d\geq 1\) (Algorithm 6). The mechanism calculates the minimum bounding box \(B\) containing all the input points; then, given a single prediction \(o\), it returns the point \(\hat{o}\in B\) closest to \(o\). We show in Appendix F.4 that the approximation ratio of the generalized Min-Bounding-Box mechanism for the utilitarian cost function (\(\operatorname{med}_{1}\)) is \(O(n)\).

Therefore, by composing Min-Bounding-Box with Algorithm 1 we get a strategyproof mechanism which has an approximation ratio of \(min\Bigl{(}1+\frac{4\delta}{1-2\delta},\sqrt{d}\Bigr{)}\) with high probability, and has an \(O(n)\) approximation ratio with probability \(o(\nicefrac{{1}}{{n}})\); this gives an expected approximation ratio of at most \(min\Bigl{(}1+\frac{4\delta}{1-2\delta},\sqrt{d}\Bigr{)}+o(1)\). The form of the resulting mechanism is simple:

**Input:**\(\delta\in[0,\frac{1}{2})\) and \(X,X^{\prime}\subseteq\mathbb{R}^{d}\) where \(X\) is reported by strategic agents and \(X^{\prime}\) contains the

Mac predictions for \(X\)

**Output:** The facility location in \(\mathbb{R}^{d}\)

return Min-Bounding-Box\((X,\text{Best-Choice-Single-Facility-Loc}(X,X^{\prime},\delta))\)

[MISSING_PAGE_FAIL:23]

One could wonder why use the minimum bounding box rather than the convex hull of the points. After all, the convex hull is always contained in the minimum bounding box, and always contains the geometric median. Unfortunately, such a mechanism is not strategyproof:

**Remark 12**.: _The mechanism obtained by replacing the minimum bounding box in Algorithm 6 with the convex hull is not a strategyproof. The example showing this is for \(d=2\): Given \(X=\{(-0.5,0),(0.5,0),(0,1)\}\), a computation shows that the agent at \((0,1)\) can lower its cost by reporting \((\nicefrac{{1}}{{2}},1)\)._

We can further generalize the mechanism for \(k\) facilities by simply using Algorithm 6 for each facility independently.

## Appendix G A Deterministic Mechanism for Balanced \(k\)-Facility Location in General Metric Spaces

For \(\beta\in[0,1]\), the \(\beta\)-balanced \(k\)-facility location problem considers \(n\) agents with locations \(X=\{x_{1},\ldots,x_{n}\}\subseteq V\). Each agent reports a location to the mechanism, and the goal is to return a multi-set \(H=(h_{1},\ldots,h_{k})\) containing \(k\) points from \(V\) that minimizes \(\operatorname{med}_{k}(X,H)\), such that the clustering induced by \(H\) is \(\beta\)-balanced (according to Definition 7).

For a small enough \(\delta\), by choosing \(b:=\nicefrac{{\beta}}{{\delta}}\) the balancedness condition translates into a minimum cluster size of \(b\delta\). Hence, we can utilize the \(\delta\)-robustness results of \((b-1)\delta\) -Balanced \(k\) -Median (Theorem 4) to immediately obtain the deterministic mechanism in Algorithm 7 with the following guarantee:

**Theorem 13**.: _Algorithm 7 is a deterministic strategyproof mechanism with a constant (\(k\)-dependent) approximation ratio of at most_

\[1+\frac{4k}{b-2-2k}.\]

**Input:**\(X\), \(X^{\prime}\subseteq V\), and \(b>1\), \(\delta\in[0,0.5)\) where \(X^{\prime}\) are the Mac predictions for \(X\)

**Output:** The \(k\) facility locations in \(V\)

return \((b-1)\delta\) -Balanced \(k\) -Median\((X^{\prime})\)

**Algorithm 7**: Balanced-k-Facility-Loc(\(X,X^{\prime},b,\delta\))

We do not specify how \((b-1)\delta\) -Balanced \(k\) -Median is implemented. If all points lie on the real line, then there exists an \(O(n^{k})\)-time algorithm for it (computing the minimum cost of all \(\binom{n}{k}\) options).

In Euclidean spaces of dimensions greater than one, the \(k\)-medians problem (and its balanced or lower bounded variant) is classified as NP-hard Megiddo and Supowit (1984); Bhattacharya et al. (2020) necessitating the use of approximation algorithms. Any approximation algorithm with approximation ratio \(c\) can be applied, incurring an additional multiplicative cost factor of \(c\). This approach allows for practical solutions within the constraints of computational complexity, ensuring that we can still achieve near-optimal placements of facilities even in high-dimensional contexts. The effect of \(\varepsilon\) values and handling the case of high probability Mac predictions is similar to the single facility location results.

## Appendix H Full Proof of Big-Cluster-Center \(\delta\)-Approximation Robustness

In this section we show that Algorithm 3 indeed has "good" approximation-robustness for unbalanced clusters by providing the full proof for Theorem 8.

Proof.: Let \(G=(g_{L},g_{R})\) be the \(2-median\) solution, where \(L=\{i\mid d(x_{i},g_{L})\leq d(x_{i},g_{R})\}\) and \(R=[n]\setminus L\). W.l.o.g \(g_{L}\leq g_{R}\).

We have a slight abuse of notation throughout for the sake of not introducing more variables, referring to \(L,R\) as multisets of points and sometimes as the sets of indices of the points in \(X\).

We assume that the clusters are \(b\delta\)-unbalanced. That is, at least one of the clusters \(L,R\) are of size less than \(b\delta n\). W.l.o.g \(|R|<b\delta n\) and so \(|L|\geq(1-b\delta)n\).

We also assume that \(\delta\) is small enough s.t. \(b\delta<\frac{1}{4}\) and so \(|L|>\frac{3n}{4}\).

Let \(H=(h_{L},h_{R})=(b-1)\delta\operatorname{-Balanced-2-Medians}(X^{\prime})\) (as in the first line of Algorithm 3). W.l.o.g \(h_{L}\leq h_{R}\).

Let \(L^{\prime},R^{\prime}\) be the multi-set of the elements closest to \(h_{L}\) and \(h_{R}\) the remaining. So \(L^{\prime}=\{i\mid d(x^{\prime}_{i},h_{L})\leq d(x^{\prime}_{i},h_{R})\}\) and \(R^{\prime}=[n]\setminus L^{\prime}\).

Let \(A:=\{i\in[n]\mid x_{i}=x^{\prime}_{i}\}\) the shared points between \(X,X^{\prime}\) (the "correct" predictions), and \(B:=[n]\setminus A\) the remaining.

Let \(m=\frac{g_{L}+g_{R}}{2}\) be the middle point between \(g_{L}\) and \(g_{R}\). Let \(m^{\prime}=\frac{h_{L}+h_{R}}{2}\) be the middle point between \(h_{L}\) and \(h_{R}\). By the definition of \(m\): All points of \(L\) lie to the left of \(m\) and all points of \(R\) lie to the right of \(m\). Similarly, all points of \(L^{\prime}\) lie to the left of \(m^{\prime}\) and all points of \(R^{\prime}\) lie to the right of \(m^{\prime}\).

We begin by proving the following simple lemma:

**Lemma 14**.: _Let \(S\subseteq\mathbb{R}^{l}\) (for some \(l\in\mathbb{N}\)) be a multi-set of \(n\) points. For any \(\beta\in[0,1]\), \(S\subseteq V\), let \(M=(m_{1},\dots,m_{k}):=\beta-balanced-k-median(S)\), s.t. \(S_{1},\dots,S_{k}\) are the disjoint clusters induced by \(m_{1},\dots,m_{k}\) of \(S\). Then for all \(i\in[k]\): \(m_{i}\) is the \(1-median\) of \(S_{i}\)_

Proof.: Consider the implementation of \(\beta-balanced-k-median\) which is obtained by taking the cost-minimizing balanced partition into \(k\)\(\beta-balanced\) clusters. That is, the implementation considers all of the partitions of \(S\) into \(k\)\(\beta-balanced\) clusters and then computes the location that minimizes the center of each cluster \(S_{i}\). For each such center \(S_{i}\), the minimizing location is (by definition) \(1-median(S_{i})\). Thus, the implementation always returns points that are the \(1-median\) of their clusters. 

We divide the proof into 4 cases as follows (also illustrated in Fig. 4):

Case 1 \[:h_{L}\leq g_{L}\text{ and }h_{R}\geq g_{R}.\] Case 2 \[:h_{L}\geq g_{L}\text{ and }h_{R}\leq g_{R}.\] Case 3 \[:h_{L}\geq g_{L}\text{ and }h_{R}\geq g_{R}.\] Case 4 \[:h_{L}\leq g_{L}\text{ and }h_{R}\leq g_{R}.\]

Cases 1 to 3 are similar and have short proofs. The hard case is Case (4).

Case 1: \(h_{L}\leq g_{L}\) and \(h_{R}\geq g_{R}\). In this case we can image the two centers moving away from one another. That is, \(g_{L}\) "moves to the left" to \(h_{L}\), and \(g_{R}\) "moves to the right" to \(h_{R}\).

If \(m^{\prime}\geq m\) then \(L^{\prime}\) contains all of the points in \(L\cap A\). and \(|L\cap A|\geq|L|-\delta n\geq n-(b+1)\delta n\). Thus, for a small enough \(\delta\): \(n-(b+1)\delta n\geq\frac{n}{2}\) and thus the algorithm returns \(h_{L}\). From Corollary 3

Figure 4: Illustration of the 4 cases. On the top of each case drawing we have the ”estimated”/”predicted” locations \(H\), computed on \(X^{\prime}\). On the bottom we have the “real” locations, \(G\), computed on \(X\).

we get an approximation-robustness of \(1+O(b\delta)=1+O(\delta)\) (since \(L^{\prime}\) is obtained from \(L\) by change or add of at most \((b+1)\delta\) elements).

Otherwise, \(m^{\prime}\leq m\). The number of elements to the right of \(g_{R}\) in \(R\) is \(\frac{|R|}{2}\) since it is the median of \(R\) (by Lemma 14).

Since \(h_{R}\geq g_{R}\) then the number of elements to the right of \(h_{R}\) in \(R^{\prime}\) is at most \(\frac{|R|}{2}+|B|\leq\frac{|R|}{2}+\delta n\).

\(h_{R}\) is the median of \(R^{\prime}\) (by Lemma 14) and thus \(|R^{\prime}|\leq 2(\frac{|R|}{2}+\delta n)=|R|+2\delta n\leq(b+2)\delta n\). Thus again by Corollary 3 we get an approximation-robustness of \(1+O(\delta)\).

Case 2: \(h_{L}\geq g_{L}\) and \(h_{R}\leq g_{R}\). In this case we can image the two centers moving towards one another. That is, \(g_{L}\) "moves to the right" to \(h_{L}\), and \(g_{R}\) "moves to the left" to \(h_{R}\).

If \(m^{\prime}\geq m\) then \(L^{\prime}\) contains all of \(L\cap A\) plus at most \((b+1)\delta n\) points and thus just like the previous case we get (from the 1-median approximation robustness) a approximation-robustness of \(1+O(\delta)\).

Otherwise \(m^{\prime}<m\). By Lemma 14 we know that \(h_{L}\) is the median of \(L^{\prime}\) and that \(g_{L}\) is the median of \(L\). Since \(h_{L}\geq g_{L}\), there are at least \(\frac{|L|}{2}-\delta n\) points in \(L^{\prime}\) to the left of \(h_{L}\). Thus: \(|L^{\prime}|\geq 2(\frac{|L|}{2}-\delta n)\geq|L|-2\delta n\geq(1-(b+2) \delta)n\), and we can utilize Corollary 3 again to get the desired.

Case 3: \(h_{L}\geq g_{L}\) and \(h_{R}\geq g_{R}\). In this case we can image both centers "moving to the right".

In this case, \(m^{\prime}=\frac{h_{L}+h_{R}}{2}\geq\frac{g_{L}+g_{R}}{2}=m\).

Just like in the previous case: \(L^{\prime}\) contains all of \(L\cap A\) plus at most \((b+1)\delta n\) points and thus (from 1-median approximation robustness) a approximation-robustness of \(1+O(\delta)\).

Case 4: \(h_{L}\leq g_{L}\) and \(h_{R}\leq g_{R}\). In this case we can image both centers "moving to the left".

By the definition of \(m^{\prime}\): \(m^{\prime}=\frac{h_{L}+h_{R}}{2}\leq\frac{g_{L}+g_{R}}{2}=m\).

Until now we had an approximation-robustness result of \(1+O(\delta)\). This case is the most difficult case, as for this one we will get a \(1.8+O(\delta)\) approximation result. Since we also have a lower bound of \(\approx 1.667+\Omega(\delta)\) we get that this is indeed "strictly" the hard case.

If \(h_{R}\geq m\) then the number of elements in \(X\) to its right is at most \(|R|\), and thus the number of elements to its right on \(X^{\prime}\) is at most \(|R|+\delta n\). Since \(h_{R}\) median of \(R^{\prime}\) it must be the case that \(|R^{\prime}|\leq 2(|R|+\delta n)\leq 2(b+1)\delta n=O(\delta)\ n\). Thus \(|L^{\prime}|=n-|R^{\prime}|\geq(1-O(\delta))n\) and therefore: (a) The algorithm returns \(h_{L}\) (since \(|L^{\prime}|\geq\frac{n}{2}\)) and (b) From the \(1-median\) robustness (Corollary 3) we get the required \(1+O(\delta)\) approximation-robustness.

Thus, let us assume that \(h_{R}<m\).

We first handle the case where \(m^{\prime}\geq g_{L}\), and then we will show a reduction from the case \(m^{\prime}>g_{L}\) to this one.

### Sub-case: \(m^{\prime}\geq g_{L}\)

Since \(m^{\prime}\) is the middle point between \(h_{L}\) and \(h_{R}\) we get that: \(m^{\prime}-h_{L}=h_{R}-m^{\prime}\) but \(m^{\prime}-h_{L}=m^{\prime}-g_{L}+g_{L}-h_{L}\geq g_{L}-h_{L}\) and so: \(g_{L}-h_{L}\leq h_{R}-m^{\prime}\). But \(h_{R}-m^{\prime}\leq h_{R}-g_{L}\) and therefore:

\[g_{L}-h_{L}\leq h_{R}-g_{L}.\] (7)

We introduce the following notations (also see Fig. 5): First, we denote the left and right parts of \(L^{\prime},R^{\prime},L,R\):

* Let \(L^{\prime}{}_{l}=\{i\in L^{\prime}\mid x^{\prime}_{i}\leq h_{L}\},L^{\prime}{}_ {r}=L^{\prime}\setminus L^{\prime}{}_{l}\).
* Let \(R^{\prime}{}_{l}=\{i\in R^{\prime}\mid x^{\prime}_{i}\leq h_{R}\},R^{\prime}{} _{r}=R^{\prime}\setminus R^{\prime}{}_{l}\).
* Let \(L_{l}=\{i\in L\mid x_{i}\leq g_{L}\},L_{r}=L\setminus L_{l}\).
* Let \(R_{l}=\{i\in R\mid x_{i}\leq g_{R}\},R_{r}=R\setminus R_{l}\).

Next, we denote the partition of \(L\) into 4 disjoint multi-sets \(S,T,U,V\) by the 3 points \(h_{L},m^{\prime},h_{R}\). So \(S=\{i\in L\mid x_{i}\leq h_{L}\}\), \(T=\{i\in L\mid i\notin S,x_{i}\leq m^{\prime}\}\), \(U=\{i\in L\mid i\notin S\cup T,x_{i}\leq h_{R}\}\), \(V=\{i\in L\mid i\notin S\cup T\cup U\}\).

Let \(m^{\prime\prime}=\frac{g_{L}+h_{R}}{2}\) be the middle point between \(g_{L}\) and \(h_{R}\). W.l.o.g we assume that \(m^{\prime\prime}>m^{\prime}\) (otherwise the proof is similar). We define \(U_{l},U_{r}\) to be the as follows: \(U_{l}=\{i\in U\mid x_{i}\leq m^{\prime\prime}\}\), \(U_{r}=U\setminus U_{l}\).

Finally, we introduce the notion of \(\alpha\)-approximately-equal: \(\approx^{\alpha}\):

Definition 14.: _For any \(a,b,\alpha\in\mathbb{R}\):_

\[a\approx^{\alpha}b\iff b-\alpha\leq a\leq b+\alpha\]

Figure 5: Illustration of case (4) where \(m^{\prime}\geq g_{L}\). On the top we have \(h_{L},h_{R}\), computed on the “predicted” locations \(X^{\prime}\). On the bottom we have the \(g_{L},g_{R}\), the \(2\text{-Medians}\) of the “real” locations \(X\). \(L^{\prime}\) and \(R^{\prime}\) are the disjoint partitions of \(X^{\prime}\) into two multi-sets of points: those closer to \(h_{L}\) and those closer to \(h_{R}\) (respectively). Similarly \(L\) and \(R\) are the disjoint multi-sets of \(g_{L},g_{R}\). \(L^{\prime}_{l},L^{\prime}_{r}\) are the disjoint partitions of \(L^{\prime}\) into two multi-sets: all points to the left of \(h_{L}\) and all points to the right of \(h_{R}\). In a similar manner \(R^{\prime},L,R\) are partitioned into their left and right parts (\(R^{\prime}_{l},R^{\prime}_{r},L_{l},L_{r},R_{l},R_{r}\)). We can also see \(S,T,U,V\) which is the disjoint partition of \(L\) determined by the points \(h_{L},m^{\prime},h_{R}\). Finally, we have \(U_{l}\) and \(U_{r}\) which are the left and right parts of \(U\).

For every \(M\subseteq X\) we define \(OPT_{M}\) to be the cost that the optimal solution (\(G\)) pays for the multi-set of points \(M\). That is: \(OPT_{M}:=\operatorname{med}_{2}(M,G)\) (we sometimes use this notation where \(M\subseteq[n]\) in which case \(OPT_{M}\) is a slight abuse of notation for \(OPT_{\{x_{i}|i\in M\}}\)).

Let \(\beta,\gamma\in[0,1]\) be the ratio between \(|S|,|V|\) and \(|L|\). That is: \(\beta:=\nicefrac{{|S|}}{{|L|}}\) and \(\gamma:=\nicefrac{{|V|}}{{|L|}}\).

Claim 15.

\[\operatorname{med}_{1}(L,h_{L})=OPT_{S}-OPT_{T\cap L_{l}}+OPT_{L_{r}}+(g_{L}- h_{L})(|L|(1-2\beta)).\]

Proof.: \[\operatorname{med}_{1}(L,h_{L}) =\sum_{i\in L}|x_{i}-h_{L}|=\sum_{i\in S}h_{L}-x_{i}+\sum_{i\in T \cup U\cup V}x_{i}-h_{L}\] \[\stackrel{{(\star)}}{{=}}\sum_{i\in S}g_{L}-x_{i}-|S |(g_{L}-h_{L})-\sum_{i\in T\cap L_{l}}g_{L}-x_{i}+|T\cap L_{l}|(g_{L}-h_{L})\] \[+\sum_{i\in(T\cap L_{r})\cup U\cup V}x_{i}-g_{L}+(|T\cap L_{r}|+| U|+|V|)(g_{L}-h_{L})\] \[\stackrel{{(\star\star)}}{{=}}OPT_{S}-OPT_{T\cap L_{ l}}+OPT_{L_{r}}+(g_{L}-h_{L})(|L|-2|S|)\] \[=OPT_{S}-OPT_{T\cap L_{l}}+OPT_{L_{r}}+(g_{L}-h_{L})(|L|(1-2\beta )).\]

(\(\star\)) - triangle inequality

(\(\star\star\)) - due to the fact that \(L_{r}=T\cap L_{r}\cup U\cup V\) and \(|L|=|S|+|U|+|V|+|T|\). 

If the algorithm returns \(h_{L}\) then we want to show that \(\operatorname{med}_{1}(L,h_{L})\leq(1.8+O(\delta))\operatorname{med}_{1}(L,g_ {L})\).

From Claim 15 we get the following equivalent condition:

\[\operatorname{med}_{1}(L,h_{L})\leq(1.8+O(\delta))OPT_{L}\iff\] \[OPT_{S}-OPT_{T\cap L_{l}}+OPT_{L_{r}}+(g_{L}-h_{L})(|L|(1-2\beta ))\leq(1.8+O(\delta))OPT_{L}\iff\] \[(g_{L}-h_{L})(|L|(1-2\beta))\leq 0.8OPT_{S}+2.8OPT_{T\cap L_{l}}+ 0.8OPT_{L_{r}}+O(\delta)OPT_{L}\iff\] \[(g_{L}-h_{L})\leq\frac{0.8OPT_{S}+2.8OPT_{T\cap L_{l}}+0.8OPT_{L_ {r}}+O(\delta)OPT_{L}}{|L|(1-2\beta)}.\] (8)

Thus, all we have to do is find the above bound for \(g_{L}-h_{L}\). Indeed we will show that this is the case. We start by proving a few claims that will help us bound \(g_{L}-h_{L}\).

Claim 16.: \((\beta+\gamma)|L|\approx^{O(\delta n)}|L|-\frac{n}{2}\)

Proof.: Since \(h_{L}\) is the median of \(L^{\prime}\), and the points in \(S\cup T\) differ from those in \(L^{\prime}\) by at most \(O(\delta n)\), we get that \(|T|\approx^{O(\delta n)}|\beta|L|\). Similarly, since \(h_{R}\) is the median of \(R^{\prime}\) and since \(|R|=n-|L|\): \(|U|\approx^{O(\delta n)}(\gamma-1)|L|+n\). By using the fact that \(|L|=|S|+|T|+|U|+|V|\) we get:

\[|L|\approx^{O(\delta n)}\beta|L|+\beta|L|+(\gamma-1)|L|+n+\gamma|L|\implies\] \[2(\beta+\gamma)|L|\approx^{O(\delta n)}2|L|-n\implies\] \[(\beta+\gamma)|L|\approx^{O(\delta n)}|L|-\frac{n}{2}.\]

Claim 17. 

\[(g_{L}-h_{L})|S|\leq OPT_{S}.\] (9)

Proof.: \[OPT_{S}=\sum_{i\in S}g_{L}-x_{i}\stackrel{{\forall x_{i}\in S; \ x_{i}\leq h_{L}}}{{\geq}}\sum_{i\in S}g_{L}-h_{L}=|S|(g_{L}-h_{L}).\]Claim 18.

\[(h_{R}-g_{L})|U_{r}|\leq 2OPT_{U_{r}}.\] (10)

Proof.: \[OPT_{U_{r}}=\sum_{i\in U_{r}}x_{i}-g_{L}\stackrel{{\forall x_{i}\in U _{r}:\;x_{i}\geq m^{\prime\prime}}}{{\geq}}\sum_{i\in U_{r}}m^{\prime\prime}-g_ {L}=|U_{r}|(m^{\prime\prime}-g_{L})=\frac{|U_{r}|}{2}(h_{R}-g_{L}).\]

where the last equality is due to the fact that \(m^{\prime\prime}\) is the middle point between \(g_{L}\) and \(h_{R}\).

Thus we get:

\((h_{R}-g_{L})|U_{r}|\leq 2OPT_{U_{r}}\) 

Claim 19.

\[(h_{R}-g_{L})|V|\leq OPT_{V}.\] (11)

Proof.: \[OPT_{V}=\sum_{i\in V}x_{i}-g_{L}\stackrel{{\forall x_{i}\in V _{r}:\;x_{i}\geq h_{R}}}{{\geq}}\sum_{i\in S}h_{R}-g_{L}=|V|(h_{R}-g_{L}).\]

Consider the clustering induced by \(H^{\prime}=(g_{L},h_{R})\). If this clustering is unbalanced then Algorithm 3 returns \(h_{L}\) since \(L_{l}\cap A\) is contained in the left cluster and since \(|L_{l}\cap A|\geq\frac{L}{2}-\delta n\geq\frac{n}{2}-O(\delta n)\) it must be that the right cluster is the smaller one. And so the left cluster has at least \(n-(b-1)\delta n=n-O(\delta n)\) elements and thus it is obtained from \(L\) by modifying or dropping at mots \(O(\delta n)\) elements and thus the algorithm returns \(h_{L}\) and like before we get a \((1+O(\delta),\delta)\) approximation-robustness.

Otherwise, the clustering induced by \(H^{\prime}\) is balanced, and we get the following claim:

Claim 20.

\((h_{R}-g_{L})(|U_{l}|-O(\delta n))\leq 2(OPT_{T\cap L_{l}}+OPT_{U_{l}})\).

Proof.: For convenience, for any multi-set of the real points \(M\subseteq X\) we denote \(\hat{M}\subseteq X^{\prime}\) to be the multi-set of the estimated points \(X^{\prime}\) that correspond to the same partition of the real line as \(M\). So we define the following multi-sets: \(\hat{L}_{l}=\{x^{\prime}_{i}\in X^{\prime}\mid x^{\prime}_{i}\leq g_{L}\}\), \(\hat{L}_{r}=\{x^{\prime}_{i}\in X^{\prime}\setminus\hat{L}_{l}\mid x^{\prime} _{i}\leq m\}\), \(\hat{S}=L^{\prime}\), \(\hat{T}=L^{\prime}\), \(\hat{U}=R^{\prime}\), \(\hat{V}=\{i\in R^{\prime}\)\(\mid x^{\prime}_{i}\in[h_{R},m]\}\), \(\hat{U}_{l}=\{x^{\prime}_{i}\in R^{\prime}\)\(\mid x^{\prime}_{i}\leq m^{\prime\prime}\}\), \(\hat{U}_{r}=\{x^{\prime}_{i}\in R^{\prime}\)\(\mid x^{\prime}_{i}>m^{\prime\prime}\}\). The reason we use this notation is that we have an inequality in terms of the points in \(X^{\prime}\) and we want to later move on to the inequality in terms of the points in \(X\). This notation will help us see the connection between the points in \(X^{\prime}\) and the points in \(X\). Each such \(M,\hat{M}\) contain the same points up to at most \(\delta n\) points.

By definition of \(H\) we know that \(\mathrm{med}_{2}(X^{\prime},H)\leq\mathrm{med}_{2}(X^{\prime},T)\) for any \(T=(t_{1},t_{2})\in\mathbb{R}^{2}\) that induces a \((b-1)\delta\)-balanced clustering of \(X^{\prime}\). Therefore, \(\mathrm{med}_{2}(X^{\prime},H)\leq\mathrm{med}_{2}(X^{\prime},(g_{L},h_{R}))\).

By the definition of \(\mathrm{med}_{2}\):

\[\sum_{x^{\prime}_{i}\in L^{\prime}}h_{L}-x^{\prime}_{i}+\sum_{x^{ \prime}_{i}\in L^{\prime}}x^{\prime}_{i}-h_{L}+\sum_{x^{\prime}_{i}\in R^{ \prime}_{i}}h_{R}-x^{\prime}_{i}+\sum_{x^{\prime}_{i}\in R^{\prime}_{r}}x^{ \prime}_{i}-h_{R}\leq\] \[\sum_{x^{\prime}_{i}\in S\cup(\hat{T}\cap\hat{L}_{l})}g_{L}-x^{ \prime}_{i}+\sum_{x^{\prime}_{i}\in(\hat{T}\cap\hat{L}_{r})\cup\hat{U}_{l}}x^{ \prime}_{i}-g_{L}+\sum_{x^{\prime}_{i}\in\hat{U}_{r}}h_{R}-x^{\prime}_{i}+\sum_ {x^{\prime}_{i}\in R^{\prime}_{r}}x^{\prime}_{i}-h_{R}.\]

Which implies:

\[\sum_{x^{\prime}_{i}\in L_{h_{1}}}h_{L}-x^{\prime}_{i}+\sum_{x^{ \prime}_{i}\in L^{\prime}_{r}}x^{\prime}_{i}-h_{L}+\sum_{x^{\prime}_{i}\in R^ {\prime}_{i}}h_{R}-x^{\prime}_{i}\] \[\leq\sum_{x^{\prime}_{i}\in S\cup(\hat{T}\cap\hat{L}_{l})}g_{L}-x ^{\prime}_{i}+\sum_{x^{\prime}_{i}\in(\hat{T}\cap\hat{L}_{r})\cup\hat{U}_{l}}x^{ \prime}_{i}-g_{L}+\sum_{x^{\prime}_{i}\in\hat{U}_{r}}h_{R}-x^{\prime}_{i}.\] (12)Let us observe LHS:

\[\sum_{x_{i}^{\prime}\in L_{h_{l}}}h_{L}-x_{i}^{\prime}+\sum_{x_{i}^{ \prime}\in L^{\prime}{}_{r}}x_{i}^{\prime}-h_{L}+\sum_{x_{i}^{\prime}\in R^{ \prime}{}_{l}}h_{R}-x_{i}^{\prime}\] \[=|L^{\prime}_{t}|(h_{L}-g_{L})+\sum_{x_{i}^{\prime}\in L_{h_{l}}}g _{L}-x_{i}^{\prime}+\sum_{x_{i}^{\prime}\in L^{\prime}{}_{r}}x_{i}^{\prime}-g_ {L}+|L^{\prime}{}_{r}|(g_{L}-h_{L})+|R^{\prime}{}_{l}|(h_{R}-g_{L})+\sum_{x_{i }^{\prime}\in R^{\prime}{}_{l}}g_{L}-x_{i}^{\prime}\] \[|L^{\prime}_{t}|\equiv|L^{\prime}_{r}|\sum_{x_{i}^{\prime}\in L_{ h_{l}}}g_{L}-x_{i}^{\prime}+\sum_{x_{i}^{\prime}\in L^{\prime}{}_{r}}x_{i}^{ \prime}-g_{L}+|R^{\prime}{}_{l}|(h_{R}-g_{L})+\sum_{x_{i}^{\prime}\in R^{ \prime}{}_{l}}g_{L}-x_{i}^{\prime}\] \[=\sum_{x_{i}^{\prime}\in S}g_{L}-x_{i}^{\prime}+\sum_{x_{i}^{ \prime}\in\hat{T}}x_{i}^{\prime}-g_{L}+|\hat{U}|(h_{R}-g_{L})+\sum_{x_{i}^{ \prime}\in\hat{U}}g_{L}-x_{i}^{\prime}\] \[=\sum_{x_{i}^{\prime}\in\hat{S}}g_{L}-x_{i}^{\prime}+\sum_{x_{i}^ {\prime}\in\hat{T}\cap\hat{L}_{l}}x_{i}^{\prime}-g_{L}+\sum_{x_{i}^{\prime} \in\hat{T}\cap\hat{L}_{r}}x_{i}^{\prime}-g_{L}+|\hat{U}|(h_{R}-g_{L})+\sum_{x_ {i}^{\prime}\in\hat{U}}g_{L}-x_{i}^{\prime}.\]

So by plugging this in (12) we get:

\[\sum_{x_{i}^{\prime}\in\hat{S}}g_{L}-x_{i}^{\prime}+\sum_{x_{i}^{ \prime}\in\hat{T}\cap\hat{L}_{l}}x_{i}^{\prime}-g_{L}+\sum_{x_{i}^{\prime}\in \hat{T}\cap\hat{L}_{r}}x_{i}^{\prime}-g_{L}+|\hat{U}|(h_{R}-g_{L})+\sum_{x_{i} ^{\prime}\in\hat{U}}g_{L}-x_{i}^{\prime}\] \[\leq\sum_{x_{i}^{\prime}\in\hat{S}\cup(\hat{T}\cap\hat{L}_{l})}g _{L}-x_{i}^{\prime}+\sum_{x_{i}^{\prime}\in\hat{T}\cap\hat{L}_{r})\cup\hat{U}_{ l}}x_{i}^{\prime}-g_{L}+\sum_{x_{i}^{\prime}\in\hat{U}_{r}}h_{R}-x_{i}^{ \prime}.\] \[\implies\] \[|\hat{U}|(h_{R}-g_{L})+\sum_{x_{i}^{\prime}\in\hat{U}}g_{L}-x_{i}^ {\prime}\leq 2\sum_{x_{i}^{\prime}\in\hat{T}\cap\hat{L}_{l}}g_{L}-x_{i}^{\prime} +\sum_{x_{i}^{\prime}\in\hat{U}_{l}}x_{i}^{\prime}-g_{L}+\sum_{x_{i}^{\prime} \in\hat{U}_{r}}h_{R}-x_{i}^{\prime}\] \[=2\sum_{x_{i}^{\prime}\in\hat{T}\cap\hat{L}_{l}}g_{L}-x_{i}^{ \prime}+\sum_{x_{i}^{\prime}\in\hat{U}_{l}}x_{i}^{\prime}-g_{L}+|\hat{U_{r}}|( h_{R}-g_{L})+\sum_{x_{i}^{\prime}\in\hat{U}_{r}}g_{L}-x_{i}^{\prime}\implies\] \[|\hat{U_{l}}|(h_{R}-g_{L})\leq 2\sum_{x_{i}^{\prime}\in\hat{T} \cap\hat{L}_{l}}g_{L}-x_{i}^{\prime}+2\sum_{x_{i}^{\prime}\in\hat{U}_{l}}x_{i} ^{\prime}-g_{L}.\] (13)

By the definition of \(\hat{T},\hat{L}_{l}\): \(\hat{T}\cap\hat{L}_{l}\) and \(T\cap L_{l}\) differ by at most \(\delta n\) elements, and for any \(x_{i}^{\prime}\in\hat{T}\cap\hat{L}_{l}\): \(x_{i}^{\prime}\geq h_{L}\). Similarly \(\hat{U_{l}}\) and \(U_{l}\) differ by at most \(\delta n\) elements and for any \(x_{i}^{\prime}\in\hat{U}_{l}\): \(x_{i}^{\prime}\leq m^{\prime\prime}\).

By plugging this in the above (13) we get:

\[(|U_{l}|-\delta n)(h_{R}-g_{L}) \leq|\hat{U_{l}}|(h_{R}-g_{L})\] \[\leq 2\sum_{x_{i}\in T\cap L_{l}}g_{L}-x_{i}+2\delta n(g_{L}-h_{L })+2\sum_{x_{i}\in U_{l}}x_{i}-g_{L}+2\delta n(m^{\prime\prime}-g_{L}).\]

Which implies:

\[(h_{R}-g_{L})|U_{l}|\leq 2(OPT_{T\cap L_{l}}+OPT_{U_{l}})+\delta n(2(g_{L}-h_{L })+2(m^{\prime\prime}-g_{L})+(h_{R}-g_{L})).\]

Since \(m^{\prime\prime}-g_{L}=(\nicefrac{{h}}{{h}}-g_{L})/_{2}\) (by the definition of \(m^{\prime\prime}\)) and from Eq. (7) we get:

\[(h_{R}-g_{L})|U_{l}|\leq 2(OPT_{T\cap L_{l}}+OPT_{U_{l}})+O(\delta n)(h_{R}-g_{L }).\]

\[(h_{R}-g_{L})(|U_{l}|-O(\delta n))\leq 2(OPT_{T\cap L_{l}}+OPT_{U_{l}}).\] (14)

Now we are ready to bound \(g_{L}-h_{L}\).

Lemma 21.: _If the algorithm returns \(h_{L}\) then:_

\[(g_{L}-h_{L})\leq\frac{0.8OPT_{S}+2.8OPT_{T\cap L_{l}}+0.8OPT_{L_{r}}+O(\delta) OPT_{L}}{|L|(1-2\beta)}.\]

Proof.: By Claim 17, Claim 18, Claim 20, Claim 19 and the fact that \(g_{L}-h_{L}\leq h_{R}-g_{L}\) we get the following four inequalities:

\[(g_{L}-h_{L})|U_{r}|\leq 2OPT_{U_{r}},\] \[(g_{L}-h_{L})(|U_{l}|-O(\delta n))\leq 2(OPT_{T\cap L_{l}}+OPT_{U_{ l}}),\] \[(g_{L}-h_{L})2|V|\leq 2OPT_{V},\] \[(g_{L}-h_{L})2|S|\leq 2OPT_{S},\]

By summing the above inequalities:

\[(g_{L}-h_{L})(2|S|+|U|+2|V|-O(\delta n))\leq 2(OPT_{S}+OPT_{T \cap L_{l}}+OPT_{U}+OPT_{V})\implies\] \[g_{L}-h_{L}\leq\frac{(OPT_{S}+OPT_{T\cap L_{l}}+OPT_{U}+OPT_{V} )}{(1-2\beta)|L|}\cdot\frac{2(1-2\beta)|L|}{2|S|+|U|+2|V|-O(\delta n)}.\] (15)

Since \(|L^{\prime}{}_{l}|=|L^{\prime}{}_{r}|\) we get that \(|T|\approx^{\delta n}|S|=\beta|L|\) and so

\[2|S|+|U|+2|V|-O(\delta n)\] \[\geq|S|+|T|+|U|+2|V|-O(\delta n)\] \[=(|S|+|T|+|U|+|V|)+(|S|+|V|)-|S|-O(\delta n)\] \[\stackrel{{(\star)}}{{=}}|L|+(\beta+\gamma)|L|- \beta|L|-O(\delta n)\] \[\stackrel{{Claim\ 16}}{{\geq}}|L|+|L|-\frac{n}{2}- \beta|L|-O(\delta n)\] \[=2|L|-\beta|L|-\frac{n}{2}-O(\delta n).\]

Where \((\star)\) is due to: \(|L|=|S|+|T|+|U|+|V|,|V|=\gamma|L|\), \(|S|=\beta|L|\).

By plugging this inequality into Eq. (15):

\[g_{L}-h_{L}\leq\frac{(OPT_{S}+OPT_{T\cap L_{l}}+OPT_{U}+OPT_{V})}{(1-2\beta) |L|}\cdot\bigg{(}\frac{2(1-2\beta)|L|}{2|L|-\beta|L|-\frac{n}{2}-O(\delta n)} \bigg{)}.\] (16)

We will bound the second term in RHS. First, we know that \(|L|\approx^{O(\delta n)}n\) and thus:

\[\frac{2(1-2\beta)|L|}{2|L|-\beta|L|-\frac{n}{2}-O(\delta n)}\leq\frac{2(1-2 \beta)(n-O(\delta n))}{\frac{3n}{2}-\beta n-O(\delta n)}\leq\frac{2(1-2\beta)} {\frac{3}{2}-\beta}(1+O(\delta))\]

We denote: \(f(\beta):=\frac{2(1-2\beta)}{\frac{3}{2}-\beta}\).

Since the algorithm returns \(h_{L}\) we deduce that \(|L^{\prime}|\geq\frac{n}{2}\). Thus, \(|L^{\prime}{}_{l}|,|L^{\prime}{}_{r}|\geq\frac{n}{4}\). But \(S\) and \(L^{\prime}{}_{l}\) differ by at most \(\delta n\) elements, and so \(|S|\geq\frac{n}{4}-\delta n\). Since \(|S|=\beta|L|\) and \(|L|\leq n\) we get: \(\beta n\geq\frac{n}{4}-\delta n\) which implies \(\beta\geq\frac{1}{4}-\delta\).

Also obviously \(\beta\leq 0.5\) since \(h_{L}\leq g_{L}\) by the definition of \(S\).

So \(f:[\frac{1}{4}-\delta,\frac{1}{2}]\rightarrow\mathbb{R}\) is a well defined function. We find its maximum:

\[f^{\prime}(\beta)=2\frac{-2(\frac{3}{2}-\beta)-(1-2\beta)(-1)}{(\frac{3}{2}- \beta)^{2}}=2\frac{-3+2\beta+1-2\beta}{(\frac{3}{2}-\beta)^{2}}=\frac{-4}{( \frac{3}{2}-\beta)^{2}}<0.\]

So \(f\) gets its maximum at the left boundary where \(\beta=\frac{1}{4}-\delta\):\[f(\frac{1}{4}-\delta)=2\frac{\frac{1}{2}+2\delta}{\frac{5}{4}+\delta}=\frac{4}{5}(1 +O(\delta)).\]

By plugging the above into Eq. (16) we get:

\[g_{L}-h_{L}\leq\frac{4}{5}\frac{(OPT_{S}+OPT_{T\cap L_{l}}+OPT_{U}+OPT_{V}+O( \delta)OPT_{L})}{(1-2\beta)|L|}.\]

What we have shown is that if the algorithm returns \(h_{L}\) we are done. If the algorithm returns \(h_{R}\) we can similarly show that in this case \(\mathrm{med}_{1}(L,h_{R})\leq(1.8+\Theta(\delta))\,\mathrm{med}_{1}(L,g_{L})\).

#### h.1.1 Handling the Case where the Algorithm Returns \(h_{R}\)

In this case the cost \(\mathrm{med}_{1}(L,h_{R})\) will be:

Claim 22.: \[\mathrm{med}_{1}(L,h_{R})=OPT_{S}+OPT_{T\cap L_{l}}-OPT_{T\cap L_{r}}-OPT_{U} +OPT_{V}+(|L|(1-2\gamma))(h_{R}-g_{L}).\]

Proof.: \[\mathrm{med}_{1}(L,h_{R}) =\sum_{i\in S\cup T\cup U}h_{R}-x_{i}+\sum_{i\in V}x_{i}-h_{R}\] \[=\sum_{i\in S\cup T\cup U}(h_{R}-g_{L})+(g_{L}-x_{i})+\sum_{i\in V }(x_{i}-g_{L})-(h_{R}-g_{L})\] \[=OPT_{S}+OPT_{T\cap L_{l}}-OPT_{T\cap L_{r}}-OPT_{U}+OPT_{V}+(| S|+|T|+|U|-|V|)(h_{R}-g_{L})\] \[\stackrel{{|S|+|T|+|U|+|V|=|L|}}{{=}}OPT_{S}+OPT_{T \cap L_{l}}-OPT_{T\cap L_{r}}-OPT_{U}+OPT_{V}+(|L|-2|V|)(h_{R}-g_{L})\] \[=OPT_{S}+OPT_{T\cap L_{l}}-OPT_{T\cap L_{r}}-OPT_{U}+OPT_{V}+(| L|(1-2\gamma))(h_{R}-g_{L}).\]

To get \(\mathrm{med}_{1}(L,h_{R})\leq(1.8+O(\delta))\,\mathrm{med}_{1}(L,g_{L})\) we need to show (by the above claim) the following lemma:

Lemma 23.: _If the algorithm returns \(h_{R}\) then:_

\[(h_{R}-g_{L})\leq\frac{0.8(OPT_{S}+OPT_{T\cap L_{l}}+OPT_{V})+2.8(OPT_{T\cap L _{r}}+OPT_{U})+O(\delta)OPT_{L}}{(1-2\gamma)|L|}.\]

Proof.: We begin by showing the following claim:

Claim 24.: \[(h_{R}-g_{L})|S|\leq(h_{R}-g_{L})|S|\leq OPT_{S}+\frac{|S|}{|U|}OPT_{U}.\]

Proof.: \(h_{R}-g_{L}=m^{\prime}-h_{L}=(m^{\prime}-g_{L})+(g_{L}-h_{L})\) and so \(g_{L}-h_{L}=(h_{R}-g_{L})-(m^{\prime}-g_{L})\). Together with Claim 17 we get:

\[(h_{R}-g_{L})|S|\leq OPT_{S}+|S|(m^{\prime}-g_{L}).\] (17)

Also, \(OPT_{U}=\sum_{i\in U}x_{i}-g_{L}\geq\sum_{i\in U}m^{\prime}-g_{L}=|U|(m^{ \prime}-g_{L})\) which implies

\[m^{\prime}-g_{L}\leq\frac{OPT_{U}}{|U|}.\] (18)

By Eq. (17) and Eq. (18) we get the desired inequality.

From Claim 24, Claim 18, Claim 20, Claim 19:

we get the following four inequalities:

\[(h_{R}-g_{L})2|S|\leq 2OPT_{S}+2\frac{|S|}{|U|}OPT_{U},\]

\[(h_{R}-g_{L})|U_{r}|\leq 2OPT_{U_{r}},\]

\[(h_{R}-g_{L})(|U_{l}|-O(\delta n))\leq 2(OPT_{T\cap L_{l}}+OPT_{U_{l}}),\]

\[(h_{R}-g_{L})2|V|\leq 2OPT_{V},\]

By summing these inequalities:

\[(h_{R}-g_{L})(2|S|+|U|+2|V|-O(\delta n))\leq\frac{OPT_{S}+OPT_{U}+OPT_{T\cap L _{l}}+OPT_{V}+\frac{2|S|}{|U|}OPT_{U}}{(1-2\gamma)|L|}\cdot 2(1-2\gamma)|L|.\] (19)

Since the algorithm returns \(h_{R}\) it must be the case that \(|R^{\prime}|\geq\frac{n}{2}\) and so \(|R^{\prime}{}_{l}|\geq\frac{n}{4}\) which means that \(|U|\geq\frac{n}{4}-\delta n\). This also means that \(|L^{\prime}|\leq\frac{n}{2}\) and thus similarly \(|S|\leq\frac{n}{4}+\delta n\). So together:

\[2\frac{|S|}{|U|}\leq\frac{\frac{n}{2}+2\delta n}{\frac{n}{4}-\delta n}=2+O( \delta).\]

By plugging this back into Eq. (19) we get:

\[(h_{R}-g_{L})(2|S|+|U|+2|V|-O(\delta n))\leq\frac{OPT_{S}+(3+O(\delta))OPT_{U }+OPT_{T\cap L_{l}}+OPT_{V}}{(1-2\gamma)|L|}\cdot 2(1-2\gamma)|L|.\] (20)

Since \(|L^{\prime}{}_{l}|=|L^{\prime}{}_{r}|\) we get that \(|T|\approx^{\delta n}|S|\) and so

\[2|S|+|U|+2|V|-O(\delta n)\] \[\geq|S|+|T|+|U|+2|V|-O(\delta n)\] \[=(|S|+|T|+|U|+|V|)+|V|-O(\delta n)\] \[\stackrel{{(\star)}}{{=}}(1+\gamma)|L|-O(\delta n)\]

Where \((\star)\) is due to: \(|L|=|S|+|T|+|U|+|V|\), \(|V|=\gamma|L|\).

Let us use this fact in Eq. (20) and get:

\[h_{R}-g_{L}\leq\frac{OPT_{S}+(3+O(\delta))OPT_{U}+OPT_{T\cap L_{l}}+OPT_{V} }{(1-2\gamma)|L|}\cdot\bigg{(}2\frac{(1-2\gamma)|L|}{(1+\gamma)|L|-O(\delta n )}\bigg{)}.\] (21)

The second term on RHS is (up to \(1+O(\delta)\) factor): \(h(\gamma):=2\frac{1-2\gamma}{1+\gamma}\). Let us find a bound f or \(h\).

Since \(|R^{\prime}|\geq\frac{n}{2}\):

\[|V|+|R|\approx=^{O(\delta n)}|R^{\prime}{}_{r}|\geq\frac{n}{4}.\]

and since \(|R|\leq b\delta n\): \(\gamma|L|=|V|\geq\frac{n}{4}-b\delta n\). We deduce:

\(\gamma\geq\frac{1}{4}-O(\delta)\). The nominator is decreasing in \(\gamma\) and the denominator is increasing in \(\gamma\) and thus the maximum is received on the left end point: \(\gamma=\frac{1}{4}-O(\delta)\). The maximum value of \(h\) is thus: \(2\frac{\frac{1}{4}-O(\delta)}{\frac{1}{4}-O(\delta)}=\frac{4}{5}+O(\delta)\).

So we found a bound for the RHS of Eq. (21) and therefore:

\[h_{R}-g_{L}\leq\frac{\frac{4}{5}OPT_{S}+\frac{12}{5}OPT_{U}+\frac{4}{5}OPT_{ T\cap L_{l}}+\frac{4}{5}OPT_{V}+O(\delta)OPT_{L}}{(1-2\gamma)|L|}.\]

### Sub-case: \(m^{\prime}<g_{L}\).

To handle this sub-case we use a reduction to the previous sub-case and show that we pay another multiplicative factor of at most \(1+O(\delta)\). We show a bound of \(\operatorname{med}_{1}(h_{L},L)\) in case the algorithm returns \(h_{L}\). In the case the algorithm returns \(h_{R}\) a bound for \(\operatorname{med}_{1}(h_{R},L)\) is achieved similarly.

Let \(\tilde{g_{L}}=m^{\prime}\). For any multi-set of points \(M\subseteq L\) let \(\widetilde{OPT_{M}}=\sum_{i\in M}|x_{i}-\tilde{g_{L}}|\).

Figure 6: Illustration of case (4) where \(m^{\prime}<g_{L}\). On the top we have the ”estimated” locations \(h_{L},h_{R}\), computed on \(X^{\prime}\). On the bottom we have the ”real” locations, \(g_{L},g_{R}\), computed on \(X\). \(L^{\prime},R^{\prime},L^{\prime}_{l},L^{\prime},R^{\prime}_{l},R^{\prime}_{r}, S,T,U,V,U_{l},U_{r}\) are all the same as before. The difference is that \(m^{\prime}\) is now to the left of \(g_{L}\).

By the exact same analysis of the case where \(m^{\prime}\geq g_{L}\), now we have that \(m^{\prime}\geq\tilde{g_{L}}\) and thus, just like before, \(\tilde{g_{L}}-h_{L}\leq 4\frac{\partial\widetilde{PT_{S}}+\partial\widetilde{PT_{T}}+ \partial\widetilde{PT_{L}}}{(1-2\beta)|L|}\).

But \(\widetilde{OPT_{S}}+\widetilde{OPT_{T}}=OPT_{S}+OPT_{T}-(g_{L}-m^{\prime})(|S| +|T|)\) and \(\widetilde{OPT_{L_{r}}}=OPT_{L_{r}}+(g_{L}-m^{\prime})|L_{r}|\).

So together we have:

\[\tilde{g_{L}}-h_{L} \leq\frac{4}{5}\frac{OPT_{S}+OPT_{T}-(g_{L}-m^{\prime})(|S|+|T|)+ OPT_{L_{r}}+(g_{L}-m^{\prime})(|U|+|V|)}{(1-2\beta)|L|}\] (22) \[=\frac{4}{5}\frac{(OPT_{S}+OPT_{T}+OPT_{L_{r}}+(g_{L}-m^{\prime })(-|S|-|T|+|U|+|V|))}{(1-2\beta)|L|}.\] (23)

Since \(|L^{\prime}|\geq\frac{n}{2}\) it must be the case that \(|S|+|T|\geq\frac{n}{2}-\delta n\) since otherwise there would be strictly less than \(\frac{n}{2}\) elements in \(|L^{\prime}|\). For this reason also \(|U|+|V|\leq\frac{n}{2}\)

So \(-|S|-|T|+|U|+|V|\leq\delta n\). By plugging this in Eq. (22) we get:

\[\tilde{g_{L}}-h_{L}\leq\frac{4}{5}\frac{(OPT_{S}+OPT_{T}+OPT_{L_{r}}+(g_{L}-m ^{\prime})\delta n)}{(1-2\beta)|L|}.\]

Note that \(\tilde{g_{L}}-h_{L}=\tilde{g_{L}}-g_{L}+g_{L}-h_{L}=g_{L}-h_{L}-(g_{L}-\tilde{ g_{L}})=(g_{L}-h_{L})-(g_{L}-m^{\prime})\).

By plugging this in the above we get:

\[g_{L}-h_{L}\leq\frac{4}{5}\frac{\left(OPT_{S}+OPT_{T}+OPT_{L_{r}}+\frac{10}{ 4}(g_{L}-m^{\prime})\delta n\right)}{(1-2\beta)|L|}.\]

Note that \(OPT_{T}=\sum_{i\in T}g_{L}-x_{i}\geq\sum_{i\in T}g_{L}-m^{\prime}\) so \(g_{L}-m^{\prime}\leq\frac{OPT_{T}}{|T|}\).

\[\tilde{g_{L}}-h_{L} \leq\frac{4}{5}\frac{\left(OPT_{S}+OPT_{T}+OPT_{L_{r}}+2.5OPT_{ T}\frac{\delta n}{|T|}\right)}{(1-2\beta)|L|}\] \[\leq\frac{4}{5}\frac{\left(OPT_{S}+OPT_{T}+OPT_{L_{r}}+2OPT_{T} \frac{\delta n}{n/2-\delta n}\right)}{(1-2\beta)|L|}\] \[=\frac{4}{5}\frac{(OPT_{S}+(1+\Theta(\delta))OPT_{T}+OPT_{L_{r}} )}{(1-2\beta)|L|}.\] (24)

and so we get the desired bound (due to the equivalent condition stated in Eq. (8).

We conclude with a final remark:

**Remark 25**.: _Algorithm 3 does not have \((c,\delta)\) approximation robustness for any \(c<\frac{5}{3}\approx 1.667\). This lower bound is obtained by observing the following instance: Let \(\varepsilon>0\) be a small enough value, \(X\in\mathbb{R}^{n}\) where \(\nicefrac{{n}}{{2}}\) points are at \(x=0\), \(\nicefrac{{n}}{{4}}\) points are at \(x=-0.5-\varepsilon\), \(\nicefrac{{n}}{{4}}\) points at \(x=-1\), and one point at \(M\gg n\). Now let \(X^{\prime}\) be obtained from \(X\) by moving the point at \(x=M\) to \(x=-1\). Calculation shows that the above instance leads to a \(\frac{5}{3}\) approximation ratio._

## Appendix I Second Facility Location: Second-Proportional-Mechanism Proofs

In this section we show the strategyproofness and the expected approximation ratio of Algorithm 2.

Proof of Lemma 6.: We follow the proof of (Lu et al., 2010, Thm 4.1). In the proof they show that given that the first facility is at \(x_{k}\) for any \(k\in[n]\), then for any \(X^{\prime}=<x^{\prime}_{i},X_{-i}>\) that is gained from \(X\) by agent \(i\) deviating from \(x_{i}\) to \(x^{\prime}_{i}\), the expected cost of agent \(i\) only increases by deviating to \(x^{\prime}_{i}\). The proof does not depend at all on the location of the first facility. Since the first facility is entirely independent of the agent reported values, then the above implies that the choice of the second facility is also strategy proof. 

Proof of Theorem 7.: The proof is similar in structure to the one given by Lu et al. (2010) for the Proportional-Mechanism; the main difference is a careful (and tight) analysis of Lemma 27 for the real line which leads to an approximation ratio of at most \(3\) rather than \(4\).

Let \(G:=\{g_{S},g_{T}\}\), \(S=\{i\mid x_{i}\in X,d(x_{i},g_{S})\leq d(x_{i},g_{T})\}\) and \(T=[n]\setminus S\).

For any \(M\subseteq X\) let \(OPT_{M}=\operatorname{med}_{2}(M,G)\). So \(OPT_{S}=\operatorname{med}_{1}(S,g_{S}),OPT_{T}=\operatorname{med}_{1}(T,g_ {T})\) and thus \(OPT=\operatorname{med}_{2}(X,G)=OPT_{S}+OPT_{T}\).

Let \(H=(g_{S},h)\) be the algorithms solution for the problem, and \(ALG=\operatorname{med}_{2}(X,H)\). We denote \(ALG_{S},ALG_{T}\) to be \(\operatorname{med}_{2}(S,H),\operatorname{med}_{2}(T,H)\) respectively, so that \(ALG=ALG_{S}+ALG_{T}\) and therefore \(\mathbb{E}[ALG]=\mathbb{E}[ALG_{S}]+\mathbb{E}[ALG_{T}]\).

Since \(ALG_{S}\leq\operatorname{med}_{1}(S,g_{S})=OPT_{S}\), we get that \(\mathbb{E}[ALG_{S}]\leq OPT_{S}\), and we only need to find a bound for \(\mathbb{E}[ALG_{T}]\).

\[\mathbb{E}[ALG_{T}] =\sum_{i\in[n]}\mathbb{E}[ALG_{T}\mid h=x_{i}]Pr(h=x_{i})\] (25) \[=\sum_{i\in T}\mathbb{E}[ALG_{T}\mid h=x_{i}]Pr(h=x_{i})+\sum_{ i\in T}\mathbb{E}[ALG_{T}\mid h=x_{i}]Pr(h=x_{i})\] (26)

In the summation above the first term is the expected cost due to choosing the second facility to be a location in \(S\) and the second term is the expected cost due to choosing the second facility to be a location in \(T\). We will bound each one individually.

For all \(i\in[n]\) let \(a_{i}=d(x_{i},g_{S})\), \(p_{i}=\frac{a_{i}}{\sum_{j\in[n]a_{j}}}\) and \(b_{i}=d(x_{i},g_{T})\). So \(\sum_{i\in S}a_{i}=OPT_{S}\), \(\sum_{i\in T}b_{i}=OPT_{T}\) and \(\sum_{i\in[n]}p_{i}=1\). For each \(i,j\in[n]\) let \(d_{i,j}=d(x_{i},x_{j})\).

**Lemma 26**.: \(\sum_{i\in S}\mathbb{E}[ALG_{T}\mid h=x_{i}]Pr(h=x_{i})\leq OPT_{S}\)__

Proof.: \[\sum_{i\in S}\mathbb{E}[ALG_{T}\mid h=x_{i}]Pr(h=x_{i})=\sum_{i \in S}\Biggl{(}\sum_{t\in T}\min\{a_{t},d_{i,t}\}\Biggr{)}\cdot p_{i}\] \[\stackrel{{(\star)}}{{\leq}}\sum_{i\in S}\Biggl{(} \sum_{t\in T}a_{t}\Biggr{)}\cdot p_{i}=\sum_{i\in S}\biggl{(}\frac{\sum_{t \in T}a_{t}}{\sum_{t\in T}a_{t}+\sum_{s\in S}a_{s}}\biggr{)}\cdot a_{i}\leq \sum_{i\in S}a_{i}=OPT_{S}\]

Where inequality \((\star)\) is gained by ignoring the second facility. 

**Lemma 27**.: \(\sum_{i\in T}\mathbb{E}[ALG_{T}\mid h=x_{i}]Pr(h=x_{i})\leq 3OPT_{T}\)__

Proof.: Let \(D=d(g_{A},g_{B})\).

\[\sum_{i\in T}\mathbb{E}[ALG_{T}\mid h=x_{i}]Pr(h=x_{i})=\sum_{ i\in T}\Biggl{[}\Biggl{(}\sum_{t\in T}\min\{a_{t},d(x_{t},x_{i})\}\Biggr{)} \cdot p_{i}\Biggr{]}\] \[\stackrel{{\text{triangle inequality}}}{{\leq}}\sum_{ i\in T}\Biggl{[}\Biggl{(}\sum_{t\in T}\min\{a_{t},b_{t}+b_{i}\}\Biggr{)} \cdot p_{i}\Biggr{]}=\sum_{i\in T}\Biggl{[}\Biggl{(}\sum_{t\in T}b_{t}+\sum_{ t\in T}\min\{a_{t}-b_{t},b_{i}\}\Biggr{)}\cdot p_{i}\Biggr{]}\] \[=\sum_{i\in T}\Biggl{[}\Biggl{(}\sum_{t\in T}b_{t}\Biggr{)}\cdot p _{i}\Biggr{]}+\sum_{i\in T}\Biggl{[}\Biggl{(}\sum_{t\in T}\min\{a_{t}-b_{t},b_ {i}\}\Biggr{)}\cdot p_{i}\Biggr{]}\]\[=\sum_{i\in T}\Biggl{[}\left(\sum_{t\in T}b_{t}\right)\cdot p_{i} \Biggr{]}+\sum_{i\in T}\Biggl{[}\left(\sum_{t\in T}\min\{a_{t}-b_{t},b_{i}\} \right)\cdot\frac{b_{i}}{\sum_{j\in[n]}a_{j}}\Biggr{]}\] \[+\sum_{i\in T}\Biggl{[}\left(\sum_{t\in T}\min\{a_{t}-b_{t},b_{i} \}\right)\cdot\frac{a_{i}-b_{i}}{\sum_{j\in[n]}a_{j}}\Biggr{]}\]

We bound each of these 3 terms individually by \(OPT_{T}\), and thus get the desired bound.

The first term bound:

\[\sum_{i\in T}\Biggl{[}\left(\sum_{t\in T}b_{t}\right)\cdot p_{i} \Biggr{]}=\left(\sum_{t\in T}b_{t}\right)\sum_{i\in T}p_{i}\leq OPT_{T}\] (27)

The second term bound:

\[\sum_{i\in T}\Biggl{[}\left(\sum_{t\in T}\min\{a_{t}-b_{t},b_{i} \}\right)\cdot\frac{a_{i}-b_{i}}{\sum_{j\in[n]}a_{j}}\Biggr{]}\leq\sum_{i\in T }\Biggl{[}\left(\sum_{t\in T}b_{i}\right)\cdot\frac{a_{i}-b_{i}}{\sum_{j\in[n ]}a_{j}}\Biggr{]}=\sum_{i\in T}\Biggl{[}|T|b_{i}\cdot\frac{a_{i}-b_{i}}{\sum_{ j\in[n]}a_{j}}\Biggr{]}\] (28)

We finish the third term bound by showing the following claim

**Claim 28**.: \[\sum_{i\in T}\Biggl{[}|T|b_{i}\cdot\frac{a_{i}-b_{i}}{\sum_{j\in[n]}a_{j}} \Biggr{]}\leq OPT_{T}\]

Proof.: Let \(T_{a}\) be the part of \(T\) closer to \(g_{S}\) and \(T_{b}\) be the other part of \(T\).

So \(T_{a}=\{i\mid i\in T,d(x_{i},g_{S})\leq d(x_{i},g_{T})\}\), and \(T_{b}=T\setminus T_{a}\). Since the points are all on the line metric:

For all \(i\in T_{a}\): \(a_{i}=d(g_{S},x_{i})=d(g_{S},g_{T})-b_{i}=D-b_{i}\).

For all \(i\in T_{b}\): \(a_{i}=d(g_{S},x_{i})=d(g_{S},g_{T})+b_{i}=D+b_{i}\).

Let us use these facts in Eq. (28) to get:

\[\sum_{i\in T}\Biggl{[}|T|b_{i}\cdot\frac{a_{i}-b_{i}}{\sum_{j\in[ n]}a_{j}}\Biggr{]}=\sum_{i\in T_{a}}\Biggl{[}|T|b_{i}\cdot\frac{D-b_{i}-b_{i}}{ \sum_{j\in[n]}a_{j}}\Biggr{]}+\sum_{i\in T_{b}}\Biggl{[}|T|b_{i}\cdot\frac{D+ b_{i}-b_{i}}{\sum_{j\in[n]}a_{j}}\Biggr{]}\] \[=\frac{|T|D}{\sum_{j\in[n]}a_{j}}\Biggl{(}\sum_{i\in T}b_{i}\Biggr{)} -\frac{2|T|\sum_{i\in T_{a}}b_{i}^{\ 2}}{\sum_{j\in[n]}a_{j}}=\frac{|T|D\cdot OPT_{T}-2|T|\sum_{i\in T_{a}}b_{i} ^{\ 2}}{\sum_{j\in[n]}a_{j}}\] \[\stackrel{{(\star)}}{{\leq}}\frac{|T|D\cdot OPT_{T}-2( OPT_{T_{a}})^{2}}{\sum_{j\in[n]}a_{j}}\stackrel{{(\star\star)}}{{\leq}} \frac{|T|D-2\frac{(OPT_{T_{a}})^{2}}{OPT_{T}}}{|T|D+OPT_{T_{b}}-OPT_{T_{a}}} \cdot OPT_{T}\] (29)

\((\star)\) is due to \(QM-AM\) inequality since

\[\sum_{i\in T_{a}}b_{i}^{\ 2}\geq\frac{\left(\sum_{i\in T_{a}}b_{i}\right)^{2} \ |T_{a}|\stackrel{{|T_{a}|\leq|T|}}{{\geq}}(OPT_{T_{a}})^{2}}{|T|}.\]\((\star\star)\) explanation:

\[\sum_{j\in[n]}a_{j}=\sum_{j\in S}a_{j}+\sum_{j\in T_{a}}a_{j}+\sum_{ j\in T_{b}}a_{j}=OPT_{S}+\sum_{j\in T_{a}}D-b_{j}+\sum_{j\in T_{b}}D+b_{j}\] \[=OPT_{S}+D|T|+OPT_{T_{b}}-OPT_{T_{a}}\geq|T|D+OPT_{T_{b}}-OPT_{T_{ a}}\]

We now show that

\[\frac{|T|D-2\frac{(OPT_{T_{a}})^{2}}{OPT_{T}}}{|T|D+OPT_{T_{b}}-OPT_{T_{a}}}\leq 1.\]

Indeed, if \(OPT_{T_{b}}\geq OPT_{T_{a}}\) then \(|T|D+OPT_{T_{b}}-OPT_{T_{a}}\geq|T|D\) and thus:

\[\frac{|T|D-2\frac{(OPT_{T_{a}})^{2}}{OPT_{T}}}{|T|D+OPT_{T_{b}}-OPT_{T_{a}}} \leq\frac{|T|D-2\frac{(OPT_{T_{a}})^{2}}{OPT_{T}}}{|T|D}\leq\frac{|T|D}{|T|D}=1.\]

Otherwise \(OPT_{T_{a}}-OPT_{T_{b}}>0\) and thus:

\[OPT_{T_{a}}>OPT_{T_{b}}\implies\] \[OPT_{T}=OPT_{T_{a}}+OPT_{T_{b}}<2OPT_{T_{a}}\implies\] \[(OPT_{T_{a}}-OPT_{T_{b}})OPT_{T}\leq 2(OPT_{T_{a}}-OPT_{T_{b}})OPT _{T_{a}}=2(OPT_{T_{a}})^{2}-2OPT_{T_{b}}OPT_{T_{a}}\leq 2(OPT_{T_{a}})^{2} \implies\] \[OPT_{T_{a}}-OPT_{T_{b}}\leq\frac{2(OPT_{T_{a}})^{2}}{OPT_{T}} \implies\frac{|T|D-2\frac{(OPT_{T_{a}})^{2}}{OPT_{T}}}{|T|D+OPT_{T_{b}}-OPT_ {T_{a}}}\leq\frac{|T|D+OPT_{T_{b}}-OPT_{T_{a}}}{|T|D+OPT_{T_{b}}-OPT_{T_{a}}}=1\]

So from Eq. (29): \(\sum_{i\in T}\Bigl{[}|T|b_{i}\cdot\frac{a_{i}-b_{i}}{\sum_{j\in[n]}a_{j}} \Bigr{]}\leq OPT_{T}\).

To summarize: \(\sum_{i\in T}\mathbb{E}[ALG_{T}\mid h=x_{i}]Pr(h=x_{i})\leq OPT_{T}+OPT_{T}+ OPT_{T}=3OPT_{T}\). 

From plugging Lemma 26, Lemma 27 into Eq. (26) we get:

\[\mathbb{E}[ALG]\leq 2OPT_{S}+3OPT_{T}\leq 3OPT.\]

**Remark 29**.: _The upper bound of \(3\) in the analysis of Theorem 7 is tight._

To show tightness, consider an instance with \(n\) points on the real line: \(\nicefrac{{n}}{{2}}\) points at \(x=0\), \(\nicefrac{{n}}{{2}}-1\) points at \(x=1\), and a single point at \(x=2\). Assume the given first facility is at \(0\). The optimal solution puts the second facility at \(1\) and pays the cost of \(1\). On the other hand \(\mathbb{E}[ALG]=\frac{\frac{1}{n}+1}{\frac{n-2}{2}+2\frac{n-2}{2}}=3\nicefrac{{ n}/{2}}{{n}/{2}+1}\approx 3\).

## Appendix J Proof of Theorem 9

Proof.: The strategyproofness is due to the fact that the first facility is chosen based only using the predictions, and the second facility choice is strategyproof due to Lemma 6.

Let \(G=(g_{L},g_{R})\) be the optimal cluster centers for \(X\), and let \(L,R\) be the respective corresponding clusters. We consider two cases: when this optimal clustering is \(b\delta\)-balanced, and when it is not; in both cases we show that the algorithm achieves low expected cost. Let \(H=(h_{1},h_{2})\) be the solution returned by Algorithm 4.

The first case is when \((L,R)\) is a \(b\delta\)-balanced clustering of \(X\). In this case we claim that

\[\mathbb{E}[\operatorname{med}_{2}(X,H)]\leq(3.6+O(\delta))\operatorname{med}_{ 2}(X,G).\]Indeed, let \(T=(t_{L},t_{R})\) be the two centers of the \((b-1)\delta\)-balanced \(2\)-medians algorithm on \(X^{\prime}\). Since \(G\) induces a \(b\delta\)-balanced clustering, Theorem 4 and our choice of \(b\) ensures that

\[\operatorname{med}_{2}(X,T)\leq 1.2\operatorname{med}_{2}(X,G).\]

Since Algorithm 3 returns one of the centers of the \((b-1)\delta\)-balanced \(2\)-medians algorithm on \(X^{\prime}\) as \(h_{1}\), let us assume w.l.o.g. that Big-Cluster-Center returns \(h_{1}=t_{L}\). Let \((X_{L},X_{R})\) and \((X^{\prime}_{L},X^{\prime}_{R})\) be the clusterings of \(X\) and \(X^{\prime}\) induced by \(T=\{t_{L},t_{R}\}\). From Theorem 7:

\[\begin{array}{ll}E[\operatorname{med}_{2}(X,H)]_{(\stackrel{{ \longrightarrow}}{{\longrightarrow}})}&\mathbb{E}[\operatorname{med}_{2}(X, \{t_{L},h_{2}\})]\leq 2\operatorname{med}_{1}(X_{L},t_{L})+3 \operatorname{med}_{1}(X_{R},median(X_{R}))\\ &\leq 2\operatorname{med}_{1}(X_{L},t_{L})+3(1+O(\delta))\operatorname{med} _{1}(X_{R},t_{R})\\ &\leq(3+O(\delta))\operatorname{med}_{2}(X,T)\leq(3.6+O(\delta)) \operatorname{med}_{2}(X,G),\end{array}\]

where the inequality \((\star)\) uses the fact that \(X_{R}\) and \(X^{\prime}_{R}\) differ on at most \(\delta|X|\) points, and hence we can sue \((1+O(\delta),\delta)\) approximation-robustness of \(1\)-Median from Corollary 3.

Now for the other case: suppose \((L,R)\) is a \(b\delta\)-unbalanced clustering. Theorem 8 implies that either \(\operatorname{med}_{1}(L,h_{1})\leq(1.8+O(\delta))\operatorname{med}_{1}(L,g_ {L})\) or \(\operatorname{med}_{1}(R,h_{1})\leq(1.8+O(\delta))\operatorname{med}_{1}(R,g_ {R})\). W.l.o.g., consider the first option. Then from Theorem 7:

\[\begin{array}{ll}E[\operatorname{med}_{2}(X,H)]&\leq 2\operatorname{med}_{1 }(L,h_{1})+3\operatorname{med}_{1}(R,g_{R})\\ &\leq 2((1.8+O(\delta))\operatorname{med}_{1}(L,g_{L}))+3\operatorname{med }_{1}(R,g_{R})\leq(3.6+O(\delta))OPT.\end{array}\]

Combining the two cases completes the proof. 

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes]
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 7, where we list possible directions to improve what we have done in this work, and directions for future work.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA]
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA]
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA]
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA]
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA]
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes]
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: It is a theoretic paper discussing beyond worst case analysis of algorithms with predictions.

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA]
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]