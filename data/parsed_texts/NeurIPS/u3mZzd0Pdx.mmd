Lower Bounds of Uniform Stability in Gradient-Based Bilevel Algorithms for Hyperparameter Optimization

Rongzhen Wang\({}^{1,2}\), Chenyu Zheng\({}^{1,2}\), Guoqiang Wu\({}^{3}\),

**Xu Min\({}^{4}\), Xiaolu Zhang\({}^{4}\), Jun Zhou\({}^{4}\), Chongxuan Li\({}^{1,2}\)**

\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) Beijing Key Laboratory of Big Data Management and Analysis Methods

\({}^{3}\) School of Software, Shandong University \({}^{4}\) Ant Group

{wangrz,cyzheng,chongxuanli}@ruc.edu.cn; guoqiangwu@sdu.edu.cn; minxu.mx@antgroup.com; {yueyin.zxl,jun.zhoujun}@antfin.com

Correspondence to Chongxuan Li.

###### Abstract

Gradient-based bilevel programming leverages unrolling differentiation (UD) or implicit function theorem (IFT) to solve hyperparameter optimization (HO) problems, and is proven effective and scalable in practice. To understand the generalization behavior, existing works establish upper bounds on the uniform stability of these algorithms, while their tightness is still unclear. To this end, this paper attempts to establish stability lower bounds for UD-based and IFT-based algorithms. A central technical challenge arises from the dependency of each outer-level update on the concurrent stage of inner optimization in bilevel programming. To address this problem, we introduce lower-bounded expansion properties to characterize the instability in update rules which can serve as general tools for lower-bound analysis. These properties guarantee the hyperparameter divergence at the outer level and the Lipschitz constant of inner output at the inner level in the context of HO. Guided by these insights, we construct a quadratic example that yields tight lower bounds for the UD-based algorithm and meaningful bounds for a representative IFT-based algorithm. Our tight result indicates that uniform stability has reached its limit in stability analysis for the UD-based algorithm.

## 1 Introduction

Hyperparameters significantly influence the convergence behavior of learning algorithms as well as the efficiency and generalization performance of the trained model [1, 2, 3]. _Hyperparameter optimization_ (HO) algorithms aim to find the best hyperparameters (associated with the optimized model parameters) on a validation set. Classical approaches for HO include grid search [4], random search [5], Bayesian optimization [6, 7, 8], and evolutionary algorithms [9, 10], which often suffer from the problem of scaling up. Recently, gradient-based methods have achieved excellent empirical performance in high-dimensional HO problems [11, 3, 12].

In gradient-based methods, HO is formulated as a bilevel programming problem. The inner level seeks the best model parameters on the training set given current hyperparameters. In the outer level, hyperparameters are optimized with gradient descent. However, the gradient is difficult to compute as it requires differentiating the optimized model parameters w.r.t. the hyperparameters. Two mainstream strategies have been developed to obtain this Jacobian by explicitly _unrolling differentiation_ (UD) [13, 2, 14] or approximately applying the _implicit function theorem_ (IFT) [1, 15, 16, 12].

To investigate the underlying reason for their success, existing work establishes generalization upper bounds based on algorithmic stability [17; 18]. In particular, [17] presents a generalization framework associated with a notion of uniform stability for general bilevel programming in HO and stability upper bounds for the UD-based algorithm. Despite their efforts, such algorithms have not been fully understood and one of the key unsolved problems is whether existing stability analyses are tight.

To this end, this paper establishes lower bounds on the stability of gradient-based bilevel programming algorithms for HO. Technically, we begin by introducing _lower-bounded expansion properties_ which inherently characterize the instability in general update rules including stochastic gradient descent (SGD) as detailed in Section 4. Our expansion properties, to a certain degree, mirror those introduced by [19], with the distinction being our emphasis on lower bounds rather than upper bounds. This approach not only enables a comparative analysis with upper bounds to evaluate their alignment (i.e., tightness) but also lays down a conceptual framework for analyzing the lower bounds of algorithmic stability, generally applicable to both single-level SGD and various bilevel algorithms.

Building upon these properties, we explore the stability of the UD-based algorithm in Section 5. We first present a recursive stability lower bound that aligns with the existing upper bound at the outer level given the expansion properties of the compound validation loss, followed by an analysis of the Lipschitz constant of the inner output to maximize those expansion coefficients. Guided by these theoretical insights, we construct a quadratic example that yields a tight lower bound for the UD-based algorithm with constant step sizes and a nearly tight lower bound with linearly decreasing step sizes with respect to key factors. Meaningful bounds for a representative IFT-based algorithm are also provided in Appendix C based on its essential connection to UD-based methods. We highlight that the example is carefully designed to obtain explicit stability lower bounds by overcoming the challenges posed by the intricate behavior of the bilevel algorithms, i.e. the dependence of each outer-level update on the current turn of inner optimization.

We outline our contributions as follows: (1) We introduce lower-bounded expansion properties that can serve as general tools for analyzing lower bounds of the stability in gradient descent. (2) To our knowledge, we present the first lower bounds of uniform stability for both the UD-based and representative IFT-based algorithms, facing the challenge posed by the intricate formulation of the outer update in bilevel optimization. (3) Our lower bounds match existing upper bounds for the UD-based algorithm, verifying that uniform stability has reached its limit in characterizing the generalization of the UD-based algorithm. Detailed results are summarized in Table 1.

## 2 Related work

Algorithmic stability [20; 21] measures the change in the model output when a single training example is replaced. It is shown to be sufficient and necessary for learnability in certain cases [22]. Stability-based generalization analysis of an algorithm typically consists of three key elements: a notion of stability, a stability-based generalization bound, and a stability analysis depending on the algorithm. Below, we introduce the related work based on these three elements.

**Algorithmic stability.**[23] introduce uniform stability, which characterizes the worst-case change of loss and presents a stability-based generalization bound with high probability. Notable efforts [24; 25; 26; 27] have been made to obtain sharper bounds for uniformly stable algorithms in general. Besides the uniform stability, various notions of stability that characterize the average change [22], local change [28], or change in the hypothesis [29; 30] are investigated for fine-grained analyses.

**Stability of stochastic gradient descent (SGD).** SGD has been one of the workhorses in deep learning and therefore attracted much attention. To unravel the mystery behind its success, [19] analyze its (randomized) uniform stability on (strongly) convex and nonconvex losses. [31] analyze the uniform stability of (S)GD for nonsmooth convex losses and provide sharp upper and lower expectation bounds. [32; 30] consider the on-average stability for SGD and build data-dependent generalization bounds to explain the effectiveness of practical techniques like proper initialization.

Recent work establishes lower bounds on the uniform stability of SGD and investigates the tightness of corresponding upper bounds. [33] proves a general minimax optimal lower bound for stability generalization error together with optimization error on convex and smooth losses. [31] finds the general technique proposed by [33] is sub-optimal in convex but nonsmooth cases, and providessharper lower bounds by constructing a special class of loss functions. [34] adopts a similar approach by construction to present lower bounds for smooth and potentially nonconvex loss functions.

In this paper, we focus on the smooth and nonconvex cases in HO and provide stability lower bounds by construction as [31] and [34]. Compared with [34], we consider a more complicated and nontrivial bilevel optimization problem, where the interaction between inner and outer processes brings a significant impact on the analysis. A detailed comparison is provided in Section 5.3 and Appendix E.

**Stability for bilevel programming.**[17] extend the notion of uniform stability to HO and analyze stability upper bounds of the UD-based algorithm, while the tightness of their result is largely open. Recently, it has been extended to the analysis of implicit gradient algorithms [18]. This paper provides the first lower bounds, generally applying to two main categories of gradient-based HO methods. There are other bilevel optimization algorithms [12; 35; 36; 37; 38; 39; 15] and settings [2; 16; 3; 40; 41; 42; 43] where our framework can potentially be extended in future work.

## 3 Problem formulation

### Elementary notations and definitions

**Scalar, vector, and matrix.** We employ lowercase letters (e.g., \(a\)), lowercase boldface letters (e.g., \(\bm{a}\)), and uppercase boldface letters (e.g., \(\bm{A}\)) to denote scalars, vectors, and matrices, respectively. For a vector \(\bm{a}\), \(\|\cdot\|\) denotes its Euclidean norm. For a matrix \(\bm{A}\), \(\|\cdot\|\) denotes its spectral norm. Additionally, let \(\bm{a}_{1}\triangleq\bm{a}_{2}\) denote \(\bm{a}_{1}\) and \(\bm{a}_{2}\) are collinear by a non-negative factor, namely, \(\exists a\geq 0\) s.t. \(\bm{a}_{1}=a\bm{a}_{2}\).

**Loss function.** A differentiable function \(\ell:\Omega\rightarrow\mathbb{R}\) is \(L\)-Lipschitz continuous if \(\forall\bm{u},\bm{v}\in\Omega\), \(\|\ell(\bm{u})-\ell(\bm{v})\|\leq L\|\bm{u}-\bm{v}\|\). It is \(\gamma\)-smooth if \(\forall\bm{u},\bm{v}\in\Omega\), \(\|\nabla\ell(\bm{u})-\nabla\ell(\bm{v})\|\leq\gamma\|\bm{u}-\bm{v}\|\).

**Twin datasets.** A pair of datasets are considered _twin datasets_ if they differ in only a single data point, denoted by \(S\simeq\bar{S}\). Throughout this paper, we use a tilde symbol to distinguish their corresponding notions, e.g., examples \(\bm{z}\) and \(\tilde{\bm{z}}\), output parameters \(\bm{w}\) and \(\tilde{\bm{w}}\).

**Asymptotic notations.** Denote with \(a_{n}\lesssim b_{n}\) that \(a_{n}\) is bounded above by \(b_{n}\) up to a constant factor for sufficiently large \(n\), and conversely by \(a_{n}\gtrsim b_{n}\). We say \(a_{n}\asymp b_{n}\) if \(a_{n}\lesssim b_{n}\) and \(a_{n}\gtrsim b_{n}\).

### HO as bilevel programming

Denote the testing, validation, and training distributions on the data space \(\mathcal{Z}\) by \(\mathcal{D}^{\mathrm{test}}\), \(\mathcal{D}^{\mathrm{val}}\) and \(\mathcal{D}^{\mathrm{tr}}\), and corresponding losses by \(\ell^{\mathrm{test}}\), \(\ell^{\mathrm{val}}\) and \(\ell^{\mathrm{tr}}\). Since the validation phase is generally regarded as a rehearsal for testing, \(\mathcal{D}^{\mathrm{val}}\) and \(\ell^{\mathrm{val}}\) are commonly assumed to be consistent with \(\mathcal{D}^{\mathrm{test}}\) and \(\ell^{\mathrm{test}}\).

Given a validation set \(S^{\mathrm{val}}=\{\bm{z}_{i}^{\mathrm{val}}\}_{i=1}^{m}\overset{\mathrm{i.i.d. }}{\sim}\left(\mathcal{D}^{\mathrm{val}}\right)^{m}\) and a training set \(S^{\mathrm{tr}}=\{\bm{z}_{j}^{\mathrm{tr}}\}_{j=1}^{n}\overset{\mathrm{i.i.d. }}{\sim}\left(\mathcal{D}^{\mathrm{tr}}\right)^{n}\), HO algorithms seek the best-performing hyperparameter-parameter pair on \(S^{\mathrm{val}}\). Denote by \(\bm{\lambda}\) the hyperparameter in space \(\Lambda\), \(\bm{\theta}\) the (model) parameter in space \(\Theta\). This process can be formulated as the following bilevel problem:

\[\hat{\bm{\lambda}}\approx\operatorname*{arg\,min}_{\bm{\lambda}\in\Lambda} \frac{1}{m}\sum_{i=1}^{m}\ell^{\mathrm{val}}(\bm{\lambda},\hat{\bm{\theta}}( \bm{\lambda});\bm{z}_{i}^{\mathrm{val}}),\text{ where }\hat{\bm{\theta}}(\bm{\lambda}) \approx\operatorname*{arg\,min}_{\bm{\theta}\in\Theta}\frac{1}{n}\sum_{j=1}^{ n}\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta};\bm{z}_{j}^{\mathrm{tr}}).\]

Here, \(\hat{\bm{\theta}}(\bm{\lambda})\) is selected by its training performance under the given \(\bm{\lambda}\) and \(\ell^{\mathrm{val}}(\bm{\lambda},\hat{\bm{\theta}}(\bm{\lambda});\bm{z})\) can be rewritten as a _compound validation loss_\(\mathcal{L}(\bm{\lambda};\bm{z})\) considering \(\hat{\bm{\theta}}(\bm{\lambda})\) a function of \(\bm{\lambda}\).

Various methods are proposed to solve this nested problem, among which gradient-based algorithms have recently achieved success in scalability [11; 3; 12]. As shown in Algorithm 1, gradient-based methods utilize SGD as the optimizer at both levels, where the primary challenge lies in the calculation of the gradient of the compound validation loss, called _hypergradient_,

\[\nabla_{\bm{\lambda}}\mathcal{L}(\bm{\lambda};\bm{z})=\nabla_{\bm{\lambda}} \ell^{\mathrm{val}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda});\bm{z})+ \underbrace{\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})}_{\text{inner Jacobian}}\nabla_{\bm{\theta}}\ell^{ \mathrm{val}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda});\bm{z}),\] (1)where the _inner Jacobian_ involves differentiating through the inner-level optimization. To this end, the UD-based methods obtain the exact inner Jacobian by directly unrolling the inner differentiation:2

Footnote 2: For formula neatness, we set an unused \(\eta_{K+1}=0\) as a placeholder, and similarly \(\alpha_{T+1}=0\) in Eq. (6).

\[\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})=-\sum_{k=0}^{K-1}\eta_{k+1} \nabla_{\bm{\theta}\bm{\lambda}}^{2}\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta }_{k})\prod_{i=k+1}^{K}(\bm{I}-\eta_{i+1}\nabla_{\bm{\theta}\bm{\theta}}^{2} \ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta}_{i})).\] (2)

While representative IFT-based methods leverage the implicit function theorem and Neumann series to obtain an alternative estimation [12]:

\[\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{\lambda})=-\eta_{K}\nabla_ {\bm{\theta}\bm{\lambda}}^{2}\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta}_{K} )\sum_{k=0}^{K-1}\Bigl{[}\bm{I}-\eta_{K}\nabla_{\bm{\theta}\bm{\theta}}^{2} \ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta}_{K})\Bigr{]}^{k}.\] (3)

Please refer to Algorithm 1 for the whole process. Notably, this paper adopts a common theoretical assumption [19; 17] of constant inner step sizes and decreasing outer step sizes.3

Footnote 3: Namely, \(\eta_{k}=\eta\) and \(\alpha_{t}\leq\frac{c}{t}\), with a constant \(c>0\). The decreasing step size is also widely adopted in the optimization convergence analysis works, such as SGD [44; 45], AdaGrad [46], Adam [47; 48].

```
1:Input: Initialization \(\bm{\lambda}_{0}\) and \(\bm{\theta}_{0}\); training set \(S^{\mathrm{tr}}\) and validation set \(S^{\mathrm{val}}\); step size scheme \(\alpha\) and \(\eta\)
2:Output: The hyperparameter \(\bm{\lambda}_{T}\) and hypothesis \(\bm{\theta}_{K}\)
3:for\(t=1\)to\(T\)do
4:for\(k=1\)to\(K\)do
5: uniformly sampling \(j_{k}\) from \([n]\)
6:\(\bm{\theta}_{k}\leftarrow\bm{\theta}_{k-1}-\eta_{k}\nabla\bm{\theta}\ell^{ \mathrm{tr}}(\bm{\lambda}_{t-1},\bm{\theta}_{k-1};\bm{z}_{j_{k}}^{\mathrm{tr}})\)
7:endfor
8: uniformly sampling \(i_{t}\) from \([m]\)
9:\(\bm{g}\leftarrow\nabla\mathcal{L}(\bm{\lambda}_{t-1};\bm{z}_{i_{t}}^{\mathrm{ val}})\quad\triangleright\) UD-based algorithm in Eq. (2), IFT-based algorithm in Eq. (3)
10:\(\bm{\lambda}_{t}\leftarrow\bm{\lambda}_{t-1}-\alpha_{t}\bm{g}\)
11:endfor
12:return\(\bm{\lambda}_{T}\) and \(\bm{\theta}_{K}\) ```

**Algorithm 1** Gradient-based bilevel HO

### Generalization and stability of HO

The generalization behavior of HO algorithms characterizes the selected model's potential performance on the unseen test data. Specifically, denoting the hyperparameter output by a stochastic HO algorithm \(\mathcal{A}\) as \(\mathcal{A}(S^{\mathrm{val}},S^{\mathrm{tr}})\), we are interested in the difference between its expected testing risk and empirical validation risk, namely _generalization error_ defined as

\[\epsilon_{\mathrm{gen}}\coloneqq\mathbb{E}_{\mathcal{A},S^{\mathrm{val}},S^{ \mathrm{tr}}}\left[\mathbb{E}_{\bm{z}\sim\mathcal{D}^{\mathrm{test}}}[\mathcal{ L}(\mathcal{A}(S^{\mathrm{val}},S^{\mathrm{tr}});\bm{z})]-\frac{1}{m}\sum_{i=1}^{m} \mathcal{L}(\mathcal{A}(S^{\mathrm{val}},S^{\mathrm{tr}});\bm{z}_{i}^{\mathrm{ val}})\right].\] (4)

Stability-based generalization theory turns this problem into measuring the algorithmic robustness. We define the notion of _uniform argument stability_ for HO algorithms, which captures the variation in algorithm outputs when replacing a single validation point.4

Footnote 4: Namely, \(\eta_{k}=\eta\) and \(\alpha_{t}\leq\frac{c}{t}\), with a constant \(c>0\). The decreasing step size is also widely adopted in the optimization convergence analysis works, such as SGD [44; 45], AdaGrad [46], Adam [47; 48].

**Definition 3.1** (Uniformly argument stability on validation).: A stochastic HO algorithm \(\mathcal{A}\) is \(\epsilon_{\mathrm{arg}}\)-uniformly argument stable on validation where

\[\epsilon_{\mathrm{arg}}\coloneqq\sup_{S^{\mathrm{val}}\subseteq\tilde{S}^{ \mathrm{val}}\in\mathbb{Z}^{m},S^{\mathrm{tr}}\in\mathbb{Z}^{n}}\mathbb{E}_{ \mathcal{A}}\bigl{[}\|\mathcal{A}(S^{\mathrm{val}},S^{\mathrm{tr}})- \mathcal{A}(\tilde{S}^{\mathrm{val}},S^{\mathrm{tr}})\|\bigr{]}.\] (5)

Our analysis mainly leverages this notion following [31], as it is the key measure for stability bounds under the Lipschitz continuous condition. \(\epsilon_{\mathrm{arg}}\) differs from the uniform stability \(\epsilon_{\mathrm{stab}}\) defined in [17; Definition 1] only by a Lipschitz constant \(L\) (i.e., \(\epsilon_{\mathrm{stab}}\leq L\epsilon_{\mathrm{arg}}\)), and our results for \(\epsilon_{\mathrm{stab}}\) are also provided in Theorem 5.6 and Theorem C.7 for direct comparison with former works. Existing stability-based generalization bound [17, Theorem 1] shows that uniform stability guarantees generalization in expectation for HO algorithms that \(\epsilon_{\mathrm{gen}}\leq\epsilon_{\mathrm{stab}}\).

Former work [17] constructs the first stability upper bound for UD-based HO algorithms. This result is fundamentally based on an upper-bounded recurrence relation of the distance between the outputs respectively optimized on twin validation sets, denoted by \(\delta_{t}\coloneqq\|\bm{\lambda}_{t}-\bm{\tilde{\lambda}}_{t}\|\) at the \(t\)-th step.

**Theorem 3.2** (Recursion upper bound for UD, Theorems 2 and 3, [17]).: _Suppose the compound validation loss \(\mathcal{L}(\cdot;\bm{z})\) is \(L\)-Lipschitz continuous and \(\gamma\)-smooth for all \(\bm{z}\in\mathbb{Z}\), and the training loss \(\ell^{\mathrm{tr}}(\bm{\lambda},\cdot;\bm{z})\) is \(\gamma^{\mathrm{tr}}\)-smooth for all \(\bm{\lambda}\in\Lambda\) and \(\bm{z}\in\mathbb{Z}\). Then for all \(1\leq t\leq T\), \(\mathbb{E}_{\mathcal{A}}[\delta_{t}]\leq\big{[}1+(1-1/m)\alpha_{t}\gamma\big{]} \mathbb{E}_{\mathcal{A}}[\delta_{t-1}]+\frac{2\alpha_{t}L}{m},\) where \(L\lesssim(1+\eta\gamma^{\mathrm{tr}})^{K},\gamma\lesssim(1+\eta\gamma^{ \mathrm{tr}})^{2K}\)._

Unrolling this recursion, we directly get the stability upper bound in recursion form:

\[\epsilon_{\mathrm{arg}}\leq\sum_{t=1}^{T}\prod_{s=t+1}^{T+1}\big{(}1+\alpha_{ s}(1-1/m)\gamma\big{)}\frac{2\alpha_{t}L}{m}.\] (6)

As this result does not explicitly display its order w.r.t. \(T\) under decreasing step sizes \(\alpha_{t}\leq c/t\), [17] further deforms Eq. (6) with the bounded loss condition to obtain \(\epsilon_{\mathrm{stab}}\lesssim T^{\frac{(1-1/m)\gamma c}{(1-1/m)\gamma c+1} }/m\).

[18] analyzes a specific IFT-based algorithm, which, under certain assumptions, achieves a similar result of \(\epsilon_{\mathrm{stab}}\lesssim T^{q}/m\) with \(q<1\). Though these stability upper bounds have been established, their tightness is rarely explored, and the stability of IFT-based algorithms remains largely open. Therefore, this paper takes a first step towards establishing stability lower bounds (namely, how unstable an algorithm can be) for both UD-based and IFT-based HO algorithms.

## 4 Expansion properties of update rules

This paper endeavors to establish tight lower bounds for uniform (argument) stability as defined in Definition 3.1, which is fundamentally the supremum of the output divergence. For iterative algorithms, this divergence accumulates recursively across the whole optimization process. Therefore, we first introduce _lower-bounded expansion properties_ in Section 4.1 to characterize update rules that will induce guaranteed divergence at each iteration. This is followed by an analysis in Section 4.2 on how the objective functions within SGD need to be structured to satisfy these properties. We will see in Section 5 that, for gradient-based HO algorithms, these properties jointly lead to a lower-bounded divergence recursion given the outer-level update properties in Theorem 5.1 and a lower-bounded Lipschitz constant of the inner output given the inner-level update properties in Theorem 5.2.

Our expansion properties correspond, to some extent, with those presented by [19] and the key difference lies in our focus on lower bounds rather than upper bounds. This approach not only facilitates comparisons with upper bounds to discuss their alignments (i.e. tightness) but also provides a general framework for analyzing the lower bounds of algorithmic stability.

### Lower-bounded expansion properties of general iterative algorithms

Let \(\bm{w}\) be a general notation for parameters (or hyperparameters) in space \(\Omega\). An update rule is a function \(G:\Omega\to\Omega\) that maps \(\bm{w}\) to its next state \(G(\bm{w})\), and an iterative algorithm is composed of a series of consecutive update rules. We denote two sequences of update rules by \(\{G_{t}\}_{t=1}^{T}\) and \(\{G^{\prime}_{t}\}_{t=1}^{T}\), and the corresponding outputs by \(\{\bm{w}_{t}\}_{t=1}^{T}\) and \(\{\bm{w}^{\prime}_{t}\}_{t=1}^{T}\).

Intrinsically, the divergence between \(\bm{w}_{t}\) and \(\bm{w}^{\prime}_{t}\) dynamically evolves across the entire process, driven by two factors: disparity in current update rules, and difference in current parameters resulting from prior updates. Our goal is to systematically analyze how variations between two update sequences lead to substantial divergence in outputs. In the following, we introduce Definition 4.1 and Definition 4.2 correspondingly to characterize properties of update rules leading to increasing divergence.

**Definition 4.1** (\(\sigma\)-divergent).: Two update rules \(G\) and \(G^{\prime}\) are \(\sigma\)_-divergent along_\(\bm{v}\) if for all \(\bm{w}\in\Omega\),

\[G(\bm{w})-G^{\prime}(\bm{w})\stackrel{{\pm}}{{=}}\bm{v},\|G(\bm {w})-G^{\prime}(\bm{w})\|\geq\sigma.\]

**Definition 4.2** (\(\rho\)-growing).: An update rule \(G\) is \(\rho\)_-growing along_\(\bm{v}\) if for all \(\bm{w},\bm{w}^{\prime}\in\Omega\) such that \(\bm{w}-\bm{w}^{\prime}\) parallel with \(\bm{v}\),

\[G(\bm{w})-G(\bm{w}^{\prime})\stackrel{{\pm}}{{=}}\bm{w}-\bm{w}^{ \prime},\|G(\bm{w})-G(\bm{w}^{\prime})\|\geq\rho\|\bm{w}-\bm{w}^{\prime}\|.\]Intuitively, \(\sigma\)-divergent update rules produce sufficiently divergent output parameters and a \(\rho\)-growing update rule scales the divergence between parameters with a sufficiently large factor. The direction \(\bm{v}\) is chosen as the most expansive direction, as detailed with a concrete example in Section 5.4.

### Lower-bounded expansion properties of SGD

One-step SGD can be generally formulated as \(G_{\ell,\alpha}(\bm{w})=\bm{w}-\alpha\nabla\ell(\bm{w})\), where the loss function directly impacts this gradient-based update rule. We now define the \(\mu\)-expansive property for the loss function which leads to the growing property of SGD.

**Definition 4.3** (\(\mu\)-expansive).: A differentiable function \(\ell:\Omega\to\mathbb{R}\) is \(\mu\)-expansive along \(\bm{v}\) if for all \(\bm{w},\bm{w}^{\prime}\in\Omega\) that \(\bm{w}-\bm{w}^{\prime}\) parallel with \(\bm{v}\), there exists \(\mu_{\bm{w},\bm{w}^{\prime}}\geq\mu\) such that

\[\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime})=-\mu_{\bm{w},\bm{w}^{\prime}} (\bm{w}-\bm{w}^{\prime}).\]

This paper mainly focuses on the case when \(\mu>0\) where the loss function is nonconvex. We have \(\mu\leq 0\) for the convex case. When \(\mu>0\), Definition 4.3 connects to \(\mu\)-strongly concavity.5 These concepts are equivalent in the one-dimensional case. In general, \(\mu\)-strongly concavity imposes uniform curvature in all directions, while \(\mu\)-expansiveness restricts concavity in only one direction with an additional restriction for the colinearity of \(\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime})\) and \(-(\bm{w}-\bm{w}^{\prime})\). See Appendix G.2 for details. We illustrate a simple loss function in Fig. 1 that satisfies Definition 4.3.

Footnote 5: Namely, \(\forall\bm{w},\bm{w}^{\prime}\in\Omega\), \((\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime}),\bm{w}-\bm{w}^{\prime})\leq- \mu\|\bm{w}-\bm{w}^{\prime}\|^{2}\).

Notably, the directional restrictions on the update rules in Definitions 4.1 to 4.3 simplify the lower-bound calculations as it enables us to focus only on the norm of the divergence at each step and get rid of directional variation, which aids in a clearer understanding of divergence dynamics. As a first attempt to establish stability lower bounds for bilevel optimization problems, our work leaves open whether these conditions can be relaxed. A potential approach might involve requiring the divergence to exhibit a specific directional component rather than strict alignment as in the current definitions.

The following lemma shows that the expansiveness of the loss function can induce the growing property of SGD.

**Lemma 4.4** (Growing property of SGD with expansive loss function, proof in Appendix B.2).: _Suppose \(\ell\) is \(\mu\)-expansive along \(\bm{v}\) and \(1+\alpha\mu\geq 0\), then \(G_{\ell,\alpha}\) is \((1+\alpha\mu)\)-growing along \(\bm{v}\)._

Figure 2: Practical output distances vs. theoretical bounds in Theorem 5.5. We implement UD-based Algorithm 1 on Example 5.3. The output hyperparameter distances with increasing \(T\) are plotted on the horizontal axis. The upper/lower bounds with corresponding \(T\) are plotted on the vertical axis. The linear trends suggest these three values are of almost the same order w.r.t. \(T\).

Lower bounds on uniform stability in HO

Based on tools introduced in Section 4, this section proceeds to precisely characterize the stability of gradient-based bilevel HO algorithms. In Section 5.1, we provide a lower-bounded recursion of hyperparameter divergence that aligns with Theorem 3.2 given the expansion properties of the outer optimization, followed by a lower bound of Lipschitz constant of the inner output given the expansion properties of the inner optimization in Section 5.2. These findings pose insights in the construction of Example 5.3 at both the inner and outer levels to maximize the instability of HO algorithms. This quadratic example produces a tight lower bound for UD-based algorithms, detailed in Section 5.4, and meaningful bounds for IFT-based algorithms, provided in Appendix C.

### Stability lower bound given outer-level expansion properties

We first establish a uniform argument stability lower bound by considering the outer level of the bilevel programming as a single-level optimization problem w.r.t. the hyperparameters. This approach takes the compound validation loss \(\mathcal{L}\) as a whole, temporarily disregarding the dependence of this loss on the inner-level solution and the inner Jacobian.

Suppose \(S^{\mathrm{val}}\) and \(\tilde{S}^{\mathrm{val}}\) are twin validation sets differing only on the \(i\)-th entry, and denote the sequences of update rules on \(S^{\mathrm{val}}\) and \(\tilde{S}^{\mathrm{val}}\) as \(\{G_{\bm{z}_{i},\alpha_{i}}\}_{t=1}^{T}\) and \(\{G_{\tilde{\bm{z}}_{i},\alpha_{i}}\}_{t=1}^{T}\)6 respectively. According to Definition 3.1, the uniform argument stability is lower-bounded by the hyperparameter distance after \(T\) steps, which primarily depends on the divergent property of update rules on different examples and the expansiveness of the compound validation loss. By characterizing these properties and utilizing Lemmas 4.4 and B.2, we obtain a lower bound of the stability in the following Theorem 5.1.

Footnote 6: We slightly abuse the notation in the subscript of the update rule as in Section 4.2, since the loss function in this contest can be solely distinguished by the selected sample. We use a similar simplification for the inner update rules in the next section.

**Theorem 5.1** (Lower bound given outer-level expansion properties, proof in Appendix B.3).: _Suppose there exists a nonzero vector \(\bm{v}\) along which \(G_{\bm{z}_{i},\alpha_{i}}\) and \(G_{\tilde{\bm{z}}_{i},\alpha_{i}}\) are \(2\alpha_{t}L^{\prime}\)-divergent and \(\mathcal{L}(\cdot;\bm{z})\) is \(\gamma^{\prime}\)-expansive for all \(\bm{z}\in S^{\mathrm{val}}\). Then we have \(\mathbb{E}_{\mathcal{A}}[\delta_{t}]\geq\big{[}1+\alpha_{t}(1-\frac{1}{m}) \gamma^{\prime}\big{]}\mathbb{E}_{\mathcal{A}}[\delta_{t-1}]+\frac{2\alpha_{t }L^{\prime}}{m}\) and_

\[\epsilon_{\mathrm{arg}}\geq\sum_{t=1}^{T}\prod_{s=t+1}^{T+1}\big{(}1+\alpha_{s }(1-1/m)\gamma^{\prime}\big{)}\frac{2\alpha_{t}L^{\prime}}{m}.\]

Theorem 5.1 echos the upper bound formulation in Eq. (6). The distinctions arise solely in the smooth/expansive coefficients \(\gamma\)/\(\gamma^{\prime}\) and the continuous/divergent coefficients \(L\)/\(L^{\prime}\). Consequently, the alignment of these two bounds (i.e., their tightness) hinges on the values of these coefficients. As detailed later in Section 5.2, we delve deeper into the coefficients present in our lower bound by unfolding the bilevel problem, focusing on the solution of the inner level and its Jacobian.

Further, Theorem 5.1 not only applies to all HO algorithms that employs outer-level SGD but also to single-level SGD. In the context of single-level SGD, the expansion properties can be directly inferred from the loss function, as elaborated in Appendix E.

### Lipschitz lower bound given inner-level expansion properties

Based on Theorem 5.1, our next step towards building stability lower bounds is analyzing the expansive coefficient \(\gamma^{\prime}\) and divergent coefficient \(L^{\prime}\) of outer-level optimization where the hypergradient is used for update. As can be observed in Eq. (1), the inner Jacobian \(\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})\) is a key bridge between the inner and outer level that significantly influence the hypergradient. Here, we measure the lower bound of its maximum volume, i.e., the Lipschitz continuity coefficient of \(\bm{\theta}_{K}(\bm{\lambda})\) regarding \(\bm{\lambda}\) denoted by \(L^{\bm{\theta}_{K}}\), to provide a guarantee for the effect of the hypergradient.

Denote corresponding inner update rules as \(G_{\bm{\lambda},\eta}\) and \(G_{\bm{\lambda}^{\prime},\eta}\) given hyperparameters \(\bm{\lambda}\) and \(\bm{\lambda}^{\prime}\). Applying them consecutively for \(K\) times, we get two sequences of inner updates. Theorem 5.2 presents how the expansion properties of the inner problem characterize the lower bound of \(L^{\bm{\theta}_{K}}\).

**Theorem 5.2** (Lower bound of Lipschitz of the inner-level solution, proof in Appendix B.4).: _Given any two hyperparameters \(\bm{\lambda},\bm{\lambda}^{\prime}\in\Lambda\), suppose there exists a nonzero vector \(\bm{v}\) along which \(G_{\bm{\lambda},\eta}\) and \(G_{\bm{\lambda}^{\prime},\eta}\) are \(\|\bm{\lambda}-\bm{\lambda}^{\prime}\|\sigma^{\mathrm{tr}}\)-divergent and \(\ell^{\mathrm{tr}}(\bm{\lambda},\cdot;z)\) is \(\mu^{\mathrm{tr}}\)-expansive for all \(\bm{z}\in S^{\mathrm{tr}}\) with \(\mu^{\mathrm{tr}}>0\). Then we have_

\[L^{\bm{\theta}_{K}}\geq\frac{\sigma^{\mathrm{tr}}}{\eta\mu^{\mathrm{tr}}}[(1+ \eta\mu^{\mathrm{tr}})^{K}-1].\]

_Omitting constants that depend on \(\eta\), \(\sigma^{\mathrm{tr}}\), and \(\mu^{\mathrm{tr}}\), we get \(L^{\bm{\theta}_{K}}\gtrsim(1+\eta\mu^{\mathrm{tr}})^{K}\)._

It is worth mentioning that our lower bound for \(L^{\bm{\theta}_{K}}\) is matched with its upper bound in [17] (see in Theorem 3 and Proposition 2), which prepares us to obtain a tight lower bound.

### An example with maximal simplification

Motivated by Theorems 5.1 and 5.2, the following example is carefully constructed, exhibiting all expansive and divergent properties as required by these theorems to establish tight lower bounds on uniform argument stability of gradient-based HO algorithms.

**Example 5.3**.: We introduce an HO problem as follows. The validation loss and training loss are given by:

\[\ell^{\mathrm{val}}(\bm{\lambda},\bm{\theta};\bm{z})=\ell^{\mathrm{tr}}(\bm{ \lambda},\bm{\theta};\bm{z})=\frac{1}{2}\bm{\theta}^{\top}\bm{A}\bm{\theta}+ \bm{\lambda}^{\top}\bm{\theta}-y\bm{x}^{\top}\bm{\theta},\]

where \(\bm{A}\in\mathbb{R}^{d\times d}\) is symmetric. Denote the eigenvalues of \(\bm{A}\) as \(\gamma_{1}\leq\cdots\leq\gamma_{d}\). Let \(\gamma_{1}<0\) and \(|\gamma_{1}|\geq|\gamma_{d}|\), and \(\bm{v}_{1}\) be a unit eigenvector for \(\gamma_{1}\). Let \(S^{\mathrm{val}}\) and \(\tilde{S}^{\mathrm{val}}\) be a pair of twin validation datasets differing at the \(i\)-th example where

\[\bm{z}_{i}=(\bm{x}_{i},y_{i})=(\bm{v}_{1},1),\tilde{\bm{z}}_{i}=(\tilde{\bm{x }}_{i},\tilde{y}_{i})=(-\bm{v}_{1},1).\]

In this example, \(\bm{A}\) determines the convexity of the problem. Throughout the main text, we consider the most common nonconvex case where \(\bm{A}\) is indefinite and symmetric. See Appendix D for the results of (strongly) convex losses.

Notably, our example satisfies Assumption B.1 adopted for establishing the stability upper bounds where the loss functions are Lipschitz continuous and smooth. Hereafter we denote \(\mathcal{L}(\cdot;\bm{z})\) as \(L\)-Lipschitz continuous and \(\gamma\)-smooth, and \(\ell^{\mathrm{tr}}(\bm{\lambda},\cdot;\bm{z})\) as \(\gamma^{\mathrm{tr}}\)-smooth, where \(\gamma^{\mathrm{tr}}=|\gamma_{1}|\).

Example 5.3 is constructed adhering to the principle of maximal simplification. Specifically, the quadratic form is essential for inducing nonconvexity. The second bilinear cross term represents the simplest scenario for interaction between hyperparameters and parameters, ensuring a non-zero inner Jacobian. The final term provides a connection for parameters and data. \(\ell^{\mathrm{val}}\) and \(\ell^{\mathrm{tr}}\) are set to be identical here for simplicity, and our results do not fundamentally depend on their consistency.

We emphasize the role of the eigenvector (i.e., \(\bm{v}_{1}\)) which corresponds to the smallest eigenvalue. It represents the least convex direction, thereby offering the greatest expansiveness of the loss (see Fig. 1), and both the inner and outer optimizations attain the highest level of divergence and expansiveness in this direction. Consequently, in Example 5.3, the distinct samples in \(S^{\mathrm{val}}\) and \(\tilde{S}^{\mathrm{val}}\) are set to align reversely with \(\bm{v}_{1}\) to make the HO algorithms unstable.

_Remark_.: The constructed example is required to meet two essential criteria: first, it must reveal the instability inherent in the algorithms; second, it must allow precise calculation of the smoothness coefficient \(\gamma\) and the expansion coefficient \(\mu\) for the compound validation loss to verify the alignment between lower and upper bounds. Simultaneously satisfying these two requirements is challenging for bilevel algorithms. In Appendix G.3, we provide a ridge regression example to illustrate how the bilevel structure complicates the analysis of stability lower bounds.

### Lower bounds of UD-based algorithms

The following proposition shows that Example 5.3 induces the expansion of UD-based algorithms.

**Proposition 5.4** (Expansion properties of UD-based algorithms, proof in Appendix B.5).: _Suppose we solve Example 5.3 by UD-based Algorithm 1 with constant inner step size \(\eta\) where \(1-\eta\gamma_{d}\geq 0\) and outer step size \(\alpha_{t}\). Then (1) the outer update rules \(G_{\bm{z}_{i},\alpha_{t}}\) and \(G_{\tilde{\bm{z}}_{i},\alpha_{t}}\) are \(2\alpha_{t}L^{\prime}\)- divergent along \(\bm{v}_{1}\), and (2) the composite validation loss \(\mathcal{L}(\cdot;\bm{z})\) is \(\gamma^{\prime}\)-expansive along \(\bm{v}_{1}\) for all \(\bm{z}\in S^{\mathrm{val}}\), where_

\[L\asymp L^{\prime}\asymp(1+\eta\gamma^{\mathrm{tr}})^{K},\gamma=\gamma^{\prime} \asymp(1+\eta\gamma^{\mathrm{tr}})^{2K}.\]Combining the lower bound in Theorem 5.1 with the upper bound in Eq. (6), we instantly get

\[\sum_{t=1}^{T}\prod_{s=t+1}^{T}\bigl{(}1+\alpha_{s}(1-1/m)\gamma\bigr{)}\frac{2 \alpha_{t}L^{\prime}}{m}\leq\epsilon_{\mathrm{arg}}\leq\sum_{t=1}^{T}\prod_{s= t+1}^{T}\bigl{(}1+\alpha_{s}(1-1/m)\gamma\bigr{)}\frac{2\alpha_{t}L}{m},\] (7)

where the bounds are in the same order w.r.t. \(T\), \(K\) and \(m\). These matching bounds in recursion form verify the **tightness** of the existing upper bound [17].

Specifically, for constant step sizes, i.e., \(\alpha_{t}=c\) for all \(t\), Eq. (7) explicitly reveals the scale of \(\epsilon_{\mathrm{arg}}\) regarding \(T\colon\epsilon_{\mathrm{arg}}\asymp\bigl{(}1+c(1-1/m)\gamma\bigr{)}^{T}/m\). However, for linearly decreasing step sizes \(\alpha_{t}\leq c/t\), additional scaling steps 7 are necessary and the deformed result is provided below.

Footnote 7: For instance, \(1+x\leq e^{x}\) is used in [19].

**Theorem 5.5** (Uniform argument stability of UD-based algorithms, proof in Appendix B.6).: _Solving Example 5.3 by UD-based Algorithm 1 with constant inner step size \(\eta\) where \(1-\eta\gamma_{d}\geq 0\) and decreasing outer step sizes \(\alpha_{t}=c/t\) with \(c\) as a positive constant has uniform argument stability that_

\[\frac{T^{\ln\bigl{(}1+(1-\frac{1}{m})c\gamma^{\prime}\bigr{)}}}{m}\lesssim \epsilon_{\mathrm{arg}}\lesssim\frac{T^{(1-\frac{1}{m})c\gamma}}{m},\]

_where \(\gamma=\gamma^{\prime}\asymp(1+\eta\gamma^{\mathrm{tr}})^{2K}\) as in Proposition 5.4._

The scaling steps unavoidably create a discrepancy between the deformed lower and upper bounds, while their quotient, \(T^{(1-\frac{1}{m})c\gamma-\ln\bigl{(}1+(1-\frac{1}{m})c\gamma^{\prime}\bigr{)}}\), is small given a small \(c\) (e.g., \(0.01\)). We compare the practical output hyperparameter distances and the theoretical bounds in Fig. 2.

Notably, the upper bound in our result is not contradictory to the existing upper bound of \(\epsilon_{\mathrm{arg}}\lesssim T^{\frac{(1-1/m)c\gamma}{(1-1/m)c\gamma+1}}/m\) in [17] because we remove the bounded loss assumption, i.e., \(\exists a,b\in\mathbb{R}\) s.t. \(\mathcal{L}\in[a,b]\). This modification is necessary to fairly compare the upper and lower bounds. Detailed discussion is provided in Appendix E.3.

Based on the results of uniform argument stability, we can further obtain similar results of uniform stability by introducing additional assumptions as below.

**Theorem 5.6** (Uniform stability of UD-based algorithms, proof in Appendix B.7).: _Following the same condition as in Theorem 5.5, and additionally, if the initial points \(\bm{\theta}_{0}=\bm{0},\bm{\lambda}_{0}=\bm{0}\), and \(\bm{v}_{1}\perp\bm{x}_{j}^{\mathrm{val}}\) for any \(j\in[m]\backslash i\) and \(\bm{v}_{1}\perp\bm{x}_{j}^{\mathrm{tr}}\) for any \(j\in[n]\), then Algorithm 1 has uniform stability that \(\frac{T^{\ln\bigl{(}1+(1-\frac{1}{m})c\gamma^{\prime}\bigr{)}}}{m}\lesssim \epsilon_{\mathrm{stab}}\lesssim\frac{T^{(1-\frac{1}{m})c\gamma}}{m},\) where \(\gamma=\gamma^{\prime}\asymp(1+\eta\gamma^{\mathrm{tr}})^{2K}\) as in Proposition 5.4._

Technically, we adopt these additional assumptions following [34] to simplify the formulation of \(\mathcal{L}(\bm{\lambda}_{T},\bm{z})-\mathcal{L}(\bm{\lambda}_{T}^{\prime},\bm {z})\) by eliminating the quadratic term and reducing it to be colinear with \(\bm{\lambda}_{T}-\bm{\lambda}_{T}^{\prime}\). By doing so, a clear relation can be established between the loss divergence and the hyperparameter divergence, which leads to a transfer from uniform argument stability to uniform stability.

_Remark_.: For now, we have characterized the stability error as an upper bound on the generalization error. Let us now examine how this stability-based generalization bound informs the allocation of data between the validation and training sets. Suppose we have a total of \(N\) data points, with \(m=aN\) assigned to the validation set \(S^{\mathrm{val}}\) and \(n=(1-a)N\) assigned to the training set \(S^{\mathrm{tr}}\), where \(a\in(0,1)\). The expected population risk can be decomposed into the generalization error and the empirical validation risk as follows:

\[\mathbb{E}_{\mathcal{A},S^{\mathrm{val}},S^{\mathrm{tr}},\bm{z}^{\mathrm{ test}}}\left[\mathcal{L}(\mathcal{A}(S^{\mathrm{val}},S^{\mathrm{tr}});\bm{z}^{ \mathrm{test}})\right]=\underbrace{\epsilon_{\mathrm{gen}}}_{\text{(I)}}+ \underbrace{\mathbb{E}_{\mathcal{A},S^{\mathrm{val}},S^{\mathrm{tr}}}\left[ \frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(\mathcal{A}(S^{\mathrm{val}},S^{\mathrm{ tr}});\bm{z}_{i}^{\mathrm{val}})\right]}_{\text{(II)}}.\]

On one hand, the generalization bound \(\epsilon_{gen}\leq\epsilon_{stab}=\Theta(1/aN)\) (as in Eq. (7)) suggests that \(a\) should be sufficiently large to keep term (II) small. On the other hand, \(a\) should also be sufficiently small to get a low validation risk in term (II), since a larger training set generally improves validation performance. Thus, selecting \(a\) involves a trade-off to optimize the overall population risk.

Conclusion and discussion

This paper establishes novel lower bounds of the uniform stability for various HO algorithms and shows the existing upper bound in UD-based algorithms is tight. This result indicates that the notion of uniform stability has reached its limit in stability analysis for the UD-based algorithm. The lower-bounded expansion properties proposed in this paper can serve as general tools for analyzing lower bounds of stability. This paper applies them to both single-level and bilevel optimization. We also discuss in detail potential extensions of our analysis framework on establishing average stability lower bounds and generalization lower bounds in Appendix H.

**Limitations and social impacts.** This paper is constrained in the scope of smooth loss functions, while non-smooth scenarios [31] remain open. Moreover, a uniform stability lower bound does not directly imply a generalization lower bound. This gap exists as algorithmic stability is inherently introduced as a theoretical tool for analyzing the generalization upper bound. Alternative approaches might include directly deriving a generalization lower bound with examples considering the data distribution. This paper is a purely theoretical work, we have not identified any direct, significant societal impacts that must be emphasized.

## Acknowledgments and Disclosure of Funding

This work was supported by Beijing Natural Science Foundation (L247030); NSF of China (Nos. 62076145, 62206159); Beijing Nova Program (No. 20230484416); Major Innovation & Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China; the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (22XNKJ13); the Natural Science Foundation of Shandong Province (Nos. ZR2022QF117), the Fundamental Research Funds of Shandong University; and the Ant Group Research Fund. The work was partially done at the Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education. G. Wu was also sponsored by the TaiShan Scholars Program.

## References

* [1] Yoshua Bengio. Gradient-based optimization of hyperparameters. _Neural computation_, 12(8):1889-1900, 2000.
* [2] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In _ICML_, volume 80, pages 1563-1572, 2018.
* [3] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In _ICLR_, 2019.
* [4] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_. Adaptive computation and machine learning. MIT Press, 2012.
* [5] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. _Journal of machine learning research_, 13(2), 2012.
* [6] Jonas Mockus. On bayesian methods for seeking the extremum. In _Optimization Techniques IFIP Technical Conference: Novosibirsk, July 1-7, 1974_, pages 400-404. Springer, 1975.
* [7] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. _Advances in neural information processing systems_, 25, 2012.
* [8] Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R Collins, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. _The Journal of Machine Learning Research_, 21(1):3098-3124, 2020.
* [9] James Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl. Algorithms for hyper-parameter optimization. _Advances in neural information processing systems_, 24, 2011.

* [10] Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. _arXiv preprint arXiv:1711.09846_, 2017.
* [11] Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based tuning of continuous regularization hyperparameters. In _International conference on machine learning_, pages 2952-2960. PMLR, 2016.
* [12] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In _AISTATS_, volume 108, pages 1540-1552, 2020.
* [13] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In _International Conference on Machine Learning_, pages 1165-1173. PMLR, 2017.
* [14] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated backpropagation for bilevel optimization. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1723-1732. PMLR, 2019.
* [15] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In _ICML_, volume 48, pages 737-746, 2016.
* [16] Aravind Rajeswaran, Chelsea Finn, Sham M. Kakade, and Sergey Levine. Meta-learning with implicit gradients. In _NeurIPS_, pages 113-124, 2019.
* [17] Fan Bao, Guoqiang Wu, Chongxuan Li, Jun Zhu, and Bo Zhang. Stability and generalization of bilevel programming in hyperparameter optimization. _Advances in neural information processing systems_, 34:4529-4541, 2021.
* [18] Congliang Chen, Li Shen, Zhiqiang Xu, Wei Liu, Zhi-Quan Luo, and Peilin Zhao. Exploring the generalization capabilities of AID-based bi-level optimization, 2024.
* [19] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In _International conference on machine learning_, pages 1225-1234. PMLR, 2016.
* [20] William H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for local discrimination rules. _The Annals of Statistics_, pages 506-514, 1978.
* [21] Luc Devroye and Terry Wagner. Distribution-free performance bounds for potential function rules. _IEEE Transactions on Information Theory_, 25(5):601-604, 1979.
* [22] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. _The Journal of Machine Learning Research_, 11:2635-2670, 2010.
* [23] Olivier Bousquet and Andre Elisseeff. Stability and generalization. _The Journal of Machine Learning Research_, 2:499-526, 2002.
* [24] Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. _Advances in Neural Information Processing Systems_, 31, 2018.
* [25] Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In _Conference on Learning Theory_, pages 1270-1279. PMLR, 2019.
* [26] Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In _Conference on Learning Theory_, pages 610-626. PMLR, 2020.
* [27] Yegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with convergence rate \(o(1/n)\). _Advances in Neural Information Processing Systems_, 34:5065-5076, 2021.

* [28] Zhun Deng, Hangfeng He, and Weijie Su. Toward better generalization bounds with locally elastic stability. In _International Conference on Machine Learning_, pages 2590-2600. PMLR, 2021.
* [29] Tongliang Liu, Gabor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis complexity. In _International Conference on Machine Learning_, pages 2159-2167. PMLR, 2017.
* [30] Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. In _International Conference on Machine Learning_, pages 5809-5819. PMLR, 2020.
* [31] Raef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradient descent on nonsmooth convex losses. _Advances in Neural Information Processing Systems_, 33:4381-4391, 2020.
* [32] Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In _International Conference on Machine Learning_, pages 2815-2824. PMLR, 2018.
* [33] Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative optimization algorithms. _arXiv preprint arXiv:1804.01619_, 2018.
* [34] Yikai Zhang, Wenjia Zhang, Sammy Bald, Vamsi Pingali, Chao Chen, and Mayank Goswami. Stability of sgd: Tightness analysis and improved bounds. In _Uncertainty in artificial intelligence_, pages 2364-2373. PMLR, 2022.
* [35] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(12):10045-10067, 2022.
* [36] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In _ICML_, volume 139 of _Proceedings of Machine Learning Research_, pages 4882-4892, 2021.
* [37] Mathieu Dagreou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. In _NeurIPS_, 2022.
* [38] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. _arXiv preprint arXiv:1802.02246_, 2018.
* [39] Xuxing Chen, Minhui Huang, Shiqian Ma, and Krishna Balasubramanian. Decentralized stochastic bilevel optimization with improved per-iteration complexity. In _ICML_, volume 202, pages 4641-4671, 2023.
* [40] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. _arXiv preprint arXiv:1611.02163_, 2016.
* [41] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. On the global optimality of model-agnostic meta-learning. In _ICML_, volume 119, pages 9837-9846, 2020.
* [42] Hong Li and Li Zhang. A bilevel learning model and algorithm for self-organizing feed-forward neural networks for pattern classification. _IEEE Transactions on Neural Networks and Learning Systems_, 32(11):4901-4915, 2020.
* [43] Risheng Liu, Jinyuan Liu, Zhiying Jiang, Xin Fan, and Zhongxuan Luo. A bilevel integrated model with data-driven layer ensemble for multi-modality image fusion. _IEEE Transactions on Image Processing_, 30:1261-1274, 2020.
* [44] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM journal on optimization_, 23(4):2341-2368, 2013.
* [45] Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. _SIAM review_, 60(2):223-311, 2018.

* [46] Li Shen, Congliang Chen, Fangyu Zou, Zequn Jie, Ju Sun, and Wei Liu. A unified analysis of adagrad with weighted aggregation and momentum acceleration. _IEEE Transactions on Neural Networks and Learning Systems_, 2023.
* [47] Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences of adam and rmsprop. In _Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition_, pages 11127-11135, 2019.
* [48] Congliang Chen, Li Shen, Fangyu Zou, and Wei Liu. Towards practical adam: Non-convexity, convergence theory, and mini-batch acceleration. _Journal of Machine Learning Research_, 23(229):1-47, 2022.
* [49] Yunwen Lei. Stability and generalization of stochastic optimization with nonconvex and nonsmooth problems. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 191-227. PMLR, 2023.
* [50] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Generalization of model-agnostic meta-learning algorithms: Recurring and unseen tasks. _Advances in Neural Information Processing Systems_, 34:5469-5480, 2021.

Overview of main results

### Overview of main contributions

### Overview of main contributions

\begin{table}
\begin{tabular}{c c c} \hline \multicolumn{2}{c}{**Our contributions**} & \multicolumn{2}{c}{**Comparable results**} \\ \hline
**Expansion** & \(\sigma\)-divergent, \(\rho\)-growing (Ours) & \(\sigma\)-bounded, \(\eta\)-expansive [19] \\ \hline \multirow{6}{*}{**UD-based algorithm**} & Recursive lower bound: & Recursive upper bound: \\  & \(\sum_{t=1}^{T}\prod_{s=t+1}^{T}(1+\alpha_{s}(1-1/m)\gamma^{\prime})^{2\alpha_{ t}L^{\prime}}\), & \(\sum_{t=1}^{T}\prod_{s=t+1}^{T}(1+\alpha_{s}(1-1/m)\gamma)\frac{2\alpha_{t}L}{m}\), \\  & where \(\gamma^{\prime}=\gamma\asymp(1+\eta\gamma^{tr})^{2K}\), & where \(\gamma\lesssim(1+\eta\gamma^{tr})^{2K}\), \\  & \(L^{\prime}\asymp(1+\eta\gamma^{tr})^{K}\) (Ours) & \(L\lesssim(1+\eta\gamma^{tr})^{K}\)[17] \\ \cline{2-3}  & Deformed lower bound: & Deformed upper bound: \\  & \(\frac{\gamma}{\gamma^{\prime}}=\gamma\asymp(1+\eta\gamma^{tr})^{2K}\) (Ours) & where \(\gamma\lesssim(1+\eta\gamma^{tr})^{2K}\) (Ours) \\ \hline \multirow{6}{*}{**IFT-based algorithm**} & Recursive lower bound: & Recursive upper bound: \\  & \(\sum_{t=1}^{T}\prod_{s=t+1}^{T}(1+\alpha_{s}(1-1/m)\gamma^{\prime})^{2\alpha_{ t}L^{\prime}}\), & \(\sum_{t=1}^{T}\prod_{s=t+1}^{T}(1+\alpha_{s}(1-1/m)\gamma)\frac{2\alpha_{t}L}{m}\), \\  & where \(\gamma^{\prime}=\gamma\asymp(1+\eta\gamma^{tr})^{2K}\), & where \(\gamma\lesssim K(1+\eta\gamma^{tr})^{2K}\), \\  & \(L^{\prime}\asymp(1+\eta\gamma^{tr})^{K}\) (Ours) & \(L\lesssim(1+\eta\gamma^{tr})^{K}\) (Ours) \\ \cline{2-3}  & Deformed lower bound & Deformed upper bound \\  & \(\frac{\lesssim T^{\ln(1+(1-1/m)\circ\gamma^{\prime})}}{m}\), & \(\lesssim\frac{T^{(1-1/m)\circ\gamma}}{m}\), \\  & where \(\gamma^{\prime}\gtrsim(1+\eta\gamma^{tr})^{2K}\) (Ours) & where \(\gamma\lesssim K(1+\eta\gamma^{tr})^{2K}\) (Ours) \\ \hline \end{tabular}
\end{table}
Table 1: Overview of main contributions. The results presented here are derived without the bounded loss assumption for fair comparison. Deformed bounds are derived under decreasing step size \(\alpha_{t}\leq c/t\) where \(c>0\) is a constant.

### Dependent graph of main results

## Appendix B Proofs of main theoretical results

### General assumptions

We first list some assumptions in the derivation for upper bounds [17], which are common theoretical conditions for an HO problem. We follow these assumptions throughout Section 5. The constructed Example 5.3 also satisfies these assumptions.

**Assumption B.1**.: Let \(\Omega\) be an open set including \(\Lambda\times\Theta\times\mathbb{Z}\), we assume that

Figure 3: Dependent graph of our main results. The blue node denotes previous results and others are our contributions. The solid line represents direct proof dependency. The dashed line is annotated with text therein.

1. \(\Lambda\) and \(\Theta\) are compact and convex with non-empty interiors, and \(\mathcal{Z}\) is compact,
2. \(\ell^{\mathrm{val}}(\bm{\lambda},\bm{\theta};\bm{z})\in C^{2}(\Omega)\), that is, \(\ell^{\mathrm{val}}\) is second order continuously differentiable on \(\Omega\),
3. \(\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta};\bm{z})\in C^{3}(\Omega)\), that is, \(\varphi_{i}\) is third order continuously differentiable on \(\Omega\),
4. \(\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta};\bm{z})\) is \(\gamma^{\mathrm{tr}}\)-smooth as a function of \(\bm{\theta}\) for all \(z\in Z\) and \(\lambda\in\Lambda\) (the first and third points imply such a constant \(\gamma^{\mathrm{tr}}\) exists).

### Proof of Lemma 4.4

Lemma 4.4: Assume \(\ell\) is \(\mu\)-expansive on \(\bm{v}\) and \(1+\alpha\mu\geq 0\), then \(G_{\ell,\alpha}\) is \((1+\alpha\mu)\)-growing on \(\bm{v}\).

Proof.: Recalling Definition 4.3, for any \(\bm{w}-\bm{w}^{\prime}\) colinear with \(\bm{v}\) we have

\[G_{\ell,\alpha}(\bm{w})-G_{\ell,\alpha}(\bm{w}^{\prime})\] \[= \bm{w}-\bm{w}^{\prime}-\alpha\big{(}\nabla\ell(\bm{w})-\nabla \ell(\bm{w}^{\prime})\big{)}\] \[= \bm{w}-\bm{w}^{\prime}+\alpha\bm{a}(\bm{w}-\bm{w}^{\prime})\] \[= (1+\alpha\mu_{\bm{w},\bm{w}^{\prime}})(\bm{w}-\bm{w}^{\prime}),\]

where \(\mu_{\bm{w},\bm{w}^{\prime}}\geq\mu\) and thus \(1+\alpha\mu_{\bm{w},\bm{w}^{\prime}}\geq 1+\alpha\mu\geq 0\) by assumption. Therefore, we have \(G_{\ell,\alpha}(\bm{w})-G_{\ell,\alpha}(\bm{w}^{\prime})\stackrel{{ \mathrm{a}}}{{=}}\bm{w}-\bm{w}^{\prime}\) and \(\|G_{\ell,\alpha}(\bm{w})-G_{\ell,\alpha}(\bm{w}^{\prime})\|\geq(1+\alpha\mu) \|\bm{w}-\bm{w}^{\prime}\|\), which implies \(G_{\ell,\alpha}\) is \((1+\alpha\mu)\)-growing on \(\bm{v}\) according to Definition 4.2. 

### Proof of Theorem 5.1

As the divergence of each step is entwined with prior results and shapes subsequent evolution, we first provide the following recursion for the parameter distance using the expansion properties.

**Lemma B.2** (Recursion of parameter divergence).: _Let the initial points be \(\bm{w}_{0}=\bm{w}_{0}^{\prime}\in\Omega\). Suppose there exists a nonzero vector \(\bm{v}\) along which, for all \(1\leq t\leq T\), \(G_{t}\neq G_{t}^{\prime}\) are \(\sigma\)-divergent and \(G_{t}\) are \(\rho\)-growing. Then we have \(\bm{w}_{t}-\bm{w}_{t}^{\prime}\stackrel{{\mathrm{a}}}{{=}}\bm{v}\) for all \(1\leq t\leq T\) and recursively,_

\[\|\bm{w}_{0}-\bm{w}_{0}^{\prime}\|=0,\|\bm{w}_{t}-\bm{w}_{t}^{\prime}\|\geq \left\{\begin{array}{ll}\rho\|\bm{w}_{t-1}-\bm{w}_{t-1}^{\prime}\|+\sigma, &G_{t}\neq G_{t}^{\prime},\\ \rho\|\bm{w}_{t-1}-\bm{w}_{t-1}^{\prime}\|,&G_{t}=G_{t}^{\prime},\end{array} \right.\quad t\geq 1.\]

Proof.: Without loss of generality, assume \(\bm{v}_{1}\) is a unit vector (i.e., \(\|\bm{v}_{1}\|=1\)). At the initial point, we have \(\bm{w}_{0}-\bm{w}_{0}^{\prime}=\bm{0}\stackrel{{\mathrm{a}}}{{=}} \bm{v}\). According to Definition 4.1 and Definition 4.2, if \(\bm{w}_{t-1}-\bm{w}_{t-1}^{\prime}\stackrel{{\mathrm{a}}}{{=}} \bm{v}\), then

\[\bm{w}_{t}-\bm{w}_{t}^{\prime} =G_{t}(\bm{w}_{t-1})-G_{t}^{\prime}(\bm{w}_{t-1}^{\prime})\] \[=\left\{\begin{array}{ll}G_{t}(\bm{w}_{t-1})-G_{t}(\bm{w}_{t-1}^ {\prime})+G_{t}(\bm{w}_{t-1}^{\prime})-G_{t}^{\prime}(\bm{w}_{t-1}^{\prime}) &G_{t}\neq G_{t}^{\prime}\\ G_{t}(\bm{w}_{t-1})-G_{t}(\bm{w}_{t-1}^{\prime})&G_{t}=G_{t}^{\prime}\end{array}\right.\] \[=\left\{\begin{array}{ll}\rho_{t}(\bm{w}_{t-1}-\bm{w}_{t-1}^{ \prime})+\sigma_{t}\bm{v},&G_{t}\neq G_{t}^{\prime},\\ \rho_{t}(\bm{w}_{t-1}-\bm{w}_{t-1}^{\prime}),&G_{t}=G_{t}^{\prime},\end{array}\right.\] \[=\left\{\begin{array}{ll}(\rho_{t}\|\bm{w}_{t-1}-\bm{w}_{t-1}^ {\prime}\|+\sigma_{t})\bm{v},&G_{t}\neq G_{t}^{\prime},\\ \rho_{t}\|\bm{w}_{t-1}-\bm{w}_{t-1}^{\prime}\|\bm{v},&G_{t}=G_{t}^{\prime},\end{array}\right.\]

where \(\rho_{t}\geq\rho\) and \(\sigma_{t}\geq\sigma\). Thus, we have the above recurrence relation for parameter distance, and all subsequent parameter divergence will be in the direction of \(\bm{v}\). 

Now we are prepared to prove Theorem 5.1: Suppose there exists a nonzero vector \(\bm{v}\) along which \(G_{\bm{z}_{i},\alpha_{t}}\) and \(G_{\hat{\bm{z}}_{i},\alpha_{t}}\) are \(2\alpha_{t}L^{\prime}\)-divergent and \(\mathcal{L}(\cdot;\bm{z})\) is \(\gamma^{\prime}\)-expansive for all \(\bm{z}\in S^{\mathrm{val}}\). Then we have

\[\epsilon_{\mathrm{arg}}\geq\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{(}1+\alpha_{s}(1- 1/m)\gamma^{\prime}\big{)}\frac{2\alpha_{t}L^{\prime}}{m}.\]

Proof.: Using Lemma 4.4, we have \(G_{\bm{z}_{i},\alpha_{t}}\) and \(G_{\hat{\bm{z}}_{i},\alpha_{t}}\) are \((1+\alpha_{t}\gamma^{\prime})\)-growing for all \(1\leq t\leq T\). Denote \(\delta_{t}=\|\bm{\lambda}_{t}-\tilde{\bm{\lambda}}_{t}\|\) for each step \(t\). As Algorithm 1 is initialized with the same starting point,we know that \(\bm{\lambda}_{0}=\bm{\lambda}_{0}\) and thus \(\delta_{0}=0\). For all \(1\leq t\leq T\), there is a probability of \(1-\frac{1}{m}\) to select the same examples and \(\frac{1}{m}\) otherwise. Consequently, by the law of total probability, we have the recurrence relation

\[\mathbb{E}_{\mathcal{A}}[\delta_{t}]\] \[=\bigg{[}1+\alpha_{t}(1-\frac{1}{m})\gamma^{\prime}\bigg{]} \mathbb{E}_{\mathcal{A}}[\delta_{t-1}]+\frac{2\alpha_{t}L^{\prime}}{m}\] (linearity of expectation) \[,\]

By unwinding the recurrence from \(T\) to \(1\), for all \(S^{\mathrm{val}}\simeq\tilde{S}^{\mathrm{val}}\in\mathbb{Z}^{m},S^{\mathrm{tr }}\in\mathbb{Z}^{n}\) we have

\[\mathbb{E}_{\mathcal{A}}[\delta_{T}]\geq\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{(} 1+\alpha_{s}(1-1/m)\gamma^{\prime}\big{)}\frac{2\alpha_{t}L^{\prime}}{m},\] (8)

which implies

\[\epsilon_{\mathrm{arg}}\geq\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{(}1+\alpha_{s} (1-1/m)\gamma^{\prime}\big{)}\frac{2\alpha_{t}L^{\prime}}{m}.\]

### Proof of Theorem 5.2

Here we prove a more general version of Theorem 5.2 in the main paper by additionally considering the cases where \(\mu^{\mathrm{tr}}\leq 0\). Theorem 5.2 in the main paper can be simply derived by letting \(\mu^{\mathrm{tr}}>0\).

**Theorem B.3** (Lower bound of Lipschitz of the inner-level solution, generalized Theorem 5.2).: _Given any two hyperparameters \(\bm{\lambda},\bm{\lambda}^{\prime}\in\Lambda\), suppose there exists a nonzero vector \(\bm{v}\) along which \(G_{\bm{\lambda},\eta}\) and \(G_{\bm{\lambda}^{\prime},\eta}\) are \(\|\bm{\lambda}-\bm{\lambda}^{\prime}\|\sigma^{\mathrm{tr}}\)-divergent and \(\ell^{\mathrm{tr}}(\bm{\lambda},\cdot;\bm{z})\) is \(\mu^{\mathrm{tr}}\)-expansive for all \(\bm{z}\in S^{\mathrm{tr}}\) where \(1+\eta\mu^{\mathrm{tr}}\geq 0\). Then we have_

\[L^{\bm{\theta}_{K}} =\left\{\begin{array}{ll}\frac{\sigma^{\mathrm{tr}}}{\eta\mu^{ \mathrm{tr}}}[(1+\eta\mu^{\mathrm{tr}})^{K}-1],&\mu^{\mathrm{tr}}>0,\\ \sigma^{\mathrm{tr}}K,&\mu^{\mathrm{tr}}=0,\\ \frac{\sigma^{\mathrm{tr}}}{\eta\|\mu^{\mathrm{tr}}}[1-(1-\eta|\mu^{\mathrm{tr }}|)^{K}],&\mu^{\mathrm{tr}}<0.\end{array}\right.\]

_Omitting constants that depend on \(\eta\), \(\sigma^{\mathrm{tr}}\), and \(\mu^{\mathrm{tr}}\), we get \(L^{\bm{\theta}_{K}}\gtrsim\left\{\begin{array}{ll}(1+\eta\mu^{\mathrm{tr}})^ {K},&\mu^{\mathrm{tr}}>0,\\ K,&\mu^{\mathrm{tr}}=0,\\ 1,&\mu^{\mathrm{tr}}<0.\end{array}\right.\)_

Proof.: First, for any \(\bm{\lambda}\) and \(\bm{\lambda}^{\prime}\), we establish a lower bound of \(\|\bm{\theta}_{K}(\bm{\lambda})-\bm{\theta}_{K}(\bm{\lambda}^{\prime})\|\) in a recursion way. Using Lemma 4.4, we have \(G_{\bm{\lambda},\eta}\) and \(G_{\bm{\lambda}^{\prime},\eta}\) are \((1+\eta\mu^{\mathrm{tr}})\)-growing. For any inner step \(1\leq k\leq K\), we have

\[\|\bm{\theta}_{k}(\bm{\lambda})-\bm{\theta}_{k}(\bm{\lambda}^{ \prime})\| =\|G_{\bm{\lambda},\eta}(\bm{\theta}_{k-1}(\bm{\lambda}))-G_{\bm{ \lambda}^{\prime},\eta}(\bm{\theta}_{k-1}(\bm{\lambda}^{\prime}))\|\] \[=\|G_{\bm{\lambda},\eta}(\bm{\theta}_{k-1}(\bm{\lambda}))-G_{\bm {\lambda},\eta}(\bm{\theta}_{k-1}(\bm{\lambda}^{\prime}))+G_{\bm{\lambda}, \eta}(\bm{\theta}_{k-1}(\bm{\lambda}^{\prime}))-G_{\bm{\lambda}^{\prime}, \eta}(\bm{\theta}_{k-1}(\bm{\lambda}^{\prime}))\|\] \[\geq\big{|}(1+\eta\mu^{\mathrm{tr}})\|\bm{\theta}_{k-1}(\bm{ \lambda})-\bm{\theta}_{k-1}(\bm{\lambda}^{\prime})\|+\sigma^{\mathrm{tr}}\| \bm{\lambda}-\bm{\lambda}^{\prime}\|\big{|}\|\bm{v}\|\] (Lemma B.2) \[=\big{|}(1+\eta\mu^{\mathrm{tr}})\|\bm{\theta}_{k-1}(\bm{\lambda}) -\bm{\theta}_{k-1}(\bm{\lambda}^{\prime})\|+\sigma^{\mathrm{tr}}\|\bm{\lambda}- \bm{\lambda}^{\prime}\|\big{|} (\|\bm{v}\|=1)\] \[=(1+\eta\mu^{\mathrm{tr}})\|\bm{\theta}_{k-1}(\bm{\lambda})-\bm{ \theta}_{k-1}(\bm{\lambda}^{\prime})\|+\sigma^{\mathrm{tr}}\|\bm{\lambda}- \bm{\lambda}^{\prime}\|. (1+\eta\mu^{\mathrm{tr}}\geq 0)\]

Using the fact that the algorithm is initialized with the same starting point and unwinding the above recurrence from \(K\) to \(1\), we obtain

\[\|\bm{\theta}_{K}(\bm{\lambda})-\bm{\theta}_{K}(\bm{\lambda}^{\prime})\|\geq\sum _{k=0}^{K-1}(1+\eta\mu^{\mathrm{tr}})^{k}\sigma^{\mathrm{tr}}\|\bm{\lambda}- \bm{\lambda}^{\prime}\|,\]

which implies that for any \(\bm{\lambda}\in\Lambda\). According to the mean value theorem for vector valued multivariable function, there exists a \(\bm{c}\) on line segment determined by \(\bm{\lambda}\) and \(\bm{\lambda}^{\prime}\) such that \(\|\nabla\bm{\theta}_{K}(\bm{c})(\bm{\lambda}-\bm{\lambda}^{\prime})\|\geq\)\(\|\bm{\theta}_{K}(\bm{\lambda})-\bm{\theta}_{K}(\bm{\lambda}^{\prime})\|\), and for triangle inequality, we have \(\|\nabla\bm{\theta}_{K}(\bm{c})\|\|\bm{\lambda}-\bm{\lambda}^{\prime}\|\geq\| \nabla\bm{\theta}_{K}(\bm{c})(\bm{\lambda}-\bm{\lambda}^{\prime})\|\). Therefore, by the definition of Lipschitz continuity, it holds that

\[L^{\bm{\theta}_{K}}\geq\|\nabla\bm{\theta}_{K}(\bm{c})\|\geq\frac{\|\bm{ \theta}_{K}(\bm{\lambda})-\bm{\theta}_{K}(\bm{\lambda}^{\prime})\|}{\|\bm{ \lambda}-\bm{\lambda}^{\prime}\|}\geq\sigma^{\mathrm{tr}}\sum_{k=0}^{K-1}(1+ \eta\mu^{\mathrm{tr}})^{k}=\left\{\begin{array}{ll}\sigma^{\mathrm{tr}}\frac {(1+\eta\mu^{\mathrm{tr}})^{K}-1}{\eta\mu^{\mathrm{tr}}},&\mu^{\mathrm{tr}}>0, \\ \sigma^{\mathrm{tr}}K,&\mu^{\mathrm{tr}}=0,\\ \sigma^{\mathrm{tr}}\frac{1-(1-\eta\mu^{\mathrm{tr}}))^{K}}{\eta|\mu^{\mathrm{ tr}}|},&\mu^{\mathrm{tr}}<0,\end{array}\right.\]

which completes the proof. 

### Proof of Proposition 5.4

Before deriving Proposition 5.4, we present a technical lemma as follows.

**Lemma B.4**.: _Suppose that \(\bm{A}\in\mathbb{R}^{d\times d}\) is a symmetric matrix. We denote \(\bm{v}_{1},\ldots,\bm{v}_{d}\) the orthogonal unit eigenvectors of \(\bm{A}\) and \(\gamma_{1}\leq\cdots\leq\gamma_{d}\) the corresponding eigenvalues, where we assume that \(1-\eta\gamma_{d}\geq 0\). Then it holds that_

\[\left\|\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\left(2\bm{I}- \eta\bm{A}\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\right)\right\| =\eta\sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}\left(2-\eta\gamma_{1} \sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}\right)\] \[\asymp\left\{\begin{array}{ll}(1-\eta\gamma_{1})^{2K},&\gamma_ {1}<0,\\ K,&\gamma_{1}=0,\\ 1,&\gamma_{1}>0.\end{array}\right.\]

Proof.: For simplicity, we denote \(\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\Big{(}2\bm{I}-\eta\bm{A}\sum_{k=0 }^{K-1}(\bm{I}-\eta\bm{A})^{k}\Big{)}\) by \(\bm{C}\), then \(\bm{C}\) is symmetric and has eigenvectors \(\bm{v}_{1},\ldots,\bm{v}_{d}\) as well. Based on the symmetric of \(\bm{C}\), \(\|\bm{C}\|\) equals to its maximum absolute eigenvalue, which can be expressed as

\[\|\bm{C}\| =\sup_{i}\|\bm{C}\bm{v}_{i}\|\|\] \[=\sup_{i}\left|\eta\sum_{k=0}^{K-1}(1-\eta\gamma_{i})^{k}\left(2- \eta\gamma_{i}\sum_{k=0}^{K-1}(1-\eta\gamma_{i})^{k}\right)\right|\] \[=\sup_{i}\left\{\begin{array}{ll}2\eta K&\gamma_{i}=0,\\ \eta\sum_{k=0}^{K-1}(1-\eta\gamma_{i})^{k}\big{(}1+(1-\eta\gamma_{i})^{K} \big{)}&\gamma_{i}\neq 0.\end{array}\right.\]

The last equation for \(\gamma_{i}\neq 0\) holds for \(1-\eta\gamma_{i}\geq 1-\eta\gamma_{d}\geq 0\).

We define \(h(\gamma)\coloneqq\eta\sum_{k=0}^{K-1}(1-\eta\gamma)^{k}\big{(}1+(1-\eta \gamma)^{K}\big{)}\), which is decreasing on \((-\infty,\gamma_{d}]\), achieving the maximum at the smallest eigenvalue \(\gamma_{1}\). Therefore,

\[\|\bm{C}\|=\eta\sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}\left(2-\eta\gamma_{1} \sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}\right)\asymp\left\{\begin{array}{ll}( 1-\eta\gamma_{1})^{2K},&\gamma_{1}<0,\\ K,&\gamma_{1}=0,\\ 1,&\gamma_{1}>0.\end{array}\right.\]

Now, we are ready to prove Proposition 5.4. Here we prove a more general version of Proposition 5.4 in the main paper by additionally considering the cases where \(\gamma_{1}\geq 0\). Proposition 5.4 in the main paper can be simply derived by letting \(\gamma_{1}<0\).

**Proposition B.5** (Expansion properties of UD-based algorithms, generalized Proposition 5.4).: _Suppose we solve Example 5.3 by UD-based Algorithm 1 with constant inner step size \(\eta\) where \(1-\eta\gamma_{d}\geq 0\)_and outer step size \(\alpha_{t}\). Then (1) the outer update rules \(G_{\bm{z}_{i},\alpha_{t}}\) and \(G_{\tilde{z}_{i},\alpha_{t}}\) are \(2\alpha_{t}L^{\prime}\)- divergent along \(\bm{v}_{1}\), and (2) the composite validation loss \(\mathcal{L}(\cdot;\bm{z})\) is \(\gamma^{\prime}\)-expansive along \(\bm{v}_{1}\) for all \(\bm{z}\in S^{\text{val}}\), where_

\[L\asymp L^{\prime}\asymp\left\{\begin{array}{ll}(1+\eta\gamma^{\text{tr}})^ {2K},&\gamma_{1}<0,\\ K,&\gamma_{1}=0,\\ 1,&\gamma_{1}>0,\end{array}\right.\text{ and }\gamma=\gamma^{\prime}\asymp \left\{\begin{array}{ll}(1+\eta\gamma^{\text{tr}})^{2K},&\gamma_{1}<0,\\ K,&\gamma_{1}=0,\\ 1,&\gamma_{1}>0.\end{array}\right.\]

Proof.: As Example 5.3 satisfies Assumption B.1, we have \(L^{\prime}\leq L\lesssim(1+\eta\gamma^{\text{tr}})^{K}\) by Theorem 3 in [17]. We are going to verify that \(L^{\prime}\gtrsim(1+\eta\gamma^{\text{tr}})^{K}\) and \(\gamma=\gamma^{\prime}\gtrsim(1+\eta\gamma^{\text{tr}})^{2K}\) in the following.

Given a hyperparameter \(\bm{\lambda}\), a constant step size \(\eta\) and a initial point \(\bm{\theta}_{0}\), at each step \(1\leq k\leq K\), we have an inner update

\[G_{\bm{\lambda},\eta}(\bm{\theta}_{k-1}) =\bm{\theta}_{k-1}-\eta\nabla_{\bm{\theta}}\ell^{\text{tr}}(\bm{ \lambda},\bm{\theta}_{k-1};\bm{z}_{j_{k}}^{\text{tr}})\] \[=\bm{\theta}_{k-1}-\eta\nabla_{\bm{\theta}}\bigg{[}\frac{1}{2} \bm{\theta}_{k-1}^{\top}\bm{A}\bm{\theta}_{k-1}+\bm{\lambda}^{\top}\bm{\theta }_{k-1}-y_{j_{k}}^{\text{tr}}\bm{x}_{j_{k}}^{\text{tr}\top}\bm{\theta}_{k-1} \bigg{]}\] \[=\bm{\theta}_{k-1}-\eta(\bm{A}\bm{\theta}_{k-1}+\bm{\lambda}-y_{j _{k}}^{\text{tr}}\bm{x}_{j_{k}}^{\text{tr}})\] \[=(\bm{I}-\eta\bm{A})\bm{\theta}_{k-1}-\eta(\bm{\lambda}-y_{j_{k}} ^{\text{tr}}\bm{x}_{j_{k}}^{\text{tr}}),\]

where \(j_{k}\) is uniformly sampled from \([n]\). Recursively, we get

\[\bm{\theta}_{K}(\bm{\lambda}) =G_{\bm{\lambda},\eta}\Big{(}G_{\bm{\lambda},\eta}\big{(}\ldots G_ {\bm{\lambda},\eta}(\bm{\theta}_{0})\big{)}\Big{)}\] \[=(\bm{I}-\eta\bm{A})^{K}\bm{\theta}_{0}-\eta\sum_{k=0}^{K-1}(\bm{I }-\eta\bm{A})^{k}(\bm{\lambda}-y_{j_{k}}^{\text{tr}}\bm{x}_{j_{k}}^{\text{tr} }),\]

so that

\[\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})=-\eta\sum_{k=0}^{K-1}(\bm{I }-\eta\bm{A})^{k}.\] (9)

As \(L^{\prime}\) and \(\gamma^{\prime}\) describe the expansion properties of SGD on \(\mathcal{L}\), we first investigate the gradient of the compound validation loss

\[\nabla\mathcal{L}(\bm{\lambda};\bm{z})=\bm{\theta}_{K}(\bm{\lambda})+\nabla_{ \bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})^{\top}\big{(}\bm{A}\bm{\theta}_{K} (\bm{\lambda})+\bm{\lambda}-y\bm{x}\big{)}.\]

For all \(\bm{\lambda}\in\Lambda\), the outer update divergence when SGD picks the distinct examples is

\[G_{\bm{z}_{i},\alpha_{t}}(\bm{\lambda})-G_{\tilde{\bm{z}}_{i}, \alpha_{t}}(\bm{\lambda}) =\alpha_{t}\big{(}\nabla\mathcal{L}(\bm{\lambda};\bm{z}_{i})- \nabla\mathcal{L}(\bm{\lambda};\tilde{\bm{z}}_{i})\big{)}\] \[=\alpha_{t}\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})^{ \top}\big{(}-y_{i}\bm{x}_{i}-(-\tilde{y}_{i}\tilde{\bm{x}}_{i})\big{)}\] \[=-2\alpha_{t}\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})^{ \top}\bm{v}_{1}\hskip 56.905512pt(y_{i}=\tilde{y}_{i}=1,\bm{x}_{i}=\bm{v}_{1}, \tilde{\bm{x}}_{i}=-\bm{v}_{1})\] \[=2\alpha_{t}\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\bm{v}_{1} \hskip 56.905512pt(\bm{A}\text{ is symmetric})\] \[=2\alpha_{t}\eta\sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}\bm{v}_{1} \hskip 56.905512pt(\bm{A}\bm{v}_{1}=\gamma_{1}\bm{v}_{1})\] \[=2\alpha_{t}\eta\sum_{k=0}^{K-1}(1+\eta\gamma^{\text{tr}})^{k}\bm{v} _{1}\hskip 56.905512pt(\gamma^{\text{tr}}=|\gamma_{1}|=-\gamma_{1})\]

Recalling Definition 4.1, we have \(G_{\bm{z}_{i},\alpha_{t}}\) and \(G_{\bm{z}_{i}^{\prime},\alpha_{t}}\) are \(\alpha_{t}L^{\prime}\)-divergent along \(\bm{v}_{1}\), where

\[L^{\prime}=\eta\sum_{k=0}^{K-1}(1+\eta\gamma^{\text{tr}})^{k}=\left\{\begin{array}[ ]{ll}\big{[}(1+\eta\gamma^{\text{tr}})^{K}-1\big{]}/\gamma^{\text{tr}}\asymp(1+ \eta\gamma^{\text{tr}})^{K},&\gamma_{1}<0,\\ \eta K\asymp K,&\gamma_{1}=0,\\ \big{[}1-(1+\eta\gamma^{\text{tr}})^{K}\big{]}/\gamma^{\text{tr}},\asymp 1& \gamma_{1}>0.\end{array}\right.\] (10)

For the case that \(\gamma_{1}<0\), we have \(L^{\prime}\asymp(1+\eta\gamma^{\text{tr}})^{K}\).

Next, we are going to clarify that \(\mathcal{L}(\bm{\lambda})\) is \(\gamma^{\prime}\)-expansive along \(\bm{v}_{1}\), and \(\gamma^{\prime}\) equals to the smooth constant \(\gamma\) of \(\mathcal{L}(\bm{\lambda})\). As \(\mathcal{L}\) is twice differentiable, according to the definition of smoothness, we have

\[\gamma =\sup_{\bm{\lambda}\in\Lambda}\lVert\nabla_{\bm{\lambda}}^{2} \mathcal{L}(\bm{\lambda};\bm{z})\rVert\] \[=\lVert\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})+\nabla _{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})^{\top}\big{(}\bm{A}\nabla_{\bm{ \lambda}}\bm{\theta}_{K}(\bm{\lambda})+\bm{I}\big{)}\rVert\] ( \[\nabla_{\bm{\lambda}}^{2}\bm{\theta}_{K}(\bm{\lambda})=\bm{0}\] ) \[=\left\lVert-\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\Bigg{(}2 \bm{I}-\eta\bm{A}\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\Bigg{)}\right\rVert\] \[=\eta\sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}\Bigg{(}2-\eta\gamma_ {1}\sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}\Bigg{)}\] (Lemma B.4) \[=\eta\sum_{k=0}^{K-1}(1+\eta\gamma^{\mathrm{tr}})^{k}\Bigg{(}2+ \eta\gamma^{\mathrm{tr}}\sum_{k=0}^{K-1}(1+\eta\gamma^{\mathrm{tr}})^{k}\Bigg{)}\] (11) \[\asymp\left\{\begin{array}{ll}(1+\eta\gamma^{\mathrm{tr}})^{2K}, &\gamma_{1}<0,\\ K,&\gamma_{1}=0,\\ 1,&\gamma_{1}>0.\end{array}\right.\]

Eq. (11) holds for \(\gamma^{\mathrm{tr}}=\lvert\gamma_{1}\rvert=-\gamma_{1}\). For all \(\bm{\lambda},\bm{\lambda}^{\prime}\in\Lambda\), such that \(\bm{\lambda}-\bm{\lambda}^{\prime}=a\bm{v}_{1}\triangleq\bm{v}_{1}\) where \(a\in\mathbb{R}_{+}\), we have

\[\nabla\mathcal{L}(\bm{\lambda};\bm{z})-\nabla\mathcal{L}(\bm{ \lambda}^{\prime};\bm{z}) =\bm{\theta}_{K}(\bm{\lambda})-\bm{\theta}_{K}(\bm{\lambda}^{ \prime})\] \[=-\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}(\bm{\lambda}-\bm{ \lambda}^{\prime})-\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\left(-\bm{A} \eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}+\bm{I}\right)(\bm{\lambda}-\bm{ \lambda}^{\prime})\] \[=-\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\left(2\bm{I}-\eta \bm{A}\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\right)(\bm{\lambda}-\bm{\lambda} ^{\prime})\] \[=-\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\Bigg{(}2\bm{I}-\eta \bm{A}\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\Bigg{)}a\bm{v}_{1}\] \[=-\eta\sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}\Bigg{(}2-\eta\gamma_ {1}\sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}\Bigg{)}a\bm{v}_{1}\] \[=-\eta\sum_{k=0}^{K-1}(1+\eta\gamma^{\mathrm{tr}})^{k}\Bigg{(}2+ \eta\gamma^{\mathrm{tr}}\sum_{k=0}^{K-1}(1+\eta\gamma^{\mathrm{tr}})^{k} \Bigg{)}(\bm{\lambda}-\bm{\lambda}^{\prime}),\] \[\coloneqq-\gamma^{\prime}(\bm{\lambda}-\bm{\lambda}^{\prime}).\]

According to Definition 4.3, this implies \(\mathcal{L}(\bm{\lambda})\) is \(\gamma^{\prime}\)-expansive along \(\bm{v}_{1}\). Therefore

\[\gamma^{\prime}=\gamma\asymp\left\{\begin{array}{ll}(1+\eta\gamma^{\mathrm{ tr}})^{2K},&\gamma_{1}<0,\\ K,&\gamma_{1}=0,\\ 1,&\gamma_{1}>0.\end{array}\right.\]

For the case that \(\gamma_{1}<0\), we obtain that \(\gamma^{\prime}=\gamma\asymp(1+\eta\gamma^{\mathrm{tr}})^{2K}\).

### Proof of Theorem 5.5

Here we prove a more general version of Theorem 5.5 in the main paper where \(\gamma_{1}<0\) by additionally considering the cases where \(\gamma_{1}\geq 0\).

[MISSING_PAGE_FAIL:21]

Then, we continue to derive the right side of the result (i.e., the upper bound). Based on Eq. (6), we have

\[\epsilon_{\mathrm{arg}} \leq\sum_{t=1}^{T}\prod_{s=t+1}^{T}\bigl{(}1+\alpha_{s}(1-1/m) \gamma\bigr{)}\frac{2\alpha_{t}L}{m}\] \[\leq\sum_{t=1}^{T-1}\prod_{s=t+1}^{T}\exp\Biggl{[}\biggl{(}1- \frac{1}{m}\biggr{)}\frac{\gamma c}{s}\Biggr{]}\frac{2cL}{tm}+\frac{2cL}{Tm}\] \[=\sum_{t=1}^{T-1}\exp\Biggl{[}\biggl{(}1-\frac{1}{m}\biggr{)} \gamma c\sum_{s=t+1}^{T}\frac{1}{s}\Biggr{]}\frac{2cL}{tm}+\frac{2cL}{Tm}\] \[\leq\sum_{t=1}^{T-1}\exp\Biggl{[}\biggl{(}1-\frac{1}{m}\biggr{)} \gamma c\ln\frac{T}{t}\Biggr{]}\frac{2cL}{tm}+\frac{2cL}{Tm}\qquad\quad( \forall t_{2}>t_{1}>1,\sum_{t=t_{1}}^{t_{2}}\frac{1}{t}\leq\ln\frac{t_{2}}{t_ {1}-1}))\] \[=\sum_{t=1}^{T}\biggl{(}\frac{T}{t}\biggr{)}^{(1-1/m)\gamma c} \frac{2cL}{tm}\] \[=\frac{2cL}{m}T^{(1-1/m)\gamma c}\sum_{t=1}^{T}t^{-\left(1-1/m \right)\gamma c-1}\] \[\leq\frac{2cL}{m}T^{(1-1/m)\gamma c}\Biggl{(}1+\int_{1}^{T}t^{- \left(1-1/m\right)\gamma c-1}dt\Biggr{)}\] \[(\forall a>0,\sum_{t=1}^{T}t^{-a-1}\leq 1+\int_{1}^{T}t^{-a-1}dt)\] \[=\frac{2cL}{m(1-1/m)c\gamma}\biggl{[}\Bigl{(}1+\bigl{(}1-1/m \bigr{)}\gamma c\Bigr{)}T^{(1-1/m)\gamma c}-1\biggr{]}\] \[=\frac{2L}{(m-1)\gamma}\biggl{[}\Bigl{(}1+\bigl{(}1-1/m\bigr{)} \gamma c\Bigr{)}T^{(1-1/m)\gamma c}-1\biggr{]}.\]

Therefore, it holds that

\[\frac{2cL^{\prime}\Biggl{[}\biggl{(}\frac{T+1}{2}\biggr{)}^{\ln\left(1+(1-1/m )c\gamma^{\prime}\right)}-1\Biggr{]}}{m\ln\bigl{(}1+(1-1/m)c\gamma^{\prime} \bigr{)}}\leq\epsilon_{\mathrm{arg}}\leq\frac{2L\biggl{[}\Bigl{(}1+\bigl{(}1 -1/m\bigr{)}\gamma c\Bigr{)}T^{\left(1-1/m\right)\gamma c}-1\biggr{]}}{(m-1) \gamma}.\] (12)

Omitting the constants depending on \(c\), \(\gamma\), and \(L\), \(\gamma t\) and \(L^{\prime}\), we have \(\frac{T^{\ln\left(1+(1-\frac{1}{m})\gamma c^{\prime}\right)}}{m}\lesssim \epsilon_{\mathrm{arg}}\lesssim\frac{T^{(1-\frac{1}{m})\gamma c\gamma}}{m}\), which completes the proof.

### Proof of Theorem 5.6

Theorem 5.6: Following the same condition as in Theorem 5.5, and additionally, if the initial points \(\bm{\theta}_{0}=\bm{0},\bm{\lambda}_{0}=\bm{0}\), and \(\bm{v}_{1}\perp\bm{x}_{j}^{\mathrm{val}}\) for any \(j\in[m]\backslash i\) and \(\bm{v}_{1}\perp\bm{x}_{j}^{\mathrm{tr}}\) for any \(j\in[n]\), then the order of uniform stability \(\epsilon_{\mathrm{stab}}\) w.r.t. \(T\) satisfies \(\frac{T^{\ln\left(1+(1-\frac{1}{m})\gamma c\gamma\right)}}{m}\lesssim\epsilon_ {\mathrm{stab}}\lesssim\frac{T^{(1-\frac{1}{m})c\gamma}}{m},\) where \(\gamma=\gamma^{\prime}\asymp(1+\eta\gamma^{\mathrm{tr}})^{2K}\) as in Proposition 5.4.

Proof.: In the following, we show that \(\epsilon_{\mathrm{stab}}\) explicitly shows the same order as \(\epsilon_{\mathrm{arg}}\) in Theorem 5.5 with additional assumptions for Example 5.3. For the upper bound, it is easy to get with Lipschitz condition that

\[\epsilon_{\mathrm{stab}} =\sup_{S^{\mathrm{val}}\simeq\tilde{S}^{\mathrm{val}}\in\mathbb{Z} ^{m},S^{\mathrm{tr}}\in\mathbb{Z}^{n},\bm{z}\in\mathbb{Z}}\mathbb{E}_{\mathcal{ A}}[\|\mathcal{L}(\mathcal{A}(S^{\mathrm{val}},S^{\mathrm{tr}});\bm{z})- \mathcal{L}(\mathcal{A}(\tilde{S}^{\mathrm{val}},S^{\mathrm{tr}});\bm{z})\|]\] \[\leq\sup_{S^{\mathrm{val}}\simeq\tilde{S}^{\mathrm{val}}\in \mathbb{Z}^{m},S^{\mathrm{tr}}\in\mathbb{Z}^{n}}\mathbb{E}_{\mathcal{A}}[L\| \mathcal{A}(S^{\mathrm{val}},S^{\mathrm{tr}})-\mathcal{A}(\tilde{S}^{\mathrm{ val}},S^{\mathrm{tr}})\|]\] \[=L\epsilon_{\mathrm{arg}},\]and according to Theorem 5.5, we have \(\epsilon_{\rm stab}\lesssim\frac{T^{(1-\frac{m}{\lambda})\epsilon\gamma}}{m}\).

To obtain the lower bound, we need to explicitly derive the optimization process of \(\mathcal{A}(S^{\rm val},S^{\rm tr})\) and \(\mathcal{A}(\tilde{S}^{\rm val},S^{\rm tr})\) (i.e., \(\bm{\lambda}_{T}\) and \(\tilde{\bm{\lambda}}_{T}\)), and corresponding loss values.

From the proof of Proposition 5.4, we know that

\[\bm{\theta}_{K}(\bm{\lambda}) =G_{\bm{\lambda},\eta}\Big{(}G_{\bm{\lambda},\eta}\big{(}\ldots G_ {\bm{\lambda},\eta}(\bm{\theta}_{0})\big{)}\Big{)}\] \[=(\bm{I}-\eta\bm{A})^{K}\bm{\theta}_{0}-\eta\sum_{k=0}^{K-1}(\bm {I}-\eta\bm{A})^{k}(\bm{\lambda}-y_{j_{k}}^{\rm tr}\bm{x}_{j_{k}}^{\rm tr})\] \[=-\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}(\bm{\lambda}-y_{j_{ k}}^{\rm tr}\bm{x}_{j_{k}}^{\rm tr}).\] ( \[\bm{\theta}_{0}=\bm{0}\] ) \[=-\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\bm{\lambda}+\eta \sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}y_{j_{k}}^{\rm tr}\bm{x}_{j_{k}}^{\rm tr}\] \[\coloneqq-\bm{B}_{K}\bm{\lambda}+\bm{b}_{K}^{\rm tr},\]

where symmetric matrix \(\bm{B}_{K}\coloneqq\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\) and vector \(\bm{b}_{K}^{\rm tr}\coloneqq\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}y_{j_{k }}^{\rm tr}\bm{x}_{j_{k}}^{\rm tr}\).

Building upon \(\bm{\theta}_{K}(\bm{\lambda})\), we can derive \(\mathcal{L}(\bm{\lambda},\bm{z})\) as

\[\mathcal{L}(\bm{\lambda},\bm{z}) =\frac{1}{2}\bm{\theta}_{K}^{\top}(\bm{\lambda})\bm{A}\bm{\theta }_{K}(\bm{\lambda})+\bm{\lambda}^{\top}\bm{\theta}_{K}(\bm{\lambda})-y\bm{x}^ {\top}\bm{\theta}_{K}(\bm{\lambda})\] \[=\frac{1}{2}(-\bm{B}_{K}\bm{\lambda}+\bm{b}_{K}^{\rm tr})^{\top} \bm{A}(-\bm{B}_{K}\bm{\lambda}+\bm{b}_{K}^{\rm tr})+\bm{\lambda}^{\top}(-\bm{B }_{K}\bm{\lambda}+\bm{b}_{K}^{\rm tr})-y\bm{x}^{\top}(-\bm{B}_{K}\bm{\lambda}+ \bm{b}_{K}^{\rm tr})\] \[=\frac{1}{2}\bm{\lambda}^{\top}(\bm{B}_{K}\bm{A}\bm{B}_{K}-2\bm{B }_{K})\bm{\lambda}+(-\bm{b}_{K}^{\rm tr\top}\bm{A}\bm{B}_{K}+\bm{b}_{K}^{\rm tr \top}+y\bm{x}^{\top}\bm{B}_{K})\bm{\lambda}+\frac{1}{2}\bm{b}_{K}^{\rm tr\top} \bm{A}\bm{b}_{K}^{\rm tr}-y\bm{x}^{\top}\bm{b}_{K}^{\rm tr},\]

whose gradient is

\[\nabla_{\bm{\lambda}}\mathcal{L}(\bm{\lambda},\bm{z}) =\bm{B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\bm{\lambda}+(-\bm{B}_{K} \bm{A}\bm{b}_{K}^{\rm tr}+\bm{b}_{K}^{\rm tr}+y\bm{B}_{K}\bm{x})\] \[=\bm{B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\bm{\lambda}+y\bm{B}_{K}\bm{x }+\bm{b}_{K}^{\rm tr}-\bm{B}_{K}\bm{A}\bm{b}_{K}^{\rm tr}.\]

Then, the update rule of \(\bm{\lambda}\) can be expressed as

\[\bm{\lambda}_{t}= \bm{\lambda}_{t-1}-\alpha_{t}\nabla_{\bm{\lambda}}\mathcal{L}( \bm{\lambda}_{t-1},\bm{z}_{i_{t}})\] \[=\bm{\lambda}_{t-1}-\alpha_{t}\Big{[}\bm{B}_{K}(\bm{A}\bm{B}_{K}-2 \bm{I})\bm{\lambda}_{t-1}+y_{i_{t}}\bm{B}_{K}\bm{x}_{i_{t}}+\bm{b}_{K}^{\rm tr }-\bm{B}_{K}\bm{A}\bm{b}_{K}^{\rm tr}\Big{]}\] \[=\big{[}\bm{I}-\alpha_{t}\bm{B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\big{]} \bm{\lambda}_{t-1}-\alpha_{t}y_{i_{t}}\bm{B}_{K}\bm{x}_{i_{t}}-\alpha_{t}(\bm{b }_{K}^{\rm tr}-\bm{B}_{K}\bm{A}\bm{b}_{K}^{\rm tr}).\]

Now, by unwinding the recurrence from \(T\) to \(1\) with \(\bm{\lambda}_{0}=\bm{0}\), we can obtain

\[\bm{\lambda}_{T} =\prod_{t=1}^{T}\big{[}\bm{I}-\alpha_{t}\bm{B}_{K}(\bm{A}\bm{B} _{K}-2\bm{I})\big{]}\bm{\lambda}_{0}+\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{[} \bm{I}-\alpha_{s}\bm{B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\big{]}(-\alpha_{t}y_{i_{ t}}\bm{B}_{K}\bm{x}_{i_{t}})\] \[\quad+\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{[}\bm{I}-\alpha_{s}\bm{ B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\big{]}(-\alpha_{t}(\bm{b}_{K}^{\rm tr}-\bm{B}_{K} \bm{A}\bm{b}_{K}^{\rm tr}))\] \[=\underbrace{\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{[}\bm{I}-\alpha_{s }\bm{B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\big{]}(-\alpha_{t}y_{i_{t}}\bm{B}_{K}\bm{x} _{i_{t}})}_{\bm{r}}\] \[\quad+\underbrace{\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{[}\bm{I}- \alpha_{s}\bm{B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\big{]}(-\alpha_{t}(\bm{b}_{K}^{ \rm tr}-\bm{B}_{K}\bm{A}\bm{b}_{K}^{\rm tr}))}_{\bm{a}_{1}}.\] ( \[\bm{\lambda}_{0}=\bm{0}\]Recall that \(S^{\mathrm{val}}\) and \(\tilde{S}^{\mathrm{val}}\) only differ in the \(i\)-th entry where \(\bm{z}_{i}=(\bm{x}_{i},y_{i})=(\bm{v}_{1},1),\tilde{\bm{z}}_{i}=(\tilde{\bm{x}} _{i},\tilde{y}_{i})=(-\bm{v}_{1},1)\). Denote \(\mathbbm{1}[\cdot]\) as the indicator function. We simplify the term \(\bm{r}\) as follows:

\[\bm{r} =\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{[}\bm{I}-\alpha_{s}\bm{B}_{K }(\bm{AB}_{K}-2\bm{I})\big{]}\left(-\alpha_{t}\bm{B}_{K}\sum_{j=1}^{m}y_{j}\bm{ x}_{j}\mathbbm{1}[i_{t}=j]\right)\] \[=\sum_{j=1}^{m}\sum_{t=1}^{T}\mathbbm{1}[i_{t}=j]\prod_{s=t+1}^{T }\big{[}\bm{I}-\alpha_{s}\bm{B}_{K}(\bm{AB}_{K}-2\bm{I})\big{]}\left(-\alpha_ {t}\bm{B}_{K}y_{j}\bm{x}_{j}\right)\] \[=\sum_{t=1}^{T}\mathbbm{1}[i_{t}=i]\prod_{s=t+1}^{T}\big{[}\bm{I} -\alpha_{s}\bm{B}_{K}(\bm{AB}_{K}-2\bm{I})\big{]}\left(-\alpha_{t}\bm{B}_{K}y _{i}\bm{x}_{i}\right)\] \[=\sum_{t=1}^{T}\mathbbm{1}[i_{t}=i]\prod_{s=t+1}^{T}\big{[}\bm{I} -\alpha_{s}\bm{B}_{K}(\bm{AB}_{K}-2\bm{I})\big{]}\left(-\alpha_{t}\bm{B}_{K} \bm{v}_{1}\right)\] \[\quad+\sum_{j\neq i}^{m}\sum_{t=1}^{T}\mathbbm{1}[i_{t}=j]\prod_{ s=t+1}^{T}\big{[}\bm{I}-\alpha_{s}\bm{B}_{K}(\bm{AB}_{K}-2\bm{I})\big{]}\left(- \alpha_{t}\bm{B}_{K}y_{j}\bm{x}_{j}\right)\] \[=\sum_{t=1}^{T}\mathbbm{1}[i_{t}=i]\prod_{s=t+1}^{T}\Bigg{[}\bm{I} -\alpha_{s}\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}(\bm{A}\eta\sum_{k=0}^{ K-1}(\bm{I}-\eta\bm{A})^{k}-2\bm{I})\Bigg{]}\left(-\alpha_{t}\eta\sum_{k=0}^{K-1}(\bm{I}- \eta\bm{A})^{k}\bm{v}_{1}\right)\] \[\quad+\sum_{j\neq i}^{m}\sum_{t=1}^{T}\mathbbm{1}[i_{t}=j]\prod_{ s=t+1}^{T}\big{[}\bm{I}-\alpha_{s}\bm{B}_{K}(\bm{AB}_{K}-2\bm{I})\big{]}\left(- \alpha_{t}\bm{B}_{K}y_{j}\bm{x}_{j}\right)\] \[=\sum_{t=1}^{T}\mathbbm{1}[i_{t}=i]\prod_{s=t+1}^{T}\Bigg{[}1- \alpha_{s}\eta\sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}(\gamma_{1}\eta\sum_{k=0}^ {K-1}(1-\eta\gamma_{1})^{k}-2)\Bigg{]}\left(-\alpha_{t}\eta\sum_{k=0}^{K-1}(1- \eta\gamma_{1})^{k}\right)\bm{v}_{1}\] \[\quad+\sum_{j\neq i}^{m}\sum_{t=1}^{T}\mathbbm{1}[i_{t}=j]\prod_{ s=t+1}^{T}\big{[}\bm{I}-\alpha_{s}\bm{B}_{K}(\bm{AB}_{K}-2\bm{I})\big{]}\left(- \alpha_{t}\bm{B}_{K}y_{j}\bm{x}_{j}\right)\] \[=-\underbrace{\sum_{t=1}^{T}\mathbbm{1}[i=i_{t}]\prod_{s=t+1}^{T} \big{[}1-\alpha_{s}L^{\prime}(\gamma_{1}L^{\prime}-2)\big{]}\left(\alpha_{t}L ^{\prime}\right)\bm{v}_{1}}_{\bm{b}=\tau\bm{v}_{1}}\] \[\quad+\underbrace{\sum_{j\neq i}^{m}\sum_{t=1}^{T}\mathbbm{1}[i_{t }=j]\prod_{s=t+1}^{T}\big{[}\bm{I}-\alpha_{s}\bm{B}_{K}(\bm{AB}_{K}-2\bm{I}) \big{]}\left(-\alpha_{t}\bm{B}_{K}y_{j}\bm{x}_{j}\right)}_{\bm{a}_{2}},\]

where \(L^{\prime}=\eta\sum_{k=0}^{K-1}(1-\eta\gamma_{1})^{k}\) as in Proposition B.5. We further define \(\bm{a}\coloneqq\bm{a}_{1}+\bm{a}_{2}\), and then \(\bm{\lambda}_{T}=\bm{a}_{1}+\bm{a}_{2}-\bm{b}=\bm{a}-\bm{b}\). Follow the same process of derivation, we have \(\tilde{\bm{\lambda}}_{T}=\bm{a}+\bm{b}\) where the opposite symbol for \(b\) arise from \(\bm{x}_{i}=\bm{v}_{1}\) while \(\tilde{\bm{x}}_{i}=\bm{v}_{1}\).

Recall that we have assumed that \(\bm{v}_{1}\perp\bm{x}_{k}^{\mathrm{tr}}\) for any \(k\in[n]\) and \(\bm{v}_{1}\perp\bm{x}_{j}^{\mathrm{val}}\) for any \(j\in[m]\) that \(j\neq i\), thus \(\bm{v}_{1}^{\top}\bm{b}_{K}^{\mathrm{tr}}=0\) and \(\bm{v}_{1}^{\top}\bm{x}_{j}=0,j\neq i\in[m]\). Therefore,

\[\bm{a}^{\top}\bm{b} =\sum_{t=1}^{T}(-\alpha_{t}(\bm{b}_{K}^{\mathrm{tr}}-\bm{B}_{K} \bm{Ab}_{K}^{\mathrm{tr}}))^{\top}\prod_{s=t+1}^{T}\big{[}\bm{I}-\alpha_{s}\bm{B }_{K}(\bm{AB}_{K}-2\bm{I})\big{]}\tau\bm{v}_{1}\] \[+\sum_{j\neq i}^{m}\sum_{t=1}^{T}\mathbbm{1}[i_{t}=j]\left(- \alpha_{t}\bm{B}_{K}y_{j}\bm{x}_{j}\right)^{\top}\prod_{s=t+1}^{T}\big{[}\bm{I}- \alpha_{s}\bm{B}_{K}(\bm{AB}_{K}-2\bm{I})\big{]}\tau\bm{v}_{1}.\]As \(\bm{B}_{K}\coloneqq\eta\sum_{k=0}^{K-1}(\bm{I}-\eta\bm{A})^{k}\) and \(\bm{v}_{1}\) is the eigenvector of \(\bm{A}\), we have \(\bm{a}^{\top}\bm{b}=0\), i.e., \(\bm{a}\perp\bm{b}\).

Now, we are ready to discuss \(\mathcal{L}(\bm{\lambda}_{T},\bm{z})-\mathcal{L}(\tilde{\bm{\lambda}}_{T},\bm{z})\).

\[\mathcal{L}(\bm{\lambda}_{T},\bm{z})-\mathcal{L}(\tilde{\bm{ \lambda}}_{T},\bm{z})\] \[=\frac{1}{2}\bm{\lambda}_{T}^{\top}\bm{B}_{K}(\bm{A}\bm{B}_{K}-2 \bm{I})\bm{\lambda}_{T}+(-\bm{b}_{K}^{\mathrm{tr}\top}\bm{A}\bm{B}_{K}+\bm{b}_ {K}^{\mathrm{tr}\top}+y\bm{x}^{\top}\bm{B}_{K})\bm{\lambda}_{T}\] \[\quad-\frac{1}{2}\tilde{\bm{\lambda}}_{T}^{\top}\bm{B}_{K}(\bm{A} \bm{B}_{K}-2\bm{I})\tilde{\bm{\lambda}}_{T}-(-\bm{b}_{K}^{\mathrm{tr}\top}\bm{ A}\bm{B}_{K}+\bm{b}_{K}^{\mathrm{tr}\top}+y\bm{x}^{\top}\bm{B}_{K})\tilde{\bm{ \lambda}}_{T}\] \[=\underbrace{\frac{1}{2}\bm{\lambda}_{T}^{\top}\bm{B}_{K}(\bm{A} \bm{B}_{K}-2\bm{I})\bm{\lambda}_{T}-\frac{1}{2}\bm{\lambda}_{T}^{\top}\bm{B}_ {K}(\bm{A}\bm{B}_{K}-2\bm{I})\tilde{\bm{\lambda}}_{T}}_{\text{e}}+(-\bm{b}_{K} ^{\mathrm{tr}\top}\bm{A}\bm{B}_{K}+\bm{b}_{K}^{\mathrm{tr}\top}+y\bm{x}^{\top }\bm{B}_{K})(\bm{\lambda}_{T}-\tilde{\bm{\lambda}}_{T}),\]

where

\[\bm{c} =\frac{1}{2}(\bm{a}-\bm{b})^{\top}\bm{B}_{K}(\bm{A}\bm{B}_{K}-2 \bm{I})(\bm{a}-\bm{b})-\frac{1}{2}(\bm{a}+\bm{b})^{\top}\bm{B}_{K}(\bm{A}\bm{ B}_{K}-2\bm{I})(\bm{a}+\bm{b})\] \[=-2\bm{a}^{\top}\bm{B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\bm{b}-2\bm{b} ^{\top}\bm{B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\bm{a}\] \[=-4\bm{a}^{\top}\bm{B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\bm{b}\] (by symmetric) \[=4\bm{a}^{\top}\bm{B}_{K}(\bm{A}\bm{B}_{K}-2\bm{I})\tau\bm{v}_{1}\] \[=4(\gamma_{1}L^{\prime}-2)\tau\bm{a}^{\top}\bm{v}_{1}\] \[=\bm{0}.\]

Therefore, \(\mathcal{L}(\bm{\lambda}_{T},\bm{z})-\mathcal{L}(\tilde{\bm{\lambda}}_{T},\bm {z})\) can be simplified as

\[\mathcal{L}(\bm{\lambda}_{T},\bm{z})-\mathcal{L}(\tilde{\bm{ \lambda}}_{T},\bm{z}) =(-\bm{b}_{K}^{\mathrm{tr}\top}\bm{A}\bm{B}_{K}+\bm{b}_{K}^{ \mathrm{tr}\top}+y\bm{x}^{\top}\bm{B}_{K})(\bm{\lambda}_{T}-\tilde{\bm{ \lambda}}_{T})\] \[=-2(-\bm{b}_{K}^{\mathrm{tr}\top}\bm{A}\bm{B}_{K}+\bm{b}_{K}^{ \mathrm{tr}\top}+y\bm{x}^{\top}\bm{B}_{K})\bm{b}\] ( \[\bm{\lambda}_{T}-\tilde{\bm{\lambda}}_{T}=-2\bm{b}\] ) \[=-2y\bm{x}^{\top}\bm{B}_{K}\bm{b}\] \[=-2yL^{\prime}\bm{x}^{\top}\bm{v}_{1}.\] ( \[\bm{b}=\tau\bm{v}_{1}\] )

Let \(\bm{z}^{*}=(\bm{v}_{1},1)\), we have

\[|\mathcal{L}(\bm{\lambda}_{T},\bm{z}^{*})-\mathcal{L}(\tilde{\bm{ \lambda}}_{T},\bm{z}^{*})|=2L^{\prime}\tau\|\bm{v}_{1}\|^{2}=2L^{\prime}\tau=L^ {\prime}\|\bm{\lambda}_{T}-\tilde{\bm{\lambda}}_{T}\|.\qquad(\|\bm{\lambda}_{ T}-\tilde{\bm{\lambda}}_{T}\|=2\tau)\]

Therefore, by the definition of \(\epsilon_{\mathrm{stab}}\), we have

\[\epsilon_{\mathrm{stab}}\geq|\mathcal{L}(\bm{\lambda}_{T},\bm{z}^{*})- \mathcal{L}(\tilde{\bm{\lambda}}_{T},\bm{z}^{*})|=L^{\prime}\|\bm{\lambda}_{T} -\tilde{\bm{\lambda}}_{T}\|\geq\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{(}1+\alpha_{ s}(1-1/m)\gamma^{\prime}\big{)}\frac{2\alpha_{t}L^{\prime}}{m},\]

where the last inequality holds for Eq. (8) in the proof of Theorem 5.1. Following the proof of Theorem B.6, we can further derive

\[\epsilon_{\mathrm{stab}}\geq\frac{2{cL^{\prime}}^{2}\Bigg{[}\Big{(}\frac{T+1} {2}\Big{)}^{\ln\left(1+(1-1/m)c\gamma^{\prime}\right)}-1\Bigg{]}}{m\mathrm{ln} \big{(}1+(1-1/m)c\gamma^{\prime}\big{)}}.\]

Omitting the constants regarding \(c\), \(\gamma^{\prime}\) and \(L^{\prime}\), we have \(\epsilon_{\mathrm{stab}}\gtrsim\frac{T^{\ln\left(1+(1-\frac{1}{m})c\gamma^{ \prime}\right)}}{m}\), which completes the proof. 

## Appendix C Deferred results of IFT-based HO algorithm

Based on Example 5.3, we also investigate and establish a stability lower bound for the IFT-based algorithm. The stability analysis for the IFT-based algorithm is conducted following the same proof idea as the UD-based algorithm. Similarly to the analysis for the UD-based algorithm, we first obtain the expansive and divergent properties of the outer level in Proposition C.5 of Appendix C. These jointly lead to uniform argument stability bounds in Theorem C.6. For completeness, we also derive an upper bound for the IFT algorithm based on existing techniques [17], presented together as follows.

We first introduce several lemmas useful for the following proofs.

**Lemma C.1** (Lemma 2 in [17]).: _Suppose \(\Lambda\) and \(\Theta\) are convex and compact with non-empty interiors, \(\mathcal{Z}\) is compact, \(\Lambda\times\bm{\Theta}\times\mathcal{Z}\) is included in an open set \(\Omega\) and \(f(\bm{\lambda},\bm{\theta};\bm{z})\in C^{k}(\Omega)\), then for all \(i\leq k-1\) order partial differential \(h(\bm{\lambda},\bm{\theta};\bm{z})\) of \(f(\bm{\lambda},\bm{\theta};\bm{z})\), we have \(\sup\limits_{\bm{\theta}\in\bm{\Theta},\bm{z}\in\mathcal{Z}}\|h(\bm{\lambda}, \bm{\theta};\bm{z})\|_{\bm{\lambda}\in\Lambda,Lip}<\infty\) and \(\sup\limits_{\bm{\lambda}\in\Lambda,\bm{z}\in\mathcal{Z}}||h(\bm{\lambda},\bm {\theta};\bm{z})||_{\bm{\theta}\in\Theta,Lip}<\infty\)._

Lemma C.1 implies that any \(i\leq 1\) order partial differential of \(\ell^{\mathrm{val}}(\bm{\lambda},\bm{\theta};\bm{z})\) is Lipschitz and any \(i\leq 2\) order partial differential of \(\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta},\bm{z})\) is Lipschitz continuous under Assumption B.1. We denote the maximal Lipschitz constants among them as \(Q\).

**Lemma C.2** (Theorem 3 in [17]).: _Denote \(\bm{\theta}_{K}(\bm{\lambda})\) as \(L^{\bm{\theta}_{K}}\)-Lipschitz continuous, we have \(L^{\bm{\theta}_{K}}\lesssim(1+\eta\gamma^{\mathrm{tr}})^{K}\)._

**Lemma C.3**.: _In the case of Example 5.3, the \(\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{\lambda})\) calculated by the IFT-based algorithm is exactly \(\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})\)._

Proof.: \[\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{\lambda}) =-\nabla^{2}_{\bm{\theta}\bm{\lambda}}\ell^{\mathrm{tr}}(\bm{ \lambda},\bm{\theta}_{K}(\bm{\lambda}))\eta\sum_{k=0}^{K-1}\Bigl{[}\bm{I}-\eta \nabla^{2}_{\bm{\theta}\bm{\theta}}\ell^{\mathrm{tr}}(\bm{\lambda},\bm{ \theta}_{K}(\bm{\lambda}))\Bigr{]}^{k}\] \[=-\eta\sum_{k=0}^{K-1}[\bm{I}-\eta\bm{A}]^{k}\] \[=\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda}).\] (Eq. ( 9 ) )

Now, we are ready to prove Propositions C.4 and C.5.

**Proposition C.4** (Lipshchitz properties of IFT-based algorithm).: _Suppose we solve Example 5.3 by IFT-based Algorithm 1 with constant inner step size \(\eta\) where \(1-\eta\gamma_{d}\geq 0\) and outer step size \(\alpha_{t}\). Then the composite validation loss \(\mathcal{L}(\cdot;\bm{z})\) is \(L\)-Lipschitz continuous and \(\gamma\)-smooth for all \(\bm{z}\in S^{\mathrm{val}}\), where_

\[L\lesssim(1+\eta\gamma^{\mathrm{tr}})^{K},\gamma\lesssim K(1+\eta\gamma^{ \mathrm{tr}})^{2K}.\]

Proof.: According to Lemma 1 in [17], the Lipschitz continuous coefficient \(L=\sup_{\bm{\lambda}\in\Lambda}\bigl{\|}\nabla_{\bm{\lambda}}\mathcal{L}(\bm {\lambda};\bm{z})\bigr{\|}\). For all \(\bm{\lambda}\in\Lambda\), we have

\[\bigl{\|}\nabla_{\bm{\lambda}}\mathcal{L}(\bm{\lambda};\bm{z}) \bigr{\|} =\Bigl{\|}\nabla_{\bm{\lambda}}\ell^{\mathrm{val}}(\bm{\lambda}, \bm{\theta}_{K}(\bm{\lambda});\bm{z})+\widehat{\nabla_{\bm{\lambda}}\bm{ \theta}_{K}}(\bm{\lambda})\nabla_{\bm{\theta}}\ell^{\mathrm{val}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda});\bm{z})\Bigr{\|}\] \[\leq\Bigl{\|}\nabla_{\bm{\lambda}}\ell^{\mathrm{val}}(\bm{\lambda },\bm{\theta}_{K}(\bm{\lambda});\bm{z})\Bigr{\|}+\Bigl{\|}\widehat{\nabla_{ \bm{\lambda}}\bm{\theta}_{K}}(\bm{\lambda})\Bigr{\|}\Bigr{\|}\nabla_{\bm{ \theta}}\ell^{\mathrm{val}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda});\bm{z })\Bigr{\|}\] \[\leq Q+Q\Bigl{\|}\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}( \bm{\lambda})\Bigr{\|}\] \[=Q+Q\Biggl{\|}\nabla^{2}_{\bm{\theta}\bm{\lambda}}\ell^{\mathrm{tr }}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda}))\eta\sum_{k=0}^{K-1}\Bigl{[}\bm{ I}-\eta\nabla^{2}_{\bm{\theta}\bm{\theta}}\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta}_{K}( \bm{\lambda}))\Bigr{]}^{k}\Biggr{\|}\] \[\leq Q+\eta Q\Bigl{\|}\nabla^{2}_{\bm{\theta}\bm{\lambda}}\ell^{ \mathrm{tr}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda}))\Bigl{\|}\sum_{k=0}^{K-1 }\Bigl{\|}\bm{I}-\eta\nabla^{2}_{\bm{\theta}\bm{\theta}}\ell^{\mathrm{tr}}(\bm{ \lambda},\bm{\theta}_{K}(\bm{\lambda}))\Bigr{\|}^{k}\] \[\leq Q+\eta Q^{2}\sum_{k=0}^{K-1}\Bigl{(}1+\eta\|\nabla^{2}_{\bm{ \theta}\bm{\theta}}\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda}) )\|\Bigr{)}^{k}\] \[\leq Q+\eta Q^{2}\sum_{k=0}^{K-1}\bigl{(}1+\eta\gamma^{\mathrm{tr }}\bigr{)}^{k}\] \[=Q+Q^{2}\frac{\bigl{(}1+\eta\gamma^{\mathrm{tr}}\bigr{)}^{K}-1}{ \gamma^{\mathrm{tr}}}.\]Omitting the constants depending on \(Q\), \(\eta\), and \(\gamma^{\mathrm{tr}}\), we get \(L\lesssim\left(\left(1+\eta\gamma^{\mathrm{tr}}\right)^{K}\right)\).

To obtain the smoothness coefficient \(\gamma\), we first discuss the Lipschitz continuity coefficient of \(\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{\lambda})\). In the following, we use \(\ell^{\mathrm{tr}}\) to represent \(\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda}))\) when there is no ambiguity.

\[\nabla_{\bm{\lambda}}\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K }}(\bm{\lambda}) =\nabla_{\bm{\lambda}}\left(-\nabla_{\bm{\theta}\bm{\lambda}}^{2} \ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda}))\eta\sum_{k=0}^{ K-1}\left[\bm{I}-\eta\nabla_{\bm{\theta}\bm{\theta}}^{2}\ell^{\mathrm{tr}}(\bm{ \lambda},\bm{\theta}_{K}(\bm{\lambda}))\right]^{k}\right)\] \[=\underbrace{-\Bigg{(}\Big{(}\nabla_{\bm{\theta}\bm{\lambda}}^{3 }\ell^{\mathrm{tr}}+\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})\nabla_{ \bm{\theta}\bm{\lambda}\bm{\theta}}^{3}\ell^{\mathrm{tr}})\Big{)}\eta\sum_{k=0 }^{K-1}\left[\bm{I}-\eta\nabla_{\bm{\theta}\bm{\theta}}^{2}\ell^{\mathrm{tr}} \right]^{k}\Bigg{)}}_{\bm{B}_{1}}\] \[\underbrace{-\Bigg{(}\nabla_{\bm{\theta}\bm{\lambda}}^{2}\ell^{ \mathrm{tr}}\eta\sum_{k=1}^{K-1}\biggl{(}-\eta\Big{(}\nabla_{\bm{\theta}\bm{ \theta}\bm{\lambda}}^{3}\ell^{\mathrm{tr}}+\nabla_{\bm{\lambda}}\bm{\theta}_{K }(\bm{\lambda})\nabla_{\bm{\theta}\bm{\lambda}\bm{\theta}}^{3}\ell^{\mathrm{ tr}})\Big{)}\bigg{)}k\Big{[}\bm{I}-\eta\nabla_{\bm{\theta}\bm{\theta}}^{2}\ell^{\mathrm{tr}}(\bm{ \lambda},\bm{\theta}_{K}(\bm{\lambda}))\Big{]}^{k-1}\Bigg{)}}_{\bm{B}_{2}}.\]

We bound the spectral norm of \(\bm{B}_{1}\) and \(\bm{B}_{2}\), respectively.

\[\|\bm{B}_{1}\| =\left\|\Big{(}\nabla_{\bm{\theta}\bm{\lambda}}^{3}\ell^{\mathrm{ tr}}+\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})\nabla_{\bm{\theta}\bm{ \lambda}\bm{\theta}}^{3}\ell^{\mathrm{tr}})\Big{)}\eta\sum_{k=0}^{K-1}\Bigl{[} \bm{I}-\eta\nabla_{\bm{\theta}\bm{\theta}}^{2}\ell^{\mathrm{tr}}\Big{]}^{k}\right\|\] \[\leq\Big{(}Q+Q\big{\|}\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{ \lambda})\big{\|}\Big{)}\Bigg{(}\eta\sum_{k=0}^{K-1}(1+\eta\gamma^{\mathrm{tr}} )^{k}\Bigg{)}\] \[\leq\Big{(}Q+QL^{\bm{\theta}_{K}}\Big{)}\Bigg{(}\eta\sum_{k=0}^{ K-1}(1+\eta\gamma^{\mathrm{tr}})^{k}\Bigg{)}\] \[\lesssim(1+\eta\gamma^{\mathrm{tr}})^{2K}.\] (Lemma C.2)

In addition,

\[\|\bm{B}_{2}\| =\left\|\nabla_{\bm{\theta}\bm{\lambda}}^{2}\ell^{\mathrm{tr}} \eta\sum_{k=1}^{K-1}\biggl{(}-\eta\Big{(}\nabla_{\bm{\theta}\bm{\theta}\bm{ \lambda}}^{3}\ell^{\mathrm{tr}}+\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{ \lambda})\nabla_{\bm{\theta}\bm{\lambda}\bm{\theta}}^{3}\ell^{\mathrm{tr}}) \Big{)}\right)k\Big{[}\bm{I}-\eta\nabla_{\bm{\theta}\bm{\theta}}^{2}\ell^{ \mathrm{tr}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda}))\Big{]}^{k-1}\right\|\] \[=\left\|\nabla_{\bm{\theta}\bm{\lambda}}^{2}\ell^{\mathrm{tr}} \eta\biggl{(}-\eta\Big{(}\nabla_{\bm{\theta}\bm{\theta}\bm{\lambda}}^{3}\ell^{ \mathrm{tr}}+\nabla_{\bm{\lambda}}\bm{\theta}_{K}(\bm{\lambda})\nabla_{\bm{ \theta}\bm{\lambda}\bm{\theta}}^{3}\ell^{\mathrm{tr}})\Big{)}\right)\sum_{k=1}^{K -1}k\Big{[}\bm{I}-\eta\nabla_{\bm{\theta}\bm{\theta}}^{2}\ell^{\mathrm{tr}}(\bm{ \lambda},\bm{\theta}_{K}(\bm{\lambda}))\Big{]}^{k-1}\right\|\] \[\leq Q\eta^{2}\Big{(}Q+QL^{\bm{\theta}_{K}}\Big{)}\Bigg{(}K(1+\eta \gamma^{\mathrm{tr}})^{K}-\frac{(1+\eta\gamma^{\mathrm{tr}})^{K}-(1+\eta\gamma^{ \mathrm{tr}})}{\eta\gamma^{\mathrm{tr}}}-1\Bigg{)}\] \[\lesssim K(1+\eta\gamma^{\mathrm{tr}})^{2K}.\]

Denote \(\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{\lambda})\) to be \(L^{\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}}\)-Lipschitz continuous for all \(\bm{\lambda}\in\Lambda\), then we have

\[L^{\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}}=\sup_{\bm{\lambda}\in \Lambda}\Bigl{\|}\nabla_{\bm{\lambda}}\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_ {K}}(\bm{\lambda})\Bigr{\|}\leq\|\bm{B}_{1}\|+\|\bm{B}_{2}\|\lesssim K(1+\eta \gamma^{\mathrm{tr}})^{2K}.\]With the above result and Lemma C.3, we have that for all \(\bm{\lambda},\bm{\lambda}^{\prime}\in\Lambda\),

\[\left\|\nabla_{\bm{\lambda}}\mathcal{L}(\bm{\lambda};\bm{z})- \nabla_{\bm{\lambda}}\mathcal{L}(\bm{\lambda}^{\prime};\bm{z})\right\| \leq\left\|\nabla_{\bm{\lambda}}\ell^{\mathrm{val}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda});\bm{z})-\nabla_{\bm{\lambda}}\ell^{\mathrm{val}} (\bm{\lambda}^{\prime},\bm{\theta}_{K}(\bm{\lambda}^{\prime});\bm{z})\right\|\] \[\quad+\left\|\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{ \lambda})\nabla_{\bm{\theta}}\ell^{\mathrm{val}}(\bm{\lambda},\bm{\theta}_{K}( \bm{\lambda});\bm{z})-\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{ \lambda}^{\prime})\nabla_{\bm{\theta}}\ell^{\mathrm{val}}(\bm{\lambda}^{ \prime},\bm{\theta}_{K}(\bm{\lambda}^{\prime});\bm{z})\right\|\] \[\leq\left\|\nabla_{\bm{\lambda}}\ell^{\mathrm{val}}(\bm{\lambda},\bm{\theta}_{K}(\bm{\lambda});\bm{z})-\nabla_{\bm{\lambda}}\ell^{\mathrm{val} }(\bm{\lambda}^{\prime},\bm{\theta}_{K}(\bm{\lambda});\bm{z})\right\|\] \[\quad+\left\|\nabla_{\bm{\lambda}}\ell^{\mathrm{val}}(\bm{\lambda }^{\prime},\bm{\theta}_{K}(\bm{\lambda});\bm{z})-\nabla_{\bm{\lambda}}\ell^{ \mathrm{val}}(\bm{\lambda}^{\prime},\bm{\theta}_{K}(\bm{\lambda}^{\prime});\bm{ z})\right\|\] \[\quad+\left\|\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{ \lambda})\nabla_{\bm{\theta}}\ell^{\mathrm{val}}(\bm{\lambda},\bm{\theta}_{K}( \bm{\lambda});\bm{z})-\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{ \lambda})\nabla_{\bm{\theta}}\ell^{\mathrm{val}}(\bm{\lambda}^{\prime},\bm{ \theta}_{K}(\bm{\lambda}^{\prime});\bm{z})\right\|\] \[\quad+\left\|\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{ \lambda})\nabla_{\bm{\theta}}\ell^{\mathrm{val}}(\bm{\lambda}^{\prime},\bm{ \theta}_{K}(\bm{\lambda}^{\prime});\bm{z})-\widehat{\nabla_{\bm{\lambda}}\bm{ \theta}_{K}}(\bm{\lambda}^{\prime})\nabla_{\bm{\theta}}\ell^{\mathrm{val}}( \bm{\lambda}^{\prime},\bm{\theta}_{K}(\bm{\lambda}^{\prime});\bm{z})\right\|\] \[\leq Q\|\bm{\lambda}-\bm{\lambda}^{\prime}\|+Q\|\bm{\theta}_{K}( \bm{\lambda})-\bm{\theta}_{K}(\bm{\lambda}^{\prime})\|\] \[\quad+Q\|\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{ \lambda})-\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}(\bm{\lambda}^{\prime})\|\] \[\leq Q\|\bm{\lambda}-\bm{\lambda}^{\prime}\|+QL^{\bm{\theta}_{K}} \|\bm{\lambda}-\bm{\lambda}^{\prime}\|\] \[\quad+L^{\theta_{K}}\Big{(}Q\|\bm{\lambda}-\bm{\lambda}^{\prime} \|+Q\|\bm{\theta}_{K}\|\bm{\lambda}-\bm{\lambda}^{\prime}\|\Big{)}\] \[\quad+QL^{\widehat{\nabla_{\bm{\lambda}}\bm{\theta}_{K}}}\|\bm{ \lambda}-\bm{\lambda}^{\prime}\|\] \[\lesssim K(1+\eta\gamma^{\mathrm{tr}})^{2K}\|\bm{\lambda}-\bm{ \lambda}^{\prime}\|.\]

which implies that \(\gamma\lesssim K(1+\eta\gamma^{\mathrm{tr}})^{2K}\). 

**Proposition C.5** (Expansion properties of IFT-based algorithms).: _Suppose we solve Example 5.3 by IFT-based Algorithm 1 with constant inner step size \(\eta\) where \(1-\eta\gamma_{d}\geq 0\) and outer step size \(\alpha_{t}\). Then (1) the outer update rules \(G_{\bm{z}_{i},\alpha_{t}}\) and \(G_{\bm{z}_{i},\alpha_{t}}\) are \(2\alpha_{t}L^{\prime}\)- divergent along \(\bm{v}_{1}\), and (2) the composite validation loss \(\mathcal{L}(\cdot,\bm{z})\) is \(\gamma^{\prime}\)-expansive along \(\bm{v}_{1}\) for all \(\bm{z}\in S^{\mathrm{val}}\), where_

\[L^{\prime}\gtrsim(1+\eta\gamma^{\mathrm{tr}})^{K},\gamma^{\prime}\gtrsim(1+\eta \gamma^{\mathrm{tr}})^{2K}.\]

Proof.: According to Lemma C.3, in the case of Example 5.3, the hypergradient calculated with the IFT-based algorithm is the same as the UD-based algorithm, which implies they achieve the same parameter divergence in this example. Therefore, we have the same result for \(L^{\prime}\) and \(\gamma^{\prime}\) as in Proposition 5.4 that \(L^{\prime}=\gtrsim(1+\eta\gamma^{\mathrm{tr}})^{K}\) and \(\gamma^{\prime}\gtrsim(1+\eta\gamma^{\mathrm{tr}})^{2K}\), which complete the proof. 

### Stability bounds of IFT-based HO algorithm

Combining the lower bound in Theorem 5.1 with the upper bound in Equation (6), we instantly have

\[\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{(}1+\alpha_{s}(1-1/m)\gamma^{\prime}\big{)} \frac{2\alpha_{t}L^{\prime}}{m}\leq\epsilon_{\mathrm{arg}}\leq\sum_{t=1}^{T} \prod_{s=t+1}^{T}\big{(}1+\alpha_{s}(1-1/m)\gamma\big{)}\frac{2\alpha_{t}L}{m}.\] (13)

**Theorem C.6** (Uniform argument stability of IFT-based algorithm, proof in Appendix C).: _Solving Example 5.3 by IFT-based Algorithm 1 with constant inner step size \(\eta\) where \(1-\eta\gamma_{d}\geq 0\) and decreasing outer step sizes \(\alpha_{t}=c/t\) with \(c\) as a positive constant \(\frac{T^{\mathrm{in}\left(1+(1-\frac{1}{m})\gamma^{\prime}\right)}}{m}\lesssim \epsilon_{\mathrm{arg}}\lesssim\frac{T^{(1-\frac{1}{m})\gamma}}{m},\) where \(\gamma\lesssim K(1+\eta\gamma^{\mathrm{tr}})^{2K},\gamma^{\prime}\gtrsim(1+\eta \gamma^{\mathrm{tr}})^{2K}\) as in Proposition C.5._

The upper bound is not limited to Example 5.3, but holds in more general case with the same mild assumption in [17] (see Assumption B.1). Notably, in contrast to the outcomes observed with the UD-based algorithm, the upper bound incorporates an extra factor of \(K\), leading to a larger upper bound for the IFT-based algorithm and a misalignment between the lower and upper bounds.

We can further establish a similar guarantee for the uniform stability \(\epsilon_{\mathrm{stab}}\) detailed in Theorem C.7.

Proof.: Based on properties in Propositions C.4 and C.5 the same proof as Theorem 5.5, we get the result. 

**Theorem C.7** (Uniform stability of IFT-based algorithm).: _Following the same condition as in Theorem C.6, and additionally, if the initial points \(\bm{\theta}_{0}=\bm{0},\bm{\lambda}_{0}=\bm{0}\), and \(\bm{v}_{1}\perp\bm{x}_{j}^{\mathrm{val}}\) for any \(j\in[m]\backslash i\) and \(\bm{v}_{1}\perp\bm{x}_{j}^{\mathrm{tr}}\) for any \(j\in[n]\), then the order of uniform stability \(\epsilon_{\mathrm{stab}}\) w.r.t. \(T\) satisfies \(\frac{T^{\ln\left(1+(1-\frac{1}{m})\circ\gamma^{\prime}\right)}}{m}\lesssim \epsilon_{\mathrm{stab}}\lesssim\frac{T^{(1-\frac{1}{m})\circ\gamma}}{m},\) where \(\gamma\) and \(\gamma^{\prime}\) are the same as in Proposition C.5._

Proof.: With the same proof as Theorem 5.6, we can get the result. 

## Appendix D Deferred results of UD-based algorithm on (strongly) convex inner loss

Recalling that in Example 5.3, \(\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta};\bm{z})=\frac{1}{2}\bm{\theta}^{ \top}\bm{A}\bm{\theta}+\bm{\lambda}^{\top}\bm{\theta}-y\bm{x}^{\top}\bm{\theta}\), where the smallest eigenvalue of \(\bm{A}\) is \(\gamma_{1}\). Therefore, when \(\gamma_{1}\geq 0\) (\(\gamma_{1}>0\)), \(\ell^{\mathrm{tr}}(\bm{\lambda},\bm{\theta};\bm{z})\) is convex (strongly convex) w.r.t. \(\bm{\theta}\) for all \(\bm{z}\in\mathscr{Z}\). Utilizing the case for \(\gamma_{1}\geq 0\) (\(\gamma_{1}>0\)) in Proposition B.5 and the same proof as in Theorem B.6 and Theorem 5.6, we can get the stability results for the convex (strongly convex) case in this section.

**Theorem D.1** (Uniform argument stability of UD-based algorithms for (strongly) convex \(\ell^{\mathrm{tr}}\)).: _Solving Example 5.3 by UD-based Algorithm 1 with constant inner step size \(\eta\) where \(1-\eta\gamma_{d}\geq 0\) and decreasing outer step sizes \(\alpha_{t}=c/t\) with \(c\) as a positive constant has uniform argument stability that_

\[\frac{T^{\ln\left(1+(1-\frac{1}{m})\circ\gamma^{\prime}\right)}}{m}\lesssim \epsilon_{\mathrm{arg}}\lesssim\frac{T^{(1-\frac{1}{m})\circ\gamma}}{m},\]

_where \(\gamma=\gamma^{\prime}\asymp K\) when \(\gamma_{1}=0\) and \(\gamma=\gamma^{\prime}\asymp 1\) when \(\gamma_{1}>0\) as in Proposition B.5._

Proof.: Please refer to the proof of Proposition B.5 and Theorem B.6 where we generalize the results in the main paper for the (strongly) convex case. 

**Theorem D.2** (Uniform stability of UD-based algorithms for (strongly) convex \(\ell^{\mathrm{tr}}\)).: _Following the same condition as in Theorem D.1, and additionally, if the initial points \(\bm{\theta}_{0}=\bm{0},\bm{\lambda}_{0}=\bm{0}\), and \(\bm{v}_{1}\perp\bm{x}_{j}^{\mathrm{val}}\) for any \(j\in[m]\backslash i\) and \(\bm{v}_{1}\perp\bm{x}_{j}^{\mathrm{tr}}\) for any \(j\in[n]\), then Algorithm 1 has uniform stability that_

\[\frac{T^{\ln\left(1+(1-\frac{1}{m})\circ\gamma^{\prime}\right)}}{m}\lesssim \epsilon_{\mathrm{stab}}\lesssim\frac{T^{(1-\frac{1}{m})\circ\gamma}}{m},\]

_where \(\gamma=\gamma^{\prime}\asymp K\) when \(\gamma_{1}=0\) and \(\gamma=\gamma^{\prime}\asymp 1\) when \(\gamma_{1}>0\) as in Proposition B.5._

Proof.: Please refer to the proof of Theorem 5.6. 

## Appendix E Deferred results of single-level SGD

As discussed in Section 4, deriving a stability lower bound entails constructing an example with maximum instability, and we need to study two aspects of the constructed example: (1) properties of the (compound) loss, and (2) stability behavior of the outer SGD corresponding to these properties. For (2), the outer level of gradient-based bilevel HO algorithms and the single-level SGD have equivalent formulation observing corresponding relations between \(\bm{\lambda}\leftrightarrow\bm{w}\), \(\mathcal{L}\leftrightarrow\ell\), \(S^{\mathrm{val}}\leftrightarrow S\) and stability definitions Definition 3.1\(\leftrightarrow\) Eq. (15). As a result, given the smoothness constants \(\gamma\) for \(\mathcal{L}\) and \(\ell\), the stability upper bounds under the bounded loss condition for the bilevel (\(\epsilon_{\mathrm{stab}}\lesssim T^{\frac{(1-1/m)\circ\gamma}{(1-1/m)\circ\epsilon +1}}/m\) in [17]) and single-level (\(\epsilon_{\mathrm{stab}}\lesssim T^{\frac{2\epsilon}{\gamma\circ\epsilon+1}}/m\) in [19]) algorithms have similar results. Given those properties, their stability lower bounds can be analyzed in a general framework: construct a well-designed example, examine its key properties, and derive the stability lower bound in response to these properties.

Our proposed lower-bounded expansion properties in Section 4 and provable stability lower bound given these properties in Theorem 5.1 are generally applicable for both bilevel and single-levelanalysis. Building upon these tools, we also establish stability lower bounds for single-level SGD in addition to our main results regarding bilevel algorithms. Notably, while the technique of stability analysis for the outer level of bilevel problems can be adapted to single-level ones, **the stability behavior of bilevel and single-level problems are not directly comparable**.

In this Section, we introduce basic concepts corresponding to stability analysis of single-level SGD in Appendix E.1 introduced by [19]. Based on this, Appendix E.2 leverages the lower-bounded expansion properties established in Section 4 to provide a stability lower bound for single-level SGD, which is tighter than the existing result in (34, Theorem 4). An upper bound is established in Appendix E.3 for a fair comparison between the lower and upper bounds without the bounded loss condition. Detailed comparison with existing works is provided in Table 2.

```
1:Input: Initialization \(\bm{w}_{0}\); dataset \(S\); step size scheme \(\alpha\)
2:Output: The parameter \(\bm{w}_{T}\)
3:for\(t=1\)to\(T\)do
4: uniformly sampling \(i_{t}\) from \([m]\)
5:\(\bm{g}\leftarrow\nabla\ell(\bm{w}_{t-1};\bm{z}_{i_{t}})\)
6:\(\bm{w}_{t}\leftarrow\bm{w}_{t-1}-\alpha_{t}\bm{g}\)
7:endfor
8:return\(\bm{w}_{T}\) ```

**Algorithm 2** Single-level SGD

### Problem formulation for the stability analysis of single-level SGD

Suppose we are interested in the distribution \(\mathcal{D}\) on data space \(\mathcal{Z}\), from which we obtain a sample \(S=\{\bm{z}_{i}\}_{i=1}^{m}\stackrel{{\mathrm{i.i.d.}}}{{\sim}} \mathcal{D}^{m}\). Suppose \(\bm{w}\) is the parameter to optimize in space \(\Omega\), and its loss on an example \(\bm{z}\) is \(\ell(\bm{w};\bm{z})\). The single-level SGD is shown in Algorithm 2. Following [19], the _generalization error_ of single-level SGD is defined as

\[\epsilon_{\mathrm{gen}}\coloneqq\mathbb{E}_{\mathcal{A},S}\left[\mathbb{E}_{ \bm{z}\sim\mathcal{D}}[\ell(\mathcal{A}(S);\bm{z})]-\frac{1}{m}\sum_{i=1}^{m} \ell(\mathcal{A}(S);\bm{z}_{i})\right],\] (14)

and we say a single-level stochastic algorithm \(\mathcal{A}\) is _\(\epsilon_{\mathrm{arg}}\)-uniformly argument stable_ if,

\[\epsilon_{\mathrm{arg}}=\sup_{S\simeq\mathcal{S}\in\mathcal{Z}^{m}}\mathbb{E}_ {\mathcal{A}}[\|\mathcal{A}(S)-\mathcal{A}(\tilde{S})\|].\] (15)

Based on these definitions, [19] has shown that stability guarantees generalization in single-level problems: if a stochastic algorithm \(\mathcal{A}\) is \(\epsilon_{\mathrm{arg}}\)-uniformly argument stable and the loss function \(\ell(\bm{w};\bm{z})\) is \(L\)-Lipschitz on \(\Omega\) for all \(\bm{z}\in\mathcal{Z}\), then we have

\[\epsilon_{\mathrm{gen}}\leq\epsilon_{\mathrm{stab}}\leq L\epsilon_{\mathrm{arg}}.\] (16)

### Proof of uniform stability lower bound

We first present a single-level example following [34].

**Example E.1**.: Suppose \(\Omega=\{\bm{w}:\|\bm{w}\|\leq W\}\) where \(W>0\), and \(\mathcal{Z}=\mathcal{X}\times\mathcal{Y}\) where \(\mathcal{X}=\{\bm{x}:\|\bm{x}\|\leq 1\}\) and \(\mathcal{Y}=[-1,1]\). Assume the loss function is \(\ell(\bm{w};\bm{z})=\frac{1}{2}\bm{w}^{\top}\bm{A}\bm{w}-y\bm{x}^{\top}\bm{w}\), where \(\bm{A}\in\mathbb{R}^{d\times d}\) is a symmetric matrix. Denote the eigenvalues of \(\bm{A}\) as \(\gamma_{1}\leq\cdots\leq\gamma_{d}\), where \(\gamma_{1}<0\) and \(|\gamma_{1}|\geq|\gamma_{d}|\), and \(\bm{v}_{1}\) as a unit eigenvector of \(\bm{A}\) for \(\gamma_{1}\). Additionally, suppose the twin datasets \(S\) and \(\tilde{S}\) are different at the \(i\)-th entry, where \(\bm{z}_{i}=(\bm{x}_{i},y_{i})=(\bm{v}_{1},1),\tilde{\bm{z}}_{i}=(\tilde{\bm{x} }_{i},\tilde{y}_{i})=(-\bm{v}_{1},1)\).

**Proposition E.2** (Lipschitz continuity and smoothness coefficients).: _In Example E.1, the loss function \(\ell(\bm{w};\bm{z})\) is \(L\)-Lipschitz continuous and \(\gamma\)-smooth on \(\Omega\) for all \(\bm{z}\in\mathcal{Z}\), where \(L\leq|\gamma_{1}|W+1\) and \(\gamma=|\gamma_{1}|\)._

Proof.: As \(\ell\) is twice differentiable on \(\Omega\times\mathcal{Z}\), we have

\[L =\sup_{z\in\mathcal{Z}}\sup_{\bm{w}\in\Omega}\|\nabla\ell(\bm{w}; \bm{z})\|=\sup_{z\in\mathcal{Z}}\sup_{\bm{w}\in\Omega}\|\bm{A}\bm{w}-y\bm{x}\| \leq|\gamma_{1}|W+1,\] \[\gamma =\sup_{\bm{z}\in\mathcal{Z}}\sup_{\bm{w}\in\Omega}\|\nabla^{2} \ell(\bm{w};\bm{z})\|=\sup_{\bm{z}\in\mathcal{Z}}\sup_{\bm{w}\in\Omega}\|\bm{A} \|=|\gamma_{1}|.\]

**Proposition E.3** (Divergent and expansive coefficients).: _Suppose we solve Example E.1 by single-level SGD, then the gradient update rules \(G_{\bm{z}_{i},\alpha_{t}}\) and \(G_{\tilde{\bm{z}}_{i},\alpha_{t}}\) are \(2\alpha_{t}L^{\prime}\)-divergent along \(\bm{v}_{1}\) and \(\ell(\bm{w};\bm{z})\) is \(\gamma^{\prime}\)-expansive along \(\bm{v}_{1}\) on \(\Omega\) for all \(\bm{z}\in S\), where \(L^{\prime}=1\) and \(\gamma^{\prime}=|\gamma_{1}|\)._

Proof.: For all \(\bm{w}\in\Omega\),

\[G_{\bm{z}_{i},\alpha_{t}}(\bm{w})-G_{\tilde{\bm{z}}_{i},\alpha_{t}}(\bm{w})=- \alpha_{t}\big{(}\nabla\ell(\bm{w};\bm{z}_{i})-\nabla\ell(\bm{w};\tilde{\bm{z} }_{i})\big{)}=-\alpha_{t}(y_{i}\bm{x}_{i}-\tilde{y}_{i}\tilde{\bm{x}}_{i})=2 \alpha_{t}\bm{v}_{1}\stackrel{{\mathrm{.}}}{{=}}\bm{v}_{1}.\]

Recalling Definition 4.1, this implies \(G_{\bm{z}_{i},\alpha_{t}}\) and \(G_{\tilde{\bm{z}}_{i},\alpha_{t}}\) are \(2\alpha_{t}L^{\prime}\)-divergent where \(L^{\prime}=1\). Additionally, for all \(\bm{w},\bm{w}^{\prime}\in\Omega\) such that \(\bm{w}-\bm{w}^{\prime}\) collinear with \(\bm{v}_{1}\) and any \(\bm{z}\in\mathcal{Z}\), we have

\[\nabla\ell(\bm{w};\bm{z})-\nabla\ell(\bm{w}^{\prime};\bm{z})=\bm{A}(\bm{w}-\bm {w}^{\prime})=\gamma_{1}(\bm{w}-\bm{w}^{\prime})=-|\gamma_{1}|(\bm{w}-\bm{w}^{ \prime}).\]

Recalling Definition 4.3, this implies \(\ell(\bm{w};\bm{z})\) is \(\gamma^{\prime}\)-expansive on \(\Omega\) for all \(\bm{z}\in\mathcal{Z}\) where \(\gamma^{\prime}=|\gamma_{1}|\). 

Given Proposition E.3, we can directly leverage Theorem 5.1 to obtain a stability lower bound.

**Theorem E.4** (Lower bound of single-level SGD in recursion form).: _In th case of Example E.1, running SGD for \(T\) steps on a \(\gamma\)-smooth loss function has uniform argument stability with_

\[\epsilon_{\mathrm{arg}}\geq\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{(}1+\alpha_{s} (1-1/m)\gamma\big{)}\frac{2\alpha_{t}}{m}.\]

Proof.: Using Theorem 5.1 and Proposition E.3, we gets the result. 

Following the proof of Theorem 5.5 we can deform the result in Theorem E.4 to display an explicit order under decreasing step sizes.

**Theorem E.5** (Lower bound of single-level SGD in deformed form).: _In th case of Example E.1, running SGD for \(T\) steps on a \(\gamma\)-smooth loss function with step sizes \(\alpha_{t}=c/t\) has uniform argument stability with_

\[\epsilon_{\mathrm{arg}}\geq\frac{2c}{m\mathrm{ln}\big{(}1+(1-1/m)c\gamma\big{)} }\Bigg{[}\bigg{(}\frac{T+1}{2}\bigg{)}^{\mathrm{ln}\big{(}1+(1-1/m)c\gamma \big{)}}-1\Bigg{]}.\]

_Omitting constant factors that depends on \(c\) and \(\gamma\), we have \(\epsilon_{\mathrm{arg}}\gtrsim\frac{T^{\mathrm{ln}\big{(}1+(1-1/m)c\gamma \big{)}}}{m}\)._

Proof.: The proof follows the scaling for the lower bound in Theorem 5.5. 

_Remark_.: Compared with Theorem 4 in [34] our result relaxes the condition and improves its order w.r.t. \(m\). To see this, we first show that the step-size settings are equivalent. In particular, \(\alpha_{t}=\frac{a}{0.99\gamma t}\) (Lemma 3, [34]) is equivalent to \(\alpha_{t}=\frac{c}{t}\) (Theorem E.5, ours) with \(c=\frac{a}{0.99\gamma}\). Based on this equivalence, we can rewrite the lower bound in [34] as \(\epsilon_{\mathrm{arg}}\gtrsim\frac{T^{0.99c\gamma}}{m1+0.99c\gamma}\) with assumptions \(c=1\), \(0<\gamma<\frac{0.1}{0.99}\) and \(T>m\) (detailed in the proof of Theorem 4, [34]). In contrast, our lower bound of \(\epsilon_{\mathrm{arg}}\gtrsim\frac{T^{\mathrm{ln}\big{(}1+(1-1/m)c\gamma \big{)}}}{m}\) in Theorem E.5 holds for any \(c>0\), \(\gamma>0\), \(T\geq 1\), relaxing the conditions. Regarding the tightness of the lower bound, our result is sharper concerning \(m\) given \(\lim_{m\to\infty}\frac{T^{0.99c\gamma}}{\frac{m+0.99c\gamma}{m}}=0\) for fixed \(T\). In addition, concerning \(T\), our result is comparable observing that the ratio of the powers on \(T\) differ slightly, namely \(0.96\leq\frac{0.99c\gamma}{\mathrm{ln}\big{(}1+(1-1/m)c\gamma\big{)}}\leq 1.06\) for all \(m\geq 100\), under the scope of application of their result (i.e., \(c=1\) and \(0<\gamma<\frac{0.1}{0.99}\)). The superiority of our lower bound stems from a loose result in Lemma 3 in [34]. Denote \(\Delta_{t}\coloneqq\bm{w}_{t}-\tilde{\bm{w}}_{t}\). It states that \(\mathbb{E}_{\mathcal{A}}[\|\Delta_{T}\|\Delta_{t_{0}}\neq 0]\geq\frac{1}{2n}(\frac{T}{t_{0}})^{0.99c\gamma}\), while this can be improved into \(\mathbb{E}_{\mathcal{A}}[\|\Delta_{T}\|\Delta_{t_{0}}\neq 0]\geq\frac{1}{2m}(\frac{T}{t_{0}})^{0.99c \gamma}+(\frac{T+1}{t_{0}+1})^{0.99c\gamma}\Delta_{t_{0}}\), which will lead to a sharper lower bound.

### Proof of uniform stability upper bound

Denote \(\delta_{t}\coloneqq\|\bm{w}_{t}-\tilde{\bm{w}}_{t}\|\). The proof leverages an intermediate result of Theorem 3.12 in [19].

**Lemma E.6** ([19], Theorem 3.12).: _Assume \(\ell(\cdot;\bm{z})\) is \(L\)-Lipschitz and \(\gamma\)-smooth for all \(\bm{z}\in\mathscr{Z}\). Running SGD with step sizes \(\alpha_{t}\), for all \(S\simeq\tilde{S}\in\mathscr{Z}^{m}\), we have the recurrence relation: \(\forall 1\leq t\leq T,\)_

\[\mathbb{E}_{\mathcal{A}}[\delta_{t}]\leq\bigg{(}1-\frac{1}{m}\bigg{)}(1+ \alpha_{t}\gamma)\mathbb{E}_{\mathcal{A}}[\delta_{t-1}]+\frac{1}{m}\big{(} \mathbb{E}_{\mathcal{A}}[\delta_{t-1}]+2\alpha_{t}L\big{)}.\]

Unwinding the recursion we have the stability upper bound.

**Theorem E.7** (Upper bound of single-level SGD in recursion form).: _Assume \(\ell(\cdot;\bm{z})\) is \(L\)-Lipschitz and \(\gamma\)-smooth for all \(\bm{z}\in\mathscr{Z}\). In th case of Example E.1, running SGD with step sizes \(\alpha_{t}\) has uniform argument stability with_

\[\epsilon_{\mathrm{arg}}\leq\sum_{t=1}^{T}\prod_{s=t+1}^{T+1}\big{(}1+(1-1/m) \alpha_{s}\gamma\big{)}\frac{2\alpha_{t}(\gamma W+1)}{m}.\]

Proof.: As defined, \(\epsilon_{\mathrm{arg}}=\sup_{S\simeq\tilde{S}\in\mathscr{Z}^{m}}\mathbb{E}_{ \mathcal{A}}[\delta_{T}]\) and. Unwinding the recursion in Lemma E.6 and using the fact that \(L\leq\gamma W+1\) in Proposition E.2, we get the result. 

Here we set an additional \(\alpha_{T+1}=0\) for the expression neatness. Recalling Theorem E.4, the upper and lower bound are in exactly the same formulation with only difference in by a constant (i.e., \(\gamma W+1\)), which means the lower and upper bound tightly match w.r.t. the key factors \(T\) and \(m\).

Considering the case of constant step size, we get \(\epsilon_{\mathrm{arg}}\asymp\frac{\big{(}1+\big{(}1-1/m\big{)}\alpha\gamma \big{)}^{T}}{m}\), showing an exploding rate w.r.t. \(T\). When adopting linearly decreasing step sizes \(\alpha_{t}\leq\frac{1}{c/t}\), the upper bound can also be deformed to reveal an explicit order w.r.t. key factors.

**Theorem E.8** (Upper bound of single-level SGD in defomed form).: _Assume \(\ell(\cdot;\bm{z})\) is \(L\)-Lipschitz and \(\gamma\)-smooth for all \(\bm{z}\in\mathscr{Z}\). Running SGD for \(T\) steps with step sizes \(\alpha_{t}\leq c/t\) has uniform argument stability of_

\[\epsilon_{\mathrm{arg}}\leq\frac{2(\gamma W+1)}{(m-1)\gamma}\bigg{[}\Big{(}1+ \big{(}1-1/m\big{)}\gamma c\Big{)}T^{(1-1/m)c\gamma}-1\bigg{]}.\]

_Omitting constant factors that depends on c, \(\gamma\) and \(W\), we have \(\epsilon_{\mathrm{arg}}\lesssim\frac{T^{(1-1/m)c\gamma}}{m}\)._

Proof.: The proof follows the scaling for the upper bound in Theorem 5.5. 

Combining Theorem E.8 and Theorem E.5, we have \(\frac{T^{\ln(1+(1-1/m)c\gamma)}}{m}\lesssim\epsilon_{\mathrm{arg}}\lesssim \frac{T^{(1-1/m)c\gamma}}{m}\). Notably, the discrepancy between the lower and upper bounds is unavoidable, stemming from the scaling steps required to obtain an explicit order, and this gap becomes small when we have large \(m\) and small \(c\gamma\). Concerning the lower and upper bounds in recursion form in Theorem E.5 and Theorem E.7, our results are tightly matched.

_Remark_.: Notice that Theorem 3.8 in [19] presents an upper bound of \(\epsilon_{\mathrm{arg}}\lesssim\frac{T^{\frac{-\gamma c}{1+1}}}{m}\), which is tighter than our result of \(\epsilon_{\mathrm{arg}}\lesssim\frac{T^{(1-1/m)\gamma c}}{m}\) but with additional bounded loss assumption that \(\ell(\bm{w};\bm{z})\in[0,1]\). Both results are based on the recurrence relation in Lemma E.6. They derive the upper bound with a hitting time \(t_{0}\) and bound the loss divergence after \(t_{0}\) with the bounding loss constant (i.e., \(1\)) and thus get a tighter upper bound. However, to derive lower bounds, we need to explicitly calculate the divergence between parameters and corresponding loss values, which will inevitably reveal all the terms in the recursion. In this case, the bounded loss assumption is not applicable and thus we present an upper bound without this condition as a fair and clear comparison with the lower bound.

We acknowledge that the bounded loss assumption is commonly adopted for upper-bound analysis in theoretical works. Despite [19], several following works also adopt this technique. [34] derive the upper bound of \(\epsilon_{\mathrm{arg}}\lesssim\frac{T^{\gamma c}}{m^{\gamma c+1}}\) in the nonconvex case with a similar approach by bounding the loss after hitting time \(t_{0}\), with an additional setting for \(t_{0}=n\). However, there appears to be a misuse of Lemma 4 in their proof of Theorem 5, which leads to their result being tighter compared to [19] inthe case of \(T^{\frac{1}{c+1}}\leq m\). Specifically, in the proof of Theorem 5, they decompose \(\mathbb{E}_{\mathcal{A}}[\|\Delta_{T}]\) into two terms that \(\mathbb{E}_{\mathcal{A}}[\|\Delta_{T}\|\Delta_{n}=0]\mathbb{P}[\Delta_{n}=0]+ \mathbb{E}_{\mathcal{A}}[\|\Delta_{T}\|\Delta_{n}\neq 0]\mathbb{P}[\Delta_{n}\neq 0]\) to bound these two terms separately. For the second term, the union bound is used to get \(\mathbb{E}_{\mathcal{A}}[\|\Delta_{T}\|\Delta_{n}\neq 0]\mathbb{P}[\Delta_{n} \neq 0]\leq\frac{1}{n}\sum_{t=1}^{n}\mathbb{E}_{\mathcal{A}}[\|\Delta_{T} \|H=t]\), where \(H=t\) denotes that \(t\) is the first time SGD pick the different entry in the twin datasets. \(\mathbb{E}_{\mathcal{A}}[\|\Delta_{T}\|H=t]\) is further bounded using Lemma 4 that \(\mathbb{E}_{\mathcal{A}}[\|\Delta_{T}\|\Delta_{t}=0]\leq(\frac{T}{t})^{a} \frac{2L}{n}\) to get \(\mathbb{E}_{\mathcal{A}}[\|\Delta_{T}\|H=t]\leq(\frac{T}{t})^{a}\frac{2L}{n}\). While this appears to be a misuse as \(H=t\) can only imply \(\Delta_{t-1}=0\) and whether \(\Delta_{t}=0\) remains uncertain, where Lemma 4 is not applicable. Another work [49] use a large constant to bound the loss divergence from the start of the evolution of the parameter divergence, which leads to an upper bound of \(\epsilon_{\mathrm{arg}}\lesssim\frac{T}{m}\) even with constant step sizes. A detailed comparison of existing results is listed in Table 2.

## Appendix F Details of simulations

The implementing code is provided in the supplementary material. All simulations can be conducted on the CPU of a laptop.

### Hyperparameter distance and stability bounds

To examine the tightness and validity of the upper and lower bounds presented in Theorem 5.5, we implement UD-based Algorithm 1 on Example 5.3 with linearly decreasing step sizes and compare the practical output hyperparameter distances with our theoretical bounds under a range of outer iterations \(T\).

Specifically, we set the loss functions and the twin validation sets as in Example 5.3 with \(\bm{A}=\begin{bmatrix}-1&0\\ 0&1\end{bmatrix}\) and \(\bm{v}_{1}=\begin{bmatrix}1\\ 0\end{bmatrix}\). The optimization is implemented with fixed \(\gamma^{\mathrm{tr}}=1\), \(K=100\), \(m=100\), \(n=100\), \(n=100\), \(\eta=0.01\), and \(c=0.01\).

The comparison is shown in Fig. 2. We plot the output hyperparameter distances with increasing \(T\) from \(1000\) to \(5000\) on the horizontal axis and the deformed lower bounds and upper bounds with corresponding \(T\) on the vertical axis. The dashed lines are linear fittings of the hyperparameter distances and the upper/lower bounds, to examine the linear trends of their relative magnitude.

### Recursive stability bounds and deformed stability bounds

Here we implement additional simulations to examine the tightness between the recursive upper/lower bounds and deformed upper/lower bounds presented in Eq. (7) and Appendix B.6. Specifically, the recursive upper bound is calculated by \(\sum_{t=1}^{T}\prod_{s=t+1}^{T}\bigl{(}1+\alpha_{s}(1-1/m)\gamma\bigr{)}2 \alpha_{t}L^{\prime}/m\) and the recursive lower bound is calculated by \(\sum_{t=1}^{T}\prod_{s=t+1}^{T}\bigl{(}1+\alpha_{s}(1-1/m)\gamma\bigr{)}2 \alpha_{t}L/m\) in Eq. (7). The deformed upper bound is calculated by \(T^{(1-\frac{1}{m})c\gamma}/m\) and the deformed lower bound is calculated by \(T^{\ln\bigl{(}1+(1-\frac{1}{m})c\gamma^{\prime}\bigr{)}}/m\). As for the coefficients: we set \(\gamma^{\mathrm{tr}}=1\) in Example 5.3. \(L^{\prime}\) is calculated with \(\bigl{[}(1+\eta\gamma^{\mathrm{tr}})^{K}-1\bigr{]}/\gamma^{\mathrm{tr}}\) as in Eq. (10) and \(L\) is calculated with \(L=0.1+1.1L^{\prime}\) as they are of the same

\begin{table}
\begin{tabular}{c c c c c} \multirow{2}{*}{Settings} & Step size & Constant \(\alpha_{t}=\alpha\) & \multicolumn{2}{c}{Decreasing \(\alpha_{t}\leq c/t\)} \\ \cline{2-5}  & Range of iterations & \(1\leq t\leq T\) & \(t_{0}\leq t\leq T\) & - \\ \cline{2-5}  & Upper bound & \(\frac{T}{m}\)[49] & \(\frac{T^{\frac{\gamma c}{\gamma c+1}}}{m}\)[19] & \(\frac{T^{(1-1/m)\gamma c}}{m}\) (Ours) \\ \cline{2-5}  & Lower bound & - & - & \(\frac{T^{\ln(1+(1-1/m)\gamma)}}{m^{1+0.99c\gamma}}\) (Ours) \\ \hline \end{tabular}
\end{table}
Table 2: A detailed comparison of existing results on uniform stability of single-level SGD. We unify the notations that the loss function is \(\gamma\)-smooth, the dataset is of size \(m\) and SGD picks the different entry for the first time at \(t_{0}\).

order of magnitude. \(\gamma=\gamma^{\prime}\) is calculated with \(\eta\sum_{k=0}^{K-1}(1+\eta\gamma^{\mathrm{tr}})^{k}\Big{(}2+\eta\gamma^{ \mathrm{tr}}\sum_{k=0}^{K-1}(1+\eta\gamma^{\mathrm{tr}})^{k}\Big{)}\) as in Eq. (11).

During the optimization, we fix \(\eta=0.01\), \(n=100\) and \(c=0.01\). For the simulation regarding \(T\), we set \(K=100\) and \(m=100\) and plot the results of the recursive upper bounds for \(T\) from \(1000\) to \(5000\) in the horizontal axis with the corresponding other three bounds in the vertical axis, shown in Fig.5. For the simulation regarding \(K\), we set \(T=1000\) and \(m=100\) and plot the results of the recursive upper bounds for \(K\) from \(25\) to \(200\) in the horizontal axis with the corresponding other three bounds in the vertical axis, shown in Fig.5. For the simulation regarding \(m\), we set \(T=1000\) and \(K=100\) and plot the results of the recursive upper bounds for \(m\) from \(100\) to \(2000\) in the horizontal axis with the corresponding other three bounds in the vertical axis, shown in Fig.5.

All curves exhibit linear trends, indicating these bounds are in the same order w.r.t. \(T\), \(K\), and \(m\).

## Appendix G Additional discussions

### Additional discussion for the definition of uniform stability on validation

In HO, the model is typically evaluated during the validation phase and is expected to generalize well on unseen test data based on its validation performance. Therefore, we focus on the impact of perturbations in the validation set in our current definition of generalization error and uniform argument stability. However, in the context of meta-learning where both datasets play crucial roles [50], considering the perturbations in the training set may provide additional insights for generalization analysis, which might be an interesting topic for future work.

### Additional discussion for expansiveness and existing concepts

We first clarify that the convex loss function corresponds to Definition 4.3 for the case when \(\mu\leq 0\). When the loss function is convex, we have \(\langle\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime}),\bm{w}-\bm{w}^{\prime} \rangle\geq 0\). According to Definition 4.3, if the loss is additionally \(\mu\)-expansive, there exist \(\mu_{\bm{w},\bm{w}^{\prime}}\geq\mu\) such that

\[\langle\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime}),\bm{w}-\bm{w }^{\prime}\rangle =\langle-\mu_{\bm{w},\bm{w}^{\prime}}(\bm{w}-\bm{w}^{\prime}), \bm{w}-\bm{w}^{\prime}\rangle\] \[=-\mu_{\bm{w},\bm{w}^{\prime}}\|\bm{w}-\bm{w}^{\prime}\|^{2}\] \[\geq 0,\]

thus we have \(\mu\leq\mu_{\bm{w},\bm{w}^{\prime}}\leq 0\).

When \(\mu>0\), \(\mu\)-strongly concavity requires \(\langle\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime}),\bm{w}-\bm{w}^{\prime} \rangle\leq-\mu\|\bm{w}-\bm{w}^{\prime}\|^{2}\) for all \(\bm{w},\bm{w}^{\prime}\in\Omega\), and Definition 4.3 restricts that for all \(\bm{w},\bm{w}^{\prime}\in\Omega\) that \(\bm{w}-\bm{w}^{\prime}\) parallel with \(\bm{v}\), there exists \(\mu_{\bm{w},\bm{w}^{\prime}}\geq\mu\) such that

\[\langle\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime}),\bm{w}-\bm{ w}^{\prime}\rangle =\langle-\mu_{\bm{w},\bm{w}^{\prime}}(\bm{w}-\bm{w}^{\prime}), \bm{w}-\bm{w}^{\prime}\rangle\] \[=-\mu_{\bm{w},\bm{w}^{\prime}}\|\bm{w}-\bm{w}^{\prime}\|^{2}\] \[\leq-\mu\|\bm{w}-\bm{w}^{\prime}\|^{2}.\]

Therefore, \(\mu\)-expansiveness along \(\bm{v}\) implies concavity only for \(\bm{w}-\bm{w}^{\prime}\) parallel with \(\bm{v}\).

On the other hand, if we have \(\langle\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime}),\bm{w}-\bm{w}^{\prime} \rangle\leq-\mu\|\bm{w}-\bm{w}^{\prime}\|^{2}\) for \(\bm{w}-\bm{w}^{\prime}\) parallel with \(\bm{v}\), then

\[\langle\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime}),\bm{w}-\bm {w}^{\prime}\rangle \leq\|\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime})\|\|\bm{w}- \bm{w}^{\prime}\|\] \[\leq-\mu\|\bm{w}-\bm{w}^{\prime}\|^{2},\]

and thus \(\|\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime})\|\leq-\mu\|\bm{w}-\bm{w}^{ \prime}\|\). Therefore, compared with strongly concavity on a single direction, \(\mu\)-expansiveness has an additional restriction for the colinearity of \(\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime})\) and \(-(\bm{w}-\bm{w}^{\prime})\).

Additionally, for the one-dimensional case, the condition \(w-w^{\prime}\) parallel with some scalar \(v\) is equivalent to \(\forall w,w^{\prime}\in\Omega\). Therefore, \(\mu\)-expansiveness along \(v\) implies \(\mu\)-strongly concavity. Conversely, \(\mu\)-strongly concavity implies that there exists a \(\mu_{\bm{w},\bm{w}^{\prime}}\geq\mu\) such that

\[(\nabla\ell(\bm{w})-\nabla\ell(\bm{w}^{\prime}))(\bm{w}-\bm{w}^{ \prime}) =-\mu_{\bm{w},\bm{w}^{\prime}}(\bm{w}-\bm{w}^{\prime})^{2}\] \[\leq-\mu(\bm{w}-\bm{w}^{\prime})^{2},\]

thus \(\mu\)-strongly concavity conversely also implies \(\mu\)-expansiveness along \(v\). Therefore, these concepts are equivalent in the one-dimensional case.

### Technical challenges of stability lower bound analysis for bilevel algorithms

The nested optimization in bilevel algorithms poses challenges to the stability analysis as the instability and simplicity of the constructed example are both crucial for deriving a tight lower bound. Specifically, to examine the alignment of the lower and upper bounds, we need to precisely calculate the smooth coefficient \(\gamma\) and expansive coefficient \(\mu\) of the compound validation loss for the constructed example. While the implicit and intricate formulation of \(\nabla\mathcal{L}(\bm{\lambda})\) in bilevel optimization makes \(\gamma\) and \(\mu\) difficult to obtain. In the following, we take ridge regression as an example to illustrate how the bilevel structure hinders the stability analysis.

**Example G.1** (Regularization coefficient in ridge regression).: The validation loss and training loss are given by \(\ell^{\mathrm{val}}(\lambda,\bm{\theta})=\frac{1}{2}(y-\bm{\theta}^{T}\bm{x} )^{2},\ \ell^{\mathrm{tr}}(\lambda,\bm{\theta})=\frac{1}{2}(y-\bm{\theta}^{T}\bm{x})^{2 }+\frac{\lambda}{2}\bm{\theta}^{\top}\bm{\theta}\). Solving it with UD-based Algorithm 1, we have the inner output as \(\bm{\theta}_{K}(\lambda)=\prod_{k=1}^{K}(\bm{I}-\eta\lambda\bm{I}-\eta\bm{x}_{j _{k}}\bm{x}_{j_{k}})^{\top}\bm{\theta}_{0}+\sum_{i=1}^{K}\prod_{l=k+1}^{K}( \bm{I}-\eta\lambda\bm{I}-\eta\bm{x}_{j_{k}}\bm{x}_{j_{l}})^{\top}\eta y_{j_{k} }\bm{x}_{j_{k}}\) and a far more complex inner Jacobian \(\nabla_{\lambda}\bm{\theta}_{K}(\lambda)\), resulting in a unmeasurable hypergradient \(\nabla\mathcal{L}(\lambda)=\nabla_{\lambda}\bm{\theta}_{K}(\lambda)(y-\bm{ \theta}_{K}(\lambda)^{\top}\bm{x})(-\bm{x})\).

These complexities obstacle us to precisely examine the divergence dynamics at each step. Therefore, we introduce expansion properties in Section 4 and the general lower bounded guarantees in Theorems 5.1 and 5.2 to jointly contribute to the careful construction of Example 5.3. As a result, Example 5.3 exhibits the maximum instability while having a relatively simple outer gradient update feasible for lower bound analysis, which will lead to tight stability lower bounds presented in Section 5.4.

Potential extension of our framework

### Extension on average stability lower bounds

Considering the similarities between average stability [32, Definition 2] and uniform stability, our techniques may be adapted to the data-dependent setting for average stability with some modifications. In this section, we present a preliminary proof sketch for establishing the stability lower bound of single-level SGD based on the variant Example E.1.

To account for the randomness in the sampled datasets, we first define some notations. Let \(S_{i}\) denote a copy of \(S\) with the \(i\)-th element replaced by \(\bm{z}_{i}^{\prime}\), \(G_{S_{i}}\)/\(G_{S_{i},t}\) and \(w_{S_{i}}\)/\(w_{S_{i},t}\) denote the SGD update rules and the updated parameters optimized on \(S\) and \(S_{i}\) at the step \(t\). With a slight adjustment Section 4.2 and a similar proof as in the current paper, Theorem 5.1 can be modified as: Suppose there exists a nonzero vector \(\bm{v}(S,S_{i})\) along which \(G_{S,t}\) and \(G_{S_{i},t}\) are \(2\alpha_{t}L^{\prime}(S,S_{i})\)-divergent and \(\mathcal{L}(\cdot;z)\) is \(\gamma_{t}^{\prime}(S,S_{i})\)-expansive for all \(\bm{w}_{S,t}-\bm{w}_{S,i}\) that parallel with \(\bm{v}(S,S_{i})\). Then we have

\[\mathbb{E}_{\mathcal{A}}[\delta_{t}(S,S_{i})]\geq[1+\alpha_{t}(1-1/m)]\gamma_{ t}^{\prime}(S,S_{i})\mathbb{E}_{\mathcal{A}}[\delta_{t-1}(S,S_{i})]+2\alpha_{t }L^{\prime}(S,S_{i})/m.\]

This recursion closely corresponds with the recursive upper bound in [32, Eq.(19)]:

\[\mathbb{E}_{\mathcal{A}}[\delta_{t}(S,S_{i})]\leq[1+\alpha_{t}(1-1/m)]\psi_{t} (S,S_{i})\mathbb{E}_{\mathcal{A}}[\delta_{t-1}(S,S_{i})]+2\alpha_{t}L(S,S_{i} )/m,\]

and the matching of \(\gamma_{t}^{\prime}\) & \(\psi_{t}\), \(L^{\prime}\) & \(L\) will further guide the design of the constructed example.

Therefore, our analysis framework can be adapted for the average stability with suitable modifications. Specifically for average stability, a core challenge is calculating the expectation over \(S\) and \(S_{i}\) for the above recursive formula, which is beyond the scope of our paper. In the following, we provide a proof sketch for establishing the average argument stability lower bound to clarify a possible approach to extend our framework.

Assume that the data follows a distribution \(\mathcal{D}\) that \(p(\bm{z})=\left\{\begin{array}{ll}0.5&\text{if }\bm{z}=(\bm{v}_{1},1),\\ 0.5&\text{if }\bm{z}=(-\bm{v}_{1},1).\end{array}\right.\)\(S\overset{\text{i.i.d.}}{\sim}\mathcal{D}^{m}\) and \(z_{i}^{\prime}\sim\mathcal{D}\) are independent of each other. The validation loss and training loss follow Example E.1. Under these assumptions, we derive that \(L^{\prime}(S,S_{i})=\|y_{i}\bm{x}_{i}-y_{i}^{\prime}\bm{x}_{i}^{\prime}\|/2\) and \(\mu_{t}(S,S_{i})=0\) for \(z_{i}=z_{i}^{\prime}\), \(\mu_{t}(S,S_{i})=|\gamma_{1}|\) for \(z_{i}\neq z_{i}^{\prime}\). This leads to the average argument stability lower bound:

\[\epsilon_{\mathrm{arg}}\geq\sum_{t=1}^{T}\prod_{s=t+1}^{T}(1+\alpha_{s}(1-1/m |\gamma_{1}|))\alpha_{t}/m.\]

### Extension on generalization lower bounds

In this section, we discuss a possible approach to extend our framework on the analysis of generalization lower bounds. We first present a lemma clarifying the fundamental equivalence between generalization and stability. Then a lower bound on the expected hyperparameter divergence is established by slightly modifying Example 5.3 with additional design on the data distribution, which will imply the generalization lower bound under certain conditions.

**Lemma H.1**.: _Let \(S^{\mathrm{val}}=(\bm{z}_{1}^{\mathrm{val}},\dots,\bm{z}_{m}^{\mathrm{val}})\) and \(S^{\mathrm{val}^{\prime}}=(\bm{z}_{1}^{\mathrm{val}^{\prime}},\dots,\bm{z}_{m} ^{\mathrm{val}^{\prime}})\) be two independent samples drawn i.i.d. from \(\mathcal{D}^{\mathrm{val}}\). Let \(\tilde{S}_{i}^{\mathrm{val}}=(\bm{z}_{1}^{\mathrm{val}},\dots,\bm{z}_{i}^{ \mathrm{val}^{\prime}},\dots,\bm{z}_{m}^{\mathrm{val}})\) denote the twin validation set of \(S^{\mathrm{val}}\) differing in the \(i\)-th example. Consider \(\epsilon_{\mathrm{gen}}\) as the generalization error defined in Eq. (4). Then, we have_

\[\epsilon_{\mathrm{gen}}=\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}_{\mathcal{A},S^{ \mathrm{tr}},S^{\mathrm{val}},\bm{z}_{i}^{\mathrm{val}^{\prime}}}[\mathcal{L}( \mathcal{A}(S^{\mathrm{val}},S^{\mathrm{tr}});\bm{z}_{i}^{\mathrm{val}^{ \prime}})-\mathcal{L}(\mathcal{A}(\tilde{S}_{i}^{\mathrm{val}},S^{\mathrm{tr}}) ;\bm{z}_{i}^{\mathrm{val}^{\prime}})].\]

Proof.: According to the definition of generalization error in Eq. (4) and the linearity of expectation,As it is assumed \(\mathcal{D}^{\rm val}=\mathcal{D}^{\rm test}\) and \({S^{\rm val}}^{\prime}\) is i.i.d. sampled from \(\mathcal{D}^{\rm val}\) independent from \(S^{\rm val}\), term (a) can be rewritten as

\[\text{(a)}=\mathbb{E}_{\mathcal{A},S^{\rm val},S^{\rm tr}}\left[\frac{1}{m}\sum_ {i=1}^{m}\mathbb{E}_{\bm{z}_{i}^{\rm val}}[\mathcal{L}(\mathcal{A}(S^{\rm val },S^{\rm tr});\bm{z}_{i}^{\rm val}{}^{\prime})]\right].\]

Under the expectation, \(S^{\rm val}\) and \(\tilde{S}_{i}^{\rm val}\) is exchangeable, then term (b) can be rewritten as

\[\text{(b)}\!=\!\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}_{\mathcal{A}, \tilde{S}_{i}^{\rm val},S^{\rm tr}}\left[\mathcal{L}(\mathcal{A}(\tilde{S}_{i }^{\rm val},S^{\rm tr});\bm{z}_{i}^{\rm val}{}^{\prime})\right]\!=\!\frac{1}{m }\sum_{i=1}^{m}\mathbb{E}_{\mathcal{A},S^{\rm val},\bm{z}_{i}^{\rm val}{}^{ \prime},S^{\rm tr}}\left[\mathcal{L}(\mathcal{A}(\tilde{S}_{i}^{\rm val},S^{ \rm tr});\bm{z}_{i}^{\rm val}{}^{\prime})\right].\]

Combining (a) and (b) leads to the equation in the theorem. 

Lemma H.1 shows a fundamental relation between generalization and stability: The generalization error equals the expected loss divergence when replacing a single example in the validation set. Stability-based generalization analysis typically takes the supremum on \(S^{\rm tr},S^{\rm val},\bm{z}_{i}^{\rm val}{}^{\prime}\) to obtain a distribution-agnostic upper bound of generalization error as

\[\epsilon_{\rm gen}\leq\epsilon_{\rm stab}:=\sup_{S^{\rm tr},S^{ \rm val},\bm{z}_{i}^{\rm val}{}^{\prime}}\mathbb{E}_{\mathcal{A}}[\mathcal{L }(\mathcal{A}(S^{\rm val},S^{\rm tr});\bm{z}_{i}^{\rm val}{}^{\prime})- \mathcal{L}(\mathcal{A}(\tilde{S}_{i}^{\rm val},S^{\rm tr});\bm{z}_{i}^{\rm val }{}^{\prime})],\]

where \(\epsilon_{\rm stab}\) is commonly upper bounded assuming \(L\)-Lipschitz of the loss as

\[\epsilon_{\rm stab}\leq L\epsilon_{\rm arg}:=L\sup_{S^{\rm tr},S^ {\rm val}\simeq\tilde{S}^{\rm val}}\mathbb{E}_{\mathcal{A}}[[]\mathcal{A}(S^{ \rm val},S^{\rm tr})-\mathcal{A}(\tilde{S}^{\rm val},S^{\rm tr})]\|.\]

This paper attempts to derive lower bounds of \(\epsilon_{\rm arg}\) which will not directly imply the generalization lower bounds because \(\epsilon_{\rm arg}\) is fundamentally a distribution-agnostic upper bound for the generalization error. In order to obtain a generalization lower bound, a promising way is to extend Example 5.3 with additional assumption on the validation distribution rather than directly specifying \(S^{\rm val}\) and \(\tilde{S}^{\rm val}\).

We present a primary result below for the lower bound of the expected hyperparameter divergence, which indicates a way to extend our methods on the analysis of generalization lower bounds.

**Example H.2**.: We introduce an HO problem as follows. Let the validation loss and the training loss be:

\[\ell^{\rm val}(\bm{\lambda},\bm{\theta};\bm{z})=\ell^{\rm tr}( \bm{\lambda},\bm{\theta};\bm{z})=\frac{1}{2}\bm{\theta}^{\top}\bm{A}\bm{ \theta}+\bm{\lambda}^{\top}\bm{\theta}-y\bm{x}^{\top}\bm{\theta},\]

where \(\bm{A}\in\mathbb{R}^{d\times d}\) is symmetric. Denote the eigenvalues of \(\bm{A}\) as \(\gamma_{1}\leq\cdots\leq\gamma_{d}\). Let \(\gamma_{1}<0\), \(\gamma_{d}\leq 0\), and \(\bm{v}_{1}\) be a unit eigenvector for \(\gamma_{1}\). Suppose the validation distribution follows:

\[p(\bm{z})=\left\{\begin{array}{ll}0.5&\text{if }\bm{z}=(\bm{v}_{1},1),\\ 0.5&\text{if }\bm{z}=(-\bm{v}_{1},1).\end{array}\right.\]

**Theorem H.3**.: _Suppose we solve Example H.2 by UD-based Algorithm 1 with constant inner step size \(\eta\) where \(1-\eta\gamma_{d}\geq 0\) and outer step size \(\alpha_{t}\). Denote that the expected hyperparameter divergence as \(\epsilon_{\rm gen,arg}\coloneqq\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}_{\mathcal{A}, S^{\rm tr},S^{\rm val},\bm{z}_{i}^{\rm val}{}^{\prime}}[\|\mathcal{A}(S^{\rm val },S^{\rm tr})-\mathcal{A}(\tilde{S}_{i}^{\rm val},S^{\rm tr})\|]\). Then, we have_

\[\epsilon_{\rm gen,arg}\geq\sum_{t=1}^{T}\prod_{s=t+1}^{T}\bigl{(}1+ \alpha_{s}(1-1/m)\gamma\bigr{)}\frac{\alpha_{t}L^{\prime}}{m},\]

_where_

\[L\asymp L^{\prime}\asymp(1+\eta\gamma^{\rm tr})^{K},\gamma= \gamma^{\prime}\asymp(1+\eta\gamma^{\rm tr})^{2K}.\]Proof.: We first decompose \(\epsilon_{\mathrm{gen,arg}}\) conditioned on the difference of \(\bm{z}_{i}^{\mathrm{val}}\) and \(\bm{z}_{i}^{\mathrm{val}^{\prime}}\) as

\[\epsilon_{\mathrm{gen,arg}}\] \[= \frac{1}{m}\sum_{i=1}^{m}\mathbb{P}[\bm{z}_{i}^{\mathrm{val}}=\bm{ z}_{i}^{\mathrm{val}^{\prime}}]\mathbb{E}_{\mathcal{A},S^{\mathrm{tr}},S^{ \mathrm{val}},\bm{z}_{i}^{\mathrm{val}^{\prime}}}[\|\mathcal{A}(S^{\mathrm{val }},S^{\mathrm{tr}});-\mathcal{A}(\tilde{S}_{i}^{\mathrm{val}},S^{\mathrm{tr}}) \|\bm{z}_{i}^{\mathrm{val}}=\bm{z}_{i}^{\mathrm{val}^{\prime}}]\] \[+\frac{1}{m}\sum_{i=1}^{m}\mathbb{P}[\bm{z}_{i}^{\mathrm{val}}\neq \bm{z}_{i}^{\mathrm{val}^{\prime}}]\mathbb{E}_{\mathcal{A},S^{\mathrm{tr}},S^{ \mathrm{val}},\bm{z}_{i}^{\mathrm{val}^{\prime}}}[\|\mathcal{A}(S^{\mathrm{val }},S^{\mathrm{tr}})-\mathcal{A}(\tilde{S}_{i}^{\mathrm{val}},S^{\mathrm{tr}}) \|\bm{z}_{i}^{\mathrm{val}}\neq\bm{z}_{i}^{\mathrm{val}^{\prime}}]\] \[= \frac{1}{m}\sum_{i=1}^{m}\mathbb{P}[\bm{z}_{i}^{\mathrm{val}}\neq \bm{z}_{i}^{\mathrm{val}^{\prime}}]\mathbb{E}_{\mathcal{A},S^{\mathrm{tr}},S^{ \mathrm{val}},\bm{z}_{i}^{\mathrm{val}^{\prime}}}[\|\mathcal{A}(S^{\mathrm{ val}},S^{\mathrm{tr}})-\mathcal{A}(\tilde{S}_{i}^{\mathrm{val}},S^{\mathrm{tr}}) \|\bm{z}_{i}^{\mathrm{val}}\neq\bm{z}_{i}^{\mathrm{val}^{\prime}}]\] \[= \frac{1}{2m}\sum_{i=1}^{m}\mathbb{E}_{\mathcal{A},S^{\mathrm{tr}},S^{\mathrm{val}},\bm{z}_{i}^{\mathrm{val}^{\prime}}}[\|\mathcal{A}(S^{\mathrm{ val}},S^{\mathrm{tr}})-\mathcal{A}(\tilde{S}_{i}^{\mathrm{val}},S^{\mathrm{tr}}) \|\bm{z}_{i}^{\mathrm{val}}\neq\bm{z}_{i}^{\mathrm{val}^{\prime}}].\]

The last equation holds for that as \(\bm{z}_{i}^{\mathrm{val}}\) and \(\bm{z}_{i}^{\mathrm{val}^{\prime}}\) are sampled from \(\mathcal{D}^{\mathrm{val}}\) specified in Example H.2 independently, which leads to \(\mathbb{P}[\bm{z}_{i}^{\mathrm{val}}\neq\bm{z}_{i}^{\mathrm{val}^{\prime}}]=1/2\).

According to Proposition 5.4 and Theorem 5.1, for any \(S^{\mathrm{tr}}\), \(i\in[m]\), and \(S^{\mathrm{val}}\simeq\tilde{S}_{i}^{\mathrm{val}}\) where \(\bm{z}_{i}^{\mathrm{val}}\neq\bm{z}_{i}^{\mathrm{val}^{\prime}}\in\{(-\bm{v}_ {1},1),(\bm{v}_{1},1)\}\), we have

\[\|\mathcal{A}(S^{\mathrm{val}},S^{\mathrm{tr}})-\mathcal{A}(\tilde{S}_{i}^{ \mathrm{val}},S^{\mathrm{tr}})\|\geq\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{(}1+ \alpha_{s}(1-1/m)\gamma\big{)}\frac{2\alpha_{t}L^{\prime}}{m}.\]

Therefore, it holds that

\[\epsilon_{\mathrm{gen,arg}}\geq\frac{1}{2m}\sum_{i=1}^{m}\sum_{t=1}^{T}\prod _{s=t+1}^{T}\big{(}1+\alpha_{s}(1-1/m)\gamma\big{)}\frac{2\alpha_{t}L^{\prime }}{m}=\sum_{t=1}^{T}\prod_{s=t+1}^{T}\big{(}1+\alpha_{s}(1-1/m)\gamma\big{)} \frac{\alpha_{t}L^{\prime}}{m}.\]

This result sheds light on the analysis of the generalization lower bound by establishing the lower bound on the expected hyperparameter divergence since it will induce a generalization lower bound if there exists a positive real constant \(\underline{L}\) such that \(|\epsilon_{\mathrm{gen}}|\geq\underline{L}\epsilon_{\mathrm{gen,arg}}\). One possible situation is that the designed compound validation loss satisfies for all \(\bm{z}\in\mathbb{Z}\), \(|\mathcal{L}(\bm{\lambda};\bm{z})-\mathcal{L}(\bm{\lambda}^{\prime};\bm{z})| \geq\underline{L}\|\bm{\lambda}-\bm{\lambda}^{\prime}\|\). As the generalization lower bound is beyond the main scope of this paper, further design and derivation may be left for future research.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our claims accurately match our theoretical results and reflect the paper's contribution and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All theoretical results include complete proofs with clearly stated assumptions in the Appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide a simulation code in the supplemental material. It is sufficient for reproducing our experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide a simulation code in the supplemental material. It is sufficient for reproducing our experimental results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide a simulation code with training details in the supplemental material. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The simulation experiments in our paper validate the theoretical results with the trend of the curves, where randomness does not affect the validity. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in our paper conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not release data or model. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.