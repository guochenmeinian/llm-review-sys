[MISSING_PAGE_FAIL:1]

Schutt et al., 2021; Batzner et al., 2022; Liu et al., 2021) to accelerate the prediction of their chemical properties as data-driven surrogate approximations. To enhance the prediction of vectorial properties, such as force fields, equivariant deep learning methods have been developed to capture permutation, translation, and rotation equivariance for equivariant property prediction (Satorras et al., 2021; Schutt et al., 2021; Tholke and Fabritiis, 2022; Thomas et al., 2018; Batzner et al., 2022; Fuchs et al., 2020; Liao and Smidt, 2023; Anderson et al., 2019; Brandstetter et al., 2022; Batatia et al., 2022). To support and facilitate the development of machine learning methods on quantum chemistry property prediction, many datasets have been generated to benchmark the respective tasks on molecular property prediction (Blum and Reymond, 2009; Ruddigkeit et al., 2012; Ramakrishnan et al., 2014; Wang et al., 2009; Nakata and Shimazaki, 2017), catalyst prediction (Chanussoot et al., 2021; Tran et al., 2023), and force field prediction (Chmiela et al., 2017, 2023).

In addition to these quantum chemistry prediction tasks, the quantum Hamiltonian is another significant and fundamental physical property that determines the quantum states and various materials properties (Marzari and Vanderbilt, 1997; Souza et al., 2001; Qian et al., 2010; Marzari et al., 2012; Bai et al., 2022). The quantum Hamiltonian can be calculated using Density Functional Theory (DFT) (Hohenberg and Kohn, 1964; Kohn and Sham, 1965) with a time complexity of \(O(n^{3}T)\), where \(n\) represents the number of electrons and \(T\) denotes the number of optimization steps required to achieve convergence. Given the high computational complexity of the DFT algorithms, accelerating such calculations for novel molecular and materials systems becomes a desirable but challenging task. To tackle this challenge, machine learning methods, such as quantum tensor networks (Li et al., 2022; Gong et al., 2023; Schutt et al., 2019; Yu et al., 2023; Unke et al., 2021), provide a highly promising approach for accelerating the DFT algorithms. These networks directly predict the final Hamiltonian matrix given the input 3D geometries, resulting in significant acceleration of calculations by orders of magnitude.

Unlike invariant chemical properties, Hamiltonian matrices obey intrinsic block-by-block matrix equivariance. This equivariance can be represented by the rotation Wigner D-Matrix, which may contain higher order rotations beyond 3D space. In order to make physically meaningful predictions, it is important to design quantum tensor network architectures that preserve this equivariance property. To perform systematic and in-depth study of this new task, there is a clear need to generate large-scale quantum tensor datasets and benchmarks. Currently, existing quantum Hamiltonian datasets

Figure 1: The target and content of the propose QH9 dataset and benchmark. Quantum tensor networks are built for predicting the target Hamiltonian matrix, facilitating the optimization loop within DFT by providing a precise approximation. Within this benchmark, stable and dynamic datasets are generated for training powerful quantum tensor networks and comprehensive evaluation metrics are proposed to measure the prediction quality.

include the MD17 (Schutt et al., 2019; Gastegger et al., 2020) and mixed MD17 (Yu et al., 2023) datasets, which consist of data for a single and four molecules, respectively. Recent public dataset NablaDFT (Khrabrov et al., 2022) contains Hamiltonian matrices for molecular conformations.

To provide a realistic dataset and comprehensive evaluation, we generate a new quantum tensor dataset named QH9. This dataset contains Hamiltonian matrices for 130,831 stable molecular geometries and 999 molecular dynamic trajectories. In order to provide comprehensive studies for quantum tensor networks, we have designed four specific tasks. The first two tasks, QH9-stable-id and QH9-stable-ood, aim to explore the performance of the networks in both in-distribution and out-of-distribution scenarios, specifically focusing on stable molecular geometries. The QH9-dynamic-geo task follows the setting of the mixed MD17, containing the same molecule with different geometries in the training, validation, and test. On the other hand, the QH9-dynamic-mol task splits the trajectories based on different molecules. Finally, we evaluate the transferability of the trained models on molecules with larger sizes, thereby testing the models' ability to generalize beyond the training dataset. To demonstrate the quality of the predicted Hamilton matrix, we use four metrics. These metrics are based on the Mean Absolute Error (MAE) of the predicted Hamiltonian matrix \(\bm{H}\), as well as the derived properties such as orbital energies \(\bm{\epsilon}\) and electronic wavefunction \(\psi\). Furthermore, to evaluate the quality of the predicted Hamiltonian in accelerating DFT calculations, we calculate the DFT optimization ratio by taking the model predictions as DFT initialization. The target and content of the proposed QH9 dataset and benchmark are demonstrated in Figure 1.

## 2 Background and Related Works

### Density Functional Theory (DFT)

Modeling the quantum states of physical systems is a central topic in computational quantum physics and chemistry. It aims to solve the Schrodinger equation (Schrodinger, 1926), which describes the electronic states shown as

\[\hat{H}\Psi\left(\bm{r}_{1},\cdots,\bm{r}_{n}\right)=E\Psi\left(\bm{r}_{1}, \cdots,\bm{r}_{n}\right),\] (1)

where \(\Psi\left(\bm{r}_{1},\cdots,\bm{r}_{n}\right)\) is the \(n\)-electronic wavefunctions and \(\bm{r}\) is the 3D coordinates. Electronic eigenvalues and wavefunctions play an important role in calculating numerous crucial physical properties, including the Highest Occupied Molecular Orbital (HOMO), the Lowest Unoccupied Molecular Orbital (LUMO), charge density and many others. However, due to the exponentially expanding input Hilbert space with the number of electrons, the computational cost to directly calculate many-electronic wavefunctions is extremely high. Therefore, various methods are proposed to approximate the solutions, such as the Hartree-Fock (HF) method (Szabo and Ostlund, 2012) that approximates the wavefunction itself, or density functional theory (Hohenberg and Kohn, 1964) that approximates the electron density. While the HF method scales with the number of electrons \(n\) as \(O(n^{4}T)\), DFT scales with \(O(n^{3}T)\) and therefore DFT is better suited for large-scale systems. DFT is based on the key discovery that the total energy and thus all ground-state properties of a system are uniquely determined by the ground-state electron density (Kohn and Sham, 1965).

Both of these approaches divide an \(n\)-electron system into a set of \(n\) non-interacting one-electron wavefunctions \(\psi_{i}(\bm{r_{i}})\), also called molecular orbitals in molecular systems. These one-electron orbitals can then be approximated by a linear combination of basis functions \(\phi_{j}(\bm{r})\) as \(\psi_{i}(\bm{r})=\sum_{j}C_{ij}\phi_{j}(\bm{r})\). The basis functions can be represented in analytical forms, such as Slater-type orbitals (STOs), Gaussian-type orbitals (GTOs), or plane waves, under numerical approximations for obtaining the coefficients matrix \(\bm{C}\). With these approximations, the original Schrodinger Equation (1) for electrons can be transformed into a matrix form as

\[\bm{H}\bm{C}_{i}=\bm{\epsilon}_{i}\bm{S}\bm{C}_{i},\] (2)

where \(\bm{H}\) is the Hamiltonian matrix, \(\bm{S}\) is the overlap matrix, and \(\bm{\epsilon}_{i}\) is the energy for the \(i\)-th orbital. The Hamiltonian matrix can be decomposed into the sum

\[\bm{H}=\bm{H}_{eN}+\bm{H}_{ee}+\bm{H}_{XC},\] (3)

which describes electron-nucleus interactions (\(\bm{H}_{eN}\)), electron-electron interactions (\(\bm{H}_{ee}\), including kinetic energy and electron-electron Coulomb repulsion energy), and exchange-correlation energy (\(\bm{H}_{XC}\)). These matrices take the electron density \(\bm{\rho}(\bm{r})\) as an input to evaluate the Hamiltonian matrix.

The exchange-correlation energy functional used in this paper was B3LYP (Lee et al., 1988; Becke, 1993), which is a hybrid functional that includes both the exchange energy from the HF method as well as a correlation potential. Thus, the complexity of using B3LYP in DFT is \(O(n^{4}T)\), which is the same as HF. We implement the GTO basis set Def2SVP (Weigend and Ahlrichs, 2005) that incorporates aspects of DFT, namely, an exchange-correlation potential, in order to more accurately capture electron-electron interactions compared to the HF method, which uses a mean field approximation of electron density.

Equation (2) is satisfied for the final Hamiltonian matrix and its coefficient matrix once self-consistency is achieved using direct inversion in the iterative subspace (DIIS) (Pulay, 1980, 1982). The equation is solved iteratively by building and solving the Hamiltonian and coefficient matrices, constructing an error vector based on a linear combination of energy differences in the previous steps, then diagonalizing and recalculating \(\bm{H}\) until the error vector is below a convergence threshold. In our QH9 datasets, we provide the Hamiltonian matrix \(\bm{H}\) used to train quantum tensor networks for directly predicting the Hamiltonian matrix.

### Group Equivariance and Equivariant Matrices

In many quantum chemistry problems, the molecular property to be predicted (e.g., energy and force) is internally invariant or equivariant to transformations in SE(3) group, including rotations and translations. Formally, for an \(n\)-atom molecule whose 3D atom coordinates are \(\bm{r}_{1},...,\bm{r}_{n}\), any transformation in SE(3) group can be described as changing the 3D atom coordinates to \(\bm{R}\bm{r}_{1}+\bm{t},...,\bm{R}\bm{r}_{n}+\bm{t}\). Here, the translation vector \(\bm{t}\in\mathbb{R}^{3}\) is an arbitrary 3D vector, and the rotation matrix \(\bm{R}\in\mathbb{R}^{3\times 3}\) satisfies that \(\bm{R}^{T}\bm{R}=\bm{I},|\bm{R}|=1\). Let \(\bm{f}(\cdot)\) map the 3D atom coordinates to an \((2\ell+1)\)-dimensional prediction target vector, we say \(\bm{f}\) is order-\(\ell\) SE(3)-equivariant if

\[\bm{f}(\bm{R}\bm{r}_{1}+\bm{t},...,\bm{R}\bm{r}_{n}+\bm{t})=D^{\ell}(\bm{R}) \bm{f}(\bm{r}_{1},...,\bm{r}_{n})\] (4)

holds for any rotation matrix \(\bm{R}\) and translation vector \(\bm{t}\), where \(D^{\ell}(\bm{R})\in\mathbb{C}^{(2\ell+1)\times(2\ell+1)}\) is the order-\(\ell\) Wigner-D matrix of \(\bm{R}\) (please refer to Section A.3 of Brandstetter et al. (2022) and Section A.2.1 of Poulenard et al. (2022) for more background information about the Wigner-D matrix). To accurately predict SE(3)-equivariant properties, an effective approach is to develop neural network models that are designed to maintain the same equivariance relations between inputs and outputs as in Equation (4). Recently, many studies have proposed SE(3)-equivariant neural network architectures by using SE(3)-invariant feature encoding (Schutt et al., 2018; Gasteiger et al., 2020, 2021; Liu et al., 2022), tensor product operations (Thomas et al., 2018; Brandstetter et al., 2022; Liao and Smidt, 2023), or atomic cluster expansion framework (Batatia et al., 2022; Drautz, 2019; Dusson et al., 2022; Kovacs et al., 2021; Musaelian et al., 2023).

Different from vector-like molecular properties, the Hamiltonian matrix \(\bm{H}\) has a much more complicated SE(3) equivariance pattern that is associated with the intrinsic angular momentum of the atomic orbital pairs. In computational quantum chemistry algorithms such as DFT, the Hamiltonian matrix \(\bm{H}\) obtained from DFT calculations can be used to represent the interactions between these atomic orbitals, and the block \(\bm{H}_{ij}\) in Hamiltonian matrix represents the interactions between the atomic orbitals \(i\) in atom \(a_{i}\) with angular quantum number \(\ell_{i}\) and atomic orbitals \(j\) in atom \(a_{j}\) with angular quantum number \(\ell_{j}\), and the shape of this block \(\bm{H}_{ij}\) is \((2\ell_{i}+1)\times(2\ell_{j}+1)\). Usually, the atomic orbitals are arranged sequentially for the orbitals in the same atom and with the same angular quantum number. For example, \(\bm{H}_{ij}\) can be located within the \(s_{i}\)-th to \((s_{i}+2\ell_{i})\)-th row, and the \(s_{j}\)-th to \((s_{j}+2\ell_{j})\)-th column of Hamiltonian matrix \(\bm{H}\). Specifically, its SE(3) equivariance can be described as

\[\bm{H}_{ij}\left(\bm{\rho}(\bm{R}\bm{r}+\bm{t})\right)=D^{\ell_{i}}(\bm{R})\bm {H}_{ij}\left(\bm{\rho}(\bm{r})\right)D^{\ell_{j}}(\bm{R}^{T}),\] (5)

where \(\bm{\rho}(\bm{r})\) is the electronic density at positioin \(\bm{r}\) and Hamiltonian matrix \(\bm{H}\) is a function of the electronic density \(\bm{\rho}(\bm{r})\) in the DFT algorithm. In other words, the SE(3) equivariance of different submatrices in \(\bm{H}\) has different mathematical forms, which is much more complicated than the SE(3) equivariance of vector-like molecular properties. Hence, it is much more challenging to develop SE(3)-equivariant neural network architectures for the prediction of Hamiltonian matrices. Nowadays, only a few studies (Li et al., 2022; Gong et al., 2023; Yu et al., 2023; Unke et al., 2021) have made initial exploration in this direction.

### Datasets for Quantum Chemistry

To facilitate the usage of machine learning models to predict chemistry properties and accelerate simulations, numerous quantum chemistry datasets have been built to provide extensive and faithful data. Here, we introduce several existing datasets that have been constructed for different tasks respectively, including molecular property prediction, catalyst modeling, molecular force field prediction, and molecular Hamiltonian matrix prediction. For molecular property prediction, the QM7 [Blum and Reymond, 2009] dataset was initially constructed using 7,165 molecules from the organic molecule database GDB-13 [Blum and Reymond, 2009], with each selected molecule having no more than 7 heavy atoms. The primary purpose of creating the QM7 dataset is to provide atomization energies as the target molecular property. Then QM9 [Ramakrishnan et al., 2014, Schutt et al., 2018] was built based on GDB-17 [Ruddigkeit et al., 2012] to provide 134k stable small organic molecules with no more than 9 heavy atoms in each molecule. Moreover, it provides 13 different important quantum chemistry properties, including HOMO and LUMO energies. Based on the molecules from PubQChem [Wang et al., 2009, 2017, Kim et al., 2019, 2021, 2023], PubQChemQC [Nakata and Shimazaki, 2017] provides 3M ground-state molecular structures as well as the HOMO-LUMO gap and excitation energies for 2M molecules. In addition to the molecular property datasets, OC20 [Chanussoit et al., 2021] and OC22 [Tran et al., 2023] were developed to provide the data of interactions of catalysts on material surfaces. They provide the geometries of the initial structures to predict the final structures or energies as well as the relaxation trajectories with energy and atomic forces. For the molecular force field prediction datasets, MD17 [Chmiela et al., 2017] and MD22 [Chmiela et al., 2023] contain atomic forces for molecular and supramolecular trajectories respectively as valuable datasets to develop machine learning methods. The last category is the Hamiltonian matrices datasets. MD17 [Schutt et al., 2019, Gastegger et al., 2020] provides the Hamiltonian matrices for single molecular dynamic trajectories to study the Hamiltonian matrices for molecules with various geometries. Building upon this dataset, mixed MD17 [Yu et al., 2023] combines four molecular trajectories in the MD17 to study Hamiltonian matrix prediction tasks with multiple molecules. NablaDFT [Khrabrov et al., 2022] has million of Hamiltonian matrices for molecular conformers and provides the MAE on predicted Hamiltonian matrices with model trained with in-distribution data split. Alongside the increasing interest in Hamiltonian matrix prediction, there is a growing need for datasets that include Hamiltonian matrices with a greater number of molecules and benchmark with comprehensive evaluation to facilitate the subsequent studies.

## 3 Datasets, Tasks, Methods, and Metrics

### Datasets

**Dataset Generation.** For the QH9 dataset, we use open-source software PySCF [Sun et al., 2018, 2020] to conduct computational quantum chemistry calculations. In the QH9, there are two sub datasets. The first one is the QH9-stable dataset containing Hamiltonian matrices for 130,831 molecules with their geometries. We obtain the molecules and their geometries in QH9-stable from the QM9 version with 130,831, which is widely used in molecular property prediction tasks in the literature [Schutt et al., 2018, Gasteiger et al., 2020, Liu et al., 2022, Wang et al., 2022]. Note that we cover all the molecules in this QM9 version. The second one is the QH9-dynamic dataset, it has molecular trajectories for \(999\) molecules and each trajectory contains \(100\) geometries. To obtain the accurate Hamiltonian matrices for this dataset, we set the hyper-parameters of the DFT algorithms to a tight level. Specifically, we set the grid density level to 3 to calculate accurate electronic density, and the SCF convergence condition is set to SCF tolerance of \(10^{-13}\) and gradient threshold of \(3.16\times 10^{-5}\) to ensure the the final states achieve tight convergence. For the density functional, we select the B3LYP exchange-correlation functional to conduct DFT calculations, and GTO orbital basis Def2SVP is selected to approximate the electronic wavefunctions. To accelerate and achieve the convergence of SCF algorithm, we use DIIS algorithm with consideration of the 8 previous steps. For the QH9-dynamic dataset, molecular dynamics simulations are conducted under the microcanonical ensemble, where the number of particles, volume, and energy remain constant (NVE). The temperature is set to 300K, and the time step for recording the molecular trajectory is set to \(0.12\) fs with \(1,000\) total steps. In the dataset collection, we sampled \(100\) time steps for each trajectory with a timestep of \(1.2\) fs. The generated QH9 dataset is available at Zenodo (https://zenodo.org/records/8274793) and GitHub (https://github.com/divelab/AIRS/tree/main/OpenDFT/QHBench/QH9).

**Dataset Statistics.** The statistical data, including the number of molecules and geometries for QH9-stable and QH9-dynamic, is presented in Table 1. These molecules consist of no more than 9 heavy atoms and are composed of four specific heavy atoms: carbon (C), nitrogen (N), oxygen (O), and fluorine (F). The distribution of molecule size for QH9-stable and QH9-dynamic is shown in Figure 1(a) and Figure 1(c). Meanwhile, the percentage of molecules in QH9-stable with different the number of heavy atoms is shown in 1(b).

### Tasks

To comprehensively evaluate the quantum Hamiltonian prediction performance, we define the following tasks based on the obtained stable and dynamic geometries in the QH9 dataset.

**QH9-stable-id.** We first randomly divide the obtained stable geometries in QH9 into three subsets, including \(80\%\) for training, \(10\%\) for validation, and \(10\%\) for testing. This serves as the basic evaluation task for predicting quantum Hamiltonian matrices.

**QH9-stable-ood.** We further split the stable geometries in QH9 by molecular size based on the number of constituting atoms. The training set consists of molecules with \(3\) to \(20\) atoms, maintaining a similar number of training samples as in the QH9-stable-id split. The validation set includes molecules with \(21\) to \(22\) atoms, while the testing set has molecules with \(23\) to \(29\) atoms. This task allows for an evaluation of the model's generalization ability under an out-of-distribution training setup.

**QH9-dynamic-geo.** For this split and the following QH9-dynamic-mol split, there are \(999\) molecular dynamics trajectories, while each trajectory includes \(100\) geometries. In QH9-dynamic-geo, the split is performed geometry-wise. Specifically, for each molecule, \(100\) geometries are randomly divided into \(80\) for training, \(10\) for validation, and \(10\) for testing. Here, the molecules in the test set are visible during training but the geometric structures are different from training structures.

**QH9-dynamic-mol.** In this QH9-dynamic-mol split, the \(999\) molecules are divided into training, validation, and testing subsets in a ratio of \(0.8/0.1/0.1\). Importantly, different from the above QH9-dynamic-geo setup, all \(100\) geometries corresponding to a specific molecule are grouped together and assigned to the same subset. This setup introduces a more challenging task than QH9-dynamic-geo since the geometries in the testing set correspond to different molecules as those in training.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Task & \# Total geometries & \# Molecules & \# Training/validation/testing geometries \\ \hline QH9-stable-id & \(130,831\) & \(130,831\) & \(104,664/13,083/13,084\) \\ QH9-stable-ood & \(130,831\) & \(130,831\) & \(104,001/17,495/9,335\) \\ QH9-dynamic-geo & \(99,900\) & \(999\) & \(79,920/9,990/9,990\) \\ QH9-dynamic-mol & \(99,900\) & \(999\) & \(79,900/9,900/10,100\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: The statistics of our defined four tasks.

Figure 2: The dataset statistics on QH9-stable and QH9-dynamic, including molecule size distribution and percentage of molecules with different number of heavy atoms.

### Methods

To predict the quantum Hamiltonian matrix, several quantum tensor networks have been proposed (Li et al., 2023; Gong et al., 2023). SchNOrb (Schutt et al., 2019) uses pairwise distance and direction as the input geometric information to predict the final Hamiltonian matrix. However, SchNOrb lacks the ability to ensure matrix equivariance and relies on data augmentation techniques to encourage equivariance. Another network, DeepH (Li et al., 2022), uses invariant local coordinate systems and a global coordinate system to handle the equivariance challenge. It uses the geometric features within these invariant local coordinate systems to predict invariant Hamiltonian matrix blocks. Next, as a post-processing step, DeepH applies a rotation using Wigner D-Matrix to transform the Hamiltonian matrix blocks from local coordinate systems back to the global coordinate system. Currently, DeepH is applied on predicting Hamiltonian matrices for materials. PhiSNet (Unke et al., 2021) uses an equivariant model architecture that inherently guarantees matrix equivariance. However, current implementation of PhiSNet is limited to supporting single molecule. This limitation arises from the design of the matrix prediction module in PhiSNet, which is designed to predict matrices for the same molecules with fixed matrix size. Therefore, equivariant quantum tensor network QHNet (Yu et al., 2023) is selected as the main baseline method in the QH9 benchmark currently. QHNet has an extendable expansion module that is built upon intermediate full orbital matrices, enabling its capability to effectively handle different molecules. This flexibility allows QHNet to accommodate various molecules in the QH9 benchmark.

### Metrics

To evaluate the quality of the predicted Hamiltonian matrix, we adopt several metrics that are used to measure both approximation accuracy and computational efficiency.

**MAE on Hamiltonian matrix H.** This metric calculates the Mean Absolute Error (MAE) between the predicted Hamiltonian matrix and the ground-truth labels from DFT calculation. Each Hamiltonian matrix consists of diagonal blocks and non-diagonal blocks, representing the interactions within individual atoms and the interactions between pairs of atoms, respectively. When the atom pair is distant, the values in the Hamiltonian matrix blocks are typically close to zero. Consequently, as the molecules increase in size, the proportion of distant atom pairs also increases, causing the overall mean value of the Hamiltonian matrix to decrease. Hence, in the subsequent experiments, we compare the MAEs of the diagonal and non-diagonal blocks separately as well as the total MAE on the Hamiltonian matrix.

**MAE on occupied orbital energies \(\epsilon\)**. Orbital energy, which includes the Highest Occupied Molecular Orbital (HOMO) and Lowest Unoccupied Molecular Orbital (LUMO) energies, is a highly significant chemical property. It can be determined by diagonalizing the Hamiltonian matrix using Equation 2. Hence, this metric can serve as a measure to reflect the quality of the predicted Hamiltonian matrix in accurately deducing the desired property. Specifically, it calculates the MAE on all the occupied molecular orbital energies \(\epsilon\) derived from the predicted and the ground-truth Hamiltonian matrix.

**Cosine similarity of orbital coefficients \(\psi\).** Electronic wavefunctions can describe the quantum states of molecular systems and are used to derive a range of chemical properties. In order to measure the similarity between the ground-truth wavefunctions and the predicted wavefunctions, we calculate the cosine similarity of the coefficients for the occupied molecular orbitals \(\psi\). The corresponding coefficients \(\mathbf{C}\) are derived from the predicted and ground-truth Hamiltonian matrix shown in Equation 2.

**Acceleration ratio.** Besides the metrics assessing molecular properties, several acceleration ratios, such as achieved ratio and error-level ratio, are proposed to measure the quality of the predicted Hamiltonian matrix in accelerating DFT calculation. Specifically, the achieved ratio calculates the ratio of the number of optimization steps between initializing with the predicted Hamiltonian matrix and using initial guess methods like minao and 1e. When the Hamiltonian matrix is accurately predicted, the Self-Consistent Field (SCF) algorithm approaches convergence, resulting in a substantial reduction in the number of optimization steps. As a comparison, the optimal ratio calculates the single optimization step for each molecule divided by the total number of steps, serving as an illustrative benchmark of the ideal performance. Meanwhile, the error-level ratio calculates the ratio over thenumber of steps required to reach a the same error level as model prediction during the DFT SCF loop compared to the total number of steps in the DFT process.

## 4 Experiments

**Setup.** To assess how deep learning approaches perform on the proposed dataset, we conduct experiments on the four designed tasks, as described in Section 3.2. To be more specific, we evaluate the performance of QHNet (Yu et al., 2023), a recently proposed SE(3)-equivariant network specifically designed for efficient and accurate quantum Hamiltonian matrix prediction. QHNet is known for its effectiveness and efficiency in handling the task at hand, making it a suitable testing method for our benchmark evaluation. For quantitative evaluation, we use the metrics as introduced in Section 3.4. Our implementation is based on PyTorch (Paszke et al., 2019), PyG (Fey and Lenssen, 2019), and e3nn (Geiger et al., 2022). We train models on either (1) a single 48GB Nvidia GeForce RTX A6000 GPU and Intel Xeon Silver 4214R CPU, or (2) a single Nvidia A100 GPU and Intel Xeon Gold 6258R CPU.

Following the model setup in QHNet, in all implemented models, we employ five node-wise interaction layers to aggregate messages from neighboring nodes to update the node irreducible representations. We train all models with a total training step of either \(210,000\) or \(260,000\) using a batch size of \(32\). To expedite the convergence of model training, following the QHNet setup, we implement a learning rate scheduler. The scheduler gradually increases the learning rate from \(0\) to a maximum value of \(5\times 10^{-4}\) over the first \(1,000\) warm-up steps. Subsequently, the scheduler linearly reduces the learning rate, ensuring it reaches \(1\times 10^{-7}\) by the final step.

**Overall performance.** We first evaluate the overall performance of the model on the four defined tasks by demonstrating its accuracy of the predicted Hamiltonian matrices on the testing set. As summarized in Table 2, the employed QHNet models can achieve a reasonably low MAE in predicting the Hamiltonian matrices on all proposed tasks. For reference, QHNet can achieve an MAE of \(83.12\times 10^{-6}E_{h}\) on the mixed MD17 dataset, which has a similar setup to our QH9-dynamic-geo setup. In addition to MAE on Hamiltonian matrices, the trained models also achieve low errors on the predicted occupied orbital energies and orbital coefficients. This aligns with the prior reported work that QHNet is effective to predict the Hamiltonian matrices for multiple molecules (Yu et al., 2023). Notably, compared to the existing Hamiltonian matrix datasets, such as MD17 (Chmela et al., 2017) and mixed MD17 (Yu et al., 2023), our proposed tasks involve predicting Hamiltonian matrices for significantly more molecules. Overall, we anticipate that the proposed new datasets and corresponding tasks can serve as more challenging and realistic testbeds for future research in Hamiltonian matrix prediction.

**Investigation on out-of-distribution generalization.** Since we maintain a similar number of training samples for QH9-stable-id and QH9-stable-ood, it is feasible to compare the performance of these two settings to investigate the out-of-distribution challenge in predicting Hamiltonian matrices. It is worth noting that we cannot directly compare the performance on their respective test sets, as reported in Table 2, to demonstrate the out-of-distribution generalizability challenge. This is because the molecules in the QH9-stable-ood test set have a larger number of atoms on average than those in QH9-stable-id. As explained in Section 3.4, molecules with larger size typically have more distant atom pairs, thus leading to a lower overall mean value of the Hamiltonian matrix. Hence, numerical results on molecules with different sizes are not directly comparable.

\begin{table}
\begin{tabular}{l l l r r r r} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Model} & \multicolumn{3}{c}{\(\mathbf{H}\left[10^{-6}E_{h}\right]\downarrow\)} & \multirow{2}{*}{\(\epsilon\)\([10^{-6}E_{h}]\downarrow\)} & \multirow{2}{*}{\(\psi\)\([10^{-2}]\uparrow\)} \\  & & diagonal & non-diagonal & all & \\ \hline QH9-stable-id & QHNet & \(111.21\) & \(73.68\) & \(76.31\) & \(798.51\) & \(95.85\) \\ QH9-stable-ood & QHNet & \(111.72\) & \(69.88\) & \(72.11\) & \(644.17\) & \(93.68\) \\ \hline QH9-dynamic-geo & QHNet & \(149.62\) & \(92.88\) & \(96.85\) & \(834.47\) & \(94.45\) \\ QH9-dynamic-mol & QHNet & \(416.99\) & \(153.68\) & \(173.92\) & \(9719.58\) & \(79.15\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: (Results updated by using the newest QHNet model.) The overall performance on the testing set on the defined four tasks. The unit for the Hamiltonian \(\mathbf{H}\) and eigenenergies \(\boldsymbol{\epsilon}\) is Hartree denoted by \(E_{h}\).

To examine the presence of the out-of-distribution issue in the Hamiltonian prediction task, we adopt an alternative evaluation strategy. To be specific, we assess models that have been trained respectively on the QH9-stable-id and QH9-stable-ood training sets, employing the same set of samples for evaluation in each instance. Specifically, we use the intersecting set of the QH9-stable-id and QH9-stable-ood testing sets as our evaluation dataset. Clearly, the samples contained within this evaluation set are previously unseen during the training phase of either model, thereby maintaining the integrity of the assessment. The evaluation set contains \(923\) molecules with \(23\) to \(29\) atoms. Under this experimental setup, the primary challenge faced by the model trained on the QH9-stable-ood training set stems from the novelty of molecular sizes during the evaluation phase. On the other hand, the model trained on the QH9-stable-id training set benefits from having been exposed to such molecular sizes during training. We denote that these two models are trained under out-of-distribution (OOD) and in-distribution (ID) training schema respectively in Table 3. By comparing the performance on the identical evaluation set, it becomes apparent that the model employing the ID training schema outperforms its OOD-trained counterpart, across all metrics including Hamiltonian MAE and predicted orbital energies and coefficients. Such a performance gap demonstrates that the out-of-distribution issue in molecular size is actually a valid concern particularly when extending trained models to molecular sizes not encountered during training.

**Geometry-wise _vs._ molecule-wise generalization.** We further explore geometry-wise and molecule-wise generalizability by analyzing the difficulty differences between the QH9-dynamic-geo and QH9-dynamic-mol tasks. We consider the results in Table 2 for these two tasks to be comparable given that both models are trained with a similar number of geometry structures. We note that the model in the QH9-dynamic-geo task demonstrates numerically better test performance than the model in the QH9-dynamic-mol task. This is consistent with our intention when designing the tasks. Specifically, in QH9-dynamic-geo, although the geometric structures in the test set are different, the molecules themselves are not entirely novel to the model due to the exposure during the training phase. In comparison, the QH9-dynamic-mol task presents a more challenging and demanding scenario. In particular, the test set geometries in QH9-dynamic-mol correspond to entirely different molecules than those seen during training. This task requires the model to generalize from its learned patterns to the unseen molecular structures. To summarize, both tasks serve as valuable testbeds in evaluating the model's generalization ability, and our analysis shows that QH9-dynamic-mol task, which requires extrapolating to entirely new molecular structures, is notably more challenging and demanding.

**Accelerating the DFT calculation.** We further measure the quality of the predicted Hamiltonian matrix by evaluating its ability in accelerating the DFT calculation. As introduced in Section 3.4, we

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Training Dataset & DFT initialization & Metric & Ratio & Training Dataset & DFT initialization & Metric & Ratio \\ \hline \multirow{4}{*}{QH9-stable-id} & \multirow{2}{*}{1e} & Optimal ratio & \(0.057\pm 0.001\) & \multirow{2}{*}{1e} & Optimal ratio & \(0.057\pm 0.004\) \\  & & Achieved ratio \(\downarrow\) & **0.395\(\pm 0.003\)** & & Achieved ratio \(\downarrow\) & \(0.400\pm 0.010\) \\  & & Error-level ratio \(\uparrow\) & **0.633\(\pm 0.007\)** & & Erm-level ratio \(\uparrow\) & \(0.620\pm 0.017\) \\  & & Optimal ratio & \(0.102\pm 0.003\) & & Optimal ratio & \(0.102\pm 0.003\) \\  & & Achieved ratio \(\downarrow\) & **0.706\(\pm 0.001\)** & & Achieved ratio \(\uparrow\) & \(0.715\pm 0.003\) \\  & & Error-level ratio \(\uparrow\) & **0.408\(\pm 0.003\)** & & Erm-level ratio \(\uparrow\) & \(0.406\pm 0.002\) \\ \hline \multirow{4}{*}{QH9-dynamic-geo} & \multirow{2}{*}{1e} & Optimal ratio & \(0.056\pm 0.008\) & \multirow{2}{*}{1e} & Optimal ratio & \(0.056\pm 0.006\) \\  & & Achieved ratio \(\downarrow\) & **0.392\(\pm 0.005\)** & & Achieved ratio \(\downarrow\) & \(0.512\pm 0.158\) \\  & & Error-level ratio \(\uparrow\) & **0.483\(\pm 0.001\)** & & Erm-level ratio \(\uparrow\) & \(0.622\pm 0.008\) \\ \cline{1-1}  & & Optimal ratio & \(0.058\pm 0.008\) & & Optimal ratio & \(0.088\pm 0.008\) \\ \cline{1-1}  & & Achieved ratio \(\downarrow\) & **0.679\(\pm 0.004\)** & & Achieved ratio \(\downarrow\) & \(0.882\pm 0.17\) \\ \cline{1-1}  & & Error-level ratio \(\uparrow\) & **0.443\(\pm 0.004\)** & & Erm-level ratio \(\uparrow\) & \(0.406\pm 0.006\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: The performance of DFT calculation acceleration. Both models, trained on the QH9-stable-id split and the QH9-stable-ood split respectively, are evaluated on a common set of \(50\) randomly chosen molecules from the intersection of their test sets, making their results directly comparable. Similarly, results from models trained on the QH9-dynamic-geo and QH9-dynamic-mol splits can be compared directly.

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline Training schema & Models & \multicolumn{3}{c}{\(\mathbf{H}\;[10^{-6}E_{h}]\downarrow\)} & \multirow{3}{*}{\(\boldsymbol{\epsilon}\;[10^{-6}E_{h}]\downarrow\)} & \multirow{3}{*}{\(\boldsymbol{\psi}\;[10^{-2}]\uparrow\)} \\  & & diagonal & non-diagonal & all & \\ \hline ID & QHNet & \(84.19\) & \(56.01\) & \(57.53\) & \(442.78\) & \(95.26\) \\ OOD & QHNet & \(113.05\) & \(70.52\) & \(72.78\) & \(630.49\) & \(94.01\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: The performance of in-distribution (ID) training and out-of-distribution (OOD) training on the constructed evaluation set for the OOD investigation.

compute the ratio of optimization steps required when initializing with the predicted Hamiltonian matrix as compared to classic initial guess methods such as minao and 1e. Note that minao diagonalizes the Fock matrix obtained from a minimal basis to get the guess orbitals, and 1e is one-electron guess which diagonalizes the core Hamiltonian to obtain the guess orbitals. In this experiment, following our data collection process, we use PySCF [22] to perform the DFT calculation with using B3LYP exchange-correlation functional and def2SVP basis set. We select DIIS as the SCF algorithm for the DFT calculation and set a grid density level of \(3\) to ensure an accurate DFT calculation. For each dataset, we compute the average optimization step ratio for \(50\) randomly selected molecules. As shown in Table 4, we provide several metrics to reflect the optimization ratio. The optimal ratio is the ratio of a single step to the DFT optimization steps. The achieve ratio is the number of optimization steps when initialized by model prediction to optimization steps by DFT initialization. The error-level ratio is the number of optimization steps that achieve similar MAE error with model prediction to the total DFT optimization steps. We can observe that when initializing from the predicted Hamiltonian matrices given by QHNet, it requires fewer optimization steps to reach the converged Hamiltonian matrix, which indicates that the predicted Hamiltonian matrix is close to the convergence condition. This set of experimental results demonstrates that machine learning approaches are helpful in accelerating the DFT calculation.

## 5 Conclusion

We are interested in accelerating computation of quantum Hamiltonian matrices, which fundamentally determine the quantum states of physical systems and chemical properties. While various invariant and equivariant deep learning methods have been developed recently, current quantum Hamiltonian datasets consist of Hamiltonian matrices of molecular dynamic trajectories for only a single and four molecules, respectively. To significantly expand the size and variety of such datasets, we generate a much larger dataset based on the QM9 molecules. Our dataset provides precise Hamiltonian matrices for 130,831 stable molecular geometries and \(999\) molecular dynamics trajectories with sampled \(100\) geometries in each trajectory. Extensive and carefully designed experiments are conducted to demonstrate the quality of our generated data.

## Acknowledgements

This work was supported in part by National Science Foundation grants IIS-2006861, CCF-1553281, DMR-2119103, DMR-2103842, CMMI-2226908, and IIS-2212419 and by the donors of ACS Petroleum Research Fund under Grant 65502-ND10. Portions of this research were conducted with the advanced computing resources provided by Texas A&M High Performance Research Computing.

## References

* Anderson et al. (2019) Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural networks. _Advances in neural information processing systems_, 32, 2019.
* Bai et al. (2022) Hexin Bai, Peng Chu, Jeng-Yuan Tsai, Nathan Wilson, Xiaofeng Qian, Qimin Yan, and Haibin Ling. Graph neural network for Hamiltonian-based material property prediction. _Neural Computing and Applications_, 34:4625-4632, 2022.
* Batatia et al. (2022) Ilyes Batatia, David Peter Kovacs, Gregor N. C. Simm, Christoph Ortner, and Gabor Csanyi. MACE: Higher order equivariant message passing neural networks for fast and accurate force fields. In _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=YPpSngE-ZU.
* Batzner et al. (2022) Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. _Nature Communications_, 13(1):2453, 2022.
* Becke (1993) Axel D. Becke. Density-functional thermochemistry. III. The role of exact exchange. _The Journal of Chemical Physics_, 98(7):5648-5652, 04 1993. ISSN 0021-9606. doi: 10.1063/1.464913. URL https://doi.org/10.1063/1.464913.
* Blum and Reymond (2009) Lorenz C Blum and Jean-Louis Reymond. 970 million druglike small molecules for virtual screening in the chemical universe database gdb-13. _Journal of the American Chemical Society_, 131(25):8732-8733, 2009.
* Brandstetter et al. (2022) Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geometric and physical quantities improve e(3) equivariant message passing. In _International Conference on Learning Representations_, 2022.
* Chanussot et al. (2021) Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, et al. Open catalyst 2020 (oc20) dataset and community challenges. _ACS Catalysis_, 11(10):6059-6072, 2021.
* Chmiela et al. (2017) Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Schutt, and Klaus-Robert Muller. Machine learning of accurate energy-conserving molecular force fields. _Science Advances_, 3(5):e1603015, 2017.
* Chmiela et al. (2023) Stefan Chmiela, Valentin Vassilev-Galindo, Oliver T Unke, Adil Kalylda, Huziel E Sauceda, Alexandre Tkatchenko, and Klaus-Robert Muller. Accurate global machine learning force fields for molecules with hundreds of atoms. _Science Advances_, 9(2):eadf0873, 2023.
* Drautz (2019) Ralf Drautz. Atomic cluster expansion for accurate and transferable interatomic potentials. _Physical Review B_, 99(1):014104, 2019.
* Dusson et al. (2022) Genevieve Dusson, Markus Bachmayr, Gabor Csanyi, Ralf Drautz, Simon Etter, Cas van der Oord, and Christoph Ortner. Atomic cluster expansion: Completeness, efficiency and stability. _Journal of Computational Physics_, 454:110946, 2022.
* Fey and Lenssen (2019) Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv Preprint, arXiv:1903.02428_, 2019.
* Fuchs et al. (2020) Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE(3)-transformers: 3D rototranslation equivariant attention networks. _Advances in Neural Information Processing Systems_, 33:1970-1981, 2020.
* Gastegger et al. (2020) Michael Gastegger, Adam McSloy, M Luya, Kristof T Schutt, and Reinhard J Maurer. A deep neural network for molecular wave functions in quasi-atomic minimal basis representation. _The Journal of Chemical Physics_, 153(4):044123, 2020.
* Gasteiger et al. (2020) Johannes Gasteiger, Janek Gross, and Stephan Gunnemann. Directional message passing for molecular graphs. In _International Conference on Learning Representations_, 2020.
* Gastegger et al. (2021)Johannes Gasteiger, Florian Becker, and Stephan Gunnemann. GemNet: Universal directional graph neural networks for molecules. _Advances in Neural Information Processing Systems_, 34:6790-6802, 2021.
* Geiger et al. [2022] Mario Geiger, Tess Smidt, Alby M., Benjamin Kurt Miller, Wouter Boomsma, Bradley Dice, Kostiantyn Lapchevskyi, Maurice Weiler, Michal Tyszkiewicz, Simon Batzner, Dylan Madistetti, Martin Uhrin, Jes Frellsen, Nuri Jung, Sophia Sanborn, Mingjian Wen, Josh Rackers, Marcel Rod, and Michael Bailey. Euclidean neural networks: e3nn, April 2022. URL https://doi.org/10.5281/zenodo.6459381.
* Gong et al. [2023] Xiaoxun Gong, He Li, Nianlong Zou, Runzhang Xu, Wenhui Duan, and Yong Xu. General framework for E(3)-equivariant neural network representation of density functional theory Hamiltonian. _Nature Communications_, 14(1):2848, 2023.
* Hohenberg and Kohn [1964] P. Hohenberg and W. Kohn. Inhomogeneous electron gas. _Phys. Rev._, 136:B864-B871, Nov 1964. doi: 10.1103/PhysRev.136.B864. URL https://link.aps.org/doi/10.1103/PhysRev.136.B864.
* Huang et al. [2023] Bing Huang, Guido Falk von Rudorff, and O Anatole von Lilienfeld. The central role of density functional theory in the ai age. _Science_, 381(6654):170-175, 2023.
* Khrabrov et al. [2022] Kuzma Khrabrov, Ilya Shenbin, Alexander Ryabov, Artem Tsypin, Alexander Telepov, Anton Alekseev, Alexander Grishin, Pavel Strashnov, Petr Zhilyaev, Sergey Nikolenko, et al. nabladft: Large-scale conformational energy and hamiltonian prediction benchmark and dataset. _Physical Chemistry Chemical Physics_, 24(42):25853-25863, 2022.
* Kim et al. [2019] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. PubChem 2019 update: improved access to chemical data. _Nucleic Acids Research_, 47(D1):D1102-D1109, 2019.
* Kim et al. [2021] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. PubChem in 2021: new data content and improved web interfaces. _Nucleic Acids Research_, 49(D1):D1388-D1395, 2021.
* Kim et al. [2023] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. PubChem 2023 update. _Nucleic Acids Research_, 51(D1):D1373-D1380, 2023.
* Kirkpatrick et al. [2021] James Kirkpatrick, Brendan McMorrow, David HP Turban, Alexander L Gaunt, James S Spencer, Alexander GDG Matthews, Annette Obika, Louis Thiry, Meire Fortunato, David Pfau, et al. Pushing the frontiers of density functionals by solving the fractional electron problem. _Science_, 374(6573):1385-1389, 2021.
* Kohn and Sham [1965] W. Kohn and L. J. Sham. Self-consistent equations including exchange and correlation effects. _Phys. Rev._, 140:A1133-A1138, Nov 1965. doi: 10.1103/PhysRev.140.A1133. URL https://link.aps.org/doi/10.1103/PhysRev.140.A1133.
* Kovacs et al. [2021] David Peter Kovacs, Cas van der Oord, Jiri Kucera, Alice EA Allen, Daniel J Cole, Christoph Ortner, and Gabor Csanyi. Linear Atomic Cluster Expansion Force Fields for Organic Molecules: Beyond RMSE. _Journal of Chemical Theory and Computation_, 17(12):7696-7711, 2021.
* Lee et al. [1988] Chengteh Lee, Weitao Yang, and Robert G. Parr. Development of the colle-salvetti correlation-energy formula into a functional of the electron density. _Phys. Rev. B_, 37:785-789, Jan 1988. doi: 10.1103/PhysRevB.37.785. URL https://link.aps.org/doi/10.1103/PhysRevB.37.785.
* Li et al. [2022] He Li, Zun Wang, Nianlong Zou, Meng Ye, Runzhang Xu, Xiaoxun Gong, Wenhui Duan, and Yong Xu. Deep-learning density functional theory hamiltonian for efficient ab initio electronic-structure calculation. _Nature Computational Science_, 2(6):367-377, 2022.
* Li et al. [2023] He Li, Zechen Tang, Xiaoxun Gong, Nianlong Zou, Wenhui Duan, and Yong Xu. Deep-learning electronic-structure calculation of magnetic superstructures. _Nature Computational Science_, 3(4):321-327, 2023.
* Li et al. [2021]Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=KwmPfAfg0DTD.
* Liao et al. [2023] Yi-Lun Liao, Brandon Wood, Abhishek Das, and Tess Smidt. Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations. _arXiv preprint arXiv:2306.12059_, 2023.
* Lin et al. [2023] Yuchao Lin, Keqiang Yan, Youzhi Luo, Yi Liu, Xiaoning Qian, and Shuiwang Ji. Efficient approximations of complete interatomic potentials for crystal property prediction. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* Liu et al. [2021] Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu, Jingtun Zhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora M Oztekin, Xuan Zhang, and Shuiwang Ji. DIG: A turnkey library for diving into graph deep learning research. _Journal of Machine Learning Research_, 22(240):1-9, 2021. URL http://jmlr.org/papers/v22/21-0343.html.
* Liu et al. [2022] Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3D molecular graphs. In _International Conference on Learning Representations_, 2022.
* Marzari and Vanderbilt [1997] Nicola Marzari and David Vanderbilt. Maximally localized generalized Wannier functions for composite energy bands. _Phys. Rev. B_, 56:12847-12865, 1997.
* Marzari et al. [2012] Nicola Marzari, Arash A. Mostofi, Jonathan R. Yates, Ivo Souza, and David Vanderbilt. Maximally localized Wannier functions: Theory and applications. _Rev. Mod. Phys._, 84:1419-1475, 2012.
* Musaelian et al. [2023] Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. _Nature Communications_, 14(1):579, 2023.
* Nakata and Shimazaki [2017] Maho Nakata and Tomomi Shimazaki. PubChemQC Project: A Large-Scale First-Principles Electronic Structure Database for Data-Driven Chemistry. _Journal of Chemical Information and Modeling_, 57(6):1300-1308, 2017.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. _Advances in Neural Information Processing Systems_, 32, 2019.
* Poulenard et al. [2022] Adrien Poulenard, Maks Ovsjanikov, and Leonidas J Guibas. Equivalence between se (3) equivariant networks via steerable kernels and group convolution. _arXiv preprint arXiv:2211.15903_, 2022.
* Pulay [1980] Peter Pulay. Convergence acceleration of iterative sequences. the case of scf iteration. _Chemical Physics Letters_, 73(2):393-398, 1980. ISSN 0009-2614. doi: https://doi.org/10.1016/0009-2614(80)80396-4. URL https://www.sciencedirect.com/science/article/pii/0009261480803964.
* Pulay [1982] Peter Pulay. Improved scf convergence acceleration. _Journal of Computational Chemistry_, 3(4):556-560, 1982. doi: https://doi.org/10.1002/jcc.540030413. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.540030413.
* Qian et al. [2010] Xiaofeng Qian, Ju Li, and Sidney Yip. Calculating phase-coherent quantum transport in nanoelectronics with _ab initio_ quasiatomic orbital basis set. _Phys. Rev. B_, 82:195442, 2010.
* Ramakrishnan et al. [2014] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. _Scientific Data_, 1(1):1-7, 2014.
* Ruddigkeit et al. [2012] Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and Jean-Louis Reymond. Enumeration of 166 Billion Organic Small Molecules in the Chemical Universe Database GDB-17. _Journal of Chemical Information and Modeling_, 52(11):2864-2875, 2012.
* Satorras et al. [2021] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. In _International Conference on Machine Learning_, pages 9323-9332. PMLR, 2021.
* Satorras et al. [2021]Erwin Schrodinger. An undulatory theory of the mechanics of atoms and molecules. _Physical Review_, 28(6):1049, 1926.
* Schutt et al. (2021) Kristof Schutt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In _International Conference on Machine Learning_, pages 9377-9388. PMLR, 2021.
* Schutt et al. (2018) Kristof T Schutt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Muller. SchNet-a deep learning architecture for molecules and materials. _The Journal of Chemical Physics_, 148(24):241722, 2018.
* Schutt et al. (2019) Kristof T Schutt, Michael Gastegger, Alexandre Tkatchenko, K-R Muller, and Reinhard J Maurer. Unifying machine learning and quantum chemistry with a deep neural network for molecular wavefunctions. _Nature Communications_, 10(1):5024, 2019.
* Souza et al. (2001) Ivo Souza, Nicola Marzari, and David Vanderbilt. Maximally localized Wannier functions for entangled energy bands. _Phys. Rev. B_, 65:035109, 2001.
* Sun et al. (2018) Qiming Sun, Timothy C Berkelbach, Nick S Blunt, George H Booth, Sheng Guo, Zhendong Li, Junzi Liu, James D McClain, Elvira R Sayfutyarova, Sandeep Sharma, et al. PySCF: the Python-based simulations of chemistry framework. _Wiley Interdisciplinary Reviews: Computational Molecular Science_, 8(1):e1340, 2018. doi: https://doi.org/10.1002/wcms.1340.
* Sun et al. (2020) Qiming Sun, Xing Zhang, Samragni Banerjee, Peng Bao, Marc Barbry, Nick S Blunt, Nikolay A Bogdanov, George H Booth, Jia Chen, Zhi-Hao Cui, et al. Recent developments in the PySCF program package. _The Journal of Chemical Physics_, 153(2), 2020. doi: https://doi.org/10.1063/5.0006074.
* Szabo and Ostlund (2012) Attila Szabo and Neil S Ostlund. _Modern Quantum Chemistry: Introduction to Advanced Electronic Structure Theory_. Courier Corporation, 2012.
* Tholke and De Fabritiis (2022) Philipp Tholke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular potentials. In _International Conference on Learning Representations_, 2022.
* Thomas et al. (2018) Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3D point clouds. _arXiv Preprint, arXiv:1802.08219_, 2018.
* Tran et al. (2023) Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, et al. The open catalyst 2022 (OC22) dataset and challenges for oxide electrocatalysts. _ACS Catalysis_, 13(5):3066-3084, 2023.
* Unke et al. (2021) Oliver Unke, Mihail Bogojeski, Michael Gastegger, Mario Geiger, Tess Smidt, and Klaus-Robert Muller. SE(3)-equivariant prediction of molecular wavefunctions and electronic densities. _Advances in Neural Information Processing Systems_, 34:14434-14447, 2021.
* Wang et al. (2022) Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. ComENet: Towards complete and efficient message passing for 3D molecular graphs. In _The 36th Annual Conference on Neural Information Processing Systems_, pages 650-664, 2022.
* Wang et al. (2009) Yanli Wang, Jewen Xiao, Tugba O Suzek, Jian Zhang, Jiyao Wang, and Stephen H Bryant. PubChem: a public information system for analyzing bioactivities of small molecules. _Nucleic Acids Research_, 37(suppl_2):W623-W633, 2009.
* Wang et al. (2017) Yanli Wang, Stephen H Bryant, Tiejun Cheng, Jiyao Wang, Asta Gindulyte, Benjamin A Shoemaker, Paul A Thiessen, Siqian He, and Jian Zhang. PubChem bioassay: 2017 update. _Nucleic Acids Research_, 45(D1):D955-D963, 2017.
* Weigend and Ahlrichs (2005) Florian Weigend and Reinhart Ahlrichs. Balanced basis sets of split valence, triple zeta valence and quadruple zeta valence quality for h to rn: Design and assessment of accuracy. _Physical Chemistry Chemical Physics_, 7(18):3297-3305, 2005.
* Yan et al. (2022) Keqiang Yan, Yi Liu, Yuchao Lin, and Shuiwang Ji. Periodic graph transformers for crystal material property prediction. In _The 36th Annual Conference on Neural Information Processing Systems_, pages 15066-15080, 2022.
* Zhang et al. (2018)Haiyang Yu, Zhao Xu, Xiaofeng Qian, Xiaoning Qian, and Shuiwang Ji. Efficient and equivariant graph networks for predicting quantum Hamiltonian. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* Zhang et al. (2023) Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Haiyang Yu, YuQing Xie, Xiang Fu, Alex Strasser, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes Stark, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, Alan Aspuru-Guzik, Erik Bekkers, Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Lio, Rose Yu, Stephan Gunnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess Smidt, and Shuiwang Ji. Artificial intelligence for science in quantum, atomistic, and continuum systems. _arXiv preprint arXiv:2307.08423_, 2023.

[MISSING_PAGE_EMPTY:16]

### Test performance over various training sets

To investigate the model performance with various number of examples in training sets, we train the QHNet on datasets with various sizes. As shown in Table 7, we can observe that while the MAE on Hamiltonian matrix and cosine similarity on wavefunction \(\psi\) are in similar level with various training sets, the MAE on occupied eigen energies \(\epsilon\) becomes much worse when the training sets become smaller.