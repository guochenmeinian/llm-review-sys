# Semi-Supervised Sparse Gaussian Classification: Provable Benefits of Unlabeled Data

 Eyar Azar

Weizmann Institute of Science

eyar.azar@weizmann.ac.il

&Boaz Nadler

Weizmann Institute of Science

boaz.nadler@weizmann.ac.il

###### Abstract

The premise of semi-supervised learning (SSL) is that combining labeled and unlabeled data yields significantly more accurate models. Despite empirical successes, the theoretical understanding of SSL is still far from complete. In this work, we study SSL for high dimensional sparse Gaussian classification. To construct an accurate classifier a key task is feature selection, detecting the few variables that separate the two classes. For this SSL setting, we analyze information theoretic lower bounds for accurate feature selection as well as computational lower bounds, assuming the low-degree likelihood hardness conjecture. Our key contribution is the identification of a regime in the problem parameters (dimension, sparsity, number of labeled and unlabeled samples) where SSL is guaranteed to be advantageous for classification. Specifically, there is a regime where it is possible to construct in polynomial time an accurate SSL classifier. However, any computationally efficient supervised or unsupervised learning schemes, that separately use only the labeled or unlabeled data would fail. Our work highlights the provable benefits of combining labeled and unlabeled data for classification and feature selection in high dimensions. We present simulations that complement our theoretical analysis.

## 1 Introduction

The presumption underlying Semi-Supervised Learning (SSL) is that more accurate predictors may be learned by leveraging both labeled and unlabeled data. Over the past 20 years, many SSL methods have been proposed and studied (Chapelle et al., 2006; Zhu & Goldberg, 2009). Indeed, on many datasets SSL yields significant improvements over supervised learning (SL) and over unsupervised learning (UL). However, there are also cases where unlabeled data does not seem to help. A fundamental theoretical issue in SSL is thus to understand under which settings can unlabeled data help to construct more accurate predictors and under which its benefit, if any, is negligible.

To address this issue, SSL was studied theoretically under various models. Several works proved that under a cluster or a manifold assumption, with sufficient unlabeled data, SSL significantly outperforms SL (Rigollet, 2007; Singh et al., 2008). In some cases, however, SSL performs similarly to UL (i.e., clustering, up to a label permutation ambiguity). In addition, Ben-David et al. (2008) described a family of distributions where SSL achieves the same error rate as SL.

In the context of the cluster assumption, a popular model for theoretical analysis is Gaussian classification, in particular binary classification for a mixture of two spherical Gaussians. In this case, the label \(Y\in\{\pm 1\}\) has probabilities \(\mathbb{P}(Y=y)=\pi_{y}\) and conditional on a label value \(y\), the vector \(\bm{x}\in\mathbb{R}^{p}\) follows a Gaussian distribution,

\[\bm{x}|y\sim\mathcal{N}(\bm{\mu}_{y},\mathbf{I}_{p})\] (1)

where \(\bm{\mu}_{1},\bm{\mu}_{-1}\in\mathbb{R}^{p}\) are both unknown. This model and related ones were studied theoretically in supervised, unsupervised and semi-supervised settings, see for example Li et al. (2017); Tifrea et al.

(2023); Wu and Zhou (2021), and references therein. Without assuming structure on the vectors \(\bm{\mu}_{y}\) or on their difference (such as sparsity), there are computationally efficient SL and UL algorithms that achieve the corresponding minimax rates. Moreover, Tifrea et al. (2023) proved that for the model (1), no SSL algorithm simultaneously improves upon the minimax-optimal error rates of SL and UL. In simple words, there do not seem to be major benefits for SSL under the model (1).

In this paper we consider a mixture of two Gaussians in a _sparse_ high dimensional setting. Specifically, we study balanced binary classification with a sparse difference in the class means, which is a specific instance of (1). Here, the joint distribution of a labeled sample \((\bm{x},y)\) is given by

\[y\sim\text{Unif}\{\pm 1\},\ \ \bm{x}|y\sim\mathcal{N}(\bm{\mu}_{y},\mathbf{I}_{ p}).\] (2)

The class means \(\bm{\mu}_{1},\bm{\mu}_{-1}\in\mathbb{R}^{p}\) are unknown, but their difference \(\Delta\bm{\mu}=\bm{\mu}_{1}-\bm{\mu}_{-1}\) is assumed to be \(k\)-sparse, with \(k\ll p\). In a supervised setting, model (2) is closely related to the sparse normal means problem, for which both minimax rates and computationally efficient (thresholding based) algorithms have been developed and analyzed, see e.g. Johnstone (1994, 2002). In an unsupervised setting, inference on the model (2) is closely related to clustering and learning mixtures of Gaussians (Azizyan et al., 2013; Jin and Wang, 2016). A key finding is that in an unsupervised setting with a sparsity assumption, there is a statistical-computational gap (Fan et al., 2018; Loffler et al., 2022). Specifically, from an information viewpoint a number of unlabeled samples \(n\) proportional to \(k\) suffices to accurately cluster and to detect the support of \(\Delta\bm{\mu}\). However, under under various hardness conjectures, unless \(n\propto k^{2}\), no polynomial time algorithm is able to even detect if the data came from one or from two Gaussians (namely, distinguish between \(\Delta\bm{\mu}=\bm{0}\) and \(\|\Delta\bm{\mu}\|=O(1)\)).

In this work we study the model (2) in a SSL setting, given \(L\) labeled samples and \(n\) unlabeled samples, all i.i.d. from (2). Despite extensive works on the SL and UL settings for the model (2), the corresponding SSL setting has received relatively little attention so far. This gives rise to several questions: On the theoretical front, what is the information lower bound for accurate classification and for recovering the support of \(\Delta\bm{\mu}\)? On the computational side, is there a computational-statistical gap in SSL? In addition, are there values of \(L\) and \(n\) for which SSL is provably beneficial as compared to SL and UL separately?

Our Contributions.(i) We derive information theoretic lower bounds for exact support recovery in the SSL setting. As described in Section 2.2, our lower bounds characterize sets of values for the number of labeled and unlabeled samples, where any estimator based on both types of data is unable to recover the support. To derive these bounds, we view SSL as a data fusion problem involving the merging of samples that come from two measurement modalities: the labeled set and the unlabeled set. In Theorem 2.3 we present a general non-asymptotic information-theoretic result for recovering a discrete parameter in this setting. This general result is applicable to other data fusion problems and may thus be of independent interest.

(ii) We present SSL computational lower bounds. These are based on the low-degree likelihood ratio hardness conjecture (Hopkins and Steurer, 2017; Kunisky et al., 2022), in an asymptotic setting where dimension \(p\to\infty\) and a suitable scaling of the sparsity \(k\), and of the number of labeled and unlabeled samples. Our main result is that there is a region of the number of labeled and unlabeled samples, whereby in a SSL setting, accurate classification and feature selection are computationally hard. Our analysis extends to the SSL case previous computational lower bounds that were derived only in UL settings. In particular, if the number of the labeled samples is too small then the statistical-computational gap still remains. To the best of our knowledge, our work is the first to extend this framework to a SSL setting.

(iii) Building upon (i) and (ii), our key contribution is the identification of a region where SSL is provably computationally advantageous for classification and feature selection. Specifically, in Section 3 we develop a polynomial time SSL algorithm, denoted LSPCA, to recover the support of \(\Delta\bm{\mu}\) and consequently construct a linear classifier. We then prove that in a suitable region for the number of labeled and unlabeled samples, LSPCA succeeds in both feature selection and accurate classification. In contrast, under the low degree ratio hardness conjecture, any computationally efficient SL or UL schemes, that use only the labeled or unlabeled data separately, would fail. In Section 4 we show via simulations the superiority of LSPCA, in both support recovery and classification error, in comparison to several SL and UL methods, a self-training SSL scheme and the SSL method of Zhao et al. (2008).

Figure 1 summarizes the picture emerging from our work in combination with previous papers that analyzed the UL and SL settings of (2), namely, the \(x\)-axis and \(y\)-axis in Figure 1. As in prior

works we consider a fixed separation \(\lambda=\|\Delta\bm{\mu}\|_{2}^{2}/4=O(1)\), where \(\Delta\bm{\mu}\) is \(k\)-sparse. The asymptotic setting is that \((k,L,n,p)\) all tend to infinity with the following scaling, which arises naturally for this problem (see Section 2): the number of labeled samples is \(L=\lfloor 2k\beta\log(p)/\lambda\rfloor\), the number of unlabeled samples scales as \(n\propto k^{\gamma}/\lambda^{2}\), and the sparsity scales as \(k\propto p^{\alpha}\) for some \(\alpha\in(0,1/2)\). The figure shows different regions in the \((\gamma,\beta)\) plane, namely as a function of the number of unlabeled and labeled samples, where classification and feature selection are either impossible, hard or computationally easy. We say that classification is impossible if for any classifier there is a \(k\)-sparse vector \(\Delta\bm{\mu}\) whose corresponding accuracy is no better than random. Similarly, we say that feature selection is impossible if for any estimator \(\hat{S}\) of size \(k\) there is a \(k\)-sparse \(\Delta\bm{\mu}\) with support \(S\) such that \(|\hat{S}\cap S|/k\to 0\) as \(p\to\infty\). Feature selection is easy if it is possible to construct in polynomial time a set \(\hat{S}\) of size \(k\) such that \(|\hat{S}\cap S|/k\to 1\). This implies that the corresponding classifier has an excess risk that asymptotically tends to zero. The green region \(\gamma\geq 2\) follows from Deshpande & Montanari (2014), since in this case support estimation is computationally feasible using only the unlabeled data. The region depicted in red is where classification and support recovery are impossible. The impossibility of support recovery follows from Ingster (1997); Donoho & Jin (2004), who proved that support recovery is feasible if and only if \(\beta>1-\alpha\). The same condition holds for classification as well, as described in the supplement. The orange and blue regions in Figure 1 follow from novel results of this paper. In the orange region, defined as \(\beta<1-\alpha\) and \(1<\gamma<2\), our computational lower bound in Theorem 2.6 suggests that any polynomial-time scheme will not succeed in accurate classification. In the blue region, characterized by \(\beta\in(1-\gamma\alpha,1-\alpha)\) and \(1<\gamma<2\), our proposed polynomial time SSL method is guaranteed to construct an accurate classifier. This is proven in Theorem 3.2. In addition, note that in this regime, the availability of unlabeled data allows to decrease the number of labeled samples by a factor of \(\frac{1-\alpha}{1-\gamma\alpha}\). Under the low degree hardness conjecture, in this blue region no computationally efficient SL or UL method that separately analyze the labeled or unlabeled samples, respectively, would succeed. We conjecture that in the remaining white region, no polynomial-time algorithm exists that is able to recover the support or able to construct an accurate classifier. In summary, our work highlights the provable computational benefits of combining labeled and unlabeled data for classification and feature selection in a high dimensional sparse setting.

NotationFor an integer \(p\), we write \([p]=\{1,...,p\}\). The cardinality of a set \(B\) is \(|B|\). For a vector \(\bm{v}\in\mathbb{R}^{p}\), we denote its restriction to a subset \(T\subset[p]\) by \(\bm{v}|_{T}\). For vectors \(\bm{a},\bm{b}\), their inner product is \(\langle\bm{a},\bm{b}\rangle\), and \(\|\bm{a}\|\) denotes the \(\ell_{2}\) norm. We say that \(f(p)=\omega(g(p))\) if for any \(c>0\), there exists \(p_{0}\geq 1\) such that \(f(p)>cg(p)\) for every \(p\geq p_{0}\).

Figure 1: Semi-supervised classification and support recovery regions. The red and green regions follow from previous works. Contributions of our work include identification of the orange and the blue regions.

Theoretical Results

In this section we present our first two contributions, namely an information-theoretic lower bound for exact support recovery of \(\Delta\bm{\mu}\), and a computational lower bound for classification and support recovery, in a SSL setting. To this end, in Section 2.1 we first review lower bounds for SL and UL settings. As we were not able to find these precise results in the literature, for completeness we present their proofs, based on Fano's inequality, in the supplementary. Our main contribution here, described in Section 2.2, is a SSL lower bound. To derive it, we view SSL as a data fusion problem with two types of data (the labeled set and the unlabeled set). The SSL lower bound then follows by a combination of the lower bounds for SL and UL.

To derive lower bounds, it suffices to consider a specific instance of (2), where the two Gaussian means are symmetric around the origin, with \(\bm{\mu}_{1}=-\bm{\mu}_{-1}=\bm{\mu}\). Hence,

\[y\sim\text{Unif}\{\pm 1\},\ \ \bm{x}|y\sim\mathcal{N}(y\bm{\mu},\mathbf{I}_{p}).\] (3)

Here, \(\bm{\mu}\in\mathbb{R}^{p}\) is an unknown \(k\)-sparse vector with \(\ell_{2}\) norm of \(\sqrt{\lambda}\). We denote its support by \(S=\text{supp}(\bm{\mu})=\{i|\mu_{i}\neq 0\}\), and by \(\mathbb{S}\) the set of all \((\frac{\rho}{k})\) possible \(k\)-sparse support sets.We denote by \(\mathcal{D}_{L}=\{(\bm{x}_{i},y_{i})\}_{i=1}^{L}\) and \(\mathcal{D}_{n}=\{\bm{x}_{i}\}_{i=L+1}^{L+n}\) the i.i.d. labeled and the unlabeled datasets, respectively.

To derive information and computational lower bounds for support recovery, it is necessary to impose a lower bound on \(\min_{i\in S}|\mu_{i}|\). As in Amini & Wainwright (2009); Krauthgamer et al. (2015), it suffices to study the set of most difficult \(k\)-sparse vectors with such a lower bound on their entries. In our case this translates to the nonzero entries of \(\bm{\mu}\) belonging to \(\{\pm\sqrt{\lambda/k}\}\). Clearly, if some signal coordinates had magnitudes larger that \(\sqrt{\lambda/k}\), then the problem of detecting them and constructing an accurate classifier would both be easier. Throughout our analysis, we assume \(\bm{\mu}\) is of this form and the sparsity \(k\) is known. All proofs appear in the supplementary.

### Information Lower Bounds (Supervised and Unsupervised)

The next theorem states a non-asymptotic result for exact support recovery in the SL case.

**Theorem 2.1**.: _Fix \(\delta\in(0,1)\). For any \((L,p,k)\) such that_

\[L<\frac{2(1-\delta)k}{\lambda}\log\left(p-k+1\right),\] (4)

_and for any support estimator \(\hat{S}\) based on \(\mathcal{D}_{L}\), it follows that \(\max_{S\in\mathbb{S}}\mathbb{P}\left(\hat{S}\neq S\right)>\delta-\frac{\log 2 }{\log(p-k+1)}\)._

Donoho & Jin (2004) proved a similar result in an asymptotic regime. Specifically, they proved that for \(k=p^{\alpha}\) and \(L=\frac{2\beta k}{\lambda}\log p\), approximate support recovery is possible if and only if \(\beta>1-\alpha\). Theorem 2.1 states a stronger non-asymptotic result for exact support recovery. It implies that even if \(\beta>1-\alpha\), it is still impossible to recover the exact support with probability tending to one if \(\beta<1\).

Next we present an information lower bound for exact support recovery in UL. Here we observe \(n\) vectors \(\bm{x}_{i}\) from (3) but not their labels \(y_{i}\).

**Theorem 2.2**.: _Fix \(\delta\in(0,1)\). For any \((n,p,k)\to\infty\) with \(\frac{k}{p}\to 0\) and_

\[n<\frac{2(1-\delta)k}{\lambda^{2}}\log\left(p-k+1\right)\max\left\{1,\lambda \right\},\] (5)

_for any support estimator \(\hat{S}\) based on \(\mathcal{D}_{n}\), as \(p\to\infty\), then \(\max_{S\in\mathbb{S}}\mathbb{P}\left(\hat{S}\neq S\right)\geq\delta\)._

The scaling in Eq. (5) appeared in several prior works on related problems. Azizyan et al. (2013) showed that for \(\lambda<1\), with number of samples \(n<\frac{k}{\lambda^{2}}\log(p/k)\), no clustering method can achieve accuracy better than random. Verzelen & Arias-Castro (2017) studied hypothesis testing whether the data came from a single Gaussian or from a mixture of two Gaussians. In Proposition 3 of their paper, they proved that for \(n\leq\frac{k}{\lambda^{2}}\log(\frac{ep}{k})\max\{1,\lambda\}\), any testing procedure is asymptotically powerless. Note that for \(k=p^{\alpha}\) with \(\alpha<1\), the lower bound derived in Verzelen & Arias-Castro (2017) has a similar form to (5) with a factor of \(1-\alpha\), which is slightly smaller. Thus the bound in (5) is sharper.

### Semi-Supervised Setting

In the SSL case, the observed data consists of two subsets, one with \(L\) labeled samples and the other with \(n\) unlabeled ones. We now develop information-theoretic and computational lower bounds for this setting. The information lower bound is based on the results in Section 2.1 for SL and UL settings. The computational lower bound relies on the low-degree likelihood hardness conjecture. Over the past 10 years, several authors studied statistical-computational gaps for various high dimensional problems. For the sparse Gaussian mixture (3) both Fan et al. (2018) and Loffler et al. (2022) derived such gaps in an UL setting. To the best of our knowledge, our work is amongst the first to explore computational-statistical gaps in a SSL setting. Our analysis, described below, shows that with relatively few labeled samples, the computational statistical gap continues to hold. In contrast, as we describe in Section 3, with a sufficiently large number of labeled samples, but not enough so solve the problem using only the labeled set, the computational-statistical gap is resolved. In particular, we present a polynomial time SSL algorithm that bridges this gap.

Information Lower Bounds.Before presenting results for the mixture model (3), we analyze a more general case. We study the recovery of a latent variable \(S\) that belongs to a large finite set \(\mathbb{S}\), given measurements from two different modalities. Formally, the problem is to recover \(S\) from two independent sets of samples \(\{\bm{x}_{i}\}_{i=1}^{N}\) and \(\{\bm{z}_{j}\}_{j=1}^{M}\) of the following form,

\[\{\bm{x}_{i}\}_{i=1}^{N}\sim f_{x}(\bm{x}|S),\quad\{\bm{z}_{j}\}_{j=1}^{M} \sim g_{z}(\bm{z}|S).\] (6)

Here, \(f_{x}(\bm{x}|S)\) and \(g_{z}(\bm{z}|S)\) are known probability density functions. These functions encode information on \(S\) from the two types of measurements. In our SSL setting, \(\bm{x}\) represents an unlabeled sample, whereas \(\bm{z}=(\bm{x},y)\) a labeled one, and \(S\) is the unknown support of \(\bm{\mu}\).

Our goal is to derive information lower bounds for this problem. To this end, we assume that \(S\) is a random variable uniformly distributed over a finite set \(\mathbb{S}\), and denote by \(I_{x}=I(\bm{x};S)\) and \(I_{z}=I(\bm{z};S)\) the mutual information of \(\bm{x}\) with \(S\) and of \(\bm{z}\) with \(S\), respectively. Further, recall a classical result in information theory that to recover \(S\) from \(N\) i.i.d. samples of \(\bm{x}\), \(N\) must scale at least linearly with \(\frac{\log|\mathbb{S}|}{I_{x}}\). A similar argument applies to \(\bm{z}\). For further details, see Scarlett & Cevher (2021). The following theorem states a general non-asymptotic information-theoretic result for recovering \(S\) from the above two sets of samples. Hence, it is applicable to other problems involving data fusion from multiple sources and may thus be of independent interest.

**Theorem 2.3**.: _Fix \(\delta\in(0,1)\). Let \(N,M\) be integers that satisfy \(\max\left\{N\cdot I_{x},\,M\cdot I_{z}\right\}<(1-\delta)\log|\mathbb{S}|\). Let \(N_{q}=\lfloor qN\rfloor\) and \(M_{q}=\lfloor(1-q)M\rfloor\), for \(q\in[0,1]\). Then, any estimator \(\hat{S}\) based on \(\{\bm{x}_{i}\}_{i=1}^{N_{q}}\) and \(\{\bm{z}_{j}\}_{j=1}^{M_{q}}\) satisfies_

\[\mathbb{P}\left(\hat{S}\neq S\right)>\delta-\frac{\log 2}{\log|\mathbb{S}|}.\]

This theorem implies that with any convex combination of samples from the two modalities, \(qN\) from the first and \((1-q)M\) from the second, accurate recovery of \(S\) is not possible if \(N\) and \(M\) are both too small. Essentially this follows from the additivity of mutual information.

Combining Theorem 2.3 with the proofs of Theorems 2.1 and 2.2 yields the following information lower bound for the semi-supervised case.

**Corollary 2.4**.: _Let \(\mathcal{D}_{L}\) and \(\mathcal{D}_{n}\) be sets of \(L\) and \(n\) i.i.d. labeled and unlabeled samples from the mixture model (3). Fix \(\delta\in(0,1)\). Let \((L_{0},n_{0},p,k)\to\infty\), with \(\frac{k}{p}\to 0\), be such that_

\[L_{0}<\frac{2(1-\delta)k}{\lambda}\log\left(p-k+1\right),\,n_{0}<\frac{2(1- \delta)k}{\lambda^{2}}\log\left(p-k+1\right)\max\left\{1,\lambda\right\}.\]

_Suppose the number of labeled and unlabeled samples satisfy \(L=\lfloor qL_{0}\rfloor\) and \(n=\lfloor(1-q)n_{0}\rfloor\) for some \(q\in[0,1]\). Then, for any estimator \(\hat{S}\) based on \(\mathcal{D}_{L}\cup\mathcal{D}_{n}\), as \(p\to\infty\)_

\[\max_{S\in\mathbb{S}}\mathbb{P}\left(\hat{S}\neq S\right)\geq\delta.\]Computational Lower Bounds.Our SSL computational lower bound is based on the low-degree framework, and its associated hardness conjecture (Hopkins and Steurer, 2017; Kunisky et al., 2022). This framework was used to derive computational lower bounds for various unsupervised high dimensional problems including sparse-PCA and sparse Gaussian mixture models (Loffler et al., 2022; Schramm and Wein, 2022; Ding et al., 2023). To the best of our knowledge, our work is the first to adapt this framework to a SSL setting.

For our paper to be self-contained, we first briefly describe this framework and its hardness conjecture. We then present its adaptation to our SSL setting. The low degree likelihood framework focuses on unsupervised _detection_ problems, specifically the ability to distinguish between two distributions \(\mathbb{P}\) and \(\mathbb{Q}\), given \(n\) i.i.d. samples. Specifically, denote the null distribution of \(n\) samples by \(\mathbb{Q}_{n}\), whereby all \(\bm{x}_{i}\sim\mathbb{Q}\), and denote by \(\mathbb{P}_{n}\) the alternative distribution, with all \(\bm{x}_{i}\sim\mathbb{P}\).

Under the low-degree framework, one analyzes how well can the distributions \(\mathbb{P}_{n}\) and \(\mathbb{Q}_{n}\) be distinguished by a low-degree multivariate polynomial \(f:\mathbb{R}^{p\times n}\to\mathbb{R}\). The idea is to construct a polynomial \(f\) which attains large values for data from \(\mathbb{P}_{n}\) and small values for data from \(\mathbb{Q}_{n}\). Specifically, the following metric plays a key role in this framework,

\[\|\mathcal{L}_{n}^{\leq D}\|:=\max_{\text{deg}(f)\leq D}\frac{\mathbb{E}_{X \sim\mathbb{P}_{n}}\left[f(X)\right]}{\sqrt{\mathbb{E}_{X\sim\mathbb{Q}_{n}} \left[f(X)\right]}},\] (7)

where the maximum is over polynomials \(f\) of degree at most \(D\). The value \(\|\mathcal{L}_{n}^{\leq D}\|\) characterizes how well degree-\(D\) polynomials can distinguish \(\mathbb{P}_{n}\) from \(\mathbb{Q}_{n}\). If \(\|\mathcal{L}_{n}^{\leq D}\|=O(1)\), then \(\mathbb{P}_{n}\) and \(\mathbb{Q}_{n}\) cannot be distinguished via a degree-\(D\) polynomial. Computational hardness results that use the low-degree framework are based on the following conjecture, which we here state informally, and refer the reader to Loffler et al. (2022) for its precise statement.

_Conjecture 2.5_ (Informal).: Let \(\mathbb{Q}_{n}\) and \(\mathbb{P}_{n}\) be two distinct distributions. Suppose that there exists \(D=\omega(\log(pn))\) for which \(\|\mathcal{L}_{n}^{\leq D}\|\) remains bounded as \(p\to\infty\). Then, there is no polynomial-time test \(T:\mathbb{R}^{p\times n}\to\{0,1\}\) that satisfies

\[\mathbb{E}_{X\sim\mathbb{P}_{n}}\left[T\left(X\right)\right]+\mathbb{E}_{X \sim\mathbb{Q}_{n}}\left[1-T\left(X\right)\right]=o(1).\]

In simple words, Conjecture 2.5 states that if \(\|\mathcal{L}_{n}^{\leq D}\|=O(1)\) as \(p\to\infty\), then it is not possible to distinguish between \(\mathbb{P}_{n}\) and \(\mathbb{Q}_{n}\) using a polynomial-time algorithm, as no test has both a low false alarm as well as a low mis-detection rate (the two terms in the equation above).

We now show how to extend this framework, focused on unsupervised detection, to our SSL setting. To this end, consider \(L+n\) samples, distributed according to either a null distribution \(\mathbb{Q}_{L+n}\) or an alternative distribution \(\mathbb{P}_{L+n}\). In our case, the null distribution is

\[\mathbb{Q}_{L+n}:\quad\bm{x}_{i}=\bm{\xi}_{i}\sim\mathcal{N}(0,\mathbb{I}_{p }),\ i\in[L+n],\] (8)

whereas the alternative belongs to the following set of distributions,

\[\mathbb{P}_{L+n}:\quad\begin{cases}\bm{x}_{i}=\bm{\mu}^{S}+\bm{\xi}_{i},\quad i \in[L],\\ \bm{x}_{i}=y_{i}\bm{\mu}^{S}+\bm{\xi}_{i},\ L<i\leq L+n.\end{cases}\] (9)

Here, \(S\) is uniformly distributed over \(\mathbb{S}\), \(\bm{\mu}^{S}(j)=\sqrt{\frac{\lambda}{k}}\mathbbm{1}\{j\in S\}\), and \(y_{i}\) are unobserved Rademacher random variables.

The next theorem presents a low-degree bound for our SSL testing problem. The scalings of \(L,n\) and \(k\) with \(p\) and \(\lambda\) are motivated by those appearing in Theorems 2.1 and 2.2.

**Theorem 2.6**.: _Let \(k=\lfloor c_{1}\rho^{\alpha}\rfloor,L=\lfloor\frac{2\beta k}{\lambda}\log(p-k)\rfloor\), \(n=\lfloor c_{2}\frac{k^{\gamma}}{\lambda^{2}}\rfloor\) and \(D=(\log p)^{2}\), for some \(\beta,\gamma,\lambda,c_{1},c_{2}\in\mathbb{R}_{+}\) and \(\alpha\in(0,\frac{1}{2})\). With the null and alternative distributions defined in (8) and (9), if \(\beta<\frac{1}{2}-\alpha\) and \(\gamma<2\), then as \(p\to\infty\)_

\[\|\mathcal{L}_{L+n}^{\leq D}\|^{2}=O(1).\]

Theorem 2.6 together with the hardness conjecture 2.5 extends to the SSL case previous computational lower bounds that were derived only in UL settings (\(\beta=0\)) (Fan et al., 2018; Loffler et al., 2022). Next, we make several remarks regarding the theorem.

**SSL statistical-computational gap.** In the rectangular region \(\beta<\frac{1}{2}-\alpha\) and \(1<\gamma<2\), depicted in orange in Figure 1, under the hardness conjecture 2.5, distinguishing between \(\mathbb{P}\) and \(\mathbb{Q}\) is computationally hard. Since testing is easier than variable selection and classification (Verzelen & Arias-Castro, 2017; Fan et al., 2018), in this region these tasks are computationally hard as well.

**Tightness of condition \(\gamma<2\) in Theorem 2.6.** This condition is sharp, since for \(\gamma\geq 2\), namely \(n\gtrsim\frac{k^{2}}{\lambda^{2}}\), the support can be recovered by a polynomial-time algorithm, such as thresholding the covariance matrix followed by PCA, see Deshpande & Montanari (2014) and Krauthgamer et al. (2015).

**Tightness of condition \(\beta<\frac{1}{2}-\alpha\).** This condition is tight for detection, though not necessarily for feature selection or classification. The reason is that for \(\beta>\frac{1}{2}-\alpha\), it is possible to distinguish between \(\mathbb{P}\) and \(\mathbb{Q}\), using only the labeled data (Ingster, 1997; Donoho & Jin, 2004).

Combining Theorems 2.1-2.6 leaves a rectangular region \(1<\gamma<2\) and \(\frac{1}{2}-\alpha<\beta<1-\alpha\) where SSL support recovery is feasible from an information view, but we do not know if it possible in a computationally efficient manner. In the next section we present a polynomial time SSL method that in part of this rectangle, depicted in blue in Figure 1, is guaranteed to recover \(S\) and construct an accurate classifier. We conclude with the following conjecture regarding the remaining white region:

_Conjecture 2.7_.: Let \(\mathcal{D}_{L},\mathcal{D}_{n}\) be sets of \(L\) and \(n\) i.i.d. labeled and unlabeled samples from the model (3). Assume, as in Theorem 2.6 that \(k\propto p^{\alpha},L=\lfloor\frac{2\beta k}{\lambda}\log(p-k)\rfloor\) and \(n\propto\frac{k^{\gamma}}{\lambda^{2}}\). Then in the white region depicted in Figure 1, no polynomial-time algorithm is able to recover the support \(S\) or to construct an accurate classifier.

## 3 Semi-Supervised Learning Scheme

We present a SSL scheme, denoted LSPCA, for the model (2), that is simple and has polynomial runtime. In subsection 3.2 we prove that in the blue region of Figure 1 it recovers the support, and thus constructs an accurate classifier. In this region, under the hardness conjecture 2.5, computationally efficient algorithms that rely solely on either labeled or unlabeled data would fail.

Preliminaries.To motivate the derivation of LSPCA, we first briefly review some properties of the sparse model (2). First, note that the covariance matrix of \(\bm{x}\) is \(\Sigma_{x}=\frac{1}{4}\Delta\bm{\mu}\Delta\bm{\mu}^{\top}+\mathbf{I}_{p}\). This is a rank-one spiked covariance model, whose leading eigenvector is \(\Delta\bm{\mu}\), up to a \(\pm\) sign. Hence, with enough unlabeled data, \(\Delta\bm{\mu}\) can be estimated by vanilla PCA on the sample covariance or by some sparse-PCA procedure taking advantage of the sparsity of \(\Delta\bm{\mu}\). Unfortunately, in high dimensions with a limited number of samples, these procedures may output quite inaccurate estimates, see for example Nadler (2008); Birnbaum et al. (2013). The main idea of our approach is to run these procedures after an initial variable screening step that uses the labeled data to reduce the dimension.

### The LSPCA Scheme

Our SSL scheme, denoted LSPCA, stands for Label Screening PCA. As described in Algorithm 1, LSPCA has two input parameters: the sparsity \(k\) and a variable screening factor \(\tilde{\beta}<1\). The scheme consists of two main steps: (i) removal of noise variables using the labeled samples; (ii) support estimation from the remaining variables using the unlabeled samples via PCA. Finally, a linear classifier is constructed via the leading eigenvector of the covariance matrix on the estimated support.

The first stage screens variables using only the labeled samples. While our setting is different, this stage is similar in spirit to Sure Independence Screening (SIS), which was developed for high-dimensional regression (Fan & Lv, 2008). To this end, our scheme first constructs the vector,

\[\bm{w}_{L}=\frac{1}{L_{+}}\sum_{i:y_{i}=1}\bm{x}_{i}-\frac{1}{L_{-}}\sum_{i:y_ {i}=-1}\bm{x}_{i},\] (10)

where \(L_{+}=\left|\{i\in[L]:y_{i}=1\}\right|\) and \(L_{-}=L-L_{+}\). With a balanced mixture \(\mathbb{P}(Y=\pm 1)=\frac{1}{2}\), it follows that \(\bm{w}_{L}\approx\Delta\bm{\mu}+\frac{2}{\sqrt{L}}N(\bm{0},\mathbf{I}_{p})\). Hence, \(\bm{w}_{L}\) can be viewed as a noisy estimate of \(\Delta\bm{\mu}\). If the number of labeled samples were large enough, then the top \(k\) coordinates of \(\bm{w}_{L}\) would coincide with the support of \(\Delta\bm{\mu}\). With few labeled samples, while not necessarily the top-\(k\), the entries of \(\bm{w}_{L}\) atthe support indices still have relative large magnitudes. Given the input parameter \(\tilde{\beta}>0\), the scheme retains the indices that correspond to the largest \(p^{1-\tilde{\beta}}\) entries in absolute value of \(\bm{w}_{L}\). We denote this set by \(S_{L}\). Note that for any \(\tilde{\beta}>0\), this step significantly reduces the dimension (as \(\tilde{\beta}>0\) then \(p^{1-\tilde{\beta}}\ll p\)). In addition, as analyzed theoretically in the next section, for some parameter regimes, this step retains in \(S_{L}\) (nearly all of) the \(k\) support indices. These two properties are essential for the success of the second stage, which we now describe.

The second step estimates the support \(S\) using the unlabeled data. Specifically, LSPCA constructs the sample covariance matrix restricted to the set \(S_{L}\),

\[\hat{\Sigma}|_{S_{L}}=\frac{1}{n}\sum_{i=L+1}^{n+L}(\bm{x}_{i}- \bar{\bm{x}})|_{S_{L}}(\bm{x}_{i}-\bar{\bm{x}})|_{S_{L}}^{\top},\] (11)

where \(\bar{\bm{x}}=\frac{1}{n}\sum_{i=L+1}^{n+L}\bm{x}_{i}\) is the empirical mean of the unlabeled data. Next, it computes the leading eigenvector \(\hat{\bm{v}}_{\text{PCA}}\) of \(\hat{\Sigma}|_{S_{L}}\). The output support set \(\hat{S}\) consists of the \(k\) indices of \(\hat{\bm{v}}_{\text{PCA}}\) with largest magnitude. Finally, the vector \(\Delta\bm{\mu}\) is (up to scaling) estimated by the leading eigenvector of \(\hat{\Sigma}\) restricted to \(\hat{S}\), with its sign determined by the labeled data.

_Remark 3.1_.: After the removal of variables in the first step, the input dimension to the second step is much lower, \(\tilde{p}=p^{1-\tilde{\beta}}\). Despite this reduction in dimension, as long as the vector \(\Delta\bm{\mu}\) is sufficiently sparse with \(k\ll\tilde{p}\), or equivalently \(\alpha<1-\tilde{\beta}\), then in the second step our goal is still to find a sparse eigenvector. Hence, instead of vanilla PCA, we may replace the second step by any suitable (polynomial time) sparse-PCA procedure. We refer to this approach as LS\({}^{2}\)PCA (Labeled Screening Sparse-PCA). As illustrated in the simulations, for finite sample sizes, this can lead to improved support recovery and lower classification errors.

### Support Recovery and Classification Guarantees for LSPCA

Before presenting our main result, we first recall two standard evaluation metrics. The classification error of a classifier \(C:\mathbb{R}^{p}\rightarrow\{\pm 1\}\) is defined as \(\mathcal{R}(C)=\mathbb{P}(C(\bm{x})\neq y)\). Its excess risk is defined as

\[\mathcal{E}(C)=\mathcal{R}(C)-\mathcal{R}^{*}=\mathcal{R}(C)- \inf_{C^{\prime}}\mathcal{R}(C^{\prime}).\] (12)

As in Verzelen & Arias-Castro (2017), the accuracy of a support estimate \(\hat{S}\) is defined by its normalized overlap with the true support, namely \(|\hat{S}\cap S|/k\). To simplify the analysis, we focus on the symmetric setting where \(\bm{\mu}_{1}=-\bm{\mu}_{-1}=\bm{\mu}\). The next theorem presents theoretical guarantees for LSPCA, in terms of support recovery and the excess risk of the corresponding classifier.

**Theorem 3.2**.: _Let \(\mathcal{D}_{L},\mathcal{D}_{n}\) be labeled and unlabeled sets of \(L\) and \(n\) i.i.d. samples from (3) with a \(k\)-sparse \(\bm{\mu}\) whose non-zero entries are \(\pm\sqrt{\lambda/k}\). Suppose that \(k=\lfloor c_{1}p^{\alpha}\rfloor,L=\lfloor\frac{2\beta k}{\lambda}\log(p-k) \rfloor,n=\lfloor c_{2}\frac{k^{\gamma}}{\lambda^{2}}\rfloor\), for some fixed \(0<\alpha<1/2,0<\beta<1-\alpha,\gamma>1\) and \(\lambda,c_{1},c_{2}\in\mathbb{R}_{+}\). Let \(\hat{S},\hat{\bm{v}}\) be the output of Algorithm 1 with input \(k\) and screening factor \(\tilde{\beta}\). If \(\beta>1-\gamma\alpha\) and \(\tilde{\beta}\in(1-\gamma\alpha,\beta)\), then_

\[\lim_{p\rightarrow\infty}\mathbb{P}\left(\tfrac{|S\cap\hat{S}|}{k}\geq 1- \epsilon\right)=1,\quad\forall\epsilon>0,\] (13)_and the excess risk of the corresponding classifier \(C(\bm{x})=\operatorname{sign}\langle\hat{\bm{\mathbf{v}}},\bm{x}\rangle\), satisfies_

\[\lim_{p\to\infty}\mathcal{E}(C)=0.\] (14)

The interesting region where Theorem 3.2 provides a non-trivial recovery guarantee is the triangle depicted in blue in Figure 1. Indeed, in this region, LSPCA recovers the support and constructs an accurate classifier. In contrast, any SL algorithm would fail, and under the low degree hardness conjecture, any computationally efficient UL scheme would fail as well. To the best of our knowledge, our work is the first to rigorously prove the computational benefits of SSL, in bridging the computational-statistical gap in high dimensions. As mentioned above, we conjecture that in the remaining white region in Figure 1, it is not possible to construct in polynomial time an accurate SSL classifier. The intuition underlying this conjecture is based on the work of Donoho & Jin (2004), where in the fully supervised (SL) setting, the authors show that there is a detection-recovery gap. Namely for a range of number of labeled samples, it is possible to detect that a sparse signal is present, but it is not possible to reliably recover its support. Intuitively, adding a few unlabeled samples should not resolve this gap.

## 4 Simulation Results

We illustrate via several simulations some of our theoretical findings. Specifically, we compare LSPCA and LS2PCA to various SL, UL and SSL schemes, in terms of both accuracy of support recovery and classification error. The sparse PCA method used in LS2PCA was iteratively proxy update (IPU) (Tian et al., 2020). We generate \(L+n\) labeled and unlabeled samples according to the model (3) with a \(k\)-sparse \(\bm{\mu}\) whose non-zero entries are \(\pm\sqrt{\lambda/k}\). The quality of a support estimate \(\hat{S}\) is measured by its normalized accuracy \(|\hat{S}\cap S|/k\). For all methods compared we assume the sparsity \(k\) is known. Hence, each method outputs a \(k-\)sparse unit norm vector \(\hat{\bm{\bm{\mu}}}\), so its corresponding linear classifier is \(\bm{x}\mapsto\operatorname{sign}\langle\hat{\bm{\mu}},\bm{x}\rangle\). Given the model (3), its generalization error is \(\Phi^{c}(\langle\hat{\bm{\mu}},\bm{\mu}\rangle)\). We run our SSL schemes with \(\tilde{\beta}=\beta-(\beta-(1-\gamma\alpha))/4\) which satisfies the requirements of Theorem 3.2. We present experiments with \(p=10^{5},k=p^{0.4}=100\) and \(\lambda=3\), though the behavior is similar for other settings as well. The error of the Bayes classifier is \(\Phi^{c}(\sqrt{\lambda})\approx 0.042\). We report the average (with \(\pm 1\) standard deviation) of the support recovery accuracy and the classification error over \(M=50\) random realizations. All experiments were run on a Intel i7 CPU 2.10 GHz.

We empirically evaluate the benefit of \(L=200\) labeled samples in addition to \(n\) unlabeled ones. We compare our SSL schemes LSPCA and LS2PCA to the following UL methods, taking all \(L+n\) samples as unlabeled: ASPCA (Birnbaum et al., 2013), and IPU (Tian et al., 2020). The SSL methods that we compare are LSDF (Zhao et al., 2008), and self-training (self-train). The self-training algorithm is similar to the approach in Oymak & Gulcu (2021), but explicitly accounts for the known sparsity \(k\): (i) compute \(\bm{w}_{L}\) of (10) using the labeled samples, and keep its \(k\) largest entries, denote the result by \(\bm{w}_{L}^{(k)}\); (ii) compute the dot products \(c_{i}=\langle\bm{w}_{L}^{(k)},\bm{x}_{i}\rangle\) and the pseudo labels \(\tilde{y}_{i}=\operatorname{sign}(c_{i})\); (iii) let \(n_{\text{eff}}\) be the cardinality of the set \(\{i:|c_{i}|>\Gamma\}\), for some threshold value \(\Gamma\geq 0\); (iv) estimate the support by the top-\(k\) coordinates in absolute value of the following vector:

\[\bm{w}_{\text{self}}=\frac{1}{L+n_{\text{eff}}}\left(\sum_{i=1}^{L}y_{i}\bm{ x}_{i}+\sum_{i=L+1}^{L+n}\mathbbm{1}\{|c_{i}|>\Gamma\}\tilde{y}_{i}\bm{x}_{i}\right)\]

In the experiments we used \(\Gamma=0.8\), which gave the best performance. Also, we implemented the SL scheme Top-K Labeled that selects the indices of the top-\(k\) entries of \(|\bm{w}_{L}|\) of (10). As shown in the supplementary, this is the maximum-likelihood estimator for \(S\) based on the labeled data.

Figure 2 illustrates our key theoretical result - that in certain cases SSL can yield accurate classification and feature selection where SL and UL simultaneously fail. The left panel of Figure 2 shows the average accuracies of support estimation for the different schemes as a function of number of unlabeled samples \(n\). Except at small values of \(n\), LS2PCA achieved the best accuracy out of all methods compared. The right panel shows the classification errors of the different methods. The black horizontal line is the error of the Bayes optimal (Oracle) classifier. As seen in the figure, our SSL schemes come close to the Bayes error while SL and UL schemes have much higher errors.

We present further experiments that empirically illustrate the benefit of using a fixed number of \(n=1000\) unlabeled samples while varying the number of labeled samples \(L\). Specifically, we compare our SSL algorithms LSPCA and LS\({}^{2}\)PCA to the SSL methods self-train and LSDF, as well as to the SL scheme Top-K Labeled, which uses only the \(L\) labeled samples. Figure 3 illustrates the support recovery accuracies and the classification error as a function of the number of labeled samples \(L\). As seen in the figure, adding \(n=1000\) unlabeled samples significantly improves the classification and support recovery accuracies.

## 5 Summary and Discussion

In this work, we analyzed classification of a mixture of two Gaussians in a sparse high dimensional setting. Our analysis highlighted provable computational benefits of SSL. Two notable limitations of our work are that we studied a mixture of only two components, both of which are spherical Gaussians. It is thus of interest to extend our analysis to more components and to other distributions.

From a broader perspective, many SSL methods for feature selection have been proposed and shown empirically to be beneficial, see for example the review by (Sheikhpour et al., 2017). An interesting open problem is to theoretically prove their benefits, over purely SL or UL. In particular it would be interesting to find cases where SSL improves over both SL and UL in its error rates, not only computationally.

Figure 3: Empirical simulation results. (Left) Support recovery, (Right) Classification error.

Figure 2: Empirical simulation results. (Left) Support recovery, (Right) Classification error.

## Acknowledgements

The research of B.N. was partially supported by ISF grant 2362/22. B.N. is an incumbent of the William Petschek professorial chair of mathematics. We thank Gil Kur and Jonathan Weed for interesting discussions. We also thank the anonymous reviewers for their valuable feedback that improved the quality of our manuscript.

## References

* 2921, 2009.
* Azizyan et al. (2013) Azizyan, M., Singh, A., and Wasserman, L. Minimax theory for high-dimensional Gaussian mixtures with sparse mean separation. _Advances in Neural Information Processing Systems_, 26, 2013.
* Ben-David et al. (2008) Ben-David, S., Lu, T., and Pal, D. Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning. In _COLT_, pp. 33-44, 2008.
* Birnbaum et al. (2013) Birnbaum, A., Johnstone, I. M., Nadler, B., and Paul, D. Minimax bounds for sparse pca with noisy high-dimensional data. _Annals of Statistics_, 41(3):1055-1084, 2013.
* Chapelle et al. (2006) Chapelle, O., Scholkopf, B., and Zien, A. _Semi-Supervised Learning_. MIT press, 2006.
* 507, 1952.
* Cover & Thomas (2006) Cover, T. M. and Thomas, J. A. _Elements of Information Theory (Second Edition)_. Wiley-Interscience, 2006.
* Deshpande & Montanari (2014) Deshpande, Y. and Montanari, A. Sparse pca via covariance thresholding. _Advances in Neural Information Processing Systems_, 27, 2014.
* Ding et al. (2023) Ding, Y., Kunisky, D., Wein, A. S., and Bandeira, A. S. Subexponential-time algorithms for sparse pca. _Foundations of Computational Mathematics_, pp. 1-50, 2023.
* 994, 2004.
* Fan & Lv (2008) Fan, J. and Lv, J. Sure Independence Screening for Ultrahigh Dimensional Feature Space. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 70(5):849-911, 2008.
* Fan et al. (2018) Fan, J., Liu, H., Wang, Z., and Yang, Z. Curse of heterogeneity: Computational barriers in sparse mixture models and phase retrieval. _arXiv preprint arXiv:1808.06996_, 2018.
* Hopkins & Steurer (2017) Hopkins, S. B. and Steurer, D. Efficient Bayesian Estimation from Few Samples: Community Detection and Related Problems. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pp. 379-390, 2017.
* Ingster (1997) Ingster, Y. I. Some problems of hypothesis testing leading to infinitely divisible distributions. _Math. Methods Statist._, 6(1):47-69, 1997.
* Jin (2009) Jin, J. Impossibility of successful classification when useful features are rare and weak. _Proceedings of the National Academy of Sciences_, 106(22):8859-8864, 2009.
* 2359, 2016.
* Johnson et al. (2005) Johnson, N. L., Kemp, A. W., and Kotz, S. _Univariate discrete distributions_, volume 444. John Wiley & Sons, 2005.
* 289, 1994.
* Johnstone (2002) Johnstone, I. M. Function estimation and Gaussian sequence models. _Unpublished_, 2002.
* Johnstone (2003)Krauthgamer, R., Nadler, B., and Vilenchik, D. Do semidefinite relaxations solve sparse PCA up to the information limit? _Annals of Statistics_, 43(3):1300-1322, 2015.
* Kunisky et al. (2022) Kunisky, D., Wein, A. S., and Bandeira, A. S. Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio. In Cerejeiras, P. and Reissig, M. (eds.), _Mathematical Analysis, its Applications and Computation_, pp. 1-50. Springer, 2022.
* Li et al. (2017) Li, T., Yi, X., Caramanis, C., and Ravikumar, P. Minimax Gaussian classification & clustering. In _Artificial Intelligence and Statistics_, pp. 1-9. PMLR, 2017.
* Loffler et al. (2022) Loffler, M., Wein, A. S., and Bandeira, A. S. Computationally efficient sparse clustering. _Information and Inference: A Journal of the IMA_, 11(4):1255-1286, 2022.
* Nadler (2008) Nadler, B. Finite sample approximation results for principal component analysis: A matrix perturbation approach. _Annals of Statistics_, 36(6):2791-2817, 2008.
* Oymak & Gulcu (2021) Oymak, S. and Gulcu, T. C. A theoretical characterization of semi-supervised learning with self-training for Gaussian mixture models. In _International Conference on Artificial Intelligence and Statistics_, pp. 3601-3609, 2021.
* Rigollet (2007) Rigollet, P. Generalization error bounds in semi-supervised classification under the cluster assumption. _Journal of Machine Learning Research_, 8(7), 2007.
* Scarlett & Cevher (2021) Scarlett, J. and Cevher, V. An introductory guide to Fano's inequality with applications in statistical estimation. In Rodrigues, M. R. D. and Eldar, Y. C. (eds.), _Information-Theoretic Methods in Data Science_, pp. 487-528. Cambridge University Press, 2021.
* Schramm & Wein (2022) Schramm, T. and Wein, A. S. Computational barriers to estimation from low-degree polynomials. _Annals of Statistics_, 50(3):1833-1858, 2022.
* Sheikhpour et al. (2017) Sheikhpour, R., Sarram, M. A., Gharaghani, S., and Chahooki, M. A. Z. A survey on semi-supervised feature selection methods. _Pattern Recognition_, 64:141-158, 2017.
* Singh et al. (2008) Singh, A., Nowak, R., and Zhu, J. Unlabeled data: Now it helps, now it doesn't. _Advances in neural information processing systems_, 21, 2008.
* Tian et al. (2020) Tian, L., Nie, F., Wang, R., and Li, X. Learning feature sparse principal subspace. _Advances in neural information processing systems_, 33:14997-15008, 2020.
* Tifrea et al. (2023) Tifrea, A., Yuce, G., Sanyal, A., and Yang, F. Can semi-supervised learning use all the data effectively? a lower bound perspective. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Verzelen & Arias-Castro (2017) Verzelen, N. and Arias-Castro, E. Detection and feature selection in sparse mixture models. _Annals of Statistics_, 45(5):1920-1950, 2017.
* Wu & Zhou (2021) Wu, Y. and Zhou, H. H. Randomly initialized EM algorithm for two-component Gaussian mixture achieves near optimality in \(O(\sqrt{n})\) iterations. _Mathematical Statistics and Learning_, 4(3), 2021.
* Yang & Barron (1999) Yang, Y. and Barron, A. Information-theoretic determination of minimax rates of convergence. _Annals of Statistics_, 27:1564-1599, 1999.
* Zhao et al. (2008) Zhao, J., Lu, K., and He, X. Locality sensitive semi-supervised feature selection. _Neurocomputing_, 71(10-12):1842-1849, 2008.
* Zhu & Goldberg (2009) Zhu, X. and Goldberg, A. B. _Introduction to semi-supervised learning_. Morgan & Claypool Publishers, 2009.

Auxiliary Lemmas

We first present several auxiliary lemmas used to prove our theorems. We denote the complement of the standard normal cumulative distribution function by \(\Phi^{c}(t)=\mathbb{P}\left(Z>t\right)\), where \(Z\sim\mathcal{N}(0,1)\). The following lemma states a well known upper bound on \(\Phi^{c}\).

**Lemma A.1**.: _For any \(t>1\),_

\[\Phi^{c}(t)\leq\frac{1}{\sqrt{2\pi t}}e^{-t^{2}/2}.\] (15)

**Lemma A.2** (Chernoff (1952)).: _Suppose \(\{x_{i}\}_{i=1}^{n}\) are i.i.d. Bernoulli random variables, with \(\Pr[x_{i}=1]=q\). Let \(X\) denote their sum. Then, for any \(\delta\geq 0\),_

\[\mathbb{P}\left(X\geq(1+\delta)nq\right)\leq e^{-\frac{\delta^{2} nq}{2+\delta}},\] (16)

_and for any \(\delta\in[0,1]\)_

\[\mathbb{P}\left(X\leq(1-\delta)nq\right)\leq e^{-\frac{\delta^{2 }nq}{2}}.\] (17)

A common approach to prove lower bounds is using Fano's inequality. Here, we use the following version of Fano's lemma, see Yang & Barron (1999).

**Lemma A.3**.: _Let \(\theta\) be a random variable uniformly distributed over a finite set \(\Theta\). Let \(\bm{z}_{1},\bm{z}_{2},\ldots,\bm{z}_{n}\) be \(n\) i.i.d. samples from a density \(f(\bm{z}|\theta)\). Then, for any estimator \(\hat{\theta}(\bm{z}_{1},\ldots,\bm{z}_{n})\in\Theta\),_

\[\mathbb{P}\left(\hat{\theta}\neq\theta\right)\geq 1-\frac{I( \theta;Z^{n})+\log 2}{\log|\Theta|},\] (18)

_where \(I(\theta;Z^{n})\) is the mutual information between \(\theta\) and the samples \(Z^{n}=(\bm{z}_{1},\bm{z}_{2},\ldots,\bm{z}_{n})\)._

In our proofs we use several well known properties of the entropy function. For convenience we here state some of them. First, we recall the explicit expression for the entropy of a multivariate Gaussian.

**Lemma A.4**.: _Let \(\bm{x}\sim\mathcal{N}(\bm{\mu},\Sigma)\). Then, its entropy is given by_

\[H(\bm{x})=\frac{p}{2}(1+\log(2\pi))+\frac{1}{2}\log\text{det}( \Sigma).\] (19)

Next, the following lemma states that the multivariate Gaussian distribution maximizes the entropy over all continuous distributions with the same covariance (Cover & Thomas, 2006, pg. 254).

**Lemma A.5**.: _Let \(\bm{x}\) be a continuous random variable with mean \(\bm{\mu}\in\mathbb{R}^{p}\) and covariance \(\Sigma\in\mathbb{R}^{p\times p}\), and let \(\bm{y}\sim\mathcal{N}(\bm{\mu},\Sigma)\). If the support of \(\bm{x}\) is all of \(\mathbb{R}^{p}\) then_

\[H(\bm{x})\leq H(\bm{y}).\] (20)

The next lemma states the sub-additive property of the entropy function.

**Lemma A.6**.: _Let \(\bm{x}\) and \(\bm{y}\) be jointly distributed random variables. Then,_

\[H(\bm{x},\bm{y})\leq H(\bm{x})+H(\bm{y}).\] (21)

To prove Theorem 2.2 we use the following lemma.

**Lemma A.7**.: _Let \(\lambda\in(0,1)\), and let \(k\) be a positive integer. Then, for \(w\sim N(0,\frac{k-1}{k})\)_

\[\mathbb{E}\left[\tanh(\lambda+\sqrt{\lambda}w)-\frac{1}{2}\tanh^{ 2}\left(\lambda+\sqrt{\lambda}w\right)\right]\leq\frac{1}{2}\left(\lambda+3 \sqrt{\lambda/k}\right)\] (22)

Proof of Lemma a.7.: Let \(q(w)=\tanh(\lambda+\sqrt{\lambda}w)-\tanh^{2}(\lambda+\sqrt{\lambda}w)\). In terms of \(q(w)\), the left hand side of (22) may be written as follows

\[\mathbb{E}\left[\tanh(\lambda+\sqrt{\lambda}w)-\frac{1}{2}\tanh^{ 2}\left(\lambda+\sqrt{\lambda}w\right)\right]=\frac{1}{2}\left(\mathbb{E} \left[\tanh(\lambda+\sqrt{\lambda}w)\right]+\mathbb{E}\left[q(w)\right] \right).\] (23)We now upper bound the two terms in the RHS of (23). We start by showing that \(\mathbb{E}\left[q(w)\right]\leq 3\sqrt{\lambda/k}\). Let \(L_{q}=\max_{w\in\mathbb{R}}|q^{\prime}(w)|\) be the Lipschitz constant of the function \(q(w)\). It is easy to show that \(L_{q}\leq 3\sqrt{\lambda}\). Let \(z\sim\mathcal{N}(0,1)\) be independent of \(w\). Using the first order Taylor expansion yields

\[\mathbb{E}_{w,z}\left[q\left(w\right)-q\left(w+\frac{1}{\sqrt{k}}z\right) \right]\leq L_{q}\frac{1}{\sqrt{k}}\,\mathbb{E}_{z}\left[|z|\right]=3\sqrt{ \frac{\lambda}{k}}\sqrt{\frac{2}{\pi}}\leq 3\sqrt{\lambda/k}.\] (24)

Next, note that \((w+\frac{1}{\sqrt{k}}z)\sim N(0,1)\). Therefore

\[\mathbb{E}_{w,z}\left[q\left(w+\frac{1}{\sqrt{k}}z\right)\right] =\int_{-\infty}^{\infty}q(v)\frac{e^{-v^{2}/2}}{\sqrt{2\pi}}dv\] \[=\int_{-\infty}^{\infty}\left(\tanh(\lambda+\sqrt{\lambda}v)- \tanh^{2}(\lambda+\sqrt{\lambda}v)\right)\frac{e^{-v^{2}/2}}{\sqrt{2\pi}}dv.\]

Making a change of variables \(x=\lambda+\sqrt{\lambda}v\), gives

\[\mathbb{E}_{w,z}\left[q\left(w+\frac{1}{\sqrt{k}}z\right)\right]=\int_{- \infty}^{\infty}\left(\tanh(x)-\tanh^{2}(x)\right)\frac{e^{-(x-\lambda)^{2}/2 \lambda}}{\sqrt{2\pi\lambda}}dx.\]

Define \(t_{\lambda}(x)=\left(\tanh(x)-\tanh^{2}(x)\right)\frac{e^{-(x-\lambda)^{2}/2 \lambda}}{\sqrt{2\pi\lambda}}\). Note that \(t_{\lambda}(x)\) is an absolutely integrable function which satisfies \(t_{\lambda}(x)=-t_{\lambda}(-x)\) for any \(\lambda\). Therefore, the above integral is equal to zero, namely \(\int_{-\infty}^{\infty}t_{\lambda}(x)dx=0\). Inserting this into (24) gives

\[\mathbb{E}_{w}\left[q\left(w\right)\right]\leq 3\sqrt{\lambda/k}.\] (25)

Next, we upper bound the first term on the RHS of Eq. (23). Denote by \(f_{W}(w)\) the probability density function of \(w\sim\mathcal{N}(0,\frac{k-1}{k})\). Then, writing the expectation explicitly gives

\[\mathbb{E}\left[\tanh(\lambda+\sqrt{\lambda}w)\right]=\int_{-\infty}^{\infty} \tanh(\lambda+\sqrt{\lambda}w)f_{W}(w)dw.\]

Making the change of variables \(x=\lambda+\sqrt{\lambda}w\) yields

\[\mathbb{E}\left[\tanh(\lambda+\sqrt{\lambda}w)\right] = \frac{1}{\sqrt{\lambda}}\int_{-\infty}^{\infty}\tanh(x)f_{W} \left(\frac{x-\lambda}{\sqrt{\lambda}}\right)dx\] \[= \frac{1}{\sqrt{\lambda}}\int_{0}^{\infty}\tanh(x)\left(f_{W} \left(\frac{x-\lambda}{\sqrt{\lambda}}\right)-f_{W}\left(\frac{x+\lambda}{ \sqrt{\lambda}}\right)\right)dx.\]

Since \(\lambda>0\), it follows that \(f_{W}\left(\frac{x-\lambda}{\sqrt{\lambda}}\right)-f_{W}\left(\frac{x+\lambda }{\sqrt{\lambda}}\right)\geq 0\), for all \(x\geq 0\). Therefore, to bound this expectation it suffices to construct a function \(g(x)\) such that \(g(x)\geq\tanh(x)\) for any \(x\geq 0\) and for which we can compute explicitly the corresponding integral. Consider the function \(g(x)=x\). It is well-known that \(x\geq\tanh(x)\), for all \(x\geq 0\). Thus,

\[\mathbb{E}\left[\tanh(\lambda+\sqrt{\lambda}w)\right]\leq\frac{1}{\sqrt{ \lambda}}\int_{0}^{\infty}x\left(f_{W}\left(\frac{x-\lambda}{\sqrt{\lambda}} \right)-f_{W}\left(\frac{x+\lambda}{\sqrt{\lambda}}\right)\right)dx=\frac{1}{ \sqrt{\lambda}}\int_{-\infty}^{\infty}xf_{W}\left(\frac{x-\lambda}{\sqrt{ \lambda}}\right)dx\]

Substituting \(w=(x-\lambda)/\sqrt{\lambda}\), yields

\[\mathbb{E}\left[\tanh(\lambda+\sqrt{\lambda}w)\right]\leq\int_{-\infty}^{ \infty}(\lambda+\sqrt{\lambda}w)f_{W}(w)dw=\mathbb{E}\left[\lambda+\sqrt{ \lambda}w\right]=\lambda.\] (26)

Combining (23),(25) and (26) gives

\[\mathbb{E}\left[\tanh(\lambda+\sqrt{\lambda}w)-\frac{1}{2}\tanh^{2}\left( \lambda+\sqrt{\lambda}w\right)\right]\leq\frac{1}{2}\left(\lambda+3\sqrt{ \lambda/k}\right).\]

\(\Box\)

**Lemma A.8**.: _Consider a sequence of classification problem of the form,_

\[y\sim\text{Unif}\{\pm 1\},\;\;\bm{x}|y\sim N(y\bm{\mu},\mathbf{I}_{p}),\]

_where the dimension \(p\to\infty\) but \(\|\bm{\mu}\|=\sqrt{\lambda}\) is fixed. Let \(\hat{\bm{\mu}}_{p}\) be a sequence of unit norm estimators and \(T_{p}(\bm{x})=\operatorname{sign}(\hat{\bm{\mu}}_{p},\bm{x})\) the corresponding classifier. Assume that every \(\epsilon>0\)_

\[\lim_{p\to\infty}\mathbb{P}(\langle\bm{\mu}/\sqrt{\lambda},\,\hat{\bm{\mu}}_{ p}\rangle>1-\epsilon)=1.\] (27)

_Then, the excess risk of the classifier \(T_{p}(\bm{x})=\operatorname{sign}(\hat{\bm{\mu}}_{p},\bm{x})\) tends to zero as \(p\) tends to infinity._

Proof.: By definition, the excess risk of the classifier \(T_{p}\) that corresponds to \(\hat{\bm{\mu}}_{p}\) can be written as

\[\mathcal{E}(T_{p})=\mathbb{P}_{\bm{\xi}\sim N(0,\mathbf{I}_{p})}(\langle\hat{ \bm{\mu}}_{p},\bm{\mu}+\bm{\xi}\rangle<0)-\Phi^{c}(\sqrt{\lambda}).\]

Since \(\bm{\xi}\) is independent of \(\hat{\bm{\mu}}_{p}\) and \(\hat{\bm{\mu}}_{p}\) has unit norm, then \(z=\langle\hat{\bm{\mu}}_{p},\bm{\xi}\rangle\sim\mathcal{N}(0,1)\), and the excess risk may be written as

\[\mathcal{E}(T_{p})=\mathcal{P}(z>\langle\hat{\bm{\mu}}_{p},\bm{\mu}\rangle)- \Phi^{c}(\sqrt{\lambda})\]

Let \(\epsilon>0\), and consider the event \(\mathcal{A}_{\epsilon}=\{\langle\bm{\mu}/\sqrt{\lambda},\,\hat{\bm{\mu}}_{p} \rangle>1-\epsilon\}\). Then,

\[\mathcal{E}(T_{p})\leq\left\{\begin{array}{ll}\Phi^{c}(\sqrt{\lambda}(1- \epsilon))-\Phi^{c}(\sqrt{\lambda})&\text{with probability }\mathcal{P}(\mathcal{A}_{ \epsilon})\\ 1&\text{with probability }1-P(\mathcal{A}_{\epsilon})\end{array}\right.\]

Since by assumption \(\mathcal{P}(\mathcal{A}_{\epsilon})\to 1\) for any \(\epsilon>0\), then the excess risk tends to zero as \(p\to\infty\). 

## Appendix B Lower Bounds - Proofs of Results in Section 2

### Proof of Theorem 2.1

Our proof relies on Fano's inequality, and is conceptually similar to the proof of Theorem 3 in Amini & Wainwright (2009). First, note that for any sub-collection \(\hat{\mathbb{S}}\subset\mathbb{S}\), we have the following

\[\max_{S\in\mathbb{S}}\mathbb{P}\left(\hat{S}\neq S\right)\geq\frac{1}{|\hat{ \mathbb{S}}|}\sum_{S\in\hat{\mathbb{S}}}\mathbb{P}\left(\hat{S}\neq S\right).\]

The right hand side in the above display is the error probability of an estimator \(\hat{S}\), where \(S\) is considered as a random variable uniformly distributed over the set \(\hat{\mathbb{S}}\). In other words, the right hand side may be written as follows,

\[\mathbb{P}\left(\text{error}\right)=\sum_{s\in\hat{\mathbb{S}}}\mathbb{P} \left(\hat{S}\neq s\,|\,S=s\right)\cdot\mathbb{P}\left(S=s\right).\]

In our proof, we consider the following sub-collection

\[\hat{\mathbb{S}}:=\left\{T\in\mathbb{S}:1,\ldots,k-1\in T\right\},\] (28)

which consists of all \(k\)-element subsets that contain the first \(k-1\) support indices \(\{1,\ldots,k-1\}\) and one from \(\{k,\ldots,p\}\). To lower bound the probability of error, we focus on a specific class of means: given the support \(S\), the mean entries have the form \(\mu_{j}=\sqrt{\frac{\lambda}{k}}\mathbbm{1}\{j\in S\}\). So, with \(S\) known, \(\bm{\mu}\) is deterministic and we write it as \(\bm{\mu}^{S}\).

In the proof we consider an equivalent model of (3), whose observations are divided by \(\sqrt{\lambda}\),

\[\bm{x}_{i}=y_{i}\bm{\theta}^{S}+\sigma\bm{\xi}_{i},\quad i=1,\ldots,L,\]

where \(\bm{\theta}^{S}(j)=\frac{1}{\sqrt{k}}\mathbbm{1}\{j\in S\},\sigma=\frac{1}{ \sqrt{\lambda}}\) and \(\bm{\xi}_{i}\sim\mathcal{N}(0,\mathbf{I}_{p})\). Since for each observation \(\bm{x}_{i}\) we also know its corresponding label \(y_{i}\in\{-1,1\}\), we may consider the following transformed observations

\[\tilde{\bm{x}}_{i}=y_{i}\bm{x}_{i}=\bm{\theta}^{S}+\sigma\tilde{\bm{\xi}}_{i}, \quad i=1,\ldots,L.\] (29)where \(\bm{\xi}_{i}=y_{i}\bm{\xi}_{i}\) has the same distribution as \(\bm{\xi}_{i}\). Denote by \(X^{L},\tilde{X}^{L},Y^{L}\) the sets of \(L\) i.i.d. samples \(\{\bm{x}_{i}\}_{i=1}^{L},\{\tilde{\bm{x}}_{i}\}_{i=1}^{L}\) and \(\{y_{i}\}_{i=1}^{L}\), respectively. To apply Fano's lemma, we consider the joint mutual information \(I\left(\left(X^{L},Y^{L}\right);S\right)\). Note that,

\[I\left(\left(X^{L},Y^{L}\right);S\right)=I\left(\left(\tilde{X}^{L},Y^{L} \right);S\right)=I\left(\tilde{X}^{L};S\right),\]

where the last equality follows from the fact that the labels \(Y^{L}\) are independent of the support \(S\) and the observations \(\tilde{X}^{L}\). Hence, it is enough to consider the samples \(\tilde{X}^{L}\) from the model in (29).

Let \(S\) be a subset chosen uniformly at random from \(\tilde{\mathrm{S}}\). Then, from Lemma A.3, it follows that

\[\mathbb{P}\left(\text{error}\right)\geq 1-\frac{I(\tilde{X}^{L};S)+\log 2}{\log| \tilde{\mathrm{S}}|}=1-\frac{I(\tilde{X}^{L};S)+\log 2}{\log(p-k+1)}.\] (30)

We now derive an upper bound on \(I(\tilde{X}^{L};S)\). First, from the relation between mutual information and conditional entropy, \(I(\tilde{X}^{L};S)=H(\tilde{X}^{L})-H(\tilde{X}^{L}|S)\). By the sub-additivity of the entropy function \(H(\tilde{X}^{L})\leq LH(\tilde{\bm{x}})\). Also, since the samples \(\tilde{\bm{x}}_{i}\) are conditionally independent given \(S\), the joint entropy \(H(\tilde{X}^{L}|S)\) can be expressed as

\[H(\tilde{X}^{L}|S)=\sum_{i\in[L]}H(\tilde{\bm{x}}_{i}|S)=LH(\tilde{\bm{x}}|S).\]

where \(\tilde{\bm{x}}\) is a single observation from the model (29). Therefore,

\[I(\tilde{X}^{L};S)\leq L\left(H(\tilde{\bm{x}})-H(\tilde{\bm{x}}|S)\right).\] (31)

By the definition of conditional entropy,

\[H(\tilde{\bm{x}}|S)=-\sum_{s\in\tilde{\mathbb{S}}}P(S=s)\int f\left(\tilde{ \bm{x}}|S\right)\log f(\tilde{\bm{x}}|S)d\tilde{\bm{x}},\]

where \(f(\tilde{\bm{x}}|S)\) the probability density function of a single random sample \(\tilde{\bm{x}}\) given \(S\). For any \(S\in\tilde{\mathbb{S}}\), the vector \((\tilde{\bm{x}}\,|\,S)\) is a \(p\)-dimensional Gaussian with mean \(\bm{\theta}^{S}\) and covariance matrix \(\sigma^{2}\mathbf{I}_{p}\). Its entropy is independent of its mean, and is given by \(\frac{p}{2}\left(1+\log(2\pi\sigma^{2})\right)\). Hence,

\[H(\tilde{\bm{x}}|S)=\frac{p}{2}\left(1+\log(2\pi)+\log(\sigma^{2})\right).\] (32)

The final step is to upper bound \(H(\tilde{\bm{x}})\). To this end, note that \(\tilde{\bm{x}}\) is distributed as a mixture of \((p-k+1)\) Gaussians, each centered at \(\bm{\theta}^{S}\) for \(S\in\tilde{\mathbb{S}}\). Let us denote its mean and covariance by \(\bm{\nu}_{x}=\mathbb{E}\left[\tilde{\bm{x}}\right]\) and \(\Sigma=\mathbb{E}\left[(\tilde{\bm{x}}-\bm{\nu}_{x})(\tilde{\bm{x}}-\bm{\nu}_ {x})^{T}\right]\), respectively. By the maximum entropy property of the Gaussian distribution (Lemma A.5), and Eq. (19) for the entropy of a multivariate Gaussian, we have

\[H(\tilde{\bm{x}})\leq H(\mathcal{N}(\bm{\nu}_{x},\Sigma))=\frac{p}{2}\left(1+ \log(2\pi)\right)+\frac{1}{2}\log\text{det}\left(\Sigma\right).\] (33)

Combining (31), (32) and (33) gives

\[I(\tilde{X}^{L};S)\leq\frac{L}{2}\left(\log\det(\Sigma)-p\log\sigma^{2}\right).\] (34)

The following lemma, proved in Appendix D, provides an upper bound for \(\log\det(\Sigma)\).

**Lemma B.1**.: _Let \(\Sigma\) be the covariance matrix of the random vector \(\tilde{\bm{x}}\) of Eq. (29), with the set \(S\) uniformly distributed on \(\tilde{\mathbb{S}}\) of Eq. (28). Then,_

\[\log\text{det}\left(\Sigma\right)\leq p\log(\sigma^{2})+(p-k+1)\log\left(1+ \frac{1}{k(p-k+1)\sigma^{2}}\right).\] (35)

Substituting this upper bound into (34) leads to

\[I(\tilde{X}^{L};S)=\frac{L}{2}(p-k+1)\log\left(1+\frac{1}{k(p-k+1)\sigma^{2}} \right)\leq\frac{L}{2k\sigma^{2}}=\frac{L\lambda}{2k}\] (36)

where the last inequality follows from \(\log(1+x)\leq x\), for all \(x>0\). Inserting (36) into (30), implies that a sufficient condition for the error probability to be greater than \(\delta-\frac{\log 2}{\log(p-k+1)}\) is \(\frac{L\lambda}{k}<2(1-\delta)\log(p-k+1)\), which completes the proof.

### Proof of Theorem 2.2

To prove Theorem 2.2 we use the following lemma, proven in Appendix D.

**Lemma B.2**.: _Let \(\bm{x}\) be a random vector from the model (3), with a vector \(\bm{\mu}^{S}\), where the random variable \(S\) is uniformly distributed over the set \(\tilde{\mathbb{S}}\) of Eq. (28), and let \(I(\bm{x};S)\) be their mutual information. Consider an asymptotic setting where \(p\to\infty\) and \(k/p\to\infty\). Then, for \(\lambda<1\), and for \(p\) and \(k\) sufficiently large with \(k/p\) sufficiently small_

\[I(\bm{x};S)\leq\frac{1}{2}\frac{\lambda^{2}}{k}(1+o(1)).\] (37)

First, note that for \(\lambda\geq 1\), the information lower bounds proven in Theorem \(2.1\) and those we aim to prove in Theorem \(2.2\) coincide. Clearly, Theorem 2.1 which considers all possible estimators based on \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{L}\), includes in particular all unsupervised estimators that ignore the labels. Hence, for \(\lambda\geq 1\), Theorem 2.2 follows from Theorem 2.1. Therefore, we consider the case \(\lambda<1\).

The proof, similar to that of Theorem \(1\), is also based on Fano's inequality. To lower bound the probability of error, we view \(S\) as a subset uniformly distributed over \(\tilde{\mathbb{S}}\), where \(\tilde{\mathbb{S}}\) is the sub-collection of support sets defined in (28). Then, using the same arguments as in the proof of Theorem \(1\),

\[\max_{S\in\mathbb{S}}\mathbb{P}\left(\hat{S}\neq S\right)\geq\frac{1}{|\tilde {\mathbb{S}}|}\sum_{S\in\tilde{\mathbb{S}}}\mathbb{P}\left(\hat{S}\neq S \right)\geq 1-\frac{I(X^{n};S)+\log 2}{\log|\tilde{\mathbb{S}}|},\] (38)

where \(X^{n}=(\bm{x}_{1},...,\bm{x}_{n})\) and \(I(X^{n};S)\) is the mutual information between the \(n\) unlabeled samples and \(S\).

We now derive an upper bound for \(I(X^{n};S)\). The sub-additivity of the entropy function, and the fact that \(\bm{x}_{i}\) are conditionally independent given \(S=s\), imply

\[I(X^{n};S)\leq n\left(H(\bm{x})-H(\bm{x}|S)\right)=nI(\bm{x};S),\] (39)

where \(\bm{x}\) is a single sample from the model (3). Hence, for \(p\) and \(k\) sufficiently large, with \(k/p\) sufficiently small, combining (37) and (39) gives \(I(X^{n};S)\leq\frac{n\lambda^{2}}{2k}(1+o(1))\). By Fano's bound in (38), the error probability is at least \(\delta\) if \(n<\frac{2(1-\delta)k}{\lambda^{2}}\log(p-k+1)\).

### Proof of Theorem 2.3

Recall that the random variable \(S\) is uniformly distributed over the set \(\mathbb{S}\). For \(X^{N_{\alpha}}=\{\bm{x}_{i}\}_{i=1}^{N_{\alpha}}\) and \(Z^{M_{\alpha}}=\{\bm{z}_{i}\}_{i=1}^{M_{\alpha}}\), their combined mutual information with the random variable \(S\) is

\[I(X^{N_{\alpha}},Z^{M_{\alpha}};S)=H(X^{N_{\alpha}},Z^{M_{\alpha}})-H(X^{N_{ \alpha}},Z^{M_{\alpha}}|S).\]

Since the two sets of samples \(\{\bm{x}_{i}\}_{i=1}^{N_{\alpha}}\), \(\{\bm{z}_{j}\}_{j=1}^{M_{\alpha}}\) are conditionally independent given \(S=s\), then

\[I(X^{N_{\alpha}},Z^{M_{\alpha}};S) =H(X^{N_{\alpha}},Z^{M_{\alpha}})-H(X^{N_{\alpha}}|S)-H(Z^{M_{ \alpha}}|S)\] \[=H(X^{N_{\alpha}},Z^{M_{\alpha}})-N_{\alpha}H(x|S)-M_{\alpha}H(z| S).\] (40)

By the sub-additivity property of the entropy function,

\[H(X^{N_{\alpha}},Z^{M_{\alpha}})\leq H(X^{N_{\alpha}})+H(Z^{M_{\alpha}})\leq N _{\alpha}H(x)+M_{\alpha}H(z).\] (41)

Hence, combining (40) and (41) yields

\[I(X^{N_{\alpha}},Z^{M_{\alpha}};S)\leq N_{\alpha}\cdot I_{x}+M_{\alpha}\cdot I _{z}.\] (42)

Combining Fano's inequality with the upper bound in (42) gives

\[\mathbb{P}\left(\hat{S}\neq S\right)\geq 1-\frac{N_{\alpha}\cdot I_{x}+M_{ \alpha}\cdot I_{z}+\log 2}{\log|\mathbb{S}|}.\]

Finally, the assumption \(\max\{N\cdot I_{\bm{x}},\,M\cdot I_{\bm{z}}\}<(1-\delta)\log|\mathbb{S}|\) yields that \(\mathbb{P}\left(\hat{S}\neq S\right)>\delta-\frac{\log 2}{\log|\mathbb{S}|}\)\(\Box\)

### Proof of Corollary 2.4

To lower bound the error probability, we view \(S\) as a random variable uniformly distributed over the discrete set \(\tilde{\mathbb{S}}\) defined in (28). By the same arguments as in the proofs of Theorems 1 and 2,

\[\max_{S\in\mathbb{S}}\mathbb{P}\left(\hat{S}\neq S\right)\geq\mathbb{P}_{S \sim U(\tilde{\mathbb{S}})}\left(\hat{S}\neq S\right).\]

We apply Theorem 2.3, with the set \(\bm{x}_{i}\) of i.i.d. unlabeled samples from (3), and the second set \(z_{i}=(\bm{x}_{i},y_{i})\) of i.i.d. labeled samples from model (3).

Next, in the proofs of Theorems 2.1 and 2.2 the following upper bounds for \(I_{\bm{z}}\) and \(I_{\bm{x}}\) were derived,

\[I_{\bm{z}}\leq\frac{\lambda}{2k},\quad I_{\bm{x}}\leq\frac{\lambda}{2k}\min\{1,\lambda\}(1+o(1)).\] (43)

Finally, by the conditions of the Corollary, the number of labeled and unlabeled samples satisfy \(L=\lfloor qL_{0}\rfloor\) and \(n=\lfloor(1-q)n_{0}\rfloor\), with \(L_{0}<\frac{2(1-\delta)k}{\lambda}\log\left(p-k+1\right)\) and \(n_{0}<\frac{2(1-\delta)k}{\lambda^{2}}\log\left(p-k+1\right)\max\{1,\lambda\}\). Hence, for sufficient large \(p\), combining these conditions with (43) gives

\[L_{0}\cdot I_{\bm{z}},\,n_{0}\cdot I_{\bm{x}}\leq(1-\delta)\log(p-k+1)\]

Therefore, by Theorem 2.3 the error probability is at least \(\delta\). 

### Proof of Theorem 2.6

Even though we analyze a SSL setting, the observed data still belongs to the additive Gaussian noise model (see Section 2 in Kunisky et al. (2022)). This key point allows to simplify the low-degree norm in our setting. Specifically, let \(Z=(\bm{z}_{1},\ldots,\bm{z}_{L+n})\) be the following set of random vectors, which are the noise-free underlying signals from the alternative \(\mathbb{P}_{L+n}\) of Eq. (9) in the main text,

\[\bm{z}_{i}=\begin{cases}\bm{\mu}^{S},\quad i\in[L],\\ y_{i}\bm{\mu}^{S},\ L<i\leq L+n.\end{cases}\]

In the equation above, \(S\) is uniformly distributed on \(\mathbb{S}\) (the set of all size-\(k\) subsets over \(p\) variables), \(\bm{\mu}^{S}\) is a \(k\)-sparse vector with support \(S\) and non-zero entries \(\sqrt{\lambda/k}\) and \(y_{i}\) are Rademacher random variables. Similarly, let \(\tilde{Z}=(\tilde{\bm{z}}_{1},\ldots,\tilde{\bm{z}}_{L+n})\) be an independent set of the underlying noise-free signals, with a possibly different support \(\tilde{S}\), and independent labels \(\tilde{y}_{i}\). Then, by Theorem 1 in Kunisky et al. (2022) the low degree norm \(\|\mathcal{L}_{L+n}^{D}\|^{2}\) can be expressed as

\[\|\mathcal{L}_{L+n}^{D}\|^{2}=\mathbb{E}_{Z,\tilde{Z}}\left[\sum_{d=0}^{D} \frac{1}{d!}\left(\sum_{i=1}^{L+n}\langle\bm{z}_{i},\tilde{\bm{z}}_{i}\rangle \right)^{d}\right].\]

Inserting the expressions for \(\bm{z}_{i}\) and \(\tilde{\bm{z}}_{i}\) into the equation above, gives

\[\|\mathcal{L}_{L+n}^{D}\|^{2}=\sum_{d=0}^{D}\frac{1}{d!}\,\mathbb{E}\left[ \left(\sum_{i=1}^{L}\left\langle\bm{\mu}^{S},\bm{\mu}^{\tilde{S}}\right\rangle +\sum_{i=L+1}^{L+n}\left\langle y_{i}\bm{\mu}^{S},\tilde{y}_{i}\bm{\mu}^{ \tilde{S}}\right\rangle\right)^{d}\right],\]

where the expectation is over the two random sets \(S,\tilde{S}\) and over the random labels \(y_{i}\) and \(\tilde{y}_{i}\). Since all these random variables are independent, the right hand side above simplifies to

\[\sum_{d=0}^{D}\frac{1}{d!}\,\mathbb{E}\left[\left(L\left\langle \bm{\mu}^{S},\bm{\mu}^{\tilde{S}}\right\rangle+\sum_{i=L+1}^{L+n}y_{i}\tilde{y }_{i}\left\langle\bm{\mu}^{S},\bm{\mu}^{\tilde{S}}\right\rangle\right)^{d}\right]\] \[= \sum_{d=0}^{D}\frac{1}{d!}\,\mathbb{E}\left[\left(\left\langle \bm{\mu}^{S},\bm{\mu}^{\tilde{S}}\right\rangle\left(L+\sum_{i=L+1}^{L+n}y_{i} \tilde{y}_{i}\right)\right)^{d}\right].\]Denote \(R_{i}=y_{i}\tilde{y}_{i}\), and note that \(R_{i}\) are Rademacher random variables, independent of \(S\) and \(\bar{S}\). Thus, the expectation above can be factored into the product of two separate expectations,

\[\|\mathcal{L}_{L+n}^{D}\|^{2}=\sum_{d=0}^{D}\frac{1}{d!}\,\mathbb{E}_{S,\bar{S} }\left[\left\langle\boldsymbol{\mu}^{S},\boldsymbol{\mu}^{\bar{S}}\right\rangle ^{d}\right]\mathbb{E}_{\{R_{j}\}_{j}}\left[\left(L+\sum_{i=L+1}^{L+n}R_{i} \right)^{d}\right].\] (44)

We now separately analyze each of these two expectations, starting from the second one. By the Binomial formula,

\[\mathbb{E}\left[\left(L+\sum_{i=L+1}^{L+n}R_{i}\right)^{d}\right]=\sum_{\ell= 0}^{d}\binom{d}{\ell}L^{d-\ell}\,\mathbb{E}\left[\left(\sum_{i=L+1}^{L+n}R_{i }\right)^{\ell}\right].\]

Note that for any odd integer \(\ell\), the \(\ell\)-th moment of the Rademacher's sum is zero. Therefore,

\[\mathbb{E}\left[\left(L+\sum_{i=L+1}^{L+n}R_{i}\right)^{d}\right]=\sum_{\ell= 0}^{\lfloor d/2\rfloor}\binom{d}{2\ell}L^{d-2\ell}\,\mathbb{E}\left[\left( \sum_{i=L+1}^{L+n}R_{i}\right)^{2\ell}\right].\]

As analyzed in Loffler et al. (2022, pg. 1274)

\[\mathbb{E}\left[\left(\sum_{i=L+1}^{L+n}R_{i}\right)^{2\ell}\right]\leq\,n^{ \ell}\,(2\ell-1)!!,\]

where \((2\ell-1)!!=(2\ell-1)(2\ell-3)\cdots 3\cdot 1=\frac{(2\ell)!}{2^{\ell}\ell!}\). Thus,

\[\mathbb{E}\left[\left(L+\sum_{i=L+1}^{L+n}R_{i}\right)^{d}\right]\leq L^{d} \sum_{\ell=0}^{\lfloor d/2\rfloor}\frac{d!}{(d-2\ell)!\,\ell!}\left(\frac{n}{ 2L^{2}}\right)^{\ell}=L^{d}\sum_{\ell=0}^{\lfloor d/2\rfloor}\binom{d}{\ell} \left(\frac{n}{2L^{2}}\right)^{\ell}\frac{(d-\ell)!}{(d-2\ell)!}.\]

Since \(\frac{(d-\ell)!}{(d-2\ell)!}=(d-\ell)\cdots(d-2\ell+1)\leq d^{\ell}\), then

\[\mathbb{E}\left[\left(L+\sum_{i=L+1}^{L+n}R_{i}\right)^{d}\right] \leq L^{d}\sum_{\ell=0}^{\lfloor d/2\rfloor}\binom{d}{\ell}\left( \frac{n\,d}{2L^{2}}\right)^{\ell}\] \[\leq L^{d}\sum_{\ell=0}^{d}\binom{d}{\ell}\left(\frac{n\,d}{2L^{2 }}\right)^{\ell}=L^{d}\left(1+\frac{nd}{2L^{2}}\right)^{d}=\left(L+\frac{nd}{2 L}\right)^{d}.\] (45)

Next, we analyze the first expectation \(\mathbb{E}_{S,\bar{S}}\left[\langle\boldsymbol{\mu}^{\bar{S}},\boldsymbol{ \mu}^{\bar{S}}\rangle^{d}\right]\) in (44). Recall that \(\mu_{j}^{S}=\sqrt{\frac{\lambda}{k}}\mathbbm{1}\{j\in S\}\). Hence, \(\langle\boldsymbol{\mu}^{S},\boldsymbol{\mu}^{\bar{S}}\rangle=\frac{\lambda}{ k}|S\cap\bar{S}|\). Denote by \(G=|S\cap\bar{S}|\) the size of the overlap between the sets. Then \(G\) is a hypergeometric random variable with the following probability distribution, for \(0\leq m\leq k,\)

\[\mathbb{P}(G=m)=\binom{k}{m}\binom{p-k}{k-m}\binom{p}{k}^{-1}.\]

From (Johnson et al., 2005, pg.268) this probability is upper bounded as follows

\[\mathbb{P}(G=m)\leq\binom{k}{m}\left(\frac{k}{p-k}\right)^{m}.\]

Therefore,

\[\mathbb{E}_{S,\bar{S}}\left[\langle\boldsymbol{\mu}^{\bar{S}},\boldsymbol{\mu }^{\bar{S}}\rangle^{d}\right]=\frac{\lambda^{d}}{k^{d}}\,\mathbb{E}\left[|S \cap\bar{S}|^{d}\right]=\frac{\lambda^{d}}{k^{d}}\sum_{m=0}^{k}m^{d}\,\mathbb{P }(G=m)\leq\frac{\lambda^{d}}{k^{d}}\sum_{m=0}^{k}m^{d}\binom{k}{m}\left(\frac{k} {p-k}\right)^{m}.\] (46)

Inserting (45) and (46) into (44) gives

\[\|\mathcal{L}_{L+n}^{D}\|^{2}\leq\sum_{d=0}^{D}\frac{\lambda^{d}}{d!k^{d}} \left(L+\frac{nd}{2L}\right)^{d}\,\sum_{m=0}^{k}m^{d}\binom{k}{m}\left(\frac{k} {p-k}\right)^{m}.\]In the above expression, since \(d\leq D\), we may upper bound \(nd/2L\) by \(nD/2L\). Furthermore, changing the order of summation between the two sums above, gives

\[\|\mathcal{L}_{L+n}^{D}\|^{2} \leq\sum_{m=0}^{k}\binom{k}{m}\left(\frac{k}{p-k}\right)^{m}\sum_{ d=0}^{D}\frac{1}{d!}\left(m\left(\frac{L\lambda}{k}+\frac{n\lambda D}{2Lk} \right)\right)^{d}\] \[\leq\sum_{m=0}^{k}\binom{k}{m}\left(\frac{k}{p-k}\right)^{m}\exp \left(m\left(\frac{L\lambda}{k}+\frac{n\lambda D}{2Lk}\right)\right).\]

According to the conditions of the Theorem, \(L=\lfloor\frac{2\beta k}{\lambda}\log(p-k)\rfloor\) and \(n=\lfloor c_{2}\frac{k^{\gamma}}{\lambda^{2}}\rfloor\) for some \(c_{2}>0\) and \(\gamma<2\). Hence, for sufficiently large \(p\), \(L>\frac{\beta k}{\lambda}\log(p-k)\). Then, inserting these values into the above gives

\[\|\mathcal{L}_{L+n}^{D}\|^{2}\leq\sum_{m=0}^{k}\binom{k}{m}\left(\frac{k}{p-k }\right)^{m}\exp\left(m\left(2\beta\log(p-k)+\frac{c_{2}k^{\gamma}D}{2\beta k^ {2}\log(p-k)}\right)\right).\]

Setting \(D=(\log(p-k))^{2}\), yields

\[\|\mathcal{L}_{L+n}^{D}\|^{2}\leq\sum_{m=0}^{k}\binom{k}{m}\left(\frac{k}{p-k }\right)^{m}\exp\left(m\log(p-k)\left(2\beta+\frac{c_{2}}{2\beta}\frac{1}{k^{2 -\gamma}}\right)\right).\]

Since \(\gamma<2\), for any fixed \(\epsilon>0\) it follows that \(\frac{c_{2}k^{\gamma}}{2\beta k^{2}}\leq\epsilon\) for sufficiently large \(k\). Therefore

\[\exp\left(m\log(p-k)\left(2\beta+\frac{c_{2}}{2\beta}\frac{1}{k^{2-\gamma}} \right)\right)\leq\exp\left(m\log(p-k)\left(2\beta+\epsilon\right)\right)=(p- k)^{m(2\beta+\epsilon)}\]

Combining the above two displays gives

\[\|\mathcal{L}_{L+n}^{D}\|^{2} \leq\sum_{m=0}^{k}\binom{k}{m}\left(\frac{k}{p-k}\right)^{m}(p-k )^{m(2\beta+\epsilon)}\] \[=\sum_{m=0}^{k}\binom{k}{m}\left(\frac{k}{(p-k)^{1-2\beta- \epsilon}}\right)^{m}=\left(1+\frac{k}{(p-k)^{1-2\beta-\epsilon}}\right)^{k}.\]

Finally, recall that by the assumptions of the theorem, \(k=\lfloor c_{1}p^{\alpha}\rfloor\) for some \(\alpha\in(0,\frac{1}{2}),c_{1}>0\) and that \(\beta<\frac{1}{2}-\alpha\). Choosing \(\epsilon=\frac{1}{2}-\alpha-\beta>0\), gives

\[\|\mathcal{L}_{L+n}^{D}\|^{2}\leq\left(1+\frac{k}{(p-k)^{1/2+\alpha-\beta}} \right)^{k}.\]

Since \(k=\lfloor c_{1}p^{\alpha}\rfloor\), then as \(p\to\infty\), the above behaves as

\[\left(1+\frac{c_{1}}{p^{1/2-\beta}(1+o(1))}\right)^{c_{1}p^{\alpha}}\]

Therefore, for \(\beta<\frac{1}{2}-\alpha\), as \(p\to\infty\), \(\|\mathcal{L}_{L+n}^{D}\|^{2}\to O(1)\). 

## Appendix C SSL Algorithm

### Proof of Theorem 3.2

We prove the theorem, assuming that LSPCA is run with the correct sparsity \(k\) and with a slightly smaller screening factor \(\tilde{\beta}=\beta-\tilde{\epsilon}\) for a fixed (though potentially arbitrarily small) \(\tilde{\epsilon}>0\), which implies the first stage retains a bit more than \(p^{1-\beta}\) of the original \(p\) variables.

Our proof relies on the following two key properties: (i) The set \(S_{L}\) of size \(\tilde{p}=\lceil p^{1-\tilde{\beta}}\rceil\), which is the output of the first step of LSPCA, contains nearly all indices of the true support; (ii) since the reduced dimension \(\tilde{p}\ll n\), the leading eigenvector of PCA is asymptotically consistent, thus allowing recovery nearly all support indices.

The following lemma formally states the first property. Its proof appears in Appendix D.

**Lemma C.1**.: _Let \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{L}\) be \(L\) i.i.d. labeled samples from the mixture model (3) with a vector \(\bm{\mu}\) of sparsity \(k=\lfloor c_{1}p^{\alpha}\rfloor\) and nonzero entries \(\pm\sqrt{\lambda/k}\). Suppose that \(L=\left\lceil\frac{2\beta k\log(p-k)}{\lambda}\right\rceil\), for some \(\beta\in(0,1-\alpha)\). Let \(\tilde{\beta}=\beta-\tilde{\epsilon}\) where \(\tilde{\epsilon}>0\) is sufficiently small so that \(\tilde{\beta}>0\). Let \(S_{L}\) be the indices of the top \(\tilde{p}=\lceil p^{1-\tilde{\beta}}\rceil\) entries of the vector \(\bm{w}_{L}\) of Eq. (10). Then, for any \(\epsilon>0\),_

\[\lim_{p\to\infty}\mathbb{P}\left(\frac{|S\cap S_{L}|}{k}\geq 1-\epsilon \right)=1.\] (47)

Proof of Theorem 3.2.: As described above, we run LSPCA with \(\tilde{\beta}=\beta-\tilde{\epsilon}\), and denote by \(S_{L}\) the set found by the first step of the algorithm. By Lemma C.1 this set satisfies Eq. (47). Denote by \(\Sigma|_{S_{L}}\) and \(\hat{\Sigma}|_{S_{L}}\) the population and sample covariance matrices restricted to the set of indices \(S_{L}\). Note that,

\[\Sigma|_{S_{L}}=\bm{\mu}|_{S_{L}}\bm{\mu}|_{S_{L}}^{\top}+\mathbf{I}_{\tilde{ p}}.\]

Hence, up to sign, the leading eigenvector of \(\Sigma|_{S_{L}}\) is \(\frac{\bm{\mu}|_{S_{L}}}{\left\|\bm{\mu}|_{S_{L}}\right\|}\). Denote by \(\hat{\mathbf{v}}_{\text{PCA}}\) the unit norm leading eigenvector of the sample covariance \(\hat{\Sigma}|_{S_{L}}\). We now show that these two eigenvectors are close to each other. Indeed, since \(\tilde{\beta}>1-\alpha\gamma\), we have \(n/\tilde{p}=p^{\alpha\gamma}/\lambda^{2}(p^{1-\beta^{*}})\to\infty\), as \(p\to\infty\). Then, combining this observation with Theorem 2.3 in (Nadler, 2008), implies that with probability tending to 1,

\[\lim_{p\to\infty}\left|\left\langle\hat{\mathbf{v}}_{\text{PCA}},\,\frac{\bm{ \mu}|_{S_{L}}}{\left\|\bm{\mu}|_{S_{L}}\right\|}\right\rangle\right|=1.\]

Since \(\bm{\mu}\in\mathbb{R}^{p}\) is a \(k\)-sparse with non-zero entries \(\pm\sqrt{\frac{\lambda}{k}}\), Eq. (47) implies that \(\left\|\bm{\mu}|_{S_{L}}\right\|\to\sqrt{\lambda}\), as \(p\to\infty\). Hence,

\[\lim_{p\to\infty}\left|\left\langle\hat{\mathbf{v}}_{\text{PCA}},\,\frac{\bm{ \mu}|_{S_{L}}}{\sqrt{\lambda}}\right\rangle\right|=1,\] (48)

which implies that with the correct sign,

\[\lim_{p\to\infty}\left\|\hat{\mathbf{v}}_{\text{PCA}}-\frac{\bm{\mu}|_{S_{L}} }{\sqrt{\lambda}}\right\|=0.\]

From now we extend \(\hat{\mathbf{v}}_{\text{PCA}}\) which originally had dimension \(|S_{L}\), to a \(p-\)dimensional vector with zeros in \(S_{L}^{c}\). Hence, since \(\left\|\bm{\mu}|_{S_{L}}\right\|\to 0\), it follows

\[\lim_{p\to\infty}\left\|\hat{\mathbf{v}}_{\text{PCA}}-\frac{\bm{\mu}}{\sqrt{ \lambda}}\right\|=0.\] (49)

Next, let us assume by contradiction that there exist \(\epsilon_{0},\delta_{0}\in(0,1)\) such that for every \(p\in\mathbb{N}\), with probability at least \(\delta_{0}\)

\[\frac{|S\cap\hat{S}|}{k}<1-\epsilon_{0}.\]

where \(\hat{S}\) is the set of top-k coordinates of \(\left|\hat{\mathbf{v}}_{\text{PCA}}\right|\). Combining this assumption and Eq. (49), with probability at least \(\delta_{0}\)

\[\lim_{p\to\infty}\left\|\hat{\mathbf{v}}_{\text{PCA}}|_{\hat{S}} \right\|^{2}=\lim_{p\to\infty}\frac{1}{\lambda}\left\|\bm{\mu}|_{\hat{S}} \right\|^{2}=\lim_{p\to\infty}\frac{1}{\lambda}\left\|\bm{\mu}|_{\hat{S}\cap S }\right\|^{2}=\lim_{p\to\infty}\frac{|\hat{S}\cap S|}{k}\leq 1-\epsilon_{0},\] (50)

where the last inequality follows from the above assumption and \(|\mu_{j}|=\sqrt{\lambda/k}\), for all \(j\in S\).

Next, from (47) and (49) it follows that for any subset \(T\) that satisfies \(S_{L}\cap S\subset T\subset S_{L}\) and \(|T|=k\),

\[\lim_{p\to\infty}\left\|\hat{\mathbf{v}}_{\text{PCA}}|_{T}\right\|^{2}=\lim_{p \to\infty}\frac{1}{\lambda}\left\|\bm{\mu}|_{T}\right\|^{2}=\lim_{p\to\infty} \frac{|S\cap S_{L}|}{k}=1.\] (51)

However, since \(\hat{S}\) is the set of the top-\(k\) indices of \(\left|\hat{\mathbf{v}}_{\text{PCA}}\right|\), for any \(|T|=k,T\subset S_{L}\)

\[\left\|\hat{\mathbf{v}}^{\text{PCA}}|_{\hat{S}}\right\|\geq\left\|\hat{ \mathbf{v}}^{\text{PCA}}|_{T}\right\|,\]

which is a contradiction to (50) and (51). Hence, for any \(\epsilon>0\), as \(p\) tends to infinity, \(\mathbb{P}\left(\frac{|S\cap\hat{S}|}{k}\geq 1-\epsilon\right)\to 1\), which completes the first part of the proof.

The second part of the proof follows from combining (48) and Lemma A.8.

Proofs of Additional Lemmas

Proof of Lemma b.1.: First, note that the mean \(\bm{\nu}_{x}=\mathbb{E}\left[\tilde{\bm{x}}\right]=\mathbb{E}_{S}\left[\bm{ \theta}^{S}\right]\) is given by

\[(\nu_{x})_{j}=\begin{cases}\frac{1}{\sqrt{k}}&1\leq j\leq k-1\\ \frac{1}{\sqrt{k}}\frac{1}{p-k+1}&k\leq j\leq p.\end{cases}\]

To derive an explicit expression for the covariance matrix \(\Sigma\) we use the law of total expectation

\[\Sigma=\frac{1}{p-k+1}\sum_{j=k}^{p}\mathbb{E}\left[(\tilde{\bm{x}}-\bm{\nu}_{ x})(\tilde{\bm{x}}-\bm{\nu}_{x})^{\top}\bigm{|}S=s_{j}\right].\] (52)

where \(s_{j}=\left[k-1\right]\cup\{j\}\) is a member of \(\tilde{\mathbb{S}}\). By definition,

\[((\tilde{\bm{x}}-\bm{\nu}_{x})\,|\,S=s_{j})=\left(\bm{\theta}^{S_{j}}-\bm{ \nu}_{x}\right)+\sigma\tilde{\bm{\xi}}=\frac{1}{\sqrt{k}}\bm{e}_{j}-\frac{1}{ \sqrt{k}(p-k+1)}\bm{u}+\sigma\tilde{\bm{\xi}}\] (53)

where \(\bm{u}=\left[\bm{0}_{k-1}^{\top},\,\bm{1}_{p-k+1}^{\top}\right]^{\top}\) and \(\{\bm{e}_{j}\}_{j\in[p]}\) denote the standard basis of \(\mathbb{R}^{p}\). Since \(\tilde{\bm{\xi}}\) is independent of \(S\), inserting (53) into (52) gives

\[\Sigma =\frac{1}{p-k+1}\sum_{j=k}^{p}\mathbb{E}\left[\left(\frac{1}{ \sqrt{k}}\left(\bm{e}_{j}-\frac{1}{(p-k+1)}\bm{u}\right)+\sigma\tilde{\bm{\xi} }\right)\left(\frac{1}{\sqrt{k}}\left(\bm{e}_{j}-\frac{1}{(p-k+1)}\bm{u} \right)+\sigma\tilde{\bm{\xi}}\right)^{\top}\right]\] \[=\frac{1}{k(p-k+1)^{2}}\left((p-k+1)\sum_{j=k}^{p}\bm{e}_{j}\bm{e }_{j}^{T}-\left(\bm{u}\sum_{j=k}^{p}\bm{e}_{j}^{\top}+\sum_{j=k}^{p}\bm{e}_{j} \bm{u}^{\top}\right)+\bm{u}\bm{u}^{\top}\right)+\sigma^{2}\mathbf{I}_{p}\]

Note that \(\sum_{j=k}^{p}\bm{e}_{j}=\bm{u}\). Thus,

\[\Sigma =\frac{1}{k(p-k+1)^{2}}\left((p-k+1)\sum_{j=k}^{p}\bm{e}_{j}\bm{e }_{j}^{T}-\bm{u}\bm{u}^{\top}\right)+\sigma^{2}\mathbf{I}_{p}\] \[\preceq\frac{1}{k(p-k+1)}\sum_{j=k}^{p}\bm{e}_{j}\bm{e}_{j}^{T}+ \sigma^{2}\mathbf{I}_{p}\] \[=\frac{1}{k(p-k+1)}\begin{bmatrix}\bm{0}_{(k-1)\times(k-1)}&\bm{ 0}_{(k-1)\times(p-k+1)}\\ \bm{0}_{(p-k+1)\times(k-1)}&\mathbf{I}_{(\mathbf{p}-\mathbf{k+1})\times( \mathbf{p}-\mathbf{k+1})}\end{bmatrix}+\sigma^{2}\mathbf{I}_{p}.\]

Therefore,

\[\log\det\left(\Sigma\right) \leq\log\left((\sigma^{2})^{p}\left(1+\frac{1}{k(p-k+1)\sigma^{2 }}\right)^{p-k+1}\right)\] \[=p\log\left(\sigma^{2}\right)+(p-k+1)\log\left(1+\frac{1}{k(p-k+ 1)\sigma^{2}}\right).\]

Proof of Lemma b.2.: By definition \(I(\bm{x};S)=H(\bm{x})-H(\bm{x}|S)\). Hence, we first derive expressions for these two terms. Since \(\bm{x}\) follows the mixture model (3), it is of the form \(\bm{x}=y\bm{\mu}^{S}+\bm{\xi}\). Given \(S=s\), \(\bm{\mu}^{s}\) is deterministic with \(\mu_{j}^{s}=\sqrt{\frac{\lambda}{k}}\,\mathbbm{1}\{j\in s\}\). Thus, the vector \((\bm{x}|S=s)\) is distributed as a mixture of two Gaussians with centers \(\pm\bm{\mu}^{s}\) and identity covariance matrix. Its density is

\[f(\bm{x}|S=s)=\frac{1}{(2\pi)^{p/2}}\left(\frac{e^{-\|\bm{x}-\bm{\mu}^{s}\|^{2 }/2}+e^{-\|\bm{x}+\bm{\mu}^{s}\|^{2}/2}}{2}\right)=\frac{e^{-\frac{\|\bm{x}\|^{2 }+\lambda}{2}}}{(2\pi)^{p/2}}\left(\frac{e^{-\langle\bm{x},\bm{\mu}^{s}\rangle }+e^{\langle\bm{x},\bm{\mu}^{s}\rangle}}{2}\right).\] (54)By the definition of conditional entropy,

\[H(\bm{x}|S)=-\sum_{s\in\mathbb{\tilde{S}}}P(S=s)\int f\left(\bm{x}|S=s\right)\log f (\bm{x}|S=s)d\bm{x}.\]

Given the structure of the vectors \(\bm{\mu}^{s}\) for all \(s\in\tilde{\mathbb{S}}\), all the integrals in the sum above give the same value. Therefore, it suffices to consider a single set \(s_{0}=\{1,\ldots,k\}\),

\[H(\bm{x}|S)=-\int f\left(\bm{x}|S=s_{0}\right)\log f(\bm{x}|S=s_{0})d\bm{x}.\] (55)

Inserting (54) into (55), gives

\[H(\bm{x}|S)=-\operatorname{\mathbb{E}}_{\bm{x}|s_{0}}\left[\log\left(\frac{e^ {-\frac{|\bm{x}|^{2}+\lambda}{2}}}{(2\pi)^{p/2}}\cosh(\langle\bm{x},\bm{\mu}^{ s_{0}}\rangle)\right)\right].\]

Note that for any \(s\in\tilde{\mathbb{S}}\), \(\operatorname{\mathbb{E}}\left[\|\bm{x}\|^{2}\right|\)\(S=s\right]=\lambda+p\). Thus,

\[H(\bm{x}|S)=C(p,\lambda)-\operatorname{\mathbb{E}}_{\bm{x}|s_{0}}\left[\log \cosh(\langle\bm{x},\bm{\mu}^{s_{0}}\rangle)\right].\] (56)

where \(C(p,\lambda)=\lambda+\frac{p}{2}(1+\log(2\pi))\).

Consider the following two independent random variables, \(w=\frac{1}{\sqrt{k}}\sum_{j=1}^{k-1}\xi_{j}\sim N\left(0,\frac{k-1}{k}\right)\) and \(\xi=\xi_{k}\sim N(0,1)\). For \(S=s_{0}\),

\[\langle\bm{x},\bm{\mu}^{s_{0}}\rangle=\langle y\bm{\mu}^{s_{0}}+\bm{\xi},\bm{ \mu}^{s_{0}}\rangle=\lambda y+\sqrt{\lambda}w+\frac{\sqrt{\lambda}}{\sqrt{k}}\xi.\]

Inserting the above into (56) gives

\[H(\bm{x}|S)=C(p,\lambda)-\operatorname{\mathbb{E}}\left[\log\cosh(\lambda y+ \sqrt{\lambda}w+\sqrt{\lambda/k}\xi)\right],\]

where the expectation is over \(w,y\) and \(\xi\). Note that \(w,y\) and \(\xi\) are independent random variables with zero mean and symmetric distributions around zero. Further, recall that \(y\) attains the values \(\pm 1\) with equal probabilities. Hence, by a symmetry argument we may set \(y=1\) and take the expectation only over \(w\) and \(\xi\). This gives

\[H(\bm{x}|S)=\lambda+\frac{p}{2}\left(1+\log 2\pi\right)-\operatorname{\mathbb{E} }\left[\log\cosh\left(\lambda+\sqrt{\lambda}w+\sqrt{\lambda/k}\xi\right) \right].\] (57)

Next, we derive an expression for \(H(\bm{x})\). Recall that \(\bm{x}\) depends on a vector \(\mu^{S}\) with \(S\) distributed uniformly at random from \(\tilde{\mathbb{S}}\) of size \(p-k+1\). Note that \(\tilde{\mathbb{S}}=\bigcup_{\ell=k}^{p}s_{\ell}\) where \(s_{\ell}=[k-1]\cup\{\ell\}\). By the law of total probability

\[f(\bm{x})=\frac{1}{|\tilde{\mathbb{S}}|}\sum_{s\in\tilde{\mathbb{S}}}f(\bm{x}| S=s)=\frac{1}{p-k+1}\sum_{\ell=k}^{p}f(\bm{x}|S=s^{\ell}).\]

Using the same analysis as before, it follows that

\[f(\bm{x})=\frac{e^{-\frac{|\bm{x}|^{2}+\lambda}{2}}}{(2\pi)^{p/2}}\cdot\frac{ 1}{p-k+1}\sum_{\ell=k}^{p}\frac{e^{-\langle\bm{x},\bm{\mu}^{s^{\ell}}\rangle}+ e^{\langle\bm{x},\bm{\mu}^{s^{\ell}}\rangle}}{2}.\]

Hence,

\[H(\bm{x})=-\operatorname{\mathbb{E}}\left[\log f(\bm{x})\right]=C(p,\lambda)- \operatorname{\mathbb{E}}\left[\log\left(\frac{1}{p-k+1}\sum_{\ell=k}^{p} \frac{e^{-\langle\bm{x},\bm{\mu}^{s^{\ell}}\rangle}+e^{\langle\bm{x},\bm{\mu} ^{s^{\ell}}\rangle}}{2}\right)\right]\] (58)

We now simplify the expectation in (58). First, by a symmetry argument, we may assume the label that corresponds to \(\bm{x}\) is simply \(y=1\). Let us simplify the inner product \(\langle\bm{x},\bm{\mu}^{s_{\ell}}\rangle\).

\[\langle\bm{x},\bm{\mu}^{s_{\ell}}\rangle=\langle\sqrt{\lambda}\bm{\mu}^{S}+ \bm{\xi},\bm{\mu}^{s_{\ell}}\rangle=\lambda\left(\frac{k-1}{k}+\frac{1\left\{ \ell\in S\right\}}{k}\right)+\sqrt{\lambda}w+\frac{\sqrt{\lambda}}{\sqrt{k}} \xi_{\ell}.\]Hence, the expectation above can be written as

\[\mathbb{E}\left[\log\left(\frac{1}{p-k+1}\sum_{\ell=k}^{p}\frac{e^{-\left(\lambda \frac{k-1}{k}+\sqrt{\lambda}w+\sqrt{\lambda/k}\xi_{\ell}+\frac{\lambda}{k}\frac{ 1}{k}\{\ell\in S\}\right)}+e^{\left(\lambda\frac{k-1}{k}+\sqrt{\lambda}w+ \sqrt{\lambda/k}\xi_{\ell}+\frac{\lambda}{k}\frac{1}{k}\{\ell\in S\}\right)}}{2 }\right)\right]\]

where the expectation is over \(S\), \(w\) and \(\{\xi_{\ell}\}_{\ell=k}^{p}\). Since \(S\) is uniformly distributed over \(\tilde{\mathbb{S}}\) and for any \(S=s^{j}\) the expectation is the same, we may thus set \(S=s^{k}=[k]\), and take the expectation only over \(w\) and \(\{\xi_{\ell}\}_{\ell=k}^{p}\).

Next, we decompose the sum inside the logarithm as \(S_{1}+S_{2}\), where

\[S_{1} = \frac{\exp\left(-\lambda\frac{k-1}{k}-\sqrt{\lambda}w\right)}{2} \cdot\frac{1}{p-k+1}\sum_{\ell=k}^{p}e^{-\sqrt{\frac{\lambda}{k}}\xi_{\ell}- \frac{\lambda}{k}\frac{1}{k}\{\ell=k\}}\] \[S_{2} = \frac{\exp\left(\lambda\frac{k-1}{k}+\sqrt{\lambda}w\right)}{2} \cdot\frac{1}{p-k+1}\sum_{\ell=k}^{p}e^{\sqrt{\frac{\lambda}{k}}\xi_{\ell}+ \frac{\lambda}{k}\frac{1}{k}\{\ell=k\}}\]

We now analyze each of these terms in an asymptotic setting where \(p,k\rightarrow\infty\) and \(k=o(p)\). To this end we write the sum in \(S_{2}\) as follows

\[\frac{1}{p-k+1}\sum_{\ell=k}^{p}e^{\sqrt{\frac{\lambda}{k}}\xi_{\ell}}+\frac{ 1}{p-k+1}e^{\sqrt{\frac{\lambda}{k}}\xi_{k}}\left(e^{\frac{\lambda}{k}}-1\right)\]

By the central limit theorem, asymptotically, the first sum may be written as \(e^{\lambda/2k}+O_{P}(\frac{\sqrt{\lambda}}{\sqrt{kp}})\), which follows from the fact that \(\mathbb{E}_{Z\sim\mathcal{N}(0,1)}[e^{tZ}]=e^{t^{2}/2}\). The second term above is \(O_{P}(\frac{\sqrt{\lambda}}{kp})\), which is negligible w.r.t. to the previous \(O_{P}\) term. Note that its expectation is finite and given by \(\frac{e^{\lambda/2k}\left(e^{\frac{\lambda}{k}}-1\right)}{p-k+1}\). The sum \(S_{1}\) can be analyzed similarly. In summary we obtain that

\[S_{1}+S_{2}=e^{\lambda/2k}\cdot\cosh\left(\lambda\frac{k-1}{k}+\sqrt{\lambda} w\right)\cdot\left(1+O_{P}\left(\frac{\sqrt{\lambda}}{\sqrt{kp}}\right)\right)\]

Hence, the expectation above simplifies to

\[\mathbb{E}[\log(S_{1}+S_{2})]=\frac{\lambda}{2k}+\mathbb{E}\left[\log\cosh \left(\lambda\frac{k-1}{k}+\sqrt{\lambda}w\right)\right]+O\left(\frac{\sqrt{ \lambda}}{\sqrt{kp}}\right)\]

Inserting the above into (58) gives

\[H(\bm{x})=C(p,\lambda)-\mathbb{E}\left[\log\cosh\left(\lambda\frac{k-1}{k}+ \sqrt{\lambda}w\right)\right]+O\left(\frac{\sqrt{\lambda}}{\sqrt{kp}}\right).\] (59)

Next, we derive an upper-bound for the mutual information \(I(\bm{x};S)=H(\bm{x})-H(\bm{x}|S)\). Combining (57) and (59), the constant \(C(p,\lambda)\) cancels out, and we obtain

\[I(\bm{x};S) =\mathbb{E}\left[\log\cosh\left(\lambda+\sqrt{\lambda}w+\sqrt{ \frac{\lambda}{k}}z\right)-\log\cosh\left(\lambda\frac{(k-1)}{k}+\sqrt{\lambda }w\right)\right]-\frac{\lambda}{2k}+O\left(\frac{\sqrt{\lambda}}{\sqrt{kp}} \right).\] \[=\mathbb{E}\left[g_{w}\left(\sqrt{\frac{\lambda}{k}}z\right)-g_{w }\left(-\frac{\lambda}{k}\right)\right]-\frac{\lambda}{2k}+O\left(\frac{\sqrt {\lambda}}{\sqrt{kp}}\right).\] (60)

where \(g_{w}(t)=\log\cosh\left(\lambda+\sqrt{\lambda}w+t\right)\). For future use, note that \(\frac{d}{dt}g_{w}(t)=\tanh(\lambda+\sqrt{\lambda}w+t)\). To upper bound \(I(\bm{x};S)\) we split the expectation in (60) into two parts as follows,

\[I(\bm{x};S)=\mathbb{E}\left[g_{w}\left(\sqrt{\frac{\lambda}{k}}z\right)-g_{w }(0)\right]+\mathbb{E}\left[g_{w}(0)-g_{w}\left(-\frac{\lambda}{k}\right) \right]-\frac{\lambda}{2k}+O\left(\frac{\sqrt{\lambda}}{\sqrt{kp}}\right).\] (61)The Taylor expansion of \(g_{w}(t)\) is given by

\[g_{w}(t)=g_{w}(0)+t\tanh(\lambda+\sqrt{\lambda}w)+\frac{t^{2}}{2}(1-\tanh^{2}( \lambda+\sqrt{\lambda}w))+\frac{t^{3}}{3!}g_{w}^{(3)}(\tau_{t}).\]

Here \(\tau_{t}\) is some number between \(0\) and \(t\), and \(g_{w}^{(3)}(\tau_{t})=-2\tanh(\lambda+\sqrt{\lambda}w+\tau_{t})+2\tanh^{3}( \lambda+\sqrt{\lambda}w+\tau_{t})\). Note that for all \(t\in\mathbb{R}\),

\[\frac{t^{3}}{3!}g_{w}^{(3)}(\tau_{t})\leq\frac{2|t^{3}|\tanh(\lambda+\sqrt{ \lambda}w+\tau_{t})}{3!}(1-\tanh^{2}(\lambda+\sqrt{\lambda}w+\tau_{t}))\leq \frac{2|t^{3}|}{3!}.\]

Since \(z,w\) are independent and \(\mathbb{E}[z]=0\), it follows that

\[\mathbb{E}\left[g_{w}\left(\sqrt{\frac{\lambda}{k}}z\right)-g_{w }(0)\right]\] \[\leq \mathbb{E}\left[\sqrt{\frac{\lambda}{k}}z\tanh\left(\lambda+ \sqrt{\lambda}w\right)+\frac{\lambda}{2k}z^{2}\left(1-\tanh^{2}\left(\lambda+ \sqrt{\lambda}w\right)\right)+\frac{2\lambda^{3/2}|z^{3}|}{3!k^{3/2}}\right]\] \[= \frac{\lambda}{2k}\,\mathbb{E}\left[1-\tanh^{2}\left(\lambda+ \sqrt{\lambda}w\right)\right]+O\left(\sqrt{\frac{\lambda^{3}}{k^{3}}}\right).\] (62)

For the second term in (61), for any value of \(w\), by the mean value theorem, it follows that

\[g_{w}(0)-g_{w}\left(-\frac{\lambda}{k}\right)=\frac{\lambda}{k}\tanh(\lambda+ \sqrt{\lambda}w+\zeta_{w})\leq\frac{\lambda}{k}\tanh(\lambda+\sqrt{\lambda}w)\]

where \(\zeta_{w}\in[-\lambda/k,0]\), and the last inequality follows from the fact that \(\tanh(\cdot)\) is a monotonically increasing function. Hence,

\[\mathbb{E}\left[g_{w}(0)-g_{w}\left(-\frac{\lambda}{k}\right)\right]\leq\frac {\lambda}{k}\,\mathbb{E}\left[\tanh(\lambda+\sqrt{\lambda}w)\right].\] (63)

Hence, combining (61), (62) and (63), yields

\[I(\bm{x};S) \leq\frac{\lambda}{2k}\,\mathbb{E}\left[1-\tanh^{2}\left(\lambda+ \sqrt{\lambda}w\right)\right]+\frac{\lambda}{k}\,\mathbb{E}\left[\tanh( \lambda+\sqrt{\lambda}w)\right]-\frac{\lambda}{2k}+O\left(\sqrt{\frac{\lambda ^{3}}{k^{3}}}+\frac{\sqrt{\lambda}}{\sqrt{kp}}\right)\] \[=\frac{\lambda}{k}\,\mathbb{E}\left[\tanh(\lambda+\sqrt{\lambda}w )-\frac{1}{2}\tanh^{2}\left(\lambda+\sqrt{\lambda}w\right)\right]+O\left( \sqrt{\frac{\lambda^{3}}{k^{3}}}+\frac{\sqrt{\lambda}}{\sqrt{kp}}\right).\] (64)

By Lemma A.7, the expectation above can be bounded as follows:

\[\mathbb{E}\left[\tanh(\lambda+\sqrt{\lambda}w)-\frac{1}{2}\tanh^{2}\left( \lambda+\sqrt{\lambda}w\right)\right]\leq\frac{1}{2}\left(\lambda+3\sqrt{ \lambda/k}\right).\]

Hence, inserting this upper bound into (64), gives

\[I(\bm{x};S)\leq\frac{1}{2}\frac{\lambda^{2}}{k}+O\left(\frac{ \sqrt{\lambda}}{\sqrt{kp}}+\sqrt{\lambda^{3}/k^{3}}\right)\]

Asymptotically, as \(p,k\rightarrow\infty\) with \(k/p\to 0\) then \(\frac{1}{k}\gg\frac{1}{\sqrt{kp}}\). Thus, the term \(\frac{1}{2}\frac{\lambda^{2}}{k}\) is asymptotically larger than the \(O(\cdot)\) terms in the display above. Hence, the inequality (37) of the lemma follows. 

Proof of Lemma c.1.: The main idea of the proof is to show that with a suitable choice of threshold \(\tau\), nearly all entries \(\bm{w}_{L}(j)\) for \(j\in S\) are above this threshold, in absolute value, whereas the number of noise magnitudes above it is smaller than \(\tilde{p}-k\). Since we prove the lemma for the case of two symmetric Gaussians \(\bm{\mu}_{1}=-\bm{\mu}_{-1}=\bm{\mu}\), for simplicity we consider the following formula for \(\bm{w}_{L}=\frac{1}{L}\sum_{i=1}^{L}y_{i}\bm{x}_{i}\). With minor adaptations one can also prove the Lemma with the original formula (10) of \(\bm{w}_{L}\).

Fix \(\epsilon>0\), and let \(\mathcal{A}_{L}\) denote the event that \(|S_{L}\cap S|\geq k(1-\epsilon)\). For any threshold \(\tau\) define the following two events,

\[\tilde{\mathcal{B}}(\tau)=\left\{\sum_{j\in S}\mathbbm{1}\{|\bm{w}_{L}(j)|> \tau\}>k(1-\epsilon)\right\},\]

and

\[\tilde{\mathcal{C}}(\tau)=\left\{\sum_{j\notin S}\mathbbm{1}\{|\bm{w}_{L}(j)| >\tau\}<\tilde{p}-k\right\}.\]

By their definition, it follows that for any \(\tau>0\)

\[\tilde{\mathcal{B}}(\tau)\cap\tilde{\mathcal{C}}(\tau)\subseteq\mathcal{A}_{ L}.\]

Furthermore, note that the two events \(\tilde{\mathcal{B}}(\tau)\) and \(\tilde{\mathcal{C}}(\tau)\) are independent. Hence, to prove that \(\mathbb{P}(\mathcal{A}_{L})\to 1\), it suffices to prove that for a suitable threshold \(\tau\),

\[\mathbb{P}\left(\tilde{\mathcal{B}}(\tau)\cap\tilde{\mathcal{C}}(\tau) \right)=\mathbb{P}\left(\tilde{\mathcal{B}}(\tau)\right)\cdot\mathbb{P}\left( \tilde{\mathcal{C}}(\tau)\right)\to 1\] (66)

In other words, it suffices to show that each of these events occurs with probability tending to one.

We start by showing that \(\Pr[\tilde{\mathcal{B}}(\tau)]\to 1\). First, let us define an even simpler event, with the absolute value removed,

\[\mathcal{B}(\tau)=\left\{\sum_{j\in S}\mathbbm{1}\{\mathrm{sign}(\mu_{j})\bm{ w}_{L}(j)>\tau\}>k(1-\epsilon)\right\}\]

Clearly \(\mathcal{B}(\tau)\subset\tilde{\mathcal{B}}(\tau)\) and thus it suffices to show that \(\mathbb{P}(\mathcal{B}(\tau))\to 1\) as \(p\to\infty\).

To this end, we consider a threshold of the form \(\tau=\sqrt{\frac{\lambda}{k}}T\), with the value of \(T\) specified below. By the sparse mixture model (3), at support coordinates,

\[\mathrm{sign}(\mu_{j})\bm{w}_{L}(j)=\sqrt{\frac{\lambda}{k}}+\frac{\mathrm{ sign}(\mu_{j})}{\sqrt{L}}\xi_{j}\]

where \(\xi_{j}\sim\mathcal{N}(0,1)\). Inserting this expression with \(L\geq\frac{2\beta k\log(p-k)}{\lambda}\) into the above, and suppressing the dependence on \(\tau\) in \(\mathcal{B}(\tau)\), gives

\[\mathbb{P}\left(\mathcal{B}\right) \geq \mathbb{P}\left(\sum_{j\in S}\mathbbm{1}\left\{1+\sqrt{\frac{1}{2 \beta\log(p-k)}}\xi_{j}>T\right\}>k(1-\epsilon)\right)\] (67) \[= \mathbb{P}\left(\sum_{j\in S}\mathbbm{1}\{\xi_{j}>-(1-T)\sqrt{2 \beta\log(p-k)}\}>k(1-\epsilon)\right).\]

Next, we choose

\[T=1-\frac{1}{(2\beta\log(p-k))^{1/4}},\] (68)

and define

\[q_{1}=\mathbb{P}\left(N(0,1)>-(2\beta\log(p-k))^{1/4}\right).\]

Since the \(\xi_{j}\)'s are all independent and \(|S|=k\), with this choice of \(T\), Eq. (67) simplifies to

\[\mathbb{P}\left(\mathcal{B}\right) \geq\mathbb{P}\left(Bin(k,q_{1})>k(1-\epsilon)\right)\]Note that \(\lim_{p\to\infty}q_{1}=1\). Thus, for sufficiently large \(p\), it holds that \(q_{1}(1-\epsilon/2)>1-\epsilon\). Hence, instead of the right hand side above we may bound \(\mathbb{P}(Bin(k,q_{1})>kq_{1}(1-\epsilon/2))\). Indeed, by Chernoff's bound (17),

\[\mathbb{P}\left(\mathcal{B}\right)\geq 1-e^{\frac{\epsilon^{2}kq_{1}}{8}}\]

which tends to one as \(p,k\to\infty\).

Next, we show that the second term in Eq. (66), \(\mathbb{P}(\bar{\mathcal{C}})\), also tends to one as \(p\to\infty\). First of all, since \(k=\lfloor c_{1}p^{\alpha}\rfloor\) and \(\alpha<1-\beta<1-\tilde{\beta}\), then \(k\ll p^{1-\tilde{\beta}}\), and thus \(\tilde{p}-k\ll\tilde{p}/2\). Hence, for \(p\) sufficiently large, we may instead consider the following event

\[\mathcal{C}(\tau)=\left\{\sum_{j\notin S}\mathbbm{1}\{|\bm{w}_{L}(j)|>\tau\}< \frac{\tilde{p}}{2}\right\}=\left\{Bin(p-k,q_{2}(\tau))<\frac{\tilde{p}}{2}\right\}\]

where \(q_{2}(\tau)=2\Phi^{c}(\sqrt{L}\tau)\). Clearly \(\mathcal{C}(\tau)\subset\bar{\mathcal{C}}(\tau)\), and we now prove that with \(\tau=\sqrt{\frac{\lambda}{k}}T\), and \(T\) given in (68) \(\mathbb{P}(\mathcal{C}(\tau))\to 1\), by applying a Chernoff bound.

To this end, we write

\[\frac{\tilde{p}}{2}=q_{2}(p-k)(1+\delta)\]

where \(\delta=\frac{\tilde{p}}{2(p-k)}\frac{1}{q_{2}}-1\).

To use Chernoff's inequality we first need to show that \(\delta\geq 0\). Indeed, as \(p\to\infty\), with \(\tau=\sqrt{\frac{\lambda}{k}}T\), and using Lemma A.1 which bounds the tail function \(\Phi^{c}\),

\[\lim_{p\to\infty}(\delta+1) =\lim_{p\to\infty}\frac{p^{1-\tilde{\beta}}}{2(p-k)}\frac{1}{2 \Phi^{c}(T\sqrt{2\beta\log(p-k)})}\] \[\geq\frac{\sqrt{\pi\beta}}{2}\lim_{p\to\infty}\frac{T\sqrt{\log(p -k)}}{(p-k)^{\tilde{\beta}}\exp(-T^{2}\beta\log(p-k))}=\frac{\sqrt{\pi\beta}} {2}\lim_{p\to\infty}\frac{T\sqrt{\log(p-k)}}{(p-k)^{\tilde{\beta}-\beta T^{2}}}\] (69)

Note that for sufficiently large \(p\),

\[\tilde{\beta}-\beta T^{2}=\beta(1-T^{2})-\tilde{\epsilon}=\frac{\beta}{(2 \beta\log(p-k))^{1/4}}\left(2-\tfrac{1}{(2\beta\log(p-k))^{1/4}}\right)- \tilde{\epsilon}<\frac{2\beta}{(2\beta\log(p-k))^{1/4}}-\tilde{\epsilon}<0.\]

Combining the above and (69) gives

\[\lim_{p\to\infty}(\delta+1)\geq\frac{\sqrt{\pi\beta}}{2}\lim_{p\to\infty}T \sqrt{\log(p-k)}=\infty.\]

Since \(\tilde{\beta}<\beta\), for sufficiently large \(p\), and \(T\) given in (68) it follows that \(\tilde{\beta}-\beta T^{2}<0\). Therefore, for sufficiently large \(p\), it follows that \(\delta>0\). By Chernoff's bound (16)

\[\mathbb{P}\left(\mathcal{C}\left(\sqrt{\tfrac{\lambda}{k}}T\right)\right)= \mathbb{P}\left(Bin(p-k,q_{2})<q_{2}(p-k)(1+\delta)\right)\geq 1-e^{-\frac{ \delta^{2}(p-k)q_{2}}{2+\delta}}.\] (70)

We now prove that the term in the exponent tends to infinity. First, note that since \(\delta\to\infty\) then \(\delta=\frac{\tilde{p}}{2(p-k)q_{2}}-1\geq\frac{\tilde{p}}{4(p-k)q_{2}}\). Hence,

\[\lim_{p\to\infty}\frac{\delta^{2}(p-k)q_{2}}{2+\delta}=\lim_{p\to\infty}\delta (p-k)q_{2}\geq\lim_{p\to\infty}\frac{\tilde{p}}{4(p-k)q_{2}}(p-k)q_{2}=\infty.\]

Combining the above and (70) gives \(\lim_{p\to\infty}\mathbb{P}\left(\mathcal{C}\left(\sqrt{\tfrac{\lambda}{k}}T \right)\right)=1\) which completes the proof.

\(\Box\)The MLE in the Supervised Setting

The following proposition states the form of the maximum likelihood estimator (MLE) for the support \(S\) in a SL setting, where the sparsity \(k\) is known, and the non-zero entries of \(\bm{\mu}\) have magnitude \(\pm\sqrt{\lambda/k}\).

**Proposition E.1**.: _Let \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{L}\) be \(L\) i.i.d. labeled samples from model (3) where \(\bm{\mu}\) is \(k\)-sparse with non-zero entries \(\pm\sqrt{\lambda/k}\), and let \(\bm{w}_{L}=\frac{1}{L}\sum_{i\in[L]}y_{i}\bm{x}_{i}\). Assuming the sparsity \(k\) is known, the MLE for \(S=\text{supp}(\bm{\mu})\) is given by the indices corresponding to the top-\(k\) magnitudes of \(\bm{w}_{L}\)._

Proof.: Under our assumptions, the set of all possible vectors \(\bm{\mu}\) has a one-to-one mapping to a support set \(S\in\mathbb{S}\) and a vector \(D\in\{-1,1\}^{k}\) containing the signs of the \(k\) non-zero entries of \(\bm{\mu}\). We thus denote \(\theta=(S,D)\), and \(\bm{\mu}^{\theta}\) by

\[\mu_{j}^{\theta}=\begin{cases}\frac{\sqrt{\lambda}}{\sqrt{k}}D_{j},&j\in S, \\ 0,&j\not\in S.\end{cases}\]

Let us denote by \(p_{X,Y}(\bm{x},y;\theta)\) the joint probability density function of a single sample \((\bm{x},y)\) from the model (3) with parameter \(\theta\). Since \(y\) is a Rademacher random variable with a distribution not dependent on \(\theta\), we may write

\[p_{X,Y}(\bm{x},y;\theta)=p_{X|Y}(\bm{x}|y;\theta)\,p_{Y}(y)=\tfrac{1}{2}p_{X|Y }(\bm{x}|y;\theta).\]

Since \(\bm{x}|y\sim\mathcal{N}(y\bm{\mu},\mathbf{I}_{p})\), the conditional density \(p_{X|Y}(\bm{x}|y;\theta)\) simplifies to

\[p_{X,Y}(\bm{x}|y,\theta)=\frac{1}{(2\pi)^{p/2}}\exp\left(-\|\bm{x}-y\bm{\mu}^ {\theta}\|^{2}/2\right).\] (71)

By definition, the maximum-likelihood estimator of \(\theta\) is given by

\[\hat{\theta}^{\text{(ML)}}=\operatorname*{arg\,max}_{\theta}\sum_{i=1}^{L} \log p_{X,Y}(\bm{x},y;\theta),\]

Inserting (71) into the above, and using \(\|\bm{\mu}^{\theta}\|=\sqrt{\lambda}\) (fixed for all \(\theta\)), gives

\[\hat{\theta}^{\text{(ML)}}=\operatorname*{arg\,max}_{\theta}\sum_{i=1}^{L} \left(-\left\|\bm{x}_{i}-y_{i}\bm{\mu}^{\theta}\right\|^{2}\right)=\operatorname *{arg\,max}_{\theta}\sum_{i=1}^{L}\left\langle y_{i}\bm{x}_{i},\bm{\mu}^{ \theta}\right\rangle=\operatorname*{arg\,max}_{\theta}\left\langle\bm{w}_{L}, \bm{\mu}^{\theta}\right\rangle.\]

Therefore, the maximum value of \(\left\langle\bm{w}_{L},\bm{\mu}^{\theta}\right\rangle\) is obtained for \(\hat{S}^{\text{(ML)}}\) being the set of indices corresponding to the \(k\) largest magnitude entries of \(\bm{w}_{L}\), and \(\hat{D}^{\text{(ML)}}=\{\text{sign}(\bm{w}_{L})_{j}:\,j\in\hat{S}^{\text{(ML) }}\}\). 

The next proposition shows that with sufficient number of labeled samples the MLE for \(S\) has significant overlap with the true support set \(S\).

**Proposition E.2**.: _Let \(\mathcal{D}_{L}=\{(\bm{x}_{i},y_{i})\}_{i=1}^{L}\) be a set of \(L\) i.i.d. labeled samples from the model (3) with a \(k\)-sparse vector \(\bm{\mu}\) whose non-zero entries are \(\pm\sqrt{\lambda/k}\). Assume that for some \(\alpha\in(0,1)\), \(k=\lfloor c_{1}p^{\alpha}\rfloor\) and that \(L=\left\lceil\frac{2\beta k\log(p-k)}{\lambda}\right\rceil\), for some \(\beta\in(0,1)\). Let \(S_{L}\) be the indices of the \(k\) largest magnitudes of the vector \(\bm{w}_{L}=\frac{1}{L}\sum_{i\in[L]}y_{i}\bm{x}_{i}\). If \(\beta>1-\alpha\), then for every \(\epsilon\in(0,1)\),_

\[\lim_{p\to\infty}\mathbb{P}\left(\frac{|S\cap S_{L}|}{k}>1-\epsilon\right)=1.\]

Proof.: Fix \(\epsilon>0\), and let \(\mathcal{A}_{L}\) denote the event that \(|S_{L}\cap S|\geq k(1-\epsilon)\). For any threshold \(\tau\) define the following two events,

\[\tilde{\mathcal{B}}(\tau)=\left\{\sum_{j\in S}\mathbbm{1}\{|\bm{w}_{L}(j)|> \tau\}>k(1-\epsilon)\right\},\quad\tilde{\mathcal{C}}(\tau)=\left\{\sum_{j\notin S }\mathbbm{1}\{|\bm{w}_{L}(j)|>\tau\}<k\epsilon\right\}.\]By their definition, it follows that for any \(\tau>0\), \(\tilde{\mathcal{B}}(\tau)\cap\tilde{\mathcal{C}}(\tau)\subset\mathcal{A}_{L}\). Since the two events \(\tilde{\mathcal{B}}(\tau)\) and \(\tilde{\mathcal{C}}(\tau)\) are independent,

\[\mathbb{P}(\mathcal{A}_{L})\geq\mathbb{P}\left(\tilde{\mathcal{B}}(\tau)\cap \tilde{\mathcal{C}}(\tau)\right)=\mathbb{P}\left(\tilde{\mathcal{B}}(\tau) \right)\cdot\mathbb{P}\left(\tilde{\mathcal{C}}(\tau)\right)\] (72)

Hence, for \(\mathbb{P}(\mathcal{A}_{L})\to 1\), it suffices that for a suitable threshold \(\tau\), both probabilities on the right hand side tend to one, We start with \(\Pr(\tilde{\mathcal{B}}(\tau))\). To this end, we define an even simpler event, with the absolute value removed,

\[\mathcal{B}(\tau)=\left\{\sum_{j\in S}\mathbbm{1}\{\operatorname{sign}(\mu_{j })\bm{w}_{L}(j)>\tau\}>k(1-\epsilon)\right\}\]

Clearly \(\mathcal{B}(\tau)\subset\tilde{\mathcal{B}}(\tau)\) and thus it suffices to show that \(\mathbb{P}(\mathcal{B}(\tau))\to 1\) as \(p\to\infty\). By the sparse mixture model (3), for \(j\in S\),

\[\operatorname{sign}(\mu_{j})\bm{w}_{L}(j)=\sqrt{\frac{\lambda}{k}}+\frac{ \operatorname{sign}(\mu_{j})}{\sqrt{L}}\xi_{j},\]

where \(\xi_{j}\sim\mathcal{N}(0,1)\). Combining a threshold value \(\tau=\sqrt{\frac{\lambda}{k}}\frac{1-\alpha+\beta}{2\beta}\) and the assumption that \(L\geq\frac{2\beta k\log(p-k)}{\lambda}\) with this expression, gives that

\[\mathbb{P}(\mathcal{B}(\tau))\geq\mathbb{P}\left(\sum_{j\in S}\mathbbm{1} \left\{\xi_{j}>-\left(\sqrt{\beta}-\sqrt{\frac{1-\alpha+\beta}{2}}\right) \sqrt{2\log p}\right\}>k(1-\epsilon)\right).\] (73)

Let \(q_{1}\) be the probabuity of each event in the above sum,

\[q_{1}=\mathbb{P}\left(N(0,1)>-\left(\sqrt{\beta}-\sqrt{\frac{1-\alpha+\beta}{ 2}}\right)\sqrt{2\log p}\right).\]

Since the \(\xi_{j}\)'s are all independent and \(|S|=k\), with this choice of \(\tau\), Eq. (73) simplifies to

\[\mathbb{P}(\mathcal{B}(\tau))\geq\mathbb{P}(Bin(k,q_{1})>k(1-\epsilon)).\]

Note that since \(\beta>1-\alpha\), then \(\lim_{p\to\infty}q_{1}=1\). Thus, for sufficiently large \(p\), it holds that \(q_{1}(1-\epsilon/2)>1-\epsilon\). Hence, the right hand side above may be bounded by \(\mathbb{P}(Bin(k,q_{1})>kq_{1}(1-\epsilon/2))\). Indeed, by Chernoff's bound (17), \(\mathbb{P}(\mathcal{B}(\tau))\to 1\) as \(p,k\to\infty\), since

\[\mathbb{P}\left(\mathcal{B}\right)\geq 1-e^{-\frac{\epsilon^{2}kq_{1}}{8}}.\]

Next, we show that the second term in Eq. (72), \(\mathbb{P}(\tilde{\mathcal{C}})\), also tends to one as \(p\to\infty\). First, note that

\[\tilde{\mathcal{C}}(\tau)=\{Bin(p-k,q_{2}(\tau))<k\epsilon\}\]

where \(q_{2}(\tau)=2\Phi^{c}(\sqrt{L}\tau)=2\Phi^{c}(\sqrt{(1-\alpha+\beta)\log(p-k)})\). We now prove that \(\mathbb{P}(\tilde{\mathcal{C}}(\tau))\to 1\), by applying a Chernoff bound. To this end, we write

\[k\epsilon=q_{2}(p-k)(1+\delta)\]

where \(\delta=\frac{k\epsilon}{(p-k)}\frac{1}{q_{2}}-1\). To use Chernoff's inequality we first need to show that \(\delta\geq 0\). Indeed, as \(p\to\infty\), using Lemma A.1 which bounds the tail function \(\Phi^{c}\),

\[\lim_{p\to\infty}(\delta+1) \geq\lim_{p\to\infty}\frac{\epsilon k}{p-k}\frac{1}{2\Phi^{c}( \sqrt{(1-\alpha+\beta)\log(p-k)})}\] \[\geq\epsilon c_{1}\sqrt{\frac{\pi(1-\alpha+\beta)}{2}}\lim_{p\to \infty}\frac{\sqrt{\log(p-k)}}{p^{1-\alpha}\exp(-\frac{1-\alpha+\beta}{2}\log (p-k))}\] \[=\epsilon c_{1}\sqrt{\frac{\pi(1-\alpha+\beta)}{2}}\lim_{p\to \infty}\frac{\sqrt{\log(p-k)}}{p^{\frac{1-\alpha-\beta}{2}}}=\infty,\]where the last equality follows from \(\beta>1-\alpha\). By Chernoff's bound (16)

\[\mathbb{P}\left(\tilde{\mathcal{C}}\left(\tau\right)\right)=\mathbb{P}\left(Bin(p- k,q_{2})<q_{2}(p-k)(1+\delta)\right)\geq 1-e^{-\frac{\delta^{2}(p-k)q_{2}}{2+ \delta}}.\] (74)

We now prove that the term in the exponent tends to infinity. Since \(\delta\to\infty\) it follows that

\[\lim_{p\to\infty}\frac{\delta^{2}(p-k)q_{2}}{2+\delta}=\lim_{p\to\infty}\delta (p-k)q_{2}\geq\lim_{p\to\infty}\frac{(1+\delta)(p-k)q_{2}}{2}=\lim_{p\to\infty }\frac{k\epsilon}{2}=\infty.\]

Combining the above and (74) gives that \(\lim_{p\to\infty}\mathbb{P}\left(\tilde{\mathcal{C}}\left(\tau\right)\right)=1\), which completes the proof. 

The above proposition implies the following result regarding accurate classification: Let \(T(\bm{x})=\operatorname{sign}\langle\hat{\bm{\mu}},\bm{x}\rangle\), with \(\hat{\bm{\mu}}=\bm{w}_{L}|_{S_{L}}\), be a linear classifier that is constructed using only the set of labeled samples \(\mathcal{D}_{L}\). If \(\beta>1-\alpha\), then combining Proposition E.2 and Lemma A.8 implies that the excess risk of \(T\) tends to zero as \(p\to\infty\).

### Impossibility of Classification

In this section we prove a lower bound for classification in the SL setting. To do that, we consider a slightly different model, known as the _rare and weak_ model. Here the sparsity of the vector \(\bm{\mu}\) is not fixed at exactly \(k\). Instead the vector is generated randomly with entries \(\mu_{j}=\sqrt{\lambda/k}B_{j}\), where \(B_{j}\sim Ber(\epsilon_{p})\) are i.i.d. Bernoulli random variables with \(\epsilon_{p}=\frac{k}{p}=p^{-(1-\alpha)}\). The next theorem implies that in the red region (impossible) of figure 1, indeed there exists (approximately) \(k\)-sparse vectors for which any classifier would asymptotically be no better than random.

**Theorem E.3**.: _Let \(\mathcal{D}_{L}=\{(\bm{x}_{i},y_{i})\}_{i=1}^{L}\) be set of L i.i.d. labeled samples from the rare weak model \(\mathcal{N}(y\bm{\mu},I)\) where all non-zero entries of \(\bm{\mu}\) are \(\pm\sqrt{\lambda/k}\). Suppose \(L=\lceil\frac{2\beta k\log p}{\lambda}\rceil\) and \(k\propto p^{\alpha}\), for some \(\alpha<1\). If \(\beta<1-\alpha\) then the classification error of any classifier based on \(\mathcal{D}_{L}\), tends to \(1/2\) as \(p\to\infty\)._

Proof.: This proof is similar to the one of Jin (2009). First, let us denote the vector of z-scores by \(\bm{z}=\frac{1}{\sqrt{L}}\sum_{i\in[L]}y_{i}\bm{x}_{i}\). Since the entries of \(\bm{\mu}\) are generated independently under the rare-weak model, the entries of \(\bm{z}\) are also independent and all have the same density, which we denote by \(f(z)\). Given \(\bm{z}\), we denote the conditional probability that the \(j\)-th entry contains a feature by \(\eta=\mathbb{P}(j\in S|\bm{z})=\mathbb{P}(j\in S|z_{j}=z)\). By Bayes theorem,

\[\eta(z)=\frac{\mathbb{P}(j\in S)f(z|j\in S)}{f(z)}=\frac{\epsilon_{p}\phi(z- \tau_{p})}{(1-\epsilon_{p})\phi(z)+\epsilon_{p}\phi(z-\tau_{p})},\]

where \(\tau_{p}=\sqrt{L\lambda/k}=\sqrt{2\beta\log p}\), and \(\phi\) is the density of \(N(0,1)\).

From Lemmas 1,2 and 4 in Jin (2009), the misclassification error of any classifier \(T\) constructed using the set \(\mathcal{D}_{L}\), can be bounded as

\[\left|\mathbb{P}(T(\bm{x})\neq y)-1/2\right|<C\left(1-(\mathbb{E}_{z}[H(z)])^{ p}\right)^{1/2},\] (75)

where \(H(z)=\mathbb{E}_{x}\left[\left(1+\eta(z)(e^{\sqrt{\lambda/k}x-\lambda/2k}-1) \right)^{1/2}\right]\) with \(x\sim N(0,1)\), and \(z\sim(1-\epsilon_{p})N(0,1)+\epsilon N(\tau_{p},1)\).

Our goal is to show that \(\mathbb{E}_{z}[H(z)]=1+o(1/p)\), which implies that asymptotically the accuracy of \(T\) is no better than random. First, combining that \(\mathbb{E}_{x}\left[e^{\sqrt{\lambda/k}x-\lambda/2k}-1\right]=0\) and the inequality \(\left|\sqrt{1+t}-1-t/2\right|\leq Ct^{2}\), for any \(t>-1\), gives

\[\left|H(z)-1\right| =\left|H(z)-1-\tfrac{1}{2}\,\mathbb{E}[\eta(z)]\,\mathbb{E}[e^{ \sqrt{\lambda/k}x-\lambda/2k}-1]\right|\] \[\leq C\eta^{2}(z)\,\mathbb{E}_{x}\left[\left(e^{\sqrt{\lambda/k} x-\lambda/2k}-1\right)^{2}\right]=C\eta^{2}(z)(e^{\lambda/k}-1).\]For sufficiently large \(p\), \(e^{\lambda/k}-1\leq 2\lambda/k\). Since \(k\propto p^{\alpha}\), then, \(|H(z)-1|\leq\tilde{C}p^{-\alpha}\eta^{2}(z)\). Hence, to prove that \(\mathbb{E}_{z}[H(z)]=1+o(1/p)\), it suffices to show that \(\mathbb{E}_{z}[\eta^{2}(z)]=o(p^{-(1-\alpha)})=o(\epsilon_{p}).\) To this end, we first note

\[\mathbb{E}[\eta^{2}(z)]=(1-\epsilon_{p})\,\mathbb{E}[\eta^{2}(w)]+\epsilon_{ p}\,\mathbb{E}[\eta^{2}(w+\tau_{p})]\leq\mathbb{E}[\eta^{2}(w)]+\epsilon_{p}\, \mathbb{E}[\eta^{2}(w+\tau_{p})],\]

where \(w\sim N(0,1)\). Write \(\mathbb{E}(\eta^{2}(z))=I+II+III+IV\), where we have split each of the expectations into two separate integrals, with the split at suitably chosen values \(t_{1}\) and \(t_{2}\),

\[I=\int_{-\infty}^{t_{1}}\eta^{2}(w)\phi(w)dw,\quad II=\int_{t_{1}}^{\infty} \eta^{2}(w)\phi(w)dw,\]

and

\[III=\epsilon_{p}\int_{-\infty}^{t_{2}}\eta^{2}(w+\tau_{p})\phi(w)dw,\quad IV= \epsilon_{p}\int_{t_{2}}^{\infty}\eta^{2}(w+\tau_{p})\phi(w)dw,\]

As we see below, the following values will be suitable to derive the required bounds: \(t_{1}=\frac{1-\alpha+\beta}{2\beta}\tau_{p}\) and \(t_{2}=\frac{1-\alpha-\beta}{2\beta}\tau_{p}\).

Starting with \(I\), note that

\[I=\int_{-\infty}^{t_{1}}\left(\frac{\epsilon_{p}\phi(w-\tau_{p})}{(1-\epsilon _{p})\phi(w)+\epsilon_{p}\phi(w-\tau_{p})}\right)^{2}\phi(w)dw\leq\int_{- \infty}^{t_{1}}\left(\frac{\epsilon_{p}\phi(w-\tau_{p})}{(1-\epsilon_{p})\phi( w)}\right)^{2}\phi(w)dw\]

For large enough \(p\), the above can be bounded via

\[I\leq 2\epsilon_{p}^{2}\int_{-\infty}^{t_{1}}\frac{\phi^{2}(w-\tau_{p})}{\phi^ {2}(w)}\phi(w)dw=C\epsilon_{p}^{2}\int_{-\infty}^{t_{1}}e^{2w\tau_{p}-\tau_{ p}^{2}-w^{2}/2}dw,\] (76)

where the equality follows from the definition of \(\phi(w)\). Completing the square, the above can be written as

\[I\leq C\epsilon_{p}^{2}\int_{-\infty}^{t_{1}}e^{-(w-2\tau_{p})^{2}/2}\ e^{ \tau_{p}^{2}}dw.\]

Changing the variable \(x=w-2\tau_{p}\) reads

\[I\leq\ C\epsilon_{p}^{2}e^{\tau_{p}^{2}}\int_{-\infty}^{t_{1}-2 \tau_{p}}e^{-x^{2}/2}dx=\ C\epsilon_{p}^{2}e^{\tau_{p}^{2}}\Phi^{c}(2\tau_{p}- t_{1})\ \leq C\epsilon_{p}^{2}e^{\tau_{p}^{2}}e^{-(2\tau_{p}-t_{1})^{2}/2}\] (77)

Finally, since \(\beta<1-\alpha\), it follows that \(C\epsilon_{p}^{2}e^{\tau_{p}^{2}}e^{-(2\tau_{p}-t_{1})^{2}/2}=o(\epsilon_{p})\).

Next, since \(\eta(w)<1\) it follows that

\[II=\int_{t_{1}}^{\infty}\eta^{2}(w)\phi(w)dz\leq\int_{t_{1}}^{\infty}\phi(w) dw=\Phi^{c}(t_{1})\leq e^{-t_{1}^{2}/2}.\] (78)

Similar to the above, under the condition \(\beta<1-\alpha\) it holds that \(e^{-t_{1}^{2}/2}=o(\epsilon_{p})\).

Next, note that

\[III =\epsilon_{p}\int_{-\infty}^{t_{2}}\left(\frac{\epsilon_{p}\phi( w)}{(1-\epsilon_{p})\phi(w+\tau_{p})+\epsilon_{p}\phi(w)}\right)^{2}\phi(w)dw\] \[\leq\epsilon_{p}\int_{-\infty}^{t_{2}}\left(\frac{\epsilon_{p} \phi(w)}{(1-\epsilon_{p})\phi(w+\tau_{p})}\right)^{2}\phi(w)dw.\]

For large enough \(p\)

\[III\leq C\epsilon_{p}\int_{-\infty}^{t_{2}}\left(\frac{\epsilon_{p}\phi(w)}{ \phi(w+\tau_{p})}\right)^{2}\phi(w)dw=C\epsilon_{p}^{3}\int_{-\infty}^{t_{2} }e^{2w\tau_{p}+\tau_{p}^{2}-w^{2}/2}dw.\]

Completing the square reads

\[III\leq C\epsilon_{p}^{3}e^{3\tau_{p}^{2}}\int_{-\infty}^{t_{2}}e^{-(w-2\tau_{ p})^{2}/2}dw=C\epsilon_{p}^{3}e^{3\tau_{p}^{2}}\Phi^{c}(2\tau_{p}-t_{2})\leq C \epsilon_{p}^{3}e^{3\tau_{p}^{2}}e^{-(2\tau_{p}-t_{2})^{2}/2}.\]Since \(\beta\leq 1-\alpha\) it holds that \(III=o(\epsilon_{p})\).

Finally, since \(\eta(w)<1\), IV can be bounded as follows

\[IV=\epsilon_{p}\int_{t_{2}}^{\infty}\phi(w)dw=\epsilon_{p}\Phi^{c}(t_{2})\leq \epsilon_{p}e^{-t_{2}^{2}/2}.\]

Again, by \(\beta<1-\alpha\), \(IV=o(\epsilon_{p})\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All the assumptions are clearly stated at the beginning of Section 2. The proofs can be found in the supplemental material. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No]

Justification:

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the necessary details regarding the experimental settings are provided in Section 4 Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Section 4 Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: As mentioned in Section 4, the experiments were run on a CPU Intel i7 2.10 GHz. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.