# What's _Left_? Concept Grounding

with Logic-Enhanced Foundation Models

 Joy Hsu

Stanford University

&Jiayuan Mao1

MIT

&Joshua B. Tenenbaum

MIT

&Jiajun Wu

Stanford University

Equal contribution. Email: joycj@stanford.edu

Footnote 1: footnotemark:

###### Abstract

Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning--using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like "_left_" can also be grounded in 3D, temporal, and action data, as in moving to your _left_. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the **L**ogic-**E**nhanced **F**ound**a**T**ion **M**odel **(**LEFT**), a unified framework that _learns_ to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains.

## 1 Introduction

The power of language lies in its generalization based on abstraction. A single concept, such as _left_, can be used across domains: in perception, we recognize the _left_ leg from a chair's 3D point cloud; in navigation, we go home by turning _left_ around the cafe at the end of the block; in manipulation, we pick and place a mug on the _left_ of a plate. _Left_ has domain-specific groundings in each of these

Figure 1: LEFT is a unified concept learning and reasoning framework that grounds modular concepts across domains, and flexibly reasons with concepts across tasks with a foundation model.

domains, but the concept symbol itself as an abstraction serves as a basis for complex and multi-step reasoning across domains.

Machine systems for visual reasoning also benefit from the disentanglement of concept learning and reasoning. Most recently, models such as VisProg (Gupta and Kembhavi, 2023) and ViperGPT (Suris et al., 2023) have leveraged large language models (LLMs) to produce programs based on language queries. For example, to answer "what's the color of the cat?", the program will first locate the cat, then query its color. Such programs are executed with pre-defined and pre-trained vision-language models (e.g., open-vocabulary segmentation models) for grounding and question-answering. These works perform well on image-based reasoning, though surprisingly, despite the generalization seemingly offered for free by language, less success has been shown in other domains.

What's _left_ to make visual reasoning systems work across domains? We identify two key shortcomings of existing methods. First, they are _inference-only_, building exclusively upon pre-trained models to interpret concepts such as _left_ during program execution. Therefore, in domains where data is scarce (e.g., 3D, human motion, or robot action), pretrained models, if there exist any, do not perform as well. Thus, we need trainable systems that learn the domain-specific groundings of concepts to execute across domains. Unfortunately, these visual reasoning models also cannot be made trainable due to their second shortcoming: they rely on a non-differentiable program executor to compose these grounding modules. Therefore, though _left_ may have different groundings in 3D scenes and robot actions compared to 2D images, these modules will not be able to adapt through additional training in those domains.

We address these issues by proposing Logic-Enhanced Foundation Models (LEFT), a model that _learns_ to ground and reason with concepts across domains. LEFT also leverages LLMs for language parsing; but unlike prior work, LEFT has trainable concept grounding modules that learn from domain data, made possible via a differentiable, domain-independent, first-order logic-based program executor. Inspired by previous works that combine symbolic programs and deep models via differentiable execution (Mao et al., 2019; Dong et al., 2019), our design no longer requires manually designed domain-specific languages and facilitates generalization across domains. Specifically, LEFT's LLM interpreter takes as input language queries, resolves textual ambiguities, and outputs a non-ambiguous program represented in a general reasoning language of first-order logic, which is shared across domains and tasks. LEFT's executor then executes the logic program with learnable domain-specific grounding modules, which are automatically initialized with concepts generated by the LLM.

LEFT enjoys the advantages of strong performance and data efficiency across domains due to its modular structure. Within a single domain, LEFT generalizes zero-shot to unseen and complex tasks, by leveraging general reasoning capabilities from the LLM interpreter, and effectively recomposing learned, grounded concepts with the generic first-order logic executor. LEFT can be viewed as a generalized framework of VisProg and ViperGPT; in domains where pre-trained models are available and training is not required (e.g., 2D images), LEFT can similarly be used inference-only.

We validate LEFT's performance on four different domains and seven tasks, ranging from 2D question answering, 3D referring expression comprehension, temporal sequence reasoning, and robotic manipulation. A general LEFT model significantly outperforms prior task-specific monolithic methods in performance and in data efficiency settings, and yields comparable performance to prior neuro-symbolic methods while requiring no pre-defined program implementations for each domain. Importantly, the unified LEFT framework can perform concept learning across domains, as well as show zero-shot transfer to three challenging and unseen reasoning tasks. In contrast, inference only LLM-based methods and general vision-language models fail to generalize.

## 2 Related Work

**LLM-based decomposition frameworks.** Our framework integrates neural networks, logic reasoning, and large language models for commonsense reasoning. The first group of related literature studies LLM-based approaches. Inspired by the success of LLMs (Brown et al., 2020), many recent works have proposed frameworks that leverage LLMs to decompose text-based tasks into a sequence of API calls to existing models (Cheng et al., 2023; Dohan et al., 2022; Beurer-Kellner et al., 2023; Zelikman et al., 2023). Compared to LEFT, these methods run inference only with LLMs and API models, and are limited to the language domain, without learning grounding to modalities. For example, LLMs may be able to reason about categories of objects inferred from language, but it cannot recognize the candidate objects from the current scenes, or generate robotics movements to move the object. Works such as Gupta and Kembhavi (2023) and Suris et al. (2023) execute programs on images, but assume API access to a set of available modules with no further training. By contrast, LEFT learns to ground modular concepts across different domains by training concept embeddings for each modality, not requiring any pre-defined and pre-trained modules.

**General vision-language models.** In recent years, unified vision-language models (VLMs) have demonstrated success in learning on multimodal domains (Cho et al., 2021, Tsimpoukelli et al., 2021, Wang et al., 2022, Alayrac et al., 2022). As a representative example, Flamingo (Alayrac et al., 2022) leverages powerful pre-trained vision and language models to achieve few-shot learning on language generation tasks, interleaving images, videos, and text, then outputting associated texts. However, these unified VLMs, and broadly end-to-end methods, are still limited in its application in different domains. In addition, in practice, they struggle with more complex unseen tasks. By contrast, LEFT performs well across domains, and can generalize to new challenging tasks that it has never seen.

**Neuro-symbolic methods.** Neuro-symbolic approaches that combine programs with neural networks have shown success in a variety of visual reasoning domains, with strong data efficiency and generalization abilities from their modular designs. Neuro-symbolic VQA (Yi et al., 2018) proposed symbolic program execution for the question-answering task, and the Neuro-Symbolic Concept Learner (NSCL; Mao et al., 2019) further improved the training paradigm by removing the requirement for dense supervision. Inspired by the success in 2D image domains (Andreas et al., 2016, Johnson et al., 2017, Han et al., 2019, Hudson and Manning, 2019, Li et al., 2020, Chen et al., 2021, Mao et al., 2021), neuro-symbolic methods have been introduced for grounding in 3D scenes (Achlioptas et al., 2020, Hong et al., 2022, Hsu et al., 2023), reasoning in temporal sequences (Chen et al., 2021, Endo et al., 2023), and robotic manipulation (Wang et al., 2023, Mao et al., 2022). However, these neuro-symbolic works require pre-defined domain-specific languages, such that task instructions are parsed into a constrained set of programs, and each program is then manually implemented in code. Therefore, it is challenging to incorporate commonsense reasoning into them, or generalize to different domains. In contrast, LEFT retains all the benefits of neuro-symbolic learning, while proposing a universal language for all domains. LEFT requires neither any domain-specific definition nor domain-specific program examples. It only leverages structures encoded through minimal prompting examples of first-order logic usage for the LLM; hence, it is generally applicable across domain.

## 3 Logic-Enhanced Foundation Model (LEFT)

Our Logic-Enhanced Foundation Model (LEFT) is a unified framework for concept learning and reasoning across different domains and tasks. It integrates large language models with differentiable logic modules, and modular neural networks for grounding concepts in each modality. Shown in Fig. 2, our system consists of three main components:

1. The first is a domain-independent LLM language interpreter, which generates first-order logic queries to the FOL execution engine (Section 3.1). The generated symbolic programs are represented with a hierarchical first-order logic structure.
2. The second is a domain-independent FOL executor, which executes the logic programs differentiably based on features of entities in the domain: objects, relations, actions, etc. (Section 3.2). LEFT's executor is fully differentiable, which allows for backpropagation.
3. The third is the domain-specific grounding modules, which consist of encoders for each modality that extract entity-centric and relational features, as well as the corresponding concept embeddings implemented as modular neural networks (Section 3.3). The concept grounding modules are initialized with concepts generated by the LLM interpreter.

LEFT is a framework of domain-independent reasoning modules (composed of the LLM interpreter and the FOL executor) and domain-specific grounding modules. With these components, LEFT conducts concept learning and reasoning across domains and tasks.

### Domain-independent LLM interpreter

We leverage a pre-trained large language model as our domain-independent language interpreter. It takes language queries as inputs and generates first-order logic queries to downstream modules. The examples we used to prompt the LLMs is a minimal set of general, domain-independent examples and system-level description of syntax rules in our FOL language. The LLM handles both natural language understanding and commonsense reasoning. For example, it needs to resolve coreferences and ambiguous modifier attachments: "Look at the book on the shelf, _the blue one_, select the object left of _it_." Furthermore, it performs commonsense reasoning, such as "Find an object that can fit into a row of blue spheres." The query it generates for the downstream FOL reasoning module will be non-ambiguous programs: \(\mathit{\iota x.blue}(x)\wedge\mathit{sphere}(x)\). This strategy maximally harnesses the reasoning capability of LLMs across tasks, while retaining strong generalization. We show in experiments that the LEFT interpreter can reason about analogies, resolve coreferences, and tackle complex puzzles.

Formally, given an input task query \(Q\) in language, our LLM interpreter outputs programs \(P\), which are programs written in first-order logic that will be passed to downstream FOL executors. Each program is composed of a function name and arguments, and can be chained together hierarchically such that the output of one function can be the input argument to another function. The LLM-interpreted program contains domain-independent FOL functions, which can take outputs from domain-specific functions as arguments. The FOL functions are a general set of pre-defined programs with built-in implementations and are listed in full in Table 1. These functions are either logic and numeric operations (such as counting, forall, exists), or functions that handle inputs and outputs (e.g., return a text description of object categories, or execute an action output by other modules). We present the syntax and built-in components of our FOL language in the next section.

LEFT leverages GPT-3.5 [Brown et al., 2020] as its LLM backbone. Importantly, the LEFT prompts are domain-independent, including simple syntax rules and examples written with general concepts such as _cat_ and _apple_ that do not occur in the any of our evaluation domains. Examples in our prompt also have minimal complexity, such as first-order logic sentences for _"Is there an apple next to the cake_", paired with short descriptions of FOL usage, such as _"to classify whether two objects have a certain property, use 'on(x, y)'_." Additionally, we use a step-by-step prompting paradigm, which

Figure 2: At the core of LEFT is a foundation model-powered language interpreter that generates first-order logic (FOL) programs from language queries, a domain-independent, differentiable FOL executor that operates on all types of entities, and domain-specific grounding modules that connect symbols in the first-order logic language with domain-specific features.

asks the LLM to first reason about the query and simplify the text in language form, then generate FOL programs.

### Domain-independent first-order logic executor

The LEFT domain-independent executor operates on first-order logic, which serves as a general reasoning language shared across different tasks. Given the parsed FOL programs \(P\) from the LLM interpreter, the executor operates \(P\) with grounding modules to return the final answer. The programs are executed recursively to model the hierarchical structure of the FOL reasoning process. The LEFT programs are implemented as differentiable first-order logic modules in Python code.

Our FOL language contains three parts: first, basic first-order logic operations such as Boolean operations and logic quantifiers; second, built-in functions such as _describe_, which generate a natural language response for an entity or a pair of entities, and _do_, which execute a robotic action; and third, domain-specific concept names, including object properties (e.g., _cat_), object relations (e.g., _left_), and actions (e.g., _pack_). These concept predicates are automatically produced by the LLM based on the language query, instead of being selected from a pre-defined set of functions; hence, the expressiveness of LEFT is not limited by the given set of primitives such as exists and count. It can learn to execute any new functions proposed by the LLM.

For a given universe of entities, the LEFT executor executes all expressions using a tensor-based representation of truth values. For example, given a scene with \(N\) objects, the execution result of the expression \(\mathit{sphere}(x)\), where \(x\) is an unquantized variable, can be represented as a vector of length \(N\), where each entry \(i\) denote whether object \(i\) is a sphere. Relation expressions such as \(\mathit{left}(x,y)\) can be represented as matrices of size \(N\times N\). Whether an object has been selected can be represented similarly, as a vector of size \(N\), where each entry \(i\) is the probability of object \(i\) being selected. In practice, values such as \(\mathit{sphere}(x)\) and \(\mathit{left}(x,y)\) will be predicted by domain-specific grounding neural networks. In Table 1, we detail the execution strategy for all operations. Essential to our goal of a general neuro-symbolic framework, LEFT only requires these listed FOL functions to reason across numerous domains, including 2D, 3D, temporal, and robotic manipulation.

The programs generated from our LLM interpreter include FOL programs along with domain-specific concepts, which are executed by our grounding modules. The LEFT executor implements these programs based on the arity (e.g., number of arguments in a function) of the first-order logic program. Unary-level functions are implemented with entity-centric features of size \(N\) for each entity from the input, and binary-level functions are implemented with relational features of size \(N\times N\), etc. Notably, each domain-specific concept is initialized by the LLM, and the function implementation is executed with modular neural networks, which does not require manual definition in code.

\begin{table}
\begin{tabular}{l l l} \hline \hline Name & Syntax & Description \\ \hline variables & \(x,y,z,\cdots\) & Variables that refer to entities in the domain. \\ \(\exists,\forall\) & \(\mathit{exists}(var,\mathit{expr})\) & \(\mathit{expr}\) is an expression that contains the variable _var_; return true if there is at least one assignment of _var_ that satisfies _expr_. \\ iota (\(\iota\)) & \(\mathit{iota}(var,\mathit{expr})\) & \(\mathit{expr}\) is an expression that contains the variable _var_; return an assignment to _var_’s that satisfies _expr_. \\ not & \(\mathit{not\ expr}\) & Compute the negation of an expression. \\ and, or & \(\mathit{expr}_{1}\) and \(\mathit{expr}_{2}\) & Compute the conjunction/disjunction of an expression. \\ count & \(\mathit{count}(var,\mathit{expr})\) & Count the number of assignments to _var_ that will make _expr_ true. \\ \(=,<,>\) & \(\mathit{eq}(\mathit{expr}_{1},\mathit{expr}_{2})\) & Built-in number comparison functions. \\ view & \(\mathit{view}(\mathit{expr})\) & \(\mathit{expr}\) is an object, e.g., \(\mathit{view}(\mathit{lx}.\mathit{blue}(x))\); it changes the view direction to \\ describe & \(\mathit{describe}(\mathit{expr})\) & \(\mathit{expr}\) is a text description, e.g., \(\mathit{describe}(\mathit{uc.color}(c,\mathit{lx}.\mathit{sphere}(x)))\); it returns the description (the color of the sphere). \\ do & \(\mathit{do}(\mathit{expr})\) & \(\mathit{expr}\) is an action, e.g., \(\mathit{do}(\mathit{iota}.\mathit{pick\_\text{-up}}(\mathit{u},\mathit{lx}. \mathit{sphere}(x)))\); it executes the action (pick up the sphere). \\ concepts & \(\mathit{left}(x,y)\) & A function that takes objects, actions, and texts as inputs and returns Boolean values. \\ \hline \hline \end{tabular}
\end{table}
Table 1: LEFT’s first-order logic language, a general reasoning language across domains.

**Execution.** We describe implementations for the FOL programs below. See Appendix for details.

_exists(var, expr)_: We first recursively execute _expr_ and get a tensor where _var_ is one of the dimensions of the tensor. We perform a max pooling over that dimension. For example, the execution of \(\mathit{sphere}(x)\) yields a vector of length \(N\), for \(\mathit{exists}(x,\mathit{sphere}(x))\), we take the max value of the returned tensor.

_iota(var, expr)_: We recursively execute _expr_ and get a vector where _var_ corresponds to the dimensions of the vector. We then perform a softmax operation over the vector.

_and(\(\mathit{expr}_{1}\), \(\mathit{expr}_{2}\))_: The recursive execution of \(\mathit{expr}_{1}\) and \(\mathit{expr}_{2}\) will yield two tensors. The conjunction of two resulting tensors is done by taking the element-wise minimum operation for two tensors. The same execution strategy is used for or operations (by taking the element-wise maximum) and logical negation (by taking the element-wise negation).

_count(var, expr)_: We recursively execute _expr_, then perform a sigmoid operation followed by a sum over the vector to compute the expected number of objects.

_eq(\(\mathit{expr}_{1}\), \(\mathit{expr}_{2}\))_: The recursive execution of \(\mathit{expr}_{1}\) and \(\mathit{expr}_{2}\) will yield two scalar values \(s_{1}\) and \(s_{2}\). We define the probability of \(s_{1}=s_{2}\) as \(\sigma\left(\alpha\cdot\left(\gamma-\left|s_{1}-s_{2}\right|\right)\right)\), where \(\alpha=8\) and \(\gamma=0.25\) are fixed hyperparameters. Less than and greater than operations are implemented similarly.

_left(\(x,y\right)\)_: As an example of a domain-specific concept, the _left_ function takes two variables as its input, and generates a score matrix of size \(N\times N\). This is done by applying \(\mathit{MLP}_{\mathit{left}}\) on the binary embedding extracted by the feature extractor. Note that throughout the execution, we keep the "logit" of the predicted values instead of normalizing them to \([0,1]\) with a sigmoidal activation. This improves the numerical stability of the execution.

### Domain-specific grounding modules

The final component of LEFT is the domain-specific grounding modules that connect concepts in the first-order logic language with modality-specific representations. LEFT does not depend on a pre-defined set of lexical concepts, and relies on LLMs to automatically extract concept names from language in the data. Our framework then initializes domain-specific grounding modules accordingly to learn the grounding of these concepts from training data. In other words, the grounding modules do not require manual definitions, can support execution of any given query, and are realized with a generic mechanism described below. The grounding modules are implemented with modular neural networks, consisting of modality-specific encoders and concept embeddings. We first extract entity-centric and relational features with modality-specific encoder \(\varepsilon\). For 2D images, \(\varepsilon\) is Faster R-CNN [Ren et al., 2015] for encoding object-centric features and DenseCLIP [Rao et al., 2022] for generating object attention masks. For 3D point clouds, \(\varepsilon\) is PointNet++ [Qi et al., 2017]; and for motion sequences, \(\varepsilon\) is the Two-Stream Adaptive Graph Convolutional Network [Shi et al., 2019]. Unary features are represented as a set of features for each entity, binary features as a 2D matrix of relations between each pair of entities, and ternary relations as a 3D matrix, etc.

Given entity-centric representations, we then execute concept functions such as _sphere_ and _left_(x, y). In general, we can have arbitrary neural networks that take the relational representation of entities and outputs a (soft) Boolean value from \([0,1]\). In this paper, we initialize a multi-layer perceptron (MLP) for each concept that has been mentioned by the large language model, to represent the concept embedding. For example, in our 2D image domain, each object \(i\) has a vector embedding \(\mathit{obj}_{i}\) extracted by a Faster R-CNN encoder. We apply \(\mathit{MLP}_{\mathit{sphere}}\) on each object representation \(\mathit{obj}_{i}\) and generate a vector of length \(N\), which is the execution result for the expression \(\mathit{sphere}(x)\).

The grounding modules are automatically initialized based on the concept name as well as _arity_ proposed by the LLM. For example, if a concept is unary (e.g., \(\mathit{red}(\mathrm{x})\)), the grounding module will be a MLP layer that maps object representations to the aforementioned soft Boolean value scalar. For the concept _beside_, the LLM specifies that the concept takes in features of two entities, and hence LEFT creates an MLP that operates on binary features. The domain-specific feature extractors and concept MLPs are trained through backpropagation, as our executor is fully differentiable.

## 4 Experiments

We evaluate LEFT on four domains and seven tasks, and show its effectiveness in multiple settings. In Section 4.1, we compare LEFT to prior state-of-the-art neuro-symbolic and monolithic end-to-endmethods on concept learning across four domains. Our model excels at task performance and also data efficiency, without requiring any domain-specific knowledge. Next, in Section 4.2, we present LEFT's zero-shot generalization performance to three novel and challenging visual reasoning tasks, and compare its reasoning capabilities to that of LLM-based decomposition frameworks and a general vision-language model. Implementation and training details for LEFT can be found in the Appendix.

### Concept learning across domains

We train and validate LEFT on six different datasets, grouped into four domains (Figure 3).

2D visual concept learning.In the first domain, we focus on the task of visual question answering in 2D images: given an image and a language query (e.g., _"How many small cylinders are partially hidden?"_), the goal is to predict an answer from a set vocabulary. The accuracy is calculated as the exact match to ground truth answers. We validate LEFT on the CLEVR and CLEVR-Humans dataset (Johnson et al., 2017), which contain images with objects rendered of different shape, size, material, and color. CLEVR contains synthetically generated queries, while the CLEVR-Humans is a human-annotated dataset with diverse natural language questions.

3D visual concept learning.The second domain evaluates 3D referring expression comprehension. The input is a set of object point clouds in the scene, and a query (e.g. _"The monitor that you would class as in the middle of the other two"_). Models are evaluated by the accuracy of selecting the correct target object. In the 3D domain where we need reference frames to disambiguate relations like _left_, the grounding of such concepts will consider the viewpoint specified in the language query, through a generic high-arity function that does not require a specific implementation. We evaluate LEFT on the ReferIt3D dataset (Achlioptas et al., 2020), with 3D point clouds from ScanNet (Dai et al., 2017), a collection of indoor scenes. ReferIt3D consists of two subsets, SR3D, which focuses on spatial reference with synthetically generated queries, and NR3D, with human-annotated queries. We use the NR3D subset from NS3D, which contain the same set of concepts as in SR3D.

Motion capture concept learning.The third domain contains human motion capture. In particular, we focus on the task of question answering in temporal motion sequences. Given a sequence of human motion, parameterized by joints, and a question (e.g. _"What action does the person do before they move backward and after they kick?"_), the goal is to predict the exact match answer. We train and evaluate LEFT on the HumanMotionQA dataset (Endo et al., 2023), which asks questions about temporal relations and a variety of attributes, including action, direction, and body parts.

Robotic manipulation.Finally, we demonstrate that LEFT can learn action concepts that correspond to robotic motions, in the context of robotic manipulation. The input to this task is an image representing a tabletop environment, and a language instruction (e.g., _"Pack the yoshi figure into the brown box."_) The goal is to generate a sequence of actions that maximizes the average total reward over this robotic manipulation task. We validate LEFT on the CLIPort dataset (Shridhar et al., 2022), focusing on the task of sequential packing Google Scanned Objects (Downs et al., 2022).

#### 4.1.1 Accuracy

We compare LEFT against two categories of top-performing models across all six datasets. The first category contains neuro-symbolic models, which require a pre-defined domain-specific language and implementations for each program. The second category contains monolithic end-to-end methods which usually require a significant amount of data to train. Notably, our unified LEFT framework is able perform on all domains, instead of requiring specific reasoning modules for each setting.

Figure 3: LEFT is evaluated on four broad categories of tasks, ranging from concept learning in 2D images, 3D point clouds, temporal motion sequences, and robotic manipulation.

Table 2 summarizes the results of different models on four datasets that involve synthetically generated language, and hence ground truth programs exist for neuro-symbolic methods and our approach. We see that across CLEVR, SR3D, HumanMotionQA, and CLIPort, LEFT achieves comparable performance to prior neuro-symbolic works (Mao et al., 2019; Hsu et al., 2023; Endo et al., 2023; Wang et al., 2023), and outperforms end-to-end methods in CLEVR, HumanMotionQA, and CLIPort (Hudson and Manning, 2018; Jain et al., 2022; Shridhar et al., 2022). Importantly, our LEFT model applies a unified language interpretation and program execution framework for across all domains and tasks, while prior work on neuro-symbolic learning requires domain-specific languages and reasoning modules for each dataset. This suggests that first-order logic language can serve as an effective and general language for representing language queries.

In Table 3, we examine LEFT on tasks with human-annotated natural language queries. The first number for LEFT represents accuracy out of all queries where the program generated by LLM is executable, while the latter number represents accuracy out of all queries, considering the percentage of LLM outputs that are not executable.

We note that MDETR (Kamath et al., 2021) outperforms LEFT on CLEVR-Humans, possibly because it can better capture the similarity between similar concepts such as _front_ and _in-front-of_. In contrast, the representations for different concepts in LEFT are independent. The prior neuro-symbolic method we compared against outperforms LEFT on NR3D, as it also leverages an LLM for language interpretation, but uses a set of domain-specific examples in prompts that are hand-crafted.

#### 4.1.2 Data efficiency

We evaluate LEFT on data efficiency settings for CLEVR (Johnson et al., 2017) and SR3D (Achlioptas et al., 2020). LEFT significantly outperforms all end-to-end methods at every data efficient percentage point tested, and performs comparably to best-performing neuro-symbolic methods (Hsu et al., 2023; Jain et al., 2022; Huang et al., 2022; Yang et al., 2021; Mao et al., 2019; Mascharka et al., 2018; Hudson and Manning, 2018) (See Table 4 and Table 5). The percentages indicate the percentage of train data used for training compared to the full dataset.

Focusing on CLEVR as an ablation study, we note that NSCL outperforms LEFT on the 10% setting. We attribute this gap to the additional domain-specific inductive biases that NSCL leverages. In particular, NSCL assumes the existence of four attribute spaces (color, shape, material, and size), and each concept lies in exactly one attribute space. Furthermore, all concepts within the same attribute

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline  & Pre-def. & 0.5\% & 1.5\% & 2.5\% & 5\% & 10\% & 100\% \\ \hline NS3D & Yes & \(0.426\) & \(0.520\) & \(0.556\) & \(0.591\) & \(0.600\) & \(0.627\) \\ \hline BUTD-DETR & No & \(0.083\) & \(0.158\) & \(0.259\) & \(0.395\) & \(0.528\) & \(\mathbf{0.670}\) \\ MVT & No & \(0.161\) & \(0.275\) & \(0.322\) & \(0.380\) & \(0.491\) & \(0.645\) \\ SAT & No & \(0.172\) & \(0.260\) & \(0.298\) & \(0.330\) & \(0.362\) & \(0.579\) \\ LEFT & No & \(\mathbf{0.410}\) & \(\mathbf{0.521}\) & \(\mathbf{0.565}\) & \(\mathbf{0.579}\) & \(\mathbf{0.591}\) & \(0.620\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on SR3D in data efficient settings. LEFT performs comparably to prior neuro-symbolic works, and outperforms end-to-end works significantly.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Pre-def. & CLEVR & SR3D & HumanMotionQA & Cliport \\ \hline Prior NS & Yes & \(0.992\) (NSCL) & \(0.627\) (NS3D) & \(0.578\) (NSPose) & \(0.797\) (ProgramPort) \\ \hline End-to-End & No & \(0.989\) (MAC) & \(\mathbf{0.670}\) (BUTD +) & \(0.430\) (MotionCLIP) & \(0.708\) (Cliport) \\ LEFT & No & \(\mathbf{0.996}\) & \(0.620\) & \(\mathbf{0.582}\) & \(\mathbf{0.793}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results comparing LEFT to prior top-performing neuro-symbolic and end-to-end works. Our unified LEFT framework yields strong performance on all settings without any pre-defined programs.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Pre-def. & CLEVR-Humans & NR3D \\ \hline Prior NS & Yes & \(0.678\) (NSVQA) & \(0.526\) (NS3D) \\ \hline End-to-End & No & \(\mathbf{0.817}\) (MDETR) & \(0.430\) (MVT) \\ LEFT & No & \(0.788\) / \(0.698\) & \(\mathbf{0.496}\) / \(\mathbf{0.485}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparisons on natural language tasks.

space are mutually exclusive. Therefore, the inductive biases encoded by NSCL can be seen as a combination of a hierarchical program-based reasoning mechanism, and additional domain-specific priors on concepts. By contrast, LEFT only leverages the program-based reasoning mechanism, which is general across many domains and tasks. In comparison of our model with other baselines and NSCL, we show that this general program-based reasoning mechanism is powerful enough to explain most of the data efficiency improvements.

### Reasoning generalization across tasks

Finally, we present LEFT's ability to zero-shot generalize to unseen tasks.

CLEVR reasoning tasks.We generate three zero-shot transfer tasks based on the CLEVR datasets (Johnson et al., 2017), with \(100\) evaluation examples each. See Figure 4 for an example of each task. The CLEVR-Ref task consists of a CLEVR image and language queries, requiring LEFT to locate the exact target object. The CLEVR-Puzzle task consists of images paired with visual puzzle descriptions, which necessitates a variable assignment puzzle solving. The CLEVR-RPM task is based on Raven's Progressive Matrices, which consists of an image and a 3x3 grid describe by language. This task requires the LLM interpreter to reason about patterns in the grid by row and column to determine the attributes of the target object, and generate queries to the FOL executor to check if such an object exists in the scene. We include dataset details in the Appendix.

#### 4.2.1 Accuracy

We train LEFT on the CLEVR question answering task, and report zero-shot transfer performance on the three unseen reasoning tasks. In Table 6, we see that LEFT achieves strong performance on all datasets. We compare LEFT performance with ground truth programs (GT programs) and with FOL programs from our LLM interpreter (LLM programs). This ablation demonstrates that there are only small drops in performance with LLM programs as compared to ground truth programs across all three complex reasoning tasks. LEFT can zero-shot transfer to novel visual reasoning tasks with LLM generated first-order logic, and effectively reuse concept embeddings, enabling flexible

Figure 4: The three zero-shot generalization tasks. As seen in the CLEVR-Ref example, LEFT’s domain-independent modules resolve the language into first-order logic and generate the answer.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & Pre-def. & 10\% & 25\% & 50\% & 100\% \\ \hline NSCL & Yes & \(0.989\) & \(0.992\) & \(0.994\) & \(0.992\) \\ TbD-Net & Yes & \(0.541\) & \(0.560\) & \(0.912\) & \(0.991\) \\ \hline MAC & No & \(0.673\) & \(0.940\) & \(0.974\) & \(0.989\) \\ LEFT & No & \(\mathbf{0.941}\) & \(\mathbf{0.991}\) & \(\mathbf{0.992}\) & \(\mathbf{0.996}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results on CLEVR in data efficient settings.

generalization. Importantly, we see that LEFT performs well on CLEVR-RPM, a challenging task which contains complex Raven's Progressive Matrices (Barrett et al., 2018) descriptions that require the LLM interpreter to reason about patterns described by language. While the prompts given to the LLM are simple, our interpreter can solve complex tasks.

As baselines, we compare LEFT with LLM-based decomposition frameworks and general vision-language models. VisProg (Gupta and Kembhavi, 2023) and ViperGPT (Suris et al., 2023) are inference only, integrated LLM with APIs frameworks, that has shown success on a variety of visual reasoning tasks. In Table 6, we see that LEFT significantly outperforms VisProg and ViperGPT on the CLEVR transfer tasks. The LLM-based decomposition frameworks often fail to output executable code for complex queries,; in addition, the accuracy of execution by the APIs is low. For example, on the CLEVR-RPM task, only \(0.40\) of queries were executed successfully by ViperGPT, with accuracy of \(0.10\) out of executable queries. As these methods execute programs with a constrained set of pre-defined functions, the accuracy for executing undefined relations, such as _to the right of_, which is learned in LEFT, is low. In addition, for complex domains such as 3D and temporal human motion sequences, it is unclear what pre-defined models should be used for these inference-only frameworks.

For comparison to general vision-language models, we evaluate OpenFlamingo (Awadalla et al., 2023), an open-sourced version of Flamingo (Alayrac et al., 2022). OpenFlamingo is a VL model backed by a CLIP vision encoder (Radford et al., 2021) and a LLAMA language encoder (Touvron et al., 2023). On both 4- and 8-shot variants, LEFT outperforms OpenFlamingo significantly. Notably, OpenFlamingo cannot predict answers for CLEVR-Ref, which requires an object index as output.

## 5 Discussion

We have presented the Logic-Enhanced Foundation Model (LEFT), a unified concept learning and reasoning framework that flexibly learns concepts across different domains, and generalizes reasoning to unseen, complex tasks. Our framework combines foundation models and first-order logic executors as domain-independent reasoning modules, and learnable grounding modules for specific domains. LEFT represents a step towards modeling general reasoning capability in a rightfully grounded way.

LEFT leverages pre-trained LLMs for domain-independent reasoning. Hence, the results of our system are limited by the quality of the LLM, which we cannot finetune. For syntax errors in LLM-generated programs, we can easily detect them and resample. However, we currently cannot recover from semantic errors in the programs. A future direction would be to incorporate execution traces or failures as additional inputs to LLMs such that they can fix the queries they generate.

LEFT may also fail with insufficient domain data, if it cannot effectively train the domain-specific grounding modules. We note that this is a limitation for most learning systems trained solely on one given dataset. To partially mitigate this issue, LEFT instructs LLMs to canonicalize concepts when reasonable, which helps learning certain rare concepts. It is also straightforward to incorporate externally trained modules, by directly integrating pretrained visual models as domain-specific grounding modules. For example, we can plug in Grounding DINO for object detection if we have prior task knowledge (Liu et al., 2023). However, for some domains, there are no off-the-shelf visual recognition models (e.g., relations and human motions); therefore, LEFT can train its concept groundings by learning from data. Notably, LEFT is also more data efficient compared to end-to-end methods due to its modular structure, as it can decompose complex concepts into primitive ones.

Our framework currently does not model the interaction between the language and perception. Instead, LEFT relies on context-dependent grounding modules which learn from data to resolve ambiguous language. Future directions for LEFT include building in pragmatic reasoning frameworks, modelling speaker intents, and leveraging feedback from perception execution.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & CLEVR-Ref & CLEVR-Puzzles & CLEVR-RPM \\ \hline LEFT + GT programs & \(1.00\) & \(0.92\) & \(1.00\) \\ \hline LEFT + LLM programs & \(\mathbf{0.94}\) & \(\mathbf{0.75}\) & \(\mathbf{0.87}\) \\ VisProg (Gupta and Kembhavi, 2023) & \(0.35\) & \(0.27\) & \(0.51\) \\ ViperGPT (Suris et al., 2023) & \(0.08\) & \(0.34\) & \(0.04\) \\ OpenFlamingo 4-shot (Awadalla et al., 2023) & N/A & \(0.54\) & \(0.52\) \\ OpenFlamingo 8-shot & N/A & \(0.57\) & \(0.52\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Zero-shot generalization of LEFT to three unseen tasks; LEFT recomposes domain-independent programs and learned concepts with the LLM interpreter for flexible transfer.

Acknowledgments.We thank Weiyu Liu and Gabriel Poesia for providing valuable feedback on the paper. This work is in part supported by the Stanford Institute for Human-Centered Artificial Intelligence (HAI), ONR MURI N00014-22-1-2740, ONR N00014-23-1-2355, Air Force Office of Scientific Research (AFOSR) YIP FA9550-23-1-0127, the Center for Brain, Minds, and Machines (CBMM), the MIT Quest for Intelligence, MIT-IBM Watson AI Lab, Analog Devices, Autodesk, J.P. Morgan, and Salesforce. JH is supported by the Knight Hennessy Scholarship and the NSF Graduate Research Fellowship.

## References

* Achlioptas et al. (2020) Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. ReferI3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes. In _ECCV_, 2020.
* Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a Visual Language Model for Few-Shot Learning. In _NeurIPS_, 2022.
* Andreas et al. (2016) Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to Compose Neural Networks for Question Answering. In _NAACL-HLT_, 2016.
* Awadalla et al. (2023) Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. OpenFlamingo: An Open-source Framework for Training Large Autoregressive Vision-language Models. _arXiv preprint arXiv:2308.01390_, 2023.
* Barrett et al. (2018) David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring Abstract Reasoning in Neural Networks. In _ICML_, 2018.
* Beurer-Kellner et al. (2023) Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. Prompting Is Programming: A Query Language for Large Language Models. In _PLDI_, 2023.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language Models are Few-Shot Learners. In _NeurIPS_, 2020.
* Chen et al. (2021a) Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, and Jingjing Liu. Meta Module Network for Compositional Visual Reasoning. In _WACV_, 2021a.
* Chen et al. (2021b) Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B Tenenbaum, and Chuang Gan. Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning. In _ICLR_, 2021b.
* Cheng et al. (2023) Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding Language Models in Symbolic Languages. In _ICLR_, 2023.
* Cho et al. (2021) Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying Vision-and-Language Tasks via Text Generation. In _ICML_, 2021.
* Dai et al. (2017) Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. In _CVPR_, 2017.
* Dohan et al. (2022) David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language Model Cascades. In _ICML Workshop_, 2022.
* Dong et al. (2019) Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural Logic Machines. In _ICLR_, 2019.
* Downs et al. (2022) Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household Items. In _ICRA_, 2022.
* Endo et al. (2023) Mark Endo, Joy Hsu, Jiaman Li, and Jiajun Wu. Motion Question Answering via Modular Motion Programs. In _ICML_, 2023.
* Gupta and Kembhavi (2023) Tanmay Gupta and Aniruddha Kembhavi. Visual Programming: Compositional Visual Reasoning without Training. In _CVPR_, 2023.
* Gupta et al. (2020)Chi Han, Jiayuan Mao, Chuang Gan, Josh Tenenbaum, and Jiajun Wu. Visual Concept-Metaconcept Learning. In _NeurIPS_, 2019.
* Hong et al. (2022) Yining Hong, Yilun Du, Chunru Lin, Josh Tenenbaum, and Chuang Gan. 3D Concept Grounding on Neural Fields. In _NeurIPS_, 2022.
* Hsu et al. (2023) Joy Hsu, Jiayuan Mao, and Jiajun Wu. NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations. In _CVPR_, 2023.
* Huang et al. (2022) Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-View Transformer for 3D Visual Grounding. In _CVPR_, 2022.
* Hudson and Manning (2019) Drew Hudson and Christopher D Manning. Learning by Abstraction: The Neural State Machine. In _NeurIPS_, 2019.
* Hudson and Manning (2018) Drew A Hudson and Christopher D Manning. Compositional Attention Networks for Machine Reasoning. In _ICLR_, 2018.
* Jain et al. (2022) Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katerina Fragkiadaki. Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds. In _ECCV_, 2022.
* Johnson et al. (2017a) Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. In _CVPR_, 2017a.
* Johnson et al. (2017b) Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Inferring and Executing Programs for Visual Reasoning. In _ICCV_, 2017b.
* Modulated Detection for End-to-End Multi-Modal Understanding. In _ICCV_, 2021.
* Li et al. (2020) Qing Li, Siyuan Huang, Yining Hong, Yixin Chen, Ying Nian Wu, and Song-Chun Zhu. Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning. In _ICML_, 2020.
* Liu et al. (2019) Runtao Liu, Chenxi Liu, Yutong Bai, and Alan L. Yuille. CLEVR-Ref+: Diagnosing Visual Reasoning With Referring Expressions. In _CVPR_, 2019.
* Liu et al. (2023) Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding DINO: Marrying DINO with Grounded Pre-training for Open-set Object Detection. _arXiv preprint arXiv:2303.05499_, 2023.
* Mao et al. (2019) Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision. In _ICLR_, 2019.
* Mao et al. (2021) Jiayuan Mao, Haoyue Shi, Jiajun Wu, Roger P. Levy, and Joshua B. Tenenbaum. Grammar-Based Grounded Lexicon Learning. In _NeurIPS_, 2021.
* Mao et al. (2022) Jiayuan Mao, Tomas Lozano-Perez, Joshua B. Tenenbaum, and Leslie Pack Kaelbing. PDSketch: Integrated Domain Programming, Learning, and Planning. In _NeurIPS_, 2022.
* Mascharka et al. (2018) David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning. In _CVPR_, 2018.
* Qi et al. (2017) Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. In _NeurIPS_, 2017.
* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In _ICML_, 2021.
* Rao et al. (2022) Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting. In _CVPR_, 2022.
* Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In _NeurIPS_, 2015.
* Shi et al. (2019) Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition. In _CVPR_, 2019.
* Shi et al. (2019)* Shridhar et al. (2022) Mohit Shridhar, Lucas Manuelli, and Dieter Fox. CLIPort: What and Where Pathways for Robotic Manipulation. In _CoRL_, 2022.
* Suris et al. (2023) Didac Suris, Sachit Menon, and Carl Vondrick. ViperGPT: Visual Inference via Python Execution for Reasoning. In _ICCV_, 2023.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLAMA: Open and Efficient Foundation Language Models. _arXiv:2302.13971_, 2023.
* Tsimpoukelli et al. (2021) Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal Few-Shot Learning with Frozen Language Models. In _NeurIPS_, 2021.
* Wang et al. (2022) Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. In _ICML_, 2022.
* Wang et al. (2023) Renhao Wang, Jiayuan Mao, Joy Hsu, Hang Zhao, Jiajun Wu, and Yang Gao. Programmatically Grounded, Compositionally Generalizable Robotic Manipulation. In _ICLR_, 2023.
* Yang et al. (2021) Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo Luo. SAT: 2D Semantics Assisted Training for 3D Visual Grounding. In _ICCV_, 2021.
* Yi et al. (2018) Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding. In _NeurIPS_, 2018.
* Zelikman et al. (2023) Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D Goodman, and Nick Haber. Parsel: A (De-)compositional Framework for Algorithmic Reasoning with Language Models. _arXiv:2212.10561_, 2023.

**Supplementary for What's _Left_? Concept Grounding**

**with Logic-Enhanced Foundation Models**

The appendix is organized as the following. In Appendix A, we formally define the first-order logic programs used by LEFT as well as discuss the broader impact of our method. In Appendix B, we present experiment results with error bars and details on the construction of zero-shot transfer tasks. In Appendix C, we provide implementation, training details, and code.

## Appendix A Left

### Function definitions

We summarize the function definitions of LEFT in Table 7. We note that \(\mathcal{E}(\textit{expr})\) represents the execution result of _expr_. If the input to \(concept\) functions contains a concrete entity (e.g., result of _iota_), we take the dot product between the entity and the MLP result. If the argument is text (e.g., color), the function is implemented as a generative function instead of a discriminative one. Ternary and high-arity relations can be generalized similarly to unary and binary relations.

\begin{table}
\begin{tabular}{l l} \hline \hline Signature \& Implementation & Semantics \\ \hline _exists(var, expr)_\(\longrightarrow\)_\(b\): boolean_ & Return true if there is at least one assignment of \\
**Implementation:**\(b=\textit{max}(\mathcal{E}(\textit{expr}))\) & _var_ that satisfies _expr_. \\ \hline _forall(var, expr)_\(\longrightarrow\)_\(b\): boolean_ & Return true if there is all assignments to _var_ satisfy _expr_. \\ \hline _iota(var, expr)_\(\longrightarrow\)_\(e\): entity_ & Return an assignment to _var_’s that satisfies _expr_. \\
**Implementation:**\(e=\textit{softmax}(\mathcal{E}(\textit{expr}))\) & \\ \hline _not(expr)_\(\longrightarrow\)_\(e\): boolean_ & Compute the negation of an expression. \\
**Implementation:**\(e_{i}=-\mathcal{E}(\textit{expr})_{i}\), for all \(i\in\{1,2,\cdots,N\}\) & \\ \hline _and(expr_\({}_{1}\), _expr_\({}_{2}\))_\(\longrightarrow\)_\(e\): boolean_ & Compute the conjunction of two expressions. \\
**Implementation:**\(e_{i}=\textit{min}(\mathcal{E}(\textit{expr}_{1})_{i},\mathcal{E}(\textit{expr}_{2}) _{i})\), for all \(i\in\{1,2,\cdots,N\}\) & \\ \hline _or(expr_\({}_{1}\), _expr_\({}_{2}\))_\(\longrightarrow\)_\(e\): boolean_ & Compute the disjunction of two expressions. \\
**Implementation:**\(e_{i}=\textit{max}(\mathcal{E}(\textit{expr}_{1})_{i},\mathcal{E}(\textit{expr}_{2}) _{i})\), for all \(i\in\{1,2,\cdots,N\}\) & \\ \hline _count(var, expr)_\(\longrightarrow\)_\(c\): integer_ & Return the count of assignments to _var_’s that satisfies _expr_. \\
**Implementation:**\(c=\sum_{i}\left[\sigma(\mathcal{E}(\textit{expr})_{i})\right]\) & Return a score indicating whether \(\mathcal{E}(\textit{expr}_{1})\) is greater than \(\mathcal{E}(\textit{expr}_{2})\). \\ \hline _greater_\(\textit{plan}(\textit{expr}_{1},\textit{expr}_{2})\)\(\longrightarrow\)_\(b\): boolean_ & Return a score indicating whether \(\mathcal{E}(\textit{expr}_{1})\) is greater than \(\mathcal{E}(\textit{expr}_{2})\). \\
**Implementation:**\(b=\sigma(\tau\cdot(\mathcal{E}(\textit{expr}_{1})-\mathcal{E}(\textit{expr}_{2}) -\gamma))\) & \\ \hline _view(expr)_ & Set the object returned by \(\mathcal{E}(\textit{expr})\) as the object that the agent is looking at. \\
**Implementation:**\(\mathcal{E}(\textit{expr})\) & Perform action computed by \(\mathcal{E}(\textit{expr})\). \\ \hline _do(expr)_ & Perform action computed by \(\mathcal{E}(\textit{expr})\). \\
**Implementation:**\(\mathcal{E}(\textit{expr})\) & \\ \hline _describe(var, expr)_\(\longrightarrow\)_\(e\): text_ & Return the best assignment to _var_ (e.g., colors, shapes, etc.) that describes the object returned by \(\mathcal{E}(\textit{expr})\). \\
**Implementation:**\(e=\textit{softmax}(\mathcal{E}(\textit{expr}))\) & A unary function that takes objects, actions, and \\ \hline _concept(var\({}_{2}\))_\(\longrightarrow\)_\(e\): boolean_ & A unary function that takes objects, actions, and \\
**Implementation:**\(e_{i}=\text{MLP}^{\textit{construct}}\left(f_{i}^{\textit{trans}}\right)\) & texts as inputs and returns Boolean values. \\ \hline _concept(var\({}_{2}\), var\({}_{3}\))_\(\longrightarrow\)_\(e\): boolean_ & A binary function that takes objects, actions, and \\
**Implementation:**\(e_{i,j}=\text{MLP}^{\textit{project}}(f_{i,j}^{\textit{binary}})\) & texts as inputs and returns Boolean values. \\ \hline \hline \end{tabular}
\end{table}
Table 7: Function definitions for first-order logic programs in LEFT.

### Broader impact

LEFT leverages a pre-trained large language model as its language interpreter, and hence, even though our prompts are general examples of first-order logic, we do not have direct control over the LLM's generation. The LLM may output harmful biases, which we will highlight as an important warning in our code release.

Experiments

### Error bars

We present LEFT performance with error bars, taken from three runs with different seeds. The results for CLEVR (Johnson et al., 2017), CLEVR-10%, and CLEVR-Humans are below. All values are averaged over three random seeds, and the \(\pm\) sign shows the computed standard error.

### Transfer task construction

We detail the construction processes for CLEVR-Ref, CLEVR-Puzzles, and CLEVR-RPM below.

CLEVR-RefWe generate the CLEVR-Ref dataset using a subset of the templates from the CLEVR-Ref+ (Liu et al., 2019) dataset. We did not directly use the CLEVR-Ref+ dataset because it contains ordinal number concepts (in particular, the first, the second, etc.) that does appear in the original CLEVR dataset. Concretely, we used the following four templates:

1. {Select, Find, Point to} the _Object 1_.
2. {Select, Find, Point to} the _Object 1_ (that is) _Relation 1_ the _Object 2_.
3. {Select, Find, Point to} the _Object 1_ (that is) _Relation 1_ the _Object 2_ and _Relation 2_ the _Object 3_.
4. There is a(n) _Object 2_ (that is) _Relation 2_ the _Object 3_, {select, find, point to} the _Object 1_ (that is) _Relation 1_ it.

Here, items such as _Object 1_, _Object 2_, and _Object 3_ will be replaced by object-level descriptors such as "shiny cube." _Relation 1_ and _Relation 2_ will be replaced by relational concepts in the dataset.

CLEVR-PuzzlesThe CLEVR-Puzzles dataset is generated based on the following template: "Can you find four objects from the image such that: _Object \(i\) is description_; _Object \(i\) is relation object j_." Here, _Object \(i\), object j_ will be replaced by "Object 1," "Object 2," "Object 3," or "Object 4;" descriptions will be replaced by object-level descriptions such as "shiny cube;" relations will be replaced by relational concepts in the dataset. In our data generation, we use four object descriptions (one for each object) and three object relation descriptions. Furthermore, we make sure that none of the object-level descriptions can uniquely identify an object, but the global solution is unique.

CLEVR-RpmThe CLEVR-RPM dataset is generated based on the following template: "There are 9 objects, ordered in a 3x3 grid: row 1 col 1 is a _description 1_; row 1 col 2 is a _description 2_; row 1 col 3 is a _description 3_; row 2 col 1 is a _description 4_; row 2 col 2 is a _description 5_; row 2 col 3 is a _description 6_; row 3 col 1 is a _description 7_; row 3 col 2 is a _description 8_; I am missing one object at row 3 col 3. Can you find an object in the scene that can fit there?" Here, descriptions are object-level descriptions such as "shiny cube." Following existing work on Raven's Progressive Matrices (Barrett et al., 2018), we randomly choose the 'progression dimensions" from color, shape, material, and size for rows and columns.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & CLEVR-10\% & CLEVR-100\% & CLEVR-Humans \\ \hline LEFT & 0.942\({}_{\pm 0.008}\) & 0.996\({}_{\pm 0.001}\) & \(0.780\)\({}_{\pm 0.003}\) / \(0.694\)\({}_{\pm 0.001}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results on the CLEVR dataset with standard errors. Average performance and standard errors are computed based on three random seeds.

Model

### Implementation details

LEFT consists of several modular components in code. The first is a GPT-3.5 (Brown et al., 2020) backed language interpreter, which takes as input queries from each dataset, and outputs first-order logic. The domain initializer takes the generated first-order logic programs and automatically initializes concept embeddings based on the parsed arity. The domain-specific grounding modules extract entity and relational features from the input modality. Finally, the domain-independent executor executes differentiable logic programs with the outputs of the grounding modules. We release our code below.

### Training details

We use the official data splits released for each dataset, CLEVR (Johnson et al., 2017), ReferIt3D (Achlioptas et al., 2020), HumanMotionQA (Endo et al., 2023), and Cliport (Shridhar et al., 2022). The core hyperparameters were set as \(128\) for concept embedding dimensions, and learning rate taken from prior neuro-symbolic concept learning repositories that we use as baselines. The full details are provided with the code, linked below.

### Compute

We trained with 1 NVIDIA Titan RTX per experiment for all datasets, from an internal cluster.

### Code

We publicly release our code. See our project website for additional details.