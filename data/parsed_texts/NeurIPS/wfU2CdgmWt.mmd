# Stochastic Optimal Control Matching

Carles Domingo-Enrich

NYU & FAIR, Meta

cd2754@nyu.edu

&Jiequn Han

Flatiron Institute

jhan@flatironinstitute.org

Brandon Amos

FAIR, Meta

bda@meta.com

&Joan Bruna

NYU & Flatiron Institute

bruna@cims.nyu.edu

&Ricky T. Q. Chen

FAIR, Meta

rtqichen@meta.com

###### Abstract

Stochastic optimal control, which has the goal of driving the behavior of noisy systems, is broadly applicable in science, engineering and artificial intelligence. Our work introduces Stochastic Optimal Control Matching (SOCM), a novel Iterative Diffusion Optimization (IDO) technique for stochastic optimal control that stems from the same philosophy as the conditional score matching loss for diffusion models. That is, the control is learned via a least squares problem by trying to fit a matching vector field. The training loss, which is closely connected to the cross-entropy loss, is optimized with respect to both the control function and a family of reparameterization matrices which appear in the matching vector field. The optimization with respect to the reparameterization matrices aims at minimizing the variance of the matching vector field. Experimentally, our algorithm achieves lower error than all the existing IDO techniques for stochastic optimal control for three out of four control problems, in some cases by an order of magnitude. The key idea underlying SOCM is the path-wise reparameterization trick, a novel technique that may be of independent interest.

## 1 Introduction

Stochastic optimal control aims to drive the behavior of a noisy system in order to minimize a given cost. It has myriad applications in science and engineering: examples include the simulation of rare events in molecular dynamics [37, 36, 85, 41], finance and economics [63, 25], stochastic filtering and data assimilation [58, 68], nonconvex optimization [19], sampling [9], power systems and energy markets [8, 66], and robotics [77, 32]. Stochastic optimal has also been impactful in fields such as mean-field games [17], optimal transport [80, 81], backward stochastic differential equations (BSDEs) [14] and large deviations [24]. Recently, it has been the basis of algorithms to sample from unnormalized densities [84, 79, 9, 71].

For continuous-time problems with low-dimensional state spaces, the standard approach to learn the optimal control is to solve the Hamilton-Jacobi-Bellman (HJB) partial differential equation (PDE) by gridding the space and using classical numerical methods. For high-dimensional problems, a large number of works parameterize the control using a neural network and train it applying a stochastic optimization algorithm on a loss function. These methods are known as _Iterative Diffusion Optimization_ (IDO) techniques [59] (see Subsec. 2.2).

It is convenient to draw an analogy between stochastic optimal control and _continuous normalizing flows_ (CNFs), which are a generative modeling technique where samples are generated by solving an ordinary differential equation (ODE) for which the vector field has been learned, initialized at a Gaussian sample. CNFs were introduced by [20] (building on top of Rezende and Mohamed [70]),and training them is similar to solving control problems because in both cases one needs to learn high-dimensional vector fields using neural networks, in continuous time.

The first algorithm developed to train normalizing flows was based on maximizing the likelihood of the generated samples (Kendal et al., 2016, Sec. 4). Obtaining the gradient of the maximum likelihood loss with respect to the vector field parameters requires backpropagating through the computation of the ODE trajectory, or equivalently, solving the _adjoint_ ODE in parallel to the original ODE. Maximum likelihood CNFs (ML-CNFs) were superseded by diffusion models (Kendal et al., 2016; Kendal and Kendal, 2017; Kendal et al., 2018) and flow-matching, a.k.a. stochastic interpolant, methods (Zhu et al., 2017; Chen et al., 2017; Chen et al., 2018; Chen et al., 2018), which are currently the preferred algorithms to train CNFs. Aside from architectural improvements such as the UNet (Kendal et al., 2016), a potential reason for the success of diffusion and flow matching models is that their _functional landscape_ is convex, unlike for ML-CNFs. Namely, vector fields are learned by solving least squares regression problems where the goal is to fit a random matching vector field. Convex functional landscapes in combination with overparameterized models and moderate gradient variance can yield very stable training dynamics and help achieve low error.

Returning to stochastic optimal control, one of the best-performing IDO techniques amounts to choosing the control objective (equation 1) as the training loss (see (12)). As in ML-CNFs, computing the gradient of this loss requires backpropagating through the computation of the trajectories of the SDE (2), or equivalently, using an adjoint method. The functional landscape of the loss is highly non-convex, and the method is prone to unstable training (see green curve in the bottom right plot of Figure 3). In light of this, a natural idea is to develop the analog of diffusion model losses for the stochastic optimal control problem, to obtain more stable training and lower error, and this is what we set out to do in our work. Our contributions are as follows:

* We introduce Stochastic Optimal Control Matching (SOCM), a novel IDO algorithm in which the control is learned by solving a least-squares regression problem where the goal is to fit a random _matching vector field_ which depends on a family of _reparameterization matrices_ that are also optimized.
* We derive a bias-variance decomposition of the SOCM loss (Prop. 2). The bias term is equal to an existing IDO loss: the _cross-entropy loss_, which shows that both algorithms have the same landscape in expectation. However, SOCM has an extra flexibility in the choice of reparameterization matrices, which affect only the variance. Hence, we propose optimizing the reparameterization matrices to reduce the variance of the SOCM objective.
* The key idea that underlies the SOCM algorithm is the _path-wise reparameterization trick_ (Prop. 1), which is a novel technique for estimating gradients of an expectation of a functional of a random process with respect to its initial value. It is of independent interest and may be more generally applicable outside of the settings considered in this paper.
* We perform experiments on four different settings where we have access to the ground-truth control. For three of these, SOCM obtains a lower \(L^{2}\) error with respect to the ground-truth control than all the existing IDO techniques, with around \(10\times\) lower error than competing methods in some instances.

## 2 Framework

### Setup and Preliminaries

Let \((\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\geq 0},\mathcal{P})\) be a fixed filtered probability space on which is defined a Brownian motion \(B=(B_{t})_{t\geq 0}\). We consider the control-affine problem

\[\min_{u\in\mathcal{U}}\mathbb{E}\big{[}\int_{0}^{T}\big{(}\tfrac {1}{2}\|u(X_{t}^{u},t)\|^{2}\!+\!f(X_{t}^{u},t)\big{)}\,\mathrm{d}t\!+\!g(X_{T }^{u})\big{]},\] (1) \[\text{s.t.}\ \mathrm{d}X_{t}^{u}\!=\!(b(X_{t}^{u},t)\!+\! \sigma(t)u(X_{t}^{u},t))\,\mathrm{d}t\!+\!\sqrt{\lambda}\sigma(t)\mathrm{d}B _{t},X_{0}^{u}\sim p_{0}\] (2)

and where \(X_{t}^{u}\in\mathbb{R}^{d}\) is the state, \(u:\mathbb{R}^{d}\times[0,T]\to\mathbb{R}^{d}\) is the feedback control and belongs to the set of admissible controls \(\mathcal{U}\), \(f:\mathbb{R}^{d}\times[0,T]\to\mathbb{R}\) is the state cost, \(g:\mathbb{R}^{d}\to\mathbb{R}\) is the terminal cost, \(b:\mathbb{R}^{d}\times[0,T]\to\mathbb{R}^{d}\) is the base drift, and \(\sigma:[0,T]\to\mathbb{R}^{d\times d}\) is the invertible diffusion coefficient and \(\lambda\in(0,+\infty)\) is the noise level. In App. A we formally define the set \(\mathcal{U}\) of admissible controls and describe the regularity assumptions needed on the control functions. In the remainder of the section we introduce relevant concepts in stochastic optimal control; we provide the most relevant proofs in App. B and refer the reader to Oksendal (Oksendal, 2016, Chap. 11) and Nusken and Richter (Nusken and Richter, 2019, Sec. 2) for a similar, more extensive treatment.

**Cost functional and value function** The _cost functional_ for the control \(u\), point \(x\) and time \(t\) is defined as \(J(u;x,t):=\mathbb{E}\big{[}\int_{t}^{T}\big{(}\frac{1}{2}\|u_{s}(X_{s}^{u})\|^{2 }+f_{s}(X_{s}^{u})\big{)}\,\mathrm{d}t+g(X_{t}^{u})\big{|}X_{t}^{u}=x\big{]}.\) That is, the cost functional is the expected value of the control objective restricted to the times \([t,T]\) with the initial value \(x\) at time \(t\). The _value function_ or _optimal cost-to-go_ at \((x,t)\) is defined as the minimum value of the cost functional across all possible controls:

\[V(x,t):=\inf_{u\in\mathcal{U}}J(u;x,t).\] (3)

Hamilton-Jacobi-Bellman equation and optimal controlIf we define the infinitesimal generator \(L:=\frac{\lambda}{2}\sum_{i,j=1}^{d}(\sigma\tau^{\top})_{ij}(t)\partial_{x_{i }}\partial_{x_{j}}+\sum_{i=1}^{d}b_{i}(x,t)\partial_{x_{i}}\), the value function solves the following Hamilton-Jacobi-Bellman (HJB) partial differential equation:

\[(\partial_{t}+L)V(x,t)-\tfrac{1}{2}\|(\sigma^{\top}\nabla V)(x,t)\|^{2}+f(x,t )=0,\qquad\qquad V(x,T)=g(x).\] (4)

The _verification theorem_[62, Sec. 2.3] states that if a function \(V\) solves the HJB equation above and has certain regularity conditions, then \(V\) is the value function (3) of the problem (1)-(2). An implication of the verification theorem is that for every \(u\in\mathcal{U}\),

\[V(x,t)+\mathbb{E}\big{[}\tfrac{1}{2}\int_{t}^{T}\|\sigma^{\top}\nabla V+u\|^{2 }(X_{s}^{u},s)\,\mathrm{d}s\,\big{|}\,X_{t}^{u}=x\big{]}=J(u,x,t).\] (5)

In particular, this implies that the unique optimal control is given in terms of the value function as \(u^{*}(x,t)=-\sigma(t)^{\top}\nabla V(x,t)\). Equation (5) can be deduced by integrating the HJB equation (4) over \([t,T]\), and taking the conditional expectation with respect to \(X_{t}^{u}=x\). We include the proof of (5) in App. B for completeness.

A pair of forward and backward SDEs (FBSDEs)Consider the pair of SDEs

\[\mathrm{d}X_{t} =b(X_{t},t)\,\mathrm{d}t+\sqrt{\lambda}\sigma(t)\mathrm{d}B_{t}, \qquad X_{0}\sim p_{0},\] (6) \[\mathrm{d}Y_{t} =(-f(X_{t},t)+\tfrac{1}{2}\|Z_{t}\|^{2})\,\mathrm{d}t+\sqrt{ \lambda}\langle Z_{t},\mathrm{d}B_{t}\rangle,\qquad Y_{T}=g(X_{T}).\] (7)

where \(Y:\Omega\times[0,T]\to\mathbb{R}\) and \(Z:\Omega\times[0,T]\to\mathbb{R}^{d}\) are progressively measurable 1 random processes. It turns out that \(Y_{t}\) and \(Z_{t}\) defined as \(Y_{t}:=V(X_{t},t)\) and \(Z_{t}:=\sigma(t)^{\top}\nabla V(X_{t},t)=-u^{*}(X_{t},t)\) satisfy (7). We include the proof in App. B for completeness.

Footnote 1: Being progressively measurable is a strictly stronger property than the notion of being a process adapted to the filtration \(\mathcal{F}_{t}\) of \(B_{t}\) (see [50]).

An analytic expression for the value functionFrom the forward-backward equations (6)-(7), one can derive a closed-form expression for the value function \(V\):

\[V(x,t)\!=\!-\lambda\log\mathbb{E}\big{[}\exp\big{(}\!-\!\lambda^{-1}\int_{t}^ {T}f(X_{s},s)\,\mathrm{d}s\!-\!\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x\big{]},\] (8)

where \(X_{t}\) is the solution of the uncontrolled SDE (6). This is a classical result, but we still include its proof in App. B. Given that \(u^{*}(x,t)=-\sigma(t)^{\top}\nabla V(x,t)\), an immediate, yet important, consequence of (8) is the following path-integral representation of the optimal control:

\[u^{*}(x,t)\!=\!\lambda\sigma(t)^{\top}\nabla_{x}\log\mathbb{E}\big{[}\exp \big{(}\!-\!\lambda^{-1}\int_{t}^{T}f(X_{s},s)\,\mathrm{d}s\!-\!\lambda^{-1}g( X_{T})\big{)}\big{|}X_{t}=x\big{]}\;.\] (9)

Remark this equation involves the gradient of logarithm of a conditional expectation, which is reminiscent of the vector fields that are learned when training diffusion models. For example, the target vector field for variance-exploding score-based diffusion loss [76] can be expressed as \(\nabla_{x}\log p_{t}(x)=\nabla_{x}\log\mathbb{E}_{Y\sim p_{\mathrm{data}}}[ \frac{\exp(-\|x-Y\|^{2}/(2\sigma_{t}^{2}))}{(2\pi\sigma_{t}^{2})^{4/2}}]\). Note, however, that in (9) the gradient is taken with respect to the initial condition of the process, which requires the development of novel techniques.

Conditioned diffusionsLet \(\mathcal{C}=C([0,T];\mathbb{R}^{d})\) be the Wiener space of continuous functions from \([0,T]\) to \(\mathbb{R}^{d}\) equipped with the supremum norm, and let \(\mathcal{P}(\mathcal{C})\) be the space of Borel probability measures over \(\mathcal{C}\). For each control \(u\in\mathcal{U}\), the controlled process in equation (2) induces a probability measure in \(\mathcal{P}(\mathcal{C})\), as the law of the paths \(X_{t}^{u}\), which we refer to as \(\mathbb{P}^{u}\). We let \(\mathbb{P}\) be the probability measure induced by the uncontrolled process (6), and define the _work functional_

\[\mathcal{W}(X,t):=\int_{t}^{T}f(X_{s},s)\,\mathrm{d}s+g(X_{T}).\] (10)It turns out (Lemma 2 in App. B) that the Radon-Nikodym derivative \(\frac{d^{\mathrm{Pu}^{*}}}{d^{\mathrm{P}}}\) satisfies \(\frac{dP^{\mathrm{Pu}^{*}}}{d^{\mathrm{P}}}(X)=\exp\big{(}\lambda^{-1}\big{(}V(X _{0},0)-\mathcal{W}(X,0)\big{)}\big{)}\). Also, a straight-forward application of the Girsanov theorem for SDEs (Cor. 1) shows that

\[\frac{dP^{\mathrm{Pu}}}{dP^{\mathrm{Pu}^{*}}}(X^{u^{*}}) = \exp\big{(}-\lambda^{-1/2}\int_{0}^{T}\langle u^{*}(X_{t}^{u^{*} },t)-u(X_{t}^{u^{*}},t),\mathrm{d}B_{t}\rangle-\tfrac{\lambda^{-1}}{2}\int_{0} ^{T}\|u^{*}(X_{t}^{u^{*}},t)-u(X_{t}^{u^{*}},t)\|^{2}\,\mathrm{d}t\big{)},\] (11)

which means that the only control \(u\in\mathcal{U}\) such that \(\mathbb{P}^{u}=\mathbb{P}^{u^{*}}\) is the optimal control itself.

### Existing approaches and related work

Low-dimensional case: solving the HJB equationFor low-dimensional control problems (\(d\leq 3\)), it is possible to grid the domain and use a numerical PDE solver to find a solution to the HJB equation (4). The main approaches include _finite difference methods_[11; 57; 4], which approximate the derivatives and gradients of the value function using finite differences, _finite element methods_[47], which involve restricting the solution to domain-dependent function spaces, and semi-Lagrangian schemes [21; 13; 12], which trace back characteristics and have better stability than finite difference methods. See Greif [33] for an overview on these techniques, and Banas et al. [4] for a comparison between them. Hutzenthaler et al. [44] introduced the multilevel Picard method, which leverages the Feynman-Kac and the Bismut-Elworthy-Li formulas to beat the curse of dimensionality in some settings [6; 46; 45; 43].

High dimensional methods leveraging FBSDEsThe FBSDE formulation in equations (6)-(7) has given rise to multiple methods to learn controls. One such approach is _least-squares Monte Carlo_ (see Pham [63, Chapter 3] and Gobet [28] for an introduction, and Gobet et al. [30], Zhang et al. [83] for an extensive analysis), where trajectories from the forward process (6) are sampled, and then regression problems are solved backwards in time to estimate the expected future cost in the spirit of dynamic programming. A second method that exploits FBSDEs was proposed by E et al. [22], Han et al. [35]. They parameterize the control using a neural network \(u_{\theta}\), and use stochastic gradient algorithms to minimize the loss \(\mathcal{L}(u_{\theta},y_{0})=\mathbb{E}[(Y_{T}(y_{0},u_{\theta})-g(X_{T}))^{2}]\), where \(Y_{T}(y_{0},u_{\theta})\) is the process in (7) with initial condition \(y_{0}\) and control \(u_{\theta}\). This algorithm can be seen as a shooting method, where the initial condition and the control are learned to match the terminal condition. Multiple recent works have combined neural networks with FBSDE Monte Carlo methods for parabolic and elliptic PDEs [5; 18; 86], control [7; 39], multi-agent games [34; 15; 16]; see [23] for a more comprehensive review.

Many of the methods referenced above and some additional ones can be seen from a common perspective using controlled diffusions. As observed in equation (11), the key idea is that learning the optimal control is equivalent to finding a control \(u\) such that the induced probability measure \(\mathbb{P}^{u}\) on paths is equal to the probability measure \(\mathbb{P}^{u^{*}}\) for the optimal control. In the paragraphs below we cover several loss that fall into this framework. All the losses below can be optimized using a common algorithmic framework, which we describe in Algorithm 1. For more details, we refer the reader to Nusken and Richter [59], which introduced this perspective and named such methods _Iterative Diffusion Optimization_ (IDO) techniques. For simplicity, we introduce the losses for the setting in which the initial distribution \(p_{0}\) is concentrated at a single point \(x_{\mathrm{init}}\); we cover the general setting in App. B.

The relative entropy loss and the adjoint methodThe relative entropy loss is defined as the Kullback-Leibler divergence between \(\mathbb{P}^{u}\) and \(\mathbb{P}^{u^{*}}\colon\mathbb{E}_{\mathbb{P}^{u}}[\log\frac{d\mathbb{P}^{u}}{ d\mathbb{P}^{u^{*}}}]\). Upon removing constant terms and factors, this loss is equivalent to (see Lemma 3 in App. B):

\[\mathcal{L}_{\mathrm{Adj}}(u):=\mathbb{E}\big{[}\int_{0}^{T}\big{(}\tfrac{1}{2} \|u(X_{t}^{u},t)\|^{2}+f(X_{t}^{u},t)\big{)}\,\mathrm{d}t+g(X_{T}^{u})\big{]}.\] (12)

This is exactly the control objective in (1). This fact has been studied extensively [10; 31; 36; 48; 67]. Hence, the relative entropy loss is very natural and widely used; see Onken et al. [61], Zhang and Chen [84] for examples on multiagent systems and sampling.

Solving optimization problems of the form (12) has a long history that dates back to Pontryagin [64]. Note that \(\mathcal{L}_{\mathrm{Adj}}(u)\) depends on \(u\) both explicitly, and implicitly through the process \(X^{u}\). To compute the gradient \(\nabla_{\theta}\hat{\mathcal{L}}_{\mathrm{Adj}}(u_{\theta_{n}})\) of a Monte Carlo approximation \(\hat{\mathcal{L}}_{\mathrm{Adj}}(u_{\theta_{n}})\) of \(\mathcal{L}_{\mathrm{Adj}}(u_{\theta_{n}})\) as required by Algorithm 1, we need to backpropagate through the simulation of the trajectories, which is why we do _not_ detach them from the computational graph. One can alternatively compute the gradient \(\nabla_{\theta}\hat{\mathcal{L}}_{\mathrm{Adj}}(u_{\theta_{n}})\) by explicitly solving an ODE, a technique known as the _adjoint method_. The adjoint method was introduced by Pontryagin [64], popularized in deep learning by Chen et al. [20], and further developed for SDEs in Li et al. [54].

The cross-entropy lossThe cross-entropy loss is defined as the Kullback-Leibler divergence between \(\mathbb{P}^{u^{*}}\) and \(\mathbb{P}^{u}\), i.e., flipping the order of the two measures: \(\mathbb{E}_{\mathbb{P}^{u^{*}}}[\log\frac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}^{u }}]\). For an arbitrary \(v\in\mathcal{U}\), this loss is equivalent to the following one (see Prop. 3(i) in App. B):

\[\begin{split}\mathcal{L}_{\mathrm{CE}}(u)&:= \mathbb{E}\big{[}\big{(}-\lambda^{-1/2}\int_{0}^{T}\langle u(X_{t}^{v},t), \mathrm{d}B_{t}\rangle-\lambda^{-1}\int_{0}^{T}\langle u(X_{t}^{v},t),v(X_{t}^ {v},t)\rangle\,\mathrm{d}t+\frac{\lambda^{-1}}{2}\int_{0}^{T}\|u(X_{t}^{v},t) \|^{2}\,\mathrm{d}t\big{)}\\ &\qquad\times\exp\big{(}-\lambda^{-1}\mathcal{W}(X^{v},0)- \lambda^{-1/2}\int_{0}^{T}\langle v(X_{t}^{v},t),\mathrm{d}B_{t}\rangle- \frac{\lambda^{-1}}{2}\int_{0}^{T}\|v(X_{t}^{v},t)\|^{2}\,\mathrm{d}t\big{)} \big{]}.\end{split}\] (13)

The cross-entropy loss has a rich literature [38, 49, 74, 85] and has been recently used in applications such as molecular dynamics [41]. Furthermore, we note that the cross-entropy loss can be significantly simplified and written in terms of the unnormalized \(L^{2}\) error of the control \(u\) with respect to the optimal control \(u^{*}\):

\[\mathcal{L}_{\mathrm{CE}}(u)=\tfrac{\lambda^{-1}}{2}\mathbb{E}\big{[}\int_{0}^ {T}\|u^{*}(X_{t}^{u^{*}},t)-u(X_{t}^{u^{*}},t)\|^{2}\,\mathrm{d}t\times\exp \big{(}-\lambda^{-1}V(X_{0}^{u^{*}},0)\big{)}\big{]}\;.\] (14)

This characterization, which is proven in Prop. 3(ii) in App. B, is relevant for us because a similar one can be written for the loss that we propose (see Prop. 2).

Variance and log-variance lossesFor an arbitrary \(v\in\mathcal{U}\), the _variance_ and the _log-variance losses_ are defined as \(\tilde{\mathcal{L}}_{\mathrm{Var}_{v}}(u)=\mathrm{Var}_{\mathbb{P}^{v}}( \frac{\mathrm{d}\mathbb{P}^{u^{*}}}{\mathrm{d}\mathbb{P}^{u}})\) and \(\tilde{\mathcal{L}}_{\mathrm{Var}_{v}}^{\log}(u)=\mathrm{Var}_{\mathbb{P}^{v}} (\log\frac{\mathrm{d}\mathbb{P}^{u^{*}}}{\mathrm{d}\mathbb{P}^{u}})\) whenever \(\mathbb{E}_{\mathbb{P}^{v}}|\frac{\mathrm{d}\mathbb{P}^{u^{*}}}{\mathrm{d} \mathbb{P}^{u}}|<+\infty\) and \(\mathbb{E}_{\mathbb{P}^{v}}|\log\frac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}^{u}}|<+\infty\), respectively. Define

\[\begin{split}&\tilde{Y}_{T}^{u,v}=-\lambda^{-1}\int_{0}^{T} \langle u(X_{t}^{v},t),v(X_{t}^{v},t)\rangle\,\mathrm{d}t\\ &\quad-\lambda^{-1}\int_{0}^{T}f(X_{t}^{v},t)\,\mathrm{d}t-\lambda ^{-1/2}\int_{0}^{T}\langle u(X_{t}^{v},t),\mathrm{d}B_{t}\rangle\\ &\quad+\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|u(X_{t}^{v},t)\|^{2} \,\mathrm{d}t.\end{split}\] (15)

Then, \(\tilde{\mathcal{L}}_{\mathrm{Var}_{v}}\) and \(\tilde{\mathcal{L}}_{\mathrm{Var}_{v}}^{\log}\) are equivalent, respectively, to the following losses (see Lemma 4):

\[\mathcal{L}_{\mathrm{Var}_{v}}(u) :=\mathrm{Var}\big{(}\exp\big{(}\tilde{Y}_{T}^{u,v}-\lambda^{-1}g( X_{T}^{v})\big{)}\big{)},\] (16) \[\mathcal{L}_{\mathrm{Var}_{v}}^{\log}(u) :=\mathrm{Var}\big{(}\tilde{Y}_{T}^{u,v}-\lambda^{-1}g(X_{T}^{v}) \big{)},\] (17)

The variance and log-variance losses were introduced by Nusken and Richter [59]. Unlike for the cross-entropy loss, the choice of the control \(v\) does lead to different losses. When using \(\mathcal{L}_{\mathrm{Var}_{v}}\) or \(\mathcal{L}_{\mathrm{Var}_{v}}^{\log}\) in Algorithm 1, the variance is computed across the \(m\) trajectories in each batch.

Moment lossFor an arbitrary \(v\in\mathcal{U}\), the moment loss is defined as

\[\mathcal{L}_{\mathrm{Mom}_{v}}(u,y_{0})=\mathbb{E}[(\tilde{Y}_{T}^{u,v}+y_{0}- \lambda^{-1}g(X_{T}^{v}))^{2}],\] (18)where \(\tilde{Y}_{T}^{u,v}\) is defined in (15). Note the similarity with the log-variance loss (17); the optimal value of \(y_{0}\) for a fixed \(u\) is \(y_{0}^{*}=\mathbb{E}[\lambda^{-1}g(X_{T}^{v})-\tilde{Y}_{T}^{u,v}]\), and plugging this into (18) yields exactly the log-variance loss. The moment loss was introduced by Hartmann et al. [39, Section III.B], and it is a generalization of the FBSDE method pioneered by E et al. [22], Han et al. [35] and referenced earlier in this subsection, which corresponds to setting \(v=0\).

## 3 Stochastic Optimal Control Matching

In this section we present our loss, _Stochastic Optimal Control Matching_ (SOCM). The corresponding method, which we describe in Algorithm 2, falls into the class of IDO techniques described in Subsec. 2.2. The general idea is to leverage the analytic expression of \(u^{*}\) in (9) to write a least squares loss for \(u\), and the main challenge is to reexpress the gradient of a conditional expectation with respect to the initial condition of the process. We do that using a novel technique which introduces certain arbitrary matrix-valued functions \(M_{t}\), that we also optimize.

**Theorem 1** (SOCM loss).: _For each \(t\in[0,T]\), let \(M_{t}:[t,T]\to\mathbb{R}^{d\times d}\) be an arbitrary matrix-valued differentiable function such that \(M_{t}(t)=\mathrm{Id}\). Let \(v\in\mathcal{U}\) be an arbitrary control. Let \(\mathcal{L}_{\mathrm{SOCM}}:L^{2}(\mathbb{R}^{d}\times[0,T];\mathbb{R}^{d}) \times L^{2}([0,T]^{2};\mathbb{R}^{d\times d})\to\mathbb{R}\) be the loss function defined as_

\[\mathcal{L}_{\mathrm{SOCM}}(u,M):=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T} \big{\|}u(X_{t}^{v},t)-w(t,v,X^{v},B,M_{t})\big{\|}^{2}\,\mathrm{d}t\times \alpha(v,X^{v},B)\big{]}\,,\] (19)

_where \(X^{v}\) is the process controlled by \(v\) (i.e., \(dX_{t}^{v}=(b(X_{t}^{v},t)+\sigma(t)v(X_{t}^{v},t))\,\mathrm{d}t+\sqrt{\lambda }\sigma(t)\,\mathrm{d}B_{t}\) and \(X_{0}^{v}\sim p_{0}\)), and_

\[w(t,v,X^{v},B,M_{t}) =\sigma(t)^{\top}\big{(}-\int_{t}^{T}M_{t}(s)\nabla_{x}f(X_{s}^{v },s)\,\mathrm{d}s-M_{t}(T)\nabla g(X_{T}^{v})\] \[\quad+\int_{t}^{T}(M_{t}(s)\nabla_{x}b(X_{s}^{v},s)-\partial_{s}M _{t}(s))(\sigma^{-1})^{\top}(s)v(X_{s}^{v},s)\,\mathrm{d}s\] \[\quad+\lambda^{1/2}\int_{t}^{T}(M_{t}(s)\nabla_{x}b(X_{s}^{v},s)- \partial_{s}M_{t}(s))(\sigma^{-1})^{\top}(s)\mathrm{d}B_{s}\big{)},\] \[\alpha(v,X^{v},B) =\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X_{t}^{v},t)\,\mathrm{d}s -\lambda^{-1}g(X_{T}^{v})\] (20) \[\qquad\qquad-\lambda^{-1/2}\int_{0}^{T}\langle v(X_{t}^{v},t), \mathrm{d}B_{t}\rangle-\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|v(X_{t}^{v},t)\|^{ 2}\,\mathrm{d}t\big{)}.\]

\(\mathcal{L}_{\mathrm{SOCM}}\) _has a unique optimum \((u^{*},M^{*})\), where \(u^{*}\) is the optimal control._

We refer to \(M=(M_{t})_{t\in[0,T]}\) as the family of _reparametrization matrices_, to the random vector field \(w\) as the _matching vector field_, and to \(\alpha\) as the _importance weight_. We present a proof sketch of Thm. 1; the full proofs for all the results in this section are in App. C.

**Proof sketch of Thm. 1** Let \(X\) be the uncontrolled process (6). Consider the loss

\[\tilde{\mathcal{L}}(u) =\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}u(X_{t},t)-u^{* }(X_{t},t)\big{\|}^{2}\,\mathrm{d}t\,\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X_{ t},t)\,\mathrm{d}t-\lambda^{-1}g(X_{T})\big{)}\big{]}\] (21) \[=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{(}\big{\|}u(X_{t},t)\big{\|}^{2}-2\langle u(X_{t},t),u^{*}(X_{t},t)\rangle+\|u^{*}(X_{t},t)\big{\|} ^{2}\big{)}\,\mathrm{d}t\] \[\qquad\qquad\times\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X_{t},t) \,\mathrm{d}t-\lambda^{-1}g(X_{T})\big{)}\big{]}.\]

Clearly, the only optimum of this loss is the optimal control \(u^{*}\). Using the analytic expression of \(u^{*}\) in (9), the cross-term can be rewritten as (see Lemma 5 in App. C):

\[\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\langle u(X_{t},t),u^{*}( X_{t},t)\rangle\,\mathrm{d}t\,\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X_{t},t)\, \mathrm{d}t-\lambda^{-1}g(X_{T})\big{)}\big{]}\] (22) \[=\lambda\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\langle}u(X _{t},t),\sigma(t)^{\top}\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\int_ {t}^{T}f(X_{s},s)\,\mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x\big{]} \big{)}\] \[\qquad\qquad\times\exp\big{(}-\lambda^{-1}\int_{0}^{t}f(X_{s},s)\, \mathrm{d}s\big{)}\,\mathrm{d}t\big{]}.\]

It remains to evaluate the conditional expectation \(\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\int_{t}^{T}f(X_{s},s)\, \mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x\big{]}\), which we do by a "reparameterization trick" that shifts the dependence on the initial value \(x\) into the stochastic processes--here we introduce a free variable \(M_{t}\)--and then applying Girsanov theorem. We coin this the _path-wise reparameterization trick_:

**Proposition 1** (Path-wise reparameterization trick for stochastic optimal control).: _For each \(t\in[0,T]\), let \(M_{t}:[t,T]\to\mathbb{R}^{d\times d}\) be an arbitrary continuously differentiable function matrix-valued function _such that \(M_{t}(t)=\mathrm{Id}\). We have that_

\[\begin{split}&\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1} \int_{t}^{T}f(X_{s},s)\,\mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x \big{]}\\ &=\mathbb{E}\big{[}\big{(}-\lambda^{-1}\int_{t}^{T}M_{t}(s)\nabla _{x}f(X_{s},s)\,\mathrm{d}s-\lambda^{-1}M_{t}(T)\nabla g(X_{T})\\ &\qquad\qquad\qquad+\lambda^{-1/2}\int_{t}^{T}(M_{t}(s)\nabla_{ x}b(X_{s},s)-\partial_{s}M_{t}(s))(\sigma^{-1})^{\top}(s)\mathrm{d}B_{s}\big{)}\\ &\qquad\qquad\qquad\times\exp\big{(}-\lambda^{-1}\int_{t}^{T}f( X_{s},s)\,\mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x\big{]}.\end{split}\] (23)

We prove a more general form of this result (Prop. 4) in Subsec. C.2 and also provide an intuitive derivation in Subsec. C.3. In the proof of Prop. 4, the reparameterization matrices \(M_{t}\) arise as the gradients of a perturbation to the process \(X_{t}\). Similar ideas can potentially be applied to derive losses for generative modeling. If we plug (23) into the right-hand side of (22), and then this back into (21), and we complete the square, we obtain that for some constant \(K\) independent of \(u\),

\[\begin{split}\tilde{\mathcal{L}}(u)=\mathbb{E}\big{[}\tfrac{1}{ T}\int_{0}^{T}\big{\|}u(X_{t},t)+\sigma(t)\big{(}\int_{t}^{T}M_{t}(s)\nabla_{x}f(X_{s},s)\,\mathrm{d}s+M_{t}(T)\nabla g(X_{T})\\ &\qquad\qquad\qquad-\lambda^{1/2}\int_{t}^{T}(M_{t}(s)\nabla_{x} b(X_{s},s)-\partial_{s}M_{t}(s))(\sigma^{-1})^{\top}(s)\mathrm{d}B_{s}\big{)} \big{\|}^{2}\,\mathrm{d}t\\ &\qquad\qquad\qquad\times\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X _{t},t)\,\mathrm{d}t-\lambda^{-1}g(X_{T})\big{)}\big{]}+K.\end{split}\]

If we perform a change of process from \(X\) to \(X^{v}\) applying the Girsanov theorem (Cor. 1 in App. C), we obtain the loss \(\mathcal{L}_{\mathrm{SOCM}}(u,M)\). 

The following result clarifies the role of reparameterization matrices, connecting the SOCM and cross-entropy losses.

**Proposition 2** (Bias-variance decomposition of the SOCM loss).: _The SOCM loss decomposes into a bias term that only depends on \(u\) and a variance term that only depends on \(M\):_

\[\begin{split}\mathcal{L}_{\mathrm{SOCM}}(u,M)=\underbrace{ \mathrm{CondVar}(w;M)}_{\underbrace{\text{Unnormalized expected}}_{\text{ conditional variance of $w$}}}+\underbrace{\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}u(X_{t}^{u^{*}},t) -u^{*}(X_{t}^{u^{*}},t)\big{\|}^{2}\,\mathrm{d}t\,e^{-\lambda^{-1}V(X_{0}^{u^ {*}},0)}\big{]}}_{\text{Unnormalized bias of $u$}},\end{split}\] (24)

_where_

\[\begin{split}\mathrm{CondVar}(w;M)\!=\!\mathbb{E}\big{[}\tfrac{1} {T}\!\int_{0}^{T}\big{\|}w(t,v,X^{v},B,M_{t})\!-\underbrace{\frac{\mathbb{E} [w(t,v,X^{v},B,M_{t})\alpha(v,X^{v},B)|X_{t}^{v},t)}{\mathbb{E}[\alpha(v,X^{ v},B)|X_{t}^{v},t)}\|^{2}\,\mathrm{d}t\,\alpha(v,X^{v},B)\big{]}}_{u^{*}(X_{t}^{v},t)} \big{\|}^{2}\,\mathrm{d}t\,\alpha(v,X^{v},B)\big{]}.\end{split}\] (25)

Remark that the bias term in equation (24) is equal to the characterization of the cross-entropy loss in (14). In other words, the landscape of \(\mathcal{L}_{\mathrm{SOCM}}(u,M)\) with respect to \(u\) is the landscape of the cross-entropy loss \(\mathcal{L}_{\mathrm{CE}}(u)\). Thus, the SOCM loss can be seen as some form of variance reduction method for the cross-entropy loss, and performs substantially better experimentally (Sec. 4). Yet, the expressions of the SOCM loss and the cross-entropy loss are very different; the former is a least squares loss and is expressed in terms of the gradients of the costs.

```
0: State cost \(f(x,t)\), terminal cost \(g(x)\), diffusion coeff. \(\sigma(t)\), base drift \(b(x,t)\), noise level \(\lambda\), number of iterations \(N\), batch size \(m\), number of time steps \(K\), initial control parameters \(\theta_{0}\), initial matrix parameters \(\omega_{0}\), loss \(\mathcal{L}_{\mathrm{SOCM}}\) in (19)
1for\(n\in\{0,\ldots,N-1\}\)do
2: Simulate \(m\) trajectories of the process \(X^{v}\) controlled by \(v=u_{\theta_{n}}\), e.g., using Euler-Maruyama updates
3: Detach the \(m\) trajectories from the computational graph, so that gradients do not backpropagate
4: Using the \(m\) trajectories, compute an \(m\)-sample Monte-Carlo approximation \(\hat{\mathcal{L}}_{\mathrm{SOCM}}(u_{\theta_{n}},M_{\omega_{n}})\) of the loss \(\mathcal{L}_{\mathrm{SOCM}}(u_{\theta_{n}},M_{\omega_{n}})\) in (19)
5: Compute the gradients \(\nabla_{(\theta,\omega)}\hat{\mathcal{L}}_{\mathrm{SOCM}}(u_{\theta_{n}},M_{ \omega_{n}})\) of \(\hat{\mathcal{L}}_{\mathrm{SOCM}}(u_{\theta_{n}},M_{\omega_{n}})\) at \((\theta_{n},\omega_{n})\)
6: Obtain \(\theta_{n+1}\), \(\omega_{n+1}\) with via an Adam update on \(\theta_{n}\), \(\omega_{n}\), resp.
7:
8:
9:
10: end for Output: Learned control \(u_{\theta_{N}}\) ```

**Algorithm 2** Stochastic Optimal Control Matching (SOCM)

For good training performance, it is critical that the gradients have high signal-to-noise ratio. Looking at the SOCM loss, a good proxy for low gradient variance is to have low variance for \(\frac{1}{T}\int_{0}^{T}\big{\|}u(X_{t}^{v},t)-w(t,v,X^{v},B,M_{t})\big{\|}^{2}\, \mathrm{d}t\times\alpha(v,X^{v},B)\), and this holds when both \(\alpha(v,X^{v},B)\) and \(w(t,v,X^{v},B,M_{t})\) have low variance. Next, we present strategies to lower the variance of these two objects.

Minimizing the variance of the importance weight \(\alpha\)We want to use a vector field \(v\) such that \(\mathrm{Var}[\alpha(v,X^{v},B)]\) is as low as possible. As shown by the following lemma, which is well-known in the literature, setting \(v\) to be the optimal control \(u^{*}\) actually achieves variance zero when we condition on the starting point of the controlled process \(X^{v}\). The proof of this result can be found in Hartmann et al. [38], but we include it in Subsec. C.5 for completeness.

**Lemma 1**.: _When we set \(v=u^{*}\), the conditional variance \(\mathrm{Var}[\alpha(v,X^{v},B)|X^{v}_{0}=x_{\mathrm{init}}]\) is zero for any \(x_{\mathrm{init}}\in\mathbb{R}^{d}\)._

Of course, we do not have access to the optimal control \(u^{*}\), but it is still a good idea to set \(v\) as the closest vector field to \(u^{*}\) that we have access to, which is typically the currently learned control. In some instances, one may benefit from using a warm-started control parameterized as \(u_{\mathrm{WS}}(x,t)+u_{\theta}(x,t)\), where the warm-start \(u_{\mathrm{WS}}\) is a reasonably good control obtained via a different strategy (see App. E).

Minimizing the variance of the matching vector field \(w\)We are interested in finding the family \(M=\left(M_{t}\right)_{t\in[0,T]}\) that minimizes the variance of \(w(t,v,X^{v},B,M_{t})\) conditioned on \(t\) and \(X_{t}\). Note that this is exactly the term \(\mathrm{CondVar}(w;M)\) in the right-hand side of equation (24). Since \(\mathrm{CondVar}(w;M)\) does not depend on the specific \(v\), the optimal \(M\) does not depend on \(v\) either. And since the second term in the right-hand side of equation (24) does not depend on \(M=\left(M_{t}\right)_{t\in[0,T]}\), minimizing \(\mathrm{CondVar}(w;M)\) is equivalent to minimizing \(\mathcal{L}(u)\) with respect to \(M\).

Parameterizing the matrices \(M_{t}\) vs solving for the optimal matricesIn practice, we parameterize the matrices \(\left(M_{t}\right)_{t\in[0,T]}\) using a function \(M_{\omega}\) with two arguments \((t,s)\). To enforce that \(M_{\omega}(t,t)=\mathrm{Id}\), we set \(M_{\omega}(t,s)=e^{-\gamma(s-t)}\mathrm{Id}+(1-e^{-\gamma(s-t)})\tilde{M}_{ \tilde{\omega}}(t,s)\), where \(\omega=(\gamma,\tilde{\omega})\), and \(\tilde{M}_{\tilde{\omega}}:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}^{d \times d}\) is an unconstrained neural network. Alternatively, Thm. 4 in App. D shows that the optimal family \(M^{*}=(M^{*}_{t})_{t\in[0,T]}\) can be characterized as the solution of a linear equation in infinite dimensions (a Fredholm equation of the first kind). The discretized linear system has \(d^{2}K\) equations and variables, \(K\) being the number of discretization time points. However, since the optimal \(M^{*}\) does not depend on \(v\) (see Remark 1), this is a computation that must be done only once and that may be affordable in some settings. We did not test this approach experimentally.

## 4 Experiments

We consider four experimental settings that we adapt from Nusken and Richter [59]: Quadratic Ornstein Uhlenbeck (easy), Quadratic Ornstein Uhlenbeck (hard), Linear Ornstein Uhlenbeck and Double Well. We describe them in detail in App. F. For all of them, we have access to the ground-truth optimal control, which means that we are able to estimate the \(L^{2}\) error incurred by the learned control \(u\). In Figure 2 we plot the control \(L^{2}\) error for each IDO algorithm described in Subsec. 2.2, and for the SOCM algorithm (Algorithm 2), for the Quadratic OU (easy) and (hard) settings. We also include two ablations of SOCM: _(i)_ a version of SOCM where the reparameterization matrices \(M_{t}\) are set fixed to the identity \(I\), _(ii)_ SOCM-Adjoint, where we estimate the conditional expectation in equation (23) using the adjoint method for SDEs instead of the path-wise reparameterization trick (see Subsec. C.4). Code can be found at https://github.com/facebookresearch/SOC-matching.

At the end of training, SOCM obtains the lowest \(L^{2}\) error, improving over all existing methods by a factor of around ten. The two SOCM ablations come in second and third by a substantial difference, which underlines the importance of the path-wise reparameterization trick. The best among existing methods is the adjoint method (the relative entropy loss). In Figure 2 (_bottom_) we show the squared norm of the gradient of each loss with respect to the parameters \(\theta\) of the control: algorithms with small noise variance have low error values.

In Figure 3, we plot the control \(L^{2}\) error for Linear Ornstein Uhlenbeck and Double Well. For Linear OU, the error is around five times smaller for SOCM than for any existing method. For Double Well, the SOCM algorithm achieves the third smallest error, slightly behind the variance loss and the adjoint method, but the latter shows instabilities. As we show in Figure 9 in App. F, these instabilities are inherent to the adjoint method and they do not disappear for small learning rates. Both in Figure 2 and Figure 3, we observe that learning the reparameterization matrices is critical to obtain gradient estimates with high signal-to-noise ratio. Double Well is a particularly interesting and challenging setting because its solution is highly multimodal: \(g\) has 1024 modes. Multimodalityis a feature observed in realistic settings, and is hard to handle because it involves learning the control correctly in each mode.

The costs \(f\) and \(g\) and the base drift \(b\) for Quadratic OU (hard) are five times those of Quadratic OU (easy). Consequently, the factor \(\alpha(v,X^{v},B)\) initially has a much larger variance for the SOCM methods, and for cross-entropy. As training progresses, \(u_{\theta_{n}}\) gets closer to \(u^{*}\), and consequently the variance of \(\alpha(v,X^{v},B)\) decreases, which in turn makes learning easier. This explains the initial slow decrease in the control error, followed by a fast drop that places SOCM well below existing algorithms. In App. E, we showcase a control warm-start strategy that can help and speed up convergence.

We also present experimental results on two-mode Gaussian mixture sampling in increasing dimension, using the Path Integral Sampler [84]. We take Gaussians with means that are 2 units apart, and identity variance. Figure 1 shows control objective estimates obtained after running the Adjoint, SOCM, and Cross-entropy algorithms for 40000 iterations, at dimensions \(d=2,8,16,32,64\), and error bars show standard errors. By Theorem 4 of [84], we know that the optimal value of the control objective is zero; Figure 1 shows the suboptimality gaps incurred by each algorithm.

Cross-entropy, which uses the same importance weight as SOCM, performs worse than the other two losses for all dimensions, and its results are particularly poor for dimension 64, because the variance of \(\alpha\) is too large for learning to happen. In this case, we see that SOCM has better variance reduction than cross-entropy, despite both using importance weighted objectives for training. We observe that the values for SOCM are slightly below that of Adjoint for most dimensions, which confirms that our method is better for this range of dimensions. If we keep increasing the dimension, SOCM also fails due to higher variance of \(\alpha\): for \(n=128\), the control objective estimates for the Adjoint, SOCM, and Cross-Entropy losses are \(0.146\pm 0.001\), \(7.49\pm 0.01\), and \(12.61\pm 0.02\), respectively.

## 5 Conclusion

Our work introduces Stochastic Optimal Control Matching, a novel Iterative Diffusion Optimization technique for stochastic optimal control that stems from the same philosophy as the conditional score matching loss for diffusion models. That is, the control is learned via a least-squares problem by trying to fit a matching vector field. The training loss is optimized with respect to both the control function and a family of reparameterization matrices which appear in the matching vector field. Optimizing the reparameterization matrices reduces the variance of the matching vector field. Experimentally, our algorithm achieves lower error than all existing IDO techniques in four settings.

One of the key ideas for deriving the SOCM algorithm is the path-wise reparameterization trick, a novel technique to obtain low-variance estimates of the gradient of the conditional expectation of a functional of a random process with respect to its initial value. An interesting future direction is to use the path-wise reparameterization trick to decrease the variance of the matching vector field for diffusion models. The main roadblock when we try to apply SOCM to more challenging problems is that the variance of the factor \(\alpha(v,X^{v},B)\) explodes when \(f\) and/or \(g\) are large, or when the dimension \(d\) is high. The large variance of \(\alpha\) is due to the mismatch between the probability measures induced by the learned control and the optimal control, and it decreases as the learned control approaches the optimal control.

The research presented is foundational, but it may serve as the basis of algorithms that improve the quality of generative models.

Figure 1: This plot shows the control objective values for different algorithms (Adjoint, SOCM, and Cross-entropy) across multiple dimensions, with error bars indicating the standard deviations. The y-axis is restricted to [0, 0.1] for better visibility of the lower range values; cross-entropy takes value \(2.915\pm 0.008\) at \(d=64\).

Funding disclosureFunded by respective author affiliations.

## References

* [1] Michael S. Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants, 2022.
* [2] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023.
* [3] Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. _Journal of Artificial Intelligence Research_, 15:319-350, 2001.

Figure 3: Plots of the \(L^{2}\) error of the learned control for the Linear Ornstein Uhlenbeck and Double Well settings.

Figure 2: Plots of the \(L^{2}\) error incurred by the learned control (_left_), and the norm squared of the gradient with respect to the parameters \(\theta\) of the control (_right_), for the Quadratic Ornstein Uhlenbeck (easy) (_top_) and (hard) (_bottom_) settings and for each IDO loss. Both plots show exponential moving averages.

* [4] L'ubomir Banas, Herbert Dawid, Tsiry Avisoa Randrianasolo, Johannes Storn, and Xin Wen. Numerical approximation of a system of hamilton-jacobi-bellman equations arising in innovation dynamics. _Journal of Scientific Computing_, 92, 2022.
* [5] Christian Beck, Sebastian Becker, Philipp Grohs, Nor Jaafari, and Arnulf Jentzen. Solving stochastic differential equations and Kolmogorov equations by means of deep learning. _arXiv:1806.00421_, 2018.
* [6] Christian Beck, Fabian Hornung, Martin Hutzenthaler, Arnulf Jentzen, and Thomas Kruse. Overcoming the curse of dimensionality in the numerical approximation of Allen-Cahn partial differential equations via truncated full-history recursive multilevel Picard approximations. _arXiv:1907.06729_, 2019.
* [7] Sebastian Becker, Patrick Cheridito, and Arnulf Jentzen. Deep optimal stopping. _Journal of Machine Learning Research_, 20, 2019.
* [8] Andrea Belloni, Luigi Piroddi, and Maria Prandini. A stochastic optimal control solution to the energy management of a microgrid with storage and renewables. In _2016 American Control Conference (ACC)_, pages 2340-2345, 2016.
* [9] Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusion-based generative modeling, 2023.
* [10] Joris Bierkens and Hilbert J Kappen. Explicit solution of relative entropy weighted control. _Systems & Control Letters_, 72:36-43, 2014.
* [11] J. Bonnans, Elisabeth Ottenwaelter, and Hasnaa Zidani. A fast algorithm for the two dimensional hjb equation of stochastic control. _M2AN. Mathematical Modelling and Numerical Analysis. ESAIM, European Series in Applied and Industrial Mathematics_, 38, 07 2004.
* [12] Elisa Calzola, Elisabetta Carlini, Xavier Dupuis, and Francisco Silva. A semi-Lagrangian scheme for Hamilton-Jacobi-Bellman equations with oblique derivatives boundary conditions. _Numerische Mathematik_, page 153, 2022.
* [13] E. Carlini, A. Festa, and N. Forcadel. A semi-Lagrangian scheme for Hamilton-Jacobi-Bellman equations on networks. _SIAM J. Numer. Anal._, 58(6):3165-3196, 2020.
* [14] Rene Carmona. _Lectures on BSDEs, stochastic control, and stochastic differential games with financial applications_, volume 1. SIAM, 2016.
* [15] Rene Carmona and Mathieu Lauriere. Convergence analysis of machine learning algorithms for the numerical solution of mean field control and games i: The ergodic case. _SIAM Journal on Numerical Analysis_, 59(3):1455-1485, 2021.
* [16] Rene Carmona and Mathieu Lauriere. Convergence analysis of machine learning algorithms for the numerical solution of mean field control and games: Ii--the finite horizon case. _The Annals of Applied Probability_, 32(6):4065-4105, 2022.
* [17] Rene Carmona, Francois Delarue, et al. _Probabilistic Theory of Mean Field Games with Applications I-II_. Springer, 2018.
* [18] Quentin Chan-Wai-Nam, Joseph Mikael, and Xavier Warin. Machine learning for semilinear PDEs. _Journal of Scientific Computing_, 79(3):1667-1712, 2019.
* [19] Pratik Chaudhari, Adam Oberman, Stanley Osher, Stefano Soatto, and Guillaume Carlier. Deep relaxation: partial differential equations for optimizing deep neural networks. _Research in the Mathematical Sciences_, 5(3):30, 2018.
* [20] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [21] Kristian Debrabant and Espen R. Jakobsen. Semi-lagrangian schemes for linear and fully non-linear diffusion equations. _Mathematics of Computation_, 82(283):1433-1462, 2013.

* [22] W. E, Jiequn Han, and Arnulf Jentzen. Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations. _Communications in Mathematics and Statistics_, 5(4):349-380, 2017.
* [23] Weinan E, Jiequn Han, and Arnulf Jentzen. Algorithms for solving high dimensional pdes: from nonlinear monte carlo to machine learning. _Nonlinearity_, 35(1):278, 2021.
* [24] Jin Feng and Thomas G Kurtz. _Large deviations for stochastic processes_. Number 131. American Mathematical Soc., 2006.
* [25] Wendell H Fleming and Jerome L Stein. Stochastic optimal control, international finance and debt. _Journal of Banking & Finance_, 28(5):979-996, 2004.
* [26] Paul Glasserman and David D. Yao. Some guidelines and guarantees for common random numbers. _Manage. Sci._, 38(6):884-908, jun 1992.
* [27] Peter W. Glynn. Stochastic approximation for monte carlo optimization. In _Proceedings of the 18th Conference on Winter Simulation_, page 356-365. Association for Computing Machinery, 1986.
* [28] Emmanuel Gobet. _Monte-Carlo methods and stochastic processes: from linear to non-linear_. CRC Press, 2016.
* [29] Emmanuel Gobet and Remi Munos. Sensitivity analysis using Ito-Malliavin calculus and martingales, and application to stochastic optimal control. _SIAM Journal on control and optimization_, 43(5):1676-1713, 2005.
* [30] Emmanuel Gobet, Jean-Philippe Lemor, Xavier Warin, et al. A regression-based Monte Carlo method to solve backward stochastic differential equations. _The Annals of Applied Probability_, 15(3):2172-2202, 2005.
* [31] Vicenc Gomez, Hilbert J Kappen, Jan Peters, and Gerhard Neumann. Policy search for path integral control. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 482-497. Springer, 2014.
* [32] Alex Gorodetsky, Sertac Karaman, and Youssef Marzouk. High-dimensional stochastic optimal control using continuous tensor decompositions. _International Journal of Robotics Research_, 37(2-3), 3 2018.
* [33] Constantin Greif. Numerical methods for hamilton-jacobi-bellman equations. 2017.
* [34] Jiequn Han and Ruimeng Hu. Deep fictitious play for finding markovian nash equilibrium in multi-agent games. In _Mathematical and scientific machine learning_, pages 221-245. PMLR, 2020.
* [35] Jiequn Han, Arnulf Jentzen, and W. E. Solving high-dimensional partial differential equations using deep learning. _Proceedings of the National Academy of Sciences_, 115(34):8505-8510, 2018.
* [36] Carsten Hartmann and Christof Schutte. Efficient rare event simulation by optimal nonequilibrium forcing. _Journal of Statistical Mechanics: Theory and Experiment_, 2012(11):P11004, 2012.
* [37] Carsten Hartmann, Ralf Banisch, Marco Sarich, Tomasz Badowski, and Christof Schutte. Characterization of rare events in molecular dynamics. _Entropy_, 16(1):350-376, 2014.
* [38] Carsten Hartmann, Lorenz Richter, Christof Schutte, and Wei Zhang. Variational characterization of free energy: Theory and algorithms. _Entropy_, 19(11), 2017.
* [39] Carsten Hartmann, Omar Kebiri, Lara Neureither, and Lorenz Richter. Variational approach to rare event simulation using least-squares regression. _Chaos: An Interdisciplinary Journal of Nonlinear Science_, 29(6):063107, 2019.
* [40] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Advances in Neural Information Processing Systems_, volume 33. Curran Associates, Inc., 2020.

* [41] Lars Holdijk, Yuanqi Du, Ferry Hooft, Priyank Jaini, Bernd Ensing, and Max Welling. Stochastic optimal control for collective variable free sampling of molecular transition paths, 2023.
* [42] James E. Hutton and Paul I. Nelson. Interchanging the order of differentiation and stochastic integration. _Stochastic Processes and their Applications_, 18(2):371-377, 1984.
* [43] Martin Hutzenthaler and Thomas Kruse. Multilevel picard approximations of high-dimensional semilinear parabolic differential equations with gradient-dependent nonlinearities. _SIAM Journal on Numerical Analysis_, 58(2):929-961, 2020.
* [44] Martin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, et al. Multilevel picard iterations for solving smooth semilinear parabolic heat equations. _arXiv preprint arXiv:1607.03295_, 2016.
* [45] Martin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, Tuan Anh Nguyen, and Philippe von Wurstemberger. Overcoming the curse of dimensionality in the numerical approximation of semilinear parabolic partial differential equations. _arXiv:1807.01212_, 2018.
* [46] Martin Hutzenthaler, Arnulf Jentzen, and Thomas Kruse. Overcoming the curse of dimensionality in the numerical approximation of parabolic partial differential equations with gradient-dependent nonlinearities. _arXiv:1912.02571_, 2019.
* [47] Max Jensen and Iain Smears. On the convergence of finite element methods for hamilton-jacobi-bellman equations. _SIAM Journal on Numerical Analysis_, 51(1):137-162, 2013.
* [48] Hilbert J Kappen, Vicenc Gomez, and Manfred Opper. Optimal control as a graphical model inference problem. _Machine learning_, 87(2):159-182, 2012.
* [49] Hilbert Johan Kappen and Hans Christian Ruiz. Adaptive importance sampling for control and inference. _Journal of Statistical Physics_, 162(5):1244-1266, 2016.
* [50] I. Karatzas and S. Shreve. _Brownian Motion and Stochastic Calculus_. Graduate Texts in Mathematics (113) (Book 113). Springer New York, 1991.
* [51] Patrick Kidger, James Foster, Xuechen Li, Harald Oberhauser, and Terry Lyons. Neural sdes as infinite-dimensional gans. In _International Conference on Machine Learning_, 2021.
* [52] Harold Kushner and Paul G Dupuis. _Numerical methods for stochastic control problems in continuous time_, volume 24. Springer Science & Business Media, 2013.
* [53] Pierre L'Ecuyer and Gaetan Perron. On the convergence rates of ipa and fdc derivative estimators. _Operations Research_, 42(4):643-656, 1994.
* [54] Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable gradients for stochastic differential equations. In _International Conference on Artificial Intelligence and Statistics_, pages 3870-3882. PMLR, 2020.
* [55] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2022.
* [56] Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos A. Theodorou, and Ricky T. Q. Chen. Generalized schrodinger bridge matching, 2023.
* [57] Jingtang Ma and Jianjun Ma. Finite difference methods for the hamilton-jacobi-bellman equations arising in regime switching utility maximization. _J. Sci. Comput._, 85(3):55, 2020.
* [58] Sanjoy K Mitter. Filtering and stochastic control: A historical perspective. _IEEE Control Systems Magazine_, 16(3):67-76, 1996.
* [59] Nikolas Nusken and Lorenz Richter. Solving high-dimensional Hamilton-Jacobi-Bellman pdes using neural networks: perspectives from the theory of controlled diffusions and measures on path space. _Partial differential equations and applications_, 2:1-48, 2021.
* [60] Bernt Oksendal. _Stochastic differential equations: an introduction with applications_. Springer Science & Business Media, 2013.

* [61] Derek Onken, Levon Nurbekyan, Xingjian Li, Samy Wu Fung, Stanley Osher, and Lars Ruthotto. A neural network approach for high-dimensional optimal control applied to multiagent path finding. _IEEE Transactions on Control Systems Technology_, 31(1):235-251, jan 2023.
* [62] Grigorios A Pavliotis. _Stochastic processes and applications: diffusion processes, the Fokker-Planck and Langevin equations_, volume 60. Springer, 2014.
* [63] Huyen Pham. _Continuous-time stochastic control and optimization with financial applications_, volume 61. Springer Science & Business Media, 2009.
* [64] L.S. Pontryagin. _The Mathematical Theory of Optimal Processes_. Interscience Publishers, 1962.
* [65] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky T. Q. Chen. Multisample flow matching with optimal transport couplings. In _International Conference on Machine Learning_, 2023.
* [66] Warren B. Powell and Stephan Meisel. Tutorial on stochastic optimization in energy--part i: Modeling and policies. _IEEE Transactions on Power Systems_, 31(2):1459-1467, 2016.
* [67] Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement learning by approximate inference. In _Twenty-Third International Joint Conference on Artificial Intelligence_, 2013.
* [68] Sebastian Reich. Data assimilation: The Schrodinger perspective. _Acta Numerica_, 28:635-711, 2019.
* [69] Martin I. Reiman and Alan Weiss. Sensitivity analysis for simulations via likelihood ratios. _Oper. Res._, 37:830-844, 1989.
* [70] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _Proceedings of the 32nd International Conference on Machine Learning_, 2015.
* [71] Lorenz Richter and Julius Berner. Improved sampling via learned diffusions. In _The Twelfth International Conference on Learning Representations_, 2024.
* [72] Geoffrey Roeder, Yuhuai Wu, and David K Duvenaud. Sticking the landing: Simple, lower-variance gradient estimators for variational inference. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [73] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.
* [74] Reuven Y Rubinstein and Dirk P Kroese. _The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning_. Springer Science & Business Media, 2013.
* [75] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _arXiv preprint arXiv:1907.05600_, 2019.
* [76] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations (ICLR 2021)_, 2021.
* [77] Evangelos Theodorou, Freek Stulp, Jonas Buchli, and Stefan Schaal. An iterative path integral stochastic optimal control approach for learning robotic tasks. _IFAC Proceedings Volumes_, 44 (1):11594-11601, 2011. 18th IFAC World Congress.
* [78] Ramon Van Handel. Stochastic calculus, filtering, and stochastic control. _Course notes, URL http://www. prince- ton. edu/rvan/acm217/ACM217_, 2007.
* [79] Francisco Vargas, Will Sussman Grathwohl, and Arnaud Doucet. Denoising diffusion samplers. In _The Eleventh International Conference on Learning Representations_, 2023.

* [80] C. Villani. _Topics in Optimal Transportation_. Graduate studies in mathematics. American Mathematical Society, 2003.
* [81] C. Villani. _Optimal Transport: Old and New_. Grundlehren der mathematischen Wissenschaften. Springer Berlin Heidelberg, 2008.
* [82] Jichuan Yang and Harold J. Kushner. A monte carlo method for sensitivity analysis and parametric optimization of nonlinear stochastic systems. _SIAM Journal on Control and Optimization_, 29(5):1216-1249, 1991.
* [83] Jianfeng Zhang et al. A numerical scheme for BSDEs. _The annals of applied probability_, 14(1):459-488, 2004.
* [84] Qinsheng Zhang and Yongxin Chen. Path integral sampler: A stochastic control approach for sampling. In _International Conference on Learning Representations_, 2022.
* [85] Wei Zhang, Han Wang, Carsten Hartmann, Marcus Weber, and Christof Schutte. Applications of the cross-entropy method to importance sampling and optimal control of diffusions. _SIAM Journal on Scientific Computing_, 36(6):A2654-A2672, 2014.
* [86] Mo Zhou, Jiequn Han, and Jianfeng Lu. Actor-critic method for high dimensional static Hamilton-Jacobi-Bellman partial differential equations based on neural networks. _SIAM Journal on Scientific Computing_, 43(6):A4043-A4066, 2021.

###### Contents

* 1 Introduction
* 2 Framework
	* 2.1 Setup and Preliminaries
	* 2.2 Existing approaches and related work
* 3 Stochastic Optimal Control Matching
* 4 Experiments
* 5 Conclusion
* A Technical assumptions
* B Proofs of Sec. 2
* C Proofs of Sec. 3
* C.1 Proof of Thm. 1 and Prop. 2
* C.2 Proof of the path-wise reparameterization trick
* C.3 Informal derivation of the path-wise reparameterization trick
* C.4 SOCM-Adjoint: replacing the path-wise reparameterization trick with the adjoint method
* C.5 Proof of Lemma 1
* D Optimal reparameterization matrices
* E Control warm-starting
* F Experimental details and additional plots
* F.1 Experimental details
* F.2 Model architectures
* F.3 Additional tables and plots

## Appendix A Technical assumptions

Throughout our work, we make the same assumptions as [59], which are needed for all the objects considered to be well-defined. Namely, we assume that:

* The set \(\mathcal{U}\) of _admissible controls_ is given by \[\mathcal{U}=\{u\in C^{1}(\mathbb{R}^{d}\times[0,T];\mathbb{R}^{d})\,|\, \exists C>0,\,\forall(x,s)\in\mathbb{R}^{d}\times[0,T],\,b(x,s)\leq C(1+|x|)\}.\]
* The coefficients \(b\) and \(\sigma\) are continuously differentiable, \(\sigma\) has bounded first-order spatial derivatives, and \((\sigma\sigma^{\top})(x,s)\) is positive definite for all \((x,s)\in\mathbb{R}^{d}\times[0,T]\). Furthermore, there exist constants \(C,c_{1},c_{2}>0\) such that \[\|b(x,s)\|\leq C(1+\|x\|), \text{(linear growth)}\] \[c_{1}\|\xi\|^{2}\leq\xi^{\top}(\sigma\sigma^{\top})(x,s)\xi\leq c _{2}\|\xi\|^{2}, \text{(ellipticity)}\] for all \((x,s)\in\mathbb{R}^{d}\times[0,T]\) and \(\xi\in\mathbb{R}^{d}\).

## Appendix B Proofs of Sec. 2

Proof of (5)By Ito's lemma, we have that

\[V(X_{T}^{u},T)-V(X_{t}^{u},t)=\int_{t}^{T}\left(\partial_{s}V(X_{ s}^{u},s)+\langle b(X_{s}^{u},s)+\sigma(X_{s}^{u},s)u(X_{s}^{u},s),\nabla V(X_{s}^ {u},s)\rangle\right.\\ +\tfrac{\lambda}{2}\sum_{i,j=1}^{d}(\sigma\sigma^{\top})_{ij}(X_ {s}^{u},s)\partial_{x_{i}}\partial_{x_{j}}V(X_{s}^{u},s)\right)\mathrm{d}s+S_{ t}^{u},\]where \(S_{t}^{u}=\sqrt{\lambda}\int_{t}^{T}\nabla V(X_{s}^{u},s)^{\top}\sigma(X_{s}^{u},s )\,{\rm d}B_{s}\). Note that by (4),

\[\partial_{s}V(X_{s}^{u},s)+\langle b(X_{s}^{u},s)+\sigma(X_{s}^{u},s)u(X_{s}^{u},s),\nabla V(X_{s}^{u},s)\rangle\] \[\qquad+\tfrac{\lambda}{2}\sum_{i,j=1}^{d}(\sigma\sigma^{\top})_{ ij}(X_{s}^{u},s)\partial_{x_{i}}\partial_{x_{j}}V(X_{s}^{u},s)\] \[=\tfrac{1}{2}\|(\sigma^{\top}\nabla V)(X_{s}^{u},s)\|^{2}-f(X_{s} ^{u},s)+\langle\sigma(X_{s}^{u},s)u(X_{s}^{u},s),\nabla V(X_{s}^{u},s)\rangle\] \[=\tfrac{1}{2}\|(\sigma^{\top}\nabla V)(X_{s}^{u},s)+u(X_{s}^{u}, s)\|^{2}-\tfrac{1}{2}\|u(X_{s}^{u},s)\|^{2}-f(X_{s}^{u},s),\]

and this implies that

\[g(X_{T}^{u})-V(X_{t}^{u},t)=\int_{t}^{T}\big{(}\tfrac{1}{2}\|( \sigma^{\top}\nabla V)(X_{s}^{u},s)+u(X_{s}^{u},s)\|^{2}-\tfrac{1}{2}\|u(X_{s }^{u},s)\|^{2}-f(X_{s}^{u},s)\big{)}\,{\rm d}s+S_{t}^{u}\] (26)

Since \(\mathbb{E}[S_{t}^{u}\,|\,X_{t}^{u}=x]=0\), rearranging (26) and taking the conditional expectation with respect to \(X_{t}^{u}\) yields the final result.

Proof of (6)-(7)By Ito's lemma, we have that

\[dV(X_{s},s)=\big{(} \partial_{s}V(X_{s},s)+\langle b(X_{s},s),\nabla V(X_{s},s)\rangle\] \[+\tfrac{\lambda}{2}\sum_{i,j=1}^{d}(\sigma\sigma^{\top})_{ij}(X_ {s},s)\partial_{x_{i}}\partial_{x_{j}}V(X_{s},s)\big{)}\,{\rm d}s+\sqrt{ \lambda}\nabla V(X_{s}^{u},s)^{\top}\sigma(X_{s}^{u},s)\,{\rm d}B_{s},\] (27)

Note that by (4),

\[\partial_{s}V(X_{s},s)+\langle b(X_{s},s),\nabla V(X_{s},s) \rangle+\tfrac{\lambda}{2}\sum_{i,j=1}^{d}(\sigma\sigma^{\top})_{ij}(X_{s},s) \partial_{x_{i}}\partial_{x_{j}}V(X_{s},s)\] \[=\tfrac{1}{2}\|(\sigma^{\top}\nabla V)(X_{s},s)\|^{2}-f(X_{s},s).\]

Plugging this into (27) concludes the proof.

Proof of (8)Since \(Y_{s}=V(X_{s},s)\) and \(Z_{s}=\sigma^{\top}(s)\nabla V(X_{s},s)=-u^{*}(X_{s},s)\) satisfy (7), we have that

\[g(X_{T})=Y_{T}=Y_{t}-\int_{t}^{T}(f(X_{s},s)-\tfrac{1}{2}\|u^{*} (X_{s},s)\|^{2})\,{\rm d}s-\sqrt{\lambda}\int_{t}^{T}\langle u^{*}(X_{s},s), {\rm d}B_{s}\rangle.\]

Hence, recalling the definition of the work functional in (10), we have that

\[\mathcal{W}(X,t)=Y_{t}+\tfrac{1}{2}\int_{t}^{T}\|u^{*}(X_{s},s) \|^{2}\,{\rm d}s-\sqrt{\lambda}\int_{t}^{T}\langle u^{*}(X_{s},s),{\rm d}B_{s }\rangle.\] (28)

By Novikov's theorem (Thm. 2), we have that

\[\mathbb{E}[\exp(-\lambda^{-1}\mathcal{W}(X,t))|X_{t}]\] \[=e^{-\lambda^{-1}Y_{t}}\mathbb{E}\big{[}\exp\big{(}\lambda^{-1/2} \int_{t}^{T}\langle u^{*}(X_{s},s),{\rm d}B_{s}\rangle-\tfrac{\lambda^{-1}}{2 }\int_{t}^{T}\|u^{*}(X_{s},s)\|^{2}\,{\rm d}s\big{)}\big{|}X_{t}\big{]}=e^{- \lambda^{-1}Y_{t}},\]

which concludes the proof of (8).

**Theorem 2** (Novikov's theorem).: _Let \(\theta_{s}\) be a locally-\(\mathcal{H}_{2}\) process which is adapted to the natural filtration of the Brownian motion \((B_{t})_{t\geq 0}\). Define_

\[Z(t)=\exp\big{(}\int_{0}^{t}\theta_{s}\,{\rm d}B_{s}-\tfrac{1}{2} \int_{0}^{t}\|\theta_{s}\|^{2}\,{\rm d}s\big{)}.\] (29)

_If for each \(t\geq 0\),_

\[\mathbb{E}\big{[}\exp\big{(}\int_{0}^{t}\|\theta_{s}\|^{2}\,{\rm d }s\big{)}\big{]}<+\infty,\]

_then for each \(t\geq 0\),_

\[\mathbb{E}[Z(t)]=1.\] (30)

_Moreover, the process \(Z(t)\) is a positive martingale, i.e. if \(\left(\mathcal{F}_{t}\right)_{t\geq 0}\) is the filtration associated to the Brownian motion \(\left(B_{t}\right)_{t\geq 0}\), then for \(t\geq s\), \(\mathbb{E}[Z_{t}|\mathcal{F}_{s}]=Z_{s}\)._

**Theorem 3** (Girsanov theorem).: _Let \(W=\left(W_{t}\right)_{t\in[0,T]}\) be a standard Wiener process, and let \(\mathbb{P}\) be its induced probability measure over \(C([0,T];\mathbb{R}^{d})\), known as the Wiener measure. Let \(Z(t)\) be as defined in (29) and suppose that the assumptions of Theorem 2 hold. Let \((\Omega,\mathcal{F})\) be the \(\sigma\)-algebra associated to \(B_{T}\). For any \(F\in\mathcal{F}\), define the measure_

\[\mathbb{Q}(F)=\mathbb{E}_{\mathbb{P}}[Z(T)\mathbf{1}_{F}]\;.\]

\(\mathbb{Q}\) _is a probability measure because of (_30_). Under the probability measure_ \(\mathbb{Q}\)_, the stochastic process_ \(\{\tilde{W}(t)\}_{0\leq t\leq T}\) _defined as_

\[\tilde{W}(t)=W(t)-\int_{0}^{t}\theta_{s}\,\mathrm{d}s\]

_is a standard Wiener process. That is, for any \(n\geq 0\) and any \(0=t_{0}<t_{1}<\cdots<t_{n}\), the increments \(\{\tilde{W}(t_{i+1})-\tilde{W}(t_{i})\}_{i=0}^{n-1}\) are independent and \(\mathbb{Q}\)-Gaussian distributed with mean zero and covariance \((t_{i+1}-t_{i})\mathrm{I}\), which means that for any \(\alpha\in\mathbb{R}^{d}\), the moment generating function of \(\tilde{W}(t_{i+1})-\tilde{W}(t_{i})\) with respect to \(\mathbb{Q}\) is as follows:_

\[\mathbb{E}_{\mathbb{Q}}[\exp(\langle\alpha,\tilde{W}(t_{i+1})- \tilde{W}(t_{i})\rangle)]\] \[:=\mathbb{E}_{\mathbb{P}}\big{[}\exp\big{(}\big{\langle}\alpha,W( t_{i+1})-\int_{0}^{t_{i+1}}\theta_{s}\,\mathrm{d}s-W(t_{i})+\int_{0}^{t_{i}} \theta_{s}\,\mathrm{d}s\big{)}\big{)}Z(T)\big{]}=\exp\big{(}\tfrac{(t_{i+1}-t _{i})\|\alpha\|^{2}}{2}\big{)}.\]

**Corollary 1** (Girsanov theorem for SDEs).: _If the two SDEs_

\[\mathrm{d}X_{t} =b_{1}(X_{t},t)\,\mathrm{d}t+\sigma(X_{t},t)\,\mathrm{d}B_{t}, \qquad X_{0}=x_{\mathrm{init}}\] \[dY_{t} =(b_{1}(Y_{t},t)+b_{2}(Y_{t},t))\,\mathrm{d}t+\sigma(Y_{t},t)\, \mathrm{d}B_{t},\qquad Y_{0}=x_{\mathrm{init}}\]

_admit unique strong solutions on \([0,T]\), then for any bounded continuous functional \(\Phi\) on \(C([0,T])\), we have that_

\[\mathbb{E}[\Phi(X)] =\mathbb{E}\big{[}\Phi(Y)\exp\big{(}-\int_{0}^{T}\sigma(Y_{t},t) ^{-1}b_{2}(Y_{t},t)\,\mathrm{d}B_{t}-\tfrac{1}{2}\int_{0}^{T}\|\sigma(Y_{t},t) ^{-1}b_{2}(Y_{t},t)\|^{2}\,\mathrm{d}t\big{)}\big{]}\] \[=\mathbb{E}\big{[}\Phi(Y)\exp\big{(}-\int_{0}^{T}\sigma(Y_{t},t)^ {-1}b_{2}(Y_{t},t)\,d\tilde{B}_{t}+\tfrac{1}{2}\int_{0}^{T}\|\sigma(Y_{t},t)^{ -1}b_{2}(Y_{t},t)\|^{2}\,\mathrm{d}t\big{)}\big{]},\]

_where \(\tilde{B}_{t}=B_{t}+\int_{0}^{t}\sigma(Y_{s},s)^{-1}b_{2}(Y_{s},s)\,\mathrm{d}s\). More generally, \(b_{1}\) and \(b_{2}\) can be random processes that are adapted to filtration of \(B\)._

**Lemma 2**.: _For an arbitrary \(v\in\mathcal{U}\), let \(\mathbb{P}^{v}\) and \(\mathbb{P}\) be respectively the laws of the SDEs_

\[\mathrm{d}X_{t}^{v} =(b(X_{t}^{v},t)+\sigma(t)v(X_{t}^{v},t))\,\mathrm{d}t+\sqrt{ \lambda}\sigma(t)\mathrm{d}B_{t},\qquad X_{0}^{v}\sim p_{0},\] \[\mathrm{d}X_{t} =b(X_{t},t)\,\mathrm{d}t+\sqrt{\lambda}\sigma(t)\mathrm{d}B_{t}, \qquad X_{0}\sim p_{0}.\]

_We have that_

\[\tfrac{d\mathbb{P}}{d\mathbb{P}^{v}}(X^{v}) =\exp\big{(}-\lambda^{-1/2}\int_{0}^{T}\langle v(X_{t}^{v},t), \mathrm{d}B_{t}^{v}\rangle+\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|v(X_{t}^{v},t )\|^{2}\,\mathrm{d}t\big{)}\] (31) \[=\exp\big{(}-\lambda^{-1/2}\int_{0}^{T}\langle v(X_{t}^{v},t), \mathrm{d}B_{t}\rangle-\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|v(X_{t}^{v},t)\|^{ 2}\,\mathrm{d}t\big{)},\] \[\tfrac{d\mathbb{P}^{v}}{d\mathbb{P}}(X) =\exp\big{(}\lambda^{-1/2}\int_{0}^{T}\langle v(X_{t},t),\mathrm{ d}B_{t}\rangle-\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|v(X_{t},t)\|^{2}\,\mathrm{d}t \big{)}.\] (32)

_where \(B_{t}^{v}:=B_{t}+\lambda^{-1/2}\int_{0}^{t}v(X_{s}^{v},s)\,\mathrm{d}s\). For the optimal control \(u^{*}\), we have that_

\[\tfrac{d\mathbb{P}}{d\mathbb{P}^{u^{*}}}(X^{u^{*}}) =\exp\big{(}\lambda^{-1}\big{(}-V(X_{0}^{u^{*}},0)+\mathcal{W}(X^ {u^{*}},0)\big{)}\big{)},\] (33) \[\tfrac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}}(X) =\exp\big{(}\lambda^{-1}\big{(}V(X_{0},0)-\mathcal{W}(X,0)\big{)} \big{)},\] (34)

_where the functional \(\mathcal{W}\) is defined in (10)._

Proof.: The proof of (31)-(32) follows directly from Cor. 1. To prove (34), we use that by (28),

\[\mathcal{W}(X,0)=V(X_{0},0)+\tfrac{1}{2}\int_{0}^{T}\|u^{*}(X_{s},s)\|^{2}\, \mathrm{d}s-\sqrt{\lambda}\int_{0}^{T}\langle u^{*}(X_{s},s),\mathrm{d}B_{s}\rangle,\] (35)

which implies that

\[\tfrac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}}(X) =\exp\big{(}\lambda^{-1/2}\int_{0}^{T}\langle u^{*}(X_{t},t), \mathrm{d}B_{t}\rangle-\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|u^{*}(X_{t},t)\|^{2} \,\mathrm{d}t\big{)}\] \[=\exp\big{(}\lambda^{-1}\big{(}V(X_{0},0)-\mathcal{W}(X,0)\big{)} \big{)}.\]

[MISSING_PAGE_FAIL:19]

where \(B_{t}^{v}:=B_{t}+\lambda^{-1/2}\int_{0}^{t}v(X_{s}^{v},s)\,\mathrm{d}s\). Also,

\[\begin{split}\frac{d^{\mathrm{p}u^{*}}}{d\mathbb{P}^{u}}(X^{v})& =\exp\big{(}\lambda^{-1}\big{(}V(X_{0}^{v},0)-\mathcal{W}(X^{v},0) \big{)}\big{)},\\ \frac{d\mathbb{P}}{d\mathbb{P}^{v}}(X^{v})&=\exp\big{(}- \lambda^{-1/2}\int_{0}^{T}\langle v(X_{t}^{v},t),\mathrm{d}B_{t}\rangle-\tfrac {\lambda^{-1}}{2}\int_{0}^{T}\|v(X_{t}^{v},t)\|^{2}\,\mathrm{d}t\big{)}.\end{split}\] (42)

If we plug (41) and (42) into the right-hand side of (40), we obtain

\[\begin{split}&\mathbb{E}_{\mathbb{P}^{u^{*}}}\big{[}\log\frac{d \mathbb{P}^{u^{*}}}{d\mathbb{P}^{u}}(X^{u^{*}})\big{]}=-\mathbb{E}_{\mathbb{P}^ {u^{*}}}\big{[}(\log\frac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}^{u^{*}}}(X^{v})+ \log\frac{d\mathbb{P}}{d\mathbb{P}^{u^{*}}}(X^{v}))\frac{d\mathbb{P}^{u^{*}}} {d\mathbb{P}}(X^{v})\frac{d\mathbb{P}}{d\mathbb{P}^{u}}(X^{v})\big{]}\\ &=-\mathbb{E}\big{[}\big{(}\lambda^{-1/2}\int_{0}^{T}\langle u(X _{t}^{v},t),\mathrm{d}B_{t}\rangle+\lambda^{-1}\int_{0}^{T}\langle u(X_{t}^{v },t),v(X_{t}^{v},t)\rangle\,\mathrm{d}t\\ &\qquad\qquad-\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|u(X_{t}^{v},t )\|^{2}\,\mathrm{d}t+\lambda^{-1}\big{(}-V(X_{0}^{v},0)+\mathcal{W}(X^{v},0) \big{)}\big{)}\\ &\qquad\times\exp\big{(}\lambda^{-1}\big{(}V(X_{0}^{v},0)- \mathcal{W}(X^{v},0)\big{)}-\lambda^{-1/2}\int_{0}^{T}\langle v(X_{t}^{v},t), \mathrm{d}B_{t}\rangle-\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|v(X_{t}^{v},t)\|^{ 2}\,\mathrm{d}t\big{)}\big{]},\end{split}\]

which concludes the proof.

To show (38), we use that by Cor. 1,

\[\begin{split}\frac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}^{u^{*}}}(X^{u^ {*}})&\!=\!\exp\big{(}-\lambda^{-1/2}\int_{0}^{T}\langle u^{*}(X_{ t}^{u^{*}},t)\!-\!u(X_{t}^{u^{*}},t),\mathrm{d}B_{t}\rangle\!-\!\tfrac{\lambda^{-1}} {2}\int_{0}^{T}\|u^{*}(X_{t}^{u^{*}},t)\!-\!u(X_{t}^{u^{*}},t)\|^{2}\,\mathrm{d }t\big{)}.\end{split}\]

Hence,

Next, we prove (ii). The first instance of \(V(X_{0}^{v},0)\) in (37) can be removed without modifying the landscape of the loss. Hence, we are left with

\[\bar{\mathcal{L}}_{\mathrm{CE}}(u)=\mathbb{E}\big{[}\big{(}- \lambda^{-1/2}\int_{0}^{T}\langle u(X_{t}^{v},t),\mathrm{d}B_{t}^{v}\rangle- \lambda^{-1}\int_{0}^{T}\langle u(X_{t}^{v},t),v(X_{t}^{v},t)\rangle\, \mathrm{d}t\] (43) \[\qquad\qquad+\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|u(X_{t}^{v},t) \|^{2}\,\mathrm{d}t-\lambda^{-1}\mathcal{W}(X^{v},0)\big{)}\] \[\qquad\qquad\times\exp\big{(}\lambda^{-1}\big{(}V(X_{0}^{v},0)\!- \!\mathcal{W}(X^{v},0)\big{)}\!-\!\lambda^{-1/2}\int_{0}^{T}\langle v(X_{t}^{v },t),\mathrm{d}B_{t}\rangle\!-\!\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|v(X_{t}^{v },t)\|^{2}\,\mathrm{d}t\big{)}\big{]}.\]

And this can be expressed as

\[\bar{\mathcal{L}}_{\mathrm{CE}}(u)=\mathbb{E}\big{[}g(u;X_{0}^{v})\exp\big{(} \lambda^{-1}V(X_{0}^{v},0)\big{)}\big{]},\]

where

\[\begin{split} g(u;x)=\mathbb{E}\big{[}\big{(}-\lambda^{-1/2}\int_ {0}^{T}\langle u(X_{t}^{v},t),\mathrm{d}B_{t}^{v}\rangle-\lambda^{-1}\int_{0}^ {T}\langle u(X_{t}^{v},t),v(X_{t}^{v},t)\rangle\,\mathrm{d}t\\ +\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|u(X_{t}^{v},t)\|^{2}\, \mathrm{d}t-\lambda^{-1}\mathcal{W}(X^{v},0)\big{)}\\ \times\exp\big{(}-\lambda^{-1}\mathcal{W}(X^{v},0)\!-\!\lambda^{-1/2 }\int_{0}^{T}\langle v(X_{t}^{v},t),\mathrm{d}B_{t}\rangle\!-\!\tfrac{ \lambda^{-1}}{2}\int_{0}^{T}\|v(X_{t}^{v},t)\|^{2}\,\mathrm{d}t\big{)}|X_{0}^{v }=x\big{]}.\end{split}\]

If we consider \(g(u;x)\) as a loss function for \(u\), note that it is equivalent to the loss \(\bar{L}_{\mathrm{CE}}(u)\) equation in (43) for the choice \(p_{0}=\delta_{x}\), i.e., \(p_{0}\) concentrated at \(x\). Since the optimal control \(u^{*}\) is independent of the starting distribution \(p_{0}\), we deduce that \(u^{*}\) is the unique minimizer of \(g(u;x)\), for all \(x\in\mathbb{R}^{d}\). In consequence, \(u^{*}\) is the unique minimizer of \(\mathcal{L}_{\mathrm{CE}}(u)=\mathbb{E}[g(u;X_{0}^{v})]\).

To prove (39), note that up to a constant term, the only difference between \(\bar{\mathcal{L}}_{\mathrm{CE}}(u)\) and \(\mathcal{L}_{\mathrm{CE}}(u)\) is the expectation is reweighted importance weight \(\exp\big{(}-\lambda^{-1}V(X_{0}^{v},0)\big{)}\). 

**Lemma 4**.: _(i) We can rewrite_

\[\begin{split}\bar{\mathcal{L}}_{\mathrm{Var}_{v}}(u)&= \mathrm{Var}\big{(}\exp\big{(}\tilde{Y}_{T}^{u,v}-\lambda^{-1}g(X_{T}^{v})+ \lambda^{-1}V(X_{0}^{v},0)\big{)}\big{)},\\ \bar{\mathcal{L}}_{\mathrm{Var}_{v}}^{\mathrm{log}}(u)& =\mathrm{Var}\big{(}\tilde{Y}_{T}^{u,v}-\lambda^{-1}g(X_{T}^{v})+\lambda^{-1}V (X_{0}^{v},0)\big{)}.\end{split}\]

_When \(p_{0}\) is concentrated at \(x_{\mathrm{init}}\), the terms \(V(x_{\mathrm{init}},0)\) are constants and can be removed without modifying the landscape. In other words, \(\bar{\mathcal{L}}_{\mathrm{Var}_{v}}\) and \(\bar{\mathcal{L}}_{\mathrm{Var}_{v}}^{\mathrm{log}}\) are equal to \(\mathcal{L}_{\mathrm{Var}_{v}}\) and \(\mathcal{L}_{\mathrm{Var}_{v}}^{\mathrm{log}}\) up to a constant term and a constant factor, respectively.__(ii) When \(p_{0}\) is general, \(\tilde{\mathcal{L}}_{\mathrm{Var}_{v}}\) and \(\mathcal{L}_{\mathrm{Var}_{v}}\) have a different landscape, and the optimum of \(\mathcal{L}_{\mathrm{Var}_{v}}\) may be different from \(u^{*}\). A related loss that does preserve the optimum is:_

\[\bar{\mathcal{L}}_{\mathrm{Var}_{v}}(u) =\mathbb{E}[\mathrm{Var}_{\mathbb{P}^{v}}(\tfrac{d\mathbb{P}^{u^{* }}}{d\mathbb{P}^{u}}(X^{v})|X^{v}_{0})\exp(-\lambda^{-1}V(X^{v}_{0},0))]\] \[=\mathbb{E}[\mathrm{Var}\big{(}\exp(\tilde{Y}^{u,v}_{T}-\lambda^ {-1}g(X^{v}_{T}))|X^{v}_{0}\big{)}].\]

_In practice, this is implemented by sampling the \(m\) trajectories in one batch starting at the same point \(X^{v}_{0}\)._

_(iii) Also, \(\tilde{\mathcal{L}}_{\mathrm{Var}_{v}}^{\mathrm{log}}\) and \(\mathcal{L}_{\mathrm{Var}_{v}}^{\mathrm{log}}\) have a different landscape, and the optimum of \(\mathcal{L}_{\mathrm{Var}_{v}}^{\mathrm{log}}\) may be different from \(u^{*}\). In particular, \(\mathcal{L}_{\mathrm{Var}_{v}}^{\mathrm{log}}(u)=\mathrm{Var}_{\mathbb{P}^{v} }(\log\tfrac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}^{u}}(X^{v})\exp(-\lambda^{-1}V(X ^{v}_{0},0)))\). A loss that does preserve the optimum \(u^{*}\) is_

\[\bar{\mathcal{L}}_{\mathrm{Var}_{v}}^{\mathrm{log}}(u) =\mathbb{E}[\mathrm{Var}_{\mathbb{P}^{v}}(\log\tfrac{d\mathbb{P}^ {u^{*}}}{d\mathbb{P}^{u}}(X^{v})|X^{v}_{0})\exp(-\lambda^{-1}V(X^{v}_{0},0))]\] \[=\mathbb{E}[\mathrm{Var}\big{(}\tilde{Y}^{u,v}_{T}-\lambda^{-1}g( X^{v}_{T})|X^{v}_{0}\big{)}].\]

Proof.: Using (34) and (31), we have that

\[\tfrac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}^{u}}(X^{v}) =\exp\big{(}\lambda^{-1}\big{(}V(X^{v}_{0},0)-\mathcal{W}(X^{v},0 )\big{)}\big{)},\] \[\tfrac{d\mathbb{P}}{d\mathbb{P}^{u}}(X^{v}) =\exp\big{(}-\lambda^{-1/2}\int_{0}^{T}\langle u(X^{v}_{t},t), \mathrm{d}B^{v}_{t}\rangle+\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|u(X^{v}_{t},t) \|^{2}\,\mathrm{d}t\big{)}\] \[=\exp\big{(}-\lambda^{-1/2}\int_{0}^{T}\langle u(X^{v}_{t},t), \mathrm{d}B_{t}\rangle-\lambda^{-1}\int_{0}^{T}\langle u(X^{v}_{t},t),v(X^{v}_ {t},t)\rangle\,\mathrm{d}t\] \[\qquad\qquad+\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|u(X^{v}_{t},t) \|^{2}\,\mathrm{d}t\big{)}.\]

Hence,

\[\log\tfrac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}^{u}}(X^{v})=\log\tfrac{d\mathbb{P} ^{u^{*}}}{d\mathbb{P}}(X^{v})+\log\tfrac{d\mathbb{P}}{d\mathbb{P}^{u}}(X^{v}) =\tilde{Y}^{u,v}_{T}-\lambda^{-1}g(X^{v}_{T})+\lambda^{-1}V(X^{v}_{0},0).\]

Since \(\tilde{\mathcal{L}}_{\mathrm{Var}_{v}}(u)=\mathrm{Var}_{\mathbb{P}^{v}}( \tfrac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}^{u}})\) and \(\tilde{\mathcal{L}}_{\mathrm{Var}_{v}}^{\mathrm{log}}(u)=\mathrm{Var}_{ \mathbb{P}^{v}}(\log\tfrac{d\mathbb{P}^{u^{*}}}{d\mathbb{P}^{u}})\), this concludes the proof of (i).

To prove (ii), note that for general \(p_{0}\), \(V(X^{v}_{0},0)\) is no longer a constant, but it is if we condition on \(X^{v}_{0}\). The proof of (iii) is analogous. 

## Appendix C Proofs of Sec. 3

### Proof of Thm. 1 and Prop. 2

We prove Thm. 1 and Prop. 2 at the same time. Recall that by (9), the optimal control is of the form \(u^{*}(x,t)=-\sigma(t)^{\top}\nabla V(x,t)\). Consider the loss

\[\tilde{\mathcal{L}}(u)=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}u(X_{t},t)+\sigma(t)^{\top}\nabla V(X_{t},t)\big{\|}^{2}\,\mathrm{d}t\,\exp\big{(}- \lambda^{-1}\int_{0}^{T}f(X_{t},t)\,\mathrm{d}t-\lambda^{-1}g(X_{T})\big{)} \big{]}.\]

Clearly, the unique optimum of \(\tilde{\mathcal{L}}\) is \(-\sigma(t)^{\top}\nabla V\). We can rewrite \(\tilde{\mathcal{L}}\) as

\[\tilde{\mathcal{L}}(u)=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T} \big{(}\big{\|}u(X_{t},t)\big{\|}^{2}+2\langle u(X_{t},t),\sigma(t)^{\top} \nabla V(X_{t},t)\rangle+\|\sigma(t)^{\top}\nabla V(X_{t},t)\big{\|}^{2}\big{)} \,\mathrm{d}t\] (44) \[\qquad\qquad\qquad\times\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X_{t },t)\,\mathrm{d}t-\lambda^{-1}g(X_{T})\big{)}\big{]}\;.\]

Hence, we can express \(\tilde{\mathcal{L}}\) as a sum of three terms: one involving \(\|u(X_{t},t)\|^{2}\), another involving \(\langle u(X_{t},t),\sigma(t)^{\top}V(X_{t},t)\rangle\), and a third one, which is constant with respect to \(u\), involving \(\|\nabla V(X_{t},t)\|^{2}\). The following lemma provides an alternative expression for the cross term:

**Lemma 5**.: _The following equality holds:_

\[\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\langle u(X_{t},t),\sigma( t)^{\top}\nabla V(X_{t},t)\rangle\,\mathrm{d}t\,\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X_{t },t)\,\mathrm{d}t-\lambda^{-1}g(X_{T})\big{)}\big{]}\] (45) \[=-\lambda\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\langle}u(X_ {t},t),\sigma(t)^{\top}\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\int_{t}^ {T}f(X_{s},s)\,\mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x\big{]}\big{]}\] \[\qquad\qquad\times\exp\big{(}-\lambda^{-1}\int_{0}^{t}f(X_{s},s)\, \mathrm{d}s\big{)}\,\mathrm{d}t\big{]}.\]Proof.: Recall the definition of \(\mathcal{W}(X,t)\) in (35), which means that

\[\mathcal{W}(X,0)=\mathcal{W}(X,t)+\int_{0}^{t}f(X_{s},s)\,\mathrm{d}s.\] (46)

Let \(\{\mathcal{F}_{t}\}_{t\in[0,T]}\) be the filtration generated by the Brownian motion \(B\). Then, equation (9) implies that

\[\sigma(t)^{\top}\nabla V(X_{t},t)=-\tfrac{\lambda\sigma(t)^{\top}\nabla_{x} \mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\mathcal{W}(X,t)\big{)}\big{|} \mathcal{F}_{t}\big{]}}{\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\mathcal{W}(X,t)\big{)}\big{|}\mathcal{F}_{t}\big{]}}\] (47)

We proceed as follows:

\[\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\langle u(X_{t},t), \sigma(t)^{\top}\nabla V(X_{t},t)\rangle\,\mathrm{d}t\,\exp\big{(}-\lambda^{-1 }\mathcal{W}(X,0)\big{)}\big{]}\] \[\overset{\text{(i)}}{=}-\lambda\mathbb{E}\big{[}\tfrac{1}{T}\int _{0}^{T}\big{\langle}u(X_{t},t),\sigma(t)^{\top}\nabla_{x}\mathbb{E}\big{[} \exp\big{(}-\lambda^{-1}\mathcal{W}(X,t)\big{)}\big{|}X_{t}=x\big{]}\big{\rangle} \exp\big{(}-\lambda^{-1}\int_{0}^{t}f(X_{s},s)\,\mathrm{d}s\big{)}\,\mathrm{d }t\big{]}\] \[=-\lambda\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\langle}u (X_{t},t),\sigma(t)^{\top}\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1} \mathcal{W}(X,t)\big{)}\big{|}\mathcal{F}_{t}\big{]}\big{\rangle}\exp\big{(}- \lambda^{-1}\int_{0}^{t}f(X_{s},s)\,\mathrm{d}s\big{)}\,\mathrm{d}t\big{]}\] \[\overset{\text{(ii)}}{=}-\lambda\mathbb{E}\big{[}\tfrac{1}{T}\int _{0}^{T}\big{\langle}u(X_{t},t),\sigma(t)^{\top}\nabla_{x}\mathbb{E}\big{[} \exp\big{(}-\lambda^{-1}\mathcal{W}(X,t)\big{)}\big{|}X_{t}=x\big{]}\big{\rangle} \exp\big{(}-\lambda^{-1}\int_{0}^{t}f(X_{s},s)\,\mathrm{d}s\big{)}\,\mathrm{d }t\big{]}.\]

Here, (i) holds by equation (47), the law of total expectation and equation (46), and (ii) holds by the Markov property of the solution of an SDE. 

The following proposition, which we prove in Subsec. C.2, provides an alternative expression for \(\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\int_{t}^{T}f(X_{s},s)\, \mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x\big{]}\). The technique, which is novel and we denote by _Girsanov reparameterization trick_, is of independent interest and may be applied in other settings, as we discuss in Sec. 5.

**Proposition 1** (Path-wise reparameterization trick for stochastic optimal control).: _For each \(t\in[0,T]\), let \(M_{t}:[t,T]\to\mathbb{R}^{d\times d}\) be an arbitrary continuously differentiable function matrix-valued function such that \(M_{t}(t)=\mathrm{Id}\). We have that_

\[\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\int_{t}^{T}f(X _{s},s)\,\mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x\big{]}\] (23) \[=\mathbb{E}\big{[}\big{(}-\lambda^{-1}\int_{t}^{T}M_{t}(s)\nabla _{x}f(X_{s},s)\,\mathrm{d}s-\lambda^{-1}M_{t}(T)\nabla g(X_{T})\] \[\qquad\qquad+\lambda^{-1/2}\int_{t}^{T}(M_{t}(s)\nabla_{x}b(X_{s},s)-\partial_{s}M_{t}(s))(\sigma^{-1})^{\top}(s)\mathrm{d}B_{s}\big{)}\] \[\qquad\qquad\times\exp\big{(}-\lambda^{-1}\int_{t}^{T}f(X_{s},s) \,\mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x\big{]}.\]

Plugging (23) into the right-hand side of (45), we obtain that

\[\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\langle u(X_{t},t), \sigma(t)^{\top}\nabla V(X_{t},t)\rangle\,\mathrm{d}t\,\exp\big{(}-\lambda^{-1 }\int_{0}^{T}f(X_{t},t)\,\mathrm{d}t-\lambda^{-1}g(X_{T})\big{)}\big{]}\] \[=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\langle}u(X_{t},t),\sigma(t)^{\top}\big{(}\int_{t}^{T}M_{t}(s)\nabla_{x}f(X_{s},s)\,\mathrm{d}s+M _{t}(T)\nabla g(X_{T})\] \[\qquad\qquad-\lambda^{1/2}\int_{t}^{T}(M_{t}(s)\nabla_{x}b(X_{s},s)-\partial_{s}M_{t}(s))(\sigma^{-1})^{\top}(s)\mathrm{d}B_{s}\big{)}\big{\rangle} \,\mathrm{d}t\] \[\qquad\qquad\times\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X_{t},t)\, \mathrm{d}t-\lambda^{-1}g(X_{T})\big{)}\big{]}.\]

If we plug this into the right-hand side of (44) and complete the squared norm, we get that

\[\tilde{\mathcal{L}}(u)=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T} \big{(}\big{\|}u(X_{t},t)-\tilde{w}(t,X,B,M_{t})\big{\|}^{2}-\big{\|}\tilde{w}( t,X,B,M_{t})\big{\|}^{2}\] \[\qquad\qquad+\big{\|}u^{*}(X_{t},t)\big{\|}^{2}\big{)}\,\mathrm{d}t \exp\big{(}-\lambda^{-1}\mathcal{W}(X,0)\big{)}\big{]}\]

where \(\tilde{w}\) is defined as:

\[\tilde{w}(t,X,B,M_{t})=\sigma(t)^{\top}\big{(}-\int_{t}^{T}M_{t}(s) \nabla_{x}f(X_{s},s)\,\mathrm{d}s-M_{t}(T)\nabla g(X_{T})\] \[\qquad\qquad+\lambda^{1/2}\int_{t}^{T}(M_{t}(s)\nabla_{x}b(X_{s},s)-\partial_{s}M_{t}(s))(\sigma^{-1})^{\top}(s)\mathrm{d}B_{s}\big{)}.\]

We also define \(\Phi(u;X,B)\) as

\[\Phi(u;X,B)=\tfrac{1}{T}\int_{0}^{T}(\big{\|}u(X_{t},t)-\tilde{w}(t,X,B,M_{t}) \big{\|}^{2})\,\mathrm{d}t.\]Now, by the Girsanov theorem (Thm. 3), we have that for an arbitrary control \(v\in\mathcal{U}\),

\[\mathbb{E}[\Phi(u;X,B)\exp\big{(}-\lambda^{-1}\mathcal{W}(X,0))]\] \[= \mathbb{E}\big{[}\Phi(u;X^{v},B^{v})\exp\big{(}-\lambda^{-1} \mathcal{W}(X^{v},0)-\lambda^{-1/2}\int_{0}^{T}\!\langle v(X^{v}_{t},t), \mathrm{d}B^{v}_{t}\rangle+\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|v(X^{v}_{t},t )\|^{2}\,\mathrm{d}t\big{)}\big{]}\] \[= \mathbb{E}\big{[}\Phi(u;X^{v},B^{v})\exp\big{(}-\lambda^{-1} \mathcal{W}(X^{v},0)-\lambda^{-1/2}\int_{0}^{T}\!\langle v(X^{v}_{t},t), \mathrm{d}B_{t}\rangle-\tfrac{\lambda^{-1}}{2}\int_{0}^{T}\|v(X^{v}_{t},t)\|^{ 2}\,\mathrm{d}t\big{)}\big{]},\]

where \(B^{v}_{t}:=B_{t}+\lambda^{-1/2}\int_{0}^{t}v(X^{v}_{s},s)\,\mathrm{d}s\). Reexpressing \(B^{v}\) in terms of \(B\), we can rewrite \(\Phi(u;X^{v},B^{v})\) and \(\tilde{w}(t,X^{v},B^{v},M_{t})\) as follows:

\[\Phi(u;X^{v},B^{v}) =\tfrac{1}{T}\int_{0}^{T}\big{\|}u(X^{v}_{t},t)-\tilde{w}(t,X^{v},B^{v},M_{t})\big{\|}^{2}\,\mathrm{d}t,\] \[\tilde{w}(t,X^{v},B^{v},M_{t}) =\sigma(t)^{\top}\big{(}-\int_{t}^{T}M_{t}(s)\nabla_{x}f(X^{v}_{s},s)\,\mathrm{d}s-M_{t}(T)\nabla g(X^{v}_{T})\] \[\qquad\qquad\qquad+\lambda^{1/2}\int_{t}^{T}(M_{t}(s)\nabla_{x}b( X^{v}_{s},s)-\partial_{s}M_{t}(s))(\sigma^{-1})^{\top}(X^{v}_{s},s)\mathrm{d}B_{s}\] \[\qquad\qquad+\int_{t}^{T}(M_{t}(s)\nabla_{x}b(X^{v}_{s},s)- \partial_{s}M_{t}(s))(\sigma^{-1})^{\top}(X^{v}_{s},s)v(X^{v}_{s},s)\mathrm{d} s\big{)}.\]

Putting everything together, we obtain that

\[\tilde{\mathcal{L}}(u)=\mathcal{L}_{\mathrm{SOCM}}(u,M)-K,\]

where \(\mathcal{L}(u,M)\) is the loss defined in (19) (note that \(w(t,v,X^{v},B,M_{t}):=\tilde{w}(t,X^{v},B^{v},M_{t})\)), and

\[K=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\!\big{(}\big{\|}\tilde{w}(t,X,B,M_ {t})\big{\|}^{2}-\big{\|}u^{*}(X_{t},t)\big{\|}^{2}\big{)}\,\mathrm{d}t\,\exp \big{(}-\lambda^{-1}\mathcal{W}(X,0)\big{)}\big{]}\]

To complete the proof of equation (24), remark that \(\tilde{\mathcal{L}}(u)\) can be rewritten as

\[\tilde{\mathcal{L}}(u) =\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}u(X_{t},t)-u^{* }(X_{t},t)\big{\|}^{2}\,\mathrm{d}t\,\exp\big{(}-\lambda^{-1}\mathcal{W}(X,0) \big{)}\big{]}\] \[=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}u(X_{t},t)-u^{* }(X_{t},t)\big{\|}^{2}\,\mathrm{d}t\,\frac{\mathrm{d}{\tilde{w}}^{*}}{\mathrm{ d}{\tilde{w}}^{*}}(X)\exp(-\lambda^{-1}V(X_{0},0))\big{]}\] \[=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}u(X_{t}^{u^{*}},t)-u^{*}(X_{t}^{u^{*}},t)\big{\|}^{2}\,\mathrm{d}t\,\exp(-\lambda^{-1}V(X_{0} ^{u^{*}},0))\big{]}.\]

It only remains to reexpress \(K\). Note that by Prop. 1, we have that

\[u^{*}(X_{t},t) =\tfrac{\mathbb{E}\big{[}\tilde{w}(t,X,B,M_{t})\exp\big{(}- \lambda^{-1}\mathcal{W}(X,0)\big{)}|_{\mathcal{F}_{t}}\big{]}}{\mathbb{E} \big{[}\exp\big{(}-\lambda^{-1}\mathcal{W}(X,0)\big{)}|_{\mathcal{F}_{t}}\big{]} }\] \[=\tfrac{\mathbb{E}\big{[}\tilde{w}(t,X,B,M_{t})\frac{d{\tilde{w}}^{ *}}{d{\tilde{w}}^{*}}(X)|_{\mathcal{F}_{t}}\big{]}\exp(-\lambda^{-1}V(X_{0},0 ))}{\mathbb{E}\big{[}\frac{d{\tilde{w}}^{*}}{d{\tilde{w}}^{*}}(X)|_{\mathcal{ F}_{t}}\big{]}\exp(-\lambda^{-1}V(X_{0},0))}=\tfrac{\mathbb{E}\big{[}\tilde{w}(t,X,B,M_{t}) \frac{d{\tilde{w}}^{*}}{d{\tilde{w}}^{*}}(X)|_{\mathcal{F}_{t}}\big{]}}{ \mathbb{E}\big{[}\frac{d{\tilde{w}}^{*}}{d{\tilde{w}}^{*}}(X)|_{\mathcal{F}_{t} }\big{]}}\] \[=\mathbb{E}\big{[}\tilde{w}(t,X^{u^{*}},B^{u^{*}},M_{t})|X_{t}^{ u^{*}}=X_{t}\big{]}\]

Hence, using the Girsanov theorem (Thm. 3) several times, we have that

\[K =\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}\tilde{w}(t,X^{u^ {*}},B^{u^{*}},M_{t})\big{\|}^{2}-\big{\|}\mathbb{E}\big{[}\tilde{w}(t,X^{u^{*}},B^{u^{*}},M_{t})|X_{t}^{u^{*}}\big{]}\big{\|}^{2}\,\mathrm{d}t\,\exp(-\lambda^{-1 }V(X_{0}^{u^{*}},0))\big{]}\] (48) \[=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}\tilde{w}(t,X^{u ^{*}},B^{u^{*}},M_{t})-\mathbb{E}\big{[}\tilde{w}(t,X^{u^{*}},B^{u^{*}},M_{t})|X_{ t}^{u^{*}}\big{]}\big{\|}^{2}\,\mathrm{d}t\,\exp(-\lambda^{-1}V(X_{0}^{u^{*}},0)) \big{]}\] \[=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}\tilde{w}(t,X,B,M_ {t})-\tfrac{\mathbb{E}[\tilde{w}(t,X,B,M_{t})\exp(-\lambda^{-1}\mathcal{W}(X,0))|X_{ t}]}{\mathbb{E}[\exp(-\lambda^{-1}\mathcal{W}(X,0))|X_{t}]}\big{\|}^{2}\,\mathrm{d}t\,\exp(- \lambda^{-1}\mathcal{W}(X,0))\big{]}\] \[=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}w(t,v,X^{v},B,M_ {t})-\tfrac{\mathbb{E}[w(t,v,X^{v},B,M_{t})\alpha(v,X^{v},B)|X_{t}^{v}]}{ \mathbb{E}[\alpha(v,X^{v},B)|X_{t}^{v}]}\big{\|}^{2}\,\mathrm{d}t\,\alpha(v,X^{v},B)\big{]},\]

which concludes the proof, noticing that \(K=\mathrm{Var}(w;M)\).

**Remark 1** (The optimal \(M_{t}\) is the same for all \(v\)).: _Looking at equation (48), we observe that \(\mathrm{Var}(w;M)\) does not depend on the base control \(v\). Since minimizing \(\mathcal{L}_{\mathrm{SOCM}}(u,M)\) with respect to \(M\) is equivalent to minimizing \(\mathrm{Var}(w;M)\), we deduce that the optimal \(M\) does not depend on the vector field \(v\)._

### Proof of the path-wise reparameterization trick (Prop. 1)

We prove a more general statement (Prop. 4), and show that Prop. 1 is a particular case of it.

**Proposition 4** (Path-wise reparameterization trick).: _Let \((\Omega,\mathcal{F},\mathbb{P})\) be a probability space, and \(B:\Omega\times[0,T]\to\mathbb{R}^{d}\) be a Brownian motion. Let \(X:\Omega\times[0,T]\to\mathbb{R}^{d}\) be the uncontrolled process given by (6), and let \(\psi:\Omega\times\mathbb{R}^{d}\times[0,T]\to\mathbb{R}^{d}\) be an arbitrary random process such that:_

* _For all_ \(z\in\mathbb{R}^{d}\)_, the process_ \(\psi(\cdot,z,\cdot):\Omega\times[0,T]\to\mathbb{R}^{d}\) _is adapted to the filtration_ \((\mathcal{F}_{s})_{s\in[0,T]}\) _of the Brownian motion_ \(B\)_._
* _For all_ \(\omega\in\Omega\)_,_ \(\psi(\omega,\cdot,\cdot):\mathbb{R}^{d}\times[0,T]\to\mathbb{R}^{d}\) _is a twice-continuously differentiable function such that_ \(\psi(\omega,z,0)=z\) _for all_ \(z\in\mathbb{R}^{d}\)_, and_ \(\psi(\omega,0,s)=0\) _for all_ \(s\in[0,T]\)_._

_Let \(F:C([0,T];\mathbb{R}^{d})\to\mathbb{R}\) be a Frechet-differentiable functional. We use the notation \(X+\psi(z,\cdot)=(X_{s}(\omega)+\psi(\omega,z,s))_{s\in[0,T]}\) to denote the shifted process, and we will omit the dependency of \(\psi\) on \(\omega\) in the proof. Then,_

\[\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-F(X)\big{)}\big{|}X_{0}=x \big{]}\] (49) \[= \mathbb{E}\big{[}\big{(}\!-\!\nabla_{z}F(X\!+\!\psi(z,\cdot))|_{z =0}\!+\!\lambda^{-1/2}\int_{0}^{T}(\nabla_{z}\psi(0,s)\nabla_{x}b(X_{s},s)\!- \!\nabla_{z}\partial_{s}\psi(0,s))(\sigma^{-1})^{\top}(s)\mathrm{d}B_{s}\big{)}\] \[\times\exp\big{(}-F(X)\big{)}\big{|}X_{0}=x\big{]}\]

Proof of Prop. 1.: Given a family of functions \((M_{t})_{t\in[0,T]}\) satisfying the conditions in Prop. 1, we can define a family \((\psi_{t})_{t\in[0,T]}\) of functions \(\psi_{t}:\mathbb{R}^{d}\times[t,T]\to\mathbb{R}^{d}\) as \(\psi_{t}(z,s)=M_{t}(s)^{\top}z\). Note that \(\psi_{t}(z,t)=z\) for all \(z\in\mathbb{R}^{d}\) and \(\psi_{t}(0,s)=0\) for all \(s\in[t,T]\), and that \(\nabla_{z}\psi_{t}(z,s)=M_{t}(s)\). Hence, \(\psi_{t}\) can be seen as a random process which is constant with respect to \(\omega\in\Omega\), and which fulfills the conditions in Prop. 4 up to a trivial time change of variable from \([t,T]\) to \([0,T]\).

We also define the family \((F_{t})_{t\in[0,T]}\) of functionals \(F_{t}:C([t,T];\mathbb{R}^{d})\to\mathbb{R}\) as \(F_{t}(X)=\lambda^{-1}\int_{t}^{T}f(X_{s},s)\,\mathrm{d}s+\lambda^{-1}g(X_{T})\). We have that

\[\nabla_{z}F_{t}(X\!+\!\psi_{t}(z,\cdot))\] \[=\nabla_{z}\big{(}\lambda^{-1}\int_{t}^{T}f(X_{s}+\psi_{t}(z,s),s) \,\mathrm{d}s+\lambda^{-1}g(X_{T}+\psi_{t}(z,T))\big{)}\] \[\stackrel{{\text{\rm(i)}}}{{=}} \lambda^{-1}\int_{t}^{T}\nabla_{z}\psi_{t}(z,s)\nabla f(X_{s}\!+\! \psi_{t}(z,s),s)\,\mathrm{d}s\!+\!\lambda^{-1}\nabla_{z}\psi_{t}(z,T)\nabla g (X_{T}\!+\!\psi_{t}(z,T))\] \[= \lambda^{-1}\int_{t}^{T}M_{t}(s)\nabla f(X_{s}\!+\!\psi_{t}(z,s), s)\,\mathrm{d}s\!+\!\lambda^{-1}M_{t}(T)\nabla g(X_{T}\!+\!\psi_{t}(z,T)),\]

where equality (i) holds by the Leibniz rule. Using that \(\psi_{t}(0,s)=0\), we obtain that:

\[\nabla_{z}F_{t}(X+\psi_{t}(z,\cdot))\big{|}_{z=0}=\lambda^{-1}\int_{t}^{T} \nabla_{z}\psi_{t}(0,s)\nabla f(X_{s},s)\,\mathrm{d}s+\lambda^{-1}\nabla_{z} \psi_{t}(T,0)\nabla g(X_{T}),\]

Up to a trivial time change of variable from \([t,T]\) to \([0,T]\), Prop. 1 follows from plugging these choices into equation (49).

**Remark 2**.: _We can use matrices \(M_{t}(s)\) that depend on the process \(X\) up to time \(s\), since the resulting processes \(\psi_{t}(\cdot,z,\cdot)\) are adapted to the filtration of the Brownian motion \(B\). More specifically, if we let \(M_{t}:\mathbb{R}^{d}\times[t,T]\to\mathbb{R}^{d\times d}\) be an arbitrary continuously differentiable function matrix-valued function such that \(M_{t}(x,t)=\mathrm{Id}\) for all \(x\in\mathbb{R}^{d}\), and we define the exponential moving average of \(X\) as the process \(X^{(v)}\) given by_

\[X^{(v)}_{t}=\upsilon\int_{0}^{t}e^{-\upsilon(t-s)}X_{s}\,\mathrm{d}s,\]

_we have that_

\[\tfrac{\mathrm{d}}{\mathrm{d}s}M_{t}(X^{(v)}_{s},s)\!=\!\langle\nabla M_{t}(X^{ (v)}_{s},s),\tfrac{\mathrm{d}X^{(v)}_{s}}{\mathrm{d}s}\rangle\!+\!\partial_{s}M _{t}(X^{(v)}_{s},s)\!=\!\lambda\langle\nabla_{x}M_{t}(X^{(v)}_{s},s),X_{s}\!-\!X ^{(v)}_{s}\rangle\!+\!\partial_{s}M_{t}(X^{(v)}_{s},s),\]_and we can write_

\[\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\int_{t}^{T}f(X_{s },s)\,\mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x\big{]}\] \[=\mathbb{E}\big{[}\big{(}-\lambda^{-1}\int_{t}^{T}M_{t}(X_{s}^{(v )},s)\nabla_{x}f(X_{s},s)\,\mathrm{d}s-\lambda^{-1}M_{t}(X_{T}^{(v)},T)\nabla g (X_{T})\] \[\qquad\qquad+\lambda^{-1/2}\int_{t}^{T}(M_{t}(X_{s}^{(v)},s)\nabla _{x}b(X_{s},s)-\tfrac{\mathrm{d}}{\mathrm{d}s}M_{t}(X_{s}^{(v)},s))(\sigma^{-1} )^{\top}(s)\mathrm{d}B_{s}\big{)}\] \[\qquad\qquad\times\exp\big{(}-\lambda^{-1}\int_{t}^{T}f(X_{s},s)\, \mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{t}=x\big{]}.\]

_Plugging this into the proof of Thm. 1, we would obtain a variant of SOCM (Alg. 2) where the matrix-valued neural network \(M_{\omega}\) takes inputs \((t,s,x)\) instead of \((t,s)\). Since the optimization class is larger, from the bias-variance in Prop. 2 we deduce that this variant would yield a lower variance of the vector field \(w\), and likely an algorithm with lower error. This is at the expense of an increased number of function evaluations (NFE) of \(M_{\omega}\); one would need \(\frac{K(K+1)m}{2}\) NFE per batch instead of only \(\frac{K(K+1)}{2}\), which may be too expensive if the architecture of \(M_{\omega}\) is large. A way to speed up the computation per batch is to parameterize \(M_{t}\) using cubic splines._

Proof of Prop. 4.: Recall that

\[\mathrm{d}X_{s}=b(X_{s},s)\,\mathrm{d}s+\sqrt{\lambda}\sigma(s)\,\mathrm{d}B_ {s},\qquad X_{0}\sim p_{0},\]

is the SDE for the uncontrolled process. For arbitrary \(x,z\in\mathbb{R}^{d}\), we consider the following SDEs conditioned on the initial points:

\[\mathrm{d}X_{s}^{(x+z)} =b(X_{s}^{(x+z)},s)\,\mathrm{d}s+\sqrt{\lambda}\sigma(s)\,\mathrm{ d}B_{s},\qquad X_{0}^{(x+z)}=x+z,\] (50) \[\mathrm{d}X_{s}^{(x)} =b(X_{s}^{(x)},s)\,\mathrm{d}s+\sqrt{\lambda}\sigma(s)\,\mathrm{ d}B_{s},\qquad X_{0}^{(x)}=x.\] (51)

Suppose that \(\psi:\mathbb{R}^{d}\times[0,T]\to\mathbb{R}^{d}\) satisfies the properties in the statement of Prop. 4. If \(\tilde{X}^{(x)}\) is a solution of

\[\mathrm{d}\tilde{X}_{s}^{(x)}=(b(\tilde{X}_{s}^{(x)}+\psi(z,s),s)-\partial_{s }\psi(z,s))\,\mathrm{d}s+\sqrt{\lambda}\sigma(s)\,\mathrm{d}B_{s},\qquad \tilde{X}_{0}^{(x)}=x,\]

then \(X^{(x+z)}=\tilde{X}^{(x)}+\psi(z,\cdot)\) is a solution of (50). This is because \(X_{0}^{(x+z)}=\tilde{X}_{0}^{(x)}+\psi(z,0)=\tilde{X}_{0}^{(x)}+z=x+z\), and

\[\mathrm{d}X_{s}^{(x+z)} =\mathrm{d}\tilde{X}_{s}^{(x)}+\partial_{s}\psi(z,s)\,\mathrm{d}s\] \[=(b(\tilde{X}_{s}^{(x)}+\psi(z,s),s)-\partial_{s}\psi(z,s))\, \mathrm{d}s+\sqrt{\lambda}\sigma(s)\,\mathrm{d}B_{s}+\partial_{s}\psi(z,s)\, \mathrm{d}s\] \[=b(X_{s}^{(x+z)},s)\,\mathrm{d}s+\sqrt{\lambda}\sigma(s)\,\mathrm{ d}B_{s},\]

Note that we may rewrite (51) as

\[\mathrm{d}X_{s}^{(x)}=(b(X_{s}^{(x)}+\psi(z,s),s)-\partial_{s}\psi (z,s))\,\mathrm{d}s\] \[\qquad\qquad+(b(X_{s}^{(x)},s)-b(X_{s}^{(x)}+\psi(z,s),s)+\partial _{s}\psi(z,s))\,\mathrm{d}s+\sqrt{\lambda}\sigma(s)\,\mathrm{d}B_{s},\qquad X _{t}^{(x)}\sim p_{0}.\]

Hence, since \(\psi(z,s)\) is a random process adapted to the filtration of \(B\), we can apply the Girsanov theorem for SDEs (Corollary 1) on \(\tilde{X}^{(x)}\) and \(X^{(x)}\), and we have that for any bounded continuous functional \(\Phi\),

\[\mathbb{E}[\Phi(\tilde{X}^{(x)})]\] \[=\mathbb{E}\big{[}\Phi(X^{(x)})\exp\big{(}\int_{0}^{T}\lambda^{- 1/2}\sigma(s)^{-1}(b(X_{s}^{(x)}+\psi(z,s),s)-b(X_{s}^{(x)},s)-\partial_{s} \psi(z,s))\,\mathrm{d}B_{s}\] \[\qquad\qquad\qquad-\tfrac{1}{2}\int_{0}^{T}\|\lambda^{-1/2}\sigma( s)^{-1}(b(X_{s}^{(x)}+\psi(z,s),s)-b(X_{s}^{(x)},s)-\partial_{s}\psi(z,s))\|^{2}\, \mathrm{d}s)\big{]}.\] (52)We can write

\[\mathbb{E}\big{[}\exp\big{(}-F(X)\big{)}\big{|}X_{0}=x+z\big{]} \stackrel{{\text{(i)}}}{{=}}\mathbb{E}\big{[}\exp\big{(}-F(X^{(x+z)}) \big{)}\big{]}\stackrel{{\text{(ii)}}}{{=}}\mathbb{E}\big{[}\exp \big{(}-F(\tilde{X}^{(x)}+\psi(z,\cdot))\big{)}\big{]}\] \[\stackrel{{\text{(iii)}}}{{=}}\mathbb{E}\big{[}\exp \big{(}-F(X^{(x)}+\psi(z,\cdot))\big{)}\] \[\qquad\times\exp\big{(}\int_{0}^{T}\lambda^{-1/2}\sigma(s)^{-1}(b(X _{s}^{(x)}+\psi(z,s),s)-b(X_{s}^{(x)},s)-\partial_{s}\psi(z,s))\,\mathrm{d}B_{s}\] \[\qquad\qquad\qquad-\tfrac{1}{2}\int_{0}^{T}\|\lambda^{-1/2}\sigma (s)^{-1}(b(X_{s}^{(x)}+\psi(z,s),s)-b(X_{s}^{(x)},s)-\partial_{s}\psi(z,s))\|^ {2}\,\mathrm{d}s\big{)}\big{]}\] \[\stackrel{{\text{(iv)}}}{{=}}\mathbb{E}\big{[}\exp \big{(}-F(X\!+\!\psi(z,\cdot))\!+\!\int_{0}^{T}\lambda^{-1/2}\sigma(s)^{-1}(b( X_{s}\!+\!\psi(z,s),s)\!-\!b(X_{s},s)\!-\!\partial_{s}\psi(z,s))\,\mathrm{d}B_{s}\] \[\qquad\qquad\qquad\qquad-\tfrac{1}{2}\int_{0}^{T}\|\lambda^{-1/2} \sigma(s)^{-1}(b(X_{s}+\psi(z,s),s)-b(X_{s},s)-\partial_{s}\psi(z,s))\|^{2}\, \mathrm{d}s)|X_{0}=x\big{]}\] (53)

Equality (i) holds by the definition of \(X^{(x+z)}\), equality (ii) holds by the fact \(X_{s}^{(x+z)}=\tilde{X}_{s}^{(x)}+\psi(z,s)\), equality (iii) holds by equation (52), and equality (iv) holds by the definition of \(X_{s}^{(x)}\). We conclude the proof by differentiating the right-hand side of (53) with respect to \(z\). Namely,

\[\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-F(X)\big{)}\big{|}X_{0}=x \big{]}=\nabla_{z}\mathbb{E}\big{[}\exp\big{(}-F(X)\big{)}\big{|}X_{0}=x+z \big{]}\big{|}_{z=0}\] \[\stackrel{{\text{(i)}}}{{=}}\mathbb{E}\big{[}\big{(} -\nabla_{z}F(X+\psi(z,\cdot))+\lambda^{-1/2}\int_{0}^{T}(\nabla_{z}\psi(0,s) \nabla_{x}b(X_{s},s)-\nabla_{z}\partial_{s}\psi(0,s))(\sigma^{-1})^{\top}(s) \mathrm{d}B_{s}\big{)}\] \[\qquad\qquad\times\exp\big{(}-F(X)\big{)}\big{|}X_{0}=x\big{]}\]

In equality (i) we used (53), and that:

* by the Leibniz rule, \[\nabla_{z}\int_{0}^{T}\|\sigma(s)^{-1}(b(X_{s}+\psi(z,s),s)-b(X_{s },s)-\partial_{s}\psi(z,s))\|^{2}\,\mathrm{d}s\big{|}_{z=0}\] \[=\int_{0}^{T}\nabla_{z}\|\sigma(s)^{-1}(b(X_{s}+\psi(z,s),s)-b(X _{s},s)-\partial_{s}\psi(z,s))\|^{2}\big{|}_{z=0}\,\mathrm{d}s=0.\]
* and by the Leibniz rule for stochastic integrals (see [42]), \[\nabla_{z}\big{(}\int_{0}^{T}\sigma(s)^{-1}(b(X_{s}+\psi(z,s),s)- b(X_{s},s)-\partial_{s}\psi(z,s))\,\mathrm{d}B_{s}\big{)}\big{|}_{z=0}\] \[=\int_{0}^{T}(\nabla_{z}\psi(0,s)\nabla_{x}b(X_{s},s)-\nabla_{z} \partial_{s}\psi(0,s))(\sigma^{-1})^{\top}(s)\,\mathrm{d}B_{s}.\]

### Informal derivation of the path-wise reparameterization trick

In this subsection, we provide an informal, intuitive derivation of the path-wise reparameterization trick as stated in Prop. 4. For simplicity, we particularize the functional \(F\) to \(F(X)=\lambda^{-1}\int_{0}^{T}f(X_{s},s)\,\mathrm{d}s+\lambda^{-1}g(X_{T})\). Consider the Euler-Maruyama discretization of the uncontrolled process \(X\) defined in (6), with \(K+1\) time steps (let \(\delta=T/K\) be the step size). This is a family of random variables \(\hat{X}=(\hat{X}_{k})_{k=0:K}\) defined as

\[\hat{X}_{0}\sim p_{0},\qquad\hat{X}_{k+1}=\hat{X}_{k}+\delta b(\hat{X}_{k},k \delta)+\sqrt{\delta\lambda}\sigma(k\delta)\varepsilon_{k},\qquad\varepsilon_{ k}\sim N(0,I).\]

Note that we can approximate

\[\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X_{s},s)\, \mathrm{d}s-\lambda^{-1}g(X_{T})\big{)}\big{|}X_{0}=x\big{]}\] \[\approx\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\delta\sum_{k=0}^{ K-1}f(\hat{X}_{k},s)-\lambda^{-1}g(\hat{X}_{K})\big{)}\big{|}\hat{X}_{0}=x\big{]},\]

and that this is an equality in the limit \(K\to\infty\), as the interpolation of the Euler-Maruyama discretization \(\hat{X}^{(x)}\) converges to the process \(X^{(x)}\). Now, remark that for \(k\in\{0,\ldots,K-1\}\), \(\hat{X}_{k+1}|\hat{X}_{k}\sim N(\hat{X}_{k}+\delta b(\hat{X}_{k},k\delta),\delta \lambda(\sigma\sigma^{\top})(k\delta))\). Hence,

\[\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\delta\sum_{k=0}^{K-1}f( \hat{X}_{k},s)-\lambda^{-1}g(\hat{X}_{K})\big{)}\big{|}\hat{X}_{0}=x\big{]}\] \[=C^{-1}\iint_{(\mathbb{R}^{d})K}\exp\big{(}-\lambda^{-1}\delta \sum_{k=0}^{K-1}f(\hat{x}_{k},s)-\lambda^{-1}g(\hat{x}_{K})\] \[\qquad\qquad\qquad\qquad-\tfrac{1}{2\delta\lambda}\sum_{k=1}^{K-1} \|\sigma^{-1}(k\delta)(\hat{x}_{k+1}\!-\!\hat{x}_{k}\!-\!\delta b(\hat{x}_{k},k \delta))\|^{2}\] \[\qquad\qquad\qquad\qquad-\tfrac{1}{2\delta\lambda}\|\sigma^{-1}(0) (\hat{x}_{1}\!-\!x\!-\!\delta b(x,0))\|^{2}\big{)}\,\mathrm{d}\hat{x}_{1}\cdots \mathrm{d}\hat{x}_{K},\]where \(C=\sqrt{(2\pi\delta\lambda)^{K}\prod_{k=0}^{K-1}\det((\sigma\sigma^{\top})(k\delta))}\). Now, let \(\psi:\mathbb{R}^{d}\times[0,T]\to\mathbb{R}^{d}\) be an arbitrary twice differentiable function such that \(\psi(z,0)=z\) for all \(z\in\mathbb{R}^{d}\), and \(\psi(0,s)=0\) for all \(s\in[0,T]\). We can write

\[\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\delta\sum_{k= 0}^{K-1}f(\hat{X}_{k},s)-\lambda^{-1}g(\hat{X}_{K})\big{)}|\hat{X}_{0}=x\big{]}\] \[=\nabla_{z}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\delta\sum_{ k=0}^{K-1}f(\hat{X}_{k},s)-\lambda^{-1}g(\hat{X}_{K})\big{)}|\hat{X}_{0}=x+z \big{]}|_{z=0}\] \[=C^{-1}\nabla_{z}\big{(}\iint_{(\mathbb{R}^{d})^{K}}\exp\big{(}- \lambda^{-1}\delta\sum_{k=0}^{K-1}f(\hat{x}_{k},s)-\lambda^{-1}g(\hat{x}_{K})\] \[-\tfrac{1}{2\delta\lambda}\sum_{k=1}^{K-1}\|\sigma^{-1}(k)\delta (\hat{x}_{k+1}\!-\!\hat{x}_{k}\!-\!\delta b(\hat{x}_{k},k\delta))\|^{2}\] \[-\tfrac{1}{2\delta\lambda}\|\sigma^{-1}(0)(\hat{x}_{1}\!-\!(x\!+ \!z)\!-\!\delta b(x\!+\!z,0))\|^{2}\big{)}\,\mathrm{d}\hat{x}_{1}\cdots\mathrm{ d}\hat{x}_{K}\big{)}|_{z=0}\] \[=C^{-1}\nabla_{z}\big{(}\iint_{(\mathbb{R}^{d})^{K}}\exp\big{(}- \lambda^{-1}\delta\sum_{k=0}^{K-1}f(\hat{x}_{k}+\psi(z,k\delta),s)-\lambda^{-1 }g(\hat{x}_{K}+\psi(z,K\delta))\] \[-\tfrac{1}{2\delta\lambda}\sum_{k=1}^{K-1}\|\sigma^{-1}(k\delta) (\hat{x}_{k+1}+\psi(z,(k+1)\delta)\!-\!\hat{x}_{k}\!-\!\psi(z,k\delta)\!-\! \delta b(\hat{x}_{k}\!+\!\psi(z,k\delta),k\delta))\|^{2}\] \[-\tfrac{1}{2\delta\lambda}\|\sigma^{-1}(0)(\hat{x}_{1}\!+\!\psi(z, \delta)\!-\!(x\!+\!\psi(z,0))\!-\!\delta b(x\!+\!\psi(z,0),0))\|^{2}\big{)}\, \mathrm{d}\hat{x}_{1}\cdots\mathrm{d}\hat{x}_{K}\big{)}|_{z=0},\] (54)

In the last equality, we used that for \(k\in\{1,\ldots,K\}\), the variables \(\hat{x}_{k}\) are integrated over \(\mathbb{R}^{d}\), which means that adding an offset \(\psi(z,k\delta)\) does not change the value of the integral. We also used that \(\psi(z,0)=z\). Now, for fixed values of \(\hat{x}=(\hat{x}_{1},\ldots,\hat{x}_{K})\), and letting \(\hat{x}_{0}=x\), we define

\[G_{\hat{x}}(z) =\lambda^{-1}\delta\sum_{k=0}^{K-1}f(\hat{x}_{k}+\psi(z,k\delta),s)+\lambda^{-1}g(\hat{x}_{K}+\psi(z,K\delta))\] \[+\tfrac{1}{2\delta\lambda}\sum_{k=0}^{K-1}\|\sigma^{-1}(k\delta) (\hat{x}_{k+1}\!+\!\psi(z,(k+1)\delta)\!-\!\hat{x}_{k}\!-\!\psi(z,k\delta)\!- \!\delta b(\hat{x}_{k}\!+\!\psi(z,k\delta),k\delta))\|^{2}.\]

Using that \(\psi(0,s)=0\) for all \(s\in[0,T]\), we have that:

\[G_{\hat{x}}(0) =\lambda^{-1}\delta\sum_{k=0}^{K-1}f(\hat{x}_{k},s)+\lambda^{-1}g (\hat{x}_{K})+\tfrac{1}{2\delta\bar{\lambda}}\sum_{k=0}^{K-1}\|\sigma^{-1}(k \delta)(\hat{x}_{k+1}\!-\!\hat{x}_{k}\!-\!\delta b(\hat{x}_{k},k\delta))\|^{2}.\]

\[\nabla G_{\hat{x}}(z)|_{z=0} =\lambda^{-1}\delta\sum_{k=0}^{K-1}\nabla\psi(0,k\delta)\nabla f( \hat{x}_{k},s)+\lambda^{-1}\nabla\psi(0,K\delta)\nabla g(\hat{x}_{K})\] \[+\tfrac{1}{\delta\bar{\lambda}}\sum_{k=0}^{K-1}(\nabla_{z}\psi(0,( k+1)\delta)-\nabla_{z}\psi(0,k\delta)\!-\!\delta\nabla\psi(0,k\delta)\nabla b( \hat{x}_{k},k\delta))\] \[\times((\sigma^{-1})^{\top}\sigma^{-1})(k\delta)(\hat{x}_{k+1}\!- \!\hat{x}_{k}\!-\!\delta b(\hat{x}_{k},k\delta))\]

And we can express the right-hand side of (54) in terms of \(G_{\hat{x}}(0)\) and \(\nabla G_{\hat{x}}(z)|_{z=0}\):

\[\nabla_{z}\big{(}C^{-1}\iint_{(\mathbb{R}^{d})^{K}}\exp\big{(}-G_ {\hat{x}}(z)\big{)}\,\mathrm{d}y_{1}\cdots\mathrm{d}y_{K}\big{)}\] \[=-C^{-1}\iint_{(\mathbb{R}^{d})^{K}}\nabla G_{\hat{x}}(z)|_{z=0} \exp\big{(}-G_{\hat{x}}(0)\big{)}\,\mathrm{d}y_{1}\cdots\mathrm{d}y_{K}\]

We define \(\epsilon_{k}=\frac{1}{\sqrt{\delta\lambda}}\sigma^{-1}(k\delta)(\hat{x}_{k+1}\! -\!\hat{x}_{k}\!-\!\hat{x}_{k}\!-\!\delta b(\hat{x}_{k},k\delta))\), and then, we are able to write

\[\hat{x}_{k+1} =\hat{x}_{k}+\delta b(\hat{x}_{k},k\delta)+\sqrt{\delta\lambda} \sigma(k\delta)\epsilon_{k},\qquad\hat{x}_{0}=x\] (55) \[G_{\hat{x}}(0) =\lambda^{-1}\delta\sum_{k=0}^{K-1}f(\hat{x}_{k},s)+\lambda^{-1}g (\hat{x}_{K})+\tfrac{1}{2}\sum_{k=0}^{K-1}\|\epsilon_{k}\|^{2},\] \[\nabla G_{\hat{x}}(z)|_{z=0} =\lambda^{-1}\delta\sum_{k=0}^{K-1}\nabla\psi(0,k\delta)\nabla f( \hat{x}_{k},s)+\lambda^{-1}\nabla\psi(0,K\delta)\nabla g(\hat{x}_{K})\] (56) \[+\sqrt{\delta\lambda^{-1}}\sum_{k=0}^{K-1}(\partial_{s}\nabla_{z} \psi(0,k\delta)+O(\delta)\!-\!\nabla\psi(0,k\delta)\nabla b(\hat{x}_{k},k\delta))( \sigma^{-1})^{\top}(k\delta)\epsilon_{k}.\]

Then, taking the limit \(K\to\infty\) (i.e. \(\delta\to 0\)), we recognize (55) as Euler-Maruyama discretization of the uncontrolled process \(X\) in equation (6) conditioned on \(X_{0}=x\), and the last term in (56) as the Euler-Maruyama discretization of the stochastic integral \(\lambda^{-1/2}\int_{0}^{T}(\partial_{s}\nabla_{z}\psi(0,s)-\nabla\psi(0,s)\nabla b(X _{s}^{(x)},s))(\sigma^{-1})^{\top}(s)\,\mathrm{d}B_{s}\). Thus,

\[\lim_{K\to\infty}\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1} \delta\sum_{k=0}^{K-1}f(\hat{X}_{k},s)-\lambda^{-1}g(\hat{X}_{K})\big{)}\big{]}\] \[=\mathbb{E}\big{[}\big{(}-\lambda^{-1}\int_{0}^{T}\nabla\psi(0,s) \nabla_{x}f(X_{s},s)\,\mathrm{d}s-\lambda^{-1}\nabla\psi(0,T)\nabla g(X_{T})\] \[+\lambda^{-1/2}\int_{0}^{T}(\nabla\psi(0,s)\nabla_{x}b(X_{s},s)- \partial_{s}\nabla\psi(0,s))(\sigma^{-1})^{\top}(s)\,\mathrm{d}B_{s}\big{)}\] \[\times\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X_{s},s)\,\mathrm{d}s- \lambda^{-1}g(X_{T})\big{)}\big{|}X_{0}=x\big{]},\]

which concludes the derivation.

### SOCM-Adjoint: replacing the path-wise reparameterization trick with the adjoint method

**Proposition 5**.: _Let \(\mathcal{L}_{\mathrm{SOCM-Adj}}:L^{2}(\mathbb{R}^{d}\times[0,T];\mathbb{R}^{d}) \to\mathbb{R}\) be the loss function defined as_

\[\mathcal{L}_{\mathrm{SOCM-Adj}}(u):=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T} \big{\|}u(X^{v}_{t},t)+\sigma(t)^{\top}a(t,X^{v})\big{\|}^{2}\,\mathrm{d}t \times\alpha(v,X^{v},B)\big{]}\;,\]

_where \(X^{v}\) is the process controlled by \(v\) (i.e., \(dX_{t}=(b(X_{t},t)+\sigma(t)v(X_{t},t))\,\mathrm{d}t+\sqrt{\lambda}\sigma(X_{t},t)\,\mathrm{d}B_{t}\) and \(X_{0}\sim p_{0}\)), \(\alpha(v,X^{v},B)\) is the importance weight defined in (20), and \(a(t,X^{v})\) is the solution of the ODE_

\[\tfrac{da(t)}{dt} =-\nabla_{x}b(X^{v}_{t},t)a(t)-\nabla_{x}f(X^{v}_{t},t),\] \[a(T) =\nabla g(X^{v}_{T}),\]

\(\mathcal{L}_{\mathrm{SOCM-Adj}}\) _has a unique optimum, which is the optimal control \(u^{*}\)._

Proof.: The proof follows the same structure as that of Thm. 1. Instead of plugging the path-wise reparameterization trick (Prop. 1) in the right-hand side of (22), we make use of Lemma 6 to evaluate \(\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\lambda^{-1}\int_{0}^{T}f(X_{t},t)\,dt -\lambda^{-1}g(X_{T})\big{)}|X_{0}=x\big{]}\). Particular cases of the result in Lemma 6 have been used in previous works such as [54, 51]. We present a more general form that covers state costs \(f\), as well as stochastic integrals. We also present a simpler proof of the result based on Lagrange multipliers. 

**Lemma 6** (Adjoint method for SDEs).: _[_54, 51_]_ _Let \(X:\Omega\times[0,T]\to\mathbb{R}^{d}\) be the uncontrolled process defined in (6), with initial condition \(X_{0}=x\). We define the random process \(a:\Omega\times[0,T]\to\mathbb{R}^{d}\) such that for all \(\omega\in\Omega\), using the short-hand \(a(t):=a(\omega,t)\),_

\[\mathrm{d}a_{t}(\omega) =\big{(}-\nabla_{x}b(X_{t}(\omega),t)a_{t}(\omega)-\nabla_{x}f(X_ {t}(\omega),t)\big{)}\,\mathrm{d}t-\nabla_{x}h(X_{t}(\omega),t)\,\mathrm{d}B _{t},\] \[a_{T}(\omega) =\nabla_{x}g(X_{T}(\omega)),\]

_we have that_

\[\nabla_{x}\mathbb{E}\big{[}\int_{0}^{T}f(X_{t}(\omega),t)\,\mathrm{ d}t+\int_{0}^{T}\langle h(X_{t}(\omega),t),\,\mathrm{d}B_{t}\rangle+g(X_{T}( \omega))|X_{0}(\omega)=x\big{]}=\mathbb{E}\big{[}a_{0}(\omega)\big{]},\] \[\nabla_{x}\mathbb{E}\big{[}\exp\big{(}-\int_{0}^{T}f(X_{t}(\omega ),t)\,\mathrm{d}t-\int_{0}^{T}\langle h(X_{t}(\omega),t),\,\mathrm{d}B_{t} \rangle-g(X_{T}(\omega))\big{)}|X_{0}(\omega)=x\big{]}\] \[=-\mathbb{E}\big{[}a_{0}(\omega)\exp\big{(}-\int_{0}^{T}f(X_{t}( \omega),t)\,\mathrm{d}t-\int_{0}^{T}\langle h(X_{t}(\omega),t),\,\mathrm{d}B_{ t}\rangle-g(X_{T}(\omega))\big{)}|X_{0}(\omega)=x\big{]}.\]

Proof.: We will use an approach based on Lagrange multipliers. Define a process \(a:\Omega\times[0,T]\to\mathbb{R}^{d}\) such that for any \(\omega\in\Omega\), \(a(\omega,\cdot)\) is differentiable. For a given \(\omega\in\Omega\), we can write

\[\int_{0}^{T}f(X_{t}(\omega),t)\,\mathrm{d}t+\int_{0}^{T}\langle h( X_{t}(\omega),t),\,\mathrm{d}B_{t}\rangle+g(X_{T}(\omega))\] \[=\int_{0}^{T}f(X_{t}(\omega),t)\,\mathrm{d}t+\int_{0}^{T}\langle h (X_{t}(\omega),t),\,\mathrm{d}B_{t}\rangle+g(X_{T}(\omega))\] \[\qquad-\int_{0}^{T}\langle a_{t}(\omega),(dX_{t}(\omega)-b(X_{t} (\omega),t)\,\mathrm{d}t-\sigma(t)\,\mathrm{d}B_{t})\rangle.\]

By Lemma 7, we have that

\[\int_{0}^{T}\langle a_{t}(\omega),dX_{t}(\omega)\rangle=\langle a_{T}(\omega),X _{T}(\omega)\rangle-\langle a_{0}(\omega),X_{0}(\omega)\rangle-\int_{0}^{T} \langle X_{t}(\omega),\tfrac{da_{t}}{dt}(\omega)\rangle\,\mathrm{d}t.\]

Hence,

\[\nabla_{x}\big{(}\int_{0}^{T}f(X_{t}(\omega),t)\,\mathrm{d}t+\int_ {0}^{T}\langle h(X_{t}(\omega),t),\,\mathrm{d}B_{t}\rangle+g(X_{T}(\omega)) \big{)}\] \[=\nabla_{x}\big{(}\int_{0}^{T}f(X_{t}(\omega),t)\,\mathrm{d}t+ \int_{0}^{T}\langle h(X_{t}(\omega),t),\,\mathrm{d}B_{t}\rangle+g(X_{T}(\omega))\] \[\qquad-\langle a_{T}(\omega),X_{T}(\omega)\rangle+\langle a_{0}( \omega),X_{0}(\omega)\rangle+\int_{0}^{T}\big{(}\langle a_{t}(\omega),b(X_{t} (\omega),t)\rangle+\langle\tfrac{da_{t}}{dt}(\omega),X_{t}(\omega)\rangle\big{)} \,\mathrm{d}t\] \[\qquad+\int_{0}^{T}\langle a_{t}(\omega),\sigma(t)\,\mathrm{d}B_{ t}\rangle\big{)}\] \[=\int_{0}^{T}\nabla_{x}X_{t}(\omega)\nabla_{x}f(X_{t}(\omega),t)\, \mathrm{d}t+\int_{0}^{T}\nabla_{x}X_{t}(\omega)\nabla_{x}h(X_{t}(\omega),t)\, \mathrm{d}B_{t}+\nabla_{x}X_{T}(\omega)\nabla_{x}g(X_{T}(\omega))\] \[\qquad-\nabla_{x}X_{T}(\omega)a_{T}(\omega)+\nabla_{x}X_{0}( \omega)a_{0}(\omega)\] \[\qquad+\int_{0}^{T}\big{(}\nabla_{x}X_{t}(\omega)\nabla_{x}b(X_{t }(\omega),t)a_{t}(\omega)+\nabla_{x}X_{t}(\omega)\tfrac{da_{t}}{dt}(\omega) \big{)}\,\mathrm{d}t\] \[=\int_{0}^{T}\nabla_{x}X_{t}(\omega)\big{(}\nabla_{x}f(X_{t}( \omega),t)+\nabla_{x}b(X_{t}(\omega),t)a_{t}(\omega)+\tfrac{da_{t}}{dt}(\omega) \big{)}\,\mathrm{d}t\] \[\qquad+\nabla_{x}X_{T}(\omega)\big{(}\nabla_{x}g(X_{T}(\omega))-a_{T }(\omega)\big{)}+a_{0}(\omega)+\int_{0}^{T}\nabla_{x}X_{t}(\omega)\nabla_{x}h(X_{t }(\omega),t)\,\mathrm{d}B_{t}.\]In the last line we used that \(\nabla_{x}X_{0}(\omega)=\nabla_{x}x=1\). If choose \(a\) such that

\[da_{t}(\omega) =\big{(}-\nabla_{x}b(X_{t}(\omega),t)a_{t}(\omega)-\nabla_{x}f(X_{ t}(\omega),t)\big{)}\,\mathrm{d}t-\nabla_{x}h(X_{t}(\omega),t)\,\mathrm{d}B_{t},\] \[a_{T}(\omega) =\nabla_{x}g(X_{T}(\omega)),\]

then we obtain that

\[\nabla_{x}\big{(}\int_{0}^{T}f(X_{t}(\omega),t)\,\mathrm{d}t+\int_{0}^{T}\langle h (X_{t}(\omega),t),\,\mathrm{d}B_{t}\rangle+g(X_{T}(\omega))\big{)}=a_{0}( \omega),\]

and by the Leibniz rule,

\[\nabla_{x}\mathbb{E}\big{[}\int_{0}^{T}f(X_{t}(\omega),t)\, \mathrm{d}t+\int_{0}^{T}\langle h(X_{t}(\omega),t),\,\mathrm{d}B_{t}\rangle+g( X_{T}(\omega))\big{]}\] \[=\mathbb{E}\big{[}\nabla_{x}\big{(}\int_{0}^{T}f(X_{t}(\omega),t )\,\mathrm{d}t+\int_{0}^{T}\langle h(X_{t}(\omega),t),\,\mathrm{d}B_{t}\rangle +g(X_{T}(\omega))\big{)}\big{]}\] \[=-\mathbb{E}\big{[}a_{0}(\omega)\exp\big{(}-\int_{0}^{T}f(X_{t}( \omega),t)\,\mathrm{d}t-\int_{0}^{T}\langle h(X_{t}(\omega),t),\,\mathrm{d}B_{ t}\rangle-g(X_{T}(\omega))\big{)}\big{]}.\]

**Lemma 7** (Stochastic integration by parts, [60]).: _Let_

\[\mathrm{d}X_{t} =a_{t}\,\mathrm{d}t+b_{t}\,\mathrm{d}W_{t}^{1},\] \[\mathrm{d}Y_{t} =f_{t}\,\mathrm{d}t+g_{t}\,\mathrm{d}W_{t}^{2}.\]

_where \(a_{t}\), \(b_{t}\), \(f_{t}\), \(g_{t}\) are continuous square integrable processes adapted to a filtration \((\mathcal{F}_{t})_{t\in[0,T]}\), and \(W^{1}\), \(W^{2}\) are Brownian motions adapted to the same filtration. Then,_

\[X_{t}Y_{t}-X_{0}Y_{0} =\int_{0}^{t}X_{s}\,\mathrm{d}Y_{s}+\int_{0}^{t}Y_{s}\,\mathrm{d }X_{s}+\int_{0}^{t}\langle\mathrm{d}X_{s},\mathrm{d}Y_{s}\rangle\] \[=\int_{0}^{t}X_{s}\,\mathrm{d}Y_{s}+\int_{0}^{t}Y_{s}\,\mathrm{d }X_{s}+\int_{0}^{t}\langle b_{s}\,\mathrm{d}W_{s}^{1},g_{s}\,\mathrm{d}W_{s}^ {2}\rangle.\]

**Remark 3** (Related work to the path-wise reparameterization trick: sensitivity analysis).: _As shown above, the adjoint method for SDEs is an alternative to the path-wise reparameterization trick. Prior to [54], an array of works developed methods to compute derivatives of functionals of stochastic processes with respect to generic parameters \(\alpha\) that appear either in the drift or diffusion coefficients [52]. This area is known as sensitivity analysis, and has been developed largely with financial applications in mind (more specifically, to compute the "Greeks"). In low dimensions, dynamic programming [3] or finite differences [26, 53] work well, but they scale poorly to high dimensions. In high dimensions, several approaches have been proposed (see the section 1 of [29] for a comprehensive although dated overview):_

* _The path-wise method (which we refer to as adjoint method) involves taking the gradient_ \(\nabla_{\alpha}\mathbb{E}[f(X_{t})]\) _inside of the expectation as_ \(\mathbb{E}[\nabla_{\alpha}f(X_{t})]\) _and was first described by_ _[_82_]__._
* _The_ likelihood method _or_ score method_ _[_27, 69_]_ _consists in rewriting_ \(\nabla_{\alpha}\mathbb{E}[f(X_{T})]\) _as_ \(\mathbb{E}[f(X_{T})H]\)_, where_ \(H\) _is a random variable which is equal to_ \(\nabla_{\alpha}\log p(\alpha,X_{T})\)_,_ \(p(\alpha,\cdot)\) _being the density of the law of_ \(X_{T}\) _with respect to the Lebesgue measure._ _[_82_]_ _provide explicit weights_ \(H\)_, under the restrictions that_ \(\alpha\) _appears only in the drift of the SDE (and not in the diffusion coefficient) and that the diffusion coefficient is elliptic, using the Girsanov theorem._ _[_29_]_ _provide an expression for_ \(H\) _in the case where_ \(H\) _also appears in the diffusion coefficient, using Malliavin calculus._

_The estimator of the path-wise reparameterization trick is formally similar to the likelihood method estimator, but it is different in that \(\alpha\) is the initial condition of the process, and does not appear either in the drift nor the diffusion coefficient._

### Proof of Lemma 1

Proof.: Since the equality (28) holds almost surely for the pair \((X,B)\), it must also hold almost surely for \((X^{v},B^{v})\), which satisfy the same SDE. That is

\[\mathcal{W}(X^{v},0)=V(X^{v}_{0},0)+\tfrac{1}{2}\int_{0}^{T}\|u^{*}(X^{v}_{s},s )\|^{2}\,\mathrm{d}s-\sqrt{\lambda}\int_{0}^{T}\langle u^{*}(X^{v}_{s},s), \mathrm{d}B^{v}_{s}\rangle,\]

Thus, we obtain that

\[\alpha(v,X^{v},B) =\exp\big{(}-\lambda^{-1}\mathcal{W}(X^{v},0)-\lambda^{-1/2}\int _{0}^{T}\langle v(X^{v}_{t},t),\mathrm{d}B^{v}_{t}\rangle+\tfrac{\lambda^{-1}} {2}\int_{0}^{T}\|v(X^{v}_{t},t)\|^{2}\,\mathrm{d}t\big{)}\] \[=\exp\big{(}-\lambda^{-1}V(X^{v}_{0},0)-\tfrac{\lambda^{-1}}{2} \int_{0}^{T}\|u^{*}(X^{v}_{s},s)\|^{2}\,\mathrm{d}s+\lambda^{-1/2}\int_{0}^{T} \langle u^{*}(X^{v}_{s},s),\mathrm{d}B^{v}_{s}\rangle\]

and this is equal to \(\exp\big{(}-V(X^{v}_{0},0)\big{)}\) when \(v=u^{*}\). Since we condition on \(X^{v}_{0}=x_{\mathrm{init}}\), we have obtained that the random variable takes constant value \(\exp\big{(}-V(x_{\mathrm{init}},0)\big{)}\) almost surely, which means that its variance is zero. 

## Appendix D Optimal reparameterization matrices

**Theorem 4** (Optimal reparameterization matrices).: _Let \(v\) be an arbitrary control in \(\mathcal{U}\). Define the integral operator \(\mathcal{T}_{t}:L^{2}([t,T];\mathbb{R}^{d\times d})\to L^{2}([t,T];\mathbb{R}^ {d\times d})\) as_

\[[\mathcal{T}_{t}(\dot{M}_{t})](s)=\int_{t}^{T}\dot{M}_{t}(s^{\prime})\mathbb{ E}\big{[}\chi(s^{\prime},X^{v},B)\chi(s,X^{v},B)^{\top}\times\alpha(v,X^{v},B) \big{]}\,\mathrm{d}s^{\prime},\]

_where_

\[\chi(t,X^{v},B):=\int_{t}^{T}\nabla_{x}f(X^{v}_{s},s)\,\mathrm{d}s +\nabla g(X^{v}_{T})+(\sigma_{t}^{-1})^{\top}(t)v(X^{v}_{t},t)\] \[\qquad\qquad-\int_{t}^{T}\nabla_{x}b(X^{v}_{s},s)(\sigma_{s}^{-1} )^{\top}(s)v(X^{v}_{t},t)\,\mathrm{d}s-\int_{t}^{T}\nabla_{x}b(X^{v}_{s},s)( \sigma_{s}^{-1})^{\top}(s)\,\mathrm{d}B_{s}.\]

_If we define \(N_{t}(s)=-\mathbb{E}\big{[}\big{(}\nabla g(X^{v}_{T})+\int_{t}^{T}\nabla_{x}f( X^{v}_{s^{\prime}},s^{\prime})\,\mathrm{d}s^{\prime}\big{)}\chi(t,X^{v},B)^{ \top}\times\alpha(v,X^{v},B)\big{]}\), the optimal \(M^{*}=(M^{*}_{t})_{t\in[0,T]}\) is of the form \(M^{*}_{t}(s)=I+\int_{t}^{s}\dot{M}^{*}_{t}(s^{\prime})\,\mathrm{d}s^{\prime}\), where \(\dot{M}^{*}_{t}\) is the unique solution of the following Fredholm equation of the first kind:_

\[\mathcal{T}_{t}(\dot{M}_{t})=N_{t}.\]

The proof of (25) shows that minimizing \(\mathrm{Var}(w;M)\) is equivalent to minimizing

\[\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\left\|w(t,v,X^{v},B,M_{t})\right\|^ {2}\mathrm{d}t\,\alpha(v,X^{v},B)\big{]}.\] (57)

To optimize with respect to \(M\), it is convenient to reexpress it in terms of \(\dot{M}=(\dot{M}_{t})_{t\in[0,T]}\) as \(M_{t}(s)=I+\int_{t}^{s}\dot{M}_{t}(s^{\prime})\,\mathrm{d}s^{\prime}\). By Fubini's theorem, we have that

\[\int_{t}^{T}M_{t}(s)\nabla_{x}f(X^{v}_{s},s)\,\mathrm{d}s =\int_{t}^{T}\big{(}I+\int_{t}^{s}\dot{M}_{t}(s^{\prime})\, \mathrm{d}s^{\prime}\big{)}\nabla_{x}f(X^{v}_{s},s)\,\mathrm{d}s\] \[=\int_{t}^{T}\nabla_{x}f(X^{v}_{s},s)\,\mathrm{d}s+\int_{t}^{T} \dot{M}_{t}(s)\int_{s}^{T}\nabla_{x}f(X^{v}_{s^{\prime}},s^{\prime})\,\mathrm{d}s^ {\prime}\,\mathrm{d}s,\] \[-\int_{t}^{T}(M_{t}(s)\nabla_{x}b(X^{v}_{s},s)-\dot{M}_{t}(s))( \sigma^{-1})^{\top}(s)v(X^{v}_{s},s)\,\mathrm{d}s\] \[=\int_{t}^{T}\dot{M}_{t}(s)(\sigma^{-1})^{\top}(s)v(X^{v}_{s},s) \,\mathrm{d}s-\int_{t}^{T}\dot{M}_{t}(s)\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{ \prime}},s^{\prime})(\sigma_{s^{\prime}}^{-1})^{\top}(s^{\prime})v(X^{v}_{s},s) \,\mathrm{d}s^{\prime}\,\mathrm{d}s,\] \[-\lambda^{1/2}\int_{t}^{T}(M_{t}(s)\nabla_{x}b(X^{v}_{s},s)-\dot{ M}_{t}(s))(\sigma^{-1})^{\top}(s)\,\mathrm{d}B_{s}\] \[=\lambda^{1/2}\big{(}\int_{t}^{T}\dot{M}_{t}(s)(\sigma^{-1})^{ \top}(s)v(X^{v}_{s},s)\,\mathrm{d}s-\int_{t}^{T}\dot{M}_{t}(s)\int_{s}^{T} \nabla_{x}b(X^{v}_{s^{\prime}},s^{\prime})(\sigma_{s^{\prime}}^{-1})^{\top}(s^ {\prime})\,\mathrm{d}B_{s^{\prime}}\,\mathrm{d}s\big{)}.\]

Hence, we can rewrite (57) as

\[\mathcal{G}(\dot{M}) =\!\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\left\|\sigma(t)^{ \top}\big{(}\int_{t}^{T}\nabla_{x}f(X^{v}_{s},s)\,\mathrm{d}s+\nabla g(X^{v}_{T})\right.\] \[+\int_{t}^{T}\dot{M}_{t}(s)\big{(}\int_{s}^{T}\nabla_{x}f(X^{v}_{s ^{\prime}},s^{\prime})\,\mathrm{d}s^{\prime}+\nabla g(X^{v}_{T})+(\sigma^{-1})^{ \top}(s)v(X^{v}_{s},s)\] \[\left.-\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{\prime}},s^{\prime})( \sigma_{s^{\prime}}^{-1})^{\top}(s^{\prime})v(X^{v}_{s},s)\,\mathrm{d}s^{\prime} \!-\!\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{\prime}},s^{\prime})(\sigma_{s^{\prime}}^{-1 })^{\top}(s^{\prime})\,\mathrm{d}B_{s^{\prime}}\right)\mathrm{d}s\big{)}\big{\|}^{2}\, \mathrm{d}t\] \[\qquad\qquad\times\alpha(v,X^{v},B)\big{]}\]The first variation \(\frac{\delta\mathcal{G}}{\delta\dot{M}}(\dot{M})\) of \(\mathcal{G}\) at \(\dot{M}\) is defined as the family \(Q=(Q_{t})_{t\in[0,T]}\) of matrix-valued functions such that for any collection of matrix-valued functions \(P=(P_{t})_{t\in[0,T]}\),

\[\partial_{\epsilon}\mathcal{V}(\dot{M}+\epsilon P)|_{\epsilon=0}=\lim_{ \epsilon\to 0}\frac{\mathcal{V}(\dot{M}+\epsilon P)-\mathcal{V}(\dot{M})}{ \epsilon}=\langle P,Q\rangle:=\int_{0}^{T}\int_{t}^{T}\langle P_{t}(s),Q_{t}( s)\rangle_{F}\,\mathrm{d}s\,\mathrm{d}t,\]

where \(\dot{M}+\epsilon P:=(\dot{M}_{t}+\epsilon P_{t})_{t\in[0,T]}\). Now, note that

\[\partial_{\epsilon}\mathcal{V}(\dot{M}+\epsilon P)|_{\epsilon=0} =\partial_{\epsilon}\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\|}\sigma(t )^{\top}\big{(}\int_{t}^{T}\nabla_{x}f(X^{v}_{s},s)\,\mathrm{d}s+\nabla g(X^{v} _{T})\] \[\qquad+\int_{t}^{T}(\dot{M}_{t}(s)+\epsilon P_{t}(s))\big{(}\int_ {s}^{T}\nabla_{x}f(X^{v}_{s^{\prime}},s^{\prime})\,\mathrm{d}s^{\prime}+\nabla g (X^{v}_{T})+(\sigma^{-1})^{\top}(s)v(X^{v}_{s},s)\] \[\qquad\qquad-\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{\prime}},s^{ \prime})(\sigma^{-1}_{s^{\prime}})^{\top}(s^{\prime})v(X^{v}_{s},s)\,\mathrm{ d}s^{\prime}-\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{\prime}},s^{\prime})(\sigma^{-1}_{s^{ \prime}})^{\top}(s^{\prime})\,\mathrm{d}B_{s^{\prime}}\big{)}\big{\|}^{2}\, \mathrm{d}t\] \[\qquad\times\alpha(v,X^{v},B)\big{\|}_{\epsilon=0}\] \[=\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\langle}\sigma(t) \sigma(t)^{\top}\big{(}\int_{t}^{T}\nabla_{x}f(X^{v}_{s},s)\,\mathrm{d}s+ \nabla g(X^{v}_{T})\] \[\qquad+\int_{t}^{T}\dot{M}_{t}(s)\big{(}\int_{s}^{T}\nabla_{x}f(X ^{v}_{s^{\prime}},s^{\prime})\,\mathrm{d}s^{\prime}+\nabla g(X^{v}_{T})+( \sigma^{-1})^{\top}(s)v(X^{v}_{s},s)\] \[\qquad\qquad-\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{\prime}},s^{\prime })(\sigma^{-1}_{s^{\prime}})^{\top}(s^{\prime})v(X^{v}_{s},s)\,\mathrm{d}s^{ \prime}-\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{\prime}},s^{\prime})(\sigma^{-1}_{s^ {\prime}})^{\top}(s^{\prime})\,\mathrm{d}B_{s^{\prime}}\big{)}\,\mathrm{d}s \big{)},\] \[\qquad\qquad\int_{t}^{T}P_{t}(s)\big{(}\int_{s}^{T}\nabla_{x}f(X ^{v}_{s^{\prime}},s^{\prime})\,\mathrm{d}s^{\prime}+\nabla g(X^{v}_{T})+( \sigma^{-1})^{\top}(s)v(X^{v}_{s},s)\] \[\qquad\qquad-\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{\prime}},s^{\prime })(\sigma^{-1}_{s^{\prime}})^{\top}(s^{\prime})v(X^{v}_{s},s)\,\mathrm{d}s^{ \prime}-\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{\prime}},s^{\prime})(\sigma^{-1}_{s ^{\prime}})^{\top}(s^{\prime})\,\mathrm{d}B_{s^{\prime}}\big{)}\,\mathrm{d}s \big{)}\,\mathrm{d}t\] \[\times\alpha(v,X^{v},B)\big{]}.\] (58)

If we define

\[\chi(s,X^{v},B):=\int_{s}^{T}\nabla_{x}f(X^{v}_{s^{\prime}},s^{ \prime})\,\mathrm{d}s^{\prime}+\nabla g(X^{v}_{T})+(\sigma^{-1})^{\top}(s)v(X^{ v}_{s},s)\] \[\qquad\qquad-\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{\prime}},s^{ \prime})(\sigma^{-1}_{s^{\prime}})^{\top}(s^{\prime})v(X^{v}_{s},s)\,\mathrm{d}s ^{\prime}-\int_{s}^{T}\nabla_{x}b(X^{v}_{s^{\prime}},s^{\prime})(\sigma^{-1}_{s ^{\prime}})^{\top}(s^{\prime})\,\mathrm{d}B_{s^{\prime}},\]

we can rewrite (58) as

\[\partial_{\epsilon}\mathcal{V}(\dot{M}+\epsilon P)|_{\epsilon=0} =\mathbb{E}\big{[}\tfrac{1}{T}\!\int_{0}^{T}\!\big{\langle} \sigma(t)\sigma(t)^{\top}\!\big{(}\!\int_{t}^{T}\nabla_{x}f(X^{v}_{s},s)\! \mathrm{d}s\!+\!\nabla g(X^{v}_{T})\!+\!\int_{t}^{T}\!M_{t}(s)\chi(s,X^{v},B) \,\mathrm{d}s\big{)},\] (59) \[\int_{t}^{T}P_{t}(s)\chi(s,X^{v},B)\mathrm{d}s\big{\rangle}\, \mathrm{d}s\times\alpha(v,X^{v},B)\big{]}\]

Now let us reexpress equation (59) as:

\[\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T}\big{\langle}\sigma\sigma^ {\top}(t)\big{(}\nabla g(X^{v}_{T})+\int_{t}^{T}\big{(}\nabla_{x}f(X^{v}_{s},s) +\dot{M}_{t}(s)\chi(s,X^{v},B)\big{)}\,\mathrm{d}s\big{)},\] \[\qquad\qquad\int_{t}^{T}P_{t}(s)\chi(s,X^{v},B)\,\mathrm{d}s\big{\rangle} \,\mathrm{d}t\times\alpha(v,X^{v},B)\big{]}\] \[\overset{\text{(i)}}{=}\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T} \int_{0}^{s}\big{\langle}P_{t}(s)\chi(s,X^{v},B),\] \[\qquad\qquad\sigma\sigma^{\top}(t)\big{(}\nabla g(X^{v}_{T})+\int_{t} ^{T}(\nabla_{x}f(X^{v}_{s^{\prime}},s^{\prime})\!+\!\dot{M}_{t}(s^{\prime}) \chi(s^{\prime},X^{v},B))\,\mathrm{d}s^{\prime}\big{)}\big{\rangle}\,\mathrm{d}t \,\mathrm{d}s\times\alpha(v,X^{v},B)\big{]}\] \[\overset{\text{(ii)}}{=}\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T} \int_{0}^{s}\big{\langle}\sigma\sigma^{\top}(t)\big{(}\nabla g(X^{v}_{T})+\int_{t} ^{T}(\nabla_{x}f(X^{v}_{s^{\prime}},s^{\prime})+\dot{M}_{t}(s^{\prime})\chi(s^{ \prime},X^{v},B))\,\mathrm{d}s^{\prime}\big{)}\chi(X^{v},s,B)\big{]}\] \[\overset{\text{(ii)}}{=}\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T} \int_{0}^{s}\big{\langle}\sigma\sigma^{\top}(t)\big{(}\nabla g(X^{v}_{T})+\int_{t} ^{T}(\nabla_{x}f(X^{v}_{s^{\prime}},s^{\prime})+\dot{M}_{t}(s^{\prime})\chi(X^{v},s,B)\big{)}\,\mathrm{d}s^{\prime}\big{)}\chi(X^{v},s,B)\big{]}\] \[\overset{\text{(iii)}}{=}\mathbb{E}\big{[}\tfrac{1}{T}\int_{0}^{T} \int_{0}^{s}\big{\langle}\sigma\sigma^{\top}(t)\big{(}\nabla g(X^{v}_{T})+\int_{t} ^{T}\big{(}\nabla_{x}f(X^{v}_{s^{\prime}},s^{\prime})+\dot{M}_{t}(s)\chi(X^{v},s,B)\big{)}\,\mathrm{d}s^{\prime}\big{)}\chi(X^{v},s,B)\big{]}\,\mathrm{d}s^{ \prime}\big{)}\chi(X^{v},of (60) must be zero for any \(P\), which implies that almost everywhere with respect to \(t\in[0,T]\), \(s\in[s,T]\),

\[\mathbb{E}\big{[}\big{(}\nabla g(X^{v}_{T})+\int_{t}^{T}(\nabla_{x}f(X^{v}_{s^{ \prime}},s^{\prime})+\dot{M}_{t}(s^{\prime})\chi(X^{v},s^{\prime},B))\,\mathrm{d }s^{\prime}\big{)}\chi(X^{v},s,B)^{\top}\alpha(v,X^{v},B)\big{]}=0.\]

To derive this, we also used that \(\sigma(t)\) is invertible by assumption.

Define the integral operator \(\mathcal{T}_{t}:L^{2}([t,T];\mathbb{R}^{d\times d})\to L^{2}([t,T];\mathbb{R}^ {d\times d})\) as

\[[\mathcal{T}_{t}(\dot{M}_{t})](s)=\int_{t}^{T}\dot{M}_{t}(s^{\prime})\mathbb{ E}\big{[}\chi(X^{v},s^{\prime},B)\chi(X^{v},s,B)^{\top}\times\alpha(v,X^{v},B) \big{]}\,\mathrm{d}s^{\prime}\]

If we define \(N_{t}(s)=-\mathbb{E}\big{[}\big{(}\nabla g(X^{v}_{T})+\int_{t}^{T}\nabla_{x}f( X^{v}_{s^{\prime}},s^{\prime})\,\mathrm{d}s^{\prime}\big{)}\chi(X^{v},s,B)^{ \top}\times\alpha(v,X^{v},B)\big{]}\), the problem that we need to solve to find the optimal \(\dot{M}_{t}\) is

\[\mathcal{T}_{t}(\dot{M}_{t})=N_{t}.\]

This is a Fredholm equation of the first kind.

**Lemma 8**.: _If \(\alpha,\beta:[0,T]\times[0,T]\to\mathbb{R}^{d}\), \(\gamma:[0,T]\to\mathbb{R}^{d}\), \(\delta:[0,T]\to\mathbb{R}^{d\times d}\) are arbitrary integrable functions, we have that_

Proof.: We have that:

\[\int_{0}^{T}\int_{t}^{T}\big{\langle}\alpha(t,s),\gamma(t)\big{\rangle}\, \mathrm{d}s\,\mathrm{d}t\stackrel{{(\ref{eq:1})}}{{=}}\int_{0}^ {T}\int_{0}^{T-t}\big{\langle}\alpha(t,T-s),\gamma(t)\big{\rangle}\,\mathrm{d }s\,\mathrm{d}t\] \[\stackrel{{(\ref{eq:2})}}{{=}}\int_{0}^{T}\int_{t}^ {t}\big{\langle}\alpha(T-t,T-s),\gamma(T-t)\big{\rangle}\,\mathrm{d}s\, \mathrm{d}t\stackrel{{(\ref{eq:3})}}{{=}}\int_{0}^{T}\int_{s}^{ T}\big{\langle}\alpha(T-t,T-s),\gamma(T-t)\big{\rangle}\,\mathrm{d}t\,\mathrm{d}s\] \[\stackrel{{(\ref{eq:4})}}{{=}}\int_{0}^{T}\int_{T-s}^ {T}\big{\langle}\alpha(T-t,s),\gamma(T-t)\big{\rangle}\,\mathrm{d}t\,\mathrm{d }s\stackrel{{(\ref{eq:5})}}{{=}}\int_{0}^{T}\int_{0}^{s}\big{\langle} \alpha(t,s),\gamma(t)\big{\rangle}\,\mathrm{d}t\,\mathrm{d}s\]

Here, in equalities (i), (ii), (iv) and (v) we make changes of variables of the form \(t\mapsto T-t\), \(s\mapsto T-s\), \(s^{\prime}\mapsto T-s^{\prime}\). In equality (iii) we use Fubini's theorem. 

## Appendix E Control warm-starting

We introduce the _Gaussian warm-start_, a control warm-start strategy that we adapt from [56], and that we use in our experiments in Figure 7. Their work tackles generalized Schrodinger bridge problems, which are different from the control setting in that the final distribution is known and there is no terminal cost. The following proposition, that provides an analytic expression of the control needed for the density of the process to be Gaussian at all times, is the foundation of our method.

**Proposition 6**.: _Given \(Z\sim N(0,I)\) define the random process \(Y\) as_

\[Y_{t}=\mu(t)+\tilde{\Gamma}(t)Z,\qquad\text{where }\mu(t)\in\mathbb{R}^{d}, \ \tilde{\Gamma}(t)=\sqrt{t}\Gamma(t)\in\mathbb{R}^{d\times d}.\] (61)

_Define the control \(u:\mathbb{R}^{d}\times[0,T]\to\mathbb{R}^{d}\) as_

\[u(x,t)=\sigma(t)^{-1}\big{(}\partial_{t}\mu(t)+\big{(}\big{(} \partial_{t}\Gamma(t)\big{)}\Gamma(t)^{-1}+\tfrac{I-(\sigma\sigma^{\top})(t)( \Sigma\Sigma^{\top})^{-1}(t)}{2t}\big{)}(x-\mu(t))-b(x,t)\big{)}.\] (62)

_Then, if \(\Gamma_{0}=\sigma(0)\), the controlled process \(X^{u}\) defined in equation (2) has the same marginals as \(Y\). That is, for all \(t\in[0,T]\), \(\mathrm{Law}(Y_{t})=\mathrm{Law}(X^{u}_{t})\)._

Proof.: Following [56], we have that

\[\partial_{t}X_{t} =\partial_{t}\mu_{t}+\partial_{t}\tilde{\Gamma}(t)Z=\partial_{t} \mu(t)+(\partial_{t}\tilde{\Gamma}(t))\tilde{\Gamma}(t)^{-1}(X_{t}-\mu(t)),\] \[\nabla\log p_{t}(x) =-\tilde{\Sigma}(t)^{-1}(x-\mu(t)),\qquad\tilde{\Sigma}(t)=\tilde{ \Gamma}(t)\tilde{\Gamma}(t)^{\top}.\]

Now, \(p_{t}\) satisfies the continuity equation

\[\partial_{t}p_{t}=-\nabla\cdot((\partial_{t}\mu(t)+(\partial_{t}\tilde{\Gamma}( t))\tilde{\Gamma}(t)^{-1}(x-\mu(t)))p_{t})\] (63)Let \(D(t)=\frac{1}{2}\sigma(t)\sigma(t)^{\top}\). We want to reexpress (63) as a Fokker-Planck equation of the form

\[\partial_{t}p_{t}= -\nabla\cdot(v(x,t)p_{t})+\sum_{i=1}^{d}\sum_{j=1}^{d}\partial_{i} \partial_{j}(D_{ij}(t)p_{t})\!=\!-\nabla\cdot(v(x,t)p_{t})\!+\!\sum_{i=1}^{d} \partial_{i}\sum_{j=1}^{d}(D_{ij}(t)\partial_{j}p_{t})\] \[= -\nabla\cdot(v(x,t)p_{t})+\nabla\cdot(D(t)\nabla p_{t})=-\nabla \cdot(v(x,t)p_{t})+\nabla\cdot(D(t)\nabla\log p_{t}(x)p_{t})\] \[= -\nabla\cdot((v(x,t)-D(t)\nabla\log p_{t}(x))p_{t}).\]

Hence, we need that

\[v(x,t)-D(t)\nabla\log p_{t} =\partial_{t}\mu(t)+(\partial_{t}\tilde{\Gamma}(t))\tilde{\Gamma }(t)^{-1}(x-\mu(t)),\] \[\implies v_{t}(x) =\partial_{t}\mu(t)+((\partial_{t}\tilde{\Gamma}(t))\tilde{ \Gamma}(t)^{-1}(x-\mu(t))+\frac{(\sigma\sigma^{\top})(t)}{2}\nabla\log p_{t}(x)\] \[=\partial_{t}\mu(t)+(\partial_{t}\tilde{\Gamma}(t))\tilde{\Gamma }(t)^{-1}(x-\mu(t))-\frac{(\sigma\sigma^{\top})(t)}{2}\Sigma(t)^{-1}(x-\mu(t)).\]

If we let \(\tilde{\Gamma}(t)=\Gamma(t)\sqrt{t}\), then \(\tilde{\Sigma}(t)=t\Gamma(t)\Gamma(t)^{\top}=t\Sigma(t)\) and \(\partial_{t}\tilde{\Gamma}(t)=\partial_{t}\Gamma(t)\sqrt{t}+\frac{\Gamma(t)} {2\sqrt{t}}\). That is,

\[v(x,t) =\partial_{t}\mu(t)+\big{(}\partial_{t}\Gamma(t)\sqrt{t}+\frac{ \Gamma(t)}{2\sqrt{t}}\big{)}\frac{\Gamma(t)^{-1}}{\sqrt{t}}(x-\mu(t))-\frac{( \sigma\sigma^{\top})(t)}{2}\frac{\Sigma(t)^{-1}}{t}(x-\mu(t))\] \[=\partial_{t}\mu(t)+\big{(}\partial_{t}\Gamma(t)\big{)}\Gamma(t )^{-1}(x-\mu(t))+\frac{1}{2t}(x-\mu(t))-\frac{(\sigma\sigma^{\top})(t)\Sigma( t)^{-1}}{2t}(x-\mu(t))\]

For \(v\) to be finite at \(t=0\), we need that \((\sigma\sigma^{\top})(0)\Sigma(0)^{-1}=I\), which holds, for example, if \(\Gamma(0)=\sigma(0)\). Also, to match the form of (2), we need that

\[v(x,t) =b(x,t)+\sigma(t)u(x,t),\] \[\implies u(x,t) =\sigma(t)^{-1}\big{(}\partial_{t}\mu_{t}+\big{(}\big{(}\partial_{ t}\Gamma(t)\big{)}\Gamma(t)^{-1}+\frac{I-(\sigma\sigma^{\top})(t)\Sigma(t)^{-1}}{2t }\big{)}(x-\mu_{t})-b(x,t)\big{)}.\]

The warm-start control is computed as the solution of a _Restricted Gaussian Stochastic Optimal Control_ problem, where we constrain the space of controls to those that induce Gaussian paths as described in Prop. 6. In practice, we learn a linear spline \(\mu=(\mu^{(b)})_{b=0}^{\mathcal{B}}\), where \(\mu^{(b)}\in\mathbb{R}^{d}\), and a linear spline \(\Gamma=(\Gamma^{(b)})_{b=0}^{\mathcal{B}}\), where \(\Gamma^{(b)}\in\mathbb{R}^{d\times d}\). These linear splines take the role of \(\mu(t)\) and \(\Sigma(t)\) in (61). Given splines \(\mu\) and \(\Gamma\), we obtain the warm-start control using (62); for a given \(t\in[0,T)\), if we let \(b_{-}=\lfloor\mathcal{B}t/T\rfloor\), \(b_{+}=b_{-}+1\), \(\Delta=T/\mathcal{B}\), we have that

\[\widehat{\mu}(t) =\frac{(t-b_{-}\Delta)\mu^{(b_{+})}+(b_{+}\Delta-t)\mu^{(b_{-})}} {\Delta},\qquad\widehat{\partial_{t}\mu}(t)=\frac{\mu^{(b_{+})}-\mu^{(b_{-})} }{\Delta},\] (64) \[\widehat{\Gamma}(t) =\frac{(t-b_{-}\Delta)\Gamma^{(b_{+})}+(b_{+}\Delta-t)\Gamma^{(b_ {-})}}{\Delta},\qquad\widehat{\partial_{t}\Gamma}(t)=\frac{\Gamma^{(b_{+})}- \Gamma^{(b_{-})}}{\Delta},\] (65) \[\hat{u}(x,t) =\sigma(t)^{-1}\big{(}\widehat{\partial_{t}\mu}(t)+\big{(}\widehat {\partial_{t}\Gamma}(t)\widehat{\Gamma}(t)^{-1}+\frac{I-(\sigma\sigma^{\top})(t) \hat{\Sigma}\Sigma^{\top})^{-1}(t)}{2t}\big{)}(x-\widehat{\mu}(t))-b(x,t)\big{)}.\] (66)

Algorithm 3 provides a method to learn the splines \(\mu\), \(\Gamma\). It is a stochastic optimization algorithms in which the spline parameters are updated by sampling \(Y_{t}\) in (61) at different times, computing the control cost relying on (66), and taking its gradient.

```
0: State cost \(f(x,t)\), terminal cost \(g(x)\), diffusion coeff. \(\sigma(t)\), base drift \(b(x,t)\), noise level \(\lambda\), number of iterations \(N\), batch size \(m\), number of time steps \(K\), number of spline knots \(\mathcal{B}\), initial mean spline knots \(\mu_{0}=(\mu_{0}^{(b)})_{b=0}^{\mathcal{B}}\), initial noise spline knots \(\Gamma_{0}=(\Gamma_{0}^{(b)})_{b=0}^{\mathcal{B}}\).
1for\(n=0:(N-1)\)do
2 Sample \(m\) i.i.d. variables \((Z_{i})_{i=1}^{n}\sim N(0,I)\) and \(m\) times \((t_{i})_{i=1}^{n}\sim\mathrm{Unif}([0,T])\).
3for\(j=0:K\)do
4 Set \(t_{j}=jT/K\), and compute \(\widehat{\mu}_{n}(t_{j})\), \(\widehat{\partial_{t}\mu}_{n}(t_{j})\), \(\widehat{\Gamma}_{n}(t_{j})\), \(\widehat{\partial_{t}\Gamma_{n}}(t_{j})\) according to (64), (65) using \(\mu_{n}\), \(\Gamma_{n}\)for\(i=1:m\)do compute \(Y_{ij}=\hat{\mu}(t_{j})+\sqrt{t_{j}\hat{\Gamma}}(t_{j})Z_{i}\) and \(\hat{u}_{n}(Y_{ij},t_{j})\) using (66);
5 end for
6 Compute \(\hat{\mathcal{L}}_{\mathrm{RGSOC}}(\mu_{n},\Gamma_{n})=\frac{1}{m}\sum_{i=1}^{m} \big{(}\frac{T}{K}\sum_{j=0}^{K-1}\big{(}\frac{1}{2}\|\hat{u}(Y_{ij},t_{j})\|^{2 }+f(Y_{ij},t_{j})\big{)}+g(Y_{iK})\big{)}\)
7 Compute the gradient of \(\hat{\mathcal{L}}_{\mathrm{RGSOC}}(\mu_{n},\Gamma_{n})\) with respect to the spline parameters \((\mu_{n},\Gamma_{n})\).
8 Obtain \(\mu_{n+1},\Gamma_{n+1}\) with via an Adam update on \(\mu_{n}\), \(\Gamma_{n}\) resp. (or another stochastic algorithm)
9
10 end for Output: Learned splines \(\mu_{N}\), \(\Gamma_{N}\), control \(\hat{u}_{N}\) ```

**Algorithm 3** Restricted Gaussian Stochastic Optimal ControlOnce we have access to the restricted control \(\hat{u}_{N}\), we can warm-start the control in Algorithms 1 and 2 by introducing \(\hat{u}_{N}\) as an offset. That is, we parameterize the control as \(u_{\theta}=\hat{u}_{N}+\hat{u}_{\theta}\).

## Appendix F Experimental details and additional plots

### Experimental details

The control \(L^{2}\) error curves show the following quantity:

\[\mathbb{E}_{t,\mathbb{D}^{u^{*}}}[\|u^{*}(X_{t}^{u^{*}},t)-u(X_{t}^{u^{*}},t) \|^{2}e^{-\lambda^{-1}V(X_{0}^{u^{*}},0)}]/\mathbb{E}_{t,\mathbb{D}^{u^{*}}}[ e^{-\lambda^{-1}V(X_{0}^{u^{*}},0)}]\]

That is, we sample trajectories using the optimal control, and compute the error using a Monte Carlo estimate. In all our experiments, the distribution \(X_{0}^{u^{*}}\) is a delta, which means that we do not need to compute \(V(X_{0}^{u^{*}},0)\). We keep an exponential moving average (EMA) estimate of the control \(L^{2}\) error, which we show in the plots. To compute it, we sample ten batches of optimally controlled trajectories every 10 training iterations, and we update the quantity with the average of the ten batches, using EMA coefficient 0.02. All other quantities shown in the plots are also smoothed out using EMA with coefficient 0.01, except for control objective values, which are computed as the average of 65536 samples, every 5000 training steps.

For all losses and all settings, we train the control using Adam with learning rate \(1\times 10^{-4}\). For SOCM, we train the reparametrization matrices using Adam with learning rate \(1\times 10^{-2}\). We use batch size \(m=128\) unless otherwise specified. When used, we run the warm-start algorithm (Algorithm 3) with \(\mathcal{B}=20\) knots, \(K=200\) time steps, and batch size \(m=512\), and we use Adam with learning rate \(3\times 10^{-4}\) for \(N=60000\) iterations.

Quadratic Ornstein-UhlenbeckThe choices for the functions of the control problem are:

\[b(x,t)=Ax,\quad f(x,t)=x^{\top}Px,\quad g(x)=x^{\top}Qx,\quad\sigma(t)= \sigma_{0}.\]

where \(Q\) is a positive definite matrix. Control problems of this form are better known as linear quadratic regulator (LQR) and they admit a closed form solution [78, Thm. 6.5.1]. The optimal control is given by:

\[u_{t}^{*}(x)=-2\sigma_{0}^{\top}F_{t}x,\]

where \(F_{t}\) is the solution of the Ricatti equation

\[\frac{dF_{t}}{dt}+A^{\top}F_{t}+F_{t}A-2F_{t}\sigma_{0}\sigma_{0}^{\top}F_{t} +P=0\]

with the final condition \(F_{T}=Q\). Within the Quadratic OU class, we consider two settings:

* Easy: We set \(d=20\), \(A=0.2I\), \(P=0.2I\), \(Q=0.1I\), \(\sigma_{0}=I\), \(\lambda=1\), \(T=1\), \(x_{\mathrm{init}}=0.5N(0,I)\). We do not use warm-start for any algorithm. We take \(K=50\) time discretization steps, and we use random seed 0.
* Hard: We set \(d=20\), \(A=I\), \(P=I\), \(Q=0.5I\), \(\sigma_{0}=I\), \(\lambda=1\), \(T=1\), \(x_{\mathrm{init}}=0.5N(0,I)\). We use the _Gaussian warm-start_ (App. E). We take batch size \(m=64\) and \(K=150\) time discretization steps, and we use random seed 0.

Linear Ornstein-UhlenbeckThe functions of the control problem are chosen as follows:

\[b(x,t)=Ax,\quad f(x,t)=0,\quad g(x)=\langle\gamma,x\rangle,\quad\sigma(t)= \sigma_{0}.\]

The optimal control for this class of problems is given by [59, Sec. A.4]:

\[u_{t}^{*}(x)=-\sigma_{0}^{\top}e^{A^{\top}(T-t)}\gamma.\]

We use exactly the same functions as [59]: we sample \(\left(\xi_{ij}\right)_{1\leq i,j\leq d}\) once at the beginning of the simulation, and set:

\[d=10,\quad A=-I+\left(\xi_{ij}\right)_{1\leq i,j\leq d},\quad \gamma=\mathds{1},\quad\sigma_{0}=I+\left(\xi_{ij}\right)_{1\leq i,j\leq d},\] \[T=1,\quad\lambda=1,\quad x_{\mathrm{init}}=0.5N(0,I).\]

We take \(K=100\) time discretization steps, and we use random seed 0.

Double WellWe also use exactly the same functions as [59], which are the following:

\[b(x,t)=-\nabla\Psi(x),\quad\Psi(x)=\sum_{i=1}^{d}\kappa_{i}(x_{i}^{2}-1)^{2}, \quad f(x)=0,\quad g(x)=\sum_{i=1}^{d}\nu_{i}(x_{i}^{2}-1)^{2},\quad\sigma_{0}= \mathrm{I},\]

where \(d=10\), and \(\kappa_{i}=5\), \(\nu_{i}=3\) for \(i\in\{1,2,3\}\) and \(\kappa_{i}=1\), \(\nu_{i}=1\) for \(i\in\{4,\ldots,10\}\). We set \(T=1\), \(\lambda=1\) and \(x_{\mathrm{init}}=0\). We take \(K=200\) time discretization steps, and we use random seed \(0\). The Double Well problem is actually highly non-trivial, and is multimodal. The only reason we can produce a "ground truth" control to compare to in this setting is that we use significant knowledge of the problem; we analytically reduce it to 1D problems by decoupling each dimension and apply numerical methods to solve the Hamilton-Jacobi-Bellman equation for these 1D problems. It is not a problem where we actually have the ground truth control in closed form.

Path Integral Sampler on Mixture of GaussiansWe set

\[b(x,t)=0,\qquad f(x,t)=0,\qquad g(x)=\log(\mu^{0}(x)/\mu(x))=-\tfrac{\|x\|^{2} }{2}-\tfrac{d}{2}\log(2\pi)-\log\mu(x),\]

where \(T=1\), and \(\mu\) is the density of a mixture of two Gaussians with means \(\pm e_{1}\), where \(e_{1}=(1,0,\ldots,0)\), and variance \(\mathrm{Id}\). Note that we take \(\mu\) to be normalized, i.e. \(\int\mu(x)\,\mathrm{d}x=1\), or equivalently, \(\log Z=\log\big{(}\int\mu(x)\,\mathrm{d}x\big{)}=0\). In Figure 1, we use the following Monte Carlo estimator of the control objective at the control \(u\):

\[\hat{S}^{u}(X)=\int_{0}^{T}\big{(}\tfrac{1}{2}\|u(X_{t}^{u},t)\|^{2}+f(X_{t}^ {u},t)\big{)}\,\mathrm{d}t+g(X_{T}^{u})+\int\langle u(X_{t}^{u},t),\mathrm{d}B _{t}\rangle.\]

Note that this estimator is unbiased because \(\mathbb{E}[\int\langle u(X_{t}^{u},t),\mathrm{d}B_{t}\rangle]=0\). This is known as the Sticking the Landing estimator, as it has zero variance when \(u\) is the optimal control [72]. The fact that \(\mathbb{E}[-\hat{S}^{u}(X)]\leq\log Z=0\) with equality when \(u=u^{*}\) is stated as [84, Thm. 4].

### Model architectures

As a general guideline, the control function can be thought of as the analog of the score function in diffusion models; hence, a natural choice for the architecture can be U-Nets or diffusion transformers if the control task is on images, audio or video. Other domains may require different architectures. In the experiments we report, we used the architecture implemented in the class FullyConnectedUNet within the file SOC_matching/models.py. It is a simplified version of the U-Net architecture where both the down-sampling and up-sampling layers are fully connected with ReLU activations, and the horizontal layers are linear transformations. We use three down-sampling and up-sampling steps, with widths 256, 128 and 64 (hence, the first down-sampling step is actually an up-sampling, because the data dimensions in our experiments range from 10 to 20).

The reparameterization matrices have an unusual trait, which is that their input dimension is small (two) while their output dimension is large (\(d^{2}\)). Hence, the kind of functions that they need to learn are low dimensional and hence easy. In our case, we used the architecture implemented in the class SigmoidMLP within the file SOC_matching/models.py, which is essentially a three layer multilayer perceptron with ReLU activations and output dimension \(d^{2}\), whose output is averaged with the identity matrix using sigmoid weights, in order to enforce that \(M_{t}(t)\) be the identity matrix.

### Additional tables and plots

Table 1 shows the average times per iteration for each algorithm. Each algorithm was run using a 16GB V100 GPU.

Figure 4 shows the control objective (1) for the four settings. The error bars for the control objective plots show the confidence intervals for \(\pm\) one standard deviation, computed via a Monte Carlo

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline SOCM & SOCM \(M_{t}=I\) & SOCM adj. & Adj. & Cross entropy & Log-variance & Moment & Variance \\ \hline
0.222 & 0.090 & 0.099 & 0.169 & 0.086 & 0.117 & 0.087 & 0.086 \\ \hline \end{tabular}
\end{table}
Table 1: Time per iteration (exponential moving average) for various algorithms in seconds per iteration, for the Quadratic OU (easy) experiments (Figure 2).

estimate using 65536 trajectories per data point. They show the standard error of the mean. As expected, SOCM also obtains the lowest values for the control objective, up to the estimation error.

Figure 5 shows the normalized standard deviation of the importance weight for the learned control \(u\): \(\sqrt{\operatorname{Var}[\alpha(u,X^{u},B)]}/\mathbb{E}[\alpha(u,X^{u},B)]\). By Lemma 1, when \(X^{u}_{0}=x_{\mathrm{init}}\) for an arbitrary \(x_{\mathrm{init}}\) (which is the case for all our experiments), this quantity is zero for the optimal control \(u^{*}\). Hence, the normalized standard deviation of \(\alpha\) is an alternative metric to measure the optimality of the learned control.

Figure 6 shows an exponential moving average of the norm squared of the gradient for Linear OU and Double Well. For Linear OU, the minimum gradient norm is achieved by the adjoint method, while for Double Well it is achieved by the cross entropy loss. The training instabilities of the adjoint method become apparent as well. Interestingly, in both settings the algorithms with smallest gradients are not SOCM, which is the algorithm with smallest error as shown in Figure 3. Understanding this phenomenon is outside of the scope of this paper.

Figure 7 shows plots of the control \(L^{2}\) error, the norm squared of the gradient, and the control objective for the Quadratic OU (hard) setting, using a warm-start strategy detailed in App. E. Figure 7 shows that SOCM is once again the algorithm that achieves the lowest error and the smallest gradients. Remark that the warm-start control is a reasonable approximation of the optimal control, as the initial control \(L^{2}\) error is much lower than in the other figures.

Figure 8 shows the value of the training loss for SOCM and its two ablations: SOCM with constant \(M_{t}=I\), and SOCM-Adjoint. For all such algorithms, the training loss is the sum of the \(L^{2}\) error of the learned control \(u\), and the expected conditional variance of the matching vector field \(w\). Thus, the difference between the training loss plots and the \(L^{2}\) error plots is the expected conditional variance of \(w\). We observe that the expected conditional variance in the Quadratic OU setting is orders of magnitude smaller for SOCM than for its two ablations. For Linear OU, SOCM and SOCM-adjoint have similar expected conditional variance, and a possible explanation is that the Linear OU setting is very simple. In the Double Well setting, the SOCM-adjoint training loss curve has spikes that are probably caused by instabilities of the adjoint method. These spikes can be attributed mostly to the expected conditional variance term, since the corresponding \(L^{2}\) error curve in Figure 3 does not present them.

Figure 9 shows that the instabilities of the adjoint method are inherent to the loss, because they also appear at small learning rates: \(3\times 10^{-5}\) is smaller than the learning rates typically used for Adam, which hover from \(1\times 10^{-4}\) to \(1\times 10^{-3}\).

Figure 4: Plots of the control objective for the four settings.

Figure 5: Plots of the normalized standard deviation of the importance weights: \(\sqrt{\mathrm{Var}[\alpha(u,X^{u},B)]}/\mathbb{E}[\alpha(u,X^{u},B)]\).

Figure 6: Plots of the norm squared of the gradient for the Linear Ornstein Uhlenbeck and Double Well settings.

Figure 7: Plots of the control \(L^{2}\) error, the norm squared of the gradient, and the control objective for the Quadratic Ornstein-Uhlenbeck (hard) setting, without using warm-start.

Figure 8: Plots of the training loss for SOCM and its two ablations: SOCM with constant \(M_{t}=I\), and SOCM-Adjoint.

Figure 9: Plots of the control \(L^{2}\) error and the norm squared of the gradient for the adjoint method on Double Well, for two different values of the Adam learning rate. The instabilities of the adjoint method persist for small learning rates, signaling an inherent issue with the loss.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction we state that we introduce SOCM, a novel algorithm to solve stochastic optimal control problems, and we claim that it outperforms all existing algorithms in three out of four settings. All of these claims are supported by our paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, in the second-to-last paragraph of Sec. 5 we discuss the main limitation of SOCM (our algorithm), which is that the variance of the importance weight \(\alpha\) is too large in certain settings. Regarding the computational efficiency of our algorithm, Table 1 shows a comparison between our algorithm and existing ones. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All the theoretical results stated in the paper are numbered and have a corresponding detailed proof in the appendices. In the main text, we indicate the precise location of the proofs. We provide a proof sketch for Theorem 1 in the main paper, and a full proof in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide a description of experimental details in Subsec. F.1, and of model architectures in Subsec. F.2. We also provide a link to the GitHub repository that contains the code for this paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We also provide a link to the GitHub repository that contains the code for this paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide a description of experimental details in Subsec. F.1, and of model architectures in Subsec. F.2. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Most of the plots in the paper do not contain error bars, because the lines show exponential moving averages, which means that the error is already smoothed out and very small. The error bars for the control objective plots do show the confidence intervals for one standard deviation, computed via a Monte Carlo estimate using 65536 trajectories per data point. They show the standard error of the mean. We state this in Subsec. F.3. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We explain the type and amount of GPUs we use in Subsec. F.3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We reviewed the NeurIPS Code of Ethics and our research conforms to it. We made sure to preserve our anonymity. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: The research presented is foundational, and the code released does not have direct societal impact. Yet, it may serve as the basis to develop algorithms that improve the quality of generative models. We state this in Sec. 5. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We did not use existing assets. Guidelines:* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We present code implementing our algorithm. The code folder contains a README which explains how to reproduce the experiments. We describe our algorithm throughout the paper, and provide details in App. F. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [No] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [No]

Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.