Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity

Kaja Gruntkowska

KAUST

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia

Alexander Tyurin

KAUST

AIRI

Skoltech

AIRI

Skoltech

KAUST

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia

###### Abstract

Effective communication between the server and workers plays a key role in distributed optimization. In this paper, we focus on optimizing communication, uncovering inefficiencies in prevalent downlink compression approaches. Considering first the pure setup where the uplink communication costs are negligible, we introduce MARINA-P, a novel method for downlink compression, employing a collection of correlated compressors. Theoretical analysis demonstrates that MARINA-P with permutation compressors can achieve a server-to-worker communication complexity improving with the number of workers, thus being provably superior to existing algorithms. We further show that MARINA-P can serve as a starting point for extensions such as methods supporting bidirectional compression: we introduce M3, a method combining MARINA-P with uplink compression and a momentum step, achieving bidirectional compression with provable improvements in total communication complexity as the number of workers increases. Theoretical findings align closely with empirical experiments, underscoring the efficiency of the proposed algorithms.

## 1 Introduction

In federated learning (McMahan et al., 2017; Konecny et al., 2016) and large-scale machine learning (Ramesh et al., 2021; OpenAI, 2023), a typical environment consists of multiple devices working together to train a model. Facilitating this collaborative process requires the transmission of substantial information (e.g., gradients, current model) between these devices. In the centralized framework, communication takes place via a server. As a result, practical challenges arise due to the large size of machine learning models and network speed limitations, potentially creating a communication bottleneck (Kairouz et al., 2021; Wang et al., 2023). One possible strategy to reduce this communication burden is to use _lossy compression_(Seide et al., 2014; Alistarh et al., 2017). Our paper focuses on this research direction.

We consider the following nonconvex distributed optimization task:

\[\min_{x\in\mathbb{R}^{d}}\left\{f(x):=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x)\right\},\] (1)

where \(x\in\mathbb{R}^{d}\) is the vector of parameters of the model, \(n\) is the number of workers and \(f_{i}\,:\,\mathbb{R}^{d}\rightarrow\mathbb{R}\), \(i\in[n]:=\{1,\ldots,n\}\) are smooth nonconvex functions. We investigate the scenario where the functions \(f_{i}\) are stored on \(n\) distinct workers, each directly connected to the server via some communication port (Kairouz et al., 2021). At present, we operate under the following generic assumptions:

**Assumption 1.1**.: The function \(f\) is \(L\)-smooth, i.e., \(\left\|\nabla f(x)-\nabla f(y)\right\|\leq L\left\|x-y\right\|\forall x,y\in \mathbb{R}^{d}\).

**Assumption 1.2**.: There exists \(f^{*}\in\mathbb{R}\) such that \(f(x)\geq f^{*}\;\forall x\in\mathbb{R}^{d}\).

In the nonconvex world, our goal is to find a (possibly) random point \(\bar{x}\) such that \(\mathbb{E}[\left\|\nabla f(\bar{x})\right\|^{2}]\leq\varepsilon\). We refer to such a point an \(\varepsilon\)-stationary point.

### Related Work

Before we discuss more advanced optimization methods, let us consider the simplest baseline: the gradient descent (GD) (Lan, 2020), which iteratively performs updates \(x^{t+1}=x^{t}-\gamma\nabla f(x^{t})=x^{t}-\gamma/n\sum_{i=1}^{n}\nabla f_{i}( x^{t})\). In the distributed setting, the method can be implemented as follows: each worker calculates \(\nabla f_{i}(x^{t})\) and sends it to the server where the gradients are aggregated, after which the server takes the step and broadcasts \(x^{t+1}\) back to the workers. With step size \(\gamma=\nicefrac{{1}}{{L}}\), GD finds an \(\varepsilon\)-stationary point after \(\mathcal{O}\left(\nicefrac{{\delta^{0}L}}{{\varepsilon}}\right)\) steps, where \(\delta^{0}:=f(x^{0})-f^{*}\) for a starting point \(x^{0}\). Since at each step the workers and the server send \(\Theta(d)\) coordinates/bits, the worker-to-server (w2s, uplink) and server-to-worker (s2w, downlink) communication costs are

\[\mathcal{O}\left(\tfrac{d\delta^{0}L}{\varepsilon}\right).\] (2)

**Definition 1.3**.: The _worker-to-server (w2s)_ and _server-to-worker (s2w)_ communication complexities of a method are the expected number of coordinates/floats that a worker sends to the server and that the server sends to a worker, respectively, to find an \(\varepsilon\)-solution. The _total communication complexity_ is the sum of these complexities.

**Unbiased compressors.** In this work, to perform lossy compression, we employ mappings from the following family:

**Definition 1.4**.: A stochastic mapping \(\mathcal{C}\;:\,\mathbb{R}^{d}\to\mathbb{R}^{d}\) is an _unbiased compressor_ if there exists \(\omega\geq 0\) such that

\[\mathbb{E}\left[\mathcal{C}(x)\right]=x,\,\mathbb{E}\left[\left\|\mathcal{C}( x)-x\right\|^{2}\right]\leq\omega\left\|x\right\|^{2}\,\forall x\in \mathbb{R}^{d}.\] (3)

We denote the family of such mappings by \(\mathbb{U}(\omega)\). A canonical example is the Rand\(K\in\mathbb{U}(\nicefrac{{d}}{{K}}-1)\) sparsifier, which preserves \(K\) random coordinates of a vector scaled by \(\nicefrac{{d}}{{K}}\)(Beznosikov et al., 2020). More examples can be found in Wangni et al. (2018); Beznosikov et al. (2020); Szlendak et al. (2021); Horvath et al. (2022). A larger family of compressors, called _biased compressors_, also exists (see Section B). In this paper, we implicitly assume that compressors are mutually independent _across iterations_ of algorithms.

**Worker-to-server compression scales with \(n\).** Many previous works ignore the s2w communication costs and focus solely on w2s compression, assuming that _broadcasting is free_. For nonconvex objective functions, the current state-of-the-art w2s communication complexities are achieved by the MARINA and DASHA methods (Gorbunov et al., 2021; Szlendak et al., 2021; Tyurin and Richtarik, 2023b). Here, two additional assumptions are needed:

**Assumption 1.5**.: The function \(f_{i}\) is \(L_{i}\)-smooth. We define \(\widehat{L}^{2}:=\frac{1}{n}\sum_{i=1}^{n}L_{i}^{2}\) and \(L_{\max}:=\max_{i\in[n]}L_{i}\).

**Assumption 1.6**.: For all \(\mathcal{C}\in\mathbb{U}(\omega),\) all calls of \(\mathcal{C}\) are mutually independent.4

Footnote 4: This assumptions means that if an algorithm calls a compressor \(\mathcal{C}\) at some points \(x_{1},\ldots,x_{m},\) then \(\mathcal{C}(x_{1}),\ldots,\mathcal{C}(x_{m})\) are _i.i.d._

Under Assumptions 1.1, 1.2, 1.5, 1.6, and considering the Rand\(K\) compressor with \(K\leq\nicefrac{{d}}{{\sqrt{n}}}\) as an example, the w2s communication complexity of both methods is

\[\underbrace{\mathcal{K}}_{\text{\# of sent coord.}}\times\underbrace{\mathcal{O} \left(\tfrac{\delta^{0}}{\varepsilon}(L+\tfrac{\omega}{\sqrt{n}}\widehat{L}) \right)}_{\text{\# of iterations}}=\mathcal{O}\left(\tfrac{d\delta^{0}\widehat{L}}{ \sqrt{n}\varepsilon}\right),\] (4)where we use the facts that \(L\leq\widehat{L}\) and \(\omega=\nicefrac{{d}}{{K}}-1\) for \(\text{Rand}K\). The key observation is that when comparing (2) and (4), one sees that (4) can be \(\sqrt{n}\) times smaller if \(\widehat{L}\approx L\). Consequently, the communication complexity of \(\text{MARINA}/\text{DASHA}\) scales with the number of workers \(n\), and can _provably_ improve the _worker-to-server_ communication complexity \(\mathcal{O}\left(d^{\delta^{0}L}\nicefrac{{L}}{{\varepsilon}}\right)\) achieved by GD.

**Server-to-worker compression does not scale with \(n\).** In certain applications, the significance of s2w communication cannot be ignored. In 4G LTE and 5G networks, w2s and s2w communication speeds can be almost the same (Huang et al., 2012) or differ by at most a factor of \(10\)(Narayanan et al., 2021). Although important, this issue is often overlooked and that is why it is the s2w communication that this work places a central emphasis on.

There exist many papers using communication compression techniques to reduce the s2w communication (Zheng et al., 2019; Liu et al., 2020; Philippenko and Dieuleveut, 2021; Fatkhullin et al., 2021; Gruntkowska et al., 2023; Tyurin and Richtarik, 2023a). However, to the best of our knowledge, under Assumptions 1.1, 1.2, 1.5, and 1.6, in the worst case, all previous theoretical s2w communication guarantees _are greater or equal_ to (2). As an example, let us consider the result from Gruntkowska et al. (2023)[Theorem E.3]. If the server employs operators from \(\mathbb{U}(\omega)\) and we ignore w2s compression, the method from Gruntkowska et al. (2023) converges in \(\mathcal{O}\left(\nicefrac{{(\omega+1)\delta^{0}L}}{{\varepsilon}}\right)\) iterations. Thus, with \(\text{Rand}K\), the s2w communication complexity is \(\mathcal{O}\left(K\times\nicefrac{{(\omega+1)\delta^{0}L}}{{\varepsilon}} \right)=\mathcal{O}\left(d^{\delta^{0}L}\nicefrac{{L}}{{\varepsilon}}\right)\). Another method, called CORE, proposed by Yue et al. (2023), achieves s2w and w2s communication complexities equal to \(\mathcal{O}\left(\nicefrac{{r_{1}(f)\delta^{0}L}}{{\varepsilon}}\right)\), where \(r_{1}(f)\) is a uniform upper bound of the trace of the Hessian. When \(r_{1}(f)\leq dL\), CORE can improve on GD. However, this complexity does not scale with \(n\) and requires an additional assumption about the Hessian of \(f\).

## 2 Contributions

In our work, we aim to investigate whether the _server-to-worker and total communication complexities_ (2) of the vanilla GD method can be improved. We make the following contributions:

1. We start by proving the impossibility of devising a method where the server communicates with the workers using unbiased compressors \(\mathbb{U}(\omega)\) (or biased compressors from Section B) and achieves an _iteration rate_ faster than \(\Omega\left(\nicefrac{{(\omega+1)L\delta^{0}}}{{\varepsilon}}\right)\) (Theorem 3.1) under Assumptions 1.1, 1.2 and 1.6. This result provides a lower bound for any method that applies such compressors to vectors sent from the server to the workers in every iteration. Moreover, we prove a more general iteration lower bound of \(\Omega\left(\nicefrac{{(\omega+1)L\delta^{0}}}{{\varepsilon}}\right)\) for all methods where the server zeroes out a coordinate with probability \(\nicefrac{{1}}{{(\omega+1)}}\) (see Remark 3.2).

2. In view of this result, it is clear that an extra assumption is needed to break the lower bound \(\Omega\left(\nicefrac{{(\omega+1)L\delta^{0}}}{{\varepsilon}}\right)\). In response, we introduce a novel assumption termed "Functional \((L_{A},L_{B})\) Inequality" (see Assumption 4.2). We prove that this assumption is relatively weak and holds, for instance, under the local smoothness of the functions \(f_{i}\) (see Assumption 1.5).

3. We develop a new method for downlink compression, \(\text{MARINA}/\text{P}\), and show that under our new assumption, along with Assumptions 1.1 1.2, and 1.6, it can achieve the iteration rate of

\[\mathcal{O}\left(\tfrac{\delta^{0}L}{{\varepsilon}}+\tfrac{\delta^{0}L_{A}( \omega+1)}{{\varepsilon}}+\tfrac{\delta^{0}L_{B}(\omega+1)}{\sqrt{n}{ \varepsilon}}\right)\]

(see Theorem D.1 with \(p=\nicefrac{{1}}{{(\omega+1)}}\) + Lemma A.5). Notably, when \(L_{A}\) is small and \(n\gg 1\), this complexity is _provably_ superior to \(\Theta((\nicefrac{{\delta^{0}L}}{{\varepsilon}}+\nicefrac{{1}}{{\varepsilon}})\) and the complexities of the previous compressed methods. In this context, \(L_{A}\) serves as a measure of the similarity between the functions \(f_{i}\), and can be bounded by the "variance" of the Hessians of the functions \(f_{i}\) (see Theorem 4.8). Thus, \(\text{MARINA}/\text{P}\)_is the first method whose iteration complexity can provably improve with the number of workers \(n\)_.

4. Moreover, \(\text{MARINA}/\text{P}\) can achieve the s2w communication complexity of

\[\mathcal{O}\left(\tfrac{d\delta^{0}L}{n{\varepsilon}}+\tfrac{d\delta^{0}L_{A}} {{\varepsilon}}\right).\]

When \(L_{A}\) is small and \(n\gg 1\), this communication complexity is provably superior to (2) and the communication complexities of the previous compressed methods.

Our theoretical improvements can be combined with techniques enhancing the _w2s communication complexities_. In particular, by combining MARINA-P with MARINA(Gorbunov et al., 2021) and adding the crucial momentum step, we develop a new method, M3, that guarantees a _total communication complexity_ (s2w + w2s) of

\[\mathcal{O}\left(\tfrac{d\delta^{0}L_{\text{max}}}{n^{1/3}\varepsilon}+\tfrac{ d\delta^{0}L_{A}}{\varepsilon}\right).\]

When \(n\gg 1\) and in the close-to-homogeneous regime, i.e., when \(L_{A}\) is small, this complexity is better than (2) and the complexities of the previous bidirectionally compressed methods.

6. Our theoretical results are supported by numerical experiments (see Section F).

## 3 Lower Bound under Smoothness

Let us first investigate the possibility of improving the iteration complexity \(\mathcal{O}\left(\nicefrac{{(\omega+1)\delta^{0}L}}{{\varepsilon}}\right)\) under Assumptions 1.1,1.2 and 1.6. In Section G, we consider a family of methods that include those proposed in Zheng et al. (2019); Liu et al. (2020); Philippenko and Dieuleveut (2021); Fatkhullin et al. (2021); Gruntkowska et al. (2023), where the server communicates with workers using unbiased/biased compressors, and establish that

**Theorem 3.1** (Slightly Less Formal Reformulation of Theorem G.5).: _Under Assumptions 1.1, 1.2 and 1.6, all methods in which the server communicates with clients using different and independent unbiased compressors from \(\mathbb{U}\left(\omega\right)\) and sends one compressed vector to each worker cannot converge before \(\Omega\left(\nicefrac{{(\omega+1)L\delta^{0}}}{{\varepsilon}}\right)\) iterations._

_Remark 3.2_.: The theorem remains applicable to biased compressors \(\mathbb{B}\left(\alpha\right)\) (see Section B) with a lower bound of \(\Theta\left(L\nicefrac{{\delta^{0}}}{{\alpha\varepsilon}}\right)\). This is because if \(\mathcal{C}\in\mathbb{U}(\omega)\), then \((\omega+1)^{-1}\mathcal{C}\in\mathbb{B}\left((\omega+1)^{-1}\right).\) We also establish a more general result (Theorem G.4): "all methods in which the server zeroes out a coordinate with probability \(\leq p\) independently _across iterations_ cannot converge before \(\Omega\left(L\nicefrac{{\delta^{0}}}{{p\varepsilon}}\right)\) iterations."

This lower bound is tight up to a constant factor. For instance, under exactly the same assumptions, the EF21-P mechanism from Gruntkowska et al. (2023) converges after \(\Theta\left(\nicefrac{{(\omega+1)L\delta^{0}}}{{\varepsilon}}\right)\) iterations. Unlike (4), this convergence rate does not scale with \(n,\) and Theorem 3.1 leaves no room for improvement. Consequently, breaking the lower bound requires an additional assumption about the structure of the problem. Before presenting our candidate assumption, we first introduce the ingredients needed to leverage it to the fullest extent: our novel downlink compression method and the type of compressors we shall employ.

## 4 The MARINA-P Method

Let us first recall the MARINA method (Gorbunov et al., 2021; Szlendak et al., 2021):

\[x^{t+1} =x^{t}-\gamma g^{t},\qquad c^{t}\sim\text{Bernoulli}(p)\] (5) \[g_{i}^{t+1} =\begin{cases}\nabla f_{i}(x^{t+1})&\text{if }c^{t}=1,\\ g^{t}+\mathcal{C}_{i}^{t}(\nabla f_{i}(x^{t+1})-\nabla f_{i}(x^{t}))&\text{if }c^{t}=0 \end{cases}\quad\text{for all }i\in[n],\] \[g^{t+1} =\frac{1}{n}\sum_{i=1}^{n}g_{i}^{t+1},\]

where \(g^{0}=\nabla f(x^{0})\). Motivated by MARINA, we design its primal counterpart, MARINA-P (Algorithm 1), operating in the primal space of the model parameters, as outlined in (6).

At each iteration of MARINA-P, the workers calculate \(\nabla f_{i}(w_{i}^{t})\) and transmit it to the server. The server then averages the gradients and updates the global model \(x^{t}\). Subsequently, with some (typically small) probability \(p\), the master sends the non-compressed vector \(x^{t+1}\) to all workers. Otherwise, the \(i^{\text{th}}\) worker receives a compressed vector \(\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})\). Each worker then uses the received message to compute \(w_{i}^{t+1}\) locally. Importantly, \(\mathcal{C}_{1}^{t}(x^{t+1}-x^{t}),\ldots,\mathcal{C}_{n}^{t}(x^{t+1}-x^{t})\)_can differ_, and this distinction will form the basis of our forthcoming advancements.

The MARINA-P Method:

Initialize vectors \(x_{0},w_{0}^{0},\ldots,w_{n}^{0}\in\mathbb{R}^{d}\), step size \(\gamma>0\), probability \(0<p\leq 1\) and compressors \(\mathcal{C}_{1}^{t},\ldots,\mathcal{C}_{n}^{t}\in\mathbb{U}(\omega_{P})\) for all \(t\geq 0\). The method iterates

\[g^{t} =\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(w_{i}^{t}),\] (6) \[x^{t+1} =x^{t}-\gamma g^{t},\] \[c^{t} \sim\text{Bernoulli}(p),\] \[w_{i}^{t+1} =\begin{cases}x^{t+1}&\text{if }c^{t}=1,\\ w_{i}^{t}+\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})&\text{if }c^{t}=0\end{cases}\] for all \(i\in[n]\).

We denote \(w^{t}:=\nicefrac{{1}}{{n}}\sum_{i=1}^{n}w_{i}^{t}\). See the implementation in Algorithm 1.

Comparing (5) and (6), MARINA-P and MARINA are dual methods: both learn control variables (\(w_{i}^{t}\) and \(g_{i}^{t}\)), compress the differences (\(x^{t+1}-x^{t}\) and \(\nabla f_{i}(x^{t+1})-\nabla f_{i}(x^{t})\)), and with some probability \(p\) send non-compressed vectors (\(x^{t+1}\) and \(\nabla f_{i}(x^{t+1})\)). However, unlike MARINA, which compresses vectors sent _from workers to server_ and operates in the _dual_ space of gradients, MARINA-P compresses messages sent _from server to workers_ and operates in the _primal_ space of arguments.

Let us take \(\text{Rand}K\in\mathbb{U}(\nicefrac{{d}}{{K}}-1)\) as an example. If we set \(p=\left(\omega+1\right)^{-1}=\nicefrac{{K}}{{d}}\) to balance heavy communications of \(x^{t+1}\) and light communications of \(\mathcal{C}_{i}^{t}\) in (6), MARINA-P averages sending \(pd+(1-p)K\leq 2K\) coordinates per iteration. Then, the lower bound from Theorem G.4 implies that at least \(\Omega\left(\nicefrac{{(\omega+1)\delta^{0}L}}{{e}}\right)\) iterations of the algorithm are needed.

At first glance, it seems that MARINA-P does not offer any extra benefits compared to previous methods, and that is true - we could not expect to break the lower bound. However, as we shall soon see, under an extra assumption, MARINA-P can achieve a communication complexity that improves with \(n\).

### Three ways to compress

Existing algorithms performing s2w compression share a common characteristic: at each iteration, the server broadcasts _the same_ message to all workers (Zheng et al., 2019; Liu et al., 2020; Fatkullin et al., 2021; Gruntkowska et al., 2023; Tyurin and Richtarik, 2023a).5 In contrast, in w2s compression methods, each worker sends to the server a _different_ message, specific to the data stored on that particular device. An analogous approach can be taken in the s2w communication: intuitively, sending \(n\) distinct messages would convey more information, potentially leading to theoretical improvements. This indeed proves to be the case. While the usual approach of the server broadcasting the same vector to all clients does not lead to an improvement over (2), allowing these vectors to differ enables a well-crafted method to achieve communication complexity that improves with \(n\) (see Corollary D.4).

Footnote 5: A notable exception form this rule is the MCM method (Philippenko and Dieuleveut, 2021) - see Appendix A.

In Appendix A we provide a detailed discussion of the topic and compare the theoretical complexities of MARINA-P when the server employs three different compression techniques: a) uses _one compressor_ and sends the same vector to all clients, b) uses a _collection of independent compressors_, or c) uses a _collection of correlated compressors_. We now turn to presenting the technique that gives the best theoretical s2w communication complexity out of these, namely the use of a set of _correlated compressors_.

### Recap: permutation compressors Perm\(K\)

Szlendak et al. (2021) propose compressors that will play a key role in our new theory. For clarity of presentation, we shall assume that \(d\geq n\) and \(n|d\).6

Footnote 6: The general definition of Perm\(K\) for \(d\bmod n\neq 0\) is presented in (Szlendak et al., 2021)[App. I].

**Definition 4.1** (Perm\(K\) (for \(d\geq n\) and \(n|d\))).: Assume that \(d\geq n\) and \(d=qn\), where \(q\in\mathbb{N}_{>0}\). Let \(\pi=(\pi_{1},\ldots,\pi_{d})\) be a random permutation of \(\{1,\ldots,d\}\). For all \(x\in\mathbb{R}^{d}\) and each \(i\in\{1,2,\ldots,n\}\), we define

\[\mathcal{C}_{i}(x):=n\times\sum\limits_{j=q(i-1)+1}^{qi}x_{\pi_{j}}e_{\pi_{j}}.\]

Unpacking this definition: when the server compresses a vector using a Perm\(K\) compressor, it randomly partitions its coordinates across the workers, so that each client receives a sparse vector containing a random subset of entries of the input vector. Like Rand\(K\), Perm\(K\) is also a sparsifier. However, unlike Rand\(K\), it does not allow flexibility in choosing \(K\), as it is fixed to \(\nicefrac{{d}}{{n}}\). Furthermore, it can be shown (Lemma A.6) that \(\mathcal{C}_{i}\in\mathbb{U}(n-1)\) for all \(i\in[n]\).

An appealing property of Perm\(K\) is the fact that

\[\tfrac{1}{n}\sum\limits_{i=1}^{n}\mathcal{C}_{i}(x)=x\] (7)

for all \(x\in\mathbb{R}^{d}\) deterministically. Here, it is important to note that by design, compressors \(\mathcal{C}_{i}\) from Definition 4.1 are _correlated_, and do not satisfy Assumption 1.6. This correlation proves advantageous - Szlendak et al. (2021) show that MARINA with Perm\(K\) compressors performs provably better than with i.i.d. Rand\(K\) compressors.

### Warmup: homogeneous quadratics

We are finally ready to present our first result showing that the s2w communication complexity can scale with the number of workers \(n\). To explain the intuition behind our approach, let us consider the simplest (and somewhat impractical) choice of functions \(f_{i}\) - the homogeneous quadratics:

\[f_{i}(x)=\tfrac{1}{2}x^{\top}\mathbf{A}x+b^{\top}x+c,\quad i\in[n],\] (8)

where \(\mathbf{A}\in\mathbb{R}^{d\times d}\) is a symmetric but not necessarily positive semidefinite matrix, \(b\in\mathbb{R}^{d}\) and \(c\in\mathbb{R}\). We now investigate the operation of MARINA-P with Perm\(K\) compressors. With probability \(p\), we have \(w^{t+1}=x^{t+1}\). Otherwise \(w^{t+1}=w^{t}+\frac{1}{n}\sum_{i=1}^{n}\mathcal{C}_{i}^{t}(x^{t+1}-x^{t}) \stackrel{{(\ref{eq:w_1})}}{{=}}x^{t+1}+(w^{t}-x^{t}).\) Hence, if we initialize \(w_{i}^{0}=x^{0}\) for all \(i\in[n],\) an inductive argument shows that \(w^{t}=x^{t}\) deterministically for all \(t\geq 0\). Then, substituting the gradients of \(f_{i}\) to (6), one gets

\[g^{t}=\tfrac{1}{n}\sum\limits_{i=1}^{n}(\mathbf{A}w_{i}^{t}+b)=\mathbf{A}w^{t }+b=\mathbf{A}x^{t}+b=\nabla f(x^{t})\]

for all \(t\geq 0\). Therefore, MARINA-P with Perm\(K\) compressor in this setting is essentially a smart implementation of vanilla GD! Indeed, for \(p\leq\nicefrac{{1}}{{n}}\), MARINA-P with Perm\(K\) sends on average \(\leq\nicefrac{{2d}}{{n}}\) coordinates to each worker, so the s2w communication complexity is

\[\tfrac{2d}{n}\times\underbrace{\mathcal{O}\Big{(}\frac{\delta^{0}L}{\varepsilon }\Big{)}}_{\textsf{GD rate}}=\mathcal{O}\left(\tfrac{d\delta^{0}L}{n\varepsilon} \right),\]

which is \(n\) times smaller than in (2)!

### Functional \((L_{a},l_{b})\) Inequality

From the discussion in Section 3, we know that to improve (2), an extra assumption about the structure of the problem is needed. Building on the example from Section 4.3, we introduce the _Functional \((L_{A},l_{B})\) Inequality_.

**Assumption 4.2** (Functional \((L_{A},L_{B})\) Inequality).: There exist constants \(L_{A},L_{B}\geq 0\) such that

\[\left\|\tfrac{1}{n}\sum_{i=1}^{n}(\nabla f_{i}(x+u_{i})-\nabla f_{i}(x))\right\|^ {2}\leq L_{A}^{2}\left(\tfrac{1}{n}\sum_{i=1}^{n}\left\|u_{i}\right\|^{2}\right)+ L_{B}^{2}\left\|\tfrac{1}{n}\sum_{i=1}^{n}u_{i}\right\|^{2}\] (9)

for all \(x,u_{1},\ldots,u_{n}\in\mathbb{R}^{d}\).

_Remark 4.3_.: A similar assumption, termed "Heterogeneity-driven Lipschitz Condition on Averaged Gradients", is proposed in Wang et al. (2023b). Our assumption aligns with theirs when \(L_{B}=0\). However, our formulation proves to be more powerful. The possibility that \(L_{B}>0\) becomes instrumental in driving the enhancements we introduce.

Assumption 4.2 is defined for all functions together, and intuitively, it tries to capture the similarities between the functions \(f_{i}\). For \(n=1\), inequality (9) reduces to

\[\left\|\nabla f(x)-\nabla f(y)\right\|^{2}\leq\left(L_{A}^{2}+L_{B}^{2} \right)\left\|x-y\right\|^{2}\forall x,y\in\mathbb{R}^{d},\]

equivalent to standard \(L\)-smoothness (Assumption 1.1) with \(L^{2}=L_{A}^{2}+L_{B}^{2}\). The Functional \((L_{A},L_{B})\) Inequality is reasonably weak also for \(n>1\), as the next theorem shows.

**Theorem 4.4**.: _For all \(i\in[n],\) assume that the functions \(f_{i}\) are \(L_{i}\)-smooth (Assumption 1.5). Then, Assumption 4.2 holds with \(L_{A}=L_{\max}\) and \(L_{B}=0\)._

Therefore, Assumption 4.2 holds whenever the functions \(f_{i}\) are smooth, which is a standard assumption in the literature. Now, returning to the example from Section 4.3,

**Theorem 4.5**.: _For all \(i\in[n],\) assume that the functions \(f_{i}\) are homogeneous quadratics defined in \((\ref{eq:f_i})\). Then, Assumption 4.2 holds with \(L_{A}=0\) and \(L_{B}=\left\|\mathbf{A}\right\|.\)_

Under Assumption 1.5, no information about the similarity of the functions \(f_{i}\) is available, yielding \(L_{B}=0\) and \(L_{A}>0\) in Theorem 4.4. However, once we have some information limiting heterogeneity, \(L_{A}\) can decrease. Notably, \(L_{A}=0\) for homogeneous quadratics. As we shall see in Section 4.5, the values \(L_{A}\) and \(L_{B}\) significantly influence the s2w communication complexity of MARINA-P, with lower \(L_{A}\) values leading to greatly improved performance.

### The Convergence Theory of MARINA-P with Perm\(K\)

We are ready to present our main convergence result, focusing on the Perm\(K\) compressor from Section 4.2. This choice simplifies the presentation, but our approach generalizes to a much larger class of compression operators. The full theoretical framework, covering all unbiased compressors, is detailed in Appendix D.

\begin{table}
\begin{tabular}{c c c} \hline \hline \multicolumn{2}{c}{Server-to-Workers Communication Complexities (s2w)} & \multicolumn{2}{c}{Total Communication Complexities (s2w + w2s)} \\ \hline
**Method** & **Complexity** & **Method** & **Complexity** \\ \hline GD & & GD \\ and other compressed methods\({}^{(n)}\) & & and other compressed \\ \hline CORE & & method\({}^{(n)}\) \\ (Vie et al., 2023) & & CORE & \(\frac{d^{2}u}{\varepsilon}\mathbf{A}\) \\ \hline MARINA-P & & & \\ with independent Rand\({}^{(K)}\) & & \(\frac{d^{2}u}{\sqrt{\varepsilon}}\frac{1}{n}\left\|\mathbf{A}_{i}\right\|+\frac{d^{ 2}\max_{i\in[n]}\left\|\mathbf{A}_{i}-\mathbf{A}\right\|}{\varepsilon}\) & \(\mathbf{M}\) \\ (Courley \(\mathbf{A}\))-D & & & \\ \hline MARINA-P with Perm\(K^{(n)}\) & & \(\frac{d^{2}\left\|\mathbf{A}_{i}\right\|}{n\varepsilon}+\frac{d^{2}\max_{i\in[n]} \left\|\mathbf{A}_{i}-\mathbf{A}\right\|}{\varepsilon}\) & \(\mathbf{M}\) \\ (Chevena \(\mathbf{S}\)-1) & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: **The worst case _communication complexities_ to find an \(\varepsilon\)-stationary point. For simplicity, we compare the complexities with non-homogeneous quadratics: \(f_{i}(x)=\tfrac{1}{2}x^{\top}\mathbf{A}_{i}x+b_{i}^{\top}x+c_{i},\) where \(\mathbf{A}_{i}\in\mathbb{R}^{d\times d}\) is symmetric but not necessarily positive semidefinite, \(b_{i}\in\mathbb{R}^{d}\) and \(c_{i}\in\mathbb{R}\) for \(i\in[n]\). We denote \(\mathbf{A}=\tfrac{1}{n}\sum_{i=1}^{n}\mathbf{A}_{i}\).

**Theorem 4.6**.: _Let Assumptions 1.1, 1.2 and 4.2 be satisfied. Set \(w_{i}^{0}=x^{0}\) for all \(i\in[n].\) Take \(\text{Perm}K\) as \(\mathcal{C}_{i}^{t}\) and \(\gamma=\left(L+L_{A}\sqrt{\omega_{P}(\nicefrac{{1}}{{p}}-1)}\right)^{-1},\) where \(\omega_{P}=n-1\) (Lemma A.6). Then, \(\mathsf{MARINA-P}\) finds an \(\varepsilon\)-stationary point after_

\[\mathcal{O}\left(\tfrac{\delta^{0}}{\varepsilon}\left(L+L_{A}\sqrt{\nicefrac{{ \omega_{P}}}{{p}}}\right)\right)\]

_iterations._

**Corollary 4.7**.: _Let \(p=\nicefrac{{K}}{{d}}\equiv\nicefrac{{1}}{{n}}\). Then, in the view of Theorem 4.6, the average s2w communication complexity of \(\mathsf{MARINA-P}\) with \(\text{Perm}K\) compressor is_

\[\mathcal{O}\left(\tfrac{d\delta^{0}L}{ne}+\tfrac{d\delta^{0}L_{A}}{ \varepsilon}\right).\] (10)

The key observation is that (10) is independent of \(L_{B},\) and only depends on \(L_{A}\). This particular property is specific to _correlated compressors_ with parameter \(\theta=0\) (defined in Appendix A), such as \(\text{Perm}K\). A similar result holds for _independent_\(\text{Rand}K\) compressors (see Corollary D.4), but the convergence rate is worse and depends on \(L_{B}\). Nevertheless, this dependence improves with \(n\).

When \(L_{A}=0\), which is the case for homogeneous quadratics, the step size bound from Theorem 4.6 simplifies to \(\gamma\leq\nicefrac{{1}}{{L}}\), the standard GD stepsize (recall that in this case our method reduces to GD). Most importantly, (10) scales with the number of workers \(n!\) Even when \(L_{A}>0,\) for sufficiently big \(n,\) (10) can improve (2) to \(\mathcal{O}\left(\nicefrac{{d\delta^{0}L_{A}}}{\varepsilon}\right).\)

Let us now investigate how the constants \(L_{A}\) and \(L_{B}\) change in the general case.

### Estimating \(L_{a}\) and \(L_{b}\) in the General Case

It is clear from Corollary 4.7 that \(\mathsf{MARINA-P}\) with \(\text{Perm}K\) shines when \(L_{A}\) is small. To gain further insights into what values \(L_{A}\) may take, we now provide an analysis based on the Hessians of the functions \(f_{i}\).

**Theorem 4.8**.: _Assume that the functions \(f_{i}\) are twice continuously differentiable, \(L_{i}\)-smooth (Assumption 1.5), and that there exist \(D_{i}\geq 0\) such that_

\[\sup_{z_{1},\ldots,z_{n}\in\mathbb{R}^{d}}\left\|\nabla^{2}f_{i}(z_{i})- \tfrac{1}{n}\sum_{j=1}^{n}\nabla^{2}f_{j}(z_{j})\right\|\leq D_{i}\] (11)

_for all \(i\in[n]\). Then, Assumption 4.2 holds with \(L_{A}=\sqrt{2}\max_{i\in[n]}D_{i}\leq 2\sqrt{2}\max_{i\in[n]}L_{i}\) and \(L_{B}=\sqrt{2}\left(\tfrac{1}{n}\sum_{i=1}^{n}L_{i}\right).\)_

Intuitively, (11) measures the similarity between the functions \(f_{i}\). The above theorem yields a more refined result than Theorem 4.4: it is always true that \(\max_{i\in[n]}D_{i}\leq 2\max_{i\in[n]}L_{i}\), and, in fact, \(\max_{i\in[n]}D_{i}\) can be much smaller, as the next result shows.

**Theorem 4.9**.: _Assume that \(f_{i}(x)=\tfrac{1}{2}x^{\top}\mathbf{A}_{i}x+b_{i}^{\top}x+c_{i},\) where \(\mathbf{A}_{i}\in\mathbb{R}^{d\times d}\) is symmetric but not necessarily positive semidefinite, \(b_{i}\in\mathbb{R}^{d}\) and \(c_{i}\in\mathbb{R}\) for \(i\in[n].\) Define \(\mathbf{A}=\frac{1}{n}\sum_{i=1}^{n}\mathbf{A}_{i}.\) Then, Assumption 4.2 holds with \(L_{A}=\sqrt{2}\max_{i\in[n]}\left\|\mathbf{A}_{i}-\mathbf{A}\right\|\) and \(L_{B}=\sqrt{2}\left(\tfrac{1}{n}\sum_{i=1}^{n}\left\|\mathbf{A}_{i}\right\| \right).\)_

Thus, \(L_{A}\) is less than or equal to \(\sqrt{2}\max_{i\in[n]}\left\|\mathbf{A}_{i}-\mathbf{A}\right\|,\) which serves as a measure of similarity between the matrices. The smaller the values of \(\left\|\mathbf{A}_{i}-\mathbf{A}\right\|\) (indicating greater similarity among the functions \(f_{i}\)), the smaller the \(L_{A}\) value.

In the view of this theorem, the s2w communication complexity of \(\mathsf{MARINA-P}\) with \(\text{Perm}K\) on non-homogeneous quadratics is

\[\mathcal{O}\left(\tfrac{d\delta^{0}\left\|\mathbf{A}\right\|}{ne}+\tfrac{d \delta^{0}\max_{i\in[n]}\left\|\mathbf{A}_{i}-\mathbf{A}\right\|}{\varepsilon }\right).\] (12)

Since the corresponding complexity of GD is

\[\mathcal{O}\left(\tfrac{d\delta^{0}\left\|\mathbf{A}\right\|}{\varepsilon} \right),\] (13)

in the close-to-homogeneous regimes (i.e., when \(\max_{i\in[n]}\left\|\mathbf{A}_{i}-\mathbf{A}\right\|\) is small), the complexity (12) can be _provably_ much smaller than (13). The same reasoning applies to the general case when thefunctions \(f_{i}\) are not quadratics: MARINA-P improves with the number of workers \(n\) in the regimes when \(D_{i}\) are small (see Theorem 4.8).

Let us note that there is another method, CORE, by Yue et al. (2023), that can also provably outperform GD, achieving the s2w communication complexity of \(\Omega\left(\nicefrac{{\delta^{0}\text{tr}\mathbf{A}}}{{e}}\right)\) on non-homogeneous quadratics. Neither their method nor ours universally provides the best possible communication guarantees. Our method excels in the close-to-homogeneous regimes: for example, if we take \(\mathbf{A}_{i}=L_{i}\mathbf{I}\) for all \(i\in[n]\), and define \(L=\nicefrac{{1}}{{n}}\sum_{i=1}^{n}L_{i}\), then the complexity of CORE is \(\Omega\left(\nicefrac{{d^{0}\mathbf{L}}}{{e}}\right),\) while ours is \(\mathcal{O}\left(\frac{d^{0}\mathbf{L}}{{n}{e}}+\frac{d^{\delta^{0}\max_{i \in[n]}}|L_{i}-L|}{{e}}\right).\) Hence, our guarantees are superior in regimes where \(\max_{i\in[n]}|L_{i}-L|\ll L\). One interesting research direction is to develop a universally better method combining the benefits of both approaches.

## 5 M3: A New Bidirectional Method

In the previous sections, we introduce a new method that provably improves the _server-to-worker_ communication, but ignores the _worker-to-server_ communication overhead. Our aim now is to treat MARINA-P as a starting point for developing methods applicable to more practical scenarios, by combining it with techniques that compress in the opposite direction. Since the theoretical state-of-the-art w2s communication complexity is obtained by MARINA (see Section 1.1), our next research step was to combine the two and analyze "MARINA + MARINA-P", but this naive approach did not yield communication complexity guarantees surpassing (2) in any regime. It became apparent that some "buffer" step between these two techniques is needed, and this step turned out to be the momentum. Our new method, M3 (Algorithm 2), is described in (14).

M3 combines (5), (6), and the momentum step \(z_{i}^{t+1}=\beta w_{i}^{t+1}+(1-\beta)z_{i}^{t}\), which is the key to our improvements. A similar technique is used to reduce the variance in Fatkhullin et al. (2023). Let us explain how M3 works in practice. First, the server calculates \(x^{t+1}\). Depending on the first probabilistic decision, it sends either \(x^{t+1}\) or \(\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})\) to the workers, who then calculate \(w_{i}^{t+1}\) locally. Next, the workers compute \(z_{i}^{t+1},\) and depending on the second probabilistic decision, they send either \(\nabla f_{i}(z_{i}^{t+1})\) or \(\mathcal{Q}_{i}^{t}(\nabla f_{i}(z_{i}^{t+1})-\nabla f_{i}(z_{i}^{t}))\) back to the server. The server aggregates the received vectors and calculates \(g^{t+1}\). As in MARINA, \(p_{P}\) and \(p_{D}\) are chosen in such a way that the non-compressed communication does not negatively affect the communication complexity. Therefore, the method predominantly transmits compressed information, with only a marginal probability of sending uncompressed vectors.

### The Convergence Theory of M3

For simplicity, we consider \(\text{Perm}K\) in the role of \(\mathcal{C}_{i}^{t}\) and \(\text{Rand}K\) in the role of \(\mathcal{Q}_{i}^{t}\). The general theory for all unbiased compressors is presented in Section E.

**Theorem 5.1**.: _Let Assumptions 1.1, 1.2, 1.5 and 4.2 be satisfied. Take \(\gamma=\left(L+34\left(nL_{A}+n^{2/3}L_{B}+n^{2/3}L_{\max}\right)\right)^{-1}\), \(p_{D}=p_{P}=\nicefrac{{1}}{{n}}\), \(\beta=n^{-2/3}\), \(w_{i}^{0}=z_{i}^{0}=x^{0}\) and \(g_{i}^{0}=\nabla f_{i}(x^{0})\) for all \(i\in[n]\). Then \(\text{\sc MARINA-P}\) with \(\mathcal{C}_{i}^{t}=\)Perm\(K\) and \(\mathcal{Q}_{i}^{t}=\) Rand\(K\) with \(K=\nicefrac{{d}}{{n}}\) finds an \(\varepsilon\)-stationary point after \(\mathcal{O}\!\left(\frac{\delta^{0}}{\varepsilon}\left(n^{2/3}L_{\max}+nL_{A} \right)\,\right)\) iterations. The total communication complexity is_

\[\mathcal{O}\left(\frac{d\delta^{0}L_{\max}}{n^{1/3}\varepsilon}+\frac{d\delta ^{0}L_{A}}{\varepsilon}\right).\] (15)

Once again, we observe improvement with the number of workers \(n\), and the obtained complexity (15) can be provably smaller than (2). Indeed, in scenarios like federated learning, where the number of workers (e.g., mobile phones) is typically large (Kairouz et al., 2021; Chowdhery et al., 2023), the first term can be significantly smaller than \(d\delta^{0}L/\varepsilon\). The second term can also be small in close-to-homogeneous regimes (see Section 4).

## 6 Experimental Highlights

This section presents insights from the experiments, with further details and additional results in Appendix F. The experiment aims to empirically test the theoretical results from Section 4. We consider a quadratic optimization problem, where the functions \(f_{i}\) are as defined in Theorem 4.9 and \(\mathbf{A}_{i}\in\mathbb{R}^{300\times 300}\). We compare GD, \(\text{\sc MARINA-P}\) sending the same message compressed using a single Rand\(K\) compressor to all workers ("SameRand\(K\)" from Appendix A), \(\text{\sc MARINA-P}\) with independent Rand\(K\) compressors, \(\text{\sc MARINA-P}\) with \(\text{Perm}K\) compressors, and \(\text{\sc EF21-P}\) with \(\text{Top}K\) compressor. We consider \(n\in\{10,100,1000\}\) and fine-tune the step size for each algorithm. The results, presented in Figure 1, align closely with the theory, with \(\text{\sc MARINA-P}\) using \(\text{Perm}K\) compressors consistently performing best. Moreover, the convergence rate of \(\text{\sc MARINA-P}\) with \(\text{Perm}K\) and independent \(\text{Rand}K\) compressors improves with \(n\). Since this is not the case for \(\text{\sc EF21-P}\), even though it outperforms \(\text{\sc MARINA-P}\) with independent Rand\(K\) compressors for \(n=10\), it falls behind for \(n\in\{100,1000\}\).

Figure 1: Experiments on the quadratic optimization problem from Section 6. We plot the norm of the gradient w.r.t. # of coordinates sent from the server to the workers.

## Acknowledgments and Disclosure of Funding

The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) Center of Excellence for Generative AI, under award number 5940, iii) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science. The work of A.T. was partially supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021).

## References

* Alistarh et al. (2017) Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. (2017). QSGD: Communication-efficient SGD via gradient quantization and encoding. In _Advances in Neural Information Processing Systems (NIPS)_, pages 1709-1720.
* Arjevani et al. (2022) Arjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro, N., and Woodworth, B. (2022). Lower bounds for non-convex stochastic optimization. _Mathematical Programming_, pages 1-50.
* Beznosikov et al. (2020) Beznosikov, A., Horvath, S., Richtarik, P., and Safaryan, M. (2020). On biased compression for distributed learning. _arXiv preprint arXiv:2002.12410_.
* Carmon et al. (2020) Carmon, Y., Duchi, J. C., Hinder, O., and Sidford, A. (2020). Lower bounds for finding stationary points I. _Mathematical Programming_, 184(1):71-120.
* Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. (2023). PaLM: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113.
* Fang et al. (2018) Fang, C., Li, C. J., Lin, Z., and Zhang, T. (2018). SPIDER: Near-optimal non-convex optimization via stochastic path integrated differential estimator. In _NeurIPS Information Processing Systems_.
* Fatkhullin et al. (2021) Fatkhullin, I., Sokolov, I., Gorbunov, E., Li, Z., and Richtarik, P. (2021). EF21 with bells & whistles: Practical algorithmic extensions of modern error feedback. _arXiv preprint arXiv:2110.03294_.
* Fatkhullin et al. (2023) Fatkhullin, I., Tyurin, A., and Richtarik, P. (2023). Momentum provably improves error feedback! _Advances in Neural Information Processing Systems_.
* Gorbunov et al. (2021) Gorbunov, E., Burlachenko, K., Li, Z., and Richtarik, P. (2021). MARINA: Faster non-convex distributed learning with compression. In _38th International Conference on Machine Learning_.
* Gruntkowska et al. (2023) Gruntkowska, K., Tyurin, A., and Richtarik, P. (2023). EF21-P and friends: Improved theoretical communication complexity for distributed optimization with bidirectional compression. In _International Conference on Machine Learning_, pages 11761-11807. PMLR.
* Horvath et al. (2022) Horvath, S., Ho, C.-Y., Horvath, L., Sahu, A. N., Canini, M., and Richtarik, P. (2022). Natural compression for distributed deep learning. In _Mathematical and Scientific Machine Learning_, pages 129-141. PMLR.
* Huang et al. (2012) Huang, J., Qian, F., Gerber, A., Mao, Z. M., Sen, S., and Spatscheck, O. (2012). A close examination of performance and power characteristics of 4G LTE networks. In _Proceedings of the 10th international conference on Mobile systems, applications, and services_, pages 225-238.
* Huang et al. (2022) Huang, X., Chen, Y., Yin, W., and Yuan, K. (2022). Lower bounds and nearly optimal algorithms in distributed learning with communication compression. _arXiv preprint arXiv:2206.03665_.
* Kairouz et al. (2021) Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al. (2021). Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210.
* Konecny et al. (2016) Konecny, J., McMahan, H. B., Yu, F. X., Richtarik, P., Suresh, A. T., and Bacon, D. (2016). Federated learning: Strategies for improving communication efficiency. _arXiv preprint arXiv:1610.05492_.
* Lan (2020) Lan, G. (2020). _First-order and stochastic optimization methods for machine learning_. Springer.
* Lester and Rafter (2015)LeCun, Y., Cortes, C., and Burges, C. (2010). MNIST handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2.
* Li et al. (2021) Li, Z., Bao, H., Zhang, X., and Richtarik, P. (2021). PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In _International Conference on Machine Learning_, pages 6286-6295. PMLR.
* Liu et al. (2020) Liu, X., Li, Y., Tang, J., and Yan, M. (2020). A double residual compression algorithm for efficient distributed learning. In _International Conference on Artificial Intelligence and Statistics_, pages 133-143. PMLR.
* Lu and De Sa (2021) Lu, Y. and De Sa, C. (2021). Optimal complexity in decentralized training. In _International Conference on Machine Learning_, pages 7111-7123. PMLR.
* McMahan et al. (2017) McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2017). Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR.
* Narayanan et al. (2021) Narayanan, A., Zhang, X., Zhu, R., Hassan, A., Jin, S., Zhu, X., Zhang, X., Rybkin, D., Yang, Z., Mao, Z. M., et al. (2021). A variegated look at 5G in the wild: performance, power, and QoE implications. In _Proceedings of the 2021 ACM SIGCOMM 2021 Conference_, pages 610-625.
* OpenAI (2023) OpenAI (2023). GPT-4 technical report. _ArXiv_, abs/2303.08774.
* Philippenko and Dieuleveut (2021) Philippenko, C. and Dieuleveut, A. (2021). Preserved central model for faster bidirectional compression in distributed settings. _Advances in Neural Information Processing Systems_, 34:2387-2399.
* Ramesh et al. (2021) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021). Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR.
* Richtarik et al. (2021) Richtarik, P., Sokolov, I., and Fatkhullin, I. (2021). EF21: A new, simpler, theoretically better, and practically faster error feedback. _In Neural Information Processing Systems, 2021_.
* Seide et al. (2014) Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D. (2014). 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In _Fifteenth Annual Conference of the International Speech Communication Association_.
* Szlendak et al. (2021) Szlendak, R., Tyurin, A., and Richtarik, P. (2021). Permutation compressors for provably faster distributed nonconvex optimization. In _International Conference on Learning Representations_.
* Tyurin and Richtarik (2023a) Tyurin, A. and Richtarik, P. (2023a). 2Direction: Theoretically faster distributed training with bidirectional communication compression. _Advances in Neural Information Processing Systems_.
* Tyurin and Richtarik (2023b) Tyurin, A. and Richtarik, P. (2023b). DASHA: Distributed nonconvex optimization with communication compression, optimal oracle complexity, and no client synchronization. _11th International Conference on Learning Representations (ICLR)_.
* Tyurin and Richtarik (2023c) Tyurin, A. and Richtarik, P. (2023c). Optimal time complexities of parallel stochastic optimization methods under a fixed computation model. _Advances in Neural Information Processing Systems_.
* Wang et al. (2023a) Wang, J., Lu, Y., Yuan, B., Chen, B., Liang, P., De Sa, C., Re, C., and Zhang, C. (2023a). CocktailSGD: Fine-tuning foundation models over 500Mbps networks. In _International Conference on Machine Learning_, pages 36058-36076. PMLR.
* Wang et al. (2023b) Wang, J., Wang, S., Chen, R.-R., and Ji, M. (2023b). A new theoretical perspective on data heterogeneity in federated optimization. In _Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities_.
* Wangni et al. (2018) Wangni, J., Wang, J., Liu, J., and Zhang, T. (2018). Gradient sparsification for communication-efficient distributed optimization. _Advances in Neural Information Processing Systems_, 31.
* Yue et al. (2023) Yue, P., Zhao, H., Fang, C., He, D., Wang, L., Lin, Z., and Zhu, S.-c. (2023). CORE: Common random reconstruction for distributed optimization with provable low communication complexity. _arXiv preprint arXiv:2309.13307_.
* Zheng et al. (2019) Zheng, S., Huang, Z., and Kwok, J. (2019). Communication-efficient distributed blockwise momentum SGD with error-feedback. _Advances in Neural Information Processing Systems_, 32.

###### Contents

* 1 Introduction
	* 1.1 Related Work
* 2 Contributions
* 3 Lower Bound under Smoothness
* 4 The MARINA-P Method
	* 4.1 Three ways to compress
	* 4.2 Recap: permutation compressors Perm\(K\)
	* 4.3 Warmup: homogeneous quadratics
	* 4.4 Functional \((L_{A},L_{B})\) Inequality
	* 4.5 The Convergence Theory of MARINA-P with Perm\(K\)
	* 4.6 Estimating \(L_{A}\) and \(L_{B}\) in the General Case
* 5 M3: A New Bidirectional Method
	* 5.1 The Convergence Theory of M3
* 6 Experimental Highlights
* A Three unbiased ways to compress
* B Biased Compressors
* C Properties of \(L_{A}\) and \(L_{B}\)
* D Convergence of MARINA-P in the General Case
* D.1 Main Results
* D.2 Proofs
* D.3 Polyak-Lojasiewicz condition
* D.3.1 Main Results
* D.3.2 Proofs
* E Convergence of M3 in the General Case
* E.1 Main Results
* E.2 Proofs
* E.3 Polyak-Lojasiewicz condition
* E.3.1 Main Results
* E.3.2 Proofs
* F Experiments
* F.1 Experiments with M3 on quadratic optimization tasks

* Experiments with an autoencoder and MNIST
* F.3 Extra experiments with quadratic optimization tasks
* Proof of the Lower Bounds
* G.1 The "difficult" function from the nonconvex world
* G.2 Theorems
* G.3 Compressed communication with independent compressors
* H Useful Identities and Inequalities
* NotationThree unbiased ways to compress

The main focus of this paper is handling the server-to-worker communication costs. To explain better where the improvements outlined in the main part of this paper come from, let us first consider the scenario where uplink communication cost is negligible but downlink communication cost is not. While we do no necessarily say that this is a realistic setup, examining it first enables us to understand how downlink compression should be performed, capturing all the intricacies.

Existing algorithms with lossy s2w and w2s communication have a certain common feature. The compression mechanism employed on the clients is very different from the one used on the server: while each client transmits to the server a different message, specific to the data stored on each device, the server broadcasts the same update to all clients. We want to question this algorithmic step and suggest to the reader that if compression is applied multiple times and each worker receives its individual update, then intuitively more information can be transmitted. A well-designed algorithm should be able to take advantage of this.

One can depart from the usual approach of sending the same update to all workers in two ways: a) compress the update \(n\) times _independently_, or b) produce \(n\) such updates in a _correlated_ way. Either way, the server broadcasts \(n\) different compressed messages rather than one, and sends a _different_ update to each worker. The key discovery here is that both a) and b) are mathematically _provably better_ than the prevalent approach of sending the same update to all clients.

This is a crucial improvement in a system where the above setup is a good approximation of reality. And even if it is not, and the current model is not perfectly capturing the reality, we can accept it for now, as it allows us to focus on the novel aspects of the approach. With that said, these considerations can serve as a starting point for thinking about _bidirectional compression_: having focused on the simplified setup and equipped with knowledge on how the compression on the master should be performed, we employ this mechanism in more complex scenarios (see Section 5).

Let us now describe the three possible ways to perform compression on the server.

"Same" compressors.The prevalent approach in downlink compression is to transmit the same update to all workers. To illustrate this, let us call a collection \(\mathcal{C}_{1},\ldots,\mathcal{C}_{n}\) of compressors "SameRand\(K\)" if for all \(i\in[n]\) we have \(\mathcal{C}_{i}=\mathcal{C}\) for some Rand\(K\) compressor \(\mathcal{C}\). Now, consider one iteration \(t\) of MARINA-P with SameRand\(K\) compressor. The server calculates \(\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})\) for \(i\in[n]\), but in this case, \(\mathcal{C}_{1}^{t}(x^{t+1}-x^{t})=\ldots=\mathcal{C}_{n}^{t}(x^{t+1}-x^{t})= \mathcal{C}^{t}(x^{t+1}-x^{t})\). Thus, applying a collection of SameRand\(K\) compressors to some vector \(x\in\mathbb{R}^{d}\) is equivalent to using a single Rand\(K\) compression operator and transmitting the same message \(\mathcal{C}(x)\) to all workers.

Independent Compressors.Rather than setting \(\mathcal{C}_{i}(x)=\mathcal{C}(x)\) for all \(i\in[n]\), one can break the dependency between the messages and allow the compressors to differ. For illustrational purposes, suppose that \(\mathcal{C}_{i},i\in[n]\) are independent Rand\(K\) compressors (Assumption 1.6). Then, applying such a collection of mappings to the vector of interest \(x\in\mathbb{R}^{d}\), one obtains \(n\) distinct and independent sparse vectors \(\mathcal{C}_{1}(x),\ldots,\mathcal{C}_{n}(x)\).

_Remark A.1_.: We are aware of only one method that uses \(n\) distinct compressors in downlink compression, Rand-MCM by Philippenko and Dieuleveut (2021). Given the absence of results in the non-convex case, let us compare the communication complexities of Rand-MCM and M3 under the Polyak-Lojasiewicz condition (Assumption D.9), which holds under strong convexity. In the strongly convex case, the proved iteration complexity of Rand-MCM is

\[\Omega\left(\frac{L_{\max}}{\mu}\left(\omega_{P}^{3/2}+\frac{\omega_{P}\omega_ {D}^{1/2}}{\sqrt{n}}+\frac{\omega_{D}}{n}\right)\log\frac{\delta^{0}}{\varepsilon }\right).\]

Assuming for simplicity that the server and the workers use Rand\(K\) compressors with \(K=\nicefrac{{d}}{{n}}\), this gives the total communication complexity of

\[\Omega\left(\frac{d}{n}\times\frac{L_{\max}}{\mu}\left(\omega_{P}^{3/2}+\frac{ \omega_{P}\omega_{D}^{1/2}}{\sqrt{n}}+\frac{\omega_{D}}{n}\right)\log\frac{ \delta^{0}}{\varepsilon}\right)=\Omega\left(\frac{d\sqrt{n}L_{\max}}{\mu}\log \frac{\delta^{0}}{\varepsilon}\right),\]

which is getting _worse_ as the number of workers \(n\) increases. Meanwhile, by Corollary E.10, the total communication complexity of M3 (where \(\mathcal{C}_{i}^{t}\) are the Perm\(K\) compressors and \(\mathcal{Q}_{i}^{t}\) are independent \(\mathsf{Rand}K\) compressors, both with \(K=d/n\)) under the Polyak-Lojasiewicz condition is

\[\mathcal{O}\left(\left(\frac{dL_{\max}}{n^{1/3}\mu}+\frac{dL_{A}}{\mu}+d\right) \log\frac{\delta^{0}}{\varepsilon}\right).\]

Since \(n\) is typically large, the total communication complexity of \(\mathsf{M3}\) can be much better than that of \(\mathsf{Rand}\mathsf{MCM}\).

Correlated Compressors.In their work, Szlendak et al. (2021) introduce an alternative class of compressors, which satisfy the following condition:

**Definition A.2** (AB-inequality (Szlendak et al., 2021)).: There exist constants \(A,B\geq 0\) such that the random operators \(\mathcal{C}_{1},\ldots\mathcal{C}_{n}\) satisfy

\[\mathbb{E}\left[\mathcal{C}_{i}(x)\right] =x,\] \[\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}\mathcal{C}_{i}( x_{i})-\frac{1}{n}\sum_{i=1}^{n}x_{i}\right\|^{2}\right] \leq A\frac{1}{n}\sum_{i=1}^{n}\left\|x_{i}\right\|^{2}-B\left\| \frac{1}{n}\sum_{i=1}^{n}x_{i}\right\|^{2}\] (16)

for all \(x,x_{1},\ldots,x_{n}\in\mathbb{R}^{d}\). If these conditions hold, we write \(\{\mathcal{C}_{i}\}_{i=1}^{n}\in\mathbb{U}(A,B)\).

Following on this idea, we introduce the concept of a _collection of correlated compressors_.

**Definition A.3** (Collection of Correlated Compressors).: There exists a constant \(\theta\geq 0\) such that the random operators \(\mathcal{C}_{1},\ldots\mathcal{C}_{n}\) satisfy:

\[\mathbb{E}\left[\mathcal{C}_{i}(x)\right] =x\] \[\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}\mathcal{C}_{i}( x)-x\right\|^{2}\right] \leq\theta\left\|x\right\|^{2}\] (17)

for all \(x\in\mathbb{R}^{d}\). If these conditions hold, we write \(\{\mathcal{C}_{i}\}_{i=1}^{n}\in\mathbb{P}(\theta)\).

Definition A.3 will play a key role in our upcoming advancements. But what makes this assumption reasonable?

First, it is easy to note that condition (17) is weaker than (16). Indeed, if \(\left\{\mathcal{C}_{i}\right\}_{i=1}^{n}\in\mathbb{U}(A,B)\), then inequality (17) holds with \(\theta:=A-B\). It turns out that it is in fact strictly weaker, as the following example shows.

_Example A.4_.: Let \(n=2\), \(d=1\). Let \(\{\zeta_{x}:x\in\mathbb{R}\}\) be a collection of independent Cauchy variables indexed by real numbers. Define \(\mathcal{C}_{1}(u)=u+\zeta_{u}\), and \(\mathcal{C}_{2}(u)=u-\zeta_{u}\). Then

\[\frac{1}{2}\left(\mathcal{C}_{1}(u)+\mathcal{C}_{2}(u)\right)=u,\]

so \(\mathcal{C}_{1}(u)\) and \(\mathcal{C}_{2}(u)\) satisfy Definition A.3 with \(\theta=0\). However, for \(u_{1}\neq u_{2}\), by the properties of Cauchy distribution we have

\[\mathbb{E}\left[\left(\frac{1}{2}\left(\mathcal{C}_{1}(u)+\mathcal{ C}_{2}(u)\right)-\frac{1}{2}\left(u_{1}+u_{2}\right)\right)^{2}\right] =\mathbb{E}\left[\left(\frac{1}{2}\left(\zeta_{1}+\zeta_{2}\right) \right)^{2}\right]\] \[=\frac{1}{4}\mathbb{E}\left[\zeta_{1}^{2}+\zeta_{2}^{2}+2\zeta_{ 1}\zeta_{2}\right]=\infty.\]

Thus, \(\mathcal{C}_{1}(u)\) and \(\mathcal{C}_{2}(u)\) do not satisfy Definition A.2.

In fact, the condition specified in Definition A.3 does not impose any restrictions on the compressor class when working with unbiased compressors. This is because, for any set of compressors \(\mathcal{C}_{1},\ldots,\mathcal{C}_{n}\in\mathbb{U}(\omega)\), there exists \(\theta\geq 0\) such that \(\{\mathcal{C}_{i}\}_{i=1}^{n}\in\mathbb{P}(\theta)\), as shown in the following lemma.

**Lemma A.5**.: __

1. _Let_ \(\mathcal{C}_{1},\ldots,\mathcal{C}_{n}\) _be a collection of compressors such that_ \(\mathcal{C}_{i}\in\mathbb{U}(\omega)\) _for all_ \(i\in[n]\)_. Then_ \(\{\mathcal{C}_{i}\}_{i=1}^{n}\in\mathbb{P}(\omega)\)

[MISSING_PAGE_EMPTY:17]

**Definition B.1**.: A stochastic mapping \(\mathcal{C}\,:\,\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) is a _biased compressor_ if there exists \(\alpha\in(0,1]\) such that

\[\mathbb{E}\left[\left\|\mathcal{C}(x)-x\right\|^{2}\right]\leq(1-\alpha)\left\|x \right\|^{2}\quad\forall x\in\mathbb{R}^{d}.\] (18)

The family of such compressors is denoted by \(\mathbb{B}(\alpha)\). It is well-known that if \(\mathcal{C}\in\mathbb{U}(\omega),\) then \((\omega+1)^{-1}\mathcal{C}\in\mathbb{B}\left((\omega+1)^{-1}\right)\), meaning that the family of biased compressors is broader. A canonical example is the Top\(K\in\mathbb{B}(\nicefrac{{K}}{{d}})\) compressor, which preserves the \(K\) largest in magnitude coordinates of the input vector (Beznosikov et al., 2020).

## Appendix C Properties of \(L_{a}\) and \(L_{b}\)

We first prove the results from Section 4, starting with calculating the constants \(L_{A}\) and \(L_{B}\) from Assumption 4.2 in some special cases.

**Theorem 4.4**.: _For all \(i\in[n],\) assume that the functions \(f_{i}\) are \(L_{i}\)-smooth (Assumption 1.5). Then, Assumption 4.2 holds with \(L_{A}=L_{\max}\) and \(L_{B}=0\)._

Proof.: From Assumption 1.5 it follows that

\[\left\|\frac{1}{n}\sum_{i=1}^{n}(\nabla f_{i}(x+u_{i})-\nabla f_{ i}(x))\right\|^{2} \stackrel{{\eqref{eq:C_A_A_A_A_A_A_A_A_A_A_A_A_A_A_A}}}{{ \leq}} \frac{1}{n}\sum_{i=1}^{n}\left\|\nabla f_{i}(x+u_{i})-\nabla f_{i}(x)\right\|^ {2}\] \[\stackrel{{\text{\rm Ass.1.5}}}{{\leq}} \frac{1}{n}\sum_{i=1}^{n}L_{i}^{2}\left\|u_{i}\right\|^{2}\] \[\leq L_{\max}^{2}\left(\frac{1}{n}\sum_{i=1}^{n}\left\|u_{i}\right\|^{ 2}\right),\]

so Assumption 4.2 holds with \(L_{A}=L_{\max}\) and \(L_{B}=0\). 

**Theorem 4.5**.: _For all \(i\in[n],\) assume that the functions \(f_{i}\) are homogeneous quadratics defined in (8). Then, Assumption 4.2 holds with \(L_{A}=0\) and \(L_{B}=\left\|\mathbf{A}\right\|.\)_Proof.: It is easy to verify that

\[\left\|\frac{1}{n}\sum_{i=1}^{n}(\nabla f_{i}(x+u_{i})-\nabla f_{i}( x))\right\|^{2} =\left\|\frac{1}{n}\sum_{i=1}^{n}\left(\mathbf{A}(x+u_{i})+b-( \mathbf{A}x+b)\right)\right\|^{2}\] \[=\left\|\mathbf{A}\left(\frac{1}{n}\sum_{i=1}^{n}u_{i}\right) \right\|^{2}\] \[\leq\left\|\mathbf{A}\right\|^{2}\left\|\frac{1}{n}\sum_{i=1}^{n} u_{i}\right\|^{2},\]

meaning that Assumption 4.2 holds with \(L_{A}=0\) and \(L_{B}=\left\|\mathbf{A}\right\|\). 

**Lemma C.1**.: _Let Assumption 1.5 hold. Then, there exist constants \(L_{A},L_{B}\geq 0\) such that Assumption 4.2 holds and \(L_{A}^{2}+L_{B}^{2}\leq L_{\max}^{2}\)._

Proof.: Assumption 1.5 gives

\[\left\|\frac{1}{n}\sum_{i=1}^{n}(\nabla f_{i}(x+u_{i})-\nabla f_{ i}(x))\right\|^{2} \stackrel{{\eqref{eq:10}}}{{\leq}} \frac{1}{n}\sum_{i=1}^{n}\left\|\nabla f_{i}(x+u_{i})-\nabla f_{i} (x)\right\|^{2}\] \[\stackrel{{\text{\rm Ass.1.5}}}{{\leq}} \frac{1}{n}\sum_{i=1}^{n}L_{i}^{2}\left\|u_{i}\right\|^{2}\] \[\leq L_{\max}^{2}\left(\frac{1}{n}\sum_{i=1}^{n}\left\|u_{i}\right\|^{ 2}\right),\]

and hence Assumption 4.2 holds with \(L_{A}^{2}=L_{\max}^{2}\) and \(L_{B}^{2}=0\). 

_Remark C.2_.: Under Assumption 4.2 we have

\[\left\|\frac{1}{n}\sum_{i=1}^{n}(\nabla f_{i}(x+u_{i})-\nabla f_{ i}(x))\right\|^{2} \stackrel{{\text{\rm Ass.4.2}}}{{\leq}}L_{A}^{2}\left(\frac{1}{ n}\sum_{i=1}^{n}\left\|u_{i}\right\|^{2}\right)+L_{B}^{2}\left\|\frac{1}{n}\sum_{i=1}^{ n}u_{i}\right\|^{2}\] \[\stackrel{{\text{\rm Ass.1.5}}}{{\leq}}\left(L_{A}^ {2}+L_{B}^{2}\right)\left(\frac{1}{n}\sum_{i=1}^{n}\left\|u_{i}\right\|^{2} \right),\]

so, in principle, one could always set \(L_{B}^{2}=0\). However, the bound could be tightened by decreasing \(L_{A}\) and increasing \(L_{B}\). The smaller \(L_{A}\), the better the performance of our algorithms (see Corollaries D.4 and E.3).

Now, we proceed to prove the result that relates the values of \(L_{A}\) and \(L_{B}\) to the Hessians of the functions \(f_{i}\).

**Theorem 4.8**.: _Assume that the functions \(f_{i}\) are twice continuously differentiable, \(L_{i}\)-smooth (Assumption 1.5), and that there exist \(D_{i}\geq 0\) such that_

\[\sup_{z_{1},\ldots,z_{n}\in\mathbb{R}^{4}}\left\|\nabla^{2}f_{i}(z_{i})-\tfrac {1}{n}\sum_{j=1}^{n}\nabla^{2}f_{j}(z_{j})\right\|\leq D_{i}\] (11)

_for all \(i\in[n]\). Then, Assumption 4.2 holds with \(L_{A}=\sqrt{2}\max_{i\in[n]}D_{i}\leq 2\sqrt{2}\max_{i\in[n]}L_{i}\) and \(L_{B}=\sqrt{2}\left(\frac{1}{n}\sum_{i=1}^{n}L_{i}\right).\)_

Proof.: By the fundamental theorem of calculus,

\[\nabla f_{i}(x+u_{i})-\nabla f_{i}(x)=\int_{0}^{1}\nabla^{2}f_{i}(x+tu_{i})u_ {i}dt=\left(\int_{0}^{1}\nabla^{2}f_{i}(x+tu_{i})dt\right)u_{i}=\mathbf{Q}_{i }u_{i},\]

[MISSING_PAGE_EMPTY:20]

_Remark C.3_.: Clearly, if Assumption 1.5 holds, i.e., if there exists \(L_{i}\geq 0\) such that \(\sup_{z_{i}\in\mathbb{R}^{d}}\left\|\nabla^{2}f_{i}(z_{i})\right\|\leq L_{i}\) for all \(i\in[n]\), then there exists \(D_{i}\) such that \(\sup_{z_{1},\ldots,z_{n}\in\mathbb{R}^{d}}\left\|\nabla^{2}f_{i}(z_{i})-\frac{1 }{n}\sum_{j=1}^{n}\nabla^{2}f_{j}(z_{j})\right\|\leq D_{i}\), which means that this latter condition is not restrictive. Indeed,

\[\left\|\nabla^{2}f_{i}(z_{i})-\frac{1}{n}\sum_{j=1}^{n}\nabla^{2} f_{j}(z_{j})\right\| \leq\left\|\nabla^{2}f_{i}(z_{i})\right\|+\left\|\frac{1}{n}\sum_{ j=1}^{n}\nabla^{2}f_{j}(z_{j})\right\|\] \[\leq\left\|\nabla^{2}f_{i}(z_{i})\right\|+\frac{1}{n}\sum_{j=1}^ {n}\left\|\nabla^{2}f_{j}(z_{j})\right\|\] \[\leq L_{i}+\frac{1}{n}\sum_{j=1}^{n}L_{j}.\]

However, \(D_{i}\) can be small even if the constants \(\{L_{i}\}\) are large, as the next theorem shows.

**Theorem 4.9**.: _Assume that \(f_{i}(x)=\frac{1}{2}x^{\top}\mathbf{A}_{i}x+b_{i}^{\top}x+c_{i},\) where \(\mathbf{A}_{i}\in\mathbb{R}^{d\times d}\) is symmetric but not necessarily positive semidefinite, \(b_{i}\in\mathbb{R}^{d}\) and \(c_{i}\in\mathbb{R}\) for \(i\in[n].\) Define \(\mathbf{A}=\frac{1}{n}\sum_{i=1}^{n}\mathbf{A}_{i}.\) Then, Assumption 4.2 holds with \(L_{A}=\sqrt{2}\max_{i\in[n]}\left\|\mathbf{A}_{i}-\mathbf{A}\right\|\) and \(L_{B}=\sqrt{2}\left(\frac{1}{n}\sum_{i=1}^{n}\left\|\mathbf{A}_{i}\right\| \right).\)_

Proof.: In this case \(\nabla^{2}f_{i}(z_{i})\equiv\mathbf{A}_{i}\), and the result easily follows from Theorem 4.8. 

## Appendix D Convergence of Marina-P in the General Case

### Main Results

As promised, we now present a result generalizing Theorem 4.6 to all unbiased compressors.

**Theorem D.1**.: _Let Assumptions 1.1, 1.2 and 4.2 be satisfied and suppose that \(\{\mathcal{C}_{i}^{t}\}_{i=1}^{n}\in\mathbb{P}(\theta)\) (Def. A.3) and \(\mathcal{C}_{i}^{t}\in\mathbb{U}(\omega_{P})\) (Def. 1.4) for all \(i\in[n]\). Let_

\[0<\gamma\leq\frac{1}{L+\sqrt{\left(L_{A}^{2}\omega_{P}+L_{B}^{2} \theta\right)\left(\frac{1}{p}-1\right)}}.\]

_Letting_

\[\Psi^{t}=f(x^{t})-f^{*}+\frac{\gamma L_{A}^{2}}{2p}\frac{1}{n} \sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2}+\frac{\gamma L_{B}^{2}}{2p} \left\|w^{t}-x^{t}\right\|^{2},\]

_for each \(T\geq 1\) we have_

\[\sum_{t=0}^{T-1}\frac{1}{T}\mathbb{E}\left[\left\|\nabla f(x^{t} )\right\|^{2}\right]\leq\frac{2\Psi^{0}}{\gamma T}.\]

Let us provide some important examples:

**Theorem D.2**.: _Let Assumptions 1.1, 1.2 and 4.2 be satisfied. Choose_

\[\gamma=\begin{cases}\left(L+\sqrt{\left(L_{A}^{2}+L_{B}^{2} \right)\nicefrac{{\omega_{P}}}{{p}}}\right)^{-1}&\text{for $\mathsf{ SameRandK}$ compressors}\\ \left(L+\sqrt{\left(L_{A}^{2}+\nicefrac{{L_{B}^{2}}}{{n}}\right)\nicefrac{{ \omega_{P}}}{{p}}}\right)^{-1}&\text{for independent $\mathsf{ RandK}$ compressors}\\ \left(L+L_{A}\sqrt{\nicefrac{{\omega_{P}}}{{p}}}\right)^{-1}&\text{for $\mathsf{ PermK}$ compressors}\end{cases}\]_and set \(w_{i}^{0}=x^{0}\) for all \(i\in[n]\). Then_ MARINA-P _finds an \(\varepsilon\)-stationary point after_

\[\bar{T}=\begin{cases}\mathcal{O}\left(\frac{\delta^{\theta}\left(L+\sqrt{\left(L _{A}^{2}+L_{B}^{2}\right)\nu/p}\right)}{\varepsilon}\right)&\text{for $\mathsf{ SameRandK}$ compressors}\\ \mathcal{O}\left(\frac{\delta^{\theta}\left(L+\sqrt{\left(L_{A}^{2}+L_{B}^{2}/n \right)\nu/p}\right)}{\varepsilon}\right)&\text{for independent $\mathsf{ RandK}$ compressors}\\ \mathcal{O}\left(\frac{\delta^{\theta}\left(L+L_{A}\sqrt{\nu/p}\right)}{ \varepsilon}\right)&\text{for $\mathsf{PermK}$ compressors}\end{cases}\]

_iterations._

_Remark D.3_.:
* The result for \(\mathsf{Perm}K\) compressors proves Theorem 4.6.
* The above theorem demonstrates the complexities for a) \(\mathsf{SameRand}K\), b) independent \(\mathsf{Rand}K\) and c) \(\mathsf{Perm}K\) compressors. However, the result applies to any families of compressors such that for all \(t\geq 0\) we have a) \(\mathcal{C}_{n}^{t}=\ldots=\mathcal{C}_{n}^{t}=\mathcal{C}^{t}\in\mathbb{U}( \omega_{P})\), b) \(\mathcal{C}_{1}^{t},\ldots,\mathcal{C}_{n}^{t}\in\mathbb{U}(\omega_{P})\) are independent, and c) \(\mathcal{C}_{1}^{t},\ldots,\mathcal{C}_{n}^{t}\in\mathbb{U}(\omega_{P})\cap \mathbb{P}(\theta)\), respectively.

We now derive the communication complexities:

**Corollary D.4**.: _Let us take \(p=\nicefrac{{1}}{{n}}\) and set \(K=\nicefrac{{d}}{{n}}\) (corresponding to the sparsification level of a \(\mathsf{Perm}K\) compressor). Then, in the view of Theorem D.2, the average s2w communication complexity of_ MARINA-P _is_

\[\begin{cases}\mathcal{O}\left(\frac{d\delta^{\theta}D}{n\varepsilon}+\frac{d \delta^{0}}{\varepsilon}\sqrt{L_{A}^{2}+L_{B}^{2}}\right)&\text{for $\mathsf{ SameRandK}$ compressors}\\ \mathcal{O}\left(\frac{d\delta^{0}D}{n\varepsilon}+\frac{d\delta^{0}}{ \varepsilon}\sqrt{L_{A}^{2}+\frac{L_{B}^{2}}{n}}\right)&\text{for independent $\mathsf{ RandK}$ compressors}\\ \mathcal{O}\left(\frac{d\delta^{0}D}{n\varepsilon}+\frac{d\delta^{0}}{ \varepsilon}L_{A}\right)&\text{for $\mathsf{PermK}$ compressors}\end{cases}\] (19)

_Remark D.5_.:
* The result for \(\mathsf{Perm}K\) compressors proves Corollary 4.7.
* The key observation from (19) is the dependence on \(L_{A}\) and \(L_{B}\). In particular, if \(L_{A}\approx 0\) (which is the case, e.g., for homogeneous quadratics), the above communication complexities are \[\begin{cases}\mathcal{O}\left(\frac{\delta^{0}}{\varepsilon}d\left(\frac{L}{ n}+L_{B}\right)\right)&\text{for $\mathsf{SameRandK}$ compressors},\\ \mathcal{O}\left(\frac{\delta^{0}}{\varepsilon}d\left(\frac{L}{n}+\frac{L_{B} }{\sqrt{n}}\right)\right)&\text{for independent $\mathsf{ RandK}$ compressors},\\ \mathcal{O}\left(\frac{\delta^{0}}{\varepsilon}d\frac{L}{n}\right)&\text{for $ \mathsf{PermK}$ compressors}.\end{cases}\] Hence, only by sending different messages to different clients, one obtains complexities improving with \(n\). In particular, for \(\mathsf{Perm}K\), the complexity scales linearly with the number of workers.

### Proofs

To prove the results from the previous section, we first establish several identities and inequalities satisfied by the sequences \(\{w_{1}^{t},\ldots,w_{n}^{t}\}_{t\geq 0}\). We start by studying the evolution of the quantity \(\left\|w_{i}^{t}-x^{t}\right\|^{2}\). In what follows, \(\mathbb{E}_{t}\left[\cdot\right]\) denotes the expectation conditioned on the first \(t\) iterations.

**Lemma D.6**.: _Let \(\mathcal{C}_{i}^{t}\in\mathbb{U}(\omega_{P})\) for all \(i\in[n]\). Then_

\[\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|w_{i}^{t+1}-x^{t+1}\right\|^{2 }\right]\leq(1-p)\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|w_{i}^{t}-x^{ t}\right\|^{2}\right]+(1-p)\omega_{P}\mathbb{E}\left[\left\|x^{t+1}-x^{t} \right\|^{2}\right].\]

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

\[=\mathbb{E}\left[\Psi^{t}\right]-\frac{\gamma}{2}\mathbb{E}\left[ \left\|\nabla f(x^{t})\right\|^{2}\right]\] \[\quad-\left(\frac{1}{2\gamma}-\frac{L}{2}-\frac{\gamma}{2p}\left(L _{A}^{2}\omega_{P}+L_{B}^{2}\theta\right)(1-p)\right)\mathbb{E}\left[\left\|x^ {t+1}-x^{t}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\Psi^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right],\]

where in the last line we use the assumption on the step size and Lemma H.2. Summing up the above inequality for \(t=0,1,\ldots,T-1\) and rearranging the terms, we get

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{t})\right\|^{2} \right]\leq\frac{2}{\gamma T}\left(\mathbb{E}\left[\Psi^{0}\right]-\mathbb{E }\left[\Psi^{T}\right]\right)\leq\frac{2\Psi^{0}}{\gamma T}.\]

With the above result, we can establish the iteration and communication complexities of MARINA-P for three different compression schemes described in Appendix A. First, let us prove a result when independent compressors are used.

**Theorem D.8**.: _Let Assumptions 1.1, 1.2 and 4.2 be satisfied and suppose that \(\mathcal{C}_{1}^{t},\ldots,\mathcal{C}_{n}^{t}\) is a collection of independent compressors (Assumption 1.6) such that \(\mathcal{C}_{i}^{t}\in\mathbb{U}(\omega)\) for all \(i\in[n]\), \(t\in\mathbb{N}\). Choose_

\[\gamma=\left(L+\sqrt{\left(L_{A}^{2}+L_{p/n}^{2}\right)\,\omega_{P}/p}\right) ^{-1}\]

_and set \(w_{i}^{0}=x^{0}\) for all \(i\in[n]\). Then MARINA-P finds an \(\varepsilon\)-stationary point after_

\[\bar{T}=\mathcal{O}\left(\frac{\delta^{0}\left(L+\sqrt{\left(L_{A}^{2}+L_{p/n }^{2}\right)\,\omega_{P}/p}\right)}{\varepsilon}\right)\]

_iterations._

Proof.: In view of Theorem D.1, the step size satisfies the inequality

\[\gamma\leq\frac{1}{L+\sqrt{\left(L_{A}^{2}\omega_{P}+L_{B}^{2}\theta\right) \left(\frac{1}{p}-1\right)}}.\]

Since by Lemma A.5, when the compressors are independent we have \(\theta=\nicefrac{{\omega_{P}}}{{n}}\), the algorithm converges in

\[\bar{T} =\frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{\left(L_{A}^{2}\omega _{P}+L_{B}^{2}\frac{\omega_{P}}{n}\right)\left(\frac{1}{p}-1\right)}\right)\] \[=\mathcal{O}\left(\frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{ \frac{\omega_{P}}{p}\left(L_{A}^{2}+\frac{L_{B}^{2}}{n}\right)}\right)\right)\] (22)

iterations. 

**Theorem D.2**.: _Let Assumptions 1.1, 1.2 and 4.2 be satisfied. Choose_

\[\gamma=\begin{cases}\left(L+\sqrt{\left(L_{A}^{2}+L_{B}^{2}\right)\nicefrac{{ \omega_{P}}}{{p}}}\right)^{-1}&\text{for $\mathsf{SameRandK}$ compressors}\\ \left(L+\sqrt{\left(L_{A}^{2}+L_{p/n}^{2}\right)\nicefrac{{\omega_{P}}}{{p} }}\right)^{-1}&\text{for independent $\mathsf{RandK}$ compressors}\\ \left(L+L_{A}\sqrt{\nicefrac{{\omega_{P}}}{{p}}}\right)^{-1}&\text{for $\mathsf{ PermK}$ compressors}\end{cases}\]_and set \(w_{i}^{0}=x^{0}\) for all \(i\in[n]\). Then_ MARINA-P _finds an \(\varepsilon\)-stationary point after_

\[\bar{T}=\begin{cases}\mathcal{O}\left(\frac{\delta^{0}\left(L+\sqrt{\left(L_{A} ^{2}+L_{B}^{2}\right)\nicefrac{{-}}{{p}}}\right)}{\varepsilon}\right)&\text{ for }\mathsf{SameRandK}\text{ compressors}\\ \mathcal{O}\left(\frac{\delta^{0}\left(L+\sqrt{\left(L_{A}^{2}+L_{B}^{2}/n \right)\nicefrac{{-}}{{p}}}\right)}{\varepsilon}\right)&\text{ for independent }\mathsf{ RandK}\text{ compressors}\\ \mathcal{O}\left(\frac{\delta^{0}\left(L+L_{A}\sqrt{\nicefrac{{-}}{{p}}} \right)}{\varepsilon}\right)&\text{ for }\mathsf{PermK}\text{ compressors}\end{cases}\]

_iterations._

Proof.: In view of Theorem D.1, the step size is such that

\[\gamma\leq\frac{1}{L+\sqrt{\left(L_{A}^{2}\omega_{P}+L_{B}^{2}\theta\right) \left(\frac{1}{p}-1\right)}}.\]

We now use Lemma A.6 and substitute the vales of \(\theta\) specific to each compression type.

For \(\mathsf{SameRand}K\), we have \(\theta=\omega_{P}\), so the algorithm converges after

\[\bar{T}=\frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{\left(L_{A}^{2}\omega_{P}+L _{B}^{2}\omega_{P}\right)\left(\frac{1}{p}-1\right)}\right)=\mathcal{O}\left( \frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{\frac{\omega_{P}}{p}\left(L_{A}^{2} +L_{B}^{2}\right)}\right)\right)\] (23)

iterations. Following the same reasoning as in the proof of Theorem D.8, for \(\mathsf{Rand}K\) we have

\[\bar{T}=\frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{\left(L_{A}^{2}\omega_{P}+L _{B}^{2}\frac{\omega_{P}}{n}\right)\left(\frac{1}{p}-1\right)}\right)=\mathcal{ O}\left(\frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{\frac{\omega_{P}}{p}\left(L_{A}^{2} +\frac{L_{B}^{2}}{n}\right)}\right)\right).\] (24)

Finally, for \(\mathsf{Perm}K\) we have \(\theta=0\), so

\[\bar{T}=\frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{L_{A}^{2}\omega_{P}\left( \frac{1}{p}-1\right)}\right)=\mathcal{O}\left(\frac{\Psi^{0}}{\varepsilon} \left(L+L_{A}\sqrt{\frac{\omega_{P}}{p}}\right)\right).\] (25)

The result follows from the fact that \(w_{i}^{0}=x^{0}\) for all \(i\in[n]\). 

**Corollary D.4**.: _Let us take \(p=\nicefrac{{1}}{{n}}\) and set \(K=\nicefrac{{d}}{{n}}\) (corresponding to the sparsification level of a \(\mathsf{Perm}K\) compressor). Then, in the view of Theorem D.2, the average s2w communication complexity of_ MARINA-P _is_

\[\begin{cases}\mathcal{O}\left(\frac{d\delta^{0}L}{n\varepsilon}+\frac{d\delta ^{0}}{\varepsilon}\sqrt{L_{A}^{2}+L_{B}^{2}}\right)&\text{ for }\mathsf{SameRandK}\text{ compressors}\\ \mathcal{O}\left(\frac{d\delta^{0}L}{n\varepsilon}+\frac{d\delta^{0}}{ \varepsilon}\sqrt{L_{A}^{2}+\frac{L_{B}^{2}}{n}}\right)&\text{ for independent }\mathsf{ RandK}\text{ compressors}\\ \mathcal{O}\left(\frac{d\delta^{0}L}{n\varepsilon}+\frac{d\delta^{0}}{ \varepsilon}L_{A}\right)&\text{ for }\mathsf{PermK}\text{ compressors}\end{cases}\] (19)

Proof.: The expected number of floats a server is relaying to each client at each iteration of MARINA-P is

\[pd+(1-p)k=\frac{d}{n}+\frac{n-1}{n}k\leq\frac{2d}{n}.\]

Next, using the results from Lemma A.6, our choice of compressors and parameters gives \(\omega_{P}=\nicefrac{{d}}{{K}}-1=n-1\) in each of the three cases. Hence, substituting \(p=\nicefrac{{1}}{{n}}\) in (23), (24) and (25), we obtain the following server-to-worker communication complexities:1. for SameRand\(K\) compressors: \[\frac{d}{n}\times\frac{\delta^{0}}{\varepsilon}\left(L+\sqrt{\omega_{P }\left(L_{A}^{2}+L_{B}^{2}\right)\left(\frac{1}{p}-1\right)}\right) =\frac{\delta^{0}}{\varepsilon}\left(\frac{d}{n}L+\frac{d}{n} \sqrt{\left(L_{A}^{2}+L_{B}^{2}\right)\left(n-1\right)^{2}}\right)\] \[=\mathcal{O}\left(\frac{\delta^{0}}{\varepsilon}\left(\frac{d}{n} L+d\sqrt{L_{A}^{2}+L_{B}^{2}}\right)\right),\]
2. for Rand\(K\) compressors: \[\frac{d}{n}\times\frac{\delta^{0}}{\varepsilon}\left(L+\sqrt{ \omega_{P}\left(L_{A}^{2}+\frac{L_{B}^{2}}{n}\right)\left(\frac{1}{p}-1 \right)}\right) =\frac{\delta^{0}}{\varepsilon}\left(\frac{d}{n}L+\frac{d}{n} \sqrt{\left(L_{A}^{2}+\frac{L_{B}^{2}}{n}\right)\left(n-1\right)^{2}}\right)\] \[=\mathcal{O}\left(\frac{\delta^{0}}{\varepsilon}\left(\frac{d}{n} L+d\sqrt{L_{A}^{2}+\frac{L_{B}^{2}}{n}}\right)\right),\]
3. for Perm\(K\) compressors: \[\frac{d}{n}\times\frac{\delta^{0}}{\varepsilon}\left(L+\sqrt{L_{ A}^{2}\omega_{P}\left(\frac{1}{p}-1\right)}\right) =\frac{\delta^{0}}{\varepsilon}\left(\frac{d}{n}L+\frac{d}{n}L_{ A}\sqrt{\left(n-1\right)^{2}}\right)\] \[=\mathcal{O}\left(\frac{\delta^{0}}{\varepsilon}\left(\frac{d}{n} L+dL_{A}\right)\right).\]

### Polyak-Lojasiewicz condition

#### d.3.1 Main Results

To complete the theory, we now establish a convergence result for MARINA-P under the Polyak-Lojasiewicz assumption.

**Assumption D.9** (Polyak-Lojasiewicz condition).: The function \(f\) satisfies Polyak-Lojasiewicz (PL) condition with parameter \(\mu\), i.e., for all \(x\in\mathbb{R}^{d}\) there exists \(x^{*}\in\arg\min_{x\in\mathbb{R}^{d}}f(x)\) such that

\[2\mu\left(f(x)-f(x^{*})\right)\leq\left\|\nabla f(x)\right\|^{2}.\] (26)

**Theorem D.10**.: _Let Assumptions 1.1, 1.2, 4.2 and D.9 be satisfied and suppose that \(\left\{\mathcal{C}_{i}^{t}\right\}_{i=1}^{n}\in\mathbb{P}(\theta)\) and \(\mathcal{C}_{i}^{t}\in\mathbb{U}(\omega_{P})\) for all \(i\in[n]\). Take_

\[0<\gamma\leq\min\left\{\frac{1}{L+\sqrt{2\left(L_{A}^{2}\omega_{P}+L_{B}^{2} \theta\right)\left(\frac{1}{p}-1\right)}},\frac{p}{2\mu}\right\}.\] (27)

_Letting_

\[\Psi^{t}=f(x^{t})-f^{*}+\frac{\gamma L_{A}^{2}}{p}\frac{1}{n}\sum_{i=1}^{n} \mathbb{E}\left[\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+\frac{\gamma L_{B}^{ 2}}{p}\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}w_{i}^{t}-x^{t}\right\|^ {2}\right],\] (28)

_for each \(T\geq 1\) we have_

\[\mathbb{E}\left[\Psi^{T}\right]\leq\left(1-\gamma\mu\right)^{T}\Psi^{0}.\]

**Corollary D.11**.: _Let \(\mathcal{C}_{i}^{t}\in\mathbb{P}(0)\) for all \(i\in[n]\) (e.g. Perm\(K\)), choose \(p=1/(\omega_{P}+1)\). Then, in the view of Theorem D.10, Algorithm 1 ensures that \(\mathbb{E}\left[f(x^{T})-f^{*}\right]\leq\varepsilon\) after_

\[\mathcal{O}\left(\max\left\{\frac{L+L_{A}\omega_{P}}{\mu},\omega_{P}+1\right\} \log\frac{\Psi^{0}}{\varepsilon}\right)\]

_iterations._

**Corollary D.12**.: _Let \(\mathcal{C}_{i}^{t}\) be the Perm\(K\) compressors (\(K=\nicefrac{{d}}{{n}}\)). Then, in the view of Corollary D.11, the s2w communication complexity of MARINA-P with Perm\(K\) is_

\[\mathcal{O}\left(\left(\frac{dL}{n\mu}+\frac{dL_{A}}{\mu}+d\right)\log\frac{ \Psi^{0}}{\varepsilon}\right).\]

#### d.3.2 Proofs

**Theorem D.10**.: _Let Assumptions 1.1, 1.2, 4.2 and D.9 be satisfied and suppose that \(\left\{\mathcal{C}_{i}^{t}\right\}_{i=1}^{n}\in\mathbb{P}(\theta)\) and \(\mathcal{C}_{i}^{t}\in\mathbb{U}(\omega_{P})\) for all \(i\in[n]\). Take_

\[0<\gamma\leq\min\left\{\frac{1}{L+\sqrt{2\left(L_{A}^{2}\omega_{P}+L_{B}^{2} \theta\right)\left(\frac{1}{p}-1\right)}},\frac{p}{2\mu}\right\}.\] (27)

_Letting_

\[\Psi^{t}=f(x^{t})-f^{*}+\frac{\gamma L_{A}^{2}}{p}\frac{1}{n}\sum_{i=1}^{n} \mathbb{E}\left[\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+\frac{\gamma L_{B}^ {2}}{p}\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}w_{i}^{t}-x^{t}\right\| ^{2}\right],\] (28)

_for each \(T\geq 1\) we have_

\[\mathbb{E}\left[\Psi^{T}\right]\leq\left(1-\gamma\mu\right)^{T}\Psi^{0}.\]

Proof.: We proceed similarly as in the proof of Theorem D.1. Combining the inequalities in Lemmas D.6 and D.7 gives

\[\frac{\gamma L_{A}^{2}}{p}\frac{1}{n}\sum_{i=1}^{n}\mathbb{E} \left[\left\|w_{i}^{t+1}-x^{t+1}\right\|^{2}\right]+\frac{\gamma L_{B}^{2}}{p }\mathbb{E}\left[\left\|w^{t+1}-x^{t+1}\right\|^{2}\right]\] \[\leq\frac{\gamma L_{A}^{2}}{p}(1-p)\frac{1}{n}\sum_{i=1}^{n} \mathbb{E}\left[\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+\frac{\gamma L_{A}^ {2}}{p}(1-p)\omega_{P}\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[\quad+\frac{\gamma L_{B}^{2}}{p}(1-p)\mathbb{E}\left[\left\|w^{t }-x^{t}\right\|^{2}\right]+\frac{\gamma L_{B}^{2}}{p}(1-p)\theta\mathbb{E} \left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[=\frac{\gamma L_{A}^{2}}{p}(1-p)\frac{1}{n}\sum_{i=1}^{n} \mathbb{E}\left[\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+\frac{\gamma L_{B}^ {2}}{p}(1-p)\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]\] \[\quad+\frac{\gamma}{p}\left(L_{A}^{2}\omega_{P}+L_{B}^{2}\theta \right)(1-p)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right].\] (29)

By adding inequalities (21) and (29), we get

\[\mathbb{E}\left[\Psi^{t+1}\right] = \mathbb{E}\left[\delta^{t+1}\right]+\frac{\gamma L_{A}^{2}}{p} \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|w_{i}^{t+1}-x^{t+1}\right\|^{2 }\right]+\frac{\gamma L_{B}^{2}}{p}\mathbb{E}\left[\left\|w^{t+1}-x^{t+1} \right\|^{2}\right]\] \[\leq \mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}-\frac {L}{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[+\frac{\gamma}{2}\left(L_{A}^{2}\frac{1}{n}\sum_{i=1}^{n} \mathbb{E}\left[\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+L_{B}^{2}\mathbb{E} \left[\left\|w^{t}-x^{t}\right\|^{2}\right]\right)\] \[+\frac{\gamma L_{A}^{2}}{p}(1-p)\frac{1}{n}\sum_{i=1}^{n} \mathbb{E}\left[\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+\frac{\gamma L_{B}^ {2}}{p}(1-p)\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]\] \[+\frac{\gamma}{p}\left(L_{A}^{2}\omega_{P}+L_{B}^{2}\theta\right) (1-p)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[= \mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]\]\[-\left(\frac{1}{2\gamma}-\frac{L}{2}-\frac{\gamma}{p}\left(L_{A}^{2} \omega_{P}+L_{B}^{2}\theta\right)\left(1-p\right)\right)\mathbb{E}\left[\left\|x ^{t+1}-x^{t}\right\|^{2}\right]\] \[+\gamma L_{A}^{2}\left(\frac{1}{p}-\frac{1}{2}\right)\frac{1}{n} \sum_{i=1}^{n}\mathbb{E}\left[\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+\gamma L _{B}^{2}\left(\frac{1}{p}-\frac{1}{2}\right)\mathbb{E}\left[\left\|w^{t}-x^{t} \right\|^{2}\right]\] \[\stackrel{{\text{Ass.D.9},(\ref{eq:2011})}}{{\leq}} \left(1-\gamma\mu\right)\mathbb{E}\left[\delta^{t}\right]+\frac{ \gamma L_{A}^{2}}{p}\left(1-\gamma\mu\right)\frac{1}{n}\sum_{i=1}^{n}\mathbb{ E}\left[\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]\] \[+\frac{\gamma L_{B}^{2}}{p}\left(1-\gamma\mu\right)\mathbb{E} \left[\left\|w^{t}-x^{t}\right\|^{2}\right]\] \[= \left(1-\gamma\mu\right)\mathbb{E}\left[\Psi^{t}\right],\]

where the last inequality follows from the Polyak-Lojasiewicz condition, Lemma H.2 and our choice of \(\gamma\). Applying the above inequality iteratively, we finish the proof. 

**Corollary D.11**.: _Let \(\mathcal{C}_{i}^{t}\in\mathbb{P}(0)\) for all \(i\in[n]\) (e.g. Perm\(K\)), choose \(p=1/(\omega_{P}+1)\). Then, in the view of Theorem D.10, Algorithm 1 ensures that \(\mathbb{E}\left[f(x^{T})-f^{*}\right]\leq\varepsilon\) after_

\[\mathcal{O}\left(\max\left\{\frac{L+L_{A}\omega_{P}}{\mu},\omega_{P}+1\right\} \log\frac{\Psi^{0}}{\varepsilon}\right)\]

_iterations._

Proof.: In view of Theorem D.10, the step size satisfies

\[\gamma\leq\min\left\{\frac{1}{L+\sqrt{2\left(L_{A}^{2}\omega_{P}+L_{B}^{2} \theta\right)\left(\frac{1}{p}-1\right)}},\frac{p}{2\mu}\right\}.\]

Therefore, since \(\theta=0\) and \(p=1/(\omega_{P}+1),\) the algorithm converges after

\[\bar{T} =\max\left\{\frac{L+\sqrt{2\left(L_{A}^{2}\omega_{P}+L_{B}^{2} \theta\right)\left(\frac{1}{p}-1\right)}}{\mu},\frac{2}{p}\right\}\log\frac{ \Psi^{0}}{\varepsilon}\] \[=\mathcal{O}\left(\max\left\{\frac{L+L_{A}\omega_{P}}{\mu},\omega _{P}+1\right\}\log\frac{\Psi^{0}}{\varepsilon}\right)\]

iterations. 

**Corollary D.12**.: _Let \(\mathcal{C}_{i}^{t}\) be the Perm\(K\) compressors (\(K=\nicefrac{{d}}{{n}}\)). Then, in the view of Corollary D.11, the s2w communication complexity of MARINA-P with Perm\(K\) is_

\[\mathcal{O}\left(\left(\frac{dL}{n\mu}+\frac{dL_{A}}{\mu}+d\right)\log\frac{ \Psi^{0}}{\varepsilon}\right).\]

Proof.: For Perm\(K\), \(\omega_{P}=n-1.\) Therefore, the iteration complexity is

\[\mathcal{O}\left(\max\left\{\frac{L+L_{A}n}{\mu},n\right\}\log\frac{\Psi^{0}}{ \varepsilon}\right).\]

Since the expected number of floats the server is relaying to each client is

\[pd+(1-p)k=\frac{d}{n}+\frac{n-1}{n}k\leq\frac{2d}{n},\]

the server-to-worker communication complexity is

\[\mathcal{O}\left(\max\left\{\frac{\frac{d}{n}L+dL_{A}}{\mu},d\right\}\log\frac{ \Psi^{0}}{\varepsilon}\right).\]Convergence of \(\mathsf{M3}\) in the General Case

We now move on to the bidirectionally compressed method. Below is a generalization of Theorem 5.1 to all unbiased compressors.

### Main Results

**Theorem E.1**.: _Let Assumptions 1.1, 1.2, 1.5 and 4.2 hold and suppose that the compressors \(\mathcal{Q}_{i}^{t}\in\mathbb{U}(\omega_{D})\) satisfy Assumption 1.6, \(\left\{\mathcal{C}_{i}^{t}\right\}_{i=1}^{n}\in\mathbb{P}(\theta)\) and \(\mathcal{C}_{i}^{t}\in\mathbb{U}(\omega_{P})\) for all \(i\in[n]\). Let \(\gamma>0\) be such that_

_Letting_

\[\Psi^{t} =\delta^{t}+\kappa\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{ i}(z_{i}^{t})\right\|^{2}+\eta\left\|z^{t}-w^{t}\right\|^{2}+\nu\frac{1}{n} \sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\] \[\quad+\rho\left\|w^{t}-x^{t}\right\|^{2}+\mu\frac{1}{n}\sum_{i=1} ^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2},\]

_where \(\kappa=\frac{\gamma}{\rho_{D}}\), \(\eta=\frac{4\gamma L_{B}^{2}}{\beta}\), \(\nu=\frac{4\gamma L_{B}^{2}}{\beta}+\frac{6\gamma\omega_{D}\beta L_{\max}^{2} }{np_{D}}\), \(\rho=32\gamma L_{B}^{2}\left(\frac{1}{\rho_{P}}+\frac{p_{P}}{\beta^{2}}\right)\) and \(\mu=32\gamma L_{A}^{2}\left(\frac{1}{\rho_{P}}+\frac{p_{P}}{\beta^{2}}\right)+ \frac{48\gamma\omega_{D}L_{\max}^{2}}{np_{D}}\left(\beta+p_{P}\right)\), \(\mathsf{M3}\) ensures that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{t})\right\|^{2} \right]=\mathcal{O}\left(\frac{\Psi^{0}}{\gamma T}\right).\]

We now simplify the above result by considering \(\theta=0\).

**Corollary E.2**.: _Let \(\mathcal{C}_{i}^{t}\in\mathbb{P}(0)\) for all \(i\in[n]\) (e.g. Perm\(K\)), choose \(p_{P}=1/(\omega_{P}+1),\,p_{D}=1/(\omega_{D}+1)\) and_

\[\beta=\min\left\{\left(\frac{n}{\omega_{D}\omega_{P}(\omega_{D}+1)}\right)^{1 /3},1\right\}.\]

_Then, in the view of Theorem E.1, the iteration complexity is_

\[\mathcal{O}\left(\frac{\Psi^{0}}{\varepsilon}\left(L_{\max}+\left(\frac{ \omega_{D}\omega_{P}(\omega_{D}+1)}{n}\right)^{1/3}L_{\max}+\sqrt{\frac{\omega _{D}(\omega_{D}+1)}{n}}L_{\max}+\sqrt{\omega_{P}(\omega_{P}+1)}L_{A}\right) \right).\]

We now give the bound for the total communication complexity of \(\mathsf{M3}\).

**Corollary E.3**.: _Let \(\mathcal{C}_{i}^{t}\) be the Perm\(K\) compressors and \(\mathcal{Q}_{i}^{t}\) be the independent (Assumption 1.6) \(\mathit{Rand}K\) compressors, both with \(K=\nicefrac{{d}}{{n}}\). Then, in the view of Corollary E.2, the iteration complexity is_

\[\mathcal{O}\left(\frac{\Psi^{0}}{\varepsilon}\left(n^{2/3}L_{\max}+nL_{A} \right)\right),\]

_and the total communication complexity is_

\[\mathcal{O}\left(\frac{\Psi^{0}}{\varepsilon}\left(\frac{dL_{\max}}{n^{1/3}} +dL_{A}\right)\right).\]

_Remark E.4_.: The above result proves the complexities from Theorem 5.1.

### Proofs

Similar to our approach from the previous section, we start by establishing several inequalities satisfied by the sequences \(\{w_{1}^{t},\ldots,w_{n}^{t}\}_{t\geq 0}\), \(\{z_{1}^{t},\ldots,z_{n}^{t}\}_{t\geq 0}\) and \(\{g_{1}^{t},\ldots,g_{n}^{t}\}_{t\geq 0}\).

**Lemma E.5**.: _Let \(\mathcal{C}_{i}^{t}\in\mathbb{U}(\omega_{P})\) for all \(i\in[n]\) and \(\left\{\mathcal{C}_{i}^{t}\right\}_{i=1}^{n}\in\mathbb{P}(\theta)\). Then_

\[\mathbb{E}_{t}\left[\left\|w_{i}^{t+1}-z_{i}^{t}\right\|^{2}\right] \leq\left\|(x^{t+1}-x^{t})-p_{P}(w_{i}^{t}-x^{t})+(w_{i}^{t}-z_{i }^{t})\right\|^{2}\] \[\quad+p_{P}\left\|w_{i}^{t}-x^{t}\right\|^{2}+\omega_{P}\left\|x^ {t+1}-x^{t}\right\|^{2}\]

_for all \(i\in[n],\) and_

\[\mathbb{E}_{t}\left[\left\|w^{t+1}-z^{t}\right\|^{2}\right] \leq\left\|(x^{t+1}-x^{t})-p_{P}(w^{t}-x^{t})+(w^{t}-z^{t}) \right\|^{2}\] \[\quad+p_{P}\left\|w^{t}-x^{t}\right\|^{2}+\theta\left\|x^{t+1}-x^ {t}\right\|^{2}.\]

Proof.: Using the definition of \(w_{i}^{t+1}\), we have

\[\mathbb{E}_{t}\left[w_{i}^{t+1}\right]=x^{t+1}+(1-p_{P})(w_{i}^{t}-x^{t})\]

and hence

\[\mathbb{E}_{t}\left[\left\|w_{i}^{t+1}-z_{i}^{t}\right\|^{2}\right] \stackrel{{\eqref{eq:p_P}}}{{=}} \left\|x^{t+1}+(1-p_{P})(w_{i}^{t}-x^{t})-z_{i}^{t}\right\|^{2}\] \[\quad+\mathbb{E}_{t}\left[\left\|w_{i}^{t+1}-(x^{t+1}+(1-p_{P})(w _{i}^{t}-x^{t}))\right\|^{2}\right]\] \[=\] \[\quad+\mathbb{E}_{t}\left[\left\|w_{i}^{t+1}-(x^{t+1}+(1-p_{P})(w _{i}^{t}-x^{t}))\right\|^{2}\right].\]

Using the definition of \(w_{i}^{t+1}\) again, we get

\[\mathbb{E}_{t}\left[\left\|w_{i}^{t+1}-z_{i}^{t}\right\|^{2}\right]\]\[= \left\|(x^{t+1}-x^{t})-p_{P}(w^{t}_{i}-x^{t})+(w^{t}_{i}-z^{t}_{i}) \right\|^{2}\] \[+p_{P}\left\|x^{t+1}-(x^{t+1}+(1-p_{P})(w^{t}_{i}-x^{t}))\right\|^{2}\] \[+(1-p_{P})\mathbb{E}_{t}\left[\left\|w^{t}_{i}+\mathcal{C}^{t}_{i} (x^{t+1}-x^{t})-(x^{t+1}+(1-p_{P})(w^{t}_{i}-x^{t}))\right\|^{2}\right]\] \[= \left\|(x^{t+1}-x^{t})-p_{P}(w^{t}_{i}-x^{t})+(w^{t}_{i}-z^{t}_{i} )\right\|^{2}+p_{P}(1-p_{P})^{2}\left\|w^{t}_{i}-x^{t}\right\|^{2}\] \[+(1-p_{P})\mathbb{E}_{t}\left[\left\|\mathcal{C}^{t}_{i}(x^{t+1}- x^{t})-(x^{t+1}-x^{t})+p_{P}(w^{t}_{i}-x^{t})\right\|^{2}\right]\] \[\stackrel{{\text{(\ref{eq:def1})}}}{{=}} \left\|(x^{t+1}-x^{t})-p_{P}(w^{t}_{i}-x^{t})+(w^{t}_{i}-z^{t}_{i} )\right\|^{2}+p_{P}(1-p_{P})^{2}\left\|w^{t}_{i}-x^{t}\right\|^{2}\] \[+(1-p_{P})p_{P}^{2}\left\|w^{t}_{i}-x^{t}\right\|^{2}+(1-p_{P}) \mathbb{E}_{t}\left[\left\|\mathcal{C}^{t}_{i}(x^{t+1}-x^{t})-(x^{t+1}-x^{t}) \right\|^{2}\right]\] \[\stackrel{{\text{Def.1.4}}}{{\leq}} \left\|(x^{t+1}-x^{t})-p_{P}(w^{t}_{i}-x^{t})+(w^{t}_{i}-z^{t}_{i} )\right\|^{2}+p_{P}\left\|w^{t}_{i}-x^{t}\right\|^{2}+\omega_{P}\left\|x^{t+1} -x^{t}\right\|^{2}.\]

Using the same reasoning, we now prove the second inequality:

\[\mathbb{E}_{t}\left[\left\|w^{t+1}-z^{t}\right\|^{2}\right]\] \[\stackrel{{\text{(\ref{eq:def1})}}}{{=}} \left\|x^{t+1}+(1-p_{P})(w^{t}-x^{t})-z^{t}\right\|^{2}+\mathbb{E}_ {t}\left[\left\|w^{t+1}-(x^{t+1}+(1-p_{P})(w^{t}-x^{t}))\right\|^{2}\right]\] \[= \left\|(x^{t+1}-x^{t})-p_{P}(w^{t}-x^{t})+(w^{t}-z^{t})\right\|^{2}\] \[= \left\|(x^{t+1}-x^{t})-p_{P}(w^{t}-x^{t})+(w^{t}-z^{t})\right\|^{2 }+p_{P}(1-p_{P})^{2}\left\|w^{t}-x^{t}\right\|^{2}\] \[+(1-p_{P})\mathbb{E}_{t}\left[\left\|w^{t}+\frac{1}{n}\sum_{i=1}^{ n}\mathcal{C}^{t}_{i}(x^{t+1}-x^{t})-(x^{t+1}+(1-p_{P})(w^{t}-x^{t}))\right\|^{2}\right]\] \[= \left\|(x^{t+1}-x^{t})-p_{P}(w^{t}-x^{t})+(w^{t}-z^{t})\right\|^{2 }+p_{P}(1-p_{P})^{2}\left\|w^{t}-x^{t}\right\|^{2}\] \[+(1-p_{P})\mathbb{E}_{t}\left[\left\|\frac{1}{n}\sum_{i=1}^{n} \mathcal{C}^{t}_{i}(x^{t+1}-x^{t})-(x^{t+1}-x^{t})+p_{P}(w^{t}-x^{t})\right\|^ {2}\right]\] \[\stackrel{{\text{(\ref{eq:def1})}}}{{=}} \left\|(x^{t+1}-x^{t})-p_{P}(w^{t}-x^{t})+(w^{t}-z^{t})\right\|^{2 }+p_{P}(1-p_{P})^{2}\left\|w^{t}-x^{t}\right\|^{2}\] \[+(1-p_{P})\mathbb{E}_{t}\left[\left\|\frac{1}{n}\sum_{i=1}^{n} \mathcal{C}^{t}_{i}(x^{t+1}-x^{t})-(x^{t+1}-x^{t})\right\|^{2}\right]+p_{P}^{2 }(1-p_{P})\left\|w^{t}-x^{t}\right\|^{2}\] \[\stackrel{{\text{Def.4.3}}}{{\leq}} \left\|(x^{t+1}-x^{t})-p_{P}(w^{t}-x^{t})+(w^{t}-z^{t})\right\|^{2}+p_{P} \left\|w^{t}-x^{t}\right\|^{2}+\theta\left\|x^{t+1}-x^{t}\right\|^{2}.\]

**Lemma E.6**.: _Let Assumption 1.5 hold. Furthermore, suppose that the compressors \(\mathcal{Q}^{t}_{i}\in\mathbb{U}(\omega_{D})\) satisfy Assumption 1.6 and that \(\mathcal{C}^{t}_{i}\in\mathbb{U}(\omega_{P})\) for \(i\in[n]\). Then_

\[\mathbb{E}\left[\left\|g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f _{i}(z^{t+1}_{i})\right\|^{2}\right]\] \[\leq\frac{\omega_{D}L^{2}_{\max}}{n}\left(4p_{P}\beta^{2}\mathbb{E }\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w^{t}_{i}-x^{t}\right\|^{2}\right]+3 \beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z^{t}_{i}-w^{t}_{i} \right\|^{2}\right]+3(\omega_{P}+1)\beta^{2}\mathbb{E}\left[\left\|x^{t+1}-x^{ t}\right\|^{2}\right]\right)\] \[\quad+(1-p_{D})\mathbb{E}\left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n} \nabla f_{i}(z^{t}_{i})\right\|^{2}\right].\]Proof.: First, from the definition of \(g_{i}^{t+1}\), we get

\[\mathbb{E}\left[\left\|g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i} (z_{i}^{t+1})\right\|^{2}\right]\] \[\qquad=(1-p_{D})\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n} \left(g_{i}^{t}+\mathcal{Q}_{i}^{t}(\nabla f_{i}(z_{i}^{t+1})-\nabla f_{i}(z_{ i}^{t}))-\nabla f_{i}(z_{i}^{t+1})\right)\right\|^{2}\right]\]

and hence

\[\mathbb{E}\left[\left\|g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f _{i}(z_{i}^{t+1})\right\|^{2}\right]\] \[\overset{\eqref{eq:g_1}}{=} (1-p_{D})\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n} \mathcal{Q}_{i}^{t}\left(\nabla f_{i}(z_{i}^{t+1})-\nabla f_{i}(z_{i}^{t}) \right)-\frac{1}{n}\sum_{i=1}^{n}\left(\nabla f_{i}(z_{i}^{t+1})-\nabla f_{i}( z_{i}^{t})\right)\right\|^{2}\right]\] \[+(1-p_{D})\mathbb{E}\left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n} \nabla f_{i}(z_{i}^{t})\right\|^{2}\right]\] \[\overset{\text{Def.\ref{eq:g_1},\eqref{eq:g_1}}}{\leq} \frac{\omega_{D}}{n}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\| \nabla f_{i}(z_{i}^{t+1})-\nabla f_{i}(z_{i}^{t})\right\|^{2}\right]+(1-p_{D} )\mathbb{E}\left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t}) \right\|^{2}\right]\] \[\overset{\text{Ass.\ref{eq:g_1}}}{\leq} \frac{\omega_{D}L_{\max}^{2}}{n}\mathbb{E}\left[\frac{1}{n}\sum_{i= 1}^{n}\left\|z_{i}^{t+1}-z_{i}^{t}\right\|^{2}\right]+(1-p_{D})\mathbb{E} \left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t})\right\|^{2 }\right].\] (30)

Let us consider the first term separately:

\[\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}-z_{i }^{t}\right\|^{2}\right] =\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|\beta w_{i}^{t+ 1}+(1-\beta)z_{i}^{t}-z_{i}^{t}\right\|^{2}\right]\] \[=\beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{ t+1}-z_{i}^{t}\right\|^{2}\right].\]

Using the result from Lemma E.5, we have

\[\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}-z_{i }^{t}\right\|^{2}\right]\] \[\leq\,\beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|(x ^{t+1}-x^{t})-p_{P}(w_{i}^{t}-x^{t})+(w_{i}^{t}-z_{i}^{t})\right\|^{2}\right]\] \[\qquad+\beta^{2}\mathbb{E}\left[p_{P}\frac{1}{n}\sum_{i=1}^{n} \left\|w_{i}^{t}-x^{t}\right\|^{2}+\omega_{P}\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[\overset{\eqref{eq:g_1}}{\leq}\beta^{2}\mathbb{E}\left[\frac{3}{ n}\sum_{i=1}^{n}\left\|w_{i}^{t}-z_{i}^{t}\right\|^{2}+4p_{P}\frac{1}{n}\sum_{i=1}^{n} \left\|w_{i}^{t}-x^{t}\right\|^{2}+(\omega_{P}+3)\left\|x^{t+1}-x^{t}\right\|^ {2}\right],\]

where in the last line we use the fact that \(p_{P}\leq 1\). It remains to substitute the above inequality in (30). 

**Lemma E.7**.: _Let \(\mathcal{C}_{i}^{t}\in\mathbb{U}(\omega_{P})\) for all \(i\in[n]\) and \(\left\{\mathcal{C}_{i}^{t}\right\}_{i=1}^{n}\in\mathbb{P}(\theta)\). Then_

\[\mathbb{E}\left[\left\|z_{i}^{t+1}-w_{i}^{t+1}\right\|^{2}\right] \leq\left(1-\frac{\beta}{2}\right)\mathbb{E}\left[\left\|z_{i}^{t }-w_{i}^{t}\right\|^{2}\right]+4\left(\frac{1}{\beta}+\omega_{P}\right) \mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[\quad+4p_{P}\left(1+\frac{p_{P}}{\beta}\right)\mathbb{E}\left[ \left\|w_{i}^{t}-x^{t}\right\|^{2}\right]\]_for all \(i\in[n]\), and_

\[\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\quad+4p_{P}\left(1+\frac{p_{P}}{\beta}\right)\mathbb{E}\left[ \left\|w^{t}-x^{t}\right\|^{2}\right].\]

Proof.: From the definition of \(z_{i}^{t+1}\), we get

\[\mathbb{E}\left[\left\|z_{i}^{t+1}-w_{i}^{t+1}\right\|^{2}\right]=\mathbb{E} \left[\left\|\beta w_{i}^{t+1}+(1-\beta)z_{i}^{t}-w_{i}^{t+1}\right\|^{2} \right]=(1-\beta)^{2}\mathbb{E}\left[\left\|w_{i}^{t+1}-z_{i}^{t}\right\|^{2} \right].\]

Then, Lemma E.5 gives

\[\mathbb{E}\left[\left\|z_{i}^{t+1}-w_{i}^{t+1}\right\|^{2}\right]\] \[\leq (1-\beta)^{2}\mathbb{E}\left[\left\|(x^{t+1}-x^{t})-p_{P}(w_{i}^ {t}-x^{t})+(w_{i}^{t}-z_{i}^{t})\right\|^{2}\right]\] \[+(1-\beta)^{2}\mathbb{E}\left[p_{P}\left\|w_{i}^{t}-x^{t}\right\| ^{2}+\omega_{P}\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[\overset{\eqref{eq:2.1},\eqref{eq:2.2}}{\leq}\] \[+4p_{P}\left(1+\frac{p_{P}}{\beta}\right)\mathbb{E}\left[\left\|w _{i}^{t}-x^{t}\right\|^{2}\right].\]

The second inequality is proved almost in the same way. First,

\[\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]=(1-\beta)^{2} \mathbb{E}\left[\left\|w^{t+1}-z^{t}\right\|^{2}\right],\]

and using Lemma E.5, we obtain

\[\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\leq\] \[\overset{\eqref{eq:2.1},\eqref{eq:2.2}}{\leq}\] \[\leq\]

[MISSING_PAGE_FAIL:35]

\[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}-\frac{L}{ 2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+\gamma\mathbb{ E}\left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t})\right\|^{2}\right]\] \[\quad+2\gamma L_{A}^{2}\frac{1}{n}\sum_{i=1}^{n}\left(\mathbb{E} \left[\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]+\mathbb{E}\left[\left\|w_{ i}^{t}-x^{t}\right\|^{2}\right]\right)\] \[\quad+2\gamma L_{B}^{2}\left(\mathbb{E}\left[\left\|z^{t}-w^{t} \right\|^{2}\right]+\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]\right)\] \[\quad+\kappa\Bigg{(}\frac{\omega_{D}L_{\max}^{2}}{n}\Bigg{(}4p_{ P}\beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t} \right\|^{2}\right]+3\beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\| z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]\] \[\quad\quad\quad\quad\quad\quad\quad\quad\quad+3(\omega_{P}+1) \beta^{2}\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\Bigg{)}\Bigg{)}\] \[\quad+\eta\Bigg{(}\left(1-\frac{\beta}{2}\right)\mathbb{E}\left[ \left\|z^{t}-w^{t}\right\|^{2}\right]+4\left(\frac{1}{\beta}+\theta\right) \mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[\quad\quad+4p_{P}\left(1+\frac{p_{P}}{\beta}\right)\mathbb{E} \left[\left\|w^{t}-x^{t}\right\|^{2}\right]\Bigg{)}\] \[\quad+\nu\Bigg{(}\left(1-\frac{\beta}{2}\right)\mathbb{E}\left[ \frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]+4 \left(\frac{1}{\beta}+\omega_{P}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t} \right\|^{2}\right]\] \[\quad\quad+4p_{P}\left(1+\frac{p_{P}}{\beta}\right)\mathbb{E} \left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]\Bigg{)}\] \[\quad+\rho(1-p_{P})\left(\mathbb{E}\left[\left\|w^{t}-x^{t} \right\|^{2}\right]+\theta\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2} \right]\right)\] \[\quad+\mu(1-p_{P})\left(\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n} \left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+\omega_{P}\mathbb{E}\left[\left\|x^ {t+1}-x^{t}\right\|^{2}\right]\right).\]

Taking \(\kappa=\frac{\gamma}{p_{D}}\) and \(\eta=\frac{4\gamma L_{B}^{2}}{\beta}\), we get \(\gamma+\kappa(1-p_{D})=\kappa\) and \(2\gamma L_{B}^{2}+\eta(1-\nicefrac{{\beta}}{{2}})=\eta\), which gives

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+\eta \mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\quad+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t +1}-w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1 }\right\|^{2}\right]+\mu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i }^{t+1}-x^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}-\frac{L }{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+\kappa \mathbb{E}\left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t}) \right\|^{2}\right]\] \[\quad+\eta\mathbb{E}\left[\left\|z^{t}-w^{t}\right\|^{2}\right]+ 2\gamma L_{A}^{2}\frac{1}{n}\sum_{i=1}^{n}\left(\mathbb{E}\left[\left\|z_{i}^{t }-w_{i}^{t}\right\|^{2}\right]+\mathbb{E}\left[\left\|w_{i}^{t}-x^{t}\right\|^ {2}\right]\right)+2\gamma L_{B}^{2}\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]\] \[\quad+\frac{\gamma}{p_{D}}\Bigg{(}\frac{\omega_{D}L_{\max}^{2}}{n} \Bigg{(}4p_{P}\beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{ t}-x^{t}\right\|^{2}\right]+3\beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n} \left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]\] \[\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad+3(\omega_{ P}+1)\beta^{2}\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\Bigg{)}\Bigg{)}\] \[\quad+\frac{4\gamma L_{B}^{2}}{\beta}\left(4\left(\frac{1}{\beta}+ \theta\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+4p_{P} \left(1+\frac{p_{P}}{\beta}\right)\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2} \right]\right)\]\[+\nu\Bigg{(}\left(1-\frac{\beta}{2}\right)\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]+4\left(\frac{1}{ \beta}+\omega_{P}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[\qquad+4p_{P}\left(1+\frac{p_{P}}{\beta}\right)\mathbb{E}\left[ \frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]\Bigg{)}\] \[+\rho\left((1-p_{P})\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{ 2}\right]+\theta\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\right)\] \[+\mu\left((1-p_{P})\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n} \left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+\omega_{P}\mathbb{E}\left[\left\|x^ {t+1}-x^{t}\right\|^{2}\right]\right).\]

We rearrange the terms to obtain

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}-\frac{ L}{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+\kappa\mathbb{E} \left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t})\right\|^{2 }\right]\] \[\quad+\eta\mathbb{E}\left[\left\|z^{t}-w^{t}\right\|^{2}\right]+ \left(\nu\left(1-\frac{\beta}{2}\right)+2\gamma L_{A}^{2}+\frac{3\gamma\omega _{D}\beta^{2}L_{\max}^{2}}{np_{D}}\right)\mathbb{E}\left[\frac{1}{n}\sum_{i=1} ^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]\] \[\quad+\left(\frac{3\gamma\omega_{D}(\omega_{P}+1)\beta^{2}L_{\max }^{2}}{np_{D}}+\rho\theta+\frac{16\gamma L_{B}^{2}}{\beta}\left(\frac{1}{\beta }+\theta\right)\right.\] \[\qquad+4\nu\left(\frac{1}{\beta}+\omega_{P}\right)+\mu\omega_{P} \Bigg{)}\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[\quad+\left(\rho(1-p_{P})+2\gamma L_{B}^{2}+\frac{16\gamma L_{B}^ {2}p_{P}}{\beta}\left(1+\frac{p_{P}}{\beta}\right)\right)\mathbb{E}\left[ \left\|w^{t}-x^{t}\right\|^{2}\right]\] \[\quad+\left(\mu(1-p_{P})+2\gamma L_{A}^{2}+4\nu p_{P}\left(1+ \frac{p_{P}}{\beta}\right)+\frac{4\gamma\omega_{D}p_{P}\beta^{2}L_{\max}^{2} }{np_{D}}\right)\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t} \right\|^{2}\right].\]

We now consider the coefficient of the term \(\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]\). Using the inequality \(xy\leq\frac{x^{2}+y^{2}}{2}\) for all \(x,y\geq 0\), we get

\[\rho(1-p_{P})+2\gamma L_{B}^{2}+\frac{16\gamma L_{B}^{2}p_{P}}{ \beta}\left(1+\frac{p_{P}}{\beta}\right) \leq\rho(1-p_{P})+16\gamma L_{B}^{2}\left(1+\frac{p_{P}}{\beta}+ \frac{p_{P}^{2}}{\beta^{2}}\right)\] \[\leq\rho(1-p_{P})+32\gamma L_{B}^{2}\left(1+\frac{p_{P}^{2}}{\beta ^{2}}\right)\] \[=\rho\]

for \(\rho=32\gamma L_{B}^{2}\left(\frac{1}{p_{P}}+\frac{p_{P}}{\beta^{2}}\right).\) With this choice of \(\rho\), we obtain

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\quad+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{ t+1}-w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1} \right\|^{2}\right]+\mu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{ t+1}-x^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}-\frac{ L}{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+\kappa \mathbb{E}\left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t}) \right\|^{2}\right]\]\[+\eta\mathbb{E}\left[\left\|z^{t}-w^{t}\right\|^{2}\right]+\rho \mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]\] \[+\left(\nu\left(1-\frac{\beta}{2}\right)+2\gamma L_{A}^{2}+\frac{3 \gamma\omega_{D}\beta^{2}L_{\max}^{2}}{np_{D}}\right)\mathbb{E}\left[\frac{1} {n}\sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]\] \[+\left(\frac{3\gamma\omega_{D}(\omega_{P}+1)\beta^{2}L_{\max}^{2} }{np_{D}}+32\gamma L_{B}^{2}\left(\frac{1}{p_{P}}+\frac{p_{P}}{\beta^{2}} \right)\theta+\frac{16\gamma L_{B}^{2}}{\beta}\left(\frac{1}{\beta}+\theta\right)\right.\] \[\qquad+4\nu\left(\frac{1}{\beta}+\omega_{P}\right)+\mu\omega_{P} \right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[+\left(\mu(1-p_{P})+2\gamma L_{A}^{2}+4\nu p_{P}\left(1+\frac{p _{P}}{\beta}\right)+\frac{4\gamma\omega_{D}p_{P}\beta^{2}L_{\max}^{2}}{np_{D} }\right)\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\| ^{2}\right].\]

Next, taking \(\nu=\frac{4\gamma L_{A}^{2}}{\beta}+\frac{6\gamma\omega_{D}\beta L_{\max}^{2}} {np_{D}}\) gives

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}-w _{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1} \right\|^{2}\right]+\mu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^ {t+1}-x^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}-\frac {L}{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+\kappa \mathbb{E}\left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t}) \right\|^{2}\right]\] \[+\eta\mathbb{E}\left[\left\|z^{t}-w^{t}\right\|^{2}\right]+\rho \mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]+\nu\mathbb{E}\left[ \frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]\] \[+\left(\frac{3\gamma\omega_{D}(\omega_{P}+1)\beta^{2}L_{\max}^{2 }}{np_{D}}+32\gamma L_{B}^{2}\left(\frac{1}{p_{P}}+\frac{p_{P}}{\beta^{2}} \right)\theta+\frac{16\gamma L_{B}^{2}}{\beta}\left(\frac{1}{\beta}+\theta\right)\right.\] \[\qquad+4\left(\frac{4\gamma L_{A}^{2}}{\beta}+\frac{6\gamma \omega_{D}\beta L_{\max}^{2}}{np_{D}}\right)\left(\frac{1}{\beta}+\omega_{P} \right)+\mu\omega_{P}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[+\left(\mu(1-p_{P})+2\gamma L_{A}^{2}+4\left(\frac{4\gamma L_{A} ^{2}}{\beta}+\frac{6\gamma\omega_{D}\beta L_{\max}^{2}}{np_{D}}\right)p_{P} \left(1+\frac{p_{P}}{\beta}\right)\right.\] \[\qquad+\left.\frac{4\gamma\omega_{D}p_{P}\beta^{2}L_{\max}^{2}}{ np_{D}}\right)\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t} \right\|^{2}\right].\]

Let us consider the last bracket:

\[\mu(1-p_{P})+2\gamma L_{A}^{2}+4\left(\frac{4\gamma L_{A}^{2}}{ \beta}+\frac{6\gamma\omega_{D}\beta L_{\max}^{2}}{np_{D}}\right)p_{P}\left(1+ \frac{p_{P}}{\beta}\right)+\frac{4\gamma\omega_{D}p_{P}\beta^{2}L_{\max}^{2}}{ np_{D}}\] \[=\mu(1-p_{P})+2\gamma L_{A}^{2}+\frac{16\gamma p_{P}L_{A}^{2}}{ \beta}+\frac{24\gamma\omega_{D}p_{P}\beta L_{\max}^{2}}{np_{D}}+\frac{16 \gamma p_{P}^{2}L_{A}^{2}}{\beta^{2}}\] \[\qquad+\frac{24\gamma\omega_{D}p_{P}^{2}L_{\max}^{2}}{np_{D}}+ \frac{4\gamma\omega_{D}p_{P}\beta^{2}L_{\max}^{2}}{np_{D}}\] \[\leq\mu(1-p_{P})+16\gamma L_{A}^{2}\left(1+\frac{p_{P}}{\beta}+ \frac{p_{P}^{2}}{\beta^{2}}\right)+\frac{24\gamma\omega_{D}p_{P}L_{\max}^{2}}{ np_{D}}\left(\beta+p_{P}+\beta^{2}\right)\] \[\leq\mu(1-p_{P})+32\gamma L_{A}^{2}\left(1+\frac{p_{P}^{2}}{\beta^ {2}}\right)+\frac{48\gamma\omega_{D}p_{P}L_{\max}^{2}}{np_{D}}\left(\beta+p_{ P}\right)\] \[=\mu\]

[MISSING_PAGE_FAIL:39]

and for \(L^{2}_{\max}\) we obtain

\[\frac{3\gamma\omega_{D}(\omega_{P}+1)\beta^{2}}{np_{D}}+\frac{24\gamma \omega_{D}}{np_{D}}+\frac{24\gamma\omega_{D}\omega_{P}\beta}{np_{D}}+\frac{48 \gamma\omega_{D}\omega_{P}\beta}{np_{D}}+\frac{48\gamma\omega_{D}\omega_{P}pp_{ P}}{np_{D}}\] \[\leq 72\gamma\omega_{D}\left(\frac{(\omega_{P}+1)\beta^{2}}{np_{D }}+\frac{1}{np_{D}}+\frac{\omega_{P}\beta}{np_{D}}+\frac{\omega_{P}p_{P}}{np_{D }}\right)\] \[\leq 144\gamma\omega_{D}\left(\frac{1}{np_{D}}+\frac{\omega_{P} \beta}{np_{D}}+\frac{\omega_{P}p_{P}}{np_{D}}\right)\] \[=144\gamma\left(\frac{\omega_{D}\omega_{P}\beta}{np_{D}}+\frac{ \omega_{D}(1+\omega_{P}p_{P})}{np_{D}}\right)\]

since \(\frac{(\omega_{P}+1)\beta^{2}}{np_{D}}\leq\frac{1}{np_{D}}+\frac{\omega_{P} \beta}{np_{D}}\). Substituting these inequalities to (31) and (32), we get

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\quad+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+ 1}-w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1} \right\|^{2}\right]+\mu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^ {t+1}-x^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}-\frac{ L}{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+\kappa \mathbb{E}\left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t}) \right\|^{2}\right]\] \[\quad+\eta\mathbb{E}\left[\left\|z^{t}-w^{t}\right\|^{2}\right]+ \nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\| ^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]+\mu \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]\] \[\quad+144\gamma\Bigg{(}\left(\frac{\theta}{p_{P}}+\frac{1+\theta p _{P}}{\beta^{2}}\right)L_{B}^{2}+\left(\frac{\omega_{P}}{p_{P}}+\frac{1+\omega _{P}p_{P}}{\beta^{2}}\right)L_{A}^{2}\] \[\quad\quad\quad\quad+\left(\frac{\omega_{D}\omega_{P}\beta}{np_{D} }+\frac{\omega_{D}(1+\omega_{P}p_{P})}{np_{D}}\right)L_{\max}^{2}\Bigg{)} \mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right].\]

By collecting all the terms w.r.t. \(\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right],\) using the step size \(\gamma\) from the theorem and Lemma H.2, we obtain

\[\mathbb{E}\left[\Psi^{t+1}\right] =\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2} \right]+\eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\quad+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t +1}-w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1} \right\|^{2}\right]\] \[\quad+\mu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{ t+1}-x^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]+\kappa\mathbb{E}\left[\left\|g^{ t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t})\right\|^{2}\right]+\eta \mathbb{E}\left[\left\|z^{t}-w^{t}\right\|^{2}\right]\] \[\quad+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t }-w_{i}^{t}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2 }\right]+\mu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t} \right\|^{2}\right]\] \[=\mathbb{E}\left[\Psi^{t}\right]-\frac{\gamma}{2}\mathbb{E}\left[ \left\|\nabla f(x^{t})\right\|^{2}\right].\]

It remains to rearrange and sum the last inequality for \(t=0,\ldots,T-1\)

**Corollary E.2**.: _Let \(\mathcal{C}_{i}^{t}\in\mathbb{P}(0)\) for all \(i\in[n]\) (e.g. Perm\(K\)), choose \(p_{P}=1/(\omega_{P}+1),\)\(p_{D}=1/(\omega_{D}+1)\) and_

\[\beta=\min\left\{\left(\frac{n}{\omega_{D}\omega_{P}(\omega_{D}+1)}\right)^{1/ 3},1\right\}.\]

_Then, in the view of Theorem E.1, the iteration complexity is_

\[\mathcal{O}\left(\frac{\Psi^{0}}{\varepsilon}\left(L_{\max}+\left(\frac{ \omega_{D}\omega_{P}(\omega_{D}+1)}{n}\right)^{1/3}L_{\max}+\sqrt{\frac{ \omega_{D}(\omega_{D}+1)}{n}}L_{\max}+\sqrt{\omega_{P}(\omega_{P}+1)}L_{A} \right)\right).\]

Proof.: By Theorem E.1, up to a constant factor, the algorithm converges after

\[\bar{T} :=\frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{\left(\frac{\theta}{ p_{P}}+\frac{1+\theta p_{P}}{\beta^{2}}\right)L_{B}^{2}+\left(\frac{\omega_{P}}{ p_{P}}+\frac{1+\omega_{P}p_{P}}{\beta^{2}}\right)L_{A}^{2}+\left(\frac{\omega_{D} \omega_{P}\beta}{np_{D}}+\frac{\omega_{D}(1+\omega_{P}p_{P})}{np_{D}}\right)L _{\max}^{2}}\right)\] \[=\frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{\frac{1}{\beta^{2}}L_ {B}^{2}+\left(\frac{\omega_{P}}{p_{P}}+\frac{1+\omega_{P}pp_{P}}{\beta^{2}} \right)L_{A}^{2}+\left(\frac{\omega_{D}\omega_{P}\beta}{np_{D}}+\frac{\omega_{ D}(1+\omega_{P}pp_{P})}{np_{D}}\right)L_{\max}^{2}}\right)\] \[\leq\frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{\frac{L_{B}^{2}}{ \beta^{2}}+\left(\frac{\omega_{P}}{p_{P}}+\frac{2}{\beta^{2}}\right)L_{A}^{2} +\left(\frac{\omega_{D}\omega_{P}\beta}{np_{D}}+\frac{2\omega_{D}}{np_{D}} \right)L_{\max}^{2}}\right)\] \[\leq\frac{\Psi^{0}}{\varepsilon}\left(L+\sqrt{\frac{L_{B}^{2}}{ \beta^{2}}+\left(\omega_{P}(\omega_{P}+1)+\frac{2}{\beta^{2}}\right)L_{A}^{2}+ \left(\frac{\omega_{D}\omega_{P}(\omega_{D}+1)\beta}{n}+\frac{2\omega_{D}( \omega_{D}+1)}{n}\right)L_{\max}^{2}}\right)\] \[\leq\frac{2\Psi^{0}}{\varepsilon}\left(L+\sqrt{\frac{L_{A}^{2}+L_ {B}^{2}}{\beta^{2}}+\frac{\omega_{D}\omega_{P}(\omega_{D}+1)\beta}{n}}L_{\max} ^{2}+\frac{\omega_{D}(\omega_{D}+1)}{n}L_{\max}^{2}+\omega_{P}(\omega_{P}+1)L _{A}^{2}\right)\]

iterations, where we use the choice of \(p_{P}\) and \(p_{D}.\) Using Lemma C.1, we have \(L_{A}^{2}+L_{B}^{2}\leq L_{\max}^{2}\) and hence

\[\bar{T} \leq\frac{2\Psi^{0}}{\varepsilon}\left(L+\sqrt{\left(\frac{1}{ \beta^{2}}+\frac{\omega_{D}\omega_{P}(\omega_{D}+1)\beta}{n}\right)L_{\max}^{2 }+\frac{\omega_{D}(\omega_{D}+1)}{n}L_{\max}^{2}+\omega_{P}(\omega_{P}+1)L_{A} ^{2}}\right)\] \[\leq\frac{4\Psi^{0}}{\varepsilon}\left(L+\sqrt{\left(1+\left( \frac{\omega_{D}\omega_{P}(\omega_{D}+1)}{n}\right)^{2/3}\right)L_{\max}^{2}+ \frac{\omega_{D}(\omega_{D}+1)}{n}L_{\max}^{2}+\omega_{P}(\omega_{P}+1)L_{A} ^{2}}\right)\] \[\leq\frac{8\Psi^{0}}{\varepsilon}\left(L_{\max}+\left(\frac{ \omega_{D}\omega_{P}(\omega_{D}+1)}{n}\right)^{1/3}L_{\max}+\sqrt{\frac{\omega_{ D}(\omega_{D}+1)}{n}}L_{\max}+\sqrt{\omega_{P}(\omega_{P}+1)}L_{A}\right),\]

where we substitute our choice of \(\beta.\) 

**Corollary E.3**.: _Let \(\mathcal{C}_{i}^{t}\) be the Perm\(K\) compressors and \(\mathcal{Q}_{i}^{t}\) be the independent (Assumption 1.6) \(\text{Rand}K\) compressors, both with \(K=\nicefrac{{d}}{{n}}\). Then, in the view of Corollary E.2, the iteration complexity is_

\[\mathcal{O}\left(\frac{\Psi^{0}}{\varepsilon}\left(n^{2/3}L_{\max}+nL_{A} \right)\right),\]

_and the total communication complexity is_

\[\mathcal{O}\left(\frac{\Psi^{0}}{\varepsilon}\left(\frac{dL_{\max}}{n^{1/3}}+dL _{A}\right)\right).\]

Proof.: The choice of compressors and parameters ensures that \(\omega_{P}=\omega_{D}=n-1\) (Lemma A.6). Thus, the iteration complexity is

\[\mathcal{O}\left(\frac{\Psi^{0}}{\varepsilon}\left(L_{\max}+\left(\frac{ \omega_{D}\omega_{P}(\omega_{D}+1)}{n}\right)^{1/3}L_{\max}+\sqrt{\frac{\omega_ {D}(\omega_{D}+1)}{n}}L_{\max}+\sqrt{\omega_{P}(\omega_{P}+1)}L_{A}\right)\right)\]\[=\mathcal{O}\left(\frac{\Psi^{0}}{\varepsilon}\left(L_{\max}+n^{2/3}L_{ \max}+\sqrt{n}L_{\max}+nL_{A}\right)\right)=\mathcal{O}\left(\frac{\Psi^{0}}{ \varepsilon}\left(n^{2/3}L_{\max}+nL_{A}\right)\right).\]

Since \(p_{P}=p_{D}=1/n\) and \(K=d/n\), on average, the algorithm sends \(\leq\frac{2d}{n}\) coordinates in both directions. Therefore, the total communication complexity is

\[\mathcal{O}\left(\frac{d}{n}\times\frac{\Psi^{0}}{\varepsilon} \left(n^{2/3}L_{\max}+nL_{A}\right)\right)=\mathcal{O}\left(\frac{\Psi^{0}}{ \varepsilon}\left(\frac{d}{n^{1/3}}L_{\max}+dL_{A}\right)\right).\]

### Polyak-Lojasiewicz condition

#### e.3.1 Main Results

As with \(\mathsf{M}\mathsf{A}\mathsf{I}\mathsf{N}\mathsf{A}\mathsf{P}\), we provide the analysis of \(\mathsf{M}\mathsf{S}\) under the Polyak-Lojasiewicz condition.

**Theorem E.8**.: _Let Assumptions 1.1, 1.2, 1.5, 4.2 and D.9 be satisfied and suppose that the compressors \(\mathcal{Q}_{i}^{t}\in\mathbb{U}(\omega_{D})\) satisfy Assumption 1.6, \(\left\{\mathcal{C}_{i}^{t}\right\}_{i=1}^{n}\in\mathbb{P}(\theta)\) and \(\mathcal{C}_{i}^{t}\in\mathbb{U}(\omega_{P})\) for all \(i\in[n]\). Let \(\gamma>0\) be such that_

\[\gamma=\min\Bigg{\{} \left(L+\sqrt{1536\Bigg{(}\left(\frac{\theta}{p_{P}}+\frac{1+ \theta p_{P}}{\beta^{2}}\right)L_{B}^{2}+\left(\frac{\omega_{P}}{p_{P}}+\frac{ 1+\omega_{P}p_{P}}{\beta^{2}}\right)L_{A}^{2}+\left(\frac{\omega_{D}\omega_{P} \beta}{np_{D}}+\frac{\omega_{D}(1+\omega_{P}pp)}{np_{D}}\right)L_{\max}^{2} \Bigg{)}\right)^{-1},\] \[\frac{p_{P}}{2\mu},\frac{p_{D}}{2\mu},\frac{\beta}{4\mu}\Bigg{\}}.\] (33)

_Letting_

\[\Psi^{t} =\delta^{t}+\kappa\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_ {i}(z_{i}^{t})\right\|^{2}+\eta\left\|z^{t}-w^{t}\right\|^{2}+\nu\frac{1}{n} \sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\] \[\quad+\rho\left\|w^{t}-x^{t}\right\|^{2}+\tau\frac{1}{n}\sum_{i= 1}^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2},\]

_where \(\kappa=\frac{2\gamma}{p_{D}}\), \(\eta=\frac{8\gamma L_{B}^{2}}{\beta}\), \(\nu=\frac{8\gamma L_{A}^{2}}{\beta}+\frac{24\gamma\omega_{D}\beta L_{\max}^{2 }}{np_{D}}\), \(\rho=128\gamma L_{B}^{2}\left(\frac{1}{p_{P}}+\frac{p_{P}}{\beta^{2}}\right)\) and \(\tau=128\gamma L_{A}^{2}\left(\frac{1}{p_{P}}+\frac{p_{P}}{\beta^{2}}\right)+ \frac{38\gamma\omega_{D}L_{\max}^{2}}{np_{D}}\left(\beta+p_{P}\right)\), \(\mathsf{M}\mathsf{S}\) ensures that for each \(T\geq 1\)_

\[\mathbb{E}\left[\Psi^{T}\right]\leq\left(1-\gamma\mu\right)^{T} \Psi^{0}.\]

**Corollary E.9**.: _Let \(\mathcal{C}_{i}^{t}\in\mathbb{P}(0)\) for all \(i\in[n]\) (e.g. Perm\(K\)), choose \(p_{P}=1/(\omega_{P}+1),\)\(p_{D}=1/(\omega_{D}+1)\) and_

\[\beta=\min\left\{\left(\frac{n}{\omega_{D}\omega_{P}(\omega_{D} +1)}\right)^{1/3},1\right\}.\]

_Then, in the view of Theorem E.8, Algorithm 2 ensures that \(\mathbb{E}\left[f(x^{T})-f^{*}\right]\leq\varepsilon\) after_

\[\mathcal{O}\left(\max\left\{\frac{\left(1+\left(\frac{\omega_{D} \omega_{P}(\omega_{D}+1)}{n}\right)^{1/3}+\sqrt{\frac{\omega_{D}(\omega_{P}+1)} {n}}\right)L_{\max}+\sqrt{\omega_{P}(\omega_{P}+1)}L_{A}}{\mu},\omega_{P}+1, \omega_{D}+1,\left(\frac{\omega_{D}\omega_{P}(\omega_{D}+1)}{n}\right)^{1/3} \right\}\log\frac{\Psi^{0}}{\varepsilon}\right)\]

_iterations._

**Corollary E.10**.: _Let \(\mathcal{C}_{i}^{t}\) be the Perm\(K\) compressors and \(\mathcal{Q}_{i}^{t}\) be the independent (Assumption 1.6) \(\mathit{R}\mathit{and}K\) compressors, both with \(K=d/n\). Then, in the view of Corollary E.9, the total communication complexity is_

\[\mathcal{O}\left(\left(\frac{dL_{\max}}{n^{1/3}\mu}+\frac{dL_{A}} {\mu}+d\right)\log\frac{\Psi^{0}}{\varepsilon}\right).\]

#### e.3.2 Proofs

**Theorem E.8**.: _Let Assumptions 1.1, 1.2, 1.5, 4.2 and D.9 be satisfied and suppose that the compressors \(\mathcal{Q}_{i}^{t}\in\mathbb{U}(\omega_{D})\) satisfy Assumption 1.6, \(\left\{\mathcal{C}_{i}^{t}\right\}_{i=1}^{n}\in\mathbb{P}(\theta)\) and \(\mathcal{C}_{i}^{t}\in\mathbb{U}(\omega_{P})\) for all \(i\in[n]\). Let \(\gamma>0\) be such that_

\[\gamma =\min\Bigg{\{}\left(L+\sqrt{1536\Bigg{(}\left(\frac{\theta}{p_{P}}+ \frac{1+\theta p_{P}}{\beta^{2}}\right)L_{B}^{2}+\left(\frac{\omega_{P}}{p_{P} }+\frac{1+\omega_{P}p_{P}}{\beta^{2}}\right)L_{A}^{2}+\left(\frac{\omega_{D} \omega_{P}\beta}{np_{D}}+\frac{\omega_{D}(1+\omega_{P}p_{P})}{np_{D}}\right)L_ {\max}^{2}\Bigg{)}}\right)^{-1},\] \[\frac{p_{P}}{2\mu},\frac{p_{D}}{2\mu},\frac{\beta}{4\mu}\Bigg{\}}.\] (33)

_Letting_

\[\Psi^{t} =\delta^{t}+\kappa\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_ {i}(z_{i}^{t})\right\|^{2}+\eta\left\|z^{t}-w^{t}\right\|^{2}+\nu\frac{1}{n} \sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\] \[\quad+\rho\left\|w^{t}-x^{t}\right\|^{2}+\tau\frac{1}{n}\sum_{i=1 }^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2},\]

_where \(\kappa=\frac{2\gamma}{p_{D}}\), \(\eta=\frac{8\gamma L_{B}^{2}}{\beta}\), \(\nu=\frac{8\gamma L_{A}^{2}}{\beta}+\frac{24\gamma\omega_{D}\beta L_{\max}^{ 2}}{np_{D}}\), \(\rho=128\gamma L_{B}^{2}\left(\frac{1}{p_{P}}+\frac{p_{P}}{\beta^{2}}\right)\) and \(\tau=128\gamma L_{A}^{2}\left(\frac{1}{p_{P}}+\frac{p_{P}}{\beta^{2}}\right)+ \frac{384\gamma\omega_{D}L_{\max}^{2}}{np_{D}}\left(\beta+p_{P}\right)\), \(\mathsf{M}\)3 ensures that for each \(T\geq 1\)_

\[\mathbb{E}\left[\Psi^{T}\right]\leq\left(1-\gamma\mu\right)^{T}\Psi^{0}.\]

Proof.: Starting as in the proof of Theorem E.1, we have

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\quad+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t +1}-w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1} \right\|^{2}\right]+\tau\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i} ^{t+1}-x^{t+1}\right\|^{2}\right]\] \[\quad+2\gamma L_{B}^{2}\left(\mathbb{E}\left[\left\|z^{t}-w^{t} \right\|^{2}\right]+\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]\right)\] \[\quad+\kappa\Bigg{(}\frac{\omega_{D}L_{\max}^{2}}{n}\Bigg{(}4p_{ P}\beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t} \right\|^{2}\right]+3\beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\| z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]\] \[\quad\qquad\qquad\qquad+3(\omega_{P}+1)\beta^{2}\mathbb{E}\left[ \left\|x^{t+1}-x^{t}\right\|^{2}\right]\Bigg{)}\Bigg{)}\] \[\quad+\kappa(1-p_{D})\mathbb{E}\left[\left\|g^{t}-\frac{1}{n} \sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t})\right\|^{2}\right]\] \[\quad+\eta\Bigg{(}\left(1-\frac{\beta}{2}\right)\mathbb{E}\left[ \left\|z^{t}-w^{t}\right\|^{2}\right]+4\left(\frac{1}{\beta}+\theta\right) \mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+4p_{P}\left(1+\frac{p_ {P}}{\beta}\right)\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]\Bigg{)}\] \[\quad+\nu\Bigg{(}\left(1-\frac{\beta}{2}\right)\mathbb{E}\left[ \frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]+4 \left(\frac{1}{\beta}+\omega_{P}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t} \right\|^{2}\right]\]\[+4p_{P}\left(1+\frac{p_{P}}{\beta}\right)\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]\Bigg{)}\] \[+\rho(1-p_{P})\left(\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2 }\right]+\theta\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\right)\] \[+\tau(1-p_{P})\left(\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n} \left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+\omega_{P}\mathbb{E}\left[\left\|x^ {t+1}-x^{t}\right\|^{2}\right]\right)\]

for some \(\kappa,\eta,\nu,\rho,\tau\geq 0\). This time, we let \(\kappa=\frac{2\gamma}{p_{D}}\) and \(\eta=\frac{8\gamma L_{B}^{2}}{\beta}\), which gives \(\gamma+\kappa(1-p_{D})=\kappa\left(1-\frac{p_{D}}{2}\right)\) and \(2\gamma L_{B}^{2}+\eta(1-\nicefrac{{\beta}}{{2}})=\eta\left(1-\frac{\beta}{ 4}\right)\). Hence

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}-w _{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1}\right\| ^{2}\right]+\tau\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t+1}-x ^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}-\frac {L}{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[+\kappa\left(1-\frac{p_{D}}{2}\right)\mathbb{E}\left[\left\|g^{t }-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t})\right\|^{2}\right]+\eta \left(1-\frac{\beta}{4}\right)\mathbb{E}\left[\left\|z^{t}-w^{t}\right\|^{2}\right]\] \[+2\gamma L_{A}^{2}\frac{1}{n}\sum_{i=1}^{n}\left(\mathbb{E}\left[ \left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]+\mathbb{E}\left[\left\|w_{i}^{t }-x^{t}\right\|^{2}\right]\right)+2\gamma L_{B}^{2}\mathbb{E}\left[\left\|w^{ t}-x^{t}\right\|^{2}\right]\] \[+\frac{2\gamma}{p_{D}}\Bigg{(}\frac{\omega_{D}L_{\max}^{2}}{n} \Bigg{(}4p_{P}\beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^ {t}-x^{t}\right\|^{2}\right]+3\beta^{2}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^ {n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]\] \[+3(\omega_{P}+1)\beta^{2}\mathbb{E}\left[\left\|x^{t+1}-x^{t} \right\|^{2}\right]\Bigg{)}\Bigg{)}\] \[+\frac{8\gamma L_{B}^{2}}{\beta}\left(4\left(\frac{1}{\beta}+ \theta\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+4p_{P} \left(1+\frac{p_{P}}{\beta}\right)\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{ 2}\right]\right)\] \[+\nu\Bigg{(}\left(1-\frac{\beta}{2}\right)\mathbb{E}\left[\frac{1 }{n}\sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]+4\left(\frac{1 }{\beta}+\omega_{P}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[+4p_{P}\left(1+\frac{p_{P}}{\beta}\right)\mathbb{E}\left[\frac{1}{n }\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]\Bigg{)}\] \[+\rho\left((1-p_{P})\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2 }\right]+\theta\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\right)\] \[+\tau\left((1-p_{P})\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n} \left\|w_{i}^{t}-x^{t}\right\|^{2}\right]+\omega_{P}\mathbb{E}\left[\left\|x^{t +1}-x^{t}\right\|^{2}\right]\right).\]

Rearranging the terms

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}-w _{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1} \right\|^{2}\right]+\tau\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^ {t+1}-x^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}-\frac{L }{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\]\[+\kappa\left(1-\frac{p_{D}}{2}\right)\mathbb{E}\left[\left\|g^{t}- \frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t})\right\|^{2}\right]+\eta\left(1- \frac{\beta}{4}\right)\mathbb{E}\left[\left\|z^{t}-w^{t}\right\|^{2}\right]\] \[+\left(\nu\left(1-\frac{\beta}{2}\right)+2\gamma L_{A}^{2}+\frac{ 6\gamma\omega_{D}\beta^{2}L_{\max}^{2}}{np_{D}}\right)\mathbb{E}\left[\frac{1} {n}\sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]\] \[+\left(\frac{6\gamma\omega_{D}(\omega_{P}+1)\beta^{2}L_{\max}^{2} }{np_{D}}+\rho\theta+\frac{32\gamma L_{B}^{2}}{\beta}\left(\frac{1}{\beta}+ \theta\right)+4\nu\left(\frac{1}{\beta}+\omega_{P}\right)+\tau\omega_{P} \right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[+\left(\rho(1-p_{P})+2\gamma L_{B}^{2}+\frac{32\gamma L_{B}^{2}p _{P}}{\beta}\left(1+\frac{p_{P}}{\beta}\right)\right)\mathbb{E}\left[\left\| w^{t}-x^{t}\right\|^{2}\right]\] \[+\left(\tau(1-p_{P})+2\gamma L_{A}^{2}+4\nu p_{P}\left(1+\frac{p_ {P}}{\beta}\right)+\frac{8\gamma\omega_{D}p_{P}\beta^{2}L_{\max}^{2}}{np_{D}} \right)\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\| ^{2}\right].\]

Considering the coefficient of \(\mathbb{E}\left[\left\|w^{t}-x^{t}\right\|^{2}\right]\) and using the inequality \(xy\leq\frac{x^{2}+y^{2}}{2}\) for all \(x,y\geq 0\), we get

\[\rho(1-p_{P})+2\gamma L_{B}^{2}+\frac{32\gamma L_{B}^{2}p_{P}}{ \beta}\left(1+\frac{p_{P}}{\beta}\right) \leq\rho(1-p_{P})+32\gamma L_{B}^{2}\left(1+\frac{p_{P}}{\beta}+ \frac{p_{P}^{2}}{\beta^{2}}\right)\] \[\leq\rho(1-p_{P})+64\gamma L_{B}^{2}\left(1+\frac{p_{P}^{2}}{ \beta^{2}}\right)\] \[=\rho\left(1-\frac{p_{P}}{2}\right),\]

where we define \(\rho=128\gamma L_{B}^{2}\left(\frac{1}{p_{P}}+\frac{p_{P}}{\beta^{2}}\right)\). Substituting this choice of \(\rho\), we obtain

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| \left[g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2} \right]+\eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\right.\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}-w _{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1} \right\|^{2}\right]+\tau\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i }^{t+1}-x^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}- \frac{L}{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[+\kappa\left(1-\frac{p_{D}}{2}\right)\mathbb{E}\left[\left\|g^{t }-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t})\right\|^{2}\right]+\eta \left(1-\frac{\beta}{4}\right)\mathbb{E}\left[\left\|z^{t}-w^{t}\right\|^{2}\right]\] \[+\rho\left(1-\frac{p_{P}}{2}\right)\mathbb{E}\left[\left\|w^{t}- x^{t}\right\|^{2}\right]+\left(\nu\left(1-\frac{\beta}{2}\right)+2\gamma L_{A}^{2}+ \frac{6\gamma\omega_{D}\beta^{2}L_{\max}^{2}}{np_{D}}\right)\mathbb{E}\left[ \frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]\] \[+\left(\frac{6\gamma\omega_{D}(\omega_{P}+1)\beta^{2}L_{\max}^{2} }{np_{D}}+128\gamma L_{B}^{2}\left(\frac{1}{p_{P}}+\frac{p_{P}}{\beta^{2}} \right)\theta+\frac{32\gamma L_{B}^{2}}{\beta}\left(\frac{1}{\beta}+\theta\right)\right.\] \[\left.\qquad+4\nu\left(\frac{1}{\beta}+\omega_{P}\right)+\tau \omega_{P}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[+\left(\tau(1-p_{P})+2\gamma L_{A}^{2}+4\nu p_{P}\left(1+\frac{ p_{P}}{\beta}\right)+\frac{8\gamma\omega_{D}p_{P}\beta^{2}L_{\max}^{2}}{np_{D}} \right)\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\|^{ 2}\right].\]

Similarly, taking \(\nu=\frac{8\gamma L_{A}^{2}}{\beta}+\frac{24\gamma\omega_{D}\beta L_{\max}^{2}}{ np_{D}}\) gives \(\nu\left(1-\frac{\beta}{2}\right)+2\gamma L_{A}^{2}+\frac{6\gamma\omega_{D}\beta^{2}L_{ \max}^{2}}{np_{D}}=\nu\left(1-\frac{\beta}{4}\right)\), so

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\|g^{t+1}- \frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+\eta \mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\]\[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}-w_{i} ^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1}\right\|^{ 2}\right]+\tau\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t+1}-x^{t+ 1}\right\|^{2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}- w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1}\right\|^{2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}- w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t}\right\|^{ 2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}- w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t}\right\|^{ 2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}- w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t}\right\|^{ 2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}- w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t}\right\|^{ 2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}- w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t}\right\|^{ 2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{ t}\right\|^{2}\right].\]

Considering the last bracket, we have

\[\tau(1-p_{P})+2\gamma L_{A}^{2}+4\left(\frac{8\gamma L_{A}^{2}}{ \beta}+\frac{24\gamma\omega_{D}\beta L_{\max}^{2}}{np_{D}}\right)p_{P}\left(1+ \frac{p_{P}}{\beta}\right)+\frac{8\gamma\omega_{DPP}\beta^{2}L_{\max}^{2}}{np _{D}}\] \[=\tau(1-p_{P})+2\gamma L_{A}^{2}+\frac{32\gamma p_{P}L_{A}^{2}}{ \beta}+\frac{32\gamma p_{P}^{2}L_{A}^{2}}{\beta^{2}}+\frac{96\gamma\omega_{DPP }\beta L_{\max}^{2}}{np_{D}}+\frac{96\gamma\omega_{DPP}^{2}L_{\max}^{2}}{np_{ D}}\] \[\quad+\frac{8\gamma\omega_{DP}p_{P}\beta^{2}L_{\max}^{2}}{np_{D}}\] \[\leq\tau(1-p_{P})+32\gamma L_{A}^{2}\left(1+\frac{p_{P}}{\beta}+ \frac{p_{P}^{2}}{\beta^{2}}\right)+\frac{96\gamma\omega_{DP}p_{P}L_{\max}^{2} }{np_{D}}\left(\beta+p_{P}+\beta^{2}\right)\] \[\leq\tau(1-p_{P})+64\gamma L_{A}^{2}\left(1+\frac{p_{P}^{2}}{ \beta^{2}}\right)+\frac{192\gamma\omega_{D}p_{P}L_{\max}^{2}}{np_{D}}\left( \beta+p_{P}\right)\] \[=\tau\left(1-\frac{p_{P}}{2}\right)\]

for \(\tau=128\gamma L_{A}^{2}\left(\frac{1}{p_{P}}+\frac{p_{P}}{\beta^{2}}\right) +\frac{384\gamma\omega_{D}L_{\max}^{2}}{np_{D}}\left(\beta+p_{P}\right)\). Then

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}- \frac{L}{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[\quad+\kappa\left(1-\frac{p_{D}}{2}\right)\mathbb{E}\left[\left\| g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t})\right\|^{2}\right]+\eta \left(1-\frac{\beta}{4}\right)\mathbb{E}\left[\left\|z^{t}-w^{t}\right\|^{2}\right]\] \[\quad+\rho\left(1-\frac{p_{P}}{2}\right)\mathbb{E}\left[\left\|w^ {t}-x^{t}\right\|^{2}\right]+\nu\left(1-\frac{\beta}{4}\right)\mathbb{E}\left[ \frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]\]\[\frac{32\gamma}{\beta^{2}}+\frac{32\gamma\omega_{P}}{\beta}+\frac{128}{p_{P}}+ \frac{128\gamma\omega_{P}p_{P}}{\beta^{2}} \leq 128\gamma\left(\frac{1}{\beta^{2}}+\frac{\omega_{P}}{p_{P}}\left(1+ \frac{p_{P}}{\beta}+\frac{p_{P}^{2}}{\beta^{2}}\right)\right)\] \[\leq 256\gamma\left(\frac{\omega_{P}}{p_{P}}+\frac{\omega_{P}p_{P} }{\beta^{2}}+\frac{1}{\beta^{2}}\right)\] \[\leq 256\gamma\left(\frac{\omega_{P}}{p_{P}}+\frac{1+\omega_{P}pp }{\beta^{2}}\right),\]

and for \(L_{\max}^{2}\) we obtain

\[\frac{6\gamma\omega_{D}(\omega_{P}+1)\beta^{2}}{np_{D}}+\frac{96\gamma\omega_ {D}}{np_{D}}+\frac{96\gamma\omega_{D}\omega_{P}\beta}{np_{D}}+\frac{384\gamma \omega_{D}\omega_{P}\beta}{np_{D}}+\frac{384\gamma\omega_{D}\omega_{P}pp_{P}}{ np_{D}}\]\[\leq 384\gamma\omega_{D}\left(\frac{(\omega_{P}+1)\beta^{2}}{np_{D}} +\frac{1}{np_{D}}+\frac{\omega_{P}\beta}{np_{D}}+\frac{\omega_{P}p_{P}}{np_{D}}\right)\] \[\leq 768\gamma\omega_{D}\left(\frac{1}{np_{D}}+\frac{\omega_{P} \beta}{np_{D}}+\frac{\omega_{P}p_{P}}{np_{D}}\right)\] \[=768\gamma\left(\frac{\omega_{D}\omega_{P}\beta}{np_{D}}+\frac{ \omega_{D}(1+\omega_{P}p_{P})}{np_{D}}\right)\]

since \(\frac{(\omega_{P}+1)\beta^{2}}{np_{D}}\leq\frac{1}{np_{D}}+\frac{\omega_{P} \beta}{np_{D}}\). Substituting these inequalities to (34) and (35), we get

\[\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\quad+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{ t+1}-w_{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1} \right\|^{2}\right]+\tau\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^ {t+1}-x^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]-\left(\frac{1}{2\gamma}-\frac{ L}{2}\right)\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]\] \[\quad+\tau\left(1-\frac{p_{P}}{2}\right)\mathbb{E}\left[\frac{1} {n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]\] \[\quad+768\gamma\Bigg{(}\left(\frac{\theta}{p_{P}}+\frac{1+\theta p _{P}}{\beta^{2}}\right)L_{B}^{2}+\left(\frac{\omega_{P}}{p_{P}}+\frac{1+\omega _{P}p_{P}}{\beta^{2}}\right)L_{A}^{2}\] \[\quad\quad\quad\quad+\left(\frac{\omega_{D}\omega_{P}\beta}{np_{D }}+\frac{\omega_{D}(1+\omega_{P}p_{P})}{np_{D}}\right)L_{\max}^{2}\Bigg{)} \mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right].\]

By collecting all the terms w.r.t. \(\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right],\) using the step size \(\gamma\) from the theorem and Lemma H.2, we obtain

\[\mathbb{E}\left[\Psi^{t+1}\right] =\mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\| g^{t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[\quad+\tau\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{ t+1}-x^{t+1}\right\|^{2}\right]\] \[\leq\mathbb{E}\left[\delta^{t}\right]-\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{t})\right\|^{2}\right]+\kappa\left(1-\frac{p_{D}}{2} \right)\mathbb{E}\left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i }^{t})\right\|^{2}\right]\] \[\quad+\eta\left(1-\frac{\beta}{4}\right)\mathbb{E}\left[\left\|z ^{t}-w^{t}\right\|^{2}\right]+\rho\left(1-\frac{p_{P}}{2}\right)\mathbb{E} \left[\left\|w^{t}-x^{t}\right\|^{2}\right]\] \[\quad+\nu\left(1-\frac{\beta}{4}\right)\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left\|z_{i}^{t}-w_{i}^{t}\right\|^{2}\right]+\tau\left(1-\frac{ p_{P}}{2}\right)\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t} \right\|^{2}\right].\]Lastly, Assumption D.9 gives

\[\mathbb{E}\left[\Psi^{t+1}\right] = \mathbb{E}\left[\delta^{t+1}\right]+\kappa\mathbb{E}\left[\left\|g^ {t+1}-\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(z_{i}^{t+1})\right\|^{2}\right]+ \eta\mathbb{E}\left[\left\|z^{t+1}-w^{t+1}\right\|^{2}\right]\] \[+\nu\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|z_{i}^{t+1}-w _{i}^{t+1}\right\|^{2}\right]+\rho\mathbb{E}\left[\left\|w^{t+1}-x^{t+1}\right\| ^{2}\right]\] \[+\tau\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left\|w_{i}^{t+1}- x^{t+1}\right\|^{2}\right]\] \[\overset{\text{\tiny{Ass.D.9,(\ref{eq:2.1})}}}{\leq} \left(1-\gamma\mu\right)\mathbb{E}\left[\delta^{t}\right]+\kappa \left(1-\gamma\mu\right)\mathbb{E}\left[\left\|g^{t}-\frac{1}{n}\sum_{i=1}^{n }\nabla f_{i}(z_{i}^{t})\right\|^{2}\right]\] \[+\rho\left(1-\gamma\mu\right)\mathbb{E}\left[\left\|w^{t}-x^{t} \right\|^{2}\right]+\tau\left(1-\gamma\mu\right)\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left\|w_{i}^{t}-x^{t}\right\|^{2}\right]\] \[= \left(1-\gamma\mu\right)\mathbb{E}\left[\Psi^{t}\right]\]

It remains to apply the last inequality iteratively to finish the proof. 

**Corollary E.9**.: _Let \(\mathcal{C}_{i}^{t}\in\mathbb{P}(0)\) for all \(i\in[n]\) (e.g. Perm\(K\)), choose \(p_{P}=1/(\omega_{P}+1),\,p_{D}=1/(\omega_{D}+1)\) and_

\[\beta=\min\left\{\left(\frac{n}{\omega_{D}\omega_{P}(\omega_{D}+1)}\right)^{1/ 3},1\right\}.\]

_Then, in the view of Theorem E.8, Algorithm 2 ensures that \(\mathbb{E}\left[f(x^{T})-f^{*}\right]\leq\varepsilon\) after_

\[\mathcal{O}\left(\max\left\{\frac{\left(1+\left(\frac{\omega_{D}\omega_{P}( \omega_{D}+1)}{n}\right)^{1/3}+\sqrt{\frac{\omega_{D}(\omega_{D}+1)}{n}}\right) L_{\max}+\sqrt{\omega_{P}(\omega_{P}+1)L_{A}}}{\mu},\omega_{P}+1,\omega_{D}+1, \left(\frac{\omega_{D}\omega_{P}(\omega_{D}+1)}{n}\right)^{1/3}\right\}\log \frac{\Psi^{0}}{\varepsilon}\right)\]

_iterations._

Proof.: Note that \(\Psi^{T}\geq f(x^{T})-f^{*}\). In view of condition (33) from Theorem E.8, the step size satisfies

\[\gamma=\Theta\Bigg{(}\min\left\{\left(L+\sqrt{\left(\frac{\theta}{p_{P}}+ \frac{1+\theta p_{P}}{\beta^{2}}\right)L_{B}^{2}+\left(\frac{\omega_{P}}{p_{P}} +\frac{1+\omega_{P}p_{P}}{\beta^{2}}\right)L_{A}^{2}+\left(\frac{\omega_{D} \omega_{P}\beta}{np_{D}}+\frac{\omega_{D}(1+\omega_{P}p_{P})}{np_{D}}\right)L_ {\max}^{2}}\right)^{-1},\]

\[\frac{p_{P}}{2\mu},\frac{p_{D}}{2\mu},\frac{\beta}{4\mu}\right\}\Bigg{)}.\]

Therefore, since \(\theta=0\), the algorithm converges after

\[\bar{T}=\mathcal{O}\left(\max\left\{\frac{L+\sqrt{\frac{1}{\beta^{2}}L_{B}^{2 }+\left(\frac{\omega_{P}}{p_{P}}+\frac{1+\omega_{P}p_{P}}{\beta^{2}}\right)L_ {A}^{2}+\left(\frac{\omega_{D}\omega_{P}\beta}{np_{D}}+\frac{\omega_{D}(1+ \omega_{P}p_{P})}{np_{D}}\right)L_{\max}^{2}}}{\mu},\frac{1}{p_{P}},\frac{1}{p _{D}},\frac{1}{\beta}\right\}\log\frac{\Psi^{0}}{\varepsilon}\right)\]

iterations. Using the choice of \(p_{P}\) and \(p_{D}\), we have

\[\bar{T}=\mathcal{O}\left(\max\left\{\frac{L+\sqrt{\frac{1}{\beta^{2}}(L_{B}^{2 }+L_{A}^{2})+\omega_{P}(\omega_{P}+1)L_{A}^{2}+\left(\frac{\omega_{D}(\omega_{D} +1)\omega_{P}\beta}{n}+\frac{\omega_{D}(\omega_{D}+1)}{n}\right)L_{\max}^{2}} }{\mu},\omega_{P}+1,\omega_{D}+1,\frac{1}{\beta}\right\}\log\frac{\Psi^{0}}{ \varepsilon}\right).\]Due to Lemma C.1, we get

\[\bar{T}=\mathcal{O}\left(\max\left\{\frac{L+\sqrt{\omega_{P}(\omega_{P}+1)L_{A}^{2 }+\left(\frac{1}{\beta^{2}}+\frac{\omega_{D}(\omega_{D}+1)\omega_{P}\beta}{n}+ \frac{\omega_{D}(\omega_{P}+1)}{n}\right)L_{\max}^{2}}}{\mu},\omega_{P}+1, \omega_{D}+1,\frac{1}{\beta}\right\}\log\frac{\Psi^{0}}{\varepsilon}\right).\]

Using the choice of \(\beta,\) we obtain the result of the theorem. 

**Corollary E.10**.: _Let \(\mathcal{C}_{i}^{t}\) be the Perm\(K\) compressors and \(\mathcal{Q}_{i}^{t}\) be the independent (Assumption 1.6) \(RandK\) compressors, both with \(K=\nicefrac{{d}}{{n}}\). Then, in the view of Corollary E.9, the total communication complexity is_

\[\mathcal{O}\left(\left(\frac{dL_{\max}}{n^{1/3}\mu}+\frac{dL_{A}}{\mu}+d \right)\log\frac{\Psi^{0}}{\varepsilon}\right).\]

Proof.: The choice of compressors and parameters ensures that \(\omega_{P}=\omega_{D}=n-1\) (Lemma A.6). Thus, the iteration complexity is

\[\mathcal{O}\left(\max\left\{\frac{\left(1+n^{2/3}+n^{1/2}\right)L _{\max}+nL_{A}}{\mu},n,n^{2/3}\right\}\log\frac{\Psi^{0}}{\varepsilon}\right)\] \[=\mathcal{O}\left(\max\left\{\frac{n^{2/3}L_{\max}+nL_{A}}{\mu},n \right\}\log\frac{\Psi^{0}}{\varepsilon}\right).\]

Since \(p_{P}=p_{D}=\nicefrac{{1}}{{n}}\) and \(K=\nicefrac{{d}}{{n}}\), on average, the algorithm sends \(\leq\nicefrac{{2d}}{{n}}\) coordinates in both directions. Therefore, the total communication complexity is

\[\mathcal{O}\left(\frac{d}{n}\times\max\left\{\frac{n^{2/3}L_{\max}+nL_{A}}{\mu},n\right\}\log\frac{\Psi^{0}}{\varepsilon}\right)=\mathcal{O}\left(\max\left\{ \frac{\frac{d}{n^{1/3}}L_{\max}+dL_{A}}{\mu},d\right\}\log\frac{\Psi^{0}}{ \varepsilon}\right).\]

## Appendix F Experiments

The experiments were prepared in Python. The distributed environment was emulated on a machine with Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz and 64 cores.

### Experiments with M3 on quadratic optimization tasks

We consider close-to-homogeneous quadratic optimization problem with \(\mathbf{A}_{i}=(1+\xi_{i})\mathbf{I}_{d}\), where \(\xi_{i}\sim\mathcal{N}(0,0.01)\) for all \(i\in[n],\) and \(d=10000\). We run two algorithms from Table 1, M3 and CORE, and check whether theory matches practice. In M3, we use Perm\(K\) followed by the natural compressor \(\mathcal{C}_{\text{nat}}\)(Horvath et al., 2022) (composition of two unbiased compressors) on the server's side, and Rand\(K\) followed by \(\mathcal{C}_{\text{nat}}\) on the workers' side. We use \(K=\lfloor d/n\rfloor\in\{1,10,100\}\) for \(n\in\{1000,100,10\}\). In CORE, the number of communicated coordinates is set to \(10\). We run each experiment \(5\) times with different seeds and plot the average to reduce the noise factor. Only the step size is fine-tuned for each algorithm.

The results are presented in Figure 2. As expected, CORE does not change its behavior as the number of workers increases from \(10\) to \(100\); this is expected since CORE does not depend on \(n\). At the same time, M3 does improve with \(n\), which supports our findings from Theorem 5.1.

### Experiments with an autoencoder and MNIST

We now compare MARINA-P, M3, CORE, EF21-P + DCGD, and GD on a non-convex autoencoder problem. We train it on the MNIST dataset (LeCun et al., 2010) with objective function \(f(\mathbf{D},\mathbf{E}):=\frac{1}{m}\sum_{i=1}^{m}\left\|\mathbf{D}\mathbf{E} b_{i}-b_{i}\right\|^{2}+\frac{\lambda}{2}\left\|\mathbf{D}\mathbf{E}-\mathbf{I} \right\|_{F}^{2},\) where \(\mathbf{D}\in\mathbb{R}^{d_{1}\times d_{2}}\), \(\mathbf{E}\in\mathbb{R}^{d_{2}\times d_{1}}\), \(b_{i}\in\mathbb{R}^{d_{1}}\) are samples,

Figure 3: Experiments on the autoencoder task from Section F.2. We plot the norm of the gradient w.r.t. # of coordinates sent from the server (s-to-w) and from the workers (w-to-s).

Figure 2: Experiments on the quadratic optimization problem from Section F.1. We plot the norm of the gradient w.r.t. # of coordinates sent from the server (s-to-w) and from the workers (w-to-s).

\(d_{1}=784\) is the number of features, \(d_{2}=16\) is the size of the encoding space, \(\lambda=0.001\) is a regularizer, and \(m=60\,000\) is the number of samples. The dimension of the problem is \(d=25\,088\). We randomly split the dataset among \(n=100\) workers. For MARINA-P and M3, we take Perm\(K\) followed by the natural compressor \(\mathcal{C}_{\text{nat}}\) on the server's side. On the workers' side, M3 uses Rand\(K\) and \(\mathcal{C}_{\text{nat}}\). For EF21-P + DCGD, we take Rand\(K\) with \(\mathcal{C}_{\text{nat}}\) on both the workers' and server's sides. In each case, \(K=\lfloor d/n\rfloor=250\). For CORE, we set the number of communicated coordinates to \(100\). As in previous experiments, we only fine-tune the step size, repeat each experiment \(5\) times, and plot the average results.

The results are presented in Figure 3. All methods with bidirectional compression: M3, CORE, and EF21-P + DCGD, converge much faster than GD. MARINA-P converges fastest only in the first plot. This is expected since it compresses only from the server to the workers. M3, CORE, and EF21-P + DCGD have similar convergence rates in both metrics, with M3 performing better in the low accuracy regime.

### Extra experiments with quadratic optimization tasks

The aim of this set of experiments is to empirically test our results under Assumption 4.2. We consider the problem of quadratic minimization with varying level of heterogeneity between the \(n\) functions stored on the workers. The goal is to minimize the squared norm of the gradient of \(\sum_{i=1}^{n}f_{i}\), where the functions \(f_{i}\) are of form

\[f_{i}(x)=\frac{1}{2}x^{T}\mathbf{A}_{i}x+b_{i}^{T}x.\]

Here, \(\mathbf{A}_{i}\) are \(d\times d\) matrices generated following the procedure in Algorithm 3, and \(b_{i}\) denotes a standard normal vector in \(\mathbb{R}^{d}\). The constants \(L_{A}\) and \(L_{B}\) from Assumption 4.2 (in this case, by Theorem 4.8\(L_{A}=\sqrt{2}\max_{i\in[n]}\|\mathbf{A}_{i}-\mathbf{A}\|\) and \(L_{B}=\sqrt{2}\left(\frac{1}{n}\sum_{i=1}^{n}\|\mathbf{A}_{i}\|\right)\)) are controlled by parameters \(v_{i}\) and \(\sigma_{i}^{2}\). In particular, for \(\sigma_{i}^{2}=0\), all workers hold the same matrix \(\mathbf{A}_{i}\), and hence in this case \(L_{A}=0\).

We compare the following algorithms:

1. MARINA-P with Perm\(K\) compressors,
2. MARINA-P with Rand\(K\) compressors,
3. MARINA-P with SameRand\(K\) compressor,
4. EF21-P with Top\(K\) compressor,
5. GD.

In all compressed methods, we set \(K=\nicefrac{{d}}{{n}}\) and use \(p=\nicefrac{{k}}{{d}}\) in MARINA-P.

The step sizes are tuned from \(2^{i},i\in\mathbb{Z}\) multiples of the values predicted by the theory (indicated by \(\times 1,\times 2,\ldots\) in the plots). We fix \(d=300\) and generate optimization tasks with \(n\in\{10,100,900\}\). The results are presented in Figures 4, 5, 6.

The empirical results align well with the theory. Among the algorithms tested, MARINA-P with Perm\(K\) compressor exhibits the best performance, while MARINA-P with SameRand\(K\) converges the slowest and comparable to GD. MARINA-P with Rand\(K\) compressor and EF21-P achieve performance levels somewhere in between. Notably, the differences between the runs of MARINA-P with different compressors become more pronounced as the value of \(n\) increases. As anticipated, the performance of MARINA-P with Rand\(K\) and Perm\(K\) compressors improves with an increase in the number of workers, while the performance of EF21-P does not follow the same behaviour. Specifically, for \(n=10\), EF21-P outperforms MARINA-P with Rand\(K\) compressor, but this pattern reverses for both \(n=100\) and \(n=1000\).

```
1:Parameters:\(v_{0},\ldots,v_{4}\in\mathbb{R}_{+}\), \(\sigma_{0},\ldots,\sigma_{4}\in\mathbb{R}_{\geq 0}\).
2:Let \[\mathbf{X}=\frac{1}{4}\begin{bmatrix}2&-1&&0\\ -1&\ddots&\ddots&\\ &\ddots&\ddots&-1\\ 0&&-1&2\end{bmatrix}\in\mathbb{R}^{300\times 300},\]
3:for\(k=0,\ldots,4\)do
4: Generate \(\xi_{i}\sim\mathcal{N}(0,\sigma_{k}^{2})\cap[-v_{0},v_{0}]\) for \(i\in[n]\)
5:for\(l=0,\ldots,4\)do
6: Set \(\mathbf{A}_{i}^{k,l}=(v_{l}+\xi_{i})\mathbf{X}\) for \(i\in[n]\)
7: Sample \(b_{i}^{k,l}\sim\mathcal{N}(0,\mathbb{I}_{d})\) for \(i\in[n]\)
8:endfor
9:Output: matrices \(\mathbf{A}_{i}^{k,l}\), vectors \(b_{i}^{k,l}\), \(i\in[n]\), \(k,l\in[4]\).
10:endfor ```

**Algorithm 3** Heterogeneous quadratic problem generation

Figure 4: Experiments on the quadratic optimization problem from Section F.3 with \(n=10\) for \(L_{A}^{2}\in\{0,1,10,100\}\) and \(L_{B}^{2}\in\{100,1000,10000,100000\}\).

Figure 5: Experiments on the quadratic optimization problem from Section F.3 with \(n=100\) for \(L^{2}_{A}\in\{0,1,10,100\}\) and \(L^{2}_{B}\in\{100,1000,10000,100000\}\).

Proof of the Lower Bounds

### The "difficult" function from the nonconvex world

In our lower bound, we use the function from Carmon et al. (2020); Arjevani et al. (2022). For any \(T\in\mathbb{N}\), let

\[F_{T}(x):=-\Psi(1)\Phi([x]_{1})+\sum_{i=2}^{T}\left(\Psi(-[x]_{i-1})\Phi(-[x]_{i })-\Psi([x]_{i-1})\Phi([x]_{i})\right),\] (36)

where

\[\Psi(x)=\begin{cases}0,&x\leq 1/2,\\ \exp\left(1-\frac{1}{(2x-1)^{2}}\right),&x\geq 1/2,\end{cases}\quad\text{and} \quad\Phi(x)=\sqrt{e}\int_{-\infty}^{x}e^{-\frac{1}{2}t^{2}}dt.\]

Carmon et al. (2020); Arjevani et al. (2022) also proved the following properties of the function:

**Lemma G.1** (Carmon et al. (2020); Arjevani et al. (2022)).: _The function \(F_{T}\) satisfies:_

1. \(F_{T}(0)-\inf_{x\in\mathbb{R}^{T}}F_{T}(x)\leq\Delta^{0}T,\) _where_ \(\Delta^{0}=12.\)__
2. _The function_ \(F_{T}\) _is_ \({}_{1}\)_-smooth, where_ \(l_{1}=152.\)__
3. _For all_ \(x\in\mathbb{R}^{T},\left\|\nabla F_{T}(x)\right\|_{\infty}\leq\gamma_{\infty},\) _where_ \(\gamma_{\infty}=23.\)__
4. _For all_ \(x\in\mathbb{R}^{T},\)__\(\text{prog}(\nabla F_{T}(x))\leq\text{prog}(x)+1.\)__
5. _For all_ \(x\in\mathbb{R}^{T},\) _if_ \(\text{prog}(x)<T,\) _then_ \(\left\|\nabla F_{T}(x)\right\|>1,\)__

where \(\text{prog}(x):=\max\{i\geq 0\,|\,x_{i}\neq 0\}\quad(x_{0}\equiv 1).\)__

The function is a standard function that is used to establish lower bounds in the nonconvex world (Carmon et al., 2020; Arjevani et al., 2022; Lu and De Sa, 2021; Tyurin and Richtarik, 2023c).

### Theorems

Our lower bound applies to the family of methods with the following structure:

```
1:Input: functions \(f_{1},\ldots,f_{n}\in\mathcal{F},\) algorithm \(A,\) probability \(p\)
2:for\(k=0,\ldots,\infty\)do
3: Server calculates a new point: \(x^{k}=B_{1}^{k}(g_{1}^{1},\ldots,g_{1}^{k},\ldots,g_{n}^{1},\ldots,g_{n}^{k})\)
4: Server aggregates all available information: \(s_{i}^{k}=B_{2,i}^{k}(g_{1}^{1},\ldots,g_{1}^{k},\ldots,g_{n}^{1},\ldots,g_{n }^{k})\)
5: Server sends sparsified vectors \(\bar{s}_{i}^{k}\) to the workers, where \[[\bar{s}_{i}^{k}]_{j}=[s_{i}^{k}]_{j}\times\eta_{i,j}^{k},\] and \(\eta_{i,j}^{k}\) is a random variable such that \(\mathbb{P}\left(\eta_{i,j}^{k}\neq 0\right)\leq p\) for all \(j\in[d^{\prime}]\) and for all \(i\in[n]\). We define \(d^{\prime}:=\text{dim(dom}(f_{1})),\) and \([\cdot]_{j}\) means the \(j^{\text{th}}\) coordinate.
6: Workers aggregate all available local information and calculate gradients: \(g_{i}^{k+1}=L_{i}^{k}(\bar{s}_{i}^{0},\ldots,\bar{s}_{i}^{k})\) (\(L_{i}^{k}\) has access to the gradient oracle of \(f_{i}\) and can call it as many times as it wants according to the rules (37) and (38))
7: Workers send \(g_{i}^{k+1}\) to the server
8:endfor ```

**Protocol 4** Protocol

We consider the following standard classes of functions and algorithms:

**Definition G.2**.: Let the function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) be differentiable, \(L\)-smooth (i.e., \(\left\|\nabla f(x)-\nabla f(y)\right\|\leq L\left\|x-y\right\|\) for all \(x,y\in\mathbb{R}^{d}\)), and \(f(0)-\inf_{x\in\mathbb{R}^{d}}f(x)\leq\delta^{0}.\) We denote the family of functions that satisfy these properties by \(\mathcal{F}_{\delta^{0},L}\).

**Definition G.3**.: Consider Protocol 4. A sequence of tuples of mappings \(A=\{(B_{1}^{k},B_{2,1}^{k},\dots,B_{2,n}^{k},L_{1}^{k},\dots,L_{n}^{k})\}_{k=0}^{\infty}\) is a zero-respecting algorithm, if,

1. \(B_{1}^{k}\,:\,\underbrace{\mathbb{R}^{d}\times\dots\times\mathbb{R}^{d}}_{n\times k \text{ times}}\to\mathbb{R}^{d}\) for all \(k\geq 1,\) and \(B_{1}^{0}\in\mathbb{R}^{d}.\)
2. \(B_{2,i}^{k}\,:\,\underbrace{\mathbb{R}^{d}\times\dots\times\mathbb{R}^{d}}_{n \times k\text{ times}}\to\mathbb{R}^{d}\) for all \(k\geq 1,\) and \(B_{2,i}^{0}\in\mathbb{R}^{d}\) for all \(i\in[n]\)
3. \(L_{i}^{k}\,:\,\underbrace{\mathbb{R}^{d}\times\dots\times\mathbb{R}^{d}}_{k+1 \text{ times}}\to\mathbb{R}^{d}\) for all \(k\geq 0\) and for all \(i\in[n].\)
4. \(\text{supp}\left(x^{k}\right)\subseteq\bigcup_{j=1}^{k}\bigcup_{i=1}^{n} \text{supp}\left(g_{i}^{j}\right),\,\text{supp}\left(s_{i}^{k}\right) \subseteq\bigcup_{j=1}^{k}\bigcup_{i=1}^{n}\text{supp}\left(g_{i}^{j}\right).\) For all \(\hat{g}_{i,1}^{k+1},\hat{g}_{i,2}^{k+1},\dots\) such that \[\text{supp}\left(\hat{g}_{i,1}^{k+1}\right) \subseteq\bigcup_{j=0}^{k}\text{supp}\left(\bar{s}_{i}^{j} \right),\] (37) \[\text{supp}\left(\hat{g}_{i,2}^{k+1}\right) \subseteq\bigcup_{j=0}^{k}\text{supp}\left(\bar{s}_{i}^{j} \right)\bigcup\text{supp}(\nabla f_{i}(\hat{g}_{i,1}^{k+1})),\] \[\text{supp}\left(\hat{g}_{i,3}^{k+1}\right) \subseteq\bigcup_{j=0}^{k}\text{supp}\left(\bar{s}_{i}^{j} \right)\bigcup\text{supp}(\nabla f_{i}(\hat{g}_{i,1}^{k+1}))\bigcup\text{ supp}(\nabla f_{i}(\hat{g}_{i,2}^{k+1})),\] \[\dots\] we have \[\text{supp}\left(g_{i}^{k+1}\right) \subseteq\bigcup_{j=1}^{\infty}\text{supp}\left(\hat{g}_{i,j}^{k+1} \right),\] (38) for all \(k\in\mathbb{N}_{0}\) and for all \(i\in[n],\) where \(\text{supp}(x):=\{i\in[d]\,|\,x_{i}\neq 0\}.\)

We denote the set of all algorithms that satisfy these properties by \(\mathcal{A}_{\text{\tiny{\it{\it{\it{\it{\it{\it{\it{\it{\it{\it{\it{\it{ \it{\it{\it{\it{\itit{\itit{\itit{\itit{\it{ it{ it                                            {{{{{{{{{{{{{{{{{{{{                                                                             }}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\      {\\\\\\\\\\\\\ \ \ \ \ \ \ \ \ \ \ \\ Next, we define

\[F_{i}(x):=\begin{cases}-\Psi(1)\Phi([x]_{1})+\sum_{2\leq j\leq T\text{ and }(j-1)\bmod n=0}\left(\Psi(-[x]_{j-1})\Phi(-[x]_{j})-\Psi([x]_{j-1})\Phi([x]_{j}) \right),&i=1\\ \sum_{2\leq j\leq T\text{ and }(j-1)\bmod n=i-1}^{T}\left(\Psi(-[x]_{j-1})\Phi(-[x]_{j})- \Psi([x]_{j-1})\Phi([x]_{j})\right),&i>1\end{cases}\]

and

\[f_{i}(x):=\frac{nL\lambda^{2}}{l_{1}}F_{i}\left(\frac{x}{\lambda}\right).\]

The idea is that we take the first block from (36) to the first worker, the second block to the second worker,..., \((n+1)^{\text{th}}\) block to the first worker, and so on. Then, one can show that

\[\frac{1}{n}\sum_{i=1}^{n}f_{i}(x)=f(x).\]

Using Lemma G.1, we obtain

\[\left\|\nabla f(x)\right\|^{2}=\frac{L^{2}\lambda^{2}}{l_{1}^{2}}\left\|\nabla F _{T}\left(\frac{x}{\lambda}\right)\right\|^{2}>\frac{L^{2}\lambda^{2}}{l_{1}^ {2}}\mathbbm{1}[\text{prog}(x)<T].\] (40)

The functions \(f_{i}\) are _zero-chain_(Arjevani et al., 2022): for all \(i\in[n]\), if \(\text{prog}(x)=j\) and \((j\bmod n)+1=i\), then \(\text{prog}(\nabla f_{i}(x))\leq j+1\), and for all \(i\in[n]\), if \(\text{prog}(x)=j\) and \((j\bmod n)+1\neq i\), then \(\text{prog}(\nabla f_{i}(x))\leq j\). Using the zero-chain property and the fact that we consider the family of zero-respecting algorithms:

1. The first non-zero coordinate can be discovered only by the first worker.
2. Assume that \(\max_{j=1}^{k}\max_{i=1}^{n}\text{prog}\left(g_{i}^{j}\right)=j\geq 1\). An algorithm can discover _one_ new non-zero coordinate in the \((j+1)^{\text{th}}\) position only if the \((j\bmod n+1)^{\text{th}}\) worker gets a _non-zero_\(j^{\text{th}}\) coordinate from _the server_. This is by the construction of the functions \(f_{i}\). Note that for \(n\geq 2\), one worker cannot discover two _consecutive_ coordinates.

Let us define

\[\xi^{j}= \mathbb{I}[\text{In the $j^{\text{th}}$ iteration, the coordinate with index $\bar{p}\equiv\underset{j=1}{\overset{k}{\max}}\max_{i=1}^{n}\text{prog}\left(g_{ i}^{j}\right)$ is not zeroed out in Line 5}\] \[\text{of Protocol 4 to the worker with index $(\bar{p}\bmod n+1)$ AND $T-1\geq\bar{p}\geq 1$}\quad(\bar{p}=0\text{ if }k=0).\]

Then, we have

\[\mathbb{P}\left(\text{prog}(x^{k})\geq T\right)\leq\mathbb{P}\left(\sum_{j=0} ^{k-1}\xi^{j}\geq T-1\right).\]

Assume that \(\mathcal{G}_{j}\) is the \(\sigma\)-algebra generated by all randomness up to the \(j^{\text{th}}\) iteration (inclusive). Then, \(\xi^{j}\) is \(\mathcal{G}_{j}\)-measurable, and, by the construction of Line 5 of Protocol 4, \(\mathbb{P}\left(\xi^{j+1}=1\big{|}\mathcal{G}_{j}\right)\leq p\), where we also use the assumption of the theorem that the sets of random variables are mutually independent. Using the standard approach with the Chernoff method (Arjevani et al., 2022; Lu and De Sa, 2021; Huang et al., 2022), one can show that

\[\mathbb{P}\left(\sum_{j=0}^{k-1}\xi^{j}\geq T-1\right)\leq\rho\]

for all

\[k\leq\frac{T-1-\log\frac{1}{\rho}}{2p}\]

and \(\rho\in(0,1]\). Therefore, we get

\[\mathbb{P}\left(\text{prog}(x^{k})\geq T\right)\leq\rho\] (41)for all

\[k\leq\frac{T-1-\log\frac{1}{\rho}}{2p}.\]

Using (40), we have

\[\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2}\right]>2\varepsilon\mathbb{P }\left(\left\|\nabla f(x^{k})\right\|^{2}>2\varepsilon\right)\geq 2\varepsilon \mathbb{P}\left(\frac{L^{2}\lambda^{2}}{l_{1}^{2}}\mathbb{1}[\text{prog}(x)<T] \geq 2\varepsilon\right).\]

Let us take \(\lambda=\frac{\sqrt{2\varepsilon}l_{1}}{L}\). Then

\[\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2}\right]>2\varepsilon \mathbb{P}\left(\frac{L^{2}\lambda^{2}}{l_{1}^{2}}\mathbb{1}[\text{prog}(x)<T] \geq 2\varepsilon\right)=2\varepsilon\mathbb{P}\left(\text{prog}(x)<T\right).\] (42)

From (41) with \(\rho=\frac{1}{2}\), we get

\[\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2}\right]>2\varepsilon\mathbb{ P}\left(\text{prog}(x)<T\right)\geq\varepsilon\] (43)

for all

\[k\leq\frac{T-1-\log 2}{2p}.\]

From (39), one can conclude that

\[T=\left\lfloor\frac{L\delta^{0}l_{1}}{2\varepsilon l_{1}^{2}\Delta^{0}} \right\rfloor.\]

By the theorem's assumption, \(L\delta^{0}\geq\bar{c}\varepsilon\). One can choose a universal constant \(\bar{c}\) such that (42) holds for

\[k\leq\Theta\left(\frac{T}{p}\right)=\Theta\left(\frac{L\delta^{0}}{\varepsilon p }\right),\]

where \(\Theta\) hides only a universal constant. 

### Compressed communication with independent compressors

Protocol 5 is exactly the same as Protocol 4 except for Line 5 and describes the family of methods that send compressed vectors from the server to the workers.

```
1:Input: functions \(f_{1},\ldots,f_{n}\in\mathcal{F},\) algorithm \(A\), compressors \(\mathcal{C}_{1},\ldots,\mathcal{C}_{n}\)
2:for\(k=0,\ldots,\infty\)do
3: Server calculates a new point: \(x^{k}=B_{1}^{k}(g_{1}^{1},\ldots,g_{1}^{k},\ldots,g_{n}^{1},\ldots,g_{n}^{k})\)
4: Server aggregates all available information: \(s_{i}^{k}=B_{2,i}^{k}(g_{1}^{1},\ldots,g_{1}^{k},\ldots,g_{n}^{1},\ldots,g_{n} ^{k})\)
5: Server sends compressed vectors \(\bar{s}_{i}^{k}=\mathcal{C}_{i}(s_{i}^{k})\) to the workers
6: Workers aggregate all available local information and calculate gradients: \(g_{i}^{k+1}=L_{i}^{k}(\bar{s}_{i}^{0},\ldots,\bar{s}_{i}^{k})\)
7: (\(L_{i}^{k}\) has access to the gradient oracle of \(f_{i}\) and can call it as many times as it wants according to the rules (37) and (38))
8: Workers send \(g_{i}^{k+1}\) to the server
9:endfor ```

**Protocol 5** Protocol with Compressors

**Theorem G.5**.: _Consider Protocol 5. Let \(\omega\geq 0,L,\delta^{0},\varepsilon>0,n\geq 2\) be any numbers such that \(\bar{c}\varepsilon<L\delta^{0}\). Then for any algorithm \(A\in\mathcal{A}_{\text{ar}},\) there exists a function \(f\in\mathcal{F}_{\delta^{0},L},\) functions \(f_{1},\ldots,f_{n}\) such that \(f=\frac{1}{n}\sum_{i=1}^{n}f_{i},\) and i.i.d. compressors \(\mathcal{C}_{1},\ldots,\mathcal{C}_{n}\in\mathbb{U}(\omega)\) such that \(\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2}\right]>\varepsilon\) for all_

\[k\leq\hat{c}\frac{(\omega+1)L\delta^{0}}{\varepsilon}.\]

_The quantities \(\bar{c}\) and \(\hat{c}\) are universal constants._Proof.: We can use the result of Theorem G.4. It is sufficient to construct an appropriate compressor. Let us define \(p:=\nicefrac{{1}}{{\omega+1}}\). We define the following compressor:

\[[\mathcal{C}(x)]_{j}:=\begin{cases}\frac{1}{p}x_{j},&j\in S,\\ 0,&j\not\in S,\end{cases}\quad\forall j\in[T],\]

where \(S\) is a random subset of \([T]\) and each element from \([T]\) appears with probability \(p\) independently. Then, \(\mathcal{C}\) is unbiased:

\[\mathbb{E}_{S}\left[[\mathcal{C}(x)]_{j}\right]=x_{j}\quad\forall j\in \mathbb{R}^{T}\]

and

\[\mathbb{E}_{S}\left[\left\|\mathcal{C}(x)\right\|^{2}\right]=\mathbb{E}_{S} \left[\sum_{j=1}^{nT}\mathbbm{1}\left[j\in S\right]\frac{1}{p^{2}}x_{j}^{2} \right]=\sum_{j=1}^{nT}\mathbb{P}\left(j\in S\right)\frac{1}{p^{2}}x_{j}^{2}= \sum_{j=1}^{nT}\frac{1}{p}x_{j}^{2}=\left(\omega+1\right)\left\|x\right\|^{2}.\]

Therefore, we get \(\mathcal{C}\in\mathbb{U}(\omega)\). Let \(\mathcal{C}_{i}\) be i.i.d. instantiations of \(\mathcal{C}\) for all \(i\in[n]\). Since \(\mathcal{C}\) is a sparsifier as in Line 5 of Protocol 4, we can use Theorem G.4 with \(p=\nicefrac{{1}}{{\omega+1}}\) to finish the proof.

## Appendix H Useful Identities and Inequalities

For all \(x,y,x_{1},\ldots,x_{m}\in\mathbb{R}^{d}\), \(s>0\) and \(\alpha\in(0,1]\), we have:

\[\left\|x+y\right\|^{2} \leq\left(1+s\right)\left\|x\right\|^{2}+\left(1+s^{-1}\right) \left\|y\right\|^{2},\] (44) \[\left\|\sum_{i=1}^{m}x_{i}\right\|^{2} \leq m\left(\sum_{i=1}^{m}\left\|x_{i}\right\|^{2}\right),\] (45) \[\left(1-\alpha\right)\left(1+\frac{\alpha}{2}\right) \leq 1-\frac{\alpha}{2},\] (46) \[\left(1-\alpha\right)\left(1+\frac{2}{\alpha}\right) \leq\frac{2}{\alpha}.\] (47)

**Variance decomposition:** For any random vector \(X\in\mathbb{R}^{d}\) and any non-random vector \(c\in\mathbb{R}^{d}\), we have

\[\mathbb{E}\left[\left\|X-c\right\|^{2}\right]=\mathbb{E}\left[\left\|X- \mathbb{E}\left[X\right]\right\|^{2}\right]+\left\|\mathbb{E}\left[X\right]-c \right\|^{2}.\] (48)

**Tower property:** For any random variables \(X\) and \(Y\), we have

\[\mathbb{E}\left[\mathbb{E}\left[X\,\middle|\,Y\right]\right]=\mathbb{E}\left[X \right].\] (49)

**Jensen's inequality:** If \(f\) is a convex function and \(X\) is a random variable, then

\[\mathbb{E}\left[f(X)\right]\geq f\left(\mathbb{E}\left[X\right]\right).\] (50)

**Lemma H.1** (Lemma \(2\) of Li et al. (2021)).: _Suppose that function \(f\) is \(L\)-smooth and let \(x^{t+1}=x^{t}-\gamma g^{t}\). Then for any \(g^{t}\in\mathbb{R}^{d}\) and \(\gamma>0\), we have_

\[f(x^{t+1})\leq f(x^{t})-\frac{\gamma}{2}\left\|\nabla f(x^{t})\right\|^{2}- \left(\frac{1}{2\gamma}-\frac{L}{2}\right)\left\|x^{t+1}-x^{t}\right\|^{2}+ \frac{\gamma}{2}\left\|g^{t}-\nabla f(x^{t})\right\|^{2}.\]

**Lemma H.2** (Lemma \(5\) of Richtarik et al. (2021)).: _Let \(a,b>0\). If \(0\leq\gamma\leq\frac{1}{\sqrt{a+b}}\), then \(a\gamma^{2}+b\gamma\leq 1\). Moreover, the bound is tight up to the factor of \(2\) since \(\frac{1}{\sqrt{a}+b}\leq\min\left\{\frac{1}{\sqrt{a}},\frac{1}{b}\right\}\leq \frac{2}{\sqrt{a+b}}\)._

## Appendix I Notation

\begin{table}
\begin{tabular}{|c|l|} \hline \multicolumn{2}{|c|}{Algorithms} \\ \hline \(n\) & number of workers/nodes/clients/devices \\ \(\gamma\) & stepsize \\ \(\mathcal{C}_{1}^{t},\ldots\mathcal{C}_{n}^{t}\) & Server-to-workers (primal) compressors \\ \(\mathcal{Q}_{1}^{t},\ldots\mathcal{Q}_{n}^{t}\) & Workers-to-server (dual) compressors \\ \(\omega_{P},\omega_{D}\) & Parameters of server-to-workers (primal) and workers-to-server (dual) compressors \\ \(\theta\) & Correlated compressors parameter (Definition A.3) \\ \(\beta\) & Momentum parameter (see Algorithm 2) \\ \hline \multicolumn{2}{|c|}{Definitions} \\ \hline \(\mathbb{U}(\omega)\) & The family of unbiased compressors with parameter \(\omega\) (Definition 1.4) \\ \(\mathbb{P}(\theta)\) & The family of correlated compressors with parameter \(\theta\) (Definition A.3) \\ \(L\) & Smoothness parameter of \(f\) (Assumption 1.1 ) \\ \(L_{i}\) & Smoothness parameter of \(f_{i}\) (Assumption 1.5) \\ \(L_{A}\), \(L_{B}\) & Parameters from Assumption 4.2 \\ \hline \multicolumn{2}{|c|}{Notation} \\ \hline \multicolumn{2}{|c|}{\([k]=\{1,\ldots,k\}\) for any positive integer \(k\)} \\ \(\mathbb{E}_{t}\left[\cdot\right]\) - expectation conditioned on the first \(t\) iterations \\ \(\delta^{t}:=f(x^{t})-f^{*}\) \\ \(\widetilde{L}^{2}:=\frac{1}{n}\sum_{i=1}^{n}L_{i}^{2}\), \(L_{\max}:=\max_{i\in[n]}L_{i}\) \\ \(w^{t}:=\nicefrac{{1}}{{n}}\sum_{i=1}^{n}w_{i}^{t}\) \\ \(g^{t}:=\nicefrac{{1}}{{n}}\sum_{i=1}^{n}g_{i}^{t}\) \\ \(z^{t}:=\nicefrac{{1}}{{n}}\sum_{i=1}^{n}z_{i}^{t}\) \\ \hline \end{tabular}
\end{table}
Table 2: Frequently used notation.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Sections 3, 4, and 5 Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Sections 4.6 and 5.1 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The assumptions and the proofs are in Section 1.1 in the appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section F Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: In the supplementary materials.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section F Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We run experiments with different seeds and plot averages to reduce noise factors (see the description in Sections F.1 and F.2). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Section F Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the code of ethics, and we are confident that our paper is in compliance with it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work considers a mathematical problem from machine learning. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Section F Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: In the supplementary materials. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.