# Dinomaly: The _Less Is More_ Philosophy in Multi-Class Unsupervised Anomaly Detection

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent studies highlighted a practical setting of unsupervised anomaly detection (UAD) that builds a unified model for multi-class images, serving as an alternative to the conventional one-class-one-model setup. Despite various advancements addressing this challenging task, the detection performance under the multi-class setting still lags far behind state-of-the-art class-separated models. Our research aims to bridge this substantial performance gap. In this paper, we introduce a minimalistic reconstruction-based anomaly detection framework, namely Dinomaly, which leverages pure Transformer architectures without relying on complex designs, additional modules, or specialized tricks. Given this powerful framework consisted of only Attentions and MLPs, we found four simple components that are essential to multi-class anomaly detection: (1) _Foundation Transformers_ that extracts universal and discriminative features, (2) _Noisy Bottleneck_ where pre-existing Dropouts do all the noise injection tricks, (3) _Linear Attention_ that naturally cannot focus, and (4) _Loose Reconstruction_ that does not force layer-to-layer and point-by-point reconstruction. Extensive experiments are conducted across three popular anomaly detection benchmarks including MVTec-AD, VisA, and the recently released Real-IAD. Our proposed Dinomaly achieves impressive image AUROC of 99.6%, 98.7%, and 89.3% on the three datasets respectively, which is not only superior to state-of-the-art multi-class UAD methods, but also surpasses the most advanced class-separated UAD records.

## 1 Introduction

Unsupervised anomaly detection (UAD) aims to detect abnormal patterns from normal images and further localize the anomalous regions. Because of the diversity of potential anomalies and their scarcity, this task is proposed to model the accessible training sets containing only normal samples as an unsupervised paradigm. UAD has a wide range of applications, e.g., industrial defect detection, medical disease screening, and video surveillance, addressing the difficulty of collecting and labeling all possible anomalies in these scenarios.

Efforts on UAD attempt to learn the distribution of available normal samples. Most advanced methods utilize networks pre-trained on large-scale datasets, e.g. ImageNet [1], for extracting discriminative and informative feature representations. Specifically, _Feature reconstruction_[2; 3; 4] and _feature distillation_ methods [5; 6] are proposed to reconstruct features of pre-trained encoders, based on the hypothesis that the networks trained on normal images can only construct normal regions, but fail for unseen anomalous regions. _Feature statistics_ methods [7; 8; 9] memorize and model all anomaly-free features extracted from pre-trained networks in training, and compare them with the test features during inference. _Pseudo-anomaly_ methods [10; 11] generate pseudo defects or noiseson normal images or features to imitate anomalies, converting UAD to supervised classification [11] or segmentation tasks [10; 12].

Conventional works on UAD build a separate model for each object category, as shown in Figure 1(a). However, this one-class-one-model setting entails substantial storage overhead for saving models [3], especially when the application scenario necessitates a large number of object classes. For UAD methods, a compact boundary of normal patterns is vital to distinguish anomalies. Once the intra-normal patterns become exceedingly complicated due to various classes, the corresponding distribution becomes challenging to measure, consequently harming the detection performance. Recently, UniAD [3] and successive studies have been proposed to train a unified model for multi-class anomaly detection (MUAD), as shown in Figure 1(b). Under this setting, the "identity mapping" that directly copies the input as the output regardless of normal or anomaly harms the performance of conventional methods [3]. This phenomenon is caused by the diversity of multi-class normal patterns that drive the network to generalize on unseen patterns.

Within two years, a number of methods have been proposed to address MUAD, such as neighbor-masked attention [3], synthetic anomalies [23], feature jitter [3], vector quantization [24], diffusion model [25; 26], and state space model (Mamba) [19]. However, there is still a non-negligible performance gap between the state-of-the-art (SoTA) MUAD methods and class-separated UAD methods, restricting the practicability of implementing unified models, as shown in Figure 1(c). In addition, previous methods employ modules and architectures delicately designed, which may not be straightforward, and consequently suffer from limited universality and usability.

In this work, we aim to catch up with the performance of class-separated anomaly detection models using a multi-class unified model, namely Dinomaly. To begin with, we build a reconstruction-based UAD framework that consists of only vanilla Transformer blocks [27], i.e. Self-Attentis and Multi-Layer Perceptrons (MLPs). Within this framework, we propose four simple but essential elements that boost Dinomaly to perform equal to or better than SoTA conventional class-separated models. First, we show that self-supervised pre-trained Vision Transformers (ViT) [28], especially the DINO family [29; 30], serve as powerful feature encoders to extract discriminative representations as

Figure 1: Setting and Performance of UAD and multi-class UAD (MUAD). (a) Task setting of class-separated UAD. (b) Task setting of MUAD. (c) Comparison of Dinomaly and previous SoTA methods [13; 14; 15; 16; 8; 17; 18; 19] on MVTec-AD [20], VisA [21], and Real-IAD [22].

reconstruction objects. Second, as an alternative to carefully designed pseudo anomaly and feature noise, we propose to activate the out-of-the-box Dropout in an MLP to prevent the network from restoring both normal and anomalous patterns, which is previously referred to as identity mapping. Third, we propose to utilize the "side effect" of Linear Attention (a computation-efficient counterpart of Softmax Attention) that makes it hard to focus on local regions, to further alleviate the issue of identity mapping. Fourth, previous methods adopt layer-to-layer and region-by-region reconstruction schemes, distilling a decoder that can well mimic the behavior of the encoder even for anomalous regions. Therefore, we propose to loosen the reconstruction constraints by grouping multiple layers as a whole and discarding well-reconstructed regions during optimization.

To validate the effectiveness of the proposed Dinomaly under MUAD setting, we conduct extensive experiments on three widely used industrial defect detection benchmarks, i.e., MVTec AD [20] (15 classes), VisA [21] (12 classes), and recently released Real-IAD (30 classes). Notably, we achieve unprecedented image-level AUROC of 99.6%, 98.7%, and 89.3% on MVTec AD, VisA, and Real-IAD, respectively, which surpasses previous SoTA methods by a large margin.

Related works are presented in Appendix A.1.

## 2 Method

### Dinomaly Framework

_"What I cannot create, I do not understand"_--Richer Feynman

The ability to recognize anomalies from what we know is an innate human capability, serving as a vital pathway for us to explore the world. Similarly, we construct a reconstruction-based framework that relies on the epistemic characteristic of artificial neural networks. Dinomaly consists of an encoder, a bottleneck, and a reconstruction decoder, as shown in Figure 2. Without loss of generality, a standard ViT-Base/14 network [28] with 12 Transformer layers is used as the encoder, extracting informative feature maps with different semantic scales. The bottleneck is a simple MLP (a.k.a. feed-forward network, FFN) that collects the feature representations of the encoder's 8 middle-level layers. The decoder is similar to the encoder, consisting of 8 Transformer layers. During training, the decoder learns to reconstruct the middle-level features of the encoder by maximizing the cosine similarity between feature maps. During inference, the decoder is expected to reconstruct normal regions of feature maps but fails for anomalous regions as it has never seen such samples.

**Foundation Transformers.** Foundation models, especially ViTs [28; 31] pre-trained on large-scale datasets, serve as a basis and starting point for specific computer vision tasks. Such networks employ self-supervised learning schemes such as contrastive learning (MoCov3 [32], DINO [29]), masked image modeling (MAE [33], SimMIM [34], BEiT [35]), and their combination (iBOT [36], DINOv2 [30]), producing universal features suitable for image-level visual tasks (image classification, instance retrieval) and pixel-level visual tasks (depth estimation, semantic segmentation).

Because of the lack of supervision in UAD, most advanced methods adopt pre-trained networks to extract discriminative features. Recent works [37; 17; 38] have discovered the advantage of robust and universal features of self-supervised models over domain-specific ImageNet features in anomaly detection tasks. In this work, we further utilize the up-to-date Transformer foundation, i.e., DINOv2 with registers [39], as the encoder of Dinomaly.

### Noisy Bottleneck.

_"Dropout is all you need."_

Generalization ability is a merit of neural networks, allowing them to perform equally well on unseen test sets. However, generalization is not so wanted in the context of unsupervised anomaly detection that leverages the epistemic nature of neural networks. With the increasing diversity of images and their patterns due to multi-class UAD settings, the decoder can generalize its reconstruction ability to unseen anomalous samples, resulting in the failure of anomaly detection using reconstruction error. This phenomenon is called "identity mapping" in previous works of literature [3; 23; 18].

A direct solution for identity mapping is to shift "reconstruction" to "restoration". Specifically, instead of directly reconstructing the normal images or features given normal inputs, previous works propose to add perturbations as pseudo anomalies on input images [10; 40; 12] or feature representations [3; 25] during network forward propagation; meanwhile, still let the decoder restore anomaly-free images or features, formulating a denoising-like framework. However, such methods employ heuristic and hand-crafted anomaly generation strategies, that are not universal across domains, datasets, and methods.

In this work, we propose to activate the pre-existing Dropout in an MLP layer. Dropout, a popular network element introduced by Hinton et al. [41] in 2014 to prevent overfitting, flourished in nearly all neural network architectures to the present day, including Transformers. In Dinomaly, Dropout is used to discard neural activations in the MLP bottleneck randomly. Instead of alleviating overfitting, the role of Dropout in Dinomaly can be explained as feature noise and pseudo feature anomaly. Although the decoder takes noisy features during training, it is encouraged to restore clean features from the encoder. Without introducing any novel modules, this paradigm forces the decoder to restore normal features given a test image with anomalies, in turn, mitigating identical mapping.

### Unfocused Linear Attention.

_"One man's poison is another man's meat"_

**Softmax Attention** is the key mechanism of Transformers, allowing the model to attend to different parts of its input token sequence. Formally, given an input sequence \(\mathbf{X}\in\mathbb{R}^{N\times d}\) with length \(N\), Attention first transforms it into three matrices: the query matrix \(\mathbf{Q}\in\mathbb{R}^{N\times d}\), the key matrix \(\mathbf{K}\in\mathbb{R}^{N\times d}\), and the value matrix \(\mathbf{V}\in\mathbb{R}^{N\times d}\):

\[\mathbf{Q}=\mathbf{X}\mathbf{W}^{Q}\:,\mathbf{K}=\mathbf{X}\mathbf{W}^{K}\:, \mathbf{V}=\mathbf{X}\mathbf{W}^{V}\:,\] (1)

where \(\mathbf{W}^{Q},\mathbf{W}^{K},\mathbf{W}^{V}\in\mathbb{R}^{d\times d}\) are learnable parameters. By computing the attention map by the query-key similarity, the output of Attention is given as: 1

Footnote 1: The full form of Attention is \(\text{Softmax}(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d}})\mathbf{V}\). The constant denominator is omitted for narrative simplicity. The multi-head mechanism that concatenates multiple Attentions is also omitted.

\[\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{ Softmax}(\mathbf{Q}\mathbf{K}^{T})\mathbf{V}\:.\] (2)

Figure 2: The framework of Dinomaly, built by pure Transformer building blocks.

Because the attention map is obtained by computing the similarity between all query-key pairs followed by row-wise Softmax, the computation complexity is \(\mathcal{O}(N^{2}d)\).

**Linear Attention** was proposed as a promising alternative to reduce the computation complexity of vanilla Softmax Attention concerning the number of tokens [42]. By substituting Softmax operation with a simple activation function \(\phi(\cdot)\) (usually \(\phi(x)=\text{elu}(x)+1\)), we can change the computation order from \((\mathbf{Q}\mathbf{K}^{T})\mathbf{V}\) to \(\mathbf{Q}(\mathbf{K}^{T}\mathbf{V})\). Formally, Linear Attention is given as:

\[\text{LinearAttention}(\mathbf{Q},\mathbf{K},\mathbf{V})=(\phi(\mathbf{Q}) \phi(\mathbf{K}^{T}))\mathbf{V}=\phi(\mathbf{Q})(\phi(\mathbf{K}^{T})\mathbf{ V})\;,\] (3)

where the computation complexity is reduced to \(\mathcal{O}(Nd^{2})\). The trade-off between complexity and expressiveness is a dilemma. Previous studies [43; 44] attribute Linear Attention's performance degradation on supervised tasks to its incompetence in focusing. Due to the absence of non-linear attention reweighting by Softmax operation, Linear Attention cannot concentrate on important regions related to the query, such as foreground and neighbors.

Back to MUAD, previous methods [3; 24] suggest adopting Attentions instead of Convolutions because Convolutions can easily learn identical mappings. Nevertheless, both operations are in danger of forming identity mapping by over-concentrating on corresponding input locations for producing the outputs:

\[\text{Conv Kernel}=\begin{bmatrix}0&0&0\\ 0&1&0\\ 0&0&0\end{bmatrix}\;,\qquad\qquad\text{Attention Map}=\begin{bmatrix}1&0&0&0\\ 0&1&0&0\\ 0&0&1&0\\ 0&0&0&1\end{bmatrix}\;.\]

In Dinomaly, we turn to leverage the "unfocusing ability" of Linear Attention. In order to probe how Attentions propagate information, we train two variants of Dinomaly using vanilla Softmax Attention or Linear Attention as the spatial mixer in the decoder and visualize their attention maps. As shown in Figure 3, Softmax Attention tends to focus on the exact region of the query, while Linear Attention spreads its attention across the whole image. This implies that Linear Attention, forced by its incompetence to focus, utilizes more long-range information to restore features at each position, reducing the chance of passing identical information of unseen patterns to the next layer

Figure 3: The decoder attention map (min-max to 0-1 for visualization) of Dinomaly with vanilla Softmax Attention _vs._ Linear Attention.

during reconstruction. Of course, employing Linear Attention also benefits from less computation, free of performance drop.

### Loose Reconstruction

_"The tighter you squeeze, the less you have."_

**Loose Constraint.** Pioneers of feature-reconstruction/distillation UAD methods [5; 2] are inspired by knowledge distillation [45]. Most reconstruction-based methods distill specific encoder layers (e.g. 3 last layers of 3 ResNet stages) by the corresponding decoder layers [2; 5; 17] (Figure 4(a)) or the last decoder layer [3; 4] (Figure 4(b)). Intuitively, with more encoder-decoder feature pairs (Figure 4(c)), UAD model can utilize more information in different layers to discriminate anomalies. However, according to the intuition of knowledge distillation, the student (decoder) can better mimic the behavior of the teacher (encoder) given more layer-to-layer supervision, which is harmful for UAD models that detect anomalies by encoder-decoder discrepancy. This phenomenon is also embodied as identity mapping. Thanks to the top-to-bottom consistency of columnar Transformer layers, we propose to loosen the layer-to-layer constraint by adding up all feature maps of interested layers as a whole group, as shown in Figure 4(d). This scheme can be seen as loosening the layer-to-layer correspondence, so that the decoder is allowed to act much more differently from the encoder when the input pattern is unseen. Because features of shallow layers contain low-level visual characters that are helpful for precise localization, we can further group the features into the low-semantic-level group and high-semantic-level group, as shown in Figure 4(e).

**Loose Loss.** Following the above analysis, we also loosen the point-by-point reconstruction loss function by discarding some points in the feature map. Here, we simply borrow the hard-mining global cosine loss [18] that detaches the gradients of well-restored feature points with low cosine distance during training. Let \(f_{E}\) and \(f_{D}\) denotes (grouped) feature maps of encoder and decoder:

\[\mathcal{L}_{global-hm}=\mathcal{D}_{cos}(\mathcal{F}(f_{E}),\mathcal{F}(f_{ D}))=1-\frac{\mathcal{F}(f_{E})^{T}\cdot\mathcal{F}(f_{D})}{\left\|\mathcal{F}(f_{E}) \right\|\ \left\|\mathcal{F}(f_{D})\right\|},\] (4)

\[f_{D}(h,w)=\left\{\begin{array}{l}sg(f_{D}(h,w))_{0.1},\text{ if }\mathcal{D}_{cos}(f_{D}(h,w),f_{E}(h,w))<90\%\left[ \mathcal{D}_{cos}(f_{D},f_{E})\right]_{batch}\\ f_{D}(h,w),\text{ else}\end{array}\right.,\] (5)

Figure 4: Schemes of reconstruction constraint. (a) Layer-to-layer (sparse). (b) Layer-to-cat-layer. (c) Layer-to-layer (dense). (d) Loose group-to-group, 1-group (Ours). (e) Loose group-to-group, 2-group (Ours).

where \(\mathcal{F}(\cdot)\) denotes flatten operation, \(f_{D}(h,w)\) represents the feature point at \((h,w)\), \(sg(\cdot)_{0.1}\) denotes shrink the gradient to one-tenth of the original 2, \(\mathcal{D}_{cos}(f_{D}(h,w),f_{E}(h,w))<90\%\left[\mathcal{D}_{cos}(f_{D},f_{E })\right]_{batch}\) selects 90% feature points with smaller cosine distance within a batch. Total loss is the mean \(\mathcal{L}_{global-hm}\) of all encoder-decoder feature pairs.

Footnote 2: Complete stop-gradient causes optimization instability occasionally.

## 3 Experiments

### Experimental Settings

**Datasets. MVTec-AD**[20] contains 15 objects (5 texture classes and 10 object classes) with a total of 3,629 normal images as the training set and 1,725 images as the test set (467 normal, 1258 anomalous). **VisA**[21] contains 12 objects. Training and test sets are split following the official splitting, resulting in 8,659 normal images in the training set and 2,162 images in the test set (962 normal, 1,200 anomalous). **Real-IAD**[22] is a large UAD dataset recently released, containing 30 distinct objects. We follow the official splitting that includes all views, resulting in 36,465 normal images in the training set and 114,585 images in the test set (63,256 normal, 51,329 anomalous).

**Metrics.** Following prior works [19; 17], we adopt 7 evaluation metrics. Image-level anomaly detection performance is measured by the Area Under the Receiver Operator Curve (AUROC), Average Precision (AP), and \(F_{1}\) score under optimal threshold (\(F_{1}\)-max). Pixel-level anomaly localization is measured by AUROC, AP, \(F_{1}\)-max and the Area Under the Per-Region-Overlap (AUPRO). The results of a dataset is the average of all classes.

**Implementation Details.** ViT-Base/14 (patchsize=14) pre-trained by DINOV2-R [39] is used as the encoder by default. The drop rate of Noisy Bottleneck is 0.2 by default and increases to 0.4 on the diverse Real-IAD. Loose constraint with 2 groups is employed, and the anomaly map is given by the mean per-point cosine distance of the 2 groups. The input image is first resized to \(448^{2}\) and then center-cropped to \(392^{2}\), so the feature map (\(28^{2}\)) is large enough for anomaly localization. StableAdamW optimizer [46] with AMSGrad [47] (more stable than AdamW [48] in training) is utilized with \(lr\)=2e-3, \(\beta\)=(0.9,0.999) and \(wd\)=1e-4. The network is trained for 10,000 iterations (steps) on MVTec-AD and VisA, and 50,000 iterations on Real-IAD. More details are available in Appendix A.2.

### Comparison to Multi-Class UAD SoTAs

We compare the proposed Dinomaly with the most advanced methods. Among them, RD4AD [2] based on feature reconstruction, SimpleNet [13] based on feature-level pseudo-anomaly, and DeST-Seg [12] based on feature reconstruction & pseudo anomaly are designed for conventional class-separated UAD settings. UniAD based on feature reconstruction, ReContrast [18] based on contrastive reconstruction, ViTAD [17] based on feature reconstruction & Transformer, DiAD [49] based on Diffusion reconstruction, and MamboAD [19] based on feature reconstruction & Mamboa are designed for MUAD settings. Notably, ViTAD and MamboAD are contemporary _arxiv_ preprints released within months. The intuitive comparison is already presented in Figure 1.

Experimental results are presented in Table 1, where Dinomaly surpasses compared methods by a large margin on all datasets and all metrics. On the most widely used MVTec-AD, Dinomaly produces image-level performance of **99.6/99.8/99.0** and pixel-level performance of **98.4/69.3/69.2/94.8**, outperforming previous SoTAs by _1.0/0.2/1.2_ and _0.7/9.1/7.7/1.6_. This result declares that the image-level performance on the MVTec-AD dataset is nearly saturated under the MUAD setting. On the popular VisA, Dinomaly achieves image-level performance of **98.7/98.9/96.2** and pixel-level performance of **98.7/53.2/55.7/94.5**, outperforming previous SoTAs by _3.2/2.5/4.2_ and _0.2/5.3/5.1/2.6_. On the Real-IAD that contains 30 classes, each with 5 camera views, we produce image-level and pixel-level performance of **89.3/86.8/80.2** and **98.8/42.8/47.1/93.9**, outperforming previous SoTAs by _3.0/2.2/3.2_ and _0.3/4.9/5.4/3.4_, indicating our scalability to extremely complex scenarios. Per-class performances and qualitative visualization are presented in Appendix A.5 and A.6. In addition, adopting a larger backbone further improves the above performances, as presented in Table A2.

### Comparison to Class-Separated UAD SoTAs

We also compare our Dinomaly with class-separated SoTAs, as shown in Table 2. On MVTec-AD and VisA, our Dinomaly under MUAD setting is comparable to conventional SoTAs that build individual models for each class [2; 13; 8; 15]. In addition, Dinomaly is subjected to nearly no performance drop compared to its class-separated counterpart on these datasets. On the complicated Real-IAD that involves more images, classes, and views, class-separated Dinomaly sets new SoTA records. Multi-class Dinomaly presents moderate performance drop but is still comparable to class-separated SoTAs.

### Ablation Study

**Overall Ablation.** We conduct experiments to verify the effectiveness of the proposed elements, i.e., Noisy Bottleneck (NB), Linear Attention (LA), Loose Constraint (LC), and Loose Loss (LL). The already-powerful baseline is Dinomaly with noiseless MLP bottleneck, Softmax Attention, dense layer-to-layer supervision, and global cosine loss [18]. Results on MVTec-AD and VisA are shown in Table 3 and Table A1, respectively. NB and LL can directly contribute to the model performance. LA and LC boost the performance with the presence of NB. The use of LC is not solely beneficial because

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{Image-level} & \multicolumn{3}{c}{Pixel-level} \\ \cline{3-10}  & & AUROC & AP & \(F_{1}\)-max & AUROC & AP & \(F_{1}\)-max & AUPRO \\ \hline \multirow{10}{*}{MVTec-AD [20]} & RD4AD [2] & 94.6 & 96.5 & 95.2 & 96.1 & 48.6 & 53.8 & 91.1 \\  & SimpleNet [13] & 95.3 & 98.4 & 95.8 & 96.9 & 45.9 & 49.7 & 86.5 \\  & DeSTSeg [12] & 89.2 & 95.5 & 91.6 & 93.1 & 54.3 & 50.9 & 64.8 \\  & UniAD [3]† & 96.5 & 98.8 & 96.2 & 96.8 & 43.4 & 49.5 & 90.7 \\  & ReContrast [18]† & 98.3 & 99.4 & 97.6 & 97.1 & 60.2 & 61.5 & 93.2 \\  & DiAD [49]† & 97.2 & 99.0 & 96.5 & 96.8 & 52.6 & 55.5 & 90.7 \\  & ViTAD [17]† & 98.3 & 99.4 & 97.3 & 97.7 & 55.3 & 58.7 & 91.4 \\  & MambaAD [19]† & 98.6 & 99.6 & 97.8 & 97.7 & 56.3 & 59.2 & 93.1 \\  & **Dinomaly** (Ours) & **99.6** & **99.8** & **99.0** & **98.4** & **69.3** & **69.2** & **94.8** \\ \hline \multirow{10}{*}{VisA [21]} & RD4AD [2] & 92.4 & 92.4 & 89.6 & 98.1 & 38.0 & 42.6 & 91.8 \\  & SimpleNet [13] & 87.2 & 87.0 & 81.8 & 96.8 & 34.7 & 37.8 & 81.4 \\  & DeSTSeg [12] & 88.9 & 89.0 & 85.2 & 96.1 & 39.6 & 43.4 & 67.4 \\  & UniAD [3]† & 88.8 & 90.8 & 85.8 & 98.3 & 33.7 & 39.0 & 85.5 \\  & ReContrast [18]† & 95.5 & 96.4 & 92.0 & 98.5 & 47.9 & 50.6 & 91.9 \\  & DiAD [49]† & 86.8 & 88.3 & 85.1 & 96.0 & 26.1 & 33.0 & 75.2 \\  & ViTAD [17]† & 90.5 & 91.7 & 86.3 & 98.2 & 36.6 & 41.1 & 85.1 \\  & MambaAD [19]† & 94.3 & 94.5 & 89.4 & 98.5 & 39.4 & 44.0 & 91.0 \\  & **Dinomaly** (Ours) & **98.7** & **98.9** & **96.2** & **98.7** & **53.2** & **55.7** & **94.5** \\ \hline \multirow{10}{*}{Real-IAD [22]} & RD4AD [2] & 82.4 & 79.0 & 73.9 & 97.3 & 25.0 & 32.7 & 89.6 \\  & SimpleNet [13] & 57.2 & 53.4 & 61.5 & 75.7 & 2.8 & 6.5 & 39.0 \\  & DeSTSeg [12] & 82.3 & 79.2 & 73.2 & 94.6 & 37.9 & 41.7 & 40.6 \\  & UniAD [3]† & 83.0 & 80.9 & 74.3 & 97.3 & 21.1 & 29.2 & 86.7 \\  & ReContrast [18]† & 86.4 & 84.2 & 77.4 & 97.8 & 31.6 & 38.2 & 91.8 \\  & DiAD [49]† & 75.6 & 66.4 & 69.9 & 88.0 & 2.9 & 7.1 & 58.1 \\  & ViTAD [17]† & 82.3 & 79.4 & 73.4 & 96.9 & 26.7 & 34.9 & 84.9 \\  & MambaAD [19]† & 86.3 & 84.6 & 77.0 & 98.5 & 33.0 & 38.7 & 90.5 \\  & **Dinomaly** (Ours) & **89.3** & **86.8** & **80.2** & **98.8** & **42.8** & **47.1** & **93.9** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance under **multi-class** UAD setting (%). †: method designed for MUAD.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c}{MVTec-AD [20]} & \multicolumn{3}{c}{VisA [21]} & \multicolumn{3}{c}{Real-IAD [22]} \\ \cline{2-10}  & I-AUROC & P-AP & P-AUPRO & I-AUROC & P-AP & P-AUPRO & I-AUROC & P-AP & P-AUPRO \\ \hline _Dinomaly_ (_MUAD_) & _99.6_ & _69.3_ & _94.8_ & _98.7_ & _53.2_ & _94.5_ & _89.3_ & _42.8_ & _93.9_ \\ \hline
**Dinomaly** & **99.7** & **68.9** & **95.0** & **98.9** & **50.7** & **95.1** & **92.0** & **45.2** & **95.1** \\ RD4AD [2] & 98.5 & 58.0 & 93.9 & 96.0 & 27.7 & 70.9 & 87.1 & n/a & 93.8 \\ PatchCore [8] & 99.1 & 56.1 & 93.5 & 95.1 & 40.1 & 91.2 & 89.4 & n/a & 91.5 \\ SimpleNet [13] & 99.6 & 54.8 & 90.0 & 96.8 & 36.3 & 88.7 & 88.5 & n/a & 84.6 \\ EfficientAD [15] & 99.1 & 63.8 & 93.5 & 98.1 & 40.8 & 94.0 & n/a & n/a & n/a \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance under conventional **class-separated** UAD setting (%). n/a: not available.

LC makes the reconstruction too easy without injected noise. Combining some of the proposed elements boosts the performance of the baseline, while employing them all produces the best results.
**Noisy Rates**. We conduct ablations on the discarding rate of the Dropouts in MLP bottleneck, as shown in Table 4. Experimental results demonstrate that Dinomaly is robust to different levels of dropout rate. **Reconstruction Constraint**. We quantitatively examine different reconstruction schemes presented in Figure 4. As shown in Table 5, group-to-group LC outperforms layer-to-layer supervision. On image-level metrics, 1-group LC with all layers added performs similarly to its 2-group counterpart that separates low-level and high-level layers; however, 1-group LC mixes low-level and high-level features which is harmful for anomaly localization. More ablations on scalability, input size, pre-trained foundations, etc., are presented in Appendix A.3.

## 4 Conclusion

Dinomaly, a minimalistic UAD framework, is proposed to address the under-performed MUAD models in this paper. We present four key elements in Dinomaly, i.e., Foundation Transformer, Noisy MLP Bottleneck, Linear Attention, and Loose Reconstruction, that can boost the performance under the challenging MUAD setting without fancy modules and tricks. Extensive experiments on MVTec AD, VisA, and Real-IAD demonstrate our superiority over previous model-unified multi-class models and even recent class-separated models, indicating the feasibility of implementing a unified model in complicated scenarios free of severe performance degradation.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Dropout rate} & \multicolumn{4}{c}{Image-level} & \multicolumn{4}{c}{Pixel-level} \\ \cline{2-9}  & AUROC & AP & \(F_{1}\)-max & AUROC & AP & \(F_{1}\)-max & AUPRO \\ \hline
0 (noiseless) & 98.19 & 99.55 & 98.51 & 97.55 & 63.11 & 64.39 & 93.33 \\
0.1 & 99.54 & 99.75 & 98.90 & **98.35** & **69.46** & **69.19** & 94.53 \\
0.2 \(\dagger\) & 99.60 & 99.78 & 99.04 & **98.35** & 69.29 & 69.17 & **94.79** \\
0.3 & **99.65** & **99.83** & 99.16 & 98.34 & 68.46 & 68.81 & 94.63 \\
0.4 & 99.64 & 99.80 & **99.23** & 98.22 & 67.95 & 68.33 & 94.57 \\
0.5 & 99.56 & 99.81 & 99.14 & 98.15 & 67.43 & 67.82 & 94.64 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablations of Dropout rates in Noisy Bottleneck, conducted on MVTec-AD (%). \(\dagger\): default.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{NB} & \multirow{2}{*}{LA} & \multirow{2}{*}{LC} & \multirow{2}{*}{LL} & \multirow{2}{*}{AUROC} & \multirow{2}{*}{AP} & \multirow{2}{*}{\(F_{1}\)-max} & \multicolumn{4}{c}{Pixel-level} \\ \cline{6-11}  & & & & & AUROC & AP & \(F_{1}\)-max & AUROC & AP & \(F_{1}\)-max & AUPRO \\ \hline \multirow{3}{*}{\(\checkmark\)} & \multirow{3}{*}{} & & & 98.41 & 99.09 & 97.41 & 97.18 & 62.96 & 63.82 & 92.95 \\  & & & & 99.06 & 99.54 & 98.31 & 97.62 & 66.22 & 66.70 & 93.71 \\  & & \(\checkmark\) & & 98.54 & 99.21 & 97.62 & 97.20 & 62.94 & 63.73 & 93.09 \\  & & \(\checkmark\) & & 98.35 & 99.04 & 97.43 & 97.10 & 61.05 & 62.73 & 92.60 \\  & & \(\checkmark\) & & 99.03 & 99.45 & 98.19 & 97.62 & 64.10 & 64.96 & 93.34 \\ \(\checkmark\) & \(\checkmark\) & & & 99.27 & 99.62 & 98.63 & 97.85 & 67.36 & 67.33 & 94.16 \\ \(\checkmark\) & \(\checkmark\) & & & 99.50 & 99.72 & 98.87 & 98.14 & 68.16 & 68.24 & 94.23 \\ \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & & 99.52 & 99.73 & 98.92 & 98.20 & 68.25 & 68.34 & 94.17 \\ \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & & 99.57 & **99.78** & 99.00 & 98.20 & 67.93 & 68.21 & 94.50 \\ \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & **99.60** & **99.78** & **99.04** & **98.35** & **69.29** & **69.17** & **94.79** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablations of Dinomaly elements on MVTec-AD (%). NB: Noisy Bottleneck. LA: Linear Attention. LC: Loose Constraint (2 groups). LL: Loose Loss. Results on VisA see Table A1.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Constraints} & \multicolumn{4}{c}{Image-level} & \multicolumn{4}{c}{Pixel-level} \\ \cline{2-9}  & AUROC & AP & \(F_{1}\)-max & AUROC & AP & \(F_{1}\)-max & AUPRO \\ \hline layer-to-layer (dense, every 1) & 99.39 & 99.68 & 98.73 & 98.12 & 68.55 & 68.63 & 94.28 \\ layer-to-layer (sparse, every 2) & 99.52 & 99.73 & 98.95 & 98.16 & 68.89 & 68.57 & 94.40 \\ layer-to-layer (sparse, every 4) & 99.54 & 99.77 & 99.05 & 98.04 & 66.69 & 67.17 & 94.07 \\ layer-to-cat-layer (every 2) & 99.48 & 99.71 & 99.26 & 97.83 & 62.29 & 62.91 & 93.16 \\ group-to-group (1 group) & **99.64** & **99.80** & **99.36** & 98.18 & 64.79 & 65.40 & 93.96 \\ group-to-group (2 groups)\(\dagger\) & 99.60 & 99.78 & 99.04 & **98.35** & **69.29** & **69.17** & **94.79** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Ablations of reconstruction constraint, conducted on MVTec-AD (%). \(\dagger\): default.

## References

* [1]J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009) Imagenet: a large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255. Cited by: SS1.
* [2]H. Deng and X. Li (2022) Anomaly detection via reverse distillation from one-class embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9737-9746. Cited by: SS1.

[MISSING_PAGE_POST]

* [18] J. Guo, S. Lu, L. Jia, W. Zhang, and H. Li, "Recontrast: Domain-specific anomaly detection via contrastive reconstruction," in _Advances in Neural Information Processing Systems (NeurIPS)_, vol. 36, pp. 10721-10740, 2023.
* [19] H. He, Y. Bai, J. Zhang, Q. He, H. Chen, Z. Gan, C. Wang, X. Li, G. Tian, and L. Xie, "Mambaad: Exploring state space models for multi-class unsupervised anomaly detection," _arXiv preprint arXiv:2404.06564_, 2024.
* [20] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, "Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 9592-9600, 2019.
* [21] Y. Zou, J. Jeong, L. Pemula, D. Zhang, and O. Dabeer, "Spot-the-difference self-supervised pre-training for anomaly detection and segmentation," in _European Conference on Computer Vision_, pp. 392-408, Springer, 2022.
* [22] C. Wang, W. Zhu, B.-B. Gao, Z. Gan, J. Zhang, Z. Gu, S. Qian, M. Chen, and L. Ma, "Real-iad: A real-world multi-view dataset for benchmarking versatile industrial anomaly detection," _arXiv preprint arXiv:2403.12580_, 2024.
* [23] Y. Zhao, "Omnial: A unified cnn framework for unsupervised anomaly localization," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3924-3933, 2023.
* [24] R. Lu, Y. Wu, L. Tian, D. Wang, B. Chen, X. Liu, and R. Hu, "Hierarchical vector quantized transformer for multi-class unsupervised anomaly detection," _arXiv preprint arXiv:2310.14228_, 2023.
* [25] H. Yin, G. Jiao, Q. Wu, B. F. Karlsson, B. Huang, and C. Y. Lin, "Lafite: Latent diffusion model with feature editing for unsupervised multi-class anomaly detection," _arXiv preprint arXiv:2307.08059_, 2023.
* [26] H. He, J. Zhang, H. Chen, X. Chen, Z. Li, X. Chen, Y. Wang, C. Wang, and L. Xie, "Diad: A diffusion-based framework for multi-class anomaly detection," _arXiv preprint arXiv:2312.06607_, 2023.
* [27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," in _Advances in Neural Information Processing Systems_, pp. 5998-6008, 2017.
* [28] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, _et al._, "An image is worth 16x16 words: Transformers for image recognition at scale," _arXiv preprint arXiv:2010.11929_, 2020.
* [29] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin, "Emerging properties in self-supervised vision transformers," in _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 9650-9660, 2021.
* [30] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, _et al._, "Dinov2: Learning robust visual features without supervision," _arXiv preprint arXiv:2304.07193_, 2023.
* [31] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," in _IEEE/CVF International Conference on Computer Vision_, pp. 10012-10022, 2021.
* [32] X. Chen, S. Xie, and K. He, "An empirical study of training self-supervised vision transformers," in _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 9640-9649, 2021.
* [33] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, "Masked autoencoders are scalable vision learners," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 16000-16009, 2022.

* [34] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, "Simmim: A simple framework for masked image modeling," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 9653-9663, 2022.
* [35] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei, "Beit v2: Masked image modeling with vector-quantized visual tokenizers," _arXiv preprint arXiv:2208.06366_, 2022.
* [36] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, "ibot: Image bert pre-training with online tokenizer," _arXiv preprint arXiv:2111.07832_, 2021.
* [37] T. Reiss, N. Cohen, E. Horwitz, R. Abutbul, and Y. Hoshen, "Anomaly detection requires better representations," in _European Conference on Computer Vision_, pp. 56-68, Springer, 2022.
* [38] Y. Lee, H. Lim, and H. Yoon, "Selformaly: Towards task-agnostic unified anomaly detection," _arXiv preprint arXiv:2307.12540_, 2023.
* [39] T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski, "Vision transformers need registers," _arXiv preprint arXiv:2309.16588_, 2023.
* [40] Y. Liang, J. Zhang, S. Zhao, R. Wu, Y. Liu, and S. Pan, "Omni-frequency channel-selection representations for unsupervised anomaly detection," _arXiv preprint arXiv:2203.00259_, 2022.
* [41] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, "Improving neural networks by preventing co-adaptation of feature detectors," _arXiv preprint arXiv:1207.0580_, 2012.
* [42] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, "Transformers are rnns: Fast autoregressive transformers with linear attention," in _International conference on machine learning_, pp. 5156-5165, PMLR, 2020.
* [43] D. Han, X. Pan, Y. Han, S. Song, and G. Huang, "Flatten transformer: Vision transformer using focused linear attention," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 5961-5971, 2023.
* [44] Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, "Efficient attention: Attention with linear complexities," in _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pp. 3531-3539, 2021.
* [45] G. Hinton, O. Vinyals, and J. Dean, "Distilling the knowledge in a neural network," _arXiv preprint arXiv:1503.02531_, 2015.
* [46] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt, "Stable and low-precision training for large-scale vision-language models," _Advances in Neural Information Processing Systems_, vol. 36, pp. 10271-10298, 2023.
* [47] S. J. Reddi, S. Kale, and S. Kumar, "On the convergence of adam and beyond," _arXiv preprint arXiv:1904.09237_, 2019.
* [48] I. Loshchilov and F. Hutter, "Decoupled weight decay regularization," _arXiv preprint arXiv:1711.05101_, 2017.
* [49] H. He, J. Zhang, H. Chen, X. Chen, Z. Li, X. Chen, Y. Wang, C. Wang, and L. Xie, "A diffusion-based framework for multi-class anomaly detection," in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 38, pp. 8472-8480, 2024.
* [50] P. Bergmann, S. Lowe, M. Fauser, D. Sattlegger, and C. Steger, "Improving unsupervised defect segmentation by applying structural similarity to autoencoders," _arXiv preprint arXiv:1807.02011_, 2018.
* [51] V. Zavrtanik, M. Kristan, and D. Skocaj, "Reconstruction by inpainting for visual anomaly detection," _Pattern Recognition_, vol. 112, p. 107706, 2021.
* [52] W. Liu, R. Li, M. Zheng, S. Karanam, Z. Wu, B. Bhanu, R. J. Radke, and O. Camps, "Towards visually explaining variational autoencoders," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 8642-8651, 2020.

* [53] D. Dehaene and P. Eline, "Anomaly localization by modeling perceptual features," _arXiv preprint arXiv:2008.05369_, 2020.
* [54] T. Schlegl, P. Seebock, S. M. Waldstein, G. Langs, and U. Schmidt-Erfurth, "f-anogan: Fast unsupervised anomaly detection with generative adversarial networks," _Medical image analysis_, vol. 54, pp. 30-44, 2019.
* [55] S. Akcay, A. Atapour-Abarghouei, and T. P. Breckon, "Ganomaly: Semi-supervised anomaly detection via adversarial training," in _Computer Vision-ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2-6, 2018, Revised Selected Papers, Part III 14_, pp. 622-637, Springer, 2019.
* [56] S. Sheynin, S. Benaim, and L. Wolf, "A hierarchical transformation-discriminating generative model for few shot anomaly detection," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 8495-8504, 2021.
* [57] J. Liu and F. Wang, "mixed attention auto encoder for multi-class industrial anomaly detection," _arXiv preprint arXiv:2309.12700_, 2023.
* [58] X. Chen and K. He, "Exploring simple siamese representation learning," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 15750-15758, 2021.
* [59] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J'egou, "Training data-efficient image transformers & distillation through attention," _arXiv preprint arXiv:2012.12877_, 2021.
* [60] S. Ren, Z. Wang, H. Zhu, J. Xiao, A. Yuille, and C. Xie, "Rejuvenating image-gpt as strong visual representation learners," _arXiv preprint arXiv:2312.02147_, 2023.
* [61] W. Yu, C. Si, P. Zhou, M. Luo, Y. Zhou, J. Feng, S. Yan, and X. Wang, "Metaformer baselines for vision," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* [62] T. Reiss, N. Cohen, L. Bergman, and Y. Hoshen, "Panda: Adapting pretrained features for anomaly detection and segmentation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2806-2814, 2021.
* [63] J. Jeong, Y. Zou, T. Kim, D. Zhang, A. Ravichandran, and O. Dabeer, "Winclip: Zero-/few-shot anomaly classification and segmentation," _arXiv preprint arXiv:2303.14814_, 2023.
* [64] C. Huang, H. Guan, A. Jiang, Y. Zhang, M. Spratling, and Y.-F. Wang, "Registration based few-shot anomaly detection," in _European Conference on Computer Vision_, pp. 303-319, Springer, 2022.
* [65] X. Jiang, J. Liu, J. Wang, Q. Nie, K. Wu, Y. Liu, C. Wang, and F. Zheng, "Softpatch: Unsupervised anomaly detection with noisy data," _Advances in Neural Information Processing Systems_, vol. 35, pp. 15433-15445, 2022.

Appendix / supplemental material

### Related Work

**Epistemic methods** are based on the assumption that the networks respond differently during inference between seen input and unseen input. Within this paradigm, _pixel reconstruction_ methods assume that the networks trained on normal images can reconstruct anomaly-free regions well, but poorly for anomalous regions. Auto-encoder (AE) [50; 51], variational auto-encoder (VAE) [52; 53], or generative adversarial network (GAN) [54; 55] are used to restore normal pixels. However, _pixel reconstruction_ models may also succeed in restoring unseen anomalous regions if they resemble normal regions in pixel values or the anomalies are barely noticeable [2]. Therefore, _feature reconstruction_ is proposed to construct features of pre-trained encoders instead of raw pixels [2; 3; 4]. To prevent the whole network from converging to a trivial solution, the parameters of the encoders are frozen during training. In _feature distillation_[5; 6], the student network is trained from scratch to mimic the output features of the pre-trained teacher network with the same input of normal images, also based on the similar hypothesis that the student trained on normal samples only succeed in mimicking features of normal regions.

**Pseudo-anomaly** methods generate handcrafted defects on normal images to imitate anomalies, converting UAD to supervised classification [11] or segmentation tasks [10]. Specifically, CutPaste [11] simulates anomalous regions by randomly pasting cropped patches of normal images. DRAEM [10] constructs abnormal regions using Perlin noise as the mask and another image as the additive anomaly. DeTSeg [12] employs a similar anomaly generation strategy and combines it with feature reconstruction. SimpleNet [13] introduces anomaly by injecting Gaussian noise in the pre-trained feature space. These methods deeply rely on how well the pseudo anomalies match the real anomalies, which makes it hard to generalize to different datasets.

**Feature statistics** methods [7; 8; 56; 9] memorize all normal features (or their modeled distribution) extracted by networks pre-trained on large-scale datasets and match them with test samples during inference. Since these methods require memorizing, processing, and matching nearly all features from training samples, they are computationally expensive in both training and inference, especially when the training set is large.

**Multi-Class UAD**. UniAD [3] first introduced multi-class anomaly detection, aiming to detect anomalies for different classes using a unified model. In this setting, conventional UAD methods often face the challenge of "identical shortcuts", where both anomaly-free and anomaly samples can be effectively recovered during inference [3]. It is caused by the diversity of multi-class normal patterns that drive the network to generalize on unseen patterns. This contradicts the fundamental assumption of epistemic methods. Many current researches focus on addressing this challenge [3; 24; 18; 57; 25]. UniAD [3] employs a neighbor-masked attention module and a feature-jitter strategy to mitigate these shortcuts. HVQ-Trans [24] proposes a vector quantization (VQ) Transformer model that induces large feature discrepancies for anomalies. LafitE [25] utilizes a latent diffusion model and introduces a feature editing strategy to alleviate this issue. DiAD [26] also employs diffusion models to address multi-class UAD settings. OmniAL [23] focuses on anomaly localization in the unified setting, preventing identical reconstruction by using synthesized pseudo anomalies. ViTAD [58] abstracts a unified feature-reconstruction UAD framework and employ Transformer building blocks. MambaAD [19] explores the recently proposed State Space Model (SSM), Mamba, in the context of multi-class UAD.

**Scope of Application.** In this work, we focus on **sensory AD** that detects regional or structural anomalies (common in practical applications such as industrial inspection, medical disease screening, etc.), which is distinguished from **semantic AD**. In sensory AD, normal and anomalous samples are the same objects except for anomaly, e.g. good cable vs. spoiled cable. In semantic AD, the class of normal samples and anomalous samples are semantically different, e.g. animals vs. vehicles. Semantic AD methods usually utilize and compare the global representation of images, which generally do not suffer from the issues of multi-class setting discussed in this paper..

### Full Implementation Details

ViT-Base/14 (patch size=14) pre-trained by DINOV2 with registers (DINOV2-R) [39] is utilized as the encoder. The discard rate of Dropout in Noisy Bottleneck is 0.2 by default, which is increased to 0.4 for the diverse Real-IAD. Loose constraint with 2 groups and \(\mathcal{L}_{global-hm}\) loss are used by default. The input image is first resized to \(448^{2}\) and then center-cropped to \(392^{2}\), so that the feature map (\(28^{2}\)) is large enough for localization. StableAdamW optimizer [46] with AMSGrad [47] is utilized with \(lr\) (learning rate)=2e-3, \(\beta\)=(0.9,0.999), \(wd\) (weight decay)=1e-4 and \(eps\)=1e-10. The network is trained for 10,000 iterations for MVTec-AD and VisA and 50,000 iterations for Real-IAD under MUAD setting. The network is trained for 5,000 iterations on each class under the class-separated UAD setting. The \(lr\) warms up from 0 to 2e-3 in the first 100 iterations and cosine anneals to 2e-4 throughout the training. The discarding rate in Equation 5 linearly rises from 0% to 90% in the first 1,000 iterations as warm-up (500 liters for class-separated setting). The anomaly map is obtained by upsampling the point-wise cosine distance between encoder and decoder feature maps (averaging if more than one pair or group). The mean of the top 1% pixels in an anomaly map is used as the image anomaly score. All experiments are conducted with random seed=1 with cuda deterministic for invariable weight initialization and batch order. Codes are implemented with Python 3.8 and PyTorch 1.12.0 cuda 11.3, and run on NVIDIA GeForce RTX3090 GPUs (24GB).

### Additional Ablation Studies and Experiments

**Ablations on VisA.** Similar to Table 3 that conduct ablation experiments on MVTec-AD, we additionally run them on VisA for further validations. As shown in Table A1, proposed components of Dinomaly contribute to the AD performances on VisA as on MVTec-AD.

**Scalability.** Previous works [3; 2; 17] reported that AD methods do not follow the model "scaling law", i.e., larger models do not necessarily produce better performance. For example, RD4AD [2] found WideResNet50 better than WideResNet101 as the encoder backbone. ViTAD [17] found ViT-Small better than ViT-Base. We conduct experiments to probe the influence of the scale of backbone Transformers in Dinomaly. ViT-Small, ViT-Base (default), and ViT-Large pre-trained by DINOv2-R are used as the encoder, respectively. ViT-Small has 12 layers, so we take the [3,4,5,...10]\(th\) layer as the interested 8 middle layers, which is the same as default ViT-Base. ViT-Large has 24 layers, so we take the [5,7,9,...19]\(th\) layer as the interested 8 middle layers. The layer hyperparameters of the decoder, such as embedding dimension and numbers of attention heads, follow the hyperparameters of the corresponding encoder. Other training strategies are identical to default. As shown in Table A2, the MUAD performance of Dinomaly follows the "scaling law". Dinomaly equipped with ViT-Small already produces state-of-the-art results. ViT-Large further boosts Dinomaly to an unprecedented higher record.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{ Arch.} & \multirow{2}{*}{Parameters} & \multirow{2}{*}{MACs} & \multirow{2}{*}{Latency} & \multicolumn{4}{c}{Image-level} & \multicolumn{4}{c}{Pixel-level} \\ \cline{5-12}  & & & & AUROC & AP & \(F_{1}\)-max & AUROC & AP & \(F_{1}\)-max & AUPRO \\ \hline ViT-Small & 37.4M & 26.3G & 6.8ms & 99.26 & 99.67 & 98.72 & 98.07 & 68.29 & 67.78 & 94.36 \\ ViT-Base & 148.0M & 104.7G & 17.2ms & 99.60 & 99.78 & 99.04 & 98.35 & 69.29 & 69.17 & 94.79 \\ ViT-Large & 275.3M & 413.5G & 41.3ms & **99.77** & **99.92** & **99.45** & **98.54** & **70.53** & **70.04** & **95.09** \\ \hline \hline \end{tabular}
\end{table}
Table A2: Comparison of different ViT architectures, conducted on MVTec-AD (%). Latency per image is measured on NVIDIA RTX3090 with batch size=16.

**Input Size.** The patch size of ViTs (usually \(14\times 14\) or \(16\times 16\)) is much larger than the stem layer's down-sampling rate of CNNs (usually \(4\times 4\)), resulting in smaller feature map size. For dense prediction tasks like semantic segmentation, ViTs usually employ a large input image size [30]. This practice holds in anomaly localization as well. In Table A3, we present the results of Dinomaly with different input resolutions. Following PatchCore [8], by default, we adopt center-crop preprocessing to reduce the influence of background, which can also cause unreachable anomalies at the edge of images. Experimental results demonstrate our robustness to input size. While small image size is enough for image-level anomaly detection, larger inputs are beneficial to anomaly localization. All experiments evaluate localization performance in a unified size of \(256\times 256\) for fairness.

**Pre-Trained Foundations.** The representation quality of the frozen backbone Transformer is of great significance to unsupervised anomaly detection. We conduct extensive experiments to probe the impact of different pre-training methods, including supervised learning and self-supervised learning. DeiT [59] is trained on ImageNet[1] in a supervised manner by distilling CNNs. MAE [33], BEiTv2 [35], and D-iGPT [60] are based on masked image modeling (MIM). Given input images with masked patches, MAE [33] is optimized to restore raw pixels; BEiTv2 [35] is trained to predict the token index of VQ-GAN and CLIP; D-iGPT [60] is trained to predict the features of CLIP model. DINO [29] is based on positive-pair contrastive learning (CL), which is also referred to as self-distillation. It trains the network to produce similar feature representations given two views (augmentations) of the same image. iBot [36] and DINov2 [30] combine MIM and CL strategies, marking the SoTA of self-supervised foundation models. DINov2-R [39] is a variation of DINov2 that employs 4 extra register tokens.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{
\begin{tabular}{c} Pre-Train \\ Method \\ \end{tabular} } & \multirow{2}{*}{Type} & \multicolumn{2}{c}{Image} & \multicolumn{2}{c}{Image-level} & \multicolumn{4}{c}{Pixel-level} \\ \cline{3-10}  & & Size & AUROC & AP & \(F_{1}\)-max & AUROC & AP & \(F_{1}\)-max & AUPRO \\ \hline DeiT[59] & Supervised & R\(512^{2}\)-C\(448^{2}\) & 98.19 & 99.24 & 97.64 & 97.93 & 68.98 & 67.91 & 91.45 \\ MAE[33] & MIM & R\(512^{2}\)-C\(448^{2}\) & 96.27 & 98.33 & 95.44 & 96.96 & 62.89 & 63.32 & 89.85 \\ D-iGPT[60] & MIM & R\(512^{2}\)-C\(448^{2}\) & 98.75 & 99.24 & 97.70 & 98.30 & 65.77 & 66.16 & 92.34 \\ DINO[29] & CL & R\(512^{2}\)-C\(448^{2}\) & 98.97 & 99.58 & 98.14 & 98.52 & 70.89 & 69.02 & 93.48 \\ iBOT[36] & CL+MIM & R\(512^{2}\)-C\(448^{2}\) & 99.22 & 99.67 & 98.57 & 98.60 & 70.78 & 69.92 & 93.33 \\ DINov2[30] & CL+MIM & R\(448^{2}\)-C\(396^{2}\) & 99.55 & 99.81 & 99.13 & 98.26 & 68.35 & 68.79 & 94.83 \\ DINOov2-R[39] & CL+MIM & R\(448^{2}\)-C\(396^{2}\) & 99.60 & 99.78 & 99.04 & 98.35 & 69.29 & 69.17 & 94.79 \\ \hline DeiT[59] & Supervised & R\(256^{2}\)-C\(224^{2}\) & 97.65 & 99.05 & 97.40 & 97.80 & 62.58 & 63.39 & 89.98 \\ MAE[33] & MIM & R\(256^{2}\)-C\(224^{2}\) & 97.25 & 98.84 & 96.94 & 97.78 & 63.00 & 64.01 & 90.95 \\ BEiTv2[35] & MIM & R\(256^{2}\)-C\(224^{2}\) & 97.70 & 99.11 & 97.39 & 97.61 & 59.79 & 62.53 & 90.10 \\ D-iGPT[60] & MIM & R\(256^{2}\)-C\(224^{2}\) & 99.21 & 99.66 & 98.47 & 98.08 & 60.05 & 63.05 & 91.78 \\ DINO[29] & CL & R\(256^{2}\)-C\(224^{2}\) & 99.20 & 99.72 & 98.77 & 98.16 & 64.16 & 65.07 & 92.02 \\ iBOT[36] & CL+MIM & R\(256^{2}\)-C\(224^{2}\) & 99.31 & 99.74 & 98.77 & 98.25 & 64.01 & 65.37 & 91.68 \\ DINOov2[30] & CL+MIM & R\(256^{2}\)-C\(224^{2}\) & 99.26 & 99.70 & 98.60 & 97.95 & 62.27 & 64.39 & 92.80 \\ DINOov2-R[39] & CL+MIM & R\(256^{2}\)-C\(224^{2}\) & 99.34 & 99.73 & 99.03 & 98.09 & 63.04 & 64.48 & 92.59 \\ \hline \hline \end{tabular}
\end{table}
Table A3: Ablations of input size, conducted on MVTec-AD (%). R\(448^{2}\)-C\(392^{2}\) represents first resizing images to 448\(\times\)448, then center cropping to 392\(\times\)392.

It is noted that most models are pre-trained with the image resolution of \(224\times 224\), except that DINOv2 [30] and DINOv2-R [39] have extra a high-resolution training phase with \(518\times 518\). However, directly using the pre-trained weights on a different resolution for UAD without fine-tuning like other supervised tasks can cause generalization problems. Therefore, by default, we still keep the feature size of all compared models to \(28\times 28\), i.e., the input size is \(392\times 392\) for ViT-Base/14 and \(448\times 448\) for ViT-Base/16. Additionally, we train Dinomaly with the low-resolution input size of \(224\times 224\). The results are presented in Table A4. Generally speaking, CL+MIM combined models outperform MIM and CL models. In addition, MIM-based models do not benefit from higher resolutions but suffer from them, indicating the lack of generalization on a different input size. Methods involving CL can better adapt to a higher resolution as they optimize the global representation of class tokens in pre-training, which is insensitive to input size. As expected, DINOv2 and DINOv2-R pre-trained on larger inputs can better benefit from higher resolution in Dinomaly. Because some methods, i.e., D-iGPT, DINO, and iBOT, produce similar results to DINOv2 in \(224\times 224\), we expect that they also have the potential to be as powerful in Dinomaly if they are pre-trained in high-resolution.

**Attention _vs._ Convolution.** Previous works and this paper have proposed to leverage attentions instead of convolutions in UAD. Here, we conduct experiments substituting the attention in the decoder of Dinomaly by convolutions as the spatial mixers. Following MetaFormer [61], we employ Inverted Bottleneck block that consists of \(1\times 1\) conv, GELU activation, \(N\times N\) deep-wise conv, and \(1\times 1\) conv, sequentially. The results are shown in Table A5, where Attentions outperform Convolutions, especially for pixel-level anomaly localization. In addition, utilizing convolutions in the decoder can still yield SoTA results, demonstrating the universality of the proposed Dinomaly.

**Neighbour-Masking.** Prior method [3] proposed to mask the keys and values in an \(n\times n\) square centered at each query, in order to alleviate identity mapping in Attention. This mechanism can also be applied to Linear Attention as well. As shown in Table A5, neighbor-masking can further improve Dinomaly with both Softmax Attention and Linear Attention moderately.

**Feature Noise.** Prior method [3] proposed to perturb the encoder features by Feature Jitter, i.e. adding Gaussian noise with _scale_ to control the noise magnitude. We evaluate the feature jitter strategy in the proposed Dinomaly by placing it at the beginning of Noisy Bottleneck. As shown in Table A6, both Dropout and Feature Jitter can be a good noise injector in Noisy Bottleneck. Meanwhile, Dropout is more robust to the noisy scale hyperparameter, and more elegant without introducing new modules.

**Random Seeds.** Due to limited computation resources, experiments in this paper are conducted for one run with random seed=1. Here, we conduct 5 runs with 5 random seeds on MVTec-AD. As shown in Table A7, Dinomaly is robust to randomness.

### Limitation

Vision Transformers are known for their high computation cost, which can be a barrier to low-computation scenarios that require inference speed. Future research can be conducted on the efficiency of Transformer-based methods, such as distillation, pruning, and hardware-friendly attention mechanism (such as FlashAttention).

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_EMPTY:19]

Figure A1: Anomaly maps visualization on MVTec-AD. All samples are randomly chosen.

Figure A2: Anomaly maps visualization on VisA. All samples are randomly chosen.

Figure A3: Anomaly maps visualization on Real-IAD. All samples are randomly chosen.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Well reflected. Guidelines:

* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Presented in Appendix.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]. Justification: No theory. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes, and code is in supplementary material. Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Yes, and code is in supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).

* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Results with mean and std are presented in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
10. If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
11. The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
12. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Yes. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: No risk. Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licenses for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]. Justification: No new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: No human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: No involving. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.