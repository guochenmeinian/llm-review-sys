# Improving Temporal Link Prediction via

Temporal Walk Matrix Projection

 Xiaodong Lu

CCSE Lab, Beihang University

Beijing, China

xiaodonglu@buaa.edu.cn

&Leilei Sun

CCSE Lab, Beihang University

Beijing, China

leileisun@buaa.edu.cn

Corresponding Author.

Tongyu Zhu

CCSE Lab, Beihang University

Beijing, China

zhutongyu@buaa.edu.cn

&Weifeng Lv

CCSE Lab, Beihang University

Beijing, China

lwf@buaa.edu.cn

###### Abstract

Temporal link prediction, aiming at predicting future interactions among entities based on historical interactions, is crucial for a series of real-world applications. Although previous methods have demonstrated the importance of relative encodings for effective temporal link prediction, computational efficiency remains a major concern in constructing these encodings. Moreover, existing relative encodings are usually constructed based on structural connectivity, where temporal information is seldom considered. To address the aforementioned issues, we first analyze existing relative encodings and unify them as a function of temporal walk matrices. This unification establishes a connection between relative encodings and temporal walk matrices, providing a more principled way for analyzing and designing relative encodings. Based on this analysis, we propose a new temporal graph neural network called TPNet, which introduces a temporal walk matrix that incorporates the time decay effect to simultaneously consider both temporal and structural information. Moreover, TPNet designs a random feature propagation mechanism with theoretical guarantees to implicitly maintain the temporal walk matrices, which improves the computation and storage efficiency. Experimental results on 13 benchmark datasets verify the effectiveness and efficiency of TPNet, where TPNet outperforms other baselines on most datasets and achieves a maximum speedup of \(33.3\times\) compared to the SOTA baseline. Our code can be found at https://github.com/lxd99/TPNet.

## 1 Introduction

Many real-world dynamic systems can be abstracted as a temporal graph [1], where entities and interactions among them are denoted as nodes and edges with timestamps respectively. Temporal link prediction, aiming at predicting future interactions based on historical interactions, is a fundamental task for temporal graph learning, which can not only help us understand the evolution pattern of the temporal graph but also is crucial for a series of real-world tasks such as recommendations for online platforms [2, 3] and information diffusion prediction [4, 5].

Relative encodings have become an indispensable module for effective temporal link prediction [6, 7, 8, 9] where, without them, node representations computed independently by neighbor aggregation will failto capture the pairwise information. As the toy example shown in Figure 1, A and F will have the same node representation due to sharing the same local structure. Thus it can not be determined whether D will interact with A or F at \(t_{3}\) according to their representations. However, by assigning nodes with relative encodings (i.e., additional node features) specific to the target link before computing the node representation, we can highlight the importance of each node and guide the representation learning process to extract pairwise information. For example, in Figure 1, we can infer from the relative encoding of E (in red circle) that D is more likely to interact with F than with A since D and F share a common neighbor, E (detailed discussed in Section 2.2). Although achieving remarkable success, injecting pairwise information based on relative encodings is still far from satisfactory.

Figure 1: Without relative encodings, the learned node representations fail to capture the correlation between nodes. (For each link \((u,v,t)\), the relative encoding here for a node \(w\) is [\(g(u,w),g(v,w)\)], where \(g(u,w)=1\) if there is an interaction between u and w before t, otherwise \(g(u,w)=0\).)

### Definitors

**Definition 1** (Temporal Graph).: A temporal graph can be considered as a sequence of non-decreasing chronological interactions \(\mathcal{G}=[(\{u_{1},v_{1}\},t_{1})\,(\{u_{2},v_{2}\},t_{2})\,\cdots]\) with \(0\leq t_{1}\leq t_{2}\leq\cdots\), where \((\{u_{i},v_{i}\},t_{i})\) can be considered as a undirected link between \(u_{i}\) and \(v_{i}\) with timestamp \(t_{i}\). Each node \(u\) can be associated with node feature \(\bm{x}_{u}\in\mathbb{R}^{d_{N}}\), and each interaction \((\{u,v\},t)\) has link feature \(\bm{e}_{u,v}^{t}\in\mathbb{R}^{d_{E}}\), where \(d_{N}\) and \(d_{E}\) denote the dimensions of the node feature and link feature.

**Definition 2** (Temporal Link Prediction).: The interaction sequence reflects the graph dynamics, and thus the ability of a model to capture the evolution pattern of a dynamic graph can be evaluated by how accurately it predicts the future interactions based on historical interactions. Formally, given the interactions before \(t\) (i.e., \(\{(\{u^{\prime},v^{\prime}\},t^{\prime})|t^{\prime}<t\}\)) and two nodes \(u\), \(v\), the temporal link prediction task aims to predict whether there will be an interaction between \(u\) and \(v\) at \(t\).

**Definition 3** (K-hop Subgraph).: We use the notation \(\mathcal{G}(t)=(\mathcal{V}(t),\mathcal{E}(t))\) to denote the graph snapshot at \(t\), where \(\mathcal{E}(t)\) includes all the interactions that happen before \(t\) and \(\mathcal{V}(t)\) includes all the nodes appear in \(\mathcal{E}(t)\). Besides, we defined the k-hop subgraph of node \(u\) as \(\mathcal{G}_{u}^{k}(t)=(\mathcal{V}_{u}^{k}(t),\mathcal{E}_{u}^{k}(t)\), where \(\mathcal{V}_{u}^{k}(t)\subset\mathcal{N}(t)\) is the set of nodes whose shortest path distance to \(u\) is less than \(k\) on \(\mathcal{G}(t)\) and \(\mathcal{E}_{u}^{k}(t)\subset\mathcal{E}(t)\) is the set of interactions between \(\mathcal{V}_{u}^{k}(t)\).

**Definition 4** (Temporal Walk).: A k-step temporal walk \(W\) on \(\mathcal{G}(t)\) is a sequence of node-time pair with decreased temporal order [6], which can be denoted as \(W=[(w_{0},t_{0}),\cdots,(w_{k},t_{k})]\) with \(t=t_{0}>t_{1}>\cdots>t_{k}\) and \((\{w_{i},w_{i+1}\},t_{i+1})\) is an edge on \(\mathcal{G}(t)\) for \(0\leq i\leq k-1\). Figure 2 shows a visual illustration of a temporal walk. Here, we stipulate the first timestamp \(t_{0}\) as the current time \(t\) to avoid undefinedness of \(t_{0}\). We use the notation \(\mathcal{M}_{u,v}^{k}(t)\) to denote the set of all k-step temporal walks from \(u\) to \(v\) on \(\mathcal{G}(t)\). Specially, \(\mathcal{M}_{u,w}^{0}(t)=\{[(u,t)]\}\) if \(u=w\) and \(\mathcal{M}_{u,w}^{0}(t)=\emptyset\) otherwise. When there is no ambiguity, we replace \(\mathcal{M}_{u,v}^{k}(t)\) with \(\mathcal{M}_{u,v}^{k}\).

### Relative Encoding for Dynamic Link Prediction

#### 2.2.1 Unified Framework

Given a future link \((u,v,t)\) to be predicted, existing temporal link prediction methods (referred as node-wise methods) usually first learn the node representations \(\bm{h}_{u}(t)\) and \(\bm{h}_{v}(t)\) independently, which are obtained by encoding their k-hop subgraphs,

\[\bm{h}_{u}(t)=f_{\text{enc}}(\mathcal{G}_{u}^{k}(t),\bm{X}_{u,k}^{N},\bm{X}_{u,k}^{E}),\ \ \ \ \bm{h}_{v}(t)=f_{\text{enc}}(\mathcal{G}_{v}^{k}(t),\bm{X}_{v,k}^{N},\bm{X}_{v,k}^{E}),\] (1)

where \(\bm{X}_{u,k}^{N}\) and \(\bm{X}_{u,k}^{E}\) are the features of nodes and edges in \(\mathcal{G}_{u}^{k}(t)\) respectively 2. The \(f_{\text{enc}}(\cdot)\) here can be any encoding function that maps a graph into a representation such as a k-layer GNN with a pooling layer Then the link likelihood \(p_{u,v}^{t}\) is given \(p_{u,v}^{t}=f_{\text{dec}}(\bm{h}_{u}(t),\bm{h}_{v}(t))\). The \(f_{\text{dec}}(\cdot)\) is a decoding function that maps the node representations into link likelihood such as an MLP with a Sigmoid output layer. Detailed discussion about existing methods can be found in Appendix F.2.

Footnote 2: For memory-based methods (i.e, TGN), \(\bm{X}_{u,k}^{N}\) represents the concatenation of node memories and features.

As mentioned in the Section 1, learning representations independently might fail to capture the pairwise information of the given link. Thus recent methods (referred as link-wise methods) inject the pairwise information by constructing a relative encoding \(\bm{r}^{w|(u,v)}\) for each node \(w\in\mathcal{V}_{u}^{k}(t)\cup\mathcal{V}_{v}^{k}(t)\) as an additional node feature (Detailed construction way for different methods will be introduced in Section 2.2.2). Then Equation (1) will be changed into

\[\bm{h}_{u}(t)=f_{\text{enc}}(\mathcal{G}_{u}^{k}(t),\bm{X}_{u,k}^{N}\oplus\bm{X }_{u,k}^{R},\bm{X}_{u,k}^{E}),\ \ \ \ \bm{h}_{v}(t)=f_{\text{enc}}(\mathcal{G}_{v}^{k}(t),\bm{X}_{v,k}^{N}\oplus\bm{X}_{v,k}^{R},\bm{X}_{v,k}^{E}),\] (2)

where \(\bm{X}_{u,k}^{R}\) is the relative encodings for nodes in \(\mathcal{G}_{u}^{k}(t)\) and \(\bm{X}_{u,k}^{N}\oplus\bm{X}_{u,k}^{R}\) indicate the new node features obtained by concatenating \(\bm{X}_{u,k}^{N}\) and \(\bm{X}_{u,k}^{R}\). Intuitively, the relative encoding \(\bm{r}^{w|(u,v)}\) for each node \(w\) reflects its importance to predicted link \((u,v,t)\), which can guide the representation learning process to extract the pairwise information specific to the predicted link from the subgraph. For the detailed construction way, the relative encoding \(\bm{r}^{w|(u,v)}\) is the concatenation of two similarity features \(\bm{r}^{w|u}\) and \(\bm{r}^{w|v}\), where \(\bm{r}^{w|u}/\bm{r}^{w|v}\) encodes some form of similarity between \(u/v\) and \(w\) (e.g., the number of k-step temporal walks from \(u\) to \(w\)). Although the designs of similarity featurefor different methods are induced from different heuristics, we find that they can be unified in a function about the temporal walk matrix, which is

\[\bm{r}^{w|u}=g([A_{u,w}^{(0)}(t),A_{u,w}^{(1)}(t),\cdots,A_{u,w}^{(k)}(t)]),\quad A _{u,w}^{(i)}(t)=\sum_{W\in\mathcal{M}_{u,w}^{i}}s(W)\;\;\text{for}\;\;0\leq i \leq k.\] (3)

The \(s(\cdot)\) here is a score function that maps a temporal walk to a scalar and \(\bm{A}^{(i)}(t)\) denotes an i-hop temporal walk matrix whose each entry \(A_{u,w}^{(i)}\) is the sum of the score of all i-step temporal walks from \(u\) to \(w\). \(g(\cdot)\) is a function applied on the vector of \([A_{u,w}^{(0)}(t)\), \(A_{u,w}^{(1)}(t),\cdots,A_{u,w}^{(k)}(t)]\in\mathbb{R}^{k+1}\) to extract similarity feature. The above equation shows that each relative encoding implies a construction of the temporal walk matrix based on weighting each temporal walk (i.e., \(s(\cdot)\)). Next, we will analyze existing relative encodings and show how they can be represented in the form of Equation (3).

#### 2.2.2 Analysis of Existing Methods

Our analysis focuses on the four representative link-wise methods DyGFormer, PINT, NAT, and CAW, which cover all the link-wise methods in the benchmark of dynamic link prediction [9].

**DyGFormer**[9]. The \(\bm{r}^{w|u}\) of DyGFormer is a one-dimensional vector representing the number of links between \(w\) and \(u\). For DyGFormer, we can first set the \(s(\cdot)\) to be a one-const function (i.e., \(s(W)=1\) in for any \(W\)), which will make \(A_{u,w}^{(k)}\) be the number of the k-step temporal walks from \(u\) to \(w\). Then, setting \(g(\cdot)\) to be a function that selects the second number of a vector (i.e., \(g([x_{0},x_{1},..,x_{k}])=x_{1}\)) will make Equation (3) generate the similarity feature of DyGFormer.

**PINT**[8]. The \(\bm{r}^{w|u}\) of PINT is a (\(k+1\))-dimensional vector, where \(r_{i}^{w|u}\) is the number of \((i-1)\)-step temporal walks from \(u\) to \(w\) for \(1\leq i\leq k+1\). Setting \(s(\cdot)\) and \(g(\cdot)\) can be set to a one-const function and an identity function respectively will make Equation (3) outputs the similarity feature of PINT.

**NAT**[7]. NAT maintains a series of fix-sized hash maps \(s_{0}^{(u)},...,s_{u}^{(k)}\) and generates the \(\bm{r}^{w|u}\) based on the hash maps. As proved in Appendix A.1, if the size of the hash maps becomes infinite, the \(\bm{r}^{w|u}\) is equivalent to a \((k+1)\)-dimensional vector, where, for \(1\leq i\leq k+1\), \(r_{i}^{w|u}=1\) if the shortest temporal walk from \(u\) to \(w\) is less than \(i\); otherwise, \(r_{i}^{w|u}=0\). Let \(h(\cdot)\) be a binary function where \(h(x)=1\) if \(x>0\) and \(h(x)=0\) otherwise. Setting the \(s(\cdot)\) to be a one-const function and \(g(\cdot)\) to a function of \(g([x_{0},x_{1},...,x_{k}])=[h(\sum_{i=0}^{0}x_{i}),h(\sum_{i=0}^{1}x_{i}),...,h (\sum_{i=0}^{k}x_{i})]\) can make the Equation (3) generate the similarity feature of NAT.

**CAWN**[6]. The similarity feature \(\bm{r}^{w|u}\) of CAWN reflects the probability of a node \(w\) being visited when performing a temporal walk from \(u\). Specifically, CAWN first samples a set of temporal walks beginning from \(u\) based on a causal sampling strategy. Then, for each node \(w\), the similarity feature \(\bm{r}^{w|u}\) is extracted based on its occurrence in the sampled walks. As proved in Appendix A.2, the similarity feature \(\bm{r}^{w|u}\) is the estimation of a (\(k+1\))-dimensional vector, where, for \(1\leq i\leq k+1\), \(r_{i}^{w|u}\) is the probability of visiting \(w\) through a (\(i-1\))-step temporal walk matrix based on the sampling strategy of CAWN, which can be represented as \(r_{i}^{w|u}=\sum_{W\in\mathcal{M}_{u,w}^{i-1}}s^{\prime}(W)\). The value \(s^{\prime}(W)\) for a given temporal walk \(W=[(w_{0},t_{0}),(w_{1},t_{1}),...,(w_{k},t_{k})]\) is defined as \(\prod_{i=0}^{k-1}\frac{\exp(-\alpha(t_{i}-t_{i+1}))}{\sum_{(i=\nu,\nu),\ell^{ \prime}\in w_{i},\ell}\exp(-\alpha(t_{i}-\ell^{\prime}))}\), where \(\alpha\) is hyperparameter to control the sampling process, \(\mathcal{E}_{w_{i},t_{i}}=\{(\{w^{\prime},w\},\ell^{\prime})|\ell^{\prime}<t_ {i}\}\) is the set of interactions attached to \(w_{i}\) before \(t_{i}\), and \(\frac{\exp(-\alpha(t_{i}-t_{i+1}))}{\sum_{(i=\nu^{\prime},w),\ell^{\prime}\in w _{i},t_{i}}}\) can be considered as the probability of moving from \((w_{i},t_{i})\) to \((w_{i+1},t_{i+1})\) in the sampling process. Setting \(s(\cdot)\) to \(s^{\prime}(\cdot)\) and \(g(\cdot)\) to be an identity function can make Equation (3) generate the similarity feature of CAWN.

**Conclusion**. According to the above analysis, we can conclude that (i) Equation (3) provides a unified view to consider the injection of pairwise information from the construction of temporal walk matrix, where different relative encodings (implicitly or explicitly) correspond to a kind of temporal walk matrix. (ii) Examining existing relative encodings from the unified view reveals their limitations. First, the relative encodings of existing methods (except CAWN) ignore the temporal information carried by each temporal walk, where their score function \(s(\cdot)\) always yield \(1\) and the entries of the temporal walk matrix is just the count of the temporal walks. Next, although CAWN considers temporal information, they estimate the temporal walk matrix from the sampled temporal walks, which needs time-consuming graph sampling and may introduce large estimation errors. In the next section, we will present a new temporal walk matrix to simultaneously consider the temporal and structural information and show how to efficiently maintain the proposed temporal walk matrix.

## 3 Methodology

TPNet mainly consists of two modules: Node Representation Maintaining (NRM) and Link Likelihood Computing (LLC). The NRM is responsible for encoding the pairwise information, which maintains a series of node representations. Such representations will be updated when new interaction occurs and can be used to decode the temporal walk information between two nodes. The LLC module is a prediction module, which utilizes the maintained node representations and auxiliary information (e.g., like features) to compute the likelihood of the link to be predicted.

### Node Representation Maintaining

**Temporal Matrix Construction**. Based on the Equation (3), designing a temporal walk matrix relies on the definition of the score function \(s(\cdot)\), where the element of a temporal walk matrix is \(A_{u,v}^{(k)}(t)=\sum_{W\in M_{u,v}^{k}}s(W)\). Unlike most previous methods that only count the number of temporal walks, we consider the temporal information carried by a temporal walk in \(s(\cdot)\) to simultaneously consider the temporal and structural information. Formally, let \(t\) be the current time, given a temporal walk \(W=[(w_{0},t_{0}),(w_{1},t_{1}),..,(w_{k},t_{k})]\), the value of the score function is \(s(W)=\prod_{i=1}^{k}e^{-\lambda(t-t_{i})}\), where \(\lambda>0\) is a hyperparameter to control the time decay weight. As the current time \(t\) goes on, for each interaction \((\{w_{i},w_{i+1}\},t_{i+1})\) in the temporal walk \(W\), its weight \(e^{-\lambda(t-t_{i})}\) will decay exponentially. The design of the score function is motivated by the widely observed time decay effect [1; 10] on the temporal graph, where the importance of interactions will decay as time goes on, benefiting better modeling the graph evolution patterns. In the following part of Section 3, the notation of \(s(\cdot)\) and \(\bm{A}^{(k)}(t)\) will specifically refer to the score function and temporal walk matrix proposed in this part. Besides, for \(\bm{A}^{(0)}(t)\), we stipulate it as an identity matrix constantly.

**Node Representation Maintaining**. Directly computing the temporal walk matrices is impractical since we need to enumerate the temporal walks and each matrix needs expensive \(O(n\times n)\) space complexity to store. Thus, we implicitly maintain the temporal walk matrices by maintaining a series of node representations \(\bm{H}^{(0)}(t),\bm{H}^{(1)}(t),...,\bm{H}^{(k)}(t)\in\mathbb{R}^{n\times d_{ R}}\), where \(d_{R}\ll n\) is the dimension of node representations and \(\bm{H}_{u}^{(l)}(t)\in\mathbb{R}^{d_{R}}\) encodes the information about the \(l\)-step temporal walks beginning from \(u\) for \(0\leq l\leq k\). The node representations will be updated when a new interaction occurs. Specifically, we first construct a random feature matrix \(\bm{P}\in\mathbb{R}^{n\times d_{R}}\), where each entry of \(\bm{P}\) is independently drawn from Gaussian distribution with mean 0 and variance \(\frac{1}{d_{R}}\). Then we initialize \(\bm{H}^{(0)}(t)\) as \(\bm{P}\) and \(\bm{H}^{(1)}(t),\bm{H}^{(2)}(t),...,\bm{H}^{(k)}(t)\) as zero matrix. When a new interaction \((u,v,t)\) occurs, for \(1\leq l\leq k\), we update the representations of \(u\) and \(v\) by

\[\bm{H}_{u}^{(l)}(t^{+})=\bm{H}_{u}^{(l)}(t)+e^{\lambda t}*\bm{H}_{v}^{(l-1)}( t),\hskip 14.226378pt\bm{H}_{v}^{(l)}(t^{+})=\bm{H}_{v}^{(l)}(t)+e^{ \lambda t}*\bm{H}_{u}^{(l-1)}(t),\] (4)

where the \(t^{+}\) denotes the time right after \(t\). A pseudocode for maintaining the node representations is shown in Algorithm 1. The maintaining mechanism here can be considered as a random feature propagation mechanism on the temporal graph, where we initialize the zero layer's representation

Figure 2: A illustration of the temporal walk.

of each node as a random feature and repeatedly propagate these features among nodes from the low layer to the high layer as interactions continuously appear. The following theorem shows the relationship between the node representations and the temporal walk matrices.

**Theorem 1**.: _If any two interactions on temporal graph \(\mathcal{G}\) have different timestamps, the obtained representations \(\bm{H}^{(0)}(t),\bm{H}^{(1)}(t),...,\bm{H}^{(k)}(t)\) satisfy \(e^{-\lambda lt}*\bm{H}^{(l)}(t)=\bm{A}^{(l)}(t)\bm{P}\) for \(0\leq l\leq k\)._

For simplicity, we assume the timestamps of the interaction are different, and we show how to update the representations when multiple interactions have the same timestamps in Appendix C. We leave the proof in the Appendix B.1. The above theorem shows that the obtained node representation is the projection (i.e. linear transformation) of the temporal walk matrices, where the transform matrix is the initial random feature matrix \(\bm{P}\). The next theorem shows that the node representations preserve the inner product of the temporal walk matrices.

**Theorem 2**.: _Given any \(\epsilon\in(0,1)\), let \(\|\cdot\|_{2}\) denote the L2 norm, \(c_{u,v}^{l_{1},l_{2}}\) denote \(\frac{1}{2}(\|\bm{A}_{u}^{(l_{1})}(t)\|_{2}^{2}+\|\bm{A}_{v}^{(l_{2})}(t)\|_{ 2}^{2})\), and \(\bm{\bar{H}}^{(l)}(t)\) denote \(e^{-\lambda lt}*\bm{H}^{(l)}(t)\), if dimension \(d_{R}\) of node representations satisfy \(d_{R}\geq\frac{24}{\epsilon^{2}}\log(4^{1/3}(k+1)n)\), then for any \(1\leq u,v\leq n\), and \(0\leq l_{1},l_{2}\leq k\), we have_

\[\mathbb{P}\left(\left|\langle\bm{\bar{H}}_{u}^{(l_{1})}(t),\bm{\bar{H}}_{v}^{ (l_{2})}(t)\rangle-\langle\bm{A}_{u}^{(l_{1})}(t),\bm{A}_{v}^{(l_{2})}(t) \rangle\right|\leq\epsilon c_{u,v}^{l_{1},l_{2}}\right)\geq 1-\frac{1}{(k+1)n},\] (5)

_where \(\langle\cdot,\cdot\rangle\) denotes the inner product and \(|\cdot|\) denotes taking absolute value._

We leave the proof in Appendix B.2. The above theorem shows that the inner product of the node representations is approximately equal to the inner product of the temporal walk matrices (i.e., \(\langle\bm{\bar{H}}_{u}^{(l_{1})}(t),\bm{\bar{H}}_{v}^{(l_{2})}(t)\rangle \approx\langle\bm{A}_{u}^{(l_{1})}(t)\rangle,\bm{A}_{v}^{(l_{2})}(t)\rangle\)). Specifically, given any error rate \(\epsilon\), if the dimension of the node representations satisfies a certain condition, the difference between \(\langle\bm{\bar{H}}_{u}^{(l_{1})}(t),\bm{\bar{H}}_{v}^{(l_{2})}(t)\rangle\) and \(\langle\bm{A}_{u}^{(l_{1})}(t)\rangle,\bm{A}_{v}^{(l_{2})}(t)\rangle\) for any \(u,v,l_{1},l_{2}\) will be less than \(\epsilon c_{u,v}^{l_{1},l_{2}}\) with high probability (\(\geq 1-\frac{1}{(k+1)n}\)). The error rate can approach 0 infinitely and thus the inner product of the node representations can approach that of temporal walk matrices infinitely, at the cost of increasing the dimension \(d_{R}\). As we will see in Section 4.3, a small dimension (\(\ll n\)) in practice is enabled to make the inner product of node representations a good estimation of that of temporal walk matrices. Additionally, due to the i-th row of \(\bm{A}^{(0)}(t)\) being the one-hot encoding of i (since it is an identity matrix), we have \(\langle\bm{A}_{u}^{(l)}(t),\bm{A}_{w}^{(0)}(t)\rangle=A_{u,w}^{(l)}(t)\). Thus, we can obtain \([A_{u,w}^{(0)}(t),\cdots,A_{u,w}^{(k)}(t)]\) in Equation (3) by calculating the inner product between all layers of \(u\)'s representation and the zero layer of \(w\)'s representation, expressed as \([\langle\bm{\bar{H}}_{u}^{(0)}(t),\bm{\bar{H}}_{w}^{(0)}(t)\rangle,\cdots, \langle\bm{\bar{H}}_{u}^{(k)}(t),\bm{\bar{H}}_{w}^{(0)}(t)\rangle]\).

**Remark.** Compared to directly computing the temporal walk matrices \(\bm{A}^{(0)}(t),..,\bm{A}^{(k)}(t)\), which needs to enumerate the temporal walks and \(O((k+1)n^{2})\) space complexity to store, maintaining the node representations largely improve the computation and storage efficiency, which only needs \(O((k+1)nd_{R})\) space complexity to store and \(O(kd_{R})\) time complexity to update when a new interaction occurs. Actually, Theorem 2 is the direct result of Theorem 1 based on Johnson-Lindenstrauss Lemma [11], where the random projection can preserve the inner product and norm [12]. Notably, the method for implicitly maintaining temporal walk matrices via random feature propagation can be extended to other types of temporal walk matrices, provided they meet a specific condition (i.e., the updating function of the temporal walk matrix can be written as the linear combination of its rows). This condition is not restrictive, and all the temporal walk matrices discussed in Section 2 fulfill it. We show their propagation mechanism and related discussion in Appendix D. In conclusion, the unified function in Equation (3), combined with methods of implicitly maintaining the temporal walk matrices, provides a new way to design a more effective and efficient way to inject pairwise information.

### Link Likelihood Computing

Given an interaction \((u,v,t)\) to be predicted, we compute its happening likelihood based on the obtained node representations and auxiliary features. Specifically, we first decode a pairwise feature \(\bm{f}_{u,v}(t)\) from the node representations obtained in Section 3.1. Then we compute the node embeddings \(\bm{h}_{u}(t)\) and \(\bm{h}_{v}(t)\) for node \(u\) and \(v\) respectively based on their historical interactions. Finally, we give the link likelihood based on \(\bm{h}_{u}(t),\bm{h}_{v}(t),\bm{f}_{u,v}(t)\). For notation simplicity, we omit the suffix of \(\bm{h}_{u}(t),\bm{h}_{v}(t),\bm{f}_{u,v}(t)\) and denote them as \(\bm{h}_{u},\bm{h}_{v},\bm{f}_{u,v}\) in the following part.

**Pairwise Feature Decoding**. Although we can obtain the \((k+1)\)-dimensional feature in Equation (3) by calculating the inner product between the zero-layer representation of one node and all layers of the other node's representation, this method does not consider the correlation between all layers of both nodes. Therefore, we use representations from all layers to decode the pairwise information. Specifically, we first take the node representation of u and v from different layers, which can be denoted as \(\bm{F}_{*}=[e^{-\lambda 0t}\bm{H}_{*}^{(0)},...,e^{-\lambda kt}\bm{H}_{*}^{(k)}] \in\mathbb{R}^{(k+1)\times d_{R}}\) with * could be u or v. Then we concatenate them together to get \(\bm{F}_{u,v}=[\bm{F}_{u},\bm{F}_{v}]\in\mathbb{R}^{2(k+1)\times d_{R}}\) and obtain the raw pairwise feature \(\bm{\tilde{f}}_{u,v}\) by computing the inner product among different rows of \(\bm{F}_{u,v}\), which is \(\bm{\tilde{f}}_{u,v}=\text{flat}(\bm{F}_{u,v}\bm{F}_{u,v}^{T})\) with flat\((\cdot)\) means flatten a matrix of \(\mathbb{R}^{2(k+1)\times 2(k+1)}\) into a vector of \(\mathbb{R}^{4(k+1)^{2}}\). Finally, we feed the raw pairwise feature \(\bm{\tilde{f}}_{u,v}\) into an MLP to get the pairwise feature \(\bm{f}_{u,v}\), which is \(\bm{f}_{u,v}=\text{MLP}(\log(\text{ReLU}(\bm{\tilde{f}}_{u,v})+1))\). The ReLU\((\cdot)\) here is used to reduce estimation error, where the inner product of temporal walk matrices should be larger than zero and we thus set it to be zero if the inner product of the node representations is negative. The \(\log(\cdot)\) is used to scale the raw pairwise feature, where the range of the inner product between different layers varies greatly and the \(+1\) is the shift term to avoid the undefined value of \(\text{log}(0)\). We will see in Section 4.3 that these two operations can improve the training stability.

**Auxiliary Feature Learning**. The auxiliary features such as link features also provide rich information for modeling the evolution patterns of the temporal graph. In this part, we follow the previous methods [13; 9] and consider the auxiliary feature learning as a sequential learning problem. Specifically, for node \(u\), we take its recent \(m\) interactions \(S_{u}=[(\{u,w_{1}\},t_{1}),...,(\{u,w_{m}\},t_{m})]\) before \(t\) and learn node embedding \(\bm{h}_{u}\) from this sequence. We first fetch the node features \(\bm{X}_{u,N}=[\bm{x}_{w_{1}},...,\bm{x}_{w_{m}}]\in\mathbb{R}^{m\times d_{N}}\) and edge features \(\bm{X}_{u,E}=[\bm{e}_{u,w_{1}}^{t_{1}},...,\bm{e}_{u,w_{1}}^{t_{m}}]\in\mathbb{ R}^{m\times d_{E}}\). For timestamps, we map the timestamps into temporal features \(\bm{X}_{u,T}=[\phi(t-t_{1}),..,\phi(t-t_{n})]\in\mathbb{R}^{m\times d_{T}}\) like [14], where \(\phi(\Delta t)=[\cos(w_{1}\Delta t),..,\cos(w_{d_{T}}\Delta t)]\) is a time encoding function to learn the periodic temporal pattern. Besides, we construct a relative encoding sequence \(\bm{X}_{u,F}=[\bm{f}_{u,w_{1}}\oplus\bm{f}_{v,w_{1}},...,\bm{f}_{u,w_{m}} \oplus\bm{f}_{v,w_{m}}]\in\mathbb{R}^{m\times 8(k+1)^{2}}\) to inject the pairwise features, where \(\bm{f}_{u,w_{m}}\) denote the pairwise feature of \(u\) and \(w_{m}\) and \(\oplus\) is the concatenation operation. After obtaining the above feature sequence, we concatenate them together and feed it into an MLP to get the final feature sequence \(\bm{Z}_{u}^{(0)}=\text{MLP}([\bm{X}_{u,N},\bm{X}_{u,E},\bm{X}_{u,T},\bm{X}_{u, F}])\in\mathbb{R}^{m\times d}\). Subsequently, we stack \(l\) layers of MLP-Mixer [15] to capture the temporal and structural dependencies within the feature sequence, which is

\[\tilde{\bm{Z}}_{u}^{(l)} =\bm{Z}_{u}^{(l-1)}+\bm{W}_{1}^{(l)}\text{GeLU}(\bm{W}_{2}^{(l)} \text{LayerNorm}(\bm{Z}_{u}^{(l-1)}))\] (6) \[\bm{Z}_{u}^{(l)} =\tilde{\bm{Z}}_{u}^{(l)}+\bm{W}_{3}^{(l)}\text{GeLU}(\bm{W}_{4}^ {(l)}\text{LayerNorm}(\bm{\tilde{Z}}_{u}^{(l)})).\]

Finally, we get the node embedding by mean pooling \(\bm{h}_{u}=\text{MEAN}(\bm{Z}_{u}^{(l)})\). The procedure to get node embedding \(\bm{h}_{v}\) is similar and for the node that does not have \(m\) interactions, we pad the feature sequence with zero. Then, the likelihood of the link \((u,v,t)\) is given by \(p_{u,v}^{\text{f}}=\text{MLP}([\bm{h}_{u},\bm{h}_{v},\bm{f}_{u,v}])\), where \(\text{MLP}(\cdot)\) is a 2-layer MLP model with Sigmoid activation function in its output layer.

## 4 Experiments

### Experimental Settings

**Datasets and Baselines**. We conduct experiments on 13 benchmark datasets for temporal link prediction, which are Wikipedia, Reddit, MOOC, LastFM, Enron, Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote and Contact. Eleven popular temporal graph learning methods are selected as baselines including JODIE, DyRep, TGAT, TGN, CAWN, EdgeBank, TCL, GraphMixer, NAT, PINT, and DyGFormer. Details about datasets and baselines can be found in Appendix F.

**Task Settings**. The task settings strictly follow [9]. Specifically, we conduct experiments under two settings: 1) the transductive setting that predicts links between nodes that have been seen during training and 2) the inductive setting that predicts links between nodes that are not seen during training. Three different negative sampling strategies introduced by [16] are used to sample the negative links and the Average Precision (AP) and Area Under the Receiver Operating Characteristic Curve (AUC-ROC) are adopted as the evaluation metrics. For dataset splitting, we chronologically split

[MISSING_PAGE_FAIL:8]

speedups compared to the SOTA baseline DyGFormer and CAWN respectively on LastFM. The CAWN and DyGFormer models utilize time-consuming graph queries (e.g., temporal walk sampling) to construct relative encodings, which constitute the main computational bottleneck and consume over \(70\%\) of the running time. In contrast, TPNet caches historical interactions into node representations and constructs pairwise features based on these representations, thereby enhancing efficiency.

**Scalability Analysis**. To further verify the scalability of TPNet, we generated a series of random graphs with an average degree fixed at 100 and the number of edges varying from \(1\mathrm{e}5\) to \(1\mathrm{e}8\). Figure 4 shows the change of running time and GPU memory of TPNet, where the growth of the running time and GPU memory is close to and less than the linear growth curve respectively, showing the good scalability of TPNet. In contrast, PINT explicitly stores the temporal walk matrices and encounters out-of-memory error when the edge number reaches \(1\mathrm{e}7\) (shown inAppendix G.3), verifying the impracticability of explicitly storing temporal walk matrices.

### Ablation Study

**Proposed Components**. To verify the effectiveness of the proposed components in TPNet, we compare TPNet with the following variants: 1) w/o NR that remove the node representations and the corresponding features that decoded from them. 2) w/o Time that only considers the structural information by setting the time decay weight \(\lambda\) to be zero. 3) w/o Scale that remove the \(\log(\cdot)\) and \(\mathrm{ReLU}(\cdot)\) in the pairwise feature decoding. As shown in Table 2, there is a dramatic performance drop of w/o NR, which shows that the pairwise information carried by the node representations plays a vital role in the performance of TPNet. There is also an obvious performance drop of w/o Time, which confirms the necessity of incorporating temporal information in temporal walk matrix construction. Besides, the unreasonable poor performance of the w/o Scale is due to the various distribution of node representations across different layers, where, without scaling the raw pairwise features, the training will be unstable, and numerical overflow errors may even occur on some datasets. Further details on the distribution of node representations from different layers are provided in Appendix G.5.

**Dimension Change**. To verify the influence of the node representation dimension. We vary the dimensions from 1 to 128 and report the performance of TPNet (denoted as TPNet-d). As shown in Figure 3, the required dimension of node representations is small, where only 1-dimensional and 16-dimensional node representations can achieve satisfactory performance on MOOC and LastFM respectively. For different datasets, we observe that the average degree may be a main influence of the required dimension, where sparse graphs (like MOOC and Wikipedia) only need a small dimension, and dense graphs (like LastFM and Enron) may require a larger dimension. Empirically, setting the dimension to be \(10*\log(2\mathrm{E})\) is enough to achieve satisfactory performance on all datasets, where \(E\) is the number of edges. Results on more datasets can be found in Appendix G.4.

## 5 Related Work

Temporal link prediction [17] aims at predicting future interactions based on historical topology, which is crucial for a series of real-world applications [2; 3; 18]. Earlier methods considered the temporal graph as a series of graph snapshots that are sampled at regularly-spaced timestamps [19; 20], which will lose the fine-grained temporal information due to ignoring the temporal orders of

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Datasets & TPNet & w/o NR & w/o Time & w/o Scale \\ \hline MOOC & \(96.39_{+0.0}\) & \(83.21_{+0.19}\) & \(94.62_{+0.04}\) & \(63.04_{+0.00}\) \\ LastFM & \(94.50_{+0.00}\) & \(76.25_{+0.00}\) & \(94.30_{+0.00}\) & N/A \\ Enron & \(92.90_{+0.00}\) & \(83.23_{+0.00}\) & \(92.85_{+0.00}\) & N/A \\ UCI & \(97.35_{+0.00}\) & \(88.70_{+1.15}\) & \(97.19_{+0.00}\) & \(73.13_{+1.25}\) \\ US Legis. & \(0.858_{+1.15}\) & \(69.47_{+1.15}\) & \(71.83_{+1.00}\) & \(70.44_{+1.97}\) \\ UN Trade & \(87.24_{+0.00}\) & \(62.56_{+0.00}\) & \(65.98_{+0.00}\) & \(56.58_{+1.00}\) \\ UN Note & \(75.12_{+0.00}\) & \(52.58_{+0.00}\) & \(54.80_{+0.00}\) & \(53.20_{+0.00}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ablation study results, where N/A indicates the numerical overflow error.

Figure 3: Influence of node representation dimension.

interactions in a graph snapshot. Recently, some continuous-time temporal graph learning methods have been proposed [21; 3; 14; 13], which consider the temporal graph as a sequence of interactions with irregular time intervals to fully capture the graph dynamics. For example, TGN [21] maintained a dynamic memory vector for each node and generated node representations by aggregating memory vectors via temporal graph attention to capture the evolution pattern of the temporal graph. However, capturing pairwise information by merely aggregating representations of neighboring nodes [22] is challenging. To address this issue, the link-wise method was proposed, which constructs relative encodings as additional node features to inject the pairwise information into the representation learning process [6; 7; 9; 8]. For example, Souza et al. [8] proposed a relative encoding based on temporal walk counting and theoretically showed that constructing the relative encodings can improve the expressive power of models in distinguishing different links. Despite these advances, existing ways to construct the relative encodings are still far from satisfactory, where computation efficiency is a main concern and temporal information is seldom considered. In this paper, we unify existing relative encodings into a function of temporal walk matrices and explore encoding the pairwise information effectively and efficiently by temporal walk matrix projection.

## 6 Limitation

One limitation of our method is that the matrix construction approach requires manual predefined settings. Different networks may necessitate distinct construction methods, potentially leading to additional human effort in experimenting with various approaches. For instance, the proposed temporal walk matrix that incorporates the time decay effect may not be optimal for networks characterized by long-term dependencies. Developing an adaptive matrix construction technique will be an interesting direction for future research.

## 7 Conclusion

In this paper, we study the problem of pairwise information injection for temporal link prediction. We unify existing construction ways of relative encodings into a unified function, which reveals a connection between the relative encoding and temporal walk matrix. Then we propose a new temporal link prediction model, TPNet, to address the computational inefficiencies and the ignorance of temporal information in previous methods. TPNet introduces a new temporal walk matrix to simultaneously consider the temporal and structural information and a random feature propagation mechanism to maintain the temporal walk matrices efficiently. Theoretically, TPNet preserves the inner product of the maintained temporal walk matrices and empirically outperforms other link-wise methods in both effectiveness and efficiency. An interesting future direction may be designing an adaptive feature propagation mechanism.

## Acknowledgments

This work was supported by the National Natural Science Foundation of China (No. 62272023 and No. 62276015).

## References

* [1]P. Holme and J. Saramaki (2012) Temporal networks. Physics reports519 (3), pp. 97-125. Cited by: SS1.
* [2]Z. Fan, Z. Liu, J. Zhang, Y. Xiong, L. Zheng, and P. S. Yu (2021) Continuous-time sequential recommendation with temporal graph collaborative transformer. In International Conference on Information and Knowledge Management, pp. 433-442. Cited by: SS1.
* [3]S. Kumar, X. Zhang, and J. Leskovec (2019) Predicting dynamic embedding trajectory in temporal interaction networks. In International Conference on Knowledge Discovery & Data Mining, pp. 1269-1278. Cited by: SS1.
* [4]F. Zhou, X. Xu, G. Trajcevski, and K. Zhang (2022) A survey of information cascade analysis: models, predictions, and recent advances. ACM Comput. Surv.54 (2), pp. 27:1-27:36. Cited by: SS1.
* [5]X. Lu, S. Ji, L. Yu, L. Sun, B. Du, and T. Zhu (2023) Continuous-time graph learning for cascade popularity prediction. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pp. 2224-2232. Cited by: SS1.
* [6]Y. Wang, Y. Chang, Y. Liu, J. Leskovec, and P. Li (2021) Inductive representation learning in temporal networks via causal anonymous walks. In 9th International Conference on Learning Representations, Cited by: SS1.
* [7]Y. Luo and P. Li (2022) Neighborhood-aware scalable temporal network representation learning. In Learning on Graphs Conference, Cited by: SS1.
* [8]A. H. Souza, D. Mesquita, S. Kaski, and V. K. Garg (2022) Provably expressive temporal graph networks. In Advances in Neural Information Processing Systems, Cited by: SS1.
* [9]L. Yu, L. Sun, B. Du, and W. Lv (2023) Towards better dynamic graph learning: new architecture and unified library. In Advances in Neural Information Processing Systems, Cited by: SS1.
* [10]G. Hoang Nguyen, J. Boaz Lee, R. A. Rossi, N. K. Ahmed, E. Koh, and S. Kim (2018) Continuous-time dynamic network embeddings. In Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon, France, April 23-27, 2018, Cited by: SS1.
* [11]D. Sivakumar (2002) Algorithmic derandomization via complexity theory. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pp. 619-626. Cited by: SS1.
* [12]S. S. Vempala (2005) The random projection method. Vol. 65, American Mathematical Soc. Cited by: SS1.
* [13]W. Cong, S. Zhang, J. Kang, B. Yuan, H. Wu, X. Zhou, H. Tong, and M. Mahdavi (2023) Do we really need complicated model architectures for temporal networks?. In The Eleventh International Conference on Learning Representations, Cited by: SS1.
* [14]D. Xu, C. Ruan, E. Korpeoglu, S. Kumar, and K. Achan (2020) Inductive representation learning on temporal graphs. In 8th International Conference on Learning Representations, Cited by: SS1.
* [15]I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, M. Lucic, and A. Dosovitskiy (2021) MLP-mixer: an all-mlp architecture for vision. In Advances in Neural Information Processing Systems, Cited by: SS1.

[MISSING_PAGE_POST]

* [18] Le Yu, Zihang Liu, Leilei Sun, Bowen Du, Chuanren Liu, and Weifeng Lv. Continuous-time user preference modelling for temporal sets prediction. _IEEE Trans. Knowl. Data Eng._, 36(4):1475-1488, 2024.
* [19] Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. Dysat: Deep neural representation learning on dynamic graphs via self-attention networks. In _WSDM '20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020_, pages 519-527. ACM, 2020.
* [20] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao B. Schardl, and Charles E. Leiserson. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence_, 2020.
* [21] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael M. Bronstein. Temporal graph networks for deep learning on dynamic graphs. _CoRR_, abs/2006.10637, 2020.
* [22] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_, 2017.
* [23] Roman Vershynin. High-dimensional probability. _University of California, Irvine_, 10:11, 2020.
* [24] Moses Charikar. Similarity estimation techniques from rounding algorithms. In John H. Reif, editor, _Proceedings on 34th Annual ACM Symposium on Theory of Computing_, 2002.
* [25] Ping Li, Trevor Hastie, and Kenneth Ward Church. Very sparse random projections. In _Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Philadelphia, PA, USA, August 20-23, 2006_, 2006.
* [26] John Wright and Yi Ma. _High-dimensional data analysis with low-dimensional models: Principles, computation, and applications_. Cambridge University Press, 2022.
* [27] James W Pennebaker, Martha E Francis, and Roger J Booth. Linguistic inquiry and word count: Liwc 2001. _Mahway: Lawrence Erlbaum Associates_, 71(2001):2001, 2001.
* [28] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. Dyrep: Learning representations over dynamic graphs. In _International Conference on Learning Representations_, 2019.
* [29] Peter J Diggle. Spatio-temporal point processes: methods and applications. _Monographs on Statistics and Applied Probability_, 107:1, 2006.
* [30] Walter Rudin. _Fourier analysis on groups_. Courier Dover Publications, 2017.
* May 3, 2018, Conference Track Proceedings_, 2018.
* [32] Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng He, Le Song, Jingren Zhou, and Hongxia Yang. TCL: transformer-based dynamic graph modelling via contrastive learning. _CoRR_, abs/2105.07944, 2021.
* [33] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In _Annual Conference on Neural Information Processing Systems_, 2021.
* [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Annual Conference on Neural Information Processing Systems_, 2017.

Analysis of Existing Relative Encodings

### Nat

In Algorithm 1 of NAT, it maintains a series of hash maps \(s_{u}^{(0)},s_{u}^{(1)},..,s_{u}^{(k)}\) for each node \(u\), which are sets of node ids. Next, we will first prove the statement holds if the size of the hash maps is infinite.

**Statement.**\(s^{(i)}\) is a set of nodes that \(u\) can approach through a temporal walk whose length is less than \(i+1\).

Initially, the \(s_{u}^{(0)}\) is set as \(\{u\}\) and \(s_{u}^{(1)},..,s_{u}^{(k)}\) are set as empty set. The statement holds. When a new interaction \((\{u,v\},t)\) occurs and if the size of the hash maps is infinite, for \(1\leq i\leq k\), the updating function of hash maps can be represented as

\[\bar{s}_{u}^{(i)}=s_{u}^{(i)}\cup s_{v}^{(i-1)},\ \ \ \ \ \bar{s}_{v}^{(i)}=s_{v}^{(i)}\cup s_{u}^{(i-1)},\] (7)

where \(\bar{s}_{u}^{(i)}\) and \(s_{u}^{(i)}\) denote the hash map after and before adding the interaction respectively. Then if the timestamp of \((\{u,v\},t)\) is larger than that of previous interactions, the newly generated temporal walks beginning from \(u\) must first visit \(v\) through \((\{u,v\},t)\) (proved in Section B.1), thus the newly added nodes that \(u\) can visit through a temporal walk with length less than \(i+1\) must belong to \(s_{v}^{(i-1)}\). So \(\bar{s}_{u}^{(i)}\) will contain all the nodes that \(u\) can approach through a temporal walk with length less than \(i+1\) after adding \((\{u,v\},t)\). The statement holds.

For a node \(w\), its similarity feature \(\bm{r}^{w|u}\) is a \((k+1)\)-dimensional vector, where for \(1\leq i\leq k+1\), \(r_{i}^{w|u}=1\) if \(w\in s_{u}^{(i-1)}\) and \(r_{i}^{w|u}=0\) otherwise. Considering that the above statement holds, the similarity feature \(\bm{r}^{w|u}\) is equivalent to a \((k+1)\)-dimensional vector, where \(r_{i}^{w|u}=1\) if the shortest temporal walk from \(u\) to \(w\) is less than \(i\); otherwise, \(r_{i}^{w|u}=0\).

```
1 Initialize \(W\) to be \(\{(u,t)\}\) ;
2forfrom\(1\) to\(k\)do
3\((w_{\text{p}},t_{\text{p}})\leftarrow\) the last (node, time) pair in \(W\);
4 Sample one \((\{w_{p},w^{\prime}\},t^{\prime})\in\mathcal{E}_{w_{\text{p}},t_{\text{p}}}\) with prob. \(\propto\)\(\exp(-\alpha(t_{\text{p}}-t^{\prime}))\) ;
5\(W_{i}\gets W_{i}\oplus(w^{\prime},t^{\prime})\);
6 Return W; ```

**Algorithm 2**Temporal Walk Extraction (\(\mathcal{G}(t)\), \(\alpha\), \(k\), \(u\))

### Cawn

For constructing \(\bm{r}^{w|u}\), CAWN first repeatedly samples \(m\) temporal walks of length \(k\) begging at \(u\) according to the sampling strategy in Algorithm 2. Then \(\bm{r}^{w|u}\) is set to be a (\(k+1\))-dimensional vector, where \(r_{i}^{w|u}\) will be the number of walks whose i-th visited node is \(w\) for \(1\leq i\leq k+1\), which can be represented as,

\[r_{i}^{w|u}=\sum_{j=1}^{m}\bm{1}_{W_{j}[i][0]=w},\] (8)

where \(W_{j}\) is the j-th sampled walks and \(\bm{1}_{W_{j}[i][0]=w}\) is \(1\) if \(W_{j}[i][0]=w\); otherwise, \(\bm{1}_{W_{j}[i][0]=w}\) is \(0\). Due to each temporal walk being sampled independently, \(\bm{1}_{W_{1}[i][0]=w},\cdots,\bm{1}_{W_{m}[i][0]=w}\) are \(m\) independent and identically distributed Bernoulli random variables \(\text{Ber}(\mu_{w}^{i-1})\), where \(\mu_{w}^{i-1}\) is the probability of reaching \(w\) from \(u\) after performing a \((i-1)\)-step temporal walk according to Algorithm 2. And according to the strong law of large numbers (Theorem 1.3.1 of [23]), the mean of these random variables (i.e., \(\frac{1}{m}\sum_{j=1}^{m}\bm{1}_{W_{j}[i][0]=w}\iff\frac{1}{m}r_{i}^{w|u}\)) will coverage to the mean as the number of sampled walks \(m\rightarrow\infty\). The mean of the \(\text{Ber}(\mu_{w}^{i-1})\) is \(\mu_{w}^{i-1}\). Expand all \((i-1)\)-step temporal walks from \(u\) to \(w\), we have

\[\mu_{w}^{i-1}=\sum_{W\in\mathcal{M}_{u,w}^{i-1}}f(W),\] (9)where \(f(\cdot)\) is the sampled probability of a walk according to Algorithm 2. For Algorithm 2, if we are current at \((w_{i},t_{i})\), then the probability of moving to \((w_{i+1},t_{i+1})\) through an interaction \((\{w_{i},w_{i+1}\},t_{i+1})\) is proportional to \(\exp(-\alpha(t_{i}-t_{i+1}))\), which is \(\frac{\exp(-\alpha(t_{i}-t_{i+1}))}{\sum_{(w^{\prime},t^{\prime})\in\mathcal{E }_{w_{i},t_{i}}}\exp(-\alpha(t_{i}-t^{\prime}))}\). Thus the probability of a temporal walk \(W=[(w_{0},t_{0}),(w_{1},t_{1}),..,(w_{k},t_{k})]\) is

\[f(W)=\prod_{i=0}^{k-1}\frac{\exp(-\alpha(t_{i}-t_{i+1}))}{\sum_{( \{w^{\prime},w\},t^{\prime})\in\mathcal{E}_{w_{i},t_{i}}}\exp(-\alpha(t_{i}-t ^{\prime}))}\] (10)

which is the same as the score function \(s^{\prime}(\cdot)\) of CAWN we shown in Section 2.2.2. Thus, \(r_{i}^{w|u}\) (multiplied with a const \(\frac{1}{m}\)) is the same as \(\sum_{W\in\mathcal{M}_{u,w}^{l-1}}s^{\prime}(W)\).

## Appendix B Proofs

### Proof of Theorem 1

The equation in Theorem 1 can be rewritten as \(\bm{H}^{(l)}(t)=e^{\lambda lt}*\bm{A}^{(l)}(t)\bm{P}\). We can consider \(e^{\lambda lt}*\bm{A}^{(l)}(t)\) as a new temporal walk matrix whose score function for a temporal walk \([(w_{0},t_{0}),(w_{1},t_{1}),...,(w_{l},t_{l})]\) at time \(t\) is \(e^{\lambda lt}*s(W)\). Expanding it, we have

\[e^{\lambda lt}*s(W)=e^{\lambda lt}*\prod_{j=1}^{l}e^{-\lambda(t-t_{j})}=\prod _{j=1}^{l}e^{-\lambda(t-t_{j})}*e^{\lambda l}=\prod_{j=1}^{l}e^{\lambda t_{j}}.\] (11)

We use the notation \(\bm{\bar{A}}^{(l)}(t)\) to denote \(e^{\lambda lt}*\bm{A}^{(l)}(t)\) and \(\bar{s}(W)\) to denote \(e^{\lambda lt}*s(W)\) for a \(l\)-step temporal walk in the following proof. The original problem is transformed into proving that \(\bm{H}^{(l)}(t)=\bm{\bar{A}}^{(l)}(t)\bm{P}\). Note that for a given temporal walk \([(w_{0},t_{0}),(w_{1},t_{1}),...,(w_{l},t_{l})]\), each term of \(\bar{s}(W)\) (i.e., \(\prod_{j=1}^{l}e^{\lambda t_{j}}\)) will no change as time \(t\) goes on. Thus the temporal walk matrices \(\bm{\bar{A}}^{(l)}(t)\) will only change when a new interaction occurs. Next, we inspect how the \(\bm{\bar{A}}^{(l)}(t)\) changes when a new interaction \((u,v,t)\) occurs. For each element \(A_{i,j}^{(l)}(t)\), its change is caused by the newly generated \(l\)-step temporal walks from \(i\) to \(j\) after adding the interaction \((u,v,t)\), which can be written as

\[A_{i,j}^{(l)}(t^{+})=A_{i,j}^{(l)}(t)+\sum_{W\in\Delta M_{i,j}^{l}}\bar{s}(W),\] (12)

where \(t^{+}\) denotes the timestamps right after \(t\) and \(\Delta M_{i,j}^{l}\) denote the newly generated \(l\)-step temporal walks from \(i\) to \(j\). According to the definition of the temporal walk, the new \(l\)-step temporal walks must begin from \(u\) or \(v\). Because if there is a temporal walk that does not begin from \(u\) or \(v\), it can be represented as \([(w_{0},t_{0}),..,(w_{i},t_{i}),(u,t_{i+1}),(v,t),..,(w_{l},t_{l}))]\) or \([(w_{0},t_{0}),..,(w_{i},t_{i}),(v,t_{i+1}),(u,t),..,(w_{l},t_{l}))]\), which means that there is an interaction \((\{w_{i},u\},t_{i+1})\) or \((\{w_{i},v\},t_{i+1})\) whose timestamp \(t_{i}\) is larger than \(t\). (Since the timestamps of a temporal walk are decreasing). This is impossible since \((u,v,t)\) is a newly happened interaction. Thus only the \(u\)-th row and \(v\)-th row of the \(\bm{\bar{A}}^{(l)}(t)\) will change. Besides for each newly generated temporal walk from \(u\), it must be \([(u,t),(v,t_{1}),..,(w_{l},t_{l}))]\), where \([(v,t_{1}),..,(w_{l},t_{l}))]\) corresponds to a \((l-1)\)-step temporal walk beginning from \(v\). And for any \((l-1)\)-step temporal walk beginning

Figure 6: Illustration of the newly generated 3-step temporal walks beginning from \(D\) after adding a new interaction \((\{C,D\},t_{4})\) There is a one-to-one map between \(\Delta M_{D,*}^{3}\) and \(M_{C,*}^{2}(t_{4})\).

from \(v\), we can add a prefix \((u,t)\) to make it become a \(l\)-step temporal walk beginning from \(u\). Thus for any \(1\leq i\leq n\), there is a one-to-one mapping between \(\Delta M_{u,i}^{l}\) and \(M_{v,i}^{l-1}(t)\) with \(M_{v,i}^{l-1}(t)\) denote the set of \((l-1)\)-th temporal walks from \(v\) to \(i\) before \(t\), and we can rewritten Equation (12) as

\[A_{u,i}^{(l)}(t^{+})=A_{u,i}^{(l)}(t)+\sum_{W\in M_{v,i}^{l-1}(t)}e^{-\lambda t }*\bar{s}(W)=A_{u,i}^{(l)}(t)+e^{-\lambda t}*A_{v,i}^{(l-1)}(t),\] (13)

The updating function for \(\bar{A}_{v,i}^{(l)}(t)\) is also similar. We give a visual illustration of the new temporal walks in Figure 6 for better understanding. Finally, writing the update formula in vector form, for any \(1\leq l\leq k\) we have

\[\bm{\bar{A}}_{u}^{(l)}(t^{+})=\bm{\bar{A}}_{u}^{(l)}(t)+e^{\lambda t}*\bm{\bar {A}}_{v}^{(l-1)}(t),\ \ \ \ \bm{\bar{A}}_{v}^{(l)}(t^{+})=\bm{\bar{A}}_{v}^{(l)}(t)+e^{\lambda t}*\bm{\bar {A}}_{u}^{(l-1)}(t),\] (14)

which is the same as Equation (14)! Thus if \(\bm{H}^{(l)}(t)=\bm{\bar{A}}^{(l)}(t)\bm{P}\) for \(0\leq l\leq k\), after adding a new interaction \((u,v,t)\), for \(0\leq l\leq k\), \(\bm{H}^{(l)}(t^{+})=\bm{\bar{A}}^{(l)}(t^{+})\bm{P}\) still holds. Because we have

\[\bm{H}_{u}^{(l)}(t^{+})=\bm{H}_{u}^{(l)}(t)+e^{\lambda t}*\bm{H}_{v}^{(l-1)}( t)=(\bm{\bar{A}}_{u}^{(l)}(t)+e^{\lambda t}*\bm{\bar{A}}_{v}^{(l-1)}(t))\bm{P}= \bm{\bar{A}}_{u}^{(l)}(t^{+})\bm{P}\] (15)

Note that the equation in Equation (1) holds at initialization, thus the equation always holds and the theorem is proved.

### Proof of Theorem 2

The Theorem 2 can be considered as a special case of the Johnson-Lindenstrauss Lemma [11], where the random projection [12, 24, 25] can preserve the norm and inner product. For the convenience of readers lacking relevant background, we follow [26] to provide the proof under the given conditions of this paper. We begin the proof by the following lemma.

**Lemma 1** ((Lemma 3.18 of [26])).: _Let \(\bm{x}\in\mathbb{R}^{d}\) be a \(d\)-dimensional random vector whose entries are independent \(\mathcal{N}(0,\frac{1}{d})\). Then for any \(\epsilon\in[0,1]\),_

\[\mathbb{P}\left(\left\|\bm{x}\right\|_{2}^{2}-1\right|>\epsilon\right)\leq 2 \exp(\frac{-\epsilon^{2}d}{8})\] (16)

The following part can be divided into 1) We first give two corollaries and their proofs. 2) Then we give the proof of Theorem 2 based on the corollaries.

#### b.2.1 Two Corolarries

Based on the above lemma, we can get the following corollaries.

**Corollary 1**.: _Given any \(\epsilon\in(0,1),\bm{x}\in\mathbb{R}^{m}\). Let \(\bm{P}\in\mathbb{R}^{d\times m}\) be a random matrix whose entries are independent \(\mathcal{N}(0,\frac{1}{d})\), if \(d\geq\frac{8}{\epsilon^{2}}\log(\frac{1}{\delta})\), we have_

\[\mathbb{P}\left((1-\epsilon)\|\bm{x}\|_{2}^{2}\leq\|\bm{P}\bm{x}\|_{2}^{2} \leq(1+\epsilon)\|\bm{x}\|_{2}^{2}\right)\geq 1-2\delta\] (17)

**Proof**. For the above corollary, we have

\[\mathbb{P}\left((1-\epsilon)\|\bm{x}\|_{2}^{2}\leq\|\bm{P}\bm{x}\|_{2}^{2} \leq(1+\epsilon)\|\bm{x}\|_{2}^{2}\right)\geq 1-2\delta\]

\[\iff\mathbb{P}\left(\left|\frac{\|\bm{P}\bm{x}\|_{2}^{2}}{\|\bm{x}\|_{2}^{2}}- 1\right|\leq\epsilon\right)\geq 1-2\delta\] (18)

Note that each entry of \(\bm{P}\) is an independent \(\mathcal{N}(0,\frac{1}{d})\), thus \(\bm{P}\bm{x}\) is a random vector whose each entry \((\bm{P}\bm{x})_{k}=\sum_{i=1}^{m}P_{k,i}*x_{i}\) is an independent \(\mathcal{N}(0,\frac{\|\bm{x}\|_{2}^{2}}{d})\) and each entry of \(\frac{\bm{P}\bm{x}}{\|\bm{x}\|_{2}}\) is an independent \(\mathcal{N}(0,\frac{1}{d})\). Substituting it to Lemma 1 and taking \(d\geq\frac{8}{\epsilon^{2}}\log(\frac{1}{\delta})\), we have

\[\mathbb{P}\left(\left\|\frac{\|\bm{P}\bm{x}\|_{2}^{2}}{\|\bm{x}\|_{2}^{2}}-1 \right|>\epsilon\right)\leq 2\exp(\frac{-\epsilon^{2}d}{8})\]

\[\leq 2\exp(\frac{-\epsilon^{2}}{8}*\frac{8}{\epsilon^{2}}*\log(\frac{1}{\delta})) \leq 2\delta\]

Based on Corollary 1, we can get the following corollary.

**Corollary 2**.: _Given any \(n\)\(m\)-dimensional vectors \(\bm{x}_{1},...,\bm{x}_{n}\), \(\epsilon\in(0,1)\), let \(\bm{P}\in\mathbb{R}^{d\times m}\) be a random matrix whose entries are independent \(\mathcal{N}(0,\frac{1}{d})\), if \(d\geq\frac{24}{\epsilon^{2}}\log(4^{1/3}n)\), for any \(1\leq i,j\leq n\), we have_

\[\mathbb{P}\left(|\langle\bm{P}\bm{x_{i}},\bm{P}\bm{x_{j}}\rangle-\langle\bm{x }_{i},\bm{x}_{j}\rangle|\leq\frac{\epsilon}{2}(\|\bm{x}_{i}\|_{2}^{2}+\|\bm{x }_{j}\|_{2}^{2})\right)\geq 1-\frac{1}{n}\] (20)

let \(E_{i,j}^{+}\) and \(E_{i,j}^{-}\) denote the event of \(\{\|\bm{P}(\bm{x_{i}}+\bm{x_{j}})\|_{2}^{2}-\|\bm{x_{i}}+\bm{x_{j}}\|_{2}^{2} \big{|}\leq\epsilon\|\bm{x_{i}}+\bm{x_{j}}\|_{2}^{2}\}\) and \(\{\|\bm{P}(\bm{x_{i}}-\bm{x_{j}})\|_{2}^{2}-\|\bm{x_{i}}-\bm{x_{j}}\|_{2}^{2} \big{|}\leq\epsilon\|\bm{x_{i}}-\bm{x_{j}}\|_{2}^{2}\}\) respectively. Since \(\bm{x_{i}}+\bm{x_{j}}\) and \(\bm{x_{i}}-\bm{x_{j}}\) can also be consider two \(m\)-dimensional vectors. Thus take it and \(d\geq\frac{24}{\epsilon^{2}}\log(4^{1/3}n)\) into Corollary 1, we have \(\mathbb{P}(E_{i,j}^{+})\geq 1-\frac{1}{2n^{3}}\) and \(\mathbb{P}(E_{i,j}^{-})\geq 1-\frac{1}{2n^{3}}\). Then let \(C_{i,j}=E_{i,j}^{+}\cap E_{i,j}^{-}\) denote that \(E_{i,j}^{+}\) and \(E_{i,j}^{-}\) hold simultaneously, according to the union bound, we have

\[\mathbb{P}(C_{i,j})=1-\mathbb{P}(\overline{E_{i,j}^{+}}\cup\overline{E_{i,j}^ {-}})\geq 1-(\mathbb{P}(\overline{E_{i,j}^{+}})+\mathbb{P}(\overline{E_{i,j}^ {-}}))\geq 1-\frac{1}{n^{3}},\] (21)

where \(\overline{E_{i,j}^{+}}\) denote that \(E_{i,j}^{+}\) does not hold and \(\overline{E_{i,j}^{+}}\cup\overline{E_{i,j}^{-}}\) denotes that \(\overline{E_{i,j}^{+}}\) or \(\overline{E_{i,j}^{-}}\) holds. According to the union bound, we further have

\[\mathbb{P}(\cap_{i,j}C_{i,j})=1-\mathbb{P}(\cup_{i,j}\overline{C_{i,j}})\geq 1 -\sum_{i,j}\mathbb{P}(\overline{C_{i,j}})\geq 1-n^{2}*\frac{1}{n^{3}}\geq 1- \frac{1}{n},\] (22)

where \(\cap_{i,j}C_{i,j}\) denote that \(C_{i,j}\) holds for any \(i,j\) and \(\cup_{i,j}\overline{C_{i,j}}\) denote that there exist \(i,j\) that \(\overline{C_{i,j}}\) holds. If \(C_{i,j}\) holds, we have

\[(1-\epsilon)\|\bm{x_{i}}+\bm{x_{j}}\|_{2}^{2} \leq\|\bm{P}(\bm{x_{i}}+\bm{x_{j}})\|_{2}^{2}\leq(1+\epsilon)\| \bm{x_{i}}+\bm{x_{j}}\|_{2}^{2}\] (23) \[(1-\epsilon)\|\bm{x_{i}}-\bm{x_{j}}\|_{2}^{2} \leq\|\bm{P}(\bm{x_{i}}-\bm{x_{j}})\|_{2}^{2}\leq(1+\epsilon)\|\bm{x_ {i}}-\bm{x_{j}}\|_{2}^{2}\] (24)

Multiplying Equation (24) with \(-1\) and adding it to (23), we have

\[|\langle\bm{P}\bm{x_{i}},\bm{P}\bm{x_{j}}\rangle-\langle\bm{x_{i}},\bm{x_{j}} \rangle|\leq\frac{\epsilon}{2}(\|\bm{x_{i}}\|_{2}^{2}+\|\bm{x_{j}}\|_{2}^{2})\] (25)

Thus we have

\[\mathbb{P}(\cap_{i,j}C_{i,j})\geq 1-\frac{1}{n}\implies \mathbb{P}(|\langle\bm{P}\bm{x_{i}},\bm{P}\bm{x_{j}}\rangle-\langle\bm{x_{i}}, \bm{x_{j}}\rangle|\leq\frac{\epsilon}{2}(\|\bm{x_{i}}\|_{2}^{2}+\|\bm{x_{j}}\|_ {2}^{2}))\geq 1-\frac{1}{n}\] (26)

holds for any \(1\leq i,j\leq n\).

#### b.2.2 Proof based on Corollaries

The matrix of \(\bm{A}^{(l)}(t)\) in Theorem 2 can be considered as \(n\) vectors with \(n\) dimensions, where each row of \(\bm{A}^{(l)}(t)\) is a \(n\)-dimensional vector. Similarly, we can consider \(\bm{A}^{(0)}(t),\cdots,\bm{A}^{(k)}(t)\) together as \(n(k+1)\) vectors with \(n\) dimensions. Considering that \(\bm{P}\in\mathbb{R}^{n\times d_{R}}\) is a random matrix where each entry is an independent \(\mathcal{N}(0,\frac{1}{d_{R}})\) and \(e^{-\lambda lt}*\bm{H}^{(l)}(t)\) is the projection of \(\bm{A}^{(l)}(t)\), substitute it into Corollary 2 and taking the number of vectors as \((k+1)n\), we can get Theorem 2.

## Appendix C Batch Updating Mechanism

For the situation where multiple interactions happen simultaneously, we can first compute each interaction's contribution independently and sum them together to update the node representations. The maintaining mechanism is shown in Algorithm 3, where we packed the interactions that happen at the same time into a set \(\mathcal{B}\) and sum the independent contribution of each interaction in \(\mathcal{B}\) into \(\Delta\bm{H}^{(1)},\cdots,\Delta\bm{H}^{(k)}\). For simplicity, we initialize \(\Delta\bm{H}^{(1)},\cdots,\Delta\bm{H}^{(k)}\) each time and it can be replaced by some efficient implementation such as scatter_add operation in pytorch.

## Appendix D Propagation Mechanism for Other Matrices

For simplicity, here we only consider the situation where the timestamp of each interaction is different and we can adopt a similar batch updating mechanism in Section C to handle the situation where multiple interactions happen simultaneously. For the updating of other types of temporal walk matrices, let \(\bm{A}(t)=[\bm{A}^{(0)}(t),\cdots,\bm{A}^{(k)}(t)]\in\mathbb{R}^{n(k+1)\times d}\) denotes the concatenation of the temporal walk matrices. If the following two situations can be satisfied simultaneously, we can apply the random feature propagation mechanism to implicitly maintain the temporal walk matrices.

**Condition 1**. After adding a new interaction \((u,v,t)\), the change of each row can be written as the linear combination of other rows, which is for each \(1\leq i\leq n(k+1)\), there exists \(k_{1},...,k_{m}\) and \(l_{1},...,l_{m}\) satisfying.

\[\bm{A}_{i}(t^{+})=k_{1}\bm{A}_{l_{1}}(t)+\cdots+k_{m}\bm{A}_{l_{m}}(t),\] (27)

where \(t^{+}\) is the time right after \(t\).

**Condition 2**. After time moving \(\Delta t\) without adding new interactions, the change of each row can be written as the linear combination of other rows, which is for each \(1\leq i\leq n(k+1)\), there exists \(k_{1},...,k_{m}\) and \(l_{1},...,l_{m}\) satisfying.

\[\bm{A}_{i}(t+\Delta t)=k_{1}\bm{A}_{l_{1}}(t)+\cdots+k_{m}\bm{A}_{l_{m}}(t)\] (28)

The motivation behind the conditions is that the projection is a linear operation. If the node representations are the projection of the temporal walk matrix at \(t\) (i.e.,\(\bm{H}(t)=\bm{A}(t)\bm{P}\)) and the updating function of the temporal walk matrix is the linear combination of other rows, then we can apply the same updating function on \(\bm{H}(t)\), which will make \(\bm{H}(t^{+})\) still be the projection of the temporal walk matrix. For example, applying (27) to \(\bm{H}(t)\) will get

\[\bm{H}_{i}(t^{+}) =k_{1}\bm{H}_{l_{1}}(t)+\cdots+k_{m}\bm{H}_{l_{m}}(t)\] (29) \[=(k_{1}\bm{A}_{l_{1}}(t)+\cdots+k_{m}\bm{A}_{l_{m}}(t))\bm{P}=\bm {A}_{i}(t^{+})\bm{P}\]

Then we can ensure that \(\bm{H}(t)\) is always the random projection of \(\bm{A}(t)\) and thus preserve the inner product of the \(\bm{A}(t)\).

### Detailed Updating Mechanism

Based on the above analysis, we give the detailed propagation mechanism of methods in Section 2. For NAT, PINT and DyGFormer, their temporal matrix element \(A_{u,v}^{(l)}(t)\) is the number of the \(l\)-step temporal walks from \(u\) to \(v\) and their feature propagation mechanism is shown in Algorithm 4, where the obtained node representations are the projection of the corresponding temporal walk matrix.

For CAWN, its score function for a temporal walk \(W=[(w_{0},t_{0}),(w_{1},t_{1}),...,(w_{l},t_{l})]\) is defined as \(s(W)=\prod_{i=0}^{l-1}\frac{\exp(-\alpha(t_{i}-t_{i+1}))}{\sum_{((w^{\prime}, w),t^{\prime})\in\sigma_{w_{i},t_{i}}}\exp(-\alpha(t_{i}-t^{\prime}))}\). As time goes on, its element of temporal walkmatrices will not change. When a new interaction \((u,v,t)\) happens, for a temporal walk matrix \(\bm{A}^{(l)}(t)\), its \(u\)-th row and \(v\)-th row will change. Formally, let \(\bm{d}(t)\in\mathbb{R}^{n}\) denotes a time decay degree vector, where for each node \(u\), \(d_{u}(t)\) is defined as \(d_{u}(t)=\sum_{(\{(u,v^{\prime}),t^{\prime}\}\in\mathcal{E}_{u,t}}\exp(-\alpha (t^{\prime}-t))\) with \(\mathcal{E}_{u,t}\) denoting the set of interactions attached to \(u\) before \(t\), then the updating function of the temporal walk matrix \(\bm{A}^{(k)}(t)\) can be represented as

\[\begin{split}\bm{A}^{(l)}_{u}(t^{+})&=\frac{d_{u} (t)}{d_{u}(t)+1}*\bm{A}^{(l)}_{u}(t)+\frac{1}{d_{u}(t)+1}*\bm{A}^{(l-1)}_{v}(t),\\ \bm{A}^{(l)}_{v}(t^{+})&=\frac{d_{v}(t)}{d_{v}(t)+1 }*\bm{A}^{(l)}_{v}(t)+\frac{1}{d_{v}(t)+1}*\bm{A}^{(l-1)}_{u}(t),\end{split}\] (30)

where \(\frac{d_{u}(t)}{d_{u}(t)+1}*\bm{A}^{(l)}_{u}(t)\) corresponds to the change in score of the old \(l\)-step temporal walks begging from \(u\) and \(\frac{1}{d_{u}(t)+1}*\bm{A}^{(l-1)}_{v}(t)\) correspond to change caused by the newly generated \(l\)-step temporal walk from \(u\) (the same for \(v\) and similar analysis about the updating of temporal walk matrix can be found in Appendix B.1). Then we can give the propagation mechanism for CAWN in Algorithm 5 based on the above analysis, where \(\bm{d}\) corresponds to the time decay degree vector.

```
1Initialize \(\bm{H}^{(0)},\bm{H}^{(1)},...,\bm{H}^{(k)}\in\mathbb{R}^{n\times d_{R}}\) as zero matrix ;
2Fill \(\bm{H}^{(0)}\) with entries independently drawn from \(\mathcal{N}(0,\frac{1}{d_{R}})\) ;
3for\((u,v,t)\in\mathcal{G}\)do
4for\(l=k\)to 1 do
5\(\bm{H}^{(l)}_{u}=\bm{H}^{(l)}_{u}+\bm{H}^{(l-1)}_{v}\) ;
6\(\bm{H}^{(l)}_{v}=\bm{H}^{(l)}_{v}+\bm{H}^{(l-1)}_{u}\) ;
7
8Return \(\bm{H}^{(0)},\bm{H}^{(1)},..,\bm{H}^{(k)}\); ```

**Algorithm 5**Propagation Mechanism for CAWN (\(\mathcal{G}\),\(\alpha\),\(k\),\(n\),\(d_{R}\))

## Appendix E Broader Impact

We proposed an effective and efficient temporal link prediction method, which may advance real-world scenarios that rely on link prediction as a cornerstone, such as recommendation systems. For potential negative impacts, overly accurate link prediction in some contexts may lead to imbalanced outcomes such as reduced diversity in recommendation systems.

Experimental Settings

### Datasets

Experiments are conducted on the following 13 benchmark datasets 3 collected by [16].

Footnote 3: https://zenodo.org/record/7213796#.Y1c06y8r30o

* **Wikipedia** is a bipartite interaction graph that records the edits on Wikipedia pages over a month. Nodes represent users and pages, and links denote the editing behaviors with timestamps. Each link is associated with a 172-dimensional Linguistic Inquiry and Word Count (LIWC) feature [27].
* **Reddit** is a bipartite graph capturing user posts under subreddits over a month. Nodes represent users and subreddits, while links denote timestamped posts. Each link carries a 172-dimensional LIWC feature.
* **MOOC** is a bipartite interaction network of online courses, where nodes represent students and course content units (e.g., videos, problem sets). Links indicate students' access to specific content units and have a 4-dimensional feature.
* **LastFM** is a bipartite network detailing song-listening behaviors of users over one month. Nodes are users and songs, and links represent listening activities.
* **Enron** records email communications between employees of the Enron Energy Corporation over three years.
* **Social Evo.** is a mobile phone proximity network tracking daily activities within an undergraduate dormitory for eight months, with each link having a 2-dimensional feature.
* **UCI** is an online communication network with nodes representing university students and links representing posted messages.
* **Flights** is a dynamic flight network showing air traffic development during the COVID-19 pandemic. Nodes represent airports, and links denote tracked flights. Each link has a weight indicating the number of flights between two airports per day.
* **Can. Parl.** is a dynamic political network recording interactions between Canadian Members of Parliament (MPs) from 2006 to 2019. Nodes represent MPs from electoral districts, and links are formed when two MPs vote "yes" on a bill. The weight of each link represents the number of times one MP voted "yes" in support of another MP in a year.
* **US Legis.** is a senate co-sponsorship network tracking social interactions between US Senators. The weight of each link indicates the number of times two senators co-sponsored a bill in a given congress.
* **UN Trade** captures food and agriculture trade between 181 nations over more than 30 years. The weight of each link represents the total normalized agriculture import or export values between two countries.
* **UN Vote** records roll-call votes in the United Nations General Assembly. A link between two nations increases in weight each time both vote "yes" on an item.
* **Contact** tracks the evolution of physical proximity among approximately 700 university students over a month. Each student has a unique identifier, and links indicate close proximity, with weights revealing the extent of physical closeness between students.

The statistics of the datasets are shown in Table 3, where #N&L Feat stands for the dimensions of node and link features.

### Baselines

We select the following eleven popular baselines:

* **JODIE**[3] designs a recurrent architecture to maintain a memory vector for each node and a projection layer to map the node memories into future representation trajectories.
* **DyRep**[28] considers each link as a temporal point process [29] and designs a deep temporal point process model to capture the dynamics of the observed process.

[MISSING_PAGE_FAIL:20]

### Influence of Node Representation Dimension

Results on Wikipedia, Enron, and UCI are shown in Figure 8.

### Statistic Analysis of Node Representations

Table 5 shows the mean of node representation norm at different layers, where aEb indicates \(a\times 10^{b}\).

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multirow{2}{*}{Edge Number} & \multicolumn{2}{c}{PINT} \\ \cline{2-4}  & Time & Memory \\ \hline
100000 & 51.18 & 92.32 \\
1000000 & 594.77 & 2175.80 \\
1000000 & N/A & OOM \\
100000000 & N/A & OOM \\ \hline \hline \end{tabular}
\end{table}
Table 4: Scalability analysis of PINT.

Figure 7: Efficiency analysis on more datasets.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline Metrics & Datasets & JODIE & DyRep & TGAT & TGN & CAWN & TCL & GraphMixer & NAT & PINT & DyGFormer & RPNet \\ \hline \multirow{9}{*}{AP} & Wikipedia & 94.82\({}_{1.03}\) & 92.43\({}_{1.37}\) & 96.22\({}_{2.07}\) & 97.83\({}_{1.04}\) & 98.24\({}_{1.03}\) & 96.22\({}_{2.10}\) & 96.65\({}_{3.41}\) & 96.30\({}_{1.03}\) & 98.03\({}_{1.04}\) & 98.59\({}_{3.03}\) & **98.91\({}_{0.01}\)** \\  & Reddit & 96.50\({}_{1.03\pm 1.9}\) & 96.09\({}_{1.03\pm 1.9}\) & 97.09\({}_{1.03\pm 1.9}\) & 97.50\({}_{1.07}\) & 98.62\({}_{1.04\pm 1.9}\) & 94.09\({}_{1.00\pm 1.07}\) & 95.26\({}_{1.03\pm 1.9}\) & 98.24\({}_{1.04\pm 1.9}\) & 98.56\({}_{1.03}\) & 98.84\({}_{1.03}\) & **98.86\({}_{1.03}\)** \\  & MOOC & 97.63\({}_{1.03\pm 1.04}\) & 81.07\({}_{1.04\pm 1.8}\) & 85.09\({}_{1.04\pm 1.8}\) & 81.42\({}_{1.04\pm 1.8}\) & 80.60\({}_{1.03\pm 1.1}\) & 81.36\({}_{2.11\pm 1.9}\) & 87.90\({}_{1.04\pm 1.9}\) & 86.56\({}_{1.04\pm 1.9}\) & **95.07\({}_{1.04\pm 1.9}\)** \\  & LastFM & 81.61\({}_{1.03\pm 1.04}\) & 83.02\({}_{1.03\pm 1.8}\) & 87.63\({}_{1.03\pm 1.04}\) & 81.45\({}_{1.02\pm 1.0}\) & 89.24\({}_{1.02\pm 1.0}\) & 82.11\({}_{1.02\pm 1.0}\) & 82.21\({}_{1.03\pm 1.9}\) & 92.42\({}_{1.04\pm 1.9}\) & **92.32\({}_{1.05}\)** & **95.36\({}_{1.01}\)** \\  & Enron & 80.72\({}_{1.17\pm 1.9}\) & 74.55\({}_{1.53\pm 1.6}\) & 67.05\({}_{1.51}\) & 77.94\({}_{1.02\pm 1.0}\) & 86.35\({}_{1.51}\) & 76.14\({}_{1.07}\) & 75.88\({}_{1.04\pm 1.9}\) & 87.18\({}_{1.14}\) & 88.12\({}_{1.03\pm 1.9}\) & 89.76\({}_{1.04}\) & **90.34\({}_{1.04\pm 1.9}\)** \\  & Social Evo. & 91.96\({}_{1.03\pm 1.9}\) & 90.40\({}_{1.04\pm 1.9}\) & 91.41\({}_{1.04\pm 1.9}\) & 90.77\({}_{1.03\pm 1.9}\) & 99.15\({}_{1.05\pm 1.0}\) & 91.86\({}_{1.04\pm 1.9}\) & 87.41\({}_{1.04\pm 1.9}\) & **92.40\({}_{1.03\pm 1.9}\)** & 93.14\({}_{1.04\pm 1.9}\) & **93.24\({}_{1.07}\)** \\  & UCI & 79.86\({}_{1.04\pm 1.9}\) & 87.54\({}_{1.05\pm 1.7}\) & 88.75\({}_{1.04\pm 1.8}\) & 88.12\({}_{1.02\pm 1.0}\) & 92.73\({}_{1.03\pm 1.0}\) & 91.19\({}_{1.04\pm 1.9}\) & 87.31\({}_{1.04\pm 1.9}\) & 94.22\({}_{1.03\pm 1.9}\) & 94.54\({}_{1.01\pm 1.9}\) & **95.74\({}_{1.04\pm 1.9}\)** \\  & Flights & 94.74\({}_{1.02\pm 1.9}\) & 92.88\({}_{1.03\pm 1.0}\) & 87.30\({}_{1.03\pm 1.0}\) & 95.03\({}_{1.03\pm 1.0}\) & 90.37\({}_{1.04\pm 1.9}\) & 83.03\({}_{1.04\pm 1.9}\) & 96.74\({}_{1.02\pm 1.9}\) & 97.25\({}_{1.03

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction list the limitations of existing relative encodings, and the goal of this paper is to tackle such limitations. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitation in the appendix. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All theorems include the full sets of assumptions and proof. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the implementation details of our method in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We include our codes in the supplemental material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Implementation details are shown in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in the appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the mean and standard deviation in tables that record the experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We list the compute resources of our experiments in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our paper follows the NeurIPS Code of Ethics in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the potential societal impacts in the appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: The proposed method does not have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We follow the license of each asset and cite them properly. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [No] Justification: We do not provide new assets in this paper. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not include crowdsourcing experiments and research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not include crowdsourcing experiments and research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.