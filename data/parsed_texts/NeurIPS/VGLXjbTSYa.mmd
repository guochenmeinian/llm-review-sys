# Delegated Classification

Eden Saig,  Inbal Talgam-Cohen,  Nir Rosenfeld

Technion - Israel Institute of Technology

Haifa, Israel

{edens,italgam,nirr}@cs.technion.ac.il

###### Abstract

When machine learning is outsourced to a rational agent, conflicts of interest might arise and severely impact predictive performance. In this work, we propose a theoretical framework for incentive-aware delegation of machine learning tasks. We model delegation as a principal-agent game, in which accurate learning can be incentivized by the principal using performance-based contracts. Adapting the economic theory of contract design to this setting, we define _budget-optimal_ contracts and prove they take a simple threshold form under reasonable assumptions. In the binary-action case, the optimality of such contracts is shown to be equivalent to the classic Neyman-Pearson lemma, establishing a formal connection between contract design and statistical hypothesis testing. Empirically, we demonstrate that budget-optimal contracts can be constructed using small-scale data, leveraging recent advances in the study of learning curves and scaling laws. Performance and economic outcomes are evaluated using synthetic and real-world classification tasks.

## 1 Introduction

The acclaimed success of machine learning at effectively solving difficult prediction tasks across diverse problem domains has made it highly appealing for firms, institutions, and individual practitioners. But machine learning has also become increasingly complex, cumbersome, and difficult to operate--and not all those who seek to learn have access to the necessary expertise, infrastructure, and designated resources required for learning effectively. This gap has created a new market for _outsourced machine learning_, in which a client interested in obtaining an accurate predictive model can hire the services of a specialized provider which, for a price, trains the model on their behalf. Consider for example a hospital purchasing a classifier for deciding between hospitalization and outpatient treatment when triaging patients. The provider invests in curating, cleaning and annotating training data, and delivers a trained model in return to payment from the hospital.

Having a budget to expend on outsourced learning [51], we model the client as aiming to obtain the best possible predictive model. At first glance, it is tempting to assume that the optimal strategy is simply to pay the provider the maximal feasible amount--and hope to get a high-end model in return. After all, if the client were to spend the budget directly on learning, investing the maximal available sum would yield the best possible results. But this neglects to account for the _incentives_ of the provider, who is interested in maximizing profit. Since the actions of the provider remain private, it is in his best interest to (secretly) minimize efforts, which in turn can result in his delivering a suboptimally-trained model. In our example, the provider can cut costs by annotating only a subset of the data, obtaining cheaper low-quality annotations, or neglecting to meticulously remove all outliers.

Outsourced learning is hence susceptible to _moral hazard_, an economic situation which might occur under information asymmetry, and to the detriment of the client. Motivated by this observation, in this paper we initiate the study of _delegated learning_, and aim to explore the economic, algorithmic, and statistical implications that occur when learning is delegated to a specialized provider. Our key novelty is in instantiating delegated learning as a problem of _optimal contract design_[10, 42, 47, 48]. Broadly, contracts are an important monetary device that allows the client to establish a payment schemewhich, if properly set, serves to align incentives and guarantee that both parties are well-off. Our main challenge is to design effective contracts specialized to the task of delegated learning on a budget.

Towards this, we begin with a conventional supervised classification setup, and impose economic structure by assuming that acquiring training examples is costly. We then conceptually "split" the conventional self-sufficient learner into two rational entities: a _principal_, who controls the budget and is interested in maximizing predictive accuracy; and an _agent_, who controls learning (in particular the training set) and is interested in maximizing profit. This allows us to model principal-agent relations as a Stackelberg game, in which the principal commits to a _contract_\(t\), determining _a priori_ the amount to be paid for every possible (stochastic) level of obtained accuracy. The agent best-responds to the contract by choosing the profit-maximizing number of samples \(n\), and training the predictive model.

Under this setting, we study the algorithmic problem of designing an optimal contract. As is standard in economic analysis, we begin with the assumption that the principal has full information on the distribution of possible outcomes for each of the agent's possible actions. In our setting, actions correspond to the number of training samples, and outcomes to the empirical classifier accuracy; thus, the main object of interest for contract design in delegated learning settings is the _learning curve_, which describes the (stochastic) performance of learning per sample size. Under certain plausible conditions on the learning curve, namely MLRP and a certain notion of concavity, our main result here is that optimal contracts are _simple_, and in particular, take on the form of simple threshold functions. Simple contracts are appealing because they are straightforward to understand and communicate; in our setting, they are also easy to compute, and we give a closed-form solution for the optimal threshold contract. Providing an interpretation of the closed-form solution, our findings establish a new connection between contracts and the renowned Neymon-Pearson lemma [45], which we consider as one of our central theoretical contributions.

We then switch gears and turn to empirically studying the construction of contracts from partial information. In particular, we consider a setting where the principal can only estimate the learning curve from small available data (e.g., by bootstrapping on small \(n\) and extrapolating). Using the recent LCDB dataset of learning curves [43], we show that threshold contracts generally perform well on estimated curves despite the inherent uncertainty. We also explore the role of different parameters of our setup, consider various tradeoffs in curve-fitting and contract design, and discuss limitations by pointing out certain failure modes to which contracts may be susceptible.

Taken together, our results shed light on why and how simple contracts for delegated learning work to correctly balance between the incentives of both delegator and delegatee in outsourced learning.

### Related work

Previous works have considered delegation of ML-related tasks that differ from our task of training a classifier: labeling of data points in [14], gathering information in [16], and computing a costly high-dimensional function in [5]. In the delegated task of [24], the agent provides a classifier and the principal verifies its near-optimality within \(\mathcal{H}\). A fundamental difference is that their agent is assumed to be adversarial rather than rational, and so interactive proofs are used instead of economic incentives.

The Neyman-Pearson lemma has recently been connected to economic design by [8] in the context of adverse selection rather than moral hazard. The agent has a hidden type (e.g., whether a new

Figure 1: Delegated classification interaction sequence. The principal examines initial information, and designs a contract \(t:\{0,\dots,m\}\rightarrow\mathbb{R}_{\geq 0}\). Having observed \(t\), the agent strategically selects a dataset size \(n\) that will maximize his expected utility. He samples a training set \(S\sim D^{n}\), incurs cost \(c_{n}\), then trains the classifier \(h\in\mathcal{H}\) and sends it to the principal. Upon receiving \(h\), the principal evaluates its accuracy on a random validation set \(V\sim D^{m}\), and pays the agent according to the contract \(t\).

drug is effective), and the optimal menu to offer this agent is designed based on the theory of e-values. When the hidden type has binary support, the method is equivalent to Neyman-Pearson. A "moral" link between the design of contracts (for a non-budgeted principal) and statistical inference (in particular likelihood ratios) was observed already in [25], but no connection was made to the power of hypothesis tests. Other intersections of ML and contracts that do not involve delegation of learning-related tasks include strategic classification [e.g., 36, 37, 3] and online learning of optimal contracts [29, 17, 52] Contract settings with a binary action and/or outcome space have been studied in [e.g., 20, 7, 22]. Not to be confused with our notion of delegation, there is a growing computational literature on delegation without monetary payments [e.g., 35].

To extrapolate from partial data, our work builds upon recent advancements in the study of learning curves, which characterize the expected generalization of learning as a function of dataset size and other exogenous factors [49]. There is growing empirical evidence that performance of modern neural networks can be predicted using simple scaling laws [e.g., 34, 41, 50, 23, 2, 46, 30], and theoretical results that back these findings in simplified settings [11, 12, 33, 9].

Perhaps closest to ours is the concurrent work [4], which analyzes a similar setting under different assumptions: Rather than assuming budget constraints, they assume that the principal's utility is linear in accuracy and payout, and the learning curve takes a specific functional form. Based on these assumptions, they derive approximately optimal linear contracts which are robust to adverse selection. In contrast, we obtain globally optimal simple contracts based on hypothesis testing, and present data-driven methods to handle partial information. Together, the two studies demonstrate the important role of simple contracts in the growing ecosystem of machine learning delegation.

## 2 Problem Setup

The core of our setting is based on a standard supervised classification task. Let \(x\in\mathcal{X}\) be features and \(y\in\mathcal{Y}\) be labels, and assume there is some unknown joint distribution \(D\) over \((x,y)\) pairs. Given a sample set \(S=\{(x_{i},y_{i})\}_{i=1}^{n}\sim D^{n}\), the goal in learning is to use \(S\) to find a classifier \(h:\mathcal{X}\to\mathcal{Y}\) from a class \(\mathcal{H}\) that maximizes expected accuracy, \(\mathrm{acc}_{D}(h)=\mathbb{P}_{(x,y)\sim D}[h(x)=y]\). Because the underlying distribution \(D\) is unknown, expected performance is estimated by the empirical average on an additional held-out validation set \(V\sim D^{m}\) of size \(m\), as \(\mathrm{acc}_{V}(h)=\frac{1}{m}\sum_{i=1}^{m}\mathds{1}\left[h(x_{i})=y_{i}\right]\), which is a consistent and unbiased estimator of \(\mathrm{acc}_{D}(h)\). We will assume throughout that both the learning algorithm and the validation set size \(m\) are known and fixed.

Learning on a budget.We will be interested in studying learning when certain resources are limited or costly. Our main focus will be on the setting where the main cost of learning is the number of labeled examples \(n\), but we note that our approach can in principle extend to other forms of learning 'effort'.1 We assume the learner has a monetary budget \(B\) to spend on samples, and is interested in maximizing accuracy under budget constraints. Let \(c_{n}\geq 0\) be the cost of \(n\) samples (assumed to be increasing in \(n\)), then the learner aims to solve:

Footnote 1: For example, [34] argue that not only training-set size, but also computation time and model size are related to accuracy through scaling laws, which have tight connections to the assumptions we discuss in Sec. 3.

\[n^{*}=\operatorname{argmax}_{n}\mathbb{E}_{h_{n}}[\mathrm{acc}_{D}(h_{n})] \quad\text{s.t.}\quad c_{n}\leq B\] (1)

where \(h_{n}\) is a classifier learned from a random dataset of size \(|S|=n\). Note that \(h_{n}\) is a random variable with distribution depending on \(n\). We denote the out-of-sample accuracy of \(h_{n}\) by \(\alpha_{n}=\mathrm{acc}_{D}(h_{n})\). When the learner is a self-sufficient entity, and when the expected \(\alpha_{n}\) improves monotonically in \(n\), then \(n^{*}\) in Eq. (1) in naturally the largest affordable \(n\) (see Sec. 3). However, as we will see, when learning is _delegated_--this seemingly straightforward observation can break.

Delegation.We model the delegation of learning as a conceptual partition of the learner into two distinct entities: an _agent_, who controls learning; and a _principal_, who controls the validation process. The principal outsources the learning task to the agent, who in turn uses the training set \(S\) to train the classifier \(h\); once delivered, the principal validates the performance of \(h\) using the validation set \(V\). Whereas the classifier's accuracy benefits the principal alone, the cost of learning (i.e., the cost of acquiring \(S\)) is born exclusively by the agent. Importantly, the amount of invested effort remains private to the agent; in our example, the principal cannot know how many examples received quality labeling. Because the agent seeks to maximize profit, the principal can use her budget as a source of monetary payment to incentivize the agent to invest in larger \(|S|=n\). Intuitively, one could expect larger payments to entail larger \(n\), and therefore higher-accuracy \(h\). However, as we will see, this is not always the case, and careful planning is required in order to fully utilize a given budget.

### Delegation as contract design

As the training set remains private to the agent, there is an information gap between the two parties. This creates a conflict of interest for the agent known as _moral hazard_[10], in which the agent may be tempted to invest sub-par effort, while claiming that efforts were in fact his honest best. In economics, the celebrated solution to moral hazard are _contracts_[31]: pay-per-performance rules that a-priori determine future payments for every possible outcome, which we formally describe next.

Contract design.A contract setting is defined by a set of actions \(\mathcal{A}=\{a_{1},\ldots,a_{N}\}\) that can be taken by the agent, and a set of possible outcomes \(j\in\{0,\ldots,m\}\). Each action \(a_{i}\) is associated with a cost \(c_{i}\), and w.l.o.g. we assume \(c_{1}\leq\cdots\leq c_{N}\) so that actions correspond to increasing _effort levels_. The agent's choice to perform action \(a\in\mathcal{A}\) yields a random outcome \(j\sim f_{a}\) for the principal, where \(f_{a}\) describes a distribution over the possible outcomes associated with action \(a\). The principal, who does not observe the agent's chosen action, can incentivize the agent through a _contract_, \(t:\{0,\ldots,m\}\to\mathbb{R}_{\geq 0}\), according to which she pays the agent \(t(j)\geq 0\) when the materialized outcome is \(j\). Given contract \(t\), let \(u_{a}(t)\) be the agent's expected _utility_ from taking action \(a\in\mathcal{A}\) at cost \(c_{a}\) (via stochastic outcomes \(j\sim f_{a}\)), and let \(a(t)\) be the agent's _best response_--an action that maximizes his expected utility (and following standard tie-breaking assumptions as in [18]). Then:

\[u_{a}(t)=\mathbb{E}_{j\sim f_{a}}[t(j)]-c_{a},\qquad\qquad a(t)\in\operatorname {argmax}_{a\in\mathcal{A}}u_{a}(t).\] (2)

Every action \(a^{*}\) that is the best response \(a^{*}=a(t)\) to some contract \(t\) is called _implementable_. In economic terms, the principal and agent are playing a _Stackelberg game_, in which the principal commits to a contract \(t\) and the agent best-responds by choosing action \(a(t)\) that maximizes his expected utility \(u_{a}(t)\). The goal of the principal is to design a contract \(t\) which incentivizes the agent to take best-response actions yielding favorable outcomes for the principal.

Contracts for delegated learning.We propose to formulate delegated learning as a problem of optimal contract design, instantiated as follows. First, we relate agent actions \(a\) with the number of samples \(n\), and denote \(\mathcal{A}=\{n_{1},\ldots,n_{N}\}\) as the possible sizes of \(S\) that the learning agent can work with. The cost of acquiring samples naturally maps as \(c_{a}=c_{n}\), and agent's best response is \(a(t)=n(t)\). Next, we associate outcomes \(j\) with accuracy for the principal by defining \(j\) as the number of validation samples (out of the possible \(m\)) on which \(h\) is correct; note this implies \(\operatorname{acc}_{V}(h)=j/m\), and we will therefore use \(j\) and \(\operatorname{acc}_{V}(h)\) as 'outcomes' interchangeably. Finally, for an action \(n\), we set \(f_{n}\) to be the distribution over possible accuracies obtained when learning with \(n\) samples, namely \(f_{n}(j)=\mathbb{P}_{h_{n},V}[\operatorname{acc}_{V}(h_{n})=j/m]\ \forall j\). We will also use the matrix form \(F_{nj}=f_{n}(j)\), where \(F\in[0,1]^{N\times(m+1)}\). Note that \(F\) admits two sources of variation: (i) _a-priori_ variation in \(h_{n}\) due to stochasticity in \(S\sim D^{n}\); and (ii) _a-posteriori_ variation in \(j\) for any fixed

Figure 2: A delegated classification setting (data from Sec. 4). **(Left)** Each costly action taken by the agent (training set size \(n\)) induces a distribution \(f_{n}\) of possible outcomes (classifier accuracy). The principal seeks to construct a contract \(t\) that incentivizes a profit-maximizing agent to take actions entailing favorable outcomes. Note the \(f_{n}\) exhibit increasing expectation, but decreasing variance, in \(n\). **(Center)** Three contracts for a given budget \(B\), mapping outcomes to payments. **(Top-right)** Agent’s utilities \(u_{n}(t)\) and best responses \(n(t)\) (stars) for each contract \(t\). **(Bottom-right)** Expected accuracies for principal resulting from each contract; here the threshold contract is optimal (see Sec. 3).

due to stochasticity in \(V\sim D^{m}\). When \(h_{n}\) is fixed, the outcome distribution admits a simple binomial form, namely \(j\sim\mathrm{Binomial}(m,\alpha_{n})\). Empirically, we observe this to be the dominant component.

### Delegation as an optimization problem

Budget-optimal contracts.Recall that the principal seeks to maximize accuracy under budget constraints (Eq. (1)). Once learning is delegated to an agent and framed as a contract design problem, the principal's objective becomes:

\[t^{*}=\mathrm{argmax}_{t\in[0,B]^{m}}\,\mathbb{E}_{h_{n(t)}}[\mathrm{acc}_{D}( h_{n(t)})]\] (3)

Contract \(t^{*}\) is chosen to incentivize the agent to invest effort \(n(t)\) (via Eq. (2)) such that the training of \(h_{n(t)}\) yields high dividends for the principal in terms of expected accuracy. We will refer to \(t^{*}\) as a _budget-optimal contract_, and to the general task of finding \(t^{*}\) as _budget-optimal contract design_.

Information structure.Delegated learning settings have actions \(n\) and costs \(c_{n}\) known to both sides, and outcome distribution \(F_{nj}\) known to the agent. For the principal, we explore varying levels of knowledge: In Sec. 3, we assume (as in the classic contract design literature) that the principal has full information of \(F\) (i.e., knows the learning curve), and focus on characterizing the optimal contract. In Sec. 4 we relax this assumption, and explore a partial-information setting in which the principal relies instead on an empirically-estimated curves \(\hat{F}\).

## 3 Budget-Optimal Contract Design

### The problem

Why agents cut corners.The conceptual challenge in designing contracts lies in that agents cannot reliably report _what_ they did. For example, consider a principal who, after delegation, received a classifier attaining 0.74 (validation) accuracy. Should she be happy? The crux is that there are two ways that this could have happened: (i) the agent invested high effort in learning (large \(n\)), but received an uninformative \(S\) by chance, and delivered a low-quality \(h\) as a result; and (ii) the agent invested low effort (small \(n\)). Since the agent's actions are private, and because outcomes are stochastic, the principal can never know for certain which is the true underlying cause. In other words, a 'lazy' (or rather strategic) agent can hide behind the uncertainty that is inherent in learning outcomes.2

Footnote 2: The agent can also hide in the uncertainty due to \(V\), particularly when \(m\) is small; see experiment in Sec. 4.2.

Contract types.To overcome this informational gap, the principal must devise a contract to align incentives and encourage the agent to prefer certain actions over others. But not all contracts are equally effective. Fig. 2 illustrates for a budget \(B\) three contract types and their economic implications:

* **Constant contract** (\(t(j)=B\)): The agent is paid \(B\) regardless of the outcome. His best-response in this case is to choose the least-costly action--to the detriment of the principal.
* **Linear contract** (\(t(j)=Bj/m\)): The agent is paid a fraction of \(B\), linear in the resulting accuracy. Linear contracts are a popular and extensively-studied class of contracts (e.g., 32; 15). Nonetheless, and though seemingly sensible, linear contracts turn out to be sub-optimal for our setting.
* **Threshold contract** (\(t(j)=B\mathds{1}\left[j\geq j_{0}\right]\) for some \(j_{0}\)): The agent is paid \(B\) provided the empirical accuracy surpasses a threshold \(j_{0}\). In the example in Fig. 2, the threshold contract is optimal.

Rather than committing _a-priori_ to some type of contract, we seek to find the best budget-optimal contract by solving Eq. (3). For this it is useful to have _structure_.

Stochastic learning curves (and where to find them).Our approach uses the observation that there is a tight connection between the set of distributions \(\{f_{n}\}\) encoded in \(F\), and _learning curves_, which describe the anticipated accuracy of a classifier as a function of the size of its training set. Learning curves typically depict only expected accuracy, but there is also inherent variation in outcomes. We will therefore broadly use the term'stochastic learning curve' to describe both mean trend _and_ variation in accuracy as a function of \(n\); formally, a stochastic learning curve is defined precisely by \(F\). This connection is useful because learning curves have structure: First, expected learning curves are typically _monotone_[34; 11]; when not [43; 49], they can be monotonized [12]. Second, stochastic learning curves are likely to satisfy the _monotone likelihood ratio property_ (MLRP), which states that the better the performance of a classifier, the more likely it was trained on more data (see Def. 1).

### Optimization via min-budget contracts

Our main technique for solving Eq. (3) relies on a reduction to what we refer to as _min-budget contracts_. Given an (implementable) target action \(n^{*}\in\mathcal{A}\), a min-budget contract for \(n^{*}\) is a contract \(t\) that incentivizes the agent to employ precisely the action \(n^{*}\), while minimizing the maximum payment by the principal \(\left\|t\right\|_{\infty}=\max_{j\in\{0,\dots,m\}}\{t(j)\}\); i.e., \(t\) implements \(n^{*}\) at minimum budget. Formally:

\[t^{*}=\operatorname*{argmin}_{t}\left\|t\right\|_{\infty}\quad\text{s.t.} \quad n(t)=n^{*}\] (4)

Our reduction relies on the following claim (Proof in Appendix B.1):

**Proposition 1**.: _Every budget-optimal contract design problem has an optimal solution which is also min-budget._

Using Prop. 1, a solution to Eq. (3) can be obtained by iteratively solving Eq. (4): For all \(n_{i}\in\mathcal{A}\), solve Eq. (4) with target action \(n^{*}=n_{i}\), and return \(t^{*}(n_{i})\) for the best implementable \(n_{i}\) whose budget does not exceed \(B\). The budget-optimal problem thus reduces to solving multiple min-budget problems.

To compute each \(t^{*}(n_{i})\) in Eq. (4), we formulate a novel MIN-BUDGET linear program (LP),3 detailed in Appx. B.2. One way to solve this LP is with generic solvers--an approach which is valid, but can be costly. One of our contributions is in identifying natural cases where min-budget contracts take on _simple_ forms, which are easier to optimize, and have practical merit. In particular, we show that binary-action contracts have _all-or-nothing_ structure (\(t(j)\in\{0,B\}\)), and plausible structural assumptions on the learning curve give rise to _threshold_ contracts (\(t(j)=B\mathds{1}\left[j\geq j_{0}\right]\) for some \(j_{0}\)). Our theoretical results are summarized in Table 1, and detailed in the rest of the section.

Footnote 3: The MIN-BUDGET LP is closely related to the well-known MIN-PAY LP from non-budgeted contract design [e.g., 19], but with a different objective, and hence very different optimal solutions (see Appx. B.3).

### All-or-nothing contracts: Binary action space and the statistical connection

We begin with a simple delegated learning setting in which the agent can choose one of two actions, \(\mathcal{A}=\{n_{1},n_{2}\}\), e.g., a'small' vs. 'large' training set, and the principal seeks to incentivize training with more data.4 This reduced case will be useful as a building block for the general case (which we return to in Sec. 3.4), and for making a precise connection between contract design and hypothesis tests.

Footnote 4: When \(n_{2}\) is not the target or when it is not implementable, then the solution is immediate: always pay \(c_{1}\).

Simple min-budget contracts for binary action space.Our first result shows that optimal binary-action contracts are all-or-nothing contracts whose budget is determined by the _total variation distance_ between outcome distributions \(f_{2}\) and \(f_{1}\), namely \(\left\|f_{2}-f_{1}\right\|_{\text{TV}}=\frac{1}{2}\sum_{j=0}^{m}\left|f_{2,j} -f_{1,j}\right|\).

**Theorem 1** (Optimal binary-action contract).: _In a binary-action contract setting with outcome distributions \(f_{1},f_{2}\) and costs \(c_{1},c_{2}\), the min-budget contract is an all-or-nothing contract, given by:_

\[t^{*}(j)=B\mathds{1}\left[f_{2}(j)\geq f_{1}(j)\right]\quad\forall j\in\{0, \dots,m\},\quad\text{ where }\;B=\nicefrac{{(c_{2}-c_{1})}}{{\left\|f_{2}-f_{1}\right\|_{\text{TV}}}}.\] (5)

The proof (in Appendix B.4.2) is by LP duality. Intuitively, the optimal contract pays the agent for outcomes that are more likely to come from \(f_{2}\) than from \(f_{1}\). Moreover, it requires a higher budget the smaller the distance is between the two distributions \(f_{1},f_{2}\). At the extremes, if their distance is 1 (i.e. no overlap among their supports), the required budget for incentivizing \(n_{2}\) is \(c_{2}-c_{1}\), whereas if their distance is 0 (i.e. \(f_{1}=f_{2}\)) it becomes impossible to incentivize \(n_{2}\).

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multicolumn{2}{c}{Problem size} & \multicolumn{2}{c}{Structural assumptions} \\ \hline Actions & Outcomes & No assumptions & MLRP & Concave-MLRP \\ \hline \(\left|\mathcal{A}\right|=2\) & any size & All-or-nothing (T1) & Threshold (B.7.1) \\ \(\left|\mathcal{A}\right|>2\) & \(\begin{array}{c}m+1=2\\ m+1>2\end{array}\) & All-or-nothing (B.5.1) & Threshold (B.7.2) \\ Simple is NP-hard (T3) & \(\exists\) non-threshold (B.7.3) & Threshold (T4) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Characterization of simple min-budget contracts in different settings. Simple contract forms include _all-or-nothing contracts_ and their subclass of _threshold contracts_. The table specifies for each configuration either the simple form that is optimal (in one case through equivalence to the Neyman-Pearson lemma), or that the simple form is non-optimal or intractable.

Formal connection to optimal hypothesis testing.Theorem 1 also uncovers a direct correspondence between optimal contracts and optimal hypothesis tests. Intuitively, given the outcome distributions \(\{f_{1},f_{2}\}\), and in order to incentivize \(n_{2}\), the principal wishes to pay the agent if the observed outcome \(j\in\{0,\ldots,m\}\) is more likely to have originated from \(f_{2}\). The principal can attempt to identify whether the outcome \(j\) is drawn from distribution \(f_{1}\) or \(f_{2}\) through hypothesis testing, where a hypothesis test \(\psi:\{0,\ldots,m\}\rightarrow\{0,1\}\) maps a sample \(j\) to either \(f_{2}\) (indicated by 1) or to the null hypothesis \(f_{1}\) (indicated by 0). For our purpose it is convenient to allow tests to be non-integral, in which case \(\psi:\{0,\ldots,m\}\rightarrow[0,1]\) maps \(j\) to a _probability_ with which it originates from \(f_{2}\). The quality of a hypothesis test is measured by summing its type-1 and type-2 errors: \(\sum_{j=0}^{m}f_{1,j}\psi_{j}+\sum_{j=0}^{m}f_{2,j}(1-\psi_{j})\). The test that minimizes this sum is known as the _most powerful_ hypothesis test, and has been characterized by Neyman and Pearson [45, 4.3].

We now turn to formally establishing the connection. For a fixed \(B\), observe that every contract with budget \(B\) can be mapped to a hypothesis test via the bijection \(\psi(j)=\nicefrac{{t(j)}}{{B}}\). Then:

**Theorem 2** (Optimal contract vs. test).: _Consider binary-action contract design with distributions \(f_{1},f_{2}\) and costs \(c_{1},c_{2}\). A contract \(t\) with budget \(B\) is optimal if and only if its corresponding hypothesis test \(\psi=\nicefrac{{t}}{{B}}\) is maximum power with type-1 and type-2 errors summing to \(1-\nicefrac{{c_{2}-c_{1}}}{{B}}\)._

The proof (Appendix B.4.2) is by a non-linear variable transformation to the MIN-BUDGET LP. Theorem 2 implies that the optimal contract for the binary-action case (Theorem 1) is equivalent to the well-known Neyman-Pearson lemma characterizing the most powerful hypothesis test:

**Lemma 1** (Neyman-Pearson [e.g., 45]).: _Let \(f_{1},f_{2}\) be two discrete probability distributions. Then the most powerful hypothesis test for \(f_{1},f_{2}\) is the likelihood ratio test \(\psi(j)=\mathds{1}\left[f_{2}(j)\geq f_{1}(j)\right]\), which attains the optimal bound \(1-\left\|p-q\right\|_{\mathrm{TV}}\) on the sum of type-1 and type-2 errors._

Theorem 2 establishes a new formal connection between the two domains of contract design and hypothesis testing. In the context of contract design, it provides a statistical interpretation: A min-budget contract can be interpreted as an optimal hypothesis test, and the ability to distinguish between the two hypotheses determines the required budget. In the converse direction, it enables a new proof for the Neyman-Pearson using the min-budget contract given by Theorem 1 (see Appendix B.4.2).

### All-or-nothing contracts: Beyond binary action

For general action spaces, optimal contracts are not guaranteed to be all-or-nothing. In fact, we show that determining whether there exists an all-or-nothing contract that is optimal is NP-hard:

**Theorem 3** (Hardness).: _Finding a min-budget all-or-nothing contract is NP-hard._

The proof is by reduction from 3SAT, and appears in Appx. B.5.2. Nonetheless, there are special but important cases--notably the binary outcome case5-- in which results from Sec. 3.3 hold, suggesting that ideas from Thm. 1 apply more broadly. The following algorithm makes use of these ideas, showing good empirical performance, and provable performance under an MLRP condition (Sec. 3.5).

Footnote 5: In this case there are _binary outcomes_ (\(m=1\)), such as when the agent’s efforts can result in either success or failure [6, 28, 21]. For this case, we show that the optimal contract is also all-or-nothing (Appx. B.5.1).

Single binding action algorithm.Revisiting the closed-form all-or-nothing contract in Eq. (5), we observe that the result is based on the fact that binary action spaces have only one alternative action. Building upon this observation, we propose the _single binding action_ (SBA) algorithm, which computes a solution to Eq. (4) in the general (many-actions) case: Given target action \(n^{*}\in\mathcal{A}\), loop over all actions \(n\neq n^{*}\), and apply the closed-form formula in Eq. (5) to obtain an (all-or-nothing) contract \(t^{*}(n,n^{*})\). If the agent's best response (Eq. (2)) satisfies \(n\left(t^{*}(n,n^{*})\right)=n^{*}\), return \(t^{*}\). If the loop ends without returning a contract, return 'fail'. The following claim shows the algorithm is sound:

**Proposition 2** (Soundness of SBA).: _When the single binding action algorithm terminates successfully, it returns an optimal contract which is an all-or-nothing contract._

Proof in Appendix B.6. Prop. 2 ensures that if SBA succeeds, then the returned all-or-nothing contract \(t^{*}\) is optimal. Moreover, failure does not preclude the existence of an optimal all-or-nothing contract. If SBA fails, we solve Eq. (4) with a generic LP solver. Empirically, SBA was successful in more than 85% of cases, and is \(\sim\)\(10^{3}\) times faster than the LP solver (Appendix C.3). Thus, this optimistic 'try SBA first' approach typically succeeds, adds negligible overhead if not, and guarantees correctness.

### Threshold contracts: MLRP assumption

In this section we provide sufficient conditions, in the form of natural structural properties of (stochastic) learning curves, that guarantee the optimality of even _simpler_ contracts--namely _threshold contracts_--and the success of SBA. With inspiration from hypothesis testing [40] and contract theory [26; 19], it is natural to consider the _monotone likelihood ratio property_ (MLRP) assumption:

**Definition 1** (MLRP [e.g., 26]).: _A contract design setting satisfies MLRP if for every pair of actions \(a,a^{\prime}\) such that \(c_{a}<c_{a^{\prime}}\), the likelihood ratio \({{}_{f^{a^{\prime}}(j)}}/{{}_{f_{a}(j)}}\) is monotonically increasing in \(j\)._

In our context, MLRP states that the better the validation-set performance of a classifier, the more likely it was trained on more data. This holds in particular for monotone learning curves with binomial outcome distribution [19; B.1]. For a binary action space, MLRP ensures that \(n_{2}\) is always implementable,6 and that the optimal contract is a threshold contract: \(t^{*}(j)=B\mathds{1}\left[j\geq j_{0}\right]\). This is by Theorem 1, and by the fact that \(\exists j_{0}\) such that \({{}_{f_{2}(j)}}/{{}_{f_{1}(j)}}\geq 1\) iff \(j\geq j_{0}\) (see also Appendix B.7.1).7 Interestingly, this is similar to the relation between the Neyman-Pearson lemma and the Karlin-Rubin theorem, which characterizes the most powerful hypothesis test under monotone likelihood ratio [40].

Footnote 6: As a corollary of a similar result for min-pay contracts [19; Lemma 7].

Footnote 7: This also holds for binary outcomes in an arbitrary action space, see Appendix B.7.2.

MLRP for general action space.MLRP does not guarantee threshold contracts in general: In Appendix B.7.3, we give a constructive counterexample satisfying MLRP, but for which the optimal contract is not threshold. However, refining MLRP to also capture 'diminishing returns' turns out to be sufficient for recovering guarantees generally. For target action \(a_{N}\), denote the _survival probability_ of an action \(a_{i}\) by \(s_{i}=\mathbb{P}_{j\sim f_{i}}[j\geq j^{*}]\), where \(j^{*}\) is the minimal outcome at which action \(a_{N}\) is more likely than \(a_{N-1}\). These will serve as formal means for capturing the concavity of learning curves.

**Definition 2** (C-MLRP).: _A contract design setting satisfies Concave-MLRP (C-MLRP) if it satisfies MLRP, and additionally the actions' survival probability is concave as a function of the actions' cost._

Our final result shows that C-MLRP guarantees optimality of threshold contracts, and success of SBA:

**Theorem 4** (Sufficiency for threshold).: _Consider a contract design setting with C-MLRP. Then the optimal contract is a threshold contract, and is recovered by the SBA algorithm._

We prove this claim by showing that concavity implies that only one alternative action is binding in the linear program equivalent to Eq. (4), reducing the problem to the two-action case. By applying Theorem 1, we obtain optimality of threshold contracts in this case as well (proof in Appendix B.7.3). This also implies that SBC always terminates successfully on inputs that satisfy C-MLRP.

In practice, we believe that C-MLRP is a reasonable assumption for realistic learning curves: in Appendix B.8.1, we prove it is satisfied by a standard theoretical model of learning curves, and in Appendix C.3, we empirically demonstrate that it is (approximately) satisfied in settings where threshold contracts are optimal. Interestingly, in our empirical study, threshold contracts were often optimal even when C-MLRP did not hold--suggesting the condition is sufficient, but not necessary (see Sec. 4.1).

## 4 Experiments

We now turn to our empirical investigation of delegated learning under full and partial information. We base our experiments on the recently curated Learning Curves Database (LCDB) [43], which includes a large collection of stochastic learning curves for multiple classification datasets and methods. For each dataset and method, the database includes held-out accuracy measurements obtained for increasing sample sizes \(n\in\left\{2^{4},2^{4.5},\ldots,2^{15}\right\}\), with multiple repetitions per \(n\); these provide us with stochastic learning curves. Here we focus primarily on the popular MNIST dataset [39] as our case study, and on MLP and GBDT as representative classifiers, but we refer the reader to Appendix C for further experiments on additional datasets and methods. Code is available at: https://github.com/edensaig/delegated-classification.

### Full information

We begin with the full information setting to explore in a clean environment how different parameters of the learning setting and environment affect predictive performance and economic outcomes.

Validation set size.Fig. 3 (left) presents typical stochastic learning curves for two learning algorithms: Multi-Layered Perceptron (MLP) and Gradient-Boosted Decision Trees (GBDT). We take an arbitrary accuracy point on the curve at \(\mathrm{acc}(n)=0.85\) (dotted line) to examine the effects of validation set size \(m\) on min-budget contracts. Notice that MLP requires larger \(n\) to obtain 0.85; Fig. 3 (center) shows how this translates to a larger required budget \(B^{*}\), which holds for all \(m\). As \(m\) increases, required budgets and the difference between them both decrease. Larger validation sets are therefore useful for reducing required budget. Nonetheless, even for reasonable \(m\), obtained budgets still remain higher than their theoretical lower bounds (target action costs \(c_{n}\).).

Budget regimes.Fig. 3 (left) also indicates two points in which the learning curves cross (dashed lines), at \(\sim\)\(0.74\) and \(\sim\)\(0.94\) accuracy. These correspond to sample sizes \(n\) for which both methods obtain matching accuracies (in expectation). For a self-sufficient learner, the implication is that at each of these points, both methods are equally costly, i.e., both cost \(c_{n}\). Interestingly, and in contrast, delegation can entail different required budgets _despite_ equal accuracies. Fig. 3 (right) shows for each target accuracy the gap in required budgets between both methods, \(\Delta B^{*}=B^{*}_{\textsc{GBDT}}-B^{*}_{\textsc{MLP}}\). As can be seen, each method is comparatively more (or less) costly in different accuracy regimes (up to 0.6; between 0.6 and 0.92; and above 0.92). Crucially, the budget gap can be large even when accuracies match (dashed lines). For example, even though both MLP and GBDT require \(n{=}362{\approx}2^{17/2}\) samples to obtain \(\sim\)\(0.74\) accuracy, GBDT is cheaper (\(\Delta B^{*}{=}-10^{2}\)); for \(\sim 0.94\) which requires \(n{=}23170{\approx}2^{29/2}\) from both, GBDT is significantly more expensive (\(\Delta B^{*}{=}10^{5}\)). The reason for this is that optimal budgets are determined by the ability to distinguish between distributions (Sec. 3.3).

Prevalence of simple contracts.To understand the applicability of our theoretical findings, in Appendix C.3 we conduct an empirical prevalence evaluation on additional learning algorithms, and across target actions. We observed that min-budget contracts assume a threshold form and the SBC algorithm returns correct results in more than 85% of cases overall. Restricting optimization to simple contracts, budget requirements were generally less than 1% higher than that of a min-budget contract, suggesting that simple contracts may provide a good approximation even when min-budget contracts do not assume a simple form.

### Partial information

We now turn to consider delegation under partial information, in which the principal must rely on an estimated learning curve. We instantiate this idea by assuming that the principal has access to a small 'pilot' dataset of size \(k\), where \(k\) is considered small. Using this set, the principal creates an estimated learning curve \(\hat{F}\) by fitting a curve to accuracies obtained for up to some \(n_{0}\leq k\), and extrapolating to larger \(n>n_{0}\). In particular, we experiment with fitting parametric power-law curves of the form \(\mathbb{E}[\alpha_{n}]=a-bn^{-c}\), which have been shown to provide good fit in various scenarios both empirically and theoretically [49; 34; 11]. Since power-law curves are monotone, composition with binomial distributions increasing in \(p\) provably results in MLRP stochastic curves [19; B.1].

Bias-variance tradeoff.Given \(k\) pilot examples, there are different ways in which the principal can use them to construct an estimated curve. Here we consider a simple tradeoff: setting \(n_{0}\) to be small but with more samples per \(n<n_{0}\) (low variance), or setting \(n_{0}\) to be large but with few samples per \(n<n_{0}\) (low bias). We define \(r\) as the number of samples per \(n\) (so low \(r\) means larger \(n_{0}\)). Then, for a

Figure 3: Delegating with full information. **(Left)** Typical learning curves for two learning algorithms on MNIST. **(Center)** Required budget for target accuracy of \(0.85\) per validation set size \(m\). **(Right)** Different cost regimes, indicating per accuracy region which of the two methods is cheaper to delegate.

given \(r\), we set \(n_{0}\) such that \(\sum_{n\leq n_{0}}r\cdot n\leq k\) (i.e., such that the total number of used samples does not exceed \(k\)). Fig. 4 (left) shows different curve fits for \(r\in\{1,3,5\}\), and corresponding \(n_{0}\). Then, Fig. 4 (center-left) shows for a certain fixed budget the accuracy level that can be attained for increasing \(k\), and as a function of \(r\). As can be seen, having sufficient points \(k\) for constructing \(\hat{F}\) is important, but performance grows quickly with \(k\) (note log-scale x-axis). It is also apparent in our example that low bias (via larger \(n_{0}\)) is much more important than low variance for constructing useful \(\hat{F}\).

Cost-efficiency tradeoff.Because the pilot set provides the principal a basic means for obtaining minimal accuracy, we can ask: given \(k\) examples, and for a fixed budget \(B\), what is the added benefit of delegating learning? For this, we define \(\mu(k)=n(\hat{t})/k\) to be the _sample-size multiplier_, i.e., the multiplicative gain in the effective number of samples due to delegation. Fig. 4 (center-right) shows \(\mu(k)\) for increasing \(k\) and across \(r\). For \(r=1\) (which is superior in terms of performance and outcomes), \(\mu\) begins at \(\sim\)10, increases to \(\sim\)30 at around \(k=190\), and slowly decreases back to \(\sim\)\(10\) towards \(k=1,000\). For \(r>1\), we observe that \(\mu\approx 1\), i.e., there is effectively no gain from delegation, until around \(k=100\), only after which some gain is restored. This highlights the importance of obtaining an accurate estimate \(\hat{F}\) in terms of the economic consequences of delegation.

Over vs. under-estimation.Typically in curve-fitting, over and under-estimation are treated equally, since both types of error can negatively affect goodness of fit and extrapolation quality. However, for delegation, the implications of over vs. under-estimation on contract outcomes are highly asymmetric. Fig. 4 (right) shows for a target incentivized number of samples \(n(t^{*})\) the relation between the (theoretical) _signed_ extrapolation error \(n(t^{*})\) (i.e., over- or under-estimate, measured in accuracy points) and the eventual loss in accuracy obtained through delegation, relative to perfect estimation. Each point in the plot corresponds to one curve-fitting instance, with points shown for varying \(k\), \(n_{0}\), and \(r\), and with multiple independent repetitions. Results show that in the under-estimation regime (i.e., _negative_ extrapolation error), loss in accuracy degrades gracefully with the estimation error. In stark contrast, even minimal over-estimation (_positive_ extrapolation error) causes accuracy to plummet dramatically, as the agent's rational response in those cases was to use the smallest dataset possible. We interpret this as a consequence of'setting the bar too high'--a rational decision to minimize effort in response to unrealistic expectations. This has important implications for the choice of how to fit and extrapolate learning curves, suggesting that contracts can be tolerant to under-estimation, while over-estimation should be avoided at all costs.

## 5 Discussion

Motivated by the increasingly-common practice of outsourcing learning tasks, this paper sets out to introduce and study the novel problem of delegated classification. Our findings suggest that conflict of interests should not be overlooked, and that contracts hold potential as a means for aligning them. Our analysis relies on a set of assumptions, which should be carefully considered by practitioners and empiricists alike; we also believe that there are likely further fruitful connections to explore between contracts and statistical hypothesis testing. As a problem of contract design, and when the learning task is reasonably well-behaved, delegated learning manifests in the form simple threshold contracts. A natural question for future work is whether simplicity also implies _robustness_ to partial knowledge--as is often the case [19].

Figure 4: Delegating with partial information. **(Left)** Extrapolated learning curves for different \(r\). **(Center-left)** Accuracy obtained via delegation per pilot set size \(k\). **(Center-right)** Multiplicative gain in effective number of samples due to delegation. **(Right)** Implications of over vs. under-estimation.

Acknowledgements.The authors would like to thank Ruth Heller, Shafi Goldwasser, Jonathan Shafer, Ohad Einav, and anonymous reviewers for their insightful remarks and valuable suggestions. Nir Rosenfeld is supported by the Israel Science Foundation grant no. 278/22. Eden Saig is supported by the Israel Council for Higher Education PBC scholarship for Ph.D. students in data science. Funded by the European Union (ERC, ALGORCONTRACT, 101077862, PI: Inbal Talgam-Cohen).

## References

* [1] Milton Abramowitz, Irene A Stegun, and Robert H Romer. Handbook of mathematical functions with formulas, graphs, and mathematical tables, 1988.
* [2] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [3] Tal Alon, Magdalen Dobson, Ariel D. Procaccia, Inbal Talgam-Cohen, and Jamie Tucker-Foltz. Multiagent evaluation mechanisms. In _AAAI 2020_, pages 1774-1781, 2020.
* [4] Nivasini Ananthakrishnan, Stephen Bates, Michael I Jordan, and Nika Haghtalab. Delegating data collection in decentralized machine learning. _arXiv preprint arXiv:2309.01837_, 2023.
* [5] Pablo D Azar and Silvio Micali. Computational principal-agent problems. _Theoretical Economics_, 13(2):553-578, 2018.
* [6] Moshe Babaioff, Michal Feldman, and Noam Nisan. Combinatorial agency. In _Proceedings of the 7th ACM Conference on Electronic Commerce_, pages 18-28, 2006.
* [7] Moshe Babaioff, Michal Feldman, Noam Nisan, and Eyal Winter. Combinatorial agency. _Journal of Economic Theory_, 147(3):999-1034, 2012.
* [8] Stephen Bates, Michael I Jordan, Michael Sklar, and Jake A Soloff. Principal-agent hypothesis testing. _arXiv preprint arXiv:2205.06812_, 2022.
* [9] Devansh Bisla, Apoorva Nandini Saridena, and Anna Choromanska. A theoretical-empirical approach to estimating sample complexity of dnns. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3270-3280, 2021.
* [10] Patrick Bolton and Mathias Dewatripont. _Contract theory_. MIT press, 2004.
* [11] Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon Van Handel, and Amir Yehudayoff. A theory of universal learning. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 532-541, 2021.
* [12] Olivier J Bousquet, Amit Daniely, Haim Kaplan, Yishay Mansour, Shay Moran, and Uri Stemmer. Monotone learning. In _Conference on Learning Theory_, pages 842-866. PMLR, 2022.
* [13] Michael L. Bynum, Gabriel A. Hackebeil, William E. Hart, Carl D. Laird, Bethany L. Nicholson, John D. Siirola, Jean-Paul Watson, and David L. Woodruff. _Pyomo-optimization modeling in python_, volume 67. Springer Science & Business Media, third edition, 2021.
* [14] Yang Cai, Constantinos Daskalakis, and Christos H. Papadimitriou. Optimum statistical estimation with strategic data sources. In _COLT 2015_, pages 280-296, 2015.
* [15] Gabriel Carroll. Robustness and linear contracts. _American Economic Review_, 105(2):536-563, 2015.
* [16] Junjie Chen, Minming Li, and Haifeng Xu. Selling data to a machine learner: Pricing via costly signaling. In _International Conference on Machine Learning_, pages 3336-3359. PMLR, 2022.
* [17] Alon Cohen, Argyrios Deligkas, and Moran Koren. Learning approximately optimal contracts. In _SAGT 2022_, pages 331-346, 2022.
* [18] Paul Dutting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In _EC 2019_, pages 369-387, 2019.

* Dutting et al. [2019] Paul Dutting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In _Proceedings of the 2019 ACM Conference on Economics and Computation_, pages 369-387, 2019.
* Dutting et al. [2021] Paul Dutting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Combinatorial contracts. In _FOCS 2021_, pages 815-826, 2021.
* Dutting et al. [2022] Paul Dutting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Combinatorial contracts. In _2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 815-826. IEEE, 2022.
* Dutting et al. [2023] Paul Dutting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Multi-agent contracts. In _STOC 2023_, 2023. To appear.
* Ghorbani et al. [2022] Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022._ OpenReview.net, 2022. URL https://openreview.net/forum?id=hR_Shu8cxcCV.
* Goldwasser et al. [2021] Shafi Goldwasser, Guy N Rothblum, Jonathan Shafer, and Amir Yehudayoff. Interactive proofs for verifying machine learning. In _12th Innovations in Theoretical Computer Science Conference (ITCS 2021)_. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2021.
* Grossman and Hart [1983] Sanford J. Grossman and Oliver D. Hart. An analysis of the principal-agent problem. _Econometrica_, 51(1):7-45, 1983.
* Grossman and Hart [1992] Sanford J Grossman and Oliver D Hart. An analysis of the principal-agent problem. In _Foundations of insurance economics_, pages 302-340. Springer, 1992.
* Hart et al. [2011] William E Hart, Jean-Paul Watson, and David L Woodruff. Pyomo: modeling and solving mathematical programs in python. _Mathematical Programming Computation_, 3(3):219-260, 2011.
* Ho et al. [2014] Chien-Ju Ho, Aleksandrs Slivkins, and Jennifer Wortman Vaughan. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. In _Proceedings of the fifteenth ACM conference on Economics and computation_, pages 359-376, 2014.
* Ho et al. [2016] Chien-Ju Ho, Aleksandrs Slivkins, and Jennifer Wortman Vaughan. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. _Journal of Artificial Intelligence Research_, 55:317-359, 2016.
* Hoiem et al. [2021] Derek Hoiem, Tanmay Gupta, Zhizhong Li, and Michal Shlapentokh-Rothman. Learning curves for analysis of deep networks. In _International conference on machine learning_, pages 4287-4296. PMLR, 2021.
* Holmstrom [1979] Bengt Holmstrom. Moral hazard and observability. _The Bell Journal of Economics_, 10(1):74-91, 1979.
* Holmstrom and Milgrom [1987] Bengt Holmstrom and Paul Milgrom. Aggregation and linearity in the provision of intertemporal incentives. _Econometrica: Journal of the Econometric Society_, pages 303-328, 1987.
* Hutter [2021] Marcus Hutter. Learning curve theory. _arXiv preprint arXiv:2102.04074_, 2021.
* Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* Kleinberg and Kleinberg [2018] Jon Kleinberg and Robert Kleinberg. Delegated search approximates efficient search. In _Proceedings of the 2018 ACM Conference on Economics and Computation_, pages 287-302, 2018.
* Kleinberg and Raghavan [2019] Jon Kleinberg and Manish Raghavan. How do classifiers induce agents to invest effort strategically? In _EC 2019_, pages 825-844, 2019.
* Kleinberg and Raghavan [2020] Jon M. Kleinberg and Manish Raghavan. Algorithmic classification and strategic effort. _SIGecom Exch._, 18(2):45-52, 2020.

* Lahaie [2008] Sebastien Lahaie. How to take the dual of a linear program. _Columbia University, New York_, 2008.
* LeCun [1998] Yann LeCun. The MNIST database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_, 1998.
* Lehmann et al. [1986] Erich Leo Lehmann, Joseph P Romano, and George Casella. _Testing statistical hypotheses_, volume 3. Springer, 1986.
* Mahmood et al. [2022] Rafid Mahmood, James Lucas, David Acuna, Daiqing Li, Jonah Philion, Jose M Alvarez, Zhiding Yu, Sanja Fidler, and Marc T Law. How much more data do I need? Estimating requirements for downstream tasks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 275-284, 2022.
* Martimort and Laffont [2009] David Martimort and Jean-Jacques Laffont. _The theory of incentives: the principal-agent model_. Princeton University Press, 2009.
* Mohr et al. [2022] Felix Mohr, Tom J Viering, Marco Loog, and Jan N van Rijn. Lcdb 1.0: An extensive learning curves database for classification tasks. _Machine Learning and Knowledge Discovery in Databases, ECMLPKDD. p. accepted. Lecture Notes in Computer Science, Springer_, 2022.
* Pedregosa et al. [2011] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* Rigollet and Hutter [2015] Phillippe Rigollet and Jan-Christian Hutter. High dimensional statistics. _Lecture notes for course 18S997_, 813(814):46, 2015.
* Rosenfeld et al. [2020] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL https://openreview.net/forum?id=ryenvpEKDr.
* Svedish Academy of Sciences [2016] Royal Swedish Academy of Sciences. Scientific background on the 2016 Nobel Prize in Economic Sciences, 2016.
* Salanie [2017] Bernard Salanie. _The Economics of Contracts: A Primer_. MIT press, 2017.
* Viering and Loog [2022] Tom Viering and Marco Loog. The shape of learning curves: a review. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* Xia et al. [2023] Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Veselin Stoyanov. Training trajectories of language models across scales. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023_, pages 13711-13738. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL -LONG.767.
* Zhao et al. [2022] Xinyi Zhao, Weixin Liang, and James Zou. Data budgeting for machine learning. _arXiv preprint arXiv:2210.00987_, 2022.
* Zhu et al. [2023] Banghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I. Jordan. The sample complexity of online contract design. In _Proceedings of the 24th ACM Conference on Economics and Computation, EC 2023, London, United Kingdom, July 9-12, 2023_, page 1188. ACM, 2023. doi: 10.1145/3580507.3597673.

Broader implications

In this paper, we set out to formalize and study the task of delegating classification through the lens of contract design. Given that our work is largely motivated by the increasingly common practice of outsourcing learning tasks to specialized service providers, we believe our algorithm, analysis, and empirical observations carry meaningful implications for practitioners and decision-makers alike. At the same time, it is important to remember that our work--as others considering economic aspects of learning--studies delegation in a simplified setting and under certain assumptions. As such, and since devising and agreeing to legally-binding contracts can have concrete implications on real-world outcomes, care should be taken when applying ideas or conclusions that derive from our work in practice.

For example, consider that our formalism relies on the assumption that the learning agent is all-knowing and rational. Yet in reality, agents must act under partial information, face irreducible (and often unquantifiable) uncertainty, and--being human--are subject to common behavioral biases. It is unclear a-priori if and how our statements and conclusions carry over to this setting. As another example, notice that our formalism considers a one-shot setting where a single contract between a single principal-agent pair is instantiated once. But in reality, competition and long-term reputation may play a significant role in determining the agent's incentive structure, and consequently, her behavior. In such cases, naively applying our framework without careful inspection of the appropriate incentives on both ends can result in sub-optimal contracts, possibly to the detriment of all involved parties. Nonetheless, given that our work aims to take an initial step towards establishing delegated learning, we view its extension to non-rational agents and temporal and competitive settings as intriguing future work.

One message that our work conveys is that in delegation, simplicity, in the form of threshold contracts, has merit. This draws connections to other works that similarly argue for simplicity as an important and useful property of effective delegation mechanisms. Our work shows that simple contracts are, under reasonable conditions, computationally feasible and theoretically optimal. Economically, threshold contracts are practically appealing since they are easy to understand, communicate, and regulate. Given that contracts are in essence social constructs, we believe these properties are key for establishing threshold contracts as effective building blocks for machine learning markets.

## Appendix B Min-budget contract design - deferred proofs

Notation.To align with traditional contract design notation, in this section, we denote the action space by \(\mathcal{A}=[n]=\{1,\ldots,n\}\), and the outcome space by \(\Omega=\{0,\ldots,m\}\). We denote by \(\Delta\left(\mathcal{X}\right)\) the set of distributions over a given set \(\mathcal{X}\). For contract design problems, we denote the outcome probabilities by \(F_{i,j}\), such that \(F_{i}\in\Delta\left(\Omega\right)\) is the outcome distribution associated with action \(i\), and also denote the cost of each action by \(c_{i}\). Given \(x\in\mathbb{R}\), we denote \(x^{+}=\operatorname{ReLU}(x)=\max\left\{0,x\right\}\). We denote the indicator function by \(\mathds{1}\left[\cdot\right]\), the total variation distance between distributions \(P,Q\) by \(\left\|P-Q\right\|_{\mathrm{TV}}\) (see Definition 4), and the survival function of \(P\in\Delta\left(\Omega\right)\) by \(\mathbb{S}_{P}(\cdot)\) (see Definition 14).

A note on individual rationality.In the contract design literature, a contract is said to be _incentive compatible_ (IC) with respect to some action \(a^{*}\) if it satisfies \(u_{a^{*}}(t)\geq u_{a}(t)\) for all actions \(a\in\mathcal{A}\). In the delegated classification setting, this corresponds to the constraint \(n(t)=n^{*}\) in Eq. (4). As an additional constraint, contracts in which the agent's expected utility is always non-negative (\(u_{n^{*}}(t)\geq 0\)) are said to be _individually rational_ (IR). In the case of delegated classification, it is natural to assume that there always exists a valid action that the agent can take at zero cost -- for example, returning a dummy classifier that always abstains from prediction (thus having zero accuracy), or a classifier which makes a prediction at random. As such, contracts in the delegated classification setting can be assumed to be individually rational without loss of generality, as any action \(n\) that is chosen by the agent has utility which is weakly larger than the utility of the zero-cost action, which is always non-negative.Moreover, even in cases where a zero-loss action does not exist, we show that individual rationality (IR) can be attained in a straightforward manner, by adding the minimal cost \(c_{1}\) to each entry of an incentive compatible (IC) contract (\(t_{j}\mapsto t_{j}+c_{1}\)):

**Claim 1**.: _Given an IC contract \(t\) with \(c_{1}>0\), the contract \(t+c_{1}\) (add \(c_{1}\) to every coordinate of the contract) is both IC and IR._

Proof.: An IC contract implementing action \(a^{*}\) satisfies \(u_{a^{*}}(t)\geq u_{a}(t)\), for all actions \(a\in\mathcal{A}\). In particular, this inequality holds in relation to the least-costly action, denoted by \(1\in\mathcal{A}\). Plugging the definition of the agent's expected utility (Eq. (2)), we obtain \(u_{a^{*}}(t)\geq\mathbb{E}_{j\sim f_{1}}[t]-c_{1}\). Thus, under the mapping \(t_{j}^{\prime}=t_{j}+c_{1}\), it holds that:

\[u_{a^{*}}(t^{\prime})=u_{a^{*}}(t+c_{1})\geq\mathbb{E}_{j\sim f_{1}}[t+c_{1}]-c_{ 1}=\mathbb{E}_{j\sim f_{1}}[t]\geq 0\]

and therefore the contract \(t^{\prime}=t+c_{1}\) is individually rational and still implements action \(a^{*}\). 

### Relation between budget-optimal and min-budget contracts

Proof of Proposition 1.: Given budget \(B>0\), denote by \(t_{\mathrm{BO}}\) the budget-optimal contract, and denote the action it implements by \(n^{*}\). By definition, \(t_{\mathrm{BO}}\) is a feasible solution to the min-budget contract design problem implementing action \(n^{*}\). Denote by \(t_{\mathrm{MB}}\) the corresponding optimal solution to the min-budget problem implementing action \(n^{*}\) (Eq. (4)). \(t_{\mathrm{MB}}\) implements the same action \(n^{*}\) by definition, and satisfies \(\left\|t_{\mathrm{MB}}\right\|_{\infty}\leq\left\|t_{\mathrm{BO}}\right\|_{ \infty}\leq B\) due to the optimization objective. Hence, \(t_{\mathrm{MB}}\) is a budget-optimal contract for the given budget \(B\), which is also min-budget. 

Iterative min-budget.To find the budget-optimal contract using iterative applications of Eq. (4), we observe that any budget-limited contract \(t:\Omega\rightarrow[0,B]\) has bounded expected pay \(\mathbb{E}_{f_{n}}[t]\leq B\) for any distribution \(f_{n}\). Hence, actions \(n^{\prime}\) with cost \(c_{n^{\prime}}>B\) cannot be implemented using budget \(B\), as the agent's utility \(u_{n^{\prime}}=\mathbb{E}_{f_{n^{\prime}}}[t]-c_{n^{\prime}}<0\) will be smaller than the utility of the zero-cost action \(u_{0}=\mathbb{E}_{f_{n}}[t]-0\geq 0\). Define the reduced action set:

\[\mathcal{A}^{\prime}=\{n\in\mathcal{A}\mid c_{n}\leq B\wedge n\text{ is implementable}\}\]

\(\mathcal{A}^{\prime}\) is finite when \(\mathcal{A}\) is finite, or when the data cost is unbounded and the learning curve is monotone. To find \(n^{*}\) within this space, go over all \(n\in\mathcal{A}^{\prime}\), calculate the minimal budget \(B_{n}\) required for implementation, and take \(\operatorname*{argmin}_{n\in\mathcal{A}^{\prime}}B_{n}\). This is possible within a finite number of steps.

### The min-budget linear program and equivalent forms

The min-budget contract (Eq. (4)) implementing action \(i\in\mathcal{A}\) is given by the MIN-BUDGET linear program:

\[\min_{t\in\mathbb{R}_{\geq 0}^{[\Omega]},B\in\mathbb{R}_{\geq 0}}B\] (6) \[\mathrm{s.t.}\] \[\forall j\in\Omega:t_{j}\leq B\] (BUDGET) \[\forall i^{\prime}\neq i:\sum_{j\in\Omega}F_{i^{\prime},j}t_{j}-c _{i^{\prime}}\leq\sum_{j\in\Omega}F_{i,j}t_{j}-c_{i}\] (IC)

The dual of the min-budget LP is given by:

**Claim 2**.: _The dual linear program of Eq. (6) is given by:_

\[\max_{\lambda\in\mathbb{R}_{\geq 0}^{n^{*}-1},\mu\in\mathbb{R}_{ \geq 0}^{[\Omega]}}\sum_{i^{\prime}\neq i}\left(c_{i}-c_{i^{\prime}}\right) \lambda_{i^{\prime}}\] (7) \[\mathrm{s.t.}\] \[\forall j\in\Omega:\sum_{i^{\prime}\neq i}\left(F_{i,j}-F_{i^{ \prime},j}\right)\lambda_{i^{\prime}}\leq\mu_{j}\] \[\sum_{j\in\Omega}\mu_{j}\leq 1\]

Proof.: We take the dual by translating the optimization problem into canonical form. The canonical form we target:

\[\min_{x\geq 0}c^{T}x\quad\text{s.t.}\quad Cx\leq d\]

and its dual, as given by Lahaie [38], is:

\[\max_{y\geq 0}-d^{T}y\quad\text{s.t.}\quad C^{T}y\geq-c\]

To translate Eq. (6), we note that in our case the components \(c\),\(C\),\(d\) are given by:\[c^{T}=(0\quad\dots\quad 0\quad 1)\in\mathbb{R}^{|\Omega|+1}\]

\[C=\left(\begin{array}{ccccc|c}F_{1,0}-F_{i,0}&\dots&&&&F_{1,|\Omega|}-F_{i,| \Omega|}&0\\ \vdots&\ddots&&&\vdots&\vdots\\ &&F_{i^{\prime},j}-F_{i,j}&&&\\ &&\ddots&&&&&\\ F_{n,0}-F_{i,0}&&&&F_{n,m}-F_{i,m}&0\\ \hline&&&&-1\\ &&I_{m+1}&&&&\vdots\\ &&&&-1\end{array}\right)\in\mathbb{R}^{(n-1+|\Omega|)\times(|\Omega|+1)}\]

\[d^{T}=(c_{1}-c_{i}\quad\dots\quad c_{n}-c_{i}\quad|\quad 0\quad\dots\quad 0)\in \mathbb{R}^{n-1+|\Omega|}\]

To simplify formulation, we denote the dual optimization variable \(y\) as follows:

\[y^{T}=(\lambda_{1}\quad\dots\quad\lambda_{i-1}\quad\lambda_{i+1}\quad\dots \lambda_{n}\quad|\quad\mu_{0}\quad\dots\quad\mu_{m})\in\mathbb{R}^{n-1+| \Omega|}\]

Under this formulation, the dual's objective is:

\[-d^{T}y=\sum_{i\neq i^{\prime}}\left(c_{i}-c_{i^{\prime}}\right)\lambda_{i^{ \prime}}\] (8)

The constraints given by the first \(m\) rows of \(C^{T}\) are:

\[\forall j\in\Omega:\sum_{i^{\prime}\neq i}\left(F_{i^{\prime},j}-F_{i,j} \right)+\mu_{j}\geq 0\]

and equivalently:

\[\forall j\in\Omega:\sum_{i^{\prime}\neq i}\left(F_{i,j}-F_{i^{\prime},j} \right)\leq\mu_{j}\] (9)

The constraint corresponding to the last row of \(C^{T}\) is given by:

\[\sum_{j\in\Omega}-\mu_{m}\geq-1\]

and equivalently:

\[\sum_{j\in\Omega}\mu_{m}\leq 1\] (10)

Combining equations (8, 9, 10) yields the linear program given by Eq. (7). 

To map between contracts and hypothesis tests, we introduce the following variable transformation:

**Definition 3** (Statistical representation of contracts).: _For a given contract \(t:\Omega\to\mathbb{R}_{\geq 0}\), denote \(B=\max_{j}t_{j}\). The statistical representation of \(t\) is given by:_

\[t_{j}=\phi_{j}\beta^{-1}\]

_where \(\phi_{j}\in[0,1]\) and \(\beta=B^{-1}\)._

Note that the transformation \((t,B)\mapsto(\phi,\beta)\) is non-linear, and well-defined for all \(B>0\). Under this variable transformation, the MIN-BUDGET transforms to an equivalent linear program:

**Lemma 2**.: _For a feasible design problem, the min-budget linear program (Eq. (6)) is equivalent to:_

\[\begin{split}&\max_{\phi\in[0,1]^{|\Omega|},\beta\in\mathbb{R}_{ \geq 0}}\beta\\ &\mathrm{s.t.}\\ &\forall i^{\prime}\neq i:\sum_{j\in\Omega}F_{i,j}(1-\phi_{j})+ \sum_{j\in\Omega}F_{i^{\prime},j}\phi_{j}\leq 1-(c_{i}-c_{i^{\prime}})\beta \end{split}\] (11)

Proof.: Given Eq. (6), define \(\phi\in[0,1]^{|\Omega|}\), \(\beta\geq 0\) according to Definition 3:

\[\begin{split} B&=\beta^{-1}\\ t_{j}&=\phi_{j}B=\phi_{j}\beta^{-1}\end{split}\] (12)

Under the transformation defined by Eq. (12), the (BUDGET) constraint in Eq. (6) transforms as follows:

\[\begin{split} t_{j}\leq B&\Leftrightarrow\phi_{j} \beta^{-1}\leq\beta^{-1}\\ &\Leftrightarrow\phi_{j}\leq 1\end{split}\] (13)

and the (IC) constraint in Eq. (6) transforms as:

\[\begin{split}\sum_{j}F_{i,j}t_{j}-c_{i}\geq\sum_{j}F_{i^{\prime}, j}t_{j}-c_{i^{\prime}}&\Leftrightarrow\sum_{j}\left(F_{i,j}-F_{i^{ \prime},j}\right)\phi_{j}\beta^{-1}\geq c_{i}-c_{i^{\prime}}\\ &\Leftrightarrow\sum_{j}F_{i,j}\phi_{j}-\sum_{j}F_{i^{\prime},j} \phi_{j}\geq\left(c_{i}-c_{i^{\prime}}\right)\beta\\ &\Leftrightarrow 1-\sum_{j}F_{i,j}(1-\phi_{j})-\sum_{j}F_{i^{\prime},j} \phi_{j}\geq\left(c_{i}-c_{i^{\prime}}\right)\beta\\ &\Leftrightarrow\sum_{j}F_{i,j}(1-\phi_{j})+\sum_{j}F_{i^{\prime },j}\phi_{j}\leq 1-\left(c_{i}-c_{i^{\prime}}\right)\beta\end{split}\] (14)

where the first equivalence is by Eq. (12), and the third equivalence is valid as \(\sum_{j}F_{i^{\prime},j}=1\) for all \(i^{\prime}\). Finally, the objective of Eq. (6) transforms as:

\[\min B\Leftrightarrow\max\beta\] (15)

Combining equations (13, 14, 15) yields the linear program in Eq. (11). 

### Relation to min-pay contract design

Min-pay contract design aims to design a contract which minimizes the expected pay under the implemented action \(n^{*}\):

\[t^{*}=\operatorname*{argmin}_{t}\mathbb{E}_{j\sim f_{n^{*}}}[t_{j}]\quad \text{s.t.}\quad n(t)=n^{*}\] (16)

In contrast to the min-budget contract (Eq. (4)), the \(\left\lVert t\right\rVert_{\infty}\) objective measuring maximal pay is replaced with the \(\mathbb{E}_{j\sim f_{n^{*}}}[t_{j}]\) objective measuring expected pay. Eq. (16) is equivalent to the MIN-PAY linear program:

\[\begin{split}&\min_{t\in\mathbb{R}_{\geq 0}^{|\Omega|}}\sum_{j \in\Omega}F_{i,j}t_{j}\\ &\mathrm{s.t.}\\ &\forall i^{\prime}\neq i:\sum_{j\in\Omega}F_{i^{\prime},j}t_{j}-c _{i^{\prime}}\leq\sum_{j\in\Omega}F_{i,j}t_{j}-c_{i}\quad\text{(IC)}\end{split}\] (17)

#### b.3.1 Equivalence of implementability

The implementatbility of min-pay contracts is characterized in Dutting et al. [19]. We cite the main result for completeness:

**Proposition 3** (Min-pay implementability; [19], A.2).: _An action \(i\in\mathcal{A}\) is implementable (up to tie-breaking) if and only if there is no convex combination of the other actions that results in the same distribution \(f_{i}=\sum_{i^{\prime}\neq i}\alpha_{i^{\prime}}f_{i^{\prime}}\), but lower cost \(c_{i}>\sum_{i^{\prime}\neq i}\alpha_{i^{\prime}}c_{i^{\prime}}\)._We leverage to Proposition 3 to characterize the implementability of min-budget contracts. This is possible due to the following connection:

**Claim 3** (Implementability equivalence).: _A contract \(t\) is feasible solution of MIN-BUDGET if and only if it is a feasible solution of MIN-PAY._

Proof.: A contract \(t\) which satisfies MIN-BUDGET (Eq. (6)) satisfies the (IC) constraint, and thus also satisfies MIN-PAY (Eq. (17)). Conversely, a contract \(t\) which satisfies MIN-PAY satisfies the (IC) constraint. Set \(B=\max_{j}t_{j}\) and obtain a feasible solution to MIN-BUDGET. 

#### b.3.2 Functional form of min-pay contracts under MLRP

The optimal min-pay contracts under the MLRP assumption (Definition 1) are characterized in [19]:

**Proposition 4** (Optimal min-pay contract under MLRP; [19], Lemma 7).: _Consider a contract design setting for which MLRP holds. If the highest-cost action \(n\) is implementable, then there is a min-pay contract has a single nonzero-payment, which is rewarded for the highest outcome \(m\)._

See Fig. 5 for a qualitative comparison of min-pay and min-budget contracts.

Min-pay contracts in delegated classification settings with MLRP.While simple, the min-pay contract given by Proposition 4 would be impractical in many realistic scenarios of delegated classification. In a delegated classification setting, the highest outcome corresponds to 100% validation set accuracy (i.e all validation set samples classified correctly). As \(m\) grows, the highest outcome becomes exponentially less likely, and the min-pay contract awards increasingly high payment with increasingly low probability (See Fig. 6). In such settings, even a slight degree of risk-aversion is likely to affect the agent's decisions: From the agent's perspective, the probability of receiving any payment from a min-pay contract may become small to a degree where even a slight degree of agent risk-aversion will manifest itself in the decision-making process. From the principal's perspective, even though min-pay contracts guarantee small payment in expectation, the amount payment in the case of a rare event would quickly become unfeasible.

### Min-budget contracts with two actions

In this section, we explore min-budget settings with two actions, and prove Theorem 2. When \(n=2\), assume without loss of generality that the contract implements action \(i=2\), and denote \(c=c_{2}-c_{1}>0\).

We recall the definition of total variation distance:

**Definition 4** (Total variation distance).: _Given two distributions \(P,Q\in\Delta\left(\Omega\right)\), the total variation distance between \(P\) and \(Q\) is:_

\[\left\|P-Q\right\|_{\mathrm{TV}}=\frac{1}{2}\left\|P-Q\right\|_{1}\]

Figure 5: Qualitative comparison of min-pay and min-budget contracts under MLRP. **(Left)** The contract design setting, representing two possible actions with binomial outcome distributions (\(p_{1}=0.5\), \(p_{2}=0.8\), \(m=15\)). Action \(1\) has zero cost \(c_{1}=0\), and action \(2\) has unit cost \(c_{2}=1\). **(Right)** The resulting min-pay (red) and min-budget (purple) contracts, obtained by solving Eq. (17) and Eq. (6), respectively. The min-pay awards payment only for the highest outcome, in alignment with Proposition 4; the min-budget is a threshold contract, in alignment with claim 12.

The following equivalent definition is useful:

**Claim 4**.: _Let \(P,Q\in\Delta\left(\Omega\right)\). It holds:_

\[\left\|P-Q\right\|_{\mathrm{TV}}=\sum_{j\in\Omega}\left(Q_{j}-P_{j}\right)^{+}\]

_where \(x^{+}=\max\left\{x,0\right\}\)._

Proof.: By definition:

\[\left\|P-Q\right\|_{\mathrm{TV}}=\frac{1}{2}\left\|P-Q\right\|_{1}\]

Decompose the \(L_{1}\) norm:

\[\frac{1}{2}\left\|P-Q\right\|_{1}=\frac{1}{2}\sum_{j\in\Omega} \left(\left(Q_{j}-P_{j}\right)^{+}+\left(P_{j}-Q_{j}\right)^{+}\right)\]

As \(\sum_{j\in\Omega}P_{j}=\sum_{j\in\Omega}Q_{j}=1\), it holds that:

\[\sum_{j\in\Omega}\left(Q_{j}-P_{j}\right)^{+}=\sum_{j\in\Omega} \left(P_{j}-Q_{j}\right)^{+}\]

and therefore \(\left\|P-Q\right\|_{\mathrm{TV}}=\sum_{j\in\Omega}\left(Q_{j}-P_{j}\right)^{+}\) as required. 

#### b.4.1 Optimal two-action contract

**Theorem 5** (Two-action min-budget contract; formal statement of Theorem 1).: _When \(n=2\), the optimal min-budget contract \(t^{*}\) is given by:_

\[t^{*}_{j}=\frac{c}{\left\|F_{2}-F_{1}\right\|_{\mathrm{TV}}} \mathds{1}\left[F_{2,j}\geq F_{1,j}\right]\] (18)

Proof.: We prove this claim using LP duality, by showing that the optimal primal objective corresponding to \(t^{*}\) is identical to a feasible solution of the dual LP.

The primal LP (Eq. (6)) for two actions is given by:

\[\min_{t\in\mathbb{R}_{\geq 0}^{[\Omega]},B\in\mathbb{R}_{\geq 0}}B\] (19) \[\mathrm{s.t.}\] \[\forall j\in\Omega:t_{j}\leq B\] (BUDGET) \[\sum_{j\in\Omega}\left(F_{2,j}-F_{1,j}\right)t_{j}\geq c\] (IC)

Figure 6: Comparison of min-pay and min-budget contracts under MLRP for varying validation set size \(m\). The delegation setting is similar to the one depicted in Fig. 5: two binomial-outcome actions (\(p_{1}=0.5\), \(p_{2}=0.8\)) and varying \(m\). Action \(1\) has zero cost \(c_{1}=0\), and action \(2\) has unit cost \(c_{2}=1\). In the left and center plots, the cost of action \(2\) is represented by an orange line (\(c_{2}=1\)) representing the lower bound as in Fig. 3 (Center). **(Left)** Expected pay \(\mathbb{E}_{j\sim f_{2}}[t_{j}]\). **(Center)** Required budget \(\left\|t\right\|_{\infty}\). **(Right)** Probability of getting any payment \(\mathbb{P}_{j\sim f_{2}}[t_{j}>0]\).

As \(t^{*}\) is bounded, the optimal objective \(B^{*}\) corresponds to the maximal possible payout of \(t^{*}\):

\[B^{*}=\max_{j\in\Omega}t_{j}^{*}=\frac{c}{\left\|F_{2}-F_{1}\right\|_{\mathrm{TV}}}\] (20)

and therefore the (BUDGET) constraint is satisfied. For the (IC) constraint, denote by \(\Omega_{\geq}\) the following set:

\[\Omega_{\geq}=\left\{j\in\Omega\mid F_{2,j}\geq F_{1,2}\right\}\]

using this notation, we note that \(t_{j}^{*}>0\) if and only if \(j\in\Omega_{\geq}\). Plugging into the constraint, we obtain:

\[\sum_{j\in\Omega}\left(F_{2,j}-F_{1,j}\right)t_{j}^{*} =\sum_{j\in\Omega_{\geq}}\left(F_{2,j}-F_{1,j}\right)B^{*}\] \[=c\frac{\sum_{j\in\Omega_{\geq}}\left(F_{2,j}-F_{1,j}\right)}{ \left\|F_{2}-F_{1}\right\|_{\mathrm{TV}}}\] \[=c\]

Where \(\left\|F_{2}-F_{1}\right\|_{\mathrm{TV}}=\sum_{j\in\Omega_{\geq}}\left(F_{2, j}-F_{1,j}\right)\) by definition. This shows that \(t^{*}\) is a feasible solution for the primal LP.

To prove optimality, we show that the the dual linear program attains an identical objective. By Claim 2, the dual LP for two actions is given by:

\[\max_{\lambda\in\mathbb{R}_{\geq 0},\mu\in\mathbb{R}_{\geq 0}^{ \left[\Omega\right]}}c\lambda\] (21) \[\mathrm{s.t.}\] \[\forall j\in\Omega:\left(F_{2,j}-F_{1,j}\right)\lambda\leq\mu_{j}\] \[\sum_{j\in\Omega}\mu_{j}\leq 1\]

Denote the vector \(\vec{v}_{j}(\lambda)\in\mathbb{R}_{\geq 0}^{\left[\Omega\right]}\):

\[\vec{v}_{j}(\lambda)=\lambda\left(F_{2,j}-F_{1,j}\right)^{+}\]

We note that outcomes \(j^{\prime}\) for which \(F_{2,j^{\prime}}\leq F_{1,j^{\prime}}\) (formally \(j^{\prime}\in\Omega\setminus\Omega_{\geq}\)) correspond to constraints which are satisfied for any \(\lambda\geq 0\). Otherwise \(j\in\Omega_{\geq}\), and in these cases \(\lambda\) can be increased until \(\vec{v}(\lambda)\) saturates the simplex constraint \(\sum_{j\in\Omega}\mu_{j}=1\). The simplex constraint is binding for a value \(\lambda^{*}>0\) satisfying:

\[\sum_{j\in\Omega}\lambda^{*}\left(F_{2,j}-F_{1,j}\right)^{+}=1\]

and therefore the optimal value \(\lambda^{*}\) of the dual LP (Eq. (21)) is:

\[c\lambda^{*}=\frac{c}{\sum_{j\in\Omega}\left(F_{2,j}-F_{1,j}\right)^{+}}= \frac{c}{\left\|F_{2}-F_{1}\right\|_{\mathrm{TV}}}\] (22)

Where the second equality is given by claim 4. The dual objective in Eq. (22) is identical to the primal objective attained by the contract \(t^{*}\) in Eq. (20), and therefore \(t^{*}\) is an optimal contract by strong LP duality. 

#### b.4.2 Contracts and hypothesis tests

We recall the formal definition of the Neyman-Pearson lemma:

**Lemma 3** (Neyman-Pearson, [e.g., 45, 4.3]).: _Let \(P,Q\in\Delta\left(\mathcal{X}\right)\) be two probability measures over an arbitrary set \(\mathcal{X}\). Then for any hypothesis test \(\psi:\mathcal{X}\to\left\{0,1\right\}\), it holds:_

\[P(\psi(x)=1)+Q(\psi(x)=0)\geq 1-\left\|P-Q\right\|_{\mathrm{TV}}.\] (23)

_Moreover, equality holds for the Likelihood Ratio test \(\psi^{*}(x)=\mathds{1}\left[q(x)\geq p(x)\right]\)._In our analysis, we will assume that the space \(\mathcal{X}\) is finite. We also note that Lemma 3 is stated for decision rules with binary output, but its domain can be extended to fractional decision functions \(\psi:\mathcal{X}\rightarrow[0,1]\) without loss of generality: As the sum of errors is linear in \(\psi\), the optimal fractional decision rule is a solution to the linear program \(\min_{\psi\in[0,1]^{\mathcal{X}}}\left(P(\psi=1)+Q(\psi=0)\right)\) where \(P(\psi=1)=\sum_{x\in\mathcal{X}}p_{x}\psi(x)\) and \(Q(\psi=0)=\sum_{x\in\mathcal{X}}q_{x}(1-\psi(x))\). The feasible region of this linear program is the hypercube \(\left[0,1\right]^{\mathcal{X}}\) and its vertices are the set of binary decision rules \(\left\{0,1\right\}^{\mathcal{X}}\), which also includes the optimal binary rule \(\psi^{*}\) given by Lemma 3. As every feasible linear program attains its optimum on a vertex, the binary \(\psi^{*}\) is also the optimal among fractional rules.

Proof of Theorem 2.: For a given two-action contract, denote \(P_{j}=F_{1,j}\), \(Q_{j}=F_{2,j}\), and \(c=c_{2}-c_{1}\). We recall that the statistical min-budget LP for two actions is given by Lemma 2:

\[\max_{\phi\in[0,1]^{[\Omega]},\beta\in\mathbb{R}_{\geq 0}}\beta\] (24) \[\mathrm{s.t.}\] \[\sum_{j\in\Omega}P_{j}\phi_{j}+\sum_{j\in\Omega}Q_{j}(1-\phi_{j}) \leq 1-c\beta\]

and the Neyman-Pearson lemma is given by Eq. (23).

Given a min-budget contract design problem, apply Theorem 1 to obtain its optimal solution, as given by Eq. (18):

\[B^{*} =\frac{c}{\left\|P-Q\right\|_{\mathrm{TV}}}\] \[t_{j}^{*} =\frac{c}{\left\|P-Q\right\|_{\mathrm{TV}}}\mathds{1}\left[Q_{j} \geq P_{j}\right]\]

Applying the transformation from Definition 3 on the optimal contract, we obtain equivalently:

\[\beta^{*} =\left(B^{*}\right)^{-1}=\frac{\left\|P-Q\right\|_{\mathrm{TV}}}{ c}\] \[\phi_{j}^{*} =t_{j}^{*}/B^{*}=\mathds{1}\left[Q_{j}\geq P_{j}\right]\]

Note that \(\phi_{j}^{*}\) is a maximum-likelihood decision rule, similar to the optimal critical function in the Neyman-Pearson lemma. Additionally, by optimality of the contract-design optimization variable \(\beta^{*}\), any feasible solution \(\phi^{\prime}\) of Eq. (24) satisfies:

\[\sum_{j=1}^{m}P_{j}\phi_{j}^{\prime}+\sum_{j=1}^{m}Q_{j}(1-\phi_{j}^{\prime}) \geq 1-c\beta^{*}=1-\left\|P-Q\right\|_{\mathrm{TV}}\]

with equality satisfied by the maximum likelihood rule \(\phi^{*}\). Therefore, the min-budget optimality of the contract \(t^{*}\) implies the power optimality of the hypothesis test \(\phi^{*}\).

Conversely, let \(\phi:\Omega\rightarrow[0,1]\) be an maximal-power hypothesis test. This provides a lower bound on the constraint in Eq. (24):

\[\sum_{j\in\Omega}P_{j}\phi_{j}+\sum_{j\in\Omega}Q_{j}(1-\phi_{j})\geq 1-\left\|P-Q \right\|_{\mathrm{TV}}\]

By the Neyman-Pearson lemma, the bound is tight for the maximum likelihood rule \(\phi^{\star}=\mathds{1}\left[Q\geq P\right]\), and therefore the optimal objective \(\beta^{\star}\) satisfies:

\[1-c\beta^{\star}=1-\left\|P-Q\right\|_{\mathrm{TV}}\]

and hence \(\beta^{\star}=\frac{\left\|P-Q\right\|_{\mathrm{TV}}}{\left\|P-Q\right\|_{ \mathrm{TV}}}\mathds{1}\left[Q_{j}\geq P_{j}\right]\), showing that the corresponding contract is min-budget optimal if the corresponding hypothesis has optimal statistical power. 

**Remark 1**.: _Since the proof of Theorem 1 is independent of the Neyman-Pearson lemma, the argument proving the optimality of \(\phi^{*}\) in the proof above implies the Neyman-Pearson lemma for finite \(\mathcal{X}\)._

### More than two actions

**Definition 5** (All-or-nothing contract).: _Let \(B>0\). An all-or-nothing contract \(t:\Omega\rightarrow\mathbb{R}_{\geq 0}\) satisfies \(t_{j}\in\{0,B\}\) for all \(j\in\Omega\)._

#### b.5.1 Binary outcomes

In this section, we show that every binary-outcome min-budget contract is all-or-nothing. This is a corollary of a more general lemma:

**Lemma 4**.: _For any feasible min-budget contract \(t^{*}\), there exists \(j_{0}\in\Omega\) such that \(t^{*}_{j_{0}}=0\)._

Proof.: By contradiction, assume that \(t^{*}_{j}>0\) for all \(j\), and denote \(j_{0}=\operatorname*{argmin}_{j}t^{*}_{j}\). Denote \(a=\min_{j}t_{j}\), and note that \(a>0\). Define the contract \(\tilde{t}\) as follows:

\[\forall j:\tilde{t}_{j}=t_{j}-a\]

By definition, \(\tilde{t}_{j}\geq 0\) for all \(j\), and \(\tilde{t}_{j_{0}}=0\). Since \(t^{*}\) is a feasible min-budget contract, it satisfies the min-budget LP in Eq. (6), and in particular the (IC) constraint:

\[\forall i\in[n-1]:\sum_{j}F_{i,j}t^{*}_{j}-c_{i}\leq\sum_{j}F_{n,j}t^{*}_{j}-c _{n}\]

Plugging in the definition \(\tilde{t}=t^{*}-a\) and using the fact that \(\sum_{j}F_{i,j}=1\) for all \(i\in[n]\), we obtain:

\[\forall i\in[n-1]:\sum_{j}F_{i,j}\tilde{t}_{j}-c_{i}\leq\sum_{j}F_{n,j}\tilde{ t}_{j}-c_{n}\]

and therefore \(\tilde{t}\) satisfies the (IC) constraint as well. This is a contradiction, since \(\tilde{t}\) is a feasible contract with a lower required budget. From this we conclude that the optimal contract must satisfy \(t_{j}=0\) for some \(j\). 

**Corollary 1**.: _In any min-budget contract design setting with two outcomes, the optimal contract is all-or-nothing._

#### b.5.2 Hardness: Basic definitions and construction

In this section, we show that finding optimal all-or-nothing contracts is NP-hard in the general case, by reduction from 3SAT.

**Definition 6** (Maximin design matrix).: _For a contract design setting with action set \(\mathcal{A}\), outcome space \(\Omega\), target action \(a^{*}\in\mathcal{A}\), and \(c_{a}<c_{a^{*}}\) for all \(a\in\mathcal{A}\setminus\{a^{*}\}\), the maximin design matrix \(A\) is defined as:_

\[A_{a\omega}=\frac{F_{a^{*},\omega}-F_{a,\omega}}{c_{a^{*}}-c_{a}}\] (25)

**Claim 5**.: _For a contract design problem, denote the maximin design matrix by \(A\). An all-or-nothing min-budget contract \(t^{*}\) can be written as \(t^{*}=\phi^{*}/\beta^{*}\), where:_

\[\phi^{*} =\operatorname*{argmax}_{\phi\in\{0,1\}^{|\Omega|}}\min_{a\in \mathcal{A}\setminus\{a^{*}\}}\left(A\phi\right)_{a}\] (26) \[\beta^{*} =\min_{a\in\mathcal{A}\setminus\{a^{*}\}}\left(A\phi^{*}\right)_ {a}\]

Proof.: By Lemma 2, Eq. (11), the min-budget contract design LP is equivalent to:

\[\max_{\phi\in[0,1]^{|\Omega|},\beta\in\mathbb{R}_{\geq 0}}\beta\] \[\text{s.t.}\] \[\forall a\in\mathcal{A}\setminus\{a^{*}\}:\sum_{\omega\in \Omega}\frac{F_{a^{*},\omega}-F_{a,\omega}}{c_{a^{*}}-c_{a}}\phi_{\omega}\geq\beta\]Where \(\beta=B^{-1}\). As \(A_{a,\omega}=\frac{F_{a^{*},\omega}-F_{a,\omega}}{c_{a^{*}}-c_{a}}\), it holds that \(\sum_{\omega\in\Omega}\frac{F_{a^{*},\omega}-F_{a,\omega}}{c_{a^{*}}-c_{a}}\phi_ {\omega}=A\phi\). The LP above is therefore equivalent to:

\[\max_{\phi\in[0,1]^{[\Omega]},\beta\in\mathbb{R}_{\geq 0}}\beta\] \[\mathrm{s.t.}\] \[\forall a\in\mathcal{A}\setminus\{a^{*}\}:A_{a}\phi\geq\beta\]

as \(\beta\) is a lower bound for every constraint, its optimal value is the maximal minimum attainable through variation of \(\phi\):

\[\max_{\phi\in[0,1]^{[\Omega]},\,a\in\mathcal{A}\setminus\{a^{*}\}}\left(A \phi\right)_{a}\]

and restricting the optimization space of \(\phi\) to \(\{0,1\}^{[\Omega]}\) yields the desired result. 

**Definition 7** (3-CNF - Conjunctive normal form).: _A 3-CNF formula over \(m\) variables and \(n\) clauses is a boolean-valued function \(\psi:\{0,1\}^{m}\rightarrow\{0,1\}\) of the form:_

\[\psi(x_{1},\ldots,x_{m})=\bigwedge_{i=1}^{n}\left(z_{i1}\lor z_{i2}\lor z_{i3}\right)\]

_where \(z_{ik}\in\{x_{1},\ldots,x_{m},\neg x_{1},\ldots,\neg x_{m}\}\)._

We assume that variables in each clause are distinct. \(\psi\) is satisfiable if and only if there exists \(x\in\{0,1\}^{m}\) such that \(\psi(x)=1\).

**Definition 8** (Number of positives in clause \(i\)).: _Given a 3-CNF \(\psi\) and an assignment \(x\in\{0,1\}^{m}\), denote by \(\sigma_{i}(\psi,x)\in\{0,\ldots,3\}\) the number of variables \(z_{ik}\) in clause \(i\) which evaluate to \(1\) under assignment \(x\)._

**Claim 6**.: _A formula \(\psi\) is satisfiable if and and only if there exists an assignment \(x\in\{0,1\}^{m}\) such that \(\min_{i\in[n]}\sigma_{i}(\psi,x)\geq 1\)._

Proof.: If \(\psi\) is satisfiable then there exists \(x\in\{0,1\}^{m}\) such that \(\psi(x)=1\). Since \(\psi\) is a 3-CNF, every clause \(i\) must evaluate to \(1\), and therefore \(\sigma_{i}(\psi,x)\geq 1\) for all \(i\in[n]\). Conversely, if there exist an assignment \(x\) such that \(\min_{i\in[n]}\sigma_{i}(\psi,x)\geq 1\) then by definition \(x\) satisfies every clause, and therefore the whole formula \(\psi\). 

#### b.5.3 Hardness: Reduction from 3SAT

Given a 3-CNF \(\psi\) with \(n\) clauses and \(m\) variables, we define a min-budget contract design problem over actions \(\mathcal{A}=[n+1]\) and outcome space \(\Omega=[m]\cup\{\mathrm{pos},\mathrm{neg},\mathrm{const}\}\). The target action is \(a^{*}=n+1\), and the cost associated with action \(i\) is:

\[c_{i}=\begin{cases}1&i=n+1\\ 0&\mathrm{otherwise}\end{cases}\] (27)

For simplicity of notations, let \(\varepsilon>0\), which will be assigned a suitable value later in the construction. The outcome distribution of the target action is denoted by \(Q\), and defined as:

\[\begin{split}&\forall j\in[m]:Q_{j}=\frac{\varepsilon}{m}\\ & Q_{\mathrm{pos}}=1-\varepsilon\left(1+\frac{3}{m}\right)\\ & Q_{\mathrm{neg}}=0\\ & Q_{\mathrm{const}}=\frac{3\varepsilon}{m}\end{split}\] (28)

Note that \(Q\) is well-defined for \(m\geq 3\) and \(\varepsilon\leq\frac{1}{2}\).

For each \(i\in[n]\), denote the number of negated variables in clause \(i\) by \(k_{i}\in\{0,\ldots,3\}\). The distribution corresponding to the \(i\)-th action is denoted by \(P^{(i)}\), and defined as:

\[\begin{split}&\forall j\in[m]:P^{(i)}_{j}=\begin{cases}0&x_{j}\text{ exists in clause }i\\ \frac{2\varepsilon}{m}&\neg x_{j}\text{ exists in clause }i\\ \frac{\varepsilon}{m}&\mathrm{otherwise}\end{cases}\\ & P^{(i)}_{\mathrm{pos}}=0\\ & P^{(i)}_{\mathrm{neg}}=1-\varepsilon\left(1+\frac{k_{i}}{m}\right)\\ & P^{(i)}_{\mathrm{const}}=(3-k_{i})\frac{\varepsilon}{m}\end{cases}\end{split}\] (29)

The distributions \(P^{(i)}\) are well-defined when \(m\geq 3\) and \(\varepsilon\leq\frac{1}{2}\). For concreteness, set:

\[\varepsilon=\frac{1}{10}\] (30)

and note that \(Q_{\mathrm{pos}}\) and \(P^{(i)}_{\mathrm{neg}}\) are both strictly positive for \(m\geq 3\) and this value of \(\varepsilon\).

Plugging equations (27, 28, 29) into Definition 6, the maximin design matrix \(A^{\psi}\in\mathbb{R}^{n\times(m+3)}\) corresponding to the contract design problem above is given by:

\[\begin{split}&\forall j\in[m]:A^{\psi}_{i,j}=\begin{cases}\frac{ \varepsilon}{m}&x_{j}\text{ exists in clause }i\text{ and is not negated}\\ -\frac{\varepsilon}{m}&x_{j}\text{ exists in clause }i\text{ and is negated}\\ 0&\mathrm{otherwise}\end{cases}\\ & A^{\psi}_{i,\mathrm{pos}}=Q_{\mathrm{pos}}\\ & A^{\psi}_{i,\mathrm{neg}}=-P^{(i)}_{\mathrm{neg}}\\ & A^{\psi}_{i,\mathrm{const}}=k_{i}\frac{\varepsilon}{m}\end{split}\] (31)

**Definition 9** (Assignment normalized contract).: _For an assignment \(x\in{\{0,1\}}^{m}\), the corresponding normalized contract \(\phi^{x}\in\{0,1\}^{m+3}\) is:_

\[\begin{split}&\forall j\in[m]:\phi^{x}_{j}=x_{j}\\ &\phi^{x}_{\mathrm{pos}}=1\\ &\phi^{x}_{\mathrm{neg}}=0\\ &\phi^{x}_{\mathrm{const}}=1\end{split}\] (32)

**Claim 7**.: _Let \(\psi\) be a 3-CNF, and \(x\in{\{0,1\}}^{m}\) an assignment. For all \(i\in[n]\):_

\[\left(A^{\psi}\phi^{x}\right)_{i}=\frac{\varepsilon}{m}\sigma_{i}(\psi,x)+Q_{ \mathrm{pos}}\]

Proof.: To prove this claim, plug \(\phi_{x}\) from Definition 9 into \(A^{\psi}\) defined in Eq. (31). To simplify notations, here we denote \(A=A^{\psi}\), \(\phi=\phi^{x}\), \(z_{i}=\{z_{i1},z_{i2},z_{i3}\}\). We obtain:

\[\begin{split}\left(A\phi\right)_{i}&=\sum_{\omega \in[m]\cup\{\mathrm{pos},\mathrm{neg},\mathrm{const}\}}A_{i,\omega}\phi_{ \omega}\\ &=\sum_{\begin{subarray}{c}j\in[m]\\ \end{subarray}}A_{i,j}\phi_{j}\\ &=\frac{\varepsilon}{m}\left(\sum_{x_{j}\in z_{i}}\phi_{j}-\sum_{ x_{j^{\prime}}\in z_{i}}\phi_{j^{\prime}}\right)\\ &=\frac{\varepsilon}{m}\left(\sum_{x_{j}\in z_{i}}\phi_{j}+\sum_{ x_{j^{\prime}}\in z_{i}}\left(1-\phi_{j^{\prime}}\right)\right)+Q_{\mathrm{pos}}\\ &=\frac{\varepsilon}{m}\sigma_{i}(\psi,x)+Q_{\mathrm{pos}}\end{split}\]

**Claim 8**.: _For a given 3-CNF \(\psi\), denote by \(\phi^{*}\) the optimal solution for Eq. (26), with \(A=A^{\psi}\). There exists an assignment \(x^{*}\) such that \(\phi^{*}=\phi^{x^{*}}\)._

Proof.: For the maximin matrix \(A^{\psi}\) defined in Eq. (31), the optimization objective in Eq. (26) is:

\[\min_{i\in[n]}\left(A^{\psi}\phi\right)_{i}\]

By the choice of \(\varepsilon\) in Eq. (30), \(A^{\psi}\) satisfies the following for all \(i\in[n]\):

\[A_{i,\mathrm{pos}}=Q_{\mathrm{pos}} >0\] \[A_{i,\mathrm{neg}}=-P_{\mathrm{neg}}^{(i)} <0\] \[A_{i,\mathrm{const}}=k_{i}\frac{\varepsilon}{m} >0\]

Thus, any optimal solution \(\phi^{*}\) must satisfy:

\[\phi_{\mathrm{pos}}^{*} =1\] \[\phi_{\mathrm{neg}}^{*} =0\] \[\phi_{\mathrm{const}}^{*} =1\]

Otherwise, the value of any \(\left(A^{\psi}\phi\right)_{i}\) would strictly increase by changing the value of \(\psi^{*}\) at the coordinates \(\left\{\mathrm{pos},\mathrm{neg},\mathrm{const}\right\}\) to the values specified above. This would increase the optimization objective \(\min_{a\in A\setminus\left\{a^{*}\right\}}\left(A^{\psi}\phi\right)_{a}\), in contradiction to the optimality of \(\phi^{*}\).

Hence, denoting \(x_{j}^{*}=\phi_{j}^{*}\) for all \(j\in[m]\), we obtain \(\phi^{*}=\phi^{x^{*}}\) as required. 

**Claim 9**.: _A 3-CNF \(\psi\) is satisfiable if and only if the optimization problem in Eq. (26) with \(A=A^{\psi}\) satisfies \(\beta^{*}\geq Q_{\mathrm{pos}}+\frac{\varepsilon}{m}\)._

Proof.: If \(\psi\) is satisfiable by assignment \(x\in\left\{0,1\right\}^{m}\), then \(\sigma_{i}(\psi,x)\geq 1\) for all \(i\in[n]\) by Claim 6. Observe that \(\beta^{*}=\left(A^{\psi}\phi^{x}\right)_{i}\), for some \(i^{*}\in[n]\) by construction. Let \(\phi^{x}\) denote the vector corresponding to the satisfying assignment, according to Definition 9. Apply Claim 7 to obtain:

\[\beta^{*} =\left(A^{\psi}\phi^{x}\right)_{i}\] \[=\frac{\varepsilon}{m}\sigma_{i^{*}}(\psi,x)+Q_{\mathrm{pos}}\] \[\geq Q_{\mathrm{pos}}+\frac{\varepsilon}{m}\]

Conversely, assume the optimal solution to Eq. (26) satisfies \(\beta^{*}\geq Q_{\mathrm{pos}}+\frac{\varepsilon}{m}\), and denote the vector attaining the optimal solution by \(\phi^{*}\). By Claim 8, there exists an assignment \(x^{*}\) such that \(\phi^{*}=\phi^{x^{*}}\). Combining the lower bound on the value of the optimal solution with the result of Claim 7, we obtain that the following holds for all \(i\in[n]\):

\[\left(A^{\psi}\phi^{x^{*}}\right)_{i}=\frac{\varepsilon}{m}\sigma_{i}(\psi,x^ {*})+Q_{\mathrm{pos}}\geq\frac{\varepsilon}{m}+Q_{\mathrm{pos}}\]

Therefore, it must hold that \(\sigma_{i}(\psi,x^{*})\geq 1\), and thus \(x^{*}\) satisfies \(\psi\) according to Claim 6. 

#### b.5.4 Proof of hardness

Proof of Theorem 3.: By reduction from 3SAT. Given a 3-CNF \(\psi\) with \(n\) clauses and \(m\) variables, construct in polynomial time the corresponding matrix \(A^{\psi}\) as defined by Eq. (31), and apply an all-or-nothing min-budget contract solver according to Eq. (26) to obtain the optimal solution. The validity of the LP is given by Eq. (26). By Claim 9, the formula \(\psi\) is satisfiable if and only if the value attained in the optimization is at least \(\frac{\varepsilon}{m}+Q_{\mathrm{pos}}\), where \(m\) is the number of variables in \(x\), and \(\varepsilon,Q_{\mathrm{pos}}\) are constants defined in Eq. (30), Eq. (28) respectively.

### The single binding action algorithm

To prove the soundness of SBA, we first make the following definition:

**Definition 10** ((\(i^{\prime}\)-IC) relaxation).: _Consider a MIN-BUDGET LP with target action \(i\in\mathcal{A}\), as given by Eq. (6). For any action \(i^{\prime}\neq i\), the (\(i^{\prime}\)-IC) relaxation of the min-budget problem is given by eliminating all (IC) constraints except for the one corresponding to action \(i^{\prime}\)._

Proof of Proposition 2.: Denote the target action by \(i\in\mathcal{A}\). On each iteration of the loop, the algorithm considers some action \(i^{\prime}\neq i\), and applies Theorem 1 on the (\(i^{\prime}\)-IC) relaxation of the original MIN-BUDGET LP. Since the relaxed LP has only one (IC) constraint, its optimal solution, denoted by \(t^{*}(i^{\prime},i)\), is given by Eq. (5).

By Definition 10, the feasible region of the original LP lies within feasible region corresponding to its (\(i^{\prime}\)-IC) relaxation. Thus, if an optimal solution of the relaxed LP lies within the feasible region of the original LP, then it is also a global optimum of the original LP. \(t^{*}(i^{\prime},i)\) lies within the feasible region of the original LP is it satisfies the remaining (IC) constraints--a condition equivalent to the notation \(a(t^{*}(i^{\prime},i))=i\) by Eq. (2). The SBA algorithm terminates successfully only in such cases, and therefore it is sound. 

A note on ties in SBA.Denote the target action by \(i\). To simplify presentation, the algorithm presentation in Section 3.4 implicitly assumes that required budgets \(\left\|t^{*}(i^{\prime},i)\right\|_{\infty}\) are distinct for every pair of actions \((i^{\prime},i)\), and therefore the return value of SBA is well-defined. In case of ties, the return value is not well-defined (as the iteration order is not well-defined), but small a modification to the algorithm allows ties to be broken explicitly without affecting other properties of the algorithm: In case of ties in optimal required budget, the soundness of the algorithm and the all-of-nothing property of the returned contracts is not affected. However, the exact functional form of the returned contract may be affected by the iteration order. In case such issue becomes relevant, the SBA algorithm can be modified to first collect a set of optimal contracts (instead of immediately returning when an optimal feasible contract is found), and then select one of the optimal contracts based on the desired criteria. As the proof of Proposition 2 does not depend on iteration order, soundness will not be affected, and worst-case time complexity will remain the same.

### MLRP

In this section, we explore contract design under the MLRP assumption, and prove Theorem 4. We first recall the statistical notion of monotone likelihood ratio:

**Definition 11** (Mononote Likelihood Ratio - MLR).: _The distributions \(P,Q\in\Delta\left(\Omega\right)\) have Monotone Likelihood Ratio when \(\frac{Q_{i}}{P_{j}}\) is monotonically increasing for all \(j\in\Omega=\{0,\ldots,m\}\). We denote this by \(P\prec Q\)._

An introduction to MLR and its relation to statistical hypothesis testing is provided in Lehmann et al. [40, Section 3.4]. Using this notation, we can reformulate Definition 1:

**Definition 12** (MLRP; reformulation of Def. 1 using MLR notation).: _A principal-agent problem satisfies the Monotone Likelihood Ratio Property if \(F_{i^{\prime}}\prec F_{i}\) for every pair of actions \(i,i^{\prime}\) such that \(c_{i^{\prime}}<c_{i}\)._

**Claim 10**.: _Let \(P,Q\in\Delta\left([m]\right)\) such that \(P\prec Q\). Then \(P_{0}>Q_{0}\) and \(P_{m}<Q_{m}\)._

Proof.: For the first inequality, assume by contradiction that \(Q_{0}\geq P_{0}\). Combining with the MLR property, this assumption implies that \(Q_{j}\geq P_{j}\) for all \(j\in\{0,\ldots,m\}\). As the outcome distributions \(P\), \(Q\) are distinct, there exists at least one \(j\) for which \(Q_{j}>P_{j}\). Summing over \(j\) yields: \(\sum_{j\in\Omega}Q_{j}>\sum_{j\in\Omega}P_{j}\), which is a contradiction, as the inequality is strict while both sides sum to \(1\). The proof for the second inequality follows in the same way. 

**Definition 13** (MLR crossing point \(j_{P,Q}^{*}\)).: _Let \(P,Q\in\Delta\left(\Omega\right)\) such that \(P\prec Q\). The crossing point of \(Q\) over \(P\) is the outcome \(j^{*}\in\{0,\ldots,m\}\) such that \(Q_{j^{*}-1}<P_{j^{*}-1}\) and \(Q_{j^{*}}\geq P_{j^{*}}\). When distribution are not clear from context, we denote \(j^{*}=j_{P,Q}^{*}\)_In words, the crossing point \(j^{*}\) is the outcome such that for every lower outcome, \(P\) is strictly more likely than \(Q\), and starting with this outcome \(Q\) is weakly more likely. By claim 10, the MLR crossing point \(j^{*}\) is uniquely defined for any \(P\prec Q\), and it holds that \(j^{*}>0\).

**Definition 14** (Survival function).: _Given \(P\in\Delta\left([m]\right)\), the survival function \(\mathbb{S}_{P}(\cdot):\Omega\rightarrow[0,1]\) is defined as:_

\[\mathbb{S}_{P}(j)=\mathbb{P}_{j^{\prime}\sim P}[j^{\prime}>j]=\sum_{j^{\prime }=j+1}^{m}P_{i}\]

Informally, the survival function is 1 minus the distribution's CDF. This function is useful in our context, since the total variation distance between two distributions (as appearing in Theorem 5) can be represented as the difference between their survival functions when they satisfy MLR. The intuitive reason for this is that MLR means the distributions are "single-crossing".

**Claim 11** (Total variation under MLR).: _Let \(P,Q\in\Delta\left(\Omega\right)\) such that \(P\prec Q\). It holds:_

\[\left\|P-Q\right\|_{\mathrm{TV}}=\mathbb{S}_{Q}(j^{*}_{P,Q}-1)-\mathbb{S}_{P} (j^{*}_{P,Q}-1)\]

Proof.: By Claim 4, it holds that:

\[\left\|P-Q\right\|_{\mathrm{TV}}=\sum_{j\in\left\{0,\ldots,m\right\}}\left(Q _{j}-P_{j}\right)^{+}\]

Using the \(j^{*}\) notation (see Definition 13), we obtain:

\[\sum_{j\in\left\{0,\ldots,m\right\}}\left(Q_{j}-P_{j}\right)^{+}=\sum_{j=j^{* }_{P,Q}}^{m}Q_{j}-P_{j}\]

And the result is obtained by applying the definition of survival function (see Definition 14). 

#### b.7.1 Two-action contracts with MLRP

Combining Theorem 5 with Claim 10 leads to a characterization of threshold contracts in the case of two actions:

**Claim 12**.: _For any two-action contract design problem satisfying MLRP, the optimal contract is a threshold contract:_

\[t^{*}_{j}=\frac{c}{\left\|F_{2}-F_{1}\right\|_{\mathrm{TV}}}\mathds{1}\left[ j\geq j^{*}_{F_{1},F_{2}}\right]\]

_where \(j^{*}_{F_{1},F_{2}}=\min\left\{j\in\left\{0,\ldots,m\right\}\mid F_{2,j}\geq F _{1,j}\right\}\) as in Definition 13._

Proof.: Combining the result of Claim 10 with the monontonicity assumption, the likelihood ratio \(F_{2,j}/F_{1,j}\) crosses \(1\) exactly once. Denote the crossing point by \(j^{*}\), and apply Theorem 5 to obtain the optimal contract. 

#### b.7.2 Two-outcome contracts with MLRP

When there are more than two actions, assume without loss of generality that the contract aims to implement the last action \(n\). The following claim establishes the existence of theshold contracts in the two-outcome setting \(\left|\Omega\right|=2\). In contrast to corollary 1, the proof in constructive, and yields a concrete contract:

**Claim 13**.: _For any contract design problem satisfying MLRP with \(n>2\) actions and \(m=2\) outcomes, the optimal min-budget contract is a threshold contract._Proof.: By Claim 2, the dual LP for two outcomes is given by:

\[\max_{\lambda\in\mathbb{R}_{\geq 0}^{n-1},\mu\in\mathbb{R}_{\geq 0}^{ 2}}\sum_{i=1}^{n-1}(c_{n}-c_{i})\lambda_{i}\] (33) \[\mathrm{s.t.}\] \[\forall j\in\{1,2\}:\sum_{i=1}^{n-1}\left(F_{n,j}-F_{i,j}\right) \lambda_{i}\leq\mu_{j}\] \[\sum_{j\in\{1,2\}}\mu_{j}\leq 1\]

From Claim 10, we obtain that \(F_{n,1}-F_{i,1}<0\) and \(F_{n,2}-F_{i,2}>0\) for all \(i\in[n-1]\), and therefore the first constraint in Eq. (33) is always satisfied for \(j=1\). Simplifying Eq. (33) we obtain:

\[\max_{\lambda\in\mathbb{R}_{\geq 0}^{n-1},\mu\in\mathbb{R}_{ \geq 0}}\sum_{i=1}^{n-1}(c_{n}-c_{i})\lambda_{i}\] (34) \[\mathrm{s.t.}\] \[\sum_{i=1}^{n-1}\left(F_{n,2}-F_{i,2}\right)\lambda_{i}\leq 1\]

which is maximized by allocating all budget to the \(\lambda_{i}\) maximizing the "bang for buck". The dual objective is therefore given by:

\[B^{*} =\max_{\lambda\in\mathbb{R}_{\geq 0}^{n-1},\mu\in\mathbb{R}_{ \geq 0}}\sum_{i\in[n-1]}(c_{n}-c_{i})\lambda_{i}\] (35) \[=\max_{i\in[n-1]}\frac{c_{n}-c_{i}}{F_{n,2}-F_{i,2}}\]

To see that a threshold contract is optimal, let \(t_{j}^{*}=B^{*}\cdot\mathds{1}\left[j=1\right]\). Primal objective is \(B^{*}\), and the contract is feasible if the primal LP is feasible. The (IC) constraint in Eq. (6) can be written as:

\[\forall i\in[n-1]:\frac{\sum_{j=1}^{2}\left(F_{n,j}-F_{i,j}\right)t_{j}}{c_{n }-c_{i}}\geq 1\]

and indeed for every action \(i\in[n-1]\):

\[\frac{\sum_{j=1}^{2}\left(F_{n,j}-F_{i,j}\right)t_{j}}{c_{n}-c_{i}} =\frac{F_{n,2}-F_{i,2}}{c_{n}-c_{i}}B^{*}\] \[=\frac{F_{n,2}-F_{i,2}}{c_{n}-c_{i}}\max_{i\in[n-1]}\frac{c_{n}-c _{i}}{F_{n,2}-F_{i,2}}\] \[\geq 1\]

And therefore \(t_{j}^{*}\) is feasible in the primal problem, and also optimal by strong LP duality. Also note that the resulting contract coincides exactly with the optimal min-pay contract as attained by Dutting et al. (19, Lemma 7). 

#### b.7.3 General contracts with MLRP

In this section, we explore min-budget contracts in MLRP settings with more than two actions and more than two outcomes. We start with a negative example, showing that the MLRP assumption is not sufficient for establishing the optimality of threshold contracts:

**Claim 14**.: _For \(|\Omega|>2\), there exists a design problem satisfying MLRP for which the optimal contract is not a threshold._

Proof.: Consider the following contract design problem:

\[F_{1} \sim\mathrm{Binomial}(10,0.5) c_{1} =0\] \[F_{2} \sim\mathrm{Binomial}(10,0.65) c_{2} =0.45\] \[F_{3} \sim\mathrm{Binomial}(10,0.8) c_{3} =1\]The distributions are binomial, and therefore the contract satisfies MLRP. Numerically solving Eq. (11) yields the following fractional contract:

\[t_{\mathrm{LP}}^{*}=(0,0,0,0,0,0,0,1.1,1.46,1.46,1.46)\]

Numerically solving Eq. (11) while imposing integer constraints \(\phi_{j}\in\{0,1\}\) yields the following contract, which has higher max payout:

\[t_{\mathrm{IP}}^{*}=(0,0,0,0,0,0,1.51,1.51,1.51)\]

Thus for this case any threshold contract is min-budget suboptimal. A graphical illustration of the proof is provided in Fig. 7. 

**Remark 2**.: _The proof of Claim 14 is provided with \(10\) outcomes (\(|\Omega|=10\)) for ease of graphical interpretation, however a minimal counterexample may also be constructed using only \(3\) outcomes. For example, consider the following design problem:_

\[F_{1} =(0.5,0.3,0.2) c_{1} =0\] \[F_{2} =(0.3,0.4,0.3) c_{2} =0.45\] \[F_{3} =(0.1,0.35,0.55) c_{3} =1\]

_The proof for this case follows in the same way, where numerical computation yields the contracts:_

\[t_{\mathrm{LP}}^{*} =(0,1.9,2.6)\] \[t_{\mathrm{IP}}^{*} =(0,2.7,2.7)\]

**Remark 3**.: _We also note that a counterexample with \(2\) outcomes is not possible due to Claim 13._

The negative example shows that even with MLRP, the optimal contract incentivizing the highest implementable action needs to rule out deviations of the agent to all other actions, and not just to the second-highest one. This makes the contract complex. We identify a natural economic condition that is sufficient for considering only a single deviation (to the second-highest action), resulting in a simple contract. Note that considering a single such deviation is equivalent to restricting the action space to actions \(\{n-1,n\}\).

### Sufficiency condition for more than two actions

For the following definition, recall the definition of MLR crossing point \(j^{*}\) (Def. 13), and survival function \(\mathbb{S}\) (Def. 14):

**Definition 15** (Concave-MLRP; Formal statement of Definition 2).: _For a contract design setting satisfying MLRP, denote by \(j^{*}=j^{*}_{F_{n},F_{n-1}}\) the crossing point of the outcome distribution corresponding to the highest action \(F_{n}\), and the outcome distribution corresponding to the second-highest action \(F_{n-1}\). For any action \(i\in[n]\), the crossing-point survival \(s_{i}\) is defined as:_

\[s_{i}=\mathbb{S}_{F_{i}}(j^{*}-1)\]

Figure 7: Graphical illustration of Claim 14. **(Left)** Outcome distributions \(F_{1},F_{2},F_{3}\) and MLR crossing point \(j^{*}_{F_{2},F_{3}}\). **(Center)** The min-budget contract \(t^{*}_{j}\), given by numerically solving Eq. (11) (purple), and numerically solving Eq. (11) while imposing integer constraints \(\phi_{j}\in\{0,1\}\) (cyan). Note that the fractional LP solution achieves lower max payout. **(Right)** Crossing-point survival \(s_{i}=\mathbb{S}_{F_{i}}(j^{*}_{F_{i},F_{n}}-1)\) as a function of cost \(c_{i}\) (see Definition 15). Note that the function is not concave, thus the sufficient condition given in Theorem 4 does not hold in this case.

In words, \(s_{i}\) is the probability to get an outcome at or above crossing-point \(j^{*}\) according to outcome distribution \(F_{i}\). From an economic perspective, for every action \(i\in[n]\), \(s_{i}\) is the agent's probability to receive any nonzero payment from choosing action \(i\), if the contract is designed by restricting the action-space to actions \(\{n-1,n\}\). Note however that the definition of \(s_{i}\) only depends on the outcome distribution \(F_{ij}\), and does not require solving the contract design problem.

#### b.8.1 Binomial power-law curves satisfy Concave-MLRP

Recent theoretical work on learning curves has focused mainly on power-law expected learning curves of the form \(a_{n}=1-\beta n^{-\gamma}\)[11, 33]. In addition, these curves were also found to provide a good fit for certain practical applications [49, POW2]. In this section, we show that a stochastic generalization of these curves satisfies the Concave-MLRP property:

**Definition 16** (Power-law stochastic learning curve).: _Let \(\beta,\gamma\in\mathbb{R}_{\geq 0}\), and \(m\in\mathbb{N}\). A delegated learning setting with actions \(\mathcal{A}\) has a realizable power-law stochastic learning curve with parameters \(\beta,\gamma\) if \(F_{i}=\mathrm{Binomial}\left(1-\beta n_{i}^{-\gamma},m\right)\) for all \(n_{i}\in\mathcal{A}\)._

For the proof, we also recall the definition of the regularized incomplete beta function:

**Definition 17** (Regularized incomplete beta function).: _For \(k_{1},k_{2}\geq 1\), the regularized incomplete beta function is defined as:_

\[I_{p}(k_{1},k_{2})=\frac{\int_{0}^{p}t^{k_{1}-1}(1-t)^{k_{2}-1}\mathrm{d}t}{ \int_{0}^{1}t^{k_{1}-1}(1-t)^{k_{2}-1}\mathrm{d}t}\]

We also recall that \(I_{p}\) is related to the survival function of the binomial distribution [e.g., 1, 6.6.4]:

\[I_{p}(k+1,n-k)=\mathbb{P}_{x\sim\mathrm{Binomial}(p,n)}[x>k]\]

**Claim 15**.: _Let \(\beta,\gamma\in\mathbb{R}_{\geq 0}\), \(m\in\mathbb{N}\). A delegated learning setting with a power-law stochastic learning curve \(F_{i}=\mathrm{Binomial}(1-\beta n_{i}^{-\gamma},m)\), action costs \(c_{i}=n_{i}\), and \(\min_{i}n_{i}\geq\left(\nicefrac{{\beta(m+\gamma^{-1})}}{{1+\gamma^{-1}}} \right)^{\frac{1}{\gamma}}\) satisfies the Concave-MLRP assumption._

Proof.: Denote the expected accuracy of each action by \(a_{n}=1-\beta n^{-\gamma}\). The survival function of the binomial distribution is given by:

\[s_{n}=I_{a_{n}}\;(j^{*},m+1-j^{*})\]

Where \(I_{a_{n}}\) is the regularized incomplete beta function (Definition 17). With slight abuse of notation, we treat \(a_{n}\) and \(s_{n}\) as continuous functions of \(n\). Taking the derivative by \(n\) and ignoring the constant denominator, we obtain:

\[\frac{\mathrm{d}s_{n}}{\mathrm{d}n}\propto a_{n}^{j^{*}-1}(1-a_{n})^{m-j^{*}} \frac{\mathrm{d}a_{n}}{\mathrm{d}n}\]

Plugging the functional form of \(a_{n}\):

\[\frac{\mathrm{d}s_{n}}{\mathrm{d}n} \propto\left(1-\beta n^{-\gamma}\right)^{j^{*}-1}\left(\beta n^{- \gamma}\right)^{m-j^{*}}\beta\gamma n^{-(\gamma+1)}\] \[\propto\left(\beta n^{-\gamma}\right)^{m-j^{*}+1+\gamma^{-1}} \left(1-\beta n^{-\gamma}\right)^{j^{*}-1}\]

As a function of \(\beta n^{-\gamma}\), the function \(\frac{\mathrm{d}s_{n}}{\mathrm{d}n}\) attains its extremum at:

\[\beta\tilde{n}^{-\gamma}=\frac{m+1-j^{*}+\gamma^{-1}}{m+\gamma^{-1}}\]

\[\tilde{n}=\left(\frac{\beta(m+\gamma^{-1})}{m+1-j^{*}+\gamma^{-1}}\right)^{ \frac{1}{\gamma}}\]

And therefore the function \(s_{n}\) has an inflection point at \(\tilde{n}\) (see Fig. 8). From the upper bound \(j^{*}\leq m\) we obtain:

\[\tilde{n}\leq\left(\frac{\beta(m+\gamma^{-1})}{1+\gamma^{-1}}\right)^{\frac{ 1}{\gamma}}=n_{0}\]

And as \(\min_{i}n_{i}\geq n_{0}\), the function \(s_{n}\) is convex as a function of \(c_{n}\) for all \(n\in\mathcal{A}\). 

The proof is illustrated in Fig. 8.

#### b.8.2 Proof of sufficiency theorem

**Theorem 6** (Concave-MLRP implies threshold contracts; formal statement of Theorem 4).: _For a contract design problem satisfying MLRP, consider \(s_{i}\) as a function of cost \(c_{i}\). If this function is concave, then the optimal contract is a threshold contract. Furthermore, the contract is successfully recovered by the SBA algorithm._

Proof.: To prove this claim, we construct a relaxed version of the min-budget LP (Eq. (6)), and apply Theorem 5 in order to solve it. We then show that this solution is also feasible for the original LP.

Given the min-budget LP, construct a relaxed LP by eliminating (IC) constraints for all \(i\leq n-2\):

\[\min_{t\in\mathbb{R}_{\geq 0}^{[\mathsf{II}]},B\in\mathbb{R}_{ \geq 0}}B\] (36) \[\mathrm{s.t.}\] \[\forall j\in\Omega:t_{j}\leq B\] \[\sum_{j\in\Omega}F_{n-1,j}t_{j}-c_{n-1}\leq\sum_{j\in\Omega}F_{n,j}t_{j}-c_{n}\]

Eq. (36) only depends on \(F_{n}\) and \(F_{n-1}\), and therefore claim 12 can be applied to obtain the optimal min-budget contract:

\[t_{j}^{*}=\frac{c_{n}-c_{n-1}}{\left\|F_{n}-F_{n-1}\right\|_{\mathrm{TV}}} \mathds{1}\left[F_{n,j}\geq F_{n-1,j}\right]\] (37)

Let \(j^{*}=\min\left\{j\in\left\{0,\ldots,m\right\}\mid F_{n,j}\geq F_{n-1,j}\right\}\) as in Definition 13, and denote \(s_{i}=\mathbb{S}_{F_{i}}(j^{*}-1)\). By claim 11, we obtain:

\[\left\|F_{n}-F_{n-1}\right\|_{\mathrm{TV}}=s_{n}-s_{n-1}\]

For all \(i<n\), denote:

\[b_{i}=\frac{c_{n}-c_{i}}{s_{n}-s_{i}}\]

Using the definition of \(b_{i}\), Eq. (37) can be rewritten as:

\[t_{j}^{*}=b_{n-1}\mathds{1}\left[j\geq j^{*}\right]\]

By assumption, \(s_{i}\) is a concave function of \(c_{i}\), and therefore the function \(b_{i}\) is monotonically non-decreasing for all \(i<n\):

\[b_{n-1}\geq b_{i}\]

Dividing both sides by \(b_{i}\), we obtain:

\[\frac{b_{n-1}}{b_{i}}\geq 1\]

Figure 8: Graphical intuition for the sufficiency condition in Claim 15. **(Left)** Expected accuracy curve for MNIST-784 MLP. Blue dots represent empirical data from the LCDB dataset, cyan curve represents power-law fit (\(a_{n}=1-\beta n^{-\gamma}\), with \(\hat{\beta}=1.89\), \(\hat{\gamma}=0.33\)). **(Center)** Crossing point survival \(s_{i}\) as a function of cost \(c_{i}\) (see Definition 15), for \(m=30\). Cyan dot represents inflection point \(\tilde{n}\approx 2022\). Red vertical line represents the inflection point bound \(n_{0}\approx 3957\) suggested by the proof. It can be observed that the curve is concave for all \(n>\tilde{n}\). **(Right)** Second derivative of the survival function \(s_{n}\), illustrating the position of the bound \(n_{0}\) in relation to the inflection point \(\tilde{n}\).

Plugging in the definition of \(b_{i}\), and multiplying both sides by \((c_{n}-c_{i})\):

\[b_{n-1}\left(s_{n}-s_{i}\right)\geq c_{n}-c_{i}\] (38)

By definition of \(t_{j}^{*}\), the expected payout of contract \(t_{j}^{*}\) under action \(i^{\prime}\in[n]\) is given by:

\[\sum_{j=0}^{m}t_{j}^{*}F_{i^{\prime},j}=b_{n-1}\sum_{j=j^{*}}^{m}F_{i^{\prime},j}=b_{n-1}s_{i^{\prime}}\] (39)

Plugging Eq. (38) into Eq. (39) yields:

\[\sum_{j=0}^{m}t_{j}F_{n,j}-\sum_{j=0}^{m}t_{j}F_{i,j}\geq c_{n}-c_{i}\] (40)

As Eq. (40) is identical to the (IC) constraint in Eq. (6), we obtain that \(t_{j}^{*}\) satisfies all the (IC) constraints in the original LP. From this we conclude that \(t_{j}^{*}\), which is a threshold contract, is also an optimal contract with the respect to the full min-budget LP.

For the second part of the claim, note that the SBA algorithm also attempts to solve Eq. (36) on the iteration corresponding to action \(N-1\). As the solution of Eq. (36) is guaranteed to be feasible for the original LP under Concave-MLRP, the SBA successfully recovers it. 

### Min-budget contracts with guaranteed minimum payout

Our problem setting assumes that the agent selects its action rationally, as described in Eq. (2). However, in some practical settings the agent may be riske to some extent, and require guaranteed minimum payment from the contract. In this section, we show that the rationality assumption is made without loss of generality in such cases: Lemma 5, which we prove below, shows that any optimal contract with guaranteed minimum payout can be represented as the sum of a min-budget contract without guaranteed payout, and a constant representing the payout guarantee.

**Definition 18** (Guaranteed minimum payout).: _Let \(\delta\geq 0\). A contract \(t\in\mathbb{R}_{\geq 0}^{|\Omega|}\) has guaranteed payout of size \(\delta\) if \(t_{j}\geq\delta\) for all \(j\in\Omega\)._

**Lemma 5**.: _A contract \(t\) is a min-budget contract with guaranteed payout \(\delta\) requiring budget \(B\) if any only if there exist a min-budget contract \(t^{\prime}\) (without guaranteed payout) requiring budget \(B^{\prime}\) such that \(t=t^{\prime}+\delta\) and \(B=B^{\prime}+\delta\)._

Proof.: When agents require a guaranteed minimum payout \(\delta\geq 0\), we add a constraint to the MIN-BUDGET linear program defined in Eq. (6), such that an optimal min-budget contract \(t\) with

Figure 9: Graphical intuition for the sufficiency condition in Theorem 4 (restated in Theorem 6). **(Left)** Outcome distribution in a contract design setting satisfying MLRP, generated by a series of binomial distributions with power-law expectation \(\operatorname{acc}_{D}(h_{n})=0.9-0.4\left(\frac{n}{100}\right)^{-0.3}\). The gray dotted line represents the MLR crossing point \(j_{F_{4},F_{6}}^{*}\) (see Definition 13). **(Center)** Crossing point survival \(s_{i}\) as a function of cost \(c_{i}\) (see Definition 15). It can be observed that the curve is concave. **(Right)** The min-budget contract implementing action \(5\). It is a threshold contract, as guaranteed by Theorem 4. Compare to Fig. 7, where the sufficiency condition does not hold.

guaranteed payout \(\delta\) is an optimal solution to the following linear program:

\[\min_{t\in\mathbb{R}^{[\Omega]}_{\geq 0},B\in\mathbb{R}_{\geq 0}}B\] (41) \[\mathrm{s.t.}\] \[\forall j\in\Omega:t_{j}\leq B\] (BUDGET) \[\forall i^{\prime}\neq i:\sum_{j\in\Omega}F_{i^{\prime},j}t_{j}-c_ {i^{\prime}}\leq\sum_{j\in\Omega}F_{i,j}t_{j}-c_{i}\] (IC) \[\forall j\in\Omega:t_{j}\geq\delta\] (MINWAGE)

To prove the first direction of the equivalence, assume that \(t\) is a min-budget contract with guaranteed payout \(\delta\) and budget \(B\), and thus an optimal solution of Eq. (41). Define the variable transformation:

\[t^{\prime} =t-\delta\] (42) \[B^{\prime} =B-t_{0}\]

By Eq. (42), the (BUDGET) constraint in Eq. (41) transforms into:

\[\forall j\in\Omega:t_{j}\leq B\] (43) \[\Leftrightarrow t^{\prime}_{j}+\delta\leq B^{\prime}+\delta\] \[\Leftrightarrow t^{\prime}_{j}\leq B^{\prime}\]

The (IC) constraint transforms into:

\[\forall i^{\prime}\neq i: \sum_{j\in\Omega}F_{i^{\prime},j}t_{j}-c_{i^{\prime}}\leq\sum_{j \in\Omega}F_{i,j}t_{j}-c_{i}\] (44) \[\Leftrightarrow\sum_{j\in\Omega}F_{i^{\prime},j}(t^{\prime}_{j}+ \delta)-c_{i^{\prime}}\leq\sum_{j\in\Omega}F_{i,j}(t^{\prime}_{j}+\delta)-c_{i}\] \[\Leftrightarrow\sum_{j\in\Omega}F_{i^{\prime},j}t^{\prime}_{j}-c_ {i^{\prime}}+\delta\underbrace{\sum_{j\in\Omega}F_{i^{\prime},j}}_{=1}\leq \sum_{j\in\Omega}F_{i,j}t^{\prime}_{j}-c_{i}+\delta\underbrace{\sum_{j\in \Omega}F_{i,j}}_{=1}\] \[\Leftrightarrow\sum_{j\in\Omega}F_{i^{\prime},j}t^{\prime}_{j}-c_ {i^{\prime}}\leq\sum_{j\in\Omega}F_{i,j}t^{\prime}_{j}-c_{i}\]

And the (MINWAGE) constraint transforms into:

\[\forall j\in\Omega: t_{j}\geq\delta\] (45) \[\Leftrightarrow t^{\prime}_{j}\geq 0\]

Plugging back the transformed constraints (44, 43, 45) into Eq. (41), we obtain that \((t^{\prime},B^{\prime})\) is an optimal solution of the original MIN-BUDGET linear program Eq. (6), and therefore \(t^{\prime}\) is an optimal min-budget contract without minimum guaranteed payout.

Conversely, assume that \(t^{\prime}\) is an optimal min-budget contract (without minimum guaranteed payout), satisfying the MIN-BUDGET linear program in Eq. (17) with budget \(B^{\prime}\). Apply the inverse transformation \(t=t^{\prime}+\delta\) and \(B=B^{\prime}+\delta\), and the equivalence relations (44, 43, 45) in the inverse direction to obtain that \(t\), \(B\) is an optimal solution to Eq. (41). 

## Appendix C Experimental details

### Data

Lcdb.We base our main experimental environment on the LCDB dataset [43], which includes a large collection of stochastic learning curves for multiple learning algorithms and classification benchmark datasets. For each learning method and benchmark dataset, the database includes held-out accuracy measurements, obtained for exponentially-increasing training set sizes \(n\in\left\{2^{4}=16,\ldots,\mathrm{round}(2^{k/2}),\ldots,2^{15}=32,768\right\}\), with multiple repetitions per \(n\) obtained by cross-validation. For all learning curves we consider in our analysis, and for each \(n\), the number of repetitions provided in the dataset is in the range \(R\in\{5,\ldots,125\}\), where the specific number of repetitions in LCDB depends on the algorithm and benchmark dataset (see the dataset documentation for additional details). For each trained classifier, each accuracy point on the learning curve is estimated using 5,000 held-out samples. Formally, and using our notation, for each learning algorithm Alg (e.g. MLP), dataset \(D\) (e.g. MNIST), and training set size \(n\), LCDB provides a set of accuracy estimates \(\left\{a_{n}^{1,\mathrm{Alg},D},\ldots,a_{n}^{R,\mathrm{Alg},D}\right\}\), such that each \(a_{n}\) is distributed according to \(a_{n}\sim\mathrm{acc}_{D}(h_{n})\), where \(h_{n}\) is a classifier trained using training set \(S\sim D^{n}\) and learning algorithm \(\mathrm{Alg}\) (see Section 2 for definition of \(\mathrm{acc}_{D}(h_{n})\)).

Benchmark dataset and algorithms.In our main analysis, we focus on the popular MNIST dataset [39, OpenML 554], and on multilayer perceptrons (MLP) and gradient-boosted decision trees (GBDT) as representative classifiers. For all classifiers, results are obtained for the respective default scikit-learn[44] implementations (e.g. MLPClassifier and GradientBoostingClassifier for MLP and GBDT, respectively).

Delegated learning settings from empirical data.For a given validation set size \(m\) where \(m=|V|\), we instantiate a contract design task \((\mathcal{A},c,\Omega,F)\) as follows: (i) the set of actions with the set of training set sizes provided by LCDB (\(\mathcal{A}=\left\{2^{4},2^{4.5},\ldots,2^{15}\right\}\)); (ii) action costs are set to fixed per-unit cost, i.e., \(c_{n}=n\); and (iii) the distribution \(F\) over outcomes \(\Omega\) is associated with a binomial mixture distribution, resulting from applying bootstrap sampling to empirical error measurements:

\[f_{n}=\frac{1}{R}\sum_{r=1}^{R}\mathrm{Binomial}(m,a_{n}^{r,\mathrm{Alg},D})\]

where \(a_{n}\) are the accuracy estimates defined above. Fig. 2 (Left) shows an example of such a setting, obtained by applying the above procedure to data describing learning curves for the MLP algorithm trained on the MNIST benchmark dataset.

### Implementation details

Code.We implement our code in Python. Our code relies on Pyomo[13, 27] and GLPK for solving linear and mixed-integer programs.

Code is available at: https://github.com/edensaig/delegated-classification.

Hardware.All experiments were run on a single laptop, with 16GB of RAM, M1 Pro processor, and with no GPU support.

Runtime.A single run consisting the entire pipeline takes roughly 5 minutes. The main bottleneck is running the LP solvers, taking roughly 70% of runtime to compute.

#### c.2.1 Contract design solvers

To find the optimal contracts, we implement and compare several different solvers:

* **LP solver**: To find min-budget contracts given outcome distributions \(\{f_{n}\}\) and costs \(\{c_{n}\}\), we solve the MIN-BUDGET LP (Eq. (6)) directly using Pyomo and GLPK. Given budget \(B\), budget-optimal contracts are obtained by iterating through the actions set \(\mathcal{A}\), invoking the min-budget solver on each target action \(n\in\mathcal{A}\), enforcing incentive compatibility against all actions \(n^{\prime}\) which satisfy \(\alpha_{n^{\prime}}<\alpha_{n}\), and taking the action yielding the maximal expected accuracy within budget (see Appendix B.1). In our code, the LP solver is implemented within the MinBudgetContract class. Typical running time for a single problem instance: \(10^{-1}\)s.
* **SBA solver**: Implementation of the single binding action algorithm presented in Section 3.4. The local solver is implemented within the MinBudgetSingleBindingConstraintContract class. Typical running time for a single problem instance: \(10^{-4}\)s.
* **Hybrid solver**: A meta-solver implementing the 'try SBA first' computational approach. The solver starts by invoking the SBA solver, and applies the LP solver if the former fails. In our code, the LP solver is implemented within the MinBudgetHybridContract class. Running time depends on whether SBA is applicable.
* **MIP solver**: To find optimal all-or-nothing contracts, we solve the MIN-BUDGET LP in its statistical formulation (Eq. (11)) while restricting \(\phi_{j}\in\{0,1\}\). This turns the LP into a mixed-integer program, which can also be solved using GLPK. In our code, the IP solver is implemented within the MinBudgetStatisticalContract class. Typical running time for a single problem instance: \(10^{-1}\)s.

* **Min-pay LP solver**: To compare between min-budget and min-pay contracts (Appendix B.3), min-pay contracts were obtained by solving the MIN-PAY LP (Eq. (17)) using Pyomo and GLPK, similar to the full min-budget LP solver. In our code, the min-pay is implemented within the MinPayContract class. Running time is similar to the other LP-based methods.
* **Full enumeration solver**: To find all-or-nothing contracts for low-dimensional problems and avoid numerical instabilities, we implement a solver which performs full enumeration of all-or-nothing contracts. By the statistical variable transformation in Definition 3, the (IC) constraint in the MIN-BUDGET LP translates to \(\sum_{j\in\Omega}\left(F_{i,j}-F_{i^{\prime},j}\right)\phi_{j}\leq(c_{i}-c_{i^ {\prime}})\beta\), where \(i\) is the target action, \(i^{\prime}\in\mathcal{A}\setminus\{i\}\), and the objective is minimize \(\beta\) (see Eq. (11)). Thus, when \(\phi\in\{0,1\}^{\Omega}\) is fixed, the optimal \(\beta\) is given by: \[\beta^{*}(\phi)=\min_{i^{\prime}\in\mathcal{A}\setminus\{i\}}\frac{\sum_{j\in \Omega}\left(F_{i,j}-F_{i^{\prime},j}\right)\phi_{j}}{c_{i}-c_{i^{\prime}}}\] and optimizing \(\beta^{*}\) over \(\phi\in\{0,1\}^{\Omega}\) yields an optimal all-or-nothing contract. The full enumaration solver is implemented within the FullEnumerationContract class. For for enumeration of all-or-nothing contracts, this solver is mostly applicable for small problem instances (\(m<20\)) due to its exponential complexity. In contrast, for threshold contracts the number of possible \(\phi\) configurations is linear in \(m\), and therefore enumeration is more efficient.
* **Fixed-budget solver**: To find budget-optimal threhsold contracts given a fixed budget \(B\), we implement a simple solver which iterates through all possible threshold contracts \(t_{j_{0}}(j)=B\mathds{1}\left[j\geq j_{0}\right]\) for all \(j_{0}\in\{0,\ldots,m\}\). The solver then simulates the agent's rational choice (Eq. (2)) and selects the value of \(j_{0}\) which incentivizes the best action. In case of ties between possible values of \(j_{0}\), they are broken in favor of larger values, as this was shown to lead to greater numerical stability. The fixed-budget solver is implemented within the FixedBudgetThresholdContract class.

### Empirical prevalence estimation

Since our theoretical analysis relies on MLRP assumptions, we would like to understand whether the results are applicable to real-world learning curves. Towards this, we run an empirical prevalence evaluation on the MNIST dataset. For each learning algorithm \(\mathrm{Alg}\), and training set size \(n\), we construct a delegated learning setting with \(m=10\) and collect the following statistics:

* Structural properties:
* **Is MLRP?*
* (% MLRP): Check whether all pairs \(n_{1},n_{2}\leq n\) such that \(c_{n_{1}}<c_{n_{2}}\) satisfy the monotone likelihood ratio property (see Definition 1).
* Computational properties:
* **SBA successful?*
* (% SBA): Check whether the SBA algorithm was successful on the given instance.
* Functional form of optimal contract:
* **Is monotone?*
* (% M): Check whether the resulting min-budget contract satisfies \(t_{j}\geq t_{j-1}\) for all \(j\in[m]\).
* **Is all or nothing?*
* (% AoN): Check whether the resulting min-budget contract is an all-or-nothing contract (i.e. whether there exists \(j_{0}\in\Omega\),\(B>0\) such that \(t_{j}=0\) for all \(j<j_{0}\) and \(t_{j}=B\) otherwise).
* **Is threshold?*
* (% T): Check whether the resulting min-budget contract is a threshold contract (i.e. whether there exists \(j_{0}\in\Omega\),\(B>0\) such that \(t_{j}=0\) for all \(j<j_{0}\) and \(t_{j}=B\) otherwise).
* Excess cost:
* **Min-budget**: Optimal objective \(B_{\mathrm{LP}}\) of MIN-BUDGET LP without additional restrictions, implemented using the full LP solver.
* **All-or-nothing budget**: Optimal objective of MIN-BUDGET LP, when the resulting contract is restricted to all-or-nothing form \(t_{j}\in\{0,B_{\mathrm{AoN}}\}\). Implemented using the full enumeration solver.
* **Threshold budget**: Optimal objective of MIN-BUDGET LP, when the resulting contract is restricted to threshold form \(t_{j}=B_{\mathrm{Thr}}\mathds{1}\left[j\geq j_{0}\right]\). Implemented using the full enumeration solver.
* The excess cost columns in Table 2 indicate the relative excess cost incurred by restricting the min-budget optimization space to simple contracts (\(\frac{(B_{\{\text{LaN},\text{Thr}\}})-B_{\text{Lp}}}{B_{\text{Lp}}}\)). As all-or-nothing contracts are a superset of threshold contracts, we expect this excess cost to be smaller than the one associated with threshold contracts. The averaging in table 2 is performed on problem instances where both all-or-nothing and threshold contracts are feasible.

Averaging over all implementable actions, we obtain the results in Table 2. The results show that threshold contracts are relatively common in this dataset (more than 85%), and that excess cost of simple contracts is relatively low (around 1%), indicating that threshold contracts may provide good approximation to the optimal min-budget contracts in some cases. All simple optimal contracts in our dataset had a threshold functional form, and some simple optimal contracts were not recovered by SBA, indicating that a tighter characterization of simple contracts may be possible.

Fig. 10 provides qualitative interpretation of the analysis for a selected subset of learners. While the survival functions are not perfectly concave (and thus don't satisfy Concave-MLRP), the SBA algorithm successfully terminates in three cases (MLP, Logistic, KNN). The binding action is \(a_{N-1}\) for two of the classifiers (MLP, KNN), similar to the condition implied by Theorem 4. Note that SBA also terminated successfully on Logistic Regression data, despite the learning curve having a distinctive non-concave shape. In the case of GBDT, the survival function is not concave, and the optimal contract does not assume a threshold form.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & Structure & Compute & \multicolumn{3}{c}{Functional form of opt. contract} & \multicolumn{2}{c}{Excess cost} \\ \cline{2-7}  & \% MLRP & \% SBA & \% M & \% AoN & \% T & AoN & T \\ \hline
**MLP** & 100\% & 94.4\% & 100\% & 94.4\% & 94.4\% & 0.04\% & 0.04\% \\
**GBDT** & 100\% & 76.5\% & 100\% & 82.4\% & 82.4\% & 0.71\% & 0.71\% \\
**Logistic** & 93.8\% & 81.2\% & 93.8\% & 93.8\% & 93.8\% & 0.00\% & 0.00\% \\
**Perceptron** & 68.8\% & 75\% & 93.8\% & 93.8\% & 93.8\% & 0.00\% & 0.00\% \\
**Linear SVM** & 71.4\% & 78.6\% & 85.7\% & 85.7\% & 85.7\% & 0.95\% & 1.45\% \\
**Poly SVM** & 100\% & 94.4\% & 100\% & 94.4\% & 94.4\% & 0.24\% & 0.24\% \\
**RBF SVM** & 100\% & 94.4\% & 100\% & 94.4\% & 94.4\% & 0.04\% & 0.04\% \\
**KNN** & 100\% & 100\% & 100\% & 100\% & 100\% & 0.00\% & 0.00\% \\ \hline
**Overall** & 92.6\% & 87.4\% & 97\% & 92.6\% & 92.6\% & 0.22\% & 0.26\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: Empirical robustness estimation on the MNIST dataset. Each row corresponds to a learning algorithm, and columns are specified in Appendix C.3. Results are averaged across all implementable actions.

Figure 10: Qualitative comparison of optimal contract computation on LCDB MNIST data (\(n^{*}=16384\), \(m=25\)). Each row represents a learning algorithm. **(Left column)** Expected accuracy curves \(\alpha_{n}\). **(Center column)** Survival function at transition \(s_{i}\), whenever SBA is successful, the binding actions are marked with triangles. **(Right column)** Optimal contract \(t_{j}^{*}\).