[MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_FAIL:2]

A plausible root of this challenge could be the absence of structurally represented knowledge. Predicated predominantly on associative training, these agents are versed in correlating traces with responses without genuinely internalizing holistic world models. In sharp contrast, humans seamlessly navigate abductive reasoning by forecasting potential trajectories leading to a perceived outcome. This intricate dance gradually transmutes from abductive to deductive reasoning, where humans harness their innate understanding of causality to deduce and mirror observed patterns. In our pursuit to mirror this quintessential human trait, we integrate Abduction from Deduction (AfD) into \(\mathbb{C}\)**Conan** via a Bayesian approach. Experimental results underscore the efficacy of AfD, indicating a substantial avenue for bolstering agent adeptness in \(\mathbb{C}\)**Conan**.

To sum up, our work makes the following three contributions:

* We usher in the novel domain of _active reasoning_, underscoring the indispensable roles of active exploration and iterative inference in abductive reasoning. This paradigm shift transforms traditional single-round passive question-answering paradigms into a more immersive format, compelling agents to actively engage with the environment to procure pivotal evidence.
* We introduce \(\mathbb{C}\)**Conan**, a new environment tailored to evaluate the abductive reasoning ability of current machine learning models within dynamic settings. \(\mathbb{C}\)**Conan** surpasses its predecessors that hinge on step-by-step deductive reasoning, revealing the limitations of present-day models.
* We formulate a new learning method for abduction, AfD, grounded in Bayesian principles. This framework elegantly reformulates abduction into deduction, proving instrumental in navigating the complex active reasoning challenges posed by \(\mathbb{C}\)**Conan**.

## 2 Related Work

Machine Abductive ReasoningAbductive reasoning, foundational to human cognition, is crucial for scientific exploration, decision-making, and problem-solving (Perice, 1965; Magnani, 2011). In the Artificial Intelligence (AI) landscape, there is a rich history of efforts to equip machines with this ability, where they use prior knowledge and sparse observations to hypothesize amidst uncertainty (Josephson and Josephson, 1996; Xu et al., 2023). Key developments span logic-based abduction (Kakas et al., 1992; Poole, 1993) and hybrid neural-symbolic methods (Rocktischel and Riedel, 2017; Zhang et al., 2021b; Li et al., 2022, 2023). With computational progress, Large Language Models (LLMs) have effectively addressed several challenges through text generation, exhibiting outstanding performance (Brown et al., 2020; OpenAI, 2023; Thoppilan et al., 2022). Modern research usually frames abductive reasoning within natural language understanding (Bhagavatula et al., 2019) or multimodal vision-language integration (Hessel et al., 2022; Liang et al., 2022). However, there is still a notable gap: many benchmarks lean heavily on deduction, sidelining abduction's interactive essence. Our work addresses this gap, emphasizing the core of _active reasoning_ in abductive contexts.

Embodied Question AnsweringEmbodied question answering enhances traditional Visual Question Answering (VQA) by placing agents in interactive environments (Johnson et al., 2017; Das et al., 2018; Gordon et al., 2018; Yu et al., 2019). In \(\mathbb{C}\)**Conan**, agents actively explore to gather data, preparing them to solve abductive questions based on partial information. Unlike standard embodied question-answering frameworks (Das et al., 2018; Gordon et al., 2018; Yu et al., 2019), where questions become simple instructions for agents, \(\mathbb{C}\)**Conan** introduces complexity: (i) its questions, rooted in high-level intent and goals, resist simple decomposition into a series of actions; (ii) agents in \(\mathbb{C}\)**Conan** act as detectives, constantly hypothesizing from observations and prior knowledge, and iterating their strategies in light of new data. For a comprehensive comparison of \(\mathbb{C}\)**Conan** with other benchmarks, see Tab. 1.

## 3 The \(\mathbb{C}\)**Conan Environment

\(\mathbb{C}\)**Conan** is crafted as an interactive question-answering environment aimed at evaluating a machine's active abductive reasoning capacity, as depicted in Fig. 1. Building on the foundation of the Crafter (Hafner, 2021), \(\mathbb{C}\)**Conan** evolves into a detective game featuring two agents: the _vandal_ and the _detective_. The gameplay kickstarts with the _vandal_ undertaking a randomly designated task, leaving behind traces for the _detective_ to unravel. Subsequently, given these traces, pertinent queries are generated. Finally, the _detective_ is spawned in the environment, tasked with navigating these traces and actively probing the environment, all to derive answers through abductive reasoning.

### Basic Components

PlaygroundOriginating from the Crafter playground, \(\mathbb{C}_{\textbf{can}}\) operates within a \(64\times 64\) grid matrix. Agents navigate this space with a localized \(9\times 9\) grid field of view centered on their current position. Once the _detective_ is created in the environment, all traces left behind by the _vandal_ persist, serving as clues for the _detective_ to unravel. While pivotal studies Johnson et al. (2016); Fan et al. (2022); Cai et al. (2023); Wang et al. (2023) address perception in 3D Minecraft settings using foundational models, our emphasis is on honing active abductive reasoning. To this end, we transition from a 3D visual perception to a 2D plane, ensuring a harmonious blend of reduced visual complexity and retaining rich interactivity Xie et al. (2021).

Items and Actions\(\mathbb{C}_{\textbf{can}}\) offers an extensive assortment of interactive items: food, materials, mobs, and tools, each tied to specific actions, as illustrated in Fig. 2. It furnishes 26 unique actions to foster agent-environment engagement. Certain actions leave traces, and together, the items and their mechanics provide a rich set of affordances for agents in the playground. This knowledge about item operations and traces aids the _detective_ in comprehending the incident scene. Advancing from its predecessor, the original Crafter, \(\mathbb{C}_{\textbf{can}}\) now boasts over 30 achievements, a significant rise of over 50%. It features 32 distinct traces covering all agent actions such as crafting, collecting, defeating, eating, drinking, and incurring injuries. This enhancement enables the design of 60 varied abductive reasoning tasks within the scene. For an in-depth overview of the playground, refer to Appx. A.

VandalEach \(\mathbb{C}_{\textbf{can}}\) map starts with the initialization of a _vandal_. This agent is driven by two primary aims: executing a specific task and preserving its existence within the environment. It is noteworthy that external threats might terminate the _vandal_ prematurely. Traces left in the aftermath of the _vandal_'s activities form the question foundation for the _detective_, with every trace potentially birthing several questions. For a detailed overview, see Sec. 3.2. We model the _vandal_ as optimal: when given a random task and the full map, it strategically delineates a sequence of subgoals based on the task dependency graph, all while ensuring its survival. In scenarios with multiple viable paths to an objective, uniform sampling comes into play. This sampling, supported by a probabilistic parser, presents varied strategies for task completion. Hence, the _detective_ must delve deeper to distinguish the actual sequence of events from possible decoys. The execution of the _vandal_'s individual actions, as per the planned subgoal sequence, is steered by a collection of pre-established policies.

DetectiveAfter generating questions from a given trace, a _detective_ is spawned to answer them. Traces left by the _vandal_ span multiple steps and are only partially observable within the _detective_'s \(9\times 9\) grid field of view. This requires the _detective_ to actively interact with the environment and gather evidence to answer the questions.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Benchmark & Format & Multimodal & Interactive & Multi-round & Abductive \\ \hline CLEVR Johnson et al. (2017) & image & ✓ & ✗ & ✗ & ✗ \\ IQA Gordon et al. (2018) & embodied & ✓ & ✓ & ✗ & ✗ \\ EmbodiedQA Das et al. (2018) & embodied & ✓ & ✓ & ✗ & ✗ \\ \(\mathcal{A}\mathcal{R}\)Dhagavatula et al. (2019) & language & ✗ & ✗ & ✗ & ✓ \\ VAR H. Liang et al. (2022) & video & ✓ & ✗ & ✗ & ✓ \\ Sherlock Hessel et al. (2022) & image & ✓ & ✗ & ✗ & ✓ \\ \(\mathbb{C}_{\textbf{can}}\) (Ours) & open-world & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparison between \(\mathbb{C}_{\textbf{can}}\) Conan and related visual reasoning benchmarks. \(\mathbb{C}_{\textbf{can}}\) is unique for its active reasoning and interactive multi-round setting on abductive reasoning tasks.**

Figure 2: **Part of the task dependency graph**. Starting from the root note, any path forms a multi-step task for an agent to interact with the environment.

Though both _detective_ and _vandal_ share the same action space, the _detective_ boasts a unique capability. It not only navigates and interacts like the _vandal_, but can also generate its own traces during its investigation. These overlaid traces from the _detective_ enhance the environment's depth and complexity. This setup pushes the agent to actively derive conclusions from its dynamic interactions. Importantly, the _detective_ is invulnerable; its focus lies squarely on problem-solving, eliminating concerns about survival or evasion. This design emphasizes active exploration and reasoning, ensuring \(\raisebox{-0.9pt}{\includegraphics[]{figures/f1.eps}}\)**Conan**'s primary goal remains addressing complex reasoning tasks and answering visual scene-related questions.

### Questions and Choices

\(\raisebox{-0.9pt}{\includegraphics[]{figures/f2.eps}}\)**Conan** is designed to assess the abductive reasoning capability of machine models through a diverse set of questions varying in difficulty and abstraction. These questions fall into three primary categories: **Intent** (local intent), **Goal** (global goal), and **Survival** (agent's survival status change). We approach evaluation as a multi-choice question-answering task. Each question offers four choices, with only one being correct. Questions and choices derive from predefined templates, as showcased in Tab. 2. For a more detailed explanation, see Appx. B.1.

**Intent** questions target the _vandal_'s immediate objectives or intentions during its task. To decipher these traces, agents must deduce the _vandal_'s underlying intent or subgoals. Solving these questions necessitates a learning model's comprehension of the local context.

**Goal** questions probe the _vandal_'s overarching objectives, extending beyond immediate intents. They necessitate grasping the wider context of a task or action sequence. Such questions query the _vandal_'s ultimate aims, demanding a learning model to reason within the broader context of the traces.

**Survival** questions address the wider investigative scope, posing added challenges to the _detective_. Centered on the _vandal_'s survival status changes during tasks (_e.g._, collecting food for sustenance), they lead to deviations from the optimal action plan. While not tied to a task's primary objective, these questions require a deeper grasp of the present context, often necessitating reasoning around potential scenarios or alternate results.

Compared with the prevalent VQA setup, wherein questions are based on factual information that is readily obtainable from the input, \(\raisebox{-0.9pt}{\includegraphics[]{figures/f2.eps}}\)**Conan** questions cannot be deciphered given only the initial information, necessitating further exploration in the scene. Unlike standard embodied question answering, \(\raisebox{-0.9pt}{\includegraphics[]{figures/f2.eps}}\)**Conan** questions cannot be directly parsed as modular primitives; they demand abductive reasoning, drawing from both new observation and former knowledge to hypothesize, validate, and revise. For benchmarking purposes, \(\raisebox{-0.9pt}{\includegraphics[]{figures/f2.eps}}\)**Conan** produced a corpus comprising 100,000 questions. These were derived from 10,000 unique scenes, generated via the Crafter's scene generator, with each scene stemming from a task executed by a _vandal_. This resulted in an average generation of 10 questions per scene.

\begin{table}
\begin{tabular}{c l} \hline \hline Type & Questions \\ \hline \multirow{4}{*}{Intent} & What did the _vandal_ make on this table? \\  & A: wood sword; B: wood pickaxe; C: iron sword; D: stone sword; \\  & Why did the _vandal_ cut a tree here? \\  & A: make table; B: make wood sword; C: make finance; D: collect apple; \\ \hline \multirow{4}{*}{Goal} & What was the _vandal_’s primary objective in this scenario? \\  & A: get diamond; B: defeat zombie; C: collect apple; D: make iron sword; \\  & What was the desired outcome of the task performed by the _vandal_? \\  & A: make steak; B: make table; C: defeat skeleton; D: collect lava; \\ \hline \multirow{4}{*}{Survival} & Why did the _vandal_ die in this situation? \\  & A: lack of water; B: lack of food; C: hurt by monster; D: hurt by lava; \\  & What could the _vandal_ have done differently to avoid a negative outcome? \\  & A: avoid monsters; B: get sleep; C: get food; D: get water; \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Examples of three categories of questions in \(\raisebox{-0.9pt}{\includegraphics[]{figures/f2.eps}}\)**Conan created from predefined templates.**

## 4 The Detective Pipeline

Conan casts the abductive reasoning challenge as a detective game, necessitating a _detective_ to efficiently explore and gather information from the environment to deduce plausible explanations (_i.e_., answers) for the given question. This process involves taking into account the temporal dependencies and incompleteness of the traces. To tackle these challenges encountered in Conan, we devise a detective pipeline, as depicted in Fig. 3.

Building on previous work that utilizes hierarchical models for task decomposition (Gordon et al., 2018; Das et al., 2018; Wijmans et al., 2019), our pipeline is structured into two primary phases: an exploration phase for trace collection, followed by an abductive reasoning phase. Initially, interaction with the playground is carried out to collect relevant visual information, which is subsequently leveraged in the reasoning phase to infer answers to the posed questions.

Computationally, our pipeline first employs RL agents as _explorers_ (see Sec. 4.1) that learn an exploration policy based on the traces and the question, thereby rendering it goal-oriented. Next, given the question, we recruit vision-language models (see Sec. 4.3) to predict the answer based on the observation. A _key-frame extractor_ (see Sec. 4.2) is inserted into the two phases to selectively identify relevant frames for abduction. The individual components undergo separate training procedures.

### Explorer for Trace Gathering

The primary responsibility of an explorer is to efficiently collect information pertinent to the provided question. Initially, masks are employed to encode questions by highlighting relevant grids. Subsequently, the explorer takes in both the observation and the target question as input and outputs the action probability.

We use a reward function that incentivizes the agent to scout for clues and traces relevant to the given question. Additionally, a penalty term is incorporated to discourage unnecessary actions and inefficient searching, thereby promoting a more targeted exploration strategy.

Specifically, the agent is rewarded with \(+1\) when a trace first appears within its local view, or \(+2\) when the trace bears a close association with the question. A substantial reward of \(+100\) is conferred upon the agent if it successfully uncovers all traces left by the _vandal_. Concurrently, the agent incurs a penalty of \(-0.1\) for every timestep elapsed, with an additional penalty of \(-1\) imposed for executing operating actions.

We evaluate multiple well-regarded RL frameworks as our explorer, including Deep Q-Network (DQN) (Mnih et al., 2015), Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), and Recurrent Proximal Policy Optimization (RecurrentPPO) (Schulman et al., 2017). The Stable-Baselines3 library (Raffin et al., 2021) is employed for all implementations.

Figure 3: **An illustration of the detective pipeline for Conan.** An RL explorer is first trained to gather traces in accordance with the given question. Given a question and the incident scene, the _detective_ calls the explorer subroutine to gather evidence. Next, the exploration sequence undergoes key-frame extraction, processed by a visual encoder, subsequently feeding into a vision-language model for answer selection.

### Key-Frame Extractor

Given that the frames gathered by the explorer tend to be excessively lengthy and redundant, a key-frame extractor is utilized to sift through and select informative frames containing crucial evidence for the _detective_. We adopt a prevalent selection strategy employed in video understanding (Arnab et al., 2021). Specifically, frames within the temporal bounds determined by the detection of the first and last traces are retained, from which \(k\) frames are uniformly sampled. This design is intended to tailor the input with the constrained context window size to downstream vision-language models.

### Vision-Language Models for Abductive Reasoning

We employ a multi-choice question-answering paradigm akin to the one used in Ding et al. (2021). Specifically, the model is presented with a question, its corresponding exploration frame sequence, and each potential answer choice, subsequently generating a score for each choice. The model is trained with a categorical cross-entropy loss. During inference, the choice with the highest score is considered the answer. We evaluate several well-established multimodal models; these models are known for their efficacy in processing both visual and textual data. Additional details on model implementation can be found in Appx. D.1.

Vanilla-TransThe first baseline method leverages a vanilla transformer encoder to fuse observation and textual inputs. Specifically,the raw symbolic map from \(\bigtriangleup\)**Conan** serves as the visual feature, while CLIP's text encoder (Radford et al., 2021) is employed to encode the textual input.

FrozenBiLMFrozenBiLM (Yang et al., 2022), a state-of-the-art model for video question answering, combines visual input with frozen bidirectional language models, trained on web-scraped multimodal data. The approach integrates a frozen language model and a frozen vision encoder with light trainable visual projection modules. FrozenBiLM is tested with BERT-Large (Kenton and Toutanova, 2019) and DeBERTa-v3 (He et al., 2022) as the language model within our question-answering system, utilizing the symbolic map from \(\bigtriangleup\)**Conan** for visual input.

Flamingo-MiniFlamingo (Alayrac et al., 2022) is a family of vision-language models adept at rapid adaptation to novel tasks with minimal annotated examples. These models can handle sequences of visual and textual data, seamlessly accommodating interleaved images or videos as input. We finetune an open-sourced Flamingo-Mini model with frozen OPT-125M (Zhang et al., 2022), utilizing the symbolic map from \(\bigtriangleup\)**Conan** for visual input.

### Abduction from Deduction (AfD)

The adage "_Set a thief to catch a thief_" suggests the use of someone with a similar background or expertise to apprehend a wrongdoer: the best vandal catchers are vandals. This notion resonates with the core principle of Abduction from Deduction (AfD): for a skillful _detective_ to abduce what a _vandal_ does, it needs an in-depth grasp of vandals' _modus operandi_, motivations, and decision-making process. Translating the implication to a mathematical language, we articulate the problem of abductive reasoning based on evidence and knowledge from known deductive transitions. It can also be seen as an extension of inverse planning (Baker et al., 2007, 2009; Baker and Tenenbaum, 2014). Formally, let \(g\) denote the goal of the _vandal_, \(O\) the _detective_'s observation, and \(S\) the playground states post the _vandal_'s actions. We then have:

\[P(g\mid O)=\operatorname*{\mathbb{E}}_{P(S\mid O)}[P(g\mid S,O)]=\operatorname* {\mathbb{E}}_{P(S\mid O)}[P(g\mid S)],\] (1)

where we assume the independence of \(g\) w.r.t. \(O\) given \(S\), as the goal ought to be clear given the states. Leveraging Bayesian rules, we further observe that

\[P(g\mid S)\propto P(S\mid g)\propto\prod_{i}\pi(a_{i}\mid s_{i},g),\] (2)

assuming a uniform prior over \(g\) and known deterministic environment transitions. Eq. (2) asserts that \(P(g\mid S)\) is proportional to a goal-conditioned forward action policy, where \(s_{i},a_{i}\to s_{i+1}\).

Intuitively, Eqs. (1) and (2) can be understood as follows: to **abduce** the _vandal_'s goal from observation, it is imperative to first reconstruct the actual states traversed by the _vandal_ and subsequently ascertain the most plausible goal that, if pursued forward, would result in those states; see Eq. (1). Eq. (2) can be interpreted as a form of **deduction**, being contingent on transition knowledge derived from a forward action policy. Hence the name Abduction from Deduction (AfD).

In practice, two approaches emerge for implementing \(P(g\mid S)\) based on Eq.2. The first entails iterating over all \(g\) and utilizing a learned or predefined \(\pi(\cdot)\) to score a lengthy sequence of states. Conversely, the second approach embraces a data-driven strategy, wherein one arbitrarily selects \(g\), samples \(S\) from \(\pi(\cdot)\), and learns a model of \(P(g\mid S)\) using the \((g,S)\) pairs. The former approach proves time-intensive during inference due to the combinatorial temporal space and expansive goal space, thereby compelling us towards the latter approach. For implementation, we train \(P(S\mid O)\) independently as a Dirac delta function of \(\delta(f(O))\) and \(P(g\mid S)\) from sampled pairs from \(\pi(\cdot)\) employed in task execution in the _vandal_. The derived goal features, along with the question, are fed into the model for answer prediction. Please refer to Appx. F for additional details.

## 5 Experiments

### Experimental Setup

ExplorationThe explorer is trained using DQN, TRPO, and RecurrentPPO for \(10^{8}\) steps, with a buffer size of \(10^{7}\) and a batch size of \(512\). In the case of DQN, training is conducted with \(\epsilon=0.96\). Each episode is capped at a maximum of \(500\) steps for the explorer. A curriculum is employed to encourage long-term exploration whilst maintaining a balance with local search: initial training is carried out with traces from long-horizon tasks like "get the diamond," compelling the agent to venture farther from its starting point. Subsequently, the agent undergoes further finetuning across the entire dataset. Such a curriculum design prevents a sole focus on local discovery. For downstream reasoning models, \(k=30\) keyframes are extracted by the key-frame extractor.

Abductive InferenceOur reasoning models are tested under three different settings: Standard, Ideal Explorer, and AfD. In the Standard setting, models undergo training and testing based on the explorer's exploration. The Ideal Explorer setting sees models leveraging on an optimal exploration policy--visible to the ground-truth _vandal_'s trajectory, albeit imperfect, it facilitates the agent in gathering sufficient evidence for reasoning. This scenario can be conceived as a measure of the reasoning model's aptitude for passive reasoning given complete information. Under the AfD setting, models are trained and used as delineated in Sec.4.4. All models are trained utilizing \(8\) NVIDIA GeForce RTX 3090 GPUs. For further training specifics, please refer to Appx. D.2.

### Results and Analysis

Fig.4 shows the learning curves of various RL agents during exploration. TRPO and RecurrentPPO manifest similar performance in terms of rewards following a substantial number of steps, markedly surpassing the DQN explorer. Additionally, we probe the impact of augmenting the maximum number of exploration steps to \(5,000\) on performance. The data suggests a marginal performance uplift. Nonetheless, we acknowledge that such a performance increment is at the expense of substantially longer exploration time and a notable surge in the accrual of unrelated information. Consequently, we select TRPO with a maximum of \(500\) steps per episode as our standard RL explorer.

Quantitative results on \(\mathbb{C}_{\textbf{Conan}}\) are depicted in Tab.3; both the standard and AfD results reported employ TRPO as the explorer. In the standard setting, we discern that while models exhibit some aptitude in tackling low-level Intent questions, they struggle with higher-level questions pertaining to Goal and Survival. Among the models, Flamingo-Mini ascends to the pinnacle with an accuracy of \(66.3\%\). FrozenBiLM models also perform relatively well. Notably, the DeBERTa variant slightly outperforms BERT, insinuating that a robust language backbone can improve general comprehension. Contrarily, the Vanilla-Trans model languishes across all tasks, achieving merely random-level performance.

Figure 4: **Learning curves of various RL explorers. The suffix \(n\) denotes the maximum number of steps per episode during exploration. Results show that (i) TRPO and RecurrentPPO markedly outperform DQN in performance, and (ii) longer episodes marginally contribute to the performance at the expense of longer exploration time and the accrual of unrelated information.**

With the Ideal Explorer, we notice a clear performance boost across all tasks, particularly in the Goal and Survival questions. These results allude to the potential bottleneck of models' abductive reasoning capability due to the insufficient information collected, underscoring the significance of effective exploration. An adept explorer can significantly aid in the accrual of useful information, informatively pursuing a hypothesis to scrutinize evidence, swiftly self-correcting upon encountering conflicting evidence, and reasonably re-planning. The findings also hint sufficient room for the RL explorer to improve. Remarkably, the Vanilla-Trans exhibits the greatest increase, insinuating that, in comparison to other baseline models, it is markedly vulnerable to insufficient evidence.

For AfD results, nearly all multimodal models exhibit performance on par with end-to-end supervisedly trained models. Remarkably, FrozenBiLM models even surpass the performance observed in standard settings. The persisting failure of Vanilla-Trans can be ascribed to its weakness in reasoning amidst incomplete observations due to the significant disparity between the familiar complete state \(S\) and incomplete observation \(O\). Examining task-specific results, a notable performance uplift in the Survival task models is discernible for almost all models relative to the standard setting, albeit sharing the same observation. These results intimate that the inclusion of deductive information sensitizes the _detective_ to _vandal_'s concerns during task execution. Nevertheless, the exhibited performance in long-term planning remains weak, reinforcing the pressing need for a better exploration policy. Critically, these models continue to find short-term intent questions to be most easily answered.

### Further Discussion

Additional ExperimentsWe further experiment in the absence of visual inputs, serving as a negative control baseline, resulting in random performance across all settings; see Appx. E. This random-level performance underscores the severe constraints imposed on the agent without visual information. The TRPO explorer shows a noticeable improvement over the ones without visual inputs, suggesting that even minimal exploration is preferable to none. Nonetheless, the performance remains relatively modest. On the other hand, the Ideal Explorer demonstrates markedly superior performance, attesting to the substantial benefits its capacity to accrue perfect trace evidence renders to the downstream reasoning task. This accentuates the imperative of effective exploration.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline get\_drink & defeat\_cow & get\_apple & defeat\_skeleton & make\_iron\_pickaxe \\
47.06 & 43.90 & 35.7 & 46.59 & 56.52 \\
100.00 & 85.37 & 78.57 & 82.95 & 52.17 \\ \hline place\_bed & make\_steak & make\_stone\_pickaxe & get\_coal & make\_stone\_sword \\
43.90 & 46.15 & 48.48 & 50.00 & 37.50 \\
87.80 & 50.00 & 39.39 & 45.45 & 41.71 \\ \hline get\_iron & get\_water & get\_stone & make\_iron\_sword & place\_furnace \\
28.57 & 45.95 & 36.84 & 56.25 & 44.44 \\
46.43 & 54.05 & 47.37 & 28.12 & 83.95 \\ \hline get\_diamond & place\_table & get\_wood & make\_wood\_pickaxe & make\_wood\_sword \\
40.62 & 39.36 & 36.00 & 40.00 & 50.00 \\
84.38 & 91.49 & 96.00 & 55.00 & 64.29 \\ \hline \hline make\_bed & get\_java & make\_bucket & get\_beef & defeat\_zoebie \\
47.83 & 50.00 & 35.29 & 53.85 & 52.50 \\
39.13 & 66.67 & 73.53 & 42.31 & 75.00 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Error analysis on \(\mathbb{Q}_{\star}\) Conan.** We examine the accuracy of FrozenBiLM-DeBERTa across various tasks, comparing two explorer groups: reasoning based on the TRPO explorer and the Ideal explorer (in vivo).

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Standard} & \multicolumn{3}{c}{Ideal Explorer} & \multicolumn{3}{c}{AfD} \\ \cline{2-11}  & \(I\) & \(G\) & \(S\) & \(O\) & \(I\) & \(G\) & \(S\) & \(O\) & \(I\) & \(G\) & \(S\) & \(O\) \\ \hline Vanilla-Trans & 32.9 & 25.0 & 24.5 & 28.8 & 64.0 & **78.4** & 58.1 & 66.1 & 24.8 & 23.3 & 24.5 & 24.3 \\ F-BiLM-BERT & 72.6 & **44.4** & **54.4** & 61.0 & 87.5 & 59.5 & 61.5 & 74.0 & 82.8 & **42.9** & **55.5** & 66.0 \\ F-BiLM-DeBERTa & 82.9 & 43.1 & 52.2 & 65.3 & **87.7** & 71.8 & **63.9** & **77.8** & 82.9 & 41.9 & 53.8 & 65.4 \\ Flamingo-Mini & **86.2** & 43.3 & 49.5 & **66.3** & 85.8 & 47.8 & 56.6 & 69.0 & **84.9** & 42.5 & 52.2 & **66.1** \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Performance of abductive reasoning models on \(\mathbb{Q}_{\star}\) Conan.** We report the question-answering accuracy (\(\%\)) across various settings, with the overall accuracy averages over all question categories. F-BiLM refers to the FrozenBiLM model. **I** denotes Intent, **G** denotes Goal, **S** denotes Survival, and **O** denotes Overall. Results exhibiting the top individual performance are highlighted in **bold**, while models with the superior overall performance are shaded in **my**.

Error AnalysisWe extend an error analysis for the "goal" split, probing the reasoning model across a spectrum of tasks. Table 4 compares two groups: reasoning based on the Ideal explorer and the TRPO explorer. The findings underscore that proficient exploration, _i.e._, the heuristic Ideal explorer who recovers the vandal's trajectory, is sufficient for satisfactory performance. However, to fully harness the potential, a more adept reasoner is requisite, one capable of deciphering the vandal's hidden states from observed traces. For instance, the act of telling trees could signify a need for either wood or food (apples), and discerning the intent solely from traces of felled trees presents a challenge. When it comes to "trace-relevant" frames or "keyframes," the Ideal explorer could ostensibly furnish all trace-relevant frames. However, the concept of keyframes remains nebulous. Within the video understanding domain, a formidable challenge lies in the extraction of "keyframes." This is a post-hoc concept that eludes straightforward acquisition upfront. A prevailing approach, aimed at augmenting efficiency (diminishing context length in Transformer), entails truncating it via every k-th frame.

Joint ReasoningThe collective enhancement of both exploration and reasoning elements emerges as quintessential, given its mirroring of human-like intelligence. For instance, by providing feedback, the reasoner can steer the explorer towards actions that are potentially more insightful and likely to produce pertinent traces. Nonetheless, practical implementation encounters significant hurdles. Assigning credit to exploratory decisions bearing long-term implications can be intricate, particularly when the outcomes of exploratory actions become evident after a substantial time lapse, thereby muddying the causal relationship between the decisions and their ultimate effect on reasoning and answering questions. This accentuates the mutual reliance between exploration and reasoning--advancement in one facet demands progression in the other, introducing a bilateral dependency that complicates optimization. The reasoning component alone demands hefty training and computational resources, especially when utilizing large language models. The demand for formidable computational power renders the simultaneous optimization of exploration and reasoning exceedingly daunting. Collectively, this approach is also widely adopted (Gordon et al., 2018; Lei et al., 2018; Kocisky et al., 2018). Consequently, we navigate along this trajectory, projecting that future endeavors on \(\mathbb{Q}\)**Conan** should prioritize reasoning above exploration.

To summarize, the engagement of a proficient explorer substantially enhances abductive reasoning, particularly in higher-level tasks such as goal-oriented and survival-centric inquiries. This underlines the criticality of exploration as a precursor to tackling abductive reasoning tasks in the presence of incomplete information. Furthermore, the achievement of the AfD hint at the potential for models to harness world knowledge, especially transition knowledge pertaining to tasks and traces, to transform abductive reasoning into deductive simulation. We posit that the presented approach resonates more with human-like reasoning, edging us closer to the core of human intelligence.

## 6 Conclusion

In this paper, we introduce \(\mathbb{\bigcirc}\)**Conan**, a benchmark tailored to evaluate and assess models' active reasoning ability in addressing incomplete-information questions in an interactive environment. \(\mathbb{\bigcirc}\)**Conan** sets itself apart from existing abductive reasoning benchmarks by incorporating an open-world playground facilitating active exploration. It differentiates itself from prevailing embodied question-answering benchmarks by introducing the demanding abductive process in question answering, necessitating multi-round abductive inference based on gathered evidence. Moreover, we propose a new learning paradigm, Abduction from Deduction (AfD), that turns the problem of abduction to deduction, exploiting the problem structure through Bayesian principles. Benchmarking the efficacy of contemporary machine learning models on \(\mathbb{\bigcirc}\)**Conan**, we elucidate the model limitations in interacting with the environment that leads to failure in higher-level, longer-term abductive reasoning.

Limitations and Future WorkIn general, we notice two significant limitations from the experimental results. For one, the explorer does not supply particularly relevant information for the reasoning model. In the human abductive reasoning process, exploration and reasoning should be closely intertwined, with an agent using the current hypothesis to guide exploration and improve its understanding. However, due to long-range exploration and complex vision-language reasoning, we only applied the conventional visual question-answering method and did not fully integrate these two processes. For another, learning naive question-answer mapping shall be sub-optimal. By leveraging the problem structure, AfD has shown improved performance on a particular set of problems. Nevertheless, the current AfD formulation is still rudimentary. We believe an in-depth understanding of the structure and well-crafted implementation could further boost performance.

AcknowledgementThe authors would like to thank Ms. Zhen Chen (BIGAI) for designing the figures, and NVIDIA for their generous support of GPUs and hardware. M.X., G.J., W.L., C.Z., and Y.Z. are supported in part by the National Key R&D Program of China (2022ZD0114900), M.X. and W.L. are supported in part by the NSFC (62172043), and Y.Z. is in part by the Beijing Nova Program.

## References

* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. (2022). Flamingo: a visual language model for few-shot learning. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Arnab et al. (2021) Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M., and Schmid, C. (2021). Vivit: A video vision transformer. In _International Conference on Computer Vision (ICCV)_.
* Baker et al. (2009) Baker, C. L., Saxe, R., and Tenenbaum, J. B. (2009). Action understanding as inverse planning. _Cognition_, 113(3):329-349.
* Baker and Tenenbaum (2014) Baker, C. L. and Tenenbaum, J. B. (2014). Modeling human plan recognition using bayesian theory of mind. In _Plan, activity, and intent recognition: Theory and practice_, volume 7, pages 177-204. Morgan Kaufmann Waltham.
* Baker et al. (2007) Baker, C. L., Tenenbaum, J. B., and Saxe, R. R. (2007). Goal inference as inverse planning. In _Annual Meeting of the Cognitive Science Society (CogSci)_.
* Bhagavatula et al. (2019) Bhagavatula, C., Le Bras, R., Malaviya, C., Sakaguchi, K., Holtzman, A., Rashkin, H., Downey, D., Yih, W.-t., and Choi, Y. (2019). Abductive commonsense reasoning. In _International Conference on Learning Representations (ICLR)_.
* Bramley et al. (2017) Bramley, N. R., Dayan, P., Griffiths, T. L., and Lagnado, D. A. (2017). Formalizing neurath's ship: Approximate algorithms for online causal learning. _Psychological Review_, 124(3):301.
* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Cai et al. (2023) Cai, S., Wang, Z., Ma, X., Liu, A., and Liang, Y. (2023). Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Das et al. (2018) Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., and Batra, D. (2018). Embodied question answering. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Ding et al. (2021) Ding, D., Hill, F., Santoro, A., Reynolds, M., and Botvinick, M. (2021). Attention over learned object embeddings enables complex visual reasoning. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Douven (2021) Douven, I. (2021). Abduction. In Zalta, E. N., editor, _The Stanford Encyclopedia of Philosophy_. Metaphysics Research Lab, Stanford University, Summer 2021 edition.
* Edmonds et al. (2018) Edmonds, M., Kubricht, James, F., Summers, C., Zhu, Y., Rothrock, B., Zhu, S.-C., and Lu, H. (2018). Human causal transfer: Challenges for deep reinforcement learning. In _Annual Meeting of the Cognitive Science Society (CogSci)_.
* Edmonds et al. (2020) Edmonds, M., Ma, X., Qi, S., Zhu, Y., Lu, H., and Zhu, S.-C. (2020). Theory-based causal transfer: Integrating instance-level induction and abstract-level structure learning. In _AAAI Conference on Artificial Intelligence (AAAI)_.
* Edmonds et al. (2019) Edmonds, M., Qi, S., Zhu, Y., Kubricht, J., Zhu, S.-C., and Lu, H. (2019). Decomposing human causal learning: Bottom-up associative learning and top-down schema reasoning. In _Annual Meeting of the Cognitive Science Society (CogSci)_.
* Fan et al. (2022) Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A. (2022). Minedojo: Building open-ended embodied agents with internet-scale knowledge. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Gigerenzer and Gaissmaier (2011) Gigerenzer, G. and Gaissmaier, W. (2011). Heuristic decision making. _Annual Review of Psychology_, 62:451-482.
* Gkioxari et al. (2018)Goodale, M. A. and Milner, A. D. (1992). Separate visual pathways for perception and action. _Trends in Neurosciences_, 15(1):20-25.
* Gordon et al. (2018) Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., and Farhadi, A. (2018). Iqa: Visual question answering in interactive environments. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Hafner (2021) Hafner, D. (2021). Benchmarking the spectrum of agent capabilities. In _International Conference on Learning Representations (ICLR)_.
* Hanson (1965) Hanson, N. R. (1965). _Patterns of discovery: An inquiry into the conceptual foundations of science_. CUP Archive.
* He et al. (2022) He, P., Gao, J., and Chen, W. (2022). Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. In _International Conference on Learning Representations (ICLR)_.
* Hessel et al. (2022) Hessel, J., Hwang, J. D., Park, J. S., Zellers, R., Bhagavatula, C., Rohrbach, A., Saenko, K., and Choi, Y. (2022). The abduction of sherlock holmes: A dataset for visual abductive reasoning. In _European Conference on Computer Vision (ECCV)_.
* Hobbs et al. (1993) Hobbs, J. R., Stickel, M. E., Appelt, D. E., and Martin, P. (1993). Interpretation as abduction. _Artificial intelligence_, 63(1-2):69-142.
* Hochreiter and Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. _Neural Computation_, 9(8):1735-1780. A4
* Johnson et al. (2017) Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. (2017). Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Johnson et al. (2016) Johnson, M., Hofmann, K., Hutton, T., and Bignell, D. (2016). The malmo platform for artificial intelligence experimentation. In _International Joint Conference on Artificial Intelligence (IJCAI)_.
* Josephson and Josephson (1996) Josephson, J. R. and Josephson, S. G. (1996). _Abductive inference: Computation, philosophy, technology_. Cambridge University Press.
* Kakas et al. (1992) Kakas, A. C., Kowalski, R. A., and Toni, F. (1992). Abductive logic programming. _Journal of Logic and Computation_, 2(6):719-770.
* Kayser et al. (2021) Kayser, M., Camburu, O.-M., Salewski, L., Emde, C., Do, V., Akata, Z., and Lukasiewicz, T. (2021). e-vil: A dataset and benchmark for natural language explanations in vision-language tasks. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Kenton and Toutanova (2019) Kenton, J. D. M.-W. C. and Toutanova, L. K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of NAACL-HLT_.
* Kocisky et al. (2018) Kocisky, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. (2018). The narrativeqa reading comprehension challenge. _Transactions of the Association for Computational Linguistics (TACL)_, 6:317-328.
* Lei et al. (2018) Lei, J., Yu, L., Bansal, M., and Berg, T. L. (2018). Tvqa: Localized, compositional video question answering. In _Annual Conference on Empirical Methods in Natural Language Processing (EMNLP)_.
* Li et al. (2020) Li, L., Chen, Y.-C., Cheng, Y., Gan, Z., Yu, L., and Liu, J. (2020). Hero: Hierarchical encoder for video+ language omni-representation pre-training. In _Annual Conference on Empirical Methods in Natural Language Processing (EMNLP)_. A5
* Li et al. (2023) Li, Q., Huang, S., Hong, Y., Zhu, Y., Wu, Y. N., and Zhu, S.-C. (2023). A minimalist dataset for systematic generalization of perception, syntax, and semantics. In _International Conference on Learning Representations (ICLR)_.
* Li et al. (2022) Li, Q., Zhu, Y., Liang, Y., Wu, Y. N., Zhu, S.-C., and Huang, S. (2022). Neural-symbolic recursive machine for systematic generalization. _arXiv preprint arXiv:2210.01603_.
* Liang et al. (2022) Liang, C., Wang, W., Zhou, T., and Yang, Y. (2022). Visual abductive reasoning. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Magnani (2011) Magnani, L. (2011). _magnani2011abduction_. Springer Science & Business Media.
* Magnani et al. (2018)Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533.
* Oaksford and Chater (1994) Oaksford, M. and Chater, N. (1994). A rational analysis of the selection task as optimal data selection. _Psychological Review_, 101(4):608.
* OpenAI (2023) OpenAI (2023). Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_.
* Peirce (1965) Peirce, C. S. (1965). _Collected papers of Charles Sanders Peirce_, volume 5. Harvard University Press.
* Poole (1993) Poole, D. (1993). Probabilistic horn abduction and bayesian networks. _Artificial Intelligence_, 64(1):81-129.
* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_.
* Raffin et al. (2021) Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N. (2021). Stable-baselines3: Reliable reinforcement learning implementations. _Journal of Machine Learning Research (JMLR)_, 22(1):12348-12355.
* Rieber (1996) Rieber, L. P. (1996). Seriously considering play: Designing interactive learning environments based on the blending of microworlds, simulations, and games. _Educational Technology Research and Development_, 44(2):43-58.
* Rizzolatti et al. (1997) Rizzolatti, G., Fadiga, L., Fogassi, L., and Gallese, V. (1997). The space around us. _Science_, 277(5323):190-191.
* Rocktaschel and Riedel (2017) Rocktaschel, T. and Riedel, S. (2017). End-to-end differentiable proving. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention (MICCAI)_.
* Schulman et al. (2015) Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy optimization. In _International Conference on Machine Learning (ICML)_.
* Schulman et al. (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_.
* Shank (1998) Shank, G. (1998). The extraordinary ordinary powers of abductive reasoning. _Theory & Psychology_, 8(6):841-860.
* Thagard (1988) Thagard, P. (1988). _Computational philosophy of science_. MIT press.
* Thoppilan et al. (2022) Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. (2022). Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_.
* Wang et al. (2023) Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y. (2023). Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. _arXiv preprint arXiv:2302.01560_.
* Wijmans et al. (2019) Wijmans, E., Datta, S., Maksymets, O., Das, A., Gkioxari, G., Lee, S., Essa, I., Parikh, D., and Batra, D. (2019). Embodied question answering in photorealistic environments with point cloud perception. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Xie et al. (2021) Xie, S., Ma, X., Yu, P., Zhu, Y., Wu, Y. N., and Zhu, S.-C. (2021). Halma: Humanlike abstraction learning meets affordance in rapid problem solving. In _ICLR Workshop on Generalization beyond the training distribution in brains and machines_.
* Xu et al. (2023) Xu, M., Jiang, G., Liang, W., Zhang, C., and Zhu, Y. (2023). Interactive visual reasoning under uncertainty. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Yang et al. (2022) Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C. (2022). Zero-shot video question answering via frozen bidirectional language models. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Yu et al. (2019) Yu, L., Chen, X., Gkioxari, G., Bansal, M., Berg, T. L., and Batra, D. (2019). Multi-target embodied question answering. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Yuan et al. (2022) Yuan, L., Gao, X., Zheng, Z., Edmonds, M., Wu, Y. N., Rossano, F., Lu, H., Zhu, Y., and Zhu, S.-C. (2022). In situ bidirectional human-robot value alignment. _Science Robotics_, 7(68).
* Zhu et al. (2021)Zhang, C., Jia, B., Edmonds, M., Zhu, S.-C., and Zhu, Y. (2021a). Acre: Abstract causal reasoning beyond covariation. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Zhang et al. (2021) Zhang, C., Jia, B., Zhu, S.-C., and Zhu, Y. (2021b). Abstract spatial-temporal reasoning via probabilistic abduction and execution. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. (2022). Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_.

## Appendix A Conan Playground

Conan's playground is a computationally efficient 2D open-world environment with diverse items and rich tasks. The most distinctive feature of Conan's playground over the original Crafter environment is that agents in Conan leave diverse traces when interacting with the environment. These traces serve as the foundation for abductive reasoning; the _detective_ has to effectively connect the traces to figure out what the _vandal_ has done.

### Items and Traces

LandBased on Crafter, there are three types of terrains that agents can walk on: _sand_, _grass_, and _path_. _Sand_ and _grass_ are soft surfaces where agents leave directional footprints after walking on them (see Fig. A1 first 2 rows in Columns 2 and 3 for examples). If a grid is left with more than one footprint, the footprints will become melded (Fig. A1 Column 4 in first 2 rows). Agents' actions will also leave traces on the terrain, _e.g._, water on the ground (Fig. A1 Column 5 first 2 rows). If an agent gets injured, blood will be shed on the ground (Fig. A1 Column 6 first 2 rows).

CreaturesThere are four creatures in the playground: _plant_, _cow_, _zombie_ and _skeleton_. _plant_ grows from sapling to ripe plant. _Cow_ randomly wander on the ground, whereas _zombie_ and _skeleton_ (monsters in general) will target agents in sight: _zombie_ chases agents and _skeleton_ shoots _arrow_ at agents. Agents can fight with creatures and kill them. These actions will leave monster bodies on the ground.

ToolsAgents can make tools on the _table_. There are 7 tools in total: _bucket_, _wood_sword_, _wood_pickxae_, _stone_sword_, _stone_pickxae_, _iron_sword_, and _iron_pickxae_. These tools can be made using different materials and used for certain tasks. Both swords and pickaxes can be used to fight with creatures, but only pickaxes can be used in mining. Buckets can be used to collect water and lava.

ActionsConan's playground enables agents to interact with objects, non-playable characters, and even other agents in the playground. Agents can cut _tree_ to get _apple_ and _wood_, as well as collect _sapling_ and grow _plant_ (Fig. A1 Row 3). They can also mine with different tools to get _stone_, _coal_, _iron_, and _diamond_. Using these materials, agents can make _bed_ for sleep, _furnace_ for keeping monsters away and grilling food, _table_ for making tools, _etc_. Of note, these items should be placed in an empty grid to use and they can be destroyed by monsters.

### Achievements and Tasks

There are 60 tasks and 39 achievements in Conan's playground. We list all achievements in Tab. A1. Tasks are composed achievements. We select 60 nontrivial and meaningful tasks from all compositions in Conan as the final task set.

### Observation and Action

Conan offers both pixel representation and symbolic representation for training agents. For pixel representation, the environment returns a \(900\times 900\) RGB image each time step for the _detective_'sFigure A1: **Items and related traces in \(\Cartriangle\) Conan.**

\(9\times 9\) local view. For symbolic representation, the environment returns a \(9\times 9\) tensor, with each entry an index representing one of 50 grid types, covering materials, resources, objects, creatures, and _etc_. The agent is always at the center of the observation.

\(\mathbb{C}\)**Conan** affords a larger action space. See Tab. A2 for a detailed list of actions.

## Appendix B \(\mathbb{C}\) Conan Questions

### Question Generation

Questions in \(\mathbb{C}\)**Conan** are generated based on _vandal_'s task-finishing process. To generate a question, (1) we initialize a playground and put the _vandal_ in it; (2) the _vandal_ is randomly assigned a task; (3) the _vandal_ tries to finish the task with the help of the pre-build parser and planner, and generates logs along the way; (4) a question is generated based on a certain part of the log. We randomly select a template from the template pool and fill placeholders with related objects in it. The answer is also parsed from the log. Other choices are sampled based on the question and the context to avoid unrelated choices that can be easily excluded.

### Question Templates

Tab. A3 lists all the templates we use for generating questions.

### Dataset Statistics

See Tab. A4 and Tab. A5 for details.

## Appendix C Explorer

The Explorer in the _detective_ is an RL agent. The agent receives an observation of a \([64,64,2]\) tensor. This tensor combines the \(9\times 9\) symbolic local view of the _detective_ and a \(64\times 64\) question mask.

The local view is zero-padded to \(64\times 64\). This ensures the agent knows its relative position on the map. Additionally, the mask is generated based on the question, with the area related to the question unmasked. The mask serves as the goal of the exploration policy.

All the RL baselines are trained for \(10^{8}\) steps. See more details below. Unless specified otherwise, parameters are set as default in Stable Baselines.

### Model Details

DQNThe DQN baseline is trained using a \(\gamma\) value of 0.98, a \(\tau\) value of 1, a learning rate of 0.0001, a buffer size of \(10^{7}\), and a batch size of 512. We leverage an MLP policy with two layers of 64 neurons each. The model is updated every 10 steps.

TRPOThe TRPO baseline updates its policy with a special KL-divergence constraint on the distance between the new and old policies. We also leverage an MLP policy for TRPO, where the same multi-layer perceptron is used for both policy and value prediction.

RecurrentPPThe ReucrrentPP baseline uses long short-term memory (LSTM) [10] as the recurrent policy. The LSTM layers' weights are initialized with standard Gaussian. We reset LSTM states at the end of the episode. The LSTMs for both the actor and the critic have the same architecture, with two LSTM layers of 256 neurons each.

### Training Details

Explorers are firstly trained on long-horizon tasks as explained in the main text. These long-horizon tasks include "get diamond," "get lava," "get water," "make iron sword," "make iron pickaxe" and "eat steak." These tasks can be further broken down into over 20 subtasks and have an average episode length of more than 200 steps. We generate 10,000 unique scenes with traces given these tasks and train explorers on them for \(10^{8}\) steps. Then the explores are fine-tuned on all tasks in \(\raisebox{-0.86pt}{\scalebox{1.0}{\mbox{\rule{0.4pt}{6.5pt}}}}\)**Conan** for \(10^{7}\) steps.

We also show the frame rate per second (FPS) for different RL baselines during training in Fig. A2. As can be seen from the figure, DQN exhibits the highest training efficiency, reaching an FPS exceeding 3000. TRPO maintains a stable FPS of 2000. On the contrary, RecurrentPPO operates significantly slower, requiring over 96 hours to complete training with 128 subproc environments, whereas TRPO accomplishes the task in just 14 hours.

## Appendix D VL Reasoning

In this section, we describe the experimental details for the Vision-Language (VL) models used in the paper.

### Model Details

Vanilla-TransFor Vanilla-Trans, the visual features together with the text features are concatenated in the format of [frame_1, frame_2,..., frame_n, question, choice_1, choice_2,..., choice_4]. Visual features, if from the symbolic observation, are directly passed into the model. Otherwise, we utilize CLIP's pre-trained image encoder (ViT-B/16) to extract features from pixel input. Text features are calculated using the text encoder of CLIP. These input features are then passed through a 6-layer Transformer model with an MLP head for classification.

FrozenBiLMWe adopt the cross-modal FrozenBiLM for \(\raisebox{-0.86pt}{\scalebox{1.0}{\mbox{\rule{0.4pt}{6.5pt}}}}\)**Conan**, drawing inspiration from models used in Multiple-choice VideoQA benchmarks such as How2QA (Li et al., 2020) and TVQA (Lei et al., 2018)1. \(\raisebox{-0.86pt}{\scalebox{1.0}{\mbox{\rule{0.4pt}{6.5pt}}}}\)**Conan** can be formulated as a multiple-choice VideoQA problem given the fixed explorer. We concatenate all of the observation frames as the video input. The questions and choices are converted into the following format: ["{question} Is it (choice_1)?,"..., "{question} Is it (choice_4)?"]. We then evaluate the probabilities of the model producing "Yes" and "No". The visual features are processed in the same way as in Vanilla-Trans and then forwarded for visual-text projection. We utilize BERT-Large and DeBERTa as our frozen language backbones in this work; however, other general language models are applicable as well.

Footnote 1: https://github.com/antoyang/FrozenBiLM

Footnote 2: https://github.com/lucidrains/flamingo-pytorch

Flamingo-MiniOur Flamingo-Mini baseline is based on an open-source implementation of the Flamingo model3, as the original Flamingo model's pre-trained weights are not accessible. Flamingo-Mini is built upon OPT-125M and CLIP pre-trained ViT-L/14 model. We also formulate \(\raisebox{-0.86pt}{\scalebox{1.0}{\mbox{\rule{0.4pt}{6.5pt}}}}\)**Conan** as a multiple-choice problem for Flamingo-Mini. The questions and choices are converted into the following format: ["Question: {question} Answer: {choice_1},"..., "Question: {question} Answer: {choice_4}"]. Each question-choice pair is fed into the model and then a binary classifier head is used on Flamingo's last layer output to predict the final answer.

### Training Details

Vanilla-Trans was trained for \(100\) epochs, with a batch size of \(128\). FrozenBiLM models were trained for \(50\) epochs, with a masking probability (for the MLM objective) of \(0.15\), a batch size of \(32\), a learning rate of \(3\times 10^{-4}\), a gradient clipping max norm of \(0.1\), and Adam as the optimizer (\(\beta_{1}=0.9,\beta_{2}=0.95,\epsilon=1\times 10^{-8}\)). Flamingo-Mini was trained for \(100\) epochs, with a learning rate of \(5\times 10^{-5}\), a batch size of \(8\), and also Adam as the optimizer (\(\beta_{1}=0.9,\beta_{2}=0.999,\epsilon=1\times 10^{-8}\)).

## Appendix E Additional Experiments

### Negative Control Baselines

We compare our VL reasoning results on the trained explorers with those on empty visual inputs as a negative control baseline. The results are shown in Tab. A6.

The results show that using empty visual inputs yields random performance across all settings. Besides, it also shows that the training QA pairs are unbiased. The TRPO explorer achieves higher performance, which suggests that the exploration strategy learned by TRPO helps gather some informative evidence for the reasoning process. The Ideal explorer is an oracle-like exploration policy that has access to perfect trace evidence and temporal information. It provides the most comprehensive information about the environment. This highlights the importance of effective exploration in improving reasoning performance. However, it does not mean that reasoning is less important, as even with the Ideal explorer, the model still could not achieve satisfactory performance. Based on all results, collecting informative evidence seems to be more important in the overall objective.

## Appendix F Abduction from Deduction (AfD)

As mentioned in Sec. 4.4, we adopt a data-driven strategy to learn a model of \(P(g\mid S)\) and simultaneously answer the questions. To be more specific, we train the _detective_ agent self-supervisedly. The _detective_ is randomly assigned with one of all possible tasks. It then finishes the task by following the action policy \(\pi(\cdot)\). Note that we assume the _detective_'s \(\pi(\cdot)\) is the same as the _vandal_'s in order to best implement the idea of AfD. Based on the task execution process, questions are generated. Since our ultimate goal is to have our models answer \(\mathbb{C}_{\bigtriangleup}\)**Conan**'s questions, we do not explicitly construct \(P(g\mid S)\), but rather consider the question-answer process as the \(g\). We then train \(P(g\mid S)\), where \(S\) is the _detective_'s observation during the task execution, and the label can be derived from the assigned tasks together with the \(\pi(\cdot)\).

Besides \(P(g\mid S)\), we still need to learn a model of \(P(S\mid O)\), which, intuitively, can be understood as inferring the true state of the environment from partial observation. In our experiment, we tried two ways to model \(P(S\mid O)\). One approach is to directly train a model using multi-frame observations to predict the states. We employed a UNet (Ronneberger et al., 2015) and a multi-layer CNN as the network. However, this method did not work effectively. Reasoning based on the reconstructed states only achieved performance at a random level. The second approach, which was finally used to report performance, aligned the hidden feature spaces from true states and observations. When training \(P(g\mid S)\), we added a head before the VL models, converting the input \(S\) into a 4096-dimensional vector. Then we trained a head on \(O\) with the same structure, minimizing the difference between features from \(O\) and features from \(S\).
To better illustrate the core components in \(\circ\)\(\downarrow\)Conan, We take the playground shown in Fig. 1 as an example. In this scenario, the assigned task is "get diamond" (Fig. A3 shows the task dependency). As shown in Fig. A4, once the vandal completes the task, it leaves behind traces in the playground. The vandal ends at the bottom of the figure. The detective then enters the playground, starting at the beginning of the traces. In this case, traces encompass footprints and remnants left after certain actions. Note that footprints cannot be left on sand or stone, and different footprints may overlap. The vandal will collect objects crafted on a table, making them invisible.

Let's suppose the detective's exploration begins by following footprints (note the context window size is 9\(\times\)9).

Firstly we can see some cut trees. As the footprints are not seriously overlapped and mostly one-directional, we can deduce the vandal did not return. After seeing the tool-making table, with the only resources being wood, we could say that the vandal could only make wooden tools, not stone swords or iron pickaxes, further restricting possible actions the vandal took.

Note that this is already critical reasoning in \(\circ\)\(\downarrow\)Conan.

Moving on, we note that footprints become missing on the sand surface. However, we note broken stones and coals. Therefore, the wooden tool to break stones and coals shall be a wooden pickaze. So the agent should have made a wooden pickaze on the table earlier. Despite the fact that the tool has been collected, we could still figure that out.

Following the reemerged footprints, we note blood and a zombie body on the ground, suggesting the vandal should have had a fight.

Searching on, we find the broken diamond. As an iron pickaze is the only tool to collect diamonds. The vandal must have built an iron pickaze with iron and coal in the furnace. With no other footprints around, we can safely conclude our search.