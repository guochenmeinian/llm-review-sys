# Demystifying amortized causal discovery with transformers

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Supervised learning approaches for causal discovery from observational data often achieve competitive performance despite seemingly avoiding explicit assumptions that traditional methods make for identifiability. In this work, we investigate CSIvA [1], a transformer-based model promising to train on synthetic data and transfer to real data. First, we bridge the gap with existing identifiability theory and show that constraints on the training data distribution implicitly define a prior on the test observations. Consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable. At the same time, we find new trade-offs. Training on datasets generated from different classes of causal models, unambiguously identifiable in isolation, improves the test generalization. Performance is still guaranteed, as the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur (which we formally prove). Overall, our study finds that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.

## 1 Introduction

Causal discovery aims to uncover the underlying causal relationships between variables of a system from pure observations, which is crucial for answering interventional and counterfactual queries when experimentation is impractical or unfeasible [2; 3; 4]. Unfortunately, causal discovery is inherently ill-posed [5]: unique identification of causal directions requires restrictive assumptions on the class of structural causal models (SCMs) that generated the data [6; 7; 8]. These theoretical limitations often render existing methods inapplicable, as the underlying assumptions are usually untestable or difficult to verify in practice [9].

Recently, supervised learning algorithms trained on synthetic data have been proposed to overcome the need for specific hypotheses, which restrains the application of classical causal discovery methods to real-world problems [1; 10; 11; 12; 13]. Seminal work from Lopez-Paz et al. [10] argues that this learning-based approach to causal discovery would allow dealing with complex data-generating processes and would greatly reduce the need for explicitly crafting identifiability conditions a-priori: despite this ambitious goal, the output of these methods is generally considered unreliable, as no theoretical guarantee is provided. A pair of non-identifiable structural causal models can be associated with different causal graphs \(\mathcal{G}\neq\hat{\mathcal{G}}\), while entailing the same joint distribution \(p\) on the system's variables. It is thus unclear how a learning algorithm presented with observational data generated from \(p\) would be able to overcome these theoretical limits and correctly identify a unique causal structure. However, the available empirical evidence seems not to care about impossibility results, as these methods yield surprising generalization results on several synthetic benchmarks. Our work aims to bridge this gap by studying the performance of a transformer architecture for causal discovery throughthe lens of the theory of identifiability from observational data. Specifically, we analyze the CSIvA (Causal Structure Induction via Attention) model for causal discovery [1], focusing on bivariate graphs, as they offer a controlled yet non-trivial setting for the investigation. As our starting point, we provide closed-form examples that identify the limitations of CSIvA in recovering causal structures of linear non-Gaussian and nonlinear additive noise models, which are notably identifiable, and demonstrate the expected failures through empirical evidence. These findings suggest that the class of structural causal models that can be identified by CSIvA is inherently dependent on the specific class of SCMs observed during training. Thus, the need for restrictive hypotheses on the data-generating process is intrinsic to causal discovery, both in the traditional and modern learning-based approaches: assumptions on the test distribution either are posited when selecting the algorithm (traditional methods) or in the choice of the training data (learning-based methods). To address this limitation, we theoretically and empirically analyze _when_ training CSIvA on datasets generated by multiple identifiable SCMs with different structural assumptions improves its generalization at test time. In summary:

* We show that the class of structural causal models that CSIvA can identify is defined by the class of SCMs observed through samples during the training. We reinforce the notion that identifiability in causal discovery inherently requires assumptions, which must be encoded in the training data in the case of learning-based approaches.
* To overcome this limitation, we study the benefits of CSIvA training on mixtures of causal models. We analyze when algorithms learned on multiple models are expected to identify broad classes of SCMs (unlike many classical methods). Empirically, we show that training on samples generated by multiple identifiable causal models with different assumptions on mechanisms and noise distribution results in significantly improved generalization abilities.

Closely related works and their relation with CSIvA.In this paper, we study _amortized inference of causal graphs_, i.e. optimization of an inference model to directly predict a causal structure from newly provided data. This is the first work that attempts to understand the connection between identifiability theory and amortized inference, while several algorithms have been proposed. In the context of purely observational data, Lopez-Paz et al. [10] defines a distribution regression problem [14] mapping the kernel mean embedding of the data distribution to a causal graph, while Li et al. [11] relies on equivariant neural network architectures. More recently, Lippe et al. [12] and Lorch et al. [13] proposed learning on interventional data, in addition to observations (in the same spirit as CSIvA). Despite different algorithmic implementations, the target object of estimation of most of these methods is the distribution over the space of all possible graphs, conditional on the input dataset (similarly, the ENCO algorithm in Lippe et al. [12] models the conditional distribution of individual edges). This justifies our choice of restricting our study to the CSIvA architecture (despite this being a clear limitation), as in the infinite observational sample limit, these methods approximate the same distribution. Methods necessarily requiring interventional data [15; 16; 17], and learning-based algorithms unsuitable for amortized inference [18; 19; 20; 21; 22] are out of the scope of this work.

## 2 Background and motivation

We start introducing structural causal models (SCMs), an intuitive framework that formalizes causal relations. Let \(X\) be a set of random variables in \(\mathbb{R}\) defined according to the set of structural equations:

\[X_{i}\coloneqq f_{i}(X_{\mathrm{PA}^{\mathcal{G}}_{i}},N_{i}),\ \ \forall i=1,\ldots,k.\] (1)

\(N_{i}\in\mathbb{R}\) are _noise_ random variables. The function \(f_{i}\) is the _causal mechanism_ mapping the set of _direct causes_\(X_{\mathrm{PA}^{\mathcal{G}}_{i}}\) of \(X_{i}\) and the noise term \(N_{i}\), to \(X_{i}\)'s value. The _causal graph_\(\mathcal{G}\) is a directed acyclic graph (DAG) with nodes \(X=\{X_{1},\ldots,X_{k}\}\), and edges \(\{X_{j}\to X_{i}:X_{j}\in X_{\mathrm{PA}^{\mathcal{G}}_{i}}\}\), with \(\mathrm{PA}^{\mathcal{G}}_{i}\) indices of the parent nodes of \(X_{i}\) in \(\mathcal{G}\). The causal model induces a density \(p_{X}\) over the vector \(X\).

### Causal discovery from observational data

Causal discovery from observational data is the inference of the causal graph \(\mathcal{G}\) from a dataset of i.i.d. observations of the random vector \(X\). In general, without restrictive assumptions on the mechanisms and the noise distributions, the direction of edges in the graph \(\mathcal{G}\) is not identifiable, i.e. it can not be found from the population density \(p_{X}\). In particular, it is possible to identify only a Markov equivalence class, which is the set of graphs encoding the same conditional independencies as the density \(p_{X}\). To clarify with an example, consider the causal graph \(X_{1}\to X_{2}\) associated with a structural causal model inducing a density \(p_{X_{1},X_{2}}\). If the model is not identifiable, there exists an SCM with causal graph \(X_{2}\to X_{1}\) that entails the same joint density \(p_{X_{1},X_{2}}\). The set \(\{X_{1}\to X_{2},X_{2}\to X_{1}\}\) is the Markov equivalence class of the graph \(X_{1}\to X_{2}\), i.e. the set of all graphs with \(X_{1},X_{2}\) mutually dependent. Clearly, in this setting, even the exact knowledge of \(p_{X_{1},X_{2}}\) cannot inform as about the correct causal direction.

**Definition 1** (Identifiable causal model).: Consider a structural causal model with underlying graph \(\mathcal{G}\) and \(p_{X}\) joint density of the causal variables. We say that the model is _identifiable_ from observational data if the density \(p_{X}\) can not be entailed by a structural causal model with graph \(\tilde{\mathcal{G}}\neq\mathcal{G}\).

We define the _post-additive noise model_ (post-ANM) as the causal model with the set of equations:

\[X_{i}\coloneqq f_{2,i}(f_{1,i}(X_{\mathrm{PA}_{i}^{\mathcal{G}}})+N_{i}),\ \forall i=1,\ldots,d,\] (2)

with \(f_{2,i}\) invertible map and mutually independent noise terms. When \(f_{2,i}\) is a nonlinear function, the post-ANM amounts to the identifiable _post-nonlinear_ model (PNL) [8]. When \(f_{2,i}\) is the identity function and \(f_{1,i}\) nonlinear, it simplifies to the nonlinear _additive noise model_ (ANM)[7; 23], which is known to be identifiable, and is described by the set of structural equations:

\[X_{i}\coloneqq f_{1,i}(X_{\mathrm{PA}_{i}^{\mathcal{G}}})+N_{i}.\] (3)

If, additionally, we restrict the mechanisms \(f_{1,i}\) to be linear and the noise terms \(N_{i}\) to a non-Gaussian distribution, we recover the identifiable _linear non-Gaussian additive model_ or LiNGAM [6]:

\[X_{i}=\sum_{j\in\mathrm{PA}_{i}^{\mathcal{G}}}\alpha_{j}X_{j}+N_{i},\quad \alpha_{j}\in\mathbb{R}.\] (4)

### Motivation and problem definition

Causal discovery from observational data relies on specific assumptions, which can be challenging to verify in practice [9]. To address this, recent methods leverage supervised learning for the amortized inference of causal graphs [1; 10; 11; 12; 13; 16; 24], optimizing an inference model to directly predict a causal structure from a provided dataset. While these approaches aim to reduce reliance on explicit identifiability assumptions, they often lack a clear connection to the existing causal discovery theory, making their outputs generally unreliable. We illustrate this limitation through an example.

**Example 1**.: We consider the CSIvA transformer architecture proposed by Ke et al. [1], which can learn a map from observational data to a causal graph. The authors of the paper show that, in the infinite sample regime, the CSIvA architecture exactly approximates the conditional distribution \(p(\cdot|\mathcal{D})\) over the space of possible graphs, given a dataset \(\mathcal{D}\). Identifiability theory in causal discovery tells us that if the class of structural causal models that generated the observations is sufficiently constrained, then there is only one graph that can fit the data within that class. For example, consider the case of a dataset that is known to be generated by a nonlinear additive noise model, and let \(p(\cdot|\mathcal{D},\text{ANM})\) be the conditional distribution that incorporates this prior knowledge on the SCM: then \(p(\cdot|\mathcal{D},\text{ANM})\) concentrates all the mass on a single point \(\mathcal{G}^{*}\), the true graph underlying the \(\mathcal{D}\) observations. Instead, in the absence of restrictions on the structural causal model, all the graphs in a Markov equivalence class are equally likely to be the correct solution given the data. Hence, \(p(\cdot|\mathcal{D})\), the distribution learned by CSIvA, assigns equal probability to each graph in the Markov equivalence class of \(\mathcal{G}^{*}\).

Our arguments of Example 1 are valid for all learning methods that approximate the conditional distribution over the space of graphs given the input data [1; 10; 11; 12; 13], and suggest that these algorithms are at most informative about the equivalence class of the causal graph underlying the observations. However, the available empirical evidence does not seem to highlight these limitations, as in practice these methods can infer the true causal DAG on several synthetic benchmarks. Thus, further investigation is necessary if we want to rely on their output in any meaningful sense. In this work, we analyze these "black-box" approaches through the lens of established theory of causal discovery from observational data (causal inference often lacks experimental data, which we do not consider). We study in detail the CSIvA architecture [1] (see Appendix A), a variation of the transformer neural network [25] for the supervised learning of algorithms for amortized causal discovery. This model is optimized via maximum likelihood estimation, i.e. finding \(\Theta\) that minimizes \(-\mathbf{E}_{\mathcal{G},\mathcal{D}}[\ln\hat{p}(\mathcal{G}|\mathcal{D}; \Theta)]\)where \(\hat{p}(\mathcal{G}|\mathcal{D};\Theta)\) is the conditional distribution of a graph \(\mathcal{G}\) given a dataset \(\mathcal{D}\) parametrized by \(\Theta\). We limit the analysis to CSIvA as it is a simple yet competitive end-to-end approach to learning causal models. While this is clearly a limitation of the paper, our theoretical and empirical conclusions exemplify both the role of theoretical identifiability in modern approaches and the new opportunities they provide. Additionally, it fits well within a line of works arguing that specifically transformers can learn causal concepts [26; 27; 28] and identify different assumptions in context [29].

## 3 Experimental results through the lens of theory

In this section, we present a comprehensive analysis of causal discovery with transformers and its relation to the theoretical boundaries of causal discovery from observational data. We show that suitable assumptions must be encoded in the training distribution to ensure the identifiability of the test data, and we additionally study the effectiveness of training on mixtures of causal models to overcome these limitations, improving generalization abilities.

### Experimental design

We concentrate our research on causal models of two variables, causally related according to one of the two graphs \(X\to Y\), \(Y\to X\). Bivariate models are the simplest non-trivial setting with a well-known theory of causality inference [7; 8; 23], but also amenable to manipulation. This allows for comprehensive training and analysis of diverse SCMs and facilitates a clear interpretation of the results.

Datasets.Unless otherwise specified, in our experiments we train CSIvA on a sample of \(15000\) synthetically generated datasets, consisting of \(1500\) i.i.d. observations. Each dataset is generated according to a single class of SCMs, defined by the mechanism type and the noise terms distribution. The coefficients of the linear mechanisms are sampled in the range \([-3,-0.5]\cup[0.5,3]\), removing small coefficients to avoid _close-to-unfaithful_ effects [30]. Nonlinear mechanisms are parametrized according to a neural network with random weights, a strategy commonly adopted in the literature of causal discovery [1; 9]. The post-nonlinearity of the PNL model consists of a simple map \(z\mapsto z^{3}\). Noise terms are sampled from common distributions and a randomly generated density that we call _mlp_, previously adopted in Montagna et al. [9], defined by a standard Gaussian transformed by a multilayer perceptron (MLP) (Appendix B.2). We name these datasets _mechanism-noise_ to refer to their underlying causal model. For example, data sampled from a nonlinear ANM with Gaussian noise are named _nonlinear-gaussian_. More details on the synthetic data generation schema are found in Appendix B.2. All data are standardized by their empirical variance to remove opportunities to learn shortcuts [31; 32; 33].

Metric and random baseline.As our metric we use the structural Hamming distance (SHD), which is the number of edge removals, insertions or flips to transform one graph to another. In the context of bivariate causal graphs with a single edge, this is simply an error count, so correct inference corresponds to \(\text{SHD}=0\), and an incorrect prediction gives \(\text{SHD}=1\). Additionally, we define a reference random baseline, which assigns a causal direction according to a fair coin, achieving \(\text{SHD}=0.5\) in expectation. Each architecture we analyze in the experiments is trained \(3\) times, with different parameter initialization and training samples: the SHD presented in the plots is the average of each of the \(3\) models on \(1500\) distinct test datasets of \(1500\) points each, and the error bars are \(95\%\) confidence intervals.

We detail the training hyperparameters in Appendix B.1. Next, we analyze our experimental results, starting by investigating how well CSIvA generalizes on distributions unseen during training.

### Warm up: is CSIvA capable of in and out-of-distribution generalization?

In-distribution generalization.First, we investigate the generalization of CSIvA on datasets sampled from the structural casual model that generates the train distribution, with mechanisms and noise distributions fixed between training and testing. We call this _in-distribution generalization_. As a benchmark, we present the performance of several state-of-the-art approaches from the literature on causal discovery: we consider the DirectLiNGAM, and NoGAM algorithms [34; 35], respectively designed for the inference on LiNGAM and nonlinear ANM generated data1. The results of Figure 1show that CSIvA can properly generalize to unseen samples from the training distribution: the majority of the trained models present SHD close to zero and comparable to the relative benchmark algorithm.

Footnote 1: https://github.com/google-research/sHD

Out-of-distribution generalization.In practice, we generally do not know the SCM defining the test distribution, so we are interested in CSIvA's ability to generalize to data sampled from a class of causal models that is unobserved during training. We call this _out-of-distribution generalization_ (OOD). We study OOD generalization to different noise terms, analyzing the network performance on datasets generated from causal models where the mechanisms are fixed with respect to the training, while the noise distribution varies (e.g., given linear-mlp training samples, testing occurs on linear-uniform data). Orthogonally to these experiments, we empirically validate CSIvA's OOD generalization over different mechanism types (linear, nonlinear, post-nonlinear), while leaving the noise distribution (mlp) fixed across test and training. In Figure 1(a), we observe that CSIvA cannot generalize across the different mechanisms, as the SHD of a network tested on unseen causal mechanisms approximates that of the random baseline. Further, Figure 1(b) shows that out-of-distribution generalization across noise terms does not work reliably, and it is hard to predict when it might occur.

Implications.CSIvA generalizes well to test data generated by the same class of SCMs used for training, in line with the findings in Ke et al. [1], which validates our implementation and training procedure. However, it struggles when the test data are out-of-distribution, not generated by causal models with the _same mechanisms and noise terms_ it was trained on. While training on a wider class of SCMs might overcome this limitation, it requires caution. The identifiability of causal graphs indeed results from the interplay between the data-generating mechanisms and noise distribution. However, as we argue in our Example 1, the class of causal models that a supervised learning algorithm can identify is generally not clear. In what follows, we investigate this point and its implications for CSIvA, showing that the identifiability of the test samples can be ensured by imposing suitable assumptions on the class of SCMs generating the training distribution.

### How does CSIvA relate to identifiability theory for causal graphs?

The CSIvA algorithm does not make structural assumptions about the causal model underlying the input data. This implies that the output of this method is unclear: as CSIvA targets the conditional distribution \(p(\cdot|\mathcal{D})\) over the space of graphs, in the absence of restrictions on the functional mechanisms

Figure 1: In-distribution generalization of CSIvA trained and tested on data generated according to the same structural causal models, fixing mechanisms, and noise distributions between training and testing). As baselines for comparison, we use DirectLiNGAM on linear SCMs and NoGAM on nonlinear ANM (we use their causal-learn and dodiscover implementations). CSIvA performance is clearly non-trivial and generalizing well.

and the distribution of the noise terms, the causal graph \(X\to Y\) is indistinguishable from \(Y\to X\), as they are both equally likely to underlie the joint density \(p_{X,Y}\) generating the data. As we discuss in Example 1, the graphical output of the trained architecture could at most identify the equivalence class of the true causal graph. Yet, our experiments of Section 3.2 show that CSIvA is capable of good in-distribution generalization, often inferring the correct DAG at test time. We explain this seeming contradiction with the following hypothesis, which motivates the analysis in the remainder of this section.

**Hypothesis 1**.: _The class of structural causal models that can be identified by CSIvA is defined by the class of structural causal models underlying the generation of the training data._

To support and clarify our statement, we present the following example, adapted from Hoyer et al. [7].

**Example 2**.: Consider the causal model \(Y=f(X)+N,\) where \(f(X)=-X\) and \(p_{X},p_{N}\) are Gumbel densities \(p_{X}(x)=\exp(-x-\exp(-x))\) and \(p_{N}(n)=\exp(-n-\exp(-n))\). This model satisfies the assumptions of the LiNGAM, so it is identifiable, in the sense that a backward linear model with the same distribution does not exist. However, in this special case, we can build a backward nonlinear additive noise model \(X=g(Y)+\tilde{N}\) with independent noise terms: taking \(p_{Y}(y)=\exp(-y-2\log(1+\exp(-y)))\) to be the density of a logistic distribution, \(p_{\tilde{N}}(\tilde{n})=\exp(-2\tilde{n}-\exp(-\tilde{n}))\) and \(g(y)=\log(1+\exp(-y))\); we see that \(p_{X,Y}\) can factorize according to two opposite causal directions, as \(p_{X,Y}(x,y)=p_{N}(y-f(x))p_{X}(x)=p_{\tilde{N}}(x-g(y))p_{Y}(y)\). Given a dataset \(\mathcal{D}\) of observations from the forward linear model, causal discovery methods like DirectLiNGAM [34] can provably identify the correct causal direction \(X\to Y\), assuming that sufficient samples are provided. Instead, the behavior of CSIvA seems hard to predict: given that the network approximates the conditional distribution \(p(\cdot|\mathcal{D})\) over the possible graphs, for \(\mathcal{D}\) with arbitrary many samples we have \(p(X\to Y|\mathcal{D})=p(Y\to X|\mathcal{D})=0.5\). On the other hand, given the prior knowledge that the data-generating SCM is a linear non-gaussian additive noise model, we have \(p(X\to Y|\mathcal{D},\text{LiNGAM})=1\), because the LiNGAM is identifiable. In this sense, the class of structural causal models that CSIvA correctly infers appears to be determined by the structural causal models underlying the generation of the training data. Under our Hypothesis 1, training CSIvA exclusively on LiNGAM-generated data is equivalent to learning the distribution \(p(\cdot|\mathcal{D},\text{LiNGAM})\), such that the network should be able to identify the forward linear model, whereas it could only infer the equivalence class of the causal graph if its training datasets include observations from a nonlinear additive noise model.

The empirical results of Figure 2(a) show that CSIvA behaves according to our hypothesis: when training exclusively occurs on datasets \(\{\mathcal{D}_{i,\rightarrow}\}_{i}\) generated by the _forward linear-gumbel model_ of Example 2, the network can identify the causal direction of test data generated according to the same SCM. Similarly, the transformer trained on datasets \(\{\mathcal{D}_{i,\leftarrow}\}_{i}\) from the _backward nonlinear model_ of the example can generalize to test data coming from the same distribution. According to our claim, instead, the network that is trained on the union of the training samples \(\{\mathcal{D}_{i,\rightarrow}\}_{i}\cup\{\mathcal{D}_{i,\leftarrow}\}_{i}\) from the forward and backward models (_50:50_ ratio in Figure 2(a)) displays the same test SHD (around \(0.5\)) as a random classifier assigning the causal direction with equal probability.

Figure 2: Out-of-distribution generalisation. We train three CSIvA models on data sampled from SCMs with linear, nonlinear additive, and post-nonlinear mechanisms; and noise fixed _mlp_ noise distribution. In Figure (a) we test across different noise distributions, with test mechanisms fixed from training. In Figure (b) we test each network on different mechanisms and fixed mlp noise. CSIvA struggles to generalize to unseen causal mechanisms and often displays degraded performance over new noise distributions.

Further, we investigate CSIvA's relation with known identifiability theory by training and testing the architecture on data from a linear Gaussian model, which is well-known to be unidentifiable. Not surprisingly, the results of Figure 2(b) show that none of the algorithms that we learn can infer the causal order of linear Gaussian models with test SHD any better than a random baseline.

Implications.Our experiments show that CSIvA learns algorithms that closely follow identifiability theory for causal discovery. In particular, while the method itself does not require explicit assumptions on the data-generating process, the chosen training data ultimately determines the class of causal models identifiable during inference. Notably, previous work has argued that supervised learning approaches in causal discovery would help with "dealing with complex data-generating processes and greatly reduce the need of explicitly crafting identifiability conditions a-priori", Lopez-Paz et al. [10]. In the case of CSIvA, this expectation does not appear to be fulfilled, as the assumptions still need to be encoded explicitly in the training data. However, this observation opens two new important questions: (1) Can we train a single network to encompass multiple (or even all) identifiable causal structures? (2) How much ambiguity might exist between these identifiable models?

### A _low-dimensions_ argument in favor of learning from multiple causal models

Example 2 of the previous section shows that elements of distinct classes of identifiable structural causal models, such as LiNGAM and nonlinear ANM, may become non-identifiable when we consider their union. In this section, we show that in the class of post-additive noise models given by equation (2) (obtained as the union of the LiNGAM, the nonlinear ANM, and the post-nonlinear model), the set of distributions that is non-identifiable is negligible. Our proposition extends the results of Hoyer et al. [7], which are limited to the case of linear and nonlinear additive noise models, and Zhang and Hyvarinen [8], which provides the conditions of identifiability of the post-ANM without bounding the set of non-identifiable distributions.

Let \(X,Y\) be a pair of random variables generated according to the causal direction \(X\to Y\) and the post-additive noise model structural equation:

\[Y=f_{2}(f_{1}(X)+N_{Y}),\] (5)

where \(N_{Y}\) and \(X\) are independent random variables, and \(f_{2}\) is invertible. If the SCM is non-identifiable, the data-generating process can be described by a _backward_ model with the structural equation:

\[X=g_{2}(g_{1}(Y)+N_{X}),\] (6)

\(N_{X}\) independent from \(Y\), and \(g_{2}\) invertible. We introduce the random variables \(\tilde{X},\tilde{Y}\), such that the forward and backward equations can be rewritten as

\[Y=f_{2}(\tilde{Y}),\quad\tilde{Y}\coloneqq f_{1}(X)+N_{Y},\] \[X=g_{2}(\tilde{X}),\quad\tilde{X}\coloneqq g_{1}(Y)+N_{X}.\]

Figure 3: Experiments on identifiability theory. In Figure (a) we test the performance on linear-Gaussian data. Models are trained with different ratios of samples from linear and nonlinear SCMs with Gaussian noise terms. The validation results showcase that the networks were trained successfully. Figure (b) shows the SHD of models trained on different ratios of _linear_ and _nonlinear invertible_ data of Example 2. CSIvA behaves according to identifiability theory, failing to predict on linear Gaussian models and _invertible_ data (50:50 ratio).

We note that this implies that the following invertible additive noise models on \(\tilde{X},\tilde{Y}\) hold:

\[\tilde{Y} =h_{Y}(\tilde{X})+N_{Y},\quad h_{Y}\coloneqq f_{1}\circ g_{2},\] (7) \[\tilde{X} =h_{X}(\tilde{Y})+N_{X},\quad h_{X}\coloneqq g_{1}\circ f_{2}.\] (8)

**Proposition 1** (Adapted from Hoyer et al. [7]).: _Let \(p_{N_{Y}},h_{X},h_{Y}\) be fixed, and define \(\nu_{Y}\coloneqq\log p_{N_{Y}}\), \(\xi\coloneqq\log p_{\tilde{X}}\). Suppose that \(p_{N_{Y}}\) and \(p_{\tilde{X}}\) are strictly positive densities, and that \(\nu_{Y},\xi,f_{1},f_{2},g_{1}\), and \(g_{2}\) are three times differentiable. Further, assume that for a fixed pair \(h_{Y},\nu_{Y}\) exists \(\tilde{y}\in\mathbb{R}\) s.t. \(\nu_{Y}^{\prime}(\tilde{y}-h_{Y}(\tilde{x}))h_{Y}^{\prime}(\tilde{x})\neq 0\) is satisfied for all but a countable set of points \(\tilde{x}\in\mathbb{R}\). Then, the set of all densities \(p_{\tilde{X}}\) of \(\tilde{X}\) such that both equations (5) and (6) are satisfied is contained in a 2-dimensional space._

Implications.Our result is closely related to Theorem 1 of Hoyer et al. [7], which we simply generalize to the post-ANM. Intuitively, it says that the space of all continuous distributions such that the bivariate post-ANM is non-identifiable is contained in a 2-dimensional space. As the space of continuous distributions of random variables is infinite-dimensional, we conclude that the post-ANM is generally identifiable, which suggests that the setting of Example 2 is rather artificial. Our results provide a theoretical ground for training causal discovery algorithms on datasets generated from multiple identifiable SCMs. This is particularly appealing in the case of CSIvA, given the poor OOD generalization ability observed in our experiments of Section 3.2.

### Can we train CSIvA on multiple causal models for better generalization?

In this section, we investigate the benefits of training over multiple causal models, i.e. on samples generated by a combination of classes of identifiable SCMs characterized by different mechanisms and noise terms distribution. Our motivation is as follows: given that our empirical evidence shows that CSIvA is capable of in-distribution generalization, whereas dramatically degrades the performance when testing occurs out-of-distribution, it is thus desirable to increase the class of causal models represented in the training datasets. We separately study the effects of training over multiple mechanisms and multiple noise distributions and compare the testing performance against architectures trained on samples of a single SCM.

Mixture of causal mechanisms.We consider four networks optimized by training of CSIvA on datasets generated from pairs (or triples) of distinct SCMs, with fixed _mlp_ noise and which differ in terms of their mechanisms type: linear and nonlinear; nonlinear and post-nonlinear; linear and post-nonlinear; linear and post-nonlinear; linear, nonlinear and post-nonlinear. The number of training datasets for each architecture is fixed (\(15000\)) and equally split between the causal models with different mechanism types. The results of Figure 4 show that the networks trained on mixtures of mechanisms all present significantly better test SHD compared to CSIvA models trained on a single mechanism type. We find that learning on multiple SCMs improves the SHD from \(\sim\!0.5\) to \(\sim\!0.2\) both on linear and nonlinear test data (Figures 3(a) and 3(b)), and even better accuracy is achieved on post-nonlinear samples, as shown in Figure 3(c).

Figure 4: Mixture of causal mechanisms. We train four models on samples from structural casual models with different mechanism types. We compare their test SHD (the lower, the better) against networks trained on datasets generated according to a single type of mechanism. The dashed line indicates the test SHD of a model trained on samples with the same mechanisms as test SCM. Training on multiple causal models with different mechanisms (_mixed_ bars) always improves performance compared to training on single SCMs.

Mixture of noise distributions.Next, we analyze the test performance of three CSIvA networks optimized on samples from structural causal models that have different distributions for their noise terms, while keeping the mechanism types fixed. Figure 5 shows that training over different noises (beta, gamma, gumbel, exponential, mlp, uniform) always results in a network that is agnostic with respect to the noise distributions of the SCM generating the test samples, always achieving \(\text{SHD}<0.1\), with the exception of datasets with mlp error terms (\(0.2\) average SHD on nonlinear and pnl data).

Implications.We have shown that learning on mixtures of SCMs with different noise term distributions and mechanism types leads to models generalizing to a much broader class of structural causal models during testing. Hence, combining datasets generated from multiple models looks like a promising framework to overcome the limited out-of-distribution generalization abilities of CSIvA observed in Section 3.2. However, it is easier to incorporate prior assumptions on the class of causal mechanisms (linear, non-linear, post-non-linear) compared to the noise distributions (which are potentially infinite). This introduces a trade-off between amortized inference and classical methods for causal discovery: for example, RESIT, NoGAM, and CAM [23, 35, 36] algorithms require no assumptions on the noise type, but only work for a limited class of mechanisms (nonlinear).

## 4 Conclusion

In this work, we investigate the interplay between identifiability theory and supervised learning for amortized inference of causal graphs, using CSIvA as the ground of our study. Consistent with classical algorithms, we demonstrate that good performance can be achieved if (i) we have a good prior on the structural causal model generating the test data (ii) the setting is identifiable. In particular, prior knowledge of the test distribution is encoded in the training data in the form of constraints on the structural causal model underlying their generation. With these results, we highlight the need for identifiability theory in modern learning-based approaches to causality, while past works have mostly disregarded this connection. Further, our findings provide the theoretical ground for training on observations sampled from multiple classes of identifiable SCMs, a strategy that improves test generalization to a broad class of causal models. Finally, we highlight an interesting new trade-off regarding identifiability: traditional methods like LiNGAM, RESIT, and PNL require strong restrictions on the structural mechanisms underlying the data generation (linear, nonlinear or post-nonlinear) while generally being agnostic relative to the noise terms distribution. Training on mixtures of causal models instead offers an alternative that is less reliant on assumptions on the mechanisms, while incorporating knowledge about all possible noise distributions in the training data is practically impossible to achieve. We leave it to future work to reproduce our analysis on a wider class of architectures, as well as extending our study to interventional data with more than two nodes.

Figure 5: Mixture of noise distributions. We train three networks on samples from SCMs with different noise terms distributions and fixed mechanism types: linear, nonlinear, and post-nonlinear. We present their test SHD (the lower, the better) on data from SCMs with the mechanisms fixed with respect to training, and noise terms changing between each dataset. Training on multiple causal models with different noises (_all distributions_ bars) always improves performance compared to training on single SCMs with fixed mlp noise (_only mlp_ bars).

## References

* Ke et al. [2022] Nan Rosemary Ke, Silvia Chiappa, Jane X. Wang, Jorg Bornschein, Anirudh Goyal, Melanie Rey, Theophane Weber, Matthew Botvinick, Michael Curtis Mozer, and Danilo Jimenez Rezende. Learning to Induce Causal Structure. In _International Conference on Learning Representations_, September 2022. URL https://openreview.net/forum?id=hp_RwhKDJ5.
* Peters et al. [2017] Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. _Elements of Causal Inference: Foundations and Learning Algorithms_. Adaptive Computation and Machine Learning. The MIT Press, Cambridge, Mass, 2017. ISBN 978-0-262-03731-0.
* Pearl [2009] Judea Pearl. _Causality_. Cambridge University Press, Cambridge, 2nd edition, 2009.
* Spirtes [2010] Peter Spirtes. Introduction to causal inference. _Journal of Machine Learning Research_, 11(54):1643-1662, 2010. URL http://jmlr.org/papers/v11/spirtes10a.html.
* Glymour et al. [2019] Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical models. _Frontiers in Genetics_, 10, 2019. ISSN 1664-8021. doi: 10.3389/fgene.2019.00524. URL https://www.frontiersin.org/articles/10.3389/fgene.2019.00524.
* Shimizu et al. [2006] Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvarinen, and Antti Kerminen. A linear non-gaussian acyclic model for causal discovery. _Journal of Machine Learning Research_, 7:2003-2030, dec 2006. ISSN 1532-4435.
* Hoyer et al. [2008] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlinear causal discovery with additive noise models. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, _Advances in Neural Information Processing Systems_, volume 21. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2008/file/f7664060cc52bc6f3d620bccdc94a4b6-Paper.pdf.
* Zhang and Hyvarinen [2009] Kun Zhang and Aapo Hyvarinen. On the identifiability of the post-nonlinear causal model. In _Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence_, UAI '09, page 647-655, Arlington, Virginia, USA, 2009. AUAI Press. ISBN 9780974903958.
* Montagna et al. [2023] Francesco Montagna, Atalanti Mastakouri, Elias Eulig, Nicoletta Nocetti, Lorenzo Rosasco, Dominik Janzing, Bryon Aragam, and Francesco Locatello. Assumption violations in causal discovery and the robustness of score matching. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 47339-47378. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/93ed74938a54a73b5e4c52bbaf42ca8e-Paper-Conference.pdf.
* Volume 37_, ICML'15, page 1452-1461. JMLR.org, 2015.
* Li et al. [2020] Hebi Li, Qi Xiao, and Jin Tian. Supervised Whole DAG Causal Discovery, June 2020.
* Lippe et al. [2022] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without acyclicity constraints. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=eYciPrLuUhG.
* Lorch et al. [2022] Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, and Bernhard Scholkopf. Amortized inference for causal structure learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=eVAJI-MMeX.
* Szabo et al. [2016] Zoltan Szabo, Bharath Sriperumbudur, Barnabas Poczos, and Arthur Gretton. Learning theory for distribution regression. _Journal of Machine Learning Research_, 17:1-40, 09 2016.

* Brouillard et al. [2020] Philippe Brouillard, Sebastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin. Differentiable causal discovery from interventional data. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 21865-21877. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/f8b7aa3a0d349d9562b424160ad18612-Paper.pdf.
* Ke et al. [2023] Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard Scholkopf, Michael Curtis Mozer, Christopher Pal, and Yoshua Bengio. Neural causal structure discovery from interventions. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=rdHVPPVuXa. Expert Certification.
* Scherrer et al. [2020] Nino Scherrer, Olexa Bilaniuk, Yashas Annadani, Anirudh Goyal, Patrick Schwab, Bernhard Scholkopf, Michael C. Mozer, Yoshua Bengio, Stefan Bauer, and Nan Rosemary Ke. Learning neural causal models with active interventions, 2022.
* Lachapelle et al. [2020] Sebastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-based neural dag learning. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=rklbK4A7DS.
* Ng et al. [2020] Ignavier Ng, AmirEmad Ghassami, and Kun Zhang. On the role of sparsity and dag constraints for learning linear dags. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS '20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.
* Zheng et al. [2018] Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Dags with no tears: Continuous optimization for structure learning. In _Neural Information Processing Systems_, 2018. URL https://api.semanticscholar.org/CorpusID:53217974.
* Zhang et al. [2022] Zhen Zhang, Ignavier Ng, Dong Gong, Yuhang Liu, Ehsan M Abbasnejad, Mingming Gong, Kun Zhang, and Javen Qinfeng Shi. Truncated matrix power iteration for differentiable DAG learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=I4aSjFR7j0m.
* Bello et al. [2022] Kevin Bello, Bryon Aragam, and Pradeep Kumar Ravikumar. DAGMA: Learning DAGs via m-matrices and a log-determinant acyclicity characterization. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=8rZYMpFUgK.
* Peters et al. [2014] Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Scholkopf. Causal discovery with continuous additive noise models. _J. Mach. Learn. Res._, 15(1):2009-2053, jan 2014. ISSN 1532-4435.
* Lowe et al. [2020] Sindy Lowe, David Madras, Richard S. Zemel, and Max Welling. Amortized causal discovery: Learning to infer causal graphs from time-series data. In _CLEaR_, 2020. URL https://api.semanticscholar.org/CorpusID:219955853.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
* Jin et al. [2024] Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, et al. Cladder: A benchmark to assess causal reasoning capabilities of language models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Zhang et al. [2024] Jiaqi Zhang, Joel Jennings, Agrin Hilmkil, Nick Pawlowski, Cheng Zhang, and Chao Ma. Towards causal foundation model: on duality between causal inference and attention, 2024.

* Scetbon et al. [2024] Meyer Scetbon, Joel Jennings, Agrin Hilmkil, Cheng Zhang, and Chao Ma. Fip: a fixed-point approach for causal generative modeling, 2024.
* Gupta et al. [2023] Shantanu Gupta, Cheng Zhang, and Agrin Hilmkil. Learned causal method prediction, 2023.
* Uhler et al. [2012] Caroline Uhler, G. Raskutti, Peter Buhlmann, and B. Yu. Geometry of the faithfulness assumption in causal inference. _The Annals of Statistics_, 41, 07 2012. doi: 10.1214/12-AOS1080.
* Geirhos et al. [2020] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2:665-673, 11 2020. doi: 10.1038/s42256-020-00257-z.
* Reisach et al. [2021] Alexander G. Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated dag! causal discovery benchmarks may be easy to game. In _Neural Information Processing Systems_, 2021. URL https://api.semanticscholar.org/CorpusID:239998404.
* Montagna et al. [2023] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, and Francesco Locatello. Shortcuts for causal discovery of nonlinear models by score matching, 2023.
* Shimizu et al. [2011] Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen, Yoshinobu Kawahara, Takashi Washio, Patrik Hoyer, and Kenneth Bollen. DirectLiNGAM: A direct method for learning a linear non-gaussian structural equation model. _Journal of Machine Learning Research_, 12, 01 2011.
* Montagna et al. [2023] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello. Causal discovery with score matching on additive models with arbitrary noise. In _2nd Conference on Causal Learning and Reasoning_, 2023. URL https://openreview.net/forum?id=rVO0Bx90deu.
* Buhlmann et al. [2014] Peter Buhlmann, Jonas Peters, and Jan Ernest. CAM: Causal additive models, high-dimensional order search and penalized regression. _The Annals of Statistics_, 42(6), dec 2014. URL https://doi.org/10.1214%2F14-aos1260.
* Kossen et al. [2021] Jannik Kossen, Neil Band, Clare Lyle, Aidan Gomez, Tom Rainforth, and Yarin Gal. Self-attention between datapoints: Going beyond individual input-output pairs in deep learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=wRXzDa2z5T.
* Lin [1997] Juan Lin. Factorizing multivariate function classes. In M. Jordan, M. Kearns, and S. Solla, editors, _Advances in Neural Information Processing Systems_, volume 10. MIT Press, 1997. URL https://proceedings.neurips.cc/paper_files/paper/1997/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf.

## Appendix A Learning to induce: causal discovery with transformers

### A supervised learning approach to causal discovery

First, we describe the training procedure for the CSIvA architecture, which aims to learn the distribution of causal graphs conditioned on observational and/or interventional datasets. We omit interventional datasets from the discussion as they are not of interest to our work. Training data are generated from the joint distribution \(p_{\mathcal{G},\mathcal{D}}\) between a graph \(\mathcal{G}\) and a dataset \(\mathcal{D}\). First, we sample a set of directed acyclic graphs \(\{\mathcal{G}^{i}\}_{i=1}^{n}\) with nodes \(X_{1},\ldots,X_{d}\), from a distribution \(p_{\mathcal{G}}\). Then, for each graph we sample a dataset of \(m\) observations of the graph nodes \(\mathcal{D}^{i}=\{x_{1}^{j},\ldots,x_{d}^{j}\}_{j=1}^{m}\), \(i=1,\ldots,n\). Hence, we build a training dataset \(\{\mathcal{G}^{i},\mathcal{D}^{i}\}_{i=1}^{n}\).

The CSIvA model defines a distribution \(\hat{p}_{\mathcal{G}|\mathcal{D}}(\cdot;\Theta)\) of graphs conditioned on the observational data and parametrized by \(\Theta\). Given an invertible map \(\mathcal{G}\mapsto A\) from a graph to its binary adjacency matrix representation of \(d\times d\) entries (where \(A_{ij}=1\) iff \(X_{i}\to X_{j}\) in \(\mathcal{G}\)), we consider an equivalent estimated distribution \(\hat{p}_{A|\mathcal{D}}(\cdot;\Theta)\), which has the following autoregressive form:

\[\hat{p}_{A,\mathcal{D}}(A|\mathcal{D};\Theta)=\prod_{l=1}^{d^{2}}\sigma(A_{l}; \rho=f_{\Theta}(A_{1},\ldots,A_{l-1},\mathcal{D})),\]

where \(\sigma(\cdot;\rho)\) is a Bernoulli distribution parametrized by \(\rho\). \(\rho\) itself is a function of \(f_{\Theta}\) defined by the encoder-decoder transformer architecture, taking as input previous elements of the matrix \(A\) (here represented as a vector of \(d^{2}\) entries) and the dataset \(\mathcal{D}\). \(\Theta\) is optimized via maximum likelihood estimation, i.e. \(\Theta^{*}=\operatorname*{argmin}_{\Theta}-\mathbf{E}_{\mathcal{G},\mathcal{D }}[\ln\hat{p}(\mathcal{G}|\mathcal{D};\Theta)]\), which corresponds to the usual cross-entropy loss for the Bernoulli distribution. Training is achieved using stochastic gradient descent, in which each gradient update is performed using a pair \((\mathcal{D}^{i},A^{i})\), \(i=1\ldots,d\). In the infinite sample limit, we have \(\hat{p}_{\mathcal{G}|\mathcal{D}}(\cdot;\Theta^{*})=p_{\mathcal{G}|\mathcal{D }}(\cdot)\), while in the finite-capacity case, it is only an approximation of the target distribution.

### CSIvA architecture

In this section, we summarize the architecture of CSIvA, a transformer neural network that can learn a map from data to causally interpreted graphs, under supervised training.

Transformer neural network.Transformers [25] are a popular neural network architecture for modeling structured, sequential data data. They consist of an _encoder_, a stack of layers that learns a representation of each element in the input sequence based on its relation with all the other sequence's elements, through the mechanism of self-attention, and a decoder, which maps the learned representation to the target of interest. Note that data for causal discovery are not sequential in their nature, which motivates the adaptations introduced by Ke et al. [1] in their CSIvA architecture.

CSIVA embeddings.Each element \(x_{i}^{j}\) of an input dataset is embedded into a vector of dimensionality \(E\). Half of this vector is allocated to embed the value \(x_{i}^{j}\) itself, while the other half is allocated to embed the unique identity for the node \(X_{i}\). We use a node-specific embedding because the values of each node may have very different interpretations and meanings. The node identity embedding is obtained using a standard 1D transformer positional embedding over node indices. The value embedding is obtained by passing \(x_{i}^{j}\), through a multi-layer perceptron (MLP).

CSIVA alternating attention.Similarly to the transformer's encoder, CSIvA stacks a number of identical layers, performing self-attention followed by a nonlinear mapping, most commonly an MLP layer. The main difference relative to the standard encoder is in the implementation of the self-attention layer: as transformers are in their nature suitable for the representation of sequences, given an input sample of \(D\) elements, self-attention is usually run across all elements of the sequence. However, data for causal discovery are tabular, rather than sequential: one option would be to unravel the \(n\times d\) matrix of the data, where \(n\) is the number of observations and \(d\) the number of variables, into a vector of \(n\cdot d\) elements, and let this be the input sequence of the encoder. CSIvA adopts a different strategy: the self-attention in each encoder layer consists of alternate passes over the attribute and the sample dimensions, known as _alternating attention_[37]. As a clarifying example, consider a dataset \(\{(x_{1}^{i},x_{2}^{i})\}_{i=1}^{n}\) of \(n\) i.i.d. samples from the joint distribution of the pair of random variables \(X_{1},X_{2}\). For each layer of the encoder, in the first step (known as _attention between attributes_), attention operates across all nodes of a single sample \((x_{1}^{i},x_{2}^{i})\) to encode the relationships between the two nodes. In the second step (_attention between samples_), attention operates across all samples \((x_{k}^{1},\ldots,x_{k}^{n}),k\in\{1,2\}\) of a given node, to encode information about the distribution of single node values.

CSIVA encoder summary.The encoder produces a summary vector \(s_{i}\) with \(H\) elements for each node \(X_{i}\), which captures essential information about the node's behavior and its interactions with other nodes. The summary representation is formed independently for each node and involves combining information across the \(n\) samples. This is achieved with a method often used with transformers that involves a weighted average based on how informative each sample is. The weighting is obtained using the embeddings of a summary "sample" \(n+1\) to form queries, and embeddings of node's samples \(\{x_{i}^{j}\}_{j=1}^{n}\) to provide keys and values, and then using standard key-value attention.

CSIVA decoder.The decoder uses the summary information from the encoder to generate a prediction of the adjacency matrix \(A\) of the underlying \(\mathcal{G}\). It operates sequentially, at each step producing a binary output indicating the prediction \(\hat{A}_{i,j}\) of \(A_{i,j}\), proceeding row by row. The decoder is an autoregressive transformer, meaning that each prediction \(\hat{A}_{i,j}\) is obtained based on all elements of \(A\) previously predicted, as well as the summary produced by the encoder. The method does not enforce acyclicity, although Ke et al. [1] shows that in cyclic outputs genereally don't occur, in practice.

## Appendix B Training details

### Hyperparameters

In Table 1 we detail the hyperparameters of the training of the network of the experiments. We define an iteration as a gradient update over a batch of \(5\) datasets. Models are trained until convergence, using a patience of \(5\) (training until five consecutive epochs without improvement) on the validation loss - this always occurs before the \(25\)-th epoch (corresponding to \(\approx 150000\) iterations). The batch size is limited to \(5\) due to memory constraints.

### Synthetic data

In this section, we provide additional details on the synthetic data generation, which was performed with the causally2 Python library [9]. Our data-generating framework follows that of Montagna et al. [9], an extensive benchmark of causal discovery methods on different classes of SCMs.

Footnote 2: https://causally.readthedocs.io/en/latest/

Causal mechanisms.The _nonlinear mechanisms_ of the PNL model and the nonlinear ANM model are generated by a neural network with one hidden layer with \(10\) hidden units, with a parametric ReLU activation function. The network weights are randomly sampled according to a standard Gaussian distribution. The _linear mechanisms_ are generated by sampling the regression coefficients in the range \([-3,-0.5]\cup[0.5,3]\).

Distribution of the noise terms.We generated datasets from structural causal models with the following distribution of the noise terms: Beta, Gamma, Gaussian (for nonlinear data), Gumbel, Exponential, and Uniform. Additionally, we define the _mlp_ distribution by nonlinear transformations of gaussian samples from a guassian distribution centered at zero and with standard deviation \(\sigma\) uniformly sampled between \(0.5\) and \(1\). The nonlinear transformation is parametrized by a neural network with one hidden layer with \(100\) units, and sigmoid activation function. The weights of the network are uniformly sampled in the range \([-1.5,1.5]\). We additionally standardized the output of each _mlp_ sample by the empirical variance computed over all samples.

Data are standardized with their empirical variance, which removes the presence of shortcuts which could be learned by the network, notably _varsortability_[32] and _score-sortability_[33].

\begin{table}
\begin{tabular}{l c} \hline \hline
**Hyperparameter** & **Value** \\ \hline Hidden state dimension & 64 \\ Encoder transformer layers & 8 \\ Decoder transformer layers & 8 \\ Num. attention heads & 8 \\ Optimizer & Adam \\ Learning rate & \(10^{-4}\) \\ Samples per dataset (\(n\)) & \(1500\) \\ Num. training datasets & \(15000\) \\ Num. iterations & \(<150000\) \\ Batch size & \(5\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Hyperparameters for the training of the CSIvA models of the experiments in Section 3.

### Computer resources

Our experiments were run on a local computing cluster, using any and all available GPUs (all NVIDIA). For replication purposes, GTX 1080 Ti's are entirely suitable, as the batch size was set to match their memory capacity, when working with bivariate graphs. All jobs ran with \(10\mathrm{GB}\) of RAM and \(4\) CPU cores. The results presented in this paper were produced after \(145\) days of GPU time, of which \(68\) were on GTX 1080 Ti's, \(13\) on RTX 2080 Ti's, \(11\) on A10s, \(19\) on A40s, and \(35\) on RTX 3090s. Together with previous experiments, while developing our code and experimental design, we used 376 days of GPU time (for reference, at a total cost of \(492.14\) Euros), similarly split across whichever GPUs were available at the time: \(219\) on GTX 1080 Ti's, \(38\) on RTX 2080 Ti's, \(18\) on A10s, \(63\) on RTX 3090s, \(31\) on A40s, and \(6\) on A100s.

## Appendix C Further experiments

We present our experimental results on one further question, to help clarify the results in the main text of the paper. Our aim is to understand when to make tradeoffs between computational resources, and having models that have been trained on a wider variety of SCMs. We compare training on multiple SCMs to single-SCM training, when all models see the same amount of training data from each SCM type as a non-mixed model (i.e. a mixed network trains on \(15,000\) linear datasets and \(15,000\) PNL datasets, instead of \(15,000\) divided between the two SCM types).

In the main text of this paper, we compare neural networks trained on a mix of structural causal models (e.g. noise distributions, or mechanism types), to models trained on a single mechanism-noise combination, where all models have the same amount of training data, \(15,000\) datasets. In mixed training, we split these evenly, so a "lin, nl" model is trained on \(7,500\) datasets from linear SCMs, and \(7,500\) from nonlinear SCMs. Our results in this framework are promising, and show that for many combinations of SCM types, we can train one model instead of two, and achieve good progress, while making a \(50\%\) savings on training costs. However, if our training budget is high/unlimited, we should also ask whether there is a downside to mixed training - can we achieve the same performance as a model trained on a single SCM type? Fig. 6 shows good results in this direction - the models trained with the same number of datasets per SCM type as an unmixed model had similar (or even better, for PNL data) performance as the un-mixed model trained on the same SCM type as the test data. These mixed models are also significantly more useful than having 2 or 3 separate models per SCM type, as they have good across-the-board performance. However, if we used the same computational resources to train 3 separate networks (one for each mechanism type) and wanted to use them for causal discovery on a dataset with unknown assumptions, we would be left with the rather difficult task of deciding which model to trust.

Figure 6: Mixtures of causal mechanisms, with varying amounts of training data. We train eight models on samples from structural causal models with different mechanisms. Four (in purple), were trained on \(15,000\) samples for each SCM type (so the lin,nl model saw \(30,000\) samples in total, and the all model saw \(45,000\)), and the other four (blue) are the same as in Fig. 4, and were trained on \(15,000\) samples in total, evenly split between the SCM types they were trained on. We compare their test SHD (the lower, the better) against networks trained on datasets generated according to a single type of mechanism. The dashed line indicates the test SHD of a model trained on samples with the same mechanisms as the test SCM. Training on multiple causal models with different mechanisms (mixed bars) always improves performance compared to training on single SCMs.

[MISSING_PAGE_FAIL:16]

from which follows:

\[\frac{\partial}{\partial\tilde{x}}\left(\frac{\frac{\partial^{2}}{ \partial\tilde{x}^{2}}\pi(\tilde{x},\tilde{y})}{\frac{\partial^{2}}{\partial \tilde{x}\partial\tilde{y}}\pi(\tilde{x},\tilde{y})}\right) =-2h_{Y}^{\prime\prime}+\frac{\nu_{Y}^{\prime}h_{Y}^{\prime \prime}}{\nu_{Y}^{\prime}h_{Y}^{\prime}}-\frac{\xi^{\prime\prime\prime}}{\nu_ {Y}^{\prime\prime}h_{Y}^{\prime}}+\frac{\nu_{Y}^{\prime\prime}\nu_{Y}^{\prime }h_{Y}^{\prime\prime}}{(\nu_{Y}^{\prime\prime})^{2}}-\frac{\nu_{Y}^{\prime}(h_ {Y}^{\prime\prime})^{2}}{\nu_{Y}^{\prime\prime}(h_{Y}^{\prime})^{2}}+\frac{ \xi^{\prime\prime\prime}\nu_{Y}^{\prime\prime\prime}h_{Y}^{\prime\prime}}{( \nu_{Y}^{\prime\prime})^{2}\nu_{Y}^{\prime\prime}(h_{Y}^{\prime})^{2}}\] \[=0.\]

where we drop the input arguments for conciseness. The equality with \(0\) is given by the equality with \((12)\). Manipulating the above expression, the first claim follows.

Part 2.Next, we prove the constraint derived on \(h_{X}\). To do this, we exploit the fact that \(\tilde{Y}\) is independent of \(N_{X}\), which implies the following condition [38]:

\[\frac{\partial^{2}}{\partial\tilde{y}\partial n_{x}}\log p(\tilde{y},n_{x})=0,\] (13)

for any \((\tilde{y},n_{x})\). According to equations (7), (8), we have that:

\[\tilde{Y}=h_{Y}(\tilde{X})+N_{Y},\] \[N_{X}=\tilde{X}-h_{X}(\tilde{Y}),\]

such that we can define an invertible map \(\Phi:(\tilde{y},n_{x})\mapsto(\tilde{x},n_{Y})\). It is easy to show that the Jacobian of the transformation has determinant \(|J_{\Phi}|=1\), such that

\[p(\tilde{y},n_{Y})=p(\tilde{x},n_{Y}),\]

where \((\tilde{x},n_{Y})=\Phi^{-1}(\tilde{y},n_{X})\). Thus, being \(\tilde{X},N_{Y}\) independent random variables, we have that:

\[\log p(\tilde{y},n_{X})=\log p(\tilde{x})+\log p(n_{Y})=\xi(\tilde{x})+\nu_{Y }(n_{Y}).\]

Given that \(\tilde{X}=h_{X}(\tilde{Y})+N_{X}\), we have that

\[\frac{\partial^{2}}{\partial\tilde{y}\partial\tilde{n}_{X}}\log p(\tilde{x}) =\xi^{\prime\prime}h_{X}^{\prime},\]

while \(N_{Y}=\tilde{Y}-h_{Y}(\tilde{X})\) implies

\[\frac{\partial^{2}}{\partial\tilde{y}\partial\tilde{n}_{X}}\log p(n_{Y})=-\nu _{Y}^{\prime\prime}h_{Y}^{\prime}+\nu_{Y}^{\prime\prime}h_{X}^{\prime}(h_{Y}^ {\prime})^{2}-\nu_{Y}^{\prime}h_{X}^{\prime}h_{Y}^{\prime\prime},\]

such that

\[\log p(\tilde{x},n_{Y})=\xi^{\prime\prime}h_{X}^{\prime}+-\nu_{Y}^{\prime \prime}h_{Y}^{\prime}+\nu_{Y}^{\prime\prime}h_{X}^{\prime}(h_{Y}^{\prime})^{2 }-\nu_{Y}^{\prime}h_{X}^{\prime}h_{Y}^{\prime\prime},\]

which must be equal to zero, being equal to the LHS of (13). Thus, we conclude that

\[\frac{1}{h_{X}^{\prime}}=\frac{\xi^{\prime\prime}+\nu_{Y}^{\prime\prime}(h_{Y} ^{\prime})^{2}-\nu_{Y}^{\prime}h_{Y}^{\prime\prime}}{\nu_{Y}^{\prime\prime}h_ {Y}^{\prime}},\]

proving the claim. 

### Proof of Proposition 1

Proof.: Under the hypothesis that equations (5), (6) hold, i.e. when the data generating process satisfy both a forward and a backward model, by Theorem 1 we have that:

\[\xi^{\prime\prime\prime}(\tilde{x})=\xi^{\prime\prime}(\tilde{x})G(\tilde{x}, \tilde{y})+H(\tilde{x},\tilde{y}),\] (14)

where

\[G(\tilde{x},\tilde{y})=\left(\frac{h_{Y}^{\prime\prime}}{h_{Y}^{\prime}}-\frac{ \nu_{Y}^{\prime\prime\prime}h_{Y}^{\prime}}{\nu_{Y}^{\prime\prime}}\right),\]

\[H(\tilde{x},\tilde{y})=\frac{\nu_{Y}^{\prime\prime\prime}h_{Y}^{\prime}h_{Y}^{ \prime\prime}}{\nu_{Y}^{\prime\prime}}-\frac{\nu_{Y}^{\prime}(h_{Y}^{\prime \prime})^{2}}{h_{Y}^{\prime}}-2\nu_{Y}^{\prime\prime}h_{Y}^{\prime\prime}h_{Y}^ {\prime}+\nu_{Y}^{\prime}h_{Y}^{\prime\prime\prime}.\]Define \(z\coloneqq\xi^{\prime\prime\prime}\), such that the above equation can be written as \(z^{\prime}(\tilde{x})=z(\tilde{x})G(\tilde{x},\tilde{y})+H(\tilde{x},\tilde{y})\). given that such function \(z\) exists, it is given by:

\[z(\tilde{x})=z(\tilde{x}_{0})e^{\int_{\tilde{x}_{0}}^{\tilde{x}}G(t,y)dt}+\int_ {\tilde{x}_{0}}^{\tilde{x}}e^{\int_{\tilde{x}}^{\tilde{x}}G(t,y)dt}H(\hat{t},y )d\hat{t}.\] (15)

Let \(\tilde{y}\) such that \(\nu_{Y}^{\prime}(\tilde{y}-h_{Y}(\tilde{x}))h_{Y}^{\prime}(\tilde{x})\neq 0\) holds for all but countable values of \(\tilde{x}\). Then, \(z\) is determined by \(z(\tilde{x}_{0})\), as we can extend equation (15) to all the remaining points. The set of all functions \(\xi\) satisfying the differential equation (14) is a 3-dimensional affine space, as fixing \(\xi(\tilde{x}_{0}),\xi^{\prime\prime}(\tilde{x}_{0}),\xi^{\prime\prime}( \tilde{x}_{0})\) for some point \(\tilde{x}_{0}\) completely determines the solution \(\xi\). Moreover, given \(\nu_{Y},h_{X},h_{Y}\) fixed, \(\xi^{\prime\prime}\) is specified by (9) of theorem 1, which implies:

\[\xi^{\prime\prime}=\frac{\nu_{Y}^{\prime\prime}h_{Y}^{\prime}}{h_{X}^{\prime}} +\nu_{Y}^{\prime}h_{Y}^{\prime\prime}-\nu_{Y}^{\prime\prime}(h_{Y}^{\prime}) ^{2},\]

which confines \(\xi\) solutions of (14) to a 2-dimensional affine space.

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Supervised learning models in causal discovery do not provide connections with the known identifiability theory. In the abstract, we present this open problem, and highlight our main empirical findings and how they connect to the theory of identifiability in causality. The content of the paper (mostly Section 3) unravels the abstract claims in all of their details. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in Section 1, paragraph "Closely related works and their relation with CSIvA", regarding the use of CSIvA as our only architecture for the experiments. Additionally, in the same paragraph, we remark that the scope of this study is limited to the context of causal discovery on observational data. Finally, in Section 2.2, we discuss our choice of limiting the empirical study to the case of bivariate graphs. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Proposition 1 is proved in detail in Appendix D.1, which is based on Theorem 1 of Zhang and Hyvarinen [8], which we report in the Appendix together with its proof. We do not provide an explicit sketch of the proof of our Proposition 1 in the main text, as we already detail the intuition behind it in the content of Section 3.3. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have specified our data generation methods in Appendix B.2, as well as the CSIvA method (which is a previously published model) in Appendix A, and our hyperparameters for training in Appendix B.1. We will also release our implementation of CSIvA, our data generation code (which is a thin wrapper around the causallyhttps://causally.readthedocs.io/en/latest/Python library), and our experimental code. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. * If the contribution is a new algorithm, the paper should make it clear how to reproduce that algorithm.

* If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
* If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
* We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
* **Open access to data and code*
* Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will release our implementation of CSIvA, our data generation code (which is a thin wrapper around the causally https://causally.readthedocs.io/en/latest/ Python library), and our experimental code. Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details*
* Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, we provide these details in Section 3.1 and Appendix B. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance**Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For each plot, we provide error bars in the form of 95% confidence intervals computed on 1.5k points (hence, it's reasonable to apply the central limit theorem to argue that the confidence intervals are valid). Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide all details on our computer resources in Appendix B.3. Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We do not believe any of the concerns in the Code of Ethics apply to our work. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work is about assessing and studying pre-existing causal discovery models. As we release no new model, there is no societal impact that could be caused by our work. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The data and models in this paper do not have high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]Justification: We cite the authors of all papers we build our work on. Additionally, we provide the URL to all previously existing code we rely on, which is available in the form of public GitHub repository under MIT license.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

Answer: [Yes] Justification: As our work is an analysis of pre-existing methods of causal discovery, we do not release new assets other than the code strictly needed for reproducing our experimental results. This code is attached to this submission to facilitate the reproducibility of our results. All the documentation necessary for reproducing our results is provided in the main manuscript.

Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?

Answer: [NA] Justification: We do not work with human subjects or crowdsourcing.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not work with human subjects or crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.