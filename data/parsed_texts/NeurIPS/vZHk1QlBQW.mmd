# ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning

Junguang Jiang, Baixu Chen, Junwei Pan, Ximei Wang, Dapeng Liu, Jie Jiang,

Mingsheng Long

Equal contribution.School of Software, BNRist, Tsinghua University, China

\({}^{\$}\)Tencent Inc, China

{jjg20,cbx22}@mails.tsinghua.edu.cn, {jonaspan,messixmwang,rocliu,zeus}@tencent.com,

mingsheng@tsinghua.edu.cn

###### Abstract

Auxiliary-Task Learning (ATL) aims to improve the performance of the target task by leveraging the knowledge obtained from related tasks. Occasionally, learning multiple tasks simultaneously results in lower accuracy than learning only the target task, which is known as negative transfer. This problem is often attributed to the gradient conflicts among tasks, and is frequently tackled by coordinating the task gradients in previous works. However, these optimization-based methods largely overlook the auxiliary-target generalization capability. To better understand the root cause of negative transfer, we experimentally investigate it from both optimization and generalization perspectives. Based on our findings, we introduce _ForkMerge_, a novel approach that periodically forks the model into multiple branches, automatically searches the varying task weights by minimizing target validation errors, and dynamically merges all branches to filter out detrimental task-parameter updates. On a series of auxiliary-task learning benchmarks, _ForkMerge_ outperforms existing methods and effectively mitigates negative transfer.

## 1 Introduction

Deep neural networks have achieved remarkable success in various machine learning applications, such as computer vision [23; 22], natural language processing [62; 11; 57], and recommendation systems [46]. However, one major challenge in training deep neural networks is the scarcity of labeled data. In recent years, Auxiliary-Task Learning (ATL) has emerged as a promising technique to address this challenge [67; 39; 43]. ATL improves the generalization of target tasks by leveraging the useful signals provided by some related auxiliary tasks. For instance, larger-scale tasks, such as user click prediction, can be utilized as auxiliary tasks to improve the performance of smaller-scale target tasks, such as user conversion prediction in recommendation [47; 36]. Self-supervised tasks on unlabeled data can serve as auxiliary tasks to improve the performance of the target task in computer vision and natural language processing, without requiring additional labeled data [34; 69; 11; 3].

However, in practice, learning multiple tasks simultaneously sometimes leads to performance degradation compared to learning only the target task, a phenomenon known as _negative transfer_[84; 75]. Even in large language models, negative transfer problems may still exist. For example, RLHF [7], a key component of ChatGPT [57], achieves negative effects on nearly half of the multiple-choice question tasks when post-training GPT-4 [58]. There has been a significant amount of methods proposed to mitigate negative transfer in ATL [71; 79; 15; 39]. Notable previous studies attribute negative transfer to the optimization difficulty, especially the gradient conflicts between different tasks, and propose to mitigate negative transfer by reducing interference between task gradients[79; 15]. Other works focus on selecting the most relevant auxiliary tasks and reducing negative transfer by avoiding task groups with severe task conflicts [71; 17]. However, despite the significant efforts to address negative transfer, its underlying causes are still not fully understood.

In this regard, we experimentally analyze potential causes of negative transfer in ATL from the perspectives of optimization and generalization. From an optimization view, our experiments suggest that _gradient conflicts do not necessarily lead to negative transfer_. For example, weight decay, a special auxiliary task, can conflict with the target task in gradients but still be beneficial to the target performance. From a generalization view, we observe that negative transfer is more likely to occur when _the distribution shift between the multi-task training data and target test data is enlarged._

Based on our above findings, we present a new approach named _ForkMerge_. Since we cannot know which task distribution combination leads to better generalization in advance, and training models for each possible distribution is prohibitively expensive, we transform the problem of combining task distributions into that of combining model hypotheses. Specifically, we fork the model into multiple branches and optimize the parameters of different branches on diverse data distributions by varying the task weights. Then at regular intervals, we merge and synchronize the parameters of each branch to approach the optimal model hypothesis. In this way, we will filter out harmful parameter updates to mitigate negative transfer and keep desirable parameter updates to promote positive transfer.

The contributions of this work are summarized as follows: (1) We systematically identify the problem and analyze the causes of negative transfer in ATL. (2) We propose _ForkMerge_, a novel approach to mitigate negative transfer and boost the performance of ATL. (3) We conduct extensive experiments and validate that _ForkMerge_ outperforms previous methods on a series of ATL benchmarks.

## 2 Related Work

### Auxiliary-Task Learning

Auxiliary-Task Learning (ATL) enhances a model's performance on a target task by utilizing knowledge from related auxiliary tasks. The two main challenges in ATL are selecting appropriate auxiliary tasks and optimizing them jointly with the target task. To find the proper auxiliary tasks for ATL, recent studies have explored task relationships by grouping positively related tasks together and assigning unrelated tasks to different groups to avoid task interference [81; 71; 17; 70]. Once auxiliary tasks are determined, most ATL methods create a unified loss by linearly combining the target and auxiliary losses. However, choosing task weights is challenging due to the exponential increase in search space with the number of tasks, and fixing the weight of each task loss can lead to negative transfer [32]. Recent studies propose various methods to automatically choose task weights, such as using one-step or multi-step gradient similarity [15; 39; 9], minimizing representation-based task distance [2] or gradient gap [67], employing a parametric cascade auxiliary network [54], or from the perspective of bargaining game [66]. However, these methods mainly address the optimization difficulty after introducing auxiliary tasks and may overlook the generalization problem.

Recently, AANG [10] formulates a novel searching space of auxiliary tasks and adopts the meta-learning technique, which prioritizes target task generalization, to learn single-step task weightings. This parallel finding highlights the importance of the target task generalization and we further introduce the multi-step task weightings to reduce the estimation uncertainty. Another parallel method, ColD Fusion [12], explores collaborative multitask learning and proposes to fuse each contributor's parameter to construct a shared model. In this paper, we further take into account the diversity of tasks and the intricacies of task relationships and derive a method for combining model parameters from the weights of task combinations.

### Multi-Task Learning

Different from ATL, Multi-Task Learning (MTL) aims to improve the performance of all tasks by learning multiple objectives from a shared representation. To facilitate information sharing and minimize task conflict, many multi-task architectures have been designed, including hard-parameter sharing [30; 22; 24] and soft-parameter sharing [51; 64; 16; 46; 44; 48; 72]. Another line of work aims to optimize strategies to reduce task conflict. Methods such as loss balancing and gradient balancing propose to find suitable task weighting through various criteria, such as task uncertainty [28], task loss magnitudes [44], gradient norm [5], and gradient directions [79; 6; 40; 41; 25; 55].

Although MTL methods can be directly used to jointly train auxiliary and target tasks, the asymmetric task relationships in ATL are usually not taken into account in MTL.

### Negative Transfer

Negative Transfer (NT) is a widely existing phenomenon in machine learning, where transferring knowledge from the source data or model can have negative impact on the target learner [63; 60; 27]. To mitigate negative transfer, domain adaptation methods design importance sampling or instance weighting strategies to prioritize related source data [75; 83]. Fine-tuning methods filter out detrimental pre-trained knowledge by suppressing untransferable spectral components in the representation [4]. MTL methods use gradient surgery or task weighting to reduce the gradient conflicts across tasks [79; 76; 25; 42]. Different from previous work, we propose to dynamically filter out harmful parameter updates in the training process to mitigate negative transfer. Besides, we provide an in-depth experimental analysis of the causes of negative transfer in ATL, which is rare in this field yet will be helpful for future research.

## 3 Negative Transfer Analysis

Problem and Notation.In this section, we assume that both the target task \(\mathcal{T}_{\text{tgt}}\) and the auxiliary task \(\mathcal{T}_{\text{aux}}\) are given. Then the objective is to find model parameters \(\theta\) that achieve higher performance on the target task by joint training with the auxiliary task,

\[\min_{\theta}\mathbb{E}_{\mathcal{T}_{\theta}}\mathcal{L}_{\text{tgt}}( \theta)+\lambda\mathbb{E}_{\mathcal{T}_{\text{aux}}}\mathcal{L}_{\text{ aux}}(\theta),\] (1)

where \(\mathcal{L}\) is the training loss, and \(\lambda\) is the relative weighting hyper-parameter between the auxiliary task and the target task. Our final objective is \(\max_{\theta}\left[\mathcal{P}(\theta)\right]\), where \(\mathcal{P}\) is the relative performance measure for the target task \(\mathcal{T}_{\text{tgt}}\), such as the accuracy in classification. Next we define the Transfer Gain to measure the impact of \(\mathcal{T}_{\text{aux}}\) on \(\mathcal{T}_{\text{tgt}}\).

**Definition 3.1** (Transfer Gain, TG).: Denote the model obtained by some ATL algorithm \(\mathcal{A}\) as \(\theta_{\mathcal{A}}(\mathcal{T}_{\text{tgt}},\mathcal{T}_{\text{aux}},\lambda)\) and the model obtained by single-task learning on target task as \(\theta(\mathcal{T}_{\text{tgt}})\). Let \(\mathcal{P}\) be the performance measure on the target task \(\mathcal{T}_{\text{tgt}}\). Then the algorithm \(\mathcal{A}\) can be evaluated by

\[TG(\lambda,\mathcal{A})=\mathcal{P}(\theta_{\mathcal{A}}(\mathcal{T}_{\text{ tgt}},\mathcal{T}_{\text{aux}},\lambda))-\mathcal{P}(\theta(\mathcal{T}_{\text{tgt}})).\] (2)

Going beyond previous work on Negative Transfer (NT) [75; 84], we further divide negative transfer in ATL into two types.

**Definition 3.2** (Weak Negative Transfer, WNT).: For some ATL algorithm \(\mathcal{A}\) with weighting hyper-parameter \(\lambda\), weak negative transfer occurs if \(TG(\lambda,\mathcal{A})<0\).

**Definition 3.3** (Strong Negative Transfer, SNT).: For some ATL algorithm \(\mathcal{A}\), strong negative transfer occurs if \(\max_{\lambda>0}TG(\lambda,\mathcal{A})<0\).

Figure 1 illustrates the difference between weak negative transfer and strong negative transfer. The most essential difference is that we might be able to avoid weak negative transfer by selecting a proper weighting hyper-parameter \(\lambda\), yet we cannot avoid strong negative transfer in this way.

Next, we will analyze negative transfer in ATL from two different perspectives: **optimization** and **generalization**. We conduct our analysis on a multi-domain image recognition dataset DomainNet [61] with ResNet-18 [23] pre-trained on ImageNet. Specifically, we use task Painting and Quickdraw in DomainNet as target tasks respectively to showcase weak negative transfer and strong negative transfer, and mix all other tasks in DomainNet as auxiliary tasks. We will elaborate on the DomainNet dataset in Appendix C.3 and provide the detailed experiment design in Appendix B.

### Effect of Gradient Conflicts

It is widely believed that gradient conflicts between different tasks will lead to optimization difficulties [79; 40], which in turn lead to negative transfer. The degree of gradient conflict is usually measured by the Gradient Cosine Similarity [79; 76; 15].

Figure 1: Weak Negative Transfer (WNT) vs. Strong Negative Transfer (SNT).

**Definition 3.4** (Gradient Cosine Similarity, GCS).: Denote \(\phi_{ij}\) as the angle between two task gradients \(\mathbf{g}_{i}\) and \(\mathbf{g}_{j}\), then we define the gradient cosine similarity as \(\cos\phi_{ij}\) and the gradients as conflicting when \(\cos\phi_{ij}<0\).

In Figure 2, we plot the correlation curve between gradient cosine similarity and transfer gain. Somewhat counterintuitively, we observe that negative transfer and gradient conflicts are not strongly correlated, and negative transfer might be severer when the task gradients are highly consistent.

_Finding 1. Negative transfer is not necessarily caused by gradient conflicts and gradient conflicts do not necessarily lead to negative transfer._

It seems contradictory to the previous work [79; 15] and the reason is that previous work mainly considers the _optimization_ convergence during training, while in our experiments we further consider the _generalization_ during evaluation (transfer gain is estimated on the validation set). Although the conflicting gradient of the auxiliary task will increase the training loss of the target task and slow down its convergence speed [37], it may also play a role similar to regularization [32], reducing the over-fitting of the target task, thereby reducing its generalization error. To confirm our hypothesis, we repeat the above experiments with the auxiliary task replaced by \(L_{2}\) regularization and observe a similar phenomenon as shown in Figure 2(c)-(d), which indicates that the gradient conflict in ATL is not necessarily harmful, as it may serve as a proper regularization.

Figure 2 also indicates that the weighting hyper-parameter \(\lambda\) in ATL has a large impact on negative transfer. A proper \(\lambda\) not only reduces negative transfer but also promotes positive transfer.

### Effect of Distribution Shift

Next, we will analyze negative transfer from the perspective of generalization. We notice that adjusting \(\lambda\) will change the data distribution that the model is fitting. For instance, when \(\lambda=0\), the model only fits the data distribution of the target task, and when \(\lambda=1\), the model will fit the interpolated distribution of the target and auxiliary tasks. Formally, given the target distribution \(\mathcal{T}_{\text{tgt}}\) and the auxiliary distribution \(\mathcal{T}_{\text{aux}}\), the interpolated distribution of the target and auxiliary task is \(\mathcal{T}_{\text{inter}}\),

\[\mathcal{T}_{\text{inter}}\sim(1-Z)\mathcal{T}_{\text{tgt}}+Z\mathcal{T}_{ \text{aux}},Z\sim\mathrm{Bernoulli}(\frac{\lambda}{1+\lambda}),\] (3)

where \(\lambda\) is the task-weighting hyper-parameter. Figure 3(a) quantitatively visualizes the distribution shift under different \(\lambda\) using t-SNE [74].

To quantitatively measure the distribution shift in ATL, we introduce the following definitions. Following the notations of [53], we consider multiclass classification with hypothesis space \(\mathcal{F}\) of scoring functions \(f:\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}\) where \(f(x,y)\) indicates the confidence of predicting \(x\) as \(y\).

**Definition 3.5** (Confidence Score Discrepancy, CSD).: Given scoring function hypothesis \(\mathcal{F}\), denote the optimal hypothesis on distribution \(\mathcal{D}\) as \(f^{*}_{\mathcal{D}}\), then confidence score discrepancy between

Figure 2: The effect of _gradient conflicts_. The correlation curve between Transfer Gain (TG) and Gradient Cosine Similarity (GCS) under different \(\lambda\). For a fair comparison, each data point starts from the same model parameters in the middle of the training process and updates with one-step multi-task gradient descent. **P** and **Q** are short for **P**ainting and **Q**uickdraw tasks, respectively.

distribution \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\) induced by \(\mathcal{F}\) is defined by

\[d_{\mathcal{F}}(\mathcal{D},\mathcal{D}^{\prime})\triangleq 1-\mathbb{E}_{x \sim\mathcal{D}^{\prime}}\max_{y\in\mathcal{S}}f_{\mathcal{D}}^{*}(x,y).\] (4)

Confidence score discrepancy between training and test data indicates how unconfident the model is on the test data, which is expected to increase when the data shift enlarges [59; 50].

Figure 3(b) indicates the correlation between confidence score discrepancy and transfer gain. For weak negative transfer tasks, when \(\lambda\) increases at first, the introduced auxiliary tasks will shift the training distribution towards the test distribution, thus decreasing the confidence score discrepancy between training and test data and improving the generalization of the target task. However, when \(\lambda\) continues to increase, the distribution shift gradually increases, finally resulting in negative transfer. For strong negative transfer tasks, there is a large gap between the distribution of the introduced auxiliary tasks and that of the target task. Thus, increasing \(\lambda\) always enlarges confidence score discrepancy and always leads to negative transfer. In summary,

_Finding 2. Negative transfer is likely to occur if the introduced auxiliary task enlarges the distribution shift between training and test data for the target task._

## 4 Methods

In Section 4.1, based on our above analysis, we will introduce how to mitigate negative transfer when the auxiliary task is determined. Then in Section 4.2, we will further discuss how to use the proposed method to select appropriate auxiliary tasks and optimize them jointly with the target task simultaneously.

### ForkMerge

In this section, we assume that the auxiliary task \(\mathcal{T}_{\text{aux}}\) is given. When updating the parameters \(\theta_{t}\) with Equation (1) at training step \(t\), we have

\[\theta_{t+1}(\lambda)=\theta_{t}-\eta(\mathbf{g}_{\text{tgt}}(\theta_{t})+ \lambda\mathbf{g}_{\text{aux}}(\theta_{t})),\] (5)

where \(\eta\) is the learning rate, \(\mathbf{g}_{\text{tgt}}\) and \(\mathbf{g}_{\text{aux}}\) are the gradients calculated from \(\mathcal{L}_{\text{tgt}}\) and \(\mathcal{L}_{\text{aux}}\) respectively. Section 3.1 reveals that the gradient conflict between \(\mathbf{g}_{\text{tgt}}\) and \(\mathbf{g}_{\text{aux}}\) does not necessarily lead to negative transfer as long as \(\lambda\) is carefully tuned and Section 3.2 shows that negative transfer is related to generalization. Thus we propose to dynamically adjust \(\lambda\) according to the target validation performance \(\widehat{\mathcal{P}}\) to mitigate negative transfer:

\[\max_{\lambda}\widehat{\mathcal{P}}(\theta_{t+1})=\widehat{\mathcal{P}}( \theta_{t}-\eta(\mathbf{g}_{\text{tgt}}(\theta_{t})+\lambda\mathbf{g}_{\text{ aux}}(\theta_{t}))).\] (6)

Figure 3: The effect of _distribution shift_. **(a)** Visualization of training distribution and test distribution under different \(\lambda\). **(b)** For weak negative transfer tasks, as \(\lambda\) increases, Confidence Score Discrepancy (CSD) first drops and then rises and Transfer Gain (TG) is first positive and then negative. For strong negative transfer tasks, CSD increases monotonically and TG remains negative.

Equation (6) is a bi-level optimization problem. One common approach is to first approximate \(\widehat{\mathcal{P}}\) with the loss of a batch of data on the validation set, and then use first-order approximation to solve \(\lambda\)[18; 43]. However, these approximations within a single step of gradient descent introduce large noise to the estimation of \(\lambda\) and also increase the risk of over-fitting the validation set. To tackle these issues, we first rewrite Equation (6) equally as

\[\max_{\lambda}\widehat{\mathcal{P}}\big{(}(1-\lambda)\theta_{t+1}(0)+\lambda \theta_{t+1}(1)\big{)},\] (7)

where \(\theta_{t+1}(0)=\theta_{t}-\eta\mathbf{g}_{\text{tgt}}(\theta_{t})\) and \(\theta_{t+1}(1)=\theta_{t}-\eta(\mathbf{g}_{\text{tgt}}(\theta_{t})+\mathbf{g} _{\text{aux}}(\theta_{t}))\). The proof is in Appendix A.1. Note that we assume the optimal \(\lambda^{*}\) satisfies \(0\leq\lambda^{*}\leq 1\), which can be guaranteed by increasing the scale of \(\mathcal{L}_{\text{aux}}\) when necessary. Yet an accurate estimation of performance \(\widehat{\mathcal{P}}\) in Equation (7) is still computationally expensive and prone to over-fitting, thus we extend the one gradient step to \(\Delta t\) steps,

\[\lambda^{*}=\arg\max_{\lambda}\widehat{\mathcal{P}}\big{(}(1-\lambda)\theta_{ t+\Delta t}(0)+\lambda\theta_{t+\Delta t}(1)\big{)}.\] (8)

Algorithm.As shown in Figure 4 and Algorithm 1, the initial model parameters \(\theta_{t}\) at training step \(t\) will first be forked into two branches. The first one will be optimized only with the target task loss \(\mathcal{L}_{\text{tgt}}\) for \(\Delta t\) iterations to obtain \(\theta_{t+\Delta t}(0)\), while the other one will be jointly trained for \(\Delta t\) iterations to obtain \(\theta_{t+\Delta t}(1)\). Then we will search the optimal \(\lambda^{*}\) that linearly combines the above two sets of parameters to maximize the validation performance \(\widehat{\mathcal{P}}\). When weak negative transfer occurs in the joint training branch, we can select a proper \(\lambda^{*}\) between \(0\) and \(1\). And when strong negative transfer occurs, we can simply set \(\lambda^{*}\) to \(0\). Finally, the newly merged parameter \(\theta_{t+\Delta t}^{*}=(1-\lambda^{*})\theta_{t+\Delta t}(0)+\lambda^{*} \theta_{t+\Delta t}(1)\) will join in a new round, being forked into two branches again and repeating the optimization process for \(\lceil\frac{T}{\Delta t}\rceil\) times.

Discussion.Compared to grid searching \(\lambda\), which is widely used in practice, ForkMerge can dynamically transfer knowledge from auxiliary tasks to the target task during training with varying \(\lambda^{*}\). In terms of computation cost, ForkMerge has a lower complexity as it only requires training \(2\) branches while grid searching has a cost proportional to the number of hyper-parameters to be searched.

```
0: initial model parameter \(\theta_{0}\), total iterations \(T\), interval \(\Delta t\)
0: final model parameter \(\theta_{T}^{*}\), task relevance \(\lambda^{*}\)
0: for\(b=0\)to 1 do \(\triangleright\) initialization
0: endfor \(t<T\)do \(\triangleright\) independent update for\(t^{\prime}=t\)to \(t+\Delta t-1\)do \(\theta_{t^{\prime}+1}^{*}=\theta_{t^{\prime}}^{*}-\eta(\mathbf{g}_{\text{tgt}}( \theta_{t^{\prime}}^{*})+b\cdot\mathbf{g}_{\text{sm}}(\theta_{t^{\prime}}^{b}))\) endfor \(\lambda^{*}\leftarrow\arg\max_{\lambda}\widehat{\mathcal{P}}((1-\lambda) \theta_{t+\Delta t}^{*}+\lambda\theta_{t+\Delta t}^{*})\) \(\triangleright\) search \(\lambda\) on the validation set \(\theta_{t+\Delta t}^{*}=(1-\lambda^{*})\theta_{t+\Delta t}^{0}+\lambda^{*} \theta_{t+\Delta t}^{*}\) for\(b=0\)to 1 do \(\theta_{t+\Delta t}^{b}\leftarrow\theta_{t+\Delta t}^{*}\) \(\triangleright\) synchronize parameters endfor \(t\gets t+\Delta t\) endwhile ```

**Algorithm 1** ForkMerge Training Pipeline.

### ForkMerge for Task Selection Simultaneously

When multiple auxiliary tasks are available, we can simply mix all the auxiliary tasks together to form a single auxiliary task. This simple strategy actually works well in most scenarios (see Section 5.2) and is computationally cheap. However, when further increasing the performance is desired, we can also dynamically select the optimal weighting for each auxiliary task. Formally, the objective

Figure 4: ForkMerge training pipeline. The model parameters will be forked into two branches, one optimized with the target task loss and the other jointly trained, and be merged at regular intervals of \(\Delta t\) steps.

when optimizing the model for the target task \(\mathcal{T}_{0}\) with multiple auxiliary tasks \(\{\mathcal{T}_{k}\}_{k=1}^{K}\) is

\[\min_{\theta}\mathbb{E}_{\mathcal{T}_{0}}\mathcal{L}_{0}(\theta)+\sum_{k=1}^{K} \lambda_{k}\mathbb{E}_{\mathcal{T}_{k}}\mathcal{L}_{k}(\theta),\] (9)

where \(\sum_{k=1}^{K}\lambda_{k}\leq 1\) and \(\forall k,\lambda_{k}\geq 0\). Using gradient descent to update \(\theta_{t}\) at training step \(t\), we have

\[\theta_{t+1}(\bm{\lambda})=\theta_{t}-\eta\sum_{k=0}^{K}\lambda_{k}\mathbf{g} _{k}(\theta_{t}),\] (10)

where \(\lambda_{0}=1\). Given \(K\) task-weighting vectors \(\{\bm{\omega}^{k}\}_{k=0}^{K}\) that satisfies \(\omega_{i}^{k}=\mathbbm{1}[i=k\text{ or }i=0]\), i.e., the \(k\)-th and \(0\)-th dimensions of \(\bm{\omega}^{k}\) are \(1\) and the rest are \(0\), and a vector \(\bm{\Lambda}\) that satisfies

\[\Lambda_{k}=\begin{cases}1-\sum_{i\neq 0}\lambda_{i},&k=0,\\ &\lambda_{k},&k\neq 0,\end{cases}\] (11)

then optimizing \(\bm{\lambda}^{*}\) in Equation (10) is equivalent to

\[\bm{\Lambda}^{*}=\arg\max_{\bm{\Lambda}}\widehat{\mathcal{P}}\big{(}\sum_{k=0 }^{K}\Lambda_{k}\theta_{t+1}(\bm{\omega}^{k})\big{)}.\] (12)

In Equation (12), the initial model parameters are forked into \(K+1\) branches, where one branch is optimized with the target task, and the other branches are jointly optimized with one auxiliary task and the target task. Then we find the optimal \(\bm{\Lambda}^{*}\) that linearly combines the \(K+1\) sets of parameters to maximize the validation performance (see proof of Equation (12) and the detailed algorithm in Appendix A.2). The training computational complexity of Equation 12 is \(\mathcal{O}(K)\), which is much lower than the exponential complexity of grid searching, but still quite large. Inspired by the early-stop approximation used in task grouping methods [71], we can prune the forking branches with \(\Lambda_{k}=0\) (strong negative transfer) and only keep the branches with the largest \(K^{\prime}<K\) values in \(\bm{\Lambda}\) after the early merge step. In this way, those useless branches with irrelevant auxiliary tasks can be stopped early. Additionally, we introduce a greedy search strategy in Algorithm 3 to further reduce the computation complexity when grid searching all possible values of \(\bm{\Lambda}\).

Lastly, we introduce a general form of ForkMerge. Assuming \(B\) candidate branches with task-weighting vectors \(\bm{\upsilon}^{b}\) (\(b=1,\ldots,B\)), the goal is to optimize \(\overline{\bm{\Lambda}}^{*}\):

\[\overline{\bm{\Lambda}}^{*}=\arg\max_{\overline{\bm{\Lambda}}}\widehat{ \mathcal{P}}\big{(}\sum_{b=1}^{B}\overline{\Lambda}_{b}\theta_{t+\Delta t}( \bm{\upsilon}^{b})\big{)}.\] (13)

From a generalization view, the mixture distributions constructed by different \(\bm{\nu}\) lead to diverse data shifts from the target distribution, yet we cannot predict which \(\bm{\nu}\) leads to better generalization. Thus, we transform the problem of mixture distribution into that of mixture hypothesis [49] and the models trained on different distributions are combined dynamically via \(\overline{\bm{\Lambda}}^{*}\) to approach the optimal parameters. Here, Equation 12 is a particular case by substituting \(B=K+1\) and \(\bm{\upsilon}^{b}_{i}=\mathbbm{1}[i=b-1\text{ or }i=0]\). By comparison, Equation 13 allows us to introduce human prior into ForkMerge by constructing more efficient branches, and also provides possibilities for combining ForkMerge with previous task grouping methods [81; 71; 17]. The detailed algorithm of Equation 13 can be found in Algorithm 2.

## 5 Experiments

We evaluate the effectiveness of ForkMerge under various settings, including multi-task learning, multi-domain learning, and semi-supervised learning. First, in Section 5.1, we illustrate the prevalence of negative transfer and explain how ForkMerge can mitigate this problem. In Section 5.2, We examine whether ForkMerge can mitigate negative transfer when joint training the auxiliary and target tasks, and compare it with other methods. In Section 5.3, we further use ForkMerge for task selection simultaneously. Experiment details can be found in Appendix C. We will provide additional analysis and comparison experiments in Appendix D. The codebase for both our method and the compared methods will be available at https://github.com/thuml/ForkMerge.

### Motivation Experiment

Negative Transfer is widespread across different tasks.In Figure 5 (a), we visualize the transfer gains between \(30\) task pairs on DomainNet, where the auxiliary and target tasks are equally weighted, and we observe that negative transfer is common in such case (\(23\) of \(30\) combinations lead to negative transfer). Besides, as mentioned in Definition 3.2 and 3.3, whether negative transfer occurs is related to a specific ATL algorithm, in Figure 5 (b), we observe that negative transfer in all \(30\) combinations can be successfully avoided when we use ForkMerge algorithm. This observation further indicates the limitation of task grouping methods [71; 17], since they use Equal Weight between tasks and may discard some useful auxiliary tasks.

Mixture of hypotheses is an approximation of mixture of distribution.Figure 6 uses the ternary heatmaps to visualize the linear combination of a set of three models optimized with different task weightings for \(25K\) iterations, including a single-task model and two multi-task models. Similar to mixing distributions for weak negative transfer task Painting (see Figure 3), the transfer gain when mixing models **P**ainting and **P**ainting+**R**eal first increases and then decreases. Also similar to mixing distributions for strong negative transfer task **Q**uickdraw, the transfer gain when mixing models **Q**uickdraw and **Q**uickdraw+**R**eal decreases monotonically. Besides, Figure 6 also indicates a good property of deep models: the loss surfaces of over-parameterized deep neural networks are quite well-behaved and smooth after convergence, which has also been mentioned by previous works [20; 35] and provides an intuitive explanation of the merge step in ForkMerge.

### Use ForkMerge for Joint Optimization

First, we use ForkMerge only for joint training of the target and auxiliary tasks. When datasets contain multiple tasks, we will mix all tasks together to form a single auxiliary task for ForkMerge. Yet for the compared methods, a distinction is still made between different tasks for better performance.

Specifically, we compare ForkMerge with: (1) Single Task Learning (STL); (2) EW, which assigns equal weight to all tasks; (3) GCS [15], an ATL approach using gradient similarity between target and auxiliary tasks; (4) OL_AUX [39], an ATL approach adjusting the loss weight based on gradient inner product; (5) ARML [67], an ATL approach adjusting the loss weight based on gradient difference; (6) Auto-\(\lambda\)[43], an ATL method that estimates loss weight through finite-difference approximation [18]; (7) Post-train, an ATL method that pre-trains the model on all tasks and then fine-tunes it for each task separately. (8) UW [28], which adjusts weights based on task uncertainty; (9) DWA [44], which adjusts weights based on loss change; (10) MGDA [65], which computes a convex combination of gradients with a minimum norm to balance tasks; (11) GradNorm [5], which rescales the gradient norms of different tasks to the same range; (12) PCGrad [79], which eliminates conflicting gradient components; (13) IMTL [41], which uses an update direction with equal projections on task gradients; (14) CAGrad [40], which optimizes for the average loss and minimum decrease rate across tasks; (15) NashMTL [55], which combines the gradients using the Nash bargaining solution. Since different tasks have varying evaluation metrics, we will report the average per-task performance improvement for each method using \(\Delta_{m}\), as defined in Appendix C.1.

Figure 5: Negative Transfer on DomainNet. The rows of each matrix represent auxiliary tasks, and the columns represent target tasks. The blue and red cells correspond to negative and positive transfer gain. Deeper colors indicate stronger impacts.

Figure 6: Ternary heatmap for mixture of model hypotheses. Each triangle vertex represents an optimized model, e.g., **P+R** is the model jointly optimized with **P**ainting and **R**eal tasks. Each point inside the triangle corresponds to a mixture of model hypotheses and its heat value measures the Transfer Gain (TG).

**Auxiliary-Task Scene Understanding.** We evaluate on the widely-used multi-task scene understanding dataset, NYUv2 [68], which contains \(3\) tasks: \(13\)-class semantic segmentation, depth estimation, and surface normal prediction. Following [55], we use \(636\), \(159\) and \(654\) images for training, validation, and test. Our implementation is based on LibMTL [38] and MTAN [44]. The results are presented in Table 1. Negative transfer is not severe on this dataset, where both _segmentation_ and _depth_ benefit from ATL and only _normal_ task gets worse. In such cases, our method still achieves significant improvement on all tasks. We also find that Post-train serves as a strong baseline in most of our ATL experiments. Its drawback is that it fails to consider the task relationship in the pre-training phase, and suffers from catastrophic forgetting during the fine-tuning process.

**Auxiliary-Domain Image Recognition.** Further, we evaluate on the widely-used multi-domain image recognition dataset, DomainNet [61], which contains \(6\) diverse visual domains and approximately \(0.6\) million images distributed among \(345\) categories, where the task difference is reflected in the marginal distribution. Our implementation is based on TLlib [26]. As the original DomainNet does not provide a separate validation set, we randomly split \(50\%\) data from the test set as the validation set. The results are presented in Table 2. DomainNet contains both positive transfer tasks (**C**lipart), weak negative transfer tasks (**I**nfograph, **P**ainting, **R**eal, **S**ketch), and strong negative transfer tasks (**Q**uickdraw). When negative transfer occurs, previous ATL methods lead to severe performance degradation, while our method can automatically avoid strong negative transfer and improve the performance over STL in other cases.

### Use ForkMerge for Task Selection Simultaneously

As mentioned in Section 4.2, when there are multiple auxiliary task candidates, we can use ForkMerge to simultaneously select auxiliary tasks and jointly train them with the target task, which is denoted as ForkMerge\({}^{\ddagger}\).

**Auxiliary-Task Scene Understanding.** In NYUv2, we have \(2\) auxiliary tasks for any target task, thus we can construct \(3\) branches with different task weights in Equation 12. In this way, we are able to select auxiliary tasks adaptively by learning different \(\Lambda\) for different branches in the merge step. As shown in Table 3, this strategy yields better overall performance.

**Auxiliary-Domain Image Recognition.** For any target task in DomainNet, we can construct up to \(6\) branches with different task weights in Equation 12, which is computationally expensive. As mentioned in Section 4.2, we will prune the branches after the first merge step to reduce the computation cost. Table 4 reveals the impact of the pruning strategy. As the number of branches increases, the gain brought by auxiliary tasks will enlarge, while the gain brought by each branch will reduce. Therefore, pruning is an effective strategy to achieve a better balance between performance and efficiency. In practical terms, when confronted with multiple auxiliary tasks, users have the flexibility to tailor the number of branches to align with their available computational resources.

**CTR and CTCVR Prediction.** We evaluate on AliExpress dataset [36], a recommendation dataset from the industry, which consists of \(2\) tasks: CTR and CTCVR, and \(4\) scenarios and more than \(100\)M records. Our implementation is based on MTReclib [85]. For any target task in AliExpress, we can construct up to \(8\) branches with different task weights, and we prune to \(3\) branches after the first merge

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{2}{c}{**Segmentation**} & \multicolumn{2}{c}{**Depth**} & \multicolumn{2}{c}{**Normal**} & \multirow{2}{*}{\(\bm{\Delta_{m}\uparrow}\)} \\ \cline{2-2} \cline{6-7}  & **mIoU\(\uparrow\)** & **Pix Acc\(\uparrow\)** & **Abs Err\(\uparrow\)** & **Rel Err\(\uparrow\)** & **Mean\(\downarrow\)** \\ \hline STL & 51.42 & 74.14 & 41.74 & 17.37 & 22.82 & - \\ \hline EW & 52.13 & 74.51 & 39.03 & 16.43 & 24.14 & 0.305 \\ UW & 52.51 & 74.72 & 39.15 & 16.56 & 25.39 & 0.65\% \\ DWA & 52.10 & 74.45 & 39.26 & 16.57 & 24.12 & 0.07\% \\ MODA & 50.79 & 73.81 & 39.19 & 16.25 & 23.14 & 1.44\% \\ GradNorm & 52.25 & 74.54 & 39.51 & 16.37 & 28.36 & 0.56\% \\ PCGrad & 51.77 & 74.72 & **38.91** & 16.36 & 24.31 & 0.22\% \\ DTL & 52.24 & 74.73 & 39.46 & **15.92** & 23.25 & 2.10\% \\ CAfGrad & 52.04 & 74.25 & 39.06 & 16.30 & 23.39 & 1.41\% \\ NashMTL & 51.73 & 74.10 & 39.55 & 16.50 & 23.21 & 1.11\% \\ \hline GCS & 52.67 & 74.59 & 39.72 & 16.64 & 24.10 & 0.09\% \\ OL\_AUX & 52.07 & 74.28 & 39.32 & 16.30 & 25.98 & 0.17\% \\ ARML & 52.73 & 74.85 & 39.61 & 16.65 & 25.89 & 0.37\% \\ Auto-A & 52.04 & 74.62 & 39.25 & 16.25 & 23.38 & 1.17\% \\ Post-train & 52.08 & 74.86 & 39.58 & 16.77 & 22.98 & 1.49\% \\ ForkMerge & **53.67** & **75.64** & **38.91** & 16.47 & **22.18** & **4.03\%** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance on NYUv2 dataset.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Methods** & **C** & **I** & **P** & **Q** & **R** & **S** & **Avg** & \(\bm{\Delta_{m}\uparrow}\) \\ \hline STL & 77.6 & 41.4 & 71.8 & **73.0** & 84.6 & 70.2 & 69.8 & - \\ \hline EW & 78.0 & 38.1 & 67.2 & 50.8 & 77.1 & 67.0 & 63.0 & -9.62\% \\ UW & 79.1 & 35.8 & 68.2 & 50.5 & 77.9 & 67step. The results are presented in Table 5. Note that improving on such a large-scale dataset with auxiliary tasks is quite difficult. Still, ForkMerge achieves the best performance with \(\Delta_{m}=\mathbf{1.30\%}\).

**Semi-Supervised Learning (SSL)**. We also evaluate on two SSL datasets, CIFAR-10 [31] and SVHN [56]. Following [67], we use Self-supervised Semi-supervised Learning (S4L) [82] as our baseline algorithm and use \(2\) self-supervised tasks, Rotation [19] and Exempler-MT [14], as our auxiliary tasks. Table 6 presents the test error of S4L using different ATL approaches, along with other SSL methods, and shows that ForkMerge consistently outperforms the compared ATL methods. Note that we do not aim to propose a novel or state-of-the-art SSL method in this paper. Instead, we find that some SSL methods use ATL and the auxiliary task weights have a great impact (see Grid Search in Table 6). Thus, we use ForkMerge to improve the auxiliary task training within the context of SSL.

## 6 Conclusion

Methods have been proposed to mitigate negative transfer in auxiliary-task learning, yet there still lacks an in-depth experimental analysis on the causes of negative transfer. In this paper, we systematically delved into the negative transfer issues and presented ForkMerge, an approach to enable auxiliary-task learning with positive transfer gains. Experimentally, ForkMerge achieves state-of-the-art accuracy on four different auxiliary-task learning benchmarks, while being computationally efficient. We view the integration of previous task grouping methods with our auxiliary task learning approach as a promising avenue for further research.

## Acknowledgements

We would like to thank many colleagues, in particular Yuchen Zhang, Jialong Wu, Haoyu Ma, Yuhong Yang, and Jincheng Zhong, for their valuable discussions. This work was supported by the National Key Research and Development Plan (2020AAA0109201), the National Natural Science Foundation of China (62022050 and 62021002), and the Beijing Nova Program (Z201100006820041).

## References

* [1] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In _ECCV_,

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{3}{c}{**CTR**} & \multicolumn{3}{c}{**CITCWR**} & \multirow{2}{*}{**Avg**} & \multirow{2}{*}{\(\Delta_{m}\uparrow\)} \\ \cline{2-2} \cline{7-11}  & **ES** & & & & & & & & & & & & & \\ \hline STL & 0.7299 & 0.7316 & 0.7237 & 0.707 & 0.8778 & 0.8682 & 0.8652 & 0.8659 & 0.7963 & - \\ \hline EW & 0.7299 & 0.7300 & 0.7248 & 0.7008 & 0.8855 & 0.8515 & 0.8666 & 0.8618 & 0.7931 & -0.3936 & - \\ DW & 0.7276 & 0.7235 & 0.7256 & 0.7048 & 0.8814 & 0.8709 & 0.8599 & 0.8498 & 0.7966 & - 0.0095 & - \\ DWA & 0.7317 & 0.7284 & 0.7257 & 0.7063 & 0.8663 & 0.8605 & 0.8669 & 0.8484 & 0.7937 & -0.2886 & \\ MGD & 0.6985 & 0.6926 & 0.7000 & 0.6676 & 0.8215 & 0.8145 & 0.7978 & 0.7917 & 0.7840 & - 5.9496 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.7996 & - 0.8996 & - 0.

2018.
* Chen et al. [2022] Shuxiao Chen, Koby Crammer, Hangfeng He, Dan Roth, and Weijie J Su. Weighted training for cross-task learning. In _ICLR_, 2022.
* Chen et al. [2020] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. In _NeurIPS_, 2020.
* Chen et al. [2019] Xinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long, and Jianmin Wang. Catastrophic forgetting meets negative transfer: Batch spectral shrinkage for safe transfer learning. In _NeurIPS_, 2019.
* Chen et al. [2018] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In _ICML_, 2018.
* Chen et al. [2020] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign dropout. In _NeurIPS_, 2020.
* Christiano et al. [2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In _NeurIPS_, 2017.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.
* Dery et al. [2021] Lucio M Dery, Yann Dauphin, and David Grangier. Auxiliary task update decomposition: The good, the bad and the neutral. In _ICLR_, 2021.
* Dery et al. [2023] Lucio M Dery, Paul Michel, Mikhail Khodak, Graham Neubig, and Ameet Talwalkar. Aang: Automating auxiliary learning. In _ICLR_, 2023.
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _NAACL_, 2019.
* Don-Yehiya et al. [2022] Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Cold fusion: Collaborative descent for distributed multitask finetuning. _arXiv preprint arXiv:2212.01378_, 2022.
* Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2020.
* Dosovitskiy et al. [2014] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In _NeurIPS_, 2014.
* Du et al. [2018] Yunshu Du, Wojciech M Czarnecki, Siddhant M Jayakumar, Mehrdad Farajtabar, Razvan Pascanu, and Balaji Lakshminarayanan. Adapting auxiliary losses using gradient similarity. _arXiv preprint arXiv:1812.02224_, 2018.
* Fernando et al. [2017] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. _CoRR_, abs/1701.08734, 2017.
* Fifty et al. [2021] Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying task groupings for multi-task learning. In _NeurIPS_, 2021.
* Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _ICML_, 2017.
* Gidaris et al. [2018] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In _ICLR_, 2018.
* Goodfellow et al. [2015] Ian Goodfellow, Oriol Vinyals, and Andrew Saxe. Qualitatively characterizing neural network optimization problems. In _ICLR_, 2015.

* [21] Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. _arXiv preprint arXiv:1706.02677_, 2017.
* [22] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _ICCV_, 2017.
* [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* [24] Falk Heuer, Sven Mantowsky, Saqib Bukhari, and Georg Schneider. Multitask-centernet (mcn): Efficient and diverse multitask learning using an anchor free approach. In _ICCV_, 2021.
* [25] Adrian Javaloy and Isabel Valera. Rotograd: Gradient homogenization in multitask learning. In _ICLR_, 2022.
* [26] Junguang Jiang, Baixu Chen, Bo Fu, and Mingsheng Long. Transfer-learning-library. https://github.com/thuml/Transfer-Learning-Library, 2020.
* [27] Junguang Jiang, Yang Shu, Jianmin Wang, and Mingsheng Long. Transferability in deep learning: A survey. _arXiv preprint arXiv:2201.05867_, 2022.
* [28] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In _CVPR_, 2018.
* [29] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* [30] Iasonas Kokkinos. Ubernet: Training a 'universal' convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In _CVPR_, 2017.
* [31] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. _Technical report, University of Toronto_, 2009.
* [32] Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, and M Pawan Kumar. In defense of the unitary scalarization for deep multi-task learning. In _NeurIPS_, 2022.
* [33] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In _ICLR_, 2017.
* [34] Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In _ICML_, 2013.
* [35] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In _NeurIPS_, 2018.
* [36] Pengcheng Li, Runze Li, Qing Da, An-Xiang Zeng, and Lijun Zhang. Improving multi-scenario learning to rank in e-commerce by exploiting task relationships in the label space. In _CIKM_, 2020.
* [37] Baijiong Lin, YE Feiyang, Yu Zhang, and Ivor Tsang. Reasonable effectiveness of random weighting: A litmus test for multi-task learning. In _TMLR_, 2022.
* [38] Baijiong Lin and Yu Zhang. LibMTL: A python library for multi-task learning. _arXiv preprint arXiv:2203.14338_, 2022.
* [39] Xingyu Lin, Harjatin Baweja, George Kantor, and David Held. Adaptive auxiliary task weighting for reinforcement learning. In _NeurIPS_, 2019.
* [40] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for multi-task learning. In _NeurIPS_, 2021.
* [41] L Liu, Y Li, Z Kuang, J Xue, Y Chen, W Yang, Q Liao, and Wayne Zhang. Towards impartial multi-task learning. In _ICLR_, 2021.
* [42] Shengchao Liu, Yingyu Liang, and Anthony Gitter. Loss-balanced task weighting to reduce negative transfer in multi-task learning. In _AAAI_, 2019.

* [43] Shikun Liu, Stephen James, Andrew J Davison, and Edward Johns. Auto-lambda: Disentangling dynamic task relationships. In _TMLR_, 2022.
* [44] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention. In _CVPR_, 2019.
* [45] Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In _ICLR_, 2017.
* [46] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In _SIGKDD_, 2018.
* [47] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In _SIGIR_, 2018.
* [48] Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas Kokkinos. Attentive single-tasking of multiple tasks. In _CVPR_, 2019.
* [49] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple sources. In _NIPS_, 2008.
* [50] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. In _NeurIPS_, 2021.
* [51] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning. In _CVPR_, 2016.
* [52] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. In _TPAMI_, 2018.
* [53] Mehryar Mohri and Andres Munoz Medina. New analysis and algorithm for learning with drifting distributions. In _International Conference on Algorithmic Learning Theory_, 2012.
* [54] Aviv Navon, Idan Achituve, Haggai Maron, Gal Chechik, and Ethan Fetaya. Auxiliary learning by implicit differentiation. In _ICLR_, 2021.
* [55] Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik, and Ethan Fetaya. Multi-task learning as a bargaining game. In _ICML_, 2022.
* [56] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In _NeurIPS_, 2011.
* [57] OpenAI. Introducing chatgpt, 2022.
* [58] OpenAI. Gpt-4 technical report, 2023.
* [59] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. In _NeurIPS_, 2019.
* [60] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. In _TKDE_, 2010.
* [61] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. _ICCV_, 2019.
* [62] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. _Technical report, OpenAI_, 2018.
* [63] Michael T. Rosenstein. To transfer or not to transfer. In _NeurIPS_, 2005.
* [64] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. _CoRR_, abs/1606.04671, 2016.

* [65] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In _NeurIPS_, 2018.
* [66] Aviv Shamsian, Aviv Navon, Neta Glazer, Kenji Kawaguchi, Gal Chechik, and Ethan Fetaya. Auxiliary learning as an asymmetric bargaining game. _arXiv preprint arXiv:2301.13501_, 2023.
* [67] Baifeng Shi, Judy Hoffman, Kate Saenko, Trevor Darrell, and Huijuan Xu. Auxiliary task reweighting for minimum-data learning. In _NeurIPS_, 2020.
* [68] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In _ECCV_, 2012.
* [69] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In _NeurIPS_, 2020.
* [70] Xiaozhuang Song, Shun Zheng, Wei Cao, James Yu, and Jiang Bian. Efficient and effective multi-task grouping via meta learning on task combinations. In _NeurIPS_, 2022.
* [71] Trevor Standley, Amir Zamir, Dawn Chen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? In _ICML_, 2020.
* [72] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In _RecSys_, 2020.
* [73] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In _NeurIPS_, 2017.
* [74] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. In _JMLR_, 2008.
* [75] Zirui Wang, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. Characterizing and avoiding negative transfer. In _CVPR_, 2019.
* [76] Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao. Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models. In _ICLR_, 2021.
* [77] Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat. Do current multi-task optimization methods in deep learning even help? In _NeurIPS_, 2022.
* [78] Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. Reducing BERT pre-training time from 3 days to 76 minutes. In _ICLR_, 2020.
* [79] Tianhe Yu, Saurabh Kumar, Abhishe Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. In _NeurIPS_, 2020.
* [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In _BMVC_, 2016.
* [81] Amir Roshan Zamir, Alexander Sax, William B. Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In _CVPR_, 2018.
* [82] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi-supervised learning. In _ICCV_, 2019.
* [83] Jing Zhang, Zewei Ding, Wanqing Li, and Philip Ogunbona. Importance weighted adversarial nets for partial domain adaptation. In _CVPR_, 2018.
* [84] Wen Zhang, Lingfei Deng, Lei Zhang, and Dongrui Wu. A survey on negative transfer. _IEEE/CAA Journal of Automatica Sinica_, 2022.
* [85] Yongchun Zhu, Yudan Liu, Ruobing Xie, Fuzhen Zhuang, Xiaobo Hao, Kaikai Ge, Xu Zhang, Leyu Lin, and Juan Cao. Learning to expand audience via meta hybrid experts and critics for recommendation and advertising. In _KDD_, 2021.

Algorithm Details

### ForkMerge

**Proof of Equation (7).**

\[\lambda^{*} =\arg\max_{\lambda}\widehat{\mathcal{P}}(\theta_{t+1})\] \[=\arg\max_{\lambda}\widehat{\mathcal{P}}(\theta_{t}-\eta(\mathbf{g} _{\text{tgt}}(\theta_{t})+\lambda\mathbf{g}_{\text{aux}}(\theta_{t})))\] \[=\arg\max_{\lambda}\widehat{\mathcal{P}}\big{(}(\theta_{t}-\eta \mathbf{g}_{\text{tgt}}(\theta_{t}))+\lambda(-\eta\mathbf{g}_{\text{aux}}( \theta_{t}))\big{)}\] \[=\arg\max_{\lambda}\widehat{\mathcal{P}}\big{(}(1-\lambda)( \theta_{t}-\eta\mathbf{g}_{\text{tgt}}(\theta_{t}))+\lambda(\theta_{t}-\eta( \mathbf{g}_{\text{tgt}}(\theta_{t})+\mathbf{g}_{\text{aux}}(\theta_{t}))) \big{)}\] \[=\arg\max_{\lambda}\widehat{\mathcal{P}}\big{(}(1-\lambda)\theta _{t+1}(0)+\lambda\theta_{t+1}(1)\big{)}.\]

**Remarks on the search step.**

We provide two search strategies as follows, and we use the first strategy in our experiments.

* _Grid Search_: Exhaustively searching the task-weighting hyper-parameter \(\lambda\) through a manually specified subset of the hyper-parameter space, such as \(\{0,0.2,0.4,0.6,0.8,1.0\}\).
* _Binary Search_: Repeatedly dividing the search interval of \(\lambda\) in half and keep the better hyper-parameter.

Random search, bayesian optimization, gradient-based optimization, and other hyper-parameter optimization methods can also be used here, and they are left to be explored in follow-up work.

In practice, the costs of estimating \(\widehat{\mathcal{P}}\) in the search step are usually negligible. Yet when the amount of data in the validation set is relatively large, we can sample the validation set to reduce the cost of estimating \(\widehat{\mathcal{P}}\).

**Remarks on extension from the one gradient step to \(\Delta t\) steps.**

1. It can effectively reduce the average cost of estimating \(\widehat{\mathcal{P}}\) at each step and avoid over-fitting the validation set.
2. It allows longer-term rewards from auxiliary tasks and leads to safer task transfer. For instance, when the accumulated gradients of some auxiliary tasks are harmful to the final target performance, the merging step can cancel the effect of these auxiliary tasks by setting their associated weights \(\lambda\) to \(0\), to mitigate strong negative transfer.
3. It increases the risk to produce bad model parameters. However, such risk is still low since deep models usually have smooth loss surfaces after convergence as shown in Section 5.1.

Figure 7 illustrates that an appropriate \(\Delta t\) can effectively promote the performance of the ForkMerge algorithm, indicating the necessity of the extension from the one gradient step in previous work to \(\Delta t\) steps. When \(\Delta t\) is small, the estimation of \(\lambda\) is short-insight and might fail to remove the harmful parameter updates when negative transfer occurs, which also indicates the limitations of methods that use single-step gradient descent to estimate \(\lambda\)[15, 43]. When \(\Delta t\) is large, the risk to get bad model parameters from the linear combination will also increase. Therefore, in our experiment, we use the validation set to pick a proper \(\Delta t\) for each dataset and use it for all tasks in this dataset.

Figure 7: Effect of the merging step \(\Delta t\) on NYUv2.

### Use ForkMerge to Select Tasks Simultaneously

Detailed Algorithm.Algorithm 2 provides the general optimization process for any task-weighting vector \(\{\boldsymbol{\nu}^{b}\}_{b=1}^{B}\). For Equation (12), we have \(B=K+1\) and \(\boldsymbol{\nu}^{b}_{i}=\mathds{1}[i=b-1\text{ or }i=0]\). For Equation (13), we have no constraints on \(B\) or \(\boldsymbol{\nu}^{b}\).

```
0: initial model parameter \(\theta_{0}\), task-weighting vector \(\{\boldsymbol{\nu}^{b}\}_{b=1}^{B}\), total iterations \(T\), interval \(\Delta t\)
0: final model parameter \(\theta_{T}^{*}\)
1: fork model into \(B\) copies \(\{\theta^{b}\}_{b=1}^{B}\)
2:for\(b=1\)to\(B\)do\(\theta_{0}^{b}\leftarrow\theta_{0}\)\(\triangleright\) initialization
3:endfor
4:while\(t<T\)do
5:for\(b=1\)to\(B\)do\(\triangleright\) independent update
6:for\(t^{\prime}=t\)to\(t+\Delta t-1\)do
7:\(\theta_{t^{\prime}+1}^{b}=\theta_{t^{\prime}}^{b}-\eta\sum_{k}\nu_{k}^{b} \mathbf{g}_{k}(\theta_{t^{\prime}}^{b})\)
8:endfor
9:endfor
10:\(\boldsymbol{\Lambda}^{*}\leftarrow\arg\max_{\boldsymbol{\Lambda}}\widehat{ \mathcal{P}}\big{(}\sum_{b}\Lambda_{b}\theta_{t+\Delta t}^{b}\big{)}\)\(\triangleright\) search \(\boldsymbol{\Lambda}\) on the validation set
11:\(\theta_{t+\Delta t}^{*}\leftarrow\sum_{b}\Lambda_{b}^{*}\theta_{t+\Delta t}^{b}\)
12:\(\triangleright\) merge parameters
13:for\(b=1\)to\(B\)do
14:\(\theta_{t+\Delta t}^{b}\leftarrow\theta_{t+\Delta t}^{*}\)\(\triangleright\) synchronize parameters
15:endfor
16:\(t\gets t+\Delta t\)
17:endwhile ```

**Algorithm 2** ForkMerge Training Pipeline with Multiple Branches

Proof of Equation (12).

The goal of selecting \(\boldsymbol{\lambda}^{*}\) in Equation (10) is to maximize the validation performance of model \(\theta_{t+1}\),

\[\boldsymbol{\lambda}^{*} =\arg\max_{\boldsymbol{\lambda}}\widehat{\mathcal{P}}(\theta_{t+1})\] \[=\arg\max_{\boldsymbol{\lambda}}\widehat{\mathcal{P}}\big{(} \theta_{t}-\eta\sum_{k}\lambda_{k}\mathbf{g}_{k}(\theta_{t})\big{)}\] //gradient descent \[=\arg\max_{\boldsymbol{\lambda}}\widehat{\mathcal{P}}\big{(} \theta_{t}-\eta\lambda_{0}\mathbf{g}_{0}(\theta_{t})-\eta\sum_{k\neq 0} \lambda_{k}\mathbf{g}_{k}(\theta_{t})\big{)}\] \[=\arg\max_{\boldsymbol{\lambda}}\widehat{\mathcal{P}}\big{(} \theta_{t}-\eta\mathbf{g}_{0}(\theta_{t})-\eta\sum_{k\neq 0}\lambda_{k} \mathbf{g}_{k}(\theta_{t})\big{)}\] \[=\arg\max_{\boldsymbol{\lambda}}\widehat{\mathcal{P}}\big{(}(1- \sum_{k\neq 0}\lambda_{k})(\theta_{t}-\eta\mathbf{g}_{0}(\theta_{t}))+(\sum_{k \neq 0}\lambda_{k})(\theta_{t}-\eta\mathbf{g}_{0}(\theta_{t}))+\sum_{k\neq 0} \lambda_{k}(-\eta\mathbf{g}_{k}(\theta_{t}))\big{)}\] \[=\arg\max_{\boldsymbol{\lambda}}\widehat{\mathcal{P}}\big{(}(1- \sum_{k\neq 0}\lambda_{k})(\theta_{t}-\eta\mathbf{g}_{0}(\theta_{t}))+\sum_{k \neq 0}\lambda_{k}(\theta_{t}-\eta\mathbf{g}_{0}(\theta_{t})-\eta\mathbf{g}_{k}( \theta_{t}))\big{)}\]

By definitions of \(\boldsymbol{\Lambda}\) and \(\{\boldsymbol{\omega}^{k}\}_{k=0}^{K}\)

\[\Lambda_{k}=\begin{cases}1-\sum_{i\neq 0}\lambda_{i},&k=0,\\ \qquad\lambda_{k},&k\neq 0,\end{cases}\]

\[\omega_{i}^{k}=\begin{cases}1,&i=0\text{ or }i=k,\\ 0,&\text{ otherwise},\end{cases}\]we can prove that optimizing \(\bm{\lambda}\) in Equation (10) is equivalent to optimizing \(\bm{\Lambda}\) as follows:

\[\bm{\Lambda}^{*}=\arg\max_{\bm{\Lambda}}\widehat{\mathcal{P}}\big{(}\sum_{k} \Lambda_{k}\theta_{t+1}(\bm{\omega}^{k})\big{)}.\]

#### Remarks on the search step.

Grid searching all possible values of \(\bm{\Lambda}\) is computationally expensive especially when \(\|\bm{\Lambda}\|\) is large. Thus, here we introduce a greedy search strategy in Algorithm 3, which reduces the computation complexity from exponential complexity to \(\mathcal{O}(\|\bm{\Lambda}\|)\).

```
0: A list of model parameters \(\theta_{1},...,\theta_{B}\) sorted in decreasing order of \(\widehat{\mathcal{P}}(\theta_{b})\).
0: optimal linear combination coefficient \(\bm{\Lambda}^{*}\)
1: unnormalized combination coefficient \(\widetilde{\bm{\Lambda}}\leftarrow\mathbf{e}_{1}\)\(\triangleright\) initialization
2:for\(b=2\)to\(B\)do
3: set upper bound \(U\leftarrow\frac{1}{b-1}\sum_{m=1}^{b-1}\widetilde{\Lambda}_{m}\)
4: grid search the optimal \(\widetilde{\Lambda}_{m}\) in range \([0,U]\) to maximize \(\widehat{\mathcal{P}}(\frac{1}{\|\bm{\Lambda}\|}\sum_{m=1}^{b}\widetilde{ \Lambda}_{m}\theta_{m})\)
5:endfor
6:\(\bm{\Lambda}^{*}\leftarrow\frac{1}{\|\bm{\Lambda}\|}\widetilde{\bm{\Lambda}}\)\(\triangleright\) normalization ```

**Algorithm 3** Greedy Search of \(\bm{\Lambda}^{*}\)

## Appendix B Analysis Details

In this section, we provide the implementation details of our analysis experiment in Section 3.

We conduct our analysis on the multi-domain image recognition dataset DomainNet [61]. In our analysis, we use task Painting and Quickdraw in DomainNet as examples of weak negative transfer and strong negative transfer, and other tasks (Real, Sketch, Infograph, Clipart) in DomainNet as auxiliary tasks. Details of these tasks are summarized in Table 8. We use ResNet-18 [23] pre-trained on ImageNet [8] for all experiments.

### Effect of Gradients Conflicts

First, we optimize the model on the target task for \(T=25\)K iterations to obtain \(\theta_{T}\). We adopt mini-batch SGD with momentum of \(0.9\) and batch size of \(48\), and the initial learning rate is set as \(0.01\) with cosine annealing strategy [45].

We repeatedly sample a mini-batch of data and estimate the gradients for the target and auxiliary task \(\mathbf{g}_{\text{tgt}}\) and \(\mathbf{g}_{\text{aux}}\). Figure 8 plots the distribution of gradient cosine similarity (GCS) between \(\mathbf{g}_{\text{tgt}}\) and \(\mathbf{g}_{\text{aux}}\). We find that the gradients of different tasks are nearly orthogonal (\(\cos\phi_{ij}\approx 0\)) in most cases, and highly consistent gradients or severely conflicting gradients are both relatively rare.

Figure 8: The distribution of Gradient Cosine Similarity (GCS). **P** and **Q** are short for Painting and Quickdraw tasks, respectively.

Then, we optimize the same \(\theta_{T}\) with one-step multi-task gradient descent estimated from different data to obtain different \(\theta_{T+1}\),

\[\theta_{T+1}(\lambda)=\theta_{T}-\eta(\mathbf{g}_{\text{tgt}}(\theta_{T})+ \lambda\mathbf{g}_{\text{aux}}(\theta_{T})),\] (14)

where \(\eta=0.01\) and \(\lambda\) takes values from \(\{0,\frac{1}{16},\frac{1}{8},\frac{1}{4},\frac{1}{2},1\}\). We evaluate \(\theta_{T+1}(\lambda)\) and \(\theta_{T+1}(0)\) on the validation set of the target task to calculate the transfer gain (TG) from single-step multi-task gradient descent

\[TG(\lambda)=\widehat{\mathcal{P}}(\theta_{T+1}(\lambda))-\widehat{\mathcal{P }}(\theta_{T+1}(0)).\] (15)

Note that we omit the notation of algorithm \(\mathcal{A}\) in Equation (14) and (15) for simplicity. Then, in Figure 2, we mark the GCS and TG of each data point and fit them with a \(3\)-order polynomial to obtain the corresponding correlation curve.

### Effect of Distribution Shift

Qualitative Visualization.We visualize by t-SNE [74] in Figure 3(a) the representations of the training and test data by the model \(\theta_{T}\) trained in Section B.1. For better visualization, we only keep the top \(10\) categories with the highest frequency in DomainNet. To visualize the impact of \(\lambda\) on the interpolated training distribution, we let the frequency of auxiliary task points be proportional to \(\lambda\). In other words, when the weighing hyper-parameter of the auxiliary task increases, the effect of the auxiliary task on the interpolated distribution will also increase.

Figure 3 provides the t-SNE visualization of training and test distributions when \(\lambda\) takes values from \(\{0,\frac{1}{16},\frac{1}{4},1\}\). We observe that for weak negative transfer tasks, when \(\lambda\) initially increases, the area of training distribution can better cover that of the test distribution. But as \(\lambda\) continues to increase, the distribution shift between the test set and the training set will gradually increase. For strong negative transfer tasks, however, the shift between the interpolated training distribution and the test distribution monotonically enlarges as \(\lambda\) increases.

Quantitative Measure.First, we jointly optimize the model on the target task and auxiliary tasks with different weighting hyper-parameter \(\lambda\) for \(T=25\)K iterations to obtain \(\theta_{T}(\lambda)\). We adopt the same hyper-parameters as in Section B.1. Then we evaluate \(\theta_{T}(\lambda)\) on the test set of the target tasks and calculate the average confidence on the test set. We can calculate the confidence score discrepancy (CSD) by Definition 3.5 and the transfer gain (TG) by

\[TG(\lambda)=\widehat{\mathcal{P}}(\theta_{T}(\lambda))-\widehat{\mathcal{P} }(\theta_{T}(0)).\] (16)

Again, we omit the notation of algorithm \(\mathcal{A}\) for simplicity. Finally, we plot the curve between CSD and TG under different \(\lambda\) in Figure 3(b).

## Appendix C Experiment Details

### Definition of \(\Delta_{m}\)

Following [55; 37], we report \(\Delta_{m}\) as the performance measure, which is the average per-task performance improvement of method \(m\) relative to the STL baseline \(b\). Formally, \(\Delta_{m}=\frac{1}{K}\sum_{k=1}^{K}(-1)^{z_{k}}(M_{m,k}-M_{b,k})/M_{b,k}\) where \(M_{b,k}\) and \(M_{m,k}\) is the performance of the \(k\)-th task obtained by the baseline method \(b\) and the compared method \(m\). \(z_{k}\) is set to \(0\) if a higher value indicates better performance for the \(k\)-th task and otherwise \(1\).

### Auxiliary-Task Scene Understanding on NYU

Experiment Details.We use DeepLabV3+ architecture [1], where a ResNet-50 network [23] pretrained on the ImageNet dataset [8] with dilated convolutions is used as a shared encoder among tasks and the Atrous Spatial Pyramid Pooling module is used as task-specific head for each task. Following [44; 79], each method is trained for \(200\) epochs with the Adam optimizer [29] and batch size of \(8\). The initial learning rate is \(10^{-4}\) and halved to \(5\times 10^{-5}\) after \(100\) epochs. In ForkMerge, the parameters are merged every \(10\) epochs. Table 7 presents the full evaluation results of Table 1.

### Auxiliary-Domain Image Recognition on DomainNet

Dataset Details.As the original DomainNet [61] does not provide a separate validation set, we randomly split \(50\%\) data from the test set as the validation set, and use the rest \(50\%\) data as the test set. For each task, the proportions of training set, validation set, and test set are approximately \(70\%/15\%/15\%\). Table 8 summarizes the statistics of this dataset. DomainNet is under Custom (research-only, non-commercial) license.

Experiment Details.We adopt mini-batch SGD with momentum of \(0.9\) and batch size of \(48\). We search the initial learning rate in \(\{0.003,0.01,0.03\}\) and adopt cosine annealing strategy [45] to adjust learning rate during training. We adopt ResNet-101 pretrained on ImageNet as the backbone. Each method is trained for \(50\)K iterations. In ForkMerge, the parameters are merged every \(12.5\)K iterations.

### CTR and CTCVR Prediction on AliExpress

Dataset Details.AliExpress [36] is gathered from the real-world traffic logs of AliExpress search system in Taobao and contains more than \(100\)M records in total. We split the first \(90\%\) data in the time sequence to be training set and the rest \(5\%\) and \(5\%\) to be validation set and test set. AliExpress consists of \(2\) tasks: click-through rate (CTR) and click-through conversion rate (CTCVR), and 4 scenarios: Spain (ES), French (FR), Netherlands (NL), and America (US). Table 9 summarizes the statistics of this dataset. AliExpress is under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license.

Experiment Details.The architecture of most methods is based on ESMM [47], which consists of a single embedding layer shared by all tasks and multiple independent DNN towers for each task. The embedding dimension for each feature field is \(128\). Each method is trained for \(50\) epochs using the Adam optimizer, with the batch size of \(2048\), learning rate of \(10^{-3}\) and weight decay of \(10^{-6}\).

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{2}{c}{**Segmentation**} & \multicolumn{2}{c}{**Depth**} & \multicolumn{3}{c}{**Normal**} & \multirow{2}{*}{\(\bm{\Delta}_{\bm{m}}\)\(\uparrow\)} \\ \cline{2-2} \cline{6-9}  & \multicolumn{2}{c}{**mIoU\(\uparrow\)**} & \multicolumn{1}{c}{**Pix Acc\(\uparrow\)**} & \multicolumn{1}{c}{**Abs Err\(\uparrow\)**} & \multicolumn{1}{c}{**Rel Err\(\downarrow\)**} & \multicolumn{1}{c}{**Mean\(\downarrow\)**} & \multicolumn{1}{c}{**Median\(\downarrow\)**} & \multicolumn{1}{c}{**11.25\(\uparrow\)**} & \multicolumn{1}{c}{**22.5\(\uparrow\)**} & \multicolumn{1}{c}{**30\(\uparrow\)**} \\ \cline{3-10} \cline{6-10} \cline{11-11}  & & & & & & **Mean\(\downarrow\)** & **Median\(\downarrow\)** & **11.25\(\uparrow\)** & **22.5\(\uparrow\)** & **30\(\uparrow\)** \\ \hline STL & 51.42 & 74.14 & 41.74 & 17.37 & 22.82 & 16.23 & 36.58 & 62.75 & 73.52 & - \\ \hline EW & 52.13 & 74.51 & 39.03 & 16.43 & 24.14 & 17.62 & 33.98 & 59.63 & 70.93 & 0.30\% \\ UW & 52.51 & 74.72 & 39.15 & 16.56 & 23.99 & 17.36 & 34.46 & 60.13 & 71.32 & 0.63\% \\ DWA & 52.10 & 74.45 & 39.26 & 16.57 & 24.12 & 17.62 & 33.88 & 59.72 & 71.08 & 0.07\% \\ RLW & 52.88 & 74.99 & 39.75 & 16.67 & 23.83 & 17.23 & 34.76 & 60.42 & 71.50 & 0.66\% \\ MGDA & 50.79 & 73.81 & 39.19 & 16.25 & 23.14 & 16.46 & 36.15 & 62.17 & 72.97 & 1.44\% \\ GradNorm & 52.25 & 74.54 & 39.31 & 16.37 & 23.86 & 17.46 & 34.13 & 60.09 & 71.45 & 0.56\% \\ PCGrad & 51.77 & 74.72 & 38.91 & 16.36 & 24.31 & 17.66 & 33.93 & 59.43 & 70.62 & 0.22\% \\ IMTL & 52.24 & 74.73 & 39.46 & **15.92** & 23.25 & 16.64 & 35.86 & 61.81 & 72.73 & 2.10\% \\ GradVac & 52.84 & 74.77 & 39.48 & 16.28 & 24.00 & 17.49 & 34.21 & 59.94 & 71.26 & 0.75\% \\ CAGGrad & 52.04 & 74.25 & 39.06 & 16.30 & 23.39 & 16.89 & 35.35 & 61.28 & 72.42 & 1.41\% \\ NashMTL & 51.73 & 74.10 & 39.55 & 16.50 & 23.21 & 16.74 & 35.39 & 61.80 & 72.92 & 1.11\% \\ \hline GCS & 52.67 & 74.59 & 39.72 & 16.64 & 24.10 & 17.56 & 34.04 & 59.80 & 71.04 & 0.09\% \\ OL\_AUX & 52.07 & 74.28 & 39.32 & 16.30 & 23.98 & 17.87 & 33.89 & 59.53 & 71.08 & 0.17\% \\ ARML & 52.73 & 74.85 & 39.61 & 16.65 & 23.89 & 17.50 & 34.24 & 59.87 & 71.39 & 0.37\% \\ Auto-\(\lambda\) & 52.40 & 74.62 & 39.25 & 16.25 & 23.38 & 17.20 & 34.05 & 61.18 & 72.05 & 1.17\% \\ Post-train & 52.08 & 74.86 & 39.58 & 16.77 & 22.98 & 16.48 & 36.04 & 62.27 & 73.20 & 1.49\% \\ ForkMerge & **53.67** & **75.64** & **38.91** & 16.47 & **22.18** & **15.60** & **37.93** & **64.29** & **74.81** & **4.03\%** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Performance on NYUv2 dataset.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Tasks** & **\#Train** & **\#Val** & **\#Test** & **Description** \\ \hline Clipart & 33.5K & 7.3K & 7.3K & collection of clipart images \\ Real & 120.9K & 26.0K & 26.0K & photos and real world images \\ Sketch & 48.2K & 10.5K & 10.5K & sketches of specific objects \\ Infograph & 36.0K & 7.8K & 7.8K & infographic images \\ Painting & 50.4K & 10.9K & 10.9K & painting depictions of objects \\ Quickdraw & 120.7K & 25.9K & 25.9K & drawings of game “Quick Draw” \\ \hline \hline \end{tabular}
\end{table}
Table 8: Overview of DomainNet dataset.

### Semi-supervised Learning on CIFAR10 and SVHN

Dataset Details.Following [67], we first split the original training set of CIFAR10 [31] and SVHN [56] into training set and validation set. Then, we randomly sample labeled images from the training set. Table 10 summarizes the statistics of CIFAR-10 and SVHN.

Experiment Details.(1) **Auxiliary Tasks.** Following [82; 67], we consider two self-supervised auxiliary tasks Rotation [19] and Exempler-MT [14]. In Rotation, we rotate each image by \([0^{\circ},90^{\circ},180^{\circ},270^{\circ}]\) and ask the network to predict the angle. In Exemplar-MT, the model is trained to extract feature invariant to a wide range of image transformations. (2) **Hyper-parameters.** We adopt Adam [29] optimizer with an initial learning rate of \(0.005\). We train each method for \(200K\) iterations and decay the learning rate by a factor of \(0.2\) at \(160K\) iterations. We use Wide ResNet-28-2 [80] as the backbone. In ForkMerge, the parameters are merged every \(10K\) iterations.

### Data Division Strategy for ForkMerge

As discussed in Section 4.2, in ForkMerge, we can construct branches with different sets of auxiliary tasks. Below we outline the specific data division strategy used in our experiments, which is consistent with previous ATL literature:

* For the NYUv2 dataset, multiple tasks share the same input, but their outputs are different. In this setup, each branch has the same input data, which includes the entire dataset. The distinction between different branches solely lies in the task weighting vector \(\{\bm{\nu}^{b}\}_{b=1}^{B}\).
* For DomainNet, AliExpress, CIFAR-10, and SVHN datasets, different tasks have both different inputs and outputs. In these cases, for each branch, if the task weighting of a specific task is set to \(0\), the data from that particular task will not be used for training the corresponding branch.

## Appendix D Additional Experiments

### Analysis on the importance of different forking branches

The importance of different forking branches is dynamic.As shown in Figure 9, the relative ratio of each forking branch is dynamic and varies from task to task, which indicates the importance of the dynamic merge mechanism.

### Analysis on the computation cost

The computation cost of Algorithm 2 is \(\mathcal{O}(K)\) and the computation cost of the pruned version is \(\mathcal{O}(B)\). Usually, only one model is optimized in most previous multi-task learning methods, yet their

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Datasets** & **\#Labed** & **\#Unlabeled** & **\#Val** & **\#Test** \\ \hline CIFAR-10 & 4000 & 41000 & 5000 & 10000 \\ SVHN & 1000 & 64931 & 7326 & 26032 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Overview of CIFAR-10 and SVHN datasets.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Statistics** & **ES** & **FR** & **NL** & **US** \\ \hline \#Product & 8.7M & 7.4M & 6M & 8M \\ \#PV & 2M & 1.7M & 1.2M & 1.8M \\ \#Impression & 31.6M & 27.4M & 17.7M & 27.4M \\ Click & 841K & 535K & 382K & 480K \\ \#Purchase & 19.1K & 14.4K & 13.8K & 10.9K \\ \hline CTR & 2.66\% & 2.01\% & 2.16\% & 1.64\% \\ CTCVR & 0.60\% & 0.54\% & 0.78\% & 0.40\% \\ \hline \hline \end{tabular}
\end{table}
Table 9: Overview of AliExpress dataset, where CTR = #Click / #Impression and CTCVR = #Purchase / #Impression.

computational costs are not necessarily \(\mathcal{O}(1)\). Gradient balancing methods, including MGDA [65], GradNorm [5], PCGrad [79], IMTL [41], GradVac [76], CAGrad [40], NashMTL [55], GCS [15], OL_AUX [39], and ARML [67], require computing gradients of each task, thus leading to \(\mathcal{O}(K)\) complexity. In addition, calculating the inner product or norm of the gradients will bring a calculation cost proportional to the number of network parameters. A common practical improvement is to compute gradients of the shared representation [65]. Yet the speedup is architecture-dependent, and this technique may degrade performance [55].

In Figure 10, we also compare the actual training time across these methods on NYUv2. We can observe that ForkMerge does not require more time than most other methods. And considering the significant performance gains it brings, these additional computational costs are also worth it. Furthermore, our fork and merge mechanism enables extremely easy asynchronous optimization which is not straightforward in previous methods, thus the training time of our method can be reduced to \(\mathcal{O}(1)\) when there are multiple GPUs available.

### Analysis on the convergence and variance

Figure 11 plots the validation performance of STL, EW, and ForkMerge throughout the training process on NYUv2. Each curve is obtained by optimizing the same method with \(5\) different seeds. Compared with single-task learning or minimizing the average loss across all tasks, ForkMerge not only improves the final generalization but also speeds up the convergence and reduces the fluctuations during training.

### Comparison with grid searching \(\lambda\)

In Section 3, we observe that adjusting the task-weighting hyper-parameter \(\lambda\) can effectively reduce the negative transfer and promote the positive transfer. [77] also suggests that sweeping the task weights should be sufficient for full exploration of the Pareto frontier at least for convex setups and observe no improvements in terms of final performance from previous MTL algorithms compared with grid search.

Figure 10: Training speed of different MTL methods on NYUv2 (\(10\) repetitions).

Figure 9: Importance of different forking branches during training on NYUv2.

[MISSING_PAGE_FAIL:22]

This makes Single Task Learning (STL) less effective and consequently leads to the Equal Weighting (EW) method outperforming STL, causing the Post-train method to fall short of EW and Auto-\(\lambda\). In this case, ForkMerge still exhibited superior performance, validating its efficacy across different network architectures.

ForkMerge with Multi-task Architectures.ForkMerge is complementary to different multi-task architectures. In Tables 13 and 14, we provide a comparison of different optimization strategies with MTAN [44] and MMoE [46] as architectures, which are widely used in multi-task computer vision tasks and multi-task recommendation tasks respectively. On these specifically designed multi-task architectures, ForkMerge is still significantly better than other methods.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{4}{c}{**Segmentation**} & \multicolumn{2}{c}{**Depth**} & \multicolumn{4}{c}{**Normal**} & \multirow{2}{*}{\(\boldsymbol{\Delta}_{m}\uparrow\)} \\ \cline{2-2} \cline{5-10}  & **mloU\(\uparrow\)** & **Pix Acc\(\uparrow\)** & & & & & & & & \\ \hline STL & 52.10 & 74.42 & 40.45 & 16.34 & 22.35 & 15.23 & 38.96 & 64.56 & 74.51 & 3.05\% \\ EW & 53.27 & 75.36 & 39.37 & 16.38 & 23.61 & 17.00 & 35.00 & 61.01 & 72.07 & 1.62\% \\ GCS & 53.05 & 74.79 & 39.50 & 16.49 & 24.05 & 17.49 & 34.14 & 59.88 & 71.13 & 0.57\% \\ QL\_AUX & 52.47 & 74.70 & 39.27 & 16.39 & 23.66 & 17.43 & 34.49 & 59.96 & 71.76 & 0.82\% \\ ARML & 52.33 & 74.59 & 39.46 & 16.61 & 23.57 & 17.41 & 34.56 & 60.12 & 72.04 & 0.55\% \\ Auto-\(\lambda\) & 52.90 & 75.03 & 39.67 & 16.45 & 22.71 & 15.60 & 38.35 & 64.09 & 73.92 & 3.18\% \\ ForkMerge\({}^{\ddagger}\) & **55.25** & **76.16** & **38.45** & **16.08** & **21.94** & **15.22** & **38.96** & **65.04** & **75.33** & **5.76\%** \\ \hline \hline \end{tabular}
\end{table}
Table 13: Performance on NYUv2 dataset by replacing the DeepLabV3+ architecture with the MTAN architecture.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multirow{2}{*}{**Batch Size**} & \multicolumn{2}{c}{**Segmentation**} & \multicolumn{2}{c}{**Depth**} & \multicolumn{4}{c}{**Normal**} & \multirow{2}{*}{\(\boldsymbol{\Delta}_{m}\uparrow\)} \\ \cline{3-3} \cline{5-10}  & & **mloU\(\uparrow\)** & **Pix Acc\(\uparrow\)** & & & & & & & \\ \cline{3-3} \cline{5-10}  & & & & & & & & & & \\ \hline EW & 8 & 52.13 & 74.51 & 39.03 & 16.43 & 24.14 & 17.62 & 33.98 & 59.63 & 70.93 & 0.30\% \\ EW & 32 & 51.40 & 73.99 & 38.86 & 16.20 & 23.99 & 17.34 & 34.58 & 60.08 & 70.85 & 0.55\% \\ ForkMerge\({}^{\ddagger}\) & 8 & **54.30** & **75.78** & **38.42** & **16.11** & **22.41** & **15.72** & **37.81** & **63.89** & **74.35** & **4.59\%** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Comparison of different methods with larger batch size training.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{4}{c}{**CTR**} & \multicolumn{4}{c}{**CVCTR**} & \multirow{2}{*}{**Avg**} & \multirow{2}{*}{\(\boldsymbol{\Delta}_{m}\uparrow\)} \\ \cline{2-2} \cline{5-10}  & **ES** & & & & & & & & & \\ \hline EW & 0.7287 & 0.7244 & 0.7225 & 0.7068 & 0.8874 & 0.8669 & 0.8688 & 0.8742 & 0.7974 & 0.11\% \\ GCS & 0.7300 & 0.7190 & 0.7270 & 0.7102 & 0.8857 & 0.8773 & 0.8680 & 0.8740 & 0.7989 & 0.29\% \\ OL\_AUX & 0.7265 & 0.7283 & 0.7264 & **0.7146** & 0.8849 & 0.8750 & 0.8710 & 0.8770 & 0.8005 & 0.50\% \\ ARML & 0.7289 & 0.7278 & 0.7248 & 0.7081 & 0.8869 & 0.8801 & 0.8714 & 0.8610 & 0.7986 & 0.26\% \\ Auto-\(\lambda\) & 0.7269 & 0.7273 & 0.7256 & 0.7111 & 0.8827 & **0.8811** & **0.8721** & 0.8726 & 0.7999 & 0.42\% \\ ForkMerge\({}^{\ddagger}\) & **0.7368** & **0.7349** & **0.7359** & 0.7116 & **0.8942** & 0.8791 & 0.8717 & **0.8840** & **0.8060** & **1.20\%** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Comparison of different methods with larger batch size training.