# Flow Matching for Scalable Simulation-Based Inference

Jonas Wildberger\({}^{*}\)

Max Planck Institute for Intelligent Systems

Tubingen, Germany

wildberger.jonas@tuebingen.mpg.de

&Maximilian Dax\({}^{*}\)

Max Planck Institute for Intelligent Systems

Tubingen, Germany

maximilian.dax@tuebingen.mpg.de

&Simon Buchholz\({}^{*}\)

Max Planck Institute for Intelligent Systems

Tubingen, Germany

sbuchholz@tue.mpg.de

&Stephen R. Green

University of Nottingham

Nottingham, United Kingdom

&Jakob H. Macke

Max Planck Institute for Intelligent Systems &

Machine Learning in Science,

University of Tubingen

Tubingen, Germany

Equal contribution

###### Abstract

Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems.

## 1 Introduction

The ability to readily represent Bayesian posteriors of arbitrary complexity using neural networks would herald a revolution in scientific data analysis. Such networks could be trained using simulated data and used for amortized inference across observations--bringing tractable inference and speed to a myriad of scientific models. Thanks to innovative architectures such as normalizing flows [1; 2], approaches to neural simulation-based inference (SBI) [3] have seen remarkable progress in recent years. Here, we show that modern approaches to deep generative modeling (particularly flow matching) deliver substantial improvements in simplicity, flexibility and scaling when adapted to SBI.

The Bayesian approach to data analysis is to compare observations to models via the posterior distribution \(p(\theta|x)\). This gives our degree of belief that model parameters \(\theta\) gave rise to an observation \(x\), and is proportional to the model likelihood \(p(x|\theta)\) times the prior \(p(\theta)\). One is typically interested in representing the posterior in terms of a collection of samples, however obtaining these through standard likelihood-based algorithms can be challenging for intractable or expensive likelihoods. In such cases, SBI offers an alternative based instead on _data simulations_\(x\sim p(x|\theta)\). Combined with deep generative modeling, SBI becomes a powerful paradigm for scientific inference [3]. Neural posterior estimation (NPE) [4; 5; 6], for instance, trains a conditional density estimator \(q(\theta|x)\) to approximate the posterior, allowing for rapid sampling and density estimation for any \(x\) consistent with the training distribution.

The NPE density estimator \(q(\theta|x)\) is commonly taken to be a (discrete) normalizing flow [1; 2], an approach that has brought state-of-the-art performance in challenging problems such as gravitational-wave inference [7]. Naturally, performance hinges on the expressiveness of \(q(\theta|x)\). Normalizing flows transform noise to samples through a discrete sequence of basic transforms. These have been carefully engineered to be invertible with simple Jacobian determinant, enabling efficient maximum likelihood training, while at the same time producing expressive \(q(\theta|x)\). Although many such discrete flows are universal density approximators [2], in practice, they can be challenging to scale to very large networks, which are needed for big-data experiments.

Recent studies [8; 9] propose neural posterior score estimation (NPSE), a rather different approach that models the posterior distribution with score-matching (or diffusion) networks. These techniques were originally developed for generative modeling [10; 11; 12], achieving state-of-the-art results in many domains, including image generation [13; 14]. Like discrete normalizing flows, diffusion models transform noise into samples, but with trajectories parametrized by a _continuous_ "time" parameter \(t\). The trajectories solve a stochastic differential equation [15] (SDE) defined in terms of a vector field \(v_{t}\), which is trained to match the score of the intermediate distributions \(p_{t}\). NPSE has several advantages compared to NPE, including the ability to combine multiple observations at inference time [9] and, importantly, the freedom to use unconstrained network architectures.

We here propose to use flow matching, another recent technique for generative modeling, for Bayesian inference, an approach we refer to as flow-matching posterior estimation (FMPE). Flow matching is also based on a vector field \(v_{t}\) and thereby also admits flexible network architectures (Fig. 1). For flow

Figure 1: Comparison of network architectures (left) and flow trajectories (right). Discrete flows (NPE, top) require a specialized architecture for the density estimator. Continuous flows (FMPE, bottom) are based on a vector field parametrized with an unconstrained architecture. FMPE uses this additional flexibility to put an enhanced emphasis on the conditioning data \(x\), which in the SBI context is typically high dimensional and in a complex domain. Further, the optimal transport path produces simple flow trajectories from the base distribution (inset) to the target.

matching, however, \(v_{t}\) directly defines the velocity field of sample trajectories, which solve ordinary differential equations (ODEs) and are deterministic. As a consequence, flow matching allows for additional freedom in designing non-diffusion paths such as optimal transport, and provides direct access to the density [16]. These differences are summarized in Tab. 1.

Our contributions are as follows:

* We adapt flow-matching to Bayesian inference, proposing FMPE. In general, the modeling requirements of SBI are different from generative modeling. For the latter, sample quality is critical, i.e., that samples lie in the support of a complex distribution (e.g., images). In contrast, for SBI, \(p(\theta|x)\) is typically less complex for fixed \(x\), but \(x\) itself can be complex and high-dimensional. We therefore consider pyramid-like architectures from \(x\) to \(v_{t}\), with gated linear units to incorporate \((\theta,t)\) dependence, rather than the typical U-Net used for images (Fig. 1). We also propose an alternative \(t\)-weighting in the loss, which improves performance in many tasks.
* Under certain regularity assumptions, we prove an upper bound on the KL divergence between the model and target posterior. This implies that estimated posteriors are mass-covering, i.e., that their support includes all \(\theta\) consistent with observed \(x\), which is highly desirable for scientific applications [17].
* We perform a number of experiments to investigate the performance of FMPE.2 Our two-pronged approach, which involves a set of benchmark tests and a real-world problem, is designed to probe complementary aspects of the method, covering breadth and depth of applications. First, on an established suite of SBI benchmarks, we show that FMPE performs comparably--or better--than NPE across most tasks, and in particular exhibits mass-covering posteriors in all cases (Sec. 4). We then push the performance limits of FMPE on a challenging real-world problem by turning to gravitational-wave inference (Sec. 5). We show that FMPE substantially outperforms an NPE baseline in terms of training time, posterior accuracy, and scaling to larger networks.

Footnote 2: Code available here.

## 2 Preliminaries

Normalizing flows.A normalizing flow [1; 2] defines a probability distribution \(q(\theta|x)\) over parameters \(\theta\in\mathbb{R}^{n}\) in terms of an invertible mapping \(\psi_{x}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\) from a simple base distribution \(q_{0}(\theta)\),

\[q(\theta|x)=(\psi_{x})_{*}q_{0}(\theta)=q_{0}(\psi_{x}^{-1}(\theta))\det \left|\frac{\partial\psi_{x}^{-1}(\theta)}{\partial\theta}\right|\,,\] (1)

where \((\cdot)_{*}\) denotes the pushforward operator, and for generality we have conditioned on additional context \(x\in\mathbb{R}^{m}\). Unless otherwise specified, a normalizing flow refers to a _discrete_ flow, where \(\psi_{x}\) is given by a composition of simpler mappings with triangular Jacobians, interspersed with shuffling of the \(\theta\). This construction results in expressive \(q(\theta|x)\) and also efficient density evaluation [2].

Continuous normalizing flows.A continuous flow [18] also maps from base to target distribution, but is parametrized by a continuous "time" \(t\in[0,1]\), where \(q_{0}(\theta|x)=q_{0}(\theta)\) and \(q_{1}(\theta|x)=q(\theta|x)\). For each \(t\), the flow is defined by a vector field \(v_{t,x}\) on the sample space.3 This corresponds to the velocity of the sample trajectories,

Footnote 3: In the SBI literature, this is also commonly referred to as “parameter space”.

\[\frac{d}{dt}\psi_{t,x}(\theta)=v_{t,x}(\psi_{t,x}(\theta)),\qquad\psi_{0,x}( \theta)=\theta.\] (2)

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & NPE & NPSE & **FMPE (Ours)** \\ \hline Tractable posterior density & Yes & No & Yes \\ Unconstrained network architecture & No & Yes & Yes \\ Network passes for sampling & Single & Many & Many \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of posterior-estimation methods.

We obtain the trajectories \(\theta_{t}\equiv\psi_{t,x}(\theta)\) by integrating this ODE. The final density is given by

\[q(\theta|x)=(\psi_{1,x})_{*}q_{0}(\theta)=q_{0}(\theta)\exp\left(-\int_{0}^{1} \operatorname{div}v_{t,x}(\theta_{t})\operatorname{d}\!t\right),\] (3)

which is obtained by solving the transport equation \(\partial_{t}q_{t}+\operatorname{div}(q_{t}v_{t,x})=0\).

The advantage of the continuous flow is that \(v_{t,x}(\theta)\) can be simply specified by a neural network taking \(\mathbb{R}^{n+m+1}\to\mathbb{R}^{n}\), in which case (2) is referred to as a _neural ODE_[18]. Since the density is tractable via (3), it is in principle possible to train the flow by maximizing the (log-)likelihood. However, this is often not feasible in practice, since both sampling and density estimation require many network passes to numerically solve the ODE (2).

Flow matching.An alternative training objective for continuous normalizing flows is provided by flow matching [16]. This directly regresses \(v_{t,x}\) on a vector field \(u_{t,x}\) that generates a target probability path \(p_{t,x}\). It has the advantage that training does not require integration of ODEs, however it is not immediately clear how to choose \((u_{t,x},p_{t,x})\). The key insight of [16] is that, if the path is chosen on a _sample-conditional_ basis,4 then the training objective becomes extremely simple. Indeed, given a sample-conditional probability path \(p_{t}(\theta|\theta_{1})\) and a corresponding vector field \(u_{t}(\theta|\theta_{1})\), we specify the sample-conditional flow matching loss as

Footnote 4: We refer to conditioning on \(\theta_{1}\) as _sample_-conditioning to distinguish from conditioning on \(x\).

\[\mathcal{L}_{\text{SCFM}}=\mathbb{E}_{t\sim\mathcal{U}[0,1],\,x\sim p(x),\, \theta_{1}\sim p(\theta|x),\,\theta_{t}\sim p_{t}(\theta_{t}|\theta_{1})}\left\| v_{t,x}(\theta_{t})-u_{t}(\theta_{t}|\theta_{1})\right\|^{2}.\] (4)

Remarkably, minimization of this loss is equivalent to regressing \(v_{t,x}(\theta)\) on the _marginal_ vector field \(u_{t,x}(\theta)\) that generates \(p_{t}(\theta|x)\)[16]. Note that in this expression, the \(x\)-dependence of \(v_{t,x}(\theta)\) is picked up via the expectation value, with the sample-conditional vector field independent of \(x\).

There exists considerable freedom in choosing a sample-conditional path. Ref. [16] introduces the family of Gaussian paths

\[p_{t}(\theta|\theta_{1})=\mathcal{N}(\theta|\mu_{t}(\theta_{1}),\sigma_{t}( \theta_{1})^{2}I_{n}),\] (5)

where the time-dependent means \(\mu_{t}(\theta_{1})\) and standard deviations \(\sigma_{t}(\theta_{1})\) can be freely specified (subject to boundary conditions5). For our experiments, we focus on the optimal transport paths defined by \(\mu_{t}(\theta_{1})=t\theta_{1}\) and \(\sigma_{t}(\theta_{1})=1-(1-\sigma_{\text{min}})t\) (also introduced in [16]). The sample-conditional vector field then has the simple form

Footnote 5: The sample-conditional probability path should be chosen to be concentrated around \(\theta_{1}\) at \(t=1\) (within a small region of size \(\sigma_{\text{min}}\)) and to be the base distribution at \(t=0\).

\[u_{t}(\theta|\theta_{1})=\frac{\theta_{1}-(1-\sigma_{\text{min}})\theta}{1-(1- \sigma_{\text{min}})t}.\] (6)

Neural posterior estimation (NPE).NPE is an SBI method that directly fits a density estimator \(q(\theta|x)\) (usually a normalizing flow) to the posterior \(p(\theta|x)\)[4, 5, 6]. NPE trains with the maximum likelihood objective \(\mathcal{L}_{\text{NPE}}=-\mathbb{E}_{p(\theta)p(x|\theta)}\log q(\theta|x)\), using Bayes' theorem to simplify the expectation value with \(\mathbb{E}_{p(x)p(\theta|x)}\to\mathbb{E}_{p(\theta)p(x|\theta)}\). During training, \(\mathcal{L}_{\text{NPE}}\) is estimated based on an empirical distribution consisting of samples \((\theta,x)\sim p(\theta)p(x|\theta)\). Once trained, NPE can perform inference for every new observation using \(q(\theta|x)\), thereby _amortizing_ the computational cost of simulation and training across all observations. NPE further provides exact density evaluations of \(q(\theta|x)\). Both of these properties are crucial for the physics application in section 5, so we aim to retain these properties with FMPE.

#### Related work

Flow matching [16] has been developed as a technique for generative modeling, and similar techniques are discussed in [19, 20, 21] and extended in [22, 23]. Flow matching encompasses the deterministic ODE version of diffusion models [10, 11, 12] as a special instance. Although to our knowledge flow matching has not previously been applied to Bayesian inference, score-matching diffusion models have been proposed for SBI in [8, 9] with impressive results. These studies, however, use stochastic formulations via SDEs [15] or Langevin steps and are therefore not directly applicable when evaluations of the posterior density are desired (see Tab. 1). It should be noted that score modeling can also be used to parameterize continuous normalizing flows via an ODE. Extension of [8; 9] to the deterministic formulation could thereby be seen as a special case of flow matching. Many of our analyses and the practical guidance provided in Section 3 therefore also apply to score matching.

We here focus on comparisons of FMPE against NPE [4; 5; 6], as it best matches the requirements of the application in section 5. Other SBI methods include approximate Bayesian computation [24; 25; 26; 27; 28], neural likelihood estimation [29; 30; 31; 32] and neural ratio estimation [33; 34; 35; 36; 37; 38; 39]. Many of these approaches have sequential versions, where the estimator networks are specifically tuned to a specific observation \(x_{\text{o}}\). FMPE has a tractable density, so it is straightforward to apply the sequential NPE [4; 5; 6] approaches to FMPE. In this case, inference is no longer amortized, so we leave this extension to future work.

## 3 Flow matching posterior estimation

To apply flow matching to SBI we use Bayes' theorem to make the usual replacement \(\mathbb{E}_{p(x)p(\theta|x)}\to\mathbb{E}_{p(\theta)p(x|\theta)}\) in the loss function (4), eliminating the intractable expectation values. This gives the FMPE loss

\[\mathcal{L}_{\text{FMPE}}=\mathbb{E}_{t\sim p(t),\;\theta_{1}\sim p(\theta),x \sim p(x|\theta_{1}),\,\theta_{t}\sim p_{t}(\theta_{t}|\theta_{1})}\left\|v_{t,x}(\theta_{t})-u_{t}(\theta_{t}|\theta_{1})\right\|^{2},\] (7)

which we minimize using empirical risk minimization over samples \((\theta,x)\sim p(\theta)p(x|\theta)\). In other words, training data is generated by sampling \(\theta\) from the prior, and then simulating data \(x\) corresponding to \(\theta\). This is in close analogy to NPE training, but replaces the log likelihood maximization with the sample-conditional flow matching objective. Note that in this expression we also sample \(t\sim p(t)\), \(t\in[0,1]\) (see Sec. 3.3), which generalizes the uniform distribution in (4). This provides additional freedom to improve learning in our experiments.

### Probability mass coverage

As we show in our examples, trained FMPE models \(q(\theta|x)\) can achieve excellent results in approximating the true posterior \(p(\theta|x)\). However, it is not generally possible to achieve _exact_ agreement due to limitations in training budget and network capacity. It is therefore important to understand how inaccuracies manifest. Whereas sample quality is the main criterion for generative modeling, for scientific applications one is often interested in the overall shape of the distribution. In particular, an important question is whether \(q(\theta|x)\) is _mass-covering_, i.e., whether it contains the full support of \(p(\theta|x)\). This minimizes the risk to falsely rule out possible explanations of the data. It also allows us to use importance sampling if the likelihood \(p(x|\theta)\) of the forward model can be evaluated, which can be used for precise estimation of the posterior [40; 41].

Consider first the mass-covering property for NPE. NPE directly minimizes the forward KL divergence \(\operatorname{D_{KL}}(p(\theta|x)||q(\theta|x))\), and thereby provides probability-mass covering results. Therefore, even if NPE is not accurately trained, the estimate \(q(\theta|x)\) should cover the entire support of the posterior \(p(\theta|x)\) and the failure to do so can be observed in the validation loss. As an illustration in an unconditional setting, we observe that a unimodal Gaussian \(q\) fitted to a bimodal target distribution \(p\) captures both modes when using the forward KL divergence \(\operatorname{D_{KL}}(p||q)\), but only a single mode when using the backwards direction \(\operatorname{D_{KL}}(q||p)\) (Fig. 2).

For FMPE, we can fit a Gaussian flow-matching model \(q(\theta)=\mathcal{N}(\hat{\mu},\hat{\sigma}^{2})\) to the same bimodal target, in this case, parametrizing the vector field as

\[v_{t}(\theta)=\frac{(\sigma_{t}^{2}+(t\hat{\sigma})^{2}-\sigma_{t})\theta_{t }+t\hat{\mu}\cdot\sigma_{t}}{t\cdot(\sigma_{t}^{2}+(t\hat{\sigma})^{2})}\] (8)

(see Appendix A), we also obtain a mass-covering distribution when fitting the learnable parameters \((\hat{\mu},\hat{\sigma})\) via (4). This provides some indication that the flow matching objective induces mass-covering behavior, and leads us to investigate the more general question of whether the mean squared error

Figure 2: A Gaussian (blue) fitted to a bimodal distribution (gray) with various objectives.

between vector fields \(u_{t}\) and \(v_{t}\) bounds the forward KL divergence. Indeed, the former agrees up to constant with the sample-conditional loss (4) (see Sec. 2).

We denote the flows of \(u_{t}\), \(v_{t}\), by \(\phi_{t}\), \(\psi_{t}\), respectively, and we set \(q_{t}=(\psi_{t})_{*}q_{0}\), \(p_{t}=(\phi_{t})_{*}q_{0}\). The precise question then is whether we can bound \(\mathrm{D}_{\mathrm{KL}}(p_{1}||q_{1})\) by \(\mathrm{MSE}_{p}(u,v)^{\alpha}\) for some positive power \(\alpha\). It was already observed in [42] that this is not true in general, and we provide a simple example to that effect in Lemma 1 in Appendix B. Indeed, it was found in [42] that to bound the forward KL divergence we also need to control the Fisher divergence, \(\int p_{t}(\mathrm{d}\theta)(\nabla\ln p_{t}(\theta)-\nabla q_{t}(\theta))^{2}\).

Here we show instead that a bound can be obtained under sufficiently strong regularity assumptions on \(p_{0}\), \(u_{t}\), and \(v_{t}\). The following statement is slightly informal, and we refer to the supplement for the complete version.

**Theorem 1**.: _Let \(p_{0}=q_{0}\) and assume \(u_{t}\) and \(v_{t}\) are two vector fields whose flows satisfy \(p_{1}=(\phi_{1})_{*}p_{0}\) and \(q_{1}=(\psi_{1})_{*}q_{0}\). Assume that \(p_{0}\) is square integrable and satisfies \(|\nabla\ln p_{0}(\theta)|\leq c(1+|\theta|)\) and \(u_{t}\) and \(v_{t}\) have bounded second derivatives. Then there is a constant \(C>0\) such that (for \(\mathrm{MSE}_{p}(u,v)<1\)))_

\[\mathrm{D}_{\mathrm{KL}}(p_{1}||q_{1})\leq C\,\mathrm{MSE}_{p}(u,v)^{\frac{1 }{2}}.\] (9)

_The proof of this result can be found in appendix B._ While the regularity assumptions are not guaranteed to hold in practice when \(v_{t}\) is parametrized by a neural net, the theorem nevertheless gives some indication that the flow-matching objective encourages mass coverage. In Section 4 and 5, this is complemented with extensive empirical evidence that flow matching indeed provides mass-covering estimates.

We remark that it was shown in [43] that the KL divergence of SDE solutions can be bounded by the MSE of the estimated score function. Thus, the smoothing effect of the noise ensures mass coverage, an aspect that was further studied using the Fokker-Planck equation in [42]. For flow matching, imposing the regularity assumption plays a similar role.

### Network architecture

Generative diffusion or flow matching models typically operate on complicated and high dimensional data in the \(\theta\) space (e.g., images with millions of pixels). One typically uses U-Net [44] like architectures, as they provide a natural mapping from \(\theta\) to a vector field \(v(\theta)\) of the same dimension. The dependence on \(t\) and an (optional) conditioning vector \(x\) is then added on top of this architecture.

For SBI, the data \(x\) is often associated with a complicated domain, such as image or time series data, whereas parameters \(\theta\) are typically low dimensional. In this context, it is therefore useful to build the architecture starting as a mapping from \(x\) to \(v(x)\) and then add conditioning on \(\theta\) and \(t\). In practice, one can therefore use any established feature extraction architecture for data in the domain of \(x\), and adjust the dimension of the feature vector to \(n=\dim(\theta)\). In our experiments, we found that the \((t,\theta)\)-conditioning is best achieved using gated linear units [45] to the hidden layers of the network (see also Fig. 1); these are also commonly used for conditioning discrete flows on \(x\).

### Re-scaling the time prior

The time prior \(\mathcal{U}[0,1]\) in (4) distributes the training capacity uniformly across \(t\). We observed that this is not always optimal in practice, as the complexity of the vector field may depend on \(t\). For FMPE we therefore sample \(t\) in (7) from a power-law distribution \(p_{\alpha}(t)\propto t^{1/(1+\alpha)},\ t\in[0,1]\), introducing an additional hyperparameter \(\alpha\). This includes the uniform distribution for \(\alpha=0\), but for \(\alpha>0\), assigns greater importance to the vector field for larger values of \(t\). We empirically found this to improve learning for distributions with sharp bounds (e.g., Two Moons in Section 4).

## 4 SBI benchmark

We now evaluate FMPE on ten tasks included in the benchmark presented in [46], ranging from simple Gaussian toy models to more challenging SBI problems from epidemiology and ecology, with varying dimensions for parameters (\(\dim(\theta)\in[2,10]\)) and observations (\(\dim(x)\in[2,100]\)). For each task, we train three separate FMPE models with simulation budgets \(N\in\{10^{3},10^{4},10^{5}\}\). Weuse a simple network architecture consisting of fully connected residual blocks [47] to parameterize the conditional vector field. For the two tasks with \(\dim(x)=100\) (B-GLM-Raw, SLCP-D), we condition on \((t,\theta)\) via gated linear units as described in Section 3.2 (Fig. 8 in Appendix C shows the corresponding performance gain). For the remaining tasks with \(\dim(x)\leq 10\) we concatenate \((t,\theta,x)\) instead. We reserve 5% of the simulations for validation. See Appendix C for details.

For each task and simulation budget, we evaluate the model with the lowest validation loss by comparing \(q(\theta|x)\) to the reference posteriors \(p(\theta|x)\) provided in [46] for ten different observations \(x\) in terms of the C2ST score [48, 49]. This performance metric is computed by training a classifier to discriminate inferred samples \(\theta\sim q(\theta|x)\) from reference samples \(\theta\sim p(\theta|x)\). The C2ST score is then the test accuracy of this classifier, ranging from 0.5 (best) to 1.0. We observe that FMPE exhibits comparable performance to an NPE baseline model for most tasks and outperforms on several (Fig. 4). In terms of the MMD metric (Fig. 6 in the Appendix), FMPE clearly outperforms NPE (but MMD can be sensitive to its hyperparameters [46]). As NPE is one of the highest ranking methods for many tasks in the benchmark, these results show that FMPE indeed performs competitively with other existing SBI methods. We report an additional baseline for score matching in Fig. 7 in the Appendix.

As NPE and FMPE both directly target the posterior with a density estimator (in contrast to most other SBI methods), observed differences can be primarily attributed to their different approaches for density estimation. Interestingly, a great performance improvement of FMPE over NPE is observed for SLCP with a large simulation budget (\(N=10^{5}\)). The SLCP task is specifically designed to have a simple likelihood but a complex posterior, and the FMPE performance underscores the enhanced flexibility of the FMPE density estimator.

Finally, we empirically investigate the mass coverage suggested by our theoretical analysis in Section 3.1. We display the density \(\log q(\theta|x)\) of the reference samples \(\theta\sim p(\theta|x)\) under our FMPE model \(q\) as a histogram (Fig. 3). All samples \(\theta\sim p(\theta|x)\) fall into the support from \(q(\theta|x)\). This becomes apparent when comparing to the density \(\log q(\theta|x)\) for samples \(\theta\sim q(\theta|x)\) from \(q\) itself. This FMPE result is therefore mass covering. Note that this does not necessarily imply conservative posteriors (which is also not generally true for the forward KL divergence [17, 50, 51]), and some parts of \(p(\theta|x)\) may still be undersampled. Probability mass coverage, however, implies that no part is entirely missed (compare Fig. 2), even for multimodal distributions such as Two Moons. Fig. 9 in the Appendix confirms the mass coverage for the other benchmark tasks.

## 5 Gravitational-wave inference

### Background

Gravitational waves (GWs) are ripples of spacetime predicted by Einstein and produced by cataclysmic cosmic events such as the mergers of binary black holes (BBHs). GWs propagate across the universe to Earth, where the LIGO-Virgo-KAGRA observatories measure faint time-series signals embedded in noise. To-date, roughly 90 detections of merging black holes and neutron stars have been made [52], all of which have been characterized using Bayesian inference to compare against theoretical models.6 These have yielded insights into the origin and evolution of black holes [53], fundamental properties of matter and gravity [54, 55], and even the expansion rate of the universe [56]. Under reasonable assumptions on detector noise, the GW likelihood is tractable,7 and inference is

Figure 3: Histogram of FMPE densities \(\log q(\theta|x)\) for reference samples \(\theta\sim p(\theta|x)\) (Two Moons task, \(N=10^{3}\)). The estimate \(q(\theta|x)\) clearly covers \(p(\theta|x)\) entirely.

typically performed using tools [57; 58; 59; 60] based on Markov chain Monte Carlo [61; 62] or nested sampling [63] algorithms. This can take from hours to months, depending on the nature of the event and the complexity of the signal model, with a typical analysis requiring up to \(\sim 10^{8}\) likelihood evaluations. The ever-increasing rate of detections means that these analysis times risk becoming a bottleneck. SBI offers a promising solution for this challenge that has thus been actively studied in the literature [64; 65; 66; 67; 68; 7; 41]. A fully amortized NPE-based method called DINGO recently achieved accuracies comparable to stochastic samplers with inference times of less than a minute per event [7]. To achieve accurate results, however, DINGO uses group-equivariant NPE [7; 69] (GNPE), an NPE extension that integrates known conditional symmetries. GNPE, therefore, does not provide a tractable density, which is problematic when verifying and correcting inference results using importance sampling [41].

### Experiments

We here apply FMPE to GW inference. As a baseline, we train an NPE network with the settings described in [7] with a few minor changes (see Appendix D).8 This uses an embedding network [71] to compress \(x\) to a 128-dimensional feature vector, which is then used to condition a neural spline flow [72]. The embedding network consists of a learnable linear layer initialized with principal components of GW simulations followed by a series of dense residual blocks [47]. This architecture is a powerful feature extractor for GW measurements [7]. As pointed out in Section 3.2, it is straightforward to reuse such architectures for FMPE, with the following three modifications: (1) we provide the conditioning on \((t,\theta)\) to the network via gated linear units in each hidden layer; (2) we change the dimension of the final feature vector to the dimension of \(\theta\) so that the network parameterizes the conditional vector field \((t,x,\theta)\to v_{t,x}(\theta)\); (3) we increase the number and width of the hidden layers to use the capacity freed up by removing the discrete normalizing flow.

Footnote 8: Our implementation builds on the public DINGO code from https://github.com/dingo-gw/dingo.

We train the NPE and FMPE networks with \(5\cdot 10^{6}\) simulations for 400 epochs using a batch size of 4096 on an A100 GPU. The FMPE network (\(1.9\cdot 10^{8}\) learnable parameters, training takes \(\approx 2\) days) is larger than the NPE network (\(1.3\cdot 10^{8}\) learnable parameters, training takes \(\approx 3\) days), but trains substantially faster. We evaluate both networks on GW150914 [73], the first detected GW. We generate a reference posterior using the method described in [41]. Fig. 5 compares the inferred posterior distributions qualitatively and quantitatively in terms of the Jensen-Shannon divergence (JSD) to the reference.9

Footnote 9: We omit the three parameters \(\phi_{\mathrm{c}},\phi_{JL},\theta_{JN}\) in the evaluation as we use phase marginalization in importance sampling and the reference therefore uses a different basis for these parameters [41]. For GNPE we report the results from [7], which are generated with slightly different data conditioning. Therefore, we do not display the GNPE results in the corner plot, and the JSDs serve only as a rough comparison. The JSD for the \(t_{\mathrm{c}}\) parameter is not reported in [7] due to a \(t_{\mathrm{c}}\) marginalized reference.

FMPE substantially outperforms NPE in terms of accuracy, with a mean JSD of \(0.5\) mnat (NPE: \(3.6\) mnat), and max JSD \(<2.0\) mnat, an indistinguishability criterion for GW posteriors [59]. Remarkably, FMPE accuracy is even comparable to GNPE, which leverages physical symmetries

Figure 4: Comparison of FMPE with NPE, a standard SBI method, across 10 benchmark tasks [46].

to simplify data. Finally, we find that the Bayesian evidences inferred with NPE (\(\log p(x)=-7667.958\pm 0.006\)) and FMPE (\(\log p(x)=-7667.969\pm 0.005\)) are consistent within their statistical uncertainties. A correct evidence is only obtained in importance sampling when the inferred posterior \(q(\theta|x)\) covers the entire posterior \(p(\theta|x)\)[41], so this is another indication that FMPE indeed induces mass-covering posteriors.

### Discussion

Our results for GW150914 show that FMPE substantially outperforms NPE on this challenging problem. We believe that this is related to the network structure as follows. The NPE network allocates roughly two thirds of its parameters to the discrete normalizing flow and only one third to the embedding network (i.e., the feature extractor for \(x\)). Since FMPE parameterizes just a vector field (rather than a collection of splines in the normalizing flow) it can devote its network capacity to the interpretation of the high-dimensional \(x\in\mathbb{R}^{15744}\). Hence, it scales better to larger networks and achieves higher accuracy. Remarkably, the performance iscomparable to GNPE, which involves a much simpler learning task with likelihood symmetries integrated by construction. This enhanced performance, comes in part at the cost of increased inference times, typically requiring hundreds of network forward passes. See Appendix D for further details.

In future work we plan to carry out a more complete analysis of GW inference using FMPE. Indeed, GW150914 is a loud event with good data quality, where NPE already performs quite well. DINGO with GNPE has been validated in a variety of settings [7; 69; 41; 74] including events with a larger performance gap between NPE and GNPE [69]. Since FMPE (like NPE) does not integrate physical symmetries, it would likely need further enhancements to fully compete with GNPE. This may require a symmetry-aware architecture [75], or simply further scaling to larger networks. A straightforward application of the GNPE mechanism to FMPE--GFMPE--is also possible, but less practical due to the higher inference costs of FMPE. Nevertheless, our results demonstrate that FMPE is a promising direction for future research in this field.

Figure 5: Results for GW150914 [73]. Left: Corner plot showing 1D marginals on the diagonal and 2D 50% credible regions. We display four GW parameters (distance \(d_{\mathrm{L}}\), time of arrival \(t_{\mathrm{c}}\), and sky coordinates \(\alpha,\delta\)); these represent the least accurate NPE parameters. Right: Deviation between inferred posteriors and the reference, quantified by the Jensen-Shannon divergence (JSD). The FMPE posterior matches the reference more accurately than NPE, and performs similarly to symmetry-enhanced GNPE. (We do not display GNPE results on the left due to different data conditioning settings in available networks.)

Conclusions

We introduced flow matching posterior estimation, a new simulation-based inference technique based on continuous normalizing flows. In contrast to existing neural posterior estimation methods, it does not rely on restricted density estimation architectures such as discrete normalizing flows, and instead parametrizes a distribution in terms of a conditional vector field. Besides enabling flexible path specifications, while maintaining direct access to the posterior density, we empirically found that regressing on a vector field rather than an entire distribution improves the scalability of FMPE compared to existing approaches. Indeed, fewer parameters are needed to learn this vector field, allowing for larger networks, ultimately enabling to solve more complex problems. Furthermore, our architecture for FMPE (a straightforward ResNet with GLU conditioning) facilitates parallelization and allows for cheap forward/backward passes.

We evaluated FMPE on a set of 10 benchmark tasks and found competitive or better performance compared to other simulation-based inference methods. On the challenging task of gravitational-wave inference, FMPE substantially outperformed comparable discrete flows, producing samples on par with a method that explicitly leverages symmetries to simplify training. Additionally, flow matching latent spaces are more naturally structured than those of discrete flows, particularly when using paths such as optimal transport. Looking forward, it would be interesting to exploit such structure in designing learning algorithms. This performance and flexibilty underscores the capability of continuous normalizing flows to efficiently solve inverse problems.

## Acknowledgements

We thank the Dingo team for helpful discussions and comments. We would like to particularly acknowledge the contributions of Alessandra Buonanno, Jonathan Gair, Nihar Gupte and Michael Purrer. This material is based upon work supported by NSF's LIGO Laboratory which is a major facility fully funded by the National Science Foundation. This research has made use of data or software obtained from the Gravitational Wave Open Science Center (gw-openscience.org), a service of LIGO Laboratory, the LIGO Scientific Collaboration, the Virgo Collaboration, and KAGRA. LIGO Laboratory and Advanced LIGO are funded by the United States National Science Foundation (NSF) as well as the Science and Technology Facilities Council (STFC) of the United Kingdom, the Max-Planck-Society (MPS), and the State of Niedersachsen/Germany for support of the construction of Advanced LIGO and construction and operation of the GEO600 detector. Additional support for Advanced LIGO was provided by the Australian Research Council. Virgo is funded, through the European Gravitational Observatory (EGO), by the French Centre National de Recherche Scientifique (CNRS), the Italian Istituto Nazionale di Fisica Nucleare (INFN) and the Dutch Nikhef, with contributions by institutions from Belgium, Germany, Greece, Hungary, Ireland, Japan, Monaco, Poland, Portugal, Spain. The construction and operation of KAGRA are funded by Ministry of Education, Culture, Sports, Science and Technology (MEXT), and Japan Society for the Promotion of Science (JSPS), National Research Foundation (NRF) and Ministry of Science and ICT (MSIT) in Korea, Academia Sinica (AS) and the Ministry of Science and Technology (MoST) in Taiwan. M.D. thanks the Hector Fellow Academy for support. J.H.M. and B.S. are members of the MLCoE, EXC number 2064/1 - Project number 390727645 and the Tubingen AI Center funded by the German Ministry for Science and Education (FKZ 01IS18039A).

## References

* Rezende and Mohamed [2015] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _International Conference on Machine Learning_, pages 1530-1538, 2015. arXiv:1505.05770.
* Papamakarios et al. [2021] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _Journal of Machine Learning Research_, 22(57):1-64, 2021. URL: http://jmlr.org/papers/v22/19-1028.html.
* Cranmer et al. [2020] Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. _Proc. Nat. Acad. Sci._, 117(48):30055-30062, 2020. arXiv:1911.01429, doi:10.1073/pnas.1912789117.

* Papamakarios and Murray [2016] George Papamakarios and Iain Murray. Fast \(\varepsilon\)-free inference of simulation models with Bayesian conditional density estimation. In _Advances in neural information processing systems_, 2016. arXiv:1605.06376.
* Lueckmann et al. [2017] Jan-Matthis Lueckmann, Pedro J Goncalves, Giacomo Bassetto, Kaan Ocal, Marcel Nonnenmacher, and Jakob H Macke. Flexible statistical inference for mechanistic models of neural dynamics. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, pages 1289-1299, 2017.
* Greenberg et al. [2019] David Greenberg, Marcel Nonnenmacher, and Jakob Macke. Automatic posterior transformation for likelihood-free inference. In _International Conference on Machine Learning_, pages 2404-2414. PMLR, 2019.
* Dax et al. [2021] Maximilian Dax, Stephen R. Green, Jonathan Gair, Jakob H. Macke, Alessandra Buonanno, and Bernhard Scholkopf. Real-Time Gravitational Wave Science with Neural Posterior Estimation. _Phys. Rev. Lett._, 127(24):241103, 2021. arXiv:2106.12594, doi:10.1103/PhysRevLett.127.241103.
* Sharrock et al. [2022] Louis Sharrock, Jack Simons, Song Liu, and Mark Beaumont. Sequential neural score estimation: Likelihood-free inference with conditional score based diffusion models. _arXiv preprint arXiv:2210.04872_, 2022.
* Geffner et al. [2022] Tomas Geffner, George Papamakarios, and Andriy Mnih. Score modeling for simulation-based inference. _arXiv preprint arXiv:2209.14249_, 2022.
* Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* Song and Ermon [2019] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 8780-8794. Curran Associates, Inc., 2021. URL: https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf.
* Ho et al. [2022] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _J. Mach. Learn. Res._, 23:47:1-47:33, 2022. URL: http://jmlr.org/papers/v23/21-0635.html.
* Song et al. [2020] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* Lipman et al. [2022] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. _CoRR_, abs/2210.02747, 2022. arXiv:2210.02747, doi:10.48550/arXiv.2210.02747.
* Hermans et al. [2021] Joeri Hermans, Arnaud Delaunoy, Francois Rozet, Antoine Wehenkel, and Gilles Louppe. Averting a crisis in simulation-based inference. _arXiv preprint arXiv:2110.06581_, 2021.
* Chen et al. [2018] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 6572-6583, 2018. URL: https://proceedings.neurips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html.

* [19] Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. _arXiv preprint arXiv:2209.15571_, 2022.
* [20] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. _arXiv preprint arXiv:2209.03003_, 2022.
* [21] Kirill Neklyudov, Daniel Severo, and Alireza Makhzani. Action matching: A variational method for learning stochastic dynamics from samples. _arXiv preprint arXiv:2210.06662_, 2022.
* [22] Aram Davtyan, Sepehr Sameni, and Paolo Favaro. Randomized conditional flow matching for video prediction. _arXiv preprint arXiv:2211.14575_, 2022.
* [23] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Conditional flow matching: Simulation-free dynamic optimal transport. _arXiv preprint arXiv:2302.00482_, 2023.
* [24] Scott A Sisson, Yanan Fan, and Mark A Beaumont. Overview of abc. In _Handbook of approximate Bayesian computation_, pages 3-54. Chapman and Hall/CRC, 2018.
* [25] Mark A Beaumont, Wenyang Zhang, and David J Balding. Approximate bayesian computation in population genetics. _Genetics_, 162(4):2025-2035, 2002.
* [26] Mark A Beaumont, Jean-Marie Cornuet, Jean-Michel Marin, and Christian P Robert. Adaptive approximate bayesian computation. _Biometrika_, 96(4):983-990, 2009.
* [27] Michael G. B. Blum and Olivier Francois. Non-linear regression models for approximate bayesian computation. _Stat. Comput._, 20(1):63-73, 2010. doi:10.1007/s11222-009-9116-0.
* [28] Dennis Prangle, Paul Fearnhead, Murray P. Cox, Patrick J. Biggs, and Nigel P. French. Semi-automatic selection of summary statistics for abc model choice, 2013. arXiv:1302.5624.
* [29] Simon Wood. Statistical inference for noisy nonlinear ecological dynamic systems. _Nature_, 466:1102-4, 08 2010. doi:10.1038/nature09319.
* [30] Christopher C Drovandi, Clara Grazian, Kerrie Mengersen, and Christian Robert. Approximating the likelihood in approximate bayesian computation, 2018. arXiv:1803.06645.
* [31] George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 837-848. PMLR, 2019.
* [32] Jan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, and Jakob H Macke. Likelihood-free inference with emulator networks. In _Symposium on Advances in Approximate Bayesian Inference_, pages 32-53. PMLR, 2019.
* [33] Rafael Izbicki, Ann Lee, and Chad Schafer. High-dimensional density ratio estimation with extensions to approximate likelihood computation. In _Artificial intelligence and statistics_, pages 420-429. PMLR, 2014.
* [34] Kim Pham, David Nott, and Sanjay Chaudhuri. A note on approximating abc-mcmc using flexible classifiers. _Stat_, 3, 03 2014. doi:10.1002/sta4.56.
* [35] Kyle Cranmer, Juan Pavez, and Gilles Louppe. Approximating likelihood ratios with calibrated discriminative classifiers. _arXiv preprint arXiv:1506.02169_, 2015.
* [36] Joeri Hermans, Volodymir Begy, and Gilles Louppe. Likelihood-free mcmc with approximate likelihood ratios. _arXiv preprint arXiv:1903.04057_, 10, 2019.
* [37] Conor Durkan, Iain Murray, and George Papamakarios. On contrastive learning for likelihood-free inference. In _International conference on machine learning_, pages 2771-2781. PMLR, 2020.
* [38] Owen Thomas, Ritabrata Dutta, Jukka Corander, Samuel Kaski, and Michael U. Gutmann. Likelihood-free inference by ratio estimation, 2020. arXiv:1611.10242.

* [39] Benjamin K Miller, Christoph Weniger, and Patrick Forre. Contrastive neural ratio estimation. _Advances in Neural Information Processing Systems_, 35:3262-3278, 2022.
* [40] Thomas Muller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan Novak. Neural importance sampling. _ACM Transactions on Graphics (TOG)_, 38(5):1-19, 2019.
* [41] Maximilian Dax, Stephen R. Green, Jonathan Gair, Michael Purrer, Jonas Wildberger, Jakob H. Macke, Alessandra Buonanno, and Bernhard Scholkopf. Neural Importance Sampling for Rapid and Reliable Gravitational-Wave Inference. _Phys. Rev. Lett._, 130(17):171403, 2023. arXiv:2210.05686, doi:10.1103/PhysRevLett.130.171403.
* [42] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023.
* [43] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. _Advances in Neural Information Processing Systems_, 34:1415-1428, 2021.
* [44] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.
* [45] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In _International conference on machine learning_, pages 933-941. PMLR, 2017.
* [46] Jan-Matthis Lueckmann, Jan Boelts, David Greenberg, Pedro Goncalves, and Jakob Macke. Benchmarking simulation-based inference. In _International Conference on Artificial Intelligence and Statistics_, pages 343-351. PMLR, 2021.
* [47] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. arXiv:1512.03385.
* [48] Jerome H Friedman. On multivariate goodness-of-fit and two-sample testing. _Statistical Problems in Particle Physics, Astrophysics, and Cosmology_, 1:311, 2003.
* [49] David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. _arXiv preprint arXiv:1610.06545_, 2016.
* [50] Arnaud Delaunoy, Joeri Hermans, Francois Rozet, Antoine Wehenkel, and Gilles Louppe. Towards reliable simulation-based inference with balanced neural ratio estimation, 2022. arXiv:2208.13624.
* [51] Arnaud Delaunoy, Benjamin Kurt Miller, Patrick Forre, Christoph Weniger, and Gilles Louppe. Balancing simulation-based inference for conservative posteriors. _arXiv preprint arXiv:2304.10978_, 2023.
* [52] R. Abbott et al. GWTC-3: Compact Binary Coalescences Observed by LIGO and Virgo During the Second Part of the Third Observing Run. _arXiv preprint arXiv:2111.03606_, 11 2021. arXiv:2111.03606.
* [53] R. Abbott et al. Population Properties of Compact Objects from the Second LIGO-Virgo Gravitational-Wave Transient Catalog. _Astrophys. J. Lett._, 913(1):L7, 2021. arXiv:2010.14533, doi:10.3847/2041-8213/abe949.
* [54] B. P. Abbott et al. GW170817: Measurements of neutron star radii and equation of state. _Phys. Rev. Lett._, 121(16):161101, 2018. arXiv:1805.11581, doi:10.1103/PhysRevLett.121.161101.
* [55] R. Abbott et al. Tests of general relativity with binary black holes from the second LIGO-Virgo gravitational-wave transient catalog. _Phys. Rev. D_, 103(12):122002, 2021. arXiv:2010.14529, doi:10.1103/PhysRevD.103.122002.

* Abbott et al. [2017] B. P. Abbott et al. A gravitational-wave standard siren measurement of the Hubble constant. _Nature_, 551(7678):85-88, 2017. arXiv:1710.05835, doi:10.1038/nature24471.
* Veitch et al. [2015] J. Veitch, V. Raymond, B. Farr, W Farr, P. Graff, S. Vitale, et al. Parameter estimation for compact binaries with ground-based gravitational-wave observations using the LALInference software library. _Phys. Rev._, D91(4):042003, 2015. arXiv:1409.7215, doi:10.1103/PhysRevD.91.042003.
* Ashton et al. [2019] Gregory Ashton et al. BILBY: A user-friendly Bayesian inference library for gravitational-wave astronomy. _Astrophys. J. Suppl._, 241(2):27, 2019. arXiv:1811.02042, doi:10.3847/1538-4365/ab06fc.
* Romero-Shaw et al. [2020] I. M. Romero-Shaw et al. Bayesian inference for compact binary coalescences with bilby: validation and application to the first LIGO-Virgo gravitational-wave transient catalogue. _Mon. Not. Roy. Astron. Soc._, 499(3):3295-3319, 2020. arXiv:2006.00714, doi:10.1093/mnras/staa2850.
* Speagle [2020] Joshua S Speagle. dynesty: a dynamic nested sampling package for estimating Bayesian posteriors and evidences. _Monthly Notices of the Royal Astronomical Society_, 493(3):3132-3158, Feb 2020. URL: http://dx.doi.org/10.1093/mnras/staa278, arXiv:1904.02180, doi:10.1093/mnras/staa278.
* Metropolis et al. [1953] Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. Equation of state calculations by fast computing machines. _The journal of chemical physics_, 21(6):1087-1092, 1953.
* Hastings [1970] W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. _Biometrika_, 57(1):97-109, 04 1970. arXiv:https://academic.oup.com/biomet/article-pdf/57/1/97/23940249/57-1-97.pdf, doi:10.1093/biomet/57.1.97.
* 859, 2006. doi:10.1214/06-BA127.
* Cuoco et al. [2020] Elena Cuoco, Jade Powell, Marco Cavaglia, Kendall Ackley, Michal Bejger, Chayan Chatterjee, Michael Coughlin, Scott Coughlin, Paul Easter, Reed Essick, et al. Enhancing gravitational-wave science with machine learning. _Machine Learning: Science and Technology_, 2(1):011002, 5 2020. arXiv:2005.03745, doi:10.1088/2632-2153/abb93a.
* Gabbard et al. [2022] Hunter Gabbard, Chris Messenger, Ik Siong Heng, Francesco Tonolini, and Roderick Murray-Smith. Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy. _Nature Phys._, 18(1):112-117, 2022. arXiv:1909.06296, doi:10.1038/s41567-021-01425-7.
* Green et al. [2020] Stephen R. Green, Christine Simpson, and Jonathan Gair. Gravitational-wave parameter estimation with autoregressive neural network flows. _Phys. Rev. D_, 102(10):104057, 2020. arXiv:2002.07656, doi:10.1103/PhysRevD.102.104057.
* Delaunoy et al. [2020] Arnaud Delaunoy, Antoine Wehenkel, Tanja Hinderer, Samaya Nissanke, Christoph Weniger, Andrew R. Williamson, and Gilles Louppe. Lightning-Fast Gravitational Wave Parameter Inference through Neural Amortization. In _Third Workshop on Machine Learning and the Physical Sciences_, 10 2020. arXiv:2010.12931.
* Green and Gair [2021] Stephen R. Green and Jonathan Gair. Complete parameter inference for GW150914 using deep learning. _Mach. Learn. Sci. Tech._, 2(3):03LT01, 2021. arXiv:2008.03312, doi:10.1088/2632-2153/abfaed.
* Dax et al. [2022] Maximilian Dax, Stephen R. Green, Jonathan Gair, Michael Deistler, Bernhard Scholkopf, and Jakob H. Macke. Group equivariant neural posterior estimation. In _International Conference on Learning Representations_, 11 2022. arXiv:2111.13139.
* Chatterjee et al. [2022] Chayan Chatterjee, Linqing Wen, Damon Beveridge, Foilova Diakogiannis, and Kevin Vinsen. Rapid localization of gravitational wave sources from compact binary coalescences using deep learning. _arXiv preprint arXiv:2207.14522_, 7 2022. arXiv:2207.14522.

* [71] Stefan T. Radev, Ulf K. Mertens, Andreass Voss, Lynton Ardizzone, and Ullrich Kothe. Bayesflow: Learning complex stochastic models with invertible neural networks, 2020. arXiv:2003.06281.
* [72] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. In _Advances in Neural Information Processing Systems_, pages 7509-7520, 2019. arXiv:1906.04032.
* [73] B.P. Abbott et al. Observation of Gravitational Waves from a Binary Black Hole Merger. _Phys. Rev. Lett._, 116(6):061102, 2016. arXiv:1602.03837, doi:10.1103/PhysRevLett.116.061102.
* [74] Jonas Wildberger, Maximilian Dax, Stephen R. Green, Jonathan Gair, Michael Purrer, Jakob H. Macke, Alessandra Buonanno, and Bernhard Scholkopf. Adapting to noise distribution shifts in flow-based gravitational-wave inference. _Phys. Rev. D_, 107(8):084046, 2023. arXiv:2211.08801, doi:10.1103/PhysRevD.107.084046.
* [75] Taco Cohen and Max Welling. Group equivariant convolutional networks. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016_, volume 48 of _JMLR Workshop and Conference Proceedings_, pages 2990-2999. JMLR.org, 2016. URL: http://proceedings.mlr.press/v48/cohenc16.html.
* [76] Mark Hannam, Patricia Schmidt, Alejandro Bohe, Leila Haegel, Sascha Husa, Frank Ohme, Geraint Pratten, and Michael Purrer. Simple model of complete precessing black-hole-binary gravitational waveforms. _Phys. Rev. Lett._, 113:151101, Oct 2014. URL: https://link.aps.org/doi/10.1103/PhysRevLett.113.151101, doi:10.1103/PhysRevLett.113.151101.
* [77] Sebastian Khan, Sascha Husa, Mark Hannam, Frank Ohme, Michael Purrer, Xisco Jimenez Forteza, and Alejandro Bohe. Frequency-domain gravitational waves from nonprecessing black-hole binaries. II. A phenomenological model for the advanced detector era. _Phys. Rev._, D93(4):044007, 2016. arXiv:1508.07253, doi:10.1103/PhysRevD.93.044007.
* technical notes for the LAL implementation. _LIGO Technical Document, LIGO-T1500602-v4_, 2016. URL: https://dcc.ligo.org/LIGO-T1500602/public.
* [79] Benjamin Farr, Evan Ochsner, Will M. Farr, and Richard O'Shaughnessy. A more effective coordinate system for parameter estimation of precessing compact binaries from gravitational waves. _Phys. Rev. D_, 90(2):024018, 2014. arXiv:1404.7070, doi:10.1103/PhysRevD.90.024018.
* [80] J.R. Dormand and P.J. Prince. A family of embedded runge-kutta formulae. _Journal of Computational and Applied Mathematics_, 6(1):19-26, 1980. URL: https://www.sciencedirect.com/science/article/pii/0771050X80900133, doi:https://doi.org/10.1016/0771-050X(80)90013-3.

Gaussian flow

We here derive the form of a vector field \(v_{t}(\theta)\) that restricts the resulting continuous flow to a one dimensional Gaussian with mean \(\hat{\mu}\) variance \(\hat{\sigma}^{2}\). With the optimal transport path \(\mu_{t}(\theta)=t\theta_{1}\), \(\sigma_{t}(\theta)=1-(1-\sigma_{\text{min}})t\equiv\sigma_{t}\) from [16], the sample-conditional probability path (5) reads

\[p_{t}(\theta|\theta_{1})=\mathcal{N}[t\theta_{1},\sigma_{t}^{2}](\theta).\] (10)

We set our target distribution

\[q_{1}(\theta_{1})=\mathcal{N}[\hat{\mu},\hat{\sigma}^{2}](\theta_{1}).\] (11)

To derive the marginal probability path and the marginal vector field we need two identities for the convolution \(*\) of Gaussian densities. Recall that the convolution of two function is defined by \(f*g(x)=\int f(x-y)g(y)\,\mathrm{d}y\). We define the function

\[g_{\mu,\sigma^{2}}(\theta)=\theta\cdot\mathcal{N}\left[\mu,\sigma^{2}\right]( \theta).\] (12)

Then the following holds

\[\mathcal{N}[\mu_{1},\sigma_{1}^{2}]*\mathcal{N}[\mu_{2},\sigma_{2 }^{2}] =\mathcal{N}[\mu_{1}+\mu_{2},\sigma_{1}^{2}+\sigma_{2}^{2}]\] (13) \[g_{0,\sigma_{1}^{2}}*\mathcal{N}[\mu_{2},\sigma_{2}^{2}] =\frac{\sigma_{1}^{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}}\left(g_{ \mu_{2},\sigma_{1}^{2}+\sigma_{2}^{2}}-\mu_{2}\,\mathcal{N}[\mu_{2},\sigma_{1 }^{2}+\sigma_{2}^{2}]\right)\] (14)

#### Marginal probability paths

Marginalizing over \(\theta_{1}\) in (10) with (11), we find

\[p_{t}(\theta) =\int p_{t}(\theta|\theta_{1})q(\theta_{1})\,d\theta_{1}\] \[=\int\mathcal{N}\left[t\theta_{1},\sigma_{t}^{2}\right](\theta) \,\mathcal{N}\left[\hat{\mu},\hat{\sigma}^{2}\right](\theta_{1})d\theta_{1}\] \[=\int\mathcal{N}\left[0,\sigma_{t}^{2}\right](\theta-t\theta_{1} )\,\,\mathcal{N}(t\hat{\mu},(t\hat{\sigma})^{2})(t\theta_{1})\cdot t\,d\theta _{1}\] (15) \[=\int\mathcal{N}\left[0,\sigma_{t}^{2}\right](\theta-\theta_{1}^ {t})\,\,\mathcal{N}\left[t\hat{\mu},(t\hat{\sigma})^{2}\right](\theta_{1}^{ t})\,d\theta_{1}^{t}\] \[=\mathcal{N}\left[t\hat{\mu},\sigma_{t}^{2}+(t\hat{\sigma})^{2} \right](\theta)\]

where we defined \(\theta_{1}^{t}=t\theta_{1}\) and used (13).

#### Marginal vector field

We now calculate the marginalized vector field \(u_{t}(\theta)\) based on equation (8) in [16]. Using the sample-conditional vector field (6) and the distributions (10) and (11) we find

\[u_{t}(\theta) =\int u_{t}(\theta|\theta_{1})\frac{p_{t}(\theta|\theta_{1})q( \theta_{1})}{p_{t}(\theta)}\,d\theta_{1}\] \[=\frac{1}{p_{t}(\theta)}\int\frac{(\theta_{1}-(1-\sigma_{\text{ min}})\theta)}{\sigma_{t}}\cdot\mathcal{N}\left[t\theta_{1},\sigma_{t}^{2} \right](\theta)\cdot\mathcal{N}\left[\hat{\mu},\hat{\sigma}^{2}\right]( \theta_{1})\,d\theta_{1}\] \[=\frac{1}{p_{t}(\theta)}\int\frac{(\theta_{1}-(1-\sigma_{\text{ min}})\theta)}{\sigma_{t}}\cdot\mathcal{N}\left[0,\sigma_{t}^{2}\right]( \theta-t\theta_{1})\cdot\mathcal{N}\left[t\hat{\mu},(t\hat{\sigma})^{2} \right](t\theta_{1})\cdot t\,d\theta_{1}\] \[=\frac{1}{p_{t}(\theta)}\int\frac{(\theta_{1}^{\prime\prime}-(1- \sigma_{\text{min}})t\cdot\theta)}{\sigma_{t}\cdot t}\cdot\mathcal{N}\left[0, \sigma_{t}^{2}\right](\theta-\theta_{1}^{\prime})\cdot\mathcal{N}\left[t\hat {\mu},(t\hat{\sigma})^{2}\right](\theta_{1}^{\prime})\cdot d\theta_{1}^{\prime}\] \[=\frac{1}{p_{t}(\theta)}\int\frac{(-\theta_{1}^{\prime\prime}+(1 -(1-\sigma_{\text{min}})t)\cdot\theta)}{\sigma_{t}\cdot t}\cdot\mathcal{N} \left[0,\sigma_{t}^{2}\right](\theta_{1}^{\prime\prime})\cdot\mathcal{N}\left[ t\hat{\mu},(t\hat{\sigma})^{2}\right](\theta-\theta_{1}^{\prime\prime})\cdot d \theta_{1}^{\prime\prime}\] \[=\frac{1}{p_{t}(\theta)}\int\frac{(-\theta_{1}^{\prime\prime}+ \sigma_{t}\cdot\theta)}{\sigma_{t}\cdot t}\cdot\mathcal{N}\left[0,\sigma_{t}^ {2}\right](\theta_{1}^{\prime\prime})\cdot\mathcal{N}\left[t\hat{\mu},(t\hat {\sigma})^{2}\right](\theta-\theta_{1}^{\prime\prime})\cdot d\theta_{1}^{\prime\prime}\] (16)where we used the change of variables \(\theta_{1}^{\prime}=t\theta_{1}\) and \(\theta_{1}^{\prime\prime}=\theta-\theta_{1}^{\prime}\). Now we evaluate this expression using (12), then the identities (13) and (14) and the marginal probability (15)

\[u_{t}(\theta) =\frac{-1}{p_{t}(\theta)\cdot\sigma_{t}\cdot t}\left(g_{0,\sigma_ {t}^{2}}*\mathcal{N}\left[t\hat{\mu},(t\hat{\sigma})^{2}\right]\right)(\theta) +\frac{\theta}{p_{t}(\theta)\cdot t}\left(\mathcal{N}\left[0,\sigma_{t}^{2} \right]*\mathcal{N}\left[t\hat{\mu},(t\hat{\sigma})^{2}\right]\right)(\theta)\] \[=\frac{-1}{p_{t}(\theta)\cdot\sigma_{t}\cdot t}\frac{(\theta-t \hat{\mu})\cdot\sigma_{t}^{2}}{\sigma_{t}^{2}+(t\hat{\sigma})^{2}}\cdot \mathcal{N}\left[t\hat{\mu},(\sigma_{t}^{2}+(t\hat{\sigma})^{2})\right]( \theta)+\frac{\theta}{p_{t}(\theta)\cdot t}\mathcal{N}\left[t\hat{\mu},( \sigma_{t}^{2}+(t\hat{\sigma})^{2})\right](\theta)\] \[=\frac{(\sigma_{t}^{2}+(t\hat{\sigma})^{2})\theta-(\theta-t\hat{ \mu})\cdot\sigma_{t}}{p_{t}(\theta)\cdot t\cdot(\sigma_{t}^{2}+(t\hat{\sigma })^{2})}\cdot p_{t}(\theta)\] \[=\frac{(\sigma_{t}^{2}+(t\hat{\sigma})^{2}-\sigma_{t})\theta+t \hat{\mu}\cdot\sigma_{t}}{t\cdot(\sigma_{t}^{2}+(t\hat{\sigma})^{2})}.\] (17)

By choosing a vector field \(v_{t}\) of the form (17) with learnable parameters \(\hat{\mu},\hat{\sigma}^{2}\), we can thus define a continuous flow that is restricted to a one dimensional Gaussian.

## Appendix B Mass covering properties of flows

In this supplement, we investigate the mass covering properties of continuous normalizing flows trained using mean squared error and in particular prove Theorem 1. We first recall the notation from the main part. We always assume that the data is distributed according to \(p_{1}(\theta)\). In addition, there is a known and simple base distribution \(p_{0}\) and we assume that there is a vector field \(u_{t}:[0,1]\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) that connects \(p_{0}\) and \(p_{1}\) in the following sense. We denote by \(\phi_{t}\) the flow generated by \(u_{t}\), i.e., \(\phi_{t}\) satisfies

\[\partial_{t}\phi_{t}(\theta)=u_{t}(\phi_{t}(\theta)).\] (18)

Then we assume that \((\phi_{1})_{*}p_{0}=p_{1}\) and we also define the interpolations \(p_{t}=(\phi_{t})_{*}p_{0}\).

We do not have access to the ground truth distributions \(p_{t}\) and the vector field \(u_{t}\) but we try to learn a vector field \(v_{t}\) approximating \(u_{t}\). We denote its flow by \(\psi_{t}\) and we define \(q_{t}=(\psi_{t})_{*}q_{0}\) and \(q_{0}=p_{0}\). We are interested in the mass covering properties of the learned approximation \(q_{1}\) of \(p_{1}\). In particular, we want to relate the KL-divergence \(\mathrm{D_{KL}}(p_{1}||q_{1})\) to the mean squared error,

\[\mathrm{MSE}_{p}(u,v)=\int_{0}^{1}\mathrm{d}t\,\int p_{t}(\mathrm{d}\theta)(u _{t}(\theta)-v_{t}(\theta))^{2},\] (19)

of the generating vector fields. The first observation is that without any regularity assumptions on \(v_{t}\) it is impossible to obtain any bound on the KL-divergence in terms of the mean squared error.

**Lemma 1**.: _For every \(\varepsilon>0\) there are vector field \(u_{t}\) and \(v_{t}\) and a base distribution \(p_{0}=q_{0}\) such that_

\[\mathrm{MSE}_{p}(u,v)<\varepsilon\text{ and }\mathrm{D_{KL}}(p_{1}||q_{1})=\infty.\] (20)

_In addition we can construct \(u_{t}\) and \(v_{t}\) such that the support of \(p_{1}\) is larger than the support of \(q_{1}\)._

Proof.: We consider the uniform distribution \(p_{0}=q_{0}\sim\mathcal{U}([-1,1])\) and the vector fields

\[u_{t}(\theta)=0\] (21)

and

\[v_{t}(\theta)=\begin{cases}\varepsilon&\text{for }0\leq\theta< \varepsilon,\\ 0&\text{otherwise}.\end{cases}\] (22)

As before, let \(\phi_{t}\) denote the flow of the vector field \(u_{t}\) and similarly \(\psi_{t}\) denote the flow of \(v_{t}\). Clearly \(\phi_{t}(\theta)=\theta\). On the other hand

\[\psi_{t}(\theta)=\begin{cases}\min(\theta+\varepsilon t,\varepsilon)&\text{ if }0\leq\theta<\varepsilon,\\ \theta&\text{otherwise}.\end{cases}\] (23)In particular

\[\psi_{1}(\theta)=\begin{cases}\varepsilon&\text{if $0\leq\theta<\varepsilon$,}\\ \theta&\text{otherwise.}\end{cases}\] (24)

This implies that \(p_{1}=(\phi_{1})_{*}p_{0}\sim\mathcal{U}([-1,1])\). On the other hand \(q_{1}=(\psi_{1})_{*}q_{0}\) has support in \([-1,0]\cup[\varepsilon,1]\). In particular, the distribution of \(q_{1}\) is not mass covering with respect to \(p_{1}\) and \(\mathrm{D}_{\mathrm{KL}}(p_{1}||q_{1})=\infty\). Finally, we observe that the MSE can be arbitrarily small

\[\mathrm{MSE}_{p}(u,v)=\int_{0}^{1}\mathrm{d}t\int p_{t}(\mathrm{d}\theta)|u_{t }(\theta)-v_{t}(\theta)|^{2}=\int_{0}^{1}\int_{0}^{\varepsilon}\frac{1}{2} \varepsilon^{2}=\frac{\varepsilon^{3}}{2}.\] (25)

Here we used that the density of \(p_{t}(\mathrm{d}\theta)\) is \(1/2\) for \(-1\leq\theta\leq 1\). 

We see that an arbitrary small MSE-loss cannot ensure that the probability distribution is mass covering and the KL-divergence is finite. On a high level this can be explained by the fact that for vector fields \(v_{t}\) that are not Lipschitz continuous the flow is not necessarily continuous, and we can generate holes in the distribution. Note that we chose \(p_{0}\) to be a uniform distribution for simplicity, but the result extends to any smooth distribution, in particular the result does not rely on the discontinuity of \(p_{0}\).

Next, we investigate the mass covering property for Lipschitz continuous flows. When the flows \(u_{t}\) and \(v_{t}\) are Lipschitz continuous (in \(\theta\)) this ensures that the flows \(\psi_{1}\) and \(\phi_{1}\) are continuous in \(x\) and it is not possible to create holes in the distribution as shown above for non-continuous vector fields. We show a weaker bound in this setting.

**Lemma 2**.: _For every \(0\leq\delta\leq 1\) there is a base distribution \(p_{0}=q_{0}\) and the are Lipschitz-continuous vector fields \(u_{t}\) and \(v_{t}\) such that \(\mathrm{MSE}_{p}(u,v)=\delta\) and_

\[\mathrm{D}_{\mathrm{KL}}(p_{1}||q_{1})\geq\frac{1}{2}\,\mathrm{MSE}_{p}(u,v)^ {1/3}.\] (26)

Proof.: We consider \(p_{0}\), \(q_{0}\) and \(u_{t}\) as in Lemma 1, and we define

\[v_{t}(\theta)=\begin{cases}2\theta&\text{for $0\leq\theta<\varepsilon$,}\\ 2\varepsilon-\theta&\text{for $\varepsilon\leq\theta<2\varepsilon$,}\\ 0&\text{otherwise.}\end{cases}\] (27)

Then we can calculate for \(0\leq\theta\leq e^{-2}\varepsilon\) that

\[\psi_{t}(\theta)=\theta e^{2t}.\] (28)

Similarly we obtain for \(\varepsilon\leq\theta\leq 2\varepsilon\) (solving the ODE \(f^{\prime}=2f\))

\[\psi_{t}(\theta)=2\varepsilon-(2\varepsilon-\theta)e^{-2t}.\] (29)

We find

\[\psi_{1}(0)=0,\;\psi_{1}(e^{-2}\varepsilon)=\varepsilon,\;\psi_{1}( \varepsilon)=2-\varepsilon e^{-2};\;\psi_{2}(2\varepsilon)=2\varepsilon.\] (30)

Next we find for the densities of \(q_{1}\) that

\[q_{1}(\psi_{1}(\theta))=q_{0}(\theta)|\psi_{1}^{\prime}(\theta)|^{-1}=\frac{1 }{2}\begin{cases}e^{-2}&\text{for $0\leq\theta\leq e^{-2}\varepsilon$,}\\ e^{2}&\text{for $\varepsilon\leq\theta\leq 2\varepsilon$.}\end{cases}\] (31)

Together with (30) this implies that the density of \(q_{1}\) is given by

\[q_{1}(\theta)=\frac{1}{2}\begin{cases}e^{-2}&\text{for $0\leq\theta\leq \varepsilon$,}\\ e^{2}&\text{for $2\varepsilon-\varepsilon e^{-2}\leq\theta\leq 2\varepsilon$.}\end{cases}\] (32)

Note that \(p_{1}(\theta)=1/2\) for \(-1\leq\theta\leq 1\) and therefore

\[\int_{0}^{\varepsilon}\ln\frac{p_{1}(\theta)}{q_{1}(\theta)}p_{1}(\mathrm{d} \theta)=\int_{0}^{\varepsilon}\ln(e^{2})\frac{1}{2}\mathrm{d}\theta=\varepsilon,\] (33)\[\int_{2\varepsilon-\varepsilon e^{-2}}^{2\varepsilon}\ln\frac{p_{1}(\theta)}{q_{1}( \theta)}p_{1}(\mathrm{d}\theta)=\int_{2\varepsilon-\varepsilon e^{-2}}^{2 \varepsilon}\ln(e^{-2})\frac{1}{2}\mathrm{d}\theta=-\varepsilon e^{-2}.\] (34)

Moreover we note

\[\int_{\varepsilon}^{2\varepsilon-\varepsilon e^{-2}}q_{1}(\mathrm{d} \varepsilon)=\int_{e^{-2}\varepsilon}^{\varepsilon}q_{0}(\mathrm{d} \varepsilon)=\frac{1}{2}\varepsilon(1-e^{-2})=\int_{\varepsilon}^{2 \varepsilon-\varepsilon e^{-2}}p_{1}(\mathrm{d}\varepsilon),\] (35)

which implies (by positivity of the KL-divergence) that

\[\int_{\varepsilon}^{2\varepsilon-\varepsilon e^{-2}}\ln\left(\frac{p_{1}( \theta)}{q_{1}(\theta)}\right)p_{1}(\mathrm{d}\theta)\geq 0.\] (36)

We infer using also \(p_{1}(\theta)=q_{1}(\theta)=1/2\) for \(\theta\in[-1,0]\cap[2\varepsilon,1]\) that

\[\mathrm{D}_{\mathrm{KL}}(p_{1}||q_{1})=\int\ln\left(\frac{p_{1}(\theta)}{q_{1 }(\theta)}\right)p_{1}(\mathrm{d}\theta)\geq\varepsilon(1-e^{-2}).\] (37)

On the other hand we can bound

\[\int_{0}^{1}\mathrm{d}t\int p_{t}(\mathrm{d}\theta)|v_{t}(\theta)-u_{t}( \theta)|^{2}=\frac{1}{2}\int_{0}^{1}\mathrm{d}t\int_{0}^{2\varepsilon}|u_{t}( \theta)|^{2}=\int_{0}^{\varepsilon}s^{2}\,\mathrm{d}s=\frac{\varepsilon^{3}}{ 3}.\] (38)

We conclude that

\[\mathrm{D}_{\mathrm{KL}}(p_{1}||q_{1})\geq\frac{1}{2}\left(\mathrm{MSE}_{p}(u, v)\right)^{1/3}.\] (39)

In particular, it is not possible to bound the KL-divergence by the MSE even when the vector fields are Lipschitz continuous. 

Let us put this into context. It was already shown in [42] that we can, in general, not bound the forward KL-divergence by the mean squared error and our Lemmas 1 and 2 are concrete examples. On the other hand, when considering SDEs the KL-divergence can be bounded by the mean squared error of the drift terms as shown in [43]. Indeed, in [42] the favorable smoothing effect was carefully investigated.

Here we show that we can alternatively obtain an upper bound on the KL-divergence when assuming that \(u_{t}\), \(v_{t}\), and \(p_{0}\) satisfy additional regularity assumptions. This allows us to recover the mass covering property from bounds on the means squared error for sufficiently smooth vector fields. The scaling is nevertheless still weaker than for SDEs.

We now state our assumptions. We denote the gradient with respect to \(\theta\) by \(\nabla=\nabla_{\mu}\) and second derivatives by \(\nabla^{2}=\nabla^{2}_{\mu\nu}\). When applying the chain rule, we leave the indices implicit. We denote by

\(|\cdot|\) the Frobenius norm \(|A|=\left(\sum_{ij}A_{ij}^{2}\right)^{1/2}\) of a matrix. The Frobenius norm is submultiplicative, i.e., \(|AB|\leq|A|\cdot|B|\) and directly generalizes to higher order tensors.

_Assumption 1_.: We assume that

\[|\nabla u_{t}|\leq L,\ |\nabla v_{t}|\leq L,\ |\nabla^{2}u_{t}|\leq L^{\prime}, \ |\nabla^{2}v_{t}|\leq L^{\prime}.\] (40)

We require one further assumption on \(p_{0}\).

_Assumption 2_.: There is a constant \(C_{1}\) such that

\[|\nabla\ln p_{0}(\theta)|\leq C_{1}(1+|\theta|).\] (41)

We also assume that

\[\mathbb{E}_{p_{0}}\,|\theta|^{2}<C_{2}<\infty.\] (42)

Note that (41) holds, e.g., if \(p_{0}\) follows a Gaussian distribution but also for smooth distribution with slower decay at \(\infty\). If we assume that \(|\nabla\ln p_{0}(\theta)|\) is bounded the proof below simplifies slightly. This is, e.g., the case if \(p_{0}(\theta)\sim e^{-|\theta|}\) as \(|\theta|\to\infty\).

We need some additional notation. It is convenient to introduce \(\phi_{t}^{s}=\phi_{t}\circ(\phi_{s})^{-1}\), i.e., the flow from time \(s\) to \(t\) (in particular \(\phi_{t}^{0}=\phi_{t}\)) and similarly for \(\psi\). We can now restate and prove Theorem 1.

**Theorem 2**.: _Let \(p_{0}=q_{0}\) and assume \(u_{t}\) and \(v_{t}\) are two vector fields whose flows satisfy \(p_{1}=(\phi_{1})_{*}p_{0}\) and \(q_{1}=(\psi_{1})_{*}q_{0}\). Assume that \(p_{0}\) satisfies Assumption 2 and \(u_{t}\) and \(v_{t}\) satisfy Assumption 1. Then there is a constant \(C>0\) depending on \(L\), \(L^{\prime}\), \(C_{1}\), \(C_{2}\), and \(d\) such that (for \(\operatorname{MSE}_{p}(u,v)<1\))_

\[\operatorname{D}_{\mathrm{KL}}(p_{1}||q_{1})\leq C\operatorname{MSE}_{p}(u,v )^{\frac{1}{2}}.\] (43)

_Remark 1_.: We do not claim that our results are optimal, it might be possible to find similar bounds for the forward KL-divergence with weaker assumptions. However, we emphasize that Lemma 2 shows that the result of the theorem is not true without the assumption on the second derivative of \(v_{t}\) and \(u_{t}\).

Proof.: We want to control \(\operatorname{D}_{\mathrm{KL}}(p_{1}||q_{1})\). It can be shown that (see equation above (25) in [43] or Lemma 2.19 in [42] )

\[\partial_{t}\operatorname{D}_{\mathrm{KL}}(p_{t}||q_{t})=-\int p_{t}(\mathrm{ d}\theta)(u_{t}(\theta)-v_{t}(\theta))\cdot(\nabla\ln p_{t}(\theta)-\nabla\ln q _{t}(\theta)).\] (44)

Using Cauchy-Schwarz we can bound this by

\[\partial_{t}\operatorname{D}_{\mathrm{KL}}(p_{t}||q_{t})\leq\left(\int p_{t} (\mathrm{d}\theta)|u_{t}(\theta)-v_{t}(\theta)|^{2}\right)^{\frac{1}{2}} \left(\int p_{t}(\mathrm{d}\theta)|\nabla\ln p_{t}(\theta)-\nabla\ln q_{t}( \theta)|^{2}\right)^{\frac{1}{2}}.\] (45)

We use the relation (see (3))

\[\ln(p_{t}(\phi_{t}(\theta_{0}))=\ln(p_{0}(\theta_{0}))-\int_{0}^{t}( \operatorname{div}u_{s})(\phi_{s}(\theta_{0}))\mathrm{d}s,\] (46)

which can be equivalently rewritten (setting \(\theta=\phi_{t}\theta_{0}\)) as

\[\ln(p_{t}(\theta))=\ln(p_{0}(\phi_{0}^{t}\theta))-\int_{0}^{t}( \operatorname{div}u_{s})(\phi_{s}^{t}\theta)\mathrm{d}s.\] (47)

We use the following relation for \(\nabla\phi_{s}^{t}\)

\[\nabla\phi_{s}^{t}(\theta)=\exp\left(\int_{t}^{s}\mathrm{d}\tau\left(\nabla u _{\tau}\right)(\phi_{\tau}^{t}(\theta))\right).\] (48)

This relation is standard and can be directly deduced from the following ODE for \(\nabla\phi_{s}^{t}\)

\[\partial_{s}\nabla\phi_{s}^{t}(\theta)=\nabla\partial_{s}\phi_{s}^{t}(\theta )=\nabla(u_{s}(\phi_{s}^{t}(\theta)))=\left((\nabla u_{s})(\phi_{s}^{t}( \theta))\right)\cdot\nabla\phi_{s}^{t}(\theta).\] (49)

We can conclude that for \(0\leq s,t\leq 1\) the bound

\[|\nabla\phi_{s}^{t}(\theta)|\leq e^{L}\] (50)

holds. We find

\[\begin{split}|\nabla\ln(p_{t}(\theta))|&=\left| \nabla\ln(p_{0})(\phi_{0}^{t}\theta)\cdot\nabla\phi_{0}^{t}(\theta)-\int_{0}^{ t}(\nabla\operatorname{div}u_{s})(\phi_{s}^{t}\theta)\cdot\nabla\phi_{s}^{t}( \theta)\mathrm{d}s\right|\\ &\leq|\nabla\ln(p_{0})(\phi_{0}^{t}\theta)|e^{L}+L^{\prime}e^{L},\end{split}\] (51)

and a similar bound holds for \(q_{t}\). In words, we have shown that the score of \(p_{t}\) at \(\theta\) can be bounded by the score of \(p_{0}\) of theta transported along the vector field \(u_{t}\) minus a correction which quantifies the change of score along the path. We now bound using the definition \(p_{t}=(\phi_{t})_{*}p_{0}\) and the assumption (41)

\[\begin{split}\int p_{t}(\mathrm{d}\theta)|\nabla\ln p_{0}(\phi_{0} ^{t}(\theta))|^{2}&=\int p_{0}(\mathrm{d}\theta_{0})|\nabla\ln p _{0}(\phi_{0}^{t}\phi_{t}(\theta_{0}))|^{2}=\mathbb{E}_{p_{0}}\left|\nabla\ln p _{0}(\theta_{0})\right|^{2}\\ &\leq\mathbb{E}_{p_{0}}(C_{1}(1+|\theta_{0}|)^{2})\leq 2C_{1}^{2}( 1+\mathbb{E}_{p_{0}}\left|\theta_{0}\right|^{2})\leq 2C_{1}^{2}(1+C_{2}^{2}).\end{split}\] (52)

Similarly we obtain using \(q_{0}=p_{0}\)

\[\int p_{t}(\mathrm{d}\theta)|\nabla\ln q_{0}(\psi_{0}^{t}\theta)|^{2}=\int p_{0 }(\mathrm{d}\theta_{0})|\nabla\ln q_{0}(\psi_{0}^{t}\phi_{t}\theta_{0})|^{2}.\] (53)In words, to control the score of \(q\) integrated with respect to \(p_{t}\) we need to control the distortion we obtain when moving forward with \(u\) and backwards with \(v\). We investigate \(\psi_{0}^{t}\phi_{t}(\theta_{0})\). We now show

\[\partial_{h}\psi_{t}^{t+h}\phi_{t+h}^{t}(\theta)|_{h=0}=u_{t}(\theta)-v_{t}( \theta).\] (54)

First, by definition of \(\phi\), we find

\[\partial_{h}\phi_{t+h}^{t}(\theta)|_{h=0}=\partial_{h}\phi_{t+h}\phi_{t}^{-1} (\theta)|_{h=0}=u_{t}(\phi_{t}\phi_{t}^{-1}(\theta))=u_{t}(\theta).\] (55)

To evaluate the second contribution we observe

\[\begin{split} 0&=\partial_{h}\theta|_{h=0}= \partial_{h}\psi_{t+h}^{t+h}(\theta)|_{h=0}=\partial_{h}\psi_{t+h}\psi_{t+h}^{ -1}(\theta)|_{h=0}\\ &=(\partial_{h}\psi_{t+h})\psi_{t}^{-1}(\theta)|_{h=0}+\psi_{t} (\partial_{h}\psi_{t+h}^{-1})(\theta)|_{h=0}=v_{t}(\psi_{t}\psi_{t}^{-1}( \theta))+\partial_{h}\psi_{t}\psi_{t+h}^{-1}(\theta)|_{h=0}\\ &=v_{t}(\theta)+\partial_{h}\psi_{t}^{t+h}(\theta)|_{h=0}\end{split}\] (56)

Now (54) follows from (55) and (56) together with \(\phi_{t}^{t}=\psi_{t}^{t}=\mathrm{Id}\). Using (54) we find

\[\partial_{t}(\psi_{0}^{t}\phi_{t})(\theta_{0})=\partial_{h}(\psi_{0}^{t}\psi_ {t}^{t+h}\phi_{t+h}^{t}\phi_{t})(\theta_{0})|_{h=0}=(\nabla\psi_{0}^{t})( \phi_{t}(\theta_{0}))\cdot\left((u_{t}-v_{t})(\phi_{t}(\theta_{0}))\right).\] (57)

Using (50) we conclude that

\[\begin{split}|\psi_{0}^{t}\phi_{t}(\theta_{0})-\theta_{0}|& \leq\left|\int_{0}^{t}\partial_{s}\psi_{0}^{s}\phi_{s}(\theta_{0}) \,\mathrm{d}s\right|\leq\int_{0}^{t}|(\nabla\psi_{0}^{s})(\phi_{s}(\theta_{0 }))|\cdot|u_{s}-v_{s}|(\phi_{s}(\theta_{0}))\,\mathrm{d}s\\ &\leq e^{L}\int_{0}^{t}|u_{s}-v_{s}|(\phi_{s}(\theta_{0}))\, \mathrm{d}s.\end{split}\] (58)

We use this and the assumption (41) to continue to estimate (53) as follows

\[\begin{split}\int p_{t}(\mathrm{d}\theta)|\nabla\ln q_{0}(\psi_{ 0}^{t}\theta)|^{2}&=\int p_{0}(\mathrm{d}\theta_{0})|\nabla\ln q _{0}(\psi_{0}^{t}\phi_{t}(\theta_{0}))|^{2}\\ &\leq C_{1}^{2}\int p_{0}(\mathrm{d}\theta_{0})(1+|\psi_{0}^{t} \phi_{t}(\theta_{0})|)^{2}\\ &\leq C_{1}^{2}\int p_{0}(\mathrm{d}\theta_{0})(1+|\psi_{0}^{t} \phi_{t}(\theta_{0})-\theta_{0}|+|\theta_{0}|)^{2}\\ &\leq 3C_{1}^{2}+3C_{1}^{2}\int p_{0}(\mathrm{d}\theta_{0})\left(| \psi_{0}^{t}\phi_{t}(\theta_{0})-\theta_{0}|^{2}+|\theta_{0}|^{2}\right)\\ &\leq 3C_{1}^{2}(1+\mathbb{E}_{p_{0}}\left|\theta_{0}\right|^{2})+3C_{1} ^{2}e^{2L}\int p_{0}(\mathrm{d}\theta_{0})\left(\int_{0}^{t}\mathrm{d}s\left| u_{s}-v_{s}|(\phi_{s}(\theta_{0}))\right)^{2}.\end{split}\] (59)

Here we used \((a+b+c)^{2}\leq 3(a^{2}+b^{2}+c^{2})\) in the second to last step. We bound the remaining integral using Cauchy-Schwarz as follows

\[\begin{split}\int p_{0}(\mathrm{d}\theta_{0})\left(\int_{0}^{t} |u_{s}-v_{s}|(\phi_{s}(\theta_{0}))\right)^{2}&\leq\int p_{0}( \mathrm{d}\theta_{0})\left(\int_{0}^{t}\mathrm{d}s\left|u_{s}-v_{s}\right|^{2} (\phi_{s}(\theta_{0}))\right)\left(\int_{0}^{t}\mathrm{d}s\,1^{2}\right)\\ &\leq t\int_{0}^{t}\mathrm{d}s\int p_{0}(\mathrm{d}\theta_{0})|u _{s}-v_{s}|^{2}(\phi_{s}(\theta_{0}))\\ &=t\int_{0}^{t}\mathrm{d}s\int p_{s}(\mathrm{d}\theta_{s})|u_{s}- v_{s}|^{2}(\theta_{s})\\ &\leq\int_{0}^{1}\mathrm{d}s\int p_{s}(\mathrm{d}\theta_{s})|u _{s}-v_{s}|^{2}(\theta_{s})=\mathrm{MSE}_{p}(u,v).\end{split}\] (60)

The last displays together imply

\[\int p_{t}(\mathrm{d}\theta)|\nabla\ln q_{0}(\psi_{0}^{t}\theta)|^{2}\leq 3C_{1 }^{2}\left(1+\mathbb{E}_{p_{0}}\left|\theta_{0}\right|^{2}+e^{2L}\,\mathrm{MSE }_{p}(u,v)\right).\] (61)Now we have all the necessary ingredients to bound the derivative of the KL-divergence. We control the second integral in (45) using (51) (and again \((\sum_{i=1}^{4}a_{i})^{2}\leq 4\sum a_{i}^{2}\)) as follows,

\[\begin{split}\int p_{t}(\mathrm{d}\theta)&|\nabla\ln p _{t}(\theta)-\nabla\ln q_{t}(\theta)|^{2}\\ &\leq 2\cdot 2^{2}\cdot L^{\prime 2}e^{2L}+4e^{2L}\int p_{t}( \mathrm{d}\theta)\left(|\nabla\ln q_{0}(\psi_{0}^{t})\theta)|^{2}+|\nabla\ln p _{0}(\phi_{0}^{t})\theta)|^{2}\right).\end{split}\] (62)

Using (52) and (61) we finally obtain

\[\begin{split}\int p_{t}(\mathrm{d}\theta)|\nabla\ln p_{t}( \theta)-\nabla\ln q_{t}(\theta)|^{2}&\leq 8\cdot L^{\prime 2}e^{2L}+C_{ 1}^{2}e^{2L}\left(20(1+C_{2}^{2})+12\,\mathrm{MSE}_{p}(u,v)\right)\\ &\leq C(1+\mathrm{MSE}_{p}(u,v))\end{split}\] (63)

for some constant \(C>0\). Finally, we obtain

\[\begin{split}\mathrm{D}_{\mathrm{KL}}(p_{1}||q_{1})& =\int_{0}^{1}\mathrm{d}t\,\partial_{t}\mathrm{D}_{\mathrm{KL}}(p_ {t}||q_{t})\\ &\leq(C(1+\mathrm{MSE}_{p}(u,v)))^{\frac{1}{2}}\int_{0}^{1} \mathrm{d}t\,\left(\int p_{t}(\mathrm{d}\theta)|u_{t}(\theta)-v_{t}(\theta)|^ {2}\right)^{\frac{1}{2}}\\ &\leq(C(1+\mathrm{MSE}_{p}(u,v)))^{\frac{1}{2}}\left(\int_{0}^{1} \mathrm{d}t\,\int p_{t}(\mathrm{d}\theta)|u_{t}(\theta)-v_{t}(\theta)|^{2} \right)^{\frac{1}{2}}\\ &\leq(C(1+\mathrm{MSE}_{p}(u,v)))^{\frac{1}{2}}\,\mathrm{MSE}_{p} (u,v)^{\frac{1}{2}}.\end{split}\] (64)

## Appendix C SBI Benchmark

In this section, we collect missing details and additional results for the analysis of the SBI benchmark in Section 4.

### Network architecture and hyperparameters

For each task and simulation budget in the benchmark, we perform a mild hyperparameter optimization. We sweep over the batch size and learning rate (which is particularly important as the simulation budgets differ by orders of magnitudes), the network size and the \(\alpha\) parameter for the time prior defined in Section 3.3 (see Tab. 2 for the specific values). We reserve 5% of the simulation budget for validation and choose the model with the best validation loss across all configurations.

### Additional results

We here provide various additional results for the SBI benchmark. First, we compare the performance of FMPE and NPE when using the Maximum Mean Discrepancy metric (MMD). The results can be found in Fig. 6. FMPE shows superior performance to NPE for most tasks and simulation budgets. Compared to the C2ST scores in Fig. 4 the improvement shown by FMPE in MMD is more substantial.

Fig. 7 compares the FMPE results with the optimal transport path from the main text with a comparable score matching model using the Variance Preserving diffusion path [15]. The score matching results were obtained using the same batch size, network size and learning rate as the FMPE network, while optimizing for \(\beta_{\text{min}}\in\{0.1,1,4\}\) and \(\beta_{\text{max}}\in\{4,7,10\}\). FMPE with the optimal transport path clearly outperforms the score-based model on almost all configurations.

In Fig. 8 we compare FMPE using the architecture proposed in Section 3.2 with \((t,\theta)\)-conditioning via gated linear units to FMPE with a naive architecture operating directly on the concatenated \((t,\theta,x)\) vector. For the two displayed tasks the context dimension \(\dim(x)=100\) is much larger than the parameter dimension \(\dim(\theta)\in\{5,10\}\), and there is a clear performance gain in using the GLU conditioning. Our interpretation is that the low dimensionality of \((t,\theta)\) means that it is not well-learned by the network when simply concatenated with \(x\).

Fig. 9 displays the densities of the reference samples under the FMPE model as a histogram for all tasks (extended version of Fig. 3). The support of the learned model \(q(\theta|x)\) covers the reference samples \(\theta\sim p(\theta|x)\), providing additional empirical evidence for the mass-covering behavior theoretically explored in Thm. 1. However, samples from the true posterior distribution may have a small density under the learned model, especially if the deviation between model and reference is high; see Lotka-Volterra (bottom right panel). Fig. 10 displays P-P plots for two selected tasks.

Finally, we study the impact of our time prior re-weighting for one example task in Fig. 11. We clearly see that our proposed re-weighting leads to increased performance by up-weighting samples for \(t\) closer to \(1\) during training.

\begin{table}
\begin{tabular}{l l} \hline \hline hyperparameter & sweep values \\ \hline hidden dimensions & \(2^{n}\) for \(n\in\{4,\dots,10\}\) \\ number of blocks & \(10,\dots,18\) \\ batch size & \(2^{n}\) for \(n\in\{2,\dots,9\}\) \\ learning rate & 1.e-3, 5.e-4, 2.e-4, 1.e-4 \\ \(\alpha\) (for time prior) & -0.25, -0.5, 0, 1, 4 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Sweep values for the hyperparamters for the SBI benchmark. We split the configurations according to simulation budgets, e.g. for 1000 simulations, we only swept over smaller values for network size and batch size. The network architecture has a diamond shape, with increasing layer width from smallest to largest and then decreasing to the output dimension. Each block consists of two fully-connected residual layers.

Figure 6: Comparison of FMPE and NPE performance across 10 SBI benchmarking tasks [46]. We here quantify the deviation in terms of the Maximum Mean Discrepancy (MMD) as an alternative metric to the C2ST score used in Fig. 4. MMD can be sensitive to its hyperparameters [46], so we use the C2ST score as a primary performance metric.

Figure 8: Comparison of the architecture proposed in Section 3.2 with gated linear units for the \((t,\theta)\)-conditioning (red) and a naive architecture based on a simple concatenation of \((t,\theta,x)\) (black). FMPE with the proposed architecture performs substantially better.

Figure 7: Comparison of FMPE with the optimal transport path (as used throughout the main paper) with comparable models trained with a Variance Preserving diffusion path [15] by regressing on the score (SMPE). Note that the SMPE baseline shown here is not directly comparable to NPSE [8, 9], as this method uses Langevin steps, which reduces the dependence of the results on the vector field for small \(t\) (at the cost of a tractable density).

Figure 9: Histogram of FMPE densities \(\log q(\theta|x)\) for samples \(\theta\sim q(\theta|x)\) and reference samples \(\theta\sim p(\theta|x)\) for simulation budgets \(N=10^{3}\) (left), \(N=10^{4}\) (center) and \(N=10^{5}\) (right). The reference samples \(\theta\sim p(\theta|x)\) are all within the support of the learned model \(q(\theta|x)\), indicating mass covering FMPE results. Nonetheless, reference samples may have a small density under \(q(\theta|x)\), if the validation loss is high, see Lotka-Volterra (LV).

Figure 11: Comparison of the time prior re-weighting proposed in Section 3.3 with a uniform prior over \(t\) on the Two Moons task (Section 4). The network trained with the re-weighted prior clearly outperforms the reference on all simulation budgets.

Figure 10: P-P plot for the marginals of the FMPE-posterior for the Two Moons (upper) and SLCP (lower) tasks for training budgets of \(10^{3}\) (left), \(10^{4}\) (center), and \(10^{5}\) (right) samples.

Gravitational-wave inference

We here provide the missing details and additional results for the gravitational wave inference problem analyzed in Section 5.

### Network architecture and hyperparameters

Compared to NPE with normalizing flows, FMPE allows for generally simpler architectures, since the output of the network is simply a vector field. This also holds for NPSE (model also defined by a vector) and NRE (defined by a scalar). Our FMPE architecture builds on the embedding network developed in [7], however we extend the network capacity by adding more residual blocks (Tab. 3, top panel). For the \((t,\theta)\)-conditioning we use gated linear units applied to each residual block, as described in Section 3.2. We also use a small residual network to embed \((t,\theta)\) before applying the gated linear units.

In this Appendix we also perform an ablation study, using the _same_ embedding network as the NPE network (Tab. 3, bottom panel). For this configuration, we additionally study the effect of conditioning on \((t,\theta)\) starting from different layers of the main residual network.

### Data settings

We use the data settings described in [7], with a few minor modifications. In particular, we use the waveform model IMRPhenomPv2 [76, 77, 78] and the prior displayed in Tab. 4. Generation of the training dataset with 5,000,000 samples takes around 1 hour on 64 CPUs. Compared to [7], we reduce the frequency range from \([20,1024]\) Hz to \([20,512]\) Hz to reduce the computational load for data preprocessing. We also omit the conditioning on the detector noise power spectral density (PSD) introduced in [7] as we evaluate on a single GW event. Preliminary tests show that the performance with PSD conditioning is similar to the results reported in this paper. All changes to the data settings have been applied to FMPE and the NPE baselines alike to enable a fair comparison.

### Additional results

Tab. 5 displays the inference times for FMPE and NPE. NPE requires only a single network pass to produce samples and (log-)probabilities, whereas many forwards passes are needed for FMPE to solve the ODE with a specific level of accuracy. A significant portion of the additional time required for calculating (log-)probabilities in conjunction with the samples is spent on computing the divergence of the vector field, see Eq. (3).

\begin{table}
\begin{tabular}{l l} \hline \hline hyperparameter & values \\ \hline residual blocks & \(2048\), \(4096\times 3\), \(2048\times 3\), \(1024\times 6\), \(512\times 8\), \(256\times 10\), \\  & \(128\times 5\), \(64\times 3\), \(32\times 3\), \(16\times 3\) \\ residual blocks \((t,\theta)\) embedding & \(16,32,64,128,256\) \\ batch size & 4096 \\ learning rate & 5.e-4 \\ \(\alpha\) (for time prior) & 1 \\ \hline residual blocks & \(2048\times 2\), \(1024\times 4\), \(512\times 4\), \(256\times 4\), \(128\times 4\), \(64\times 3\), \\  & \(32\times 3\), \(16\times 3\) \\ residual blocks \((t,\theta)\) embedding & \(16,32,64,128,256\) \\ batch size & 4096 \\ learning rate & 5.e-4 \\ \(\alpha\) (for time prior) & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameters for the FMPE models used in the main text (top) and in the ablation study (bottom, see Fig. 12). The network is composed of a sequence of residual blocks, each consisting of two fully-connected hidden layers, with a linear layer between each pair of blocks. The ablation network is the same as the embedding network that feeds into the NPE normalizing flow.

Fig. 12 presents a comparison of the FMPE performance using networks of the same hidden dimensions as the NPE embedding network (Tab. 3 bottom panel). This comparison includes an ablation study on the timing of the \((t,\theta)\) GLU-conditioning. In the top-row network, the \((t,\theta)\) conditioning is applied only after the 256-dimensional blocks. In contrast, the middle-row network receives \((t,\theta)\) immediately after the initial residual block. With FMPE we can achieve performance comparable to NPE, while having only \(\approx 1/3\) of the network size (most of the NPE network parameters are in the flow). This suggests that parameterizing the target distribution in terms of a vector field requires less learning capacity, compared to directly learning its density. Delaying the \((t,\theta)\) conditioning until the final layers impairs performance. However, the number of FLOPs at inference is considerably reduced, as the context embedding can be cached and a network pass only involves the few layers with the \((t,\theta)\) conditioning. Consequently, there's a trade-off between accuracy and inference speed, which we will explore in a greater scope in future work.

\begin{table}
\begin{tabular}{l l l} \hline \hline Description & Parameter & Prior \\ \hline component masses & \(m_{1}\), \(m_{2}\) & \([10,120]\)\(\mathrm{M}_{\odot}\), \(m_{1}\geq m_{2}\) \\ chirp mass & \(M_{c}=(m_{1}m_{2})^{\frac{3}{5}}/(m_{1}+m_{2})^{\frac{1}{5}}\) & \([20,120]\)\(\mathrm{M}_{\odot}\) (constraint) \\ mass ratio & \(q=m_{2}/m_{1}\) & [0.125, 1.0] (constraint) \\ spin magnitudes & \(a_{1},a_{2}\) & \([0,0.99]\) \\ spin angles & \(\theta_{1}\), \(\theta_{2}\), \(\phi_{12}\), \(\phi_{JL}\) & standard as in [79] \\ time of coalescence & \(t_{c}\) & \([-0.03,0.03]\) s \\ luminosity distance & \(d_{L}\) & \([100,1000]\) Mpc \\ reference phase & \(\phi_{c}\) & \([0,2\pi]\) \\ inclination & \(\theta_{JN}\) & \([0,\pi]\) uniform in sine \\ polarization & \(\psi\) & \([0,\pi]\) \\ sky position & \(\alpha,\beta\) & uniform over sky \\ \hline \hline \end{tabular}
\end{table}
Table 4: Priors for the astrophysical binary black hole parameters. Priors are uniform over the specified range unless indicated otherwise. Our models infer the mass parameters in the basis \((M_{c},q)\) and marginalize over the phase parameter \(\phi_{c}\).

Figure 12: Jensen-Shannon divergence between inferred posteriors and the reference posteriors for GW150914 [73]. We compare two FMPE models with the same architecture as the NPE embedding network, see Tab. 3 bottom panel. For the model in the first row, the GLU conditioning of \((\theta,t)\) is only applied before the final 128-dim blocks. The model in the middle row is given the context after the very first 2048 block.

\begin{table}
\begin{tabular}{l r r} \hline \hline  & Network Passes & Inference Time (per batch) \\ \hline FMPE (sample only) & 248 & 26s \\ FMPE (sample and log probs) & 350 & 352s \\ \hline NPE (sample and log probs) & 1 & 1.5s \\ \hline \hline \end{tabular}
\end{table}
Table 5: Inference times per batch for FMPE and NPE on a single Nvidia A100 GPU, using the training batch size of 4096. We solve the ODE for FMPE using the dopri5 discretization [80] with absolute and relative tolerances of 1e-7. For FMPE, generation of the (log-)probabilities additionally requires the computation of the divergence, see equation (3). This needs additional memory and therefore limits the maximum batch size that can be used at inference.