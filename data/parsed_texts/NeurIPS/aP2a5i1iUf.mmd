# Understanding Mode Connectivity via Parameter Space Symmetry

 Bo Zhao

University of California San Diego

bozhao@ucsd.edu

&Nima Dehmamy

IBM Research

nima.dehmamy@ibm.com

Robin Walters

Northeastern University

r.walters@northeastern.edu

&Rose Yu

University of California San Diego

roseyu@ucsd.edu

###### Abstract

It has been observed that the global minimum of neural networks is connected by curves on which train and test loss is almost constant. This phenomenon, often referred to as mode connectivity, has inspired various applications such as model ensembling and fine-tuning. Despite empirical evidence, a theoretical explanation is still lacking. We explore the connectedness of minimum through a new approach, parameter space symmetry. By relating topology of symmetry groups to topology of minima, we provide the number of connected components of full-rank linear networks. In particular, we show that skip connections reduce the number of connected components. We then prove mode connectivity up to permutation for linear networks. We also provide explicit expressions for connecting curves in minimum induced by symmetry.

## 1 Introduction

Among recent studies on the loss landscape, a particularly interesting discovery is mode connectivity [5; 10], which refers to the phenomenon that distinct minima found by stochastic gradient descent (SGD) can be connected by continuous paths through the high-dimensional parameter space of neural networks. Mode connectivity has implications on other phenomena in deep learning such as the lottery ticket hypothesis [8] and loss landscape and training trajectory analysis [11]. Additionally, mode connectivity has inspired applications in diverse fields, including model ensembling [10; 2; 3], model averaging [15; 30], pruning [8], improving adversarial robustness [34], and fine-tuning for altering prediction mechanism [20].

Discrete symmetry, especially permutation, is well-known to be related to mode connectivity. In particular, the neural network output is invariant to permuting the neurons [13]. [6] conjectures that all minima found by SGD are linearly connected up to permutation. Various algorithms have since been developed to find the optimal permutation for linear mode connectivity [1]. However, compared to discrete symmetry, the role of continuous symmetry remains less studied. Continuous symmetry groups with continuous actions define positive dimensional connected spaces in the minimum [32]. We explore the connectedness of minimum through continuous symmetries in the parameter space.

We reveal the role of symmetry in the connectivity of minimum by relating properties of topological groups to their orbits and the minimum. Our results show that both continuous and discrete symmetry are important and useful in understanding the origin and failure cases of mode connectivity. Our work highlights a new approach towards understanding the topology of the minimum and complements previous theories on mode connectivity [31; 9; 23; 24; 18; 27; 25].

## 2 Connectedness of minima

### Linear network with invertible weights

Let **Param** be the space of parameters. Consider the multi-layer loss function \(L:\textbf{Param}\rightarrow\mathbb{R}\),

\[L:\textbf{Param}\rightarrow\mathbb{R},\hskip 56.905512pt(W_{1},...,W_{l}) \mapsto||Y-W_{l}...W_{1}X||_{2}^{2}.\] (1)

where \(X,Y\in\mathbb{R}^{h\times h}\) are the input and output of the network. In this subsection, we assume that both \(X,Y\) have rank \(h\), and \(\textbf{Param}=(\mathbb{R}^{h\times h})^{l}\). Then \(L\) has a \(GL_{h}(\mathbb{R})^{l-1}\) symmetry, which acts on **Param** by \(g\cdot(W_{1},...,W_{l})=(g_{1}W_{1},g_{2}W_{2}g_{1}^{-1},...,g_{l-1}W_{l-1}g_{ l-2}^{-1},W_{l}g_{l-1}^{-1})\), for \((g_{1},...,g_{l-1})\in GL_{h}(\mathbb{R})^{l-1}\).

Let \(L^{-1}(c)=\{\theta\in\textbf{Param}:L(\theta)=c\}\) be a level set of \(L\). Since \(\|\cdot\|_{2}\geq 0\) and \(L^{-1}(0)\neq\emptyset\), the minimum value of \(L\) is 0. By relating the topology of \(GL(\mathbb{R})\) and \(L^{-1}(0)\), we have the following observations on the structure of the minimum of \(L\).

**Proposition 2.1**.: _There is a homeomorphism between \(L^{-1}(0)\) and \((\mathrm{GL}_{h})^{l-1}\)._

Since \((\mathrm{GL}_{h})^{l-1}\) has \(2^{l-1}\) connected components and homeomorphism preserves topological properties, \(L^{-1}(0)\) also has \(2^{l-1}\) connected components.

**Corollary 2.2**.: _The minimum of \(L\) has \(2^{l-1}\) connected components._

### ResNet with 1D weights

The topological properties of the minimum depend on the architecture. As an example of this dependency, we show that adding a skip connection changes the number of connected components of the minimum.

Consider a residual network \(W_{3}(W_{2}W_{1}X+\varepsilon X)\) and loss function

\[L(W_{3},W_{2},W_{1})=||Y-W_{3}(W_{2}W_{1}X+\varepsilon X)||_{2},\] (2)

where \((W_{1},W_{2},W_{3})\in\textbf{Param}=\mathbb{R}^{n\times n}\times\mathbb{R}^ {n\times n}\times\mathbb{R}^{n\times n}\), \(\varepsilon\in\mathbb{R}\), and data \(X\in\mathbb{R}^{n\times n},Y\in R^{n\times n}\). The following proposition states that for a three-layer residual network with weight matrices of dimension \(1\times 1\), the number of components of the minimum is smaller than that of a linear network without the skip connection.

**Proposition 2.3**.: _Let \(n=1\). Assume that \(X,Y\neq 0\). When \(\varepsilon=0\), the minimum of \(L\) has 4 connected components. When \(\varepsilon\neq 0\), the minimum of \(L\) has 3 connected components._

The \(\varepsilon=0\) case follows from Corollary 2.2. For the \(\varepsilon\neq 0\) case, the proof decomposes the minimum of \(L\) into two sets \(S_{1}\) and \(S_{0}\), corresponding to the minima without the skip connection and an extra set of solutions because of the skip connection. \(S_{1}\) is homeomorphic to \(GL_{1}\times GL_{1}\) and has 4 connected components. \(S_{0}\) is a line and has 1 connected component. Two components of \(S_{1}\) are connected to \(S_{0}\), while the other two components of \(S_{1}\) are not. Therefore, \(S_{0}\) connects two components of \(S_{1}\). As a result, the minimum of \(L\) has 3 connected components. Full proof can be found in Appendix C.3.

Figure 1 visualizes the minimum without and with the skip connection. This result reveals the effect of skip connection on the connectedness of minimum, which may lead to a new explanation of the effectiveness of ResNets [12] and DenseNets [14]. We leave the connection between the topology of minimum and the optimization and generalization property of neural networks to future work.

## 3 Mode connectivity

From the examples in the previous section, the connectedness of the minimum is related to the symmetry of the loss function under certain conditions. In this section, we explore applications of this insight in explaining mode connectivity.

### Mode connectivity up to permutation

For the family of linear neural networks defined in Section 2.1, we show that permutation allows us to connect points in the minimum that are not connected without permutation. Our results support the empirical observation that neuron alignment by permutation improves mode connectivity [29].

Consider again the linear network (1) with full rank weights. When \(l=2\), the minimum of \(L\) has 2 connected components. Any \(g\in GL\) that is not on the identity component can take a point on one connected component of the minimum to the other.

**Lemma 3.1**.: _Consider two points \((W_{1},W_{2}),(W_{1}^{\prime},W_{2}^{\prime})\in L^{-1}(0)\) that are not connected in \(L^{-1}(0)\). For any \(g\in GL(h)\) such that \(det(g)<0\), \(g\cdot(W_{1},W_{2})\) and \((W_{1}^{\prime},W_{2}^{\prime})\) are connected in \(L^{-1}(0)\)._

When the hidden dimension \(h\geq 2\), there exists a permutation \(g\) such that \(det(g)>0\), and a permutation \(g\) such that \(det(g)<0\). Therefore, all points on the minimum of \(L\) are connected up to permutation.

**Proposition 3.2**.: _Assume that \(h\geq 2\). For all \((W_{1},...,W_{l}),(W_{1}^{\prime},...,W_{l}^{\prime})\in L^{-1}(0)\), these exists a list of permutation matrices \(P_{1},...,P_{l-1}\) such that \((W_{1}P_{1},P_{1}^{-1}W_{2}P_{2},...,P_{l-2}W_{l-1}P_{l-1},P_{l-1}W_{l})\) and \((W_{1}^{\prime},...,W_{l}^{\prime})\) are connected in \(L^{-1}(0)\)._

### Failure case of linear mode connectivity

In addition to helping show the connectedness of minimum, symmetry relates the set of points in minimum to groups and provides a way to find all points in the minimum. As an application, we show that linear mode connectivity fails to hold in multi-layer regressions. The following proposition says that in two-layer full-rank linear networks, the error barrier in the linear interpolation between two solutions can be arbitrarily large.

**Proposition 3.3**.: _Consider the setting in Section 2.1. For any \(k>0\), there exist \((W_{1},W_{2}),(W_{1}^{\prime},W_{2}^{\prime})\in L^{-1}(0)\) that belong to the same connected component of \(L^{-1}(0)\) and \(0<\alpha<1\), such that \(L((1-\alpha)W_{1}+\alpha W_{1}^{\prime},(1-\alpha)W_{2}+\alpha W_{2}^{\prime})>k\)._

The result holds when there is a homogeneous activation (\(\sigma(cz)=c^{\alpha}\sigma(z)\)). However, when \(\alpha\neq 1\), the proof needs a different choice of \(m\). Figure 2 visualizes the two points on the minimum of a two-layer network with weights of dimension \(1\times 1\) and the linear interpolation between them. One possible reason why linear mode connectivity is observed in practice is that only a small part of the minima is reachable by SGD due to implicit bias [21].

Figure 1: Minimum of (a) 3-layer linear net \(||Y-W_{3}W_{2}W_{1}X||_{2}\) and (b) 3-layer linear net with a residual connection \(||Y-W_{3}(W_{2}W_{1}X+X)||_{2}\), where \(X=1\), \(Y=1\), and \(W_{1},W_{2},W_{3}\in\mathbb{R}\).

Figure 2: Interpolation between 2 minima of loss function \(||Y-W_{2}W_{1}X||_{2}\) with 1 dimensional weights. Loss on the interpolation can be unbounded.

Curves on minimum from group actions

The minima of overparametrized ReLU networks consist of affine subspaces [28]. With activations that are not piecewise linear, the minimum may be curved. As a result, the paths connecting two points in the minimum may not be linear. Previously, these paths are discovered empirically by finding parametric curves on which the expected loss is minimized [10]. An alternative and principled way to find curves on the minima is to use parameter space symmetry.

Suppose the loss function \(L:\mathbf{Param}\rightarrow\mathbb{R}\) admits a \(G\) symmetry. Consider the following curve for a point \(\bm{w}\in\mathbf{Param}\) and \(M\in\text{Lie}(G)\):

\[\gamma_{M}:\mathbb{R}\times\mathbf{Param} \rightarrow\mathbf{Param},\] \[\gamma_{M}(t,\bm{w}) =\exp{(tM)}\cdot\bm{w}.\] (3)

Since \(\exp{(tM)}\in G\) and the action of \(G\) preserves the value of \(L\), every point on \(\gamma_{M}\) is in the same \(L\) level set as \(\bm{w}\). This provides a way to find a curve of constant loss between two points that are in the same orbit. Concretely, given two points \(\bm{w}_{1}\) and \(\bm{w}_{2}=g\cdot\bm{w}_{1}\), let \(\gamma\) be the following curve:

\[\gamma:[0,1]\times G\times\mathbf{Param} \rightarrow\mathbf{Param},\] \[\gamma(t,g,\bm{w}) =\exp{(t\log(g))}\cdot\bm{w}.\] (4)

Note that \(\gamma(0,g,\bm{w}_{1})=\bm{w}_{1}\), \(\gamma(1,g,\bm{w}_{1})=\bm{w}_{2}\), and \(L(\gamma(t,g,\bm{w}_{1}))=L(\bm{w}_{1})=L(\bm{w}_{2})\) for all \(t\in[0,1]\). Hence, \(\gamma\) is a curve that connects the points \(\bm{w}_{1}\) and \(\bm{w}_{2}\), and every point on \(\gamma\) has the same loss value as \(L(\bm{w}_{1})=L(\bm{w}_{2})\).

For a group \(G\), the curve \(\gamma\) is defined when the map \(\cdot:G\times\mathbf{Param}\rightarrow\mathbf{Param}\) is continuous and \(\text{id}\cdot\bm{w}=\bm{w}\) for all \(\bm{w}\in\mathbf{Param}\), even if it is not a group action or does not preserve loss. However, when \(\cdot\) does not preserve loss, the loss can change on \(\gamma\). Consider our two-layer network and the following map:

\[\cdot:GL(h,\mathbb{R})\times\mathbf{Param} \rightarrow\mathbf{Param}\] \[g\cdot(U,V) =(U\sigma(VX)\sigma(gVX)^{\dagger},gV).\] (5)

When \(\sigma\) is the identity function, \(\cdot\) preserves the loss value, and \(\gamma\) defines a curve on the minimum. In general, the map (5) does not preserve loss when batch size \(k\) is larger than hidden dimension \(h\). However, the maximum change of loss on \(\gamma\) can be bounded as follows. Let \(U^{\prime},V^{\prime}=g\cdot(U,V)\). We have

\[\|U\sigma(VX)-U^{\prime}\sigma(V^{\prime}X)\|=\|U\sigma(VX)\left(I-\sigma(gVX )^{\dagger}\sigma(gVX)\right)\|\leq\|U\sigma(VX)\|.\] (6)

The last steps follows from the fact that \(\sigma(gVX)^{\dagger}\sigma(gVX)\) is a projection.

## 5 Discussion

In this work, we study topological properties of the loss level sets by relating their topology to the topology of symmetry groups. We derive the number of connected components of full-rank multi-layer networks with and without skip connections, and prove mode connectivity up to permutation for full-rank linear regressions. Using symmetry in the parameter space, we construct an explicit expression for curves that connect two points in the same orbit.

While symmetry appears to be a useful tool for studying the loss landscape, our current results rely on the existence of a homeomorphism between symmetry groups and the minimum. A future direction is to explore the possibility of removing this assumption. Another interesting direction is to investigate additional links between different architecture choices, such as normalization, and connectedness of the minimum. On the application side, the impact of these results can benefit from further study on the connection between the topology of minimum and generalization ability of neural networks.

The connectedness results obtained from symmetry raise a number of interesting questions related to mode connectivity. For example, it would be interesting to understand when and why there is no significant change in loss on the linear interpolation between two minima. One possible explanation is that there always exists a \(\gamma\) defined in the way above that is close to the line formed by the linear interpolation. Another possible reason is that the dimension of minimum is usually high, and a significant part of the linear interpolation is within the minimum with high probability. Moreover, it has been observed that the train and test accuracy are both near constant on the paths that connect different SGD solutions [10]. If these paths correspond to a group action, this implies that the action's dependence on data is weak.

## Acknowledgments and Disclosure of Funding

We thank Jordan Ganev for helpful comments on proofs in Appendix B. This work was supported in part by the U.S. Army Research Office under Army-ECASE award W911NF-07-R-0003-03, the U.S. Department Of Energy, Office of Science, IARPA HAYSTAC Program, NSF Grants #2205093, #2146343, and #2134274. R. Walters is supported by NSF grants #2107256 and #2134178.

## References

* Ainsworth et al. [2023] Samuel K. Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. _International Conference on Learning Representations_, 2023.
* Benton et al. [2021] Gregory Benton, Wesley Maddox, Sanae Lotfi, and Andrew Gordon Wilson. Loss surface simplexes for mode connecting volumes and fast ensembling. In _International Conference on Machine Learning_, pages 769-779. PMLR, 2021.
* Benzzing et al. [2022] Frederik Benzzing, Simon Schug, Robert Meier, Johannes Von Oswald, Yassir Akram, Nicolas Zucchet, Laurence Aitchison, and Angelika Steger. Random initialisations performing above chance and how to find them. _14th Annual Workshop on Optimization for Machine Learning (OPT2022)_, 2022.
* Brea et al. [2019] Johanni Brea, Berlin Simsek, Bernd Illing, and Wulfram Gerstner. Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape. _arXiv preprint arXiv:1907.02911_, 2019.
* Draxler et al. [2018] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In _International conference on machine learning_, pages 1309-1318. PMLR, 2018.
* Entezari et al. [2022] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. _International Conference on Learning Representations_, 2022.
* Ferbach et al. [2023] Damien Ferbach, Baptiste Goujaud, Gauthier Gidel, and Aymeric Dieuleveut. Proving linear mode connectivity of neural networks via optimal transport. _arXiv preprint arXiv:2310.19103_, 2023.
* Frankle et al. [2020] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In _International Conference on Machine Learning_, pages 3259-3269. PMLR, 2020.
* Freeman and Bruna [2017] C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. In _5th International Conference on Learning Representations, ICLR_, 2017.
* Garipov et al. [2018] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. _Advances in neural information processing systems_, 31, 2018.
* Gotmare et al. [2018] Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Using mode connectivity for loss landscape analysis. _35th International Conference on Machine Learning's Workshop on Modern Trends in Nonconvex Optimization for Machine Learning_, 2018.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hecht-Nielsen [1990] Robert Hecht-Nielsen. On the algebraic structure of feedforward network weight spaces. In _Advanced Neural Computers_, pages 129-135. Elsevier, 1990.
* Huang et al. [2017] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.

* [15] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. _Conference on Uncertainty in Artificial Intelligence_, 2018.
* [16] Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur. Repair: Renormalizing permuted activations for interpolation repair. _International Conference on Learning Representations_, 2023.
* [17] Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, Joao Sedoc, and Naomi Saphra. Linear connectivity reveals generalization strategies. _International Conference on Learning Representations_, 2023.
* [18] Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Rong Ge, and Sanjeev Arora. Explaining landscape connectivity of low-cost solutions for multilayer nets. _Advances in neural information processing systems_, 32, 2019.
* [19] John Lee. _Introduction to topological manifolds_, volume 202. Springer Science & Business Media, 2010.
* [20] Ekdeep Singh Lubana, Eric J Bigelow, Robert P Dick, David Krueger, and Hidenori Tanaka. Mechanistic mode connectivity. In _International Conference on Machine Learning_, pages 22965-23004. PMLR, 2023.
* [21] Hancheng Min, Salma Tarmoun, Rene Vidal, and Enrique Mallada. On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks. In _International Conference on Machine Learning_, pages 7760-7768. PMLR, 2021.
* [22] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? _Advances in neural information processing systems_, 33:512-523, 2020.
* [23] Quynh Nguyen. On connected sublevel sets in deep learning. In _International conference on machine learning_, pages 4790-4799. PMLR, 2019.
* [24] Quynh Nguyen. A note on connectivity of sublevel sets in deep learning. _arXiv preprint arXiv:2101.08576_, 2021.
* [25] Quynh N Nguyen, Pierre Brechet, and Marco Mondelli. When are solutions connected in deep networks? _Advances in Neural Information Processing Systems_, 34:20956-20969, 2021.
* [26] Fabrizio Pittorino, Antonio Ferraro, Gabriele Perugini, Christoph Feinauer, Carlo Baldassi, and Riccardo Zecchina. Deep networks on toroids: Removing symmetries reveals the structure of flat regions in the landscape geometry. In _Proceedings of the 39th International Conference on Machine Learning_, pages 17759-17781, 2022.
* [27] Alexander Shevchenko and Marco Mondelli. Landscape connectivity and dropout stability of sgd solutions for over-parameterized neural networks. In _International Conference on Machine Learning_, pages 8773-8784. PMLR, 2020.
* [28] Berlin Simsek, Francois Ged, Arthur Jacot, Francesco Spadaro, Clement Hongler, Wulfram Gerstner, and Johanni Brea. Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances. In _International Conference on Machine Learning_, pages 9722-9732. PMLR, 2021.
* [29] Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, and Rongjie Lai. Optimizing mode connectivity via neuron alignment. _Advances in Neural Information Processing Systems_, 33:15300-15311, 2020.
* [30] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _International Conference on Machine Learning_, pages 23965-23998. PMLR, 2022.

* [31] David Yunis, Kumar Kshitij Patel, Pedro Henrique Pamplona Savarese, Gal Vardi, Jonathan Frankle, Matthew Walter, Karen Livescu, and Michael Maire. On convexity and linear mode connectivity in neural networks. In _OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)_, 2022.
* [32] Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, and Nima Dehmany. Symmetries, flat minima, and the conserved quantities of gradient flow. _International Conference on Learning Representations_, 2023.
* [33] Bo Zhao, Robert M Gower, Robin Walters, and Rose Yu. Improving convergence and generalization using parameter symmetries. _arXiv preprint arXiv:2305.13404_, 2023.
* [34] Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin. Bridging mode connectivity in loss landscapes and adversarial robustness. _International Conference on Learning Representations_, 2020.
* [35] Zhanpeng Zhou, Yongyi Yang, Xiaojiang Yang, Junchi Yan, and Wei Hu. Going beyond linear mode connectivity: The layerwise linear feature connectivity. _arXiv preprint arXiv:2307.08286_, 2023.

## Appendix A Related Work

Mode connectivity[10] and [5] discover empirically that the global minimum of neural networks are connected by curves on which train and test loss is almost constant. It is then observed that SGD solutions are linearly connected if they are trained from pre-trained weights [22] or share a short period of training at the beginning [8]. Additionally, neuron alignment by permutation improves mode connectivity [29]. Then, [6] conjecture that all minima found by SGD are linearly connected up to permutation. Following the conjecture, [1] develop algorithms that finds the optimal alignment for linear mode connectivity, and [16] further reduce the barrier by rescaling the preactivations of interpolated networks. A few papers propose theoretical explanation of linear mode connectivity using different tools. [35] shows that the feature maps of each layer are also linearly connected and identify conditions that guarantees linear connectivity. [31] seeks to explain linear mode connectivity through finding a convex hull defined by SGD trajectory endpoints. [7] uses optimal transport theory to prove that wide two-layer neural networks trained using SGD are linearly connected with high probability. It is worth noting that linear mode connectivity does not always hold outside of computer vision. Language models that are not linearly connected have different generalization strategies [17]. [20] further show that the lack of linear connectivity indicates that the two models rely on different attributes to make predictions.

Theory on connectedness of minimumSeveral work explores the theoretical explanation of mode connectivity by studying the connectedness of sub-level sets. [9] shows that the minimum is connected for 2-layer linear network without regularization, and for deeper linear networks with \(L2\) regularization. Futhermore, they show that the minimum of a two-layer ReLU network is asymptotically connected, that is, there exists a path connecting any two solutions with bounded error. [23] proves that the sublevel sets are connected in pyramidal networks with piecewise linear activation functions and first hidden layer wider than \(2N\), where \(N\) is the number of training data). The width requirement is later improved to \(N+1\)[24]. [18] prove the existence of a piece-wise linear path between two solutions for ReLU networks, if they are both dropout stable, or both noise stable and sufficiently overparametrized. [27] generalizes this proof to show that wider neural networks are more connected, following the observation that SGD solutions for wider neural network are more dropout stable. [25] gives a new upper bound of the loss barrier between solutions using the loss of sparse subnetworks that are optimized, which is a milder condition than dropout stability.

Symmetry in the loss landscapeDiscrete symmetries have inspired a line of work on loss landscape topology. [4] shows that permutations of a given layer are connected within a loss level set. Through examining the permutation symmetries, [28] characterize the geometry of the global minima manifold for networks without other symmetries and show that adding one neuron to each layer in a minimal network connects the permutation equivalent global minima. By removing permutation and rescaling symmetries, [26] study the geometry of minima in the functional space. [32] finds a set of nonlinear continuous symmetries that partially parametrizes the minimum. [33] uses symmetry induced curves to approximate the curvature of the minimum.

## Appendix B Background

In this section, we review mathematical concepts used in the paper and list some useful results on the number of connected components of topological spaces. We refer readers to [19] for a more detailed introduction to this topic.

### Connected components

Consider two topological spaces \(X\) and \(Y\). A map \(f:X\to Y\) is _continuous_ if for every open subset \(U\subseteq Y\), its preimage \(f^{-1}(U)\) is open in \(X\). If \(X\) and \(Y\) are metric spaces with metrics \(d_{X}\) and \(d_{Y}\) respectively, this is equivalent to the delta-epsilon definition. That is, \(f\) is continuous if at every \(x\in X\), for any \(\epsilon>0\) there exists \(\delta>0\) such that \(d_{X}(x,y)<\delta\) implies \(d_{Y}(f(x),f(y))<\epsilon\) for all \(y\in Y\).

A topological space is _connected_ if it cannot be expressed as the union of two disjoint, nonempty, open subsets. A topological space \(X\) is _path connected_ if for every \(p,q\in X\), there is a continuous map \(f:[0,1]\to X\) such that \(f(0)=p\) and \(f(1)=q\). Path connectedness implies connectedness, but the converse is not true [19]. [23] studies the path connectedness of sublevel sets of loss functions.

The following theorem is the main intuition of this paper and will appear frequently in proofs.

**Theorem B.1** (Theorem 4.7 in [19]).: _Let \(X,Y\) be topological spaces and let \(f:X\to Y\) be a continuous map. If \(X\) is connected, then \(f(X)\) is connected._

A map \(f\) is a _homeomorphism_ from \(X\) to \(Y\) if \(f\) is bijective and both \(f\) and \(f^{-1}\) are continuous. \(X\) and \(Y\) are _homeomorphic_ if such a map exists. A _(connected) component_ of a topological space \(X\) is a maximal nonempty connected subset of \(X\). The components of \(X\) form a partition of \(X\). The next two corollaries of Theorem B.1 show that connectedness and the number of connected components are topological properties. That is, they are preserved under homeomorphisms.

**Corollary B.2**.: _Let \(f:X\to Y\) be a homeomorphism from \(X\) to \(Y\), and let \(U\subseteq X\) be a subset of \(X\) with the subspace topology. Then \(U\) is connected if and only if \(f(U)\subseteq Y\) is connected._

Proof.: By the definition of homeomorphism, \(f\) and \(f^{-1}\) are continuous. From Theorem B.1, if \(U\in X\) is connected, then \(f(U)\in Y\) is connected. Similarly, if \(f(U)\) is connected, then \(f^{-1}(f(U))=U\) is connected. 

**Corollary B.3**.: _Let \(X\) be a topological space that has \(N\) components. Let \(Y\) be a topological space homeomorphic to \(X\). Then \(Y\) has \(N\) components._

Proof.: Let \(C_{1},...,C_{N}\) be the components of \(X\). Let \(f\) be a homeomorphism from \(X\) to \(Y\). Since \(f\) is bijective and \(C_{1},...,C_{N}\) is a partition of \(X\), \(f(C_{1}),...,f(C_{N})\) is a partition of \(Y\). From Theorem B.1, since \(C_{1},...,C_{N}\) are all connected, so are \(f(C_{1}),...,f(C_{N})\).

Lastly, we need to show that \(f(C_{1}),...,f(C_{N})\) are maximally connected. Suppose there exists a set \(U\subseteq Y\), such that \(U\not\subseteq f(C_{i})\) and \(f(C_{i})\cup U\) is connected for some \(i\). Then by Theorem B.1, \(f^{-1}(f(C_{i})\cup U)\supset C_{i}\) is connected in \(X\). This contradicts the fact that \(C_{i}\) is a maximal component in \(X\). Therefore, \(f(C_{1}),...,f(C_{N})\) are maximally connected.

Since \(f(C_{1}),...,f(C_{N})\) partitions \(Y\) and are maximally connected, \(Y\) has \(N\) components. 

Another consequence of Theorem B.1 is the following upper bound on the number of components of the image of a continuous map.

**Proposition B.4**.: _Let \(f:X\to Y\) be a continuous map. The number of components of the image \(f(X)\subseteq Y\) is at most the number of components of \(X\)._Proof.: Let \(C_{1},...,C_{N}\) be the components of \(X\). Since \(C_{i}\) is continuous and the action is continuous, according to Theorem B.1, \(f(C_{i})\) is continuous for all \(i\in\{1,...,N\}\). Additionally, since \(\bigcup_{i=1}^{N}C_{i}=X\), we have \(\bigcup_{i=1}^{N}f(C_{i})=f(X)\). Therefore, there is a surjective map from \(\{f(C_{1}),...,f(C_{N})\}\) to the set of components of \(f(X)\), which implies that \(f(X)\) has at most \(N\) components. 

Let \(X_{1},...,X_{n}\) be topological spaces. The _product space_ is their Cartesian product \(X_{1}\times...\times X_{n}\) endowed with the product topology. Denote \(\pi_{0}(X)\) as the set of connected components of a space \(X\). The following proposition provides a way to count the components of a product space.

**Proposition B.5**.: _Consider \(n\) topological spaces \(X_{1},...,X_{n}\). Then \(|\pi_{0}(X_{1}\times...\times X_{n})|=\prod_{i=0}^{n}|\pi_{0}(X_{i})|\)._

Proof.: When \(n=1\), the number of components of the product space is \(|\pi_{0}(X_{1})|\).

For the \(n>1\) case, since \(X_{1}\times...\times X_{n}=(X_{1}\times...\times X_{n-1})\times X_{n}\), it suffices to show that \(|\pi_{0}(A\times B)|=|\pi_{0}(A)||\pi_{0}(B)|\) for any topological spaces \(A\) and \(B\). Let \(f:\pi_{0}(A)\times\pi_{0}(B)\rightarrow\pi_{0}(A\times B)\) be the map that assigns \(C\in\pi_{0}(A)\times\pi_{0}(B)\) to the element in \(\pi_{0}(A\times B)\) that contains \(C\). Then \(f\) is surjective because \(\pi_{0}(A)\times\pi_{0}(B)\) forms a partition of \(A\times B\). To prove that \(f\) is injective, suppose that \(f(C_{1})=f(C_{2})\) for \(C_{1},C_{2}\in\pi_{0}(A)\times\pi_{0}(B)\). Consider the projection \(\pi_{A}:A\times B\to A\). Since \(\pi_{A}\) is continuous and \(C_{1},C_{2}\) belong to the same component of \(A\times B\), \(\pi_{A}(C_{1})\) and \(\pi_{A}(C_{2})\) belong to the same component of \(A\). Similarly, \(\pi_{B}(C_{1})\) and \(\pi_{B}(C_{2})\) belong to the same component of \(B\) under the projection \(\pi_{B}:A\times B\to B\). Since all components of \(A\) and \(B\) are maximally connected, we have \(C_{1}=C_{2}\), which implies that \(f\) is injective. Since \(f\) is a bijection from \(\pi_{0}(A)\times\pi_{0}(B)\) to \(\pi_{0}(A\times B)\), \(|\pi_{0}(A\times B)|=|\pi_{0}(A)||\pi_{0}(B)|\). 

### Groups

A _group_ is a set \(G\) together with a law of composition, that satisfies \((ab)c=a(bc)\)\(\forall a,b,c\in G\), \(\exists 1\) such that \(1a=a1=a\)\(\forall a\in G\), and \(\forall a\in G\), \(\exists b\) such that \(ab=ba=1\). An _action_ of a group \(G\) on a set \(S\) is a map \(\cdot:G\times S\to S\), that satisfies id \(\cdot s=s\) for all \(s\in S\) and \((gg^{\prime})\cdot s=g\cdot(g^{\prime}\cdot s)\) for all \(g,g^{\prime}\) in \(G\) and all \(s\) in \(S\). The _orbit_ of \(s\in S\) is the set \(O(s)=\{s^{\prime}\in S\mid s^{\prime}=gs\text{ for some }g\text{ in }G\}\).

A _topological group_ is a group \(G\) endowed with a topology such that multiplication and inverse are both continuous. A recurring example is the general linear group \(GL_{n}(\mathbb{R})\), with the subspace topology obtained from \(\mathbb{R}^{n^{2}}\). \(GL_{n}(\mathbb{R})\) has two connected components, which correspond to the preimages of the positive and negative reals under the determinant map.

The _product_ of groups \(G_{1},...,G_{n}\) is a group denoted by \(G_{1}\times...\times G_{n}\). The elements in \(G_{1}\times...\times G_{n}\) is the product set of \(G_{1},...,G_{n}\). The group structure is defined by identity \((1,...,1)\), inverse \((g_{1},...,g_{n})^{-1}=(g_{1}^{-1},...,g_{n}^{-1})\), and multiplication rule \((g_{1},...,g_{n})(g_{1}^{\prime},...,g_{n}^{\prime})=(g_{1}g_{1}^{\prime},...,g _{n}g_{n}^{\prime})\).

### Relating connectedness of groups, orbits, and level sets

From Theorem B.1, continuous maps preserve connectedness. Through continuous actions, we study the connectedness of orbits and level sets by relating them to the connectedness of more familiar objects such as the general linear group.

In the main text, we focus on the case of bijective actions. Here we only assume the action to be continuous and try to bound the number of components of the orbits. As an immediate consequence of Proposition B.4, an orbit cannot have more components than the group.

**Corollary B.6**.: _Assume that the action of a group \(G\) on \(S\) is continuous. Then the number of connected components of orbit \(O(s)\) is smaller than or equal to the number of connected components of \(G\), for all \(s\) in \(S\)._

Proof.: An orbit \(O(s)\) is the image of the group action, which we assume to be continuous. The result follows from Proposition B.4. 

Let \(X\) be a topological space and \(L:X\rightarrow\mathbb{R}\) a continuous function on \(X\). A topological group \(G\) is said to be a _symmetry group_ of \(L\) if \(L(g\cdot x)=L(x)\) for all \(g\in G\) and \(x\in X\). In this case, the action can be defined on a level set of \(L\), \(L^{-1}(c)\) with a \(c\in\mathbb{R}\), as \(G\times L^{-1}(c)\to L^{-1}(c)\). If the minimum of \(L\) consists of a single orbit, Corollary B.6 extends immediately to the number of components of the minimum.

**Corollary B.7**.: _Let \(L\) be a function with a symmetry group \(G\). If the minimum of \(L\) consists of a single \(G\)-orbit, then the number of connected components of the minimum is smaller or equal to the number of connected components of \(G\)._

Generally, symmetry groups do not act transitively on a level set \(L^{-1}(c)\in\mathbf{Param}\). In this case, the connectedness of the orbits does not directly inform the connectedness of the level set.

**Proposition B.8**.:
1. _There exists a space_ \(X\)_, a group_ \(G\)_, and an action of_ \(G\)_, such that each orbit for the action of_ \(G\) _is connected and_ \(X\) _is not connected._
2. _There exists a space_ \(X\)_, a group_ \(G\)_, and an action of_ \(G\)_, such that each orbit for the action of_ \(G\) _is disconnected and_ \(X\) _is connected._

Proof.: For part (a), consider a subspace of \(\mathbb{R}^{2}\), \(X=X_{1}\cup X_{2}\) where \(X_{1}=\{(x,y):x=0,y>0\}\) and \(X_{2}=\{(x,y):x=1,y>0\}\). The space \(X\) is not connected. Let \(G\) be the multiplicative group of positive real numbers and act on \(X\) by multiplication on the second coordinate. Then there are two orbits, \(X_{1}\) and \(X_{2}\), which are both connected.

For part (b), consider the space \(X=\mathbb{R}^{2}\setminus\{0\}\). Then \(X\) is connected. Let \(G\) be the multiplicative group of real numbers, which acts on \(X\) by multiplication on both coordinates. That is, \(g\cdot(x_{1},x_{2})=(gx,gx_{2}),\forall(x_{1},x_{2})\in X,\forall g\in G\). The orbit of any point \((x_{1},x_{2})\in X\) is not connected. 

Nevertheless, since the set of orbits partitions the space, we can use the following bound on the number of components of the space.

**Proposition B.9**.: _Let \(X\) be a topological space and let \(X=\coprod_{i}X_{i}\) be a partition of \(X\) into disjoint subspaces. Then \(|\pi_{0}(X)|\leq\sum_{i}|\pi_{0}(X_{i})|\)._

Proof.: Let \(S=\{A\subseteq X\,:\,\exists i,A\text{ is a component of }X_{i}\}\) be the union of the components of the subspaces. Then \(S\) is a partition of \(X\), and every element in \(S\) is connected. Therefore, there is a surjective map from \(S\) to \(\pi_{0}(X)\), defined by mapping each \(s\in S\) to the element of \(\pi_{0}(X)\) that includes \(s\). This implies that \(|\pi_{0}(X)|\leq|S|=\sum_{i=1}^{n}|\pi_{0}(X_{i})|\). 

Consider a topological space \(X\) and a group \(G\) that acts on \(X\). Let \(O=\{O_{1},...,O_{n}\}\) be the set of orbits. By Proposition B.9, the number of components of the orbits give the following upper bound on the number of components of the space: \(|\pi_{0}(X)|\leq\sum_{i=1}^{n}|\pi_{0}(O_{i})|\).

## Appendix C Missing Proofs

### Proof of Proposition 2.1

Proof.: Recall that \(W_{1},...,W_{n},X,Y\) are matrices in \(\mathbb{R}^{h\times h}\), and \(X,Y\) are both full rank. Consider the map

\[f:(\mathrm{GL}_{h})^{l-1}\to L^{-1}(0),\quad(g_{1},...,g_{l-1})\mapsto(g_{1} X^{-1},g_{2},...,g_{l-1},Y\prod_{i}^{l-1}g_{i}^{-1}).\] (7)

The inverse \(f^{-1}:(W_{1},...,W_{l})\mapsto(W_{1}X,W_{2},W_{3},...,W_{l-1})\) is well defined, because \(X\), \(W_{1},W_{2},W_{3},...,W_{l-1}\) are all full-rank. Since both \(f\) and \(f^{-1}\) are continuous, \(f\) is a homeomorphism between \((\mathrm{GL}_{h})^{l-1}\) and \(L^{-1}(0)\). 

### Proof of Corollary 2.2

Proof.: From Proposition \(2.1\), \(L^{-1}(0)\) is homeomorphic to \((\mathrm{GL}_{h})^{l-1}\). According to Corollary B.3, this implies that \(L^{-1}(0)\) has the same number of connected components as \((\mathrm{GL}_{h})^{l-1}\). From Proposition B.5, \(GL_{h}(\mathbb{R})^{l-1}\) has \(2^{l-1}\) connected components. Therefore, \(L^{-1}(0)\) has \(2^{l-1}\) connected components.

[MISSING_PAGE_FAIL:11]

### Proof of Lemma 3.1

Proof.: Consider the map \(f\) and its inverse \(f^{-1}\) defined in (7) in the proof of Proposition 2.1. Let \(g=f^{-1}(W_{1},W_{2})\) and \(g^{\prime}=f^{-1}(W^{\prime}_{1},W^{\prime}_{2})\). By Corollary B.2, since \((W_{1},W_{2})\) and \((W^{\prime}_{1},W^{\prime}_{2})\) are not in the same connected component of \(L^{-1}(0)\), \(g\) and \(g^{\prime}\) are not in the same connected component of \(GL_{h}\). Equivalently, \(det(gg^{\prime})<0\). Consider a \(g_{1}\in GL_{h}\) such that \(det(g)<0\). Then \(det(g_{1}gg^{\prime})>0\), which means that \(g_{1}g\) and \(g^{\prime}\) belong to the same connected component of \(GL_{h}\). Therefore, according to Corollary B.2, \(g_{1}\cdot(W_{1},W_{2})=f(g_{1}g)\) and \((W^{\prime}_{1},W^{\prime}_{2})=f(g^{\prime})\) belong to the same connected component of \(L^{-1}(0)\). 

### Proof of Proposition 3.2

Proof.: Let \((g_{1},...,g_{l-1}),(g^{\prime}_{1},...,g^{\prime}_{l-1})\in(GL_{h})^{n-1}\) such that \(f(g_{1},...,g_{l-1})=(W_{1},...,W_{l})\) and \(f(g^{\prime}_{1},...,g^{\prime}_{l-1})=(W^{\prime}_{1},...,W^{\prime}_{l})\). Let \(P_{0}=I\). For \(i=1,...,l-1\), if \(det(g_{i}g^{\prime}_{i}P^{-1}_{i-1})>0\), set \(P_{i}\) to \(I\). Otherwise, we set \(P_{i}\) to an arbitrary element in \(P\in S_{h}\setminus A_{h}\), which is not empty when \(h\geq 2\).

Let \((g^{\prime\prime}_{1},...,g^{\prime\prime}_{l-1})\in(GL_{h})^{n-1}\) such that \(f(g^{\prime\prime}_{1},...,g^{\prime\prime}_{l-1})=(W_{1}P_{1},P^{-1}_{1}W_{2} P_{2},...,P_{l-2}W_{l-1}P_{l-1},\)\(P_{l-1}W_{l})\). By the way we construct \(P_{i}\)'s, we have \(g^{\prime\prime}_{i}=P^{-1}_{i-1}g^{\prime}_{i}P_{i}\) and \(det(g_{i}g^{\prime\prime}_{i})>0\). Therefore, \(g_{i}\) and \(g^{\prime\prime}_{i}\) belong to the same connected component of \((GL_{h})^{l-1}\) for all \(i\). Since \(f\) is a homeomorphism between \((\mathrm{GL}_{h})^{l-1}\) and \(L^{-1}(0)\), \((W_{1}P_{1},P^{-1}W_{2}P_{2},...,P_{l-2}W_{l-1}P_{l-1},P_{l-1}W_{l})\) and \((W^{\prime}_{1},...,W^{\prime}_{l})\) are connected in \(L^{-1}(0)\). 

### Proof of Proposition 3.3

Proof.: Let \((W_{2},W_{1})\in L^{-1}(0)\) be an arbitrary point on the minimum of \(L\). Let \(\alpha=0.5\), \(m=\frac{4\sqrt{k}}{\|Y\|_{2}}+2\), and \((W^{\prime}_{2},W^{\prime}_{1})=(W_{2}m^{-1},mW_{1})\). Then

\[L((1-\alpha)W_{1}+\alpha W^{\prime}_{1},(1-\alpha)W_{2}+\alpha W ^{\prime}_{2})\] \[= ||Y-((1-\alpha)W_{2}+\alpha W^{\prime}_{2})((1-\alpha)W_{1}+ \alpha W^{\prime}_{1})X||^{2}_{2}\] \[= ||Y-(1-\alpha)^{2}W_{2}W_{1}X-\alpha^{2}W^{\prime}_{2}W^{\prime} _{1}X-(1-\alpha)\alpha(m+m^{-1})W_{2}W_{1}X||^{2}_{2}\] \[= ||(1-(1-\alpha)^{2}-\alpha^{2}-(1-\alpha)\alpha(m+m^{-1}))Y||^{2}_ {2}\] \[= ||(2\alpha(1-\alpha)-\alpha(1-\alpha)(m+m^{-1}))Y||^{2}_{2}\] \[= ||(\alpha(1-\alpha)(2-m-m^{-1})Y||^{2}_{2}\] \[= (2-m-m^{-1})^{2}0.25^{2}||Y||^{2}_{2}\]

Note that \(\alpha(1-\alpha)=0.25\) and \(m+m^{-1}>2\). Substitute in \(m\), we have

\[L((1-\alpha)W_{1}+\alpha W^{\prime}_{1},(1-\alpha)W_{2}+\alpha W^{\prime}_{2}) >||(2-m)^{2}0.25^{2}||Y||^{2}_{2}=k.\] (11)