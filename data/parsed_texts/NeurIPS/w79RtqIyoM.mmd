# Compositional Sculpting of

Iterative Generative Processes

 Timur Garipov\({}^{1}\)

Correspondence to Timur Garipov (timur@csail.mit.edu).

Sebastiaan De Peuter\({}^{2}\)

Ge Yang\({}^{1,4}\)

Vikas Garg\({}^{2,5}\)

Samuel Kaski\({}^{2,3}\)

Tommi Jaakkola\({}^{1}\)

\({}^{1}\)MIT CSAIL \({}^{2}\)Aalto University \({}^{3}\)University of Manchester

\({}^{4}\)Institute for Artificial Intelligence and Fundamental Interactions \({}^{5}\)YaiYai Ltd

###### Abstract

High training costs of generative models and the need to fine-tune them for specific tasks have created a strong interest in model reuse and composition. A key challenge in composing iterative generative processes, such as GFlowNets and diffusion models, is that to realize the desired target distribution, all steps of the generative process need to be coordinated, and satisfy delicate balance conditions. In this work, we propose Compositional Sculpting: a general approach for defining compositions of iterative generative processes. We then introduce a method for sampling from these compositions built on classifier guidance. We showcase ways to accomplish compositional sculpting in both GFlowNets and diffusion models. We highlight two binary operations -- the harmonic mean (\(p_{1}\otimes p_{2}\)) and the contrast (\(p_{1}\)) between pairs, and the generalization of these operations to multiple component distributions. We offer empirical results on image and molecular generation tasks. Project codebase: https://github.com/timgaripov/compositional-sculpting.

## 1 Introduction

Large-scale general-purpose pre-training of machine learning models has produced impressive results in computer vision [1; 2; 3], image generation [4; 5; 6], natural language processing [7; 8; 9; 10; 11], robotics [12; 13; 14], and basic sciences [15]. By distilling vast amounts of data, such models can produce powerful inferences that lead to emergent capabilities beyond the specified training objective [16]. However, generic pre-trained models are often insufficient for specialized tasks in engineering and basic sciences. Field-adaptation via techniques such as explicit fine-tuning on bespoke datasets [17], human feedback [18], or cleverly designed prompts [19; 20] is therefore often required. Alternatively, capabilities of pre-trained models can be utilized and extended via model composition.

Compositional generation [21; 22; 23; 24; 25; 26; 27; 28] views a complex target distribution in terms of simpler pre-trained building blocks which can be mixed and matched into a tailored solution to a specialized task. Given a set of base models capturing different properties of the data, composition provides a way to fuse these models into a single composite model with capacity greater than any individual base model. In this way it allows one to specify distributions over examples that exhibit multiple desired properties simultaneously [22]. The need to construct complex distributions adhering to multiple constraints arises in numerous practical multi-objective design problems such as molecule generation [29; 30; 31]. In this context, compositional modeling provides mechanisms for control of the resulting distribution and exploration of different trade-offs between the objectives and constraints.

Prior work on generative model composition [21; 22; 24] has developed operations for piecing together Energy-Based Models (EBMs) via algebraic manipulations of their energy functions. For example, consider two distributions \(p_{1}(x)\propto\exp(-E_{1}(x))\) and \(p_{2}(x)\propto\exp(-E_{2}(x))\) induced by energyfunctions \(E_{1}\) and \(E_{2}\). Their _product_\(p_{\text{prod}}(x)\propto p_{1}(x)p_{2}(x)\propto\exp(-(E_{1}(x)+E_{2}(x)))\) and _negation_\(p_{\text{neg}}(x)\propto p_{1}(x)/(p_{2}(x))^{\gamma}\propto\exp(-(E_{1}(x)- \gamma\,E_{2}(x)))\), \(\gamma>0\) correspond to operations on the underlying energy functions. The product assigns high likelihood to points \(x\) that have high likelihood under both base distributions but assigns low likelihood to points that have close-to-zero likelihood under one (or both). The negation distribution assigns high likelihood to points that are likely under \(p_{1}\) but unlikely under \(p_{2}\) and assigns low likelihood to points that are likely under \(p_{2}\) but unlikely under \(p_{1}\).

Iterative generative processes including diffusion models [32, 33, 5, 34] and GFlowNets [35, 36] progressively refine coarse objects into cleaner ones over multiple steps. Realizing effective compositions of these models is complicated by the fact that simple alterations in their generation processes result in non-trivial changes in the distributions of the final objects. Unlike for EBMs, products and negations of diffusion models cannot be realized through simple algebraic operations on their score functions. Du et al. [28] show that the result of the addition of score functions is not equal to the score of the diffused product distribution and develop a method that corrects the sum-of-scores sampling via additional MCMC steps nested under each step of the diffusion time loop.

Jain et al. [31] develop Multi-Objective GFlowNets (MOGFNs), an extension of GFlowNets for multi-objective optimization tasks. While a vanilla GFlowNet captures a distribution induced by a single reward (objective) function \(p(x)\propto R(x)\) (see Section 2 for details), an MOGFN aims to learn a single conditional model that can realize distributions corresponding to various combinations (e.g. a convex combination) of multiple reward functions. Though a single MOGFN realizes a spectrum of compositions of base reward functions, the approach assumes access to the base rewards at training time. Moreover, MOFGNs require the set of possible composition operations to be specified at training time. In this work, we address post hoc composition of pre-trained GFlowNets (or diffusion models) and provide a way to create compositions that need not be specified in advance.

In this work, we introduce Compositional Sculpting, a general approach for the composition of pre-trained models. We highlight two special examples of binary operations -- _harmonic mean_: \((p_{1}\bigotimes p_{2})\) and _contrast_: \((p_{1}\bigotimes p_{2})\). More general compositions are obtained as conditional distributions in a probabilistic model constructed on top of pre-trained base models. We show that these operations can be realized via classifier guidance. We provide results of empirical verification of our method on molecular generation (with GFlowNets) and image generation (with diffusion models).

## 2 Background

Generative flow networks (GFlowNets).GFlowNets [35, 36] are an approach for generating structured objects (e.g. graphs) from a discrete space \(\mathcal{X}\). Given a "reward function" \(R(x)\geq 0\), a GFlowNet seeks to sample from \(p(x)=R(x)/Z\), where \(Z=\sum_{x}R(x)\), i.e. the model assigns larger probabilities to high-reward objects.

Starting at a fixed initial state \(s_{0}\), objects \(x\) are generated through a sequence of changes corresponding to a trajectory of incomplete states \(\tau=(s_{0}\to s_{1}\to...\to s_{n-1}\to x)\). The structure of possible trajectories corresponds to by a DAG \((\mathcal{S},\mathcal{A})\) where \(\mathcal{S}\) is a set of states (both complete and incomplete) and \(\mathcal{A}\) is the set of directed edges (actions) \(s\to s^{\prime}\). The set of complete objects (terminal states) \(\mathcal{X}\) is a subset of \(S\). The generation process starts at \(s_{0}\) and follows a parameterized stochastic "forward policy" \(P_{F}(s^{\prime}|s;\theta)\) which for each state \(s\in\mathcal{S}\setminus\mathcal{X}\) specifies a probability distribution over all possible successor states \(s^{\prime}:(s\to s^{\prime})\in\mathcal{A}\). The process terminates once a terminal state is reached.

Figure 1: **Composition operators.** (a,b) base distributions \(p_{1}\) and \(p_{2}\). (c) harmonic mean of \(p_{1}\) and \(p_{2}\). (d) contrast of \(p_{1}\) with \(p_{2}\) (e) reverse contrast \(p_{1}\bigotimes p_{2}\). Lines show the contours of the PDF level sets.

Diffusion models.Diffusion models [32; 33; 34; 5; 37] are a family of generative models developed for continuous domains. Given an empirical (data) distribution \(\hat{p}(x)\!=\!\nicefrac{{1}}{{n}}\sum_{i}\delta_{\hat{x}_{i}}(x)\) in \(\mathcal{X}=\mathbb{R}^{d}\), diffusion models seek to approximate \(\hat{p}(x)\) via a generative process \(p(x)\).

A diffusion process is a noising process that gradually destroys the original "clean" data \(x\). Viewed as a stochastic differential equation (SDE) [34], it is a time-indexed collection of random variables \(\{x_{i}\}_{t=0}^{T}\) in \(\mathcal{X}=\mathbb{R}^{d}\) which interpolates between the data distribution \(p_{0}(x)=\hat{p}(x)\) at \(t=0\) and a prior distribution \(p_{T}(x)\) at \(t=T\). The evolution of \(x_{t}\) is described by the "forward SDE" \(dx_{t}=f_{t}(x_{t})\,dt+g_{t}\,dw_{t}\) with drift coefficient \(f_{t}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) and diffusion coefficient \(g_{t}\in\mathbb{R}\). Here, \(w_{t}\) is the standard Wiener process. Crucially, the coefficients \(f_{t}\), \(g_{t}\) are generally chosen such that the prior \(p_{T}\) and the transition probabilities \(p_{st}(x_{t}|x_{s})\), \(0\leq s<t\leq T\) have a closed form (see [34]).

Song et al. [34] invoke a result from the theory of stochastic processes [38] which gives the expression for the reverse-time process or "backward SDE": \(dx_{t}=\left[f_{t}(x_{t})-g_{t}^{2}\nabla_{x}\log p_{t}(x_{t})\right]dt+g_{t} \,d\overline{w}_{t}\),where \(\overline{w}_{t}\) is the standard Wiener process in reversed time. This SDE involves the known coefficients \(f_{t}\), \(g_{t}\) and the unknown score function \(\nabla_{x}\log p_{t}(\cdot)\) of the marginal distribution \(p_{t}(\cdot)\) at time \(t\). A "score network" \(s_{t}(x;\theta)\) (a deep neural network with parameters \(\theta\)) is trained to approximate \(\nabla_{x}\log p_{t}(x)\). Once \(s_{t}(\cdot;\theta)\) is trained, sampling reduces to numerical integration of the backward SDE.

Classifier guidance in diffusion models.Classifier guidance [32; 39] is a technique for controllable generation in diffusion models. Suppose that each example \(x_{0}\) is accompanied by a discrete class label \(y\). The goal is to sample from the conditional distribution \(p_{0}(x_{0}|y)\). The Bayes rule \(p_{t}(x_{t}|y)\propto p_{t}(x_{t})p_{t}(y|x_{t})\) implies the score-function decomposition \(\nabla_{x_{t}}\log p_{t}(x_{t}|y)=\nabla_{x_{t}}\log p_{t}(x_{t})+\nabla_{x_{t }}\log p_{t}(y|x_{t})\), where the first term is already approximated by a pre-trained unconditional diffusion model and the second term can be derived from a time-dependent classifier \(p_{t}(y|x_{t})\). Therefore, the stated goal can be achieved by first training the classifier \(p_{t}(y|x_{t})\) using noisy samples \(x_{t}\) from the intermediate steps of the process, and then plugging in the expression for the conditional score into the backward SDE sampling process [34].

## 3 Related Work

Generative model composition.In Section 1 we reviewed prior work on energy-based composition operations and Multi-Objective GFlowNets. Learning mixtures of Generative Adversarial Networks has been addressed in [40], where the mixture components are learned simultaneously, and in [41], where the components are learned one by one in an adaptive boosting fashion. Algorithms for additive and multiplicative boosting of generative models have been developed in [42].

This work focuses on the composition of pre-trained models. Assuming that each pre-trained model represents the distribution of examples demonstrating certain concepts (e.g. molecular properties), the composition of models is equivalent to concept composition (e.g. property "A" AND property "B"). The inverse problem is known as "unsupervised concept discovery", where the goal is to automatically discover composable concepts from data. Unsupervised concept discovery and concept composition methods have been proposed for energy-based models [25] and text-to-image diffusion models [43].

Controllable generation.Generative model composition is a form of post-training control of the generation process - an established area of research. A simple approach to control is training a conditional generative model \(p(x|c)\) on pairs \((x,c)\) of objects \(x\) and conditioning information \(c\). Annotations \(c\) can be class labels [39], text prompts [4; 6; 44], semantic maps, and images [4]. Different from out work, this assumes that generation control operations are specified at training time. Dhariwal and Nichol [39] apply classifier guidance [32] on top of (conditional) diffusion models to improve their fidelity. Ho and Salimans [45] develop classifier-free guidance by combining conditional and unconditional score functions. In ControlNet [17], an additional network is trained to enable a pre-trained diffusion model to incorporate previously unavailable conditioning information. Meng et al. [46] and Couairon et al. [47] develop semantic image editing methods which first partially noise and then denoise an image to generate an edited version, possibly conditioned on a segmentation mask [47]. Similar to conditional diffusion models, conditional GFlowNets have been used to condition generation on reward exponents [36] or combinations of multiple predefined reward functions [31]. Note that the methods developed in this work can be combined with conditional diffusion models and GFlowNets: \(p(x|c_{1})\),..., \(p(x|c_{m})\) can act as base generative models to be composed.

Compositional generalization.The notion of compositionality has a broad spectrum of interpretations across a variety of disciplines including linguistics, cognitive science, and philosophy. Hupkes et al. [48] collect a list of aspects of compositionality from linguistic and philosophical theories and design practical tests for neural language models. Conwell and Ullman [49] empirically examine the relational understanding of DALL-E 2 [50], a text-guided image generation model, and point out limitations in the model's ability to capture relations such as "in", "on", "hanging over", etc. In this work, we focus on a narrow but well-defined type of composition where we seek to algebraically compose probability densities in a controllable fashion, such that we can emphasize or de-emphasize regions in the data space where specific base distributions have high density.

Connections between GFlowNets and diffusion models.Our method is applicable to compositions of both GFlowNets and diffusion models. This is due to deep connections between these two model families. GFlowNets were initially developed for generating discrete (structured) data [36] and diffusion models were initially developed for continuous data [5; 32]. Lahlou et al. [51] develop an extension of GFlowNets for DAGs with continuous state-action spaces. Zhang et al. [52] point out unifying connections between GFlowNets and other generative model families, including diffusion models. In this work, we articulate another aspect of the relation between GFlowNets and diffusion models: in Section 5.2 we derive the expressions for mixture GFlowNet policies and classifier-guided GFlowNet policies analogous to those derived for diffusion models in [32; 39; 53; 54].

## 4 Compositional Sculpting of Generative Models

Suppose we can access a number of pre-trained generative models \(\{p_{i}(x)\}_{i=1}^{m}\) over a common domain \(\mathcal{X}\). We may wish to compose these distributions such that we can, say, draw samples that are likely to arise from \(p_{1}(x)\) and \(p_{2}(x)\), or that are likely to arise from \(p_{1}(x)\) but not from \(p_{2}(x)\). In other words, we wish to specify a distribution that we can shape to emphasize and de-emphasize specific base models.

### Binary Composition Operations

Let us first focus on composing two base models. We could specify the composition as a weighted sum \(\widetilde{p}(x)=\sum_{j=1}^{2}\omega_{j}p_{i}(x)\) with weights \(\omega_{1},\omega_{2}\geq 0\) summing to one. The weights determine the prevalence of each base model in the composition, but beyond that our control is limited. We cannot emphasize regions where \(p_{1}\) and \(p_{2}\) both have high density, or de-emphasize regions where \(p_{2}\) has high density.

An alternative is to use conditioning to shape a prior \(\widetilde{p}(x)\) based on the base models. When we condition \(x\) on some observation \(y_{1}\), the resulting posterior takes the form \(\widetilde{p}(x|y_{1})\propto\widetilde{p}(y_{1}|x)\widetilde{p}(x)\). Points \(x\) that match \(y_{1}\) according to \(\widetilde{p}(y_{1}|x)\) will have increased density, and the density of points that do not match it decreases. Intuitively, by defining \(y_{1}\in\{1,2\}\) as the event that \(x\) was generated by a specific base model, we can shape a prior \(\widetilde{p}(x)\) based on the densities of the base models. To this end we define a uniform prior over \(y_{k}\) and define the conditional density \(p(x|y_{1}=i)\) to represent the fact that \(x\) was generated from \(p_{i}(x)\). This gives us the following model:

\[\widetilde{p}(x|y_{1}=i)=p_{i}(x),\quad\widetilde{p}(y_{1}=i)=1/2,\quad \widetilde{p}(x)=\widetilde{p}(x|y_{1}=1)\widetilde{p}(y_{1}=1)+\widetilde{p} (x|y_{1}=2)\widetilde{p}(y_{1}=2).\] (1)

Under this model, the prior \(\widetilde{p}(x)\) is a uniform mixture of the base models. The likelihood of \(y_{1}\)

\[\widetilde{p}(y_{1}=1|x)=1-\widetilde{p}(y_{1}=2|x)=p_{1}(x)/(p_{1}(x)+p_{2}( x)),\] (2)

implied by this model tells us how likely it is that \(x\) was generated by \(p_{1}(x)\) rather than \(p_{2}(x)\). In fact, it corresponds to the output of an optimal classifier trained to tell \(p_{1}(x)\) and \(p_{2}(x)\) apart.

Our goal is to realize compositions which generate samples likely to arise from both \(p_{1}(x)\) and \(p_{2}(x)\) or from \(p_{1}(x)\) but not \(p_{2}(x)\). Thus we introduce a second observation \(y_{2}\in\{1,2\}\) such that \(y_{1}\) and \(y_{2}\) are independent and identically distributed given \(x\). The resulting model and inferred posterior are:

\[\widetilde{p}(x,y_{1},y_{2})=\widetilde{p}(x)\prod_{k=1}^{2}\widetilde{p}(y_{ k}|x),\ \ \widetilde{p}(x)=\frac{1}{2}p_{1}(x)+\frac{1}{2}p_{2}(x),\ \ \widetilde{p}(y_{k}=i|x)=p_{i}(x)/(p_{1}(x)+p_{2}(x)),\] (3)

\[\widetilde{p}(x|y_{1}=i,\,y_{2}=j)\propto\widetilde{p}(x)\widetilde{p}(y_{1}= i|x)\widetilde{p}(y_{2}=j|x)\propto p_{i}(x)p_{j}(x)/(p_{1}(x)+p_{2}(x)).\] (4)

The above posterior shows clearly how conditioning on observations \(y_{1}=i,\,y_{2}=j\) has shaped the prior mixture to accentuate regions in the posterior where the observed base models \(i,j\) have high density.

Conditioning on observations \(y_{1}=1\) and \(y_{2}=2\), or equivalently \(y_{1}=2,y_{2}=1\), results in the posterior

\[(p_{1}\otimes p_{2})(x):=p(x|y_{1}=1,y_{2}=2)\propto p_{1}(x)p_{2}(x)/(p_{1}(x) +p_{2}(x)).\] (5)

We refer to this posterior as the "**harmonic mean** of \(p_{1}\) and \(p_{2}\)", and denote it as a binary operation \(p_{1}\otimes p_{2}\). Its value is high only at points that have high likelihood under both \(p_{1}(x)\) and \(p_{2}(x)\) at the same time (Figure 1(c)). Thus, the harmonic mean is an alternative to the product operation for EBMs. The harmonic mean is commutative (\(p_{1}\otimes p_{2}=p_{2}\otimes p_{1}\)) and is undefined when \(p_{1}\) and \(p_{2}\) have disjoint supports, since then the RHS of (5) is zero everywhere.

Conditioning on observations \(y_{1}=1\) and \(y_{2}=1\) results in the posterior

\[(p_{1}\otimes p_{2})(x):=\widetilde{p}(x|y_{1}=1,y_{2}=1)\propto(p_{1}(x))^{2 }/(p_{1}(x)+p_{2}(x)).\] (6)

We refer to this binary operation, providing an alternative to the negation operation in EBMs, as the "**contrast** of \(p_{1}\) and \(p_{2}\)", and denote it as \(p_{1}\otimes p_{2}\). The ratio (6) is high when \(p_{1}(x)\) is high and \(p_{2}(x)\) is low (Figure 1(d)). The contrast is not commutative (\(p_{1}\otimes p_{2}\neq p_{2}\otimes p_{1}\), unless \(p_{1}=p_{2}\)). We denote the reverse contrast as \(p_{1}\otimes p_{2}=p_{2}\otimes p_{1}\). Appendix C provides a detailed comparison between the contrast and negation operations, and between the harmonic mean and product operations.

Controlling the individual contributions of \(p_{1}\) and \(p_{2}\) to the composition.In order to provide more control over the extent of individual contributions of \(p_{1}\) and \(p_{2}\) to the composition, we modify model (3). Specifically, we introduce an interpolation parameter \(\alpha\) and change the likelihood of observation \(y_{2}\) in (3): \(\widetilde{p}(y_{2}=i|x;\alpha)=(\alpha p_{1}(x))^{|i=1}\cdot((1-\alpha)p_{2} (x))^{|i=2}/\left(\alpha p_{1}(x)+(1-\alpha)p_{2}(x)\right)\), where \(\alpha\in(0,1)\) and \([\cdot]\) denotes the indicator function. Conditional distributions in this model give **harmonic interpolation2** and **parameterized contrast**:

Footnote 2: the harmonic interpolation approaches \(p_{1}\) when \(\alpha\to 0\) and \(p_{2}\) when \(\alpha\to 1\)

\[(p_{1}\otimes_{(1-\alpha)}p_{2})(x)\propto\frac{p_{1}(x)p_{2}(x)}{\alpha p_{1 }(x)+(1-\alpha)p_{2}(x)},\quad(p_{1}\otimes_{(1-\alpha)}p_{2})(x)\propto\frac {(p_{1}(x))^{2}}{\alpha p_{1}(x)+(1-\alpha)p_{2}(x)}.\] (7)

Operation chaining.As the operations we have introduced result in proper distributions, we can create new \(N\)-ary operations by chaining binary (and \(N\)-ary) operations together. For instance, chaining binary harmonic means gives the harmonic mean of three distributions

\[((p_{1}\otimes p_{2})\otimes p_{3})(x)=(p_{1}\otimes(p_{2}\otimes p_{3}))(x) \propto\frac{p_{1}(x)p_{2}(x)p_{3}(x)}{p_{1}(x)p_{2}(x)+p_{1}(x)p_{3}(x)+p_{2} (x)p_{3}(x)}.\] (8)

### Compositional Sculpting: General Approach

The above approach for realizing compositions of two base models can be generalized to compositions of \(m\) base models \(p_{1}(x),\ldots,p_{m}(x)\) controlled by \(n\) observations. Though operator chaining can also realize compositions of \(m\) base models, our generalized method allows us to specify compositions more flexibly and results in different compositions. We introduce an augmented probabilistic model \(\widetilde{p}(x,y_{1},\ldots,y_{n})\) as a joint distribution over the original objects \(x\in\mathcal{X}\) and \(n\) observation variables \(y_{1}\in\mathcal{Y},\ldots,y_{n}\in\mathcal{Y}\) where \(\mathcal{Y}=\{1,\ldots,m\}\). By defining appropriate conditionals \(p(y_{k}|x)\) we can controllably shape a prior \(\widetilde{p}(x)\) into a posterior \(\widetilde{p}(x|y_{1},\ldots,y_{n})\).

As in the binary case, we propose to use a uniformly-weighted mixture of the base models \(\widetilde{p}(x)=(1/n)\cdot\sum_{i=1}^{m}p_{i}(x)\). The support of this mixture is the union of the supports of the base models: \(\bigcup_{i=1}^{m}\operatorname{supp}\{p_{i}(x)\}=\operatorname{supp}\{ \widetilde{p}(x)\}\). This is essential as the prior can only be shaped in places where it has non-zero density. As before we define the conditionals \(p(y_{k}=i|x)\) to correspond to the observation that \(x\) was generated by base model \(i\). The resulting full model is

\[\widetilde{p}(x,y_{1},\ldots,y_{n})=\widetilde{p}(x)\prod_{k=1}^{n}\widetilde{ p}(y_{k}|x),\ \widetilde{p}(x)=\frac{1}{m}\sum_{i=1}^{m}p_{i}(x),\ \widetilde{p}(y_{k}=i)=\frac{1}{m},\ \widetilde{p}(y_{k}=i|x)=\frac{p_{i}(x)}{\sum_{j=1}^{m}p_{j}(x)},\] (9)

Note that under this model the mixture can be represented as \(\widetilde{p}(x)=\sum_{y_{k}=1}^{m}\widetilde{p}(x|y_{k})\widetilde{p}(y_{k})\) for any \(k\).

The inferred posterior over \(x\) for this model is

\[\widetilde{p}(x|y_{1}=i_{1},\ldots,y_{n}=i_{n}) \propto\widetilde{p}(x)\widetilde{p}(y_{1}=i_{1},\ldots,y_{n}=i_{n }|x)\] (10) \[\propto\widetilde{p}(x)\prod_{k=1}^{n}\widetilde{p}(y_{k}=i_{k}|x )\propto\left(\prod_{k=1}^{n}p_{i_{k}}(x)\right)\bigg{/}\left(\sum_{j=1}^{m}p_ {j}(x)\right)^{n-1}.\] (11)The posterior \(\widetilde{p}(x|y_{1}=i_{1},\ldots,y_{n}=i_{n})\) is a composition of distributions \(\{p_{i}(x)\}_{i=1}^{m}\) that can be adjusted by choosing values for \(y_{1},\ldots,y_{n}\). By adding or omitting an observation \(y_{k}=i\) we can _sculpt_ the posterior to our liking, emphasizing or de-emphasizing regions of \(\mathcal{X}\) where \(p_{i}\) has high density. The observations can be introduced with multiplicities (e.g., \(y_{1}=1,y_{2}=1,y_{3}=2\)) to further strengthen the effect. Moreover, one can choose to introduce all observations simultaneously as in (10) or sequentially as in (11). As we show below (Section 5.1 for GFlowNets; Appendix A.2 for diffusion models), the composition (10) can be realized by a sampling policy that can be expressed as a function of the pre-trained (base) sampling policies.

Special instances and general formulation.The general approach outlined in this section is not limited to choices we made to construct the model in equation (9), i.e. \(\widetilde{p}(x)\) does not have to be a uniformly weighted mixture of the base distributions, \(y_{1},\ldots,y_{n}\) do not have to be independent and identically distributed given \(x\), and different choices of the likelihood \(\widetilde{p}(y=i|x)\) are possible. For instance, in the model for parameterized operations (7) the likelihoods of observations \(\widetilde{p}(y_{1}|x)\), \(\widetilde{p}(y_{2}|x)\) differ.

## 5 Compositional Sculpting of Iterative Generative Processes

In this Section, we show how to apply the model above to compose GFlowNets, and how one can use classifier guidance to sample from the composition. The similar method for diffusion model composition is described in Appendix A.2.

### Composition of GFlowNets

Besides a sample \(x\) from \(p_{i}(x)\), a GFlowNet also generates a trajectory \(\tau\) which ends in the state \(x\). Thus, we extend the model \(\widetilde{p}(x,y_{1},\ldots,y_{n})\), described above, and introduce \(\tau\) as a variable with conditional distribution \(\widetilde{p}(\tau|y_{k}=i)=\prod_{t=0}^{|\tau|-1}p_{i,F}(s_{t+1}|s_{t})\), where \(p_{i,F}\) is the forward policy of the GFlowNet that samples from \(p_{i}\).

Our approach for sampling from the composition is conceptually simple. Given \(m\) base GFlowNets that sample from \(p_{1},\ldots,p_{m}\) respectively, we start by defining the prior \(\widetilde{p}(x)\) as the uniform mixture of these GFlowNets. Proposition 5.1 shows that this mixture can be realized by a policy constructed from the forward policies of the base GFlowNets. We then apply classifier guidance to this mixture to sample from the composition. Proposition 5.2 shows that classifier guidance results in a new policy which can be constructed directly from the GFlowNet being guided.

**Proposition 5.1** (GFlowNet mixture policy).: _Suppose distributions \(p_{1}(x),\ldots,p_{m}(x)\) are realized by GFlowNets with forward policies \(p_{1,F}(\cdot),\ldots,p_{m,F}(\cdot|\cdot)\). Then, the mixture distribution \(p_{M}(x)=\sum_{i=1}^{m}\omega_{i}p_{i}(x)\) with \(\omega_{1},\ldots,\omega_{m}\geq 0\) and \(\sum_{i=1}^{m}\omega_{i}=1\) is realized by the GFlowNet forward policy_

\[p_{M,F}(s^{\prime}|s)=\sum_{i=1}^{m}p(y=i|s)p_{i,F}(s^{\prime}|s),\] (12)

_where \(y\) is a random variable such that the joint distribution of a GFlowNet trajectory \(\tau\) and \(y\) is given by \(p(\tau,y=i)=\omega_{i}p_{i}(\tau)\) for \(i\in\{1,\ldots,m\}\)._

**Proposition 5.2** (GFlowNet classifier guidance).: _Consider a joint distribution \(p(x,y)\) over a discrete space \(\mathcal{X}\times\mathcal{Y}\) such that the marginal \(p(x)\) is realized by a GFlowNet with forward policy \(p_{F}(\cdot|\cdot)\). Assume that the joint distribution of \(x\), \(y\), and GFlowNet trajectories \(\tau=(s_{0}\ldots s_{n}=x)\) decomposes as \(p(\tau,x,y)=p(\tau,x)p(y|x)\), i.e. \(y\) is independent of the intermediate states \(\{s_{i}\}_{i=0}^{n-1}\) in \(\tau\) given \(x\). Then,_

1. _For all non-terminal nodes_ \(s\in S\setminus\mathcal{X}\)_, the probabilities_ \(p(y|s)\) _satisfy_ \[p(y|s)=\sum_{s^{\prime}:(s=s^{\prime})\in\mathcal{A}}p_{F}(s^{\prime}|s)p(y|s^{ \prime}).\] (13)
2. _The conditional distribution_ \(p(x|y)\) _is realized by the classifier-guided policy_ \[p_{F}(s^{\prime}|s,y)=p_{F}(s^{\prime}|s)\cdot p(y|s^{\prime})\big{/}\,p(y|s).\] (14)

Note that (13) ensures that \(\sum_{s^{\prime}:(s\to s^{\prime})\in\mathcal{A}}p_{F}(s^{\prime}|s,y)=1\).

Proposition 5.1 is analogous to results on mixtures of diffusion models (Theorem 1 of Peluchetti [53], Theorem 1 of Lipman et al. [54]). Proposition 5.2 is analogous to classifier guidance for diffusion models [32, 39]. To the best of our knowledge, our work is the first to derive both results for GFlowNets.

Theorem 5.3 summarizes our approach. The propositions and the theorem are proved in Appendix D.

**Theorem 5.3**.: _Suppose distributions \(p_{1}(x),\ldots,p_{m}(x)\) are realized by GFlowNets with forward policies \(p_{1,F}(\cdot|\cdot)\),..., \(p_{m,F}(\cdot|\cdot)\) respectively. Let \(y_{1},\ldots,y_{n}\) be random variables defined by (9). Then, the conditional \(\widetilde{p}(x|y_{1},\ldots,y_{n})\) is realized by the forward policy_

\[p_{F}(s^{\prime}|s,y_{1},\ldots,y_{n})=\frac{\widetilde{p}(y_{1},\ldots,y_{n} |s^{\prime})}{\widetilde{p}(y_{1},\ldots,y_{n}|s)}\sum_{i=1}^{m}p_{i,F}(s^{ \prime}|s)\widetilde{p}(y\!=\!i|s)\] (15)

Note that the result of conditioning on observations \(y_{1},\ldots,y_{n}\) is just another GFlowNet policy. Therefore, to condition on more observations, one can apply classifier guidance repeatedly.

### Classifier Training (GFlowNets)

The evaluation of policy (15) requires knowledge of the probabilities \(\widetilde{p}(y_{1},\ldots,y_{n}|s)\), \(\widetilde{p}(y|s)\). These probabilities can be estimated by a classifier fitted to trajectories sampled from the base GFlowNets.

Let \(\widetilde{Q}_{\phi}(y_{1},\ldots,y_{n}|s)\) be a classifier with parameters \(\phi\) that we wish to train to approximate the ground-truth conditional \(\widetilde{p}(y_{1},\ldots,y_{n}|s)\). Under model (9), \(y_{1},\ldots,y_{n}\) are dependent given a state \(s\in S\setminus\mathcal{X}\), but, are independent given a terminal state \(x\in\mathcal{X}\). This motivates separate treatment of terminal and non-terminal states.

Learning the terminal state classifier.For a terminal state \(x\), the variables \(y_{1},\ldots,y_{n}\) are independent, hence we can use the factorization \(\widetilde{Q}_{\phi}(y_{1},\ldots,y_{n}|x)=\prod_{k=1}^{n}\widetilde{Q}_{\phi }(y_{k}|x)\). Moreover, all distributions on the _r.h.s._ must be the same, i.e. it is enough to learn just \(\widetilde{Q}_{\phi}(y_{1}|x)\). This marginal classifier can be learned by minimizing the cross-entropy loss

\[\mathcal{L}_{\text{T}}(\phi)=\operatorname*{\mathbb{E}}_{(\widehat{\gamma}, \widehat{y}_{1})\sim\widetilde{\mathcal{W}}(y_{1})\widetilde{p}(x|y_{1})} \left[-\log\widetilde{Q}_{\phi}(y_{1}\!=\!\widehat{y}_{1}|x\!=\!\widehat{x}) \right].\] (16)

Here \(\widehat{\gamma}_{1}\) is sampled from \(\widetilde{p}(y_{1})\), which is uniform under our choice of \(\widetilde{p}(x)\). Then, \(\widehat{x}|(y_{1}\!=\!\widehat{\gamma}_{1})\) is generated from the base GFlowNet \(p_{\widehat{\gamma}_{1}}\), since (9) implies that \(\widetilde{p}(x|y\!=\!\widehat{y}_{1})=p_{\widehat{\gamma}_{1}}(x)\).

Learning the non-terminal state classifier.Given a non-terminal state \(s\in S\setminus\mathcal{X}\), we need to model \(y_{1},\ldots,y_{n}\) jointly, and training requires sampling tuples \((\widehat{s},\widehat{y}_{1},\ldots,\widehat{y}_{n})\). Non-terminal states \(s\) can be generated as intermediate states in trajectories \(\tau=(s_{0}\to s_{1}\to\ldots\to x)\). Given a sampled trajectory \(\widehat{\tau}\) and a set of labels \(\widehat{y}_{1},\ldots,\widehat{y}_{n}\) we denote the trajectory cross-entropy by

\[\mathcal{L}(\widehat{\tau},\widehat{y}_{1},\ldots,\widehat{y}_{n};\phi)=\sum _{t=0}^{|\tau|-1}\left[-\log\widetilde{Q}_{\phi}(y_{1}\!=\!\widehat{y}_{1}, \ldots,y_{n}\!=\!\widehat{y}_{n}|s\!=\!\widehat{s}_{t})\right].\] (17)

Pairs \((\widehat{\tau},\widehat{y}_{1})\) can be generated in the same way as in the terminal classifier training above: 1) \(\widehat{y}_{1}\sim\widetilde{p}(y_{1})\); 2) \(\widehat{\tau}\sim p_{\widehat{y}_{1}}(\tau)\). Sampling \(\widehat{y}_{2},\ldots,\widehat{y}_{n}\) given \(\widehat{x}\) (the terminal state of \(\widehat{\tau}\)) requires access to values \(\widetilde{p}(y_{k}=\widehat{y}_{k}|\widehat{x})\), which are not directly available. However, if the terminal classifier is learned as described above, the estimates \(w_{i}(\widehat{x};\phi)=\widetilde{Q}_{\phi}(y_{1}\!=\!i|x\!=\!\widehat{x})\) can be used instead. In this case, the loss and the sampling procedure for the non-terminal classifier rely on the outputs of the terminal classifier. In order to train two classifiers simultaneously, and avoid the instability due to a feedback loop, we employ the "target network" technique developed in the context of deep Q-learning [55]. We introduce a "target network" parameter vector \(\overline{\phi}\) which is used to produce the estimates \(w_{i}(\widehat{\mathcal{X}};\overline{\phi})\) for the non-terminal loss. We update \(\overline{\phi}\) as the exponential moving average of the recent iterates of \(\phi\).

After putting all components together the training loss for the non-terminal state classifier is

\[\mathcal{L}_{N}(\phi,\overline{\phi})=\operatorname*{\mathbb{E}}_{(\widehat{ \tau},\widehat{y}_{1})\sim\widetilde{p}(\tau,y_{1})}\left[\sum_{\widehat{y}_{2 }=1}^{m}\cdots\sum_{\widehat{y}_{n}=1}^{m}\left(\prod_{k=2}^{n}w_{\widehat{y}_{k }}(\widehat{x};\overline{\phi})\right)\mathcal{L}(\widehat{\tau},\widehat{y }_{1},\ldots,\widehat{y}_{n};\phi)\right].\] (18)

We refer the reader to Appendix D.4 for a more detailed derivation of the loss (18). Algorithm A.1 shows the complete classifier training procedure.

## 6 Experiments

2D distributions via GFlowNet.We tested our GFlowNet composition method on 2D grid [36], a controlled domain, where the ground-truth composite distributions can be evaluated directly.

In the 2D grid domain, the states are the cells of an \(H\times H\) grid. The starting state is the upper-left cell \(s_{0}=(0,0)\). At each state, the allowed actions are: 1) move right; 2) move down; 3) terminate the trajectory at the current position. We first trained GFlowNets \(p_{i}(x)\propto R_{i}(x)\) with reward functions \(R_{i}(x)>0\), and then trained classifiers and constructed compositions following Theorem 5.3.

Figure 2 (top row) shows the distributions obtained by composing two pre-trained GFlowNets (top row; left). The harmonic mean \(p_{1}\otimes p_{2}\), covers the regions that have high probability under both \(p_{1}\) and \(p_{2}\) and excludes locations where either of the probabilities is low. \(p_{1}\bigodot p_{2}\) resembles \(p_{1}\) but the relative masses of the modes of \(p_{1}\) are modulated by \(p_{2}\): regions with high \(p_{2}\) have lower probability under contrast. The parameterized contrast \(p_{1}\bigodot_{0.95}p_{2}\) with \(\alpha=0.05\) magnifies the contrasting effect: high \(p_{2}(x)\) implies very low (\(p_{1}\bigodot_{0.95}p_{2}\))(\(x\)). The bottom row of Figure 2 shows operations on 3 distributions. The conditional \(\widetilde{p}_{i}(x|y_{1}\)=1, \(y_{2}\)=2) is concentrated on the points that have high likelihood under both \(p_{1}\) and \(p_{2}\). Similarly, the value \(\widetilde{p}(x|y_{1}\)=1, \(y_{2}\)=2, \(y_{3}\)=3) is high if \(x\) is likely to be observed under all three distributions at the same time. The conditions \(\widetilde{p}(x|y_{1}\)=2, \(y_{2}\)=2) and \(\widetilde{p}(x|y_{1}\)=2, \(y_{2}\)=2, \(y_{3}\)=2) highlight the points with high \(p_{2}(x)\) but low \(p_{1}(x)\) and \(p_{3}(x)\). Conditioning on three labels results in a sharper distribution compared to double-conditioning. We provide quantitative results and further details in Appendix F.1. The classifier learning curves are provided in Appendix G.4.

Molecule generation via GFlowNet.Next, we evaluated our method for GFlowNet composition on a large and highly structured data space, and assessed the effect that composition operations have

Figure 3: **Reward distributions in the molecular generation domain. (a) Base GFlowNets at \(\beta\!=\!32\): \(p_{\text{SEH}}\) and \(p_{\text{SA}}\) are trained with \(R_{\text{SEH}}(x)^{32}\) and \(R_{\text{SA}}(x)^{32}\). (b) harmonic mean of \(p_{\text{SEH}}\) and \(p_{\text{SA}}\) at \(\beta\!=\!32\). (c) contrasts at \(\beta\!=\!32\). (d) base GFlowNets at \(\beta\!=\!96\). (e) harmonic mean at \(\beta\!=\!96\). Lines show the contours of the level sets of the kernel density estimates in the (\(R_{\text{SEH}}\), \(R_{\text{SA}}\)) plane.**

Figure 2: **Composed GFlowNets on \(32\times 32\) grid domain. (Top) operations on two distributions. (Bottom) operations on three distributions. Cell probabilities are shown with color, darker is higher. Red circles indicate the high probability regions of \(p_{1}\), \(p_{2}\), \(p_{3}\).**

[MISSING_PAGE_FAIL:9]

three diffusion models trained to generate MNIST [56] digits \(\{0,1,2,3\}\) in two colors: cyan and bige. Each model was trained to generate digits with a specific property: \(p_{1}\) generated cyan digits, \(p_{2}\) generated digits less than \(2\), and \(p_{3}\) generated even digits.

We built the composition iteratively via the factorization \(\widetilde{p}(x|y_{1},y_{2},y_{3})\propto\widetilde{p}(x)\widetilde{p}(y_{1},y _{2}|x)\widetilde{p}(y_{3}|x,y_{1},y_{2})\). To this end, we first trained a classifier \(\widetilde{Q}(y_{1},y_{2}|x_{t})\) on trajectories sampled from the base models. This allowed us to generate samples from \(\widetilde{p}(x|y_{1},y_{2})\). We then trained an additional classifier \(\widetilde{Q}(y_{3}|x,y_{1},y_{2})\) on trajectories from compositions defined by \((y_{1},y_{2})\) to allow us to sample from \(\widetilde{p}(x|y_{1},y_{2},y_{3})\). Further details can be found in Appendix F.3.

Figure 5 shows samples from the pre-trained models and from selected compositions. The negating effect of _not_ conditioning on observations is clearly visible in the compositions using two variables. For example, \(\widetilde{p}(x|y_{1}=1,y_{2}=1)\) only generates cyan 3's. Because there we _do not_ condition on \(p_{2}\) or \(p_{3}\), the composition excludes digits that have high probability under \(p_{2}\) or \(p_{3}\), i.e. those that are less than \(2\) or even. In \(\widetilde{p}(x|y_{1}=1,y_{2}=3)\), cyan even digits have high density under both \(p_{1}\) and \(p_{3}\), but because \(p_{2}\) is not conditioned on, the composition excludes digits less than two (i.e. cyan 0's). Finally, \(\widetilde{p}(x|y_{1}=1,y_{2}=2,y_{3}=3)\) generates only cyan 0's, on which all base models have high density.

## 7 Conclusion

We introduced Compositional Sculpting, a general approach for composing iterative generative models. Compositions are defined through "observations", which enable us to emphasize or de-emphasize the density of the composition in regions where specific base models have high density. We highlighted two binary compositions, harmonic mean and contrast, which are analogous to the product and negation operations defined on EBMs. A crucial feature of the compositions we have introduced is that we can sample from them directly. By extending classifier guidance we are able to leverage the generative capabilities of the base models to produce samples from the composition. Through empirical experiments, we validated our approach for composing diffusion models and GFlowNets on toy domains, molecular generation, and image generation.

Broader impact.We proposed a mathematical framework and methods for the composition of pre-trained generative models. While the primary emphasis of our work is on advancing foundational research on generative modeling methodology and principled sampling techniques, our work inherits ethical concerns associated with generative models such as creation of deepfake content and misinformation dissemination, as well as reproduction of biases present in the datasets used for model training. If not carefully managed, these models can perpetuate societal biases, exacerbating issues of fairness and equity. Our work further contributes to research on the reuse of pre-trained models. This research direction promotes eco-friendly AI development, with the long-term goal of reducing energy consumption and carbon emissions associated with large-scale generative model training.

Figure 5: **Composed diffusion models on colored MNIST.** Samples from 3 pre-trained diffusion models and their various compositions.

## Acknowledgements

TG and TJ acknowledge support from the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, DARPA Accelerated Molecular Discovery program, the NSF Expeditions grant (award 1918839) "Understanding the World Through Code", and from the MIT-DSTA collaboration.

SK and SDP were supported by the Technology Industries of Finland Centennial Foundation and the Jane and Aatos Erkko Foundation under project Interactive Artificial Intelligence for Driving R&D, the Academy of Finland (flagship programme: Finnish Center for Artificial Intelligence, FCAI; grants 328400, 345604 and 341763), and the UKRI Turing AI World-Leading Researcher Fellowship, EP/W002973/1.

VG acknowledges support from the Academy of Finland (grant decision 342077) for "Human-steered next-generation machine learning for reviving drug design", the Saab-WASP initiative (grant 411025), and the Jane and Aatos Erkko Foundation (grant 7001703) for "Biodesign: Use of artificial intelligence in enzyme design for synthetic biology".

GY acknowledges support from the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org).

We thank Sammie Katt and Pavel Izmailov for the helpful discussions and assistance in making the figures.

We thank NeurIPS 2023 anonymous reviewers for the helpful feedback on our work.

## References

* Kirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross B. Girshick. Segment anything. _ArXiv_, abs/2304.02643, 2023.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Cherti et al. [2023] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2818-2829, 2023.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _ArXiv_, abs/1810.04805, 2018.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Chatgpt [2023] OpenAI. Chatgpt (mar 14, 2023 version). Large language model, 2023. URL https://chat.openai.com/chat.

* [10] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alexandru Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmin Bastings, Jannis Bulian, Xavier Garcia, Jiamino Ni, Andrew Chen, Kathleen Keneely, J. Clark, Stephan Lee, Daniel H Garrette, James Lee-Thorp, Colin Raffel, Noam M. Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy B. Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling up models and data with t5x and seqio. _ArXiv_, abs/2203.17189, 2022.
* [11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _ArXiv_, abs/2204.02311, 2022.
* [12] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Panzag R. Sankei, Kevin Sayed, Jaspari Singh, Sumedh Anand Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Ho Vuong, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. _ArXiv_, abs/2212.06817, 2022.
* [13] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter R. Florence. Palm-e: An embodied multimodal language model. _ArXiv_, abs/2303.03378, 2023.
* [14] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as is say: Grounding language in robotic affordances. In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, _Proceedings of The 6th Conference on Robot Learning_, volume 205 of _Proceedings of Machine Learning Research_, pages 287-318. PMLR, 2023.
* [15] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* [16] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Feresthe Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilfroshan, J. F. Nyarko, Giray Ogut, Laurel J. Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan,Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R'e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramer, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Kikun Zhang, Yuhui Zhang, Lucia Zheng, Katilyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. _ArXiv_, abs/2108.07258, 2021.
* Zhang and Agrawala [2023] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _ArXiv_, abs/2302.05543, 2023.
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In _Advances in Neural Information Processing Systems_, volume 35, pages 27730-27744, 2022.
* Sur'is et al. [2023] D'idac Sur'is, Sachit Menon, and Carl Vondrick. Vivergpt: Visual inference via python execution for reasoning. _ArXiv_, abs/2303.08128, 2023.
* Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.
* Hinton [1999] Geoffrey E Hinton. Products of experts. In _Ninth International Conference on Artificial Neural Networks_, volume 1, 1999.
* Hinton [2002] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. _Neural computation_, 14(8):1771-1800, 2002.
* Vedantam et al. [2018] Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin Murphy. Generative models of visually grounded imagination. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=HkCsm61Rb.
* Du et al. [2020] Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models. _Advances in Neural Information Processing Systems_, 33:6637-6647, 2020.
* Du et al. [2021] Yilun Du, Shuang Li, Yash Sharma, Josh Tenenbaum, and Igor Mordatch. Unsupervised learning of compositional energy concepts. _Advances in Neural Information Processing Systems_, 34:15608-15620, 2021.
* Liu et al. [2021] Nan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, and Antonio Torralba. Learning to compose visual relations. _Advances in Neural Information Processing Systems_, 34:23166-23178, 2021.
* Liu et al. [2022] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII_, pages 423-439. Springer, 2022.
* Du et al. [2023] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In _International Conference on Machine Learning_, pages 8489-8510. PMLR, 2023.
* Jin et al. [2020] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using interpretable substructures. In _International conference on machine learning_, pages 4849-4859. PMLR, 2020.
* Xie et al. [2021] Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. MARS: Markov Molecular Sampling for Multi-objective Drug Discovery. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=kH8Su4ebxFXY.

* Jain et al. [2023] Moksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlownets. In _International Conference on Machine Learning_, pages 14631-14653. PMLR, 2023.
* Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* Song and Ermon [2020] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. _Advances in neural information processing systems_, 33:12438-12448, 2020.
* Song et al. [2021] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=PxTIG12RRHS.
* Bengio et al. [2023] Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. _Journal of Machine Learning Research_, 24(210):1-55, 2023. URL http://jmlr.org/papers/v24/22-0364.html.
* Bengio et al. [2021] Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. _Advances in Neural Information Processing Systems_, 34:27381-27394, 2021.
* Song and Ermon [2019] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* Anderson [1982] Brian D.O. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* Hoang et al. [2018] Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. Mgan: Training generative adversarial nets with multiple generators. In _International conference on learning representations_, 2018.
* Tolstikhin et al. [2017] Ilya O Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Scholkopf. Adagan: Boosting generative models. _Advances in neural information processing systems_, 30, 2017.
* Grover and Ermon [2018] Aditya Grover and Stefano Ermon. Boosted generative models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* Liu et al. [2023] Nan Liu, Yilun Du, Shuang Li, Joshua B Tenenbaum, and Antonio Torralba. Unsupervised compositional concepts discovery with text-to-image generative models. _ArXiv_, abs/2306.05357, 2023.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* Ho and Salimans [2021] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications_, 2021. URL https://openreview.net/forum?id=qW8AKxfYbI.
* Meng et al. [2022] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=aBsCjcPu_tE.

* Couairon et al. [2023] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=3lge0p5o-M-.
* Hupkes et al. [2020] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: How do neural networks generalise? _Journal of Artificial Intelligence Research_, 67:757-795, 2020.
* Conwell and Ullman [2022] Colin Conwell and Tomer Ullman. Testing relational understanding in text-guided image generation. _ArXiv_, abs/2208.00005, 2022.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _ArXiv_, abs/2204.06125, 2022.
* Lahlou et al. [2023] Salem Lahlou, Tristan Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex Hernandez-Garcia, Lena Nehale Ezzine, Yoshua Bengio, and Nikolay Malkin. A theory of continuous generative flow networks. In _International Conference on Machine Learning_, pages 18269-18300. PMLR, 2023.
* Zhang et al. [2022] Dinghuai Zhang, Ricky TQ Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with gflownets. _ArXiv_, abs/2209.02606, 2022.
* Peluchetti [2022] Stefano Peluchetti. Non-denoising forward-time diffusions, 2022. URL https://openreview.net/forum?id=oVfIKuqfC.
* Lipman et al. [2023] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t.
* Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
* LeCun [1998] Yann LeCun. The mnist database of handwritten digits. _http://yann.lecun.com/exdb/mnist/_, 1998.
* Brigo [2008] Damiano Brigo. The general mixture-diffusion sde and its relationship with an uncertain-volatility option model with volatility-asset decorrelation. _ArXiv_, abs/0812.4052, 2008.
* Malkin et al. [2022] Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in GFlownets. In _Advances in Neural Information Processing Systems_, volume 35, pages 5955-5967, 2022.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _ArXiv_, abs/1412.6980, 2014.
* Gilmer et al. [2017] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* Ertl and Schuffenhauer [2009] Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. _Journal of cheminformatics_, 1:1-11, 2009.
* Landrum [2010] Greg Landrum. Rdkit: Open-source cheminformatics, 2010. URL https://www.rdkit.org/.
* Bickerton et al. [2012] G Richard Bickerton, Gaia V Paolini, Jeremy Besnard, Sorel Muresan, and Andrew L Hopkins. Quantifying the chemical beauty of drugs. _Nature chemistry_, 4(2):90-98, 2012.
* Yun et al. [2019] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. _Advances in neural information processing systems_, 32, 2019.

- MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.
* [66] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _Advances in Neural Information Processing Systems_, 33:7537-7547, 2020.
* [67] Matthew D Zeiler. Adadelta: an adaptive learning rate method. _ArXiv_, abs/1212.5701, 2012.
* [68] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.

## Appendix A Compositional Sculpting of Iterative Generative Processes (Section 5, continued)

### Classifier Training (GFlowNets): Algorithm

```
1:Initialize \(\bm{\phi}\) and set \(\overline{\bm{\phi}}=\bm{\phi}\)
2:for\(\text{step}=1,\ldots,\text{num\_steps}\)do
3:for\(i=1,\ldots,\text{m\_do}\)
4: Sample \(\widehat{\tau}_{i}\sim p_{i}(\tau)\)
5:endfor
6:\(\mathcal{L}_{T}(\bm{\phi})=-\sum_{i=1}^{m}\log\widetilde{Q}_{ \bm{\phi}}(y_{1}\!=\!i|x\!=\!\widehat{\chi}_{i})\) {Terminal state loss (16)}
7:\(w_{i}(\widehat{\chi}_{j};\overline{\bm{\phi}})\!=\!\widetilde{Q}_{\overline{ \bm{\phi}}}(y_{k}\!=\!i|x\!=\!\widehat{\chi}_{j})\), \(i,j\in\{1,\ldots m\}\) {Terminal probability estimates} {Non-terminal state loss (17)-(18)}
8:\(\mathcal{L}_{N}(\bm{\phi},\overline{\bm{\phi}})=\sum_{\widehat{\tau}_{i}=1}^{ m}\ldots\sum_{\widehat{\tau}_{n}=1}^{m}\left(\prod_{k=2}^{n}w_{\widehat{\tau}_{k}}( \widehat{\chi}_{\widehat{\tau}_{1}};\overline{\bm{\phi}})\right)\,\ell( \widehat{\tau}_{\widehat{\tau}_{1}},\widehat{\tau}_{1},\ldots\widehat{\tau}_ {n};\bm{\phi})\)
9:\(\mathcal{L}(\bm{\phi},\overline{\bm{\phi}})=\mathcal{L}_{T}(\bm{\phi})+\gamma (\text{step})\cdot\mathcal{L}_{N}(\bm{\phi},\overline{\bm{\phi}})\)
10: Update \(\bm{\phi}\) using \(\nabla_{\bm{\phi}}\mathcal{L}(\bm{\phi},\overline{\bm{\phi}})\); update \(\overline{\bm{\phi}}=\beta\overline{\bm{\phi}}+(1-\beta)\bm{\phi}\)
11:endfor ```

**Algorithm A.1** Compositional Sculpting: classifier training

### Composition of Diffusion Models

In this section, we show how the method introduced above can be applied to diffusion models. First, we adapt the model we introduced in (9)-(11) to diffusion models. A diffusion model trained to sample from \(p_{i}(x)\) generates a trajectory \(\tau=\{x_{1}\}_{t=0}^{T}\) over a range of time steps which starts with a randomly sampled state \(x_{T}\) and ends in \(x_{0}\), where \(x_{0}\) has distribution \(p_{i,t=0}(x)=p_{i}(x)\). Thus, we must adapt our model to reflect this. We introduce a set of mutually dependent variables \(x_{t}\) for \(t\in(0,T]\) with as conditional distribution the transition kernel of the diffusion model \(p_{i}(x_{t}|x_{0})\).

Given \(m\) base diffusion models that sample from \(p_{1},\ldots,p_{m}\) respectively, we define the prior \(\overline{p}(x)\) as a mixture of these diffusion models. Proposition A.1 shows that this mixture is a diffusion model that can be constructed directly from the base diffusion models. We then apply classifier guidance to this mixture to sample from the composition. We present an informal version of the proposition below. The required assumptions and the proof are provided in Appendix D.5.

**Proposition A.1** (Diffusion mixture SDE).: _Suppose distributions \(p_{1}(x),\ldots,p_{m}(x)\) are realized by diffusion models with forward SDEs \(dx_{i,t}=f_{i,t}(x_{i,t})\ dt+g_{i,t}\ adv_{i,t}\) and score functions \(s_{i,t}(\cdot)\), respectively. Then, the mixture distribution \(p_{M}(x)=\sum_{i=1}^{m}\omega_{i}p_{i}(x)\) with \(\omega_{1}\ldots\omega_{m}\geq 0\) and \(\sum_{i=1}^{m}\omega_{i}=1\) is realized by a diffusion model with forward SDE_

\[dx_{t}=\underbrace{\left[\sum_{i=1}^{m}p(y\!=\!i|x_{t})f_{i,t}(x_{t})\right] }_{f_{M,t}(x_{t})}dt+\underbrace{\sqrt{\sum_{i=1}^{m}p(y\!=\!i|x_{t})g_{i,t}^{ 2}}}_{g_{M,t}(x_{t})}\ dw_{t},\] (19)

_and backward SDE_

\[dx_{t}=\left[\sum_{i=1}^{m}p(y\!=\!i|x_{t})\left(f_{i,t}(x_{t})-g_{i,t}^{2}s_{ i,t}(x_{t})\right)\right]\ dt+\sqrt{\sum_{i=1}^{m}p(y\!=\!i|x_{t})g_{i,t}^{2}}\ d \overline{w}_{t},\] (20)

_with_

\[p(y\!=\!i|x_{t})=\frac{\omega_{i}p_{i,t}(x_{t})}{\sum_{i=1}^{m}\omega_{i}p_{i, t}(x_{t})}.\] (21)If the base diffusion models have a common forward SDE \(dx_{i,t}=f_{t}(x_{i,t})\,dt+g_{t}\,dw_{i,t}\), equations (19)-(20) simplify to

\[dx_{t}=f_{t}(x_{t})dt+g_{t}dw_{t},\quad d\,x_{t}=\left[f_{t}(x_{t})-g_{t}^{2} \left(\sum_{i=1}^{m}p(y\!=\!i|x_{t})s_{i,t}(x_{t})\right)\right]\,dt+g_{t}d \widetilde{w}_{t}.\] (22)

Theorem A.2 summarizes the overall approach.

**Theorem A.2**.: _Suppose distributions \(p_{1}(x),\ldots,p_{m}(x)\) are realized by diffusion models with forward SDEs \(dx_{i,t}=f_{i,t}(x_{i,t})\,dt+g_{i,t}\,dw_{i,t}\) and score functions \(s_{i,t}(\cdot)\), respectively. Let \(y_{1},\ldots\,y_{n}\) be random variables defined by (9). Then, the conditional \(\widetilde{p}(x|y_{1},\ldots,y_{n})\) is realized by a classifier-guided diffusion with backward SDE_

\[dx_{t}=v_{C,t}(x_{t},y_{1},\ldots,y_{n})dt+g_{C,t}(x_{t})d \widetilde{w}_{t},\] (23)

_with_

\[v_{C,t}(x_{t},y_{1},\ldots,y_{n})=\sum_{i=1}^{m}\widetilde{p}(y \!=\!i|x_{t})\Big{(}f_{i,t}(x_{t})-g_{i,t}^{2}\Big{(}s_{i,t}(x_{t})+\nabla_{x_ {t}}\log\widetilde{p}(y_{1},\ldots,y_{n}|x_{t})\Big{)}\Big{)},\] (24) \[g_{C,t}(x_{t})=\sqrt{\sum_{i=1}^{m}\widetilde{p}(y\!=\!i|x_{t})g _{i,t}^{2}}.\] (25)

The proof of Theorem A.2 is provided in Appendix D.6.

### Classifier Training (Diffusion Models)

We approximate the inferential distributions in equations (22) and (23) with a time-conditioned classifier \(\widetilde{Q}_{\phi}(y_{1},\ldots,y_{n}|x_{t})\) with parameters \(\phi\). Contrary to GFlowNets, which employed a terminal and non-terminal state classifier, here we only need a single time-dependent classifier. The classifier is trained with different objectives on terminal and non-terminal states. The variables \(y_{1},\ldots,y_{n}\) are dependent given a state \(x_{t}\) for \(t\in[0,T)\), but are independent given the terminal state \(x_{T}\). Thus, when training on terminal states we can exploit this independence. Furthermore, we generally found it beneficial to initially train only on terminal states. The loss for the non-terminal states depends on classifications of the terminal state of the associated trajectories, thus by minimizing the classification error of terminal states first, we reduce noise in the loss calculated for the non-terminal states later.

For a terminal state \(x_{0}\), the classifier \(\widetilde{Q}_{\phi}(y_{1},\ldots,y_{n}|x_{t})\) can be factorized as \(\prod_{k=1}^{n}\widetilde{Q}_{\phi}(y_{k}|x_{0})\). Hence we can train \(\widetilde{Q}\) by minimizing the cross-entropy loss

\[\mathcal{L}_{\mathrm{T}}(\phi)=\operatorname*{\mathbb{E}}_{(\widehat{x}_{0}, \widehat{y}_{1})\sim\widetilde{p}(x,y_{1})}\left[-\log\widetilde{Q}_{\phi}(y_ {1}\!=\!\widehat{y}_{1}|x_{0}\!=\!\widehat{x}_{0})\right].\] (26)

Samples \(\widetilde{p}(x_{0},y_{1})\) can be generated according to the factorization \(\widetilde{p}(y_{1})\widetilde{p}(x_{0}|y_{1})\). First, \(\widehat{y}_{1}\) is sampled from \(\widetilde{p}(y_{1})\), which is uniform under our choice of \(\widetilde{p}(x_{0})\). Then, \(\widehat{x}_{0}|(y_{1}\!=\!\widehat{y}_{1})\) is generated from the reverse SDE of base diffusion model \(p_{\widehat{y}_{1}}(x)\). Note that equation (9) implies that all observations have the same conditional distribution given \(x\). Thus, \(\widetilde{Q}_{\phi}(y_{1}|x_{0})\) is also a classifier for observations \(y_{2},\ldots,y_{n}\).

For a non-terminal state \(x_{t}\) with \(t\in(0,T]\), we must train \(\widetilde{Q}\) to predict \(y_{1},\ldots,y_{n}\) jointly. For a non-terminal state \(\widehat{x}_{t}\) and observations \(\widehat{y}_{1},\ldots,\widehat{y}_{n}\), the cross-entropy loss is

\[\mathcal{L}(\widehat{x}_{t},\widehat{y}_{1},\ldots,\widehat{y}_{n};\phi)=- \log\widetilde{Q}_{\phi}(y_{1}\!=\!\widehat{y}_{1},\ldots,y_{n}\!=\!\widehat{y }_{n}|x_{t}\!=\!\widehat{x}_{t}).\] (27)

Tuples \((\widehat{x}_{t},\widehat{y}_{1},\ldots,\widehat{y}_{n})\) are obtained as follows: 1) \(\widehat{y}_{1}\sim\widetilde{p}(y_{1})\); 2) A trajectory \(\tau=\{x_{t}\}_{t=0}^{T}\) is sampled from the reverse SDE of diffusion model \(y_{1}\). At this point, we would ideally sample \(\widehat{y}_{2},\ldots,\widehat{y}_{n}\) given \(\widehat{x}_{0}\) but this requires access to \(\widetilde{p}(y_{k}\!=\!\widehat{y}_{k}|\widehat{x}_{0})\). Instead, we approximate this with \(w_{i}(\widehat{x};\phi)=\widetilde{Q}_{\phi}(y_{1}\!=\!i|x_{0}\!=\!\widehat{x }_{0})\) and marginalize over \(\widehat{y}_{2},\ldots,\widehat{y}_{n}\) to obtain the cross-entropy loss

\[\mathcal{L}_{N}(\phi,\overline{\phi})=\operatorname*{\mathbb{E}}_{(\widehat{x}, \widehat{y}_{1})\sim\widetilde{p}(x,y_{1})}\left[\sum_{\widehat{x}_{t}\in \widehat{\gamma}(\widehat{x}_{0})}\sum_{\widehat{y}_{2}=1}^{m}\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!Classifier Guidance for Parameterized Operations

This section covers the details of classifier guidance and classifier training for the parameterized operations (Section 4.1).

The complete probabilistic model for the parameterized operations on two distributions is given by

\[\widetilde{p}(x,y_{1},y_{2};\alpha)=\widetilde{p}(x)\widetilde{p}(y_{1}|x) \widetilde{p}(y_{2}|x;\alpha),\ \ \ \widetilde{p}(x)=\frac{1}{2}p_{1}(x)+\frac{1}{2}p_{2}(x),\] (29a) \[\widetilde{p}(y_{1}\!=\!i|x)=\frac{p_{i}(x)}{p_{1}(x)+p_{2}(x)},\ \ \widetilde{p}(y_{2}\!=\!i|x;\alpha)\!=\!\frac{\big{(}\alpha p_{1}(x)\big{)}^{ \{i=1\}}\cdot\big{(}(1\!-\!\alpha)p_{2}(x)\big{)}^{\{i=2\}}}{\alpha p_{1}(x)+(1 -\alpha)p_{2}(x)}.\] (29b)

While in the probabilistic model (3) all observations \(y_{i}\) are exchangeable, in the parameterized model (29) \(y_{1}\) and \(y_{2}\) are not symmetric. This difference requires changes in the classifier training algorithm for the parameterized operations.

We develop the method for the parameterized operations based on two observations:

* \(y_{1}\) appears in (29) in the same way as in (3);
* the likelihood \(\widetilde{p}(y_{2}|x;\alpha)\) of \(y_{2}\) given \(x\) can be expressed as the function of \(\widetilde{p}(y_{1}|x)\) and \(\alpha\): \[\widetilde{p}(y_{2}\!=\!1|x;\alpha) =\frac{\alpha p_{1}(x)}{\alpha p_{1}(x)+(1-\alpha)p_{2}(x)}= \frac{\alpha\frac{p_{1}(x)}{p_{1}(x)+p_{2}(x)}}{\alpha\frac{p_{1}(x)+p_{2}(x) }{p_{1}(x)+p_{2}(x)}+(1-\alpha)\frac{p_{2}(x)}{p_{1}(x)+p_{2}(x)}}\] (30a) \[=\frac{\alpha\widetilde{p}(y_{1}\!=\!1|x)}{\alpha\widetilde{p}(y_{1}\!=\!1|x )+(1-\alpha)\widetilde{p}(y_{1}\!=\!2|x)},\] \[\widetilde{p}(y_{2}\!=\!2|x;\alpha) =\frac{(1-\alpha)p_{2}(x)}{\alpha p_{1}(x)+(1-\alpha)p_{2}(x)}= \frac{(1-\alpha)\frac{p_{2}(x)}{p_{1}(x)+p_{2}(x)}}{\alpha\frac{p_{1}(x)+p_{2 }(x)}{p_{1}(x)+p_{2}(x)}}\] (30b) \[=\frac{(1-\alpha)\widetilde{p}(y_{1}\!=\!2|x)}{\alpha\widetilde{p}(y_{1}\!=\! 1|x)+(1-\alpha)\widetilde{p}(y_{1}\!=\!2|x)}.\]

These two observations combined suggest the training procedure where 1) the terminal state classifier is trained to approximate \(\widetilde{p}(y_{1}\!=\!i|x)\) in the same way as in Section 5.2; 2) the probability estimates \(w_{i}(\widehat{x},\widehat{\alpha};\phi)\approx\widetilde{p}(y_{2}\!=\!i; \widehat{\alpha})\) are expressed through the learned terminal state classifier \(\widetilde{p}(y_{1}\!=\!i|x)\) via (30). Below we provide details of this procedure for the case of GFlowNet composition.

Learning the terminal state classifier.The marginal \(y_{1}\) classifier \(\widetilde{Q}_{\phi}(y_{1}|x)\) is learned by minimizing the cross-entropy loss

\[\mathcal{L}_{T}(\phi)=\operatorname*{E}_{(\widehat{x},\widehat{y}_{1})\sim \widetilde{p}(x,y_{1})}\left[-\log\widetilde{Q}_{\phi}(y_{1}\!=\!\widehat{y}_{ 1}|x=\widehat{x})\right].\] (31)

Then, the joint classifier \(\widetilde{Q}_{\phi}(y_{1},y_{2}|x;\alpha)\) is constructed as

\[\widetilde{Q}_{\phi}(y_{1},y_{2}|x;\alpha)=\widetilde{Q}_{\phi}(y_{1}|x) \widetilde{Q}_{\phi}(y_{2}|x;\alpha),\] (32)

where \(\widetilde{Q}_{\phi}(y_{2}|x;\alpha)\) can be expressed through the marginal \(\widetilde{Q}_{\phi}(y_{1}|x)\) via (30).

Learning the non-terminal state classifier.The non-terminal state classifier \(\widetilde{Q}(y_{1},y_{2}|s;\alpha)\) models \(y_{1}\) and \(y_{2}\) jointly. Note that \(\alpha\) is one of the inputs to the classifier model. Given a sampled trajectory \(\widehat{\tau}\), labels \(\widehat{y}_{1},\widehat{y}_{2}\), and \(\widehat{\alpha}\), the total cross-entropy loss of all non-terminal states in \(\hat{t}\) is

\[\ell(\widehat{\tau},\widehat{y}_{1},\widehat{y}_{2},\widehat{\alpha};\phi)= \sum_{t=0}^{|\tau|-1}\left[-\log\widetilde{Q}_{\phi}(y_{1}\!=\!\widehat{y}_{ 1},y_{2}\!=\!\widehat{y}_{2}|s\!=\!\widehat{s}_{t};\widehat{\alpha})\right].\] (33)The pairs \((\widehat{\tau},\widehat{\mathcal{Y}}_{1})\) can be generated via a sampling scheme similar to the one used for the terminal state classifier loss above: 1) \(\widehat{y}_{1}\sim\widetilde{\rho}(y_{1})\) and 2) \(\widehat{\tau}\sim p_{\widehat{y}_{1}}(\tau)\). An approximation of the distribution of \(\widehat{y}_{2}\) given \(\widehat{\tau}\) is constructed using (30):

\[w_{1}(\widehat{x},\widehat{a};\phi)=\frac{\widehat{a}\widehat{Q} _{\phi}(y_{1}\!=\!1|x\!=\!\widehat{x})}{\widehat{a}\widetilde{Q}_{\phi}(y_{1} \!=\!1|x\!=\!\widehat{x})+(1-\widehat{a})\widetilde{Q}_{\phi}(y_{1}\!=\!2|x \!=\!\widehat{x})}\approx\widetilde{p}(y_{2}\!=\!1|x=\widehat{x};\widehat{a}),\] (34a) \[w_{2}(\widehat{x},\widehat{a};\phi)=\frac{(1-\widehat{a}) \widetilde{Q}_{\phi}(y_{1}\!=\!2|x\!=\!\widehat{x})}{\widehat{a}\widetilde{Q} _{\phi}(y_{1}\!=\!1|x\!=\!\widehat{x})+(1-\widehat{a})\widetilde{Q}_{\phi}(y_ {1}\!=\!2|x\!=\!\widehat{x})}\approx\widetilde{p}(y_{2}\!=\!2|x\!=\!\widehat{ x};\widehat{a}).\] (34b)

Since these expressions involve outputs of the terminal state classifier which is being trained simultaneously, we again (see Section 5.2) introduce the target network parameters \(\widehat{\phi}\) that are used to compute the probability estimates (34).

The training loss for the non-terminal state classifier is

\[\mathcal{L}_{N}(\phi,\overline{\phi})=\mathop{\mathbb{E}}_{\widehat{a}\sim \rho(\alpha)}\mathop{\mathbb{E}}_{\widehat{\rho}(\widehat{\tau},y_{1})}\! \left[\sum_{\widehat{y}_{2}=1}^{2}w_{\widehat{y}_{2}}(\widehat{x},\widehat{a} ;\overline{\phi})\epsilon(\widehat{\tau},\widehat{y}_{1},\widehat{y}_{2}, \widehat{a};\phi)\right],\] (35)

where \(p(\alpha)\) is sampling distribution over \(\alpha\in(0,1)\). In our experiments, we used the following sampling scheme for \(\alpha\):

\[\widehat{z}\sim U[-B,\,B],\qquad\widehat{a}=\frac{1}{1+\exp(-\widehat{z})}.\] (36)

## Appendix C Analysis of Compositional Sculpting and Energy Operations

The harmonic mean and contrast operations we have introduced are analogous to the product and negation operations for EBMs respectively. Although the harmonic mean and product operations are quite similar in practice, unlike the negation operation our proposed contrast operation always results in a valid probability distribution. Figure C.1 shows the results of these operations applied to two Gaussian distributions. The harmonic mean and product, shown in panel (b), are both concentrated on points that have high probability under both Gaussians. Figure C.1(c) shows parameterized contrasts \(p_{1}\,\bigodot\,(1-\alpha)p_{2}\) at different values of \(\alpha\), and panel (d) shows negations \(p_{1}\,\operatorname{neg}_{j}\,p_{2}\) at different values of \(\gamma\). The effect of negation at \(\gamma=0.1\) resembles the effect of the contrast operation: the density retreats from the high likelihood region of \(p_{2}\). However, as \(\gamma\) increases to 0.5 the distribution starts to concentrate excessively on the values \(x<-3\). This is due to the instability of division \(p_{1}(x)/(p_{2}(x))^{\gamma}\) in regions where \(p_{2}(x)\to 0\). Proposition C.1 shows that negation \(p_{1}\,\operatorname{neg}_{j}\,p_{2}\) in many cases results in an improper (non-normalizable) distribution.

Mathematical analysis of operations.Harmonic mean and product are not defined for pairs of distributions \(p_{1}\), \(p_{2}\) which have disjoint supports. In such cases, attempts at evaluation of the expressions for \(p_{1}\otimes p_{2}\) and \(p_{1}\,\operatorname{prod}\,p_{2}\) will lead to impossible probability distributions that have zero probability mass (density) everywhere3. The result of both harmonic mean and product are correctly defined for any pair of distributions \(p_{1}\), \(p_{2}\) that have non-empty support intersection.

Footnote 3: Informal interpretation: distributions with disjoint supports have empty intersections (think of the intersection of sets analogy)

Notably, contrast is well-defined for any input distributions while negation is ill-defined for some input distributions \(p_{1}\), \(p_{2}\) as formally stated below (see Figure C.1 (d) for a concrete example).

**Proposition C.1**.:
1. _For any_ \(\alpha\in(0,1)\) _the parameterized contrast operation_ \(p_{1}\,\boldsymbol{\Theta}_{\,(1-\alpha)}\,\,p_{2}\) _(_7_) is well-defined: gives a proper distribution for any pair of distributions_ \(p_{1}\)_,_ \(p_{2}\)_._
2. _For any_ \(\gamma\in(0,1)\) _there are infinitely many pairs of distributions_ \(p_{1}\)_,_ \(p_{2}\) _such that the negation_ \[(p_{1}\,neg_{\gamma}\,\,p_{2})(x)\propto\exp\left\{-\left(E_{1}(x)-\gamma\,E_ {2}(x)\right)\right\}\propto\frac{p_{1}(x)}{\left(p_{2}(x)\right)^{\gamma}}.\] (37) _results in an improper (non-normalizable) distribution._

Proof.: Without loss of generality, we prove the claims of the proposition assuming absolutely continuous distributions \(p_{1}\), \(p_{2}\) with probability density functions \(p_{1}(\cdot)\), \(p_{2}(\cdot)\).

Claim 1.For any two distributions \(p_{1}\), \(p_{2}\) we have \(p_{1}(x)\geq 0\), \(p_{2}(x)\geq 0\), \(\int_{x}p_{1}(x)\,dx=\int_{x}p_{2}(x)\,dx=1<\infty\). Then, the RHS of the expression for the parameterized contrast operation \(p_{1}\,\boldsymbol{\Theta}_{\,(1-\alpha)}\,\,p_{2}\) (7) satisfies

\[\frac{p_{1}(x)^{2}}{ap_{1}(x)+(1-\alpha)p_{2}(x)}=\frac{p_{1}(x)}{\alpha} \cdot\underbrace{\frac{p_{1}(x)}{p_{1}(x)+\frac{(1-\alpha)}{a}p_{2}(x)}}_{\leq 1 }\leq\frac{p_{1}(x)}{\alpha},\quad\forall\,x\in\operatorname{supp}(p_{1})\cup \operatorname{supp}(p_{2}).\] (38)

For points \(x\not\in\operatorname{supp}(p_{1})\cup\operatorname{supp}(p_{2})\), we set \(\frac{p_{1}(x)^{2}}{ap_{1}(x)+\{1-\alpha\}p_{2}(x)}=0\) since by construction the composite distributions do not have probability mass outside of the union of the supports of the original distributions. The above implies that

\[\int_{x}\frac{p_{1}^{2}(x)}{ap_{1}(x)+(1-\alpha)p_{2}(x)}\,dx\leq\frac{1}{ \alpha}\int_{X}p_{1}(x)\,dx=\frac{1}{\alpha}<\infty.\] (39)

Therefore, the RHS of the expression for the parameterized contrast operation \(p_{1}\,\boldsymbol{\Theta}_{\,(1-\alpha)}\,\,p_{2}\) (7) can be normalized, and the distribution \(p_{1}\,\boldsymbol{\Theta}_{\,(1-\alpha)}\,\,p_{2}\) is well-defined.

Claim 2.For any \(\gamma\in(0,1)\) we provide an infinite collection of distribution pairs \(p_{1}\) and \(p_{2}\) such that negation \(p_{1}\,\operatorname{neg}_{\gamma}\,\,p_{2}\) results in a non-normalizable distribution.

For the given \(\gamma\in(0,1)\) we select four numbers \(\mu_{1}\in\mathbb{R}\), \(\mu_{2}\in\mathbb{R}\), \(\sigma_{1}>0\), \(\sigma_{2}>0\) such that

\[\sigma_{1}^{2}\geq\frac{1}{\gamma}\sigma_{2}^{2},\] (40)

Consider univariate normal distributions \(p_{1}(x)=\mathcal{N}(x;\mu_{1},\sigma_{1}^{2})\), \(p_{2}(x)=\mathcal{N}(x;\mu_{2},\sigma_{2}^{2})\) with density functions

\[p_{i}(x)=\mathcal{N}(x;\mu_{i},\sigma_{i}^{2})=\frac{1}{\sqrt{2\pi\sigma_{i}^{ 2}}}\exp\left\{-\frac{(x-\mu_{i})^{2}}{2\sigma_{i}^{2}}\right\},\qquad i\in\{ 1,2\}.\] (41)

For such \(p_{1}\) and \(p_{2}\), the RHS of (37) is

\[\frac{p_{1}(x)}{(p_{2}(x))^{\gamma}}=\frac{1}{\left(\sqrt{2\pi}\right)^{1- \gamma}}\frac{\sigma_{2}^{\gamma}}{\sigma_{1}}\exp\left\{x^{2}\left(\frac{ \gamma\sigma_{1}^{2}-\sigma_{2}^{2}}{2\sigma_{1}^{2}\sigma_{2}^{2}}\right)+x \left(\frac{\mu_{1}}{\sigma_{1}^{2}}-\gamma\frac{\mu_{2}}{\sigma_{2}^{2}} \right)+\gamma\frac{\mu_{2}^{2}}{2\sigma_{2}^{2}}-\frac{\mu_{1}^{2}}{2\sigma_ {1}^{2}}\right\}.\] (42)

Condition (40) implies that the quadratic function under the exponent above has a non-negative coefficient for \(x^{2}\). Therefore this function either grows unbounded as \(x\to\infty\) (if the coefficients for the quadratic and linear terms are not zero), or constant (if the coefficients for quadratic and linear terms are zero). In either case, \(\int_{\mathbb{R}}p_{1}(x)/(p_{2}(x))^{\gamma}\,dx=\infty\).

Proofs and Derivations

### Proof of Proposition 5.1

Our goal is to show that the policy (12) induces the mixture distribution \(p_{M}(x)=\sum_{i=1}^{m}\omega_{i}p_{i}(x)\).

Preliminaries.In our proof below we use the notion of "the probability of observing a state \(s\in S\) on a GFlowNet trajectory". Following Bengio et al. [35], we abuse the notation and denote this probability by

\[p_{i}(s)\triangleq p_{i}(\{\tau\,:\,s\in\tau\,\})=\sum_{\tau\in \mathcal{T}_{t_{0},s}}\prod_{i=0}^{|\tau|-1}p_{i,F}(s_{\tau}|s_{\tau-1}),\] (43)

where \(\mathcal{T}_{s_{0},s}\) is the set of all (sub)trajectories starting at \(s_{0}\) and ending at \(s\). The probabilities induced by the policy (12) are denoted by \(p_{M}(s)\). Note that \(p_{i}(s)\) and \(p_{M}(s)\) should not be interpreted as probability mass functions over the set of states \(S\). In particular \(p_{i}(s_{0})=p_{M}(s_{0})=1\) and sums \(\sum_{i\in S}p_{i}(s)\), \(\sum_{i\in S}p_{M}(s)\) are not equal to \(1\) (unless \(S=\{s_{0}\}\)). However, the functions \(p_{i}(\cdot)\), \(p_{M}(\cdot)\) restricted to the set of terminal states \(\mathcal{X}\) give valid probability distributions over \(\mathcal{X}\): \(\sum_{x\in\mathcal{X}}p_{i}(x)=\sum_{x\in\mathcal{X}}p_{M}(x)=1\).

By definition \(p_{i}(\cdot)\) and \(p_{M}(\cdot)\) satisfy the recurrent relationship

\[p_{i}(s)=\sum_{s_{*}\,:\,(s_{*}\,\to\,s)\in\mathcal{A}}p_{i}(s_{*})p_{i,F}(s|s _{*}),\qquad p_{M}(s)=\sum_{s_{*}\,:\,(s_{*}\,\to\,s)\in\mathcal{A}}p_{M}(s_{*} )p_{M,F}(s|s_{*}).\] (44)

The joint distribution of \(y\) and \(\tau\) described in the statement of Proposition 5.1 is \(p(\tau,y\!=\!i)=w_{i}p_{i}(\tau)\). This joint distribution over \(y\) and trajectories implies the following expressions for the distributions involving intermediate states \(s\).

\[p(y\!=\!i)=\omega_{i},\] (45) \[p(\tau\,|y\!=\!i)=p_{i}(\tau)=\prod_{t=0}^{|\tau|-1}p_{i,F}(s_{ \tau}|s_{t-1}),\] (46) \[p(\tau)=\sum_{i=1}^{m}p(\tau\,|y\!=\!i)p(y\!=\!i)=\sum_{i=1}^{m} \omega_{i}p_{i}(\tau),\] (47) \[p(s|y\!=\!i)=p_{i}(s),\] (48) \[p(s)=\sum_{i=1}^{m}p(s|y\!=\!i)p(y\!=\!i)=\sum_{i=1}^{m}\omega_{i }p_{i}(s).\] (49)

Proof.Using the notation introduced above, we can formally state our goal. We need to show that \(p_{M}(x)\) induced by \(p_{M,F}\) gives the mixture distribution

\[p_{M}(x)=\sum_{i=1}^{m}\omega_{i}p_{i}(x).\] (50)

We prove a more general equation for all states \(s\in S\)

\[p_{M}(s)=\sum_{i=1}^{m}\omega_{i}p_{i}(s)\] (51)

by induction over the DAG \((S,\mathcal{A})\).

Base case.Consider the initial state \(s_{0}\in S\). By definition \(p_{i}(s_{0})=p_{M}(s_{0})=1\) which implies

\[p_{M}(s_{0})=\sum_{i=1}^{m}\omega_{i}p_{i}(s_{0}).\] (52)

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

\[=\frac{p(y|s)}{p(y)}p(s)\quad\{\text{used (\ref{eq:p_1})}\}\] (79) \[=p(s|y),\quad\{\text{used Bayes' theorem}\}\] (80)

which proves (73) for state \(s\).

### Proof of Theorem 5.3

By Proposition 5.1 we have that the policy

\[p_{M,F}(s^{\prime}|s)=\sum_{i=1}^{m}p_{i,F}(s^{\prime}|s)\widetilde{p}(y\!=\!i| s),\] (81)

generates the mixture distribution \(p_{M}(x)=\frac{1}{m}\sum_{i=1}^{m}p_{i}(x)\).

In the probabilistic model \(\widetilde{p}(x,y_{1}\,\ldots\,y_{n})\) the marginal distribution \(\widetilde{p}(x)=p_{M}(x)\) is realized by the mixture policy \(p_{M,F}\). Therefore, \(\widetilde{p}(x,y_{1},\ldots\,y_{n})\) satisfies the conditions of Proposition 5.2 which states that the conditional distribution \(\widetilde{p}(x|y_{1},\ldots\,y_{n})\) is realized by the classifier-guided policy

\[p_{F}(s^{\prime}|s,y_{1},\ldots,y_{n})=p_{M,F}(s^{\prime}|s)\frac{\widetilde{ p}(y_{1},\ldots,y_{n}|s^{\prime})}{\widetilde{p}(y_{1},\ldots,y_{n}|s)}= \frac{\widetilde{p}(y_{1},\ldots\,y_{n}\,|\,s^{\prime})}{\widetilde{p}(y_{1}, \ldots\,y_{n}\,|\,s)}\sum_{i=1}^{m}p_{i,F}(s^{\prime}|s)\widetilde{p}(y\!=\! i|s).\] (82)

### Detailed Derivation of Classifier Training Objective

This section provides a more detailed step-by-step derivation of the non-terminal state classifier training objective (18).

Step 1.Our goal is to train a classifier \(\widetilde{Q}(y_{1},\ldots\,y_{n}|s)\). This classifier can be obtained as the optimal solution of

\[\min_{\phi}\mathbb{E}_{\widehat{\tau},\widehat{y}_{1},\ldots\widehat{y}_{n} \sim\widetilde{p}(\tau,y_{1},\ldots,y_{n})}\big{[}\ell(\widehat{\tau}, \widehat{y}_{1},\ldots,\widehat{y}_{n};\phi)\big{]},\] (83)

where \(\ell(\cdot)\) is defined in equation (17). An unbiased estimate of the loss (and its gradient) can be obtained by sampling \((\widehat{\tau},\widehat{y}_{1},\ldots,\widehat{y}_{n})\) and evaluating (17) directly. However sampling tuples \((\tau,y_{1},\ldots,y_{n})\) is not straightforward. The following steps describe our proposed approach to the estimation of expectation in (83).

Step 2.The expectation in (83) can be expressed as

\[\mathbb{E}_{\widehat{\tau},\widehat{y}_{1}\sim\widetilde{p}(\tau,y_{1})} \left[\sum_{\widehat{y}_{2}=1}^{m}...\sum_{\widehat{y}_{n}=1}^{m}\left(\prod _{i=2}^{n}\widetilde{p}(y_{i}\!=\!\widehat{y}_{i}|x\!=\!\widehat{x})\right) \ell(\widehat{\tau},\widehat{y}_{1},\ldots,\widehat{y}_{n};\phi)\right],\] (84)

where we re-wrote the expectation over \((y_{2},\ldots,y_{n})|\tau\) as the explicit sum of the form \(\mathbb{E}_{q(z)}[g(z)]=\sum_{z\in Z}q(z)g(z)\). The expectation over \((\tau,y_{1})\) can be estimated by sampling pairs \((\widehat{\tau},\widehat{y}_{1})\) as described in the paragraph after equation (17): 1) \(\widehat{y}_{1}\sim\widetilde{p}(y_{1})\) and 2) \(\widehat{\tau}\sim p_{\widehat{y}_{1}}(\tau)\). The only missing part is the probabilities \(\widetilde{p}(y_{i}\!=\!\widehat{y}_{i}|x\!=\!\widehat{x})\) which are not directly available.

Step 3.Our proposal is to approximate these probabilities as \(\widetilde{p}(y_{1}\!=\!j|x\!=\!\widehat{x})\approx w_{j}(\widehat{x};\phi)= \widetilde{Q}_{\phi}(y_{1}\!=\!j|x\!=\!\widehat{x})\). The idea here is that the terminal state classifier \(\widetilde{Q}_{\phi}(y_{1}|x)\), when trained to optimality, produces outputs exactly equal to the probabilities \(\widetilde{p}(y_{1}|x)\), and the more the classifier is trained the better is the approximation of the probabilities.

Step 4.Steps 1-3, give a procedure where the computation of the non-terminal state classification loss requires access to the terminal state classifier. As we described in the paragraph preceding equation (18), we propose to train non-terminal and terminal classifiers simultaneously and introduce "target network" parameters. The weights \(w\) are computed by the target network \(\widetilde{Q}_{\widetilde{\phi}}\).

Combining all the steps above, we arrive at objective (18) which we use to estimate the expectation in (83).

Note that equation (18) involves summation over \(\widehat{y}_{2},\ldots\widehat{y}_{n}\) with \(m^{n-1}\) terms in the sum. If values of \(n\) and \(m\) are small, the sum can be evaluated directly. In general, one could trade off estimation accuracy for improved speed by replacing the summation with Monte Carlo estimation. In this case, the values \(\widehat{y}_{k}\) are sampled from the categorical distributions \(Q_{\overline{\phi}}(y|x)\). Note that labels can be sampled in parallel since \(y_{i}\) are independent given \(x\).

### Assumptions and Proof of Proposition a.1

This subsection provides a formal statement of the assumptions and a more detailed formulation of Proposition a.1.

The assumptions, the formulation of the result, and the proof below closely follow those of Theorem 1 of Peluchetti [53]. Theorem 1 in [53] generalizes the result of Brigo [57] (Corollary 1.3), which derives the SDE for mixtures of 1D diffusion processes.

We found an error in the statement and the proof of Theorem 1 (Appendix A.2 of [53]). The error makes the result of [53] for \(D\)-dimensional diffusion processes disagree with the result of [57] for 1-dimensional diffusion processes.

Here we provide a corrected version of Theorem 1 of [53] in a modified notation and a simplified setting (mixture of finite rather than infinite number of diffusion processes). Most of the content is directly adapted from [53].

Notation:

* for a vector-valued \(f:\mathbb{R}^{D}\to\mathbb{R}^{D}\), the divergence of \(f\) is denoted as \(\nabla\cdot(f(x))=\sum_{d=1}^{D}\frac{\partial}{\partial x_{d}}f_{d}(x)\),
* for a scalar-values \(a:\mathbb{R}^{D}\to\mathbb{R}\), the divergence of the gradient of \(a\) (the Laplace operator) is denoted by \(\Delta(a(x))=\nabla\cdot(\nabla a(x))=\sum_{d=1}^{D}\frac{\partial^{2}}{ \partial x_{d}^{2}}a(x)\).

**Assumption 1** (SDE solution).: A given \(D\)-dimensional SDE\((f,g)\):

\[dx_{t}=f_{t}(x_{t})dt+g_{t}dw_{t},\] (85)

with associated initial distribution \(p_{0}(x)\) and integration interval \([0,T]\) admits a unique strong solution on \([0,T]\).

**Assumption 2** (SDE density).: A given \(D\)-dimensional SDE\((f,g)\) with associated initial distribution \(p_{0}(x)\) and integration interval \([0,T]\) admits a marginal density on \((0,T)\) with respect to the \(D\)-dimensional Lebesgue measure that uniquely satisfies the Fokker-Plank (Kolmogorov-forward) partial differential equation (PDE):

\[\frac{\partial p_{t}(x)}{\partial t}=-\nabla\cdot(f_{t}(x)p_{t}(x))+\frac{1} {2}\Delta(g_{t}^{2}p_{t}(x)).\] (86)

**Assumption 3** (positivity).: For a given stochastic process, all finite-dimensional densities, conditional or not, are strictly positive.

**Theorem D.1** (Diffusion mixture representation).: _Consider the family of \(D\)-dimensional SDEs on \(t\in[0,T]\) indexed by \(i\in\{1,\ldots,m\}\),_

\[dx_{i,t}=f_{i,t}(x_{i,t})dt+g_{i,t}dw_{i,t},\qquad x_{i,0}\sim p_{i,0},\] (87)

_where the initial distributions \(p_{i,0}\) and the Wiener processes \(w_{i,t}\) are all independent. Let \(p_{i,t}\), \(t\in(0,T)\) denote the marginal density of \(x_{i,t}\). For mixing weights \(\{\omega_{i}\}_{i=1}^{m}\), \(\omega_{i}\geq 0\), \(\sum_{i=1}^{m}\omega_{i}=1\), define the mixture marginal density \(p_{M,t}\) for \(t\in(0,T)\) and the mixture initial distribution \(p_{M,0}\) by_

\[p_{M,t}(x)=\sum_{i=1}^{m}\omega_{i}p_{i,t}(x)\quad p_{M,0}(x)=\sum_{i=1}^{m} \omega_{i}p_{M,0}(x).\] (88)

_Consider the \(D\)-dimensional SDE on \(t\in[0,T]\) defined by_

\[f_{M,t}(x)=\frac{\sum_{i=1}^{m}\omega_{i}p_{i,t}(x)f_{i,t}(x)}{p_{M,t}(x)}, \qquad g_{M,t}(x)=\sqrt{\frac{\sum_{i=1}^{m}\omega_{i}p_{i,t}(x)g_{i,t}^{2}}{ p_{M,t}(x)}},\] (89)\[dx_{t}=f_{M,t}(x_{t})dt+g_{M,t}(x_{t})dw_{t},\qquad x_{M,0}\sim p_{M,0}.\] (90)

_It is assumed that all diffusion processes \(x_{i,t}\) and the diffusion process \(x_{M,t}\) satisfy the regularity Assumptions 1, 2, and 3. Then the marginal distribution of the diffusion \(x_{M,t}\) is \(p_{M,t}\)._

Proof.: For \(0<t<T\) we have that

\[\frac{\partial p_{M,t}(x)}{\partial t}=\frac{\partial}{\partial t }\left(\sum_{i=1}^{m}\omega_{i}p_{i,t}(x)\right)\] (91) \[=\sum_{i=1}^{m}\omega_{i}\frac{\partial p_{i,t}(x)}{\partial t}\] (92) \[=\sum_{i=1}^{m}\omega_{i}\left(-\nabla\cdot(f_{i,t}(x)p_{i,t}(x) )+\frac{1}{2}\Delta(g_{i,t}^{2}p_{i,t}(x))\right)\] (93) \[=\sum_{i=1}^{m}\omega_{i}\left(-\nabla\cdot\left(\frac{p_{i,t}(x )f_{i,t}(x)}{p_{M,t}(x)}p_{M,t}(x)\right)+\frac{1}{2}\Delta\left(\frac{p_{i,t} (x)g_{i,t}^{2}}{p_{M,t}(x)}p_{M,t}(x)\right)\right)\] (94) \[=-\nabla\cdot\left(\sum_{i=1}^{m}\frac{\omega_{i}p_{i,t}(x)f_{i, t}(x)}{p_{M,t}(x)}p_{M,t}(x)\right)+\frac{1}{2}\Delta\left(\sum_{i=1}^{m} \frac{\omega_{i}p_{i,t}(x)g_{i,t}^{2}}{p_{M,t}(x)}p_{M,t}(x)\right)\] (95) \[=-\nabla\cdot(f_{M,t}(x)p_{M,t}(x))+\frac{1}{2}\Delta(g_{M,t}^{2 }p_{M,t}(x)).\] (96)

The second is an exchange of the order of summation and differentiation, the third line is the application of the Fokker-Planck PDEs for processes \(x_{i,t}\), the fourth line is a rewriting in terms of \(p_{M,t}\), the fifth line is another exchange of the order of summation and differentiation. The result follows by noticing that \(p_{M,t}(x)\) satisfies the Fokker-Planck equation of SDE\((f_{M},g_{M})\). 

Proof of Proposition a.1.Below, we show that the result of Proposition a.1 follows from Theorem d.1.

First, we rewrite \(f_{M,t}(x_{t})\) and \(g_{M,t}(x_{t})\) in (89) in terms of the classifier probabilities (21):

\[f_{M,t}(x_{t})=\frac{\sum_{i=1}^{m}\omega_{i}p_{i,t}(x_{t})f_{i, t}(x_{t})}{p_{M,t}(x_{t})}=\sum_{i=1}^{m}p(y\!=\!i|x_{t})f_{i,t}(x_{t}),\] (97) \[g_{M,t}(x_{t})=\sqrt{\frac{\sum_{i=1}^{m}\omega_{i}p_{i,t}(x_{t })g_{i,t}^{2}}{p_{M,t}(x_{t})}}=\sqrt{\sum_{i=1}^{m}p(y\!=\!i|x_{t})g_{i,t}^{2 }}.\] (98)

With these expressions, we apply the result of Theorem d.1 to the base forward processes \(dx_{i,t}=f_{i,t}(x_{i,t})\,dt+g_{i,t}\,dw_{i,t}\) and obtain the mixture forward process in equation (19).

From the forward process, we derive the backward process following Song et al. [34]. Using the result of Anderson [38], the backward process for (19) is given by

\[dx_{t}=\left[f_{M,t}(x_{t})-\nabla_{x_{t}}(g_{M,t}^{2}(x_{t}))-g_{M,t}^{2}(x_ {t})\nabla_{x_{t}}\log p_{M,t}(x_{t})\right]dt+g_{M,t}(x_{t})\,d\overline{w}_{t}.\] (99)

Note that the term \(\nabla_{x_{t}}(g_{M,t}^{2}(x_{t}))\) is due to the fact that the diffusion coefficient \(g_{M,t}(x_{t})\) in (19) is a function of \(x\) (cf., equation (16), Appendix A in [34]). This term can be transformed as follows

\[\nabla_{x_{t}}(g_{M,t}^{2}(x_{t}))=\nabla_{x_{t}}\left(\sum_{i=1} ^{m}p(y\!=\!i|x_{t})g_{i,t}^{2}\right)\] (100) \[=\sum_{i=1}^{m}g_{i,t}^{2}\nabla_{x_{t}}p(y\!=\!i|x_{t})\] (101) \[=\sum_{i=1}^{m}p(y\!=\!i|x_{t})g_{i,t}^{2}\nabla_{x_{t}}\log p(y \!=\!i|x_{t})\] (102)\[=\sum_{i=1}^{m}p(y\!=\!i|x_{t})g_{t,t}^{2}\nabla_{x_{t}}\Big{(}\log \omega_{t}+\log p_{t,t}(x_{t})-\log p_{M,t}(x_{t})\Big{)}\] (103) \[=\sum_{i=1}^{m}p(y\!=\!i|x_{t})g_{t,t}^{2}\Big{(}\nabla_{x_{t}} \log p_{t,t}(x_{t})-\nabla_{x_{t}}\log p_{M,t}(x_{t})\Big{)}\] (104) \[=\Bigg{(}\sum_{i=1}^{m}p(y\!=\!i|x_{t})g_{t,t}^{2}s_{t,t}(x_{t}) \Bigg{)}-g_{M,t}^{2}(x_{t})\nabla_{x_{t}}\log p_{M,t}(x_{t}).\] (105)

Substituting the last expression in (99), we notice that the term \(g_{M,t}^{2}(x_{t})\nabla_{x_{t}}\log p_{M,t}(x_{t})\) cancels out, and, after simple algebraic manipulations, we arrive at (20).

### Proof of Theorem a.2

We prove Theorem A.2 in the assumptions of Section D.5. In Section D.5 we established that the mixture diffusion process has the forward SDE

\[dx_{t}=f_{M,t}(x_{t})dt+g_{M,t}(x_{t})\,dw_{t}.\] (106)

and the backward SDE

\[dx_{t}=\left[f_{M,t}(x_{t})-\nabla_{x_{t}}(g_{M,t}^{2}(x_{t}))-g_{M,t}^{2}(x_ {t})\nabla_{x_{t}}\log p_{M,t}(x_{t})\right]dt+g_{M,t}(x_{t})\,d\overline{w}_{ t}.\] (107)

We apply classifier guidance with classifier \(\widetilde{p}(y_{1},\ldots,y_{n}|x_{t})\) to the mixture diffusion process, following Song et al. [34] (see equations (48)-(49) in [34]). The backward SDE of the classifier-guided mixture diffusion is

\[dx_{t}=\Big{[}f_{M,t}(x_{t})-\nabla_{x_{t}}(g_{M,t}^{2}(x_{t})) -g_{M,t}^{2}(x_{t})\Big{(}\nabla_{x_{t}}\log p_{M,t}(x_{t})+\nabla_{x_{t}}\log \widetilde{p}(y_{1},\ldots,y_{n}|x_{t})\Big{)}\Big{]}\,dt\] (108) \[\quad\quad\quad+g_{M,t}(x_{t})\,d\overline{w}_{t}.\] (109)

Finally, we arrive at (23) by substituting (105) in the above, canceling out the term \(g_{M,t}^{2}(x_{t})\nabla_{x_{t}}\log p_{M,t}(x_{t})\), and applying simple algebraic manipulations.

## Appendix E Implementation Details

### Classifier Guidance in GFlowNets

Classifier guidance in GFlowNets (14) is realized through modification of the base forward policy via the multiplication by the ratio of the classifier outputs \(n(y|s^{\prime})/n(y|s)\). The ground truth (theoretically optimal) non-terminal state classifier \(p(y|s)\) by Proposition 5.2 satisfies (13) which ensures that the guided policy (14) is valid, i.e. for any state \(s\in S\)

\[\sum_{s^{\prime}:(s\to s^{\prime})\in\mathcal{A}}p_{F}(s^{ \prime}|s,y) =\sum_{s^{\prime}:(s\to s^{\prime})\in\mathcal{A}}p_{F}(s^{ \prime}|s)\frac{p(y|s^{\prime})}{p(y|s)}\] (110) \[=\frac{1}{p(y|s)}\underbrace{\sum_{s^{\prime}:(s\to s^{\prime})\in \mathcal{A}}p_{F}(s^{\prime}|s)p(y|s^{\prime})}_{=p(y|s)\text{ by Proposition \ref{prop:p}}}=1.\] (111)

In practice, the ground truth values of \(p(y|s)\) are unavailable. Instead, an approximation \(Q_{\phi}(y|s)\approx p(y|s)\) is learned. Equation (13) might not hold for the learned classifier \(Q_{\phi}\), but we still wish to use \(\widetilde{Q}_{\phi}\) for classifier guidance in practice. In order to ensure that the classifier-guided policy is valid in practice even when the approximation \(Q_{\phi}(y|s)\) of the classifier \(p(y|s)\) is used, we implement guidance as described below.

First, we express the guided policy (14) in terms of log-probabilities:

\[\log p_{F}(s^{\prime}|s,y)=\log p_{F}(s^{\prime}|s)+\log p(y|s^{\prime})-\log p (y|s).\] (112)Parameterizing distributions through log-probabilities is common practice in probabilistic modeling: GFlowNet forward policies [36; 58] and probabilistic classifiers are typically parameterized by deep neural networks that output logits (unnormalized log-probabilities).

Second, in the log-probability parameterization the guided policy (14) can be equivalently expressed as

\[p_{F}(s^{\prime}|s,y) =\big{[}\operatorname*{softmax}\big{(}\log p_{F}(\cdot|s)+\log p (y|\cdot)-\log p(y|s)\big{)}\big{]}_{s^{\prime}}\] (113) \[=\frac{\exp\big{(}\log p_{F}(s^{\prime}|s)+\log p(y|s^{\prime})- \log p(y|s)\big{)}}{\sum_{s^{\prime\prime}:(s\to s^{\prime\prime})\in A}\exp \big{(}\log p_{F}(s^{\prime\prime}|s)+\log p(y|s^{\prime\prime})-\log p(y|s) \big{)}}.\] (114)

In theory, the softmax operation can be replaced with simple exponentiation, i.e. the numerator in (114) is sufficient on its own since Proposition 5.2 ensures that the sum in the denominator equals to 1. However, using the softmax is beneficial in practice when we substitute learned classifier \(Q_{\phi}(y|s)\) instead of the ground truth classifier \(p(y|s)\). Indeed when \(Q_{\phi}(y|s)\) does not satisfy (13), the softmax operation ensures that the guided policy

\[p_{F}(s^{\prime}|s,y)=\big{[}\operatorname*{softmax}\big{(}\log p_{F}(\cdot|s) +\log Q_{\phi}(y|\cdot)-\log Q_{\phi}(y|s)\big{)}\big{]}_{s^{\prime}}\] (115)

is valid (i.e. probabilities sum up to 1 over \(s^{\prime}\)). The fact the softmax expression is valid in theory ensures that policy (115) guided by \(Q_{\phi}(y|s)\) approaches the ground truth policy (guided by \(p(y|s)\)) as \(Q_{\phi}(y|s)\) approaches \(p(y|s)\) throughout training.

## Appendix F Experiment details

### 2D Distributions with GFlowNets

The base GFlowNet forward policies \(p_{i,F}(s^{\prime}|s;\theta)\) were parameterized as MLPs with 2 hidden layers and 256 units in each hidden layer. The cell coordinates of a state \(s\) on the 2D \(32\times 32\) grid were one-hot encoded. The dimensionality of the input was \(2\cdot 32\). The outputs of the forward policy network were the logits of the softmax distribution over 3 action choices: 1) move down; 2) move right; 3) stop.

We trained base GFlowNets with the trajectory balance loss [58]. We fixed the uniform backward policy in the trajectory balance objective. We used Adam optimizer [59] with learning rate 0.001, and pre-train the base models for 20 000 steps with batch size 16 (16 trajectories per batch). The log of the total flow \(\log Z_{\theta}\) was optimized with Adam with a learning rate 0.1. In order to promote exploration in trajectory sampling for the trajectory balance objective, we used the sampling policy which takes random actions (uniformly) with probability 0.05 but, otherwise, follows the current learned forward policy.

The classifier was parameterized as MLP with 2 hidden layers and 256 units in each hidden layer. The inputs to the classifier were one-hot encoded cell coordinates, terminal state flag (\(\{0,1\}\)), and \(\log(\nicefrac{{\alpha}}{{1-\alpha}})\) (in the case of parameterized operations). The classifier outputs were the logits of the joint label distribution \(\widetilde{Q}_{\phi}(y_{1},\dots,y_{n}|s)\) for non-terminal states \(s\) and the logits of the marginal label distribution \(\widetilde{Q}_{\phi}(y_{1}|x)\) for terminal states \(x\).

We trained the classifier with the loss described in Section 5.2. We used Adam with learning rate 0.001. We performed 15 000 training steps with batch size 64 (64 trajectories sampled from each of the base models per training step). We updated the target network parameters \(\overline{\phi}\) as the exponential moving average (EMA) of \(\phi\) with the smoothing factor 0.995. We linearly increased the weight \(\gamma\)(step) of the non-terminal state loss from 0 to 1 throughout the first 3 000 steps and kept constant \(\gamma=1\) afterward. For the \(\alpha\)-parameterized version of the classifier (Section B), we used the following sampling scheme for \(\alpha\): \(\widehat{z}\sim U[-3.5,3.5]\), \(\widehat{\alpha}=\frac{1}{1+\exp(-\widehat{z})}\).

Quantitative evaluation.For each of the composite distributions shown in Figure 2 we evaluated the L1-distance \(L_{1}(p_{\text{method}},p_{\text{GT}})=\sum_{x\in X}|p_{\text{method}}(x)-p_{ \text{GT}}(x)|\) between the distribution \(p_{\text{method}}\) induced by the classifier-guided policy and the ground-truth composition distribution \(p_{\text{GT}}\) computed from the known base model probabilities \(p_{i}\). The evaluation results are presented below.

Figure 2 **top row**. \(p_{1}\otimes p_{2}\): \(L_{1}=0.071\); \(p_{1}\mathbin{\hbox{\boldmath$\bigcirc$}}\mathbin{\hbox{\boldmath$\bigcirc$}} \mathbin{\hbox{\boldmath$\bigcirc$}}p_{2}\): \(L_{1}=0.086\); \(p_{1}\mathbin{\hbox{\boldmath$\bigcirc$}}\mathbin{\hbox{\boldmath$\bigcirc$}} \mathbin{\hbox{\boldmath$\bigcirc$}}_{0.95}\)\(p_{2}\): \(L_{1}=0.167\).

Figure 2 **bottom row**. \(\widetilde{p}(x|y_{1}=1,y_{2}=2)\): \(L_{1}=0.076\); \(\widetilde{p}(x|y_{1}=1,y_{2}=2,y_{3}=3)\): \(L_{1}=0.087\); \(\widetilde{p}(x|y_{1}=2,y_{2}=2)\): \(L_{1}=0.112\); \(\widetilde{p}(x|y_{1}=2,y_{2}=2,y_{3}=2)\): \(L_{1}=0.122\).

Figure G.6 shows the distance between the composition and the ground truth as a function of the number of training steps for the classifier as well as the terminal and non-terminal classifier learning curves.

### Molecule Generation

Domain.In the molecule generation task [36], the objects \(x\in\mathcal{X}\) are molecular graphs. The non-terminal states \(s\in\mathcal{S}\setminus\mathcal{X}\) are incomplete molecular graphs. The transitions from a given non-terminal state \(s\) are of two types: 1) fragment addition \(s\to s^{\prime}\): new molecular graph \(s^{\prime}\) is obtained by attaching a new fragment to the molecular graph \(s\); 2) stop action \(s\to x\): if \(s\neq s_{0}\), then the generation process can be terminated at the molecular graph corresponding to the current state (note that new terminal state \(x\in\mathcal{X}\) is different from \(s\in\mathcal{S}\setminus\mathcal{X}\), but both states correspond to the same molecular graph).

Rewards.We trained GFlowNets using 3 reward functions: **SEH**, a reward computed by an MPNN [60] that was trained by Bengio et al. [36] to estimate the binding energy of a molecule to the soluble epoxide hydrolase protein; **SA**, an estimate of synthetic accessibility [61] computed with tools from RDKit library [62]; **QED**, a quantitative estimate of drug-likeness [63] which is also computed with RDKit. We normalized all reward functions to the range \([0,1]\). Higher values of SEH, SA, and QED correspond to stronger binding, higher synthetic accessibility, and higher drug-likeness respectively. Following Bengio et al. [36], we introduced the parameter \(\beta\) which controls the sharpness (temperature) of the target distribution: \(p(x)\propto R(x)^{\beta}\), increasing \(\beta\) results in a distribution skewed towards high-reward objects. We experimented with two \(\beta\) values, 32 and 96 (Figure 3(a),3(d)).

Reward normalization.We used the following normalization rules for SEH, SA, and QED rewards in the molecule domain.

* \(\text{SEH}=\text{SEH}_{\text{new}}/\text{s}\);
* \(\text{SA}=\frac{10-\text{SA}_{\text{new}}}{9}\);
* \(\text{QED}=\text{QED}_{\text{raw}}\).

Training and evaluation.After training the base GFlowNets with the reward functions described above, we trained classifiers with Algorithm A.1. The classifier was parameterized as a graph neural network based on a graph transformer architecture [64]. Compared to the 2D grid domain (Section 6, "2D distributions via GFlowNet"), we can not directly evaluate the distributions obtained by our approach. Instead, we analyzed the samples generated by the composed distributions. We sampled \(5\,000\) molecules from each composed distribution obtained with our approach as well as the base GFlowNets. We evaluated the sample collections with the two following strategies. **Reward evaluation** (Figure 3, Table 1) we analyzed the distributions of rewards across the sample collections. The goal is to see whether the composition of GFlowNets trained for different rewards leads to noticeable changes in reward distribution. **Distribution distance evaluation** (Figure 4, Table G.5) : we used the samples to estimate the pairwise distances between the distributions. Specifically, for a given pair of distributions represented by two collections of samples \(D_{A}=\{x_{A,i}\}_{i=1}^{n}\), \(D_{B}=\{x_{B,i}\}_{i=1}^{n}\) we computed the earth mover's distance \(d(D_{A},D_{B})\) with ground molecule distance given by \(d(x,x^{\prime})=(\max\{s(x,x^{\prime}),10^{-3}\})^{-1}-1\), where \(s(x,x^{\prime})\in[0,1]\) is the Tanimoto similarity over Morgan fingerprints of molecules \(x\) and \(x^{\prime}\).

Training details and hyperparameters.The base GFlowNet policies were parameterized as graph neural networks with Graph Transformer architecture [64]. We used 6 transformer layers with an embedding of size 128. The input to the Graph Transformer was the graph of fragments with node attributes describing fragments and edge attributes describing attachment points of the edges.

The base GFlowNets were trained with trajectory balance loss. We used Adam optimizer. For the policy network \(p_{F}(s^{\prime}|s;\theta)\), we set the initial learning rate 0.0005 and exponentially decayed with the factor \(2^{\text{step}/200000}\). For the log of the total flow \(\log\,Z_{\theta}\) we set the initial learning rate 0.0005 and exponentially decayed with the factor \(2^{\nicefrac{{1}{1}{1}}{1}}\). We trained the base GFlowNets for \(15\,000\) steps with batch size \(64\) (\(64\) trajectories per batch). In order to promote exploration in trajectory sampling for the trajectory balance objective, we used the sampling policy which takes random actions (uniformly) with probability \(0.1\) but, otherwise, follows the current learned forward policy.

The classifier was parameterized as a graph neural network with Graph Transformer architecture. We used \(4\) transformer layers with embedding size \(128\). The inputs to the classifier were the fragment graph, terminal state flag (\(\{0,1\}\)), and \(\log(\nicefrac{{a}}{{1}}(1-\nicefrac{{a}}{{0}}))\) (in case of parameterized operations). The classifier outputs were the logits of the joint label distribution \(\widetilde{Q}_{\phi}(y_{1},\dots,y_{n}|s)\) for non-terminal states \(s\) and the logits of the marginal label distribution \(\widetilde{Q}_{\phi}(y_{1}|x)\) for terminal states \(x\).

We trained the classifier with the loss described in Section 5.2. We used Adam with learning rate \(0.001\). We performed \(15\,000\) training steps with batch size \(8\) (\(8\) trajectories sampled from each of the base models per training step). We updated the target network parameters \(\overline{\phi}\) as the exponential moving average (EMA) of \(\phi\) with the smoothing factor \(0.995\). We linearly increased the weight \(\gamma\)(step) of the non-terminal state loss from \(0\) to \(1\) throughout the first \(4\,000\) steps and kept constant \(\gamma=1\) afterward. For the \(\alpha\)-parameterized version of the classifier (Section B), we used the following sampling scheme for \(\alpha\): \(\widehat{z}\sim U[-5.5,5.5]\), \(\widehat{\alpha}=\frac{1}{1+\exp(-\widehat{z})}\).

### Colored MNIST Generation via Diffusion Models

The colored MNIST experiment in Section 6 ("Colored MNIST generation via diffusion models") follows the method for composing diffusion models introduced in Appendix A.2. The three base diffusion models were trained on colored MNIST digits generated from the original MNIST dataset. These colored digits were created by mapping MNIST images from their grayscale representation to either the red or green channel, leaving the other channels set to \(0\). For Figure 5 we post-processed the red and green images generated by the base models and compositions into beige and cyan respectively, which are more accessible colors for colorblind people.

Models, training details, and hyperparameters.The base diffusion models were defined as VE SDEs [34]. Their score models were U-Net [65] networks consisting of \(4\) convolutional layers with \(64\), \(128\), \(256\), and \(256\) channels and \(4\) matching transposed convolutional layers. Time was encoded using \(256\)-dimensional Gaussian random features [66]. The score model was trained using Adam optimizer [59] with a learning rate decreasing exponentially from \(10^{-2}\) to \(10^{-4}\). We performed \(200\) training steps with batch size \(32\).

The first classifier \(\widetilde{Q}(y_{1},y_{2}|x_{t})\) was a convolutional network consisting of \(2\) convolutional layers with \(64\) and \(96\) channels and three hidden layers with \(512\), \(256\) and \(256\) units. This classifier is time-dependent and used \(128\)-dimensional Gaussian random features to embed the time. The output was a \(3\)x\(3\) matrix encoding the predicted log-probabilities. The classifier was trained on trajectories sampled from the reverse SDE of the base diffusion models using the AdaDelta optimizer [67] with a learning rate of \(1.0\). We performed \(700\) training steps with batch size \(128\). For the first \(100\) training steps the classifier was only trained on terminal samples.

The second conditional classifier \(\widetilde{Q}(y_{3}|y_{1},y_{2},x_{t})\) was a similar convolutional network with \(2\) convolutional layers with \(64\) channels and two hidden layers with \(256\) units. This classifier is conditioned both on time and on \((y_{1},y_{2})\). The time variable was embedded used \(128\)-dimensional Gaussian random features. The \((y_{1},y_{2})\) variables were encoded using a \(1\)-hot encoding scheme. The output of the classifier was the three predicted log-probabilities for \(y_{3}\). Contrary to the first classifier, this one was not trained on the base diffusion models but rather on samples from the posterior \(\widetilde{p}(x|y_{1},y_{2})\). It's loss function was:

\[\mathcal{L}_{c}(\phi)=\underset{(\widehat{x}_{0},\widehat{y}_{3},\widehat{y}_ {2},\widehat{y}_{1},t)\sim\widetilde{p}(x_{0},x_{t}|y_{1},y_{2})\widetilde{p} (y_{1})\widetilde{p}(y_{2})p(t)}{\mathbb{E}}\left[\sum_{\widehat{y}_{3}=1}^{m }-\nicefrac{{\omega}}{{\widehat{y}_{3}}}(\widehat{x}_{0})\log\widetilde{Q}_{ \phi}(\widehat{y}_{3}|\widehat{y}_{1},\widehat{y}_{2},\widehat{x}_{t})\right]\] (116)

where \(\nicefrac{{\omega}}{{\widehat{y}_{3}}}(\widehat{x}_{0})\) is estimated using the first classifier. The classifier was trained using the AdaDelta optimizer [67] with a learning rate of \(0.1\). We performed \(200\) training steps with batch size \(128\).

Sampling.Sampling from both the individual base models and the composition was done using the Predictor-Corrector sampler [34]. We performed sampling over \(500\) time steps to generate the samples shown in Figure 5. The samples used to train the classifier were generated using the same method.

When sampling from the composition we found that using scaling for the classifier guidance was generally necessary to achieve high-quality results. Without scaling, the norm of the gradient over the first and second classifier was too small relative to the gradient predicted by the score function, and hence did not sufficiently steer the mixture towards samples from the posterior. Experimentally, we found that scaling factor 10 for the first classifier and scaling factor 75 for the second produced high quality results.

## Appendix G Additional Results

### Binary Operations for MNIST Digit Generation via Diffusion Models

Here we present a variant of the colored digit generation experiment from Section 6 ("Colored MNIST generation via diffusion models"). using 2 diffusion models. This allows us to better illustrate the harmonic mean and contrast operations on this image domain. In a similar fashion to the experiment in Section 6 ("Colored MNIST generation via diffusion models"), we trained two diffusion models to generate colored MNIST digits. \(p_{1}\) was trained to generate red and green 0 digits and \(p_{2}\) was trained to generate green 0 and 1 digits. As before, we used post-processing to map green to cyan and red to beige.

Implementation details.The diffusion models used in this experiment and their training procedure were exactly the same as in Section F.3. The sampling method used to obtain samples from the base models and their compositions was also the same. We found that scaling the classifier guidance was generally required for high-quality results, and used a scaling factor of 20 in this experiment.

The classifier was a convolutional network with 2 convolutional layers consisting of 32 and 64 channels and two hidden layers with 512 and 128 units. The classifier's time input was embedded using 128-dimensional Gaussian random features. The output was a 2x2 matrix encoding the predicted log-probabilities \(\widetilde{Q}(y_{1},y_{2}\,|\,x_{t})\). The classifier was trained on trajectories, sampled from the reverse SDE of the base diffusion models, using the AdaDelta optimizer [67] with a learning rate of 0.1 and a decay rate of 0.97. We performed 200 training steps with batch size 128. For the first 100 training steps, the classifier was only trained on terminal samples.

Results.Figure G.1 shows samples obtained from the two trained diffusion models \(p_{1}\), \(p_{2}\) and from the harmonic mean and contrast compositions of these models. We observe that the harmonic mean generates only cyan zero digits, because this is the only type of digit on which both \(p_{1}\) and \(p_{2}\) have high density. The contrast \(p_{1}\)\(\bigtriangledown\)\(p_{2}\) generates beige zero digits from \(p_{1}\). However, unlike \(p_{1}\), it does not generate cyan zero digits, as \(p_{2}\) has high density there. The story is similar for \(p_{1}\)\(\bigtriangledown\)\(p_{2}\), which generates cyan one digits from \(p_{2}\), but not zero digits due to \(p_{1}\) having high density over those.

### MNIST Subset Generation via Diffusion Models

We report in this section on additional results for composing diffusion models on the standard MNIST dataset. We trained two diffusion models: \(p_{\{0,\dots,5\}}\) was trained to generate MNIST digits 0 through 5,

Figure G.1: **Diffusion model composition on colored MNIST.** (a,b) samples from base diffusion models. (c-e) samples from the resulting harmonic mean and contrast compositions.

[MISSING_PAGE_FAIL:33]

### Chaining: Sequential Composition of Multiple Distributions

We present results on chaining binary composition operations sequentially on a custom colored MNIST dataset.

Setup.We start with three base generative models that are trained to generate \(p_{1}\), \(p_{2}\) and \(p_{3}\) in Figure 5. Specifically, \(p_{1}\) is a uniform distribution over digits \(\{0,1,2,3,4,5\}\), \(p_{2}\) is a uniform distribution over even digits \(\{0,2,4,6,8\}\), and \(p_{3}\) is a uniform distribution over digits divisible by 3: \(\{0,3,6,9\}\). Note that we use a different color for each digit consistent across \(p_{1},p_{2},p_{3}\). Our goal is to produce combinations of chained binary operations involving all three distributions, where two of them were combined first, then in a second step, combined with the third distribution through either harmonic mean \(\bigotimes\) or contrast \(\bigtriangledown\).

Binary Classifier Training.Consider, for example, the operation \((p_{1}\bigotimes p_{2})\bigtriangledown p_{3}\). We use the same classifier training procedure for \(p_{1}\) versus \(p_{2}\), as well as the composite model \((p_{1}\bigotimes p_{2})\) versus \(p_{3}\), except that in the later case we sample from composite model as a whole. Our classifier training simultaneously optimizes the terminal classifier and the intermediate state classifier.

Implementation Details.We adapted diffusion model training code for the base distributions from [33]. Our diffusion model used a UNet backbone with four latent layers on both the contracting and the expansive path. The contracting channel dimensions were [256, 64, 128, 64] with the kernel size 3, and strides [1, 2, 2, 2]. The time embedding used a mixture of 128 sine and cosine feature pairs, with a total of 256 dimensions. These features were passed through a sigmoid activation and then expanded using a different linear head for each layer. The activations were then added to the 2D feature maps at each layer channel-wise. We used a fixed learning rate of 0.01 with Adam optimizer [59].

We adapted the classifier training code from the MNIST example in Pytorch [68]. Our binary classifier has two latent layers with channel dimensions [32, 64], stride 1 and kernel width of 3. We use dropout on both layers: \(p_{1}=25\%,p_{2}=50\%\). We train the network for 200 epochs on data sampled online in batches of 256 from each source model, and treat the composite model in the second step the same way. We use the Adadelta [67] optimizer with the default setting of learning rate 1.0.

Sampling.We generate samples according to Appendix A.2 and multiply the gradient by \(\alpha=20\).

Results.Row 3 from Figure 5 contains mostly zeros with a few exceptions. This is in agreement with harmonic mean being symmetric. In \(p_{1}\bigotimes(p_{2}\bigotimes p_{3})\), digits that are outside of the intersection still appear with thin strokes.

Figure G.5: **Chaining Binary Operations.** (a-c) Samples from 3 pre-trained diffusion models. (d-l) Samples from binary compositions. (row 3) The harmonic mean between all three. (row 4 and beyond) various ways to chain the operations. Parentheses indicate the order of composition.

### Classifier Learning Curves and Training Time

We empirically evaluated classifier training time and learning curves. The results are shown in Figures 10, 11, 12 and Tables 1, 13.

Figures 10, 11, 12 show the cross-entropy loss of the classifier for terminal (16) and non-terminal states (18) as a function of the number of training steps for the GFlowNet 2D grid domain, the molecular generation domain, and the Colored MNIST digits domain respectively. They show that the loss drops quickly but remains above 0. Figure 11 further shows the distances between the learned compositions and the ground truth distributions as a function of the number of training steps of the classifier. For all compositions, as the classifier training progresses, the distance to the ground truth distribution decreases. Compared to the distance at initialization we observe almost an order of magnitude distance reduction by the end of the training.

The runtime of classifier training is shown in Tables 1 and 13. We report the total runtime, as well as separate measurements for the time spent sampling trajectories and training the classifier. The classifier training time is comparable to the base generative model training time. However, most of the classifier training time (more than 70%, or even 90%) was spent on sampling trajectories from base generative models. Our implementation of the training could be improved in this regard, e.g. by sampling a smaller set of trajectories once and re-using trajectories for training and by reducing the number of training steps (the loss curves in Figures 10, 11, 12 show that classification losses plateau quickly).

### Analysis of Sample Diversity of Base GFlowNets in Molecule Generation Domain

In order to assess the effect of the reward exponent \(\beta\) on mode coverage and sample diversity, we evaluated samples generated from GFlowNets pre-trained with different values of \(\beta\). The results are in Tables 3 and 4. The details of the evaluation and the reported metrics are described in the table captions. As expected, larger reward exponents shift the learned distributions towards high-scoring molecules (the total number of molecules with scores above the threshold increases). For 'SA' and 'QED' models we don't observe negative effects of large \(\beta\) on sample diversity and mode coverage: the average pairwise similarity of top 1 000 molecules doesn't grow as \(\beta\) increases and the ratio of Tanimoto-separated modes remains high. For 'SEH' models we observe a gradual increase in the average pairwise similarity of top 1 000 molecules and a gradual decrease in the ratio of Tanimoto-separated modes. However, the total number of separated modes grows as \(\beta\) increases, which indicates that larger reward exponents do not lead to mode dropping.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & SEH & \multicolumn{2}{c}{SA} & \multicolumn{2}{c}{QED} \\ \hline \(\beta\) = 1 & 15 / & 17 & 37 / & 37 & 0 / & 0 \\ \(\beta\) = 4 & 12 / & 17 & 82 / & 82 & 0 / & 0 \\ \(\beta\) = 10 & 85 / & 109 & 332 / & 337 & 18 / & 18 \\ \(\beta\) = 16 & 190 / & 280 & 886 / & 910 & 253 / & 253 \\ \(\beta\) = 32 & 992 / & 1821 & 2859 / & 3080 & 3067 / & 3124 \\ \(\beta\) = 96 & 1619 / & 4609 & 4268 / & 4983 & 4470 / & 4980 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Number of Tanimoto-separated modes found above reward threshold. For each combination (reward, \(\beta\)) a GFlowNet was trained with the corresponding reward \(R(x)^{\beta}\), and then 5 000 molecules were generated. Cell format is \(A\)/\(B\), where \(A\) is the number of Tanimoto-separated modes found above the reward threshold, and \(B\) is the total number of generated molecules above the threshold. Analogously to Figure 14 in [36], we consider having found a new mode representative when a new molecule has Tanimoto similarity smaller than 0.7 to every previously found modes representative molecule. Reward thresholds (in \([0,1]\), normalized values) are SEH: 0.875, SA: 0.75, QED: 0.75. Note that the normalized threshold of 0.875 for SEH corresponds to the unnormalized threshold of 7 used in [36].

\begin{table}
\begin{tabular}{l l l} \hline \hline Base GFlowNet training steps & 20 000 \\ Base GFlowNet batch size & 64 \\ Base GFlowNet training dataset replaced real time & 64 \\ Base GFlowNet training dataset replaced real time & 64 \\ Classifier training steps & 15 000 \\ Classifier batch size & 8 \\ \(\beta\)N trajectories per base model & 14 (all states used) \\ Classifier training total elapsed real time & 9th 2m 19s \\ Classifier training data generation time & 6h 35m 58k (73\%) \\ \hline \hline \end{tabular} 
\begin{tabular}{l l} \hline \hline Base diffusion training steps & 200 \\ Base diffusion batch size & 32 \\ Base diffusion training elapsed real time & 10m 6s \\ \hline Classifier training steps & 200 \\ Classifier batch size & 128 trajectories per base model (85 time-steps per trajectory) \\ Classifier training total elapsed real time & 30m 12s \\ Classifier training data generation time & 29m 22s (97\%) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Summary of base diffusion and classifier training time in MNIST image generation domain. The experimental setup corresponds to Section G.1 and Figure 1. All models were trained with a single Tesla V100 GPU.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & SEH & \multicolumn{2}{c}{SA} & \multicolumn{2}{c}{QED} \\ \hline \(\beta\) = 1 & 0.527 & 0.539 & 0.480 \\ \(\beta\) = 4 & 0.529 & 0.527 & 0.464 \\ \(\beta\) = 10 & 0.535 & 0.500 & 0.438 \\ \(\beta\) = 16 & 0.548 & 0.465 & 0.422 \\ \(\beta\) = 32 & 0.585 & 0.411 & 0.398 \\ \(\beta\) = 96 & 0.618

[MISSING_PAGE_EMPTY:38]