# On Softmax Direct Preference Optimization for Recommendation

Yuxin Chen\({}^{1}\)1 Junfei Tan\({}^{2}\)1 An Zhang\({}^{1}\)2

Zhengyi Yang\({}^{2}\) Leheng Sheng\({}^{1}\) Enzhi Zhang\({}^{3}\)

Xiang Wang\({}^{2}\) Tat-Seng Chua\({}^{1}\)

\({}^{1}\)National University of Singapore

\({}^{2}\)University of Science and Technology of China

\({}^{3}\)Hokkaido University

yuxin.chen@u.nus.edu, sober_clever@mail.ustc.edu.cn,anzhang@u.nus.edu

leheng.sheng@u.nus.edu, enzhi.zhang.n6@elms.hokudai.ac.jp

yangzhy1998@gmail.com, xiangwang1223@gmail.com, dcscts@nus.edu.sg

Footnote 1: These authors contributed equally to this work.

Footnote 2: An Zhang is the corresponding author.

###### Abstract

Recommender systems aim to predict personalized rankings based on user preference data. With the rise of Language Models (LMs), LM-based recommenders have been widely explored due to their extensive world knowledge and powerful reasoning abilities. Most of the LM-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target response and fine-tuning LM with a language modeling loss. However, the current objective fails to fully leverage preference data and is not optimized for personalized ranking tasks, which hinders the performance of LM-based recommenders. Inspired by the current advancement of Direct Preference Optimization (DPO) in human preference alignment and the success of softmax loss in recommendations, we propose Softmax-DPO (**S-DPO**) to instill ranking information into the LM to help LM-based recommenders distinguish preferred items from negatives, rather than solely focusing on positives. Specifically, we incorporate multiple negatives in user preference data and devise an alternative version of DPO loss tailored for LM-based recommenders, which is extended from the traditional full-ranking Plackett-Luce (PL) model to partial rankings and connected to softmax sampling strategies. Theoretically, we bridge S-DPO with the softmax loss over negative sampling and find that it has an inherent benefit of mining hard negatives, which assures its exceptional capabilities in recommendation tasks. Empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of S-DPO to effectively model user preference and further boost recommendation performance while providing better rewards for preferred items. Our codes are available at https://github.com/chenyuxin1999/S-DPO.

## 1 Introduction

Recommender systems aim to predict personalized rankings based on user preference data, _i.e.,_ historical interactions such as purchases, clicks, and ratings [1, 2]. Recently, leveraging the extensive world knowledge and powerful reasoning abilities of language models (LMs) [3, 4, 5, 6], LM-based recommenders have been broadly explored [7, 8, 9]. These recommenders convert historical interaction data into language prompts and either perform in-context learning or fine-tune LMs, demonstrating notableadvantages, including zero-shot and few-shot reasoning [10, 11, 12, 13], enhanced generalization abilities [14, 15], and rich semantic understanding [16, 17, 18, 19]. However, current LM-based recommenders typically utilize language modeling loss for personalized ranking objectives--predicting the next token--which significantly differs from the objective of modeling user preferences in recommendation tasks [20, 21].

We argue that the current objective of LM-based recommenders does not fully utilize preference data and is not optimized for personalized ranking tasks, thereby hindering recommendation performance. Most LM-based recommenders address recommendation tasks by leveraging specialized language prompts [14, 17, 22, 23, 24], incorporating collaborative signals as a new modality [18, 19, 25], or extending the vocabulary of LMs with item tokens [16, 26, 27, 28, 29, 30]. Typically, these recommenders pair each language prompt, including the user's historical interaction item lists, with a single positive item and then update LM parameters using language modeling loss [14, 19]. Despite being designed for recommendation tasks, these LM-based recommenders do not consider negative items and are not directly optimized for personalized rankings. Such a training paradigm fails to fully leverage user preference data and overlooks the role of negative items in recommendations, thereby impeding the alignment of LMs with user preferences.

Inspired by the success of using human-labeled data to align LMs with human preferences [31, 32, 33] and advancements in direct preference optimization (DPO) [34, 35, 36], we make progress on aligning LMs with recommendations by fine-tuning them to predict the next item in accordance with the user's preference. This preference alignment stage aims to instill ranking information into the LMs and help recommenders distinguish preferred items from negatives, rather than solely focus on positives.

Towards this end, we incorporate multiple negatives in user preference data and devise an alternative version of DPO loss tailored for recommendation, connected to softmax sampling strategies [37, 38, 20, 39], which we call **S-DPO**. Specifically, we first devise supervised fine-tuning to inject domain knowledge and improve LM's ability to follow the instructions before preference alignment phase, following [14, 33]. In the preference alignment stage, instead of constructing solely positive pairs, we initially pair each language prompt with both positive and randomly sampled multiple negatives to build text-based preference data. Building upon these preference data, we extend conventional DPO with the Bradley-Terry preference model [34, 40] on pairwise data to the Plackett-Luce preference model [41, 42], which handles relative rankings among multiple samples. Furthermore, we generalize the traditional Plackett-Luce preference model, which is designed for full relative rankings, to accommodate partial rankings, a more natural fit for recommendation tasks.

Benefiting from the multiple negatives in preference data, S-DPO offers three appealing properties. On the one hand, S-DPO serves as the first specialized personalized ranking loss for LM-based recommenders, effectively utilizing multiple negatives and acknowledging the importance of preference data. Empirically, we demonstrate that it provides more effective ranking gradients and better rewards for preferred items compared with DPO (_cf._ Section 4.2). On the other hand, we theoretically bridge the DPO loss with the pairwise BPR loss [43] over pairwise data and connect S-DPO with the softmax loss over negative sampling (also known as contrastive loss in self-supervised recommendations, which achieves state-of-the-art performance [44, 45, 37]). This connection naturally underscores the ranking performance of S-DPO and highlights the critical role of multiple negatives. Furthermore, gradient analysis demonstrates that S-DPO has an inherent benefit of mining hard negative examples similar to contrastive learning paradigm [38], which not only boosts the performance but also accelerates the training process (_cf._ Section 3.1), assuring its exceptional capabilities in recommendation tasks.

Overall, our contributions can be concluded as follows:

* We are among the first to point out that the widely used language modeling loss in LM-based recommendation is not designed for ranking tasks and fails to fully utilize user preference data, thereby hindering recommendation performance.
* We propose S-DPO, an alternative version of DPO loss extended from the traditional Plackett-Luce preference model, incorporating multiple negatives to instill ranking information into LM and tailoring for LM-based recommenders.
* We theoretically bridge S-DPO with the softmax loss over negative sampling to highlight the critical role of multiple negatives and find its inherent benefit of mining hard negatives, assuring its capabilities.

## 2 Preliminary

In this section, we first formalize sequential recommendation as the task of aligning language models (LMs) with user preferences. Then, we discuss the general framework of current LM-based recommenders that utilizes language modeling loss to fine-tune LMs. Finally, we outline the training process widely used to align LMs with human preferences, including reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO).

Task Formulation.Given the historical interactions \(\mathcal{H}_{u}\) of one user \(u\) in chronological order, the goal of LM-based sequential recommender \(\mathcal{M}_{\theta}\), where \(\theta\) represents trainable parameters, is to select the item \(i_{p}\) preferred by user \(u\) from candidate set \(C=\{i_{j}\}_{j=1}^{N}\), where \(N\) is the number of candidates. This task requires that item \(i_{p}\) be preferred over the other candidate items, denoted by \(\mathcal{I}_{d}=C\backslash\{i_{p}\}\). This requirement explicitly defines a multi-negative preference understanding for LM-based recommenders, which can be formulated as follows:

\[\forall i_{d}\in\mathcal{I}_{d},\quad i_{p}\succ_{u}i_{d},\] (1)

wherein \(\succ_{u}\) stands for the preference of user \(u\).

Fine-tuning LM-based recommenders.Current LM-based recommenders widely adopt supervised fine-tuning (SFT) [33] on recommendation-specific data to enhance their performance [7; 9]. Generally, this involves two steps: structuring recommendation data as text-based pairs and then fine-tuning LMs based on these pairs. In the first step, for user \(u\), a recommendation task prompt \(x_{u}\) encompasses the user's historical interactions \(\mathcal{H}_{u}\), the candidate item set \(C\), and a description of the sequential recommendation task. This prompt \(x_{u}\) is paired with the title of the preferred item \(i_{p}\) in the candidate set \(C\), denoted as \(e_{p}\), to form the pair data \((x_{u},e_{p})\). In the second step, the \((x_{u},e_{p})\) pairs are utilized to fine-tune the LM-based recommender \(\mathcal{M}_{\theta}\) through language modeling loss. This loss, commonly used in SFT in language modeling tasks, implicitly treats the recommendation task as predicting the next token based on preceding tokens. Formally, the objective of optimizing the LM-based recommender \(\mathcal{M}_{\theta}\) with pair data \((x_{u},e_{p})\) can be formulated as:

\[\max_{\theta}\sum_{(x_{u},e_{p})}\sum_{t=1}^{|e_{p}|}\log(P_{\theta}((e_{p})_{ t}|x_{u},{(e_{p})}_{<t}),\] (2)

where \(|e_{p}|\) is the number of tokens in \(e_{p}\), \((e_{p})_{t}\) is the \(t\)-th token of \(e_{p}\) and \((e_{p})_{<t}\) is the tokens preceding \((e_{p})_{t}\).

However, recommendation tasks are essentially user preference alignment tasks, as formalized in the above task formulation, and differ from language modeling tasks that consider only positive responses. Such a gap necessitates further exploration into aligning LM-based recommenders with user preference, an area that has been underexplored.

RLHF pipeline and DPO.Recent studies in natural language processing (NLP) explore the use of human-labeled pairwise data as a reward signal to align LMs with human preferences, such as RLHF [33] and DPO [34]. Specifically, the RLHF [33] pipeline adds two additional phases after the SFT phase: reward model training and reinforcement learning (RL) optimization. After obtaining the SFT model \(\pi^{\mathrm{SFT}}\), RLHF further optimizes it with pairwise preference data.

Figure 1: Framework of S-DPO. Different from existing methods which fine-tune LMs with a language modeling loss without tailoring for recommendations, S-DPO proposes to explicitly instill ranking information into LMs. To take one step further, S-DPO incorporates multiple negatives in user preference data and generalizes pairwise DPO loss to softmax ranking loss.

Inspired by the success of RLHF in NLP, we leverage RLHF to inject recommendation-specific user pairwise preference into LM-based recommenders. Let \(\mathcal{E}=\{e_{j}\}_{j=1}^{N}\) denote the title set of candidate items, where \(e_{j}\) denotes the title of item \(i_{j}\). Given two items \(i_{j},i_{k}\in\mathcal{C}\), the user preference \(i_{j}>_{u}i_{k}\) can be seamlessly transformed into a response preference, stipulating that \(e_{j}\) is preferred over \(e_{k}\) when presented with prompt \(x_{u}\), denoted as \(e_{j}\succ e_{k}|x_{u}\). By sampling one misperferred item \(i_{d}\) from misperferred candidate set \(\mathcal{I}_{d}\), we can curate a preference dataset \(\{(e_{p},e_{d},x_{u})\}\).

After that, RLHF utilizes a preference model for preference distribution modeling, such as Bradley-Terry (BT) model [40]. This preference model assumes there is a latent function \(r(x_{u},e_{j})\) representing the reward of prompt-response pair \((x_{u},e_{j})\). The bigger reward \(r(x_{u},e_{j})\) means the more user \(u\) prefers item \(i\). From this perspective, reward function \(r(x_{u},e_{j})\) serves as a scoring function that quantifies the preference of user \(u\) to item \(i\). Besides, the preference model defines a mapping from the reward function \(r(x_{u},e_{j})\) to a preference distribution \(p_{r}(e_{j}\succ e_{k}|x_{u})\). Based on preference distribution, an optimal reward function is trained by maximizing the likelihood of preference data. The training objective of this phase is as follows:

\[\mathcal{L}_{\mathrm{RM}}=-\mathbb{E}_{(x_{u},e_{p},e_{d})}[\log p_{r}(e_{p} \succ e_{d}|x_{u})].\] (3)

Let \(\pi_{\theta}(e|x_{u})\) be the probability that LM-based recommender \(\mathcal{M}_{\theta}\) output title \(e\) given prompt \(x_{u}\). The final reinforcement learning phase aims to maximize the expected reward of policy while not deviate too far from the reference model, formulating the following objective for optimal policy:

\[\max_{\pi_{\theta}}\mathbb{E}_{x_{u}\sim\mathcal{D},e\sim\pi_{ \theta}(e|x_{u})}[r(x_{u},e)]-\beta\mathbb{D}_{\mathrm{KL}}[\pi_{\theta}(e|x_{ u})||\pi_{\mathrm{ref}}(e|x_{u})],\] (4)

where \(\mathcal{D}\) denotes the distribution of \(x_{u}\) and \(\pi_{\mathrm{ref}}=\pi^{\mathrm{SFT}}\).

A recent study, DPO [34], theoretically proves the optimal policy in a closed form to Eq.(4) is

\[\pi^{*}(e|x_{u})=\frac{1}{Z(x_{u})}\pi_{\mathrm{ref}}(e|x_{u}) \mathrm{exp}\left(\frac{1}{\beta}r(x_{u},e)\right),\] (5)

which is equivalent to

\[r(x_{u},e)=\beta\log\frac{\pi(e|x_{u})}{\pi_{\mathrm{ref}}(e|x_{ u})}+\beta\log Z(x_{u}),\] (6)

where \(Z(x_{u})=\sum_{e}\pi_{\mathrm{ref}}(e|x_{u})\mathrm{exp}\left(\frac{1}{\beta}r (x_{u},e)\right)\) is the partition function.

By defining \(p_{r}(e_{p}\succ e_{d}|x_{u})\) as \(\sigma(r(x_{u},e_{p})-r(x_{u},e_{d}))\) in Eq.(3) according to the BT model used in RLHF and substituting term \(r(x_{u},e)\) in Eq.(3) with Eq.(6), the last two phases of RLHF pipeline can be equivalently transformed into optimizing DPO loss below:

\[\mathcal{L}_{\mathrm{DPO}}=-\mathbb{E}_{(x_{u},e_{p},e_{d})}\left[\log\sigma \left(\beta\log\frac{\pi_{\theta}(e_{p}|x_{u})}{\pi_{\mathrm{ref}}(e_{p}|x_{u })}-\beta\log\frac{\pi_{\theta}(e_{d}|x_{u})}{\pi_{\mathrm{ref}}(e_{d}|x_{u})} \right)\right],\] (7)

wherein \(\sigma(x)\) is the sigmoid function.

DPO is able to directly extract the optimal policy from pairwise preference data, making it more practical for preference alignment than RLHF. Nevertheless, DPO and RLHF are usually designed for pairwise preference. The oversight of other negative items impedes the performance of the LM-based recommenders. To bridge the gap, we expand DPO to S-DPO in recommendation tasks, in consideration of multiple negative items.

## 3 Methodology

### Derivation of S-DPO loss

To align LM-based recommender \(\mathcal{M}_{\theta}\) with multi-negative preference, we first derive the preference distribution and then propose a new loss function called S-DPO (depicted in Figure 1).

Multi-negative Preference Distribution.As mentioned in Section 2, for user \(u\), there is a partial ranking stipulating \(i_{p}\succ_{u}i_{d},\forall\,i_{d}\in\mathcal{I}_{d}\) in sequential recommendation tasks. Let \(\mathcal{E}_{d}\) be the titles of dispreferred items \(\mathcal{I}_{d}\). The aforementioned partial ranking is equivalent to \(e_{p}\succ e_{d}|x_{u},\forall e_{d}\in\mathcal{E}_{d}\), from which a multi-negative preference dataset \(\{x_{u},e_{p},\mathcal{E}_{d}\}\) can be curated in an analogous way to RLHF.

For the dataset pairing one preferred item with multiple dispreferred items, we leverage the Plackett-Luce (PL) model [41, 42] to build preference distribution. Given prompt \(x_{u}\), \(K\) titles \(e_{1},e_{2},\cdots,e_{K}\) and a permutation \(\tau:[K]\rightarrow[K]\) reflecting the user preference, with \(\tau(j)\) denoting the \(j\)-th element of permutation \(\tau\), the PL model estimates that the ranking \(e_{\tau(1)},e_{\tau(2)},\cdots,e_{\tau(K)}\) turns out true, as:

\[p(\tau|e_{1},e_{2},\cdots,e_{K},x_{u})=\prod_{j=1}^{K}\frac{\exp \left(r(x_{u},e_{\tau(j)})\right)}{\Sigma_{l=j}^{K}\mathrm{exp}(r\left(x_{u},e _{\tau(l)})\right)}.\] (8)

By enumerating all the permutations starting with \(p\) and calculating sum of their probability given by the PL model, the final multi-negative preference distribution \(p^{*}\) can be derived as:

\[p^{*}(e_{p}\succ e_{d},\forall e_{d}\in\mathcal{E}_{d}|x_{u})= \frac{\exp(r(x_{u},e_{p}))}{\sum_{j=1}^{K}\exp(r(x_{u},e_{j}))}.\] (9)

For brevity, the complete derivation is delegated to Appendix A.1.

Deriving S-DPO.By substituting reward function \(r(x_{u},e)\) in Eq.(9) with Eq.(6), the multi-negative preference distribution can be rewritten as:

\[p^{*}(e_{p}\succ e_{d},\forall\,e_{d}\in\mathcal{E}_{d}|x_{u})= \frac{1}{1+\sum_{e_{d}\in\mathcal{E}_{d}}\exp\left(\beta\log\frac{\pi(e_{d}|x_ {u})}{\pi_{\mathrm{ref}}(e_{d}|x_{u})}-\beta\log\frac{\pi(e_{p}|x_{u})}{\pi_{ \mathrm{ref}}(e_{p}|x_{u})}\right)}.\] (10)

Through plugging distribution given by Eq.(10) in the reward learning objective in Eq.(3), our S-DPO loss can be formulated for policy \(\pi_{\theta}\) as:

\[\mathcal{L}_{\mathrm{S-DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})=- \mathbb{E}_{(x_{u},e_{p},\mathcal{E}_{d})\sim\mathcal{D}}\left[\log\sigma \left(-\mathrm{log}\,\sum_{e_{d}\in\mathcal{E}_{d}}\exp\left(\beta\log\frac{ \pi_{\theta}(e_{d}|x_{u})}{\pi_{\mathrm{ref}}(e_{d}|x_{u})}-\beta\log\frac{ \pi_{\theta}(e_{p}|x_{u})}{\pi_{\mathrm{ref}}(e_{p}|x_{u})}\right)\right) \right].\] (11)

Notably, when the number of candidates \(N\) is 2, which means there is only one dispreferred item, S-DPO reduces to DPO. The proof is provided in Appendix A.2.

Gradient Analysis.We conduct gradient analysis on S-DPO. The gradient of \(\mathcal{L}_{\mathrm{S-DPO}}\) with respect to parameters \(\theta\) takes the following formulation:

\[\nabla_{\theta}\mathcal{L}_{\mathrm{S-DPO}}(\pi_{\theta};\pi_{ \mathrm{ref}})=\] \[-\beta\mathbb{E}_{(x_{u},e_{p},\mathcal{E}_{d})}\bigg{[}\underbrace {\sigma\left(\log\sum_{e_{d}\in\mathcal{E}_{d}}\exp(g(e_{d},e_{p},x_{u})) \right)}_{\text{higher weight when reward deviates from preference}}\cdot\bigg{[}\nabla_{ \theta}\log\pi_{\theta}(e_{p}|x_{u})-\sum_{e_{d}\in\mathcal{E}_{d}} \underbrace{\nabla_{\theta}\log\pi_{\theta}(e_{d}|x_{u})}_{\text{higher weight when reward is larger}}\bigg{]},\]

wherein \(g(e_{j},e_{k},x_{u})=r_{\theta}(x_{u},e_{j})-r_{\theta}(x_{u},e_{k})\) and similar to DPO, \(r_{\theta}(x_{u},e)=\beta\mathrm{log}\frac{\pi_{\theta}(e|x_{u})}{\pi_{\mathrm{ ref}}(e|x_{u})}\) is the implicit reward function defined by \(\pi_{\theta}\). See Appendix A.3 for a complete derivation.

Recap the DPO gradient below:

\[\nabla_{\theta}\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})=- \beta\mathbb{E}_{(x_{u},e_{p},e_{d})}\bigg{[}\underbrace{\sigma(g(e_{d},e_{p},x_ {u}))}_{\text{higher weight when reward is wrong}}\cdot[\nabla_{\theta}\log\pi_{ \theta}(e_{p}|x_{u})-\nabla_{\theta}\log\pi_{\theta}(e_{d}|x_{u})]\bigg{]}.\]

Similar to DPO, the gradient of S-DPO loss increases the likelihood of the preferred item and decreases the likelihood of all the dispreferred items. Each example is also weighed by how much the implicit reward \(r(x_{u},e)\) deviates from the preference data. However, compared with DPO, S-DPO harnesses information of multiple dispreferred items in this weight.

Moreover, S-DPO treats gradients of different negative (dispreferred) items differently by assigning the gradient of each negative item with an extra weight \(\frac{1}{\sum_{s^{\prime}_{d}\in\mathcal{I}}\exp(g(e^{\prime}_{d},e_{d},x_{u}) )}=\frac{\exp(rg(x_{u},e_{d}))}{\sum_{s^{\prime}_{d}\in\mathcal{I}}\exp(rg(x_{u },e^{\prime}_{d}))}\). This term reflects the relative reward of each negative item compared with other negative items. Similar to [38], we can categorize negative items into two categories: (1) Hard negative items, whose reward \(r_{\theta}(x_{u},e_{d})=\beta\frac{\pi_{\theta}(e_{d}|x_{u})}{\pi_{\text{ref}} (e_{d}|x_{u})}\) is relatively high, making it more probable to be chosen by LM-based recommenders; (2) Easy negative items, whose reward \(r_{\theta}(x_{u},e_{d})\) is relatively low, making it less likely to be output. For hard negative items, the extra weight term \(\frac{\exp(rg(x_{u},e_{d}))}{\sum_{s^{\prime}_{d}\in\mathcal{I}}\exp(rg(x_{u },e^{\prime}_{d}))}\) tends to be larger, leading to more decline for likelihood. This mechanism makes LM-based recommenders more discriminative and endows S-DPO with more effectiveness and stability than DPO.

### Properties of S-DPO

In this section, we will discuss the structural correlation between DPO and BPR [43], together with S-DPO and softmax loss [39], which demonstrates the advantage of S-DPO over DPO and language modeling loss.

For user \(u\), preferred item \(i_{p}\) and one dispreferred \(i_{d}\in\mathcal{I}_{d}\), BPR loss takes the form:

\[\mathcal{L}_{\text{BPR}}=-\mathbb{E}_{(u,i_{p},i_{d})}\left[\log\sigma\left(f( u,i_{p})-f(u,i_{d})\right)\right],\] (12)

wherein \(f(u,i)\) represents preference score of user \(u\) for item \(i\).

Similarly, given dispreferred item set \(\mathcal{I}_{d}\), the softmax loss takes the form:

\[\mathcal{L}_{\text{softmax}}=-\mathbb{E}_{(u,i_{p},\mathcal{I}_{d})}\left[ \log\sigma\left(-\log\sum_{i_{d}\in\mathcal{I}_{d}}\exp\left(f(u,i_{d})-f(u,i _{p})\right)\right)\right].\] (13)

Review the DPO loss in Eq.(7) and S-DPO loss in Eq.(11). Notably, term \(\beta\log\frac{\pi_{\theta}(e|x_{u})}{\pi_{\text{ref}}(e|x_{u})}\) is the implicit reward function, denoted by \(r_{\theta}(x_{u},e)\) in Section 3.1. According to Section 2, \(r_{\theta}(e,x_{u})\) reflects the preference of user \(u\) to item \(i\) corresponding to title \(e\). When the reference model has no knowledge about recommendation, _i.e.,_ when \(\pi_{\text{ref}}(e|x_{u})\) is approximately a uniform distribution, term \(r_{\theta}(x_{u},e)=\beta\log\frac{\pi_{\theta}(e|x_{u})}{\pi_{\text{ref}}(e| x_{u})}\) exactly reveals absolute preference. Hence, \(r_{\theta}(x_{u},e)\) possesses a similar function to \(f(u,i)\).

From this perspective, DPO and S-DPO can be seen as special patterns of BPR and softmax loss, respectively. Given the effectiveness of BPR and InfoNCE loss in recommendation, we argue that sampled-based loss which explicitly compares preferred and dispreferred items such as DPO and S-DPO is more suitable for training LM-based recommenders than only utilizing language modeling loss. Moreover, as softmax loss works better than BPR loss in multi-negative scenarios [20], it can be inferred that S-DPO will be more tailored for multi-negative user preference alignment than DPO.

## 4 Experiments

In this section, we aim to answer the following research questions:

* **RQ1:** How does S-DPO compare with traditional and LM-based sequential recommendation models on performance?
* **RQ2:** How does the LM-based recommender benefit from the multiple negatives in S-DPO?
* **RQ3:** What are the impacts of the essential parameters (\(\beta\)) on S-DPO?

Baselines.We thoroughly compare S-DPO with three categories of recommenders in sequential recommendations: traditional recommenders (GRU4Rec [46], Caser [47], SASRec [48]), LM-enhanced recommenders (MoRec [49]) and LM-based recommenders (LLaMA2 [32], Chat-REC[15], TALLRec [14], LLaRA [19]). See detailed introduction and comparison of baselines in Appendix B.

Datasets.We conduct extensive experiments on three real-world benchmark datasets which differ in size and domain (Movielens [50], Goodreads3, and LastFM [51]). Following standard settings of [15, 19], we employ a commonly used metric Hit Ratio@1 (HR@1) for performance evaluation and an additional metric Valid Ratio to evaluate the LM-based methods' ability to generate appropriate responses. See detailed introductions of datasets and evaluation metrics in Appendix B.

Footnote 3: https://www.goodreads.com

Implementation.We implement all LM-based recommenders on 4 NVIDIA A100 GPUs. For all LM-based recommenders, we conduct a supervised fine-tuning stage for a maximum of 5 epochs. For S-DPO and its variants, we conduct a preference alignment stage for a further 3 epochs. Different from existing methods, we only optimize loss on item titles and find it effective in recommendation tasks. Refer to Appendix B for more implementation details.

### Overall Performance Comparison (RQ1)

Table 1 presents a comparative analysis of the performance of our proposed S-DPO and baselines. Bold and underlined indicate the best and the second-best performance, respectively. We observe that:

* **LM-based recommenders have driven impressive performance breakthroughs compared with traditional recommenders.** Our results reveal that traditional recommenders outperform untuned LM-based recommenders (LLaMA, ChatRec) but fall short compared to LM-based recommenders fine-tuned on historical interactions (TALLRec and LLaRA). It is noted that untuned LM-based recommenders are limited by inadequate instruction-following capabilities (indicated by a low valid ratio) or a lack of domain-specific knowledge (indicated by a suboptimal performance), which highlights the necessity of the supervised fine-tuning stage to further ground the inherent ability of language models down to sequential recommendation tasks. Moreover, MoRec also exhibits suboptimal performance compared to its traditional variant because it leaves the reasoning ability of LM untouched. The superior performance of recent LM-based recommenders indicates the significant roles of knowledge and reasoning ability in language models for recommendation tasks in semantically informative datasets, which highlights the potential of LM-based recommenders.
* **Tailoring language models for recommendation task further boosts the performance of LM-based recommenders.** For LM-based recommenders, the substantial performance gap between fine-tuned and untuned approaches emphasizes the importance of tailoring models for recommendations. TALLRec adapts LM for recommendation by supervised fine-tuning LM on historical interactions, surpassing traditional recommenders. Additionally, LLaRA consistently outperformed TALLRec across all datasets, suggesting that introducing collaborative signals through appropriate item representations is a viable direction for further adapting LM. However, existing LM-based methods adapt LM from either item representation methods or corpus construction, leaving the adaptation of optimization objectives unexplored. Instead, S-DPO aligns the language model with multi-negative

\begin{table}
\begin{tabular}{c|c|c c c|c c|c c c} \hline \hline  & & \multicolumn{3}{c|}{Goodreads} & \multicolumn{3}{c|}{LastFM} & \multicolumn{3}{c}{MovieLens} \\  & & HR@1 & ValidRatio & Rel.Ipv & HR@1 & ValidRatio & Rel.Ipv & HR@1 & ValidRatio & Rel.Ipv \\ \hline Traditional & GRU4Rec & 0.3867 & 1.0000 & _70.91\%_ & 0.2616 & 1.0000 & _153.36\%_ & 0.3750 & 1.0000 & _40.35\%_ \\  & Caser & 0.4174 & 1.0000 & _58.34\%_ & 0.2233 & 1.0000 & _196.82\%_ & 0.3861 & 1.0000 & _36.31\%_ \\  & SASRec & 0.3581 & 1.0000 & _84.56\%_ & 0.2233 & 1.0000 & _196.82\%_ & 0.3444 & 1.0000 & _52.82\%_ \\ \hline LM-based & LLaMA2 & 0.0233 & 0.3845 & _2736.48\%_ & 0.02046 & 0.3443 & _2594.31\%_ & 0.0421 & 0.4421 & _1150.12\%_ \\  & ChatRec & 0.3306 & 1.0000 & _99.91\%_ & 0.3770 & 1.0000 & _75.81\%_ & 0.2000 & 0.9895 & _163.15\%_ \\  & MoRec & 0.2877 & 1.0000 & _129.72\%_ & 0.1652 & 1.0000 & _301.21\%_ & 0.2822 & 1.0000 & _86.50\%_ \\  & TALLRec & 0.4983 & 0.9573 & _32.63\%_ & 0.4180 & 0.9836 & _58.56\%_ & 0.3895 & 0.9263 & _35.12\%_ \\  & LLaRA & 0.5292 & 0.9950 & 24.89\% & 0.4508 & 0.9918 & _47.03\%_ & 0.4737 & 0.9684 & _11.10\%_ \\ \hline Ours & S-DPO & **0.6609** & 0.9900 & - & **0.6628** & 0.9992 & - & **0.5263** & 0.9895 & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: The performance comparison on three real-world datasets. “Rel.Ipv” denotes the relative improvement of S-DPO compared with baselines.

user preference data by extending DPO to include a softmax ranking loss, making it a more appropriate loss function for recommendation tasks.
* **S-DPO consistently outperforms all traditional recommenders and the state-of-the-art LM-based recommenders on all datasets.** S-DPO shows an improvement ranging from 11.10% to 47.03% on Hit Ratio@1 compared to the second-best baseline. Building on a supervised fine-tuning stage, we observe a further improvement to the preference alignment stage, which explicitly instills ranking information into LM and utilizes preference data with multiple negative samples. Such superior performance suggests that explicitly tailoring LM for recommendation using user preference data at the training objective level is more effective than other LM-based recommenders. By leveraging the inherent abilities of the LM and incorporating ranking information from user preference data, S-DPO effectively differentiates between preferred and less preferred items. Notably, the preference alignment stage hardly harms the inherent ability of LM, illustrated by a high valid ratio.

### Study on S-DPO

Ablation Study.To investigate the effect of explicit ranking optimization and multiple negative samples of S-DPO, we compare it with the vanilla supervised fine-tuned model (w/o ranking), and a variant of S-DPO with only a single negative sample (w/o multi-neg), downgrading to pairwise DPO loss. The experimental results are reported in Figure 1(a). We can observe that DPO can achieve an overall better performance compared to SFT, which underscores the effectiveness of instilling ranking relationships into existing LM-based recommenders. With a more effective ranking gradient provided by multiple negative samples, S-DPO can further boost performance and achieve the best among all baseline methods and variants.

Study on the number of negative samples (RQ2).Benefiting from the utilization of multiple negative pairs in preference data, our S-DPO offers two empirically appealing properties compared to DPO: 1) S-DPO has more effective gradients facilitating the optimization; 2) S-DPO provides a better boost for rewards of preferred items compared to DPO. Figure 1(b) provides the comparison of validation loss between S-DPO and DPO, illustrating that the loss of S-DPO decreases faster and more significantly. This observation demonstrates that multiple negative pairs provide larger and more meaningful gradients for model optimization, which is attributed to the inherent benefit of S-DPO to mine negative samples [38] (_cf._ Section 3.1).

On the other hand, we study the behavior of S-DPO which is illustrated in Figure 1(c). We surprisingly find that S-DPO exhibits continually increasing rewards of preferred items that are more significant and stable than DPO, which shows better effectiveness in distinguishing preferred items and a potential of mitigating data likelihood decline issues[52, 53].

To further verify the superiority of the multiple negative samples of S-DPO compared with DPO, we select the number of negative samples from {1, 3, 5, 8, 10, 15} to conduct experiments to explore the potential of the number of negative samples, with the results depicted in Figure 2(a). It can be observed that utilizing multiple negative samples allows the model to achieve better performance than with a single one. Furthermore, as the number of negative samples increases, the model's performance exhibits continual improvements. We attribute this success of S-DPO to more effective ranking

Figure 2: Study on S-DPO. (1(a)) Ablation study of S-DPO compared with SFT and DPO on three datasets. (1(b)) Comparison of the trend of validation loss between DPO and S-DPO on LastFM. (1(c)) Comparison of the reward of preferred items between DPO and S-DPO on LastFM.

gradients provided by multiple negatives which can be connected to the superior performance of contrastive loss in self-supervised recommendations [38, 39, 54].

To validate the superiority of S-DPO over the DPO variant with multi-negatives, we conduct effectiveness and efficiency comparisons. Table 2 demonstrates that introducing more negative samples benefits both DPO and S-DPO, and S-DPO achieves comparable performance with fewer training steps. We further analyze how S-DPO outperforms DPO in terms of computational efficiency. While the complexity of DPO for \(K\) negative samples is \(\Theta(2KC_{\mathcal{M}}S_{t})\), where \(C_{\mathcal{M}}+1\) denotes the base LLM's computational complexity and \(S_{t}\) represents the size of the training data, S-DPO's complexity for the same number of negatives is reduced to \(\Theta((K+1)(C_{\mathcal{M}}+1)S_{t})\). This efficiency gain can be expressed by scaling the complexity of DPO by the factor \(\frac{1}{2}+\frac{1}{2K}+\frac{1}{2C_{\mathcal{M}}}+\frac{1}{2KC_{\mathcal{M}}}\), highlighting that S-DPO offers significant advantages, especially when working with a larger number of negative samples.

Study on values of \(\beta\) (RQ3).In S-DPO, \(\beta\) is a hyperparameter controlling the deviation of LM from the base reference policy [34]. Typically, a smaller value of \(\beta\) implies that the language model is more heavily influenced by the preference signals and vice versa. In this section, we select the value of \(\beta\) from {0.1, 0.5, 1, 3, 5} to explore the effect of \(\beta\) on S-DPO. As indicated in Figure 2(b) through 2(c), a higher \(\beta\) can achieve overall better performance in our task, while a lower \(\beta\) may overwhelm the model's learned knowledge from the supervised fine-tuning stage, as evidenced by both low valid ratio and hit ratio. On the other hand, an excessively large \(\beta\) prevents the model from effectively learning ranking relationships, leading to suboptimal performance. In all our main experiments and studies, we set \(\beta\) as 1 to achieve a balance between ranking signals and inherent knowledge of language models.

## 5 Related Work

### LM for Recommendation

Recent advancements in recommendation systems have increasingly incorporated Language Models (LMs) due to their extensive knowledge and robust reasoning abilities. This integration occurs primarily in two forms: LM-enhanced recommenders and LM-based recommenders. LM-enhanced recommenders utilize LM embedding as semantic representations to provide contrastive signals [55, 56, 57, 58] or utilize LM as advanced feature extractors improving the representation of user and

\begin{table}
\begin{tabular}{l|c c c c c|c|c} \hline \hline
**Datasets** & \multicolumn{3}{c}{**LastFM**} & \multicolumn{3}{c}{**MovieLens**} & \multicolumn{2}{c|}{**Goodreads**} & \multicolumn{1}{c}{**Complexity**} \\ \hline
**Measure** & **HitRatio@1** & **ValidRatio** & **HitRatio@1** & **ValidRatio** & **HitRatio@1** & **ValidRatio** & \\ \hline
**DPO-1negative** & 0.6342 & 0.9972 & 0.4947 & 0.9684 & 0.6381 & 0.9900 & \(\Theta(2KC_{\mathcal{M}}S_{t})\) \\
**DPO-\(k\)negative** & 0.6413 & 0.9964 & 0.4947 & 0.9474 & 0.6628 & 0.9900 & \(\Theta(2KC_{\mathcal{M}}S_{t})\) \\
**S-DPO-\(k\)negative** & **0.6477** & 0.9980 & **0.5263** & 0.9895 & **0.6661** & 0.9950 & \(\Theta((K+1)(C_{\mathcal{M}}+1)S_{t})\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Effectiveness comparison between DPO with single negative, a variant of DPO with multiple negatives and S-DPO with the same number of negatives (we set \(K\) as 3 to get the performance in this table).

Figure 3: Studies on values of \(\beta\) and negative samples numbers of S-DPO on LastFM. (2(a)) Performance comparisons with varying numbers of negative samples (\(\beta=1\)). (2(b)) Performance comparisons with varying values of \(\beta\) setting negative samples number as 3. (2(c)) Validity comparisons with varying values of \(\beta\) setting negative samples number as 3.

item features [59; 60; 61]. However, these systems still rely on traditional recommenders for the final recommendation task, which leaves the reasoning ability of LM largely untouched.

On the other hand, LM-based recommenders directly employ LMs for making recommendations. Early works leverage LMs' in-context learning capabilities for zero-shot or few-shot recommendations, demonstrating significant potential [10; 11; 12; 15]. However, untuned LM-based recommenders are limited by inadequate instruction-following capabilities and a lack of domain-specific knowledge. To bridge this gap, recent efforts in this category include supervised fine-tuning of LMs on the historical interactions to enhance their performance in recommendation tasks [14; 17; 23; 24; 27]. More recently, researchers have discovered that exploring item representation methods in the finetuning phase may further boost LM's ability for recommendation [30]. This branch of works includes integrating collaborative signals [18; 19; 25; 62; 63; 64], adjusting numeric representations [22; 65; 66] or introducing additional item tokens [16; 26; 28; 29].

However, existing finetuned methods follow the training objective of language generation without any specific adjustments for personalized ranking. Different from them, S-DPO proposes to explicitly optimize item ranking information on preference data.

### Preference Alignment of Language Models

Reinforcement Learning from Human Feedback (RLHF) [31; 32; 33] is a prevalent method of LMs to learn from human preferences. The RLHF pipeline comprises reward model training and reinforcement learning (RL) optimization, the latter of which suffers instability and inefficiency. Direct Preference Optimization (DPO) [34] bypasses the brittle RL phase via a particular reward model parameterization and is thus simpler to implement while still keeping the performance of RLHF.

DPO proves to be effective in many scopes, like NLP [34; 67] and multimodal LMs [35; 68; 69; 70]. Besides, several variants have been proposed for further improvement of DPO. \(\Psi\)PO [71] is a generalization of DPO loss and its representative IPO can better overcome the problem of overfitting. ODPO [36] treats preference pairs differently by stipulating that the likelihood gap of two responses should be greater than a corresponding offset value. KTO [72] utilizes prospect theory for preference alignment tasks. Other variants including GPO [73], \(f\)-DPO [74], RSO [75] also enhance or expand DPO in various aspects. Despite these contributions, the possibilities for leveraging and further adapting DPO for recommendation are still largely unexplored and few studies discuss extending DPO to handle multi-negative scenarios.

## 6 Limitation

Despite effectiveness, there are several limitations not addressed in this paper. On the one hand, the number of negative samples is capped at 15 in our experiments. The potential of multiple negative samples hasn't been fully explored due to the limited time and computation resources. On the other hand, increasing the number of negative examples inevitably results in higher training costs, a phenomenon that becomes more pronounced as the number of negative examples grows in the context of language models.

## 7 Conclusion

In this work, we devised a principled Softmax-DPO (S-DPO) loss specially tailored for LM-based recommenders, utilizing multiple negatives in preference data to explicitly instill ranking information into LM. Empirically, S-DPO surpasses all baseline models including traditional and LM-based methods on three datasets in sequential recommendation tasks while successfully providing better rewards for preferred items compared to DPO. Grounded by theoretical proof, we bridge S-DPO with the softmax loss in self-supervised recommendations, underscoring the ranking performance of S-DPO and highlighting the critical roles of multiple negatives. Also, we theoretically find that S-DPO has an inherent benefit to mine hard negatives which provide larger and more effective gradients to model optimization, assuring its exceptional capabilities in recommendation tasks. We believe that S-DPO, as a generalization of DPO, provides valuable insights for future LM-based recommenders and has the potential to benefit research fields other than recommender systems4.

## Acknowledgments and Disclosure of Funding

This research is supported by the National Science and Technology Major Project (2023ZD0121102), the NExT Research Center, National Natural Science Foundation of China (92270114). This research is also supported by the advanced computing resources provided by the Supercomputing Center of the USTC.

## References

* [1] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. Deep learning for sequential recommendation: Algorithms, influential factors, and evaluations. _ACM Trans. Inf. Syst._, 2020.
* [2] Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z. Sheng, and Mehmet A. Orgun. Sequential recommender systems: Challenges, progress and prospects. In _IJCAI_, 2019.
* [3] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. _CoRR_, abs/2303.18223, 2023.
* [4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _NeurIPS_, 2020.
* [5] Jingnan Zheng, Han Wang, An Zhang, Tai D. Nguyen, Jun Sun, and Tat-Seng Chua. Ali-agent: Assessing llms' alignment with human values via agent-based evaluation. In _NeurIPS_, 2024.
* [6] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, _NeurIPS_, 2022.
* [7] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. Recommender systems in the era of large language models (llms). _CoRR_, abs/2307.02046, 2023.
* [8] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. How can recommender systems benefit from large language models: A survey. _CoRR_, abs/2306.05817, 2023.
* [9] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. A survey on large language models for recommendation. _CoRR_, abs/2305.19860, 2023.
* [10] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. Is chatgpt a good recommender? A preliminary study. _CoRR_, abs/2304.10149, 2023.
* [11] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. Uncovering chatgpt's capabilities in recommender systems. In _RecSys_, 2023.
* [12] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian J. McAuley, and Wayne Xin Zhao. Large language models are zero-shot rankers for recommender systems. In _ECIR_, 2024.

* [13] An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. On generative agents in recommendation. _CoRR_, abs/2310.10108, 2023.
* [14] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In _RecSys_, 2023.
* [15] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. Chat-rec: Towards interactive and explainable llms-augmented recommender system. _CoRR_, abs/2303.14524, 2023.
* [16] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for recommender systems. _CoRR_, abs/2311.01343, 2023.
* [17] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. Recommendation as instruction following: A large language model empowered recommendation approach. _CoRR_, abs/2305.07001, 2023.
* [18] Zhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and Xiangnan He. Large language model can interpret latent space of sequential recommender. _CoRR_, abs/2310.20487, 2023.
* [19] Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, and Xiang Wang. Llara: Aligning large language models with sequential recommenders. _CoRR_, abs/2312.02445, 2023.
* [20] Steffen Rendle. Item recommendation from implicit feedback. In _Recommender Systems Handbook_, pages 143-171. Springer US, 2022.
* [21] Lanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Mingchen Cai, Wayne Xin Zhao, and Ji-Rong Wen. Prompting large language models for recommender systems: A comprehensive framework and empirical analysis. _CoRR_, abs/2401.04997, 2024.
* [22] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language processing (RLP): A unified pretrain, personalized prompt & predict paradigm (P5). In _RecSys_, 2022.
* [23] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. M6-rec: Generative pretrained language models are open-ended recommender systems. _CoRR_, abs/2205.08084, 2022.
* [24] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation. _CoRR_, abs/2308.11131, 2023.
* [25] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. Collm: Integrating collaborative embeddings into large language models for recommendation. _CoRR_, abs/2310.19488, 2023.
* [26] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q. Tran, Jonah Samost, Maciej Kula, Ed H. Chi, and Mahesh Sathiamoorthy. Recommender systems with generative retrieval. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, _NeurIPS_, 2023.
* [27] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. ONCE: boosting content-based recommendation with both open- and closed-source large language models. In Luz Angelica Caudillo-Mata, Silvio Lattanzi, Andres Munoz Medina, Leman Akoglu, Aristides Gionis, and Sergei Vassilvitskii, editors, _WSDM_, 2024.
* [28] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, and Yu Shi. Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations. _CoRR_, abs/2402.17152, 2024.

* [29] Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, Ming Chen, and Ji-Rong Wen. Adapting large language models by integrating collaborative semantics for recommendation. _CoRR_, abs/2311.09049, 2023.
* [30] Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. How to index item ids for recommendation foundation models. In Qingyao Ai, Yiqin Liu, Alistair Moffat, Xuanjing Huang, Tetsuya Sakai, and Justin Zobel, editors, _SIGIR-AP_, 2023.
* [31] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize with human feedback. In _NeurIPS_, 2020.
* [32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martin, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poutlon, Jeremy Rezisentein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288, 2023.
* [33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In _NeurIPS_, 2022.
* [34] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In _NeurIPS_, 2023.
* [35] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, and Yiming Yang. Direct preference optimization of video large multimodal models from language model reward. _CoRR_, abs/2404.01258, 2024.
* [36] Afra Amini, Tim Vieira, and Ryan Cotterell. Direct preference optimization with an offset. _CoRR_, abs/2402.10571, 2024.
* [37] An Zhang, Leheng Sheng, Zhibo Cai, Xiang Wang, and Tat-Seng Chua. Empowering collaborative filtering with principled adversarial contrastive loss. In _NeurIPS_, 2023.
* [38] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. Self-supervised graph learning for recommendation. In _SIGIR_, 2021.
* [39] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _CoRR_, abs/1807.03748, 2018.
* [40] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 1952.
* [41] R. L. Plackett. The analysis of permutations. _Journal of the Royal Statistical Society. Series C (Applied Statistics)_, 1975.
* [42] R. Duncan Luce. _Individual Choice Behavior: A Theoretical analysis_. 1959.
* [43] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. BPR: bayesian personalized ranking from implicit feedback. _CoRR_, abs/1205.2618, 2012.

* [44] Junliang Yu, Xin Xia, Tong Chen, Lizhen Cui, Nguyen Quoc Viet Hung, and Hongzhi Yin. Xssimgcl: Towards extremely simple graph contrastive learning for recommendation. _CoRR_, abs/2209.02544, 2022.
* [45] An Zhang, Wenchang Ma, Xiang Wang, and Tat-Seng Chua. Incorporating bias-aware margins into contrastive loss for collaborative filtering. In _NeurIPS_, 2022.
* [46] Balazs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. In _ICLR_, 2016.
* [47] Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional sequence embedding. In _WSDM_, 2018.
* [48] Wang-Cheng Kang and Julian J. McAuley. Self-attentive sequential recommendation. In _ICDM_, 2018.
* [49] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. Where to go next for recommender systems? ID- vs. modality-based recommender models revisited. In _SIGIR_, 2023.
* [50] F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. _ACM Trans. Interact. Intell. Syst._, 2016.
* [51] Ivan Cantador, Peter Brusilovsky, and Tsvi Kuflik, editors. _Proceedings of the 2nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems, HetRec '11, Chicago, Illinois, USA, October 27, 2011_, 2011. ACM.
* [52] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. _CoRR_, abs/2402.13228, 2024.
* [53] Huayu Chen, Guande He, Hang Su, and Jun Zhu. Noise contrastive alignment of language models with explicit rewards. _CoRR_, abs/2402.05369, 2024.
* [54] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. Are graph augmentations necessary?: Simple graph contrastive learning for recommendation. In _SIGIR_, 2022.
* [55] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Llmrec: Large language models with graph augmentation for recommendation. In Luz Angelica Caudillo-Mata, Silvio Lattanzi, Andres Munoz Medina, Leman Akoglu, Aristides Gionis, and Sergei Vassilvitskii, editors, _WSDM_, 2024.
* [56] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Representation learning with large language models for recommendation. _CoRR_, abs/2310.15950, 2023.
* [57] Yupeng Hou, Zhankui He, Julian J. McAuley, and Wayne Xin Zhao. Learning vector-quantized item representation for transferable sequential recommenders. In _WWW_, pages 1162-1171. ACM, 2023.
* [58] Leheng Sheng, An Zhang, Yi Zhang, Yuxin Chen, Xiang Wang, and Tat-Seng Chua. Language models encode collaborative signals in recommendation. _arXiv preprint arXiv:2407.05441_, 2024.
* [59] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. Towards open-world recommendation with knowledge augmentation from large language models. _CoRR_, abs/2306.10933, 2023.
* [60] Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, and Linjian Mo. Breaking the length barrier: Llm-enhanced CTR prediction in long textual user behaviors. _CoRR_, abs/2403.19347, 2024.
* [61] Zheng Chen. PALR: personalization aware llms for recommendation. _CoRR_, abs/2305.07622, 2023.

* [62] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. Ctrl: Connect tabular and language model for ctr prediction. _arXiv preprint arXiv:2306.02841_, 2023.
* [63] Xiaoyu Kong, Jiancan Wu, An Zhang, Leheng Sheng, Hui Lin, Xiang Wang, and Xiangnan He. Customizing language models with instance-wise lora for sequential recommendation. In _NeurIPS_, 2024.
* [64] Xiaohao Liu, Jie Wu, Zhulin Tao, Yunshan Ma, Yinwei Wei, and Tat-seng Chua. Harnessing large language models for multimodal product bundling. _arXiv preprint arXiv:2407.11712_, 2024.
* [65] Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and Yongfeng Zhang. VIP5: towards multimodal foundation models for recommendation. In _EMNLP (Findings)_, 2023.
* [66] Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, See-Kiong Ng, and Tat-Seng Chua. A multi-facet paradigm to bridge large language model and recommendation. _CoRR_, abs/2310.06491, 2023.
* [67] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. _CoRR_, abs/2401.10020, 2024.
* [68] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. _CoRR_, abs/2311.16839, 2023.
* [69] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In _AAAI_, 2024.
* [70] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models. _CoRR_, abs/2312.10665, 2023.
* [71] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In _AISTATS_, 2024.
* [72] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. KTO: model alignment as prospect theoretic optimization. _CoRR_, abs/2402.01306, 2024.
* [73] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Remi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Avila Pires, and Bilal Piot. Generalized preference optimization: A unified approach to offline alignment. _CoRR_, abs/2402.05749, 2024.
* [74] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse KL: generalizing direct preference optimization with diverse divergence constraints. _CoRR_, abs/2309.16240, 2023.
* [75] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. _CoRR_, abs/2309.06657, 2023.
* [76] OpenAI. GPT-4 technical report. _CoRR_, abs/2303.08774, 2023.
* [77] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _CoRR_, abs/2302.13971, 2023.
* [78] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling. In _ICML_, 2023.

* [79] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. _CoRR_, abs/2310.06825, 2023.

Mathematical Derivations

### Deriving Preference Distribution

The PL model takes the form:

\[p^{*}(\tau|e_{e},i_{2},\cdots,e_{K},x_{u})=\prod_{j=1}^{K}\frac{\exp\left(r(x_{u},e_{\tau(j)})\right)}{\Sigma_{l=j}^{K}\text{exp}(r\left(x_{u},e_{\tau(l)}) )\right)}.\] (14)

The ranking in multi-negative preference data is \(e_{p}\succ e_{d}|x_{u},\forall e_{d}\in\mathcal{E}_{d}\). Our new preference distribution that estimates the probability of the ranking can be derived:

\[p^{*}(e_{p}\succ e_{d},\forall\,e_{d}\in\mathcal{I}_{d}|x_{u},e _{p},\mathcal{E}_{d}) =\sum_{\tau\in\{\tau^{\prime}|\tau^{\prime}(1)=p\}}p^{*}(\tau|x_{ u},e_{p},\mathcal{E}_{d})\] \[=\sum_{\tau\in\{\tau^{\prime}|\tau^{\prime}(1)=p\}}\prod_{j=1}^{K }\frac{\exp\left(r(x_{u},e_{\tau(j)})\right)}{\sum_{l=j}^{K}\exp\left(r(x_{u},e_{\tau(l)})\right)}\] \[=\frac{\exp(r(x_{u},e_{p}))}{\sum_{j=1}^{K}\exp(r(x_{u},e_{j}))} \times\sum_{\tau^{\prime}\in\text{Per}(\text{ind}(\mathcal{I}_{d}))}\prod_{j =1}^{K-1}\frac{\exp\left(r(x_{u},e_{\tau^{\prime}(j)})\right)}{\sum_{l=j}^{K- 1}\exp(r(x_{u},e_{\tau^{\prime}(l)}))}\] \[=\frac{\exp(r(x_{u},e_{p}))}{\sum_{j=1}^{K}\exp(r(x_{u},e_{j}))},\]

wherein \(\text{ind}(\mathcal{E}_{d})\) denotes the indices of titles in \(\mathcal{E}_{d}\) and \(\text{Per}(\text{ind}(\mathcal{E}_{d}))\) denotes the set of permutations of index set \(\text{ind}(\mathcal{E}_{d})\). The third equation is because a permutation of \(\{1,2\cdots,K\}\) starting with \(p\) can be divided into the prefix \(p\) and a subsequent permutation of the rest indices, which is exactly \(\text{Per}(\text{ind}(\mathcal{E}_{d}))\).

### Connection Between DPO and S-DPO

When \(N=2\), the following equations hold:

\[\mathcal{L}_{\text{S-DPO}}(\pi_{\theta};\pi_{\text{ref}})\] (Eq.(11)) \[=-\mathbb{E}_{(x_{u},e_{p},\mathcal{E}_{d})}\left[\log\sigma \left(-\log\sum_{e_{d}\in\mathcal{E}_{d}}\exp\left(\beta\log\frac{\pi_{ \theta}(e_{d}|x_{u})}{\pi_{\text{ref}}(e_{d}|x_{u})}-\beta\log\frac{\pi_{ \theta}(e_{p}|x_{u})}{\pi_{\text{ref}}(e_{p}|x_{u})}\right)\right)\right]\] \[=-\mathbb{E}_{(x_{u},e_{p},e_{d})}\left[\log\sigma\left(-\log \exp\left(\beta\log\frac{\pi_{\theta}(e_{d}|x_{u})}{\pi_{\text{ref}}(e_{d}|x_ {u})}-\beta\log\frac{\pi_{\theta}(e_{p}|x_{u})}{\pi_{\text{ref}}(e_{p}|x_{u}) }\right)\right)\right]\qquad\text{($N=2$)}\] \[=-\mathbb{E}_{(x_{u},e_{p},e_{d})}\left[\log\sigma\left(\beta \log\frac{\pi_{\theta}(e_{p}|x_{u})}{\pi_{\text{ref}}(e_{p}|x_{u})}-\beta\log \frac{\pi_{\theta}(e_{d}|x_{u})}{\pi_{\text{ref}}(e_{d}|x_{u})}\right)\right]\] \[=\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}}).\]

Therefore, DPO is a special case of S-DPO.

### Deriving the Gradient of S-DPO Loss

Let \(V(\theta;e_{d})=g(e_{d},e_{p},x_{u})=\beta\log\frac{\pi_{\theta}(e_{d}|x_{u})} {\pi_{\text{ref}}(e_{d}|x_{u})}-\beta\log\frac{\pi_{\theta}(e_{p}|x_{u})}{\pi_ {\text{ref}}(e_{p}|x_{u})}\) and the S-DPO loss takes the following form:

\[\mathcal{L}_{\text{S-DPO}}(\pi_{\theta};\pi_{\text{ref}})\] \[=-\mathbb{E}_{(x_{u},e_{p},\mathcal{E}_{d})}\left[\log\sigma \left(-\log\sum_{e_{d}\in\mathcal{E}_{d}}\exp(V(\theta;e_{d}))\right)\right]\] (15)The gradient of \(V(\theta;e_{d})\) can be formulated as:

\[\nabla_{\theta}\,V(\theta;e_{d})=\beta(\nabla_{\theta}\log\pi_{\theta}(e_{d}|x_{u })-\nabla_{\theta}\log\pi_{\theta}(e_{p}|x_{u}))\] (16)

Using properties of the sigmoid function that \(\sigma^{\prime}(x)=\sigma(x)(1-\sigma(x))=\sigma(x)\sigma(-x)\) and thus \(\left(\left(\log\sigma(x)\right)^{\prime}=\frac{1}{\sigma(x)}\times\sigma(x) \sigma(-x)=\sigma(-x)\), we have:

\[\nabla_{\theta}\,\mathcal{L}_{\mathrm{S-DPO}}(\pi_{\theta};\pi_{ \mathrm{ref}})\] \[=-\mathbb{E}_{(x_{u},e_{p},\mathcal{E}_{d})}\left[\nabla_{\theta }\log\sigma\left(-\log\sum_{e_{d}\in\mathcal{E}_{d}}\exp(V(\theta;e_{d})) \right)\right]\] \[=\mathbb{E}_{(x_{u},e_{p},\mathcal{E}_{d})}\left[\sigma\left( \log\sum_{e_{d}\in\mathcal{E}_{d}}\exp(V(\theta;e_{d}))\right)\cdot\nabla_{ \theta}\log\,\sum_{e_{d}\in\mathcal{E}_{d}}\exp(V(\theta;e_{d}))\right]\] \[\left(\left(\log\!\sigma(x)\right)^{\prime}=\sigma(-x)\right)\] \[=\mathbb{E}_{(x_{u},e_{p},\mathcal{E}_{d})}\left[\sigma\left( \log\sum_{e_{d}\in\mathcal{E}_{d}}\exp(V(\theta;e_{d}))\right)\cdot\frac{ \sum_{e_{d}\in\mathcal{E}_{d}}\exp(V(\theta;e_{d}))\cdot\nabla_{\theta}\,V( \theta;e_{d})}{\sum_{e_{d}^{\prime}\in\mathcal{E}_{d}}\exp(V(\theta;e_{d}^{ \prime}))}\right]\] \[=-\beta\mathbb{E}_{(x_{u},e_{p},\mathcal{E}_{d})}\left[\sigma \left(\log\sum_{e_{d}\in\mathcal{E}_{d}}\exp(V(\theta;e_{d}))\right)\cdot\sum_ {e_{d}\in\mathcal{E}_{d}}\frac{\nabla_{\theta}\log\pi_{\theta}(e_{p}|x_{u})- \nabla_{\theta}\log\pi_{\theta}(e_{d}|x_{u})}{\sum_{e_{d}^{\prime}\in\mathcal{ E}_{d}}\exp(V(\theta;e_{d}^{\prime})-V(\theta;e_{d}))}\right]\] (Eq. (16)) \[=-\beta\mathbb{E}_{(x_{u},e_{p},\mathcal{E}_{d})}\left[\sigma \left(\log\sum_{e_{d}\in\mathcal{E}_{d}}\exp(g(e_{d},e_{p},x_{u}))\right)\cdot \sum_{e_{d}\in\mathcal{E}_{d}}\frac{\nabla_{\theta}\log\pi_{\theta}(e_{p}|x_{u })-\nabla_{\theta}\log\pi_{\theta}(e_{d}|x_{u})}{\sum_{e_{d}^{\prime}\in \mathcal{E}_{d}}\exp(g(e_{d}^{\prime},e_{d},x_{u}))}\right]\] (Definition of \[V(\theta;e_{d})\] ) \[=-\beta\mathbb{E}_{(x_{u},e_{p},\mathcal{E}_{d})}\left[\sigma \left(\log\sum_{e_{d}\in\mathcal{E}_{d}}\exp(g(e_{d},e_{p},x_{u}))\right)\cdot \left[\nabla_{\theta}\log\pi_{\theta}(e_{p}|x_{u})-\sum_{e_{d}\in\mathcal{E}_{ d}}\frac{\nabla_{\theta}\log\pi_{\theta}(e_{d}|x_{u})}{\sum_{e_{d}^{\prime}\in \mathcal{E}_{d}}\exp(g(e_{d}^{\prime},e_{d},x_{u}))}\right]\right]\]

The last equation is because:

\[\sum_{e_{d}\in\mathcal{E}_{d}}\frac{1}{\sum_{e_{d}^{\prime}\in\mathcal{E}_{d}} \exp(g(e_{d}^{\prime},e_{d},x_{u}))}=\frac{\sum_{e_{d}\in\mathcal{E}_{d}}\exp( V(\theta;e_{d}))}{\sum_{e_{d}^{\prime}\in\mathcal{E}_{d}}\exp(V(\theta;e_{d}^{ \prime}))}=1\]

## Appendix B Experimental Settings

### Baselines

We compare the performance of S-DPO, against both traditional and LM-based baselines to showcase the effectiveness of our method. Specifically, for traditional methods, we have:

* **GRU4Rec**[46] utilizes the GRU (Gated Recurrent Unit) architecture to model sequences, enabling effective prediction in recommendation tasks.
* **Caser**[47] employs both horizontal and vertical convolutional operations to enhance the capture of high-order interactions within item sequences, improving recommendation accuracy.
* **SASRec**[48] incorporates a multi-head self-attention mechanism in its self-attentive sequential recommendation model, facilitating the modeling of intricate sequential data patterns.

For LM-enhanced method, we have:

* **MoRec**[49] advances traditional recommendation systems by incorporating the modality features of items instead of the id feature. we employ BERT for the text encoder and SASRec for the recommendation architecture.

For LM-based methods, we have:

* **LLaMA2**[32] utilized vanilla LLaMA2-7B to directly generate recommendation results through direct prompting.

* **Chat-REC**[15] is implemented based on the framework discussed in [15], we retain user interaction sequences consisting of item titles as use profiles for a fair comparison. We use GPT4 [76] as its primary large language model.
* **TallRec**[14] first propose to transform interaction sequences into textual prompts and then fine-tunes large language models using domain-specific corpus.
* **LLaRA**[19] combines collaborative signals from traditional recommendation systems into the fine-tuning of large language models for improved recommendation performance.

### Datasets

To evaluate the effectiveness of S-DPO, we conduct experiments on three widely used real-world datasets: Movielens [50], Goodreads5, and LastFM [51]. The statistics of datasets are illustrated in Table 3. The MovieLens dataset is widely used for movie recommendation tasks and includes user ratings and movie titles, we select the MovieLens100K dataset in our experiment. Similarly, Goodreads is sourced from a social book cataloging website, where users can explore, rate, and review a variety of books. LastFM dataset comprises users' listening history and artists' names from the Last.fm online music platform. Following [19], we maintain their titles as textual descriptions for each dataset. For Goodreads, we remove users and books with less than 20 interactions, which keeps the same as the processing of MovieLens. For all datasets, we organize sequences chronologically before dividing the data into training, validation, and testing sets in an 8:1:1 ratio to prevent any potential information leakage.

Footnote 5: https://www.goodreads.com

### Implementation Details

We implement all approaches with Python 3.9.7, PyTorch 2.2.2, and transformers 4.38.2 on 4 NVIDIA A100s. We select Llama2-7B [32] as the LM backbone for S-DPO. Following [19], we randomly select prompts from several prompt formats during training and evaluation to ensure flexibility and generality. For optimization of all the traditional methods, the Adam optimizer is employed with a learning rate adjusted to 0.001, and a batch size configured at 256. All models undergo L2 regularization, with coefficients experimentally determined from [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]. In all experiments involving large language models, we train each method for a maximum of 5 epochs using a batch size of 128 and select the checkpoint with the lowest loss on the validation set as the final checkpoint. A warm-up strategy is applied to the learning rate, starting at 5% of its maximum value, and gradually adjusting it through a cosine scheduler throughout the training process. For S-DPO and all of its ablation studies, we further conduct preference training for further 3 epochs with a batch size of 128 and a learning rate of 1e-5. Setting the value of \(\beta\) as 1, we search the number of negative samples in [3,5] for the main results. The effects of both factors are further explored in 4.2.

### Evaluation Metrics

Given that LMs primarily produce textual responses rather than comprehensive item rankings, we utilize a re-ranking metric in line with previous research [19] to assess recommendation performance. For each sequence, a candidate set is constructed by randomly selecting 20 non-interacted items and always includes the correct item. We assess all models based on their ability to pinpoint the correct item within this candidate set, employing the HitRatio@1 (HR@1) metric for performance evaluation. Following [19], we also introduce an additional metric called the Valid Ratio to evaluate the LM-based methods' adherence to instructions and their ability to generate appropriate responses. Due to the difficulty LMs face in producing ranked results for candidate items, position-aware metrics like NDCG are deemed unsuitable for this evaluation.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Dataset & MovieLens & Goodreads & LastFM \\ \hline \#Sequence & 943 & 6,031 & 1,220 \\ \#Items & 1,682 & 4,500 & 4,606 \\ \#Interactions & 100,000 & 220,100 & 73,510 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Statistics of datasets.

## Appendix C Study on Backbone Language Models

In order to examine whether the superiority of S-DPO loss over traditional language modeling loss can be generalized across different backbone language models, we conducted experiments using models with varying architectures and sizes, including LLAMA1-7b [77], Pythia-2.8b [78], and Mistral-7b [79]. These models were tested on two distinct datasets: LastFM and MovieLens. We evaluated three training approaches: language models fine-tuned with standard language modeling loss, and models further trained using DPO and S-DPO losses. Due to limitations in computational resources and time, we experimented with two variants of S-DPO: S-DPO (3 negatives) and S-DPO (8 negatives). As shown in Table 4, S-DPO consistently outperformed the language modeling loss, enhancing model performance while maintaining or even improving the validity of the generated outputs. Additionally, the performance continued to improve as the number of negative samples increased from 3 to 8.

## Appendix D Broader Impact

We left further exploration of softmax ranking loss in LM including more negative samples and validation on various settings as future works. We believe that S-DPO, a generalization of DPO loss has the potential to benefit other research areas other than recommender systems. This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\begin{table}
\begin{tabular}{c|c|c c|c c|c c} \hline \hline  & & \multicolumn{2}{c|}{LLAMA1-7B} & \multicolumn{2}{c}{Mistral-7B} & \multicolumn{2}{c}{Pythia-2.8B} \\  & & HR@1 & ValidRatio & HR@1 & ValidRatio & HR@1 & ValidRatio \\ \hline LastFM & Vanilla & 0.0465 & 0.5872 & 0.0633 & 0.3648 & 0.0265 & 0.3648 \\  & Language Modeling & 0.5980 & 0.9980 & **0.7828** & 0.9992 & 0.1611 & 0.4281 \\  & DPO & 0.6084 & 0.9976 & 0.7415 & 0.9964 & 0.1896 & 0.4220 \\  & S-DPO (3 negatives) & 0.6285 & 0.9976 & 0.7679 & 0.9972 & 0.1948 & 0.4689 \\  & S-DPO (8 negatives) & **0.6365** & 0.9988 & 0.7820 & 0.9972 & **0.2200** & 0.4685 \\ \hline MovieLens & Vanilla & 0.0316 & 0.5158 & 0.0842 & 0.6737 & 0.0421 & 0.4421 \\  & Language Modeling & 0.3895 & 0.9684 & 0.4211 & 0.9895 & 0.1053 & 0.5684 \\  & DPO & 0.3789 & 0.9684 & 0.4421 & 0.9684 & 0.1271 & 0.8449 \\  & S-DPO (3 negatives) & **0.4526** & 0.9474 & 0.4421 & 0.9895 & 0.1271 & 0.8737 \\  & S-DPO (8 negatives) & **0.4526** & 0.9579 & **0.4947** & 0.9895 & **0.1474** & 0.8737 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The performance comparison among three different backbone language models on LastFM and MovieLens.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly state the claim including both empirical results and theoretical findings. Refer to Section 1 for more details. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We explicitly discuss the limitations of our work in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide the full set of assumptions, together with complete and correct proofs of our theory in Section 2, Section **??** and Appendix A. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide extensive implementation details in Section B.3 including package version, device, datasets and hyperparameter setting. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide our anonymized versions of data and codes with the link in the abstract. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the details about the experimental setting and datasets can be found in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We validate the p-value of experiment in Table 1. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the details of computation resource needed to reproduce the experiment and also the number of epochs we conduct in Section B.3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We strictly follow the NeurIPS Code of Ethics to conduct our research. Specifically, we have taken steps to ensure that our data collection methods are ethical, our experiments are conducted with responsibility, and all potential biases are thoroughly addressed. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The work presented in our paper aims to instill ranking information into LMs to improve the performance of LM-based recommenders. We do not foresee any negative societal impacts stemming from the outcomes of our research. We further claim our broader impact in Appendix D. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Our language model only serves as a recommender from a given item list, which makes it impossible to be misused or dual-use. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We incorporate three datasets in our experiments including MovieLens [50], Goodreads6 and LastFM [51], all of which are open-source. Also, the backend language model used in our research is LLAMA2-7B [32], which is also an open-source model. The details of our used assets can be found in Appendix B. Guidelines: Footnote 6: https://www.goodreads.com

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We won't release any assets in our research except necessary checkpoint and processed data in order to reproduce our experiments. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing or research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.