# On Tractable \(\Phi\)-Equilibria in Non-Concave Games

 Yang Cai

Yale University

yang.cai@yale.edu

Constantinos Daskalakis

MIT CSAIL

costis@csail.mit.edu

Haipeng Luo

University of Southern California

haipengl@usc.edu

Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

Weiqiang Zheng

Yale University

weiqiang.zheng@yale.edu

###### Abstract

While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to a coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when utilities are non-concave - a common scenario in machine learning applications involving strategies parameterized by deep neural networks, or when agents' utilities are computed by neural networks, or both. Non-concave games introduce significant game-theoretic and optimization challenges: (i) Nash equilibria may not exist; (ii) local Nash equilibria, though they exist, are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria generally have infinite support and are intractable. To sidestep these challenges, we revisit the classical solution concept of \(\Phi\)-equilibria introduced by Greenwald and Jafari [1], which is guaranteed to exist for an arbitrary set of strategy modifications \(\Phi\) even in non-concave games [1]. However, the tractability of \(\Phi\)-equilibria in such games remains elusive. In this paper, we initiate the study of tractable \(\Phi\)-equilibria in non-concave games and examine several natural families of strategy modifications. We show that when \(\Phi\) is finite, there exists an efficient uncoupled learning algorithm that approximates the corresponding \(\Phi\)-equilibria. Additionally, we explore cases where \(\Phi\) is infinite but consists of local modifications, showing that Online Gradient Descent can efficiently approximate \(\Phi\)-equilibria in non-trivial regimes.

## 1 Introduction

Von Neumann's celebrated minimax theorem establishes the existence of Nash equilibrium in all two-player zero-sum games where the players' utilities are continuous as well as _concave_ in their own strategy [15].1 This assumption that players' utilities are concave, or quasi-concave, in their own strategies has been a cornerstone for the development of equilibrium theory in Economics, Game Theory, and a host of other theoretical and applied fields that make use of equilibrium concepts. In particular, (quasi-)concavity is key for showing the existence of many types of equilibrium, from generalizations of min-max equilibrium [14, 15] to competitive equilibrium in exchange economies [1, 2], mixed Nash equilibrium in finite normal-form games [21], and, more generally, Nash equilibrium in (quasi-)concave games [13, 15].

Not only are equilibria guaranteed to exist in concave games, but it is also well-established--thanks to a long line of work at the interface of game theory, learning and optimization whose origins can be traced to Dantzig's work on linear programming [11], Brown and Robinson's work on fictitious play [12, 13], Blackwell's approachability theorem [1] and Hannan's consistency theory [10]--that several solution concepts are efficiently computable both centrally and via decentralized learning dynamics. For instance, it is well-known that the learning dynamics produced when the players of a game iteratively update their strategies using no-regret learning algorithms, such as online gradient descent, is guaranteed to converge to Nash equilibrium in two-player zero-sum concave games, and to coarse correlated equilibrium in multi-player general-sum concave games [1]. The existence of such simple decentralized dynamics further justifies using these solution concepts to predict the outcome of real-life multi-agent interactions where agents deploy strategies, obtain feedback, and use that feedback to update their strategies.

While (quasi-)concave utilities have been instrumental in the development of equilibrium theory, as described above, they are also too restrictive an assumption. Several modern applications and outstanding challenges in Machine Learning, from training Generative Adversarial Networks (GANs) to Multi-Agent Reinforcement Learning (MARL) as well as generic multi-agent Deep Learning settings where the agents' strategies are parameterized by deep neural networks or their utilities are computed by deep neural networks, or both, give rise to games where the agents' utilities are _non-concave_ in their own strategies. We call these games _non-concave_, following [14].

Unfortunately, classical equilibrium theory quickly hits a wall in non-concave games. First, Nash equilibria are no longer guaranteed to exist. Second, while mixed Nash, correlated and coarse correlated equilibria do exist--under convexity and compactness of the strategy sets [15], which we have been assuming all along in our discussion so far, they have infinite support, in general [11]. Finally, they are computationally intractable; so, a fortiori, they are also intractable to attain via decentralized learning dynamics.

In view of the importance of non-concave games in emerging ML applications and the afore-described state-of-affairs, our investigation is motivated by the following broad and largely open question:

**Question from [14]:**_Is there a theory of non-concave games? What solution concepts are meaningful, universal, and tractable?_

### Contributions

We study Daskalakis' question through the lens of the classical solution concept of \(\Phi\)-equilibria introduced by Greenwald and Jafari [1]. This concept is guaranteed to exist for virtually any set of strategy modifications \(\Phi\), even in non-concave games, as demonstrated by Stoltz and Lugosi [1].2 However, the tractability of \(\Phi\)-equilibria in such games remains elusive. In this paper, we initiate the study of tractable \(\Phi\)-equilibria in non-concave games and examine several natural families of strategy modifications.

\begin{table}
\begin{tabular}{c c c c} \hline Solution Concept & Incentive Guarantee & Existence & Complexity \\ \hline Nash equilibrium & & & \\ \hline Mixed Nash equilibrium & Global stability & ✓ & NP-hard \\ \hline (Coarse) Correlated equilibrium & & ✓ & [14]: A222 \\ \hline Strict local Nash equilibrium & Local stability & ✗ & \\ \hline Second-order local Nash equilibrium & Second-order stability & ✗ & \\ \hline Local Nash equilibrium & First-order stability & ✓ & PEMD-hard [14]: \\ \hline \(\Phi\)**-equilibrium (finite \(|\Phi|\))** & \begin{tabular}{c} Stability against \\ finite deviations \\ \end{tabular} & ✓ & \begin{tabular}{c} Efficient \(\varepsilon\)-approximation \\ for any \(\varepsilon\) - (Theorem 2) \\ \end{tabular} \\ \hline \(\mathrm{Conv}(\Phi(\delta))\)**-equilibrium (finite \(|\Phi(\delta)|\))** & & ✓ & \begin{tabular}{c} Efficient \(\varepsilon\)-approximation \\ for \(\varepsilon=\Omega(\delta^{2})\) (Theorem 4) \\ \end{tabular} \\ \hline \(\Phi_{\mathrm{proj}}(\delta)\)**-equilibrium** & First-order stability & ✓ & \begin{tabular}{c} Efficient \(\varepsilon\)-approximation \\ via GDo/G for \(\varepsilon=\Omega(\delta^{2})\) (Theorem 3,9) \\ \end{tabular} \\ \hline \(\Phi_{\mathrm{Int}}(\delta)\)**-equilibrium** & & \begin{tabular}{c} When \(\varepsilon=\Omega(\delta^{2})\) \\ \end{tabular} & ✓ & 
\begin{tabular}{c} Efficient \(\varepsilon\)-approximation \\ via no-regret learning for \(\varepsilon=\Omega(\delta^{2})\) (Theorem 5) \\ \end{tabular} \\ \hline \end{tabular}
\end{table}
Table 1: A comparison between different solution concepts in multi-player non-concave games. We include definitions of Nash equilibrium, mixed Nash equilibrium, (coarse) correlated equilibrium, strict local Nash equilibrium, and second-order local Nash equilibrium in B. We also give a detailed discussion on the existence and complexity of these solution concepts in B.

\(\Phi\)**-Equilibrium.** The concept of \(\Phi\)-equilibrium generalizes (coarse) correlated equilibrium. A \(\Phi\)-equilibrium is a joint distribution over \(\Pi_{i=1}^{n}\mathcal{X}_{i}\), the Cartesian product of all players' strategy sets, and is defined in terms of a set, \(\Phi^{\mathcal{X}_{i}}\), of _strategy modifications_, for each player \(i\). The set \(\Phi^{\mathcal{X}_{i}}\) contains functions mapping \(\mathcal{X}_{i}\) to itself. A joint distribution over strategy profiles qualifies as a \(\Phi=\Pi_{i=1}^{n}\Phi^{\mathcal{X}_{i}}\)-equilibrium if no player \(i\) can increase their expected utility by using any strategy modification function, \(\phi_{i}\in\Phi^{\mathcal{X}_{i}}\), on the strategy sampled from the joint distribution. The larger the set \(\Phi\), the stronger the incentive guarantee offered by the \(\Phi\)-equilibrium. For example, if \(\Phi^{\mathcal{X}_{i}}\) contains all constant functions, the corresponding \(\Phi\)-equilibrium coincides with the notion of coarse correlated equilibrium. Throughout the paper, we also consider \(\varepsilon\)-approximate \(\Phi\)-equilibria, where no player can gain more than \(\varepsilon\) by deviating using any function from \(\Phi^{\mathcal{X}_{i}}\). We study several families of \(\Phi\) and illustrate their relationships in Figure 1.

**Finite Set of Global Deviations.** The first case we consider is when each player \(i\)'s set of strategy modifications, \(\Phi^{\mathcal{X}_{i}}\), contains a finite number of arbitrary functions mapping \(\mathcal{X}_{i}\) to itself. As shown in [1], if there exists an online learning algorithm where each player \(i\) is guaranteed to have sublinear \(\Phi^{\mathcal{X}_{i}}\)-regret, the empirical distribution of joint strategies played converges to a \(\Phi=\Pi_{i=1}^{n}\Phi^{\mathcal{X}_{i}}\)-equilibrium. Gordon, Greenwald, and Marks [1] consider \(\Phi\)-regret minimization but for concave reward functions, and their results, therefore, do not apply to non-concave games. Stoltz and Lugosi [1] provide an algorithm that achieves no \(\Phi^{\mathcal{X}_{i}}\)-regret in non-concave games; however, their algorithm requires a fixed-point computation per step, making it computationally inefficient.3 Our first contribution is to provide an efficient randomized algorithm that achieves no \(\Phi^{\mathcal{X}_{i}}\)-regret for each player \(i\) with high probability.

Footnote 3: The existence of the fixed point is guaranteed by the Schauder-Cauty fixed-point theorem [1], a generalization of the Brouwer fixed-point theorem. Hence, it’s unlikely such fixed points are tractable.

**Contribution 1:** Let \(\mathcal{X}\) be a strategy set (not necessarily compact or convex), and \(\Psi\) an arbitrary finite set of strategy modification functions for \(\mathcal{X}\). We design a randomized online learning algorithm that achieves \(O\left(\sqrt{T\log|\Psi|}\right)\)\(\Psi\)-regret, with high probability, for _arbitrary_ bounded reward functions on \(\mathcal{X}\) (Theorem 2). The algorithm operates in time \(\sqrt{T}|\Psi|\) per iteration. If every player in a _non-concave_ game adopts this algorithm, the empirical distribution of strategy profiles played forms an \(\varepsilon\)-approximate \(\Phi=\Pi_{i=1}^{n}\Phi^{\mathcal{X}_{i}}\)-equilibrium, with high probability, for any \(\varepsilon>0\), after \(\mathrm{poly}\left(\frac{1}{\varepsilon},\log\left(\max_{i}|\Phi^{\mathcal{X}_ {i}}|\right),\log n\right)\) iterations.

If players have infinitely many global strategy modifications, we can extend Algorithm 1 by discretizing the set of strategy modifications under mild assumptions, such as the modifications being Lipschitz (Corollary 1). The empirical distribution of the strategy profiles still converges to the corresponding \(\Phi\)-equilibrium, but at a much slower rate of \(O(T^{-\frac{1}{d+2}})\), where \(d\) is the dimension of the set of strategies. Additionally, the algorithm requires exponential time in the dimension per iteration, making it inefficient. This inefficiency is unavoidable, as the problem remains intractable even when \(\Phi\) contains only constant functions.

Figure 1: The relationship between different solution concepts in non-concave games. An arrow from one solution concept to another means the former is contained in the latter. The dashed arrow from \(\mathrm{Conv}(\Phi(\delta))\)-equilibria to \(\Phi_{\mathrm{Finite}}\)-equilibria means the former is contained in the latter when \(\Phi(\delta)=\Phi_{\mathrm{Finite}}\).

To address the limitations associated with infinitely large global strategy modifications, a natural approach is to focus on local deviations instead. The corresponding \(\Phi\)-equilibrium will guarantee local stability. The study of local equilibrium concepts in non-concave games has received significant attention in recent years--see e.g., [1, 1, 2, 3, 4]. However, these solution concepts either are not guaranteed to exist, are restricted to sequential two-player zero-sum games [13], only establish local convergence guarantees for learning dynamics--see e.g., [1, 2, 3], only establish asymptotic convergence guarantees--see e.g., [15], or involve non-standard solution concepts where local stability is not with respect to a distribution over strategy profiles [1].

We study the tractability of \(\Phi\)-equilibrium with infinitely large \(\Phi\) sets that consist solely of local strategy modifications. These local solution concepts are guaranteed to exist in general multi-player non-concave games. Specifically, we focus on the following three families of natural deviations.

* **Projection based Local Deviations:** Each player \(i\)'s set of strategy modifications, denoted by \(\Phi^{\mathcal{X}_{i}}_{\mathrm{Proj}}(\delta)\), contains all deviations that attempt a small step from their input in a fixed direction and project if necessary, namely are of the form \(\phi_{v}(x)=\Pi_{\mathcal{X}_{i}}[x-v]\), where \(\|v\|\leq\delta\) and \(\Pi_{\mathcal{X}_{i}}\) stands for the \(\ell_{2}\)-projection onto \(\mathcal{X}_{i}\).
* **Convex Combination of Finitely Many Local Deviations:** Each player \(i\)'s set of strategy modifications, denoted by \(\mathrm{Conv}(\Phi^{\mathcal{X}_{i}}(\delta))\), contains all deviations that can be represented as a convex combination of a finite set of \(\delta\)-local strategy modifications, i.e., \(\|\phi(x)-x\|\leq\delta\) for all \(\phi\in\Phi^{\mathcal{X}_{i}}(\delta)\).
* **Interpolation based Local Deviations:** each player \(i\)'s set of local strategy modifications, denoted by \(\Phi^{\mathcal{X}_{i}}_{\mathrm{Int}}(\delta)\), that contains all deviations that _interpolate_ between the input strategy and another strategy in \(\mathcal{X}_{i}\). Formally, each element \(\phi_{\lambda,x^{\prime}}(x)\) of \(\Phi^{\mathcal{X}_{i}}_{\mathrm{Int}}(\delta)\) can be represented as \((1-\lambda)x+\lambda x^{\prime}\) for some \(x^{\prime}\in\mathcal{X}_{i}\) and \(\lambda\leq\delta/D_{\mathcal{X}_{i}}\) (\(D_{\mathcal{X}_{i}}\) is the diameter of \(\mathcal{X}_{i}\)).

For our three families of local strategy modifications, we explore the tractability of \(\Phi\)-equilibrium within a regime we term the _first-order stationary regime_, where \(\varepsilon=\Omega(\delta^{2})\)4, with \(\delta\) representing the maximum deviation allowed for a player. An \(\varepsilon\)-approximate \(\Phi\)-equilibrium in this regime ensures first-order stability. This regime is particularly interesting for two reasons: (i) Daskalakis, Skoulakis, and Zampetakis [1] have demonstrated that computing an \(\varepsilon\)-approximate \(\delta\)-local Nash equilibrium in this regime is intractable.5 This poses an intriguing question: can correlating the players' strategies, as in a \(\Phi\)-equilibrium, potentially make the problem tractable? (ii) Extending our algorithm, initially designed for finite sets of strategy modifications, to these three sets of local deviations results in inefficiency; specifically, the running time becomes exponential in one of the problem's natural parameters. Designing efficient algorithms for this regime thus presents challenges. Despite these, we show the following:

Footnote 4: The regime \(\varepsilon=\Omega(\delta)\) is trivial when the utility is Lipschitz.

Footnote 5: A strategy profile is considered an \(\varepsilon\)-approximate \(\delta\)-local Nash equilibrium if no player can gain more than \(\varepsilon\) by deviating within a \(\delta\) distance.

* **Contribution 2:** For any \(\delta>0\), for each of the three families of infinite \(\delta\)-local strategy modifications mentioned above, there exists an efficient uncoupled learning algorithm that converges to an \(\varepsilon\)-approximate \(\Phi\)-equilibrium of the non-concave game in the first-order stationary regime, i.e., \(\varepsilon=\Omega(\delta^{2})\).

We present our results for the projection-based local deviation in Theorem3 and Theorem9. Our result for the convex combination of local deviations can be found in Theorem4. Theorem5 contains our result for the interpolation-based local deviations. Similar to the finite case, our algorithms build on the connection between \(\Phi\)-regret minimization and \(\Phi\)-equilibrium. Given that our strategy modifications are non-standard, it is a priori unclear how to minimize the corresponding \(\Phi\)-regret. For instance, to our knowledge, no algorithm is known to minimize \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret even when the reward functions are concave, and provably \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret is incomparable to external regret (Examples3 and 4). However, via a novel analysis, we show that Online Gradient Descent (GD) and Optimistic Gradient (OG) achieve a near-optimal \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret guarantee (Theorem3 and Theorem8). Our results provide efficient uncoupled algorithms to compute \(\varepsilon\)-approximate \(\Phi(\delta)\)-equilibria in the first-order stationary regime \(\varepsilon=\Omega(\delta^{2})\).

Further related work is discussed in AppendixA.

## 2 Preliminaries

A ball of radius \(r>0\) centered at \(x\in\mathbb{R}^{d}\) is denoted by \(B_{d}(x,r):=\{x^{\prime}\in\mathbbm{R}^{d}:\|x-x^{\prime}\|\leq r\}\). We use \(\|\cdot\|\) for \(\ell_{2}\) norm throughout. We also write \(B_{d}(\delta)\) for a ball centered at the origin with radius \(\delta\). For \(a\in\mathbbm{R}\), we use \([a]^{+}\) to denote \(\max\{0,a\}\). We denote \(D_{\mathcal{X}}\) as the diameter of a set \(\mathcal{X}\).

**Continuous / Smooth Games.** An \(n\)-player _continuous game_ has a set of \(n\) players \([n]:=\{1,2,\ldots,n\}\). Each player \(i\in[n]\) has a nonempty convex and compact strategy set \(\mathcal{X}_{i}\subseteq\mathbbm{R}^{d_{i}}\). For a joint strategy profile \(x=(x_{i},x_{-i})\in\prod_{j=1}^{n}\mathcal{X}_{j}\), the reward of player \(i\) is determined by a utility function \(u_{i}:\prod_{j=1}^{n}\mathcal{X}_{j}\rightarrow[0,1]\). We denote by \(d=\sum_{i=1}^{n}d_{i}\) the dimensionality of the game and assume \(\max_{i\in[n]}\{D_{\mathcal{X}_{i}}\}\leq D\). A _smooth game_ is a continuous game whose utility functions further satisfy the following assumption.

**Assumption 1** (Smooth Games).: _The utility function \(u_{i}(x_{i},x_{-i})\) for any player \(i\in[n]\) is differentiable and satisfies_

1. _(G-Lipschitzness)_ \(\|\nabla_{x_{i}}u_{i}(x)\|\leq G\) _for all_ \(i\) _and_ \(x\in\prod_{j=1}^{n}\mathcal{X}_{j}\)_;_
2. _(L-smoothness) there exists_ \(L_{i}>0\) _such that_ \(\|\nabla_{x_{i}}u_{i}(x_{i},x_{-i})-\nabla_{x_{i}}u_{i}(x^{\prime}_{i},x_{-i}) \|\leq L_{i}\|x_{i}-x^{\prime}_{i}\|\) _for all_ \(x_{i},x^{\prime}_{i}\in\mathcal{X}_{i}\) _and_ \(x_{-i}\in\prod_{j\neq i}\mathcal{X}_{j}\)_. We denote_ \(L=\max_{i}L_{i}\) _as the smoothness of the game._

Crucially, we make no assumption on the concavity of \(u_{i}(x_{i},x_{-i})\).

\(\Phi\)**-equilibrium and \(\Phi\)-regret.** Below we formally introduce the concept of \(\Phi\)-equilibrium and its relationship with online learning and \(\Phi\)-regret minimization.

**Definition 1** (\(\Phi\)-equilibrium [1, 15]).: _In a continuous game, a distribution \(\sigma\) over joint strategy profiles \(\Pi_{i=1}^{n}\mathcal{X}_{i}\) is an \(\varepsilon\)-approximate \(\Phi\)-equilibrium for some \(\varepsilon\geq 0\) and a profile of strategy modification sets \(\Phi=\Pi_{i=1}^{n}\Phi_{i}\) if and only if for all player \(i\in[n]\), \(\max_{\phi\in\Phi_{i}}\mathbb{E}_{x\sim\sigma}[u_{i}(\phi(x_{i}),x_{-i})]\leq \mathbb{E}_{x\sim\sigma}[u_{i}(x)]+\varepsilon\). When \(\varepsilon=0\), we call \(\sigma\) a \(\Phi\)-equilibrium._

We consider the standard online learning setting: at each day \(t\in[T]\), the learner chooses an action \(x^{t}\) from a nonempty convex compact set \(\mathcal{X}\subseteq\mathbbm{R}^{m}\) and the adversary chooses a possibly non-convex loss function \(f^{t}:\mathcal{X}\rightarrow\mathbbm{R}\), then the learner suffers a loss \(f^{t}(x^{t})\) and receives feedback. In this paper, we focus on two feedback models: (1) the player receives an oracle for \(f^{t}(\cdot)\); (2) the player receives only the gradient \(\nabla f^{t}(x^{t})\). The classic goal of an online learning algorithm is to minimize the _external regret_ defined as \(\mathrm{Reg}^{T}:=\max_{x\in\mathcal{X}}\sum_{t=1}^{T}(f^{t}(x^{t})-f^{t}(x))\). An algorithm is called _no-regret_ if its external regret is sublinear in \(T\). The notion of \(\Phi\)-regret generalizes external regret by allowing more general strategy modifications.

**Definition 2** (\(\Phi\)-regret).: _Let \(\Phi\) be a set of strategy modification functions \(\{\phi:\mathcal{X}\rightarrow\mathcal{X}\}\). For \(T\geq 1\), the \(\Phi\)-regret of an online learning algorithm is \(\mathrm{Reg}^{T}_{\Phi}:=\max_{\phi\in\Phi}\sum_{t=1}^{T}\left(f^{t}(x^{t})-f^{ t}(\phi(x^{t}))\right)\). An algorithm is called no \(\Phi\)-regret if its \(\Phi\)-regret is sublinear in \(T\)._

Many classic notions of regret can be interpreted as \(\Phi\)-regret. For example, the external regret is \(\Phi_{\mathrm{ext}}\)-regret where \(\Phi_{\mathrm{ext}}\) contains all constant strategy modifications \(\phi_{x^{*}}(x)=x^{*}\) for all \(x^{*}\in\mathcal{X}\). The _swap regret_ on simplex \(\Delta^{m}\) is \(\Phi_{\mathrm{swap}}\)-regret where \(\Phi_{\mathrm{swap}}\) contains all linear transformations \(\phi:\Delta^{m}\rightarrow\Delta^{m}\). A fundamental result for learning in games is that no-\(\Phi\)-regret learning dynamics in games converge to an approximate \(\Phi\)-equilibrium [1].

**Theorem 1** ([1]).: _If each player \(i\)'s \(\Phi_{i}\)-regret is upper bounded by \(\mathrm{Reg}^{T}_{\Phi_{i}}\), then their empirical distribution of strategy profiles played is an \((\max_{i\in[n]}\mathrm{Reg}^{T}_{\Phi_{i}}/T)\)-approximate \(\Phi\)-equilibrium._

## 3 Tractable \(\Phi\)-Equilibrium for Finite \(\Phi\) via Sampling

In this section, we revisit the problem of computing and learning an \(\Phi\)-equilibrium in non-concave games when each player's set of strategy modifications \(\Phi^{\mathcal{X}_{i}}\) is finite.

The pioneering work of Stoltz and Lugosi [15] gives a no-\(\Phi\)-regret algorithm for this case where each player chooses a distribution over strategies in each round. This result also implies convergence to \(\Phi\)-equilibrium. However, the algorithm by Stoltz and Lugosi [15] is not computationally efficient. In each iteration, their algorithm requires computing a distribution that is stationary under atransformation that can be represented as a mixture of the modifications in \(\Phi\). The existence of such a stationary distribution is guaranteed by the Schauder-Cauty fixed-point theorem [1], but the distribution might require exponential support and be intractable to find.

Our main result in this section is an efficient \(\Phi\)-regret minimization algorithm (Algorithm 1) that circumvents the step of the exact computation of a stationary distribution. Consequently, our algorithm also ensures efficient convergence to a \(\Phi\)-equilibrium when adopted by all players.

``` Input:\(x_{\text{root}}\in\mathcal{X}\), \(h\geq 2\), an external regret minimization algorithm \(\mathfrak{R}_{\Phi}\) over \(\Phi\) Output: A \(\Phi\)-regret minimization algorithm for \(\mathcal{X}\)
1functionNextStrategy() \(p^{t}\leftarrow\mathfrak{R}_{\Phi}\).NextStrategy(). Note that \(p_{t}\) is a distribution over \(\Phi\). return\(x^{t}\leftarrow\textsc{SampleStrategy}(x_{\text{root}},h,p^{t})\).
2functionObserveReward(\(u^{t}(\cdot)\))
3 Set \(u^{t}_{\Phi}(\phi)=u^{t}(\phi(x^{t}))\) for all \(\phi\in\Phi\). \(\mathfrak{R}_{\Phi}\).ObserveReward(\(u^{t}_{\Phi}(\cdot)\)). ```

**Algorithm 1**\(\Phi\)-regret minimization for non-concave reward via sampling

**Theorem 2**.: _Let \(\mathcal{X}\) be a strategy set (not necessarily compact or convex), \(\Phi\) be an arbitrary finite set of strategy modifications over \(\mathcal{X}\), and \(u^{1}(\cdot),\ldots,u^{T}(\cdot)\) be an arbitrary sequence of possibly non-concave reward functions from \(\mathcal{X}\) to \([0,1]\). If we instantiate Algorithm 1 with \(\mathfrak{R}_{\Phi}\) being the Hedge algorithm over \(\Phi\) and \(h=\sqrt{T}\), the algorithm guarantees that, with probability at least \(1-\beta\), it produces a sequence of strategies \(x^{1},\ldots,x^{T}\) with \(\Phi\)-regret at most \(\max_{\phi\in\Phi}\sum_{t=1}^{T}u^{t}(\phi(x^{t}))-\sum_{t=1}^{T}u^{t}(x^{t}) \leq 8\sqrt{T(\log|\Phi|+\log(1/\beta))}\). Moreover, the algorithm runs in time \(O(\sqrt{T}|\Phi|)\) per iteration._

_If all players in a non-concave continuous game employ Algorithm 1, then with probability at least \(1-\beta\), for any \(\varepsilon>0\), the empirical distribution of strategy profiles played forms an \(\varepsilon\)-approximate \(\Phi=\Pi_{i=1}^{n}\Phi^{\mathcal{X}_{i}}\)-equilibrium, after \(\operatorname{poly}\left(\frac{1}{\varepsilon},\log\left(\max_{i}|\Phi^{ \mathcal{X}_{i}}|\right),\log\frac{n}{\beta}\right)\) iterations._

**High-level ideas.** We adopt the framework in [15]. The framework contains two steps in each iteration \(t\): (1) the learner runs a no-external-regret algorithm over \(\Phi\) which outputs \(p^{t}\in\Delta(\Phi)\) in each iteration \(t\); (2) the learner chooses a stationary distribution \(\mu^{t}=\sum_{\phi\in\Phi}p^{t}\phi(\mu^{t})\), where we slightly abuse notation to use \(\phi(\mu^{t})\) to denote the image measure of \(\mu\) by \(\phi\). However, how to compute the stationary distribution \(\mu^{t}\) efficiently is unclear. We essentially provide a computationally efficient way to carry out step (2) without computing this stationary distribution.

* We first construct an \(\varepsilon\)-approximate stationary distribution by recursively applying strategy modifications from \(\Phi\). The constructed distribution can be viewed as a tree. Our construction is inspired by the recent work of Zhang, Anagnostides, Farina, and Sandholm [14] for concave games. The main difference here is that for non-concave games, the distribution needs to be approximately stationary with respect to a _mixture_ of strategy modifications rather than a single one as in concave games. Consequently, this leads to an approximate stationary distribution with prohibitively high support size \(\left(|\Phi|\right)^{\sqrt{T}}\), as opposed to \(\sqrt{T}\) in [14] for concave games.
* Despite the exponentially large support size of the distribution, we utilize its tree structure to design a simple and efficient sampling procedure that runs in time \(\sqrt{T}\). Equipped with such a sampling procedure, we provide an efficient randomized algorithm that generates a sequence of strategies so that, with high probability, the \(\Phi\)-regret for this sequence of strategies is at most \(O(\sqrt{T\log|\Phi|})\).

We defer the full proof of Theorem2 to Section3.1. An extension of Theorem2 to infinite \(\Phi\) holds when the rewards \(\{u^{t}\}_{t\in[T]}\) are \(G\)-Lipschitz and \(\Phi\) admits an \(\alpha\)-cover with size \(N(\alpha)\). In particular, when \(\Phi\) is the set of all \(M\)-Lipschitz functions over \([0,1]^{d}\), \(\Phi\) admits an \(\alpha\)-cover with \(\log N(\alpha)\) of the order \((1/\alpha)^{d}\)[12]. In this case, we have

**Corollary 1**.: _There is a randomized algorithm such that, with probability at least \(1-\beta\), the \(\Phi\)-regret is bounded by \(c\cdot T^{\frac{d+1}{d+2}}\cdot\log(1/\beta)\), where \(c\) only depends on \(G\) and \(M\). The algorithm runs in time \(\operatorname{poly}(T,N(T^{-1/(d+2)}))\)._

### Proof of Theorem2

For a distribution \(\mu\in\Delta(\mathcal{X})\) over strategy space \(\mathcal{X}\), we slightly abuse notation and define its expected utility as \(u^{t}(\mu):=\mathbb{E}_{x\sim\mu}[u^{t}(x)]\in[0,1]\). We define \(\phi(\mu)\) the image of \(\mu\) under transformation \(\phi\). In each iteration \(t\), the learner chooses their strategy \(x^{t}\in\mathcal{X}\) according to the distribution \(\mu^{t}\). For a sequence of strategies \(\{x^{t}\}_{t\in[T]}\), the \(\Phi\)-regret is \(\operatorname{Reg}_{\Phi}^{T}:=\max_{\phi\in\Phi}\left\{\sum_{t=1}^{T}\left(u ^{t}(\phi(x^{t}))-u^{t}(x^{t})\right)\right\}.\) Algorithm1 uses an external regret minimization algorithm \(R_{\Phi}\) over \(\Phi\) which outputs a distribution \(p^{t}\in\Delta(\Phi)\). We can then decompose the \(\Phi\)-regret into two parts.

\[\operatorname{Reg}_{\Phi}^{T} =\underbrace{\max_{\phi\in\Phi}\left\{\sum_{t=1}^{T}u^{t}(\phi(x^ {t}))-\mathbb{E}_{\phi^{\prime}\sim p^{t}}\big{[}u^{t}(\phi^{\prime}(x^{t})) \big{]}\right\}}_{\text{I: external regret over $\Phi$}}+\underbrace{\sum_{t=1}^{T} \mathbb{E}_{\phi^{\prime}\sim p^{t}}\big{[}u^{t}(\phi^{\prime}(x^{t}))\big{]} -u^{t}(x^{t})}_{\text{II: approximation error of stationary distribution}}.\]

I: Bounding the external regret over \(\Phi\).The external regret over \(\Phi\) can be bounded directly. This is equivalent to an online expert problem: in each iteration \(t\), the external regret minimizer \(\mathfrak{R}_{\Phi}\) chooses \(p^{t}\in\Delta(\Phi)\) and the adversary then determines the utility of each expert \(\phi\in\Phi\) as \(u^{t}(\phi(x^{t}))\). We choose the external regret minimizer \(\mathfrak{R}_{\Phi}\) to be the Hedge algorithm [13]. Then we have \(\max_{\phi\in\Phi}\left\{\sum_{t=1}^{T}u^{t}(\phi(x^{t}))-\mathbb{E}_{\phi^{ \prime}\sim p^{t}}[u^{t}(\phi^{\prime}(x^{t}))]\right\}\leq 2\sqrt{T\log|\Phi|}\) (Theorem6), where we use the fact that the utility function \(u^{t}\) is bounded in \([0,1]\).

II: Bounding error due to sampling from an approximate stationary distribution.We first define a distribution \(\mu^{t}\) over \(\mathcal{X}\) using a complete \(|\Phi|\)-ary tree with depth \(h\). The root of this tree is an arbitrary strategy \(x_{\text{root}}\in\mathcal{X}\). Each internal node \(x\) has exactly \(|\Phi|\) children, denoted as \(\{\phi(x)\}_{\phi\in\Phi}\). The distribution \(\mu^{t}\) is supported on the nodes of this tree. Next, we define the probability for each node under the distribution \(\mu^{t}\). The root node \(x_{\text{root}}\) receives probability \(\frac{1}{h}\). The probability of other nodes is defined in a recursive manner. For every node \(x=\phi(x_{p})\) where \(x_{p}\) is its parent, \(x\) receives probability \(\Pr_{\mu^{t}}[x]=\Pr_{\mu^{t}}[x_{p}]\cdot p^{t}(\phi)\). It is then clear that the total probability of the children of a node \(x_{p}\) is exactly \(\Pr_{\mu^{t}}[x\) is \(x_{p}\)'s child\(]=\sum_{\phi}\Pr[x_{p}]\cdot p^{t}(\phi)=\Pr[x_{p}]\). Denote the set of nodes in depth \(k\) as \(N_{k}\). We have \(\Pr_{\mu^{t}}[x\in N_{k}]=\frac{1}{h}\) for every depth \(1\leq k\leq h\). Thus the distribution \(\mu^{t}\) supports on \(\frac{|\Phi|^{h}-1}{|\Phi|-1}\) points and is well-defined. By the construction above, we know \(x^{t}\) output by Algorithm2 is a sample from \(\mu^{t}\).

Now we show that the approximation error of \(\mu^{t}\) is bounded by \(O(\frac{1}{h})\). We can evaluate the approximation error of \(\mu^{t}\):

\[\mathbb{E}_{\phi\sim p^{t}}\big{[}u^{t}(\phi(\mu^{t}))\big{]}-u^{ t}(\mu^{t}) =\mathbb{E}_{\phi\sim p^{t}}\bigg{[}\sum_{k=1}^{h}\sum_{x\in N_{k} }\Pr_{\mu^{t}}[x]u^{t}(\phi(x))\bigg{]}-\left[\sum_{k=1}^{h}\sum_{x\in N_{k} }\Pr_{\mu^{t}}[x]u^{t}(x)\right]\] \[=\sum_{k=1}^{h}\sum_{x\in N_{k}}\bigg{(}\mathbb{E}_{\phi\sim p^{t} }\bigg{[}\Pr_{\mu^{t}}[x]u^{t}(\phi(x))\bigg{]}-\Pr_{\mu^{t}}[x]u^{t}(x) \bigg{)}.\]We recall that for a node \(x=\phi(x_{p})\) with \(x_{p}\) being its parent, we have \(\Pr_{\mu^{\ell}}[x]=\Pr_{\mu^{\ell}}[x_{p}]\cdot p^{t}(\phi)\). Thus for any \(1\leq k\leq h-1\), we have

\[\sum_{x\in N_{h}}\bigg{(}\mathbb{E}_{\phi\sim p^{t}}\Big{[}\Pr_{ \mu^{\ell}}[x]u^{t}(\phi(x))\Big{]}-\Pr_{\mu^{\ell}}[x]u^{t}(x)\bigg{)} =\sum_{x\in N_{k}}\left(\sum_{\phi\in\Phi}p^{t}(\phi)\Pr_{\mu^{ \ell}}[x]u^{t}(\phi(x))-\Pr_{\mu^{\ell}}[x]u^{t}(x)\right)\] \[=\sum_{x\in N_{k+1}}\Pr_{\mu^{\ell}}[x]u^{t}(x)-\sum_{x\in N_{k}} \Pr_{\mu^{\ell}}[x]u^{t}(x).\]

Using the above equality, we get

\[\mathbb{E}_{\phi\sim p^{t}}\big{[}u^{t}(\phi(\mu^{t}))\big{]}-u^{ t}(\mu^{t})\] \[=\sum_{k=1}^{h-1}\sum_{x\in N_{k+1}}\Pr_{\mu^{\ell}}[x]u^{t}(x)+ \sum_{x\in N_{h}}\sum_{\phi\in\Phi}p^{t}(\phi)\Pr_{\mu^{\ell}}[x]u^{t}(\phi(x ))-\sum_{k=2}^{h}\sum_{x\in N_{k}}\Pr_{\mu^{\ell}}[x]u^{t}(x)-\Pr_{\mu^{\ell}} [x_{\text{root}}]u^{t}(x_{\text{root}})\] \[=\sum_{x\in N_{h}}\sum_{\phi\in\Phi}p^{t}(\phi)\Pr_{\mu^{\ell}}[x ]u^{t}(\phi(x))-\Pr_{\mu^{\ell}}[x_{\text{root}}]u^{t}(x_{\text{root}})\leq \frac{1}{h},\]

where in the last inequality we use the fact that \(\sum_{x\in N_{k}}\Pr_{\mu^{\ell}}[x]=\frac{1}{h}\) for all \(1\leq k\leq h\) and the utility function \(u^{t}\) is bounded in \([0,1]\). Therefore, for \(x^{t}\sim\mu^{t}\), the sequence of random variables \(\{\sum_{t=1}^{\tau}\left(\mathbb{E}_{\phi\sim p^{t}}[u^{t}(\phi(x^{t}))]-u^{t} (x^{t})-\frac{1}{h}\right)\}_{\tau\geq 1}\) is a super-martingale. Thanks to the boundedness of the utility function, we can apply the Hoeffding-Azuma Inequality and get for any \(\varepsilon>0\).

\[\Pr\left[\sum_{t=1}^{T}\bigg{(}\mathbb{E}_{\phi\sim p^{t}}\big{[}u^{t}(\phi(x^ {t}))\big{]}-u^{t}(x^{t})-\frac{1}{h}\bigg{)}\geq\varepsilon\right]\leq\exp \left(-\frac{\varepsilon^{2}}{8T}\right).\] (1)

Combining the bounds for 1 and 2 with \(\varepsilon=\sqrt{8T\log(1/\beta)}\) and \(h=\sqrt{T}\), we have, with probability \(1-\beta\), that \(\operatorname{Reg}_{\Phi}^{T}\leq 2\sqrt{T\log|\Phi|}+\frac{T}{h}+\sqrt{8T \log(1/\beta)}\leq 8\sqrt{T(\log|\Phi|+\log(1/\beta))}\).

**Convergence to \(\Phi\)-equilibrium.** If all players in a non-concave continuous game employ Algorithm 1, then we know for each player \(i\), with probability \(1-\frac{\beta}{n}\), its \(\Phi^{\mathcal{X}_{i}}\)-regret is upper bounded by \(8\sqrt{T(\log|\Phi^{\mathcal{X}_{i}}|+\log(n/\beta))}\). By a union bound over all \(n\) players, we get with probability \(1-\beta\), every player \(i\)'s \(\Phi^{\mathcal{X}_{i}}\)-regret is upper bounded by \(8\sqrt{T(\log|\Phi^{\mathcal{X}_{i}}|+\log(n/\beta))}\). Now by Theorem 1, we know the empirical distribution of strategy profiles played forms an \(\varepsilon\)-approximate \(\Phi=\Pi_{i=1}^{n}\Phi^{\mathcal{X}_{i}}\)-equilibrium, as long as \(T\geq 64(\log(\max_{i}|\Phi^{\mathcal{X}_{i}}|)+\log(n/\beta))/\varepsilon^{2}\) iterations. 

## 4 Approximate \(\Phi\)-Equilibria under Infinite Local Strategy Modifications

This section studies \(\Phi\)-equilibrium when \(|\Phi|\) is infinite. It is, in general, computationally hard to compute a \(\Phi\)-equilibrium even if \(\Phi\) contains all constant deviations. Instead, we focus on \(\Phi\) that consists solely of local strategy modifications. We introduce several natural classes of local strategy modifications and provide efficient online learning algorithms that converge to \(\varepsilon\)-approximate \(\Phi\)-equilibrium in the first-order stationary regime where \(\varepsilon=\Omega(\delta^{2}L)\). These approximate \(\Phi\)-equilibria guarantee first-order stability.

**Definition 3** (\(\delta\)-local strategy modification).: _For each agent \(i\), we call a set of strategy modifications \(\Phi^{\mathcal{X}_{i}}\)\(\delta\)-local if for all \(x\in\mathcal{X}_{i}\) and \(\phi_{i}\in\Phi^{\mathcal{X}_{i}}\), \(\|\phi_{i}(x)-x\|\leq\delta\). We use notation \(\Phi^{\mathcal{X}_{i}}(\delta)\) to denote a \(\delta\)-local strategy modification set for agent \(i\). We also use \(\Phi(\delta)=\Pi_{i=1}^{n}\Phi^{\mathcal{X}_{i}}(\delta)\) to denote a profile of \(\delta\)-local strategy modification sets._

Below we present a useful reduction from computing an \(\varepsilon\)-approximate \(\Phi(\delta)\)-equilibrium in _non-concave_ smooth games to \(\Phi^{\mathcal{X}_{i}}(\delta)\)-regret minimization against _convex_ losses for any \(\varepsilon\geq\frac{\delta^{2}L}{2}\). The key observation here is that the \(L\)-smoothness of the utility function permits, within a \(\delta\)-neighborhood, a \(\frac{\delta^{2}L}{2}\)-approximation using a linear function. We defer the proof to Appendix D.

**Lemma 1** (No \(\Phi(\delta)\)-Regret for Convex Losses to Approximate \(\Phi(\delta)\)-Equilibrium in Non-Concave Games).: _For any \(T\geq 1\) and \(\delta>0\), let \(\mathcal{A}\) be an algorithm that guarantees to achieve no more than \(\operatorname{Reg}_{\Phi^{\mathcal{X}_{i}}(\delta)}^{T}\Phi^{\mathcal{X}_{i}} (\delta)\)-regret for convex loss functions for each agent \(i\in[n]\). Then_

[MISSING_PAGE_EMPTY:9]

### Convex Combination of Finite Local Strategy Modifications

This section considers \(\operatorname{Conv}(\Phi)\) where \(\Phi\) is a finite set of local strategy modifications. The set of infinite strategy modifications \(\operatorname{Conv}(\Phi)\) is defined as \(\operatorname{Conv}(\Phi)=\{\phi_{p}(x)=\sum_{\phi\in\Phi}p(\phi)\phi(x):p\in \Delta(\Phi)\}\). Our main result is an efficient algorithm (Algorithm 3) that guarantees convergence to an \(\varepsilon\)-approximate \(\operatorname{Conv}(\Phi)\)-equilibrium in a smooth game satisfying Assumption 1 for any \(\varepsilon>\delta^{2}L\). Due to space constraints, we defer Algorithm 3 and the proof to Appendix F.

**Theorem 4**.: _Let \(\mathcal{X}\) be a convex and compact set, \(\Phi\) be an arbitrary finite set of \(\delta\)-local strategy modification functions for \(\mathcal{X}\), and \(u^{1}(\cdot),\ldots,u^{T}(\cdot)\) be a sequence of \(G\)-Lipschitz and \(L\)-smooth but possibly non-concave reward functions from \(\mathcal{X}\) to \([0,1]\). If we instantiate Algorithm 3 with \(\mathfrak{R}_{\Phi}\) being the Hedge algorithm over \(\Delta(\Phi)\) and \(K=\sqrt{T}\), the algorithm guarantees that, with probability at least \(1-\beta\), it produces a sequence of strategies \(x^{1},\ldots,x^{T}\) with \(\operatorname{Conv}(\Phi)\)-regret at most \(8\sqrt{T}(G\delta\sqrt{\log|\Phi|}+\sqrt{\log(1/\beta)})+\delta^{2}LT\). The algorithm runs in time \(\sqrt{T}|\Phi|\) per iteration._

_If all players in a non-concave smooth game employ Algorithm 3, then with probability \(1-\beta\), for any \(\varepsilon>0\), the empirical distribution of strategy profiles played forms an \((\varepsilon+\delta^{2}L)\)-approximate \(\Phi=\Pi_{i=1}^{n}\Phi^{\mathcal{X}_{i}}\)-equilibrium,, after \(\operatorname{poly}\left(\frac{1}{\varepsilon},G,\log\left(\max_{i}|\Phi^{ \mathcal{X}_{i}}|\right),\log\frac{n}{\beta}\right)\) iterations._

### Interpolation-Based Local Strategy Modifications

We introduce a natural set of local strategy modifications and the corresponding local equilibrium notion. Given any set of (possibly non-local) strategy modifications \(\Psi=\{\psi:\mathcal{X}\rightarrow\mathcal{X}\}\), we define a set of _local_ strategy modifications as follows: for \(\delta\leq D_{\mathcal{X}}\) and \(\lambda\in[0,1]\), each strategy modification \(\phi_{\lambda,\psi}\) interpolates the input strategy \(x\) with the modified strategy \(\psi(x)\): formally,

\[\Phi^{\mathcal{X}}_{\operatorname{Int},\Psi}(\delta):=\left\{\phi_{\lambda, \psi}(x):=(1-\lambda)x+\lambda\psi(x):\psi\in\Psi,\lambda\leq\delta/D_{ \mathcal{X}}\right\}.\]

Note that for any \(\psi\in\Psi\) and \(\lambda\leq\frac{\delta}{D\lambda}\), we have \(\|\phi_{\lambda,\psi}(x)-x\|=\lambda\|x-\psi(x)\|\leq\delta\), representing the locality constraint. The induced \(\Phi^{\mathcal{X}}_{\operatorname{Int},\Psi}(\delta)\)-regret can be written as \(\operatorname{Reg}^{T}_{\operatorname{Int},\Psi,\delta}:=\max_{\psi\in\Psi, \lambda\leq\frac{\delta}{D_{\mathcal{X}}}}\sum_{t=1}^{T}\left(f^{t}(x^{t})-f^{ t}((1-\lambda)x^{t}+\lambda\psi(x^{t}))\right)\). To guarantee convergence to the corresponding \(\Phi\)-equilibrium, it suffices to minimize \(\Phi^{\mathcal{X}}_{\operatorname{Int},\Psi}(\delta)\)-regret against convex losses, which we show further reduces to \(\Psi\)-regret minimization against convex losses (Theorem 10 in Appendix G).

**CCE-like Instantiation.** In the special case where \(\Psi\) contains only _constant_ strategy modifications (i.e., \(\psi(x)=x^{*}\) for all \(x\)), we get a coarse correlated equilibrium (CCE)-like instantiation of local equilibrium, which limits the gain by interpolating with any _fixed_ strategy. We denote the resulting set of local strategy modification simply as \(\Phi^{\mathcal{X}}_{\operatorname{Int}}(\delta)\). We can apply any no-external regret algorithm for efficient \(\Phi^{\mathcal{X}}_{\operatorname{Int}}(\delta)\)-regret minimization and computation of \(\varepsilon\)-approximate \(\Phi_{\operatorname{Int}}(\delta)\)-equilibrium in the first-order stationary regime as summarized in Theorem 5. We also discuss faster convergence rates in the game setting in Appendix G.

**Theorem 5**.: _For the Online Gradient Descent algorithm (GD) [21] with step size \(\eta=\frac{D_{\mathcal{X}}}{G\sqrt{T}}\), its \(\Phi^{\mathcal{X}}_{\operatorname{Int}}(\delta)\)-regret is at most \(2\delta G\sqrt{T}\). Furthermore, for any \(\delta>0\) and any \(\varepsilon>\frac{\delta^{2}L}{2}\), when all players employ the GD algorithm in a smooth game, their empirical distribution of played strategy profiles converges to an \((\varepsilon+\frac{\delta^{2}L}{2})\)-approximate \(\Phi_{\operatorname{Int}}(\delta)\)-equilibrium in \(O(1/\varepsilon^{2})\) iterations._

## 5 Discussion and Future Directions

**Lower Bound in the Global Regime** When \(\delta\) equals the diameter of our strategy set, it is NP-hard to compute an \(\varepsilon\)-approximate \(\Phi(\delta)\)-equilibrium (for \(\Phi(\delta)=\Phi_{\operatorname{Proj}}(\delta),\Phi_{\operatorname{Int}}( \delta)\)), even when \(\varepsilon=\Theta(1)\) and \(G,L=O(\operatorname{poly}(d))\). Moreover, given black-box access to value and gradient queries, finding such equilibria requires exponentially many queries in at least one of the parameters \(d,G,L,1/\varepsilon\). These results are presented as Theorem 12 and Theorem 13 in Appendix I.

**More Efficient \(\Phi\)-Equilibria** We include the discussion of another natural class of local strategy modifications that is based on beam search, where GD suffers linear regret to Appendix H. This result shows that even for simple local strategy modification sets \(\Phi(\delta)\), the landscape of efficient local \(\Phi(\delta)\)-regret minimization is already quite rich and many questions remain open. A fruitful future direction is to identify more classes of \(\Phi\) that admit efficient regret minimization.

### Acknowledgements

We thank the anonymous reviewers for their constructive comments that improves the paper. Y.C. acknowledge the support from the NSF Awards CCF-1942583 (CAREER) and CCF-2342642. W.Z. was supported by the NSF Awards CCF-1942583 (CAREER), CCF-2342642, and a Research Fellowship from the Center for Algorithms, Data, and Market Design at Yale (CADMY). H.L. was supported by NSF award IIS-1943607.

## References

* [AD54] Kenneth J Arrow and Gerard Debreu. "Existence of an equilibrium for a competitive economy". In: _Econometrica_ (1954), pp. 265-290.
* [AFS23] Ioannis Anagnostides, Gabriele Farina, and Tuomas Sandholm. "Near-Optimal Phi-Regret Learning in Extensive-Form Games". In: _International Conference on Machine Learning (ICML)_. 2023.
* [AGH19] Naman Agarwal, Alon Gonen, and Elad Hazan. "Learning in non-convex games with an optimization oracle". In: _Conference on Learning Theory_. PMLR. 2019, pp. 18-29.
* [AHK12] Sanjeev Arora, Elad Hazan, and Satyen Kale. "The multiplicative weights update method: a meta-algorithm and applications". In: _Theory of computing_ 8.1 (2012), pp. 121-164.
* [ALW21] Jacob Abernethy, Kevin A Lai, and Andre Wibisono. "Last-iterate convergence rates for min-max optimization: Convergence of hamiltonian gradient descent and consensus optimization". In: _Algorithmic Learning Theory_. PMLR. 2021, pp. 3-47.
* [Ana+22a] Ioannis Anagnostides, Constantinos Daskalakis, Gabriele Farina, Maxwell Fishelson, Noah Golowich, and Tuomas Sandholm. "Near-optimal no-regret learning for correlated equilibria in multi-player general-sum games". In: _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing (STOC)_. 2022.
* [Ana+22b] Ioannis Anagnostides, Gabriele Farina, Christian Kroer, Chung-Wei Lee, Haipeng Luo, and Tuomas Sandholm. "Uncoupled Learning Dynamics with \(O(\log T)\) Swap Regret in Multiplayer Games". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2022.
* [Aue+02] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. "The nonstochastic multiarmed bandit problem". In: _SIAM journal on computing_ 32.1 (2002), pp. 48-77.
* [AZ22] Amir Ali Ahmadi and Jeffrey Zhang. "On the complexity of finding a local minimizer of a quadratic function over a polytope". In: _Mathematical Programming_ 195.1 (2022), pp. 783-792.
* [AZF19] Sergul Aydore, Tianhao Zhu, and Dean P Foster. "Dynamic local regret for non-convex online forecasting". In: _Advances in neural information processing systems_ 32 (2019).
* [Bai+22] Yu Bai, Chi Jin, Song Mei, Ziang Song, and Tiancheng Yu. "Efficient Phi-Regret Minimization in Extensive-Form Games via Online Mirror Descent". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 22313-22325.
* [Ber+23] Martino Bernasconi, Matteo Castiglioni, Alberto Marchesi, Francesco Trovo, and Nicola Gatti. "Constrained Phi-Equilibria". In: _International Conference on Machine Learning_. 2023.
* [Bla56] David Blackwell. "An analog of the minimax theorem for vector payoffs." In: _Pacific Journal of Mathematics_ 6.1 (Jan. 1956). Publisher: Pacific Journal of Mathematics, A Non-profit Corporation, pp. 1-8.
* [Bro51] George W Brown. "Iterative solution of games by fictitious play". In: _Act. Anal. Prod Allocation_ 13.1 (1951), p. 374.
* [Bub+15] Sebastien Bubeck et al. "Convex optimization: Algorithms and complexity". In: _Foundations and Trends(r) in Machine Learning_ 8.3-4 (2015), pp. 231-357.
* [Cau01] Robert Cauty. "Solution du probleme de point fixe de Schauder". en. In: _Fundamenta Mathematicae_ 170 (2001). Publisher: Instytut Matematyczny Polskiej Akademii Nauk, pp. 231-246.
* [CDT09] Xi Chen, Xiaotie Deng, and Shang-Hua Teng. "Settling the complexity of computing two-player Nash equilibria". In: _Journal of the ACM (JACM)_ 56.3 (2009), pp. 1-57.

* [CL06] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.
* [CP20] Xi Chen and Binghui Peng. "Hedging in games: Faster convergence of external and swap regrets". In: _Advances in Neural Information Processing Systems (NeurIPS)_ 33 (2020), pp. 18990-18999.
* [CZ23] Yang Cai and Weiqiang Zheng. "Accelerated Single-Call Methods for Constrained Min-Max Optimization". In: _International Conference on Learning Representations (ICLR)_ (2023).
* [Dag+24] Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. "From External to Swap Regret 2.0: An Efficient Reduction for Large Action Spaces". In: _Proceedings of the 56th Annual ACM Symposium on Theory of Computing_. 2024, pp. 1216-1222.
* [Das+23] Constantinos Daskalakis, Noah Golowich, Stratis Skoulakis, and Emmanouil Zampetakis. "STay-ON-the-Ridge: Guaranteed Convergence to Local Minimax Equilibrium in Nonconvex-Nonconcave Games". In: _The Thirty Sixth Annual Conference on Learning Theory_. PMLR. 2023, pp. 5146-5198.
* [Das22] Constantinos Daskalakis. "Non-Concave Games: A Challenge for Game Theory's Next 100 Years". In: _Cowles Preprints_ (2022).
* [DDJ21] Jelena Diakonikolas, Constantinos Daskalakis, and Michael Jordan. "Efficient methods for structured nonconvex-nonconcave min-max optimization". In: _International Conference on Artificial Intelligence and Statistics_ (2021).
* [Deb52] Gerard Debreu. "A social equilibrium existence theorem". In: _Proceedings of the National Academy of Sciences_ 38.10 (1952), pp. 886-893.
* [DFG21] Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. "Near-optimal no-regret learning in general games". In: _Advances in Neural Information Processing Systems (NeurIPS)_ (2021).
* [DGP09] Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. "The complexity of computing a Nash equilibrium". In: _Communications of the ACM_ 52.2 (2009), pp. 89-97.
* [DP18] Constantinos Daskalakis and Ioannis Panageas. "The limit points of (optimistic) gradient descent in min-max optimization". In: _the 32nd Annual Conference on Neural Information Processing Systems (NeurIPS)_. 2018.
* [DSZ21] Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. "The complexity of constrained min-max optimization". In: _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing (STOC)_. 2021.
* [Fan53] Ky Fan. "Minimax theorems". In: _Proceedings of the National Academy of Sciences_ 39.1 (1953), pp. 42-47.
* [Far+22a] Gabriele Farina, Ioannis Anagnostides, Haipeng Luo, Chung-Wei Lee, Christian Kroer, and Tuomas Sandholm. "Near-optimal no-regret learning dynamics for general convex games". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 39076-39089.
* [Far+22b] Gabriele Farina, Andrea Celli, Alberto Marchesi, and Nicola Gatti. "Simple Uncoupled No-regret Learning Dynamics for Extensive-form Correlated Equilibrium". In: _Journal of the ACM_ 69.6 (2022).
* [FCR20] Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. "Implicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study". In: _International Conference on Machine Learning_. PMLR. 2020, pp. 3133-3144.
* [FR21] Tanner Fiez and Lillian J Ratliff. "Local convergence analysis of gradient descent ascent with finite timescale separation". In: _Proceedings of the International Conference on Learning Representation_. 2021.
* [FS99] Yoav Freund and Robert E. Schapire. "Adaptive Game Playing Using Multiplicative Weights". In: _Games and Economic Behavior_ 29 (1999), pp. 79-103.
* [Geo63] George B. Dantzig. _Linear Programming and Extensions_. Princeton University Press, 1963.

* [GGM08] Geoffrey J Gordon, Amy Greenwald, and Casey Marks. "No-regret learning in convex games". In: _Proceedings of the 25th international conference on Machine learning_. 2008, pp. 360-367.
* [GJ03] Amy Greenwald and Amir Jafari. "A general class of no-regret learning algorithms and game-theoretic equilibria". In: _Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003. Proceedings_. Springer. 2003, pp. 2-12.
* [Gli52] Irving L Glicksberg. "A further generalization of the Kakutani fixed theorem, with application to Nash equilibrium points". In: _Proceedings of the American Mathematical Society_ 3.1 (1952), pp. 170-174.
* [GLZ18] Xiand Gao, Xiaobo Li, and Shuzhong Zhang. "Online learning with non-convex losses and non-stationary regret". In: _International Conference on Artificial Intelligence and Statistics_. PMLR. 2018, pp. 235-243.
* [GZL23] Ziwei Guan, Yi Zhou, and Yingbin Liang. "Online Nonconvex Optimization with Limited Instantaneous Oracle Feedback". In: _The Thirty Sixth Annual Conference on Learning Theory_. PMLR. 2023, pp. 3328-3355.
* [Han57] James Hannan. "Approximation to Bayes risk in repeated play". In: _Contributions to the Theory of Games_ 3 (1957), pp. 97-139.
* [Hel+20] Amelie Heliou, Matthieu Martin, Panayotis Mertikopoulos, and Thibaud Rahier. "Online non-convex optimization with imperfect feedback". In: _Advances in Neural Information Processing Systems_ 33 (2020), pp. 17224-17235.
* [HMC21a] Nadav Hallak, Panayotis Mertikopoulos, and Volkan Cevher. "Regret minimization in stochastic non-convex learning via a proximal-gradient approach". In: _International Conference on Machine Learning_. PMLR. 2021, pp. 4008-4017.
* [HMC21b] Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. "The limits of min-max optimization algorithms: Convergence to spurious non-critical sets". In: _International Conference on Machine Learning_. PMLR. 2021, pp. 4337-4348.
* [HSZ17] Elad Hazan, Karan Singh, and Cyril Zhang. "Efficient regret minimization in non-convex games". In: _International Conference on Machine Learning_. PMLR. 2017, pp. 1433-1441.
* [JNJ20] Chi Jin, Praneeth Netrapalli, and Michael Jordan. "What is local optimality in nonconvex-nonconcave minimax optimization?" In: _International conference on machine learning (ICML)_. PMLR. 2020, pp. 4880-4889.
* [Kar14] Samuel Karlin. _Mathematical Methods and Theory in Games, Programming, and Economics: Volume 2: The Theory of Infinite Games_. Elsevier, 2014.
* [Kar59] Samuel Karlin. _Mathematical methods and theory in games, programming, and economics: Volume II: the theory of infinite games_. Vol. 2. Addision-Wesley, 1959.
* [Kri+15] Walid Krichene, Maximilian Balandat, Claire Tomlin, and Alexandre Bayen. "The hedge algorithm on a continuum". In: _International Conference on Machine Learning_. PMLR. 2015, pp. 824-832.
* [McK54] Lionel McKenzie. "On equilibrium in Graham's model of world trade and other competitive systems". In: _Econometrica_ (1954), pp. 147-161.
* [MK87] Katta G. Murty and Santosh N. Kabadi. "Some NP-complete problems in quadratic and nonlinear programming". en. In: _Mathematical Programming_ 39.2 (June 1987), pp. 117-129.
* [MM10] Odalric-Ambrym Maillard and Remi Munos. "Online learning in adversarial lipschitz environments". In: _Joint european conference on machine learning and knowledge discovery in databases_. Springer. 2010, pp. 305-320.
* [Mor+21a] Dustin Morrill, Ryan D'Orazio, Reca Sarfati, Marc Lanctot, James R Wright, Amy R Greenwald, and Michael Bowling. "Hindsight and sequential rationality of correlated play". In: _Proceedings of the AAAI Conference on Artificial Intelligence_. Vol. 35. 6. 2021, pp. 5584-5594.
* [Mor+21b] Dustin Morrill, Ryan D'Orazio, Marc Lanctot, James R Wright, Michael Bowling, and Amy R Greenwald. "Efficient deviation types and learning for hindsight rationality in extensive-form games". In: _International Conference on Machine Learning_. PMLR. 2021, pp. 7818-7828.

* [MRS20] Eric Mazumdar, Lillian J Ratliff, and S Shankar Sastry. "On gradient-based learning in continuous games". In: _SIAM Journal on Mathematics of Data Science 2.1_ (2020), pp. 103-131.
* [MV21] Oren Mangoubi and Nisheeth K Vishnoi. "Greedy adversarial equilibrium: an efficient alternative to nonconvex-nonconcave min-max optimization". In: _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_. 2021, pp. 896-909.
* [MZ19] Panayotis Mertikopoulos and Zhengyuan Zhou. "Learning in games with continuous action sets and unknown payoff functions". In: _Mathematical Programming_ 173 (2019), pp. 465-507.
* [Nas50] John F Nash Jr. "Equilibrium points in n-person games". In: _Proceedings of the national academy of sciences_ 36.1 (1950), pp. 48-49.
* [Neu28] J v. Neumann. "Zur theorie der gesellschaftsspiele". In: _Mathematische annalen_ 100.1 (1928), pp. 295-320.
* [Pet+22] Thomas Pethick, Puya Latafat, Panagiotis Patrinos, Olivier Fercoq, and Volkan Cevhera. "Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems". In: _International Conference on Learning Representations (ICLR)_. 2022.
* [Pil+22] Georgios Piliouras, Mark Rowland, Shayegan Omidshafiei, Romuald Elie, Daniel Hennes, Jerome Connor, and Karl Tuyls. "Evolutionary dynamics and phi-regret minimization in games". In: _Journal of Artificial Intelligence Research_ 74 (2022), pp. 1125-1158.
* [PR24] Binghui Peng and Aviad Rubinstein. "Fast swap regret minimization and applications to approximate correlated equilibria". In: _Proceedings of the 56th Annual ACM Symposium on Theory of Computing_. 2024, pp. 1223-1234.
* [RBS16] Lillian J Ratliff, Samuel A Burden, and S Shankar Sastry. "On the characterization of local Nash equilibria in continuous games". In: _IEEE transactions on automatic control_ 61.8 (2016), pp. 2301-2307.
* [Rob51] Julia Robinson. "An iterative method of solving a game". In: _Annals of mathematics_ (1951), pp. 296-301.
* [Ros65] J Ben Rosen. "Existence and uniqueness of equilibrium points for concave n-person games". In: _Econometrica_ (1965), pp. 520-534.
* [RS13] Sasha Rakhlin and Karthik Sridharan. "Optimization, learning, and games with predictable sequences". In: _Advances in Neural Information Processing Systems_ (2013).
* [RST11] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. "Online learning: Beyond regret". In: _Proceedings of the 24th Annual Conference on Learning Theory_. JMLR Workshop and Conference Proceedings. 2011, pp. 559-594.
* [Sha24] Dravyansh Sharma. "No Internal Regret with Non-convex Loss Functions". In: _Proceedings of the AAAI Conference on Artificial Intelligence_. Vol. 38. 13. 2024, pp. 14919-14927.
* [Sio58] Maurice Sion. "On general minimax theorems." In: _Pacific J. Math._ 8.4 (1958), pp. 171-176.
* [SL07] Gilles Stoltz and Gabor Lugosi. "Learning correlated equilibria in games with compact sets of strategies". In: _Games and Economic Behavior_ 59.1 (2007), pp. 187-208.
* [SMB22] Ziang Song, Song Mei, and Yu Bai. "Sample-efficient learning of correlated equilibria in extensive-form games". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 4099-4110.
* [SN20] Arun Sai Suggala and Praneeth Netrapalli. "Online non-convex learning: Following the perturbed leader is optimal". In: _Algorithmic Learning Theory_. PMLR. 2020, pp. 845-861.
* [Syr+15] Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. "Fast convergence of regularized learning in games". In: _Advances in Neural Information Processing Systems (NeurIPS)_ (2015).
* [VF08] Bernhard Von Stengel and Francoise Forges. "Extensive-form correlated equilibrium: Definition and computational complexity". In: _Mathematics of Operations Research_ 33.4 (2008), pp. 1002-1022.

* [WZB20] Yuanhao Wang, Guodong Zhang, and Jimmy Ba. "On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach". In: _International Conference on Learning Representations (ICLR)_. 2020.
* [Zha+24] Brian Hu Zhang, Ioannis Anagnostides, Gabriele Farina, and Tuomas Sandholm. "Efficient \(\Phi\)-Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games". In: _Advances in Neural Information Processing Systems_. 2024.
* [Zin03] Martin Zinkevich. "Online convex programming and generalized infinitesimal gradient ascent". In: _Proceedings of the 20th international conference on machine learning (ICML)_. 2003.

###### Contents

* 1 Introduction
	* 1.1 Contributions
* 2 Preliminaries
* 3 Tractable \(\Phi\)-Equilibrium for Finite \(\Phi\) via Sampling
	* 3.1 Proof of Theorem 2
* 4 Approximate \(\Phi\)-Equilibria under Infinite Local Strategy Modifications
	* 4.1 Projection-Based Local Strategy Modifications
	* 4.2 Convex Combination of Finite Local Strategy Modifications
	* 4.3 Interpolation-Based Local Strategy Modifications
* 5 Discussion and Future Directions
* A Related Work
* B Additional Preliminaries: Solution Concepts in Non-Concave Games
* C Rerget Bound for Hedge
* D Proof of Lemma 1
* E Missing Details in Section 4.1
* E.1 Differences between External Regret and \(\Phi^{X}_{\mathrm{Proj}}\)-regret
* E.2 Proof of Theorem 3
* E.3 Lower bounds for \(\Phi^{X}_{\mathrm{Proj}}\)-Regret
* E.3.1 Proof of Theorem 7
* E.3.2 Proof of Proposition 1
* E.4 Improved \(\Phi^{X}_{\mathrm{Proj}}(\delta)\)-Regret in the Game Setting and Proof of Theorem 9
* E.4.1 Proof of Theorem 8
* E.4.2 Proof of Theorem 9
* F Missing Details in Section 4.2
* F.1 Proof of Theorem 4
* G Missing details in Section 4.3
* H Beam-Search Local Strategy Modifications and Local Equilibria
* H.1 Proof of Theorem 11
* I Hardness in the Global Regime
* L.1 Proof of Theorem 13
* [J]Removing the \(D\) dependence for \(\Phi_{\mathrm{Proj}}^{\mathcal{X}}\)-regret
* [J.1]One-Dimensional Case
* [J.2]\(d\)-Dimensional Box Case

## Appendix A Related Work

Non-Concave Games.An important special case of multi-player games are two-player zero-sum games, which are defined in terms of some function \(f:\mathcal{X}\times\mathcal{Y}\to\mathbbm{R}\) that one of the two players say the one choosing \(x\in\mathcal{X}\), wants to minimize, while the other player, the one choosing \(y\in\mathcal{Y}\), wants to maximize. Finding Nash equilibrium in such games is tractable in the _convex-concave_ setting, i.e. when \(f(x,y)\) is convex with respect to the minimizing player's strategy, \(x\), and concave with respect to the maximizing player's strategy, \(y\), but it is computationally intractable in the general _nonconvex-nonconcave_ setting. Namely, a Nash equilibrium may not exist, and it is NP-hard to determine if one exists and, if so, find it. Moreover, in this case, stable limit points of gradient-based dynamics are not necessarily Nash equilibria, not even local Nash equilibria [10, 11]. Moreover, there are examples including the "Polar Game" [26] and the "Forsaken Matching Pennies" [13] showing that for GD / OG and many other no-regret learning algorithms in nonconvex-nonconcave min-max optimization, the last-iterate does not converge and even the average-iterate fails to be a stationary point. We emphasize that the convergence guarantees we provide for GD / OG in Section 4.1 and Section 4.3 holds for the empirical distribution of play, not the average-iterate or the last-iterate.

A line of work focuses on computing Nash equilibrium under additional structure in the game. This encompasses settings where the game satisfies the (weak) Minty variational inequality [14, 15, 21, 22], or is sufficiently close to being bilinear [1]. However, the study of universal solution concepts in the nonconvex-nonconcave setting is sparse. Daskalakis, Skoulakis, and Zampetakis [20] proved the existence and computational hardness of local Nash equilibrium. In a more recent work, [16] proposes second-order algorithms with asymptotic convergence to local Nash equilibrium. Several works study sequential two-player zero-sum games with additional assumptions about the player who goes second. They propose equilibrium concepts such as _local minimax points_[17], _differentiable Stackelberg equilibrium_[18], and _greedy adversarial equilibrium_[19]. Notably, local minimax points are stable limit points of Gradient-Descent-Ascent (GDA) dynamics [15, 20, 21] while greedy adversarial equilibrium can be computed efficiently using second-order algorithms in the unconstrained setting [19]. In contrast to these studies, we focus on the more general case of multi-player non-concave games.

Local Equilibrium.To address the limitations associated with classical, global equilibrium concepts, a natural approach is to focus on developing equilibrium concepts that guarantee local stability instead. One definition of interest is the strict local Nash equilibrium, wherein each player's strategy corresponds to a local maximizer of their utility function, given the other players' strategies. Unfortunately, a strict local Nash equilibrium may not always exist, as demonstrated in Example 1. Furthermore, a weaker notion--the second-order local Nash equilibrium, where each player has no incentive to deviate based on the second-order Taylor expansion estimate of their utility, is also not guaranteed to exist as illustrated in Example 1. What's more, it is NP-hard to check whether a given strategy profile is a strict local Nash equilibrium or a second-order local Nash equilibrium, as implied by the result of Murty and Kabadi [13] and Ahmadi and Zhang [1].6 Finally, one can consider _local Nash equilibrium_, a first-order stationary solution, which is guaranteed to exist [20]. Unlike non-convex optimization, where targeting first-order local optima sidesteps the intractability of global optima, this first-order local Nash equilibrium has been recently shown to be intractable, even in 

[MISSING_PAGE_FAIL:18]

**Definition 7** (Strict Local Nash Equilibrium).: _In a continuous game, a strategy profile \(x\in\prod_{j=1}^{n}\mathcal{X}_{j}\) is a strict local Nash equilibrium if and only if for every player \(i\in[n]\), there exists \(\delta>0\) such that_

\[u_{i}(x_{i}^{\prime},x_{-i})\leq u_{i}(x),\forall x_{i}^{\prime}\in B_{d_{i}}(x _{i},\delta)\cap\mathcal{X}_{i}.\]

**Definition 8** (Second-order Local Nash Equilibrium).: _Consider a continuous game where each utility function \(u_{i}(x_{i},x_{-i})\) is twice-differentiable with respect to \(x_{i}\) for any fixed \(x_{-i}\). A strategy profile \(x\in\prod_{j=1}^{n}\mathcal{X}_{j}\) is a second-order local Nash equilibrium if and only if for every player \(i\in[n]\), \(x_{i}\) maximizes the second-order Taylor expansion of its utility functions at \(x_{i}\), or formally,_

\[\langle\nabla_{x_{i}}u_{i}(x),x_{i}^{\prime}-x_{i}\rangle+(x_{i}^{\prime}-x_{ i})^{\top}\nabla_{x_{i}}^{2}u_{i}(x)(x_{i}^{\prime}-x_{i})\leq 0,\forall x_{i}^{ \prime}\in\mathcal{X}_{i}.\]

ExistenceMixed Nash equilibria exist in continuous games, thus smooth games [1, 10, 11]. By definition, an MNE is also a CE and a CCE. This also proves the existence of CE and CCE. In contrast, strict local Nash equilibria, second-order Nash equilibria, or (pure) Nash equilibria may not exist in a smooth non-concave game, as we show in the following example.

**Example 1**.: _Consider a two-player zero-sum non-concave game: the action sets are \(\mathcal{X}_{1}=\mathcal{X}_{2}=[-1,1]\) and the utility functions are \(u_{1}(x_{1},x_{2})=-u_{2}(x_{1},x_{2})=(x_{1}-x_{2})^{2}\). Let \(x=(x_{1},x_{2})\in\mathcal{X}_{1}\times\mathcal{X}_{2}\) be any strategy profile: if \(x_{1}=x_{2}\), then player 1 is not at a local maximizer; if \(x_{1}\neq x_{2}\), then player 2 is not at a local maximizer. Thus \(x\) is not a strict local Nash equilibrium. Since the utility function is quadratic, we conclude that the game also has no second-order local Nash equilibrium._

Computational ComplexityConsider a single-player smooth non-concave game with a quadratic utility function \(f:\mathcal{X}\rightarrow\mathbb{R}\). The problem of finding a _local_ maximizer of \(f\) can be reduced to the problem of computing a NE, a MNE, a CE, a CCE, a strict local Nash equilibrium, or a second-order local Nash equilibrium. Since computing a local maximizer or checking if a given point is a local maximizer is NP-hard [13], we know that the computational complexities of NE, MNE, CE, CCE, strict local Nash equilibria, and second-order local Nash equilibria are all NP-hard.

Representation ComplexityKarlin [12] present a two-player zero-sum non-concave game whose unique MNE has infinite support. Since in a two-player zero-sum game, the marginal distribution of a CE or a CCE is an MNE, it also implies that the representation complexity of any CE or CCE is infinite. We present the example in Karlin [12] here for completeness and also prove that the game is Lipschitz and smooth.

**Example 2** ([12, Chapter 7.1, Example 3]).: _We consider a two-player zero-sum game with action sets \(\mathcal{X}_{1}=\mathcal{X}_{2}=[0,1]\). Let \(p\) and \(q\) be two distributions over \([0,1]\). The only requirement for \(p\) and \(q\) is that their cumulative distribution functions are not finite-step functions. For example, we can take \(p=q\) to be the uniform distribution._

_Let \(\mu_{n}\) and \(\nu_{n}\) denote the \(n\)-th moments of \(p\) and \(q\), respectively. Define the utility function_

\[u(x,y)=u_{1}(x,y)=-u_{2}(x,y)=\sum_{n=0}^{\infty}\frac{1}{2^{n}}(x^{n}-\mu_{n}) (y^{n}-\nu_{n}),\quad 0\leq x,y\leq 1.\]

**Claim 1**.: _The game in Example 2 is \(2\)-Lipschitz and \(6\)-smooth, and \((p,q)\) is its unique (mixed) Nash equilibrium._

Proof.: Fix any \(y\in[0,1]\), since \(|\frac{1}{2^{n}}(y^{n}-\nu_{n})nx^{n-1}|\leq\frac{n}{2^{n}}\), the series of \(\nabla_{x}u(x,y)\) is uniformly convergent. We have \(|\nabla_{x}u(x,y)|\leq\sum_{n=0}^{\infty}\frac{n}{2^{n}}\leq 2,\quad y\in[0,1]\). Similarly, we have \(|\nabla_{x}^{2}u(x,y)|\leq\sum_{n=0}^{\infty}\frac{n^{2}}{2^{n}}\leq 6\) for all \(y\in[0,1]\). By symmetry, we also have \(|\nabla_{y}(x,y)|\leq 2\) and \(|\nabla_{y}^{2}(x,y)|\leq 6\) for all \(x,y\in[0,1]\). Thus, the game is \(2\)-Lispchitz and \(6\)-smooth.

Since \(|\frac{1}{2^{n}}(x^{n}-\mu_{n})(y^{n}-\nu_{n})|\leq\frac{1}{2^{n}}\), the series of \(u(x,y)\) is absolutely and uniformly convergent. We have

\[\int_{0}^{1}u(x,y)\mathrm{d}F_{p}(x) =\sum_{n=0}^{\infty}\frac{1}{2^{n}}(y^{n}-\nu_{n})\int_{0}^{1}(x^ {n}-\mu_{n})\mathrm{d}F_{p}(x)\equiv 0,\] \[\int_{0}^{1}u(x,y)F_{q}(y) =\sum_{n=0}^{\infty}\frac{1}{2^{n}}(x^{n}-\mu_{n})\int_{0}^{1}(y^ {n}-\nu_{n})\mathrm{d}F_{q}(y)\equiv 0.\]In particular, \((p,q)\) is a mixed Nash equilibrium, and the value of the game is \(0\). Suppose \((p^{\prime},q^{\prime})\) is also a mixed Nash equilibrium. Then \((p,q^{\prime})\) is a mixed Nash equilibrium. Note that \(p\) supports on every point in \([0,1]\). As a consequence, we have

\[0\equiv\int_{0}^{1}u(x,y)\mathrm{d}F_{q^{\prime}}(y)=\sum_{n=0}^{\infty}\frac{1 }{2^{n}}(x^{n}-\mu_{n})(\nu^{\prime}_{n}-\nu_{n})\]

for all \(x\in[0,1]\), where \(\nu^{\prime}_{n}\) is the \(n\)-th moment of \(q^{\prime}\). Since the series vanished identically, the coefficients of each power of \(x\) must vanish. Thus \(\nu^{\prime}_{n}=\nu_{n}\) and \(q^{\prime}=q\). Similarly, we have \(p^{\prime}=p\), and the mixed Nash equilibrium is unique. 

## Appendix C Rerget Bound for Hedge

**Theorem 6** ([1]).: _In an \(N\)-expert problem, assume all the rewards are bounded, i.e., \(u^{t}\in[-M,M]^{N}\), then the Hedge algorithm with step size \(\eta=\min\{\frac{1}{M},\frac{\sqrt{\log N}}{M\sqrt{T}}\}\) has regret_

\[\max_{p\in\Delta(N)}\sum_{t=1}^{T}\left\langle u^{t},p\right\rangle-\sum_{t=1 }^{T}\left\langle u^{t},p^{t}\right\rangle\leq 2M\sqrt{T\log N}.\]

## Appendix D Proof of Lemma 1

Let \(\{f^{t}\}_{t\in[T]}\) be a sequence of non-convex \(L\)-smooth loss functions satisfying Assumption 1. Let \(\{x^{t}\}_{t\in[T]}\) be the iterates produced by \(\mathcal{A}\) against \(\{f^{t}\}_{t\in[T]}\). Then \(\{x^{t}\}_{t\in[T]}\) is also the iterates produced by \(\mathcal{A}\) against a sequence of linear loss functions \(\{\left\langle\nabla f^{t}(x^{t}),\cdot\right\rangle\}\). For the latter, we know

\[\max_{\phi\in\Phi^{\mathcal{X}}(\delta)}\sum_{t=1}^{T}\left\langle\nabla f^{t} (x^{t}),x^{t}-\phi(x^{t})\right\rangle\leq\mathrm{Reg}_{\Phi^{\mathcal{X}}( \delta)}^{T}.\]

Then using \(L\)-smoothness of \(\{f^{t}\}\) and the fact that \(\|\phi(x)-x\|\leq\delta\) for all \(\phi\in\Phi(\delta)\), we get

\[\max_{\phi\in\Phi^{\mathcal{X}}(\delta)}\sum_{t=1}^{T}f^{t}(x^{t}) -f^{t}(\phi(x^{t})) \leq\max_{\phi\in\Phi^{\mathcal{X}}(\delta)}\sum_{t=1}^{T}\left( \left\langle\nabla f^{t}(x^{t}),x^{t}-\phi(x^{t})\right\rangle+\frac{L}{2} \left\|x^{t}-\phi(x^{t})\right\|^{2}\right)\] \[\leq\mathrm{Reg}_{\Phi^{\mathcal{X}}(\delta)}^{T}+\frac{\delta^{2} LT}{2}.\]

This completes the proof of the first part.

Let each player \(i\in[n]\) employ algorithm \(\mathcal{A}\) in a smooth game independently and produces iterates \(\{x^{t}\}\). The averaged joint strategy profile \(\sigma^{T}\) that chooses \(x^{t}\) uniformly at random from \(t\in[T]\) satisfies for any player \(i\in[n]\),

\[\max_{\phi\in\Phi^{\mathcal{X}_{i}}(\delta)}\mathbb{E}_{x\sim\sigma }[u_{i}(\phi(x_{i}),x_{-i})]-\mathbb{E}_{x\sim\sigma}[u_{i}(x)]\] \[=\max_{\phi\in\Phi^{\mathcal{X}_{i}}(\delta)}\frac{1}{T}\sum_{t=1 }^{T}\left(u_{i}(\phi(x_{i}^{t}),x_{-i}^{t})-u_{i}(x^{t})\right)\] \[\leq\frac{\mathrm{Reg}_{\Phi^{\mathcal{X}_{i}}(\delta)}^{T}}{T}+ \frac{\delta^{2}L}{2}.\]

Thus \(\sigma^{T}\) is a \((\max_{i\in[n]}\{\mathrm{Reg}_{\Phi^{\mathcal{X}_{i}}(\delta)}^{T}\}\cdot T^{ -1}+\frac{\delta^{2}L}{2})\)-approximate \(\Phi(\delta))\)-equilibrium. This completes the proof of the second part.

## Appendix E Missing Details in Section 4.1

### Differences between External Regret and \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}\)-regret

In the following two examples, we show that \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret is incomparable with external regret for convex loss functions. A sequence of actions may suffer high \(\mathrm{Reg}^{T}\) but low \(\mathrm{Reg}_{\mathrm{Proj},\delta}^{T}\) (Example 3), and vise versa (Example 4).

**Example 3**.: _Let \(f^{1}(x)=f^{2}(x)=\left|x\right|\) for \(x\in\mathcal{X}=[-1,1]\). Then the \(\Phi_{\mathrm{Proj}}^{\mathcal{X}}(\delta)\)-regret of the sequence \(\{x^{1}=\frac{1}{2},x^{2}=-\frac{1}{2}\}\) for any \(\delta\in(0,\frac{1}{2})\) is \(0\). However, the external regret of the same sequence is \(1\). By repeating the construction for \(\frac{7}{2}\) times, we conclude that there exists a sequence of actions with \(\mathrm{Reg}_{\mathrm{Proj},\delta}^{T}=0\) and \(\mathrm{Reg}^{T}=\frac{T}{2}\) for all \(T\geq 2\)._

**Example 4**.: _Let \(f^{1}(x)=-2x\) and \(f^{2}(x)=x\) for \(x\in\mathcal{X}=[-1,1]\). Then the \(\Phi_{\mathrm{Proj}}^{\mathcal{X}}(\delta)\)-regret of the sequence \(\{x^{1}=\frac{1}{2},x^{2}=0\}\) for any \(\delta\in(0,\frac{1}{2})\) is \(\delta\). However, the external regret of the same sequence is \(0\). By repeating the construction for \(\frac{7}{2}\) times, we conclude that there exists a sequence of actions with \(\mathrm{Reg}_{\mathrm{Proj},\delta}^{T}=\frac{\delta T}{2}\) and \(\mathrm{Reg}^{T}=0\) for all \(T\geq 2\)._

At a high level, the external regret competes against a fixed action, whereas \(\Phi_{\mathrm{Proj}}^{\mathcal{X}}(\delta)\)-regret is more akin to the notion of _dynamic regret_, competing with a sequence of varying actions. When the environment is stationary, i.e., \(f^{t}=f\) (Example 3), a sequence of actions that are far away from the global minimum must suffer high regret, but may produce low \(\Phi_{\mathrm{Proj}}^{\mathcal{X}}(\delta)\)-regret since the change to the cumulative loss caused by a fixed-direction deviation could be neutralized across different actions in the sequence. In contrast, in a non-stationary (dynamic) environment (Example 4), every fixed action performs poorly, and a sequence of actions could suffer low regret against a fixed action but the \(\Phi_{\mathrm{Proj}}^{\mathcal{X}}(\delta)\)-regret that competes with a fixed-direction deviation could be large. Nevertheless, despite these differences between the two notions of regret as shown above, they are _compatible_ for convex loss functions: our main results in this section provide algorithms that minimize external regret and \(\Phi_{\mathrm{Proj}}^{\mathcal{X}}(\delta)\)-regret simultaneously.

### Proof of Theorem 3

Proof.: Let us denote \(v\in B_{d}(\delta)\) a fixed deviation and define \(p^{t}=\Pi_{\mathcal{X}}[x^{t}-v]\). By standard analysis of GD [21] (see also the proof of [21, Theorem 3.2] ), we have

\[\sum_{t=1}^{T}\left(f^{t}(x^{t})-f^{t}(p^{t})\right) \leq\sum_{t=1}^{T}\frac{1}{2\eta}\left(\left\|x^{t}-p^{t}\right\| ^{2}-\left\|x^{t+1}-p^{t}\right\|^{2}+\eta^{2}\left\|\nabla f^{t}(x^{t}) \right\|^{2}\right)\] \[\leq\sum_{t=1}^{T-1}\frac{1}{2\eta}\left(\left\|x^{t+1}-p^{t+1} \right\|^{2}-\left\|x^{t+1}-p^{t}\right\|^{2}\right)+\frac{\delta^{2}}{2\eta} +\frac{\eta}{2}G^{2}T,\]

where the last step uses \(\left\|x^{1}-p^{1}\right\|\leq\delta\) and \(\left\|\nabla f^{t}(x^{t})\right\|\leq G\). Here the terms \(\left\|x^{t+1}-p^{t+1}\right\|^{2}-\left\|x^{t+1}-p^{t}\right\|^{2}\) do not telescope, and we further relax them in the following key step.

Key Step:We relax the first term as:

\[\left\|x^{t+1}-p^{t+1}\right\|^{2}-\left\|x^{t+1}-p^{t}\right\|^ {2} =\left\langle p^{t}-p^{t+1},2x^{t+1}-p^{t}-p^{t+1}\right\rangle\] \[=2\left\langle p^{t}-p^{t+1},v\right\rangle+2\left\langle p^{t}-p ^{t+1},x^{t+1}-v-p^{t+1}\right\rangle-\left\|p^{t}-p^{t+1}\right\|^{2}\] \[\leq 2\left\langle p^{t}-p^{t+1},v\right\rangle-\left\|p^{t}-p^{t+1 }\right\|^{2},\]

where in the last inequality we use the fact that \(p^{t+1}\) is the projection of \(x^{t+1}-v\) onto \(\mathcal{X}\) and \(p^{t}\) is in \(\mathcal{X}\). Now we get a telescoping term \(2\langle p^{t}-p^{t+1},u\rangle\) and a negative term \(-\left\|p^{t}-p^{t+1}\right\|^{2}\). The negative term is useful for improving the regret analysis in the game setting, but we ignore it for now. Combining the two inequalities above, we have

\[\sum_{t=1}^{T}\left(f^{t}(x^{t})-f^{t}(p^{t})\right) \leq\frac{\delta^{2}}{2\eta}+\frac{\eta}{2}G^{2}T+\frac{1}{\eta} \sum_{t=1}^{T-1}\left\langle p^{t}-p^{t+1},v\right\rangle\] \[=\frac{\delta^{2}}{2\eta}+\frac{\eta}{2}G^{2}T+\frac{1}{\eta} \left\langle p^{1}-p^{T},v\right\rangle\leq\frac{\delta^{2}}{2\eta}+\frac{ \eta}{2}G^{2}T+\frac{\delta D_{\mathcal{X}}}{\eta}.\]

Since the above holds for any \(v\) with \(\left\|v\right\|\leq\delta\), it also upper bounds \(\mathrm{Reg}_{\mathrm{Proj},\delta}^{T}\)

### Lower bounds for \(\Phi^{X}_{\mathrm{Proj}}\)-Regret

**Theorem 7** (Lower bound for \(\Phi^{X}_{\mathrm{Proj}}(\delta)\)-regret against convex losses).: _For any \(T\geq 1\), \(D_{\mathcal{X}}>0\), \(0<\delta\leq D_{\mathcal{X}}\), and \(G\geq 0\), there exists a distribution \(\mathcal{D}\) on \(G\)-Lipschitz linear loss functions \(f^{1},\ldots,\bar{f}^{T}\) over \(\mathcal{X}=[-D_{\mathcal{X}},D_{\mathcal{X}}]\) such that for any online algorithm, its \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret on the loss sequence satisfies \(\mathbb{E}_{\mathcal{D}}[\mathrm{Reg}^{T}_{\mathrm{Proj},\delta}]=\Omega( \delta G\sqrt{T})\)._

**Remark 2**.: _A keen reader may notice that the \(\Omega(G\delta\sqrt{T})\) lower bound in Theorem 7 does not match the \(O(G\sqrt{\delta D_{\mathcal{X}}T})\) upper bound in Theorem 3, especially when \(D_{\mathcal{X}}\gg\delta\). A natural question is: which of them is tight? We conjecture that the lower bound is tight. In fact, for the special case where the feasible set \(\mathcal{X}\) is a box, we obtain a \(D_{\mathcal{X}}\)-independent bound \(O(d^{\frac{1}{4}}G\delta\sqrt{T})\) using a modified version of GD, which is tight when \(d=1\). See Appendix J for a detailed discussion._

This lower bound suggests that GD achieves near-optimal \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret for convex losses. For \(L\)-smooth _non-convex_ loss functions, we provide another \(\Omega(\delta^{2}LT)\) lower bound for algorithms that satisfy the linear span assumption. The _linear span_ assumption states that the algorithm produces \(x^{t+1}\in\{\Pi_{\mathcal{X}}[\sum_{i\in[t]}a_{i}\cdot x^{i}+b_{i}\cdot\nabla f ^{i}(x^{i})]:a_{i},b_{i}\in\mathbb{R},\forall i\in[t]\}\) as essentially the linear combination of the previous iterates and their gradients. Many online algorithms such as online gradient descent and optimistic gradient satisfy the linear span assumption. Combining with Lemma 1, this lower bound suggests that GD attains nearly optimal \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret, even in the non-convex setting, among a natural family of gradient-based algorithms.

**Proposition 1** (Lower bound for \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret against non-convex losses).: _For any \(T\geq 1\), \(\delta\in(0,1)\), and \(L\geq 0\), there exists a sequence of \(L\)-Lipschitz and \(L\)-smooth non-convex loss functions \(f^{1},\ldots,f^{T}\) on \(\mathcal{X}=[-1,1]\) such that for any algorithm that satisfies the linear span assumption, its \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret on the loss sequence is \(\mathrm{Reg}^{T}_{\mathrm{Proj},\delta}\geq\frac{\delta^{2}LT}{2}\)._

#### e.3.1 Proof of Theorem 7

Our proof technique comes from the standard one used in multi-armed bandits [Aue+02, Theorem 5.1]. Suppose that \(f^{t}(x)=g^{t}x\). We construct two possible environments. In the first environment, \(g^{t}=G\) with probability \(\frac{1+\varepsilon}{2}\) and \(g^{t}=-G\) with probability \(\frac{1-\varepsilon}{2}\); in the second environment, \(g^{t}=G\) with probability \(\frac{1-\varepsilon}{2}\) and \(g^{t}=-G\) with probability \(\frac{1+\varepsilon}{2}\). We use \(\mathbb{E}_{i}\) and \(\mathbb{P}_{i}\) to denote the expectation and probability measure under environment \(i\), respectively, for \(i=1,2\). Suppose that the true environment is uniformly chosen from one of these two environments. Below, we show that the expected regret of the learner is at least \(\Omega(\delta G\sqrt{T})\).

Define \(N_{+}=\sum_{t=1}^{T}\mathbbm{I}\{x^{t}\geq 0\}\) be the number of times \(x^{t}\) is non-negative, and define \(f^{1:T}=(f^{1},\ldots,f^{T})\). Then we have

\[|\mathbb{E}_{1}[N_{+}]-\mathbb{E}_{2}[N_{+}]| =\left|\sum_{f^{1:T}}\left(\mathbb{P}_{1}(f^{1:T})\mathbb{E} \left[N_{+}\mid f^{1:T}\right]-\mathbb{P}_{2}(f^{1:T})\mathbb{E}\left[N_{+} \mid f^{1:T}\right]\right)\right|\] (enumerate all possible sequences of \[f^{1:T}\] \[\leq T\sum_{f^{1:T}}\left|\mathbb{P}_{1}(f^{1:T})-\mathbb{P}_{2}(f ^{1:T})\right|\] \[=T\|\mathbb{P}_{1}-\mathbb{P}_{2}\|_{\mathrm{TV}}\] \[\leq T\sqrt{(2\ln 2)\mathrm{KL}(\mathbb{P}_{1},\mathbb{P}_{2})} \text{(Pinsker's inequality)}\] \[=T\sqrt{(2\ln 2)T\varepsilon\ln\frac{1+\varepsilon}{1-\varepsilon}} \leq T\sqrt{(4\ln 2)T\varepsilon^{2}}.\] (2)

[MISSING_PAGE_FAIL:23]

_Choosing step size \(\eta=\frac{\sqrt{\delta D_{\mathcal{X}}}}{2G\sqrt{T}}\), we have \(\operatorname{Reg}_{\operatorname{Proj},\delta}^{T}\leq 4G\sqrt{\delta D_{ \mathcal{X}}T}\)._

**Theorem 9** (Improved Individual \(\Phi_{\operatorname{Proj}}^{X}(\delta)\)-Regret of OG in the Game Setting).: _In a \(G\)-Lipschitz \(L\)-smooth (in the sense of Assumption 2) game, when all players employ \(OG\) with step size \(\eta>0\), then for each player \(i\), \(\delta>0\), and \(T\geq 1\), their individual \(\Phi_{\operatorname{Proj}}^{X_{i}}(\delta)\)-regret denoted as \(\operatorname{Reg}_{\operatorname{Proj},\delta}^{T,i}\) is \(\operatorname{Reg}_{\operatorname{Proj},\delta}^{T,i}\leq\frac{\delta D}{\eta }+\eta G^{2}+3nL^{2}G^{2}\eta^{3}T\). Choosing \(\eta=\min\{(\delta D/(nL^{2}G^{2}T))^{\frac{1}{4}},(\delta D)^{\frac{1}{2}}/G\}\), we have \(\operatorname{Reg}_{\operatorname{Proj},\delta}^{T,i}\leq 4(\delta D)^{\frac{3}{4}}(nL^{2}G^{2}T)^{ \frac{1}{4}}+2\sqrt{\delta D}G\). Furthermore, for any \(\delta>0\) and any \(\varepsilon>0\), their empirical distribution of played strategy profiles converges to an \((\varepsilon+\frac{\delta^{2}L}{2})\)-approximate \(\Phi_{\operatorname{Proj}}(\delta)\)-equilibrium in \(O(1/\varepsilon^{\frac{4}{3}})\) iterations._

#### e.4.1 Proof of Theorem 8

Proof.: Fix any deviation \(v\) that is bounded by \(\delta\). Let us define \(p^{0}=w^{0}\) and \(p^{t}=\Pi_{\mathcal{X}}[x^{t}-v]\). Following standard analysis of OG [13], we have

\[\sum_{t=1}^{T}f^{t}(x^{t})-f^{t}(p^{t})\leq\sum_{t=1}^{T}\left\langle \nabla f^{t}(x^{t}),x^{t}-p^{t}\right\rangle\] \[\leq\sum_{t=1}^{T}\frac{1}{2\eta}\Big{(}\big{\|}w^{t-1}-p^{t} \big{\|}^{2}-\big{\|}w^{t}-p^{t}\big{\|}^{2}\Big{)}+\eta\big{\|}g^{t}-g^{t-1} \big{\|}^{2}-\frac{1}{2\eta}\Big{(}\big{\|}x^{t}-w^{t}\big{\|}^{2}+\big{\|}x^ {t}-w^{t-1}\big{\|}^{2}\Big{)}\] \[\leq\sum_{t=1}^{T}\left(\frac{1}{2\eta}\big{\|}w^{t-1}-p^{t} \big{\|}^{2}-\frac{1}{2\eta}\big{\|}w^{t-1}-p^{t-1}\big{\|}^{2}+\eta\big{\|}g ^{t}-g^{t-1}\big{\|}^{2}-\frac{1}{2\eta}\big{\|}x^{t}-w^{t-1}\big{\|}^{2}\right)\] (3)

Now we apply a similar analysis from Theorem 3 to upper bound the term \(\left\|w^{t-1}-p^{t}\right\|^{2}-\left\|w^{t-1}-p^{t-1}\right\|^{2}\):

\[\big{\|}w^{t-1}-p^{t}\big{\|}^{2}-\big{\|}w^{t-1}-p^{t-1}\big{\|} ^{2}\] \[=\big{\langle}p^{t-1}-p^{t},2w^{t-1}-2p^{t}\big{\rangle}-\big{\|} p^{t}-p^{t-1}\big{\|}^{2}\] \[=2\big{\langle}p^{t-1}-p^{t},v\big{\rangle}+2\big{\langle}p^{t-1} -p^{t},w^{t-1}-v-p^{t}\big{\rangle}-\big{\|}p^{t}-p^{t-1}\big{\|}^{2}\] \[=2\big{\langle}p^{t-1}-p^{t},v\big{\rangle}+2\big{\langle}p^{t-1} -p^{t},x^{t}-v-p^{t}\big{\rangle}+2\big{\langle}p^{t-1}-p^{t},w^{t-1}-x^{t} \big{\rangle}-\big{\|}p^{t}-p^{t-1}\big{\|}^{2}\] \[\leq 2\big{\langle}p^{t-1}-p^{t},v\big{\rangle}+\big{\|}x^{t}-w^{t- 1}\big{\|}^{2},\]

where in the last-inequality we use \(\langle p^{t-1}-p^{t},x^{t}-u-p^{t}\rangle\leq 0\) since \(p^{t}=\Pi_{\mathcal{X}}[x^{t}-v]\) and \(\mathcal{X}\) is a compact convex set; we also use \(2\langle a,b\rangle-b^{2}\leq a^{2}\). In the analysis above, unlike the analysis of GD where we drop the negative term \(-\big{\|}p^{t}-p^{t-1}\big{\|}^{2}\), we use \(-\big{\|}p^{t}-p^{t-1}\big{\|}^{2}\) to get a term \(\|x^{t}-w^{t-1}\|^{2}\) which can be canceled by the last term in (3).

Now we combine the above two inequalities. Since the term \(\left\|x^{t}-w^{t-1}\right\|^{2}\) cancels out and \(2\langle p^{t-1}-p^{t},v\rangle\) telescopes, we get

\[\sum_{t=1}^{T}f^{t}(x^{t})-f^{t}(p^{t})\leq\frac{\langle p^{0}-p^{T},u\rangle}{ \eta}+\sum_{t=1}^{T}\eta\big{\|}g^{t}-g^{t-1}\big{\|}^{2}\leq\frac{\delta D_{ \mathcal{X}}}{\eta}+\eta\sum_{t=1}^{T}\big{\|}g^{t}-g^{t-1}\big{\|}^{2}.\]

#### e.4.2 Proof of Theorem 9

In the analysis of Theorem 8 for the adversarial setting, the term \(\left\|g^{t}-g^{t-1}\right\|^{2}\) can be as large as \(4G^{2}\). In the game setting where every player \(i\) employs OG, \(g_{i}^{t}\),i.e., \(-\nabla_{x_{i}}u_{i}(x)\), depends on other players' action \(x_{-i}^{t}\). Note that the change of the players' actions \(\left\|x^{t}-x^{t-1}\right\|^{2}\) is only \(O(\eta^{2})\). Such stability of the updates leads to an improved upper bound on \(\left\|g_{i}^{t}-g_{i}^{t-1}\right\|^{2}\) and hence also an improved \(O(T^{\frac{1}{4}})\)\(\Phi_{\operatorname{Proj}}^{\mathcal{X}}(\delta)\)-regret for the player.

Proof.: Let us fix any player \(i\in[n]\) in the smooth game. In every step \(t\), player \(i\)'s loss function \(f^{t}:\mathcal{X}_{i}\to\mathbb{R}\) is \(\left\langle-\nabla_{x_{i}}u_{i}(x^{t}),\cdot\right\rangle\) determined by their utility function \(u_{i}\) and all players' actions \(x^{t}\). Therefore, their gradient feedback is \(g^{t}=-\nabla_{x_{i}}u_{i}(x^{t})\). For all \(t\geq 2\), we have

\[\left\|g^{t}-g^{t-1}\right\|^{2} =\left\|\nabla u_{i}(x^{t})-\nabla u_{i}(x^{t-1})\right\|^{2}\] \[\leq L^{2}\left\|x^{t}-x^{t-1}\right\|^{2}\] \[=L^{2}\sum_{i=1}^{n}\left\|x_{i}^{t}-x_{i}^{t-1}\right\|^{2}\] \[\leq 3L^{2}\sum_{i=1}^{n}\left(\left\|x_{i}^{t}-w_{i}^{t}\right\| ^{2}+\left\|w_{i}^{t}-w_{i}^{t-1}\right\|^{2}+\left\|w_{i}^{t-1}-x_{i}^{t-1} \right\|^{2}\right)\] \[\leq 3nL^{2}\eta^{2}G^{2},\]

where we use \(L\)-smoothness of the utility function \(u_{i}\) in the first inequality; we use the update rule of \(\textsc{OG}\) and the fact that gradients are bounded by \(G\) in the last inequality.

Applying the above inequality to the regret bound obtained in Theorem 8, the individual \(\Phi_{\mathrm{Proj}}^{\mathcal{X}}(\delta)\)-regret of player \(i\) is upper bounded by

\[\mathrm{Reg}_{\mathrm{Proj},\delta}^{T,i}\leq\frac{\delta D}{\eta}+\eta G^{2}+ 3nL^{2}G^{2}\eta^{3}T.\]

Choosing \(\eta=\min\{(\delta D/(nL^{2}G^{2}T))^{\frac{1}{4}},(\delta D)^{\frac{1}{2}}/G\}\), we have \(\mathrm{Reg}_{\mathrm{Proj},\delta}^{T,i}\leq 4(\delta D)^{\frac{3}{4}}(nL^{2}G^{ 2}T)^{\frac{1}{4}}+2\sqrt{\delta D}G\). Using Lemma 1, we have the empirical distribution of played strategy profiles converge to an \((\varepsilon+\frac{\delta^{2}L}{2})\)-approximate \(\Phi_{\mathrm{Proj}}(\delta))\)-equilibrium in \(O(1/\varepsilon^{\frac{4}{3}})\) iterations. 

## Appendix F Missing Details in Section 4.2

``` Input:\(x_{1}\in\mathcal{X}\), \(K\geq 2\), a no-external-regret algorithm \(\mathfrak{R}_{\Phi}\) against linear reward over \(\Delta(\Phi)\) Output: A \(\mathrm{Conv}(\Phi)\)-regret minimization algorithm over \(\mathcal{X}\)
1functionNextStrategy() \(p^{t}\leftarrow\mathfrak{R}_{\Phi}\).NextStrategy(). Note that \(p_{t}\) is a distribution over \(\Phi\). \(x_{k}\leftarrow\phi_{p^{t}}(x_{k-1})\), for all \(2\leq k\leq K\) return\(x^{t}\leftarrow\) uniformly at random from \(\{x_{1},\ldots,x_{K}\}\).
2functionObserveReward(\(\nabla_{x}u^{t}(x^{t})\)) \(u_{\Phi}^{t}(\cdot)\leftarrow\) a linear reward over \(\Delta(\Phi)\) with \(u_{\Phi}^{t}(\phi)=\left\langle\nabla_{x}u^{t}(x^{t}),\phi(x^{t})-x^{t}\right\rangle\) for all \(\phi\in\Phi\). \(\mathfrak{R}_{\Phi}\).ObserveReward(\(u_{\Phi}^{t}(\cdot)\)). ```

**Algorithm 3**\(\mathrm{Conv}(\Phi)\)-regret minimization for Lipschitz smooth non-concave rewards

### Proof of Theorem 4

**Proof Sketch.** We adopt the framework in [1, 1] (as described in Section 3) with two main modifications. First, we utilize the \(L\)-smoothness of the utilities to transform the problem of external regret over \(\Delta(\Phi)\) against non-concave rewards into a linear optimization problem. Second, we use the technique of "fixed point in expectation" [15] to circumvent the intractable problem of finding a fixed point.

Proof.: For a sequence of strategies \(\{x^{t}\}_{t\in[T]}\), its \(\mathrm{Conv}(\Phi)\)-regret is

\[\mathrm{Reg}_{\mathrm{Conv}(\Phi)}^{T} =\max_{\phi\in\mathrm{Conv}(\Phi)}\left\{\sum_{t=1}^{T}\left(u^{t} (\phi(x^{t}))-u^{t}(x^{t})\right)\right\}\] \[=\underbrace{\max_{p\in\Delta(\Phi)}\left\{\sum_{t=1}^{T}u^{t}( \phi_{p}(x^{t}))-u^{t}(\phi_{p^{t}}(x^{t}))\right\}}_{\text{I: external regret over $\Delta(\Phi)$}}+\underbrace{\sum_{t=1}^{T}u^{t}(\phi_{p^{t}}(x^{t}))-u^{t}(x^{t})}_{ \text{II: approximation error of fixed point}}.\]Bounding External Regret over \(\Delta(\Phi)\)We can define a new reward function \(f^{t}(p):=u^{t}(\phi_{p}(x^{t}))\) over \(p\in\Delta(\Phi)\). Since \(u^{t}\) is non-concave, the reward \(f^{t}\) is also non-concave and it is computational intractable to minimize external regret. We use locality to avoid computational barrier. Here we use the fact that \(\Phi=\Phi(\delta)\) contains only \(\delta\)-local strategy modifications. Then by \(L\)-smoothness of \(u^{t}\), we know for any \(p\in\Delta(\Phi)\)

\[\big{|}u^{t}(\phi_{p}(x^{t})-u^{t}(x^{t})-\big{\langle}\nabla u^{t}(x^{t}), \phi_{p}(x^{t})-x^{t}\big{\rangle})\big{|}\leq\frac{L}{2}\big{\|}\phi_{p}(x^{t })-x^{t}\big{\|}^{2}\leq\frac{\delta^{2}L}{2}.\]

Thus we can approximate the non-concave optimization problem by a linear optimization problem over \(\Delta(\Phi)\) with only second-order error \(\frac{\delta^{2}L}{2}\). Here we use the notation \(a=b\pm c\) to mean \(b-c\leq a\leq b+c\).

\[u^{t}(\phi_{p}(x^{t})-u^{t}(x^{t}) =\big{\langle}\nabla u^{t}(x^{t}),\phi_{p}(x^{t})-x^{t}\big{\rangle} \pm\frac{\delta^{2}L}{2}\] \[=\left\langle\nabla u^{t}(x^{t}),\sum_{\phi\in\Phi}p(\phi)\phi(x^ {t})-x^{t}\right\rangle\pm\frac{\delta^{2}L}{2}\] \[=\sum_{\phi\in\Phi}p(\phi)\big{\langle}\nabla u^{t}(x^{t}),\phi(x ^{t})-x^{t}\big{\rangle}\pm\frac{\delta^{2}L}{2}.\]

We can then instantiate the external regret \(\mathfrak{R}_{\Phi}\) as the Hedge algorithm over reward \(f^{t}(p)=\sum_{\phi\in\Phi}p(\phi)\langle\nabla u^{t}(x^{t}),\phi(x^{t})-x^{t}\rangle\) and get

\[\max_{p\in\Delta(\Phi)}\left\{\sum_{t=1}^{T}u^{t}(\phi_{p}(x^{t}) )-u^{t}(\phi_{p^{t}}(x^{t}))\right\}\] \[\leq 2G\delta\sqrt{T\log|\Phi|}+\delta^{2}LT,\]

where we use the fact that \(\langle\nabla u^{t}(x^{t}),\phi(x^{t})-x^{t}\rangle\leq\|\nabla u^{t}(x^{t}) \|\cdot\|\phi(x^{t})-x^{t}\|\leq G\delta\).

Bounding error due to sampling from a fixed point in expectationWe choose \(x_{1}\) as an arbitrary point in \(\mathcal{X}\). Then we recursively apply \(\phi_{p^{t}}\) to get

\[x_{k}=\phi_{p^{t}}(x_{k-1})=\sum_{\phi\in\Phi}p^{t}(\phi)\phi(x_{k-1}),\forall 2 \leq k\leq K.\]

We denote \(\mu^{t}=\mathrm{Uniform}\{x_{k}:1\leq k\leq K\}\). Then the strategy \(x^{t}\sim\mu^{t}\) is sampled from \(\mu^{t}\). We have that \(\mu^{t}\) is an approximate fixed-point in expectation / stationary distribution in the sense that

\[\mathbb{E}_{\mu^{t}}\big{[}u^{t}(\phi_{p^{t}}(x^{t}))-u^{t}(x^{t} )\big{]} =\frac{1}{K}\sum_{k=1}^{K}u^{t}(\phi_{p^{t}}(x_{k})-u^{t}(x_{k}))\] \[=\frac{1}{K}\big{(}u^{t}(\phi_{p^{t}}(x_{K}))-u^{t}(x_{1})\big{)}\] \[\leq\frac{1}{K}.\]

Thanks to the boundedness of \(u^{t}\), we can use Hoeffding-Azuma's inequality to conclude that

\[\Pr\left[\sum_{t=1}^{T}\bigg{(}u^{t}(\phi_{p^{t}}(x^{t}))-u^{t}(x^{t})-\frac{1 }{K}\bigg{)}\geq\epsilon\right]\leq\exp{\left(-\frac{\varepsilon^{2}}{8T} \right)}.\] (4)

for any \(\varepsilon>0\). Combining the above with \(\varepsilon=\sqrt{8T\log(1/\beta)}\) and \(K=\sqrt{T}\), we get with probability at least \(1-\beta\),

\[\mathrm{Reg}_{\mathrm{Conv}(\Phi)}^{T} \leq 2G\delta\sqrt{T\log|\Phi|}+\delta^{2}LT+\sqrt{T}+\sqrt{8T \log(1/\beta)}\] \[\leq 8\sqrt{T}\Big{(}G\delta\sqrt{\log|\Phi|}+\sqrt{\log(1/\beta)} \Big{)}+\delta^{2}LT.\]Convergence to \(\Phi\)-equilibriumIf all players in a non-concave continuous game employ Algorithm 1, then we know for each player \(i\), with probability \(1-\frac{\beta}{n}\), its \(\Phi^{\mathcal{X}_{i}}\)-regret is upper bounded by

\[8\sqrt{T}\bigg{(}G\delta\sqrt{\log|\Phi^{\mathcal{X}_{i}}|}+\sqrt{\log(n/\beta) }\bigg{)}+\delta^{2}LT.\]

By a union bound over all \(n\) players, we get with probability \(1-\beta\), every player \(i\)'s \(\Phi^{\mathcal{X}_{i}}\)-regret is upper bounded by \(8\sqrt{T}(G\delta\sqrt{\log|\Phi^{+\mathcal{X}_{i}}|}+\sqrt{\log(n/\beta)})+ \delta^{2}LT\). Now by Theorem 1, we know the empirical distribution of strategy profiles played forms an \((\varepsilon+\delta^{2}L)\)-approximate \(\Phi=\Pi_{i=1}^{n}\Phi^{\mathcal{X}_{i}}\)-equilibrium, as long as \(T\geq\frac{128(G^{2}\delta^{2}\log|\Phi^{\mathcal{X}_{i}}|+\log(n/\beta))}{ \varepsilon^{2}}\) iterations. 

## Appendix G Missing details in Section 4.3

We introduce a natural set of local strategy modifications and the corresponding local equilibrium notion. Given any set of (possibly non-local) strategy modifications \(\Psi=\{\psi:\mathcal{X}\rightarrow\mathcal{X}\}\), we define a set of _local_ strategy modifications as follows: for \(\delta\leq D_{\mathcal{X}}\) and \(\lambda\in[0,1]\), each strategy modification \(\phi_{\lambda,\psi}\) interpolates the input strategy \(x\) with the modified strategy \(\psi(x)\): formally,

\[\Phi^{\mathcal{X}}_{\mathrm{Int},\Psi}(\delta):=\left\{\phi_{\lambda,\psi}(x) :=(1-\lambda)x+\lambda\psi(x):\psi\in\Psi,\lambda\leq\delta/D_{\mathcal{X}} \right\}.\]

Note that for any \(\psi\in\Psi\) and \(\lambda\leq\frac{\delta}{D_{\mathcal{X}}}\), we have \(\|\phi_{\lambda,\psi}(x)-x\|=\lambda\|x-\psi(x)\|\leq\delta\), respecting the locality constraint. The induced \(\Phi^{\mathcal{X}}_{\mathrm{Int},\Psi}(\delta)\)-regret can be written as \(\mathrm{Reg}^{T}_{\mathrm{Int},\Psi,\delta}:=\max_{\psi\in\Psi,\lambda\leq \frac{\delta}{D_{\mathcal{X}}}}\sum_{t=1}^{T}\left(f^{t}(x^{t})-f^{t}((1- \lambda)x^{t}+\lambda\psi(x^{t}))\right)\). We now define the corresponding \(\Phi_{\mathrm{Int},\Psi}(\delta)\)-equilibrium.

**Definition 9**.: _Define \(\Phi_{\mathrm{Int},\Psi}(\delta)=\Pi_{j=1}^{n}\Phi^{\mathcal{X}_{j}}_{\mathrm{ Int},\Psi_{j}}(\delta)\). In a continuous game, a distribution \(\sigma\) over strategy profiles is an (\(\varepsilon\)-approximate \(\Phi_{\mathrm{Int},\Psi}(\delta)\)-equilibrium if and only if for all player \(i\in[n]\),_

\[\max_{\psi\in\Psi_{i,\lambda}\leq\delta/D_{\mathcal{X}_{i}}}\mathbb{E}_{x \sim\sigma}[u_{i}((1-\lambda)x_{i}+\lambda\psi(x_{i}),x_{-i})]\leq\mathbb{E}_ {x\sim\sigma}[u_{i}(x)]+\varepsilon.\]

Intuitively speaking, when a correlation device recommends strategies to players according to an \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int},\Psi}(\delta)\)-equilibrium, no player can increase their utility by more than \(\varepsilon\) through a local deviation by interpolating with a (possibly global) strategy modification \(\psi\in\Psi\). The richness of \(\Psi\) determines the incentive guarantee provided by an \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int},\Psi}(\delta)\)-equilibriumas well as its computational complexity. When we choose \(\Psi\) to be the set of all possible strategy modifications, the corresponding notion of local equilibrium--limiting the gain of a player by interpolating with any strategy--resembles that of a _correlated equilibrium_.

Computation of \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int},\Psi}(\delta)\)-Equilibrium.By Lemma 1, we know computing an \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int},\Psi}(\delta)\)-equilibrium reduces to minimizing \(\Phi^{\mathcal{X}}_{\mathrm{Int},\Psi}(\delta)\)-regret against convex loss functions. We show that minimizing \(\Phi^{\mathcal{X}}_{\mathrm{Int},\Psi}(\delta)\)-regret against convex loss functions further reduces to \(\Psi\)-regret minimization against linear loss functions.

**Theorem 10**.: _Let \(\mathcal{A}\) be an algorithm with \(\Psi\)-regret \(\mathrm{Reg}^{T}_{\Psi}(G,D_{\mathcal{X}})\) for linear and \(G\)-Lipschitz loss functions over \(\mathcal{X}\). Then, for any \(\delta>0\), the \(\Phi^{\mathcal{X}}_{\mathrm{Int},\Psi}(\delta)\)-regret of \(\mathcal{A}\) for convex and \(G\)-Lipschitz loss functions over \(\mathcal{X}\) is at most \(\frac{\delta}{D_{\mathcal{X}}}\cdot\left[\mathrm{Reg}^{T}_{\Psi}(G,D_{ \mathcal{X}})\right]^{+}\)._

Proof.: By definition and convexity of \(f^{t}\), we get

\[\max_{\phi\in\Phi^{\mathcal{X}}_{\mathrm{Int},\Psi}(\delta)}\sum _{t=1}^{T}f^{t}(x^{t})-f^{t}(\phi(x^{t})) =\max_{\psi\in\Psi,\lambda\leq\frac{\delta}{D_{\mathcal{X}}}} \sum_{t=1}^{T}f^{t}(x^{t})-f^{t}((1-\lambda)x^{t}+\lambda\psi(x^{t}))\] \[\leq\frac{\delta}{D_{\mathcal{X}}}\left[\max_{\psi\in\Psi}\sum_{t =1}^{T}\left\langle\nabla f^{t}(x^{t}),x^{t}-\psi(x^{t})\right\rangle\right]^{ +}.\]Note that when \(f^{t}\) is linear, the reduction is without loss. Thus, any worst-case \(\Omega(r(T))\)-lower bound for \(\Psi\)-regret implies a \(\Omega(\frac{\delta}{D_{\mathcal{X}}}\cdot r(T))\) lower bound for \(\Phi_{\mathrm{Int},\Psi}(\delta)\)-regret. Moreover, for any set \(\Psi\) that admits efficient \(\Psi\)-regret minimization algorithms such as swap transformations over the simplex and more generally any set such that (i) all modifications in the set can be represented as linear transformations in some finite-dimensional space and (ii) fixed point computation can be carried out efficiently for any linear transformations [1], we also get an efficient algorithm for computing an \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int},\Psi}(\delta)\)-equilibrium in the first-order stationary regime.

CCE-like InstantiationIn the special case where \(\Psi\) contains only _constant_ strategy modifications (i.e. \(\psi(x)=x^{*}\) for all \(x\)), we get a coarse correlated equilibrium (CCE)-like instantiation of local equilibrium, which limits the gain by interpolating with any _fixed_ strategy. We denote the resulting set of local strategy modification simply as \(\Phi_{\mathrm{Int}}^{\mathcal{X}}\). We can apply any no-external regret algorithm for efficient \(\Phi_{\mathrm{Int}}^{\mathcal{X}}\)-regret minimization and computation of \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int}}(\delta)\)-equilibrium in the first-order stationary regime as summarized in Theorem5.

The above \(\Phi_{\mathrm{Int}}^{\mathcal{X}}(\delta)\)-regret bound of \(O(\sqrt{T})\) is derived for the adversarial setting. In the game setting, where each player employs the same algorithm, players may have substantially lower external regret [1, 1, 2, 1, 1] but we need a slightly stronger smoothness assumption than Assumption1. This assumption is naturally satisfied by finite normal-form games and is also made for results about concave games [15]. Using Assumption2 and Lemma1, the no-regret learning dynamics of [15] that guarantees \(O(\log T)\) individual external regret in concave games can be applied to smooth non-concave games so that the individual \(\Phi_{\mathrm{Int}}^{\mathcal{X}}(\delta)\)-regret of each player is at most \(O(\log T)+\frac{\delta^{2}LT}{2}\). This gives an algorithm with faster \(\tilde{O}(1/\varepsilon)\) convergence to an \((\varepsilon+\frac{\delta^{2}L}{2})\)-approximate \(\Phi_{\mathrm{Int}}(\delta)\)-equilibrium than GD.

## Appendix H Beam-Search Local Strategy Modifications and Local Equilibria

In Section4.1 and Section4.3, we have shown that GD achieves near-optimal performance for both \(\Phi_{\mathrm{Int}}^{\mathcal{X}}(\delta)\)-regret and \(\Phi_{\mathrm{Proj}}^{\mathcal{X}}(\delta)\)-regret. In this section, we introduce another natural set of local strategy modifications, \(\Phi_{\mathrm{Beam}}^{\mathcal{X}}(\delta)\), which is similar to \(\Phi_{\mathrm{Proj}}^{\mathcal{X}}(\delta)\). Specifically, the set \(\Phi_{\mathrm{Beam}}^{\mathcal{X}}(\delta)\) contains deviations that try to move as far as possible in a fixed direction (see Figure2 for an illustration of the difference between \(\phi_{\mathrm{Beam},v}(x)\) and \(\phi_{\mathrm{Proj},v}(x)\)):

\[\Phi_{\mathrm{Beam}}^{\mathcal{X}}(\delta):=\{\phi_{\mathrm{Beam},v}(x)=x- \lambda^{*}v:v\in B_{d}(\delta),\lambda^{*}=\max\{\lambda:x-\lambda v\in \mathcal{X},\lambda\in[0,1]\}\}.\]

It is clear that \(\|\phi_{\mathrm{Beam},v}(x)-x\|\leq\|v\|\leq\delta\). We can similarly derive the notion of \(\Phi_{\mathrm{Beam}}^{\mathcal{X}}\)-regret and \((\varepsilon,\Phi_{\mathrm{Beam}}(\delta))\)-equilibrium. Surprisingly, we show that GD suffers linear \(\Phi_{\mathrm{Beam}}^{\mathcal{X}}(\delta)\)-regret (proof deferred to AppendixH.1).

**Theorem 11**.: _For any \(\delta,\eta<\frac{1}{2}\) and \(T\geq 1\), there exists a sequence of linear loss functions \(\{f^{t}:\mathcal{X}\subseteq[0,1]^{2}\to\mathbbm{R}\}_{t\in[T]}\) such that GD with step size \(\eta\) suffers \(\Omega(\delta T)\)\(\Phi_{\mathrm{Beam}}^{\mathcal{X}}(\delta)\)-regret._

### Proof of Theorem 11

Let \(\mathcal{X}\subset\mathbb{R}^{2}\) be a triangle region with vertices \(A=(0,0)\), \(B=(1,1)\), \(C=(\delta,0)\). Consider \(v=(-\delta,0)\). The initial point is \(x_{1}=(0,0)\).

The adversary will choose \(\ell_{t}\) adaptively so that \(x_{t}\) remains on the boundary of \(\mathcal{X}\) and cycles clockwise (i.e., \(A\to\cdots\to B\to\cdots\to C\to\cdots\to A\to\cdots\)). To achieve this, the adversary will repeat the following three phases:

1. Keep choosing \(\ell_{t}=u_{\overrightarrow{BA}}\) (\(u_{\overrightarrow{BA}}\) denotes the unit vector in the direction of \(\overrightarrow{BA}\)) until \(x_{t+1}\) reaches \(B\).
2. Keep choosing \(\ell_{t}=u_{\overrightarrow{CB}}\) until \(x_{t+1}\) reaches \(C\).
3. Keep choosing \(\ell_{t}=u_{\overrightarrow{AC}}\) until \(x_{t+1}\) reaches \(A\).

In Phase 1, \(x_{t}\in\overline{AB}\). By the choice of \(v=(-\delta,0)\), we have \(x_{t}-\phi_{v}(x_{t})=(-\delta(1-x_{t,1}),0)\), and the instantaneous regret is \(\frac{\delta(1-x_{t,1})}{\sqrt{2}}\geq 0\).

In Phase 2, \(x_{t}\in\overline{BC}\). By the choice of \(v=(-\delta,0)\), we have \(x_{t}-\phi_{v}(x_{t})=(0,0)\), and the instantaneous regret is \(0\).

In Phase 3, \(x_{t}\in\overline{CA}\). By the choice of \(v=(-\delta,0)\), we have \(x_{t}-\phi_{v}(x_{t})=(-\delta+x_{t,1},0)\), and the instantaneous regret is \(-\delta+x_{t,1}\leq 0\).

In each cycle, the number of rounds in Phase 1 is of order \(\Theta(\frac{\sqrt{2}}{\eta})\), the number of rounds in Phase 2 is between \(O(\frac{1}{\eta})\) and \(O(\frac{\sqrt{2}}{\eta})\), the number of rounds in Phase 3 is of order \(\Theta(\frac{\delta}{\eta})\).

Therefore, the cumulative regret in each cycle is roughly

\[\frac{\sqrt{2}}{\eta}\times\frac{0.5\delta}{\sqrt{2}}+0+\frac{\delta}{\eta} \left(-0.5\delta\right)=\frac{0.5\delta-0.5\delta^{2}}{\eta}.\]

On the other hand, the number of cycles is no less than \(\frac{T}{\frac{\sqrt{2}}{\eta}+\frac{\sqrt{2}}{\eta}+\frac{\delta}{\eta}}= \Theta(\eta T)\). Overall, the cumulative regret is at least \(\frac{0.5\delta-0.5\delta^{2}}{\eta}\times\Theta(\eta T)=\Theta(\delta T)\) as long as \(\delta<0.5\).

## Appendix I Hardness in the Global Regime

In the first-order stationary regime \(\delta\leq\sqrt{2\varepsilon/L}\), \((\varepsilon,\delta)\)-local Nash equilibrium is intractable, and we have shown polynomial-time algorithms for computing the weaker notions of \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int}}(\delta))\)-equilibrium and \(\varepsilon\)-approximate \(\Phi_{\mathrm{Proj}}(\delta))\)-equilibrium. A natural question is whether correlation enables efficient computation of \(\varepsilon\)-approximate \(\Phi(\delta))\)-equilibrium when \(\delta\) is in the global regime, i.e., \(\delta=\Omega(\sqrt{d})\). In this section, we prove both computational hardness and a query complexity lower bound for both notions in the global regime

To prove the lower bound results, we only require a single-player game. The problem of computing an \(\varepsilon\)-approximate \(\Phi(\delta)\)-equilibrium becomes: given scalars \(\varepsilon,\delta,G,L>0\) and a polynomial-time Turing machine \(\mathcal{C}_{f}\) evaluating a \(G\)-Lipschitz and \(L\)-smooth function \(f:[0,1]^{d}\to[0,1]\) and its gradient \(\nabla f:[0,1]^{d}\to\mathbb{R}^{d}\), we are asked to output a distribution \(\sigma\) that is an \(\varepsilon\)-approximate \(\Phi(\delta)\)-equilibrium or \(\perp\) if such equilibrium does not exist.

Hardness of finding \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int}}^{\mathcal{X}}(\delta)\)-equilibria in the global regimeWhen \(\delta=\sqrt{d}\), which equals to the diameter \(D\) of \([0,1]^{d}\), then the problem of finding an \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int}}^{\mathcal{X}}(\delta)\)-equilibrium is equivalent to finding a \((\varepsilon,\delta)\)-local minimum of \(f\): assume \(\sigma\) is an \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int}}^{\mathcal{X}}(\delta)\)-equilibrium of \(f\), then there exists \(x\in[0,1]^{d}\) in the support of \(\sigma\) such that

\[f(x)-\min_{x^{*}\in[0,1]^{d}\cap B_{d}(x^{*},\delta)}f(x^{*})\leq\varepsilon.\]

Then hardness of finding an \(\varepsilon\)-approximate \(\Phi_{\mathrm{Int}}^{\mathcal{X}}(\delta)\)-equilibrium follows from hardness of finding a \((\varepsilon,\delta)\)-local minimum of \(f\)[4]. The following Theorem is a corollary of Theorem 10.3 and 10.4 in [4].

**Theorem 12** (Hardness of finding \(\varepsilon\)-approximate \(\Phi^{\mathcal{X}}_{\mathrm{Int}}(\delta)\)-equilibria in the global regime).: _In the worst case, the following two holds._

* _Computing an_ \(\varepsilon\)_-approximate_ \(\Phi^{\mathcal{X}}_{\mathrm{Int}}(\delta)\)_-equilibrium for a game on_ \(\mathcal{X}=[0,1]^{d}\) _with_ \(G=\sqrt{d}\)_,_ \(L=d\)_,_ \(\varepsilon\leq\frac{1}{24}\)_,_ \(\delta=\sqrt{d}\) _is NP-hard._
* \(\Omega(2^{d}/d)\) _value/gradient queries are needed to determine an_ \(\varepsilon\)_-approximate_ \(\Phi^{\mathcal{X}}_{\mathrm{Int}}(\delta)\)_-equilibrium for a game on_ \(\mathcal{X}=[0,1]^{d}\) _with_ \(G=\Theta(d^{15})\)_,_ \(L=\Theta(d^{22})\)_,_ \(\varepsilon<1\)_,_ \(\delta=\sqrt{d}\)_._

Hardness of finding \(\varepsilon\)-approximate \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-equilibria in the global regime

**Theorem 13** (Hardness of of finding \(\varepsilon\)-approximate \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-equilibria in the global regime).: _In the worst case, the following two holds._

* _Computing an_ \(\varepsilon\)_-approximate_ \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)_-equilibrium for a game on_ \(\mathcal{X}=[0,1]^{d}\) _with_ \(G=\Theta(d^{15})\)_,_ \(L=\Theta(d^{22})\)_,_ \(\varepsilon<1\)_,_ \(\delta=\sqrt{d}\) _is NP-hard._
* \(\Omega(2^{d}/d)\) _value/gradient queries are needed to determine an_ \(\varepsilon\)_-approximate_ \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)_-equilibrium for a game on_ \(\mathcal{X}=[0,1]^{d}\) _with_ \(G=\Theta(d^{15})\)_,_ \(L=\Theta(d^{22})\)_,_ \(\varepsilon<1\)_,_ \(\delta=\sqrt{d}\)_._

The hardness of computing \(\varepsilon\)-approximate \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-equilibrium also implies a lower bound on \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret in the global regime.

**Corollary 2** (Lower bound of \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret against non-convex functions).: _In the worst case, the \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-regret of any online algorithm is at least \(\Omega(2^{d}/d,T)\) even for loss functions \(f:[0,1]^{d}\to[0,1]\) with \(G,L=\mathrm{poly}(d)\) and \(\delta=\sqrt{d}\)._

The proofs of Theorem13 and Corollary2 can be found in the next two sections.

### Proof of Theorem13

We will reduce the problem of finding an \(\varepsilon\)-approximate \(\Phi^{\mathcal{X}}_{\mathrm{Proj}}(\delta)\)-equilibrium in smooth games to finding a satisfying assignment of a boolean function, which is NP-complete.

**Fact 1**.: _Given only black-box access to a boolean formula \(\phi:\{0,1\}^{d}\to\{0,1\}\), at least \(\Omega(2^{d})\) queries are needed in order to determine whether \(\phi\) admits a satisfying assignment \(x^{*}\) such that \(\phi(x^{*})=1\). The term black-box access refers to the fact that the clauses of the formula are not given, and the only way to determine whether a specific boolean assignment is satisfying is by querying the specific binary string. Moreover, the problem of finding a satisfying assignment of a general boolean function is NP-hard._

We revisit the construction of the hard instance in the proof of [13, Theorem 10.4] and use its specific structures. Given black-box access to a boolean formula \(\phi\) as described in Fact1, following [13], we construct the function \(f_{\phi}(x):[0,1]^{d}\to[0,1]\) as follows:

1. for each corner \(v\in V=\{0,1\}^{d}\) of the \([0,1]^{d}\) hypercube, we set \(f_{\phi}(x)=1-\phi(x)\).
2. for the rest of the points \(x\in[0,1]^{d}/V\), we set \(f_{\phi}(x)=\sum_{v\in V}P_{v}(x)\cdot f_{\phi}(v)\) where \(P_{v}(x)\) are non-negative coefficients defined in [13, Definition 8.9].

The function \(f_{\phi}\) satisfies the following properties:

1. if \(\phi\) is not satisfiable, then \(f_{\phi}(x)=1\) for all \(x\in[0,1]^{d}\) since \(f_{\phi}(v)=1\) for all \(v\in V\); if \(\phi\) has a satisfying assignment \(v^{*}\), then \(f_{\phi}(v^{*})=0\).
2. \(f_{\phi}\) is \(\Theta(d^{12})\)-Lipschitz and \(\Theta(d^{25})\)-smooth.
3. for any point \(x\in[0,1]^{d}\), the set \(V(x):=\{v\in V:P_{v}(x)\neq 0\}\) has cardinality at most \(d+1\) while \(\sum_{v\in V}P_{v}(x)=1\); any value / gradient query of \(f_{\phi}\) can be simulated by \(d+1\) queries on \(\phi\).

In the case there exists a satisfying argument \(v^{*}\), then \(f_{\phi}(v^{*})=0\). Define the deviation \(e\) so that \(e[i]=1\) if \(v^{*}[i]=0\) and \(e[i]=-1\) if \(v^{*}[i]=1\). It is clear that \(\|e\|=\sqrt{d}=\delta\). By properties of projection on \([0,1]^{d}\), for any \(x\in[0,1]^{d}\), we have \(\Pi_{[0,1]^{n}}[x-v]=v^{*}\). Then any \(\varepsilon\)-approximate \(\Phi_{\mathrm{Proj}}^{X}(\delta)\)-equilibrium \(\sigma\) must include some \(x^{*}\in\mathcal{X}\) with \(f_{\phi}(x^{*})<1\) in the support, since \(\varepsilon<1\). In case there exists an algorithm \(\mathcal{A}\) that computes an \(\varepsilon\)-approximate \(\Phi_{\mathrm{Proj}}^{X}(\delta)\)-equilibrium, \(\mathcal{A}\) must have queried some \(x^{*}\) with \(f_{\phi}(x^{*})<1\). Since \(f_{\phi}(x^{*})=\sum_{v\in V(x^{*})}P_{v}(x^{*})f_{\phi}(v)<1\), there exists \(\hat{v}\in V(x^{*})\) such that \(f_{\phi}(\hat{v})=0\). Since \(|V(x^{*})|\leq d+1\), it takes addition \(d+1\) queries to find \(\hat{v}\) with \(f_{\phi}(\hat{v})=0\). By 1 and the fact that we can simulate every value/gradient query of \(f_{\phi}\) by \(d+1\) queries on \(\phi\), \(\mathcal{A}\) makes at least \(\Omega(2^{d}/d)\) value/gradient queries.

Suppose there exists an algorithm \(\mathcal{B}\) that outputs an \(\varepsilon\)-approximate \(\Phi_{\mathrm{Proj}}^{X}(\delta)\)-equilibrium \(\sigma\) in time \(T(\mathcal{B})\) for \(\varepsilon<1\) and \(\delta=\sqrt{d}\). We construct another algorithm \(\mathcal{C}\) for SAT that terminates in time \(T(\mathcal{B})\cdot\mathrm{poly}(d)\). \(\mathcal{C}\): (1) given a boolean formula \(\phi\), construct \(f_{\phi}\) as described above; (2) run \(\mathcal{B}\) and get output \(\sigma\) (3) check the support of \(\sigma\) to find \(v\in\{0,1\}^{d}\) such that \(f_{\phi}(v)=0\); (3) if finds \(v\in\{0,1\}^{d}\) such that \(f_{\phi}(v)=0\), then \(\phi\) is satisfiable, otherwise \(\phi\) is not satisfiable. Since we can evaluate \(f_{\phi}\) and \(\nabla f_{\phi}\) in \(\mathrm{poly}(d)\) time and the support of \(\sigma\) is smaller than \(T(\mathcal{B})\), the algorithm \(\mathcal{C}\) terminates in time \(O(T(\mathcal{B})\cdot\mathrm{poly}(d))\). The above gives a polynomial time reduction from SAT to finding an \(\varepsilon\)-approximate \(\Phi_{\mathrm{Proj}}^{X}(\delta)\)-equilibrium and proves the NP-hardness of the latter problem.

### Proof of Corollary 2

Let \(\phi:\{0,1\}^{d}\to\{0,1\}\) be a boolean formula and define \(f_{\phi}:[0,1]^{d}\to[0,1]\) the same as that in Theorem 13. We know \(f_{\phi}\) is \(\Theta(\mathrm{poly}(d))\)-Lipschitz and \(\Theta(\mathrm{poly}(d))\)-smooth. Now we let the adversary pick \(f_{\phi}\) each time. For any \(T\leq O(2^{d}/d)\), in case there exists an online learning algorithm with \(\mathrm{Reg}_{\mathrm{Proj},\delta}^{T}<\frac{T}{2}\), then \(\sigma:=\frac{1}{T}\sum_{t=1}^{T}1_{x^{t}}\) is an \((\frac{1}{2},\delta)\)-equilibrium. Applying 13 and the fact that in this case, \(\mathrm{Reg}_{\mathrm{Proj},\delta}^{T}\) is non-decreasing with respect to \(T\) concludes the proof.

## Appendix J Removing the \(D\) dependence for \(\Phi_{\mathrm{Proj}}^{X}\)-regret

For the regime \(\delta\leq D_{\mathcal{X}}\) which we are more interested in, the lower bound in Theorem 7 is \(\Omega(G\delta\sqrt{T})\) while the upper bound in Theorem 3 is \(O(G\sqrt{\delta D_{\mathcal{X}}T})\). They are not tight especially when \(D_{\mathcal{X}}\gg\delta\). A natural question is: _which of them is the tight bound?_ We conjecture that the lower bound is tight. In fact, for the special case where the feasible set \(\mathcal{X}\) is a _box_, we have a way to obtain a \(D_{\mathcal{X}}\)-independent bound \(O(d^{\frac{1}{4}}G\delta\sqrt{T})\), which is tight when \(d=1\). Below, we first describe the improved strategy in \(1\)-dimension. Then we show how to extend it to the \(d\)-dimensional box setting.

### One-Dimensional Case

In one-dimension, we assume that \(\mathcal{X}=[a,b]\) for some \(b-a\geq 2\delta\) (if \(b-a\leq 2\delta\), then our original bound in Theorem 3 is already of order \(G\delta\sqrt{T}\)). We first investigate the case where \(f^{t}(x)\) is a linear function, i.e., \(f^{t}(x)=g^{t}x\) for some \(g^{t}\in[-G,G]\). The key idea is that we will only select \(x^{t}\) from the two intervals \([a,a+\delta]\) and \([b-\delta,b]\), and never play \(x^{t}\in(a+\delta,b-\delta)\). To achieve so, we concatenate these two intervals, and run an algorithm in this region whose diameter is only \(2\delta\). The key property we would like to show is that the regret is preserved in this modified problem.

More precisely, given the original feasible set \(\mathcal{X}=[a,b]\), we create a new feasible set \(\mathcal{Y}=[-\delta,\delta]\) and apply our algorithm GD in this new feasible set. The loss function is kept as \(f^{t}(x)=g^{t}x\). Whenever the algorithm for \(\mathcal{Y}\) outputs \(y^{t}\in[-\delta,0]\), we play \(x^{t}=y^{t}+a+\delta\) in \(\mathcal{X}\); whenever it outputs \(y^{t}\in(0,\delta]\), we play \(x^{t}=y^{t}+b-\delta\). Below we show that the regret is the same in these two problems. Notice that when \(y^{t}\leq 0\), we have for any \(v\in[-\delta,\delta]\),

\[x^{t}-\Pi_{\mathcal{X}}[x^{t}-v] =x^{t}-\max\left(\min\left(x^{t}-v,b\right),a\right)\] \[=x^{t}-\max\left(x^{t}-v,a\right)\] \[\qquad\qquad\qquad\qquad(x^{t}-v=y^{t}+a+\delta-v\leq a+2\delta \leq b\text{ always holds})\] \[=y^{t}+a+\delta-\max\left(y^{t}+a+\delta-v,a\right)\] \[=y^{t}-\max\left(y^{t}-v,-\delta\right)\] \[=y^{t}-\max\left(\min\left(y^{t}-v,\delta\right),-\delta\right) \qquad\qquad\text{ ($y^{t}-v\leq\delta$ always holds)}\] \[=y^{t}-\Pi_{\mathcal{Y}}[y^{t}-v]\]

Similarly, when \(y^{t}>0\), we can follow the same calculation and prove \(x^{t}-\Pi_{\mathcal{X}}[x^{t}-v]=y^{t}-\Pi_{\mathcal{Y}}[y^{t}-v]\). Thus, the regret in the two problems:

\[g^{t}\left(x^{t}-\Pi_{\mathcal{X}}[x^{t}-v]\right)\quad\text{ and}\quad g^{t}\left(y^{t}-\Pi_{\mathcal{Y}}[y^{t}-v]\right)\]

are exactly the same for any \(v\). Finally, observe that the diameter of \(\mathcal{Y}\) is only of order \(O(\delta)\). Thus, the upper bound in Theorem3 would give us an upper bound of \(O(G\sqrt{\delta\cdot\delta T})=O(G\delta\sqrt{T})\).

For convex \(f^{t}\), we run the algorithm above with \(g^{t}=\nabla f^{t}(x^{t})\). Then by convexity we have

\[f^{t}(x^{t})-f^{t}(\Pi_{\mathcal{X}}[x^{t}-v])\leq g^{t}(x^{t}-\Pi_{\mathcal{X }}[x^{t}-v])=g^{t}(y^{t}-\Pi_{\mathcal{Y}}[y^{t}-v]),\]

so the regret in the modified problem (which is \(O(G\delta\sqrt{T})\)) still serves as a regret upper bound for the original problem.

### \(d\)-Dimensional Box Case

A \(d\)-dimensional box is of the form \(\mathcal{X}=[a_{1},b_{1}]\times[a_{2},b_{2}]\times\dots\times[a_{d},b_{d}]\). The box case is easy to deal with because we can decompose the regret into individual components in each dimension. Namely, we have

\[f^{t}(x^{t})-f^{t}(\Pi_{\mathcal{X}}[x^{t}-v]) \leq\nabla f^{t}(x^{t})^{\top}\left(x^{t}-\Pi_{\mathcal{X}}[x^{t }-v]\right)\] \[=\sum_{i=1}^{d}g_{i}^{t}\left(x_{i}^{t}-\Pi_{\mathcal{X}_{i}}[x_ {i}^{t}-v_{i}]\right)\]

where we define \(\mathcal{X}_{i}=[a_{i},b_{i}]\), \(g^{t}=\nabla f^{t}(x^{t})\), and use subscript \(i\) to indicate the \(i\)-th component of a vector. The last equality above is guaranteed by the box structure. This decomposition allows as to view the problem as \(d\) independent \(1\)-dimensional problems.

Now we follow the strategy described in SectionJ.1 to deal with individual dimensions (if \(b_{i}-a_{i}<2\delta\) then we do not modify \(\mathcal{X}_{i}\); otherwise, we shrink \(\mathcal{X}_{i}\) to be of length \(2\delta\)). Applying the analysis of Theorem3 to each dimension, we get

\[\sum_{i=1}^{d}g_{i}^{t}\left(x_{i}^{t}-\Pi_{\mathcal{X}_{i}}[x_ {i}^{t}-v_{i}]\right)\] \[\leq\sum_{i=1}^{d}\left(\frac{v_{i}^{2}}{2\eta}+\frac{\eta}{2} \sum_{t=1}^{T}(g_{i}^{t})^{2}+\frac{|v_{i}|\times 2\delta}{\eta}\right)\text{ (the diameter in each dimension is now bounded by $2\delta$)}\] \[\leq O\left(\frac{\delta\sum_{i=1}^{d}|v_{i}|}{\eta}+\eta G^{2}T\right)\] \[\leq O\left(\frac{\delta^{2}\sqrt{d}}{\eta}+\eta G^{2}T\right). \qquad\qquad\qquad\text{(by Cauchy-Schwarz, $\sum_{i}|v_{i}|\leq\sqrt{d}\sqrt{\sum_{i}|v_{i}|^{2}}\leq\delta\sqrt{d}$)}\]

Choosing the optimal \(\eta=\frac{d^{\frac{1}{\delta}}\delta}{G\sqrt{T}}\), we get the regret upper bound of order \(O\left(d^{\frac{1}{\delta}}G\delta\sqrt{T}\right)\).

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We state the problem setting and our main contributions in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: Our results provide efficient uncoupled algorithms to compute \(\varepsilon\)-approximate \(\Phi\)-equilibria in the first-order stationary regime. We leave the computational complexity of finding \(\varepsilon\)-approximate \(\Phi\)-equilibria beyond the first-order stationary regime as an open question. Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide assumptions and proofs for our theoretical results in the main body and the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA]Justification: This paper does not contain experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This paper does not contain experimental results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This paper does not contain experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This paper does not contain experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This paper does not contain experimental results. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and the current paper conforms the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a purely theoretical paper and we do not see any immediate societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: This is a purely theoretical paper Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This is a purely theoretical paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This is a purely theoretical paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not contain experiments. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not contain experiments. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.