# Learning Probabilistic Symmetrization for

Architecture Agnostic Equivariance

 Jinwoo Kim  Tien Dat Nguyen  Ayhan Suleymanzade

Hyeokjun An  Seunghoon Hong

KAIST

###### Abstract

We present a novel framework to overcome the limitations of equivariant architectures in learning functions with group symmetries. In contrary to equivariant architectures, we use an arbitrary base model such as an MLP or a transformer and symmetrize it to be equivariant to the given group by employing a small equivariant network that parameterizes the probabilistic distribution underlying the symmetrization. The distribution is end-to-end trained with the base model which can maximize performance while reducing sample complexity of symmetrization. We show that this approach ensures not only equivariance to given group but also universal approximation capability in expectation. We implement our method on various base models, including patch-based transformers that can be initialized from pretrained vision transformers, and test them for a wide range of symmetry groups including permutation and Euclidean groups and their combinations. Empirical tests show competitive results against tailored equivariant architectures, suggesting the potential for learning equivariant functions for diverse groups using a non-equivariant universal base architecture. We further show evidence of enhanced learning in symmetric modalities, like graphs, when pretrained from non-symmetric modalities, like vision. Code is available at https://github.com/jw9730/lps.

## 1 Introduction

Many perception problems in machine learning involve functions that are invariant or equivariant to certain symmetry group of transformations of data. Examples include learning on sets and graphs, point clouds, molecules, proteins, and physical data, to name a few [10, 12]. Equivariant architecture design has emerged as a successful approach, where every building block of a model is carefully restricted to be equivariant to a symmetry group of interest [5, 12, 23]. However, equivariant architecture design faces fundamental limitations, as individual construction of models for each group can be laborious or computationally expensive [91, 58, 64, 45], the architectural restrictions often lead to limited expressive power [101, 56, 105, 40], and the knowledge learned from one problem cannot be easily transferred to others of different symmetries as the architecture would be incompatible.

This motivates us to seek a **symmetrization** solution that can achieve group invariance and equivariance with general-purpose, group-agnostic architectures such as an MLP or a transformer. As a basic form of symmetrization, any parameterized function \(f_{\theta}:\mathcal{X}\rightarrow\mathcal{Y}\) on vector spaces \(\mathcal{X}\), \(\mathcal{Y}\) can be made invariant or equivariant by group averaging [102, 67], _i.e._, averaging over all possible transformations of inputs \(\mathbf{x}\in\mathcal{X}\) and outputs \(\mathbf{y}\in\mathcal{Y}\) by a symmetry group \(G=\{g\}\):

\[\phi_{\theta}(\mathbf{x})=\frac{1}{|G|}\sum_{g\in G}g\cdot f_{\theta}(g^{-1} \cdot\mathbf{x}),\] (1)

where \(\phi_{\theta}\) is equivariant or invariant to the group \(G\). An important advantage is that the symmetrized function \(\phi_{\theta}\) can leverage the expressive power of the base function \(f_{\theta}\); it has been shown that \(\phi_{\theta}\) is a universal approximator of invariant or equivariant functions if \(f_{\theta}\) is a universal approximator [102],which includes an MLP [35] or a transformer [104]. On the other hand, an immediate challenge is that for many practical groups involving permutation and rotations, the cardinality of the group \(|G|\) is large or infinite, so the exact averaging is intractable. Due to this, existing symmetrization approaches often focus on small finite groups [4, 65, 94, 42], manually derive smaller subsets of the entire group _e.g._, a frame [74] to average over [67, 68], or implement a relaxed version of equivariance [41, 83].

An alternative method for tractability is to interpret Eq.1 as an expectation with uniform distribution \(\text{Unif}(G)\) over the compact group \(G\)[62], and use sampling-based average to estimate it [102, 67, 68]:

\[\phi_{\theta}(\mathbf{x})=\mathbb{E}_{g\sim\text{Unif}(G)}\left[g\cdot f_{ \theta}(g^{-1}\cdot\mathbf{x})\right],\] (2)

where \(g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x})\) serves as an unbiased estimator of \(\phi_{\theta}(\mathbf{x})\). While simple and general, this approach has practical issues that the base function \(f_{\theta}\) is burdened to learn all equally possible group transformations, and the expectedly high variance of the estimator can lead to challenges in sampling-based training due to large variance of gradients as well as sample complexity of inference.

Our key idea is to replace the uniform distribution \(\text{Unif}(G)\) for the expectation in Eq.2 with a parameterized distribution \(p_{\omega}(g|\mathbf{x})\) in a way that equivariance and expressive power are always guaranteed, and train it end-to-end with the base function \(f_{\theta}\) to directly minimize task loss. We show that the distribution \(p_{\omega}(g|\mathbf{x})\) only needs to satisfy one simple condition to guarantee equivariance and expressive power: it has to be probabilistically equivariant [9]. This allows us to generally implement \(p_{\omega}(g|\mathbf{x})\) as a noise-outsourced map \((\mathbf{x},\boldsymbol{\epsilon})\mapsto g\) with an invariant noise \(\boldsymbol{\epsilon}\) and a small equivariant network \(q_{\omega}\), which enables gradient-based training with reparameterization [47]. As \(p_{\omega}\) is trained, it can enhance the learning of \(f_{\theta}\) by producing group transformations of lower variance compared to \(\text{Unif}(G)\) so that \(f_{\theta}\) is less burdened, and coordinating with \(f_{\theta}\) to maximize task performance. We refer to our approach as probabilistic symmetrization. An overview is provided in Figure1.

We implement and test our method with two general-purpose architectures as the base function \(f_{\theta}\): MLP and transformer. In particular, our transformer backbone is architecturally identical to patch-based vision transformers [26], which allows us to initialize most of its parameters from ImageNet-21k pretrained weights [89] and only replace the input and output projections to match task dimensions. We implement the conditional distribution \(p_{\omega}(g|\mathbf{x})\) for a wide range of practical symmetry groups including permutation \((\text{S}_{n})\) and Euclidean groups \((\text{O}/\text{SO}(d)\) and \(\text{E}/\text{SE}(d))\) and their product combinations (_e.g._, \(\text{S}_{n}\times\text{O}(3)\)), all of which are combinatorial or infinite groups. Empirical tests on a wide range of invariant and equivariant tasks involving graphs and motion data show competitive results against tailored equivariant architectures as well as existing symmetrization methods [74, 41]. This suggests the potential for learning invariant or equivariant functions for diverse groups with a group-agnostic general-purpose backbone. We further show evidence that pretraining from non-symmetric modality (vision) leads to enhanced learning in symmetric modality (graphs).

Figure 1: Overview of probabilistic symmetrization. We symmetrize an unconstrained base function \(f_{\theta}\) into an equivariant function \(\phi_{\theta,\omega}\) for group \(G\) using a learned equivariant distribution \(p_{\omega}(g|\mathbf{x})\).

Probabilistic Symmetrization for Equivariance

We introduce and analyze our approach called probabilistic symmetrization, which involves an equivariant distribution \(p_{\omega}\) and group-agnostic base function \(f_{\theta}\), in Section 2.1. We then describe implementation of \(p_{\omega}\) for practical groups including permutations and rotations in Section 2.2. Then, we describe our choice of base function \(f_{\theta}\) focusing on MLP and, transformers in particular, in Section 2.3. All proofs can be found in Appendix A.1.

Problem SetupIn general, our goal is to construct a function \(\phi:\mathcal{X}\rightarrow\mathcal{Y}\) on finite vector spaces \(\mathcal{X},\mathcal{Y}\) that is invariant or equivariant to symmetry specified by a group \(G=\{g\}\)1. This is formally described by specifying how the group act as transformations on the input and output. A group representation \(\rho:G\rightarrow\text{GL}(\mathcal{X})\), where \(\text{GL}(\mathcal{X})\) is the set of all invertible matrices on \(\mathcal{X}\), associates each group element \(g\in G\) to an invertible matrix \(\rho(g)\) that transforms a given vector \(\mathbf{x}\in\mathcal{X}\) through \(\mathbf{x}\mapsto g\cdot\mathbf{x}=\rho(g)\mathbf{x}\). Given that, a function \(\phi:\mathcal{X}\rightarrow\mathcal{Y}\) is \(G\) equivariant if:

Footnote 1: In this paper, we assume the group \(G\) to be compact.

\[\phi(\rho_{1}(g)\mathbf{x})=\rho_{2}(g)\phi(\mathbf{x}),\quad\forall\mathbf{x }\in\mathcal{X},g\in G,\] (3)

where the representations \(\rho_{1}\) and \(\rho_{2}\) are on the input and output, respectively. \(G\) invariance is a special case of equivariance when the output representation is trivial, \(\rho_{2}(g)=\mathbf{I}\).

### Probabilistic Symmetrization

To construct a \(G\) equivariant function \(\phi_{\theta}\), group averaging symmetrizes an arbitrary base function \(f_{\theta}:\mathcal{X}\rightarrow\mathcal{Y}\) by taking expectation with uniform distribution over the group (Eq. (2)). Instead, we propose to use an input-conditional parameterized distribution \(p_{\omega}(g|\mathbf{x})\) and symmetrize \(f_{\theta}\) as follows:

\[\phi_{\theta,\omega}(\mathbf{x})=\mathbb{E}_{p_{\omega}(g|\mathbf{x})}\left[ \rho_{2}(g)f_{\theta}(\rho_{1}(g)^{-1}\mathbf{x})\right],\] (4)

where the distribution \(p_{\omega}(g|\mathbf{x})\) itself satisfies probabilistic \(G\) equivariance:

\[p_{\omega}(g|\mathbf{x})=p_{\omega}(g^{\prime}g|\rho_{1}(g^{\prime})\mathbf{ x}),\quad\forall\mathbf{x}\in\mathcal{X},g,g^{\prime}\in G.\] (5)

Importantly, we show that probabilistic symmetrization with equivariant \(p_{\omega}\) guarantees equivariance as well as expressive power of the symmetrized \(\phi_{\theta,\omega}\).

**Theorem 1**.: _If \(p_{\omega}\) is \(G\) equivariant, then \(\phi_{\theta,\omega}\) is \(G\) equivariant for arbitrary \(f_{\theta}\)._

Proof.: The proof can be found in Appendix A.1.1. 

**Theorem 2**.: _If \(p_{\omega}\) is \(G\) equivariant and \(f_{\theta}\) is a universal approximator, then \(\phi_{\theta,\omega}\) is a universal approximator of \(G\) equivariant functions._

Proof.: The proof can be found in Appendix A.1.2. 

While the base function \(f_{\theta}\) that guarantees universal approximation can be chosen in a group-agnostic manner _e.g._, an MLP [35, 20] or a transformer on token sequences [104], the distribution \(p_{\omega}\) needs to be instantiated group specifically to satisfy \(G\) equivariance. A simplistic choice is using uniform distribution \(\text{Unif}(G)\) for all inputs \(\mathbf{x}\) with no parameterization (reducing to group averaging), which is technically equivariant and therefore guarantees equivariance and universality. However, appropriately parameterizing and learning \(p_{\omega}\) can provide distinguished advantages compared to the uniform distribution, as it can **(1)** learn from data to collaborate with (pre-trained) base function \(f_{\theta}\) to maximize task performance and **(2)** learn to produce more consistent samples \(g\sim p_{\omega}(g|\mathbf{x})\) that can offer more stable gradients for the base function \(f_{\theta}\) during early training.

We now provide a generic blueprint of \(G\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) for any compact group \(G\). Our goal is to sample \(g\sim p_{\omega}(g|\mathbf{x})\) to obtain group representation \(\rho(g)\) for symmetrization (Eq. (4)) in a differentiable manner so that \(p_{\omega}\) can be trained end-to-end. Since we only need sampling and there is no need to evaluate likelihoods, we simply implement \(p_{\omega}(g|\mathbf{x})\) as a noise-outsourced, differentiable transformation \(q_{\omega}(\mathbf{x},\boldsymbol{\epsilon})\) of a noise variable \(\boldsymbol{\epsilon}\in\mathcal{E}\) that directly outputs a group representation \(\rho(g)\):

\[\rho(g)=q_{\omega}(\mathbf{x},\boldsymbol{\epsilon}),\quad\boldsymbol{ \epsilon}\sim p(\boldsymbol{\epsilon}),\] (6)where \(q_{\omega}\) is \(G\) equivariant and \(p(\bm{\epsilon})\) is \(G\) invariant under an appropriate representation \(\rho^{\prime}\):

\[q_{\omega}(\rho_{1}(g)\mathbf{x},\rho^{\prime}(g)\bm{\epsilon})=\rho(g)q_{ \omega}(\mathbf{x},\bm{\epsilon}),\quad p(\bm{\epsilon})=p(\rho^{\prime}(g)\bm {\epsilon}),\quad\forall\mathbf{x}\in\mathcal{X},\bm{\epsilon}\in\mathcal{E},g \in G.\] (7)

Given above implementation, we can show the \(G\) equivariance of \(p_{\omega}\):

**Theorem 3**.: _If \(q_{\omega}\) is \(G\) equivariant and \(p(\bm{\epsilon})\) is \(G\) invariant under representation \(\rho^{\prime}\) that \(|\det\rho^{\prime}(g)|=1\forall g\in G\), the distribution \(p_{\omega}(g|\mathbf{x})\) characterized by \(q_{\omega}:(\mathbf{x},\bm{\epsilon})\mapsto\rho(g)\) is \(G\) equivariant._

Proof.: The proof can be found in Appendix A.1.3. 

In practice, one can use any available \(G\) equivariant neural network to implement \(q_{\omega}\), _e.g._, a graph neural network for the symmetric group \(\mathsf{S}_{n}\), or an equivariant MLP which can be constructed for any matrix group [30]. Since we expect most of the reasoning to be done by the base function \(f_{\theta}\), the equivariant network \(q_{\omega}\) can be small and relatively less expressive. This allows us to get less affected by their known issues in expressiveness and scaling [101, 70, 14, 40]. For the noise \(\bm{\epsilon}\sim p(\bm{\epsilon})\), simple choices often suffice for \(G\) invariance. For example, standard normal \(\bm{\epsilon}\sim\mathcal{N}(0,\mathbf{I}_{n})\) provides invariance for the symmetric group \(\mathsf{S}_{n}\) as well as the (special) orthogonal group \(\mathsf{O}(n)\) and \(\mathsf{SO}(n)\).

One important detail in designing \(q_{\omega}\) is constraining its output to be a valid group representation \(\rho(g)\). For this, we apply appropriate postprocessing to refine neural network features into group representations, _e.g._, Gram-Schmidt orthogonalization to obtain a representation \(\rho(g)\in\mathbb{R}^{n\times n}\) of the orthogonal group \(g\in\mathsf{O}(n)\). Importantly, to not break the \(G\) equivariance of \(q_{\omega}\), this postprocessing needs to be equivariant itself, _e.g._, Gram-Schmidt process is itself \(\mathsf{O}(n)\) equivariant [41]. Implementations of \(p_{\omega}\) for a range of practical symmetry groups are provided in detail in Section 2.2.

### Equivariant Distribution \(p_{\omega}\)

We present implementations of the \(G\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) for a range of practical groups demonstrated in our experiments (Section 3). Formal proofs of correctness are in Appendix A.1.4.

Symmetric Group \(\mathsf{S}_{n}\)The symmetric group \(\mathsf{S}_{n}\) over a finite set of \(n\) elements contains all permutations of the set, which describes symmetry to ordering desired for learning set and graph data. The base representation is given by \(\rho(g)=\mathbf{P}_{g}\) where \(\mathbf{P}_{g}\in\{0,1\}^{n\times n}\) is a permutation matrix for \(g\).

To implement \(\mathsf{S}_{n}\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) that provides permutation matrices \(\mathbf{P}_{g}\) from graph data \(\mathbf{x}\), we use the following design. We first sample invariant noise \(\bm{\epsilon}\in\mathbb{R}^{n\times d}\) from i.i.d. uniform \(\text{Unif}[0,\eta]\) with noise scale \(\eta\). For the \(\mathsf{S}_{n}\) equivariant map \(q_{\omega}:(\mathbf{x},\bm{\epsilon})\mapsto\rho(g)\), we first use a graph neural network (GNN) as an equivariant map \((\mathbf{x},\bm{\epsilon})\mapsto\mathbf{Z}\) that outputs nodewise scalar \(\mathbf{Z}\in\mathbb{R}^{n}\). Then, assuming \(\mathbf{Z}\) is tie-free2, we use below argort operator \(\mathbf{Z}\mapsto\mathbf{P}_{g}\) to obtain permutation matrix [73, 98]:

Footnote 2: This can be assumed since the invariant noise \(\bm{\epsilon}\in\mathbb{R}^{n\times d}\) serves as tiebreaker between the \(n\) nodes.

\[\mathbf{P}_{g}=\text{eq}(\mathbf{Z}\mathbf{1}^{\top},\mathbf{1}\text{sort}( \mathbf{Z})^{\top}),\] (8)

where eq denotes elementwise equality indicator. The argsort operator is \(\mathsf{S}_{n}\) equivariant, _i.e._, it maps \(\mathbf{P}_{g^{\prime}}\mathbf{Z}\mapsto\mathbf{P}_{g^{\prime}}\mathbf{P}_{g}\) for all \(\mathbf{P}_{g^{\prime}}\in\mathsf{S}_{n}\). To backpropagate through \(\mathbf{P}_{g}\) during training, we use straight-through gradient estimator [6] with an approximate permutation matrix \(\hat{\mathbf{P}}_{g}\approx\mathbf{P}_{g}\) obtained from a differentiable relaxation of argsort operator [61, 33, 98]; details can be found in Appendix A.3.1.

Orthogonal Group \(\mathsf{O}(n)\), \(\mathsf{SO}(n)\)The orthogonal group \(\mathsf{O}(n)\) contains all roto-reflections in \(\mathbb{R}^{n}\) around origin, and the special orthogonal group \(\mathsf{SO}(n)\) contains all rotations without reflections. These groups describe rotation symmetries desirable in learning geometric data. The base group representation for \(\mathsf{O}(n)\) is given by \(\rho(g)=\mathbf{Q}_{g}\) where \(\mathbf{Q}_{g}\) is the orthogonal matrix for \(g\). For \(\mathsf{SO}(n)\), the representation is \(\rho(g)=\mathbf{Q}_{g}^{+}\) where \(\mathbf{Q}_{g}^{+}\) is the orthogonal matrix of \(g\) with determinant \(+1\).

To implement \(\mathsf{O}(n)\)/\(\mathsf{SO}(n)\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) that provides orthogonal matrices given input data \(\mathbf{x}\), we use the following design. We first sample invariant noise \(\bm{\epsilon}\in\mathbb{R}^{n\times d}\) from i.i.d. normal \(\mathcal{N}(0,\eta^{2})\) with noise scale \(\eta\). For the \(\mathsf{O}(n)\)/\(\mathsf{SO}(n)\) equivariant map \(p_{\omega}:(\mathbf{x},\bm{\epsilon})\mapsto\rho(g)\), we first use an equivariant neural network as a map \((\mathbf{x},\bm{\epsilon})\mapsto\mathbf{Z}\) that outputs \(n\) features \(\mathbf{Z}\in\mathbb{R}^{n\times n}\). Then, assuming \(\mathbf{Z}\) is full-rank3, we use Gram-Schmidt process for orthogonalization, which is differentiable and \(\mathbf{O}(n)\) equivariant [41]. This completes the postprocessing \(\mathbf{Z}\mapsto\mathbf{Q}_{g}\) for \(\mathbf{O}(n)\). For \(\mathbf{SO}(n)\), we can further use a simple scale operator \(\mathbf{Q}\mapsto\mathbf{Q}_{g}^{+}\) to set the determinant of orthogonalized matrix to \(+1\):

\[\text{scale}:\left[\begin{array}{c|c|c}\mathbf{Q}_{1}&...&\mathbf{Q}_{n}\\ \end{array}\right]\mapsto\left[\begin{array}{c|c|c}\text{det}(\mathbf{Q}) \cdot\mathbf{Q}_{1}&...&\mathbf{Q}_{n}\\ \end{array}\right],\] (9)

The scale operator is differentiable and \(\text{SO}(n)\) equivariant, _i.e._, it maps \(\mathbf{Q}_{g^{\prime}}^{+}\mathbf{Q}\mapsto\mathbf{Q}_{g^{\prime}}^{+}\mathbf{ Q}_{g}^{+}\) for all \(\mathbf{Q}_{g^{\prime}}^{+}\in\text{SO}(n)\), thereby completing the postprocessing \(\mathbf{Z}\mapsto\mathbf{Q}_{g}^{+}\) for \(\text{SO}(n)\).

Euclidean Group \(\mathrm{E}(n)\), \(\text{SE}(n)\)The Euclidean group \(\mathrm{E}(n)\) contains all roto-translations and reflections in \(\mathbb{R}^{n}\) and their combinations, and the special Euclidean group \(\text{SE}(n)\) contains all roto-translations without reflections. These groups are desired in learning physical systems such as a particle in motion. Formally, the Euclidean group is given as a combination of orthogonal group and translation group \(\text{E}(n)=\mathbf{O}(n)\ltimes\text{T}(n)\), and similarly \(\text{SE}(n)=\text{SO}(n)\ltimes\text{T}(n)\). As the translation group \(\text{T}(n)\) is non-compact which violates our assumption for symmetrization, we handle it separately. Following prior work [74; 41], we subtract the centroid \(\bar{\mathbf{x}}\) from the input data \(\mathbf{x}\) as \(\mathbf{x}-\bar{\mathbf{x}}\), and add it to the rotation symmetrized output as \(\bar{\mathbf{x}}+g\cdot f_{\theta}(g^{-1}\cdot(\mathbf{x}-\bar{\mathbf{x}}))\) where \(g\) is sampled from \(\mathbf{O}(n)/\text{SO}(n)\) equivariant \(p_{\omega}(g|\mathbf{x}-\bar{\mathbf{x}})\). This makes the overall symmetrized function \(\text{E}(n)/\text{SE}(n)\) equivariant.

Product Group \(H\times K\)While we have described several groups individually, in practice we often encounter product combinations of groups \(G=H\times K\) that describe joint symmetry to each group \(H\) and \(K\). For example, \(\text{S}_{n}\times\text{O}(3)\) describes joint symmetry to permutations and rotations, which is desired in learning point clouds, molecules, and particle interactions. In general, an element of \(H\times K\) is given as \(g=(h,k)\) where \(h\in H\) and \(k\in K\), and group operations are applied elementwise \(gg^{\prime}=(h,k)(h^{\prime},k^{\prime})=(hh^{\prime},kk^{\prime})\). The base group representation is accordingly given as pair of representations \(\rho(g)=(\rho(h),\rho(k))\). While a common approach to handling \(H\times K\) is _partially_ symmetrizing on \(H\) and imposing \(K\) equivariance on the base architecture (_e.g._, rotational symmetrization of a graph neural network for \(\text{S}_{n}\times\text{O}(3)\) equivariance [74; 41]), we extend to _full symmetrization_ on \(H\times K\) since our goal is not imposing any constraint on the base function \(f_{\theta}\).

To implement \(H\times K\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) that gives \(\rho(g)=(\rho(h),\rho(k))\) from data \(\mathbf{x}\), we use the following design. We first sample invariant noise \(\bm{\epsilon}\) from i.i.d. normal \(\mathcal{N}(0,\eta^{2})\) with scale \(\eta\). For the \(H\times K\) equivariant map \(q_{\omega}:(\mathbf{x},\bm{\epsilon})\mapsto(\rho(h),\rho(k))\), we employ a \(H\times K\) equivariant neural network as a map \((\mathbf{x},\bm{\epsilon})\mapsto(\mathbf{Z}_{H},\mathbf{Z}_{K})\) such that the postprocessing for each group \(H\) and \(K\) provides maps \(\mathbf{Z}_{H}\mapsto\rho(h)\) and \(\mathbf{Z}_{K}\mapsto\rho(k)\) respectively, leading to full representation \(\rho(g)=(\rho(h),\rho(k))\). For this whole procedure to be \(H\times K\) equivariant, it is sufficient to have \(\mathbf{Z}_{H}\) be \(K\) invariant and \(\mathbf{Z}_{K}\) be \(H\) invariant. These are special cases of \(H\times K\) equivariance, and is supported by a range of equivariant neural networks especially regarding \(\text{S}_{n}\times\text{O}(3)\) or \(\text{S}_{n}\times\text{SO}(3)\) equivariances [23].

### Base Function \(f_{\theta}\)

We now describe the choice of group-agnostic base function \(f_{\theta}:\mathbf{x}\mapsto\mathbf{y}\). As group symmetry is handled by the equivariant distribution \(p_{\omega}(g|\mathbf{x})\), any symmetry concern is _hidden_ from \(f_{\theta}\), allowing the inputs \(\mathbf{x}\) and outputs \(\mathbf{y}\) to be treated as plain multidimensional arrays. This allows us to implement \(f_{\theta}\) with powerful general-purpose architectures, namely as an MLP that operates on flattened vectors of inputs and outputs, or a transformer as we describe below.

Let inputs \(\mathbf{x}\in\mathcal{X}=\mathbb{R}^{n_{1}\times...\times n_{n}\times c}\) and outputs \(\mathbf{y}\in\mathcal{Y}=\mathbb{R}^{n^{\prime}_{1}\times...\times n^{\prime}_{ 1}\times c^{\prime}}\) be multidimensional arrays with \(c\) and \(c^{\prime}\) channels, respectively. Our transformer base function \(f_{\theta}:\mathbf{x}\mapsto\mathbf{y}\) is given as:

\[f_{\theta}=\text{detokenize}\circ\text{transformer}\circ\text{tokenize},\] (10)

where \(\text{tokenize}:\mathcal{X}\rightarrow\mathbb{R}^{m\times d}\) parses input array to a sequence of \(m\) tokens, \(\text{transformer}:\mathbb{R}^{m\times d}\rightarrow\mathbb{R}^{m\times d}\) is a standard transformer encoder on tokens used in language and vision [24; 26], and detokenize \(:\mathbb{R}^{m\times d}\rightarrow\mathcal{Y}\) decodes encoded tokens to output array. For the tokenizer and detokenizer, we can use linear projections on flattened chunks of the array, which directly extends flattened patch projections in vision transformers to higher dimensions [26; 85; 86]. This enables mapping between different dimensional inputs and outputs, _e.g_, for graph node classification with \(\mathbf{x}\in\mathbb{R}^{n\times n\times c}\) and \(\mathbf{y}\in\mathbb{R}^{n\times c^{\prime}}\), it is possible to use 2D patch projection for the input and 1D projection for the output.

Above choice of \(f_{\theta}\) offers important advantages including universal approximation [104] and ability to share and transfer the learned knowledge in \(\theta\) over different domains of different group symmetries. Remarkably, this allows us to directly leverage large-scale pre-trained parameters from data-abundant domains for learning on symmetric domains. In our experiments, we only replace the tokenizer and detokenizer of a vision transformer pre-trained on ImageNet-21k [99; 26] and fine-tune it to perform diverse \(\mathsf{S}_{n}\) equivariant tasks such as graph classification, node classification, and link prediction.

### Relation to Other Symmetrization Approaches

We discuss relation of our symmetrization method to prior ones, specifically group averaging [102; 4], frame averaging [74], and canonicalization [41]. An extended discussion on broader related work can be found in Appendix A.2. Since these symmetrization methods share common formalization \(\mathbf{y}=\mathbb{E}_{g}[g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x})]\), one can expect a close theoretical relationship between them. We observe that probabilistic symmetrization is quite general; based on particular choices of the \(G\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\), it can become most of the related symmetrization methods as special cases. This can be easily seen for group averaging [102], as the distribution \(p_{\omega}\) can reduce to the uniform distribution \(\text{Unif}(G)\) over the group. Frame averaging [74] also takes an average, but over a subset of group given by a frame \(F:\mathcal{X}\to 2^{G}\setminus\emptyset\); importantly, it is required that the frame itself is \(G\) equivariant \(F(\rho(g)\mathbf{x})=gF(\mathbf{x})\). We can make the following connection between our method and frame averaging, by adopting the concept of stabilizer subgroup \(G_{\mathbf{x}}=\{g\in G:\rho(g)\mathbf{x}=\mathbf{x}\}\):

**Proposition 1**.: _Probabilistic symmetrization with \(G\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) can become frame averaging [74] by assigning uniform density to a set of orbits \(G_{\mathbf{x}}g\) for some group elements \(g\)._

Proof.: The proof can be found in Appendix A.1.5. 

Canonicalization [41] uses a _single_ group element for symmetrization, produced by a trainable canonicalizer \(C_{\omega}:\mathcal{X}\to G\). Here, it is required that the canonicalizer itself satisfies _relaxed_\(G\) equivariance \(C(\rho(g)\mathbf{x})=gg^{\prime}C(\mathbf{x})\) up to arbitrary action from the stabilizer \(g^{\prime}\in G_{\mathbf{x}}\). We now show:

**Proposition 2**.: _Probabilistic symmetrization with \(G\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) can become canonicalization [41] by assigning uniform density to a single orbit \(G_{\mathbf{x}}g\) of some group element \(g\)._

Proof.: The proof can be found in Appendix A.1.5. 

Assuming that stabilizer \(G_{\mathbf{x}}\) is trivial, this can be implemented with our method by removing random noise \(\bm{\epsilon}\), which reduces \(p_{\omega}\) to deterministic map \(\rho(g)=q_{\omega}(\mathbf{x})\). We use this approach to implement canonicalizer for the \(\mathsf{S}_{n}\) group, while [41] only provides canonicalizers for Euclidean groups.

## 3 Experiments

We empirically demonstrate and analyze probabilistic symmetrization on a range of symmetry groups \(\mathsf{S}_{n}\), \(\mathsf{E}(3)\) (\(\mathsf{O}(3)\)), and the product \(\mathsf{S}_{n}\times\mathsf{E}(3)\) (\(\mathsf{S}_{n}\times\mathsf{O}(3)\)), on a variety of invariant and equivariant tasks, with general-purpose base functions \(f_{\theta}\) chosen as MLP and transformer optionally with pretraining from vision domain. Details of the experiments are in Appendix A.3, and supplementary experiments on other base functions and comparisons to other symmetrization approaches are in Appendix A.4.

### Graph Isomorphism Learning with MLP

Building expressive neural networks for graphs (\(\mathsf{S}_{n}\)) has been considered important and challenging, as simple and efficient GNNs are often limited in expressive power to certain Weisfeiler-Lehman isomorphism tests like 1-WL [101; 56]. Since an MLP equipped with probabilistic symmetrization is in theory universal and \(\mathsf{S}_{n}\) equivariant, it has potential for graph learning that require high expressive power. To explicitly test this, we adopt the experimental setup of [74] and use two datasets on graph separation task (\(\mathsf{S}_{n}\) invariant). GRAPH8c [2] consists of all non-isomorphic connected graphs with 8 nodes, and EXP [1] consists of 3-WL distinguishable graphs that are not 2-WL distinguishable. We compare our method to standard GNNs as well as an MLP symmetrized with group averaging [102], frame averaging [74], and canonicalization [41]. Our method uses the same MLP architecture to symmetrization baselines, and its \(\mathsf{S}_{n}\) equivariant distribution \(p_{\omega}\) for symmetrization is implementedusing a 3-layer GIN [101] which is 1-WL expressive. We use 10 samples for symmetrization during both training and testing. Further details can be found in Appendix A.3.3, and supplementary results on symmetrization of a different base model can be found in Appendix A.4.1.

The results are in Table 1. At random initialization, all symmetrization methods can provide perfect separation of all graphs, similar to PPGN [56] and GNNML3 [2] that are equivariant neural networks carefully designed to be 3-WL expressive. However, when trained with gradient descent to solve classification problem, naive symmetrization with group averaging fails, presumably because the MLP fails to adjust to equally possible \(64!\) permutations of \(64\) nodes in maximum. On the other hand, our method is able to learn the task, achieving the same accuracy to frame averaging that utilizes costly eigendecomposition of graph Laplacian [74]. What makes our method work while group averaging fails? We conjecture this is since the distribution \(p_{\omega}(g|\mathbf{x})\) can learn to provide more consistent permutations during early training, as we illustrate in Figure 2. In the figure, we measured the consistency of samples from \(p_{\omega}(g|\mathbf{x})\) over training progress by sampling \(N=50\) permutation matrices \(\mathbf{P}_{g}\sim p_{\omega}(g|\mathbf{x})\) and measuring the row-wise entropy of their average \(\bar{\mathbf{P}}=\sum\mathbf{P}_{g}/N\) for each input \(\mathbf{x}\). The more consistent the sampled permutations, the sharper their average, and lesser the entropy. As training progresses, \(p_{\omega}\) learns to produce more consistent samples, which coincides with the initial increase in task performance. Given that, a natural question would be: if we enforce the samples \(g\sim p_{\omega}(g|\mathbf{x})\) to be consistent from the first place, would it work? To answer this, we also tested a non-probabilistic version of our model that uses a single permutation per input \(\rho(g)=q_{\omega}(\mathbf{x})\), which is a canonicalizer under relaxed equivariance [41] as described in Section 2.4. As in Table 1, canonicalization fails, suggesting that probabilistic nature of \(p_{\omega}(g|\mathbf{x})\) can be _beneficial_ for learning. For another deterministic version of our model made by fixing the noise \(\boldsymbol{\epsilon}\) (Eq. (6)) at initialization, the performance drops to 79.5%, further implying that stochasticity of \(p_{\omega}(g|\mathbf{x})\) has a role.

\begin{table}
\begin{tabular}{l c c c c c} \hline method & arch. & sym. & GRAPHSc \(\downarrow\) & EXP \(\downarrow\) & EXP-classify \(\uparrow\) \\ GCN [48] & S\({}_{n}\) & - & 4755 & 600 & 50\% \\ GAT [97] & S\({}_{n}\) & - & 1828 & 600 & 50\% \\ GIN [101] & S\({}_{n}\) & - & 386 & 600 & 50\% \\ CheNet [22] & S\({}_{n}\) & - & 44 & 71 & 82\% \\ \hline PPGN [56] & S\({}_{n}\) & - & 0 & 0 & **100\%** \\ GNNML3 [2] & S\({}_{n}\) & - & 0 & 0 & **100\%** \\ \hline MLP-GA [102] & - & S\({}_{n}\) & 0 & 0 & 50\% \\ MLP-PA [74] & - & S\({}_{n}\) & 0 & 0 & **100\%** \\ MLP-Cnnical. & - & S\({}_{n}\) & 0 & 0 & 50\% \\ MLP-PS (Ours), fixed \(\boldsymbol{\epsilon}\) & - & S\({}_{n}\) & 0 & 0 & 79.5\% \\ MLP-PS (Ours) & - & S\({}_{n}\) & 0 & 0 & **100\%** \\ \hline \end{tabular}
\end{table}
Table 1: Results for S\({}_{n}\) invariant graph separation. We use two tasks, one for counting pairs of graphs not separated by a model at random initialization (GRAPH8c and EXP), and one for learning to classify EXP to two classes (EXP-classify). For EXP-classify, we report the test accuracy at best validation accuracy. The columns arch. and sym. denote architectural and symmetrized equivariance, respectively. The results for baselines are from [74] except for MLP-Canonical. which is tested by us.

### Particle Dynamics Learning with Transformer

Learning sets or graphs attributed with position and velocity in 3D (\(\text{S}_{n}\times\text{E}(3)\)) is practically significant as they universally appear in physics, chemistry, and biology applications. While prior symmetrization methods employ an already \(\text{S}_{n}\) equivariant base function and partially symmetrize the \(\text{E}(3)\) part, we attempt to symmetrize the entire product group \(\text{S}_{n}\times\text{E}(3)\) and choose the base model \(f_{\theta}\) as a sequence transformer to leverage its expressive power. For empirical demonstration, we adopt the experimental setup of [41] and use the \(n\)-body dataset [84; 31] where the task is predicting the position of \(n=5\) charged particles after certain time given their initial position and velocity in \(\mathbb{R}^{3}\) (\(\text{S}_{n}\times\text{E}(3)\) equivariant). We compare our method to \(\text{S}_{n}\times\text{E}(3)\) equivariant neural networks and partial symmetrization methods applying \(\text{E}(3)\) symmetrization to GNNs. We also test prior symmetrization methods on the full group \(\text{S}_{n}\times\text{E}(3)\) along our method, but could not test for frame averaging since equivariant frames for the full group \(\text{S}_{n}\times\text{E}(3)\) was not available in current literature. Our method is implemented using a transformer with sequence positional encodings with around 2.3\(\times\) parameters of the baselines, and the \(\text{S}_{n}\times\text{E}(3)\) equivariant distribution \(p_{\omega}\) for symmetrization is implemented using a 2-layer Vector Neurons [23] that has around 0.03\(\times\) of parameters to the transformer. We use 20 samples for symmetrization during training, and use 10\(\times\) sample size for testing since the task is regression where appropriate variance reduction is necessary to guarantee a reliable performance. Further details can be found in Appendix A.3.4, and supplementary results on \(\text{E}(3)\) partial symmetrization of GNN base model can be found in Appendix A.4.2.

The results are in Table 2. We observe simple group averaging exhibits a surprisingly strong performance, as it achieves 0.00414 MSE and already outperforms previous state of the art 0.0043 MSE. This is because the permutation component of the symmetry is fairly small, with \(n=5\) particles interacting with each other, such that combining it with an expressive base model \(f_{\theta}\) (a sequence transformer) can adjust to \(5!=120\) equally possible permutations and their rotations in 3D. Nevertheless, our method outperforms group averaging and achieves a new state of the art 0.00401 MSE, presumably as the parameterized distribution \(p_{\omega}\) learns to further maximize task performance. On the other hand, the canonicalization approach, implemented by eliminating noise variable \(\epsilon\) from our method (Section 2.4), performs relatively poorly. We empirically observe that \(f_{\theta}\) memorizes the per-input canonical orientations provided by \(\rho(g)=q_{\omega}(\mathbf{x})\) that do not generalize to test inputs. This again shows that probabilistic nature of \(p_{\omega}(g|\mathbf{x})\) can be beneficial for performance.

### Graph Pattern Recognition with Vision Transformer

One important goal of our approach, and symmetrization in general, is to _decouple_ the symmetry of problem from the base function \(f_{\theta}\), such that we can leverage knowledge learned from other symmetries by transferring the parameters \(\theta\). We demonstrate an extreme case by transferring the parameters of a vision transformer [26] trained on large-scale image classification (translation invariant) to solve node classification on graphs (\(\text{S}_{n}\) equivariant) for the first time in literature. For this, we use the PATTERN dataset [27] that contains 14,000 purely topological random SBM graphs with 44-188 nodes, whose task is finding certain subgraph pattern by binary node classification.

\begin{table}
\begin{tabular}{l l l l} \hline method & arch. & sym. & Position MSE \(\downarrow\) \\ \hline SE(3) Transformer [31] & \(\text{S}_{n}\times\text{SE}(3)\) & - & 0.0244 \\ TFN [91] & \(\text{S}_{n}\times\text{SE}(3)\) & - & 0.0155 \\ Radial Field [49] & \(\text{S}_{n}\times\text{E}(3)\) & - & 0.0104 \\ EGNN [84] & \(\text{S}_{n}\times\text{E}(3)\) & - & 0.0071 \\ \hline GNN-FA [74] & \(\text{S}_{n}\) & \(\text{E}(3)\) & 0.0057 \\ GNN-Canonical. [41] & \(\text{S}_{n}\) & \(\text{E}(3)\) & 0.0043 \\ \hline Transformer-Canonical. & - & \(\text{S}_{n}\times\text{E}(3)\) & 0.00508 \\ Transformer-GA & - & \(\text{S}_{n}\times\text{E}(3)\) & 0.00414 \(\pm\) 0.00001 \\ Transformer-PS (Ours) & - & \(\text{S}_{n}\times\text{E}(3)\) & **0.00401 \(\pm\) 0.00001** \\ \hline \end{tabular}
\end{table}
Table 2: Results for \(\text{S}_{n}\times\text{E}(3)\) equivariant \(n\)-body problem. The columns arch. and sym. denote architectural and symmetrized equivariance, respectively. We report test MSE at best validation MSE, along with the standard deviation for GA and Ours where predictions are stochastic. The results for baselines are from [41] except symmetrized transformers which are tested by us.

Based on pre-trained ViT [26; 89], we construct the base model \(f_{\theta}\) by modifying only the input and output layers to take the flattened patches of 2D zero-padded adjacency matrices of size 188\(\times\)188 and produce output as 1D per-node classification logits of length 188 with 2 channels. In addition to standard GNNs4, we compare group averaging, frame averaging, and canonicalization to our method, and also test whether pre-trained representations from ImageNet-21k is beneficial for the task. For the \(\text{S}_{n}\) equivariant distribution \(p_{\omega}\) in our method and canonicalization, we use a 3-layer GIN [101] with only 0.02% of the base model parameters. Further details can be found in Appendix A.3.5.

Footnote 4: We note that careful engineering such as graph positional encoding improves performance of GNNs, but we have chosen simple and representative GNNs in the benchmark [27] to provide a controlled comparison.

The results are in Table 3. First, we observe that transferring pre-trained ViT parameters consistently improves node classification for all symmetrization methods. It indicates that some traits of the pre-trained visual representation can benefit learning graph tasks which vastly differ in both the underlying symmetry (translation invariance \(\rightarrow\text{S}_{n}\) equivariance) and the data generating process (natural images \(\rightarrow\) random process of SBM). In particular, it is somewhat surprising that vision pretraining allows group averaging to achieve 84.351% accuracy, on par with GNN baselines, considering that memorizing all 188! equally possible permutations in this dataset is impossible. We conjecture that group averaged ViT can in some way learn meaningful graph representation internally to solve the task, and vision pretraining helps in acquiring the representation by providing a good initialization point or transferable computation motifs.

On the other hand, frame averaging shows a low performance, 79.637% accuracy with vision pretraining, which is also surprising considering that frames vastly reduce the sample space of symmetrization in general; in fact, the size of frame of each graph in PATTERN is exactly 1. We empirically observe that, unlike group averaging, ViT with frame averaging memorizes frames of training graphs rather than learning generalizable graph representations. In contrast, canonicalization that also uses a single sample per graph successfully learns the task with 86.166% accuracy. We conjecture that the learnable orderings provided by an equivariant neural network \(\rho(g)=q_{\omega}(\mathbf{x})\) is more flexible and generalizable to unseen graphs compared to frames computed from fixed graph Laplacian eigenvectors. Lastly, our method achieves a better performance compared to other symmetrization methods, and the performance consistently improves with vision pretraining and more samples for testing. As a result, our model based on pre-trained ViT and 10 samples for testing achieves 86.285% test accuracy, surpassing all baselines.

\begin{table}
\begin{tabular}{l c c} \hline \hline method & pretrain. & Accuracy \(\uparrow\) \\ \hline GCN [48], 16 layers & - & 85.614 \\ GAT [97], 16 layers & - & 78.271 \\ GatedGCN [11], 16 layers & - & 85.568 \\ GIN [101], 16 layers & - & 85.387 \\ RingGANN [16], 2 layers & - & 86.245 \\ RingGNN [16], 8 layers & - & diverged \\ PPGN [56], 3 layers & - & 85.661 \\ PPGN [56], 8 layers & - & diverged \\ \hline ViT-GA, 1-sample & - & 76.776 \(\pm\) 0.137 \\ ViT-GA, 10-sample & - & 83.119 \(\pm\) 0.048 \\ ViT-GA, 1-sample & ImageNet-21k & 81.407 \(\pm\) 0.101 \\ ViT-GA, 10-sample & ImageNet-21k & 84.351 \(\pm\) 0.053 \\ \hline ViT-FA & - & 70.063 \\ ViT-FA & ImageNet-21k & 79.637 \\ \hline ViT-Canonical. & - & 85.460 \\ ViT-Canonical. & ImageNet-21k & 86.166 \\ \hline ViT-PS (Ours), 1-sample & - & 85.542 \(\pm\) 0.012 \\ ViT-PS (Ours), 10-sample & - & 85.635 \(\pm\) 0.021 \\ ViT-PS (Ours), 1-sample & ImageNet-21k & 86.226 \(\pm\) 0.028 \\ ViT-PS (Ours), 10-sample & ImageNet-21k & **86.285 \(\pm\) 0.015** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results for \(\text{S}_{n}\) equivariant node classification on PATTERN. We report test accuracy at the best validation accuracy, along with the standard deviation for GA and Ours where predictions are stochastic. The results for GNN baselines are from [27].

### Real-World Graph Learning with Vision Transformer

Having observed that pre-trained ViT can learn graph tasks well when symmetrized with our method, we now provide a preliminary test of it in real-world graph learning. We use three real-world graph datasets from [28] that involve chemical and biological graphs. PCQM-Contact dataset contains 529,434 molecular graphs with 53 nodes in maximum, and the task is contact map prediction framed as link prediction (S\({}_{n}\) equivariant), on whether two atoms would be proximal when the molecule is in 3D space. Peptides-func and Peptides-struct are based on the same set of 15,535 protein graphs with 444 nodes in maximum and the tasks are property prediction (S\({}_{n}\) invariant), requiring multi-label classification for Peptides-func and regression for Peptides-struct. The tasks require complex understanding of how the amino acids of the proteins would interact in 3D space. We implement our method using a ViT-Base pre-trained on ImageNet-21k as the base model \(f_{\theta}\) and a 3-layer GIN as the equivariant distribution \(p_{\omega}(g|\mathbf{x})\), following our model in Section 3.3. We use 10 samples for both training and testing. Further details can be found in Appendix A.3.6.

The results are in Table 4. In Peptides-func and PCQM-Contact, the pre-trained ViT symmetrized with our method achieves a significantly higher performance compared to both GNNs and graph transformers, improving previous best by a large margin5 (0.6527 \(\rightarrow\) 0.8311 for Peptides-func AP, 0.1355 \(\rightarrow\) 0.3268 for PCQM-Contact Hits@1). This demonstrates the scalability of our method as Peptides-func involves 444 maximum nodes, and also its generality as it performs well for both S\({}_{n}\) invariant (Peptides-func) and equivariant (PCQM-Contact) tasks. We also note that, unlike some baselines, our method does not require costly Laplacian eigenvectors to compute positional encoding. On the Peptides-struct, our method achieves a slightly lower performance to SOTA graph transformers while still better than GNNs. We conjecture that regression is harder for the model to learn due to its stochasticity in predictions, and leave improving regression performance as future work.

Footnote 5: We note that the baseline architectures are constructed within 500k parameter budget as a convention [28], while we use an identical architecture to ViT-Base to leverage pre-trained representations.

## 4 Conclusion

We presented probabilistic symmetrization, a general framework that learns a distribution of group transformations conditioned on input data for symmetrization of an arbitrary function. By characterizing that the only condition for such distribution is equivariance to data symmetry, we instantiated models for a wide range of groups, including symmetric, orthogonal, Euclidean groups and their product combinations. Our experiments demonstrated that the proposed framework achieves consistent improvement over other symmetrization methods, and is competitive or outperforms equivariant networks on various datasets. We also showed that transferring pre-trained parameters across data in different symmetries can sometimes be surprisingly beneficial. Our approach has weaknesses such as sampling cost, which we further discuss in Appendix A.5; we plan to address these in future work.

AcknowledgementsThis work was supported in part by the National Research Foundation of Korea (NRF2021R1C1C1012540 and NRF2021R1A4A3032834) and IITP grant (2021-0-00537, 2019-0-00075, and 2021-0-02068) funded by the Korea government (MSIT).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline method & Peptides-func & Peptides-struct & \multicolumn{4}{c}{PCQM-Contact} \\ \hline  & AP \(\uparrow\) & MAE \(\downarrow\) & Hits@1 \(\uparrow\) & Hits@3 \(\uparrow\) & Hits@10 \(\uparrow\) & MRR \(\uparrow\) \\ \hline GCN [48] & 0.5930 & 0.3496 & 0.1321 & 0.3791 & 0.8256 & 0.3234 \\ GCNII [15] & 0.5543 & 0.3471 & 0.1325 & 0.3607 & 0.8116 & 0.3161 \\ GINE [36] & 0.5498 & 0.3547 & 0.1337 & 0.3642 & 0.8147 & 0.3180 \\ GatedGCN [11] & 0.5864 & 0.3420 & 0.1279 & 0.3783 & 0.8433 & 0.3218 \\ GatedGCN+RWSE [11] & 0.6069 & 0.3357 & 0.1288 & 0.3808 & 0.8517 & 0.3242 \\ \hline Transformer+LapPE [28] & 0.6326 & 0.2529 & 0.1221 & 0.3679 & 0.8517 & 0.3174 \\ SAN+LapPE [50] & 0.6384 & 0.2683 & 0.1355 & 0.4004 & 0.8478 & 0.3350 \\ SAN+RWSE [50] & 0.6439 & 0.2545 & 0.1312 & 0.4030 & 0.8550 & 0.3341 \\ GraphGPS [76] & 0.6535 & 0.2500 & - & - & - & 0.3337 \\ Explorer [87] & 0.6527 & **0.2481** & - & - & - & 0.3637 \\ \hline ViT-PS (Ours) & **0.8311** & 0.2662 & **0.3268** & **0.6693** & **0.9524** & **0.5329** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results for real-world graph tasks. We report test performance at best validation performance.

## References

* [1] R. Abboud, I. I. Ceylan, M. Grohe, and T. Lukasiewicz. The surprising power of graph neural networks with random node initialization. In _IJCAI_, 2021. (Cited on 6, 23)
* [2] M. Balcilar, P. Heroux, B. Gauzere, P. Vasseur, S. Adam, and P. Honeine. Breaking the limits of message passing graph neural networks. In _ICML_, 2021. (Cited on 6, 7, 23)
* [3] S. Basu, P. Kattare, P. Sattigeri, V. Chenthamarakshan, K. Driggs-Campbell, P. Das, and L. R. Varshney. Equivariant few-shot learning from pretrained models. _arXiv_, 2023. (Cited on 22)
* [4] S. Basu, P. Sattigeri, K. N. Ramamurthy, V. Chenthamarakshan, K. R. Varshney, L. R. Varshney, and P. Das. Equi-tuning: Group equivariant fine-tuning of pretrained models. _arXiv_, 2022. (Cited on 2, 6, 22)
* [5] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. F. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, H. F. Song, A. J. Ballard, J. Gilmer, G. E. Dahl, A. Vaswani, K. R. Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. M. Botvinick, O. Vinyals, Y. Li, and R. Pascanu. Relational inductive biases, deep learning, and graph networks. _arXiv_, 2018. (Cited on 1, 22)
* [6] Y. Bengio, N. Leonard, and A. C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. _arXiv_, abs/1308.3432, 2013. (Cited on 4, 24)
* [7] G. W. Benton, M. Finzi, P. Izmailov, and A. G. Wilson. Learning invariances in neural networks from training data. In _NeurIPS_, 2020. (Cited on 22, 23)
* [8] B. Bevilacqua, F. Frasca, D. Lim, B. Srinivasan, C. Cai, G. Balamurugan, M. M. Bronstein, and H. Maron. Equivariant subgraph aggregation networks. In _ICLR_, 2022. (Cited on 22)
* [9] B. Bloem-Reddy and Y. W. Teh. Probabilistic symmetries and invariant neural networks. _J. Mach. Learn. Res._, 2020. (Cited on 2)
* [10] A. Bogatskiy, S. Ganguly, T. Kipf, R. Kondor, D. W. Miller, D. Murnane, J. T. Offermann, M. Pettee, P. Shanahan, C. Shimmin, and S. Thais. Symmetry group equivariant architectures for physics. _arXiv_, 2022. (Cited on 1, 22)
* [11] X. Bresson and T. Laurent. Residual gated graph convnets. _arXiv_, 2019. (Cited on 9, 10)
* [12] M. M. Bronstein, J. Bruna, T. Cohen, and P. Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv_, 2021. (Cited on 1, 22)
* [13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In _NeurIPS_, 2020. (Cited on 22)
* [14] C. Cai and Y. Wang. A note on over-smoothing for graph neural networks. _arXiv_, 2020. (Cited on 4, 22)
* [15] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li. Simple and deep graph convolutional networks. In _ICML_, 2020. (Cited on 10)
* [16] Z. Chen, S. Villar, L. Chen, and J. Bruna. On the equivalence between graph isomorphism testing and function approximation with gnns. In _NeurIPS_, 2019. (Cited on 9)
* [17] E. Chien, C. Pan, J. Peng, and O. Milenkovic. You are allset: A multiset function framework for hypergraph neural networks. In _ICLR_, 2022. (Cited on 22)
* [18] T. Cohen and M. Welling. Group equivariant convolutional networks. In _ICML_, 2016. (Cited on 22)
* [19] T. S. Cohen and M. Welling. Steerable cnns. In _ICLR_, 2017. (Cited on 22)
* [20] G. Cybenko. Approximation by superpositions of a sigmoidal function. _Math. Control. Signals Syst._, 1989. (Cited on 3, 22)
* [21] G. Dasoulas, L. D. Santos, K. Scaman, and A. Virmaux. Coloring graph neural networks for node disambiguation. In _IJCAI_, 2020. (Cited on 22)
* [22] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In _NeurIPS_, 2016. (Cited on 7)
* [23] C. Deng, O. Litany, Y. Duan, A. Poulenard, A. Tagliasacchi, and L. J. Guibas. Vector neurons: A general framework for so(3)-equivariant networks. In _ICCV_, 2021. (Cited on 1, 5, 8, 22, 24)
* [24] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT_, 2019. (Cited on 5, 22)
* [25] T. Dinh, Y. Zeng, R. Zhang, Z. Lin, M. Gira, S. Rajput, J. Sohn, D. S. Papailiopoulos, and K. Lee. LIFT: language-interfaced fine-tuning for non-language machine learning tasks. In _NeurIPS_, 2022. (Cited on 22)* [26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [27] V. P. Dwivedi, C. K. Joshi, T. Laurent, Y. Bengio, and X. Bresson. Benchmarking graph neural networks. _arXiv_, 2020.
* [28] V. P. Dwivedi, L. Rampasek, M. Galkin, A. Parviz, G. Wolf, A. T. Luu, and D. Beaini. Long range graph benchmark. In _NeurIPS_, 2022.
* [29] W. Falcon. Pytorch lightning. https://github.com/Lightning-AI/lightning, 2019.
* [30] M. Finzi, M. Welling, and A. G. Wilson. A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. In _ICML_, 2021.
* [31] F. Fuchs, D. E. Worrall, V. Fischer, and M. Welling. Se(3)-transformers: 3d roto-translation equivariant attention networks. In _NeurIPS_, 2020.
* [32] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In _ICML_, 2017.
* [33] A. Grover, E. Wang, A. Zweig, and S. Ermon. Stochastic optimization of sorting networks via continuous relaxations. In _ICLR_, 2019.
* [34] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). _arXiv_, 2018.
* [35] K. Hornik, M. B. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. _Neural Networks_, 1989.
* [36] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. S. Pande, and J. Leskovec. Strategies for pre-training graph neural networks. In _ICLR_, 2020.
* [37] A. Immer, T. F. A. van der Ouderaa, G. Ratsch, V. Fortuin, and M. van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations. In _NeurIPS_, 2022.
* [38] A. Jaegle, S. Borgeaud, J. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, O. J. Henaff, M. M. Botvinick, A. Zisserman, O. Vinyals, and J. Carreira. Perceiver IO: A general architecture for structured inputs & outputs. In _ICLR_, 2022.
* [39] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General perception with iterative attention. In _ICML_, 2021.
* [40] C. K. Joshi, C. Bodnar, S. V. Mathis, T. Cohen, and P. Lio. On the expressive power of geometric graph neural networks. _arXiv_, 2023.
* [41] S. Kaba, A. K. Mondal, Y. Zhang, Y. Bengio, and S. Ravanbakhsh. Equivariance with learned canonicalization functions. _arXiv_, 2022.
* [42] P. Kicki, P. Skrzypczynski, and M. Ozay. A new approach to design symmetry invariant neural networks. In _IJCNN_, 2021.
* [43] J. Kim, D. Nguyen, S. Min, S. Cho, M. Lee, H. Lee, and S. Hong. Pure transformers are powerful graph learners. In _NeurIPS_, 2022.
* [44] J. Kim, S. Oh, S. Cho, and S. Hong. Equivariant hypergraph neural networks. In _ECCV_, 2022.
* [45] J. Kim, S. Oh, and S. Hong. Transformers generalize deepsets and can be extended to graphs and hypergraphs. In _NeurIPS_, 2021.
* [46] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* [47] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In _ICLR_, 2014.
* [48] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In _ICLR_, 2017.
* [49] J. Kohler, L. Klein, and F. Noe. Equivariant flows: Sampling configurations for multi-body systems with symmetric energies. _arXiv_, 2019.
* [50] D. Kreuzer, D. Beaini, W. L. Hamilton, V. Letourneau, and P. Tossou. Rethinking graph transformers with spectral attention. _NeurIPS_, 2021.
* [51] J. Lee, Y. Lee, J. Kim, A. R. Kosiorek, S. Choi, and Y. W. Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In _ICML_, 2019.
* [52] J. Li, D. Li, S. Savarese, and S. C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv_, 2023.
* [53] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In _ICLR_, 2019.
** [54] K. Lu, A. Grover, P. Abbeel, and I. Mordatch. Frozen pretrained transformers as universal computation engines. In _AAAI_, 2022.
* [55] S. Luo, T. Chen, Y. Xu, S. Zheng, T. Liu, D. He, and L. Wang. One transformer can understand both 2d & 3d molecular data. _arXiv_, 2022.
* [56] H. Maron, H. Ben-Hamu, H. Serviansky, and Y. Lipman. Provably powerful graph networks. In _NeurIPS_, 2019.
* [57] H. Maron, H. Ben-Hamu, N. Shamir, and Y. Lipman. Invariant and equivariant graph networks. In _ICLR_, 2019.
* [58] H. Maron, E. Fetaya, N. Segol, and Y. Lipman. On the universality of invariant networks. In _ICML_, 2019.
* [59] H. Maron, O. Litany, G. Chechik, and E. Fetaya. On learning sets of symmetric elements. In _ICML_, 2020.
* [60] B. D. McKay, M. A. Yirik, and C. Steinbeck. Surge: a fast open-source chemical graph generator. _J. Cheminformatics_, 2022.
* [61] G. E. Mena, D. Belanger, S. W. Linderman, and J. Snoek. Learning latent permutations with gumbel-sinkhorn networks. In _ICLR_, 2018.
* [62] F. Mezzadri. How to generate random matrices from the classical compact groups. _Notices of the American Mathematical Society_, 2006.
* [63] E. Min, R. Chen, Y. Bian, T. Xu, K. Zhao, W. Huang, P. Zhao, J. Huang, S. Ananiadou, and Y. Rong. Transformer for graphs: An overview from architecture perspective. _arXiv_, 2022.
* [64] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In _AAAI_, 2019.
* [65] S. C. Mouli and B. Ribeiro. Neural networks for learning counterfactual g-invariances from single environments. In _ICLR_, 2021.
* [66] L. Muller, M. Galkin, C. Morris, and L. Rampasek. Attending to graph transformers. _arXiv_, 2023.
* [67] R. L. Murphy, B. Srinivasan, V. Rao, and B. Ribeiro. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. In _ICLR_, 2019.
* [68] R. L. Murphy, B. Srinivasan, V. A. Rao, and B. Ribeiro. Relational pooling for graph representations. In _ICML_, 2019.
* [69] H. NT and T. Maehara. Revisiting graph neural networks: All we have is low-pass filters. _arXiv_, 2019.
* [70] K. Oono and T. Suzuki. Graph neural networks exponentially lose expressive power for node classification. In _ICLR_, 2020.
* [71] F. Paischer, T. Adler, V. P. Patil, A. Bitto-Nemling, M. Holzleitner, S. Lehner, H. Eghbal-Zadeh, and S. Hochreiter. History compression via language models in reinforcement learning. In _ICML_, 2022.
* [72] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In _NeurIPS_, 2019.
* [73] S. Prillo and J. M. Eisenschlos. Softsort: A continuous relaxation for the argsort operator. In _ICML_, 2020.
* [74] O. Puny, M. Atzmon, E. J. Smith, I. Misra, A. Grover, H. Ben-Hamu, and Y. Lipman. Frame averaging for invariant and equivariant network design. In _ICLR_, 2022.
* [75] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 2020.
* [76] L. Rampasek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and D. Beaini. Recipe for a general, powerful, scalable graph transformer. In _NeurIPS_, 2022.
* [77] M. Rath and A. P. Condurache. Boosting deep neural networks with geometrical prior knowledge: A survey. _arXiv_, 2020.
* [78] S. Ravanbakhsh, J. G. Schneider, and B. Poczos. Equivariance through parameter-sharing. In _ICML_, 2017.
** Ribeiro et al. [2020] L. F. R. Ribeiro, M. Schmitt, H. Schutze, and I. Gurevych. Investigating pretrained language models for graph-to-text generation. _arXiv_, 2020.
* Romero and Lohit [2022] D. W. Romero and S. Lohit. Learning partial equivariances from data. In _NeurIPS_, 2022.
* Rommel et al. [2022] C. Rommel, T. Moreau, and A. Gramfort. Deep invariant networks with differentiable augmentation layers. In _NeurIPS_, 2022.
* Rothermel et al. [2021] D. Rothermel, M. Li, T. Rocktaschel, and J. Foerster. Don't sweep your learning rate under the rug: A closer look at cross-modal transfer of pretrained transformers. _arXiv_, 2021.
* Sannai et al. [2021] A. Sannai, M. Kawano, and W. Kumagai. Equivariant and invariant reynolds networks. _arXiv_, 2021.
* Satorras et al. [2021] V. G. Satorras, E. Hoogeboom, and M. Welling. E(n) equivariant graph neural networks. In _ICML_, 2021.
* Shen et al. [2023] J. Shen, L. Li, L. M. Dery, C. Staten, M. Khodak, G. Neubig, and A. Talwalkar. Cross-modal fine-tuning: Align then refine. _arXiv_, 2023.
* Shi et al. [2016] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In _CVPR_, 2016.
* Shirzad et al. [2023] H. Shirzad, A. Velingker, B. Venkatachalam, D. J. Sutherland, and A. K. Sinop. Explorer: Sparse transformers for graphs. _arXiv_, 2023.
* Srinivasan and Ribeiro [2020] B. Srinivasan and B. Ribeiro. On the equivalence between positional node embeddings and structural graph representations. In _ICLR_, 2020.
* Steiner et al. [2021] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and L. Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. _arXiv_, 2021.
* Thiede et al. [2021] E. H. Thiede, W. Zhou, and R. Kondor. Autobahn: Automorphism-based graph neural nets. In _NeurIPS_, 2021.
* Thomas et al. [2018] N. Thomas, T. E. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley. Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point clouds. _arXiv_, 2018.
* Topping et al. [2022] J. Topping, F. D. Giovanni, B. P. Chamberlain, X. Dong, and M. M. Bronstein. Understanding oversquashing and bottlenecks on graphs via curvature. In _ICLR_, 2022.
* van der Ouderaa and van der Wilk [2022] T. F. A. van der Ouderaa and M. van der Wilk. Learning invariant weights in neural networks. In J. Cussens and K. Zhang, editors, _UAI_, 2022.
* van der Pol et al. [2020] E. van der Pol, D. E. Worrall, H. van Hoof, F. A. Oliehoek, and M. Welling. MDP homomorphic networks: Group symmetries in reinforcement learning. In _NeurIPS_, 2020.
* van der Wilk et al. [2018] M. van der Wilk, M. Bauer, S. T. John, and J. Hensman. Learning invariances using the marginal likelihood. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _NeurIPS_, 2018.
* Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In _NeurIPS_, 2017.
* Velickovic et al. [2018] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph attention networks. In _ICLR_, 2018.
* Winter et al. [2021] R. Winter, F. Noe, and D. Clevert. Permutation-invariant variational autoencoder for graph-level representation learning. In _NeurIPS_, 2021.
* Wolf et al. [2019] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, and J. Brew. Huggingface's transformers: State-of-the-art natural language processing. _arXiv_, 2019.
* Xiong et al. [2020] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In _ICML_, 2020.
* Xu et al. [2019] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In _ICLR_, 2019.
* Yarotsky [2018] D. Yarotsky. Universal approximations of invariant maps by neural networks. _arXiv_, 2018.
* Ying et al. [2021] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T. Liu. Do transformers really perform bad for graph representation? In _NeurIPS_, 2021.
* Yun et al. [2020] C. Yun, S.hojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? In _ICLR_, 2020.
* Zhang et al. [2023] B. Zhang, S. Luo, L. Wang, and D. He. Rethinking the expressive power of gnns via graph biconnectivity. _arXiv_, 2023.

Appendix

### Proofs

#### a.1.1 Proof of Theorem 1 (Section 2.1)

**Theorem 1**.: _If \(p_{\omega}\) is \(G\) equivariant, then \(\phi_{\theta,\omega}\) is \(G\) equivariant for arbitrary \(f_{\theta}\)._

Proof.: We prove \(\phi_{\theta,\omega}(\rho_{1}(g^{\prime})\mathbf{x})=\rho_{2}(g^{\prime})\phi_{ \theta,\omega}(\mathbf{x})\) for all \(\mathbf{x}\in\mathcal{X}\) and \(g^{\prime}\in G\). From Eq. (4), we have:

\[\phi_{\theta,\omega}(\rho_{1}(g^{\prime})\mathbf{x})=\mathbb{E}_{p_{\omega}(g| \rho_{1}(g^{\prime})\mathbf{x})}\left[\rho_{2}(g)f_{\theta}(\rho_{1}(g)^{-1} \rho_{1}(g^{\prime})\mathbf{x})\right].\] (11)

Let us introduce transformed random variable \(h=g^{\prime-1}g\in G\) such that \(g=g^{\prime}h\). Since the distribution \(p_{\omega}\) is \(G\) equivariant, we can see that \(p_{\omega}(g|\rho_{1}(g^{\prime})\mathbf{x})=p_{\omega}(g^{\prime-1}g|\rho_{1} (g^{\prime-1})\rho_{1}(g^{\prime})\mathbf{x})=p_{\omega}(g^{\prime-1}g| \mathbf{x})=p_{\omega}(h|\mathbf{x})\). Thus, we can rewrite the above expectation with respect to \(h\) as follows:

\[\phi_{\theta,\omega}(\rho_{1}(g^{\prime})\mathbf{x}) =\mathbb{E}_{p_{\omega}(h|\mathbf{x})}\left[\rho_{2}(g^{\prime}h) f_{\theta}(\rho_{1}(g^{\prime}h)^{-1}\rho_{1}(g^{\prime})\mathbf{x})\right]\] \[=\mathbb{E}_{p_{\omega}(h|\mathbf{x})}\left[\rho_{2}(g^{\prime} )\rho_{2}(h)f_{\theta}(\rho_{1}(h)^{-1}\rho_{1}(g^{\prime})^{-1}\rho_{1}(g^{ \prime})\mathbf{x})\right]\] \[=\rho_{2}(g^{\prime})\mathbb{E}_{p_{\omega}(h|\mathbf{x})}\left[ \rho_{2}(h)f_{\theta}(\rho_{1}(h)^{-1}\mathbf{x})\right]\] \[=\rho_{2}(g^{\prime})\phi_{\theta,\omega}(\mathbf{x}),\] (12)

showing the \(G\) equivariance of \(\phi_{\theta,\omega}\) for arbitrary \(f_{\theta}\). 

#### a.1.2 Proof of Theorem 2 (Section 2.1)

**Theorem 2**.: _If \(p_{\omega}\) is \(G\) equivariant and \(f_{\theta}\) is a universal approximator, then \(\phi_{\theta,\omega}\) is a universal approximator of \(G\) equivariant functions._

Proof.: The proof is inspired by universality proofs of prior symmetrization approaches [102, 74, 41]. Let \(\psi:\mathcal{X}\rightarrow\mathcal{Y}\) be an arbitrary \(G\) equivariant function. By equivariance of \(\psi\), we have:

\[\left\|\psi(\mathbf{x})-\phi_{\theta,\omega}(\mathbf{x})\right\| =\left\|\psi(\mathbf{x})-\mathbb{E}_{p_{\omega}(g|\mathbf{x})} \left[\rho_{2}(g)f_{\theta}(\rho_{1}(g)^{-1}\mathbf{x})\right]\right\|\] \[=\left\|\mathbb{E}_{p_{\omega}(g|\mathbf{x})}\left[\rho_{2}(g) \rho_{2}(g)^{-1}\psi(\mathbf{x})\right]-\mathbb{E}_{p_{\omega}(g|\mathbf{x}) }\left[\rho_{2}(g)f_{\theta}(\rho_{1}(g)^{-1}\mathbf{x})\right]\right\|\] \[=\left\|\mathbb{E}_{p_{\omega}(g|\mathbf{x})}\left[\rho_{2}(g) \psi(\rho_{1}(g)^{-1}\mathbf{x})\right]-\mathbb{E}_{p_{\omega}(g|\mathbf{x}) }\left[\rho_{2}(g)f_{\theta}(\rho_{1}(g)^{-1}\mathbf{x})\right]\right\|\] \[=\left\|\mathbb{E}_{p_{\omega}(g|\mathbf{x})}\left[\rho_{2}(g) \psi(\rho_{1}(g)^{-1}\mathbf{x})-\rho_{2}(g)f_{\theta}(\rho_{1}(g)^{-1} \mathbf{x})\right]\right\|.\] (13)

As \(\mathcal{Y}\) is finite-dimensional, we can assume that the linear operators in \(\text{GL}(\mathcal{Y})\) are bounded and so is the induced operator norm of group representation \(\|\rho_{2}(g)\|\) for all \(g\in G\). Thus, we have:

\[\left\|\psi(\mathbf{x})-\phi_{\theta,\omega}(\mathbf{x})\right\| \leq\max_{h\in G}\left\|\rho_{2}(h)\right\|\left\|\mathbb{E}_{p_{ \omega}(g|\mathbf{x})}\left[\psi(\rho_{1}(g)^{-1}\mathbf{x})-f_{\theta}(\rho_ {1}(g)^{-1}\mathbf{x})\right]\right\|\] \[\leq c\left\|\mathbb{E}_{p_{\omega}(g|\mathbf{x})}\left[\psi(\rho _{1}(g)^{-1}\mathbf{x})-f_{\theta}(\rho_{1}(g)^{-1}\mathbf{x})\right]\right\|.\] (14)

for some \(c>0\). If \(f_{\theta}\) is a universal approximator, for any compact set \(\mathcal{K}\subseteq\mathcal{X}\) and any \(\epsilon>0\), there exists some \(\theta\) such that \(\left\|\psi(\mathbf{x})-f_{\theta}(\mathbf{x})\right\|\leq\epsilon\) for all \(\mathbf{x}\in\mathcal{K}\). Consider the set \(\mathcal{K}_{\text{sym}}=\cup_{g\in G}\rho_{1}(g)\mathcal{K}\) where \(\rho_{1}(g)\mathcal{K}\) denotes the image of the set \(\mathcal{K}\) under linear transformation by \(\rho_{1}(g)\). We use the fact that \(\mathcal{K}_{\text{sym}}\) is also a compact set since it is the image of the compact set \(G\times\mathcal{K}\) under continuous map \((g,\mathbf{x})\mapsto\rho_{1}(g)\mathbf{x}\). As a consequence, for any compact set \(\mathcal{K}\subseteq\mathcal{X}\) and any \(\epsilon/c>0\), there exists some \(\theta\) such that \(\max_{g\in G}\left\|\psi(\rho_{1}(g)\mathbf{x})-f_{\theta}(\rho_{1}(g)\mathbf{x })\right\|\leq\epsilon/c\) for all \(\mathbf{x}\in\mathcal{K}\). Since a group is closed under inverse, for any compact set \(\mathcal{K}\subseteq\mathcal{X}\) and any \(\epsilon>0\), there exists some \(\theta\) such that:

\[\left\|\psi(\mathbf{x})-\phi_{\theta,\omega}(\mathbf{x})\right\| \leq c\left\|\mathbb{E}_{p_{\omega}(g|\mathbf{x})}\left[\psi(\rho_{1 }(g)^{-1}\mathbf{x})-f_{\theta}(\rho_{1}(g)^{-1}\mathbf{x})\right]\right\|\] \[\leq c\max_{g\in G}\left\|\psi(\rho_{1}(g)^{-1}\mathbf{x})-f_{ \theta}(\rho_{1}(g)^{-1}\mathbf{x})\right\|\] \[=\epsilon,\] (15)

for all \(\mathbf{x}\in\mathcal{K}\), showing that \(\phi_{\theta,\omega}\) is a universal approximator of \(G\) equivariant functions. 

While we have assumed that the group \(G\) is compact in the proof, we conjecture that the results can be extended to non-compact groups if we make an alternative assumption that the distribution \(p_{\omega}(g|\mathbf{x})\) is compactly supported for all \(\mathbf{x}\in\mathcal{K}\). We leave proving this as a future work.

#### a.1.3 Proof of Theorem 3 (Section 2.1)

**Theorem 3**.: _If \(q_{\omega}\) is \(G\) equivariant and \(p(\bm{\epsilon})\) is \(G\) invariant under representation \(\rho^{\prime}\) that \(|\det\rho^{\prime}(g)|=1\forall g\in G\), the distribution \(p_{\omega}(g|\mathbf{x})\) characterized by \(q_{\omega}:(\mathbf{x},\bm{\epsilon})\mapsto\rho(g)\) is \(G\) equivariant._

Proof.: We prove \(p_{\omega}(g^{\prime}g|\rho_{1}(g^{\prime})\mathbf{x})=p_{\omega}(g|\mathbf{x})\) for all \(\mathbf{x}\in\mathcal{X}\) and \(g,g^{\prime}\in G\). In general, we are interested in obtaining a faithful representation \(\rho\), _i.e._, such that \(\rho(g)\) is distinct for each \(g\). We can interpret the probability \(p_{\omega}(g|\mathbf{x},\bm{\epsilon})\) as a delta distribution centered at the group representation \(\rho(g)\):

\[p_{\omega}(g|\mathbf{x},\bm{\epsilon})=\delta(\rho(g)=q_{\omega}(\mathbf{x}, \bm{\epsilon})).\] (16)

To obtain \(p_{\omega}(g|\mathbf{x})\), we marginalize over \(p(\bm{\epsilon})\):

\[p_{\omega}(g|\mathbf{x}) =\int_{\bm{\epsilon}}p_{\omega}(g|\mathbf{x},\bm{\epsilon})p( \bm{\epsilon})d\bm{\epsilon}\] \[=\int_{\bm{\epsilon}}\delta(\rho(g)=q_{\omega}(\mathbf{x},\bm{ \epsilon}))p(\bm{\epsilon})d\bm{\epsilon}.\] (17)

Let us consider \(p_{\omega}(g^{\prime}g|\rho_{1}(g^{\prime})\mathbf{x})\):

\[p_{\omega}(g^{\prime}g|\rho_{1}(g^{\prime})\mathbf{x}) =\int_{\bm{\epsilon}}\delta(\rho(g^{\prime}g)=q_{\omega}(\rho_{1} (g^{\prime})\mathbf{x},\bm{\epsilon}))p(\bm{\epsilon})d\bm{\epsilon}.\] (18)

Using the \(G\) equivariance of \(q_{\omega}\), we have:

\[q_{\omega}(\rho_{1}(g^{\prime})\mathbf{x},\bm{\epsilon}) =\rho(g^{\prime})q_{\omega}(\rho_{1}(g^{\prime-1})\rho_{1}(g^{ \prime})\mathbf{x},\rho^{\prime}(g^{\prime-1})\bm{\epsilon})\] \[=\rho(g^{\prime})q_{\omega}(\mathbf{x},\rho^{\prime}(g^{\prime-1} )\bm{\epsilon})\] (19)

which leads to the following:

\[p_{\omega}(g^{\prime}g|\rho_{1}(g^{\prime})\mathbf{x}) =\int_{\bm{\epsilon}}\delta(\rho(g^{\prime}g)=\rho(g^{\prime})q_{ \omega}(\mathbf{x},\rho^{\prime}(g^{\prime-1})\bm{\epsilon}))p(\bm{\epsilon})d \bm{\epsilon}\] \[=\int_{\bm{\epsilon}}\delta(\rho(g)=q_{\omega}(\mathbf{x},\rho^{ \prime}(g^{\prime-1})\bm{\epsilon}))p(\bm{\epsilon})d\bm{\epsilon}.\] (20)

Note that the second equality follows from invertibility of \(\rho(g^{\prime})\). We now introduce a change of variables \(\bm{\epsilon}^{\prime}=\rho^{\prime}(g^{\prime-1})\bm{\epsilon}\) that \(\bm{\epsilon}=\rho^{\prime}(g^{\prime})\bm{\epsilon}^{\prime}\):

\[p_{\omega}(g^{\prime}g|\rho_{1}(g^{\prime})\mathbf{x})=\int_{\bm{\epsilon}^{ \prime}}\delta(\rho(g)=q_{\omega}(\mathbf{x},\bm{\epsilon}^{\prime}))p(\rho^{ \prime}(g^{\prime})\bm{\epsilon}^{\prime})\frac{1}{|\det\rho^{\prime}(g^{ \prime-1})|}d\bm{\epsilon}^{\prime}.\] (21)

With \(|\det\rho^{\prime}(g^{\prime-1})|=1\), and \(G\) invariance of \(p(\bm{\epsilon})\) which gives \(p(\rho^{\prime}(g^{\prime})\bm{\epsilon}^{\prime})=p(\bm{\epsilon}^{\prime})\), we get:

\[p_{\omega}(g^{\prime}g|\rho_{1}(g^{\prime})\mathbf{x}) =\int_{\bm{\epsilon}^{\prime}}\delta(\rho(g)=q_{\omega}(\mathbf{x},\bm{\epsilon}^{\prime}))p(\bm{\epsilon}^{\prime})d\bm{\epsilon}^{\prime}\] \[=p_{\omega}(g|\mathbf{x}),\] (22)

showing the \(G\) equivariance of \(p_{\omega}(g|\mathbf{x})\). 

#### a.1.4 Proof of Validity for Implemented Equivariant Distributions \(p_{\omega}\) (Section 2.2)

We formally show \(G\) equivariance of the implemented distributions \(p_{\omega}(g|\mathbf{x})\) presented in Section 2.2. All implementations have a form of noise-outsourced function \(q_{\omega}:(\mathbf{x},\bm{\epsilon})\mapsto\rho(g)\) using distribution \(\bm{\epsilon}\sim p(\bm{\epsilon})\) and map \(q_{\omega}\) which is composed of \(G\) equivariant neural network and postprocessing to \(\rho(g)\). From Theorem 3, for \(G\) equivariance of \(p_{\omega}(g|\mathbf{x})\), it is sufficient to show \(G\) invariance of \(p(\bm{\epsilon})\) under a representation \(\rho^{\prime}\) such that \(|\det\rho^{\prime}(g)|=1\) along with \(G\) equivariance of \(q_{\omega}\), which we show below.

Symmetric Group \(\text{S}_{n}\)We recall that \(p_{\omega}(g|\mathbf{x})\) for the symmetric group \(\text{S}_{n}\) is implemented as below:

1. Sample node-level noise \(\bm{\epsilon}\in\mathbb{R}^{n\times d}\) from i.i.d. uniform \(\text{Unif}[0,\eta]\).
2. Use a GNN to obtain node-level scalar features \((\mathbf{x},\bm{\epsilon})\mapsto\mathbf{Z}\in\mathbb{R}^{n}\).

3. Assuming \(\mathbf{Z}\) is tie-free, use argsort [98] to obtain group representation \(\mathbf{Z}\mapsto\mathbf{P}_{g}=\rho(g)\). \[\mathbf{P}_{g}=\text{eq}(\mathbf{Z}\mathbf{1}^{\top},\mathbf{1}\text{sort}( \mathbf{Z})^{\top}),\] (23) where eq denotes elementwise equality indicator.

We now show the following:

**Proposition 3**.: _The proposed distribution \(p_{\omega}(g|\mathbf{x})\) for the symmetric group \(\mathbf{S}_{n}\) is equivariant._

Proof.: Given \(p(\boldsymbol{\epsilon})\) is elementwise i.i.d., it is \(\mathbf{S}_{n}\) invariant under the base representation \(\rho^{\prime}(g)=\mathbf{P}_{g}\) which satisfies \(|\det\mathbf{P}_{g}|=1\) from orthogonality. As a GNN is \(\mathbf{S}_{n}\) equivariant, we only need to show \(\mathbf{S}_{n}\) equivariance of \(\text{argsort}:\mathbf{Z}\mapsto\mathbf{P}_{g}\). This can be shown by transforming \(\mathbf{Z}\) with any permutation matrix \(\mathbf{P}_{g^{\prime}}\). Since sort operator and any row replicated matrices are invariant to \(\mathbf{P}_{g^{\prime}}\), we have:

\[\text{eq}(\mathbf{P}_{g^{\prime}}\mathbf{Z}\mathbf{1}^{\top}, \mathbf{1}\text{sort}(\mathbf{P}_{g^{\prime}}\mathbf{Z})^{\top}) =\text{eq}(\mathbf{P}_{g^{\prime}}\mathbf{Z}\mathbf{1}^{\top}, \mathbf{1}\text{sort}(\mathbf{Z})^{\top})\] \[=\text{eq}(\mathbf{P}_{g^{\prime}}\mathbf{Z}\mathbf{1}^{\top}, \mathbf{P}_{g^{\prime}}\mathbf{1}\text{sort}(\mathbf{Z})^{\top}).\] (24)

Since eq commutes with \(\mathbf{P}_{g^{\prime}}\), we have:

\[\text{eq}(\mathbf{P}_{g^{\prime}}\mathbf{Z}\mathbf{1}^{\top}, \mathbf{1}\text{sort}(\mathbf{P}_{g^{\prime}}\mathbf{Z})^{\top}) =\text{eq}(\mathbf{P}_{g^{\prime}}\mathbf{Z}\mathbf{1}^{\top}, \mathbf{P}_{g^{\prime}}\mathbf{1}\text{sort}(\mathbf{Z})^{\top})\] \[=\mathbf{P}_{g^{\prime}}\text{eq}(\mathbf{Z}\mathbf{1}^{\top}, \mathbf{1}\text{sort}(\mathbf{Z})^{\top})\] \[=\mathbf{P}_{g^{\prime}}\mathbf{P}_{g},\] (25)

showing that argsort is \(\mathbf{S}_{n}\) equivariant, _i.e._, it maps \(\mathbf{P}_{g^{\prime}}\mathbf{Z}\mapsto\mathbf{P}_{g^{\prime}}\mathbf{P}_{g}\) for all \(\mathbf{P}_{g^{\prime}}\in\mathbf{S}_{n}\). Combining the above, by Theorem 3, the distribution \(p_{\omega}(g|\mathbf{x})\) is \(\mathbf{S}_{n}\) equivariant. 

Orthogonal Group \(\text{O}(n)\), \(\text{SO}(n)\)We recall that \(p_{\omega}(g|\mathbf{x})\) for the orthogonal group \(\text{O}(n)\) or special orthogonal group \(\text{SO}(n)\) is implemented as follows:

1. Sample noise \(\boldsymbol{\epsilon}\in\mathbb{R}^{n\times d}\) from i.i.d. normal \(\mathcal{N}(0,\eta^{2})\).
2. Use an \(\text{O}(n)\)/\(\text{SO}(n)\) equivariant neural network to obtain \(n\) features \((\mathbf{x},\boldsymbol{\epsilon})\mapsto\mathbf{Z}\in\mathbb{R}^{n\times n}\).
3. Assuming \(\mathbf{Z}\) is full-rank, use Gram-Schmidt process [41] to obtain an orthogonal matrix \(\mathbf{Z}\mapsto\mathbf{Q}\).
4. For the \(\text{O}(n)\) group, use the obtained matrix as group representation \(\mathbf{Q}=\mathbf{Q}_{g}=\rho(g)\).
5. For the \(\text{SO}(n)\) group, use below scale operator to obtain group representation \(\mathbf{Q}\mapsto\mathbf{Q}_{g}^{+}=\rho(g)\). \[\text{scale}:\left[\begin{array}{c|c}\mathbf{Q}_{1}&...&\left|\mathbf{Q}_ {n}\right.\end{array}\right]\mapsto\left[\begin{array}{c|c}\text{det}( \mathbf{Q})\cdot\mathbf{Q}_{1}&...&\left|\mathbf{Q}_{n}\right.\end{array}\right].\] (26)

We now show the following:

**Proposition 4**.: _The proposed distribution \(p_{\omega}(g|\mathbf{x})\) for the orthogonal group \(\text{O}(n)\) is equivariant._

Proof.: Without loss of generality, let us omit the scale \(\eta\) for brevity, which gives that each column \(\boldsymbol{\epsilon}_{i}\in\mathbb{R}^{n}\) of the noise \(\boldsymbol{\epsilon}\) independently follows multivariate standard normal \(\boldsymbol{\epsilon}_{i}\sim\mathcal{N}(0,\mathbf{I}_{n})\). Then, the density \(p(\boldsymbol{\epsilon}_{i})=(2\pi)^{-n/2}\exp{(-\|\boldsymbol{\epsilon}_{i}\| _{2}^{2}/2)}\) is invariant under orthogonal transformation \(\mathbf{Q}\) since \(\|\mathbf{Q}\boldsymbol{\epsilon}_{i}\|_{2}^{2}=(\mathbf{Q}\boldsymbol{\epsilon }_{i})^{\top}\mathbf{Q}\boldsymbol{\epsilon}_{i}=\boldsymbol{\epsilon}_{i}^{ \top}\mathbf{Q}^{\top}\mathbf{Q}\boldsymbol{\epsilon}_{i}=\boldsymbol{ \epsilon}_{i}^{\top}\boldsymbol{\epsilon}_{i}=\|\boldsymbol{\epsilon}_{i}\|_{2}^ {2}\). Therefore, the distribution \(p(\boldsymbol{\epsilon})\) is invariant under the base representation \(\rho^{\prime}(g)=\mathbf{Q}_{g}\) which satisfies \(|\det\rho^{\prime}(g)|=1\) from orthogonality. As we use an equivariant neural network to obtain \(\mathbf{Z}\), and Gram-Schmidt procedure \(\mathbf{Z}\mapsto\mathbf{Q}_{g}\) is \(\text{O}(n)\) equivariant (Theorem 5 of [41]), by Theorem 3, the distribution \(p_{\omega}(g|\mathbf{x})\) is \(\text{O}(n)\) equivariant. 

**Proposition 5**.: _The proposed distribution \(p_{\omega}(g|\mathbf{x})\) for special orthogonal group \(\text{SO}(n)\) is equivariant._

Proof.: From the proof of Proposition 4, it follows that the distribution \(p(\boldsymbol{\epsilon})\) is invariant under the base representation \(\rho^{\prime}(g)=\mathbf{Q}_{g}^{+}\) which satisfies \(|\det\rho^{\prime}(g)|=1\) due to orthogonality. As we use an equivariant neural network to obtain \(\mathbf{Z}\), and Gram-Schmidt procedure \(\mathbf{Z}\mapsto\mathbf{Q}\) has \(\text{O}(n)\) equivariance which implies \(\text{SO}(n)\) equivariance because of \(\text{SO}(n)\leq\text{O}(n)\), we only need to show \(\text{SO}(n)\) equivariance of scale : \(\mathbf{Q}\mapsto\mathbf{Q}^{+}_{g}\). This can be done by transforming \(\mathbf{Q}\) with an orthogonal \(\mathbf{Q}^{+}_{g^{\prime}}\) of determinant \(+1\). Since \(\text{det}(\mathbf{Q}^{+}_{g}\mathbf{Q})=\text{det}(\mathbf{Q}^{+}_{g^{\prime}}) \text{det}(\mathbf{Q})=\text{det}(\mathbf{Q})\), we have the following:

\[\text{scale}(\mathbf{Q}^{+}_{g^{\prime}}\mathbf{Q}) =\left[\begin{array}{c}\text{det}(\mathbf{Q}^{+}_{g^{\prime}} \mathbf{Q})\cdot(\mathbf{Q}^{+}_{g^{\prime}}\mathbf{Q})_{1}\ \left|\...\ \right|\ (\mathbf{Q}^{+}_{g^{\prime}} \mathbf{Q})_{n}\end{array}\right]\] \[=\left[\begin{array}{c}\text{det}(\mathbf{Q})\cdot(\mathbf{Q}^ {+}_{g^{\prime}}\mathbf{Q})_{1}\ \left|\...\ \right|\ (\mathbf{Q}^{+}_{g^{\prime}} \mathbf{Q})_{n}\end{array}\right].\] (27)

Also, scaling the first column of the product \(\mathbf{Q}^{+}_{g^{\prime}}\mathbf{Q}\) with \(\text{det}(\mathbf{Q})\) is equivalent to scaling the first column of \(\mathbf{Q}\) with \(\text{det}(\mathbf{Q})\) then computing the product since \((\mathbf{Q}^{+}_{g^{\prime}}\mathbf{Q})_{ij}=\sum_{k}\mathbf{Q}^{+}_{g^{ \prime}ik}\mathbf{Q}_{kj}\). This gives:

\[\text{scale}(\mathbf{Q}^{+}_{g^{\prime}}\mathbf{Q}) =\mathbf{Q}^{+}_{g^{\prime}}\left[\begin{array}{c}\text{det}( \mathbf{Q})\cdot\mathbf{Q}_{1}\end{array}\left|\...\ \right|\ \mathbf{Q}_{n}\end{array}\right]\] \[=\mathbf{Q}^{+}_{g^{\prime}}\text{scale}(\mathbf{Q}),\] (28)

showing that scale operator is \(\text{SO}(n)\) equivariant. We also note that \(\text{scale}(\mathbf{Q})\) gives orthogonal matrix of determinant \(+1\), as it returns \(\mathbf{Q}\) if \(\text{det}(\mathbf{Q})=+1\), otherwise (\(\text{det}(\mathbf{Q})=-1\) since \(\mathbf{Q}\) is orthogonal) scales the first column by \(-1\) which flips determinant to \(+1\) while not affecting orthogonality. Combining the above, by Theorem 3, the distribution \(p_{\omega}(g|\mathbf{x})\) is \(\text{SO}(n)\) equivariant. 

Euclidean Group \(\text{E}(n)\), \(\text{SE}(n)\)We recall that, unlike the other groups, we handle the Euclidean group \(\text{E}(n)\) and special Euclidean group \(\text{SE}(n)\) at symmetrization level as the translation component \(\text{T}(n)\) in \(\text{E}(n)=\text{O}(n)\ltimes\text{T}(n)\) and \(\text{SE}(n)=\text{SO}(n)\ltimes\text{T}(n)\) is non-compact. This is done as follows:

\[\phi_{\theta,\omega}(\mathbf{x})=\mathbb{E}_{p_{\omega}(g|\mathbf{x}-\bar{ \mathbf{x}}\mathbf{1}^{\top})}\left[\bar{\mathbf{x}}\mathbf{1}^{\top}+g\cdot f _{\theta}(g^{-1}\cdot(\mathbf{x}-\bar{\mathbf{x}}\mathbf{1}^{\top}))\right],\] (29)

where \(\bar{\mathbf{x}}\in\mathbb{R}^{n}\) is centroid (mean over channels) of data \(\mathbf{x}\in\mathbb{R}^{n\times d}\) and distribution \(p_{\omega}\) is \(\text{O}(n)\)/\(\text{SO}(n)\) equivariant for \(\text{E}(n)\)/\(\text{SE}(n)\) equivariant symmetrization, respectively. We now show the following:

**Proposition 6**.: _The proposed symmetrization \(\phi_{\theta,\omega}\) for the Euclidean group \(\text{E}(n)\) is equivariant._

Proof.: We prove \(\phi_{\theta,\omega}(g^{\prime}\cdot\mathbf{x})=g^{\prime}\cdot\phi_{\theta, \omega}(\mathbf{x})\) for all \(\mathbf{x}\in\mathcal{X}\) and \(g^{\prime}\in\text{E}(n)\). From Eq. (29), we have:

\[\phi_{\theta,\omega}(g^{\prime}\cdot\mathbf{x})=\mathbb{E}_{p_{\omega}(g|g^{ \prime}\cdot\mathbf{x}-\bar{g^{\prime}}\cdot\mathbf{x}\mathbf{1}^{\top})} \left[\ \overline{g^{\prime}\cdot\mathbf{x}}\mathbf{1}^{\top}+g\cdot f_{\theta}(g^{-1} \cdot(g^{\prime}\cdot\mathbf{x}-\overline{g^{\prime}\cdot\mathbf{x}}\mathbf{1 }^{\top}))\right].\] (30)

In general, an element of Euclidean group \(g^{\prime}\in\text{E}(n)\) acts on data \(\mathbf{x}\in\mathbb{R}^{n\times d}\) via \(g^{\prime}\cdot\mathbf{x}=\mathbf{Q}_{g^{\prime}}\mathbf{x}+\mathbf{t}_{g^{ \prime}}\mathbf{1}^{\top}\) where \(\mathbf{Q}_{g^{\prime}}\in\text{O}(n)\) is its rotation component and \(\mathbf{t}_{g^{\prime}}\in\mathbb{R}^{n}\) is its translation component [74, 41]. With this, the centroid of the transformed data \(g^{\prime}\cdot\mathbf{x}\) is given as follows:

\[\overline{g^{\prime}\cdot\mathbf{x}}=\overline{\mathbf{Q}_{g^{\prime}}\mathbf{x} +\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}}=\overline{\mathbf{Q}_{g^{\prime}} \mathbf{x}}+\mathbf{t}_{g^{\prime}}=\mathbf{Q}_{g^{\prime}}\bar{\mathbf{x}}+ \mathbf{t}_{g^{\prime}},\] (31)

which leads to the following:

\[g^{\prime}\cdot\mathbf{x}-\overline{g^{\prime}\cdot\mathbf{x}} \mathbf{1}^{\top} =\mathbf{Q}_{g^{\prime}}\mathbf{x}+\mathbf{t}_{g^{\prime}}\mathbf{1}^ {\top}-\mathbf{Q}_{g^{\prime}}\bar{\mathbf{x}}\mathbf{1}^{\top}-\mathbf{t}_{g^{ \prime}}\mathbf{1}^{\top}\] \[=\mathbf{Q}_{g^{\prime}}(\mathbf{x}-\bar{\mathbf{x}}\mathbf{1}^{ \top}).\] (32)

Above shows that subtracting centroid eliminates the translation component of the problem and leaves \(\text{O}(n)\) equivariance component. Based on that, we have the following:

\[\phi_{\theta,\omega}(g^{\prime}\cdot\mathbf{x}) =\mathbb{E}_{p_{\omega}(g|\mathbf{Q}_{g^{\prime}}(\mathbf{x}-\bar{ \mathbf{x}}\mathbf{1}^{\top}))}\left[\mathbf{Q}_{g^{\prime}}\bar{\mathbf{x}} \mathbf{1}^{\top}+\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}+g\cdot f_{\theta}(g^{ -1}\cdot(\mathbf{Q}_{g^{\prime}}(\mathbf{x}-\bar{\mathbf{x}}\mathbf{1}^{\top})))\right]\] \[=\mathbb{E}_{p_{\omega}(g|g^{\prime}\cdot(\mathbf{x}-\bar{\mathbf{ x}}\mathbf{1}^{\top}))}\left[g^{\prime}\cdot\bar{\mathbf{x}}\mathbf{1}^{\top}+g\cdot f_{ \theta}(g^{-1}g^{\prime}\cdot(\mathbf{x}-\bar{\mathbf{x}}\mathbf{1}^{\top})) \right]+\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}.\] (33)

Note that, inside the expectation, we interpret the rotation component of \(g^{\prime}\) as an element of the orthogonal group \(\text{O}(n)\). Similar as in the proof of Theorem 1, we introduce transformed random variable \(h=g^{\prime-1}g\in\text{O}(n)\) that \(g=g^{\prime}h\). Since the distribution \(p_{\omega}\) is \(\text{O}(n)\) equivariant, we can see that \(p_{\omega}(g|g^{\prime}\cdot(\mathbf{x}-\overline{\mathbf{x}}\mathbf{1}^{\top}))=p_ {\omega}(g^{\prime-1}g|g^{\prime-1}g^{\prime}\cdot(\mathbf{x}-\overline{\mathbf{ x}}\mathbf{1}^{\top}))=p_{\omega}(g^{\prime-1}g|\mathbf{x}-\overline{\mathbf{x}} \mathbf{1}^{\top})\). Thus we can rewrite the above expectation with respect to \(h\) as follows:

\[\phi_{\theta,\omega}(g^{\prime}\cdot\mathbf{x}) =\mathbb{E}_{p_{\omega}(h|\mathbf{x}-\overline{\mathbf{x}} \mathbf{1}^{\top})}\left[g^{\prime}\cdot\bar{\mathbf{x}}\mathbf{1}^{\top}+g^{ \prime}h\cdot f_{\theta}((g^{\prime}h)^{-1}g^{\prime}\cdot(\mathbf{x}-\overline {\mathbf{x}}\mathbf{1}^{\top}))\right]+\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}\] \[=\mathbb{E}_{p_{\omega}(h|\mathbf{x}-\overline{\mathbf{x}} \mathbf{1}^{\top})}\left[g^{\prime}\cdot\bar{\mathbf{x}}\mathbf{1}^{\top}+g^{ \prime}h\cdot f_{\theta}(h^{-1}\cdot(\mathbf{x}-\overline{\mathbf{x}}\mathbf{ 1}^{\top}))\right]+\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}\] \[=\mathbf{Q}_{g^{\prime}}\mathbb{E}_{p_{\omega}(h|\mathbf{x}- \overline{\mathbf{x}}\mathbf{1}^{\top})}\left[\bar{\mathbf{x}}\mathbf{1}^{ \top}+h\cdot f_{\theta}(h^{-1}\cdot(\mathbf{x}-\overline{\mathbf{x}}\mathbf{1}^ {\top}))\right]+\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}\] \[=\mathbf{Q}_{g^{\prime}}\phi_{\theta,\omega}(\mathbf{x})+\mathbf{ t}_{g^{\prime}}\mathbf{1}^{\top}\] \[=g^{\prime}\cdot\phi_{\theta,\omega}(\mathbf{x}),\] (34)

showing the \(\mathrm{E}(n)\) equivariance of \(\phi_{\theta,\omega}\). 

**Proposition 7**.: _The proposed symmetrization \(\phi_{\theta,\omega}\) for special Euclidean group \(\mathrm{SE}(n)\) is equivariant._

Proof.: We prove \(\phi_{\theta,\omega}(g^{\prime}\cdot\mathbf{x})=g^{\prime}\cdot\phi_{\theta, \omega}(\mathbf{x})\) for all \(\mathbf{x}\in\mathcal{X}\) and \(g^{\prime}\in\mathrm{SE}(n)\), in an analogous manner to the proof of Proposition 6. From Eq. (29), we have:

\[\phi_{\theta,\omega}(g^{\prime}\cdot\mathbf{x})=\mathbb{E}_{p_{\omega}(g|g^{ \prime}\cdot\mathbf{x}-\overline{g^{\prime}}\mathbf{x}\mathbf{1}^{\top})} \left[\overline{g^{\prime}\cdot\mathbf{x}}\mathbf{1}^{\top}+g\cdot f_{\theta}( g^{-1}\cdot(g^{\prime}\cdot\mathbf{x}-\overline{g^{\prime}\cdot\mathbf{x}} \mathbf{1}^{\top}))\right].\] (35)

In general, an element of special Euclidean group \(g^{\prime}\in\mathrm{SE}(n)\) acts on data \(\mathbf{x}\in\mathbb{R}^{n\times d}\) via \(g^{\prime}\cdot\mathbf{x}=\mathbf{Q}_{g^{\prime}}^{+}\mathbf{x}+\mathbf{t}_{g^ {\prime}}\mathbf{1}^{\top}\) where \(\mathbf{Q}_{g^{\prime}}^{+}\in\mathrm{SO}(n)\) is rotation component and \(\mathbf{t}_{g^{\prime}}\in\mathbb{R}^{n}\) is translation [74, 41]. With this, the centroid of the transformed data \(g^{\prime}\cdot\mathbf{x}\) is given as follows:

\[\overline{g^{\prime}\cdot\mathbf{x}}=\mathbf{Q}_{g^{\prime}}^{+}\mathbf{x}+ \mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}=\mathbf{Q}_{g^{\prime}}^{+}\mathbf{x }+\mathbf{t}_{g^{\prime}}=\mathbf{Q}_{g^{\prime}}^{+}\overline{\mathbf{x}}+ \mathbf{t}_{g^{\prime}},\] (36)

which leads to the following:

\[g^{\prime}\cdot\mathbf{x}-\overline{g^{\prime}\cdot\mathbf{x}} \mathbf{1}^{\top} =\mathbf{Q}_{g^{\prime}}^{+}\mathbf{x}+\mathbf{t}_{g^{\prime}} \mathbf{1}^{\top}-\mathbf{Q}_{g^{\prime}}^{+}\overline{\mathbf{x}}\mathbf{1}^{ \top}-\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}\] \[=\mathbf{Q}_{g^{\prime}}^{+}(\mathbf{x}-\overline{\mathbf{x}} \mathbf{1}^{\top}).\] (37)

Similar as in Proposition 6, subtracting centroid only leaves \(\mathrm{SO}(n)\) component. We then have:

\[\phi_{\theta,\omega}(g^{\prime}\cdot\mathbf{x}) =\mathbb{E}_{p_{\omega}(g|\mathbf{Q}_{g^{\prime}}^{+}(\mathbf{x}- \overline{\mathbf{x}}\mathbf{1}^{\top}))}\left[\mathbf{Q}_{g^{\prime}}^{+} \mathbf{x}\mathbf{1}^{\top}+\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}+g\cdot f_{ \theta}(g^{-1}\cdot(\mathbf{Q}_{g^{\prime}}^{+}(\mathbf{x}-\overline{\mathbf{x} }\mathbf{1}^{\top})))\right]\] \[=\mathbb{E}_{p_{\omega}(g|g^{\prime}\cdot(\mathbf{x}-\overline{ \mathbf{x}}\mathbf{1}^{\top}))}\left[g^{\prime}\cdot\bar{\mathbf{x}}\mathbf{1}^{ \top}+g\cdot f_{\theta}(g^{-1}g^{\prime}\cdot(\mathbf{x}-\overline{\mathbf{x}} \mathbf{1}^{\top}))\right]+\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top},\] (38)

where, inside the expectation, we interpret the rotation component of \(g^{\prime}\) as an element of the special orthogonal group \(\mathrm{SO}(n)\). Similar as in Theorem 1, we introduce \(h=g^{\prime-1}g\in\mathrm{SO}(n)\) that \(g=g^{\prime}h\). As the distribution \(p_{\omega}\) is \(\mathrm{SO}(n)\) equivariant, we have \(p_{\omega}(g|g^{\prime}\cdot(\mathbf{x}-\overline{\mathbf{x}}\mathbf{1}^{\top}))=p _{\omega}(g^{\prime-1}g|g^{\prime-1}g^{\prime}\cdot(\mathbf{x}-\overline{\mathbf{x} }\mathbf{1}^{\top}))=p_{\omega}(h|\mathbf{x}-\overline{\mathbf{x}}\mathbf{1}^{ \top})\). We then rewrite the expectation with respect to \(h\):

\[\phi_{\theta,\omega}(g^{\prime}\cdot\mathbf{x}) =\mathbb{E}_{p_{\omega}(h|\mathbf{x}-\overline{\mathbf{x}}\mathbf{1}^ {\top})}\left[g^{\prime}\cdot\bar{\mathbf{x}}\mathbf{1}^{\top}+g^{\prime}h\cdot f _{\theta}((g^{\prime}h)^{-1}g^{\prime}\cdot(\mathbf{x}-\overline{\mathbf{x}} \mathbf{1}^{\top}))\right]+\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}\] \[=\mathbb{E}_{p_{\omega}(h|\mathbf{x}-\overline{\mathbf{x}} \mathbf{1}^{\top})}\left[g^{\prime}\cdot\bar{\mathbf{x}}\mathbf{1}^{\top}+g^{ \prime}h\cdot f_{\theta}(h^{-1}\cdot(\mathbf{x}-\overline{\mathbf{x}}\mathbf{1}^{ \top}))\right]+\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}\] \[=\mathbf{Q}_{g^{\prime}}^{+}\mathbb{E}_{p_{\omega}(h|\mathbf{x}- \overline{\mathbf{x}}\mathbf{1}^{\top})}\left[\bar{\mathbf{x}}\mathbf{1}^{\top}+h \cdot f_{\theta}(h^{-1}\cdot(\mathbf{x}-\overline{\mathbf{x}}\mathbf{1}^{\top})) \right]+\mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}\] \[=\mathbf{Q}_{g^{\prime}}^{+}\phi_{\theta,\omega}(\mathbf{x})+ \mathbf{t}_{g^{\prime}}\mathbf{1}^{\top}\] \[=g^{\prime}\cdot\phi_{\theta,\omega}(\mathbf{x}),\] (39)

showing the \(\mathrm{SE}(n)\) equivariance of \(\phi_{\theta,\omega}\). 

Product Group \(H\times K\)For the product group \(H\times K\), we assume that the base representation for each element \(g=(h,k)\) is given as a pair of representations \(\rho(g)=(\rho(h),\rho(k))\). Without loss of generality, we further assume that the representation \(\rho(g)\) can be expressed as the Kronecker product \(\rho(g)=\rho(h)\otimes\rho(k)\) that acts on flattened data \(\text{vec}(\mathbf{x})\) as \(\mathbf{x}\mapsto\text{vec}^{-1}(\rho(g)\text{vec}(\mathbf{x}))\). This follows the standard approach in equivariant deep learning [30, 57] that deals with composite representations using direct sum and tensor products of base group representations.

Above approach applies to many practical product groups, including sets and graphs with Euclidean attributes (\(\text{S}_{n}\times\text{O}(d)/\text{SO}(d)\)6) and sets of symmetric elements (\(\text{S}_{n}\times H\)) in general [59]example, for the group \(\text{S}_{n}\times\text{O}(d)\) on data \(\mathbf{x}\in\mathbb{R}^{n\times d}\), an element \(g=(h,k)\) has representation \(\rho(g)=\rho(h)\otimes\rho(k)\in\mathbb{R}^{nd\times nd}\) combined from permutation \(\rho(h)\in\mathbb{R}^{n\times n}\) and rotation \(\rho(k)\in\mathbb{R}^{d\times d}\), which acts by \(\mathbf{x}\mapsto\text{vec}^{-1}(\rho(g)\text{vec}(\mathbf{x}))\) or more simply \(\mathbf{x}\mapsto\rho(h)\mathbf{x}\rho(k)^{\top}\).

Now we recall that the \(p_{\omega}(g|\mathbf{x})\) for the product group \(H\times K\) is implemented as follows:

1. Sample noise \(\boldsymbol{\epsilon}\in\mathcal{E}\) from i.i.d. normal \(\mathcal{N}(0,\eta^{2})\) such that \(p(\boldsymbol{\epsilon})\) is invariant under representations of \(H\) and \(K\) that satisfy \(|\det\rho^{\prime}(h)|=1\) and \(|\det\rho^{\prime}(k)|=1\), respectively. For example, for \(\text{S}_{n}\times\text{O}(d)\), the noise \(\boldsymbol{\epsilon}\in\mathbb{R}^{n\times d}\) that follows i.i.d. normal \(\mathcal{N}(0,\eta^{2})\) is invariant under base representations of both \(\text{S}_{n}\) and \(\text{O}(d)\) which are orthogonal.
2. Use a \(H\times K\) equivariant neural network to obtain features \((\mathbf{x},\boldsymbol{\epsilon})\mapsto(\mathbf{Z}_{H},\mathbf{Z}_{K})\) where \(\mathbf{Z}_{H}\) is \(K\) invariant and \(\mathbf{Z}_{K}\) is \(H\) invariant. For example, for \(\text{S}_{n}\times\text{O}(d)\), we expect node-level scalar features \(\mathbf{Z}_{\text{S}_{n}}\in\mathbb{R}^{n}\) to be \(\text{O}(d)\) invariant and \(d\) global rotary features \(\mathbf{Z}_{\text{O}(d)}\in\mathbb{R}^{d\times d}\) to be \(\text{S}_{n}\) invariant.
3. Apply postprocessing for \(H\) and \(K\) groups onto \(\mathbf{Z}_{H}\) and \(\mathbf{Z}_{K}\) respectively to obtain representations \(\mathbf{Z}_{H}\mapsto\rho(h)\) and \(\mathbf{Z}_{K}\mapsto\rho(k)\) of \(H\) and \(K\) groups respectively. For example, for \(\text{S}_{n}\times\text{O}(d)\), we use argsort in Eq. (23) to obtain \(\mathbf{Z}_{\text{S}_{n}}\mapsto\rho(h)\) and Gram-Schmidt process to obtain \(\mathbf{Z}_{\text{O}(d)}\mapsto\rho(k)\).
4. Combine the representations \(\rho(g)=(\rho(h),\rho(k))\) to obtain a representation for the \(H\times K\) group.

We now show the following:

**Proposition 8**.: _The proposed distribution \(p_{\omega}(g|\mathbf{x})\) for the product group \(H\times K\) is equivariant._

Proof.: By assumption, \(p(\boldsymbol{\epsilon})\) is invariant under representations of \(H\) and \(K\) that satisfy \(|\det\rho^{\prime}(h)|=1\) and \(|\det\rho^{\prime}(k)|=1\), respectively. This implies \(H\times K\) invariance as well, since \(p(\boldsymbol{\epsilon})=p(h\cdot\boldsymbol{\epsilon})=p(k\cdot\boldsymbol{ \epsilon})\) for all \(\boldsymbol{\epsilon}\in\mathcal{E},h\in H,k\in K\) gives \(p(k\cdot h\cdot\boldsymbol{\epsilon})=p(k\cdot(h\cdot\boldsymbol{\epsilon}))= p(h\cdot\boldsymbol{\epsilon})=p(\boldsymbol{\epsilon})\), and Kronecker product of matrices of determinant \(1\) gives a matrix of determinant \(1\). Furthermore, the map \((\mathbf{x},\boldsymbol{\epsilon})\mapsto(\rho(h),\rho(k))=\rho(g)\) is overall \(H\times K\) equivariant, since an input transformed with \(g^{\prime}=(h^{\prime},k^{\prime})\) is first mapped by the equivariant neural network as \((g^{\prime}\cdot\mathbf{x},g^{\prime}\cdot\boldsymbol{\epsilon})\mapsto(h^{ \prime}\cdot\mathbf{Z}_{H},k^{\prime}\cdot\mathbf{Z}_{K})\), then postprocessed as \((h^{\prime}\cdot\mathbf{Z}_{H},k^{\prime}\cdot\mathbf{Z}_{K})\mapsto(\rho(h^{ \prime})\rho(h),\rho(k^{\prime})\rho(k))=(\rho(h^{\prime}),\rho(k^{\prime})) \cdot(\rho(h),\rho(k))=\rho(g^{\prime})\rho(g)\). Combining the above, by Theorem 3, the distribution \(p_{\omega}(g|\mathbf{x})\) is \(H\times K\) equivariant. 

#### a.1.5 Proof of Proposition 1 and Proposition 2 (Section 2.4)

Before proceeding to proofs, we recall that the stabilizer subgroup \(G_{\mathbf{x}}\) of a group \(G\) for \(\mathbf{x}\) is defined as \(\{g^{\prime}\in G:g^{\prime}\cdot\mathbf{x}=\mathbf{x}\}\) and acts on a given group element \(g\in G\) through left multiplication \(g\mapsto g^{\prime}g\). For some \(g\in G\), by \(G_{\mathbf{x}}g\) we denote its orbit under the action by \(G_{\mathbf{x}}\), _i.e._, the set of elements in \(G\) to which \(g\) can be moved by the action of elements \(g^{\prime}\in G_{\mathbf{x}}\). Importantly, we can show the following:

**Property 1**.: _Any group \(G\) is a union of disjoint orbits \(G_{\mathbf{x}}g\) of equal cardinality._

Proof.: Let us consider the equivalence relation \(\sim\) on \(G\) induced by the action of the stabilizer \(G_{\mathbf{x}}\), defined as \(g\sim h\iff h\in G_{\mathbf{x}}g\). The orbits \(G_{\mathbf{x}}g\) are the equivalence classes under this relation, and the set of all orbits of \(G\) under the action of \(G_{\mathbf{x}}\) forms a partition of \(G\) (_i.e._, the quotient \(G/G_{\mathbf{x}}\)). Furthermore, since \(G_{\mathbf{x}}\leq G\) and right multiplication by some \(g\in G\) is a faithful action of \(G\) on itself, we have \(|G_{\mathbf{x}}g|=|G_{\mathbf{x}}|\) for all \(g\in G\), which shows that all orbits \(G_{\mathbf{x}}g\) have equal cardinality.

Figure 3: Visual illustration of the symmetrization methods based on probabilities assigned upon the partitioning of the group \(G\) into orbits \(G_{\mathbf{x}}g\). Note that, while we use concentric circles of different perimeters to illustrate each orbit, all orbits actually have an identical cardinality \(|G_{\mathbf{x}}g|=|G_{\mathbf{x}}|\).

The partition of group \(G\) into disjoint orbits \(G_{\mathbf{x}}g\) is illustrated in the first and second panel of Figure 3. We now show the following:

**Property 2**.: \(G\) _equivariant \(p_{\omega}(g|\mathbf{x})\) assigns identical probability to all elements on each orbit \(G_{\mathbf{x}}g\)._

Proof.: With equivariance, we have \(p_{\omega}(g|\mathbf{x})=p_{\omega}(g^{\prime}g|g^{\prime}\cdot\mathbf{x})\). Since \(g^{\prime}\cdot\mathbf{x}=\mathbf{x}\) for all \(g^{\prime}\in G_{\mathbf{x}}\), we have \(p_{\omega}(g|\mathbf{x})=p_{\omega}(g^{\prime}g|\mathbf{x})\) for all \(g^{\prime}\in G_{\mathbf{x}}\); all elements on orbit \(G_{\mathbf{x}}g\) have an identical probability. 

Property 2 characterizes probability distributions over \(G\) that can be expressed with \(p_{\omega}(g|\mathbf{x})\), which we illustrate in the third panel of Figure 3. Intuitively, \(p_{\omega}(g|\mathbf{x})\) assigns constant probability densities over each of the orbit \(G_{\mathbf{x}}g\) that partitions \(G\) as shown in Property 1. We now prove Proposition 1 and Proposition 2 by showing that \(p_{\omega}(g|\mathbf{x})\) can become frame and canonicalizer as special cases:

**Proposition 1**.: _Probabilistic symmetrization with \(G\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) can become frame averaging [74] by assigning uniform density to a set of orbits \(G_{\mathbf{x}}g\) for some group elements \(g\)._

Proof.: A frame is defined as a set-valued function \(F:\mathcal{X}\to 2^{G}\setminus\emptyset\) that satisfies \(G\) equivariance \(F(g\cdot\mathbf{x})=gF(\mathbf{x})\)[74]. For some frame \(F\), frame averaging is defined as follows:

\[\frac{1}{|F(\mathbf{x})|}\sum_{g\in F(\mathbf{x})}\left[g\cdot f_{\theta}(g^{- 1}\cdot\mathbf{x})\right],\] (40)

which can be equivalently written as the below expectation:

\[\mathbb{E}_{g\sim\text{Unif}(F(\mathbf{x}))}\left[g\cdot f_{\theta}(g^{-1} \cdot\mathbf{x})\right].\] (41)

From Theorem 3 of [74], we have that \(F(\mathbf{x})\) is a disjoint union of equal size orbits \(G_{\mathbf{x}}g\). Therefore, \(\text{Unif}(F(\mathbf{x}))\) is a uniform probability distribution over the union of the orbits. This can be expressed by a \(G\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) by assigning identical probability over all orbits in the frame \(F\) and zero probability to all orbits not in the frame (illustrated in the fourth panel of Figure 3). Therefore, probabilistic symmetrization can become frame averaging. 

**Proposition 2**.: _Probabilistic symmetrization with \(G\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) can become canonicalization [41] by assigning uniform density to a single orbit \(G_{\mathbf{x}}g\) of some group element \(g\)._

Proof.: A canonicalizer is defined as a (possibly stochastic) parameterized map \(C_{\omega}:\mathcal{X}\to G\) that satisfies relaxed \(G\) equivariance \(C_{\omega}(g\cdot\mathbf{x})=gg^{\prime}C_{\omega}(\mathbf{x})\) for some \(g^{\prime}\in G_{\mathbf{x}}\)[41]. For some canonicalizer \(C_{\omega}\), canonicalization is defined as follows:

\[g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x}),\quad g=C_{\omega}(\mathbf{x}).\] (42)

From relaxed \(G\) equivariance, we have \(C_{\omega}(\mathbf{x})=g^{\prime}C_{\omega}(\mathbf{x})\) for some \(g^{\prime}\in G_{\mathbf{x}}\). A valid choice for the canonicalizer \(C_{\omega}\) is a stochastic map that samples from the uniform distribution over a frame \(C_{\omega}(\mathbf{x})\sim\text{Unif}(F_{\omega}(\mathbf{x}))\) where the frame is assumed to always provide a single orbit \(F_{\omega}(\mathbf{x})=G_{\mathbf{x}}g\). In this case, canonicalization is equivalent to a 1-sample estimation of the below expectation:

\[\mathbb{E}_{g\sim\text{Unif}(F_{\omega}(\mathbf{x}))}\left[g\cdot f_{\theta}( g^{-1}\cdot\mathbf{x})\right].\] (43)

Furthermore, uniform distribution over the single-orbit frame \(\text{Unif}(F_{\omega}(\mathbf{x}))\) can be expressed by a \(G\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\) by assigning nonzero probability to the single orbit \(G_{\mathbf{x}}g\) and assigning zero probability to the rest (illustrated in the last panel of Figure 3). Therefore, probabilistic symmetrization can become canonicalization. 

### Extended Related Work (Continued from Section 2.4)

Our work draws inspiration from an extensive array of prior research, ranging from equivariant architectures and symmetrization to general-purpose deep learning with transformers. This section outlines a comprehensive review of these fields, spotlighting ideas specifically relevant to our work.

Equivariant ArchitecturesEquivariant architectures, defined by the group equivariance of their building blocks, have been a prominent approach for equivariant deep learning [12, 10]. These architectures have been primarily developed for data types associated with permutation and Euclidean group symmetries, including images [18, 19], sets, graphs, and hypergraphs [5, 57, 8], and geometric graphs [23, 84, 91]. Additionally, they have been extended to more general data types under arbitrary finite group [78] and matrix group symmetries [30]. However, they face challenges such as limited expressive power [101, 56, 64, 105, 40] and architectural issues like over-smoothing [70, 14, 69] and over-squashing [92] in graph neural networks. Our work aims to develop an equivariant deep learning approach that relies less on equivariant architectures, to circumvent these limitations and enhance parameter sharing and transfer across varying group symmetries.

SymmetrizationOur approach is an instance of symmetrization for equivariant deep learning which aims to achieve group equivariance using base models with unconstrained architectures. This is in general accomplished by averaging over specific group transformations of the input and output such that the averaged output exhibits equivariance. This allows us to leverage the expressive power of the base model _e.g._, achieve universal approximation using an MLP [35, 20] or a transformer [104], and potentially share or transfer parameters across different group symmetries. Existing literature has explored the choices of group transformations and base models for symmetrization. A straightforward approach is to average over the entire group [102], which is suitable for small, finite groups [4, 65, 42, 94] and requires sampling-based estimation for large groups such as permutations [67, 68, 88, 21]. Recent studies have attempted to identify smaller, input-dependent subsets of the group for averaging. Frame averaging [74] employs manually discovered set-values functions called frames, which still demand sampling-based estimation for certain worst-case inputs. Canonicalization [41] utilizes a single group transformation predicted by a neural network, but sacrifices strict equivariance. Our approach jointly achieves equivariance and end-to-end learning by utilizing parameterized, input-conditional equivariant distributions. Furthermore, our approach is one of the first demonstrations of symmetrization for the permutation group in real-world graph recognition task. Concerning the base model, previous work mostly examined small base models like an MLP or partial symmetrization of already equivariant models like GNNs. Few studies have explored symmetrizing pre-trained models for small finite groups [4, 3], and to our knowledge, we are the first to investigate symmetrization of a pre-trained standard transformer for permutation groups or any large group generally.

Transformer ArchitecturesA significant motivation of our work is to combine the powerful scaling and transfer capabilities of the standard transformer architecture [96] with equivariant deep learning. The transformer architecture has driven major breakthroughs in language and vision domains [96, 24, 13, 75], and proven its ability to learn diverse modalities [39, 38] or transfer knowledge across them [85, 54, 82, 25, 71, 52]. Although transformer-style architectures have been developed for symmetric data modalities like sets [51], graphs [103, 45, 43, 50, 66, 63], hypergraphs [17, 44], and geometric graphs [31, 55], they often require specific architectural modifications to achieve equivariance to the given symmetry group, compromising full compatibility with transformer architectures used in language and vision domains. Apart from a few studies on linguistic graph encoding with language models [79], we believe we are the first to propose a general framework that facilitates full compatibility of the standard transformer architecture for learning symmetric data. For example, we have shown that a pre-trained vision transformer could be repurposed to encode graphs.

Learning Distribution of Data AugmentationsSince our approach parameterizes a distribution \(p_{\omega}(g|\mathbf{x})\) on a group for symmetrization of form \(\phi_{\theta,\omega}(\mathbf{x})=\mathbb{E}_{g}[g\cdot f_{\theta}(g^{-1}\cdot \mathbf{x})]\) and learns it from data, one may find similarity to Augerino [7] and related approaches [77, 81, 95, 93, 80, 37] that learn distributions over data augmentations (_e.g._, \(p_{\omega}(g)\)) for a similar symmetrization. The key difference is that, while these approaches aim to discover underlying (approximate) symmetry constraint from data and searches over a space of different group symmetries, our objective aims to obtain an exact \(G\) equivariant symmetrization \(\phi_{\theta,\omega}(\mathbf{x})\) given the known symmetry group \(G\) of data (_e.g._, \(G=\mathrm{S}_{\mathrm{n}}\) for graphs). Because of this, the symmetrizing distribution has to be designed differently. In our case, we parameterize the distribution \(p_{\omega}(g|\mathbf{x})\) itself to be equivariant to a specific given group \(G\), while for augmentation learning approaches, the distribution \(p_{\omega}(g)\) is parameterized for a different purpose of covering a range of different group symmetry constraints and their approximations (_e.g._, a set of 2D affine transformations [7]). This leads to advantages of our approach if the symmetry group \(G\) is known, as **(1)** our approach can learn non-trivial and useful distribution \(p_{\omega}(g|\mathbf{x})\) per input data \(\mathbf{x}\) while keeping the symmetrized function \(\phi_{\theta,\omega}(\mathbf{x})\) exactly \(G\) equivariant, while augmentation learning does not guarantee equivariance for a given group in general and often has to reduce to trivial group averaging \(p_{\omega}=\text{Unif}(G)\) to be exactly \(G\) equivariant, and **(2)** while augmentation learning has to employ regularization [7] or model selection [37] to prevent collapse to trivial symmetry that is the least constrained and would fit the training data most easily [37], our approach fixes and enforces equivariance for the given symmetry group \(G\) by construction, which allows us to use regular maximum likelihood objective for training without the need to address symmetry collapse.

### Experimental Details (Section 3)

We provide details of the datasets and models used in our experiments in Section 3. The details of the datasets from the original papers [2, 1, 27, 28, 31, 84] can be found in Table 5 and Table 6.

a.3.1 Implementation Details of \(p_{\omega}\) for Symmetric Group \(\text{S}_{n}\) (Section 3.1, 3.3, 3.4)

In all experiments regarding the symmetric group \(\text{S}_{n}\), we implement the \(\text{S}_{n}\) equivariant distribution \(p_{\omega}(g|\mathbf{x})\), _i.e._, \(q_{\omega}:(\mathbf{x},\boldsymbol{\epsilon})\mapsto\mathbf{P}_{g}\) as a 3-layer GIN with 64 hidden dimensions [101] that has around 25k parameters. Specifically, given a graph \(\mathbf{x}\) with node features \(\mathbf{X}\in\mathbb{R}^{n\times d_{\text{u}}}\) and adjacency matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\),7 we first augment a virtual node [32] which is connected to all nodes to facilitate global interaction while retaining \(\text{S}_{n}\) equivariance, as follows:

Footnote 7: We do not utilize edge attributes in equivariant distribution \(p_{\omega}\), while we utilize them in base model \(f_{\theta}\).

\[\mathbf{X}^{\prime}=\left[\mathbf{X};\mathbf{v}\right],\quad\mathbf{A}^{\prime }=\left[\begin{array}{cc}\mathbf{A}&\mathbf{1}\\ \mathbf{1}^{\top}&0\end{array}\right],\] (44)

where the feature of the virtual node \(\mathbf{v}\in\mathbb{R}^{d_{\text{u}}}\) is a trainable parameter. Then, we prepared the input node features \(\mathbf{H}\in\mathbb{R}^{(n+1)\times d_{\text{u}}}\) to the GIN as \(\mathbf{H}=\mathbf{X}^{\prime}+\boldsymbol{\epsilon}\) where the noise \(\boldsymbol{\epsilon}\in\mathbb{R}^{(n+1)\times d_{\text{u}}}\) is i.i.d. sampled from \(\text{Unif}[0,\eta]\) with scale hyperparameter \(\eta\). Then, we employ following 3-layer GIN with 64 hidden dimensions to obtain processed node features \(\mathbf{H}^{\prime}\in\mathbb{R}^{(n+1)\times 1}\):

\[\mathbf{H}^{\prime}=\text{GINConv}_{64,64,1}\circ\text{GINConv}_{64,64}\circ \text{GINConv}_{d_{\text{u}},64,64}(\mathbf{H}),\] (45)

where each \(\text{GINConv}_{d_{1},d_{2},d_{3}}\) computes below with a two-layer elementwise MLP \(:\mathbb{R}^{n\times d_{1}}\rightarrow\mathbb{R}^{n\times d_{3}}\) with hidden dimension \(d_{2}\), ReLU activation, batch normalization, and trained scalar \(e\):

\[\mathbf{H}\mapsto\text{MLP}((\mathbf{A}^{\prime}+(1+e)\mathbf{I})\mathbf{H}).\] (46)

\begin{table}
\begin{tabular}{l c c c c} \hline Dataset & Size & Max \# Nodes & Average \# Nodes & Average \# Edges \\ \hline GRAPH8c & 11,117 & 8 & 8 & 28.82 \\ EXP-classify & 1,200 & 64 & 44.44 & 110.21 \\ \hline \(n\)-body & 7,000 & 5 & 5 & Fully Connected \\ \hline PATTERN & 14,000 & 188 & 117.47 & 4749.15 \\ \hline Peptides-func & & & & \\ Peptides-struct & 15,535 & 444 & 150.94 & 307.30 \\ \hline PCQM-Contact & 529,434 & 53 & 30.14 & 61.09 \\ \hline \end{tabular}
\end{table}
Table 6: Statistics of the datasets.

\begin{table}
\begin{tabular}{l c c c c} \hline Dataset & Symmetry & Domain & Task & Feat. (dim) \\ \hline GRAPH8c & \multirow{2}{*}{\(\text{S}_{n}\times\text{E}(3)\) Equivariant} & \multirow{2}{*}{Graph Isomorphism} & Graph Separation & \multirow{2}{*}{Adj. (1)} \\ EXP-classify & & & Graph Classification & \\ \hline \(n\)-body & \(\text{S}_{n}\times\text{E}(3)\) Equivariant & Physics & Position Regression & Pos. (3) + Vel. (3) + Charge (1) \\ \hline PATTERN & \(\text{S}_{n}\) Equivariant & Mathematical Modeling & Node Classification & Rand. Node Attr. (3) + Adj. (1) \\ \hline Peptides-func & \multirow{2}{*}{\(\text{S}_{n}\times\text{E}(3)\) Equivariant} & \multirow{2}{*}{Chemistry} & Graph Classification & \multirow{2}{*}{Atom (9) + Bond (3) + Adj. (1)} \\ Peptides-struct & & & Graph Regression & \\ \hline PCQM-Contact & \(\text{S}_{n}\) Equivariant & Quantum Chemistry & Link Prediction & Atom (9) + Bond (3) + Adj. (1) \\ \hline \end{tabular}
\end{table}
Table 5: Overview of the datasets.

Then, from the processed node features \(\mathbf{H}^{\prime}\in\mathbb{R}^{(n+1)\times 1}\), we finally obtain the features \(\mathbf{Z}\in\mathbb{R}^{n}\) for postprocessing by discarding the feature of the virtual node. Then, postprocessing into a permutation matrix is done with \(\text{argsort}:\mathbf{Z}\mapsto\mathbf{P}_{g}\in\mathbb{R}^{n\times n}\) as in Eq. (8).

TrainingTo backpropagate through \(\mathbf{P}_{g}\) for end-to-end training of \(p_{\omega}(g|\mathbf{x})\), we use straight-through gradient estimator [6] with an approximate permutation matrix \(\hat{\mathbf{P}}_{g}\approx\mathbf{P}_{g}\).8 For this, we first apply L2 normalization \(\mathbf{Z}\mapsto\bar{\mathbf{Z}}\) and use the below differentiable relaxation of the argsort operator [61, 33, 98]:

Footnote 8: In PyTorch [72], one can simply replace \(\mathbf{P}_{g}\) with \((\mathbf{P}_{g}-\hat{\mathbf{P}}_{g})\).detach() \(+\hat{\mathbf{P}}_{g}\) during forward passes.

\[\hat{\mathbf{P}}_{g}=S(-|\bar{\mathbf{Z}}\mathbf{1}^{\top}-\mathbf{1}\text{ sort}(\bar{\mathbf{Z}})^{\top}|/\tau),\] (47)

where \(S(\cdot/\tau)\) is Sinkhorn operator [61] with temperature hyperparameter \(\tau\in\mathbb{R}_{+}\) that performs elementwise exponential followed by iterative normalization of rows and columns. Following [61], we use 20 Sinkhorn iterations which worked robustly in all our experiments. For the correctness of straight-through gradients, it is desired that \(\hat{\mathbf{P}}_{g}\) closely approximates the real permutation matrix \(\mathbf{P}_{g}\) during training. For this, we choose the temperature \(\tau\) to be small, \(0.01\) in general, and following prior work [98], employ a regularizer on the mean of row- and column-wise entropy of \(\hat{\mathbf{P}}_{g}\) with a strength of \(0.1\) in all experiments. The \(\text{S}_{n}\) equivariance of the relaxed argsort \(\mathbf{Z}\mapsto\hat{\mathbf{P}}_{g}\) can be shown in a similar way to Proposition 3 from the fact that elementwise subtraction, absolute, scaling by \(-1/\tau\), exponential, and iterative normalization of rows and columns all commute with \(\mathbf{P}_{g^{\prime}}\in\text{S}_{n}\).

a.3.2 Implementation Details of \(p_{\omega}\) for Product Group \(\text{S}_{n}\times\text{E}(3)\) (Section 3.2)

In our \(n\)-body experiment on the product group \(\text{S}_{n}\times\text{E}(3)\), we implement the \(\text{S}_{n}\times\text{O}(3)\) equivariant distribution \(p_{\omega}(g|\mathbf{x}-\bar{\mathbf{x}}\mathbf{1}^{\top})\), _i.e._, \(q_{\omega}:(\mathbf{x}-\bar{\mathbf{x}}\mathbf{1}^{\top},\boldsymbol{\epsilon}) \mapsto(\mathbf{P}_{g},\mathbf{Q}_{g})\) based on a 2-layer Vector Neurons version of DGCNN with 96 hidden dimensions [23] that has around 7k parameters. Due to the architecture's complexity, we focus on describing input and output of the network and postprocessing, and guide the readers to the original paper [23] for further architectural details. In a high-level, the Vector Neurons receives position \(\mathbf{P}\in\mathbb{R}^{n\times 3}\) and velocity \(\mathbf{V}\in\mathbb{R}^{n\times 3}\) of the zero-centered input \(\mathbf{x}-\bar{\mathbf{x}}\mathbf{1}^{\top}\) with noises \(\boldsymbol{\epsilon}_{1},\boldsymbol{\epsilon}_{2}\in\mathbb{R}^{n\times 3}\) i.i.d. sampled from normal \(\mathcal{N}(0,\eta^{2})\) with scale hyperparameter \(\eta\), and produces features \(\mathbf{H}_{\text{S}_{n}}\in\mathbb{R}^{n\times 3\times d_{1}}\) and \(\mathbf{H}_{\text{O}(3)}\in\mathbb{R}^{n\times 3\times d_{2}}\) with \(d_{1}=1\) and \(d_{2}=3\) as follows:

\[\mathbf{H}_{\text{S}_{n}},\mathbf{H}_{\text{O}(3)}=\text{VN-DGCNN}(\mathbf{P}+ \boldsymbol{\epsilon}_{1},\mathbf{V}+\boldsymbol{\epsilon}_{2}).\] (48)

Then, we apply \(\text{O}(3)\) invariant pooling on \(\mathbf{H}_{\text{S}_{n}}\) and \(\text{S}_{n}\) invariant pooling on \(\mathbf{H}_{\text{O}(3)}\), both supported as a part of [23], to obtain features for postprocessing \(\mathbf{Z}_{\text{S}_{n}}\in\mathbb{R}^{n\times 1}\) and \(\mathbf{Z}_{\text{O}(3)}\in\mathbb{R}^{3\times 3}\), respectively:

\[\mathbf{Z}_{\text{S}_{n}}=\text{Pool}_{\text{O}(3)}(\mathbf{H}_{\text{S}_{n}}), \quad\mathbf{Z}_{\text{O}(3)}=\text{Pool}_{\text{S}_{n}}(\mathbf{H}_{\text{O} (3)}).\] (49)

Then, postprocessing with \(\text{argsort}:\mathbf{Z}_{\text{S}_{n}}\mapsto\mathbf{P}_{g}\in\mathbb{R}^{n \times n}\) and Gram-Schmidt orthogonalization \(\mathbf{Z}_{\text{O}(3)}\mapsto\mathbf{Q}_{g}\in\mathbb{R}^{3\times 3}\) is performed identically as described in the main text (Section 2.2). For the straight-through gradient estimation of the argsort operator, we use relaxed argsort described in Appendix A.3.1, with the only difference of using the temperature \(\tau=0.1\).

#### a.3.3 Graph Isomorphism Learning with MLP (Section 3.1)

Base Model \(f_{\theta}\)For EXP and EXP-classify, the model is given adjacency matrix \(\mathbf{A}\in\mathbb{R}^{64\times 64}\) and binary node features \(\mathbf{X}\in\mathbb{R}^{64}\) which are zero-padded to maximal number of nodes \(64\). For GRAPH8c, the input graphs are all of size \(8\) without node features, and the model is given adjacency matrix \(\mathbf{A}\in\mathbb{R}^{8\times 8}\). For EXP-classify, the prediction target is a scalar binary classification logit.

For the base model for EXP-classify, we use a 5-layer MLP \(f_{\theta}:\mathbb{R}^{64\times 64+64}\to\mathbb{R}\) on flattened and concatenated adjacency matrix and node features, with an identical architecture to other symmetrization baselines (MLP-GA and MLP-FA [74]) as in below:

\[f_{\theta}=\text{FC}_{1,10}\circ\text{FC}_{10,2048}\circ\text{FC}_{2048,4096} \circ\text{FC}_{4096,2048}\circ\text{FC}_{2048,4160},\] (50)

where \(\text{FC}_{d_{2},d_{1}}:\mathbb{R}^{d_{1}}\to\mathbb{R}^{d_{2}}\) denotes a fully-connected layer and ReLU activation is omitted. For EXP, we drop the last layer to obtain 10-dimensional output. For GRAPH8c, we use the following architecture \(f_{\theta}:\mathbb{R}^{8\times 8}\to\mathbb{R}^{10}\) that takes flattened adjacency to produce 10-dimensional output [74]:

\[f_{\theta}=\text{FC}_{10,64}\circ\text{FC}_{64,128}\circ\text{FC}_{128,64}.\] (51)TrainingFor EXP-classify, we train our models with binary cross-entropy loss using Adam optimizer [46] with batch size 100 and learning rate 1e-3 for 2,000 epochs, which takes around 30 minutes on a single RTX 3090 GPU with 24GB using PyTorch [72]. We additionally apply 200 epochs of linear learning rate warm-up and gradient norm clipping at 0.1, which we found helpful for stabilizing the training. For the equivariant distribution \(p_{\omega}\), we use noise scale \(\eta=1\). Since EXP and GRAPH8c concern randomly initialized models, we do not train the models for these tasks.

#### a.3.4 Particle Dynamics Learning with Transformer (Section 3.2)

Base Model \(f_{\theta}\)The model is given zero-centered position \(\mathbf{P}\in\mathbb{R}^{5\times 3}\) and velocity \(\mathbf{V}\in\mathbb{R}^{5\times 3}\) of 5 particles at a time point with pairwise charge difference \(\mathbf{C}\in\mathbb{R}^{5\times 5}\) and squared distance \(\mathbf{D}\in\mathbb{R}^{5\times 5}\). We set the prediction target as difference of position \(\Delta\mathbf{P}\in\mathbb{R}^{5\times 3}\) after a certain time.

For the base model, we use a 8-layer transformer encoder \(f_{\theta}:\mathbb{R}^{25\times 8}\rightarrow\mathbb{R}^{25\times 3}\) that operates on sequences of length \(25\) with dimension \(8\). At each prediction, we first organize the input into a single tensor \(\in\mathbb{R}^{5\times 5\times 8}\) by placing \(\mathbf{P}\) and \(\mathbf{V}\) on the diagonals of \(\mathbf{C}\) and \(\mathbf{D}\), and then turn the tensor into a sequence of \(25\) tokens \(\in\mathbb{R}^{25\times 8}\) by flattening the first two axes. Analogously, we organize the output of the model into a tensor \(\in\mathbb{R}^{5\times 5\times 3}\) and take the diagonal entries as the predictions. For the transformer, we use the standard implementation provided in PyTorch [72, 96], with \(64\) hidden dimensions, \(4\) attention heads, GELU activation [34] in feedforward networks, PreLN [100], learnable 1D positional encoding, and an MLP prediction head with 1 hidden layer. The model has around 208k trainable parameters, around 2.3\(\times\) compared to the GNN backbones of \(\mathrm{E}(3)\) symmetrization baselines in the benchmark (GNN-FA and GNN-Canonical.) with 92k parameters.

TrainingWe train our models with MSE loss using Adam optimizer [46] with batch size 100 and learning rate 1e-3 for 10,000 epochs, which takes around 8.5 hours on a single RTX 3090 GPU with 24GB using PyTorch [72]. We use weight decay with strength 1e-12 and dropout on the distribution \(p_{\omega}\) with probability 0.08. For the equivariant distribution \(p_{\omega}\), we use noise scale \(\eta=1\).

#### a.3.5 Graph Pattern Recognition with Vision Transformer (Section 3.3)

Base Model \(f_{\theta}\)The model is given adjacency matrix \(\mathbf{A}\in\mathbb{R}^{188\times 188}\) and node features \(\mathbf{X}\in\mathbb{R}^{188\times 3}\) zero-padded to maximal 188 nodes. The prediction target is node classification logits \(\mathbf{Y}\in\mathbb{R}^{188\times 2}\).

For the base model, we use a transformer with an identical architecture to ViT-Base [26] that operates on 224\(\times\)224 images with 16\(\times\)16 patch, using configuration from HuggingFace [99] model hub. We first remove the input patch projection and output head layers, which gives us a backbone transformer : \(\mathbb{R}^{(14\times 14)\times 768}\rightarrow\mathbb{R}^{(14\times 14) \times 768}\) on sequences of \((224/16)\times(224/16)=14\times 14\) tokens. Then, we use the following as the base model \(f_{\theta}:(\mathbf{A},\mathbf{X})\mapsto\mathbf{Y}\):

\[f_{\theta}(\mathbf{A},\mathbf{X})=\text{detokenize}\left(\text{transformer} \left(\text{tokenize}(\mathbf{A},\mathbf{X})\right)\right),\] (52)

where, for tokenize : \(\mathbb{R}^{188\times 188\times 1}\times\mathbb{R}^{188\times 3} \rightarrow\mathbb{R}^{(14\times 14)\times 768}\) we organize the input into a single tensor \(\in\mathbb{R}^{188\times 188\times 4}\) by placing \(\mathbf{X}\) on the diagonals of \(\mathbf{A}\) and apply 2D convolution with kernel size and stride 14, and for \(\text{detokenize}:\mathbb{R}^{(14\times 14)\times 768}\rightarrow\mathbb{R}^{188 \times 2}\) we apply transposed 2D convolution with kernel size and stride 14 to obtain a tensor \(\in\mathbb{R}^{188\times 188\times 2}\) and take its diagonal entries as output.

TrainingWe train our models with binary cross-entropy loss weighted inversely by class size [27] using AdamW [53] optimizer with batch size 128, learning rate 1e-5, and weight decay 0.01. We train the models for 25k steps under learning rate warm-up for 5k steps then linear decay to 0 with early stopping based on validation loss, which usually takes less than 5 hours on 8 RTX 3090 GPUs with 24GB using PyTorch Lightning [29]. For the equivariant distribution \(p_{\omega}\) we use noise scale \(\eta=1\) and dropout with probability 0.1. For probabilistic symmetrization that involves sampling-based estimation, we use sample size 1 for training. For group averaging, sample size 1 for training led to optimization challenges, and therefore we use sample size 10 for training which yielded better results.

#### a.3.6 Real-World Graph Learning with Vision Transformer (Section 3.4)

Base Model \(f_{\theta}\)For Peptides-func/struct, the model is given adjacency matrix \(\mathbf{A}\in\mathbb{R}^{444\times 44}\), node features \(\mathbf{X}\in\mathbb{R}^{444\times 64}\), and edge features \(\mathbf{E}\in\mathbb{R}^{444\times 444\times 7}\), zero-padded to maximal 444 nodes. The prediction target is binary classification logits \(\mathbf{Y}\in\mathbb{R}^{10}\) for Peptides-func, and regression targets\(\mathbf{Y}\in\mathbb{R}^{11}\) for Peptides-struct. For PCQM-Contact, the model is given adjacency matrix \(\mathbf{A}\in\mathbb{R}^{53\times 53}\), node features \(\mathbf{X}\in\mathbb{R}^{53\times 68}\), and edge features \(\mathbf{E}\in\mathbb{R}^{53\times 53\times 6}\), zero-padded to maximal 53 nodes. The prediction target is binary edge classification logit \(\mathbf{Y}\in\mathbb{R}^{53\times 53\times 1}\).

For the base model, we use a transformer with an identical architecture to ViT-Base that operates on \(14\times 14\) tokens, same as in Appendix A.3.5. For Peptides-func and Peptides-struct, we use the following as the base model \(f_{\theta}:(\mathbf{A},\mathbf{X},\mathbf{E})\mapsto\mathbf{Y}\):

\[f_{\theta}(\mathbf{A},\mathbf{X},\mathbf{E})=\text{detokenize}_{\text{[cls ]}}\left(\text{transformer}\left(\text{tokenize}_{\text{2D}}(\mathbf{A}, \mathbf{E})+\text{tokenize}_{\text{1D}}(\mathbf{X})\right)\right),\] (53)

where \(\text{tokenize}_{\text{2D}}:\mathbb{R}^{444\times 444\times(1+7)}\to \mathbb{R}^{(14\times 14)\times 768}\) is 2D convolution with kernel size and stride 32, \(\text{tokenize}_{\text{1D}}:\mathbb{R}^{444\times 64}\to\mathbb{R}^{196\times 768}\) is 1D convolution with kernel size and stride 3, and \(\text{detokenize}_{\text{[cls]}}\) performs linear projection of the global [cls] token [26] to the target dimensionality. For PCQM-Contact, we use the following as base model \(f_{\theta}:(\mathbf{A},\mathbf{X},\mathbf{E})\mapsto\mathbf{Y}\):

\[f_{\theta}(\mathbf{A},\mathbf{X},\mathbf{E})=\text{detokenize}_{\text{2D}} \left(\text{transformer}\left(\text{tokenize}_{\text{2D}}(\mathbf{A}, \mathbf{E})+\text{tokenize}_{\text{1D}}(\mathbf{X})\right)\right),\] (54)

where \(\text{tokenize}_{\text{2D}}:\mathbb{R}^{53\times 53\times(1+6)}\to \mathbb{R}^{(14\times 14)\times 768}\) is 2D convolution with kernel size and stride 4, \(\text{tokenize}_{\text{1D}}:\mathbb{R}^{53\times 64}\to\mathbb{R}^{196\times 768}\) is 1D convolution with kernel size and stride 1, and \(\text{detokenize}_{\text{2D}}:\mathbb{R}^{(14\times 14)\times 768}\to\mathbb{R}^{53 \times 53\times 1}\) is transposed 2D convolution with kernel size and stride 4.

TrainingWe train our models with cross-entropy for classification and L1 loss for regression using AdamW [53] optimizer with batch size 128, learning rate 1e-5 except for PCQM-Contact where we use 5e-5, and weight decay 0.01. We train the models for 50k steps under learning rate warm-up for 5k steps then linear decay to 0 with early stopping based on validation loss, which usually takes less than 12 hours on 8 RTX 3090 GPUs with 24GB using PyTorch Lightning [29]. For the equivariant distribution \(p_{\omega}\), we use noise scale \(\eta=1\), and use dropout with probability 0.1 except for PCQM-Contact where we do not use dropout. We use 10 samples for estimation during training.

### Supplementary Experiments (Continued from Section 3)

In this section, we present additional experimental results that supplement the experiments in Section 3 but could not be included in the main text due to space constraints.

#### a.4.1 Graph Isomorphism Learning (Section 3.1)

In our experiments on graph isomorphism learning in Section 3.1, we mainly experimented for S\({}_{n}\) symmetrization of an MLP. Here, we provide supplementary results on S\({}_{n}\) symmetrization of a GIN base model with node identifiers, following [74]. The results can be found in Table 7. In accordance with Section 3.1, our approach successfully performs S\({}_{n}\) symmetrization of GIN-ID.

\begin{table}
\begin{tabular}{l c c c c} \hline method & arch. & sym. & GRAPH8c \(\downarrow\) & EXP \(\downarrow\) & EXP-classify \(\uparrow\) \\ \hline GIN-ID-GA & - & S\({}_{n}\) & 0 & 0 & 50\% \\ GIN-ID-FA & - & S\({}_{n}\) & 0 & 0 & **100\%** \\ GIN-ID-Canonical. & - & S\({}_{n}\) & 0 & 0 & 84\% \\ GIN-ID-PS (Ours) & - & S\({}_{n}\) & 0 & 0 & **100\%** \\ \hline \end{tabular}
\end{table}
Table 7: Supplementary results for S\({}_{n}\) invariant graph separation with S\({}_{n}\) symmetrized GIN-ID base function. Baseline scores for GIN-ID-GA and GIN-ID-FA are taken from [74].

\begin{table}
\begin{tabular}{l c c c} \hline method & arch. & sym. & Position MSE \(\downarrow\) \\ \hline GNN-FA & S\({}_{n}\) & E\((3)\) & 0.0057 \\ GNN-Canonical. & S\({}_{n}\) & E\((3)\) & 0.0043 \\ GNN-Canonical. (Reproduced) & S\({}_{n}\) & E\((3)\) & 0.00457 \\ GNN-GA & S\({}_{n}\) & E\((3)\) & 0.00408 \(\pm\) 0.00002 \\ GNN-PS (Ours) & S\({}_{n}\) & E\((3)\) & **0.00386 \(\pm\) 0.00001** \\ \hline \end{tabular}
\end{table}
Table 8: Supplementary results for S\({}_{n}\)\(\times\) E\((3)\) equivariant \(n\)-body with E\((3)\) symmetrized GNN base function. Baseline scores for GNN-FA and GNN-Canonical. are from [74] and [41], respectively.

#### a.4.2 Particle Dynamics Learning (Section 3.2)

In our experiments on \(n\)-body dataset in Section 3.2, we experimented for \(\text{S}_{n}\times\text{E}(3)\) symmetrization using a 1D sequence transformer architecture which has \(2.3\times\) parameters compared to baselines. To provide parameter-matched comparison against baselines in literature, we apply our approach for \(\text{E}(3)\) symmetrization of \(\text{S}_{n}\) equivariant GNN base model that is widely used in literature [74; 41]. We faithfully follow [74; 41] on the experimental setups including training hyperparameters and the configuration of GNN base model, and only add \(\text{E}(3)\) equivariant distribution \(p_{\omega}(g|\mathbf{x}-\bar{\mathbf{x}}\mathbf{1}^{\top})\), _i.e._, \(q_{\omega}:(\mathbf{x}-\bar{\mathbf{x}}\mathbf{1}^{\top},\mathbf{\epsilon}) \mapsto\mathbf{Q}_{g}\) by utilizing the 2-layer Vector Neurons architecture described in Appendix A.3.2 using only its \(\text{O}(3)\) prediction head. We use 20 samples for training and testing. The results can be found in Table 8. In accordance with the results in Section 3.2, our approach outperforms other symmetrization approaches and achieves a new state-of-the-art of 0.00386 MSE.

#### a.4.3 Effect of Sample Size on Training and Inference

In this section, we provide additional analysis on how the sample size for estimation of symmetrized function (Eq. (4)) affects training and inference. We use the experimental setup of EXP-classify (Section 3.1; \(\text{S}_{n}\) invariance) and analyze the behavior of MLP-PS with identical initialization and hyperparameters, only controlling sample sizes \(\in\{1,2,5,10,20,50\}\) for training. Specifically, we analyze **(1)** variance of permutation matrices \(\mathbf{P}_{g}\sim p_{\omega}(g|\mathbf{x})\) measured indirectly by the entropy of their aggregation \(\bar{\mathbf{P}}=\sum\mathbf{P}_{g}/N\) as in Section 3.1, **(2)** sample variance of the unbiased estimator \(g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x})\) of the symmetrized function \(\phi_{\theta,\omega}(\mathbf{x})\) as in Eq. (4), and **(3)** sample mean and variance

Figure 4: Test accuracy of MLP \(f_{\theta}\) symmetrized by equivariant distribution \(p_{\omega}(g|\mathbf{x})\) trained on EXP-classify dataset across a range of training sample sizes. Inference sample size is set to 10.

Figure 5: Row- and column-wise entropy of aggregated permutation matrices \(\mathbf{P}_{g}\sim p_{\omega}(g|\mathbf{x})\) after trained on EXP-classify dataset across a range of training sample sizes. Dashed line indicates entropy measured with random permutation matrices from \(\text{Unif}(G)\).

of the estimated task loss \(\mathcal{L}(\mathbf{y},g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x}))\) where \(\mathcal{L}\) is binary cross entropy. All measurements are repeated 100 times and averaged over the inputs and labels \((\mathbf{x},\mathbf{y})\) of the validation dataset.

Observations are as follows. First, models trained with smaller sample sizes need more iterations to converge, but after sufficient training (2000 epochs), all achieve \(>95\%\) test accuracy when evaluated with 10 samples (Figure 4). Second, models trained with smaller sample sizes tend to be more sample efficient, _i.e._, tend to perform a lower variance estimation. Their distribution \(p_{\omega}(g|\mathbf{x})\) tend to learn more low-variance permutations (Figure 5), and the models tend to learn low-variance estimation of output and loss (left and center panels of Figure 6). This indicates that small sample size may serve as a regularizer that encourages lower variance of the estimator. However, this regularization effect is not always beneficial in terms of task loss (right panel of Figure 6), as training sample size 1 achieves a poor task loss for all sample sizes presumably due to the optimization challenge caused by over-regularization. In other words, the sample size for training introduces a tradeoff; a small sample size takes more training iterations to converge, but serves as a regularizer that encourages lower variance of the estimator and thus a better inference time sample efficiency. On the other hand, larger sample sizes for inference consistently benefits all models (Figure 6).

Interestingly, this observed tendency is consistent with the theoretical claims in literature [67; 68] on the sampling based training of symmetrized models, which we reprise here. When training the symmetrized model \(\phi_{\theta,\omega}(\mathbf{x})\) in Eq. (4), we cannot directly observe \(\phi_{\theta,\omega}(\mathbf{x})\), but observe samples of its unbiased estimator \(g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x})\). Thus, it can be questionable what objective we are actually optimizing during the sampling-based training. Based on [67; 68], it turns out that minimizing a convex loss function \(\mathcal{L}\) on the estimated output \(g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x})\) is equivalent to minimizing an upper bound to the true objective on the symmetrized output \(\phi_{\theta,\omega}(\mathbf{x})\). This is because our estimation is no longer unbiased when computing loss, as we have the following from Jensen's inequality:

\[\mathbb{E}_{p_{\omega}(g|\mathbf{x})}[\mathcal{L}(\mathbf{y},g\cdot f_{\theta} (g^{-1}\cdot\mathbf{x}))]\geq\mathcal{L}(\mathbf{y},\mathbb{E}_{p_{\omega}(g |\mathbf{x})}[g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x})])=\mathcal{L}(\mathbf{ y},\phi_{\theta,\omega}(\mathbf{x})).\] (55)

That is, minimizing the sampling-based loss is minimizing an upper-bound surrogate to the true objective. It has been claimed that optimizing this upper bound has an implicit low-variance regularization effect [67; 68], which is consistent with our observations. This also roughly explains why our distribution \(p_{\omega}(g|\mathbf{x})\) does not collapse to uniform distribution although we do not impose any low-variance regularization explicitly; training to directly minimize the task loss with samples implicitly nudges the distribution towards low-variance solutions.

#### a.4.4 Additional Comparison to Group Averaging

In this section, we provide additional analysis of our approach in comparison to sampling-based group averaging in terms of sample variance and convergence. We use the experimental setup of EXP-classify (Section 3.1; S\({}_{n}\) invariance) and experiment with MLP-PS and MLP-GA.

We first analyze whether using the equivariant distribution \(p_{\omega}(g|\mathbf{x})\) for symmetrization offers a lower variance estimation, _i.e._, a better sample efficiency, compared to group averaging with \(\text{Unif}(G)\). This supplements the results in Figure 2 that \(p_{\omega}(g|\mathbf{x})\) learns to produce low-variance permutations compared to \(\text{Unif}(G)\). Specifically, we fix a randomly initialized MLP \(f_{\theta}\) and symmetrize it using our approach and group averaging. We then measure **(1)** the sample variance of the unbiased estimator

Figure 6: Variance of estimation of MLP \(f_{\theta}\) symmetrized by equivariant distribution \(p_{\omega}(g|\mathbf{x})\) and trained on EXP-classify dataset for a range of training and inference sample sizes.

\(g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x})\) of the symmetrized function (Eq. (1) and Eq. (4)), and **(2)** the sample variance of the estimated task loss \(\mathcal{L}(\mathbf{y},g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x}))\) where \(\mathcal{L}\) is binary cross entropy. All measurements are repeated 100 times and averaged over the inputs and labels \((\mathbf{x},\mathbf{y})\) of the validation dataset. The results are in Figure 7, showing that symmetrization with equivariant distribution \(p_{\omega}(g|\mathbf{x})\) consistently offers a lower variance estimation than group averaging across inference sample sizes.

In addition, we analyze whether the equivariant distribution \(p_{\omega}(g|\mathbf{x})\) for symmetrization offers more stable gradients for the base function \(f_{\theta}\) during training compared to group averaging, as conjectured in Section 2. For this, we fix a randomly initialized MLP \(f_{\theta}\) and symmetrize it using our approach and group averaging. For every few training epochs, we measure the full gradient of the task loss over the entire training dataset with respect to the parameters of the base MLP \(f_{\theta}\). This averages out the variance from individual data points and provides the net direction of the gradient on the base function offered by \(p_{\omega}(g|\mathbf{x})\) or \(\text{Unif}(G)\). The results are in Figure 8, showing that that symmetrization with equivariant distribution \(p_{\omega}(g|\mathbf{x})\) offers a consistently larger magnitude of the net gradient, while group averaging with \(\text{Unif}(G)\) leads to near-zero net gradients. This indicates, for \(\text{Unif}(G)\), the gradients from each training data instances are oriented in a largely divergent manner and therefore the training signal is collectively not very informative, while using \(p_{\omega}(g|\mathbf{x})\) for symmetrization leads to more consistent gradient across training data instances, _i.e._, it offers a more stable training signal.

#### a.4.5 Additional Comparison to Canonicalization

In this section, we provide additional analysis of our approach in comparison to canonicalization [41] that uses a single group element \(g\) from an equivariant canonicalizer \(C_{\omega}:\mathbf{x}\mapsto\rho(g)\). The main claim is that there always exist certain inputs that canonicalization fails to guarantee exact \(G\) equivariance, while our approach guarantees equivariance for all inputs in expectation as in Theorem 1.

Figure 8: Norm of full gradient over training epochs with respect to the parameters of an identically initialized MLP symmetrized by equivariant distribution (PS) and uniform distribution (GA).

More specifically, let us recall the definition of \(G\) equivariant canonicalizer from [41]. A canonicalizer \(C_{\omega}\) is \(G\) equivariant if \(C_{\omega}(g\cdot\mathbf{x})=\rho(g)C_{\omega}(\mathbf{x})\) for all \(g\in G\) and \(\mathbf{x}\in\mathcal{X}\). Consider an input \(\mathbf{x}\) which has a non-trivial stabilizer \(G_{\mathbf{x}}=h\in G|h\cdot\mathbf{x}=\mathbf{x}\), _i.e._, has inner symmetries. It can be shown that equivariant canonicalizers are ill-defined for these inputs. To see this, let \(g_{1}=gh_{1}\) and \(g_{2}=gh_{2}\) for some \(g\in G\) and any \(h_{1},h_{2}\in G_{\mathbf{x}}\) where \(h_{1}\neq h_{2}\). Then we have \(C_{\omega}(g_{1}\cdot\mathbf{x})=C_{\omega}(gh_{1}\cdot\mathbf{x})=C_{\omega} (g\cdot\mathbf{x})=C_{\omega}(gh_{2}\cdot\mathbf{x})=C_{\omega}(g_{2}\cdot \mathbf{x})\), implying that \(\rho(g_{1})C_{\omega}(\mathbf{x})=\rho(g_{2})C_{\omega}(\mathbf{x})\). Since \(g_{1}\neq g_{2}\), this contradicts the group axiom, and thus an equivariant canonicalizer cannot exist for inputs with non-trivial inner-symmetries. To handle all inputs, canonicalization [41] adopts relaxed equivariance: a canonicalizer \(C_{\omega}\) satisfies relaxed equivariance if \(C_{\omega}(g\cdot\mathbf{x})=\rho(gh)C_{\omega}(\mathbf{x})\) up to arbitrary action from the stabilizer \(h\in G_{\mathbf{x}}\). As a result, the symmetrization \(\phi_{\theta,\omega}(\mathbf{x})=g\cdot f_{\theta}(g^{-1}\cdot\mathbf{x})\) performed using a relaxed canonicalizer \(C_{\omega}\) only guarantees relaxed equivariance \(\phi_{\theta,\omega}(g\cdot\mathbf{x})=gh\cdot\phi_{\theta,\omega}(\mathbf{x})\) up to arbitrary action from the stabilizer \(h\in G_{\mathbf{x}}\) (Theorem A.2 of [41]). In other words, canonicalization does not guarantee equivariance for inputs with inner symmetries.

To visually demonstrate this, we perform a minimal experiment using several graphs \(\mathbf{x}\) with non-trivial stabilizers \(G_{\mathbf{x}}\), _i.e._, inner symmetries, taken from [90]. We fix a randomly initialized MLP

Figure 11: A graph \(\mathbf{x}\) with stabilizer subgroup \(G_{\mathbf{x}}\cong\mathrm{D}_{4}\).

\(f_{\theta}:\mathbb{R}^{n\times n}\rightarrow\mathbb{R}^{n}\) and symmetrize it using our approach and canonicalization. When symmetrized, the MLP is expected to provide scalar embedding of each node, which we color-code for visualization. The results are in Figures 9, 10, and 11. For each graph, we illustrate three panels: the leftmost one illustrates the color-coding of the inner symmetry of nodes (automorphism), the middle one illustrates node embedding from MLP-PS, and the rightmost one illustrates embedding from MLP-Canonical. If a method is \(G\) equivariant, it is expected to give identical embeddings for automorphic nodes, since an equivariant model cannot distinguish them in principle [88]. As in the Figures 9, 10, and 11, in the presence of inner symmetry (left panels), MLP with probabilistic symmetrization (middle panels) achieves \(G\) equivariance and produces close embeddings for automorphic nodes. However, the same MLP with canonicalization produces relatively unstructured embeddings (right panels). The result illustrates a potential advantage of probabilistic symmetrization over canonicalization when learning data with inner symmetries, which is often found in applications such as molecular graphs [60].

### Limitations and Broader Impacts (Continued from Section 4)

While the equivariance, universality, simplicity, and scalability of our approach offers a potential for positive impact for deep learning for chemistry, biology, physics, and mathematics, it also has limitations and potential negative impacts. The main limitation of our work is that it trades off certain desirable traits in equivariant deep learning in favor of achieving architecture agnostic equivariance. For example, **(1)** our approach is less interpretable compared to equivariant architectures due to less structured computations in the base model, **(2)** our approach is presumably less parameter and data efficient compared to equivariant architectures due to less imposed prior knowledge on parameterization, and **(3)** our approach is expected to be challenged when input size generalization is required, partially because the maximum input size has to be specified in advance. Another genuine weakness compared to canonicalization is that, our method is stochastic and therefore incurs \(\mathcal{O}(N)\) cost when using \(N\) samples for estimation. These limitations might lead to potential negative environmental impacts, since less interpretability and lower efficiency implies higher reliance on larger models with more computation cost. We acknowledge the aforementioned limitations and impacts of our work, and will make effort to address them in follow-up research. For example, we believe data efficiency of our approach could improve with pretrained knowledge transfer from other domains, as it would impose a strong prior on the hypothesis space and may work similarly to architectural priors that benefit data efficiency. Also, for the sampling cost, since the sampling is completely parallelizable and analogous to using a larger batch size, we believe it can be overcome to some degree by leveraging parallel computing techniques developed for scaling batch size.