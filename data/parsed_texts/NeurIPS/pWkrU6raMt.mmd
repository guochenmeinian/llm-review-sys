# SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking

 Soukayna Mouatadid1, Paulo Orenstein2, Genevieve Flaspohler3, Miruna Oprescu4,

**Judah Cohen3, Franklyn Wang5, Sean Knight3, Maria Geogdzhayeva3, Sam Levang6, Ernest Fraenkel3, Lester Mackey4**

1University of Toronto, 2IMPA, 3MIT, 4Microsoft Research, 5Harvard, 6Salient Predictions

###### Abstract

Subseasonal forecasting of the weather two to six weeks in advance is critical for resource allocation and advance disaster notice but poses many challenges for the forecasting community. At this forecast horizon, physics-based dynamical models have limited skill, and the targets for prediction depend in a complex manner on both local weather and global climate variables. Recently, machine learning methods have shown promise in advancing the state of the art but only at the cost of complex data curation, integrating expert knowledge with aggregation across multiple relevant data sources, file formats, and temporal and spatial resolutions. To streamline this process and accelerate future development, we introduce \(\mathsf{SubseasonalClimateUSA}\), a curated dataset for training and benchmarking subseasonal forecasting models in the United States. We use this dataset to benchmark a diverse suite of subseasonal models, including operational dynamical models, classical meteorological baselines, and ten state-of-the-art machine learning and deep learning-based methods from the literature. Overall, our benchmarks suggest simple and effective ways to extend the accuracy of current operational models. \(\mathsf{SubseasonalClimateUSA}\) is regularly updated and accessible via the https://github.com/microsoft/subseasonal_data/ Python package.

## 1 Introduction

Weather and climate forecasting are fundamental scientific problems with many applications, including agriculture, energy grids, transportation and disaster prevention [36, 71, 55]. Indeed, short-term and long-term operational forecasts are critically employed in many sectors of our society and the world economy. However, skillful forecasts for the subseasonal regime--that is, 2 to 6 weeks ahead--still present operational challenges due to the chaotic nature of the weather [32] and to the interaction of weather and climate variables operating at different spatial and temporal scales [66].

In recent years, these challenges have spurred intense activity from both the meteorological and machine learning communities. On the one hand, steady advances have extended the reach of physics-based dynamical models of the atmosphere and oceans into the subseasonal realm [64, 50, 29]. On the other, parallel efforts from the machine learning community have led to improved predictive skill through new models trained on historical observational data and dynamical model forecasts [31, 67, 68, 69, 23, 2, 19, 78, 69, 39].

Nevertheless, developing and benchmarking new subseasonal models remains challenging due to a lack of standardized, curated datasets targeting this forecast horizon. The data necessary for subseasonal predictions are often collected from multiple data sources, each with its own data processing pipeline, and then standardized into a common spatial and temporal resolution. Because there is uncertainty regarding the main drivers of subseasonal phenomena, domain experts are typically needed to determine which features are likely to carry signal, and different developersemploy different aggregation techniques over time and space. Finally, the changing nature of weather makes the forecasting task hard to compare across different regions and years. This, in turn, has spurred several subseasonal forecasting challenges to benchmark existing solutions [47, 46, 65].

In this paper, we introduce \(\mathsf{SubseasonalClimateUSA}\), a diverse collection of ground-truth measurements and dynamical forecasts for subseasonal prediction over the contiguous United States (U.S.). We include spatiotemporal measurements with known subseasonal impact (including, e.g., temperature, precipitation, sea surface temperature, and geopotential height); the states of known subseasonal drivers such as El Nino-Southern Oscillation (ENSO) and the Madden-Julian oscillation (MJO); and dynamical predictions for temperature and precipitation from eight operational models including the U.S. Climate Forecast System version 2 (CFSv2) and the leading subseasonal model from the European Centre for Medium-Range Weather Forecasts (ECMWF). The dataset is regularly updated and accessible via the open-source \(\mathsf{subseasonal\_data}\) Python package for easy retrieval.

We then use the \(\mathsf{SubseasonalClimateUSA}\) data to benchmark a wide range of subseasonal models, highlighting their strengths and weaknesses. These models include traditional meteorological benchmarks (e.g., Persistence and Climatology), operational dynamical models (e.g., CFSv2 and ECMWF), and ten state-of-the-art deep learning (e.g., N-BEATS [48] and Informer [79]) and machine learning (e.g., CFSv2++ [39] and Prophet [62]) forecasters. Two of these models (Salient 2.0, the best-performing deep learning method, and LocalBoosting) were new creations of this work, and nine required new subseasonal forecasting implementations now available via the \(\mathsf{subseasonal\_toolkit}\) Python package. Model performance is measured for four standard subseasonal tasks: predicting average temperature 3-4 and 5-6 weeks ahead and predicting accumulated precipitation 3-4 and 5-6 weeks ahead. They are evaluated in terms of accuracy (measured by spatial root mean squared error) and skill (measured by uncentered anomaly correlation), over the years 2011-2020. Overall, we find that the simplest learned models typically outperform the meteorological baselines, the leading operational models, and the remaining learning methods. Additionally, we show that ensembling different methods through online learning leads to further gains in terms of both accuracy and skill.

Our aims in releasing \(\mathsf{SubseasonalClimateUSA}\) are twofold. First, we aim to facilitate the development of skillful learning-based subseasonal forecasting models by providing a comprehensive, standardized, and machine-learning-friendly dataset. Second, we aim to provide standardized benchmarks for subseasonal forecasting progress. To this end, we define four core subseasonal prediction tasks that users can use as benchmarking targets: forecasting (i) temperature in weeks 3-4, (ii) temperature in weeks 5-6, (iii) precipitation in weeks 3-4, and (iv) precipitation in weeks 5-6. Significant advances in any of these tasks would have significant implications for the allocation of water resources, agricultural production, and disaster relief [47, 71].

**Related Work** There are several datasets available for benchmarking weather models. For example, both the National Oceanic and Atmospheric Administration (NOAA) and the ECMWF provide reanalysis datasets tracking weather variables from the whole globe from the 1940s until today [24, 21], and global model simulations can be found in datasets provided by the World Climate Research Programme [13] and ECMWF [6]. More recently, several new datasets have been made available targeting specific AI applications in weather. For instance, classifying clouds [53], studying storm morphology [18] and nowcasting [16], predicting tropical cyclone intensity [35] and air quality metrics [4], and analyzing watershed-scale hydrometeorological time series [1] and river flows [17]. There are also more general-purpose datasets, such as WeatherBench [52], which provides a benchmark for forecasting different medium-range weather variables 3 to 5 days out. For a general overview of weather datasets for machine learning, see [11].

While these datasets have helped advance weather prediction in different tasks, there are no general datasets specifically targeting the subseasonal scale for the U.S. There have been instead several competitions targeting this lead time including the U.S. Bureau of Reclamation (USBR) Sub-Seasonal Climate Forecast Rodeos [47, 46] and the World Meteorological Organization Seasonal-to-Subseasonal (S2S) Artificial Intelligence (AI) Challenge [65]. Furthermore, the SubX Experiment [50] also makes a series of subseasonal models available for benchmarking (which are included in the \(\mathsf{SubseasonalClimateUSA}\) dataset). Finally, the precursor of this work, the SubseasonalRodeo dataset of [23], targets only the Western U.S., offers only a static data snapshot ending in 2018, provides no forecasts from the leading subseasonal dynamical model (ECMWF), and includes only coarse-grained (monthly) forecasts from the North American Multi-Model Ensemble, with limited utility for weekly or biweekly forecasting.

In contrast, SubseasonalClimateUSA is a modern, regularly updated resource targeting the contiguous U.S. with granular (daily and subweekly) forecasts from ECMWF and seven other operational dynamical models in the SubX consortium [25]. Notably, both the present work and past studies have found complementary predictive signals in physics-based dynamical model forecasts and pure observational data that can lead to better forecasts than either data source alone [see, e.g., 23, 39]. In fact, recent work has demonstrated that even the least skill operational dynamical models can produce forecasts with skill comparable to the best when corrected suitably with observational data [39]. As a result, we have endeavored to include both granular measurements and granular model forecasts in the SubseasonalClimateUSA dataset to best equip future model developers, researchers, and forecasters.

## 2 The SubseasonalClimateUSA dataset

The SubseasonalClimateUSA dataset houses a diverse collection of ground-truth measurements and dynamical model forecasts relevant to forecasting at subseasonal timescales. The dataset is regularly updated, CC BY 4.0 licensed, accessible via the open-source subseasonal_data Python package, and documented at the URL https://github.com/microsoft/subseasonal_data/blob/main/DATA.md. We summarize dataset contents, sources, and processing steps below and provide supplementary details in Appendix A.

**Data Collection and Processing** Figure 1 summarizes the SubseasonalClimateUSA data collection and processing pipeline. The pipeline collects raw data from seven meteorological data sources (contributing different variables, resolutions, and file formats), passes all data through a common preprocessing pipeline, and outputs a standardized collection of machine-learning-ready Python Pandas DataFrames and Series objects stored in HDF5 format. Each file contributes data variables falling into one of three categories: (i) spatial (varying with the target grid point but not the target date); (ii) temporal (varying with the target date but not the target grid point); (iii) spatiotemporal (varying with both the target grid point and the target date). Data representing ground-truth measurements are typically downloaded daily on \(0.25^{\circ}\) or \(0.5^{\circ}\) latitude-longitude grids. However, subseasonal forecasts are typically issued on coarser \(1^{\circ}\) or \(1.5^{\circ}\) grids and averaged over two-week periods [47, 46, 65]. As a result, unless otherwise noted below, temporal and spatiotemporal variables arising from daily data sources were derived by averaging input values over a \(14\)-day rolling window, and spatial and spatiotemporal variables were derived by interpolating input data to a \(1^{\circ}\) latitude-longitude grid and retaining only the grid points belonging to the contiguous U.S. To accommodate ECMWF forecasts, which were only made available on a \(1.5^{\circ}\) latitude-longitude grid [12], we additionally download SubX forecasts at \(1.5^{\circ}\) resolution and interpolate temperature and precipitation onto the same grid.

Figure 1: Schematic of the SubseasonalClimateUSA data collection and processing pipeline.

Following the protocol adopted by the USBR Sub-Seasonal Climate Forecast Rodeos [47, 46], our temperature and precipitation variables are interpolated onto fixed \(1^{\circ}\times 1^{\circ}\) (NUM_LAT=181, NUM_LON=360) and \(1.5^{\circ}\times 1.5^{\circ}\) (NUM_LAT=121, NUM_LON=240) grids using the NCAR Command Language function area_hi2lores_Wrap with arguments new_lat = latGlobeF(NUM_LAT, "lat", "latitude", "degrees_north"): new_lon = lonGlobeF(NUM_LON, "1on", "longitude", "degrees_east"); wgt = cos(lat*pi/180.0) (so that points are weighted by the cosine of the latitude in radians); opt@critpc = 50 (to require only 50% of the values to be present to interpolate); and fiCyclic = True (indicating global data with longitude values that do not quite wrap around the globe). All remaining spatial and spatiotemporal variables are interpolated using the Climate Data Operators operator remapdis (distance-weighted average interpolation) with target grid r360x181.

**Data Features** The variables comprising the Subseasonal ClimateUSA dataset include:

* **Temperature** (global and U.S., 1979-present): daily mean of maximum and minimum temperature at 2 meters in \({}^{\circ}\)C [14, 40].
* **Precipitation** (global and U.S., 1948-present): daily accumulated precipitation in mm, aggregated by summing over a rolling two-week window instead of averaging [77, 7, 76, 41, 42].
* **Sea surface temperature and sea ice concentration** (global, 1981-present): daily variables that track variability in the oceans; the top three principal components for each variable were extracted using global 1981-2010 loadings [54, 45].
* **Stratospheric geopotential height, zonal winds, and longitudinal winds** (global, 1948-present): daily geopotential height at 10, 100, 500, 850 millibars and zonal and longitudinal winds at 250 and 925 millibars as indicators of polar vortex variability; the top three principal components of each feature were extracted from global 1948-2010 loadings [24, 44].
* **Surface pressure and relative humidity** (U.S., 1948-present): daily pressure and relative humidity near the surface (sigma level 0.995) [24, 44].
* **Sea level pressure, precipitable water for entire atmosphere, and potential evaporation** (U.S., 1948-present): daily mean of pressure in millibars, amount of water in the atmosphere available for precipitation in kg/m\({}^{2}\), and potential evaporation rate at surface [24, 44].
* **Elevation** and **Koppen-Geiger climate classification** (global): multi-resolution terrain elevation data [9] and Koppen-Geiger climate classification [26] for each grid point.
* **Madden-Julian Oscillation** (MJO, 1974-present): daily measure of tropical convection known to impact subseasonal climate; phase and amplitude were extracted (but not aggregated) [70, 38].
* **Multivariate ENSO index** (MEI.v2, 1979-present): bimonthly scalar summary of the state of the El Nino-Southern Oscillation, an ocean-atmosphere coupled climate mode [73, 74, 43].
* **CFSv2** (U.S., 1999-present): daily 32-member ensemble mean forecasts of temperature and precipitation from the coupled atmosphere-ocean-land dynamical model with 0.5-29.5 day lead times [57, 25, 61].
* **SubX** (U.S., 1999-present): subweekly forecasts and hindcasts from seven dynamical models (GMAO-GEOS, NRL-NESM, RSMAS-CCSM4, ESRL-FIM, EMC-GEFS, ECCC-GEM, NCEP-CFSV2) and their multi-model mean for temperature and precipitation on a \(1.5^{\circ}\times 1.5^{\circ}\) latitude-longitude grid [25, 61].
* **ECMWF** (U.S., 1995-present): control and perturbed forecasts and reforecasts of precipitation and temperature on a \(1.5^{\circ}\times 1.5^{\circ}\) latitude-longitude grid [64, 12].

An example of Subseasonal ClimateUSA observations and dynamical model forecasts is displayed in Figure 2.

**Dataset Limitations** Here, we highlight several limitations of the Subseasonal ClimateUSA dataset. First, the dataset was designed for forecasting in the contiguous U.S., and hence several variables are only available in that region. In future work, we aim to develop an analogous dataset for global subseasonal forecasting. Second, subseasonal forecasts are commonly made at the biweekly temporal resolution and \(1^{\circ}\) or \(1.5^{\circ}\) spatial resolution provided in the Subseasonal ClimateUSA dataset [47, 46, 65]; however, these resolutions alone are insufficient for more localized forecasting problems without additional downscaling. Finally, many of our variables have undergone regridding via interpolation, which, while standard, can still introduce inaccuracies.

## 3 Subseasonal Forecasting Tasks

We study model performance through four canonical subseasonal forecasting tasks: predicting two variables--average temperature (\({}^{\circ}\)C) and accumulated precipitation (mm) over a two-week period--each over two time horizons: 15-28 days ahead (weeks 3-4) and 29-42 days ahead (weeks 5-6). We forecast each variable at \(G=862\) locations on a \(1^{\circ}\times 1^{\circ}\) latitude-longitude grid covering the contiguous U.S. These prediction targets and time horizons were the focus of the Sub-Seasonal Climate Forecast Rodeos [47; 46], two yearlong real-time forecasting competitions sponsored by USBR and NOAA to advance the state of subseasonal climate prediction. The same targets are used by water managers to apportion water resources, control wildfires, and anticipate droughs and other extreme weather [47; 71].

We evaluate each forecast according to two metrics recommended by the USBR [47; 46]: root mean squared error (RMSE) and _skill_ (also known as uncentered anomaly correlation [72]). For a two-week period starting on date \(t\), let \(\mathbf{y}_{t}\in\mathbb{R}^{G}\) denote the vector of ground-truth measurements \(y_{t,g}\) for each grid point \(g\) and \(\hat{\mathbf{y}}_{t}\in\mathbb{R}^{G}\) denote a corresponding vector of forecasts. In addition, define climatology \(\mathbf{c}_{t}\) as the average ground-truth values for a given month and day over the years 1981-2010. Then the RMSE is given by

\[\mathrm{RMSE}(\hat{\mathbf{y}}_{t},\mathbf{y}_{t})=\sqrt{\frac{1}{G}\sum_{g= 1}^{G}(\hat{y}_{t,g}-y_{t,g})^{2}}\in\mathbb{R}_{+}\]

with a smaller value indicating a more accurate forecast, and skill is defined by

\[\mathrm{skill}(\hat{\mathbf{y}}_{t},\mathbf{y}_{t})=\frac{\langle\mathbf{y}_{ t}-\mathbf{c}_{t},\mathbf{y}_{t}-\mathbf{c}_{t}\rangle}{\|\hat{\mathbf{y}}_{t}- \mathbf{c}_{t}\|_{2}\cdot\|\mathbf{y}_{t}-\mathbf{c}_{T}\|_{2}}\in[-1,1]\]

with a larger value indicating higher quality. For a collection of dates, we report average RMSE and average percentage skill, which is \(100\) times the average skill.

Training and Validation RecommendationsWe recommend adopting the Wednesdays of each complete year from 2011 onward as a standard test set when evaluating performance year by year (as in Figure 3) and the Wednesdays from the ten year period 2011-2020 as a standard test set when comparing with the overall (Table 1) or seasonal (Figure 3) performance reported in this work. We also recommend a progressive training and validation protocol in which, to produce a forecast for a given target date, a model can be (re)trained and (re)tuned using any data fully observable on the associated forecast issuance date (i.e., \(14\) days prior for weeks 3-4 and \(28\) days prior for weeks 5-6). All of the models evaluated in Section 5 respect this protocol. In addition, the Subseasonal ClimateUSA dataset provides convenient combination dataframes containing target variables associated with predictive features lagged by an appropriate amount to ensure that each predictive feature was fully observed on the issuance date associated with the target date.

Figure 2: Example of Subseasonal ClimateUSA observations and dynamical model forecasts.

Benchmark Models

Our experiments will evaluate three classes of forecasting methods: three standard meteorological baselines, the adaptive bias correction (ABC) models introduced in [39], and seven other state-of-the-art machine learning and deep learning methods drawn from the literature. We will also evaluate ensemble forecasts derived from these models. Open-source model implementations are available via the subseasonal_toolkit Python package. Most models train on all available data (subject to the progressive training and validation constraint of Section 3) and tune hyperparameters using a second round of progressive evaluation over the prior three years. Appendix B contains supplementary implementation details for each model, including training, hyperparameter tuning, and testing details.

Meteorological BaselinesWe first consider three standard subseasonal forecasting baselines.

Climatology. Climatology is a standard subseasonal benchmark for the expected temperature or precipitation at a location. For a given grid point and target date, it forecasts the average value of the target variable on the same day and month over 1981-2010 [3].

Debiased CFSv2. CFSv2 is the U.S. operational dynamical model commonly used for subseasonal forecasting [57]. Debiased CFSv2 is a corrected ensemble forecast used as a benchmark in the two Subseasonal Climate Forecast Rodeo competitions [46, 23]. First, a CFSv2 ensemble forecast is formed by averaging 32 forecasts for the target period based on 4 different model initializations produced at 8 different lead times. The ensemble is then _debiased_ by adding the mean value of the target variable on the target month and day over the period 1999-2010 and subtracting the mean ensemble CFSv2 reforecast over the same period.

Persistence. This baseline [37, 69] forecasts the most recently observed two-week target value.

ABC ModelsWe next evaluate the ABC models introduced in [39]. ABC is a hybrid physics-plus-learning approach that takes as input a dynamical model forecast (here, CFSv2) and uses the historical record of prior forecasts and observations to correct the model's output and improve predictive skill. While the three learning models contributing to ABC described below are simple and computationally inexpensive, Section 5 shows that each enhancement improves over both operational practice and state-of-the-art learning techniques.

Climatology++. Climatology++ is as an adaptive form of Climatology that learns how many prior years and how many dates in a window around the target date to include in a smoothed historical mean or geometric median estimate for a given grid point, target date, and target variable; importantly, unlike a static climatology, Climatology++ allows these learned window sizes to vary over time to adapt to noise levels and variability.

CFSv2++. CFSv2++ is a learned correction for raw CFSv2 forecasts. After averaging CFSv2 forecasts over a range of issuance dates and lead times, CFSv2++ debiases the ensemble forecast by adding the mean value of the target variable and subtracting the mean forecast over a learned window of observations around the target day of year. The range of ensembled lead times, the number of averaged issuance dates, and the size of the observation window employed are selected adaptively.

Persistence++. For each grid point and target date, Persistence++ predicts a target variable as a function of lagged measurements, Climatology, and CFSv2 forecasts observable for the same grid point on the forecast issuance date. These features are combined using a linear least squares regression trained on all available historical data available as of the forecast issuance date.

State-of-the-art Learning MethodsWe also consider seven state-of-the-art learning methods.

AutoKNN. AutoKNN [23] was part of a winning solution in the Subseasonal Climate Forecast Rodeo I [46]. Our implementation adapts it to target RMSE as an error metric.

Informer. The Informer [79] is a transformer-based deep learning model for time series shown to have state-of-the-art performance on a number of short term weather forecasting tasks.

LocalBoosting. LocalBoosting is a decision tree model using CatBoost [51] over features in a bounding box around the target to extract meaningful spatial information.

MultiLLR. The MultiLLR [23] model is a customized backward stepwise procedure to select Subseasonal ClimateUSA features relevant for prediction and local linear regression to combine those features into a forecast for each grid point.

N-BEATS. N-BEATS [48] is a neural network time series forecaster that obtained state-of-the-art results on the Makridakis M3 [33] and M4 [34] benchmarks for time-series forecasting.

ProphetThe Prophet model of [62] is an additive regression model for time-series and a winning solution in the Subseasonal Forecast Rodeo II [46].

Salient 2.0.: Salient 2.0 is an ensemble of fully-connected neural networks, trained on historical sea surface temperature (SST) data. It is based on Salient [59], a winning solution for the Subseasonal Forecast Rodeo I [46].

**Ensembles** Ensemble forecasts that combine the predictions of multiple models have been shown to improve the performance of long-, mid-, and short-range operational forecasting [10; 49; 23]. Here, we evaluate two ensembling strategies: Uniform ABC, which forms an equal-weighted average of the ABC model forecasts [27], and Online ABC, which uses the AdaHedgeD algorithm of [15] to choose weights adaptively to reflect relative model performance. See Appendices B.11 and B.12 for details.

## 5 Benchmark Results

We now turn to evaluating the models of Section 4 on the four subseasonal forecasting tasks of Section 3. We generate forecasts for each Wednesday in the years 2011-2020 and, for each reported period, we assess both mean RMSE relative to a baseline model and average percentage skill.

**Overall Performance** Table 1 summarizes model performance across the entire ten-year period 2011-2020. On each task, we find that the ABC models provide both the best RMSE and the best skill performance. For example, on the two precipitation tasks, Climatology++ alone improves upon debiased CFSv2 RMSE by 9% and skill by 161-250%, outperforming each of the meteorological baselines and state-of-the-art learning methods. On the two temperature tasks, CFSv2++ and Persistence++ each outperform all meteorological baselines and state-of-the-art learning methods, with CFSv2++ improving debiased CFSv2 RMSE by 6-7% and skill by 30-53%. On every task, we observe further improvements in both RMSE and skill by ensembling the predictions of the three ABC models.

One might wonder how the simple ABC models are able to outperform both the standard meteorological baselines and the state-of-the-art learning methods. We believe the answer to this question is multifaceted. First, even the leading physics-based dynamical models are subject to inaccuracies due to inexact measurement, incomplete representation of the environment, imperfect simulation, and chaos [32]. Second, many of the more elaborate machine learning models studied in this work appear to be prone to overfitting in the presence of the relatively high noise levels of subseasonal forecasting. Third, the best-performing models, while relatively simple, are hybrid physics-plus-learning models designed to leverage the strengths of an underlying dynamical model while simultaneously enhancing its predictive skill by reducing its systemic bias.

Amongst the state-of-the-art learning methods, we find that Prophet performs the best for temperature weeks 5-6 and the two precipitation tasks, while MultiLLR performs the best for temperature weeks 3-4. Amongst the neural network methods (Informer, N-BEATS, and Salient 2.0), Salient 2.0 is the top performer with skill that rivals the other learning methods and precipitation RMSE that outpaces debiased CFSv2. In the more detailed analyses to follow, we omit Informer and N-BEATS due to space constraints and their relatively poor performance overall.

**Performance by Season and by Year** We observe the same trends when performance is disaggregated by season or by year (Figure 3). For example, in every season, Climatology++ outperforms debiased CFSv2 and each state-of-the-art learner for the two precipitation tasks, while CFSv2++ and Persistence++ outperform debiased CFSv2 and each state-of-the-art learner each season for the two temperature tasks. Similarly and despite significant heterogeneity in all models' performances from year to year, the ABC models provide the best RMSE performance in 9 out of 10 years for temperature weeks 3-4 and in 10 out of 10 years for the two precipitation tasks. Indeed, Persistence++ alone dominates the temperature weeks 3-4 baselines and learners every year save 2019, and the more detailed RMSE summary of Appendix C.2 shows that the Uniform and Online ABC ensembles dominate the precipitation baselines and learners every year. In Figure 3, we observe nearly identical improvement patterns for skill.

[MISSING_PAGE_FAIL:8]

**Western U.S. Competition Results** Finally, we evaluate our models on the exact geographic region and target dates of the recent Subseasonal Climate Forecast Rodeo II competition. Specifically, we produce forecasts for the Western U.S. region, delimited by latitudes 25N to 50N and longitudes 125W to 93W, at a 1\({}^{\circ}\)\(\times\) 1\({}^{\circ}\) resolution for a total of \(G=514\) grid points. Forecasts were issued every two weeks for a yearlong period with initial issuance date October 29, 2019 and final issuance date October 27, 2020, leading to a noisier evaluation with only 26 observations.

Table 8 in Appendix C.7 compares the predictive accuracy of the models studied in this work with the accuracy of the contest baselines (debiased CFSv2, Climatology, and, for precipitation only, the Rodeo I Salient model of [59]) and the performance of the top competitors for each task. For temperature weeks 3-4, Persistence++ provides a 16.59% improvement over the mean debiased CFSv2 RMSE, outperforming the contest baselines, the state-of-the-art learning methods, and all but two of the competitors (the top three competitors improved by 17.12%, 16.67%, and 15.47%). For temperature weeks 5-6, CFSv2++ provides a 9.26% improvement over debiased CFSv2 and, despite its simplicity, outperforms the contest baselines, the state-of-the-art learning methods, and all of the

Figure 3: Per season and per year average skill and improvement over mean debiased CFSv2 RMSE across the contiguous U.S. and the years 2011–2020. Despite their simplicity, the ABC models (solid lines) consistently outperform debiased CFSv2 and the state-of-the-art learners (dotted lines).

competitors in the subseasonal forecasting competition (the top competitor improved by 8.47%). On this task, the Uniform and Online ABC ensembles also outperform all competitors.

On the precipitation tasks, the Salient baseline performed strongly and ultimately placed second and fourth respectively for the weeks 3-4 and weeks 5-6 tasks. Our Salient 2.0 model also performs remarkably well, outscoring all contestants and baselines with 12.65% improvement for weeks 3-4. For comparison, the top competitors for weeks 3-4 and weeks 5-6 improved by 11.54% and 8.63% respectively. Our Uniform ABC ensemble outperforms the remaining baselines and state-of-the-art learning methods but falls short of the exceptional Salient performance. In this setting, applying the adaptive online learning ensemble to the union of the ABC models and the state-of-the-art learners (denoted by Online ABC + Learning in Table 8) allows the user to exploit the irregular complementary benefits of the learning methods yielding 12.52% and 8.18% improvements in weeks 3-4 and 5-6.

## 6 Conclusion

In this work, we release Subseasonal ClimateUSA, a dataset for subseasonal forecasting in the U.S. It is routinely updated and can be accessed as a Python package. The dataset includes a variety of features that are relevant at the subseasonal timescale, including precipitation, temperature, surface pressure, relative humidity, geopotential height, sea surface temperature, sea ice concentration, MJO, and MEI. We use this dataset to train and benchmark multifarious models, including deep learning solutions, dynamical models, and simple learned corrections, as well ensembling strategies. Our experiments with temperature and precipitation forecasting in the contiguous U.S. show that simple learning-based corrections to operational dynamical models yield low-cost strategies that are 10% more accurate and 329% more skillful than the U.S. operational CFSv2 and outperform state-of-the-art machine and deep learning methods, as well as the leading ECMWF dynamical model. Overall, we find that the Subseasonal ClimateUSA dataset facilitates both the training and benchmarking of subseasonal forecasting models and hope that it will stimulate new advances in extended range forecasting.

Figure 4: Percentage improvement over mean debiased CFSv2 RMSE in the contiguous U.S. over 2011–2020. White grid points indicate negative or 0% improvement.

## Acknowledgments and Disclosure of Funding

We acknowledge the agencies that support the SubX system, and we thank the climate modeling groups (Environment Canada, NASA, NOAA/NCEP, NRL and University of Miami) for producing and making available their model output. NOAA/MAPP, ONR, NASA, NOAA/NWS jointly provided coordinating support and led development of the SubX system. This work is based on S2S data. S2S is a joint initiative of the World Weather Research Programme (WWRP) and the World Climate Research Programme (WCRP). The original S2S database is hosted at ECMWF as an extension of the TIGGE database. This work was supported by Microsoft AI for Earth (S.M. and G.F.); the Climate Change AI Innovation Grants program (S.M., P.O., G.F., J.C., E.F., and L.M.), hosted by Climate Change AI with the support of the Quadrature Climate Foundation, Schmidt Futures, and the Canada Hub of Future Earth; FAPERJ (Fundacao Carlos Chagas Filho de Amparo a Pesquisa do Estado do Rio de Janeiro) grant SEI-260003/001545/2022 (P.O.); NOAA grant OAR-WPO-2021-2006592 (G.F., J.C., and L.M.); and the National Science Foundation grant PLR-1901352 (J.C.).

## References

* [1] Nans Addor, Andrew J Newman, Naoki Mizukami, and Martyn P Clark. The camels data set: catchment attributes and meteorology for large-sample studies. _Hydrology and Earth System Sciences_, 21(10):5293-5313, 2017.
* [2] Troy Arcomano, Istvan Szunyogh, Jaideep Pathak, Alexander Wikner, Brian R Hunt, and Edward Ott. A machine learning-based global atmospheric forecast model. _Geophysical Research Letters_, 47(9):e2020GL087776, 2020.
* [3] Anthony Arguez, Imke Durre, Scott Applequist, Russell S Vose, Michael F Squires, Xungang Yin, Richard R Heim Jr, and Timothy W Owen. Noaa's 1981-2010 us climate normals: an overview. _Bulletin of the American Meteorological Society_, 93(11):1687-1697, 2012.
* [4] Clara Betancourt, Timo Stomberg, Ribana Roscher, Martin G Schultz, and Scarlet Stadtler. Aq-bench: A benchmark dataset for machine learning on global air quality metrics. _Earth System Science Data_, 13(6):3013-3033, 2021.
* [5] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-range global weather forecasting with 3d neural networks. _Nature_, 619(7970):533-538, 2023.
* [6] Philippe Bougeault, Zoltan Toth, Craig Bishop, Barbara Brown, David Burridge, De Hui Chen, Beth Ebert, Manuel Fuentes, Thomas M Hamill, Ken Mylne, et al. The thorpx interactive grand global ensemble. _Bulletin of the American Meteorological Society_, 91(8):1059-1072, 2010.
* [7] Mingyue Chen, Wei Shi, Pingping Xie, Viviane BS Silva, Vernon E Kousky, R Wayne Higgins, and John E Janowiak. Assessing objective techniques for gauge-based analyses of global daily precipitation. _Journal of Geophysical Research: Atmospheres_, 113(D4), 2008.
* [8] J. Cohen, D. Coumou, J. Hwang, L. Mackey, P. Orenstein, S. Totz, and E. Tziperman. S2S reboot: An argument for greater inclusion of machine learning in subseasonal to seasonal (S2S) forecasts. _WIREs Climate Change_, 10, 2018.
* [9] Jeffrey J Danielson and Dean B Gesch. _Global multi-resolution terrain elevation data 2010 (GMTED2010)_. US Department of the Interior, US Geological Survey, 2011.
* [10] Jun Du, Judith Berner, Roberto Buizza, Martin Charron, Pieter Leopold Houtekamer, Dingchen Hou, Isidora Jankov, Mu Mu, Xuguang Wang, Mozheng Wei, et al. Ensemble methods for meteorological predictions. _Office note (National Centers for Environmental Prediction (U.S.))_, 2018.
* [11] Peter D Dueben, Martin G Schultz, Matthew Chantry, David John Gagne, David Matthew Hall, and Amy McGovern. Challenges and benchmark datasets for machine learning in the atmospheric sciences: Definition, status, and outlook. _Artificial Intelligence for the Earth Systems_, 1(3):e210002, 2022.

* [12] ECMWF S2S data. Ecmwf S2S ecmf: Ecmwf ensemble. https://iridl.ldeo.columbia.edu/SOURCES/.ECMWF/.S2S/.ECMF/, 2021.
* [13] Veronika Eyring, Sandrine Bony, Gerald A Meehl, Catherine A Senior, Bjorn Stevens, Ronald J Stouffer, and Karl E Taylor. Overview of the coupled model intercomparison project phase 6 (cmip6) experimental design and organization. _Geoscientific Model Development_, 9(5):1937-1958, 2016.
* [14] Yun Fan and Huug Van den Dool. A global monthly land surface air temperature analysis for 1948-present. _Journal of Geophysical Research: Atmospheres_, 113(D1), 2008.
* [15] Genevieve Flaspohler, Francesco Orabona, Judah Cohen, Soukayna Mouatadid, Miruna Oprescu, Paulo Orenstein, and Lester Mackey. Online learning with optimism and delay. In _International Conference on Machine Learning_. PMLR, 2021.
* [16] Gabriele Franch, Valerio Maggio, Luca Coviello, Marta Pendesini, Giuseppe Jurman, and Cesare Furlanello. Taasrad19, a high-resolution weather radar reflectivity dataset for precipitation nowcasting. _Scientific Data_, 7(1):234, 2020.
* [17] Isaac Godfried, Kriti Mahajan, Maggie Wang, Kevin Li, and Pranjalya Tiwari. Flowdb a large scale precipitation, river, and flash flood dataset. _arXiv preprint arXiv:2012.11154_, 2020.
* [18] Alex M Haberlie, Walker S Ashley, and Marisa R Karpinski. Mean storms: Composites of radar reflectivity images during two decades of severe thunderstorm events. _International Journal of Climatology_, 41:E1738-E1756, 2021.
* [19] Sijie He, Xinyan Li, Timothy DelSole, Pradeep Ravikumar, and Arindam Banerjee. Subseasonal climate forecasting via machine learning: Challenges, analysis, and advances. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 169-177, 2021.
* [20] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.
* [21] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andras Horanyi, Joaquin Munoz-Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global reanalysis. _Quarterly Journal of the Royal Meteorological Society_, 146(730):1999-2049, 2020.
* [22] S. Hoyer and J. Hamman. xarray: N-D labeled arrays and datasets in Python. _Journal of Open Research Software_, 5(1), 2017.
* [23] Jessica Hwang, Paulo Orenstein, Judah Cohen, Karl Pfeiffer, and Lester Mackey. Improving subseasonal forecasting in the western U.S. with machine learning. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, KDD '19, page 2325-2335, New York, NY, USA, 2019. Association for Computing Machinery.
* [24] Eugenia Kalnay, Masao Kanamitsu, Robert Kistler, William Collins, Dennis Deaven, Lev Gandin, Mark Iredell, Suranjana Saha, Glenn White, John Woollen, et al. The ncep/ncar 40-year reanalysis project. _Bulletin of the American meteorological Society_, 77(3):437-472, 1996.
* [25] BP Kirtman, K Pegion, T DelSole, M Tippett, AW Robertson, M Bell, R Burgman, H Lin, J Gottschalck, DC Collins, et al. The subseasonal experiment (subx). _IRI Data Library_, 10:D8PG249H, 2017.
* [26] Markus Kottek, Jurgen Grieser, Christoph Beck, Bruno Rudolf, and Franz Rubel. World map of the Koppen-Geiger climate classification updated. _Meteorologische Zeitschrift_, 15(3):259-263, 2006.
* [27] TN Krishnamurti, Chandra M Kishtawal, Timothy E LaRow, David R Bachiochi, Zhan Zhang, C Eric Williford, Sulochana Gadgil, and Sajani Surendran. Improved weather and seasonal climate forecasts from multimodel superensemble. _Science_, 285(5433):1548-1550, 1999.

* [28] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklyn Stott, Alexander Pritzel, Shakir Mohamed, and Peter Battaglia. Learning skillful medium-range global weather forecasting. _Science_, 382(6677):1416-1421, 2023.
* [29] Andrea L Lang, Kathleen Pegion, and Elizabeth A Barnes. Introduction to special collection: "Bridging weather and climate: Subseasonal-to-seasonal (S2S) prediction". _Journal of Geophysical Research: Atmospheres_, 125(4):e2019JD031833, 2020.
* [30] DJ Lea, I Mirouze, MJ Martin, RR King, A Hines, D Walters, and M Thurlow. Assessing a new coupled data assimilation system based on the met office coupled atmosphere-land-ocean-sea ice model. _Monthly Weather Review_, 143(11):4678-4694, 2015.
* [31] Laifang Li, Raymond W Schmitt, Caroline C Ummenhofer, and Kristopher B Karnauskas. Implications of north atlantic sea surface salinity for summer precipitation over the us midwest: Mechanisms and predictive value. _Journal of Climate_, 29(9):3143-3159, 2016.
* [32] E.N. Lorenz. Deterministic nonperiodic flow. _Journal of the Atmospheric Sciences_, 20(2):130-141, 1963.
* [33] Spyros Makridakis and Michele Hibon. The m3-competition: results, conclusions and implications. _International journal of forecasting_, 16(4):451-476, 2000.
* [34] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: 100,000 time series and 61 forecasting methods. _International Journal of Forecasting_, 36(1):54-74, 2020.
* [35] Manil Maskey, Rahul Ramachandran, Muthukumaran Ramasubramanian, Iksha Gurung, Brian Freitag, Aaron Kaulfus, Drew Bollinger, Daniel J Cecil, and Jeffrey Miller. Deepti: Deep-learning-based tropical cyclone intensity estimation system. _IEEE journal of selected topics in applied Earth observations and remote sensing_, 13:4271-4281, 2020.
* [36] William J Merryfield, Johanna Baehr, Lauriane Batte, Emily J Becker, Amy H Butler, Caio AS Coelho, Gokhan Danabasoglu, Paul A Dirmeyer, Francisco J Doblas-Reyes, Daniela IV Domeisen, et al. Current and emerging developments in subseasonal to decadal prediction. _Bulletin of the American Meteorological Society_, 101(6):E869-E896, 2020.
* [37] Marion P Mittermaier. The potential impact of using persistence as a reference forecast on perceived forecast skill. _Weather and forecasting_, 23(5):1022-1031, 2008.
* [38] MJO data. Real-time multivariate Madden Julian Oscillation index. https://iridl.ldeo.columbia.edu/dochelp/QA/Technical/citation.html, 2021.
* [39] Soukayna Mouatadid, Paulo Orenstein, Genevieve Flaspohler, Judah Cohen, Miruna Oprescu, Ernest Fraenkel, and Lester Mackey. Adaptive bias correction for improved subseasonal forecasting. _arXiv preprint arXiv:2209.10666_, 2022.
* [40] NOAA/OAR/ESRL PSL. CPC global temperature data. ftp://ftp.cdc.noaa.gov/Datasets/cpc_global_temp/, 2021.
* [41] NOAA/OAR/ESRL PSL. CPC global unified precipitation data. ftp://ftp.cdc.noaa.gov/Datasets/cpc_global_precip/, 2021.
* [42] NOAA/OAR/ESRL PSL. CPC US unified precipitation data. ftp://ftp.cdc.noaa.gov/Datasets/cpc_us_precip/, 2021.
* [43] NOAA/OAR/ESRL PSL. Multivariate El Nino/Southern Oscillation (ENSO) index. https://psl.noaa.gov/enso/mei/data/meiv2.data, 2021.
* [44] NOAA/OAR/ESRL PSL. NCEP reanalysis data. Geopotential height, zonal wind, and longitudinal wind: ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/pressure/; Relative humidity, sea level pressure, and precipitable water for entire atmosphere: ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis/surface/; Pressure at the surface and potential evaporation: ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis/surface_gauss/, 2021.

* [45] NOAA/OAR/ESRL PSL. NOAA high resolution SST data. ftp://ftp.cdc.noaa.gov/Projects/Datasets/noaa.oisst.v2.highres/, 2021.
* [46] Kenneth Nowak, Ian M Ferguson, Jennifer Beardsley, and Levi D Brekke. Enhancing western united states sub-seasonal forecasts: Forecast rodeo prize competition series. In _AGU Fall Meeting 2020_. AGU, 2020.
* [47] Kenneth Nowak, RS Webb, R Cifelli, and LD Brekke. Sub-seasonal climate forecast rodeo. In _2017 AGU Fall Meeting, New Orleans, LA_, pages 11-15, 2017.
* [48] Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: neural basis expansion analysis for interpretable time series forecasting. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* [49] Tim Palmer. The ecmwf ensemble prediction system: Looking back (more than) 25 years and projecting forward 25 years. _Quarterly Journal of the Royal Meteorological Society_, 145:12-24, 2019.
* [50] Kathy Pegion, Ben P Kirtman, Emily Becker, Dan C Collins, Emerson LaJoie, Robert Burgman, Ray Bell, Timothy DelSole, Dughong Min, Yuejian Zhu, et al. The subseasonal experiment (subx): A multimodel subseasonal prediction experiment. _Bulletin of the American Meteorological Society_, 100(10):2043-2060, 2019.
* [51] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. In _Advances in neural information processing systems_, pages 6638-6648, 2018.
* [52] Stephan Rasp, Peter D Dueben, Sebastian Scher, Jonathan A Weyn, Soukayna Mouatadid, and Nils Thuerey. Weatherbench: A benchmark data set for data-driven weather forecasting. _Journal of Advances in Modeling Earth Systems_, 12(11):e2020MS002203, 2020.
* [53] Stephan Rasp, Hauke Schulz, Sandrine Bony, and Bjorn Stevens. Combining crowdsourcing and deep learning to explore the mesoscale organization of shallow convection. _Bulletin of the American Meteorological Society_, 101(11):E1980-E1995, 2020.
* [54] Richard W Reynolds, Thomas M Smith, Chunying Liu, Dudley B Chelton, Kenneth S Casey, and Michael G Schlax. Daily high-resolution-blended analyses for sea surface temperature. _Journal of climate_, 20(22):5473-5496, 2007.
* [55] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. Tackling climate change with machine learning. _ACM Computing Surveys (CSUR)_, 55(2):1-96, 2022.
* [56] Sebastian Ruder. An overview of multi-task learning in deep neural networks. _arXiv preprint arXiv:1706.05098_, 2017.
* [57] Suranjana Saha, Shrinivas Moorthi, Xingren Wu, Jiande Wang, Sudhir Nadiga, Patrick Tripp, David Behringer, Yu-Tai Hou, Hui-ya Chuang, Mark Iredell, et al. The ncep climate forecast system version 2. _Journal of climate_, 27(6):2185-2208, 2014.
* [58] Ray Schmitt. private communication, Jul. 2021.
* [59] Raymond Schmitt. Salient predictions: Validation summary. https://storage.googleapis.com/content.salientpredictions.com/Salient%20Validation%20Summary.pdf, 2019. Accessed: 2021-05-29.
* [60] Vishwak Srinivasan, Justin Khim, Arindam Banerjee, and Pradeep Ravikumar. Subseasonal climate prediction in the western us using bayesian spatial models. In _Uncertainty in artificial intelligence_, volume 37, 2021.
* [61] SubX data. http://iridl.ldeo.columbia.edu/SOURCES/.Models/.SubX/, DOI: https://doi.org/10.7916/D8PG249H, 2021.

* [62] Sean J Taylor and Benjamin Letham. Forecasting at scale. _The American Statistician_, 72(1):37-45, 2018.
* [63] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020.
* [64] F Vitart, C Ardilouze, A Bonet, A Brookshaw, M Chen, C Codorean, M Deque, L Ferranti, E Fucile, M Fuentes, et al. The subseasonal to seasonal (S2S) prediction project database. _Bulletin of the American Meteorological Society_, 98(1):163-173, 2017.
* E2886, 2022.
* [66] Frederic Vitart, Andrew W Robertson, and David LT Anderson. Subseasonal to seasonal prediction project: Bridging the gap between weather and climate. _Bulletin of the World Meteorological Organization_, 61(2):23, 2012.
* [67] C Wang, Z Jia, Z Yin, F Liu, G Lu, and J Zheng. Improving the accuracy of subseasonal forecasting of china precipitation with a machine learning approach. front. _Earth Sci_, 9:659310, 2021.
* [68] Duncan Watson-Parris. Machine learning for weather and climate are worlds apart. _Philosophical Transactions of the Royal Society A_, 379(2194):20200098, 2021.
* [69] Jonathan A. Weyn, Dale R. Durran, Rich Caruana, and Nathaniel Cresswell-Clay. Sub-seasonal forecasting with a large ensemble of deep-learning weather prediction models. _Journal of Advances in Modeling Earth Systems_, 13(7):e2021MS002502, 2021.
* [70] Matthew C Wheeler and Harry H Hendon. An all-season real-time multivariate mjo index: Development of an index for monitoring and prediction. _Monthly weather review_, 132(8):1917-1932, 2004.
* [71] Christopher J. White, Henrik Carlsen, Andrew W. Robertson, Richard J.T. Klein, Jeffrey K. Lazo, Arun Kumar, Frederic Vitart, Erin Coughlan de Perez, Andrea J. Ray, Virginia Murray, Sukaina Bharwani, Dave MacLeod, Rachel James, Lora Fleming, Andrew P. Morse, Bernd Eggen, Richard Graham, Erik Kjellstrom, Emily Becker, Kathleen V. Pegion, Neil J. Holbrook, Darryn McEvoy, Michael Depledge, Sarah Perkins-Kirkpatrick, Timothy J. Brown, Roger Street, Lindsey Jones, Tomas A. Remenyi, Indi Hodgson-Johnston, Carlo Buontempo, Rob Lamb, Holger Meinke, Berit Arheimer, and Stephen E. Zebiak. Potential applications of subseasonal-to-seasonal (S2S) predictions. _Meteorological Applications_, 24(3):315-325, 2017.
* [72] Daniel S Wilks. _Statistical methods in the atmospheric sciences_, volume 100. Academic press, 2011.
* [73] Klaus Wolter and Michael S Timlin. Monitoring enso in coads with a seasonally adjusted principal. In _Proc. of the 17th Climate Diagnostics Workshop, Norman, OK, NOAA/NMC/CAC, NSSL, Oklahoma Clim. Survey, CIMMS and the School of Meteor., Univ. of Oklahoma, 52_, volume 57, 1993.
* [74] Klaus Wolter and Michael S Timlin. Measuring the strength of ENSO events: How does 1997/98 rank? _Weather_, 53(9):315-324, 1998.
* [75] Klaus Wolter and Michael S Timlin. El Nino/Southern Oscillation behaviour since 1871 as diagnosed in an extended multivariate ENSO index (MEI.ext). _International Journal of Climatology_, 31(7):1074-1087, 2011.

* [76] Pingping Xie, M Chen, and W Shi. CPC unified gauge-based analysis of global daily precipitation. In _Preprints, 24th Conf. on Hydrology, Atlanta, GA, Amer. Meteor. Soc_, volume 2, 2010.
* [77] Pingping Xie, Mingyue Chen, Song Yang, Akiyo Yatagai, Tadahiro Hayasaka, Yoshihiro Fukushima, and Changming Liu. A gauge-based analysis of daily precipitation over east asia. _Journal of Hydrometeorology_, 8(3):607-626, 2007.
* [78] Akio Yamagami and Mio Matsueda. Subseasonal forecast skill for weekly mean atmospheric variability over the northern hemisphere in winter and its relationship to midlatitude teleconnections. _Geophysical Research Letters_, 47(17):e2020GL088508, 2020.
* [79] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021_, page online. AAAI Press, 2021.

Supplementary Material for SubseasonalClimateUSA:

A Dataset for Subseasonal Forecasting and Benchmarking

## Appendix A SubseasonalClimateUSA Supplementary Details

The subseasonal_data Python package provides a detailed description of the SubseasonalClimateUSA dataset contents, sources, and processing steps at the following URL:

https://github.com/microsoft/subseasonal_data/blob/main/DATA.md

### Computational Environment Details

The SubseasonalClimateUSA update pipeline runs in a Docker container on an Azure E16-4ds_v4 instance with 4 vCPUs/cores and 128 GB of RAM. The update pipeline runs in under 6 hours at a total cost of $5. All benchmarking experiments were run on the Massachusetts Institute of Technology engaging cluster (https://engaging-web.mit.edu/eofe-wiki/).

### Western U.S. Competition Data Details

Following [23], for the Western U.S. competition experiments of Section 5, the sea surface temperature and sea ice concentration variables were formed by identifying the top three principal components for each variable restricted to the Pacific basin region (20S to 65N, 150E to 90W) using loadings from 1981-2010.

## Appendix B Model Implementation Details

This section describes the implementation details for each learning model, including the training, hyperparameter tuning, and validation protocols. All models were implemented in Python 3.

### Climatology++

Climatology++ was trained and tuned following the protocol described in [39]; see Algorithm 1. Figure 5 displays the selected window length (the span \(s\)) and number of years \(Y\) for each target date in 2011-2020 when forecasting for the contiguous U.S. We see that the temperature models preferred fewer training years and larger windows around the target day in recent history but focused more exclusively on the target day of year (via a span of \(0\)) in 2013-2016 and preferred more training years in 2011. Meanwhile, the precipitation models selected the largest available window (corresponding to higher bias but lower variance estimates) for nearly every target date.

### CFSv2++

CFSv2++ was trained and tuned following the protocol described in [39]; see Algorithm 2. Figure 6 displays the selected window length (the span \(s\)), lead time range, and issuance date count for each target date in 2011-2020 when forecasting for the contiguous U.S. In each task, we observe a

Figure 5: Climatology++ hyperparameters automatically selected for each target date in 2011–2020.

significant amount of variability in the optimal span, lead, and date count selections, highlighting the value of adaptive ensembling and debiasing over the static ensembling and debiasing strategies employed by standard debiased CFSv2.

```
0: test date \(t^{\star}\); # train years \(Y\); span \(s\); loss \(\in\{\text{RMSE},\text{MSE}\}\); training set ground truth \((\mathbf{y}_{t})_{t\in\mathcal{T}}\)
0: days per year \(D=365.242199\); # training years \(Y=12\) \(\mathcal{S}=\{t\in\mathcal{T}:\texttt{year\_diff}\coloneqq\lfloor\frac{t^{ \star}-t}{D}\rfloor\leq Y\text{ and day\_diff}\coloneqq\frac{365}{2}-\left|\lfloor(t^{ \star}-t)\mod D\right\rfloor-\frac{365}{2}\rvert\leq s\}\)// Form CFSv2 ensemble forecast across issuance dates and lead times \(l\in\mathcal{L}\) for training and test dates \(t\in\mathcal{S}\cup\{t^{\star}\}\)do \(\bar{\mathbf{f}}_{t}=\text{mean}((\bar{\mathbf{f}}_{t-t-d+1,l})_{1\leq d\leq d ^{\star},l\in\mathcal{L}})\) output\(\bar{\mathbf{f}}_{t^{\star}}+\text{mean}((\mathbf{y}_{t}-\bar{\mathbf{f}}_{t})_{t\in \mathcal{S}})\) ```

**Algorithm 1** Climatology++

### Persistence++

Persistence++ was trained and tuned following the protocol described in [39]; see Algorithm 3. Figures 7 to 10 display the learned Persistence++ regression weights for the final target date in 2020 for each of the four contiguous U.S. forecasting tasks. In each case, we observe significant spatial variation in the optimal weights used to combine lagged measurements, climatology, and CFSv2 ensemble forecasts.

Figure 6: CFSv2++ hyperparameters automatically selected for each target date in 2011–2020.

``` input lead time \(l^{\star}\); training set ground truth, climatology, and dynamical forecasts \((\mathbf{y}_{t},\mathbf{c}_{t},\mathbf{f}_{t,l})_{t\in\mathcal{T},l\in\mathcal{L}}\) initialize forecast period length \(L=14\) // Form dynamical ensemble forecast across subseasonal lead times \(l\geq l^{\star}\) for training dates \(t\in\mathcal{T}\)do \(\mathbf{\tilde{f}}_{t}=\text{mean}((\mathbf{f}_{t,l})_{l\geq l^{\star}})\) // Combine ensemble forecast, climatology, and lagged measurements for grid points \(g=1\)to\(G\)do \(\hat{\boldsymbol{\beta}}_{g}\in\operatorname*{argmin}_{\boldsymbol{\beta}} \sum_{t\in\mathcal{T}}(y_{t,g}-\boldsymbol{\beta}^{\top}[1,c_{t,g},y_{t-l^{ \star}-L-1,g},y_{t-2l^{\star}-L-1,g},\bar{f}_{t-l^{\star}-1,g}])^{2}\) output coefficients \((\hat{\boldsymbol{\beta}}_{g})_{g=1}^{G}\) ```

**Algorithm 3** Persistence++

Figure 8: Spatial variation in Persistence++ learned regression weights when forecasting temperature in weeks 5-6 for the final target date, December 23, 2020.

Figure 7: Spatial variation in Persistence++ learned regression weights when forecasting temperature in weeks 3-4 for the final target date, December 23, 2020.

### AutoKNN

The AutoKNN model of [23] was part of a winning solution in the Subseasonal Climate Forecast Rodeo I [46] and was shown to outperform deep fully connected neural networks [19]. AutoKNN first identifies a set of historical dates most similar to the target date and then forecasts a weighted locally linear combination of the anomalies measured on similar dates and recent dates. Our implementation matches that of [23] but adapts the model to target our primary RMSE objective by (i) using mean (negative) RMSE as the similarity measure instead of mean skill, (ii) using raw measurement vectors \(\mathbf{y}_{t}\) instead of anomaly vectors \(\mathbf{a}_{t}\), and (iii) using equal datapoint weights in the final local linear regression.

TrainingThe k-nearest neighbors (KNN) step of AutoKNN identifies a set of historical dates most similar to the target date and while the autoregression step forecasts a weighted locally linear combination of the anomalies measured on similar dates and recent dates. For a given target date \(t^{*}\) and lead time \(l^{*}\), the AutoKNN training set is restricted to data fully observable one day prior to

Figure 10: Spatial variation in Persistence++ learned regression weights when forecasting precipitation in weeks 5-6 for the final target date, December 23, 2020.

Figure 9: Spatial variation in Persistence++ learned regression weights when forecasting precipitation in weeks 3-4 for the final target date, December 23, 2020.

the issuance date, that is, to dates \(t\leq t^{\star}-l^{\star}-L-1\) where \(L=14\) represents the forecast period length.

TuningAll hyperparameters were set to the default values specified in [23].

### Informer

The Informer [79], a transformer-based deep learning model, was retrained every four months to predict temperature from past temperature and precipitation from past precipitation independently at each grid point.

FeaturesFor a given grid point and target date \(t^{\star}\), the input features used to construct a forecast are the lagged target variable observations from dates \(t_{\text{last}},t_{\text{last}}-1,t_{\text{last}}-2,\cdots,t_{\text{last}}-95\) for where \(t_{\text{last}}=t^{\star}-l^{\star}-L\) represents the last complete observation prior to \(t^{\star}\) and \(L=14\) represents the forecast period length.

TrainingWe divide the set of target dates in 2011-2020 into consecutive, non-overlapping four-month blocks and retrain the Informer model after every four-month block. For a given lead time \(l^{\star}\), grid point, and four-month block beginning with date \(t^{\star}\):

1. The training set is chosen to start at most \(10,000\) days before \(t\) (or at the beginning of the training set, whichever is later) and then ends \(301\) days before \(t\).
2. The validation set is chosen to start \(300\) days before \(t\) and to end on the date prior to \(t^{\star}-l^{\star}-L\).
3. We use early stopping with patience equal to three to determine when to stop the training: when we have three consecutive epochs \(e+1,e+2,e+3\) with validation loss no lower than that of epoch \(e\), we terminate training and use the model at epoch \(e\) as the final trained model.
4. We use the trained model to generate forecasts for each target date in the four-month block.

TuningWe use the default Informer architecture and hyperparameters for univariate time series forecasting: the model has 3 encoder layers, 2 decoder layers, and has an 8-headed attention with 7-dimensional keys and feed-forward layers with 1024 hidden units, and has GeLU activations [20].

### LocalBoosting

In recent subseasonal experiments of [19], boosted decision tree models yielded the best performance. Our boosted decision tree model, based on CatBoost [51], uses as features the value of 10 SubseasonAlClimateUSA variables in a geographic region around the target grid point. This gives the algorithm enough flexibility to adapt the weights of the features to each particular grid point while still taking into account neighboring spatial information. The geographic region is determined by a bounding box of 2 degrees in each direction, and the 10 variables are chosen for each task via their predictive power on validation years.

TrainingFor a given target date \(t^{\star}\) and lead time \(l^{\star}\), the LocalBoosting training set \(\mathcal{T}\) is restricted to data fully observable one day prior to the issuance date, that is, to dates \(t\leq t^{\star}-l^{\star}-L-1\) where \(L=14\) represents the forecast period length. LocalBoosting uses CatBoost [51] to regress, for each gridpoint and each date, the value of a set of lagged weather variables in a geographic region around the gridpoint.

TuningThere are two hyperparameters to consider: (i) which lagged weather variables to use; and (ii) the number of neighborhood cells around a gridpoint to define the geographic region. Bounding boxes of with side length of 2 or 3 cells were considered. Larger sizes were computationally infeasible. In each case, the 10 or 20 most important features in the SubseasonalClimateUSA dataset were considered. Here, features were chosen by their performance over 2001-2010 in terms of RMSE.

For each target date, LocalBoosting is run with the hyperparameter configuration that achieved the smallest mean RMSE over the preceding \(3\) years. See Figure 11 for a visualization of the hyperparameters automatically selected for each target date in 2011-2020.

### MultiLLR

The MultiLLR model of [23] was also part of a winning solution in the Subseasonal Climate Forecast Rodeo I [46] and has since been used to improve subseasonal precipitation prediction in China [67]. For each target date, MultiLLR uses a backward stepwise procedure to select the most predictive features and then applies local linear regression for each grid point to combine those features into a final prediction. Our implementation matches that of [23] but adapts the model to target our primary RMSE objective by using mean (negative) RMSE instead of mean skill as the feature selection criterion. In addition, we replace their MEI features with corresponding MEI.v2 features and their monthly dynamical forecast features with daily debiased CFSv2 forecasts.

TrainingFor a given target date \(t^{\star}\) and lead time \(l^{\star}\), the MultiLLR training set is restricted to data fully observable one day prior to the issuance date, that is, to dates \(t\leq t^{\star}-l^{\star}-L-1\) where \(L=14\) represents the forecast period length. The coarse-grained dynamical input feature nnmme_wo_ccsm3_nasa of [23] was replaced with the daily debiased CFSv2 forecast features

* subx_cfsv2_tmp2m-14.5d_shift15 and subx_cfsv2_tmp2m-0.5d_shift15 for predicting temperature at weeks 3-4,
* subx_cfsv2_tmp2m-28.5d_shift29 and subx_cfsv2_tmp2m-0.5d_shift29 for predicting temperature at weeks 5-6,
* subx_cfsv2_precip-14.5d_shift15 and subx_cfsv2_precip-0.5d_shift15 for predicting precipitation at weeks 3-4, and
* subx_cfsv2_precip-28.5d_shift29 and subx_cfsv2_precip-0.5d_shift29 for predicting precipitation at weeks 5-6.

TuningAll hyperparameters were set to the default values specified in [23], save for the tolerance parameter which was set to \(0.001\) to accommodate the new RMSE selection criterion.

### N-beats

N-BEATS [48], a neural network for time series data, is retrained N-BEATS every two months to predict temperature from past temperature and precipitation from past precipitation independently at each grid point.

FeaturesFor a given grid point and target date \(t^{\star}\), the input features used to construct a forecast are the lagged target variable observations from dates \(t_{last},t_{last}-4,t_{last}-8,\cdots,t_{last}-48\) for where \(t_{last}=t^{\star}-l^{\star}-L\) represents the last complete observation prior to \(t^{\star}\) and \(L=14\) represents the forecast period length.

TrainingWe divide the set of target dates in 2011-2020 into consecutive, non-overlapping two-month blocks and retrain the N-BEATS model after every two-month block. For a given lead time \(l^{\star}\), grid point, and two-month block beginning with date \(t^{\star}\):

1. We train on all dates \(t\leq t^{\star}-l^{\star}-L\).

Figure 11: LocalBoosting hyperparameters automatically selected for each target date in 2011–2020.

2. For the initial two-month block, we train for \(30\) epochs.
3. For subsequent two-month blocks, we initialize our model weights to the learned weights from the prior block and then fine-tune for \(8\) epochs.
4. We use the trained model to generate forecasts for each target date in the two-month block.

TuningWe use the default N-BEATS architecture and hyperparameters for univariate time series forecasting [48]. The N-BEATS model has two stacks, where each stack is used to understand different patterns. Each stack consists of three blocks, which themselves are each comprised of six fully connected layers with ReLU activations. We used the Adam optimizer with learning rate \(0.001\) and a batch size of \(512\).

### Prophet

The Prophet model of [62] was one of the winning solutions in the Subseasonal Forecast Rodeo II [46]. Prophet is an additive regression model for time-series forecasting that predicts weekly and yearly seasonal trends on top of a piecewise linear or logistic growth curve. We trained the model to predict each grid point independently with yearly seasonality enabled (to capture predictable whether trends) and weekly seasonality disabled.

TrainingProphet takes as input a sequence of (univariate) time-series values and then predicts the next \(k\) dates from those, arbitrarily far in the future.

First, we split the problem into a many univariate time-series prediction problems. When evaluating, we consider periods of 4 months (e.g. January 2010 - April 2010), train the model on all available historical temperatures (which may depend on the lead time) at that grid point, and make predictions for that four month period. We then run this for all relevant periods of four months.

TuningThe prophet model is trained in the univariate mode with yearly seasonality on (to capture predictable weather trends), weekly seasonality off (as weekends are unlikely to be special).

### Salient 2.0

We developed the Salient 2.0 model based on Salient [59], a winning solution for the Subseasonal Forecast Rodeo I [46]. Salient consists of an ensemble of feed-forward fully-connected neural networks, using historical sea surface temperature (SST) data from 1990 to 2017 and an encoding of the day of the year as features. Salient's training protocol follows a multi-task learning framework [56]. It starts by training 50 randomly generated fully connected neural networks, each of which provides a prediction for the average temperature and accumulated precipitation at 3, 4, 5, and 6 weeks ahead, at every grid cell. The forecasts are then obtained by combining the predictions for weeks 3 and 4 and for weeks 5 and 6. The final ensemble model forecasts correspond to the mean of the top 10 ensemble members with the lowest validation error.

For Salient 2.0 in this work, the input features were augmented with geopotential heights at different pressure levels (10, 100, 500 and 850 hPa) along with MEI and MJO indices. In addition, instead of training the ensemble on the whole of the 1990-2017 data, a sequence of models was trained using data up until each of the years in our validation period of 2010-2020. These submodels with earlier training data cut-offs were then used to generate hindcasts that informed the model's tuning decisions.

Salient 2.0 relies on two sources of sea surface temperature training data. The first data source is the weekly sea surface temperature from NOAA [54] and covers dates from 17 January 1990 to 02 February 2017. This dataset contains weekly data cenetered around Wednesdays and has a \(1^{\circ}\times 1^{\circ}\) resolution. It was re-gridded to a \(4^{\circ}\times 4^{\circ}\) using spline interpolation of order equal to 2, under the Python Scipy package [63].

For dates from February 2017 to present, a second source of sea surface temperature data from the MET Office [30] is used. This dataset has a daily temporal resolution and is averaged to obtain weekly data centered around Wednesdays. This data is initially downloaded in a \(0.25^{\circ}\times 0.25^{\circ}\) spatial resolution and is re-gridded to a \(4^{\circ}\times 4^{\circ}\). A linear interpolation is then used to fill in any missing values (under the Python Scipy package [63]).

TrainingSalient 2.0 is an ensemble of 50 feed-forward fully connected neural networks. All 50 neural networks are trained on a pre-determined combination of input features, including weekly sea surface temperature, MEI, as well as the phase and amplitude features of MJO. The combinations of input features considered are:

* d2wk_cop_sst: sea surface temperature,
* d2wk_cop_sst_mei: sea surface temperature and ENSO,
* d2wk_cop_sst_mjo: sea surface temperature and MJO,
* d2wk_cop_sst_mei_mjo: sea surface temperature, ENSO and MJO.

In total, four ensemble models, corresponding to the four input feature combinations above, each including 50 neural networks, are trained. Each ensemble model is trained in a rolling fashion, where the start year of the training dataset is 1990 and the ensemble is trained up until each year in the range (2006, 2019), where a model ending on a year \(y\) is used to generate forecasts for target dates with year \(y+1\).

For each of the 50 neural networks within a given ensemble model, the input features can further be augmented using a time vector obtained by converting dates to a float representing the fraction of the year passed by that date. The addition of the time vector is decided by generating a random integer in the range \([0,1]\), with 1 corresponding to the addition of the time vector and 0 otherwise. Additionally, for each of the 50 neural networks, the input feature vector for an individual training example consists of a concatenation of the prior 10 weeks of data.

For each of the 50 neural networks within a given ensemble model, the output consists of a prediction for the average temperature and accumulated precipitation at 3, 4, 5 and 6 weeks ahead. The predictions for weeks 3 and 4 and the predictions for weeks 5 and 6 are combined separately, by averaging temperatures and summing precipitation. The top 10 neural networks with the lowest validation error are selected as the final ensemble members. The final predictions for each ensemble model correspond to the mean of the 10 selected ensemble members.

TuningEach of the 50 neural networks within a given ensemble model is trained using a batch size equal to 128 and a train ratio equal to 0.89. In addition, each neural network's hyperparameters are randomly generated, with the number of epochs sampled in the range \([100,500]\), the number of layers in the range \([3,7]\), and the number of units per layer in the range \([100,600]\).

At test time, for each target date, Salient 2.0 is run using the ensemble model that achieved the smallest mean RMSE over the preceding 3 years. Figure 12 shows which ensemble models were used to generate predictions for which target dates.

### Uniform Ensemble

We consider two Uniform Ensemble models, where the ensemble is produced using a set of models \(C\) as input:

Figure 12: Salient 2.0 hyperparameters automatically selected for each target date in 2011–2020.

[MISSING_PAGE_FAIL:25]

[MISSING_PAGE_EMPTY:26]

[MISSING_PAGE_EMPTY:28]

### Spatial Improvement over Mean Debiased CFSv2 RMSE

Figure 15 presents the spatial improvement of each model over debiased CFSv2 when predicting U.S. temperature across 2011-2020. For both the weeks 3-4 and the weeks 5-6 lead times, the ABC models uniformly improve over debiased CFSv2 and outperform both the baseline models and the state-of-the-art learning methods. The best overall performance at each lead time is obtained by the Online ABC ensemble.

Figure 16 displays the spatial improvement of each model over debiased CFSv2 when forecasting precipitation across 2011-2020. For precipitation, all models exhibit larger gains over debiased CFSv2, and all models, including Climatology, achieve larger improvements in the Western U.S. than in the Eastern U.S.

Figure 16: Percentage improvement over mean debiased CFSv2 RMSE when forecasting precipitation in the contiguous U.S. over 2011–2020. White grid points indicate negative or 0% improvement, and the mean percentage improvement is given in parentheses.

Figure 15: Percentage improvement over mean debiased CFSv2 RMSE when forecasting temperature in the contiguous U.S. over 2011–2020. White grid points indicate negative or 0% improvement, and the mean percentage improvement is given in parentheses.

### Spatial Bias Maps

This section explores the spatial bias (the mean forecast minus the mean observation at each grid point in the contiguous U.S.) of each model across 2011-2020. The temperature maps of Figure 17 indicate a cold bias for most models over the southern half of the U.S. and an additional warm bias for several models in the center north. This warm bias is particularly pronounced for Salient 2.0. In precipitation maps of Figure 18, all models Salient 2.0 and AutoKNN show wet biases in the western half of the U.S. and dry biases in the eastern half. AutoKNN exhibits a dry bias extending from the Eastern U.S. to include the Northern U.S. as well, while Salient 2.0 displays a strong dry bias across the entire contiguous U.S. that is especially pronounced in the eastern half.

The Prophet model is noticeably less biased than the other evaluated models; however, this bias reduction does not immediately translate into improved performance, as Prophet is outperformed by ABC models with larger bias in all four tasks. This indicates that Prophet's reduced bias comes at a cost of unnecessarily high variance relative to the dominating ABC model forecasts.

### GraphCast Comparison Details

To generate a weeks 3-4 GraphCast forecast of temperature and precipitation for a given target date, we

1. downloaded the pretrained GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 - mesh 2to6 - precipitation input and output model weights from https://console.cloud.google.com/storage/browser/dm_graphcast,
2. initialized the model with the three days of ECMWF Reanalysis v5 (ERA5) data preceding the associated issuance date, as described in https://github.com/google-deepmind/graphcast,
3. repeatedly ran the model (using autoregressive rollout as in https://github.com/google-deepmind/graphcast/blob/main/graphcast_demo.ipynb) for the maximum supported number of time steps (12) and reinitialized the model with its own forecasted output until \(30\) days of forecasts had been produced,

Figure 17: Model bias when forecasting temperature in the contiguous U.S. over 2011–2020.

Figure 18: Model bias when forecasting precipitation in the contiguous U.S. over 2011–2020.

4. aggregated the 2m_temperature and total_precipitation_6hr forecasts over days 14-28,
5. regridded the aggregated forecasts to our standard \(1\times 1^{\circ}\) grid using xarray interp_like[22], and
6. converted temperature to degrees Celsius and precipitation to millimeters.

Since GraphCast was trained on data through 2017, Table 7 summarizes the performance of GraphCast across the post-training years 2018-2020. Similarly to the deep learning models previously evaluated in Table 1, GraphCast outperforms debiased CFSv2 in terms of skill, underperforms debiased CFSv2 in terms of RMSE, and strongly underperforms the ABC ensemble models in both metrics when forecasting either temperature or precipitation in weeks \(3\)-\(4\).

Figure 19 compares the spatial improvement of GraphCast over debiased CFSv2 when forecasting input in the contiguous U.S.) of GraphCast versus the ABC ensemble models across 2018-2020. For

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline  & & \multicolumn{2}{c}{RMSE \(\%\) Improvement} & \multicolumn{2}{c}{Average \(\%\) Skill} \\ \cline{3-6} Group & Model & Temperature & Precipitation & Temperature & Precipitation \\ \hline Baselines & Deb. CFSv2 & – & – & 19.98\(\pm\)4.84 & 3.12\(\pm\)2.99 \\ \cline{2-6} Learning & GraphCast & \(-\)34.96\(\pm\)7.46 & \(-\)34.26\(\pm\)7.57 & 20.17\(\pm\)4.70 & 5.55\(\pm\)2.81 \\ \cline{2-6} Ensembles & Uniform ABC & **9.70\(\pm\)2.19** & **8.41\(\pm\)1.18** & 34.37\(\pm\)4.58 & **18.34\(\pm\)2.59** \\  & Online ABC & 9.37\(\pm\)2.07 & 8.37\(\pm\)1.25 & **34.67\(\pm\)4.71** & 18.07\(\pm\)2.76 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Average percentage skill and percentage improvement over mean debiased CFSv2 RMSE across 2018–2020 in the contiguous U.S. along with a 95% bootstrap confidence interval. The best performing model overall is shown in green.

Figure 19: Percentage improvement over mean debiased CFSv2 RMSE when forecasting temperature or precipitation in the contiguous U.S. over 2018–2020. White grid points indicate negative or 0% improvement, and the mean percentage improvement is given in parentheses.

precipitation, GraphCast has a substantially drier bias than the ABC models throughout most of the contiguous U.S. For temperature, GraphCast has a warmer bias in the West.

### Western U.S. Competition Results

Table 8 displays the performance improvement over the baseline debiased CFSv2 in the Western U.S., over 26 dates in 2019-2020. This region was the focus of the Subseasonal Forecast Rodeo I, and we include the performance of the three best contestants in the table. The ABC ensembles were either very competitive or better than the other contestants.

\begin{table}
\begin{tabular}{l l r r r r} \hline \hline Group & Model & Temp. weeks 3-4 & Temp. weeks 5-6 & Precip. weeks 3-4 & Precip. weeks 5-6 \\ \hline Contest baselines & Salient & — & — & 11.10 & 7.02 \\  & Climatology & 10.22 & \(-0.76\) & 5.82 & 2.25 \\ \cline{2-6} Contestants & \(1^{st}\) place & **17.12** & **8.47** & **11.54** & **8.63** \\  & \(2^{nd}\) place & 16.67 & 7.04 & 11.10 & 8.03 \\  & \(3^{rd}\) place & 15.47 & 6.90 & 10.62 & 7.94 \\ \cline{2-6} Learning & AutoKNN & 13.09 & 2.90 & 7.50 & 3.05 \\  & LocalBoosting & 12.85 & 4.09 & 7.25 & 3.71 \\  & MultiLLR & 9.54 & 1.12 & 8.95 & 4.58 \\  & Prophet & **15.68** & **6.86** & 6.88 & 3.40 \\  & Salient 2.0 & 11.15 & 2.91 & **12.65** & **8.56** \\ \cline{2-6} ABC & Climatology++ & 15.54 & 6.43 & 8.35 & 4.69 \\  & CFSv2++ & 6.67 & **9.26** & **8.70** & **5.51** \\  & Persistence++ & **16.59** & 8.27 & 8.20 & 4.51 \\ \cline{2-6} Ensembles & Uniform ABC & 14.96 & **9.58** & 9.31 & 5.89 \\  & Uniform ABC + Learning & 15.89 & 8.79 & 10.43 & 6.79 \\  & Online ABC & **16.71** & 8.70 & 8.85 & 5.19 \\  & Online ABC + Learning & 14.70 & 7.97 & **12.52** & **8.18** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Percentage improvement over mean debiased CFSv2 RMSE over \(26\) contest dates (2019-2020) in the Western U.S. The best performing models within each class of models are shown in bold, while the best performing models overall are shown in green.

Figure 20: Bias of GraphCast vs. ABC ensembles when forecasting temperature or precipitation in the contiguous U.S. over 2018–2020.

### Salient 2.0 Dry Bias

We hypothesized that the exceptional Western U.S. contest performance of Salient 2.0 in Table 8 was due in part to the dry bias observed in Figure 18, as 2020 was an unusually dry year in the Western U.S. To explore this hypothesis, we focus on forecasting precipitation weeks 3-4 in the Western U.S. and display in Figure 21 (left) the percentage improvement of Salient 2.0 and CFSv2++ over debiased CFSv2 alongside the inverse total precipitation each year in 2011-2020. As anticipated, the steep decrease in cumulative precipitation for 2020 is accompanied by a steep increase in Salient 2.0 predictive accuracy. In addition, the rises and falls in total precipitation track the accuracy of Salient 2.0 well but appear largely unassociated with the performance curve of other accurate models like CFSv2++. The scatter plots and best-fit lines of Figure 21 (right) paint a similar picture. Salient 2.0 exhibits a distinctly negative correlation between percentage improvement and total precipitation in the Western U.S., while this relationship is absent for other accurate models like CFSv2++.

The developers of the Rodeo I Salient model attribute this dry bias to the log-normal distribution of precipitation, which leads to more frequent anomalously dry conditions than anomalously wet conditions and in turn encourages drier model forecasts [58].

More recent versions of the Salient model mitigate this bias by training on seasonal anomalies in place of raw temperature and precipitation values [58].

Figure 21: Temporal plot (left) and scatter plot (right) of yearly total precipitation and percentage improvement over mean debiased CFSv2 RMSE in the Western U.S. across 2011–2020.