# Space-Time Continuous PDE Forecasting using Equivariant Neural Fields

David M. Knigge\({}^{*,1}\), David R. Wessels\({}^{*,1}\), Riccardo Valperga\({}^{1}\), Samuele Papa\({}^{1}\), Jan-Jakob Sonke\({}^{2}\),

**Efstatios Gavves \({}^{\dagger,1}\), Erik J. Bekkers \({}^{\dagger,1}\)**

\({}^{1}\)University of Amsterdam \({}^{2}\) Netherlands Cancer Institute

d.m.knigge@uva.nl, d.r.wessels@uva.nl

###### Abstract

Recently, Conditional Neural Fields (NeFs) have emerged as a powerful modelling paradigm for PDEs, by learning solutions as flows in the latent space of the Conditional NeF. Although benefiting from favourable properties of NeFs such as grid-agnosticity and space-time-continuous dynamics modelling, this approach limits the ability to impose known constraints of the PDE on the solutions - e.g. symmetries or boundary conditions - in favour of modelling flexibility. Instead, we propose a space-time continuous NeF-based solving framework that - by preserving geometric information in the latent space - respects known symmetries of the PDE. We show that modelling solutions as flows of pointclouds over the group of interest \(G\) improves generalization and data-efficiency. We validated that our framework readily generalizes to unseen spatial and temporal locations, as well as geometric transformations of the initial conditions - where other NeF-based PDE forecasting methods fail - and improve over baselines in a number of challenging geometries.

## 1 Introduction

Partial Differential Equations (PDEs) are a foundational tool in modelling and understanding spatio-temporal dynamics across diverse scientific domains. Classically, PDEs are solved using numerical methods such as finite elements, finite volumes, or spectral methods. In recent years, Deep Learning (DL) methods have emerged as promising alternatives due to abundance of observed and simulated data as well as the accessibility to computational resources, with applications ranging from fluid simulations and weather modelling [51; 7] to biology [33].

Figure 1: We propose to solve an equivariant PDE in function space by solving an equivariant ODE in latent space. Through our proposed framework, which leverages _Equivariant Neural Fields_\(f_{\theta}\), a field \(\nu_{t}\) is represented by a set of latents \(z^{\nu}_{t}=\{(p^{\nu}_{i},\mathbf{c}^{\nu}_{i})\}_{i=1}^{N}\) consisting of a _pose_\(p_{i}\) and context vector \(\mathbf{c}_{i}\). Using meta-learning, the initial latent \(z^{\nu}_{0}\) is fit in only 3 SGD steps, after which an equivariant neural ODE \(F_{\psi}\) models the solution as a latent flow.

The systems modelled by PDEs often have underlying symmetries. For example, heat diffusion or fluid dynamics can be modeled with differential operators which are rotation equivariant, e.g., given a solution to the system of PDEs, its rotation is also a valid solution 1. In such scenarios it is sensible, and even desirable, to design neural networks that incorporate and preserve such symmetries to improve generalization and data-efficiency [12; 48; 4].

Footnote 1: Assuming boundary conditions are symmetric, i.e. they transform according to the relevant group action.

Crucially, DL-based approaches often rely on data sampled on a regular grid, without the inherent ability to generalize outside of it, which is restrictive in many scenarios [40]. To this end, [51] propose to use Neural Fields (NeFs) for modelling and forecasting PDE dynamics. This is done by fitting a neural ODE [11] to the conditioning variables of a conditional Neural Field trained to reconstruct states of the PDE [13]. However, this approach fails to leverage aforementioned known symmetries of the system. Furthermore, using neural fields as representations has proved difficult due to the non-linear nature of neural networks [13; 3; 35], limiting performance in more challenging settings. We posit that NeF-based modelling of PDE dynamics benefits from representations that account for the symmetries of the system as this allows for introducing inductive biases into the model that ought to be reflected in solutions. Furthermore, we show that through meta-learning [28; 45] the NeF backbone improves performance for complex PDEs by further structuring the NeF's latent space, simplifying the task of the neural ODE.

We introduce a framework for _space-time continuous equivariant PDE solving_, by adapting a class of \(\mathrm{SE}(\mathrm{n})\)-Equivariant Neural Fields (ENFs) to PDE-specific symmetries. We leverage the ENF as representation for modelling spatiotemporal dynamics. We solve PDEs by learning a flow in the latent space of the ENF - starting at a point \(z_{0}\) corresponding to the initial state of the PDE - with an equivariant graph-based neural ODE [11] we develop from previous work [5]. We extend the ENF to equivariances beyond \(\mathrm{SE}(\mathrm{n})\), by extending its weight-sharing scheme to equivalance classes for specific symmetries relevant to our setting. Furthermore, we show how meta-learning [14; 28; 45; 13], can not only significantly reduce inference time of the proposed framework, but also substantially simplify the structure of the latent space of the ENF, thereby simplifying the learning process of the latent dynamics for the neural ODE model. We present the following contributions:

* We introduce a framework for spatio-temporally continuous PDE solving that respects known symmetries of the PDE through equivariance constraints.
* in terms of MSE
- in spatio-temporally continuous settings, i.e. evaluated _off_ the training grid and beyond the training horizon.
* We show how meta-learning improves the structure of the latent space of the ENF, simplifying the learning process, leading to better performance in solving PDEs.

We structure the paper as follows: in Sec. 2 we provide an overview of the mathematical preliminaries and describe the problem setting. Our proposed framework is introduced in Sec. 3. We validate our framework on different PDEs defined over a variety of geometries in Sec. 4, with differing equivariance constraints, showing competitive performance over other neural PDE solvers.We provide an in-depth positioning of our approach in relation to other work in Appx. A.

## 2 Mathematical background and problem setting

Continuous spatiotemporal dynamics forecasting.The setting considered is data-driven learning of the dynamics of a system described by continuous observables. In particular, we consider flows of fields, denoted with \(\dot{\nu}:\mathbb{R}^{d}\times[0,T]\rightarrow\mathbb{R}^{c}\). We use \(\dot{\nu}_{\mathfrak{t}}\) as a shorthand for \(\dot{\nu}(\cdot,t)\). We assume the flow is governed by a PDE, and consider the Initial Value Problem (IVP) of predicting \(\dot{\nu}_{t}\) from a given \(\nu_{0}\). The dataset consists of field snapshots \(\nu:\mathcal{X}\times[T]\rightarrow\mathbb{R}^{c}\), in which \([T]:=1,2,\ldots,T\) denotes the set of time points on which the flow is sampled and \(\mathcal{X}\subset\mathbb{R}^{d}\) is a set of coordinate values. For each time point we are given a set of input-output pairs \([\mathcal{X},\nu(\mathcal{X})]\) where \(\nu(\mathcal{X})\subset\mathbb{R}^{c}\) are the values of the field at those coordinates. Importantly, the location at which the field is sampled need not be regular, i.e., we do not require the training data to be on a grid or to be regularly spaced in time, nor need coordinate values be identical for train and test sets. Following [51], we distinguish between \(t_{\text{in}}\) - referring to values within the training time horizon \([0,T]\) - and \(t_{\text{out}}\) - analogously to values beyond \(T\)Neural Fields in dynamics modelling.Conditional Neural fields (NeFs) are a class of coordinate-based neural networks, often trained to reconstruct directly-sampled input continuously. More specifically, a conditional neural field \(f_{\theta}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{d}\) is a field -parameterized by a neural network with parameters \(\theta\)- that maps input coordinates \(x\in\mathbb{R}^{n}\) in the data domain alongside conditioning latents \(z\) to \(d\)-dimensional signal values \(\nu(x)\in\mathbb{R}^{d}\). By associating a conditioning latent \(z^{\nu}\in\mathbb{R}^{c}\) to each signal \(\nu\), a single conditional NeF \(f_{\theta}:\mathbb{R}^{n}\times\mathbb{R}^{c}\rightarrow\mathbb{R}^{d}\) can learn to represent families \(\mathcal{D}\) of continuous signals such that \(\forall\,\nu\in\mathcal{D}:f(x)\approx f_{\theta}(x;z^{\nu})\). [51] propose to use conditional NeFs for PDE modelling by learning a continuous flow in the latent space of a conditional neural field. In particular, a set of latents \(\{z^{\nu}_{i}\}_{i=1}^{T}\) are obtained by fitting a conditional neural field to a given set of observations \(\{\nu_{i}\}_{i=1}^{T}\) at timesteps \(1,...,T\); simultaneously, a neural ODE [11]\(F_{\psi}\) is trained to map pairs of temporally contiguous latents s.t. solutions correspond to the trajectories traced by the learned latents. Though this approach yields impressive results for sparse and irregular data in planar PDEs, we show it breaks down on complex geometries. We hypothesize that this is due to lack of a latent space that preserves relevant geometric transformations that characterize the symmetries of the systems we are modelling, and as such propose an extension of this framework where such symmetries are preserved.

Symmetries and weight sharing.Given a group \(G\) with identity element \(e\in G\), and a set \(X\), a _group action_ is a map \(\mathcal{T}:G\times X\to X\). For simplicity, we denote the action of \(g\in G\) on \(x\in X\) as \(gx:=\mathcal{T}(g,x)\), and call _\(G\)-space_ a smooth manifold equipped with a \(G\) action. A group action is homomorphic to \(G\) with its group product, namely it is such that \(ex=x\) and \((gh)x=g(hx)\). As an example, we are interested in the Special Euclidean group \(\mathrm{SE}(\mathrm{n})\)=\(\mathbb{R}^{n}\rtimes SO(n)\): group elements of \(\mathrm{SE}(\mathrm{n})\) are identified by a translation \(t\in\mathbb{R}^{n}\) and rotations \(\mathbf{R}\in SO(n)\) with group operation \(gg^{\prime}=\left(t,\mathbf{R}_{\theta}\right)\left(t^{\prime},\mathbf{R}_{ \theta^{\prime}}\right)=\left(\mathbf{R}\mathbf{x}^{\prime}+\mathbf{x},\mathbf{ R}\mathbf{R}_{\theta^{\prime}}\right)\); We denote by \(\mathcal{L}_{g}\) the left action of \(G\) on function spaces defined as \(\mathcal{L}_{g}f(\mathbf{x}^{\prime})=f(g^{-1}\mathbf{x}^{\prime})=f(\mathbf{ R}_{\theta}^{-1}(\mathbf{x}^{\prime}-\mathbf{x}))\). Many PDEs are defined by _equivariant_ differential operators such that for a given state \(\nu\): \(\mathcal{L}_{g}\mathcal{N}[\nu]=\mathcal{N}[\mathcal{L}_{g}\nu]\). If the boundary conditions do not break the symmetry, namely if the boundary is symmetric with respect to the same group action, then a \(G\)-transformed solution to the IVP for some \(\nu_{0}\) corresponds to the solution for the \(G\)-transformed initial value. For example, laws of physics do not depend on the choice of coordinate system, this implies that many PDEs are defined by \(\mathrm{SE}(\mathrm{n})\)-equivariant differential operators. The _geometric deep learning_ literature shows that models can benefit from leveraging the inherent symmetries or invariances present in the data by constraining the searchable function space through _weight sharing_[9, 25, 5]. Recall that in our framework we model flows of fields, solutions to PDEs defined by equivariant differential operators, with ordinary differential equations in the latent space of conditional neural fields. We leverage the symmetries of the system for two key aspects of the proposed method: first by making the relation between signals and corresponding latents equivariant; second, by using equivariant ODEs, namely ODEs defined by equivariant vector fields: if \(\frac{d\dot{z}}{d\tau}\)=\(F(z)\) is such that \(F\left(gz\right)=gF\left(z\right)\), then solutions are mapped to solutions by the group action.

## 3 Method

We adapt the work of [51], and consider the following optimization problem 2:

Footnote 2: We highlight that [51] optimize latents \(z^{\nu}_{t}\), neural field \(f_{\theta}\), and ODE \(F_{\psi}\) using two separate objectives. We instead found that our framework is more stable under single-objective optimization.

\[\underset{\theta,\psi,z_{\tau}}{\text{min}}\ \ \mathbb{E}_{\nu\in D,x\in \mathcal{X},t\in[T]}\left\|\nu_{t}(x)-f_{\theta}(x;z^{\nu}_{t})\right\|_{2}^{2}, \quad\text{where}\quad z^{\nu}_{t}=z^{\nu}_{0}+\int_{0}^{t}F_{\psi}(z^{\nu}_ {\tau})d\tau\,,\] (1)

with \(f_{\theta}(x;z^{\nu}_{t})\) a decoder tasked with reconstructing state \(\nu_{t}\) from latent \(z^{\nu}_{t}\) and \(F_{\psi}\) a neural ODE that maps a latent to its temporal derivative: \(\frac{dz^{\nu}_{\tau}}{d\tau}\)=\(F_{\psi}(z^{\nu}_{\tau})\), modelling the solution as flow in latent space starting at the initial latent \(z^{\nu}_{0}\) - see Fig. 1 for a visual intuition.

Equivariant space-time continuous dynamics forecasting.A PDE defined by a \(G\)-equivariant differential operator - for which \(\mathcal{L}_{g}\mathcal{N}[\nu]=\mathcal{N}[\mathcal{L}_{g}\nu]\) - are such that solutions are mapped to other solutions by the group action if the boundary conditions are symmetric. We would like to leverage this property, and _constrain_ the neural ODE \(F_{\psi}\) such that the solutions it finds in latent space can be mapped onto each other by the group action. Our motivation for this is twofold: (1) it is natural for our model to have, by construction, the geometric properties that the modelled system is known to posses - (2) to get more structured latent representations and facilitate the job of the neural ODE. To achieve this we first need the latent space \(Z\) to be equipped with a well-defined group action with respect to which \(\forall g\in G,z\in Z:F_{\psi}(gz)=\)\(gF_{\psi}(z)\), and, most importantly, we need the relation between the reconstructed field and the corresponding latent to be equivariant, i.e.,

\[\forall g\in G\,,\,x\in\mathcal{X}:\mathcal{L}_{g}f_{\theta}(x;z_{t}^{\nu})=f_ {\theta}(g^{-1}x;z_{t}^{\nu})=f_{\theta}(x;gz_{t}^{\nu}).\] (2)

Note that, somewhat imprecisely, we call this condition _equivariance_ to convey the idea even though it is not, strictly speaking, the commonly used definition of equivariance for general operators. If we consider the decoder as a mapping from latents to fields, we can make the notion of equivariance of this mapping more precise. Namely

\[f(x)=D_{\theta}(z),D_{\theta}(z):z_{t}^{\nu}\mapsto f_{\theta}(\cdot;z_{t}^{\nu })\,,f(g^{-1}x)=D_{\theta}(gz),D_{\theta}(gz):g\,z_{t}^{\nu}\mapsto f_{\theta }(g^{-1}\,\cdot;z_{t}^{\nu})\,.\] (3)

In Sec. 3.1 we describe the Equivariant Neural Field (ENF)-based decoder, which satisfies equation (2). Second, in Sec. 3.2 we outline the graph-based equivariant neural ODE. Sec. 3.3 explains the motivation for- and use of- meta-learning for obtaining the ENF backbone parameters. We show how the combination of equivariance and meta-learning produce much more structured latent representations of continuous signals (Fig. 3).

### Representing PDE states with Equivariant Neural Fields

We briefly recap ENFs here, referring the reader to [49] for more detail. We extend ENFs to symmetries for PDEs over varying geometries.

ENFs as cross-attention over bi-invariant attributes.Atention-based conditional neural fields represent a signal \(\nu\in\mathcal{D}\) with a corresponding _latent set_\(z^{\nu}\)[52]. This class of conditional neural fields obtain signal-specific reconstructions \(\nu(x)\approx f_{\theta}(x;z^{\nu})\) through a cross-attention operation between the latent set \(z^{\nu}\) and input coordinates \(x\). ENFs [49] extend this approach by imposing equivariance constraints w.r.t a group \(\mathrm{G}\subseteq\mathrm{SE}(\mathrm{n})\) on the relation between the neural field and the latents such that transformations to the signal \(\nu\) correspond to transformation of the latent \(z^{\nu}\) (Eq. (2)). For this condition to hold, we need a well-defined action on the latent space \(Z\) of \(f_{\theta}\). To this end, ENFs define elements of the latent set \(z^{\nu}\) as tuples of _pose_\(p_{i}\in\mathrm{G}\) and _context_\(\mathbf{c}_{i}\in\mathbb{R}^{d}\), \(z^{\nu}:=\{(p_{i},\mathbf{c}_{i})\}_{i=1}^{N}\). The latent space is then equipped with a group action defined as \(gz=\{(gp_{i},\mathbf{c}_{i})\}_{i=1}^{N}\). To achieve equivariance over transformations ENFs follow [5] where equivariance is achieved with convolutional _weight-sharing_ over equivalence classes of points pairs \(x,x^{\prime}\). ENFs instead extend weight-sharing to cross-attention over _bi-invariant_ attributes of \(z,x\) pairs.

Weight-sharing over bi-invariant attributes of \(z,x\) is motivated by Eq. 2, by which we have:

\[f_{\theta}(x;z)=f_{\theta}(gx;gz).\] (4)

Intuitively, the above equation says that a transformation \(g\) on the domain of \(f_{\theta}\), i.e. \(g^{-1}x\), can be undone by _also_ acting with \(g\) on \(z\). In other words, the output of the neural field \(f_{\theta}\) should be _bi-invariant_ to \(g-\)transformations of the pair \(z,x\). For a specific pair \((z_{i},x_{m})\in Z\times X\), the term bi-invariant attribute \(\mathbf{a}_{i,m}\) describes a function \(\mathbf{a}:(z_{i},x_{m})\mapsto\mathbf{a}(z_{i},x_{m})\) such that \(\mathbf{a}(z_{i},x_{m})=\mathbf{a}(gz_{i},gx_{m})\). Thorughout the paper we use \(\mathbf{a}_{i,m}\) as shorthand for \(\mathbf{a}(z_{i},x_{m})\).

To parameterize \(f_{\theta}\), we can accordingly choose any function that is bi-invariant to \(G-\)transformations of \(z,x\). In particular, for an input coordinate \(x_{m}\) ENFs choose to make \(f_{\theta}\) a cross-attention operation between attributes \(\mathbf{a}_{i,m}\) and the invariant context vectors \(\mathbf{c}_{i}\):

\[f_{\theta}(x_{m},z)=\mathrm{cross\_attn}(\mathbf{a}_{\cdot;m},\mathbf{c}_{ \cdot;},\mathbf{c}_{\cdot})\] (5)

As an example, for \(\mathrm{SE}(\mathrm{n})\)-equivariance, we can define the bi-invariant simply using the group action: \(\mathbf{a}_{i,m}^{\mathrm{SE}(\mathrm{n})}=p_{i}^{-1}x_{m}=\mathbf{R}_{i}^{T} (x_{m}-x_{i})\), which is bi-invariant by:

\[\forall g\in\mathrm{SE}(\mathrm{n}):\ \ (p_{i},x)\ \mapsto\ (g\,p_{i},g\,x)\quad \Leftrightarrow\quad p_{i}^{-1}x\ \mapsto\ (g\,p_{i})^{-1}g\,x=p_{i}^{-1}g^{-1}g\,x=p_{i}^{-1}x\,.\] (6)

Figure 2: The proposed framework respects predefined symmetries of the PDE: a rotated solution \(\mathcal{L}_{g}\nu_{T}\) may be obtained either by solving from latent \(z_{0}^{\nu}\) (top-left) and transforming the solution \(z_{T}^{\nu}\) (top-right) to \(gz_{T}^{\nu}\) (bottom-right) or transforming \(z_{0}^{\nu}\) to \(gz_{0}^{\nu}\) (bottom-left) and solving this.

Bi-invariant attributes for PDE solving.As explained above, ENF is equivariant to \(\mathrm{SE(n)}\)-transformations by defining \(f_{\theta}\) as a function of an \(\mathrm{SE(n)}\)\(-\)bi-invariant attribute \(\mathbf{a}^{\mathrm{SE(n)}}\). Although many physical processes adhere to roto-translational symmetries, we are also interested in solving PDEs that - due to the geometry of the domain, their specific formulation, and/or their boundary conditions - are not fully \(\mathrm{SE(n)}\)\(-\)equivariant. As such, we are interested in extending ENFs to equivariances that are not strictly (subsets of) \(\mathrm{SE(n)}\), which we show we can achieve by finding bi-invariants that respect these particular transformations. Below, we provide two examples, the other invariants we use in the experiments - including a "bi-invariant" \(\mathbf{a}^{\underline{0}}\) that is not actually bi-invariant to any geometric transformations, which we use to ablate over equivariance constraints - are in Appx. D.

_The flat 2-torus._ When the physical domain of interest is continuous and extends indefinitely, periodic boundary conditions are often used, i.e. the PDE is defined over a space topologically equivalent to that of the 2-torus. Such boundary conditions break \(\mathrm{SO(2)}\) symmetries; assuming the domain has periodicity \(\pi\) and none of the terms of this PDE depend on the choice of coordinate frame, these boundary conditions imply that the PDE is equivariant to periodic translations: the group of translations modulo \(\pi\): \(\mathbb{T}^{2}\equiv\mathbb{R}^{2}/\mathbb{Z}^{2}\). In this case, periodic functions over \(x,y\) with periods \(\pi\) would work as a bi-invariant, i.e. using poses \(p\in\mathbb{T}^{2}\), \(\mathbf{a}^{\mathbb{T}^{2}}=\cos(2\pi(x_{0}-p_{0}))+\cos(2\pi(x_{1}-p_{1}))\) - which happens to be bi-invariant to rotations by \(\frac{\pi}{2}\) as well. Instead, since we do not assume any rotational symmetries to exist on the torus, we opt for a non-rotationally symmetric function:

\[\mathbf{a}_{i,m}^{\mathbb{T}^{2}}=\cos(2\pi(x_{i}^{0}-p_{i}^{0}))\oplus\cos(2 \pi(x_{i}^{1}-p_{i}^{1})),\] (7)

where \(\oplus\) denotes concatenation. This bi-invariant is used in experiments on Navier-Stokes over the flat 2-Torus.

_The 2-sphere._ In some settings a PDE may be symmetric only to rotations along a certain axes. An example is that of the global shallow-water equations on the two-sphere - used to model geophysical processes such as atmospheric flow [16], which are characterised by rotational symmetry only along the earth's axis of rotation due to inclusion of a term for Coriolis acceleration that breaks full \(\mathrm{SO(3)}\) equivariance. We use poses \(p\in\mathrm{SO(3)}\) parametrised by Euler angles \(\phi,\theta,\gamma\), and spherical coordinates \(\phi,\theta\) for \(x\in S^{2}\). We make the first two Euler angles coincide with the spherical coordinates and define a bi-invariant for rotations around the axis \(\theta=\pi\).

\[\mathbf{a}_{i,m}^{\text{SW}}=\Delta\phi_{p_{i},x_{m}}\oplus\theta_{p_{i}} \oplus\gamma_{p_{i}}\oplus\theta_{x_{m}},\] (8)

where \(\Delta\phi_{p_{i},x_{m}}\)\(=\)\(\phi_{p_{i}}\)\(-\)\(\phi_{x_{m}}\)\(-\)\(2\pi\) if \(\phi_{p_{i}}\)\(-\)\(\phi_{x_{m}}\)\(>\)\(\pi\) and \(\Delta\phi_{p_{i},x_{m}}\)\(=\)\(\phi_{p_{i}}\)\(-\)\(\phi_{x_{m}}\)\(+\)\(2\pi\) if \(\phi_{p_{i}}\)\(-\)\(\phi_{x_{m}}\)\(<\)\(-\pi\), to adjust for periodicity.

In summary, to parameterize an ENF equivariant with respect to a specific group we are simply required to find attributes that are bi-invariant with respect to the same group. In general we achieve this by using group-valued poses and their action on the PDE domain.

### PDE solution as latent space flow

Let \(z_{0}^{\nu}\) be a latent set that faithfully reconstructs the initial state \(\nu_{0}\). We want to define a neural ODE \(F_{\psi}\) that map latents \(z_{t}^{\nu}\) to their temporal derivatives \(\frac{dz_{\tau}^{\nu}}{d\tau}\)\(=\)\(F_{\psi}(z_{\tau}^{\nu})\) that is equivariant with respect to the group action: \(gF_{\psi}(z_{\tau}^{\nu})\)\(=\)\(F_{\psi}(gz_{\tau}^{\nu})\). To this end, we use a _message passing neural network_ (MPNN) to learn a flow of poses \(p_{i}\) and contexts \(\mathbf{c}_{i}\) over time. We base our architecture on P@NITA [5], which employs convolutional weight-sharing over bi-invariants for \(\mathrm{SE(n)}\). For an in-depth recap of message-passing frameworks, we refer the reader to Appx. A. Since \(F_{\psi}\) is required to be equivariant w.r.t. the group action, any updates to the poses \(p_{i}\) should also be equivariant. [41] proposes to parameterize an equivariant node position update by using a basis spanned by relative node positions \(x_{j}-x_{i}\). In our setting, poses \(p_{i}\) are points on a manifold \(M\) equipped with a group action. As such, we analogously propose parameterizing pose updates by a weighted combination of logarithmic maps \(\log_{p_{i}}(p_{j})\), which intuitively describe the relative position between \(p_{i},p_{j}\) in the tangent space \(T_{p_{i}}M\), or the displacement from \(p_{i}\) to \(p_{j}\). We integrate the resulting pose update over the manifold through the exponential map \(\exp_{p_{i}}\). In the euclidean case \(\log_{p_{i}}(p_{j})\)\(=\)\(x_{j}-x_{i}\) and we get back node position updates per [41]. In short, the message passing layers we use consist of the following update functions:

\[\mathbf{c}_{i}^{l+1}=\sum_{(p_{j},\bm{e}_{j})\in z^{\nu,l}}k^{\text{ context}}(\mathbf{a}_{i,j}^{l})\mathbf{c}_{j}^{l},\quad p_{i}^{l+1}=\exp_{p_{i}^{l}} \bigg{(}\frac{1}{N}\sum_{(p_{j}^{l},\bm{e}_{j}^{l})\in z^{\nu,l}}k^{\text{pose}}( \mathbf{a}_{i,j}^{l})\mathbf{c}_{j}^{l}\log_{p_{i}^{l}}(p_{j}^{l})\bigg{)},\] (9)

with \(k^{\text{context}},k^{\text{pose}}\) message functions weighting the incoming context and pose updates, parameterized by a two-layer MLP as a function of the respective bi-invariant.

### Obtaining the initial latent \(z_{0}^{\nu}\)

Until now we've not discussed how to obtain latents corresponding to the initial condition \(z_{0}^{\nu}\). An approach often used in conditional neural field literature is that of autodecoding [36], where latents \(z^{\nu}\) are optimized for reconstruction of the input signal \(\nu\) with SGD. Optimizing a NeF for reconstruction does not necessarily lead to good quality representations [35], i.e. using MSE-based autodecoding to obtain latents \(z_{t}^{\nu}\) - as is proposed by [51] - may complicate the latent space, impeding optimization of the neural ODE \(F_{\psi}\). Moreover, autodecoding requires many optimization steps at inference (for reference, [51] use 300-500 steps). [13] propose meta-learning as a way to overcome long inference times, as it allows for fitting latents in a few steps - typically three or four. We hypothesize that meta-learning may also structure the latent space - similar to the impact of equivariance constraints, since the very limited number of optimization steps requires efficient organization of latents \(z_{t}^{\nu}\) around the (shared) initialization, forcing together the latent representation of contiguous states. To this end, we propose to use meta-learning for obtaining the initial latent \(z_{0}^{\nu}\), which is then unrolled by the neural ode \(F_{\psi}\) to find solutions \(z_{t}^{\nu}\).

### Equivariance and meta-learning structure the latent space \(Z\)

As a first validation of the hypotheses that both equivariance constraints and meta-learning introduce structure to the latent space of \(f_{\theta}\), we visualize latent spaces of different variants of the ENF. We fit ENFs to a dataset consisting of solutions to the heat equation for various initial conditions (details in Appx. E). For each sample \(\nu_{t}\), we obtain a set of latents \(z_{t}^{\nu}\), which we average over the invariant context vectors \(\mathbf{c}_{i}\in\mathbb{R}^{c}\) to obtain a single vector in \(\mathbb{R}^{c}\) invariant to a group action according to the chosen bi-invariant. Next, we apply T-SNE [47] to the resulting vectors in \(\mathbb{R}^{c}\). We use three setups: (a) no meta-learning, \(\theta\) and latents \(z_{t}^{\nu}\) optimized for every \(\nu_{t}\) separately using autodecoding [36], and no equivariance imposed (per Eq. 15), shown in Fig. 2(a). (b) meta-learning is used to obtain \(\theta,z_{t}^{\nu}\), but no equivariance imposed, shown in Fig. 2(b) and (c) meta-learning is used to obtain \(\theta,z_{t}^{\nu}\) and \(\mathrm{SE}(2)\)-equivariance is imposed by weight-sharing over \(\mathbf{a}^{\mathrm{SE}(n)}\) bi-invariants, shown in Fig. 2(c). The results confirm our intuition that both meta-learning and equivariance improve latent-space structure.

Recap: optimization objective.We use a meta-learning inner-loop [28; 13] to obtain the initial latent \(z_{0}^{\nu}\) under supervision of coordinate-value pairs \((x,\nu(x)_{0})_{x\in\mathcal{X}}\) from \(\nu_{0}\). This latent is unrolled for \(t_{\text{train}}\) timesteps using \(F_{\psi}\). The obtained latents are used to reconstruct states \(z_{t}^{\nu}\) along the trajectory

Figure 3: We show the impact of meta-learning and equivariance on the latent space of the ENF when representing trajectories of PDE states. Fig. 2(a) shows a T-SNE plot of the latent space of \(f_{\theta}\) when \(z_{t}^{\nu}\) is optimized with autodecoding, and no weight sharing over bi-invariants is enforced. Fig. 2(b) shows the latent space when meta-learning is used, but no weight sharing is enforced. Fig. 2(c) shows the latent space when \(z_{t}^{\nu}\) are obtained using meta-learning and \(f_{\theta}\) shares weights over \(\mathbf{a}^{\mathrm{SE}(n)}\).

of \(\nu\), and parameters of \(f_{\theta},F_{\psi}\) are optimised for reconstruction MSE, as shown in the left-hand side of Eq. 1. See Appx. B for detailed pseudocode of this process.

## 4 Experiments

We intend to show the impact of symmetry-preservation in continuous PDE solving. To this end we perform a range of experiments assessing different qualities of our model on tasks with different symmetries. First, we investigate the **equivariance properties** of our framework by evaluating it against unseen geometric transformations of the initial conditions. Next, we assess **generalization** and **extrapolation** capabilities w.r.t. unseen spatial locations and time horizons inside and outside the time ranges seen during training respectively, **robustness** to partial test-time observations, and **data-efficiency**. As the continuous nature of NeF-based PDE solving allows, we verify these properties for PDEs defined over **challenging geometries**: the plane \(\mathbb{R}^{2}\), 2-torus \(\mathbb{T}^{2}\) and the sphere \(S^{2}\) and the 3D ball \(\mathbb{B}^{3}\). Architectural details and hyperparameters are in Appx. E. _Code is available on GitHub_.

Additionally, we validate our model on a benchmark of PDEs that exhibits **no transformation symmetries**: the CFDBench [32] benchmark. We include details on parameter counts, memory usage and runtimes of our model compared to baselines in Appx. F.

### Datasets and evaluation

All datasets are obtained by randomly sampling disjoint sets of initial conditions for train and test sets, and solving them using numerical methods. Dataset-specific details on generation can be found in Appx E. **"Heat equation on \(\mathbb{R}^{2}\) and \(S^{2}\).** The heat equation describes diffusion over a surface: \(\frac{dc}{dt}=D\nabla^{2}c\), where \(c\) is a scalar field, and \(D\) is the diffusivity coefficient. We solve it on the 2D plane where \(\nabla^{2}c=\frac{\partial^{2}c}{\partial x_{1}}+\frac{\partial^{2}c}{ \partial x_{2}}\) - and on the 2-sphere \(S^{2}\) where in spherical coordinates: \(\nabla^{2}c=\left(\frac{1}{\sin\theta}\frac{\partial}{\partial\theta}\left( \sin\theta\frac{\partial c}{\partial\theta}\right)+\frac{1}{\sin^{2}\theta} \frac{\partial^{2}c}{\partial\phi^{2}}\right)\). Although a relatively simple PDE, we find that defining it over a non-trivial geometry such as the sphere proves hard for non-equivariant methods. **"Navier-Stokes on \(\mathbb{T}^{2}\)**. We solve 2D Navier Stokes [43] for an incompressible fluid with dynamics \(\frac{dv}{dt}=-u\nabla v+v\Delta\mu+f,v=\nabla\times u,\nabla u=0\), where \(u\) is the velocity field, \(v\) the vorticity, \(\mu\) the viscosity and \(f\) a forcing term (see Appx. E). We create a dataset of solutions for the vorticity using Gaussian random fields as initial conditions. Due to the incompressibility condition, it is natural to solve this PDE with periodic boundary conditions corresponding to the topology of a 2-Torus \(\mathbb{T}^{2}\) - implying equivariance to periodic translation. **"Shallow-water on \(\mathbb{S}^{2}\)**. The global shallow-water equations model large-scale oceanic and atmospheric flow on the globe, derived from Navier-Stokes under assumption of shallow fluid depth. The global shallow-water equations (see Appx. E) include terms for Coriolis acceleration, which makes this problem equivariant to rotation along the globe's axis of rotation. We follow the IVP specified by [16], and create a dataset of paired vorticity-fluid height solutions. **"Internally-heated convection in a 3D ball.** We solve the Boussinesq equation for internally heated convection in a ball, a model relevant for example in the context of the Earth's mantle convection. It involves continuity equations for mass conservation, momentum equations for fluid flow under pressure, viscous forces and buoyancy, and a term modelling heat transfer. We generate initial conditions varying the internal temperature using \(N(0,1)\) noise and obtain solutions for the temperature defined over a regular spherical \(\phi,\theta,r\) grid. **"CFDBench**[32] consists of a set of one-step solutions (pairs of input and output states \(\nu_{t},\nu_{t+1}\)) for classic computational fluid dynamics (CFD) problems, with varying fluid properties, boundary conditions and geometries. The goal of this dataset is to assess generalizability of DL-based PDE solvers over problem parameters. This dataset does not exhibit transformation symmetries because of the absolute position of obstacles in the geometry and orientation of the flow.

Figure 4: A train and test sample from the planar diffusion dataset. Initial conditions for train and test are spikes in disjoint subsets of \(\mathbb{R}^{2}\).

Evaluation.All reported MSE values are for predictions obtained given only the initial condition \(v_{0}\), with std over 3 runs. We evaluate two settings for train and test sets both: **generalization setting** with time evolution happening within the seen horizon during training (\(t_{\text{in}}\)); and, **extrapolation setting** with the time evolution happening outside the seen horizon during training (\(t_{\text{out}}\)). For both cases we measure the mean-squared error (MSE). To position our work relative to competitive data-driven PDE solvers, on the 2D-Navier-Stokes experiment we provide comparisons with a range of baselines. In most other settings these models cannot straightforwardly be applied, and we only compare to [51], to our knowledge the only other fully continuous PDE solving method in literature. For the Navier-Stokes and Internally-Heated Convection experiments, we compare with Transolver [50], which has shown SOTA results as DL-based PDE solving method for general geometries.

Equivariance properties - heat equation on the plane.To verify our framework respects the posed equivariance constraints, we create a dataset of solutions to the heat equation that _requires_ a neural solver to respect equivariance constraints to achieve good performance. Specifically, for initial conditions we randomly insert a pulse of variable intensity in \(x=(x_{1},x_{2})\in\mathbb{R}^{2}\) s.t. \(-1{<}x_{1}{<}1,0{<}x_{2}{<}1\) for the training data and \(-1{<}x_{1}{<}1,-1{<}x_{2}{<}0\) for the test data. Intuitively, train and test sets contain spikes under different disjoint sets of root-translations (see Fig. 4). We train variants of our framework with (\(\mathbf{a}^{\text{SE}(2)}\), Eq. 6) and without (\(\mathbf{a}^{\emptyset}\), Eq. 15) equivariance constraints. In this dataset, we set \(t_{\text{in}}=[0,...,9]\), and evaluation horizon \(t_{\text{out}}=[10,...,20]\). Results in Tab. 1 show that the non-equivariant model, as well as the baseline [51] are unable to successfully solve test initial conditions, whereas the equivariant model performs well.

Robustness to subsampling & time-horizons - Navier-Stokes on the 2-Torus.We perform an experiment assessing the impact of equivariance constraints and meta-learning on robustness to sparse test-time observations of the initial condition. To this end, we train a model with (\(\mathbf{a}^{\text{T}^{2}}\), Eq. 7), without (\(\mathbf{a}^{\emptyset}\), Eq. 15) equivariance constraints, and one with equivariance constraints and without meta-learning (AD \(\mathbf{a}^{\text{T}^{2}}\), Eq. 7), on a fully-observed train set. The training horizon \(t_{\text{in}}=[0,...,9]\), and evaluation horizon \(t_{\text{out}}=[10,...,20]\). Subsequently, we apply the trained model to the problem of solving from sparse initial conditions \(v_{0}\), with observation rates where \(50\%\) and \(5\%\) of the initial condition is observed (Tab. 2). Approaches operating on discrete (CNODE [2]) and regular grids (FNO [29], G-FNO [20]) perform very well when evaluated on fully-observed regular grids, outperforming continuous approaches (ours, [51]). However, we note 

[MISSING_PAGE_FAIL:9]

sets, but overfits to this time horizon, generalizing poorly beyond. We additionally show results for sparsely observed input states, showing the limitations of non NeF-based solvers to generalize over irregular changing observation grids. We interpret these results as an indication of a marked reduction in solving-complexity and improved generalization when correctly accounting for a PDE's symmetries.

Non-symmetric PDEsLastly, we evaluate our model in the setting when solving PDEs that do not exhibit any global symmetries to assess whether the preservation of symmetries in the latent space of our model precludes application to non-symmetric PDEs. We train a translation-equivariant (\(\mathbf{a}^{\mathbb{R}^{2}}\)) model, i.e. one with equivariance properties identical to a CNN, on the Cavity, Dam and Cylinder flows from CFDBench [32]. Comparing to the baselines set by the dataset authors, we see our method improves significantly over a number of classical baselines, despite no global transformational symmetries being present in the problems being solved.

## 5 Limitations & Future work

We're interested in exploring the application of our ENF-based PDE solving framework to larger-scale, more complex problems. Throughout our experiments with ENFs we noticed that more complex signals, e.g. higher resolution PDEs, may be fit easily either by increasing the ENF hidden size or by increasing the number of latents used. However, either of these changes significantly impacts computational complexity, due to the calculation of attention coefficients in the latent space of the ENF for every latent-input coordinate pair. A possible way of addressing this is detailed in [49]; we can approximate the output of the attention operation through limiting the number of latents attended to (using k-nearest neighbours). This may open the door to modelling more complex, larger-scale dynamics than learned in present experiments.

## 6 Conclusion

We introduce a novel equivariant space-time continuous framework for solving partial differential equations (PDEs). Uniquely - our method handles sparse or irregularly sampled observations of the initial state while respecting symmetry-constraints and boundary conditions of the underlying PDE. We clearly show the benefit of symmetry-preservation over a range of challenging tasks, where existing methods fail to capture the underlying dynamics.

## References

* [1] Ilze Amanda Auzina, Cagatay Yildiz, Sara Magliacane, Matthias Bethge, and Efstratios Gavves. Modulated neural odes. _Advances in Neural Information Processing Systems_, 36, 2024.
* [2] Ibrahim Ayed, Emmanuel De Bezenac, Arthur Pajot, and Patrick Gallinari. Learning the spatio-temporal dynamics of physical processes from partial observations. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 3232-3236. IEEE, 2020.
* [3] Matthias Bauer, Emilien Dupont, Andy Brock, Dan Rosenbaum, Jonathan Richard Schwarz, and Hyunjik Kim. Spatial functa: Scaling functa to imagenet classification and generation. _arXiv preprint arXiv:2302.03130_, 2023.

\begin{table}
\begin{tabular}{c c c c} \hline  & Cavity & Dam & Cylinder \\ \hline Identity & 6.42E-02 & 1.69E-03 & 7.54E-02 \\ Auto-FFN & 4.42E-02 & 1.68E-03 & 7.54E-02 \\ Auto-DeepONet & 6.39E-02 & 1.64E-03 & 7.53E-02 \\ Auto-EDeepONet & 6.45E-02 & 1.49E-03 & 7.43E-02 \\ Auto-DeepONetCNN & 6.33E-02 & 1.68E-03 & 7.52E-02 \\ FNO & 2.61E-02 & 8.75E-05 & 1.15E-03 \\ U-Net & 1.58E-02 & 1.70E-03 & 5.49E-05 \\ Ours \(\mathbf{a}^{\mathbb{R}_{2}}\) & **1.10E-02** & **8.19E-05** & **1.42E-05** \\ \hline \end{tabular}
\end{table}
Table 6: Test MSE \(\downarrow\) for CFDBench auto-regressive one forward propagation considering all properties for the cavity, dam and cylinder flows. Baselines taken from [32].

Figure 8: Test samples (top) and corresponding predictions from our model equivariant to \(S^{2}\)-rotations in the ball. (Eq. 14)

* [4] Erik J Bekkers. B-spline cnns on lie groups. In _International Conference on Learning Representations_, 2019.
* [5] Erik J Bekkers, Sharvaree Vadgama, Rob D Hesselink, Putri A van der Linden, and David W Romero. Fast, expressive se \((n)\) equivariant networks through weight-sharing in position-orientation space. _arXiv preprint arXiv:2310.02970_, 2023.
* [6] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geometric and physical quantities improve e (3) equivariant message passing. _arXiv preprint arXiv:2110.02905_, 2021.
* [7] Johannes Brandstetter, Rianne van den Berg, Max Welling, and Jayesh K Gupta. Clifford neural layers for pde modeling. _arXiv preprint arXiv:2209.04934_, 2022.
* [8] Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers. _arXiv preprint arXiv:2202.03376_, 2022.
* [9] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv preprint arXiv:2104.13478_, 2021.
* [10] Keaton J. Burns, Geoffrey M. Vasil, Jeffrey S. Oishi, Daniel Lecoanet, and Benjamin P. Brown. Dedalus: A flexible framework for numerical simulations with spectral methods. _Physical Review Research_, 2(2):023068, April 2020. doi: 10.1103/PhysRevResearch.2.023068.
* [11] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. _Advances in neural information processing systems_, 31, 2018.
* [12] Taco Cohen and Max Welling. Group equivariant convolutional networks. In _International conference on machine learning_, pages 2990-2999. PMLR, 2016.
* [13] Emilien Dupont, Hyunjik Kim, SM Eslami, Danilo Rezende, and Dan Rosenbaum. From data to functa: Your data point is a function and you can treat it like one. _arXiv preprint arXiv:2201.12204_, 2022.
* [14] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
* [15] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In _International Conference on Machine Learning_, pages 3165-3176. PMLR, 2020.
* [16] Joseph Galewsky, Richard K Scott, and Lorenzo M Polvani. An initial-value problem for testing numerical models of the global shallow-water equations. _Tellus A: Dynamic Meteorology and Oceanography_, 56(5):429-440, 2004.
* [17] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* [18] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. _Advances in neural information processing systems_, 32, 2019.
* [19] Xiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady flow approximation. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 481-490, 2016.
* [20] Jacob Helwig, Xuan Zhang, Cong Fu, Jerry Kurtin, Stephan Wojtowytsch, and Shuiwang Ji. Group equivariant fourier neural operators for partial differential equations. _Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023._, 2023.
* [21] Quercus Hernandez, Alberto Badias, David Gonzalez, Francisco Chinesta, and Elias Cueto. Structure-preserving neural networks. _Journal of Computational Physics_, 426:109950, 2021.

* [22] Pengzhan Jin, Zhen Zhang, Aiqing Zhu, Yifa Tang, and George Em Karniadakis. Sympnets: Intrinsic structure-preserving symplectic networks for identifying hamiltonian systems. _Neural Networks_, 132:166-179, 2020.
* [23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [24] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [25] David M Knigge, David W Romero, and Erik J Bekkers. Exploiting redundancy: Separable group convolutional networks on lie groups. In _International Conference on Machine Learning_, pages 11359-11386. PMLR, 2022.
* [26] Miltiadis Miltos Kofinas, Erik Bekkers, Naveen Nagaraja, and Efstratios Gavves. Latent field discovery in interacting dynamical systems with neural fields. _Advances in Neural Information Processing Systems_, 36, 2023.
* [27] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. _arXiv preprint arXiv:2108.08481_, 2021.
* [28] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot learning. _arXiv preprint arXiv:1707.09835_, 2017.
* [29] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020.
* [30] Yongtuo Liu, Sara Magliacane, Miliatidis Kofinas, and Efstratios Gavves. Graph switching dynamical systems. In _International Conference on Machine Learning_, pages 21867-21883. PMLR, 2023.
* [31] Yongtuo Liu, Sara Magliacane, Miliatidis Kofinas, and Efstratios Gavves. Amortized equation discovery in hybrid dynamical systems, 2024.
* [32] Yining Luo, Yingfa Chen, and Zhen Zhang. Cfdbench: A comprehensive benchmark for machine learning methods in fluid dynamics. _arXiv preprint arXiv:2310.05963_, 2023.
* [33] Philipp Moser, Wolfgang Fenz, Stefan Thumfart, Isabell Ganitzer, and Michael Giretzlehner. Modeling of 3d blood flows with physics-informed neural networks: Comparison of network architectures. _Fluids_, 8(2):46, 2023.
* [34] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. _arXiv preprint arXiv:1803.02999_, 2018.
* [35] Samuele Papa, David M Knigge, Riccardo Valperga, Nikita Moriakov, Miltos Kofinas, Jan-Jakob Sonke, and Efstratios Gavves. Neural modulation fields for conditional cone beam neural tomography. _arXiv preprint arXiv:2307.08351_, 2023.
* [36] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 165-174, 2019.
* [37] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* [38] Adeel Pervez, Francesco Locatello, and Efstratios Gavves. Mechanistic neural networks for scientific machine learning. _arXiv preprint arXiv:2402.13077_, 2024.
* [39] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning mesh-based simulation with graph networks. _arXiv preprint arXiv:2010.03409_, 2020.

* Prashhofer et al. [2022] Michael Prashhofer, Tim De Ryck, and Siddhartha Mishra. Variable-input deep operator networks. _arXiv preprint arXiv:2205.11404_, 2022.
* Satorras et al. [2021] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In _International conference on machine learning_, pages 9323-9332. PMLR, 2021.
* Sitzmann et al. [2020] Vincent Sitzmann, Eric Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. Metasdf: Meta-learning signed distance functions. _Advances in Neural Information Processing Systems_, 33:10136-10147, 2020.
* Stokes et al. [1851] George Gabriel Stokes et al. On the effect of the internal friction of fluids on the motion of pendulums. 1851.
* Tancik et al. [2020] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _Advances in neural information processing systems_, 33:7537-7547, 2020.
* Tancik et al. [2021] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2846-2855, 2021.
* Valperga et al. [2022] Riccardo Valperga, Kevin Webster, Dmitry Turaev, Victoria Klein, and Jeroen Lamb. Learning reversible symplectic dynamics. In _Proceedings of The 4th Annual Learning for Dynamics and Control Conference_, volume 168 of _Proceedings of Machine Learning Research_, pages 906-916. PMLR, 23-24 Jun 2022.
* Van der Maaten and Hinton [2008] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* Weiler and Cesa [2019] Maurice Weiler and Gabriele Cesa. General e (2)-equivariant steerable cnns. _Advances in neural information processing systems_, 32, 2019.
* Wessels et al. [2024] David R Wessels, David M Knigge, Samuele Papa, Riccardo Valperga, Efstratios Gavves, and Erik J Bekkers. Grounding continuous representations in geometry: Equivariant neural fields. _ArXiv Preprint arXiv:_, 2024.
* Wu et al. [2024] Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, and Mingsheng Long. Transolver: A fast transformer solver for pdes on general geometries. _arXiv preprint arXiv:2402.02366_, 2024.
* Yin et al. [2022] Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy, and Patrick Gallinari. Continuous pde dynamics forecasting with implicit neural representations. _arXiv preprint arXiv:2209.14855_, 2022.
* Zhang et al. [2023] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dhape2vecset: A 3d shape representation for neural fields and generative diffusion models. _ACM Transactions on Graphics (TOG)_, 42(4):1-16, 2023.
* Zhdanov et al. [2024] Maksim Zhdanov, David Ruhe, Maurice Weiler, Ana Lucic, Johannes Brandstetter, and Patrick Forre. Clifford-steerable convolutional neural networks. _arXiv preprint arXiv:2402.14730_, 2024.
* Zwicker [2020] David Zwicker. py-pde: A python package for solving partial differential equations. _Journal of Open Source Software_, 5(48):2158, 2020. doi: 10.21105/joss.02158. URL https://doi.org/10.21105/joss.02158.

Related work

DL approaches to dynamics modellingIn recent years, the learning of spatiotemporal dynamics has been receiving significant attention, either for modelling interacting systems [31; 30], scientific Machine Learning [51; 8; 7; 38; 26; 53], or even videos [1]. Most DL methods for solving PDEs attempt to directly replace solvers with mappings between finite-dimensional Euclidean spaces, i.e. through the use of CNNs [19; 2] or GNNs [39; 8] often applied autoregressively to an observed (discretized) PDE state. Instead, the Neural Operator (NO) [27] paradigm attempts to learn infinite-dimensional operators, i.e. mappings between function spaces, with limited success. Fourier Neural Operator (FNO) [29] extends this method by performing convolutions in the spectral domain. FNO obtains much improved performance, but due to its reliance on FFT is limited to data on regular grids.

Inductive biases in DL and dynamics modellingGeometric Deep Learning aims to improve model generalization and performance by constraining/designing a model's space of learnable functions based on geometric principles. Prominent examples include Group Equivariant Convolutional Networks and Steerable CNNs [12; 4], generalizations of CNNs that respect symmetries of the data - such as dilations and continuous rotations [48; 15; 25]. Analogously, Graph Neural Networks (GNNs) [24] or Message Passing Neural Networks (MPNNS) [17] are a variant of neural network that respects set-permutations naturally found in graph data. They are typically formulated for graphs \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), with nodes \(i\in\mathcal{V}\) and edges \(\mathcal{E}\). Typically nodes are embedded into a node vector \(f_{i}^{0}\), which is subsequently updated over multiple layers of _message passing_. Message passing consists of (1) computing messages \(m_{i,j}\) over edges \(i,j\) from node \(j\) to \(i\) with the message function (taking into account edge attributes \(a_{i,j}\): \(m_{i,j}=\phi_{m}(f_{i}^{1},f_{j}^{2},a_{i,j})\) (2) aggregating incoming messages: \(m_{i}=\sum_{j\in\mathcal{N}(i)}m_{i,j}\), (3) computing updated node features \(f_{i}^{i+1}=\phi_{u}(f_{i}^{i},m_{i})\).

Recently, such methods have also been adapted for sparse physical data, e.g. for molecular property prediction [41; 6] - where the GNN is additionally required to respect transformation symmetries. [5] unifies these approaches to equivariance under the guise of _weight sharing_ over equivalence classes defined by bi-invariant attributes of pairs of nodes \(i,j\), a viewpoint we leverage in constructing the equivariant conditioning latent \(z_{t}^{\nu}\) corresponding to a PDE state \(\nu_{t}\). In the context of dynamics modelling, equivariant architectures have been employed to incorporate various properties of physical systems in the modelling process, examples of such properties are the symplectic structure [22], discrete symmetries such as reversing symmetries [46] and energy conservation [18; 21].

Neural Fields in dynamics modellingConditional Neural fields (NeFs) are a class of coordinate-based neural networks, often trained to reconstruct discretely-sampled input continuously. More specifically, a conditional neural field \(f_{\theta}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{d}\) is a field -parameterized by a neural network with parameters \(\theta\)- that maps input coordinates \(x\in\mathbb{R}^{n}\) in the data domain alongside conditioning latents \(z\) to \(d\)-dimensional signal values \(f(x)\in\mathbb{R}^{d}\). By associating a conditioning latent \(z^{\nu}\in\mathbb{R}^{c}\) to each signal \(\nu\), a single conditional NeF \(f_{\theta}:\mathbb{R}^{n}\times\mathbb{R}^{c}\rightarrow\mathbb{R}^{d}\) can learn to represent families \(\mathcal{D}\) of continuous signals such that \(\forall\,\nu\in\mathcal{D}:f(\mathbf{x})\approx f_{\theta}(\mathbf{x};\mathbf{ z}^{\nu})\). [13] showed the viability of using the latents \(\mathbf{z}^{i}\) as representations for downstream tasks (e.g. classification, generation) proposing a framework for _learning on neural fields_. This framework inherits desirable properties of neural fields, such as inherent support for sparsely and/or irregularly sampled data, and independence to signal resolution. [51] propose to use conditional NeFs for PDE modelling by learning a continuous flow in the latent space of a conditional neural field. In particular, a set of latents \(\{\mathbf{z}_{i}^{\nu}\}_{i=1}^{T}\) are obtained by fitting a conditional neural field to a given set of observations \(\{\nu_{i}\}_{i=1}^{T}\) at timesteps \(1,...,T\); simultaneously, a neural ODE [11]\(F_{\psi}\) is trained to map pairs of temporally continuous latents s.t. solutions correspond to the trajectories traced by the learned latents. Though this approach yields impressive results for sparse and irregular data in planar PDEs, we show it breaks down on more challenging geometries. We hypothesize that this is due to a lack of a latent space that preserves relevant geometric transformation with respect to which systems we are modelling are symmetric, and as such propose an extension of this framework where such symmetries are preserved.

Obtaining Neural Fields representationsMost NeF-based approach to representation or reconstruction use SGD to optimize (a subset of) the parameters of the NeF, inevitably leading to significant overhead in inference; conditional NeFs require optimizing a (set of) latents from initialization to reconstruct for a novel sample. Accordingly, research has explored ways of addressing this limitation. [42; 45] propose using Meta-Learning [14; 34] to optimize for an initialization for the NeF from which it is possible to reconstruct for a novel sample in as few as 3 gradient descent steps. [13] propose to meta-learn the NeF backbone, but fix the initialization for the latent \(\mathbf{z}\) and instead optimize the learning rate used in its optimization using Meta-SGD [28]. Recently, work has also explored the relation between initialization/optimization of a NeF and its value as downstream representation; [35] show that (1) using a shared NeF initialization and (2) limiting the number of gradient updates to the NeF improves performance in downstream tasks, as this simplifies the complex relation between a NeFs parameter space and its output function space. We combine these insights and make Meta-Learning part of our equivariant PDE solving pipeline, as it enables fast inference and we show it to simplify the latent space of the ENF, improving performance of the neural ODE solver.

Pseudocode for optimization objective

See Alg. 1 for pseudocode of the training loop that we use, written for a single datasample for simplicity of notation. For simplicity, we further assume we're using an euler stepper to solve the neural ODE, but this can be replaced by any solver. For inference, this stratagem is identical, except we do not perform gradient updates to \(\theta,\psi\).

``` Randomly initialize neural field \(f_{\theta}\) Randomly initialize neural ode \(F_{\psi}\) while not done do  Sample initial states and coordinates \(\nu_{0}\).  Initialize latents \(z_{0}^{\nu}\leftarrow\{(p_{i},\mathbf{c}_{i})\}_{i=1}^{N}\). for all step \(\in N_{\text{initial state opt}}=3\)do \(z_{0}^{\nu}\gets z_{0}^{\nu}-\epsilon\nabla_{z_{0}^{\nu}}\mathcal{L}_{ \text{mse}}\big{(}f_{\theta}(\cdot,z_{0}^{\nu}),\nu_{0}\big{)}\big{)}\) endfor for all\(t\in[1,...,t_{\text{in}}]\)do \(z_{t}^{\nu}\gets z_{0}^{\nu}+\int_{0}^{t}F_{\psi}(z_{\tau}^{\nu})d\tau\) endfor Update \(\theta,\psi\) per: \(\theta\leftarrow\theta-\eta\nabla_{\theta}\mathcal{L}_{\text{mse}}^{\prime}\), \(\psi\leftarrow\psi-\eta\nabla_{\psi}\mathcal{L}_{\text{mse}}^{\prime}\) with \(\mathcal{L}_{\text{mse}}^{\prime}=(\big{\{}f_{\theta}(\cdot,z_{t}^{\nu}),\nu_ {t}\big{\}}_{t=0}^{t_{\text{in}}})\) endwhile ```

**Algorithm 1** Optimization objective

## Appendix C Equivariant Neural Fields

ENF to reconstruct PDE statesFor ease of notation we denote \(\mathbf{P}\) and \(\mathbf{C}\) the matrices containing poses and corresponding appearances stacked row-wise, i.e. \(\mathbf{P}_{i,:}=p_{i}^{T}\) and \(\mathbf{C}_{i,:}=\mathbf{c}_{i}^{T}\). Furthermore, we denote \(\mathbf{A}\) as the matrix containing all bi-invariants \(\mathbf{a}_{i,m}\) stacked row-wise, i.e. \(\mathbf{A}_{i,:}=\mathbf{a}_{i,m}^{T}\):

\[f_{\theta}(\mathbf{x};z^{\nu_{t}}):=\mathrm{softmax}\left(\frac{\mathbf{Q}( \mathbf{A})\mathbf{K}^{T}(\mathbf{C})}{\sqrt{d_{k}}}+\mathbf{G}(\mathbf{A}) \right)\mathbf{V}(\mathbf{C};\mathbf{A}),\] (10)

where the softmax is applied over the latent set and with \(d_{k}\) the hidden dimensionality of the ENF. The query matrix \(\mathbf{Q}\) is constructed as \(\mathbf{Q}\)=\(\mathbf{W}_{q}\gamma_{q}^{T}(\mathbf{A})\), \(\gamma_{q}\) a Gaussian RFF embedding [44], followed by a linear layer \(\mathbf{W}_{q}\), i.e. \(\mathbf{Q}\) consists of the RFF embedded bi-invariants of the input coordinate \(x_{m}\) and each of the latent poses \(p_{i}\) stacked row-wise. The key matrix is given by a learnable linear transformation \(\mathbf{W}_{k}\) of the context vectors \(\mathbf{c}_{i}\): \(\mathbf{K}\)=\(\mathbf{W}_{k}\mathbf{C}^{T}\). The attention coefficients which result from the inner product of \(\mathbf{Q},\mathbf{K}\) are weighted by a Gaussian window \(\mathbf{G}\) whose magnitude is conditioned on a distance measure on the relative distance between latent poses and input coordinates as: \(\mathbf{G}_{i}=\sigma_{\text{in}}(||p_{i}-\mathbf{x}||^{2})\), with \(\sigma_{\text{in}}\) a hyperparameter which determines the _locality_ of each of the latents. Finally the value matrix is calculated as a learnable linear transformation \(\mathbf{W}_{v}\) of the appearances \(\mathbf{A}\), conditioned through FiLM modulation [37] by a second RFF embedding of the relative poses split into scale- and shift modulations: \(\mathbf{V}\)=\(\mathbf{W}_{v}\mathbf{A}\odot\mathbf{W}_{v_{0}}\gamma_{v_{0}}(\mathbf{A})+ \mathbf{W}_{v_{g}}\gamma_{v_{0}}(\mathbf{A})\). The latents \(z_{t}^{\nu}\) are optimized for a single state \(\nu_{t}\), whereas the parameters \(\theta\) of the ENF backbone - which consist of all the learnable parameters of the linear \(\mathbf{W}_{q},\mathbf{W}_{k},\mathbf{W}_{v},\mathbf{W}_{v_{0}},\mathbf{W}_{v_{ \beta}}\) used to construct \(\mathbf{Q},\mathbf{K},\mathbf{V}\) - are shared over all states.

The overall architecture consists of a linear layer \(\mathbf{W}\mathbb{R}^{c}\rightarrow\mathbb{R}^{d}\) applied to \(\mathbf{c}_{i}\in\mathbb{R}^{c}\), followed by a layernorm. After this, the cross attention listed above is applied, followed by three \(d\)-dim linear layers, the final one mapping to the output dimension \(\mathbb{R}^{\text{out}}\).

Equivariance follows from sharing \(\mathbf{Q},\mathbf{K},\mathbf{V}\) over equivalence classesNote that the latent space of the ENF is equipped with a group action as: \(gz_{t}^{\nu}=\{(gp_{i},\mathbf{a}_{i})\}_{i=1}^{N}\). As an example, \(\mathrm{SE}(2)\)-equivariance of the ENF follows from bi-invariance of the quantity \(\mathbf{a}\) used to construct \(\mathbf{Q}\) under the group action:

\[\forall g\in SE(n):\ (p_{i},\mathbf{x})\ \mapsto\ (g\,p_{i},g\,\mathbf{x}) \Leftrightarrow p_{i}^{-1}\mathbf{x}\ \mapsto\ (g\,p_{i})^{-1}g\,\mathbf{x}=p_{i}^{-1}g^{-1}g\,\mathbf{x}=p_{i}^{-1}g^{-1}g\,.\] (11)

And so, constructing the matrix containing the relative poses of bi-transformed poses and coordinates \((g\mathbf{P})^{-1}g\mathbf{x}\) as \(((g\mathbf{P})^{-1}g\mathbf{x})_{i,:}=p_{i}^{-1}g^{-1}g\mathbf{x}=p_{i}^{-1} \mathbf{x}\), we trivially have:

\[\forall g\in SE(n):(p_{i},\mathbf{x})\mapsto(g\,p_{i},g\,\mathbf{x}) \Leftrightarrow \mathbf{Q}(\mathbf{A})\mapsto\mathbf{Q}(g\mathbf{A})=\mathbf{Q}( \mathbf{A}).\] (12)Defining additional bi-invariant attributes

Other examples of the bi-invariants attributes that are used in the experiments section are listed here.

_Full rotation symmetries on the 2-sphere_ For the global shallow water equations we defined \(\mathbf{a}^{\mathrm{SW}}\) as an attribute that is bi-invariant only to rotations over globe's axis, i.e. rotations over \(\phi\). In our experiments we also solve diffusion over the sphere, which is fully \(\mathrm{SO}(3)\) rotationally symmetric. To achieve equivariance to full 3d rotations, we take poses \(p\in\mathrm{SO}(3)\) parameterized by Euler angles which act on points \(x\in S^{2}\) parameterized by 3D unit vectors \(\mathbf{x}\) through 3D-rotation matrices, allowing us to calculate the bi-invariant \(p^{-1}x\):

\[\mathbf{a}^{\mathrm{SO}(3)}_{i,m}=\mathbf{R}_{i}\mathbf{x}_{m}.\] (13)

This bi-invariant is used in our experiments for diffusion on the 2-sphere.

_The 3D ball \(\mathbb{B}^{3}\)._ We experiment with Boussinesq equation for internally heated convection in a ball. The PDE is fully rotationally symmetric, but since the heat source \(K\) is at a fixed point (the center of the ball resp.), it is not symmetric to translations of the initial conditions within the ball. As such, we let \(p\in\mathrm{SO}(3)\times\mathbb{R}\) with \(\phi,\theta,\gamma,r\) s.t. \(0<r<1\). The PDE is defined over spherical coordinates \((\phi,\theta,r)\), which we map to vectors in \(\mathbf{x}\in\mathbb{R}^{3}\). We then use the following bi-invariant, which is only symmetric to rotations in \(\mathrm{SO}(3)\):

\[\mathbf{a}^{\mathbb{B}^{3}}_{i,m}=\mathbf{R}_{i}\mathbf{x}_{m}\oplus r_{p_{i} }\oplus r_{x_{m}}.\] (14)

_No transformation symmetries_. A simple "bi-invariant" for this setting that preserves all geometric information is given by simply concatenating coordinates \(p\) with coordinates \(x\):

\[\mathbf{a}^{\emptyset}_{i,m}=p_{i}\oplus x_{m}\] (15)

Parameterizing the cross-attention operation in Eq. 5 as function of this bi-invariant results in a framework without any equivariance constraints. We use this in experiments to ablate over equivariance constraints and its impact on performance.

## Appendix E Experimental Details

### Dataset creation

For creating the dataset of PDE solutions we used py-pde [54] for Navier-Stokes and the diffusion equation on the plane. For the shallow-water equation and the diffusion equation on the sphere, as well as the internally heated convection in a 3D ball we used Dedalus [10].

Diffusion on the plane.For the diffusion equation on the plane we use as initial conditions narrow spikes centred at random locations in the left half of the domain for the train set, and in the right half of the domain for the test set. States are defined on a 64 \(\times\) 64 grid ranging from -3 to 3. Initial conditions are randomly sampled uniformly between -2 and 2 for \(x\) and 0 and 2 for \(y\) in the training set and between -2 and 2 for \(x\) and -2 and 0 for \(y\). A random value uniformly sampled between 5.0 and 5.5 is inserted at the randomly sampled location. We solve the equation with an Euler solver for 27 steps, discarding the first 7, with a timestep \(dt=0.01\). We generate 1024 training and 128 test trajectories.

Navier-Stokes on the flat 2-torus.For Navier-Stokes on the flat 2-torus we use Gaussian random fields as initial conditions and solve the PDE using a Cranck-Nicholson method with timestep \(dt=1.0\) for 20 steps. The PDE is \(\frac{dv}{dt}=-u\nabla v+v\Delta\mu+f,v=\nabla\times u,\nabla u=0\), where \(u\) is the velocity field, \(v\) the vorticity, \(\mu\) the viscosity and \(f\) a forcing term

\[\frac{dv}{dt}=-u\nabla v+v\Delta\mu+f\] \[v=\nabla\times u\] \[\nabla u=0,\]

where \(u\) is the velocity field, \(v\) the vorticity, \(\mu\) the viscosity and \(f\) a forcing term. We set viscosity to \(1E-3\), resulting with our setup in a Reynolds number of \(\sim\frac{1}{v}=1000\). States are defined on a 64 \(\times\) 64 grid. We generate 8192 training and 512 test trajectories.

Diffusion on the 2-sphere.For the diffusion dataset on the sphere, states are defined over a \(128\times 64\)\(\phi,\theta\) grid. Initial conditions are generated as a gaussian peak inserted at a random point on the sphere with \(\sigma=0.25\). The equation is solved for 20 timesteps with RK4 and \(dt=1.0\). We generate 256 training and 64 test trajectories.

Spherical shallow-water equations [16].The global shallow-water equations are

\[\frac{du}{dt} =-fk\times u-g\nabla h+\nu\Delta u\] \[\frac{dh}{dt} =-h\nabla\cdot u+\nu\Delta h,\]

where \(\frac{d}{dt}\) is the material derivative, \(k\) is the unit vector orthogonal to the surface of the sphere, \(u\) is the velocity field that is tangent to the spherical surface and and \(h\) is the thickness of the fluid layer. The rest are constant parameters of the Earth (see [16] for details). As initial conditions we follow [16] and use basic zonal flow, representing a mid-latitude tropospheric jet, with a correspondingly balanced height field.

\[u(\phi)=\begin{cases}0&\text{for }\phi\leq\phi_{0}\\ \frac{u_{\text{max}}}{e_{n}}\exp\left[\frac{1}{(\phi-\phi_{0})(\phi-\phi_{1})} \right]&\text{for }\phi_{0}<\phi<\phi_{1}\\ 0&\text{for }\phi\geq\phi_{1}\end{cases}\]

Where \(u_{\text{max}}=80ms^{-1}\), \(\phi_{0}=\pi/7,\phi_{1}=\pi/2-\phi_{1}\), and \(e_{n}=\text{exp}[-4(\phi_{1}-\phi_{0})^{2}]\). With this initial zonal flow, we numerically integrate the balance equation

\[gh(\phi)=gh_{0}-\int^{\phi}au(\phi^{\prime})\left[f+\frac{\tan(\phi^{\prime}) }{a}u(\phi^{\prime})\right]\,d\phi^{\prime},\]

to obtain the height \(h\). We then randomly generate small un-balanced perturbations \(h^{\prime}\) to the height field

\[h^{\prime}(\theta,\phi)=\hat{h}\cos(\phi)e^{-(\theta_{2}-\theta/\alpha)^{2}}e^ {-[(\phi_{2}-\phi)/\beta]^{2}}\]

by uniformly sampling \(\alpha\), \(\beta\), \(\hat{h}\), \(\theta_{2}\), and \(\phi_{2}\) within a neighbourhood of the values use in [16]. States are defined on a 192 \(\times\) 96 grid for the high-resolution dataset, which is subsequently downsampled by \(2\times 2\) mean pooling to a 96 \(\times\) 48 grid. We generate 512 training trajectories and 64 test trajectories.

Internally-heated convection in the ball.The equations for the internally-heated convection system are listed here, they include thermal diffusivity (\(\kappa\)) and kinematic viscosity (\(\nu\)), given by:

\[\kappa=(\text{Ra}\cdot\text{Pr})^{-1/2}\]

\[\nu=\left(\frac{\text{Ra}}{\text{Pr}}\right)^{-1/2}\]

We set \(\text{Ra}=1e-6\) and \(\text{Pr}=1\).

1. Incompressibility condition (continuity equation):

\[\nabla\cdot\mathbf{u}+\tau_{p}=0\]

2. Momentum equation (Navier-Stokes equation):

\[\frac{\partial\mathbf{u}}{\partial t}-\nu\nabla^{2}\mathbf{u}+\nabla p- \mathbf{r}T+\text{lift}(\tau_{u})=-\mathbf{u}\times(\nabla\times\mathbf{u})\]

3. Temperature equation:

\[\frac{\partial T}{\partial t}-\kappa\nabla^{2}T+\text{lift}(\tau_{T})=- \mathbf{u}\cdot\nabla T+\kappa T_{\text{source}}\]

4. Shear stress boundary condition (stress-free condition):

\[\text{Shear Stress}=0\text{ on the boundary}\]

5. No penetration boundary condition (radial component of velocity at \(r=1\)):

\[\text{radial}(\mathbf{u}(r=1))=0\]

6. Thermal boundary condition (radial gradient of temperature at \(r=1\)):

\[\text{radial}(\nabla T(r=1))=-2\]

7. Pressure gauge condition:

\[\int p\,dV=0\]

The boundary conditions imposed are stress-free and no-penetration for the velocity field and a constant thermal flux at the outer boundary. These conditions are enforced using penalty terms (\(\tau\)) that are lifted into the domain using higher-order basis functions.

States are defined over a \(64\times 24\times 24\)\(\phi,\theta,r\) grid. We use a SBDF2 solver which we constrain by \(dt_{\text{min}}=1e-4\) and \(dt_{\text{max}}=2e-2\). We evolve the PDE for 26 timesteps, discarding the first 6. We generate 512 training trajectories and 64 test trajectories.

### Training details

We provide hyperparameters per experiment. We optimize the weights of the neural field \(f_{\theta}\), and neural ODE \(F_{\psi}\) with Adam [23] with a learning rate of 1E-4 and 1E-3 respectively. We initialize the inner learning rate that we use in Meta-SGD [28] for learning \(z^{\nu}\) at 1.0 for \(p\) and 5.0 for \(\mathsf{c}\). For the neural ODE \(F_{\psi}\), we use 3 of our message passing layers in the architecture specified in [5], with a hidden dimensionality of 128. The std parameter of the RFF embedding functions \(\gamma_{q},\gamma_{v_{\alpha}},\gamma_{v_{\beta}}\) (see Appx. C), is chosen per experiment. We run all experiments on a single A100. All experiments are ran 3 times.

Diffusion on the plane.We use 4 latents with \(\mathsf{c}\in\mathbb{R}^{16}\). We set the hidden dim of the ENF to 64 and use 2 attention heads. We train the model for 1000 epochs. We set \(\gamma_{q}=0.05,\gamma_{v_{\alpha}}=0.01,\gamma_{v_{\beta}}=0.01\). We use a batch size of 8. The model takes approximately 8 hours to train.

Navier-Stokes on the flat 2-torus.We use 4 latents with \(\mathbf{c}\in\mathbb{R}^{16}\). We set the hidden dim of the ENF to 64 and use 2 attention heads. We train the model for 2000 epochs. We set \(\gamma_{q}=0.05,\gamma_{v_{\alpha}}=0.2,\gamma_{v_{\beta}}=0.2\). We use a batch size of 4. The model takes approximately 48 hours to train.

Diffusion on the 2-sphere.We use 18 latents with \(\mathbf{c}\in\mathbb{R}^{4}\). We set the hidden dim of the ENF to 16 and use 2 attention heads. We train the model for 1500 epochs. We set \(\gamma_{q}=0.01,\gamma_{v_{\alpha}}=0.01,\gamma_{v_{\beta}}=0.01\). We use a batch size of 2. The model takes approximately 12 hours to train.

Spherical shallow-water equations [16].We use 8 latents with \(\mathbf{c}\in\mathbb{R}^{3}2\). We set the hidden dim of the ENF to 128, and use 2 attention heads. We train the model for 1500 epochs. We set \(\gamma_{q}=0.05,\gamma_{v_{\alpha}}=0.2,\gamma_{v_{\beta}}=0.2\). We use a batch size of 2. The model takes approximately 24 hours to train.

Internally-heated convection in the ballWe use 8 latents with \(\mathbf{c}\in\mathbb{R}^{3}2\). We set the hidden dim of the ENF to 128, and use 2 attention heads. We train the model for 1500 epochs. We set \(\gamma_{q}=0.05,\gamma_{v_{\alpha}}=0.2,\gamma_{v_{\beta}}=0.2\). We use a batch size of 2. The model takes approximately 24 hours to train.

CFDBench [32]We use 25 latents with \(\mathbf{c}\in\mathbb{R}^{16}\). We set the hidden dim of the ENF to 128, and use 1 attention head. We set \(\gamma_{q}=0.05,\gamma_{v_{\alpha}}=0.1,\gamma_{v_{\beta}}=0.1\). We train the model for 100 epochs on a single A100 GPU, taking about 16 hours.

BaselinesAs baseline models on Navier-Stokes we train FNO and GFNO [29] with 8 modes and 32 channels for 700 epochs (until convergence). We train CNODE [2] with 4 layers of size 64 for 300 epochs (until convergence). We train DINO on all experiments for 2000 epochs with an architecture as specified in [51]. For the IHC and shallow-water experiments, we increase the latent dim from 100 to 200, the number of layers for the neural ODE from 3 to 5, and the latent dim of the neural field decoder from 64 to 256, as per [51].

For the Navier-Stokes and Internally-Heated Convection experiments we additionally train the Transolver [50] model. We adapt the Transolver model in PyTorch from the official github repository, and use the same hyperparameter settings as in the original paper (resulting in a 7.1M parameter model) - modifying the training objective to be identical to the autoregressive one we use in our experiments (i.e. mapping from frame to frame). During training in the Navier-Stokes task we observed noticeable instabilities, with the method producing high-frequency artefacts in rollouts in this autoregressive setting. For sparsely subsampled initial conditions, performance deteriorates further - highlighting the benefit of NeF-based continuous PDE solving.

For experiments on internally-heated convection, due to the large size of the input frames, we were required to scale down the Transolver model size in order to be able to fit the model on our A100 GPU, from 256 to 64 hidden units, resulting in a 1.2M parameter model - somewhat comparable in size to the model we use in our experiments (889K). We train the model for 2000 epochs on an A100 GPU, taking approximately 30 hours.

## Appendix F Additional results

Parameter counts, memory and time efficiencyIn order to compare parameter, memory and time efficiency of our method to other baselines, we provide details of our models compared to [51] in Tab. 7, and memory usage/inference time compared to the models we used as baselines in the Navier-Stokes experiments in 8. We note that although our method is not the most memory efficient - meta-learning requires a significant computational overhead - it is the fastest in inference settings when unrolling an unseen state. This is attributable to (1) the fact that we're using a latent-space solver which operates on a drastically compressed representation of the PDE state, and use (2) meta-learning over auto-decoding, which means we only require 3 gradient descent steps to obtain the initial state compared to 300-500 typically used in auto-decoding [51].

Error accumulation on long rollouts for Navier-StokesIn order to better empirically assess error accumulation of our method for long-term extrapolation, we provide experimental results for unrolling of Navier-Stokes for 80 timesteps. We apply our model both in a setting with fully observed initial conditions and sparsely observed (50%) initial conditions, and provide plots of accumulated MSE along these trajectories in Figs. 9,10, and a visualization of the solution and error in Fig. 11. We compare against DINO [51] and FNO [29], and show that we consistently improve over DINO in terms of long-term extrapolation. FNO achieves better extrapolation limits due to reduced error accumulation in the fully observed setting, but very rapidly deteriorates even within the train horizon with sparsely observed initial conditions, whereas our proposed approach loses very little in terms of extrapolation performance in this sparse setting.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \# params \(f_{\text{r}}\) & \# params \(F_{\text{r}}\) & tot \# params \\ \hline \multicolumn{4}{c}{Navier-Stokes} \\ DINO [51] & 333k & 502k & 833k \\ Ours & 354k & 531k & 885k \\ \hline \multicolumn{4}{c}{Shallow-Water} \\ DINO [51] & 639k & 902k & 1.5M \\ Ours & 356k & 533k & 889k \\ \hline \multicolumn{4}{c}{Nierarchly-Hierarch Convection} \\ DINO [51] & 639k & 702k & 1.3M \\ Ours & 356k & 533k & 889k \\ \hline \hline \end{tabular}
\end{table}
Table 7: Parameter count for model used in Table 8: Parameter count and runtimes for for models used in Navier-Stokes experiments. Note that training runtimes are measured per epoch, inference runtimes are measured for unrolling a single 20-step trajectory. GPU memory allocations are measured per trajectory.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We feel the abstract and introduction accurately reflect the contributions made in the paper, the experiments in Sec. 4 clearly show that our framework achieves improved performance exactly because of the equivariance constraints. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss some limitations of the model's performance, namely the inability to generalize well on \(t_{\text{out}}\) for the global shallow-water equitions, and our hypothesis for the cause of this; namely limited reconstruction accuracy of the decoder on complex states \(\nu\). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The main claim of the paper; that we obtain equivarance by weight-sharing over bi-invariances is proven in Eq. 4. Guidelines:* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The results should be reproducible based only on the contents of this paper and the references made in it. Code will be made available for a camera-ready version, which also includes code for generating the datasets of solutions and experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is added to the submission. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.

* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: These details are listed in Appx. E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: std is given for experimental results over 3 runs. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details on the hardware on which we run the experiments, the approximate runtime per run. This should be enough information to reproduce.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We respect the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper provides a way for improving PDE forecasting. This can have many positive societal impacts, e.g. improving weather forecasting for early warning, material sciences. However, the framework could provide erroneous predictions, without any indication of its confidence, which could have major negative consequences. Future work should look into associating confidence intervals to predictions, to get indications of the viability of results. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]Justification: We want the paper to reproduciblebl and as such we provide code. There are no major risks for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Authors are original owners, or credit relevant owners as required. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Paper does not provide new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.