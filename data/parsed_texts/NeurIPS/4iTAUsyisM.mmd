# Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness

Chung-En Tsai

Department of Computer Science and Information Engineering

National Taiwan University

chungentsai@ntu.edu.tw

Ying-Ting Lin

Department of Computer Science and Information Engineering

National Taiwan University

R08922060@ntu.edu.tw

Yen-Huan Li

Department of Computer Science and Information Engineering

Department of Mathematics

Center for Quantum Science and Engineering

National Taiwan University

yenhuan.li@csie.ntu.edu.tw

###### Abstract

This work introduces the first small-loss and gradual-variation regret bounds for online portfolio selection, marking the first instances of data-dependent bounds for online convex optimization with non-Lipschitz, non-smooth losses. The algorithms we propose exhibit sublinear regret rates in the worst cases and achieve logarithmic regrets when the data is "easy," with per-round time almost linear in the number of investment alternatives. The regret bounds are derived using novel smoothness characterizations of the logarithmic loss, a local norm-based analysis of following the regularized leader (FTRL) with self-concordant regularizers, which are not necessarily barriers, and an implicit variant of optimistic FTRL with the log-barrier.

## 1 Introduction

Designing an optimal algorithm for online portfolio selection (OPS), with respect to both regret and computational efficiency, has remained a significant open problem in online convex optimization for over three decades1. OPS models long-term investment as a multi-round game between two strategic players--the market and the Investor--thereby avoiding the need for hard-to-verify probabilistic models for the market. In addition to its implications for robust long-term investment, OPS is also a generalization of probability forecasting and universal data compression [5].

Footnote 1: Readers are referred to recent papers, such as those by Luo et al. [22], van Erven et al. [35], Mhammedi and Rakhlin [24], Zimmert et al. [38], and Jezequel et al. [17], for reviews on OPS.

The primary challenge in OPS stems from the absence of Lipschitzness and smoothness in the loss functions. Consequently, standard online convex optimization algorithms do not directly apply. For example, standard analyses of online mirror descent (OMD) and following the regularized leader (FTRL) bound the regret by the sum of the norms of the gradients (see, e.g., the lecture notes byOrabona [28] and Hazan [12]). In OPS, the loss functions are not Lipschitz, so these analyses do not yield a sub-linear regret rate. Without Lipschitzness, the self-bounding property of smooth functions enables the derivation of a "small-loss" regret bound for smooth loss functions [33]. However, the loss functions in OPS are not smooth either.

The optimal regret rate of OPS is known to be \(O(d\log T)\), where \(d\) and \(T\) denote the number of investment alternatives and number of rounds, respectively. This optimal rate is achieved by Universal Portfolio [9, 10]. Nevertheless, the current best implementation of Universal Portfolio requires \(O(d^{4}T^{14})\) per-round time [20], too long for the algorithm to be practical. Subsequent OPS algorithms can be classified into two categories.

* Algorithms in the first category exhibit near-optimal \(\tilde{O}(d)\) per-round time and moderate \(\tilde{O}(\sqrt{dT})\) regret rates. This category includes the barrier subgradient method [26], SoftBayes [30], and LB-OMD [34].
* Algorithms in the second category exhibit much faster \(\tilde{O}(\operatorname{poly}(d)\operatorname{polylog}(T))\) regret rates with much longer \(\tilde{O}(\operatorname{poly}(d)\operatorname{poly}(T))\) per-round time, which is, however, significantly shorter than the per-round time of Universal Portfolio. This category includes ADA-BARONS [22], PAE+DONS [24], BISONS [38], and VB-FTRL [17].

The aforementioned results are worst-case and do not reflect how "easy" the data is. For instance, if the price relatives of the investment alternatives remain constant over rounds, then a small regret is expected. _Data-dependent regret bounds_ refer to regret bounds that maintain acceptable rates in the worst case and much better rates when the data is easy. In this work, we consider three types of data-dependent bounds.

* A _small-loss bound_ bounds the regret by the cumulative loss of the best action in hindsight.
* A _gradual-variation bound_ bounds the regret by the gradual variation (6) of certain characteristics of the loss functions, such as the gradients and price relatives.
* A _second-order bound_ bounds the regret by the variance or other second-order statistics of certain characteristics of the loss functions, such as the gradients and price relatives.

Few studies have explored data-dependent bounds for OPS. These studies rely on the so-called no-junk-bonds assumption [2], requiring that the price relatives of all investment alternatives are bounded from below by a positive constant across all rounds. Given this assumption, it is easily verified that the losses in OPS become Lipschitz and smooth. Consequently, the result of Orabona et al. [29] implies a small-loss regret bound; Chiang et al. [8] established a gradual-variation bound in the price relatives; and Hazan and Kale [14] proved a second-order bound also in the price relatives. These bounds are logarithmic in the number of rounds \(T\) in the worst cases and can be constant when the data is easy.

The no-junk-bonds assumption may not always hold. Hazan and Kale [14] raised the question of whether it is possible to eliminate this assumption. In this work, we take the initial step towards addressing the question. Specifically, we prove Theorem 1.1.

**Theorem 1.1**.: _In the absence of the no-junk-bonds assumption, two algorithms exist that possess a gradual-variation bound and a small-loss bound, respectively. Both algorithms attain \(O(d\operatorname{polylog}(T))\) regret rates in the best cases and \(\tilde{O}(\sqrt{dT})\) regret in the worst cases, with \(\tilde{O}(d)\) per-round time._

Theorem 1.1 represents the first data-dependent bounds for OPS that do not require the no-junk-bonds assumption. To the best of our knowledge, this also marks the first data-dependent bounds for online convex optimization with non-Lipschitz non-smooth losses. In the worst cases, Theorem 1.1 ensures that both algorithms can compete with the OPS algorithms of the first category mentioned above. In the best cases, both algorithms achieve a near-optimal regret with a near-optimal per-round time, surpassing the OPS algorithms of the second category in terms of the computational efficiency. Table 1 in Appendix A presents a detailed summary of existing OPS algorithms in terms of the worst-case regrets, best-case regrets, and per-round time.

We also derived a second-order regret bound by aggregating a variant of optimistic FTRL with different learning rates. The interpretation of the result is not immediately clear. Therefore, we detail the result in Appendix I.

Technical Contributions.The proof of Theorem 1.1 relies on several key technical breakthroughs:

* Theorem 3.2 provides a general regret bound for optimistic FTRL with self-concordant regularizers, which may not be barriers, and time-varying learning rates. The bound generalizes those of Rakhlin and Sridharan [31] and Zimmert et al. [38, Appendix H] and is of independent interest.
* Existing results on small-loss and gradual-variation bounds assume the loss functions are smooth, an assumption that does not hold in OPS. In Section 4, we present Lemma 4.3 and Lemma 4.7, which serve as local-norm counterparts to the Lipschitz gradient condition and the self-bounding property of convex smooth functions, respectively.
* To apply Theorem 3.2, the gradient estimates and iterates in optimistic FTRL must be computed concurrently. Consequently, we introduce Algorithm 2, a variant of optimistic FTRL with the log-barrier and validate its definition and time complexity.
* The gradual-variation and small-loss bounds in Theorem 1.1 are achieved by two novel algorithms, Algorithm 3 and Algorithm 4, respectively. Both are instances of Algorithm 2.

Notations.For any natural number \(N\), we denote the set \(\{1,\ldots,N\}\) by \([N]\). The sets of non-negative and strictly positive numbers are denoted by \(\mathbb{R}_{+}\) and \(\mathbb{R}_{++}\), respectively. The \(i\)-th entry of a vector \(v\in\mathbb{R}^{d}\) is denoted by \(v(i)\). The probability simplex in \(\mathbb{R}^{d}\), the set of entry-wise non-negative vectors of unit \(\ell_{1}\)-norm, is denoted by \(\Delta_{d}\). We often omit the subscript for convenience. The closure and relative interior of a set \(\mathcal{X}\) is denoted by \(\operatorname{cl}\mathcal{X}\) and \(\operatorname{ri}\mathcal{X}\), respectively. The \(\ell_{p}\)-norm is denoted by \(\left\|\cdot\right\|_{p}\). The all-ones vector is denoted by \(e\). For any two vectors \(u\) and \(v\) in \(\mathbb{R}^{d}\), their entry-wise product and division are denoted by \(u\odot v\) and \(u\oslash v\), respectively. For time-indexed vectors \(a_{1},\ldots,a_{t}\in\mathbb{R}^{d}\), we denote the sum \(a_{1}+\cdots+a_{t}\) by \(a_{1:t}\).

## 2 Related Works

### Log-Barrier for Online Portfolio Selection

All algorithms we propose are instances of optimistic FTRL with the log-barrier regularizer. The first use of the log-barrier in OPS can be traced back to the barrier subgradient method proposed by Nesterov [26]. Later, Luo et al. [22] employed a hybrid regularizer, which incorporated the log-barrier, in the development of ADA-BARRONS. This marked the first OPS algorithm with a regret rate polylogarithmic in \(T\) and an acceptable per-round time complexity of \(O(d^{2.5}T)\). Van Erven et al. [35] conjectured that FTRL with the log-barrier (LB-FTRL) achieves the optimal regret. The conjecture was recently refuted by Zimmert et al. [38], who established a regret lower bound for LB-FTRL. Jezequel et al. [17] combined the log-barrier and volumetric barrier to develop VB-FTRL, the first algorithm with near-optimal regret and an acceptable per-round time complexity of \(O(d^{2}T)\). These regret bounds are worst-case and do not directly imply our results.

### FTRL with Self-Concordant Regularizer

Abernethy et al. [1] showed that when the regularizer is chosen as a self-concordant barrier of the constraint set, the regret of FTRL is bounded by the sum of dual local norms of the gradients. Rakhlin and Sridharan [31] generalized this result for optimistic FTRL.

The requirement for the regularizer to be a barrier is restrictive. For instance, while the log-barrier is self-concordant, it is not a barrier of the probability simplex. To address this issue, van Erven et al. [35], Mhammedi and Rakhlin [24], and Jezequel et al. [17] introduced an affine transformation such that, after the transformation, the log-barrier becomes a self-concordant barrier of the constraint set. Nonetheless, this reparametrization complicates the proofs.

Theorem 3.2 in this paper shows that optimistic FTRL with a self-concordant regularizer, _without the barrier requirement_, still satisfies a regret bound similar to that by Rakhlin and Sridharan [31]. The proof of Theorem 3.2 aligns with the analyses by Mohri and Yang [25], McMahan [23], and Joulani et al. [19] of FTRL with optimism and adaptivity, as well as the local-norm based analysis by Zimmert et al. [38, Appendix H].

In comparison, Theorem 3.2 generalizes the analysis of Zimmert et al. [38, Appendix H] for optimistic algorithms and time-varying learning rates; Theorem 3.2 differs from the analyses of Mohri and Yang[25], McMahan [23], and Joulani et al. [19] in that they require the regularizer to be strongly convex, whereas the log-barrier is not.

### Data-Dependent Bounds

The following is a summary of relevant literature on the three types of data-dependent bounds. For a more comprehensive review, readers may refer to, e.g., the lecture notes of Orabona [28].

* Small-loss bounds, also known as \(L^{\star}\) bounds, were first derived by Cesa-Bianchi et al. [6] for online gradient descent for quadratic losses. Exploiting the self-bounding property, Srebro et al. [33] proved a small-loss bound for convex smooth losses. Orabona et al. [29] proved a logarithmic small-loss bound when the loss functions are not only smooth but also Lipschitz and exp-concave.
* Chiang et al. [8] derived the first gradual-variation bound, bounding the regret by the variation of the gradients over the rounds. Rakhlin and Sridharan [31, 32] interpreted the algorithm proposed by Chiang et al. [8] as optimistic online mirror descent and also proposed optimistic FTRL with self-concordant barrier regularizers. Joulani et al. [18] established a gradual-variation bound for optimistic FTRL.
* Cesa-Bianchi et al. [7] initiated the study of second-order regret bounds. Hazan and Kale [13] derived a regret bound characterized by the empirical variance of loss vectors for online linear optimization. In the presence of the no-junk-bonds assumption, Hazan and Kale [14] proved a regret bound for OPS characterized by the empirical variance of price relatives.

Except for those for specific loss functions, these data-dependent bounds assume either smoothness or Lipschitzness of the loss functions. Nevertheless, both assumptions are violated in OPS.

Recently, Hu et al. [16] established small-loss and gradual-variation bounds in the context of Riemannian online convex optimization. We are unaware of any Riemannian structure on the probability simplex that would render the loss functions in OPS geodesically convex and geodesically smooth. For instance, Appendix B shows that the loss functions in OPS are not geodesically convex on the Hessian manifold induced by the log-barrier.

## 3 Analysis of Optimistic FTRL with Self-Concordant Regularizers

This section presents Theorem 3.2, a general regret bound for optimistic FTRL with regularizers that are self-concordant _but not necessarily barriers_. This regret bound forms the basis for the analyses in the remainder of the paper and, as detailed in Section 2.2, generalizes the results of Rakhlin and Sridharan [31] and Zimmert et al. [38, Appendix H].

Consider the following online linear optimization problem involving two players, Learner and Reality. Let \(\mathcal{X}\subseteq\mathbb{R}^{d}\) be a closed convex set. At the \(t\)-th round,

* first, Learner announces \(x_{t}\in\mathcal{X}\);
* then, Reality announces a vector \(v_{t}\in\mathbb{R}^{d}\);
* finally, Learner suffers a loss given by \(\langle v_{t},x_{t}\rangle\).

For any given time horizon \(T\in\mathbb{N}\), the regret \(R_{T}(x)\) is defined as the difference between the cumulative loss of Learner and that yielded by the action \(x\in\mathcal{X}\); that is,

\[R_{T}(x)\coloneqq\sum_{t=1}^{T}\left\langle v_{t},x_{t}\right\rangle-\sum_{t =1}^{T}\left\langle v_{t},x\right\rangle,\quad\forall x\in\mathcal{X}.\]

The objective of Learner is to achieve a small regret against all \(x\in\mathcal{X}\). Algorithm 1 provides a strategy for Learner, called optimistic FTRL.

We focus on the case where the regularizer \(\varphi\) is a self-concordant function.

**Definition 3.1** (Self-concordant functions).: _A closed convex function \(\varphi:\mathbb{R}^{d}\to(-\infty,\infty]\) with an open domain \(\operatorname{dom}\varphi\) is said to be \(M\)-self-concordant if it is three-times continuously differentiable on \(\operatorname{dom}\varphi\) and_

\[\left|D^{3}\varphi(x)[u,u,u]\right|\leq 2M\left\langle u,\nabla^{2}\varphi(x) u\right\rangle^{3/2},\quad\forall x\in\operatorname{dom}\varphi,u\in \mathbb{R}^{d}.\]Suppose that \(\nabla^{2}\varphi\) is positive definite at a point \(x\). The associated local and dual local norms are given by \(\left\|v\right\|_{x}\coloneqq\sqrt{\langle v,\nabla^{2}\varphi(x)v\rangle}\) and \(\left\|v\right\|_{x,*}\coloneqq\sqrt{\langle v,\nabla^{-2}\varphi(x)v\rangle}\), respectively. Define \(\omega(t)\coloneqq t-\log(1+t)\).

**Theorem 3.2**.: _Let \(\varphi\) be an \(M\)-self-concordant function such that \(\mathcal{X}\) is contained in the closure of \(\operatorname{dom}\varphi\) and \(\min_{x\in\mathcal{X}}\varphi(x)=0\). Suppose that \(\nabla^{2}\varphi(x)\) is positive definite for all \(x\in\mathcal{X}\cap\operatorname{dom}\varphi\) and the sequence of learning rates \(\{\eta_{t}\}\) is non-increasing. Then, Algorithm 1 satisfies_

\[R_{T}(x)\leq\frac{\varphi(x)}{\eta_{T}}+\sum_{t=1}^{T}\left(\langle v_{t}-\hat {v}_{t},x_{t}-x_{t+1}\rangle-\frac{1}{\eta_{t-1}M^{2}}\omega(M\|x_{t}-x_{t+1} \|_{x_{t}})\right).\]

_If in addition, \(\eta_{t-1}\|v_{t}-\hat{v}_{t}\|_{x_{t},*}\leq 1/(2M)\) for all \(t\in\mathbb{N}\), then Algorithm 1 satisfies_

\[R_{T}(x)\leq\frac{\varphi(x)}{\eta_{T}}+\sum_{t=1}^{T}\eta_{t-1}\|v_{t}-\hat{ v}_{t}\|_{x_{t},*}^{2}.\]

The proof of Theorem 3.2 is deferred to Appendix D. It is worth noting that the crux of the proof lies in Lemma D.1; the remaining steps follow standard procedure.

## 4 "Smoothness" in Online Portfolio Selection

### Online Portfolio Selection

Online Portfolio Selection (OPS) is a multi-round game between two players, say Investor and Market. Suppose there are \(d\) investment alternatives. A portfolio of Investor is represented by a vector in the probability simplex in \(\mathbb{R}^{d}\), which indicates the distribution of Investor's wealth among the \(d\) investment alternatives. The price relatives of the investment alternatives at the \(t\)-th round are listed in a vector \(a_{t}\in\mathbb{R}^{d}_{+}\).

The game has \(T\) rounds. At the \(t\)-th round,

* first, Investor announces a portfolio \(x_{t}\in\Delta\subset\mathbb{R}^{d}\);
* then, Market announces the price relatives \(a_{t}\in\mathbb{R}^{d}_{+}\);
* finally, Investor suffers a loss given by \(f_{t}(x_{t})\), where the loss function \(f_{t}\) is defined as \[f_{t}(x)\coloneqq-\log\left\langle a_{t},x\right\rangle.\]

The objective of Investor is to achieve a small regret against all portfolios \(x\in\Delta\), defined as2

Footnote 2: Because the vectors \(a_{t}\) can have zero entries, in general, \(\operatorname{dom}f_{t}\) does not contain \(\Delta\).

\[R_{T}(x)\coloneqq\sum_{t=1}^{T}f_{t}(x_{t})-\sum_{t=1}^{T}f_{t}(x),\quad \forall x\in\Delta\cap\bigcap_{t=1}^{T}\operatorname{dom}f_{t}.\]

In the context of OPS, the regret corresponds to the logarithm of the ratio between the wealth growth rate of Investor and that yielded by the constant rebalanced portfolio represented by \(x\in\Delta\).

**Assumption 1**.: _The vector of price relatives \(a_{t}\) is non-zero and satisfies \(\left\|a_{t}\right\|_{\infty}=1\) for all \(t\in\mathbb{N}\)._The assumption on \(\left\|a_{t}\right\|_{\infty}\) does not restrict the problem's applicability. If the assumption does not hold, then we can consider another OPS game with \(a_{t}\) replaced by \(\tilde{a}_{t}\coloneqq a_{t}/\left\|a_{t}\right\|_{\infty}\) and develop algorithms and define the regret with respect to \(\tilde{a}_{t}\). It is obvious that the regret values defined with \(a_{t}\) and \(\tilde{a}_{t}\) are the same.

The following observation, readily verified by direct calculation, will be useful in the proofs.

**Lemma 4.1**.: _The vector \(x\odot(-\nabla f_{t}(x))\) lies in \(\Delta\) for all \(x\in\operatorname{ri}\Delta\) and \(t\in\mathbb{N}\)._

### Log-Barrier

Standard online convex optimization algorithms, such as those in the lecture notes by Orabona [28] and Hazan [12], assume that the loss functions are either Lipschitz or smooth.

**Definition 4.2** (Lipschitzness and smoothness).: _A function \(\varphi\) is said to be Lipschitz with respect to a norm \(\left\|\cdot\right\|\) if_

\[\left|\varphi(y)-\varphi(x)\right|\leq L\|y-x\|,\quad\forall x,y\in\operatorname {dom}\varphi\]

_for some \(L>0\). It is said to be smooth with respect to the norm \(\left\|\cdot\right\|\) if_

\[\left\|\nabla\varphi(y)-\nabla\varphi(x)\right\|_{*}\leq L^{\prime}\|y-x\|, \quad\forall x,y\in\operatorname{dom}\nabla\varphi\] (1)

_for some \(L^{\prime}>0\), where \(\left\|\cdot\right\|_{*}\) denotes the dual norm._

Given that \(\left\langle a_{t},x\right\rangle\) can be arbitrarily close to zero on \(\Delta\cap\operatorname{dom}f_{t}\) in OPS, it is well known that there does not exist a Lipschitz parameter \(L\) nor a smoothness parameter \(L^{\prime}\) for all loss functions \(f_{t}\). Therefore, standard online convex optimization algorithms do not directly apply.

We define the log-barrier as3

Footnote 3: The definition here slightly differs from those typically seen in the literature with an additional \(-d\log d\) term. The additional term helps us remove a \(\log d\) term in the regret bounds.

\[h(x)\coloneqq-d\log d-\sum_{i=1}^{d}\log x(i),\quad\forall(x(1),\ldots,x(d)) \in\mathbb{R}_{++}^{d}.\] (2)

It is easily checked that the local and dual local norms associated with the log-barrier are given by

\[\left\|u\right\|_{x}\coloneqq\left\|u\oslash x\right\|_{2},\quad\left\|u \right\|_{x,*}=\left\|u\odot x\right\|_{2}.\] (3)

In the remainder of the paper, we will only consider this pair of local and dual local norms.

Note that Lipschitzness implies boundedness of the gradient. The following observation motivates the use of the log-barrier in OPS, showing that the gradients in OPS are bounded with respect to the dual local norms defined by the log-barrier.

**Lemma 4.3**.: _It holds that \(\left\|\nabla f_{t}(x)\right\|_{x,*}\leq 1\) for all \(x\in\operatorname{ri}\Delta\) and \(t\in\mathbb{N}\)._

A similar result was proved by van Erven et al. [35, (2)]. We provide a proof of Lemma 4.3 in Section E for completeness.

The following fact will be useful.

**Lemma 4.4** (Nesterov [27, Example 5.3.1 and Theorem 5.3.2]).: _The log-barrier \(h\) and loss functions \(f_{t}\) in OPS are both \(1\)-self-concordant._

### "Smoothness" in OPS

Existing results on small-loss and gradual-variation bounds require the loss functions to be smooth. For example, Chiang et al. [8] exploited the definition of smoothness (1) to derive gradual-variation bounds; Srebro et al. [33] and Orabona et al. [29] used the "self-bounding property," a consequence of smoothness, to derive small-loss bounds.

**Lemma 4.5** (Self-bounding property [33, Lemma 2.1]).: _Let \(f:\mathbb{R}^{d}\to\mathbb{R}\) be an \(L\)-smooth convex function with \(\operatorname{dom}f=\mathbb{R}^{d}\). Then, \(\left\|\nabla f(x)\right\|_{*}^{2}\leq 2L(f(x)-\min_{y\in\mathbb{R}^{d}}f(y))\) for all \(x\in\mathbb{R}^{d}\)._

While the loss functions in OPS are not smooth, we provide two smoothness characterizations of the loss functions in OPS. The first is analogous to the definition of smoothness (1).

**Lemma 4.6**.: _Let \(f(x)=-\log\left\langle a,x\right\rangle\) for some \(a\in\mathbb{R}_{+}^{d}\). Under Assumption 1 on the vector \(a\),_

\[\left\|(x\odot\nabla f(x))-\left(y\odot\nabla f(y)\right)\right\|_{2}\leq 4\min \left\{\left\|x-y\right\|_{x},\left\|x-y\right\|_{y}\right\},\quad\forall x,y \in\operatorname{ri}\Delta,\]

_where \(\odot\) denotes the entrywise product and \(\left\|\cdot\right\|_{x}\) and \(\left\|\cdot\right\|_{y}\) are the local-norms defined by the log-barrier (3)._

The second smoothness characterization is analogous to the self-bounding property (Lemma 4.5). For any \(x\in\operatorname{ri}\Delta\) and \(v\in\mathbb{R}^{d}\), define

\[\alpha_{x}(v)\coloneqq\frac{-\sum_{i=1}^{d}x^{2}(i)v(i)}{\sum_{i=1}^{d}x^{2}( i)},\quad\forall x\in\operatorname{ri}\Delta,\] (4)

where \(x(i)\) and \(v(i)\) denote the \(i\)-th entries of \(x\) and \(v\), respectively.

**Lemma 4.7**.: _Let \(f(x)=-\log\left\langle a,x\right\rangle\) for some \(a\in\mathbb{R}_{+}^{d}\). Then, under Assumption 1 on the vector \(a\), it holds that_

\[\left\|\nabla f(x)+\alpha_{x}(\nabla f(x))e\right\|_{x,*}^{2}\leq 4f(x),\quad \forall x\in\operatorname{ri}\Delta,\]

_where the notation \(e\) denotes the all-ones vector and \(\left\|\cdot\right\|_{x,*}\) denotes the dual local norm defined by the log-barrier (3)._

**Remark 4.8**.: _The value \(\alpha_{x}(v)\) is indeed chosen to minimize \(\left\|v+\alpha e\right\|_{x,*}^{2}\) over all \(\alpha\in\mathbb{R}\)._

The proofs of Lemma 4.6 and Lemma 4.7 are deferred to Appendix E.

## 5 LB-FTRL with Multiplicative-Gradient Optimism

Define \(g_{t}\coloneqq\nabla f_{t}(x_{t})\). By the convexity of the loss functions, OPS can be reduced to an online linear optimization problem described in Section 3 with \(v_{t}=g_{t}\) and \(\mathcal{X}\) being the probability simplex \(\Delta\). Set the regularizer \(\varphi\) as the log-barrier (2) in Optimistic FTRL (Algorithm 1). By Lemma 4.4, for the regret guarantee in Theorem 3.2 to be valid, it remains to ensure that \(\hat{g}_{t}\), the estimate of \(g_{t}\), is selected to satisfy \(\eta_{t-1}\norm{g_{t}-\hat{g}_{t}}_{x_{t},*}\leq 1/2\) for all \(t\). However, as \(x_{t}\), which defines the dual local norm, depends on \(\hat{g}_{t}\) in Algorithm 1, selecting such \(\hat{g}_{t}\) is non-trivial.

To address this issue, we introduce Algorithm 2. This algorithm simultaneously computes the next iterate \(x_{t+1}\) and the gradient estimate \(\hat{g}_{t+1}\) by solving a system of nonlinear equations (5). As we estimate \(x_{t+1}\odot g_{t+1}\) instead of \(g_{t+1}\), we call the algorithm LB-FTRL with Multiplicative-Gradient Optimism. Here, LB indicates that the algorithm adopts the log-barrier as the regularizer.

```
1:Input: A sequence of learning rates \(\{\eta_{t}\}\subseteq\mathbb{R}_{++}\).
2:\(h(x)\coloneqq-d\log d-\sum_{i=1}^{d}\log x(i)\).
3:\(\hat{g}_{1}\coloneqq 0\).
4:\(x_{1}\leftarrow\arg\min_{x\in\Delta}\eta_{0}^{-1}h(x)\).
5:for all\(t\in\mathbb{N}\)do
6: Announce \(x_{t}\) and receive \(a_{t}\).
7:\(g_{t}\coloneqq\nabla f_{t}(x_{t})\).
8: Choose an estimate \(p_{t+1}\in\mathbb{R}^{d}\) for \(x_{t+1}\odot g_{t+1}\).
9: Compute \(x_{t+1}\) and \(\hat{g}_{t+1}\) such that \[\begin{cases}x_{t+1}\odot\hat{g}_{t+1}=p_{t+1},\\ x_{t+1}\in\arg\min_{x\in\Delta}\left\langle g_{1:t},x\right\rangle+\left\langle \hat{g}_{t+1},x\right\rangle+\eta_{t}^{-1}h(x).\end{cases}\] (5)
10:endfor ```

**Algorithm 2** LB-FTRL with Multiplicative-Gradient Optimism for OPS

By the definitions of the dual local norm (3) and \(p_{t}\) (5), we write

\[\norm{g_{t}-\hat{g}_{t}}_{x_{t},*}=\norm{x_{t}\odot g_{t}-x_{t}\odot\hat{g}_{t }}_{2}=\norm{x_{t}\odot g_{t}-p_{t}}_{2}.\]It suffices to choose \(p_{t}\) such that \(\eta_{t-1}{\left\|{x_{t}\odot g_{t}-p_{t}}\right\|_{2}}\leq 1/2\). Indeed, Algorithm 3 and Algorithm 4 correspond to choosing \(p_{t}=x_{t-1}\odot g_{t-1}\) and \(p_{t}=0\), respectively. Algorithm 5 in Appendix I corresponds to choosing \(p_{t}=(1/\eta_{0:t-2})\sum_{t=1}^{t-1}\eta_{\tau-1}x_{\tau}\odot g_{\tau}\).

Theorem 5.1 guarantees that \(x_{t}\) and \(\hat{g}_{t}\) are well-defined and can be efficiently computed. Its proof and the computational details can be found in Appendix F.1.

**Theorem 5.1**.: _If \(\eta_{t}p_{t+1}\in\left[-1,0\right]^{d}\), then the system of nonlinear equations (5) has a solution. The solution can be computed in \(\hat{O}(d)\) time._

Algorithm 2 corresponds to Algorithm 1 with \(v_{t}=g_{t}\), \(\hat{v}_{t}=\hat{g}_{t}=p_{t}\odot x_{t}\), and \(\varphi(x)=h(x)\). Corollary 5.2 then follows from Theorem 3.2. Its proof can be found in Appendix F.2.

**Corollary 5.2**.: _Assume that the sequence \(\{\eta_{t}\}\) is non-increasing and \(p_{t}\in\left(-\infty,0\right]^{d}\) for all \(t\in\mathbb{N}\). Under Assumption 1, Algorithm 2 satisfies_

\[R_{T}(x)\leq\frac{d\log T}{\eta_{T}}+\sum_{t=1}^{T}\left(\left\langle{g_{t}-p_ {t}\odot x_{t},x_{t}-x_{t+1}}\right\rangle-\frac{1}{\eta_{t-1}}\omega({\left\| x_{t}-x_{t+1}\right\|_{x_{t}}})\right)+2,\]

_In addition, for any sequence of vectors \(\{u_{t}\}\) such that \(\eta_{t-1}{\left\|{(g_{t}+u_{t})\odot x_{t}-p_{t}}\right\|_{2}}\leq 1/2\) and \(\left\langle{u_{t},x_{t}-x_{t+1}}\right\rangle=0\) for all \(t\in\mathbb{N}\), Algorithm 2 satisfies_

\[R_{T}(x)\leq\frac{d\log T}{\eta_{T}}+\sum_{t=1}^{T}\eta_{t-1}{\left\|{(g_{t}+u _{t})\odot x_{t}-p_{t}}\right\|_{2}^{2}}+2.\]

**Remark 5.3**.: _The vectors \(u_{t}\) are deliberately introduced to derive a small-loss bound for OPS._

## 6 Data-Dependent Bounds for OPS

### Gradual-Variation Bound

We define the _gradual variation_ as

\[V_{T}\coloneqq\sum_{t=2}^{T}{\left\|{\nabla f_{t}(x_{t-1})-\nabla f_{t-1}(x_ {t-1})}\right\|_{x_{t-1},*}^{2}}\leq\sum_{t=2}^{T}\max_{x\in\Delta}{\left\|{ \nabla f_{t}(x)-\nabla f_{t-1}(x)}\right\|_{x,*}^{2}},\] (6)

where \({\left\|{\cdot}\right\|_{*}}\) denotes the dual local norm associated with the log-barrier. The definition is a local-norm analog to the existing one [8, 18], defined as \(\sum_{t=2}^{T}\max_{x\in\Delta}{\left\|{\nabla f_{t}(x)-\nabla f_{t-1}(x)} \right\|^{2}}\) for a _fixed_ norm \({\left\|{\cdot}\right\|}\). Regarding Lemma 4.3, our definition appears to be a natural extension.

In this sub-section, we introduce Algorithm 3, LB-FTRL with Last-Multiplicative-Gradient Optimism, and Theorem 6.1, which provides the first gradual-variation bound for OPS. Algorithm 3 is an instance of Algorithm 2 with \(p_{1}=0\) and \(p_{t}=x_{t-1}\odot g_{t-1}\) for \(t\geq 2\). Note that the learning rates specified in Theorem 6.1 do not require the knowledge of \(V_{T}\) in advance and can be computed on the fly.

The proof of Theorem 6.1 can be found in Appendix G.

**Theorem 6.1**.: _Let \(\eta_{0}=\eta_{1}=1/(16\sqrt{2})\) and \(\eta_{t}=\sqrt{d/(512d+2+V_{t})}\) for \(t\geq 2\). Then, Algorithm 3 satisfies_

\[R_{T}(x)\leq(\log T+8)\sqrt{dV_{T}+512d^{2}}+\sqrt{2d}\log T+2-128\sqrt{2d}, \quad\forall T\in\mathbb{N}.\]

By the definition of the dual local norm (3) and Lemma 4.1,

\[V_{T}=\sum_{t=2}^{T}{\left\|{x_{t-1}\odot\nabla f_{t}(x_{t-1})-x_{t-1}\odot \nabla f_{t-1}(x_{t-1})}\right\|_{2}^{2}}\leq 2(T-1).\]

As a result, the worst-case regret of Algorithm 3 is \(O(\sqrt{dT}\log T)\), comparable to the regret bounds of the barrier subgradient method [26], Soft-Bayes [30], and LB-OMD [34] up to logarithmic factors. On the other hand, if the price relatives remain constant over rounds, then \(V_{T}=0\) and \(R_{T}=O(d\log T)\)Time Complexity.The vectors \(g_{t}\) and \(p_{t+1}\), as well as the the quantity \(V_{t}\), can be computed using \(O(d)\) arithmetic operations. By Theorem 5.1, the iterate \(x_{t+1}\) can be computed in \(\tilde{O}(d)\) arithmetic operations. Therefore, the per-round time of Algorithm 3 is \(\tilde{O}(d)\).

### Small-Loss Bound

In this sub-section, we introduce Algorithm 4, Adaptive LB-FTRL, and Theorem 6.2, the first small-loss bound for OPS. The algorithm is an instance of Algorithm 2 with \(p_{t}=0\). Then, \(\hat{g}_{t+1}=0\) and \(x_{t+1}\) is directly given by Line 8 of Algorithm 4. Note that Theorem 5.1 still applies.

```
1:\(h(x):=-d\log d-\sum_{i=1}^{d}\log x(i)\).
2:\(x_{1}\leftarrow\arg\min_{x\in\Delta}\eta_{0}^{-1}h(x)\).
3:for all\(t\in\mathbb{N}\)do
4: Announce \(x_{t}\) and receive \(a_{t}\).
5:\(g_{t}\leftarrow\nabla f_{t}(x_{t})=-\frac{a_{t}}{\langle a_{t},x_{t}\rangle}\).
6:\(\alpha_{t}\leftarrow\alpha_{x_{t}}(g_{t})\) (see the definition (4)).
7:\(\eta_{t}\leftarrow\frac{\sqrt{d}}{\sqrt{4d+1+\sum_{i=1}^{d}\|g_{t}+\alpha_{x_{ t}}\|_{x_{t},\star}^{2}}}\).
8:\(x_{t+1}\leftarrow\arg\min_{x\in\Delta}\left\langle g_{1:t},x\right\rangle+\eta _{t}^{-1}h(x)\).
9:endfor ```

**Algorithm 4** Adaptive LB-FTRL for OPS

The proof of Theorem 6.2 is provided in Appendix H.

**Theorem 6.2**.: _Let \(L_{T}^{\star}=\min_{x\in\Delta}\sum_{t=1}^{T}f_{t}(x)\). Then, under Assumption 1, Algorithm 4 satisfies_

\[R_{T}(x)\leq 2(\log T+2)\sqrt{4dL_{T}^{\star}+4d^{2}+d}+d(\log T+2)^{2}.\]

Under Assumption 1,

\[L_{T}^{\star}=\min_{x\in\Delta}\sum_{t=1}^{T}-\log\left\langle a_{t},x\right\rangle \leq\sum_{t=1}^{T}-\log\frac{\left\|a\right\|_{1}}{d}=\sum_{t=1}^{T}\log d=T \log d.\]

Assuming \(T>d\), the worst-case regret bound is \(O(\sqrt{dT\log d}\log T)\), also comparable to the regret bounds of the barrier subgradient method [26], Soft-Bayes [30], and LB-OMD [34] up to logarithmic factors. On the other hand, suppose that there exists an \(i^{\star}\in[d]\) such that \(a_{t}(i^{\star})=1\) for all \(t\in[T]\). That is, the \(i^{\star}\)-th investment alternative always outperforms all the other investment alternatives. Then, \(L_{T}^{\star}=0\) and \(R_{T}=O(d\log^{2}T)\).

Assumption 1 does not restrict the applicability of Theorem 6.2, as mentioned earlier. If the assumption does not hold, Theorem 6.2 is applied with respect to the normalized price relatives \(\tilde{a}_{t}=a_{t}/\left\|a_{t}\right\|_{\infty}\). In this case, \(L_{T}^{\star}\) is defined with respect to \(\{\tilde{a}_{t}\}\).

Time Complexity.Since \(\left\|g_{t}+\alpha_{t}e\right\|_{x_{t,*}}^{2}=\left\|x_{t}\odot g_{t}+\alpha_{t} x_{t}\right\|_{2}^{2}\), it is obvious that computing \(\alpha_{t}\), \(\eta_{t}\), and \(g_{1:t}\) can be done in \(O(d)\) arithmetic operations. By Theorem 5.1, the iterate \(\underline{x}_{t+1}\) can be computed in \(\tilde{O}(d)\) arithmetic operations. Hence, the per-round time of Algorithm 4 is \(\tilde{O}(d)\).

## 7 Concluding Remarks

We have presented Theorem 6.1 and Theorem 6.2, the first gradual-variation and small-loss bounds for OPS that do not require the no-junk-bonds assumption, respectively. The algorithms exhibit sublinear regrets in the worst cases and achieve logarithmic regrets in the best cases, with per-round time almost linear in the dimension. They mark the first data-dependent bounds for non-Lipschitz non-smooth losses.

A potential direction for future research is to extend our analyses for a broader class of online convex optimization problems. In particular, it remains unclear how to extend the two smoothness characterizations (Lemma 4.6 and Lemma 4.7) for other loss functions.

Orabona et al. [29] showed that achieving a regret rate of \(O(d^{2}+\log L_{T}^{\star})\) is possible under the no-junk-bonds assumption, where \(L_{T}^{\star}\) denotes the cumulative loss of the best constant rebalanced portfolio. This naturally raises the question: can a similar regret rate be attained without relying on the no-junk-bonds assumption? If so, then the regret rate will be constant in \(T\) in the best cases and logarithmic in \(T\) in the worst cases. However, considering existing results in probability forecasting with the logarithmic loss [5, Chapter 9]--a special case of OPS without the no-junk-bonds assumption--such a data-dependent regret rate seems improbable. Notably, classical rate-optimal algorithms for probability forecasting with the logarithmic loss, such as Shtarkov's minimax-optimal algorithm, the Laplace mixture, and the Krichevsky-Trofimov mixture, all achieve logarithmic regret rates for all possible data sequences [5, Chapter 9].

Zhao et al. [37] showed that for Lipschitz and smooth losses, an algorithm with a gradual-variation bound automatically achieves a small-loss bound. Generalizing their argument for non-Lipschitz non-smooth losses is a natural direction to consider.

## Acknowledgments and Disclosure of Funding

The authors are supported by the Young Scholar Fellowship (Einstein Program) of the National Science and Technology Council of Taiwan under grant number NSTC 112-2636-E-002-003, by the 2030 Cross-Generation Young Scholars Program (Excellent Young Scholars) of the National Science and Technology Council of Taiwan under grant number NSTC 112-2628-E-002-019-MY3, and by the research project "Pioneering Research in Forefront Quantum Computing, Learning and Engineering" of National Taiwan University under grant number NTU-CC-112L893406.

## References

* Abernethy et al. [2012] Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Interior-Point Methods for Full-Information and Bandit Online Learning. _IEEE Trans. Inf. Theory_, 58(7):4164-4175, 2012.
* Agarwal et al. [2006] Amit Agarwal, Elad Hazan, Satyen Kale, and Robert E. Schapire. Algorithms for Portfolio Management Based on the Newton Method. In _Proc. 23rd Int. Conf. Machine Learning_, pages 9-16, 2006.
* Bauschke et al. [2017] Heinz H. Bauschke, Jerome Bolte, and Marc Teboulle. A descent lemma beyond Lipschitz gradient continuity: first-order methods revisited and applications. _Math. Oper. Res._, 42(2):330-348, 2017.
* Boumal [2023] Nicolas Boumal. _An introduction to optimization on smooth manifolds_. Cambridge University Press, 2023.
* Cesa-Bianchi and Lugosi [2006] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, Learning, and Games_. Cambridge University Press, 2006.
* Cesa-Bianchi et al. [1996] Nicolo Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-Case Quadratic Loss Bounds for Prediction Using Linear Functions and Gradient Descent. _IEEE Trans. Neural Netw._, 7(3):604-619, 1996.

* [7] Nicolo Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for prediction with expert advice. _Mach. Learn._, 66:321-352, 2007.
* [8] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online Optimization with Gradual Variations. In _Proc. 25th Annu. Conf. Learning Theory_, volume 23, pages 6.1-6.20, 2012.
* [9] Thomas M Cover. Universal portfolios. _Math. Financ._, 1991.
* [10] Thomas M. Cover and Erick Ordentlich. Universal portfolios with side information. _IEEE Trans. Inf. Theory_, 42(2):348-363, 1996.
* [11] H. Gzyl and F. Nielsen. Geometry of the probability simplex and its connection to the maximum entropy method. _J. Appl. Math., Stat. Inf._, 16(1):25-35, 2020.
* [12] Elad Hazan. _Introduction to Online Convex Optimization_. The MIT Press, 2022.
* [13] Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs. _Mach. Learn._, 80:165-188, 2010.
* [14] Elad Hazan and Satyen Kale. An Online Portfolio Selection Algorithm with Regret Logarithmic in Price Variation. _Math. Financ._, 25(2):288-310, 2015.
* [15] David P. Helmbold, Robert E. Schapire, Yoran Singer, and Manfred K. Warmuth. On-Line Portfolio Selection Using Multiplicative Updates. _Math. Financ._, 8(4):325-347, 1998.
* [16] Zihao Hu, Guanghui Wang, and Jacob Abernethy. Minimizing Dynamic Regret on Geodesic Metric Spaces. _arXiv preprint arXiv:2302.08652_, 2023.
* [17] Remi Jezequel, Dmitrii M. Ostrovskii, and Pierre Gaillard. Efficient and Near-Optimal Online Portfolio Selection. _arXiv preprint arXiv:2209.13932_, 2022.
* [18] Pooria Joulani, Andras Gyorgy, and Csaba Szepesvari. A Modular Analysis of Adaptive (Non-) Convex Optimization: Optimism, Composite Objectives, and Variational Bounds. In _Proc. 28th Int. Conf. Algorithmic Learning Theory_, volume 76, pages 681-720, 2017.
* [19] Pooria Joulani, Andras Gyorgy, and Csaba Szepesvari. A modular analysis of adaptive (non-) convex optimization: Optimism, composite objectives, variance reduction, and variational bounds. _Theor. Comput. Sci._, 808:108-138, 2020. Special Issue on Algorithmic Learning Theory.
* [20] A. Kalai and S. Vempala. Efficient Algorithms for Universal Portfolios. In _Proc. 41st Annu. Symposium on Foundations of Computer Science_, pages 486-491, 2000.
* [21] Haihao Lu, Robert M. Freund, and Yurii Nesterov. Relatively smooth convex optimization by first-order methods, and applications. _SIAM J. Optim._, 28(1):333-354, 2018.
* [22] Haipeng Luo, Chen-Yu Wei, and Kai Zheng. Efficient Online Portfolio with Logarithmic Regret. In _Adv. Neural Information Processing Systems_, volume 31, 2018.
* [23] H. Brendan McMahan. A survey of Algorithms and Analysis for Adaptive Online Learning. _J. Mach. Learn. Res._, 18(90):1-50, 2017.
* [24] Zakaria Mhammedi and Alexander Rakhlin. Damped Online Newton Step for Portfolio Selection. In _Proc. 35th Annu. Conf. Learning Theory_, 2022.
* [25] Mehryar Mohri and Scott Yang. Accelerating Online Convex Optimization via Adaptive Prediction. In _Proc. 19th Int. Conf. Artificial Intelligence and Statistics_, volume 51, pages 848-856, 2016.
* [26] Yurii Nesterov. Barrier subgradient method. _Math. Program., Ser. B_, 127:31-56, 2011.
* [27] Yurii Nesterov. _Lectures on Convex Optimization_. Springer, second edition, 2018.
* [28] Francesco Orabona. A Modern Introduction to Online Learning. _arXiv preprint arXiv:1912.13213_, 2022.

* [29] Francesco Orabona, Nicolo Cesa-Bianchi, and Claudio Gentile. Beyond Logarithmic Bounds in Online Learning. In _Proc. 15th Int. Conf. Artificial Intelligence and Statistics_, volume 22, pages 823-831, 2012.
* [30] Laurent Orseau, Tor Lattimore, and Shane Legg. Soft-Bayes: Prod for Mixtures of Experts with Log-Loss. In _Proc. 28th Int. Conf. Algorithmic Learning Theory_, volume 76, pages 372-399, 2017.
* [31] Alexander Rakhlin and Karthik Sridharan. Online Learning with Predictable Sequences. In _Proc. 26th Annu. Conf. Learning Theory_, pages 993-1019. PMLR, 2013.
* [32] Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In _Adv. Neural Information Processing Systems 26_, 2013.
* [33] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, Low Noise and Fast Rates. In _Adv. Neural Information Processing Systems_, volume 23, 2010.
* [34] Chung-En Tsai, Hao-Chung Cheng, and Yen-Huan Li. Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States. In _Proc. 34th Int. Conf. Algorithmic Learning Theory_, 2023.
* [35] Tim van Erven, Dirk Van der Hoeven, Wojciech Kotlowski, and Wouter M. Koolen. Open Problem: Fast and Optimal Online Portfolio Selection. In _Proc. 33rd Annu. Conf. Learning Theory_, volume 125, pages 3864-3869, 2020.
* [36] V. Vovk. A Game of Prediction with Expert Advice. _J. Comput. Syst. Sci._, 56(2):153-173, 1998.
* [37] Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Adaptivity and non-stationarity: Problem-dependent dynamic regret for online convex optimization. _arXiv preprint arXiv:2112.14368_, 2021.
* [38] Julian Zimmert, Naman Agarwal, and Satyen Kale. Pushing the Efficiency-Regret Pareto Frontier for Online Learning of Portfolios and Quantum States. In _Proc. 35th Annu. Conf. Learning Theory_, 2022.

A Summary of OPS Algorithms

## Appendix B Loss Functions in OPS are not Geodesically Convex

Let \(h(x)\coloneqq-d\log d-\sum_{i=1}^{d}\log x(i)\) be the log-barrier. Let \((\mathcal{M},g)\) be the Hessian manifold induced by the log-barrier, where \(\mathcal{M}=\operatorname{ri}\Delta\) and \(g\) is the Riemannian metric defined by \(\left\langle u,v\right\rangle_{x}\coloneqq\left\langle u,\nabla^{2}h(x)v\right\rangle\).

**Lemma B.1** (Gzyl and Nielsen [11, Theorem 3.1]).: _Let \(x,y\in\operatorname{ri}\Delta\). The geodesic \(\gamma:[0,1]\to\operatorname{ri}\Delta\) connecting \(x,y\) is given by_

\[\gamma(t)(i)=\frac{x(i)^{1-t}y(i)^{t}}{\sum_{j=1}^{d}x(j)^{1-t}y(j)^{t}},\quad \forall i\in[d],\]

_where \(\gamma(t)(i)\) denotes the \(i\)-th entry of the vector \(\gamma(t)\in\operatorname{ri}\Delta\)._

The proposition below shows that the loss functions in OPS are not geodesically convex on \((\mathcal{M},g)\) in general.

**Proposition B.2**.: _Let \(a=(2,1)\in\mathbb{R}^{2}_{++}\). The function \(f(x)=-\log\left\langle a,x\right\rangle\) is not geodesically convex on \((\mathcal{M},g)\)._

Proof.: It suffices to show that \(f\circ\gamma:[0,1]\to\mathbb{R}\) is not a convex function [4, Definition 11.3]. Let \(x=(1/2,1/2)\) and \(y=(1/(1+\operatorname{e}),\operatorname{e}/(1+\operatorname{e}))\). By Lemma B.1, the geodesic connecting \(x,y\) is \(\gamma(t)=(1/(1+\operatorname{e}^{t}),\operatorname{e}^{t}/(1+\operatorname{ e}^{t}))\). We write

\[(f\circ\gamma)^{\prime\prime}(t)=\frac{(2-\operatorname{e}^{2t})\text{e}^{t}}{ (2+\operatorname{e}^{t})^{2}(1+\operatorname{e}^{t})^{2}}<0,\quad\forall t> \frac{\log 2}{2},\]

showing that \(f\circ\gamma\) is not convex. The proposition follows. 

## Appendix C Properties of Self-Concordant Functions

This section reviews properties of self-concordant functions relevant to our proofs. Readers are referred to the book by Nesterov [27] for a complete treatment.

\begin{table}
\begin{tabular}{l|c|c} \hline \hline \multirow{2}{*}{Algorithms} & Regret (\(\tilde{O}\)) & \multirow{2}{*}{Per-round time (\(\tilde{O}\))} \\ \cline{2-3}  & Best-case & Worst-case \\ \hline Universal Portfolio [9, 20] & \(d\log T\) & \(d^{4}T^{14}\) \\ \hline \(\operatorname{\widehat{EG}}\)[15, 34] & \(d^{1/3}T^{2/3}\) & \(d\) \\ \hline BSM [26], Soft-Bayes [30], LB-OMD [34] & \(\sqrt{dT}\) & \(d\) \\ \hline ADA-BARRONS [22] & \(d^{2}\log^{4}T\) & \(d^{2.5}T\) \\ \hline LB-FTRL without linearized losses [35] & \(d\log^{d+1}T\) & \(d^{2}T\) \\ \hline PAE+DONS [24] & \(d^{2}\log^{5}T\) & \(d^{3}\) \\ \hline BISONS [38] & \(d^{2}\log^{2}T\) & \(d^{3}\) \\ \hline VB-FTRL [17] & \(d\log T\) & \(d^{2}T\) \\ \hline
**This work** (Algorithm 3) & \(d\log T\) & \(\sqrt{dT}\) & \(d\) \\ \hline
**This work** (Algorithm 4) & \(d\log^{2}T\) & \(\sqrt{dT}\) & \(d\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: A summary of existing algorithms for online portfolio selection without the no-junk-bonds assumption. Assume \(T\gg d\).

Throughout this section, let \(\varphi\) be a self-concordant function (Definition 3.1) and \(\left\lVert\cdot\right\rVert_{x}\) be the associated local norm, i.e.,

\[\left\lVert u\right\rVert_{x}\coloneqq\sqrt{\left\langle u,\nabla^{2}\varphi(x )u\right\rangle},\quad\forall u\in\mathbb{R}^{d},x\in\operatorname{dom}\varphi.\]

Self-concordant functions are neither smooth nor strongly convex in general. The following theorem indicates that nevertheless, they are locally smooth and strongly convex.

**Theorem C.1** (Nesterov [27, Theorem 5.1.7]).: _For any \(x,y\in\operatorname{dom}\varphi\) such that \(r\coloneqq\left\lVert y-x\right\rVert_{x}<1/M\),_

\[(1-Mr)^{2}\nabla^{2}\varphi(x)\leq\nabla^{2}\varphi(y)\leq\frac{1}{(1-Mr)^{2} }\nabla^{2}\varphi(x).\]

Define \(\omega(t)\coloneqq t-\log(1+t)\). Let \(\omega_{*}\) be its Fenchel conjugate, i.e., \(\omega_{*}(t)=-t-\log(1-t)\).

**Theorem C.2** (Nesterov [27, Theorem 5.1.8 and 5.1.9]).: _For any \(x,y\in\operatorname{dom}\varphi\) with \(r\coloneqq\left\lVert y-x\right\rVert_{x}\),_

\[\begin{split}&\left\langle\nabla\varphi(y)-\nabla\varphi(x),y-x \right\rangle\geq\frac{r^{2}}{1+Mr},\\ &\varphi(y)\geq\varphi(x)+\left\langle\nabla\varphi(x),y-x \right\rangle+\frac{1}{M^{2}}\omega(Mr).\end{split}\] (7)

_If \(r<1/M\), then_

\[\begin{split}&\left\langle\nabla\varphi(y)-\nabla\varphi(x),y-x \right\rangle\leq\frac{r^{2}}{1-Mr},\\ &\varphi(y)\leq\varphi(x)+\left\langle\nabla\varphi(x),y-x \right\rangle+\frac{1}{M^{2}}\omega_{*}(Mr).\end{split}\] (8)

The following proposition bounds the growth of \(\omega\) and \(\omega_{*}\).

**Proposition C.3** (Nesterov [27, Lemma 5.1.5]).: _For any \(t\geq 0\),_

\[\frac{t^{2}}{2(1+t)}\leq\omega(t)\leq\frac{t^{2}}{2+t}.\]

_For any \(t\in[0,1)\),_

\[\frac{t^{2}}{2-t}\leq\omega_{*}(t)\leq\frac{t^{2}}{2(1-t)}.\]

It is easily checked that both the loss functions in OPS and the log-barrier are not only self-concordant but also self-concordant barriers.

**Definition C.4**.: _A \(1\)-self-concordant function \(\varphi\) is said to be a \(\nu\)-self-concordant barrier if_

\[\left\langle\nabla\varphi(x),u\right\rangle^{2}\leq\nu\left\langle u,\nabla^ {2}\varphi(x)u\right\rangle,\quad\forall x\in\operatorname{dom}\varphi,u\in \mathbb{R}^{d}.\]

**Lemma C.5** (Nesterov [27, (5.4.3)]).: _The log-barrier (2) is a \(d\)-self-concordant barrier._

Appendix D Proof of Theorem 3.2 (Regret Analysis of Optimistic FTRL With Self-Conordant Regularizers)

### Proof of Theorem 3.2

The proof of the following stability lemma is deferred to the next subsection.

**Lemma D.1**.: _Define \(F_{t+1}(x)\coloneqq\left\langle v_{1:t},x\right\rangle+\eta_{t}^{-1}\varphi(x)\). Let \(\{\eta_{t}\}\) be a non-increasing sequence of strictly positive numbers such that \(\eta_{t-1}\|v_{t}-\hat{v}_{t}\|_{x_{t},*}\leq 1/(2M)\). Then, Algorithm 1 satisfies_

\[\begin{split} F_{t}(x_{t})-F_{t+1}(x_{t+1})+\left\langle v_{t},x_ {t}\right\rangle&\leq\left\langle v_{t}-\hat{v}_{t},x_{t}-x_{t+1 }\right\rangle-\frac{1}{\eta_{t-1}M^{2}}\omega(M\|x_{t+1}-x_{t}\|_{x_{t}})\\ &\leq\frac{1}{\eta_{t-1}M^{2}}\omega_{*}(\eta_{t-1}M\|v_{t}-\hat {v}_{t}\|_{x_{t},*})\\ &\leq\eta_{t-1}\|v_{t}-\hat{v}_{t}\|_{x_{t},*}^{2}.\end{split}\]Define \(\hat{F}_{t}(x)\coloneqq F_{t}(x)+\langle\hat{v}_{t},x\rangle\). Since \(\hat{v}_{T+1}\) has no effect on the regret, we set \(\hat{v}_{T+1}=0\). By the strong FTRL lemma [28, Lemma 7.1],

\[R_{T}(x) =\sum_{t=1}^{T}(\langle v_{t},x_{t}\rangle-\langle v_{t},x\rangle)\] \[=\langle\hat{v}_{T+1},x\rangle+\frac{\varphi(x)}{\eta_{T}}-\min_{ x\in\mathcal{X}}\frac{\varphi(x)}{\eta_{0}}+\sum_{t=1}^{T}(\hat{F}_{t}(x_{t})- \hat{F}_{t+1}(x_{t+1})+\langle v_{t},x_{t}\rangle)\] \[\quad+\hat{F}_{T+1}(x_{T+1})-\hat{F}_{T+1}(x)\] \[\leq\frac{\varphi(x)}{\eta_{T}}+\sum_{t=1}^{T}(\hat{F}_{t}(x_{t} )-\hat{F}_{t+1}(x_{t+1})+\langle v_{t},x_{t}\rangle)\] \[\leq\frac{\varphi(x)}{\eta_{T}}+\sum_{t=1}^{T}(F_{t}(x_{t})-F_{t+ 1}(x_{t+1})+\langle v_{t},x_{t}\rangle).\]

The penultimate line above follows from the assumption that \(\min_{x\in\mathcal{X}}\varphi(x)=0\) and that \(x_{T+1}\) minimizes \(\hat{F}_{T+1}\) on \(\mathcal{X}\); the last line above follows from a telescopic sum and \(\hat{v}_{1}=\hat{v}_{T+1}=0\). The theorem then follows from Lemma D.1.

### Proof of Lemma D.1

Since \(\varphi\) is \(M\)-self-concordant, the function \(\eta_{t-1}F_{t}\) is \(M\)-self-concordant. By Theorem C.2,

\[\eta_{t-1}F_{t}(x_{t+1})-\eta_{t-1}F_{t}(x_{t})\] \[\quad\geq\langle\eta_{t-1}\nabla F_{t}(x_{t}),x_{t+1}-x_{t} \rangle+\frac{1}{M^{2}}\omega(M\|x_{t}-x_{t+1}\|_{x_{t}})\] \[\quad=\langle\eta_{t-1}(\nabla F_{t}(x_{t})+\hat{v}_{t}),x_{t+1}- x_{t}\rangle-\eta_{t-1}\left\langle\hat{v}_{t},x_{t+1}-x_{t}\right\rangle+ \frac{1}{M^{2}}\omega(M\|x_{t}-x_{t+1}\|_{x_{t}}).\]

Since \(x_{t}\) minimizes \(F_{t}(x)+\langle\hat{v}_{t},x\rangle\) on \(\mathcal{X}\), the optimality condition implies

\[\langle\nabla F_{t}(x_{t})+\hat{v}_{t},x_{t+1}-x_{t}\rangle\geq 0.\]

Then, we obtain

\[\eta_{t-1}F_{t}(x_{t+1})-\eta_{t-1}F_{t}(x_{t})\geq-\eta_{t-1}\left\langle \hat{v}_{t},x_{t+1}-x_{t}\right\rangle+\frac{1}{M^{2}}\omega(M\|x_{t+1}-x_{t} \|_{x_{t}}).\]

Next, by the non-increasing of \(\{\eta_{t}\}\), \(\varphi(x_{t+1})\geq 0\), and the last inequality,

\[F_{t}(x_{t})-F_{t+1}(x_{t+1})+\langle v_{t},x_{t}\rangle\] \[\quad=F_{t}(x_{t})-F_{t}(x_{t+1})+\langle v_{t},x_{t}\rangle- \langle v_{t},x_{t+1}\rangle+\left(\frac{1}{\eta_{t-1}}-\frac{1}{\eta_{t}} \right)\varphi(x_{t+1})\] \[\quad\leq F_{t}(x_{t})-F_{t}(x_{t+1})+\langle v_{t},x_{t}-x_{t+1}\rangle\] \[\quad\leq\langle v_{t}-\hat{v}_{t},x_{t}-x_{t+1}\rangle-\frac{1} {\eta_{t-1}M^{2}}\omega(M\|x_{t+1}-x_{t}\|_{x_{t}}).\]

This proves the first inequality in the lemma.

By Holder's inequality,

\[F_{t}(x_{t})-F_{t+1}(x_{t+1})+\langle v_{t},x_{t}\rangle\leq\|v_{t}-\hat{v}_{ t}\|_{x_{t},*}\|x_{t}-x_{t+1}\|_{x_{t}}-\frac{1}{\eta_{t-1}M^{2}}\omega(M\|x_{t}-x_{t+ 1}\|_{x_{t}}).\] (9)

By the Fenchel-Young inequality,

\[\omega(M\|x_{t}-x_{t+1}\|_{x_{t}})+\omega_{*}(\eta_{t-1}M\|v_{t}-\hat{v}_{t} \|_{x_{t},*})\geq\eta_{t-1}M^{2}\|x_{t+1}-x_{t}\|_{x_{t}}\|v_{t}-\hat{v}_{t}\| _{x_{t},*}.\] (10)

Combining (9) and (10) yields

\[F_{t}(x_{t})-F_{t+1}(x_{t+1})+\langle v_{t},x_{t}\rangle\leq\frac{1}{\eta_{t-1 }M^{2}}\omega_{*}(\eta_{t-1}M\|v_{t}-\hat{v}_{t}\|_{x_{t},*}).\]

which proves the second inequality. The third inequality in the lemma then follows from the assumption that \(\eta_{t-1}\|v_{t}-\hat{v}_{t}\|_{x_{t},*}\leq 1/(2M)\) and Proposition C.3.

Proofs in Section 4 (Smoothness Characterizations)

### Proof of Lemma 4.3

We write

\[\left\|\nabla f(x)\right\|_{x,*}^{2}=\frac{\sum_{i=1}^{d}a(i)^{2}x(i)^{2}}{\left( \sum_{i=1}^{d}a(i)x(i)\right)^{2}}\leq 1.\]

### Proof of Lemma 4.6

Define \(r\coloneqq\left\|x-y\right\|_{y}\). Consider the following two cases.

1. Suppose that \(r\geq 1/2\). By Lemma 4.1, \[\left\|x\odot\nabla f(x)-y\odot\nabla f(y)\right\|_{2}\leq\sqrt{2}\leq 4\cdot \frac{1}{2}\leq 4\left\|x-y\right\|_{y}.\]
2. Suppose that \(r<1/2\). We write \[r^{2}=\left\|x-y\right\|_{y}^{2}=\sum_{i=1}^{d}\left(1-\frac{x(i)}{y(i)} \right)^{2}\geq\left(1-\frac{x(i)}{y(i)}\right)^{2},\quad\forall i\in[d],\] showing that \[0<1-r\leq\frac{x(i)}{y(i)}\leq 1+r,\quad\forall i\in[d].\] Then [10, Lemma 1], \[\frac{\left\langle a,y\right\rangle}{\left\langle a,x\right\rangle}=\frac{ \sum_{i}a(i)y(i)}{\sum_{i}a(i)x(i)}\leq\max_{i\in[d]}\frac{a(i)y(i)}{a(i)x(i)} =\max_{i\in[d]}\frac{y(i)}{x(i)}\leq\frac{1}{1-r};\] similarly, \[\frac{\left\langle a,y\right\rangle}{\left\langle a,x\right\rangle}\geq\frac{ 1}{1+r}.\] We obtain \[\frac{-2r}{1-r}=1-\frac{1+r}{1-r}\leq 1-\frac{x(i)\left\langle a,y\right\rangle}{y(i) \left\langle a,x\right\rangle}\leq 1-\frac{1-r}{1+r}=\frac{2r}{1+r}.\] Since \(r<1/2\), \[\left(1-\frac{x(i)\left\langle a,y\right\rangle}{y(i)\left\langle a,x \right\rangle}\right)^{2}\leq\max\left\{\left(\frac{-2r}{1-r}\right)^{2}, \left(\frac{2r}{1+r}\right)^{2}\right\}=\frac{4r^{2}}{(1-r)^{2}}<16r^{2}.\] Therefore, \[\left\|x\odot\nabla f(x)-y\odot\nabla f(y)\right\|_{2}^{2} =\sum_{i=1}^{d}\left(\frac{a(i)x(i)}{\left\langle a,x\right\rangle }-\frac{a(i)y(i)}{\left\langle a,y\right\rangle}\right)^{2}\] \[=\sum_{i=1}^{d}\left(\frac{a(i)y(i)}{\left\langle a,y\right\rangle }\right)^{2}\left(1-\frac{x(i)\left\langle a,y\right\rangle}{y(i)\left\langle a,x\right\rangle}\right)^{2}\] \[<16r^{2}\sum_{i=1}^{d}\left(\frac{a(i)y(i)}{\left\langle a,y \right\rangle}\right)^{2}\] \[<16r^{2},\] showing that \(\left\|x\odot\nabla f(x)-y\odot\nabla f(y)\right\|_{2}<4\left\|x-y\right\|_{y}\).

Combining the two cases, we obtain \(\left\|x\odot\nabla f(x)-y\odot\nabla f(y)\right\|_{2}\leq 4\left\|x-y\right\|_{y}\). Since \(x\) and \(y\) are symmetric, we also have \(\left\|x\odot\nabla f(x)-y\odot\nabla f(y)\right\|_{2}\leq 4\left\|x-y\right\|_{x}\). This completes the proof.

### Proof of Lemma 4.7

We will use the notion of relative smoothness.

**Definition E.1** (Bauschke et al. [3], Lu et al. [21]).: _A function \(f\) is said to be \(L\)-smooth relative to a function \(h\) if the function \(Lh-f\) is convex._

**Lemma E.2** (Bauschke et al. [3, Lemma 7]).: _Let \(f(x)=-\log\left\langle a,x\right\rangle\) for some non-zero \(a\in\mathbb{R}_{+}^{d}\). Then, \(f\) is \(1\)-smooth relative to the logarithmic barrier \(h\)._

Fix \(x\in\operatorname{ri}\Delta\). By Lemma E.2, the function \(h-f\) is convex and hence

\[h(x)-f(x)+\left\langle\nabla h(x)-\nabla f(x),v\right\rangle\leq h(x+v)-f(x+v ),\quad\forall v\in\operatorname{ri}\Delta-x,\]

where \(\operatorname{ri}\Delta-x\coloneqq\{u-x|u\in\operatorname{ri}\Delta\}\). By Lemma 4.4, Theorem C.2, and Proposition C.3, if \(\left\|v\right\|_{x}\leq 1/2\),

\[h(x+v)-h(x)-\left\langle\nabla h(x),v\right\rangle\leq\omega_{*}(\left\|v \right\|_{x})\leq\left\|v\right\|_{x}^{2}.\]

Combining the two inequalities above yields

\[\left\langle-\nabla f(x),v\right\rangle-\left\|v\right\|_{x}^{2}\leq f(x)-f(x+ v).\]

Since \(f(x+v)\geq\min_{y\in\Delta}f(y)=0\) (Assumption 1) and \(\left\langle e,v\right\rangle=0\), we write

\[\left\langle-\nabla f(x)-\alpha e,v\right\rangle-\left\|v\right\|_{x}^{2}\leq f (x),\quad\forall\alpha\in\mathbb{R},v\in\operatorname{ri}\Delta-x\text{ such that }\left\|v\right\|_{x}\leq 1/2.\]

The left-hand side is continuous in \(v\), so the condition that \(v\in\operatorname{ri}\Delta-x\) can be relaxed to \(v\in\Delta-x\coloneqq\{u-x|u\in\Delta\}\). We get

\[\left\langle-\nabla f(x)-\alpha e,v\right\rangle-\left\|v\right\|_{x}^{2}\leq f (x),\quad\forall\alpha\in\mathbb{R},v\in\Delta-x\text{ such that }\left\|v\right\|_{x}\leq 1/2.\] (11)

We show that choosing

\[v=-c\nabla^{-2}h(x)(\nabla f(x)+\alpha_{x}(\nabla f(x))e),\] (12)

for any \(c\in(0,1/2]\) ensures that \(v\in\Delta-x\) and \(\left\|v\right\|_{x}\leq 1/2\).

* First, we check whether \(\left\|v\right\|_{x}\leq 1/2\). We write \[\left\|v\right\|_{x} =c\left\|\nabla^{-2}h(x)(\nabla f(x)+\alpha_{x}(\nabla f(x))e) \right\|_{x}\] \[=c\left\|\nabla f(x)+\alpha_{x}(\nabla f(x))e\right\|_{x,*}\] \[\leq c\left\|\nabla f(x)\right\|_{x,*}\] \[\leq c\] \[\leq 1/2,\] where the last line follows from Remark 4.8 and Lemma 4.3.
* Then, we check whether \(v\in\Delta-x\) or, equivalently, whether \(v+x\in\Delta\). By the definition of \(\alpha_{x}(\nabla f(x))\) (4), each entry of \(v\) is given by \[v_{i}=-cx(i)^{2}\left(\nabla_{i}f(x)-\frac{\sum_{j}x(j)^{2}\nabla_{j}f(x)}{ \sum_{j}x(j)^{2}}\right),\quad\forall i\in[d],\] where \(\nabla_{j}f(x)\) is the \(j\)-th entry of \(\nabla f(x)\). Obviously, \[\sum_{i=1}^{d}v(i)=\sum_{i=1}^{d}-cx(i)^{2}\left(\nabla_{i}f(x)-\frac{\sum_{j} x(j)^{2}\nabla_{j}f(x)}{\sum_{j}x(j)^{2}}\right)=0,\] so \(\sum_{i}x(i)+v(i)=1\). It remains to check whether \(x(i)+v(i)\geq 0\) for all \(i\in[d]\). Notice that \(\nabla f(x)\) is entrywise negative. Then, \[x(i)+v(i) =x(i)-cx(i)^{2}\left(\nabla_{i}f(x)-\frac{\sum_{j}x(j)^{2}\nabla_{j }f(x)}{\sum_{j}x(j)^{2}}\right)\] \[\geq x(i)+cx(i)^{2}\frac{\sum_{j}x(j)^{2}\nabla_{j}f(x)}{\sum_{j}x (j)^{2}}.\]To show that the lower bound is non-negative, we write

\[-x(i)^{2}\frac{\sum_{j}x(j)^{2}\nabla_{j}f(x)}{\sum_{j}x(j)^{2}} =\frac{x(i)^{2}\sum_{j}a(j)x(j)^{2}}{\sum_{j}x(j)^{2}\left\langle a,x\right\rangle}\] \[\leq\frac{x(i)^{2}}{\sum_{j}x(j)^{2}}\max_{j\in[d]}x(j)\] \[=x(i)\cdot\max_{j}\frac{x(i)x(j)}{\sum_{k}x(k)^{2}}.\]

If \(i=j\), then \(\max_{j}\frac{x(i)x(j)}{\sum_{k}x(k)^{2}}\leq 1\); otherwise,

\[\max_{j\neq i}\frac{x(i)x(j)}{\sum_{k}x(k)^{2}}\leq\max_{j\neq i}\frac{x(i)x( j)}{x(i)^{2}+x(j)^{2}}\leq\frac{1}{2}.\]

Therefore,

\[x(i)+v(i)\geq(1-c)x(i)\geq\frac{1}{2}x(i)\geq 0,\quad\forall i\in[d].\]

Plug the chosen \(v\) (12) into the inequality (11). We write

\[f(x) \geq\sup_{0\leq c\leq 1/2}c\|\nabla f(x)+\alpha_{x}(\nabla f(x))e \|_{x,*}^{2}-c^{2}\|\nabla f(x)+\alpha_{x}(\nabla f(x))e\|_{x,*}^{2}\] \[=\sup_{0\leq c\leq 1/2}(c-c^{2})\|\nabla f(x)+\alpha_{x}(\nabla f (x))e\|_{x,*}^{2}\] \[=\frac{1}{4}\|\nabla f(x)+\alpha_{x}(\nabla f(x))e\|_{x,*}^{2}.\]

This completes the proof.

## Appendix F Proof in Section 5 (LB-FTRL with Multiplicative-Gradient Optimism)

### Proof of Theorem 5.1

We first check existence of a solution. For convenience, we omit the subscripts in \(p_{t+1}\) and \(\eta_{t}\) and write \(g\) for \(g_{1:t}\).

**Proposition F.1**.: _Assume that \(\eta p\in[-1,0]^{d}\). Define_

\[\lambda^{\star}\in\operatorname*{arg\,min}_{\lambda\in\operatorname*{dom} \psi}\psi(\lambda),\quad\psi(\lambda)\coloneqq\lambda-\sum_{i=1}^{d}(1-\eta p (i))\log(\lambda+\eta g(i)).\] (13)

_Then, \(\lambda^{\star}\) exists and is unique. The pair \((x_{t+1},\hat{g}_{t+1})\), defined by_

\[x_{t+1}(i)\coloneqq\frac{1-\eta p(i)}{\lambda^{\star}+\eta g(i)},\quad\hat{g} _{t+1}(i)\coloneqq p(i)\cdot\frac{\lambda^{\star}+\eta g(i)}{1-\eta p(i)},\] (14)

_solves the system of nonlinear equations (5)._

We will use the following observation [27, Theorem 5.1.1].

**Lemma F.2**.: _The function \(\psi\) is \(1\)-self-concordant._

Proof of Proposition F.1.: By Lemma F.2 and the fact that \(\operatorname*{dom}\psi\) does not contain any line, the minimizer \(\lambda^{\star}\) exists and is unique [27, Theorem 5.1.13]; sine \(\eta p\in[-1,0]^{d}\) and \(\lambda\in\operatorname*{dom}\psi\), we have \(x_{t+1}(i)\geq 0\); moreover,

\[\psi^{\prime}(\lambda^{\star})=1-\sum_{i=1}^{d}\frac{1-\eta p(i)}{\lambda^{ \star}+\eta g(i)}=0.\]

Then,

\[\sum_{i=1}^{d}x_{t+1}(i)=1-\psi^{\prime}(\lambda^{\star})=1,\]showing that \(x_{t+1}\in\Delta\).

Then, we check whether \((x_{t+1},\hat{g}_{t+1})\) satisfies the two equalities (5). It is obvious that \(x_{t+1}\odot\hat{g}_{t+1}=p\). By the method of Lagrange multiplier, if there exists \(\lambda\in\mathbb{R}\) such that

\[\eta g(i)+\eta\hat{g}_{t+1}(i)-\frac{1}{x_{t+1}(i)}+\lambda=0,\quad\sum_{i=1}^ {d}x_{t+1}(i)=1,\quad x_{t+1}(i)\geq 0,\]

then \(x_{t+1}\in\arg\min_{x\in\Delta}\left\langle g,x\right\rangle+\left\langle\hat {g}_{t+1},x\right\rangle+\eta^{-1}h(x)\). It is easily checked that the above conditions are satisfied with \(\lambda=\lambda^{\star}\) and (14). The proposition follows. 

We now analyze the time complexity. Nesterov [26, Section 7][27, Appendix A.2] proved that when \(p=0\), Newton's method for solving (13) reaches the region of quadratic convergence of the intermediate Newton's method [27, Section 5.2.1] after \(O(\log d)\) iterations. A generalization for possibly non-zero \(p\) is provided in Proposition F.3 below. The proof essentially follows the approach of Nesterov [27, Appendix A.2]. It is worth noting that while the function \(\psi\) is self-concordant, directly applying existing results on self-concordant minimization by Newton's method [27, Section 5.2.1] does not yield the desired \(O(\log d)\) iteration complexity bound.

Starting with an initial iterate \(\lambda_{0}\in\mathbb{R}\), Newton's method for the optimization problem (13) iterates as

\[\lambda_{t+1}=\lambda_{t}-\psi^{\prime}(\lambda_{t})/\psi^{\prime\prime}( \lambda_{t}),\quad\forall t\in\{0\}\cup\mathbb{N}.\] (15)

**Proposition F.3**.: _Assume that \(\eta p\in[-1,0]^{d}\). Define \(i^{\star}\in\arg\min_{i\in[d]}(-g(i))\). Then, Newton's method with the initial iterate_

\[\lambda_{0}=1-\eta g(i^{\star})\]

_enters the region of quadratic convergence of the intermediate Newton's method [27, Section 5.2.1] after \(O(\log d)\) iterations._

Firstly, we will establish Lemma F.4 and Lemma F.5. These provide characterizations of \(\psi^{\prime}\) and \(\lambda_{t}\) that are necessary for the proof of Proposition F.3.

**Lemma F.4**.: _The following hold._

* \(\psi^{\prime}\) _is a strictly increasing and strictly concave function._
* \(\psi^{\prime}(\lambda^{\star})=0\)_._
* \(\lambda_{0}\leq\lambda^{\star}\)_._

Proof of Lemma F.4.: The first item follows by a direct calculation:

\[\psi^{\prime\prime}(\lambda)=\sum_{i=1}^{d}\frac{1-\eta p(i)}{(\lambda+\eta g (i))^{2}}>0,\quad\psi^{\prime\prime\prime}(\lambda)=-\sum_{i=1}^{d}\frac{2 \left(1-\eta p(i)\right)}{(\lambda+\eta g(i))^{3}}<0.\]

The second item has been verified in the proof of Proposition F.1. As for the third item, we write

\[\psi^{\prime}(\lambda_{0})=1-\sum_{i=1}^{d}\frac{1-\eta p(i)}{\lambda_{0}+ \eta g(i)}\leq 1-\frac{1-\eta p(i^{\star})}{\lambda_{0}+\eta g(i^{\star})}=\eta p (i^{\star})\leq 0=\psi^{\prime}(\lambda^{\star}).\]

The third item then follows from \(\psi^{\prime}(\lambda_{0})\leq\psi^{\prime}(\lambda^{\star})\) and the first item. 

**Lemma F.5**.: _Let \(\{\lambda_{t}\}\) be the sequence of iterates of Newton's method (15). For any \(t\geq 0\), \(\psi^{\prime}(\lambda_{t})\leq 0\) and \(\lambda_{t}\leq\lambda^{\star}\)._

Proof of Lemma F.5.: Recall that \(\operatorname{dom}\psi=(-g(i^{\star}),\infty)\). We proceed by induction. Obviously, \(\lambda_{0}\in\operatorname{dom}\psi\). By Lemma F.4, the lemma holds for \(t=0\). Assume that the lemma holds for some non-negative integer \(t\). By the iteration rule of Newton's method (15) and the assumption that \(\psi^{\prime}(\lambda_{t})\leq 0\),

\[\lambda_{t+1}=\lambda_{t}-\frac{\psi^{\prime}(\lambda_{t})}{\psi^{\prime\prime }(\lambda_{t})}\geq\lambda_{t},\]which ensures that \(\lambda_{t+1}\in\operatorname{dom}\psi\). By Lemma F.4 (i) and the iteration rule of Newton's method (15), we have

\[\psi^{\prime}(\lambda_{t+1})\leq\psi^{\prime}(\lambda_{t})+\psi^{\prime\prime}( \lambda_{t})(\lambda_{t+1}-\lambda_{t})=\psi^{\prime}(\lambda_{t})-\psi^{ \prime\prime}(\lambda_{t})\frac{\psi^{\prime}(\lambda_{t})}{\psi^{\prime\prime }(\lambda_{t})}=0.\]

By Lemma F.4 (i) and Lemma F.4 (ii), \(\lambda_{t+1}\leq\lambda^{\star}\). The lemma follows. 

Proof of Proposition F.3.: Define the Newton decrement as

\[\delta(\lambda)\coloneqq\frac{|\psi^{\prime}(\lambda_{t})|}{\left|\psi^{ \prime}(\lambda)/\sqrt{\psi^{\prime\prime}(\lambda)}\right|}.\]

By Lemma F.2, the region of quadratic convergence is [27, Theorem 5.2.2]

\[\mathcal{Q}\coloneqq\{\lambda\in\operatorname{dom}\psi:\delta(\lambda)<1/2\}.\]

Define \(T\coloneqq\lceil(9+7\log_{2}d)/4\rceil\). By Lemma F.5, \(\psi^{\prime}(\lambda_{t})\leq 0\) for all \(t\geq 0\). If \(\psi^{\prime}(\lambda_{t})=0\) for some \(0\leq t\leq T\), then Newton's method reaches \(\mathcal{Q}\) within \(T\) steps and the proposition follows. Now, let us assume that \(\psi^{\prime}(\lambda_{t})<0\) for all \(0\leq t\leq T\). By the concavity of \(\psi^{\prime}\) (Lemma F.4 (i)),

\[\psi^{\prime}(\lambda_{t-1})\leq\psi^{\prime}(\lambda_{t})+\psi^{\prime\prime }(\lambda_{t})(\lambda_{t-1}-\lambda_{t})=\psi^{\prime}(\lambda_{t})+\frac{ \psi^{\prime\prime}(\lambda_{t})}{\psi^{\prime\prime}(\lambda_{t-1})}\psi^{ \prime}(\lambda_{t-1}).\]

Divide both sides by \(\psi^{\prime}(\lambda_{t-1})\). By Lemma F.5 and the AM-GM inequality,

\[1\geq\frac{\psi^{\prime}(\lambda_{t})}{\psi^{\prime}(\lambda_{t-1})}+\frac{ \psi^{\prime\prime}(\lambda_{t})}{\psi^{\prime\prime}(\lambda_{t-1})}\geq 2\sqrt{ \frac{\psi^{\prime}(\lambda_{t})\psi^{\prime\prime}(\lambda_{t})}{\psi^{ \prime}(\lambda_{t-1})\psi^{\prime\prime}(\lambda_{t-1})}}.\]

Then, for all \(1\leq t\leq T+1\),

\[|\psi^{\prime}(\lambda_{t})\psi^{\prime\prime}(\lambda_{t})|\leq\frac{1}{4}| \psi^{\prime}(\lambda_{t-1})\psi^{\prime\prime}(\lambda_{t-1})|.\]

This implies that for all \(0\leq t\leq T+1\),

\[|\psi^{\prime}(\lambda_{t})\psi^{\prime\prime}(\lambda_{t})|\leq\frac{1}{4^{ t}}|\psi^{\prime}(\lambda_{0})\psi^{\prime\prime}(\lambda_{0})|,\]

which further implies that

\[\delta(\lambda_{t})^{2}=\frac{(\psi^{\prime}(\lambda_{t})\psi^{\prime\prime}( \lambda_{t}))^{2}}{(\psi^{\prime\prime}(\lambda_{t}))^{3}}\leq\frac{1}{16^{t}} \cdot\frac{(\psi^{\prime}(\lambda_{0})\psi^{\prime\prime}(\lambda_{0}))^{2}}{( \psi^{\prime\prime}(\lambda_{t}))^{3}}.\]

It remains to estimate \(\psi^{\prime}(\lambda_{0})\), \(\psi^{\prime\prime}(\lambda_{0})\), and \(\psi^{\prime\prime}(\lambda_{t})\). Given \(\eta p\in[-1,0]^{d}\) and \(\lambda_{0}+\eta g(i)\geq 1\), through direct calculation,

\[|\psi^{\prime}(\lambda_{0})| =\sum_{i=1}^{d}\frac{1-\eta p(i)}{\lambda_{0}+\eta g(i)}-1\leq \sum_{i=1}^{d}(1-\eta p(i))-1<2d,\] \[\psi^{\prime\prime}(\lambda_{0}) =\sum_{i=1}^{d}\frac{1-\eta p(i)}{(\lambda_{0}+\eta g(i))^{2}} \leq\sum_{i=1}^{d}\frac{2}{1}=2d.\]

Since

\[\psi^{\prime}(\lambda_{t})=1-\sum_{i=1}^{d}\frac{1-\eta p(i)}{\lambda_{t}+ \eta g(i)}\leq 0,\]

by the Cauchy-Schwarz inequality,

\[\psi^{\prime\prime}(\lambda_{t})=\sum_{i=1}^{d}\frac{1-\eta p(i)}{(\lambda_{t}+ \eta g(i))^{2}}\geq\frac{\left(\sum_{i=1}^{d}\frac{1-\eta p(i)}{\lambda_{t}+ \eta g(i)}\right)^{2}}{\sum_{i=1}^{d}(1-\eta p(i))}\geq\frac{1}{\sum_{i=1}^{d }(1-\eta p(i))}\geq\frac{1}{2d}.\]

Therefore, for \(0\leq t\leq T+1\),

\[\delta(\lambda_{t})^{2}<\frac{(2d)^{7}}{16^{t}}.\]

In particular, \(\delta(\lambda_{T+1})<1/2\), i.e., \(\lambda_{T+1}\in\mathcal{Q}\). This concludes the proof.

Proposition F.3 suggests that after \(O(\log d)\) iterations of Newton's method, we can switch to the intermediate Newton's method which exhibits quadratic convergence [27, Theorem 5.2.2]. Each iteration of both Newton's method and the intermediate Newton's method takes \(O(d)\) time. Therefore, it takes \(\tilde{O}(d\log d)\) time to approximate \(\lambda^{*}\). Given \(\lambda^{*}\), both \(x_{t+1}\) and \(\hat{g}_{t+1}\) can be computed in \(O(d)\) time, according to their respective definitions in Equation (14). Therefore, the overall time complexity of computing \(x_{t+1}\) and \(\hat{g}_{t+1}\) is \(\tilde{O}(d)\).

### Proof of Corollary 5.2

By the definition of local norms (3), we write

\[\left\|g_{t}-\hat{g}_{t}+u_{t}\right\|_{x_{t},*}=\left\|(g_{t}+u_{t})\odot x_{ t}-p_{t}\odot x_{t}\odot x_{t}\right\|_{2}=\left\|(g_{t}+u_{t})\odot x_{t}-p_{t} \right\|_{2}.\]

By Lemma 4.4 and Theorem 3.2, if \(\eta_{t-1}\|(g_{t}+u_{t})\odot x_{t}-p_{t}\|_{2}\leq 1/2\), then Algorithm 2 satisfies

\[R_{T}(x)\leq\frac{h(x)}{\eta_{T}}+\sum_{t=1}^{T}\eta_{t-1}\|(g_{t}+u_{t})\odot x _{t}-p_{t}\|_{2}^{2}.\]

For any \(x\in\Delta\), define \(x^{\prime}=(1-1/T)x+e/(dT)\). Then,

\[h(x^{\prime})\leq-d\log d-\sum_{i=1}^{d}\log\left(\frac{1}{dT}\right)=d\log T, \quad\forall x\in\Delta\]

and hence [22, Lemma 10]

\[R_{T}(x)\leq R_{T}(x^{\prime})+2\leq\frac{d\log T}{\eta_{T}}+\sum_{t=1}^{T} \eta_{t-1}\|(g_{t}+u_{t})\odot x_{t}-p_{t}\|_{2}^{2}+2.\]

## Appendix G Proof of Theorem 6.1 (Gradual-Variation Bound)

### Proof of Theorem 6.1

We will use the following lemma, whose proof is postponed to the end of the section.

**Lemma G.1**.: _Let \(\{x_{t}\}\) be the iterates of Algorithm 2. If for all \(t\geq 1\),_

\[\eta_{t}\leq\frac{1}{6},\quad\eta_{t-1}\|g_{t}-\hat{g}_{t}\|_{x_{t},*}\leq \frac{1}{6},\quad\left(1-\frac{1}{6\sqrt{d}}\right)\eta_{t-1}\leq\eta_{t}\leq \eta_{t-1},\quad\text{and}\quad\left\|\hat{g}_{t}\right\|_{x_{t},*}\leq 1,\] (16)

_then \(\left\|x_{t+1}-x_{t}\right\|_{x_{t}}\leq 1.\)_

We start with checking the conditions in Lemma G.1.

* It is obvious that \(\eta_{t}\leq 1/(16\sqrt{2})\leq 1/6\).
* By the definition of the dual local norm (3), the definition of \(\hat{g}_{t}\), and Lemma 4.3, \[\left\|\hat{g}_{t}\right\|_{x_{t},*}=\left\|\hat{g}_{t}\odot x_{t}\right\|_{2} =\left\|g_{t-1}\odot x_{t-1}\right\|_{2}=\left\|g_{t-1}\right\|_{x_{t-1},*} \leq 1,\quad\forall t\geq 2.\]
* Then, by the triangle inequality and Lemma 4.3, \[\eta_{t-1}\|g_{t}-\hat{g}_{t}\|_{x_{t},*}\leq\frac{1}{16\sqrt{2}}\left(\left\| g_{t}\right\|_{x_{t},*}+\left\|\hat{g}_{t}\right\|_{x_{t},*}\right)\leq\frac{1}{8 \sqrt{2}}\leq\frac{1}{6}.\]* It is obvious that the sequence \(\{\eta_{t}\}\) is non-increasing. By the triangle inequality, Lemma 4.3 and the fact that \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\) for non-negative numbers \(a\) and \(b\), we write \[\frac{\eta_{t}}{\eta_{t-1}} =\frac{\sqrt{512d+2+V_{t-1}}}{\sqrt{512d+2+V_{t}}}\] \[\geq\frac{\sqrt{512d+2+V_{t-1}}}{\sqrt{512d+2+V_{t-1}+2}}\] \[\geq\frac{\sqrt{512d+2+V_{t-1}}}{\sqrt{512d+2+V_{t-1}+\sqrt{2}}}\] \[=1-\frac{\sqrt{2}}{\sqrt{512d+2+V_{t-1}}+\sqrt{2}}\] \[\geq 1-\frac{1}{16\sqrt{d}}\] \[\geq 1-\frac{1}{6\sqrt{d}}.\]

Then, Lemma G.1 implies that

\[r_{t}\coloneqq\left\lVert x_{t+1}-x_{t}\right\rVert_{x_{t}}\leq 1.\]

By Proposition C.3,

\[\omega(\left\lVert x_{t}-x_{t+1}\right\rVert_{x_{t}})\geq\frac{r_{t}^{2}}{4}.\]

By Corollary 5.2,

\[R_{T}\leq\frac{d\log T}{\eta_{T}}+\sum_{t=1}^{T}\left(\left\langle g_{t}-p_{t} \oslash x_{t},x_{t}-x_{t+1}\right\rangle-\frac{1}{4\eta_{t-1}}r_{t}^{2}\right) +2.\]

For \(t\geq 2\), by Holder's inequality and the definition of the dual local norm (3),

\[\left\langle g_{t}-p_{t}\oslash x_{t},x_{t}-x_{t+1}\right\rangle \leq\left\lVert g_{t}-p_{t}\oslash x_{t}\right\rVert_{x_{t},*}r_{t}\] \[=\left\lVert x_{t}\odot g_{t}-p_{t}\right\rVert_{2}r_{t}\] \[=\left\lVert x_{t}\odot g_{t}-x_{t-1}\odot g_{t-1}\right\rVert_ {2}r_{t}.\]

By the triangle inequality,

\[\left\lVert x_{t}\odot g_{t}-x_{t-1}\odot g_{t-1}\right\rVert_{2 }r_{t}\] \[\leq\left\lVert x_{t}\odot g_{t}-x_{t-1}\odot\nabla f_{t}(x_{t-1 })\right\rVert_{2}r_{t}+\left\lVert x_{t-1}\odot\nabla f_{t}(x_{t-1})-x_{t-1} \odot g_{t-1}\right\rVert_{2}r_{t}.\]

We bound the two terms separately. By the AM-GM inequality and Lemma 4.6, the first term is bounded by

\[\left\lVert x_{t}\odot g_{t}-x_{t-1}\odot\nabla f_{t}(x_{t-1}) \right\rVert_{2}r_{t} \leq 4\eta_{t-1}\|x_{t}\odot g_{t}-x_{t-1}\odot\nabla f_{t}(x_{t-1 })\|_{2}^{2}+\frac{1}{16\eta_{t-1}}r_{t}^{2}\] \[\leq 64\eta_{t-1}r_{t-1}^{2}+\frac{1}{16\eta_{t-1}}r_{t}^{2}.\]

By the AM-GM inequality, the second term is bounded by

\[\left\lVert x_{t-1}\odot\nabla f_{t}(x_{t-1})-x_{t-1}\odot g_{t- 1}\right\rVert_{2}r_{t} =\left\lVert\nabla f_{t}(x_{t-1})-g_{t-1}\right\rVert_{x_{t-1},*}r _{t}\] \[\leq 4\eta_{t-1}\|\nabla f_{t}(x_{t-1})-g_{t-1}\|_{x_{t-1},*}^{2}+ \frac{1}{16\eta_{t-1}}r_{t}^{2}.\]

Combine all inequalities above. We obtain

\[\left\langle g_{t}-p_{t}\oslash x_{t},x_{t}-x_{t+1}\right\rangle- \frac{1}{4\eta_{t-1}}r_{t}^{2}\] (17) \[\leq 4\eta_{t-1}\|\nabla f_{t}(x_{t-1})-g_{t-1}\|_{x_{t-1},*}^{2} +64\eta_{t-1}r_{t-1}^{2}-\frac{1}{8\eta_{t-1}}r_{t}^{2}\]For \(t=1\), by a similar argument,

\[\left\langle g_{1},x_{1}-x_{2}\right\rangle-\frac{1}{4\eta_{0}}r_{1}^{2}\leq r_{1 }-\frac{1}{4\eta_{0}}r_{1}^{2}\leq\frac{1}{2}(4\eta_{0}+\frac{1}{4\eta_{0}}r_{1 }^{2})-\frac{1}{4\eta_{0}}r_{1}^{2}=2\eta_{0}-\frac{1}{8\eta_{0}}r_{1}^{2}.\] (18)

By combining (17) and (18) and noticing that \(\eta_{t-1}\eta_{t}\leq 1/512\),

\[\sum_{t=1}^{T}\left\langle g_{t}-p_{t}\oslash x_{t},x_{t}-x_{t+1} \right\rangle-\frac{1}{4\eta_{t-1}}r_{t}^{2}\] \[\qquad\leq 2\eta_{0}+\sum_{t=2}^{T}4\eta_{t-1}\|\nabla f_{t}(x_{t-1 })-g_{t-1}\|_{x_{t-1},*}^{2}+\sum_{t=1}^{T-1}\left(64\eta_{t}-\frac{1}{8\eta_{ t-1}}\right)r_{t}^{2}\] \[\qquad\leq 2\eta_{0}+\sum_{t=2}^{T}4\eta_{t-1}\|\nabla f_{t}(x_{t- 1})-g_{t-1}\|_{x_{t-1},*}^{2}.\]

By Corollary 5.2,

\[R_{T}\leq\frac{d\log T}{\eta_{T}}+2\eta_{0}+\sum_{t=2}^{T}4\eta_{t-1}\|\nabla f _{t}(x_{t-1})-\nabla f_{t-1}(x_{t-1})\|_{x_{t-1},*}^{2}+2.\]

Plug in the choice of learning rates into the regret bound. We obtain [28, Lemma 4.13]

\[R_{T} \leq\sqrt{d}\log T\sqrt{V_{T}+512d+2}+4\sqrt{d}\sum_{t=2}^{T} \frac{\|\nabla f_{t}(x_{t-1})-\nabla f_{t-1}(x_{t-1})\|_{x_{t-1},*}^{2}}{\sqrt {512d+2}+V_{t-1}}+3\] \[\leq\sqrt{d}\log T\sqrt{V_{T}+512d+2}+4\sqrt{d}\sum_{t=2}^{T} \frac{\|\nabla f_{t}(x_{t-1})-\nabla f_{t-1}(x_{t-1})\|_{x_{t-1},*}^{2}}{\sqrt {512d+V_{t}}}+3\] \[\leq\sqrt{d}\log T(\sqrt{V_{T}+512d}+\sqrt{2})+4\sqrt{d}\int_{0} ^{V_{T}}\frac{\mathrm{d}s}{\sqrt{512d+s}}+3\] \[\leq(\log T+8)\sqrt{dV_{T}+512d^{2}}+\sqrt{2d}\log T+2-128\sqrt{ 2d}.\]

### Proof of Lemma G.1

Define

\[y_{t}\in\operatorname*{arg\,min}_{x\in\Delta}\left\langle g_{1:t},x\right\rangle +\frac{1}{\eta_{t-1}}h(x),\quad z_{t}\in\operatorname*{arg\,min}_{x\in\Delta} \left\langle g_{1:t},x\right\rangle+\frac{1}{\eta_{t}}h(x).\]

By the triangle inequality,

\[\left\|x_{t}-x_{t+1}\right\|_{x_{t}}\leq\left\|x_{t}-y_{t}\right\|_{x_{t}}+ \left\|y_{t}-z_{t}\right\|_{x_{t}}+\left\|z_{t}-x_{t+1}\right\|_{x_{t}}.\] (19)

Define \(d_{1}\coloneqq\left\|x_{t}-y_{t}\right\|_{x_{t}}\), \(d_{2}\coloneqq\left\|y_{t}-z_{t}\right\|_{y_{t}}\), and \(d_{3}\coloneqq\left\|z_{t}-x_{t+1}\right\|_{x_{t+1}}\). Suppose that \(d_{i}\leq 1/5\) for all \(1\leq i\leq 3\), which we will verify later. By Theorem C.1,

\[(1-d_{1})^{2}\nabla^{2}h(x_{t})\leq\nabla^{2}h(y_{t})\leq\frac{1}{(1-d_{1})^{ 2}}\nabla^{2}h(x_{t}),\]

\[(1-d_{2})^{2}\nabla^{2}h(y_{t})\leq\nabla^{2}h(z_{t})\leq\frac{1}{(1-d_{2})^{ 2}}\nabla^{2}h(y_{t}),\] (20)

\[(1-d_{3})^{2}\nabla^{2}h(x_{t+1})\leq\nabla^{2}h(z_{t})\leq\frac{1}{(1-d_{3}) ^{2}}\nabla^{2}h(x_{t+1}).\]

We obtain

\[\left\|y_{t}-z_{t}\right\|_{x_{t}}\leq\frac{d_{2}}{1-d_{1}},\quad\left\|z_{t}- x_{t+1}\right\|_{x_{t}}\leq\frac{d_{3}}{(1-d_{1})(1-d_{2})(1-d_{3})}.\]

Then, the inequality (19) becomes

\[\left\|x_{t}-x_{t+1}\right\|_{x_{t}}\leq d_{1}+\frac{d_{2}}{1-d_{1}}+\frac{d_ {3}}{(1-d_{1})(1-d_{2})(1-d_{3})}.\]It is easily checked that the maximum of the right-hand side is attained at \(d_{1}=d_{2}=d_{3}=1/5\) and the maximum value equals \(269/320\). This proves the lemma.

Now, we prove that \(d_{i}\leq 1/5\) for all \(1\leq i\leq 3\).

1. (Upper bound of \(d_{1}\).) By the optimality conditions of \(x_{t}\) and \(y_{t}\), \[\left\langle\eta_{t-1}g_{1:t}+\nabla h(y_{t}),x_{t}-y_{t}\right\rangle\geq 0\] \[\left\langle\eta_{t-1}g_{1:t-1}+\eta_{t-1}\hat{g}_{t}+\nabla h(x_ {t}),y_{t}-x_{t}\right\rangle\geq 0.\] Sum up the two inequalities. We get \[\eta_{t-1}\left\langle g_{t}-\hat{g}_{t},x_{t}-y_{t}\right\rangle\geq\left\langle \nabla h(x_{t})-\nabla h(y_{t}),x_{t}-y_{t}\right\rangle\] By Holder's inequality, Lemma 4.4, and Theorem C.2, we obtain \[\eta_{t-1}\|g_{t}-\hat{g}_{t}\|_{x_{t},s}d_{1}\geq\frac{d_{1}^{2}}{1+d_{1}}.\] Then, by the assumption (16), we write \[d_{1}\leq\frac{\eta_{t-1}\|g_{t}-\hat{g}_{t}\|_{x_{t},*}}{1-\eta_{t-1}\|g_{t} -\hat{g}_{t}\|_{x_{t},*}}\leq 1/5.\]
2. (Upper bound of \(d_{2}\).) By the optimality conditions of \(y_{t}\) and \(z_{t}\), \[\left\langle g_{1:t}+\frac{1}{\eta_{t}}\nabla h(z_{t}),y_{t}-z_{t }\right\rangle\geq 0\] \[\left\langle g_{1:t}+\frac{1}{\eta_{t-1}}\nabla h(y_{t}),z_{t}-y_ {t}\right\rangle\geq 0.\] Sum up the two inequalities. We get \[\left(\frac{1}{\eta_{t}}-\frac{1}{\eta_{t-1}}\right)\left\langle\nabla h(y_ {t}),y_{t}-z_{t}\right\rangle\geq\frac{1}{\eta_{t}}\left\langle\nabla h(y_{t} )-\nabla h(z_{t}),y_{t}-z_{t}\right\rangle.\] By Definition C.4, Lemma C.5, and Theorem C.2, \[\left(\frac{1}{\eta_{t}}-\frac{1}{\eta_{t-1}}\right)\sqrt{d}d_{2}\geq\frac{1 }{\eta_{t}}\frac{d_{2}^{2}}{1+d_{2}}.\] Then, by the assumption (16), we write \[d_{2}\leq\frac{(1-\eta_{t}/\eta_{t-1})\sqrt{d}}{1-(1-\eta_{t}/\eta_{t-1}) \sqrt{d}}\leq\frac{1}{5}.\]
3. (Upper bound of \(d_{3}\).) By the optimality conditions of \(z_{t}\) and \(x_{t+1}\), \[\left\langle\eta_{t}g_{1:t}+\eta_{t}\hat{g}_{t+1}+\nabla h(x_{t+1} ),z_{t}-x_{t+1}\right\rangle\geq 0\] \[\left\langle\eta_{t}g_{1:t}+\nabla h(z_{t}),x_{t+1}-z_{t}\right\rangle \geq 0.\] Sum up the two inequalities. We get \[\eta_{t}\left\langle\hat{g}_{t+1},z_{t}-x_{t+1}\right\rangle\geq\left\langle \nabla h(z_{t})-\nabla h(x_{t+1}),z_{t}-x_{t+1}\right\rangle.\] By Holder's inequality, the assumption (16), and Theorem C.2, we obtain \[\eta_{t}d_{3}\geq\eta_{t}\|\hat{g}_{t+1}\|_{x_{t+1},*}d_{3}\geq\frac{d_{3}^{ 2}}{1+d_{3}}.\] Then, by the assumption (16), we write \[d_{3}\leq\frac{\eta_{t}}{1-\eta_{t}}\leq 1/5.\]Proof of Theorem 6.2 (Small-Loss Bound)

Recall that \(\alpha_{t}\) minimizes \(\left\|g_{t}+\alpha e\right\|_{x_{t,*}}^{2}\) over all \(\alpha\in\mathbb{R}\) (Remark 4.8). We write

\[\left\|g_{t}+\alpha_{t}e\right\|_{x_{t,*}}^{2}\leq\left\|g_{t}\right\|_{x_{t,*} }^{2}\leq 1,\]

which, with the observation that \(\eta_{t}\leq 1/2\) for all \(t\in\mathbb{N}\), implies that \(\eta_{t-1}\|g_{t}+\alpha_{t}e\|_{x_{t,*}}\leq 1/2\). Then, since the all-ones vector \(e\) is orthogonal to the set \(\Delta-\Delta\), by Corollary 5.2 with \(p_{t}=0\) and \(u_{t}=\alpha_{t}e\), we get

\[R_{T} \leq\frac{d\log T}{\eta_{T}}+\sum_{t=1}^{T}\eta_{t-1}\|(g_{t}+ \alpha_{t}e)\odot x_{t}\|_{2}^{2}+2\] \[=\frac{d\log T}{\eta_{T}}+\sum_{t=1}^{T}\eta_{t-1}\|g_{t}+\alpha _{t}e\|_{x_{t,*}}^{2}+2\]

Plug in the explicit expressions of the learning rates \(\eta_{t}\). We write [28, Lemma 4.13]

\[R_{T} \leq\frac{d\log T}{\eta_{T}}+\sqrt{d}\sum_{t=1}^{T}\frac{\left\| g_{t}+\alpha_{t}e\right\|_{x_{t,*}}^{2}}{\sqrt{4d+1+\sum_{\tau=1}^{t-1}\left\| g_{\tau}+\alpha_{\tau}e\right\|_{x_{\tau,*}}^{2}}}+2\] \[\leq\frac{d\log T}{\eta_{T}}+\sqrt{d}\sum_{t=1}^{T}\frac{\left\| g_{t}+\alpha_{t}e\right\|_{x_{t,*}}^{2}}{\sqrt{4d+\sum_{\tau=1}^{t}\left\| g_{\tau}+\alpha_{\tau}e\right\|_{x_{\tau,*}}^{2}}}+2\] \[\leq\sqrt{d}\log T\sqrt{4d+1+\sum_{t=1}^{T}\left\|g_{t}+\alpha_{t }e\right\|_{x_{t,*}}^{2}}+\sqrt{d}\int_{0}^{\sum_{t=1}^{T}\left\|g_{t}+\alpha_ {t}e\right\|_{x_{t,*}}^{2}}\frac{\mathrm{d}s}{\sqrt{4d+s}}+2\] \[\leq(\log T+2)\sqrt{d\sum_{t=1}^{T}\left\|g_{t}+\alpha_{t}e \right\|_{x_{t,*}}^{2}+4d^{2}+d}\] \[\leq(\log T+2)\sqrt{d\sum_{t=1}^{T}\left\|g_{t}+\alpha_{t}e \right\|_{x_{t,*}}^{2}+4d^{2}+d}.\]

Finally, by Lemma 4.7,

\[R_{T}\leq(\log T+2)\sqrt{4d\sum_{t=1}^{T}f_{t}(x_{t})+4d^{2}+d}.\]

The theorem then follows from Lemma 4.23 of Orabona [28].

## Appendix I A Second-Order Regret Bound

In this section, we introduce AA + LB-FTRL with Average-Multiplicative-Gradient Optimism (Algorithm 6) that achieves a second-order regret bound. This bound is characterized by the variance-like quantity

\[Q_{T}\coloneqq\min_{p\in\mathbb{R}^{d}}\sum_{t=1}^{T}\left\|x_{t}\odot\nabla f _{t}(x_{t})-p\right\|_{2}^{2}.\]

For comparison, existing second-order bounds are characterized by the quantity

\[V_{T}\coloneqq\min_{u\in\mathbb{R}^{d}}\sum_{t=1}^{T}\left\|\nabla f_{t}(x_{t })-u\right\|_{2}^{2},\]\(T\) times the empirical variance of the gradients (see, e.g., Section 7.12.1 in the lecture note of Orabona [28]). Given that every \(p\in\mathbb{R}^{d}\) can be expressed as \(p=x_{t}\odot v\) for \(v=p\odot x_{t}\), and by the definition of the dual local norm (3), we obtain

\[Q_{T}=\min_{v\in\mathbb{R}^{d}}\sum_{t=1}^{T}\left\|\nabla f_{t}(x_{t})-v \right\|_{x_{t},*}^{2},\]

showing that \(Q_{T}\) is a local-norm analogue of \(V_{T}\).

The flexibility introduced by optimizing \(Q_{T}\) over all \(v\in\mathbb{R}^{d}\) allows for a fast regret rate when the data is easy. In particular, Theorem I.2 in Section I.3 shows that when \(Q_{T}\leq 1\), then Algorithm 6 achieves a fast \(O(d\log T)\) regret rate. However, while \(Q_{T}\) looks reasonable as a mathematical extension of \(V_{T}\), the empirical variance interpretation for \(V_{T}\) does not extend to \(Q_{T}\). Interpreting \(Q_{T}\) or seeking a "more natural" variant of \(Q_{T}\) is left as a future research topic.

### The Gradient Estimation Problem

To obtain a better regret bound by using Corollary 5.2, one has to design a good strategy of choosing \(p_{t}\). In Section 6.1, we have used \(p_{t}=x_{t-1}\odot g_{t-1}\). In this section, we consider a different approach.

Consider the following online learning problem, which we call the _Gradient Estimation Problem_. It is a multi-round game between Learner and Reality. In the \(t\)-th round, Learner chooses a vector \(p_{t}\in\mathbb{R}^{d}\); then, Reality announces a convex loss function \(\ell_{t}(p)=\eta_{t-1}\|x_{t}\odot g_{t}-p\|_{2}^{2}\); finally, Learner suffers a loss \(\ell_{t}(p_{t})\).

### LB-FTRL with Average-Multiplicative-Gradient Optimism

In the gradient estimation problem, the loss functions are strongly-convex. A natural strategy of choosing \(p_{t}\) is then following the leader (FTL). In the \(t\)-th round, FTL suggests choosing

\[p_{t}=\frac{1}{\eta_{0:t-2}}\sum_{\tau=1}^{t-1}\eta_{\tau-1}x_{\tau}\odot g_ {\tau}\in\operatorname*{arg\,min}_{p\in\mathbb{R}^{d}}\sum_{\tau=1}^{t-1}\ell _{\tau}(p).\]

This strategy leads to Algorithm 5. Theorem I.1 provides a regret bound for Algorithm 5.

```
1:Input: A sequence of learning rates \(\{\eta_{t}\}_{t\geq 0}\subseteq\mathbb{R}_{++}\).
2:\(h(x):=-d\log d-\sum_{i=1}^{d}\log x(i)\).
3:\(x_{1}\leftarrow\operatorname*{arg\,min}_{x\in\Delta}\eta_{0}^{-1}h(x)\).
4:for all\(t\in\mathbb{N}\)do
5: Announce \(x_{t}\) and receive \(a_{t}\).
6:\(g_{t}\coloneqq\nabla f_{t}(x_{t})\).
7:\(p_{t+1}\leftarrow(1/\eta_{0:t-1})\sum_{\tau=1}^{t}\eta_{\tau-1}x_{\tau}\odot g _{\tau}\).
8: Solve \(x_{t+1}\) and \(\hat{g}_{t+1}\) satisfying \[\begin{cases}x_{t+1}\odot\hat{g}_{t+1}=p_{t+1},\\ x_{t+1}\in\operatorname*{arg\,min}_{x\in\Delta}\left\langle g_{1:t},x\right\rangle +\left\langle\hat{g}_{t+1},x\right\rangle+\eta_{t}^{-1}h(x).\end{cases}\]
9:endfor ```

**Algorithm 5** LB-FTRL with Average-Multiplicative-Gradient Optimism

**Theorem I.1**.: _Assume that the sequence of learning rates \(\{\eta_{t}\}\) is non-increasing and \(\eta_{0}\leq 1/(2\sqrt{2})\). Then, Algorithm 5 achieves_

\[R_{T}\leq\frac{d\log T}{\eta_{T}}+2\sum_{t=1}^{T}\frac{\eta_{t-1}^{2}}{\eta_{0: t-1}}+2+\min_{p\in\mathbb{R}^{d}}\sum_{t=1}^{T}\eta_{t-1}\|x_{t}\odot\nabla f_{t}(x_{t} )-p\|_{2}^{2}.\]

_If \(\eta_{t}=\eta\leq 1/(2\sqrt{2})\) is a constant, then_

\[R_{T}\leq\frac{d\log T}{\eta}+2\eta(\log T+1)+2+\eta Q_{T}.\]Proof.: Since \(p_{t}\) is a convex combination of \(x_{\tau}\odot g_{\tau}\), by Lemma 4.1, \(p_{t}\in-\Delta\). We restrict the action set of the gradient estimation problem to be \(-\Delta\). By the regret bound of FTL for strongly convex losses [28, Corollary 7.24],

\[\sum_{t=1}^{T}\ell_{t}(p_{t})-\min_{p\in-\Delta}\sum_{t=1}^{T}\ell_{t}(p)\leq \frac{1}{2}\sum_{t=1}^{T}\frac{(2\sqrt{2}\eta_{t-1})^{2}}{2\eta_{0:t-1}}=2 \sum_{t=1}^{T}\frac{\eta_{t-1}^{2}}{\eta_{0:t-1}}.\]

Then, by Corollary 5.2,

\[R_{T}\leq\frac{d\log T}{\eta_{T}}+2+\sum_{t=1}^{T}\ell_{t}(p_{t})\leq\frac{d \log T}{\eta_{T}}+2+2\sum_{t=1}^{T}\frac{\eta_{t-1}^{2}}{\eta_{0:t-1}}+\min_{ p\in-\Delta}\sum_{t=1}^{T}\ell_{t}(p).\]

The first bound in the theorem follows from

\[\min_{p\in-\Delta}\sum_{t=1}^{T}\ell_{t}(p)=\min_{p\in\mathbb{R}^{d}}\sum_{t= 1}^{T}\eta_{t-1}\|x_{t}\odot g_{t}-p\|_{2}^{2}.\]

The second bound in the theorem follows from the inequality \(\sum_{t=1}^{T}\eta^{2}/(t\eta)\leq\eta(\log T+1)\). 

Time Complexity.Suppose that a constant learning rate is used. Then, both \(g_{t}\) and \(p_{t+1}\) can be computed in \(O(d)\) arithmetic operations. By Theorem 5.1, \(x_{t+1}\) can be computed in \(O(d)\) time. Hence, the per-round time of Algorithm 5 is \(\tilde{O}(d)\).

### AA + LB-FTRL with Average-Multiplicative-Gradient Optimism

The constant learning rate that minimizes the regret bound in Theorem I.1 is \(\eta=O(\sqrt{d\log T/Q_{T}})\). However, the value of \(Q_{T}\) is not unknown to Investor initially.

In this section, we propose Algorithm 6, which satisfies a second-order regret bound without the need for knowing \(Q_{T}\) in advance. Algorithm 6 uses multiple instances of Algorithm 5, which we call _experts_, with different learning rates. The outputs of the experts are aggregated by the Aggregating Algorithm (AA) due to Vovk [36].

``` Input: The time horizon \(T\).
1:\(K\coloneqq 1+\lceil\log_{2}T\rceil\).
2:for all\(k\in[K]\)do
3:\(w_{1}^{(k)}\gets 1\).
4:\(q^{(k)}\gets 2^{k}\).
5:\(\eta^{(k)}\leftarrow\frac{\sqrt{d\log T}}{2\sqrt{2d\log T}+\sqrt{q^{(k)}}}\).
6: Initialize Algorithm 5 with the constant learning rate \(\eta^{(k)}\) as the \(k\)-th expert \(\mathcal{A}_{k}\).
7: Obtain \(x_{1}^{(k)}\) from \(\mathcal{A}_{k}\).
8:endfor
9:\(x_{1}\coloneqq\frac{1}{\sum_{k=1}^{K}w_{1}^{(k)}}\sum_{k=1}^{K}w_{1}^{(k)}x_{1 }^{(k)}\).
10:for all\(t\in[T]\)do
11: Observe \(a_{t}\).
12:for all\(k\in[K]\)do
13:\(w_{t+1}^{(k)}=w_{t}^{(k)}\left\langle a_{t},x_{t}^{(k)}\right\rangle\).
14: Feed \(a_{t}\) into \(\mathcal{A}_{k}\) and obtain \(x_{t+1}^{(k)}\).
15:endfor
16: Output \(x_{t+1}\coloneqq\frac{1}{\sum_{k=1}^{K}w_{t+1}^{(k)}}\sum_{k=1}^{K}w_{t+1}^{(k )}x_{t+1}^{(k)}\).
17:endfor ```

**Algorithm 6** AA + LB-FTRL with Average-Multiplicative-Gradient Optimism

The regret bound of Algorithm 6 is stated in Theorem I.2, which follows from the regret of AA and Theorem I.1.

[MISSING_PAGE_EMPTY:28]