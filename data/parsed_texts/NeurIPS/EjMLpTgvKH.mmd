# Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics

Leon Klein

Freie Universitat Berlin

leon.klein@fu-berlin.de&Andrew Y. K. Foong1

Microsoft Research AI4Science

andrewfoong@microsoft.com&Tor Erlend Fjelde1

University of Cambridge

ter30@cam.ac.uk

Bruno Mlodozeniec1

University of Cambridge

bkm28@cam.ac.uk

Marc Brockschmidt2

University of Cambridge

ter30@cam.ac.uk

Sebastian Nowozin2

Frank Noe

Microsoft Research AI4Science

Freie Universitat Berlin

franknoe@microsoft.com

Footnote 1: Equal contribution.

Footnote 2: Work done while at Microsoft Research.

###### Abstract

Molecular dynamics (MD) simulation is a widely used technique to simulate molecular systems, most commonly at the all-atom resolution where equations of motion are integrated with timesteps on the order of femtoseconds (\(1\mathrm{fs}=10^{-15}\mathrm{s}\)). MD is often used to compute equilibrium properties, which requires sampling from an equilibrium distribution such as the Boltzmann distribution. However, many important processes, such as binding and folding, occur over timescales of milliseconds or beyond, and cannot be efficiently sampled with conventional MD. Furthermore, new MD simulations need to be performed for each molecular system studied. We present _Timewarp_, an enhanced sampling method which uses a normalising flow as a proposal distribution in a Markov chain Monte Carlo method targeting the Boltzmann distribution. The flow is trained offline on MD trajectories and learns to make large steps in time, simulating the molecular dynamics of \(10^{5}-10^{6}\,\mathrm{fs}\). Crucially, Timewarp is _transferable_ between molecular systems: once trained, we show that it generalises to unseen small peptides (2-4 amino acids) at all-atom resolution, exploring their metastable states and providing wall-clock acceleration of sampling compared to standard MD. Our method constitutes an important step towards general, transferable algorithms for accelerating MD.

## 1 Introduction

Molecular dynamics (MD) is a well-established technique for simulating physical systems at the atomic level. When performed accurately, it provides unrivalled insight into the detailed mechanics of molecular motion, without the need for wet lab experiments. MD simulations have been used to understand processes of central interest in molecular biophysics, such as protein folding [29; 22], protein-ligand binding [1], and protein-protein association [31]. Many crucial applications of MD boil down to efficiently sampling from the _Boltzmann distribution_, _i.e._, the equilibrium distributionof a molecular system at a temperature \(T\). Let \((x^{p}(t),x^{v}(t)):=x(t)\in\mathbb{R}^{6N}\) be the state of the molecule at time \(t\), consisting of the positions \(x^{p}(t)\in\mathbb{R}^{3N}\) and velocities \(x^{v}(t)\in\mathbb{R}^{3N}\) of the \(N\) atoms in Cartesian coordinates. The Boltzmann distribution is given by:

\[\mu(x^{p},x^{v})\propto\exp\left(-\tfrac{1}{k_{B}T}(U(x^{p})+K(x^{v}))\right), \quad\mu(x^{p})=\int\mu(x^{p},x^{v})\,\mathrm{d}x^{v}.\] (1)

where \(U(x^{p})\) is the potential energy, \(K(x^{v})\) is the kinetic energy, and \(k_{B}\) is Boltzmann's constant. Many important quantities, such as the free energies of protein folding and protein-ligand binding, can be computed as expectations under \(\mu(x^{p})\). A popular MD method to sample from \(\mu(x^{p})\) is _Langevin dynamics_[19], which obeys the following stochastic differential equation (SDE):

\[m_{i}\mathrm{d}x^{v}_{i}=-\nabla_{i}U\mathrm{d}t-\gamma m_{i}x^{v}_{i} \mathrm{d}t+\sqrt{2m_{i}\gamma k_{B}T}\mathrm{d}B_{t}.\] (2)

Here \(i\) indexes the atoms, \(m_{i}\) is the mass of atom \(i\), \(U(x^{p})\) is the potential energy, \(\gamma\) is a friction parameter, and \(\mathrm{d}B_{t}\) is a standard Brownian motion process. Starting from an initial state \(x(0)\), simulating Equation (2), along with the relationship \(\mathrm{d}x^{p}=x^{v}\mathrm{d}t\), yields values of \(x(t)\) that are distributed according to the Boltzmann distribution as \(t\to\infty\). Standard MD libraries discretise this SDE with a timestep \(\Delta t\), which must be chosen to be \(\sim 1\mathrm{fs}=10^{-15}\mathrm{s}\) for stability. Unfortunately, many biomolecules contain metastable states separated by energy barriers that can take milliseconds of MD simulation time (\(\sim 10^{12}\) sequential integration steps) to cross, rendering this approach infeasible. To overcome this, prior work has produced an array of _enhanced sampling methods_, such as coarse graining [3, 14] and metadynamics [18]. However, these methods require domain knowledge specific to each molecular system to implement effectively.

Standard MD simulations do not transfer information between molecular systems: for each system studied, a new simulation must be performed. This is a wasted opportunity: many molecular systems exhibit closely related dynamics, and simulating one system should yield information relevant to similar systems. In particular, proteins, being comprised of sequences of 20 kinds of amino acids, are prime candidates to study this kind of transferability. We propose _Timewarp_, a general, transferable enhanced sampling method which uses a _normalising flow_[35] as a proposal for a Markov chain Monte Carlo (MCMC) method targeting the Boltzmann distribution. Our main contributions are:

1. We present the first ML algorithm working in general Cartesian coordinates that demonstrates _transferability_ to small peptide systems unseen during training.
2. We demonstrate, for the first time, wall-clock acceleration of asymptotically unbiased MCMC sampling of the Boltzmann distribution of unseen peptide systems.
3. We define an MCMC algorithm targeting the Boltzmann distribution using a conditional normalising flow as a proposal distribution, with a Metropolis-Hastings (MH) correction step to ensure detailed balance (Section 3.4).
4. We design a permutation equivariant, transformer-based normalising flow.
5. We produce a novel training dataset of MD trajectories of thousands of small peptides.
6. We show that even when deployed _without_ the MH correction (Section 3.5), Timewarp can be used to explore metastable states of new peptides much faster than MD.

Figure 1: (a) Initial state \(x(t)\) (_Left_) and accepted proposal state \(x(t+\tau)\sim p_{\theta}(x(t+\tau)|x(t))\) (_Right_) sampled with Timewarp for the dipeptide HT (unseen during training). (b) TICA projections of simulation trajectories, showing transitions between metastable states, for a short MD simulation (_Left_) and Timewarp MCMC (_Right_), both run for 30 minutes of wall-clock time. Timewarp MCMC achieves a speed-up factor of \(\approx 33\) over MD in terms of effective sample size per second.

The code is available here: https://github.com/microsoft/timewarp. The datasets are available upon request3.

Footnote 3: Please contact andrewfoong@microsoft.com for dataset access.

## 2 Related work

There has recently been a surge of interest in deep learning on molecules. _Boltzmann generators_[28; 17; 15] use flows to sample from the Boltzmann distribution asymptotically unbiased. There are two ways to generate samples: (i) Produce i.i.d. samples from the flow and use statistical reweighting to debias expectation values. (ii) Use the Boltzmann generator in an MCMC framework [4], as in Timewarp. Currently, Boltzmann generators lack the ability to generalize across multiple molecules, in contrast to Timewarp. The only exception is [11], who propose a diffusion model in torsion space and use the underlying ODE as a transferable Boltzmann generator. However, in contrast to Timewarp, they use internal coordinates and do not operate in the all atom system. Moreover, [13; 25] recently introduced a Boltzmann generators in Cartesian coordinates for molecules, potentially enabling transferable training. Recently, [46] proposed _GeoDiff_, a diffusion model that predicts molecular conformations from a molecular graph. Like Timewarp, GeoDiff works in Cartesian coordinates and generalises to unseen molecules. However, GeoDiff was not applied to proteins, but small molecules, and does not target the Boltzmann distribution. In contrast to Timewarp, [37] learn the transition probability for multiple time-resolutions, accurately capturing the dynamics. However, they do not show transferability between systems. Most similarly to Timewarp, in recent work, [8] trained a transferable ML model to simulate the time-coarsened dynamics of polymers. However, unlike Timewarp, their model acts on coarse grained representations. Additionally, it was not applied to proteins, and there is no MH step, which means that errors can accumulate in the simulation without being corrected.

_Markov state models_ (MSMs) [32; 40; 10] work by running many short MD simulations, which are used to define a discrete state space, along with an estimated transition probability matrix. Similarly to Timewarp, MSMs estimate the transition probability between the state at a time \(t\) and the time \(t+\tau\), where \(\tau\gg\Delta t\). Recent work has applied deep learning to MSMs, leading to _VAMPnets_[23] and _deep generative MSMs_[45], which replace the MSM data-processing pipeline with deep networks. In contrast to Timewarp, these models are not transferable and model the dynamics in a coarse-grained, discrete state space, rather than in the all-atom coordinate representation.

There has been much previous work on _neural adaptive samplers_[38; 20; 21], which use deep generative models as proposal distributions. _A-NICE-MC_[38] uses a volume-preserving flow trained using a likelihood-free adversarial method. Other methods use objective functions designed to encourage exploration. The entropy term in our objective function is inspired by [41]. In contrast to these methods, Timewarp focuses on _generalising_ to new molecular systems without retraining.

Numerous enhanced sampling methods exist to for MD, such as parallel tempering [39; 6] or proposing updates of collective variables along transition paths [18; 27]. Given Timewarp's ability to accelerate MD, it often offers the opportunity to be integrated with these techniques.

## 3 Method

Consider the distribution of \(x(t+\tau)\) induced by an MD simulation of Equation (2) for a time \(\tau\gg\Delta t\), starting from \(x(t)\). We denote this conditional distribution by \(\mu(x(t+\tau)|x(t))\). Timewarp uses a deep probabilistic model to approximate \(\mu(x(t+\tau)|x(t))\) (see Figure 1). Once trained, the model is used in an MCMC method to sample from the Boltzmann distribution.

### Conditional normalising flows

We fit a _conditional normalising flow_, \(p_{\theta}(x(t+\tau)|x(t))\), to \(\mu(x(t+\tau)|x(t))\), where \(\theta\) are learnable parameters. Normalising flows are defined by a base distribution (usually a standard Gaussian), and a _diffeomorphism_\(f\), _i.e._ a differentiable bijection with a differentiable inverse. Specifically, we set \(p_{\theta}(x(t+\tau)|x(t))\) as the density of the distribution defined by the following generative process:

\[z^{p},z^{v}\sim\mathcal{N}(0,I),\quad x^{p}(t+\tau),x^{v}(t+\tau)\coloneqq f _{\theta}(z^{p},z^{v};x^{p}(t),x^{v}(t)).\] (3)Here \(z^{p}\in\mathbb{R}^{3N}\) and \(z^{v}\in\mathbb{R}^{3N}\). For all settings of \(\theta\) and \(x(t)\), \(f_{\theta}(\ \cdot\ ;x(t))\) is a diffeomorphism that takes the latent variables \((z^{p},z^{v})\in\mathbb{R}^{6N}\) to \((x^{p}(t+\tau),x^{v}(t+\tau))\in\mathbb{R}^{6N}\). The conditioning state \(x(t)\) parameterises a family of diffeomorphisms, defining a _conditional_ normalising flow [44]. Note that there are no invertibility constraints on the mapping from \(x(t)\) to the output \(x(t+\tau)\), only the map from \(z\) to \(x(t+\tau)\) must be invertible. Using the change of variables formula, we can evaluate:

\[p_{\theta}(x(t+\tau)|x(t))=\mathcal{N}\left(f_{\theta}^{-1}(x(t+\tau);x(t));0, I\right)\left|\det\mathcal{J}_{f_{\theta}^{-1}(\ ;x(t))}(x(t+\tau))\right|,\]

where \(f_{\theta}^{-1}(\ \cdot\ ;x(t)):\mathbb{R}^{6N}\rightarrow\mathbb{R}^{6N}\) is the inverse of the diffeomorphism \(f_{\theta}(\ \cdot\ ;x(t))\), and \(\mathcal{J}_{f_{\theta}^{-1}(\ ;x(t))}(x(t+\tau))\) denotes the Jacobian of \(f_{\theta}^{-1}(\ \cdot\ ;x(t))\) evaluated at \(x(t+\tau)\).

### Dataset generation

We generate MD trajectories by integrating Equation (2) using the _OpenMM_ library [7]. We simulate small proteins (peptides) in implicit water, _i.e._, without explicitly modelling the degrees of freedom of the water molecules. Specifically, we generate a dataset of trajectories \(\mathcal{D}=\{\mathcal{T}_{i}\}_{i=1}^{P}\), where \(P\) is the number of peptides. Each MD trajectory is temporally subsampled with a spacing of \(\tau\), so that \(\mathcal{T}_{i}=(x(0),x(\tau),x(2\tau),\ldots)\). During training, we randomly sample pairs \(x(t),x(t+\tau)\) from \(\mathcal{D}\). Each pair represents a sample from the conditional distribution \(\mu(x(t+\tau)|x(t))\). Additional details are provided in Appendix E. Since the flow is trained on trajectory data from _multiple_ peptides, we can deploy it at test time to generalise to _new_ peptides not seen in the training data.

### Augmented normalising flows

We are typically primarily interested in the distribution of the _positions_\(x^{p}\), rather than the velocities \(x^{v}\). Thus, it is not necessary for \(x^{v}(t),x^{v}(t+\tau)\) to represent the actual velocities of the atoms in Equation (3). We hence simplify the learning problem by treating \(x^{v}\) as _non-physical auxiliary variables_ within the _augmented normalising flow_ framework [9]. For each datapoint \(x(t)=x^{p}(t),x^{v}(t)\) in \(\mathcal{D}\), instead of obtaining \(x^{v}(t)\) by recording the velocities in the MD trajectory, we _discard_ the MD velocity and independently draw \(x^{v}(t)\sim\mathcal{N}(0,I)\). The auxiliary variables \(x^{v}(t)\) now contain no information about the future state \(x^{p}(t+\tau),x^{v}(t+\tau)\), since \(x^{v}(t)\) and \(x^{v}(t+\tau)\) are drawn independently. Hence we can simplify \(f_{\theta}\) to depend only on \(x^{p}(t)\), with \(x^{p}(t+\tau),x^{v}(t+\tau)\coloneqq f_{\theta}(z^{p},z^{v};x^{p}(t))\). We include auxiliary variables for two reasons: First, they increase the expressivity of the distribution for \(x^{p}\) without a prohibitive increase in computational cost [9; 2]. Second, constructing a conditional flow that respects _permutation equivariance_ is simplified with auxiliary variables -- see Section 4.1.

### Targeting the Boltzmann distribution with asymptotically unbiased MCMC

After training the flow \(p_{\theta}(x(t+\tau)|x(t))\), we use it as a proposal distribution in an MCMC method to target the joint distribution of the positions \(x^{p}\) and the auxiliary variables \(x^{v}\), which has density:

\[\mu_{\mathrm{aug}}(x^{p},x^{v})\propto\exp\left(-\tfrac{U(x^{p})}{k_{B}T} \right)\mathcal{N}(x^{v};0,I).\] (4)

Starting from an initial state \(X_{0}=(X_{0}^{p},X_{0}^{v})\in\mathbb{R}^{6N}\) for state \(m=0\), we iterate:

\[\tilde{X}_{m}\sim p_{\theta}(\ \cdot\ |X_{m}^{p}),\quad X_{m+1}:=\begin{cases} \tilde{X}_{m}&\text{with probability }\alpha(X_{m},\tilde{X}_{m})\\ X_{m}&\text{with probability }1-\alpha(X_{m},\tilde{X}_{m})\end{cases}\] (5)

where \(\alpha(X_{m},\tilde{X}_{m})\) is the _Metropolis-Hastings (MH) acceptance ratio_[24] targeting Equation (4):

\[\alpha(X,\tilde{X})=\min\left(1,\,\frac{\mu_{\mathrm{aug}}(\tilde{X})p_{ \theta}(X\ |\ \tilde{X}^{p})}{\mu_{\mathrm{aug}}(X)p_{\theta}(\tilde{X}\ |\ X^{p})}\right)\] (6)

The flow used for \(p_{\theta}\) must allow for efficient sampling _and_ exact likelihood evaluation, which is crucial for fast implementation of Equations (5) and (6). Additionally, after each MH step, we resample the auxiliary variables \(X^{v}\) using a _Gibbs sampling_ update:

\[(X_{m}^{p},X_{m}^{v})\leftarrow(X_{m}^{p},\epsilon),\quad\epsilon\sim\mathcal{ N}(0,I).\] (7)Iterating these updates yields a sample \(X^{p}_{m},X^{v}_{m}\sim\mu_{\mathrm{aug}}\) as \(m\to\infty\). To obtain a Boltzmann-distributed sample of the positions \(X^{p}_{m}\sim\mu\), we simply discard the auxiliary variables \(X^{v}_{m}\). As sending \(m\to\infty\) is infeasible, we simulate the chain until a fixed budget is reached. In practice, we find that acceptance rates for our models can be low, around \(1\%\). However, we stress that even with a low acceptance rate, our models can lead to faster exploration if the changes proposed are large enough, as we demonstrate in Section 6. Furthermore, we introduce a _batch sampling_ procedure which significantly speeds up sampling whilst maintaining detailed balance. This procedure samples a batch of proposals with a single forward pass, and accepts the first proposal that meets the MH acceptance criterion. Pseudocode for the MCMC algorithm is given in Algorithm 1 in Appendix C.

### Fast but biased exploration of the state space without MH corrections

Instead of using the MH correction to guarantee asymptotically unbiased samples, we can opt to use Timewarp in a simple _exploration_ algorithm. In this case, we neglect the MH correction and accept all proposals with energy changes below some cutoff. This allows much faster exploration of the state space, and in Section 6 we show that, although technically biased, this often leads to qualitatively accurate free energy estimates. It also succeeds in discovering all metastable states orders of magnitude faster than Algorithm 1 and standard MD, which could be used, _e.g._, to provide initialisation states for a subsequent MSM method. Pseudocode is given in Algorithm 2 in Appendix D.

## 4 Model architecture

We now describe the architecture of the flow \(f_{\theta}(z^{p},z^{v};x^{p}(t))\), which is shown in Figure 2.

RealNVP coupling flowOur architecture is based on RealNVP [5], which consists of a stack of _coupling layers_ which affinely transform subsets of the dimensions of the latent variable based on the other dimensions. Specifically, we transform the position variables based on the auxiliary variables, and vice versa. In the \(\ell\)th coupling layer of the flow, the following transformations are implemented:

\[z^{p}_{\ell+1} =s^{p}_{\ell,\theta}(z^{v}_{\ell};x^{p}(t))\odot z^{p}_{\ell}+t^ {p}_{\ell,\theta}(z^{v}_{\ell};x^{p}(t)),\] (8) \[z^{v}_{\ell+1} =s^{v}_{\ell,\theta}(z^{p}_{\ell+1};x^{p}(t))\odot z^{v}_{\ell}+t ^{v}_{\ell,\theta}(z^{p}_{\ell+1};x^{p}(t)).\] (9)

Going forward, we suppress the coupling layer index \(\ell\). Here \(\odot\) is the element-wise product, and \(s^{p}_{\theta}:\mathbb{R}^{3N}\to\mathbb{R}^{3N}\) is our _atom transformer_, a neural network based on the transformer architecture [43] that takes the auxiliary latent variables \(z^{v}\) and the conditioning state \(x(t)\) and outputs scaling factors for the position latent variables \(z^{p}\). The function \(t^{p}_{\theta}:\mathbb{R}^{3N}\to\mathbb{R}^{3N}\) is implemented as another atom transformer, which uses \(z^{v}\) and \(x(t)\) to output a translation of the position latent variables \(z^{p}\). The affine transformations of the position variables (in Equation (8)) are interleaved with similar affine transformations for the auxiliary variables (in Equation (9)). Since the scale and translation factors for the positions depend only on the auxiliary variables, and vice versa, the Jacobian of the transformation is lower triangular, allowing for efficient computation of the density. The full flow \(f_{\theta}\) consists of \(N_{\mathrm{coupling}}\) stacked coupling layers, beginning from \(z\sim\mathcal{N}(0,I)\) and ending with a sample from \(p_{\theta}(x(t+\tau)|x(t))\). This is depicted in Figure 2, Left. Note that there is a skip connection such that the output of the flow predicts the _change_\(x(t+\tau)-x(t)\), rather than \(x(t+\tau)\) directly.

Atom transformerWe now describe the _atom transformer_ network. Let \(x^{p}_{i}(t),z^{p}_{i},z^{v}_{i}\), all elements of \(\mathbb{R}^{3}\), denote respectively the position of atom \(i\) in the conditioning state, the position latent variable for atom \(i\), and the auxiliary latent variable for atom \(i\). To implement an atom transformer which takes \(z^{v}\) as input (such as \(s^{p}_{\theta}(z^{v},x^{p}(t))\) and \(t^{p}_{\theta}(z^{v},x^{p}(t))\) in Equation (8)), we first concatenate the variables associated with atom \(i\). This leads to a vector \(a_{i}:=[x^{p}_{i}(t),h_{i},z^{v}_{i}]^{\mathsf{T}}\in\mathbb{R}^{H+6}\), where \(z^{p}_{i}\) has been excluded since \(s^{p}_{\theta},t^{p}_{\theta}\) are not allowed to depend on \(z^{p}\). Here \(h_{i}\in\mathbb{R}^{H}\) is a learned embedding vector which depends only on the atom type. The vectors \(a_{i}\) are fed into an MLP \(\phi_{\mathrm{in}}:\mathbb{R}^{H+6}\to\mathbb{R}^{D}\), where \(D\) is the feature dimension of the transformer. The vectors \(\phi_{\mathrm{in}}(a_{1}),\ldots,\phi_{\mathrm{in}}(a_{N})\) are then fed into \(N_{\mathrm{transformer}}\) stacked transformer layers. After the transformer layers, they are passed through another atom-wise MLP, \(\phi_{\mathrm{out}}:\mathbb{R}^{D}\to\mathbb{R}^{3}\). The final output is in \(\mathbb{R}^{3N}\) as required. This is depicted in Figure 2, Middle. When implementing \(s^{v}_{\theta}\) and \(t^{v}_{\theta}\) from Equation (9), a similar procedure is performed on the vector \([x^{p}_{i}(t),h_{i},z^{p}_{i}]^{\mathsf{T}}\), but now including \(z^{p}_{i}\) and excluding \(z^{v}_{i}\). There are two key differences between the atom transformer and the architecture in [43]. First, to maintain permutation equivariance,we do not use a positional encoding. Second, instead of dot product attention, we use a simple _kernel self-attention_ module, which we describe next.

Kernel self-attentionWe motivate the kernel self-attention module with the observation that physical forces acting on the atoms in a molecule are _local: i.e._, they act more strongly on nearby atoms. Intuitively, for values of \(\tau\) that are not too large, the positions at time \(t+\tau\) will be more influenced by atoms that are nearby at time \(t\), compared to atoms that are far away. Thus, we define the attention weight \(w_{ij}\) for atom \(i\) attending to atom \(j\) as follows:

\[w_{ij}=\frac{\exp(-\|x_{i}^{p}-x_{j}^{p}\|_{2}^{2}/\ell^{2})}{\sum_{j^{\prime} =1}^{N}\exp(-\|x_{i}^{p}-x_{j^{\prime}}^{p}\|_{2}^{2}/\ell^{2})},\] (10)

where \(\ell\) is a lengthscale parameter. The outputs \(\{r_{\mathrm{out},i}\}_{i=1}^{N}\), given the inputs \(\{r_{\mathrm{in},i}\}_{i=1}^{N}\), are then:

\[r_{\mathrm{out},i}=\sum_{j=1}^{N}w_{ij}V\cdot r_{\mathrm{in},j},\] (11)

where \(V\in\mathbb{R}^{d_{\mathrm{out}}\times d_{\mathrm{in}}}\) is a learnable matrix. This kernel self-attention is an instance of the RBF kernel attention investigated in [42]. Similarly to [43], we introduce a _multihead_ version, where each head has a different lengthscale. This is illustrated in Figure 2, Right. We found that kernel self-attention was significantly faster to compute than dot product attention, and performed similarly.

### Symmetries

The MD dynamics respects certain physical _symmetries_ that would be advantageous to incorporate. We now describe how each of these symmetries is incorporated in Timewarp.

Permutation equivarianceLet \(\sigma\) be a permutation of the \(N\) atoms. Since the atoms have no intrinsic ordering, the only effect of a permutation of \(x(t)\) on the future state \(x(t+\tau)\) is to permute the atoms similarly, _i.e._,

\[\mu(\sigma x(t+\tau)|\sigma x(t))=\mu(x(t+\tau)|x(t)).\] (12)

Our conditional flow satisfies permutation equivariance exactly. To show this, we use the following proposition proved in Appendix A.1, which is an extension of [16, 36] for conditional flows:

Figure 2: Schematic illustration of the Timewarp conditional flow architecture, described in Section 4. _Left_: A single conditional RealNVP coupling layer. _Middle_: A single atom transformer module. _Right_: the multihead kernel self-attention module.

**Proposition 4.1**.: _Let \(\sigma\) be a symmetry action, and let \(f(\ \cdot\ ;\ \cdot\ )\) be an equivariant map such that \(f(\sigma z;\sigma x)=\sigma f(z;x)\) for all \(\sigma,z,x\). Further, let the base distribution \(p(z)\) satisfy \(p(\sigma z)=p(z)\) for all \(\sigma,z\). Then the conditional flow defined by \(z\sim p(z)\), \(x(t+\tau):=f(z;x(t))\) satisfies \(p(\sigma x(t+\tau)|\sigma x(t))=p(x(t+\tau)|x(t))\)._

Our flow satisfies \(f_{\theta}(\sigma z;\sigma x(t))=\sigma f_{\theta}(z;x(t))\) since the transformer is permutation equivariant, and permuting \(z\) and \(x(t)\) together permutes the inputs. Furthermore, the base distribution \(p(z)=\mathcal{N}(0,I)\) is permutation invariant. Note that the presence of auxiliary variables allows us to easily construct a permutation equivariant coupling layer.

Translation and rotation equivarianceConsider a transformation \(T=(R,a)\) that acts on \(x^{p}\):

\[Tx_{i}^{p}=Rx_{i}^{p}+a,\quad 1\leq i\leq N,\] (13)

where \(R\) is a \(3\times 3\) rotation matrix, and \(a\in\mathbb{R}^{3}\) is a translation vector. We would like the model to satisfy \(p_{\theta}(Tx(t+\tau)|Tx(t))=p_{\theta}(x(t+\tau)|x(t))\). We achieve translation equivariance by subtracting the average position of the atoms in the initial state (Appendix A.2). Rotation equivariance is not encoded in the architecture but is handled by data augmentation: each training pair \((x(t),x(t+\tau))\) is acted upon by a random rotation matrix \(R\) to form \((Rx(t),Rx(t+\tau))\) in each iteration.

## 5 Training objective

The model is trained in two stages: (i) _likelihood training_, the model is trained via maximum likelihood on pairs of states from the trajectories in the dataset. Let \(k\) index training pairs, such that \(\{(x^{(k)}(t),x^{(k)}(t+\tau))\}_{k=1}^{K}\) represents all pairs of states at times \(\tau\) apart in \(\mathcal{D}\). We optimise:

\[\mathcal{L}_{\mathrm{lik}}(\theta):=\tfrac{1}{K}\sum_{k=1}^{K}\log p_{\theta} (x^{(k)}(t+\tau)|x^{(k)}(t)).\] (14)

(ii) _acceptance training_, the model is fine-tuned to maximise the probability of MH acceptance. Let \(x^{(k)}(t)\) be sampled uniformly from \(\mathcal{D}\). Then, we use the model to sample \(\tilde{x}_{\theta}^{(k)}(t+\tau)\sim p_{\theta}(\,\cdot\,|x^{(k)}(t))\) using Equation (3). We use this to optimise the acceptance probability in Equation (6) with respect to \(\theta\). Let \(r_{\theta}(X,\tilde{X})\) denote the model-dependent term in the acceptance ratio in Equation (6):

\[r_{\theta}(X,\tilde{X}):=\frac{\mu_{\mathrm{aug}}(\tilde{X})p_{\theta}(X\mid \tilde{X}^{p})}{\mu_{\mathrm{aug}}(X)p_{\theta}(\tilde{X}\mid X^{p})}.\] (15)

The acceptance objective is then given by:

\[\mathcal{L}_{\mathrm{acc}}(\theta):=\tfrac{1}{K}\sum_{k=1}^{K}\log r_{\theta} (x^{(k)}(t),\tilde{x}_{\theta}^{(k)}(t+\tau)).\] (16)

Training to maximise the acceptance probability can lead to the model proposing changes that are too small: if \(\tilde{x}_{\theta}^{(k)}(t+\tau)=x^{(k)}(t)\), then all proposals will be accepted. To mitigate this, during acceptance training, we use an objective which is a weighted average of \(\mathcal{L}_{\mathrm{acc}}(\theta)\), \(\mathcal{L}_{\mathrm{lik}}(\theta)\) and a Monte Carlo estimate of the average differential entropy,

\[\mathcal{L}_{\mathrm{ent}}(\theta):=-\tfrac{1}{K}\sum_{k=1}^{K}\log p_{\theta} (\tilde{x}_{\theta}^{(k)}(t+\tau)|x^{(k)}(t)).\] (17)

## 6 Experiments

We evaluate Timewarp on small peptide systems. To compare with MD, we focus on the slowest transitions between metastable states, as these are the most difficult to traverse. To find these, we use _time-lagged independent component analysis_ (TICA) [30], a linear dimensionality reduction technique that maximises the autocorrelation of the transformed coordinates. The slowest components, TIC 0 and TIC 1, are of particular interest. To measure the speed-up achieved by Timewarp, we compute the _effective sample size_ per second of wall-clock time (ESS/s) for the TICA components. The ESS/s is given by

\[\text{ESS/s}=\frac{M_{\text{eff}}}{t_{\text{sampling}}}=\frac{M}{t_{\text{ sampling}}\left(1+2\sum_{\tau=1}^{\infty}\rho_{\tau}\right)},\] (18)where \(M\) is the chain length, \(M_{\text{eff}}\) is the effective number of samples, \(t_{\text{sampling}}\) is the sampling wall-clock time, and \(\rho_{\tau}\) is the autocorrelation for the lag time \(\tau\)[26]. The speed-up factor is defined as the ESS/s achieved by Timewarp divided by the ESS/s achieved by MD. Additional experiments and results can be found in Appendix B. We train three flow models on three datasets: (i) \(AD\), consisting of simulations of alanine dipeptide, (ii) \(2AA\), with peptides with 2 amino acids, and (iii) \(4AA\), with peptides with 4 amino acids. All datasets are created with MD simulations performed with the same parameters (see Appendix E). For 2AA and 4AA, we train on a randomly selected trainset of short trajectories (\(50\text{ns}=10^{8}\) steps), and evaluate on unseen test peptides. The relative frequencies of the amino acids in 2AA and 4AA are similar across the splits. For 4AA, the training set consists of about \(1\%\) of the total number of possible tetrapeptides (\(20^{\text{t}}\)), making the generalisation task significantly more difficult than for 2AA. For more details see Table 2 in Appendix E.

Alanine dipeptide (AD)We first investigate alanine dipeptide, a small (22 atoms) single peptide molecule. We train Timewarp on AD as described in Section 5 and sample new states using Algorithm 1 for a chain length of 10 million, accepting roughly \(2\%\) of the proposals. In Figure 3a we visualise the samples using a _Ramachandran plot_[33], which shows the distribution of the backbone dihedral angles \(\varphi\) and \(\psi\). Each mode in the plot represents a metastable state. We see that the Timewarp samples closely match MD, visiting all the metastable states with the correct relative weights. In Figure 3b we plot the free energy (_i.e._, the relative log probability) of the \(\varphi\) and \(\psi\) angles, again showing close agreement. The roughness in the plot is due to some regions of state space having very few samples. In Figure 3c we show, for an initial state \(x(t)\), the conditional distribution of MD obtained by integrating Equation (2), \(\mu(x(t+\tau)|x(t))\), compared with the model \(p_{\theta}(x(t+\tau)|x(t))\), demonstrating close agreement. Finally, Figure 3d shows the time-evolution of the \(\varphi\) angle for MD and Timewarp. Timewarp exhibits significantly more transitions between the metastable states than MD. As a result, the autocorrelation along the \(\varphi\) angle decays much faster in terms of wall-clock time, resulting in a \(\approx 7\times\) speed-up in terms of ESS/s compared to MD (see Appendix B.4).

Dipeptides (2AA)Next, we demonstrate transferability on dipeptides in 2AA. After training on the train dipeptides, we deploy Timewarp with Algorithm 1 on the test dipeptides for a chain length of 20 million. Timewarp achieves acceptance probabilities between \(0.03\%\) and \(2\%\) and explores all metastable states (Appendix B.1). The results are shown for the dipeptides QW and HT in Figure 3ef, showing close agreement between Timewarp and long MD chains (1 \(\mu\)s = \(2\times 10^{9}\) steps). For these dipeptides, Timewarp achieves ESS/s speed-up factors over MD of 5 and 33 respectively (Appendix B.4). In Figure 4, Left, we show the speed-up factors for Timewarp verses MD for each of the \(100\) test dipeptides. Timewarp provides a median speed-up factor of about five across these peptides. In addition, we generate samples with the Timewarp model _without_ the MH correction as detailed in Section 3.5. We sample \(100\) parallel chains for only \(10000\) steps starting from the same

Figure 3: _Left half:_ **Alanine dipeptide experiments.** (a) Ramachandran plots for MD and Timewarp samples generated according to Algorithm 1. (b) Free energy comparison for the two dihedral angles \(\varphi\) and \(\psi\). (c) Ramachandran plots for the conditional distribution of MD compared with the Timewarp model. Red cross denotes initial state. (d) Time dependence of the \(\varphi\) dihedral angle of MD and the Markov chain generated with the Timewarp model. _Right half:_ **Experiments on 2AA test dipeptides QW (top row) and HT (bottom row). (e) TICA plots for a long MD chain and samples generated with the Timewarp MCMC algorithm (Algorithm 1). (f) Free energy comparison for the MD trajectory, Timewarp MCMC (Algorithm 1), and Timewarp exploration (Algorithm 2).

initial state for each test peptide. For each peptide we select _only one_ of these chains that finds all meta-stable states for evaluations. As before, we compute the ESS/s to compare with MD, showing a median speedup factor of \(\approx 600\) (Figure 3(c)). Note that the actual speedup when using all the chains sampled in parallel will be much larger. Timewarp exploration leads to free energy estimates that are qualitatively similar to MD, but less accurate than Timewarp MCMC (Figure 2(f)).

Tetrapeptides (4AA)Finally, we study the more challenging 4AA dataset. After training on the trainset, we sample \(20\) million Markov chain states for each test tetrapeptide using Algorithm 1 and compare with long MD trajectories (\(1\mu\)s). In contrast to the simpler dipeptides, both Timewarp MCMC and the long MD trajectories miss some metastable states. However, Timewarp in exploration mode (Algorithm 2) can be used as a validation tool to quickly verify exploration of the whole state space. Figure 4(a) shows that metastable states unexplored by MD and Timewarp MCMC can be found by the Timewarp exploration algorithm. We carefully confirm the physical validity of these discovered states by running shorter MD trajectories in their vicinity (see Appendix B.5), to ensure that they are not simply artefacts invented by the model. As with 2AA, we again report the speedup factors for Timewarp relative to MD in Figure 3(b),d. Although Timewarp MCMC fails to speed up sampling for most tetrapeptides, Timewarp _exploration_ shows a median speedup factor of \(\approx 50\). For 8 test tetrapeptides, MD fails to explore all metastable states, whereas Timewarp succeeds -- these are marked in green. For 10 tetrapeptides, Timewarp MCMC fails to find all metastable states found by MD -- these are marked in grey. Figure 4(b) shows that when Timewarp MCMC discovers all metastable states, its free energy estimates match those of MD very well. However, it sometimes

Figure 4: Speed-up factors in terms of ESS/s ratios for the slowest TICA component for the Timewarp MCMC and exploration algorithms, compared to MD. The dashed red line shows a speed-up factor of one. Gray areas depict peptides where Timewarp fails to explore all meta-stable states within \(20\) million steps, but MD does. Green areas depict peptides where MD fails to find all metastable states, but Timewarp does. (a), (c) Speed-up for the Timewarp MCMC algorithm (Algorithm 1) on test dipeptides (2AA) and tetrapeptides (4AA), respectively. (b), (d) Speed-up for the Timewarp exploration algorithm (Algorithm 2) on test dipeptides (2AA) and tetrapeptides (4AA), respectively.

Figure 5: Experiments on 4AA test tetrapeptides SAEL, CTSA and LPEM (top, middle and bottom rows respectively). Samples were generated via MD, Timewarp exploration (Algorithm 2), and Timewarp MCMC (Algorithm 1). (a) TICA plots of samples. (b) Free energies along the first two TICA components. (c) Potential energy distribution.

misses metastable states leading to poor free energy estimates in those regions. Figure 5c shows that Timewarp MCMC also leads to a potential energy distribution that matches MD very closely. In contrast, Timewarp exploration discovers all metastable states (even ones that MD misses), but has less accurate free energy plots. It also has a potential energy distribution that is slightly too high relative to MD and Timewarp MCMC.

## 7 Limitations / Future work

The Timewarp MCMC algorithm generates low acceptance probabilities (\(<1\%\)) for most peptides (see Appendix B.1). However, this is not a limitation in itself. In general, a larger proposal timestep \(\tau\) yields smaller acceptance rates as the prediction problem becomes more difficult. However, due to Algorithm 1, we can evaluate multiple samples in parallel at nearly no additional costs. As a result, a lower acceptance rate, when coupled with a larger timestep \(\tau\), is often a favorable trade-off. While we speed-up only roughly a third of the 4AA peptides when using the MH correction, beating MD in wall-clock time on unseen peptides in the all-atom representation is a challenging task which has not been demonstrated by ML methods before. Furthermore, one could consider targeting systems using a semi-empirical force field instead of a classical one. Given that Timewarp requires considerably fewer energy evaluations than MD simulations, one can anticipate a more substantial acceleration in this context.

Although MD and Timewarp MCMC fail to find some metastable states that were found by Timewarp exploration, we refrained from running MD and Timewarp MCMC longer due to the high computational cost (Appendix F). Timewarp generates fewer samples compared to traditional MD simulations within the same timeframe. Consequently, this scarcity of samples becomes even more pronounced in transition states, which makes Timewarp difficult to apply to chemical reactions.

Timewarp could be integrated with other enhanced sampling methods, like parallel tempering or transition path sampling. In the case of parallel tempering, the effective integration requires the training of the Timewarp model across multiple temperatures, which then allows to sample all the replicas with Timewarp instead of MD. We could also alternate Timewarp proposals with learned updates to collective variables, like dihedral angles. These combined steps would still allow unbiased sampling from the target distribution [27].

Moreover, we only studied small peptide systems in this work. Scaling Timewarp to larger systems remains a topic for future research, and there are several promising avenues to consider. One approach is to explore different network architectures, potentially capturing all the symmetries inherent in the system. Another option is to study coarse-grained structures instead of all-atom representations, to reduce the dimensionality of the problem.

## 8 Conclusion

We presented Timewarp, a transferable enhanced sampling method which uses deep networks to propose large conformational changes when simulating molecular systems. We showed that Timewarp used with an MH correction can accelerate asymptotically unbiased sampling on many unseen dipeptides, allowing faster computation of equilibrium expectation values. Although this acceleration was only possible for a minority of the _tetrapeptides_ we considered, we showed that Timewarp used _without_ the MH correction explores the metastable states of both dipeptides and tetrapeptides much faster than standard MD, and we verify the metastable states discovered are physically meaningful. This provides a promising method to quickly validate if MD simulations have visited all metastable states. Although further work needs to be done to scale Timewarp to larger, more interesting biomolecules, this work clearly demonstrates the ability of deep learning algorithms to leverage transferability to accelerate the MD sampling problem.

## Acknowledgments

We thank Bas Veeling, Claudio Zeni, Andrew Fowler, Lixin Sun, Chris Bishop, Rianne van den Berg, Hannes Schulz, Max Welling and the entire Microsoft AI4Science team for insightful discussions and computing help.

## References

* [1] Ignasi Buch, Toni Giorgino, and Gianni De Fabritiis. Complete reconstruction of an enzyme-inhibitor binding process by molecular dynamics simulations. _Proceedings of the National Academy of Sciences_, 108(25):10184-10189, 2011.
* [2] Jianfei Chen, Cheng Lu, Biqi Chenli, Jun Zhu, and Tian Tian. Vflow: More expressive generative flows with variational data augmentation. In _International Conference on Machine Learning_, pages 1660-1669. PMLR, 2020.
* [3] Cecilia Clementi. Coarse-grained models of protein folding: toy models or predictive tools? _Current opinion in structural biology_, 18(1):10-15, 2008.
* [4] Manuel Dibak, Leon Klein, Andreas Kramer, and Frank Noe. Temperature steerable flows and Boltzmann generators. _Phys. Rev. Res._, 4:L042005, Oct 2022.
* [5] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In _International Conference on Learning Representations_, 2017.
* [6] David J. Earl and Michael W. Deem. Parallel tempering: Theory, applications, and new perspectives. _Phys. Chem. Chem. Phys._, 7:3910-3916, 2005.
* [7] Peter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A Beauchamp, Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al. Openmm 7: Rapid development of high performance algorithms for molecular dynamics. _PLoS computational biology_, 13(7):e1005659, 2017.
* [8] Xiang Fu, Tian Xie, Nathan J. Rebello, Bradley Olsen, and Tommi S. Jaakkola. Simulate time-integrated coarse-grained molecular dynamics with multi-scale graph networks. _Transactions on Machine Learning Research_, 2023.
* [9] C. Huang, Laurent Dinh, and Aaron C. Courville. Augmented normalizing flows: Bridging the gap between generative flows and latent variable models. _ArXiv_, abs/2002.07101, 2020.
* [10] Brooke E Husic and Vijay S Pande. Markov state models: From an art to a science. _Journal of the American Chemical Society_, 140(7):2386-2396, 2018.
* [11] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi S. Jaakkola. Torsional diffusion for molecular conformer generation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [12] William Thomson Baron Kelvin. _The molecular tactics of a crystal_. Clarendon Press, 1894.
* [13] Leon Klein, Andreas Kramer, and Frank Noe. Equivariant flow matching. _arXiv preprint arXiv:2306.15030_, 2023.
* [14] Sebastian Kmiecik, Dominik Gront, Michal Kolinski, Lukasz Wieteska, Aleksandra Elzbieta Dawid, and Andrzej Kolinski. Coarse-grained protein models and their applications. _Chemical reviews_, 116(14):7898-7936, 2016.
* [15] Jonas Kohler, Michele Invernizzi, Pim de Haan, and Frank Noe. Rigid body flows for sampling molecular crystal structures. In _International Conference on Machine Learning, ICML 2023_, volume 202 of _Proceedings of Machine Learning Research_, pages 17301-17326. PMLR, 2023.
* [16] Jonas Kohler, Leon Klein, and Frank Noe. Equivariant flows: exact likelihood generative learning for symmetric densities. In _International Conference on Machine Learning_, pages 5361-5370. PMLR, 2020.
* [17] Jonas Kohler, Andreas Kramer, and Frank Noe. Smooth normalizing flows. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 2796-2809. Curran Associates, Inc., 2021.
* [18] Alessandro Laio and Michele Parrinello. Escaping free-energy minima. _Proceedings of the National Academy of Sciences_, 99(20):12562-12566, 2002.

* [19] Paul Langevin. Sur la theorie du mouvement brownien. _Compt. Rendus_, 146:530-533, 1908.
* [20] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with neural networks. In _International Conference on Learning Representations_, 2018.
* [21] Zengyi Li, Yubei Chen, and Friedrich T. Sommer. A neural network MCMC sampler that maximizes proposal entropy. _Entropy_, 23(3), 2021.
* [22] Kresten Lindorff-Larsen, Stefano Piana, Ron O Dror, and David E Shaw. How fast-folding proteins fold. _Science_, 334(6055):517-520, 2011.
* [23] Andreas Mardt, Luca Pasquali, Hao Wu, and Frank Noe. VAMPnets for deep learning of molecular kinetics. _Nature communications_, 9(1):1-11, 2018.
* [24] Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. Equation of state calculations by fast computing machines. _The Journal of Chemical Physics_, 21(6):1087-1092, 1953.
* [25] Laurence I Midgley, Vincent Stimper, Javier Antoran, Emile Mathieu, Bernhard Scholkopf, and Jose Miguel Hernandez-Lobato. Se (3) equivariant augmented coupling flows. _arXiv preprint arXiv:2308.10364_, 2023.
* [26] Radford M Neal. _Probabilistic inference using Markov chain Monte Carlo methods_. Department of Computer Science, University of Toronto Toronto, ON, Canada, 1993.
* [27] Jerome P Nilmeier, Gavin E Crooks, David DL Minh, and John D Chodera. Nonequilibrium candidate monte carlo is an efficient tool for equilibrium simulation. _Proceedings of the National Academy of Sciences_, 108(45):E1009-E1018, 2011.
* [28] Frank Noe, Simon Olsson, Jonas Kohler, and Hao Wu. Boltzmann generators -- sampling equilibrium states of many-body systems with deep learning. _Science_, 365:eaaw1147, 2019.
* [29] Frank Noe, Christof Schutte, Eric Vanden-Eijnden, Lothar Reich, and Thomas R Weikl. Constructing the equilibrium ensemble of folding pathways from short off-equilibrium simulations. _Proceedings of the National Academy of Sciences_, 106(45):19011-19016, 2009.
* [30] Guillermo Perez-Hernandez, Fabian Paul, Toni Giorgino, Gianni De Fabritiis, and Frank Noe. Identification of slow molecular order parameters for Markov model construction. _The Journal of chemical physics_, 139(1):07B604_1, 2013.
* [31] Nuria Plattner, Stefan Doerr, Gianni De Fabritiis, and Frank Noe. Complete protein-protein association kinetics in atomic detail revealed by molecular dynamics simulations and Markov modelling. _Nature chemistry_, 9(10):1005-1011, 2017.
* [32] Jan-Hendrik Prinz, Hao Wu, Marco Sarich, Bettina Keller, Martin Senne, Martin Held, John D Chodera, Christof Schutte, and Frank Noe. Markov models of molecular kinetics: Generation and validation. _The Journal of chemical physics_, 134(17):174105, 2011.
* [33] G N Ramachandran, C Ramakrishnan, and V Sasisekharan. Stereochemistry of polypeptide chain configurations. _Journal of Molecular Biology_, pages 95-99, 1963.
* [34] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, KDD '20, page 3505-3506, New York, NY, USA, 2020. Association for Computing Machinery.
* [35] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _International conference on machine learning_, pages 1530-1538. PMLR, 2015.
* [36] Danilo Jimenez Rezende, Sebastien Racaniere, Irina Higgins, and Peter Toth. Equivariant Hamiltonian flows. _arXiv preprint arXiv:1909.13739_, 2019.
* [37] Mathias Schreiner, Ole Winther, and Simon Olsson. Implicit transfer operator learning: Multiple time-resolution surrogates for molecular dynamics. _arXiv preprint arXiv:2305.18046_, 2023.

* [38] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-NICE-MC: Adversarial training for MCMC. _Advances in Neural Information Processing Systems_, 30, 2017.
* [39] Robert H Swendsen and Jian-Sheng Wang. Replica monte carlo simulation of spin-glasses. _Physical review letters_, 57(21):2607, 1986.
* [40] William C Swope, Jed W Pitera, and Frank Suits. Describing protein folding kinetics by molecular dynamics simulations. 1. theory. _The Journal of Physical Chemistry B_, 108(21):6571-6581, 2004.
* [41] Michalis Titsias and Petros Dellaportas. Gradient-based adaptive Markov chain Monte Carlo. _Advances in Neural Information Processing Systems_, 32, 2019.
* [42] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 4344-4353, Hong Kong, China, November 2019. Association for Computational Linguistics.
* [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems_, 30, 2017.
* [44] Christina Winkler, Daniel Worrall, Emiel Hoogeboom, and Max Welling. Learning likelihoods with conditional normalizing flows. _arXiv preprint arXiv:1912.00042_, 2019.
* [45] Hao Wu, Andreas Mardt, Luca Pasquali, and Frank Noe. Deep generative Markov state models. _Advances in Neural Information Processing Systems_, 31, 2018.
* [46] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In _International Conference on Learning Representations_, 2022.

Symmetries of the architecture

### Proof of Proposition 4.1

In this appendix we provide more details on the equivariance of the Timewarp architecture. We first prove Proposition 4.1 from the main body:

Proof.: Let \(X(t+\tau)_{x(t)}\) denote the random variable obtained by sampling \(Z\sim p(z)\) and computing \(X(t+\tau):=f(Z;x(t))\). Here we subscript \(X(t+\tau)_{x(t)}\) by \(x(t)\) to emphasize that this is the random variable obtained when conditioning the flow on \(x(t)\). We first note that the equivariance condition on the densities \(p(\sigma x(t+\tau)|\sigma x(t))=p(x(t+\tau)|x(t))\) is equivalent to the following constraint on the random variables:

\[X(t+\tau)_{\sigma x(t)}\overset{d}{=}\sigma X(t+\tau)_{x(t)},\] (19)

where \(\overset{d}{=}\) denotes equality in distribution. To see this, let \(p_{X}\) denote the density of the random variable \(X\). Then, in terms of densities, Equation (19) is equivalent to stating that, for all \(x\),

\[p_{X(t+\tau)_{\sigma x(t)}}(x) =p_{\sigma X(t+\tau)_{x(t)}}(x)\] (20) \[=p_{X(t+\tau)_{x(t)}}(\sigma^{-1}x),\] (21)

where in Equation (21) we used the change-of-variables formula, along with the fact that the group actions we consider (rotations, translations, permutations) have unit absolute Jacobian determinant. Redefining \(x\leftarrow\sigma x\), we get that for all \(x\),

\[p_{X(t+\tau)_{\sigma x(t)}}(\sigma x)=p_{X(t+\tau)_{x(t)}}(x).\] (22)

Recalling the notation that \(X(t+\tau)_{x(t)}\) is interpreted as the random variable obtained by conditioning the flow on \(x(t)\), this can be written as

\[p(\sigma x|\sigma x(t))=p(x|x(t))\] (23)

which is exactly the equivariance condition stated in terms of densities above. Having rephrased the equivariance condition in terms of random variables in Equation (19), the proof of Proposition 4.1 is straightforward.

\[X(t+\tau)_{\sigma x(t)} :=f(Z,\sigma x(t))\] (24) \[\overset{d}{=}f(\sigma Z,\sigma x(t))\] (25) \[=\sigma f(Z,x(t))\] (26) \[:=\sigma X(t+\tau)_{x(t)},\] (27)

where in Equation (25) we used the fact that the base distribution \(p(z)\) is \(\sigma\)-invariant. 

### Translation equivariance via canonicalisation

We now describe the canonicalisation technique used to make our models translation equivariant. Let \(q(x^{p}(t+\tau),x^{v}(t+\tau)|x^{p}(t))\) be an arbitrary conditional density model, which is not necessarily translation equivariant. We can make it translation equivariant in the following way. Let \(\overline{x^{p}}\) denote the average position of the atoms,

\[\overline{x^{p}}:=\frac{1}{N}\sum_{i=1}^{N}x_{i}^{p}.\] (28)

Then we define

\[p(x^{p}(t+\tau),x^{v}(t+\tau)|x^{p}(t)):=q(x^{p}(t+\tau)-\overline{x^{p}(t)},x^{v}(t+\tau)|x^{p}(t)-\overline{x^{p}(t)})\] (29)

where the subtraction of \(\overline{x^{p}(t)}\) is broadcasted over all atoms. We now consider the effect of translating both \(x^{p}(t)\) and \(x^{p}(t+\tau)\) by the same amount. Let \(a\) be a translation vector in \(\mathbb{R}^{3}\). Then

\[p(x^{p}(t+\tau)+a, x^{v}(t+\tau)|x^{p}(t)+a)\] (30) \[=q(x^{p}(t+\tau)+a-(\overline{x^{p}(t)+a}),x^{v}(t+\tau)|x^{p}(t )+a-(\overline{x^{p}(t)+a}))\] (31) \[=q(x^{p}(t+\tau)+a-\overline{x^{p}(t)}-a,x^{v}(t+\tau)|x^{p}(t)+ a-\overline{x^{p}(t)}-a)\] (32) \[=q(x^{p}(t+\tau)-\overline{x^{p}(t)},x^{v}(t+\tau)|x^

[MISSING_PAGE_FAIL:15]

Figure 8: Experiments for the 2AA test dipeptides DH (first row), GT (second row), TK (third row), and CW (last row). Comparison of the long MD trajectory and Timewarp MCMC (Algorithm 1). (a) TICA plots. (b) Free energy comparison of the first two TICA components. (c) Autocorrelation for the TIC 0 component. (d) Time dependence of the TIC 0 component.

Figure 7: Experiments on 2AA test dipeptides QW (top row), HT (middle row) and GP (bottom row). Comparison of the long MD trajectory and Timewarp MCMC (Algorithm 1). (a) TICA plots. (b) Free energy comparison of the first two TICA components. (c) Autocorrelation for the TIC 0 component.

emphphasising the importance of the Metropolis-Hastings correction to obtain unbiased samples from the Boltzmann distribution with the Timewarp model.

### Autocorrelations

In Section 6 we compute the speedup of the Timewarp model by comparing the effective sample sizes per second (Equation (18)) for the slowest transition with MD. As the ESS depends on the autocorrelation, it is also insightful to look at the autocorrelation decay in terms of wall-clock time. We show some example autocorrelations for the investigated peptides in Figures 7, 8 and 10. Note that the area under the autocorrelation curve is inversely proportional to the ESS.

### Exploration of new metastable states

For some tetraeptides in the test set even long MD trajectories (\(1\mu\)s) miss some metastable states, _e.g._ for LYVI and CSTA shown in Figure 12a. However, we can easily explore these with the Timewarp exploration algorithm (Algorithm 2). To confirm that these additional metastable states are in fact

Figure 10: Comparing the conditional Boltzmann distributions generated with MD trajectories and the Timewarp model for alanine dipeptide. (a) Ramachandran plots for the conditional distributions compared with the equilibrium Boltzmann distribution. The red cross indicates the conditioning state. This is similar to the plot shown in Figure 3c, but here showing a different conditioning state. The match between the conditional distributions is not as close here as it is for Figure 3c, which could be because here the conditioning state is chosen to be in the less likely metastable state. (b) Projections on the first two dihedral angles for the conditional distributions. (c) Potential energies of the conditional distributions. (d) Autocorrelations for samples generated according to the MCMC algorithm (Algorithm 1) compared with a long MD trajectory. Note that this autocorrelation plot is not for the conditional distribution, but corresponds to the results shown in Figure 3.

Figure 9: Experiments on 4AA test tetrapeptides AWCK, LYVI and CSFQ (top, middle and bottom rows respectively). Samples were generated via MD, Timewarp exploration (Algorithm 2), and Timewarp MCMC (Algorithm 1). (a) TICA plots of samples. (b) Free energies along the first two TICA components. (c) Potential energy distribution. For AWCK all metastable states are found by all methods, for LYVI the MD trajectory misses one state, and for CSFQ Timewarp MCMC misses the slowest transition. In all cases Timewarp exploration discovers all metastable states.

[MISSING_PAGE_FAIL:18]

```
0: Initial state \(X_{0}=(X_{0}^{p},X_{0}^{v})\), chain length \(M\), proposal batch size \(B\). \(m\gets 0\) while\(m<M\)do  Sample \(\tilde{X}_{1},\dots,\tilde{X}_{B}\sim p_{\theta}(\,\cdot\,|X_{m}^{p})\) {Batch sample} for\(b=1,\dots,B\)do \(\epsilon\sim\mathcal{N}(0,I)\) {Resample auxiliary variables} \(X_{b}\leftarrow(X_{m}^{p},\epsilon)\)  Sample \(I_{b}\sim\mathrm{Bernoulli}(\alpha(X_{b},\tilde{X}_{b}))\) endfor if\(S:=\{b:I_{b}=1,1\leq b\leq B\}\neq\emptyset\)then \(a=\min(S)\) {First accepted sample} \((X_{m+1}^{p},\dots,X_{m+a-1}^{p})\leftarrow(X_{m}^{p},\dots,X_{m}^{p})\) \(X_{m+a}^{p}\leftarrow\tilde{X}_{a}^{p}\) \(m\gets m+a\) else \((X_{m+1}^{p},\dots,X_{m+B}^{p})\leftarrow(X_{m}^{p},\dots,X_{m}^{p})\) \(m\gets m+B\) endif endwhile output\(X_{0}^{p},\dots X_{M}^{p}\) ```

**Algorithm 1** Timewarp MH-corrected MCMC with batched proposals

Figure 12: Validation of new metastable states found with Timewarp exploration. The red crosses indicate the different conditioning states. (a) TICA plots for (not conditional) samples generated via a long MD trajectory, Timewarp exploration (Algorithm 2), and Timewarp MCMC (Algorithm 1). Timewarp exploration discovers some metastable states unseen in the long MD trajectories (bottom of LYVI plot, bottom left of CTSA plot). (b) Conditional distributions generated with MD or with the Timewarp model, starting from states visited by the _long_ MD trajectory shown in (a). Some short MD trajectories now discover the new metastable states, verifying that they are indeed valid states. Only the final state of each short MD trajectory is recorded. (c) Conditional distributions generated with MD or with the Timewarp model, starting from the new metastable states discovered by one of the _short_ MD trajectories shown in (b).

Note that unlike Algorithm 1, there is no need for the auxiliary variables, since the conditional flow only depends on the positions, and no MH acceptance ratio is computed here. The potential energy \(U\) includes here also a large penalty if the ordering of a chirality center changes as described in Appendix A.3. As sampling proposals from Timewarp can be batched, we can generate \(B\) chains in parallel, all starting from the same initial state. This batched sampling procedure leads to even further speedups. For all exploration experiments we use a batch size of 100, and run \(M=10000\) exploration steps. The maximum allowed energy change cutoff is set at \(\Delta U_{\max}=300\)kJ/mol.

## Appendix E Dataset details

We evaluate our model on three different datasets, AD, 2AA, and 4AA, as introduced in Section 6. All datasets are simulated in implicit solvent using the openMM library [7]. For all MD simulations we use the parameters shown in Table 1.

We present more dataset details, like simulation times and number of peptides, in Table 2.

## Appendix F Hyperparameters

Depending on the dataset, different Timewarp model sizes were used, as shown in Table 3. For all datasets the Multihead kernel self-attention layer consists of \(6\) heads with lengthscales \(\ell_{i}=\{0.1,0.2,0.5,0.7,1.0,1.2\}\), given in nanometers.

\begin{table}
\begin{tabular}{l c} \hline \hline Force Field & amber-14 \\ Time step & \(0.5\)fs \\ Friction coefficient & \(0.3\frac{1}{\text{ps}}\) \\ Temperature & \(310\)K \\ Integrator & LangevinMiddelIntegrator \\ \hline \hline \end{tabular}
\end{table}
Table 1: OpenMM MD simulation parameters

\begin{table}
\begin{tabular}{l c c} \hline \hline Dataset name & AD & 2AA & 4AA \\ \hline Training set simulation time & \(100\) ns & \(50\) ns & \(50\) ns \\ Test set simulation time & \(100\) ns & \(1\)\(\mu\)s & \(1\)\(\mu\)s \\ MD integration step \(\Delta t\) & \(0.5\) fs & \(0.5\) fs & \(0.5\) fs \\ Timewarp prediction time \(\tau\) & \(0.5\times 10^{6}\)fs & \(0.5\times 10^{6}\)fs & \(0.5\times 10^{5}\)fs \\ No. of training peptides & \(1\) & \(200\) & \(1400\) \\ No. of training pairs per peptide & \(2\times 10^{5}\) & \(1\times 10^{4}\) & \(1\times 10^{4}\) \\ No. of test peptides & \(1\) & \(100\) & \(30\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Dataset detailsThe \(\phi_{\text{in}}\) and \(\phi_{\text{out}}\) MLPs use SiLUs as activation functions, while the Transformer MLPs use ReLUs. Note the transformer MLP refers to the atom-wise MLP shown in Figure 2, Middle inside the transformer block. The shapes of these MLPs vary for the different datasets as shown in Table 4.

The first linear layers in the kernel self-attention module always has the shape \([128,768]\) (in Section 4 denoted as \(V\)), and the second (after concatenating the output of head head) has the shape \([768,128]\). The transformer feature dimension \(D\) is for all datasets \(128\).

After likelihood training, we fine-tune the model for the AD and 2AA dataset with a combination of all three losses discussed in Section 5. We did not perform fine tuning for the model trained on the 4AA dataset. We use a weighted sum of the losses with weights detailed in Table 5.

We use the _FusedLamb_ optimizer and the _DeepSpeed_ library [34] for all experiments. The batch size as well as the training times are reported in Table 6.

All simulations are started with a learning rate of \(5\times 10^{-4}\), the learning rate is then consecutively decreased by a factor of \(2\) upon hitting training loss plateaus.

## Appendix G Computing infrastructure

The training was performed on \(4\) NVIDIA A-100 GPUs for the 2AA and 4AA datasets and on a single NVIDIA A-100 GPU for the AD dataset. Inference with the model as well as all MD simulations were conducted on single NVIDIA V-100 GPUs for AD and 2AA, and on single NVIDIA A-100 GPUs for 4AA.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset + training method & Batch size & No. of A-100s & Training time \\ \hline AD -- likelihood & \(256\) & 1 & 1 week \\ AD -- acceptance & \(64\) & 1 & 2 days \\
2AA -- likelihood & \(256\) & 4 & 2 weeks \\
2AA -- acceptance & \(256\) & 4 & 4 days \\
4AA -- likelihood & \(256\) & 4 & 3 weeks \\ \hline \hline \end{tabular}
\end{table}
Table 6: Timewarp training parameters

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset & \(\phi_{\text{in}}\) MLP & \(\phi_{\text{out}}\) MLP & Transformer MLP \\ \hline AD & \([70,256,128]\) & \([128,256,3]\) & \([128,256,128]\) \\
2AA & \([70,256,128]\) & \([128,256,3]\) & \([128,256,128]\) \\
4AA & \([134,2048,128]\) & \([128,2048,3]\) & \([128,2048,128]\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Timewarp MLP layer sizes

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset & \(\mathcal{L}_{\text{lik}}(\theta)\) & \(\mathcal{L}_{\text{acc}}(\theta)\) & \(\mathcal{L}_{\text{ent}}(\theta)\) \\ \hline AD & \(0.99\) & \(0.01\) & \(0.1\) \\
2AA & \(0.9\) & \(0.1\) & \(0.1\) \\
4AA & \(1\) & \(0\) & \(0\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Timewarp loss weighting factors