# Sharpness-diversity tradeoff:

improving flat ensembles with SharpBalance

 Haiquan Lu\({}^{1}\)1, Xiaotian Liu\({}^{2}\)2,Yefan Zhou\({}^{2}\)2,Qunli Li\({}^{3}\)3,

**Kurt Keutzer\({}^{4}\)**, **Michael W. Mahoney\({}^{4,5,6}\)**, **Yujun Yan\({}^{2}\)**, **Huanrui Yang\({}^{4}\)**, **Yaoqing Yang\({}^{2}\)**

\({}^{1}\) Nankai University

\({}^{2}\) Dartmouth College

\({}^{3}\) University of California San Diego

\({}^{4}\) University of California at Berkeley

\({}^{5}\) International Computer Science Institute

\({}^{6}\) Lawrence Berkeley National Laboratory

First four authors contributed equally.

###### Abstract

Recent studies on deep ensembles have identified the sharpness of the local minima of individual learners and the diversity of the ensemble members as key factors in improving test-time performance. Building on this, our study investigates the interplay between sharpness and diversity within deep ensembles, illustrating their crucial role in robust generalization to both in-distribution (ID) and out-of-distribution (OOD) data. We discover a trade-off between sharpness and diversity: minimizing the sharpness in the loss landscape tends to diminish the diversity of individual members within the ensemble, adversely affecting the ensemble's improvement. The trade-off is justified through our theoretical analysis and verified empirically through extensive experiments. To address the issue of reduced diversity, we introduce SharpBalance, a novel training approach that balances sharpness and diversity within ensembles. Theoretically, we show that our training strategy achieves a better sharpness-diversity trade-off. Empirically, we conducted comprehensive evaluations in various data sets (CIFAR-10, CIFAR-100, TinyImageNet) and showed that SharpBalance not only effectively improves the sharpness-diversity trade-off, but also significantly improves ensemble performance in ID and OOD scenarios. Our code has been made open-source.2

Footnote 2: https://github.com/haiquanlu/SharpBalance

## 1 Introduction

There has been interest in understanding the properties of neural networks (NNs) and their implications for robust generalization to both in-distribution (ID) and out-of-distribution (OOD) data [1]. Two properties of particular importance, sharpness (or flatness) [14, 15, 16, 17, 18] and diversity [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], have been shown to have a significant influence on performance. In the context of _deep ensembles_[11, 12, 13, 14, 15, 16, 17, 18, 19, 20], diversity (which measures the variance in output between independently-trained models) is shown to be critical in enhancing ensemble accuracy. Sharpness, on the other hand, quantifies the curvature of local minima and is believed to be empirically correlated with an individual model's generalization ability.

Recent research on loss landscapes (Yang et al., 2021) highlights that a single structural property of the loss landscape is insufficient to fully capture a model's generalizability, and it underscores the importance of a joint analysis of sharpness and diversity. Despite significant efforts in studying sharpness and diversity individually, a gap persists in understanding their relationship, particularly in the context of ensemble learning. Our work seeks to bridge this gap by investigating ensemble learning through the lens of loss landscapes, with a specific focus on the interplay between sharpness and diversity.

**Sharpness-diversity trade-off.** Our examination of loss landscape structure for ensembling revealed a "trade-off" between the diversity of individual NNs and the sharpness of the local minima to which they converge. This trade-off introduces a potential limitation to the achievable performance of the deep ensemble: the test accuracy of individual NN can be improved as the sharpness is reduced, but it simultaneously reduces diversity, thereby compromising the ensembling improvement (evidence in Section 4.2 and 4.4). This trade-off is visually summarized in the lower transition branch in Figure 0(a). We also developed theories (in Section 3) to verify the trade-off. The theoretical results characterizing this phenomenon are visualized in Figure 0(b), and the experimental observation is presented in Figure 0(c). In Section 4.2, we also verified the existence of the trade-off by varying the experimental setting to include different datasets and different levels of overparameterization (e.g., changing model width).

SharpBalance mitigates the trade-off and improves ensembling performance.To address the challenge presented by the sharpness-diversity tradeoff, we propose a novel ensemble training method called SharpBalance. This method aims to simultaneously reduce the sharpness of individual NNs and prevent diversity reduction among them, as demonstrated in the upper transition branch of Figure 0(a). This method is designed based on our theoretical results, which suggest that training different ensemble members using a loss function that aims to reduce sharpness on different subsets of the training data can improve the trade-off between sharpness and diversity. Our theoretical results are summarized in Figure 0(b). Aligned with theoretical insights, our SharpBalance method lets each ensemble member minimize the sharpness objective exclusively on a subset of training data, termed the _sharpness-aware set_. The sharpness-aware set of each ensemble member is diversified by an adaptive strategy based on data-dependent sharpness measures. As shown in Figure 0(c), we verify

Figure 1: **(Sharpness-diversity trade-off and SharpBalance).****(a)** Caricature illustrating the sharpness-diversity trade-off that emerges in an ensemble’s loss landscape induced by the Sharpness-aware Minimization (SAM) optimizer. We propose SharpBalance to address this trade-off. Each black circle represents an individual NN in a three-member ensemble. The distance between circles represents the diversity between NNs and the ruggedness of the basin represents the sharpness of each NN. **(b)** Theoretically proving the existence of the sharpness-diversity trade-off and improvement from SharpBalance, plotting the analytic representation of sharpness and diversity from Theorem 1 and Theorem 2 by changing the perturbation radius \(\rho\) of SAM. SharpBalance achieves a larger diversity for the same level of sharpness. **(c)** Empirical results of verifying sharpness-diversity trade-off improvement from SharpBalance. Each marker represents a three-member ResNet18 ensemble trained on CIFAR-10. Diversity is measured by the variance of individual models’ predictions, and sharpness is measured by the adaptive worst-case sharpness, both defined in Section 2.

that SharpBalance improves the sharpness-diversity tradeoff in training the ResNet18 ensemble on CIFAR10. We conducted experiments on three classification datasets to show that SharpBalance boosts ensembling performance in ID and OOD data.

Our contributions are summarized as follows:

* **Comprehensive identification of the sharpness-diversity trade-off:** This work provides a thorough examination of the phenomenon sharpness-diversity trade-off where reducing the sharpness of individual models can decrease diversity between models within an ensemble. We demonstrate this effect through extensive experiments across various settings,using different sharpness and diversity measures, as well as different model capacities. Our findings show that this trade-off can negatively affect the ensemble improvements.
* **Novel theory:** We prove the existence of the trade-off under a novel theoretical framework based on rigorous analysis of sharpness-aware training objectives (Foret et al., 2021; Behdin and Mazumder, 2023). Our analysis borrows tools from analyzing Wishart moments (Bishop et al., 2018), and characterizes the exact dynamics of training, bias-variance tradeoff, and the upper and lower bounds of sharpness. Notably, our novel theoretical analysis generalizes existing analysis to ensemble members trained with different data, which is the key to analyzing our own training method SharpBalance.
* **Effective approach:** To mitigate the sharpness-diversity trade-off, we introduce SharpBalance, an ensemble training approach. Our theoretical framework demonstrates that SharpBalance provably achieves improvements on the sharpness-diversity trade-off by reducing sharpness while mitigating the decrease in diversity. Empirically, we confirm this improvement and demonstrate that SharpBalance enhances overall ensemble performance, outperforming baseline methods in CIFAR-10, CIFAR-100 (Krizhevsky, 2009), TinyImageNet (Le and Yang, 2015), and their corrupted versions to assess OOD performance.

We provide a more detailed discussion on related work in Appendix B.

## 2 Background

**Preliminaries.** We use a NN denoted as \(f_{\bm{\theta}}:\mathbb{R}^{d_{n}}\rightarrow\mathbb{R}^{d_{out}}\), where \(\bm{\theta}\in\mathbb{R}^{p}\) denotes the trainable parameters. The training dataset comprises \(n\) data-label pairs \(\mathcal{D}=\{(\bm{x}_{1},\bm{y}_{1})\,,\ldots,(\bm{x}_{n},\bm{y}_{n})\}\). The training loss of NN \(f_{\bm{\theta}}\) over a dataset \(\mathcal{D}\) can be defined as \(\mathcal{L}_{\mathcal{D}}(\bm{\theta})=\frac{1}{n}\sum_{i=1}^{n}\ell\left(f_{ \bm{\theta}}\left(\bm{x}_{i}\right),\bm{y}_{i}\right)\). Here \(\ell(\cdot)\) is a loss function, which, for instance, can be the cross entropy loss of \(\ell_{2}\) loss. We construct a deep ensemble consisting of \(m\) distinct NNs \(f_{\bm{\theta}_{1}}\),..., \(f_{\bm{\theta}_{m}}\). For classification tasks, the ensemble's output is derived by averaging the predicted logits of these individual networks. We use _flat ensemble_ to mean the deep ensemble in which each ensemble member is trained using a sharpness-aware optimization method (Foret et al., 2021), differentiating it from other ensemble approaches.

**Diversity metrics.** Distinct measures of diversity have been proposed in the literature (Laviolette et al., 2017; Fort et al., 2019; Dietterich, 2000; Baek et al., 2022; Ortega et al., 2022; Theisen et al., 2023), and they are primarily calculated using the predictions made by individual models. Ortega et al. (2022) define diversity \(\mathbb{D}(\bm{\theta})\) to be the variance of model outputs averaged over the data-generating distribution, which we adopt in the theoretical analysis:

\[\mathbb{D}(\bm{\theta})=\mathbb{E}_{\mathcal{D}}[\text{Var}(f_{\bm{\theta}}( \mathcal{D}))].\] (1)

In our experiments, diversity is measured using variance defined above, as well as two other widely used metrics in ensemble learning, namely Disagreement Error Ratio (DER) (Theisen et al., 2023) defined in equation (2), and KL divergence (Kullback and Leibler, 1951) defined in equation (11) in the appendices. We show in Section 4.2 that our main claim generalizes to these three metrics in characterizing the diversity between members within an ensemble. Specifically, denote \(\mathcal{P}\) as the distribution of model weights \(\bm{\theta}\) after training. Then, the DER is defined as

\[\text{DER}=\frac{E_{\bm{\theta},\bm{\theta}^{{}^{\prime}}\sim\mathcal{P}}[ \text{Dis}(f_{\bm{\theta}},f_{\bm{\theta}^{{}^{\prime}}})]}{E_{\bm{\theta} \sim\mathcal{P}}[\mathcal{E}(f_{\bm{\theta}})]},\] (2)

where \(\text{Dis}(f_{\bm{\theta}},f_{\bm{\theta}^{{}^{\prime}}})\) is the prediction disagreement (Masegosa, 2020; Mukhoti et al., 2021; Jiang et al., 2022) between two classifier \(f_{\bm{\theta}},f_{\bm{\theta}^{{}^{\prime}}}\), and \(\mathcal{E}(f_{\bm{\theta}})\) is the prediction error.

**Sharpness Metric.** In accordance with the definition proposed by Foret et al. (2021), we characterize the _first-order sharpness_ of a model as the worst-case perturbation within a radius of \(\rho_{0}\). Mathematically, the sharpness \(\kappa\) of a model \(\bm{\theta}\) is expressed as follows:

\[\kappa(\bm{\theta};\rho_{0})=\max_{\|\bm{\varepsilon}\|_{2}\leq\rho_{0}}\mathcal{ L}_{\mathcal{D}}(\bm{\theta}+\bm{\varepsilon})-\mathcal{L}_{\mathcal{D}}(\bm{ \theta}).\]

Empirically, we measure the sharpness of the NN via the adaptive worst-case sharpness (Kwon et al., 2021; Andriushchenko et al., 2023). The adaptive worst-case sharpness captures how much the loss can increase within the perturbation radius \(\rho_{0}\) of \(\bm{\theta}\):

\[\max_{\|T_{\bm{\theta}}^{-1}\|_{2}\leq\rho_{0}}\mathcal{L}_{\mathcal{D}}(\bm {\theta}+\bm{\varepsilon})-\mathcal{L}_{\mathcal{D}}(\bm{\theta}),\] (3)

where \(\bm{\theta}=[\theta_{1},\dots,\theta_{l}]\), and \(T_{\bm{\theta}}=\operatorname{diag}\left(\left|\theta_{1}\right|,\dots, \left|\theta_{l}\right|\right)\). \(T_{\bm{\theta}}^{-1}\) is a normalization operator to make sharpness "scale-free", that is, such that scaling operations on \(\bm{\theta}\) that do not alter NN predictions will not impact the sharpness measure.

**Ensembling.** We characterize the effectiveness of ensembling by the metric called ensemble improvement rate (EIR) (Theisen et al., 2023), which is defined as the ensembling improvement over the average performance of single models. Let \(\mathcal{E}_{\text{ens}}\) denote the test error of an ensemble; the EIR is then defined as follows:

\[\text{EIR}=\frac{E_{\bm{\theta}\sim\mathcal{P}}[\mathcal{E}(f_{\bm{\theta}})] -\mathcal{E}_{\text{ens}}}{E_{\bm{\theta}\sim\mathcal{P}}[\mathcal{E}(f_{\bm {\theta}})]}.\] (4)

**Sharpness Aware Minimization (SAM).**SAM(Foret et al., 2021) has been shown to be an effective method for improving the generalization of NNs by reducing the sharpness of local minima. It essentially functions by penalizing the maximum loss within a specified radius \(\rho\) of the current parameter \(\theta\). The training objective of SAM is to minimize the following loss function:

\[\mathcal{L}_{\mathcal{D}}^{\text{SAM}}(\bm{\theta}):=\max_{\|\bm{\varepsilon} \|_{2}\leq\rho}\mathcal{L}_{\mathcal{D}}(\bm{\theta}+\bm{\varepsilon})+ \lambda\|\bm{\theta}\|_{2}^{2},\] (5)

where \(\lambda\) is the hyperparameter of a standard \(\ell_{2}\) regularization term.

## 3 Theoretical Analysis of Sharpness-diversity Trade-off

This section theoretically analyzes the sharpness-diversity trade-off. The diversity among individual models is quantified using equation (1). The first theorem establishes the existence of a trade-off between sharpness and diversity. The second theorem demonstrates that training models with only a subset of data samples leads to a more favorable trade-off between these two metrics.

**Sharpness and Diversity of SAM.** Assume the training data matrix \(\mathbf{A}\in\mathbb{R}^{n_{\text{tr}}\times d_{\text{tr}}}\) and test data matrix \(\mathbf{T}\in\mathbb{R}^{n_{\text{tr}}\times d_{\text{tr}}}\) are random with entries drawn from Gaussian \(\mathcal{N}(0,\frac{1}{d_{\text{tr}}}\mathbf{I})\). Suppose the model weight at the 0-th time step, \(\bm{\theta}_{0}\), is initialized randomly such that \(\mathbb{E}[\bm{\theta}_{0}]=\bm{0}\) and \(\mathbb{E}[\bm{\theta}_{0}\bm{\theta}_{0}^{T}]=\sigma^{2}\mathbf{I}\) and updated with a quadratic optimization objective through SAM. The learned weight matrix after \(k\) time steps is denoted as \(\bm{\theta}_{k}\). Let \(\bm{\theta}^{*}\) be the teacher model (i.e., ground-truth model) such that \(\mathbf{A}\bm{\theta}^{*}=\mathbf{y}^{(\mathbf{A})}\) and \(\mathbf{T}\bm{\theta}^{*}=\mathbf{y}^{(\mathbf{T})}\), where \(\mathbf{y}^{(\mathbf{D})}\) is the label vector for data matrix \(\mathbf{D}\). Given a perturbation radius \(\rho_{0}\), the sharpness of a model after \(k\) iteration under the random matrix assumption is defined as

\[\kappa(\bm{\theta}_{k};\rho_{0})=\mathbb{E}_{\mathbf{A}}[\max_{\|\bm{ \varepsilon}\|_{2}\leq\rho_{0}}f\left(\mathbb{E}_{\bm{\theta}_{0}}\left[\bm{ \theta}_{k}\right]+\bm{\varepsilon};\mathbf{A}\right)-f\left(\mathbb{E}_{\bm {\theta}_{0}}\left[\bm{\theta}_{k}\right];\mathbf{A}\right)],\]

which is the expected fluctuation of the model output after perturbation over the data distribution. For simplicity, we denote \(\kappa(\bm{\theta}_{k}^{SAM};\rho_{0})=\kappa_{k}^{SAM}\) for the rest of the paper. We derive an explicit formulation of diversity and upper and lower bounds of sharpness for models optimized with SAM in Theorem 1. Detailed proof can be found in Appendix C.1.

**Theorem 1** (Diversity and Sharpness of SAM).: _Let \(\bm{\theta}_{0}\) be initialized randomly such that \(\mathbb{E}[\bm{\theta}_{0}]=\bm{0}\) and \(\mathbb{E}[\bm{\theta}_{0}\bm{\theta}_{0}^{T}]=\sigma^{2}\mathbf{I}\) Suppose \(\bm{\theta}_{k}^{SAM}\) is the model weight after \(k\) iterations of training with SAM on \(\mathbf{A}\in\mathbb{R}^{n_{\text{tr}}\times d_{\text{tr}}}\) and evaluated on \(\mathbf{T}\in\mathbb{R}^{n_{\text{tr}}\times d_{\text{tr}}}\). Let \(\eta\) be the step size, \(\rho\) be the perturbation radius in SAM and \(\rho_{0}\) be the radius for measuring sharpness \(\kappa_{k}^{SAM}\). Then_

\[\mathbb{D}(\bm{\theta}_{k}^{SAM}) =\phi(2k,0)\sigma^{2},\] \[\frac{\rho_{0}^{2}}{2}\left(\sqrt{\frac{n_{\text{tr}}}{d_{\text{ tr}}}}-1\right)^{2}+\rho_{0}\sqrt{\phi(2k,2)}\|\bm{\theta}^{*}\|_{2}-G\leq \kappa_{k}^{SAM}\leq\frac{\rho_{0}^{2}}{2}\left(\sqrt{\frac{n_{ \text{tr}}}{d_{\text{tr}}}}+1\right)^{2}+\rho_{0}\sqrt{\phi(2k,2)}\|\bm{\theta}^ {*}\|_{2},\]_where_

\[\phi(i,j):= \mathds{1}_{j=0}+\sum_{k_{1}+k_{2}+k_{3}=i}\frac{i!}{k_{1}!k_{2}!k_{3 }!}(-\eta)^{k_{2}+k_{3}}\rho^{k_{3}}\left(\frac{n_{\text{w}}}{d_{\text{in}}} \right)^{m}\sum_{l=1}^{m}\left(\frac{d_{\text{in}}}{n_{\text{w}}}\right)^{m-l} \mathcal{O}(1+1/d_{\text{in}})N_{m,l},\] \[G=\frac{\phi(4k,4)-\phi(2k,2)^{2}}{2\phi(2k,2)^{3/2}\|\bm{\theta}^ {*}\|_{2}}\text{, and }m=k_{2}+2k_{3}+j\text{. }N_{m,l}=\frac{1}{l}\binom{m-1}{l-1}\binom{m}{l-1}\]

To provide a clearer understanding of the relationship between sharpness and diversity, Figure 2 presents a trade-off curve between these two metrics. The estimated sharpness and diversity are displayed on the \(x\) and \(y\) axes, respectively. Each point in the plot corresponds to a model trained using SAM with a different \(\rho\) value, showcasing the outcome of varying perturbation radius. In these experiments, we evaluated the sharpness and diversity of the models empirically and compared them to the estimates obtained using Theorem 1. The soundness of Theorem 1 and tightness of the derived bounds are further supported by empirical evidence, as depicted in Figure 2. Further verification results supporting our theoretical analysis are provided in Appendix C.3

**Training with Data Subsets.** Assume \(\mathbf{A}\) is partitioned into \(S\) horizontal submatrices, such that \(\mathbf{A}=[\mathbf{A}_{1}^{T},\mathbf{A}_{2}^{T},\ldots,\mathbf{A}_{S}^{T}]^ {T}\). We show in Theorem 2 a similar analysis of the sharpness and diversity of ensembles for which each model is trained with a submatrix. Under this setting, we first selected a subset of data \(\mathbf{A}_{s}\) uniformly at random and then train the model with the selected subset with SAM.

**Theorem 2** (Diversity and Sharpness when Models are Trained on Subsets).: _Suppose the training data matrix \(\mathbf{A}\) is partitioned into \(S\) horizontal submatrices. Let \(\bm{\theta}_{0}\) be initialized randomly such that \(\mathbb{E}[\bm{\theta}_{0}]=\mathbf{0}\) and \(\mathbb{E}[\bm{\theta}_{0}\bm{\theta}_{0}^{T}]=\sigma^{2}\mathbf{I}\). Let \(\bm{\theta}_{k}^{SharpBal}\) be the model weight trained with SAM for \(k\) iterations on the submatrix \(\mathbf{A}_{s}\in\mathbb{R}^{\frac{n_{\text{w}}}{S}\times d_{\text{in}}}\), selected uniformly at random, and evaluated on test data \(\mathbf{T}\in\mathbb{R}^{n_{\text{w}}\times d_{\text{in}}}\). Let \(\eta\) be the step size, \(\rho\) be the perturbation radius in SAM, \(\rho_{0}\) be the radius for measuring sharpness \(\kappa_{k}^{SAM}\), and \(r=\frac{n_{\text{w}}}{Sd_{\text{in}}}\). Then_

\[\mathbb{D}(\bm{\theta}_{k}^{SharpBal})= \phi^{\prime}(2k,0)\sigma^{2}\] \[+\frac{S-1}{d_{\text{in}}S}\left(\phi^{\prime}(2k,0)-\phi^{\prime }(k,0)^{2}\right)\|\bm{\theta}^{*}\|_{2}^{2},\]

_and_

\[\kappa_{k}^{SharpBal}(\rho_{0})\leq\frac{\rho_{0}^{2}}{2}\left(\sqrt{\frac{n_{ \text{w}}}{d_{\text{in}}}}+1\right)^{2}+\frac{\rho_{0}}{S}\sqrt{C}\|\bm{\theta} ^{*}\|_{2},\]

_where_

\[C= S\phi^{\prime}(2k,2)+2rS(S-1)\phi^{\prime}(2k,1)+2S(S-1)\phi^{ \prime}(k,2)\phi^{\prime}(k,0)\] \[+r(1+r)S(S-1)\phi^{\prime}(2k,0)+2S(S-1)\phi^{\prime}(k,1)\phi^{ \prime}(k,1)\] \[+\frac{3}{2}r(1+r)S(S-1)(S-2)\phi^{\prime}(k,0)^{2}+\frac{3}{2}r^ {2}S(S-1)(S-2)\phi^{\prime}(2k,0)\] \[+3rS(S-1)(S-2)\phi^{\prime}(k,0)\phi^{\prime}(k,1)+r^{2}S(S-1)(S- 2)(S-3)\phi^{\prime}(k,0)^{2},\] \[\phi^{\prime}(i,j):= \mathds{1}_{j=0}+\sum_{k_{1}+k_{2}+k_{3}=i}\frac{i!}{k_{1}!k_{2}!k _{3}!}(-\eta)^{k_{2}+k_{3}}\rho^{k_{3}}\left(\frac{n_{\text{w}}}{Sd_{\text{in} }}\right)^{m}\sum_{l=1}^{m}\left(\frac{Sd_{\text{in}}}{n_{\text{w}}}\right)^{m-l }\mathcal{O}(1+\frac{1}{d_{\text{in}}})N_{m,l},\]

_where \(m=k_{2}+2k_{3}+j\). \(N_{m,l}=\frac{1}{l}\binom{m-1}{l-1}\binom{m}{l-1}\) is the Narayana number._

The proof of Theorem 2 is provided in Appendix C.2. Similar experimental validations are conducted to verify Theorem 2, with results also presented in Appendix C. The main insight from Theorem 2 is

Figure 2: **(Theoretical vs. Simulated sharpness-diversity trade-off).** This figure illustrates the relationship between sharpness (upper and lower bounds) and diversity as predicted by Thereom 1 and as observed in simulations. Note that the upper and lower bounds correspond to the sharpness values plotted along the x-axis, with the upper bound positioned to the right and the lower bound to the left. Also, note that the bounds provided are for the expected sharpness, which means that random fluctuations can cause the simulation results to move beyond these bounds.

that training models on a randomly selected data subset offers a better trade-off between sharpness and diversity compared to training on the complete dataset. This idea is further illustrated in Figure 0(b), where we compare the sharpness upper bound and diversity of models trained on the full dataset (labeled as SAM) and those trained on subsets (labeled as SharpBalance). The results demonstrate that SharpBalance achieves a more favorable trade-off. For a given level of sharpness, deep ensembles with models trained on subsets of the data exhibit higher diversity compared to those trained on the entire dataset. This indicates that minimizing sharpness on randomly sampled data subsets for each model within the ensemble promotes the diversity among the models, thereby enhancing the sharpness-diversity trade-off.

## 4 Experiments

In this section, we describe our experiments. In particular, following Section 4.1 where we describe our experimental setup, in Section 4.2, we provide an empirical evaluation across various datasets to explore the trade-off between sharpness and diversity. We also examine how this trade-off changes with different levels of overparameterization. Then, in Section 4.3 and 4.4, we elaborate the SharpBalance algorithm and compare its performance with baseline methods.

### Experimental setup

Here, we describe the experiment setup for Section 4.2. Each ensemble member is trained individually using SAM with a consistent perturbation radius \(\rho\), as defined in equation (5). We adjust \(\rho\) across different ensembles to achieve varying levels of minimized sharpness. Sharpness for each NN was measured using the adaptive worst-case sharpness metric, defined in equation (3). The sharpness measurement was done on the training set, using 100 batches of size 5. The diversity between NNs is measured using DER defined in equation (2). The diversity between ensemble members is tested on OOD data. We evaluated this trade-off using a variety of image classification datasets, including CIFAR-10, CIFAR-100 (Krizhevsky, 2009), TinyImageNet (Le and Yang, 2015), and their corrupted versions (Hendrycks and Dietterich, 2019). For the setup of Section 4.4, we used the same datasets and architecture. The hyperparameters of the baseline methods has been carefully tuned. The hyperparameters for conducting the experiments are detailed in Appendix D.

### Empirical validation of Sharpness-diversity trade-off

We provide empirical observation to validate and explore the sharpness-diversity trade-off. Figure 3 presents the validation of observing the trade-off phenomenon on training ResNet18 ensembles on CIFAR10 applying three different metrics to measure the diversity. The results demonstrate that this trade-off phenomenon generalizes to the three diversity metrics defined in Section 2. Figure 4 presents the validation on three different datasets. In the following empirical study, DER will be the primary metric for measuring diversity of models.

Figure 3: (**Varying diversity measure in empirical study).** Three different metrics are employed to measure the diversity of individual models within an ensemble, i.e., Variance in equation (1), DER in equation (2), and KL divergence in equation (11). The results of the three metrics show consistent trends, demonstrating the sharpness-diversity trade-off: lower sharpness is correlated with lower diversity. The experiment is conducted by training a three-member ResNet18 ensemble on CIFAR10.

Experimental results obtained with the other two metrics are available in Appendix E. The three sets of results first verify that minimizing individual member's sharpness indeed reduces diversity. This is confirmed by the consistent trends of markers moving from upper right to lower left. Second, the first row of Figure 4 shows that an ensemble with decreased diversity (lower in \(y\)-axis) shows a lower ensemble improvement rate (from red to blue), highlighting the negative impact of this trade-off. Lastly, the second row shows when the sharpness of the individual model is reduced (lower in \(x\)-axis), the individual model's OOD accuracy is improved (from blue to red), demonstrating the benefits of minimizing sharpness. We verify the robustness of the phenomenon by measuring the sharpness and diversity using different metrics in Appendix E.

Figure 5 illustrates the trade-off curves as the overparameterization level of the model is adjusted by changing width or sparsity (introduced using model pruning). This visualization confirms that the trade-off is a consistent phenomenon across models of different sizes, and the ensemble provides less improvement (blue color) at the lower left end of each trade-off curve. It also highlights that models with smaller or sparser configurations show a more significant trade-off effect, as evidenced by the steeper slopes and higher coefficient values of the linear fitting curves. As sparse ensembles are now being used to demonstrate the benefits of ensembling for efficient models (Liu et al., 2022; Diffenderfer et al., 2021; Whitaker and Whitley, 2022; Kobayashi et al., 2022; Zimmer et al., 2024), addressing the conflict between sharpness and diversity becomes particularly crucial.

### Our SharpBalance method

Here, we describe the design and implementation of our main method, SharpBalance. Figure 6 provides an overview. Our approach is motivated by the theoretical analysis in Section 3, which suggests that having each ensemble member minimize sharpness on diverse subsets of the data can lead to a better trade-off between sharpness and diversity. SharpBalance aims to achieve the optimal balance by applying SAM to a carefully selected subset of the data, while performing standard optimization on the remaining samples. More specifically, for each ensemble member NN \(f_{\theta_{i}}\), our method divides the entire training dataset \(\mathcal{D}\) into two distinct subsets: sharpness-aware set \(\mathcal{D}^{i}_{\text{SAM}}\) and normal set \(\mathcal{D}^{i}_{\text{Normal}}\). The model is trained to optimize the sharpness reduction objective on \(\mathcal{D}^{i}_{\text{SAM}}\)

Figure 4: **(Empirical observations of sharpness-diversity trade-off). The identified trade-off shows that while reducing sharpness enhances individual model performance, it concurrently lowers diversity and thus diminishes the ensemble improvement rate. _First row_: the color encoding represents the ensemble improvement rate (EIR) defined in equation (4), from red to blue means ensembling improvement decreases. _Second row_: the color encoding represents the individual ensemble member’s OOD accuracy, from blue to red means individual performance becomes better. Each marker represents a three-member ResNet18 ensemble trained with SAM with a different perturbation radius.**

while it optimizes the normal training objective on \(\mathcal{D}^{i}_{\text{Normal}}\). These training objectives are denoted as \(\mathcal{L}^{\text{SAM}}_{\mathcal{D}^{i}_{\text{SMM}}}(\bm{\theta}_{i})\) and \(\mathcal{L}_{\mathcal{D}^{i}_{\text{Normal}}}(\bm{\theta}_{i})\), respectively. The \(\mathcal{D}^{i}_{\text{SAM}}\) is selected by an adaptive strategy from the whole dataset \(\mathcal{D}\): it is composed of the union of samples that are deemed "sharp" by all other members of the ensemble except the \(i\)-th. Specifically, for each model, we pick the subset of data samples with the top-\(k\)% highest "per-data-sample sharpness." Then, we take the union of all such subsets expect the \(i\)-th for creating the subset \(\mathcal{D}^{i}_{\text{SAM}}\). This partition of data samples can be efficiently computed in parallel as there is no sequential dependency on the training of the ensemble members. However, SharpBalance can be easily adapted for sequential training if memory constraints permit training only one model at a time.

**Per-data-sample sharpness.** This metric is designed to efficiently assess the sharpness of a model for individual data samples. For each data point \((\bm{x}_{j},\bm{y}_{j})\), sharpness is quantified using the Fisher Information Matrix (FIM), which is expressed as \(\nabla_{\bm{\theta}}\ell(f_{\bm{\theta}}(\bm{x}_{j}),\bm{y}_{j})\nabla_{\bm{ \theta}}\ell(f_{\bm{\theta}}(\bm{x}_{j}),\bm{y}_{j})^{T}\). Following a well-established approach (Bottou et al., 2018), we approximate the trace of the FIM by computing the squared \(\ell_{2}\) norm of the gradient: \(\|\nabla_{\bm{\theta}}\ell(f_{\bm{\theta}}(\bm{x}_{j}),\bm{y}_{j})\|_{2}^{2}\). Other common sharpness metrics, such as worst-case sharpness, trace of the Hessian, or Hessian eigenvalues, are computationally slightly more expensive to approximate (Yao et al., 2020, 2021), but are expected to lead to similar results.

Figure 5: **(Sharpness-diversity trade-off in models varying overparameterization levels).** Different types of markers represent models with varying degrees of overparameterization, determined by changing the model width (a) or sparsity (b). Each marker represents a three-member ensemble trained with SAM with a different perturbation radius. The \(\beta\) reflects the rate of decline in the trade-off curve, calculated via applying linear fitting over the ensembles at each level of overparameterization. A higher \(\beta\) points to a steeper decline in the trade-off. Ensembles with narrower widths or increased sparsity display more pronounced trade-off effects. The model used in ResNet18 and the dataset is CIFAR-10.

Figure 6: **(System diagram of SharpBalance).** Each ensemble member \(f_{\bm{\theta}_{i}}\) optimizes the sharpness reduction objective on subset \(\mathcal{D}^{i}_{\text{SAM}}\) and the normal training objective on \(\mathcal{D}^{i}_{\text{Normal}}\). \(\mathcal{D}^{i}_{\text{SAM}}\) is formed by selecting data samples from \(\mathcal{D}\) that significantly affect the loss landscape sharpness of other ensemble members.

### Empirical evaluation of SharpBalance

We evaluate SharpBalance by benchmarking it against both a standard Deep Ensemble, trained using SGD, and a Deep Ensemble enhanced with SAM. The results are presented in Figure 7 for CIFAR-10, CIFAR-100, and TinyImageNet. The comparison between the middle and left bars shows that SAM enhances individual model performance by reducing sharpness. However, this reduction in sharpness also diminishes the overall ensemble effectiveness by lowering diversity, exemplifying the sharpness-diversity trade-off discussed in Section 4.2. Further comparison between the right and middle bars shows that SharpBalance maintains or improves individual performance while improving ensemble effectiveness.

We also evaluate SharpBalance on different ensemble sizes. As shown in Figure 8, SharpBalance demonstrates more pronounced empirical improvements as the number of ensemble models increases. The accuracy difference between SharpBalance and the baseline methods becomes more significant, especially on corrupted data. Specifically, SharpBalance outperforms the baselines by up to 1.30% when ensembling 5 models on CIFAR100-C dataset.

To further evaluate SharpBalance, we provide corroborating results in Appendix F, which includes:

* We evaluate SharpBalance on different severity of the corruption on CIFAR10-C, CIFAR100-C and Tiny-ImageNet-C. SharpBalance increasingly outperforms the baselines as the severity of the corruption increases. We also evaluate the proposed method using uncertainty metrics such as negative log-likelihood and expected calibration error.
* We further evaluate SharpBalance on other model architectures and tasks, such as WideResNet, ViT, and ALBERT (Lan et al., 2020) on language tasks.
* We compare our method of measuring sharpness with another method of measuring the curvature of the loss around a data point (Garg and Roy, 2023) and show the strong correlation between these two methods.
* We further compare SharpBalance with ensemble baseline EoA (Arpit et al., 2022), an improved version of SAM (for which individual models in an ensemble are trained with different \(\rho\) values) and GSAM (Zhuang et al., 2022). Results show that SharpBalance can significantly outperform the baselines.

Figure 7: **(Main results: SharpBalance improves the overall ensembling performance and mitigates the reduced ensembling improvement caused by sharpness-diversity trade-off). The three-member ResNet18 ensemble is trained with different methods on three datasets. The first row reports the OOD accuracy and the second row reports the ID accuracy. The lower part of each bar with the diagonal lines represents the individual model performance. The upper part of each bar represents the ensembling improvement. The results are reported by averaging three ensembles, and each ensemble is comprised of three models.*** We demonstrate that, compared to training a deep ensemble with SAM, our method adds only minimal computational cost. The extra time complexity is dominated by the computation of Fisher trace for evaluating per-sample sharpness, which empirically increases the training time by 1%.

## 5 Conclusion

Our theoretical and empirical analyses demonstrate the existence of a sharpness-diversity trade-off when sharpness-minimization training methods are applied to deep ensembles. This leads to two main insights that are relevant for improving model performance. First, reducing the sharpness in individual models proves to be beneficial in enhancing the performance of the ensemble as a whole. Second, the accompanying reduction in diversity suggests that popular ensembling methods have limitations, and also highlights the potential for more sophisticated designs that promote diversity among models with lower sharpness. These results are particularly timely, given recent theoretical work on characterizing ensemble improvement (Theisen et al., 2023). In response to these findings, we have proposed SharpBalance, which "diagnoses" the training data by evaluating the sharpness of each sample and then fine-tunes the training of individual models to focus on a diverse subset of the sharpest training data samples. This targeted approach helps maintain diversity among models while also reducing their individual sharpness. Extensive evaluations indicate that SharpBalance not only improves the sharpness-diversity trade-off but also delivers superior OOD performance for both dense and sparse models across various datasets and architectures when compared to other ensembling approaches.

**Limitations.** One limitation of the study is that our theoretical analysis in Section 3 relies on the assumption that the data matrices \(\mathbf{A},\mathbf{T}\) follow a Gaussian distribution and assumed the optimization objective to be quadratic, which may not always hold in practice. Despite the potentially strong assumptions, our empirical findings in Section 4 show that the conclusions remain robust in real-world datasets with various model architectures. This suggests the insights discovered in our study are applicable to a wider range of real-world scenarios, beyond just those strictly adhering to the Gaussian assumption. Nevertheless, future research could explore how such assumptions can be relaxed and extend the theoretical analysis to a weaker condition.

**Acknowledgements.** Michael W. Mahoney would like to acknowledge the UC Berkeley CLTC, ARO, IARPA (contract W911NF20C0035), NSF, and ONR for providing partial support of this work. Kurt Keutzer would like to acknowledge support from Berkeley Deep Drive. Yaoqing Yang would like to acknowledge support from DOE under Award Number DE-SC0025584, DARPA under Agreement

Figure 8: SharpBalance **achieves more pronounced improvement when increasing the number of ensembling models.** “EIR” represents the ensemble improvement rate, which is defined in Section 2, the larger the better. \(x\)-axis represents the number of individual models in one ensemble.

number HR00112490441, and Dartmouth College. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.

## References

* Andriushchenko et al. (2023) Maksym Andriushchenko, Francesco Croce, Maximilian Mueller, Matthias Hein, and Nicolas Flammarion. A modern look at the relationship between sharpness and generalization. In _International Conference on Machine Learning_, 2023.
* Arpit et al. (2022) Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving model selection and boosting performance in domain generalization. _Advances in Neural Information Processing Systems_, 2022.
* Baek et al. (2022) Christina Baek, Yiding Jiang, Aditi Raghunathan, and J Zico Kolter. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. _Advances in Neural Information Processing Systems_, 35:19274-19289, 2022.
* Behdin and Mazumder (2023) Kayhan Behdin and Rahul Mazumder. On statistical properties of sharpness-aware minimization: Provable guarantees. _arXiv preprint arXiv:2302.11836_, 2023.
* Bishop et al. (2018) Adrian N Bishop, Pierre Del Moral, Angele Niclas, et al. An introduction to wishart matrix moments. _Foundations and Trends(r) in Machine Learning_, 11(2):97-218, 2018.
* Bottou et al. (2018) Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. _SIAM review_, 60(2):223-311, 2018.
* Breiman (2001) Leo Breiman. Random forests. _Machine learning_, 45(1):5-32, 2001.
* Bui et al. (2024) Anh Bui, Vy Vo, Tung Pham, Dinh Phung, and Trung Le. Diversity-aware agnostic ensemble of sharpness minimizers. _arXiv preprint arXiv:2403.13204_, 2024.
* Cha et al. (2021) Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. _Advances in Neural Information Processing Systems_, 34:22405-22418, 2021.
* Chen and Guestrin (2016) Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In _Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining_, pages 785-794, 2016.
* Dietterich (2000) Thomas G Dietterich. Ensemble methods in machine learning. In _International workshop on multiple classifier systems_, pages 1-15. Springer, 2000.
* Diffenderfer et al. (2021) James Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura. A winning hand: Compressing deep networks can improve out-of-distribution robustness. _Advances in neural information processing systems_, 34:664-676, 2021.
* Dinh et al. (2017) Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In _International Conference on Machine Learning_, pages 1019-1028. PMLR, 2017.
* Du et al. (2024) Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent Tan. Efficient sharpness-aware minimization for improved training of neural networks. In _International Conference on Learning Representations_, 2024.
* Foret et al. (2021) Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations_, 2021.
* Fort et al. (2019) Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective. _arXiv preprint arXiv:1912.02757_, 2019.
* Freund (1995) Yoav Freund. Boosting a weak learning algorithm by majority. _Information and computation_, 121(2):256-285, 1995.
* Freund and Schapire (1997) Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of computer and system sciences_, 55(1):119-139, 1997.
* Ganaie et al. (2022) Mudasir A Ganaie, Minghui Hu, AK Malik, M Tanveer, and PN Suganthan. Ensemble deep learning: A review. _Engineering Applications of Artificial Intelligence_, 115:105151, 2022.
* Ghahahramani et al. (2018)Xiang Gao, Meera Sitharam, and Adrian E. Roitberg. Bounds on the jensen gap, and implications for mean-concentrated distributions. _The Australian Journal of Mathematical Analysis and Applications_, 2019.
* Garg and Roy (2023) Isha Garg and Kaushik Roy. Samples with low loss curvature improve data efficiency. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20290-20300, 2023.
* Granziol (2020) Diego Granziol. Flatness is a false friend. _arXiv preprint arXiv:2006.09091_, 2020.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hendrycks and Dietterich (2019a) Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations_, 2019a.
* Hendrycks and Dietterich (2019b) Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations_, 2019b.
* Hinton and van Camp (1993) Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In _Proceedings of the Sixth Annual Conference on Computational Learning Theory_, 1993.
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. Flat Minima. _Neural Computation_, 9(1):1-42, 1997.
* Izmailov et al. (2018) Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In _34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018_, pages 876-885. Association For Uncertainty in Artificial Intelligence (AUAI), 2018.
* Jiang et al. (2023) Weisen Jiang, Hansi Yang, Yu Zhang, and James Kwok. An adaptive policy to employ sharpness-aware minimization. In _International Conference on Learning Representations_, 2023.
* Jiang et al. (2022) Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of SGD via disagreement. In _International Conference on Learning Representations_, 2022.
* Kaddour et al. (2022) Jean Kaddour, Linqing Liu, Ricardo Silva, and Matt J Kusner. When do flat minima optimizers work? _Advances in Neural Information Processing Systems_, 35:16577-16595, 2022.
* Keskar et al. (2016) Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2016.
* Workshop on Challenges & Perspectives in Creating Large Language Models_, 2022.
* Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
* 86, 1951. doi: 10.1214/aoms/1177729694.
* Kwon et al. (2021) Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In _International Conference on Machine Learning_, pages 5905-5914. PMLR, 2021.
* Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.
* Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In _International Conference on Learning Representations_, 2020.
* Lan et al. (2018)Francois Laviolette, Emilie Morvant, Liva Ralaivola, and Jean-Francis Roy. Risk upper bounds for general ensemble methods with an application to multiclass classification. _Neurocomputing_, 219:15-25, 2017.
* Le and Yang (2015) Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. 2015.
* Lee et al. (2022) Yoonho Lee, Huaxiu Yao, and Chelsea Finn. Diversify and disambiguate: Learning from under-specified data. In _ICML 2022: Workshop on Spurious Correlations, Invariance and Stability_, 2022.
* Liu et al. (2022) Shiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen, Ghada Sokar, Elena Mocanu, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Deep ensembling with no overhead for either training or testing: The all-round blessings of dynamic sparsity. In _International Conference on Learning Representations_, 2022.
* Masegosa (2020) Andres Masegosa. Learning under model misspecification: Applications to variational and ensemble methods. _Advances in Neural Information Processing Systems_, 33:5479-5491, 2020.
* Mehrtash et al. (2020) Alireza Mehrtash, Purang Abolmaesumi, Polina Golland, Tina Kapur, Demian Wassermann, and William Wells. Pep: Parameter ensembling by perturbation. _Advances in neural information processing systems_, 33:8895-8906, 2020.
* Mukhoti et al. (2021) Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deterministic neural networks with appropriate inductive biases capture epistemic and aleatoric uncertainty. _preprint arXiv:2102.11582_, 2021.
* Neyshabur et al. (2018) Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In _International Conference on Learning Representations_, 2018.
* Ortega et al. (2022) Luis A Ortega, Rafael Cabanas, and Andres Masegosa. Diversity and generalization in neural network ensembles. In _International Conference on Artificial Intelligence and Statistics_, pages 11720-11743. PMLR, 2022.
* Ovadia et al. (2019) Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. _Advances in neural information processing systems_, 32, 2019.
* Pang et al. (2019) Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via promoting ensemble diversity. In _International Conference on Machine Learning_, pages 4970-4979. PMLR, 2019.
* Parker-Holder et al. (2020) Jack Parker-Holder, Luke Metz, Cinjon Resnick, Hengyuan Hu, Adam Lerer, Alistair Letcher, Alexander Peysakhovich, Aldo Pacchiano, and Jakob Foerster. Ridge rider: Finding diverse solutions by following eigenvectors of the hessian. _Advances in Neural Information Processing Systems_, 33:753-765, 2020.
* Rame et al. (2022) Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. _Advances in Neural Information Processing Systems_, 35:10821-10836, 2022.
* Theisen et al. (2023) Ryan Theisen, Hyunsuk Kim, Yaoqing Yang, Liam Hodgkinson, and Michael W Mahoney. When are ensembles really effective? _Advances in neural information processing systems_, 2023.
* Valentini and Dietterich (2003) Giorgio Valentini and Thomas G Dietterich. Low bias bagged support vector machines. In _Proceedings of the 20th International Conference on Machine Learning (ICML-03)_, pages 752-759, 2003.
* Vershynin (2018) Roman Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
* Whitaker and Whitley (2022) Tim Whitaker and Darrell Whitley. Prune and tune ensembles: low-cost ensemble learning with sparse independent subnetworks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 8638-8646, 2022.
* Wu et al. (2019)Yaoqing Yang, Liam Hodgkinson, Ryan Theisen, Joe Zou, Joseph E Gonzalez, Kannan Ramchandran, and Michael W Mahoney. Taxonomizing local versus global structure in neural network loss landscapes. _Advances in Neural Information Processing Systems_, 34:18722-18733, 2021.
* Yao et al. (2018) Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. Hessian-based analysis of large batch training and robustness to adversaries. _Advances in Neural Information Processing Systems_, 31, 2018.
* Yao et al. (2020) Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks through the lens of the hessian. In _2020 IEEE international conference on big data (Big data)_, pages 581-590. IEEE, 2020.
* Yao et al. (2021) Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney. Adahessian: An adaptive second order optimizer for machine learning. In _proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 10665-10673, 2021.
* Zhuang et al. (2022) Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha C Dvornek, James S Duncan, Ting Liu, et al. Surrogate gap minimization improves sharpness-aware training. In _International Conference on Learning Representations_, 2022.
* Zimmer et al. (2024) Max Zimmer, Christoph Spiegel, and Sebastian Pokutta. Sparse model soups: A recipe for improved pruning via model averaging. In _International Conference on Learning Representations_, 2024.

## Appendix A Impact Statement

This paper uncovers a trade-off between sharpness and diversity in deep ensembles and introduces a novel training strategy to achieve an optimal balance between these two crucial metrics. While the proposed method could potentially be misused for malicious purposes, we believe that the study itself does not pose any direct negative societal impact. More importantly, this research advances the field of ensemble learning and contribute to the development of more reliable deep ensemble models. These advancements consequently result in enhanced robustness when dealing with OOD data and enable the quantification of uncertainty, thereby strengthening the reliability and applicability of deep learning systems in real-world scenarios.

## Appendix B Related work

**Ensembling**. Diversity is one of the major factors that contribute to the success of the ensembling method. Popular ensemble techniques have been developed for tree-type individual learners, which are known to have a high variance. This is evident such as in [Breiman, 2001, Chen and Guestrin, 2016, Freund, 1995, Freund and Schapire, 1997]. In contrast, more stable algorithms, such as support vector machines (SVM) type learners, are less commonly used for ensembles, unless they are tuned to a low-bias, high-variance regime, as explored in [Valentini and Dietterich, 2003]. When it comes to diversity and ensembling, NNs are known to exhibit properties different than traditional models, e.g., as described in recent theoretical and empirical work on loss landscapes and emsemble improvement [Theisen et al., 2023, Yang et al., 2021]. Therefore, ensembling techniques that work well for traditional models (e.g., tree-type models) often underperform the simple yet efficient deep ensembles method [Fort et al., 2019, Ortega et al., 2022] that uses the independent initialization and optimization. Previous literature has explored various new methods to learn diverse NNs [Lee et al., 2022, Rame et al., 2022, Pang et al., 2019, Parker-Holder et al., 2020]. Our work is different from previous work in that we study flat ensembles obtained from sharpness-aware training methods, especially focusing on diversifying flat ensembles by reducing the overlap between sharpness-aware data subsets. While our work demonstrates significant improvements in OOD generalization, it is known that (in some cases, see also Theisen et al. [2023]) deep ensembling is a simple, yet effective method to improve OOD performance [Diffenderfer et al., 2021]. Therefore, we compare the OOD performance of SharpBalance to deep ensembles.

**Sharpness and generalization**. A large body of work has studied the relationship between the sharpness (or flatness) of minima and the generalizability of models [Hochreiter and Schmidhuber, 1997, Hinton and van Camp, 1993, Keskar et al., 2016, Neyshabur et al., 2018, Yang et al., 2021, Kaddour et al., 2022, Yao et al., 2021, Yao et al., 2020]. Works such as those by [Hochreiter and Schmidhuber, 1997] and [Hinton and van Camp, 1993] use Bayesian learning and minimum description length to explain why we should train models to flat minima. [Keskar et al., 2016] introduces a sharpness-based metric, demonstrating how large-batch training can skew NNs towards sharp local minima, adversely affecting generalization. In addition, [Neyshabur et al., 2018] uses a PAC-Bayesian framework to prove bounds on generalization, which can be interpreted as the relationship between sharpness and test accuracy. Furthermore, [Cha et al., 2021] presents a theoretical exploration of the link between the sharpness of minima and OOD generalization.

Motivated by the good generalization property of flat minima, variants of sharpness-guided optimization techniques have been proposed [Yao et al., 2018, 2021, Du et al., 2024, Jiang et al., 2023], including sharpness-aware minimization [Foret et al., 2021]. The DiWA method [Rame et al., 2022] observed that SAM can decrease the diversity of models in the context of weight averaging (WA) [Izmailov et al., 2018]. However, WA imposes constraints on different models, requiring them to share the same initialization and stay close to each other in the parameter space. In contrast, our work focuses on deep ensembles that do not pose additional constraints on the training trajectories of individual ensemble members. Previous work by [Behdin and Mazumder, 2023] provided a theoretical characterization of important statistical properties for kernel regression models and single-layer ReLU networks, optimized using SAM on noisy datasets. Our theoretical analysis borrows ideas from [Behdin and Mazumder, 2023] and extends the analysis using random matrix theory. DASH was proposed in [Bui et al., 2024] to minimize the generalization loss by adding KL divergence constrainton the output logits of ensemble members. The authors believe that the decrease in diversity is a result of models being initialized closely and updated with the same direction. In contrast, SharpBalance observed that the sharpness-diversity trade-off is ubiquitous across various settings and provides a rigorous theoretical quantification that characterizes the interplay of the two metrics. Compare to DASH, SharpBalance provably achieves improved performance and is simple, effective, and computationally cheap to implement.

## Appendix C Proof of Theorems in Section 3

Recall that SAM updates the model weights, ignoring the normalization constant and regularization, through the following recursive rule

\[\bm{\theta}_{k+1}^{SAM}=\bm{\theta}_{k}^{SAM}-\eta\nabla f\left(\bm{\theta}_ {k}^{SAM}+\rho\nabla f(\bm{\theta}_{k}^{SAM})\right).\]

We first show an unrolling of the iterative optimization on a quadratic objective.

**Theorem 3** (Unrolling Sam).: _Let \(\bm{\theta}^{*}\) be the teacher model. Let \(\bm{\theta}_{0}\) be randomly initialized and updated with SAM to solve a quadratic objective \(\mathcal{L}_{\mathbf{A}}(\bm{\theta})=\frac{1}{2}(\bm{\theta}-\bm{\theta}^{* })^{T}\mathbf{A}^{T}\mathbf{A}(\bm{\theta}-\bm{\theta}^{*})\). Then,_

\[\bm{\theta}_{k+1}^{SAM}=\eta\sum_{i=0}^{k}\mathbf{B}^{i}\left(\mathbf{A}^{T} \mathbf{A}+\rho(\mathbf{A}^{T}\mathbf{A})^{2}\right)\bm{\theta}^{*}+\mathbf{B }^{k+1}\bm{\theta}_{0},\]

_where \(\mathbf{B}=\mathbf{I}-\eta\mathbf{A}^{T}\mathbf{A}-\eta\rho(\mathbf{A}^{T} \mathbf{A})^{2}\)._

Proof.: The gradient of the objective \(f\) is given by \(\nabla f(\bm{\theta})=\mathbf{A}^{T}\mathbf{A}(\bm{\theta}-\bm{\theta}^{*})\). Therefore,

\[\bm{\theta}_{k}^{SAM}+\rho\nabla f(\bm{\theta}_{k}^{SAM})=(\mathbf{I}+\rho \mathbf{A}^{T}\mathbf{A})\bm{\theta}_{k}^{SAM}-\rho\mathbf{A}^{T}\mathbf{A} \bm{\theta}^{*}.\]

With SAM update,

\[\bm{\theta}_{k+1}^{SAM} =\bm{\theta}_{k}^{SAM}-\eta\nabla f\left(\bm{\theta}_{k}^{SAM}+ \rho\nabla f(\bm{\theta}_{k}^{SAM})\right)\] \[=\bm{\theta}_{k}^{SAM}-\eta\mathbf{A}^{T}\mathbf{A}\left(( \mathbf{I}+\rho\mathbf{A}^{T}\mathbf{A})\bm{\theta}_{k}^{SAM}-\rho\mathbf{A} ^{T}\mathbf{A}\bm{\theta}^{*}-\bm{\theta}^{*}\right)\] \[=\left(\mathbf{I}-\eta\mathbf{A}^{T}\mathbf{A}-\eta\rho(\mathbf{A }^{T}\mathbf{A})^{2}\right)\bm{\theta}_{k}^{SAM}+\eta\left(\mathbf{A}^{T} \mathbf{A}+\rho(\mathbf{A}^{T}\mathbf{A})^{2}\right)\bm{\theta}^{*}\] \[=\eta\sum_{i=0}^{k}\mathbf{B}^{i}\left(\mathbf{A}^{T}\mathbf{A}+ \rho(\mathbf{A}^{T}\mathbf{A})^{2}\right)\bm{\theta}^{*}+\mathbf{B}^{k+1}\bm{ \theta}_{0},\]

where the last equation is obtained by recursively unrolling the weight by previous updates. 

Theorem 3 offers a valuable tool to analyze the statistical behavior of the models optimized by SAM. However, one more ingredient is required to arrive at the interesting conclusions claimed in Section 3, the random matrix theory. Recall that the data matrix \(\mathbf{A}\in\mathbb{R}^{n_{k}\times d_{\text{in}}}\) is random with entries drawn from Gaussian \(\mathcal{N}(0,\mathbf{I}/d_{\text{in}})\). As a result, entries in \(\mathbf{A}^{T}\mathbf{A}\) follows the Wishart distribution and according to Corollary 3.3 in Bishop et al. (2018), for \(k\geq 1\),

\[\mathbb{E}[(\mathbf{A}^{T}\mathbf{A})^{k}]=\left(\frac{n_{\text{tr}}}{d_{\text {in}}}\right)^{k}\sum_{i=1}^{k}\left(\frac{d_{\text{in}}}{n_{\text{tr}}} \right)^{k-i}\mathcal{O}\left(1+1/d_{\text{in}}\right)N_{k,i}\mathbf{I},\] (6)

where \(N_{k,i}=\frac{1}{i}\binom{k-1}{i-1}\binom{k}{i-1}\) is the Narayana number. With the help of this Corollary, we now prove a proposition on the expectation of \(\mathbf{B}^{k}\).

**Proposition 1** (Expectation of Wishart Moments).: _Let \(i,j\) be non-negative integers, then_

\[\mathbb{E}_{\mathbf{A}}[\mathbf{B}^{i}(\mathbf{A}^{T}\mathbf{A})^{j}]=\phi(i, j)\mathbf{I},\]

_where_

\[\phi(i,j):= \mathds{I}_{j=0}+\sum_{k_{1}+k_{2}+k_{3}=i}\frac{i!}{k_{1}!k_{2}!k_ {3}!}(-\eta)^{k_{2}+k_{3}}\rho^{k_{3}}\left(\frac{n_{\text{tr}}}{d_{\text{in}} }\right)^{m}\sum_{l=1}^{m}\left(\frac{d_{\text{in}}}{n_{\text{tr}}}\right)^{m- l}\mathcal{O}(1+1/d_{\text{in}})N_{m,l},\]

_and \(m=k_{2}+2k_{3}+j\)._

[MISSING_PAGE_FAIL:18]

As a result, \(\mathbb{E}_{\bm{\theta}_{0}}[\bm{\theta}_{k}^{SAM}]=\bm{\theta}^{*}-\left(\mathbf{I}- \eta\bm{\Lambda}^{T}\mathbf{A}-\eta\rho(\mathbf{A}^{T}\mathbf{A})^{2}\right)^{k }\bm{\theta}^{*}=\bm{\theta}^{*}-\mathbf{B}^{k}\bm{\theta}^{*}.\) By definition,

\[n_{\text{te}}\text{Bias}^{2}(\bm{\theta}_{k}^{SAM}) =\mathbb{E}_{\mathbf{A},\mathbf{T}}[\sum_{i=1}^{p}(\mathbb{E}_{ \bm{\theta}_{0}}[f(\bm{\theta}_{k}^{SAM};\mathbf{T}_{i})]-y_{i}^{(\mathbf{T}) })^{2}]\] \[=\mathbb{E}_{\mathbf{A},\mathbf{T}}[(\mathbb{E}_{\bm{\theta}_{0} }[\bm{\theta}_{k}^{SAM}]-\bm{\theta}^{*})^{T}\mathbf{T}^{T}\mathbf{T}(\mathbb{ E}_{\bm{\theta}_{0}}[\bm{\theta}_{k}^{SAM}]-\bm{\theta}^{*})]\] \[=\mathbb{E}_{\mathbf{A}}[(\mathbb{E}_{\bm{\theta}_{0}}[\bm{\theta }_{k}^{SAM}]-\bm{\theta}^{*})^{T}\mathbb{E}_{\mathbf{T}}[\mathbf{T}^{T} \mathbf{T}](\mathbb{E}_{\bm{\theta}_{0}}[\bm{\theta}_{k}^{SAM}]-\bm{\theta}^{* })]\] \[=\frac{n_{\text{te}}}{d_{\text{in}}}\mathbb{E}_{\mathbf{A}}[(\bm{ \theta}^{*})^{T}\mathbf{B}^{2k}\bm{\theta}^{*}]\] \[=\frac{n_{\text{te}}}{d_{\text{in}}}\phi(2k,0)\|\bm{\theta}^{*} \|_{2}^{2},\] \[n_{\text{te}}\text{Error}(\bm{\theta}_{k}^{SAM}) =\mathbb{E}_{\mathbf{A},\mathbf{T},\theta_{0}}[\sum_{i=1}^{p}(y_{ i}^{(\mathbf{T})}-f(\bm{\theta}_{k}^{SAM};\mathbf{T}_{i}))^{2}]\] \[=\mathbb{E}_{\mathbf{A},\mathbf{T},\theta_{0}}[(\bm{\theta}^{*}- \bm{\theta}_{k}^{SAM})^{T}\mathbf{T}^{T}\mathbf{T}(\bm{\theta}^{*}-\bm{\theta} _{k}^{SAM})]\] \[=\mathbb{E}_{\mathbf{A},\mathbf{T},\theta_{0}}[(\bm{\theta}^{*}- \bm{\theta}_{0})^{T}\mathbf{B}^{k}\mathbf{T}^{T}\mathbf{T}\mathbf{B}^{k}(\bm{ \theta}^{*}-\bm{\theta}_{0})]\] \[=\mathbb{E}_{\mathbf{A},\mathbf{T},\theta_{0}}[(\bm{\theta}^{*})^ {T}\mathbf{B}^{k}\mathbf{T}^{T}\mathbf{T}\mathbf{B}^{k}\bm{\theta}^{*}]+ \mathbb{E}_{\mathbf{A},\mathbf{T},\theta_{0}}[\bm{\theta}_{0}^{T}\mathbf{B}^{ k}\mathbf{T}^{T}\mathbf{T}\mathbf{B}^{k}\bm{\theta}_{0}]\] \[=\frac{n_{\text{te}}}{d_{\text{in}}}\phi(2k,0)\|\bm{\theta}^{*} \|_{2}^{2}+n_{\text{te}}\phi(2k,0)\sigma^{2},\]

Since \(\mathbb{E}_{\mathbf{T}}[\mathbf{T}^{T}\mathbf{T}]=\frac{n_{\text{te}}}{d_{ \text{in}}}\mathbf{I}\) and \(\mathbb{E}[\bm{\theta}_{0}\bm{\theta}_{0}^{T}]=\sigma^{2}\mathbf{I}.\) Hence,

\[\mathbb{D}(\bm{\theta}_{k}^{SAM})=\text{Var}\left(f(\bm{\theta}_{k}^{SAM}; \mathbf{T})\right)=\frac{1}{n_{\text{te}}}\left(n_{\text{te}}\text{Error}( \bm{\theta}_{k}^{SAM})-n_{\text{te}}\text{Bias}^{2}(\bm{\theta}_{k}^{SAM}) \right)=\phi(2k,0)\sigma^{2}.\]

Recall that given a perturbation radius \(\rho_{0}\), the sharpness is defined as

\[\kappa(\bm{\theta}_{k})=\mathbb{E}_{A}[\max_{\|\bm{\varepsilon}\|_{2}\leq\rho _{0}}f\left(\mathbb{E}_{\bm{\theta}_{0}}\left[\bm{\theta}_{k}\right]+\bm{ \varepsilon}\right)-f\left(\mathbb{E}_{\bm{\theta}_{0}}\left[\bm{\theta}_{k} \right]\right)].\]

We first compute

\[f\left(\mathbb{E}_{\bm{\theta}_{0}}\left[\bm{\theta}_{k}^{SAM} \right]+\bm{\varepsilon};\mathbf{A}\right) =\frac{1}{2}(\mathbb{E}_{\bm{\theta}_{0}}\left[\bm{\theta}_{k}^{SAM }\right]+\bm{\varepsilon}-\bm{\theta}^{*})^{T}A^{T}A(\mathbb{E}_{\bm{\theta}_{0 }}\left[\bm{\theta}_{k}^{SAM}\right]+\bm{\varepsilon}-\bm{\theta}^{*})\] \[=\frac{1}{2}(\bm{\varepsilon}-\mathbf{B}^{k}\bm{\theta}^{*})^{T} \mathbf{A}^{T}\mathbf{A}(\bm{\varepsilon}-\mathbf{B}^{k}\bm{\theta}^{*})\] \[=\frac{1}{2}\bm{\varepsilon}^{T}\mathbf{A}^{T}\mathbf{A}\bm{ \varepsilon}-\bm{\varepsilon}^{T}\mathbf{B}^{k}\mathbf{A}^{T}\mathbf{A}\bm{ \theta}^{*}+\frac{1}{2}(\bm{\theta}^{*})^{T}\mathbf{B}^{2k}\mathbf{A}^{T} \mathbf{A}\bm{\theta}^{*}.\] (7)

Similarly,

\[f\left(\mathbb{E}_{\bm{\theta}_{0}}\left[\bm{\theta}_{k}^{SAM}\right];\mathbf{ A}\right)=\frac{1}{2}(\bm{\theta}^{*})^{T}\mathbf{B}^{2k}\mathbf{A}^{T}\mathbf{A}\bm{ \theta}^{*}.\] (8)

Let \(\lambda_{min}\) be the least eigenvalue of \(\mathbf{A}^{T}\mathbf{A}\). By subtracting equation (7) with equation (8), we have

\[\kappa_{k}^{SAM} =\mathbb{E}_{\mathbf{A}}[\max_{\|\bm{\varepsilon}\|_{2}\leq\rho_{0 }}\frac{1}{2}\bm{\varepsilon}^{T}\mathbf{A}^{T}\mathbf{A}\bm{\varepsilon}-\bm{ \varepsilon}^{T}\mathbf{B}^{k}\mathbf{A}^{T}\mathbf{A}\bm{\theta}^{*}]\] \[\geq\mathbb{E}_{\mathbf{A}}[\max_{\|\bm{\varepsilon}\|_{2}=\rho_{0 }}\frac{1}{2}\lambda_{min}\|\mathbf{U}^{T}\bm{\varepsilon}\|_{2}^{2}-\bm{ \varepsilon}^{T}\mathbf{B}^{k}\mathbf{A}^{T}\mathbf{A}\bm{\theta}^{*}]\] \[\geq\mathbb{E}_{\mathbf{A}}[\max_{\begin{subarray}{c}\|\bm{ \varepsilon}\|_{2}=\rho_{0}\\ \bm{\varepsilon}=\bm{\mathrm{U}}\bm{\mathrm{V}}\end{subarray}}\frac{1}{2}\lambda_ {min}\|\mathbf{U}^{T}\bm{\varepsilon}\|_{2}^{2}-\bm{\varepsilon}^{T}\mathbf{B}^{k} \mathbf{A}^{T}\mathbf{A}\bm{\theta}^{*}]\] \[=\mathbb{E}_{\mathbf{A}}[\max_{\|\bm{\psi}\|_{2}=\rho_{0}}\frac{1}{2} \lambda_{min}\|\mathbf{v}\|_{2}^{2}-\min_{\|\bm{\varepsilon}\|_{2}=\rho_{0}} \bm{\varepsilon}^{T}\mathbf{B}^{k}\mathbf{A}^{T}\mathbf{A}\bm{\theta}^{*}]\] \[=\mathbb{E}_{\mathbf{A}}[\frac{2}{2}\lambda_{min}\rho_{0}^{2}+\rho_{0 }\|\mathbf{B}^{k}\mathbf{A}^{T}\mathbf{A}\bm{\theta}^{*}\|_{2}].\]

The smallest singular value \(\lambda_{min}\) of a random \(n\times d_{\text{in}}\) matrix \(\mathbf{A}\) can be bounded by the following inequality on the smallest singular value \(\sigma_{min}(A)\) by Vershynin (2018), assuming \(n_{\text{tr}}\geq d_{\text{in}}\), then almost surely \[\mathbb{E}_{\mathbf{A}}[\sigma_{min}(\mathbf{A})]\geq\sqrt{\frac{n_{\mathsf{tr}}}{d_ {\mathsf{in}}}}-1.\]

Therefore, \(\mathbb{E}_{A}[\lambda_{min}]\geq\mathbb{E}_{A}[\sigma_{min}(A)]^{2}\geq\left( \sqrt{\frac{n_{\mathsf{tr}}}{d_{\mathsf{in}}}}-1\right)^{2}\). Now we show a lower bound on \(\mathbb{E}_{\mathbf{A}}[\rho_{0}\|\mathbf{B}^{k}\mathbf{A}^{T}\mathbf{A} \boldsymbol{\theta}^{*}\|_{2}]\). By Gao et al. [2019], the Jensen gap \((\mathbb{E}[Z])^{1/2}-\mathbb{E}[(Z)^{1/2}]\) is upper bounded by \(\frac{\mathsf{Var}(Z)}{2}\) when \(Z\) is non-negative and \(\mathbb{E}[Z]=1\). Notice that

\[\mathbb{E}_{\mathbf{A}}[\rho_{0}\|\mathbf{B}^{k}\mathbf{A}^{T}\mathbf{A} \boldsymbol{\theta}^{*}\|_{2}]=\rho_{0}\mathbb{E}_{\mathbf{A}}[(\boldsymbol{ \theta}^{*})^{T}\mathbf{B}^{2k}(\mathbf{A}^{T}\mathbf{A})^{2}\boldsymbol{ \theta}^{*}\big{)}^{1/2}],\]

and we let \(Z=(\boldsymbol{\theta}^{*})^{T}\mathbf{B}^{2k}(\mathbf{A}^{T}\mathbf{A})^{2} \boldsymbol{\theta}^{*}\). Then \(\mathbb{E}_{\mathbf{A}}[Z]=\phi(2k,2)\|\boldsymbol{\theta}^{*}\|_{2}^{2}\) and

\[\text{Var}[Z]=\left(\phi(4k,4)-\phi(2k,2)^{2}\right)\|\boldsymbol{\theta}^{*} \|_{2}^{2}.\]

By normalizing \(Z\) and applying the Jensen gap upperbound, we have

\[\mathbb{E}_{\mathbf{A}}[\rho_{0}\|\mathbf{B}^{k}\mathbf{A}^{T}\mathbf{A} \boldsymbol{\theta}^{*}\|_{2}]\geq\rho_{0}\sqrt{\phi(2k,2)}\|\boldsymbol{ \theta}^{*}\|_{2}^{2}-\frac{\phi(4k,4)-\phi(2k,2)^{2}}{2\phi(2k,2)^{3/2}\| \boldsymbol{\theta}^{*}\|_{2}}.\]

As a result,

\[\kappa_{k}^{SAM}\geq\frac{\rho_{0}^{2}}{2}\left(\sqrt{\frac{n_{\mathsf{tr}}}{d _{\mathsf{in}}}}-1\right)^{2}+\rho_{0}\sqrt{\phi(2k,2)}\|\boldsymbol{\theta}^ {*}\|_{2}-\frac{\phi(4k,4)-\phi(2k,2)^{2}}{2\phi(2k,2)^{3/2}\|\boldsymbol{ \theta}^{*}\|_{2}}.\]

The derivation of the upper bound follows from a similar proof, ignoring the Jensen gap.

### Proof of Theorem 2

Below we show a proof of Theorem 2.

Proof.: We apply SVD to \(\mathbf{A}_{s}\) to obtain \(\mathbf{A}_{s}=\mathbf{V}_{s}\Sigma_{s}\mathbf{U}_{s}^{T}\) and \(\mathbf{A}_{s}^{T}\mathbf{A}=\mathbf{U}_{s}\Sigma_{s}^{2}\mathbf{U}_{s}^{T}\). Let \(\mathbf{D}_{s}=\Sigma_{s}^{2}\) and \(\mathbf{B}_{s}=\mathbf{I}-\eta\mathbf{A}_{s}^{T}\mathbf{A}_{s}-\eta\rho( \mathbf{A}_{s}^{T}\mathbf{A}_{s})^{2}\). By Theorem 3 and a similar derivation in the proof of Theorem 1,

\[\boldsymbol{\theta}_{k}^{SharpBal} =\eta\sum\limits_{j=0}^{k-1}\mathbf{B}_{s}^{j}\left(\mathbf{A}_{ s}^{T}\mathbf{A}_{s}+\rho(\mathbf{A}_{s}^{T}\mathbf{A}_{s})^{2}\right) \boldsymbol{\theta}^{*}+\mathbf{B}_{s}^{k}\boldsymbol{\theta}_{0}\] \[=\boldsymbol{\theta}^{*}+\left(\mathbf{I}-\eta\mathbf{A}_{s}^{T} \mathbf{A}_{s}-\eta\rho(\mathbf{A}_{s}^{T}\mathbf{A}_{s})^{2}\right)^{k}( \boldsymbol{\theta}_{0}-\boldsymbol{\theta}^{*}).\]

As a result, \(\mathbb{E}_{\boldsymbol{\theta}_{0},s}[\boldsymbol{\theta}_{k}^{Sharpbal}]= \mathbb{E}_{s}[\boldsymbol{\theta}^{*}-\mathbf{B}_{s}^{k} \boldsymbol{\theta}^{*}]=\boldsymbol{\theta}^{*}-\frac{1}{S}\sum\limits_{s=1}^{S }\mathbf{B}_{s}^{k}\boldsymbol{\theta}^{*}.\)

Applying Proposition 1, we have

\[\mathbb{E}_{\mathbf{A}}[\mathbf{B}_{s}^{i}(\mathbf{A}_{s}^{T}\mathbf{A}_{s})^{ j}]=\phi^{\prime}(i,j),\]

where

\[\phi^{\prime}(i,j)=\mathds{1}_{j=0}+\sum\limits_{k_{1}+k_{2}+k_{3}=i}\frac{i!}{ k_{1}!k_{2}!k_{3}!}(-\eta)^{k_{2}+k_{3}}\rho^{k_{3}}\left(\frac{n_{\mathsf{tr}}}{Sd_{ \mathsf{in}}}\right)^{m}\sum\limits_{l=1}^{m}\left(\frac{Sd_{\mathsf{in}}}{n_{ \mathsf{tr}}}\right)^{m-l}N_{m,l}.\]

Then,

\[n_{\mathsf{te}}\text{Bias}^{2}(\boldsymbol{\theta}_{k}^{SAM}) =\mathbb{E}_{\mathbf{A},\mathbf{T}}[\left(\mathbb{E}_{\boldsymbol {\theta}_{0},s}[\boldsymbol{\theta}_{k}^{Sharpbal}]-\boldsymbol{\theta}^{*} \right)^{T}\mathbf{T}\mathbf{T}\left(\mathbb{E}_{\boldsymbol{\theta}_{0},s}[ \boldsymbol{\theta}_{k}^{Sharpbal}]-\boldsymbol{\theta}^{*}\right)]\] \[=\frac{n_{\mathsf{te}}}{d_{\mathsf{in}}}\mathbb{E}_{\mathbf{A}}[(- \frac{1}{S}\sum\limits_{s=1}^{S}\mathbf{B}_{s}^{k}\boldsymbol{\theta}^{*})^{T}(- \frac{1}{S}\sum\limits_{s^{\prime}=1}^{S}\mathbf{B}_{s^{\prime}}^{k}\boldsymbol{ \theta}^{*})]\] \[=\frac{n_{\mathsf{te}}}{d_{\mathsf{in}}S^{2}}\mathbb{E}_{\mathbf{A}}[ \sum\limits_{s=1}^{S}\mathbf{B}_{s}^{k}\boldsymbol{\theta}^{*}\sum\limits_{s^{ \prime}=1}^{S}\mathbf{B}_{s^{\prime}}^{k}]\|\boldsymbol{\theta}^{*}\|_{2}^{2}\] \[=\frac{n_{\mathsf{te}}}{d_{\mathsf{in}}S}\left(\phi^{\prime}(2k,0) +(s-1)\phi^{\prime}(k,0)^{2}\right)\|\boldsymbol{\theta}^{*}\|_{2}^{2}.\]The last equality is the result of applying \(\mathbb{E}_{\mathbf{A}}[B_{s}^{i}]=\phi^{\prime}(i,0)\) with different combinations of \(\mathbf{B}_{s}\), \(\mathbf{B}_{s^{\prime}}\), counting multiplicity. Similarly,

\[n_{\text{te}}\text{Error}(\boldsymbol{\theta}_{k}^{SharpBal}) =\mathbb{E}_{\mathbf{A},\mathbf{T},\boldsymbol{\theta}_{0},s}[( \boldsymbol{\theta}^{*})^{T}\mathbf{B}_{s}^{k}\mathbf{T}^{T}\mathbf{TB}_{s}^{k }\boldsymbol{\theta}^{*}]+\mathbb{E}_{\mathbf{A},\mathbf{T},\boldsymbol{ \theta}_{0},s}[\boldsymbol{\theta}_{0}^{T}\mathbf{B}_{s}^{k}\mathbf{T}^{T} \mathbf{TB}_{s}^{k}\boldsymbol{\theta}_{0}]\] \[=\frac{n_{\text{te}}}{d_{\text{in}}}\phi^{\prime}(2k,0)\| \boldsymbol{\theta}^{*}\|_{2}^{2}+n_{\text{te}}\phi^{\prime}(2k,0)\sigma^{2}.\]

Therefore,

\[\text{Var}\left(f(\boldsymbol{\theta}_{k}^{SharpBal};\mathbf{T} )\right)= \frac{1}{n_{\text{te}}}\text{Error}(\boldsymbol{\theta}_{k}^{SharpBal })-n_{\text{te}}\text{Bias}^{2}(\boldsymbol{\theta}_{k}^{SharpBal})\right)\] \[= \phi^{\prime}(2k,0)\sigma^{2}+\frac{S-1}{d_{\text{in}}S}\left( \phi^{\prime}(2k,0)-\phi^{\prime}(k,0)^{2}\right)\|\boldsymbol{\theta}^{*}\|_ {2}^{2}.\]

When the model is trained on the submatrix, the sharpness of model \(\boldsymbol{\theta}_{k}^{SharpBal}\) is defined as

\[\kappa_{k}^{SharpBal}=\mathbb{E}_{\mathbf{A}}[\max_{\|\boldsymbol{\varepsilon }\|_{2}\leq\rho_{0}}f\left(\mathbb{E}_{\boldsymbol{\theta}_{0},s}\left[ \boldsymbol{\theta}_{k}^{SharpBal}\right]+\boldsymbol{\varepsilon};\mathbf{A} \right)-f\left(\mathbb{E}_{\boldsymbol{\theta}_{0},s}\left[\boldsymbol{\theta} _{k}^{SharpBal}\right];\mathbf{A}\right)].\]

From a similar analysis of the proof for Theorem 1,

\[\kappa_{k}^{SharpBal}\leq\frac{\rho_{0}^{2}}{2}\left(\sqrt{\frac{n_{\text{tr}}} {d_{\text{in}}}}+1\right)^{2}+\frac{\rho_{0}}{S}\mathbb{E}_{\mathbf{A}}[\| \sum_{s=1}^{S}\mathbf{B}_{s}^{k}\mathbf{A}^{T}\mathbf{A}\boldsymbol{\theta}^ {*}\|_{2}],\]

and with \(r=\frac{n_{\text{te}}}{Sd_{\text{in}}}\),

\[\mathbb{E}_{\mathbf{A}}[\|\sum_{s=1}^{S}\mathbf{B}_{s}^{k}\mathbf{ A}^{T}\mathbf{A}\boldsymbol{\theta}^{*}\|_{2}]= \mathbb{E}_{\mathbf{A}}[((\boldsymbol{\theta}^{*})^{T}\mathbf{A} \mathbf{A}\sum_{s=1}^{S}\mathbf{B}_{s}^{k}\sum_{s^{\prime}=1}^{S}\mathbf{B}_{s }^{\prime k}\mathbf{A}^{T}\mathbf{A}\boldsymbol{\theta}^{*})^{1/2}]\] \[\leq \left((\boldsymbol{\theta}^{*})^{T}\mathbb{E}_{\mathbf{A}}[\sum_{ j=1}^{S}\mathbf{A}_{j}^{T}\mathbf{A}_{j}\sum_{s=1}^{S}\mathbf{B}_{s}^{k}\sum_{s^{ \prime}=1}^{S}\mathbf{B}_{s}^{\prime k}\sum_{l=1}^{S}\mathbf{A}_{l}^{T} \mathbf{A}_{l}]\boldsymbol{\theta}^{*}\right)^{1/2}\] \[= (S\phi^{\prime}(2k,2)+2rS(S-1)\phi^{\prime}(2k,1)+2S(S-1)\phi^{ \prime}(k,2)\phi^{\prime}(k,0)\] \[+r(1+r)S(S-1)\phi^{\prime}(2k,0)+2S(S-1)\phi^{\prime}(k,1)\phi^{ \prime}(k,1)\] \[+\frac{3}{2}r(1+r)S(S-1)(S-2)\phi^{\prime}(k,0)^{2}\] \[+\frac{3}{2}r^{2}S(S-1)(S-2)\phi^{\prime}(2k,0)\] \[+3rS(S-1)(S-2)\phi^{\prime}(k,1)\phi^{\prime}(k,0)\] \[+r^{2}S(S-1)(S-2)(S-3)\phi^{\prime}(k,0)^{2})^{1/2}\|\boldsymbol{ \theta}^{*}\|_{2}.\]

The last equality is the result of applying \(\mathbb{E}_{\mathbf{A}}[B_{s}^{i}(\mathbf{A}_{s}^{T}\mathbf{A}_{s})^{j}]=\phi^ {\prime}(i,j)\) with different combinations of \(\mathbf{B}_{s}\), \(\mathbf{B}_{s^{\prime}}\), \(\mathbf{A}_{j}^{T}\mathbf{A}_{j}\), and \(\mathbf{A}_{l}^{T}\mathbf{A}_{l}\), counting multiplicity and the fact that \(\mathbb{E}_{\mathbf{A}}[(\mathbf{A}_{s}^{T}\mathbf{A}_{s})^{2}]=r(1+r)\mathbf{I}\). In conclusion,

\[\kappa_{k}^{SharpBal}\leq\frac{\rho_{0}^{2}}{2}\left(\sqrt{\frac{n_{\text{tr}}} {d_{\text{in}}}}+1\right)^{2}+\frac{\rho_{0}}{S}\sqrt{C}\|\boldsymbol{\theta}^ {*}\|_{2},\]

where

\[C= S\phi^{\prime}(2k,2)+2rS(S-1)\phi^{\prime}(2k,1)+2S(S-1)\phi^{ \prime}(k,2)\phi^{\prime}(k,0)\] \[+r(1+r)S(S-1)\phi^{\prime}(2k,0)+2S(S-1)\phi^{\prime}(k,1)\phi^{ \prime}(k,1)\] \[+\frac{3}{2}r(1+r)S(S-1)(S-2)\phi^{\prime}(k,0)^{2}+\frac{3}{2}r^{2 }S(S-1)(S-2)\phi^{\prime}(2k,0)\] \[+3rS(S-1)(S-2)\phi^{\prime}(k,0)\phi^{\prime}(k,1)+r^{2}S(S-1)(S- 2)(S-3)\phi^{\prime}(k,0)^{2}.\]

The claims in Theorem 2 is further supported by the experimental validations with results presented in Figure 9.

### Empirical Verification of Theorem 1 and 2

To demonstrate the robustness and tightness of the bounds presented in Theorem 1, we provide verification results across a range of parameter configurations. Interestingly, the observed model behaviors closely align with the upper bound derived in Theorem 1, highlighting the effectiveness of our theoretical analysis in capturing the underlying dynamics of the ensemble. Figure 10 illustrates these results, with each sub-figure corresponding to a specific combination of \(k\) and \(\eta\) with \(\rho\) from range \(0.5\) to \(0.3\). In these experiment, we generated 50 random data matrices \(\mathbf{A}\) of size \(3000\times 150\) and test data \(\mathbf{T}\) of size \(1000\times 150\). For each random dataset, we initialized 50 random model weights \(\bm{\theta}_{0}\) and collected the expected statistics of interest after training. To measure the sharpness \(\kappa_{k}^{SAM}\), we employed projected gradient ascent to find the optimal perturbation, using a step size of \(0.01\) and a maximum of \(50\) steps. Similar experiments are performed to verify the derivations in Theorem 2 with results presented in Figure 11, with the number of partitions \(S=10\).

## Appendix D Hyperparamter setting

### Datasets

We first evaluate on image classification datasets CIFAR-10 and CIFAR-100. The corresponding OOD robustness is evaluated on CIFAR-10C and CIFAR-100C [16]. The experiments are carried out on ResNet18 [11]. We use a batch size of 128, a momentum of 0.9, and a weight decay of 0.0005 for model training. TinyImageNet is an image classification dataset consisting of 100K images for training and 10K images for in-distribution testing. We evaluate ensemble's OOD robustness on TinyImageNetC [16].

### Hyperparamter setting for empirical sharpness-diversity trade-off

Here, we provide the hyperparameter for the experiments in Section 4.2. When using adaptive worst-case sharpness for sharpness measurement, the size of neighborhood \(\gamma\) defined in equation (3) needed to be specified, we use a \(\gamma\) of 0.5 for all the results in Figure 1 and Figure 5. Additionally, when training NNs in the ensemble, we change the perturbation radius \(\rho\) of SAM so that we can study the trade-off. The range of \(\rho\) for the results in Figure 1 is \(\{0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.3\}\), the range of \(\rho\) for the results in Figure 5 is \(\{0.01,0.015,0.02,0.025,0.03,0.05,0.1,0.2,0.3,0.4\}\).

Figure 9: (**Theoretical vs. Simulated sharpness-diversity trade-off in SharpBalance)** This figure illustrates the relationship between sharpness(upper bound) and diversity as predicted by Theorem 2 and as observed in simulations under different configurations. **(a)** validates our theoretical results by varying the perturbation radius \(\rho\) from \(1.0\) to \(0.4\). **(b)** validates the derivation by varying number of iterations \(k\) from \(1\) to \(15\). These results demonstrate the soundness of our derivation across a range of parameters.

### Hyperparamter setting for SharpBalance

**Hyperparameter setting on CIFAR10/100**. For experiments on CIFAR10/100, we train an NN from scratch with basic data augmentations, including random cropping, padding by four pixels, and random horizontal flipping. We use a batch size of 128, a momentum of 0.9, and a weight decay of 0.0005. For deep ensemble, we train each model for 200 epochs.

In addition, we use 10% of the training set as the validation set for selecting \(\rho\) and \(k\) based on the ensemble's performance. We make a grid search for \(\rho\) over \(\{0.01,0.02,0.05,0.1,0.2,0.5\}\). For SharpBalance, we use the same \(\rho\) as SAM and search \(k\) over \(\{0.2,0.3,0.4,0.5,0.6\}\). \(T_{d}\) is another hyperparameter introduced by SharpBalance, we use a \(T_{d}\) of 10 for all experiments on CIFAR10, a \(T_{d}\) of 100 and 150 respectively when training dense and sparse models on CIFAR100. See Table 1 for the optimal \(\rho\) and \(k\) after grid search.

**Hyperparameter setting on TinyImageNet**. For experiments on TinyImageNet, we adopt basic data augmentations, including random cropping, padding by four pixels, and random horizontal flipping. We train each model for 200 epochs. We use a batch size of 128, a momentum of 0.9, a weight decay of 5e-4, a \(T_{d}\) of 100, an initial learning rate of 0.1, and decay it with a factor of 10 at Epoch 100 and 150. We search \(\rho\) and \(k\) in the same range as what we do on CIFAR10/100. See Table 1 for the optimal \(\rho\) and \(k\) after grid search.

Figure 10: **(Theoretical vs. Simulated sharpness-diversity trade-off in SAM). This figure compares the sharpness and diversity as predicted by Theorem 1 and as observed in simulations under various parameter configurations. Results demonstrates the robustness of our theoretical analysis and tightness of the derived sharpness upper bound.**

## Appendix E Ablation studies on loss landscape metrics

In this section, we show that the sharpness-diversity trade-off generalizes to different measurements of sharpness and diversity. The results are presented in Figure 12.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Dataset** & **Model** & **Method** & \(\rho\) & \(k\) & \(T_{d}\) \\ \multirow{3}{*}{CIFAR10} & ResNet18 & Deep Ensemble & - & - & - \\  & ResNet18 & Deep Ensemble+SAM & 0.2 & - & - \\  & ResNet18 & SharpBalance & 0.2 & 0.4 & 100 \\ \hline \multirow{3}{*}{CIFAR100} & ResNet18 & Deep Ensemble & - & - & - \\  & ResNet18 & Deep Ensemble+SAM & 0.2 & - & - \\  & ResNet18 & SharpBalance & 0.2 & 0.5 & 100 \\ \hline \multirow{3}{*}{TinyImageNet} & ResNet18 & Deep Ensemble & - & - & - \\  & ResNet18 & Deep Ensemble+SAM & 0.2 & - & - \\ \cline{1-1}  & ResNet18 & SharpBalance & 0.2 & 0.3 & 100 \\ \hline \end{tabular}
\end{table}
Table 1: Hyperparamter setting for results in Section 4.4, we report the optimal \(\rho\) and \(k\) after grid search. Each result in Figure 7 is averaged over three ensembles, which corresponds to 9 random seeds, the random seeds we use are \(\{13,17,27,113,117,127,43,59,223\}\).

Figure 11: (**Theoretical vs. Simulated sharpness-diversity trade-off in SharpBalance).** This figure compares the sharpness and diversity as predicted by Theorem 2 and as observed in simulations under various parameter configurations. The observed model behaviors align closely with our derived upper bounds.

**Sharpness metric.** In the main paper, we use adaptive worst-case sharpness defined in equation (3), the parameter neighborhood is bounded by \(\ell_{2}\) norm. In this section, we consider two more sharpness metrics (Kwon et al., 2021; Andriushchenko et al., 2023): adaptive worst-case sharpness with the parameter neighborhood bounded by \(\ell_{\infty}\) norm (referred to as adaptive \(\ell_{\infty}\) worst-case sharpness); and adaptive average case sharpness bounded by \(\ell_{2}\) norm (termed average case sharpness).

The adaptive \(\ell_{\infty}\) worst-case sharpness is defined as:

\[\underset{\|T^{-1}_{\bm{\theta}}\in\bm{\|}_{\infty}\leq\rho_{0}}{\text{max}} \mathcal{L}_{\mathcal{D}}(\bm{\theta}+\bm{\varepsilon})-\mathcal{L}_{ \mathcal{D}}(\bm{\theta}).\] (9)

The average case sharpness is defined as:

\[\mathbb{E}_{\bm{\varepsilon}\sim\mathcal{N}\left(0,\rho_{0}^{2}\operatorname{ diag}(T_{\bm{\theta}}^{2})\right)}\quad\mathcal{L}_{\mathcal{D}}(\bm{\theta}+\bm{ \varepsilon})-\mathcal{L}_{\mathcal{D}}(\bm{\theta}),\] (10)

where \(\rho_{0}\) is the neighborhood size of current parameter \(\bm{\theta}\). \(T_{\bm{\theta}}\) is a normalization operator that ensures the sharpness measure is invariant with respect to the re-scaling operation of the parameter. The results, illustrated in Figures 12, corroborate our observation of a trade-off between sharpness and diversity.

**Diversity metric**. We consider Kullback-Leibler (KL) Divergence (Kullback and Leibler, 1951) as an alternative diversity metric, which is also widely used in previous literature to gauge the diversity of two ensemble members (Fort et al., 2019; Liu et al., 2022). Specifically, the KL-divergence between the outputs of two ensemble members given a data sample \((\bm{x},\bm{y})\) is defined as:

\[\text{KL}\left(f_{\bm{\theta}_{1}}(\bm{x}),f_{\bm{\theta}_{2}}(\bm{x})\right) =\mathbb{E}_{f_{\bm{\theta}_{1}}(\bm{x})}\left[\log f_{\bm{\theta}_{1}}(\bm{x })-\log f_{\bm{\theta}_{2}}(\bm{x})\right].\] (11)

We measure the KL divergence on each data sample in the test data and then average the measured KL divergence. The results for KL-divergence are shown in Figure 12, which demonstrate the trade-off remains consistent for different diversity metrics.

Figure 12: **(Ablation study of varying sharpness and diversity metrics to corroborate existence of sharpness-diversity trade-off).** (a)(d) Varying sharpness metric by using the adaptive \(\ell_{\infty}\) worst-case sharpness. (b)(e) Varying sharpness metric by using the adaptive \(\ell_{2}\) average case sharpness. (c)(f) Varying diversity metric by using the KL divergence. The sharpness-diversity trade-off is still observed in all the settings. The \(x\)-axis and \(y\)-axis are in log scale. The notation \(\beta\) stands for the slope of the linear regression function fitted on all the ensembles trained by SAM.

[MISSING_PAGE_EMPTY:26]

### Sharpness-aware set: hard vs easy examples

SharpBalance aims to achieve the optimal balance by applying SAM to a carefully selected subset of the data while performing standard optimization on the remaining samples. In our work, sharpness is determined by the curvature of the loss around the model's weights, whereas [14] determines it based on the curvature of the loss around a data point. In Figure 14, we rank 1000 samples using both metrics and found a strong correlation between these two.

Figure 14: Rank correlation between fisher trace and loss curvature around input data

Figure 13: The three-member WRN-40-2 ensemble is trained with different methods on two datasets. The first row reports the OOD accuracy and the second row reports the ID accuracy. The lower part of each bar with the diagonal lines represents the individual model performance. The upper part of each bar represents the ensembling improvement. The results are reported by averaging three ensembles, and each ensemble is comprised of three models.

### Comparison with more baselines

We compare SharpBalance with stronger ensemble method EoA [Arpit et al., 2022] and stronger SAM methods. We carefully tuned the hyperparameters for EoA. EoA fine-tuned a pre-trained model; and in our paper, all models are trained from scratch. We compare SharpBalance with another SAM baseline: SAM +, where three individual models are trained with different \(\rho\) values, e.g., 0.05, 0.1, and 0.2, respectively. From Table 6, SharpBalance outperforms these two baselines both in-distribution and OOD generalization.

In Table 7, we combine GSAM [Zhuang et al., 2022] with Deep Ensemble as a new baseline method "Deep Ensemble + GSAM", and incorporate the GSAM into our method SharpBalance. The results show that the new baseline with GSAM outperforms the original baseline in ID and OOD performance but still underperforms SharpBalance (w/ SAM). Furthermore, we enhance SharpBalance by replacing the SAM with GSAM, which leads to better ID performance.

\begin{table}
\begin{tabular}{c l c c} \hline \hline Dataset & Method & ACC & cACC \\ \hline \multirow{3}{*}{CIFAR10} & SAM + & 96.03 & 76.29 \\  & EoA & 95.55 & 75.57 \\ \cline{2-4}  & SharpBalance & 96.18 (\(\pm\)0.15) & 77.32 (\(\pm\)1.03) \\ \hline \hline \multirow{3}{*}{CIFAR100} & SAM + & 79.67 & 51.28 \\  & EoA & 79.53 & 51.45 \\ \cline{1-1}  & SharpBalance & 79.84 (\(\pm\)0.17) & 52.46 (\(\pm\)1.01) \\ \hline \hline \end{tabular}
\end{table}
Table 6: SharpBalance outperforms EoA and SAM + both in-distribution and OOD generalization on CIFAR10 and CIFAR100.

\begin{table}
\begin{tabular}{c|c|c} \hline \hline Method & CIFAR100 Acc & CIFAR100-C Acc \\ \hline Deep Ensemble & 79.28 & 50.67 \\ \hline Deep Ensemble + SAM & 79.50 & 51.28 \\ SharpBalance (SAM) & 79.84 & **52.46** \\ \hline Deep Ensemble + GSAM & 79.74 & 51.37 \\ SharpBalance (GSAM) & **80.01** & 51.92 \\ \hline \hline \end{tabular}
\end{table}
Table 7: (**Comparing our method SharpBalance with stronger SAM baseline).** The ensemble test accuracy is reported and each ensemble comprises three members. GSAM improves the original baseline method with SAM and SharpBalance. The model is ResNet18.

Figure 15: (**Uncertainty metrics on CIFAR100-C**). “ECE” represents expected calibration error, and “NLL” represents negative log-likelihood. Both metrics are lower the better. The model architecture is ResNet-18. The uncertainty metrics demonstrate the superior performance of SharpBalance. \(x\)-axis represents the number of individual models in one ensemble.

[MISSING_PAGE_FAIL:29]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Main claims made in the abstract and introduction accurately reflect the scope and contribution of the paper, supported by our theoretical and experimental results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of the work are discussed in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Proofs of all theoretical results are provided in Appendix C and the assumptions are clearly stated in the theorems. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper fully discloses all relevant information including detailed description of the algorithms, datasets, experimental set up in Section 4 and hyperparameters in Appendix D to reproduce the main experimental results claimed in the paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The implementation can be found through the anonymous github repository and the zip file uploaded as the supplementary materials. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental settings with hyperparameters are provided in both Section 4 and Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The results are accompanied by error bars, for the experiments in Section 4.4 that support the main claims of the paper. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The information on the computer resources for experiments can be found in Appendix G. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The impact statement of the study can be found in Appendix A. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The original papers that produced the code or dataset are appropriately cited in this work. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with Human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.