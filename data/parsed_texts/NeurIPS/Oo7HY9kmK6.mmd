# Mean-Field Langevin Dynamics for Signed Measures

via a Bilevel Approach

 Guillaume Wang1

1Ecole polytechnique federale de Lausanne

Alireza Mousavi-Hosseini1

1University of Toronto and Vector Institute

guillaume.wang@epfl.ch, mousavi@cs.toronto.edu, lenaic.chizat@epfl.ch

Lenaic Chizat1

1Ecole polytechnique federale de Lausanne

###### Abstract

Mean-field Langevin dynamics (MLFD) is a class of interacting particle methods that tackle convex optimization over probability measures on a manifold, which are scalable, versatile, and enjoy computational guarantees. However, some important problems - such as risk minimization for infinite width two-layer neural networks, or sparse deconvolution - are originally defined over the set of signed, rather than probability, measures. In this paper, we investigate how to extend the MFLD framework to convex optimization problems over signed measures. Among two known reductions from signed to probability measures - the _lifting_ and the _bilevel_ approaches - we show that the bilevel reduction leads to stronger guarantees and faster rates (at the price of a higher per-iteration complexity). In particular, we investigate the convergence rate of MFLD applied to the bilevel reduction in the low-noise regime and obtain two results. First, this dynamics is amenable to an annealing schedule, adapted from [10], that results in improved convergence rates to a fixed multiplicative accuracy. Second, we investigate the problem of learning a single neuron with the bilevel approach and obtain local exponential convergence rates that depend polynomially on the dimension and noise level (to compare with the exponential dependence that would result from prior analyses).

## 1 Introduction

Let \(\mathcal{M}(\mathcal{W})\) be the set of finite signed measures on a compact Riemannian manifold without boundaries \(\mathcal{W}\) and let \(G:\mathcal{M}(\mathcal{W})\to\mathbb{R}\) be a convex function, assumed smooth in the sense of Assumption 1 below. In this paper, we investigate optimization methods to solve

\[\min_{\nu\in\mathcal{M}(\mathcal{W})}G_{\lambda}(\nu), G_{\lambda}(\nu)\coloneqq G(\nu)+\frac{\lambda}{2}\|\nu\|_{TV}^{2},\] (1.1)

where \(\|\cdot\|_{TV}\) is the total variation norm and \(\lambda>0\) the regularization level.2 This covers for instance risk minimization for infinite-width 2-layer neural networks (2NN) [1, 1] by taking \(\mathcal{W}=\mathbb{S}^{d}\) the unit sphere in \(\mathbb{R}^{d+1}\) or \(\mathcal{W}=\mathbb{R}^{d+1}\) and

Footnote 2: The square exponent on \(\|\cdot\|_{TV}\) might appear unusual, but it is convenient for our subsequent developments. We show in App. A that the regularization path is the same with or without the square.

\[G(\nu)=\mathbb{E}_{(x,y)\sim\rho}\Big{[}\ell(h(\nu,x),y)\Big{]} \qquad\text{ where }\qquad h(\nu,x)=\int_{\mathcal{W}}\varphi(\langle x,w \rangle)\mathrm{d}\nu(w).\] (1.2)

Here \(\varphi:\mathbb{R}\to\mathbb{R}\) is the activation function, \(h(\nu,\cdot)\) is the predictor parameterized by \(\nu\), \(G\) is the (population or empirical) risk under the data distribution \(\rho\in\mathcal{P}(\mathbb{R}^{d+1}\times\mathbb{R})\), and \(\ell\) is smooth (uniformlyin \(y\)) and convex in its first argument. These 2NNs will be our guiding examples throughout, but note that the class of problems covered by Eq. (1.1) is more general and includes for instance sparse deconvolution via the Beurling-LASSO estimator [11] or optimal design [12].

To tackle such problems, interacting particle methods use the parameterization \(\nu=\sum_{i=1}^{m}r_{i}\delta_{w_{i}}\) and apply gradient methods in a well-chosen geometry [10, 11, 1]. They have recently gained traction thanks to their scalability and flexibility, and in the context of 2NNs, the usual gradient descent algorithm is an instance of such a method. On the downside, _global_ convergence guarantees remain difficult to obtain due to the nonconvex nature of the reparameterized problem and existing positive results require either very specific settings [12], or modifications of the dynamics which often limit their scalability2.

Footnote 2: Such as forcing the particles to remain close to their initial position [10], or adding new particles using a potentially hard linear minimization oracle [13].

In a related, but slightly different context, mean-field Langevin dynamics (MFLD) solve entropy-regularized problems of the form

\[\min_{\mu\in\mathcal{P}(\mathcal{W}^{\prime})}F_{\beta}(\mu), F_{\beta}(\mu)\coloneqq F(\mu)+\beta^{-1}H(\mu),\] (1.3)

where \(\mathcal{P}(\mathcal{W}^{\prime})\) is the space of probability measures on a manifold \(\mathcal{W}^{\prime}\) (typically \(\mathbb{R}^{d}\)), \(F:\mathcal{P}(\mathcal{W}^{\prime})\to\mathbb{R}\) is a (sufficiently regular) convex functional, \(H(\mu)=\int\log(\mathrm{d}\mu/\mathrm{d}\operatorname{vol})\mathrm{d}\mu\) is the negative differential entropy and \(\beta>0\). These dynamics are obtained as the mean-field limit of _noisy_ interacting particles dynamics [14, 15] and converge globally at an exponential rate [16, 17], under two key conditions on \(F\): (i) a notion of regularity, which we refer to as _displacement smoothness_ (see P1 below) and (ii) a _uniform log-Sobolev inequality (LSI)_ condition (see P2 below). These mean-field, continuous-time guarantees have been further refined into computational guarantees for fully discrete algorithms [13, 15]. The favorable properties of MFLD naturally lead to the following question:

_Can we efficiently solve problems of the form Eq. (1.1) using MFLD?_

At first, it is not obvious that MFLD can be applied at all since it is originally defined only for problems over probability measures. However, we can find in the literature two general recipes to reduce a problem over \(\mathcal{M}(\mathcal{W})\) to a problem over \(\mathcal{P}(\mathcal{W}^{\prime})\), thus amenable to MFLD. The first one is a _lifting_ reduction, that takes \(\mathcal{W}^{\prime}=\mathbb{R}\times\mathcal{W}\) where the extra dimension serves to encode the signed mass of particles [14, Section A.2][10]. The second one, that takes \(\mathcal{W}^{\prime}=\mathcal{W}\), is a _bilevel_ reduction [1, 12] that uses a variational representation of the regularizer \(\|\cdot\|_{TV}^{2}\), common in the multiple kernel learning literature [10]. A first task is thus to compare the behavior of MFLD on these two approaches. Furthermore, MFLD involves an entropic regularization which is absent from Eq. (1.1). A second task is thus to analyze the behavior of MFLD in the large \(\beta\) regime, when the regularization vanishes.

In this work, we tackle these two tasks and make the following contributions:

* In Sec. 3, we introduce the lifting and bilevel reductions and compare the "displacement smoothness" (P1) and "uniform LSI" (P2) properties of the resulting problems. These properties play a central role in the global convergence analysis of MFLD. Specifically, we consider a large class of lifting reductions and show that none satisfies simultaneously (P1) and (P2) unless \(\lambda\) is large. In contrast, the bilevel reduction satisfies both under mild assumptions. So in the sequel we focus on MFLD applied to the bilevel reduction.
* In Sec. 4, we investigate what convergence rates can be obtained for the problem (1.1) by using MFLD on the bilevel formulation. While a classical simulated annealing technique yields convergence in \(O(\log\log t/\log t)\), we show that the structure of the bilevel objective is in fact amenable to a more efficient annealing schedule, adapted from [15], that reaches a fixed multiplicative accuracy, say \(1.01\inf G_{\lambda}\), in time \(e^{O(\lambda^{-1}\log\lambda^{-1})}\) instead of \(e^{O(\lambda^{-2})}\) for the classical schedule.
* In Sec. 5, to obtain a more complete picture, we investigate the problem of learning a single neuron. Here, using a Lyapunov type argument, we show that the _local_ convergence rate of MFLD applied to the bilevel formulation scales polynomially in \(\beta\) and \(d\), at odds with all previous MFLD analyses which had exponential dependencies.

All proofs are deferred to the Appendix.

### Related work

Particle methods and mean-field limits.Interacting particle systems have been studied for decades in various fields, see e.g. [11, 12, 13]. Their more recent connection with the standard training of 2NNs [10, 11, 12, 13] has suggested new settings of analysis, where convexity of the functional plays a key role, and has led to many developments. In particular, the case of MFLD (under study here) quickly progressed from nonquantitative guarantees [13, 14], to mean-field convergence rates [15, 16] and fully discrete computational guarantees [17, 18, 19] in the span of a few years. Recent progress also address its accelerated (underdamped) version [13, 14], which could also be of interest in our setting.

Multiple kernel learning and bilevel training of NNs.The lifting reductions we consider are inspired by the unbalanced optimal transport literature [10], while the bilevel reduction comes from the Multiple Kernel Learning (MKL) literature [13, 12, 14] (see [1] for an account). While the latter is usually studied with a discrete domain \(\mathcal{W}\) (see also [16, 17] for recent computational considerations), it was suggested for the training of large width 2NN in [1] and used in conjonction with MFLD in [15] (more details below). Relatedly, a recent line of work studies the (noiseless) training of 2NN in a two-timescale regime, where the outer layer is trained at a much faster rate than the inner layer [1, 16, 17]. This implicitly corresponds to optimizing the bilevel objective and leads to improved convergence guarantees.

The work that is closest to ours is [10], which considers the MFLD on a 2NN with weight decay where the outer layer is optimized at each step. They interpret the resulting dynamics as a kernel learning dynamics and study properties of the learnt kernel and its associated RKHS. While they do not formulate explicitly the problem Eq. (1.1), it can be shown that our approaches are equivalent when considering \(\mathcal{W}=\mathbb{R}^{d+1}\) in Eq. (1.2) (and adding an extra regularization). The details are given in Sec. A.2. Key advantages of our formulation with \(\mathcal{W}=\mathbb{S}^{d}\) are that we cover the case of unbounded homogeneous activation functions (such as ReLU), and can obtain improved LSI.

## 2 Background on guarantees for mean-field Langevin dynamics

The MFLD is defined as the Wasserstein gradient flow \((\mu_{t})_{t\in\mathbb{R}_{+}}\) in \(\mathcal{P}(\Omega)\) of an objective of the form Eq. (1.3). It is characterized as the solution to the partial differential equation (PDE)

\[\partial_{t}\mu_{t}=\mathrm{div}(\mu_{t}\nabla F^{\prime}[\mu_{t}])+\beta^{-1} \Delta\mu_{t},\qquad\mu_{0}\in\mathcal{P}(\Omega).\] (2.1)

where \(F^{\prime}[\mu]:\Omega\to\mathbb{R}\) is the _first variation_ of \(F\) at \(\mu\)[11, Sec. 7.2], defined by \(\lim_{\epsilon\downarrow 0}\frac{1}{\epsilon}(F(\mu+\epsilon(\mu^{\prime}-\mu))-F( \mu))=\int F^{\prime}[\mu]\mathrm{d}(\mu^{\prime}-\mu)\) for any \(\mu^{\prime}\in\mathcal{P}(\Omega)\). This PDE corresponds to the mean-field limit (\(N\to\infty\)) of the noisy particle gradient flow \(\omega_{t}\in\Omega^{N}\):

\[\forall i\leq N,\;\mathrm{d}\omega_{t}^{i}=-N\nabla_{\omega_{t}^{i}}F^{(N)} \left(\omega_{t}^{1},...,\omega_{t}^{N}\right)\mathrm{d}t+\sqrt{2\beta^{-1}} \mathrm{d}B_{t}^{i},\qquad\omega_{0}^{i}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mu_{0}\]

where \(F^{(N)}\left(\omega^{1},...,\omega^{N}\right)=F\left(\frac{1}{N}\sum_{i=1}^{N} \delta_{\omega^{i}}\right)\) and the \(B_{t}^{i}\) are \(N\) independent Brownian motions on \(\Omega\). The convergence guarantees for MFLD rely on three key properties:

1. **(Convexity)**\(F\) is convex and is such that \(F_{\beta}\) admits a minimizer \(\mu_{\beta}^{*}\).
2. **(Displacement smoothness)**\(F\) is \(L\)-displacement smooth, in the sense that3 Footnote 3: Strictly speaking, (P1) is only a sufficient condition for displacement smoothness (see details in App. B). We refer to (P1) as displacement smoothness in this paper for conciseness only. \[\forall\mu\in\mathcal{P}_{2}(\Omega),\;\forall\omega\in\Omega,\;\max_{ \begin{subarray}{c}s\in T\cup\Omega\\ \left\|s\right\|_{\omega}\leq 1\end{subarray}}\;\left|\nabla^{2}\,F^{\prime}[\mu](s,s) \right|\leq L,\] \[\text{and}\quad\quad\forall\mu,\mu^{\prime}\in\mathcal{P}_{2}(\Omega),\; \forall\omega\in\Omega,\;\left\|\nabla F^{\prime}[\mu]-\nabla F^{\prime}[ \mu^{\prime}]\right\|_{\omega}\leq L\;W_{2}(\mu,\mu^{\prime}),\] where \(\nabla^{2}\) denotes the Riemannian Hessian.
3. **(Uniform LSI)** There exists \(\alpha>0\) such that \(\forall t\geq 0\), \(F_{\beta}\) satisfies local \(\alpha\)-LSI at \(\mu_{t}\), as in Def. 2.1.

**Definition 2.1** (Local LSI).: We say that a functional \(F_{\beta}=F+\beta^{-1}H\) satisfies local \(\alpha\)-LSI at \(\mu\in\mathcal{P}(\Omega)\) if \(Z\coloneqq\int_{\Omega}\exp\left(-\beta F^{\prime}[\mu]\right)\mathrm{d}\omega<\infty\) and the _proximal Gibbs measure_\(\hat{\mu}\coloneqq Z^{-1}\exp(-\beta F^{\prime}[\mu])\in\mathcal{P}(\Omega)\) satisfies \(\alpha\)-LSI, that is

\[\forall\mu^{\prime}\in\mathcal{P}(\Omega),\ H\left(\mu^{\prime}|\hat{\mu} \right)\leq\frac{1}{2\alpha}I(\mu^{\prime}|\hat{\mu}),\]

where the relative entropy and relative Fisher Information are respectively defined as

\[H\left(\mu^{\prime}|\hat{\mu}\right)\coloneqq\int_{\Omega}\log\left(\frac{ \mathrm{d}\mu^{\prime}}{\mathrm{d}\hat{\mu}}\right)\mathrm{d}\mu^{\prime}, \qquad\quad I(\mu^{\prime}|\hat{\mu})\coloneqq\int_{\Omega}\left\|\nabla\log \frac{\mathrm{d}\mu^{\prime}}{\mathrm{d}\hat{\mu}}(\omega)\right\|_{\omega}^ {2}\mathrm{d}\mu^{\prime}(\omega),\]

and \(\|\cdot\|_{\omega}\) denotes the Riemannian metric.

We review some useful criteria for LSI in App. B. In particular, the uniform LSI property (P2) holds for example when training two-layer neural networks with a frozen second layer, under some technical assumptions such as bounded activation function. In fact in that case, the proximal Gibbs measures \(\hat{\mu}\) even satisfy LSI uniformly for _all_\(\mu\in\mathcal{P}(\Omega)\)[10, 11].

Note that the Riemannian gradient \(\nabla\) and the Laplace-Beltrami operator \(\Delta\) appearing in (2.1), as well as the definition of Brownian motion, depend on the Riemannian metric of \(\Omega\). This dependency is reflected in (P1) and (P2).

The global convergence of MFLD is guaranteed by the following theorem, with a rate.

**Theorem 2.1** ([10, Thm. 3.2][11, Thm. 1]).: _Consider \(F:\mathcal{P}(\Omega)\to\mathbb{R}\) and \((\mu_{t})\) as in (2.1). If (P0), (P1) and (P2) are satisfied then for \(t\geq 0\) it holds_

\[\beta^{-1}H(\mu_{t}|\mu_{\beta}^{*})\leq F_{\beta}(\mu_{t})-F_{\beta}(\mu_{ \beta}^{*})\leq\exp(-2\beta^{-1}\alpha\ t)\Big{(}F_{\beta}(\mu_{0})-F_{\beta} (\mu_{\beta}^{*})\Big{)}.\]

Note that although the \(L\)-smoothness constant does not appear in Thm. 2.1, it does appear in the discrete-time guarantees of [12], and is thus an important quantity in practice. In this paper, we limit our analysis to the mean-field dynamics (2.1) because its time-discretization has not yet been studied on Riemannian manifolds. In continuous time, the proof of Thm. 2.1 translates directly to Riemannian manifolds thanks to our definition of (P1), see App. B.

## 3 Reductions from signed measures to probability measures

In order to apply the MFLD framework to solve our initial problem over signed measures (1.1), we must first recast it as an optimization problem over probability measures. In this section we build two such reductions, and discuss the properties (P0, P1 and P2) of the resulting problems.

### Reduction by lifting

Reductions by lifting consist in representing signed measures as projections of probability measures in the higher dimensional space \(\Omega=\mathbb{R}\times\mathcal{W}\). This construction involves the \(1\)-homogeneous projection operator4\(\bm{h}:\mathcal{P}_{1}(\Omega)\to\mathcal{M}(\mathcal{W})\) characterized by

Footnote 4: We could consider more general \(p\)-homogeneous projections as in [13], but we show in Sec. C.2 that we can always bring ourselves back to the case \(p=1\) up to a change of metric.

\[\forall\varphi\in\mathcal{C}(\mathcal{W},\mathbb{R}),\ \int_{\mathcal{W}} \varphi(w)(\bm{h}\mu)(\mathrm{d}w)=\int_{\Omega}r\varphi(w)\mu(\mathrm{d}r, \mathrm{d}w),\]

where \(\mathcal{P}_{p}(\Omega)\) is the subset of \(\mathcal{P}(\Omega)\) for which \(\int|r|^{p}\mathrm{d}\mu(\mathrm{d}r,\mathrm{d}w)<+\infty\). For instance, it acts on discrete measures as \(\bm{h}\left(\frac{1}{m}\sum_{j=1}^{m}\delta_{(r_{j},w_{j})}\right)=\frac{1}{ m}\sum_{j=1}^{m}r_{j}\delta_{w_{j}}\). We also define, for \(b\in[1,2]\) and \(\mu\in\mathcal{P}_{b}(\Omega)\), \(\Psi_{b}(\mu)\coloneqq\left(\int_{\Omega}|r|^{b}\mathrm{d}\mu(r,w)\right)^{2/b}\). The objective functional of the lifted problem is then defined, for \(\mu\in\mathcal{P}_{b}(\Omega)\), as

\[F_{\lambda,b}(\mu)\coloneqq G(\bm{h}\mu)+\frac{\lambda}{2}\Psi_{b}(\mu).\] (3.1)

It is equivalent to minimize \(G_{\lambda}\) or \(F_{\lambda,b}\), as shown in the following statement.

**Proposition 3.1**.: _Let \(\nu\in\mathcal{M}(\mathcal{W})\). For any \(\mu\in\mathcal{P}_{b}(\mathcal{W})\) such that \(\mathbf{h}\mu=\nu\), it holds \(F_{\lambda,b}(\mu)\geq G_{\lambda}(\nu)\), and equality holds for \(\mu(\mathrm{d}r,\mathrm{d}w)=\delta_{f(w)}(\mathrm{d}r)\frac{|\nu|(\mathrm{d}w )}{\|\nu\|_{TV}}\) where \(f(w)=\|\nu\|_{TV}\frac{\mathrm{d}\nu}{\mathrm{d}|\nu|}(w)\) (and only for this \(\mu\) when \(b>1\)). In particular, if \(G_{\lambda}\) admits a minimizer then \(F_{\lambda,b}\) does too, and it holds_

\[\min_{\mu\in\mathcal{P}_{b}(\Omega)}F_{\lambda,b}(\mu)=\min_{\nu\in\mathcal{M} (\mathcal{W})}G_{\lambda}(\nu).\]

It is not difficult to see that \(F_{\lambda,b}\) satisfies (P0) as long as \(G_{\lambda}\) admits a minimizer. In order to study (P1) and (P2), we need to define a Riemannian metric on \(\Omega\). Following [10], we consider a general class of Riemannian metrics on \(\Omega^{*}\coloneqq\mathbb{R}^{*}\times\mathcal{W}\), parameterized by \(q_{r},q_{w}\in\mathbb{R}\) and \(\Gamma>0\), defined by

\[\left\langle\begin{pmatrix}\delta r_{1}\\ \delta w_{1}\end{pmatrix},\begin{pmatrix}\delta r_{2}\\ \delta w_{2}\end{pmatrix}\right\rangle_{(r,w)}=\Gamma^{-1}\left|r\right|^{q_{r} }\frac{\delta r_{1}\delta r_{2}}{r^{2}}+\left|r\right|^{q_{w}}\left\langle \delta w_{1},\delta w_{2}\right\rangle_{w}.\] (3.2)

This indeed defines an inner product on \(T_{(r,w)}\Omega^{*}\coloneqq\mathbb{R}\times T_{w}\mathcal{W}\) that varies smoothly, and so equips \(\Omega^{*}\) with a (disconnected) Riemannian manifold structure [14]. Intuitively, the parameter \(\Gamma\) will govern the relative speed of the weight or position variables along gradient flows; larger \(\Gamma\) means faster weight updates.

Two particular cases of this construction appear (sometimes implicitly) in the literature on 2NN:

1. when \(q_{r}=2\) and \(q_{w}=0\), the metric (3.2) extends to the product metric on \(\Omega=\mathbb{R}\times\mathcal{W}\). With \(\mathcal{W}=\mathbb{R}^{d+1}\), this corresponds to the usual parameterization of 2NNs and is the setting of most previous works applying MFLD to 2NN (with a weight decay regularization on the second layer for \(b=2\) and \(\lambda>0\)).
2. when \(q_{r}=q_{w}=1\), \(\Omega^{*}\) is isometric to the union of two copies of the (tipless) metric cone over \(\mathcal{W}\)[1] (via the mapping \((r,\omega)\mapsto(\text{sign}(r),\sqrt{|r|},\omega)\)). This is the natural setting for optimization over signed measures; and with \(\mathcal{W}=\mathbb{S}^{d}\), is equivalent to the parameterization of 2NNs with ReLU activation and balanced initialization [11, App. H].

Issues caused by the disconnectedness of \(\Omega^{*}\).On the level of the equivalence of variational problems, one can check that the statement of Prop. 3.1 also holds if \(\Omega=\mathbb{R}\times\mathcal{W}\) is replaced by \(\Omega^{*}=\mathbb{R}^{*}\times\mathcal{W}\). However, when the manifold \(\Omega^{*}\) is truly disconnected,5 then \(\mathcal{P}(\Omega)\) is not connected in the sense of absolutely continuous curves in Wasserstein space. More precisely, \(\Omega^{*}\) is the disjoint union of \(\Omega^{*}_{-}=\mathbb{R}^{*}_{+}\times\mathcal{W}\) and \(\Omega^{*}_{-}=\mathbb{R}^{*}_{-}\times\mathcal{W}\), and one can show that (for certain choices of \(q_{r},q_{w}\)), if \((\mu_{t})_{t}\) is a Wasserstein gradient flow (or any other absolutely continuous curve), then \(\mu_{t}(\Omega^{*}_{+})=\mu_{0}(\Omega^{*}_{+})\) for all \(t\).

Footnote 5: This issue also occurs in the case \(q_{r}=q_{w}=1\), even though \(\Omega^{*}\) can be completed into a topologically connected set by adding an element \(0\) “bridging” the two cones \(\Omega^{*}_{+}\) and \(\Omega^{*}_{-}\). Indeed, any particle reaching \(0\) remains at \(0\) for all subsequent times. Besides, this completion is not itself a manifold, as \(0\) is a singularity.

Moreover, supposing for simplicity that \(G_{\lambda}\) has a unique minimizer \(\nu\) and that \(b>1\), then \(F_{\lambda,b}\) has a unique minimizer \(\mu^{*}\), and \(\mu^{*}(\Omega^{*}_{+})=\nu_{+}(\mathcal{W})/\left\|\nu\right\|_{TV}\) where \(\nu=\nu_{+}-\nu_{-}\) is the Jordan decomposition of \(\nu\). Therefore, Wasserstein gradient flow for \(F_{\lambda,b}\) can only converge to \(\mu^{*}\) if it was initialized such that \(\mu_{0}(\Omega^{*}_{+})=\mu^{*}(\Omega^{*}_{+})\). In terms of particle methods, this means that the fraction of the particles \((r_{i},w_{i})\) initialized with \(r_{i}>0\) must be precisely \(\mu^{*}(\Omega^{*}_{+})\). A similar problem arises if we apply MFLD to \(F_{\lambda,b}\), since it is nothing else than Wasserstein gradient flow for \(F_{\lambda,b}+\beta^{-1}H\); but it is more tedious to discuss formally, as \(F_{\lambda,b}+\beta^{-1}H\) does not have a minimizer in general.

In order to bypass this limitation, one may focus on settings where the ratio \(\nu_{+}(\mathcal{W})/\left\|\nu\right\|_{TV}\) for the optimal \(\nu\) is known in advance, e.g., the problem (1.1) constrained to non-negative measures, or on choices of \(q_{r},q_{w}\) for which \(\Omega^{*}\) can be extended into a connected manifold, such as the product metric \(q_{r}=2,q_{w}=0\). However, even in those cases, MFLD on \(F_{\lambda,b}\) presents other limitations.

Incompatibility with MFLD.We now show that, in spite of the degrees of freedom given by the parameters \(q_{r},q_{w}\) and \(b\), satisfying both (P1) and (P2) requires restrictive assumptions. This suggests that the lifting approach is fundamentally incompatible with MFLD.

**Proposition 3.2**.: _Consider \(F_{\lambda,b}\) from Eq. (3.1) and \(\Omega^{*}\) equipped with the metric (3.2). Suppose \(G^{\prime}[\nu]\) is continuous for all \(\nu\) and that there exists \(\nu\) such that \(\nabla^{2}G^{\prime}[\nu]\) is not constant equal to \(0\). Then_

* _If_ \(q_{r}\neq 1\) _or_ \(q_{w}\neq 1\) _or_ \(b\neq 1\)_, then_ (_P_1_) _does not hold._
* _If_ \(q_{r}=q_{w}=b=1\)_, then for any_ \(\mu\in\mathcal{P}_{1}(\Omega)\)_, there exists_ \(\lambda_{0}>0\) _such that_ \(F_{\lambda,b}+\beta^{-1}H\) _does not satisfy local LSI at_ \(\mu\) _for any_ \(\lambda<\lambda_{0}\) _(in particular (_P_2_) does not hold unless_ \(\lambda\) _is large enough)._

When \(q_{r}=q_{w}=b=1\) and \(\lambda\) is large enough, then it can indeed be shown that Thm. 2.1 applies under natural conditions, see for instance [14, Sec. 5.1].

_Remark 3.1_.: For functionals of the form \(G_{\lambda,s}=G(\nu)+\frac{\lambda}{s}\left\|\nu\right\|_{TV}^{s}\), instead of (1.1) which corresponds to \(s=2\), one can formulate a similar reduction by posing \(\Psi_{b,s}(\mu)=(\int_{\Omega}\left|r\right|^{b}\mathrm{d}\mu(r,w))^{s/b}\) and \(F_{\lambda,b,s}(\mu)=G(\bm{h}\mu)+\frac{\lambda}{s}\Psi_{b,s}(\mu)\). The statements of Prop. 3.1 and Prop. 3.2 hold true with \(G_{\lambda}\) replaced by \(G_{\lambda,s}\), and \(F_{\lambda,b}\) by \(F_{\lambda,b,s}\), for any \(1\leq b\leq s\), as can be shown by very simple adaptations of the proofs (only the second inequality in the proof of Lem. C.1, and the definition of \(\lambda^{\prime}\) in (C.2), need to be adapted). Note that the problem considered in [14] is of the form \(G(\nu)+\lambda\left\|\nu\right\|_{TV}\), and they analyzed Wasserstein gradient flow on \(F_{\lambda,1,1}\) with \(q_{r}=q_{w}=1\) (in particular the issues caused by the disconnectedness of \(\Omega^{*}\) are bypassed thanks to the choice \(b=1\)). The above discussion shows that applying MFLD to that problem would only yield convergence guarantees for \(\lambda\) large enough.

### Reduction by bilevel optimization

We define the bilevel objective functional \(J_{\lambda}\) for \(\eta\in\mathcal{P}(\mathcal{W})\) as6

Footnote 6: We use \(\int_{\mathcal{W}}\frac{\left|\nu\right|^{2}}{\eta}\) as a shorthand for \(\int_{\mathcal{W}}\big{(}\frac{\mathrm{d}\nu}{\mathrm{d}\eta}(w)\big{)}^{2} \mathrm{d}\eta(w)\).

\[J_{\lambda}(\eta)\coloneqq\inf_{\nu\in\mathcal{M}(\mathcal{W})}G(\nu)+\frac{ \lambda}{2}\int_{\mathcal{W}}\frac{\left|\nu\right|^{2}}{\eta}.\] (3.3)

It can be derived using the variational representation of the squared TV-norm [1, 1]: for any \(\nu\in\mathcal{M}(\Omega)\), one has \(\left\|\nu\right\|_{TV}^{2}=\min_{\eta\in\mathcal{P}(\mathcal{W})}\int_{ \mathcal{W}}\frac{\left|\nu\right|^{2}}{\eta}\). By exchanging infima, it thus holds \(\inf_{\nu\in\mathcal{M}(\mathcal{W})}G_{\lambda}(\nu)=\inf_{\eta\in\mathcal{P} (\mathcal{W}),\nu\in\mathcal{M}(\mathcal{W})}G(\nu)+\frac{\lambda}{2}\int \frac{\left|\nu\right|^{2}}{\eta}=\inf_{\eta\in\mathcal{P}(\mathcal{W})}J_{ \lambda}(\eta)\). Moreover, the objective minimized in (3.3) is jointly convex in \((\eta,\nu)\) and partial minimization preserves convexity, so \(J_{\lambda}\) is convex. Let us gather these crucial remarks in a formal statement.

**Proposition 3.3**.: _The bilevel objective \(J_{\lambda}\) is convex and \(\inf_{\mathcal{P}(\mathcal{W})}J_{\lambda}=\inf_{\mathcal{M}(\mathcal{W})}G_ {\lambda}\). Moreover, if \(G_{\lambda}\) admits a minimizer \(\nu\in\mathcal{M}(\mathcal{W})\), then \(\arg\min J_{\lambda}=\Big{\{}\frac{\left|\nu\right|}{\left\|\nu\right\|_{TV} },\nu\in\arg\min G_{\lambda}\Big{\}}\)._

Link between the lifted and bilevel reductions.The equality case in the statement of Prop. 3.1 shows that we can restrict the lifted reduction to measures \(\mu\in\mathcal{P}_{b}(\Omega)\) of the form \(\mu(\mathrm{d}r,\mathrm{d}w)=\delta_{f(w)}(\mathrm{d}r)\eta(\mathrm{d}w)\) for some \(f:\mathcal{W}\to\mathbb{R}\) and \(\eta\in\mathcal{P}(\mathcal{W})\). Since they satisfy \(\bm{h}\mu(\mathrm{d}w)=f(w)\eta(\mathrm{d}w)\), the lifted reduction with \(b=2\) thus rewrites

\[\min_{\eta\in\mathcal{P}(\mathcal{W})}\min_{f\in L^{2}(\eta)}G(f\eta)+\frac{ \lambda}{2}\int_{\mathcal{W}}f(w)^{2}\mathrm{d}\eta(w).\]

After the change of variable \((\nu,\eta)=(f\eta,\eta)\), the outer objective is precisely \(J_{\lambda}(\eta)\). Thus, Wasserstein gradient flow on \(J_{\lambda}\) can be seen as a two-timescale optimization dynamics: it is the Wasserstein gradient flow on \(F_{\lambda,2}\) in the limit where \(\Gamma\to\infty\). In the context of 2NN training with the parametrization (i), this amounts to training the output layer infinitely faster than the input layer, as done in [1, 2, 3, 1]. This remark allows to implement the bilevel MFLD numerically by discretizing in time the system of SDEs, for fixed large \(N\) and \(\Gamma\),

\[\forall i\leq N, \mathrm{d}r_{t}^{i}=-\Gamma\ \nabla_{r^{i}}F_{\lambda,2}^{\prime}[\mu_{t}](r_{t}^{i},w_{t}^{i}) \mathrm{d}t =-\Gamma\ \big{(}G^{\prime}[\nu_{t}](w_{t}^{i})+\lambda r_{t}^{i}\big{)} \,\mathrm{d}t\] (3.4) \[\mathrm{d}w_{t}^{i}=-\nabla_{w^{i}}F_{\lambda,2}^{\prime}[\mu_{t} ](r_{t}^{i},w_{t}^{i})\mathrm{d}t+\sqrt{2\beta^{-1}}\mathrm{d}B_{t}^{i} =-r_{t}^{i}\nabla G^{\prime}[\nu_{t}](w_{t}^{i})\mathrm{d}t+\sqrt{2\beta^{-1 }}\mathrm{d}B_{t}^{i}\]where \(\mu_{t}=\frac{1}{N}\sum_{i=1}^{N}\delta_{\langle r^{i}_{t},w^{i}_{t}\rangle}\) and \(\nu_{t}=\frac{1}{N}\sum_{i=1}^{N}r^{i}_{\delta}\delta_{w^{i}_{t}}\), and taking \(\eta_{t}=\frac{1}{N}\sum_{i=1}^{n}\delta_{w^{i}_{t}}\). Notice the absence of noise term on the weight variables \(r\); it reflects the fact that MFLD for the bilevel objective is _not_ a limit case of MFLD for the lifted objective, as the noise would prevent to reach optimality in the inner problem.

Compability with MFLD.We now show that, in contrast to the lifting reduction, the bilevel reduction is amenable to MFLD. The main assumption on (1.1) is as follows.

**Assumption 1**.: \(G:\mathcal{M}(\mathcal{W})\to\mathbb{R}\) is non-negative and admits second variations, and for each \(i\in\{0,1,2\}\), there exist \(L_{i},B_{i}<\infty\) such that \(\left\|\nabla^{i}G^{\prime\prime}[\nu](w,w^{\prime})\right\|_{w}\leq L_{i}\) and \(\left\|\nabla^{i}G^{\prime}[\nu]\right\|_{w}\leq L_{i}\left\|\nu\right\|_{TV} +B_{i}\) for all \(\nu\in\mathcal{M}(\mathcal{W})\) and \(w,w^{\prime}\in\mathcal{W}\). Moreover there exists \(\widetilde{L}_{2}<\infty\) such that \(\left\|\nabla_{w}\nabla_{w^{\prime}}G^{\prime\prime}[\nu](w,w^{\prime}) \right\|\leq\widetilde{L}_{2}\) for all \(\nu,w,w^{\prime}\). Furthermore, \(\mathcal{W}\) is compact and the uniform probability measure \(\tau\) on \(\mathcal{W}\) satisfies LSI with constant \(\alpha_{\tau}\).

Concrete settings that satisfy Assumption 1 are discussed in Sec. 5. The following proposition confirms the compatibility with MFLD and gives quantitative bounds on the LSI constant.

**Proposition 3.4**.: _Under Assumption 1, \(J_{\lambda}\) satisfies (P0), (P1) and (P2). More precisely, for any \(\eta\in\mathcal{P}(\mathcal{W})\), \(J_{\lambda}+\beta^{-1}H\) satisfies local LSI at \(\eta\) with the constant \(\alpha_{\tilde{\eta}}=\alpha_{\tau}\exp\left(-\frac{1}{\lambda}L_{0}\beta J_{ \lambda}(\eta)\right)\). Further, \(J_{\lambda}+\beta^{-1}H\) satisfies \(\alpha\)-LSI uniformly along the MFLD trajectory \((\eta_{t})_{t}\) with the constant \(\alpha=\alpha_{\tau}\exp\left(-\frac{1}{\lambda}L_{0}\beta\min\left\{G(0),J_ {\lambda}(\eta_{0})+\beta^{-1}H\left(\eta_{0}|\tau\right)\right\}\right)\)._

In view of the negative result of Prop. 3.2 for the lifting reduction, and the positive result of Prop. 3.4 for the bilevel reduction, in the sequel we focus on MFLD applied on \(J_{\lambda}\), which we will refer to as MFLD-Bilevel.

## 4 Global convergence and annealing for MFLD-Bilevel

While the bounds from Prop. 3.4 along with Thm. 2.1 allow to establish global convergence to minimizers of \(J_{\lambda}+\beta^{-1}H\), our aim is to minimize the unregularized bilevel objective \(J_{\lambda}\). This can be achieved by annealing the temperature parameter \(\beta^{-1}\) along the dynamics. Namely, Theorem 4.1 of [10] guarantees that by choosing \(\beta_{t}=c\log(t)\) for an appropriate constant \(c\), the annealed MFLD trajectory

\[\partial_{t}\eta_{t}=\operatorname{div}(\eta_{t}\nabla J_{\lambda}^{*}[\eta_{ t}])+\beta_{t}^{-1}\Delta\eta_{t}\]

satisfies \(J_{\lambda}(\eta_{t})-\inf J_{\lambda}=O\left(\frac{\log\log t}{\log t}\right)\). This is a very slow rate however.

In this section, we show that the structure of \(J_{\lambda}\) originating from the bilevel reduction can be exploited to go beyond the generic guarantees from [10, Thm. 4.1]. Namely, we study in detail an alternative temperature annealing strategy, and we show that it improves upon the classical one \(\beta_{t}\sim\log(t)\) in terms of convergence to a fixed multiplicative accuracy.

### Faster convergence to a fixed multiplicative accuracy

**Definition 4.1**.: Suppose \(0\not\in\arg\min G\), so that \(J_{\lambda}^{*}\coloneqq\inf J_{\lambda}>0\). We will say that MFLD-Bilevel with a given temperature annealing schedule \((\beta_{t})_{\geq 0}\)_converges to \((1+\Delta)\)-multiplicative accuracy in time-complexity \(T_{\Delta}\)_, for a fixed positive constant \(\Delta\) (say \(\Delta=0.01\)), if \(J_{\lambda}(\eta_{T_{\Delta}})\leq(1+\Delta)J_{\lambda}^{*}\).

Note that in machine learning settings where the problem (1.1) corresponds to learning with overparameterized models, it is realistic to assume \(J_{\lambda}^{*}\) to be small (as long as the regularization \(\lambda\) is small), and \(T_{\Delta}\) is the time it takes for the annealed MFLD to achieve a suboptimality of at most \(\Delta J_{\lambda}^{*}\).

For ease of comparison, let us report the time-complexity \(T_{\Delta}\) that can be achieved by simply running MFLD-Bilevel with a constant but well-chosen \(\beta\), based on the bounds from Prop. 3.4 and Thm. 2.1.

**Proposition 4.1** (Baseline "annealing" schedule: constant \(\beta_{t}\)).: _Under Assumption 1, let \(\Delta>0\) and assume that \(\Delta\leq\frac{L_{0}L_{1}G(0)}{\lambda^{2}J_{\lambda}^{*}}\). Then, MFLD-Bilevel with the temperature schedule \(\forall t,\beta_{t}=\frac{4d}{\Delta J_{\lambda}^{*}}\log\left(\frac{CB}{ \Delta J_{\lambda}^{*}}\right)\) converges to \((1+\Delta)\)-multiplicative accuracy in time_

\[T_{\Delta}\leq\frac{C^{\prime}}{\Delta J_{\lambda}^{*}}\log\left(\frac{CB}{ \Delta J_{\lambda}^{*}}\right)\cdot\exp\left(\frac{C^{\prime}L_{0}G(0)}{ \lambda\;\Delta J_{\lambda}^{*}}\log\left(\frac{CB}{\Delta J_{\lambda}^{*}} \right)\right)\cdot\log\left(\frac{2G(0)}{\Delta J_{\lambda}^{*}}+C^{\prime}H \left(\eta_{0}|\tau\right)\right)\]

_where \(B=\operatorname{poly}(L_{0},L_{1},B_{1},G(0),\lambda^{-1})\) and \(C,C^{\prime}\) are constants dependent on \(\mathcal{W}\) (and \(d\) and \(\alpha_{\tau}\))._For the annealing schedule \(\beta_{t}\sim\log(t)\), the time-complexity \(T_{\Delta}\) that can be guaranteed from inspecting the proof of [16, Thm. 4.1] has the same dependency on \(d,\lambda\) and \(J_{\lambda}^{*}\) as for the baseline \(\bar{\beta}_{t}=\mathrm{cst}\).

Improved annealing schedule.Recall the result of Prop. 3.4: for any \(\beta>0\), \(J_{\lambda}+\beta^{-1}H\) satisfies local \(\alpha_{\bar{\eta}}\)-LSI at \(\eta\) with \(\alpha_{\bar{\eta}}=\alpha_{\tau}\exp(-\frac{L_{0}}{\lambda}\beta J_{\lambda}( \eta))\). Informally, if we manage to control \(J_{\lambda}(\eta_{t})\) along the annealed MFLD trajectory and show that it decreases, then we can increase \(\beta_{t}\) at the same rate, while retaining the same local LSI constant. This observation and the resulting annealing procedure were introduced in [23], in a 2NN classification setting with the logistic loss. There the optimal value of the loss functional, corresponding to our \(J_{\lambda}^{*}\), is \(0\), and the annealing procedure yields favorable rates for global convergence. Here we show that this procedure is also applicable for MFLD-Bilevel, as soon as \(G\) satisfies the mild Assumption 1, yielding favorable rates for convergence to a fixed multiplicative accuracy.7

Footnote 7: In fact, the annealing procedure of Thm. 4.2 would also yield a rate of convergence for any \(\mathcal{J}:\mathcal{P}(\mathcal{W})\to\mathbb{R}\) with \(\mathcal{J}^{\prime\prime}[\eta](w,w^{\prime})\) uniformly bounded and \(\inf\mathcal{J}>0\), instead of \(J_{\lambda}\); but the resulting bound on \(T_{\Delta}\) would have an additional factor of \((\inf\mathcal{J})^{1/2}\) inside the exponential. See Sec. E.2 for a detailed discussion.

**Theorem 4.2**.: _Under Assumption 1, there exist constants \(B=\mathrm{poly}(L_{i},B_{i},G(0),\lambda^{-1})\) and \(C_{i}\) dependent only on \(G(0)\), \(H(\eta_{0})\), \(\mathcal{W}\) (and \(d\) and \(\alpha_{\tau}\)) such that the following holds. For any \(\Delta\leq\frac{B}{J_{\lambda}^{*}}\),_

_MFLD-Bilevel with the temperature schedule \((\beta_{t})_{t\geq 0}\) defined by \(\forall k\leq K,\forall t\in[t_{k},t_{k+1}],\beta_{t}=2^{k}d\) where \(t_{0}=0\) and \(K=\lceil 2\log_{2}(B/(\Delta J_{\lambda}^{*}))\rceil\) and_

\[t_{k+1}-t_{k}=C_{1}2^{k}\ k\cdot\exp\left(\frac{L_{0}d}{\lambda}\left(\frac{C _{3}}{\Delta}\log\left(\frac{B}{\Delta J_{\lambda}^{*}}\right)+C_{2}\right) \right),\]

_achieves \((1+\Delta)\)-multiplicative accuracy, with time-complexity_

\[T_{\Delta}\leq t_{K+1}\leq\frac{C_{4}}{\Delta J_{\lambda}^{*}}\log\left(\frac {B}{\Delta J_{\lambda}^{*}}\right)^{2}\cdot\exp\left(\frac{L_{0}d}{\lambda} \left(\frac{C_{3}}{\Delta}\log\left(\frac{B}{\Delta J_{\lambda}^{*}}\right)+C_ {2}\right)\right).\]

Note that assuming that \(G\) admits a minimizer \(\nu_{0}\) and that \(\min G=0\), as is typically the case in over-parametrized machine learning settings, then by the envelope theorem \(J_{\lambda}^{*}=\inf\left(G+\frac{\lambda}{2}\left\lVert\cdot\right\rVert_{TV }^{2}\right)=\frac{\left\lVert\nu_{0}\right\rVert_{TV}^{2}}{2^{2}}\lambda+o(\lambda)\). So in the regime of small \(\lambda\), ignoring the subexponential factors, the time complexity bound achieved by the annealing schedule of Thm. 4.2 scales as \(\exp\left(c\lambda^{-1}\log\lambda^{-1}\right)\) for a constant \(c\). This improves upon the time complexity bound of the classical annealing procedure \(\beta_{t}\sim\log(t)\) (the same as in Prop. 4.1), which scales as \(\exp(c^{\prime}\lambda^{-2})\).

## 5 Local LSI constant at optimality for learning a single neuron

Devising temperature annealing schemes for global convergence, as illustrated in the previous section, relies on bounds on the local LSI constant at every iterate \(\eta_{t}\) of the (annealed) MFLD. Such bounds are readily provided by the widely applicable Holley-Stroock perturbation argument, on which for example our Prop. 3.4 is based, but may be overly pessimistic. Indeed in this section, we demonstrate that for MFLD-Bilevel, _the LSI constant at convergence can be independent of \(\beta\), \(\lambda\) and \(d\), instead of exponential in \(\beta\)_ as a global analysis would suggest.

More precisely, we are interested in \(\alpha^{*}\), the best local LSI constant of \(J_{\lambda,\beta}\coloneqq J_{\lambda}+\beta^{-1}H\left(\cdot|\tau\right)\), at \(\eta_{\lambda,\beta}\coloneqq\arg\min J_{\lambda,\beta}\). In fact the proximal Gibbs measure of the optimum is the optimum itself: \(\overline{\eta_{\lambda,\beta}}=\eta_{\lambda,\beta}\), so \(\alpha^{*}\) is precisely the LSI constant of \(\eta_{\lambda,\beta}\). A bound on \(\alpha^{*}\) is of interest, especially in the regime of large \(\beta\) (low entropic regularization), for two reasons. Firstly, it directly implies a local convergence bound on MFLD-Bilevel, as shown in the proposition below. Secondly, characterizing the dependency of \(\alpha^{*}\) on \(\beta\) may open the way to more efficient temperature annealing strategies; but this is out of the scope of this paper.

**Proposition 5.1**.: _Under Assumption 1, suppose \(\eta_{\lambda,\beta}\) satisfies LSI with some constant \(\alpha_{\beta}^{*}\). For any \(\varepsilon>0\), there exists a sublevel set of \(J_{\lambda,\beta}\) such that, for any initialization \(\eta_{0}\) in this sublevel set, \(J_{\lambda,\beta}(\eta_{t})-\inf J_{\lambda,\beta}\leq(J_{\lambda,\beta}(\eta_ {0})-\inf J_{\lambda,\beta})\,e^{-\left(\alpha_{\beta}^{*}\beta^{-1}- \varepsilon\right)t}\)._For the local LSI analysis, we focus on a specific setting of (1.1), namely, least-squares regression using a 2NN with a normalization constraint on the first-layer weights, and a single-neuron teacher network. See Fig. 1 for an illustrative numerical experiment. Note that Assumption 2, with additional bounded-moment assumptions on \(\varphi\) and \(\rho\), is a special case of Assumption 1, as shown in Prop. F.4.

**Assumption 2**.: \(\mathcal{W}=\mathbb{S}^{d}\) is the Euclidean sphere in \(\mathbb{R}^{d+1}\) and there exist \(\rho\) a covariate distribution over \(\mathbb{R}^{d+1}\), \(y\in L_{\rho}^{2}(\mathbb{R}^{d+1})\) a fixed target function, and \(\varphi:\mathbb{R}\to\mathbb{R}\) a \(\mathcal{C}^{2}\) activation function such that \(G(\nu)=\frac{1}{2}\mathbb{E}_{x\sim\rho}\left|\hat{y}_{\nu}(x)-y(x)\right|^{2}\) where \(\hat{y}_{\nu}(x)=\int_{\mathcal{W}}\varphi(\langle w,x\rangle)\mathrm{d}\nu(w)\).

Under the above assumption, we show in Prop. F.1 a simplified expression for the bilevel objective and its first variation,

\[J_{\lambda}(\eta)=\frac{\lambda}{2}\langle y,(K_{\eta}+\lambda\,\mathrm{id})^ {-1}y\rangle_{L_{\rho}^{2}},\qquad J_{\lambda}^{\prime}[\eta](w)=-\frac{ \lambda}{2}\langle\varphi(\langle w,\cdot\rangle),(K_{\eta}+\lambda\, \mathrm{id})^{-1}y\rangle_{L_{\rho}^{2}}^{2},\]

where \(K_{\eta}\) is the integral operator in \(L_{\rho}^{2}\) of the kernel \(k_{\eta}(x,x^{\prime})=\int\varphi(\langle w,x\rangle)\varphi(\langle w,x^{ \prime}\rangle)\mathrm{d}\eta(w)\) and \(\mathrm{id}\) is the identity operator on \(L_{\rho}^{2}\). Additionally, we make the following assumption on the data distribution \(\rho\) and on the response \(y\).

**Assumption 3**.: \(\rho\) is rotationally invariant and the labels come from a single-index model: \(y=\varphi(\langle v,x\rangle)\) for some fixed \(v\in\mathcal{W}\).

With the above assumptions, we can state the main theorem of this section.

**Theorem 5.2**.: _Under Assumptions 2 and 3, there exists a function \(g:[-1,+1]\to\mathbb{R}_{+}\) such that \(J_{\lambda}^{\prime}[\delta_{v}](w)=-\lambda g(\langle w,v\rangle)\) for any \(w\in\mathbb{S}^{d}\). Suppose that \(\lambda\leq 1\) and that there exist constants \(c_{i},C_{i}>0\) such that for all \(r\in[-1,+1]\),_

\[c_{1}\leq g^{\prime}(r)\leq C_{1},\quad g^{\prime\prime}(r)\geq-C_{2},\quad \left|g^{\prime\prime}(r)(1-r^{2})^{1/2}\right|\leq C_{3},\quad\left|g^{\prime \prime\prime}(r)(1-r^{2})^{3/2}\right|\leq C_{4}.\]

_Then there exist constants \(\alpha_{v}\), \(D_{0}\) (dependent only on the \(c_{i},C_{i}\)) such that for any \(\beta\geq D_{0}d\lambda^{-1}\), \(\widehat{\delta_{v}}\propto e^{-\beta J_{\lambda}^{\prime}[\delta_{v}]}\tau\) satisfies \(\alpha_{v}\)-LSI. Furthermore, if additionally \(\frac{1}{d^{2}}\mathbb{E}_{x\sim\rho}\left\|x\right\|^{4},\left\|\varphi^{(i) }\right\|_{L^{4}(\rho)}<\infty\) for \(i\in\{0,1,2\}\) where \(\left\|\varphi\right\|_{L^{p}(\rho)}^{p}\coloneqq\int|\varphi(\langle w,x \rangle)|^{p}\mathrm{d}\rho(x)\) (independent of \(w\) as \(\rho\) is rotationally invariant), then there exists a constant \(\alpha^{*}\) dependent only on those constants and on the \(c_{i},C_{i}\) such that, provided that \(\beta\geq\mathrm{poly}(d,\lambda^{-1})\), \(\eta_{\lambda,\beta}\) satisfies \(\alpha^{*}\)-LSI._

The proof is based on the observation that \(\eta_{\lambda,\beta}\approx\arg\min J_{\lambda}=\delta_{v}\) the Dirac measure at \(v\), for certain regimes of \(\beta\) and \(\lambda\), in the Wasserstein metric. Thus we show that \(J_{\lambda}^{\prime}[\delta_{v}]\) is amenable to a Lyapunov type argument inspired from [10, 11], and then transfer its properties to \(J_{\lambda}^{\prime}[\eta_{\lambda,\beta}]\).

Figure 1: The regularized training loss \(G_{\lambda}(\nu)\) (1.1) of a 2NN with the ReLU activation, learning a teacher 2NN with the 4th degree Hermite polynomial as its activation. In both plots, \(d=10\) and \(\lambda=\beta^{-1}=10^{-3}\). The implementation details are provided in Sec. F.4. Plots are averaged over 5 experiments. \(G_{\lambda}^{*}\) is the best value achieved at each experiment. In Fig. ((b)), “Conic” refers to using the metric (3.2) with \(q_{r}=1,q_{w}=1\), while “Canonical” refers to the choice of \(q_{r}=2,q_{w}=0\).

We now verify the assumptions of Thm. 5.2 for a class of smooth, non-negative, and monotone activations which includes some popular practical choices such as the Softplus \(\varphi(z)=\ln(1+e^{z})\) and sigmoid \(\varphi(z)=1/(1+e^{-z})\). While we only consider smooth activations here for simplicity, certain non-smooth activations such as a leaky version of ReLU can also satisfy the conditions of Thm. 5.2.

**Proposition 5.3**.: _Suppose Assumptions 2 and 3 hold, and \(b_{1}(d+1)\leq\mathbb{E}[\left\lVert x\right\rVert^{2}]\leq\mathbb{E}[\left\lVert x \right\rVert^{12}]^{1/6}\leq b_{2}(d+1)\) for constants \(b_{1},b_{2}>0\). Let \(m\coloneqq 2b_{2}^{3/2}/b_{1}\). Suppose \(\varphi\) and \(\varphi^{\prime}\) are non-negative, \(\inf_{|z|\leq m}\varphi(z)\wedge\varphi^{\prime}(z)>0\) and \(\left\lVert\varphi^{(i)}\right\rVert_{L^{4}(\rho)}<\infty\) for \(i\leq 3\). Then, \(\varphi\) satisfies the assumptions of Thm. 5.2 with constants that only depend on \(b_{1}\), \(b_{2}\), and \(\varphi\)._

## 6 Conclusion

In this paper, we investigated how mean-field Langevin dynamics (MFLD), an optimization dynamics over probability measures with global convergence guarantees, can be leveraged to solve convex optimization problems over signed measures of the form (1.1). For a large class of objectives \(G\), we highlighted that MFLD with a lifting approach necessarily runs into some issues, whereas the bilevel approach always inherits the guarantees of MFLD, leading to convergence guarantees for \(G_{\lambda}\) via annealing. Finally, turning to a 2-layer NN learning task which can be stated as an instance of (1.1), we showed that the local LSI constant of MFLD-Bilevel can scale much more favorably with \(d\) and \(\beta\) than a generic analysis would suggest.

Another approach to tackle (1.1) could be to build noisy particle dynamics directly in the space of signed measures, complementing the MFLD updates with, for instance, a birth-death process. A challenge then is to build such dynamics that can be efficiently discretized. It is also an interesting question for future works to find other settings to which MFLD can be extended, beyond signed measures.

## References

* [AH12] Kendall Atkinson and Weimin Han. _Spherical harmonics and approximations on the unit sphere: an introduction_. Vol. 2044. Springer Science & Business Media, 2012.
* [Bac17] Francis Bach. "Breaking the curse of dimensionality with convex neural networks". In: _The Journal of Machine Learning Research_ 18.1 (2017), pp. 629-681.
* [Bac19] Francis Bach. "The "\(\eta\)-trick" reloaded: multiple kernel learning". In: (2019).
* [Bac21] Francis Bach. "The quest for adaptivity". In: (2021).
* [BGL14] Dominique Bakry, Ivan Gentil, and Michel Ledoux. _Analysis and geometry of Markov diffusion operators_. Vol. 103. Springer, 2014.
* [BRVDM05] Yoshua Bengio, Nicolas Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. "Convex neural networks". In: _Advances in neural information processing systems_ 18 (2005).
* [BMZ23] Raphael Berthier, Andrea Montanari, and Kangjie Zhou. "Learning time-scales in two-layers neural networks". In: _arXiv preprint arXiv:2303.00055_ (2023).
* [BBP23] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. "On learning gaussian multi-index models with gradient flow". In: _arXiv preprint arXiv:2310.19793_ (2023).
* [Bou23] Nicolas Boumal. _An introduction to optimization on smooth manifolds_. Cambridge University Press, 2023.
* [BBI01] Dmitri Burago, Yuri Burago, and Sergei Ivanov. _A course in metric geometry_. Vol. 33. American Mathematical Society Providence, 2001.
* [CD13] Rene Carmona and Francois Delarue. "Probabilistic analysis of mean-field games". In: _SIAM Journal on Control and Optimization_ 51.4 (2013), pp. 2705-2734.
* [CVBM02] Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, and Sayan Mukherjee. "Choosing multiple parameters for support vector machines". In: _Machine learning_ 46 (2002), pp. 131-159.
* [CLRW24] Fan Chen, Yiqing Lin, Zhenjie Ren, and Songbo Wang. "Uniform-in-time propagation of chaos for kinetic mean field Langevin dynamics". In: _Electronic Journal of Probability_ 29 (2024), pp. 1-43.

* [CRW22] Fan Chen, Zhenjie Ren, and Songbo Wang. "Uniform-in-time propagation of chaos for mean field langevin dynamics". In: _arXiv preprint arXiv:2212.03050_ (2022).
* [Chi17] Lenaic Chizat. "Unbalanced optimal transport: Models, numerical methods, applications". PhD thesis. Universite Paris sciences et lettres, 2017.
* [Chi22a] Lenaic Chizat. "Convergence rates of gradient methods for convex optimization in the space of measures". In: _Open Journal of Mathematical Optimization_ 3 (2022), pp. 1-19.
* [Chi22b] Lenaic Chizat. "Mean-Field Langevin Dynamics: Exponential Convergence and Annealing". In: _Transactions on Machine Learning Research_ (2022).
* [Chi22c] Lenaic Chizat. "Sparse optimization on measures with over-parameterized gradient descent". In: _Mathematical Programming_ 194.1-2 (2022), pp. 487-532.
* [CB18] Lenaic Chizat and Francis Bach. "On the global convergence of gradient descent for over-parameterized models using optimal transport". In: _Advances in neural information processing systems_ 31 (2018).
* [CB20] Lenaic Chizat and Francis Bach. "Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss". In: _Conference on Learning Theory_. PMLR. 2020, pp. 1305-1338.
* [DG12] Yohann De Castro and Fabrice Gamboa. "Exact reconstruction using Beurling minimal extrapolation". In: _Journal of Mathematical Analysis and applications_ 395.1 (2012), pp. 336-354.
* [DDPS19] Quentin Denoyelle, Vincent Duval, Gabriel Peyre, and Emmanuel Soubies. "The sliding Frank-Wolfe algorithm and its application to super-resolution microscopy". In: _Inverse Problems_ 36.1 (2019), p. 014001.
* [FE12] Christopher Frye and Costas J Efthimiou. "Spherical harmonics in p dimensions". In: _arXiv preprint arXiv:1205.3548_ (2012).
* [FW23] Qiang Fu and Ashia Wilson. "Mean-field Underdamped Langevin Dynamics and its Space-Time Discretization". In: _arXiv preprint arXiv:2312.16360_ (2023).
* [GCM23] Sebastien Gadat, Yohann de Castro, and Clement Marteau. "FastPart: Over-Parameterized Stochastic Gradient Descent for Sparse optimisation on Measures". In: (2023).
* [GV79] A. Gray and L. Vanhecke. "Riemannian geometry as determined by the volumes of small geodesic balls". In: _Acta Mathematica_ 142 (1979), pp. 157-198.
* [GGGM21] Charles Guille-Escuret, Manuela Giroti, Baptiste Goujaud, and Ioannis Mitliagkas. "A study of condition numbers for first-order optimization". In: _International Conference on Artificial Intelligence and Statistics_. PMLR. 2021, pp. 1261-1269.
* [HS86] Richard Holley and Daniel W Stroock. "Logarithmic Sobolev inequalities and stochastic Ising models". In: (1986).
* [HRSS21] Kaitong Hu, Zhenjie Ren, David Siska, and Lukasz Szpruch. "Mean-field Langevin dynamics and energy landscape of neural networks". In: _Annales de l'Institut Henri Poincare (B) Probabilites et statistiques_. Vol. 57. 4. Institut Henri Poincare. 2021, pp. 2043-2065.
* [KZCE+24] Yunbum Kook, Matthew S Zhang, Sinho Chewi, Murat A Erdogdu, et al. "Sampling from the Mean-Field Stationary Distribution". In: _arXiv preprint arXiv:2402.07355_ (2024).
* [Lac18] Daniel Lacker. "Mean field games and interacting particle systems". In: _preprint_ (2018).
* [LCBGJ04] Gert RG Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui, and Michael I Jordan. "Learning the kernel matrix with semidefinite programming". In: _Journal of Machine Learning Research_ 5.Jan (2004), pp. 27-72.
* [Lee18] John M Lee. _Introduction to Riemannian manifolds_. Vol. 2. Springer, 2018.
* [LE23] Mufan Li and Murat A Erdogdu. "Riemannian langevin algorithm for solving semidefinite programs". In: _Bernoulli_ 29.4 (2023), pp. 3093-3113.
* [LMZ20] Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. "Learning over-parametrized two-layer neural networks beyond NTK". In: _Conference on learning theory_. PMLR. 2020, pp. 2613-2682.

* [LMS18] Matthias Liero, Alexander Mielke, and Giuseppe Savare. "Optimal entropy-transport problems and a new Hellinger-Kantorovich distance between positive measures". In: _Inventiones mathematicae_ 211.3 (2018), pp. 969-1117.
* [MB23] Pierre Marion and Raphael Berthier. "Leveraging the two-timescale regime to demonstrate convergence of neural networks". In: _Advances in Neural Information Processing Systems_ 36 (2023).
* [MMN18] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. "A mean field view of the landscape of two-layer neural networks". In: _Proceedings of the National Academy of Sciences_ 115.33 (2018), E7665-E7671.
* [MS14] Georg Menz and Andre Schlichting. "Poincare and logarithmic Sobolev inequalities by decomposition of the energy landscape". In: _The Annals of Probability_ 42.5 (2014), pp. 1809-1884.
* [MZ04] Ilya Molchanov and Sergei Zuyev. "Optimisation in space of measures and optimal design". In: _ESAIM: Probability and Statistics_ 8 (2004), pp. 12-24.
* [NS17] Atsushi Nitanda and Taiji Suzuki. "Stochastic particle gradient descent for infinite ensembles". In: _arXiv preprint arXiv:1712.05438_ (2017).
* [NWS22] Atsushi Nitanda, Denny Wu, and Taiji Suzuki. "Convex analysis of the mean field Langevin dynamics". In: _International Conference on Artificial Intelligence and Statistics_. PMLR. 2022, pp. 9741-9757.
* [OV00] Felix Otto and Cedric Villani. "Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality". In: _Journal of Functional Analysis_ 173.2 (2000), pp. 361-400.
* [PP21] Clarice Poon and Gabriel Peyre. "Smooth bilevel programming for sparse regularization". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 1543-1555.
* [PP23] Clarice Poon and Gabriel Peyre. "Smooth over-parameterized solvers for non-smooth structured optimization". In: _Mathematical Programming_ (2023), pp. 1-56.
* [RBCG08] Alain Rakotomamonjy, Francis Bach, Stephane Canu, and Yves Grandvalet. "SimpleMKL". In: _Journal of Machine Learning Research_ 9 (2008), pp. 2491-2521.
* [RV22] Grant Rotskoff and Eric Vanden-Eijnden. "Trainability and accuracy of artificial neural networks: An interacting particle system approach". In: _Communications on Pure and Applied Mathematics_ 75.9 (2022), pp. 1889-1935.
* [San15] Filippo Santambrogio. "Optimal transport for applied mathematicians". In: _Birkauser, NY_ 55.58-63 (2015), p. 94.
* [SS20] Justin Sirignano and Konstantinos Spiliopoulos. "Mean field analysis of neural networks: A central limit theorem". In: _Stochastic Processes and their Applications_ 130.3 (2020), pp. 1820-1852.
* [SWN23] Taiji Suzuki, Denny Wu, and Atsushi Nitanda. "Mean-field Langevin dynamics: Time-space discretization, stochastic gradient, and variance reduction". In: _Advances in Neural Information Processing Systems_ 36 (2023).
* [SWON23] Taiji Suzuki, Denny Wu, Kazusato Oko, and Atsushi Nitanda. "Feature learning via mean-field Langevin dynamics: classifying sparse parities and beyond". In: _Thirty-seventh Conference on Neural Information Processing Systems_. 2023.
* [Szn91] Alain-Sol Sznitman. "Topics in propagation of chaos". In: _Ecole d'ete de probabilites de Saint-Flour XIX--1989_ 1464 (1991), pp. 165-251.
* [TS24] Shokichi Takakura and Taiji Suzuki. _Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective_. 2024.
* [Ver18] Roman Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge University Press, 2018.
* [Vil09] Cedric Villani. _Optimal transport: old and new_. Vol. 338. Springer, 2009.
* [YWR23] Yuling Yan, Kaizheng Wang, and Philippe Rigollet. "Learning gaussian mixtures using the Wasserstein-Fisher-Rao gradient flow". In: _arXiv preprint arXiv:2301.01766_ (2023).

Details for Sec. 1 (introduction)

Using \(\left\lVert\cdot\right\rVert_{TV}^{2}\) vs. \(\left\lVert\cdot\right\rVert_{TV}\) as the regularization term

The optimization problems we consider in this paper are of the form (1.1), that is, for ease of reference,

\[\min_{\nu\in\mathcal{M}(\mathcal{W})}G_{\lambda}(\nu), G_{\lambda}(\nu)\coloneqq G(\nu)+\frac{\lambda}{2}\left\lVert\nu \right\rVert_{TV}^{2}.\]

Note the regularization term \(\frac{\lambda}{2}\left\lVert\nu\right\rVert_{TV}^{2}\). This is to be contrasted with the more usual form of optimization problems

\[\min_{\nu\in\mathcal{M}(\mathcal{W})}\widetilde{G}_{\tilde{\lambda}}(\nu), \widetilde{G}_{\tilde{\lambda}}(\nu)\coloneqq G(\nu)+\tilde{\lambda} \left\lVert\nu\right\rVert_{TV},\]

which uses \(\left\lVert\nu\right\rVert_{TV}\) as the regularization.

On the level of variational problems, these two classes of problems are equivalent, in the sense that

\[\{0\}\cup\bigcup_{\lambda\geq 0}\operatorname*{arg\,min}G_{\lambda}=\{0\}\cup \bigcup_{\tilde{\lambda}\geq 0}\operatorname*{arg\,min}\widetilde{G}_{\tilde{ \lambda}}\]

where "\(0\)" refers to the zero measure on \(\mathcal{W}\). Indeed, note that by convexity, the argmins are determined by the respective first-order optimality conditions, so that

\[\bigcup_{\lambda\geq 0}\operatorname*{arg\,min}G_{\lambda} =\left\{\nu\in\mathcal{M}(\mathcal{W});\;\forall w,G^{\prime}[ \nu](w)+\lambda\left\lVert\nu\right\rVert_{TV}\frac{\nu(\mathrm{d}w)}{\left \lvert\nu(\mathrm{d}w)\right\rvert}=0,\;\;\lambda\in\mathbb{R}_{+}\right\}\] \[\bigcup_{\tilde{\lambda}\geq 0}\operatorname*{arg\,min}\widetilde{G}_{ \tilde{\lambda}} =\left\{\nu\in\mathcal{M}(\mathcal{W});\;\forall w,G^{\prime}[ \nu](w)+\tilde{\lambda}\frac{\nu(\mathrm{d}w)}{\left\lvert\nu(\mathrm{d}w) \right\rvert}=0,\;\;\tilde{\lambda}\in\mathbb{R}_{+}\right\}.\]

To see that the set on the first line is contained in the second, let \(\nu\in\operatorname*{arg\,min}G_{\lambda}\), then \(\nu\) satisfies the first-order optimality condition for \(\widetilde{G}_{\tilde{\lambda}}\) with \(\tilde{\lambda}=\lambda\left\lVert\nu\right\rVert_{TV}\). Conversely, if \(\nu\in\operatorname*{arg\,min}\widetilde{G}_{\tilde{\lambda}}\) then either \(\nu=0\) or \(\nu\in\operatorname*{arg\,min}G_{\lambda}\) with \(\lambda=\frac{\tilde{\lambda}}{\left\lVert\nu\right\rVert_{TV}}\).

In terms of optimization convergence guarantees, when using the reduction by lifting, the problems with \(\left\lVert\cdot\right\rVert_{TV}\) vs. with \(\left\lVert\cdot\right\rVert_{TV}^{2}\) regularization give rise to similar analyses, as discussed in Rem. 3.1. However when using the reduction by bilevel optimization, it seems that only the problem with \(\left\lVert\cdot\right\rVert_{TV}^{2}\) regularization is amenable to a precise analysis. This is perhaps most apparent in our derivation of the simplified expression for the bilevel objective, Prop. D.2.

### Detailed comparison with Takakura and Suzuki [15]

In this subsection, we show that the learning dynamics considered by [15, Sec. 2, 3] is an instance of a variant of MFLD applied to the bilevel reduction of (1.1). We do this by recalling their setting (in the case of single-task learning for simplicity) in notations that are compatible with ours.

* For a set of first-layer weights \(w_{i}\in\mathcal{W}\coloneqq\mathbb{R}^{d}\) and second-layer weights \(a_{i}\in\mathbb{R}\) (for \(1\leq i\leq N\)), and an activation function \(\varphi:\mathbb{R}\to\mathbb{R}\), the associated 2NN is defined as \(x\mapsto\frac{1}{N}\sum_{i=1}^{N}a_{i}\varphi(w_{i}^{\top}x)\).
* For \(\mu\in\mathcal{P}(\mathbb{R}\times\mathcal{W})\), the associated infinite-width 2NN is \(x\mapsto\int_{\mathbb{R}\times\mathcal{W}}a\varphi(w^{\top}x)\mathrm{d}\mu(a,w)\). Note that in our notation of Sec. 3.1, this also writes \(x\mapsto\int_{\mathcal{W}}\varphi(w^{\top}x)\mathrm{d}[\boldsymbol{h}\mu](w)\).
* Consider a data distribution \(\rho(\mathrm{d}x,\mathrm{d}y)\in\mathcal{P}(\mathbb{R}_{x}^{d}\times\mathbb{R }_{y})\). We may define the Hilbert space of predictors \(\mathcal{H}=L_{\rho^{s}}^{2}(\mathbb{R}_{x}^{d})\), and the "single first-layer neuron predictor" mapping \(\phi:\mathcal{W}\to\mathcal{H}\) by \(\phi(w)(x)=\varphi(w^{\top}x)\). The predictor associated to an infinite-width 2NN parametrized by \(\mu\) is then \(\int_{\mathbb{R}\times\mathcal{W}}a\phi(w)\mathrm{d}\mu(a,w)\).

* Consider a loss function \(\ell(\dot{y},y):\mathbb{R}_{y}\times\mathbb{R}_{y}\to\mathbb{R}\), inducing a risk functional over predictors given by \(R(h)=\mathbb{E}_{(x,y)\sim\rho}[\ell(h(x),y)]\). We may define the unregularized risk functional over (infinite-width) 2NN weights by \[\mathcal{L}(\mu)=R\left(\int_{\mathbb{R}\times\mathcal{W}}a\phi(w)\mathrm{d} \mu(a,w)\right)=R\left(\int_{\mathcal{W}}\phi(w)\mathrm{d}[\bm{h}\mu](w)\right).\] Accordingly, let the operator \(\Phi:\mathcal{M}(\mathcal{W})\to\mathcal{H}\) such that \(\Phi\nu=\int_{\mathcal{W}}\phi(w)\mathrm{d}\nu(w)\), and \[G(\nu)=R(\Phi\nu)=R\left(\int_{\mathcal{W}}\phi(w)\mathrm{d}\nu(w)\right).\] Then the unregularized risk is \(\mathcal{L}(\mu)=G(\bm{h}\mu)\).
* The regularized risk functional considered in [15, Sec. 2.1] is \[\mathcal{F}(\mu) =R\left(\int_{\mathbb{R}\times\mathcal{W}}a\phi(w)\mathrm{d}\mu( a,w)\right)+\frac{\lambda}{2}\int_{\mathbb{R}\times\mathcal{W}}a^{2}\mathrm{d}\mu(a,w)+ \frac{1}{2\sigma^{2}}\int_{\mathbb{R}\times\mathcal{W}}\left\|w\right\|^{2} \mathrm{d}\mu(a,w)\] (A.1) (More precisely, "\(F(f,\eta)\)" in their notation corresponds to our \(\mathcal{F}\left(\delta_{f(w)}(\mathrm{d}a)\eta(\mathrm{d}w)\right)\), their "\(\vec{\lambda}_{a}\)" corresponds to our \(\lambda\), and their "\(\vec{\lambda}_{w}\)" corresponds to our \(1/\sigma^{2}\).) Note that, in our notation of Sec. 3.1, \[\mathcal{F}(\mu)=F_{\lambda,2}(\mu)+\frac{1}{2\sigma^{2}}\int_{\mathbb{R}\times \mathcal{W}}\left\|w\right\|^{2}\mathrm{d}\mu(a,w).\]
* The bilevel limiting functional, which is the main object of study of [15, Sec. 2.1], is then defined as the mapping \(\mathcal{G}:\mathcal{P}(\mathcal{W})\to\mathbb{R}\) such that \[\mathcal{G}(\eta) =\inf_{f:\mathcal{W}\to\mathbb{R}}\mathcal{F}\left(\delta_{f(w)}( \mathrm{d}r)\eta(\mathrm{d}w)\right),\quad\text{corresponding precisely to}\] \[\mathcal{G}(\eta) =J_{\lambda}(\eta)+\frac{1}{2\sigma^{2}}\int_{\mathcal{W}}\left\| w\right\|^{2}\mathrm{d}\eta(w)\] in our notation of Sec. 3.2 (see the paragraph "Link between the lifted and bilevel reductions"). Interestingly, the convexity of \(\mathcal{G}\) is almost immediate with our presentation, as it is expressed as a partial minimization of a convex function, whereas the proof of the convexity of \(\mathcal{G}\) in [15] is quite involved. They also introduce a functional "\(U\)" which corresponds precisely to our \(J_{\lambda}(\eta)\), and which is an important auxiliary object in their analysis.
* The learning dynamics studied from Section 2.3 onwards in [15] (except for the label noise procedure in Section 5), is precisely MFLD for \(\mathcal{G}(\eta)\): \[\partial_{t}\eta_{t} =\beta^{-1}\Delta\eta_{t}+\operatorname{div}\left(\eta_{t}\nabla \mathcal{G}^{\prime}[\eta_{t}]\right)\] \[=\beta^{-1}\Delta\eta_{t}+\operatorname{div}\left(\eta_{t}\left( \nabla J_{\lambda}^{\prime}[\eta_{t}]+\frac{1}{\sigma^{2}}w\right)\right)\] (A.2) (and their constant "\(\lambda\)" corresponds to our \(\beta^{-1}\)).

"MFL + confining" dynamics.The PDE (A.2) can be interpreted as a variant of MFLD for \(J_{\lambda}\) in two ultimately equivalent ways: one is as the MFLD PDE (2.1) with an added "confining" term \(-\frac{1}{\sigma^{2}}w\), which intuitively encourages the noisy particles to remain close to the origin. Another is as Wasserstein gradient flow for the regularized functional

\[J_{\lambda,\beta,\sigma} =J_{\lambda}+\beta^{-1}H+\frac{1}{2\sigma^{2}}\int_{\mathcal{W}} \left\|w\right\|^{2}\mathrm{d}\eta(w)\] \[=J_{\lambda}+\beta^{-1}H\left(\cdot\Big{|}\beta^{-1/2}\sigma \gamma\right)\quad\text{where}\quad\beta^{-1/2}\sigma\gamma\coloneqq\mathcal{ N}(0,\beta^{-1}\sigma^{2}I_{d}),\]whereas MFLD for \(J_{\lambda}\) is the Wasserstein gradient flow for the functional regularized by entropy only, \(J_{\lambda,\beta}=J_{\lambda}+\beta^{-1}H\left(\cdot|\tau\right)=J_{\lambda}+\beta ^{-1}H+\cst\). Unsurprisingly in view of this second interpretation, the distribution \(\beta^{-1/2}\sigma\gamma\) plays a similar role in the analysis of convergence of (A.2) [13, Lemma 3.5], as played by the uniform measure \(\tau\) in our paper: the local LSI property of \(J_{\lambda,\beta,\sigma}\) (resp. \(J_{\lambda,\beta}\)) is obtained by applying the Holley-Stroock argument using \(\beta^{-1/2}\sigma\gamma\) (resp. \(\tau\)) as a reference measure.

Note that the additional confining term \(-\frac{1}{\sigma^{2}}w\) in (A.2) cannot be captured straightforwardly by any additional penalty term on the objective \(G\) from (1.1). Indeed, informally, the three terms in (A.1) each have a different homogeneity in the variable \(a\). Rather, the confining term in \(\sigma\) should be viewed as corresponding to another regularization term added to (1.3), besides the entropy one in \(\beta^{-1}\).

In short, while our work considers MFLD i.e. Wasserstein gradient flow for \(F+\beta^{-1}H\) as the main "algorithmic primitive", the work of [13] considers a MFL+confining dynamics, i.e. Wasserstein gradient flow for \(F+\beta^{-1}H\left(\cdot|\beta\sigma^{2}\gamma\right)\).

Summary of differences.On a technical level, the learning dynamics considered by [13] corresponds to a special case of a variant of the MFLD-bilevel we consider from Sec. 3.2 onwards. Namely, they focus on instances of the problem (1.1) where \(G\) has a particular form, corresponding to learning with 2NN; and they consider \(\mathcal{W}=\mathbb{R}^{d}\) and use an additional confining term \(-\frac{1}{\sigma^{2}}w\) in the MFLD dynamics, while we consider settings where \(\mathcal{W}\) is a compact Riemannian manifold, and no additional confining term is needed.

We also emphasize that, while our work and that of [13] cover some similar settings, our focus is quite different. In that work, the key object of interest is the kernel that is learned by MFLD in a 2NN setting (\((x,x^{\prime})\mapsto\int\varphi(x^{\top}w)\varphi(x^{\top}w^{\prime})\mathrm{ d}\eta(w)\) in the notation of our second bullet point above). By contrast, our main motivation is a general optimization question: how to use MFLD as an algorithmic primitive for problems of the form (1.1). In particular we do not assume a particular form for \(G\) except in Sec. 5, and we pay special attention to the bounds on the local LSI constants of \(J_{\lambda}\) along the MFLD trajectory, instead of using the global uniform LSI bound (compare Prop. 3.4 and [13, Lemma 3.5]).

## Appendix B Details for Sec. 2 (background about MFLD)

### The displacement smoothness property

For MFLD (Eq. (2.1)) to be well-posed, we require that \(F\) is \(L\)-smooth along Wasserstein geodesics for some \(L<+\infty\). More precisely, for any constant-speed Wasserstein geodesic \((\mu_{t})_{t\in[0,1]}\subset\mathcal{P}_{2}(\Omega)\) with \(W_{2}(\mu_{0},\mu_{1})=1\), \(t\mapsto F(\mu_{t})\) should be \(L\)-smooth in the usual sense of continuous optimization. This property ensures that the PDE defining MFLD has a unique solution [10, App. A], and is also helpful to ensure convergence of explicit time-discretization schemes [26]. The following proposition gives a practical sufficient condition.

**Proposition B.1**.: _Suppose \(F:\mathcal{P}_{2}((\Omega,g))\to\mathbb{R}\) is twice differentiable in the Wasserstein sense. Let \(0\leq L<\infty\). Suppose that \(F\) satisfies (P1), i.e.,_

\[\forall\mu\in\mathcal{P}_{2}(\Omega),\ \forall\omega\in\Omega,\ \max_{ \begin{subarray}{c}s\in T_{\omega}\subset \Omega\\ \|s\|_{\omega}\leq 1\end{subarray}}\ \left|\nabla^{2}\,F^{\prime}[\mu](s,s)\right|\leq L\] \[\text{and}\ \ \ \ \forall\mu,\mu^{\prime}\in\mathcal{P}_{2}(\Omega),\ \forall\omega\in\Omega,\ \left\|\nabla F^{\prime}[\mu]-\nabla F^{\prime}[\mu^{\prime}]\right\|_{ \omega}\leq L\ W_{2}(\mu,\mu^{\prime})\]

_where \(\nabla^{2}\) denotes the Riemannian Hessian. Then \(F\) is \(2L\)-smooth along Wasserstein geodesics._

The first condition can be stated as \(F^{\prime}[\mu]:\Omega\to\mathbb{R}\) having Lipschitz-continuous gradients in the Riemannian sense [1, Coroll. 10.47], whereas the second condition can be interpreted as a displacement Lipschitz-continuity of \(\mu\mapsto F^{\prime}[\mu](\omega)\) for each \(\omega\) uniformly.

Proof.: Let a constant-speed Wasserstein geodesic \((\mu_{t})_{t\in[0,1]}\subset\mathcal{P}_{2}(\Omega)\) with \(W_{2}(\mu_{0},\mu_{1})=1\), and pose \(f(t)=F(\mu_{t})\). We want to show that \(f\) is \(2L\)-smooth in the usual sense of continuous optimization, for which it suffices to show that \(\forall t,\ \ |f^{\prime\prime}(t)|\leq 2L\).

By [20, Eq. (13.6)] there exist functions \(\phi_{t}:\Omega\to\mathbb{R}\) such that \(\begin{cases}\partial_{t}\mu_{t}=-\mathrm{div}(\nabla\phi_{t}\mu_{t})\\ \partial_{t}\phi_{t}=-\frac{1}{2}\left\|\nabla\phi_{t}\right\|^{2}\end{cases}\) and \(\int\mathrm{d}\mu_{t}\,\left\|\nabla\phi_{t}\right\|^{2}=W_{2}^{2}(\mu_{0},\mu_ {1})=1\) for all \(t\). So we can compute explicitly:

\[f^{\prime}(t)=\frac{d}{dt}F(\mu_{t})=\int\mathrm{d}\mu_{t}\,\left< \nabla F^{\prime}[\mu_{t}],\nabla\phi_{t}\right>\] \[f^{\prime\prime}(t)=\int\mathrm{d}(\partial_{t}\mu_{t})\,\left< \nabla F^{\prime}[\mu_{t}],\nabla\phi_{t}\right>+\int\mathrm{d}\mu_{t}\,\frac {d}{dt}\left<\nabla F^{\prime}[\mu_{t}],\frac{d}{dt}\nabla\phi_{t}\right>\] \[=\int\mathrm{d}\mu_{t}\,\left<\nabla\Big{[}\left<\nabla F^{ \prime}[\mu_{t}],\nabla\phi_{t}\right>\Big{]},\nabla\phi_{t}\right>+\int \mathrm{d}\mu_{t}\,\left(\left<\nabla F^{\prime}[\mu_{t}],\frac{d}{dt}\nabla \phi_{t}\right>+\left<\frac{d}{dt}\nabla F^{\prime}[\mu_{t}],\nabla\phi_{t} \right>\right)\] \[=\int\mathrm{d}\mu_{t}\,\left.\nabla^{2}\,F^{\prime}[\mu_{t}]( \nabla\phi_{t},\nabla\phi_{t})\right.\] \[+\int\mathrm{d}\mu_{t}\,\left.\nabla^{2}\,\phi_{t}\,(\nabla F^{ \prime}[\mu_{t}],\nabla\phi_{t})+\int\mathrm{d}\mu_{t}\,\left<\nabla F^{ \prime}[\mu_{t}],\nabla\partial_{t}\phi_{t}\right>\] \[+\int\mathrm{d}\mu_{t}\,\left<\frac{d}{dt}\nabla F^{\prime}[\mu_{t }],\nabla\phi_{t}\right>.\]

Now the first line can be bounded using the first condition of (P1): writing \(s_{t}(\omega)=\frac{\nabla\phi_{t}(\omega)}{\left\|\nabla\phi_{t}(\omega) \right\|}\) for all \(t\) and \(\omega\),

\[\left|\int\mathrm{d}\mu_{t}\,\left.\nabla^{2}\,F^{\prime}[\mu_{t}](\nabla \phi_{t},\nabla\phi_{t})\right|=\left|\int\mathrm{d}\mu_{t}\,\left\|\nabla \phi_{t}\right\|^{2}\nabla^{2}\,F^{\prime}[\mu_{t}](s_{t},s_{t})\right|\leq L \cdot\int\mathrm{d}\mu_{t}\,\left\|\nabla\phi_{t}\right\|^{2}=L.\]

Moreover, one can show by direct computation that the second line is zero, using that \(\partial_{t}\phi_{t}=-\frac{1}{2}\left\|\nabla\phi_{t}\right\|^{2}\). For the third line, we have

\[\left|\int\mathrm{d}\mu_{t}\,\left.\left<\frac{d}{dt}\nabla F^{\prime}[\mu_{t} ],\nabla\phi_{t}\right>\right|\leq\int\mathrm{d}\mu_{t}\,\left\|\nabla\phi_{t }\right\|\cdot\sup_{t\in[0,1]}\sup_{\omega\in\Omega}\left\|\frac{d}{dt} \nabla F^{\prime}[\mu_{t}](\omega)\right\|\]

since \(\left(\int\mathrm{d}\mu_{t}\,\left\|\nabla\phi_{t}\right\|\right)^{2}\leq\int \mathrm{d}\mu_{t}\,\left\|\nabla\phi_{t}\right\|^{2}=1\). Finally, let us show that the second condition of (P1) implies a bound on the last quantity: for all \(\omega\in\Omega\), by applying the assumption to \(\mu=\mu_{t}\) and \(\mu^{\prime}=\mu_{s}\),

\[\frac{\left\|\nabla F^{\prime}[\mu_{s}](\omega)-\nabla F^{\prime}[\mu_{t}]( \omega)\right\|_{\omega}}{s-t}\leq\frac{L\,W_{2}(\mu_{s},\mu_{t})}{s-t}=L\]

since \((\mu_{t})_{t}\) is a constant-speed geodesic with \(W_{2}(\mu_{0},\mu_{1})=1\). So by letting \(s\to t\) we obtain that \(\left\|\frac{d}{dt}\nabla F^{\prime}[\mu_{t}](\omega)\right\|\leq L\) for all \(t\in[0,1]\), \(\omega\in\Omega\). Thus we have shown \(|f^{\prime\prime}(t)|\leq 2L\), and so \(F\) is \(2L\)-smooth along Wasserstein geodesics. 

### Classical sufficient conditions for LSI

For ease of reference we reproduce here a classical sufficient condition for a probability measure \(\mu\in\mathcal{P}(\Omega)\) to satisfy LSI.

**Lemma B.2** (Holley-Stroock bounded perturbation argument [14]).: _Let \(\mu,\mu_{0}\in\mathcal{P}(\Omega)\) such that \(\mu\) is absolutely continuous w.r.t. \(\mu_{0}\). Suppose that \(\mu_{0}\) satisfies LSI with constant \(\alpha\) and that \(-M\leq\log\frac{\mathrm{d}\mu}{\mathrm{d}\mu_{0}}(\omega)+c\leq M\) for all \(\omega\in\mathrm{supp}(\mu_{0})\), for some \(c\in\mathbb{R}\) and \(M\geq 0\). Then \(\mu\) satisfies LSI with constant \(\alpha e^{-M}\)._

## Appendix C Details for Sec. 3.1 (reduction by lifting)

### Proof of Prop. 3.1

Here we present a slightly stronger version of Prop. 3.1 that uses the \(p\)-homogeneous projection operator for arbitrary \(p>0\), in preparation for the next subsection, where we show that one can restrict attention to the case \(p=1\) as done in the main text.

Recall that we let \(\Omega=\mathbb{R}\times\mathcal{W}\). For any \(p>0\), we denote by \(\boldsymbol{h}^{p}:\mathcal{P}(\Omega)\to\mathcal{M}(\mathcal{W})\) the signed \(p\)-homogeneous projection operator [18] defined by

\[\forall\varphi\in\mathcal{C}(\mathcal{W},\mathbb{R}),\ \int_{\mathcal{W}} \varphi(w)(\boldsymbol{h}^{p}\mu)(\mathrm{d}w)=\int_{\Omega}\operatorname{sign }(r)\left|r\right|^{p}\varphi(w)\mu(\mathrm{d}r,\mathrm{d}w).\]

More concretely, for atomic measures, \(\boldsymbol{h}^{p}\left(\frac{1}{m}\sum_{j=1}^{m}\delta_{(r_{j},w_{j})}\right)= \frac{1}{m}\sum_{j=1}^{m}\operatorname{sign}(r_{j})\left|r_{j}\right|^{p} \delta_{w_{j}}\).

**Lemma C.1**.: _For \(b\in[1,2]\) and \(p>0\), let \(\Psi_{b,p}:\mathcal{P}(\Omega)\to\mathbb{R}\cup\{+\infty\}\) defined by \(\Psi_{b,p}(\mu)\coloneqq\left(\int_{\Omega}\left|r\right|^{p^{b}}\mathrm{d}\mu (r,w)\right)^{2/b}\) if \(\mu\in\mathcal{P}_{pb}(\Omega)\), and \(+\infty\) otherwise. Then_

\[\min_{\mu\text{ s.t. }\boldsymbol{h}^{p}\mu=\nu}\Psi_{b,p}(\mu)=\left\|\nu \right\|_{TV}^{2}.\]

_Moreover, if \(b=1\) then the set of minimizers is_

\[\left\{\mu\in\mathcal{P}(\mathcal{W});\ \ \boldsymbol{h}^{p}\mu=\nu\text{ and } \forall w,\operatorname{supp}(\mu(\cdot|w))\subset\mathbb{R}_{+}\text{ or }\operatorname{supp}(\mu(\cdot|w))\subset\mathbb{R}_{-}\right\},\]

_and if \(b>1\) there is a unique minimizer which is \(\delta_{f(w)}(\mathrm{d}r)\frac{\left|\nu\right|(\mathrm{d}w)}{\left\|\nu \right\|_{TV}}\) where \(f(w)=\left\|\nu\right\|_{TV}^{1/p}\frac{\mathrm{d}\nu}{\mathrm{d}\left|\nu \right|}(w)\)._

Proof.: For any \(\mu\in\mathcal{P}(\Omega)\) such that \(\boldsymbol{h}^{p}=\nu\),

\[\left\|\boldsymbol{h}^{p}\mu\right\|_{TV}=\max_{\phi:\mathcal{W} \to[-1,1]}\int_{\Omega}\operatorname{sign}(r)\left|r\right|^{p}\phi(w)\mathrm{ d}\mu(r,w)\leq\int_{\Omega}\left|r\right|^{p}\mathrm{d}\mu(r,w)\] \[\text{ so }\ \left\|\nu\right\|_{TV}^{2}=\left\|\boldsymbol{h}^{p}\mu \right\|_{TV}^{2}\leq\left(\left(\int_{\Omega}\left|r\right|^{p}\mathrm{d}\mu (r,w)\right)^{b}\right)^{2/b}\leq\left(\int_{\Omega}\left|r\right|^{pb} \mathrm{d}\mu(r,w)\right)^{2/b}=\Psi_{b,p}(\mu),\]

where the first inequality follows from the triangle inequality, and the second inequality follows from Jensen's inequality since \(t\mapsto t^{b}\) is convex on \(\mathbb{R}_{+}\). Note that the first inequality above holds with equality if and only if there exists \(\phi:\mathcal{W}\to[-1,1]\) such that \(\operatorname{sign}(r)\phi(w)\geq 0\) for all \((r,w)\in\operatorname{supp}(\mu)\), i.e., if the conditional distribution \(\mu(\mathrm{d}r|w)\) is either supported on \(\mathbb{R}_{+}\) or supported on \(\mathbb{R}_{-}\) for each \(w\). Conversely, the value \(\left\|\nu\right\|_{TV}^{2}\) is attained by letting \(\mu(\mathrm{d}r,\mathrm{d}w)=\delta_{f(w)}(\mathrm{d}r)\frac{\left|\nu\right| (\mathrm{d}w)}{\left\|\nu\right\|_{TV}}\) where \(f(w)=\left\|\nu\right\|_{TV}^{1/p}\frac{\mathrm{d}\nu}{\mathrm{d}\left|\nu \right|}(w)\). This proves that \(\min_{\mu:\boldsymbol{h}^{p}\mu=\nu}\Psi_{b,p}(\mu)=\left\|\nu\right\|_{TV}^{2}\).

For \(b=1\), \(t\mapsto t^{b}=t\) is linear, so equality always holds in Jensen's inequality. So the set of minimizers is all of \(\left\{\mu\in\mathcal{P}(\mathcal{W});\ \ \boldsymbol{h}^{p}\mu=\nu\text{ and } \forall w,\operatorname{supp}(\mu(\cdot|w))\subset\mathbb{R}_{+}\text{ or }\operatorname{supp}(\mu(\cdot|w))\subset\mathbb{R}_{-}\right\}\).

For \(b>1\), \(t\mapsto t^{b}\) is strictly convex, the second inequality above holds with equality if and only if there exists a constant \(c\) such that \(\left|r\right|^{p}=c\) for all \((r,w)\in\operatorname{supp}(\mu)\). So for \(\mu\) to be a minimizer, the conditional distribution \(\mu(\mathrm{d}r|w)\) must be concentrated on \(\{c^{1/p},-c^{1/p}\}\) for each \(w\). Moreover, for the first inequality above to hold, the conditional distribution at each \(w\) must be either supported on \(\mathbb{R}_{+}\) or suported on \(\mathbb{R}_{-}\), so there exists a function \(f:\mathcal{W}\to\{c^{1/p},-c^{1/p}\}\) such that \(\mu(\mathrm{d}r,\mathrm{d}w)=\delta_{f(w)}(\mathrm{d}r)\mu^{w}(\mathrm{d}w)\) where \(\mu^{w}\in\mathcal{P}(\mathcal{W})\) denotes the marginal distribution. Since \(\boldsymbol{h}^{p}\mu=\nu\), then for all fixed \(w\), \(\int_{\mathbb{R}_{+}}\operatorname{sign}(r)\left|r\right|^{p}\mu(\mathrm{d}r, \mathrm{d}w)=\operatorname{sign}(f(w))c\mu^{w}(\mathrm{d}w)=\nu(\mathrm{d}w)\). So \(\operatorname{sign}(f(w))=\operatorname{sign}(\frac{\mathrm{d}\nu}{\mathrm{d} \mu^{w}}(w))=\frac{\mathrm{d}\nu}{\mathrm{d}\left|\nu\right|}(w)\) and \(\mu^{w}(\mathrm{d}w)=\frac{1}{c}\left|\nu\right|(\mathrm{d}w)\) since \(\mu^{w}\) is a probability measure so non-negative, and integrating on both sides over \(\Omega\) shows that \(c=\left\|\nu\right\|_{TV}\). Hence the only minimizer is \(\mu(\mathrm{d}r,\mathrm{d}w)=\delta_{f(w)}(\mathrm{d}r)\frac{\left|\nu\right|( \mathrm{d}w)}{\left\|\nu\right\|_{TV}}\) where \(f(w)=c^{1/p}\frac{\mathrm{d}\nu}{\mathrm{d}\left|\nu\right|}(w)\). 

Prop. 3.1 follows directly as a special case of the following proposition with \(p=1\).

**Proposition C.2**.: _Let any \(p>0\) and \(b\in[1,2]\) and let \(\Psi_{b,p}:\mathcal{P}(\Omega)\to\mathbb{R}\cup\{+\infty\}\) as in the lemma above. Consider the optimization problem over probability measures, with \(\lambda>0\),_

\[\min_{\mu\in\mathcal{P}(\Omega)}F_{\lambda,b,p}(\mu)\quad\text{where}\quad F_{ \lambda,b,p}(\mu)=G(\boldsymbol{h}^{p}\mu)+\frac{\lambda}{2}\Psi_{b,p}(\mu).\] (C.1)

_Then \(\min_{\mathcal{P}(\Omega)}F_{\lambda,b,p}=\min_{\mathcal{M}(\mathcal{W})}G_{\lambda}\)._

_Moreover, if \(b>1\) then \(\operatorname{arg\,min}F=\left\{\delta_{\left\|\nu\right\|_{TV}^{1/p}\frac{ \mathrm{d}\nu}{\mathrm{d}\left|\nu\right|_{TV}}}(\mathrm{d}r)\Proof.: The fact that \(\min_{\mathcal{P}(\Omega)}F_{\lambda,b,p}=\min_{\mathcal{M}(\mathcal{W})}G_{\lambda}\) can be seen directly as follows:

\[\min_{\mu\in\mathcal{P}(\Omega)}F(\mu) =\min_{\mu\in\mathcal{P}(\Omega)}G(\bm{h}^{p}\mu)+\frac{\lambda}{2 }\Psi_{b,p}(\mu)\] \[=\min_{\nu\in\mathcal{M}(\Omega)}\left[\min_{\mu\in\mathcal{P}( \Omega):\bm{h}^{p}=\nu}G(\bm{h}^{p}\mu)+\frac{\lambda}{2}\Psi_{b,p}(\mu)\right]\] \[=\min_{\nu\in\mathcal{M}(\Omega)}G(\nu)+\frac{\lambda}{2}\left[ \min_{\mu\mathcal{P}(\Omega):\bm{h}^{p}=\nu}\Psi_{b,p}(\mu)\right]\] \[=\min_{\nu\in\mathcal{M}(\Omega)}G(\nu)+\frac{\lambda}{2}\left\| \nu\right\|_{TV}^{2}\ =\min_{\nu\in\mathcal{M}(\Omega)}G_{\lambda}(\nu)\]

where we used the lemma above at the fourth equality. The characterization of \(\arg\min F\) in terms of \(\arg\min G\) follows from the characterization of the minimizers of the inner minimization \(\left[\min_{\mu\in\mathcal{P}(\Omega):\bm{h}^{p}=\nu}\Psi_{b}(\mu)\right]\) in the third line, which is given by the lemma above.

Furthermore, \(F_{\lambda,b,p}\) is convex since \(G\) and \(\Psi_{b,p}\) are. 

### Equivalence of using \((cp,cq_{r},cq_{w},\Gamma/c^{2})\) for any \(c>0\) by reparametrizing

Equivalence of Riemannian structures on \(\Omega^{*}\) for \((cq_{r},cq_{w},\Gamma/c^{2})\) for \(c>0\).Recall that we consider equipping \(\Omega^{*}=\mathbb{R}^{*}\times\mathcal{W}\) with a Riemannian metric of the form (3.2), reproduced here for ease of reference:

The following proposition shows that, in fact, different choices of \(q_{r},q_{w}\) and \(\Gamma\) lead to the same geometry, up to a reparametrization of the form \((a,w)=(r^{\alpha},w)\) (for \(r>0\)). Namely it is equivalent to use the metric with exponents \((q_{r},q_{w})\) or with \(\left(\frac{q_{r}}{\alpha},\frac{q_{w}}{\alpha}\right)\), up to adjusting \(\Gamma\).

**Proposition C.3**.: _For any \(q_{r},q_{w}\), denote by \(g_{[q_{r},q_{w},\Gamma]}\) the metric \(g_{(r,w)}=\begin{bmatrix}\Gamma^{-1}&|r|^{q_{r}-2}&0\\ 0&|r|^{q_{w}}\,g_{w}\end{bmatrix}\) on \(\Omega^{*}=\mathbb{R}^{*}\times\mathcal{W}\). Then for any \(q_{r},q_{w}\in\mathbb{R}\) and \(\Gamma,\alpha>0\), the map \(T_{\alpha}:\left(\Omega^{*},g_{[q_{r},q_{w},\Gamma]}\right)\to\left(\Omega^{*},g_{[\frac{q_{r}}{\alpha},\frac{q_{w}}{\alpha},\alpha^{2}\Gamma]}\right)\) defined by \(T_{\alpha}(r,w)=\left(\operatorname{sign}(r)\left|r\right|^{\alpha},w\right)\) is an isometry._

Proof.: Since \(\Omega^{*}\) is a disjoint manifold: \(\Omega^{*}=\mathbb{R}^{*}_{+}\times\mathcal{W}\cup\mathbb{R}^{*}_{-}\times \mathcal{W}\), and since \(T_{\alpha}(\mathbb{R}^{*}_{+}\times\mathcal{W})=\mathbb{R}^{*}_{+}\times \mathcal{W}\), it suffices to check that the restricted map \(T^{+}_{\alpha}:\left(\mathbb{R}^{*}_{+}\times\mathcal{W},g_{[q_{r},q_{w}, \Gamma]}\right)\to\left(\mathbb{R}^{*}_{+}\times\mathcal{W},g_{[\frac{q_{r}}{ \alpha},\frac{q_{w}}{\alpha},\alpha^{2}\Gamma]}\right)\) is an isometry (as well as the analogous statement for the restricted map \(T^{-}_{\alpha}\), but it will follow analogously).

Indeed, denote by \(\tilde{g}\) the metric on \(\mathbb{R}^{*}_{+}\times\mathcal{W}\) induced by \(T^{+}_{\alpha}\). It is given by, for \((a,w)=T^{+}_{\alpha}(r,w)=(r^{\alpha},w)\), so \(\frac{\delta a}{a}=\alpha\frac{dr}{r}\),

\[\begin{pmatrix}\delta r_{1}\\ \delta w_{1}\end{pmatrix}\cdot g_{(r,w)}\begin{pmatrix}\delta r_{2}\\ \delta w_{2}\end{pmatrix}=\begin{pmatrix}\delta a_{1}\\ \delta w_{1}\end{pmatrix}\cdot\tilde{g}_{(a,w)}\begin{pmatrix}\delta a_{2}\\ \delta w_{2}\end{pmatrix}=\begin{pmatrix}\alpha a_{\frac{1}{r}}^{\perp}\delta r _{1}\\ \delta w_{1}\end{pmatrix}\cdot\tilde{g}_{(a,w)}\begin{pmatrix}\alpha a_{\frac{1}{r} }^{\perp}\delta r_{2}\\ \delta w_{2}\end{pmatrix}\] \[\text{so }\tilde{g}_{(a,w)}=\begin{bmatrix}\frac{r}{\alpha a}&0\\ 0&1\end{bmatrix}g_{(r,w)}\begin{bmatrix}\frac{r}{\alpha a}&0\\ 0&1\end{bmatrix}\] \[\qquad\qquad\qquad\qquad=\begin{bmatrix}\frac{r^{2}}{\alpha a^{2}}\Gamma^{-1}r^{q_{r}-2}&0\\ 0&r^{q_{w}}g_{w}\end{bmatrix}=\begin{bmatrix}\Gamma^{-1}\alpha^{-2}a^{q_{r}/ \alpha-2}&0\\ 0&a^{q_{w}/\alpha}g_{w}\end{bmatrix}.\]

So \(\tilde{g}\) is precisely \(g_{[\frac{q_{r}}{\alpha},\frac{q_{w}}{\alpha},\alpha^{2}\Gamma]}\) on \(\mathbb{R}^{*}_{+}\times\mathcal{W}\), which proves the claim. 

Equivalence of the Wasserstein gradient flow of \(F_{\lambda,b,p}\) for \((cp,cq_{r},cq_{w},\Gamma/c^{2})\) for any \(c>0\).Proposition C.4**.: _Let \(T:(\Omega_{1},g_{[1]})\to(\Omega_{2},g_{[2]})\) an isometry between Riemannian manifolds. Let \(F:\mathcal{P}(\Omega_{1})\to\mathbb{R}\) (sufficiently regular) and \((\mu_{t})_{t}\) a Wasserstein gradient flow for \(F\), i.e., \(\partial_{t}\mu_{t}=-\mathrm{div}(\mu_{t}\nabla F^{\prime}[\mu_{t}])\) (where \(\nabla\) denotes Riemannian gradient in \((\Omega_{1},g_{[1]})\)). Then, \((\tilde{\mu})_{t}\coloneqq(T_{\sharp}\mu_{t})_{t}\) is a Wasserstein gradient flow for \(\tilde{F}:\mathcal{P}(\Omega_{2})\to\mathbb{R}\) defined by \(\tilde{F}(\tilde{\mu})=F(T^{-1}_{\sharp}\tilde{\mu})\)._Proof.: First note that \(g_{[2]}\) is given by, for all \(y=T(x)\in\Omega_{2}\), so \(dy=DT(x)dx\) where \(D\) denotes the differential,

\[\delta y^{\top}\ g_{[2]y}\,\delta y^{\prime} =\delta x^{\top}\ g_{[1]x}\,\delta x^{\prime}=\delta y^{\top}((DT( x))^{-1})^{\top}\ g_{[1]x}\ (DT(x))^{-1}\delta y^{\prime}\] \[\text{so}\quad g_{[1]x}^{-1}=(DT(x))^{-1})\ g_{[2]T(x)}^{-1}\ ((DT(x))^{-1})^{\top}.\]

Also note that \(\widetilde{F}^{\prime}[\tilde{\mu}](y)=F^{\prime}[T_{\sharp}^{-1}\tilde{\mu}] (T^{-1}(y))\), as one can check directly by computing \(\lim_{\varepsilon\to 0}\frac{1}{\varepsilon}\left[\widetilde{F}(\tilde{\mu}+ \varepsilon\tilde{\nu})-\widetilde{F}(\tilde{\mu})\right]=\lim_{\varepsilon \to 0}\frac{1}{\varepsilon}\left[F(T_{\sharp}^{-1}\tilde{\mu}+\varepsilon T _{\sharp}^{-1}\tilde{\nu})-F(T_{\sharp}^{-1}\tilde{\mu})\right]\). In particular \(D\widetilde{F}^{\prime}[\tilde{\mu}](y)=DF^{\prime}[T_{\sharp}^{-1}\tilde{ \mu}](T^{-1}(y))(DT(T^{-1}(y)))^{-1}\). Then for any \(\varphi:\Omega_{2}\to\mathbb{R}\),

\[\frac{d}{dt}\int_{\Omega_{2}}\varphi\mathrm{d}\tilde{\mu}_{t} =\frac{d}{dt}\int_{\Omega_{1}}\varphi(T(x))\mathrm{d}\mu_{t}(x)\] \[=\int_{\Omega_{1}}D\varphi(T(x))DT(x)\ g_{[1]}^{-1}\ DF^{\prime}[ \mu_{t}](x)\mathrm{d}\mu_{t}(x)\] \[=\int_{\Omega_{1}}D\varphi(y)\ g_{[2]}^{-1}\ D\widetilde{F}^{ \prime}[\tilde{\mu}_{t}](y)\mathrm{d}\tilde{\mu}_{t}(y).\]

That is, \(\partial_{t}\tilde{\mu}_{t}=-\mathrm{div}(\tilde{\mu}_{t}g_{[2]}^{-1}D \widetilde{F}^{\prime}[\tilde{\mu}_{t}])\), i.e., \((\tilde{\mu}_{t})_{t}\) is a Wasserstein gradient flow for \(\widetilde{F}\). 

**Proposition C.5**.: _Consider the functionals \(F_{\lambda,b,p}\) over \(\mathcal{P}(\Omega)\) from Prop. C.2 and the Riemannian metrics \(g_{[qr,q_{w},\Gamma]}\) over \(\Omega^{\star}\) from Prop. C.3, where \(\Omega=\mathbb{R}\times\mathcal{W}\) and \(\Omega^{\star}=\mathbb{R}^{\star}\times\mathcal{W}\)._

_Fix \(q_{r},q_{w}\in\mathbb{R}\), \(\Gamma,p,\lambda>0\) and \(b\in[1,2]\). Let \((\mu_{t})_{t}\) the Wasserstein gradient flow for \(F_{\lambda,b,p}\) over \((\Omega^{\star},g_{[qr,q_{w},\Gamma]})\), starting from some \(\mu_{0}\in\mathcal{P}(\Omega^{\star})\)._

_Let \(\alpha>0\) and \(T_{\alpha}:\Omega^{\star}\to\Omega^{\star}\) defined by \(T_{\alpha}(r,w)=(\mathrm{sign}(r)\,|r|^{\alpha}\,,w)\). Then \((\tilde{\mu}_{t})_{t}\coloneqq((T_{\alpha})_{\sharp}\mu_{t})_{t}\) coincides with the Wasserstein gradient flow for \(F_{\tilde{\lambda},\tilde{b},\tilde{p}}\) over \((\Omega^{\star},g_{[qr,\tilde{q}_{w},\widetilde{\Gamma}]})\) starting from \(\tilde{\mu}_{0}=(T_{\alpha})_{\sharp}\mu_{0}\), where_

\[\tilde{p}=\frac{p}{\alpha},\qquad\quad\tilde{q}_{r}=\frac{q_{r}}{\alpha}, \qquad\quad\tilde{q}_{w}=\frac{q_{w}}{\alpha},\qquad\quad\widetilde{\Gamma}= \alpha^{2}\Gamma,\qquad\quad\tilde{\lambda}=\lambda,\qquad\quad\tilde{b}=b.\]

Proof.: The proposition follows from an application of Prop. C.4 with \(T=T_{\alpha}\), \(\Omega_{1}=(\Omega^{\star},g_{[qr,q_{w},\Gamma]})\), \(\Omega_{2}=(\Omega^{\star},g_{[qr,q_{w}^{\prime},\Gamma^{\prime}]})\) and \(F=F_{\lambda,b,p}\). Indeed the fact that \(T_{\alpha}\) is an isometry from \(\Omega_{1}\) to \(\Omega_{2}\) was shown in Prop. C.3. It only remains to show that \(F\circ T_{\sharp}^{-1}=F_{\tilde{\lambda},\tilde{b},\tilde{p}}\). And indeed for any \(\tilde{\mu}\in\mathcal{P}(\Omega^{\star})\),

\[F_{\lambda,b,p}((T_{\alpha})_{\sharp}^{-1}\tilde{\mu})=F_{\lambda,b,p}((T_{ \alpha^{-1}})_{\sharp}\tilde{\mu})=G\left(\bm{h}^{p}(T_{\alpha^{-1}})_{ \sharp}\tilde{\mu}\right)+\frac{\lambda}{2}\Psi_{b,p}\left((T_{\alpha^{-1}})_{ \sharp}\tilde{\mu}\right),\]

and \(\bm{h}^{p}(T_{\alpha^{-1}})_{\sharp}\tilde{\mu}=\bm{h}^{p/\alpha}\tilde{\mu}\), since for any \(\varphi:\mathcal{W}\to\mathbb{R}\),

\[\int_{\mathcal{W}}\varphi\mathrm{d}\left[\bm{h}^{p}(T_{\alpha^{-1 }})_{\sharp}\tilde{\mu}\right] =\int_{\mathbb{R}}\int_{\mathcal{W}}\varphi(w)\,\mathrm{sign}(r)\, |r|^{p}\left[(T_{\alpha^{-1}})_{\sharp}\tilde{\mu}\right](\mathrm{d}r, \mathrm{d}w)\] \[=\int_{\mathbb{R}}\int_{\mathcal{W}}\varphi(w)\,\mathrm{sign}( \tilde{r})\,|\tilde{r}|^{p/\alpha}\,\tilde{\mu}(\mathrm{d}\tilde{r},\mathrm{d}w )=\int_{\mathcal{W}}\varphi\mathrm{d}\left[\bm{h}^{p/\alpha}\tilde{\mu}\right],\]

and

\[\Psi_{b,p}((T_{\alpha^{-1}})_{\sharp}\tilde{\mu})=\left(|r|^{p}\,\mathrm{d} \left[(T_{\alpha^{-1}})_{\sharp}\tilde{\mu}\right]\right)^{2/b}=\left(|\tilde{r }|^{pb/\alpha}\,\mathrm{d}\tilde{\mu}(r,w)\right)^{2/b}.\]

This confirms that \(F\circ T_{\sharp}^{-1}=F_{\tilde{\lambda},\tilde{b},\tilde{p}}\) and concludes the proof. 

Thus, it is equivalent to consider the lifting reduction with the hyperparameters \((p,q_{r},q_{w},\Gamma)\) or with \(\left(cp,cq_{r},cq_{w},\Gamma/c^{2}\right)\) for any \(c>0\).

_Remark C.1_.: The choice \(p=q_{r}=q_{w}\) plays a special role, as Wasserstein gradient flows \((\mu_{t})_{t}\) on \(\mathcal{P}(\mathbb{R}_{+}^{\star}\times\mathcal{W})\) for functionals of the form \(\mu\mapsto G(\bm{h}^{p}\mu)\) then correspond to gradient flows \((\nu_{t})_{t}\) on \(\mathcal{M}_{+}(\mathcal{W})\) for \(G\) in the Wasserstein-Fisher-Rao geometry [10, Prop. 2.1], via \(\nu_{t}=\bm{h}^{p}\mu_{t}\). This correspondence is lost however for functionals of the form of \(F_{\lambda,b,p}\) as in Prop. C.2 with \(\lambda\neq 0\).

Equivalence of MFLD of \(F_{\lambda,b,p}\) for \((cp,cq_{r},cq_{w},\Gamma/c^{2})\) for any \(c>0\).Since MFLD for \(F_{\lambda,b,p}\) is the Wasserstein gradient flow of \(F_{\lambda,b,p}+\beta^{-1}H\), then by Prop. C.4, by proceeding similarly as in the proof of Prop. C.5, it suffices to check that \(\tilde{\mu}\mapsto H((T_{\alpha^{-1}})_{\sharp}\tilde{\mu})\) is equal to \(H\) itself, up to an additive constant. And indeed, since \(T_{\alpha^{-1}}\) is invertible, by data processing inequality for differential entropy, we have \(H((T_{\alpha^{-1}})_{\sharp}\tilde{\mu})=H(\tilde{\mu})\) for all \(\tilde{\mu}\in\mathcal{P}(\Omega^{*})\).

### Proof of Prop. 3.2

**Lemma C.6**.: _Let \(F_{\lambda,b,p}\) defined in (C.1) and \(\Omega=\mathcal{W}\times\mathbb{R}\). For any \(\mu\in\mathcal{P}(\Omega)\),_

\[F^{\prime}_{\lambda,b,p}[\mu](r,w)=\operatorname{sign}(r)\left|r \right|^{p}G^{\prime}[\boldsymbol{h}^{p}\mu](w)+\lambda^{\prime}\left|r \right|^{pb}\] (C.2)

_where \(\lambda^{\prime}=\lambda_{\frac{1}{b}}\Psi_{b,p}(\mu)^{1-\frac{b}{2}}\)._

Proof.: For any \(\mu^{\prime}\in\mathcal{P}(\Omega)\),

\[\lim_{\varepsilon\to 0}\frac{1}{\varepsilon}\left[(G\circ \boldsymbol{h}^{p})(\mu+\varepsilon\mu^{\prime})-(G\circ\boldsymbol{h}^{p})( \mu)\right]=\lim_{\varepsilon\to 0}\frac{1}{\varepsilon}\left[G(\boldsymbol{h}^{p}\mu+ \varepsilon\boldsymbol{h}^{p}\mu^{\prime})-G(\boldsymbol{h}^{p}\mu)\right]\] \[=\int_{\mathcal{W}}G^{\prime}[\boldsymbol{h}^{p}\mu](w)\mathrm{d }\left[\boldsymbol{h}^{p}\mu^{\prime}\right](w)=\int_{\mathbb{R}\times \mathcal{W}}\operatorname{sign}(r)\left|r\right|^{p}G^{\prime}[\boldsymbol{h}^ {p}\mu](w)\mathrm{d}\mu^{\prime}(r,w)\]

and so \((G\circ\boldsymbol{h}^{p})^{\prime}\left[\mu\right](r,w)=\operatorname{sign}( r)\left|r\right|^{p}G^{\prime}[\boldsymbol{h}^{p}\mu](w).\) Moreover

\[\Psi_{b,p}(\mu) =\left(\int_{\Omega}\left|r\right|^{pb}\mathrm{d}\mu(r,w)\right) ^{\frac{2}{b}}\] \[\Psi^{\prime}_{b,p}[\mu](r,w) =\frac{2}{b}\left(\int_{\Omega}\left|r\right|^{rp}\mathrm{d}\mu( r^{\prime},w^{\prime})\right)^{\frac{2}{b}-1}\left|r\right|^{pb}=\frac{2}{b}\Psi_{b,p} (\mu)^{1-\frac{b}{2}}\left|r\right|^{pb}.\]

Summing the results of these two calculations gives the first variation of \(F_{\lambda,b,p}=G\circ\boldsymbol{h}^{p}+\frac{\lambda}{2}\Psi_{b,p}\). 

**Lemma C.7**.: _Let \(f:\mathbb{R}_{+}^{*}\times\mathcal{W}\to\mathbb{R}\) defined by \(f(r,w)=r^{p}\tilde{\phi}(w)+\lambda^{\prime}r^{pb}\), for some \(p,\lambda^{\prime}>0\), \(b\in[1,2]\), and \(\tilde{\phi}:\mathcal{W}\to\mathbb{R}\). Assume that \(\nabla^{2}\,\tilde{\phi}\) is not constant equal to \(0\)._

_Consider \(\mathbb{R}_{+}^{*}\times\mathcal{W}\) equipped with the Riemannian metric (3.2). If \(f\) has Lipschitz-continuous Riemannian gradients, then necessarily \(b=1\) and \(p=q_{r}=q_{w}\), or \(b=1\) and \(p=q_{r}/2=q_{w}/2\) and \(\nabla^{2}\tilde{\phi}(w)=\Gamma p^{2}\left(\tilde{\phi}(w)+\lambda^{\prime} \right)g_{w}\) for all \(w\)._

The proof of Lem. C.7 is technical, so it is deferred to the next section.

Proof of Prop. 3.2.: Let us prove the first item in the proposition. Suppose by contraposition that \(F_{\lambda,b}\) does satisfy (P1). Let any \(\nu\in\mathcal{M}(\mathcal{W})\) such that \(\nabla^{2}G^{\prime}[\nu]\) is not constant equal to \(0\), and consider some \(\mu\in\mathcal{P}(\Omega)\) to be chosen such that \(\boldsymbol{h}\mu=\nu\). Then by the first condition of (P1), \(f\coloneqq F^{\prime}_{\lambda,b}[\mu]\big{|}_{\mathbb{R}_{+}^{*}\times \mathcal{W}}\) the restriction of \(F^{\prime}_{\lambda,b}[\mu]\) to \(\mathbb{R}_{+}^{*}\times\mathcal{W}\) must have Lipschitz-continuous Riemannian gradients. More explicitly, by (C.2), \(f(r,w)=rG^{\prime}[\nu](w)+\lambda^{\prime}_{\mu}r^{b}\) where \(\lambda^{\prime}_{\mu}=\frac{\lambda}{b}\Psi_{b}(\mu)^{1-\frac{b}{2}}\). So by Lem. C.7, necessarily \(b=1\), and so \(\lambda^{\prime}_{\mu}=\lambda\Psi_{1}(\mu)^{1/2}\). If \(\tilde{\phi}=G^{\prime}[\nu]\) satisfies \(\nabla^{2}\tilde{\phi}(w)=\Gamma p^{2}\left(\tilde{\phi}(w)+\lambda^{\prime}_{ \mu}\right)g_{w}\) for all \(w\), pick any other \(\mu^{\prime}\) such that \(\boldsymbol{h}\mu^{\prime}=\nu\) and \(\Psi_{1}(\mu^{\prime})\neq\Psi_{1}(\mu)\) - the existence of such a \(\mu^{\prime}\) follows from the first step in the proof of Lem. C.1. Then by applying the above reasoning to \(F^{\prime}_{\lambda,b}[\mu^{\prime}]\big{|}_{\mathbb{R}_{+}^{*}\times\mathcal{W}}\) instead of \(f\), since \(\lambda^{\prime}_{\mu^{\prime}}\neq\lambda^{\prime}_{\mu}\), we also have by Lem. C.7 that \(p=q_{r}=q_{w}\). This shows that if \(F_{\lambda,b}\) satisfies (P1) then \((q_{r},q_{w},b)=(1,1,1)\), which was the announced necessary condition.

We now turn to the second item of the proposition. Suppose that \(q_{r}=q_{w}=b=1\). For any \(\mu\in\mathcal{P}_{1}(\Omega)\), denote

\[\lambda_{0\mu}=\sup_{w\in\mathcal{W}}\frac{\left|G^{\prime}[ \boldsymbol{h}\mu](w)\right|}{\Psi_{1}(\mu)^{1/2}}.\]Let us show that if \(\lambda<\lambda_{0\mu}\), then \(F_{\lambda,1}\) does not satisfy local LSI at \(\mu\). Suppose that \(\lambda<\lambda_{0\mu}\), i.e., there exists \(w_{0}\in\mathcal{W}\) such that

\[\Psi_{1}(\mu)^{1/2}\lambda<|G^{\prime}[\bm{h}\mu](w_{0})|\,.\]

Let us distinguish cases between \(G^{\prime}[\bm{h}\mu](w_{0})\geq 0\) or \(G^{\prime}[\bm{h}\mu](w_{0})<0\).

First suppose \(G^{\prime}[\bm{h}\mu](w_{0})\geq 0\), so that \(\Psi_{1}(\mu)^{1/2}\lambda<G^{\prime}[\bm{h}\mu](w_{0})\). By continuity of \(G^{\prime}[\bm{h}\mu]\), let \(N\subset\mathcal{W}\) an open neighborhood of \(w_{0}\) such that \(\forall w\in N,\Psi_{1}(\mu)^{1/2}\lambda<G^{\prime}[\bm{h}\mu](w)\). Then, since \(F^{\prime}_{\lambda,1}[\mu](r,w)=|r|\left(\operatorname{sign}(r)G^{\prime}[ \bm{h}\mu](w)+\lambda\Psi_{1}(\mu)^{1/2}\right)\) by (C.2),

\[\forall r\in\mathbb{R}_{-},\forall w\in N,\;F^{\prime}_{\lambda,1 }[\mu](r,w) =|r|\left(-G^{\prime}[\bm{h}\mu](w)+\lambda\Psi_{1}(\mu)^{1/2} \right)\leq 0\] \[\text{and so }\quad\int_{\mathbb{R}}\int_{\mathcal{W}}e^{-\beta F^{ \prime}_{\lambda,1}[\mu](r,w)}\mathrm{d}r\mathrm{d}w \geq\int_{\mathbb{R}_{-}}\int_{N}e^{-\beta F^{\prime}_{\lambda,1 }[\mu](r,w)}\mathrm{d}r\mathrm{d}w\] \[\geq\int_{\mathbb{R}_{-}}\int_{N}1\;\mathrm{d}r\mathrm{d}w\;=+\infty.\]

This contradicts the exponential integrability condition in the definition of local LSI, and so \(F_{\lambda,1}\) does not satisfy local LSI at \(\mu\).

Likewise, now suppose that \(G^{\prime}[\bm{h}\mu](w_{0})<0\), so that \(\Psi_{1}(\mu)^{1/2}\lambda<-G^{\prime}[\bm{h}\mu](w_{0})\). By continuity of \(G^{\prime}[\bm{h}\mu]\), let \(N\subset\mathcal{W}\) an open neighborhood of \(w_{0}\) such that \(\forall w\in N,\Psi_{1}(\mu)^{1/2}\lambda<-G^{\prime}[\bm{h}\mu](w)\). Then

\[\forall r\in\mathbb{R}_{+},\forall w\in N,\;F^{\prime}_{\lambda, 1}[\mu](r,w) =|r|\left(G^{\prime}[\bm{h}\mu](w)+\lambda\Psi_{1}(\mu)^{1/2} \right)\leq 0\] \[\text{and so }\quad\int_{\mathbb{R}}\int_{\mathcal{W}}e^{-\beta F^{ \prime}_{\lambda,1}[\mu](r,w)}\mathrm{d}r\mathrm{d}w \geq\int_{\mathbb{R}_{+}}\int_{N}e^{-\beta F^{\prime}_{\lambda,1} [\mu](r,w)}\mathrm{d}r\mathrm{d}w\] \[\geq\int_{\mathbb{R}_{+}}\int_{N}1\;\mathrm{d}r\mathrm{d}w\;=+\infty.\]

As in the previous case, we conclude that \(F_{\lambda,1}\) does not satisfy local LSI at \(\mu\). 

### Proof of Lem. C.7 via computing the Hessians under the lifted Riemannian geometry

We start by a general lemma. We use \(D\) to denote differentials, and for a function \(f:\mathbb{R}_{+}^{*}\times\mathcal{W}\to\mathbb{R}\), we will write \(D_{r}f=\frac{\partial f(r,w)}{\partial r}\) and \(D_{w}f=\frac{\partial f(r,w)}{\partial w}\).

**Lemma C.8**.: _Let \((\mathcal{W},g)\) a Riemannian manifold. Let \(\Omega_{+}^{*}=\mathbb{R}_{+}^{*}\times\mathcal{W}\) and consider_

\[\overline{g}_{(r,w)}=\begin{bmatrix}\alpha(r)^{-1}&0\\ 0&\beta(r)^{-1}g_{w}\end{bmatrix}\]

_for smooth positive functions \(\alpha,\beta:\mathbb{R}_{+}^{*}\to\mathbb{R}_{+}^{*}\). This defines a smooth Riemannian metric \(\overline{g}\) on \(\Omega_{+}^{*}\). Denote by \(\overline{g}_{(r,w)}\), \(\overline{\nabla}\), \(\overline{\Gamma}\), \(\overline{\nabla^{2}}\) the Riemannian metric, gradient, Christoffel symbols, resp. Hessian on \(\Omega_{+}^{*}\), and by \(g_{w},\nabla,\Gamma,\nabla^{2}\) the corresponding objects on the original space \(\mathcal{W}\)._

_Let \(f:\Omega_{+}^{*}\to\mathbb{R}\) a smooth scalar field. Write for convenience \(f_{r}(w)=f(r,w)\), so that for example \(\nabla f_{r}(w)=g_{w}^{-1}D_{w}f(r,w)\), and note that \(D_{r}\nabla f_{r}(w)=\nabla D_{r}f_{r}(w)\). Fix a local coordinate chart on \(\mathcal{W}\). This induces a local coordinate chart on \(\Omega_{+}^{*}\) by adding the index \(0\) for the variable \(r\). Then the Riemannian Hessian \(f\) at \((r,w)\) is given in coordinates by_

\[\overline{\nabla^{2}}\,f^{00} =\alpha(r)^{2}D_{rr}^{2}f+\frac{1}{2}\alpha(r)\alpha^{\prime}(r)D _{r}f\] \[\overline{\nabla^{2}}\,f^{i0} =\overline{\nabla^{2}}\,f^{0i} =\alpha(r)\beta(r)\nabla D_{r}f_{r}(w)^{i}+\frac{1}{2}\alpha(r) \beta^{\prime}(r)\nabla f_{r}(w)^{i}\] \[\overline{\nabla^{2}}\,f^{ij} =\beta(r)^{2}\,\nabla^{2}\,f_{r}(w)^{ij}-\frac{1}{2}\alpha(r) \beta^{\prime}(r)\cdot D_{r}f\cdot(g_{w}^{-1})^{ij}.\]

Proof.: We will use uppercase letters for indes ranging over \([0,d]\) and lowercase for \([1,d]\), with the index \(0\) corresponding to the variable \(r\); for example \(\overline{\nabla}f(r,w)^{0}=\alpha(r)D_{r}f(r,w)\). We will use Einstein summation notation freely. With slight abuse of notation we denote \((g^{ij})_{ij}=g^{-1}\) for the inverse matrix of the metric \((g_{ij})_{ij}=g\), and likewise for \(\overline{g}^{IJ},\overline{g}_{IJ}\), so that for example \(\overline{g}^{00}=\alpha(r)\).

We start by using that [18, Example 4.22, Eq. (5.10)]

\[\overline{\nabla^{2}}\,f(r,w)^{IJ} =\overline{g}^{IK}\overline{g}^{JL}\left[\frac{\partial^{2}f}{ \partial\omega^{K}\partial\omega^{L}}-\overline{\Gamma}^{M}_{KL}\frac{ \partial f}{\partial^{M}\omega}\right]\] \[\text{and}\quad\overline{\Gamma}^{M}_{IJ} =\frac{1}{2}\overline{g}^{MK}\left[\frac{\partial\overline{g}_{ KI}}{\partial\omega^{J}}+\frac{\partial\overline{g}_{KJ}}{\partial\omega^{I}}- \frac{\partial\overline{g}_{IJ}}{\partial\omega^{K}}\right]\]

where \(\omega=(r,w)\), and that the analogous formulas hold for \(f_{r}:\mathcal{W}\to\mathbb{R}\) for all \(r\) and for \(\Gamma^{m}_{ij}\) the Christoffel symbols of \(\mathcal{W}\).

By direct computations, we find that for all \(i,j,m\in[1,d]\),

\[\overline{\Gamma}^{0}_{00} =-\frac{1}{2}\frac{\alpha^{\prime}(r)}{\alpha(r)} \overline{\Gamma}^{0}_{i0} =\overline{\Gamma}^{0}_{0i} =0 \overline{\Gamma}^{0}_{ij} =\frac{1}{2}\alpha(r)\frac{\beta^{\prime}(r)}{\beta(r)^{2}}g_{ij}\] \[\overline{\Gamma}^{m}_{00} =0 \overline{\Gamma}^{m}_{i0} =\overline{\Gamma}^{m}_{0i} =-\frac{1}{2}\frac{\beta^{\prime}(r)}{\beta(r)}\delta^{m}_{i} \overline{\Gamma}^{m}_{ij} =\Gamma^{m}_{ij}.\]

So by direct computations, we find that

\[\overline{\nabla^{2}}\,f^{00} =\alpha(r)^{2}D_{rr}^{2}f+\frac{1}{2}\alpha(r)\alpha^{\prime}(r)D _{r}f\] \[\overline{\nabla^{2}}\,f^{i0} =\overline{\nabla^{2}}\,f^{0i} =\alpha(r)\beta(r)\nabla D_{r}f_{r}(w)^{i}+\frac{1}{2}\alpha(r) \beta^{\prime}(r)\nabla f_{r}(w)^{i}\] \[\overline{\nabla^{2}}\,f^{ij} =\beta(r)^{2}\,\nabla^{2}\,f_{r}(w)^{ij}-\frac{1}{2}\alpha(r) \beta^{\prime}(r)\cdot D_{r}f\cdot g^{ij},\]

as announced. 

**Corollary C.9**.: _Let \(f:\Omega^{*}_{+}=\mathbb{R}^{*}_{+}\times\mathcal{W}\to\mathbb{R}\) defined by \(f(r,w)=r^{p}\tilde{\phi}(w)+\lambda^{\prime}r^{pb}\), for some \(p>0\), \(b\in[1,2]\), \(\lambda^{\prime}\geq 0\) and \(\tilde{\phi}:\mathcal{W}\to\mathbb{R}\)._

_Consider \(\Omega^{*}_{+}\) equipped with the Riemannian metric (3.2). Then the Riemannian Hessian of \(f\) is given in coordinates by_

\[\overline{\nabla^{2}}\,f^{00} =\Gamma^{2}p(p-q_{r}/2)r^{2-2q_{r}+p}\tilde{\phi}(w)+\Gamma^{2}pb \lambda^{\prime}(pb-q_{r}/2)r^{2-2q_{r}+pb}\] \[\overline{\nabla^{2}}\,f^{i0} =\overline{\nabla^{2}}\,f^{0i} =\Gamma(p-q_{w}/2)r^{1-q_{r}-q_{w}+p}\nabla\tilde{\phi}(w)^{i}\] \[\overline{\nabla^{2}}\,f^{ij} =r^{p-2q_{w}}\,\nabla^{2}\,\tilde{\phi}(w)^{ij}+\frac{1}{2}\Gamma q _{w}r^{-q_{r}-q_{w}}\cdot\left(pr^{p}\tilde{\phi}(w)+pb\lambda^{\prime}r^{pb} \right)(g_{w}^{-1})^{ij}.\]

Proof.: Continuing with the same notations as in the proof of the lemma above, we have

\[D_{r}f =pr^{p-1}\tilde{\phi}(w)+pb\lambda^{\prime}r^{pb-1} \qquad D_{rr}^{2}f =p(p-1)r^{p-2}\tilde{\phi}(w)+pb(pb-1)\lambda^{\prime}r^{pb-2}\] \[\nabla f_{r}(w)^{i} =r^{p}\nabla\tilde{\phi}(w)^{i} \nabla^{2}\,f_{r}(w)^{ij} =r^{p}\,\nabla^{2}\,\tilde{\phi}(w)^{ij}\] \[\nabla D_{r}f_{r}(w)^{i} =pr^{p-1}\nabla\tilde{\phi}(w)^{i}\]

and so

\[\overline{\nabla^{2}}\,f^{00} =\alpha(r)^{2}\left(p(p-1)r^{p-2}\tilde{\phi}(w)+pb(pb-1)\lambda ^{\prime}r^{pb-2}\right)+\frac{1}{2}\alpha(r)\alpha^{\prime}(r)\left(pr^{p-1} \tilde{\phi}(w)+pb\lambda^{\prime}r^{pb-1}\right)\] \[=\alpha(r)p\left(\alpha(r)(p-1)+\frac{1}{2}r\alpha^{\prime}(r) \right)r^{p-2}\tilde{\phi}(w)+\alpha(r)pb\lambda^{\prime}\left(\alpha(r)(pb-1)+ \frac{1}{2}r\alpha^{\prime}(r)\right)r^{pb-2}\] \[\overline{\nabla^{2}}\,f^{i0} =\overline{\nabla^{2}}\,f^{0i}=\alpha(r)\beta(r)\cdot pr^{p-1} \nabla\tilde{\phi}(w)^{i}+\frac{1}{2}\alpha(r)\beta^{\prime}(r)\cdot r^{p} \nabla\tilde{\phi}(w)^{i}\] \[=\alpha(r)\left(\beta(r)p+\frac{1}{2}r\beta^{\prime}(r)\right)r^{ p-1}\nabla\tilde{\phi}(w)^{i}\] \[\overline{\nabla^{2}}\,f^{ij} =\beta(r)^{2}\cdot r^{p}\,\nabla^{2}\,\tilde{\phi}(w)^{ij}-\frac{ 1}{2}\alpha(r)\beta^{\prime}(r)\cdot\left(pr^{p-1}\tilde{\phi}(w)+pb\lambda^ {\prime}r^{pb-1}\right)\cdot g^{ij}.\]

By substituting \(\alpha(r)^{-1}=\Gamma^{-1}r^{q_{r}-2}\) and \(\beta(r)^{-1}=r^{q_{w}}\), i.e. \(\alpha(r)=\Gamma r^{2-q_{r}}\) and \(\beta(r)=r^{-q_{w}}\), we obtain the announced formulas.

Proof of Lem. C.7.: Continuing with the same notations as in the proofs of the lemma and of the corollary above, note that \(f:\Omega_{+}^{\ast}=\mathbb{R}_{+}^{\ast}\times\mathcal{W}\to\mathbb{R}\) having Lipschitz-continuous gradients in the Riemannian sense is equivalent to [10, Coroll. 10.47]

\[\sup_{\omega\in\Omega_{+}^{\ast}}\ \sup_{\begin{subarray}{c}s\in T_{u}\Omega_{+}^{ \ast}\\ \left\|s\right\|_{\omega}=1\end{subarray}}\left\|\nabla^{2}\,f(\omega)^{IJ} \overline{g}_{JK}s^{K}\right\|_{\omega}<\infty.\]

Rewriting everything in coordinates, this means that the matrix \(\widetilde{H}(\omega)=\left(\sqrt{\widetilde{g}}_{IK}\ \overline{\nabla^{2}}\,f(\omega)^{IJ}\ \sqrt{g}_{JL}\right)_{KL}\in\mathbb{R}^{(d+1)\times(d+1)}\) must be bounded, uniformly in \(\omega\in\Omega_{+}^{\ast}\), where \((\sqrt{g}_{IJ})_{IJ}=\sqrt{\widetilde{g}}\) denotes the square root of the positive-definite matrix \(\overline{g}\) (pointwise for each \(\omega\)). Concretely, for all \(i,j\in[1,d]\),

\[\sqrt{\widetilde{g}}_{00}=\alpha(r)^{-1/2}=\Gamma^{-1/2}r^{q_{r}/2-1},\ \ \ \ \sqrt{\widetilde{g}}_{i0}=0,\ \ \ \ \sqrt{\widetilde{g}}_{ij}=\beta(r)^{-1/2}\sqrt{g}_{ij}=r^{q_{\omega}/2}\sqrt {g}_{ij}\]

and

\[\widetilde{H}(\omega)_{00} =\overline{g}_{00}\ \overline{\nabla^{2}}\,f^{00}\] \[=\Gamma p(p-q_{r}/2)r^{-q_{r}+p}\tilde{\phi}(w)+\Gamma pb\lambda^ {\prime}(pb-q_{r}/2)r^{-q_{r}+pb}\] \[\widetilde{H}(\omega)_{j0} =\sqrt{\widetilde{g}}_{00}\sqrt{\widetilde{g}}_{ji}\ \overline{\nabla^{2}}\,f^{i0}\] \[=\Gamma^{1/2}(p-q_{w}/2)r^{-q_{r}/2-q_{w}/2+p}\cdot\sqrt{g}_{ji} \nabla\tilde{\phi}(w)^{i}\] \[\widetilde{H}(\omega)_{kl} =\sqrt{\widetilde{g}}_{ki}\sqrt{\widetilde{g}}_{lj}\ \overline{\nabla^{2}}\,f^{ij}\] \[=r^{p-q_{w}}\cdot\sqrt{g}_{ki}\sqrt{g}_{lj}\ \nabla^{2}\,\tilde{\phi}(w)^{ij}+\Gamma\frac{1}{2}q_{w}r^{-q_{r}}\cdot\left( pr^{p}\tilde{\phi}(w)+pb\lambda^{\prime}r^{pb}\right)\delta_{kl}.\]

(Note that here the indels do not respect the covariant/contravariant convention, i.e., "\(\sqrt{\widetilde{g}}_{IK}\)" and "\(\widetilde{H}(\omega)_{KL}\)" do not stand for covariant tensors: we really manipulate everything in coordinates explicitly.)

Now, note that the desired condition means that \(\widetilde{H}(\omega)_{KL}\) should remain bounded both for \(r\to+\infty\) and \(r\to 0\). That is, the exponents of \(r\) in the non-zero terms must all be \(0\). Thus, since we assume that \(\lambda^{\prime}\neq 0\), and that \(\nabla^{2}\tilde{\phi}\) is not constant equal to \(0\) and so in particular \(\tilde{\phi}\) and \(\nabla\tilde{\phi}\) are not constant,

* Uniform boundedness of the second term in \(\widetilde{H}(\omega)_{kl}\) implies that \(b=1\). Indeed \(\lambda^{\prime}\neq 0\), and the first term (in \(\nabla^{2}\tilde{\phi}\)) cannot cancel out both the term in \(\tilde{\phi}(w)r^{p-q_{r}}\) and the term in \(\lambda^{\prime}r^{pb-q_{r}}\) if they scale differently with \(r\). This also implies that either \(p=q_{w}=q_{r}\) or that \(q_{w}=q_{r}\) and \(\nabla^{2}\tilde{\phi}(w)^{ij}=\frac{1}{2}\Gamma q_{w}p\left(\tilde{\phi}(w)+ \lambda^{\prime}\right)g^{ij}\) for all \(w\).
* Uniform boundedness of \(\widetilde{H}(\omega)_{00}\) implies that \(p=q_{r}\) or \(p=q_{r}/2\).
* Uniform boundedness of \(\widetilde{H}(\omega)j0\) implies that \(p=\frac{q_{r}+q_{w}}{2}\) or \(p=q_{w}/2\). We saw in the first point that \(q_{r}=q_{w}\), so equivalently \(p=q_{r}=q_{w}\) or \(p=q_{r}/2=q_{w}/2\).

Thus we get that \(f\) can have Lipschitz-continuous Riemannian gradients only if \(b=1\) and \(p=q_{r}=q_{w}\), or if \(b=1\) and \(p=q_{r}/2=q_{w}/2\) and \(\nabla^{2}\tilde{\phi}(w)=\Gamma p^{2}\left(\tilde{\phi}(w)+\lambda^{\prime} \right)g_{w}\) for all \(w\). 

## Appendix D Details for Sec. 3.2 (reduction by bilevel optimization)

### Proof of Prop. 3.3

In preparation for the proof of Prop. 3.3, let us first provide a formal proof of the variational representation of the squared-TV norm mentioned at the beginning of Sec. 3.2, with a characterization of the set of minimizers. See [17, App. 1] for the rigorous justification of these arguments in the more general context of minimization of convex and positively \(1\)-homogeneous integral functionals over the space of signed measures.

**Lemma D.1** ("\(\eta\)-trick" for the squared TV-norm).: _We have_

\[\left\|\nu\right\|_{TV}^{2}=\left(\int_{\mathcal{W}}\left|\nu(dw)\right|\right)^ {2}=\inf_{\eta\in\mathcal{P}(\mathcal{W})}\int_{\mathcal{W}}\frac{\left|\nu(dw) \right|^{2}}{\eta(dw)}=\inf_{\begin{subarray}{c}\eta\in\mathcal{P}(\mathcal{W }),\ f:\mathcal{W}\rightarrow\mathbb{R}\\ \text{s.t.}\ f\eta=\nu\end{subarray}}\int_{\mathcal{W}}\left|f\right|^{2}d\eta.\]

_Moreover the infimum in the third expression is attained at (and only at) \(\eta(dw)=\frac{\left|\nu(dw)\right|}{\left\|\nu\right\|_{TV}}\), and the infimum in the fourth expression is attained at (and only at) the same \(\eta\) and \(f=\frac{\nu(dw)}{\left|\nu(dw)\right|}\left\|\nu\right\|_{TV}\)._

Proof.: The infimum in the third expression is the value of a convex constrained minimization problem, whose Lagrangian is \(\mathcal{L}(\eta;\lambda)=\int\frac{\left|\nu\right|^{2}}{\eta}+\lambda\left( \int\mathrm{d}\eta-1\right)\). The dual optimality condition implies \(\forall w\in\mathrm{supp}(\eta),\lambda=\frac{\mathrm{d}\nu}{\mathrm{d}\eta} (w)^{2}\), so the infinimum is attained at \(\eta(\mathrm{d}w)=\frac{\left|\nu(\mathrm{d}w)\right|}{\left\|\nu\right\|_{ TV}}\), with optimal value \(\left\|\nu\right\|_{TV}^{2}\).

The optimality condition for the infimum in the fourth expression follows directly from the one for the third expression and from the constraint \(f\eta=\nu\). 

Proof of Prop. 3.3.: By the lemma above,

\[\inf_{\eta\in\mathcal{P}(\mathcal{W})}J_{\lambda}(\eta) =\inf_{\eta\in\mathcal{P}(\mathcal{W}),f:\mathcal{W}\rightarrow \mathbb{R}}\ G(f\eta)+\frac{\lambda}{2}\int_{\mathcal{W}}\left|f\right|^{2}d\eta\] \[=\inf_{\nu\in\mathcal{M}(\mathcal{W})}\inf_{\begin{subarray}{c} \eta\in\mathcal{P}(\mathcal{W}),\ f:\mathcal{W}\rightarrow\mathbb{R}\\ \text{s.t.}\ f\eta=\nu\end{subarray}}\ G(f\eta)+\frac{\lambda}{2}\int_{ \mathcal{W}}\left|f\right|^{2}d\eta\] \[=\inf_{\nu\in\mathcal{M}(\mathcal{W})}G(\nu)+\frac{\lambda}{2} \left[\inf_{\begin{subarray}{c}\eta\in\mathcal{P}(\mathcal{W}),\ f:\mathcal{W} \rightarrow\mathbb{R}\\ \text{s.t.}\ f\eta=\nu\end{subarray}}\ \int_{\mathcal{W}}\left|f\right|^{2}d\eta\right]\] \[=\inf_{\nu\in\mathcal{M}(\mathcal{W})}G(\nu)+\frac{\lambda}{2} \left\|\nu\right\|_{TV}^{2}\ =\inf_{\nu\in\mathcal{M}(\mathcal{W})}G_{\lambda}(\nu).\]

Hence the equality of the optimal values. The claimed characterization of \(\arg\min J_{\lambda}\) in terms of \(\arg\min G_{\lambda}\) follows from the characterization of the minimizers of the inner minimization \(\left[\inf_{\begin{subarray}{c}\eta\in\mathcal{P}(\mathcal{W}),\ f:\mathcal{W} \rightarrow\mathbb{R}\\ \text{s.t.}\ f\eta=\nu\end{subarray}}\frac{\lambda}{2}\int_{\mathcal{W}} \left|f\right|^{2}d\eta\right]\) in the third line, which is given by the lemma above.

Furthermore, \(J_{\lambda}\) is convex as the partial minimization of \((\eta,\nu)\mapsto G(\nu)+\frac{\lambda}{2}\int\frac{\left|\nu\right|^{2}}{\eta}\), which is jointly convex. 

### Proof of the explicit form of the two-timescale SDE (3.4)

For ease of reference, we recall here the two-timescale SDE (3.4):

\[\forall i\leq N,\begin{cases}\mathrm{d}r_{t}^{i}=-\Gamma\ \nabla_{r^{i}}F_{\lambda,2}^{\prime}\left[\frac{1}{N}\sum_{j=1}^{N}\delta_{(r_ {t}^{j},w_{t}^{j})}\right](r_{t}^{i},w_{t}^{i})\mathrm{d}t\\ \mathrm{d}w_{t}^{i}=-\nabla_{w^{i}}F_{\lambda,2}^{\prime}\left[\frac{1}{N}\sum _{j=1}^{N}\delta_{(r_{t}^{j},w_{t}^{j})}\right](r_{t}^{i},w_{t}^{i})\mathrm{d }t+\sqrt{2\beta^{-1}}\mathrm{d}B_{t}^{i}.\end{cases}\]

By (C.2) with \(b=2\) and \(p=1\),

\[F_{\lambda,2}^{\prime}[\mu](r,w) =rG^{\prime}[\bm{h}\mu](w)+\frac{\lambda}{2}\left|r\right|^{2}\] \[\text{so}\quad\nabla_{r}F_{\lambda,2}^{\prime}[\mu](r,w) =G^{\prime}[\bm{h}\mu](w)+\lambda r\] \[\text{and}\quad\nabla_{w}F_{\lambda,2}^{\prime}[\mu](r,w) =r\nabla G^{\prime}[\bm{h}\mu](w).\]

Finally, by definition \(\bm{h}\left[\frac{1}{N}\sum_{j=1}^{N}\delta_{(r^{j},w^{j})}\right]=\frac{1}{N} \sum_{j=1}^{N}r^{j}\delta_{w^{j}}\). Hence the second part of (3.4).

### Proof of Prop. 3.4 (\(J_{\lambda}\) satisfies P0, P1 and P2)

Simplifying the expression of the bilevel objective.The following expressions will be useful throughout our analyses of the bilevel problem (3.3).

**Proposition D.2**.: _We have that \(J_{\lambda}(\eta)=G(f_{\eta}\eta)+\frac{\lambda}{2}\int\left|f_{\eta}\right|^{2} \mathrm{d}\eta\) where \(f_{\eta}\) is the unique solution of the fixed-point equation_

\[\forall w\in\mathcal{W},\;f_{\eta}(w)=-\frac{1}{\lambda}G^{\prime}[f_{\eta} \eta](w).\] (D.1)

_Furthermore,_

\[J_{\lambda}^{\prime}[\eta](w)=-\frac{\lambda}{2}\left|f_{\eta}\right|^{2}(w).\] (D.2)

Proof.: Consider the optimization problem defining \(J_{\lambda}(\eta)\), for a fixed \(\eta\),

\[\min_{f\in L_{\eta}^{2}(\mathcal{W})}G(f\eta)+\frac{\lambda}{2}\int_{\mathcal{ W}}\left|f\right|^{2}\mathrm{d}\eta.\]

This problem is convex since \(G\) is, and strongly convex in \(L_{\eta}^{2}(\mathcal{W})\) thanks to the term in \(\lambda\). So there exists a unique solution which we denote by \(\tilde{f}_{\eta}\in L_{\eta}^{2}(\mathcal{W})\), and it is characterized by the first-order optimality condition:

\[G^{\prime}[\tilde{f}_{\eta}\;\eta]\;\eta+\lambda\tilde{f}_{\eta}\;\eta=0\; \;\text{in}\;\mathcal{M}(\mathcal{W}).\]

Now let \(f_{\eta}=-\frac{1}{\lambda}G^{\prime}[\tilde{f}_{\eta}\eta]\), which is defined over all of \(\mathcal{W}\). Then \(f_{\eta}\) satisfies the fixed-point equation (D.1) by construction. Conversely, for any solution \(g_{\eta}\) of (D.1), its restriction to \(\mathrm{supp}(\eta)\) viewed as an element \(\tilde{g}_{\eta}\) of \(L_{\eta}^{2}(\mathcal{W})\) must in particular satisfy \(G^{\prime}[\tilde{g}_{\eta}\eta]\eta+\lambda\tilde{g}_{\eta}\eta=0\) in \(\mathcal{M}(\mathcal{W})\), and so \(\tilde{g}_{\eta}=\tilde{f}_{\eta}\), and so \(g_{\eta}=-\frac{1}{\lambda}G^{\prime}[\tilde{g}_{\eta}\eta]=-\frac{1}{\lambda} G^{\prime}[\tilde{f}_{\eta}\eta]=f_{\eta}\).

Furthermore, by differentiability of \(G\) then \(\eta\mapsto\tilde{f}_{\eta}\) is continuous (in the total variation sense). So in turn, \(\eta\mapsto f_{\eta}(w)\) the unique solution of (D.1) is continuous for each \(w\) (in the total variation sense). So by the envelope theorem, since for any fixed \(f\) the first variation of \(\eta\mapsto G(f\eta)+\frac{\lambda}{2}\int\left|f\right|^{2}\mathrm{d}\eta\) is \(w\mapsto f(w)G^{\prime}[f\eta](w)+\frac{\lambda}{2}\left|f(w)\right|^{2}\),

\[J_{\lambda}^{\prime}[\eta](w) =f_{\eta}(w)G^{\prime}[f_{\eta}\eta](w)+\frac{\lambda}{2}\left|f _{\eta}(w)\right|^{2}\] \[=-\frac{\lambda}{2}\left|f_{\eta}(w)\right|^{2}=-\frac{1}{2 \lambda}\left|G^{\prime}[f_{\eta}\eta]\right|^{2}(w),\]

which is precisely Eq. (D.2). 

We remark that the above manipulations rely crucially on the fact that the optimization problem (1.1) is over signed measures and not just non-negative measures - as otherwise we would additionally need to constrain \(f\geq 0\) -, and on the regularization term being \(\left\|\nu\right\|_{TV}^{2}\) instead of \(\left\|\nu\right\|_{TV}\).

**Preliminary estimates.**

**Lemma D.3**.: _Under Assumption 1, for any \(\nu\in\mathcal{M}(\mathcal{W})\), we have_

\[\sup_{w\in\mathcal{W}}\left|G^{\prime}[\nu](w)\right|^{2}\leq 2L_{0}G(\nu).\]

Proof.: We follow the proof technique of [1, Appendix D]. Let \(w_{0}\in\mathcal{W}\) and \(\nu^{\prime}=\nu-\frac{1}{L_{0}}G^{\prime}[\nu](w_{0})\delta_{w_{0}}\). By mean-value theorem there exists \(\theta\in(0,1)\) such that \(G(\nu^{\prime})-G(\nu)=\int G^{\prime}[\nu+\theta(\nu^{\prime}-\nu)]\mathrm{d} (\nu^{\prime}-\nu)\), and so

\[\inf G\leq G(\nu^{\prime}) \leq G(\nu)+\int G^{\prime}[\nu]\mathrm{d}(\nu^{\prime}-\nu)+ \frac{L_{0}}{2}\left\|\nu^{\prime}-\nu\right\|_{TV}^{2}\] \[=G(\nu)-\frac{1}{L_{0}}G^{\prime}[\nu](w_{0})^{2}+\frac{1}{2L_{0 }}G^{\prime}[\nu](w_{0})^{2}=G(\nu)-\frac{1}{2L_{0}}G^{\prime}[\nu](w_{0})^{2}.\]

Hence, since \(G\) is non-negative by Assumption 1,

\[\forall w\in\mathcal{W},\;\frac{1}{2L_{0}}G^{\prime}[\nu](w)^{2}\leq G(\nu)- \inf G\leq G(\nu)\qed\]

**Lemma D.4**.: _Under Assumption 1, let \(\eta\in\mathcal{P}(\mathcal{W})\) and let \(f_{\eta}\) as in (D.1). Then_

\[\sup_{\mathcal{W}}\left|f_{\eta}\right|\leq\frac{1}{\lambda}\sqrt{2L_{0}J_{ \lambda}(\eta)}\]

_and for each \(i\in\{1,2\}\),_

\[\sup_{w\in\mathcal{W}}\left\|\nabla^{i}f_{\eta}\right\|_{w}\leq\frac{L_{i}}{ \lambda^{2}}\sqrt{2L_{0}J_{\lambda}(\eta)}+\frac{B_{i}}{\lambda}.\]

_Moreover, \(J_{\lambda}(\eta)\leq G(0)\) for all \(\eta\in\mathcal{P}(\mathcal{W})\)._

Proof.: For the first inequality, by definition \(G^{\prime}[f_{\eta}\eta]=-\lambda f_{\eta}\) for all \(w\in\mathcal{W}\), so

\[\lambda^{2}\left|f_{\eta}(w)\right|^{2}=\left|G^{\prime}[f_{\eta}\eta](w) \right|^{2}\leq 2L_{0}G(f_{\eta}\eta)\leq 2L_{0}\left(G(f_{\eta}\eta)+\frac{ \lambda}{2}\int\left|f_{\eta}\right|^{2}\mathrm{d}\eta\right)=2L_{0}J_{ \lambda}(\eta)\]

where the first inequality follows from Lem. D.3.

For the second part, by Assumption 1, \(\forall\nu\in\mathcal{M}(\mathcal{W})\), \(\sup_{w}\left\|\nabla^{i}G^{\prime}[\nu]\right\|_{w}\leq L_{i}\left\|\nu\right\| _{TV}+B_{i}\), so

\[\lambda\left\|\nabla^{i}f_{\eta}\right\|_{w}=\left\|\nabla^{i}G^ {\prime}[f_{\eta}\eta]\right\|_{w} \leq B_{i}+L_{i}\left\|f_{\eta}\eta\right\|_{TV}=B_{i}+L_{i}\int \left|f_{\eta}\right|\mathrm{d}\eta\] \[\leq B_{i}+L_{i}\sup_{\mathcal{W}}\left|f_{\eta}\right|\leq B_{i}+L _{i}\frac{1}{\lambda}\sqrt{2L_{0}J_{\lambda}(\eta)}\]

by the first part of the lemma.

Finally, the uniform bound on \(J_{\lambda}(\eta)\) follows by taking \(f=0\) in the infimum defining \(J_{\lambda}\): \(J_{\lambda}(\eta)=\inf_{f\in L_{\eta}^{2}}G(f\eta)+\frac{\lambda}{2}\int\left| f\right|^{2}\mathrm{d}\eta\leq G(0)\). 

**Lemma D.5**.: _Under Assumption 1, \(J_{\lambda}:\mathcal{P}(\mathcal{W})\to\mathbb{R}\) is weakly continuous and_

\[\forall\eta,\eta^{\prime}\in\mathcal{P}(\mathcal{W}),\ |J_{\lambda}(\eta)-J_{ \lambda}(\eta^{\prime})|\leq BW_{2}(\eta,\eta^{\prime})\]

_where \(B=\sqrt{2L_{0}G(0)}\cdot\left(\frac{L_{1}}{\lambda^{2}}\sqrt{2L_{0}G(0)}+\frac {B_{1}}{\lambda}\right)\)._

Proof.: For any \(\eta\in\mathcal{P}(\mathcal{W})\), letting \(f_{\eta}\) as in (D.1), we have \(J_{\lambda}^{\prime}[\eta](w)=-\frac{\lambda}{2}\left|f_{\eta}\right|^{2}(w)\) so

\[\nabla J_{\lambda}^{\prime}[\eta](w) =-\lambda f_{\eta}(w)\nabla f_{\eta}(w)\] \[\left\|\nabla J_{\lambda}^{\prime}[\eta](w)\right\|_{w} \leq\lambda\sup_{\mathcal{W}}\left|f_{\eta}\right|\cdot\sup_{ \mathcal{W}}\left\|\nabla f_{\eta}\right\|\] \[\leq\lambda\cdot\frac{1}{\lambda}\sqrt{2L_{0}G(0)}\cdot\left( \frac{L_{1}}{\lambda^{2}}\sqrt{2L_{0}G(0)}+\frac{B_{1}}{\lambda}\right)=:B<\infty\]

by Lem. D.4, uniformly in \(\eta\in\mathcal{P}(\mathcal{W})\) and \(w\in\mathcal{W}\). So by Lem. D.8 below, we have \(|J_{\lambda}(\eta)-J_{\lambda}(\eta^{\prime})|\leq BW_{2}(\eta,\eta^{\prime})\) for all \(\eta,\eta^{\prime}\in\mathcal{P}(\mathcal{W})\). Moreover \(W_{2}\) metrizes weak convergence, so \(J_{\lambda}\) is weakly continuous. 

**Lemma D.6**.: _Under Assumption 1, let \(w^{\prime}\in\mathcal{W}\) and \(\eta\in\mathcal{P}(\mathcal{W})\). Let \(h:\mathcal{W}\to\mathbb{R}\) and suppose that_

\[\forall w\in\mathcal{W},\ \lambda h(w)+\int G^{\prime\prime}[f_{\eta}\eta](w,w^{ \prime\prime})d\eta(w^{\prime\prime})h(w^{\prime\prime})=-G^{\prime\prime}[f_{ \eta}\eta](w,w^{\prime})f_{\eta}(w^{\prime}).\]

_Then \(\sup_{w\in\mathcal{W}}\left|h(w)\right|\leq\left(1+\frac{L_{0}}{\lambda}\right) \frac{L_{0}}{\lambda}\sqrt{2L_{0}G(0)}\)._

_Alternatively, suppose that there exists \(s\in T_{w^{\prime}}\mathcal{W}\) with \(\left\|s\right\|_{w^{\prime}}=1\) such that_

\[\forall w\in\mathcal{W},\ \lambda h(w)+\int G^{\prime\prime}[f_{\eta}\eta](w,w^{ \prime\prime})d\eta(w^{\prime\prime})h(w^{\prime\prime})=-\left\langle s^{ \prime},\nabla_{w^{\prime}}\left[G^{\prime\prime}[f_{\eta}\eta](w,w^{\prime})f_ {\eta}(w^{\prime})\right]\right\rangle_{w^{\prime}}.\]

_Then \(\sup_{w\in\mathcal{W}}\left|h(w)\right|\leq\left(1+\frac{L_{0}}{\lambda}\right) \cdot\left(\left(1+\frac{L_{0}}{\lambda}\right)\frac{L_{1}}{\lambda}\sqrt{2L_{0}G (0)}+\frac{L_{0}B_{1}}{\lambda}\right)\)._Proof.: Let \(\mathcal{G}:L^{2}_{\eta}(\mathcal{W})\to L^{2}_{\eta}(\mathcal{W})\) the operator

\[(\mathcal{G}\tilde{h})(w)=\int G^{\prime\prime}[f_{\eta}\eta](w,w^{\prime\prime} )d\eta(w^{\prime\prime})\tilde{h}(w^{\prime\prime}).\]

\(\mathcal{G}\) is well-defined as a bounded operator, since Assumption 1 implies that \(|G^{\prime\prime}[f_{\eta}\eta](w,w^{\prime})|\leq L_{0}\). Note that \(G^{\prime\prime}[f_{\eta}\eta](w,w^{\prime\prime})\) is symmetric in \(w\) and \(w^{\prime\prime}\), and that by convexity of \(G\), \(G^{\prime\prime}[f_{\eta}\eta](w,w^{\prime\prime})\geq 0\) for all \(w,w^{\prime\prime}\). Consequently, \(\mathcal{G}\) is a symmetric positive-semi-definite operator from \(L^{2}_{\eta}(\mathcal{W})\) to itself.

On the other hand, let \(V_{1}(\cdot)=-G^{\prime\prime}[f_{\eta}\eta](\cdot,w^{\prime})f_{\eta}(w^{ \prime})\). By Lem. D.4 we have

\[\left\|V_{1}\right\|_{L^{2}_{\eta}}\leq\sup_{\mathcal{W}}|V_{1}| \leq\sup_{\mathcal{W}\times\mathcal{W}}|G^{\prime\prime}[f_{\eta}\eta]|\cdot \sup_{\mathcal{W}}|f_{\eta}|\leq L_{0}\cdot\frac{1}{\lambda}\sqrt{2L_{0}G(0)} =:\overline{V}_{1}.\]

Also let \(V_{2}(\cdot)=-\left<s^{\prime},\nabla_{w^{\prime}}\left[G^{\prime\prime}[f_{ \eta}\eta](\cdot,w^{\prime})f_{\eta}(w^{\prime})]\right>_{w^{\prime}}\). Then by Lem. D.4,

\[\left\|V_{2}\right\|_{L^{2}_{\eta}}\leq\sup_{\mathcal{W}}|V_{2}| \leq\sup_{w,w^{\prime}}\left\|\nabla_{w^{\prime}}G^{\prime\prime}[f_{ \eta}\eta](w,w^{\prime})\right\|\cdot\sup_{\mathcal{W}}|f_{\eta}|+\sup_{ \mathcal{W}\times\mathcal{W}}|G^{\prime\prime}[f_{\eta}\eta]|\cdot\sup_{ \mathcal{W}}\left\|\nabla f_{\eta}\right\|\] \[\leq L_{1}\cdot\frac{1}{\lambda}\sqrt{2L_{0}G(0)}+L_{0}\cdot \left(\frac{L_{1}}{\lambda^{2}}\sqrt{2L_{0}G(0)}+\frac{B_{1}}{\lambda}\right)\] \[=\left(1+\frac{L_{0}}{\lambda}\right)\frac{L_{1}}{\lambda}\sqrt{ 2L_{0}G(0)}+\frac{L_{0}B_{1}}{\lambda}=:\overline{V}_{2}.\]

Denote by \(\tilde{h}\) the restriction of \(h\) to \(\operatorname{supp}(\eta)\) viewed as an element of \(L^{2}_{\eta}(\mathcal{W})\). Then, denoting by \(\operatorname{id}\) the identity operator on \(L^{2}_{\eta}(\mathcal{W})\), we may rewrite the assumption as \((\lambda\operatorname{id}+\mathcal{G})\tilde{h}=V_{j}\) for \(j=1\) or \(2\), and so

\[\sqrt{\int|h|^{2}\operatorname{d}\eta}=\left\|\tilde{h}\right\|_{L^{2}_{\eta}} =\left\|(\lambda\operatorname{id}+\mathcal{G})^{-1}V_{j}\right\|_{L^{2}_{\eta} }\leq\lambda^{-1}\left\|V_{j}\right\|_{L^{2}_{\eta}}\leq\lambda^{-1}\overline{ V}_{j}\]

since \(\mathcal{G}\) is positive-semi-definite and \(\lambda>0\). Thus for any \(w\in\mathcal{W}\), we get the point-wise bound

\[\lambda h(w) =V_{j}(w)-\int d\eta(w^{\prime\prime})G^{\prime\prime}[f_{\eta} \eta](w,w^{\prime\prime})h(w^{\prime\prime})\] \[\lambda\left|h(w)\right| \leq|V_{j}(w)|+\int d\eta(w^{\prime\prime})\left|G^{\prime\prime}[ f_{\eta}\eta](w,w^{\prime\prime})\right|\left|h(w^{\prime\prime})\right|\] \[\leq\overline{V}_{j}+\left\|G^{\prime\prime}[f_{\eta}\eta](w, \cdot)\right\|_{L^{2}_{\eta}}\left\|h\right\|_{L^{2}_{\eta}}\] \[\leq\overline{V}_{j}+L_{0}\cdot\lambda^{-1}\overline{V}_{j}.\qed\]

**Lemma D.7**.: _Under Assumption 1, let \(\eta,\eta^{\prime}\in\mathcal{P}(\mathcal{W})\) and let \(f_{\eta},f_{\eta^{\prime}}\) as in (D.1). Then there exist constants \(H,H^{\prime}\) dependent only on \(\lambda^{-1},G(0)\) and \(L_{0},L_{1},B_{1},\widetilde{L}_{2}\) such that_

\[\sup_{\mathcal{W}}|f_{\eta}-f_{\eta^{\prime}}|\leq HW_{2}(\eta,\eta^{\prime}) \qquad\text{and}\qquad\sup_{w\in\mathcal{W}}\left\|\nabla f_{\eta}-\nabla f_{ \eta^{\prime}}\right\|_{w}\leq H^{\prime}W_{2}(\eta,\eta^{\prime}).\]

Proof.: For each \(w\in\mathcal{W}\), we denote the first variation of \(\eta\mapsto f_{\eta}(w)\) by \(w^{\prime}\mapsto\frac{\delta f_{\eta}(w)}{\delta\eta(\operatorname{d}w^{\prime})}\). Let us show that this quantity is uniformly bounded.8 By definition, for any \(w\in\mathcal{W}\) and \(\eta\in\mathcal{P}(\mathcal{W})\) and \(w^{\prime}\in\mathcal{W}\),

Footnote 8: The rigorous proof that the first variation \(\frac{\delta f_{\eta}(w)}{\delta\eta(\operatorname{d}w^{\prime})}\) is well-defined for all \(w,w^{\prime}\in\mathcal{W}\) and \(\eta\in\mathcal{P}(\mathcal{W})\) would follow from the same derivations as for the uniform bound, so we omit it here.

\[\lambda f_{\eta}(w)+G^{\prime}[f_{\eta}\eta](w)=0\] so \[\lambda\frac{\delta f_{\eta}(w)}{\delta\eta(w^{\prime})}+G^{\prime \prime}[f_{\eta}\eta](w,w^{\prime})f_{\eta}(w^{\prime})+\int\left(G^{\prime \prime}[f_{\eta}\eta](w,\cdot)\right)\operatorname{d}\left(\eta\frac{\delta f_{ \eta}(\cdot)}{\delta\eta(w^{\prime})}\right)=0\] \[\lambda\frac{\delta f_{\eta}(w)}{\delta\eta(w^{\prime})}+\int G^{ \prime\prime}[f_{\eta}\eta](w,w^{\prime\prime})\eta(\operatorname{d}w^{\prime \prime})\frac{\delta f_{\eta}(w^{\prime\prime})}{\delta\eta(w^{\prime})}=-G^{ \prime\prime}[f_{\eta}\eta](w,w^{\prime})f_{\eta}(w^{\prime}).\] (D.3)So by Lem. D.6 applied to \(h=\frac{\delta f_{\eta}(\cdot)}{\delta\eta(w^{\prime})}\), we indeed have that \(\frac{\delta f_{\eta}(w)}{\delta\eta(w^{\prime})}\) is bounded by a constant uniformly in \(w,w^{\prime}\) and \(\eta\).

Let us now show that

\[\sup_{w\in\mathcal{W}}\sup_{\eta\in\mathcal{P}(\mathcal{W})}\sup_{w^{\prime}\in \mathcal{W}}\left\|\nabla_{w^{\prime}}\frac{\delta f_{\eta}(w)}{\delta\eta( \mathrm{d}w^{\prime})}\right\|_{w^{\prime}}\leq H\]

for a constant \(H\) depending only on \(\lambda^{-1},L_{0},L_{1},B_{1},G(0)\). Indeed, it suffices to show that for any \(s^{\prime}\in T_{w^{\prime}}\mathcal{W}\) such that \(\left\|s^{\prime}\right\|_{w^{\prime}}=1\), \(\left\langle s^{\prime},\nabla_{w^{\prime}}\frac{\delta f_{\eta}(w)}{\delta \eta(\mathrm{d}w^{\prime})}\right\rangle_{w^{\prime}}\right|\leq H\). Now, starting from (D.3) - which holds for all \(w,w^{\prime},\eta-\) and differentiating with respect to \(w^{\prime}\) in the direction \(s^{\prime}\), we get that

\[\lambda\left\langle s^{\prime},\nabla_{w^{\prime}}\frac{\delta f _{\eta}(w)}{\delta\eta(w^{\prime})}\right\rangle_{w^{\prime}}+\int G^{\prime \prime}[f_{\eta}\eta](w,w^{\prime\prime})\eta(\mathrm{d}w^{\prime\prime}) \left\langle s^{\prime},\nabla_{w^{\prime}}\frac{\delta f_{\eta}(w^{\prime \prime})}{\delta\eta(w^{\prime})}\right\rangle_{w^{\prime}}\\ =-\left\langle s^{\prime},\nabla_{w^{\prime}}\left[G^{\prime \prime}[f_{\eta}\eta](w,w^{\prime})f_{\eta}(w^{\prime})\right]\right\rangle_{w ^{\prime}}\]

and so \(h(w)=\left\langle s^{\prime},\nabla_{w^{\prime}}\frac{\delta f_{\eta}(w)}{ \delta\eta(\mathrm{d}w^{\prime})}\right\rangle_{w^{\prime}}\) satisfies the conditions of Lem. D.6, which proves the claim.

Next let us show that

\[\sup_{w\in\mathcal{W}}\sup_{\begin{subarray}{c}s\in T_{w}\mathcal{W}\\ \left\|s\right\|_{w}=1\end{subarray}}\sup_{\eta\in\mathcal{P}(\mathcal{W})} \sup_{w^{\prime}\in\mathcal{W}}\left\|\nabla_{w^{\prime}}\frac{\delta\left\langle s,\nabla f_{\eta}(w)\right\rangle_{w}}{\delta\eta(\mathrm{d}w^{\prime})}\right\| _{w^{\prime}}\leq H^{\prime}\]

for a constant \(H^{\prime}\) depending only on \(\lambda^{-1},L_{0},L_{1},B_{1},G(0)\) and \(\widetilde{L}_{2}\). Indeed, starting from (D.3) and differentiating with respect to \(w^{\prime}\) in the direction \(s^{\prime}\), and differentiating with respect to \(w\) in the direction \(s\), we get

\[\lambda\left\langle s^{\prime},\nabla_{w^{\prime}}\frac{\delta \left\langle s,\nabla f_{\eta}(w)\right\rangle_{w}}{\delta\eta(w^{\prime})} \right\rangle_{w^{\prime}}+\int\nabla_{w}G^{\prime\prime}[f_{\eta}\eta](w,w^{ \prime\prime})\eta(\mathrm{d}w^{\prime\prime})\left\langle s^{\prime},\nabla _{w^{\prime}}\frac{\delta f_{\eta}(w^{\prime\prime})}{\delta\eta(w^{\prime})} \right\rangle_{w^{\prime}}\\ =-\left\langle s,\nabla_{w}\left\{\left\langle s^{\prime}, \nabla_{w^{\prime}}\left[G^{\prime\prime}[f_{\eta}\eta](w,w^{\prime})f_{\eta}( w^{\prime})]\right\rangle_{w^{\prime}}\right\}\right\rangle_{w}\]

and so

\[\lambda\left\|\nabla_{w^{\prime}}\frac{\delta\left\langle s, \nabla f_{\eta}(w)\right\rangle_{w}}{\delta\eta(\mathrm{d}w^{\prime})}\right\| _{w^{\prime}} \leq\left\|\nabla_{w}\nabla_{w^{\prime}}G^{\prime\prime}[f_{\eta }\eta]\right\|\cdot|f_{\eta}(w^{\prime})|+\left\|\nabla_{w}G^{\prime\prime}[ f_{\eta}\eta]\right\|_{w}\cdot\left\|\nabla f_{\eta}(w^{\prime})\right\|_{w^{ \prime}}\] \[\qquad+\sup_{w^{\prime\prime}\in\mathcal{W}}\left\|\nabla_{w}G^{ \prime\prime}[f_{\eta}\eta](w,w^{\prime\prime})\right\|_{w}\cdot\sup_{w^{ \prime\prime}\in\mathcal{W}}\left\|\nabla_{w^{\prime}}\frac{\delta f_{\eta}( w^{\prime\prime})}{\delta\eta(\mathrm{d}w^{\prime})}\right\|_{w^{\prime}}\] \[\leq\widetilde{L}_{2}\cdot\frac{1}{\lambda}\sqrt{2L_{0}G(0)}+L_{1 }\cdot\left(\frac{L_{1}}{\lambda^{2}}\sqrt{2L_{0}G(0)}+\frac{B_{1}}{\lambda} \right)+L_{1}\cdot H=:H^{\prime}\]

by Assumption 1.

Now fix \(w\in\mathcal{W}\). By Lem. D.8 below applied to \(F(\eta)=f_{\eta}(w)\), we have that

\[\left|f_{\eta}(w)-f_{\eta^{\prime}}(w)\right|\leq\sup_{\eta^{\prime\prime}\in \mathcal{P}(\mathcal{W})}\sup_{w^{\prime}\in\mathcal{W}}\left\|\nabla_{w^{ \prime}}\frac{\delta f_{\eta^{\prime\prime}}(w)}{\delta\eta^{\prime\prime}( \mathrm{d}w^{\prime})}\right\|_{w^{\prime}}W_{2}(\eta,\eta^{\prime})\leq HW_{2}( \eta,\eta^{\prime}).\]

Likewise, fix any \(w\in\mathcal{W}\) and let \(s=\frac{\nabla f_{\eta^{\prime}}(w)-\nabla f_{\eta}(w)}{\left\|\nabla f_{\eta^{ \prime}}(w)-\nabla f_{\eta}(w)\right\|_{w}}\in T_{w}\mathcal{W}\). Then by Lem. D.8 below applied to \(F(\eta)=\left\langle s,\nabla f_{\eta}(w)\right\rangle_{w}\),

\[\left\|\nabla f_{\eta^{\prime}}(w)-\nabla f_{\eta}(w)\right\|=\left\langle s, \nabla f_{\eta^{\prime}}(w)\right\rangle_{w}-\left\langle s,\nabla f_{\eta}(w) \right\rangle_{w}\leq H^{\prime}W_{2}(\eta,\eta^{\prime}).\qed\]

**Lemma D.8**.: _Let \(\mathcal{W}\) a compact Riemannian manifold and \(F:\mathcal{P}(\mathcal{W})\to\mathbb{R}\) such that_

\[\forall\eta\in\mathcal{P}(\mathcal{W}),\forall w\in\mathcal{W},\ \left\|\nabla F^{\prime}[\eta](w)\right\|_{w}\leq B.\]

_Then_

\[\forall\eta,\eta^{\prime}\in\mathcal{P}(\mathcal{W}),\ \left|F(\eta)-F(\eta^{ \prime})\right|\leq BW_{1}(\eta,\eta^{\prime})\leq BW_{2}(\eta,\eta^{\prime}).\]Proof.: For any \(x,y\in\mathcal{W}\), pose \((\Sigma_{\theta}(x,y))_{\theta\in[0,1]}\) the constant-speed length-minimizing geodesic in \(\mathcal{W}\) interpolating between \(x\) and \(y\). Also pose \(\Sigma_{\theta}^{\prime}(x,y)=\frac{d}{d\theta}\Sigma_{\theta}(x,y)\in T_{ \Sigma_{\theta}(x,y)}\mathcal{W}\) for any \(\theta\). For example if \(\mathcal{W}=\mathbb{R}^{d}\), \(\Sigma_{\theta}(x,y)=x+\theta(y-x)\) and \(\Sigma_{\theta}^{\prime}(x,y)=y-x\) for all \(\theta\).

Let \(\gamma\) the optimal coupling between \(\eta,\eta^{\prime}\) in the \(W_{1}\) sense, and for all \(\theta\in[0,1]\), \(\eta_{\theta}=(\Sigma_{\theta})_{\sharp}\gamma\) the pushforward measure of \(\gamma\) by \(\Sigma_{\theta}\). Note that for any \(\theta\in[0,1]\),

\[\frac{d}{d\theta}F(\eta_{\theta})=\int_{\mathcal{W}}F^{\prime}[\eta_{\theta}] \operatorname{d}\left(\partial_{\theta}\eta_{\theta}\right)\]

and that

\[\forall\varphi:\mathcal{W}\to\mathbb{R},\;\frac{d}{d\theta}\int_ {\mathcal{W}}\varphi\mathrm{d}\eta_{\theta} =\frac{d}{d\theta}\iint_{\mathcal{W}\times\mathcal{W}}\varphi( \Sigma_{\theta}(x,y))\mathrm{d}\gamma(x,y)\] \[=\iint_{\mathcal{W}\times\mathcal{W}}\frac{d}{d\theta}\varphi( \Sigma_{\theta}(x,y))\mathrm{d}\gamma(x,y)\] \[=\iint_{\mathcal{W}\times\mathcal{W}}\left\langle\Sigma_{\theta} ^{\prime}(x,y),\nabla\varphi(\Sigma_{\theta}(x,y))\right\rangle_{\Sigma_{ \theta}(x,y)}\mathrm{d}\gamma(x,y).\]

(The interchange of \(\frac{d}{d\theta}\) and \(\iint_{\mathcal{W}\times\mathcal{W}}\) on the second line can be justified by the dominated convergence theorem assuming that \(\varphi\) has bounded \(\mathcal{C}^{1}\) norm, which is the case of \(F^{\prime}[\eta_{\theta}]\) by assumption.) So by Cauchy-Schwarz inequality,

\[\frac{d}{d\theta}F(\eta_{\theta}) =\iint_{\mathcal{W}\times\mathcal{W}}\left\langle\Sigma_{\theta} ^{\prime}(x,y),\nabla F^{\prime}[\eta_{\theta}](\Sigma_{\theta}(x,y))\right\rangle _{\Sigma_{\theta}(x,y)}\mathrm{d}\gamma(x,y)\] \[\left|\frac{d}{d\theta}F(\eta_{\theta})\right| \leq\iint_{\mathcal{W}\times\mathcal{W}}\left\|\Sigma_{\theta} ^{\prime}(x,y)\right\|_{\Sigma_{\theta}(x,y)}\cdot\left\|\nabla F^{\prime}[ \eta_{\theta}](\Sigma_{\theta}(x,y))\right\|_{\Sigma_{\theta}(x,y)}\mathrm{d} \gamma(x,y)\] \[\leq\sup_{w\in\mathcal{W}}\sup_{\eta^{\prime}\in\mathcal{P}( \mathcal{W})}\left\|\nabla F^{\prime}[\eta](w)\right\|_{w}\cdot\iint_{ \mathcal{W}\times\mathcal{W}}\left\|\Sigma_{\theta}^{\prime}(x,y)\right\|_{ \Sigma_{\theta}(x,y)}\mathrm{d}\gamma(x,y)\] \[\leq B\cdot\iint_{\mathcal{W}\times\mathcal{W}}\mathrm{dist}(x,y) \mathrm{d}\gamma(x,y)=BW_{1}(\eta,\eta^{\prime})\]

by definition of the geodesic \((\Sigma_{\theta}(x,y))_{\theta\in[0,1]}\) and by definition of the optimal coupling \(\gamma\). Finally,

\[\left|F(\eta)-F(\eta^{\prime})\right|=\left|\int_{0}^{1}\frac{d}{d\theta}F( \eta_{\theta})\;\mathrm{d}\theta\right|\leq\sup_{\theta\in[0,1]}\left|\frac{d }{d\theta}F(\eta_{\theta})\right|\leq BW_{1}(\eta,\eta^{\prime}).\qed\]

Proof of the Proposition.: 

Proof of Prop. 3.4.: **We first check** (P0). The fact that \(J_{\lambda}\) is convex is given by Prop. 3.3. Moreover, let any \(\beta>0\) and let us check that \(J_{\lambda,\beta}\coloneqq J_{\lambda}+\beta^{-1}H\left(\cdot|\tau\right)\) has a minimizer. Indeed, \(J_{\lambda,\beta}\) is weakly continuous as shown in Lem. D.5, and non-negative so lower-bounded. Since \(\mathcal{W}\) is compact then any set of probability measures on \(\mathcal{W}\) is tight, i.e., any sequence in \(\mathcal{P}(\mathcal{W})\) has a weakly convergent subsequence. So we conclude by the direct method of calculus of variations: let a sequence \((\eta_{n})_{n}\) such that \(J_{\lambda,\beta}(\eta_{n})\to\inf_{\mathcal{P}(\mathcal{W})}J_{\lambda,\beta}\) and extract a weakly convergent subsequence with limit \(\eta_{\infty}\); then by weak continuity \(\eta_{\infty}\) is a minimizer of \(J_{\lambda,\beta}\).

**We now show that \(J_{\lambda}\) satisfies** (P1). Recall from (D.2) that \(J_{\lambda}^{\prime}[\eta](w)=-\frac{\lambda}{2}\left|f_{\eta}\right|^{2}(w)\) with \(f_{\eta}=-\frac{1}{\lambda}G^{\prime}[f_{\eta}\eta]\) over \(\mathcal{W}\). Let us show the first condition for (P1):

\[\forall\eta\in\mathcal{P}_{2}(\mathcal{W}),\;\forall w\in\mathcal{W},\;\max_{ \begin{subarray}{c}s\in T_{\mathcal{W}}\\ \left\|s\right\|_{w}\leq 1\end{subarray}}\;\left|\nabla^{2}\,J_{\lambda}^{ \prime}[\eta](s,s)\right|\leq\Lambda\]

for some \(\Lambda<\infty\), where \(\nabla^{2}\) denotes the Riemannian Hessian. We have

\[\nabla J_{\lambda}^{\prime}[\eta](w) =-\lambda f_{\eta}(w)\nabla f_{\eta}(w)\] \[\nabla^{2}\,J_{\lambda}^{\prime}[\eta](w) =-\lambda f_{\eta}(w)\nabla^{2}f_{\eta}(w)-\lambda\nabla f_{\eta} (w)\nabla^{\top}f_{\eta}(w)\]and so, for all \(s\in T_{w}\mathcal{W}\) such that \(\left\|s\right\|_{w}\leq 1\),

\[\left|\nabla^{2}J_{\lambda}^{\prime}[\eta](s,s)\right| \leq\lambda\left|f_{\eta}\right|\left\|\nabla^{2}f_{\eta}\right\|+ \lambda\left\|\nabla f_{\eta}\right\|^{2}\] \[\leq\sqrt{2L_{0}G(0)}\left(\frac{L_{2}}{\lambda^{2}}\sqrt{2L_{0} G(0)}+\frac{B_{2}}{\lambda}\right)+\lambda\left(\frac{L_{1}}{\lambda^{2}}\sqrt{2L_{0}G (0)}+\frac{B_{1}}{\lambda}\right)^{2}\]

by Lem. D.4.

Let us now check the second condition for (P1), namely that

\[\forall w\in\mathcal{W},\;\forall\eta,\eta^{\prime}\in\mathcal{P}_{2}( \mathcal{W}),\;\;\left\|\nabla J_{\lambda}^{\prime}[\eta]-\nabla J_{\lambda}^ {\prime}[\eta^{\prime}]\right\|_{w}\leq\Lambda\;W_{2}(\eta,\eta^{\prime})\]

for some \(\Lambda<\infty\). Indeed,

\[\left\|\nabla J_{\lambda}^{\prime}[\eta]-\nabla J_{\lambda}^{ \prime}[\eta^{\prime}]\right\|_{w}\] \[=\lambda\left\|f_{\eta}\nabla f_{\eta}-f_{\eta^{\prime}}\nabla f _{\eta^{\prime}}\right\|\leq\lambda\left(\left\|f_{\eta}(\nabla f_{\eta}- \nabla f_{\eta^{\prime}})\right\|+\left\|(f_{\eta}-f_{\eta^{\prime}})\nabla f _{\eta^{\prime}}\right\|\right)\] \[\leq\lambda\left(\sup_{\eta^{\prime}}\sup_{\mathcal{W}}\left|f_{ \eta^{\prime\prime}}\right|\cdot\sup_{\mathcal{W}}\left\|\nabla f_{\eta}- \nabla f_{\eta^{\prime}}\right\|+\sup_{\eta^{\prime\prime}}\sup_{\mathcal{W}} \left\|\nabla f_{\eta^{\prime\prime}}\right\|\cdot\sup_{\mathcal{W}}\left|f_ {\eta}-f_{\eta^{\prime}}\right|\right)\] \[\leq\lambda\left(\frac{1}{\lambda}\sqrt{2L_{0}G(0)}\cdot H^{ \prime}W_{2}(\eta,\eta^{\prime})+\left(\frac{L_{1}}{\lambda^{2}}\sqrt{2L_{0}G (0)}+\frac{B_{1}}{\lambda}\right)\cdot HW_{2}(\eta,\eta^{\prime})\right) \eqqcolon\Lambda W_{2}(\eta,\eta^{\prime})\]

by Lem. D.4 and Lem. D.7.

**We now turn to the proof of (P2)** with the quantitative bound on the local LSI constant. Let \(\eta\in\mathcal{P}(\mathcal{W})\). By the first part of Lem. D.4, we directly have that

\[|J_{\lambda}^{\prime}[\eta](w)|=\frac{\lambda}{2}\left|f_{\eta}\right|^{2}(w) \leq\frac{L_{0}}{\lambda}J_{\lambda}(\eta).\]

In particular, by the Holley-Stroock bounded perturbation argument [10], the proximal Gibbs measure \(\hat{\eta}\coloneqq e^{-\beta J_{\lambda}^{\prime}[\eta]}\tau/Z\) satisfies LSI with constant \(\alpha_{\hat{\eta}}=\alpha_{\tau}\exp\left(-\frac{1}{\lambda}L_{0}\beta J_{ \lambda}(\eta)\right)\).

Finally, we turn to the proof of the bound on the uniform LSI constant along the MFLD trajectory \((\eta_{t})_{t\geq 0}\). Given the bound on the local LSI constants, it suffices to show that

\[\forall\eta\in\mathcal{P}(\mathcal{W}),\;J_{\lambda}(\eta)\leq G(0)\quad \text{and}\quad\forall t\geq 0,\;J_{\lambda}(\eta_{t})\leq J_{\lambda}(\eta_{0})+ \beta^{-1}H\left(\eta_{0}|\tau\right).\]

The first bound was shown in Lem. D.4. For the second bound, note that \(J_{\lambda}(\eta_{t})+\beta^{-1}H\left(\eta_{t}|\tau\right)\) decreases with \(t\), since MFLD is precisely the Wasserstein gradient flow for \(\eta\mapsto J_{\lambda}(\eta)+\beta^{-1}H(\eta)\) and \(H(\eta)\) and \(H\left(\eta|\tau\right)\) differ by a constant. So, since relative entropy is non-negative,

\[J_{\lambda}(\eta_{t})\leq J_{\lambda}(\eta_{t})+\beta^{-1}H\left(\eta_{t}| \tau\right)\leq J_{\lambda}(\eta_{0})+\beta^{-1}H\left(\eta_{0}|\tau\right)\]

for all \(t\geq 0\), as desired. 

## Appendix E Details for Sec. 4 (global convergence by annealing)

The following preliminary lemma allows to control the effect of entropic regularization, using a box-kernel smoothing technique similar to [13].

**Lemma E.1**.: _Let \(\mathcal{W}\) a \(d\)-dimensional compact Riemannian manifold and denote by \(\tau\) the uniform probability measure over \(\mathcal{W}\). Let \(\mathcal{J}:\mathcal{P}(\mathcal{W})\to\mathbb{R}\) and \(\eta^{*}\in\mathcal{P}(\mathcal{W})\), and suppose that there exist constants \(A,B>0\) such that_

\[\forall\eta\;\;\text{s.t.}\;\;W_{1}(\eta,\eta^{*})\leq A,\quad\mathcal{J}( \eta)-\mathcal{J}(\eta^{*})\leq BW_{\infty}(\eta,\eta^{*}).\]

_Denote \(\mathcal{J}_{\beta}=\mathcal{J}+\beta^{-1}H\left(\cdot|\tau\right)\), for any \(\beta>0\). Then_

\[\min_{\eta:W_{1}(\eta,\eta^{*})\leq\mathcal{A}}\mathcal{J}_{\beta}(\eta)\; \leq\;\mathcal{J}(\eta^{*})+\inf_{0<\epsilon\leq\min\{1,A\}}\left[B\epsilon+ \frac{d}{\beta}\log\left(\frac{1}{\epsilon}\right)+\frac{\log C}{\beta}\right]\]

_where \(C\coloneqq\left[\inf_{w\in\mathcal{W}}\inf_{0<\epsilon\leq 1}\;\epsilon^{-d} \cdot\tau\left(\{w^{\prime};\operatorname{dist}_{\mathcal{W}}(w,w^{\prime})\leq \epsilon\}\right)\right]^{-1}\)._Proof.: The proof is adapted from [11]. It is based on constructing an \(\epsilon\)-smoothed version of \(\eta^{*}\), i.e. a measure \(\eta_{\epsilon}\) which admits a density w.r.t. \(\tau\) while being close to \(\eta^{*}\) in an appropriate sense.

Let any \(0<\epsilon\leq\min\{1,A\}\). Given \(w\in\mathcal{W}\), define the probability measure \(\gamma_{\epsilon,w}(\mathrm{d}w^{\prime})\) as the uniform probability measure over the geodesic ball \(B_{\epsilon}(w)\coloneqq\{w\in\mathcal{W};\mathrm{dist}(w,w^{\prime})\leq\epsilon\}\). In other words, \(\frac{\mathrm{d}\gamma_{\epsilon,w}}{\mathrm{d}\tau}(w^{\prime})\coloneqq\frac {1(w^{\prime}\in B_{\epsilon}(w))}{\tau(B_{\epsilon}(w))}\). Then, let \(\gamma_{\epsilon}(\mathrm{d}w,\mathrm{d}w^{\prime})=\eta^{*}(\mathrm{d}w) \gamma_{\epsilon,w}(\mathrm{d}w^{\prime})\in\mathcal{P}(\mathcal{W}\times \mathcal{W})\), and let \(\eta_{\epsilon}(\mathrm{d}w^{\prime})=\int_{w\in\mathcal{W}}\gamma_{\epsilon} (\mathrm{d}w,\mathrm{d}w^{\prime})\) its second marginal.

One can then verify that

\[\frac{\mathrm{d}\eta_{\epsilon}}{\mathrm{d}\tau}(w^{\prime})=\int_{w\in \mathcal{W}}\frac{\mathrm{d}\gamma_{\epsilon,w}}{\mathrm{d}\tau}(w^{\prime}) \eta^{*}(\mathrm{d}w)=\int_{w\in\mathcal{W}}\frac{1(w^{\prime}\in B_{\epsilon }(w))}{\tau(B_{\epsilon}(w))}\eta^{*}(\mathrm{d}w).\]

Moreover there exists a positive constant \(C\) such that \(\tau(B_{\epsilon}(w))\geq C^{-1}\epsilon^{d}\) for all \(\epsilon\leq 1\)[10, Theorem 3.3]. As a consequence,

\[H\left(\eta_{\epsilon}|\tau\right)=\int\mathrm{d}\eta_{\epsilon}(w^{\prime}) \log\frac{\mathrm{d}\eta_{\epsilon}}{\mathrm{d}\tau}(w^{\prime})\leq\sup_{w \in\mathcal{W}}-\log\tau(B_{\epsilon}(w))\leq d\log(1/\epsilon)+\log C.\]

Furthermore, by definition of the coupling \(\gamma_{\epsilon}\), we have \(W_{1}(\eta_{\epsilon},\eta^{*})\leq W_{\infty}(\eta_{\epsilon},\eta^{*})\leq \epsilon\leq A\). Therefore, by assumption \(\mathcal{J}(\eta_{\epsilon})-\mathcal{J}(\eta^{*})\leq BW_{\infty}(\eta_{ \epsilon},\eta^{*})\leq B\epsilon\), and so

\[\min_{\eta:W_{1}(\eta,\eta^{*})\leq A}\mathcal{J}_{\beta}(\eta) \leq\mathcal{J}_{\beta}(\eta_{\epsilon})=\mathcal{J}(\eta_{\epsilon})+ \beta^{-1}H\left(\eta_{\epsilon}|\tau\right)\] \[\leq\mathcal{J}(\eta^{*})+B\epsilon+\beta^{-1}\left(d\log(1/ \epsilon)+\log C\right),\]

and the inequality of the lemma follows by taking the infimum over \(\epsilon\). 

### Proof of Prop. 4.1

We state and prove a more precise version of Prop. 4.1 below.

**Proposition E.2**.: _Under Assumption 1, let \(\Delta>0\) and assume that \(\Delta\leq\frac{2L_{0}L_{1}G(0)}{\lambda^{2}J_{\lambda}^{*}}\). Then MFLD-Bilevel with the temperature schedule \(\forall t,\beta_{t}=\frac{4d}{\Delta J_{\lambda}^{*}}\log\left(\frac{4C^{1/d}B }{\Delta J_{\lambda}^{*}}\right)\) converges to \((1+\Delta)\)-multiplicative accuracy in time_

\[T_{\Delta}\leq\frac{2d}{\alpha_{\tau}\Delta J_{\lambda}^{*}}\log\left(\frac{4 C^{1/d}B}{\Delta J_{\lambda}^{*}}\right)\cdot\exp\left(\frac{4dL_{0}G(0)}{ \lambda\Delta J_{\lambda}^{*}}\log\left(\frac{4C^{1/d}B}{\Delta J_{\lambda}^ {*}}\right)\right)\cdot\log\left(\frac{2J_{\lambda}(\eta_{0})}{\Delta J_{ \lambda}^{*}}+\frac{H\left(\eta_{0}|\tau\right)}{2\log C}\right)\]

_where \(C=\max\left\{1,\left[\inf_{w\in\mathcal{W}}\inf_{0<\epsilon\leq 1}\, \epsilon^{-d}\cdot\tau\left(\{w^{\prime};\mathrm{dist}_{\mathcal{W}}(w,w^{ \prime})\leq\epsilon\}\right)\right]^{-1}\right\}\)._

Proof of Prop. E.2.: Let \((\eta)_{t}\) the MFLD-Bilevel trajectory with constant inverse temperature parameter \(\beta\) to be chosen. Denote \(J_{\lambda,\beta}=J_{\lambda}+\beta^{-1}H\left(\cdot|\tau\right)\). Recall that by Prop. 3.4, \(J_{\lambda,\beta}\) satisfies \(\alpha_{\beta}\)-LSI uniformly along the MFLD trajectory with \(\alpha_{\beta}=\alpha_{\tau}\exp\left(-\frac{1}{\lambda}L_{0}\beta G(0)\right)\). So by Thm. 2.1, for all \(t\),

\[J_{\lambda}(\eta_{t})\leq J_{\lambda,\beta}(\eta_{t})\leq\inf J_{\lambda,\beta }+e^{-2\beta^{-1}\alpha_{\beta}t}\left(J_{\lambda,\beta}(\eta_{0})-\inf J_{ \lambda,\beta}\right)\leq\inf J_{\lambda,\beta}+e^{-2\beta^{-1}\alpha_{\beta}t }J_{\lambda,\beta}(\eta_{0}),\]

where in the first inequality we used that \(J_{\lambda,\beta}-J_{\lambda}=\beta^{-1}H\left(\cdot|\tau\right)\geq 0\).

Furthermore, by applying Lem. E.1 to \(\mathcal{J}=J_{\lambda}\), \(\eta^{*}=\arg\min J_{\lambda}\), \(A=\infty\) and \(B=\sqrt{2L_{0}G(0)}\cdot\left(\frac{L_{1}}{\lambda^{2}}\sqrt{2L_{0}G(0)}+\frac{ B_{1}}{\lambda}\right)\) the constant from Lem. D.5, we find that

\[\inf J_{\lambda,\beta}\leq\inf J_{\lambda}+\inf_{0<\epsilon\leq 1}\left[B \epsilon+\frac{d}{\beta}\log\frac{1}{\epsilon}+\frac{\log C}{\beta}\right].\]

Taking \(\beta=\frac{d}{B}s\) for some \(s\geq 1\) to be chosen, and evaluating at the infimum at \(\epsilon=\frac{d}{\beta B}\), we get

\[\inf J_{\lambda,\beta}\leq J_{\lambda}^{*}+\frac{d+\log C^{\prime}}{\beta}-\frac {d}{\beta}\log\left(\frac{d}{\beta B}\right).\]where \(C^{\prime}=\max\{1,C\}\). So in order to guarantee that \(J_{\lambda}(\eta_{t})\leq(1+\Delta)J_{\lambda}^{*}\), it suffices to take \(t\) such that

\[J_{\lambda}^{*}+\frac{d+\log C^{\prime}}{\beta}-\frac{d}{\beta} \log\left(\frac{d}{\beta B}\right)+e^{-2\beta^{-1}\alpha_{\beta}t}\left(J_{ \lambda}(\eta_{0})+\beta^{-1}H\left(\eta_{0}|\tau\right)\right)\leq(1+\Delta)J _{\lambda}^{*}\] \[\text{i.e.}\quad t\geq\frac{\beta}{2\alpha_{\beta}}\log\left( \frac{J_{\lambda}(\eta_{0})+\beta^{-1}H\left(\eta_{0}|\tau\right)}{\Delta J_{ \lambda}^{*}-\left(\frac{d+\log C^{\prime}}{\beta}-\frac{d}{\beta}\log\left( \frac{d}{\beta B}\right)\right)}\right)\eqqcolon T_{s},\]

assuming that \(\Delta\) is large enough so that the above expression is well-defined. More explicitly, substituting the value of \(\alpha_{\beta}\) and of \(\beta=\frac{d}{B}s\), we have

\[T_{s} =\frac{\beta}{2\alpha_{\tau}}\cdot\exp\left(\frac{1}{\lambda}L_{ 0}\beta G(0)\right)\cdot\log\left(\frac{J_{\lambda}(\eta_{0})+\beta^{-1}H\left( \eta_{0}|\tau\right)}{\Delta J_{\lambda}^{*}-\left(\frac{d+\log C^{\prime}}{ \beta}-\frac{d}{\beta}\log\left(\frac{d}{\beta B}\right)\right)}\right)\] \[=\frac{sd/B}{2\alpha_{\tau}}\cdot\exp\left(s\frac{1}{\lambda B}L_ {0}dG(0)\right)\cdot\log\left(\frac{J_{\lambda}(\eta_{0})+\frac{B}{sd}H\left( \eta_{0}|\tau\right)}{\Delta J_{\lambda}^{*}-\frac{B}{s}\left(1+d^{-1}\log C^ {\prime}+\log s\right)}\right).\]

Noting that

\[\log\frac{s\Delta J_{\lambda}^{*}}{4B}=\log s-\log\frac{4B}{\Delta J _{\lambda}^{*}} \leq\frac{s\Delta J_{\lambda}^{*}}{4B}-1\] \[\text{so}\quad\frac{B}{s}\left(1+d^{-1}\log C^{\prime}+\log s \right) \leq\frac{B}{s}\left(d^{-1}\log C^{\prime}+\log\frac{4B}{\Delta J _{\lambda}^{*}}+\frac{s\Delta J_{\lambda}^{*}}{4B}\right)\] \[=\frac{B}{s}\left(d^{-1}\log C^{\prime}+\log\frac{4B}{\Delta J_{ \lambda}^{*}}\right)+\frac{\Delta J_{\lambda}^{*}}{4},\]

choose henceforth \(s=\max\left\{1,\frac{4B}{\Delta J_{\lambda}^{*}}\left(d^{-1}\log C^{\prime}+ \log\frac{4B}{\Delta J_{\lambda}^{*}}\right)\right\}\), so that

\[\Delta J_{\lambda}^{*}-\frac{B}{s}\left(1+d^{-1}\log C^{\prime}+\log s\right) \geq\frac{\Delta J_{\lambda}^{*}}{2}.\]

To simplify the final statement, we make the assumption that \(\Delta\) is small enough so that \(1\leq\frac{4B}{\Delta J_{\lambda}^{*}}\left(d^{-1}\log C^{\prime}+\log\frac{4 B}{\Delta J_{\lambda}^{*}}\right)\). More explicitly, since we were careful to choose \(C^{\prime}\geq 1\),

\[1\leq\frac{4B}{\Delta J_{\lambda}^{*}}\left(d^{-1}\log C^{\prime} +\log\frac{4B}{\Delta J_{\lambda}^{*}}\right)\iff\frac{\Delta J_{\lambda}^{*} }{4B}+\log\frac{\Delta J_{\lambda}^{*}}{4B}\leq d^{-1}\log C^{\prime}\] \[\iff \frac{\Delta J_{\lambda}^{*}}{4B}\leq 1\quad\text{and}\quad\log\frac{ \Delta J_{\lambda}^{*}}{4B}\leq-1\iff\frac{\Delta J_{\lambda}^{*}}{4B}\leq \min\{1,e^{-1}\}=e^{-1}\] \[\iff \Delta\leq\frac{4Be^{-1}}{J_{\lambda}^{*}}=\frac{4e^{-1}}{J_{ \lambda}^{*}}\cdot\sqrt{2L_{0}G(0)}\left(\frac{L_{1}}{\lambda^{2}}\sqrt{2L_{0} G(0)}+\frac{B_{1}}{\lambda}\right)\] \[\iff \Delta\leq\frac{4e^{-1}}{J_{\lambda}^{*}}\cdot\frac{2L_{0}L_{1}G(0 )}{\lambda^{2}}\iff\Delta\leq\frac{1}{J_{\lambda}^{*}}\cdot\frac{2L_{0}L_{1} G(0)}{\lambda^{2}}.\]

Then \(s=\frac{4B}{\Delta J_{\lambda}^{*}}\left(d^{-1}\log C^{\prime}+\log\frac{4B}{ \Delta J_{\lambda}^{*}}\right)\), \(\beta=\frac{4d}{\Delta J_{\lambda}^{*}}\left(d^{-1}\log C^{\prime}+\log\frac{4 B}{\Delta J_{\lambda}^{*}}\right)\geq\frac{4}{\Delta J_{\lambda}^{*}}\log C^{\prime}\), and

\[T_{s}\leq\frac{\beta}{2\alpha_{\tau}}\cdot\exp\left(\frac{1}{ \lambda}L_{0}\beta G(0)\right)\cdot\log\left(\frac{J_{\lambda}(\eta_{0})+ \beta^{-1}H\left(\eta_{0}|\tau\right)}{\Delta J_{\lambda}^{*}/2}\right)\] \[\leq\frac{2d}{\alpha_{\tau}\Delta J_{\lambda}^{*}}\log\left(\frac {4C^{\prime 1/d}B}{\Delta J_{\lambda}^{*}}\right)\cdot\exp\left(\frac{4dL_{0}G(0)}{ \lambda\Delta J_{\lambda}^{*}}\log\left(\frac{4C^{\prime 1/d}B}{\Delta J_{\lambda}^{*}} \right)\right)\cdot\log\left(\frac{2J_{\lambda}(\eta_{0})}{\Delta J_{\lambda}^ {*}}+\frac{H\left(\eta_{0}|\tau\right)}{2\log C^{\prime}}\right)\eqqcolon \overline{T}_{\Delta}.\]

Hence the time-complexity upper bound of \(\overline{T}_{\Delta}\) for reaching \((1+\Delta)\)-multiplicative accuracy.

### General annealing procedure and its convergence guarantee

The following theorem builds upon and generalizes the idea of [14, Sec. 4.1] to objective functionals \(\mathcal{J}\) that have a positive optimal value. It ensures fast convergence to a fixed multiplicative accuracy.

**Theorem E.3**.: _Let \(\mathcal{W}\) a \(d\)-dimensional compact Riemannian manifold, so in particular the uniform measure \(\tau\) over \(\mathcal{W}\) satisfies \(\alpha_{\tau}\)-LSI for some \(\alpha_{\tau}>0\). Let \(\mathcal{J}:\mathcal{P}(\mathcal{W})\to\mathbb{R}_{+}\) convex, suppose that \(\mathcal{J}^{*}\coloneqq\min\mathcal{J}>0\) and that there exists a minimizer \(\eta^{*}\). Suppose that there exist constants \(\kappa_{1},C_{L},A>0\) such that_

1. \(\left\|\mathcal{J}^{\prime}[\eta]\right\|_{\infty}\leq\kappa_{1}\mathcal{J}( \eta)\) _for all_ \(\eta\in\mathcal{P}(\mathcal{W})\)_._
2. \(\mathcal{J}(\eta)-\mathcal{J}(\eta^{*})\leq C_{L}W_{\infty}(\eta,\eta^{*})\) _for all_ \(\eta\in\mathcal{P}(\mathcal{W})\) _such that_ \(W_{1}(\eta,\eta^{*})\leq A\)_._

_Fix \(0<\delta\leq\frac{C_{L}\min\{1,A\}}{\mathcal{J}^{*}}\). Let \(\eta_{t}^{k}\) the iterates of the annealing procedure of Algorithm 1 with initialization \(\beta_{0}=d\) and with the schedule \(K=\lceil\log_{2}(1/(\delta\mathcal{J}^{*}))\rceil\) and_

\[T_{k}=2^{k-1}d\log\left(2^{k}\mathcal{J}_{\beta_{0}}(\eta_{0})\right)\cdot \alpha_{\tau}^{-1}\exp\left(2\kappa_{1}d\left(\delta^{-1}+\log\left(\frac{C_{L }C^{1/d}}{\delta\mathcal{J}^{*}}\right)+2+\frac{\mathcal{J}_{\beta_{0}}(\eta_ {0})}{2}\right)\right)\] (E.1)

_where \(C\coloneqq\left[\inf_{w\in\mathcal{W}}\inf_{0<\epsilon\leq 1}\ \epsilon^{-d}\cdot\tau \left(\{w^{\prime};\mathrm{dist}_{\mathcal{W}}(w,w^{\prime})\leq\epsilon\} \right)\right]^{-1}\)._

_Then \(\mathcal{J}(\eta_{T_{K}}^{K})\leq\mathcal{J}^{*}\left(1+3+2\delta\log\left( \frac{C_{L}C^{1/d}}{\delta\mathcal{J}^{*}}\right)\right)\), and the total time-complexity is given by_

\[\sum_{k=0}^{K}T_{k}\leq\frac{d}{\delta\mathcal{J}^{*}}\log\left(\frac{J_{\beta _{0}}(\eta_{0})}{\delta\mathcal{J}^{*}}\right)\cdot\alpha_{\tau}^{-1}\exp \left(2\kappa_{1}d\left(\delta^{-1}+\log\left(\frac{C_{L}C^{1/d}}{\delta \mathcal{J}^{*}}\right)+2+\frac{\mathcal{J}_{\beta_{0}}(\eta_{0})}{2}\right) \right).\]

Let us discuss the assumptions of Thm. E.3 and possible generalizations.

* Note that the condition 2. of the theorem holds as soon as \(\mathcal{J}^{\prime}[\eta]:\mathcal{W}\to\mathbb{R}\) is \(C_{L}\)-Lipschitz for all \(\eta\in\mathcal{P}(\mathcal{W})\), as shown in Lem. D.8, since \(W_{1}\leq W_{2}\leq W_{\infty}\).
* The annealing procedure and its convergence guarantee can be generalized to a non-compact manifold \(\mathcal{W}\) by modifying MFLD to include a confining potential term, as discussed in Sec. A.2.
* Condition 1. of the theorem actually holds for any \(\mathcal{J}\) such that \(\sup_{\eta,w,w^{\prime}}|\mathcal{J}^{\prime\prime}[\eta](w,w^{\prime})|\leq L<\infty\) and \(\mathcal{J}^{*}>0\), with the constant \(\kappa_{1}=\sqrt{\frac{2L}{\mathcal{J}^{*}}}\). Indeed, one can then show similarly to Lem. D.3 that \[\left\|\mathcal{J}^{\prime}[\eta]\right\|_{\infty}^{2}\leq 2L\left(\mathcal{J}( \eta)-\mathcal{J}^{*}\right)\leq 2L\mathcal{J}(\eta)\leq 2L\frac{\mathcal{J}( \eta)^{2}}{\mathcal{J}^{*}}.\] However, when plugging in \(\kappa_{1}=\sqrt{2L/\mathcal{J}^{*}}\) into the bounds of the theorem, one obtains a less favorable dependency of the total time-complexity in \(\mathcal{J}^{*}\). In particular, note that the total time-complexity guaranteed by the theorem scales exponentially in \(\kappa_{1}\) and polynomially in \(1/\mathcal{J}^{*}\).

* The way that the condition 1. of the theorem comes into the proof, is that it allows to guarantee a local LSI constant of \(\mathcal{J}+\beta_{\_}t^{-1}H\) at \(\eta_{\_}t\) of \(\alpha_{\_}{\eta_{\_}t}=\operatorname{cst}\cdot e^{-\kappa_{\_}1}\beta_{\_} \mathcal{J}(\eta_{\_}t)\). One could similarly formulate an annealing procedure, and state convergence guarantees, tailored to objectives \(\mathcal{J}\) that satisfy different criteria for LSI, such as the Bakry-Emery curvature-dimension criterion.

The remainder of this subsection is dedicated to proving Thm. E.3.

Proof of Thm. E.3.: Fix any \(0<\delta\leq\frac{C_{\_}L\min\{1,A\}}{\mathcal{J}^{*}}\). Let, for any \(\beta>0\), \(\mathcal{J}_{\_}{\beta}=\mathcal{J}+\beta^{-1}H\left(\cdot|\tau\right)\).

By condition 1. of the theorem and the Holley-Stroock bounded perturbation argument, for any \(t,k\), the proximal Gibbs measure \(\widehat{\eta_{\_}t^{k}}\propto e^{-\beta_{\_}k\mathcal{J}^{*}[\eta_{\_}t^{k} ]}\tau\) satisfies LSI with the constant

\[\alpha_{\_}\tau\exp\left(-\beta_{\_}k\kappa_{\_}1\mathcal{J}(\eta_{\_}t^{k} )\right)\geq\inf_{\_}t^{\prime}\geq\alpha_{\_}\tau\exp\left(-\beta_{\_}k \kappa_{\_}1\mathcal{J}(\eta_{\_}t^{k})\right)\eqqcolon\alpha(k).\]

That is, for any \(k\), \(\mathcal{J}_{\_}{\beta_{\_}k}\) satisfies \(\alpha(k)\)-LSI at \(\eta_{\_}t^{k}\) for all \(t\geq 0\). (To see that \(\alpha(k)>0\), note that for any \(k,t\), \(\mathcal{J}(\eta_{\_}t^{k})\leq\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}t^{k}) \leq\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}0^{k})\), since \(H\left(\cdot|\tau\right)\) is non-negative and \((\eta_{\_}t^{k})_{\_}t\) is a Wasserstein gradient flow of \(\mathcal{J}_{\_}{\beta_{\_}k}\), and so \(\alpha(k)=\inf_{\_}t\geq 0\,\alpha_{\_}\tau\exp\left(-\beta_{\_}k\kappa_{\_}1 \mathcal{J}(\eta_{\_}t^{k})\right)\geq\alpha_{\_}\tau\exp\left(-\beta_{\_}k \kappa_{\_}1\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}0^{k})\right)>0\); but we will not make use of this rough bound in the sequel.)

Now let

\[T_{\_}k=\frac{\beta_{\_}k}{2\underline{\alpha}(k)}\log\left(\frac{\beta_{\_}k }{d}\bar{c}_{\_}k\right)\]

for some \(\underline{\alpha}(k)\leq\alpha(k)\) and \(\bar{c}_{\_}k\geq\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}0^{k})-\min\mathcal{ J}_{\_}{\beta_{\_}k}\) to be chosen. Then by Thm. 2.1 applied to \(\mathcal{J}_{\_}{\beta_{\_}k}\), we obtain

\[\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}{T_{\_}k}^{k}) \leq\min\mathcal{J}_{\_}{\beta_{\_}k}+\exp\left(-2\beta_{\_}k^{-1 }\alpha(k)T_{\_}k\right)\cdot\left(\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}0^{k })-\min\mathcal{J}_{\_}{\beta_{\_}k}\right)\] \[\leq\min\mathcal{J}_{\_}{\beta_{\_}k}+\left[\frac{\beta_{\_}k}{d} \left(\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}0^{k})-\min\mathcal{J}_{\_}{\beta_ {\_}k}\right)\right]^{-1}\cdot\left(\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}0^{ k})-\min\mathcal{J}_{\_}{\beta_{\_}k}\right)\] \[=\min\mathcal{J}_{\_}{\beta_{\_}k}+\frac{d}{\beta_{\_}k}.\]

Further, by Lem. E.1,

\[\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}{T_{\_}k}^{k}) \leq\mathcal{J}^{*}+\inf_{\_}{0<\epsilon\leq\min\{1,A\}}\left[C_{ \_}L\epsilon+\frac{d}{\beta_{\_}k}\log\left(\frac{1}{\epsilon}\right)+\frac{ \log C}{\beta_{\_}k}\right]+\frac{d}{\beta_{\_}k}\] \[\leq\mathcal{J}^{*}(1+\delta)+\frac{d}{\beta_{\_}k}\log\left( \frac{C_{\_}L}{\delta\mathcal{J}^{*}}\right)+\frac{d+\log C}{\beta_{\_}k},\] (E.2)

where the last inequality follows by choosing \(\epsilon=\frac{\delta\mathcal{J}^{*}}{C_{\_}L}\leq\min\{1,A\}\) since \(\delta\leq\frac{C_{\_}L\min\{1,A\}}{\mathcal{J}^{*}}\).

Then, for all \(k\geq 1\) and \(t\geq 0\),

\[\beta_{\_}k\mathcal{J}(\eta_{\_}t^{k})\leq\beta_{\_}k\mathcal{J}_{\_}{\beta_{\_} k}(\eta_{\_}t^{k})\leq\beta_{\_}k\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}0^{k})= \beta_{\_}k\mathcal{J}_{\_}{\beta_{\_}k}(\eta_{\_}{T_{\_}k-1}^{k-1})\leq\beta_ {\_}k\mathcal{J}_{\_}{\beta_{\_}k-1}(\eta_{\_}{T_{\_}k-1}^{k-1})=2\beta_{\_}k \mathcal{J}_{\_}{\beta_{\_}k-1}(\eta_{\_}{T_{\_}k-1}^{k-1}),\]

where we used successively that \(\mathcal{J}_{\_}{\beta_{\_}k}-\mathcal{J}=\beta_{\_}k}^{-1}H\left(\cdot|\tau\right)\geq 0\), that \((\eta_{\_}t^{k})_{\_}t\) is a Wasserstein gradient flow for \(\mathcal{J}_{\_}{\beta_{\_}k}\), that \(\mathcal{J}_{\_}{\beta_{\_}k-1}-\mathcal{J}_{\_}{\beta_{\_}k}=(\beta_{\_}{k-1}^ {-1}-\beta_{\_}k}^{-1})H\left(\cdot|\tau\right)\geq 0\) since \((\beta_{\_}k)_{\_}k\) is increasing, and that by definition \(\beta_{\_}k}=2^{k}\beta_{\_}0\). So by (E.2),

\[\beta_{\_}k\mathcal{J}(\eta_{\_}t^{k})\leq 2\beta_{\_}{k-1} \mathcal{J}_{\_}{\beta_{\_}k-1}(\eta_{\_}{T_{\_}k-1}^{k-1}) \leq 2\beta_{\_}k-1\mathcal{J}^{*}(1+\delta)+2d\log\left(\frac{C_{\_}L }{\delta\mathcal{J}^{*}}\right)+2d+2\log C\] \[\leq 2\frac{d}{\delta}(1+\delta)+2d\log\left(\frac{C_{\_}L}{\delta \mathcal{J}^{*}}\right)+2d+2\log C\] \[=2d\left(\delta^{-1}+\log\left(\frac{C_{\_}L}{\delta\mathcal{J}^{* }}\right)+2+\frac{\log C}{d}\right)\]since our choice of \(\beta_{0}=d\) and \(K=\lceil\log_{2}(1/(\delta\mathcal{J}^{*}))\rceil\) ensures that \(\beta_{k-1}\leq\beta_{K}=2^{K}\beta_{0}\leq\frac{d}{\delta\mathcal{J}^{*}}\). For \(k=0\) and all \(t\geq 0\), we have more simply \(\beta_{0}\mathcal{J}(\eta_{t}^{0})\leq\beta_{0}\mathcal{J}_{\beta_{0}}(\eta_{ 0}^{0})\leq\beta_{0}\mathcal{J}_{\beta_{0}}(\eta_{0})=d\mathcal{J}_{\beta_{0}} (\eta_{0})\). As a result, for all \(k\geq 0\) we have

\[\forall t\geq 0,\ \beta_{k}\mathcal{J}(\eta_{t}^{k})\leq 2d\left(\delta^{-1}+ \log\left(\frac{C_{L}}{\delta\mathcal{J}^{*}}\right)+2+\frac{\log C}{d}+\frac {1}{2}\mathcal{J}_{\beta_{0}}(\eta_{0})\right)\]

and so

\[\alpha(k) =\inf_{t\geq 0}\alpha_{\tau}\exp\left(-\kappa_{1}\beta_{k} \mathcal{J}(\eta_{t}^{k})\right)\] \[\geq\alpha_{\tau}\exp\left(-2\kappa_{1}d\left(\delta^{-1}+\log \left(\frac{C_{L}}{\delta\mathcal{J}^{*}}\right)+2+\frac{\log C}{d}+\frac{1}{2 }\mathcal{J}_{\beta_{0}}(\eta_{0})\right)\right)=:\underline{\alpha}(k).\]

Moreover, we can choose \(\bar{c}_{k}\) as

\[\mathcal{J}_{\beta_{k}}(\eta_{0}^{k})=\mathcal{J}_{\beta_{k}}( \eta_{T_{k-1}}^{k-1}) \leq\mathcal{J}_{\beta_{k-1}}(\eta_{T_{k-1}}^{k-1})\leq\mathcal{J}_{ \beta_{k-1}}(\eta_{0}^{k-1})\leq...\leq\mathcal{J}_{\beta_{0}}(\eta_{0})\quad \text{by induction},\] \[\text{so}\quad\mathcal{J}_{\beta_{k}}(\eta_{0}^{k})-\min\mathcal{ J}_{\beta_{k}}\leq\mathcal{J}_{\beta_{0}}(\eta_{0})=:\bar{c}_{k}.\]

Therefore, more explicitly,

\[T_{k} =\frac{\beta_{k}}{2\underline{\alpha}(k)}\log\left(\frac{\beta_{ k}}{d}\bar{c}_{k}\right)\] \[=\frac{\beta_{k}}{2}\log\left(\frac{\beta_{k}}{d}\mathcal{J}_{ \beta_{0}}(\eta_{0})\right)\cdot\alpha_{\tau}^{-1}\exp\left(2\kappa_{1}d \left(\delta^{-1}+\log\left(\frac{C_{L}}{\delta\mathcal{J}^{*}}\right)+2+\frac {\log C}{d}+\frac{1}{2}\mathcal{J}_{\beta_{0}}(\eta_{0})\right)\right)\] \[=2^{k-1}d\cdot\log\left(2^{k}\mathcal{J}_{\beta_{0}}(\eta_{0}) \right)\cdot\alpha_{\tau}^{-1}\exp\left(2\kappa_{1}d\left(\delta^{-1}+\log \left(\frac{C_{L}}{\delta\mathcal{J}^{*}}\right)+2+\frac{\log C}{d}+\frac{1}{2 }\mathcal{J}_{\beta_{0}}(\eta_{0})\right)\right)\]

since \(\beta_{k}=2^{k}\beta_{0}=2^{k}d\). Note that

\[\sum_{k=0}^{K}2^{k-1}\log\left(2^{k}\mathcal{J}_{\beta_{0}}(\eta _{0})\right) =\sum_{k=0}^{K}2^{k}\frac{\log J_{\beta_{0}}(\eta_{0})}{2}+\sum_{k= 0}^{K}k2^{k-1}\log(2)\] \[=(2^{K+1}-1)\frac{\log J_{\beta_{0}}(\eta_{0})}{2}+\log(2)\left( (K-1)2^{K}+1\right)\] \[\leq 2^{K}\log J_{\beta_{0}}(\eta_{0})+\log(2)K2^{K}\] \[\leq\frac{1}{\delta\mathcal{J}^{*}}\log J_{\beta_{0}}(\eta_{0})+ \frac{1}{\delta\mathcal{J}^{*}}\log\left(\frac{1}{\delta\mathcal{J}^{*}}\right)\] \[=\frac{1}{\delta\mathcal{J}^{*}}\log\left(\frac{J_{\beta_{0}}( \eta_{0})}{\delta\mathcal{J}^{*}}\right)\]

since \(K=\lceil\log_{2}(1/(\delta\mathcal{J}^{*}))\rceil\), hence the announced bound on the total time-complexity \(\sum_{k=0}^{K}T_{k}\).

Finally, at round \(K=\lceil\log_{2}(1/(\delta\mathcal{J}^{*}))\rceil\), then \(\beta_{K}=2^{K}\beta_{0}=2^{K}d\in\left[\frac{1}{2}\frac{d}{\delta\mathcal{J}^{ *}},\frac{d}{\delta\mathcal{J}^{*}}\right]\), so by (E.2),

\[\mathcal{J}(\eta_{T_{K}}^{K})\leq\mathcal{J}_{\beta_{K}}(\eta_{T_ {K}}^{K}) \leq\mathcal{J}^{*}(1+\delta)+\frac{d}{\beta_{K}}\log\left(\frac{C_ {L}}{\delta\mathcal{J}^{*}}\right)+\frac{d+\log C}{\beta_{K}}\] \[\leq\mathcal{J}^{*}\left(1+3\delta+2\delta\frac{\log(C)}{d}+2 \delta\log\left(\frac{C_{L}}{\delta\mathcal{J}^{*}}\right)\right),\]

which completes the proof. 

### Proof of Thm. 4.2

We state a slightly more precise version of Thm. 4.2 below, and prove it as a corollary of the more general Thm. E.3. Then Thm. 4.2 follows by choosing \(\delta=\Theta(\frac{\Delta}{\log(B/(\Delta\mathcal{J}_{\lambda}^{*}))})\), gathering the constants appearing in the bounds, noting that \(J_{\lambda,\beta_{0}}(\eta_{0})\leq J_{\lambda}(\eta_{0})+dH\left(\eta_{0} |\tau\right)\leq G(0)+dH(\eta_{0})+d\log\operatorname{vol}(\mathcal{W})\).

**Theorem E.4**.: _Under Assumption 1, there exists constants \(B=\mathrm{poly}(L_{i},B_{i},G(0),\lambda^{-1})\) and \(C\) dependent only on \(\mathcal{W}\) such that the following holds. For any \(\delta\leq\frac{B}{J_{\lambda}^{*}}\), MFLD-Bilevel with the temperature schedule \((\beta_{t})_{t\geq 0}\) defined by \(\forall k\leq K,\forall t\in[t_{k},t_{k+1}],\beta_{t}=2^{k}d\) where \(t_{0}=0\) and \(K=\lceil\log_{2}(1/(\delta\mathcal{J}^{*}))\rceil\) and_

\[t_{k+1}-t_{k}=2^{k-1}d\log\left(2^{k}J_{\lambda,\beta_{0}}(\eta_{0})\right) \cdot\alpha_{\tau}^{-1}\exp\left(\frac{2L_{0}d}{\lambda}\left(\delta^{-1}+ \log\left(\frac{BC^{1/d}}{\delta J_{\lambda}^{*}}\right)+2+\frac{J_{\lambda, \beta_{0}}(\eta_{0})}{2}\right)\right),\]

_achieves \((1+\Delta)\)-multiplicative accuracy, where \(\Delta=3\delta+2\delta\log\left(\frac{BC^{1/d}}{\delta J_{\lambda}^{*}}\right)\), with time-complexity_

\[T_{\Delta}\leq t_{K+1}\leq\frac{d}{\delta J_{\lambda}^{*}}\log\left(\frac{J_{ \lambda,\beta_{0}}(\eta_{0})}{\delta J_{\lambda}^{*}}\right)\cdot\alpha_{\tau }^{-1}\exp\left(\frac{2L_{0}d}{\lambda}\left(\delta^{-1}+\log\left(\frac{BC^{1 /d}}{\delta J_{\lambda}^{*}}\right)+2+\frac{J_{\lambda,\beta_{0}}(\eta_{0})}{ 2}\right)\right).\]

Proof of Thm.4.2.: Let us show that the conditions of Thm.E.3 are satisfied, under Assumption 1, for \(\mathcal{J}=J_{\lambda}\). \(J_{\lambda}\) is convex and non-negative, and it is implied throughout Sec.4.1 that \(\inf J_{\lambda}>0\), for the notion of convergence to a fixed multiplicative accuracy to apply (Def.4.1). The existence of a minimizer \(\eta^{*}\) is ensured by the weak convexity of \(J_{\lambda}\), by a similar argument as the proof of (P0) in Sec.D.3. We have the condition 1. with \(\kappa_{1}=\frac{L_{0}}{\lambda}\), i.e. \(\left\|J_{\lambda}^{\prime}[\eta]\right\|_{\infty}\leq\frac{L_{0}}{\lambda}J_ {\lambda}(\eta)\), by the first part of Lem.D.4. We also have condition 2. with \(A=\infty\) and \(C_{L}=B\coloneqq\sqrt{2L_{0}G(0)}\cdot\left(\frac{L_{1}}{\lambda^{2}}\sqrt{2 L_{0}G(0)}+\frac{B_{1}}{\lambda}\right)\), as shown in Lem.D.5, since \(W_{1}\leq W_{2}\leq W_{\infty}\).

Note that annealed MFLD-Bilevel with the announced temperature annealing schedule \((\beta_{t})_{t}\), precisely corresponds to Algorithm1 with the schedule (E.1) applied to \(\mathcal{J}=J_{\lambda}\). So the announced time-complexity bound follows directly from the application of Thm.E.3. 

## Appendix F Details for Sec.5 (estimates of the local LSI constant)

We begin by presenting the proof of Prop.5.1, which states that bounding the LSI constant of \(\eta_{\lambda,\beta}\) leads to a local convergence rate.

Proof of Prop.5.1.: For any \(\eta\in\mathcal{P}(\mathcal{W})\), we denote \(\hat{\eta}(\mathrm{d}w)=e^{-\beta\,J_{\lambda}^{\prime}[\eta](w)}\tau(\mathrm{ d}w)/Z_{\eta}\) where \(Z_{\eta}=\int e^{-\beta\,J_{\lambda}^{\prime}[\eta]}\mathrm{d}\tau\). First note that for any \(\eta,\eta^{\prime}\in\mathcal{P}(\mathcal{W})\),

\[\left|\log\frac{\mathrm{d}\hat{\eta}}{\mathrm{d}\hat{\eta^{ \prime}}}(w)+(\log Z_{\eta}-\log Z_{\eta^{\prime}})\right| =\beta\left|J_{\lambda}^{\prime}[\eta](w)-J_{\lambda}^{\prime}[ \eta^{\prime}](w)\right|\] \[=\beta\frac{\lambda}{2}\left|f_{\eta}(w)^{2}-f_{\eta^{\prime}}(w )^{2}\right|\] \[\leq\beta\frac{\lambda}{2}\left(\left|f_{\eta}\right|+\left|f_{ \eta^{\prime}}\right|\right)(w)\cdot\left|f_{\eta}-f_{\eta^{\prime}}\right|(w)\] \[\leq\beta\frac{\lambda}{2}\cdot 2\frac{1}{\lambda}\sqrt{2L_{0}G(0)} \cdot HW_{2}(\eta,\eta^{\prime})\eqqcolon\widetilde{H}W_{2}(\eta,\eta^{ \prime})\]

by Lem.D.4 and Lem.D.7, where \(H\) is a constant dependent only on \(\lambda^{-1},G(0),L_{0},L_{1},B_{1},\widetilde{L}_{2}\).

Now suppose that \(\eta_{\lambda,\beta}=\arg\min J_{\lambda,\beta}=\widehat{\eta_{\lambda,\beta}}\) satisfies \(\alpha^{*}\)-LSI. Let \(\varepsilon>0\) and \(\eta_{0}\) in the \(\delta\)-sublevel set of \(J_{\lambda,\beta}\), i.e., \(\eta_{0}\in S_{\delta}\coloneqq J_{\lambda,\beta}^{-1}((-\infty,\inf J_{ \lambda,\beta}+\delta])\), for some \(\delta>0\) to be chosen. Denote by \((\eta_{t})_{t}\) the MFLD trajectory for \(J_{\lambda,\beta}\) initialized at \(\eta_{0}\). Note that \(S_{\delta}\) is stable by MFLD since \(J_{\lambda,\beta}(\eta_{t})\) decreases with \(t\). So it suffices to show that \(J_{\lambda,\beta}\) satisfies \((\alpha^{*}-\varepsilon)\)-LSI uniformly over \(S_{\delta}\).

Choose any \(\eta\in S_{\delta}\), i.e., such that \(J_{\lambda,\beta}(\eta)-\inf J_{\lambda,\beta}\leq\delta\). In particular by Thm.2.1, it holds

\[\beta^{-1}H\left(\eta|\eta_{\lambda,\beta}\right)\leq J_{\lambda,\beta}(\eta)- \inf J_{\lambda,\beta}\leq\delta.\]

Furthermore, since \(\eta_{\lambda,\beta}\) satisfies LSI with constant \(\alpha^{*}\) then it also satisfies the following Talagrand inequality, as shown in [10]:

\[\forall\eta^{\prime},\ W_{2}(\eta^{\prime},\eta_{\lambda,\beta})\leq\sqrt{ \frac{2}{\alpha^{*}}H\left(\eta^{\prime}|\eta_{\lambda,\beta}\right)}.\]Then by the equality noted above, we have

\[\left|\log\frac{\mathrm{d}\hat{\eta}}{\mathrm{d}\eta_{\lambda,\beta}}(w)+c\right| \leq\widetilde{H}W_{2}(\eta,\eta_{\lambda,\beta})\leq\widetilde{H}\sqrt{\frac{2 }{\alpha^{*}}H\left(\eta|\eta_{\lambda,\beta}\right)}\leq\widetilde{H}\sqrt{ \frac{2}{\alpha^{*}}}\cdot\sqrt{\beta\delta}\ \ \rightleftharpoons:M\sqrt{\delta}\]

for some \(c\in\mathbb{R}\), and so by the Holley-Stroock bounded perturbation argument, \(\hat{\eta}\) satisfies LSI with constant \(\alpha^{*}e^{-M\sqrt{\delta}}\geq\alpha^{*}-\varepsilon\) for \(\delta\) small enough. 

### Preliminary estimates for \(J_{\lambda}\) under Assumption 2

Throughout the remainder of this appendix, in the context of Assumption 2, we will use the notations

* the Hilbert space \(\mathcal{H}=L^{2}_{\rho}(\mathbb{R}^{d+1})\) with the inner product \(\left\langle f,g\right\rangle_{\mathcal{H}}=\mathbb{E}_{x\sim\rho}f(x)g(x)\),
* the feature map \(\phi:\mathcal{W}\to\mathcal{H}\) given by \(\phi(w)(x)=\varphi(\left\langle w,x\right\rangle)\),
* the symmetric positive-semi-definite operator in \(\mathcal{H}\): \(K_{\eta}=\int\phi(w)\phi(w)^{*}\mathrm{d}\eta(w)\), where \(*\) denotes adjoint in \(\mathcal{H}\).
* For any \(h\in\mathcal{H}\), we denote by \(\left\langle h,\nabla\phi(w)\right\rangle_{\mathcal{H}}\) (resp. \(\left\langle h,\nabla^{2}\,\phi(w)\right\rangle_{\mathcal{H}}\)) the gradient (resp. Hessian) at \(w\) of \(w\mapsto\left\langle h,\phi(w)\right\rangle_{\mathcal{H}}\).

The usefulness of these notations is justified by Prop. F.1 below, which gives a simplified expression for \(J_{\lambda}\) and \(J_{\lambda}^{\prime}\).

**Proposition F.1**.: _Under Assumption 2, letting the Hilbert space \(\mathcal{H}=L^{2}_{\rho}(\mathbb{R}^{d+1})\) and the feature map \(\phi:\mathcal{W}\to\mathcal{H}\) given by \(\phi(w)(x)=\varphi(\left\langle w,x\right\rangle)\), we have_

\[J_{\lambda}(\eta)=\frac{\lambda}{2}\langle y,(K_{\eta}+\lambda\,\mathrm{id})^{ -1}y\rangle_{\mathcal{H}},\qquad\ J_{\lambda}^{\prime}[\eta](w)=-\frac{ \lambda}{2}\langle\phi(w),(K_{\eta}+\lambda\,\mathrm{id})^{-1}y\rangle_{ \mathcal{H}}^{2},\]

_with \(K_{\eta}=\int\phi(w)\phi(w)^{*}\mathrm{d}\eta(w)\), where \(*\) denotes adjoint in \(\mathcal{H}\). More explicitly, \(K_{\eta}\) is the integral operator of the kernel \(k_{\eta}(x,x^{\prime})=\int\varphi(\left\langle w,x\right\rangle)\varphi( \left\langle w,x^{\prime}\right\rangle)\mathrm{d}\eta(w)\) with respect to the distribution \(x\sim\rho\), i.e.,_

\[\forall h\in\mathcal{H}=L^{2}_{\rho}(\mathbb{R}^{d+1}),\ \ (K_{\eta}h)(x)= \mathbb{E}_{x^{\prime}\sim\rho}\left[k_{\eta}(x,x^{\prime})h(x^{\prime}) \right]\quad\text{in }L^{2}_{\rho}.\]

Proof.: Under Assumption 2 we have

\[G(\nu)=\frac{1}{2}\mathbb{E}_{x\sim\rho}\left|\int_{\mathcal{W}}\varphi( \left\langle w,x\right\rangle)\mathrm{d}\nu(w)-y(x)\right|^{2}=\frac{1}{2} \left\|\int_{\mathcal{W}}\phi(w)\mathrm{d}\nu(w)-y\right\|_{\mathcal{H}}^{2},\]

so the optimization problem (3.3) defining \(J_{\lambda}(\eta)\), for a fixed \(\eta\), writes

\[\min_{f\in L^{2}_{\eta}(\mathcal{W})}\frac{1}{2}\left\|\int_{\mathcal{W}}\phi (w)f(w)\mathrm{d}\eta(w)-y\right\|_{\mathcal{H}}^{2}+\frac{\lambda}{2}\int_{ \mathcal{W}}\left|f\right|^{2}(w)\mathrm{d}\eta(w).\]

This problem is strictly convex thanks to the term in \(\lambda\), and the FOC is \(\forall w,\ \left\langle\int\phi f\mathrm{d}\eta-y,\phi(w)\eta(\mathrm{d}w) \right\rangle_{\mathcal{H}}+\lambda f(w)\eta(\mathrm{d}w)=0\). So the unique minimum \(f_{\eta}\) is a solution of the fixed point equation \(f(w)=-\frac{1}{\lambda}\left\langle\int\phi f\mathrm{d}\eta-y,\phi(w)\right\rangle _{\mathcal{H}}\) in \(L^{2}_{\eta}(\mathcal{W})\). In particular, denoting \(\hat{h}_{\eta}=-\frac{1}{\lambda}\left(\int\phi f_{\eta}\mathrm{d}\eta-y\right)\), then \(f_{\eta}(w)=\left\langle\hat{h}_{\eta},\phi(w)\right\rangle_{\mathcal{H}}\) and, integrating against \(\phi\eta\),

\[\int_{\mathcal{W}}f_{\eta}(w)\phi(w)\mathrm{d}\eta(w)=\int_{ \mathcal{W}}\phi(w)\ \phi(w)^{*}\hat{h}_{\eta}\ \mathrm{d}\eta(w)\] \[\iff-\lambda\hat{h}_{\eta}+y=K_{\eta}\hat{h}_{\eta}\iff(K_{\eta}+ \lambda\,\mathrm{id})\hat{h}_{\eta}=y\iff\hat{h}_{\eta}=(K_{\eta}+\lambda\, \mathrm{id})^{-1}y,\]

where \(a^{*}\)\(b=\left\langle a,b\right\rangle_{\mathcal{H}}\) and \(K_{\eta}=\int_{\mathcal{W}}\phi(w)\phi(w)^{*}\mathrm{d}\eta(w)\). So the optimal value \(J_{\lambda}(\eta)\) is

\[J_{\lambda}(\eta) =\frac{1}{2}\left\|\int_{\mathcal{W}}\phi(w)f_{\eta}(w)\mathrm{d} \eta(w)-y\right\|_{\mathcal{H}}^{2}+\frac{\lambda}{2}\int_{\mathcal{W}}\left|f_{ \eta}\right|^{2}(w)\mathrm{d}\eta(w)\] (F.1) \[=\frac{1}{2}\left\|\lambda\hat{h}_{\eta}\right\|_{\mathcal{H}}^{2} +\frac{\lambda}{2}\int_{\mathcal{W}}\hat{h}_{\eta}^{*}\phi(w)\ \phi(w)^{*}\hat{h}_{\eta}\ \mathrm{d}\eta(w)\] \[=\frac{1}{2}\left\langle\lambda\hat{h}_{\eta},\lambda\hat{h}_{\eta }\right\rangle_{\mathcal{H}}+\frac{\lambda}{2}\left\langle\hat{h}_{\eta},K_{\eta} \hat{h}_{\eta}\right\rangle_{\mathcal{H}}=\frac{1}{2}\left\langle\lambda\hat{h}_{ \eta},\lambda\hat{h}_{\eta}+K_{\eta}\hat{h}_{\eta}\right\rangle_{\mathcal{H}}\] \[=\frac{1}{2}\left\langle\lambda\hat{h}_{\eta},y\right\rangle_{ \mathcal{H}}=\frac{\lambda}{2}\langle y,(K_{\eta}+\lambda\,\mathrm{id})^{-1}y \rangle_{\mathcal{H}}.\]Further, by applying the envelope theorem on (F.1) (and reasoning similarly to the proof of Prop. D.2 to deal with \(w\not\in\operatorname{supp}(\eta)\), by extending \(f_{\eta}\in L^{2}_{\eta}(\mathcal{W})\) into a function \(\mathcal{W}\to\mathbb{R}\)), we then have

\[\forall w\in\mathcal{W},\ J^{\prime}_{\lambda}[\eta](w) =\left\langle\int\phi f_{\eta}\mathrm{d}\eta-y,\phi(w)f_{\eta}(w )\right\rangle_{\mathcal{H}}+\frac{\lambda}{2}\left|f_{\eta}\right|^{2}(w)\] \[=-\lambda\left|f_{\eta}\right|^{2}(w)+\frac{\lambda}{2}\left|f_{ \eta}\right|^{2}(w)\ \ =-\frac{\lambda}{2}\left|f_{\eta}\right|^{2}(w)=-\frac{ \lambda}{2}\left\langle\hat{h}_{\eta},\phi(w)\right\rangle_{\mathcal{H}}^{2}.\]

The characterization of \(K_{\eta}\) as the integral operator in \(L^{2}_{\rho}(\mathbb{R}^{d+1})\) of the kernel \(k_{\eta}(x,x^{\prime})=\int_{\mathcal{W}}\phi(w)(x)\ \phi(w)(x^{\prime})\mathrm{d}\eta(w)\) follows directly from the definition \(K_{\eta}=\int_{\mathcal{W}}\phi(w)\phi(w)^{*}\mathrm{d}\eta(w)\), since

\[\forall h\in\mathcal{H},\ K_{\eta}h =\int_{\mathcal{W}}\phi(w)\left\langle\phi(w),h\right\rangle_{ \mathcal{H}}\mathrm{d}\eta(w),\] \[(K_{\eta}h)(x) =\int_{\mathcal{W}}\phi(w)(x)\ \mathbb{E}_{x^{\prime}\sim_{\rho}} \left[\phi(w)(x^{\prime})h(x^{\prime})\right]\ \mathrm{d}\eta(w)\] \[=\mathbb{E}_{x^{\prime}\sim_{\rho}}\left[\int_{\mathcal{W}}\phi( w)(x)\phi(w)(x^{\prime})\ h(x^{\prime})\ \mathrm{d}\eta(w)\right]\] \[=\mathbb{E}_{x^{\prime}\sim_{\rho}}\left[k_{\eta}(x,x^{\prime})h( x^{\prime})\right].\qed\]

We have the following Wasserstein Lipschitz-continuity properties for the bilevel objective functional \(J_{\lambda}\).

**Proposition F.2**.: _Under Assumption 2, suppose furthermore that \(\sup_{w}\left\|\nabla^{i}\phi(w)\right\|_{\mathcal{H}}\leq B_{i}<\infty\) for \(i\in\{0,1,2\}\). Then for any \(w\in\mathcal{W}=\mathbb{S}^{d}\) and any \(\eta,\eta^{\prime}\in\mathcal{P}(\mathcal{W})\), it holds_

\[\left|J_{\lambda}(\eta)-J_{\lambda}(\eta^{\prime})\right| \leq\frac{B_{0}B_{1}}{\lambda}\left\|y\right\|_{\mathcal{H}}^{2} \cdot W_{1}(\eta,\eta^{\prime})\] \[\text{and}\quad\left|J_{\lambda}^{\prime}[\eta](w)-J_{\lambda}^{ \prime}[\eta^{\prime}](w)\right| \leq\frac{2B_{0}^{3}B_{1}}{\lambda^{2}}\left\|y\right\|_{\mathcal{H }}^{2}\cdot W_{1}(\eta,\eta^{\prime})\] \[\text{and}\quad\left\|\nabla J_{\lambda}^{\prime}[\eta](w)- \nabla J_{\lambda}^{\prime}[\eta^{\prime}](w)\right\|_{w} \leq\frac{4B_{0}^{2}B_{1}^{2}}{\lambda^{2}}\left\|y\right\|_{ \mathcal{H}}^{2}\cdot W_{1}(\eta,\eta^{\prime})\] \[\text{and}\quad\left\|\nabla^{2}\,J_{\lambda}^{\prime}[\eta](w)- \nabla^{2}\,J_{\lambda}^{\prime}[\eta^{\prime}](w)\right\|_{\text{op }w} \leq\frac{4B_{0}B_{1}(B_{0}B_{2}+B_{1}^{2})}{\lambda^{2}}\left\|y\right\|_{ \mathcal{H}}^{2}\cdot W_{1}(\eta,\eta^{\prime}).\]

Proof.: By Prop. F.1,

\[J_{\lambda}^{\prime}[\eta](w) =-\frac{\lambda}{2}\left\langle\phi(w),(K_{\eta}+\lambda\, \mathrm{id})^{-1}y\right\rangle_{\mathcal{H}}^{2}\ \ \text{where}\ \ K_{\eta}=\int_{\mathcal{W}}\phi(w^{\prime\prime})\phi(w^{\prime\prime})^{*} \ \mathrm{d}\eta(w^{\prime\prime})\] (F.2) \[\left\|\nabla J_{\lambda}^{\prime}[\eta](w)\right\|_{w} \leq\lambda\left\|\phi(w)\right\|_{\mathcal{H}}\left\|\nabla\phi( w)\right\|_{w}\left\|(K_{\eta}+\lambda\,\mathrm{id})^{-1}y\right\|_{\mathcal{H}}^{2}\] \[\leq\lambda B_{0}B_{1}\left\|y\right\|_{\mathcal{H}}^{2}\left\|(K _{\eta}+\lambda)^{-1}\right\|_{\mathrm{op}}^{2}\] \[\leq\frac{1}{\lambda}B_{0}B_{1}\left\|y\right\|_{\mathcal{H}}^{2}\]

since \(K_{\eta}\) is positive-semi-definite by definition and so \(\left\|(K_{\eta}+\lambda)^{-1}\right\|_{\mathrm{op}}=\sigma_{\max}((K_{\eta}+ \lambda\,\mathrm{id})^{-1})=\left[\sigma_{\min}(K_{\eta}+\lambda\,\mathrm{id}) \right]^{-1}\leq\lambda^{-1}\). So by applying Lem. D.8, this shows the first inequality.

Moreover, the first variation of \(K_{\eta}\) at any \(\eta\) is \(w^{\prime}\mapsto\phi(w^{\prime})\phi(w^{\prime})^{*}\), thus by the formula \(\partial(X^{-1})=-X^{-1}(\partial X)X^{-1}\) for the derivative of a matrix inverse,

\[\frac{\delta}{\delta\eta(w^{\prime})}(K_{\eta}+\lambda\,\mathrm{id})^{-1}=-(K_{ \eta}+\lambda\,\mathrm{id})^{-1}\cdot\phi(w^{\prime})\phi(w^{\prime})^{*}\cdot(K _{\eta}+\lambda\,\mathrm{id})^{-1},\]and so, letting for concision \(M=(K_{\eta}+\lambda\operatorname{id})^{-1}\),

\[J^{\prime\prime}_{\lambda}[\eta](w,w^{\prime})\] \[=-\lambda\left\langle\phi(w),(K_{\eta}+\lambda\operatorname{id})^ {-1}y\right\rangle_{\mathcal{H}}\left\langle\phi(w),-(K_{\eta}+\lambda \operatorname{id})^{-1}\cdot\phi(w^{\prime})\phi(w^{\prime})^{*}\cdot(K_{\eta} +\lambda\operatorname{id})^{-1}y\right\rangle_{\mathcal{H}}\] \[=-\lambda\left\langle\phi(w),My\right\rangle_{\mathcal{H}}\left\langle \phi(w),-M\cdot\phi(w^{\prime})\phi(w^{\prime})^{*}\cdot My\right\rangle_{ \mathcal{H}}\] \[=\lambda\,\,\left\langle\phi(w),My\right\rangle_{\mathcal{H}}\, \,\,\,\left\langle\phi(w),M\phi(w^{\prime})\right\rangle_{\mathcal{H}}\,\,\, \,\left\langle\phi(w^{\prime}),My\right\rangle_{\mathcal{H}}.\]

As a result,

\[\nabla_{w}J^{\prime\prime}_{\lambda}[\eta](w,w^{\prime})=\lambda \left\langle\phi(w^{\prime}),My\right\rangle_{\mathcal{H}}\cdot\\ \left(\left\langle\nabla\phi(w),My\right\rangle\cdot\left\langle \phi(w),M\phi(w^{\prime})\right\rangle_{\mathcal{H}}+\left\langle\phi(w),My \right\rangle\cdot\left\langle\nabla\phi(w),M\phi(w^{\prime})\right\rangle_{ \mathcal{H}})\]

and, using again that \(\left\|M\right\|_{\operatorname{op}}=\left\|(K_{\eta}+\lambda)^{-1}\right\|_{ \operatorname{op}}\leq\lambda^{-1}\),

\[\left\|\nabla_{w}J^{\prime\prime}_{\lambda}(w,w^{\prime})\right\|_{w}\leq \lambda B_{0}\lambda^{-1}\left\|y\right\|_{\mathcal{H}}\cdot 2B_{0}^{2}B_{1} \lambda^{-2}\left\|y\right\|_{\mathcal{H}}=2\lambda^{-2}B_{0}^{3}B_{1}\left\|y \right\|_{\mathcal{H}}^{2}.\]

Then applying Lem.D.8 shows the second inequality.

Furthermore, for a fixed \(w\in\mathcal{W}\), continuing from the expression of \(\nabla_{w}J^{\prime\prime}_{\lambda}[\eta](w,w^{\prime})\) derived above,

\[\nabla_{w^{\prime}}\nabla_{w}J^{\prime\prime}_{\lambda}[\eta](w,w^{\prime})\] \[=\lambda\left\langle\nabla\phi(w^{\prime}),My\right\rangle_{ \mathcal{H}}\cdot\left(\left\langle\nabla\phi(w),My\right\rangle\cdot\left \langle\phi(w),M\phi(w^{\prime})\right\rangle_{\mathcal{H}}+\left\langle\phi(w ),My\right\rangle\cdot\left\langle\nabla\phi(w),M\phi(w^{\prime})\right\rangle _{\mathcal{H}})\] \[+\lambda\left\langle\phi(w^{\prime}),My\right\rangle_{\mathcal{H }}\cdot\left(\left\langle\nabla\phi(w),My\right\rangle\cdot\left\langle\phi(w ),M\nabla\phi(w^{\prime})\right\rangle_{\mathcal{H}}+\left\langle\phi(w),My \right\rangle\cdot\left\langle\nabla\phi(w),M\nabla\phi(w^{\prime})\right\rangle _{\mathcal{H}}\right),\]

so \(\left\|\nabla_{w^{\prime}}\nabla_{w}J^{\prime\prime}_{\lambda}[\eta](w,w^{ \prime})\right\|\leq 4\lambda^{-2}B_{0}^{2}B_{1}^{2}\left\|y\right\|_{\mathcal{H}}^ {2}\), and the third inequality follows by applying Lem.D.8 to \(\eta\mapsto\left\langle s,\nabla J^{\prime}_{\lambda}[\eta](w)\right\rangle_{w}\) for \(s\in T_{w}\mathcal{W}\) arbitrary.

Finally, by differentiating the expression of \(\nabla_{w^{\prime}}\nabla_{w}J^{\prime\prime}_{\lambda}[\eta](w,w^{\prime})\) once more with respect to \(w\) we get that, for any fixed \(w\in\mathcal{W}\),

\[\nabla_{w^{\prime}}\nabla_{w}^{2}J^{\prime\prime}_{\lambda}[\eta]( w,w^{\prime})\] \[=\lambda\left\langle\nabla\phi(w^{\prime}),My\right\rangle_{ \mathcal{H}}\cdot\left(\left\langle\nabla^{2}\phi(w),My\right\rangle\cdot \left\langle\phi(w),M\phi(w^{\prime})\right\rangle_{\mathcal{H}}+\left\langle \nabla\phi(w),My\right\rangle\cdot\left\langle\nabla\phi(w),M\phi(w^{\prime}) \right\rangle_{\mathcal{H}}\right)\] \[+\lambda\left\langle\nabla\phi(w^{\prime}),My\right\rangle_{ \mathcal{H}}\cdot\left(\left\langle\nabla\phi(w),My\right\rangle\cdot\left\langle \nabla\phi(w),M\phi(w^{\prime})\right\rangle_{\mathcal{H}}+\left\langle\phi(w ),My\right\rangle\cdot\left\langle\nabla^{2}\phi(w),M\phi(w^{\prime})\right\rangle _{\mathcal{H}}\right)\] \[+\lambda\left\langle\phi(w^{\prime}),My\right\rangle_{\mathcal{H }}\cdot\left(\left\langle\nabla^{2}\phi(w),My\right\rangle\cdot\left\langle \nabla\phi(w),M\nabla\phi(w^{\prime})\right\rangle_{\mathcal{H}}+\left\langle \phi(w),My\right\rangle\cdot\left\langle\nabla\phi(w),M\nabla\phi(w^{\prime}) \right\rangle_{\mathcal{H}}\right)\] \[+\lambda\left\langle\phi(w^{\prime}),My\right\rangle_{\mathcal{H }}\cdot\left(\left\langle\nabla\phi(w),My\right\rangle\cdot\left\langle \nabla\phi(w),M\nabla\phi(w^{\prime})\right\rangle_{\mathcal{H}}+\left\langle \phi(w),My\right\rangle\cdot\left\langle\nabla^{2}\phi(w),M\nabla\phi(w^{ \prime})\right\rangle_{\mathcal{H}}\right),\]

hence \(\left\|\nabla_{w^{\prime}}\nabla_{w}^{2}J^{\prime\prime}_{\lambda}[\eta](w,w^{ \prime})\right\|\leq\lambda^{-2}\left\|y\right\|^{2}B_{0}B_{1}(4B_{2}B_{0}+4B_{1} ^{2})\), and the fourth inequality follows by applying Lem.D.8 to \(\eta\mapsto\left\langle s,\nabla^{2}J^{\prime}_{\lambda}[\eta](w)\cdot s \right\rangle_{w}\) for \(s\in T_{w}\mathcal{W}\) arbitrary. 

The following lemma provides explicit upper estimates of the regularity constants \(B_{0},B_{1},B_{2}\) of \(\phi\) appearing in Prop.F.2, in terms of the activation function \(\varphi\) and the data distribution \(\rho\).

**Lemma F.3**.: _Under Assumption 2, recall that \(\phi:\mathcal{W}\to\mathcal{H}=L_{\rho}^{2}(\mathbb{R}^{d+1})\) is defined by \(\phi(w)(x)=\varphi(\left\langle w,x\right\rangle)\), and that \(\varphi:\mathbb{R}\to\mathbb{R}\) is \(\mathcal{C}^{2}\). There exists a universal constant \(c>0\) such that_

\[\sup_{w\in\mathbb{S}^{d}}\left\|\phi(w)\right\|_{\mathcal{H}} \leq\left\|\varphi\right\|_{L^{2}(\rho)},\] \[\sup_{w\in\mathbb{S}^{d}}\left\|\nabla\phi(w)\right\|_{\mathcal{H}} \leq\left\|\varphi^{\prime}\right\|_{L^{4}(\rho)}N_{4}(\rho),\] \[\sup_{w\in\mathbb{S}^{d}}\left\|\nabla^{2}\phi(w)\right\|_{ \mathcal{H}} \leq\left(\left\|\varphi^{\prime\prime}\right\|_{L^{4}(\rho)}+\left\| \varphi^{\prime}\right\|_{L^{4}(\rho)}\right)N_{4}(\rho)\]

_where_

\[N_{4}(\rho)\coloneqq\sup_{\left\|u\right\|_{2}\leq 1}\left(\mathbb{E}_{x\sim\rho} \left\langle u,x\right\rangle^{4}\right)^{1/4}\,\,\,\,\text{and}\,\,\,\,\,\forall f:\mathbb{R}\to\mathbb{R},\,\,\,\,\left\left\|f\right\|_{L^{p}(\rho)} \coloneqq\sup_{w\in\mathbb{S}^{d}}\left(\mathbb{E}_{x\sim\rho}\left|f( \left\langle w,x\right\rangle)\right|^{p}\right)^{1/p}.\]

_Note that if \(\rho\) is rotationally invariant, then \(\mathbb{E}_{x\sim\rho}\left|f(\left\langle w,x\right\rangle)\right|^{p}\) is independent of \(w\), and there exists a universal constant \(c\) such that \(N_{4}(\rho)\leq cd^{-1/2}\big{(}\mathbb{E}_{x\sim\rho}\left\|x\right\|^{4}\big{)}^ {1/4}\)._Proof.: For the first inequality, we have by definition

\[\sup_{w}\left\|\varphi(w)\right\|_{\mathcal{H}}=\sup_{w}\sqrt{\mathbb{E}_{x\sim \rho}\left|\varphi(\left\langle w,x\right\rangle)\right|^{2}}=\left\|\varphi \right\|_{L^{2}(\rho)}.\]

For the second inequality, define the orthogonal projector \(\Pi_{w}=I_{d+1}-ww^{\top}:\mathbb{R}^{d+1}\to T_{w}\mathbb{S}^{d}=\{w\}^{\perp}\) for any \(w\in\mathbb{S}^{d}\). Then \(\left[\nabla\phi(w)\right](x)=\varphi^{\prime}(\left\langle w,x\right\rangle) \Pi_{w}x\), so by Cauchy-Schwarz inequalities,

\[\left\|\nabla\phi(w)\right\|_{\mathcal{H}} =\Big{(}\sup_{\left\|I\right\|_{L^{2}(\rho)}\leq 1}\sup_{ \begin{subarray}{c}s\in T_{w}\mathbb{S}^{d}\\ \left\|s\right\|_{w}=1\end{subarray}}\mathbb{E}_{x\sim\rho}\left[f(x)\left\langle s,\nabla\phi(w)(x)\right\rangle_{w}\right]\Big{)}\] \[=\sup_{\begin{subarray}{c}s\in T_{w}\mathbb{S}^{d}\\ \left\|s\right\|_{w}=1\end{subarray}}\mathbb{E}_{x\sim\rho}\left[\left\langle s,\nabla\phi(w)(x)\right\rangle_{w}^{2}\right]^{1/2}=\sup_{\begin{subarray}{c }s\in T_{w}\mathbb{S}^{d}\\ \left\|s\right\|_{w}=1\end{subarray}}\mathbb{E}_{x\sim\rho}\left[\left|\varphi ^{\prime}(\left\langle w,x\right\rangle)\right|^{2}\left\langle\Pi_{w}s,x \right\rangle^{2}\right]^{1/2}\] \[\leq\left(\mathbb{E}_{x\sim\rho}\left[\left|\varphi^{\prime}( \left\langle w,x\right\rangle)\right|^{4}\right]\right)^{1/4}\cdot\sup_{\left\| u\right\|_{2}=1}\left(\mathbb{E}_{x\sim\rho}\left[\left\langle u,x\right\rangle^{4} \right]\right)^{1/4}\]

since \(\left\|s\right\|_{w}=\left\|\Pi_{w}s\right\|_{2}\).

For the third inequality, the Riemannian Hessian of \(\phi(w)=\varphi(\left\langle w,\cdot\right\rangle):\mathbb{S}^{d}\to\mathbb{R}\) is given by

\[\left[\nabla^{2}\phi(w)\right](x)=\nabla_{w}^{2}\varphi(\left\langle w,x \right\rangle)=\nabla_{w}^{\top}\left[\varphi^{\prime}(\left\langle w,x \right\rangle)\Pi_{w}x\right]=\Pi_{w}\left(\varphi^{\prime\prime}(\left\langle w,x\right\rangle)xx^{\top}-\varphi^{\prime}(\left\langle w,x\right\rangle) \left\langle w,x\right\rangle\right)\Pi_{w},\]

so similarly by Cauchy-Schwarz inequalities,

\[\left\|\nabla^{2}\phi(w)\right\|_{\mathcal{H}} \leq\sup_{\begin{subarray}{c}s\in T_{w}\mathbb{S}^{d}\\ \left\|s\right\|_{w}=1\end{subarray}}\mathbb{E}_{x\sim\rho}\left[\left| \varphi^{\prime\prime}(\left\langle w,x\right\rangle)\right|^{2}\left\langle s,\Pi_{w}x\right\rangle^{2}\right]^{1/2}+\mathbb{E}_{x\sim\rho}\left[\left| \varphi^{\prime}(\left\langle w,x\right\rangle)\right|^{2}\left\langle w,x \right\rangle^{2}\right]^{1/2}\] \[\leq\left(\mathbb{E}_{x\sim\rho}\left[\left|\varphi^{\prime \prime}(\left\langle w,x\right\rangle)\right|^{4}\right]\right)^{1/4}\cdot \sup_{\begin{subarray}{c}s\in T_{w}\mathbb{S}^{d}\\ \left\|s\right\|_{w}=1\end{subarray}}\left(\mathbb{E}_{x\sim\rho}\left[\left \langle\Pi_{w}s,x\right\rangle^{4}\right]\right)^{1/4}\] \[\quad+\left(\mathbb{E}_{x\sim\rho}\left[\left|\varphi^{\prime}( \left\langle w,x\right\rangle)\right|^{4}\right]\right)^{1/4}\left(\mathbb{E}_{ x\sim\rho}\left[\left\langle w,x\right\rangle^{4}\right]\right)^{1/4}.\]

Finally, suppose that \(\rho\) is rotationally invariant, and let us show that \(N_{4}(\rho)\leq cd^{-1/2}\left(\mathbb{E}_{x\sim\rho}\left\|x\right\|^{4} \right)^{1/4}\) for some universal constant \(c\). Indeed, for \(x\sim\rho\), we have that \(x\) and \(\overline{x}=x/\left\|x\right\|\) are independent and that \(\overline{x}\sim\tau\). Therefore,

\[N_{4}^{4}(\rho)=\sup_{\left\|u\right\|_{2}\leq 1}\mathbb{E}_{x\sim\rho}\left\|x \right\|^{4}\left\langle u,x/\left\|x\right\|\right\rangle^{4}=\sup_{\left\|u \right\|_{2}\leq 1}\left(\mathbb{E}_{x\sim\rho}\left\|x\right\|^{4}\right)\cdot \left(\mathbb{E}_{\overline{x}\sim\tau}\left\langle u,\overline{x}\right\rangle ^{4}\right),\]

and \(\sup_{\left\|u\right\|_{2}\leq 1}\mathbb{E}_{\overline{x}\sim\tau}\left\langle u,\overline{x} \right\rangle^{4}\leq\frac{c}{(d+1)^{2}}\) for some universal constant \(c\), which is a direct consequence of the fact that \(\left\langle u,\overline{x}\right\rangle\) is sub-Gaussian with sub-Gaussian norm \(\tilde{c}/\sqrt{d+1}\) for some universal constant \(\tilde{c}\)[20, Theorem 3.4.6], along with the moment bound for sub-Gaussian random variables [20, Proposition 2.5.2] 

Finally, we check rigorously in the following proposition that Assumption 2 with proper additional regularity assumptions on \(\varphi\) and \(\rho\), is a special case of Assumption 1.

**Proposition E.4**.: _Consider \(\mathcal{W}=\mathbb{S}^{d}\) and \(G:\mathcal{M}(\mathcal{W})\to\mathbb{R}\) defined as in Assumption 2. Suppose furthermore that \(N_{4}(\rho)\) and \(\left\|\cdot\right\|_{L^{p}(\rho)}\) are defined in Lem. F.3. Then, \(G\) and \(\mathcal{W}\) satisfy Assumption 1._

Proof.: The fact that \(\mathbb{S}^{d}\) satisfies \(\alpha_{\tau}\)-LSI with \(\alpha_{\tau}=d-1\) is classical and can be found in [1, Sec. 5.7].

By definition, \(G(\nu)=\frac{1}{2}\left\|\int_{\mathcal{W}}\phi(w)\mathrm{d}\nu(w)-y\right\|_{ \mathcal{H}}^{2}\), so \(G\) is non-negative and admits second variations: for any \(\nu\in\mathcal{M}(\mathcal{W})\) and \(w,w^{\prime}\in\mathbb{S}^{d}\),

\[G^{\prime}[\nu](w) =\left\langle\phi(w),\int_{\mathcal{W}}\phi(w^{\prime})\mathrm{d} \nu(w^{\prime})-y\right\rangle_{\mathcal{H}}\] \[G^{\prime\prime}[\nu](w,w^{\prime}) =\left\langle\phi(w),\phi(w^{\prime})\right\rangle_{\mathcal{H}}\] \[\text{and}\quad\nabla_{w}G^{\prime\prime}[\nu](w,w^{\prime}) =\left\langle\nabla\phi(w),\phi(w^{\prime})\right\rangle_{ \mathcal{H}}\] \[\nabla_{w}^{2}G^{\prime\prime}[\nu](w,w^{\prime}) =\left\langle\nabla^{2}\phi(w),\phi(w^{\prime})\right\rangle_{ \mathcal{H}}\] \[\nabla_{w}\nabla_{w^{\prime}}G^{\prime\prime}[\nu](w,w^{\prime}) =\left\langle\nabla\phi(w),\nabla\phi(w^{\prime})\right\rangle_{ \mathcal{H}}.\]

Consequently, denoting \(C_{i}=\sup_{w\in\mathbb{S}^{d}}\left\|\nabla^{i}\phi\right\|_{\mathcal{H}}\) for \(i\in\{0,1,2\}\), which are all finite by Lem. F.3,

\[\left|G^{\prime\prime}[\nu](w,w^{\prime})\right| \leq C_{0}^{2}\eqqcolon L_{0}\] \[\left\|\nabla_{w}G^{\prime\prime}[\nu](w,w^{\prime})\right\|_{w} \leq C_{0}C_{1}\eqqcolon L_{1}\] \[\left\|\nabla_{w}^{2}G^{\prime\prime}[\nu](w,w^{\prime})\right\| _{w} \leq C_{0}C_{2}\eqqcolon L_{2}\] \[\left\|\nabla_{w}\nabla_{w^{\prime}}G^{\prime\prime}[\nu](w,w^{ \prime})\right\| \leq C_{1}^{2}\eqqcolon\widetilde{L}_{2}.\]

Now for each \(i\in\{0,1,2\}\),

\[\forall(\nu,w,w^{\prime}),\left\|\nabla_{w}^{i}G^{\prime\prime}[\nu](w,w^{ \prime})\right\|_{w} \leq L_{i}\implies\forall(\nu,\nu^{\prime},w),\left\|\nabla^{i}G^{ \prime}[\nu]-\nabla^{i}G^{\prime}[\nu^{\prime}]\right\|_{w}\leq L_{i}\left\| \nu-\nu^{\prime}\right\|_{TV}.\]

Indeed, the right-hand side can be shown by applying the mean-value theorem to \(g(\theta)=\left\langle s,\nabla^{i}G^{\prime}[\nu+\theta(\nu^{\prime}-\nu)](w )\right\rangle_{w}\) over \(\theta\in[0,1]\) for each \(s\in(T_{w}\mathcal{W})^{\otimes i}\). Thus, to show the existence of \(B_{i}<\infty\) such that \(\forall(\nu,w,w^{\prime}),\left\|\nabla^{i}G^{\prime}[\nu]\right\|_{w}\leq L_ {i}\left\|\nu\right\|_{TV}+B_{i}\), it suffices to check that there exists \(\nu_{0}\) such that \(\left\|\nu_{0}\right\|_{TV}\) and \(\sup_{w}\left\|\nabla^{i}G^{\prime}[\nu_{0}]\right\|_{w}<\infty\). Note that for any \(\nu\) and \(w\),

\[\nabla^{i}G^{\prime}[\nu](w) =\left\langle\nabla^{i}\phi(w),\int_{\mathcal{W}}\phi(w^{\prime}) \mathrm{d}\nu(w^{\prime})-y\right\rangle_{\mathcal{H}},\] \[\text{thus}\quad\nabla^{i}G^{\prime}[0](w) =-\left\langle\nabla^{i}\phi(w),y\right\rangle_{\mathcal{H}}\] \[\text{and}\quad\sup_{w}\left\|\nabla^{i}G^{\prime}[0](w)\right\|_{w} \leq C_{i}\left\|y\right\|_{\mathcal{H}}<\infty.\]

Hence the existence of the \(B_{i}<\infty\) is verified. This finishes the verification of Assumption 1. 

### Proof of Thm. 5.2

In the single-index setting of Assumption 3, it is intuitive that \(\delta_{v}\) is a minimizer of \(J_{\lambda}\), for any \(\lambda\geq 0\), and that \(\eta_{\lambda,\beta}\) and \(\delta_{v}\) are close in certain regimes of \(\beta\) and \(\lambda\). For this reason, we will first investigate the properties of \(J_{\lambda}^{\prime}[\delta_{v}]\) as a proxy of \(J_{\lambda}^{\prime}[\eta_{\lambda,\beta}]\), to show that it is amenable to a refined analysis for proving LSI, in Sec. F.2.1. This step uses a Lyapunov approach inspired by [14, 15]. We will then prove that these properties carry from \(J_{\lambda}^{\prime}[\delta_{v}]\) over to \(J_{\lambda}^{\prime}[\eta_{\lambda,\beta}]\), in Sec. F.2.2, thanks to a quantitative bound on \(W_{2}(\eta_{\lambda,\beta},\delta_{v})\) proved in Sec. F.2.3.

**Lemma F.5**.: _Under Assumptions 2 and 3, we have_

\[\forall w\in\mathbb{S}^{d},J_{\lambda}^{\prime}[\delta_{v}](w) =-\frac{\lambda}{2}\left(\lambda+\left\|\phi(v)\right\|_{\mathcal{ H}}^{2}\right)^{-2}\left\langle\phi(v),\phi(w)\right\rangle_{\mathcal{H}}^{2}\] \[=-\frac{\lambda}{2}\left(\lambda+\left\|\varphi\right\|_{L^{2}( \rho)}^{2}\right)^{-2}\left|\mathbb{E}_{x\sim\rho}\varphi(\langle x,v\rangle) \varphi(\langle x,w\rangle)\right|^{2}\] \[=-\lambda g(\langle v,w\rangle)\]

_for some \(g:[-1,+1]\to\mathbb{R}\)._

Proof.: By Prop. F.1, since \(y=\phi(v)\),

\[J_{\lambda}^{\prime}[\delta_{v}]=-\frac{\lambda}{2}\left\langle\phi(w),(K_{ \delta_{v}}+\lambda\,\mathrm{id})^{-1}\phi(v)\right\rangle_{\mathcal{H}}^{2}.\]Since \(\phi(v)\) is an eigenvector of \(K_{\delta_{v}}=\int_{\mathcal{W}}\phi(w^{\prime})\phi(w^{\prime})^{*}\mathrm{d} \delta_{v}=\phi(v)\phi(v)^{*}\) with eigenvalue \(\left\|\phi(v)\right\|_{\mathcal{H}}^{2}=\mathbb{E}_{x\sim\rho}\varphi\langle \langle x,v\rangle\rangle^{2}=\left\|\varphi\right\|_{L^{2}(\rho)}^{2}\), it is also an eigenvector of \((K_{\delta_{v}}+\lambda\,\mathrm{id})^{-1}\) with eigenvalue \((\left\|\varphi(v)\right\|_{\mathcal{H}}^{2}+\lambda)^{-1}\), whence the expression of \(J_{\lambda}^{\prime}[\delta_{v}]\) follows.

Moreover, by rotational invariance of \(\rho\), \(\mathbb{E}_{x\sim\rho}\varphi\langle\langle x,v\rangle\rangle\varphi(\langle x,w\rangle)\) depends only on \(\langle v,w\rangle\), for all \(w\in\mathbb{S}^{d}\). In other words, there exists \(g\) such that \(J_{\lambda}^{\prime}[\delta_{v}]=-\lambda g(\langle v,\cdot\rangle)\). 

f.2.1 Lyapunov function analysis for bounding the LSI constant of \(\widehat{\delta_{v}}\propto e^{-\beta J_{\lambda}^{\prime}[\delta_{v}]}\tau\)

Observe that by the assumption \(g^{\prime}\geq c_{1}>0\) of Thm. 5.2, \(J_{\lambda}^{\prime}[\delta_{v}]=-\lambda g(\langle v,\cdot\rangle)\) has a unique global minimum at \(v\). Moreover, our other assumptions on \(g\) will imply that the Riemannian Hessian at optimum \(\nabla^{2}\,J_{\lambda}^{\prime}[\delta_{v}](v)\) is positive definite. This motivates us to follow the strategy of [12, Thm. 3.4] for proving LSI for \(\widehat{\delta_{v}}\propto e^{-\beta J_{\lambda}^{\prime}[\delta_{v}]}\tau\). Let us first outline the strategy and recall some useful classical notions.

The generator of the Langevin diffusion with invariant measure \(\exp(-\beta f)\tau/Z\) is

\[\mathcal{L}=\Delta-\beta\langle\nabla f,\nabla\rangle.\] (F.3)

Define \(U=\{w:\mathrm{dist}_{\mathcal{W}}(w,v)\leq r\}\) for some \(v\in\mathbb{S}^{d}\), with \(r>0\) to be chosen later. We say \(W:\mathbb{S}^{d}\to[1,\infty)\) is a Lyapunov function if \(\frac{\mathcal{L}W}{W}\leq-\theta+b\mathbf{1}_{U}\), for constants \(\theta>0\) and \(b\geq 0\). When proving functional inequalities for a Gibbs measure \(\exp(-\beta f)\tau/Z\), a typical choice of Lyapunov function is \(W=\exp(\beta(f-\min f)/2)\), for which the Lyapunov condition writes

\[\frac{\beta\Delta f}{2}-\frac{\beta^{2}\left\|\nabla f\right\|^{2}}{4}\leq- \theta+b\mathbf{1}_{U}.\] (F.4)

Further, we say a probability measure \(\nu\in\mathcal{P}(\mathbb{S}^{d})\) satisfies a local Poincare inequality on \(U\) with constant \(\kappa_{U}\) if

\[\int_{U}f^{2}\mathrm{d}\nu\leq\frac{1}{\kappa_{U}}\int_{U}\left\|\nabla f \right\|^{2}\mathrm{d}\nu,\quad\text{for all smooth $f:U\to\mathbb{R}$ such that $\int_{U}f\mathrm{d}\nu=0$}.\]

Notice that \(U\) has a convex boundary, thus we can use the Bakry-Emery criterion as adapted to manifolds with convex boundaries by [12, Proposition B.11] to prove a local Poincare inequality on \(U\). Specifically, it suffices to have \(\inf_{w\in U}\lambda_{\min}(\nabla^{2}f(w))>0\).

In summary, a Lyapunov condition of the form (F.4), along with a control on the eigenspectrum of \(\nabla^{2}f(w)\), implies an LSI for \(e^{-\beta f}\tau/Z\). We record this fact in the theorem below, working out the proper dependence on problem parameters for future use.

**Theorem F.6**.: _Let \(v\in\mathbb{S}^{d}\), \(0<\lambda\leq 1\) and \(f:\mathbb{S}^{d}\to\mathbb{R}\) of the form \(f(w)=-\lambda g(\langle w,v\rangle)\) for some increasing function \(g:[-1,1]\to\mathbb{R}\). Suppose there exist constants \(D_{0},D_{1},D_{2},D_{3},D_{4}>0\), and \(r\in(0,\pi/2)\) such that if \(\beta\geq D_{0}d\lambda^{-1}\) then_

\[\forall w\in\mathbb{S}^{d},\ \frac{1}{2}\Delta f-\frac{\beta}{4} \left\|\nabla f\right\|^{2} \leq D_{1}\lambda d\] ( \[\mathrm{L}_{\mathbb{S}^{d}}\] ) \[\forall w\in\mathbb{S}^{d}\setminus U,\ \frac{1}{2}\Delta f-\frac{\beta}{4} \left\|\nabla f\right\|^{2} \leq-D_{2}\beta\lambda^{2}\] ( \[\mathrm{L}_{U}\] ) \[\forall w\in\mathbb{S}^{d},\ \lambda_{\min}(\nabla^{2}f(w)) \geq-D_{3}\lambda\] ( \[\mathrm{C}_{\mathbb{S}^{d}}\] ) \[\forall w\in U,\ \lambda_{\min}(\nabla^{2}f(w)) \geq D_{4}\lambda\] ( \[\mathrm{C}_{U}\] )

_where \(U=\left\{w\in\mathbb{S}^{d};\ \mathrm{dist}_{\mathcal{W}}(w,v)\leq r\right\}\). Then (provided that \(\beta\geq D_{0}d\lambda^{-1}\)) the probability measure \(\nu=\exp(-\beta f)\tau/Z\) satisfies \(\alpha\)-LSI for a constant \(\alpha\) dependent only on the \(D_{i}\) and on \(r\)._

_Furthermore, if the condition on \(\beta\) is replaced by \(\beta\geq D_{0}^{\prime}d^{4}\lambda^{-4}\) and if (\(\mathrm{L}_{\mathbb{S}^{d}}\)) is replaced by_

\[\forall w\in\mathbb{S}^{d},\ \frac{1}{2}\Delta f-\frac{\beta}{4}\left\|\nabla f \right\|^{2}\leq D_{1}^{\prime}\lambda d\beta^{3/4},\] ( \[\mathrm{L}_{\mathbb{S}^{d}}^{\prime}\] )

_then (provided that \(\beta\geq D_{0}^{\prime}d^{4}\lambda^{-4}\)) \(\nu\) satisfies \(\alpha^{\prime}\)-LSI for a constant \(\alpha^{\prime}\) dependent only on \(D_{0}^{\prime},D_{1}^{\prime},\)\(D_{2},D_{3},D_{4}\) and on \(r\)._Proof.: By the Lyapunov criterion for Poincare inequality [12, Thm. 4.6.2], if the generator \(\mathcal{L}\) given by (F.3) satisfies the Lyapunov condition \(\frac{\kappa}{W}\leq-\theta+b\mathds{1}_{U}\) for some \(\theta>0\), \(b\geq 0\), \(U\subset\mathbb{S}^{d}\) and \(W:\mathbb{S}^{d}\to\mathbb{R}\), and if \(\nu\) satisfies a local Poincare inequality on \(U\) with constant \(\kappa_{U}\), then \(\nu\) satisfies a Poincare inequality on \(\mathbb{S}^{d}\) with constant \(\kappa\geq\frac{\theta}{1+\frac{\kappa}{\kappa_{U}}}\).

Let us apply this to \(W=\exp(\beta(f-\min f)/2)\). By (\(\mathbb{L}_{\mathbb{S}^{d}}\)) and (\(\mathbb{L}_{U}\)), the Lyapunov condition holds with \(\theta=D_{2}\beta^{2}\lambda^{2}\) and \(b=D_{1}\lambda\beta(d-1)+D_{2}\beta^{2}\lambda^{2}\). Moreover, since \(U\) has a convex boundary (the geodesic in \(\mathbb{S}^{d}\) between any two points in \(U\) remains in \(U\) for \(r<\pi/2\)), by [13, Proposition B.11]\(\nu\) satisfies a local Poincare inequality on \(U\) with constant

\[\kappa_{U}\geq\mathrm{Ric}_{g}+\beta\lambda_{\min}(\nabla^{2}f(w))\geq d-1+ \beta\lambda D_{4}\]

where \(\mathrm{Ric}_{g}\) denotes the Ricci curvature of \(\mathbb{S}^{d}\). As a result, \(\nu\) satisfies Poincare inequality with constant

\[\kappa\geq\frac{D_{2}\beta^{2}\lambda^{2}}{1+\frac{D_{1}\lambda\beta d+D_{2} \beta^{2}\lambda^{2}}{d-1+\beta\lambda D_{4}}}\geq C\beta\lambda,\] (F.5)

for some constant \(C\) depending only on the \(D_{i}\), where we used that \(\beta\geq D_{0}d\lambda^{-1}\).

Moreover, by [13, Proposition 9.17], if \(\nu\in\mathcal{P}(\mathbb{S}^{d})\) satisfies the Poincare inequality with constant \(\kappa\), and \(\beta\nabla^{2}f+\mathrm{Ric}_{g}\succcurlyeq-\beta K\) for some \(K>0\) on \(\mathbb{S}^{d}\), then for \(\beta\geq 1\), \(\nu\) satisfies the LSI with constant \(\alpha=\frac{\kappa}{11\beta K}\). By the assumptions of the theorem, this indeed holds with \(K=D_{3}\lambda\). Consequently, \(\nu\) satisfies LSI with constant \(\alpha=C/(11D_{3})\), which finishes the proof of the first part of the theorem.

The second part, with (\(\mathbb{L}_{\mathbb{S}^{d}}^{\prime}\)) instead of (\(\mathbb{L}_{\mathbb{S}^{d}}\)), follows by a similar reasoning, except that "\(D_{1}\)" should be replaced by "\(D_{1}^{\prime}\beta^{3/4}\)" in the calculation of (F.5). This still leads to a bound of the form \(\kappa\geq C^{\prime}\beta\lambda\) provided that \(\beta\geq D_{0}^{\prime}d^{4}\lambda^{-4}\), and the rest of the proof follows without change. 

We now verify that \(J_{\lambda}^{\prime}[\delta_{v}]\) satisfies the conditions of Thm. F.6.

**Proposition F.7**.: _Under the assumptions of Thm. 5.2, \(f_{0}\coloneqq J_{\lambda}^{\prime}[\delta_{v}]\) satisfies the conditions of Thm. F.6 with \(D_{0},...,D_{4},r\) dependent only on \(c_{1},C_{1},C_{2},C_{3}\)._

Proof.: The Riemannian gradient and Hessian of \(f_{0}=J_{\lambda}^{\prime}[\delta_{v}]=-\lambda g(\langle v,\cdot\rangle)\) are given by

\[\nabla f_{0}(w) =-\lambda g^{\prime}(\langle w,v\rangle)\Pi_{w}v\] \[\text{and}\quad\nabla^{2}\,f_{0}(w) =-\lambda\Pi_{w}\left(g^{\prime\prime}(\langle w,v\rangle)vv^{ \top}-g^{\prime}(\langle w,v\rangle)\,\langle w,v\rangle\,I_{d+1}\right)\Pi_{w}\]

where \(\Pi_{w}=I_{d+1}-ww^{\top}:\mathbb{R}^{d+1}\to T_{w}\mathbb{S}^{d}=\{w\}^{\perp}\) for any \(w\in\mathbb{S}^{d}\). This can be shown by considering the smooth extension of \(f_{0}\) to \(\mathbb{R}^{d+1}\to\mathbb{R}\) defined by \(x\mapsto-\lambda g(\langle v,x\rangle)\) and using that \(\mathbb{S}^{d}\) is a sub-Riemannian manifold of \(\mathbb{R}^{d+1}\)[1, Chap. 5]. In particular since \(v^{\top}\Pi_{w}\Pi_{w}v=1-\langle w,v\rangle^{2}\) and \(\mathrm{Tr}\,\Pi_{w}=d\),

\[\left\|\nabla f_{0}(w)\right\|^{2} =\lambda^{2}g^{\prime}(\langle w,v\rangle)^{2}(1-\langle w,v \rangle^{2})\] (F.6) \[\text{and}\quad\Delta f_{0}(w) =\mathrm{Tr}\,\nabla^{2}\,f_{0}(w) =-\lambda\left(g^{\prime\prime}(\langle w,v\rangle)(1-\langle w,v \rangle^{2})-g^{\prime}(\langle w,v\rangle)\,\langle w,v\rangle\,d\right).\]

Pose \(U=\left\{w\in\mathbb{S}^{d}:\mathrm{dist}_{\mathbb{S}^{d}}(w,v)\leq r\right\}\) for some \(r>0\) to be chosen.

Let us verify (\(\mathbb{L}_{\mathbb{S}^{d}}\)). We have for all \(w\in\mathbb{S}^{d}\)

\[\frac{1}{2}\Delta f_{0}-\frac{\beta}{4}\left\|\nabla f_{0}\right\|^{2}=-\frac{ \lambda}{4}\left(2g^{\prime\prime}(\langle w,v\rangle)+\beta\lambda g^{\prime} (\langle w,v\rangle)^{2}\right)\ (1-\langle w,v\rangle^{2})+\frac{\lambda}{2}g^{\prime}( \langle w,v\rangle)\langle w,v\rangle d.\]

The second term is bounded by \(\frac{\lambda}{2}C_{1}d\). We can ensure that the first term is non-positive by appropriately restricting \(\beta\) as follows:

\[\inf_{[-1,1]}\left[2g^{\prime\prime}+\beta\lambda(g^{\prime})^{2}\right]\geq 0 \iff 2(\inf g^{\prime\prime})+\beta\lambda(\inf g^{\prime})^{2}\geq 0\] \[\iff -2C_{2}+\beta\lambda c_{1}^{2}\geq 0\iff\beta\geq\frac{2C_{2}}{c_{1}^{2}} \lambda^{-1}.\]Let us verify (L\({}_{U}\)). We can upper-bound the first term in (F.6) by a negative quantity by restricting \(\beta\) further: by a similar calculation as just above,

\[\beta\geq\frac{4C_{2}}{c_{1}^{2}\lambda}\implies\inf_{[-1,1]}\left[2g^{\prime \prime}+\frac{\beta}{2}\lambda(g^{\prime})^{2}\right]\geq 0\implies 2g^{\prime\prime}+\beta \lambda(g^{\prime})^{2}\geq\frac{\beta}{2}\lambda(g^{\prime})^{2}\text{ \; over }[-1,1].\]

Then for all \(w\in\mathbb{S}^{d}\setminus U\), we have \(r\leq\operatorname{dist}_{\mathcal{W}}(w,v)=\arccos(\langle w,v\rangle)\leq \frac{\pi}{2}\sqrt{1-\langle w,v\rangle^{2}}\), and so

\[\frac{1}{2}\Delta f_{0}-\frac{\beta}{4}\left\|\nabla f_{0}\right\| ^{2} \leq-\frac{\lambda}{4}\left(\frac{1}{2}\beta\lambda g^{\prime}( \langle w,v\rangle)^{2}\right)\;(1-\langle w,v\rangle^{2})+\frac{\lambda}{2}g^ {\prime}(\langle w,v\rangle)\langle w,v\rangle d\] \[=\frac{\lambda}{4}g^{\prime}(\langle w,v\rangle)\left\{-\frac{ \beta\lambda}{2}g^{\prime}(\langle w,v\rangle)(1-\langle w,v\rangle^{2})+2 \langle w,v\rangle d\right\}\] \[\leq\frac{\lambda}{4}g^{\prime}(\langle w,v\rangle)\left\{-\frac{ 2\beta\lambda c_{1}r^{2}}{\pi^{2}}+2\langle w,v\rangle d\right\}\] \[\leq-\frac{\lambda}{4}g^{\prime}(\langle w,v\rangle)\cdot\frac{ \beta\lambda c_{1}r^{2}}{\pi^{2}}\;\;\leq-\frac{c_{1}^{2}}{4\pi^{2}}\beta \lambda^{2}r^{2}\]

provided that \(\beta\geq\frac{2\pi^{2}d}{\lambda c_{1}r^{2}}\).

To verify (C\({}_{\mathbb{S}^{d}}\)), simply note that, since \(\left\|\Pi_{w}vv^{\top}\Pi_{w}\right\|_{\mathrm{op}}=\left\|\Pi_{w}v\right\|^{ 2}=1-\left\langle w,v\right\rangle^{2}\),

\[\forall w,\;\left\|\nabla^{2}f_{0}(w)\right\|_{\mathrm{op}} \leq\lambda g^{\prime\prime}(\langle w,v\rangle)(1-\langle w,v \rangle^{2})+\lambda C_{1}\] \[\leq\lambda\left[\sup_{s\in[-1,1]}g^{\prime\prime}(s)(1-s^{2}) \right]+\lambda C_{1}\;\;\leq(C_{3}+C_{1})\lambda,\]

and therefore, \(\inf_{w\in\mathbb{S}^{d}}\lambda_{\min}(\nabla^{2}f_{0}(w))\geq-\left(\sup_{w }\left\|\nabla^{2}f_{0}(w)\right\|_{\mathrm{op}}\right)\geq-(C_{3}+C_{1})\lambda\).

Finally, let us verify (C\({}_{U}\)). Indeed, for any \(w\in U\),

\[\lambda_{\min}(\nabla^{2}f_{0}(w)) =\min_{\|u\|^{2}=1,\langle u,w\rangle=0}-\lambda g^{\prime\prime} (\langle w,v\rangle)\langle u,v\rangle^{2}+\lambda g^{\prime}(\langle w,v \rangle)\langle w,v\rangle\] \[\geq-\lambda\left|g^{\prime\prime}(\langle w,v\rangle)\right| \max_{\|u\|^{2}=1,\langle u,w\rangle=0}\langle u,v\rangle^{2}+\lambda c_{1} \langle w,v\rangle\] \[=-\lambda\left|g^{\prime\prime}(\langle w,v\rangle)\right|(1- \langle w,v\rangle^{2})+\lambda c_{1}\langle w,v\rangle,\]

where the bound of the second term follows from \(\langle w,v\rangle\geq 0\), which can be ensured by taking \(r\leq\frac{\pi}{2}\). Since \(w\in U\iff\langle w,v\rangle\geq\cos(r)\geq 1-r^{2}\), it follows that

\[\lambda_{\min}(\nabla^{2}f_{0}(w)) \geq-\lambda\left[\sup_{\cos r\leq s\leq 1}\left|g^{\prime \prime}(s)\right|(1-s^{2})\right]+\lambda c_{1}\cos r\] \[\geq-\lambda C_{3}\left[\sup_{\cos r\leq s\leq 1}\sqrt{1-s^{2}} \right]+\lambda c_{1}\cos r\] \[=\lambda\left(-C_{3}\sin r+c_{1}\cos r\right)\geq\lambda\frac{c_{1 }}{2}\]

for a certain choice of \(r\) small enough, dependent only on \(c_{1}\) and \(C_{3}\). 

#### f.2.2 Lyapunov function analysis for bounding the LSI constant of \(\eta_{\lambda,\beta}\)

To prove Thm. 5.2, it only remains to show that the conditions of Thm. F.6 are satisfied for \(J_{\lambda}^{\prime}[\eta_{\lambda,\beta}]\) instead of \(J_{\lambda}^{\prime}[\delta_{v}]\).

**Lemma F.8**.: _Under the setting of Assumptions 2 and 3, \(\eta_{\lambda,\beta}\) is rotationally invariant except for the direction \(v\), or formally \(Rv=v\implies R_{\sharp}\eta_{\lambda,\beta}=\eta_{\lambda,\beta}\) for orthonormal matrices \(R\), where \(R_{\sharp}\eta\) denotes the pushforward measure. Moreover, there exists \(g_{\eta}:[-1,1]\to\mathbb{R}\) such that for all \(w\in\mathbb{S}^{d}\), \(J_{\lambda}^{\prime}[\eta_{\lambda,\beta}](w)=-\lambda g_{\eta}(\langle w,v\rangle)\)._

Proof.: The lemma follows directly from the fact that \(\rho\) is rotationally invariant and that \(y=\phi(v)\)

**Lemma F.9**.: _Under Assumption 2, suppose furthermore that \(\sup_{w}\left\|\nabla^{i}\phi(w)\right\|_{\mathcal{H}}\leq B_{i}<\infty\) for \(i\in\{0,1,2\}\). Then we have, for any \(\eta,\eta^{\prime}\in\mathcal{P}(\mathcal{W})\),_

\[\forall w\in\mathbb{S}^{d},\ \ \left|\frac{1}{2}\Delta J_{ \lambda}^{\prime}[\eta]-\frac{\beta}{4}\left\|\nabla J_{\lambda}^{\prime}[\eta ]\right\|^{2}-\frac{1}{2}\Delta J_{\lambda}^{\prime}[\eta^{\prime}]+\frac{ \beta}{4}\left\|\nabla J_{\lambda}^{\prime}[\eta^{\prime}]\right\|^{2}\right|\] \[\leq\left(d\frac{2B_{0}B_{1}(B_{0}B_{2}+B_{1}^{2})}{\lambda^{2}} \left\|y\right\|_{\mathcal{H}}^{2}+\beta\frac{2B_{0}^{3}B_{1}^{3}}{\lambda^{3 }}\left\|y\right\|_{\mathcal{H}}^{4}\right)W_{1}(\eta,\eta^{\prime})\] \[\text{and}\quad\left|\lambda_{\min}(\nabla^{2}J_{\lambda}^{\prime }[\eta])-\lambda_{\min}(\nabla^{2}J_{\lambda}^{\prime}[\eta^{\prime}])\right| \leq\frac{4B_{0}B_{1}(B_{0}B_{2}+B_{1}^{2})}{\lambda^{2}}\left\|y\right\|_{ \mathcal{H}}^{2}W_{1}(\eta,\eta^{\prime}).\]

Proof.: By Prop. F.2,

\[\left\|\nabla^{2}J_{\lambda}^{\prime}[\eta](w)-\nabla^{2}J_{\lambda}^{\prime }[\eta^{\prime}](w)\right\|_{\text{op}}\leq\frac{4B_{0}B_{1}(B_{0}B_{2}+B_{1}^ {2})}{\lambda^{2}}\left\|y\right\|_{\mathcal{H}}^{2}W_{1}(\eta,\eta^{\prime})\]

and \(\left|\lambda_{\min}(\nabla^{2}J_{\lambda}^{\prime}[\eta](w))-\lambda_{\min}( \nabla^{2}J_{\lambda}^{\prime}[\eta^{\prime}](w))\right|\leq\left\|\nabla^{2} J_{\lambda}^{\prime}[\eta](w)-\nabla^{2}J_{\lambda}^{\prime}[\eta^{\prime}](w) \right\|_{\text{op}}\) by Weyl's inequality. This shows the second inequality of the lemma.

For the first inequality, we have \(\Delta J_{\lambda}^{\prime}[\eta](w)=\operatorname{Tr}\nabla^{2}\left.J_{ \lambda}^{\prime}[\eta](W)\right.\) and so

\[\left|\frac{1}{2}\Delta J_{\lambda}^{\prime}[\eta]-\frac{1}{2}\Delta J_{ \lambda}^{\prime}[\eta^{\prime}]\right|\leq\frac{d}{2}\left\|\nabla^{2}J_{ \lambda}^{\prime}[\eta](w)-\nabla^{2}J_{\lambda}^{\prime}[\eta^{\prime}](w) \right\|_{\text{op}}\leq\frac{d}{2}\frac{4B_{0}B_{1}(B_{0}B_{2}+B_{1}^{2})}{ \lambda^{2}}\left\|y\right\|_{\mathcal{H}}^{2}W_{1}(\eta,\eta^{\prime}).\]

Moreover, we showed in (F.2) resp. in Prop. F.2 that

\[\left\|\nabla J_{\lambda}^{\prime}[\eta]\right\|\leq\frac{B_{0}B_{1}}{\lambda }\left\|y\right\|_{\mathcal{H}}^{2}\quad\text{and}\quad\left\|\nabla J_{ \lambda}^{\prime}[\eta]-\nabla J_{\lambda}^{\prime}[\eta^{\prime}]\right\|\leq \frac{4B_{0}^{2}B_{1}^{2}}{\lambda^{2}}\left\|y\right\|_{\mathcal{H}}^{2}W_{1} (\eta,\eta^{\prime}),\]

so

\[\left|\frac{\beta}{4}\left\|\nabla J_{\lambda}^{\prime}\eta\right\| \right\|^{2}-\frac{\beta}{4}\left\|\nabla J_{\lambda}^{\prime}[\eta^{\prime}] \right\|^{2}\right| \leq\frac{\beta}{4}\cdot 2\frac{B_{0}B_{1}\left\|y\right\|_{\mathcal{H}}^{2}}{ \lambda}\cdot\frac{4B_{0}^{2}B_{1}^{2}\left\|y\right\|_{\mathcal{H}}^{2}}{ \lambda^{2}}W_{1}(\eta,\eta^{\prime})\] \[=\beta\frac{2B_{0}^{3}B_{1}^{3}\left\|y\right\|_{\mathcal{M}}^{4} }{\lambda^{3}}W_{1}(\eta,\eta^{\prime}),\]

which implies the first inequality of the lemma by triangle inequality. 

We can now proceed to the proof of Thm. 5.2, thanks to a bound on \(W_{2}(\eta_{\lambda,\beta},\delta_{v})\) under Assumption 3 proved in the next section.

Proof of Thm. 5.2.: For concision, in this proof, we will use the notations \(O(\cdot),\Omega(\cdot),\Theta(\cdot),\lesssim\) to hide constants dependent only on \(\left\|\varphi\right\|_{L^{2}(\rho)},\left\|\varphi^{\prime}\right\|_{L^{4}( \rho)},\left\|\varphi^{\prime\prime}\right\|_{L^{4}(\rho)},\mathbb{E}_{x\sim \rho}\left\|x\right\|^{4}/d^{2},c_{1},C_{1},C_{2},C_{3}\) and \(C_{4}\).

We established in Prop. F.7 that \(f_{0}\coloneqq J_{\lambda}^{\prime}[\delta_{v}]\) satisfies the conditions (L\({}_{\mathbb{S}^{d}}\)) (L\({}_{U}\)) (C\({}_{\mathbb{S}^{d}}\)) (C\({}_{U}\)) of Thm. F.6 with some constants \(D_{i},r=O(1)\) (in fact only dependent on \(c_{1},C_{1},C_{2},C_{3}\)) provided that \(\beta\geq D_{0}d\lambda^{-1}\). Thus, the first part of the theorem concerning the LSI of \(\widehat{\delta_{v}}\propto e^{-\beta J_{\lambda}^{\prime}[\delta_{v}]}\tau\), follows from Thm. F.6. To prove the second part of the theorem, it suffices to show that \(f^{*}\coloneqq J_{\lambda}^{\prime}[\eta_{\lambda,\beta}]\) satisfies the conditions (L\({}_{\mathbb{S}^{d}}\)) (L\({}_{U}\)) (C\({}_{\mathbb{S}^{d}}\)) (C\({}_{U}\)) of Thm. F.6 with some constants \(\widetilde{D}_{0}^{\prime},\widetilde{D}_{1}^{\prime},\widetilde{D}_{2}, \widetilde{D}_{3},\widetilde{D}_{4},r=\Theta(1)\).

By Lem. F.3, there exist constants \(B_{i}=O(1)\) such that \(\sup_{w}\left\|\nabla^{i}\phi(w)\right\|_{\mathcal{H}}\leq B_{i}\), for \(i\in\{0,1,2\}\). Moreover, by Lem. F.12 below, provided that \(\beta\geq\Omega(d\lambda)\), one has

\[W_{2}(\eta_{\lambda,\beta,\delta_{v}})\lesssim\sqrt{\beta^{-1}d\lambda^{-1}\cdot \log(\beta d^{-1}\lambda^{-1})}\ \ \coloneqq:\overline{W}.\]

Now by the conditions (L\({}_{\mathbb{S}^{d}}\)) (L\({}_{U}\)) (C\({}_{\mathbb{S}^{d}}\)) (C\({}_{U}\)) for \(f=f_{0}\) and \(D_{i}=\Theta(1)\) (by Prop. F.7), from Lem. F.9 along with the triangle inequality we have

\[\forall w\in\mathbb{S}^{d},\ \ \frac{1}{2}\Delta f^{*}-\frac{\beta}{4} \left\|\nabla f^{*}\right\|^{2} \lesssim\lambda d+(d\lambda^{-2}+\beta\lambda^{-3})\overline{W}\] \[\forall w\in\mathbb{S}^{d}\setminus U,\ \ \frac{1}{2}\Delta f^{*}-\frac{\beta}{4} \left\|\nabla f^{*}\right\|^{2} \leq-D_{2}\beta\lambda^{2}+E_{2}\cdot(d\lambda^{-2}+\beta\lambda^{-3}) \overline{W}\] \[\forall w\in\mathbb{S}^{d},\ \ \lambda_{\min}(\nabla^{2}f^{*}(w)) \gtrsim-\lambda-\lambda^{-2}\overline{W}\] \[\forall w\in U,\ \ \lambda_{\min}(\nabla^{2}f^{*}(w)) \geq D_{4}\lambda-E_{4}\cdot\lambda^{-2}\overline{W}\]

for some constants \(E_{2},E_{4}=O(1)\). So,* (\(\mathbb{L}^{\prime}_{\mathbb{S}^{d}}\)) for \(f^{*}\) can be ensured with \(\widetilde{D}^{\prime}_{1}=O(1)\) provided that \((d\lambda^{-2}+\beta\lambda^{-3})\overline{W}=(\beta^{-1}d\lambda+1)\beta \lambda^{-3}\overline{W}=O(\lambda d\beta^{3/4})\). Since we already assume that \(\beta\geq\Omega(d\lambda)\), this is equivalent to \(\beta\lambda^{-3}\overline{W}=O(\lambda d\beta^{3/4})\), i.e., \(\beta^{1/4}\lambda^{-4}d^{-1}\overline{W}=O(1)\).
* (\(\mathbb{L}_{U}\)) can be ensured with \(\widetilde{D}_{2}=\frac{D_{2}}{2}\) if \(\beta\) is such that \(E_{2}(d\lambda^{-2}+\beta\lambda^{-3})\overline{W}\leq\frac{D_{2}}{2}\beta \lambda^{2}\), i.e., \((\beta^{-1}d\lambda+1)\lambda^{-5}\overline{W}\leq\frac{D_{2}}{2E_{2}}\). Since we already assume that \(\beta\geq\Omega(d\lambda)\), this is equivalent to \(\lambda^{-5}\overline{W}\leq F_{2}\) for a certain \(F_{2}=\Theta(1)\).
* (\(\mathbb{C}_{\mathbb{S}^{d}}\)) can be ensured with \(\widetilde{D}_{3}=O(1)\) provided that \(\lambda^{-2}\overline{W}=O(\lambda)\), i.e., \(\lambda^{-3}\overline{W}=O(1)\).
* (\(\mathbb{C}_{U}\)) can be ensured with \(\widetilde{D}_{4}=\frac{D_{4}}{4}\) if \(E_{4}\lambda^{-2}\overline{W}\leq\frac{D_{4}}{2}\lambda\), i.e., \(\lambda^{-3}\overline{W}\leq\frac{D_{4}}{2E_{4}}\eqqcolon F_{4}=\Theta(1)\).

In summary, since we assume \(\lambda\leq 1\), we have \(\lambda^{-3}\leq\lambda^{-5}\) and \(\lambda^{-4}d^{-1}\leq\lambda^{-5}\). Hence we will choose \(\beta\) such that \(\beta^{1/4}d^{-1}\lambda^{-4}\overline{W}=O(1)\) and \(\lambda^{-5}\overline{W}\leq F_{2}\) for a certain \(F_{2}=\Theta(1)\), and this will ensure all four conditions with constants \(\widetilde{D}^{\prime}_{1},\widetilde{D}_{2},\widetilde{D}_{3},\widetilde{D}_{ 4}=\Theta(1)\). For choices of \(\beta\) such that \(\beta\geq d^{4}\lambda^{-4}\), it suffices to have \(\beta^{1/4}d^{-1}\lambda^{-4}\overline{W}\leq F_{2}\). Now substituting the definition of \(\overline{W}\), this sufficient condition rewrites

\[\beta^{1/4}d^{-1}\lambda^{-4}\overline{W}\leq F_{2}\iff\beta^{1/2}d^{-2} \lambda^{-8}\cdot\beta^{-1}d\lambda^{-1}\log\left(\frac{\beta}{d\lambda} \right)=\beta^{-1/2}\lambda^{-9}d^{-1}\log\left(\frac{\beta}{d\lambda}\right) \leq F_{2}^{2}.\]

Since \(\forall\varepsilon,x>0,\varepsilon\log x=\log x^{\varepsilon}\leq x^{\varepsilon}\), then for any \(\varepsilon>0\) it suffices to choose \(\beta\) such that

\[\beta^{-1/2}\lambda^{-9}d^{-1}\left(\frac{\beta}{d\lambda}\right)^{\varepsilon} \leq\varepsilon F_{2}^{2}\iff\beta^{1/2-\varepsilon}\geq\varepsilon^{-1}F_{2 }^{-2}\lambda^{-9-\varepsilon}d^{-1-\varepsilon}.\]

Choosing e.g. \(\varepsilon=\frac{1}{4}\), we get that a sufficient condition is \(\beta\geq\Omega(\operatorname{poly}(\lambda^{-1},d))\).

Hence we may apply the second part of Thm. F.6 to \(f^{*}=J^{\prime}_{\lambda}[\eta_{\lambda,\beta}]\) with constants \(\widetilde{D}^{\prime}_{1},\widetilde{D}_{2},\widetilde{D}_{3},\widetilde{D}_ {4}=O(1)\), provided that \(\beta\geq\Omega(\operatorname{poly}(\lambda^{-1},d))\). This concludes the proof of the second part of the theorem. 

#### f.2.3 Bound on \(W_{1}(\eta_{\lambda,\beta},\delta_{v})\)

The following lemma shows a form of weak coercivity of \(J_{\lambda}\).

**Lemma F.10**.: _Under Assumptions 2 and 3, if furthermore there exist \(c_{1},C_{1},C_{3},C_{4}>0\) such that_

\[\forall r\in[-1,+1],\ \ c_{1}\leq g^{\prime}(r)\leq C_{1},\quad\left|g^{\prime \prime}(r)(1-r^{2})^{1/2}\right|\leq C_{3},\quad\left|g^{\prime\prime\prime}(r)( 1-r^{2})^{3/2}\right|\leq C_{4},\]

_then there exists a constant \(\alpha_{g}\) dependent only on \(c_{1},C_{1},C_{3},C_{4}\) such that_

\[\forall\eta,\ J_{\lambda}(\eta)-J_{\lambda}(\delta_{v})\geq\lambda\alpha_{g}W_{ 2}^{2}(\eta,\delta_{v}).\]

Proof.: Since \(J_{\lambda}\) is convex,

\[J_{\lambda}(\eta)-J_{\lambda}(\delta_{v})\geq\int_{\mathbb{S}^{d} }J^{\prime}_{\lambda}[\delta_{v}]d(\eta-\delta_{v}) =-\lambda\int_{\mathbb{S}^{d}}g(\langle v,w\rangle)\mathrm{d}( \eta-\delta_{v})(w)\] \[=\lambda\int_{\mathbb{S}^{d}}\left[g(1)-g(\langle v,w\rangle) \right]d\eta(w).\]

Now let \(U_{r}=\left\{w\in\mathbb{S}^{d};\operatorname{dist}_{\mathbb{S}^{d}}(w,v)\leq r\right\}\) for some \(r>0\) to be chosen. We will compute the integral separately on \(U_{r}\) and on \(\mathbb{S}^{d}\setminus U_{r}\).

For the part \(\int_{U_{r}}\), we proceed by a second-order Taylor expansion. Namely, for any \(w\in U_{r}\setminus\{v\}\), let \(e\perp v\) such that \(w=\cos(\theta)v+\sin(\theta)e\) for some \(0<\theta\leq r\), since \(\operatorname{dist}_{\mathbb{S}^{d}}(w,v)=\arccos(\langle w,v\rangle)=\theta\). Then \(g(\langle v,w\rangle)=g(\cos\theta)\), and

\[\frac{d}{d\theta}g(\cos\theta) =-\sin(\theta)g^{\prime}(\cos\theta)\] \[\frac{d^{2}}{d\theta^{2}}g(\cos\theta) =\sin(\theta)^{2}g^{\prime\prime}(\cos\theta)-\cos(\theta)g^{ \prime}(\cos\theta)\] \[\frac{d^{3}}{d\theta^{3}}g(\cos\theta) =-\sin(\theta)^{3}g^{\prime\prime\prime}(\cos\theta)+3\sin(\theta) \cos(\theta)g^{\prime\prime}(\cos\theta)+\sin(\theta)g^{\prime}(\cos\theta).\]Notice that by our assumptions on \(g\), it is smooth enough at \(1\) so that \(\sin(\theta)g^{\prime}(\cos\theta)\to 0\) and \(\sin(\theta)^{2}g^{\prime\prime}(\cos\theta)\to 0\) as \(\theta\to 0\). Further,

\[\sup_{\theta}\frac{\mathrm{d}^{3}}{\mathrm{d}\theta^{3}}g(\cos\theta)\leq C_{4} +3C_{3}+C_{1}=:6M_{3,g}.\]

Consequently, by a univariate Taylor expansion with remainder in Langrange form around \(\theta=0\), for all \(0<\theta\leq r\), provided that we choose \(r\leq\frac{g^{\prime}(1)}{2M_{3,g}}\), we have

\[g(\cos\theta) =g(1)+0+\frac{1}{2}(0-g^{\prime}(1))\theta^{2}+\frac{1}{6}(g\circ \cos)^{(3)}(u)\theta^{3}\ \text{ for some }u\in[0,r]\] \[\leq g(1)-\frac{1}{2}g^{\prime}(1)\theta^{2}+\frac{1}{6}\left[ \sup_{[0,r]}\,(g\circ\cos)^{(3)}\right]\theta^{3}\] \[\leq g(1)-\frac{1}{2}g^{\prime}(1)\theta^{2}+M_{3,g}\theta^{3}=g (1)-\left(\frac{1}{2}g^{\prime}(1)-M_{3,g}\theta\right)\theta^{2}\] \[\leq g(1)-\frac{1}{4}g^{\prime}(1)\theta^{2}.\] (F.7)

In other words,

\[\forall w\in U_{r},\ \ g(1)-g(\langle v,w\rangle) \geq\frac{1}{4}g^{\prime}(1)\operatorname{dist}_{\mathbb{S}^{d}} (w,v)^{2},\] \[\text{and so,} \int_{U_{r}}\left[g(1)-g(\langle v,w\rangle)\right]\mathrm{d}\eta (w) \geq\frac{1}{4}g^{\prime}(1)\int_{U_{r}}\operatorname{dist}_{\mathbb{S}^{d}} (w,v)^{2}\,\mathrm{d}\eta(w).\]

For the part \(\int_{\mathbb{S}^{d}\setminus U_{r}}\), since \(g\) is increasing on \([-1,1]\) since \(g^{\prime}\geq c_{1}>0\), we have

\[\int_{\mathbb{S}^{d}\setminus U_{r}}\left[g(1)-g(\langle v,w \rangle)\right]\mathrm{d}\eta(w) \geq\left[g(1)-g(\cos(r))\right]\left[1-\eta(U_{r})\right]\] \[\geq\left[\frac{1}{4}g^{\prime}(1)r^{2}\right]\left[1-\eta(U_{r})\right]\]

where the second inequality follows from the Taylor expansion (F.7) above applied to \(\theta=r\).

Thus we showed

\[J_{\lambda}(\eta)-J_{\lambda}(\delta_{v}) \geq\lambda\left\{\left[\frac{1}{4}g^{\prime}(1)r^{2}\right]\left[ 1-\eta(U_{r})\right]+\frac{g^{\prime}(1)}{4}\int_{U_{r}}\operatorname{dist}_{ \mathbb{S}^{d}}(w,v)^{2}\mathrm{d}\eta(w)\right\}\] \[=\frac{\lambda g^{\prime}(1)}{4}\left\{r^{2}\left[1-\eta(U_{r}) \right]+\int_{U_{r}}\operatorname{dist}_{\mathbb{S}^{d}}(w,v)^{2}\mathrm{d} \eta(w)\right\}.\]

On the other hand, since \(\operatorname{dist}_{\mathbb{S}^{d}}(v,w)=\arccos(\langle v,w\rangle)\),

\[W_{2}^{2}(\eta,\delta_{v}) =\int_{\mathbb{S}^{d}\setminus U_{r}}\operatorname{dist}_{ \mathbb{S}^{d}}(v,w)^{2}\mathrm{d}\eta(w)+\int_{U_{r}}\operatorname{dist}_{ \mathbb{S}^{d}}(v,w)^{2}\mathrm{d}\eta(w)\] \[\leq\pi^{2}\left[1-\eta(U_{r})\right]+\int_{U_{r}}\operatorname {dist}_{\mathbb{S}^{d}}(v,w)^{2}\mathrm{d}\eta(w).\]

Hence

\[J_{\lambda}(\eta)-J_{\lambda}(\delta_{v}) \geq\frac{\lambda g^{\prime}(1)}{4}\cdot\sup_{0\leq r\leq\frac{g ^{\prime}(1)}{2M_{3,g}}}\min\left[\frac{r^{2}}{\pi^{2}},1\right]W_{2}^{2}(\eta,\delta_{v})\] \[=\lambda\cdot\frac{g^{\prime}(1)}{4}\min\left[\left(\frac{g^{ \prime}(1)}{2M_{3,g}}\right)^{2}/\pi^{2},1\right]\cdot W_{2}^{2}(\eta,\delta_ {v})\] \[\geq\lambda\cdot\frac{c_{1}}{4}\min\left[\left(\frac{c_{1}}{2M_{3,g}}\right)^{2}/\pi^{2},1\right]\cdot W_{2}^{2}(\eta,\delta_{v})\ \ =:\lambda\alpha_{g}W_{2}^{2}(\eta,\delta_{v}).\]

Notice that \(\alpha_{g}\) only depends on \(c_{1},C_{1},C_{3},C_{4}\)We will use the following fact about the surface area of a small hyperspherical cap around a pole for bounding \(W_{1}(\eta_{\lambda,\beta},\delta_{v})\). It essentially shows that, for \(\mathcal{W}=\mathbb{S}^{d}\), the constant called \(C\) in the statement of Lem. E.1 scales with dimension as \(2^{-d}\lesssim C^{-1}\lesssim 1/\sqrt{d}\).

**Lemma F.11**.: _Fix \(d\geq 2\) and \(v\in\mathbb{S}^{d}\) and denote by \(\tau\) the uniform measure on \(\mathbb{S}^{d}\). For any \(\epsilon>0\), let \(S_{\epsilon}=\left\{w\in\mathbb{S}^{d}:\mathrm{dist}_{\mathbb{S}^{d}}(w,v)\leq \epsilon\right\}\). There exist universal constants \(C_{-},C_{+}>0\) such that_

\[\forall 0<\epsilon\leq\frac{\pi}{4},\quad C_{-}^{-1}\left(\epsilon/2\right)^{d} \leq\tau(S_{\epsilon})\leq C_{+}\ \epsilon^{d}/\sqrt{d}.\]

Proof.: For \(w\sim\tau\), the distribution of \(\langle w,v\rangle\) admits a probability density function \(h(z)=(1-z^{2})^{d/2-1}/Z\), where

\[Z=\int_{-1}^{1}(1-z^{2})^{d/2-1}\mathrm{d}z=B\left(\frac{d}{2},\frac{1}{2} \right)=\frac{\Gamma\left(\frac{d}{2}\right)\sqrt{\pi}}{\Gamma\left(\frac{d+1 }{2}\right)}.\]

Note that by Gautschi's inequality \(\forall s\in(0,1),\forall x>0,\ \ x^{1-s}<\frac{\Gamma(x+1)}{\Gamma(x+s)}<(x+1)^{1-s}\) applied to \(s=\frac{1}{2}\) and \(x=\frac{d-1}{2}\), we have \(\sqrt{\frac{d-1}{2}}<\frac{\Gamma\left(\frac{d+1}{2}\right)}{\Gamma\left(\frac {d}{2}\right)}<\sqrt{\frac{d+1}{2}}\), so

\[\sqrt{\frac{2\pi}{d+1}}\leq Z\leq\sqrt{\frac{2\pi}{d-1}}.\]

By definition, since \(\mathrm{dist}_{\mathbb{S}^{d}}(w,v)=\arccos(\langle w,v\rangle)\), \(\tau(S_{\epsilon})=\int_{\cos(\epsilon)}^{1}h(z)\mathrm{d}z\). One can verify

\[\forall\,0<\epsilon\leq\frac{\pi}{4},\ \sqrt{1-\epsilon^{2}}\leq\cos( \epsilon)\leq\sqrt{1-\frac{\epsilon^{2}}{4}}.\]

So for all \(0<\epsilon\leq\frac{\pi}{4}\),

\[\tau(S_{\epsilon})=\int_{\cos(\epsilon)}^{1}h(z)\mathrm{d}z \leq\int_{\sqrt{1-\epsilon^{2}}}^{1}h(z)\mathrm{d}z\] \[=Z^{-1}\int_{\sqrt{1-\epsilon^{2}}}^{1}(1-z^{2})^{d/2-1}\mathrm{d }z=Z^{-1}\int_{1-\epsilon^{2}}^{1}(1-t)^{d/2-1}\frac{\mathrm{d}t}{2\sqrt{t}}\] \[\leq Z^{-1}\frac{1}{2\sqrt{1-\epsilon^{2}}}\int_{1-\epsilon^{2}}^{ 1}(1-t)^{d/2-1}\mathrm{d}t=Z^{-1}\frac{1}{2\sqrt{1-\epsilon^{2}}}\int_{0}^{ \epsilon^{2}}t^{d/2-1}\mathrm{d}t\] \[=Z^{-1}\frac{1}{2\sqrt{1-\epsilon^{2}}}\cdot\frac{2}{d}[\epsilon^ {2}]^{d/2}\leq Z^{-1}\frac{1}{d\sqrt{1-(\pi/4)^{2}}}\epsilon^{d}\ \ \leq C_{+} \epsilon^{d}/\sqrt{d}\]

for some universal constant \(C_{+}\). In the other direction,

\[\tau(S_{\epsilon}) \geq\int_{\sqrt{1-\epsilon^{2}/4}}^{1}h(z)\mathrm{d}z=Z^{-1} \int_{\sqrt{1-\epsilon^{2}/4}}^{1}(1-z^{2})^{d/2-1}\mathrm{d}z=Z^{-1}\int_{1- \epsilon^{2}/4}^{1}(1-t)^{d/2-1}\frac{\mathrm{d}t}{2\sqrt{t}}\] \[\geq Z^{-1}\frac{1}{2}\int_{1-\epsilon^{2}/4}^{1}(1-t)^{d/2-1} \mathrm{d}t=Z^{-1}\frac{1}{2}\int_{0}^{\epsilon^{2}/4}t^{d/2-1}\mathrm{d}t\] \[=Z^{-1}\frac{1}{2}\frac{2}{d}[\epsilon^{2}/4]^{d/2}=Z^{-1}\frac{1} {d}(\epsilon/2)^{d}\ \ \geq c(\epsilon/2)^{d}/\sqrt{d}.\]

for some universal constants \(c\). By repeating the same argument with \(\sqrt{1-\frac{\epsilon^{2}}{4}}\) replaced by \(\sqrt{1-\frac{\epsilon^{2}}{3.9}}\), we get \(\tau(S_{\epsilon})\geq c^{\prime}(\epsilon/1.99)^{d}/\sqrt{d}\geq C_{-}^{-1}( \epsilon/2)^{d}\) for some universal constants \(c^{\prime},C_{-}\). 

The following lemma combines the weak coercivity and weak Lipschitz-continuity of \(J_{\lambda}\) by a \(\Gamma\)-convergence type argument, to show an explicit bound on \(W_{1}(\eta_{\lambda,\beta},\delta_{v})\). It quantifies the intuitive fact that \(\eta_{\lambda,\beta}\) converges weakly to \(\delta_{v}\) when \(\beta^{-1}\to 0\) or \(\lambda\to+\infty\).

**Lemma F.12**.: _Under Assumptions 2 and 3, if \(\sup_{w}\left\|\nabla^{i}\phi(w)\right\|_{\mathcal{H}}\leq B_{i}<\infty\) for \(i\in\{0,1\}\), and if \(\beta\geq\frac{4d\lambda}{\pi}\big{(}B_{0}B_{1}\left\|y\right\|_{\mathcal{H}}^{2 }\big{)}^{-1}\), then_

\[W_{2}(\eta_{\lambda,\beta},\delta_{v})\leq\sqrt{\frac{1}{\alpha_{g}}\frac{ \beta^{-1}d}{\lambda}\left(\widetilde{C}+\log\left(B_{0}B_{1}\left\|y\right\|_ {\mathcal{H}}^{2}\right)-\log\left(\beta^{-1}d\lambda\right)\right)}\]

_where \(\widetilde{C}\) is a universal constant and \(\alpha_{g}\) is the constant from Lem. F.10._

Proof.: Since \(\eta_{\lambda,\beta}=\arg\min J_{\lambda,\beta}\) and \(J_{\lambda,\beta}=J+\beta^{-1}H\left(\cdot|\tau\right)\), then for any \(\eta^{\sigma}\in\mathcal{P}(\mathcal{W})\),

\[J_{\lambda}(\eta_{\lambda,\beta})\leq J_{\lambda}(\eta_{\lambda,\beta})+ \beta^{-1}H\left(\eta_{\lambda,\beta}|\tau\right)=J_{\lambda,\beta}(\eta_{ \lambda,\beta})\leq J_{\lambda,\beta}(\eta^{\sigma})=J_{\lambda}(\eta^{ \sigma})+\beta^{-1}H\left(\eta^{\sigma}|\tau\right).\]

Further, we showed in Lem. F.10 that \(\forall\eta,\ J_{\lambda}(\eta)-J_{\lambda}(\delta_{v})\geq\lambda\alpha_{g} \cdot W_{2}^{2}(\eta,\delta_{v})\), so

\[\lambda\alpha_{g}\cdot W_{2}^{2}(\eta_{\lambda,\beta},\delta_{v})\leq J_{ \lambda}(\eta_{\lambda,\beta})-J_{\lambda}(\delta_{v})\leq J_{\lambda}(\eta^ {\sigma})-J_{\lambda}(\delta_{v})+\beta^{-1}H\left(\eta^{\sigma}|\tau\right).\]

It remains to upper-bound the right-hand side, which we do by choosing as \(\eta^{\sigma}\) a box-kernel smoothed version of \(\delta_{v}\) (this part the proof is essentially an instantantiation of Lem. E.1). Specifically, let \(\eta^{\sigma}\) be the uniform measure over the spherical cap \(S_{\sigma}=\left\{w\in\mathbb{S}^{d};\operatorname{dist}_{\mathbb{S}^{d}}(w,v )\leq\sigma\right\}\) for \(\sigma\) to be chosen. We showed in Prop. F.2 that

\[J_{\lambda}(\eta^{\sigma})-J_{\lambda}(\delta_{v})\leq\frac{B_{0}B_{1}\left\| y\right\|_{\mathcal{H}}^{2}}{\lambda}\cdot W_{1}(\eta^{\sigma},\delta_{v})\]

where \(\sup_{w}\left\|\nabla^{i}\phi(w)\right\|_{\mathcal{H}}\leq B_{i}\), and by definition

\[W_{1}(\eta^{\sigma},\delta_{v})=\int\operatorname{dist}_{\mathbb{S}^{d}}(w,v )\;\mathrm{d}\eta^{\sigma}(w)=\frac{1}{\operatorname{vol}(S_{\sigma})}\int_{ S_{\sigma}}\operatorname{dist}_{\mathbb{S}^{d}}(w,v)\;\mathrm{d}\operatorname{ vol}(w)\leq\sigma.\]

Moreover by Lem. F.11, provided that \(0<\sigma\leq\frac{\pi}{4}\),

\[H\left(\eta^{\sigma}|\tau\right)=\int\mathrm{d}\eta_{\sigma}\log\frac{ \mathrm{d}\eta_{\sigma}}{\mathrm{d}\tau}=\log\frac{\operatorname{vol}( \mathbb{S}^{d})}{\operatorname{vol}(S_{\sigma})}=-\log\tau(S_{\sigma})\leq \log C-d\log\frac{\sigma}{2}\]

for some universal constant \(C\), and let us assume w.l.o.g. that \(C>1\), so that \(\log C\leq d\log C\). Thus

\[J_{\lambda}(\eta^{\sigma})-J_{\lambda}(\delta_{v})+\beta^{-1}H\left(\eta^{ \sigma}|\tau\right)\leq\frac{B_{0}B_{1}\left\|y\right\|_{\mathcal{H}}^{2}}{ \lambda}\sigma-\beta^{-1}d\log\sigma+\beta^{-1}d\log 2C.\]

Therefore, taking the infimum over \(0<\sigma\leq\frac{\pi}{4}\),

\[\lambda\alpha_{g}\cdot W_{2}^{2}(\eta_{\lambda,\beta},\delta_{v}) \leq\inf_{0<\sigma\leq\frac{\pi}{4}}\frac{B_{0}B_{1}\left\|y \right\|_{\mathcal{H}}^{2}}{\lambda}\sigma-\beta^{-1}d\log\sigma+\beta^{-1}d \log 2C\] \[=\beta^{-1}d-\beta^{-1}d\log\frac{\beta^{-1}d\lambda}{B_{0}B_{1} \left\|y\right\|_{\mathcal{H}}^{2}}+\beta^{-1}d\log 2C\] \[=\beta^{-1}d\left(1+\log(2C)-\log(\beta^{-1}d\lambda)+\log\left(B_ {0}B_{1}\left\|y\right\|_{\mathcal{H}}^{2}\right)\right),\]

where on the second line we used that the unconstrained infimum of the right-hand side over \(\sigma>0\) is attained at \(\sigma=\frac{\beta^{-1}d\lambda}{B_{0}B_{1}\left\|y\right\|_{\mathcal{H}}^{2}}\), which is indeed less than \(\frac{\pi}{4}\) by assumption. This shows the bound

\[W_{2}(\eta_{\lambda,\beta},\delta_{v})\leq\sqrt{\frac{1}{\lambda\alpha_{g}} \beta^{-1}d\left(1+\log(2C)-\log(\beta^{-1}d\lambda)+\log\left(B_{0}B_{1} \left\|y\right\|_{\mathcal{H}}^{2}\right)\right)},\]

and the bound announced in the proposition follows by gathering some universal constants into \(\widetilde{C}\)

### Proof of Prop. 5.3 (examples of activations satisfying the assumptions)

Before presenting the proof, we recall a few concepts from the theory of spherical harmonics, and refer to [1, 10] for more details. Let \(\tau\) be the uniform probability measure on \(\mathbb{S}^{d}\). The spherical harmonics in dimension \(d+1\) form an orthonormal basis of \(L^{2}(\tau)\). We denote them by \(\{Y_{kj}\}_{k,j}\), where \(k\geq 0\) and \(1\leq j\leq N(d,k)\), where \(N(d,0)=1\) and \(N(d,k)=\frac{2k+d-1}{k}\binom{k+d-2}{d-1}\) for \(k\geq 1\) (for \(k=0\) we have \(Y_{01}=1\)). Consequently, any \(\phi\in L^{2}(\tau)\) can be written as

\[\phi=\sum_{k=0}^{\infty}\sum_{j=1}^{N(d,k)}\langle\phi,Y_{kj}\rangle_{L^{2}( \tau)}Y_{kj}.\]

Let \(P_{k,d}\) be the Legendre polynomial (a.k.a. Gegenbauer polynomial) of degree \(k\) in dimension \(d+1\), normalized such that \(P_{k,d}(1)=1\). Thanks to Rodrigues' formula [1, Theorem 2.23], we can express Legendre polynomials as,

\[P_{k,d}(t)=\frac{(-1)^{k}\Gamma(d/2)}{2^{k}\Gamma(k+d/2)}(1-t^{2})^{(2-d)/2} \left(\frac{\mathrm{d}}{\mathrm{d}t}\right)^{k}(1-t^{2})^{k+(d-2)/2}.\]

We now go over some useful properties of spherical harmonics and Legendre polynomials.

* **(Addition Formula)** We have the following formula which relates Legendre polynomials to spherical harmonics [1, Theorem 2.9], \[\sum_{j=1}^{N(d,k)}Y_{kj}(w)Y_{kj}(v)=N(d,k)P_{k,d}(\langle w,v\rangle),\quad \forall w,v\in\mathbb{S}^{d}.\]
* **(Hecke-Funk Formula)** Suppose \(\phi\in L^{2}(\tau)\) is given by \(\phi(\cdot)=\varphi(\langle w,\cdot\rangle)\) for some \(w\in\mathbb{S}^{d}\). Then [1, Theorem 2.22], \[\langle\phi,Y_{kj}\rangle_{L^{2}(\tau)}=\frac{\Gamma((d+1)/2)}{\Gamma(d/2) \sqrt{\pi}}Y_{kj}(w)\int_{-1}^{1}\varphi(t)P_{k}(t)(1-t^{2})^{(d-2)/2}\mathrm{ d}t.\]
* **(Orthogonality of Legendre Polynomials)** Using the addition formula and orthonormality of spherical harmonics, for every \(k,k^{\prime}\geq 0\) we have, \[\langle P_{k,d}(\langle w,\cdot\rangle),P_{k^{\prime},d}(\langle v,\cdot \rangle)_{L^{2}(\tau)}=\frac{\delta_{kk^{\prime}}P_{k,d}(\langle w,v\rangle)}{ N(d,k)}.\]
* **(Derivative of Legendre Polynomials)** For every \(k\geq j\), we have the following identity for derivatives of Legendre polynomials [1, Equation (2.89)], \[P_{k,d}^{(j)}(t)=c_{j,k,d}P_{k-j,d+2j}(t),\] where \(P_{k,d}^{(j)}\) denotes the \(j\)th derivative of \(P_{k,d}\), and \[c_{j,k,d}=\frac{k(k-1)\ldots(k-j+1)(k+d-1)(k+d)\ldots(k+d+j-2)}{d(d+2)\ldots(d +2j-2)}.\] (F.8) Notice that for \(j>k\) we have \(P_{k,d}^{(j)}=0\).

We use the tools introduced above to prove the following lemma.

**Lemma F.13**.: _Suppose \(\rho\) is a spherically symmetric probability measure on \(\mathbb{R}^{d+1}\). Define \(q:[-1,1]\to\mathbb{R}\) via \(q(\langle w,v\rangle)=\int\varphi(\langle w,x\rangle)\varphi(\langle v,x \rangle)\mathrm{d}\rho(x)\) for \(w,v\in\mathbb{S}^{d}.\) Then, for every \(j\geq 1\),_

\[q^{(j)}(\langle w,v\rangle)=\frac{1}{(d+1)(d+3)\ldots(d+2j-1)}\int\|x\|^{2j} \,\varphi^{(j)}(\langle w,x\rangle)\varphi^{(j)}(\langle v,x\rangle)\mathrm{d }\rho(x),\]

_where \(\varphi^{(j)}\) denotes the \(j\)th derivative of \(\varphi\)._Proof.: We being by introducing the notation \(\varphi_{r}(\langle w,x\rangle)=\varphi(r\langle w,x\rangle)\). Doing so allows us to only consider functions on \(\mathbb{S}^{d}\) by conditioning on the norm of input \(\|x\|\). Notice that

(F.9)

where

\[q_{r}(\langle w,v\rangle)\coloneqq\int\varphi(r\langle w,x\rangle)\varphi(r \langle v,x\rangle)\mathrm{d}\tau(x)=\langle\varphi_{r}(\langle w,\cdot\rangle),\varphi_{r}(\langle v,\cdot\rangle)\rangle_{L^{2}(\tau)}.\]

By the Hecke-Funk formula,

\[\langle\varphi_{r}(\langle w,\cdot\rangle),Y_{kj}(\cdot)\rangle_{L^{2}(\tau) }=\bar{\alpha}_{k,r}Y_{kj}(w)\coloneqq\frac{\alpha_{k,r}}{\sqrt{N(d,k)}}Y_{kj} (w),\]

where

\[\bar{\alpha}_{k,r}\coloneqq\frac{\Gamma((d+1)/2)}{\Gamma(d/2)\sqrt{\pi}}\int_ {-1}^{1}\varphi(rt)P_{k}(t)(1-t^{2})^{(d-2)/2}\mathrm{d}t.\]

Then, by the expansion of \(\varphi_{r}(\langle w,\cdot\rangle)\) in the basis of spherical harmonics,

\[\varphi_{r}(\langle w,\cdot\rangle)=\sum_{k=0}^{\infty}\sum_{j=1}^{N(d,k)} \frac{\alpha_{k,r}}{\sqrt{N(d,k)}}Y_{kj}(w)Y_{kj}(\cdot)=\sum_{k=0}^{\infty} \sqrt{N(d,k)}\alpha_{k,r}P_{k,d}(\langle w,\cdot\rangle).\] (F.10)

Via the formula for inner products of Legendre polynomials, we obtain

\[q_{r}(\langle w,v\rangle)=\sum_{k=0}^{\infty}\alpha_{k,r}^{2}N(d,k)\langle P_{ k,d}(\langle w,\cdot\rangle,P_{k,d}(\langle v,\cdot\rangle)_{L^{2}(\tau)}=\sum_{k=0} ^{\infty}\alpha_{k,r}^{2}P_{k,d}(\langle w,v\rangle).\]

As a result,

\[q_{r}^{(j)}(\langle w,v\rangle)=\sum_{k=0}^{\infty}\alpha_{k,r}^{2}P_{k,d}^{( j)}(\langle w,v\rangle)=\sum_{k=j}^{\infty}\alpha_{k,r}^{2}c_{j,k,d}P_{k-j,d+2j}( \langle w,v\rangle),\] (F.11)

where \(c_{j,k,d}\) is given by (F.8). On the other hand, we can directly obtain from (F.10),

\[\varphi_{r}^{(j)}(\langle w,x\rangle)=\sum_{k=0}^{\infty}\sqrt{N(d,k)}\alpha_{ k,r}P_{k,d}^{(j)}(\langle w,x\rangle)=\sum_{k=j}^{\infty}\sqrt{N(d,k)}\alpha_{k,r}c_{j,k,d}P_{k-j,d+2j}(\langle w,x\rangle).\]

Therefore,

\[\langle\varphi_{r}^{(j)}(\langle w,\cdot\rangle),\varphi_{r}^{(j)}(\langle v, \cdot\rangle)\rangle_{L^{2}(\tau)}=\sum_{k=j}^{\infty}\frac{\alpha_{k,r}^{2}c_ {j,k,d}^{2}N(d,k)}{N(d+2j,k-j)}P_{k-j,d+2j}(\langle w,v\rangle).\]

Moreover, it is straightforward to verify that

\[\frac{c_{j,k,d}N(d,k)}{N(d+2j,k-j)}=(d+1)(d+3)\ldots(d+2j-1)\]

for \(k\geq j\). Therefore,

\[\langle\varphi_{r}^{(j)}(\langle w,\cdot\rangle),\varphi_{r}^{( j)}(\langle v,\cdot\rangle)\rangle_{L^{2}(\tau)} =(d+1)(d+3)\ldots(d+2j-1)\sum_{k=j}^{\infty}\alpha_{k,r}^{2}c_{j,k,d}P_{k-j,d+2j}(\langle w,v\rangle)\] \[=(d+1)(d+3)\ldots(d+2j-1)q_{r}^{(j)}(\langle w,v\rangle),\]

where the last identity follows from (F.11). We can now use the fact that \(\varphi_{r}^{(j)}=r\varphi^{(j)}\), and plug the above back into (F.9) to obtain

\[q^{(j)}(\langle w,v\rangle)=\mathbb{E}_{\|x\|}\left[q_{\|x\|}^{( j)}(\langle w,v\rangle)\right] =\mathbb{E}_{\|x\|}\int\frac{\|x\|^{2j}}{(d+1)(d+3)\ldots(d+2j-1) }\varphi^{(j)}(\|x\|\,\langle w,\bar{x}\rangle)\varphi^{(j)}(\|x\|\,\langle v,\bar{x}\rangle)\mathrm{d}\tau(\bar{x})\] \[=\int\frac{\|x\|^{2j}}{(d+1)(d+3)\ldots(d+2j-1)}\varphi^{(j)}( \langle w,x\rangle)\varphi^{(j)}(\langle v,x\rangle)\mathrm{d}\rho(x),\]

which concludes the proof.

We are now ready to state the proof of Prop. 5.3.

Proof of Prop. 5.3.: Recall \(g(\langle w,v\rangle)=\frac{\langle\phi(w),\phi(v)\rangle_{\mathcal{H}}^{2}}{2( \lambda+\|\phi(v)\|_{\mathcal{H}}^{2})^{2}}\). Let \(q(\langle w,v\rangle)=\langle\phi(w),\phi(v)\rangle_{\mathcal{H}}\). Consequently,

\[g^{\prime}=\frac{qq^{\prime}}{(\lambda+\|\phi(v)\|_{\mathcal{H}}^{2})^{2}}, \quad g^{\prime\prime}=\frac{qq^{\prime\prime}+q^{\prime\,2}}{(\lambda+\|\phi( v)\|_{\mathcal{H}}^{2})^{2}},\quad g^{\prime\prime\prime}=\frac{3q^{\prime}q^{ \prime\prime}+qq^{\prime\prime\prime}}{(\lambda+\|\phi(v)\|_{\mathcal{H}}^{2} )^{2}}.\]

We proceed to bound each term separately. By non-negativity of \(\phi\), for any \(r>0\), we have

\[q(\langle w,v\rangle) =\mathbb{E}\left[\varphi(\langle w,x\rangle)\varphi(\langle v,x \rangle)\right]\] \[\geq\mathbb{E}\left[\varphi(\langle w,x\rangle)\phi(\langle v,x \rangle)\mathbf{1}\big{(}|\langle w,x\rangle|\leq r,|\langle v,x\rangle|\leq r \big{)}\right]\] \[\geq(\inf_{|z|\leq r}\varphi(z))^{2}\left(1-\mathbb{P}[\langle w,x \rangle^{2}>r^{2}]-\mathbb{P}[\langle v,x\rangle^{2}>r^{2}]\right)\] \[\geq(\inf_{|z|\leq r}\varphi(z))^{2}\left(1-\frac{2\mathbb{E}[ \left\|x\right\|^{2}]}{(d+1)r^{2}}\right),\]

where the last inequality follows from Markov inequality along with the fact that \(\mathbb{E}[xx^{\top}]=\frac{\mathbb{E}[\left\|x\right\|^{2}]}{d+1}I_{d+1}\) for spherically symmetric distributions. Thus, by choosing \(r=m=\frac{2b_{2}\sqrt{b_{2}}}{b_{1}}\), we have \(q(z)\geq\frac{1}{2}(\inf_{|z|\leq m}\varphi(z))\). Furthermore, by the Cauchy-Schwartz inequality, \(q(\langle w,v\rangle)\leq\mathbb{E}[\varphi(\langle w,x\rangle)^{2}]=\|\varphi \|_{L^{2}(\rho)}^{2}\). Next, we move on to bounding \(q^{\prime}\). Let \(\bar{x}\sim\tau\) be a uniform random vector on \(\mathbb{S}^{d}\). Then, for any \(r>0\), by Lem. F.13,

\[q^{\prime}(\langle w,v\rangle) =\frac{1}{d+1}\mathbb{E}\left[\left\|x\right\|^{2}\varphi^{ \prime}(\langle w,x\rangle)\varphi^{\prime}(\langle v,x\rangle)\right]\] \[=\frac{1}{d+1}\mathbb{E}\left[\left\|x\right\|^{2}\mathbb{E} \left[\varphi^{\prime}(\langle w,x\rangle)\varphi^{\prime}(\langle v,x\rangle) \,\left\|\,\left\|x\right\|\right]\right]\] \[\geq\frac{(\inf_{|z|\leq r}\varphi^{\prime}(z))^{2}}{d+1}\mathbb{ E}\left[\left\|x\right\|^{2}\mathbb{P}\left[\left\{|\langle w,\bar{x} \rangle|\leq\frac{r}{\left\|x\right\|}\right\}\cap\left\{|\langle v,\bar{x} \rangle|\leq\frac{r}{\left\|x\right\|}\right\}\,|\,\left\|x\right\|\right]\right]\] \[\geq\frac{(\inf_{|z|\leq r}\varphi^{\prime}(z))^{2}}{d+1}\mathbb{ E}\left[\left\|x\right\|^{2}\left(1-\mathbb{P}\left[\langle w,\bar{x} \rangle^{2}>\frac{r^{2}}{\left\|x\right\|^{2}}\,|\,\left\|x\right\|\right]- \mathbb{P}\left[\langle v,\bar{x}\rangle^{2}>\frac{r^{2}}{\left\|x\right\|^{2 }}\,|\,\left\|x\right\|\right]\right)\right]\] \[\geq\frac{(\inf_{|z|\leq r}\varphi^{\prime}(z))^{2}}{d+1}\mathbb{ E}\left[\left\|x\right\|^{2}\left(1-\frac{2\left\|x\right\|^{2}}{r^{2}(d+1)}\right) \right].\]

Consequently, by choosing \(r=m=\frac{2b_{2}\sqrt{b_{2}}}{b_{1}}\), we obtain \(q^{\prime}\geq\frac{b_{1}}{2}(\inf_{|z|\leq m}\phi^{\prime}(z))^{2}\). Moreover, by the Cauchy-Schwartz inequality, \(q^{\prime}\leq b_{2}\left\|\varphi^{\prime}\right\|_{L^{4}(\rho)}^{2}\). As a result,

\[\frac{b_{1}(\inf_{|z|\leq m}\varphi(z))^{2}(\inf_{|z|\leq m}\varphi^{\prime}(z ))^{2}}{(\lambda+\|\varphi\|_{L^{2}(\rho)}^{2})^{2}}\leq g^{\prime}\leq\frac{b_ {2}\left\|\varphi\right\|_{L^{2}(\rho)}^{2}\left\|\varphi^{\prime}\right\|_{L^ {4}(\rho)}^{2}}{(\lambda+\|\varphi\|_{L^{2}(\rho)}^{2})^{2}}.\]

Furthermore, by Lem. F.13 and the Cauchy-Schwartz inequality,

\[|q^{\prime\prime}|\leq\frac{b_{2}^{2}(d+1)}{d+3}\left\|\varphi^{\prime\prime} \right\|_{L^{4}(\rho)}^{2},\quad|q^{\prime\prime\prime}|\leq\frac{b_{2}^{3}(d +1)^{2}}{(d+3)(d+5)}\left\|\phi^{\prime\prime\prime}\right\|_{L^{4}(\rho)}^{2}.\]

Hence,

\[\frac{-b_{2}^{2}\left\|\varphi^{\prime\prime}\right\|_{L^{4}(\rho)}^{2}\left\| \varphi\right\|_{L^{4}(\rho)}^{2}}{(\lambda+\|\varphi\|_{L^{2}(\rho)}^{2})^{2 }}\leq g^{\prime\prime}\leq\frac{b_{2}^{2}\left\|\varphi^{\prime\prime}\right\|_ {L^{4}(\rho)}^{2}\left\|\varphi\right\|_{L^{2}(\rho)}^{2}+b_{2}^{2}\left\| \varphi^{\prime}\right\|_{L^{4}(\rho)}^{4}}{(\lambda+\|\varphi\|_{L^{2}(\rho)}^ {2})^{2}},\]

and

\[|g^{\prime\prime\prime}|\leq\frac{3b_{2}^{3}\left\|\phi^{\prime}\right\|_{L^{4} (\rho)}^{2}\left\|\phi^{\prime\prime}\right\|_{L^{4}(\rho)}^{2}+b_{2}^{3}\left\| \phi\right\|_{L^{2}(\rho)}^{2}\left\|\phi^{\prime\prime\prime}\right\|_{L^{4}( \rho)}^{2}}{(\lambda+\|\varphi\|_{L^{2}(\rho)}^{2})^{2}},\]

which completes the proof.

### Implementation details for Fig. 1

We consider the problem (1.1) where \(\mathcal{W}=\mathbb{S}^{d}\) and \(G\) is defined as in Assumption 2, where \(d=10\), \(\lambda=10^{-3}\) and

* \(y:\mathbb{R}^{d+1}\to\mathbb{R}\) is given by a teacher 2NN with 5 neurons defined as follows. The first-layer weights are orthonormal, drawn from the Haar measure, and the second layer weights are drawn i.i.d. from \(\mathcal{N}(0,1.8I_{d})\). Its activation is \(\varphi_{\mathrm{teacher}}(z)=\frac{z^{4}-6z^{2}+3}{\sqrt{24}}\), which is the normalized 4th degree Hermite polynomial.
* \(\rho\) is the empirical distribution of a (covariate) dataset \((x_{i})_{i\leq n}\) of \(n=100\) training samples, sampled i.i.d. from \(\mathcal{N}\left(\begin{pmatrix}0_{d}\\ 1\end{pmatrix},\begin{pmatrix}I_{d}&0\\ 0&0\end{pmatrix}\right)\), with the last coordinate representing bias.
* The activation function \(\varphi\) of the student 2NN \(\hat{y}_{\nu}\) is the ReLU, \(\varphi(z)=\max(0,z)\).

We performed 5 different runs, each corresponding to a different teacher network (\(y\)) and training dataset (\(\rho\)), and tested all the algorithms considered at each run. So the objective functional \(G_{\lambda}\) is different for each run, which is why the values shown on the \(y\)-axis are offset by \(G_{\lambda}^{\star}\), the best value achieved by any of the algorithms considered for each run.

For the algorithms using the bilevel formulation, we computed the values and the Wasserstein gradients of \(J_{\lambda}\) explicitly by the formulas from Prop. F.1 and (F.2) (the matrix \(K_{\eta}+\lambda\operatorname{id}\) in \(L_{\rho}^{2}\simeq\mathbb{R}^{n}\) is inverted explicitly).

For the algorithms using MFLD, we used \(\beta^{-1}=10^{-3}\). We ran the Euler-Maruyama discretization of the noisy particle gradient flow SDE described in Sec. 2 (with an inexact simulation of the Brownian increments described below), using \(N=1000\) particles - corresponding to the width of the student 2NN -, and a step size of \(10^{-2}\) for (1a) and \(10^{-3}\) for (1b). For Wasserstein GF without noise, we used the same discretization but with \(\beta^{-1}=0\).

Concerning the initialization of the particles \((r^{i},w^{i})_{i\leq N}\) - corresponding to the second resp. first-layer weights of the student network -, the \(w_{0}^{i}\) are drawn i.i.d. uniformly on \(\mathbb{S}^{d}\), and for the algorithms using the lifting formulation, the \(r_{0}^{i}\) are drawn i.i.d. from \(\mathcal{N}(0,1)\).

Note that our simulations of Brownian motion are not exact. To implement MFLD on \(\mathbb{S}^{d}\), we simply took gradient steps in \(\mathbb{R}^{d+1}\) with added Gaussian noise, and projected the weights back to the sphere.

The code to reproduce this experiment can be found at https://github.com/mousavih/2024-MFLD-bilevel.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims made in the abstract and introduction match the paper's contributions. In particular the three bullet points concluding the introduction summarize the paper's contributions section by section. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The scope and limitations of each optimization dynamics considered is clearly discussed within each section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: For each theorem or proposition or corollary or lemma, be it in the main text or in the appendix, the assumptions are clearly stated, and all proofs are provided. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The contributions of this work are theoretical. A numerical illustration is given in Fig. 1, for which the implementation details allowing to reproduce the experiment are provided in Sec. F.4. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide in Sec. F.4 full details for the small numerical experiment of Fig. 1, which are sufficient to reproduce the experiment. The code we used will also be made public at a later date. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The setup of the numerical experiment of Fig. 1 is very simple. Moreover full details are provided in Sec. F.4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The purpose of the small experiment from Fig. 1 is to compare the qualitative behavior of several algorithms: advantage of MFLD over Wasserstein GF in Fig. 0(a), and advantage of MFLD-Bilevel over MFLD-Lifting in Fig. 0(b). This qualitative behavior is clear-cut across the 5 runs, all of which are shown. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The very small scale of the numerical experiment of Fig. 1 means that any standard laptop or desktop computer can be used to reproduce it in, with a runtime of a few minutes. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the Code of Ethics and have not found any deviation of our work from it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: The contributions of this work are theoretical. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The contributions of this work are theoretical. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.