# Confident Natural Policy Gradient for Local Planning in \(q_{\pi}\)-realizable Constrained MDPs

 Tian Tian

University of Alberta, Edmonton

ttian@ualberta.ca

&Lin F. Yang

University of California, Los Angeles

linyang@ee.ucla.edu

Corresponding author.

Csaba Szepesvari

University of Alberta, Google DeepMind, Edmonton

szepesva@ualberta.ca

Corresponding author.

###### Abstract

The constrained Markov decision process (CMDP) framework emerges as an important reinforcement learning approach for imposing safety or other critical objectives while maximizing cumulative reward. However, the current understanding of how to learn efficiently in a CMDP environment with a potentially infinite number of states remains under investigation, particularly when function approximation is applied to the value functions. In this paper, we address the learning problem given linear function approximation with \(q_{\pi}\)-realizability, where the value functions of all policies are linearly representable with a known feature map, a setting known to be more general and challenging than other linear settings. Utilizing a local-access model, we propose a novel primal-dual algorithm that, after \(\tilde{O}(\mathrm{poly}(d)\epsilon^{-3})\)1 queries, outputs with high probability a policy that strictly satisfies the constraints while nearly optimizing the value with respect to a reward function. Here, \(d\) is the feature dimension and \(\epsilon>0\) is a given error. The algorithm relies on a carefully crafted off-policy evaluation procedure to evaluate the policy using historical data, which informs policy updates through policy gradients and conserves samples. To our knowledge, this is the first result achieving polynomial sample complexity for CMDP in the \(q_{\pi}\)-realizable setting.

Footnote 1: Here \(\tilde{O}(\cdot)\) hides \(\log\) factors.

## 1 Introduction

In the classical reinforcement learning (RL) framework, optimizing a single objective above all else can be challenging for safety-critical applications like autonomous driving, robotics, and Large Language Models (LLMs). For example, it may be difficult for an LLM agent to optimize a single reward that fulfills the objective of generating helpful responses while ensuring that the messages are harmless (Dai et al., 2024). In autonomous driving, designing a single reward often requires reliance on complex parameters and hard-coded knowledge, making the agent less efficient and adaptive (Kamran et al., 2022). Optimizing a single objective in motion planning involves combining heterogeneous quantities like path length and risks, which depend on conversion factors that are not necessarily straightforward to determine (Feyzabadi and Carpin, 2014).

The constrained Markov decision process (CMDP) framework (Altman, 2021) emerges as an important RL approach for imposing safety or other critical objectives while maximizing cumulativereward (Wachi and Sui, 2020; Dai et al., 2024; Kamran et al., 2022; Wen et al., 2020; Girard and Reza Emami, 2015; Feyzabadi and Carpin, 2014).

In addition to the single reward function optimized under a standard Markov decision process (MDP), CMDP considers multiple reward functions, with one designated as the primary reward function. The goal of a CMDP is to find a policy that maximizes the primary reward function while satisfying constraints defined by the other reward functions. Although the results of this paper can be applied to multiple constraint functions, for simplicity of presentation, we consider the CMDP problem with only one constraint function.

Our current understanding of how to learn efficiently in a CMDP environment with a potentially infinite number of states remains limited, particularly when function approximation is applied to the value functions. Most works studying the sample efficiency of a learner have focused on the tabular or simple linear CMDP setting (see related works for more details). However, there has been little work in the more general settings such as the \(q_{\pi}\)-realizability, which assumes the value function of all policies can be approximated by a linear combination of a feature map with unknown parameters. Unlike Linear MDPs (Yang and Wang, 2019; Jin et al., 2020), where the transition model is assumed to be linearly representable by a feature map, \(q_{\pi}\)-realizability only imposes the assumption on the existence of a feature map to represent value functions of policies.

Nevertheless, the generality of \(q_{\pi}\)-realizability comes with a price, as it becomes considerably more challenging to design effective learning algorithms, even for the unconstrained settings. For the general online setting, we are only aware of one sample-efficient MDP learning algorithm (Weisz et al., 2023), which, however, is computationally inefficient. To tackle this issue, a line of research (Kearns et al., 2002; Yin et al., 2022; Hao et al., 2022; Weisz et al., 2022) applies the _local-access model_, where the RL algorithm can restart the environment from any visited states - a setting that is also practically motivated, especially when a simulator is provided. The local-access model is more general than the generative model (Kakade, 2003; Sidford et al., 2018; Yang and Wang, 2019; Lattimore et al., 2020; Vaswani et al., 2022), which allows visitation to arbitrary states in an MDP. The local-access model provides the ability to unlock both the sample and computational efficiency of learning with \(q_{\pi}\)-realizability for the unconstrained MDP settings. However, it remains unclear whether we can harness the power of local-access for CMDP learning.

In this paper, we present a systematic study of CMDP for large state spaces, given \(q_{\pi}\)-realizable function approximation in the local-access model. We summarize our contributions as follows:

* We design novel, computationally efficient primal-dual algorithms to learn CMDP near-optimal policies with the local-access model and \(q_{\pi}\)-realizable function classes. The algorithms can return policies with small constraint violations or even no constraint violations and can handle model misspecification.
* We provide theoretical guarantees for the algorithms, showing that they can compute an \(\epsilon\)-optimal policy with high probability, making no more than \(\tilde{O}(\mathrm{poly}(d)\epsilon^{-3})\) queries to the local-access model. The returned policies can strictly satisfy the constraint.
* Under the misspecification setting with a misspecification error \(\omega\), we show that our algorithms achieve an \(\tilde{O}(\omega)+\epsilon\) sub-optimality with high probability, maintaining the same sample efficiency of \(\tilde{O}(\mathrm{poly}(d)\epsilon^{-3})\).

## 2 Related works

Most provably efficient algorithms developed for CMDP are in the tabular and linear MDP settings. In the tabular setting, most notably are the works by (Efroni et al., 2020; Liu et al., 2021; Zheng and Ratliff, 2020; Vaswani et al., 2022; Kalagarla et al., 2021; Yu et al., 2021; Gattami et al., 2021; HasanzadeZonuzy et al., 2021; Chen et al., 2021; Kitamura et al., 2024). Work by Vaswani et al. (2022) have showed their algorithm uses no more than \(\tilde{O}\left(\frac{SA}{(1-\gamma)^{3}\epsilon^{2}}\right)\) samples to achieve relaxed feasibility and \(\tilde{O}\left(\frac{SA}{(1-\gamma)^{3}\zeta^{2}\epsilon^{2}}\right)\) samples to achieve strict feasibility. Here, the \(\gamma\in[0,1)\) is the discount factor and \(\zeta\in(0,\frac{1}{1-\gamma}]\) is the Slater's constant, which characterizes the size of the feasible region and hence the hardness of the CMDP. In their work, they have also provided a lower bound of \(\Omega\left(\frac{SA}{(1-\gamma)^{5}\zeta^{2}\epsilon^{2}}\right)\) on the sample complexity under strict feasibility. However, all the aforementioned results all scale polynomially with the cardinality of the state space.

For problems with large or possibly infinite state spaces, works by (Jain et al., 2022; Ding et al., 2021; Miryoosefi and Jin, 2022; Ghosh et al., 2024; Liu et al., 2022) have used linear function approximations to address the curse of dimensionality. All these works, except Jain et al. (2022); Liu et al. (2022), make the linear MDP assumption, where the transition function is linearly representable.

Under the generative model, for the infinite horizon discounted case, the online algorithm proposed in Jain et al. (2022) achieves a regret of \(\tilde{O}(\sqrt{d}/\sqrt{K})\) with \(\tilde{O}(\sqrt{d}/\sqrt{K})\) constraint violation, where \(K\) is the number of iterations. Work by Liu et al. (2022) is able to achieve a faster \(O(\ln(K)/K)\) convergence rate for both the reward suboptimality and constraint violation. For the online access setting under linear MDP assumption, Ding et al. (2021); Ghosh et al. (2024) achieve a regret of \(\tilde{O}(\mathrm{poly}(d)\,\mathrm{poly}(H)\sqrt{T})\) with \(\tilde{O}(\mathrm{poly}(d)\,\mathrm{poly}(H)\sqrt{T}))\) violations, where \(T\) is the number of episodes and \(H\) is the horizon term.

Miryoosefi and Jin (2022) presented an algorithm that achieves a sample complexity of \(\tilde{O}\left(\frac{d^{3}H^{6}}{\epsilon^{2}}\right)\), where \(d\) is the dimension of the feature space and \(H\) is the horizon term in the finite horizon CMDP setting. In the more general setting under \(q_{\pi}\)-realizability, the best-known upper bounds are in the unconstrained MDP setting.

In the unconstrained MDP setting with access to a local-access model, early work by Kearns et al. (2002) have developed a tree-search style algorithms under this model, albeit in the tabular setting. Under \(v^{*}\)-realizability, Weisz et al. (2021) presented a planner that returns an \(\epsilon\)-optimal policy using \(O((dH/\epsilon)^{|\mathcal{A}|})\) queries to the simulator. More works by (Yin et al., 2022; Hao et al., 2022; Weisz et al., 2022) have considered the local-access model with \(q_{\pi}\)-realizability assumption. Recent work by Weisz et al. (2022) have shown their algorithm can return a near-optimal policy that achieves a sample complexity of \(\tilde{O}\left(\frac{d}{(1-\gamma)^{4}\epsilon^{2}}\right)\).

## 3 Problem formulation

### Constrained MDP

We consider an infinite-horizon discounted CMDP \((\mathcal{S},\mathcal{A},P,r,c,\gamma,b,s_{0})\) consisting a possibly countably infinite state space \(\mathcal{S}\) with a finite set of actions \(\mathcal{A}\), a reward function \(r:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\), a constraint function \(c:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\), a discount factor \(\gamma\in[0,1)\), a constraint threshold \(b\geq 0\), and a fixed initial state \(s_{0}\). Let \(\mathcal{M}_{1}(X)\) denote the space of probability distributions supported on the set \(X\). Then, the transition probability \(P:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{M}_{1}(\mathcal{S})\).

Define a set of stationary randomized policies \(\Pi_{\text{rand}}\), and a policy \(\pi\in\Pi_{\text{rand}}\) maps states to probability distributions over the actions (i.e., \(\pi:\mathcal{S}\rightarrow\mathcal{M}_{1}(\mathcal{A})\)). Given a \(\pi\in\Pi_{\text{rand}}\), the policy \(\pi\) interacts with the CMDP starting from any state \(s\in\mathcal{S}\) through discrete steps indexed by \(t\in\mathbb{N}_{0}\), where \(\mathbb{N}_{0}=\{0,1,2,\dots\}\). This interaction generates a trajectory of \(\{S_{t},A_{t}\}_{t\in\mathbb{N}_{0}}\), where \(S_{0}=s,A_{t}\sim\pi(\cdot|S_{t})\), and \(S_{t+1}\sim P(\cdot|S_{t},A_{t})\). The reward action-value function is defined as \(q_{\pi}^{r}(s,a)=\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}r(S_{t},A_{t})|S_{ 0}=s,A_{0}=a\right]\). Similarly, the constraint action-value function is defined as \(q_{\pi}^{c}(s,a)=\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}c(S_{t},A_{t})|S_ {0}=s,A_{0}=a\right]\). The reward state-value function \(v_{\pi}^{r}(s)=\langle\pi(\cdot|s),q_{\pi}^{r}(s,\cdot)\rangle\), where \(\langle\cdot,\cdot\rangle\) denotes the inner product over actions. Likewise, the constraint state-value function \(v_{\pi}^{c}(s)=\langle\pi(\cdot|s),q_{\pi}^{c}(s,\cdot)\rangle\).

The objective of the CMDP is to find a policy \(\pi\) that maximizes the state-value function \(v_{\pi}^{r}\) starting from a given state \(s_{0}\), while ensuring that the constraint \(v_{\pi}^{c}(s_{0})\geq b\) is satisfied:

\[\max_{\pi\in\Pi_{\text{rand}}}v_{\pi}^{r}(s_{0})\quad s.t.\quad v_{\pi}^{c}(s_{0 })\geq b.\] (1)

We assume the existence of a feasible solution to eq. (1) and let \(\pi^{*}\) denote a solution to eq. (1). A quantity unique to CMDP is the Slater's constant, which is denoted as \(\zeta=\max_{\pi}v_{\pi}^{c}(s_{0})-b\). Slater's constant characterizes the size of the feasibility region, and hence the hardness of the problem.

Because the state space can be large or possibly infinite, we use linear function approximation to approximate the values of stationary randomized policies. Let \(\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\) be a feature map. We assume that both \(q_{\pi}^{r}\) and \(q_{\pi}^{c}\) satisfy the following condition:

**Assumption 1**: _(\(q_{\pi}\)-realizability) There exists \(B>0\) and a misspecification error \(\omega\geq 0\) such that for every \(\pi\in\Pi_{\text{rand}}\), there exists a weight vector \(w_{\pi}\in\mathbb{R}^{d}\), \(\|w_{\pi}\|_{2}\leq B\), and ensures \(|q_{\pi}(s,a)-\langle w_{\pi},\phi(s,a)\rangle|\leq\omega\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\)._

A **mixture policy** is defined as a policy randomly selected from a finite set of policies \(\{\pi_{0},\cdots,\pi_{K}\}\) and executed for all subsequent steps. For example, a mixture policy \(\bar{\pi}_{K}\) is constructed by sampling a policy \(\pi_{k}\) with probability \(\frac{1}{K}\) and following it. The value function of such mixture policy for state \(s\in\mathcal{S}\) is given by \(v_{\bar{\pi}_{K}}(s)=\frac{1}{K}\sum_{k=0}^{K-1}v_{\pi_{k}}(s)\), where \(v_{\pi_{k}}(s)\) is the value function of the individual policy \(\pi_{k}\). Note that \(\bar{\pi}_{K}\) is a non-stationary policy, and the set of non-stationary policies includes the set of stationary randomized policies \(\Pi_{\text{rand}}\).

We assume access to a local access model, where the agent can query the simulator only for states that have been encountered during previous simulations. Our goal is to design an algorithm that outputs a near-optimal mixture policy \(\bar{\pi}_{K}\), whose performance can be characterized in one of two ways.

For a given target error \(\epsilon>0\), the **relaxed feasibility** requires the returned policy \(\bar{\pi}_{K}\) whose sub-optimality gap \(v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\pi_{K}}(s_{0})\) is bounded by \(\epsilon\), while allowing for a small constraint violation. Formally, we require \(\bar{\pi}_{K}\) such that

\[v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\bar{\pi}_{K}}(s_{0})\leq\epsilon\quad s.t\quad v ^{c}_{\bar{\pi}_{K}}(s_{0})\geq b-\epsilon.\]

On the other hand, **strict-feasibility** requires the returned policy \(\bar{\pi}_{K}\) whose sub-optimality gap \(v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\bar{\pi}_{K}}(s_{0})\) is bounded by \(\epsilon\) while not allowing any constraint violation. Formally, we require \(\bar{\pi}_{K}\) such that

\[v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\bar{\pi}_{K}}(s_{0})\leq\epsilon\quad s.t\quad v ^{c}_{\bar{\pi}_{K}}(s_{0})\geq b.\]

#### Notations

For any real number \(a\in\mathbb{R}\), we let \(\lfloor a\rfloor\) to denote the smallest integer \(i\) such that \(i\leq a\). For vector \(x\in\mathbb{R}^{d}\), let \(\|x\|_{1}=\sum_{i}|x_{i}|\), \(\|x\|_{2}=\sqrt{\sum_{i}x_{i}^{2}}\), and \(\|x\|_{\infty}=\max_{i}|x_{i}|\). For a positive definite matrix \(A\in\mathbb{R}^{d\times d}\), the \(\|x\|_{A}^{2}=x^{\top}Ax\). We let \(\operatorname{proj}_{[a_{1},a_{2}]}(\lambda)=\arg\min_{p\in[a_{1},a_{2}]}| \lambda-p|\), and \(\operatorname{trunc}_{[a_{1},a_{2}]}(y)=\min\{\max\{y,a_{1}\},a_{2}\}\). For any two positive numbers \(a,b\), we write \(a=O(b)\) if there exists an absolute constant \(c>0\) such that \(a\leq cb\). We use the \(\tilde{O}\) to hide any polylogarithmic terms.

## 4 Confident-NPG-CMDP, a local-access algorithm for CMDP

In this section, we introduce a primal-dual algorithm, which we call _Confident-NPG-CMDP_ (see algorithm 1).

### A primal-dual approach

We approach solving the CMDP problem by framing it as an equivalent saddle-point problem:

\[\max_{\pi}\min_{\lambda\geq 0}L(\pi,\lambda),\]

where \(L:\Pi_{\text{rand}}\times\mathbb{R}_{+}\to\mathbb{R}\) is the Lagrange function. For a policy \(\pi\in\Pi_{\text{rand}}\) and a Lagrange multiplier \(\lambda\in\mathbb{R}_{+}\), we have

\[L(\pi,\lambda)=v^{r}_{\pi}(s_{0})+\lambda(v^{c}_{\pi}(s_{0})-b).\]

Let \((\pi^{*},\lambda^{*})\) be a solution to this saddle-point problem. By an equivalence to a LP formulation and strong duality (Altman, 2021), \(\pi^{*}\) is the policy that achieves the optimal value in the CMDP as defined in eq. (1). An optimal Lagrange multiplier \(\lambda^{*}\in\arg\min_{\lambda\geq 0}L(\pi^{*},\lambda)\), Therefore, solving eq. (1) is equivalent to finding a saddle-point of the Lagrange function.

A typical primal dual algorithm that finds the saddle-point will proceed in an iterative fashion alternating between a policy update using policy gradient and a dual variable update using mirror descent. The policy gradient is computed with respect to the primal value \(q^{p}_{\pi_{k},\lambda_{b}}=q^{r}_{\pi_{k}}+\lambda_{k}q^{c}_{\pi_{k}}\) and the mirror descent is computed with respect to the constraint value \(v^{c}_{\pi_{k}}(s_{0})=\langle\pi_{k}(\cdot|s_{0}),q^{c}_{\pi_{k}}(s_{0}, \cdot)\rangle\).

```
1:Input:\(s_{0}\) (initial state), \(\epsilon\) (target accuracy), \(\delta\in(0,1]\) (failure probability); \(\gamma\) (discount factor)
2:Initialize:
3: Define \(K,\eta_{1},\eta_{2},m\) according to Theorem 1 for relaxed-feasibility and Theorem 2 for strict-feasibility,
4: Set \(L\leftarrow\lfloor\lfloor K\rfloor/(\lfloor m\rfloor+1)\rfloor\).
5: For each iteration \(k\in\{0,\ldots,\lfloor K\rfloor\}:\pi_{k}\leftarrow\operatorname{Unif}(\mathcal{ A}),\ \tilde{Q}_{k}^{p}(\cdot,\cdot)\gets 0,\ \tilde{V}_{k}^{c}(\cdot)\gets 0\), and \(\lambda_{k}\gets 0\).
6: For each phase \(l\in\{0,\ldots,L+1\}:\mathcal{C}_{l}\leftarrow(),\ D_{l}\leftarrow\{\}\)
7: For \(a\in\mathcal{A}\): if \((s_{0},a)\not\in\operatorname{ActionCov}(\mathcal{C}_{0})\), then append \((s_{0},a)\) to \(\mathcal{C}_{0}\) and set \(\bot\) to \(D_{0}[(s_{0},a)]\quad\triangleright\) see ActionCov defined in eq. (4)
8:while True do\(\triangleright\) main loop
9: Let \(\ell\) be the smallest integer s.t. \(D_{\ell}[z^{\prime}]=\bot\) for some \(z^{\prime}\in\mathcal{C}_{\ell}\)
10: Let \(z\) be the first state-action pair in \(\mathcal{C}_{\ell}\) s.t. \(D_{\ell}[z]=\bot\)
11: If \(\ell=L+1\), then return \(\bar{\pi}_{K}\)
12:\(k_{\ell}\leftarrow\ell\times(\lfloor m\rfloor+1)\)\(\triangleright\) iteration corresponding to phase \(\ell\)
13:\((result,discovered)\leftarrow\) Gather-data(\(\pi_{k_{\ell}},\mathcal{C}_{\ell},\alpha,z\))
14:if\(discovered\) is True then
15: Append \(result\) to \(\mathcal{C}_{0}\) and set \(\bot\) to \(D_{0}[result]\)\(\triangleright\)\(result\) is a state-action pair
16: Goto line 8
17:\(D_{\ell}[z]\gets result\)
18:if\(\exists z^{\prime}\in\mathcal{C}_{\ell}\) s.t. \(D_{\ell}[z^{\prime}]=\bot\)then
19:\(k_{\ell+1}\gets k_{\ell}+(\lfloor m\rfloor+1)\) if \(k_{\ell}+(\lfloor m\rfloor+1)\leq\lfloor K\rfloor\) otherwise \(\lfloor K\rfloor\)
20:for\(k=k_{\ell},\ldots,k_{\ell+1}-1\)do\(\triangleright\) off-policy iterations reusing \(\mathcal{C}_{\ell},D_{\ell}\)
21:\(Q_{k}^{r},\ Q_{k}^{c}\gets LSE(\mathcal{C}_{\ell},D_{\ell},\pi_{k},\pi_ {k_{\ell}})\)
22: For \(s\in\operatorname{Cov}(\mathcal{C}_{\ell})\setminus Cov(\mathcal{C}_{\ell+1})\), and for \(a\in\mathcal{A}\)
23:\(\tilde{Q}_{k}^{p}(s,a)\leftarrow\operatorname{trunc}_{[0,\frac{1}{1-\gamma}]}Q _{k}^{r}(s,a)+\lambda_{k}\operatorname{trunc}_{[0,\frac{1}{1-\gamma}]}Q_{k}^ {c}(s,a)\)
24:\(\tilde{V}_{k}^{c}(s)\leftarrow\operatorname{trunc}_{[0,\frac{1}{1-\gamma}]} \langle\pi_{k}(\cdot|s),Q_{k}^{c}(s,\cdot)\rangle\)
25:\(\triangleright\) update policy
26: For \(s,a\in\mathcal{S}\times\mathcal{A}\):
27:\(\pi_{k+1}(a|s)\leftarrow\begin{cases}\pi_{k+1}(a|s)&\text{if }s\in \operatorname{Cov}(\mathcal{C}_{\ell+1})\\ \pi_{k}(a|s)\frac{\exp(\eta_{1}\tilde{Q}_{k}^{p}(s,a))}{\sum_{a^{\prime}\in \mathcal{A}}\pi_{k}(a^{\prime}|s)\exp(\eta_{1}\tilde{Q}_{k}^{p}(s,a^{\prime})) }&\text{otherwise}\end{cases}\)
28:\(\triangleright\) update dual variable
29:\(\lambda_{k+1}\leftarrow\begin{cases}\lambda_{k+1}&\text{if }s_{0}\in \operatorname{Cov}(\mathcal{C}_{\ell+1})\\ \operatorname{proj}_{[0,U]}\left(\lambda_{k}-\eta_{2}(\tilde{V}_{k}^{c}(s_{0}) -b)\right)&\text{otherwise}.\end{cases}\)
30: For \(z\in\mathcal{C}_{\ell}\) s.t. \(z\not\in\mathcal{C}_{\ell+1}\): append \(z\) to \(\mathcal{C}_{\ell+1}\) and set \(\bot\) to \(D_{\ell+1}[z]\) ```

**Algorithm 1** Confident-NPG-CMDPGiven that we do not have access to an oracle for exact policy evaluations, we must collect data to estimate the primal and constraint values. If we have the least-squares estimates of \(q^{r}_{\pi_{k}}\) and \(q^{c}_{\pi_{k}}\), denoted by \(Q^{r}_{k}\) and \(Q^{c}_{k}\), respectively, then we can compute the least-squares estimate \(Q^{p}_{k}=Q^{r}_{k}+\lambda_{k}Q^{c}_{k}\) to be the estimate of the primal value \(q^{p}_{\pi_{k},\lambda_{k}}\). Additionally, we can compute \(V^{c}_{k}(s_{0})=\langle\pi_{k}(\cdot|s_{0}),Q^{c}_{k}(s_{0},\cdot)\rangle\) to be the least-squares estimate of the constraint value \(v^{c}_{\pi_{k}}(s_{0})\). Then, for any given \((s,a)\in\mathcal{S}\times\mathcal{A}\), our algorithm makes a policy update of the following form:

\[\pi_{k+1}(a|s)\propto\pi_{k}(a|s)\exp(\eta_{1}Q^{p}_{k}(s,a)),\] (2)

followed by a dual variable update of the following form:

\[\lambda_{k+1}\leftarrow\lambda_{k}-\eta_{2}\left(V^{c}_{k}(s_{0})-b\right),\]

where the \(\eta_{1}\) and \(\eta_{2}\) are the step-sizes.

### Core set and least square estimates

To construct the least-squares estimates, let us assume for now that we are given a set of state-action pairs, which we call the core set \(\mathcal{C}\). By organizing the feature vector of each state-action pair in \(\mathcal{C}\) row-wise into a matrix \(\Phi_{\mathcal{C}}\in\mathbb{R}^{|\mathcal{C}|\times d}\), we can write the covariance matrix as \(V(\mathcal{C},\alpha)=\Phi_{\mathcal{C}}^{\top}\Phi_{\mathcal{C}}+\alpha I\). For each \((s,a)\in\mathcal{C}\), suppose we have run Monte Carlo rollouts using the rollout policy \(\pi\) with the local access simulator to obtain an averaged Monte Carlo return denoted by \(\bar{q}(s,a)\). Then we gather all the state-action pairs into a vector \(\bar{q}\in\mathbb{R}^{|\mathcal{C}|}\). For any state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\), the least-square estimate of action-value \(q_{\pi}\) is defined to be

\[Q(s,a)=\langle\phi(s,a),V(\mathcal{C},\alpha)^{-1}\Phi_{\mathcal{C}}^{\top} \bar{q}\rangle.\] (3)

Since the algorithm can only rely on estimates for policy improvement and constraint evaluation, it is imperative that these estimates closely approximate their true action values. In the local access setting, an algorithm may not be able to visit all state-action pairs, so we cannot guarantee that the estimates will closely approximate the true action values for all state-action pairs. However, we can ensure the accuracy of the estimates for a subset of states.

Given \(\mathcal{C}\), let us define a set of state-action pairs whose features satisfies the condition \(\|\phi(s,a)\|_{V(\mathcal{C},\alpha)^{-1}}\leq 1\), then we call this set the action-cover of \(\mathcal{C}\):

\[\mathrm{ActionCov}(\mathcal{C})=\{(s,a)\in\mathcal{S}\times\mathcal{A}:\|\phi( s,a)\|_{V(\mathcal{C},\alpha)^{-1}}\leq 1\}.\] (4)

Following from the action-cover, we have the cover of \(\mathcal{C}\). For a state \(s\) to be in the cover of \(\mathcal{C}\), all its actions \(a\in\mathcal{A}\), the pair \((s,a)\) is in the action-cover of \(\mathcal{C}\). In other words,

\[\mathrm{Cov}(\mathcal{C})=\{s\in\mathcal{S}:\forall a\in\mathcal{A},(s,a)\in \mathrm{ActionCov}(\mathcal{C})\}.\]

For any \(s\in\mathrm{Cov}(\mathcal{C})\), we can ensure the least square estimate \(Q(s,a)\) defined by eq. (3) closely approximates its true action value \(q_{\pi}(s,a)\) for all \(a\in\mathcal{A}\). However, such a core set \(\mathcal{C}\) is not available before the algorithm is run. Therefore, we need an algorithm that will build a core set incrementally in the local-access setting while planning. To achieve this, we build our algorithm on CAPI-QPI-Plan (Weisz et al., 2022), using similar methodology for core set building and data gathering.

### Core set building and data gathering to control the accuracy of the least-square estimates

Confident-NPG-CMDP does not collect data in every iteration but collects data in interval of \(m=O\left(\ln(1+\rho_{0})\operatorname{poly}(\epsilon^{-1}(1-\gamma)^{-1})\right)\), where \(\rho_{0}\geq 0\) is an user defined constant. During each data collection phase, the algorithm performs on-policy evaluation. Between these phases, it conducts \((\lfloor m\rfloor+1)\) off-policy evaluations, reusing data from the most recent on-policy iteration.

By setting \(\rho_{0}\) to a positive value, we impose an upper bound of \(1+\rho_{0}\) on the per-trajectory importance sampling ratio used in off-policy evaluations, and \(m\) is adjusted accordingly to maintain this bound. The total number of data collection phases is \(L=\lfloor\lfloor K\rfloor/(\lfloor m\rfloor+1)\rfloor\), where \(K\) is the total number of iterations. When \(\rho_{0}\) is set to zero, we have \(L=K\), resulting in a purely on-policy version of the algorithm.

Confident-NPG-CMDP maintains a set of core sets \(\{\mathcal{C}_{l}\}_{l=0}^{L+1}\), one for each data collection phases. Each core set \(\mathcal{C}_{l}\) is a list of state-action pairs. Due to the off-policy evaluations, Confident-NPG-CMDP also maintains a set of data sets \(\{D_{l}\}_{l=0}^{L}\). Initially, all core sets are empty, all policies are initialized to the uniform policy, and all data sets are empty.

The algorithm begins by adding the feature vectors corresponding to \((s_{0},a)\) for all actions \(a\in\mathcal{A}\) that are not in the action-cover of \(\mathcal{C}_{0}\). These feature vectors are considered informative. For every \((s,a)\in\mathcal{C}_{0}\), the algorithm adds an entry to \(D_{0}\) and sets its value to the placeholder \(\bot\), indicating that there is no roll-out data yet. Then, in line 9 of algorithm 1, the algorithm finds the smallest integer \(l\in\{0,\ldots,L\}\) such that the corresponding \(D_{l}\) has an entry without roll-out data (i.e., it contains the placeholder \(\bot\)). When such a phase is found, a running phase begins, denoted by \(\ell\) in algorithm 1. We note that when \(\ell=L+1\), the algorithm returns and no roll-outs are stored.

Since only one running phase \(\ell\) can be active at a time, and \(\ell\) can only take value \(l\in\{0,\ldots,L\}\), the algorithm updates the policies of the corresponding iterations in line 27, updates the dual variables of these iterations in line 29, and extends the core set for the next phase in line 30.

Suppose during a running phase with \(\ell=l\), while performing the roll-out in Gather-data subroutine (algorithm 3 in Appendix A), if any state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\) is not in the action-cover of \(\mathcal{C}_{\ell}\), the current running phase stops and the newly discovered state-action pair is added to \(\mathcal{C}_{0}\) in line 15. The same state-action pair is then propagated to \(\mathcal{C}_{1}\) and so on by line 30.

Once a state-action pair is added to a core set by line 7, line 15, and line 30, it remains in that core set for the duration of the algorithm. This means that any \(\mathcal{C}_{l}\), \(l\in\{0,\ldots,L+1\}\) can grow in size and be extended multiple times during the execution of the algorithm. When any new state-action pair is added to a core set, the least-square estimate should be recomputed with the newly added information. This implies that the policy needs to be updated and data re-collected. However, we can avoid restarting the entire data collection procedure by updating only the policy for states that are newly added to the extended core set. We elaborate on this approach further in the next paragraph.

When the algorithm enters the running phase \(\ell=l\), and the Gather-data subroutine returns, the LSE subroutine (algorithm 4) computes the least-squares estimate \(Q_{k}^{r},Q_{k}^{\ast}\) using the most recently extended core set \(\mathcal{C}_{\ell}\) for each corresponding iteration \(k=k_{\ell},\ldots,k_{\ell+1}-1\). Subsequently, \(\tilde{Q}_{k}^{p}\) of line 23 of algorithm 1 is updated with the newly updated least-square estimates \(Q_{k}^{r},Q_{k}^{\ast}\). However, the policy \(\pi_{k+1}\) will only be updated for states that are newly covered by \(\mathcal{C}_{\ell}\) (i.e., \(s\in\operatorname{Cov}(\mathcal{C}_{\ell})\setminus\operatorname{Cov}( \mathcal{C}_{\ell+1})\)). For any states that are already covered by \(\mathcal{C}_{\ell}\) (i.e., \(s\in\operatorname{Cov}(\mathcal{C}_{\ell+1})\)), the policy remains unchanged from its previous update using the \(\tilde{Q}^{p}\) at that time. By updating the policy in this manner, the accuracy guarantee of \(\tilde{Q}_{k}^{p}(s,a)\) with respect to \(q_{\pi_{k},\lambda_{k}}^{p}(s,a)\) is ensured not just for \(\pi_{k}\), but for an extended set of policies defined as follows:

**Definition 1**: _For any policy \(\pi\) from the set of randomized policies \(\Pi_{rand}\) and any subset \(\mathcal{X}\subseteq\mathcal{S}\), the extended set of policies is defined as:_

\[\Pi_{\pi,\mathcal{X}}=\{\pi^{\prime}\in\Pi_{\text{rand}}\mid\pi(\cdot|s)=\pi^{ \prime}(\cdot|s)\text{ for all }s\in\mathcal{X}\}.\]

By maintaining a set of core sets, gathering data via the Gather-data subroutine (algorithm 3 in Appendix A), making policy updates by line 27, and dual variable updates by line 29, we have:

**Lemma 1**: _Whenever LSE subroutine in line 21 of Confident-NPG-CMDP is executed during a running phase \(\ell=l\) for \(l\in\{0,\ldots,L\}\), the least-square estimate \(\tilde{Q}_{k}^{p}(s,a)\) satisfies the following condition for all iterations \(k=k_{\ell},\ldots,k_{\ell+1}-1\) associated with this phase and for all \(s\in\operatorname{Cov}(\mathcal{C}_{\ell})\) and \(a\in\mathcal{A}\),_

\[|\tilde{Q}_{k}^{p}(s,a)-q_{\pi_{k}^{p},\lambda_{k}}^{p}(s,a)|\leq\epsilon^{ \prime}\quad\text{for all }\pi_{k}^{\prime}\in\Pi_{\pi_{k},\operatorname{Cov}( \mathcal{C}_{\ell})},\] (5)

_where \(\epsilon^{\prime}=(1+U)(\omega+\sqrt{\alpha}B+(\omega+\epsilon)\sqrt{\hat{d}})\) with \(\tilde{d}=\tilde{O}(d)\) and \(U\) is an upper bound on the optimal Lagrange multiplier. Similarly, for initial state \(s_{0}\), we have_

\[|\tilde{V}_{k}^{c}(s_{0})-v_{\pi_{k}^{c}}^{c}(s_{0})|\leq\omega+\sqrt{\alpha}B+ (\omega+\epsilon)\sqrt{\hat{d}}\quad\text{for all }\pi_{k}^{\prime}\in\Pi_{\pi_{k}, \operatorname{Cov}(\mathcal{C}_{\ell})}.\] (6)

The accuracy guarantee of eq. (5) and eq. (6) are maintained throughout the execution of the algorithm. By lemma 4.5 of Weisz et al. (2022) (restated in lemma 6 in Appendix A), for any past version of \(\mathcal{C}_{l}\) and the corresponding policy \(\pi_{k}^{\text{best}}\) associated with \(\mathcal{C}_{l}^{\text{past}}\), we have \(\Pi_{\pi_{k},\text{Cov}(\mathcal{C}_{l})}\subseteq\Pi_{\pi_{k}^{\text{post}}, \text{Cov}(\mathcal{C}_{l}^{\text{post}})}\). This means that if eq.5 and eq.6 hold true for any policy in \(\Pi_{\pi_{k}^{\text{post}},\text{Cov}(\mathcal{C}_{l}^{\text{post}})}\), they will also hold true for any future updated policy \(\pi_{k}\).

### Differences between Confident-NPG-CMDP and CAPI-QPI-Plan

CAPI-QPI-Plan is designed for unconstrained MDPs and returns a deterministic policy, which may not be feasible in the constrained setting. In contrast, Confident-NPG-CMDP returns a soft mixture policy \(\bar{\pi}_{K}\), ensuring that \(\bar{\pi}_{K}(a|s)>0\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\).

In constrained MDPs, controlling the dual variable via mirror descent adds an \(\epsilon^{-2}\) factor to the sample complexity. Directly applying CAPI-QPI-Plan would increase the complexity to \(\tilde{O}(\epsilon^{-4})\) due to the need to manage both the dual variable and estimation error. To address this, Confident-NPG-CMDP employs the natural policy gradient for policy improvement and leverages the softmax policy structure to perform off-policy estimation, thereby reducing the complexity to \(\tilde{O}(\epsilon^{-3})\).

By employing a per-trajectory importance sampling ratio, we weigh the Monte Carlo returns generated from data collected in earlier on-policy phases, resulting in unbiased estimates of action values with respect to the target policy. However, this ratio can become large if there is a substantial difference between the on-policy and target policies. To mitigate this, the algorithm collects data at intervals of \(m\), effectively determining when to gather new data as the policy significantly diverges from an earlier recent data-gathering iteration. By setting \(\rho_{0}>0\), we can bound the per-trajectory importance sampling ratio, thus controlling the interval \(m\) for resampling on-policy data to produce well-controlled estimators.

Key algorithmic differences between Confident-NPG-CMDP and CAPI-QPI-Plan:

1. **Policy Improvement Step:** Confident-NPG-CMDP utilizes a softmax over the estimated action-values, whereas CAPI-QPI-Plan employs a greedy approach.
2. **Dual Variable Computation:** Confident-NPG-CMDP requires computation of the dual variable inherent in primal-dual algorithms.
3. **Data Sampling Strategy:** Unlike CAPI-QPI-Plan, Confident-NPG-CMDP does not sample data at every iteration but collects data at specific intervals to control the importance sampling ratio.

In the next two sections, we will demonstrate how these changes ensure a feasible mixture policy for the CMDP and address the additional analytical challenges.

## 5 Confident-NPG-CMDP satisfies relaxed-feasibility

With the accuracy guarantee of the least-square estimates, we prove that at the termination of Confident-NPG-CMDP, the returned mixture policy \(\bar{\pi}_{K}\) satisfies relaxed-feasibility. We note that because of the execution of line 30 in algorithm 1, at termination, one can show using induction that \(\mathcal{C}_{0}=\mathcal{C}_{1}=\cdots=\mathcal{C}_{L+1}\). Therefore, \(\text{Cov}(\mathcal{C}_{0})=\text{Cov}(\mathcal{C}_{1})=\cdots=\text{Cov}( \mathcal{C}_{L})\). Thus, it is sufficient to only consider \(\mathcal{C}_{0}\) at the termination of the algorithm. By line 7 of algorithm 1, we have ensured \(s_{0}\in\text{Cov}(\mathcal{C}_{0})\).

By employing the primal-dual approach discussed in section4, we reduce the CMDP problem to an unconstrained problem with a single reward function of the form \(r_{\lambda}=r+\lambda c\). Therefore, we can apply lemma12 from the Confident-NPG algorithm in the single-reward setting (see AppendixA) to our Confident-NPG-CMDP algorithm, replacing \(\pi\) with \(\pi^{*}\). Consequently, the value difference between \(\pi^{*}\) and \(\bar{\pi}_{K}\) can be bounded, which leads to:

**Lemma 2**: _Let \(\delta\in(0,1]\) be the failure probability, \(\epsilon>0\) be the target accuracy, and \(s_{0}\) be the initial state. Assuming for all \(s\in\text{Cov}(\mathcal{C}_{0})\) and all \(a\in\mathcal{A}\), \(|\tilde{Q}_{k}^{p}(s,a)-q_{\pi_{k}^{\prime},\lambda_{k}}^{p}(s,a)|\leq\epsilon^ {\prime}\) and \(|\tilde{V}_{k}^{c}(s_{0})-v_{\pi_{k}^{\prime}}^{c}(s_{0})|\leq\omega+\sqrt{ \alpha}B+(\omega+\epsilon)\sqrt{\tilde{d}}\) for all \(\pi_{k}^{\prime}\in\Pi_{\pi_{k},\text{Cov}(\mathcal{C}_{0})}\), then, with probability \(1-\delta\)_Confident-NPG-CMDP returns a mixture policy \(\bar{\pi}_{K}\) that satisfies the following,_

\[v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\bar{\pi}_{K}}(s_{0})\leq\frac{5 \epsilon^{\prime}}{1-\gamma}+\frac{(\sqrt{2\ln(A)}+1)(1+U)}{(1-\gamma)^{2}\sqrt {K}},\] \[b-v^{c}_{\bar{\pi}_{K}}(s_{0})\leq[b-v^{c}_{\bar{\pi}_{K}}(s_{0}) ]_{+}\leq\frac{5\epsilon^{\prime}}{(1-\gamma)(U-\lambda^{*})}+\frac{(\sqrt{2 \ln(A)}+1)(1+U)}{(1-\gamma)^{2}(U-\lambda^{*})\sqrt{K}},\]

_where \(\epsilon^{\prime}=(1+U)(\omega+(\sqrt{\alpha}B+(\omega+\epsilon)\sqrt{\tilde{ d}}))\) with \(\tilde{d}=\tilde{O}(d)\), and \(U\) is an upper bound on the optimal Lagrange multiplier._

By setting the parameters to appropriate values, it follows from lemma 2 that we obtain the following result:

**Theorem 1**: _With probability \(1-\delta\), the mixture policy \(\bar{\pi}_{K}\) returned by confident-NPG-CMDP ensures that_

\[v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\bar{\pi}_{K}}(s_{0}) =\tilde{O}(\sqrt{d}(1-\gamma)^{-2}\zeta^{-1}\omega)+\epsilon,\] \[v^{c}_{\bar{\pi}_{K}}(s_{0}) \geq b-\left(\tilde{O}(\sqrt{d}(1-\gamma)^{-2}\zeta^{-1}\omega)+ \epsilon\right).\]

_if we choose \(n=\tilde{O}(\epsilon^{-2}\zeta^{-2}(1-\gamma)^{-4}d)\), \(\alpha=O\left(\epsilon^{2}\zeta^{2}(1-\gamma)^{4}\right)\), \(K=\tilde{O}\left(\epsilon^{-2}\zeta^{-2}(1-\gamma)^{-6}\right)\), \(\eta_{1}=\tilde{O}\left((1-\gamma)^{2}\zeta K^{-1/2}\right)\), \(\eta_{2}=\zeta^{-1}K^{-1/2}\), \(H=\tilde{O}\left((1-\gamma)^{-1}\right)\), \(m=\tilde{O}\left(\epsilon^{-1}\zeta^{-2}(1-\gamma)^{-2}\right)\), and \(L=\lfloor K/(\lfloor m\rfloor+1)\rfloor=\tilde{O}\left(\epsilon^{-1}(1-\gamma) ^{-4}\right)\) total number of data collection phases._

_Furthermore, the algorithm utilizes at most \(\tilde{O}(\epsilon^{-3}\zeta^{-3}d^{2}(1-\gamma)^{-11})\) queries in the local-access setting._

**Remark 1:** In the presence of misspecification error \(\omega>0\), the reward suboptimality and constraint violation is \(\tilde{O}(\omega)+\epsilon\) with the same sample complexity.

**Remark 2:** Suppose the Slater's constant \(\zeta\) is much smaller than the suboptimality bound of \(\tilde{O}(\omega)+\epsilon\), and it is reasonable to set \(\zeta=\epsilon\). Then, the sample complexity is \(\tilde{O}(\epsilon^{-6}(1-\gamma)^{-11}d^{2})\), which is independent of \(\zeta\).

**Remark 3:** Our algorithm requires the Slater's constant \(\zeta\), which can be estimated by running CAPI-QPI-Plan only on the constraint function \(c\), treating it as an unconstrained optimization problem. This yields an approximation of \(\max_{\pi}v^{c}_{\pi}(s_{0})\), allowing us to estimate \(\zeta\). Performing this estimation before executing Confident-NPG-CMDP adds only an additive term to the overall sample complexity.

## 6 Confident-NPG-CMDP satisfies strict-feasibility

To address the strict feasibility problem, where no constraint violations are permitted (i.e., \(v^{c}_{\bar{\pi}_{K}}\geq b\)), the algorithm must solve a more conservative CMDP. We define a surrogate CMDP with the tuple \((\mathcal{S},\mathcal{A},P,r,c,\gamma,b^{\prime},s_{0})\), where \(b^{\prime}=b+\Delta\) for some \(\Delta\geq 0\). Note that \(b^{\prime}\geq b\), imposing stricter constraints than the original problem. The optimal policy of this surrogate CMDP ensures compliance with the original constraint and is defined as follows:

\[\pi^{*}_{\triangle}\in\arg\max v^{r}_{\pi}(s_{0})\quad s.t.\quad v^{c}_{\pi}( s_{0})\geq b^{\prime}.\] (7)

Notice that \(\pi^{*}_{\triangle}\) is a more conservative policy than \(\pi^{*}\), where \(\pi^{*}\) is the optimal policy of the original CMDP objective eq. (1). By solving this surrogate CMDP using Confident-NPG-CMDP and applying the result of theorem 1, we obtain a \(\bar{\pi}_{K}\) that would satisfy

\[v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\bar{\pi}_{K}}(s_{0})\leq\bar{\epsilon}\quad s.t. \quad v^{c}_{\bar{\pi}_{K}}(s_{0})\geq\quad b^{\prime}-\bar{\epsilon},\]

where \(\bar{\epsilon}=\tilde{O}(\omega)+\epsilon\). Expanding out \(b^{\prime}\), we have \(v^{c}_{\bar{\pi}_{K}}(s_{0})\geq b+\triangle-\bar{\epsilon}\). If we can set \(\triangle\) such that \(\triangle-\bar{\epsilon}\geq 0\), then \(v^{c}_{\bar{\pi}_{K}}(s_{0})\geq b\), which satisfies strict-feasibility. We show this formally in the next theorem, where \(\triangle=O(\epsilon(1-\gamma)\zeta)\) and is incorporated into the algorithmic parameters for ease of presentation.

**Theorem 2**: _With probability \(1-\delta\), a target \(\epsilon>0\), the mixture policy \(\bar{\pi}_{K}\) returned by confident-NPG-CMDP ensures that \(v^{\epsilon}_{\pi^{*}}(s_{0})-v^{\epsilon}_{\pi_{K}}(s_{0})\leq\epsilon\) and \(v^{\epsilon}_{\pi_{K}}(s_{0})\geq b\), if assuming the misspecification error \(\omega\leq\epsilon\zeta^{2}(1-\gamma)^{3}(1+\sqrt{\tilde{d}})^{-1}\), and if we choose \(\alpha=O\left(\epsilon^{2}\zeta^{3}(1-\gamma)^{5}\right),K=\tilde{O}\left( \epsilon^{-2}\zeta^{-4}(1-\gamma)^{-8}\right),n=\tilde{O}\left(\epsilon^{-2} \zeta^{-4}(1-\gamma)^{-8}d\right),H=\tilde{O}\left((1-\gamma)^{-1}\right),m= \tilde{O}\left(\epsilon^{-1}\zeta^{-2}(1-\gamma)^{-3}\right)\), and \(L=\lfloor K/(\lfloor m\rfloor+1)\rfloor=\tilde{O}((\epsilon^{-1}\zeta^{-2}(1- \gamma)^{-5}))\) total data collection phases._

_Furthermore, the algorithm utilizes at most \(\tilde{O}(\epsilon^{-3}\zeta^{-6}(1-\gamma)^{-14}d^{2})\) queries in the local-access setting._

**Remark 1:** We note that by solving this conservative CMDP incurs a higher sample complexity, necessitating a separate treatment for this setting. Additionally, in the presence of a misspecification error \(\omega>0\), the strict-feasibility setting requires additional assumptions on \(\omega\), whereas the relaxed-feasibility setting does not. The sample complexity of the relaxed-feasibility setting can be independent of Slater's constant, whereas for strict feasibility, the returned policy must strictly adhere to constraints, and we cannot simply set Slater's constant \(\zeta\) to \(\epsilon\) and disregard its impact.

## 7 A discussion on memory cost and some implementation details

The overall memory requirement is \(\tilde{d}nH(L+1)+\tilde{d}+(L+1)(m+1)\tilde{d}d\). The term \(\tilde{d}nH(L+1)\) comes from maintaining \(L+1\) copies of the core sets, and each core set contains no more than \(\tilde{d}\) state-action pairs. For each state-action pair in \(\mathcal{C}_{l}\) for \(l\in\{0,\ldots,L\}\), the algorithm stores \(n\) trajectories consisting of \(H\) tuples \((s,a,r,c)\).

In phase \(L+1\), the algorithm terminates, so no roll-outs are stored. The second term \(\tilde{d}\) accounts for the elements stored in \(\mathcal{C}_{L+1}\), which has no more than \(\tilde{d}\) elements.

Finally, the last term is the memory required to store the least-square weights of the estimator during core set extensions. Each core set \(\mathcal{C}_{l}\) can undergo up to \(\tilde{d}\) extensions. Recall that one state-action pair is added to \(\mathcal{C}_{0}\) at a time, and subsequently propagated to \(\mathcal{C}_{1},\mathcal{C}_{2}\) and so on, ensuring that each core set contains no more than \(\tilde{d}\) elements. During every extension of \(\mathcal{C}_{l}\), the newly added state-action pairs are marked, and up to \(m+1\) least-square weights are stored to account for the corresponding iterations associated with \(\mathcal{C}_{l}\). Since each weight vector has a dimension \(d\), and there are \(L+1\) core sets maintained this manner, the total memory required to store all least-square weights is bounded by \((L+1)(m+1)\tilde{d}d\).

We store the least-squares weights because the algorithm must return a mixture policy, which requires access to all policies \(\pi_{0},\ldots,\pi_{K-1}\). Instead of storing each \(\pi_{k}\) for \(k=0,\ldots,K-1\) across the entire state-action space, the algorithm tracks the state-action pairs newly added to each core set during extensions and saves their corresponding least-squares weights for each extension. With this stored information and the initialization of \(\pi_{0}\), a subroutine can reconstruct the policies \(\pi_{k}(\cdot|s)\) for any \(s\) and iteration \(k\) as needed. Please refer to Appendix E for a brief discussion on how to mark the state-action pairs, store the least-square weights, and use this information to reconstruct the policies as required.

## 8 Conclusion

We have presented a primal-dual algorithm for planning in CMDP with large state spaces, given \(q_{\pi}\)-realizable function approximation. The algorithm, with high probability, returns a policy that achieves both the relaxed and strict feasibility CMDP objectives, using no more than \(\tilde{O}(\epsilon^{-3}d^{2}\operatorname{poly}(\zeta^{-1}(1-\gamma)^{-1}))\) queries to the local-access simulator.

Our algorithm does not query the simulator and collect data in every iteration. Instead, the algorithm queries the simulator only at fixed intervals. Between these data collection intervals, our algorithm improves the policy using off-policy optimization. This approach makes it possible to achieve the desired sample complexity in both feasibility settings.

## Acknowledgments and Disclosure of Funding

Tian Tian would like to thank Roshan Shariff and Kenny Young for their insightful comments and helpful feedback during the preparation of this manuscript. Csaba Szepesvari also gratefully acknowledges funding from the Canada CIFAR AI Chairs Program, Amii, and NSERC. Lin Yang is supported in part by NSF \(\#\)2221871 and an Amazon Faculty Award.

## References

* Altman (2021) Altman, E. (2021). _Constrained Markov decision processes_. Routledge.
* Chen et al. (2021) Chen, Y., Dong, J., and Wang, Z. (2021). A primal-dual approach to constrained markov decision processes. _arXiv preprint arXiv:2101.10895_.
* Dai et al. (2024) Dai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang, Y., and Yang, Y. (2024). Safe RLHF: Safe reinforcement learning from human feedback. In _The Twelfth International Conference on Learning Representations_.
* Ding et al. (2021) Ding, D., Wei, X., Yang, Z., Wang, Z., and Jovanovic, M. (2021). Provably efficient safe exploration via primal-dual policy optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 3304-3312. PMLR.
* Efroni et al. (2020) Efroni, Y., Mannor, S., and Pirotta, M. (2020). Exploration-exploitation in constrained mdps. _CoRR_, abs/2003.02189.
* Feyzabadi and Carpin (2014) Feyzabadi, S. and Carpin, S. (2014). Risk-aware path planning using hirerachical constrained markov decision processes. In _2014 IEEE International Conference on Automation Science and Engineering (CASE)_, pages 297-303.
* Gattami et al. (2021) Gattami, A., Bai, Q., and Aggarwal, V. (2021). Reinforcement learning for constrained markov decision processes. In _International Conference on Artificial Intelligence and Statistics_, pages 2656-2664. PMLR.
* Ghosh et al. (2024) Ghosh, A., Zhou, X., and Shroff, N. (2024). Towards achieving sub-linear regret and hard constraint violation in model-free rl. In _International Conference on Artificial Intelligence and Statistics_, pages 1054-1062. PMLR.
* Girard and Reza Emami (2015) Girard, J. and Reza Emami, M. (2015). Concurrent markov decision processes for robot team learning. _Engineering Applications of Artificial Intelligence_, 39:223-234.
* Hao et al. (2022) Hao, B., Lazic, N., Yin, D., Abbasi-Yadkori, Y., and Szepesvari, C. (2022). Confident least square value iteration with local access to a simulator. In _International Conference on Artificial Intelligence and Statistics_, pages 2420-2435. PMLR.
* HasanzadeZonuzy et al. (2021) HasanzadeZonuzy, A., Kalathil, D., and Shakkottai, S. (2021). Model-based reinforcement learning for infinite-horizon discounted constrained markov decision processes. _IJCAI 2021_.
* Jain et al. (2022) Jain, A., Vaswani, S., Babanezhad, R., Szepesvari, C., and Precup, D. (2022). Towards painless policy optimization for constrained mdps. In _Uncertainty in Artificial Intelligence_, pages 895-905. PMLR.
* Jin et al. (2020) Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In _Conference on learning theory_, pages 2137-2143. PMLR.
* Kakade (2003) Kakade, S. M. (2003). _On the sample complexity of reinforcement learning_. University of London, University College London (United Kingdom).
* Kalagarla et al. (2021) Kalagarla, K. C., Jain, R., and Nuzzo, P. (2021). A sample-efficient algorithm for episodic finite-horizon mdp with constraints. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 8030-8037.
* Krizhevsky et al. (2014)Kamran, D., Simao, T. D., Yang, Q., Ponnambalam, C. T., Fischer, J., Spaan, M. T., and Lauer, M. (2022). A modern perspective on safe automated driving for different traffic dynamics using constrained reinforcement learning. In _2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)_, pages 4017-4023. IEEE.
* Kearns et al. (2002) Kearns, M., Mansour, Y., and Ng, A. Y. (2002). A sparse sampling algorithm for near-optimal planning in large markov decision processes. _Machine learning_, 49:193-208.
* Kitamura et al. (2024) Kitamura, T., Kozuno, T., Kato, M., Ichihara, Y., Nishimori, S., Sannai, A., Sonoda, S., Kumagai, W., and Matsuo, Y. (2024). A policy gradient primal-dual algorithm for constrained mdps with uniform pac guarantees. _arXiv preprint arXiv:2401.17780_.
* Lattimore and Szepesvari (2020) Lattimore, T. and Szepesvari, C. (2020). _Bandit algorithms_. Cambridge University Press.
* Lattimore et al. (2020) Lattimore, T., Szepesvari, C., and Weisz, G. (2020). Learning with good feature representations in bandits and in rl with a generative model. In _International conference on machine learning_, pages 5662-5670. PMLR.
* Liu et al. (2021) Liu, T., Zhou, R., Kalathil, D., Kumar, P. R., and Tian, C. (2021). Learning policies with zero or bounded constraint violation for constrained mdps. _CoRR_, abs/2106.02684.
* Liu et al. (2022) Liu, T., Zhou, R., Kalathil, D., Kumar, P. R., and Tian, C. (2022). Policy optimization for constrained mdps with provable fast global convergence.
* Miryoosefi and Jin (2022) Miryoosefi, S. and Jin, C. (2022). A simple reward-free approach to constrained reinforcement learning. In _International Conference on Machine Learning_, pages 15666-15698. PMLR.
* Sidford et al. (2018) Sidford, A., Wang, M., Wu, X., Yang, L., and Ye, Y. (2018). Near-optimal time and sample complexities for solving markov decision processes with a generative model. _Advances in Neural Information Processing Systems_, 31.
* Vaswani et al. (2022) Vaswani, S., Yang, L., and Szepesvari, C. (2022). Near-optimal sample complexity bounds for constrained mdps. _Advances in Neural Information Processing Systems_, 35:3110-3122.
* Wachi and Sui (2020) Wachi, A. and Sui, Y. (2020). Safe reinforcement learning in constrained markov decision processes. In _International Conference on Machine Learning_, pages 9797-9806. PMLR.
* Weisz et al. (2021) Weisz, G., Amortila, P., Janzer, B., Abbasi-Yadkori, Y., Jiang, N., and Szepesvari, C. (2021). On query-efficient planning in mdps under linear realizability of the optimal state-value function. In _Conference on Learning Theory_, pages 4355-4385. PMLR.
* Weisz et al. (2022) Weisz, G., Gyorgy, A., and Szepesvari, C. (2022). Confident approximate policy iteration for efficient local planning in q pi realizable mdps. _arXiv preprint arXiv_, 2210.
* Weisz et al. (2023) Weisz, G., Gyorgy, A., and Szepesvari, C. (2023). Online rl in linearly \(q^{\pi}\)-realizable mdps is as easy as in linear mdps if you learn what to ignore.
* Wen et al. (2020) Wen, L., Duan, J., Li, S. E., Xu, S., and Peng, H. (2020). Safe reinforcement learning for autonomous vehicles through parallel constrained policy optimization. In _2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)_, pages 1-7.
* Yang and Wang (2019) Yang, L. and Wang, M. (2019). Sample-optimal parametric q-learning using linearly additive features. In _International conference on machine learning_, pages 6995-7004. PMLR.
* Yin et al. (2022) Yin, D., Hao, B., Abbasi-Yadkori, Y., Lazic, N., and Szepesvari, C. (2022). Efficient local planning with linear function approximation. In _International Conference on Algorithmic Learning Theory_, pages 1165-1192. PMLR.
* Yu et al. (2021) Yu, T., Tian, Y., Zhang, J., and Sra, S. (2021). Provably efficient algorithms for multi-objective competitive rl. In _International Conference on Machine Learning_, pages 12167-12176. PMLR.
* Zheng and Ratliff (2020) Zheng, L. and Ratliff, L. (2020). Constrained upper confidence reinforcement learning. In Bayen, A. M., Jadbabaie, A., Pappas, G., Parrilo, P. A., Recht, B., Tomlin, C., and Zeilinger, M., editors, _Proceedings of the 2nd Conference on Learning for Dynamics and Control_, volume 120 of _Proceedings of Machine Learning Research_, pages 620-629. PMLR.

Confident-NPG in a single reward setting

The pseudo code of Confident-NPG with a single reward setting is the same as Confident-NPG-CMDP in algorithm 1, except that line 24 and line 29 will not appear in Confident-NPG. Additionally, the LSE subroutine returns just \(Q^{r}\), and the policy update will be with respect to \(\tilde{Q}^{r}\). For the complete pseudo code of Confident-NPG in the single reward setting, please see algorithm 2. In the following analysis, for convenience, we omit the superscript \(r\).

```
0:\(s_{0}\) (initial state), \(\epsilon\) (target accuracy), \(\delta\in(0,1]\) (failure probability), \(c\geq 0\), \(\gamma\), \(K=\frac{2\ln[\lfloor\mathcal{A}\rfloor]}{(1-\gamma)^{\epsilon}\epsilon^{2}}\), \(\eta_{1}=(1-\gamma)\sqrt{\frac{2\ln[\lfloor\mathcal{A}\rfloor]}{K}}\), \(m=\frac{\ln(1+\rho_{0})}{2\epsilon(1-\gamma)\ln\left(\frac{\delta}{(1-\gamma )^{2}}\right)}\), \(L=\lfloor\lfloor K\rfloor/(\lfloor m\rfloor+1)\rfloor\). Initialize: for each iteration \(k\in\{0,\ldots,\lfloor K\rfloor\}:\pi_{k}\leftarrow\text{Uniform}(\mathcal{A}), \;\tilde{Q}^{r}_{k}(s,a)\gets 0\), for all \(s,a\in\mathcal{S}\times\mathcal{A}\), and \(\lambda_{k}\gets 0\). For each phase \(l\in\{0,\ldots,L+1\}:\mathcal{C}_{l}\leftarrow(),\;D_{l}\leftarrow\{\}\)
1:for\(a\in\mathcal{A}\)do
2:if\((s_{0},a)\not\in\text{Action}\text{Cov}(\mathcal{C}_{0})\)then
3:\(\text{Append}\;(s_{0},a)\) to \(\mathcal{C}_{0}\); \(D_{0}[(s_{0},a)]\leftarrow\perp\)
4:while True do\(\triangleright\) main loop
5: Get the smallest integer \(\ell\) s.t. \(D_{\ell}[z^{\prime}]=\perp\) for some \(z^{\prime}\in\mathcal{C}_{\ell}\)
6: Get the first state-action pair \(z\) in \(\mathcal{C}_{\ell}\) s.t. \(D_{\ell}[z]=\perp\)
7:if\(\ell=L+1\)thenreturn\(\bar{\pi}_{K}\)
8:\(k_{\ell}\leftarrow\ell\times(\lfloor m\rfloor+1)\)\(\triangleright\) iteration corresponding to phase \(\ell\)
9:\((result,discovered)\leftarrow\text{Gather-data}(\pi_{k_{\ell}},\mathcal{C}_{\ell}, \alpha,z)\)
10:if\(discovered\) is True then
11:\(\triangleright\) result is a state-action pair
12:\(\text{Append}\;result\) to \(\mathcal{C}_{0}\); \(D_{0}[result]\leftarrow\perp\)
13:break
14:\(\triangleright\)\(result\) is a set of \(n\)\(H\)-horizon trajectories \(\sim\pi_{k_{\ell}}\) starting at \(z\)
15:\(D_{\ell}[z]\gets result\)
16:if\(\not\exists z^{\prime}\in\mathcal{C}_{\ell}\) s.t. \(D_{\ell}[z^{\prime}]=\perp\)then
17:\(k_{\ell+1}\gets k_{\ell}+(\lfloor m\rfloor+1)\) if \(k_{\ell}+(\lfloor m\rfloor+1)\leq\lfloor K\rfloor\) otherwise \(\lfloor K\rfloor\)
18:\(\triangleright\) update policy for every \(k\in[k_{\ell},k_{\ell+1}-1]\) using \(\mathcal{C}_{\ell},D_{\ell}\)
19:for\(k=k_{\ell},\ldots,k_{\ell+1}-1\)do
20:\(Q^{r}_{k}\)\(\_\leftarrow\)\(LSE(\mathcal{C}_{\ell},D_{\ell},\pi_{k},\pi_{k_{\ell}})\)
21:\(\triangleright\) update variables and improve policy
22:for all \(s\in\text{Cov}(\mathcal{C}_{\ell})\setminus\text{Cov}(\mathcal{C}_{\ell+1})\), and for all \(a\in\mathcal{A}\)do
23:\(\tilde{Q}^{r}_{k}(s,a)\gets Trunc_{[0,\frac{1}{1-\gamma}]}Q^{r}_{k}(s,a)\)
24:for all \(s,a\in\mathcal{S}\times\mathcal{A}\)do
25:\(\pi_{k+1}(a|s)\leftarrow\begin{cases}\pi_{k+1}(a|s)&\text{if }s\in\text{Cov}( \mathcal{C}_{\ell+1})\\ \pi_{k}(a|s)\frac{\exp(\eta_{1}\tilde{Q}^{r}_{k}(s,a))}{\sum_{a^{\prime}\in \mathcal{A}}\pi_{k}(a^{\prime}|s)\exp(\eta_{1}\tilde{Q}^{r}_{k}(s,a^{\prime})) }&\text{otherwise}\end{cases}\)
26:for\(z\in\mathcal{C}_{\ell}\) s.t. \(z\not\in\mathcal{C}_{\ell+1}\)do
27:\(\text{Append}\;z\) to \(\mathcal{C}_{\ell+1}\); \(D_{\ell+1}[z]\leftarrow\perp\) ```

**Algorithm 2** Confident-NPG

### The Gather-data subroutine

Given a core set \(\mathcal{C}\), a behaviour policy \(\mu\), a starting state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\) along with some algorithmic parameters, the Gather-data subroutine (algorithm 3) will either 1) return a newly discovered state-action pair, or 2) return a set of \(n\) trajectories. Each trajectory is generated by running the behaviour policy \(\mu\) with the simulator for \(H\) consecutive steps. For \(i=1,\ldots,n\), let \(\tau^{i}_{s,a}\) denote the \(i\)th trajectory starting from \(s,a\) to be \(\{S^{i}_{0}=s,A^{i}_{0}=a,R^{i}_{1},C^{i}_{1},\cdots,S^{i}_{H-1},A^{i}_{H-1},R^ {i}_{H},C^{i}_{H},S^{i}_{H}\}\). Then the \(i\)-th discounted cumulative rewards \(G(\tau^{i}_{s,a})=\sum_{h=0}^{H-1}\gamma^{h}R^{i}_{h+1}\). For a target policy \(\pi\), then the empirical mean of the discounted sum of rewards is as follows,

\[\bar{q}(s,a)=\frac{1}{n}\sum_{i=1}^{n}\rho(\tau^{i}_{s,a})G(\tau^{i}_{s,a}),\] (8)

where \(\rho(\tau^{i}_{s,a})=\Pi_{h=1}^{H-1}\frac{\pi(A^{i}_{h}|S^{i}_{h})}{\mu(A^{i}_{ h}|S^{i}_{h})}\) is the per-trajectory importance sampling ratio.

For some given \(\bar{s}\) and \(\bar{a}\), we establish the following relationship between the target policy \(\pi\) and the behavior policy \(\mu\):

\[\pi(\bar{a}|\bar{s})\propto\mu(\bar{a}|\bar{s})\exp(f(\bar{s},\bar{a}))\quad \text{s.t.}\quad\sup_{\bar{s},\bar{a}}|f(\bar{s},\bar{a})|\leq\frac{\ln(1+\rho_ {0})}{2H},\] (9)

where \(f(\bar{s},\bar{a}):\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{+}\) and \(\rho_{0}\geq 0\) is a given constant. By establishing the relationship stated in eq. (9), the importance sampling ratio \(\rho(\tau^{i}_{s,a})\) can be bounded by \(1+\rho_{0}\) as this is proven in the following lemma:

**Lemma 3**: _Suppose the trajectory \(\tau=(S_{0},A_{0},R_{1},S_{1},A_{1},\cdots,S_{H-1},A_{H-1},R_{H})\) is sampled from a behaviour policy \(\mu\), and \(\mu\) is related to the target policy \(\pi\) via eq. (9). The per-trajectory importance sampling ratio_

\[\rho(\tau)=\Pi_{h=1}^{H-1}\frac{\pi(A_{h}|S_{h})}{\mu(A_{h}|S_{h})}\leq 1+ \rho_{0}.\]Proof: Let \(l_{0}=\sup_{s,a}|f(s,a)|\), where \(f\) is defined in eq. (9). For any \((s,a)\in\{(S_{h},A_{h})\}_{h=1}^{H-1}\),

\[\pi(a|s) =\mu(a|s)\frac{\exp(f(s,a))}{\sum_{a^{\prime}}\mu(a^{\prime}|s)\exp (f(s,a^{\prime}))}\leq\mu(a|s)\frac{\exp(l_{0})}{\sum_{a^{\prime}}\mu(a^{\prime }|s)\exp(-l_{0})}\] \[\leq\mu(a|s)\exp(2l_{0}).\]

We see that \(\frac{\pi(a|s)}{\mu(a|s)}\leq\exp(2l_{0})\), and it follows that \(\Pi_{h=1}^{H-1}\frac{\pi(A_{h}|S_{h})}{\mu(A_{h}|S_{h})}\leq\exp(2l_{0}H)\). By assumption, \(l_{0}\leq\frac{\ln(1+\rho_{0})}{2H}\), then \(\exp(2l_{0}H)\leq\exp\left(2H\frac{\ln(1+\rho_{0})}{2H}\right)\leq 1+\rho_{0}\). 

Define the probability distribution \(P_{\pi,s,a}\) over trajectory \(\{S_{h},A_{h},R_{h+1}\}_{h\geq 0}\) as follows: the initial state \(S_{0}\) is set deterministically to \(s\), and the initial action \(A_{0}\) is set deterministically to \(a\). For each subsequent time step \(h\geq 0\), the next state \(S_{h+1}\) is sampled according to the transition probability \(P(\cdot|S_{h},A_{h})\), and the next action \(A_{h+1}\) is sampled from the policy \(\pi(\cdot|S_{h+1})\). It follows that \(\mathbb{E}_{\pi,s,a}\) denotes the expectation with respect to distribution \(P_{\pi,s,a}\).

Now, we show that for all \((s,a)\in\mathcal{C}\), \(|\bar{q}(s,a)-q_{\pi}(s,a)|\leq\epsilon\), where \(\epsilon>0\) is a given target error. Additionally, the accuracy guarantee of \(|\bar{q}(s,a)-q_{\pi}(s,a)|\leq\epsilon\) continues to holds for the extended set of policies defined in definition 1. Formally, we state the main result of this section.

**Lemma 4**: _For any \(s,a\in\mathcal{S}\times\mathcal{A},\mathcal{X}\subset\mathcal{S}\), the Gather-data subroutine will either return with \(((s^{\prime},a^{\prime}),\text{True})\) for some \(s^{\prime}\not\in\mathcal{X}\), or it will return with \((D[(s,a)],\text{False})\), where \(D[(s,a)]\) is a set of \(n\) independent trajectories generated by a behavior policy \(\mu\) starting from \((s,a)\). When Gather-data returns False for \((s,a)\), we assume 1) the behavior policy \(\mu\) and target policy \(\pi\) for all the states and actions encountered in the trajectories stored in \(D[(s,a)]\) satisfy eq. (9) and 2) \(\bar{q}(s,a)\) is an unbiased estimate of \(\mathbb{E}_{\pi^{\prime},s,a}[\sum_{h=0}^{H-1}\gamma^{h}R_{h+1}]\) for all \(\pi^{\prime}\in\Pi_{\pi,\mathcal{X}}\). Then, the importance-weighted return \(\bar{q}(s,a)\) constructed from \(D[(s,a)]\) according to eq. (8) will, with probability \(1-\delta^{\prime}\),_

\[|\bar{q}(s,a)-q_{\pi^{\prime}}(s,a)|\leq\epsilon\quad\text{for all }\pi^{\prime}\in\Pi_{\pi,\mathcal{X}}.\]

Proof: The proof follows similar reasoning to Lemma 4.2 Weisz et al. (2022).

Recall \(D[(s,a)]\) stores \(n\) number of trajectories indexed by \(i\), where each trajectory \(\tau^{i}_{s,a}=(S^{i}_{0}=s,A^{i}_{0}=a,R^{i}_{0},\ldots,S^{i}_{H-1})\sim\mu\). The per-trajectory importance sampling ratio \(\rho(\tau^{i}_{s,a})=\Pi_{h=1}^{H-1}\frac{\pi(A^{i}_{h}|S^{i}_{h})}{\mu(A^{i}_ {h}|S^{i}_{h})}\), and the discounted cumulative return is \(\sum_{h=0}^{H-1}\gamma^{h}R^{i}_{h+1}\). By the triangle inequality,

\[|\bar{q}(s,a)-q_{\pi}(s,a)|=|\frac{1}{n}\sum_{i=1}^{n}\Pi_{h=0}^{H -1}\frac{\pi(A^{i}_{h}|S^{i}_{h})}{\mu(A^{i}_{h}|S^{i}_{h})}\sum_{h=0}^{H-1} \gamma^{h}R^{i}_{h+1}-q_{\pi}(s,a)|\] \[\leq|\frac{1}{n}\sum_{i=1}^{n}\rho(\tau^{i}_{s,a})\sum_{h=0}^{H-1 }\gamma^{h}R^{i}_{h+1}-\mathbb{E}_{\pi,s,a}\sum_{h=0}^{H-1}\gamma^{h}R_{h+1}| +|E_{\pi,s,a}\sum_{h=0}^{H-1}\gamma^{h}R_{h+1}-q_{\pi}(s,a)|.\] (10)

The goal is to bound each of the two terms in eq. (10) by \(\frac{\epsilon}{4}\) so that the sum of the two is \(\frac{\epsilon}{2}\).

By assumption, the policies \(\pi\) and \(\mu\) satisfies eq. (9) for all state-action pairs \((S^{i}_{h},A^{i}_{h})\) extracted from the \(i\)-trajectory \(\tau^{i}_{s,a}\). Second, \(\bar{q}(s,a)\) is assumed to be an unbiased estimate of \(\mathbb{E}_{\pi,s,a}\left[\sum_{h=0}^{H-1}\gamma^{h}R_{h+1}\right]\). Note that for all \(i=1,\ldots,n\), the importance weighted cumulative return \(\rho(\tau^{i}_{s,a})\sum_{h=0}^{H-1}\gamma^{h}R^{i}_{h+1}\) are independent random variables, and the value of each such random variable \(\in\left[0,\frac{1+\rho_{0}}{1-\gamma}\right]\). This is because 1) \(\sum_{h=0}^{H-1}\gamma^{h}R^{i}_{h+1}\leq\frac{1}{1-\gamma}\) since the rewards take values in the range of \([0,1]\), and 2) \(\rho(\tau^{i}_{s,a})\leq 1+\rho_{0}\) by lemma 3. We apply Hoeffding's inequality,

\[\mathbb{P}\left(\left|\frac{1}{n}\sum_{i=1}^{n}\rho(\tau^{i}_{s,a})\sum_{h=0}^{ H-1}\gamma^{h}R^{i}_{h+1}-\mathbb{E}_{\pi,s,a}\sum_{h=0}^{H-1}\gamma^{h}R_{h+1} \right|>\frac{\epsilon}{4}\right)\leq 2\exp\left(-\frac{2n\left(\frac{\epsilon}{4}\right)^{2}}{ \left(\frac{1+\rho_{0}}{1-\gamma}\right)^{2}}\right).\]Then, we have with probability \(1-\delta^{\prime}/2\), where \(\delta^{\prime}=2\exp\left(-\frac{2n\epsilon^{2}}{16\left(\frac{1+\rho_{0}}{1- \gamma}\right)^{2}}\right)\), the first term in eq. (10) \(|\frac{1}{n}\sum_{i=1}^{n}\rho(\tau_{s,a}^{i})\sum_{h=0}^{H-1}\gamma^{h}R_{h+1 }^{i}-\mathbb{E}_{\pi,s,a}\sum_{h=0}^{H-1}\gamma^{h}R_{h+1}|\leq\frac{\epsilon }{4}\). For the second term in eq. (10),

\[|E_{\pi,s,a}\sum_{h=0}^{H-1}\gamma^{h}R_{h+1}-q_{\pi}(s,a)|=|\mathbb{E}_{\pi,s,a}\sum_{h=H}^{\infty}\gamma^{h}R_{h+1}|\leq\frac{\gamma^{H}}{1-\gamma}.\]

By the choice of \(H=\frac{\ln(4/(1-\gamma)\epsilon)}{1-\gamma}\), we have \(\frac{\gamma^{H}}{1-\gamma}\leq\frac{\epsilon}{4}\). Putting everything together, we get \(|\bar{q}(s,a)-q_{\pi}(s,a)|\leq\frac{\epsilon}{2}\). To get the final result, we need to upper bound \(|q_{\pi}(s,a)-q_{\pi^{\prime}}(s,a)|\) by \(\frac{\epsilon}{2}\), so that \(|\bar{q}(s,a)-q_{\pi^{\prime}}(s,a)|\leq|\bar{q}(s,a)-q_{\pi}(s,a)|+|q_{\pi}(s,a)-q_{\pi^{\prime}}(s,a)|\leq\epsilon\).

Recall that \(\pi\) and \(\pi^{\prime}\) differs in distributions over states that are not in \(\mathcal{X}\). For a trajectory \((S_{0}=s,A_{0}=a,S_{1},\dots)\), let \(T\) be the smallest positive integer such that \(S_{T}\not\in\mathcal{X}\), then the distribution of the trajectory \((S_{0}=s,A_{0}=a,S_{1},\dots,S_{T})\) are the same under \(P_{\pi,s,a}\) and \(P_{\pi^{\prime},s,a}\) because \(\pi(\cdot|s)=\pi^{\prime}(\cdot|s)\) for all \(s\in\mathcal{X}\). Then,

\[|q_{\pi}(s,a)-q_{\pi^{\prime}}(s,a)|=\left|\mathbb{E}_{\pi,s,a} \left[\sum_{t=0}^{T-1}\gamma^{t}R_{t}+\gamma^{T}v_{\pi}(S_{T})\right]- \mathbb{E}_{\pi^{\prime},s,a}\left[\sum_{t=0}^{T-1}\gamma^{t}R_{t}+\gamma^{T} v_{\pi^{\prime}}(S_{T})\right]\right|\] \[=\sum_{s^{\prime}\in\mathcal{X},a^{\prime}}P_{\pi,s,a}(S_{T-1}=s^ {\prime},A_{T-1}=a^{\prime})P(S_{T}|s^{\prime},a^{\prime})\gamma^{T}v_{\pi}(S_ {T})\] \[-\sum_{s^{\prime}\in\mathcal{X},a^{\prime}}P_{\pi^{\prime},s,a}(S_ {T-1}=s^{\prime},A_{T-1}=a^{\prime})P(S_{T}|s^{\prime},a^{\prime})\gamma^{T}v_ {\pi^{\prime}}(S_{T})\] \[=\sum_{s^{\prime}\in\mathcal{X},a^{\prime}}P_{\pi,s,a}(S_{T-1}=s^ {\prime},A_{T-1}=a^{\prime})P(S_{T}|s^{\prime},a^{\prime})\gamma^{T}\left(v_{ \pi}(S_{T})-v_{\pi^{\prime}}(S_{T})\right)\] \[\leq\frac{1}{1-\gamma}\sum_{s^{\prime}\in\mathcal{X},a^{\prime}} P_{\pi,s,a}(S_{T-1}=s^{\prime},A_{T-1}=a^{\prime})P(S_{T}|s^{\prime},a^{ \prime})\gamma^{T}=\frac{1}{1-\gamma}\mathbb{E}_{\pi,s,a}[\gamma^{T}]\] \[=\frac{1}{1-\gamma}\sum_{t=1}^{\infty}\gamma^{t}P_{\pi,s,a}(T=t) \leq\frac{1}{1-\gamma}\sum_{t=1}^{H-1}P_{\pi,s,a}(T=t)\gamma^{0}+\frac{1}{1- \gamma}\sum_{t=H}^{\infty}P_{\pi,s,a}(T\geq H)\gamma^{H}\] \[\leq\frac{1}{1-\gamma}P_{\pi,s,a}(1\leq T<H)+\frac{\gamma^{H}}{1- \gamma}.\]

Recall \(P_{\pi,s,a}(1\leq T<H)=\sum_{t=1}^{H-1}\sum_{s^{\prime}\in\mathcal{X},a^{ \prime}\in\mathcal{A}}P_{\pi,s,a}(S_{t-1}=s^{\prime},A_{t-1}=a^{\prime})P(S_{ T}|s^{\prime},a^{\prime})\), and recall \(S_{0}=s,A_{0}=a\), then by the law of total probability, we have

\[P_{\pi,s,a}(S_{t}=s^{\prime},A_{t}=a^{\prime})\] \[=\sum_{\begin{subarray}{c}s_{1},\dots,s_{t-1},\\ a_{1},\dots,a_{t-1}\end{subarray}}\Pi_{i=0}^{t-1}P(S_{i+1}=s_{i+1}|S_{i}=s_{i},A _{i}=a_{i})\left(\Pi_{i=1}^{t}\pi(A_{i}=a_{i}|S_{i}=s_{i})\right)\] \[=\sum_{\begin{subarray}{c}s_{1},\dots,s_{t-1},\\ a_{1},\dots,a_{t-1}\end{subarray}}\Pi_{i=0}^{t-1}P(S_{i+1}=s_{i+1}|S_{i}=s_{i},A _{i}=a_{i})\Pi_{i=1}^{t}\frac{\pi(A_{i}=a_{i}|S_{i}=s_{i})}{\mu(A_{i}=a_{i}|S_{i }=s_{i})}\mu(A_{i}=a_{i}|S_{i}=s_{i})\] \[\leq(1+\rho_{0})\sum_{\begin{subarray}{c}s_{1},\dots,s_{t-1},\\ a_{1},\dots,a_{t-1}\end{subarray}}\Pi_{i=0}^{t-1}P(S_{i+1}=s_{i+1}|S_{i}=s_{i},A _{i}=a_{i})\Pi_{i=1}^{t}\mu(A_{i}=a_{i}|S_{i}=s_{i})\] (11) \[=(1+\rho_{0})P_{\mu,s,a}(S_{t}=s^{\prime},A_{t}=a^{\prime}).\]

To get eq. (11), we use lemma 3 and that \(1\leq t\leq H-1\).

Altogether, we have

\[|q_{\pi}(s,a)-q_{\pi^{\prime}}(s,a)|\] \[\leq\frac{1}{1-\gamma}\sum_{t=1}^{H-1}\sum_{s^{\prime}\in\mathcal{X },a^{\prime}\in\mathcal{A}}(1+\rho_{0})P_{\mu,s,a}(S_{t-1}=s^{\prime},A_{t-1}= a^{\prime})P(S_{T}|s^{\prime},a^{\prime})\] \[+\frac{\gamma^{H}}{1-\gamma}\] \[\leq\frac{1+\rho_{0}}{1-\gamma}P_{\mu,s,a}(1\leq T<H)+\frac{ \epsilon}{4}.\]

Now, we bound \(P_{\mu,s,a}(1\leq T<H)\). For each \((s,a)\in\mathcal{X}\), in the \(i\)th rollouts, let \(I_{i}(s,a)\) be an indicator function, where it takes the value \(1\) when the event that \(S_{T}\not\in\mathcal{X}\) occurs during \(1\leq T<H\). Then \(\mathbb{E}_{\mu,s,a}[I_{i}(s,a)]=P_{\mu,s,a}(1\leq T<H)\). By another Hoeffding's inequality,

\[\mathbb{P}\left(|\mathbb{E}_{\mu,s,a}[I_{i}(s,a)]-\frac{1}{n} \sum_{i=1}^{n}I_{i}(s,a)|>\frac{\epsilon(1-\gamma)}{4(1+\rho_{0})}\right) \leq 2\exp\left(-\frac{2n(\epsilon(1-\gamma)/4(1+\rho_{0}))^{2}}{ \left(1\right)^{2}}\right)\] \[=2\exp\left(-\frac{2n\epsilon^{2}}{16\left(\frac{1+\rho_{0}}{1- \gamma}\right)^{2}}\right)=\delta^{\prime}.\]

Then, with probability \(1-\delta^{\prime}/2\),

\[|\mathbb{E}_{\mu,s,a}[I_{i}(s,a)]-\frac{1}{n}\sum_{i=1}^{n}I_{i}( s,a)|\leq\frac{\epsilon(1-\gamma)}{4(1+\rho_{0})},\] (12)

When Gather-data subroutine returns, all indicators \(I_{i}(s,a)=0\) for all \((s,a)\in\mathcal{X}\) and \(i\in[n]\), then we have

\[P_{\mu,s,a}(1\leq T<H)\leq\frac{\epsilon(1-\gamma)}{4(1+\rho_{0} )}.\] (13)

Putting everything together, we have the result. 

### The LSE subroutine

```
1:\(\mathcal{C},D,\pi_{k},\pi^{\prime}_{k}\)
2:for\(s,a\in\mathcal{C}\)do
3:for every \(\tau^{i}_{s,a}\in D[(s,a)]\) for every \(i\in[n]\)do
4: extract \(\{S^{i}_{0},A^{i}_{0},R^{i}_{1},C^{i}_{1},S^{i}_{1},A^{i}_{1}\cdots S^{i}_{H},A^{i}_{H}\}\) from \(\tau^{i}_{s,a}\)
5: compute \(G^{i}_{r}(s,a)\leftarrow\sum_{h=0}^{H-1}\gamma^{h}R^{i}_{h+1}\); \(G^{i}_{c}(s,a)\leftarrow\sum_{h=0}^{H-1}\gamma^{h}C^{i}_{h+1}\)
6: compute \(\rho^{i}(s,a)\leftarrow\Pi_{h=1}^{H-1}\frac{\pi_{h}(A^{i}_{h}|S^{i}_{h})}{ \pi^{i}_{k}(A^{i}_{h}|S^{i}_{h})}\)
7:\(\bar{q}^{r}(s,a)\leftarrow\frac{1}{n}\sum_{i=1}^{n}\rho^{i}(s,a)G^{i}_{r}(s,a) ;\ \bar{q}^{c}(s,a)\leftarrow\frac{1}{n}\sum_{i=1}^{n}\rho^{i}(s,a)G^{i}_{c}(s,a)\)
8:\(w^{r}\leftarrow\left(\Phi^{\top}_{\mathcal{C}}\Phi_{\mathcal{C}}+\alpha I \right)^{-1}\Phi^{\top}_{\mathcal{C}}\bar{q}^{r}\); \(w^{c}\leftarrow\left(\Phi^{\top}_{\mathcal{C}}\Phi_{\mathcal{C}}+\alpha I \right)^{-1}\Phi^{\top}_{\mathcal{C}}\bar{q}^{c}\)
9:\(Q^{r}(s,a)\leftarrow\left\langle w^{r},\phi(s,a)\right\rangle\), \(Q^{c}(s,a)\leftarrow\left\langle w^{c},\phi(s,a)\right\rangle\) for all \(s,a\)return\(Q^{r},Q^{c}\) ```

**Algorithm 4** LSE

Given a core set \(\mathcal{C}\), a set of trajectories, a behaviour policy \(\mu\), a target policy \(\pi\), the LSE subroutine (algorithm 4) returns a least-square estimate \(Q\) of \(q_{\pi}\).

If the core set \(\mathcal{C}\) is empty, we define \(Q(\cdot,\cdot)\) to be zero. Then, for a target accuracy \(\epsilon>0\) and a uniform misspecification error \(\omega\) defined in assumption 1, we have a bound on the accuracy of \(\bar{q}\) with respect to \(q_{\pi}\) as given by the next lemma.

[MISSING_PAGE_FAIL:18]

### The accuracy of least-square estimates

Given a core set \(\mathcal{C}\) and a target policy \(\pi\), for any \(s\in\mathrm{Cov}(\mathcal{C}),a\in\mathcal{A}\), the feature vector \(\phi(s,a)\) satisfies \(\|\phi(s,a)\|_{V(\mathcal{C},\alpha)^{-1}}\leq 1\). Then, by lemma 5, we have \(|Q(s,a)-q_{\pi}(s,a)|=O(\omega+\epsilon)\) for any \(s\in\mathrm{Cov}(\mathcal{C})\). In this section, we verify whether this accuracy is maintained throughout the execution of our algorithm.

We note that policy improvements can only occur during a running phase \(\ell=l\). When all \((s,a)\) pairs in \(\mathcal{C}_{l}\) have their placeholder value \(\bot\) replaced by trajectories, algorithm 2 executes line 17 to line 27. During each iteration from \(k_{\ell}\) to \(k_{\ell+1}-1\), the LSE subroutine is executed. The accuracy of \(\bar{q}_{k}\) is used to bound the estimation error in lemma 5. Therefore, we will first verify that the accuracy guarantee of \(\bar{q}_{k}(s,a)\) used in lemma 4 is indeed satisfied by the main algorithm and maintained throughout its execution.

Once a state-action pair is added to a core set, it remains in that core set for the duration of the algorithm. This means that any core set \(\mathcal{C}_{l}\) for \(l\in\{0,\ldots,L+1\}\) can grow in size over time. When a core set \(\mathcal{C}_{l}\) is extended during a running phase \(\ell=l-1\), the least-square estimate will need be updated based on the newly extended \(\mathcal{C}_{l}\) in running phase \(\ell=l\), which contains newly discovered features. However, the policy is update only for states that are newly covered by the extended core set \(\mathcal{C}_{l}\) using the newly improved estimates. Meanwhile, the policy for other states that have already been updated by a prior softmax update remain unchanged. Note that after line 27 of algorithm 2 is run, the next phase's core set \(\mathcal{C}_{l+1}\) will be set to \(\mathcal{C}_{l}\), which means that any state that was once newly covered by \(\mathcal{C}_{l}\) is no longer considered newly covered. Consequently, the policy for those states will remain unchanged throughout the rest of the algorithm's execution. By updating the policies accordingly, we arrive at the following lemma, which will be crucial in proving the accuracy guarantee of the least-squares estimators.

**Lemma 6**: _For any \(l\in\{0,\ldots,L\}\), let \(\mathcal{C}_{l}^{\text{past}}\) be any past version of \(\mathcal{C}_{l}\) and let \(\pi_{k}^{\text{past}}\) for \(k=k_{l},\cdots,k_{l+1}-1\) be the corresponding policies associated with \(\mathcal{C}_{l}^{\text{past}}\). If at any later point during the execution of the algorithm, \(\pi_{k}\) is updated again, then it holds that_

\[\pi_{k}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})}\subseteq\Pi_{\pi_{k}^{ \text{past}},\mathrm{Cov}(\mathcal{C}_{l}^{\text{past}})}.\]

_Additionally, for any states that have been covered by \(\mathcal{C}_{l}^{\text{past}}\), it will continue to be covered by \(\mathcal{C}_{l}\) throughout the execution of the algorithm. In other words, \(s\in\mathrm{Cov}(\mathcal{C}_{l}^{\text{past}})\subseteq\mathrm{Cov}(\mathcal{ C}_{l})\)._

Proof: The proof follows similar logic to Lemma 4.5 of Weisz et al. (2022). Recall for matrices \(A,B\), \(A\geq B\) means that \(A-B\) is positive semidefinite.

Because \(\mathcal{C}_{l}\supseteq\mathcal{C}_{l}^{\text{past}}\), there may be more rows added to \(\Phi_{\mathcal{C}_{l}}\) than \(\Phi_{\mathcal{C}_{l}^{\text{past}}}\). Recall \(V(\mathcal{C}_{l},\alpha)=\Phi_{\mathcal{C}_{l}}^{\top}\Phi_{\mathcal{C}_{l}}+\alpha I\), and likewise for \(V(\mathcal{C}_{l}^{\text{past}})\) except \(\Phi_{\mathcal{C}_{l}}\) is replaced by \(\mathcal{C}_{l}^{\text{past}}\). Note that both \(V(\mathcal{C}_{l},\alpha),V(\mathcal{C}_{l}^{\text{past}},\alpha)\) are dimension \(d\times d\). Let \(\mathcal{C}_{l}^{\prime}\) contain a set of state-action pairs that are in \(\mathcal{C}_{l}\setminus\mathcal{C}_{l}^{\text{past}}\). Then, \(V(\mathcal{C}_{l},\alpha)-V(\mathcal{C}_{l}^{\text{past}},\alpha)=\sum_{(s,a) \in\mathcal{C}_{l}^{\prime}}\phi(s,a)\phi(s,a)^{\top}\). Since any rank-1 matrices is positive semidefinite and their sum is also positive semidefinite, it follows that \(V(\mathcal{C}_{l},\alpha)\geq V(\mathcal{C}_{l}^{\text{past}},\alpha)\).

Since \(V(\mathcal{C}_{l},\alpha)\) and \(V(\mathcal{C}^{\text{past}},\alpha)\) are symmetric positive definite matrices, then it follows that \(V(\mathcal{C}_{l},\alpha)^{-1}\leq V(\mathcal{C}_{l}^{\text{past}},\alpha)^{-1}\). From this, we see that for any \(x\in\mathbb{R}^{d}\), \(\|x\|_{V(\mathcal{C}_{l},\alpha)^{-1}}\leq\|x\|_{V(\mathcal{C}_{l}^{\text{past} },\alpha)^{-1}}\). Then, it follows that \(\mathrm{ActionCov}(\mathcal{C}_{l}^{\text{past}})\subseteq\mathrm{ActionCov}( \mathcal{C}_{l})\), and likewise \(\mathrm{Cov}(\mathcal{C}_{l}^{\text{past}})\subseteq\mathrm{Cov}(\mathcal{C}_{l})\). Therefore, for an \(s\in\mathrm{Cov}(\mathcal{C}_{l}^{\text{past}})\), the same state \(s\in\mathrm{Cov}(\mathcal{C}_{l})\), and the second result follows. Finally, by the definition of the extended policy set definition 1, \(\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})}\subseteq\Pi_{\pi_{k}^{\text{past}}, \mathrm{Cov}(\mathcal{C}_{l}^{\text{past}})}\). \(\blacksquare\)

**Lemma 7**: _For any \(l\in\{0,\ldots,L\}\) and any \((s,a)\in\mathcal{C}_{l}\), the importance-weighted \(\bar{q}_{k}(s,a)\) computed in the LSE subroutine during the running phase \(\ell=l\) is an unbiased estimator of the expected discounted reward:_

\[E_{\pi_{k}^{\prime},s,a}\left[\sum_{h=0}^{H-1}\gamma^{h}R_{h+1}\right]\quad\text {for }\pi_{k}^{\prime}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})},\]

_for iterations \(k=k_{l},\cdots,k_{l+1}-1\) associated with this phase._Proof: For the algorithm to execute the LSE subroutine, every \((s,a)\in\mathcal{C}_{l}\) must have its placeholder value \(\bot\) in \(D_{l}[(s,a)]\) replaced with trajectories. Trajectories are stored in \(D_{l}\) only when the Gather-data subroutine returns "discovered is False" during the running phase \(\ell=l\). This ensures that every state within these trajectories has passed the uncertainty test, thereby ensuring that all such states are in the cover of \(\mathcal{C}_{l}\) and will remain in the cover of \(\mathcal{C}_{l}\) for the duration of the algorithm, as established in lemma 6. Additionally, once trajectories for a \((s,a)\in\mathcal{C}_{l}\) are stored in \(D_{l}\), they remain unchanged throughout the algorithm's execution. We aim to show that \(\bar{q}_{k}(s,a)\) computed using \(D_{l}[(s,a)]\), is an unbiased estimate of the stated quantity for all iterations associated with the phase. This will be established through the following inductive arguments.

**Base case:** for a \((s,a)\in\mathcal{C}_{l}\), the trajectories are generated and stored in \(D_{l}[(s,a)]\) for the first time during the running phase \(\ell=l\).

We let \(\tau^{i}_{s,a}\) denote the \(i\)-th trajectory \((S^{i}_{0}=s,A^{i}_{0}=a,R^{i}_{1},S^{i}_{1},\ldots,S^{i}_{H-1},A^{i}_{H-1},R^ {i}_{H})\) generated by \(\pi_{k_{l}}\) interacting with the simulator, and there are \(n\) such trajectories stored in \(D_{l}[(s,a)]\). Then, for all \(k=k_{l},\cdots,k_{l+1}-1\), the return

\[\bar{q}_{k}(s,a)=\frac{1}{n}\sum_{i=1}^{n}\Pi_{h=1}^{H-1}\frac{\pi_{k}(A^{i}_{ h}|S^{i}_{h})}{\pi_{k_{l}}(A^{i}_{h}|S^{i}_{h})}G(\tau^{i}_{s,a}),\]

where \(G(\tau^{i}_{s,a})=\sum_{h=0}^{H-1}\gamma^{h}R_{h+1}\).

The behavior policy \(\pi_{k_{l}}\) is updated in a previous loop through the algorithm when \(\ell=l-1\). For iterations, starting with \(k=k_{l}+1,\ldots,k_{l+1}-1\), the policy \(\pi_{k}\) is updated in iteration \(k-1\). Thus, the most recent policy \(\pi_{k}\) and the behaviour policy \(\pi_{k_{l}}\) are available for the computation of the importance sampling ratio: \(\rho_{k}(\tau^{i}_{s,a})=\Pi_{h=1}^{H-1}\frac{\pi_{k}(A^{i}_{h}|S^{i}_{h})}{ \pi_{k_{l}}(A^{i}_{h}|S^{i}_{h})}\). We show that the importance weighted return \(\rho_{k}(\tau^{i}_{s,a})G(\tau^{i}_{s,a})\) is an unbiased estimate of \(\mathbb{E}_{\pi_{k},s,a}[G(\tau^{i}_{s,a})]\):

\[\mathbb{E}_{\pi_{k_{l}},s,a}\left[\rho_{k}(\tau^{i}_{s,a})\sum_{h =0}^{H-1}\gamma^{h}R^{i}_{h+1}\right]\] \[=\mathbb{E}_{\pi_{k_{l}},s,a}\left[\frac{\delta(s,a)P(S_{1}|S_{0} =s,A_{0}=a)\pi_{k}(A_{1}|S_{1})\ldots\pi_{k}(A_{H-1}|S_{H-1})}{\delta(s,a)P(S_{ 1}|S_{0}=s,A_{0}=a)\pi_{k_{l}}(A_{1}|S_{1})\ldots\pi_{k_{l}}(A_{H-1}|S_{H-1})} \sum_{h=0}^{H-1}\gamma^{h}R^{i}_{h+1}\right]\] \[=\mathbb{E}_{\pi_{k},s,a}\left[\sum_{h=0}^{H-1}\gamma^{h}R^{i}_{h +1}\right],\]

where \(\delta(s,a)\) is the dirac-delta function. Note, for the on-policy iteration \(k=k_{l}\), the importance sampling ratio \(\rho_{k}(\tau^{i}_{s,a})=1\), and the result is trivially satisfied.

Finally, since all the states in the trajectories are in \(\mathrm{Cov}(\mathcal{C}_{l})\), it follows that any policy \(\pi^{\prime}_{k}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})}\) produces the same \(\rho_{k}(\tau^{i}_{s,a})\). The return \(\rho_{k}(\tau^{i}_{s,a})G(\tau^{i}_{s,a})\) is an unbiased estimate of \(E_{\pi^{\prime}_{k},s,a}[G(\tau^{i}_{s,a}]\) for all \(\pi^{\prime}_{k}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})}\). This is true for all \(i=1,\ldots,n\). Consequently, \(\bar{q}_{k}(s,a)\) is an unbiased estimate of \(\mathbb{E}_{\pi^{\prime}_{k},s,a}[\sum_{h=0}^{H-1}R_{h+1}]\) for all for all \(\pi^{\prime}_{k}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})}\).

The requirement that the importance weighted \(\bar{q}_{k}\) be unbiased for all policies \(\pi^{\prime}_{k}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})}\) is important. This ensures that if \(\pi_{k}\) is to be updated in a future loop through the algorithm again, the estimates remain unbiased and unchanged.

**Previously generated trajectories:** for any \((s,a)\in\mathcal{C}_{l}\), the trajectories have already been generated and stored in \(D_{l}[(s,a)]\) during a previous loop of the algorithm when \(\ell=l\).

Let \(D_{l}^{\text{past}}[(s,a)]\) denote a past snapshot of the data stored for a \((s,a)\in\mathcal{C}_{l}^{\text{past}}\). Let \(\pi_{k}^{\text{past}}\) for \(k=k_{l},\ldots k_{l+1}-1\) denote the policies associated with \(\mathcal{C}_{l}^{\text{past}}\) after line 27 has been run. Finally, let \(\tau^{i,\text{past}}_{s,a}\) denote the \(i\)-th trajectory stored in \(D_{l}^{\text{past}}[(s,a)]\).

Assume the importance weighted return \(\rho_{k}(\tau^{i,\text{past}}_{s,a})G(\tau^{i,\text{past}}_{s,a})\) is an unbiased estimate of \(\mathbb{E}_{\tilde{\pi}_{k},s,a}[G(\tau^{i,\text{past}}_{s,a})]\) for all \(\tilde{\pi}_{k}\in\Pi_{\pi_{k}^{\text{past}},\mathrm{Cov}(\mathcal{C}_{l}^{ \text{past}})}\) for \(k=k_{l},\ldots,k_{l+1}-1\). When the algorithm executes a loop with \(\ell=l\) again, by lemma 6, the most recent policy \(\pi_{k}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})}\subseteq\Pi_{\pi_{k}^{ \text{past}},\mathrm{Cov}(\mathcal{C}_{l}^{\text{past}})}\), then \(\rho_{k}(\tau^{i,\text{past}}_{s,a})G(\tau^{i,\text{past}}_{s,a})\) is also an unbiased estimate of \(\mathbb{E}_{\pi_{k}^{\prime},s,a}[G(\tau^{i,\text{past}}_{s,a})]\) for all \(\pi^{\prime}_{k}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})}\).

Once \(D_{l}^{\text{past}}[(s,a)]\) is populated with trajectories, \(D_{l}^{\text{past}}[(s,a)]\) remain unchanged throughout the execution of the algorithm. Therefore, \(G(\tau_{s,a}^{i})=G(\tau_{s,a}^{i,\text{past}})\). Since all the states in the trajectories are in \(\operatorname{Cov}(\mathcal{C}_{l}^{\text{past}})\subseteq\operatorname{Cov}( \mathcal{C}_{l})\) by lemma 6, any policy \(\tilde{\pi}_{k}\in\Pi_{\pi_{k}^{\text{past}},\operatorname{Cov}(\mathcal{C}_{ l}^{\text{past}})}\) produces the same \(\rho_{k}(\tau_{s,a}^{i})\). Thus, we have \(\rho_{k}(\tau_{s,a}^{i})G(\tau_{s,a}^{i})=\rho_{k}(\tau_{s,a}^{i,\text{past}}) G(\tau_{s,a}^{i,\text{past}})\). It follows that \(\rho_{k}(\tau_{s,a}^{i})G(\tau_{s,a}^{i})\) is an unbiased estimate of \(\mathbb{E}_{\pi_{k}^{i},s,a}[G(\tau_{s,a}^{i})]\) for all \(\pi_{k}^{\prime}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{l})}\). This is true for all \(i=1,\ldots,n\). Consequently, \(\bar{q}_{k}(s,a)\) is an unbiased estimate of \(\mathbb{E}_{\pi_{k}^{i},s,a}[\sum_{h=0}^{H-1}R_{h+1}]\) for all \(\pi_{k}^{\prime}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{l})}\). 

**Lemma 8**: _Whenever the LSE-subroutine of Confident-NPG is executed during a running phase \(\ell=l\) for \(l\in\{0,\ldots,L\}\), the behaviour policy \(\pi_{k_{\ell}}(\cdot|s)\) and target policy \(\pi_{k}(\cdot|s)\) for \(k=k_{\ell},\ldots,k_{\ell+1}-1\) satisfy eq. (9) for any \(s\in\operatorname{Cov}(\mathcal{C}_{\ell})\)._

Proof: Recall that the behavior policy \(\pi_{k_{l}}\) is updated during a previous loop of the algorithm when \(\ell=l-1\). By the time the LSE subroutine is executed, \(\pi_{k_{\ell}}\) will be the policy that generated the data. Therefore, for the on-policy iteration where \(k=k_{\ell}\), eq. (9) is trivially satisfied.

For subsequent iterations, starting with \(k=k_{\ell}+1,\ldots,k_{\ell+1}-1\), for any \(s\in\operatorname{Cov}(\mathcal{C}_{\ell})\), the policy \(\pi_{k}(\cdot|s)\) will have either performed a softmax update for the first time or remain unchanged from a previous softmax update based on an earlier least-square estimate. Either way, for any \(s\in\operatorname{Cov}(\mathcal{C}_{\ell})\), the target policy \(\pi_{k}\) and behaviour policy \(\pi_{k_{\ell}}\) relate to each other in the form of \(\pi_{k}(\cdot|s)\propto\pi_{k_{\ell}}(\cdot|s)\exp(\eta_{1}\sum_{t=k_{\ell}}^ {k-1}\bar{Q}_{t}(s,a))\). Since \(\bar{Q}_{t}(s,a)\in[0,\frac{1}{1-\gamma}]\) for any \(t=k_{\ell},\ldots,k-1\), then it follows that

\[0\leq\eta_{1}\sum_{t=k_{\ell}}^{k-1}\bar{Q}_{t}(s,a)\leq\eta_{1}(k-k_{\ell}) \frac{1}{1-\gamma}\leq\frac{\eta_{1}((\lfloor m\rfloor+1)-1)}{1-\gamma}.\]

By choosing \(\eta_{1}=(1-\gamma)\sqrt{\frac{2\ln(\lfloor\mathcal{A}\rfloor)}{K}},H=\frac{ \ln(4/\epsilon(1-\gamma))}{1-\gamma},m=\frac{\ln(1+\rho_{0})}{2H\epsilon(1- \gamma)^{2}}\), and \(K=\frac{2\ln(\mathcal{A})}{(1-\gamma)^{4}\epsilon^{2}}\), we have \(\frac{\eta_{1}((\lfloor m\rfloor+1)-1)}{1-\gamma}\leq\frac{\eta_{1}m}{1- \gamma}=\frac{\ln(1+\rho_{0})}{2H}\). Then it follows that eq. (9) is satisfied. 

**Lemma 9**: _For any \(l\in\{0,\ldots,L\}\) and any \((s,a)\in\mathcal{C}_{l}\), the importance-weighted \(\bar{q}_{k}(s,a)\) computed in the LSE subroutine during the running phase \(\ell=l\) satisfies the following with probability \(1-\delta^{\prime}\),_

\[|\bar{q}_{k}(s,a)-q_{\pi_{k}^{\prime}}(s,a)|\leq\epsilon\quad\text{for }\pi_{k}^{ \prime}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{l})}\] (16)

_for all iterations \(k=k_{l},\ldots,k_{l+1}-1\) associated with this phase._

Proof: We apply lemma 4 to each \((s,a)\in\mathcal{C}_{l}\). To ensure the applicability of the lemma, we verify its two conditions: 1) the policies satisfy eq. (9) and 2) the estimate are unbiased.

We note that when the LSE-subroutine is executed during a running phase with \(\ell=l\), the Gather-data subroutine has already completed, and the algorithm trajectories for each state-action pair \((s,a)\in\mathcal{C}_{l}\) are stored in \(D_{l}[(s,a)]\). For any trajectory to be stored in \(D_{l}\), this means that every state within the trajectories has passed the uncertainty test, ensuring that all such states are in the cover of \(\mathcal{C}_{l}\). By lemma 6, these states will continue to be covered by \(\mathcal{C}_{l}\) throughout the execution of the algorithm. The implication of this is that all the states in a trajectory of \(D_{l}[(s,a)]\) satisfy eq. (9) by lemma 8.

Second, by lemma 7, the importance weighted return \(\bar{q}_{k}(s,a)\) is unbiased estimate of any \(\mathbb{E}_{\pi_{k}^{\prime},s,a}\left[\sum_{h=0}^{H-1}\gamma^{h}R_{h+1}\right]\) for all \(\pi_{k}^{\prime}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{l})}\). Altogether, by lemma 4, we can ensure eq. (16) holds.

Consider a past loop through the algorithm with \(\ell=l\), let \(\mathcal{C}_{l}^{\text{past}}\) be the core set and \(\pi_{k}^{\text{past}}\) for \(k=k_{l},\ldots,k_{l+1}-1\) be the policies associated with \(\mathcal{C}_{l}^{\text{past}}\) after line 27 has been run. If eq. (16) holds for all \(\tilde{\pi}_{k}\in\Pi_{\pi_{k}^{\text{past}},\operatorname{Cov}(\mathcal{C}_{l}^ {\text{past}})}\), then the accuracy of \(\bar{q}_{k}\) will continue to hold for any future update of \(\pi_{k}\) because \(\pi_{k}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{l})}\)\(\Pi_{\pi_{k}^{\text{past}},\operatorname{Cov}(\mathcal{C}_{l}^{\text{past}})}\) by lemma 6. 

**Lemma 10** (Weisz et al. (2022)): _At any time during the execution of the main algorithm, for all \(l\in\{0,\ldots,L\},\) the size of each \(\mathcal{C}_{l}\) is bounded:_

\[|\mathcal{C}_{l}|\leq 4d\ln\left(1+\frac{4}{\alpha}\right)=\bar{d}=\tilde{O}(d),\]_where the \(\alpha\) is the smallest eigenvalue of \(V(\mathcal{C},\alpha)\) and \(N\) is the radius of the Euclidean ball containing all the feature vectors._

**Lemma 11**: _Whenever LSE subroutine of Confident-NPG is executed during a running phase \(\ell=l\) for \(l\in\{0,\dots,L\}\), the least-square estimate \(\tilde{Q}_{k}(s,a)\) satisfies the following condition for all iterations \(k=k_{\ell},\cdots,k_{\ell+1}-1\) associated with this phase and for all \(s\in\mathrm{Cov}(\mathcal{C}_{\ell})\) and \(a\in\mathcal{A}\),_

\[|\tilde{Q}_{k}(s,a)-q_{\pi_{k}^{\prime}}(s,a)|\leq\epsilon^{\prime}\quad\text{ for all }\pi_{k}^{\prime}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{\ell})},\] (17)

_where \(\epsilon^{\prime}=\omega+\sqrt{\alpha}B+(\omega+\epsilon)\sqrt{\tilde{d}}\)._

Proof: We prove the result by induction similar to Lemma F.1 of Weisz et al. (2022). We let \(\mathcal{C}_{l}^{-},\pi_{k}^{-},\tilde{Q}_{k}^{-}\) to denote the value of variable \(\mathcal{C}_{l},\pi_{k},\tilde{Q}_{k}\) at the time when line 17 to line 27 were most recently executed with \(\ell=l\) in a previous loop through the algorithm. If such time does not exist, we let their values be the initialization values. Only after the execution of line line 27 will \(\mathcal{C}_{l}^{-}\) change and as well as \(\mathcal{C}_{l+1}\), and this is the only time that \(\mathcal{C}_{l+1}\) can be changed. Therefore, at the start of a new loop, we see that \(\mathcal{C}_{l+1}=\mathcal{C}_{l}^{-}\). This also holds at the initialization of the algorithm, we conclude that at the start of each loop, \(\mathrm{Cov}(\mathcal{C}_{l+1})=\mathrm{Cov}(\mathcal{C}_{l}^{-})\).

At initialization, \(\tilde{Q}_{k}=0\) for any \(k\in\{0,\dots,K\}\) and \(C_{l}=()\) for all \(l\in\{0,\dots,L\}\). By applying lemma 5 (Lemma 4.3 of Weisz et al. (2022)), for any \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[|\tilde{Q}_{k}(s,a)-q_{\pi_{k}^{\prime}}(s,a)|\leq\omega+\sqrt{\alpha}B\leq \epsilon^{\prime},\]

which satisfies eq. (17).

Next, let us consider the start of a loop after \(\ell=l\) is set and assume that the inductive hypothesis holds for the previous time line 17 to line 27 were executed with the same value of \(\ell=l\). For any \(s\in\mathrm{Cov}(\mathcal{C}_{l-1}^{-})\), policy \(\pi_{k_{i}}(\cdot|s)\) would have already been set in a previous loop with value \(l-1\) and remains unchanged in the current loop. By lemma 9, the condition of lemma 5 holds, then by lemma 5, we have for any \(s\in\mathrm{Cov}(\mathcal{C}_{l-1}^{-})\),

\[|\tilde{Q}_{k_{l}}^{-}(s,\cdot)-q_{\pi_{k_{l}^{\prime}}}(s,\cdot)|\leq\omega+ \sqrt{\alpha}B+(\omega+\epsilon)\sqrt{\tilde{d}}\quad\text{for }\pi_{k_{l}^{\prime}}\in\Pi_{\pi_{k_{l}^{ \prime}},\mathrm{Cov}(\mathcal{C}_{l-1}^{-})},\]

where \(\left\|\phi(s,\cdot)\right\|_{V(\mathcal{C}_{l-1}^{-},\alpha)^{-1}}\leq 1\) because \(s\in\mathrm{Cov}(\mathcal{C}_{l-1}^{-})\) and \(|C_{l-1}^{-}|\leq\tilde{d}\) by lemma 10. Recall by definition, \(\tilde{Q}_{k_{l}}=\tilde{Q}_{k_{l}}^{-},\pi_{k_{l}}=\pi_{k_{l}^{\prime}}\), \(C_{l}=C_{l-1}^{-}\), and \(\mathrm{Cov}(\mathcal{C}_{l})=\mathrm{Cov}(\mathcal{C}_{l-1}^{-})\). It follows that for any \(s\in\mathrm{Cov}(\mathcal{C}_{l}),|\tilde{Q}_{k_{l}}(s,\cdot)-q_{\pi_{k_{l}^{ \prime}}}(s,\cdot)|\leq\epsilon^{\prime}\) for \(\pi_{k_{l}^{\prime}}\in\Pi_{\pi_{k_{l}},\mathrm{Cov}(\mathcal{C}_{l})}\).

For any \(s\) that is already covered by \(\mathcal{C}_{l}\) (i.e., \(s\in\mathrm{Cov}(\mathcal{C}_{l}^{-})\)), and for any off-policy iteration \(k=k_{l}+1,\cdots,k_{l+1}-1\), \(\tilde{Q}_{k}(s,\cdot)=\tilde{Q}_{k}^{-}(s,\cdot)\). Additionally, the policy \(\pi_{k}(\cdot|s)\) would already have been set in a previous running loop with the same value of \(l\) and remains unchanged in the current loop. For \(s\in\mathrm{Cov}(\mathcal{C}_{l}^{-})\), by lemma 9, the condition of lemma 5 holds, and then by lemma 5,

\[|\tilde{Q}_{k}^{-}(s,\cdot)-q_{\pi_{k}^{\prime}}(s,\cdot)|\leq\omega+\sqrt{ \alpha}B+(\omega+\epsilon)\sqrt{\tilde{d}}\quad\text{for }\pi_{k^{\prime}}\in\Pi_{\pi_{k^{ -}_{k}},\mathrm{Cov}(\mathcal{C}_{l}^{-})},\]

where \(\left\|\phi(s,\cdot)\right\|_{V(\mathcal{C}_{l}^{-},\alpha)^{-1}}\leq 1\) because \(s\in\mathrm{Cov}(\mathcal{C}_{l}^{-})\) and \(|\mathcal{C}_{l}^{-}|\leq\sqrt{\tilde{d}}\) by lemma 10. By lemma 6, \(\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})}\subseteq\Pi_{\pi_{k^{\prime}}, \mathrm{Cov}(\mathcal{C}_{l}^{-})}\). By definition, \(\tilde{Q}_{k}(s,\cdot)=\tilde{Q}_{k}^{-}(s,\cdot)\) for \(s\in\mathrm{Cov}(\mathcal{C}_{l+1})=\mathrm{Cov}(\mathcal{C}_{l}^{-})\), \(|\tilde{Q}_{k}(s,\cdot)-q_{\pi_{k}^{\prime}}(s,\cdot)|\leq\epsilon^{\prime}\) for any \(\pi_{k}^{\prime}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l+1})}\).

Finally, for any \(s\) that is newly covered by \(\mathcal{C}_{l}\) (i.e., \(s\not\in\mathrm{Cov}(\mathcal{C}_{l+1})\)), and for all \(k=k_{l},\dots,k_{l+1}-1\), \(\tilde{Q}_{k}(s,\cdot)=Q_{k}(s,\cdot)\). By lemma 9, the condition of lemma 5 holds, and then by lemma 5, we have

\[|Q_{k}(s,\cdot)-q_{\pi_{k}^{\prime}}(s,\cdot)|\leq\omega+\sqrt{\alpha}B+( \omega+\epsilon)\sqrt{\tilde{d}}\quad\text{for }\pi_{k^{\prime}}\in\Pi_{\pi_{k},\mathrm{Cov}( \mathcal{C}_{l})},\]

where \(\left\|\phi(s,\cdot)\right\|_{V(\mathcal{C}_{l},\alpha)^{-1}}\leq 1\) and \(|\mathcal{C}_{l}|\leq\tilde{d}\) by lemma 10. 

**Lemma 12**: _For any \(\delta^{\prime}\in(0,1]\), a target accuracy \(\epsilon>0\), misspecification error \(\omega\geq 0\), and initial state \(s_{0}\), with probability at least \(1-\delta^{\prime}\), the value difference between any \(\pi\in\Pi_{\text{rand}}\) and the mixture policy \(\bar{\pi}_{K}\) returned by Confident-NPG has the following bound:_

\[v_{\pi}(s_{0})-v_{\bar{\pi}_{K}}(s_{0})\leq\frac{4\epsilon^{\prime}}{1-\gamma}+ \frac{1}{K(1-\gamma)}\sum_{k=0}^{K-1}\mathbb{E}_{s^{\prime}\sim d_{\pi}(s_{0}), s^{\prime}\in\mathrm{Cov}(\mathcal{C}_{0})}\left[\langle\tilde{Q}_{k}(s^{\prime}, \cdot),\pi(\cdot|s^{\prime})-\pi_{k}(\cdot|s^{\prime})\rangle\right].\]Proof: For any \(l\in\{0,\ldots,L\}\) and for all iterations \(k=k_{l},\ldots,k_{l+1}-1\) associated with \(l\), define

\[\pi_{k}^{+}(\cdot|s)=\begin{cases}\pi_{k}(\cdot|s)&\text{if }s\in\operatorname{Cov}( \mathcal{C}_{l})\\ \pi(\cdot|s)&\text{otherwise.}\end{cases}\]

Then, for any \(s\in\operatorname{Cov}(\mathcal{C}_{l})\),

\[v_{\pi}(s)-v_{\pi_{k}}(s)=v_{\pi}(s)-v_{\pi_{k}^{+}}(s)+v_{\pi_{ k}^{+}}(s)-v_{\pi_{k}}(s)\] \[=\underbrace{\frac{1}{1-\gamma}\mathbb{E}_{s^{\prime}\sim d_{\pi }(s)}\left[\langle q_{\pi_{k}^{+}}(s^{\prime},\cdot),\pi(\cdot|s^{\prime})-\pi_ {k}^{+}(\cdot|s^{\prime})\rangle\right]}_{II}\quad\text{by performance difference lemma}\] \[+\underbrace{\langle q_{\pi_{k}^{+}}(s,\cdot),\pi_{k}^{+}(\cdot| s)\rangle-\langle q_{\pi_{k}}(s,\cdot),\pi_{k}(\cdot|s)\rangle}_{II},\]

where \(d_{\pi}(s)\) is the discounted state occupancy measure induced by following \(\pi\) starting from \(s\).

To bound term \(II\), we note that for any \(s\in\operatorname{Cov}(\mathcal{C}_{l})\), we have \(\pi_{k}^{+}(\cdot|s)=\pi_{k}(\cdot|s)\) and both \(\pi_{k},\pi_{k}^{+}(\cdot|s)\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{ l})}\). By lemma 11, we have for any \(s\in\operatorname{Cov}(\mathcal{C}_{l}),a\in\mathcal{A}\), \(|\tilde{Q}_{k}(s,a)-q_{\pi_{k}^{\prime}}(s,a)|\leq\epsilon^{\prime}\) for any \(\pi_{k}^{\prime}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{l})}\). Then, for any \(s\in\operatorname{Cov}(\mathcal{C}_{l}),a\in\mathcal{A}\),

\[|q_{\pi_{k}^{+}}(s,a)-q_{\pi_{k}}(s,a)|\leq|q_{\pi_{k}^{+}}(s,a)-\tilde{Q}_{k} (s,a)|+|\tilde{Q}_{k}(s,a)-q_{\pi_{k}}(s,a)|\leq 2\epsilon^{\prime}.\]

It follows that for any \(s\in\operatorname{Cov}(\mathcal{C}_{l})\),

\[\langle q_{\pi_{k}^{+}}(s,\cdot),\pi_{k}^{+}(\cdot|s)\rangle- \langle q_{\pi_{k}}(s,\cdot),\pi_{k}(\cdot|s)\rangle =\langle\pi_{k}(\cdot|s),q_{\pi_{k}^{+}}(s,\cdot)-q_{\pi_{k}}(s, \cdot)\rangle\] \[\leq|\langle\pi_{k}(\cdot|s),q_{\pi_{k}^{+}}(s,\cdot)-q_{\pi_{k} }(s,\cdot)\rangle|\] \[\leq\|q_{\pi_{k}^{+}}(s,\cdot)-q_{\pi_{k}}(s,\cdot)\|_{\infty}\| \pi_{k}(\cdot|s)\|_{1}\] \[\leq 2\epsilon^{\prime}.\]

To bound term \(I\), we note that for any \(s\not\in\operatorname{Cov}(\mathcal{C}_{l})\), \(\pi_{k}^{+}(\cdot|s)=\pi(\cdot|s)\) and \(\pi_{k}^{+}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{l})}\), then

\[\frac{1}{1-\gamma}\mathbb{E}_{s^{\prime}\sim d_{\pi}^{\pi}}\left[ \langle q_{\pi_{k}^{+}}(s^{\prime},\cdot),\pi(\cdot|s^{\prime})-\pi_{k}^{+}( \cdot|s^{\prime})\rangle\right]\] \[=\frac{1}{1-\gamma}\mathbb{E}_{s^{\prime}\sim d_{\pi}^{\pi},s^{ \prime}\in\operatorname{Cov}(\mathcal{C}_{l})}\left[\langle q_{\pi_{k}^{+}}(s ^{\prime},\cdot),\pi(\cdot|s^{\prime})-\pi_{k}^{+}(\cdot|s^{\prime})\rangle\right]\] \[+\frac{1}{1-\gamma}\mathbb{E}_{s^{\prime}\sim d_{\pi}^{\pi},s^{ \prime}\not\in\operatorname{Cov}(\mathcal{C}_{l})}\left[\langle q_{\pi_{k}^{+}} (s^{\prime},\cdot),\pi(\cdot|s^{\prime})-\pi_{k}^{+}(\cdot|s^{\prime})\rangle\right]\] \[=\frac{1}{1-\gamma}\mathbb{E}_{s^{\prime}\sim d_{\pi}^{\pi},s^{ \prime}\in\operatorname{Cov}(\mathcal{C}_{l})}\left[\langle q_{\pi_{k}^{+}}(s ^{\prime},\cdot)-\tilde{Q}_{k}(s^{\prime},\cdot),\pi(\cdot|s^{\prime})-\pi_{k}^ {+}(\cdot|s^{\prime})\rangle\right]\] \[+\frac{1}{1-\gamma}\mathbb{E}_{s^{\prime}\sim d_{\pi}^{\pi},s^{ \prime}\in\operatorname{Cov}(\mathcal{C}_{l})}\left[\langle\tilde{Q}_{k}(s^{ \prime},\cdot),\pi(\cdot|s^{\prime})-\pi_{k}^{+}(\cdot|s^{\prime})\rangle\right]\] \[\leq\frac{1}{1-\gamma}\mathbb{E}_{s^{\prime}\sim d_{\pi}^{\pi},s^{ \prime}\in\operatorname{Cov}(\mathcal{C}_{l})}\left[\|q_{\pi_{k}^{+}}(s^{ \prime},\cdot)-\tilde{Q}_{k}(s^{\prime},\cdot)\|_{\infty}\|\pi(\cdot|s^{\prime })-\pi_{k}^{+}(\cdot|s^{\prime})\|_{1}\right]\quad\text{by Holder's inequality}\] \[+\frac{1}{1-\gamma}\mathbb{E}_{s^{\prime}\sim d_{\pi}^{\pi},s^{ \prime}\in\operatorname{Cov}(\mathcal{C}_{l})}\left[\langle\tilde{Q}_{k}(s^{ \prime},\cdot),\pi(\cdot|s^{\prime})-\pi_{k}^{+}(\cdot|s^{\prime})\rangle\right]\] \[\leq\frac{2\epsilon^{\prime}}{1-\gamma}\quad\text{by lemma \ref{lemma:bound} and }\|\pi^{*}(\cdot|s^{\prime})-\pi_{k}^{+}(\cdot|s^{\prime})\|_{1}\leq 2\] \[+\frac{1}{1-\gamma}\mathbb{E}_{s^{\prime}\sim d_{\pi}^{\pi},s^{ \prime}\in\operatorname{Cov}(\mathcal{C}_{l})}\left[\langle\tilde{Q}_{k}(s^{ \prime},\cdot),\pi(\cdot|s^{\prime})-\pi_{k}(\cdot|s^{\prime})\rangle\right]\] \[=\frac{2\epsilon^{\prime}}{1-\gamma}+\frac{1}{1-\gamma}\mathbb{E}_{s^{ \prime}\sim d_{\pi}^{\pi},s^{\prime}\in\operatorname{Cov}(\mathcal{C}_{l})} \left[\langle\tilde{Q}_{k}(s^{\prime},\cdot),\pi(\cdot|s^{\prime})-\pi_{k}(\cdot|s^{ \prime})\rangle\right]\]In summary, for any \(l\), for any \(k=k_{l},\ldots,k_{l+1}-1\) associated with \(l\), and for any \(s\in\operatorname{Cov}(\mathcal{C}_{l})\),

\[v_{\pi}(s)-v_{\pi_{k}}(s)\leq\frac{4\epsilon^{\prime}}{1-\gamma}+\frac{1}{1- \gamma}\mathbb{E}_{s^{\prime}\sim d_{\pi(s),s^{\prime}\in\operatorname{Cov}( \mathcal{C}_{l})}}\left[\langle\tilde{Q}_{k}(s^{\prime},\cdot),\pi(\cdot|s^{ \prime})-\pi_{k}(\cdot|s^{\prime})\rangle\right].\]

Because of line 27 of algorithm 2, one can use induction to show that by the time Confident-NPG terminates, all the \(\mathcal{C}_{l}\) for \(l\in\{0,\ldots,L+1\}\) will be equal. Therefore, the cover of \(\mathcal{C}_{l}\) for all \(l\in\{0,\ldots,L+1\}\) are also equal. Thus, it is sufficient to only consider \(\mathcal{C}_{0}\) at the end of the algorithm. Because of line 3 of algorithm 2, \(s_{0}\in\operatorname{Cov}(\mathcal{C}_{0})\). Putting everything together, the value difference can be bounded as follows,

\[\frac{1}{K}\sum_{k=0}^{K-1}\left(v_{\pi}(s_{0})-v_{\pi_{k}}(s_{0 })\right)=\frac{1}{K}\sum_{l=0}^{L}\sum_{k=k_{l}}^{k_{l+1}-1}\left(v_{\pi}(s_{ 0})-v_{\pi_{k}}(s_{0})\right)\] \[\leq\frac{1}{K}\sum_{l=0}^{L}\sum_{k=k_{l}}^{k_{l+1}-1}\frac{4 \epsilon^{\prime}}{1-\gamma}\] \[+\frac{1}{K(1-\gamma)}\sum_{l=0}^{L}\sum_{k=k_{l}}^{k_{l+1}-1} \mathbb{E}_{s^{\prime}\sim d_{\pi}(s_{0}),s^{\prime}\in\operatorname{Cov}( \mathcal{C}_{l})}\left[\langle\tilde{Q}_{k}(s^{\prime},\cdot),\pi(\cdot|s^{ \prime})-\pi_{k}(\cdot|s^{\prime})\rangle\right]\] \[\leq\frac{4\epsilon^{\prime}}{1-\gamma}+\frac{1}{K(1-\gamma)} \sum_{k=0}^{K-1}\mathbb{E}_{s^{\prime}\sim d_{\pi}(s_{0}),s^{\prime}\in \operatorname{Cov}(\mathcal{C}_{0})}\left[\langle\tilde{Q}_{k}(s^{\prime}, \cdot),\pi(\cdot|s^{\prime})-\pi_{k}(\cdot|s^{\prime})\rangle\right].\]

## Appendix B Confident-NPG-CMDP

We include the proofs of lemmas that appear in prior works and supporting lemmas that are helpful proving the lemmas in the main section. The lemmas that appear in the main section will have the same numbering here.

### The accuracy of least-square estimates

Once a state-action pair is added to a core set, it remains in that core set for the duration of the algorithm. This means that any \(\mathcal{C}_{l}\) for \(l\in\{0,\ldots,L+1\}\) can grow in size. When a core set \(\mathcal{C}_{l}\) is extended during a running phase \(\ell=l\), the least-square estimates will need be updated based on the newly extended \(\mathcal{C}_{l}\) which contains newly discovered features. However, the policy is update only for states that are newly covered by the extended core set \(\mathcal{C}_{l}\) using the newly improved estimates. Note that after line 30 of algorithm 1 is run, the next phase's core set \(\mathcal{C}_{l+1}\) will be set to \(\mathcal{C}_{l}\), which means that any state that was once newly covered by \(\mathcal{C}_{l}\) is no longer considered newly covered. Consequently, the policy for those states will remain unchanged throughout the rest of the algorithm's execution.

We introduce hypothetical \(\tilde{Q}_{k}^{r}\) and \(\tilde{Q}_{k}^{c}\) to reflect the value of \(\tilde{Q}_{k}^{p}\), used in the update of \(\pi_{k+1}\) for \(k=k_{l},\ldots,k_{l+1}-1\) associated with running phase \(\ell=l\). At initialization, \(\tilde{Q}_{k}^{r}(s,a)=0,\tilde{Q}_{k}^{c}(s,a)=0\) for all \(k=0,\ldots,K\), \(s\in\mathcal{S}\) and \(a\in\mathcal{A}\). The values are specified in the following cases when line 27 is run:

\[\tilde{Q}_{k}^{r}(s,a) \leftarrow\begin{cases}\tilde{Q}_{k}^{r}(s,a)&\text{if }s\in \operatorname{Cov}(\mathcal{C}_{l+1})\\ Q_{k}^{r}(s,a)&\text{if }s\in\operatorname{Cov}(\mathcal{C}_{l})\setminus \operatorname{Cov}(\mathcal{C}_{l+1})\\ \text{initial value }0&\text{if }s\notin\operatorname{Cov}(\mathcal{C}_{l}), \end{cases}\] \[\tilde{Q}_{k}^{c}(s,a) \leftarrow\begin{cases}\tilde{Q}_{k}^{c}(s,a)&\text{if }s\in \operatorname{Cov}(\mathcal{C}_{l+1})\\ Q_{k}^{c}(s,a)&\text{if }s\in\operatorname{Cov}(\mathcal{C}_{l})\setminus \operatorname{Cov}(\mathcal{C}_{l+1})\\ \text{initial value }0&\text{if }s\notin\operatorname{Cov}(\mathcal{C}_{l}), \end{cases}\]

where \(Q_{k}^{r}(s,a),Q_{k}^{c}(s,a)\) are the least-square estimates using the most recently extended \(\mathcal{C}_{l}\) at that time. The dual variable \(\lambda_{k}\) is defined in line 29. Therefore, the \(\tilde{Q}_{k}^{p}(s,a)\) used in the update of policy at line 27 can be written as \(\tilde{Q}_{k}^{p}(s,a)=\operatorname{trunc}_{[0,\frac{1}{1-\gamma}]}\tilde{Q}_{ k}^{r}(s,a)+\lambda_{k}\operatorname{trunc}_{[0,\frac{1}{1-\gamma}]}\tilde{Q}_{k}^{c}(s,a)\).

**Lemma 1**: _Whenever LSE subroutine in line 21 of Confident-NPG-CMDP is executed during a running phase \(\ell=l\) for \(l\in\{0,\ldots,L\}\), the least-square estimate \(\tilde{Q}_{k}^{p}(s,a)\) satisfies the following condition for all iterations \(k=k_{\ell},\ldots,k_{\ell+1}-1\) associated with this phase and for all \(s\in\operatorname{Cov}(\mathcal{C}_{\ell})\) and \(a\in\mathcal{A}\),_

\[|\tilde{Q}_{k}^{p}(s,a)-q_{\pi_{k}^{\prime},\lambda_{k}}^{p}(s,a)|\leq\epsilon ^{\prime}\quad\text{for all }\pi_{k}^{\prime}\in\Pi_{\pi_{k},\operatorname{Cov}( \mathcal{C}_{\ell})},\]

_where \(\epsilon^{\prime}=(1+U)(\omega+\sqrt{\alpha}B+(\omega+\epsilon)\sqrt{\tilde{d }})\) with \(\tilde{d}=\tilde{O}(d)\) and \(U\) is an upper bound on the optimal Lagrange multiplier. Similarly, for initial state \(s_{0}\), we have_

\[|\tilde{V}_{k}^{c}(s_{0})-v_{\pi_{k}^{\prime}}^{c}(s_{0})|\leq\omega+\sqrt{ \alpha}B+(\omega+\epsilon)\sqrt{\tilde{d}}\quad\text{for all }\pi_{k}^{\prime}\in\Pi_{\pi_{k}, \operatorname{Cov}(\mathcal{C}_{\ell})}.\]

Proof: By using the primal-dual approach, we have reduced the CMDP problem to an unconstrained problem with a single reward of the form \(r_{\lambda}=r+\lambda c\).

Because of line 7 have executed before entering the loop and line 30 have been executed in the previous phase \(\ell=l-1\), the initial state \(s_{0}\in\operatorname{Cov}(\mathcal{C}_{\ell})\). If \(s_{0}\) is in \(\operatorname{Cov}(\mathcal{C}_{\ell})\) for the first time (i.e. \(s_{0}\in\operatorname{Cov}(\mathcal{C}_{\ell})\setminus\operatorname{Cov}( \mathcal{C}_{\ell+1})\) ), then the dual variable \(\lambda_{k}\) makes a mirror descent update in line 29 using \(V_{k}^{c}(s_{0})\) at that time. After line 30 is executed, the core set for the next phase \(\mathcal{C}_{\ell+1}=\mathcal{C}_{\ell}\). This means that any states, including \(s_{0}\), that are covered by \(\mathcal{C}_{\ell}\) are then covered by \(\mathcal{C}_{\ell+1}\). By lemma 6, the initial state \(s_{0}\) will continue to be covered by \(\mathcal{C}_{\ell+1}\) for the remainder of the algorithm's execution. This implies that the dual variable \(\lambda_{k}\) referenced in this lemma remains fixed at the value set when \(s_{0}\) is covered by \(\mathcal{C}_{\ell}\) for the first time and does not change thereafter for the duration of the algorithm's execution.

Then the proof of this lemma follows similar logic to lemma 11 in the single reward setting. The result of lemma 11 uses lemma 5. For lemma 5 to hold, lemma 9 is used to verify the conditions sufficient for lemma 5 to hold. For lemma 9 to hold, one of the requirement is that the behaviour policy \(\pi_{k_{\ell}}\) and the target policy \(\pi_{k}\) must satisfy eq. (9). In the following paragraphs, we show that eq. (9) indeed hold with appropriate changes to the parameters of interest. Then it follows that lemma 9 holds and consequently lemma 5 holds. Once all the sufficient conditions hold, by following similar logic as in lemma 11, we have the proof.

Since the policies are updated with respect to \(\tilde{Q}^{p}\) instead of \(\tilde{Q}\) of the single-reward setting, we need to make adjustment to \(\eta_{1},H,m,K\) to ensure \(\pi_{k_{\ell}}\) and \(\pi_{k}\) indeed satisfy eq. (9). First, note the value \(\tilde{Q}_{k}^{p}\) for \(k=0,\ldots,K\) are in the range of \(0\) and \(\frac{1+U}{1-\gamma}\). The upper bound value is the result of the primary reward function taking values in the range of \([0,1]\) and the dual variable taking values in the range of \([0,U]\). The value \(U\) is defined in lemma 13 for relaxed-feasibility and in lemma 15 for strict-feasibility, and it is an upper bound on the optimal dual variable (i.e., \(\lambda^{*}\leq U\)). By similar argument to lemma 8, we make the following changes to \(\eta_{1},H,m,K\). We set the step size \(\eta_{1}=\frac{1-\gamma}{1+U}\sqrt{\frac{2\ln(|\mathcal{A}|)}{K}}\), the total number of iterations \(K=\frac{6^{2}(\sqrt{2\ln(|\mathcal{A}|)}+1)^{2}(1+U)^{2}}{(1-\gamma)^{4}\epsilon ^{2}}\), and \(H=\frac{\ln((30\sqrt{\tilde{d}}(1+U))/((1-\gamma)^{2}\epsilon))}{1-\gamma}\). Then, it follows that \(m=\frac{(1+U)\ln(1+\rho_{0})}{2H\epsilon(1-\gamma)^{2}}\).

Next, from lemma 7, we have each \(\bar{q}_{k}^{r}(s,a)\) and \(\bar{q}_{k}^{c}(s,a)\) is an unbiased estimate of \(\mathbb{E}_{\pi_{k}^{\prime},s,a}[\sum_{h=0}^{H-1}\gamma^{h}R_{h+1}]\) and \(\mathbb{E}_{\pi_{k}^{\prime},s,a}[\sum_{h=0}^{H-1}\gamma^{h}C_{h+1}]\) respectively for all \(\pi_{k}^{\prime}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{l})}\). Let \(\delta^{\prime}=2\exp\left(-\frac{2n\left(\frac{\gamma}{4}\right)^{2}}{\left( \frac{(1+\rho_{0})}{1-\gamma}\right)^{2}}\right)\). By lemma 9, with probability \(1-\delta^{\prime}\), we have for any \((s,a)\in\mathcal{C}_{l}\),

\[|\bar{q}_{k}^{r}(s,a)-q_{\pi_{k}^{\prime}}^{r}|\leq\epsilon,\quad|\bar{q}_{k}^{c }(s,a)-q_{\pi_{k}^{\prime}}^{c}|\leq\epsilon\quad\text{for all }\pi_{k}^{\prime}\in\Pi_{\pi_{k}, \operatorname{Cov}(\mathcal{C}_{l})}.\]

Then the conditions of lemma 5 hold, and by similar argument to lemma 11 using lemma 5, we have for each \((s,a)\in\operatorname{Cov}(\mathcal{C}_{l})\),

\[|\tilde{Q}_{k}^{r}(s,a)-q_{\pi_{k}^{\prime}}^{r}(s,a)|\leq\omega+ \sqrt{\alpha}B+(\omega+\epsilon)\sqrt{\tilde{d}},\] \[|\tilde{Q}_{k}^{c}(s,a)-q_{\pi_{k}^{\prime}}^{c}(s,a)|\leq\omega+ \sqrt{\alpha}B+(\omega+\epsilon)\sqrt{\tilde{d}},\]for all \(\pi^{\prime}_{k}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{\ell})}\). Then it follows that for a given \(\lambda_{k}\),

\[|\tilde{Q}^{p}_{k}(s,a)-q^{p}_{\pi^{\prime}_{k},\lambda_{k}}(s,a)|=|(\tilde{Q}^{ r}_{k}(s,a)-q^{r}_{\pi^{\prime}_{k}}(s,a))+\lambda_{k}(\tilde{Q}^{c}_{k}(s,a)-q^{c}_{ \pi^{\prime}_{k}}(s,a))|\leq\epsilon^{\prime},\]

for all \(\pi^{\prime}_{k}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{\ell})}\).

Finally, since \(s_{0}\in\operatorname{Cov}(\mathcal{C}_{\ell})\) and \(\tilde{V}^{c}_{k}(s_{0})=\langle\pi^{\prime}_{k}(\cdot|s_{0}),\tilde{Q}^{c}_{ k}(s_{0},\cdot)\rangle\), therefore \(|\tilde{V}^{c}_{k}(s_{0})-v^{c}_{\pi_{k}}(s_{0})|=|\langle\pi^{\prime}_{k}( \cdot|s_{0}),\tilde{Q}^{c}_{k}(s_{0},\cdot)-q^{c}_{\pi^{\prime}_{k}}(s_{0}, \cdot)\rangle|\leq\omega+\sqrt{\alpha}B+(\omega+\epsilon)\sqrt{\tilde{d}}\) for all \(\pi^{\prime}_{k}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{\ell})}\). \(\blacksquare\)

## Appendix C Relaxed-feasibility

**Lemma 13**: _[Lemma 4.1 of Jain et al. (2022)] Let \(\lambda^{*}\) be the optimal dual variable that satisfies \(min_{\lambda\geq 0}\max_{\pi}v^{r}_{\pi}(\rho)+\lambda(v^{c}_{\pi}(\rho)-b)\). If we choose_

\[U=\frac{2}{\zeta(1-\gamma)},\]

_then \(\lambda^{*}\leq U\)._

Proof: Let \(\pi^{*}_{c}(\rho)=\arg\max v^{c}_{\pi}(\rho)\), and recall that \(\zeta=v^{c}_{\pi^{*}_{c}}(\rho)-b>0\), then

\[v^{r}_{\pi^{*}}(\rho)=\max_{\pi}\min_{\lambda\geq 0}v^{r}_{\pi}(\rho)+\lambda(v^ {c}_{\pi}(\rho)-b).\]

By Altman (2021),

\[v^{r}_{\pi^{*}}(\rho) =\min_{\lambda\geq 0}\max_{\pi}v^{r}_{\pi}(\rho)+\lambda(v^{c}_{ \pi}(\rho)-b)\] \[=\max_{\pi}v^{r}_{\pi}(\rho)+\lambda^{*}(v^{c}_{\pi}(\rho)-b)\] \[\geq v^{r}_{\pi^{*}_{c}}(\rho)+\lambda^{*}(v^{c}_{\pi^{*}_{c}}( \rho)-b)\] \[\geq v^{r}_{\pi^{*}_{c}}(\rho)+\lambda^{*}\zeta.\]

After rearranging terms, we have

\[\lambda^{*}\leq\frac{v^{r}_{\pi^{*}}(\rho)-v^{r}_{\pi^{*}_{c}}(\rho)}{\zeta} \leq\frac{1}{\zeta(1-\gamma)}.\]

By choosing \(U=\frac{2}{\zeta(1-\gamma)}\), we have \(\lambda^{*}\leq U\). \(\blacksquare\)

**Definition 2**: \[R^{p}(\pi^{*},K) =\sum_{k=0}^{K-1}\mathbb{E}_{s^{\prime}\sim d_{\pi^{*}}(s_{0}),s^{ \prime}\in\operatorname{Cov}(\mathcal{C}_{0})}\left[\langle\pi^{*}(\cdot|s^{ \prime})-\pi_{k}(\cdot|s^{\prime}),\tilde{Q}^{r}_{k}(s^{\prime},\cdot)+\lambda_ {k}\tilde{Q}^{c}_{k}(s^{\prime},\cdot)\rangle\right],\] \[R^{d}(\lambda,K) =\sum_{k=0}^{K-1}(\lambda_{k}-\lambda)(\tilde{V}^{c}_{k}(s_{0})-b).\]

**Lemma 2**: _Let \(\delta\in(0,1]\) be the failure probability, \(\epsilon>0\) be the target accuracy, and \(s_{0}\) be the initial state. Assuming for all \(s\in\operatorname{Cov}(\mathcal{C}_{0})\) and all \(a\in\mathcal{A}\), \(|\tilde{Q}^{p}_{k}(s,a)-q^{p}_{\pi^{\prime}_{k},\lambda_{k}}(s,a)|\leq\epsilon^ {\prime}\) and \(|\tilde{V}^{c}_{k}(s_{0})-v^{c}_{\pi^{\prime}_{k}}(s_{0})|\leq\omega+\sqrt{ \alpha}B+(\omega+\epsilon)\sqrt{\tilde{d}}\) for all \(\pi^{\prime}_{k}\in\Pi_{\pi_{k},\operatorname{Cov}(\mathcal{C}_{0})}\), then, with probability \(1-\delta\), Confident-NPG-CMDP returns a mixture policy \(\bar{\pi}_{K}\) that satisfies the following,_

\[v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\pi_{K}}(s_{0})\leq\frac{5\epsilon ^{\prime}}{1-\gamma}+\frac{(\sqrt{2\ln(|\mathcal{A}|)}+1)(1+U)}{(1-\gamma)^{2} \sqrt{K}},\] \[b-v^{c}_{\bar{\pi}_{K}}(s_{0})\leq[b-v^{c}_{\bar{\pi}_{K}}(s_{0} )]_{+}\leq\frac{5\epsilon^{\prime}}{(1-\gamma)(U-\lambda^{*})}+\frac{(\sqrt{2 \ln(|\mathcal{A}|)}+1)(1+U)}{(1-\gamma)^{2}(U-\lambda^{*})\sqrt{K}},\]

_where \(\epsilon^{\prime}=(1+U)(\omega+(\sqrt{\alpha}B+(\omega+\epsilon)\sqrt{\tilde{d} }))\) with \(\tilde{d}=\tilde{O}(d)\)._Proof: For the following result, we consider a \(k\in\{0,\ldots,K\}\) with its corresponding \(l\in\{0,\ldots,L\}\). At the time of termination, all \(\mathcal{C}_{l}\) are equal.

To obtain a bound on the suboptimality and the constraint violation, we apply lemma 12 with \(\pi=\pi^{*}\) of CMDP, \(\tilde{Q}_{k}^{p}=\tilde{Q}_{k}^{r}+\lambda_{k}\tilde{Q}_{k}^{c}\) instead of \(\tilde{Q}_{k}\), and lemma 1 instead of lemma 11 of the single reward setting. Then, we have

\[\frac{1}{K}\sum_{k=0}^{K-1}v_{\pi^{*},\lambda_{k}}^{p}(s_{0})-v_{ \pi_{k},\lambda_{k}}^{p}(s_{0})\] (18) \[\leq\frac{4\epsilon^{\prime}}{1-\gamma}+\frac{1}{K(1-\gamma)} \sum_{k=0}^{K-1}\mathbb{E}_{s^{\prime}\sim d_{\pi^{*}}(s_{0}),s^{\prime}\in \operatorname{Cov}(\mathcal{C}_{0})}\left[\langle\tilde{Q}_{k}^{r}(s^{\prime},\cdot)+\lambda_{k}\tilde{Q}_{k}^{c}(s^{\prime},\cdot),\pi^{*}(\cdot|s^{ \prime})\rangle\right]\] \[=\frac{4\epsilon^{\prime}}{1-\gamma}+\frac{R^{p}(\pi^{*},K)}{K(1 -\gamma)}.\]

By Proposition 28.6 of Lattimore and Szepesvari (2020), the primal regret \(R^{p}(\pi^{*},K)\leq\frac{1+U}{1-\gamma}\sqrt{2K\ln(|\mathcal{A}|)}\) with \(\eta_{1}=\frac{1-\gamma}{1+U}\sqrt{\frac{2\ln(|\mathcal{A}|)}{K}}\). Expanding eq. (18) in terms of \(v^{r},v^{c}\), we have

\[\frac{1}{K}\sum_{k=0}^{K-1}v_{\pi^{*}}^{r}(s_{0})-v_{\pi}^{r}(s_{0 })+\frac{1}{K}\sum_{k=0}^{K-1}\lambda_{k}(v_{\pi^{*}}^{c}(s_{0})-v_{\pi_{k}}^ {c}(s_{0}))\] \[\leq\frac{4\epsilon^{\prime}}{1-\gamma}+\frac{1+U}{(1-\gamma)^{2 }}\sqrt{\frac{2\ln(|\mathcal{A}|)}{K}}.\] (19)

Furthermore, by lemma 1, we have \(|\tilde{Q}_{k}^{c}(s,a)-q_{\pi_{k}^{\prime}}^{c}(s,a)|\leq\omega+\sqrt{\alpha }B+(\omega+\epsilon)\sqrt{\tilde{d}}\) for any \(s\in\operatorname{Cov}(\mathcal{C}_{l})\). Recall \(\tilde{V}_{k}^{c}(s_{0})=\langle\pi_{k}(\cdot|s_{0}),\tilde{Q}_{k}^{c}(s_{0}, \cdot)\rangle\), then it follows that \(\lambda_{k}(v_{\pi_{k}}^{c}(s_{0})-\tilde{V}_{k}^{c}(s_{0}))\leq|\lambda_{k}( v_{\pi_{k}}^{c}(s_{0})-\tilde{V}_{k}^{c}(s_{0}))|\leq U(\omega+\sqrt{ \alpha}B+(\omega+\epsilon)\sqrt{\tilde{d}})\leq\epsilon^{\prime}\).

\[\frac{1}{K}\sum_{k=0}^{K-1}\lambda_{k}(v_{\pi_{k}}^{c}(s_{0})-v_{ \pi^{*}}^{c}(s_{0}))\leq\frac{1}{K}\sum_{k=0}^{K-1}\lambda_{k}(v_{\pi_{k}}^{c}( s_{0})-b)\] \[=\frac{1}{K}\sum_{k=0}^{K-1}\lambda_{k}(v_{\pi_{k}}^{c}(s_{0})- \tilde{V}_{k}^{c}(s_{0}))+\lambda_{k}(\tilde{V}_{k}^{c}(s_{0})-b)\] \[\leq\epsilon^{\prime}+\frac{R^{d}(0,K)}{K}\] \[\leq\epsilon^{\prime}+\frac{U}{(1-\gamma)\sqrt{K}}.\]

The update to the dual variable is a mirror descent algorithm. By Proposition 28.6 of Lattimore and Szepesvari (2020), the dual regret \(R^{d}(0,K)\leq\frac{U\sqrt{K}}{1-\gamma}\) with \(\eta_{2}=\frac{U(1-\gamma)}{\sqrt{K}}\). Altogether,

\[\frac{1}{K}\sum_{k=0}^{K-1}v_{\pi^{*}}^{r}(s_{0})-v_{\pi_{k}}^{r}( s_{0})\leq\frac{4\epsilon^{\prime}}{1-\gamma}+\frac{1+U}{(1-\gamma)^{2}}\sqrt{ \frac{2\ln(|\mathcal{A}|)}{K}}+\epsilon^{\prime}+\frac{U}{(1-\gamma)\sqrt{K}}\] \[\leq\frac{5\epsilon^{\prime}}{1-\gamma}+\frac{(\sqrt{2\ln(| \mathcal{A}|)}+1)(1+U)}{(1-\gamma)^{2}\sqrt{K}}\]For bounding the constraint violations, we first incorporate \(R^{d}(\lambda,K)\) into eq. (19) and rearrange terms to obtain:

\[\frac{1}{K}\sum_{k=0}^{K-1}v_{\pi^{*}}^{r}(s_{0})-v_{\pi_{k}}^{r}(s _{0})+\frac{\lambda}{K}\sum_{k=0}^{K-1}(b-v_{\pi_{k}}^{c}(s_{0}))\] \[\leq\frac{1}{K}\sum_{k=0}^{K-1}(\lambda_{k}-\lambda)(v_{\pi_{k}}^ {c}(s_{0})-b)+\frac{4\epsilon^{\prime}}{1-\gamma}+\frac{(1+U)\sqrt{2\ln(|\mathcal{ A}|)}}{(1-\gamma)^{2}\sqrt{K}}\] \[=\frac{1}{K}\sum_{k=0}^{K-1}(\lambda_{k}-\lambda)(v_{\pi_{k}}^{c} (s_{0})-\tilde{V}_{k}^{c}(s_{0}))+\frac{1}{K}\sum_{k=0}^{K-1}(\lambda_{k}- \lambda)(\tilde{V}_{k}^{c}(s_{0})-b)\] \[+\frac{4\epsilon^{\prime}}{1-\gamma}+\frac{(1+U)\sqrt{2\ln(| \mathcal{A}|)}}{(1-\gamma)^{2}\sqrt{K}}\] \[=\epsilon^{\prime}+\frac{R^{d}(\lambda,K)}{K}+\frac{4\epsilon^{ \prime}}{1-\gamma}+\frac{(1+U)\sqrt{2\ln(|\mathcal{A}|)}}{(1-\gamma)^{2}\sqrt {K}}\] \[\leq\frac{5\epsilon^{\prime}}{1-\gamma}+\frac{(1+U)(\sqrt{2\ln(| \mathcal{A}|)}+1)}{(1-\gamma)^{2}\sqrt{K}}\]

There are two constraint cases. Case one is no violation: \(b-v_{\pi_{K}}^{c}(s_{0})\leq 0\). Then, it also holds that \(b-\triangle-v_{\pi_{K}}^{c}(s_{0})\leq 0\) for any \(\triangle\geq 0\), which is what we want to show. Case two is violation: \(b-v_{\pi_{K}}^{c}(s_{0})>0\), for which case, \(\lambda=U\). Using notation \([x]_{+}=\max\{x,0\}\), we have

\[\frac{1}{K}\sum_{k=0}^{K-1}v_{\pi^{*}}^{r}(s_{0})-v_{\pi_{k}}^{r}( s_{0})+\frac{U}{K}\left[\sum_{k=0}^{K}b-v_{\pi}^{c}(s_{0})\right]_{+}\] \[\leq\frac{5\epsilon^{\prime}}{1-\gamma}+\frac{(1+U)(\sqrt{2\ln(| \mathcal{A}|)}+1)}{(1-\gamma)^{2}\sqrt{K}}.\]

By Lemma B.2 of Jain et al. (2022), we have

\[[b-v_{\pi_{K}}^{c}(s_{0})]_{+}\leq\frac{5\epsilon^{\prime}}{(1- \gamma)(U-\lambda^{*})}+\frac{(\sqrt{2\ln(|\mathcal{A}|)}+1)(1+U)}{(1-\gamma) ^{2}(U-\lambda^{*})\sqrt{K}}.\]

**Theorem 1**: _With probability \(1-\delta\), the mixture policy \(\bar{\pi}_{K}\) returned by confident-NPG-CMDP ensures that_

\[v_{\pi^{*}}^{r}(s_{0})-v_{\pi_{K}}^{r}(s_{0}) =\frac{5(1+U)(1+\sqrt{\tilde{d}})}{1-\gamma}\omega+\epsilon,\] (20) \[v_{\pi_{K}}^{c}(s_{0}) \geq b-\left(\frac{5(1+U)(1+\sqrt{\tilde{d}})}{(1-\gamma)} \omega+\epsilon\right).\] (21)

_if we choose \(n=\frac{30^{2}(1+\rho_{0})^{2}(1+U)^{2}\tilde{d}}{2\epsilon^{2}(1-\gamma)^{4}} \ln\left(\frac{8\tilde{d}(L+1)}{\delta}\right)\), \(\alpha=\frac{(1-\gamma)^{2}\epsilon^{2}}{30^{2}(1+U)^{2}B^{2}}\), \(K=\frac{6^{2}(\sqrt{2\ln(|\mathcal{A}|)}+1)^{2}(1+U)^{2}}{(1-\gamma)^{4} \epsilon^{2}}\), \(\eta_{1}=\frac{1-\gamma}{1+U}\sqrt{\frac{2\ln(|\mathcal{A}|)}{K}}\), \(\eta_{2}=\frac{U(1-\gamma)}{\sqrt{K}}\), \(H=\frac{\ln((30\sqrt{\tilde{d}}(1+U))/((1-\gamma)^{2}\epsilon))}{1-\gamma}\), \(m=\frac{(1+U)\ln(1+\rho_{0})}{2\epsilon H(1-\gamma)^{2}}\), and \(U=\frac{2}{\zeta(1-\gamma)}\)._

_Furthermore, the algorithm utilizes at most \(\tilde{O}(d^{2}(1+U)^{3}\epsilon^{-3}(1-\gamma)^{-8})\) queries in the local-access setting._

Proof: From lemma 2, we have

\[v_{\pi^{*}}^{r}(s_{0})-v_{\pi_{K}}^{r}(s_{0}) \leq\frac{5\epsilon^{\prime}}{(1-\gamma)}+\frac{(\sqrt{2\ln(| \mathcal{A}|)}+1)(1+U)}{(1-\gamma)^{2}\sqrt{K}},\] (22) \[b-v_{\pi_{K}}^{c}(s_{0}) \leq\frac{5\epsilon^{\prime}}{(1-\gamma)(U-\lambda^{*})}+\frac{( \sqrt{2\ln(|\mathcal{A}|)}+1)(1+U)}{(1-\gamma)^{2}(U-\lambda^{*})\sqrt{K}},\] (23)Let \(C=\frac{1}{\zeta(1-\gamma)}\) for a \(\zeta\in(0,\frac{1}{1-\gamma}]\). By lemma 13, we chose \(U=2C\) and \(\lambda^{*}\leq C\). It follows that \(\frac{1}{U-\lambda^{*}}\leq\frac{1}{C}=\zeta(1-\gamma)\leq 1\), and thus the right hand side of eq. (23) is upper bounded by the right hand side of eq. (22). Recall \(\epsilon^{\prime}=(1+U)\left(\omega+\left(\sqrt{\alpha}B+(\omega+\epsilon) \sqrt{\bar{d}}\right)\right)\). Then, the goal is to set the parameters \(H,n,K\), and \(\alpha\) appropriately so that the \(A,B\) and \(C\) of the following expression, when added together, is less than \(\epsilon\):

\[\frac{5(1+U)(1+\sqrt{\bar{d}})\omega}{1-\gamma}+\underbrace{\frac{5(1+U)\sqrt {\alpha}B}{1-\gamma}}_{A}+\underbrace{\frac{5(1+U)\epsilon\sqrt{\bar{d}}}{1- \gamma}}_{B}+\underbrace{\frac{(\sqrt{2\ln(|\mathcal{A}|)}+1))(1+U)}{(1- \gamma)^{2}\sqrt{K}}}_{C}.\] (24)

First, we set \(n\) appropriately so that the failure probability is well controlled. The failure probability depends on the number of times Gather-data subroutine (algorithm 3) is executed. Gather-data is run for phase \(0,\ldots,L\). Each phase has at most \(\bar{d}\) elements, and recall \(\bar{d}\) is defined in lemma 10. Therefore, Gather-data would return success at most \(\bar{d}\) times. Altogether, Gather-data can return success at most \(\bar{d}(L+1)\) times, each with probability of at least \(1-\delta^{\prime}=1-\delta/(\bar{d}(L+1))\). By a union bound, Gather-data returns success in all occasions with probability \(1-\delta\).

By setting \(H=\frac{\ln((30\sqrt{\bar{d}}(1+U))/((1-\gamma)^{2}\epsilon))}{1-\gamma}\) and \(n=\frac{30^{2}(1+\rho_{0})^{2}(1+U)^{2}\bar{d}}{2\epsilon^{2}(1-\gamma)^{4}} \ln\left(\frac{8\bar{d}(L+1)}{\delta}\right)\), we have for any \(l\in\{0,\ldots,L\}\), \(k=k_{l},\ldots,k_{l+1}-1\), the \(|\bar{q}_{k}^{r}(s,a)-q_{\pi_{k}^{r}}^{r}(s,a)|\leq\frac{4}{6}\frac{(1-\gamma )\epsilon}{5(1+U)\sqrt{\bar{d}}}\) and \(|\bar{q}_{k}^{c}(s,a)-q_{\pi_{k}^{r}}^{c}(s,a)|\leq\frac{4}{6}\frac{(1-\gamma )\epsilon}{5(1+U)\sqrt{\bar{d}}}\) hold for all \(\pi_{k}^{\prime}\in\Pi_{\pi_{k},\mathrm{Cov}(\mathcal{C}_{l})}\) with probability at least \(1-\delta\). Then, this is used in the accuracy guarantee of the least-square estimate (lemma 1) and finally in the suboptimality bound of lemma 2.

Then, we can set \(\alpha\) of eq. (24) to be equal to \(\frac{\epsilon}{6}\) and solve for \(\alpha=\frac{\epsilon^{2}(1-\gamma)^{2}}{30^{2}(1+U)^{2}B^{2}}\). Finally, by setting \(K=\frac{6^{2}(\sqrt{2\ln(|\mathcal{A}|)}+1)^{2}(1+U)^{2}}{(1-\gamma)^{4} \epsilon^{2}}\), we have \(C\) of eq. (24) be less than \(\frac{\epsilon}{6}\). Altogether, we have the reward suboptimality satisfying eq. (20) and constraint satisfying eq. (21).

For the query complexity, we note that our algorithm does not query the simulator in every iteration, but at fixed intervals, which we call phases. Each phase is \(m\) iterations in length. There are total of \(L=\lfloor K/(\lfloor m\rfloor+1)\rfloor\leq K/m=\tilde{O}\left((1+U)(1-\gamma)^ {-3}\epsilon^{-1}\right)\) phases. In each phases, Gather-data subroutine (algorithm 3) can be run. Each time Gather-data returns success with trajectories, the subroutine would have made at most \(nH\) queries. Gather-data is run for each of the elements in \(\mathcal{C}_{l}\), \(l\in\{0,\ldots,L\}\). By the time the algorithm terminates, all \(\mathcal{C}_{l}\)'s are the same. Since there are at most \(\tilde{O}(d)\) elements in each \(\mathcal{C}_{l}\), the algorithm will make a total of \(nH(L+1)|\mathcal{C}_{0}|\) number of queries to the simulator. Since we have \(H=\tilde{O}((1-\gamma)^{-1})\), \(n=\tilde{O}((1+U)^{2}d\epsilon^{-2}(1-\gamma)^{-4})\) and \(L=\tilde{O}((1+U)\epsilon^{-1}(1-\gamma)^{-3})\), the sample complexity is \(\tilde{O}(d^{2}(1+U)^{3}(1-\gamma)^{-8}\epsilon^{-3})\).

## Appendix D Strict-feasibility

**Lemma 14**: _Let \(\pi_{\triangle}^{*}\) be defined as in eq. (7) and \(\pi^{*}\) be an optimal policy of CMDP. Then, for a \(\triangle>0\),_

\[v_{\pi^{*}}^{r}(s_{0})-v_{\pi_{\triangle}^{*}}^{r}(s_{0})\leq\lambda^{*}\triangle,\]

_where \(\lambda^{*}\) is an optimal dual variable that satisfies \(\min_{\lambda\geq 0}\max_{\pi}v_{\pi}^{r}(s_{0})+\lambda(v_{\pi}^{c}(s_{0})-b^{ \prime})\)._

Proof:

\[v_{\pi_{\triangle}^{*}}^{r}(s_{0})=\max_{\pi}\min_{\lambda\geq 0}v_{\pi}^{r}(s_{0 })+\lambda(v_{\pi}^{c}(s_{0})-b^{\prime}).\]By Altman (2021),

\[v^{r}_{\pi^{\bot}_{\Delta}}(s_{0}) =\min_{\lambda\geq 0}\max_{\pi}v^{r}_{\pi}(s_{0})+\lambda(v^{c}_{ \pi}(s_{0})-b^{\prime})\] \[=\max_{\pi}v^{r}_{\pi}(s_{0})+\lambda^{*}(v^{c}_{\pi}(s_{0})-b^{ \prime})\] \[\geq v^{r}_{\pi^{*}}(s_{0})+\lambda^{*}(v^{c}_{\pi^{*}}(s_{0})-(b+ \triangle))\] \[\geq v^{r}_{\pi^{*}}(s_{0})+\lambda^{*}(b-\triangle)\quad\text{ because }v^{c}_{\pi^{*}}(s_{0})\geq b\] \[=v^{r}_{\pi^{*}}(s_{0})-\lambda^{*}\triangle.\]

After rearranging the terms, we get the result.

**Lemma 15**: _Let \(\lambda^{*}\) be the optimal dual variable that satisfies \(min_{\lambda\geq 0}\max_{\pi}V^{r}_{\pi}(s_{0})+\lambda(V^{c}_{\pi}(s_{0})-b^{ \prime})\). If we choose_

\[U=\frac{4}{\zeta(1-\gamma)},\]

_then \(\lambda^{*}\leq U\) requiring that \(\triangle\in(0,\frac{\zeta}{2})\)._

Proof: Let \(\pi^{*}_{c}(s_{0})=\arg\max V^{c}_{\pi}(s_{0})\), and recall that \(\zeta=V^{c}_{\pi^{c}_{c}}(s_{0})-b>0\), then

\[v^{r}_{\pi^{\bot}_{\triangle}}(s_{0}) =\max_{\pi}\min_{\lambda\geq 0}v^{r}_{\pi}(s_{0})+\lambda(v^{c}_{ \pi}(s_{0})-b^{\prime})\]

By Altman (2021),

\[v^{r}_{\pi^{\bot}_{\triangle}}(s_{0}) =\min_{\lambda\geq 0}\max_{\pi}v^{r}_{\pi}(s_{0})+\lambda(v^{c}_{ \pi}(s_{0})-b^{\prime})\] \[=\max_{\pi}v^{r}_{\pi^{c}_{c}}(s_{0})+\lambda^{*}(V^{c}_{\pi}(s_{ 0})-b^{\prime})\] \[\geq v^{r}_{\pi^{c}_{c}}(s_{0})+\lambda^{*}(v^{c}_{\pi^{c}_{c}}(s _{0})-(b+\triangle))\] \[=v^{r}_{\pi^{c}_{c}}(s_{0})+\lambda^{*}(\zeta-\triangle).\]

If we require \(\triangle\in(0,\frac{\zeta}{2})\), then we have

\[v^{r}_{\pi^{\bot}_{\triangle}}(s_{0}) \geq v^{r}_{\pi^{c}_{c}}(s_{0})+\lambda^{*}(\zeta-\frac{\zeta}{2})\] \[=v^{r}_{\pi^{c}_{c}}(s_{0})+\frac{\lambda^{*}\zeta}{2}\] (25)

After rearranging terms in eq. (25), we have

\[\lambda^{*}\leq\frac{2(v^{r}_{\pi^{\bot}_{\triangle}}(s_{0})-v^{r}_{\pi^{c}_{c }}(s_{0}))}{\zeta}\leq\frac{2}{\zeta(1-\gamma)}.\]

By choosing \(U=\frac{4}{\zeta(1-\gamma)}\), \(\lambda^{*}\leq U\).

**Theorem 2**: _With probability \(1-\delta\), a target \(\epsilon>0\), the mixture policy \(\bar{\pi}_{K}\) returned by confident-NPG-CMDP ensures that \(v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\bar{\pi}_{K}}(s_{0})\leq\epsilon\) and \(v^{c}_{\bar{\pi}_{K}}(s_{0})\geq b\), if assuming the misspecification error \(\omega\leq\frac{\triangle(1-\gamma)}{\tau 0(1+U)(1+\sqrt{d})}\), and if we choose \(\triangle=\frac{\epsilon(1-\gamma)\zeta}{8},\alpha=\frac{\triangle^{2}(1- \gamma)^{2}}{70^{2}(1+U)^{4}B^{2}},K=\frac{14^{2}(\sqrt{2\ln(|\mathcal{A}|)}+1) ^{2}(1+U)^{2}}{(1-\gamma)^{4}\triangle^{2}},n=\frac{(14*5)^{2}(1+p_{0})^{2} \bar{d}(1+U)^{2}}{2\triangle^{2}(1-\gamma)^{4}}\ln\left(\frac{8\bar{d}(L+1)}{ \delta}\right),H=\frac{\ln\left(\frac{14*5(1+U)\sqrt{d}}{\triangle(1-\gamma)^ {2}}\right)}{\frac{\triangle(1-\gamma)^{2}}{1-\gamma}},m=\frac{(1+U)\ln(1+p_{0 })}{2\triangle H(1-\gamma)^{2}},U=\frac{4}{\zeta(1-\gamma)}.\)_

_Furthermore, the algorithm utilizes at most \(\tilde{O}(d^{2}(1+U)^{3}(1-\gamma)^{-11}\epsilon^{-3}\zeta^{-3})\) queries in the local-access setting._

Proof: Let \(\lambda^{*}\) be the optimal dual variable that satisfies the Lagrangian primal-dual of the surrogate CMDP defined by eq. (7) (i.e., \(\lambda^{*}=\arg\min_{\lambda\geq 0}\max_{\pi}v^{r}_{\pi}(s_{0})+\lambda(v^{c}_{ \pi}(s_{0})-b^{\prime})\)).

\[v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\bar{\pi}_{K}}(s_{0})\] \[=\underbrace{\left[v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\pi^{\bot}_{ \triangle}}(s_{0})\right]}_{\text{surrogate suboptimality}}+\underbrace{\left[v^{r}_{\pi^{ \bot}_{\triangle}}(s_{0})-v^{r}_{\pi^{\bot}_{K}}(s_{0})\right]}_{\text{Confident- NPG-CMDP suboptimality}}\] \[\leq\lambda^{*}\triangle+\bar{\epsilon},\]where \(\bar{\epsilon}=\frac{5(1+U)(1+\sqrt{d})_{\omega}}{1-\gamma}+\frac{5(1+U)\sqrt{ \alpha}B}{1-\gamma}+\frac{5(1+U)\epsilon\sqrt{d}}{1-\gamma}+\frac{(\sqrt{2\ln(| \mathcal{A}|)}+1)(1+U)}{(1-\gamma)^{2}\sqrt{K}}\). By lemma 14, \(v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\pi_{\triangle}}(s_{0})\leq\lambda^{*}\triangle\). We can further upper bound \(\lambda^{*}\) by \(U=\frac{\sqrt{2\ln(|\mathcal{A}|)}+1)(1+U)}{(1-\gamma)^{2}\sqrt{K}}\). By lemma 14, \(v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\pi_{\triangle}}(s_{0})\leq\lambda^{*}\triangle\). We can further upper bound \(\lambda^{*}\) by \(U=\frac{\sqrt{2\ln(|\mathcal{A}|)}+1)(1+U)}{(1-\gamma)^{2}\sqrt{K}}\) using lemma 15 and requiring \(\triangle\in\left(0,\frac{\zeta}{2}\right)\). Together with theorem 1, we have Confident-NPG-CMDP return \(\bar{\pi}_{K}\) s.t.

\[v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\pi_{K}}(s_{0})\leq\frac{4\triangle}{\zeta(1- \gamma)}+\bar{\epsilon}\quad\text{and}\] (26)

\[b^{\prime}-V^{c}_{\bar{\pi}_{K}}(s_{0})\leq\bar{\epsilon}.\]

Now, we need to set \(\triangle\) such that 1) \(\triangle\in\left(0,\frac{\zeta}{2}\right)\) and 2) \(\triangle-\bar{\epsilon}\geq 0\) are satisfied. If we choose \(\triangle=\frac{\epsilon(1-\gamma)\zeta}{8}\), then the first condition is satisfied. This is because \(\epsilon\in\left(0,\frac{1}{1-\gamma}\right]\), and thus \(\triangle\leq\frac{\zeta}{8}<\frac{\zeta}{2}\).

Next, we check if our choice of \(\triangle=\frac{\epsilon(1-\gamma)\zeta}{8}\) satisfies \(\triangle-\bar{\epsilon}\geq 0\). For the condition \(\triangle-\bar{\epsilon}\geq 0\) to be true, we make an assumption on the misspecification error \(\omega\leq\frac{\triangle(1-\gamma)}{70(1+U)(1+\sqrt{d})},\) and pick \(n,\alpha,K,\eta_{1},\eta_{2},H,m\) to be the values outlined in this theorem. Consequently, we have \(\bar{\epsilon}=\frac{1}{2}\triangle\). Then, we have ensured the condition \(\triangle-\bar{\epsilon}\geq 0\) is satisfied.

We note that because \(\zeta\in\left(0,\frac{1}{1-\gamma}\right)\), we have \(\bar{\epsilon}\leq\frac{\epsilon}{16}\leq\epsilon\). following from eq. (26), we have \(v^{r}_{\pi^{*}}(s_{0})-v^{r}_{\pi_{K}}(s_{0})\leq\epsilon\) and \(b^{\prime}-V^{c}_{\pi_{K}}(s_{0})\leq\frac{\triangle}{2}\). Then it follows that \(b+\frac{\triangle}{2}\leq V^{c}_{\pi_{K}}(s_{0})\). Strict-feasibility is achieved.

For the query complexity, we note that our algorithm does not query the simulator in every iteration, but at fixed intervals, which we call phases. Each phase is \(m\) iterations in length. There are total of \(L=\lfloor K/(\lfloor m\rfloor+1)\rfloor\leq K/m=\tilde{O}\left((1+U)(1-\gamma) ^{-3}\triangle^{-1}\right)\) phases. In each phase, Gather-data subroutine (algorithm 3) can be run. Each time Gather-data subroutine returns with trajectories, the subroutine would have made at most \(nH\) queries. Gather-data is run for each of the element in \(\mathcal{C}_{l}\), \(l\in\left\{0,\dots,L\right\}\). By the time the algorithm terminates, all \(\mathcal{C}_{l}\)'s are the same. Since there are at most \(\tilde{O}(d)\) elements in each \(\mathcal{C}_{l}\), the algorithm will make a total of \(nH(L+1)|\mathcal{C}_{0}|\) number of queries to the simulator. Since we have \(H=\tilde{O}((1-\gamma)^{-1})\), \(n=\tilde{O}((1+U)^{2}d(1-\gamma)^{-4}\triangle^{-2})\), \(L=\tilde{O}\left((1+U)(1-\gamma)^{-3}\triangle^{-1}\right)\), and \(\triangle=\frac{\epsilon(1-\gamma)}{8}\), the sample complexity is \(\tilde{O}(d^{2}(1+U)^{3}(1-\gamma)^{-11}\epsilon^{-3}\zeta^{-3})\). 

## Appendix E A discussion on memory cost and some implementation details

By recording the states added to each core set during extensions and their corresponding least-squares weights, we can reconstruct the policy as needed. This section explains how to track this information and how it facilitates policy reconstruction.

In phase \(l\), the policies \(\pi_{k}\) for iterations \(k=k_{l}+1,\dots,k_{l+1}-1\) depend on the core set \(\mathcal{C}_{l}\). Since \(\mathcal{C}_{l}\) can be extended multiple times, these policies may change accordingly. However, we do not want to change the action distribution for states have already passed the uncertainty test in previous extensions (i.e. \(s\in\operatorname{Cov}(\mathcal{C}_{l+1})\)). For such states, action distributions are based on the least-square estimation of the core set at that time they passed the uncertainty test for the first time (i.e., \(s\in\operatorname{Cov}(\mathcal{C}_{l})\setminus\operatorname{Cov}(\mathcal{C}_ {l+1})\)). Therefore, it is essential to track newly added states in each extension and store their corresponding least-square weights to recompute their action distributions.

To achieve this, \(\mathcal{C}_{0}\) is extended only via line 7 and line 15 of algorithm 1, while other core sets \(\mathcal{C}_{l}\), where \(l\in\left\{1,\dots,L+1\right\}\) are extended solely via line 30 during the running phase \(\ell=l-1\). We mark newly added elements in line 7, line 15, and line 30. After executing line 21, we store the least-square weights associated with these newly added state-action pairs.

By keeping track of the state-action pairs that are newly added in each extension and saving the corresponding least-square weights, we can construct the policy \(\pi_{k+1}\) associated with \(\mathcal{C}_{l}\). Let \(\mathcal{C}_{l}^{0}=\emptyset\), and \(\mathcal{C}_{l}^{i}\) denote all state-action pairs added to \(\mathcal{C}_{l}\) in extension \(i\) for \(i=1\) up to at most \(\tilde{d}\). Let \(w_{k}^{i}\) represent the least-square weight computed using \(\mathcal{C}_{l}=\mathcal{C}_{l}^{0}\cup\mathcal{C}_{l}^{1}\cup\mathcal{C}_{l}^{2} \cup\dots\cup\mathcal{C}_{l}^{i}\) for the \(k\)-th iteration. When \(\mathcal{C}_{l}\) is extended for the \((i+1)\)-th time, let \(\mathcal{C}_{l}^{i+1}\) be the set of newly added state-action pairs,making the latest \(\mathcal{C}_{l}=\mathcal{C}_{l}^{0}\cup\mathcal{C}_{l}^{1}\cup\mathcal{C}_{l}^{2} \cup\cdots\cup\mathcal{C}_{l}^{i+1}\). The least-squares weight \(w_{k}^{i+1}\) is then computed using \(\mathcal{C}_{l}\). When line 27 of algorithm 1 is executed, \(\pi_{k+1}\) remains unchanged for the rest of the algorithm's execution for any states already in \(\mathrm{Cov}(\mathcal{C}_{0}^{0}\cup\mathcal{C}_{l}^{1}\cup\cdots\cup\mathcal{ C}_{l}^{i})\), equivalent to \(\mathrm{Cov}(\mathcal{C}_{l+1})\) in line 27, because line 30 would have been executed in the \(i\)-th extension, making \(\mathcal{C}_{l+1}=\mathcal{C}_{0}^{0}\cup\mathcal{C}_{l}^{1}\cup\cdots\cup \mathcal{C}_{l}^{i}\). For states in \(\mathrm{Cov}(\mathcal{C}_{0}^{0}\cup\mathcal{C}_{l}^{1}\cup\cdots\cup\mathcal{ C}_{l}^{i+1})\setminus\mathrm{Cov}(\mathcal{C}_{0}^{0}\cup\mathcal{C}_{l}^{1} \cup\cdots\cup\mathcal{C}_{l}^{i})\) (equivalent to \(\mathrm{Cov}(\mathcal{C}_{l})\setminus\mathrm{Cov}(\mathcal{C}_{l+1})\) in line 27), \(\pi_{k+1}\) makes a softmax update using \(w_{k}^{i+1}\). For all other states not in \(\mathrm{Cov}(\mathcal{C}_{l})\), the policy remains as \(\pi_{k}\).

A subroutine can start with \(\pi_{0}\), use the stored data to compute and return \(\pi_{k}(\cdot|s)\) for any \(s\) and \(k\). By tracking newly added elements and the corresponding least-square weights, the algorithm can reconstruct policies \(\pi_{0},\ldots,\pi_{K}\). This approach enables the algorithm to return the value of a mixture policy at termination.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Reviewers may find results from section 4 leading to the main results stated in section 5 and section 6. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are discussed Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The assumptions are listed in every lemma and theorems. In the supplementary section, all supporting lemmas will be proven or cited. All the lemmas are presented in sequence leading up to the two main theorems. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This paper does not include experiment Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This paper does not include experiment Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This paper does not include experiment Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This paper does not include experiment Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This paper does not include experiment Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: I have read the ethics guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: By understanding the sample complexity of CMDP, a framework used by many of the safe reinforcement learning research, we can design more efficient algorithms. This potentially has broader positive impact for real world applications. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not pose such risk as it contains no data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use any data, model or code. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not contain any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.