# On Sample-Efficient Offline Reinforcement Learning:

Data Diversity, Posterior Sampling, and Beyond

 Thanh Nguyen-Tang

Johns Hopkins University

Baltimore, MD 21218

nguyent@cs.jhu.edu

&Raman Arora

Johns Hopkins University

Baltimore, MD 21218

arora@cs.jhu.edu

###### Abstract

We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to _unify_ three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve _comparable_ sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sample complexity of the RO-based algorithm compared to the VS-based algorithm, whereas posterior sampling is rarely considered in offline RL due to its explorative nature. Notably, our proposed model-free PS-based algorithm for offline RL is _novel_, with sub-optimality bounds that are _frequentist_ (i.e., worst-case) in nature.

## 1 Introduction

Learning from previously collected experiences is a vital capability for reinforcement learning (RL) agents, offering a broader scope of applications compared to online RL. This is particularly significant in domains where interacting with the environment poses risks or high costs. However, effectively extracting valuable policies from historical datasets remains a considerable challenge, especially in high-dimensional spaces where the ability to generalize across various scenarios is crucial. In this paper, our objective is to comprehensively examine the efficiency of offline RL in the context of (value) function approximation. We aim to analyze this within the broader framework of general data collection settings.

The problem of learning from historical datasets for sequential decision-making, commonly known as _offline RL_ or _batch RL_, originated in the early 2000s (Ernst et al., 2005; Antos et al., 2006; Lange et al., 2012) and has recently regained significant attention (Levine et al., 2020; Uehara et al., 2022). In offline RL, where direct interaction with environments is not possible, our goal is to learn an effective policy by leveraging pre-collected datasets, typically obtained from different policies known as _behavior policies_. The sample efficiency of an offline RL algorithm is measured by the sub-optimality of the policies it executes compared to a "good" comparator policy, which may or may not be an optimal policy. Due to the lack of exploration inherent in offline RL, designing an algorithm with low sub-optimality requires employing the fundamental principle of _pessimistic extrapolation_. This means that the agent extrapolates from the offline data while considering the worst-case scenariosthat are consistent with that data. Essentially, the _diversity_ present in the offline data determines the agent's ability to construct meaningful extrapolations. Hence, a suitable notion of data diversity plays a crucial role in offline RL.

To address the issue of data diversity, several prior methods have made the assumption that the offline data is _uniformly diverse_ - this implies that the data should cover the entire trajectory space with some probability that is bounded from below (Munos and Szepesvari, 2008; Chen and Jiang, 2019; Nguyen-Tang et al., 2022). This assumption is often too strong and not feasible in many practical scenarios. In more recent approaches (Jin et al., 2021; Xie et al., 2021; Uehara and Sun, 2022; Chen and Jiang, 2022; Rashidinejad et al., 2023), the stringent assumption of uniform diversity has been relaxed to only require _partial_ diversity in the offline data. Various measures have been proposed to capture this partial diversity, such as single-policy concentrability coefficients (Liu et al., 2019; Rashidinejad et al., 2021; Yin and Wang, 2021), relative condition numbers (Agarwal et al., 2021; Uehara and Sun, 2022), and Bellman residual ratios (Xie et al., 2021). These measures aim to quantify the extent to which the data captures diverse states and behaviors. However, it should be noted that in some practical scenarios, these measures may become excessive or may not hold at all.

In terms of algorithmic approaches, existing sample-efficient offline RL algorithms explicitly construct pessimistic estimates of models or value functions to effectively learn from datasets with partial diversity. This is typically achieved through the construction of lower confidence bounds (LCBs) (Jin et al., 2021; Rashidinejad et al., 2021) or version spaces (VS) (Xie et al., 2021; Zanette et al., 2021). LCB-based algorithms incorporate a bonus term subtracted from the value estimates to enforce pessimism across all state-action pairs and stages. However, it has been observed that LCB-based algorithms tend to impose unnecessarily aggressive pessimism, leading to sub-optimal bounds (Zanette et al., 2021). On the other hand, VS-based algorithms search through the space of consistent hypotheses to identify the one with the smallest value in the initial states. These algorithms have demonstrated state-of-the-art bounds (Zanette et al., 2021; Xie et al., 2021).

In contrast to LCB-based and VS-based algorithms, regularized (minimax) optimization (RO) and posterior sampling (PS) are more amenable to tractable implementations but are relatively new in the offline RL literature. The RO-based algorithm initially introduced by (Xie et al., 2021; Algorithm 1) incorporates pessimism implicitly through a regularization term that promotes pessimism in the initial state. This approach eliminates the need for an intractable search over the version space. However, Xie et al. (2021) demonstrate that the RO-based algorithm exhibits a significantly slower sub-optimality rate than standard VS-based algorithms. Specifically, the RO-based algorithm achieves a sub-optimality rate of \(K^{-1/3}\), whereas VS-based algorithms achieve a faster rate of \(K^{-1/2}\), where \(K\) represents the number of episodes in the offline data.

On the other hand, posterior sampling (PS) (Thompson, 1933; Russo and Van Roy, 2014), a popular and successful method in online RL, is rarely explored in the context of offline RL. PS involves sampling from a constructed posterior distribution over the model or value function and acting accordingly. However, PS is less commonly considered in offline RL due to its explorative nature, which stems from the randomness of the posterior distribution. This randomness is well-suited for addressing the exploration challenge in online RL tasks (Zhang, 2022; Dann et al., 2021; Zhong et al., 2022; Agarwal and Zhang, 2022). The only work that considers PS for offline RL is Uehara and Sun (2022), where they maintain a posterior distribution over Markov decision process (MDP) models. However, this model-based PS approach is limited to small-scale problems where computing the optimal policy from an MDP model is computationally feasible. In addition, this work only provides a weak form of guarantees via Bayesian bounds.

In the context of (value) function approximation, achieving sample-efficient offline RL relies on certain conditions that facilitate effective learning. The identification of the minimum condition required for sample efficiency, as well as the algorithms that can exploit such conditions, is an important research question that we aim to address here. We advance our understanding by making the following contributions: (I) _We introduce a new notion of data diversity that subsumes and expands all the prior distribution shift measures in offline RL_, and (II) _We show that all VS-based, RO-based and PS-based algorithms are in fact (surprisingly) competitive to each other, i.e., under standard assumptions, they achieve the same sub-optimality bounds (up to constant and log factors)._ We summarize our key results in comparison with related work in Table 1. Our results further expand the class of sample-efficient offline RL problems (Figure 1) and provide more choices of offline RL algorithms with competitive guarantees and tractable approximations for practitioners to choose from.

For establishing (II), we need to construct concrete VS-based, RO-based and PS-based algorithms. While the key components of the VS-based and RO-based algorithms appear in the literature (Xie et al., 2021), we propose a novel, a first-of-its-kind, model-free posterior sampling algorithm for offline RL. The algorithm contains two new ingredients: a pessimistic prior that encourages pessimistic value functions when being sampled from the posterior distribution and integration of posterior sampling with the actor-critic framework that incrementally updates the learned policy.

Overview of Techniques.Our analysis method presents a "decoupling" argument tailored for the batch setting, drawing inspiration from recent decoupling arguments in the online RL setting (Foster et al., 2021; Jin et al., 2021; Zhang, 2022; Dann et al., 2021; Zhong et al., 2022; Agarwal and Zhang, 2022). The core idea behind our decoupling argument is to establish a relationship between the Bellman error under any comparator policy \(\pi\) and the squared Bellman error under the behavior policy. This relationship is mediated through our novel concept of data diversity, denoted as \(\mathcal{C}(\pi;\epsilon_{c})\), which is defined in detail in Definition 3. This allows to separate the sub-optimality of a learned policy into two main sources of errors: the extrapolation error, which captures the out-of-distribution (OOD) generalization from the behavior policy to a target policy, and the in-distribution error, which focuses on generalization within the same behavior distribution. The OOD error is effectively managed by controlling the data diversity \(\mathcal{C}(\pi;\epsilon_{c})\), while the in-distribution error is carefully addressed by utilizing the algorithmic structures and the martingale counterpart to Bernstein's inequality (i.e., Freedman's inequality).

In the process of bounding the in-distribution error of our proposed PS algorithm that we built upon the technique of Dann et al. (2021), we correct a non-rigorous argument of Dann et al. (2021) (which we discuss in detail in Section E.3.1) and develop a new technical argument to handle the statistical dependence induced by the data-dependent target policy in the actor-critic framework. Our new argument carefully incorporates the uniform convergence argument into the in-expectation bounds of PS. We give a detailed description of this argument in Section E.3. As an immediate application, our technique fixes a technical mistake involving how to handle the statistical dependence induced by the min player in the self-play posterior sampling algorithm of Xiong et al. (2022).

## 2 Background and Problem Formulation

### Episodic Time-inhomogenous Markov Decision Process

Let \(\mathcal{S}\) and \(\mathcal{A}\) denote Lebesgue-measurable state and action spaces (possibly infinite), respectively. Let \(\mathcal{P}(\mathcal{S})\) denote the space of all probability distributions over \(\mathcal{S}\). We consider an episodic time-inhomogeneous Markov decision process \(M=(\mathcal{S},\mathcal{A},P,r,H)\), where

\begin{table}
\begin{tabular}{|l|l|l|} \hline
**Algorithms** & **Sub-optimality Bound** & **Data** \\ \hline VS in (Xie et al., 2021) & \(Hb\sqrt{C_{2}(\pi)\cdot\ln(|\mathcal{F}|\cdot|\mathbb{H}^{all}|)\cdot K^{-1/2}}\) & 1 \\ \hline RO in (Xie et al., 2021) & \(Hb\sqrt{C_{2}(\pi)\cdot\sqrt[]{h(|\mathcal{F}|\cdot|\mathbb{H}^{all}(T)|)}\cdot K ^{-1/3}+Hb/\sqrt{T}}\) & 1 \\ \hline MBPS in (Uehara and Sun, 2022) & \(Hb\sqrt{C^{\text{diag}}\cdot\ln|\mathcal{M}|\cdot K^{-1/2}}\) (Bayesian) & 1 \\ \hline
**VS in Algorithm 2** & \(Hb\sqrt{C(\pi;1)/(K)\cdot\ln(|\mathcal{F}|\cdot|\mathbb{H}^{all}(T)|)\cdot K ^{-1/2}+Hb/\sqrt{T}}\) & A \\ \hline
**RO in Algorithm 3** & \(Hb\sqrt{C(\pi;1)/(K)\cdot\ln(|\mathcal{F}|\cdot|\mathbb{H}^{all}(T)|)\cdot K ^{-1/2}+Hb/\sqrt{T}}\) & A \\ \hline
**MFPS in Algorithm 4** & \(Hb\sqrt{C(\pi;1)/(K)\cdot\ln(|\mathcal{F}|\cdot|\mathbb{H}^{all}(T)|)\cdot K ^{-1/2}+Hb/\sqrt{T}}\) (frequentist) & A \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of our bounds with SOTA bounds for offline RL under partial coverage and function approximation, where gray cells mark our contributions. **Algorithms**: VS = version space, RO = regularized optimization, MBPS = model-based posterior sampling, and MFPS = model-free posterior sampling. **Sub-optimality bound**: \(K\) = #number of episodes, \(\pi\) = an _arbitrary_ comparator policy, \(H\) = horizon, \(b\) = boundedness, \(T\) = the number of algorithmic updates, \(\ln|\mathcal{F}|,\ln|\Pi^{soft}(T)|,\ln|\Pi^{all}|,\ln|\mathcal{M}|\): complexity measures of some value function class \(\mathcal{F}\), “induced” policy class \(\Pi^{soft}(T)\), the class of all comparator policies \(\Pi^{all}\), and model class \(\mathcal{M}\), where typically \(\Pi^{soft}(T)\subset\Pi^{all},\forall T\). **Data**: I = independent episodes, A = adaptively collected data. Here \(\mathcal{C}(\pi;1/\sqrt{K})\) and \(C_{2}(\pi)\) are some measures of extrapolation from the offline data to target policy \(\pi\).

\(\{\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{P}(\mathcal{S})\}^{H}\) are the transition probabilities (where \([H]:=\{1,\ldots,H\}\)), \(r=\{r_{h}\}_{h\in[H]}\in\{\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\}^{H}\) is the mean reward functions, and \(H\in\mathbb{N}\) is the length of the horizon for each episode. For any policy \(\pi=\{\pi_{h}\}_{h\in[H]}\in\{\mathcal{S}\rightarrow\mathcal{P}(\mathcal{A})\}^ {H}\), the action-value functions and the value functions under policy \(\pi\) are defined, respectively, as \(Q^{\pi}_{h,M}(s,a)=\mathbb{E}_{\pi}[\sum_{i=h}^{H}r_{i}(s_{i},a_{i})\mid(s_{h},a _{h})=(s,a)]\), and \(V^{\pi}_{h,M}(s)=\mathbb{E}_{\pi}[\sum_{i=h}^{H}r_{i}(s_{i},a_{i})|s_{h}=s]\). Here \(\mathbb{E}_{\pi}[\cdot]\) denotes the expectation with respect to the randomness of the trajectory \((s_{h},a_{h},\ldots,s_{H},a_{H})\), with \(a_{i}\sim\pi_{i}(\cdot|s_{i})\) and \(s_{i+1}\sim P_{i}(\cdot|s_{i},a_{i})\) for all \(i\). For any policy \(\pi\), we define the visitation density probability functions \(d^{\pi}_{M}=\{d^{\pi}_{h,M}\}_{h\in[H]}\in\{\mathcal{S}\times\mathcal{A} \rightarrow\mathbb{R}_{+}\}^{H}\) as \(d^{\pi}_{h}(s,a):=\frac{d\Pr(s_{h},a_{h})=(s,a)|\pi,M)}{d\rho(s,a)}\) where \(\rho\) is the Lebesgue measure on \(\mathcal{S}\times\mathcal{A}\) and \(\Pr((s_{h},a_{h})=(s,a)|\pi,M)\) is the probability of policy \(\pi\) reaching state-action \(s(s)\) at timestep \(h\). The Bellman operator \(\mathbb{T}^{\pi}_{h}\) is defined as \([\mathbb{T}^{\pi}_{h}Q](s,a):=r_{h}(s,a)+\mathbb{E}_{s^{\prime}\sim P_{h}( \cdot|s,a),a^{\prime}\sim\pi_{h+1}(\cdot|s^{\prime})}\left[Q(s^{\prime},a^{ \prime})\right]\), for any \(Q:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\). Let \(\pi^{*}\) be an optimal policy, i.e., \(Q^{\pi}_{h}\left(s,a\right)\geq Q^{\pi}_{h}(s,a),\forall(s,a,h,\pi)\in\mathcal{ S}\times\mathcal{A}\times[H]\times\Pi^{all}\), where \(\Pi^{all}:=\{\mathcal{S}\rightarrow\mathcal{P}(\mathcal{A})\}^{H}\) is the set of all possible policies. For simplicity, we assume that the initial state \(s_{1}\) is deterministic across all episodes.1 We also assume that there is some \(b>0\) such that for any trajectory \((s_{1},a_{1},r_{1},\ldots,s_{H},a_{H},r_{H})\) generated under any policy, \(|r_{h}|\leq b,\forall h\) and \(|\sum_{h=1}^{H}r_{h}|\leq b\) almost surely.2 This boundedness assumption is standard and subsumes the boundedness conditions in the previous works, e.g., Zanette et al. (2021) set \(b=1\) and Jin et al. (2021) use \(b=H\) (and further assume that \(r_{h}\in[0,1],\forall h\)).3 Without loss of generality, we assume that \(b\geq 1\).

Footnote 1: This assumption is merely for the sake of clean presentation which does not affect any results.

Footnote 2: Note that we allow the reward samples to be negative.

Additional Notation.For any \(u:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) and any \(\pi:\mathcal{S}\rightarrow\mathcal{P}(\mathcal{A})\), we overload the notation \(u(s,\pi):=\mathbb{E}_{a\sim\pi(\cdot|s)}\left[u(s,a)\right]\). For any \(f:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), denote the supremum norm \(\|f\|_{\infty}=\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}|f(s,a)|\). We write \(\mathbb{E}[g]^{2}:=(\mathbb{E}[g])^{2}\). For a probability measure \(\nu\) on some measurable space \((\Omega,\mathcal{B})\), we denote by \(\operatorname{supp}(\nu)\) the support of \(\nu\), \(\operatorname{supp}(\nu):=\{B\in\mathcal{B}:\nu(B)>0\}\). We denote \(x\lesssim y\) to mean that \(x=\mathcal{O}(y)\).

### Offline Data Generation

Denote the pre-collected dataset by \(\mathcal{D}:=\{(s^{t}_{h},a^{t}_{h},r^{t}_{h})\}_{h\in[H]}^{t\in[K]}\), where \(s^{t}_{h+1}\sim P_{h}(\cdot|s^{t}_{h},a^{t}_{h})\) and \(\mathbb{E}[r^{t}_{h}|s^{t}_{h},a^{t}_{h}]=r_{h}(s^{t}_{h},a^{t}_{h})\). We consider the adaptively collected data setting where the offline data is collected by _time-varying_ behavior policies \(\{\mu^{k}\}_{k\in[K]}\), concretely, defined as follows.

**Definition 1** (Adaptively collected data3).: \(\mu^{k}\) _is a function of \(\{(s^{i}_{h},a^{i}_{h},r^{i}_{h})\}_{h\in[H]}^{i\in[k-1]},\ \forall k\in[K]\)._

Footnote 3: We can replace the condition \(|r_{h}|\leq b,\forall h\) with \(1\)-sub-Gaussian condition: \(r_{h}\sim R_{h}(s_{h},a_{h})\) wherein \(R_{h}(s_{h},a_{h})\) is sub-Gaussian with mean \(r_{h}(s_{h},a_{h})\) – which replaces \(b\) in our main theorems by \(b+\ln(KH/\delta)\).

For simplicity, we denote \(\mu=\frac{1}{K}\sum_{k=1}^{K}\mu^{k}\), \(d^{\mu}=\frac{1}{K}\sum_{k=1}^{K}d^{\mu^{k}}\), and \(\mathbb{E}_{\mu}[\cdot]=\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{\mu^{k}}[\cdot]\). The setting of adaptively collected data covers a common practice where the offline data is collected by using some adaptive experimentation (Zhan et al., 2023). When \(\mu^{1}=\cdots=\mu^{K}\), it recovers the setting of independent episodes in Duan et al. (2020).

Value sub-optimality.The goodness of a learned policy \(\hat{\pi}=\hat{\pi}(\mathcal{D})\) against a comparator policy \(\pi\) for the underlying MDP \(M\) is measured by the (value) sub-optimality defined as

\[\operatorname{SubOpt}^{M}_{\pi}(\hat{\pi}):=V^{\pi}_{1}(s_{1})-V^{\hat{\pi}}_{1} (s_{1}).\] (1)

Whenever the context is clear, we drop \(M\) in \(Q^{\pi}_{M}\), \(V^{\pi}_{M}\), \(d^{\pi}_{M}\), and \(\operatorname{SubOpt}^{M}_{\pi}(\hat{\pi})\).

### Policy and function classes

Next, we define the policy space and the action-value function space over which we optimize the value sub-optimality. We consider a (Cartesian product) function class \(\mathcal{F}=\mathcal{F}_{1}\times\cdots\times\mathcal{F}_{H}\in\{\mathcal{S} \times\mathcal{A}\rightarrow[-b,b]\}^{H}\). The function class \(\mathcal{F}\) induces the following (Cartesian product) policy class \(\Pi^{soft}(T)=\Pi^{soft}_{1}(T)\times\cdots\times\Pi^{soft}_{H}(T)\), where \(\Pi^{soft}_{h}(T):=\{\pi_{h}(a|s)\propto\exp(\eta\sum_{i=1}^{t}g_{i}(s,a)):t\in[T ],g_{i}\in\mathcal{F}_{h},\forall i\in[t],\eta\in[0,1]\}\) for any \(T\in\mathbb{N}\). The motivation for the induced policy class \(\Pi^{soft}(T)\) is from the soft policy iteration (SPI) update where we incrementally update the policy.

We now discuss a set of assumptions that we impose on the policy and function classes.

**Assumption 2.1** (Approximate realizability).: _There exist \(\{\xi_{h}\}_{h\in[H]}\) where \(\xi_{h}\geq 0\) such that,_

\[\sup_{T\in\mathbb{N},\pi\in\Pi^{soft}(T),(s_{h},a_{h})\in\operatorname{supp}(d ^{\pi}_{h})}\inf_{f\in\mathcal{F}}|f_{h}(s_{h},a_{h})-Q^{\pi}_{h}(s_{h},a_{h})| \leq\xi_{h},\ \ \forall h\in[H].\]

Assumption 2.1 establishes that \(\mathcal{F}\) can realize \(Q^{\pi}\) for any \(\pi\in\Pi^{soft}(T)\) up to some error \(\xi\in\mathbb{R}^{H}\) in the supremum norm over the \(\mu\)-feasible state-action pairs. It strictly generalizes the assumption in Zanette et al. (2021) which restricts \(\xi_{h}=0,\ \forall h\) (i.e., assume realizability) and the assumption in Xie et al. (2021) which constrains the approximation error under any feasible state-action distribution.

The realizability in _value_ functions alone is known to be insufficient for sample-efficient offline RL (Wang et al., 2021); thus, one needs to impose a stronger assumption for polynomial sample complexity of model-free methods.5 In this paper, we impose an assumption on the closedness of the Bellman operator.

Footnote 5: A stronger form of realizability is sufficient for polynomial sample complexity, e.g., realizability for a density ratio w.r.t. the behavior state-action distribution in dual-primal methods (Zhan et al., 2022; Chen and Jiang, 2022; Rashidinejad et al., 2023) or realizability for the underlying MDP in model-based methods (Uehara and Sun, 2022). Instead, we pursue model-free value-based methods.

**Assumption 2.2** (General Restricted Bellman Closedness).: _There exists \(\nu\in\mathbb{R}^{H}\) such that_

\[\sup_{T\in\mathbb{N},f_{h+1}\in\mathcal{F}_{h+1},\tilde{\pi}\in\Pi^{soft}(T)} \inf_{f_{h}^{\prime}\in\mathcal{F}_{h}}\|f_{h}^{\prime}-\mathbb{T}_{h}^{\tilde {\pi}}f_{h+1}\|_{\infty}\leq\nu_{h},\ \ \forall h\in[H].\]

Assumption 2.2 ensures that the value function space \(\mathcal{F}\) and the induced policy class \(\Pi^{soft}(T)\) for any \(T\in\mathbb{N}\) are closed under the Bellman operator up to some error \(\nu\in\mathbb{R}^{H}\) in the supremum norm. This assumption is a direct generalization of the Linear Restricted Bellman Closedness in Zanette et al. (2021) from a linear function class to a general function class. As remarked by Zanette et al. (2021), the Linear Restricted Bellman Closedness is already strictly more general than the low-rank MDPs (Yang and Wang, 2019; Jin et al., 2020).

### Effective sizes of policy and function classes

When the function class and the policy class have finite elements, we use their cardinality \(|\mathcal{F}_{h}|\) and \(|\Pi^{soft}_{h}(T)|\) to measure their sizes (Jiang et al., 2017; Xie et al., 2021). When they have infinite elements, we use log-covering numbers, defined as

\[d_{\mathcal{F}}(\epsilon):=\max_{h\in[H]}\ln N(\epsilon;\mathcal{F}_{h},\| \cdot\|_{\infty}),\ \text{and}\ d_{\Pi}(\epsilon,T):=\max_{h\in[H]}\ln N(\epsilon;\Pi^{soft}_{h}(T),\|\cdot\|_{1,\infty}),\]

where \(\|\pi-\pi^{\prime}\|_{1,\infty}=\sup_{s\in\mathcal{S}}\int_{\mathcal{A}}|\pi( a|s)-\pi^{\prime}(a|s)|d\rho(a)\) for any \(\pi,\pi^{\prime}\in\{\mathcal{S}\to\mathcal{P}(\mathcal{A})\}\) and \(N(\epsilon;\mathcal{X},\|\cdot\|)\) denotes the covering number of a pseudometric space \((\mathcal{X},\|\cdot\|)\) with metric \(\|\cdot\|\)(Zhang, 2023, e.g. Definition 4.1).

We also define a complexity measure that depends on a prior distribution \(p_{0}\) over \(\mathcal{F}\) that we employ to favor certain regions of the function space. Our notion, presented in Definition 2, is simply a direct adaptation of a similar notation of Dann et al. (2021) to the actor-critic setting.

**Definition 2**.: _For any function \(f^{\prime}\in\mathcal{F}_{h+1}\) and any policy \(\tilde{\pi}\in\Pi^{all}\), we define \(\mathcal{F}_{\tilde{h}}^{\tilde{\pi}}(\epsilon;f^{\prime}):=\{f\in\mathcal{F} _{h}:\|f-\mathbb{T}_{h}^{\tilde{\pi}}f^{\prime}\|_{\infty}\leq\epsilon\}\), for any \(\epsilon\geq 0\), and subsequently define_

\[d_{0}(\epsilon):=\sup_{T\in\mathbb{N},f\in\mathcal{F},\tilde{\pi}\in\Pi^{soft}(T )}\sum_{h=1}^{H}\ln\frac{1}{p_{0,h}(\mathcal{F}_{h}^{\tilde{\pi}}(\epsilon;f_{h +1}))},d^{\prime}_{0}(\epsilon):=\sup_{T\in\mathbb{N},\tilde{\pi}\in\Pi^{soft}(T )}\sum_{h=1}^{H}\ln\frac{1}{p_{0,h}(\mathcal{F}_{h}^{\tilde{\pi}}(\epsilon;Q^ {\tilde{\pi}}_{h+1}))}.\]

The quantity \(d_{0}(\epsilon)\) and \(d^{\prime}_{0}(\epsilon)\) measures the concentration of the prior \(p_{0}\) over all functions \(f\in\mathcal{F}\) that are \(\epsilon\)-close (element-wise) under \(\mathbb{T}^{\tilde{\pi}}\) and \(\epsilon\)-close (element-wise) to \(Q^{\tilde{\pi}}_{h}\), respectively. If a stronger version of Assumption 2.1 is met, i.e., \(Q^{\tilde{\pi}}_{h}\in\mathcal{F}_{h},\forall\tilde{\pi}\in\Pi^{all}_{h},h\in[H]\), we have \(d^{\prime}_{0}(\epsilon)\leq d_{0}(\epsilon),\forall\epsilon\). For the finite function class \(\mathcal{F}\) and an uninformative prior \(p_{0,h}(f_{h})=1/|\mathcal{F}_{h}|\), under a stronger version of Assumption 2.2, i.e., \(\nu_{h}=0,\forall h\), we have \(d_{0}(\epsilon)\leq\sum_{h=1}^{H}\ln|\mathcal{F}_{h}|=\ln|\mathcal{F}|.\) For a parametric model, where each \(f_{h}=f_{h}^{\theta}\) is represented by a \(d\)-dimensional parameter \(\theta\in\Omega_{h}^{\theta}\subset\mathbb{R}^{d}\), a prior over \(\Omega^{\theta}\) induces a prior over \(\mathcal{F}\). If each \(\Omega_{h}^{\theta}\) is compact, we can generally assume the prior that satisfies \(\sup_{\theta}\ln\frac{1}{p_{0,h}(\theta^{\prime}:\|\theta-\theta^{\prime}\| \leq\epsilon)}\leq d\ln(c_{0}/\epsilon)\) for some constant \(c_{0}\). If \(f_{h}=f_{h}^{\theta}\) is Lipschitz in \(\theta\), we can assume that \(\sup_{\theta}\ln\frac{1}{p_{0,h}(\theta^{\prime}:\|\theta-\theta^{\prime}\| \leq\epsilon)}\leq c_{1}d\ln(c_{2}/\epsilon)\) for some constants \(c_{1},c_{2}\). Overall, we can assume that \(d_{0}(\epsilon)\leq c_{1}Hd\ln(c_{2}/\epsilon)\). A similar discussion can be found in Dann et al. (2021).

## 3 Algorithms

Next, we present concrete instances of PS-based, RO-based, and VS-based algorithms. The RO-based and VS-based algorithms presented here are slight refinements of their original versions in Xie et al. (2021). The PS-based algorithm is novel. All three algorithms resemble the actor-critic style update, inspired by Zanette et al. (2021). We refer to this generic framework as GOPO (Generic Offline Policy Optimization) presented in Algorithm 1. At each round \(t\), a critic estimates the value \(Q_{h}^{t}\) of the actor (i.e., policy \(\pi^{t}\)) using the procedure

```
1:Offline data \(\mathcal{D}\), function class \(\mathcal{F}\), learning rate \(\eta>0\), and iteration number \(T\)
2:Uniform policy \(\pi^{1}=\{\pi_{h}^{1}\}_{h\in[H]}\)
3:for\(t=1,\dots,T\)do
4:\(Q^{t}=\big{[}\text{CriticCompute}\big{]}(\pi^{t},\mathcal{D},\mathcal{F},\dots)\)
5:\(\pi_{h}^{t+1}(a|s)\propto\pi_{h}^{t}(a|s)\exp(\eta Q_{h}^{t}(s,a)),\forall( s,a,h)\)
6:endfor
7:\(\hat{\pi}\sim\mathrm{Uniform}(\{\pi^{t}\}_{t\in[T]})\) ```

**Algorithm 1** GOPO\((\mathcal{D},\mathcal{F},\eta,T,\)CriticCompute\()\): Generic Offline Policy Optimization Framework

To incorporate the pessimism principle, a critic should generate pessimistic estimates of the value of the actor \(\pi^{t}\) in Line 3. This is where the three approaches differ - each invokes a different method to compute the critic. Here, we provide a detailed description of the critic module for each approach. To aid the presentation, we introduce the total temporal difference (TD) loss \(\hat{L}_{\tilde{\pi}}\), defined as \(\hat{L}_{\tilde{\pi}}(f_{h},f_{h+1}):=\sum_{k=1}^{K}l_{\tilde{\pi}}(f_{h},f_{h +1};z_{h}^{k})\), where \(z_{h}:=(s_{h},a_{h},r_{h},s_{h+1})\), \(z_{h}^{k}:=(s_{h}^{k},a_{h}^{k},r_{h}^{k},s_{h+1}^{k})\), and \(l_{\tilde{\pi}}(f_{h},f_{h+1};z_{h}):=(f_{h}(s_{h},a_{h})-r_{h}-f_{h+1}(s_{h+1},\tilde{\pi}))^{2}\).

Version Space-based Critic (VSC) (Algorithm 2).Given the actor \(\pi^{t}\), at each step \(h\in[H]\), VSC directly maintains a local regression constraint using the offline data: \(\hat{L}_{\pi^{t}}(f_{h},f_{h+1})\leq\inf_{g\in\mathcal{F}}\hat{L}_{\pi^{t}}(g _{h},f_{h+1})+\beta\), where \(\beta\) is a confidence parameter and \(\hat{L}_{\pi^{t}}(\cdot,\cdot)\) is serving as a proxy to the squared Bellman residual at step \(h\). By taking the function that minimizes the initial value, VSC then finds the most pessimistic value function \(\underline{Q}^{t}\) from the version space \(\mathcal{F}(\beta;\pi^{t})\subseteq\mathcal{F}\). In general, the constrained optimization in Line 2 is computationally intractable. Note that a minimax variant of GOPO+VSC first appeared in Xie et al. (2021), where they directly perform an (intractable) search over the policy space, instead of using the multiplicative weights algorithm (Line 4) of Algorithm 1.

Regularized Optimization-based Critic (ROC) (Algorithm 3).Instead of solving the global constrained optimization in VSC, ROC solves \(\arg\inf_{f\in\mathcal{F}}\left\{\lambda f_{1}(s_{1},\pi_{1}^{t})+\mathcal{L}_ {\pi^{t}}(f)\right\}\), where \(\lambda\) is a regularization parameter and \(\mathcal{L}_{\pi^{t}}(f)\), defined in Line 1 of Algorithm 3. Note that in ROC, pessimism is implicitly encouraged through the regularization term \(\lambda f_{1}(s_{1},\pi^{t})\). We remark that, unlike VSC, ROC admits tractable approximations that use adversarial training and work competitively in practice (Cheng et al., 2022). Note that a discounted variant of GOPO-ROC first appears in (Xie et al., 2021) in discounted MDPs.

```
1:\(\mathcal{L}_{\pi^{t}}(f):=\sum_{h=1}^{H}\hat{L}_{\pi^{t}}(f_{h},f_{h+1})\) \(-\inf_{g\in\mathcal{F}}\sum_{h=1}^{H}\hat{L}_{\pi^{t}}(g_{h},f_{h+1})\)
2:\(\underline{Q}^{t}\leftarrow\arg\inf_{f\in\mathcal{F}}\left\{\lambda f_{1}(s_{1}, \pi^{t})+\mathcal{L}_{\pi^{t}}(f)\right\}\) ```

**Algorithm 2** VSC(\(\mathcal{D},\mathcal{F},\pi^{t},\lambda\)): \(\text{Version Space-based Critic}\)

```
Posterior sampling-based critic (PSC) in Algorithm 4.Instead of solving a regularized minimax optimization, PSC samples the value function \(Q_{h}^{t}\) from the data posterior \(\hat{\pi}(f|\mathcal{D},\pi^{t})\propto\bar{p}_{0}(f)\cdot p(\mathcal{D}|f,\pi^ {t})\), where \(\bar{p}_{0}(f)\) is the prior over \(\mathcal{F}\) and \(\overline{p(\mathcal{D}|f,\pi^{t})}\) is the likelihood function of the offline data \(\mathcal{D}\). To formulate the likelihood function \(p(\mathcal{D}|f,\pi^{t})\), we make use of the squared TD error \(\hat{L}_{\pi^{t}}(\cdot,\cdot)\) and normalization method in (Dann et al., 2021) to construct an unbiased proxy of the squared Bellman errors. In particular, \(p(\mathcal{D}|f,\pi^{t})=\prod_{h\in[H]}\frac{\exp(-\gamma\hat{L}_{\pi^{t}}(f_ {h},f_{h+1}))}{\mathbb{E}_{f_{h}\sim p_{0,h}}\exp(-\gamma\hat{L}_{\pi^{t}}(f_ {h}^{t},f_{h+1}))}\), where \(\gamma\) is a learning rate and \(p_{0}\) is an (unregularized) prior over \(\mathcal{F}\). A value function sampled from the posterior with this likelihood function is encouraged to have small squared TD errors. The key ingredient in our algorithmic design is the "pessimistic" prior \(\bar{p}_{0}(f)=\exp(-\lambda f_{1}(s_{1},\pi_{1}))p_{0}(f)\) where we add a new regularization term \(\exp(-\lambda f_{1}(s_{1},\pi_{1}))\), with \(\lambda\) being a regularization parameter - which is inspired by the optimistic prior in the online setting (Zhang, 2022; Dann et al., 2021). This pessimistic prior encourages the value function sampled from the posterior to have a small value in the initial state, implicitly enforcing pessimism. We remark that PSC requires a sampling oracle and expectation oracle (to compute the normalization term in the posterior distribution), which could be amenable to tractable approximations, including replacing expectation oracle with a sampling oracle (Agarwal and Zhang, 2022) while the sampling oracle can be implemented via first-order sampling methods (Welling and Teh, 2011) or ensemble methods (Osband et al., 2016).

```
1:\(\underline{Q}^{t}\sim\hat{p}(f|\mathcal{D},\pi^{t})\propto\exp\left(-\lambda f _{1}(s_{1},\pi^{t})\right)p_{0}(f)\prod_{h\in[H]}\frac{\exp\left(-\gamma\hat{ L}_{\pi^{t}}(f_{h},f_{h+1})\right)}{\mathbb{E}_{f_{h}\sim p_{0,h}}\exp\left(- \gamma\hat{L}_{\pi^{t}}(f_{h}^{t},f_{h+1})\right)}\)
2:\(\underline{Q}^{t}\) ```

**Algorithm 4** PSC\((\mathcal{D},\mathcal{F},\pi^{t},\lambda,\gamma,p_{0})\): Posterior Sampling-based critic

## 4 Main Results

In this section, we shall present the upper bounds of the sub-optimality of the policies executed by GOPO-VSC, GOPO-ROC, and GOPO-PSC. Our upper bounds are expressed in terms of a new notion of data diversity.

### Data diversity

We now introduce the key notion of data diversity for offline RL. Since the offline learner does not have direct access to the trajectory of a comparator policy \(\pi\in\Pi^{all}\), they can only observe partial information about the goodness of \(\pi\) channeled through the "transferability" with the behavior policy \(\mu\). The transferability from \(\mu\) to \(\pi\) depends on how _diverse_ the offline data induced by \(\mu\) can be in supporting the extrapolation to \(\pi\). Many prior works require uniform diversity where \(\mu\) covers all feasible scenarios of all comparator policies \(\pi\). The data diversity can be essentially captured by how well the Bellman error under the state-action distribution induced by \(\mu\) can predict the counterpart quantity under the state-action distribution induced by \(\pi\). Our notion of data diversity, which is inspired by the notion of task diversity in transfer learning literature (Tripuraneni et al., 2020; Watkins et al., 2023), essentially encodes the ratio of some proxies of expected Bellman errors induced by \(\mu\) and \(\pi\), and is defined as follows.

**Definition 3**.: _For any comparator policy \(\pi\in\Pi^{all}\), we measure the data diversity of the behavior policy \(\mu\) with respect to a target policy \(\pi\) by_

\[\mathcal{C}(\pi;\epsilon):=\max_{h\in[H]}\chi_{(\mathcal{F}_{h}- \mathcal{F}_{h})}(\epsilon;d_{h}^{\pi},d_{h}^{\mu}),\forall\epsilon\geq 0,\] (2)

_where \(\mathcal{F}_{h}-\mathcal{F}_{h}\) is the Minkowski difference between the function class \(\mathcal{F}_{h}\) and itself, i.e., \(\mathcal{F}_{h}-\mathcal{F}_{h}:=\{f_{h}-f_{h}^{\prime}:f_{h},f_{h}^{\prime}\in \mathcal{F}\}\), and \(\chi_{\mathcal{Q}}(\epsilon;q,p)\) is the discrepancy between distributions \(q\) and \(p\) under the witness of function class \(\mathcal{Q}\) defined as_

\[\chi_{\mathcal{Q}}(\epsilon;q,p)=\inf\left\{C\geq 0:(\mathbb{E}_{q}[g])^{2} \leq C\cdot\mathbb{E}_{p}[g^{2}]+\epsilon,\forall g\in\mathcal{Q}\right\}\]

_with \(\mathcal{Q}\) being a function class and \(p\) and \(q\) being two distributions over the same domain._

Up to a small additive error \(\epsilon\), a finite \(\mathcal{C}(\pi;\epsilon)\) ensures that a proxy of the Bellman error under the \(\pi\)-induced state-action distribution is controlled by that under the \(\mu\)-induced state-action distribution.

Despite the abstraction in the definition of this data diversity, it is _always_ upper bounded by the single-policy concentrability coefficient (Liu et al., 2019; Rashidinejad et al., 2021) and the relative condition number (Agarwal et al., 2021; Uehara et al., 2022; Uehara and Sun, 2022) that are both commonly used in many prior offline RL works. We further discuss our data diversity measure in more detail in Section 4.2.

### Offline learning guarantees

We now utilize data diversity to give learning guarantees of the considered algorithms for extrapolation to an arbitrary comparator policy \(\pi\in\Pi^{all}\). To aid the representation, in all of the following theorems we are about to present, we shall set \(\eta=\sqrt{\frac{\ln\operatorname{Vol}(\mathcal{A})}{4(e-2)b^{2}T}}\) in Algorithm 1, where \(\operatorname{Vol}(\mathcal{A})\) is the volume of the action set \(\mathcal{A}\) (e.g., \(\operatorname{Vol}(\mathcal{A})=|\mathcal{A}|\) for finite \(\mathcal{A}\)), and define, for simplicity, the misspecification errors \(\zeta_{msp}:=K\sum_{h=1}^{H}\left(\nu_{h}^{2}+b\nu_{h}\right)\), \(\tilde{\zeta}_{msp}:=\zeta_{msp}+bK\sum_{h=1}^{H}\xi_{h}\), \(\bar{\nu}:=\sum_{h=1}^{H}\nu_{h}\), the optimization error \(\zeta_{opt}:=Hb\sqrt{T^{-1}\ln\operatorname{Vol}(\mathcal{A})}\), and the complexity measures \(\tilde{d}_{opt}(\epsilon,T):=\max\{d_{\mathcal{F}}(\epsilon),d_{\Pi}(\epsilon, T)\}\), and \(\tilde{d}_{ps}(\epsilon,T):=\max\{d_{\mathcal{F}}(\epsilon),d_{\Pi}(\epsilon,T), \frac{d_{0}(\epsilon)}{\gamma Hb^{2}},\frac{d_{0}^{\prime}(\epsilon)}{\gamma Hb ^{2}}\}\).

``` Theorem 1 (Guarantees for GOPO-VSC). Let \(\hat{\pi}^{vs}\) be the output of Algorithm 1 invoked with \(\boxed{CriticCompute}\) being VSC(\(\mathcal{D},\mathcal{F},\pi^{t},\beta\)) (Algorithm 2) with \(\beta=\mathcal{O}(Hb^{2}\max\{\tilde{d}_{opt}(\epsilon,T),\ln(H/\delta)\}+b^{ 2}K\epsilon+bK\max_{h\in[H]}\xi_{h})\). Fix any \(\delta\in(0,1]\). Under Assumption 2.1-2.2, with probability at least \(1-2\delta\) (over the randomness of the offline data), for any \(\epsilon,\epsilon_{c},\lambda>0\), and any \(\pi\in\Pi^{all}\), we have \[\mathbb{E}\left[\operatorname{SubOpt}_{\pi}(\hat{\pi}^{vs})| \mathcal{D}\right] \lesssim\frac{Hb^{2}\cdot\max\{\tilde{d}_{opt}(\epsilon,T),\ln(H/ \delta)\}+b^{2}KH\epsilon+\tilde{\zeta}_{msp}}{\lambda}+\frac{\lambda H\cdot \mathcal{C}(\pi;\epsilon_{c})}{2K}\] \[+H\epsilon_{c}+\xi_{1}+\bar{\nu}+\zeta_{opt}.\] ```

**Theorem 2** (Guarantees for GOPO-ROC).: _Let \(\hat{\pi}^{ro}\) be the output of Algorithm 1 invoked with \(\epsilon,\epsilon_{c},\lambda>0\), and any \(\pi\in\Pi^{all}\), we have_

\[\mathbb{E}\left[\operatorname{SubOpt}_{\pi}(\hat{\pi}^{ro})| \mathcal{D}\right] \lesssim\frac{Hb^{2}\cdot\max\{\tilde{d}_{opt}(\epsilon,T),\ln \frac{H}{\delta}\}+b^{2}KH\epsilon+\tilde{\zeta}_{msp}}{\lambda}+\frac{\lambda H \cdot\mathcal{C}(\pi;\epsilon_{c})}{2K}\] \[+H\epsilon_{c}+\xi_{1}+\bar{\nu}+\zeta_{opt}.\]

``` Theorem 3 (Guarantees for GOPO-PSC). Let \(\hat{\pi}^{ps}\) be the output of Algorithm 1 invoked with \(\boxed{CriticCompute}\) being PSC(\(\mathcal{D},\mathcal{F},\pi^{t},\lambda,\gamma,p_{0}\)) (Algorithm 4). Under Assumption 2.2, for any \(\gamma\in[0,\frac{1}{144(e-2)b^{2}}]\), and \(\epsilon,\epsilon_{c},\delta,\lambda>0\), and any \(\pi\in\Pi^{all}\), we have \[\mathbb{E}\left[\operatorname{SubOpt}_{\pi}(\hat{\pi}^{ps})\right] \lesssim\frac{\gamma Hb^{2}\cdot\max\{\tilde{d}_{ps}(\epsilon,T), \ln\frac{\ln Kb^{2}}{\delta}\}+\gamma b^{2}KH\cdot\max\{\epsilon,\delta\}+ \gamma\zeta_{msp}}{\lambda}\] \[+\frac{\lambda H\cdot\mathcal{C}(\pi;\epsilon_{c})}{K\gamma}+H \epsilon_{c}+\epsilon+\bar{\nu}+\zeta_{opt}.\]

Our results provide a family of upper bounds on the sub-optimality of each of \(\{\hat{\pi}^{vs},\hat{\pi}^{ro},\hat{\pi}^{ps}\}\), indexed by our choices of the comparator \(\pi\) with the data diversity \(\mathcal{C}(\pi;\epsilon_{c})\), additive (extrapolation) error \(\epsilon_{c}\), the discretization level \(\epsilon\) in log-covering numbers, the "failure" probability \(\delta\), and other algorithm-dependent parameters (\(\lambda\) for \(\hat{\pi}^{ro}\) and (\(\lambda\), \(\gamma\)) for \(\hat{\pi}^{ps}\)). Note that the optimization error \(\zeta_{opt}\) captures the error rate of the actor and can be made arbitrarily small with large iteration number \(T\) whereas \(\zeta_{msp}\), \(\tilde{\zeta}_{msp}\), \(\bar{\nu}\), and \(\xi_{1}\) are simply misspecification errors aggregated over all stages. Also note that our bound does not scale with the complexity of the comparator policy class \(\Pi^{all}\). We next highlight the key characteristics of our main results in comparison with existing work.

**(I) Tight characterization of data diversity.** Our bounds in all the above theorems are expressed in terms of \(\mathcal{C}(\pi;\epsilon_{c})\). Several remarks are in order. First, \(\mathcal{C}(\pi;\epsilon_{c})\) is a _non-increasing_ function of \(\epsilon_{c}\); thus \(\mathcal{C}(\pi;\epsilon_{c})\) is always smaller or at least equal to \(\mathcal{C}(\pi;0)\). In fact, it is possible that \(\mathcal{C}(\pi;0)=\infty\) yet \(\mathcal{C}(\pi;\epsilon_{c})<\infty\) for some \(\epsilon>0\). For instance, if there exists \(g\in\mathcal{Q}\) such that \(g(x)=0,\forall x\in\operatorname{supp}(p)\) and \(\{x:g(x)\neq 0\}\) has a positive measure under \(q\), then \(\chi_{\mathcal{Q}}(0;q,p)=\infty\) while \(\chi_{\mathcal{Q}}(\sup_{g\in\mathcal{Q}}\mathbb{E}_{q}[g]^{2};q,p)=0\). Second, \(\mathcal{C}(\pi;0)\) is always bounded from above by (often substantially smaller than) the _single-policy concentrability coefficient_ between the \(\pi\)-induced and \(\mu\)-induced state-action distribution (Liu et al., 2019; Rashidinejad et al., 2021), which been used extensively in recent offline RL works (Yin and Wang, 2021; Nguyen-Tang et al., 2022; Jin et al., 2022; Zhan et al., 2022; Nguyen-Tang and Arora, 2023; Zhao et al., 2023). This is essentially because \(d^{\pi}\) can cover the region that is not covered by \(d^{\mu}\) but still the integration of functions in \(\mathcal{F}_{h}-\mathcal{F}_{h}\) over two distributions are close to each other. Third, \(\mathcal{C}(\pi;0)\) is always upper bounded by the _relative condition numbers_ used in (Agarwal et al., 2021; Uehara et al., 2022; Uehara and Sun, 2022). Our data diversity at \(\epsilon=0\) is similar to the notion of distribution mismatch in Duan et al. (2020), Ji et al. (2022), though our notion is motivated by transfer learning and discovered naturally from our decoupling argument. Our data diversity measure at \(\epsilon=0\) is smaller than the Bellman residual ratio measure used in Xie et al. (2021) (follows using Jensen's inequality). Finally, the concurrent work of Di et al. (2023) proposed a notion of \(D^{2}\)-divergence to capture the data disparity of a data point to the offline data. Our data diversity is in general less restricted as we only need to ensure the diversity between two data distributions (of the target policy and the behavior policy), not necessarily between each of their individual data points.

In summary, \(\mathcal{C}(\pi;\epsilon_{c})\), to the best of our knowledge, provides the tightest characterization of distribution mismatch compared to the prior data coverage notions. We sketch the relationships of the discussed notions in Figure 1, where with our data diversity notion, we show that the scenarios for the offline data in which offline RL is learnable are enlarged compared to the picture depicted by the prior data coverage notions.

**(II) Competing with all comparator policies simultaneously.** Similar to some recent results in offline RL, our offline RL algorithms compete with all comparator policies that are supported by offline data in some sense. In particular, the choice of the comparator \(\pi\) provides the flexibility to _automatically_ compete with the best policy within a certain diversity level of our choice. For instance, if we want to limit the level \(\mathcal{C}(\pi;\epsilon_{c})\leq C\) for some arbitrary \(C>0\), our bound automatically competes with \(\pi=\arg\max_{\pi\in\Pi^{\text{all}}}\{V_{1}^{\pi}(s):\mathcal{C}(\pi;\epsilon_ {c})\leq C\}\). This is immensely meaningful since the offline data might not support extrapolation to an optimal policy in practice.

**(III) State-of-the-art bounds for standard assumptions.** We compare our bounds with other recent guarantees of similar assumptions.6 To ease comparison, we assume for simplicity, that there is no misspecification, i.e., \(\nu_{h}=\xi_{h}=0,\forall h\in[H]\), and \(T\geq K\ln\operatorname{Vol}(\mathcal{A})\), and we minimize the bounds in Theorem 2 and Theorem 3 with respect to \(\lambda\). The three theorems can then be simplified into a unified result presented in Proposition 1.

Footnote 6: Recent primal-dual methods achieve favorable guarantees for offline RL. However, these guarantees are not directly comparable to the guarantees of our value-based methods due to a different set of assumptions. Nonetheless, we make a detailed discussion in Section A.2.

**Proposition 1** (A unified guarantee for VS, RO and PS).: _Under Assumption 2.1-2.2 with no misspecification, i.e., \(\nu_{h}=\xi_{h}=0,\forall h\in[H]\), \(\forall\hat{\pi}\in\{\hat{\pi}^{vs},\hat{\pi}^{ro},\hat{\pi}^{ps}\}\), \(\mathbb{E}[\operatorname{SubOpt}_{\pi}(\hat{\pi})]=\tilde{\mathcal{O}}(Hb \sqrt{\tilde{d}(1/K,T)\cdot\mathcal{C}(\pi;1/\sqrt{K})/K}+\xi_{opt})\), where \(\tilde{d}(1/K,T)=\tilde{d}_{opt}(1/K,T)\) if \(\hat{\pi}\in\{\hat{\pi}^{vs},\hat{\pi}^{ro}\}\) and \(\tilde{d}(1/K,T)=\tilde{d}_{ps}(1/K,T)\) if \(\hat{\pi}=\hat{\pi}^{ps}\). In addition,_

Figure 1: The relations of sample-efficient offline RL classes under different data coverage measures. Given the same MDP and a target policy (e.g., an optimal policy of the MDP), each data coverage measure induces a corresponding set of behavior policies (represented by the rectangle labelled by the data coverage measure) from which the target policy is offline-learnable.

* _If_ \(\mathcal{F}_{h}\) _and_ \(\Pi_{h}^{soft}(T)\) _have finite elements for all_ \(h\in[H]\)_,_ \(\tilde{d}(1/K,T)=\mathcal{O}(\max_{h\in[H]}\max\{\ln|\mathcal{F}_{h}|,\ln|\Pi_{h }^{soft}(T)|\})\)_;_
* _If_ \(\mathcal{F}_{h}=\{(s,a)\mapsto(\phi_{h}(s,a),w):\|w\|_{2}\leq b\}\) _is a linear model, where_ \(\phi_{h}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{d}\) _is a known feature map and w.l.o.g._ \(\max_{h}\|\phi_{h}\|_{\infty}\leq 1\)_,_ \(\tilde{d}(1/K,T)=\mathcal{O}(d\log(1+KTb)),\forall T\)_._

Proposition 1 essentially asserts that VS-based, RO-based, and PS-based algorithms obtain comparable guarantees for offline RL in the realizable case. We now compare our results to related work in various instantiation of function classes.

**Compared with Xie et al. (2021) when the function class is finite.** In this case, the analysis of the VS-based algorithms and RO-based algorithms of Xie et al. (2021) give the bounds that in our setting can be translated7 into: \(Hb\sqrt{\max_{h}\ln(|\mathcal{F}_{h}||\Pi_{h}^{soft}|)\cdot C_{2}(\pi)/K}\) and \(Hb\sqrt{C_{2}(\pi)}\sqrt[3]{\max_{h}\ln(|\mathcal{F}_{h}||\Pi_{h}^{soft}(T)|)/K }+Hb/\sqrt{T}\), respectively, where \(C_{2}(\pi):=\max_{h\in[H],\in\Pi^{all},f\in\mathcal{F}}\frac{\|f_{h}-\pi_{h}^{ \star}f_{h+1}\|_{2}^{2}\cdot\pi}{\|f_{h}-\pi_{h}^{\star}f_{h+1}\|_{2}^{2}\cdot \pi}\). Instead, our bounds for both the VS-based and RO-based algorithms are \(Hb\sqrt{\max_{h}\ln(|\mathcal{F}_{h}||\Pi_{h}^{soft}(T)|)\cdot\mathcal{C}(\pi ;1/\sqrt{K})/K}+Hb/\sqrt{T}\). We improve upon the results of Xie et al. (2021) on several fronts. First, our diversity measures \(\mathcal{C}(\pi;1/K)\) is always smaller than their measure \(C_{2}(\pi)\), since \(\mathcal{C}(\pi;1/\sqrt{K})\leq\mathcal{C}(\pi;0)\leq C_{2}(\pi)\). Second, for the VS-based algorithm, \(\Pi^{soft}(T)\subset\Pi^{all},\forall T\), our bound is always tighter. In fact, \(|\Pi^{all}|\) is arbitrarily large that bounds depending on this quantity is vacuous. Third, for the RO-based algorithm, the rates in terms of \(K\) in the bound of Xie et al. (2021) are slower than that in our bound. Specifically, if \(\Pi_{h}^{soft}(T)=\tilde{\mathcal{O}}_{T}(1)\), then these rates are \(K^{-1/3}\) vs \(K^{-1/2}\) (with an optimal choice of \(T=K\) for both bounds). If we consider the worst case that \(\Pi_{h}^{soft}(T)=\mathcal{O}(T\log|\mathcal{F}_{h}|)\), then these rates are \(K^{-1/5}\) vs \(K^{-1/4}\) (with an optimal choice of \(T=K^{2/5}\) and \(T=\sqrt{K}\) in the respective bounds). Finally, our results hold under the general adaptively collected data rather than their independent episode setting. We summarize the bounds in the finite function class cases in Table 1, and give comparisons for the linear model cases in Table 2.

Footnote 7: Xie et al. (2021) consider discounted MDP and a _restricted_ policy class for the comparator class.

**Compared with LCB-based algorithms.** When \(\mathcal{F}_{h}\) is a \(d\)-dimensional linear model with feature maps \(\{\phi_{h}\}_{h\in[H]}\), our bounds reduce into \(Hb\sqrt{d\cdot K^{-1}\cdot\mathcal{C}(\pi;1/\sqrt{K})}\) (Proposition 1), which matches the order of (and potentially tighter than) the bound in Zanette et al. (2021), since \(\mathcal{C}(\pi;1/\sqrt{K})\) is always smaller (or at least equal to) than the relative condition number \(\max_{h}\sup_{x\in\mathbb{R}^{d}}\frac{x^{T}\mathbb{E}_{h}[\phi_{h}(s,a_{h}) \phi_{h}(s,a_{h})^{T}]x}{x^{2}\mathbb{E}_{h}[\phi_{h}(s,a_{h})\phi_{h}(s,a_{h })^{T}]x}\). Compared with the bound of LCB-based algorithms in Jin et al. (2021), we improve a factor \(\sqrt{d}\) and holds under the more general Assumption 2.2 which includes low-rank MDPs. In a more refined analysis (Xiong et al., 2023), the LCB-based algorithm obtains the same dependence on \(d\) for low-rank MDPs as our guarantees. However, this improvement relies on a uniform coverage assumption, i.e., \(\min_{h\in[H]}\lambda_{\min}\left(\mathbb{E}_{(s_{h},a_{h})\sim d_{h}^{ \mu}}\left[\phi_{h}(s_{h},a_{h})\phi_{h}(s_{h},a_{h})^{T}\right]\right)>0\), which we do not require. Di et al. (2023) generalize the results of Xiong et al. (2023) from linear MDPs to MDPs with general function approximation. However, they still rely on a uniform coverage assumption. Finally note that, for VS-based and RO-based algorithms, we provide high-probability bounds for a smoothing version of \(\hat{\pi}\) over the randomization of the algorithms, not for \(\hat{\pi}\) itself.

**Compared with model-based PS.**Uehara and Sun (2022) consider model-based PS for offline RL, where they obtain the _Bayesian_ sub-optimality bound of \(H^{2}\sqrt{C^{\text{Bayes}}\cdot\ln|\mathcal{M}|/K}\) where \(C^{\text{Bayes}}\) is the Bayesian version of a relative condition number and \(\mathcal{M}\) is a finite model class. Two key distinctions are that our method in Algorithm 4 is model-free, and our achieved bound is in the frequentist (i.e., worst-case) nature, which is a stronger result than the Bayesian bound of the same order.

## 5 Conclusion

We contributed to the understanding of sample-efficient offline RL in the context of (value) function approximation. We proposed a notion of data diversity that generalizes the previous data coverage measures and importantly expands the class of sample-efficient offline RL. We studied three different algorithms: VS, RO, and PS, where the PS-based algorithm is our novel proposal. We showed that VS, RO, and PS all have same-order guarantees under standard assumptions.

## Acknowledgements

This research was supported, in part, by DARPA GARD award HR00112020004, NSF CAREER award IIS-1943251, funding from the Institute for Assured Autonomy (IAA) at JHU, and the Spring'22 workshop on "Learning and Games" at the Simons Institute for the Theory of Computing.

## References

* Agarwal and Zhang (2022) Alekh Agarwal and Tong Zhang. Non-linear reinforcement learning in large action spaces: Structural conditions and sample-efficiency of posterior sampling. In _Conference on Learning Theory_, pages 2776-2814. PMLR, 2022.
* Agarwal et al. (2021) Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. _J. Mach. Learn. Res._, 22(98):1-76, 2021.
* Antos et al. (2006) Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path. In Gabor Lugosi and Hans Ulrich Simon, editors, _Learning Theory, 19th Annual Conference on Learning Theory, COLT 2006, Pittsburgh, PA, USA, June 22-25, 2006, Proceedings_, volume 4005 of _Lecture Notes in Computer Science_, pages 574-588. Springer, 2006.
* Arora et al. (2012) Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-algorithm and applications. _Theory of Computing_, 8(6):121-164, 2012. doi: 10.4086/toc.2012. v008a006.
* Bartlett et al. (2005) Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. _The Annals of Statistics_, 33(4):1497-1537, 2005.
* Chen and Jiang (2019) Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In _International Conference on Machine Learning_, pages 1042-1051. PMLR, 2019.
* Chen and Jiang (2022) Jinglin Chen and Nan Jiang. Offline reinforcement learning under value and density-ratio realizability: the power of gaps. In _Uncertainty in Artificial Intelligence_, pages 378-388. PMLR, 2022.
* Cheng et al. (2022) Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. In _International Conference on Machine Learning_, pages 3852-3878. PMLR, 2022.
* Dann et al. (2021) Christoph Dann, Mehryar Mohri, Tong Zhang, and Julian Zimmert. A provably efficient model-free posterior sampling method for episodic reinforcement learning. In _Advances in Neural Information Processing Systems_, 2021.
* Di et al. (2023) Qiwei Di, Heyang Zhao, Jiafan He, and Quanquan Gu. Pessimistic nonlinear least-squares value iteration for offline reinforcement learning. _arXiv preprint arXiv:2310.01380_, 2023.
* Duan et al. (2020) Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function approximation. In _International Conference on Machine Learning_, pages 2701-2709. PMLR, 2020.
* Ernst et al. (2005) Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. _Journal of Machine Learning Research_, 6, 2005.
* Foster et al. (2021) Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* Freedman (1975) David A. Freedman. On tail probabilities for martingales. _The Annals of Probability_, 3(1):100-118, 1975. ISSN 00911798.
* Gabbianelli et al. (2023) Germano Gabbianelli, Gergely Neu, Nneka Okolo, and Matteo Papini. Offline primal-dual reinforcement learning for linear mdps. _arXiv preprint arXiv:2305.12944_, 2023.
* Ghahramani et al. (2018)Xiang Ji, Minshuo Chen, Mengdi Wang, and Tuo Zhao. Sample complexity of nonparametric off-policy evaluation on low-dimensional manifolds using deep networks. _arXiv preprint arXiv:2206.02887_, 2022.
* Jiang et al. (2017) Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_, pages 1704-1713. PMLR, 2017.
* Jin et al. (2020) Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* Jin et al. (2021a) Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021a.
* Jin et al. (2021b) Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_, pages 5084-5096. PMLR, 2021b.
* Jin et al. (2022) Ying Jin, Zhimei Ren, Zhuoran Yang, and Zhaoran Wang. Policy learning" without"overlap: Pessimism and generalized empirical Bernstein's inequality. _arXiv preprint arXiv:2212.09900_, 2022.
* Lange et al. (2012) Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In _Reinforcement learning_, pages 45-73. Springer, 2012.
* Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Liu et al. (2019) Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with stationary distribution correction. In Amir Globerson and Ricardo Silva, editors, _Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019_, volume 115 of _Proceedings of Machine Learning Research_, pages 1180-1190. AUAI Press, 2019.
* Massart (2000) Pascal Massart. Some applications of concentration inequalities to statistics. _Annales de la Faculte des Sciences de Toulouse_, 9:245-303, 2000.
* Munos and Szepesvari (2008) Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. _J. Mach. Learn. Res._, 9:815-857, 2008.
* Nguyen-Tang and Arora (2023) Thanh Nguyen-Tang and Raman Arora. VIPer: Provably efficient algorithm for offline RL with neural function approximation. In _The Eleventh International Conference on Learning Representations_, 2023.
* Nguyen-Tang et al. (2022a) Thanh Nguyen-Tang, Sunil Gupta, A. Tuan Nguyen, and Svetha Venkatesh. Offline neural contextual bandits: Pessimism, optimization and generalization. In _International Conference on Learning Representations_, 2022a.
* Nguyen-Tang et al. (2022b) Thanh Nguyen-Tang, Sunil Gupta, Hung Tran-The, and Svetha Venkatesh. On sample complexity of offline reinforcement learning with deep reLU networks in besov spaces. _Transactions of Machine Learning Research_, 2022b.
* Nguyen-Tang et al. (2023) Thanh Nguyen-Tang, Ming Yin, Sunil Gupta, Svetha Venkatesh, and Raman Arora. On instance-dependent bounds for offline reinforcement learning with linear function approximation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 9310-9318, 2023.
* Osband et al. (2016) Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. _Advances in neural information processing systems_, 29, 2016.
* Ozdaglar et al. (2023) Asuman E Ozdaglar, Sarath Pattathil, Jiawei Zhang, and Kaiqing Zhang. Revisiting the linear-programming framework for offline rl with general function approximation. In _International Conference on Machine Learning_, pages 26769-26791. PMLR, 2023.
* O'Hagan et al. (2017)* Rashidinejad et al. (2021) Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _Advances in Neural Information Processing Systems_, 34, 2021.
* Rashidinejad et al. (2023) Paria Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao. Optimal conservative offline RL with general function approximation via augmented lagrangian. In _The Eleventh International Conference on Learning Representations_, 2023.
* Russo and Van Roy (2014) Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. _Mathematics of Operations Research_, 39(4):1221-1243, 2014.
* Thompson (1933) William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25:285-294, 1933.
* Tripuraneni et al. (2020) Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. _Advances in neural information processing systems_, 33:7852-7862, 2020.
* Uehara and Sun (2022) Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. In _International Conference on Learning Representations_, 2022.
* Uehara et al. (2022a) Masatoshi Uehara, Chengchun Shi, and Nathan Kallus. A review of off-policy evaluation in reinforcement learning. _arXiv preprint arXiv:2212.06355_, 2022a.
* Uehara et al. (2022b) Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline RL in low-rank MDPs. In _International Conference on Learning Representations_, 2022b.
* Wang et al. (2021) Ruosong Wang, Dean Foster, and Sham M. Kakade. What are the statistical limits of offline RL with linear function approximation? In _International Conference on Learning Representations_, 2021.
* Watkins et al. (2023) Austin Watkins, Enayat Ullah, Thanh Nguyen-Tang, and Raman Arora. Optimistic rates for multi-task representation learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Welling and Teh (2011) Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 681-688, 2011.
* Xie et al. (2021) Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_, 34, 2021.
* Xiong et al. (2022) Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, and Tong Zhang. A self-play posterior sampling algorithm for zero-sum markov games. In _International Conference on Machine Learning_, pages 24496-24523. PMLR, 2022.
* Xiong et al. (2023) Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, Liwei Wang, and Tong Zhang. Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent MDP and markov game. In _The Eleventh International Conference on Learning Representations_, 2023.
* Yang and Wang (2019) Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In _International Conference on Machine Learning_, pages 6995-7004. PMLR, 2019.
* Yin and Wang (2021) Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with pessimism. _Advances in neural information processing systems_, 34, 2021.
* Zanette et al. (2020) Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_, pages 10978-10989. PMLR, 2020.
* Zanette et al. (2021) Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. _Advances in neural information processing systems_, 34:13626-13640, 2021.
* Zhai et al. (2020)Ruohan Zhan, Zhimei Ren, Susan Athey, and Zhengyuan Zhou. Policy learning with adaptively collected data. _Management Science_, 2023.
* Zhan et al. [2022] Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability and single-policy concentrability. In Po-Ling Loh and Maxim Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 2730-2775. PMLR, 02-05 Jul 2022.
* Zhang [2022] Tong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. _SIAM Journal on Mathematics of Data Science_, 4(2):834-857, 2022.
* Zhang [2023] Tong Zhang. _Mathematical Analysis of Machine Learning Algorithms_. Cambridge University Press, 2023.
* Zhao et al. [2023] Yulai Zhao, Zhuoran Yang, Zhaoran Wang, and Jason D Lee. Local optimization achieves global optimality in multi-agent reinforcement learning. _arXiv preprint arXiv:2305.04819_, 2023.
* Zhong et al. [2022] Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. A posterior sampling framework for interactive decision making. _arXiv preprint arXiv:2211.01962_, 2022.
* Zhu et al. [2023] Hanlin Zhu, Paria Rashidinejad, and Jiantao Jiao. Importance weighted actor-critic for optimal conservative offline reinforcement learning. _arXiv preprint arXiv:2301.12714_, 2023.

###### Contents

* 1 Introduction
* 2 Background and Problem Formulation
	* 2.1 Episodic Time-inhomogenous Markov Decision Process
	* 2.2 Offline Data Generation
	* 2.3 Policy and function classes
	* 2.4 Effective sizes of policy and function classes
* 3 Algorithms
* 4 Main Results
	* 4.1 Data diversity
	* 4.2 Offline learning guarantees
* 5 Conclusion
* Appendix A Extended Discussion
* A.1 Linear function classes
* A.2 Comparison with primal-dual methods for offline RL
* Appendix B Preparation
* B.1 Variance condition and Bernstein's inequality
* B.2 Functional projections for misspecification
* B.3 Induced MDPs
* B.4 Error decomposition
* B.5 Decoupling lemma
* B.6 Regret of the multiplicative weights algorithm for the actors
* Appendix C Proof of Theorem 1
* C.1 Proof of Theorem 1
* C.2 Proof of Lemma C.1
* C.3 Proof of Lemma C.2
* Appendix D Proof of Theorem 2
* Appendix E Proof of Theorem 3
* E.1 Generalized form of posterior and Proposition 2
* E.2 Proof of Theorem 3
* E.3 Proof of Proposition 2
* E.3.1 Lower-bounding log-partition function.

* E.3.2 Upper-bounding log-partition function
* E.3.3 Proof of Proposition 2
* F Proof of Proposition 1
* G Support Lemmas

## Appendix A Extended Discussion

We also extend the discussion of our data diversity in comparison to the existing distribution mismatch measures in the special case where each \(\mathcal{F}_{h}\) is a linear function in a known feature map.

### Linear function classes

In this section, we consider the linear model cases, where there are known feature maps \(\phi_{h}:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\) and w.l.o.g. \(\max_{h\in[H]}\|\phi(\cdot,\cdot)\|_{\infty}\leq 1\), such that \(\mathcal{F}_{h}=\{\phi_{h}(\cdot,\cdot)^{T}w:w\in\mathbb{R}^{d},\|w\|_{2}^{2} \leq b\}\). Recall that, in this case, e.g., it follows from (Zanette et al., 2021, Lemma 6), that we have

\[\log N(\epsilon;\mathcal{F}_{h},\|\cdot\|_{\infty}) \leq d\log(1+\frac{2}{\epsilon}),\] \[\log N(\epsilon;\Pi_{h}^{soft}(T),\|\cdot\|_{\infty}) \leq d\log(1+\frac{16bT}{\epsilon}).\]

Thus, our bounds from Proposition 1 can simplified as

\[\tilde{\mathcal{O}}(Hb\sqrt{d\log(1+16bKT)\cdot\mathcal{C}(\pi;1/ \sqrt{K})/K}+Hb\sqrt{T^{-1}\ln\operatorname{Vol}(\mathcal{A})})\] \[=\tilde{\mathcal{O}}\left(Hb\sqrt{\frac{\max\{d\mathcal{C}(\pi;1/ \sqrt{K}),\ln\operatorname{Vol}(\mathcal{A})\}}{K}}\right)\]

where we choose \(T=K\). To simplify the comparison, we assume that \(d\mathcal{C}(\pi;1/\sqrt{K})\geq\ln\operatorname{Vol}(\mathcal{A})\). Let us now compute various notions of data coverage in this linear model case. s We first need to define the following quantities (various forms of covariance matrices).

\[\Sigma_{h} :=\lambda I+\sum_{k=1}^{K}\phi_{h}(s_{h}^{k},a_{h}^{k})\phi_{h}( s_{h}^{k},a_{h}^{k})^{T},\] \[\Lambda_{h} :=\lambda I+\sum_{k=1}^{K}\phi_{h}(s_{h}^{k},a_{h}^{k})\phi_{h}( s_{h}^{k},a_{h}^{k})^{T}/[\mathbb{V}_{h}V_{h+1}^{\pi}](s_{h}^{k},a_{h}^{k}),\] \[\bar{\phi}_{h}^{\pi} :=\mathbb{E}_{\pi}[\phi_{h}(s_{h},a_{h})],\] \[\bar{\Sigma}_{h} :=\mathbb{E}_{\mu}\left[\phi(s_{h},a_{h})\phi(s_{h},a_{h})^{T} \right].\]

We define the following distribution mismatch quantities, which were used in the literature.

\[C_{pevi}(\pi) :=\max_{h\in[H]}\left(\mathbb{E}_{\pi}\left[\|\phi_{h}(s_{h},a_{ h})\|_{\Sigma_{h}^{-1}}\right]\right)^{2},\] \[C_{pevi-adv}(\pi) :=\max_{h\in[H]}\left(\mathbb{E}_{\pi}\left[\|\phi_{h}(s_{h},a_{ h})\|_{\Lambda_{h}^{-1}}\right]\right)^{2},\] \[C_{pacle}(\pi) :=\max_{h\in[H]}\|\bar{\phi}_{h}^{\pi}\|_{\Sigma_{h}^{-1}}^{2},\] \[C_{bcp}(\pi) :=\max_{h\in[H]}\left(\mathbb{E}_{\pi}\left[\|\phi_{h}(s_{h},a_{ h})\|_{\bar{\Sigma}_{h}}\right]\right)^{2}.\]

The sub-optimality bounds of various methods are summarized in Table 2. For comparing our data diversity measure with different notions of distribution mismatch, we have

\[C_{pevi}(\pi)\geq C_{pacle}(\pi)\approx\mathcal{C}(\pi;0)/K\leq C_{bcp}(\pi)/K.\]

where the "\(\approx\)" denotes that the involved terms scale in the same order and can be implied by Fredman's matrix inequality (see (Duan et al., 2020, Lemma B.5)) (under additional conditions). Note that \(\mathcal{C}(\pi;1/\sqrt{K})\leq\mathcal{C}(\pi;0)\), thus our data diversity is the tightest quantity among all that are considered.

Note that the data coverage measure in Xiong et al. (2023), roughly speaking, can be bounded as follows:

\[C_{pevi-adv}(\pi)\leq b^{2}C_{pevi}(\pi),\]

where we use the inequality \([\mathbb{V}_{h}V_{h+1}^{\pi}](s_{h}^{k},a_{h}^{k})\leq b^{2}\). Thus the bound of Xiong et al. (2023) in general has a tighter dependence on \(b\) (which implicitly depends on \(H\)) than all the bounds of all other works considered in Table 2, due to that Xiong et al. (2023) incorporated the variance information into the estimation via the variance-weighted value iteration algorithm. However, obtaining this improved bound in Xiong et al. (2023) relies on a uniform coverage assumption which we do not require.

### Comparison with primal-dual methods for offline RL

As opposed to the value-based methods we considered in our paper, an important alternative approach to offline RL is the primal-dual methods (Zhan et al., 2022; Chen and Jiang, 2022; Rashidinejad et al., 2023; Gabbianelli et al., 2023; Ozdaglar et al., 2023). However, the guarantees of primal-dual methods use a different set of assumptions than the value-based methods we consider (the former assumes realizability for the ratio between the state-action occupancy density of the target policy and the state-action occupancy density of the behavior policy, except for Gabbianelli et al. (2023) where this realizability assumption is implicitly encoded under a stronger assumption of linear MDP). This makes the results presented in our paper and the results in the primal-dual methods not directly comparable.

Since the work of Gabbianelli et al. (2023) considers linear MDPs, it is more comparable (than the other primal-dual methods we mentioned) to the instantiating of our results to the linear function class. Gabbianelli et al. (2023) consider primal-dual methods for offline RL in both infinite-horizon discounted MDP and average-reward MDP. Our analysis framework for the regularized optimization method in the episodic MDP should work for the infinite-horizon discounted MDP as well, where the regularized optimization achieves the optimal sample complexity of \(\mathcal{O}(\epsilon^{-2})\) while the sample complexity in Gabbianelli et al. (2023) in this setting is \(\mathcal{O}(\epsilon^{-4})\). However, Gabbianelli et al. (2023) offers a better computational complexity (\(\mathcal{O}(K)\) vs \(\mathcal{O}(K^{7/5})\)) and also works in the average-reward MDP setting which is beyond the episodic MDP setting considered in our work; though our bounds hold for general function approximation that is beyond the strong assumption of linear MDPs.

The concurrent work of Zhu et al. (2023) combined the actor-critic framework with marginalized importance sampling (MIS) for an RO-based algorithm, which also improves the sub-optimal rate of order \(1/K^{1/3}\) by Xie et al. (2021), Cheng et al. (2022) to the optimal rate of order \(1/\sqrt{K}\). Instead, we obtain the optimal rate of order \(1/\sqrt{K}\) with a refined analysis for a standard RO-based algorithm. That is, unlike Zhu et al. (2023), we do not use MIS; consequently, we do not require the realizability assumption for the ratio between the state-action occupancy density of the target policy and that of the behavior policy.

## Appendix B Preparation

We now get into more involved parts where we present the proof process and the technical results for obtaining Theorem1, Theorem2, and Theorem3. In order to prove our main results in Section4, we shall need some old tools and develop some new useful tools. For convenience, we start out with both old and new notations of quantities summarized in Table3 that we are going to use frequently in our proofs.

The quantity \(l_{\tilde{\pi}}(f_{h},f_{h+1};z_{h})\) can be viewed as _a temporal difference (TD) loss function_ defined on data point \(z_{h}\) conditioned on each \(f_{h+1}\) and \(\tilde{\pi}\). The quantity \(\mathbb{T}_{h}^{\tilde{\pi}}f_{h+1}\) can be viewed as _the Bellman regression function_, where, conditioned on each \(f_{h+1}\) and \(\tilde{\pi}\), for any \((s_{h},a_{h})\), we have

\[\mathbb{T}_{h}^{\tilde{\pi}}f_{h+1}(s_{h},a_{h})=\mathbb{E}_{r_{h},s_{h+1}|s_{ h},a_{h}}[r_{h}+f_{h+1}(s_{h+1},\tilde{\pi})]=\operatorname*{arg\,inf}_{g} \mathbb{E}_{r_{h},s_{h+1}|s_{h},a_{h}}l_{\tilde{\pi}}(g,f_{h+1};z_{h}).\]

Thus, the quantity \(\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h})\) can be referred to as the _excess TD loss_, incurred by the predictor \(f_{h}\), relative to the TD regression function \(\mathbb{T}_{h}^{\tilde{\pi}}f_{h+1}\), on data \(z_{h}\) and conditioned on \(f_{h+1}\) and \(\tilde{\pi}\).

### Variance condition and Bernstein's inequality

We also define the \(\sigma\)-algebra \(\mathcal{A}_{h}^{k}:=\sigma(\mathcal{D}_{k-1}\cup\{(s_{h^{\prime}}^{k},a_{h^{ \prime}}^{k},r_{h^{\prime}}^{k})\}_{h^{\prime}\in[h-1]}\cup(s_{h}^{k},a_{h}^{k}))\) and denote \(\mathbb{E}_{k,h}[\cdot]:=\mathbb{E}[\cdot|\mathcal{A}_{h}^{k}]\). The following lemma establishes the variance condition on the excess TD loss, a TD analogous to the variance condition that is widely used in the empirical process theory (Massart, 2000).

**Lemma B.1**.: _For any \(\mathcal{A}_{h}^{k}\)-measurable policy \(\pi\), we have_

\[\mathbb{E}_{k,h}[\Delta L_{\pi}(f_{h},f_{h+1};z_{h}^{k})] =\mathcal{E}_{h}^{\pi}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k})^{2},\] \[\mathbb{E}_{k,h}[\Delta L_{\pi}(f_{h},f_{h+1};z_{h}^{k})^{2}] \leq 36b^{2}\mathcal{E}_{h}^{\pi}(f_{h},f_{h+1})(s_{h}^{k},a_{h} ^{k})^{2}.\]

Proof of Lemma b.1.: The result directly exploits the boundedness of the TD loss function and the squared loss is Lipschitz. In concrete, it is a direct application of LemmaG.1. 

The following lemma establishes the martingale extension of Bernstein's inequality, typically called Freedman's inequality (Freedman, 1975). In this lemma, we prove a slightly modified version of the original Freedman's inequality for our own convenience. The proof for this lemma is elementary which we also show here.

\begin{table}
\begin{tabular}{l|l|l} \hline Name & Notation & Expression \\ \hline transition sample & \(z_{h}^{k}\) & \((s_{h}^{k},a_{h}^{k},r_{h}^{k},a_{h+1}^{k})\) \\ transition sample & \(z_{h}\) & \((s_{h},a_{h},r_{h},s_{h+1})\) \\ Bellman error & \(\mathcal{E}_{h}^{s}(f_{h},f_{h+1})(s_{h},a_{h})\) & \((\mathbb{T}_{h}^{s}f_{h+1}-f_{h})(s_{h},a_{h})\) \\ TD loss function & \(l_{\tilde{\pi}}(f_{h},f_{h+1};z_{h})\) & \((f_{h}(s_{h},a_{h})-r_{h}-f_{h+1}(s_{h+1},\tilde{\pi}))^{2}\) \\ empirical squared Bellman error (SBE) & \(\hat{L}_{\tilde{\pi}}(f_{h},f_{h+1})\) & \(\sum_{h=1}^{K}l_{\tilde{\pi}}(f_{h},f_{h+1};z_{h}^{k})\) \\ empirical bias-adjusted SBE & \(\mathcal{L}_{\tilde{\pi}}(f)\) & \(\sum_{h=1}^{H}L_{\tilde{\pi}}(f_{h},f_{h+1})-\operatorname*{arg\,inf}_{g\in \mathcal{F}}\sum_{h=1}^{H}\hat{L}_{\tilde{\pi}}(g_{h},f_{h+1})\) \\ excess TD loss & \(\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h})\) & \(l_{\tilde{\pi}}(f_{h},f_{h+1};z_{h})-l_{\tilde{\pi}}(\mathbb{T}_{h}^{\tilde{\pi} }f_{h+1},f_{h+1};z_{h})\) \\ – & \(\mathbb{E}_{h}[\cdot]\) & \(\frac{1}{K}\sum_{h=1}^{K}\mathbb{E}_{\mu^{\prime}}[\cdot]\) \\ – & \(\mathbb{E}_{h}[\cdot](:=\mathbb{E}_{\mu^{\prime}}[\cdot])\) & \(\mathbb{E}\left[\cdot\left[z_{h}^{i}\right]

**Lemma B.2** (Freedman's inequality).: _Let \(X_{1},\ldots,X_{T}\) be any sequence of real-valued random variables. Denote \(\mathbb{E}_{t}[\cdot]=\mathbb{E}[\cdot|X_{1},\ldots,X_{t-1}]\). Assume that \(X_{t}\leq R\) for some \(R>0\) and \(\mathbb{E}_{t}[X_{t}]=0\) for all \(t\). Define the random variables_

\[S:=\sum_{t=1}^{T}X_{t},\quad V:=\sum_{i=1}^{T}\mathbb{E}_{t}[X_{t}^{2}].\]

_Then for any \(\delta>0\), with probability at least \(1-\delta\), for any \(\lambda\in[0,1/R]\),_

\[S\leq(e-2)\lambda V+\frac{\ln(1/\delta)}{\lambda}.\]

Proof of Lemma b.2.: Let us define the following sequence of random variables: \(Z_{0}=1,Z_{t}=Z_{t-1}\frac{e^{\lambda X_{t}}}{\mathbb{E}_{t}[e^{\lambda X_{t} }]}\). We have

\[\mathbb{E}_{t}[Z_{t}]=\mathbb{E}_{t}\left[Z_{t-1}\frac{e^{\lambda X_{t}}}{ \mathbb{E}_{t}[e^{\lambda X_{t}}]}\right]=\frac{Z_{t-1}}{\mathbb{E}_{t}[e^{ \lambda X_{t}}]}\mathbb{E}_{t}[e^{\lambda X_{t}}]=Z_{t-1}.\]

Thus, we have

\[\mathbb{E}[Z_{T}]=\mathbb{E}\mathbb{E}_{T}[Z_{T}]=\mathbb{E}[Z_{T-1}]=\ldots= \mathbb{E}[Z_{0}]=1.\]

Note that

\[Z_{T}=\frac{e^{\lambda S}}{\prod_{t=1}^{T}\mathbb{E}_{t}[e^{\lambda X_{t}}]}= \frac{e^{\lambda S}}{\sum_{t=1}^{T}e^{\ln\mathbb{E}_{t}[e^{\lambda X_{t}}]}}= \exp\left(\lambda S-\sum_{t=1}^{T}\ln\mathbb{E}_{t}[e^{\lambda X_{t}}]\right).\] (3)

Since \(Z_{T}\geq 0\), it follows from Markov's inequality that, for any \(\delta>0\), we have

\[\Pr(Z_{T}\geq 1/\delta)\leq\delta\mathbb{E}[Z_{T}]=\delta.\] (4)

We now bound the logarithmic moment generating function \(\ln\mathbb{E}_{t}[e^{\lambda X_{t}}]\) using elementary inequalities: For any \(\lambda\in[0,1/R]\), we have

\[\ln\mathbb{E}_{t}[e^{\lambda X_{t}}]\leq\mathbb{E}_{t}[e^{\lambda X_{t}}]-1 \leq\lambda\mathbb{E}_{t}[X_{t}]+(e-2)\lambda^{2}\mathbb{E}_{t}[X_{t}^{2}],\] (5)

where the first inequality uses \(\ln z\leq z-1,\forall z\geq 0\) and the second inequality uses that \(e^{z}\leq 1+z+(e-2)z^{2},\forall z\leq 1\) and that \(\lambda X_{t}\leq 1\).

Plugging Equation (5) into Equation (3), then all together into Equation (4) complete the proof.

### Functional projections for misspecification

Since Assumption 2.1 and Assumption 2.2 allow misspecification up to some errors \(\xi\) and \(\nu\), while we are working on the function class \(\mathcal{F}\), we rely on the following projection operators, Definition 4 and Definition 5, to handle misspecification.

**Definition 4** (Projection of action-value functions).: _For any \(\tilde{\pi}\in\Pi^{soft}(T)\) for some \(T\in\mathbb{N}\), we define the projection of the state-action value function \(\tilde{\pi}\) onto \(\mathcal{F}\) as_

\[\mathrm{Proj}_{\mathcal{F}}(Q^{\tilde{\pi}}):=\operatorname*{arg\,min}_{f\in \mathcal{F}}\left\{|f_{h}(s_{h},a_{h})-Q_{h}^{\tilde{\pi}}(s_{h},a_{h})|, \forall h\in[H],(s_{h},a_{h})\in\mathrm{supp}(d_{h}^{\mu})\right\}.\]

By Assumption 2.1, we have

\[|\mathrm{Proj}_{\mathcal{F}}(Q^{\tilde{\pi}})(s_{h},a_{h})-Q_{h}^{\tilde{\pi} }(s_{h},a_{h})|\leq\xi_{h},\ \forall h\in[H],(s_{h},a_{h})\in\mathrm{supp}(d_{h}^{\mu}).\]

**Definition 5** (Projection of Bellman operations).: _For any \(f\in\mathcal{F}\) and \(\tilde{\pi}\in\Pi^{soft}(T)\) for some \(T\), we define the projection of the Bellman operation \(\mathrm{T}^{\tilde{\pi}}f\) onto \(\mathcal{F}\) as_

\[\mathrm{Proj}_{\mathcal{F}}(\mathrm{T}^{\tilde{\pi}}f):=\operatorname*{arg\,min }_{f^{\prime}\in\mathcal{F}}\left\{\|f_{h}^{\prime}-\mathrm{T}_{h}^{\tilde{ \pi}}f_{h+1}\|_{\infty},\forall h\in[H]\right\}.\]

By Assumption 2.2, we have

\[\|\mathrm{Proj}_{\mathcal{F}}(\mathrm{T}^{\tilde{\pi}}f)-\mathrm{T}_{h}^{\tilde {\pi}}f_{h+1}\|_{\infty}\leq\nu_{h},\forall h\in[H].\]

### Induced MDPs

We now introduce the notion of _induced_ MDPs which is originally used in (Zanette et al., 2021).

**Definition 6** (Induced MDPs).: _For any policy \(\pi\in\Pi^{all}\) and any sequence of functions \(Q=\{Q_{h}\}_{h\in[H]}\in\{\mathcal{S}\times\mathcal{A}\to\mathbb{R}\}^{H}\), the \((Q,\pi)\)-induced MDPs, denoted by \(M(Q,\pi)\) is the MDP that is identical to the original MDP \(M\) except only that the expected reward of \(M(Q,\pi)\) is given by \(\{r_{h}^{\pi,Q}\}_{h\in[H]}\), where_

\[r_{h}^{\pi,Q}(s,a):=r_{h}(s,a)-\mathcal{E}_{h}^{\pi}(f_{h},f_{h+1})(s,a).\]

By definition of \(M(\pi,Q)\), \(Q\) is the fixed point of the Bellman equation \(Q_{h}=\mathbb{T}_{h,M(\pi,Q)}^{\pi}Q_{h+1}\).

**Lemma B.3**.: _For any \(\pi\in\Pi^{all}\) and any sequence of functions \(Q=\{Q_{h}\}_{h\in[H]}\in\{\mathcal{S}\times\mathcal{A}\to\mathbb{R}\}^{H}\), we have_

\[Q_{M(\pi,Q)}^{\pi}=Q,\]

_where \(M(\pi,Q)\) is the induced MDP given in Definition 6._

### Error decomposition

The key starting point for the proofs of all of the three main theorems is the following error decomposition that decomposes the sub-optimality into three sources of errors: the Bellman error under the comparator policy \(\pi\), the gap values in the initial states, and the online-regret term due to the induced MDPs. In online RL, the sub-optimality of a greedy policy against an optimal policy can be decomposed into the sub-optimality in the Bellman errors and the error in the initial states (Dann et al., 2021), using the standard value-function error decomposition in (Jiang et al., 2017, Lemma 1). However, in our setting, we compete against an arbitrary policy \(\pi\) (not necessarily an optimal policy) and the learned policy \(\pi^{t}\) is not greedy with respect to the current action-value function \(\underline{Q}^{t}\) - thus (Jiang et al., 2017, Lemma 1) cannot apply here. Instead, we develop an error decomposition - Lemma B.4 which generalizes what was implicit in Zanette et al. (2021).

**Lemma B.4** (Error decomposition).: _For any action-value functions \(Q\in\{\mathcal{S}\times\mathcal{A}\to\mathbb{R}\}^{H}\) and any policies \(\pi,\tilde{\pi}\in\Pi^{all}\), we have_

\[\mathrm{SubOpt}_{\pi}^{M}(\tilde{\pi})=\sum_{h=1}^{H}\mathbb{E}_{\pi}[ \mathcal{E}_{h}^{\tilde{\pi}}(Q_{h},Q_{h+1})(s_{h},a_{h})]+Q_{1}(s_{1},\tilde {\pi}_{1})-V_{1}^{\tilde{\pi}}(s_{1})+\mathrm{SubOpt}_{\pi}^{M(Q,\tilde{\pi}) }(\tilde{\pi}).\]

Proof of Lemma b.4.: We have

\[\mathrm{SubOpt}_{\pi}^{M}(\tilde{\pi})=V_{1}^{\pi}(s_{1})-V_{1}^ {\tilde{\pi}}(s_{1})\] \[=\left(V_{1}^{\pi}(s_{1})-V_{1,M(Q,\tilde{\pi})}^{\pi}(s_{1}) \right)+\left(V_{1,M(Q,\tilde{\pi})}^{\tilde{\pi}}(s_{1})-V_{1}^{\tilde{\pi}} (s_{1})\right)+\left(V_{1,M(Q,\tilde{\pi})}^{\pi}(s_{1})-V_{1,M(Q,\tilde{\pi}) }^{\tilde{\pi}}(s_{1})\right)\] \[=\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E}_{h}^{\tilde{\pi}}(Q_ {h},Q_{h+1})(s_{h},a_{h})]+Q_{1}(s_{1},\tilde{\pi}_{1})-V_{1}^{\tilde{\pi}}(s_ {1})+\mathrm{SubOpt}_{\pi}^{M(Q,\tilde{\pi})}(\tilde{\pi}),\]

where in the last equality, for the first term, we use, by Definition 6, that

\[V_{1}^{\pi}(s_{1})-V_{1,M(Q,\tilde{\pi})}^{\pi}(s_{1})=\sum_{h=1}^{H}\mathbb{E }_{\pi}\left[r_{h}(s_{h},a_{h})-r_{h}^{\tilde{\pi},Q}(s_{h},a_{h})\right]=\sum _{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E}_{h}^{\tilde{\pi}}(Q_{h},Q_{h+1})(s_{h},a _{h})],\]

for the second term, we use, by Lemma B.3, that

\[V_{1,M(Q,\tilde{\pi})}^{\pi}(s_{1})=Q_{1,M(Q,\tilde{\pi})}^{\pi}(s_{1},\tilde{ \pi}_{1})=Q_{1}(s_{1},\tilde{\pi}),\]

and for the last term, we use the definition of value sub-optimality in Equation (1).

### Decoupling lemma

One of the central tools for our proofs is the following decoupling lemma. The decoupling lemma essentially decouples the Bellman residuals under the \(\pi\)-induced state-action distribution into the squared Bellman residuals under the \(\mu\)-induced state-action distribution and the new data diversity measure in Definition 3 and additive terms of low order.

**Lemma B.5** (Decoupling argument).: _Under Assumption 2.2, for any \(f\in\mathcal{F}\), any \(\tilde{\pi}\in\Pi^{soft}(T)\) for some \(T\), any \(\pi\in\Pi^{all}\), any \(\lambda>0\), and any \(\epsilon\geq 0\), we have_

\[\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E}_{h}^{\tilde{\pi}}(f_{h },f_{h+1})(s_{h},a_{h})] \leq\frac{1}{2\lambda}\sum_{h=1}^{H}\left(\sum_{k=1}^{K}\mathbb{E }_{\mu^{k}}\left[\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h},a_{h})^{2} \right]+K\nu_{h}^{2}+4bK\nu_{h}\right)\] \[+\frac{\lambda H\cdot\mathcal{C}(\pi;\epsilon)}{2K}+H\epsilon+ \sum_{h=1}^{H}\nu_{h},\]

_where \(\mathcal{C}(\pi;\epsilon)\) is defined in Definition 3._

Proof of Lemma b.5.: We have

\[\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E}_{h}^{\tilde{\pi}}(f_{ h},f_{h+1})(s_{h},a_{h})]=\sum_{h=1}^{H}\mathbb{E}_{\pi}\left[(\mathbb{T}_{h}^{ \tilde{\pi}}f_{h+1}-f_{h})(s_{h},a_{h})\right]\] \[\leq\sum_{h=1}^{H}\mathbb{E}_{\pi}\left[(\mathrm{Proj}_{\mathcal{ F}_{h}}(\mathbb{T}_{h}^{\tilde{\pi}}f_{h+1})-f_{h})(s_{h},a_{h})\right]+\bar{\nu}\] \[\leq\sum_{h=1}^{H}\sqrt{\mathcal{C}(\pi,\epsilon)\mathbb{E}_{\mu }\left[(\mathrm{Proj}_{\mathcal{F}_{h}}(\mathbb{T}_{h}^{\tilde{\pi}}f_{h+1})-f _{h})(s_{h},a_{h})^{2}\right]}+H\epsilon+\bar{\nu}\] \[\leq\sum_{h=1}^{H}\sqrt{\mathcal{C}(\pi,\epsilon)\left(\mathbb{E }_{\mu}[\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h},a_{h})^{2}]+\nu_{h} ^{2}+4b\nu_{h}\right)}+H\epsilon+\bar{\nu}\] \[\leq\frac{K}{2\lambda}\sum_{h=1}^{H}\left(\mathbb{E}_{\mu}[ \mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h},a_{h})^{2}]+\nu_{h}^{2}+4b \nu_{h}\right)+\frac{\lambda H\mathcal{C}(\pi,\epsilon)}{2K}+H\epsilon+\bar{ \nu},\]

where the first inequality uses Assumption 2.2, the second inequality uses the definition of \(C_{\pi}(\epsilon)\), the third inequality uses Assumption 2.2 (again), the fourth inequality uses Cauchy-Schwartz inequality, and the last inequality uses the AM-GM inequality \(\sqrt{xy}\leq\frac{K}{2\lambda}x+\frac{\lambda}{2K}y\). 

### Regret of the multiplicative weights algorithm for the actors

Now we establish the regret bound for the online-regret term due to the induced MDPs. The result in the following lemma is quite standard and can be readily generalized from a similar result in Zanette et al. (2021). We present the proof here for completeness.

**Lemma B.6**.: _Consider an arbitrary sequence of value functions \(\{Q^{t}\}_{t\in[T]}\) such that \(\max_{h,t}\|Q^{t}_{h}\|_{\infty}\leq b\) and define the following sequence of policies \(\{\pi^{t}\}_{t\in[T+1]}\) where_

\[\pi^{1}(\cdot|s) =\mathrm{Uniform}(\mathcal{A}),\forall s,\] \[\pi^{t+1}_{h}(a|s) \propto\pi^{t}_{h}(a|s)\exp\left(\eta Q^{t}_{h}(s,a)\right), \forall(s,a,h,t).\]

_Suppose \(\eta=\sqrt{\frac{\ln\mathrm{Vol}(\mathcal{A})}{4(e-2)b^{2}T}}\) and \(T\geq\frac{\ln\mathrm{Vol}(\mathcal{A})}{(e-2)}\), where \(\mathrm{Vol}(\mathcal{A})\) denotes the volume of the action set \(\mathcal{A}\). 8 For an arbitrary policy \(\pi\in\Pi^{all}\), we have_

Footnote 8: When \(|\mathcal{A}|<\infty\), \(\mathrm{Vol}(\mathcal{A})=|\mathcal{A}|\).

\[\sum_{t=1}^{T}\left(V_{1,M(\pi^{t},Q^{t})}^{\pi}(s_{1})-V_{1,M(\pi^{t},Q^{t})} ^{\pi^{t}}(s_{1})\right)\leq 4Hb\sqrt{T\ln\mathrm{Vol}(\mathcal{A})}.\]Proof of Lemma b.6.: The proof for this lemma is quite standard as shown in Zanette et al. [2021]. We rewrote the proof with a slight modification for completeness. For simplicity, we write \(M_{t}:=M(\pi^{t},Q^{t})\). We will see that the key property that enables this lemma is that \(Q^{t}_{h}=Q^{\pi^{t}}_{h,M_{t}}\) (Lemma B.3), which allows us to relate the value difference lemma to the log policy ratio. Using the value difference lemma (Lemma G.2), we have

\[V^{\pi}_{1,M_{t}}(s_{1})-V^{\pi^{t}}_{1,M_{t}}(s_{1})=\sum_{h=1}^{H}\mathbb{E} _{\pi}A^{\pi^{t}}_{h,M_{t}}(s_{h},a_{h}),\]

where \(\mathbb{E}_{\pi}\) is the expectation over the random trajectory \((s_{1},a_{1},\ldots,s_{H},a_{H})\) generated by \(\pi\) (and the underlying MDP \(M_{t}\)9). For any \(V:\mathcal{S}\to\mathbb{R}\), it follows from the definition of \(\{\pi^{t}\}\) update that we have

Footnote 9: Note that \(\Pr((s_{1},a_{1},\ldots,s_{H},a_{H})|\pi,M)=\Pr((s_{1},a_{1},\ldots,s_{H},a_{ H})|\pi,M_{t})\) since \(M_{t}\) and \(M\) have identical transition kernels.

\[\log\frac{\pi^{t+1}_{h}(a|s)}{\pi^{t}_{h}(a|s)} =\eta Q^{t}_{h}(s,a)-\log\left(\mathbb{E}_{a\sim\pi^{t}_{h}(\cdot |s)}\left[\exp\left(\eta Q^{t}_{h}(s,a)\right)\right]\right)\] \[=\eta(Q^{t}_{h}(s,a)-V(s))-\log\left(\mathbb{E}_{a\sim\pi^{t}_{h} (\cdot|s)}\left[\exp\left(\eta(Q^{t}_{h}(s,a)-V(s))\right)\right]\right).\]

In the equation above, noting that \(Q^{t}_{h}=Q^{\pi^{t}}_{h,M_{t}}\) (Lemma B.3) and replacing \(V(s)\) by \(V^{\pi^{t}}_{h,M_{t}}\), we have

\[\log\frac{\pi^{t+1}_{h}(a|s)}{\pi^{t}_{h}(a|s)}=\eta A^{\pi^{t}}_{h,M_{t}}(s, a)-\log\left(\mathbb{E}_{a\sim\pi^{t}_{h}(\cdot|s)}\left[\exp\left(\eta A^{ \pi^{t}}_{h,M_{t}}(s,a)\right)\right]\right),\] (6)

where we define the advantage function \(A^{\pi}_{M}=\{A^{\pi}_{h,M}\}_{h\in[H]}\) as

\[A^{\pi}_{h,M}(s,a):=Q^{\pi}_{h,M}(s,a)-V^{\pi}_{h,M}(s),\forall(s,a,h).\]

Note that \(|A^{\pi^{t}}_{h,M_{t}}(s,a)|\leq 2b\). By choosing \(\eta\in(0,1/(2b))\), we have

\[\log\left(\mathbb{E}_{a\sim\pi^{t}_{h}(\cdot|s)}\left[\exp\left( \eta A^{\pi^{t}}_{h,M_{t}}(s,a)\right)\right]\right)\] \[\leq\log\left(\mathbb{E}_{a\sim\pi^{t}_{h}(\cdot|s)}\left[1+\eta A ^{\pi^{t}}_{h,M_{t}}(s,a)+(e-2)\eta^{2}A^{\pi^{t}}_{h,M_{t}}(s,a)^{2}\right]\right)\] \[=\log\left(\mathbb{E}_{a\sim\pi^{t}_{h}(\cdot|s)}\left[1+(e-2) \eta^{2}A^{\pi^{t}}_{h,M_{t}}(s,a)^{2}\right]\right)\] \[\leq\log(1+(e-2)\eta^{2}4b^{2})\] \[\leq 4(e-2)b^{2}\eta^{2}\] (7)

where the first inequality uses that \(e^{x}\leq 1+x+(e-2)x^{2},\forall x\leq 1\) and \(|\eta A^{\pi^{t}}_{h,M_{t}}(s,a)|\leq 1,\forall(s,a)\), the first equality uses that \(\mathbb{E}_{a\sim\pi^{t}_{h}(\cdot|s)}\left[A^{\pi^{t}}_{h,M_{t}}(s,a)\right]=0\), the second inequality uses that \(|A^{\pi^{t}}_{h,M_{t}}(s,a)|\leq 2b\), and the last inequality uses that \(\log(1+x)\leq x,\forall x\geq 0\). Combining Equation (7) and Equation (6), we have

\[A^{\pi^{t}}_{h,M_{t}}(s,a)\leq\frac{1}{\eta}\log\frac{\pi^{t+1}_{h}(a|s)}{\pi^ {t}_{h}(a|s)}+4(e-2)b^{2}\eta.\]

Thus, for any \(h\in[H]\), we have

\[\sum_{t=1}^{T}\mathbb{E}_{\pi}A^{\pi^{t}}_{h,M_{t}}(s_{h},a_{h})\] \[\leq\frac{1}{\eta}\sum_{t=1}^{T}\left(\mathbb{E}_{\pi}\left[KL[ \pi_{h}(\cdot|s_{h})\|\pi^{t}_{h}(\cdot|s_{h})]\right]-\mathbb{E}_{\pi}\left[ KL[\pi_{h}(\cdot|s_{h})\|\pi^{t+1}_{h}(\cdot|s_{h})]\right]\right)+4(e-2)Tb^{2}\eta\]\[=\frac{1}{\eta}\left(\mathbb{E}_{\pi}\left[KL[\pi_{h}(\cdot|s_{h})\| \pi^{1}_{h}(\cdot|s_{h})]\right]-\mathbb{E}_{\pi}\left[KL[\pi_{h}(\cdot|s_{h})\| \pi^{T+1}_{h}(\cdot|s_{h})]\right]\right)+4(e-2)Tb^{2}\eta\] \[\leq\frac{1}{\eta}\mathbb{E}_{\pi}\left[KL[\pi_{h}(\cdot|s_{h}) \|\pi^{1}_{h}(\cdot|s_{h})]\right]+4(e-2)Tb^{2}\eta\] \[\leq\frac{1}{\eta}\log(\operatorname{Vol}(\mathcal{A}))+4(e-2)Tb ^{2}\eta,\]

where the second inequality uses the non-negativity of KL divergence, and the last inequality uses that \(KL[\pi_{h}(\cdot|s_{h})\|\pi^{1}_{h}(\cdot|s_{h})]=-H[\pi_{h}(\cdot|s_{h})]+ \log(\operatorname{Vol}(\mathcal{A}))\leq\log(\operatorname{Vol}(\mathcal{A}))\) where \(\pi^{1}\) is uniform over \(\mathcal{A}\) and \(\operatorname{Vol}(\mathcal{A})\) denotes the volume over the compact set \(\mathcal{A}\). Combining all pieces together, we have

\[\sum_{t=1}^{T}\mathbb{E}_{\pi}A^{\pi^{t}}_{h,M_{t}}(s_{h},a_{h})\leq\frac{1}{ \eta}H\log(\operatorname{Vol}(\mathcal{A}))+4(e-2)THb^{2}\eta.\]

Minimizing the RHS of the above equation with respect to \(\eta\) yields \(\eta=\sqrt{\frac{\log\operatorname{Vol}(\mathcal{A})}{4(e-2)b^{2}T}}\) and

\[\sum_{t=1}^{T}\mathbb{E}_{\pi}A^{\pi^{t}}_{h,M_{t}}(s_{h},a_{h})\leq 2H\sqrt{4(e-2 )b^{2}T\log\operatorname{Vol}(\mathcal{A})}\leq 4Hb\sqrt{T\log\operatorname{ Vol}(\mathcal{A})}.\]

Finally, we need that

\[\eta=\sqrt{\frac{\log\operatorname{Vol}(\mathcal{A})}{4(e-2)b^{2}T}}\leq\frac{ 1}{2b},\]

which implies \(T\geq\frac{\ln\operatorname{Vol}(\mathcal{A})}{(e-2)}\). 

We are now ready to establish the proofs of our three main theorems.

## Appendix C Proof of Theorem 1

To construct our proof for Theorem 1, we first establish two following lemmas. The first lemma, Lemma C.1 establishes that the in-distribution squared Bellman residuals are bounded by the unbiased proxy of the squared Bellman error \(\mathcal{L}_{\tilde{\pi}}(f)\), up to some estimation and approximation errors. The second lemma, Lemma C.2, asserts that the unbiased proxy of the squared Bellman error at the projection of \(Q^{\tilde{\pi}}\) is close to zero, up to some estimation and approximation errors.

**Lemma C.1**.: _For any \(\delta>0,\epsilon>0\) and any \(T\in\mathbb{N}\), under Assumption 2.2, with probability at least \(1-\delta\), it holds uniformly over all \(f\in\mathcal{F}\) and \(\tilde{\pi}\in\Pi^{soft}(T)\) that_

\[\sum_{h=1}^{H}\sum_{k=1}^{K}\mathbb{E}_{k}\left[\mathcal{E}_{h}^ {\tilde{\pi}}(f_{h},f_{h+1})(s_{h},a_{h})^{2}\right] \leq 2\mathcal{L}_{\tilde{\pi}}(f)+40b(b+2)KH\epsilon+12bK\sum_{h=1 }^{H}\nu_{h}\] \[+144(e-2)b^{2}H\left[d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon, T)+\ln(1/\delta)\right],\]

_where \(\mathcal{L}_{\tilde{\pi}}(f):=\sum_{h=1}^{H}\hat{L}_{\tilde{\pi}}(f_{h},f_{h+1})- \inf_{g\in\mathcal{F}}\sum_{h=1}^{H}\hat{L}_{\tilde{\pi}}(g_{h},f_{h+1})\)._

**Lemma C.2**.: _Under Assumption 2.1, for any \(T\in\mathbb{N}\), with probability at least \(1-\delta\), it holds uniformly for any \(\tilde{\pi}\in\Pi^{soft}(T)\) that_

\[\mathcal{L}_{\tilde{\pi}}(\operatorname{Proj}_{\mathcal{F}}(Q^{\tilde{\pi}}) )\leq 36(e-2)b^{2}H\left(2d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln \frac{H}{\delta}\right)+6b(3b+4)\epsilon KH+15bK\sum_{h=1}^{H}\xi_{h},\]

_where \(\operatorname{Proj}_{\mathcal{F}}(Q^{\tilde{\pi}})\) is the projection of \(Q^{\tilde{\pi}}\) onto \(\mathcal{F}\), formally defined in Definition 4._

### Proof of Theorem 1

With the two lemmas above, we are ready to prove Theorem 1. This proof is also laying a foundational step for our proofs of Theorem 2 and Theorem 3 that we shall present shortly. The proofs for the two lemmas above are presented immediately after the proof of Theorem 1.

Proof of Theorem 1.: Using Lemma B.4, we have

\[\operatorname{SubOpt}_{\pi}^{M}(\pi^{t})=\sum_{h=1}^{H}\mathbb{E}_{\pi}[ \mathcal{E}_{h}^{\pi^{t}}(\underline{Q}_{h}^{t},\underline{Q}_{h+1}^{t})(s_{ h},a_{h})]+\Delta_{1}\underline{Q}_{1}(s_{1},\pi_{1}^{t})+\operatorname{ SubOpt}_{\pi}^{M_{t}}(\pi^{t})\]

where we denote

\[M_{t} :=M(\underline{Q}^{t},\pi^{t}),\] \[\Delta_{1}\underline{Q}_{1}(s_{1},\pi^{t}) :=\underline{Q}_{1}(s_{1},\pi^{t})-V_{1}^{\pi^{t}}(s_{1}).\]

Bounding \(\sum_{t=1}^{T}\operatorname{SubOpt}_{\pi}^{M_{t}}(\pi^{t})\).Note that \(\sum_{t=1}^{T}\operatorname{SubOpt}_{\pi}^{M_{t}}(\pi^{t})\) can be controlled by standard tools from online learning (Lemma B.6); thus it remains to control the first \(H+1\) terms.

Bounding \(\Delta_{1}\underline{Q}_{1}(s_{1},\pi_{1}^{t})\).Due to Lemma C.2, the event that \(\operatorname{Proj}_{\mathcal{F}}(Q^{\pi^{t}})\in\mathcal{F}(\beta;\pi^{t})\) holds occur at probability at least \(1-\delta\). Furthermore, under this event, we have

\[\Delta_{1}\underline{Q}_{1}(s_{1},\pi_{1}^{t}) =\underline{Q}_{1}(s_{1},\pi^{t})-V_{1}^{\pi^{t}}(s_{1})\] \[\leq\operatorname{Proj}_{\mathcal{F}_{1}}(Q_{1}^{\pi^{t}})-V_{1} ^{\pi^{t}}(s_{1})\] \[\leq\xi_{1},\]

where the first inequality exploits Line 2 of Algorithm 2, and the last inequality uses Assumption 2.1.

Bounding \(\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E}_{h}^{\pi^{t}}(\underline{Q}_{h}^{ t},\underline{Q}_{h+1}^{t})(s_{h},a_{h})]\).It follows from Lemma B.5 that

\[\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E}_{h}^{\pi^{t}}(\underline{Q}_{h}^{ t},\underline{Q}_{h+1}^{t})(s_{h},a_{h})] \leq\sqrt{HC(\pi;\epsilon)\sum_{h=1}^{H}\left(\mathbb{E}_{\mu}[ \mathcal{E}_{h}^{\hat{\pi}}(\underline{Q}_{h}^{t},\underline{Q}_{h+1}^{t})(s _{h},a_{h})^{2}]+\nu_{h}^{2}+4b\nu_{h}\right)}\] \[+H\epsilon+\bar{\nu}.\]

The term \(\sum_{h=1}^{H}\mathbb{E}_{\mu}[\mathcal{E}_{h}^{\hat{\pi}}(\underline{Q}_{h}^{ t},\underline{Q}_{h+1}^{t})(s_{h},a_{h})^{2}]\) is bounded by Lemma C.1, with notice that \(\mathcal{L}_{\pi^{t}}(\underline{Q}^{t})\leq\beta\) (due to the definition of \(\mathcal{F}(\beta;\pi^{t})\) in Algorithm 2).

Combining the three steps above via the union bound completes our proof. 

We now prove the two support lemmas.

### Proof of Lemma c.1

Proof of Lemma c.1.: Let us consider any fixed \(f\in\mathcal{F}\) and any \(\pi\in\Pi^{all}\). By Lemma B.1, we have

\[\mathbb{E}_{k,h}[\Delta L_{\pi}(f_{h},f_{h+1};z_{h}^{k})] =\mathcal{E}_{h}^{\pi}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k})^{2},\] \[\mathbb{E}_{k,h}[\Delta L_{\pi}(f_{h},f_{h+1};z_{h}^{k})^{2}] \leq 36b^{2}\mathcal{E}_{h}^{\pi}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k })^{2}.\]

Combining with Lemma B.2, we have that with probability at least \(1-\delta\), for any \(\iota\in[0,\frac{1}{13b^{2}}]\),

\[\sum_{k=1}^{K}\mathbb{E}_{k}[\mathcal{E}_{h}^{\pi}(f_{h},f_{h+1}) (s_{h},a_{h})^{2}]-\sum_{k=1}^{K}\Delta L_{\pi}(f_{h},f_{h+1};z_{h}^{k})\] \[\leq 36(e-2)b^{2}\iota\sum_{k=1}^{K}\mathbb{E}_{k}[\mathcal{E}_{h} ^{\pi}(f_{h},f_{h+1})(s_{h},a_{h})^{2}]+(1/\iota)\log(1/\delta).\]By setting \(\iota=\frac{1}{72(e-2)b^{2}}\), the above inequality becomes

\[\sum_{k=1}^{K}\mathbb{E}_{k}[\mathcal{E}_{h}^{\pi}(f_{h},f_{h+1})(s_{h},a_{h})^{2 }]\leq 2\sum_{k=1}^{K}\Delta L_{\pi}(f_{h},f_{h+1};z_{h}^{k})+144(e-2)b^{2}\ln(1/ \delta).\]

For any \(\epsilon>0\), and for any \(f\in\mathcal{F},\pi\in\Pi^{soft}(T)\), by definition of \(\epsilon\)-covering, there exist \(f^{\prime}\) and \(\pi^{\prime}\) in the \(\epsilon\)-cover of \(\mathcal{F}\) and \(\Pi^{soft}(T)\), i.e.,

\[\|f_{h}-f^{\prime}_{h}\|_{\infty}\leq\epsilon,\|\pi_{h}-\pi^{\prime}_{h}\|_{ 1,\infty}\leq\epsilon.\]

By simple calculations, we have

\[|\mathcal{E}_{h}^{\pi}(f_{h},f_{h+1})(s_{h},a_{h})^{2}-\mathcal{ E}_{h}^{\pi^{\prime}}(f^{\prime}_{h},f^{\prime}_{h+1})(s_{h},a_{h})^{2}| \leq 4b(b+2)\epsilon,\] \[|\Delta L_{\pi}(f_{h},f_{h+1};z_{h}^{k})-\Delta L_{\pi^{\prime}}( f^{\prime}_{h},f^{\prime}_{h+1};z_{h}^{k})| \leq 18b(b+2)\epsilon.\]

Thus, by the union bound, we have with probability at least \(1-\delta\), it holds uniformly over all \(f\in\mathcal{F},\pi\in\Pi^{soft}(T)\) that

\[\sum_{h=1}^{H}\sum_{k=1}^{K}\mathbb{E}_{k}[\mathcal{E}_{h}^{\pi}( f_{h},f_{h+1})(s_{h},a_{h})^{2}] \leq 2\sum_{h=1}^{H}\sum_{k=1}^{K}\Delta L_{\pi}(f_{h},f_{h+1};z_{h} ^{k})+40b(b+2)KH\epsilon\] \[+144(e-2)b^{2}\sum_{h=1}^{H}\ln(N(\epsilon;\mathcal{F}_{h},\| \cdot\|_{\infty})N(\epsilon;\Pi^{soft}_{h}(T),\|\cdot\|_{1,\infty})/\delta).\]

Finally, notice that

\[|l_{\pi}(\mathbb{T}_{h}^{\pi}f_{h+1},f_{h+1};z_{h})-l_{\pi}( \mathrm{Proj}_{\mathcal{F}_{h}}(\mathbb{T}_{h}^{\pi}f_{h+1}),f_{h+1};z_{h})| \leq 6b\nu_{h}.\]

Thus, we have

\[\sum_{h=1}^{H}\sum_{k=1}^{K}\Delta L_{\pi}(f_{h},f_{h+1};z_{h}^{k})\leq \mathcal{L}_{\pi}(f)+6bK\sum_{h=1}^{H}\nu_{h}.\]

We can then conclude our proof. 

### Proof of Lemma c.2

In order to prove Lemma C.2, we shall first prove the following lemma, which establishes the confidence radius of the empirical squared Bellman errors that we used to establish the version space in Algorithm 2.

**Lemma C.3**.: _Consider any \(\delta>0,\epsilon>0,T\in\mathbb{N}\), let_

\[\beta_{\epsilon}:=36(e-2)b^{2}\left(2d_{\mathcal{F}}(\epsilon)+d_{ \Pi}(\epsilon,T)+\ln(H/\delta)\right)+6b(3b+4)\epsilon K.\]

_With probability at least \(1-\delta\), it holds uniformly over any \(\pi\in\Pi^{soft}(T)\), \(f\in\mathcal{F}\), and \(h\in[H]\) that_

\[\sum_{k=1}^{K}\left(\mathbb{T}_{h}^{\pi}f_{h+1}(x_{h}^{k})-r_{h}^{k}-f_{h+1}(s _{h+1}^{k},\pi_{h+1})\right)^{2}\leq\inf_{g_{h}\in\mathcal{F}_{h}}\sum_{k=1}^{ K}\left(g_{h}(x_{h}^{k})-r_{h}^{k}-f_{h+1}(s_{h+1}^{k},\pi_{h+1})\right)^{2}+ \beta_{\epsilon}.\]

Proof of Lemma c.3.: Let us fix any \(h\in[H]\). For any \((f,g,\pi)\in\mathcal{F}\times\mathcal{F}\times\Pi^{soft}(T)\) and any \(k\in[K]\), define the following random variable

\[Z_{k,h}(f,g,\pi):=\left(g_{h}(x_{h}^{k})-r_{h}^{k}-f_{h+1}(s_{h+1}^{k},\pi_{h+ 1})\right)^{2}-\left(\mathbb{T}_{h}^{\pi}f_{h+1}(x_{h}^{k})-r_{h}^{k}-f_{h+1}(s _{h+1}^{k},\pi_{h+1})\right)^{2}.\]

Denote

\[\mathbb{E}_{k,h}[\cdot]:=\mathbb{E}\left[\cdot\left|\{z_{h}^{i}\}_{h\in[H]}^{ i\in[k-1]},s_{1}^{k},a_{1}^{k},r_{1}^{k},\ldots,s_{h-1}^{k},a_{h-1}^{k},r_{h-1}^{k },s_{h}^{k},a_{h}^{k}\right].\]By Lemma B.1, we have

\[\mathbb{E}_{k,h}\left[Z_{k,h}(f,g,\pi)\right] =\mathcal{E}_{h}^{\pi}(g_{h},f_{h+1})(s_{h}^{k},a_{h}^{k})^{2},\] \[\mathbb{E}_{k,h}\left[Z_{k,h}^{2}(f,g,\pi)\right] \leq 36b^{2}\mathcal{E}_{h}^{\pi}(g_{h},f_{h+1})(s_{h}^{k},a_{h}^{ k})^{2}.\]

Thus, combing with Lemma B.2, for any \((f,g,\pi)\in\mathcal{F}\times\mathcal{F}\times\Pi^{soft}(T)\), with probability at least \(1-\delta\), for any \(\iota\in[0,\frac{1}{136^{2}}]\),

\[\sum_{k=1}^{K}\mathbb{E}_{k,h}\left[Z_{k,h}(f,g,\pi)\right]-\sum_{k=1}^{K}Z_{ k,h}(f,g,\pi)\leq 36(e-2)b^{2}\iota\sum_{k=1}^{K}\mathcal{E}_{h}^{\pi}(g_{h},f_{ h+1})(s_{h}^{k},a_{h}^{k})^{2}+\frac{\ln(1/\delta)}{\iota}.\]

By setting \(\iota=1/(36(e-2)b^{2}),\) the above inequality becomes

\[-\sum_{k=1}^{K}Z_{k,h}(f,g,\pi)\leq 36(e-2)b^{2}\ln(1/\delta).\] (8)

For any \(\epsilon>0\), let \(\mathcal{F}^{\epsilon}\) and \(\Pi^{\epsilon}\) be \(\epsilon\)-covers of \(\mathcal{F}\) and \(\Pi^{soft}(T)\), respectively, with respect to \(\|\cdot\|_{\infty}\) and \(\|\cdot\|_{\infty,1}\), respectively, where \(\|u-v\|_{\infty}:=\sup_{(s,a)}|u(s,a)-v(s,a)|\) and \(\|\pi-\pi^{\prime}\|_{\infty,1}:=\sup_{s}\sum_{a\in\mathcal{A}}|\pi(a|s)-\pi^{ \prime}(a|s)|\). Using the union bound, it follows from Equation (8) that with probability at least \(1-\delta\), it holds uniformly over any \(h\in[H]\) and any \((f,g,\pi)\in\mathcal{F}^{\epsilon}\times\mathcal{F}^{\epsilon}\times\Pi^{\epsilon}\) that

\[-\sum_{k=1}^{K}Z_{k,h}(f,g,\pi)\leq 18(e-2)b^{2}\left[\ln(H/\delta)+2d_{ \mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)\right].\]

For any \((f,g,\pi)\in\mathcal{F}\times\mathcal{F}\times\Pi^{soft}(T)\), there exist \((f_{\epsilon},g_{\epsilon},\pi_{\epsilon})\in\mathcal{F}^{\epsilon}\times \mathcal{F}^{\epsilon}\times\Pi^{\epsilon}\) such that

\[\|f_{h}-(f_{\epsilon})_{h}\|_{\infty}\leq\epsilon,\|g_{h}-(g_{\epsilon})_{h} \|_{\infty}\leq\epsilon,\|\pi_{h}-(\pi_{\epsilon})_{h}\|_{\infty,1}\leq \epsilon,\forall h\in[H].\]

It is easy to compute the discretization error that

\[Z_{k,h}(f,g,\pi)-Z_{k,h}(f_{\epsilon},g_{\epsilon},\pi_{\epsilon})\leq 18b(b +1)\epsilon.\]

Using the discretization argument and the union bound complete our proof. 

We are now ready to prove Lemma C.2.

Proof of Lemma C.2.: Consider the event that the inequality in Lemma C.3 holds. Under this event, for any \(\tilde{\pi}\in\Pi^{soft}(T)\), we have

\[\sum_{k=1}^{K}l_{\tilde{\pi}}(\mathrm{Proj}_{\mathcal{F}_{h}}(Q_ {h}^{\tilde{\pi}}),\mathrm{Proj}_{\mathcal{F}_{h+1}}(Q_{h+1}^{\tilde{\pi}});z _{h}^{k})\leq\sum_{k=1}^{K}l_{\tilde{\pi}}(Q_{h}^{\pi^{\prime}},Q_{h+1}^{ \tilde{\pi}};z_{h}^{k})+6bK\xi_{h}\] \[=\sum_{k=1}^{K}l_{\tilde{\pi}}(\mathbb{T}_{h}^{\tilde{\pi}}Q_{h+1} ^{\tilde{\pi}},Q_{h+1}^{\tilde{\pi}};z_{h}^{k})+6bK\xi_{h}\] \[\leq\sum_{k=1}^{K}l_{\tilde{\pi}}(\mathbb{T}_{h}^{\tilde{\pi}} \mathrm{Proj}_{\mathcal{F}_{h+1}}(Q_{h+1}^{\tilde{\pi}}),\mathrm{Proj}_{ \mathcal{F}_{h+1}}(Q_{h+1}^{\tilde{\pi}});z_{h}^{k})+12bK\xi_{h}\] \[\leq\sum_{k=1}^{K}l_{\tilde{\pi}}(g_{h},\mathrm{Proj}_{\mathcal{F }_{h+1}}(Q_{h+1}^{\tilde{\pi}});z_{h}^{k})+\beta_{\epsilon}+12bK\xi_{h}\ \ \ \text{(for any $g_{h}\in\mathcal{F}_{h}$)}\] \[\leq\sum_{k=1}^{K}l_{\tilde{\pi}}(g_{h},Q_{h+1}^{\tilde{\pi}};z_{h }^{k})+\beta_{\epsilon}+15bK\xi_{h},\]

where we use Assumption 2.1 for the first, second, and last inequalities, the third inequality uses Lemma C.3, and the equality uses \(Q_{h+1}^{\tilde{\pi}}=\mathbb{T}_{h}^{\tilde{\pi}}Q_{h+1}^{\tilde{\pi}}\). Rearranging the last inequality completes our proof.

## Appendix D Proof of Theorem 2

In this appendix, we present our complete argument to establish Theorem 2. In order to prove Theorem 2, the key is to establish a connection from the squared Bellman error under the data distribution \(\mu\) to the regularized objective in Algorithm 3. This key idea should become clear in the following proof.

Proof of Theorem 2.: Similar to the proof of Theorem 1, our starting point is using Lemma B.4:

\[\mathrm{SubOpt}_{\pi}^{M}(\pi^{t})=\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E} _{h}^{\pi^{t}}(\underline{Q}_{h}^{t},\underline{Q}_{h+1}^{t})(s_{h},a_{h})]+ \Delta_{1}\underline{Q}_{1}(s_{1},\pi_{1}^{t})+\mathrm{SubOpt}_{\pi}^{M_{t}}( \pi^{t})\]

and we bound \(\sum_{t=1}^{T}\mathrm{SubOpt}_{\pi}^{M_{t}}(\pi^{t})\) using Lemma B.6. We now bound the remaining terms.

For any \(\gamma>0\), we have

\[\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E}_{h}^{\pi^{t}}( \underline{Q}_{h}^{t},\underline{Q}_{h+1}^{t})(s_{h},a_{h})]\] \[\leq\frac{K}{2\lambda}\sum_{h=1}^{H}\left(\mathbb{E}_{\mu}[ \mathcal{E}_{h}^{\pi^{t}}(\underline{Q}_{h}^{t},\underline{Q}_{h+1}^{t})(s_{h },a_{h})^{2}]+\nu_{h}^{2}+4b\nu_{h}\right)+\frac{\lambda H\mathcal{C}(\pi, \epsilon)}{2K}+H\epsilon+\bar{\nu}\] \[\leq\frac{\mathcal{L}_{\pi^{t}}(\underline{Q})+0.5\iota_{1}+0.5 K\bar{\nu}^{2}+2bK\bar{\nu}}{\lambda}+\frac{\lambda HC(\pi,\epsilon)}{2K}+H \epsilon+\bar{\nu},\]

where the first inequality uses Lemma B.5 and the second inequality uses Lemma C.1, and here \(\iota_{1}:=40b(b+2)KH\epsilon+12bK\sum_{h=1}^{H}\nu_{h}+144(e-2)b^{2}H\left[d _{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln(1/\delta)\right]\). Thus, we have

\[\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E}_{h}^{\pi^{t}}( \underline{Q}_{h}^{t},\underline{Q}_{h+1}^{t})(s_{h},a_{h})]+\Delta_{1} \underline{Q}_{1}(s_{1},\pi_{1}^{t})\] \[\leq\frac{\mathcal{L}_{\pi^{t}}(\underline{Q})+\lambda\Delta_{1} \underline{Q}_{1}(s_{1},\pi_{1}^{t})+0.5\iota_{1}+0.5K\sum_{h=1}^{H}\nu_{h}^{2 }+2bK\sum_{h=1}^{H}\nu_{h}}{\lambda}+\frac{\lambda HC(\pi,\epsilon)}{2K}\] \[+H\epsilon+\sum_{h=1}^{H}\nu_{h}\] \[\leq\frac{\mathcal{L}_{\pi^{t}}(\mathrm{Proj}_{\mathcal{F}}(Q^{ \pi^{t}}))+\lambda\Delta_{1}\mathrm{Proj}_{\mathcal{F}_{1}}(Q_{1}^{\pi^{t}}) (s_{1},\pi_{1}^{t})+0.5\iota_{1}+\sum_{h=1}^{H}\nu_{h}^{2}+2bK\sum_{h=1}^{H} \nu_{h}}{\lambda}\] \[+\frac{\lambda HC(\pi,\epsilon)}{2K}+H\epsilon+\sum_{h=1}^{H}\nu _{h}\] \[\leq\frac{\iota_{2}+\lambda\xi_{1}+0.5\iota_{1}+\sum_{h=1}^{H}\nu _{h}^{2}+2bK\sum_{h=1}^{H}\nu_{h}}{\lambda}+\frac{\lambda HC(\pi,\epsilon)}{2 K}+H\epsilon+\sum_{h=1}^{H}\nu_{h},\]

where the second inequality uses the fact that \(\underline{Q}_{h}^{t}\) is a minimizer over \(\mathcal{F}\ni\mathrm{Proj}_{\mathcal{F}}(Q^{\pi^{t}})\) of \(\mathcal{L}_{\pi^{t}}(f)+\lambda f_{1}(s_{1},\pi_{1}^{t})\) (which has the same minimizer as \(\mathcal{L}_{\pi^{t}}(f)+\lambda\Delta_{1}f_{1}(s_{1},\pi_{1}^{t})\)), and the last inequality uses Lemma C.2, and here we define \(\iota_{2}:=36(e-2)b^{2}H\left(2d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+ \ln\frac{H}{\delta}\right)+6b(3b+4)\epsilon KH+15bK\sum_{h=1}^{H}\xi_{h}\). 

## Appendix E Proof of Theorem 3

In this appendix, we give our complete proof for Theorem 3. In order to develop our argument for proving Theorem 3, we shall start with a generalized form of posterior sampling in Section E.1 and develop our key support result in Proposition 2. We then use Proposition 2 and the similar machinery developed in Section D to complete our argument for proving Theorem 3.

### Generalized form of posterior and Proposition 2

We start with recalling the posterior distribution defined in Line 1 of Algorithm 4 as

\[\hat{p}(f|\mathcal{D},\pi)\propto\exp\left(-\lambda f_{1}(s_{1},\pi_{1})\right)p _{0}(f)\prod_{h\in[H]}\frac{\exp\left(-\gamma\hat{L}_{\pi}(f_{h},f_{h+1})\right) }{\mathbb{E}_{f_{h}^{\prime}\sim p_{0,h}}\exp\left(-\gamma\hat{L}_{\pi}(f_{h}^{ \prime},f_{h+1})\right)}.\] (9)

Similar to the proof strategy in Dann et al. (2021), we now consider a slightly more general form of the posterior distribution with an extra parameter \(\alpha\in[0,1]\) and in an equivalent but more useful form. In concrete, consider any \(\alpha\in[0,1]\) and define the potential functions:

\[\widehat{\Phi}_{h}(f,\pi;\mathcal{D}) :=-\ln p_{0}(f_{h})+\alpha\gamma\sum_{k=1}^{K}\Delta L_{\tilde{\pi }}(f_{h},f_{h+1};z_{h}^{k})\] \[+\alpha\ln\mathbb{E}_{\tilde{f}_{h}\sim p_{0}}\exp\left(-\gamma \sum_{k=1}^{K}\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h}^{k})\right),\] \[\widehat{\Phi}(f,\pi;\mathcal{D}) :=\sum_{h=1}^{H}\widehat{\Phi}_{h}(f,\pi;\mathcal{D}),\] \[\Delta_{1}f_{1}(s_{1},\pi) :=f_{1}(s_{1},\pi)-V_{1}^{\pi}(s_{1}).\]

where recall that \(\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h}^{k})\) is defined in Table 3. Define the generalized posterior distribution

\[\hat{p}(f|\mathcal{D},\pi)\propto\exp\left(-\widehat{\Phi}(f,\pi;\mathcal{D}) -\lambda\Delta f_{1}(s_{1},\pi)\right),\] (10)

where it is equivalent to the posterior defined in Equation (9) when \(\alpha=1\). We shall use Equation (10) for the posterior for the rest of this section. We shall also define the complexity measure of this generalized posterior - a counterpart to that of the canonical posterior form in Definition 2.

**Definition 7**.: _Define_

\[\kappa_{h}(\alpha,\epsilon,\tilde{\pi}):=(1-\alpha)\ln\mathbb{E}_{f_{h+1}\sim p _{0}}\left[p_{0,h}\left(\mathcal{F}_{h}^{\tilde{\pi}}(\epsilon;f_{h+1}) \right)^{-\alpha/(1-\alpha)}\right],\]

_where recall that \(\mathcal{F}_{h}^{\tilde{\pi}}(\epsilon;f_{h+1})=\{f^{\prime}\in\mathcal{F}_{h} :\sup_{s,a}|\mathcal{E}_{h}^{\tilde{\pi}}(f^{\prime},f_{h+1})(s,a)|\leq\epsilon\}\) which is defined in Definition 2. Define the complexity measure_

\[d_{0}(\epsilon,\alpha):=\sup_{T\in\mathbb{N},\tilde{\pi}\in\Pi^{soft}(T)}\sum_ {h=1}^{H}\kappa_{h}(\alpha,\epsilon,\tilde{\pi}).\] (11)

Note that we have

\[\lim_{\alpha\to 1^{-}}d_{0}(\epsilon,\alpha)=d_{0}(\epsilon).\]

We now state our key milestone result - Proposition 2 to support the argument for proving Theorem 3. The proof of Proposition 2 is deferred to Section E.3.

Notation \(\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})}\).Note that in Algorithm 4, each policy \(\pi^{t}\) for \(t\in[T]\) is a random variable that depends on both the offline data \(\mathcal{D}\) and the randomization of sampling from the posteriors. That is, when conditioned on the offline data \(\mathcal{D}\), each \(\pi_{t}\) is still a random variable. We denote \(P_{t}(\cdot|\mathcal{D})\) as the posterior distribution of \(\pi^{t}\) conditioned on \(\mathcal{D}\). Note that for any \(\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})\) and any \(t\in[T]\), we have \(\tilde{\pi}\in\Pi^{soft}(T)\).

**Proposition 2**.: _For any \(\gamma\in[0,\frac{1}{144(e-2)b^{2}}]\), \(\epsilon>0\), \(\delta>0\), \(\alpha\in(0,1]\), \(T\in\mathbb{N}\), and any \(t\in[T]\) and \(\lambda>0\), we have,_

\[\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot| \mathcal{D})}\mathbb{E}_{f\sim\tilde{p}(\cdot|\mathcal{D},\tilde{\pi})}\left[0.1 25\alpha\gamma K\sum_{h=1}^{H}\mathbb{E}_{\mu}[\mathcal{E}_{h}^{\tilde{\pi}}(f_ {h},f_{h+1})(s_{h},a_{h})^{2}]+\lambda\Delta f_{1}(s_{1},\tilde{\pi})\right]\]\[\lesssim\lambda\epsilon+\alpha\gamma Hb^{2}\cdot\max\{d_{\mathcal{F}}( \epsilon),d_{\Pi}(\epsilon,T),\ln\frac{\ln Kb^{2}}{\delta}\}+\alpha\gamma b^{2} KH\cdot\max\{\epsilon,\delta\}+\gamma HK\frac{\epsilon^{2}}{\alpha}\] \[+\sum_{h=1}^{H}\sup_{\tilde{\pi}_{h}\in\Pi_{h}^{soft}(T)}\kappa_{h} (\alpha,\epsilon,\tilde{\pi}_{h})+\sup_{\tilde{\pi}\in\Pi^{soft}(T)}\sum_{h=1}^ {H}\ln\frac{1}{p_{0}(\mathcal{F}_{h}(\epsilon;Q_{h}^{\tilde{\pi}_{h}}))}.\]

We now have all main components needed to construct our argument for proving Theorem3.

### Proof of Theorem3

Proof of Theorem3.: We start with the error decomposition argument.

Step 1: Error decomposition.Similar to the first step of the proof of Theorem2, using LemmaB.4, we have

\[\mathrm{SubOpt}_{\pi}^{M}(\pi^{t})=\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E} _{h}^{\pi^{t}}(Q_{h}^{t},\underline{Q}_{h+1}^{t})(s_{h},a_{h})]+\Delta_{1} \underline{Q}_{1}(s_{1},\pi_{1}^{t})+\mathrm{SubOpt}_{\pi}^{M_{t}}(\pi^{t})\]

where we denote \(M_{t}:=M(\underline{Q}^{t},\pi^{t})\) and \(\Delta_{1}\underline{Q}_{1}(s_{1},\pi^{t}):=\underline{Q}_{1}(s_{1},\pi^{t})- V_{1}^{\pi^{t}}(s_{1})\). Since term \(\sum_{t=1}^{T}\mathrm{SubOpt}_{\pi}^{M_{t}}(\pi^{t})\) can be controlled LemmaB.6, it remains to control

\[J :=\mathbb{E}_{\mathcal{D}}\left[\sum_{h=1}^{H}\mathbb{E}_{\pi}[ \mathcal{E}_{h}^{\pi^{t}}(\underline{Q}_{h}^{t},\underline{Q}_{h+1}^{t})(s_{ h},a_{h})]+\Delta_{1}\underline{Q}_{1}(s_{1},\pi_{1}^{t})\right]\] \[=\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot |\mathcal{D})}\mathbb{E}_{f\sim\tilde{\rho}(\cdot|\tilde{\pi},\mathcal{D})} \left[\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+ 1})(s_{h},a_{h})]+\Delta_{1}f_{1}(s_{1},\tilde{\pi}_{1})\right].\]

Step 2: Decoupling argument.Using LemmaB.5, we have

\[\sum_{h=1}^{H}\mathbb{E}_{\pi}[\mathcal{E}_{h}^{\tilde{\pi}}(f_{h },f_{h+1})(s_{h},a_{h})]+\Delta_{1}f_{1}(s_{1},\tilde{\pi}_{1})\] \[\leq\frac{0.125K\gamma}{\lambda}\sum_{h=1}^{H}\left(\mathbb{E}_{ \mu}[\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h},a_{h})^{2}]+\nu_{h}^{2 }+4b\nu_{h}\right)+\frac{0.5\lambda H\mathcal{C}(\pi,\epsilon_{c})}{K\gamma}+ \Delta_{1}f_{1}(s_{1},\tilde{\pi}_{1})\] \[+H\epsilon_{c}+\sum_{h=1}^{H}\nu_{h}\] \[=\frac{0.125K\gamma\sum_{h=1}^{H}\mathbb{E}_{\mu}[\mathcal{E}_{h}^ {\tilde{\pi}}(f_{h},f_{h+1})(s_{h},a_{h})^{2}]+\lambda\Delta_{1}f_{1}(s_{1}, \tilde{\pi}_{1})+\iota_{1}}{\lambda}+\frac{0.5\lambda H\mathcal{C}(\pi, \epsilon_{c})}{K\gamma}\] \[+H\epsilon_{c}+\sum_{h=1}^{H}\nu_{h}\]

where \(\iota_{1}:=0.125K\gamma\left(\sum_{h=1}^{H}\nu_{h}^{2}+4b\sum_{h=1}^{H}\nu_{h }\right)\).

Applying Proposition2, taking the limit \(\alpha\to 1^{-}\), and re-organizing the terms complete our proof.

It remains to prove Proposition2, which is the focus of the remaining appendix.

### Proof of Proposition2

Our proof strategy for Proposition2 builds upon Dann et al. (2021) where the central idea in the proof is to upper and lower bound the log-partition function - which in our case is as follows:

\[Z_{t}:=\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot| \mathcal{D})}\mathbb{E}_{f\sim\tilde{p}(\cdot|\mathcal{D},\tilde{\pi})}\left[ \tilde{\Phi}(f,\tilde{\pi};\mathcal{D})+\lambda\Delta f_{1}(s_{1},\tilde{\pi}) +\ln\hat{p}(f|\mathcal{D},\tilde{\pi})\right],\] (12)for any \(t\in[T]\) and any \(T\in\mathbb{N}\). The key technical distinction is that we need to handle the statistical dependence induced by \(\mathbb{E}_{\hat{\pi}\sim P_{t}(\cdot|\mathcal{D})}-\) which is absent in Dann et al. (2021). In concrete, when \(\tilde{\pi}\) depends on \(\mathcal{D}\), then

\[\mathbb{E}\Delta L_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k})\neq \mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k})^{2},\]

since \(\tilde{\pi}\) depends on \((s_{h}^{k},a_{h}^{k})\). We develop an machinery to handle such issue in posterior sampling by carefully controlling the variance of the variable of interest (thus we can leverage the variance-dependent concentration inequality in Lemma B.2) and integrating it into posterior sampling using a uniform convergence argument. Roughly speaking, several milestone results during the process of developing our proof argument, we need to bound the form of

\[\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})} \mathbb{E}_{f\sim\tilde{p}(\cdot|\mathcal{D},\tilde{\pi})}\left[S(f,\tilde{ \pi},\mathcal{D})\right]\]

where \(S(f,\tilde{\pi},\mathcal{D})\) is a function of \(f,\tilde{\pi},\mathcal{D}\). It is useful to view \(S(f,\tilde{\pi},\mathcal{D})\) as a stochastic process indexed by \((f,\tilde{\pi})\). In our machinery, we shall first construct an upper bound on the variance of the random process, namely

\[V(f,\tilde{\pi})\geq\mathbb{E}_{\mathcal{D}}[S(f,\tilde{\pi},\mathcal{D})^{2}].\]

Using a discretization argument, the union bound and Lemma B.2, we have with probability at least \(1-\delta\), for any \(f\in\mathcal{F},\tilde{\pi}\in\Pi^{soft}(T)\), for any \(t\in[0,\frac{1}{\sup S(f,\tilde{\pi},\mathcal{D})}]\), we have

\[S(f,\tilde{\pi},\mathcal{D})\leq O_{K}(1)+\mathbb{E}_{\mathcal{D}}\left[S(f, \tilde{\pi},\mathcal{D})\right]+(e-2)t\mathbb{E}_{\mathcal{D}}[S(f,\tilde{\pi },\mathcal{D})^{2}]+\frac{\ln(N/\delta)}{t}\]

where \(O_{K}(1)\) is a discretization error that can be controlled, and \(N\) is a covering number of \(\mathcal{F}\times\Pi^{soft}(T)\). Note that \(S(f,\tilde{\pi},\mathcal{D})\) often involves the squared loss which satisfies the Bernstein condition (see Lemma G.1) - thus we can roughly bound \(\mathbb{E}_{\mathcal{D}}[S(f,\tilde{\pi},\mathcal{D})^{2}]\leq\alpha\| \mathbb{E}_{\mathcal{D}}\left[S(f,\tilde{\pi},\mathcal{D})\right]|\) for some constant \(\alpha\). To integrate the high-probability bound into in-expected bound, we use the argument:

\[\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot |\mathcal{D})}\mathbb{E}_{f\sim\tilde{p}(\cdot|\mathcal{D},\tilde{\pi})}\left[ S(f,\tilde{\pi},\mathcal{D})\right]\leq O_{K}(1)+\mathbb{E}_{\mathcal{D}} \mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})}\mathbb{E}_{\mathcal{D}} \left[S(f,\tilde{\pi},\mathcal{D})\right]\] \[+(e-2)t\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t }(\cdot|\mathcal{D})}\mathbb{E}_{\mathcal{D}}[S(f,\tilde{\pi},\mathcal{D})^{2} ]+\frac{\ln(N/\delta)}{t}+\delta\sup S(f,\tilde{\pi},\mathcal{D}).\]

#### e.3.1 Lower-bounding log-partition function.

In this appendix, we give a lower bound of the log-partition function defined in Equation (12). The final lower bound is presented in Proposition 3. In order to establish such a lower bound, we first present a series of support lemmas that will culminate into Proposition 3.

The following lemma decomposes the log-partition function \(Z\) into different terms that we shall control separately.

**Lemma E.1**.: _For any \(t\in[T]\) and any \(T\in\mathbb{N}\), we have_

\[Z_{t} \geq\underbrace{\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi} \sim P_{t}(\cdot|\mathcal{D})}\mathbb{E}_{f\sim\tilde{p}(\cdot|\mathcal{D}, \tilde{\pi})}\left[\lambda\Delta f_{1}(s_{1},\tilde{\pi})+(1-0.5\alpha)\ln \frac{\hat{p}(f_{1}|\mathcal{D},\tilde{\pi})}{p_{0}(f_{1})}\right]}_{A_{t}}\] \[+0.5\alpha\sum_{h=1}^{H}\underbrace{\mathbb{E}_{\mathcal{D}} \mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})}\mathbb{E}_{f\sim\tilde{p }(\cdot|\mathcal{D},\tilde{\pi})}\left[2\gamma\sum_{k=1}^{K}\Delta L_{\tilde{ \pi}}(f_{h},f_{h+1};z_{h}^{k})+\ln\frac{\hat{p}(f_{h},f_{h+1}|\mathcal{D}, \tilde{\pi})}{p_{0}(f_{h},f_{h+1})}\right]}_{B_{h,t}}\] \[+\sum_{h=1}^{H}\underbrace{\mathbb{E}_{\mathcal{D}}\mathbb{E}_{ \tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})}\mathbb{E}_{f\sim\tilde{p}(\cdot| \mathcal{D},\tilde{\pi})}\left[\alpha\ln\mathbb{E}_{f_{h}^{\prime}\sim p_{0}} \exp\left(-\gamma\sum_{k=1}^{K}\Delta L_{\tilde{\pi}}(f_{h}^{\prime},f_{h+1}; z_{h}^{k})\right)+(1-\alpha)\ln\frac{\hat{p}(f_{h+1}|\mathcal{D},\tilde{\pi})}{p_{0}(f_{h+1 })}\right]}_{C_{h,t}}.\]

Proof of Lemma e.1.: This is a simple adaptation of the decomposition in (Dann et al., 2021, Lemma 6). 

We now control each term of the above decomposition of \(Z\) separately - where a majority of these steps are where our technical arguments depart from those in Dann et al. (2021). In particular, Lemma E.4, Lemma E.5, and Lemma E.7 are our _new_ technical results.

**Bounding \(A_{t}\).**

**Lemma E.2**.: _We have_

\[A_{t}\geq\lambda\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot| \mathcal{D})}\mathbb{E}_{f\sim\hat{p}(\cdot|\mathcal{D},\tilde{\pi})}\Delta f_{ 1}(s_{1},\tilde{\pi}).\]

Proof of Lemma e.2.: It simply follows from that:

\[\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})} \mathbb{E}_{f\sim\hat{p}(\cdot|\mathcal{D},\tilde{\pi})}\left[(1-0.5\alpha) \ln\frac{\hat{p}(f_{1}|\mathcal{D},\tilde{\pi})}{p_{0}(f_{1})}\right]=(1-0.5 \alpha)D_{\mathrm{KL}}[\hat{p}(\cdot|\mathcal{D},\tilde{\pi})\|p_{0}]\geq 0.\]

**Bounding \(B_{h,t}\).**

**Lemma E.3**.: _For any \(f,\tilde{\pi}\), \(0\leq\gamma\leq\frac{1}{72(e-2)b^{2}}\), and \(h\in[H]\), we have_

\[\ln\mathbb{E}_{(s_{h+1},r_{h})\sim P_{h}(\cdot|s_{h},a_{h})} \exp{(-2\gamma\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h}))}\leq-2\gamma(1-72(e -2)\gamma b^{2})\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h},a_{h})^{2}.\]

Proof of Lemma e.3.: For simplicity, we write \(\mathbb{E}=\mathbb{E}_{(s_{h+1},r_{h})\sim P_{h}(\cdot|s_{h},a_{h})}\). We have

\[\ln\mathbb{E}\exp{(-2\gamma\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z _{h}))} \leq\mathbb{E}\exp{(-2\gamma\Delta L_{\tilde{\pi}}(f_{h},f_{h+1}; z_{h}))}-1\] \[\leq-2\gamma\mathbb{E}\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h} )+(e-2)4\gamma^{2}\mathbb{E}\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h})^{2}\] \[\leq-2\gamma(1-(e-2)2\gamma 36b^{2})\mathcal{E}_{h}^{2}(f_{h},f_{h+1},\tilde{\pi})(s_{h},a_{h})\]

where the first inequality uses \(\ln x\leq x-1,\forall x\geq 0\), the second inequality uses \(e^{x}\leq 1+x+(e-2)x^{2},\forall|x|\leq 1\) and \(|2\gamma\mathbb{E}\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h})|\leq 18\gamma b^{2}\leq 1\), the third inequality uses Lemma B.1 and \(\gamma\leq\frac{1}{72(e-2)b^{2}}\). 

**Lemma E.4**.: _Define the random variable_

\[\xi_{h}^{\tilde{\pi}}(f_{h},f_{h+1};z_{h}):=-2\gamma\Delta L_{\tilde{\pi}}(f_{ h},f_{h+1};z_{h})-\ln\mathbb{E}_{(s_{h+1},r_{h})\sim P_{h}(\cdot|s_{h},a_{h})} \exp{(-2\gamma\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h}))}\,.\]

_For any \(\gamma\in[0,\frac{1}{144(e-2)b^{2}}]\), \(t\in[0,\frac{1}{26\gamma b^{2}}]\), \(\epsilon>0\), \(\delta>0\), \(T\in\mathbb{N}\) with probability at least \(1-\delta\), it holds uniformly over all \(\tilde{\pi}\in\Pi^{soft}(T),f_{h}\in\mathcal{F}_{h},f_{h+1}\in\mathcal{F}_{h+1}\) that_

\[\sum_{k=1}^{K}\xi_{h}^{\tilde{\pi}}(f_{h},f_{h+1};z_{h}^{k})\leq D+c\sum_{k=1} ^{K}e_{k}^{2},\]

_where_

\[\begin{cases}D&:=120\gamma b(b+2)K\epsilon+\frac{2d_{\mathcal{F}}(\epsilon)+d _{\mathcal{H}}(\epsilon,T)+\ln(1/\delta)}{t},\\ c&:=320b^{2}\gamma^{2}(e-2)t,\\ e_{k}&:=\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k}). \end{cases}\] (13)

Proof of Lemma e.4.: For simplicity, denote

\[\begin{cases}u_{k}&:=\ln\mathbb{E}_{(s_{h+1},r_{h})\sim P_{h}(\cdot|s_{h},a_{ h})}\exp{\left(-2\gamma\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h}^{k}) \right)}\,,\\ v_{k}&:=-2\gamma\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h}^{k}),\\ w_{k}&:=v_{k}-u_{k},\\ e_{k}&:=\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k}).\end{cases}\] (14)

We have

\[u_{k}\geq\mathbb{E}_{(s_{h+1},r_{h})\sim P_{h}(\cdot|s_{h},a_{h})}\ln\exp{ \left(-2\gamma\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h}^{k})\right)}=-2\gamma e _{k}^{2},\]where the first inequality uses Jensen's inequality for concave function \(\ln(\cdot)\) and the equality uses Lemma B.1. Now using Lemma E.3 with \(\gamma\leq\frac{1}{144(e-2)b^{2}}\), we have

\[u_{k}\leq-\gamma e_{k}^{2}.\] (15)

We also have \(\mathbb{E}v_{k}=-2\gamma e_{k}^{2}\) by Lemma B.1. Thus, we have

\[|u_{k}|\leq 2\gamma e_{k}^{2},\text{ and }\mathbb{E}[w_{k}]=-2\gamma e_{k}^{2}-u_ {k}\leq 0.\]

Hence, we have

\[\mathbb{E}w_{k}^{2} =\mathbb{E}(v_{k}-u_{k})^{2}\] \[\leq 2\mathbb{E}(v_{k}^{2}+u_{k}^{2})\] \[\leq 288b^{2}\gamma^{2}e_{k}^{2}+8\gamma^{2}e_{k}^{4}\] \[\leq 320b^{2}\gamma^{2}e_{k}^{2}\]

where the first inequality uses Cauchy-Schwartz inequality, the second inequality uses Lemma B.1 and that \(|u_{k}|\leq 2\gamma e_{k}^{2}\), and the last inequality uses that \(|e_{k}|\leq 2b\). Also note that \(|w_{k}|\leq|v_{k}|+|u_{k}|\leq 2\gamma(9b^{2})+2\gamma(4b^{2})=26\gamma b^{2}\). Thus, by Lemma B.2, for any \(\delta>0\), for any \(t\in[0,\frac{1}{26\gamma b^{2}}]\), with probability at least \(1-\delta\), we have

\[\sum_{k=1}^{K}w_{k} \leq\sum_{k=1}^{K}\mathbb{E}w_{k}+(e-2)t\cdot\mathbb{E}\sum_{k=1} ^{K}w_{k}^{2}+\frac{\ln(1/\delta)}{t}\] \[\leq 320b^{2}\gamma^{2}(e-2)t\sum_{k=1}^{K}e_{k}^{2}+\frac{\ln(1/ \delta)}{t}.\]

We apply the discretization argument and the union bound to obtain that: For any \(\delta>0,\epsilon>0\), \(T\in\mathbb{N}\) it holds uniformly over all \(\tilde{\pi}\in\Pi_{h}^{soft}(T),f_{h}\in\mathcal{F}_{h},f_{h+1}\in\mathcal{F}_ {h+1}\) that

\[\sum_{k=1}^{K}w_{k}\leq 120\gamma b(b+2)K\epsilon+320b^{2}\gamma^{2}(e-2)t \sum_{k=1}^{K}e_{k}^{2}+\frac{2d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+ \ln(1/\delta)}{t}.\]

**Lemma E.5**.: _For any \(\gamma\in[0,\frac{1}{144(e-2)b^{2}}]\), \(\epsilon>0\), \(\delta>0\), we have_

\[B_{h,t} \geq 0.5\gamma\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi} \sim P_{t}(\cdot|\mathcal{D})}\mathbb{E}_{f\sim\tilde{p}(\cdot|\mathcal{D}, \tilde{\pi})}\left[\sum_{k=1}^{K}\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})( s_{h}^{k},a_{h}^{k})^{2}\right]\] \[\geq-120\gamma b(b+2)K\epsilon-640(e-2)b^{2}\gamma\left(2d_{ \mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln(1/\delta)\right)-26\gamma b^{2 }K\delta.\]

Proof of Lemma e.5.: Define the random variables \(u_{k},v_{k},w_{k},e_{k}\) as Equation (14). Recall \(D,c\) are defined in Equation (13) for any \(t\in[0,\frac{1}{26\gamma b^{2}}]\). Define the event \(E\) such that the inequality

\[\sum_{k=1}^{K}\xi_{h}^{\tilde{\pi}}(f_{h},f_{h+1};z_{h}^{k})\leq \underbrace{320b^{2}\gamma^{2}(e-2)t}_{c}\sum_{k=1}^{K}e_{k}^{2}+D,\] (16)

holds uniformly over all \(\tilde{\pi}\in\Pi^{soft}(T),f_{h}\in\mathcal{F}_{h},f_{h+1}\in\mathcal{F}_{h+1}\). By Lemma E.4, we have

\[\Pr(E)\geq 1-\delta,\text{ thus }\Pr(E^{c})\leq\delta.\]

We have

\[\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})} \mathbb{E}_{f\sim\tilde{p}(\cdot|\mathcal{D},\tilde{\pi})}\left[\sum_{k=1}^{K} (-w_{k}+ce_{k}^{2})+\ln\frac{\tilde{p}(f_{h},f_{h+1}|\mathcal{D},\tilde{\pi})} {p_{0}(f_{h},f_{h+1})}\right]\]\[\geq\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot| \mathcal{D})}\inf_{p}\mathbb{E}_{f\sim p}\left[\sum_{k=1}^{K}(-w_{k}+ce_{k}^{2})+ \ln\frac{p(f_{h},f_{h+1})}{p_{0}(f_{h},f_{h+1})}\right]\] \[=-\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot |\mathcal{D})}\ln\mathbb{E}_{f_{h},f_{h+1}\sim p_{0}}\exp\left(\sum_{k=1}^{K}( w_{k}-ce_{k}^{2})\right)\] \[=-\mathbb{E}_{\mathcal{D}}1\{E\}\mathbb{E}_{\tilde{\pi}\sim P_{t} (\cdot|\mathcal{D})}\ln\mathbb{E}_{f_{h},f_{h+1}\sim p_{0}}\exp\left(\sum_{k=1 }^{K}(w_{k}-ce_{k}^{2})\right)\] \[\geq-\mathbb{E}_{\mathcal{D}}1\{E\}\mathbb{E}_{\tilde{\pi}\sim P_ {t}(\cdot|\mathcal{D})}\ln\mathbb{E}_{f_{h},f_{h+1}\sim p_{0}}\exp\left(\sum_{ k=1}^{K}(w_{k}-ce_{k}^{2})\right)-26\gamma b^{2}K\delta\] \[\geq-D-26\gamma b^{2}K\delta,\] (17)

where the first equality uses Lemma G.3, the second inequality uses that \(\Pr(E^{c})\leq\delta\) and \(\sum_{k=1}^{K}(w_{k}-ce_{k}^{2})\leq\sum_{k=1}^{K}w_{k}\leq 26\gamma b^{2}K\) and the last inequality uses Equation (16). Thus, using the same notations as Lemma Lemma E.4, we have

\[B_{h,t} =\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot |\mathcal{D})}\mathbb{E}_{f\sim\hat{p}(\cdot|\mathcal{D},\tilde{\pi})}\left[- \sum_{k=1}^{K}v_{k}+\ln\frac{\hat{p}(f_{h},f_{h+1}|\mathcal{D},\tilde{\pi})}{p _{0}(f_{h},f_{h+1})}\right]\] \[=\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot |\mathcal{D})}\mathbb{E}_{f\sim\hat{p}(\cdot|\mathcal{D},\tilde{\pi})}\left[ \sum_{k=1}^{K}(-w_{k}+ce_{k}^{2})+\ln\frac{\hat{p}(f_{h},f_{h+1}|\mathcal{D}, \tilde{\pi})}{p_{0}(f_{h},f_{h+1})}\right]\] \[+\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot |\mathcal{D})}\mathbb{E}_{f\sim\hat{p}(\cdot|\mathcal{D},\tilde{\pi})}\left[ \sum_{k=1}^{K}(-u_{k}-ce_{k}^{2})\right]\] \[\geq-D-26\gamma b^{2}K\delta+(\gamma-c)\mathbb{E}_{\mathcal{D}} \mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})}\mathbb{E}_{f\sim\hat{p} (\cdot|\mathcal{D},\tilde{\pi})}\left[\sum_{k=1}^{K}e_{k}^{2}\right]\]

where the inequality uses Equation (17) and Equation (15). Finally, setting

\[t=\frac{1}{640b^{2}(e-2)\gamma}<\frac{1}{13b^{2}\gamma}\]

completes our proof.

From squared Bellman errors to _in-expectation_ squared Bellman errors and fixing a non-rigorous argument of Dann et al. (2021).Lemma E.5 only bounds \(B_{h}\) with the squared Bellman errors \(\sum_{k=1}^{K}\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k}) ^{2}\) while the _in-expectation_ squared Bellman errors \(\sum_{k=1}^{K}\mathbb{E}_{\mu^{k}}[\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1}) (s_{h},a_{h})^{2}]\) are what we need for showing Proposition 2. There is no an immediate path to go from the squared Bellman error to the in-expectation squared Bellman errors as the order of \(\mathbb{E}_{\mathcal{D}}\) and \(\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})}\mathbb{E}_{f\sim\hat{p}( \cdot|\mathcal{D},\tilde{\pi})}\) are **not** exchangeable, i.e.,

\[\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot |\mathcal{D})}\mathbb{E}_{f\sim\hat{p}(\cdot|\mathcal{D},\tilde{\pi})}\left[ \sum_{k=1}^{K}\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k}) ^{2}\right]\] \[\neq\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}( \cdot|\mathcal{D})}\mathbb{E}_{f\sim\hat{p}(\cdot|\mathcal{D},\tilde{\pi})} \left[\sum_{k=1}^{K}\mathbb{E}_{\mu^{k}}[\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_ {h+1})(s_{h},a_{h})^{2}]\right].\] (18)

A similar caveat arises in the online setting in Dann et al. (2021) as well. In particular, a non-rigorous argument of Dann et al. (2021, Lemma 8) is that they conclude (an online analogue of) the LHS of Equation (18) is equal to (an online analogue of) its RHS.

[MISSING_PAGE_FAIL:35]

**Bounding \(C_{h,t}\).**

**Lemma E.8**.: _For any \(\epsilon>0\) and \(T\in\mathbb{N}\), we have_

\[C_{h,t}\geq-\max_{\tilde{\pi}\in\Pi_{h}^{soft}(T)}\kappa_{h}(\alpha,\epsilon, \tilde{\pi})-\gamma\alpha 6bK\epsilon.\]

_where \(\kappa_{h}(\alpha,\epsilon,\tilde{\pi})\) is defined in Equation (11)._

Proof of Lemma E.8.: We have

\[C_{h,t}=\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{ t}(\cdot|\mathcal{D})}\mathbb{E}_{f\sim\tilde{p}(\cdot|\mathcal{D},\tilde{ \pi})}\left[\alpha\ln\mathbb{E}_{f_{h}^{\prime}\sim p_{0}}\exp\left(-\gamma \sum_{k=1}^{K}\Delta L_{\tilde{\pi}}(f_{h}^{\prime},f_{h+1};z_{h}^{k})\right)+ (1-\alpha)\ln\frac{\tilde{p}(f_{h+1}|\mathcal{D},\tilde{\pi})}{p_{0}(f_{h+1})}\right]\] \[=(1-\alpha)\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P _{t}(\cdot|\mathcal{D})}\mathbb{E}_{f\sim\tilde{p}(\cdot|\mathcal{D},\tilde{ \pi})}\left[\frac{\alpha}{1-\alpha}\ln\mathbb{E}_{f_{h}^{\prime}\sim p_{0}} \exp\left(-\gamma\sum_{k=1}^{K}\Delta L_{\tilde{\pi}}(f_{h}^{\prime},f_{h+1};z_ {h}^{k})\right)+\ln\frac{\tilde{p}(f_{h+1}|\mathcal{D},\tilde{\pi})}{p_{0}(f_ {h+1})}\right]\] \[\geq-(1-\alpha)\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi} \sim P_{t}(\cdot|\mathcal{D})}\ln\mathbb{E}_{f_{h+1}\sim p_{0}}\left(\mathbb{E }_{f_{h}^{\prime}\sim p_{0}}\exp\left(-\gamma\sum_{k=1}^{K}\Delta L_{\tilde{ \pi}}(f_{h}^{\prime},f_{h+1};z_{h}^{k})\right)\right)^{\frac{-\alpha}{1-\alpha }}\] \[\geq-\max_{\tilde{\pi}\in\Pi^{soft}(T)}\kappa_{h}(\alpha,\epsilon, \tilde{\pi})-\gamma\alpha 6bK\epsilon.\]

where the first inequality uses Lemma G.3 and the last inequality uses the following inequalities: For any \(f_{h}\in\mathcal{F}_{h}(\epsilon,f_{h+1},\tilde{\pi})\), we have

\[|\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h})| \leq 6b|\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})|\leq 6b \epsilon;\text{ thus}\] \[\mathbb{E}_{f_{h}^{\prime}\sim p_{0}}\exp\left(-\gamma\sum_{k=1} ^{K}\Delta L_{\tilde{\pi}}(f_{h}^{\prime},f_{h+1};z_{h})\right) \geq p_{0,h}(\mathcal{F}_{h}^{\tilde{\pi}}(\epsilon,f_{h+1}))\cdot \exp(-\gamma 6bK\epsilon).\]

We are now ready to state the complete form of the lower bound of \(Z\).

**Proposition 3**.: _For any \(\gamma\in[0,\frac{1}{144(e-2)b^{2}}]\), \(\epsilon>0\), \(\delta>0\), and any \(t\in[T]\), we have,_

\[Z \geq\lambda\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P _{t}(\cdot|\mathcal{D})}\mathbb{E}_{f\sim\tilde{p}(\cdot|\mathcal{D},\tilde{ \pi})}\Delta f_{1}(s_{1},\tilde{\pi})\] \[+0.125\alpha\gamma\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}\mathbb{E }_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})}\mathbb{E}_{f\sim\tilde{p}(\cdot| \mathcal{D},\tilde{\pi})}\sum_{k=1}^{K}\mathbb{E}_{\mu^{k}}\left[\mathcal{E}_{ h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h},a_{h})^{2}\right]\] \[-0.5\alpha H\left(120\gamma b(b+2)K\epsilon+640(e-2)\gamma b^{2} \left(2d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln(1/\delta)\right)-13 \alpha\gamma b^{2}KH\delta\right.\] \[-0.25\alpha\gamma H\left(b(b+2)K\epsilon+\frac{32}{3}b^{2}\left(2 d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln\frac{14Kb^{2}}{\delta}\right)+1+2Kb^{2} \delta\right)\] \[-\sum_{h=1}^{H}\max_{\tilde{\pi}\in\Pi^{soft}(T)}\kappa_{h}( \alpha,\epsilon,\tilde{\pi})-\gamma\alpha 6bKH\epsilon.\]

Proof of Proposition 3.: Using Lemma E.1, it suffices to bound terms \(A\), \(B_{h}\), and \(C_{h}\) defined in Lemma E.1. For this purpose, we use

* Lemma E.2: To bound \(A_{t}\),
* Lemma E.5 and Lemma E.7: To bound \(B_{h,t}\),
* Lemma E.8: To bound \(C_{h,t}\).

The result is then simply a direct combination of the above lemmas.

[MISSING_PAGE_FAIL:37]

\[\leq\left(\alpha\gamma+t(e-2)36b^{2}\gamma^{2}\right)\sum_{k=1}^{K}e_{k}^{2}+ \frac{\ln(1/\delta)}{t}.\]

Using the discretization argument and the union bound, we have that: For any \(\epsilon>0,\delta>0\), we have

\[\Pr(E)\geq 1-\delta,\text{ thus }\Pr(E^{c})\leq\delta,\]

where \(E\) denotes that event that for any \(t\in[0,\frac{1}{13\alpha\gamma b^{2}}]\),

\[\sum_{k=1}^{K}x_{k}\leq 30\alpha\gamma b(b+2)K\epsilon+\left(\alpha\gamma+t(e- 2)36b^{2}\alpha^{2}\gamma^{2}\right)\sum_{k=1}^{K}e_{k}^{2}+\frac{2d_{\mathcal{ F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln(1/\delta)}{t},\]

any \(f_{h}\in\mathcal{F}_{h}\), \(f_{h+1}\in\mathcal{F}_{h+1},\tilde{\pi}\in\Pi_{h}^{soft}(T)\). Thus, we have

\[\leq\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}( \cdot|\mathcal{D})}\mathbb{E}_{f\sim p}\bigg{[}30\alpha\gamma b(b+2)K\epsilon\] \[+\left(\alpha\gamma+t(e-2)36\alpha^{2}b^{2}\gamma^{2}\right)\sum_ {k=1}^{K}e_{k}^{2}+\frac{2d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln(1 /\delta)}{t}\bigg{]}+9\alpha\gamma Kb^{2}\delta\] \[\leq\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}( \cdot|\mathcal{D})}\mathbb{E}_{f\sim p}\bigg{[}30b(b+2)K\epsilon\] \[+\left(\alpha\gamma+t(e-2)36\alpha^{2}b^{2}\gamma^{2}\right)\sum_ {k=1}^{K}e_{k}^{2}+\frac{2d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln(1 /\delta)}{t}\bigg{]}+9\alpha\gamma Kb^{2}\delta.\]

Picking \(t=\frac{1}{13\alpha\gamma b^{2}}\) completes the proof. 

The following lemma bounds the in-expectation negation of the loss proxy \(\Delta L_{\tilde{\pi}}\).

**Lemma E.10**.: _For any \(\delta>0,\epsilon>0,\gamma>0\), any \(\tilde{f}_{h}\in\mathcal{F}_{h}\), any \(t\in[T]\), and any distribution \(p\) over \(\mathcal{F}\), we have_

\[\leq 36(e-2)\gamma b^{2}\left(d_{\mathcal{F}}(\epsilon)+d_{\Pi}( \epsilon,T)+\ln(1/\delta)\right)\] \[+9\gamma Kb^{2}\delta+30\gamma b(b+2)K\epsilon.\]

Proof of Lemma e.10.: For simplicity, define

\[y_{k} :=-\gamma\Delta L_{\tilde{\pi}}(f_{h},f_{h+1};z_{h}^{k}),\] \[e_{k} :=\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k}).\]

By Lemma B.1, we have

\[\mathbb{E}[y_{k}] =-\gamma e_{k}^{2},\] \[\mathbb{E}[y_{k}^{2}] \leq 36b^{2}\gamma^{2}e_{k}^{2}.\]

Thus, by Lemma B.2, for any \(\delta>0\), with probability at least \(1-\delta\), for any \(t\in[0,\frac{1}{13\gamma b^{2}}]\) we have

\[\sum_{k=1}^{K}y_{k} \leq\sum_{k=1}^{K}\mathbb{E}[y_{k}]+t(e-2)\sum_{k=1}^{K}\mathbb{ E}[y_{k}^{2}]+\frac{\ln(1/\delta)}{t}\] \[\leq-\gamma\left(1-36t(e-2)b^{2}\gamma\right)\sum_{k=1}^{K}e_{k} ^{2}+\frac{\ln(1/\delta)}{t}.\]Setting \(t=\frac{1}{36(e-2)\theta^{2}\gamma}<\frac{1}{13\gamma b^{2}}\) in the above inequality, we obtain

\[\sum_{k=1}^{k}y_{k}\leq 36(e-2)b^{2}\gamma\ln(1/\delta).\]

Using the discretization argument and the union bound, we have that: For any \(\epsilon>0,\delta>0\), we have

\[\Pr(E)\geq 1-\delta,\text{ thus }\Pr(E^{c})\leq\delta,\]

where \(E\) denotes that event,

\[\sum_{k=1}^{K}y_{k}\leq 30\gamma b(b+2)K\epsilon+36(e-2)\gamma b^{2}\left(d_{ \mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln(1/\delta)\right),\]

any \(f_{h+1}\in\mathcal{F}_{h+1},\tilde{\pi}\in\Pi_{h}^{soft}(T)\). Thus, we have

\[\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot |\mathcal{D})}\mathbb{E}_{f\sim p}\left[\sum_{k=1}^{K}y_{k}\right]=\mathbb{E}_ {\mathcal{D}}1\{E\}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})} \mathbb{E}_{f\sim p}\left[\sum_{k=1}^{K}y_{k}\right]+\mathbb{E}_{\mathcal{D}}1 \{E^{c}\}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})}\mathbb{E}_{f \sim p}\left[\sum_{k=1}^{K}y_{k}\right]\] \[\leq 30\gamma b(b+2)K\epsilon+36(e-2)\gamma b^{2}\left(d_{ \mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln(1/\delta)\right)+9\gamma Kb^{2}\delta.\]

\(\Box\)

The following lemma bounds the in-expectation squared Bellman errors with the regularization term and the data distribution term, under the infimum realization of the data distribution \(p\).

**Lemma E.11**.: _For any \(\epsilon>0,\beta\geq 0\), any \(t\in[T]\), we have_

\[\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot |\mathcal{D})}\inf_{p}\mathbb{E}_{f\sim p}\bigg{[}\lambda\Delta f_{1}(s_{1}, \tilde{\pi})+\ln\frac{p(f)}{p_{0}(f)}+\beta\sum_{h=1}^{H}\sum_{k=1}^{K} \mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k})^{2}\bigg{]}\] \[\leq\lambda\epsilon-\inf_{\tilde{\pi}\in\Pi^{soft}(T)}\ln p_{0}( \mathcal{F}_{h}(\epsilon;Q_{h}^{\tilde{\pi}}))+4\beta HK\epsilon^{2}.\]

_where recall that \(\mathcal{F}_{h}(\epsilon;f_{h+1})\) is defined in Definition 2._

Proof of Lemma e.11.: For any \(f\in\mathcal{F}(\epsilon;Q^{\tilde{\pi}})\), we have

\[\|f_{h}-Q_{h}^{\tilde{\pi}}\|_{\infty}\leq\epsilon,\forall h.\]

Thus, we have

\[|\mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s,a)| \leq\|\mathbb{T}_{h}^{\tilde{\pi}}f_{h}-f_{h+1}\|_{\infty}=\| \mathbb{T}_{h}^{\tilde{\pi}}f_{h}-\mathbb{T}_{h}^{\tilde{\pi}}Q_{h}^{\tilde{ \pi}}-f_{h+1}+Q_{h+1}^{\tilde{\pi}}\|_{\infty}\] \[\leq\|\mathbb{T}_{h}^{\tilde{\pi}}f_{h}-\mathbb{T}_{h}^{\tilde{ \pi}}Q_{h}^{\tilde{\pi}}\|_{\infty}+\|f_{h+1}-Q_{h+1}^{\tilde{\pi}}\|_{\infty}\] \[\leq 2\epsilon.\]

Thus, by choosing

\[p(f)=\frac{p_{0}(f)1\{f\in\mathcal{F}(\epsilon;\tilde{\pi})\}}{p_{0}( \mathcal{F}(\epsilon;\tilde{\pi}))},\]

we have

\[\inf_{p}\mathbb{E}_{f\sim p}\bigg{[}\lambda\Delta f_{1}(s_{1}, \tilde{\pi})+\ln\frac{p(f)}{p_{0}(f)}+\beta\sum_{h=1}^{H}\sum_{k=1}^{K} \mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h}^{k},a_{h}^{k})^{2}\bigg{]}\] \[\leq\lambda\epsilon-\ln p_{0}(\mathcal{F}_{h}(\epsilon;\tilde{\pi }))+4\beta HK\epsilon^{2}.\]

\(\Box\)

We by now have everything needed to prove Proposition 2.

#### e.3.3 Proof of Proposition 2

Proof of Proposition 2.: By Proposition 3, we have

\[Z_{t} \geq\lambda\mathbb{E}_{\mathcal{P}}\mathbb{E}_{\tilde{\pi}\sim P_{t} (\cdot|\mathcal{D})}\mathbb{E}_{\tilde{f}\sim\dot{\rho}(\cdot|\mathcal{D},\tilde {\pi})}\Delta f_{1}(s_{1},\tilde{\pi})\] \[+0.125\alpha\gamma\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}\mathbb{ E}_{\tilde{\pi}\sim P_{t}(\cdot|\mathcal{D})}\mathbb{E}_{\tilde{f}\sim\dot{ \rho}(\cdot|\mathcal{D},\tilde{\pi})}\sum_{k=1}^{K}\mathbb{E}_{\mu^{k}}\left[ \mathcal{E}_{h}^{\tilde{\pi}}(f_{h},f_{h+1})(s_{h},a_{h})^{2}\right]\] \[-0.5\alpha H\left(120\gamma b(b+2)K\epsilon+640(e-2)\gamma b^{2} \left(2d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln(1/\delta)\right) \right)-13\alpha\gamma b^{2}KH\delta\] \[-0.25\alpha\gamma H\left(b(b+2)K\epsilon+\frac{32}{3}b^{2}\left(2 d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\frac{\ln\ln 4Kb^{2}}{\delta} \right)+1+2Kb^{2}\delta\right)\] \[-\sum_{h=1}^{H}\max_{\tilde{\pi}\in\Pi^{\circ f_{t}}(T)}\kappa_{ h}(\alpha,\epsilon,\tilde{\pi})-\gamma\alpha 6bKH\epsilon.\]

By Proposition 4, we have

\[Z_{t} \leq\lambda\epsilon-\inf_{\tilde{\pi}\in\Pi^{\circ f_{t}}(T)} \sum_{h=1}^{H}\ln p_{0}(\mathcal{F}_{h}(\epsilon;\tilde{\pi}))+4\gamma\left( \alpha+\frac{3(e-2)}{\alpha}\right)HK\epsilon^{2}\] \[+60\alpha\gamma b(b+2)KH\epsilon+\alpha b^{2}\gamma H\left(13+36 (e-2)\right)\left(2d_{\mathcal{F}}(\epsilon)+d_{\Pi}(\epsilon,T)+\ln(1/\delta )\right)+18\alpha\gamma KHb^{2}\delta.\]

Thus, we have

\[\mathbb{E}_{\mathcal{D}}\mathbb{E}_{\tilde{\pi}\sim P_{t}(\cdot| \mathcal{D})}\mathbb{E}_{\tilde{f}\sim\dot{\rho}(\cdot|\mathcal{D},\tilde{\pi })}\left[0.125\alpha\gamma K\sum_{h=1}^{H}\mathbb{E}_{\mu}[\mathcal{E}_{h}^{ \tilde{\pi}}(f_{h},f_{h+1})(s_{h},a_{h})^{2}]+\lambda\Delta f_{1}(s_{1},\tilde{ \pi})\right]\] \[\lesssim\lambda\epsilon+\alpha\gamma Hb^{2}\cdot\max\{d_{ \mathcal{F}}(\epsilon),d_{\Pi}(\epsilon,T),\ln\frac{\ln Kb^{2}}{\delta}\}+ \alpha\gamma b^{2}KH\cdot\max\{\epsilon,\delta\}+\gamma HK\frac{\epsilon^{2}}{\alpha}\] \[+\sum_{h=1}^{H}\max_{\tilde{\pi}_{h}\in\Pi^{\circ f_{t}}_{h}} \kappa_{h}(\alpha,\epsilon,\tilde{\pi}_{h})+\sup_{\tilde{\pi}\in\Pi^{\circ f _{t}}(T)}\sum_{h=1}^{H}\ln\frac{1}{p_{0}(\mathcal{F}_{h}(\epsilon;Q_{h}^{ \tilde{\pi}_{h}}))}.\]

## Appendix F Proof of Proposition 1

In this appendix, we prove Proposition 1, which is a simple reduction from Theorem 1, Theorem 2, and Theorem 3.

Proof of Proposition 1.: We recall that Proposition 1 consists of two parts of statements: Part (i) - the simplified bounds of all three algorithms into one unified form under no misspecification, and Part (ii) - the specialization of the unified bound into the special cases of finite function classes and linear function classes.

Part (i): The unified sub-optimality bounds for VS, RO, and PS

We recall that the first part of Proposition 1 is that:

\[\forall\tilde{\pi}\in\{\hat{\pi}^{vs},\hat{\pi}^{ro},\hat{\pi}^{ ps}\},\mathbb{E}_{\mathcal{D}}\mathrm{SubOpt}_{\pi}(\hat{\pi})=\tilde{\mathcal{O}} \left(\frac{Hb}{\sqrt{K}}\sqrt{\tilde{d}(1/K)\cdot\mathcal{C}(\pi;1/\sqrt{K})} +\frac{Hb\sqrt{\ln\mathrm{Vol}(\mathcal{A})}}{T}\right),\] (19)

where

\[\tilde{d}(1/K)=\begin{cases}\tilde{d}_{opt}(1/K,T)&\text{ if }\hat{\pi}\in\{ \hat{\pi}^{vs},\hat{\pi}^{ro}\},\\ \tilde{d}_{ps}(1/K,T)&\text{ if }\hat{\pi}=\hat{\pi}^{ps},\end{cases}\]

where we recall in Section 4.2 that

\[\tilde{d}_{opt}(\epsilon,T):=\max\{d_{\mathcal{F}}(\epsilon),d_{\Pi}(\epsilon, T)\},\]\[\tilde{d}_{ps}(\epsilon,T):=\max\{d_{\mathcal{F}}(\epsilon),d_{\Pi}(\epsilon,T), \frac{d_{0}(\epsilon)}{\gamma Hb^{2}},\frac{d_{0}^{\prime}(\epsilon)}{\gamma Hb^{ 2}}\},\]

and \(d_{\mathcal{F}}(\epsilon)\), \(d_{\Pi}(\epsilon,T)\), \(d_{0}(\epsilon)\), and \(d_{0}^{\prime}(\epsilon)\) are defined in Section 2.4. Also recall that for Proposition 1, we assume that there is no misspecification, i.e., \(\xi_{h}=\nu_{h}=0,\forall h\in[H]\).

For \(\hat{\pi}^{vs}\).It follows from Theorem 1, where we choose \(\epsilon_{c}=1/\sqrt{K}\), and \(\epsilon=1/K\) that with probability at least \(1-2\delta\), we have

\[\begin{split}\mathrm{SubOpt}_{\pi}(\hat{\pi}^{vs})& \lesssim\sqrt{K^{-1}\cdot H\cdot\mathcal{C}(\pi;1/\sqrt{K})(Hb^{2} \max\{\tilde{d}_{opt}(1/K,T),\ln(H/\delta)\}+b^{2}H)}+H/\sqrt{K}+\zeta_{opt} \\ &\lesssim\sqrt{K^{-1}\cdot H^{2}b^{2}\cdot\mathcal{C}(\pi;1/ \sqrt{K})\max\{\tilde{d}_{opt}(1/K,T),\ln(H/\delta)\}}+\frac{Hb\sqrt{\ln\mathrm{ Vol}(\mathcal{A})}}{T}\end{split}\]

Thus we have

\[\begin{split}\mathrm{SubOpt}_{\pi}(\hat{\pi}^{vs})& =\mathcal{O}\left(\frac{Hb}{\sqrt{K}}\sqrt{\mathcal{C}(\pi;1/\sqrt{K}) \cdot\max\{\tilde{d}_{opt}(1/K,T),\ln(H/\delta)\}}+\frac{Hb\sqrt{\ln\mathrm{ Vol}(\mathcal{A})}}{T}\right).\end{split}\] (20)

For \(\hat{\pi}^{ro}\).The sub-optimality bound for \(\hat{\pi}^{ro}\) is obtained from Theorem 2 with the same parameter setting as that for \(\hat{\pi}^{vs}\), where we set \(\epsilon=1/K\), \(\epsilon_{c}=1/\sqrt{K}\), and \(T\geq K\ln\mathrm{Vol}(\mathcal{A})\). Additionally, we shall need to set the regularization parameter \(\lambda\). Since the bound in Theorem 2 holds for any \(\lambda>0\), we shall minimize this bound with respect to \(\lambda>0\), which results in the optimal \(\lambda\) as

\[\lambda_{*}=\sqrt{\frac{2KHb^{2}\cdot\max\{\tilde{d}_{opt}(1/K,T),\ln(H/ \delta)\}}{H\cdot\mathcal{C}(\pi,1/\sqrt{K})}}.\]

and the sub-optimality bound as

\[\begin{split}\mathrm{SubOpt}_{\pi}(\hat{\pi}^{ro})& =\mathcal{O}\left(\frac{Hb}{\sqrt{K}}\sqrt{\mathcal{C}(\pi;1/\sqrt{K}) \cdot\max\{\tilde{d}_{opt}(1/K,T),\ln(H/\delta)\}}+\frac{Hb\sqrt{\ln\mathrm{ Vol}(\mathcal{A})}}{T}\right).\end{split}\] (21)

For \(\hat{\pi}^{ps}\).We specialize the sub-optimality of \(\hat{\pi}^{ps}\) from Theorem 3. Similar to the case of \(\hat{\pi}^{vs}\) and \(\hat{\pi}^{ro}\), we set: \(\epsilon=1/K\), \(\epsilon_{c}=1/\sqrt{K}\), and \(T\geq K\ln\mathrm{Vol}(\mathcal{A})\). Additionally, we need to set the failure probability \(\delta\in[0,1]\), the learning rate \(\gamma\in[0,\frac{1}{144(e-2)b^{2}}]\) and the regularization parameter \(\lambda>0\). For \(\delta\), we set \(\delta=1/K\). For \(\lambda\), we minimize the bound in Theorem 3 with respect to \(\lambda\), which results into \(\lambda=\lambda_{*}\) which is give as

\[\lambda_{*}=\gamma\sqrt{\frac{KHb^{2}\cdot\max\{\tilde{d}_{ps}(1/K,T),\ln(K \ln(Kb^{2}))\}}{H\cdot\mathcal{C}(\pi,1/\sqrt{K})}},\]

turns the sub-optimality bound into

\[\mathbb{E}_{\mathcal{D}}\mathrm{SubOpt}_{\pi}(\hat{\pi}^{ps})=\mathcal{O} \left(\frac{Hb}{\sqrt{K}}\sqrt{\mathcal{C}(\pi;1/\sqrt{K})\cdot\max\{\tilde{d }_{ps}(1/K,T),\ln(K\ln(Kb^{2}))\}}+\frac{Hb\sqrt{\ln\mathrm{Vol}(\mathcal{A})} }{T}\right).\] (22)

Finally, we choose \(\gamma\in[0,\frac{1}{144(e-2)b^{2}}]\) to minimize \(\tilde{d}_{ps}(\epsilon,T)=\max\{d_{\mathcal{F}}(\epsilon),d_{\Pi}(\epsilon,T), \frac{d_{0}(\epsilon)}{\gamma Hb^{2}},\frac{d_{0}^{\prime}(\epsilon)}{\gamma Hb^ {2}}\}\), which occurs at \(\gamma=\frac{1}{144(e-2)b^{2}}\), and thus

\[\tilde{d}_{ps}(\epsilon,T)=\max\left\{d_{\mathcal{F}}(\epsilon),d_{\Pi}( \epsilon,T),\frac{144(e-2)d_{0}(\epsilon)}{H},\frac{144(e-2)d_{0}^{\prime}( \epsilon)}{H}\right\}.\] (23)

Overall, we have that Equation (20), Equation (21), and Equation (22) can be unified into Equation (19).

### Part (ii): Specializing to the finite function classes and linear function classes

We consider two common cases.

Case 1. Finite function class.We consider the case that \(\mathcal{F}_{h}\) and \(\Pi_{h}^{soft}(T)\) have finite elements for all \(h\in[H]\). Then we have \(\tilde{d}(\epsilon)=\mathcal{O}(\max_{h\in[H]}\max\{\ln|\mathcal{F}_{h}|,\ln| \Pi_{h}^{soft}(T)|\}),\forall\epsilon\), due to that \(d_{0}^{\prime}(\epsilon)\leq d_{0}(\epsilon)\leq H\max_{h\in[H]}\ln|\mathcal{F }_{h}|\) and Equation23.

Case 2. Linear function class.We consider the case that the function class \(\mathcal{F}_{h}\) is linear in some (known) feature map \(\phi_{h}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{d}\). Concretely, the corresponding function class and the policy class defined in Section2.3 are simplified into:

\[\mathcal{F}_{h}=\{(s,a)\mapsto\langle\phi_{h}(s,a),w\rangle:\|w\|_{2}\leq b\},\]

\[\Pi_{h}^{soft}(T):=\left\{(s,a)\mapsto\frac{\exp(\langle\phi_{h}(s,a),\theta \rangle)}{\sum_{a^{\prime}\in\mathcal{A}}\exp(\langle\phi_{h}(s,a^{\prime}), \theta\rangle)}:\|\theta\|_{2}\leq\eta T\right\}.\]

We have

\[d_{\mathcal{F}}(\epsilon) \leq d\ln(1+\frac{2b}{\epsilon}),\] \[d_{\Pi}(\epsilon,T) \leq d\ln(1+\frac{16\eta T}{\epsilon}),\] \[d_{0}^{\prime}(\epsilon) \leq d_{0}(\epsilon) \leq c_{1}dH\ln(c_{2}/\epsilon),\]

where the first two inequalities use (Zanette et al., 2021, Lemma 6) and the last inequality follows the discussion in Section2.4. Note that \(d_{\Pi}(\epsilon,T)\) depends only logarithmically in \(T\).

## Appendix G Support Lemmas

In this section, for convenience, we present some simple yet useful lemmas that our proofs above often refer to.

The following lemma establishes the variance condition for the squared loss, which is typically used along with Bernstein's inequality.

**Lemma G.1**.: _Consider any real-valued function class \(\mathcal{F}\). Consider the squared loss \(L(f(x),y)=(f(x)-y)^{2}\). Assume bounded loss \(L(f(x),y)\leq M^{2}\) for any \(f\in\mathcal{F}\), for some \(M>0\). Let \(f_{*}(x)=\mathbb{E}[y|x]\) and assume that \(L(f_{*}(x),y)\leq B^{2}\) for some \(B>0\) (we do not require that \(f_{*}\in\mathcal{F}\)). Let \(z=(x,y)\) and define_

\[\mathcal{G}=\{\phi(\cdot):\phi(z)=L(f(x),y)-L(f_{*}(x),y),f\in\mathcal{F}\}.\]

_Then, for all \(\phi\in\mathcal{G}\), we have_

\[\mathbb{E}_{y}[\phi(z)^{2}]\leq 2(M^{2}+B^{2})\mathbb{E}_{y}[\phi(z)],\forall x.\]

_where \(\mathbb{E}_{y}\) is the expectation taken over \(y\) given \(x\)._

Proof of LemmaG.1.: Consider any \(\phi\in\mathcal{G}\) (with the corresponding \(f\in\mathcal{F}\)). For any \(x\), we have

\[\mathbb{E}_{y}[\phi(z)]=(f(x)-f_{*}(x))^{2}.\]

Thus, we have

\[\phi(z)^{2} =(f(x)-f_{*}(x))^{2}(f(x)+f_{*}(x)-2y)^{2}\] \[\leq(f(x)-f_{*}(x))^{2}2[(f(x)-y)^{2}+(f_{*}(x)-y)^{2}]\] \[\leq 2(M^{2}+B^{2})(f(x)-f_{*}(x))^{2}.\]

The first inequality uses Cauchy-Schwartz. The second inequality uses that \(f_{*}\in\mathcal{F}\) and \(L(f(x),y)\leq M^{2},\forall f\in\mathcal{F}\). Thus, for any \(x\), we have

\[\mathbb{E}_{y}[\phi(z)^{2}]\leq 2(M^{2}+B^{2})(f(x)-f_{*}(x))^{2}=2(M^{2}+B^{2 })\mathbb{E}_{y}[\phi(z)].\]

The equation uses that \(\mathbb{E}_{y}[\phi(z)]=(f(x)-f_{*}(x))^{2}\)The following lemma is a simple decomposition of the value gap in the initial state, typically known as the performance difference lemma in the RL literature.

**Lemma G.2** (Performance difference lemma).: _For any policy \(\pi,\widetilde{\pi}\), we have_

\[V_{1}^{\pi}(s_{1})-V_{1}^{\widetilde{\pi}}(s_{1})=\sum_{h=1}^{H}\mathbb{E}_{\pi }\left[Q_{h}^{\widetilde{\pi}}(s_{h},a_{h})-V_{h}^{\widetilde{\pi}}(s_{h}) \right],\]

_where \(\mathbb{E}_{\pi}\) denotes the expectation over the random trajectory \((s_{1},a_{1},\ldots,s_{h},a_{h})\) generated by \(\pi\) (and the underlying MDP)._

Proof of Lemma g.2.: We simply expand \(V_{1}^{\pi}(s_{1})=\mathbb{E}_{a_{1},s_{2}|s_{1},\pi}[r_{1}(s_{1},a_{1})+V_{2}^ {\pi}(s_{2})]\) and use recursion to obtain the lemma. 

The following lemma presents a simple connection from a form of a log partition function to the expectation under the infimum realization of the sampling distribution.

**Lemma G.3**.: _For any density functions \(p\) and \(p_{0}\) and any function \(f\), we have_

\[\inf_{p}\mathbb{E}_{x\sim p(x)}\left[f(x)+\ln\frac{p(x)}{p_{0}(x)}\right]\geq -\ln\mathbb{E}_{x\sim p_{0}}\exp(-f(x)).\]

Proof of Lemma g.3.: Define the density function

\[q(x)=\frac{p_{0}(x)\exp(-f(x))}{Z(f)}\text{ where }Z(f):=\mathbb{E}_{x\sim p _{0}(x)}\exp(-f(x)).\]

Then, we have

\[\mathbb{E}_{x\sim p(x)}\left[f(x)+\ln\frac{p(x)}{p_{0}(x)}\right] =\mathbb{E}_{x\sim p(x)}\ln\frac{p(x)}{q(x)}-\ln Z(f)\] \[=KL[p\|q]-\ln Z(f)\] \[\geq-\ln Z(f).\]