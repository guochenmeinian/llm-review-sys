# Is Distance Matrix Enough for Geometric Deep Learning?

Zian Li\({}^{1,2}\), Xiyuan Wang\({}^{1,2}\), Yinan Huang\({}^{1}\), Muhan Zhang\({}^{1,}\)

\({}^{1}\)Institute for Artificial Intelligence, Peking University

\({}^{2}\)School of Intelligence Science and Technology, Peking University

Corresponding author: Muhan Zhang (muhan@pku.edu.cn).

###### Abstract

Graph Neural Networks (GNNs) are often used for tasks involving the 3D geometry of a given graph, such as molecular dynamics simulation. While incorporating Euclidean distance into Message Passing Neural Networks (referred to as Vanilla DisGNN) is a straightforward way to learn the geometry, it has been demonstrated that Vanilla DisGNN is geometrically incomplete. In this work, we first construct families of novel and symmetric geometric graphs that Vanilla DisGNN cannot distinguish even when considering all-pair distances, which greatly expands the existing counterexample families. Our counterexamples show the inherent limitation of Vanilla DisGNN to capture symmetric geometric structures. We then propose \(k\)-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of \(k\)-DisGNNs from three perspectives: 1. They can learn high-order geometric information that cannot be captured by Vanilla DisGNN. 2. They can unify some existing well-designed geometric models. 3. They are universal function approximators from geometric graphs to scalars (when \(k\geq 2\)) and vectors (when \(k\geq 3\)). Most importantly, we establish a connection between geometric deep learning (GDL) and traditional graph representation learning (GRL), showing that those highly expressive GNN models originally designed for GRL can also be applied to GDL with impressive performance, and that existing complicated, equivariant models are not the only solution. Experiments verify our theory. Our \(k\)-DisGNNs achieve many new state-of-the-art results on MD17.

## 1 Introduction

Many real-world tasks are relevant to learning the geometric structure of a given graph, such as molecular dynamics simulation, physical simulation, drug designing and protein structure prediction [15, 20, 19]. Usually in these tasks, the coordinates of nodes and their individual properties, such as atomic numbers, are given. The goal is to accurately predict both invariant properties, such as the energy of the molecule, and equivariant properties, such as the force acting on each atom. This kind of graphs is also referred to as _geometric graphs_ by researchers [1, 14].

In recent years, Graph Neural Networks (GNNs) have achieved outstanding performance on such tasks, as they can learn a representation for each graph or node in an end-to-end fashion, rather than relying on handcrafted features. In such setting, a straightforward idea is to incorporate Euclidean distance, the most basic geometric feature, into Message Passing Neural Network [18] (we call these models Vanilla DisGNN) [15, 16]. While Vanilla DisGNN is simple yet powerful when considering all-pair distances (we assume all-pairdistance is used by default to research a model's maximal expressive power), it has been proved to be geometrically incomplete (Zhang et al., 2021; Garg et al., 2020; Schutt et al., 2021; Pozdnyakov and Ceriotti, 2022), i.e., there exist pairs of geometric graphs which Vanilla DisGNN cannot distinguish. It has led to the development of various GNNs which go beyond simply using _distance_ as input. Instead, these models use complex group irreducible representations, first-order equivariant representations or manually-designed complex invariant geometric features such as angles or dihedral angles to learn better representations from geometric graphs. It is generally believed that pure distance information is insufficient for complex GDL tasks (Klicpera et al., 2020; Schutt et al., 2021).

On the other hand, it is well known that the _distance matrix_, which contains the distances between all pairs of nodes in a geometric graph, holds all of the geometric structure information (Satorras et al., 2021). This suggests that it is possible to obtain all of the desired geometric structure information from _distance graphs_, i.e., complete weighted graphs with Euclidean distance as edge weight. Therefore, the complex GDL task with node coordinates as input (i.e., 3D graph) may be transformed into an equivalent graph representation learning task with distance graphs as input (i.e., 2D graph).

Thus, a desired question is: Can we design theoretically and experimentally powerful geometric models, which can learn the rich geometric patterns purely from distance matrix?

In this work, we first revisit the counterexamples in previous work used to show the incompleteness of Vanilla DisGNN. These existing counterexamples either consider only finite pairs of distance (a cutoff distance is used to remove long edges) (Schutt et al., 2021; Zhang et al., 2021; Garg et al., 2020), which can limit the representation power of Vanilla DisGNN, or lack diversity thus may not effectively reveal the inherent limitation of Vanilla DisGNN (Pozdnyakov and Ceriotti, 2022). In this regard, we further constructed plenty of novel and symmetric counterexamples, as well as a novel method to construct families of counterexamples based on several basic units. These counterexamples significantly enrich the current set of counterexample families, and can reveal the inherent limitations of MPNNs in _capturing symmetric configurations_, which can explain the reason why they perform badly in real-world tasks where lots of symmetric (sub-)structures are included.

Given the limitations of Vanilla DisGNN, we propose \(k\)-DisGNNs, models that take pair-wise distance as input and aim for learning the rich geometric information contained in geometric graphs. \(k\)-DisGNNs are mainly based on the well known \(k\)-(F)WL test (Cai et al., 1992), and include three versions: \(k\)-DisGNN, \(k\)-F-DisGNN and \(k\)-E-DisGNN. We demonstrate the superior geometric learning ability of \(k\)-DisGNNs from three perspectives. We first show that \(k\)-DisGNNs can capture arbitrary \(k\)- or \((k+1)\)-order **geometric information** (multi-node features), which cannot be achieved by Vanilla DisGNN. Then, we demonstrate the high **generality** of our framework by showing that \(k\)-DisGNNs can implement DimeNet (Klicpera et al., 2020) and GemNet (Gasteiger et al., 2021), two well-designed state-of-the-art models. A key insight is that these two models are both augmented with manually-designed high-order geometric features, including angles (three-node features) and dihedral angles (four-node features), which correspond to the \(k\)-tuples in \(k\)-DisGNNs. However, \(k\)-DisGNNs can learn _more than_ these handcrafted features in an end-to-end fashion. Finally, we demonstrate that \(k\)-DisGNNs can act as **universal function approximators** from geometric graphs to **scalars** (i.e., E(3) invariant properties) when \(k\geq 2\) and **vectors** (i.e., first-order O(3) equivariant and translation invariant properties) when \(k\geq 3\). This essentially answers the question posed in our title: _distance matrix is sufficient for GDL_. We conduct experiments on benchmark datasets where our models achieve **state-of-the-art results** on a wide range of the targets in the MD17 dataset.

Our method reveals the high potential of the most basic geometric feature, _distance_. Highly expressive GNNs originally designed for traditional GRL can naturally leverage such information as edge weight, and achieve high theoretical and experimental performance in geometric settings. This **opens up a new door for GDL research** by transferring knowledge from traditional GRL, and suggests that existing complex, equivariant models may not be the only solution.

## 2 Related Work

**Equivariant neural networks.** Symmetry is a rather important design principle for GDL (Bronstein et al., 2021). In the context of geometric graphs, it is desirable for models to be equivariant or invariant under the group (such as E(3) and SO(3)) actions of the input graphs. These models include those using group irreducible representations (Thomas et al., 2018; Batzner et al., 2022; Anderson et al., 2019; Fuchs et al., 2020), complex invariant geometric features (Schutt et al., 2018; Klicperaet al., 2020; Gasteiger et al., 2021) and first-order equivariant representations (Satorras et al., 2021; Schutt et al., 2021; Tholke and De Fabritiis, 2021). Particularly, Dym and Maron (2020) proved that both Thomas et al. (2018) and Fuchs et al. (2020) are universal for SO(3)-equivariant functions. Besides, Villar et al. (2021) showed that one could construct powerful (first-order) equivariant outputs by leveraging invariances, highlighting the potential of invariant models to learn equivariant targets.

**GNNs with distance.** In geometric settings, incorporating 3D distance between nodes as edge weight is a simple yet efficient way to improve geometric learning. Previous work (Maron et al., 2019; Morris et al., 2019; Zhang and Li, 2021; Zhao et al., 2022) mostly treat distance as an auxiliary edge feature for better experimental performance and do not explore the expressiveness or performance of using pure distance for geometric learning. Schutt et al. (2018) proposes to expand distance using a radial basis function as a continuous-filter and perform convolutions on the geometric graph, which can be essentially unified into the Vanilla DisGNN category. Zhang et al. (2021); Garg et al. (2020); Schutt et al. (2021); Pozdnyakov and Ceriotti (2022) demonstrated the limitation of Vanilla DisGNN by constructing pairs of non-congruent geometric graphs which cannot be distinguished by it, thus explaining the poor performance of such models (Schutt et al., 2018; Kearnes et al., 2016). However, none of these studies proposed a complete purely distance-based geometric model. Recent work (Hordan et al., 2023) proposed theoretically complete GNNs for distinguishing geometric graphs, but they go beyond pair-wise distance and instead utilize gram matrices or coordinate projection.

**Expressive GNNs.** In traditional GRL (no 3D information), it has been proven that the expressiveness of MPNNs is limited by the Weisfeiler-Leman test (Weisfeiler and Leman, 1968; Xu et al., 2018; Morris et al., 2019), a classical algorithm for graph isomorphism test. While MPNNs can distinguish most graphs (Babai and Kucera, 1979), they are unable to count rings, triangles, or distinguish regular graphs, which are common in real-world data such as molecules (Huang et al., 2023). To increase the expressiveness and design space of GNNs, Morris et al. (2019, 2020) proposed high-order GNNs based on \(k\)-WL. Maron et al. (2019) designed GNNs based on the folklore WL (FWL) test (Cai et al., 1992) and Azizian and Lelarge (2020) showed that these GNNs are the most powerful GNNs for a given tensor order. Beyond these, there are subgraph GNNs (Zhang and Li, 2021; Bevilacqua et al., 2021; Frasca et al., 2022; Zhao et al., 2021), substructure-based GNNs (Bouritsas et al., 2022; Horn et al., 2021; Bodnar et al., 2021) and so on, which are also strictly more expressive than MPNNs. For a comprehensive review on expressive GNNs, we refer the readers to Zhang et al. (2023).

More related work can be referred to Appendix E.

## 3 Preliminaries

In this paper, we denote multiset with \(\{\!\}\). We use \([n]\) to represent the set \(\{1,2,...,n\}\). A complete weighted graph with \(n\) nodes is denoted by \(G=(V,\bm{E})\), where \(V=[n]\) and \(\bm{E}=[e_{ij}]_{n\times n}\in\mathbb{R}^{n\times n}\). The neighborhoods of node \(i\) are denoted by \(N(i)\).

**Distance graph vs. geometric graph.** In many tasks, we need to deal with geometric graphs, where each node \(i\) is attached with its 3D coordinates \(\mathbf{x}_{i}\in\mathbb{R}^{3}\) in addition to other invariant features. Geometric graphs contain rich geometric information useful for learning chemical or physical properties. However, due to the significant variation of coordinates under E(3) transformations, they can be redundant when it comes to capturing geometric structure information.

Corresponding to geometric graphs are distance graphs, i.e., _complete_ weighted graph with Euclidean distance as edge weight. Unlike geometric graphs, distance graphs do not have explicit coordinates attached to each node, but instead, they possess distance features that are naturally _invariant_ under E(3) transformation and can be _readily utilized_ by most GNNs originally designed for traditional GRL. Distance also provides an inherent _inductive bias_ for effectively modeling the interaction/relationship between nodes. Moreover, a distance graph maintains all the essential _geometric structure information_, as stated in the following theorem:

**Theorem 3.1**.: _(_Satorras et al._,_ 2021_)_ _Two geometric graphs are congruent (i.e., they are equivalent by permutation of nodes and E(3) transformation of coordinates) \(\iff\) their corresponding distance graphs are isomorphic._

By additionally incorporating _orientation_ information, we can learn equivariant features as well (Villar et al., 2021). Hence, distance graphs provide a way to **represent geometric graphs without referring to a canonical coordinate system**. This urges us to study their full potential for GDL.

Weisfeiler-Leman Algorithms.Weisfeiler-Lehman test (also called as 1-WL) (Weisfeiler and Leman, 1968) is a well-known efficient algorithm for graph isomorphism test (traditional setting, no 3D information). It iteratively updates the labels of nodes according to nodes' own labels and their neighbors' labels, and compares the histograms of the labels to distinguish two graphs. Specifically, we use \(l_{i}^{t}\) to denote the label of node \(i\) at iteration \(t\), then 1-WL updates the node label by

\[l_{i}^{t+1}=\mathrm{HASH}(l_{i}^{t},\{\!\!\{l_{j}^{t}\mid j\in N(i)\}\!\!\}),\] (1)

where \(\mathrm{HASH}\) is an injective function that maps different inputs to different labels. However, 1-WL cannot distinguish all the graphs (Zhang and Li, 2021), and thus \(k\)-dimensional WL (\(k\)-WL) and \(k\)-dimensional Folklore WL (\(k\)-FWL), \(k\geq 2\), are proposed to boost the expressiveness of WL test.

Instead of updating the label of nodes, \(k\)-WL and \(k\)-FWL update the label of \(k\)-tuples \(\bm{v}:=(v_{1},v_{2},...,v_{k})\in V^{k}\), denoted by \(l_{\bm{v}}\). Both methods initialize \(k\)-tuples' labels according to their isomorphic types and update the labels iteratively according to their \(j\)-neighbors \(N_{j}(\bm{v})\). The difference between \(k\)-WL and \(k\)-FWL mainly lies in the definition of \(N_{j}(\bm{v})\). To be specific, tuple \(\bm{v}\)'s \(j\)-neighbors in \(k\)-WL and \(k\)-FWL are defined as follows respectively

\[N_{j}(\bm{v})=\{\!\!\{v_{1},...,v_{j-1},w,v_{j+1},...,v_{k}\}\mid w \in V\!\!\},\] (2) \[N_{j}^{F}(\bm{v})=\big{(}(j,v_{2},...,v_{k}),(v_{1},j,...,v_{k} ),...,(v_{1},v_{2},...,j)\big{)}.\] (3)

To update the label of each tuple, \(k\)-WL and \(k\)-FWL iterates as follows

\[k\text{-WL}:\;l_{\bm{v}}^{t+1}=\mathrm{HASH}\Big{(}l_{\bm{v}}^{t},\{\!\!\{l_ {\bm{w}}^{t}\mid\bm{w}\in N_{j}(\bm{v})\}\!\!\}\mid j\in[k]\big{)}\Big{)},\] (4)

\[k\text{-FWL}:\;l_{\bm{v}}^{t+1}=\mathrm{HASH}\Big{(}l_{\bm{v}}^{t},\{\!\!\{l_ {\bm{w}}^{t}\mid\bm{w}\in N_{j}^{F}(\bm{v})\}\mid j\in[|V|]\!\!\}\Big{)}.\] (5)

According to Cai et al. (1992), there always exists a pair of non-isomorphic graphs that cannot be distinguished by \(k\)-(F)WL but can be distinguished by \((k+1)\)-(F)WL. This means that \(k\)-(F)WL forms a strict hierarchy, which, however, still cannot solve the graph isomorphism problem with a finite \(k\).

## 4 Revisiting the Incompleteness of Vanilla DisGNN

As mentioned in previous sections, Vanilla DisGNN is the edge-enhanced version of MPNN, which can unify many model frameworks (Schutt et al., 2018; Kearnes et al., 2016). Its message passing formula can be generalized from its discrete version, which we call 1-WL-E (Pozdnyakov and Ceriotti, 2022):

\[l_{i}^{t+1}=\mathrm{HASH}(l_{i}^{t},\{\!\!\{l_{j}^{t},e_{ij}\}\mid j\in|V|\!\! \}).\] (6)

To provide valid counterexamples that Vanilla DisGNN cannot distinguish, Pozdnyakov and Ceriotti (2022) proposed both finite-size and periodic counterexamples and showed that they included real chemical structures. This work demonstrated for the first time the inherent limitations of Vanilla DisGNN. However, it should be noted that the essential change between these counterexamples is merely the distortion of \(C^{\pm}\) atoms, which lacks diversity despite having high manifold dimensions. In this regard, constructing new families of counterexamples cannot only enrich the diversity of existing families, but also demonstrate Vanilla DisGNN's limitations from different angles.

In this paper, we give a **simple** valid counterexample which Vanilla DisGNN cannot distinguish even with an infinite cutoff, as shown in Figure 1. In both geometric graphs, all nodes have exactly the same unordered list of distances (infinite cutoff considered), which means that Vanilla DisGNN will always label them identically. Nevertheless, the two geometric graphs are obviously non-congruent, since there are two small equilateral triangles on the right, and zero on the left. Beyond this, we construct three kinds of counterexamples in Appendix A, which can significantly enrich the counterexamples found by Pozdnyakov and Ceriotti (2022), including:

Figure 1: A pair of geometric graphs that are non-congruent but cannot be distinguished by Vanilla DisGNN. Only red nodes belong to the two geometric graphs. The grey nodes and the “edges” are for visualization purpose only. Note that the nodes of the geometric graphs are sampled from regular icosahedrons.

1. Individual counterexamples sampled from regular polyhedrons that can be directly verified.
2. Small families of counterexamples that can be transformed in one dimension.
3. Families of counterexamples constructed by _arbitrary combinations_ of basic symmetric units.

These counterexamples further demonstrate the incompleteness of Vanilla DisGNN in learning the geometry: even quite simple geometric graphs as shown in Figure 1 cannot be distinguished by it. While the counterexamples we constructed may not correspond to real molecules, such symmetric structures are commonly seen in other geometry-relevant tasks, such as physical simulations and point clouds. The inability to distinguish these symmetric structures can have a significant impact on the geometric learning performance of models, even for non-degenerated configurations, as demonstrated by Pozdnyakov and Ceriotti (2022). This perspective can provide an additional explanation for the poor performance of models such as SchNet (Schutt et al., 2018) and inspire the design of more powerful and efficient geometric models.

## 5 \(k\)-DisGNNs: \(k\)-order Distance Graph Neural Networks

In this section, we propose the framework of \(k\)-DisGNNs, _complete and universal_ geometric models learning from _pure distance features_. \(k\)-DisGNNs consist of three versions: \(k\)-DisGNN, \(k\)-F-DisGNN, and \(k\)-E-DisGNN. Detailed implementation is referred to Appendix C.

**Initialization Block.** Given a geometric \(k\)-tuple \(\bm{v}\), high-order DisGNNs initialize it with an injective function by \(h_{\bm{v}}^{0}=f_{\mathrm{init}}\Big{(}\big{(}z_{v_{i}}\mid i\in[k]\big{)}, \big{(}e_{v_{i}v_{j}}\mid i,j\in[k],i<j\big{)}\Big{)}\in\mathbb{R}^{K}\), where \(K\) is the hidden dimension. This function can injectively embed the _ordered_ distance matrix along with the \(z\) type of each node within the \(k\)-tuple, thus preserving all the geometric information within it.

**Message Passing Block.** The key difference between the three versions of high-order DisGNNs lies in their message passing blocks. The message passing blocks of \(k\)-DisGNN and \(k\)-F-DisGNN are based on the paradigms of \(k\)-WL and \(k\)-FWL, respectively. Their core message passing function \(f_{\mathrm{MP}}^{t}\) and \(f_{\mathrm{MP}}^{\mathrm{F},t}\) are formulated simply by replacing the _discrete_ tuple labels \(l_{\bm{v}}^{t}\) in \(k\)-(F)WL (Equation (4, 5)) with _continuous_ geometric tuple representation \(h_{\bm{v}}^{t}\in\mathbb{R}^{K}\), see Equation (7, 8). Tuples containing local geometric information interact with each other in message passing blocks, thus allowing models to learn considerable global geometric information (as explained in Section 6.1).

\[k-\mathrm{DisGNN} :h_{\bm{v}}^{t+1}=f_{\mathrm{MP}}^{t}\Big{(}h_{\bm{v}}^{t},\big{(} \big{\{}h_{\bm{w}}^{t}\mid\bm{w}\in N_{j}(\bm{v})\big{\}}\mid j\in|k|\big{)} \Big{)},\] (7) \[k-\mathrm{F}-\mathrm{DisGNN} :h_{\bm{v}}^{t+1}=f_{\mathrm{MP}}^{\mathrm{F},t}\Big{(}h_{\bm{v} }^{t},\big{\{}h_{\bm{w}}^{t}\mid\bm{w}\in N_{j}^{F}(\bm{v})\big{\}}\mid j\in [|V|\big{\}}\Big{)}.\] (8)

However, during message passing in \(k\)-DisGNN, the information about distance is not explicitly used but is embedded implicitly in the initialization block when each geometric tuple is embedded according to its distance matrix and node types. This means that \(k\)-DisGNN is **unable to capture the relationship** between a \(k\)-tuple \(\bm{v}\) and its neighbor \(\bm{w}\) during message passing, which could be very helpful for learning the geometric structural information of the graph. For example, in a physical system, a tuple \(\bm{w}\) far from \(\bm{v}\) should have much less influence on \(\bm{v}\) than another tuple \(\bm{w}^{\prime}\) near \(\bm{v}\).

Based on this observation, we propose \(k\)**-E-DisGNN**, which maintains a representation \(e_{ij}^{t}\) for each edge \(e_{ij}\) at every time step and explicitly incorporates it into the message passing procedure. The edge representation \(e_{ij}^{t}\) is defined as

\[e_{ij}^{t}=f_{\mathrm{e}}^{t}\Big{(}e_{ij},\big{(}\big{\{}h_{\bm{w}}^{t}\mid \bm{w}\in V^{k},\bm{w}_{u}=i,\bm{w}_{v}=j\big{\}}\big{|}\mid u,v\in[k],u<v\big{)} \Big{)}.\] (9)

Note that \(e_{ij}^{t}\) not only contains the distance between nodes \(i\) and \(j\), but also pools all the tuples related to edge \(ij\), making it informative and general. For example, in the special case \(k=2\), Equation (9) is equivalent to \(e_{ij}^{t}=f_{\mathrm{e}}^{t}(e_{ij},h_{ij}^{t})\).

We realize the message passing function of \(k\)-E-DisGNN, \(f_{\mathrm{MP}}^{\mathrm{E},t}\), by replacing the neighbor representation, \(h_{\bm{w}}^{t}\) in \(f_{\mathrm{MP}}^{t}\), with \(\big{(}h_{\bm{w}}^{t},e_{\bm{v}\setminus\bm{w},\bm{w}\setminus\bm{v}}^{t}\big{)}\) where \(\bm{v}\setminus\bm{w}\) gives the only element in \(\bm{v}\) but not in \(\bm{w}\), see Equation 10. In other words, \(e_{\bm{v}\setminus\bm{w},\bm{w}\setminus\bm{v}}^{t}\) gives the representation of the edge connecting \(\bm{v},\bm{w}\). By this means, a tuple can be aware of **how far** it is to its neighbors and by what kind of edges each neighbor is connected. This can boost the ability of geometric structure learning (as explained in Section 6.1).

\[k-\mathrm{E}-\mathrm{DisGNN}:h_{\bm{v}}^{t+1}=f_{\mathrm{MP}}^{\mathrm{E},t}\Big{(} h_{\bm{v}}^{t},\big{(}\big{\{}\!\big{\{}\!\big{\}}(h_{\bm{w}}^{t},e_{\bm{v} \setminus\bm{w},\bm{w}\setminus\bm{v}}^{t})\mid\bm{w}\in N_{j}(\bm{v})\big{\}} \big{|}\ j\in|k|\big{)}\Big{)}.\] (10)

**Output Block.** The output function \(t=f_{\mathrm{out}}\big{(}\big{\{}\!\big{\}}\big{\{}\!\big{\}}h_{\bm{v}}^{T}\mid \bm{v}\in V^{k}\big{\}}\big{)}\), where \(T\) is the final iteration, injectively pools all the tuple representations, and generates the E(3) and permutation invariant geometric target \(t\in\mathbb{R}\).

Like the conclusion about \(k\)-(F)WL for unweighted graphs, the expressiveness (in terms of approximating functions) of DisGNNs does _not decrease_ as \(k\) increases. This is simply because all \(k\)-tuples are contained within some \((k+1)\)-tuples, and by designing message passing that ignores the last index, we can implement \(k\)-DisGNNs with \((k+1)\)-DisGNNs.

## 6 Rich Geometric Information Learned by \(k\)-DisGNNs

In this section, we aim to delve deeper into the geometry learning capability of \(k\)-DisGNNs from various perspectives. In Subsection 6.1, we will examine the **high-order geometric features** that the models can extract from geometric graphs. This analysis is more intuitive and aids in comprehending the high geometric expressiveness of \(k\)-DisGNNs. In Subsection 6.2, we will show that DimeNet and GemNet, two classical and widely-used GNNs for GDL employing invariant geometric representations, are **special cases** of \(k\)-DisGNNs, highlighting the generality of \(k\)-DisGNNs. In the final subsection, we will show the **completeness** (distinguishing geometric graphs) and **universality** (approximating functions over geometric graphs) of \(k\)-DisGNNs, thus answering the question posed in the title: distance matrix is enough for GDL.

### Ability of Learning High-Order Geometry

We first give the concept of _high-order geometric information_ for better understanding.

**Definition 6.1**.: \(k\)-order geometric information is the _E(3)-invariant_ features calculable from \(k\) nodes' 3D coordinates.

For example, in a 4-tuple, one can find various high-order geometric information, including distance (2-order), angles (3-order) and dihedral angles (4-order), as shown in Figure 2(A1). Note that high-order information is not limited to the common features listed above: we show some of other possible 3-order geometric features in Figure 2(A2).

We first note that \(k\)-DisGNNs can learn and process at least \(k\)-order geometric information. Theorem 3.1 states that we can reconstruct the whole geometry of the \(k\)-tuple from the embedding of its distance matrix. In other words, we can extract all the desired \(k\)-order geometric information solely from the embedding of the \(k\)-tuple's distance matrix, which is calculated at the initialization step of \(k\)-DisGNNs, with a learnable function.

Figure 2: **A**: High-order geometric information contained in the distance matrix of \(k\)-tuples. We mark different orders with different colors, with brown, green, blue for 2-,3-,4-order respectively. (A1) High-order geometric information contained in 4-tuples, including distances, angles and dihedral angles. (A2) More 3-order geometric features, such as vertical lines, middle lines and the area of triangles. **B**: Examples explaining that neighboring 3-tuples can form a 4-tuple. Blue represents the center tuple, the other colors represent neighbor tuples, and the red node is the one node of that neighbor tuple which is not in the center tuple. (B1) Example for 3-E-DisGNN. With the green edge, two 3-tuples can form a 4-tuple. (B2) Example for 3-F-DisGNN. Four 3-tuples form a 4-tuple.

Furthermore, both \(k\)-E-DisGNN and \(k\)-F-DisGNN can actually **learn \((k+1)\)-order geometric information** in their message passing layers. For \(k\)-E-DisGNN, information about \((h_{\bm{v}},h_{\bm{w}},e_{\bm{v}\setminus\bm{w},\bm{w}\setminus\bm{v}})\), where \(\bm{w}\) is some \(j\)-neighbor of \(\bm{v}\), is included in the input of its update function. Since the distance matrices of tuples \(\bm{v}\) and \(\bm{w}\) can be reconstructed from \(h_{\bm{v}}\) and \(h_{\bm{w}}\), the all-pair distances of \((\bm{v}_{1},\bm{v}_{2},...,\bm{v}_{k},\bm{w}\setminus\bm{v})\) can be reconstructed from \((h_{\bm{v}},h_{\bm{w}},e_{\bm{v}\setminus\bm{w},\bm{w}\setminus\bm{v}})\), as shown in Figure 2(B1). Similarly, the distance matrix of \((\bm{v}_{1},\bm{v}_{2},...,\bm{v}_{k},j)\) can also be reconstructed from \(\big{(}h_{\bm{v}},(h_{\bm{w}}^{t}\mid\bm{w}\in N_{j}^{F}(\bm{v}))\big{)}\) in update function of \(k\)-F-DisGNN as shown in Figure 2(B2). These enable \(k\)-E-DisGNN and \(k\)-F-DisGNN to reconstruct all \((k+1)\)-tuples' distance matrices during message passing and thus learn all the \((k+1)\)-order geometric information contained in the graph.

With the ability to learn high-order geometric information, \(k\)-DisGNNs can learn geometric structures that cannot be captured by Vanilla DisGNNs. For example, consider the counterexample shown in Figure 1. The two geometric graphs have a different number of small equilateral triangles, which cannot be distinguished by Vanilla DisGNNs even with infinite message passing layers. However, \(3\)-DisGNN and \(2\)-E/F-DisGNN can easily distinguish the graphs by counting the number of small equilateral triangles, which is actually a kind of 3-order geometric information. This example also illustrates that high-order DisGNNs are _strictly more powerful_ than Vanilla DisGNNs.

### Unifying Existing Geometric Models with DisGNNs

There have been many attempts to improve GDL models by _manually_ designing and incorporating high-order geometric features such as angles (3-order) and dihedral angles (4-order). These features are all invariant geometric features that can be learned by some \(k\)-DisGNNs. It is therefore natural to ask whether these models can be implemented by \(k\)-DisGNNs. In this subsection, we show that two classical models, DimeNet [14] and GemNet [11], are just special cases of \(k\)-DisGNNs, thus unifying existing methods based on _hand-crafted_ features with a learning paradigm that learns _arbitrary_ high-order features from distance matrix.

DimeNet embeds atom \(i\) with a set of incoming messages \(m_{ji}\), i.e., \(h_{i}=\sum_{j\in N_{i}}m_{ji}\), and updates the message \(m_{ji}\) by

\[m_{ji}^{t+1}=f_{\mathrm{MP}}^{\mathrm{D}}\big{(}m_{ji}^{t},\sum_{k}f_{\mathrm{ int}}^{\mathrm{D}}(m_{kj}^{t},d_{ji},\phi_{kji})\big{)},\] (11)

where \(d_{ji}\) is the distance between node \(j\) and node \(i\), and \(\phi_{kji}\) is the angle \(kji\). We simplify the subscript of \(\sum\), same for that in Equation (12). The detailed ones are referred to Appendix B.2, B.1.

DimeNet is one early approach that uses geometric features to improve the geometry learning ability of GNNs, especially useful for learning the _angle-relevant_ energy or other chemical properties. Note that the angle information that DimeNet explicitly incorporates into its message passing function is actually a kind of 3-order geometric information, which can be exploited from distance matrices by our 2-E/F-DisGNN with a learning paradigm. This gives the key insight for the following proposition.

**Proposition 6.2**.: _2-E-DisGNN and 2-F-DisGNN can implement DimeNet._

GemNet is also a graph neural network designed to process graphs embedded in Euclidean space. Unlike DimeNet, GemNet incorporates both angle and _dihedral angle_ information into the message passing functions, with a 2-step message passing scheme. The core procedure is as follows:

\[m_{ca}^{t+1}=f_{\mathrm{MP}}^{\mathrm{G}}\big{(}m_{ca}^{t},\sum_{b,d}f_{ \mathrm{int}}^{\mathrm{G}}(m_{db}^{t},d_{db},\phi_{cab},\phi_{abd},\theta_{ cabd})\big{)},\] (12)

where \(\theta_{cabd}\) is the dihedral angle of planes \(cab\) and \(abd\). The use of dihedral angle information allows GemNet to learn at least 4-order geometric information, making it much more powerful than models that only consider angle information. We now prove that GemNet is just a special case of 3-E/F-DisGNN, which can also learn 4-order geometric information during its message passing stage.

**Proposition 6.3**.: _3-E-DisGNN and 3-F-DisGNN can implement GemNet._

It is worth noting that while both \(k\)-DisGNNs and existing models can learn some \(k\)-order geometric information, \(k\)-DisGNNs have the advantage of **learning arbitrary \(k\)-order geometric information**. This means that we can learn different high-order geometric features according to specific tasks in a _data-driven_ manner, including but not limited to angles and dihedral angles.

### Completeness and Universality of \(k\)-DisGNNs

We have provided an illustrative example in Section 6.1 that demonstrates the ability of \(k\)-DisGNNs to distinguish certain geometric graphs, which is not achievable by Vanilla DisGNNs. Actually, all counterexamples presented in Appendix A can be distinguished by 3-DisGNNs or 2-E/F-DisGNNs. This leads to a natural question: Can \(k\)-DisGNNs distinguish all geometric graphs with a finite and small value of \(k\)? Furthermore, can they approximate arbitrary functions over geometric graphs?

We now present our main findings that elaborate on the **completeness** and **universality** of \(k\)-DisGNNs with finite and small \(k\), which essentially highlight the high theoretical expressiveness of \(k\)-DisGNNs.

**Theorem 6.4**.: _(informal) Let \(\mathcal{M}(\theta)\in\mathrm{MinMods}=\{\)1-round 4-DisGNN, 2-round 3-E/F-DisGNN\(\}\) with parameters \(\theta\). Denote \(h_{m}=f_{\mathrm{node}}\big{(}\big{\{}\!\{h_{n}^{T}\mid\bm{v}_{0}=m\}\!\big{\}} \big{)}\in\mathbb{R}^{K^{\prime}}\) as node \(m\)'s representation produced by \(\mathcal{M}\) where \(f_{\mathrm{node}}\) is an injective multiset function, and \(\bm{x}_{m}^{c}\) as node \(m\)'s coordinates w.r.t. the center. Denote \(\mathrm{MLPs}:\mathbb{R}^{K^{\prime}}\rightarrow\mathbb{R}\) as a multi-layer perceptron. Then we have:_

1. _(Completeness) There exists_ \(\theta_{0}\) _such that_ \(\mathcal{M}(\theta_{0})\) _can distinguish all pairs of non-congruent geometric graphs._
2. _(Universality for scalars)_ \(\mathcal{M}(\theta)\) _is a universal function approximator for continuous,_ \(E(3)\) _and permutation invariant functions_ \(f:\mathbb{R}^{3\times n}\rightarrow\mathbb{R}\) _over geometric graphs._
3. _(University for vectors)_ \(f_{\mathrm{out}}^{\mathrm{equiv}}=\sum_{m=1}^{|V|}\mathrm{MLPs}\big{(}h_{m} \big{)}\bm{x}_{m}^{c}\) _is a universal function approximator for continuous,_ \(O(3)\) _equivariant and translation and permutation invariant functions_ \(f:\mathbb{R}^{3\times n}\rightarrow\mathbb{R}^{3}\) _over geometric graphs._

In fact, completeness directly implies universality for scalars [11]. Since we can actually recover the whole geometry from arbitrary node representation \(h_{m}\), universality for vectors can be proved with conclusions from Villar et al. [2021]. Also note that as order \(k\) and round number go higher, the completeness and universality still hold. Formal theorems and detailed proof are provided in Appendix B.3.

There is a concurrent work by Delle Rose et al. [2023], which also investigates the completeness of \(k\)-WL on distance graphs. In contrast to our approach of using only one tuple's final representation to reconstruct the geometry, they leverage all tuples' final representations. They prove that with the distance matrix, 3-round geometric 2-FWL and 1-round geometric 3-FWL are complete for geometry. This conclusion can also be extended to \(k\)-DisGNNs, leading to the following theorem:

**Theorem 6.5**.: _The completeness and universality for scalars stated in Theorem 6.4 hold for \(\mathcal{M}(\theta)\in\mathrm{MinMods}_{\mathrm{ext}}=\{\)3-round 2-E/F-DisGNN, 1-round 3-E/F-DisGNN\(\}\) with parameters \(\theta\)._

This essentially lowers the required order \(k\) for achieving universality on scalars to 2. However, due to the lower order and fewer rounds, the expressiveness of a single node's representation may be limited, thus the universality for vectors stated in Theorems 6.4 may not be guaranteed.

## 7 Experiments

In this section, we evaluate the experimental performance of \(k\)-DisGNNs. Our main objectives are to answer the following questions:

**Q1** Does 2/3-E/F-DisGNN outperform their counterparts in experiments (corresponding to Section 6.2)?

**Q2** As universal models for scalars (when \(k\geq 2\)), do \(k\)-DisGNNs also have good experimental performance (corresponding to Section 6.3)?

**Q3** Does incorporating well-designed edge representations in the \(k\)-E-DisGNN result in improved performance?

The best and the second best results are shown in **bold** and underline respectively in tables. Detailed experiment configuration and supplementary experiment information can be found in Appendix D. Our code is available at https://github.com/GraphPKU/DisGNN.

**MD17**.: MD17 [16] is a dataset commonly used to evaluate the performance of machine learning models in the field of molecular dynamics. It contains a collection of moleculardynamics simulations of small organic molecules such as aspirin and ethanol. Given the atomic numbers and coordinates, the task is to predict the energy of the molecule and the atomic forces. We mainly focus on the comparison between 2-F-DisGNN/3-E-DisGNN and DimeNet/GemNet. At the same time, we also compare \(k\)-DisGNNs with the state-of-the-art models: FCHL [Christensen et al., 2020], PaiNN [Schutt et al., 2021], NequIP [Batzner et al., 2022], TorchMD [Tholke and De Fabritiis, 2021], GNN-LF [Wang and Zhang, 2022]. The results are shown in Table 1. 2-F-DisGNN and 3-E-DisGNN outperform their counterparts on **16/16** and 6/8 targets, respectively, with an average improvement of 43.9% and 12.23%, suggesting that data-driven models can subsume carefully designed manual features given high expressiveness. In addition, \(k\)-DisGNNs also achieve the **best performance** on 8/16 targets, and achieve the second-best performance on **all the other targets**, outperforming the best baselines GNN-LF and TorchMD by 10.19% and 12.32%, respectively. Note that GNN-LF and TorchMD are all complex equivariant models. Beyond these, we perform experiments on **revised MD17**, which has better data quality, and \(k\)-DisGNNs outperforms the SOTA models [Batatia et al., 2022, Musaelian et al., 2023] on a wide range of targets. The results are referred to Appendix D.2. The results firmly answer **Q1** and **Q2** posed at the beginning of this section and demonstrate the potential of pure distance-based methods for graph deep learning.

**QM9.** QM9 [Ramakrishnan et al., 2014, Wu et al., 2018] consists of 134k stable small organic molecules with 19 regression targets. The task is like that in MD17, but this time we want to predict the molecule's properties such as the dipole moment. We mainly compare 2-F-DisGNN with DimeNet on this dataset and the results are shown in Table 2. 2-F-DisGNN outperforms DimeNet on **11/12** targets, by 14.27% on average, especially on targets \(\mu\) (65.0%) and \(\langle R^{2}\rangle\) (90.4%), which also answers **Q1** well. A full comparison to other state-of-the-art models is included in Appendix D.2.

**Effectiveness of edge representations.** To answer **Q3**, we split the edge representation \(e_{ij}^{t}\) (Equation (9)) into two parts, namely the pure distance (the first element) and the tuple representations (the second element), and explore whether incorporating the two elements is beneficial. We conduct experiments on MD17 with three versions of 2-DisGNNs, including 2-DisGNN (no edge representation), 2-e-DisGNN (add only edge weight) and 2-E-DisGNN (add full edge representation). The results are shown in Table 3. Both 2-e-DisGNN and 2-E-DisGNN exhibit significant performance improvements over 2-DisGNN, highlighting the significance of edge representation. Furthermore, 2-E-DisGNN outperforms 2-DisGNN and 2-e-DisGNN on 16/16 and 12/16 targets, respectively, with average improvements of 39.8% and 3.8%. This verifies our theory that capturing the _full edge representation_ connecting two tuples can boost the model's representation power.

\begin{table}
\begin{tabular}{c|c c c c c c|c c c|c} \hline Target & & FCHL & PaiNN & NequIP & TorchMD & GNN-LF & DimeNet & 2F-Dis. & GemNet & 3E-Dis. \\ \hline \multirow{3}{*}{aspirin} & E & 0.182 & 0.167 & - & **0.124** & 0.1342 & 0.204 & 0.1305 & - & 0.1466 \\  & F & 0.478 & 0.338 & 0.348 & 0.255 & 0.2018 & 0.499 & **0.1393** & 0.2168 & 0.2060 \\  & E & - & - & - & **0.056** & 0.0686 & 0.078 & 0.0683 & - & 0.0795 \\  & F & - & - & 0.187 & 0.201 & 0.1506 & 0.187 & 0.1474 & **0.1453** & 0.1471 \\ \multirow{3}{*}{ethanol} & E & 0.054 & 0.064 & - & 0.054 & 0.0520 & 0.064 & **0.0502** & - & 0.0541 \\  & F & 0.136 & 0.224 & 0.208 & 0.116 & 0.0814 & 0.230 & **0.0478** & 0.0853 & 0.0617 \\  & E & 0.081 & 0.091 & - & 0.079 & 0.0764 & 0.104 & **0.0730** & - & 0.0739 \\  & F & 0.245 & 0.319 & 0.337 & 0.176 & 0.1259 & 0.383 & **0.0786** & 0.1545 & 0.0974 \\  & E & 0.117 & 0.166 & - & **0.085** & 0.1136 & 0.122 & 0.1146 & - & 0.1135 \\  & F & 0.151 & 0.077 & 0.097 & 0.06 & 0.0550 & 0.215 & 0.0518 & 0.0553 & **0.0478** \\ \multirow{3}{*}{salicyl.} & E & 0.114 & 0.166 & - & **0.094** & 0.1081 & 0.134 & 0.1071 & - & 0.1084 \\  & F & 0.221 & 0.195 & 0.238 & 0.135 & 0.1005 & 0.374 & **0.0862** & 0.1048 & 0.1186 \\  & E & 0.098 & 0.095 & - & **0.074** & 0.0930 & 0.102 & 0.0922 & - & 0.1004 \\  & F & 0.203 & 0.094 & 0.101 & 0.066 & 0.0543 & 0.216 & **0.395** & 0.0600 & 0.0455 \\  & E & 0.104 & 0.106 & - & **0.096** & 0.1037 & 0.115 & 0.1036 & - & 0.1037 \\  & F & 0.105 & 0.139 & 0.173 & 0.094 & **0.0751** & 0.301 & 0.0876 & 0.0969 & 0.0921 \\ \hline \multirow{3}{*}{Avg improv.} & \multirow{3}{*}{E} & 11.58\% & -2.09\% & - & **26.40\%** & 17.05\% & 0.00\% & 18.18\% & - & 13.515\% \\  & & 5 & 7 & - & **1** & 3 & 6 & 2 & - & 4 \\ \hline \multirow{3}{*}{Avg improv.} & \multirow{3}{*}{F} & 31.85\% & 39.13\% & 29.86\% & 52.40\% & 63.53\% & 0.00\% & **69.68\%** & 60.96\% & 65.27\% \\  & & 7 & 6 & 8 & 5 & 3 & 9 & **1** & 4 & 2 \\ \hline \multirow{3}{*}{Rank} & \multirow{3}{*}{F} & 31.85\% & 39.13\% & 29.86\% & 52.40\% & 63.53\% & 0.00\% & **69.68\%** & 60.96\% & 65.27\% \\  & & 7 & 6 & 8 & 5 & 3 & 9 & **1** & 4 & 2 \\ \hline \end{tabular}
\end{table}
Table 1: MAE loss on MD17. Energy (E) in kcal/mol, force (F) in kcal/mol/Å. We color the cell if DisGNNs outperforms its counterpart. The improvement ratio is calculated relative to DimeNet. \(k\)-DisGNNs rank **top 2** on force prediction (which determines the accuracy of molecular dynamics [Gasteiger et al., 2021]) on average.

## 8 Conclusions and Limitations

**Conclusions.** In this work we have thoroughly studied the ability of GNNs to learn the geometry of a graph solely from its distance matrix. We expand on the families of counterexamples that Vanilla DisGNNs are unable to distinguish from their distance matrices by constructing families of symmetric and diverse geometric graphs, revealing the inherent limitation of Vanilla DisGNNs in capturing symmetric configurations. To better leverage the geometric structure information contained in distance graphs, we proposed \(k\)-DisGNNs, geometric models with high generality (ability to unify two classical and widely-used models, DimeNet and GemNet), provable completeness (ability to distinguish all pairs of non-congruent geometric graphs) and universality (ability to universally approximate scalar functions when \(k\geq 2\) and vector functions when \(k\geq 3\)). In experiments, \(k\)-DisGNNs outperformed previous state-of-the-art models on a wide range of targets of the MD17 dataset. Our work reveals the potential of using expressive GNN models, which were originally designed for traditional GRL, for the GDL tasks, and opens up new opportunities for this domain.

**Limitations.** Although DisGNNs can achieve universality (for scalars) with a small order of 2, the results are still generated under the assumption that the distance graph is complete. In practice, for large geometric graphs such as proteins, this assumption may lead to intractable computation due to the \(O(n^{3})\) time complexity and \(O(n^{2})\) space complexity. However, we can still balance theoretical universality and experimental tractability by using either an appropriate cutoff, which can furthest preserve the completeness of the distance matrix in local clusters while cutting long dependencies to improve efficiency (and generalization), or using sparse but expressive GNNs such as subgraph GNNs (Zhang and Li, 2021). We leave it for future work.

## Acknowledgement

Muhan Zhang is partially supported by the National Natural Science Foundation of China (62276003) and Alibaba Innovative Research Program.

## References

* Schmitz et al. (2019) Gunnar Schmitz, Ian Heide Godliebsen, and Ove Christiansen. Machine learning for potential energy surfaces: An extensive database and assessment of methods. _The Journal of chemical physics_, 150 (24):244113, 2019.
* Schmitz et al. (2019)

\begin{table}
\begin{tabular}{c c|c c} \hline Target & Unit & DimeNet & 2F-Dis. \\ \hline \(\mu\) & D & 0.0286 & **0.0100** \\ \(\alpha\) & \(a_{0}^{3}\) & 0.0469 & **0.0431** \\ \(\epsilon_{\text{HOMO}}\) & meV & 27.8 & **21.81** \\ \(\epsilon_{\text{LUMO}}\) & meV & 19.7 & 21.22 \\ \(\Delta\epsilon\) & meV & 34.8 & **31.3** \\ \(\langle R^{2}\rangle\) & \(a_{0}^{2}\) & 0.331 & **0.0299** \\ ZPVE & meV & 1.29 & **1.26** \\ \(U_{0}\) & meV & 8.02 & **7.33** \\ \(U\) & meV & 7.89 & **7.37** \\ \(H\) & meV & 8.11 & **7.36** \\ \(G\) & meV & 8.98 & **8.56** \\ \(c_{v}\) & \(\text{cal}/\text{mol}/\text{K}\) & 0.0249 & **0.0233** \\ \hline \end{tabular}
\end{table}
Table 2: Comparison of 2-F-DisGNN and DimeNet on QM9.

\begin{table}
\begin{tabular}{c|c|c c c} \hline Target & & 2-Dis. & 2e-Dis. & 2E-Dis. \\ \hline \multirow{3}{*}{aspirin} & E & 0.2120 & 0.1362 & **0.1280** \\  & F & 0.4483 & 0.1749 & **0.1279** \\  & E & 0.1533 & 0.0723 & **0.0700** \\  & F & 0.2049 & **0.1475** & 0.1529 \\  & E & 0.0529 & 0.0506 & **0.0502** \\  & F & 0.0850 & 0.0533 & **0.0438** \\  & E & 0.0868 & 0.0736 & **0.0732** \\  & F & 0.2124 & 0.0981 & **0.0861** \\  & E & 0.1163 & **0.1133** & 0.1149 \\  & F & 0.1318 & **0.0407** & 0.0531 \\  & E & 0.1209 & 0.1089 & **0.1074** \\  & F & 0.2723 & 0.0960 & **0.0806** \\  & E & 0.1437 & 0.0924 & **0.0920** \\  & F & 0.1229 & 0.0528 & **0.0431** \\  & E & 0.1152 & 0.1041 & **0.1037** \\  & F & 0.2631 & **0.0874** & 0.0933 \\ \hline \end{tabular}
\end{table}
Table 3: Effectiveness of edge representations on MD17. Energy (E) in kcal/mol, force (F) in kcal/mol/Å.

Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In _International Conference on Machine Learning_, pages 8459-8468. PMLR, 2020.
* Jumper et al. (2021) John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* Guo et al. (2020) Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep learning for 3d point clouds: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 43(12):4338-4364, 2020.
* Bronstein et al. (2021) Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv preprint arXiv:2104.13478_, 2021.
* Han et al. (2022) J. Han, Y. Rong, T. Xu, and W. Huang. Geometrically equivariant graph neural networks: A survey. 2022.
* Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* Schutt et al. (2018) Kristof T Schutt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Muller. Schnet-a deep learning architecture for molecules and materials. _The Journal of Chemical Physics_, 148(24):241722, 2018.
* Kearnes et al. (2016) Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. _Journal of computer-aided molecular design_, 30:595-608, 2016.
* Zhang et al. (2021) Yaolong Zhang, Junfan Xia, and Bin Jiang. Physically motivated recursively embedded atom neural networks: incorporating local completeness and nonlocality. _Physical Review Letters_, 127(15):156002, 2021.
* Garg et al. (2020) Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of graph neural networks. In _International Conference on Machine Learning_, pages 3419-3430. PMLR, 2020.
* Schutt et al. (2021) Kristof Schutt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In _International Conference on Machine Learning_, pages 9377-9388. PMLR, 2021.
* Pozdnyakov and Ceriotti (2022) Sergey N Pozdnyakov and Michele Ceriotti. Incompleteness of graph convolutional neural networks for points clouds in three dimensions. _arXiv preprint arXiv:2201.07136_, 2022.
* Klicpera et al. (2020) Johannes Klicpera, Janek Gross, and Stephan Gunnemann. Directional message passing for molecular graphs. _arXiv preprint arXiv:2003.03123_, 2020a.
* Satorras et al. (2021) Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In _International conference on machine learning_, pages 9323-9332. PMLR, 2021.
* Cai et al. (1992) Jin-Yi Cai, Martin Furer, and Neil Immerman. An optimal lower bound on the number of variables for graph identification. _Combinatorica_, 12(4):389-410, 1992.
* Gasteiger et al. (2021) Johannes Gasteiger, Florian Becker, and Stephan Gunnemann. Gemmet: Universal directional graph neural networks for molecules. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 6790-6802. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/35cf8659cfcb13224cb47863a34fc58-Paper.pdf.
* Thomas et al. (2018) Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. _arXiv preprint arXiv:1802.08219_, 2018.
* Toshima et al. (2018)Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. _Nature communications_, 13(1):1-11, 2022.
* Anderson et al. (2019) Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural networks. _Advances in neural information processing systems_, 32, 2019.
* Fuchs et al. (2020) Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d rotation-translation equivariant attention networks. _Advances in Neural Information Processing Systems_, 33:1970-1981, 2020.
* Tholke and Fabritiis (2021) Philipp Tholke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular potentials. In _International Conference on Learning Representations_, 2021.
* Dym and Maron (2020) Nadav Dym and Haggai Maron. On the universality of rotation equivariant point cloud networks. _arXiv preprint arXiv:2010.02449_, 2020.
* Villar et al. (2021) Soledad Villar, David W Hogg, Kate Storey-Fisher, Weichi Yao, and Ben Blum-Smith. Scalars are universal: Equivariant machine learning, structured like classical physics. _Advances in Neural Information Processing Systems_, 34:28848-28863, 2021.
* Maron et al. (2019) Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. _Advances in neural information processing systems_, 32, 2019.
* Morris et al. (2019) Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 4602-4609, 2019.
* Zhang and Li (2021) Muhan Zhang and Pan Li. Nested graph neural networks. _Advances in Neural Information Processing Systems_, 34:15734-15747, 2021.
* Zhao et al. (2022) Lingxiao Zhao, Louis Hartel, Neil Shah, and Leman Akoglu. A practical, progressively-expressive gnn. _arXiv preprint arXiv:2210.09521_, 2022.
* Hordan et al. (2023) Snir Hordan, Tal Amir, Steven J Gortler, and Nadav Dym. Complete neural networks for euclidean graphs. _arXiv preprint arXiv:2301.13821_, 2023.
* Weisfeiler and Leman (1968) Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which appears therein. _NTI, Series_, 2(9):12-16, 1968.
* Xu et al. (2018) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* Babai and Kucera (1979) Laszlo Babai and Ludik Kucera. Canonical labelling of graphs in linear average time. In _20th Annual Symposium on Foundations of Computer Science (sfcs 1979)_, pages 39-46. IEEE, 1979.
* Huang et al. (2023) Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. Boosting the cycle counting power of graph neural networks with i\({}^{2}\)-gnns. 2023.
* Morris et al. (2020) Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings. _Advances in Neural Information Processing Systems_, 33:21824-21840, 2020.
* Azizian and Lelarge (2020) Waiss Azizian and Marc Lelarge. Expressive power of invariant and equivariant graph neural networks. _arXiv preprint arXiv:2006.15646_, 2020.
* Bevilacqua et al. (2021) Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. _arXiv preprint arXiv:2110.02910_, 2021.
* Frasca et al. (2022) Fabrizio Frasca, Beatrice Bevilacqua, Michael Bronstein, and Haggai Maron. Understanding and extending subgraph gnns by rethinking their symmetries. _Advances in Neural Information Processing Systems_, 35:31376-31390, 2022.
* Furner et al. (2019)* Zhao et al. (2021) Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any gnn with local structure awareness. _arXiv preprint arXiv:2110.03753_, 2021.
* Bouritsas et al. (2022) Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(1):657-668, 2022.
* Horn et al. (2021) Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, and Karsten Borgwardt. Topological graph neural networks. _arXiv preprint arXiv:2102.07835_, 2021.
* Bodnar et al. (2021) Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F Montufar, Pietro Lio, and Michael Bronstein. Weisfeiler and lehman go topological: Message passing simplicial networks. In _International Conference on Machine Learning_, pages 1026-1037. PMLR, 2021.
* Zhang et al. (2023) Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. _arXiv preprint arXiv:2301.09505_, 2023.
* Rose et al. (2023) Valentino Delle Rose, Alexander Kozachinskiy, Cristobal Rojas, Mircea Petrache, and Pablo Barcelo. Three iterations of \((d-1)\)-wl test distinguish non isometric clouds of \(d\)-dimensional points. _arXiv e-prints_, pages arXiv-2303, 2023.
* Chmiela et al. (2017) Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Schutt, and Klaus-Robert Muller. Machine learning of accurate energy-conserving molecular force fields. _Science advances_, 3(5):e1603015, 2017.
* Christensen et al. (2020) Anders S Christensen, Lars A Bratholm, Felix A Faber, and O Anatole von Lilienfeld. Fchl revisited: Faster and more accurate quantum machine learning. _The Journal of chemical physics_, 152(4):044107, 2020.
* Wang and Zhang (2022) Xiyuan Wang and Muhan Zhang. Graph neural network with local frame for molecular potential energy surface. _arXiv preprint arXiv:2208.00716_, 2022.
* Batatia et al. (2022) Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and Gabor Csanyi. Mace: Higher order equivariant message passing neural networks for fast and accurate force fields. _Advances in Neural Information Processing Systems_, 35:11423-11436, 2022.
* Musaelian et al. (2023) Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. _Nature Communications_, 14(1):579, 2023.
* Ramakrishnan et al. (2014) Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. _Scientific data_, 1(1):1-7, 2014.
* Wu et al. (2018) Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* Maron et al. (2018) Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. _arXiv preprint arXiv:1812.09902_, 2018.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Christensen and Lilienfeld (2020) Anders S Christensen and O Anatole Von Lilienfeld. On the role of gradients for machine learning of molecular energies and forces. _Machine Learning: Science and Technology_, 1(4):045018, 2020.
* Klicpera et al. (2020) Johannes Klicpera, Shankar Giri, Johannes T Margraf, and Stephan Gunnemann. Fast and uncertainty-aware directional message passing for non-equilibrium molecules. _arXiv preprint arXiv:2011.14115_, 2020b.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Klicpera et al. (2020)Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Tolstikhin et al. [2021] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. _Advances in neural information processing systems_, 34:24261-24272, 2021.
* Liu et al. [2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* Xia et al. [2023] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z Li. Mole-bert: Rethinking pre-training graph neural networks for molecules. 2023.
* Lu et al. [2023] Shuqi Lu, Zhifeng Gao, Di He, Linfeng Zhang, and Guolin Ke. Highly accurate quantum chemical property prediction with uni-mol+. _arXiv preprint arXiv:2303.16982_, 2023.
* Morris et al. [2022] Christopher Morris, Gaurav Rattan, Sandra Kiefer, and Siamak Ravanbakhsh. Speqnets: Sparsity-aware permutation-equivariant graph networks. _arXiv preprint arXiv:2203.13913_, 2022.
* Bartok et al. [2010] Albert P Bartok, Mike C Payne, Risi Kondor, and Gabor Csanyi. Gaussian approximation potentials: The accuracy of quantum mechanics, without the electrons. _Physical review letters_, 104(13):136403, 2010.
* Bartok et al. [2013] Albert P Bartok, Risi Kondor, and Gabor Csanyi. On representing chemical environments. _Physical Review B_, 87(18):184115, 2013.
* Shapeev [2016] Alexander V Shapeev. Moment tensor potentials: A class of systematically improvable interatomic potentials. _Multiscale Modeling & Simulation_, 14(3):1153-1173, 2016.
* Behler and Parrinello [2007] Jorg Behler and Michele Parrinello. Generalized neural-network representation of high-dimensional potential-energy surfaces. _Physical review letters_, 98(14):146401, 2007.
* Drautz [2019] Ralf Drautz. Atomic cluster expansion for accurate and transferable interatomic potentials. _Physical Review B_, 99(1):014104, 2019.
* Joshi et al. [2023] Chaitanya K Joshi, Cristian Bodnar, Simon V Mathis, Taco Cohen, and Pietro Lio. On the expressive power of geometric graph neural networks. _arXiv preprint arXiv:2301.09308_, 2023.

Counterexamples for Vanilla DisGNNs

In this section, we will give some counterexamples, i.e., pairs of geometric graphs that are not congruent but cannot be distinguished by Vanilla DisGNNs. We will organize the section as follows: First, we will give some isolated counterexamples that can be directly verified; Then, we will give some counterexamples that can hold the property (i.e., to be counterexamples) after some simple continuous transformation; Finally, we will give a family of counterexamples that can be obtained by the combination of some basic units. Since the latter two cases cannot be directly verified, we will also give the detailed proof.

We say there are \(M\)_kinds_ of nodes if there are \(M\) different labels after infinite iterations of Vanilla DisGNNs. Note that in this section, all the grey nodes and the "edges" are just for **visualization purpose**. Only colored nodes belong to the geometric graph.

Note that we provide **verification programs** in our code for all the counterexamples. The first type of counterexamples (Appendix A.1) is limited in quantity and can be directly verified. The remaining two types (Appendix A.2 and Appendix A.3) are families of counterexamples and have an infinite number of cases. Our code can verify them to some extent by randomly selecting some parameters. Theoretical proofs for these two types are also provided in Appendix A.2 and Appendix A.3, respectively.

### Counterexamples That Can Be Directly Verified

Since the distance graph, which DisGNNs take as input, is a kind of complete graph and contains lots of geometric constraints, if we want to get a pair of non-congruent geometric graphs that cannot be distinguished by DisGNNs, they must exhibit great _symmetry_ and have just _minor difference_. Inspired by this, we can try to sample nodes from regular polyhedrons, which themselves exhibit high symmetry. The first pair of geometric graphs is sampled from regular icosahedrons, which is referred to the main body of our paper, see Figure 1. Since all the nodes from both graphs have the same unordered distance list, there is only **1** kind of nodes for both graphs. We note that it is the only counterexample we can sample from just regular icosahedron.

The following pairs of geometric graphs are all sampled from regular dodecahedrons. We will distinguish them by the graph size.

**6-size and 14-size counterexamples.**

A pair of 6-size geometric graphs (red nodes) and a pair of 14-size geometric graphs (blue nodes) that are counterexamples are shown in the figure above. They are actually complementary in terms of forming all vertices of a regular dodecahedron.

For the 6-size counterexample, the label distribution after infinite rounds of Vanilla DisGNN is shown in the picture above, and the nodes are colored differently to represent different labels. It can be directly verified that there are **2** kinds of nodes. Note that the two geometric graphs are non-congruent: The isosceles triangle of the two geometric graphs formed by one green point and two pink nodes have the same leg length but different base lengths.

For the 14-size counterexample, we give the conclusion that there are **4** kinds of nodes and the number of nodes of each kind is 2, 4, 4, 4 respectively. Since the counterexamples here and in the following are complex and not so intuitionistic, we recommend readers to directly verify them with our programs.

**8-size and 12-size counterexamples.**

A pair of 8-size geometric graphs (red nodes) and a pair of 12-size geometric graphs (blue nodes) that are counterexamples are shown in the figure above.

For the 8-size counterexample, note that it is actually obtained by carefully inserting two nodes into the 6-size counterexample mentioned just before. We note that there are **2** kinds of nodes in this counterexample, and the numbers of nodes of each kind are 4, 4 respectively.

For the 12-size counterexample, it is actually obtained by carefully deleting two nodes from the 14-size counterexample (corresponds to the way to obtain the 8-size counterexample) mentioned just before. There are **4** kinds of nodes in this counterexample, and the numbers of nodes of each kind are 4, 4, 2, 2 respectively.

**Two pairs of 10-size counterexamples.**

The above figure shows the first pair of 10-size counterexamples. There are **3** kinds of nodes and the numbers of nodes of each kind are 4, 4, 2 respectively.

The above figure shows the second pair of 10-size counterexamples. There is actually only **1** kind of nodes, all the nodes of both graphs have the same unordered distance list. It is interesting because there are two regular pentagons in the left graph and a ring in the right graph, which means that the two graphs are non-congruent, and it is quite similar to the case shown in Figure 1, where there are two equilateral triangles in the left graph and also a ring in the right graph.

In the end, we note that the above counterexamples are probably all the counterexamples one can sample from a single regular polyhedron.

### Counterexamples That Hold After Some Continuous Transformation

Inspired by the intuition mentioned in Appendix A.1, we can also sample nodes from the combination of regular polyhedrons, which also have great symmetry but with more nodes.

**Cube + regular octahedron.**

The first two counterexamples are sampled from the combination of a cube and a regular octahedron. We combine them in the following way: Bring the center of both polyhedrons together, and align the vertex of the regular octahedron, the center of the cube's face, and the center of the polyhedrons in a straight line. We do not limit the **relative size** of the two polyhedrons: one can scale the cube to any size as long as the constraints are not broken. In this way, a _small_ family of counterexamples is given (we say small here because the dimension of the transformation space is small).

_Proof of the red case._ Let the side length of the cube be \(2a\), the diagonal of the regular octahedron be \(2b\). Initially, all nodes are labeled \(l_{0}\). We now simulate Vanilla DisGNNs on both graphs.

At the first iteration, there are actually two kinds of unordered distance lists for both of the graphs: For the vertexes on the regular octahedron, the list is \(\Big{\{}\big{(}(2b,l_{0}),1\big{)},\big{(}(\sqrt{3a^{2}+b^{2}+2ab},l_{0}),2 \big{)},\)\(\big{(}(\sqrt{3a^{2}+b^{2}-2ab},l_{0}),2\big{)}\Big{\}}\). For the vertexes on the cube, the list is \(\Big{\{}\big{(}(2a,l_{0}),1\big{)},\big{(}(2\sqrt{2}a,l_{0}),1\big{)},\big{(} (2\sqrt{3}a,l_{0}),1\big{)},\)\(\big{(}(\sqrt{3a^{2}+b^{2}+2ab},l_{0}),1\big{)},\)\(\big{(}(\sqrt{3a^{2}+b^{2}-2ab},l_{0}),1\big{)}\Big{\}}\). Thus they will be labeled differently at this step, let the labels be \(l_{1}\) and \(l_{2}\) respectively.

At the second iteration, the lists of all the vertexes on the regular octahedron from both graphs are still the same, i.e. \(\Big{\{}\big{(}(2b,l_{1}),1\big{)},\big{(}(\sqrt{3a^{2}+b^{2}+2ab},l_{2}),2 \big{)},\big{(}(\sqrt{3a^{2}+b^{2}-2ab},l_{2}),2\big{)}\Big{\}}\), as well as the lists of all the vertexes on the cube from both graphs, i.e. \(\Big{\{}\big{(}(2a,l_{2}),1\big{)},\big{(}(2\sqrt{2}a,l_{2}),1\big{)},\)\(\big{(}(2\sqrt{3}a,l_{2}),1\big{)},\)\(\big{(}(\sqrt{3a^{2}+b^{2}+2ab},l_{1}),1\big{)},\big{(}(\sqrt{3a^{2}+b^{2}-2ab},l_{1}),1 \big{)}\Big{\}}\).

Since Vanilla DisGNNs cannot subdivide the vertexes at the second step, they can still not at the following steps. So it cannot distinguish the two geometric graphs. But they are non-congruent: on the left graph, the nodes on the regular octahedron can only form isosceles triangles with the nodes on the face diagonal of the cube. On the right graph, they can only form isosceles triangles with nodes on the cubes that are on the same side. We note that the counterexample in Figure 1 is not a special case of this, because on the left graph of this case all the nodes are on the same surface, but none of graphs in Figure 1 have all nodes on the same surface.

_Proof of the blue case._ The proof of the blue case is quite similar to that of the red case; thus, we omit the proof here. Note that the labels will stabilize after the first iteration. At the end of Vanilla DisGNN, there are **2** kinds of nodes, each kind has 4 nodes. And the two geometric graphs are non-congruent because the plane formed by four nodes on the regular octahedron is perpendicular to the plane formed by four nodes on the cube in the left graph, and is not in the right graph.

**2 cubes.**

The second case is the combination of 2 cubes. We combine them in the following way: Let the center of the two cubes coincide as well as the axis formed by the centers of two opposite faces. Also, we do not limit the **relative size** of the two cubes: one can scale the cube to any size as long as the constraints are not broken. The proof of this case is quite similar to that in _Cube + regular octahedron_; thus, we omit the proof here. Note that the labels will stabilize after the first iteration. If the sizes of the two cubes are the same, then at the end of Vanilla DisGNN, there is only **1** kind of node. If not, there are **2** kinds of nodes, each kind has 4 nodes.

### Families of Counterexamples

In this section, we will prove that any one of the counterexamples given in Appendix A.1 can be **augmented** to a family. To facilitate the discussion, we will introduce some notations.

**Notation.** Consider an arbitrary counterexample \((G_{ori}^{L},G_{ori}^{R})\) given in Appendix A.1. It is worth noting that for each graph, the nodes are sampled from a regular polyhedron and are located on the same spherical surface. We can denote the geometric graph on this sphere as \(G_{ori,r}\), where \(r\) represents the radius of the sphere. Since the nodes are sampled from some regular polyhedron (denoted by \(G_{all,r}\)), there must be nodes that aren't sampled, which we call as complementary nodes. We denote the corresponding geometric graph as \(G_{com,r}\). Given two graphs \(G_{Q_{1},r_{1}}\), \(G_{Q_{2},r_{2}}\) where \(Q_{i}\in\{ori,com,all\}\) for \(i\in[2]\), we say that we **align** them if we make sure that the spherical centers of the two geometric graphs coincide and one can get \(G_{Q_{2},r_{2}}\) by just scaling all the nodes of \(G_{Q_{2},r_{1}}\) along the spherical center. We use \(\mathcal{L}=\left\{(v,l_{v})\mid v\in V\right\}\) to represent a **label state** for some graph \(G\), and call it a **stable** state if Vanilla DisGNN cannot further refine the labels of the graph \(G\) with initial label \(\mathcal{L}\). We say that \(\mathcal{L}\) is the **final** state of \(G\) if it is the label state obtained by performing Vanilla DisGNN on \(G\) with no initial labels for infinite rounds. Note that final state is a special stable state. We denote the final state of \(G_{ori}\) and \(G_{com}\) as \(\mathcal{L}_{ori}\) and \(\mathcal{L}_{com}\) respectively.

Now, let us introduce an augmentation method for arbitrary counterexample \((G_{ori}^{L},G_{ori}^{R})\) shown in Appendix A.1. For a given set \(\mathcal{QR}_{k}=\left\{(Q_{1},r_{1}),(Q_{2},r_{2}),...,(Q_{k},r_{k})\right\}\) where \(r_{i}>0,Q_{i}\in\{ori,com,all\}\) and \(r_{i}\neq r_{j}\) for \(i\neq j\), we get the pair of augmented geometric graphs \((G^{L},G^{R})=\operatorname{AU}\nolimits_{\mathcal{QR}_{k}}(G_{ori}^{L},G_{ori }^{R})\) by the following steps (take \(G^{L}\) for example):

1. Generate \(k\) geometric graphs \(G_{Q_{1},r_{1}}^{L},i\in[k]\) according to \(\mathcal{QR}_{k}\).
2. Align the \(k\) geometric graphs.
3. Combine the \(k\) geometric graphs \(G_{Q_{i},r_{i}}^{L},i\in[k]\) to obtain \(G^{L}\).

We give an example of the augmentation in Figure 3. Now we give our theorem.

**Theorem A.1**.: _Consider an arbitrary counterexample \((G_{ori}^{L},G_{ori}^{R})\) given in Appendix A.1. Let \(\mathcal{QR}_{k}=\left\{(Q_{1},r_{1}),(Q_{2},r_{2}),\ldots,(Q_{k},r_{k})\right\}\) be an arbitrary set, where \(Q_{i}\in\{ori,com,all\}\) and \(r_{i}\neq r_{j}\) for \(i\neq j\). If \(\exists i\in[k]\) such that \(Q_{i}\neq all\), then the augmented geometric graphs \((G^{L},G^{R})=\operatorname{AUG}_{\mathcal{QR}_{k}}(G^{L}_{ori},G^{R}_{ori})\) form a counterexample._

Before presenting the proof of the theorem, we will introduce several lemmas that will be used in the proof.

**Lemma A.2**.: _If Vanilla DisGNN cannot distinguish \((G^{L},G^{R})\) with some initial label \((\mathcal{L}^{L},\mathcal{L}^{R})\), then Vanilla DisGNN cannot distinguish \((G^{L},G^{R})\) without initial label (with identical labels)._

Proof of Lemma a.2.: We use \(l^{t}_{u}\) to represent the label of \(u\) after \(t\) iterations of Vanilla DisGNN without initial labels, and use \(l^{\mathcal{L},t}_{u}\) to represent the label of \(u\) after \(t\) iterations of Vanilla DisGNN with initial label \(\mathcal{L}\). We use \(l^{\infty}_{u}\) and \(l^{\mathcal{L},\infty}_{u}\) to represent the corresponding final label. In order to prove this lemma, we first prove a conclusion: **for arbitrary graphs \(G=(V,E)\) and arbitrary initial label \(\mathcal{L}\), let \(u,v\) be two arbitrary nodes in \(V\), then we have \(\forall t\in\mathbb{Z}^{+}\), \(l^{t}_{u}\neq l^{t}_{v}\to l^{\mathcal{L},t}_{u}\neq l^{\mathcal{L},t}_{v}\).** We'll prove the conclusion by induction.

* \(t=1\) holds. If \(\big{(}l^{0}_{u},\{\!\!\{(d_{us},l^{0}_{s})\mid s\in N(u)\}\!\!\}\big{)}\neq \big{(}l^{0}_{v},\{\!\!\{(d_{us},l^{0}_{s})\mid s\in N(v)\}\!\!\}\big{)}\), then we have \(\{\!\!\{d_{us}\mid s\in N(u)\}\!\!\}\neq\{\!\!\{d_{vs}\mid s\in N(v)\}\!\!\}\), since \(l^{0}\) are all identical for \(s\in V\). Then it is obvious that \(\big{(}l^{\mathcal{L},0}_{u},\{\!\!\{(d_{us},l^{\mathcal{L},0}_{s})\mid s\in N (u)\}\!\!\}\neq\big{(}l^{\mathcal{L},0}_{v},\{\!\!\{d_{vs},l^{\mathcal{L},0}_{ s}\}\mid s\in N(v)\}\!\!\}\big{)}\).
* For \(\forall k\in\mathbb{Z}^{+}\), \(t=k\) holds \(\Rightarrow t=k+1\) holds. Assume \(t=k+1\) does not hold, i.e., \(\exists u,v\in V\), \(l^{(k+1)}_{u}\neq l^{(k+1)}_{v}\) but \(l^{\mathcal{L},(k+1)}_{u}=l^{\mathcal{L},(k+1)}_{v}\). We can derive from \(l^{\mathcal{L},(k+1)}_{u}=l^{\mathcal{L},(k+1)}_{v}\) that \[l^{\mathcal{L},(k+1)}_{u}=l^{\mathcal{L},(k+1)}_{v}\] \[\Rightarrow\] (13) Since \(t=k\) holds, for \(\forall s_{1},s_{2}\), we have \[l^{\mathcal{L},k}_{s_{1}}=l^{\mathcal{L},k}_{s_{2}}\] \[\Rightarrow \ l^{k}_{s_{1}}=l^{k}_{s_{2}}.\] Together with Equation (13), we have \(l^{k}_{u}=l^{k}_{v}\) and \(\{\!\!\{(d_{us},l^{k}_{s})\mid s\in N(u)\}\!\!\}=\{\!\!\{(d_{vs},l^{k}_{s}) \mid s\in N(v)\}\!\!\}\). This means \(l^{(k+1)}_{u}=l^{(k+1)}_{v}\), which is contradictory to assumptions.

Figure 3: An example of the augmentation. In this example, \((G^{L}_{ori},G^{R}_{ori})\) are from Figure 1, and \(\mathcal{QR}_{3}=\big{\{}(ori,r_{r}),(all,r_{p}),(com,r_{g})\big{\}}\) (with \(r,p,g\) representing red, purple and green respectively). Nodes with different colors indicate that they are derived from different \(G_{Q,r}\) generations. The augmented graphs \((G^{L},G^{R})=\operatorname{AUG}_{\mathcal{QR}3}(G^{L}_{ori},G^{R}_{ori})\) consist of all the colored nodes.

This means that the conclusion holds for \(\forall t\in\mathbb{Z}^{+}\), as well as \(t=\infty\). In other words, specifying an initial label \(\mathcal{L}\) will only result in the subdivision of the final labels.

Back to Lemma A.2, if Vanilla DisGNN cannot distinguish \((G^{L},G^{R})\) even when the final labels are furthermore subdivided, it can still not when the final labels are not subdivided, i.e., cannot distinguish \((G^{L},G^{R})\) without initial labels. 

**Lemma A.3**.: _Consider an arbitrary counterexample \((G^{L}_{ori},G^{R}_{ori})\) given in Appendix A.1. We have that \((\mathcal{L}^{L}_{ori}\cup\mathcal{L}^{L}_{com},\mathcal{L}^{R}_{ori}\cup \mathcal{L}^{R}_{com})\) is a pair of **stable states** of \((G^{L}_{all},G^{R}_{all})\). Denote the labels as \(\mathcal{L}^{L}_{all}\) and \(\mathcal{L}^{R}_{all}\) respectively. Moreover, we have that Vanilla DisGNN cannot distinguish \((G^{L}_{all},G^{R}_{all})\) with initial labels \(\mathcal{L}^{L}_{all}\) and \(\mathcal{L}^{R}_{all}\)._

Proof of Lemma a.3.: Since the number of situations is finite, this lemma can be directly verified using a computer program. It is worth noting that we have also included the **verification program** in our code. 

This lemma allows us to draw several useful prior conclusions. In the graph \(G_{all}\), the node set \(V_{all}\) can be split into two subsets by definition, namely \(V_{ori}\) and \(V_{com}\). Now for arbitrary node \(u\) from \(G^{L}_{all}\) with initial label \(\mathcal{L}^{L}_{all}\) and \(v\) from \(G^{R}_{all}\) with initial label \(\mathcal{L}^{R}_{all}\), if the initial labels of node \(u\) and node \(v\) are the same, they must be in the same subset, i.e., respectively in \(V^{L}_{ori}\) and \(V^{R}_{ori}\) or \(V^{L}_{com}\) and \(V^{R}_{com}\). Without loss of generality, let us assume that both \(u\) and \(v\) are in \(V_{ori}\). Since \((\mathcal{L}^{L}_{all},\mathcal{L}^{R}_{all})\) is a stable state of \((G^{L}_{all},G^{R}_{all})\), we can obtain the following equation:

\[\big{(}l_{u},\{\!\!\{(d_{us},l_{s})\mid s\in V^{L}_{all}\}\!\!\!\}\big{)}=\big{(} l_{v},\{\!\!\{(d_{vs},l_{s})\mid s\in V^{R}_{all}\}\!\!\!\}\big{)}.\] (14)

Since \((\mathcal{L}^{L}_{ori},\mathcal{L}^{R}_{ori})\) is a stable state of the induced subgraphs \((G^{L}_{ori},G^{R}_{ori})\) where the node sets are \((V^{L}_{ori},V^{R}_{ori})\), we can obtain the following equation:

\[\big{(}l_{u},\{\!\!\{(d_{us},l_{s})\mid s\in V^{L}_{ori}\}\!\!\!\}\big{)}= \big{(}l_{v},\{\!\!\{(d_{vs},l_{s})\mid s\in V^{R}_{ori}\}\!\!\!\}\big{)}.\] (15)

Notice that in Equation (14, 15), we can obtain a new equation by replacing \(V^{L}_{ori}\) or \(V^{L}_{all}\) with \((V^{L}_{ori}-u)\) or \((V^{L}_{all}-u)\) and doing the same for the \(v\) side, while keeping the equation valid, since \(l_{u}=l_{v}\). By subtracting Equation (15) from Equation (14), we obtain the following equation:

\[\big{(}l_{u},\{\!\!\{(d_{us},l_{s})\mid s\in V^{L}_{com}\}\!\!\!\}\big{)}= \big{(}l_{v},\{\!\!\{(d_{vs},l_{s})\mid s\in V^{R}_{com}\}\!\!\!\}\big{)}.\] (16)

It is important to note that the same conclusion holds if both \(u\) and \(v\) are from type _com_. These equations provide valuable **prior knowledge** that we can use to prove Theorem A.1.

Now, let us prove Theorem A.1. Our proof is divided into four steps:

1. Construct an initial state \((\mathcal{L}^{L},\mathcal{L}^{R})\) for the augmented geometric graphs \((G^{L},G^{R})\).
2. Prove that \((\mathcal{L}^{L},\mathcal{L}^{R})\) is a pair of stable states for \((G^{L},G^{R})\).
3. Explain that Vanilla DisGNN cannot distinguish \((G^{L},G^{R})\) with initial labels \((\mathcal{L}^{L},\mathcal{L}^{R})\).
4. Explain that \((G^{L},G^{R})\) are non-congruent.

By applying Lemma A.2, we can get the conclusion from step 2 and 3 that Vanilla DisGNN cannot distinguish \((G^{L},G^{R})\) without initial labels. Since \((G^{L},G^{R})\) are non-congruent (as established in step 4), it follows that \((G^{L},G^{R})\) is a counterexample.

Proof of Theorem a.1.:

**Step 1.** We first construct an initial state for the augmented graphs \((G^{L},G^{R})\).

For simplicity, we omit the superscripts, as the rules are the same for both graphs. For each layer (i.e., the sphere of some radius) of the graph \(G\), we label the nodes with \(\mathcal{L}_{tp}\) if the layer is of type \(tp\), for \(tp\in\{ori,com,all\}\). Note that we also ensure that the labels in different layers are distinct.

**Step 2.** Now we prove that \((\mathcal{L}^{L},\mathcal{L}^{R})\) is a stable state for \((G^{L},G^{R})\). This is equivalent to proving that for arbitrary node \(u\) in \((G^{L},\mathcal{L}^{L})\) and node \(v\) in \((G^{R},\mathcal{L}^{R})\), if \(l^{0}_{u}=l^{0}_{v}\), then \(l^{1}_{u}=l^{1}_{v}\).

Since \(G^{L}\) is considered as a complete distance graph by DisGNNs, the neighbors of node \(u\) are all the nodes in \(G^{L}\) except itself. We denote the neighbor nodes from the layer with radius \(r_{i}\) by \(N_{r_{i}}(u)\)This is similar for node \(v\). Now we need to prove that \(\big{(}l_{v}^{0},\{\!\{(d_{us},l_{s}^{0})\mid s\in\bigcup_{i\in[k]}N_{r_{i}}(u)\} \!\}\big{)}=\big{(}l_{v}^{0},\{\!\{(d_{us},l_{s}^{0})\mid s\in\bigcup_{i\in[k]}N _{r_{i}}(v)\}\!\}\big{)}\). Since \(l_{u}^{0}=l_{v}^{0}\), we only need to prove that \(\{\!\{(d_{us},l_{s}^{0})\mid s\in\bigcup_{i\in[k]}N_{r_{i}}(u)\}\!\}=\{\!\{(d_{ vs},l_{s}^{0})\mid s\in\bigcup_{i\in[k]}N_{r_{i}}(v)\}\!\}\).

We split the multiset \(\{\!\{(d_{us},l_{s}^{0})\mid s\in\bigcup_{i\in[k]}N_{r_{i}}(u)\}\!\}\) into \(k\) multisets, namely \(\{\!\{(d_{us},l_{s}^{0})\mid s\in N_{r_{i}}(u)\}\!\}\), \(i\in[k]\), and do the same for the multiset of node \(v\). Our goal is to prove that \(\{\!\{(d_{us},l_{s}^{0})\mid s\in N_{r_{i}}(u)\}\!\}=\{\!\{(d_{vs},l_{s}^{0}) \mid s\in N_{r_{i}}(v)\}\!\}\) for each \(i\in[k]\).

Let the coordinates of node \(s\) in a spherical coordinate system be \((r_{s},\theta_{s},\phi_{s})\). Since nodes \(u\) and \(v\) have the same initial label, they must be from the same layer, meaning that \(r_{u}=r_{v}\). Additionally, we always realign the coordinate systems of \(G^{L}\) and \(G^{R}\) to ensure that the direction of the polyhedra is the same. The distance between node \(u\) and node \(s\) in a spherical coordinate system is given by \(\sqrt{r_{u}^{2}+r_{s}^{2}-2r_{u}r_{s}}\big{(}\sin\theta_{u}\sin\theta_{s}\cos (\phi_{u}-\phi_{s})+\cos\theta_{u}\cos\theta_{s}\big{)}\). We denote the angle term \(\big{(}\sin\theta_{u}\sin\theta_{s}\cos(\phi_{u}-\phi_{s})+\cos\theta_{u}\cos \theta_{s}\big{)}\) by \(\Theta_{us}\) for simplicty.

For each value of \(r_{i}\), it can be one of three types: _ori_, _com_, or _all_. Similarly, nodes \(u\) and \(v\) can belong to one of three categories: _ori_, _com_, _all_. Regardless of the combination, these situations can always be found in the Equations (14, 15, 16) derived from Lemma A.3 and can produce the following conclusion:

\[\big{(}l_{u}^{0},\{\!\{(\Theta_{us},l_{s}^{0})\mid s\in N_{r_{i}}(u)\}\!\} \big{)}=\big{(}l_{v}^{0},\{\!\{(\Theta_{vs},l_{s}^{0})\mid s\in N_{r_{i}}(v) \}\!\}\big{)}.\]

Then we have:

\[\big{(}l_{u}^{0},\{\!\{(\sqrt{r_{u}^{2}+r_{i}^{2}-2r_{u}r_{i} \Theta_{us}},l_{s}^{0})\mid s\in N_{r_{i}}(u)\}\!\}\big{)}\] \[= \big{(}l_{v}^{0},\{\!\{(\sqrt{r_{v}^{2}+r_{i}^{2}-2r_{v}r_{i} \Theta_{vs}},l_{s}^{0})\mid s\in N_{r_{i}}(v)\}\!\}\big{)}\]

since \(r_{u}=r_{i}\). This means that for all values of \(r_{i}\), \(\{\!\{(d_{us},l_{s}^{0})\mid s\in N_{r_{i}}(u)\}\!\}=\{\!\{(d_{vs},l_{s}^{0}) \mid s\in N_{r_{i}}(v)\}\!\}\). By merging all the multisets of radius \(r_{i}\), we can get \(\{\!\{(d_{us},l_{s}^{0})\mid s\in\bigcup_{i\in[k]}N_{r_{i}}(u)\}\!\}=\{\!\{(d_{ vs},l_{s}^{0})\mid s\in\bigcup_{i\in[k]}N_{r_{i}}(v)\}\!\}\), which concludes the proof.

**Step 3.** Since the stable states \((\mathcal{L}^{L},\mathcal{L}^{R})\) are obtained by assigning the stable states of each layer, i.e. \(\mathcal{L}_{com}\), \(\mathcal{L}_{ori}\) or \(\mathcal{L}_{all}\), that cannot be distinguished by Vanilla DisGNN, the histogram of both graphs are exactly the same, which means that Vanilla DisGNN cannot distinguish the two graphs.

**Step 4.** Notice that the construction of an isomorphic mapping requires that the radius of the layer of node \(u\) in \(G^{L}\) and node \(v\) in \(G^{R}\) must be the same. If there exists a pair of layers in the two geometric graphs that are non-congruent, then the two geometric graphs are non-congruent. According to the definition of \((G^{L},G^{R})\), such layers always exist, therefore the two geometric graphs are non-congruent.

Proofs

### Proof of Proposition 6.3

The main proof of our proposition is to construct the key procedures in GemNet [Gasteiger et al., 2021] using basic message passing layers of 3-E/F-DisGNN, and the whole model can be constructed by stacking these message passing layers up. Since the \(k\)-E-DisGNN and \(k\)-F-DisGNN share the same initialization and output blocks and have similar update procedure (they can both learn 4-order geometric information), we mainly focus on the proof of E version, and one can check easily that the derivation for F version also holds.

**Basic methods.** Assume that we want to learn \(\mathcal{O}_{\mathrm{tar}}=f_{\mathrm{tar}}(\mathcal{I}_{\mathrm{tar}})\) with our function \(\mathcal{O}_{\mathrm{fit}}=f_{\mathrm{fit}}(\mathcal{I}_{\mathrm{fit}})\). In our proof, the form of \(\mathcal{O}_{\mathrm{tar}}\) and \(\mathcal{O}_{\mathrm{fit}}\), as well as the form of \(\mathcal{I}_{\mathrm{tar}}\) and \(\mathcal{I}_{\mathrm{fit}}\), are quite different. For example, consider the case where \(\mathcal{O}_{\mathrm{fit}}\) is an embedding \(h_{abc}\) for a 3-tuple \(abc\) and \(\mathcal{I}_{\mathrm{fit}}=\mathcal{I}_{\mathrm{fit}}^{(abc)}\) contains the information of all the neighbors of \(abc\), while \(\mathcal{O}_{\mathrm{tar}}\) is an embedding \(m_{ab}\) for a 2-tuple \(ab\) and \(\mathcal{I}_{\mathrm{tar}}=\mathcal{I}_{\mathrm{tar}}^{(ab)}\) contains the information of all the neighbors of \(ab\). Therefore, directly learning functions by \(f_{\mathrm{fit}}\) that produces exactly the same output as \(f_{\mathrm{tar}}\) is inappropriate. Instead, we will learn functions that can calculate several different outputs in a way \(f_{\mathrm{tar}}\) does and appropriately embeds them into the output of \(f_{\mathrm{fit}}\). For example, we still consider the case mentioned before, and we want to learn a function \(f_{\mathrm{fit}}\) that can extract \(\mathcal{I}_{\mathrm{tar}}^{(ab)},\mathcal{I}_{\mathrm{tar}}^{(ac)},\mathcal{I }_{\mathrm{tar}}^{(bc)},\mathcal{I}_{\mathrm{tar}}^{(ba)},\mathcal{I}_{\mathrm{ tar}}^{(ca)},\mathcal{I}_{\mathrm{tar}}^{(cb)}\) from \(\mathcal{I}_{\mathrm{fit}}^{(abc)}\) respectively, and calculates \(m_{ab},m_{ac},m_{bc},m_{ba},m_{ca},m_{cb}\) with these information like \(f_{\mathrm{tar}}\), and embed them into the output \(h_{abc}\) in an injective way.

Since we realize \(f_{\mathrm{fit}}\) as a universal function approximator (such as MLPs and deep multisets), \(f_{\mathrm{fit}}\) can always learn the function we want. So the only concern is that **whether we can extract exact**\(\mathcal{I}_{\mathrm{tar}}\)**from**\(\mathcal{I}_{\mathrm{fit}}\), i.e., whether there exists an injective function \((\mathcal{I}_{\mathrm{tar}}^{(1)},\mathcal{I}_{\mathrm{tar}}^{(2)},..., \mathcal{I}_{\mathrm{tar}}^{(n)})=f_{\mathrm{ext}}(\mathcal{I}_{\mathrm{fit}})\). We will mainly discuss about this in our proof.

**Notations.** We use the superscript \(G\) to represent functions in GemNet and _3E_ to represent functions in 3-E-DisGNN, and use \(\mathcal{I}\) with the same superscript and subscript to represent the input of a function. We use the superscript \((a_{1}a_{2}..a_{k})\) to represent some input \(\mathcal{I}\) if it's for tuple \((a_{1}a_{2}..a_{k})\). If there exists an injective function \(\mathcal{I}_{\mathrm{tar}}=f_{\mathrm{ext}}(\mathcal{I}_{\mathrm{fit}})\), then we denote it by \(\mathcal{I}_{\mathrm{fit}}\rightarrow\mathcal{I}_{\mathrm{tar}}\), meaning that \(\mathcal{I}_{\mathrm{tar}}\) can be derived from \(\mathcal{I}_{\mathrm{fit}}\). If some geometric information \(\mathcal{I}_{\mathrm{geo}}\) (such as distance and angles) is contained in the distance matrix of a tuple \(a_{1}a_{2}..a_{k}\), then we denote it by \(\mathcal{I}_{\mathrm{geo}}\in a_{1}a_{2}..a_{k}\). For simplicity, We omit all the time superscript \(t\) if the context is clear.

#### b.1.1 Construction of Embedding Block

**Initialization of directional embeddings.** GemNet initialize all the two-tuples \(m\) (also called directional embeddings) at the embedding block by the following paradigm

\[m_{ab}=f_{\mathrm{init}}^{\mathrm{G}}(z_{a},z_{b},d_{ab}).\] (17)

At the initial step, 3-E-DisGNN follows the paradigm outlined below:

\[h_{abc}=f_{\mathrm{init}}^{\mathrm{3E}}(z_{a},z_{b},z_{c},d_{ab},d_{ac},d_{bc}).\] (18)

Then we have

\[\mathcal{I}_{\mathrm{init}}^{\mathrm{3E},(abc)}\rightarrow(\mathcal{I}_{ \mathrm{init}}^{\mathrm{G},(ab)},\mathcal{I}_{\mathrm{init}}^{\mathrm{G},(ac)}, \mathcal{I}_{\mathrm{init}}^{\mathrm{G},(bc)},\mathcal{I}_{\mathrm{init}}^{ \mathrm{G},(ba)},\mathcal{I}_{\mathrm{init}}^{\mathrm{G},(ca)},\mathcal{I}_{ \mathrm{init}}^{\mathrm{G},(cb)}),\]

meaning that we can extract all the information to calculate \(m_{ab},m_{ac},m_{bc},m_{ba},m_{ca},m_{cb}\) from the input of \(f_{\mathrm{init}}^{\mathrm{3E}}\). And thanks to the universal property of \(f_{\mathrm{init}}^{\mathrm{3E}}\), we can approximate a function that accurately calculates these variables and injectively embeds them into \(h_{abc}\), such that \(h_{abc}\rightarrow(m_{ab},m_{ac},m_{bc},m_{ba},m_{ca},m_{cb})\).

**Initialization of atom embeddings.** GemNet initializes all the one-tuples \(u_{i}\) (also called atom embeddings) at the embedding block simply by passing atomic number \(z_{i}\) through an embedding layer.

Note that since we can learn all the \(m_{ij}\) and embed them into \(h_{abc}\) by \(f_{\mathrm{init}}^{\mathrm{3E}}\), it is also possible to learn all the \(u_{i},i\in\{a,b,c\}\) and embed them into \(h_{abc}\) at the same time with some function, since \(\mathcal{I}_{\mathrm{init}}^{\mathrm{3E},(abc)}\) contains \(z_{i},i\in\{a,b,c\}\). Therefore, \(h_{abc}\rightarrow(u_{a},u_{b},u_{c})\) holds.

**Initialization of geometric information.** It is an important observation that since \(f_{\mathrm{init}}^{\mathrm{3E}}\) take all the pair-wise distance within 3-tuple \(abc\) as input, all the geometric information within the tuple can be included in \(h_{abc}\). This makes geometric information rich in the 3-tuple embedding \(h_{abc}\).

To remind the readers, now we prove that there exists a function \(f_{\mathrm{init}}^{\mathrm{3E}}\) which can correctly calculate \(m_{ij}\) (\(i\neq j,i,j\in\{a,b,c\}\)), \(u_{i}\) (\(i\in\{a,b,c\}\)) and all the geometric information \(\mathcal{I}_{\mathrm{geo}}\) within the triangle \(abc\), and injectively embed them into \(h_{abc}\):

\[h_{abc}\rightarrow\Big{(}\big{(}m_{ij}\mid i\neq j,i,j\in\{a,b,c\}\big{)}, \big{(}u_{i}\mid(i\in\{a,b,c\})\big{)},\big{(}\mathcal{I}_{\mathrm{geo}}\mid \mathcal{I}_{\mathrm{geo}}\in abc\big{)}\Big{)}.\]

#### b.1.2 Construction of Atom Embedding Block

**Atom embedding block.** GemNet updates the 1-tuple embeddings \(u\) by summing up all the relevant \(2\)-tuple embeddings \(m\) in the atom emb block. The process can be formulated as

\[u_{a}=f_{\mathrm{atom}}^{\mathrm{G}}\big{(}\{\!\{(m_{ka},e_{\mathrm{RBF}}^{( ka)})\mid k\in[N]\}\!\}\big{)}.\] (19)

Since this function involves the process of pooling, it cannot be learned by \(f_{\mathrm{init}}^{\mathrm{3E}}\). However, it can be easily learned by the basic message passing layers of 3-E-DisGNN \(f_{\mathrm{MP}}^{\mathrm{3E}}\), which is formulated as

\[h_{abc}=f_{\mathrm{MP}}^{\mathrm{3E}}\big{(}h_{abc},\{\!\{(h_{kbc},e_{ak})\mid k \in[N]\}\!\},\{\!\{(h_{akc},e_{ebk})\mid k\in[N]\}\!\},\{\!\{(h_{abk},e_{ck}) \mid k\in[N]\}\!\}\big{)},\] (20)

\[\text{where }e_{ab}=f_{\mathrm{e}}^{\mathrm{3E}}\big{(}d_{ab},\{\!\{h_{ kab}\mid k\in[N]\}\!\},\{\!\{h_{abk}\mid k\in[N]\}\!\},\{\!\{h_{abk}\mid k\in[N]\} \!\}\big{)}.\] (21)

We now want to learn a function \(f_{\mathrm{MP}}^{\mathrm{3E}}\) that updates the \(u_{a},u_{b},u_{c}\) embedded in \(h_{abc}\) like \(f_{\mathrm{atom}}^{\mathrm{G}}\) and keep the other variables and information unchanged. Note that \(h_{abc}\) is the first input of \(f_{\mathrm{MP}}^{\mathrm{3E}}\), so all the old information is maintained. As what we talked about earlier, the main focus is to check whether the information to update \(u\) is contained in \(f_{\mathrm{MP}}^{\mathrm{3E}}\);'s input. In fact, the following derivation holds

\[\mathcal{I}_{\mathrm{MP}}^{\mathrm{3E},(abc)} \rightarrow\{\!\{(h_{akc},e_{bk})\mid k\in[N]\}\!\} \rightarrow\{\!\{\!\{h_{akc}\mid k\in[N]\}\!\}\!\}\] \[\rightarrow\{\!\{(m_{ka},d_{ka})\mid k\in[N]\}\!\} \rightarrow\{\!\{(m_{ka},e_{\mathrm{RBF}}^{(ka)})\mid k\in[N]\}\!\}= \mathcal{I}_{\mathrm{atom}}^{\mathrm{G},(a)}.\]

Note that \(d_{ka}\in abc\), and \(e_{\mathrm{RBF}}^{(ka)}\) can be calculated from \(d_{ka}\). Similarly, we can derive that \(\mathcal{I}_{\mathrm{MP}}^{\mathrm{3E},(abc)}\rightarrow\mathcal{I}_{\mathrm{ atom}}^{\mathrm{G},(b)}\) and \(\mathcal{I}_{\mathrm{MP}}^{\mathrm{3E},(abc)}\rightarrow\mathcal{I}_{\mathrm{ atom}}^{\mathrm{G},(c)}\). This means we can update \(u\) in \(h\) using a basic message passing layer of 3-E-DisGNN.

#### b.1.3 Construction of Interaction Block

**Message passing.** There are two key procedures in GemNet's message passing block, namely two-hop geometric message passing (Q-MP) and one-hop geometric message passing (T-MP), which can be abstracted as follows

\[\mathrm{T-MP:}\] (22) \[\mathrm{Q-MP:}\] (23)

Note that what we need to construct is a function \(f_{\mathrm{MP}}^{\mathrm{3E}}\) that can update the information about \(m_{ij},i,j\in\{a,b,c\},i\neq j\) embedded in \(h_{abc}\) just like what \(f_{\mathrm{TMP}}^{\mathrm{G}}\) and \(f_{\mathrm{QMP}}^{\mathrm{G}}\) do, and keep the other variables and information unchanged. Since it is quite similar among different \(m_{ij}\), we will just take the update process of \(m_{ab}\) for example.

First, T-MP. For this procedure, the following derivation holds

\[\mathcal{I}_{\mathrm{MP}}^{\mathrm{3E},(abc)} \rightarrow\big{(}h_{abc},\{\!\{(h_{kbc},e_{ak})\mid k\in[N]\}\!\} \rightarrow\{\!\{(h_{abc},h_{kbc},e_{ak})\mid k\in[N]\}\!\}\] \[\rightarrow\{\!\{(m_{kb},d_{kb},d_{ab},\phi_{abk})\mid k\in[N]\} \!\}\rightarrow\{\!\{\!(m_{kb},e_{\mathrm{RBF}}^{(kb)},e_{\mathrm{CBF}}^{( abk)})\mid k\in[N],k\neq a\}\!\}=\mathcal{I}_{\mathrm{TMP}}^{\mathrm{G},(ab)}.\]

Note that in the derivation above, there is an important conclusion implicitly used: the tuple \((h_{abc},h_{kbc},e_{ak})\) actually contains all the geometric information in the 4-tuple \(abck\), because thedistance matrix of the four nodes can be obtained from it. Thus \(d_{kb}\), \(d_{ab}\), \(\phi_{abk}\) can be obtained from it, and since \(e_{\mathrm{RBF}}^{(kb)}\) and \(e_{\mathrm{CBF}}^{(abc)}\) are just calculated from these geometric variables, the derivation holds.

And we can exclude the element in the multiset where the index \(k=a\), simply because these tuples have different patterns from others.

Second, Q-MP. In Q-MP, the pooling objects consist of two indices (which we call two-order pooling), namely \(k_{1}\) and \(k_{2}\) in Equation (23). One alternative way to do this is two-step pooling, i.e. pool two times and once for one index. For example we can pool index \(k_{1}\) before we pool index \(k_{2}\) as follows:

\[m_{ab}=f_{\mathrm{QMP}}^{\mathrm{G}}(\big{\{}\!\{(m_{k_{2}k_{1}},e_{\mathrm{RBF }}^{(k_{2}k_{1})},e_{\mathrm{CBF}}^{(bk_{1}k_{2})},e_{\mathrm{SBF}}^{(abk_{1}k _{2})})\mid k_{1}\in[N],k_{1}\neq a\}\!\mid\!k_{2}\in[N],k_{2}\neq b,a\}\!\big{\}}.\] (24)

Note that the expressiveness of two-step pooling is not less than two-order pooling, i.e.

\[\{\!\{(m_{k_{2}k_{1}},e_{\mathrm{RBF}}^{(k_{2}k_{1})},e_{\mathrm{CBF }}^{(bk_{1}k_{2})},e_{\mathrm{SBF}}^{(abk_{1}k_{2})})\mid k_{1}\in[N],k_{1} \neq a\}\!\mid\!k_{2}\in[N],k_{2}\neq b,a\}\!\!\}\] \[\rightarrow \{\!\{(m_{k_{2}k_{1}},e_{\mathrm{RBF}}^{(k_{2}k_{1})},e_{\mathrm{ CBF}}^{(bk_{1}k_{2})},e_{\mathrm{SBF}}^{(abk_{1}k_{2})})\mid k_{1},k_{2}\in[N],k_{1} \neq a,k_{2}\neq b,a\}\!\}.\]

Thus, if we use two-step pooling to implement Q-MP, it does not reduce expressiveness. Inspired by this, in order to update \(m_{ab}\) in \(h_{abc}\) like \(f_{\mathrm{QMP}}^{\mathrm{G}}\), we first learn a function by \(f_{\mathrm{MP}}^{3\mathrm{E}}\) that calculates an intermediate variable \(w_{c}=f_{\mathrm{inter1}}^{3\mathrm{E}}\big{(}\{\!\{(m_{c},e_{\mathrm{RBF}}^{ (ck)},e_{\mathrm{CBF}}^{(bkc)},e_{\mathrm{SBF}}^{(abc)})\mid k\in[N],k\neq a\} \!\big{)}\!\big{)}\) by pooling all the \(m_{ck}\) at index \(k\), which is feasible because the following derivation holds

\[\mathcal{I}_{\mathrm{MP}}^{3\mathrm{E},(abc)} \rightarrow\big{(}h_{abc},\{\!\{(h_{abk},e_{ck})\mid k\in[N]\} \!\}\rightarrow\{\!\{(h_{abc},h_{abk},e_{ck})\mid k\in[N]\}\!\}\] \[\rightarrow\{\!\{(m_{ck},d_{ck},d_{bk},\phi_{abk},\theta_{abk}) \mid k\in[N]\}\!\}\] \[\rightarrow\{\!\{(m_{ck},e_{\mathrm{RBF}}^{(ck)},e_{\mathrm{CBF}}^ {(bkc)},e_{\mathrm{SBF}}^{(abc)})\mid k\in[N],k\neq a\}\!\}=\mathcal{I}_{ \mathrm{inter}}^{3\mathrm{E},(c)}.\]

Note that in the derivation process above, \(m_{ck}\) is directly derived from \(e_{ck}\) because it contains the information by definition 21. Then we apply another message passing layer \(f_{\mathrm{MP}}^{3\mathrm{E}}\) but this time we learn a function that just pools all the \(w_{c}\) in \(h_{abc}\) and finally updates \(m_{ab}\):

\[\mathcal{I}_{\mathrm{MP}}^{3\mathrm{E},(abc)}\rightarrow\{\!\{(h_{abk},e_{ck })\mid k\in[N]\}\!\}\rightarrow\{\!\{h_{abk}\mid k\in[N]\}\!\}\rightarrow\{\! \{w_{k}\mid k\in[N],k\neq b,a\}\!\}.\]

This means we can realize \(f_{\mathrm{QMP}^{\prime}}^{\mathrm{G}}\) by stacking two message passing layers up.

**Atom self-interaction.** This sub-block actually involves two procedures: First, update atom embeddings \(u\) according to the updated directional embeddings \(m\). Second, update the directional embeddings \(m\) according to the updated atom embeddings \(u\). The first step is actually an atom embedding block, which is already realized by our \(f_{\mathrm{MP}}^{3\mathrm{E}}\) in Appendix B.1.1. The second procedure can be formulated as

\[m_{ab}=f_{\mathrm{self-inter}}^{\mathrm{G}}(u_{a},u_{b},m_{ab}).\] (25)

It is obvious that we can update \(m_{ab}\) in \(h_{abc}\) by \(f_{\mathrm{MP}}^{3\mathrm{E}}\) because the following derivation holds

\[\mathcal{I}_{\mathrm{MP}}^{3\mathrm{E},(abc)}\to h_{abc}\rightarrow(u_{a},u_{b },m_{ab})=\mathcal{I}_{\mathrm{self-inter}}^{\mathrm{G},(ab)}.\]

#### b.1.4 Construction of Output Block

In GemNet, the final output \(t\) is obtained by summing up all the sub-outputs from each interaction block. While it is possible to add additional sub-output blocks to our model, in our proof we only consider the case where the output is obtained solely from the final interaction block.

The following function produces the output of GemNet

\[t=\sum_{a\in[N]}W_{\mathrm{out}}\big{(}f_{\mathrm{atom}}^{\mathrm{G}}(\{\!\{(m_{ ka},e_{\mathrm{RBF}}^{(ka)})\mid k\in[N]\}\!\})\big{)},\] (26)

where \(W_{\mathrm{out}}\) is a learnable matrix.

And our output function is

\[t=f_{\mathrm{output}}^{3\mathrm{E}}\big{(}\{\!\{h_{abc}\mid a,b,c\in[N]\}\!\} \big{)}.\] (27)

We can realize Equation (26) by stacking a message passing layer and an output block. First, the message passing layer updates all the atom embeddings \(u\) in \(h_{abc}\), and then the output block extracts \(u\) from \(h\) as follows, and calculates \(t\) like GemNet.

\[\mathcal{I}_{\mathrm{output}}^{3\mathrm{E},(abc)}\rightarrow\{\!\{h_{aaa}\mid a \in[N]\}\!\}\rightarrow\{\!\{u_{a}\mid a\in[N]\}\!\}.\]

### Proof of Proposition 6.2

In this section, we will follow the basic method and notations in Appendix B.1. We will mainly prove for 2-F-DisGNN, and since when \(k=2\), \(k\)-E-DisGNN's update function can implement \(k\)-F-DisGNN's (See Appendix B.4), the derivations also holds for 2-E-DisGNN. We will use the superscript \(D\) to represent functions in DimeNet and 2\(F\) to represent functions in 2-F-DisGNN.

#### b.2.1 Construction of Embedding Block

DimeNet initializes all the two tuples in the embedding block just like GemNet, which can be formulated as

\[m_{ab}=f_{\mathrm{init}}^{\mathrm{D}}(z_{a},z_{b},d_{ab}).\] (28)

What 2-F-DisGNN does at the initialization step is

\[h_{ab}=f_{\mathrm{init}}^{\mathrm{2F}}(z_{a},z_{b},d_{ab}).\] (29)

Now assume we want to learn such a function that can learn both \(m_{ab}\) and \(m_{ba}\), then embed it into \(h_{ab}\). As what we talked about in Appendix B.1, it is possible because the following derivation holds

\[\mathcal{I}_{\mathrm{init}}^{\mathrm{2F},(ab)}\rightarrow(\mathcal{I}_{ \mathrm{init}}^{\mathrm{D},(ab)},\mathcal{I}_{\mathrm{init}}^{\mathrm{D},(ba)}).\]

Note that \(h_{ab}\) also contains the geometric information, but in this case, just the distance. Different from GemNet, DimeNet does not "track" the 1-tuple embeddings: it does not initialize and update the atom embeddings in the model, and only pool the 2-tuples into 1-tuples in the output block. So there is no need to embed the embeddings of atom \(a\) and atom \(b\) into \(h_{ab}\).

#### b.2.2 Construction of Interaction Block

The message passing framework of DimeNet (so-called directional message passing) can be formulated as

\[m_{ab}=f_{\mathrm{MP}}^{\mathrm{D}}\big{(}\!\big{\{}\!\big{(}m_{ka},e_{ \mathrm{RBF}}^{(ab)},e_{\mathrm{CBF}}^{(kab)}\big{)}\mid k\in[N],k\neq b\!\big{\}} \!\big{)}.\] (30)

And the message passing framework of 2-F-DisGNN can be formulated as

\[h_{ab}=f_{\mathrm{MP}}^{\mathrm{2F}}\big{(}h_{ab},\{\!\big{(}h_{kb},h_{ak} \big{)}\mid k\in[N],k\neq b\!\big{\}}\!\big{)}.\] (31)

Now we want to learn a function \(f_{\mathrm{MP}}^{\mathrm{2F}}\) that can updates the \(m_{ab}\) and \(m_{ba}\) embedded in \(h_{ab}\) like what \(f_{\mathrm{MP}}^{\mathrm{D}}\) does. We need to check if \(f_{\mathrm{MP}}^{\mathrm{2F}}\) have sufficient information to update \(m_{ab}\) and \(m_{ba}\) embedded in its output \(h_{ab}\). The derivation holds:

\[\mathcal{I}_{\mathrm{MP}}^{\mathrm{2F},(ab)} \rightarrow\] \[\rightarrow\]

Note that we again used the observation that tuple \((h_{ab},h_{kb},h_{ak})\) contains all the geometric information of 3-tuple \(abk\). Similarly we can get \(\mathcal{I}_{\mathrm{MP}}^{\mathrm{2F},(ab)}\rightarrow\mathcal{I}_{\mathrm{MP }}^{\mathrm{D},(ba)}\).

#### b.2.3 Construction of Output Block

The output block of DimeNet is quite similar to that of GemNet (Appendix B.1.4), which can be formulated as

\[u_{a} =f_{\mathrm{atom}}^{\mathrm{D}}\big{(}\!\big{\{}\!\big{(}m_{ka},e_ {\mathrm{RBF}}^{(ka)}\big{)}\mid k\in[N]\!\big{\}}\!\big{)},\] (32) \[t =\sum_{a\in[N]}u_{a}.\] (33)

This can be realized by stacking a message passing layer of 2-F-DisGNN and its output block up.

The message passing layer of 2-F-DisGNN can learn \(u_{a}\) and \(u_{b}\) and embed it into the output \(h_{ab}\) because the following derivation holds

\[\mathcal{I}_{\mathrm{MP}}^{\mathrm{2F},(ab)} \rightarrow\] \[\rightarrow\]Similarly we can get \(\mathcal{I}_{\mathrm{MP}}^{2\mathrm{F},(ab)}\rightarrow\mathcal{I}_{\mathrm{atom}}^{ \mathrm{D},(b)}\).

And the output block of 2-F-DisGNN is formulated as follows

\[t=f_{\mathrm{output}}^{2\mathrm{F}}\big{(}\{\!\!\{h_{ab}\mid a,b\in[N]\}\!\!\} \big{)}.\] (34)

Note that the following derivation holds

\[\mathcal{I}_{\mathrm{output}}^{2\mathrm{F},(ab)}\rightarrow\{\!\!\{h_{aa} \mid a\in[N]\}\!\!\}\rightarrow\{\!\!\{u_{a}\mid a\in[N]\}\!\!\}.\]

This means that the output block of 2-F-DisGNN can implement the sum operation in Equation (33).

### Proof of Theorem 6.4

We first restate the theorem formally.

**Theorem B.1**.: _Let \(\mathcal{M}(\theta)\in\mathrm{MinMods}=\{\)1-round 4-DisGNN, 2-round 3-E-DisGNN, 2-round 3-F-DisGNN\(\}\) with parameters \(\theta\). Denote \(h_{m}=f_{\mathrm{node}}\big{(}\{\!\!\{h_{\bm{v}}^{T}\mid\bm{v}\in V^{k},\bm{v}_ {0}=m\}\!\!\}\big{)}\in\mathbb{R}^{K^{\prime}}\) as node \(m\)'s representation produced by \(\mathcal{M}\) where \(f_{\mathrm{node}}\) is an injective multiset function, and \(\bm{\mathrm{x}}_{m}^{c}\) as node \(m\)'s coordinates w.r.t. the center. Denote \(\mathrm{MLPs}(\theta^{\prime}):\mathbb{R}^{K^{\prime}}\rightarrow\mathbb{R}\) as a multi-layer perceptron with parameters \(\theta^{\prime}\)._

1. _(_**Completeness**_) Given arbitrary two geometric graphs_ \(\mathbf{X}_{\mathbf{1}},\mathbf{X}_{2}\in\mathbb{R}^{3\times n}\)_, then there exists_ \(\theta_{0}\) _such that_ \(\mathcal{M}(\mathbf{X_{\mathbf{1}}};\theta_{0})=\mathcal{M}(\mathbf{X_{\mathbf{ 2}}};\theta_{0})\iff\mathbf{X_{\mathbf{1}}}\) _and_ \(\mathbf{X_{\mathbf{1}}}\) _are congruent._
2. _(_**Universality for scalars**_) Let_ \(f:\mathbb{R}^{3\times n}\rightarrow\mathbb{R}\) _be an arbitrary continuous,_ \(E(3)\) _invariant and permutation invariant function over geometric graphs, then for any compact set_ \(M\subset\mathbb{R}^{3\times n}\) _and_ \(\epsilon>0\)_, there exists_ \(\theta_{0}\) _such that_ \(\forall\mathbf{X}\in M,|f(\mathbf{X})-\mathcal{M}(\mathbf{X};\theta_{0})|\leq\epsilon\)_._
3. _(_**Universality for vectors**_) Let_ \(f:\mathbb{R}^{3\times n}\rightarrow\mathbb{R}^{3}\) _be an arbitrary continuous,_ \(O(3)\) _equivariant, permutation and translation invariant function over geometric graphs, let_ \(f_{\mathrm{out}}^{\mathrm{equiv}}=\sum_{m=1}^{|V|}\mathrm{MLPs}\big{(}h_{m} \big{)}\bm{\mathrm{x}}_{m}^{c}\)_, then for any compact set_ \(M\subset\mathbb{R}^{3\times n}\) _and_ \(\epsilon>0\)_, there exists_ \(\theta_{0}\) _for_ \(\mathcal{M}\) _and_ \(\theta_{0}^{\prime}\) _for_ \(\mathrm{MLPs}\) _such that_ \(\forall\mathbf{X}\in M,|f(\mathbf{X})-f_{\mathrm{out}}^{\mathrm{equiv}}( \mathbf{X};\theta_{0},\theta_{0}^{\prime})|\leq\epsilon\)_._

In the forthcoming proof for completeness part, we examine the optimal scenario for \(k\)-(E/F-)DisGNNs, namely the geometric version \(k\)-(E/F)WL, where the hyper-parameters such as hidden dimensions are set and parameters \(\theta\) of \(\mathcal{M}(\theta)\) are learned to ensure that all the relevant functions are hash functions. To simplify the analysis for \(k\)-EWL, we substitute the edge representation in \(k\)-E-DisGNN with the edge weight (distance) alone, which is already enough.

**Basic idea for proof of completeness.** We prove the completeness part of the theorem, which demonstrates that the optimal DisGNNs have the ability to distinguish all geometric graphs, through a _reconstruction_ method. Specifically, if we can obtain all points' coordinates up to permutation and E(3) translation from the output of DisGNNs, it implies that DisGNNs can differentiate between all geometric graphs.

#### b.3.1 Proof of Completeness for One-Round 4-WL

Proof.: We begin by assuming the existence of four affinely independent nodes, meaning that there exists a 4-tuple of nodes that can form a tetrahedron. If this condition is not met, and all the nodes lie on the same plane, the problem is simplified, as the cloud degenerates into 2D space. We will address this case later in the discussion. We assume the point number is \(N\).

Our first step is to prove that we can recover the 4-tuple formed by the four affinely independent nodes from the output of one-round 4-WL, denoted by \(\{\!\!\{c_{\mathbf{i}}^{1}\mid\bm{i}\in[N]^{4}\}\!\!\}\). Since all the tuple colors are initialized injectively according to the tuple's distance matrices, there exists a function \(f^{0}\) such that \(f^{0}(c_{\mathbf{i}}^{0})=D_{\bm{i}}\), where \(D_{\bm{i}}\) is the distance matrix of tuple \(\bm{i}\). Note that since the update procedures of the colors are \(\mathrm{HASH}\) functions, \(c_{\bm{i}}^{t}=\mathrm{HASH}(c_{\bm{i}}^{t-1},C_{0,\bm{i}}^{t-1},C_{1,\bm{i}}^{ t-1},C_{2,\bm{i}}^{t-1},C_{3,\bm{i}}^{t-1})\) where \(C_{m,\bm{i}}^{t-1}\) denotes the color multiset of tuple \(\bm{i}\)'s \(m\)-th neighbors, there also exists a series functions \(f^{t}\) such that \(f^{t}(c_{\bm{i}}^{t})=c_{\bm{i}}^{t-1}\). This simply means that the latter colors of a tuple will contain all the information of its former colors. Given the fact, we can use the function \(f^{1}\circ f^{0}\) to reconstruct the distance matrices of all the tuples, and find the one that represents a tetrahedron geometry (Theorem 3.1). We mark the found tuple with \(\bm{k}\).

Now we prove that one can reconstruct the whole geometry from just \(c_{\bm{k}}^{1}\).

First, we can reconstruct the 4 points' 3D coordinates (given any arbitrary center and orientation) from \(\bm{k}\)'s distance matrix (Theorem 3.1). Formally, there exists a function \(f^{D}(D_{\bm{k}})=X_{\bm{k}}\), where \(X_{\bm{k}}\in\mathbb{R}^{4*3}\) represents the coordinates. And since \(f^{1}\circ f^{0}(c_{\bm{k}}^{1})=D_{\bm{k}}\), we have that \(f^{1}\circ f^{0}\circ f^{D}(c_{\bm{k}}^{1})=X_{\bm{k}}\).

The update function of \(c_{\bm{k}}\) is \(c_{\bm{k}}^{1}=\operatorname{HASH}(c_{\bm{k}}^{0},C_{0,\bm{k}}^{0},C_{1,\bm{k} }^{0},C_{2,\bm{k}}^{0},C_{3,\bm{k}}^{0})\), where \(C_{m,\bm{k}}^{0}=\{\!\!\{c_{\Phi_{m}(\bm{k},j)}^{0}\mid j\in[N]\}\!\!\},m\in[4]\) and \(\Phi_{m}(\bm{k},j)\) replaces the \(m\)-th element in tuple \(\bm{k}\) with \(j\). Since the function is \(\operatorname{HASH}\) function, we can also reconstruct each of \(C_{m,\bm{k}}^{0}\) from \(c_{\bm{k}}^{1}\). In \(C_{m,\bm{k}}^{0}\), there exists \(N\) 4-tuples' initial color, from which we can reconstruct the 4-tuple's distance matrix. Note that given a color \(c_{\Phi_{m}(\bm{k},j)}^{0}\) in the multiset, we can reconstruct 3 distances, namely \(d(x_{j},x_{\bm{k}_{(m+1)\%4}}),d(x_{j},x_{\bm{k}_{(m+2)\%4}}),d(x_{j},x_{\bm{k }_{(m+3)\%4}})\), where \(x_{j}\) representes the 3D coordinates of point \(j\) and \(d(x,y)\) calculates the \(l_{2}\) norm of \(x,y\).

There is a strong geometric constrain in 3D space: Given the distances between a point and 3 affinely independent points (whose postions are known), one can calculate at most 2 possible positions of the point, and the 2 possible positions are mirror-symmetric relative to the plane formed by the 3 affinely independent points. Moreover, the point is on the plane formed by the 3 affinely independent points iff there is only one solution of the distance equations.

Now since we have already reconstructed \(X_{\bm{k}}\), and \(\bm{k}_{(m+1)\%4}\), \(\bm{k}_{(m+2)\%4}\), \(\bm{k}_{(m+3)\%4}\) are affinely independent, we can calculate 2 possible positions of point \(j\). And for the whole multiset \(C_{m,\bm{k}}^{0}\), we can calculate a \(N\)-size multiset where each element is a pair of possible positions of some point, denoted as \(P_{m,\bm{k}}=\{\!\!\{x_{j}^{(1)},x_{j}^{(2)}\}\mid j\in[N]\!\!\}\). Note that now the possible geometry of the \(N\) points form a 0-dimension manifold, with no more than \(2^{N}\) elements.

There are two important properties of \(P_{m,\bm{k}}\):

1. Given arbitrary two elements (pairs) of \(P_{m,\bm{k}}\), denoted as \(p_{j_{1}}\) and \(p_{j_{2}}\), we have \(p_{j_{1}}=p_{j_{2}}\) or \(p_{j_{1}}\cap p_{j_{2}}=\emptyset\). These correspond to two possible situations: \(j_{1}\) and \(j_{2}\) are either mirror-symmetric relative to the plane formed by \(\bm{k}_{(m+1)\%4},\bm{k}_{(m+2)\%4},\bm{k}_{(m+3)\%4}\), or not.
2. Arbitrary element \(p_{j}\) of \(P_{m,\bm{k}}\) has multiplicity value at most 2. This corresponds two possible situations: \(j\) either has a mirror-symmetric point relative to the plane formed by \(\bm{k}_{(m+1)\%4},\bm{k}_{(m+2)\%4},\bm{k}_{(m+3)\%4}\), or does not.

The follows proves that the four 0-dimension manifolds determined by \(P_{m,\bm{k}},m\in[4]\) can intersect at a unique solution, which is the real geometry.

We first "refine" the multiset \(P_{m,\bm{k}}\). There are 3 kinds of pairs in \(P_{m,\bm{k}}\):

1. The pair \(\{x_{j}^{(1)},x_{j}^{(2)}\}\) that have multiplicity value 2. This means that there are two mirror-symmetric points relative to the plane formed by \(\bm{k}_{(m+1)\%4},\bm{k}_{(m+2)\%4},\bm{k}_{(m+3)\%4}\). So we can ensure that the two points with coordinates \(x_{j}^{(1)},x_{j}^{(2)}\) both exist.
2. The pair where \(x_{j}^{(1)}=x_{j}^{(2)}=x_{j}\). This means that by solving the 3 distance equations, there is only one solution. So we can uniquely determine the coordinates of point \(j\) (and the point is on the plane).
3. The other pairs.

We can determine the real coordinates of the points from the first two kinds of pairs, so we record the real coordinates and delete them from the multiset \(P_{m,\bm{k}}\) (for the first kind, delete both two pairs). We denote the real coordinates determined by \(P_{m,\bm{k}}\) as \(A_{m}\). Now we have 4 preliminarily refined sets, denoted as \(P_{m,\bm{k}}^{\prime}\). Note that now in each \(P_{m,\bm{k}}^{\prime}\):

1. Arbitrary two elements intersect at \(\emptyset\).
2. Arbitrary element has two distinct possible coordinates, one is real and the other is fake.

[MISSING_PAGE_EMPTY:28]

As the update procedure, \(c^{2}_{\bm{k}}=\mathrm{HASH}(c^{1}_{\bm{k}},\{\!\{\!\{c^{1}_{\Phi_{0}(\bm{k},j)},c^{ 1}_{\Phi_{1}(\bm{k},j)},c^{1}_{\Phi_{2}(\bm{k},j)}\}\mid j\in N\!\}\!\})\), each tuple \((c^{1}_{\Phi_{0}(\bm{k},j)},c^{1}_{\Phi_{1}(\bm{k},j)},c^{1}_{\Phi_{2}(\bm{k},j )})\) in the multiset contains the distance \(d_{\bm{k}_{0},j},d_{\bm{k}_{1},j},d_{\bm{k}_{2},j}\). Thus, we can calculate at most two possible coordinates of node \(j\), and the two coordinates are mirror-symmetric relative to plane \(\bm{k}\). Like in the proof of 4-WL, we denote the final calculaoed possible coordinates-pair multiset as \(P_{\bm{k}}=\{\!\{x^{(1)}_{j},x^{(2)}_{j}\}\mid j\in[N]\!\}\!\).

We then further find a node \(j_{0}\) that is not on plane \(\bm{k}\) from the multiset term in \(c^{2}_{\bm{k}}\), and its colors' tuple is \((c^{1}_{\Phi_{0}(\bm{k},j_{0})},c^{1}_{\Phi_{1}(\bm{k},j_{0})},c^{1}_{\Phi_{2}( \bm{k},j_{0})})\). Note that the three colors are at time-step 1, which means that they already aggregated neighbors' information for one-round. So by repeating the above procedure, we can again get a possible coordinates-pair multiset \(P_{\Phi_{m}(\bm{k},j_{0})}\) from each of \(c^{1}_{\Phi_{m}(\bm{k},j_{0})}\) where \(m\in[3]\).

Note that the four plane, \(\bm{k},\Phi_{0}(\bm{k},j_{0}),\Phi_{1}(\bm{k},j_{0}),\Phi_{2}(\bm{k},j_{0})\), do not intersect at a common point. So we can do as what we do in the proof of 4-WL, to repeatedly refine each multiset \(P\) and get all the real coordinates, thus reconstruct the whole geometry. 

#### b.3.3 Proof of Completeness for Two-Round 3-E-DisGNN

Proof.: As previous, our discussion is under the fact that the point cloud does not degenerates to a 2D or 1D point cloud. Otherwise, the problem is quite trivial.

So we first find the tuple containing three nodes that are affinely independent from the 2-round 3-E-DisGNN's output \(\{\!\{c^{2}_{\bm{i}}\mid\bm{i}\in[N]^{3}\!\}\!\}\), and refer to it as tuple \(\bm{k}\). Its output color \(c^{2}_{\bm{k}}\) can derive the three points' coordinates. The update procedure is \(c^{2}_{\bm{k}}=\mathrm{HASH}(c^{1}_{\bm{k}},\{\!\{c^{1}_{\Phi_{0}(\bm{k},j)}, d_{j},\bm{k}_{0})\mid j\in[N]\!\}\!\},\{\!(c^{1}_{\Phi_{1}(\bm{k},j)},d_{j},\bm{k}_{1 })\mid j\in[N]\!\}\!\},\{\!(c^{1}_{\Phi_{2}(\bm{k},j)},d_{j},\bm{k}_{2})\mid j \in[N]\!\}\!\})\). Since in the first multiset \(\{\!(c^{1}_{\Phi_{0}(\bm{k},j)},d_{j},\bm{k}_{0})\mid j\in[N]\!\}\!\}\), the element \((c^{1}_{\Phi_{0}(\bm{k},j)},d_{j},\bm{k}_{0})\) contains distances \(d_{j,\bm{k}_{0}},d_{j,\bm{k}_{1}},d_{j,\bm{k}_{2}}\), we can calculate a possible coordinates-pair multiset as \(P_{\bm{k}}=\{\!\{x^{(1)}_{j},x^{(2)}_{j}\}\mid j\in[N]\!\}\!\}\). As we discussed before, the elements in \(P_{\bm{k}}\) may only have multiplicity value 1 or 2. So we discuss for two possible situations:

1. All the elements (coordinates pairs) in \(P_{\bm{k}}\) either have multiplicity value 2 or the two coordinates within the pair are the same. This means that we can determine all the real coordinates from \(P_{\bm{k}}\), as we discussed in the proof of 4-WL.
2. There exists some element in \(P_{\bm{k}}\), that have two distinct coordinates within the pair, and have multiplicity value 1. We denote the node corresponding to such element \(j_{0}\). We can uniquely find its relevant color (i.e., \(c^{1}_{\Phi_{0}(\bm{k},j_{0})},c^{1}_{\Phi_{1}(\bm{k},j_{0})},c^{1}_{\Phi_{2}( \bm{k},j_{0})}\)) among the three multisets of \(c^{2}_{\bm{k}}\)'s input, because only one \((c^{1}_{\Phi_{m}(\bm{k},j)},d_{j},\bm{k}_{m})\) will derive the pair of possible coordinates of \(j_{0}\), which is unique. Now, as we discussed in the proof of 3-FWL, we can reconstruct the whole geometry.

#### b.3.4 Proof of Universality for Scalars

We have shown in the preceding three subsections that for each \(\mathcal{M}(\theta)\in\mathrm{MinMods}\), there exists a parameter \(\theta_{0}\) such that \(\mathcal{M}(\theta_{0})\) is complete for distinguishing all geometric graphs that are not congruent. Specifically, \(\mathcal{M}(\theta_{0})\) generates a unique \(K\)-dimensional intermediate representation for geometric graphs that are not congruent at the output block. According to Theorem 4.1 in Hordan et al. (2023), by passing the intermediate representation through a MLP, \(\mathcal{M}(\theta_{0})\) can achieve universality for scalars (note that MLPs are already included in the output block of \(\mathcal{M}\)).

#### b.3.5 Proof of Universality for Vectors

The key to proving universality for vectors of function \(f^{\mathrm{equiv}}_{\mathrm{out}}\) is Proposition 10 proposed by Villar et al. (2021). The goal is to prove that function \(f=\mathrm{MLPs}\big{(}h_{m}\big{)}\) can approximate all functions that are O(3)-invariant and permutation-invariant with respect to all nodes except the \(m\)-th node (We denote the corresponding group as \(\mathcal{G}_{m}\)).

First of all, note that \(h_{m}\) is \(\mathcal{G}_{m}\)-invariant. That's because \(h_{\bm{v}}\) (\(\bm{v}\in V^{k}\)) is invariant under O(3) actions, and \(h_{m}\) is obtained by pooling all the \(k\)-tuples whose first index is \(m\). When permuting nodes except the \(m\)-th one, the multiset \(\{\!\!\{\bm{v}\mid\bm{v}_{0}=m,\bm{v}\in V^{k}\}\!\!\}\) does not change. According to Theorem 4.1 in Hordan et al. (2023), to prove that \(f_{\rm out}^{\rm equiv}\) is universal, we need to show that \(h_{m}\) can distinguish inputs \(\bm{X}\in\mathbb{R}^{n\times 3}\) on different \(\mathcal{G}_{m}\) orbits.

To see this, recall that \(h_{m}=f_{\rm node}\big{(}\{\!\!\{h_{\bm{v}}^{T}\mid\bm{v}_{0}=m\}\!\!\}\big{)}\), and as we demonstrated in the first three subsections, there always exists some \(h_{\bm{v}}^{T}\) in the multiset \(\{\!\!\{h_{\bm{v}}^{T}\mid\bm{v}_{0}=m\}\!\!\}\) that can reconstruct the entire geometry. For example, in the case of 3-F-DisGNN, if the point cloud does not degenerate into a 1-D point cloud, there must exist \(\bm{v}\) where \(\bm{v}_{0}=m\) and which contains three nodes that are affinely independent (for arbitrary \(m\)), which can reconstruct the entire geometry as demonstrated in B.3.2. It is worth noting that this reconstruction is started at the \(m\)-th node, thus if two point clouds share the same \(h_{m}\), they must be related by a \(\mathcal{G}_{m}\) action.

Finally, it is worth noting that an alternative perspective to understand the role of group \(\mathcal{G}_{m}\) is through the **universality for node-wise representation**, \(h_{m}\). The statement "Two geometric graphs are related by a \(\mathcal{G}_{m}\) action" is equivalent to the statement "the two geometric graphs are congruent, and the \(m\)-th nodes in the two geometric graphs are related by an autoisomorphism." The ability of models to generate node representations that can separate different \(\mathcal{G}_{m}\) orbits implies their capability to generate universal node-wise representations, which is indeed a more powerful property than global universality.

### Proof of Theorem 6.5

Delle Rose et al. (2023) is our concurrent work, which demonstrated that geometric 3-round 2-FWL and 1-round 3-FWL can distinguish all non-congruent geometric graphs. We refer the readers to read the proof provided by the work, and in comparison to our previous proof, they further make good use of the global properties, i.e., the multiset of all the tuple representations, to get several desired conclusions. In this subsection, we make use of their findings: Since 3-round 2-F-DisGNN and 1-round 3-F-DisGNN are continuous versions of these methods and can achieve injectiveness with a parameter \(\theta_{0}\), they can also achieve completeness.

It is important to note that the update function of 2-E-DisGNN can be used to implement that of 2-F-DisGNN: When the order is 2, the edge representation \(e_{ij}^{i}\) is simplified as \(e_{ij}^{i}=(e_{ij},h_{ij})\). As a result, the neighbor component of 2-F-DisGNN \(\{\!\!\{(h_{ik},h_{kj})\mid k\in V\!\!\}\!\}\) is encompassed in the 1-neighbor component of 2-E-DisGNN \(\{\!\!\{(h_{kj},e_{ik}^{i})\mid k\in V\!\!\}\!\}\). Hence, the 3-round 2-E-DisGNN is also complete. When the round number is limited to 1, the update function of 3-E-DisGNN can also implement that of 3-F-DisGNN. This is due to the fact that during the initialization step, each 3-tuple is embedded based on the distance matrix. Consequently, the neighbor component of 3-F-DisGNN, \(\{\!\!\{(h_{mjk},h_{imk},h_{ijm})\mid m\in V\!\!\}\!\}\), solely represents the distance matrix of \(ijkm\), which can be obtained from any \(s\)-neighbor component (\(s\in[3]\)) of 3-E-DisGNN (It should be noted that when the iteration number is not 1, determining whether the update function of 3-E-DisGNN can implement that of 3-F-DisGNN is not trivial). Thus, the E-version DisGNNs are also complete. Moreover, as per our proof in Section B.3.4, they are also universal for scalars.

**Discussion of universality for vectors.** The primary obstacle to proving the universality of \(\mathcal{M}\in\mathrm{Minmod}_{\rm ext}\) for vectors lies in the limited expressive power of individual nodes, which may not reconstruct the whole geometry. The problem cannot be easily solved by concating \(h_{m}\) with the global universal representation \(h=\{\!\!\{h_{\bm{v}}^{T}\mid\bm{v}\in V^{k}\!\!\}\!\}\). Specifically, given \((h_{m1},h_{1})\) and \((h_{m2},h_{2})\) for two \(N\)-node point clouds, if the two tuples are the same, we know that the two point clouds are related by an E(3) transformation and a permutation of the \(N\) nodes. However, we cannot guarantee that the permutation (or there exists another permutation) maps the \(m\)-th node in the first point cloud to the \(m\)-th node in the second. For instance, even though two points in a point cloud may not be related by an autoisomorphism, they may still have the same representation after applying \(\mathcal{M}\in\mathrm{Minmod}_{\rm ext}\) (Possible counterexamples may be derived from this situation). As a result, \(f\) may not be universal for all \(G_{m}\)-invariant functions, and \(f_{\rm out}^{\rm equiv}\) may not be universal for all vector functions. We leave this as an open question.

Detailed Model Design and Analysis

**Radial basis function.** In \(k\)-DisGNNs, we use radial basis functions (RBF) \(f_{\mathrm{e}}^{\mathrm{rbf}}:\mathbb{R}\rightarrow\mathbb{R}^{H_{e}}\) to expand the distance between two nodes into an \(H_{e}\)-dimension vector. This can reduce the number of learnable parameters and additionally provides a helpful inductive bias (Klicpera et al., 2020). The appropriate choice of RBF is beneficial, and we use nexpnorm RBF defined as

\[f_{e}^{\mathrm{rbf}}(e_{ij})[k]=e^{-\beta_{k}(\exp(-e_{ij})-\mu_{k})^{2}},\] (35)

where \(\beta_{k},\mu_{k}\) are coefficients of the \(k^{\mathrm{th}}\) basis. Experiments show that this RBF performs better than others, such as the Bessel function used in Klicpera et al. (2020), Gasteiger et al. (2021).

**Initialization function.** We realize \(f_{\mathrm{init}}\) of \(k\)-DisGNNs' initialization block in Section 5 as follows

\[f_{\mathrm{init}}(\bm{v})=\bigodot_{i\in[k]}f_{z}^{i}\big{(}f_{z}^{\mathrm{ emb}}(z_{v_{i}})\big{)}\odot\bigodot_{i,j\in[k],i<j}f_{e}^{ij}\big{(}f_{ \mathrm{e}}^{\mathrm{rbf}}(e_{v_{i}v_{j}})\big{)},\] (36)

where \(\odot\) represents Hadamard product and \(\bigodot\) represents Hadamard product over all the elements in the subscript. \(f_{z}^{\mathrm{emb}}:\mathbb{Z}^{+}\rightarrow\mathbb{R}^{H_{z}}\) is a learnable embedding function, \(f_{e}^{\mathrm{rbf}}:\mathbb{R}^{+}\rightarrow\mathbb{R}^{H_{e}}\) is a radial basis function, and \(f_{z}^{i},f_{e}^{ij}\) are neural networks such as \(\mathrm{MLPs}\) that maps the embeddings of \(z\) and \(e\) to the common continuous vector space \(\mathbb{R}^{K}\).

\(f_{\mathrm{init}}\) can learn an injective representation for \(\bm{v}\) as long as the embedding dimensions are high enough, just like passing the concatenation of \(z_{v_{i}}\) and \(e_{v_{i}v_{j}}\) through an MLP. We chose this function form for the better experimental performance.

Note that by this means, tuples with different **equality patterns**(Maron et al., 2018) can be distinguished, i.e., get different representations, without explicitly incorporating the representation of equality pattern. This is because in the context of distance graphs, tuples with different equality patterns will have quite different **distance matrices** (elements are zero at the positions where two nodes are the same) and thus can be captured by \(f_{\mathrm{init}}\).

**Injective functions.** We realize all the message passing functions as injective functions to ensure expressiveness. To be specific, we embed all the multisets in Equation (7, 9, 10) and in output function with the injective multiset function proposed in Xu et al. (2018), and use the matrix multiplication methods proposed in Maron et al. (2019) to implement the message passing functions of \(k\)-F-DisGNN (Equation (8)).

**Inductive bias.** Although theoretically there is no need to distinguish tuples with different equality patterns explicitly, we still do so to incorporate inductive bias into the model for better learning. Specifically, we modify the initialization function and the output function as follows: 1. At the initialization step, we learn embeddings for different equality patterns and incorporate them into the results of Equation (36) through a Hadamard product. 2. At the output step, we separate tuples where all sub-nodes are the same and the others into different multisets. These modifications are both beneficial for training and generalization.

Detailed Experiments

### Experimental Setup

**Training Setting.** For QM9, we use the mean squared error (MSE) loss for training. For MD17, we use the weighted loss function

\[\mathcal{L}(\bm{X},\bm{z})=(1-\rho)|f_{\theta}(\bm{X},\bm{z})-\hat{t}(\bm{X}, \bm{z})|+\frac{\rho}{N}\sum_{i=1}^{N}\sqrt{\sum_{\alpha=1}^{3}(-(\frac{\partial f _{\theta}(\bm{X},\bm{z})}{\partial\bm{x}_{i\alpha}}-\hat{F}_{i\alpha}(\bm{X}, \bm{z}))^{2}},\] (37)

where the force ratio \(\rho\) is fixed as 0.999 (For revised MD17, we set \(\rho\) to 0.99 for several targets since the data quality is better).

We follow the same dataset split as GemNet (Gasteiger et al., 2021). We optimize all models using Adam (Kingma and Ba, 2014) with exponential decay and plateau decay learning rate schedulers, and also a linear learning rate warm-up. To prevent overfitting, we use early stopping on validation loss and an exponential moving average (EMA) with decay rate 0.99 for model parameters during validation and test. We follow DimeNet's (Klicpera et al., 2020) setting to calculate \(\Delta\epsilon\) by taking \(\epsilon_{\mathrm{LUMO}}-\epsilon_{\mathrm{HOMO}}\) and use the atomization energy for \(U_{0},U,H\) and \(G\) on QM9. Experiments are conducted on Nvidia RTX 3090 and Nvidia RTX 4090. Results are average of three runs with different random seeds. Detailed training setting can be referred to Table 4.

**Model hyperparameters.** The key model hyperparameters we coarsely tune are the rbf dimension, hidden dimension, and number of message passing blocks. For rbf dimension, we use 16 for MD17 and 32 for QM9. We choose the number of message passing blocks from \(\{4,5,6\}\). For hidden dimension, we use 512 for 2-DisGNNs and 320 for 3-DisGNNs. The detailed hyperparameters can be found in our codes.

### Supplementary Experimental Information

**Discussion of results in MD17.** In our experiments on MD17, most of the state-of-the-art performance we achieve is on force targets, and the loss on energy targets is relatively higher, see Table 1. On force prediction tasks, \(k\)-DisGNNs rank top 2 on force prediction tasks on average, and outperform the best results by a significant margin on several molecules, such as aspirin and malonaldehyde. However, the results on the energy prediction tasks are relatively lower, with 2-F-DisGNN ranking \(2^{\mathrm{nd}}\) and 3-E-DisGNN ranking \(4^{\mathrm{th}}\).

This is due to the fact that we have assigned a quite **high weight** (0.999) to the **force loss** during training, similar to what GemNet(Gasteiger et al., 2021) does. In comparison, TorchMD (Tholke and De Fabritiis, 2021) assigned a weight of 0.8 to the force loss, resulting in better results on energy targets, but not as good results on force targets.

Actually, in molecular simulations, force prediction is a more challenging task. It determines the **accuracy** of molecular simulations and reflects the performance of a model better (Gasteiger et al., 2021) (One can see that the energy performance of different models does not vary much). Therefore, it makes more sense to focus on force prediction. Furthermore, previous researches (Batzner et al., 2022, Christensen and Von Lilienfeld, 2020) have found that models can achieve significantly lower

\begin{table}
\begin{tabular}{l c c c} \hline  & MD17 & revised MD17 & QM9 \\ \hline Train set size & 1000 & 950 & 110000 \\ Val. set size & 1000 & 50 & 10000 \\ batch size & 2 & 2 & 16 \\ warm-up epochs & 25 & 25 & 5 \\ initial learning rate & 0.001 & 0.001 & 0.0005 \\ decay on plateau patience (epochs) & 15 & 15 & 10 \\ decay on plateau cooldown (epochs) & 15 & 15 & 10 \\ decay on plateau threshold & 0.001 & 0.001 & 0.001 \\ decay on plateau factor & 0.7 & 0.7 & 0.5 \\ \hline \end{tabular}
\end{table}
Table 4: Training settings.

energy loss on the revised MD17 dataset than on the original MD17 dataset, while holding similar force accuracy on the two datasets. As analyzed in Batzner et al. (2022), this suggests that the **noise floor** on the original MD17 dataset is higher on the energies, indicating that better force prediction results are more meaningful than energy prediction results on the original MD17 datasets.

**Revised MD17.** For a comprehensive comparison, we also conducted experiments on the revised MD17 dataset, which has **higher data quality** than original MD17, and compared with two state-of-the-art models, MACE (Batatia et al., 2022) and Allegro (Musaelian et al., 2023). The results are shown in Table 5. 2-F-DisGNN achieved 10 best and 4 the second best results out of 20 targets. Note that MACE and Allegro are both models that leverage complex equivariant representations. This further demonstrates the high potential of distance-based models in geometry learning and molecular dynamic simulation.

**Supplementary Results on QM9.** We present the full results on QM9 in Table 6. We compare our model with 7 other models, including those that use invariant geometric features: SchNet (Schutt et al., 2018), DimeNet (Klicpera et al., 2020), DimeNet++(Klicpera et al., 2020), a model that uses group irreducible representations: Cormorant(Anderson et al., 2019), those that use first-order equivariant representations: TorchMD (Tholke and De Fabritiis, 2021), PaiNN (Schutt et al., 2021) and a model that uses local frame methods: GNN-LF (Wang and Zhang, 2022). We calculate the average improvements of all the models relative to DimeNet and list them in the table.

\begin{table}
\begin{tabular}{c c|c c c c c c|c c} \hline Target & Unit & SchNet & Cormor. & DimeNet++ & Torchmd & PaiNN & GNN-LF & DimeNet & 2F-Dis. \\ \hline \(\mu\) & D & 0.033 & 0.038 & 0.0297 & **0.002** & 0.012 & 0.013 & 0.0286 & 0.0100 \\ \(\alpha\) & \(a_{0}^{3}\) & 0.235 & 0.085 & 0.0435 & **0.01** & 0.045 & 0.0353 & 0.0469 & 0.0431 \\ \(\epsilon_{\rm{HOMO}}\) & \(\rm{meV}\) & 41 & 34 & 24.6 & **21.2** & 27.6 & 23.5 & 27.8 & 21.81 \\ \(\epsilon_{\rm{LUMO}}\) & \(\rm{meV}\) & 34 & 38 & 19.5 & 17.8 & 20.4 & **17** & 19.7 & 21.22 \\ \(\Delta\epsilon\) & \(\rm{meV}\) & 63 & 61 & **32.6** & 38 & 45.7 & 37.1 & 34.8 & **31.3** \\ \(\langle R^{2}\rangle\) & \(a_{0}^{2}\) & 0.073 & 0.961 & 0.331 & **0.015** & 0.066 & 0.037 & 0.331 & 0.0299 \\ ZPVE & \(\rm{meV}\) & 1.7 & 2.027 & 1.21 & 2.12 & 1.28 & **1.19** & 1.29 & 1.26 \\ \(U_{0}\) & \(\rm{meV}\) & 14 & 22 & 6.32 & 6.24 & 5.85 & **5.3** & 8.02 & 7.33 \\ \(U\) & \(\rm{meV}\) & 19 & 21 & 6.28 & 6.3 & 5.83 & **5.24** & 7.89 & 7.37 \\ \(H\) & \(\rm{meV}\) & 14 & 21 & 6.53 & 6.48 & 5.98 & **5.48** & 8.11 & 7.36 \\ \(G\) & \(\rm{meV}\) & 14 & 20 & 7.56 & 7.64 & 7.35 & **6.84** & 8.98 & 8.56 \\ \(c_{v}\) & \(\rm{cal/mol/K}\) & 0.033 & 0.026 & 0.023 & 0.026 & 0.024 & **0.022** & 0.0249 & 0.0233 \\ \hline Avg improv. & -78.99\% & -98.22\% & 9.42\% & 25.00\% & 17.50\% & 27.82\% & 0.00\% & 18.83\% \\ Rank & 7 & 8 & 5 & 2 & 4 & 1 & 6 & 3 \\ \hline \end{tabular}
\end{table}
Table 6: MAE loss on QM9.

\begin{table}
\begin{tabular}{c c|c c c} \hline Target & MACE & Allegro & 2F-Dis. \\ \hline aspirin & E & 0.0507 & 0.0530 & **0.0465** \\  & F & 0.1522 & 0.1683 & **0.1515** \\ azobenz. & E & **0.0277** & **0.0277** & 0.0315 \\  & F & 0.0692 & **0.0600** & 0.1121 \\ benzene & E & 0.0092 & 0.0069 & **0.0013** \\  & F & 0.0069 & **0.0406** & 0.0085 \\ ethanol & E & 0.0092 & 0.0092 & **0.0065** \\  & F & 0.0484 & 0.0484 & **0.0379** \\ malonal. & E & 0.0184 & 0.0138 & **0.0129** \\  & F & 0.0945 & 0.0830 & **0.0782** \\ napthal. & E & 0.0115 & **0.0046** & 0.0103 \\  & F & 0.0369 & **0.0208** & 0.0478 \\ paracet. & E & **0.0300** & 0.0346 & 0.0310 \\  & F & **0.1107** & 0.1130 & 0.1178 \\ salicyl. & E & 0.0208 & 0.0208 & **0.0174** \\  & F & 0.0715 & **0.0669** & 0.0860 \\ toluene & E & 0.0115 & 0.0092 & **0.0051** \\  & F & 0.0346 & 0.0415 & **0.0284** \\ uracil & E & **0.0115** & 0.0138 & 0.0131 \\  & F & 0.0484 & **0.0415** & 0.0828 \\ \hline \end{tabular}
\end{table}
Table 5: MAE loss on revised MD17. Energy (E) in kcal/mol, force (F) in kcal/mol/Å.

Although our models exhibit significant improvements on the MD17 datasets, their performance on the QM9 datasets is relatively less remarkable. This discrepancy may be attributed to the fact that the QM9 datasets pose a greater challenge in terms of a model's generalization performance: Unlike MD17, QM9 comprises various kinds of molecules and regression targets. However, since \(k\)-DisGNNs rely purely on the fundamental geometric feature of distance, they may inherently require a larger amount of data to learn the significant geometric patterns within geometric structures and enhance their generalization performance, compared to models that explicitly incorporate equivariant representations or pre-calculated high-order geometric features. Nevertheless, we believe that such learning paradigm, which _employs highly expressive models to extract information from the most basic components in a data-driven manner_, has immense performance potential and will exhibit even greater performance gains when provided with more data points, increased model scales, better data quality or advanced pre-training methods, as already validated in fields such as natural language processing (Brown et al., 2020; Devlin et al., 2018), computer vision (Tolstikhin et al., 2021; Liu et al., 2021; Dosovitskiy et al., 2020), and molecule-related pre-training methods (Xia et al., 2023; Lu et al., 2023).

### Time and Memory Consumption

We performed experiments on the MD17 dataset to compare the training and inference time, as well as the GPU memory consumption of our models with their counterparts DimeNet and GemNet. The experiments are conducted on Nvidia RTX 3090, and the results are shown in Table 7.

2-F-DisGNN demonstrates significantly better time and memory efficiency compared to DimeNet and GemNet. Even when compared to the quite efficient model, TorchMD, 2-F-DisGNN shows competitive efficiency. This is partly because it utilizes high-order tensors and employs dense calculation, resulting in significant acceleration on GPUs. However, due to the high theoretical complexity and high hidden dimension, 3-E-DisGNN shows relatively worse performance. Regarding this limitation, we have listed several possible solutions in Section 8 and recent work on simplifying and accelerating \(k\)-WL (Zhao et al., 2022; Morris et al., 2022) when \(k\) is large may also be applied, which is left for future work. Nonetheless, given that 2-F-DisGNN already satisfies the requirements for both theoretical (Section 6.3) and experimental (Section 7) performance, our primary focus lies on this model, and the analysis further emphasizes the practical potential of 2-F-DisGNN.

## Appendix E Supplementary Related Work

There is a wealth of research in the field of interatomic potentials, which introduced various methods for modeling the atomic environment as representations (Bartok et al., 2010, 2013; Shapeev, 2016; Behler and Parrinello, 2007). In particular, ACE (Atomic Cluster Expansion) (Drautz, 2019) has presented a framework that serves as a unifying approach for these methods and can calculate high-order complete polynomial basis features with small cost regardless of the body order. Building on Drautz (2019); Batatia et al. (2022) proposed a GNN variant called MACE that can effectively exploit the rich geometric information in atoms' local environment. Additionally, Joshi et al. (2023) proposed

\begin{table}
\begin{tabular}{c|c c c|c c} \hline  & 2FDis. & DimeNet & TorchMD & 3EDis. & GemNet \\ \hline Asp. & 234/66/2985 & 270/74/5691 & 101/39/2072 & 873/273/18585 & 556/182/7169 \\ Ben. & 92/24/1058 & 162/44/1815 & 110/41/905 & 224/61/3644 & 380/130/1789 \\ Eth. & 61/18/637 & 163/43/791 & 103/43/532 & 112/30/1561 & 462/158/804 \\ Mal. & 61/18/640 & 161/43/791 & 103/42/532 & 113/30/1561 & 356/118/804 \\ Nap. & 184/50/2193 & 238/68/4578 & 111/45/1692 & 652/185/11804 & 444/141/5253 \\ Sal. & 137/37/1758 & 206/58/3468 & 119/42/1401 & 458/127/8281 & 405/136/3778 \\ Tol. & 131/36/1560 & 200/54/3126 & 107/25/1326 & 399/109/6915 & 473/153/3328 \\ Ura. & 93/24/1058 & 169/45/1782 & 113/42/906 & 225/61/3644 & 369/136/1776 \\ \hline Avg. & 124/34/1486 & 196/53/2755 & 108/39/1170 & 382/109/6999 & 430/144/3087 \\ \hline \end{tabular}
\end{table}
Table 7: Training time, inference time and GPU memory consumption on MD17. The batch size is 32 for 2-F-DisGNN, DimeNet and TorchMD, and 12 for 3-E-DisGNN and GemNet. Training time in ms, inference GPU memory consumption in MB. Evaluated on Nvidia A100.

geometric variants of the WL test to characterize the expressiveness of invariant and equivariant geometric GNNs for the general graph setting. All of Drautz (2019); Batatia et al. (2022); Joshi et al. (2023) leverage the many-body expansion and can include \(k\)-tuples during embedding atom's neighbors, allowing atoms to obtain \(k\)-order geometric information from their local environments like \(k\)-DisGNNs.

However, we note that there are key differences between these works (based on many-body expansion) and \(k\)-DisGNNs in terms of their message passing units and use of representations. The former assumes that the total energy can be approximated by the sum of energies of atomic environments of individual atoms and therefore use atoms as their message passing units, while \(k\)-DisGNNs pass messages among \(k\)-tuples and pool tuple representations to obtain global representation. In addition, while interatomic potential methods leverage equivariant representations to enhance their expressiveness, \(k\)-DisGNNs completely decouple E(3)-symmetry and only use the most basic geometric invariant feature, distance, to achieve universality and good experimental results.