# Adaptive Experimentation When You Can't Experiment

 Yao Zhao

University of Arizona

yaoz@arizona.edu

&Kwang-Sung Jun

University of Arizona

kjun@cs.arizona.edu

&Tanner Fiez

Amazon

fieztann@amazon.com

&Lalit Jain

University of Washington

lalitj@uw.edu

###### Abstract

This paper introduces the _confounded pure exploration transductive linear bandit_ (CPET-LB) problem. As a motivating example, often online services cannot directly assign users to specific control or treatment experiences either for business or practical reasons. In these settings, naively comparing treatment and control groups that may result from self-selection can lead to biased estimates of underlying treatment effects. Instead, online services can employ a properly randomized encouragement that incentivizes users toward a specific treatment. Our methodology provides online services with an adaptive experimental design approach for learning the best-performing treatment for such _encouragement designs_. We consider a more general underlying model captured by a linear structural equation and formulate pure exploration linear bandits in this setting. Though pure exploration has been extensively studied in standard adaptive experimental design settings, we believe this is the first work considering a setting where noise is confounded. Elimination-style algorithms using experimental design methods in combination with a novel finite-time confidence interval on an instrumental variable style estimator are presented with sample complexity upper bounds nearly matching a minimax lower bound. Finally, experiments are conducted that demonstrate the efficacy of our approach.

## 1 Introduction

In this study, we present a methodology for adaptive experimentation in scenarios characterized by potential confounding. Online services routinely conduct thousands of A/B tests annually [23]. In most online A/B/N experimentation, meticulous user-level randomization is essential to ensure unbiased estimates of _treatment effects at the population level_, commonly known as average treatment effects (ATE). In this setting, firms are often interested in understanding the treatment with the highest average outcome if presented to all members of the population. However, in many settings firms may not be able to randomize, for example if a feature must be rolled out to all users for various business reasons [30]. In such instances, users may choose to engage with a feature or not based on potentially unobservable preferences. Thus the resulting measured outcome may be correlated with the decision to engage in a specific feature. I.e. the underlying characteristics of the user _confound_ the relationship between the decision to use the feature being evaluated and the effect of the feature. Thus, naively comparing the average outcome for users who engage with a feature with those who do not suffers from a (selection) bias. This setting is captured in Figure 1.

A potential solution is for services to employ _encouragement designs_ where users are presented with incentives that encourage users to engage with a specific feature [4, 7, 12]. As a concrete example, many online services have introduced membership levels with different offerings andprices available to all users. Given a set of membership level options, the service is interested in knowing the counterfactual of which level has the optimal outcome (e.g., total revenue) if every user chooses to join that membership level. In this setting, encouragements could be coupons or trials for corresponding membership levels. In these settings, the firm can use intent-to-treat (ITT) estimates for the treatment effect which naively compare the average outcomes between the groups given different encouragements. In practice, given an encouragement a user may not engage with the corresponding feature choosing either the control or a different feature. Hence, the resulting ITT estimate may be a diluted estimate of the ATE [3]. However, all is not lost: if the encouragement presented to a user is properly randomized, and the service guarantees that the encouragement only affects the outcome through the choice of user treatment, then the encouragement acts as an _instrumental variable_. Standard analysis from the econometrics and compliance literature show that two-stage least squares (2SLS) estimators can then be used to provide consistent estimates of treatment effects.

At the same time, firms are also increasingly utilizing adaptive experimentation techniques, often known as _pure exploration multi-armed bandits_ (MAB) algorithms [26, 15], to accelerate traditional A/B/N testing. Pure exploration MAB techniques promise to deliver accurate inferences in a fraction of the time and cost as traditional methods. Similar to A/B testing, bandit methods assume users are properly randomized and can fail to learn the optimal treatment if naively used and may be sample inefficient if they fail to take the confounded structure into account.

**Contributions.** In this work, we provide a methodology for experimenters seeking to use adaptive experimentation in settings with confounding where encouragements are available. We formulate this work in the more general and novel setting of _confounded pure exploration transductive linear bandits_ (CPET-LB) (Section 1.1). We present algorithms using experimental design for the CPET-LB problem and analyze the resulting sample complexity. As we demonstrate, even in the simple multi-armed bandit setting described above, computing an effective sampling pattern requires using the machinery of linear bandits. Without knowledge of the underlying structural model, existing linear bandit approaches could lead to suboptimal sampling. The main technical challenge we face is simultaneously improving our estimate of the structural model while designing with inaccurate estimates (Section 3). This approach crucially relies on our development of novel finite-time confidence bounds for two-stage least squares (2SLS) style estimators that may be of independent interest (Section 2.2). Moreover, we provide worst-case sample complexity lower bounds that are nearly matched by our sample complexity upper bounds (Appendix D). Though the goal of this work is primarily theoretical, we empirically show the efficacy of our method over existing solutions (Appendix E).

### General Problem Formulation

A confounded pure exploration transductive linear bandits (CPET-LB) instance consists of finite collections of measurement vectors \(\mathcal{Z}\subset\mathbb{R}^{d}\) and evaluation vectors \(\mathcal{W}\subset\mathbb{R}^{d}\). At each time \(t\in\mathbb{N}\), the learner selects \(z_{t}\in\mathcal{Z}\) and observes a pair of noisy responses \(x_{t}\in\mathcal{X}\subseteq\mathbb{R}^{d}\) and \(y_{t}\in\mathbb{R}\) generated via the structural equation model

\[x_{t}=\Gamma^{\top}z_{t}+\eta_{t},\quad y_{t}=x_{t}^{\top}\theta+ \varepsilon_{t}, \tag{1}\]

where \(\Gamma\in\mathbb{R}^{d\times d}\) and \(\theta\in\mathbb{R}^{d}\) are model parameters.1 We define the history \(\mathcal{H}_{t-1}=\{(z_{s},x_{s},y_{s})\}_{s<t}\) and \(\mathbb{E}_{t-1}[\cdot]=\mathbb{E}[\cdot|\mathcal{H}_{t-1}|]\) denoting the conditional expectation under the filtration generated by \(\mathcal{H}_{t-1}\). The noise \(\{\eta_{t}\}_{t=1}^{\infty}\) and \(\{\varepsilon_{t}\}_{t=1}^{\infty}\) satisfy the following set of assumptions unless otherwise noted.

Footnote 1: We assume throughout that \(\Gamma\in\mathbb{R}^{d\times d}\) is an invertible matrix.

**Assumption 1**.: We assume \(\varepsilon_{t}\mid\mathcal{H}_{t-1}\) is 1-sub-Gaussian (and thus \(\mathbb{E}[\varepsilon_{t}\mid\mathcal{H}_{t-1}]=0\)). Furthermore, \(\eta_{t}\mid\mathcal{H}_{t-1}\) is \(\sigma_{\eta}^{2}\)-sub-Gaussian vectors (and thus \(\mathbb{E}[\eta_{t}\mid\mathcal{H}_{t-1}]=0\)), i.e.,

\[\forall\beta\in\mathbb{R},\max_{a:\|a\|_{2}\leq 1}\mathbb{E}[\exp\bigl{(}\beta \langle a,\eta_{t}\rangle\bigr{)}]\leq\exp\left(\frac{\beta^{2}\sigma_{\eta}^ {2}}{2}\right)\;.\]

Figure 1: Causal graph of the model.

**Goal.** The objective is to identify \(w^{*}:=\arg\max_{w\in\mathcal{W}}w^{\top}\theta\) with probability at least \(1-\delta\) for \(\delta\in(0,1)\) while taking a minimum number of measurements.

In the setting where \(\Gamma=I,\eta=0\) and \(\mathbb{E}_{t-1}[\varepsilon_{t}|x_{t}]=0\), our setting reduces to the standard _pure exploration transductive linear bandit_ problem [33, 15]. In general, the joint noise process \(\big{[}\eta_{t},\varepsilon_{t}\big{]}\) may be dependent across the entries. In particular, we are allowing for the data generating process to be _endogenous_, meaning that \(\mathbb{E}_{t-1}[\varepsilon_{t}|x_{t}]\neq 0\). That is, \(\varepsilon_{t}\) can affect not just \(y_{t}\), but also \(x_{t}\) given a choice of \(z_{t}\). The presence of endogeneity is a key challenge in the CPET-LB problem.

**Assumption 2** (Exclusion Restriction).: We assume that \(\mathbb{E}_{t-1}[z_{t}\varepsilon_{t}]=0\), or alternatively that \(z_{t}\) is uncorrelated with \(\varepsilon_{t}\).

The variable \(z_{t}\) is commonly referred to as an _instrumental variable_[3]. We consider algorithms for the CPET-LB problem that stop at a \(\mathcal{H}_{t}\)-measurable time \(\tau\in\mathbb{N}\), and produce a recommendation \(\widehat{w}\in\mathcal{W}\). The goal is \(\delta\)-PAC algorithms with efficient sample complexity guarantees.

**Definition 1.1** (\(\delta\)-Pac).: We say an algorithm is \(\delta\)-PAC for a CPET-LB problem with \(\mathcal{W},\mathcal{Z}\subset\mathbb{R}^{d}\) if for all \(\theta\in\mathbb{R}^{d}\) and \(\Gamma\in\mathbb{R}^{d\times d}\), it holds that \(\mathbb{P}_{\theta,\Gamma}(\widehat{w}\neq w^{*})\leq\delta\) for \(\delta\in(0,1)\).

### Encouragement Designs

The CPET-LB feedback model generalizes the classical _compliance_ setting.

**Compliance as a Special Case.** In _compliance_ problems, a decision-maker has access to a set of treatment that can be offered to users, while the users themselves have the option to accept the treatment they are presented or instead opt-in to a different treatment. The goal is to identify the treatment with the optimal average outcome if all users were to accept it. Specifically, given a finite set \(\mathcal{A}=\{1,2,\ldots,d\}\), a decision-maker presents user \(t\in\mathbb{N}\) with an encouragement for a treatment \(i\in\mathcal{A}\), the user then selects treatment \(j\in\mathcal{A}\) where potentially \(j\neq i\), and an outcome \(y_{t}\) results. To capture compliance with the CPET-LB framework, we set \(\mathcal{Z}=\mathcal{X}=\mathcal{W}=\{e_{1},\cdots,e_{d}\}\) and the parameter \(\Gamma\) captures the probability of accepting a treatment given an encouragement for a potentially different treatment. Specifically, \(\Gamma(i,j)=\mathbb{P}\big{(}x_{t}=e_{i}\mid z_{t}=e_{j}\big{)}\), and a straightforward computation shows that \(x_{t}=\Gamma^{\top}z_{t}+\eta_{t}\) where \(\mathbb{E}[\eta_{t}|z_{t}]=0\) with

\[\eta_{t}=x_{t}-\Big{[}\mathbb{P}\big{(}e_{1}\mid z_{t}\big{)},\cdots,\mathbb{P }\big{(}e_{d}\mid z_{t}\big{)}\Big{]}^{\top}. \tag{2}\]

Moreover, \(y_{t}=x_{t}^{\top}\theta+\varepsilon_{t}\) gives the resulting reward, which is clearly correlated with the user choice so that \(\text{cov}(\eta_{t},\varepsilon_{t})\neq 0\). Finally, \(e_{i}^{\top}\theta=\theta_{i}\) gives the expected value of treatment \(i\) over the population and our goal is to identify \(w^{*}=\arg\max_{e_{i}\in\mathcal{W}}e_{i}^{\top}\theta\). 2 Note that when \(\Gamma=I\), we automatically have that \(\eta_{t}=0\) and there is no confounding. This reduces to the standard MAB setting.

Footnote 2: Our setting differs slightly from the traditional compliance setting based on a potential outcomes framework [3]. Our setting is equivalent to one where we assume that there is a constant treatment effect. See Chapter 4 of [3] for further discussion of the differences. Thus in our setting, learning \(\theta_{i}\) and the local average treatment effect (LATE) on compilers are the same.

**Motivating Compliance Example.** As a motivating compliance example representing the membership level discussion from the introduction, consider a location model that assumes each user \(t\in\mathbb{N}\) arriving online has an underlying unobserved one-dimensional preference \(u_{t}\sim\mathcal{N}(0,\sigma_{u}^{2})\). If an algorithm presents the user with encouragement \(z_{I_{t}}=e_{I_{t}}\) for \(I_{t}\in\mathcal{A}\), then the user selects into the membership level given by \(J_{t}=\min_{j\in\mathcal{A}}|I_{t}+u_{t}-j|\) so that \(x_{t}=e_{J_{t}}\). This process captures a user being more likely to opt-in to membership levels that are closer to the encouragement that they were presented. The outcome is then given by \(y_{t}=x_{t}^{\top}\theta+u_{t}\).

We conduct an experiment with this problem instance (see Fig. 2 and Appendix B for more details). Specifically, \(d=6\), \(\theta=\begin{bmatrix}1&-0.95&0&0.45&0.95&0.99\end{bmatrix}\), and \(\sigma_{u}^{2}=0.35\). Observe that \(w^{*}=e_{1}=\arg\max_{w\in\mathcal{W}}w^{\top}\theta\). An upper confidence bound (UCB) selection strategy is simulated that maintains estimates of the mean reward of each encouragement \(i\in\mathcal{A}\), namely \(\widehat{\mu}_{i,t}=\sum_{s=1}^{t}\mathbf{1}\{z_{t}=e_{i}\}y_{t}\), and then pulls the one with the highest UCB. The UCB selection strategy is combined with a pair of recommendation strategies. The UCB-OLS algorithm estimates the mean reward of each treatment using an OLS estimator, namely \(\widehat{\theta}_{\text{LS}}^{\,t}=\sum_{s=1}^{t}\mathbf{1}\{x_{t}=e_{i}\}y_ {t}/\sum_{s=1}^{t}\mathbf{1}\{x_{t}=e_{i}\}\), and recommends \(\arg\max_{a\in\mathcal{A}}\widehat{\theta}_{\text{LS}}^{\,t}\). Moreover, the UCB-IV algorithm uses an instrumental variable-estimator (see the next section) that incorporates knowledge of \(\Gamma\) similar to 2SLS to deconfouestimates of the mean rewards of each treatment and recommends the treatment with the maximum estimate. The results over 100 simulations are shown in Fig. 2. UCB-OLS completely fails to identify \(\theta_{1}=\arg\max_{i\in d}\theta_{i}\) due to a biased estimate, whereas UCB-IV does better. However, UCB-IV methods seem to have a constant probability of error. To see why, note that the expected reward from pulling \(z=e_{i}\) is \(e_{i}^{\top}\Gamma\theta\). These values are plotted in orange in Figure 1(a). In particular, with some constant probability, UCB zeroes in on arm 6 becauses of the mean estimates on the \(z\)'s, and as a result fails to give enough samples to learn that arm 1 is indeed the best. In contrast, our proposed method CPEG, Algorithm 1 manages to find the best arm with significantly higher probability.

**Notation.** Let \(\Delta(\mathcal{Z})=\{\lambda\in\mathbb{R}^{|\mathcal{Z}|}:\lambda\geq 0,\sum_{z \in\mathcal{Z}}\lambda_{z}=1\}\) denote the set of probability distributions over the set \(\mathcal{Z}\). Given a distribution \(\lambda\in\Delta(\mathcal{Z})\) and matrix \(\Gamma\in\mathbb{R}^{d\times d}\), define the operator \(A(\lambda,\Gamma):=\sum_{z\in\mathcal{Z}}\lambda_{z}\Gamma^{\top}zz^{\top}\Gamma\). Given \(Z\in\mathbb{R}^{T\times d}\) and \(\Gamma\in\mathbb{R}^{d\times d}\), define the operator \(\bar{A}(Z,\Gamma):=\sum_{t=1}^{T}\Gamma^{\top}z_{t}z_{t}^{\top}\Gamma=\Gamma^ {\top}Z^{\top}Z\Gamma\) where \(z_{t}\in\mathbb{R}^{d}\) denotes row \(t\) of \(Z\). Given a vector \(x\in\mathbb{R}^{d}\) and a symmetric positive-definite matrix \(A\in\mathbb{R}^{d\times d}\) we let \(\|x\|_{\mathcal{Z}}^{2}=x^{\top}Ax\). We adopt the standard notation that \((a\lor b)\equiv\max\{a,b\}\) and \((a\wedge b)\equiv\min\{a,b\}\) for \(a,b\in\mathbb{R}\). \(\sigma_{\min}(A),\sigma_{\max}(A)\) denote the minimum and maximum singular value of a matrix \(A\). We denote by \(\text{polylog}(x_{1},\ldots,x_{n})\) any polylogarithmic factors of \(x_{1},\ldots,x_{n}\).

### Related Works

Our work is at the intersection of several parallel tracks of literature, pure exploration linear bandits, causal bandits, and econometrics. The most relevant work on pure exploration in linear bandits is the RAGE algorithm of [15; 33]. RAGE is nearly instance optimal for linear bandits in the non-confounded setting. Extensions of RAGE to various noise models including logistic and heteroskedastic noise have been considered [35; 20]. Other algorithms for pure exploration linear bandits have been proposed - and we leave it for future work to extend the ideas of this paper to those settings [27; 10].

Confounding in bandits was first considered in the regret minimization setting by [5]. They introduces the Multi-armed bandit with unobserved confounders (MABUC) problem. They empirically demonstrate traditional bandit algorithms can have linear regret in this setting and provide an algorithm that effectively employs observed intuition. The early work of [21] also assumes there is an additional unobserved latent class at each time that determines confounding in a compliance setting. They provide novel notions of regret, relative to the instrument with the highest reward (\(\arg\max Z^{\top}\Gamma^{\top}\theta\) in our notation), the highest treatment (\(\arg\max_{w}w^{\top}\theta\)), regret relative to the best latent class at each time, and regret on the set of "compliers". They discuss the suitability of these various notions of regret, and discuss when sublinear regret is possible. We remark that their approach is similar to ours in the sense that they assume a form of homogeneous effects across the population, and use an estimate of \(\Gamma\). Recently [24] also consider the problem of compliance, however they don't take explicit non-confounding into account and assume an explicit parametric model that determines the non-compliance. This is analogous to the Heckman selection model considered in econometrics [18].

The recent works of [11; 36; 17] considered an online setting where at each time they observe a set \(\{(x_{t},z_{t})\}\) where \(x_{t}\) is the action of interest and \(z_{t}\) is an associated instrument. If action \(I_{t}\) is selected, the reward observed is \(y_{t}=x_{I_{t}}^{\top}\theta_{t}+\varepsilon_{t}\), where \(x_{t}\) may be endogeneous. Similar to the standard linear bandit setting [1; 26], the goal is to minimize regret relative to the best action at each time. We remark that this setting is very different from ours. Effectively, we are choosing which instrument to select at each time to learn the best-performing treatment - in particular we can't choose a particular intervention. In their setting, they are choosing an intervention at each time and using the instrument purely for de-confounding the result. Experimental design for instruments to have more effective estimation has been considered by [8].

In the causal bandit problem, an underlying causal graph between a set of interventions and a reward value is assumed. Actions correspond to intervening (i.e. a "do" operation [31]) at one or more specific nodes in the causal graph and then observing the corresponding value at the reward node. Causal bandits have been studied extensively in the regret setting [25; 28; 6] and the pure exploration setting [32]. Though past works have allowed for unobserved confounders in the graph e.g. [29], their goal is to learn the best performing intervention, which in our setting would be \(\arg\max_{z\in Z}z^{\top}\Gamma\theta\) instead of \(w^{*}\).

Encouragement designs have been considered in many applications in online and offline settings. One of the earliest works on encouragement designs is [7], which considers the problem of using encouragements to determine the impact of coupons at a grocery store. More recent applications include [4; 30; 13] all in the context of online services and treatments that are required to be served to all users. Most of these works consider a small number of treatments and a heterogeneous treatment effect - hence are interested in LATE estimator. As far as we are aware, we are the only work that considers adaptive encouragement design in the context of the model given in Equation 1 and for multiple treatments.

## 2 Estimators and Inference

We now present estimators for the unknown parameter \(\theta\) and prove the associated statistical properties. The estimators discussed in this section are critical to our algorithmic solution outlined in Section 3.

### Estimators

Before describing our solution concept, we quickly review potential options for estimating \(\theta\) based on a dataset \(Z_{T}=[z_{1},\cdots,z_{T}]^{\top}\in\mathbb{R}^{T\times d},X_{T}=[x_{1}, \cdots,x_{T}]^{\top}\in\mathbb{R}^{T\times d}\), \(Y_{T}=[y_{1},\cdots,y_{T}]^{\top}\in\mathbb{R}^{T}\), assumed to be generated according to the model in Eq. (1). Recall that the _ordinary-least-squares_ (OLS) estimator for \(\theta\) is given by

\[\widehat{\theta}_{\texttt{LS}}:=\arg\min_{\widehat{\theta}\in\mathbb{R}^{d}} \sum_{t=1}^{T}(y_{t}-x_{t}^{\top}\widehat{\theta})^{2}=(X_{T}^{\top}X_{T})^{- 1}X_{T}^{\top}Y_{T}=\theta+(X_{T}^{\top}X_{T})^{-1}X_{T}^{\top}\varepsilon_{T}. \tag{3}\]

Observe that \(\widehat{\theta}_{\texttt{LS}}\) is potentially a biased and inconsistent estimator for \(\theta\) in the presence of endogenous noise since \(\mathbb{E}[\varepsilon_{t}|x_{t}]\neq 0\). To remediate this problem, we define a general class of estimators that includes several standard estimators. Given an invertible matrix \(\Psi\in\mathbb{R}^{d\times d}\), let \(\bar{X}_{T}:=Z_{T}\Psi\), and consider corresponding estimators termed \(\Psi\)-IV estimators of the form

\[\widehat{\theta}_{\Psi}:=(\bar{X}_{T}^{\top}\bar{X}_{T})^{-1}\bar{X}_{T}^{\top }Y_{T}=(\Psi^{\top}Z_{T}^{\top}Z_{T}\Psi)^{-1}\Psi^{\top}Z_{T}^{\top}Y_{T}=(Z_ {T}^{\top}Z_{T}\Psi)^{-1}Z_{T}^{\top}Y_{T}. \tag{4}\]

When \(\Psi=I\) we recover the OLS estimator. In the rest of the paper, we will focus on two different potential options for \(\Psi\).

**Case 1:** Oracle. \(\Psi=\Gamma\). To begin, observe that the structural equation model from Eq. (1) can be combined by substituting the second equation into the first to obtain the reduced form

\[y_{t}=z_{t}^{\top}\Gamma\theta+\eta_{t}^{\top}\theta+\varepsilon_{t}. \tag{5}\]

Since \(z_{t}\) is independent of the i.i.d. process \(\eta_{t}^{\top}\theta+\varepsilon_{t}\), the least squares estimator which regresses \(y_{t}\) onto \(z_{t}^{\top}\Gamma\) is unbiased for estimation of \(\theta\) and given by

\[\widehat{\theta}_{\texttt{oracle}}=(\bar{X}_{T}^{\top}\bar{X}_{T})^{-1}\bar{ X}_{T}^{\top}Y_{T}=(Z_{T}^{\top}Z_{T}\Gamma)^{-1}Z_{T}^{\top}Y_{T}. \tag{6}\]

This estimator will be used to design our general solution concept presented in Section 3. Of course in practice we cannot expect to know \(\Gamma\), but we may be able to estimate it.

**Case 2:**P-2SLS, \(\Psi=\widehat{\Gamma}\).** We consider a setting where \(\widehat{\Gamma}\) is an (unbiased) estimator of \(\Gamma\), learned using least-squares from an _independent_ dataset \(Z_{T_{1}}=[z_{1}^{\prime},\cdots,z_{T_{1}}^{\prime}],X_{T_{1}}=[z_{1}^{\prime}, \cdots,z_{T_{1}}^{\prime}]\) collected non-adaptively.3 That is, \(\widehat{\Gamma}=(Z_{T_{1}}^{\top}Z_{T_{1}})^{-1}Z_{T_{1}}^{\top}X_{T_{1}}\) and:

Footnote 3: Formally, we say that a set of data \((Z_{T},X_{T},Y_{T})\) generated via the model in Eq. (1) is collected non-adaptively from an experimental design if \(z_{t}\) is \(\mathcal{H}_{0}\) measurable for all \(1\leq t\leq T\).

\[\widehat{\theta}_{\texttt{P-2SLS}}=(\widehat{\Gamma}^{\top}Z_{T}^{\top}Z_{T} \widehat{\Gamma})^{-1}\widehat{\Gamma}^{\top}Z_{T}^{\top}Y_{T}.\]

We refer to the resulting estimator as a pseudo two stage least squares (P-2SLS) estimator. The main advantage of the P-2SLS estimator over standard 2SLS (given in Appendix C) is easier inference since now \(\{\varepsilon_{t}\}_{t\leq T}\) of our dataset is independent of the measurements of the first dataset \(Z_{T_{1}},X_{T_{1}}\). In the econometrics literature, such an estimator is referred to as a _two-sample 2SLS estimator_[19].

### Confidence Intervals

In the section that follows, we develop a general algorithmic approach that relies on _experimental design_ aimed at reducing the uncertainty in our estimates of the optimal treatment. To this end, we first develop finite-time confidence intervals for estimators presented in the previous section given data generated according to the model in Eq. (1) and collected from non-adaptive designs.

We begin by characterizing the properties of the noise structure in the combined model of Eq. (5) with the following set of results.

**Lemma 2.1**.: _Under Assumption 1, the noise process \(\nu:=\eta^{\top}\theta+\varepsilon\) is \(\sigma_{\nu}^{2}\)-sub-Gaussian where \(\sigma_{\nu}^{2}=2(\sigma_{\eta}^{2}\|\theta\|_{2}^{2}+1)\), specifically when the instance is compliance, \(\sigma_{\nu}^{2}=2(4\|\theta\|_{2}^{2}+1)\)._

**Oracle Confidence Interval**. As in the last section, we assume that we have access to a dataset \((Z_{T},X_{T},Y_{T})\) generated according to Eq. 1 and collected non-adaptively. Given Lemma 2.1, it can be shown that \(w^{\top}\widehat{\theta}_{\texttt{oracle}}\) is a sub-Gaussian random variable satisfying the following.

**Lemma 2.2**.: _With probability at least \(1-\delta\) for \(\delta\in(0,1)\) and \(w\in\mathbb{R}^{d}\),_

\[|w^{\top}(\widehat{\theta}_{\texttt{oracle}}-\theta)|\leq\sqrt{2\sigma_{\nu}^ {2}\|w\|_{\bar{A}(Z_{T},\Gamma)^{-1}}^{2}\log\bigl{(}2/\delta\bigr{)}},\]

_where \(\sigma_{\nu}^{2}\) is the sub-Gaussian parameter of the noise \(\nu:=\eta^{\top}\theta+\varepsilon\) characterized in Lemma 2.1._

The proof of this result is in Appendix G.2.

**P-2SLS Confidence Interval**. We now present a novel finite-time confidence interval for the P-2SLS estimator. As discussed in the previous section with respect to this estimator, we assume access a set of data \((Z_{T_{1}},X_{T_{1}})\) generated according to Eq. (1) and collected non-adaptively for the purpose of estimating \(\Gamma\). Moreover, assume access to a separate set of data \((Z_{T_{2}},X_{T_{2}},Y_{T_{2}})\) generated according to Eq. (1) and collected non-adaptively for the purpose of estimating \(\theta\).

**Theorem 2.3**.: _Suppose that \(\widehat{\Gamma}=(Z_{T_{1}}^{\top}Z_{T_{1}})^{-1}Z_{T_{1}}^{\top}X_{T_{1}}\) and \(\widehat{\theta}_{\texttt{P-2SLS}}=(\widehat{\Gamma}^{\top}Z_{T_{2}}Z_{T_{2}} \widehat{\Gamma})^{-1}\widehat{\Gamma}Z_{T_{2}}^{\top}Y_{T_{2}}\). Then, for any \(w\in\mathcal{W}\), with probability at least \(1-\delta\) for \(\delta\in(0,1)\),_

\[|w^{\top}(\widehat{\theta}_{\texttt{P-2SLS}}-\theta)|\leq\|w\|_{\bar{A}(Z_{T _{2}},\widehat{\Gamma})^{-1}}\sqrt{2\sigma_{\nu}^{2}\log\biggl{(}\frac{4}{ \delta}\biggr{)}}+\|w\|_{\bar{A}(Z_{T_{1}},\widehat{\Gamma})^{-1}}\|\theta\|_{ 2}\sqrt{\sigma_{\eta}^{2}\overline{\log\bigl{(}Z_{T_{1}},\delta/4\bigr{)}}},\]

_where_

\[\overline{\log}(Z_{T},\delta):=8d\ln\left(1+\frac{2TL_{z}^{2}}{d(2\wedge\sigma _{\min}(Z_{T_{1}}^{\top}Z_{T_{1}}))}\right)+16\ln\left(\frac{2\cdot\!\delta^{ d}}{\delta}\cdot\log_{2}^{2}\left(\frac{4}{2\wedge\sigma_{\min}(Z_{T_{1}}^{\top}Z_{T_{1} })}\right)\right).\]

The proof is presented in Appendix G.3. Observe that the first term in the P-2SLS estimator confidence interval given by \(\sqrt{2\sigma_{\nu}^{2}\|w\|_{\bar{A}(Z_{T_{2}},\widehat{\Gamma})^{-1}}^{2}\log \bigl{(}4/\delta\bigr{)}}\) matches the Oracle estimator confidence interval in Lemma 2.2 when \(\widehat{\Gamma}=\Gamma\). The second term scaling like \(\mathcal{O}(\|w\|_{\bar{A}(Z_{T_{2}},\widehat{\Gamma})^{-1}}\|\theta\|_{2} \sigma_{\eta}\sqrt{d+\log\bigl{(}1/\delta\bigr{)}})\), is an upper bound on the approximation error \(w^{\top}(\widehat{\Gamma}^{-1}\Gamma-I)\theta\) for any \(w\in\mathbb{R}^{d}\), assuming that \(\widehat{\Gamma}\) is learned from an OLS estimator (see Theorem G.3 for details).

We will see that the form of this confidence interval is particularly convenient for our algorithmic approach given in Section 3. In particular, the form of the variance \(\|w\|_{A(Z_{T_{2}},\Gamma)^{-1}}^{2}\) on the first term only depends on a design over instruments. Thus, we can choose an experimental design over \(Z\)'s which reduces this variance optimally.

_Remark 2.4_.: In practice we expect the first stage of samples, \((Z_{T_{1}},X_{T_{1}})\) to be collected from either a burn-in period or from existing historical data. We remark that assuming two stages of samples is common in the orthogonal and double machine learning for estimating nuisance parameters in the data generating process (e.g. \(\Gamma\)) [9]. Our result matches the existing literature on the asymptotic variance of two sample 2SLS estimators (e.g., Theorem 1 of [19]).

_Remark 2.5_.: The asymptotic variance of standard 2SLS is known to involve a factor \(\sigma_{\varepsilon}^{2}\), instead of \(\sigma_{\nu}^{2}\) as we have [18]. Recent work by [11] shows a variance involving \(d\sigma_{\varepsilon}^{2}\). However, it's unclear how to use the form of their confidence interval directly for experimental design. In addition, their work is not sufficiently general to handle the general forms of noise that we consider in Lemma 2.1.

## 3 Adaptive Experimental Design Algorithms

We now present adaptive experimental design algorithms for the CPET-LB problem. Our main insight utilizes Eq. 1 by plugging the model for \(x\) into the top equation resulting in the relationship

\[y=z^{\top}\Gamma\theta+\theta^{\top}\eta+\varepsilon.\]

When \(\Gamma\) is known, by Eq. 5, we see that CPET-LB reduces to a standard pure exploration transductive linear bandit problem where the measurement set is given by \(\{\Gamma^{\top}z\}_{z\in\mathcal{Z}}\subset\mathbb{R}^{d}\), the evaluation set is \(\mathcal{W}\subset\mathbb{R}^{d}\), and the feedback model is given by \(y=v^{\top}\theta+\nu\) where the noise \(\nu=\theta^{\top}\eta+\varepsilon\) is sub-Gaussian and as before the goal is to identify \(\arg\max_{w\in\mathcal{W}}w^{\top}\theta\). An existing approach to this problem is given by the RAGE algorithm [15], which we use as the basis of our approach. Addressing the case of unknown \(\Gamma\) is our major algorithmic contribution, where we develop solutions to improve our estimate of \(\Gamma\) and learn \(w^{*}\) simultaneously. As a warm-up to this approach, we first consider the setting when \(\Gamma\) is known.

### Warm-Up: Known Structural Model

Algorithm 1 assumes a parameter \(L_{\nu}\), which acts as an upper bound on the sub-Gaussian constant of the noise \(\nu=\theta^{\top}\eta+\varepsilon\). In each round \(k\), an active set of potentially optimal vectors \(\widehat{\mathcal{W}}_{k}\subset\mathcal{W}\) is maintained. CPEG aims to sample in such a way that reduces the uncertainty of the estimates on the _gaps_\((w-w^{\prime})^{\top}\theta\) for each pair \(w,w^{\prime}\in\widehat{\mathcal{W}}_{k}\) maximally each round. In any given round the algorithm takes \(N_{k}\) samples \(Z_{N_{k}}\), the confidence interval of Lemma 2.2 shows that the error in estimating \((w-w^{\prime})^{\top}\theta\) scales with \(\|w-w^{\prime}\|_{[\Gamma^{\top}Z_{N_{k}}^{\top}Z_{N_{k}}\Gamma)^{-1}}^{2}\). This motivates utilizing an _experimental design_ approach where we choose a distribution \(\lambda_{k}\in\Delta(\mathcal{Z})\) to minimize \(\max_{w,w^{\prime}\in\widehat{\mathcal{W}}_{k}}\|w-w^{\prime}\|_{(\sum_{z\in \mathcal{Z}}\lambda_{z}\Gamma^{\top}zz^{\top}\Gamma)^{-1}}\). The number of resulting samples taken from this design \(N_{k}\) is chosen to guarantee that the confidence interval of Lemma 2.2 is less than \(2^{-k}\). Then, the elimination step in Line 8 guarantees that all \(w\in\mathcal{W}\) such that \((w^{*}-w)^{\top}\theta>2\cdot 2^{-k}\) are then eliminated from the active set by round \(k+1\) of the procedure. To actually choose our samples, as is common in this literature [15], we use an efficient rounding procedure, ROUND that requires a minimum number of samples \(r(\omega)\).

**Sample Complexity Guarantee.** The sample complexity of Algorithm 1 depends on the following problem-dependent quantity \(\rho^{*}(\gamma)\) that captures the underlying hardness of a problem instance in terms of \((\mathcal{W},\mathcal{Z},\Gamma,\theta)\), when \(\gamma=0\), we abbreviate \(\rho^{*}(0)=\rho^{*}\),

\[\rho^{*}(\gamma)=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w\in\mathcal{W} \setminus\{w^{*}\}}\frac{\|w^{*}-w\|_{(\sum_{z\in\mathcal{Z}}\lambda_{z} \Gamma^{\top}zz^{\top}\Gamma)^{-1}}^{2}}{\langle w^{*}-w,\theta\rangle^{2} \vee\gamma^{2}}. \tag{7}\]

**Theorem 3.1**.: _Algorithm 1 is \(\delta\)-PAC and terminates in at most \(c(1+\omega)L_{\nu}\rho^{*}\log(1/\delta)+cr(\omega)\) samples, where \(c\) hides logarithmic factors of \(\Delta:=\min_{w}\langle w^{*}-w,\theta\rangle\) and \(|\mathcal{W}|\), as well as constants._

The proof of this result is in Appendix H.1. In the unconfounded case when \(\Gamma=I\) and \(\eta=0,\mathbb{E}_{t-1}[\varepsilon_{t}|x_{t}]=0\) this matches the sample complexity of [15]. In particular, for the case where \(\mathcal{Z}=\mathcal{X}=\mathcal{W}\), the problem further reduces to a standard multi-armed bandit, and if \(\varepsilon\) is 1-sub-Gaussian noise, [33] shows that \(\rho^{*}=O(\sum_{i=2}^{d}(\theta_{1}-\theta_{i})^{-2}))\), which is the optimal sample complexity of best-arm identification for multi-armed bandits. The following lemma shows that the conditioning of \(\Gamma\) can have a strong impact on the resulting sample complexity.

**Lemma 3.2**.: _For the compliance setting, we have \(\min_{\lambda\in\Delta^{d}}\max_{j,j^{\prime}}\|e_{j}-e_{j^{\prime}}\|_{(\sum_{i =1}^{d}\lambda_{i}\Gamma^{\top}e_{i}e_{i}^{\top}\Gamma)^{-1}}^{2}\leq d\max_{j,j^{\prime}}\|\Gamma^{-1}(e_{j}-e_{j^{\prime}})\|_{2}^{2}\). Furthermore, \(\rho^{*}\leq\frac{d\sigma_{\min}^{2}(\Gamma)^{-1}}{\Delta_{\min}^{2}}\)._

To further illustrate the impact of \(\Gamma\), imagine an extreme setting where \(\Gamma=(1-\varepsilon)/d\mathbf{1}\mathbf{1}^{\top}+\varepsilon I\) and \(\varepsilon\approx 0\), i.e. \(\Gamma\) is a perturbation of \(1/d\mathbf{1}\mathbf{1}^{\top}\). It's straightforward to show that the upper bound in the first display of Lemma 3.2 is of the order \(O(d\varepsilon^{-2})\) (this is also a lower bound - see Appendix K.1). In particular, the upper bound on the sample complexity is of the form \(d\varepsilon^{-2}/\Delta_{\min}^{2}\). This is in sharp contrast to the linear bandit case, when \(\Gamma=I\) and we are guaranteed a sample complexity of no more than \(d/\Delta_{\min}^{2}\) samples. To gain some intuition, regardless of the choice of \(\lambda\), \(\sum_{i\leq d}\lambda_{i}\Gamma^{\top}e_{i}e_{i}^{\top}\Gamma\approx\Gamma\). As a result, \(\rho^{*}\rightarrow\infty\) as \(\varepsilon\to 0\). Intuitively in the limit, regardless of which instrument \(i\leq d\) is being pulled, the resulting distribution on the treatments is uniform (the instruments are _weak_). Thus, it is impossible to deconfound the measurement noise, and recover an estimate of \(\theta\). This is a phenomenon which does not arise in the standard multi-armed bandit case with unconfounding.

_Remark 3.3_.: We also consider a setting where instead of given \(\Gamma\) directly, we are given an estimate \(\widehat{\Gamma}\) of \(\Gamma\) based on offline data. We discuss such an adaptation of Algorithm 1 to this setting in Appendix I and provide a sample complexity which reflects the error in \(\Gamma\) (scaling with \(\rho^{*}(\gamma)\) for \(\gamma>0\)). We remark that this result is subsumed by the approach of Section 3.2 and so we omit it in the main text.

```
1:Input \(\mathcal{Z},\mathcal{W},\Psi=\Gamma,\delta,L_{\nu}\geq\sigma_{\nu}^{2},\omega\),
2:Initialize:\(k=1,\mathcal{W}_{1}=\mathcal{W},\zeta_{1}=1\)
3:Set \(f(w,w^{\prime},\Gamma,\lambda):=\|w-w^{\prime}\|_{(\sum_{z\in Z}\lambda_{z} \Gamma^{\top}z_{z}\Gamma)^{-1}}^{2}\)
4:while\(|\mathcal{W}_{k}|>1\)do
5:\(\lambda_{k}=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime}\in \mathcal{W}_{k}}f(w,w^{\prime},\Gamma,\lambda)\).
6:\(\rho(\mathcal{W}_{k})=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime} \in\mathcal{W}_{k}}f(w,w^{\prime},\Gamma,\lambda)\).
7:\(N_{k}:=\lceil 2(1+\omega)2^{\mathrm{sk}}\rho(\mathcal{W}_{k})L_{\nu}\log(4k^{2}| \mathcal{W}|/\delta)\rceil\lor r(\omega)\)
8: Pull arms in \(Z_{N_{k}}=\text{ROUND}(\lambda_{k},N_{k})\) and observe \(Y_{N_{k}}\).
9: Compute \(\widehat{\theta}_{\Gamma}^{k}=\left(Z_{N_{k}}^{\top}Z_{N_{k}}\Gamma\right)^{- 1}Z_{N_{k}}^{\top}Y_{N_{k}}\)
10:\(\mathcal{W}_{k+1}=\mathcal{W}_{k}\backslash\{w\in\mathcal{W}_{k}|\exists w^{ \prime}\in\mathcal{W}_{k},\langle w^{\prime}-w,\widehat{\theta}_{\Gamma}^{k} \rangle>2^{-k}\},k\gets k+1\)
11:endwhile
12:Output:\(w\in\mathcal{W}_{k}\)
```

**Algorithm 1** CPEG:Confounded pure exploration with \(\Gamma\)

**Lower bound.** Due to the noise model from confounding and the dependence of the noise \(\theta^{\top}\eta+\varepsilon\), the instance-dependent lower bounds of [15] do not immediately apply. We develop a lower bound tailored for the confounding setting **that nearly match the upper bounds** of our algorithms. What's more, our lower bound illustrates the additional difficulty that arises from confounding by an additional factor of \(d^{2}\) compared to the standard transductive linear bandit problem in the most general setting where entries of \(\eta\) are sub-Gaussian, but not necessarily independent nor bounded. Due to space limit, we defer it to Appendix D.

### Fully Unknown Structural Model

We now consider the setting where \(\Gamma\) is fully unknown. The difficulty of this setting is that the data collection process needs to support both estimation of \(\Gamma\) and \(\theta\) simultaneously. Our algorithm, built upon Algorithm 1, is summarized in Algorithm 3. At its core, each phase of the algorithm is divided into two sub-phases, for estimating \(\Gamma\) and \(\theta\) respectively. Specifically, the second sub-phase is essentially same as Algorithm 1 with \(\widehat{\Gamma}_{k}\) in place of \(\Gamma\) where \(\widehat{\Gamma}_{k}\) is estimated from the first sub-phase. The main novelty of our algorithmic design lies in the first sub-phase, which resolves the challenge of performing the optimal design for estimating \(\Gamma\). To explain this challenge, the confidence interval for P-2SLS estimators of Theorem 2.3 indicates that one should pull arms so that we control both \(D_{2}:=\max_{w,w^{\prime}}\|w-w^{\prime}\|_{\tilde{A}(Z_{T_{2}},\widehat{ \Gamma}_{k})}^{2}\) (error from \(\hat{\theta}_{\text{P-2SLS}}\)) and \(D_{1}:=\max_{w,w^{\prime}}\|w-w^{\prime}\|_{\tilde{A}(Z_{T_{1}},\widehat{ \Gamma}_{k})}^{2}\) (error from \(\widehat{\Gamma}_{k}\)) to be below the target error \(O(\zeta_{k}^{2})\) at each phase (ignoring unimportant factors for discussion). Controlling \(D_{2}\) is trivial, which is done in the second sub-phase as we described above.

However, for \(D_{1}\), a similar strategy cannot be done because the estimate \(\widehat{\Gamma}_{k}\) is computed directly by sampling arms in \(Z_{T_{1}}\). That is, the ideal design, based on which we _will_ collect data points \(z_{1},\ldots,z_{n}\), requires access to the random matrix \(\widehat{\Gamma}_{k}\) that can only be computed _after_ sampling \(z_{1},\ldots,z_{n}\). Thiscreates a cycle that seems impossible to resolve. Such an issue, to our knowledge, has not been seen in existing work on pure exploration, and thus resolving it is our key technical contribution.

Our solution is to compute the design based on \(\widehat{\Gamma}_{k}\) from the previous phase. We then perform a doubling trick where we double the sample size (while following the computed design) until \(D_{1}\) becomes smaller than the target error \(O(\zeta_{k}^{2})\). The intuition is that in later phases the estimate \(\widehat{\Gamma}_{k}\) from the previous phase will be accurate enough to ensure that the design is efficient. Note that this novel algorithm induces extra randomness in how many samples we end up collecting in the first sub-phase, which remains random even after conditioning on the history, unlike the second sub-phase. This makes the analysis challenging, which we describe after the main result.

Our algorithm additionally employs the so-called E-optimal design to ensure that the covariance matrix of the collected data used to estimate \(\Gamma\) is well-conditioned. This conditioning is required to ensure that \(\widehat{\Gamma}_{k}\) concentrates fast enough to \(\Gamma\) as shown in the analysis. The E-optimal design is a well-known design objective in experimental design that aims to maximize the smallest singular value: \(\lambda_{E}^{*}:=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\sigma_{\max}(V^{-1}( \lambda))\), where \(V=\sum_{z\in\mathcal{Z}}\lambda_{z}zz^{\top}\). We denote \(\kappa_{0}^{-1}:=\sigma_{\max}(V^{-1}(\lambda_{E}^{*}))=\sigma_{\min}^{-1}(V( \lambda_{E}^{*}))\) as the smallest singular value achieved by the E-optimal design.

We present our analysis result Theorem 3.4 where we show that, even without knowledge of \(\Gamma\), the sample complexity scales with the key problem difficulty \(\rho^{*}\) almost matching the sample complexity of Algorithm 1 which relies on knowledge of \(\Gamma\).

**Theorem 3.4**.: _Algorithm 3 is \(\delta\)-PAC and terminates in at most_

\[(1+\omega)((L_{\nu}\log\bigl{(}1/\delta\bigr{)}+L_{\eta}\|\theta\|_{2}^{2}(d+ \log\bigl{(}1/\delta\bigr{)}))\rho^{*}+(d+\log\bigl{(}1/\delta\bigr{)})(L_{\eta }\|\theta\|_{2}^{2}\rho_{0}+M))\]

_pulls, ignoring both of the additive and multiplicative logarithms of \(\Delta,|\mathcal{W}|,\rho^{*},\rho_{0},M\), where_

\[\rho_{0}=\max_{w\in\mathcal{W}\setminus\{w^{*}\}}\|w^{*}-w\|_{(\sum_{z\in \mathcal{Z}}\lambda_{E,z}\Gamma^{\top}zz^{\top}\Gamma)^{-1}}^{2},\text{ and }M=\frac{32L_{\eta}}{\gamma_{\min}^{2}\sigma_{\min}\bigl{(}A(\lambda_{E},I)\bigr{)}}\lor 1.\]

_Note that \(\rho_{0}\) does not get hurt by \(\langle w^{*}-w,\theta\rangle\), (\(\rho^{*}\) does). It comes from the fact that in the first phase, we initialize that algorithm with E-optimal design._

The challenge of the analysis can be summarized in two-fold. First, since the concentration result in Theorem 2.3 is w.r.t. \(\widehat{\Gamma}_{k}\), we need to analyze how the random matrix \(\widehat{\Gamma}_{k}\) concentrates around \(\Gamma\) and how this impacts the sample complexity. For this, we develop a novel concentration inequality that relates the confidence width involving \(\hat{\Gamma}\) from Theorem 2.3 with the same quantity involving \(\Gamma\) in place of \(\hat{\Gamma}\). Second, our algorithm creates a long-range error propagation, which is highly nontrivial to analyze. To see this, the quality of \(\hat{\Gamma}_{k}\) is affected by the design objective function \(\max_{w,w^{\prime}}f(w,w^{\prime},\hat{\Gamma}_{k-1},\lambda)\), which depends on the error of the estimate \(\hat{\Gamma}_{k-1}\) from the previous phase. This error is, in turn, affected by the error of \(\hat{\Gamma}_{k-2}\) by the same mechanism. This is repeated all the way back to the first phase. Thus, any abnormal behavior from the first iteration will have a cumulative impact to even the end. In our analysis, we successfully analyze how the error is propagated from the previous iterations, which forms a complicated recursion. Resolving this recursion is our key novelty in the analysis.

_Remark 3.5_.: Our algorithm requires knowledge of a lower bound \(\gamma_{\min}\) of \(\lambda_{\min}(\Gamma)\). The knowledge of \(\gamma_{\min}\) is for simplicity only as one can obtain such a lower bound that is at least half of the true value \(\lambda_{\min}(\Gamma)\) via an efficient sampling procedure that we describe in Appendix K.

```
Input \(\mathcal{W},\hat{\Gamma},\zeta,\delta,\omega,\lambda_{E},M,L_{\eta}\) Define \(\mathsf{Stop}(\mathcal{W},Z,\Gamma,\delta):=\max_{w,w^{\prime}\in\mathcal{W}} \bigl{\|}w-w^{\prime}\bigr{\|}_{\bar{\lambda}(Z,\Gamma)^{-1}}\|\theta\|_{2} \sqrt{L_{\eta}\overline{\log(Z,\delta)}}\) Initialize \(\ell=1\), \(N_{0,0}=0\)\(\triangleright\) doubling trick initialization if\(\hat{\Gamma}=\bot\)then while\(\ell=1\) or \(\mathsf{Stop}\Bigl{(}\mathcal{W},Z_{0,\ell},\hat{\Gamma}^{\prime},\delta_{\ell} \Bigr{)}>1\)do  get \(2^{\ell-1}\Bigl{(}r(\omega)\vee\frac{2}{\kappa_{0}}\Bigr{)}\) samples denoted as \(\{Z_{0,\ell},X_{0,\ell},Y_{0,\ell}\}\) per design \(\lambda_{E}\)\(\triangleright\) via ROUND Update \(\hat{\Gamma}^{\prime}\) by OLS on \(\{Z_{0,\ell},X_{0,\ell}\},\ell\leftarrow\ell+1\) endwhile else \(\hat{\lambda}=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime}\in \mathcal{W}}f(w,w^{\prime},\hat{\Gamma},\lambda)\) \(N^{\prime}=\left\lfloor 4gdM\ln\left(1+2M\Bigl{(}d+L_{z}^{2}\Bigr{)}+2M2gdM\right)+8M\ln \left(\frac{2\cdot\delta^{a}}{\delta}\right)\vee r(\omega)\right\rfloor\) while\(\ell=1\) or \(\mathsf{Stop}\Bigl{(}\mathcal{W},Z_{0,\ell}\cup Z_{1,\ell},\hat{\Gamma},\delta_{\ell} \Bigr{)}>\zeta\)do \(N_{1,\ell}=2^{\ell}N^{\prime}\)\(\triangleright\) doubling trick update get \(N_{1,\ell}\) samples per \(\bar{\lambda}\) denoted as \(\{Z_{1,\ell},X_{1,\ell},Y_{1,\ell}\}\)\(\triangleright\) via ROUND \(N_{0,\ell}=\left\lceil 2gdM\ln\left(M\Bigl{(}d+N_{1,\ell}+L_{z}^{2}\Bigr{)}\right)+4M \ln\left(\frac{2\cdot\delta^{d}}{\delta_{\ell}}\right)\lor r(\omega)\vee\frac {2}{\kappa_{0}}\right\rceil\)  get \((N_{0,\ell}-N_{0,\ell-1})\) samples per \(\lambda_{E}\) augmented to \(\{Z_{0,\ell-1},X_{0,\ell-1}\}\) and get \(\{Z_{0,\ell},X_{0,\ell}\}\) Update \(\hat{\Gamma}^{\prime}\) by OLS on \(\{Z_{0,\ell}\cup Z_{1,\ell},X_{0,\ell}\cup X_{1,\ell}\}\), \(\ell\leftarrow\ell+1\) endwhile Output:\(\hat{\Gamma}^{\prime}\)
```

**Algorithm 4**\(\Gamma\) - estimator

**Experiments.** We provide experiments for the instance of Section 1.2 in the Appendix E. The experiments show that our approach is more sample efficient than natural passive baselines (e.g. A/B testing), or naively applying existing Pure-Exploration linear bandit methods and performs similarly to the oracle complexity.

## 4 Conclusion

This work introduces the CPET-LB problem in which the learning protocol is characterized by a linear structural equation model governed by parameters \(\Gamma\) and \(\theta\). We provide a general solution that simultaneously estimates the structural model while optimally designing to learn the best-arm. The key ideas behind our approach are based on linear experimental design techniques, an instrumental variable estimator whose variance can be controlled by the design, and novel finite-time confidence intervals on this estimator. This paper presents a number of directions for future work including considering situations where the \(d_{z}\neq d_{x}\), analysis to improve the dependence on the underlying noise variance, and the pursuit of a tight information-theoretic instance-dependent lower-bound. We hope that this line of work motivates increased discussion of the real impact of confounding on applicability of adaptive experimentation.

## Acknowledgements

Kwang-Sung Jun and Yao Zhao were supported in part by the National Science Foundation under grant CCF-2327013.

## References

* [1] Abbasi-Yadkori, Y., Pal, D., and Szepesvari, C. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* [2] Allen-Zhu, Z., Li, Y., Singh, A., and Wang, Y. Near-optimal discrete optimization for experimental design: A regret minimization approach. _Mathematical Programming_, 186:439-478, 2021.
* [3] Angrist, J. D., Imbens, G. W., and Rubin, D. B. Identification of causal effects using instrumental variables. _Journal of the American Statistical Association_, 91(434):444-455, 1996.
* [4] Bakshy, E., Eckles, D., and Bernstein, M. S. Designing and deploying online field experiments. In _International Conference on World Wide Web_, pages 283-292, 2014.
* [5] Bareinboim, E., Forney, A., and Pearl, J. Bandits with unobserved confounders: A causal approach. _Advances in Neural Information Processing Systems_, 28, 2015.
* [6] Bilodeau, B., Wang, L., and Roy, D. Adaptively exploiting d-separators with causal bandits. _Advances in Neural Information Processing Systems_, 35:20381-20392, 2022.
* [7] Bradlow, E. Encouragement designs: an approach to self-selected samples in an experimental design. _Marketing Letters_, 9:383-391, 1998.
* [8] Chandak, Y., Shankar, S., Syrgkanis, V., and Brunskill, E. Adaptive instrument design for indirect experiments. _arXiv preprint arXiv:2312.02438_, 2023.
* [9] Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. Double/debiased machine learning for treatment and structural parameters, 2018.
* [10] Degenne, R., Menard, P., Shang, X., and Valko, M. Gamification of pure exploration for linear bandits. In _International Conference on Machine Learning_, pages 2432-2442. PMLR, 2020.
* [11] Della Vecchia, R. and Basu, D. Online instrumental variable regression: Regret analysis and bandit feedback. _arXiv preprint arXiv:2302.09357_, 2023.
* [12] Elbers, B. Encouragement designs and instrumental variables for a/b testing, August 2023. URL [https://engineering.atspotify.com/2023/08/encouragement-designs-and-instrumental-variables-for-a-b-testing/](https://engineering.atspotify.com/2023/08/encouragement-designs-and-instrumental-variables-for-a-b-testing/). Accessed: 2024-05-19.
* [13] Engineering, S. Encouragement designs and instrumental variables for a/b testing, 2023. Accessed: December 26, 2023.
* [14] Even-Dar, E., Mannor, S., Mansour, Y., and Mahadevan, S. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. _Journal of machine learning research_, 7(6), 2006.
* [15] Fiez, T., Jain, L., Jamieson, K. G., and Ratliff, L. Sequential experimental design for transductive linear bandits. _Advances in Neural Information Processing Systems_, 32, 2019.
* [16] Gales, S. B., Sethuraman, S., and Jun, K.-S. Norm-agnostic linear bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 73-91. PMLR, 2022.
* [17] Gong, X. and Zhang, J. Dual instrumental method for confounded kernelized bandits. _arXiv preprint arXiv:2209.03224_, 2022.
* [18] Greene, W. H. _Econometric analysis_. Pearson Education India, 2003.

* [19] Inoue, A. and Solon, G. Two-sample instrumental variables estimators. _The Review of Economics and Statistics_, 92(3):557-561, 2010.
* [20] Jun, K.-S., Jain, L., Mason, B., and Nassif, H. Improved confidence bounds for the linear logistic model and applications to bandits. In _International Conference on Machine Learning_, pages 5148-5157. PMLR, 2021.
* [21] Kallus, N. Instrument-armed bandits. In _Algorithmic Learning Theory_, pages 529-546. PMLR, 2018.
* [22] Katz-Samuels, J., Jain, L., Karnin, Z., and Jamieson, K. G. An empirical process approach to the union bound: Practical algorithms for combinatorial and linear bandits. In _Advances in Neural Information Processing Systems_, volume 33, pages 10371-10382, 2020.
* [23] Kohavi, R., Tang, D., and Xu, Y. _Trustworthy online controlled experiments: A practical guide to a/b testing_. Cambridge University Press, 2020.
* [24] Kveton, B., Liu, Y., Kruijssen, J. M., and Nie, Y. Non-compliant bandits. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, pages 1138-1147, 2023.
* [25] Lattimore, F., Lattimore, T., and Reid, M. D. Causal bandits: Learning good interventions via causal inference. _Advances in Neural Information Processing Systems_, 29, 2016.
* [26] Lattimore, T. and Szepesvari, C. _Bandit algorithms_. Cambridge University Press, 2020.
* [27] Li, Z., Jamieson, K., and Jain, L. Optimal exploration is no harder than thompson sampling. _arXiv preprint arXiv:2310.06069_, 2023.
* [28] Lu, Y., Meisami, A., Tewari, A., and Yan, W. Regret analysis of bandit problems with causal background knowledge. In _Conference on Uncertainty in Artificial Intelligence_, pages 141-150. PMLR, 2020.
* [29] Malek, A., Aglietti, V., and Chiappa, S. Additive causal bandits with unknown graph. In _International Conference on Machine Learning_, 2023.
* [30] Mummalaneni, S., Yoganarasimhan, H., and Pathak, V. V. Producer and consumer engagement on social media platforms. _SSRN 4173537_, 2022.
* [31] Pearl, J. _Causality_. Cambridge university press, 2009.
* [32] Sen, R., Shanmugam, K., Dimakis, A. G., and Shakkottai, S. Identifying best interventions through online importance sampling. In _International Conference on Machine Learning_, pages 3057-3066. PMLR, 2017.
* [33] Soare, M., Lazaric, A., and Munos, R. Best-arm identification in linear bandits. _Advances in Neural Information Processing Systems_, 27, 2014.
* [34] Vershynin, R. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [35] Weltz, J., Fiez, T., Volfovsky, A., Laber, E., Mason, B., Nassif, H., and Jain, L. Experimental designs for heteroskedastic variance. _arXiv preprint arXiv:2310.04390_, 2023.
* [36] Zhang, J., Chen, Y., and Singh, A. Causal bandits: Online decision-making in endogenous settings. _arXiv preprint arXiv:2211.08649_, 2022.

## Appendix A Broader Impacts

This work is algorithmic and not tied to a particular application that would have immediate negative impact.

## Appendix B Illustrative Example

We now present an illustrative experiment that highlights the challenges of endogenous noise and the insufficiency of standard experimentation approaches used in the absence of confounding.

**Instance Definition.** Toward connecting back to membership example in Section 1, consider that a service has \(d\) membership options given by the set \(\mathcal{A}=\{1,\ldots,d\}\). Let the set \(\mathcal{Z}=\{e_{1},\cdots,e_{d}\}\) represent encouragements (incentives or advertisements) for the corresponding membership options given by \(\mathcal{W}=\mathcal{X}=\{e_{1},\cdots,e_{d}\}\). We consider a location model that assumes each user \(t\in\mathbb{N}\) arriving online has an underlying unobserved one-dimensional preference \(u_{t}\sim\mathcal{N}(0,\sigma_{u}^{2})\). If an algorithm presents the user with encouragement \(z_{I_{t}}=e_{I_{t}}\) for \(I_{t}\in\mathcal{A}\), then the user selects into themembership level given by \(J_{t}=\min_{j\in\mathcal{A}}|I_{t}+u_{t}-j|\) so that \(x_{t}=e_{J_{t}}\). For a visual depiction, see Figure 2(a). This process captures a user being more likely to opt-in to membership levels that are closer to the encouragement that they were presented. The outcome is then given by \(y_{t}=x_{t}^{\top}\theta+u_{t}\). This problem instance is a specific compliance instance. For this experiment, we take \(d=6\), let \(\theta=\begin{bmatrix}1&-0.95&0&0.45&0.95&0.99\end{bmatrix}\) and \(\sigma_{u}^{2}=0.35\). Observe that the optimal evaluation vector is \(w^{*}=e_{1}=\arg\max_{w\in\mathcal{W}}w^{\top}\theta\).

We simulate a UCB strategy which maintains estimates of the average reward of each of the possible \(d\) incentives, namely \(\widehat{\mu}_{i,t}=\sum_{s=1}^{t}\mathbf{1}\{z_{t}=e_{i}\}y_{t}\) and then pulls the one with the highest upper confidence bound. This models current practice of using a bandit algorithm to select which incentive to show a user. Our results averaged over 100 simulations are in Figure 2(d). At each round we estimate the average reward of each level using an OLS estimator, i.e. \(\widehat{\theta}_{i,t}^{OLS}=\sum_{s=1}^{t}\mathbf{1}\{x_{t}=e_{i}\}y_{t}/ \sum_{s=1}^{t}\mathbf{1}\{x_{t}=e_{i}\}\), and check whether it matches the true value (denoted as UCB-OLS). We also consider an instrumental variable-estimator (see the next Section) which incorporates knowledge of \(\Gamma\) similar to 2SLS to deconfound our estimate (UCB-IV). As the plot demonstrates, UCB-OLS completely fails to identify \(\theta_{1}=\arg\max_{i\in d}\theta_{i}\) (this line is hard to see it is at 0) due to a biased estimate, whereas UCB-IV does better. However, UCB-IV methods seem to have a constant probability of error. To see why, note that the expected reward from pulling \(z=e_{i}\) is \(e_{i}^{\top}\Gamma\theta\). These values are plotted in orange in Figure 2(c). In particular, with some constant probability, UCB runs on the empirical rewards from pulling \(z\)'s zeroes in on arm 6, and as a result fails to give enough samples to learn that arm 1 is indeed the best. In contrast, our proposed method CPEG, Algorithm 1 manages to find the best arm with significantly higher probability (the algorithm was run with \(\delta=.1\)) in the given time horizon.

## Appendix C Standard 2SLS estimator

Consider \(\Psi=\widehat{\Gamma}_{2SLS}=(Z_{T}^{\top}Z_{T})^{-1}Z_{T}^{\top}X_{T}\). In this setting, we recover the standard two-stage-least-squares (2SLS) estimator,

\[\widehat{\theta}_{\texttt{2SLS}}=(X_{T}^{\top}Z_{T}(Z_{T}^{\top}Z_{T})^{-1}Z_{ T}^{\top}X_{T})^{-1}X_{T}^{\top}(Z_{T}^{\top}Z_{T})^{-1}Z_{T}^{\top}Y_{T}=(Z_{T}^ {\top}X_{T})^{-1}Z_{T}^{\top}Y_{T}.\]

Note that the 2SLS estimator is a biased, but consistent estimator of the parameter \(\theta\)[3, 18].

Note that in particular, the asymptotic variance of 2SLS is known to be \(\sigma_{\varepsilon}^{2}\|w\|_{(\widehat{\Gamma}_{\texttt{2SLS}}^{\top}Z_{T}^{ \top}Z_{T}\widehat{\Gamma}_{\texttt{2SLS}})^{-1}}\)[18].

Recent work by [11] provides a confidence interval of the form \(|w^{\top}(\widehat{\theta}_{\texttt{2SLS}}-\theta)|\leq O(d\sigma_{\varepsilon} ^{2}\|w\|_{(\widehat{\Gamma}_{\texttt{2SLS}}^{\top}Z_{T}^{\top}Z_{T}\widehat{ \Gamma}_{\texttt{2SLS}})^{-1}}\sqrt{\log(T/\delta)})\). However, it's unclear how to use the form of their confidence interval directly for experimental design due to the dependence of \(\hat{\Gamma}_{2SLS}\) on the random quantity \(X\). In addition, their work is not sufficiently general to handle the general forms of noise that we consider in Lemma 2.1.

## Appendix D A non-interactive lower bound

Due to the noise model from confounding and the dependence of the noise \(\theta^{\top}\eta+\varepsilon\), the instance-dependent lower bounds of [15] do not immediately apply. In this section, we develop a lower bound tailored for the confounding setting.

Toward characterizing the optimal sample complexity, we develop a lower bound for a specific non-adaptive algorithm \(\mathcal{A}\) that has access to the matrix \(\Gamma\) governing the structural equation model. In particular, suppose that the non-adaptive algorithm \(\mathcal{A}\) is allowed to select a sequence of \(T\) measurements \(\{z_{I_{1}},\ldots z_{I_{t}},\ldots,z_{I_{T}}\}\) to query prior to collecting any observations, where \(I_{t}\) represents the index of the vector \(z\in\mathcal{Z}\) chosen at time \(t\in\{1,\ldots,T\}\). Then, given the observations \(\{y_{1},\ldots,\ldots y_{t},\ldots,y_{T}\}\) generated by the environment, a candidate optimal vector \(\widehat{w}\in\mathcal{W}\) is returned by the algorithm. We are interested in the necessary number of observations \(T\) that must be collected in order to ensure \(\mathbb{P}(\widehat{w}\neq w^{*})\leq\delta\) for some \(\delta\in(0,1)\). Thus, it is natural that the optimal non-adaptive algorithm \(\mathcal{A}\) using the estimator \(\widehat{\theta}_{\texttt{oracle}}\) forms a recommendation rule such that \(\widehat{w}=\arg\max_{w\in\mathcal{W}}w^{\top}\widehat{\theta}_{\texttt{ oracle}}\). We now state our lower bound result with respect to the non-adaptive oracle algorithm.

**Theorem D.1** (Non-Adaptive Oracle Lower Bound).: _Consider a problem instance characterized by \(\mathcal{W}\subset\mathbb{R}^{d},\mathcal{Z}\subset\mathbb{R}^{d}\), \(\Gamma\in\mathbb{R}^{d\times d}\), and \(\theta\in\mathbb{R}^{d}\). Assume \(\Gamma\) is known, \(\theta\) is unknown, and the noise process is jointly Gaussian and defined by \(\gamma:=\begin{bmatrix}\eta&\varepsilon\end{bmatrix}\sim\mathcal{N}(0,\Sigma)\) where \(\Sigma\in\mathbb{R}^{(d+1)\times(d+1)}\) is an arbitrary correlation matrix. For \(\delta\in(0,0.05]\), if the non-adaptive oracle algorithm acquires \(T\leq\sigma^{2}\rho^{*}\log(1/\delta)/2\) samples on the problem instance where \(\sigma^{2}:=v^{\top}\Sigma v\) and \(v:=\begin{bmatrix}\theta&1\end{bmatrix}\in\mathbb{R}^{d+1}\), then \(\mathbb{P}(\widehat{w}\neq w^{*})\geq\delta\)._

**Corollary D.2**.: _There exists a problem instance characterized by \(\mathcal{W}\subset\mathbb{R}^{d},\mathcal{Z}\subset\mathbb{R}^{d}\), \(\Gamma\in\mathbb{R}^{d\times d}\), and \(\theta\in\mathbb{R}^{d}\) with a noise process satisfying Assumption 1 such that if the non-adaptive oracle algorithm acquires \(T\leq\max\{d\|\theta\|_{2}^{2},\sqrt{d}\|\theta\|_{2}\}\rho^{*}\log(1/\delta )/2\) samples, then \(\mathbb{P}(\widehat{w}\neq w^{*})\geq\delta\) for \(\delta\in(0,0.05]\)._

The proof of Theorem D.1 is in Appendix F.1. Notably, the result is reminiscent of lower bounds for the standard pure exploration transductive linear bandit problem without confounding [15, 22] when given the measurement set \(\{\Gamma^{\top}z\}_{z\in\mathcal{W}}\), evaluation set \(\mathcal{Z}\), and parameter \(\theta\).

Notably, the upper bounds for our algorithms nearly match the lower bound of Theorem D.1. However, it is interesting to observe that the sample complexity incurs an additional factor of \(d^{2}\) relative to the standard transductive linear bandit problem in the most general setting where entries of \(\eta\) are sub-Gaussian, but not necessarily independent nor bounded. This illustrates the additional difficulty that arises from confounding. We point out that this is not likely to be a tight lower bound. Inparticular, it is a lower bound with respect to a non-adaptive algorithm that uses the particular choice of estimator. We leave improved lower bounds to future work.

## Appendix E Experiments

We now present experiments a collection of experiments on CPET-LB problem instances. The experiments demonstrate that our approach produces efficient designs for inference and estimation.

### Comparison Algorithms

The baselines that our approaches are compared with are discussed below. We run experiments both when \(\Gamma\) is known and when \(\Gamma\) is fully unknown.

#### e.1.1 Known \(\Gamma\).

To standardize the experiments, the baselines considered run in rounds mirroring the structure of Algorithm 1. Specifically, in round \(k\in\mathbb{N}\) a sampling algorithm selects a design \(\lambda_{k}\in\Delta(\mathcal{Z})\), collects \(N_{k}\) samples from the design, and forms a \(\Psi-\mathrm{IW}\) estimate of \(\theta\) with \(\Psi=\Gamma\) that is combined with a confidence interval (Lemma 2.2 to either eliminate evaluation vectors or validate a stopping condition. The number of samples \(N_{k}\) taken in round \(k\in\mathbb{N}\) by any of the algorithms is given by \(N_{k}=\lceil 2(1+\omega)\zeta_{k}^{-2}\rho(\mathcal{W}_{k})L_{\nu}\log(4k^{2}| \mathcal{W}|/\delta)\rceil\vee r(\omega)\) where \(\rho(\mathcal{W}_{k})=\max_{w,w^{\prime}\in\mathcal{W}_{k}}f(w,w^{\prime}, \Gamma,\lambda)\) for design \(\lambda_{k}\in\Delta(\mathcal{Z})\) and an active set of evaluation vectors \(\mathcal{W}_{k}\). The round sample count guarantees that given any experimental design, all vectors \(w\in\mathcal{W}\) such that \((w^{*}-w)^{\top}\theta>2\cdot 2^{-k}\) can be determined to be suboptimal by the end of round \(k\). The sampling methods we consider are now described.

* _Static Oracle._ This design selects \(\lambda_{k}=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w\in\mathcal{W} \setminus\{w^{*}\}}f(w^{*},w^{\prime},\Gamma,\lambda)\).
* _Static XY-Optimal._ This design selects \(\lambda_{k}=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime}\in \mathcal{W}}f(w,w^{\prime},\Gamma,\lambda)\).
* _Static Uniform._ This design selects \(\lambda_{k,z}=1/|\mathcal{Z}|\ \forall\ z\in\mathcal{Z}\).
* _Adaptive Uniform (SE)._ This design selects \(\lambda_{k,w}=1/|\mathcal{W}_{k}|\ \forall\ w\in\mathcal{W}_{k}\). Note that this algorithm is effectively an adaption of action-elimination [14].

The static designs are independent of the round and simply terminate when all evaluation vectors can be eliminated except for a recommended optimal vector \(\widehat{w}\).

#### e.1.2 Unknown \(\Gamma\).

For this set of experiments, we compare Algorithm 3 against a collection of variations of the sampling procedures. Specifically, we compare against methods that either replace only the experimental design for estimating \(\Gamma\), or only the experimental design for estimating \(\theta\), or both with uniform sampling. We label the approaches as \(N-N\), where \(N\) represents the sampling approaches (XY or uniform) for \(\Gamma\) and \(\theta\) respectively. Moreover, to make our approach more practical, we modify the algorithm so that \(\overline{\log}(Z_{T},\delta)=4d+\log\bigl{(}1/\delta\bigr{)}\). The step of incrementally adding more E-optimal design samples is also removed, so we collect E-optimal design samples only once in the beginning of Algorithm 4. We find that even with these modifications to the algorithm, correctness is maintained empirically.

### Experiment 1: Jump-Around Instance

We first return back to the location model of Section B. Recall that \(\mathcal{Z}=\mathcal{W}=\mathcal{X}=\{e_{1},\cdots,e_{d}\}\). For this experiment, we take \(d=6\), let \(\theta=\begin{bmatrix}1&-0.95&0.45&0.45&0.95&0.45\end{bmatrix}\) and \(\sigma_{u}^{2}=0.275\). The results of the experiment are shown in Figure 3(a) for the case of known \(\Gamma\). We see that Algorithm 1 performs much better than the baselines and nearly matches the oracle design. Delving into the approach, it is able to quickly eliminate all but \(w_{1}\) and \(w_{d-1}\) and then puts more mass on \(z_{1}\) and \(z_{d-1}\) to reduce the uncertainty on \(w_{1}\) and \(w_{d-1}\). For the case of unknown \(\Gamma\), the results are shown in Figure 3(d), where \(\theta_{5}\) is reduced to \(0.9\) so that all approaches could finish.

### Experiment 2: Interpolation Instance

Let \(\mathcal{Z}=\mathcal{W}=\mathcal{X}=\{e_{1},\cdots,e_{d}\}\) define the measurement, evaluation, and observation sets. We first consider that \(\Gamma:=\frac{(1-\varepsilon)}{d}1_{d}1_{d}^{\top}+\varepsilon I_{d}\) for a parameter \(\varepsilon\in(0,1)\) where \(1_{d}\) is a \(d\)-dimensional vector of 1's and \(I_{d}\) is the \(d\)-dimensional identity matrix. For this experiment, we take \(d=4\) and let \(\theta=\begin{bmatrix}0.5&0.583&0.67&0.75\end{bmatrix}\). As in all compliance instances, \(\eta_{t}=x_{t}-\Gamma^{\top}z_{t}\), and in this simulation \(\eta_{t}=0.4\eta_{t}^{\top}v_{t}\), where \(v_{t}=\bar{v}_{t}/\|\bar{v}_{t}\|_{2}\) and \(\bar{v}_{t}\sim\mathcal{N}(0,I_{d})\). The results of the experiment are shown in Figure 4b for \(\varepsilon\in\{1,0.9,0.8,0.7\}\) with \(\Gamma\) known. Note that Static-XY and Uniform overlap, and SE and CPEG overlap. We see that Algorithm 1 and the adaptive uniform strategy perform similarly and near optimally. This is to be expected since the most efficient way to gather observations for treatments is to encourage that treatment, given that if the encouragement is not followed each of the alternatives is equally likely and provides no additional information of interest. Moreover, as discussed earlier, the problem gets more challenging as \(\Gamma\to 1_{d}1_{d}^{\top}/d\). Note that the identity matrix could be replaced with a permutation matrix, in which case uniform sampling with elimination becomes highly suboptimal. The results for the case of \(\Gamma\) unknown are shown in Figure 4e with \(\varepsilon=0.99\). This shows the value that comes from the experimental design for estimating both \(\Gamma\) and \(\theta\).

To demonstrate the superiority of our algorithm over SE, we also consider that \(\Gamma:=\frac{(1-\varepsilon)}{d}1_{d}1_{d}^{\top}+\varepsilon I_{d}^{p}\), where \(I_{d}^{p}\) is a permutation matrix as follows,

\[I_{d}^{p}=\begin{bmatrix}0&1&0&0\\ 0&0&1&0\\ 0&0&0&1\\ 1&0&0&0\end{bmatrix}.\]

All other settings remain the same as in the previous interpolation instance. The results for the known \(\Gamma\) case, shown in Figure 4c, indicate that SE exhibits significant underperformance due to its sampling rule not accounting for the permutation effect in \(\Gamma\). In contrast, CPEG consistently achieves near-optimal performance. Note that Static-XY and Uniform still overlap. Figure 4f presents the results for the unknown \(\Gamma\) case, where we can notice that, comparing with Figure 4e, estimating different permutation matrices (with the identity matrix as a special case) does not affect problem difficulty.

Figure 4: Sample complexity for algorithms on CPET-LB problems. Our approach is consistently competitive across the experiments.)Proofs of the lower bound

### Proof of Theorem d.1

_Theorem d.1._ Consider a problem instance characterized by \(\mathcal{W}\subset\mathbb{R}^{d},\mathcal{Z}\subset\mathbb{R}^{d}\), \(\Gamma\in\mathbb{R}^{d\times d}\), and \(\theta\in\mathbb{R}^{d}\). Assume \(\Gamma\) is known, \(\theta\) is unknown, and the noise process is jointly Gaussian and defined by \(\gamma:=\begin{bmatrix}\eta&\varepsilon\end{bmatrix}\sim\mathcal{N}(0,\Sigma)\) where \(\Sigma\in\mathbb{R}^{(d+1)\times(d+1)}\) is an arbitrary correlation matrix. For \(\delta\in(0,0.05]\), if the non-adaptive oracle algorithm acquires \(T\leq\sigma^{2}\rho^{*}\log\bigl{(}1/\delta\bigr{)}/2\) samples on the problem instance where \(\sigma^{2}:=v^{\top}\Sigma v\) and \(v:=\begin{bmatrix}\theta&1\end{bmatrix}\in\mathbb{R}^{d+1}\), then \(\mathbb{P}(\widehat{w}\neq w^{*})\geq\delta\).

Proof.: We begin by recalling the framework of the non-adaptive oracle algorithm and discussing the properties of its estimator for the noise structure described in the statement of the result.

Non-Adaptive Oracle and Instance Definition.The non-adaptive oracle algorithm \(\mathcal{A}\) selects \(T\) measurements to query prior to collecting any data. Let \(I_{t}\) represent the index of the vector \(z\in\mathcal{Z}\) chosen at time \(t\in\{1,\ldots,T\}\). The noise process for the instance under consideration is assumed to be jointly Gaussian and defined by \(\gamma_{t}:=\begin{bmatrix}\eta_{t}&\varepsilon_{t}\end{bmatrix}\sim\mathcal{N }(0,\Sigma)\) where \(\Sigma\in\mathbb{R}^{(d+1)\times(d+1)}\) is an arbitrary positive semidefinite matrix. Defining \(\bar{x}_{I_{t}}:=\Gamma^{\top}z_{I_{t}}\), \(v:=\begin{bmatrix}\theta&1\end{bmatrix}\in\mathbb{R}^{d+1}\) and \(\nu_{t}:=v^{\top}\gamma_{t}\), the feedback model can be described as follows:

\[y_{t} =x_{t}\theta+\varepsilon_{t}\] \[=(\Gamma^{\top}z_{I_{t}})^{\top}\theta+\eta_{t}^{\top}\theta+ \varepsilon_{t}\] \[=:(\Gamma^{\top}z_{I_{t}})^{\top}\theta+v^{\top}\gamma_{t}\] \[=:\bar{x}_{I_{t}}^{\top}\theta+\nu_{t}.\]

Observe that the noise is independent and identically distributed as \(\nu_{t}\sim\mathcal{N}(0,\sigma^{2})\) where \(\sigma^{2}:=v^{\top}\Sigma v\) since \(\gamma_{t}\sim\mathcal{N}(0,\Sigma)\). Moreover, the noise process is exogeneous with \(\mathbb{E}[\nu_{t}|\bar{x}_{I_{t}}]=0\) since \(\bar{x}_{I_{t}}\) is deterministic given the index choice \(I_{t}\).

Let \(\{z_{I_{t}}\}_{t=1}^{T},\{\bar{x}_{I_{t}}\}_{t=1}^{T}\), and \(\{y_{t}\}_{t=1}^{T}\) denote the observations collected by the non-adaptive oracle algorithm \(\mathcal{A}\) and define \(Z_{T}\in\mathbb{R}^{T\times d}\), \(\bar{X}_{T}\in\mathbb{R}^{T\times d}\), and \(Y_{T}\in\mathbb{R}^{T}\) to contain the respective stacked observations. Algorithm \(\mathcal{A}\) obtains an estimate \(\widehat{\theta}_{\texttt{oracle}}\) by minimizing the sum of squares as follows:

\[\widehat{\theta}_{\texttt{oracle}}:=\arg\min_{\widehat{\theta} \in\mathbb{R}^{d}}\sum_{t=1}^{T}(y_{t}-\bar{x}_{I_{t}}\widehat{\theta})^{2}=( \bar{X}_{T}^{\top}\bar{X}_{T})^{-1}\bar{X}_{T}^{\top}Y_{T}=(Z_{T}^{\top}Z_{T} \Gamma)^{-1}Z_{T}^{\top}Y_{T}.\]

Given \(\widehat{\theta}_{\texttt{oracle}}\), the non-adaptive oracle algorithms \(\mathcal{A}\) returns a recommendation defined by \(\widehat{w}=\arg\max_{w\in\mathcal{W}}w^{\top}\widehat{\theta}_{\texttt{oracle}}\). Note that since \(\widehat{\theta}_{\texttt{oracle}}\) is obtained by least squares with exogeneous, independent and identically distributed mean-zero Gaussian noise, it is straightforward to verify the estimator is distributed as

\[\widehat{\theta}_{\texttt{oracle}}-\theta\sim\mathcal{N}\Bigl{(}0,\sigma^{2} \cdot\bar{A}(Z_{T},\Gamma)^{-1}\Bigr{)}, \tag{8}\]

where

\[\bar{A}(Z_{T},\Gamma):=\Big{(}\sum_{t=1}^{T}\Gamma^{\top}z_{I_{t}}z_{I_{t}} \Gamma\Big{)}=\bar{X}_{T}^{\top}\bar{X}_{T}=\Gamma^{\top}Z_{T}^{\top}Z_{T}\Gamma.\]

Proof by Contradiction.To begin, recall that

\[\rho^{*}:=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w\in\mathcal{W}\setminus \{w^{*}\}}\frac{\|w^{*}-w\|_{A(\lambda,\Gamma)^{-1}}^{2}}{\langle w^{*}-w, \theta\rangle^{2}}.\]

Suppose for the sake of contradiction that the number of samples collected by the non-adaptive oracle algorithm \(\mathcal{A}\) is \(T\leq\sigma^{2}\rho^{*}\log\bigl{(}1/\delta\bigr{)}/2\) and \(\mathbb{P}(\widehat{w}\neq w^{*})<\delta\) for \(\delta\in(0,0.05]\). To reach a contradiction, we analyze the distribution of \((w-w^{*})^{\top}\widehat{\theta}\) for some \(w\neq w^{*}\) and show that with probability at least \(\delta\) it is positive. We remark that this proof follows similar techniques to that of the proof of Theorem 3 of [22].

Let \(\overline{\lambda}\in\Delta(\mathcal{Z})\) represent the empirical sampling distribution of the algorithm \(\mathcal{A}\), which is defined such that \(\overline{\lambda}_{z}=\frac{1}{T}\sum_{t=1}^{T}\mathbb{1}\{z_{I_{t}}=z\}\) for each \(z\in\mathcal{Z}\). Moreover, define

\[\rho^{*}(\overline{\lambda}):=\max_{w\in\mathcal{W}\setminus\{w^{*}\}}\frac{\|w ^{*}-w\|_{A(\overline{\lambda},\Gamma)^{-1}}^{2}}{\langle w^{*}-w,\theta\rangle ^{2}}\quad\text{and}\quad\widetilde{w}\in\operatorname*{arg\,max}_{w\in \mathcal{W}\setminus\{w^{*}\}}\frac{\|w^{*}-w\|_{A(\overline{\lambda},\Gamma)^ {-1}}^{2}}{\langle w^{*}-w,\theta\rangle^{2}}.\]

Note that \(\rho^{*}(\overline{\lambda})\geq\rho^{*}\) and observe by definition,

\[\frac{A(\overline{\lambda},\Gamma)^{-1}}{T}:=\frac{(\sum_{z\in\mathcal{Z}} \overline{\lambda}_{z}\Gamma^{\top}zz^{\top}\Gamma)^{-1}}{T}=\Big{(}\sum_{t=1} ^{T}\Gamma^{\top}z_{I_{t}}z_{I_{t}}\Gamma\Big{)}^{-1}:=\bar{A}(Z_{T},\Gamma)^ {-1}.\]

Thus, by Eq. (8),

\[\widehat{\theta}_{\texttt{0racle}}-\theta\sim\mathcal{N}\Big{(}0,\sigma^{2} \cdot\frac{A(\overline{\lambda},\Gamma)^{-1}}{T}\Big{)},\]

and

\[\frac{(\widetilde{w}-w^{*})^{\top}(\widehat{\theta}_{\texttt{0racle}}-\theta)} {\langle w^{*}-\widetilde{w},\theta\rangle}\sim\mathcal{N}\Big{(}0,\sigma^{2} \cdot\frac{\|w^{*}-\widetilde{w}\|_{A(\overline{\lambda},\Gamma)^{-1}}^{2}}{T \cdot\langle w^{*}-\widetilde{w},\theta\rangle^{2}}\Big{)}.\]

Furthermore, by the definition of \(\rho^{*}(\overline{\lambda})\), the assumption \(T\leq\sigma^{2}\rho^{*}\log\bigl{(}1/\delta\bigr{)}/2\), and the fact \(\rho^{*}(\overline{\lambda})\geq\rho^{*}\), we obtain

\[\mathbb{V}\Big{(}\frac{(\widetilde{w}-w^{*})^{\top}(\widehat{\theta}_{\texttt {0racle}}-\theta)}{\langle w^{*}-\widetilde{w},\theta\rangle}\Big{)}=\sigma^{ 2}\cdot\frac{\|w^{*}-\widetilde{w}\|_{A(\overline{\lambda},\Gamma)^{-1}}^{2}}{ T\cdot\langle w^{*}-\widetilde{w},\theta\rangle^{2}}:=\sigma^{2}\cdot\frac{\rho^{*}( \overline{\lambda})}{T}\geq\frac{2}{\log\bigl{(}1/\delta\bigr{)}}. \tag{9}\]

Now, consider a random variable \(W\sim\mathcal{N}(0,1)\). Proposition 2.1.2 of Vershynin [34] gives an anti-concentration result showing that for all \(\zeta>0\),

\[\mathbb{P}(W\geq\zeta)\geq\Big{(}\frac{1}{\zeta}-\frac{1}{\zeta^{3}}\Big{)} \frac{1}{\sqrt{2\pi}}e^{-\zeta^{2}/2}. \tag{10}\]

We apply this result to the quantity \((\widetilde{w}-w^{*})^{\top}(\widehat{\theta}_{\texttt{0racle}}-\theta)/\langle w ^{*}-\widetilde{w},\theta\rangle\) to conclude that \((\widetilde{w}-w^{*})^{\top}\widehat{\theta}_{\texttt{0racle}}>0\) with probability at least \(\delta\). Toward doing so, let \(c\in(1,1.15]\) be a constant and define

\[\widetilde{W}\sim\mathcal{N}\Big{(}0,\frac{2}{\log\bigl{(}1/\delta\bigr{)}} \Big{)}\quad\text{and}\quad W:=\frac{\widetilde{W}}{\sqrt{2/\log\bigl{(}1/ \delta\bigr{)}}}\quad\text{and}\quad\gamma:=\frac{c}{\sqrt{2/\log\bigl{(}1/ \delta\bigr{)}}}.\]

Observe that \(W\sim\mathcal{N}(0,1)\). The following analysis holds for \(\delta\in(0,0.05]\) given that \(c\in(1,1.15]\) as assumed:

\[\mathbb{P}\Big{(}\frac{(\widetilde{w}-w^{*})^{\top}(\widehat{ \theta}_{\texttt{0racle}}-\theta)}{\langle w^{*}-\widetilde{w},\theta\rangle} \geq c\Big{)} \geq\mathbb{P}(\widetilde{W}\geq c)\] (By Eq. 9 ) \[=\mathbb{P}\Big{(}\frac{\widetilde{W}}{\sqrt{2/\log\bigl{(}1/\delta \bigr{)}}}\geq\frac{c}{\sqrt{2/\log\bigl{(}1/\delta\bigr{)}}}\Big{)}\] \[:=\mathbb{P}(W\geq\gamma)\] \[\geq\Big{(}\frac{1}{\gamma}-\frac{1}{\gamma^{3}}\Big{)}\frac{1}{ \sqrt{2\pi}}e^{-\gamma^{2}/2}\quad\text{(Proposition \ref{prop: 1.2 Vershynin 34})}\] \[=\Big{(}\frac{\sqrt{2}}{c\sqrt{\log\bigl{(}1/\delta\bigr{)}}}- \frac{\sqrt{2}^{3}}{c^{3}\sqrt{\log\bigl{(}1/\delta\bigr{)}^{3}}}\Big{)}\frac{1 }{\sqrt{2\pi}}\delta^{c^{2}/4}\] \[\geq\delta.\]

The final inequality can be verified computationally. Thus, with probability at least \(\delta\) for \(\delta\in(0,0.05]\), we obtain

\[(\widetilde{w}-w^{*})^{\top}\widehat{\theta}_{\texttt{0racle}} \geq c(w^{*}-\widetilde{w})^{\top}\theta+(\widetilde{w}-w^{*})^{ \top}\theta\] \[=c(w^{*}-\widetilde{w})^{\top}\theta-(w^{*}-\widetilde{w})^{\top}\theta\] \[=(c-1)(w^{*}-\widetilde{w})^{\top}\theta\] \[>0.\]Observe that the final inequality holds since \(c>1\) and \((w^{*}-\widehat{w})^{\top}\theta>0\) by definition.

This result directly implies that with probability at least \(\delta\) for \(\delta\in(0,0.05]\), the vector \(\widehat{w}\) returned by algorithm \(\mathcal{A}\) is not \(w^{*}\). This is a contradiction, so we conclude that if the non-adaptive oracle algorithm \(\mathcal{A}\) acquires \(T\leq\sigma^{2}\rho^{*}\log\bigl{(}1/\delta\bigr{)}/2\) samples, then \(\mathbb{P}(\widehat{w}\neq w^{*})\geq\delta\) for \(\delta\in(0,0.05]\).

### Proof of Corollary d.2

Corollary d.2.: There exists a problem instance characterized by \(\mathcal{W}\subset\mathbb{R}^{d},\mathcal{Z}\subset\mathbb{R}^{d}\), \(\Gamma\in\mathbb{R}^{d\times d}\), and \(\theta\in\mathbb{R}^{d}\) with a noise process satisfying Assumption 1 such that if the non-adaptive oracle algorithm acquires \(T\leq\max\{d\|\theta\|_{2}^{2},\sqrt{d}\|\theta\|_{2}\}\rho^{*}\log\bigl{(}1/ \delta\bigr{)}/2\) samples, then \(\mathbb{P}(\widehat{w}\neq w^{*})\geq\delta\) for \(\delta\in(0,0.05]\).

Proof.: To begin, consider the specifications of Theorem d.1 and its result. That is, a problem an arbitrary instance characterized by \(\mathcal{W}\subset\mathbb{R}^{d},\mathcal{Z}\subset\mathbb{R}^{d}\), \(\Gamma\in\mathbb{R}^{d\times d}\), and \(\theta\in\mathbb{R}^{d}\) where the noise process is jointly Gaussian and defined by \(\gamma:=[\eta\quad\varepsilon]\sim\mathcal{N}(0,\Sigma)\) where \(\Sigma\in\mathbb{R}^{(d+1)\times(d+1)}\) is an arbitrary correlation matrix. Observe that the noise process defined by \(\gamma\) satisfies Assumption 1. The result states that if the non-adaptive oracle algorithm acquires \(T\leq\sigma^{2}\rho^{*}\log\bigl{(}1/\delta\bigr{)}/2\) samples on the problem instance where \(\sigma^{2}:=v^{\top}\Sigma v\) and \(v:=\begin{bmatrix}\theta&1\end{bmatrix}\in\mathbb{R}^{d+1}\), then \(\mathbb{P}(\widehat{w}\neq w^{*})\geq\delta\) for \(\delta\in(0,0.05]\). From this point, we show that there exists a parameter \(\theta\) and correlation matrix \(\Sigma\) such that \(\sigma^{2}:=v^{\top}\Sigma v\geq\max\{d\|\theta\|_{2}^{2},\sqrt{d}\|\theta\|_ {2}\}\) in order to reach the stated conclusion.

Notation.Let \(\zeta_{\eta_{i},\varepsilon}\in[-1,1]\) denote the correlation between \(\eta_{i}\) and \(\varepsilon\) for \(i\in\{1,\ldots,d\}\). Similarly, let \(\zeta_{\eta_{i},\eta_{j}}=\zeta_{\eta_{j},\eta_{i}}\in[-1,1]\) denote the correlation between \(\eta_{i}\) and \(\eta_{j}\) for \(i\neq j\in\{1,\ldots,d\}\). Note that the correlation of \(\eta_{i}\) with itself for \(i\in\{1,\ldots,d\}\) is \(\sigma_{\eta_{i}}^{2}=\zeta_{\eta_{i},\eta_{i}}=1\) and similarly the correlation of \(\varepsilon\) with itself is \(\sigma_{\varepsilon}^{2}=\zeta_{\varepsilon,\varepsilon}=1\). The correlation matrix \(\Sigma\) is then given by

\[\Sigma=\begin{bmatrix}1&\zeta_{\eta_{2},\eta_{1}}&\cdots&\zeta_{\eta_{d},\eta _{1}}&\zeta_{\varepsilon,\eta_{1}}\\ \zeta_{\eta_{1},\eta_{2}}&1&\cdots&\zeta_{\eta_{d},\eta_{2}}&\zeta_{\varepsilon,\eta_{2}}\\ \vdots&\vdots&\ddots&\vdots&\vdots\\ \zeta_{\eta_{1},\eta_{d}}&\zeta_{\eta_{2},\eta_{d}}&\cdots&1&\zeta_{\varepsilon,\eta_{d}}\\ \zeta_{\eta_{1},\varepsilon}&\zeta_{\eta_{2},\varepsilon}&\cdots&\zeta_{\eta_{ d},\varepsilon}&1\end{bmatrix}:=\begin{bmatrix}\Sigma_{\theta}&\zeta_{\eta_{1}, \varepsilon}\\ \zeta_{\eta_{2},\varepsilon}\\ \vdots\\ \zeta_{\eta_{d},\varepsilon}\end{bmatrix},\]

where

\[\Sigma_{\theta}=\begin{bmatrix}1&\zeta_{\eta_{2},\eta_{1}}&\cdots&\zeta_{ \eta_{d},\eta_{1}}\\ \zeta_{\eta_{1},\eta_{2}}&1&\cdots&\zeta_{\eta_{d},\eta_{2}}\\ \vdots&\vdots&\ddots&\vdots\\ \zeta_{\eta_{1},\eta_{d}}&\zeta_{\eta_{2},\eta_{d}}&\cdots&1\end{bmatrix}\in[- 1,1]^{d\times d}\quad\text{and}\quad\zeta_{\eta,\varepsilon}=\begin{bmatrix} \zeta_{\eta_{1},\varepsilon}\\ \zeta_{\eta_{2},\varepsilon}\\ \vdots\\ \zeta_{\eta_{d},\varepsilon}\end{bmatrix}\in[-1,1]^{d}.\]

Moreover, we use the notation \(\Sigma_{\theta,i}^{\top}\in\mathbb{R}^{d}\) to denote the \(i\)-th row of \(\Sigma_{\theta}^{\top}\), or equivalently the \(i\)-th column of \(\Sigma_{\theta}\), for any \(i\in\{1,\ldots,d\}\).

Lower Bounding Noise Variance.Given the above notation, we now work toward lower bounding \(\sigma^{2}:=v^{\top}\Sigma v\). Observe that by algebraic manipulations,

\[v^{\top}\Sigma v :=\begin{bmatrix}\theta\\ 1\end{bmatrix}^{\top}\begin{bmatrix}\Sigma_{\theta}&\zeta_{\eta, \varepsilon}\\ \zeta_{\eta,\varepsilon}^{\top}&1\end{bmatrix}\begin{bmatrix}\theta\\ 1\end{bmatrix}\] \[=\begin{bmatrix}\theta^{\top}\Sigma_{\theta,1}^{\top}+\zeta_{ \eta_{1},\varepsilon}&\theta^{\top}\Sigma_{\theta,2}^{\top}+\zeta_{\eta_{2}, \varepsilon}&\cdots&\theta^{\top}\Sigma_{\theta,d}^{\top}+\zeta_{\eta_{d}, \varepsilon}&\theta^{\top}\zeta_{\eta,\varepsilon}+1\end{bmatrix}\begin{bmatrix} \theta\\ 1\end{bmatrix}\] \[=\theta^{\top}\sum_{i=1}^{d}\Sigma_{\theta,i}^{\top}\theta_{i}+ \sum_{i=1}^{d}\zeta_{\eta_{i},\varepsilon}\theta_{i}+\theta^{\top}\zeta_{\eta, \varepsilon}+1\] \[=\theta^{\top}\Sigma_{\theta}\theta+2\theta^{\top}\zeta_{\eta, \varepsilon}+1.\]

Since \(\Sigma_{\theta}\) is a real symmetric matrix, an eigendecomposition exists such that \(\Sigma_{\theta}=Q\Lambda Q^{\top}\) where \(\Lambda=\text{diag}(\lambda_{1},\ldots,\lambda_{d})\in\mathbb{R}^{d\times d}\) is a diagonal matrix containing the eigenvalues of \(\Sigma_{\theta}\) and \(Q\in\mathbb{R}^{d\times d}\)is an orthogonal matrix with columns corresponding to the eigenvectors of \(\Sigma_{\theta}\). Let \(q_{i}:=Q_{i}^{\top}\) denote column \(i\) of the matrix \(Q\) for \(i=\{1,\ldots,d\}\), which is equivalently eigenvector \(i\) of \(\Sigma_{\theta}\) for \(i=\{1,\ldots,d\}\). Without loss of generality, assume that the eigenvectors are of unit length so that \(\|q_{i}\|_{2}=1\) for all \(i=\{1,\ldots,d\}\).

Given this information, suppose that the parameter \(\theta\) in the instance is equal to a scalar multiple of the eigenvector of \(\Sigma_{\theta}\) corresponding to the maximum eigenvalue. Note this is equivalent to the statement that \(\theta\) is equal to some scalar multiple of the column \(q_{*}\in\mathbb{R}^{d}\) of the matrix \(Q\) where \(q_{*}\) is the eigenvector of \(\Sigma_{\theta}\) corresponding to the maximum eigenvalue \(\lambda^{*}\). Thus, we take \(\theta=c\cdot q_{*}\) for some \(c\in\mathbb{R}\) and observe that \(\|\theta\|_{2}=c\). Toward quantifying the value of \(v^{\top}\Sigma v\) for the problem instance, we begin by characterizing \(\theta^{\top}\Sigma_{\theta}\theta\) for the choice of \(\theta\). Consider the following analysis:

\[\theta^{\top}\Sigma_{\theta}\theta =\theta^{\top}Q\Lambda Q^{\top}\theta\] \[:=(cq_{*})^{\top}Q\Lambda Q^{\top}(cq_{*})\] ( \[\theta:=cq_{*}\] ) \[=\|\theta\|_{2}^{2}q_{*}^{\top}\big{[}q_{1}\quad\cdots\quad q_{*} \quad\cdots\quad q_{d}\big{]}\Lambda\big{[}q_{1}\quad\cdots\quad q_{*}\quad \cdots\quad q_{d}\big{]}^{\top}q_{*}\] \[=\|\theta\|_{2}^{2}\big{[}0\quad\cdots\quad\|q_{*}\|_{2}^{2}\quad \cdots\quad 0\big{]}\text{diag}(\lambda_{1},\ldots,\lambda^{*},\ldots,\lambda_{1 })\big{[}0\quad\cdots\quad\|q_{*}\|_{2}^{2}\quad\cdots\quad 0\big{]}^{\top}\] \[=\|\theta\|_{2}\lambda^{*}.\] ( \[\|q_{*}\|_{2}=1\] )

Thus, in general for this choice of \(\theta\),

\[v^{\top}\Sigma_{\theta}v=\|\theta\|_{2}^{2}\lambda^{*}+2\theta^{\top}\zeta_{ \eta,\varepsilon}+1.\]

To conclude, take \(\Sigma_{\theta}:=\mathbf{1}_{d}\mathbf{1}_{d}^{\top}\) where \(\mathbf{1}_{d}\) represents the \(d\)-dimensional vector of all ones. Since this is a rank-1 matrix, the maximum eigenvalue is \(\lambda^{*}=\mathbf{1}_{d}^{\top}\mathbf{1}_{d}=d\) and the remainder of the eigenvalues are zero. Observe that \(q_{*}=\mathbf{1}_{d}/\sqrt{d}\) is an eigenvector corresponding to the maximum eigenvalue since \(\mathbf{1}_{d}\mathbf{1}_{d}^{\top}q_{*}=dq_{*}\). Thus,

\[v^{\top}\Sigma_{\theta}v =\|\theta\|_{2}^{2}\lambda^{*}+2\theta^{\top}\zeta_{\eta, \varepsilon}+1\] \[:=\|\theta\|_{2}^{2}\lambda^{*}+2\|\theta\|_{2}\mathbf{1}_{d}^{ \top}\mathbf{1}_{d}/\sqrt{d}+1\qquad\text{ ($\theta:=cq_{*}:=\|\theta\|_{2}\mathbf{1}_{d}/\sqrt{d}=$ and $\zeta_{\eta,\varepsilon}:=\mathbf{1}_{d}$)}\] \[=d\|\theta\|_{2}^{2}+2\sqrt{d}\|\theta\|_{2}+1\] \[\geq\max\{d\|\theta\|_{2}^{2},\sqrt{d}\|\theta\|_{2}\}.\]

This completes the proof since we have shown that there exists a parameter \(\theta\) and correlation matrix \(\Sigma\) such that \(\sigma^{2}:=v^{\top}\Sigma v\geq\max\{d\|\theta\|_{2}^{2},\sqrt{d}\|\theta\| _{2}\}\), which by Theorem D.1 allows us to make the stated conclusion. 

## Appendix G Proofs of the confidence interval

### Proof of Lemma 2.1

The first statement is an immediate consequence of Lemma G.2 and the second statement is proven in Lemma G.1.

**Lemma G.1**.: _In the compliance model, the noise \(\eta^{\top}\theta+\varepsilon\) follows a \(\Big{(}8\|\theta\|_{2}^{2}+2\Big{)}\)-sub-Gaussian distribution._

Proof.: In compliance, we have \(z,x\in\{e_{1},\cdots,e_{d}\}\), and

\[\eta=x-\Big{(}\mathbb{P}\big{(}e_{1}\mid z\big{)},\cdots,\mathbb{P}\big{(}e_{d }\mid z\big{)}\Big{)}^{\top}.\]Let us figure out the sub-Gaussian parameter of the random vector \(\eta\). Fix any unit vector \(a\). First, we have \(\mathbb{E}[\langle\eta,a\rangle]=0\). Second, we have

\[\Big{|}\eta^{\top}a\Big{|} \leq\|\eta\|_{2}\] (Cauchy-Schwarz inequality) \[\leq\left\|x-\Big{(}\mathbb{P}\big{(}e_{1}\mid z\big{)},\cdots, \mathbb{P}\big{(}e_{d}\mid z\big{)}\Big{)}^{\top}\right\|_{2}\] \[\leq\left(\|x\|_{2}+\left\|\Big{(}\mathbb{P}\big{(}e_{1}\mid z \big{)},\cdots,\mathbb{P}\big{(}e_{d}\mid z\big{)}\Big{)}^{\top}\right\|_{2}\right)\] \[=2.\]

Thus, \(\eta^{\top}a\) is bounded and zero-mean and thus \(2^{2}\)-sub-Gaussian. This implies that

\[\forall\beta,\max_{a:\|a\|\leq 1}\mathbb{E}[\exp\!\big{(}\beta\langle \eta,a\rangle\big{)}]\leq\exp\!\left(\frac{\beta^{2}2^{2}}{2}\right).\]

and thus \(\eta\) is a \(2^{2}\)-sub-Gaussan random vector. Then, \(\eta^{\top}\theta\) is \((2\|\theta\|)^{2}\)-sub-Gaussian.

Using Lemma G.2, we have that \(\eta^{\top}\theta+\varepsilon\) is \(2(4\|\theta\|^{2}+1)\)-sub-Gaussian.

**Lemma G.2**.: _Let \(A\) and \(B\) random variables that are each \(\sigma_{A}^{2}\)- and \(\sigma_{B}^{2}\)-sub-Gaussian but are correlated. Then, \(A+B\) is \(2(\sigma_{A}^{2}+\sigma_{B}^{2})\)-sub-Gaussian._

Proof.: By definition of sub-Gaussian, we have for any \(\gamma\in\mathbb{R}\),

\[\mathbb{E}\Big{[}\exp\!\big{(}\gamma(A+B)\big{)}\Big{]} =\mathbb{E}\big{[}\exp(\gamma A)\exp(\gamma B)\big{]}\] (Cauchy-Schwarz) \[\leq\sqrt{\exp\!\big{(}2\gamma^{2}\sigma_{A}^{2}\big{)}}\sqrt{ \exp\!\big{(}2\gamma^{2}\sigma_{B}^{2}\big{)}}\] \[\leq\exp\!\big{(}2\gamma^{2}(\sigma_{A}^{2}+\sigma_{B}^{2})\big{)}.\]

### Proof of Lemma 2.2

_Lemma 2.2._ Suppose that \(T\) observations are collected non-adaptively from the structural equation model in Eqs. (1) and \(\Gamma\in\mathbb{R}^{d\times d}\) is known. Then, with probability at least \(1-\delta\) for \(\delta\in(0,1)\) and \(w\in\mathbb{R}^{d}\),

\[|w^{\top}(\widehat{\theta}_{\texttt{oracle}}-\theta)|\leq\sqrt{2\sigma_{ \nu}^{2}\|w\|_{\bar{A}(Z_{T},\Gamma)^{-1}}\log\!\big{(}2/\delta\big{)}}.\]

where \(\sigma_{\nu}^{2}\) is the sub-Gaussian parameter of the noise process \(\nu:=\eta^{\top}\theta+\varepsilon\) as characterized in Lemma 2.1.

Proof.: Given the knowledge of \(\Gamma\), we have the oracle 2SLS estimator

\[\widehat{\theta}_{\texttt{oracle}}=\left(\sum_{t=1}^{T}z_{s}\! \left(\Gamma^{\top}z_{s}\right)^{\top}\right)^{-1}\sum_{t=1}^{T}z_{s}y_{t}= \left(\sum_{t=1}^{T}z_{s}z_{s}^{\top}\Gamma\right)^{-1}\sum_{t=1}^{T}z_{s}y_{t}.\]

Note that

\[y_{t}=x_{t}^{\top}\theta+\varepsilon_{t}=\left(\Gamma^{\top}z_{t}\right)^{ \top}\theta+\eta_{t}^{\top}\theta+\varepsilon_{t}.\]Denote \(\nu_{t}:=\eta_{t}^{\top}\theta+\varepsilon_{t}\). For any \(w\in\mathcal{W}\), we have

\[\left\langle\widehat{\theta}_{\texttt{oracle}}-\theta,w\right\rangle =\left\langle\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top}\Gamma\right)^{ -1}\sum_{t=1}^{T}z_{s}y_{t}-\theta,w\right\rangle\] \[=\left\langle\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top}\Gamma\right)^{ -1}\sum_{t=1}^{T}z_{s}\Big{(}z_{s}^{\top}\Gamma\theta+\nu_{t}\Big{)}-\theta,w\right\rangle\] \[=\left\langle\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top}\Gamma\right)^{ -1}\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top}\Gamma\theta+\sum_{t=1}^{T}z_{s}\nu_{t} \right)-\theta,w\right\rangle\] \[=\left\langle\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top}\Gamma\right)^{ -1}\sum_{t=1}^{T}z_{s}\nu_{t},w\right\rangle\] \[=\sum_{q=1}^{t}\left\langle\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top} \Gamma\right)^{-1}z_{q},w\right\rangle\!\nu_{q}.\]

By Lemma G.1, the noise \(\nu_{t}\) is \(\sigma_{\nu}^{2}\)-sub-Gaussian, we have

\[\left\langle\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top}\Gamma\right)^{-1}z_{q},w \right\rangle\!\nu_{q}\]

is \(\left\langle\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top}\Gamma\right)^{-1}z_{q},w \right\rangle^{2}\sigma_{\nu}^{2}\)-sub-Gaussian. Thus

\[\mathbb{P}\!\left(\left\langle\widehat{\theta}_{\texttt{oracle}}-\theta,w \right\rangle\geq\sqrt{2\sum_{q=1}^{t}\!\left\langle\left(\sum_{t=1}^{T}z_{s} z_{s}^{\top}\Gamma\right)^{-1}z_{q},w\right\rangle^{2}\!\left(\frac{1}{\delta} \right)}\right)\leq\delta.\]

Concisely,

\[\sum_{q=1}^{t}\!\left\langle\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top }\Gamma\right)^{-1}\!z_{q},w\right\rangle^{2} =w^{\top}\!\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top}\Gamma\right)^{ -1}\!\left(\sum_{q=1}^{t}z_{q}z_{q}^{\top}\right)\!\left(\left(\sum_{t=1}^{T}z _{s}z_{s}^{\top}\Gamma\right)^{-1}\right)^{\top}w\] \[=w^{\top}\Gamma^{-1}\!\left(\left(\sum_{t=1}^{T}z_{s}z_{s}^{\top }\Gamma\right)^{-1}\right)^{\top}w\] \[=w^{\top}\!\left(\Gamma^{\top}\!\left(\sum_{t=1}^{T}z_{s}z_{s}^{ \top}\right)\Gamma\right)^{-1}w. \tag{11}\]

Thus,

\[\mathbb{P}\!\left(\left\langle\widehat{\theta}_{\texttt{oracle}}-\theta,w \right\rangle\geq\sqrt{2w^{\top}\!\left(\Gamma^{\top}\!\left(\sum_{t=1}^{T}z _{s}z_{s}^{\top}\right)\Gamma\right)^{-1}w\sigma_{\nu}^{2}\log\!\left(\frac{1}{ \delta}\right)}\right)\leq\delta.\]

We can further write it as

\[\mathbb{P}\!\left(\left\langle\widehat{\theta}_{\texttt{oracle}}-\theta,w \right\rangle\geq\sqrt{2\|w\|_{\left(\Gamma^{\top}\!\left(\sum_{t=1}^{T}z_{s} z_{s}^{\top}\right)\Gamma\right)^{-1}\sigma_{\nu}^{2}\log\!\left(\frac{1}{\delta} \right)}}\right)\leq\delta.\]

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

**Lemma G.4**.: _Suppose we have \(z_{1},\ldots,z_{T}\in\mathbb{R}^{d}\) and \(\eta_{1},\ldots,\eta_{T}\in\mathbb{R}^{d}\) such that \(\eta_{T}\)\(z_{1},\eta_{1},\ldots,z_{T-1},\eta_{T-1},z_{T}\) is \(\sigma_{\eta}^{2}\)-sub-Gaussian vector (defined in Assumption 1). Let \(Z,S\in\mathbb{R}^{T\times d}\) be matrices whose \(t\)-th row is \(z_{t}^{\top}\) and \(\eta_{t}^{\top}\) respectively. Suppose \(\|z_{t}\|\leq L_{z},\forall t\). Let \(V=Z^{\top}Z\). Then, \(\forall\delta\in(0,1)\), we have, with probability at least \(1-\delta\),_

\[\Big{\|}V^{-1/2}Z_{T}^{\top}S\Big{\|}_{\mathsf{op}}\leq\sigma_{\eta}\sqrt{8d \ln\left(1+\frac{2TL_{z}^{2}}{d(2\wedge\sigma_{\min}(V))}\right)+16\ln\left( \frac{2\cdot 6^{d}}{\delta}\cdot\log_{2}^{2}\left(\frac{4}{2\wedge\sigma_{\min}(V) }\right)\right)}.\]

_We abbreviate \(\overline{\log}(Z_{T},\delta):=8d\ln\left(1+\frac{2TL_{z}^{2}}{d(2\wedge\sigma _{\min}(V))}\right)+16\ln\left(\frac{2\cdot 6^{d}}{\delta}\cdot\log_{2}^{2} \left(\frac{4}{2\wedge\sigma_{\min}(V)}\right)\right)\)._

Proof.: By the definition of operator norm, we have

\[\Big{\|}V^{-1/2}Z_{T}^{\top}S\Big{\|}_{\mathsf{op}} =\sup_{\{x\|x\|_{2}=1\}}\Big{\|}V^{-1/2}Z_{T}^{\top}Sx\Big{\|}_{2}\] \[=\sup_{\{x\|x\|_{2}=1\}}\sqrt{x^{\top}S^{\top}Z_{T}V^{-1}Z_{T}^{ \top}Sx}\] \[=\sup_{\{x\|x\|_{2}=1\}}\Big{\|}Z_{T}^{\top}Sx\Big{\|}_{V^{-1}}.\]

Considering a fixed \(w\in\mathbb{R}^{d}\), by Lemma G.5, we have with probability at least \(1-\delta\),

\[\Big{\|}Z_{T}^{\top}Sx\Big{\|}_{V^{-1}}\leq\sqrt{2}\sigma_{\eta}\sqrt{d\ln \left(1+\frac{2TL_{z}^{2}}{d(2\wedge\sigma_{\min}(V))}\right)+2\ln\left(\frac{ 2}{\delta}\cdot\log_{2}^{2}\left(\frac{4}{2\wedge\sigma_{\min}(V)}\right) \right)}.\]

By Lemma G.6 and a union bound, for the \(\varepsilon\)-covering \(\mathcal{C}_{\varepsilon}\), we have the following event happens with probability no more than \(\delta\):

\[\mathcal{E}:=\left\{\exists x\in\mathcal{C}_{\varepsilon},\Big{\|}Z_{T}^{\top }Sx\Big{\|}_{V^{-1}}\geq\sqrt{2}\sigma_{\eta}\sqrt{d\ln\left(1+\frac{2TL_{z}^ {2}}{d(2\wedge\sigma_{\min}(V))}\right)+2\ln\left(\frac{2|\mathcal{C}_{ \varepsilon}|}{\delta}\cdot\log_{2}^{2}\left(\frac{4}{2\wedge\sigma_{\min}(V) }\right)\right)}\right\}.\]

We abbreviate \(\widehat{\log}(Z_{T},\delta):=\sqrt{2}\sqrt{d\ln\left(1+\frac{2TL_{z}^{2}}{d( 2\wedge\sigma_{\min}(V))}\right)+2\ln\left(\frac{2|\mathcal{C}_{\varepsilon }|}{\delta}\cdot\log_{2}^{2}\left(\frac{4}{2\wedge\sigma_{\min}(V)}\right)\right)}\)

When \(\mathcal{E}\) does not happen, we have

\[\Big{\|}V^{-1/2}Z^{\top}S\Big{\|}_{\mathsf{op}} =\sup_{\{x\|x\|=1\}}\Big{\|}Z_{T}^{\top}Sx\Big{\|}_{V^{-1}}\] \[=\sup_{\{x\|x=1\}}\min_{\{y\|y\in C_{\varepsilon}\}}\Big{\|}Z_{T} ^{\top}S(x-y+y)\Big{\|}_{V^{-1}}\] \[\leq\sup_{\{x\|x\|=1\}}\min_{\{y\|x\in C_{\varepsilon}\}}\Big{(} \Big{\|}Z_{T}^{\top}S(x-y)\Big{\|}_{V^{-1}}+\Big{\|}Z_{T}^{\top}Sy\Big{\|}_{V^{ -1}}\Big{)}\] \[\leq\sup_{\{x\|x\|=1\}}\min_{\{y\|x\|=0\}}\Big{(}\Big{\|}V^{-1/2} Z^{\top}S\Big{\|}_{\mathsf{op}}\Big{\|}x-y\Big{\|}_{2}+\sigma_{\eta}\widehat{ \log}(Z_{T},\delta)\Big{)}\] \[\leq\varepsilon\Big{\|}V^{-1/2}Z_{T}^{\top}S\Big{\|}_{\mathsf{op }}+\sigma_{\eta}\widehat{\log}(Z_{T},\delta).\]

Thus,

\[\Big{\|}V^{-1/2}Z_{T}^{\top}S\Big{\|}_{\mathsf{op}}\leq\frac{\sigma_{\eta}}{1- \varepsilon}\widehat{\log}(Z_{T},\delta).\]

By choosing \(\varepsilon=\frac{1}{2}\) and Lemma G.6, we have

\[\Big{\|}V^{-1/2}Z_{T}^{\top}S\Big{\|}_{\mathsf{op}}\leq\sigma_{\eta}\sqrt{8d \ln\left(1+\frac{2TL_{z}^{2}}{d(2\wedge\sigma_{\min}(V))}\right)+16\ln\left(\frac {2\cdot 6^{d}}{\delta}\cdot\log_{2}^{2}\left(\frac{4}{2\wedge\sigma_{\min}(V)}\right) \right)}.\]

[MISSING_PAGE_FAIL:27]

Proofs of sample complexity when given \(\Gamma\)

### Proof of Theorem 3.1

```
Input \(\mathcal{Z},\mathcal{W},\Gamma,\delta,\varepsilon,L_{\nu}\geq\sigma_{\nu}^{2}\) Initialize:\(k=1,\mathcal{W}_{k}=\mathcal{W}\) Define \(f(w,w^{\prime},\Gamma,\lambda):=\|w-w^{\prime}\|_{(\sum_{\varepsilon}\varepsilon \,\mathbb{Z}^{\top}\lambda_{s}zz^{\top}\Gamma)^{-1}}^{2}\) while\(\left|\mathcal{\hat{W}}_{k}\right|>1\)do \(\lambda_{k}=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime}\in \mathcal{W}_{k}}f(w,w^{\prime},\Gamma,\lambda)\) \(\rho(\mathcal{W}_{k})=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime} \in\mathcal{W}_{k}}f(w,w^{\prime},\Gamma,\lambda)\) \(\zeta_{k}=2^{-k}\) \(N_{k}=\left\lceil 2(1+\omega)\zeta_{k}^{-2}\rho(\mathcal{W}_{k})L_{\nu}\log \Bigl{(}\frac{4k^{2}|\mathcal{W}|}{\delta}\Bigr{)}\right\rceil\lor r(\omega)\) \(Z_{N_{k}}=\text{ROUND}(\lambda_{k},N_{k})\) Pull arms in \(Z_{N_{k}}\) and observe \(Y_{N_{k}}\)  Compute \(\widehat{\theta}^{k}_{\texttt{oracle}}=\left(Z_{N_{k}}^{\top}Z_{N_{k}} \Gamma\right)^{-1}Z_{N_{k}}^{\top}Y_{N_{k}}\) \(\mathcal{W}_{k+1}=\mathcal{W}_{k}\backslash\left\{w\in\mathcal{W}_{k}\mid\exists w ^{\prime}\in\mathcal{W}_{k},\text{s.t.},\left<w^{\prime}-w,\widehat{\theta}^{ k}_{\texttt{oracle}}\right>>\zeta_{k}\right\}\) \(k=k+1\) endwhile Output :\(\mathcal{W}_{k}\)
```

**Algorithm 5** Confounded pure exploration with known \(\Gamma\)

Theorem 3.1: Algorithm 1 is \(\delta\)-PAC for the CPET-LB problem and terminates in at most \(c(1+\omega)L_{\nu}\rho^{*}\log\bigl{(}1/\delta\bigr{)}+cr(\omega)\) samples, where \(c\) hides logarithmic factors of \(\Delta\) and \(|\mathcal{W}|\).

Proof: **Part 1 \(\delta\)-Pac**

By the confidence interval in Lemma 2.2, we have, with probability at least \(1-\frac{\delta}{2k^{2}|\mathcal{W}|}\),

\[\left|\left<\widehat{\theta}^{k}_{\texttt{oracle}}-\theta,w-w^{ *}\right>\right| \leq\|w-w^{*}\|_{\left(\Gamma^{\top}\left(\sum_{s=1}^{N_{k}}z_{ t_{s}}\tau_{t_{s}}^{\top}\right)\right)^{-1}}\sqrt{2\sigma_{\nu}^{2}\log \biggl{(}\frac{4k^{2}|\mathcal{W}|}{\delta}\biggr{)}}\] \[\leq\frac{\sqrt{1+\omega}\|w-w^{*}\|_{\left(\Gamma^{\top}\lambda_ {s}zz^{\top}\Gamma\right)^{-1}}}{\sqrt{N_{k}}}\sqrt{2\sigma_{\nu}^{2}\log \biggl{(}\frac{4k^{2}|\mathcal{W}|}{\delta}\biggr{)}}\] \[\leq\zeta_{k}.\]

Define a good event \(\mathcal{E}_{k,w}\) for each \(k\) and \(\mathcal{W}_{k}\) as

\[\mathcal{E}_{k,w}=\left\{\left|\left<\widehat{\theta}^{k}_{\texttt{oracle}}- \theta,w-w^{*}\right>\right|\leq\zeta_{k}\right\}.\]We claim that with probability at least \(1-\delta\), the event \(\bigcap_{k=1}^{\infty}\bigcap_{w\in\mathcal{W}_{k}}\mathcal{E}_{k,w}\) holds. It can be proved by a union bound.

\[\mathbb{P}\Bigg{(}\Bigg{(}\bigcap_{k=1}^{\infty}\bigcap_{w\in \mathcal{W}_{k}}\mathcal{E}_{w,k}\Bigg{)}^{c}\Bigg{)}\leq \sum_{k=1}^{\infty}\sum_{w\in\mathcal{W}_{k}}\mathbb{P}\Big{(} \mathcal{E}_{k,w}^{c}\Big{)}\] \[\leq \sum_{k=1}^{\infty}\sum_{w\in\mathcal{W}_{k}}\frac{\delta}{2k^{2} |\mathcal{W}|}\] \[\leq \frac{\delta}{2}\sum_{k=1}^{\infty}\frac{1}{k^{2}}\] \[\leq \delta, \tag{15}\]

where the last step is by the fact that \(\sum_{k=1}^{\infty}\frac{1}{k^{2}}=\frac{\pi^{2}}{6}<2\). Under the event \(\bigcap_{k=1}^{\infty}\bigcap_{w\in\mathcal{W}_{k}}\mathcal{E}_{w,k}\), to show that the best arm is never eliminated, it suffices to show that for any sub-optimal arm \(w\in\mathcal{W}_{k}\),

\[\Big{\langle}w-w^{*},\widehat{\theta}_{\texttt{oracle}}^{k}\Big{\rangle} =\Big{\langle}w-w^{*},\widehat{\theta}_{\texttt{oracle}}^{k}- \theta\Big{\rangle}+\langle w-w^{*},\theta\rangle\] \[\leq\Big{\langle}w-w^{*},\widehat{\theta}_{\texttt{oracle}}^{k} -\theta\Big{\rangle}\] \[\leq\zeta_{k}.\]

Thus the best arm \(w^{*}\) never satisfies the elimination condition. Next we show that at the end of stage \(k\), any suboptimal arm \(w\) that satisfies

\[\langle\theta,w^{*}-w\rangle\geq 2\zeta_{k}\]

is eliminated. To show this, we need to show that \(w\) satisfies the elimination condition,

\[\max_{w^{*}\in\mathcal{W}_{k}}\Big{\langle}\widehat{\theta}_{ \texttt{oracle}}^{k},w^{\prime}-w\Big{\rangle}\geq \Big{\langle}\widehat{\theta}_{\texttt{oracle}}^{k},w^{*}-w\Big{\rangle}\] \[= \Big{\langle}\widehat{\theta}_{\texttt{oracle}}^{k}-\theta,w^{*} -w\Big{\rangle}+\langle\theta,w^{*}-w\rangle\] \[\geq -\zeta_{k}+2\zeta_{k}\] \[= \zeta_{k}.\]

This implies that with probability at least \(1-\delta\), \(w^{*}\) always survives.

**Part 2 sample complexity**

Define \(S_{k}=\big{\{}w\in\mathcal{W}\mid\langle w^{*}-w,\theta\rangle\leq 4\zeta_{k} \big{\}}\). Thus with probability at least \(1-\delta\), we have \(\bigcap_{k}\big{\{}\mathcal{W}_{k}\subseteq S_{k}\big{\}}\). This implies the following is true with probability at least \(1-\delta\) for all \(k\)

\[\rho(\mathcal{W}_{k}) =\min_{\lambda\in\Delta(\Delta)}\max_{w,w^{\prime}\in\mathcal{W}_ {k}}\big{\|}w-w^{\prime}\big{\|}_{\left(\sum_{z\in Z}\Gamma^{\top}\lambda_{z} z^{z\top}\Gamma\right)^{-1}}^{2}\] \[\leq\min_{\lambda\in\Delta(\Delta)}\max_{w,w^{\prime}\in S_{k}} \big{\|}w-w^{\prime}\big{\|}_{\left(\sum_{z\in Z}\Gamma^{\top}\lambda_{z}z^{z \top}\Gamma\right)^{-1}}^{2}\] \[=\rho(S_{k}).\]

Define \(\Delta\) to be the minimum gap between \(w^{*}\) and any other \(w\in\mathcal{W}\), i.e., \(\Delta:=\min_{w\in\mathcal{W}\setminus\{w^{*}\}}\langle w^{*}-w,\theta\rangle\). Then for \(k\geq\left\lceil\log\!\left(4\Delta^{-1}\right)\right\rceil\), we have \(S_{k}=\{w^{*}\}\) with probability at least \(1-\delta\). The total sample complexity is the summation of the number of samples in each round,

\[\sum_{k=1}^{\left\lceil\log\left(4\Delta^{-1}\right)\right\rceil}N_{k}\] \[=\sum_{k=1}^{\left\lceil\log\left(4\Delta^{-1}\right)\right\rceil} \left(\left\lceil 4(1+\omega)\zeta_{k}^{-2}\rho(\mathcal{W}_{k})L_{\nu}\log \left(\frac{4k^{2}|\mathcal{W}|}{\delta}\right)\right\rceil\lor r(\omega)\right)\] \[\leq\sum_{k=1}^{\left\lceil\log\left(4\Delta^{-1}\right)\right\rceil} \left(8(1+\omega)2^{2k}\rho(\mathcal{W}_{k})L_{\nu}\log\left(\frac{4k^{2}| \mathcal{W}|}{\delta}\right)\lor r(\omega)\right)\] \[\leq\sum_{k=1}^{\left\lceil\log\left(4\Delta^{-1}\right)\right\rceil} \left(8(1+\omega)2^{2k}\rho(\mathcal{S}_{k})L_{\nu}\log\left(\frac{4k^{2}| \mathcal{W}|}{\delta}\right)\lor r(\omega)\right). \tag{16}\]

On the other hand, we have

\[\rho^{*}=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w\in\mathcal{W }\setminus\{w^{*}\}}\frac{\|w^{*}-w\|_{(\sum_{z\in Z}\Gamma^{\top}\lambda_{z} zz^{\top}\Gamma)^{-1}}^{2}}{\langle w^{*}-w,\theta\rangle^{2}}\] \[=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{k}\max_{w\in S_{k}} \frac{\|w^{*}-w\|_{(\sum_{z\in Z}\Gamma^{\top}\lambda_{z}zz^{\top}\Gamma)^{-1 }}^{2}}{\langle w^{*}-w,\theta\rangle^{2}}\] \[\geq \frac{1}{\left\lceil\log(4\Delta^{-1})\right\rceil}\min_{\lambda \in\Delta(\mathcal{Z})}\sum_{k=1}^{\left\lceil\log\left(4\Delta^{-1}\right) \right\rceil}\max_{w\in S_{k}}\frac{\|w^{*}-w\|_{(\sum_{z\in Z}\Gamma^{\top} \lambda_{z}zz^{\top}\Gamma)^{-1}}^{2}}{\langle w^{*}-w,\theta\rangle^{2}}\] \[\geq \frac{1}{16\left\lceil\log(4\Delta^{-1})\right\rceil}\sum_{k=1}^{ \left\lceil\log\left(4\Delta^{-1}\right)\right\rceil}2^{2k}\min_{\lambda\in \Delta(\mathcal{Z})}\max_{w\in S_{k}}\|w^{*}-w\|_{(\sum_{z\in Z}\Gamma^{\top} \lambda_{z}zz^{\top}\Gamma)^{-1}}^{2}\] \[\geq \frac{1}{64\left\lceil\log(4\Delta^{-1})\right\rceil}\sum_{k=1}^{ \left\lceil\log\left(4\Delta^{-1}\right)\right\rceil}2^{2k}\min_{\lambda\in \Delta(\mathcal{Z})}\max_{w,w^{\prime}\in S_{k}}\|w-w^{\prime}\|_{(\sum_{z \in Z}\Gamma^{\top}\lambda_{z}zz^{\top}\Gamma)^{-1}}^{2}\] \[= \frac{1}{64\left\lceil\log(4\Delta^{-1})\right\rceil}\sum_{k=1}^{ \left\lceil\log\left(4\Delta^{-1}\right)\right\rceil}2^{2k}\rho(S_{k}), \tag{17}\]

where the last inequality is by the triangle inequality, i.e.,

\[\max_{w,w^{\prime}\in S_{k}}\left\|w-w^{\prime}\right\|_{(\sum_{z \in Z}\Gamma^{\top}\lambda_{z}zz^{\top}\Gamma)^{-1}}^{2}\leq \max_{w,w^{\prime}\in S_{k}}\left(\left\|w-w^{*}\right\|_{(\sum_{z \in Z}\Gamma^{\top}\lambda_{z}zz^{\top}\Gamma)^{-1}}+\left\|w^{*}-w^{\prime} \right\|_{(\sum_{z\in Z}\Gamma^{\top}\lambda_{z}zz^{\top}\Gamma)^{-1}}\right) ^{2}\] \[\leq 4\max_{w\in S_{k}}\|w-w^{*}\|_{(\sum_{z\in Z}\Gamma^{\top} \lambda_{z}zz^{\top}\Gamma)^{-1}}^{2}.\]

Combining (16) and (17), we have

\[\sum_{k=1}^{\left\lceil\log\left(4\Delta^{-1}\right)\right\rceil}N_{k}\leq c(1+\omega)L_{\nu}\log\left(4\Delta^{-1}\right)\log \left(\frac{\log\left(4\Delta^{-1}\right)^{2}|\mathcal{W}|}{\delta}\right) \rho^{*}+\log\left(4\Delta^{-1}\right)r(\omega).\]

## Appendix I Proofs of sample complexity when given \(\widehat{\Gamma}\)

```
Input \(\mathcal{Z},\mathcal{W},\widehat{\Gamma},\delta,\varepsilon,L_{\nu}\geq\sigma_{\nu }^{2},L_{\eta}\geq\sigma_{\eta}^{2}\) Initialize:\(k=1,\hat{\mathcal{W}}_{k}=\mathcal{W}\), calculate \(\gamma\) using (18) Define \(f(w,w^{\prime},\Gamma,\lambda):=\|w-w^{\prime}\|_{(\sum_{\begin{subarray}{c}z \in Z\\ \epsilon\geq\Gamma^{\top}\lambda_{z}zz^{\top}\Gamma)^{-1}\end{subarray}}^{2}}\) while\(\exists w,w^{\prime}\in\mathcal{W}_{k},\text{s.t.},\left<w^{\prime}-w,\widehat{ \theta}_{\text{oracle}}^{k}\right>>4\gamma\) or \(\zeta_{k}>\gamma\)do \(\lambda_{k}=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime}\in \mathcal{W}_{k}}f(w,w^{\prime},\widehat{\Gamma},\lambda)\) \(\rho(\mathcal{W}_{k})=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime} \in\mathcal{W}_{k}}f(w,w^{\prime},\widehat{\Gamma},\lambda)\) \(\zeta_{k}=2^{-k}\) \(N_{k}=\left[2(1+\omega)\Big{(}\zeta_{k}^{-2}\wedge\frac{1}{\gamma^{2}}\Big{)} \rho(\mathcal{W}_{k})L_{\nu}\log\left(\frac{4k^{2}|\mathcal{W}|}{\delta} \right)\right]\lor r(\omega)\) \(Z_{N_{k}}=\text{ROUND}(\lambda_{k},N_{k})\)  Pull arms in \(Z_{N_{k}}\) and observe \(Y_{N_{k}}\)  Compute \(\widehat{\theta}_{\text{P-2ELS}}^{k}=\left(Z_{N_{k}}^{\top}Z_{N_{k}}\widehat{ \Gamma}\right)^{-1}Z_{N_{k}}^{\top}Y_{N_{k}}\) \(\mathcal{W}_{k+1}=\mathcal{W}_{k}\setminus\left\{w\in\mathcal{W}_{k}\mid\exists w ^{\prime}\in\mathcal{W}_{k},\text{s.t.},\left<w^{\prime}-w,\widehat{\theta}_{ \text{P-2ELS}}^{k}\right>\zeta_{k}+\gamma\right\}\) \(k=k+1\) endwhile Output: any \(w\in\mathcal{W}_{k}\)
```

**Algorithm 6** Confounded pure exploration with known \(\widehat{\Gamma}\)

**Theorem I.1**.: _Suppose that we have \(\widehat{\Gamma}\) that is an OLS estimate from an offline dataset \(\{Z_{T},X_{T}\}\) collected non-adaptively through a fixed design \(\xi\) and the efficient rounding procedure ROUND,as well as \(T\geq\frac{4\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\xi,\Gamma)\big{)}} \overline{\log}\big{(}Z_{T},\delta/2\big{)}\). Algorithm 6 guarantees that with probability at least \(1-\delta\), a \(6\gamma\)-good arm is returned, where_

\[\gamma:=\max_{w,w^{\prime}\in\mathcal{W}}\bigl{\|}w-w^{\prime} \bigr{\|}_{\widehat{A}(Z_{T},\widehat{\Gamma})^{-1}}\|\theta\|_{2}\sqrt{L_{ \eta}\overline{\log}(Z_{T},\delta)}. \tag{18}\]

_Also, the algorithm terminates in at most_

\[c(1+\omega)L_{\nu}\log\bigl{(}1/\delta\bigr{)}\rho^{*}(\gamma)+ c(1+\omega)^{2}L_{\nu}\log\bigl{(}1/\delta\bigr{)}\frac{\sigma_{\eta}^{2} \overline{\log}(Z_{T},\delta)}{\sigma_{\min}\big{(}A(\xi,\Gamma)\big{)}}\frac{ \rho(\xi,\gamma)}{T}\lor cr(\omega)\]

_samples, where \(c\) hides logarithmic factors of \(\Delta\) and \(|\mathcal{W}|\) and \(\gamma\)._

\[\rho(\lambda,\gamma):=\max_{w\in\mathcal{W}\setminus\{w^{*}\}} \frac{\|w^{*}-w\|_{A(\lambda,\Gamma)^{-1}}^{2}}{\langle w^{*}-w,\theta\rangle ^{2}\vee\gamma^{2}}\]

_and_

\[\rho^{*}(\gamma)=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w\in \mathcal{W}\setminus\{w^{*}\}}\frac{\|w^{*}-w\|_{A(\lambda,\Gamma)^{-1}}^{2} }{\langle w^{*}-w,\theta\rangle^{2}\vee\gamma^{2}}.\]

Proof.: The proof can be divided into four steps:

* The best arm \(w^{*}\) is never eliminated.
* At the end of stage \(k\), any suboptimal arm \(w\) that satisfies \(\langle\theta,w^{*}-w\rangle\geq 2\zeta_{k}+2\gamma\) is eliminated.
* The stopping condition is met in finite time.
* When the stopping condition is met, there are only \(6\gamma\)-good arms left.

* The upper bound of the sample complexity.

**Step 1: The best arm \(w^{*}\) is never eliminated.**

By the confidence interval in Theorem 2.3, we have, with probability at least \(1-\frac{\delta}{2k^{2}|\mathcal{W}|}\), for any \(k\) and \(w\in\mathcal{W}_{k}\),

\[\left|\left\langle\widehat{\theta}^{k}_{\text{P-2SLS}}-\theta,w-w^ {*}\right\rangle\right| \leq\|w-w^{*}\|_{\bar{A}(Z_{N_{k}},\widehat{\Gamma})^{-1}}\sqrt{2 \sigma_{\nu}^{2}\log\!\left(\frac{4k^{2}|\mathcal{W}|}{\delta}\right)}+\gamma\] \[\leq\frac{\sqrt{1+\omega}\|w-w^{*}\|_{A(\lambda_{k},\widehat{ \Gamma})^{-1}}}{\sqrt{N_{k}}}\sqrt{2\sigma_{\nu}^{2}\log\!\left(\frac{4k^{2}| \mathcal{W}|}{\delta}\right)}+\gamma\] \[\leq\frac{\sqrt{1+\omega}\|w-w^{*}\|_{A(\lambda_{k},\widehat{ \Gamma})^{-1}}}{\sqrt{\left[2(1+\omega)\zeta_{k}^{-2}\rho(\mathcal{W}_{k})L_{ \nu}\log\!\left(\frac{4k^{2}|\mathcal{W}|}{\delta}\right)\right]\vee r(\omega) }}\sqrt{2\sigma_{\nu}^{2}\log\!\left(\frac{4k^{2}|\mathcal{W}|}{\delta}\right) }+\gamma\] \[\leq\zeta_{k}+\gamma.\]

Define a good event \(\mathcal{E}_{k,w}\) for each \(k\) and \(w\in\mathcal{W}_{k}\) as

\[\mathcal{E}_{k,w}=\left\{\left|\left\langle\widehat{\theta}^{k}_{\text{P-2SLS }}-\theta,w-w^{*}\right\rangle\right|\leq\zeta_{k}+\gamma\right\}.\]

By the same calculation as (15), we claim that with probability at least \(1-\delta\), the event \(\bigcap_{k=1}^{\infty}\bigcap_{w\in\mathcal{W}_{k}}\mathcal{E}_{k,w}\) holds. Under the event \(\bigcap_{k=1}^{\infty}\bigcap_{w\in\mathcal{W}_{k}}\mathcal{E}_{w,k}\), to show that the best arm is never eliminated, it suffices to show that for any sub-optimal arm \(w\in\mathcal{W}_{k}\),

\[\left\langle w-w^{*},\widehat{\theta}^{k}_{\text{P-2SLS}}\right\rangle =\left\langle w-w^{*},\widehat{\theta}^{k}_{\text{P-2SLS}}-\theta \right\rangle+\left\langle w-w^{*},\theta\right\rangle\] \[\leq\left\langle w-w^{*},\widehat{\theta}^{k}_{\text{P-2SLS}}-\theta\right\rangle\] \[\leq\zeta_{k}+\gamma.\]

Thus the best arm \(w^{*}\) never satisfies the elimination condition.

**Step 2: At the end of stage \(k\), any suboptimal arm \(w\) that satisfies \(\left\langle\theta,w^{*}-w\right\rangle\geq 2\zeta_{k}+2\gamma\) is eliminated.**

To prove this, we show that such arm \(w\) must satisfy the elimination condition,

\[\max_{w^{\prime}\in\mathcal{W}_{k}}\langle\widehat{\theta}^{k}_{ \text{P-2SLS}},w^{\prime}-w\rangle\geq \Big{\langle}\widehat{\theta}^{k}_{\text{P-2SLS}},w^{*}-w\Big{\rangle}\] \[= \Big{\langle}\widehat{\theta}^{k}_{\text{P-2SLS}}-\theta,w^{*}-w \Big{\rangle}+\left\langle\theta,w^{*}-w\right\rangle\] \[\geq -\zeta_{k}-\gamma+2\zeta_{k}+2\gamma\] \[= \zeta_{k}+\gamma.\]

Thus the arm \(w\) is eliminated.

**Step 3: The stopping condition is met in finite time.**

Given the result in Step 2 and the fact that \(\zeta_{k}\) is an exponentially decreasing sequence, we know that all of the arms \(w\) satisfying \(\left\langle\theta,w^{*}-w\right\rangle>2\gamma\) will be eliminated in finite time. This means that only the arms \(w\) satisfying \(\left\langle\theta,w^{*}-w\right\rangle\leq 2\gamma\) will remain. We need to show that \(\forall w,w^{\prime}\in\mathcal{W}_{k},\left\langle w^{\prime}-w,\widehat{ \theta}^{k}_{\text{P-2SLS}}\right\rangle\leq 4\gamma\) can be achieved in finite time. When \(\zeta_{k}\leq\gamma\), (which will happen in finite time),

\[\left\langle w^{\prime}-w,\widehat{\theta}^{k}_{\text{P-2SLS}}\right\rangle= \left\langle w^{\prime}-w+w^{*}-w^{*},\widehat{\theta}^{k}_{\text {P-2SLS}}\right\rangle\] \[= \left\langle w^{*}-w,\widehat{\theta}^{k}_{\text{P-2SLS}}\right\rangle +\left\langle w^{\prime}-w^{*},\widehat{\theta}^{k}_{\text{P-2SLS}}\right\rangle\] \[= \left\langle w^{*}-w,\widehat{\theta}^{k}_{\text{P-2SLS}}-\theta+ \theta\right\rangle+\left\langle w^{\prime}-w^{*},\widehat{\theta}^{k}_{\text {P-2SLS}}-\theta+\theta\right\rangle\] \[= \left\langle w^{*}-w,\widehat{\theta}^{k}_{\text{P-2SLS}}-\theta \right\rangle+\left\langle w^{*}-w,\theta\right\rangle+\left\langle w^{\prime }-w^{*},\widehat{\theta}^{k}_{\text{P-2SLS}}-\theta\right\rangle+\left\langle w ^{\prime}-w^{*},\theta\right\rangle\] \[\leq \zeta_{k}+\gamma+2\gamma+\zeta_{k}+\gamma\] \[= 4\gamma.\]

**Step 4: When the stopping condition is met, there are only \(6\gamma\)-good arms left.**

For any \(w\in\mathcal{W}_{k}\), we have

\[\left\langle w^{*}-w,\theta\right\rangle= \left\langle w^{*}-w,\theta-\widehat{\theta}^{k}_{\text{P-2SLS}} \right\rangle+\left\langle w^{*}-w,\widehat{\theta}^{k}_{\text{P-2SLS}}\right\rangle\] \[\leq \zeta_{k}+\gamma+4\gamma\] \[= 6\gamma.\]

**Step 5: The upper bound of the sample complexity.**

Define \(\mathcal{W}(2\gamma)\) as the set of \(2\gamma\)-good arms, i.e., \(\mathcal{W}(2\gamma):=\left\{w\in\mathcal{W}\mid\left\langle\theta,w^{*}-w \right\rangle\leq 2\gamma\right\}\). Then the best arm in the set \(\mathcal{W}\backslash\mathcal{W}(2\gamma)\) has a suboptimality gap \(\min_{w\in\mathcal{W}\backslash\mathcal{W}(2\gamma)}(\theta,w^{*}-w)\). We define \(\Delta_{\min}(2\gamma):=\min_{w\in\mathcal{W}\backslash\mathcal{W}(2\gamma)} \left\langle\theta,w^{*}-w\right\rangle-2\gamma\). By the result in Step 2, we know that after \(k^{*}:=\left\lceil\log\!\left(4\Delta_{\min}(2\gamma)^{-1}\right)\right\rceil\) stages, all of the arms in \(\mathcal{W}\backslash\mathcal{W}(2\gamma)\) are eliminated. Define \(S_{k}=\left\{w\in\mathcal{W}\mid\left\langle w^{*}-w,\theta\right\rangle\leq 4 \zeta_{k}+2\gamma\right\}\). Thus with probability at least \(1-\delta\), we have \(\bigcap_{k}\left\{\mathcal{W}_{k}\subseteq S_{k}\right\}\).

The sample complexity of the algorithm is the total number of samples pulled, which is

\[\sum_{k=1}^{k^{*}}N_{k}\] \[= \sum_{k=1}^{k^{*}}\!\left(\left\lceil 2(1+\omega)\!\left(\zeta_{k} ^{-2}\wedge\frac{1}{\gamma^{2}}\right)\!\rho(\mathcal{W}_{k})L_{\nu}\log\! \left(\frac{4k^{2}|\mathcal{W}|}{\delta}\right)\right\rceil\lor r(\omega)\right)\] \[\leq \sum_{k=1}^{k^{*}}\!\left(3(1+\omega)\!\left(2^{2k}\wedge\frac{1 }{\gamma^{2}}\right)\!\rho(\mathcal{W}_{k})L_{\nu}\log\!\left(\frac{4k^{2}| \mathcal{W}|}{\delta}\right)\lor r(\omega)\right)\] \[\leq \sum_{k=1}^{k^{*}}\!\left(3(1+\omega)\!\left(2^{2k}\wedge\frac{1 }{\gamma^{2}}\right)\!\rho(\mathcal{S}_{k})L_{\nu}\log\!\left(\frac{4k^{2}| \mathcal{W}|}{\delta}\right)\lor r(\omega)\right). \tag{19}\]

Note that the factor of \(\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime}\in S_{k}}\|w-w^{\prime }\|_{A(\lambda,\Gamma)^{-1}}^{2}\) has the underlying true \(\Gamma\) in it, while the algorithm uses the plugged-in \(\widehat{\Gamma}\). We need to relate it to \(\rho(S_{k}):=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime}\in S_{k}} \|w-w^{\prime}\|_{A(\lambda,\widehat{\Gamma})^{-1}}^{2}\). By Lemma J.9, and defining \(\lambda_{z}^{*}(S_{k})\) as the optimal design for \(S_{k}\), i.e.,

\[\lambda_{k}^{*}=\operatorname*{arg\,min}_{\lambda\in\Delta(\mathcal{Z})}\max_{ w,w^{\prime}\in S_{k}}\|w-w^{\prime}\|_{A(\lambda,\Gamma)^{-1}}^{2}.\]

[MISSING_PAGE_EMPTY:34]

Proofs of sample complexity with unknown \(\Gamma\)

```
Input\(\mathcal{Z},\mathcal{W},\delta,L_{\nu}\geq\sigma_{\nu}^{2},L_{\eta}\geq\sigma_{\eta} ^{2},\omega,\gamma_{\min}\leq\lambda_{\min}(\Gamma),\lambda_{E},\kappa_{0}\) Initialize:\(k=1,\mathcal{W}_{1}=\mathcal{W},\widehat{\Gamma}_{0}=\perp,\zeta_{1}=1\) Define\(f(w,w^{\prime},\Gamma,\lambda):=\left\lVert w-w^{\prime}\right\rVert_{(\sum_{z\in Z} \Gamma^{\top}\lambda_{z}zz^{\top}\Gamma)^{-1}}^{2}\), \(M:=\frac{32L_{\eta}}{\gamma_{\min}^{2}\sigma_{\min}\big{(}A\left(\lambda_{E},I \right)\big{)}}\lor 1\), \(\delta_{k,\ell}:=\frac{\delta}{4\lambda_{E}^{2}\ell^{2}}\) while\(|\mathcal{W}_{k}|>1\)do \(\widehat{\Gamma}_{k}=\Gamma-\texttt{estimator}\Big{(}\mathcal{W}_{k},\widehat{ \Gamma}_{k-1},\zeta_{k},\delta,k,\omega,\lambda_{E},M,L_{\eta}\Big{)}\)\(\triangleright\) Step 1: update \(\widehat{\Gamma}\) \(\widehat{\theta}_{\texttt{P-2ELS}}^{k}=\theta-\texttt{estimator}\Big{(} \mathcal{W}_{k},\delta,\zeta_{k},\widehat{\Gamma}_{k},\omega,L_{\nu},k\Big{)}\)\(\triangleright\) Step 2: update \(\widehat{\theta}\) \(\mathcal{W}_{k+1}=\mathcal{W}_{k}\setminus\left\{w\in\mathcal{W}_{k}\mid\exists w ^{\prime}\in\mathcal{W}_{k},\texttt{s.t.},\Big{\langle}w^{\prime}-w,\widehat{ \theta}_{\texttt{P-2ELS}}^{k}\Big{\rangle}>\zeta_{k}\right\}\)\(\triangleright\) Step 3: elimination \(k\gets k+1\), \(\zeta_{k}=2^{-k}\) endwhile Output:\(\mathcal{W}_{k}\)
```

**Algorithm 7** Optimal design with unknown \(\Gamma\)

The algorithms of \(\Gamma-\texttt{estimator}\) and \(\theta-\texttt{estimator}\) we present below are slightly different from the one in the main text. In the main text, we omit the phase index \(k\) in the algorithm for simplicity.

```
Input\(\mathcal{W}_{k},\widehat{\Gamma}_{k-1},\zeta_{k},\delta,k,\omega,\lambda_{E},M,L_{\eta}\) Define\(\texttt{Stop}(\mathcal{W},Z,\Gamma,\delta):=\max_{w,w^{\prime}\in\mathcal{W}} \bigl{\lVert}w-w^{\prime}\bigr{\rVert}_{\bar{A}\left(Z,\Gamma\right)^{-1}} \lVert\theta\rVert_{2}\sqrt{L_{\eta}\overline{\log}(Z,\delta)}\) Initialize\(\ell=1\), \(N_{k,0,0}=0\)\(\triangleright\) doubling trick initialization if\(k=1\)then while\(\ell=1\) or \(\texttt{Stop}\Big{(}\mathcal{W}_{k},Z_{k,0,\ell},\widehat{\Gamma}_{k},\delta_{k, \ell}\Big{)}>1\)do  get \(2^{\ell-1}\Big{(}r(\omega)\vee\frac{2}{\kappa_{0}}\Big{)}\) samples denoted as \(\left\{Z_{k,0,\ell},X_{k,0,\ell},Y_{k,0,\ell}\right\}\) per design \(\lambda_{E}\triangleright\) via ROUND Update \(\widehat{\Gamma}_{k}\) by OLS on \(\left\{Z_{k,0,\ell},X_{k,0,\ell}\right\}\), \(\ell\leftarrow\ell+1\) endwhile else \(\widehat{\lambda}_{k}=\arg\min_{\lambda\in\Delta\left(Z\right)}\max_{w,w^{ \prime}\in\mathcal{W}_{k}}f(w,w^{\prime},\widehat{\Gamma}_{k-1},\lambda)\) \(N^{\prime}=\left\lfloor 4gdM\ln\Big{(}1+2M\big{(}d+L_{z}^{2}\big{)}+2M2gdM\Big{)}+8M\ln\Big{(} \frac{2\cdot 6^{d}}{\delta_{k,1}}\Big{)}\lor r(\omega)\right\rfloor\) while\(\ell=1\) or \(\texttt{Stop}\Big{(}\mathcal{W}_{k},Z_{k,0,\ell}\cup Z_{k,1,\ell},\widehat{\Gamma}_{k}, \delta_{k,\ell}\Big{)}>\zeta_{k}\)do \(N_{k,1,\ell}=2^{\ell}N^{\prime}\) \(\triangleright\) doubling trick update  get \(N_{k,1,\ell}\) samples per \(\tilde{\lambda}_{k}\) denoted as \(\left\{Z_{k,1,\ell},X_{k,1,\ell},Y_{k,1,\ell}\right\}\)\(\triangleright\) via ROUND \(N_{k,0,\ell}=\left\lceil 2gdM\ln\Big{(}M\big{(}d+N_{k,1,\ell}+L_{z}^{2}\big{)} \Big{)}+4M\ln\Big{(}\frac{2\cdot 6^{d}}{\delta_{k,\ell}}\Big{)}\lor r(\omega)\vee\frac{2}{ \kappa_{0}}\right\rceil\)  get \(\left(N_{k,0,\ell}-N_{k,0,\ell-1}\right)\) samples per \(\lambda_{E}\) augmented to \(\left\{Z_{k,0,\ell-1},X_{k,0,\ell-1}\right\}\) and get \(\left\{Z_{k,0,\ell},X_{k,0,\ell}\right\}\) Update \(\widehat{\Gamma}_{k}\) by OLS on \(\left\{Z_{k,0,\ell}\cup Z_{k,1,\ell},X_{k,0,\ell}\cup X_{k,1,\ell}\right\}\), \(\ell\leftarrow\ell+1\) endwhile endif Output:\(\widehat{\Gamma}_{k}\)
```

**Algorithm 8**\(\Gamma-\texttt{estimator}\)

**Lemma J.1**.: _Algorithm 3 and \(N_{k,1,\ell},N_{k,0,\ell}\) guarantees three properties:_

* _Property 1:_ \(N_{k,1,\ell}\geq N_{k,0,\ell}\)_._
* _Property 2:_ \(\frac{1}{2}\big{(}N_{k,0,\ell}+N_{k,1,\ell}\big{)}\leq N_{k,0,\ell-1}+N_{k,1, \ell-1}\)_._
* _Property 3:_ \(\frac{N_{k,1,1}}{8d\ln\left(1+\frac{N_{k,1,1}L_{2}^{2}}{d}\right)+16\ln\left(2 \frac{2\cdot d^{d}k^{2}}{d}\right)}\leq M\ln(dM)\)_._

Proof.: For Property 1, recall that \(N_{k,0,\ell}\) and \(N_{k,1,\ell}\) are defined as

\[N_{k,0,\ell}=\left\lceil 2gdM\ln\left(M\Big{(}d+N_{k,1,\ell}+L_{z}^{2} \Big{)}\right)+4M\ln\left(\frac{2\cdot 6^{d}}{\delta_{k,\ell}}\right)\lor r( \omega)\right\rceil\]

and

\[N_{k,1,\ell}=2^{\ell}\left\lfloor 4gdM\ln\left(1+2M\Big{(}d+L_{z}^{2} \Big{)}+2M2gdM\right)+8M\ln\left(\frac{2\cdot 6^{d}}{\delta_{k,1}}\right)\lor r( \omega)\right\rfloor.\]

We prove the result by induction. For the result for \(\ell=1\), note that \(N_{k,1,1}\) is of the form \(N_{k,1,1}=2a+2b\ln(1+2c+2bd)\) and \(N_{k,0,1}\) is of the form \(N_{k,0,1}=a+b\ln(c+dr)\), where

* \(a=4M\ln\left(\frac{2\cdot 6^{d}}{\delta_{k,1}}\right)\)
* \(b=2gdM\)
* \(c=M\big{(}d+L_{z}^{2}\big{)}\)
* \(d=M\)
* \(r=N_{k,1,\ell}\).

By the contraposition of Lemma K.5, we have

\[r>2a+2b\ln(1+2c+2bd)\implies r>a+b\ln(c+dr).\]

Thus we have \(N_{k,1,1}\geq N_{k,0,1}\). Now we assume the result holds for \(\ell\), i.e., \(N_{k,1,\ell}\geq N_{k,0,\ell}\) and prove that it holds for \(\ell+1\). We have

\[N_{k,1,\ell+1}=2N_{k,1,\ell}\geq 2N_{k,0,\ell}.\]

It suffices to prove that

\[2N_{k,0,\ell}\geq N_{k,0,\ell+1}.\]We have

\[2N_{k,0,\ell}\] \[= 2\left[2gdM\ln\left(M\Big{(}d+N_{k,1,\ell}+L_{z}^{2}\Big{)}\right)+ 4M\ln\left(\frac{2\cdot 6^{d}}{\delta_{k,\ell}}\right)\lor r(\omega)\right]\] \[\geq 2gdM\ln\left(M^{2}\Big{(}d+N_{k,1,\ell}+L_{z}^{2}\Big{)}^{2} \right)+8M\ln\left(\frac{2\cdot 6^{d}\ell^{2}}{\delta_{k,1}}\right)\lor r(\omega)\] \[\geq 2gdM\ln\left(M^{2}\Big{(}d+N_{k,1,\ell}+L_{z}^{2}\Big{)}^{2} \right)+8M\left(\ln\left(\frac{2\cdot 6^{d}}{\delta_{k,1}}\right)+2\ln(\ell) \right)\lor r(\omega)\] \[\geq 2gdM\ln\left(M^{2}\Big{(}d+N_{k,1,\ell}+L_{z}^{2}\Big{)}^{2} \right)+4M\left(\ln\left(\frac{2\cdot 6^{d}}{\delta_{k,1}}\right)+2\ln(\ell+1) \right)\lor r(\omega)\] \[(2\ln\ell\geq\ln(\ell+1)\text{ for }\ell\geq 2)\] \[\geq 2gdM\ln\left(M\Big{(}d+2N_{k,1,\ell}+L_{z}^{2}\Big{)}\right)+4M \ln\left(\frac{2\cdot 6^{d}\left(\ell+1\right)^{2}}{\delta_{k,1}}\right)\lor r (\omega)\] \[= N_{k,0,\ell+1}.\]

For Property 2, it suffices to prove that

\[\frac{1}{2}N_{k,0,\ell}\leq N_{k,0,\ell-1}.\]

This is equivalent to prove that

\[\frac{1}{2}\big{(}a+b\ln(c+2dr)\big{)}\leq a+b\ln(c+dr).\]

We have

\[\frac{1}{2}\big{(}a+b\ln(c+2dr)\big{)}\leq a+b\ln(c+dr)\] \[\Longleftrightarrow\frac{1}{2}b\ln(c+2dr)\leq\frac{1}{2}a+b\ln(c +dr)\] \[\Longleftrightarrow b\ln\Bigl{(}\sqrt{c+2dr}\Bigr{)}\leq\frac{1}{2}a+b\ln(c+dr)\] \[\Longleftarrow\sqrt{c+2dr}\leq c+dr\] \[\Longleftrightarrow c+2dr\leq c^{2}+2cdr+d^{2}r^{2}\] \[\Longleftrightarrow dr(dr+2c-2)+c^{2}-c\geq 0.\]

The last inequality holds because \(c=M>1\).

For Property 3, we have,

\[\frac{4dM\ln\left(1+2M\big{(}d+L_{z}^{2}\big{)}+2MdM\right)+8M\ln \left(\frac{2\cdot 6^{d}k^{2}}{\delta}\right)}{8d\ln\left(1+\frac{N_{k,1,1}L_{z}^{2}}{ d}\right)+16\ln\left(\frac{2\cdot 6^{d}k^{2}}{\delta}\right)}\] \[\leq \frac{4dM\ln\left(1+2M\big{(}d+L_{z}^{2}\big{)}+2MdM\right)}{8d \ln\left(1+\frac{N_{k,1,1}L_{z}^{2}}{d}\right)+16\ln\left(\frac{2\cdot 6^{d}k^{2}}{ \delta}\right)}+\frac{8M\ln\left(\frac{2\cdot 6^{d}k^{2}}{\delta}\right)}{8d\ln \left(1+\frac{N_{k,1,1}L_{z}^{2}}{d}\right)+16\ln\left(\frac{2\cdot 6^{d}k^{2}}{ \delta}\right)}\] \[\leq \frac{M\ln\left(1+2M\big{(}d+L_{z}^{2}\big{)}+2MdM\right)}{2\ln \left(1+ML_{z}^{2}\right)}+\frac{M}{2}\] (loosely apply \[N_{k,1,1}\geq dM\] ) \[\leq M\ln(dM).\]

**Theorem J.2**.: _Algorithm 3 guarantees that with probability at least \(1-\delta\), the best arm is returned, and the algorithm terminates in at most_

\[(1+\omega)\Bigg{(}\bigg{(}L_{\nu}\log\bigl{(}1/\delta\bigr{)}+L_{ \eta}\|\theta\|_{2}^{2}\Bigl{(}d+\log\bigl{(}1/\delta\bigr{)}\Bigr{)}\bigg{)} \rho^{*}+\Bigl{(}d+\log\bigl{(}1/\delta\bigr{)}\Bigr{)}\Bigl{(}L_{\eta}\| \theta\|_{2}^{2}\rho_{0}+M\Bigr{)}\Bigg{)}\]

_pulls, ignoring both of the additive and multiplicative logarithms of \(\Delta,|\mathcal{W}|,\rho^{*},\rho_{0},M\), where_

\[\rho^{*}=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w\in\mathcal{ W}\setminus\{w^{*}\}}\frac{\|w^{*}-w\|_{(\sum_{z\in\mathcal{Z}}\lambda_{z} \Gamma^{\top}zz^{\top}\Gamma)^{-1}}^{2}}{\langle w^{*}-w,\theta\rangle^{2}},\]

_and_

\[\rho_{0}=\max_{w\in\mathcal{W}\setminus\{w^{*}\}}\|w^{*}-w\|_{( \sum_{z\in\mathcal{Z}}\lambda_{E,z}\Gamma^{\top}zz^{\top}\Gamma)^{-1}}^{2},\]

_and_

\[M=\frac{32L_{\eta}}{\gamma_{\min}^{2}\sigma_{\min}\bigl{(}A( \lambda_{E},I)\bigr{)}}\lor 1.\]

_Note that \(\rho_{0}\) does not get hurt by \(\langle w^{*}-w,\theta\rangle\). It comes from the fact that in the first phase, we initialize that algorithm with E-optimal design._

Proof.: **Part 1: correctness of the algorithm**

The idea of the proof is similar to the proof of Theorem 3.1.

Recall that the confidence interval of P-2SLS can be break down into two terms (12).

Given \(\widehat{\Gamma}\), for a \(w\in\mathcal{W}\), with probability at least \(1-\frac{\delta}{2}\) the first term satisfies

\[\sum_{q=1}^{T_{2}}\!\left\langle\left(\sum_{t=1}^{T_{2}}z_{I_{t}} z_{I_{t}}^{\top}\widehat{\Gamma}\right)^{-1}z_{I_{q}},w\right\rangle\!\nu_{q} \leq\|w\|_{\tilde{A}(Z_{T_{2}},\widehat{\Gamma})^{-1}}\sqrt{2\sigma_{\nu}^{2} \log\biggl{(}\frac{4}{\delta}\biggr{)}}.\]

For any \(w\in\mathcal{W}\), with probability at least \(1-\frac{\delta}{2}\), the second term satisfies

\[\left\langle\left(\widehat{\Gamma}^{-1}-\Gamma^{-1}\right) \Gamma\theta,w\right\rangle\leq\|w\|_{\tilde{A}(Z_{T_{1}},\widehat{\Gamma})^{- 1}}\|\theta\|_{2}\sqrt{\sigma_{\eta}^{2}\overline{\log}\bigl{(}Z_{T_{1}}, \delta/4\bigr{)}}.\]

Note that by Lemma G.4, the above inequality holds for all \(w\in\mathcal{W}\), and the RHS is essentially a result of

\[\left\|V^{-1/2}Z_{T}^{\top}S\right\|_{\text{op}}\leq\sqrt{\sigma_{ \eta}^{2}\overline{\log}\bigl{(}Z_{T},\delta/4\bigr{)}}.\]

In the vanilla form of the confidence of P-2SLS, we can define good events as

* for the first term, for any \(w\in\mathcal{W}\), \[\sum_{q=1}^{T_{2}}\!\left\langle\left(\sum_{t=1}^{T_{2}}z_{I_{t }}z_{I_{t}}^{\top}\widehat{\Gamma}\right)^{-1}z_{I_{q}},w\right\rangle\!\nu_{q }\leq\|w\|_{\tilde{A}(Z_{T_{2}},\widehat{\Gamma})^{-1}}\sqrt{2\sigma_{\nu}^{2 }\log\bigl{(}16k^{2}|\mathcal{W}|/\delta\bigr{)}}.\]
* for the second term, \(\left\|V^{-1/2}Z_{T}^{\top}S\right\|_{\text{op}}\leq\sqrt{\sigma_{\eta}^{2} \overline{\log}\bigl{(}Z_{T},\delta/4\bigr{)}}\).

For our algorithm design, since we use the doubling trick for the first sub-phase, we need to define the good event for the first sub-phase as the samples from each doubling trick iteration satisfies the self-normalized concentration inequality of Lemma G.4.

We define the good event for \(\ell\)-th doubling trick iteration in the first sub-phase of phase \(k\) as

\[\mathcal{E}^{1}_{k,\ell}=\left\{\left\|V_{k,\ell}^{-1/2}Z_{k,\ell}^{\top}S_{k, \ell}\right\|_{\text{op}}^{2}\leq\sigma_{\eta}^{2}\overline{\log}\bigg{(}Z_{k, \ell},\frac{\delta}{4k^{2}(\ell+1)^{2}}\bigg{)}\right\},\]

where \(Z_{k,\ell}=Z_{k,0,\ell}\cup Z_{k,1,\ell}\), \(V_{k,\ell}=Z_{k,\ell}^{\top}Z_{k,\ell}\) and \(S_{k,\ell}\) is stacked noise matrix during collecting samples \(Z_{k,\ell}\) per the model \(X=Z\Gamma+S\). By a union bound, we have

\[\mathbb{P}\!\left(\left(\bigcap_{k=1}^{\infty}\bigcap_{\ell=1}^{\infty} \mathcal{E}^{1}_{k,\ell}\right)^{c}\right)\leq\sum_{k=1}^{\infty}\sum_{\ell=1} ^{\infty}\mathbb{P}\Big{(}\mathcal{E}^{1}_{k,\ell}\Big{)}\leq\delta/2.\]

For the second sub-phase in phase \(k\), we define the good event for the second sub-phase in phase \(k\) and \(w\in\mathcal{W}\) as

\[\mathcal{E}^{2}_{k,w}=\left\{\sum_{q=1}^{N_{k,2}}\!\left\langle\left(\sum_{t=1 }^{N_{k,2}}z_{t_{z}}z_{t_{t}}^{\top}\widehat{\Gamma}\right)^{-1}z_{t_{q}},w \right\rangle\!\nu_{q}\leq\|w\|_{\bar{A}(Z_{k,2},\widehat{\Gamma})^{-1}}\sqrt{2 \sigma_{\nu}^{2}\log(16k^{2}|\mathcal{W}|/\delta)}\right\}.\]

By a union bound, we have

\[\mathbb{P}\!\left(\left(\bigcap_{k=1}^{\infty}\bigcap_{w\in\mathcal{W}_{k}} \mathcal{E}^{2}_{k,w}\right)^{c}\right)\leq\sum_{k=1}^{\infty}\sum_{w\in \mathcal{W}}\mathbb{P}\Big{(}\mathcal{E}^{2}_{k,w}\Big{)}\leq\delta/2.\]

Under the good event \(\left(\bigcap_{k=1}^{\infty}\bigcap_{\ell=1}^{\infty}\mathcal{E}^{1}_{k,\ell} \right)\bigcap\left(\bigcap_{k=1}^{\infty}\bigcap_{w\in\mathcal{W}_{k}} \mathcal{E}^{2}_{k,w}\right)\), we have with probability at least \(1-\delta\), for all \(k\) and \(w\in\mathcal{W}\),

\[\left|\left\langle\widehat{\theta}^{k}_{\mathbb{P}\text{-2SLS}}-\theta,w-w^{*} \right\rangle\right|\leq\zeta_{k}.\]

The rest of proof is same as the proof of Theorem 3.1.

**Part 2: sample complexity of algorithm**

**Sample complexity for first sub-phase**

Recall that \(\lambda_{E}=\arg\max_{\lambda\in\Delta(\mathcal{Z})}\sigma_{\min}\!\left(\sum _{z\in\mathcal{Z}}\lambda_{z}zz^{\top}\right)\) is the E-optimal design to maximize the minimum singular value of \(\sum_{z\in\mathcal{Z}}\lambda_{z}zz^{\top}\) and \(\kappa_{0}=\max_{\lambda}\sigma_{\min}\!\left(\sum_{z\in\mathcal{Z}}\lambda_{ z}zz^{\top}\right)\) is the maximum minimum singular value of \(\sum_{z\in\mathcal{Z}}\lambda_{z}zz^{\top}\). At the beginning of first sub-phase in phase \(k\), the algorithm first samples \(N_{k,0,0}\) arms according to \(\lambda_{E}\).

Before we proceed to the main proof of the sample complexity, we first address a minor technique issue to avoid cumbersomeness. For the logarithmic term that appears in the algorithm and confidence interval,

\[\overline{\log}(Z_{T},\delta):=8d\ln\left(1+\frac{2TL_{z}^{2}}{d(2\wedge\sigma _{\min}(Z_{T}^{\top}Z_{T}))}\right)+16\ln\left(\frac{2\!\cdot\!6^{d}}{\delta} \!\cdot\!\log_{2}^{2}\left(\frac{4}{2\wedge\sigma_{\min}(Z_{T}^{\top}Z_{T})} \right)\right),\]

when \(2\wedge\sigma_{\min}(V)=2\), it is equivalent to

\[\widetilde{\log}(T,\delta):=8d\ln\left(1+\frac{TL_{z}^{2}}{d}\right)+16\ln \left(\frac{2\!\cdot\!6^{d}}{\delta}\right).\]

Our algorithm design guarantees that \(2\wedge\sigma_{\min}(V)=2\) is always true whenever we need to use the logarithmic term \(\overline{\log}(Z_{T},\delta)\), given that the samples of our interest always includes the E-optimaldesign samples \(Z_{k,0,\ell}\) and the number of samples from the E-optimal design \(\left|Z_{k,0,\ell}\right|\) is always larger than \(\frac{2}{\kappa_{0}}\). So for the remaining part of the proof, we will use \(\widetilde{\log}(N,\delta)\) instead of \(\overline{\log}(Z_{T},\delta)\).

Denote the samples of E-optimal design that mixed into the samples from \(\ell\)-th doubling trick iteration in the first sub-phase of phase \(k\) as \(Z_{k,0,\ell}\) and \(\left|Z_{k,0,\ell}\right|=N_{k,0,\ell}\). By Lemma J.3, our choice of \(N_{k,0,\ell}\)

\[N_{k,0,\ell}\geq\frac{64gd\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\lambda_{E},\Gamma)\big{)}}\ln\Bigg{(}\frac{32g\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A( \lambda_{E},\Gamma)\big{)}}\Big{(}d+N_{k,1,\ell}L_{z}^{2}+L_{z}^{2}\Big{)} \Bigg{)}+\frac{128g\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\lambda_{E},\Gamma )\big{)}}\ln\Bigg{(}\frac{2\cdot\!\!6^{d}}{\delta}\Bigg{)},\]

is a sufficient condition to guarantee

\[N_{k,0,\ell}\geq\frac{4g\sigma_{\eta}^{2}\widetilde{\log}\big{(}N_{k,0,\ell}+ N_{k,1,\ell},\delta_{k,\ell}\big{)}}{\sigma_{\min}\big{(}A(\lambda_{E},\Gamma) \big{)}}\lor r(\omega). \tag{22}\]

Multiply both sides of (22) by \(\frac{N_{k,0,\ell}+N_{k,1,\ell}}{N_{k,0,\ell}}\) and we have

\[N_{k,0,\ell}+N_{k,1,\ell}\geq\Bigg{(}\frac{4g\sigma_{\eta}^{2}\widetilde{\log }\big{(}N_{k,0,\ell}+N_{k,1,\ell},\delta_{k,\ell}\big{)}}{\sigma_{\min}\big{(} A(\lambda_{E},\Gamma)\big{)}}\lor r(\omega)\Bigg{)}\frac{N_{k,0,\ell}+N_{k,1,\ell}}{N _{k,0,\ell}}.\]

By Property 1 of Lemma J.1, we have \(\alpha_{k,\ell}:=\frac{N_{k,0,\ell}}{N_{k,0,\ell}+N_{k,1,\ell}}<1/2\), then

\[N_{k,0,\ell}+N_{k,1,\ell} \geq\frac{1}{\alpha_{k,\ell}}\Bigg{(}\frac{4g\sigma_{\eta}^{2} \widetilde{\log}\big{(}N_{k,0,\ell}+N_{k,1,\ell},\delta_{k,\ell}\big{)}}{ \sigma_{\min}\big{(}A(\lambda_{E},\Gamma)\big{)}}\lor r(\omega)\Bigg{)}\] \[=\frac{4g\sigma_{\eta}^{2}\widetilde{\log}\big{(}N_{k,0,\ell}+N_{ k,1,\ell},\delta_{k,\ell}\big{)}}{\sigma_{\min}\big{(}A(\alpha_{k,\ell}\lambda_{E}, \Gamma)\big{)}}\lor\frac{r(\omega)}{\alpha_{k,\ell}}\] \[\geq\frac{4g\sigma_{\eta}^{2}\widetilde{\log}\big{(}N_{k,0,\ell}+ N_{k,1,\ell},\delta_{k,\ell}\big{)}}{\sigma_{\min}\big{(}A(\alpha_{k,\ell}\lambda_{E} +(1-\alpha_{k,\ell})\tilde{\lambda}_{k},\Gamma)\big{)}}\lor r(\omega). \tag{23}\]

These condition on \(N_{k,0,\ell}\) and \(N_{k,0,\ell}+N_{k,1,\ell}\) are needed for the proof.

Denote the total number of doubling trick iterations as \(L_{k}\) for phase \(k\). In the case of \(L_{k}=1\), the samples from the first doubling trick iteration satisfies stopping condition of the first sub-phase already, and the algorithm will not enter the second doubling trick iteration. Thus the total number of samples for the first sub-phase is

\[N_{k,0,1}+N_{k,1,1}\leq 2N_{k,1,1}\] (Property 1 of Lemma J.1) \[\leq 8gdM\ln\left(1+2M\Big{(}d+L_{z}^{2}\Big{)}+2M2gdM\right)+16M\ln \left(\frac{2\cdot\!\!6^{d}}{\delta_{k,1}}\right)\lor r(\omega).\]

In the case of \(L_{k}>1\), for \(\ell\in\{1,\cdots,L_{k}\}\), denote \(\widehat{\Gamma}^{\ell}\) as the estimate of \(\Gamma\) at the end of the \(\ell\)-th doubling trick iteration. With these notations, we have

* At the end of \(L_{k}\)-th doubling trick iteration, the stopping condition is satisfied, i.e., \[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\!\!\big{\|}w-w^{\prime}\big{\|}_{\tilde{ A}(Z_{k,0,L_{k}}\cup Z_{k,1,L_{k}},\widehat{\Gamma}^{L_{k}})-1}^{2}\|\theta\|_{2}^{2}L_{ \eta}\widetilde{\log}\big{(}N_{k,0,L_{k}}+N_{k,1,L_{k}},\delta_{k,L_{k}}\big{)} \leq\zeta_{k}^{2}.\] (24) We short \(\widetilde{\log}\big{(}N_{k,0,\ell}+N_{k,1,\ell},\delta_{k,\ell}\big{)}= \widetilde{\log}_{k,\ell}\).
* At the end of \((L_{k}-1)\)-th doubling trick iteration, the stopping condition is not satisfied, i.e., \[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\!\!\big{\|}w-w^{\prime}\big{\|}_{\tilde{ A}(Z_{k,0,L_{k}}\cup Z_{k,1,L_{k}-1},\widehat{\Gamma}^{L_{k}})-1}^{2}\|\theta\|_{2}^{2}L_{ \eta}\widetilde{\log}_{k,L_{k}-1}>\zeta_{k}^{2}.\] (25)Denote \(\xi_{L_{k}}\) as the empirical distribution of \(Z_{k,0,L_{k}}\cup Z_{k,1,L_{k}}\). Then above two conditions imply that the number of samples for \(L_{k}\)-th and \((L_{k}-1)\)-th doubling trick iterations respectively satisfy

\[N_{k,0,L_{k}}+N_{k,1,L_{k}}\geq\frac{\|\theta\|_{2}^{2}L_{n}\widetilde{\log_{k,L_{k}}}}{\zeta_{k}^{2}}\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{ \prime}\bigr{\|}_{A(\xi_{L_{k}},\widehat{\Gamma}^{L_{k}})^{-1}}^{2}. \tag{26}\]

and

\[N_{k,0,L_{k}-1}+N_{k,1,L_{k}-1}<\frac{\|\theta\|_{2}^{2}L_{n}\widetilde{\log_{ k,L_{k}-1}}}{\zeta_{k}^{2}}\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{ \prime}\bigr{\|}_{A(\xi_{L_{k}-1},\widehat{\Gamma}^{L_{k}-1})^{-1}}^{2}. \tag{27}\]

Note that by Property 2 of Lemma J.1,

\[\frac{1}{2}\bigl{(}N_{k,0,L_{k}}+N_{k,1,L_{k}}\bigr{)}\leq N_{k,0,L_{k}-1}+N_{ k,1,L_{k}-1}.\]

Thus

\[N_{k,0,L_{k}}+N_{k,1,L_{k}}<\frac{2\|\theta\|_{2}^{2}L_{n}\widetilde{\log_{k,L _{k}-1}}}{\zeta_{k}^{2}}\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{ \prime}\bigr{\|}_{A(\xi_{L_{k}-1},\widehat{\Gamma}^{L_{k}-1})^{-1}}^{2}. \tag{28}\]

For any \(\ell\in\{1,\ldots,L_{k}\}\), by Lemma J.7, the factor of \(\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\xi_{ \ell},\widehat{\Gamma}^{\ell})^{-1}}^{2}\) can be upper bounded by

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|} _{A(\xi_{\ell},\widehat{\Gamma}^{\ell})^{-1}}^{2}\] \[\leq 3\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime} \bigr{\|}_{A(\xi_{\ell},\widehat{\Gamma}^{L_{k-1}})^{-1}}^{2}+2\max_{w,w^{ \prime}\in\mathcal{W}_{k}}\Bigl{\|}((\widehat{\Gamma}^{\ell})^{-\top}-( \widehat{\Gamma}^{L_{k-1}})^{-\top})\bigl{(}w-w^{\prime}\bigr{)}\Bigr{\|}_{ \bigl{(}\sum_{z}\xi_{\ell}zz^{\top}\bigr{)}^{-1}}^{2}\] \[\leq 3\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime} \bigr{\|}_{A(\xi_{\ell},\widehat{\Gamma}^{L_{k-1}})^{-1}}^{2}+2\max_{w,w^{ \prime}\in\mathcal{W}_{k}}\bigl{\|}(\Gamma^{-\top}-(\widehat{\Gamma}^{\ell})^ {-\top})\bigl{(}w-w^{\prime}\bigr{)}\Bigr{\|}_{\bigl{(}\sum_{z}\xi_{\ell}zz^{ \top}\bigr{)}^{-1}}^{2}\] \[+2\max_{w,w^{\prime}\in\mathcal{W}_{k}}\Bigl{\|}(\Gamma^{-\top}-( \widehat{\Gamma}^{L_{k-1}})^{-\top})\bigl{(}w-w^{\prime}\bigr{)}\Bigr{\|}_{ \bigl{(}\sum_{z}\xi_{\ell}zz^{\top}\bigr{)}^{-1}}^{2}. \tag{29}\]

We will upper bound the three terms in the RHS of (29) separately.

**For the first term**, by Lemma J.6,

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\xi_{ \ell},\widehat{\Gamma}^{L_{k-1}})^{-1}}^{2}\leq 4\max_{w,w^{\prime}\in \mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\tilde{\lambda}_{k},\widehat {\Gamma}^{L_{k-1}})^{-1}}^{2}, \tag{30}\]

where \(\tilde{\lambda}_{k}\) is the optimal design for \(\mathcal{W}_{k}\) in the first sub-phase of phase \(k-1\) (based on the last doubling trick iteration), i.e.,

\[\tilde{\lambda}_{k}=\arg\min_{\lambda\in\Delta(\Delta(\mathcal{Z})}\max_{w,w^{ \prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda,\widehat{ \Gamma}^{L_{k-1}})^{-1}}^{2}.\]

Then for any \(\lambda\),

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|} _{A(\tilde{\lambda}_{k},\widehat{\Gamma}^{L_{k-1}})^{-1}}^{2}\] \[\leq 3\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime} \bigr{\|}_{A(\lambda,\Gamma)^{-1}}^{2}+2\max_{w,w^{\prime}\in\mathcal{W}_{k}} \bigl{\|}\bigl{(}\Gamma^{-\top}-(\widehat{\Gamma}^{L_{k-1}})^{-\top}\bigr{)} \bigl{(}w-w^{\prime}\bigr{)}\bigr{\|}_{A(\lambda,I)^{-1}}^{2}\] (Lemma J.7) \[\stackrel{{ b_{1}}}{{\leq}} 3\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime} \bigr{\|}_{A(\lambda_{k}^{\prime},\Gamma)^{-1}}^{2}+2\max_{w,w^{\prime}\in \mathcal{W}_{k}}\bigl{\|}(\Gamma^{-\top}-(\widehat{\Gamma}^{L_{k-1}})^{-\top} )\bigl{(}w-w^{\prime}\bigr{)}\bigr{\|}_{\bigl{(}\sum_{z}\lambda_{k}^{\prime}zz^ {\top}\bigr{)}^{-1}}^{2}\] \[\stackrel{{ b_{2}}}{{\leq}} 6\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime} \bigr{\|}_{A(\lambda_{k}^{*},\Gamma)^{-1}}^{2}+2\max_{w,w^{\prime}\in\mathcal{W}_{k }}\bigl{\|}(\Gamma^{-\top}-(\widehat{\Gamma}^{L_{k-1}})^{-\top})\bigl{(}w-w^{ \prime}\bigr{)}\bigr{\|}_{\bigl{(}\sum_{z}\lambda_{k}^{\prime}zz^{\top}\bigr{)} ^{-1}}^{2}. \tag{31}\]

where for \((b_{1})\), we plug in \(\lambda_{k}^{\prime}:=\alpha_{k}^{*}\lambda_{E}+(1-\alpha_{k}^{*})\lambda_{k}^{*}\), with \(\alpha_{k}^{*}\leq 1/2\) will be defined later and \(\lambda_{k}^{*}\) is the optimal design for \(\mathcal{W}_{k}\) given \(\Gamma\), i.e.,

\[\lambda_{k}^{*}=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime}\in \mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda,\Gamma)^{-1}}^{2}.\]\((b_{2})\) is due to the fact that \(\alpha_{k}^{*}\leq 1/2\). For the second term in the RHS of (31), given the condition (23), i.e.,

\[N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}}\geq\frac{4gL_{\eta}\widetilde{\log}_{k-1,L_{k -1}}}{\sigma_{\min}\big{(}A(\xi_{L_{k-1}},\Gamma)\big{)}}\]

by Lemma J.11 we have,

\[\Big{\|}(\Gamma^{-\top}-(\widehat{\Gamma}^{L_{k-1}})^{-\top})\big{(} w-w^{\prime}\big{)}\Big{\|}_{(\sum_{*}\lambda_{k}^{\prime}z^{\top})^{-1}}^{2}\] \[\leq \frac{2L_{\eta}\widetilde{\log}_{k-1,L_{k-1}}}{\sigma_{\min} \big{(}A(\lambda_{k}^{\prime},\Gamma)\big{)}}\frac{1}{N_{k-1,0,L_{k-1}}+N_{k- 1,1,L_{k-1}}}\big{\|}w-w^{\prime}\big{\|}_{A(\xi_{L_{k-1}},\Gamma)^{-1}}^{2}\] \[\leq \frac{1}{N_{k-1,0,L_{k-1}}}\frac{2L_{\eta}\widetilde{\log}_{k-1,L _{k-1}}}{\sigma_{\min}\big{(}A(\lambda_{k}^{\prime},\Gamma)\big{)}}\frac{N_{k- 1,0,L_{k-1}}}{N_{k-1,0,L_{k-1}+N_{k-1,1,L_{k-1}}}}\big{\|}w-w^{\prime}\big{\|} _{A(\xi_{L_{k-1}},\Gamma)^{-1}}^{2}\] \[\leq \frac{1}{N_{k-1,0,L_{k-1}}}\frac{2L_{\eta}\widetilde{\log}_{k-1,L _{k-1}}}{\sigma_{\min}\big{(}A(\lambda_{k}^{\prime}\lambda_{E},\Gamma)\big{)}} \frac{N_{k-1,0,L_{k-1}}}{N_{k-1,0,L_{k-1}+N_{k-1,1,L_{k-1}}}}\big{\|}w-w^{ \prime}\big{\|}_{A(\xi_{L_{k-1}},\Gamma)^{-1}}^{2}\] \[\leq \frac{1}{N_{k-1,0,L_{k-1}}}\frac{2L_{\eta}\widetilde{\log}_{k-1,L _{k-1}}}{\sigma_{\min}\big{(}A(\lambda_{E},\Gamma)\big{)}}\big{\|}w-w^{\prime} \big{\|}_{A(\xi_{L_{k-1}},\Gamma)^{-1}}^{2}\] (set \[\alpha_{k}^{*}=\alpha_{k-1}\] ) \[\overset{b_{1}}{\leq} \frac{1}{4g}\big{\|}w-w^{\prime}\big{\|}_{A(\xi_{L_{k-1}},\Gamma)^ {-1}}^{2}\] \[\overset{b_{2}}{\leq} \frac{6}{4g}\big{\|}w-w^{\prime}\big{\|}_{A(\xi_{L_{k-1}},\widehat {\Gamma}^{L_{k-1}})^{-1}}^{2}. \tag{32}\]

where for \((b_{1})\), we use the condition (22) on \(N_{k-1,0,L_{k-1}}\), and \((b_{2})\) is due to Lemma J.5. Plug (32) into (31), we have

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime}\big{\|}_{A(\tilde{ \lambda}_{k},\widehat{\Gamma}^{L_{k-1}})^{-1}}^{2}\leq 6\max_{w,w^{\prime}\in \mathcal{W}_{k}}\big{\|}w-w^{\prime}\big{\|}_{A(\lambda_{k}^{\prime},\Gamma)^{ -1}}^{2}+\frac{3}{g}\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime} \big{\|}_{A(\xi_{L_{k-1}},\widehat{\Gamma}^{L_{k-1}})^{-1}}^{2}. \tag{33}\]

Plug (33) into (30), we have the first term in the RHS of (29) can be upper bounded by

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime}\big{\|} _{A(\xi_{\ell},\widehat{\Gamma}^{L_{k-1}})^{-1}}^{2}\] \[\leq 24\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime} \big{\|}_{A(\lambda_{k}^{\prime},\Gamma)^{-1}}^{2}+\frac{12}{g}\max_{w,w^{ \prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime}\big{\|}_{A(\xi_{L_{k-1}},\widehat {\Gamma}^{L_{k-1}})^{-1}}^{2}\] \[\leq 24\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime} \big{\|}_{A(\lambda_{k}^{\prime},\Gamma)^{-1}}^{2}+\frac{12}{g}\frac{\zeta_{k-1 }^{2}}{\|\theta\|_{2}^{2}L_{\eta}\widetilde{\log}_{k-1,L_{k-1}}}(N_{k-1,0,L_{ k-1}}+N_{k-1,1,L_{k-1}}), \tag{34}\]

where for the last inequality we use the fact that the stopping condition is satisfied at the end of \((L_{k-1})\)-th doubling trick iteration for phase \(k-1\) per (26).

**For the second term** in the RHS of (29), by Lemma J.10, when the condition (23) is satisfied, i.e., when

\[N_{k,0,\ell}+N_{k,1,\ell}\geq\frac{4gL_{\eta}\widetilde{\log}_{k,\ell}}{\sigma_ {\min}\big{(}A(\xi_{\ell},\Gamma)\big{)}},\]

we have

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\Big{\|}(\Gamma^{-\top}-( \widehat{\Gamma}^{\ell})^{-\top})\big{(}w-w^{\prime}\big{)}\Big{\|}_{(\sum_{*} \xi_{\ell}z^{\top})^{-1}}^{2}\leq \frac{1}{g}\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{ \prime}\big{\|}_{A(\xi_{\ell},\Gamma)^{-1}}^{2}\] \[\leq \frac{6}{g}\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime }\big{\|}_{A(\xi_{\ell},\widehat{\Gamma}^{\ell})^{-1}}^{2}, \tag{35}\]

where the last inequality is due to Lemma J.5.

**For the third term** in the RHS of (29), by Lemma J.11, when the condition (23) is satisfied, i.e., when

\[N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}}\geq\frac{4gL_{\eta}\widetilde{ \log}_{k-1,L_{k-1}}}{\sigma_{\min}\big{(}A(\xi_{L_{k-1}},\Gamma)\big{)}}\]

we have

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\hskip-2.845276pt\left\|( \Gamma^{-\top}-(\widehat{\Gamma}^{L_{k-1}})^{-\top})\big{(}w-w^{\prime}\big{)} \right\|_{\big{(}\sum_{s}\xi_{t}z^{\tau}\big{)}^{-1}}^{2}\] \[\leq \frac{2L_{\eta}\widetilde{\log}_{k-1,L_{k-1}}}{\sigma_{\min} \big{(}A(\xi_{t},\Gamma)\big{)}}\frac{1}{N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}}} \max_{w,w^{\prime}\in\mathcal{W}_{k}}\hskip-2.845276pt\left\|w-w^{\prime} \right\|_{A(\xi_{L_{k-1}},\Gamma)^{-1}}^{2}\] \[\leq \frac{2L_{\eta}\widetilde{\log}_{k-1,L_{k-1}}}{\sigma_{\min} \big{(}A(\alpha_{t}\lambda_{E},\Gamma)\big{)}}\frac{1}{N_{k-1,0,L_{k-1}}+N_{k- 1,1,L_{k-1}}}\max_{w,w^{\prime}\in\mathcal{W}_{k}}\hskip-2.845276pt\left\|w-w^ {\prime}\right\|_{A(\xi_{L_{k-1}},\widehat{\Gamma}^{L_{k-1}})^{-1}}^{2}\] \[\leq \frac{N_{k,0,\ell}+N_{k,1,\ell}}{N_{k,0,\ell}}\frac{1}{N_{k-1,0,L _{k-1}}+N_{k-1,1,L_{k-1}}}\frac{2\widetilde{\log}_{k-1,L_{k-1}}}{\sigma_{\min }\big{(}A(\lambda_{E},\Gamma)\big{)}}(N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}})\frac {\zeta_{k-1}^{2}}{\left\|\theta\right\|_{2}^{2}\widetilde{\log}_{k-1,L_{k-1}}}\] \[\leq \frac{N_{k,0,\ell}+N_{k,1,\ell}}{N_{k,0,\ell}}\frac{2}{\sigma_{ \min}\big{(}A(\lambda_{E},\Gamma)\big{)}}\frac{\zeta_{k-1}^{2}}{\left\|\theta \right\|_{2}^{2}}\] \[\stackrel{{ k_{2}}}{{\leq}} \frac{N_{k,0,\ell}+N_{k,1,\ell}}{4gL_{\eta}\widetilde{\log}_{k, \ell}}\frac{\zeta_{k-1}^{2}}{\left\|\theta\right\|_{2}^{2}} \tag{36}\]

where \((b_{1})\) is due to (26), \((b_{2})\) is due to (22). Plug (34), (35) and (36) into (29), we have

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\hskip-2.845276pt\left\|w-w^ {\prime}\right\|_{A(\xi_{t},\widehat{\Gamma}^{\ell})^{-1}}^{2}\] \[\leq 72\max_{w,w^{\prime}\in\mathcal{W}_{k}}\hskip-2.845276pt\left\| w-w^{\prime}\right\|_{A(\lambda_{k}^{*},\Gamma)^{-1}}^{2}+\frac{36}{g}\frac{ \zeta_{k-1}^{2}}{\left\|\theta\right\|_{2}^{2}L_{\eta}\widetilde{\log}_{k-1,L _{k-1}}}(N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}})\] \[+\frac{12}{g}\max_{w,w^{\prime}\in\mathcal{W}_{k}}\hskip-2.845276pt \left\|w-w^{\prime}\right\|_{A(\xi_{t},\widehat{\Gamma}^{\ell})^{-1}}^{2}+ \frac{2}{g}\frac{N_{k,0,\ell+1}+N_{k,1,\ell+1}}{L_{\eta}\widetilde{\log}_{k, \ell}}\frac{\zeta_{k-1}^{2}}{\left\|\theta\right\|_{2}^{2}}. \tag{37}\]

With \(\ell=L_{k}-1\) and the fact that \(g>24\) whose exact value will be set later, we can rearrange (37) as

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\hskip-2.845276pt\left\|w-w^ {\prime}\right\|_{A(\xi_{L_{k-1}},\widehat{\Gamma}^{L_{k-1}})^{-1}}^{2}\] \[\leq 144\max_{w,w^{\prime}\in\mathcal{W}_{k}}\hskip-2.845276pt\left\| w-w^{\prime}\right\|_{A(\lambda_{k}^{*},\Gamma)^{-1}}^{2}+\frac{72}{g}\frac{ \zeta_{k-1}^{2}}{\left\|\theta\right\|_{2}^{2}L_{\eta}\widetilde{\log}_{k-1,L _{k-1}}}(N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}})\] \[+\frac{1}{g}\frac{N_{k,0,L_{k}}+N_{k,1,L_{k}}}{L_{\eta}\widetilde{ \log}_{k,L_{k}-1}}\frac{\zeta_{k-1}^{2}}{\left\|\theta\right\|_{2}^{2}}. \tag{38}\]

Note that the LHS of above can be lower bounded by (28),

\[\frac{\zeta_{k}^{2}}{2\left\|\theta\right\|_{2}^{2}L_{\eta}\widetilde{\log}_{k,L_{k}-1}}(N_{k,0,L_{k}}+N_{k,1,L_{k}})<\max_{w,w^{\prime}\in\mathcal{W}_{k}} \hskip-2.845276pt\left\|w-w^{\prime}\right\|_{A(\xi_{L_{k-1}},\widehat{\Gamma }^{L_{k-1}})^{-1}}^{2}. \tag{39}\]

Rearrange the terms in (39) and (38) and setting \(g\) to be larger enough and using the fact that \(\zeta_{k}=\zeta_{k-1}/2\), we have

\[N_{k,0,L_{k}}+N_{k,1,L_{k}}\leq \frac{576}{\zeta_{k}^{2}}\hskip-2.845276pt\left\|\theta\right\|_{ 2}^{2}L_{\eta}\widetilde{\log}_{k,L_{k}-1}\max_{w,w^{\prime}\in\mathcal{W}_{k}} \hskip-2.845276pt\left\|w-w^{\prime}\right\|_{A(\lambda_{k}^{*},\Gamma)^{-1}}^{2 }+\frac{72}{g}\frac{\widetilde{\log}_{k,L_{k}-1}}{\widetilde{\log}_{k-1,L_{k-1}} }(N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}}).\]

Note that by definition \(\widetilde{\log}_{k,L_{k}-1}<\widetilde{\log}_{k,L_{k}}\), thus

\[N_{k,0,L_{k}}+N_{k,1,L_{k}}\leq \frac{576}{\zeta_{k}^{2}}\hskip-2.845276pt\left\|\theta\right\|_{ 2}^{2}L_{\eta}\widetilde{\log}_{k,L_{k}}\max_{w,w^{\prime}\in\mathcal{W}_{k}} \hskip-2.845276pt\left\|w-w^{\prime}\right\|_{A(\lambda_{k}^{*},\Gamma)^{-1}}^{2} +\frac{72}{g}\frac{\widetilde{\log}_{k,L_{k}}}{\widetilde{\log}_{k-1,L_{k-1}} }(N_{k-1,0,L_{k-1}}+N_{k-1,1,L_{k-1}}).\]Denote \(D_{k}:=N_{k,0,L_{k}}+N_{k,1,L_{k}}\) and divide both sides of above by \(\widetilde{\log}_{k,L_{k}}\), we have

\[\frac{D_{k}}{\widetilde{\log}_{k,L_{k}}}\leq\frac{576}{\zeta_{k}^{2}}\|\theta\| _{2}^{2}L_{\eta}\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime} \bigr{\|}_{A(\lambda_{k}^{*},\Gamma)^{-1}}^{2}+\frac{72}{g}\,\frac{D_{k-1}}{ \widetilde{\log}_{k-1,L_{k-1}}}.\]

In the case of \(L_{k}=1\), by Property 3 of Lemma J.1, we have

\[\frac{D_{k}}{\widetilde{\log}_{k,L_{k}}}=\frac{N_{k,1,1}}{8d\ln\left(1+\frac{ N_{k,1,1}L_{k}^{2}}{d}\right)+16\ln\left(2\cdot\frac{6\cdot d^{4}k^{2}}{\delta} \right)}\leq M\ln(dM).\]

Thereby we have

\[\frac{D_{k}}{\widetilde{\log}_{k,L_{k}}}\leq\max\left\{M\ln(dM),\frac{576}{ \zeta_{k}^{2}}\|\theta\|_{2}^{2}L_{\eta}\max_{w,w^{\prime}\in\mathcal{W}_{k}} \bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{k}^{*},\Gamma)^{-1}}^{2}+\frac{72} {g}\,\frac{D_{k-1}}{\widetilde{\log}_{k-1,L_{k-1}}}\right\}. \tag{40}\]

Taking a summation over \(k\) on both sides of (40), we have

\[\sum_{k=1}^{K^{*}}\frac{D_{k}}{\widetilde{\log}_{k,L_{k}}}= \frac{D_{1}}{\widetilde{\log}_{1,L_{1}}}+\sum_{k=2}^{K^{*}}\frac{D _{k}}{\widetilde{\log}_{k,L_{k}}} \tag{41}\] \[\leq \frac{D_{1}}{\widetilde{\log}_{1,L_{1}}}+\sum_{k=2}^{K^{*}}\max \left\{M\ln(dM),\frac{576}{\zeta_{k}^{2}}\|\theta\|_{2}^{2}L_{\eta}\max_{w,w^ {\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{k}^{*}, \Gamma)^{-1}}^{2}+\frac{72}{g}\,\frac{D_{k-1}}{\widetilde{\log}_{k-1,L_{k-1}} }\right\}\] (42) \[\leq \frac{D_{1}}{\widetilde{\log}_{1,L_{1}}}+\sum_{k=2}^{K^{*}}M\ln (dM)+\sum_{k=2}^{K^{*}}\left(\frac{576}{\zeta_{k}^{2}}\|\theta\|_{2}^{2}L_{ \eta}\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A( \lambda_{k}^{*},\Gamma)^{-1}}^{2}+\frac{72}{g}\,\frac{D_{k-1}}{\widetilde{ \log}_{k-1,L_{k-1}}}\right)\] (43) \[\leq \frac{D_{1}}{\widetilde{\log}_{1,L_{1}}}+\sum_{k=2}^{K^{*}}M\ln (dM)+\sum_{k=2}^{K^{*}}\frac{576}{\zeta_{k}^{2}}\|\theta\|_{2}^{2}L_{\eta}\max _{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{k }^{*},\Gamma)^{-1}}^{2}+\sum_{k=2}^{K^{*}}\frac{72}{g}\,\frac{D_{k-1}}{ \widetilde{\log}_{k-1,L_{k-1}}}. \tag{44}\]

Thus by setting \(g=72\times 2\) and rearranging the terms, we have

\[\sum_{k=1}^{K^{*}}\frac{D_{k}}{\widetilde{\log}_{k,L_{k}}}\leq \frac{2D_{1}}{\widetilde{\log}_{1,L_{1}}}+2\sum_{k=2}^{K^{*}}M\ln(dM)+2\sum_{ k=2}^{K^{*}}\frac{576}{\zeta_{k}^{2}}\|\theta\|_{2}^{2}L_{\eta}\max_{w,w^{ \prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{k}^{*}, \Gamma)^{-1}}^{2}. \tag{45}\]

For \(D_{1}\), which corresponds to the first sub-phase where we use E-optimal design with doubling trick, we have the stopping condition,

\[\max_{w,w^{\prime}\in\mathcal{W}_{1}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(Z_{1,0,L_{1}},\mathbb{R}^{L_{1}})^{-1}}^{2}\|\theta\|_{2}^{2}L_{\eta}\widetilde{\log }_{1,L_{1}}\leq\zeta_{1}^{2}.\]

This implies that when the stopping condition is met

\[N_{1,0,L_{1}}\geq\frac{\|\theta\|_{2}^{2}L_{\eta}\widetilde{\log}_{1,L_{1}}}{ \zeta_{1}^{2}}\max_{w,w^{\prime}\in\mathcal{W}_{1}}\bigl{\|}w-w^{\prime}\bigr{\|} _{A(\xi_{L_{1}},\mathbb{R}^{L_{1}})^{-1}}^{2}. \tag{46}\]

and

\[N_{1,0,L_{1}-1}<\frac{2\|\theta\|_{2}^{2}L_{\eta}\widetilde{\log}_{1,L_{1}-1}} {\zeta_{1}^{2}}\max_{w,w^{\prime}\in\mathcal{W}_{1}}\bigl{\|}w-w^{\prime}\bigr{\|} _{A(\xi_{L_{1}-1},\mathbb{R}^{L_{1}-1})^{-1}}^{2}. \tag{47}\]Since we use E-optimal design for the first phase, the factor of \(\left\|w-w^{\prime}\right\|_{A(\xi_{L_{1}-1},\widehat{\Gamma}^{L_{1}-1})^{-1}}^{2}\) can be upper bounded by

\[\left\|w-w^{\prime}\right\|_{A(\xi_{L_{1}-1},\widehat{\Gamma}^{L_{ 1}-1})^{-1}}^{2}\] \[\leq 3\left\|w-w^{\prime}\right\|_{A(\xi_{L_{1}-1},\Gamma)^{-1}}^{2}+ 2\left\|(\Gamma^{-\top}-(\widehat{\Gamma}^{L_{1}-1})^{-\top})\big{(}w-w^{ \prime}\big{)}\right\|_{\left(\sum_{z}\xi_{L_{1}-1}zz^{\top}\right)^{-1}}^{2}\] (Lemma J.7) \[\leq 3\left\|w-w^{\prime}\right\|_{A(\lambda_{E},\Gamma)^{-1}}^{2}+2 \left\|w-w^{\prime}\right\|_{A(\lambda_{E},\Gamma)^{-1}}^{2}\] (Lemma J.10) \[\leq 6\left\|w-w^{\prime}\right\|_{A(\lambda_{E},\Gamma)^{-1}}^{2}. \tag{48}\]

Plug (48) into (47), and use the fact that \(2N_{1,0,L_{1}-1}=N_{1,0,L_{1}},\zeta_{1}=1\), we have

\[N_{1,0,L_{1}}\leq 24\|\theta\|_{2}^{2}L_{\eta}\widetilde{\log_{1,L_{1}-1}} \max_{w,w^{\prime}\in\mathcal{W}_{1}}\left\|w-w^{\prime}\right\|_{A(\lambda_{E },\Gamma)^{-1}}^{2}\leq 24\|\theta\|_{2}^{2}L_{\eta}\widetilde{\log_{1,L_{1}}} \max_{w,w^{\prime}\in\mathcal{W}_{1}}\left\|w-w^{\prime}\right\|_{A(\lambda_{E },\Gamma)^{-1}}^{2}. \tag{49}\]

Thus we have,

\[\frac{D_{1}}{\widetilde{\log}_{1,L_{1}}}\leq\frac{2N_{1,0,L_{1}}}{\widetilde{ \log}_{1,L_{1}}}\leq 48\|\theta\|_{2}^{2}L_{\eta}\max_{w,w^{\prime}\in\mathcal{W}_{1 }}\left\|w-w^{\prime}\right\|_{A(\lambda_{E},\Gamma)^{-1}}^{2}=:\rho_{1}.\]

By the same calculation as (17) and (19), we have

\[\left\|\theta\right\|_{2}^{2}L_{\eta}\sum_{k=2}^{K^{*}}\frac{1}{\zeta_{k}^{2}} \max_{w,w^{\prime}\in\mathcal{W}_{k}}\left\|w-w^{\prime}\right\|_{A(\lambda_{ E}^{2},\Gamma)^{-1}}^{2}\leq c\|\theta\|_{2}^{2}L_{\eta}K^{*}\rho^{*}=:\rho_{2},\]

where \(c\) is an absolute constant. Next we lower bound the left hand side of (45). To do this, we first upper bound \(\widetilde{\log}_{k,L_{k}}\) as

\[\widetilde{\log}_{k,L_{k}}= 8d\ln\left(1+\frac{D_{k}L_{z}^{2}}{d}\right)+16\ln\left(\frac{2 \cdot 6^{d}k^{2}L_{k}^{2}}{\delta}\right)\] \[\leq 8d\ln\left(1+\frac{D_{k}L_{z}^{2}}{d}\right)+32\ln(L_{k})+16\ln \left(\frac{2\cdot 6^{d}k^{2}}{\delta}\right)\] \[\leq 8d\ln\left(1+\frac{D_{k}L_{z}^{2}}{d}\right)+32\ln\left(\log_{2} \left(\frac{D_{k}}{d}\right)\right)+16\ln\left(\frac{2\cdot 6^{d}k^{2}}{\delta}\right)\] \[\leq 32d\ln\left(1+\frac{D_{k}L_{z}^{2}}{d}\right)+16\ln\left(\frac{ 2\cdot 6^{d}k^{2}}{\delta}\right), \tag{50}\]

where the inequality above uses the fact that \(L_{k}\) is the index of the last doubling trick iteration for phase \(k\), by the design of the doubling trick, we have \(L_{k}\leq\log_{2}\left(\frac{D_{k}}{d}\right)\).

\[\sum_{k=1}^{K^{*}}\frac{D_{k}}{\widetilde{\log}_{k,L_{k}}}= \sum_{k=1}^{K^{*}}\frac{D_{k}}{8d\ln\left(1+\frac{D_{k}L_{z}^{2}}{ d}\right)+16\ln\left(\frac{2\cdot 6^{d}k^{2}L_{k}^{2}}{\delta}\right)}\] \[\geq \sum_{k=1}^{K^{*}}\frac{D_{k}}{32d\ln\left(1+\frac{D_{k}L_{z}^{2 }}{d}\right)+16\ln\left(\frac{2\cdot 6^{d}K^{*2}}{\delta}\right)}\] (due to (50)) \[\geq \sum_{k=1}^{K^{*}}\frac{D_{k}}{32d\ln\left(1+\frac{\sum_{k=1}^{K ^{*}}D_{k}L_{k}^{2}}{d}\right)+16\ln\left(\frac{2\cdot 6^{d}K^{*2}}{\delta} \right)}\] \[= \frac{1}{32d\ln\left(1+\frac{\sum_{k=1}^{K^{*}}D_{k}L_{k}^{2}}{ d}\right)+16\ln\left(\frac{2\cdot 6^{d}K^{*2}}{\delta}\right)}\sum_{k=1}^{K^{*}}D_{k}.\]Denote \(\rho_{3}:=K^{*}M\ln(dM)\), looking back at (44), we have

\[\sum_{k=1}^{K^{*}}D_{k}\leq\Bigg{(}32d\ln\Bigg{(}1+\frac{\sum_{k=1}^{K^{*}}D_{k}L _{z}^{2}}{d}\Bigg{)}+16\ln\Bigg{(}\frac{2.6^{d}K^{*2}}{\delta}\Bigg{)}\Bigg{)}( \rho_{1}+\rho_{2}+\rho_{3}).\]

By Lemma K.5, we have

\[\sum_{k=1}^{K^{*}}D_{k}\leq 32(\rho_{1}+\rho_{2}+\rho_{3})\ln\Bigg{(} \frac{2.6^{d}K^{*2}}{\delta}\Bigg{)}+64d(\rho_{1}+\rho_{2}+\rho_{3})\ln\Bigg{(} 3+(\rho_{1}+\rho_{2}+\rho_{3})\frac{2L_{z}^{2}}{d}\Bigg{)}\,. \tag{51}\]

#### Sample complexity for second sub-phase

The design for the second sub-phase of phase \(k\) is based on \(\widehat{\Gamma}^{L_{k}}\), the estimate of \(\Gamma\) at the end of the \(L_{k}\)-th doubling trick iteration in the first sub-phase of phase \(k\),

\[\hat{\lambda}_{k}=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime}\in \mathcal{W}_{k}}\big{\|}w-w^{\prime}\big{\|}_{A(\lambda,\widehat{\Gamma}^{L_{ k}})^{-1}}^{2}.\]

Then for any \(\lambda\), by Lemma J.7 we have

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime}\big{\|} _{A(\hat{\lambda}_{k},\widehat{\Gamma}^{L_{k}})^{-1}}^{2}\] \[\leq 3\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime}\big{\| }_{A(\lambda,\Gamma)^{-1}}^{2}+2\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|} \big{(}\Gamma^{-\top}-(\widehat{\Gamma}^{L_{k}})^{-\top}\big{)}\big{(}w-w^{ \prime}\big{)}\big{\|}_{A(\lambda,I)^{-1}}^{2}\]

We plug in \(\lambda_{k}^{\prime\prime}:=\alpha_{k}^{*}\lambda_{E}+(1-\alpha_{k}^{*})\lambda _{k}^{*}\), with \(\alpha_{k}^{*}\leq 1/2\) will be defined later and \(\lambda_{k}^{*}\) is the optimal design for \(\mathcal{W}_{k}\),

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime}\big{\|} _{A(\hat{\lambda}_{k},\widehat{\Gamma}^{L_{k}})^{-1}}^{2}\] \[\leq 3\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime}\big{\| }_{A(\lambda_{k}^{\prime\prime},\Gamma)^{-1}}^{2}+2\max_{w,w^{\prime}\in \mathcal{W}_{k}}\big{\|}(\Gamma^{-\top}-(\widehat{\Gamma}^{L_{k}})^{-\top}) \big{(}w-w^{\prime}\big{)}\big{\|}_{(\sum_{x}\lambda_{k}^{\prime\prime}zz^{ \top})^{-1}}^{2}\] \[\leq 6\max_{w,w^{\prime}\in\mathcal{W}_{k}}\big{\|}w-w^{\prime}\big{\| }_{A(\lambda_{k}^{\prime},\Gamma)^{-1}}^{2}+2\max_{w,w^{\prime}\in\mathcal{W}_ {k}}\big{\|}(\Gamma^{-\top}-(\widehat{\Gamma}^{L_{k}})^{-\top})\big{(}w-w^{ \prime}\big{)}\big{\|}_{(\sum_{x}\lambda_{k}^{\prime\prime}zz^{\top})^{-1}}^{2}, \tag{52}\]

where the last inequality is due to the fact that \(\alpha_{k}^{*}\leq 1/2\). For the second term in the RHS of above, by Lemma J.11 we have,

\[\Big{\|}(\Gamma^{-\top}-(\widehat{\Gamma}^{L_{k}})^{-\top})\big{(} w-w^{\prime}\big{)}\Big{\|}_{(\sum_{x}\lambda_{k}^{\prime\prime}zz^{\top})^{-1}}^{2}\] \[\leq \frac{4L_{\eta}\widetilde{\log}_{k,L_{k}}}{\sigma_{\min}\big{(}A (\lambda_{k}^{\prime\prime},\Gamma)\big{)}}\big{\|}w-w^{\prime}\big{\|}_{A(Z_ {k,0}\cup Z^{L_{k}},\Gamma)^{-1}}^{2}\] \[\leq \frac{1}{N_{k,0,L_{k}}}\frac{4L_{\eta}\widetilde{\log}_{k}}{\sigma _{\min}\big{(}A(\lambda_{k}^{\prime\prime},\Gamma)\big{)}}\frac{N_{k,0,L_{k}}}{N _{k,0,L_{k}}+N_{k,1L_{k}}}\big{\|}w-w^{\prime}\big{\|}_{A(\xi_{L_{k}},\Gamma)^ {-1}}^{2}\] \[\leq \frac{1}{N_{k,0,L_{k}}}\frac{4L_{\eta}\widetilde{\log}_{k,L_{k}}} {\sigma_{\min}\big{(}A(\alpha_{k}^{*}\lambda_{E},\Gamma)\big{)}}\frac{N_{k,0,L_{k }}}{N_{k,0,L_{k}}+N_{k,1,L_{k}}}\big{\|}w-w^{\prime}\big{\|}_{A(\xi_{L_{k}}, \Gamma)^{-1}}^{2}\] \[\leq \frac{1}{N_{k,0,L_{k}}}\frac{4L_{\eta}\widetilde{\log}_{k,L_{k}}} {\sigma_{\min}\big{(}A(\lambda_{E},\Gamma)\big{)}}\big{\|}w-w^{\prime}\big{\|}_{ A(\xi_{L_{k}},\Gamma)^{-1}}^{2}\] (set \[\alpha_{k}^{*}=\alpha_{k}\] ) \[\stackrel{{ b_{1}}}{{\leq}} \frac{1}{g}\big{\|}w-w^{\prime}\big{\|}_{A(\xi_{L_{k}},\Gamma)^{-1}}^{2}\] \[\stackrel{{ b_{2}}}{{\leq}} \frac{1}{g}\big{\|}w-w^{\prime}\big{\|}_{A(\xi_{L_{k}},\widehat{ \Gamma}^{L_{k}})^{-1}}^{2}, \tag{53}\]where for \((b_{1})\), we have \(N_{k,0,L_{k}}\geq\frac{4gL_{\nu}\log_{k_{1},L_{k}}}{\sigma_{\min}\big{(}A(\lambda_ {E},\Gamma)\big{)}}\), and \((b_{2})\) is due to Lemma J.5. Plug (53) into (52), we have

\[\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|} _{A(\hat{\lambda}_{k},\hat{\Gamma}^{L_{k}})^{-1}}^{2}\leq 6\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime} \bigr{\|}_{A(\lambda_{k}^{*},\Gamma)^{-1}}^{2}+\frac{2}{g}\max_{w,w^{\prime} \in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\xi_{L_{k}},\mathbb{P}^{ L_{k}})^{-1}}^{2}\] \[\leq 6\max_{w,w^{\prime}\in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime} \bigr{\|}_{A(\lambda_{k}^{*},\Gamma)^{-1}}^{2}+\frac{2}{g}\frac{\zeta_{k}^{2}} {\bigl{\|}\theta\bigr{\|}_{2}^{2}L_{\eta}\widetilde{\log}_{k,L_{k}}}(N_{k,0,L _{k}}+N_{k,1,L_{k}}), \tag{54}\]

where the last inequality is due to (26). According to the algorithm design, the number of samples in the second sub-phase of phase \(k\) is defined as

\[N_{k,2}=\left[2(1+\omega)\zeta_{k}^{-2}\rho(\mathcal{W}_{k})L_{\nu}\log\! \left(\frac{4k^{2}|\mathcal{W}|}{\delta}\right)\right]\lor r(\omega),\]

with \(\rho(\mathcal{W}_{k})=\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime} \in\mathcal{W}_{k}}\|w-w^{\prime}\|_{A(\lambda_{k},\hat{\Gamma}^{L_{k}})^{-1}}^ {2}\). Then we have, by setting \(g\geq 4\),

\[N_{k,2}\lessapprox(1+\omega)\zeta_{k}^{-2}L_{\nu}\max_{w,w^{\prime}\in\mathcal{ W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{k}^{*},\Gamma)^{-1}}^{2}\log\! \left(\frac{4k^{2}|\mathcal{W}|}{\delta}\right)+(1+\omega)(N_{k,0,L_{k}}+N_{k, 1,L_{k}}),\]

where '\(\lessapprox\)' hides logarithmic factors of \(|\mathcal{W}|\) for the second term and constants for simplicity. Plug (54) into the above inequality. Also note that by Lemma 2.1, we can always set \(L_{\nu}=2(\|\theta\|_{2}^{2}L_{\eta}+1)\). Thus,

\[\sum_{k=1}^{K^{*}}N_{k,2}\lessapprox (1+\omega)\sum_{k=1}^{K^{*}}\zeta_{k}^{-2}L_{\nu}\max_{w,w^{\prime} \in\mathcal{W}_{k}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{k}^{*},\Gamma)^ {-1}}^{2}\log\!\left(\frac{4k^{2}|\mathcal{W}|}{\delta}\right)+(1+\omega)\sum _{k=1}^{K^{*}}(N_{k,0,L_{k}}+N_{k,1,L_{k}})\] \[\lessapprox (1+\omega)K^{*}L_{\nu}\rho^{*}\log\!\left(\frac{4K^{*2}|\mathcal{ W}|}{\delta}\right)+(1+\omega)\sum_{k=1}^{K^{*}}(N_{k,0,L_{k}}+N_{k,1,L_{k}}).\]

This essentially means that the sample complexity for the second sub-phase \(\sum N_{k,2}\) can be upper bounded by summation of the sample complexity we pay for Algorithm 1 and the sample complexity of the first sub-phase \(\sum_{k=1}^{K^{*}}(N_{k,0,L_{k}}+N_{k,1,L_{k}})\). Combine (51) and (55), we conclude the result.

**Lemma J.3**.: _Denote_

\[\widetilde{\log}\bigl{(}N_{k,0},\delta_{k,0}\bigr{)}=8d\ln\left(1+\frac{N_{k,0 }L_{z}^{2}}{d}\right)+16\ln\left(\frac{2\cdot\!\delta^{d}}{\delta}\right).\]

_A sufficient condition for_

\[N_{k,0,\ell}\geq\frac{g\sigma_{\eta}^{2}\widetilde{\log}\bigl{(}N_{k,0,\ell}+N _{k,1,\ell},\delta_{k,\ell}\bigr{)}}{\sigma_{\min}\big{(}A(\lambda_{E},\Gamma) \big{)}}\lor r(\omega). \tag{55}\]

_to hold is_

\[N_{k,0,\ell}\geq\frac{16gd\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\lambda_{E},\Gamma)\big{)}}\ln\!\left(\frac{8g}{\sigma_{\min}\big{(}A(\lambda_{E},\Gamma) \big{)}}\Big{(}d+N_{k,1,\ell}L_{z}^{2}+L_{z}^{2}\Big{)}\right)+\frac{32g\sigma_{ \eta}^{2}}{\sigma_{\min}\big{(}A(\lambda_{E},\Gamma)\big{)}}\ln\!\left(\frac{2 \cdot\!\delta^{d}}{\delta}\right)\!.\]

Proof.: Given

\[\widetilde{\log}\bigl{(}N_{k,0,\ell}+N_{k,1,\ell},\delta_{k,\ell}\bigr{)}=8d \ln\left(1+\frac{(N_{k,0,\ell}+N_{k,1,\ell})L_{z}^{2}}{d}\right)+16\ln\left( \frac{2\cdot\!\delta^{d}}{\delta}\right).\]

By Lemma J.4, for the formula

\[X\geq A\ln\left(D+BX\right)+C\]

we have * \(A=\frac{8gd\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\lambda_{E},\Gamma)\big{)}}\),
* \(B=\frac{L_{2}^{2}}{d}\),
* \(C=\frac{16g\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\lambda_{E},\Gamma)\big{)} }\ln\Big{(}\frac{2\cdot 6^{d}}{\delta}\Big{)}\),
* \(D=1+\frac{N_{k,1,\ell}L_{\varepsilon}^{2}}{d}\).

Thus a sufficient condition for the inequality to hold is

\[N_{k,0,\ell}\geq \frac{16gd\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\lambda_{E}, \Gamma)\big{)}}\ln\Bigg{(}\frac{8gd\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A( \lambda_{E},\Gamma)\big{)}}\Bigg{(}1+\frac{N_{k,1,\ell}L_{\varepsilon}^{2}}{d }+\frac{L_{\varepsilon}^{2}}{d}\Bigg{)}\Bigg{)}+\frac{32g\sigma_{\eta}^{2}}{ \sigma_{\min}\big{(}A(\lambda_{E},\Gamma)\big{)}}\ln\Bigg{(}\frac{2\cdot 6^{d}}{ \delta}\Bigg{)}\] \[= \frac{16gd\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\lambda_{E}, \Gamma)\big{)}}\ln\Bigg{(}\frac{8g\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A( \lambda_{E},\Gamma)\big{)}}\Big{(}d+N_{k,1,\ell}L_{\varepsilon}^{2}+L_{ \varepsilon}^{2}\Big{)}\Bigg{)}+\frac{32g\sigma_{\eta}^{2}}{\sigma_{\min}\big{(} A(\lambda_{E},\Gamma)\big{)}}\ln\Bigg{(}\frac{2\cdot 6^{d}}{\delta}\Bigg{)}.\]

**Lemma J.4**.: _Let \(X\geq 1,A,B\geq 0\), then a sufficient condition for \(X\geq A\ln\big{(}D+BX)+C\) is_

\[X\geq 2A\ln(AD+AB)+2C.\]

Proof.: The proof is motivated by Gales et al. [16]. Let \(f\in(0,1)\), then

\[X\geq A\ln\big{(}D+BX\big{)}+C\] \[\Leftrightarrow X\geq A\ln\left(\frac{fX}{A}\right)+A\ln\left(\frac{A}{f}\bigg{(} \frac{D}{X}+B\bigg{)}\right)+C\] \[\Leftarrow X\geq A\left(\frac{fX}{A}-1\right)+A\ln\left(\frac{A}{f} \bigg{(}\frac{D}{X}+B\bigg{)}\right)+C\] (since \[\ln(x)\leq x-1\] ) \[\Leftarrow X(1-f)\geq A\ln\left(\frac{A}{2f}\bigg{(}\frac{D}{X}+B \bigg{)}\right)+C\] \[\Leftrightarrow X\geq\frac{1}{1-f}A\ln\left(\frac{A}{2f}\bigg{(}\frac{D}{X}+B \bigg{)}\right)+\frac{c}{1-f}.\]

Set \(f=1/2\) and by the fact \(X\geq 1\), we have

\[X\geq 2A\ln(AD+AB)+2C.\]

**Lemma J.5**.: _Suppose that we have a data set \(\{Z_{T},X_{T}\}\). Denote the empirical distribution of \(Z_{T}\) as \(\xi\). The number of samples satisfies_

\[T\geq\frac{8\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\xi,\Gamma)\big{)}}\overline {\log}(Z_{T},\delta)\lor r(\omega).\]

\(\widehat{\Gamma}\) _is the OLS estimate of \(\Gamma\) based on \(\{Z_{T},X_{T}\}\). Then_

\[\|w\|_{A(\xi,\Gamma)^{-1}}^{2}\leq 6\|w\|_{A(\xi,\widehat{\Gamma})^{-1}}^{2}.\]

Proof.: By Lemma J.7, we have

\[\|w\|_{A(\xi,\Gamma)^{-1}}^{2}\leq 3\|w\|_{A(\xi,\widehat{\Gamma})^{-1}}^{2}+2\Big{\|}(\Gamma^{- \top}-\widehat{\Gamma}^{-\top})w\Big{\|}_{A(\xi,I)^{-1}}^{2}\] \[\leq 3\|w\|_{A(\xi,\widehat{\Gamma})^{-1}}^{2}+\frac{1}{2}\|w\|_{A( \xi,\Gamma)^{-1}}^{2},\]where the last inequality is due to Lemma J.10 with \(g=2\). Rearranging the terms, we have

\[\|w\|^{2}_{A(\xi,\Gamma)^{-1}}\leq 6\|w\|^{2}_{A(\xi,\widehat{\Gamma})^{-1}}.\]

**Lemma J.6**.: _Suppose that we use ROUND to sample \(N_{0}\) arms according to \(\lambda_{E}\) denoted as \(Z_{0}\), and \(N_{1}\) arms according to \(\lambda_{1}\) denoted as \(Z_{1}\), with \(N_{1}\geq N_{0}\). Denote the empirical distribution of all the collected samples as \(\xi\), then_

\[\|w\|^{2}_{A(\xi,\Gamma)}\leq 4\|w\|^{2}_{A(\lambda_{1},\Gamma)^{-1}}.\]

Proof.: Denote the empirical distribution of \(Z_{0}\) as \(\xi_{0}\), and the empirical distribution of \(Z_{1}\) as \(\xi_{1}\), then we have

\[\|w\|^{2}_{A(Z_{0}\cup Z_{1},\Gamma)^{-1}}= w^{\top}\Bigg{(}\sum_{z\in Z_{0}\cup Z_{1}}\Gamma^{\top}zz^{\top} \Gamma\Bigg{)}^{-1}w\] \[= w^{\top}\Bigg{(}\sum_{z\in Z_{0}}\Gamma^{\top}zz^{\top}\Gamma+ \sum_{z\in Z_{1}}\Gamma^{\top}zz^{\top}\Gamma\Bigg{)}^{-1}w\] \[= w^{\top}\Bigg{(}N_{0}\sum_{z\in\mathcal{Z}}\xi_{z,0}\Gamma^{ \top}zz^{\top}\Gamma+N_{1}\sum_{z\in\mathcal{Z}}\xi_{z,1}\Gamma^{\top}zz^{ \top}\Gamma\Bigg{)}^{-1}w\] \[= \frac{1}{N_{0}+N_{1}}w^{\top}\Bigg{(}\frac{N_{0}}{N_{0}+N_{1}} \sum_{z\in\mathcal{Z}}\xi_{z,0}\Gamma^{\top}zz^{\top}\Gamma+\frac{N_{1}}{N_{0} +N_{1}}\sum_{z\in\mathcal{Z}}\xi_{z,1}\Gamma^{\top}zz^{\top}\Gamma\Bigg{)}^{-1}w\] \[\leq \frac{1}{N_{0}+N_{1}}w^{\top}\Bigg{(}\frac{N_{1}}{N_{0}+N_{1}} \sum_{z\in\mathcal{Z}}\xi_{z,1}\Gamma^{\top}zz^{\top}\Gamma\Bigg{)}^{-1}w\] \[\leq \frac{2}{N_{0}+N_{1}}w^{\top}\Bigg{(}\sum_{z\in\mathcal{Z}}\xi_{z,1}\Gamma^{\top}zz^{\top}\Gamma\Bigg{)}^{-1}w\] ( \[N_{1}\geq N_{0}\] ) \[\leq \frac{4}{N_{0}+N_{1}}w^{\top}\Bigg{(}\sum_{z\in\mathcal{Z}} \lambda_{1}\Gamma^{\top}zz^{\top}\Gamma\Bigg{)}^{-1}w\] \[= \frac{4}{N_{0}+N_{1}}\|w\|^{2}_{A(\lambda_{1},\Gamma)^{-1}}.\]

The result follows by noting that

\[\|w\|^{2}_{A(Z_{0}\cup Z_{1},\Gamma)^{-1}}=\frac{1}{N_{0}+N_{1}}\|w\|^{2}_{A( \xi,\Gamma)}.\]

**Lemma J.7**.: _Suppose that we have an estimate \(\widehat{\Gamma}\) that is invertible, for any \(x\in\mathbb{R}^{d}\) and covariance matrix \(V\), we have_

\[\|x\|^{2}_{(\Gamma^{\top}VT)^{-1}}\leq 3\|x\|^{2}_{(\widehat{\Gamma}^{\top}VT \widehat{\Gamma})^{-1}}+2\Big{\|}(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})x \Big{\|}^{2}_{V^{-1}}. \tag{56}\]

Proof.: Suppose we have an estimate \(\widehat{\Gamma}\). Then,

\[\|x\|^{2}_{(\Gamma^{\top}VT)^{-1}}=\|x\|^{2}_{(\widehat{\Gamma}^{\top}VT \widehat{\Gamma})^{-1}}+\|x\|^{2}_{(\Gamma^{\top}VT)^{-1}}-\|x\|^{2}_{(\widehat {\Gamma}^{\top}VT\widehat{\Gamma})^{-1}}.\]Note that

\[(\Gamma^{\top}V\Gamma)^{-1}-(\widehat{\Gamma}^{\top}V\widehat{\Gamma})^ {-1} =\Gamma^{-1}V^{-1}\Gamma^{-\top}-\widehat{\Gamma}^{-1}V^{-1} \widehat{\Gamma}^{-\top}\] \[=\Gamma^{-1}V^{-1}\Gamma^{-\top}-\widehat{\Gamma}^{-1}V^{-1} \widehat{\Gamma}^{-\top}+\Gamma^{-1}V^{-1}\widehat{\Gamma}^{-\top}-\Gamma^{-1} V^{-1}\widehat{\Gamma}^{-\top}\] \[=\Gamma^{-1}V^{-1}(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})+( \Gamma^{-1}-\widehat{\Gamma}^{-1})V^{-1}\widehat{\Gamma}^{-\top}.\]

Thus,

\[\|x\|_{(\Gamma^{\top}V\Gamma)^{-1}}^{2} =\|x\|_{(\widehat{\Gamma}^{\top}V\widehat{\Gamma})^{-1}}^{2}+x^{ \top}\Gamma^{-1}V^{-1}(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})x+x^{\top}( \Gamma^{-1}-\widehat{\Gamma}^{-1})V^{-1}\widehat{\Gamma}^{-\top}x\] \[\leq\|x\|_{(\Gamma^{\top}V\widehat{\Gamma})^{-1}}^{2}+\|x\|_{( \Gamma^{\top}V\Gamma)^{-1}}\Big{\|}(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})x \Big{\|}_{V^{-1}}+\Big{\|}(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})x\Big{\|}_ {V^{-1}}\|x\|_{(\widehat{\Gamma}^{\top}V\widehat{\Gamma})^{-1}}\] \[\leq\|x\|_{(\widehat{\Gamma}^{\top}V\widehat{\Gamma})^{-1}}^{2}+ \frac{1}{2}\|x\|_{(\Gamma^{\top}V\Gamma)^{-1}}^{2}+\frac{1}{2}\Big{\|}(\Gamma^ {-\top}-\widehat{\Gamma}^{-\top})x\Big{\|}_{V^{-1}}^{2}+\frac{1}{2}\Big{\|}( \Gamma^{-\top}-\widehat{\Gamma}^{-\top})x\Big{\|}_{V^{-1}}^{2}+\frac{1}{2}\|x \|_{(\widehat{\Gamma}^{\top}V\widehat{\Gamma})^{-1}}^{2}.\] (AM-GM)

This implies that

\[\|x\|_{(\Gamma^{\top}V\Gamma)^{-1}}^{2}\leq 3\|x\|_{(\Gamma^{\top}V\widehat{ \Gamma})^{-1}}^{2}+2\Big{\|}(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})x\Big{\|} _{V^{-1}}^{2}.\]

**Lemma J.8**.: _Suppose \(\lambda_{f}=\arg\min f(\lambda)\) and \(\lambda_{g}=\arg\min g(\lambda)\) and \(f(\lambda)\leq g(\lambda)+h(\lambda)\), then_

\[f(\lambda_{f})\leq g(\lambda_{g})+h(\lambda_{g}).\]

Proof.: \[f(\lambda_{f})\leq\min_{\lambda}\bigl{(}g(\lambda)+h(\lambda)\bigr{)}\leq g( \lambda_{g})+h(\lambda_{g}).\]

**Lemma J.9**.: _Define \(\lambda_{z}^{*}\) and \(\tilde{\lambda}_{z}\)_

\[\lambda_{z}^{*}:=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime}\in \mathcal{W}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{z},\Gamma)^{-1}}^{2},\]

_and_

\[\tilde{\lambda}_{z}:=\arg\min_{\lambda\in\Delta(\mathcal{Z})}\max_{w,w^{\prime }\in\mathcal{W}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{z},\widehat{\Gamma} )^{-1}}^{2}\]

_as the optimal design regarding \(\Gamma\) and that regarding its estimate \(\widehat{\Gamma}\) respectively. Then, we have_

Proof.: By Lemma J.7, for any \(w,w^{\prime}\in\mathcal{W}\) and \(\lambda_{z}\in\Delta_{\mathcal{Z}}\),

\[\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{z},\widehat{\Gamma})^{-1}}^{2}\leq 3 \bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{z},\Gamma)^{-1}}^{2}+2\Big{\|}( \Gamma^{-\top}-\widehat{\Gamma}^{-\top})\bigl{(}w-w^{\prime})\Big{\|}_{(\sum_{ z}\lambda_{z}zz^{\top})^{-1}}^{2}.\]

Thus

\[\max_{w,w^{\prime}\in\mathcal{W}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{z}, \widehat{\Gamma})^{-1}}^{2}\leq\max_{w,w^{\prime}\in\mathcal{W}}3\bigl{\|}w-w^ {\prime}\bigr{\|}_{A(\lambda_{z},\Gamma)^{-1}}^{2}+\max_{w,w^{\prime}\in \mathcal{W}}2\Big{\|}(\Gamma^{-\top}-\widehat{\Gamma}_{k}^{-\top})\bigl{(}w-w^ {\prime}\bigr{)}\Big{\|}_{(\sum_{z}\lambda_{z}zz^{\top})^{-1}}^{2}.\]

By Lemma J.8,

\[\max_{w,w^{\prime}\in\mathcal{W}}\bigl{\|}w-w^{\prime}\bigr{\|}_{A(\tilde{ \lambda}_{z},\widehat{\Gamma})^{-1}}^{2}\leq\max_{w,w^{\prime}\in\mathcal{W}}3 \bigl{\|}w-w^{\prime}\bigr{\|}_{A(\lambda_{z}^{*},\Gamma)^{-1}}^{2}+\max_{w,w^ {\prime}\in\mathcal{W}}2\Big{\|}(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})\bigl{(}w -w^{\prime}\bigr{)}\Big{\|}_{(\sum_{z}\lambda_{z}^{*}zz^{\top})^{-1}}^{2}.\]

**Lemma J.10**.: _Suppose that we have \(\widehat{\Gamma}\) that is an OLS estimate from an offline dataset \(\{Z_{T_{1}},X_{T_{1}}\}\) collected non-adaptively through a fixed design \(\lambda\) and the efficient rounding procedure ROUND. Let \(V=Z_{T_{1}}^{\top}Z_{T_{1}}\). Then, for any \(x\in\mathbb{R}^{d}\) and \(g\geq 1\), we have, with probability \(1-\delta\),_

\[\left\|(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})x\right\|_{V^{-1}}^{2}\leq\frac {1}{g}\|x\|_{\widetilde{A}(Z_{T_{1}},\Gamma)^{-1}}^{2},\]

_when_

\[T_{1}\geq\frac{4g\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\lambda,\Gamma)\big{)} \overline{\log}(Z_{T_{1}},\delta)\lor 2p, \tag{57}\]

_where \(p\) is the cardinality of support of \(\lambda\)._

Proof.: We first show that

\[\left\|(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})x\right\|_{V^{-1}}^ {2}\] (Lemma J.12) \[= \left\|\Gamma^{-\top}S^{\top}Z_{T_{1}}V^{-1}(\Gamma+V^{-1}Z_{T_{1 }}^{\top}S)^{-\top}x\right\|_{V^{-1}}^{2}\] \[\frac{a_{1}}{2}\] \[\leq \left\|V^{-\frac{1}{2}}\Gamma^{-\top}S^{\top}Z_{T_{1}}V^{-\frac{1 }{2}}\right\|_{\mathrm{op}}^{2}\left\|V^{-\frac{1}{2}}(\Gamma+V^{-1}Z_{T_{1}}^{ \top}S)^{-\top}x\right\|_{2}^{2}\] ( \[\left\|Ax\right\|\leq\left\|A\right\|_{\mathrm{op}}\|x\|\] \[= \left\|\Gamma^{-1}V^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}}\left\| V^{-1/2}Z_{T_{1}}^{\top}S\right\|_{\mathrm{op}}^{2}\left\|V^{-1/2}(\Gamma+V^{-1}Z_{T_{1 }}^{\top}S)^{-\top}x\right\|_{2}^{2}\] \[= \left\|\Gamma^{-1}V^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}}\left\| V^{-1/2}Z_{T_{1}}^{\top}S\right\|_{\mathrm{op}}^{2}\left\|V^{-1/2}(\Gamma+V^{-1}Z_{T_{1 }}^{\top}S)^{-\top}x\right\|_{2}^{2}\] \[= \left\|\Gamma^{-1}V^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}}\left\| V^{-1/2}Z_{T_{1}}^{\top}S\right\|_{\mathrm{op}}^{2}\left\|V^{-1/2}\Gamma^{-\top}x-V^{-1/2} \Gamma^{-\top}\left(V^{-1}Z_{T_{1}}^{\top}S\right)^{\top}\left(\Gamma+V^{-1}Z _{T_{1}}^{\top}S\right)^{-\top}x\right\|_{2}^{2}\] \[\stackrel{{ a_{2}}}{{\leq}} \left\|\Gamma^{-1}V^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}}\left\| V^{-1/2}Z_{T_{1}}^{\top}S\right\|_{\mathrm{op}}^{2}\left(\left\|V^{-1/2}\Gamma^{- \top}x\right\|_{2}^{2}+\left\|V^{-1/2}\Gamma^{-\top}\left(V^{-1}Z_{T_{1}}^{ \top}S\right)^{\top}\left(\Gamma+V^{-1}Z_{T_{1}}^{\top}S\right)^{-\top}x \right\|_{2}^{2}\right)\!.\]

We can upper bound the the term \(\left\|V^{-1/2}\Gamma^{-\top}\Big{(}V^{-1}Z_{T_{1}}^{\top}S\Big{)}^{\top} \Big{(}\Gamma+V^{-1}Z_{T_{1}}^{\top}S\Big{)}^{-\top}x\right\|_{2}^{2}=:\mathfrak{V}\) by noticing that it appears in both of \(a_{1},a_{2}\) above. Thus we have the inequality

\[\mathfrak{V}\leq\left\|\Gamma^{-1}V^{-1}\Gamma^{-\top}\right\|_{ \mathrm{op}}\left\|V^{-1/2}Z_{T_{1}}^{\top}S\right\|_{\mathrm{op}}^{2}\left( \left\|V^{-1/2}\Gamma^{-\top}x\right\|_{2}^{2}+\mathfrak{V}\right)\!.\]

By rearranging the terms, we have

\[\mathfrak{V}\leq\frac{1}{1-\left\|\Gamma^{-1}V^{-1}\Gamma^{-\top} \right\|_{\mathrm{op}}\left\|V^{-1/2}Z_{T_{1}}^{\top}S\right\|_{\mathrm{op}}^ {2}}\left\|\Gamma^{-1}V^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}}\left\|V^{-1/2 }Z_{T_{1}}^{\top}S\right\|_{\mathrm{op}}^{2}\left\|V^{-1/2}\Gamma^{-\top}x \right\|_{2}^{2}.\]

By Lemma G.4, with probability \(1-\delta\), we have

\[\left\|V^{-1/2}Z_{T_{1}}^{\top}S\right\|_{\mathrm{op}}^{2}\leq\sigma_{\eta}^{2} \overline{\log}(Z_{T_{1}},\delta).\]

Thus, with probability \(1-\delta\), we have

\[\mathfrak{V}\leq\frac{1}{1-\left\|\Gamma^{-1}V^{-1}\Gamma^{-\top} \right\|_{\mathrm{op}}\sigma_{\eta}^{2}\overline{\log}(Z_{T_{1}},\delta)}\left\| \Gamma^{-1}V^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}}\sigma_{\eta}^{2}\overline {\log}(Z_{T_{1}},\delta)\Big{\|}V^{-1/2}\Gamma^{-\top}x\right\|_{2}^{2}.\]To further upper bound \(\mathfrak{V}\), we first find a sufficient condition on \(T_{1}\) such that \(\left\|\Gamma^{-1}V^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}}\sigma_{\eta}^{2} \overline{\log}(Z_{T_{1}},\delta)\leq\frac{1}{2g},g\geq 1\). By Lemma J.14, when \(T_{1}\geq 2p\),

\[\left\|\Gamma^{-1}V^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}}\sigma_{\eta}^{2} \overline{\log}(Z_{T_{1}},\delta)\leq\frac{2\sigma_{\eta}^{2}}{T_{1}\sigma_{ \min}\Big{(}\sum_{z\in\mathcal{Z}}\lambda_{z}\Gamma^{\top}zz^{\top}\Gamma\Big{)} }\overline{\log}(Z_{T_{1}},\delta).\]

To upper bound the right hand side by \(\frac{1}{2g}\), we need

\[T_{1}\geq\frac{4g\sigma_{\eta}^{2}}{\sigma_{\min}\Big{(}\sum_{z \in\mathcal{Z}}\lambda_{z}\Gamma^{\top}zz^{\top}\Gamma\Big{)}}\overline{\log} (Z_{T_{1}},\delta). \tag{58}\]

With this condition (58), we have with probability \(1-\delta\),

\[\mathfrak{V}\leq \frac{1}{1-\left\|\Gamma^{-1}V^{-1}\Gamma^{-\top}\right\|_{ \mathrm{op}}\sigma_{\eta}^{2}\overline{\log}(Z_{T_{1}},\delta)}\left\|\Gamma^{ -1}V^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}}\sigma_{\eta}^{2}\overline{\log} (Z_{T_{1}},\delta)\left\|V^{-1/2}\Gamma^{-\top}x\right\|_{2}^{2}\] \[\leq \frac{1}{1-\frac{1}{2g}}\frac{1}{2g}\Big{\|}V^{-1/2}\Gamma^{- \top}x\Big{\|}_{2}^{2}\] \[\leq \frac{1}{g}\Big{\|}V^{-1/2}\Gamma^{-\top}x\Big{\|}_{2}^{2}. \tag{59}\]

**Lemma J.11**.: _Suppose that we have \(\widehat{\Gamma}\) that is an OLS estimate from an offline dataset \(\{Z_{T_{1}},X_{T_{1}}\}\) collected non-adaptively through a fixed design \(\lambda\) and the efficient rounding procedure ROUND. Let \(\hat{V}\) be any positive definite matrix. Then, for any \(x\in\mathbb{R}^{d}\), we have, with probability \(1-\delta\),_

\[\left\|(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})x\right\|_{\hat{V }^{-1}}^{2}\leq 2\Big{\|}\Gamma^{-1}\hat{V}^{-1}\Gamma^{-\top}\Big{\|}_{ \mathrm{op}}\sigma_{\eta}^{2}\overline{\log}(Z_{T_{1}},\delta)\|x\|_{A(Z_{T_{ 1}},\Gamma)^{-1}}^{2},\]

_when_

\[T_{1}\geq\frac{4\sigma_{\eta}^{2}}{\sigma_{\min}\big{(}A(\lambda, \Gamma)\big{)}}\overline{\log}(Z_{T_{1}},\delta)\lor 2p,\]

_where \(p\) is the cardinality of support of \(\lambda\)._

Proof.: \[\left\|(\Gamma^{-\top}-\widehat{\Gamma}^{-\top})x\right\|_{\hat{V }^{-1}}^{2}\] \[= \left\|\Gamma^{-\top}S^{\top}Z_{T_{1}}V^{-1}(\Gamma+V^{-1}Z_{T_{1 }}^{\top}S)^{-\top}x\right\|_{\hat{V}^{-1}}^{2}\] (Lemma J.12) \[\stackrel{{ a.i.}}{{=}} \left\|\hat{V}^{-\frac{1}{2}}\Gamma^{-\top}S^{\top}Z_{T_{1}}V^{-1}( \Gamma+V^{-1}Z_{T_{1}}^{\top}S)^{-\top}x\right\|_{2}^{2}\] \[\leq \left\|\hat{V}^{-\frac{1}{2}}\Gamma^{-\top}S^{\top}Z_{T_{1}}V^{- \frac{1}{2}}\right\|_{\mathrm{op}}^{2}\left\|V^{-\frac{1}{2}}(\Gamma+V^{-1}Z_{T _{1}}^{\top}S)^{-\top}x\right\|_{2}^{2}\] ( \[\left\|Ax\right\|\leq\left\|\Delta\right\|_{\mathrm{op}}\|x\|\] ) \[\leq \left\|\hat{V}^{-\frac{1}{2}}\Gamma^{-\top}\right\|_{\mathrm{op}} ^{2}\Big{\|}S^{\top}Z_{T_{1}}V^{-\frac{1}{2}}\Big{\|}_{\mathrm{op}}^{2}\Big{\|} V^{-\frac{1}{2}}(\Gamma+V^{-1}Z_{T_{1}}^{\top}S)^{-\top}x\Big{\|}_{2}^{2}\] ( \[\left\|AB\right\|_{\mathrm{op}}\leq\left\|A\right\|_{\mathrm{op}} \|B\Big{\|}_{\mathrm{op}}\] \[= \left\|\Gamma^{-1}\hat{V}^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}} \Big{\|}V^{-1/2}Z_{T_{1}}^{\top}S\Big{\|}_{\mathrm{op}}^{2}\Big{\|}V^{-1/2} \Big{(}\Gamma+V^{-1}Z_{T_{1}}^{\top}S\Big{)}^{-\top}x\Big{\|}_{2}^{2}\] \[= \left\|\Gamma^{-1}\hat{V}^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}} \Big{\|}V^{-1/2}Z_{T_{1}}^{\top}S\Big{\|}_{\mathrm{op}}^{2}\Big{\|}V^{-1/2} \Gamma^{-\top}x-V^{-1/2}\Gamma^{-\top}\Big{(}V^{-1}Z_{T_{1}}^{\top}S\Big{)}^{ \top}\Big{(}\Gamma+V^{-1}Z_{T_{1}}^{\top}S\Big{)}^{-\top}x\Big{\|}_{2}^{2}\] (Lemma J.13: \[(A+B)^{-1}=A^{-1}-(A+B)^{-1}BA^{-1}\] \[\stackrel{{ a.i.}}{{\leq}} \left\|\Gamma^{-1}\hat{V}^{-1}\Gamma^{-\top}\right\|_{\mathrm{op}} \Big{\|}V^{-1/2}Z_{T_{1}}^{\top}S\Big{\|}_{\mathrm{op}}^{2}\Bigg{(}\left\|V^{-1/2 }\Gamma^{-\top}x\right\|_{2}^{2}+\left\|V^{-1/2}\Gamma^{-\top}\Big{(}V^{-1}Z_{T _{1}}^{\top}S\Big{)}^{\top}\Big{(}\Gamma+V^{-1}Z_{T_{1}}^{\top}S\Big{)}^{-\top}x \right\|_{2}^{2}\Bigg{)}.\]Given the condition (58) holds, we have with probability \(1-\delta\),

\[\left\|\left(\Gamma^{-\top}-\widehat{\Gamma}^{-\top}\right)x\right\| _{\dot{V}^{-1}}^{2}\] \[\stackrel{{ a_{1}}}{{\leq}} \left\|\Gamma^{-1}\dot{V}^{-1}\Gamma^{-\top}\right\|_{\mathrm{op }}\left\|V^{-1/2}Z_{T_{1}}^{\top}S\right\|_{\mathrm{op}}^{2}\left(\left\|V^{-1/ 2}\Gamma^{-\top}x\right\|_{2}^{2}+\left\|V^{-1/2}\Gamma^{-\top}x\right\|_{2}^{ 2}\right)\] \[= 2\left\|\Gamma^{-1}\dot{V}^{-1}\Gamma^{-\top}\right\|_{\mathrm{op }}\left\|V^{-1/2}Z_{T_{1}}^{\top}S\right\|_{\mathrm{op}}^{2}\left\|V^{-1/2} \Gamma^{-\top}x\right\|_{2}^{2}\] \[\leq 2\left\|\Gamma^{-1}\dot{V}^{-1}\Gamma^{-\top}\right\|_{\mathrm{ op}}\sigma_{\eta}^{2}\overline{\log}(Z_{T_{1}},\delta)\|x\|_{(\Gamma^{\top}V )^{-1}}^{2},\]

where \((a_{1})\) is due to (59) and setting \(g=1\). 

**Lemma J.12**.: _For a least square estimate \(\widehat{\Gamma}\) that is estimated through a design matrix \(Z\) and \(\widehat{\Gamma}\) is invertible, we have_

\[\widehat{\Gamma}^{-1}-\Gamma^{-1}=-\Big{(}\Gamma+V^{-1}Z^{\top}S\Big{)}^{-1}V^ {-1}Z^{\top}S\Gamma^{-1}.\]

Proof.: Since \(\widehat{\Gamma}\) is a least square estimator, we have

\[\widehat{\Gamma}=\Gamma+V^{-1}Z^{\top}S.\]

By Lemma J.13, we have

\[\widehat{\Gamma}^{-1}-\Gamma^{-1}= \Big{(}\Gamma+V^{-1}Z^{\top}S\Big{)}^{-1}-\Gamma^{-1}\] \[= -\Big{(}\Gamma+V^{-1}Z^{\top}S\Big{)}^{-1}V^{-1}Z^{\top}S\Gamma^{ -1}.\]

**Lemma J.13**.: _For two invertible matrixes \(A,B\in\mathbb{R}^{d\times d}\), we have_

\[(A+B)^{-1}=A^{-1}-(A+B)^{-1}BA^{-1}.\]

Proof.: We have

\[(A+B)^{-1} =A^{-1}+(A+B)^{-1}-A^{-1}\] \[=A^{-1}+\Big{(}(A+B)^{-1}A-I\Big{)}A^{-1}\] \[=A^{-1}+(A+B)^{-1}\big{(}A-(A+B)\big{)}A^{-1}\] \[=A^{-1}-(A+B)^{-1}BA^{-1}.\]

**Lemma J.14**.: _Suppose that we have a design matrix \(Z_{T}\) that is sampled from a distribution \(\lambda\in\Delta_{\mathcal{Z}}\), with the efficient rounding procedure ROUND. Let \(p\) represent the cardinality of support of \(\lambda\). We have, if \(T\geq 2p\),_

\[\left\|\left(\Gamma^{\top}Z_{T}^{\top}Z_{T}\Gamma\right)^{-1}\right\|_{\mathrm{ op}}\leq\frac{2}{T\sigma_{\min}\Big{(}\sum_{z\in\mathcal{Z}}\lambda_{z} \Gamma^{\top}zz^{\top}\Gamma\Big{)}}.\]

_where \(\sigma_{\min}(\cdot)\) is the smallest singular value of a matrix._

Proof.: Suppose that each arm \(z\in\mathcal{Z}\) is sampled \(t_{z}\) times, the empirical distribution of \(Z_{T}\) is \(\xi:=\left(\frac{t_{z}}{T}\right)_{z\in\mathcal{Z}}\). Thus, we have

\[\left\|\left(\Gamma^{\top}Z_{T}^{\top}Z_{T}\Gamma\right)^{-1} \right\|_{\mathrm{op}} =\frac{1}{\sigma_{\min}\big{(}\Gamma^{\top}Z_{T}^{\top}Z_{T}\Gamma \big{)}}\] \[=\frac{1}{\sigma_{\min}\Big{(}T\sum_{z\in\mathcal{Z}}\xi_{z} \Gamma^{\top}zz^{\top}\Gamma\Big{)}}\] \[=\frac{1}{T\sigma_{\min}\Big{(}\sum_{z\in\mathcal{Z}}\xi_{z} \Gamma^{\top}zz^{\top}\Gamma\Big{)}}.\]By Fiez et al. [15, Proposition 2], we have

\[\sum_{z\in\mathcal{Z}}\xi_{z}\Gamma^{\top}zz^{\top}\Gamma\succeq\alpha\sum_{z\in \mathcal{Z}}\lambda_{z}\Gamma^{\top}zz^{\top}\Gamma,\]

where \(\alpha\in[\frac{T}{T+2p},1]\) when \(T\geq 2p\). Given the fact that both of \(\sum_{z\in\mathcal{Z}}\xi_{z}\Gamma^{\top}zz^{\top}\Gamma\) and \(\sum_{z\in\mathcal{Z}}\lambda_{z}\Gamma^{\top}zz^{\top}\Gamma\) are positive definite, we have \(\sigma_{\min}\Big{(}\sum_{z\in\mathcal{Z}}\xi_{z}\Gamma^{\top}zz^{\top}\Gamma \Big{)}\geq\alpha\sigma_{\min}\Big{(}\sum_{z\in\mathcal{Z}}\lambda_{z}\Gamma^{ \top}zz^{\top}\Gamma\Big{)}\). Thus, we have

\[\left\|\Big{(}\Gamma^{\top}Z_{T}^{\top}Z_{T}\Gamma\Big{)}^{-1}\right\|_{\rm op }\leq\frac{1}{\alpha T\sigma_{\min}\Big{(}\sum_{z\in\mathcal{Z}}\lambda_{z} \Gamma^{\top}zz^{\top}\Gamma\Big{)}}.\]

When \(T\geq 2p\), we have \(\alpha\geq 1/2\), which implies the result. 

## Appendix K Estimating \(\lambda_{\min}(\Gamma)\)

In this section, we introduce a simple adaptive procedure that finds a high probability lower bound on \(\gamma_{\min}^{*}:=\lambda_{\min}(\Gamma)\) that is sufficiently accurate (i.e., within a constant factor of \(\gamma_{\min}^{*}\)). For simplicity, we assume \(\|z_{t}\|\leq 1,\forall t\) in this section.

Our algorithm leverages confidence bounds to adaptively determine how many samples we like to take. Let \(\hat{\Gamma}_{t}:=(Z^{\top}Z)^{-1}Z^{\top}X\) be the least square estimate of \(\Gamma\) after sampling \(t\) times to obtain \(\{(z_{s},x_{s})\}_{s=1}^{t}\) where \(Z\in\mathbb{R}^{t\times d}\) and \(X\in\mathbb{R}^{t\times d}\) are the design matrices. Let \(V_{t}:=\sum_{s=1}^{t}z_{s}z_{s}^{\top}\). We define the lower and upper confidence bound for \(\gamma_{\min}^{*}\) as follows:

\[\text{LCB}(t):=\lambda_{\min}(\hat{\Gamma}_{t})-\sqrt{\frac{\psi_{t}}{t}}, \quad\text{UCB}(t):=\lambda_{\min}(\hat{\Gamma}_{t})+\sqrt{\frac{\psi_{t}}{t}}\]

where

\[\psi_{t}=\sigma_{\min}^{-1}\left(\frac{1}{t}V_{t}\right)\cdot\left(8d\ln \left(1+\frac{2t}{d(2\wedge\sigma_{\min}(V_{t}))}\right)+16\ln\left(\frac{2 \cdot 6^{d}}{\delta}\cdot\log_{2}^{2}\left(\frac{4}{2\wedge\sigma_{\min}(V_{t })}\right)\right)\right)\.\]

and \(\text{LCB}(0):=-\infty\) and \(\text{UCB}(0):=\infty\).

The following lemma shows that \(\text{LCB}(t)\) and \(\text{UCB}(t)\) form a valid anytime confidence bound for \(\gamma_{\min}^{*}\).

**Lemma K.1**.: _(Correctness of the confidence bounds)_

\[1-\delta\leq\mathbb{P}(\forall t\geq 1,\text{LCB}(t)\leq\gamma_{\min}^{*}\leq \text{UCB}(t))\]

Equipped with the confidence bounds, we are now ready to describe our algorithm for learning \(\gamma_{\min}^{*}\) (see Algorithm 10). Since the tightness of the confidence bounds depends on the smallest eigenvalue of \(V_{t}\), it is natural to use the E-optimal design as defined in Section 3.2. Recall that the solution of the E-optimal design is \(\lambda_{E}^{*}\) and \(\kappa_{0}\) is the smallest singular value achieved by \(\lambda_{E}^{*}\). We take in a rounding procedure for the E-optimal design \(\text{ROUND}_{E}(\lambda,t)\) that takes in \(t\) samples and design \(\lambda\) and outputs integer sample count assignments \(\{N_{z}\}_{z\in Z}\) so that if we sample according to these counts then we have

\[\sigma_{\min}^{-1}(\frac{1}{t}V_{t})\leq(1+\omega)\kappa_{0}^{-1} \tag{60}\]

After determining the base sample counts \(\{m_{z}\}_{z\in\mathcal{Z}}\) by \(\text{ROUND}(\lambda_{E}^{*},\lceil r(\omega)\rceil,\omega)\), we start doubling the sample size until we satisfy the condition \(\text{LCB}(t)>\frac{1}{2}\text{UCB}(t)\). Note that the sampling scheme in the while loop is designed such that the total number of samples collected up to (and including) \(j\)-th iteration is \(2^{J-1}\lceil r(\omega)\rceil\). Once the loop stops, we return \(\text{LCB}(t)\) as the claimed lower approximation of the \(\gamma_{\min}^{*}\).

Let \(N_{w}\) be the total number of samples we used in Algorithm 10. Then, the next theorem shows that the estimate returned by our algorithm is both a valid lower bound to \(\gamma_{\min}^{*}\) and sufficiently accurate.

**Theorem K.2**.: _(Correctness of Algorithm 10) The total number of samples denoted by \(N_{w}\) used in Algorithm 10 satisfies that, with probability at least \(1-\delta\),_

\[\frac{\gamma_{\min}^{*}}{2}<\text{LCB}(N_{w})\leq\gamma_{\min}^{*}\]

We next analyze the sample complexity of the algorithm, which essentially shows the scaling of \(\frac{1}{(\gamma_{\min}^{*})^{2}\kappa_{0}}\) even if the algorithm does not need knowledge of \(\gamma_{\min}^{*}\).

**Theorem K.3**.: _(Sample complexity of Algorithm 10) Then, with \(\omega=1\), we have, with probability at least \(1-\delta\),_

\[N_{w}=O\left(r(1)+(\gamma_{\min}^{*})^{-2}\kappa_{0}^{-1}\cdot\left(d\cdot \text{polylog}((\gamma_{\min}^{*})^{-2},\kappa_{0}^{-1},d)+\ln\bigl{(}2/\delta \bigr{)}\right)\right)\]

We remark that Allen-Zhu et al. [2] provides a rounding procedure with \(r(\varepsilon)=O(d/\varepsilon^{2})\).

Proof of Lemma k.1.: Note that \(\hat{\Gamma}=\Gamma+V_{t}^{-1}Z^{\top}S\) where \(Z,S\in\mathbb{R}^{t\times d}\) is the design matrices with \(s\)-th row being \(z_{s}^{\top}\) and \(\eta_{s}^{\top}\) respectively. Using Lemma G.4, we have, with probability at least \(1-\delta\),

\[\forall t\geq 1,\left\lVert\hat{\Gamma}_{t}-\Gamma\right\rVert_{ \text{op}}^{2} =\left\lVert V_{t}^{-1}Z^{\top}S\right\rVert_{\text{op}}^{2}\] \[\leq\left\lVert V_{t}^{-1/2}\right\rVert_{\text{op}}^{2}\bigl{\lVert} V_{t}^{-1/2}Z^{\top}S\bigr{\rVert}_{\text{op}}^{2}\] \[=\frac{1}{t\sigma_{\min}(\frac{1}{t}V_{t})}\bigl{\lVert}V_{t}^{-1 /2}Z^{\top}S\bigr{\rVert}_{\text{op}}^{2}\] \[\leq\frac{1}{t\sigma_{\min}(\frac{1}{t}V_{t})}\cdot\left(8d\ln \left(1+\frac{2t}{d(2\wedge\sigma_{\min}(V_{t}))}\right)+16\ln\left(\frac{2 \cdot 6^{d}}{\delta}\cdot\log_{2}^{2}\left(\frac{4}{2\wedge\sigma_{\min}(V_{t})} \right)\right)\right)\] ( Lemma G.4) \[=\frac{1}{t}\psi_{t}\]

The well-known Weyl's theorem implies that \(\max_{k}|\lambda_{k}(\hat{\Gamma}_{t})-\lambda_{k}(\Gamma)|\leq\left\lVert \hat{\Gamma}_{t}-\Gamma\right\rVert_{\text{op}}\) where \(\lambda_{k}(A)\) is the \(k\)-th largest singular value of the matrix \(A\). Choosing \(k=d\) and combining it with the display above conclude the proof. 

Proof of Theorem k.2.: We assume \(\forall t\geq 1,\text{LCB}(t)\leq\gamma_{\min}^{*}\leq\text{UCB}(t)\), which happens with probability at least \(1-\delta\). Then, it is trivial to see that \(\text{LCB}(N_{w})\leq\gamma_{\min}^{*}\).

For the other inequality, we use the fact that the stopping condition was satisfied with \(N_{w}\):

\[\text{LCB}(N_{w})>\frac{1}{2}\text{UCB}(N_{w})\geq\frac{\gamma_{\min}^{*}}{2}\;.\]

This concludes the proof.

Proof of Theorem k.3.: We assume \(\forall t\geq 1,\text{LCB}(t)\leq\lambda_{\min}(\Gamma)\leq\text{UCB}(t)\), which happens with probability at least \(1-\delta\). In this proof, we let \(\hat{\gamma}_{\min}:=\lambda_{\min}(\hat{\Gamma}_{N_{w}})\) and \(\gamma_{\min}^{*}:=\lambda_{\min}(\Gamma)\) for brevity.

If \(N_{w}=\lceil r(1)\rceil\), then there is nothing to prove. Otherwise, the loop in the algorithm was iterated more than once. Then, since the stopping condition was satisfied with \(N_{w}\), we have that in the previous iteration where \(t=N_{w}/2\) the stopping condition was not satisfied. Thus,

\[2\text{LCB}(N_{w}/2)\leq\text{UCB}(N_{w}/2)\;.\]

Using the following two inequalities:

\[2\text{LCB}(N_{w}/2) \geq 2\left(\hat{\gamma}_{\min}-\sqrt{\frac{\psi_{N_{w}/2}}{N_{w}/ 2}}\right)\geq 2\left(\gamma_{\min}^{*}-2\sqrt{\frac{\psi_{N_{w}/2}}{N_{w}/ 2}}\right)\] \[\text{UCB}(N_{w}/2) \leq\hat{\gamma}_{\min}+\sqrt{\frac{\psi_{N_{w}/2}}{N_{w}/2}}\leq \gamma_{\min}^{*}+2\sqrt{\frac{\psi_{N_{w}/2}}{N_{w}/2}}\;,\]

we have

\[\gamma_{\min}^{*}\leq 6\sqrt{\frac{\psi_{N_{w}/2}}{N_{w}/2}}\implies N_{w} \leq\frac{72}{(\gamma_{\min}^{*})^{2}}\psi_{N_{w}/2}\;.\]

On the other hand, with the rounding procedure, we have

\[\sigma_{\min}^{-1}\left(\sum_{t=1}^{N}z_{t}z_{t}^{\top}\right)= \frac{1}{N}\sigma_{\min}^{-1}\left(\frac{1}{N}\sum_{t=1}^{N}z_{t}z_{t}^{\top} \right)=\frac{1}{N}\sigma_{\max}\left(\left(\frac{1}{N}\sum_{t=1}^{N}z_{t}z_{t }^{\top}\right)^{-1}\right)\leq\frac{1}{N}(1+\omega)\kappa_{0}^{-1}=\frac{2}{N} \kappa_{0}^{-1}\]

since \(\omega=1\). Using this and the fact that \(N_{w}\geq d\), it is easy to see that there exists an absolute constant \(c_{1}\) such that

\[\psi_{N_{w}/2}\leq c_{1}\kappa_{0}^{-1}\left(d\ln\left(1+N_{w}+ \kappa_{0}^{-1}\right)+\ln\left(\frac{2}{\delta}\right)\right)\;.\]

Then, there exists an absolute constant \(c_{2}\) such that

\[N_{w}\leq(\gamma_{\min}^{*})^{-2}\cdot c_{2}\kappa_{0}^{-1}\left(d\ln\left(1+ N_{w}+\kappa_{0}^{-1}\right)+\ln\left(\frac{2}{\delta}\right)\right)\]

We have \(N_{w}\) on both sides. We invoke Lemma K.5 with \(r=1+N_{w}\) to obtain

\[N_{w}\leq 1+2c_{2}(\gamma_{\min}^{*})^{-2}\kappa_{0}^{-1}\left(d\ln\Bigl{(}1+2 \kappa_{0}^{-1}(1+c_{2}d(\gamma_{\min}^{*})^{-2})\Bigr{)}+\ln\left(\frac{2}{ \delta}\right)\right)\;.\]

**Lemma K.4**.: _Let \(A,\Gamma\in\mathbb{R}^{d\times d}\) where \(A\) is symmetric positive semi-definite. Then,_

\[\sigma_{\min}\Bigl{(}\Gamma^{\top}A\Gamma\Bigr{)}\geq\sigma_{\min} (\Gamma)^{2}\sigma_{\min}(A)\]

Proof.: \[(\sigma_{\min}(\Gamma^{\top}A\Gamma))^{-1} =\|\Gamma^{-1}A^{-1}\Gamma^{-T}\|_{\text{op}}\] \[\leq\|\Gamma^{-1}\|_{\text{op}}\|A^{-1}\|_{\text{op}}\|\Gamma^{-T }\|_{\text{op}}\qquad\text{ (submultiplicity of the operator norm)}\] \[=\sigma_{\min}(\Gamma)^{-2}\sigma_{\min}(A)^{-1}\]

Taking the inverse on both sides concludes the proof.

[MISSING_PAGE_FAIL:57]

Proof.: \[r \leq\alpha\ln\biggl{(}r(\frac{1}{r}+1)\biggr{)}+\beta\] \[\leq\alpha\ln\biggl{(}\frac{r}{2\alpha}.2\alpha(\frac{1}{r}+1) \biggr{)}+\beta\] \[\leq\alpha\left(\frac{r}{2\alpha}-1+\ln\biggl{(}2\alpha(\frac{1}{ r}+1)\biggr{)}\right)+\beta\] ( \[\forall x,\ln(x)\leq x-1\] ) \[\implies r \leq 2\alpha\ln\biggl{(}\frac{2}{e}\alpha(\frac{1}{r}+1)\biggr{)}+\beta\]

If \(r\leq 2\alpha\), then there is nothing to prove. If \(r>2\alpha\), then \(r\leq 2\alpha\ln(1+\alpha)+2\beta\). Either case, we have \(r\leq 2\alpha\ln(e+\alpha)+2\beta\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Abstract and introduction summarize our main theorems. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: For example, we explain our limitations on lower bound. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The assumptions are given is the beginning of the paper and referred later when necessary. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Details on the experiment setup are given. It is mainly a theoretical paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: It is mainly a theoretical paper. We attach the code used for the simulation. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: It is mainly a theoretical paper with some simulations in appendix. No deep learning or other complicated training involved. But we attach our code for reference. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bar is reported, e.g., Fig 2b. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: It is mainly a theoretical paper. Some simulations are given in the appendix. No need for heavy computation resource. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: This paper conforms all the code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: we discuss it in the appendix. It is mainly a theoretical and algorithmic paper, without any sensitive data, very unlikely to have negative impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: the paper poses no such risks, no such data or model involved. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We do not use any existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not produce any new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not use anything like crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not use anything like crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.