# Cookie Consent Has Disparate Impact on

Estimation Accuracy

 Erik Miehling Rahul Nair Elizabeth Daly

Karthikeyan Natesan Ramamurthy Robert Redmond

IBM Research

erik.miehling@ibm.com

{rahul.nair,elizabeth.daly}@ie.ibm.com

{knatesa,rredmond}@us.ibm.com

###### Abstract

Cookies are designed to enable more accurate identification and tracking of user behavior, in turn allowing for more personalized ads and better performing ad campaigns. Given the additional information that is recorded, questions related to privacy and fairness naturally arise. How does a user's consent decision influence how much the system can learn about their demographic and tastes? Is the impact of a user's consent decision on the recommender system's ability to learn about their latent attributes uniform across demographics? We investigate these questions in the context of an engagement-driven recommender system using simulation. We empirically demonstrate that when consent rates exhibit demographic-dependence, user consent has a disparate impact on the recommender agent's ability to estimate users' latent attributes. In particular, we find that when consent rates are demographic-dependent, a user disagreeing to share their cookie may counter-intuitively cause the recommender agent to know more about the user than if the user agreed to share their cookie. Furthermore, the gap in base consent rates across demographics serves as an amplifier: users from the lower consent rate demographic who provide consent generally experience higher estimation errors than the same users from the higher consent rate demographic, and conversely for users who choose to withhold consent, with these differences increasing in consent rate gap. We discuss the need for new notions of fairness that encourage consistency between a user's privacy decisions and the system's ability to estimate their latent attributes.

## 1 Introduction

Increased regulation surrounding cookies has emerged in recent years in response to growing concerns over online privacy and data protection. In the EU, cookie policy is driven primarily by the General Data Protection Regulation [(13)] and the ePrivacy Directive [(10)] which dictate that if a website wishes to use cookies to identify and track users, beyond "strictly necessary cookies," then consent must be explicitly obtained from the user. There is currently no federal law regulating the use of cookies in the US, however, several states have imposed regulation that requires websites to disclose their data collection practices to users [(5, 40)].

Marketers use cookies to narrow in on target audiences most likely to buy their products, primarily via improved user tracking and enhanced personalization. The actual mechanisms for obtaining user consent can introduce some additional fairness and privacy concerns. Many cookie consent interfaces are designed such that web content is at least partially obscured until the user makes aconsent decision. These interfaces often exhibit _deceptive design patterns_[38, 34, 15, 16] - design practices that incentivize users to make choices that lower their privacy. While some websites do offer a clear "reject all" button, many present the user with a decision between a simple "I agree" or "accept all" and a much less immediate choice of "manage preferences." This leads to many users agreeing to cookic tracking simply out of convenience [17]. Furthermore, and an observation that forms the basis of the current paper, evidence suggests that users exhibit inherent differences in their likelihood of agreeing to cookic tracking. A recent study by YouGov [45] revealed that a user's consent rate is influenced by their age and culture/geographical location. Of the markets surveyed in the study, users ranged from 64% agree (Poland) to 32% agree (US), with older individuals being less likely to provide consent than younger individuals, perhaps partially due to their overall lowered trust in tech companies when it comes to their personal data [46].

In this paper, we investigate how demographic-dependent consent rates impact the ability of the recommender system to learn users' latent attributes and offer targeted recommendations. The main finding of our paper is that when user consent rates differ by demographics, the recommender system possesses disparate estimation accuracies across users in different demographics. The finding is based on a simple model of recommendations where, prior to the interaction, users provide cookie consent according to demographic-dependent probabilities. Users' consent decisions, along with the cookie for those who have provided consent, allow the recommender agent to form refined beliefs on users' demographics and preferences. Through sequential interaction with the users, the agent learns to personalize content to maximize engagement. We empirically illustrate the following:

* _Disparate impact of consent:_ Under demographic-dependent consent rates, users' consent decisions can have a disparate impact on the recommender agent's estimation accuracy of users' latent attributes. In particular, we find that withholding consent can lead to _lower_ estimation errors for users in the lower consent rate population. In other words, a recommender system may know more about a person who opts not be tracked compared to those who willingly provide tracking information. Additionally, users from a population with a low consent rate who provide consent experience higher estimation errors than the same users from a high consent rate population.
* _Amplification effects:_ The difference in consent rates across cohorts serves as an amplifier for the above effects, with the disparities in estimation errors across cohorts increasing in the consent rate gap.

Note that our model is simple: ads are described by a single feature, the user pool is fixed, and user affinities/preferences do not change over time. This simplicity is intentional, with the goal being to isolate and understand the effects of specific model aspects (i.e., impact of different user consent rates) without the additional noise and confounding factors present in real-world settings. The observations on disparate impact that emerge from our setting should serve as an indicator that the same negative results may also exist in more complex recommender systems. Additionally, given the fundamental nature of the observed disparities, we believe that the search for mitigation strategies in this simplified setting can provide useful insights for the design of more complex systems.

## 2 Background

**Recommender systems.** The main challenge in recommender systems is making recommendations when only a sparse set of preference data is available. Broadly, recommender systems address this missingness via two approaches: content-based filtering, and collaborative filtering.1 Content-based filtering methods use known information about users and items to suggest content. For example, a content-based movie recommender system may compare known features of a movie (degree of humor, action, drama) with known user preferences to suggest movies to users that yield the greatest similarity. Collaborative filtering, on the other hand aims to \(learn\) these relevant features based on patterns in the observed preference/response data. Items in a collaborative filtering-based system are recommended to users based on what content other users with similar behavior have consumed (hence the term collaborative).

A popular collaborative-filtering algorithm is matrix factorization. Matrix factorization asserts that user data is not missing uniformly at random, but rather according to some low rank structure of which it aims to discover. The learned factors can then be used to infer unobserved ratings for other user-item pairs, in turn driving recommendations. Associating each user with factor \(u_{i}\in\mathbb{R}^{k}\) and each ad with factor \(v_{a}\in\mathbb{R}^{k}\), latent factor estimates \((\hat{\bm{u}},\hat{\bm{v}})=((u_{i})_{i},(v_{a})_{a})\) are obtained under standard matrix factorization via solution of a regularized loss-minimization on the known user responses

\[(\hat{\bm{u}},\hat{\bm{v}})=\operatorname*{argmin}_{(\bm{u},\bm{v})}\bigg{[} \sum_{(i,a)\in\mathcal{R}}(u_{i}^{\top}v_{a}-r_{i,a})^{2}+\lambda\bigg{(}\sum_ {i}\|u_{i}\|_{2}+\sum_{a}\|v_{a}\|_{2}\bigg{)}\bigg{]}\]

where \(\mathcal{R}\) is the set of user-item pairs \((i,a)\) where a response \(r_{i,a}\) has been recorded. A variety of modifications of standard matrix factorization exist (25), including incorporating ranking biases, user and item side information, varying confidence weights, and temporal effects. The above optimization is typically carried out on a mix of both historical data and new interaction data, with the latter data incorporated periodically via retraining.

**Cookies, consent, and behavioral advertising.** Cookies are small pieces of data that are stored on a user's device with the specific purpose ranging from storing login credentials (first-party cookies) to tracking user behavior for targeted advertising (third-party cookies). Given their capability of enabling inference of potentially sensitive user information (9, 39), regulation has emerged to increase transparency around their use. While specific laws vary, many countries now require web publishers to present some form of banner or pop-up either informing the user that information is being recorded or requesting the user to indicate their cookie consent decision prior to browsing the page.

Behavioral advertising describes the practice of using learned user behavior (e.g., user preferences) to make personalized recommendations. Cookies help to facilitate inference of this information via tracking user behavior across websites (e.g., search history, clicks, purchases). The end goal of behavioral advertising is to increase relevancy of recommendations, in turn increasing user engagement and the return of the ad campaign.

## 3 Related work

**Algorithmic feedback loops.** Much of the analysis of algorithmic feedback loops in the literature is centered on recommender systems and the trade-off between recommendation accuracy and topic diversity. It is generally accepted that as the recommender system learns to generate high engagement recommendations, topic diversity suffers which in turn may cause filter bubbles and echo chambers to emerge (51, 26, 33, 22, 6, 27, 24, 11, 18, 30, 50, 14). While a variety of mitigation techniques have been proposed - including techniques to identify and remove these effects (37), slow degeneracy (24), improve user heterogeneity (43), and disentangle user interest from user conformity (49) - the question of how to best balance accuracy with diversity, and addressing broader fairness concerns, is still very much an active area of research.

The above papers illustrate the many potential downsides of engagement-driven recommendation and collaborative filtering. To the best of the authors' knowledge, our paper is the first to analyze the impact of consent rates on the dynamics of the recommendation process.

**Privacy, fairness, and bias considerations in recommender systems.** Given that recommender systems are designed to personalize content to users, questions related to privacy, fairness, and bias naturally arise (23, 12, 7). Many notions of recommender system bias have been studied in the literature including _participation inequality_ where system usage varies across a user's attributes (e.g., gender, race, language, etc.), _selection bias_ where the users' choice behavior leads to non-representative item feedback, _conformity bias_ where users in similar groups act similarly, _exposure bias_ where suppliers/items exhibit fundamentally different levels of visibility, and _popularity bias_ where popular items are recommended more often than long tail items, among various others. Furthermore, it is widely recognized that the inherent feedback nature of recommender systems amplifies these biases (6, 30).

Research on the privacy and fairness of recommender systems aims to gain a deeper understanding of how the structure of the recommender system influences these biases and subsequently propose mitigation strategies via modified recommendation algorithms, with a large thread of research concerning the design of privacy-preserving recommender systems (1, 31, 47, 3). Most related to our paper are the structural questions of characterizing the ability of the recommender system to recover protected attributes from ratings of both a homogeneous user pool (42) and a mix of public/private users (44). Notably, (44) demonstrates that only a small number of public users (users willing to share preference information) with a large number of ratings in a pool of private users (users with hidden preferences) is sufficient to produce accurate system-wide recommendations. In contrast, our paper studies how inherent differences in user consent rates influences the system's ability to estimate users' latent attributes across groups.

**Simulators for recommender systems.** Given the complexity of the interactions between users and the learning behavior of the agent, simulators have become an increasingly popular tool for understanding the dynamics of recommender systems. Agent-based modeling allows for simulation of fine-grained user interactions, facilitating useful insights without running costly field tests. A variety of open-source recommender system simulators have emerged in recent years, namely RecoGym (35), RecSim (21), PyRecGym (36), Surprise (20), RecSim NG (32), and others (48), (8). The simulator developed for purposes of this study augments RecSim with the ability for the learning agent to maintain asymmetric (Bayesian) uncertainty over users.

## 4 A Simple Recommendation Model with Cookie Consent

Consider a recommendation environment consisting of a recommender, termed the _agent_, sequentially interacting with a fixed population of \(n\) individuals or _users_ (see Fig. 1). Before the interaction begins, users make consent decisions according to known cohort-dependent probabilities. The agent uses the consent decisions and revealed cookies to form refined (interim) beliefs on the users' cohorts, which in turn guide recommendations. The agent is periodically retrained using the updated history of recommendation-response pairs, with the overall goal of recommending content that maximizes engagement.

**Advertisement model.** Each ad is described by a single _topic_ feature \(\tau\in\mathcal{T}=\{\tau^{1},\ldots,\tau^{m}\}\), where \(m\) is the number of possible ad topics. In each round \(t\), a subset of \(l<m\) ad topics are sampled to form the current set of recommendable ads \(\mathcal{A}_{t}\), termed the _ad pool_.

**User model.** Each user \(i\in[n]\) is described by three features: a _cookie_\(\phi_{i}\in\Phi=\{\phi^{1},\ldots,\phi^{c}\}\), a demographic or _cohort_\(\theta_{i}\in\Theta=\{\theta^{1},\ldots,\theta^{d}\}\), and a vector of _topic affinities_\(\alpha_{i}\in\mathbb{R}_{+}^{m}\). Each user's cookie-cohort pair is drawn from a known joint prior \(\mu\in\Delta(\Phi\times\Theta)\), i.e., knowledge of a user's cookie is at least partially informative for its cohort. Consent decisions are dictated by cohort-dependent probabilities \(q_{\theta}\), \(\theta\in\Theta\), with each user \(i\) in cohort \(\theta\) revealing its cookie with the agent according to \(X_{i}\sim\text{Bernoulli}(q_{\theta})\).2 Topic affinities are generated according to log-normal distributions, with each user \(i\) from a given cohort \(\theta\) possessing affinities \(\alpha_{i}\sim\text{LogNormal}(\rho_{\theta},\sigma_{\theta}^{2})\), where \(\rho_{\theta}\in\mathbb{R}^{m}\) and \(\sigma_{\theta}\in\mathbb{R}_{+}^{m}\) are the cohort-dependent means and standard deviations of \(\alpha_{i}\)'s natural logarithm.

Figure 1: The recommendation process. The advertisement sampler generates the ad pool (assumed to be refreshed in each round \(t\)). The user sampler initializes both latent attributes of users and consent decisions. In each round, the agent generates recommendations (ads) and records user responses (clicks).

In each round \(t\), each user is faced with a single ad and makes a binary decision to either click or not click. Specifically, when user \(i\) is recommended an ad \(a\), the user first scores the ad via the utility function \(u(a;\alpha_{i})=\alpha_{i}^{\top}\ \mathbbm{1}_{\tau_{a}}\) where \(\mathbbm{1}_{\tau_{a}}\in\mathbb{R}^{m}\) is the indicator vector on topics (a vector of zeros with a one in location of the topic of ad \(a\)). User \(i\)'s choice to click on \(a\), denoted by \(c_{i,a}\in\{0,1\}\), is stochastic and is dictated by \(c_{i,a}\sim\text{Bernoulli}(p_{i,a})\), where \(p_{i,a}\) is the click probability given by the following logit model

\[p_{i,a}=\frac{\exp(u(a;\alpha_{i}))}{\exp(u_{0})+\exp(u(a;\alpha_{i}))}\] (1)

where \(u_{0}\) is the _no-click-mass_ used to model the possibility of the user not clicking on \(a\), assumed to be homogenous across users.

**Recommender agent.** The agent faces a fundamental trade-off between recommending high-engagement content and learning more about users' tastes. Our recommendation model combines two foundational models from the literature, namely incorporation of confidence weights [(19, 25)] into an online matrix factorization procedure [(41)]. Online matrix factorization addresses the conflicting objectives of the agent - exploration to improve estimates of latent factors, and exploitation of known high engagement content - by interleaving estimation of the latent factors with specification of recommendations. Including confidence weights allow us to model the agent's heterogeneous uncertainty on users due to their consent decisions and personalized recommendations. Note that while this is a stylized recommender model, it possesses the main features seen in more complex systems, specifically online learning and heterogeneous uncertainty across users.

Formally, the recommendation process evolves as follows. Given the agent's prior belief, \(\mu\in\Delta(\Phi\times\Theta)\), the users' consent decisions are used to form interim cohort beliefs \(\tilde{\mu}_{i}\in\Delta(\Theta)\) for each user \(i\). If user \(i\) gives consent (\(x_{i}=1\)) then the agent is informed of the user's cookie, \(\phi_{i}\), and its updated belief of the user's cohort is formed as \(\tilde{\mu}_{i}(\vartheta)=p(\vartheta\mid x_{i}=1,\phi_{i})=\frac{q_{\theta }\mu(\phi_{i},\vartheta)}{\sum_{\vartheta^{\prime}}q_{\vartheta^{\prime}}\mu( \phi_{i},\vartheta^{\prime})}\) for each \(\vartheta\in\Theta\). If user \(i\) does not provide consent (\(x_{i}=0\)), no cookie information is revealed and the agent's belief of the user's cohort is given by \(\tilde{\mu}_{i}(\vartheta)=p(\vartheta\mid x_{i}=0)=\frac{(1-q_{\theta})_{ \Sigma}\mu(\varphi,\vartheta)}{\sum_{\vartheta^{\prime}}(1-q_{\theta^{\prime} })_{\Sigma}\mu(\varphi^{\prime},\vartheta^{\prime})}\), \(\vartheta\in\Theta\). Interim beliefs \(\tilde{\mu}=(\tilde{\mu}_{1},\ldots,\tilde{\mu}_{n})\) are used to form beliefs \(\mu_{0}=(\mu_{0,1},\ldots,\mu_{0,n})\) via an offline response set \(\mathcal{L}_{0}\) of recommendation-response pairs of the form \(\{(a_{i},c_{i,a})\}\) across users.

The recommender model at round \(t\) is represented by a pair of user-ad latent factor estimates \((\hat{\bm{u}}_{t},\hat{\bm{v}}_{t})\). Recommendations are generated via an \(\varepsilon\)-greedy bandit: with probability \(\varepsilon\), the ad recommended to user \(i\) in round \(t\), denoted by \(a_{i,t}\), is chosen uniformly at random, \(a_{i,t}\sim\text{U}(\mathcal{A}_{t})\), and with probability \(1-\varepsilon\), the recommendation is the ad in the current ad pool with the highest estimated value \(a_{i,t}=\operatorname*{argmax}_{a\in\mathcal{A}_{t}}\hat{u}_{t,t}^{\top}\hat{v }_{a,t}\). Responses to the recommendations are appended to the response set, \(\mathcal{L}_{t+1}=\mathcal{L}_{t}\cup\{(a_{i},c_{i,a})\mid i\in[n],a_{i}\in[m]\}\), and are used to maintain a cumulative count of clicks \(r_{i,a,t}\) across user-ad pairs.

Retraining consists of updating cohort beliefs and latent factor estimates using recommendation-response pairs, and is performed every \(T_{b}\) rounds. If \(t\) is a retraining round, cohort beliefs are first updated according to a Bayesian update \(\mu_{i,t}=f(\mu_{i,t-1},\mathcal{L}_{t}\setminus\mathcal{L}_{t-T_{b}})\) where \(\mathcal{L}_{t}\setminus\mathcal{L}_{t-T_{b}}\) is the set of responses since the previous retraining round. The heterogeneous beliefs on user cohorts gives rise to a weighted procedure. For a given set of cohort beliefs \(\mu_{t}\), the agent computes confidence weights \(\bar{w}_{t}=(\bar{w}_{i,a,t})_{i,a}\) as the expected probability for seeing the current response counts. Specifically, each weight \(\bar{w}_{i,a,t}\) is the expected binomial probability, given \(\mu_{t}\), for seeing \(r_{i,a,t}\) defined as

\[\bar{w}_{i,a,t}=\mathbb{E}_{\vartheta_{i}\sim\mu_{i,t}}[p_{t}(I_{i,a,t},r_{i,a,t },\vartheta_{i})]\]

where \(p_{t}(I,r,\vartheta)\) is the binomial probability at round \(t\) given impressions \(I\), positive response counts \(r\), and cohort \(\vartheta\). Updated latent factor estimates are computed via a confidence-weighted matrix factorization procedure as

\[(\hat{\bm{u}}_{t},\hat{\bm{v}}_{t})=\operatorname*{argmin}_{(u,v)\in\mathcal{U} \times\mathcal{V}}\bigg{[}\sum_{(a_{i},c_{i,a})\in\mathcal{L}_{t}}\!\!\!\bar{w} _{i,a,t}(u_{i}^{\top}v_{a}-r_{i,a,t})^{2}+\lambda\bigg{(}\sum_{i\in[n]}\|u_{i }\|_{2}+\sum_{a\in[m]}\|v_{a}\|_{2}\bigg{)}\bigg{]}\] (2)

where \(\mathcal{U}=\{(u_{1},\ldots,u_{n})\mid u_{i}\in\mathbb{R}^{k},i\in[n]\}\), \(\mathcal{V}=\{(v_{1},\ldots,v_{m})\mid v_{a}\in\mathbb{R}^{k},a\in[m]\}\) are the latent factor spaces, \(\mathcal{L}_{t}\) is the current history of responses, and \(\lambda>0\) is a regularization weight. If \(t\) is not a retraining round, then cohort beliefs and latent factor estimates are propagated as is from the previous round. Expressions for the Bayesian updates and expected binomial probabilities can be found in Appendix A, with pseudocode of the recommendation process in Appendix B.

Experiments

The following experiments study the dynamics of the recommendation process described in Section 4 particularly as it relates to users' consent rates. Empirical results were obtained via a simulator based on RecSim (21). Full details of the simulator and the experimental setup can be found in the supplementary material (Appendix C). Base model parameters assumed throughout this section are: number of users \(n=1000\), number of ads \(m=200\), ad pool size \(l=50\), and number of cohorts \(d=2\). Sensitivity analyses can be found in Appendix D.

The following notation/terminology will be used throughout this section to aid explanations. Let \(N_{1}^{\vartheta}=\{i\in[n]\mid x_{i}=1,\theta_{i}=\vartheta\}\) denote the set of users in cohort \(\vartheta\) who have decided to share their cookie, with \(N_{0}^{\vartheta}\) defined analogously. We refer to \(N_{1}^{\vartheta}\) as the _consent group_ (in cohort \(\vartheta\)) and \(N_{0}^{\vartheta}\) as the _non-consent group_. Estimation errors on cohorts for each group are quantified by log loss, i.e., for a set of users \(N\) in group \((\vartheta,x)\), the (average) log loss is denoted by \(L_{x}^{\vartheta}(\mu)=-\frac{1}{|N|}\sum_{i\in N}\sum_{\vartheta\in\Theta} \mathbf{1}(\theta_{i}=\vartheta)\log(\mu_{i}(\vartheta))\). Estimation errors on topic affinities are given by MSE.

### Impact of consent on estimation accuracy

The following set of experiments investigates the impact of cohort-dependent consent rates on the agent's estimates of the users' cohorts and topic affinities. Given users' consent decisions, and cookies for those who decided to provide consent, the agent forms interim beliefs on each user, \(\tilde{\mu}_{i}\), \(i\in[n]\), as per the model description in Section 4. Fig. 2 illustrates the impact of these consent decisions on cohort and affinity estimation errors, for \(\Theta=\{\theta,\theta^{\prime}\}\), in two cases: homogeneous consent rates (Fig. 2(a)) and heterogeneous consent rates (Fig. 2(b)).

Fig. 2(a) illustrates that when users' consent rates are identical across cohorts, estimation errors for the non-consent group \(N_{0}^{\theta}\) are higher than the errors for the consent group \(N_{1}^{\theta}\) regardless of their cohort \(\theta\).

As seen in Fig. 2(b), this intuitive result breaks down when users' consent rates differ across cohorts. While withholding consent leads to a higher estimation error for the higher consent rate group, \(\theta^{\prime}\), withholding consent can lead to a _lower_ estimation error for the lower consent rate group, \(\theta\). This inversion effect persists for the lower consent rate population for any sufficiently large separation of consent rates across cohorts, as illustrated by Fig. 3. These cohort-dependent effects of consent are summarized by Observation 1.

**Observation 1** (Comparison within cohorts).: _Let \(\Theta=\{\theta,\theta^{\prime}\}\) and let \(\mu\) be any non-fully informative prior. If \(q_{\theta}=q_{\theta^{\prime}}\) (consent rates are independent of cohort) then non-consent leads to a higher estimation error than consent. If \(q_{\theta}<q_{\theta^{\prime}}\) (consent rates are cohort-dependent with users in cohort \(\theta\) providing consent as a lower rate than users in cohort \(\theta^{\prime}\)), then:_

1. _There exists a (prior-dependent) constant_ \(\delta_{\mu}\) _such that for_ \(q_{\theta^{\prime}}-q_{\theta}>\delta_{\mu}\)_, the non-consent group in cohort_ \(\theta\)_,_ \(N_{0}^{\theta}\)_, experiences lower estimation errors than the consent group,_ \(N_{1}^{\theta}\)_._
2. _The non-consent group in cohort_ \(\theta^{\prime}\)_,_ \(N_{0}^{\theta^{\prime}}\)_, experiences higher estimation errors than the consent group,_ \(N_{1}^{\theta^{\prime}}\)_._

Figure 2: Estimation errors over training rounds \(t\) (\(T_{b}=1\)) as a function of consent rates under binary cookie space \(\Phi=\{\phi,\phi^{\prime}\}\) and symmetric prior \(\mu(\phi,\theta)=\mu(\phi^{\prime},\theta^{\prime})=0.4\).

Observation 1 holds for any non-fully informative prior, meaning that there exists at least one cookie that does not reveal the user's cohort with certainty - a property that is almost certainly satisfied in practical (i.e., noisy) recommender systems.

For partially informative priors (each cookie only partially reveals the user's cohort), the statement of the above observation can be understood by looking at the agent's sources of information. Consider the following signals: i) the cookie value in the event that the user provided consent and, ii) under heterogeneous consent rates, the consent decision itself. The interplay between these two sources of information means that, under some priors, the act of a user withholding consent can itself be more informative than providing consent and revealing their cookie. Fig. 3 illustrates the estimation errors for different consent rate regimes under symmetric prior \(\mu(\theta,\phi)=\mu(\theta^{\prime},\phi^{\prime})=0.45\).

The above figure, specifically Fig. 3(a), illustrates that under a very informative prior, a small gap in consent rates, \((q_{\theta},q_{\theta^{\prime}})=(0.2,0.4)\), yields a consent signal that contains relatively low informativeness compared to the revealed cookie value, causing consent to yield lower estimation errors for both cohorts. However, as the absolute consent rates increase in both cohorts, this same consent rate gap (compare Figs. 3(a) and (c)) yields a consent signal that is sufficiently informative to yield lower estimation errors for withholding consent for the lower consent rate group.

Generally the larger the gap in consent rates, the more information the consent signal carries for the user's cohort. Apart from very informative priors, the consent signal carries sufficiently rich information to yield lower estimation errors for withholding consent for the lower consent rate population even for small gaps in the consent rates.

Observation 1 compares the impact of consent decisions within a given cohort under homogeneous and heterogeneous consent rates. A comparison of the impact of the same consent decision across users in different cohorts is summarized by the following observation.

**Observation 2** (Comparison across cohorts).: _Let \(\Theta=\{\theta,\theta^{\prime}\}\) and let \(\mu\) be any partially informative prior. If \(q_{\theta}=q_{\theta}^{\prime}\), then the consent group experiences the same estimation errors regardless of cohort (similarly for non-consent). If \(q_{\theta}<q_{\theta^{\prime}}\), then:_

1. _There exists a (prior-dependent) constant_ \(\eta_{\mu}\) _such that for_ \(q_{\theta^{\prime}}-q_{\theta}>\eta_{\mu}\)_, the non-consent group in cohort_ \(\theta^{\prime}\)_,_ \(N_{0}^{\theta^{\prime}}\)_, experiences higher estimation errors than the non-consent group in cohort_ \(\theta\)_,_ \(N_{0}^{\theta}\)_._
2. _The consent group in cohort_ \(\theta^{\prime}\)_,_ \(N_{1}^{\theta^{\prime}}\)_, experiences lower estimation errors than the consent group in cohort_ \(\theta\)_,_ \(N_{1}^{\theta}\)_._

### Amplification effects

The following experiments investigate how the disparities observed in (O.1) and (O.2) are influenced by the relative consent rates across cohorts. For \(\Theta=\{\theta,\theta^{\prime}\}\), define \(\delta_{t}^{\theta}=\bar{L}_{0}^{\vartheta}(\mu_{t})-\bar{L}_{1}^{\vartheta}( \mu_{t})\) as the difference in mean cohort errors between non-consent and consent for users in cohort \(\vartheta\in\Theta\). Additionally, define \(\gamma_{t}^{x}=\bar{L}_{x}^{\theta^{\prime}}(\mu_{t})-\bar{L}_{x}^{\theta}( \mu_{t})\) as the difference in mean cohort errors for consent decision \(x\).

Figure 3: Impact of consent versus non-consent on the agentâ€™s cohort estimation error in each cohort for various consent rate regimes under symmetric prior \(\mu(\phi,\theta)=\mu(\phi^{\prime},\theta^{\prime})=0.45\).

In words, the quantities \(\delta^{\vartheta}_{t}\), \(\vartheta\in\Theta\), represent how much worse the agent's estimation error is under non-consent versus consent for users in cohort \(\vartheta\). The quantity \(\gamma^{0}_{t}\) (resp. \(\gamma^{1}_{t}\)) represents how much higher the agent's estimation error is for a user who withheld (resp. provided) consent if they belonged to cohort \(\theta^{\prime}\) versus if they belonged to cohort \(\theta\). Figs. 4 and 5 illustrate how these quantities differ as a function of consent rates for a given number of training rounds (\(\tau=10\)).

Fig. 4 illustrates that the relative loss in accuracy for non-consent (compared to consent) is greatest in cohort \(\theta\) when \(q_{\theta}\) is minimal and \(q_{\theta^{\prime}}\) is maximal, with the opposite effects observed in cohort \(\theta^{\prime}\). Fig. 5 illustrates that the users who withhold consent are on average impacted greater by a given gap in consent rates than users who consent, e.g., compare the magnitude of the values at \((q_{\theta},q_{\theta^{\prime}})=(0.25,0.75)\) for non-consent versus consent.

In summary, the gap in consent rates serves as an amplifier for an individual user's consent decision. Users from the higher consent rate cohort who withhold consent experience higher estimation errors than the same users from the lower consent rate cohort, with the opposite holding for users in the consent group.

Figure 4: Relative errors for non-consent versus consent as a function of consent rates \((q_{\theta},q_{\theta^{\prime}})\) for each cohort and fixed number of training rounds \(\tau=10\).

Figure 5: Relative errors for a given consent decision across cohorts as a function of consent rates \((q_{\theta},q_{\theta^{\prime}})\) for fixed number of training rounds \(\tau=10\).

Discussion

Our results demonstrate that even in a simple recommendation model with reasonable modeling assumptions, the impact of a user's cookie consent decision is not necessarily straightforward. Particularly, when consent rates exhibit demographic-dependence, the user's consent decision itself acts as a signal, carrying information beyond the information contained in the actual cookie value. This leads to a demographic-dependent impact of consent, resulting in a user's privacy-protective decision (of disagreeing to share their cookie) potentially causing the system to know more about them than if the user were to intentionally reveal their cookie information.

To draw a comparison with the large body of existing literature in the fairness of recommender systems, many of the existing papers (see the papers referenced in Section 3) draw attention to the way in which the system learns similarities among users over the recommendation process as a primary source of unfairness. Indeed, this is fundamental to how matrix factorization algorithms (and many more complex recommender systems) operate: users who behave similarly, i.e., how they respond to the same recommendations, will be deemed as being similar (even falsely so). This will lead to these users seeing similar recommendations which can, in turn, lead to undesirable outcomes.

However, one of the goals of our paper is to draw attention to the informational effects of consent as a potential additional source of unfairness. When consent is a conscious decision of the user, and the rate of a given consent decision differs based on sensitive attributes of the user, e.g., age, then the system can use the consent decision to learn more about the user's sensitive attributes. When embedded within an algorithmic system, platforms may unintentionally profile users based on the absence of data.

These observations have possible implications in the design of recommender systems. Any information that the system can use to draw similarities will be used, most notably how users respond to recommendations, but also, and the focus of our paper, the inherent likelihood of a user to provide consent to cookie sharing. These similarities are used to infer to which group a user belongs and subsequently drive recommendations. The implications for algorithm design is that one must be mindful of the complexities of information revelation especially as it relates to how the algorithm will use the revealed information.

**New notions of fairness.** The design of recommender systems that are more aware of the informational effects of users' decisions may require new notions of fairness. It is well-known in the fairness community that static fairness metrics are insufficient for dynamic settings, and can actually actively harm the group they intend to protect [(28)].

In our setting, whether the system possessing heterogeneous accuracy about users results in a fair or unfair outcome depends on the product being recommended. Given our observations that the specific impact of a user's consent decision varies depending on the user's demographic, it is not hard to see that such accuracy disparity may lead to unfair situations. For example, consider an ad recommendation setting where some of the ads may be predatory to older individuals. According to our model, an older individual's desire to remain more private by deciding to disagree to cookie sharing may allow the system to know more about them, simply by virtue that more older individuals choose to not share their cookies, and subsequently make them a more likely target for such ads. Consequently, we emphasize the need for development of fairness metrics that ensure consistency of an individual user's privacy-protective decision and the amount (or accuracy) of information that the system knows about that user.

It is worth noting that this informational consistency property is distinct from the literature on privacy-preserving recommender systems. For instance, differentially-private recommender systems [(31, 29)] ensure individual-anonymity, via careful injection of noise, while still enabling extraction of aggregate-level information used to feed the recommendation algorithm. In contrast, a consistency-based approach would hypothetically still allow for some individuals to be (more accurately) identified if they choose to be (e.g., to receive more targeted recommendations) while honoring the privacy of individuals who chose to not share their information.

**Towards industry reform.** Our results contribute to the growing sentiment that the digital advertising space is in need of reform, particularly due to the unintended consequences of "the collection of personal data, tracking, and massive-scale profiling" [(2)]. As the move away from cookie-based tracking gains momentum, alternative mechanisms for user profiling are becoming more prevalent.

In the context of an advertising environment without cookies (often referred to as the _cookieless_ future), the shift to these alternative data collection methods may lead to informational disparities across companies. Given that the current advertising ecosystem contains players of vastly different sizes, ending the use of cookies may give asymmetric power to companies that have already collected significant amounts of personal data. Indeed, tools that facilitate access to user information that would otherwise be tracked by cookies are already in use, e.g., websites that allow users to create an account by using their Google/Microsoft credentials (a practice known as "auto-linking").

In general, these issues point to the fact that controls and regulations should be based on the downstream effects of user interactions with the advertising system. In particular, policy must carefully consider how much information has been extracted from a given user, whether it be from individual decisions/responses of the user or gleaned from inferred similarities with other users, and its privacy-compromising effects. To foster transparency, it would be beneficial for the industry to derive estimates of the monetary value of user information and share them with the public. In general, the advertising industry can be more transparent, not just about user data but also concerning the downstream effects on users, since real-world advertising engines are vastly more complex than our simplified setting and may not lend themselves to simple mathematical modeling.

**Limitations.** Our findings on the disparate impact of consent should be interpreted with the understanding that our model necessarily has some limitations. Broadly, our model uses a simplified definition of cookies in which cookies serves as a proxy for the users' cohorts. While the reason for this simplicity is to extract insights that depend directly on the user's cookie consent decisions, extending the definition of a cookie to more realistic settings by including user click/behavioral information would likely generate additional insights. Secondly, to capture the core aspects of recommender systems, our recommendation model is based on foundational recommendation algorithms (namely online + confidence-weighted matrix factorization). While these algorithms form the basis for modern recommender systems, it would be worthwhile to see how the insights extend to more modern algorithms. Lastly, we consider simplified ad and user pools; consideration of additional ad features and a more realistic dynamic user pool could influence the findings.

## 7 Concluding remarks and future directions

We've investigated the question of how heterogeneous cookie consent rates among users influence the recommender agent's ability to learn users' latent features (e.g., their demographics and tastes). We've empirically discovered, through construction of a simulator, that disparities in the agent's estimation accuracy across users emerge when consent rates exhibit demographic-dependence. Our observations show that seemingly simple informational decisions by users (i.e., whether to share their cookie) can have complex effects on the agent's information. Consideration of informational effects of users' decisions is crucial in the design of recommender systems. We encourage the development of new fairness metrics for dynamic settings that enforce consistency between a user's privacy decision and the amount of information that the system knows about them.

Future work focuses on studying the downstream effects of our observations via validation in real-world recommender systems. An additional direction is quantification of the aforementioned fairness consistency property such that it can be embedded as a constraint in the design of recommender system algorithms.

**Acknowledgments.** This research was funded in part by the Horizon Europe project AutoFair (grant agreement ID: #101070568).

## References

* [1] Esma Aimeur, Gilles Brassard, Jose M Fernandez, and Flavien Serge Mani Onana. Alambic: a privacy-preserving recommender system for electronic commerce. _International Journal of Information Security_, 7(5):307-334, 2008.
* [2] C Armitage, N Botton, L Dejeu-Castang, and L Lemoine. _Study on the impact of recent developments in digital advertising on privacy, publishers and advertisers : final report_. European Commission and Directorate-General for Communications Networks, Content and Technology (Publications Office of the European Union), 2023. doi: doi/10.2759/294673.
* [3] Shahriar Badsha, Xun Yi, and Ibrahim Khalil. A practical privacy-preserving recommender system. _Data Science and Engineering_, 1:161-177, 2016.
* [4] Erion Cano and Maurizio Morisio. Hybrid recommender systems: A systematic literature review. _Intelligent Data Analysis_, 21(6):1487-1524, 2017.
* [5] CCPA. AB-375 privacy (personal information, businesses): California consumer privacy act. https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180AB375. Accessed: 2023-03-31.
* [6] Allison JB Chaney, Brandon M Stewart, and Barbara E Engelhardt. How algorithmic confounding in recommendation systems increases homogeneity and decreases utility. In _Proceedings of the 12th ACM conference on recommender systems_, pages 224-232, 2018.
* [7] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. Bias and debias in recommender system: A survey and future directions. _arXiv preprint arXiv:2010.03240_, 2020.
* [8] Michael D Ekstrand, Allison Chaney, Pablo Castells, Robin Burke, David Rohde, and Manel Slokom. Simurec: Workshop on synthetic data and simulation methods for recommender systems research. In _Proceedings of the 15th ACM Conference on Recommender Systems_, pages 803-805, 2021.
* [9] Steven Englehardt, Dillon Reisman, Christian Eubank, Peter Zimmerman, Jonathan Mayer, Arvind Narayanan, and Edward W Felten. Cookies that give you away: The surveillance implications of web tracking. In _Proceedings of the 24th International Conference on World Wide Web_, pages 289-299, 2015.
* [10] ePD. Directive on privacy and electronic communications. https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32002L0058:en:HTML. Accessed: 2023-03-20.
* [11] Andres Ferraro, Dietmar Jannach, and Xavier Serra. Exploring longitudinal effects of session-based recommendations. In _Proceedings of the 14th ACM Conference on Recommender Systems_, pages 474-479, 2020.
* [12] Arik Friedman, Bart P Knijnenburg, Kris Vanhecke, Luc Martens, and Shlomo Berkovsky. Privacy aspects of recommender systems. _Recommender Systems Handbook_, pages 649-688, 2015.
* [13] GDPR. Regulation (EU) 2016/679 of the European Parliament and of the Council. https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A02016R0679-20160504. Accessed: 2023-03-20.
* [14] Nada Ghanem, Stephan Leitner, and Dietmar Jannach. Balancing consumer and business value of recommender systems: A simulation-based analysis. _Electronic Commerce Research and Applications_, 55:101195, 2022.
* [15] Paul Grassl, Hanna Schraffenberger, Frederik Zuiderveen Borgesius, and Moniek Buijzen. Dark and bright patterns in cookie consent requests. _Journal of Digital Social Research_, 3(1):1-38, 2021.

* Gray et al. [2021] Colin M Gray, Cristiana Santos, Natalia Bielova, Michael Toth, and Damian Clifford. Dark patterns and the legal requirements of consent banners: An interaction criticism perspective. In _Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems_, pages 1-18, 2021.
* Habib et al. [2022] Hana Habib, Megan Li, Ellie Young, and Lorrie Cranor. "Okay, whatever": An evaluation of cookie consent interfaces. In _Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems_, pages 1-27, 2022.
* Holtz et al. [2020] David Holtz, Ben Carterette, Praveen Chandar, Zahra Nazari, Henriette Cramer, and Sinan Aral. The engagement-diversity connection: Evidence from a field experiment on spotify. In _Proceedings of the 21st ACM Conference on Economics and Computation_, pages 75-76, 2020.
* Hu et al. [2008] Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback datasets. In _2008 Eighth IEEE International Conference on Data Mining_, pages 263-272. IEEE, 2008.
* Hug [2020] Nicolas Hug. Surprise: A python library for recommender systems. _Journal of Open Source Software_, 5(52):2174, 2020.
* le et al. [2019] Eugene le, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. Recsim: A configurable simulation platform for recommender systems. _arXiv preprint arXiv:1909.04847_, 2019.
* Jannach et al. [2015] Dietmar Jannach, Lukas Lerche, Iman Kamehkhosh, and Michael Jugovac. What recommenders recommend: an analysis of recommendation biases and possible countermeasures. _User Modeling and User-Adapted Interaction_, 25:427-491, 2015.
* Jeckmans et al. [2013] Arjan JP Jeckmans, Michael Beye, Zekeriya Erkin, Pieter Hartel, Reginald L Lagendijk, and Qiang Tang. Privacy in recommender systems. _Social Media Retrieval_, pages 263-281, 2013.
* Jiang et al. [2019] Ray Jiang, Silvia Chiappa, Tor Lattimore, Andras Gyorgy, and Pushmeet Kohli. Degenerate feedback loops in recommender systems. In _Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society_, pages 383-390, 2019.
* Koren et al. [2009] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. _Computer_, 42(8):30-37, 2009.
* Lathia et al. [2010] Neal Lathia, Stephen Hailes, Licia Capra, and Xavier Amatriain. Temporal diversity in recommender systems. In _Proceedings of the 33rd international ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 210-217, 2010.
* Lee and Hosanagar [2019] Dokyun Lee and Kartik Hosanagar. How do recommender systems affect sales diversity? a cross-category investigation via randomized field experiment. _Information Systems Research_, 30(1):239-259, 2019.
* Liu et al. [2018] Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In _International Conference on Machine Learning_, pages 3150-3158. PMLR, 2018.
* Liu et al. [2015] Ziqi Liu, Yu-Xiang Wang, and Alexander Smola. Fast differentially private matrix factorization. In _Proceedings of the 9th ACM Conference on Recommender Systems_, pages 171-178, 2015.
* Mansoury et al. [2020] Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin Burke. Feedback loop and bias amplification in recommender systems. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 2145-2148, 2020.
* McSherry and Mironov [2009] Frank McSherry and Ilya Mironov. Differentially private recommender systems: Building privacy into the Netflix prize contenders. In _Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 627-636, 2009.
* Mladenov et al. [2021] Martin Mladenov, Chih-Wei Hsu, Vihan Jain, Eugene le, Christopher Colby, Nicolas Mayoraz, Hubert Pham, Dustin Tran, Ivan Vendrov, and Craig Boutilier. Recsim NG: Toward principled uncertainty modeling for recommender ecosystems. _arXiv preprint arXiv:2103.08057_, 2021.

* [33] Tien T Nguyen, Pik-Mai Hui, F Maxwell Harper, Loren Terveen, and Joseph A Konstan. Exploring the filter bubble: the effect of using recommender systems on content diversity. In _Proceedings of the 23rd International Conference on World Wide Web_, pages 677-686, 2014.
* [34] Midas Nouwens, Ilaria Liccardi, Michael Veale, David Karger, and Lalana Kagal. Dark patterns after the gdpr: Scraping consent pop-ups and demonstrating their influence. In _Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems_, pages 1-13, 2020.
* [35] David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile, and Alexandros Karatzoglou. Recogym: A reinforcement learning environment for the problem of product recommendation in online advertising. _arXiv preprint arXiv:1808.00720_, 2018.
* [36] Bichen Shi, Makbule Gulcin Ozsoy, Neil Hurley, Barry Smyth, Elias Z Trigos, James Geraci, and Aonghus Lawlor. Pyrecgym: A reinforcement learning gym for recommender systems. In _Proceedings of the 13th ACM Conference on Recommender Systems_, pages 491-495, 2019.
* [37] Ayan Sinha, David F Gleich, and Karthik Ramani. Deconvolving feedback loops in recommender systems. _Advances in Neural Information Processing Systems_, 29, 2016.
* [38] Than Htut Soe, Oda Elise Nordberg, Frode Guribye, and Marija Slavkovik. Circumvention by design-dark patterns in cookie consent for online news outlets. In _Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society_, pages 1-12, 2020.
* [39] Michael Carl Tschantz, Serge Egelman, Jaeyoung Choi, Nicholas Weaver, and Gerald Friedland. The accuracy of the demographic inferences shown on google's ad settings. In _Proceedings of the 2018 Workshop on Privacy in the Electronic Society_, pages 33-41, 2018.
* [40] VCDPA. Virginia acts of assembly: Virginia consumer data protection act. https://lis.virginia.gov/cgi-bin/legp604.exe?212+ful+HB2307ER. Accessed: 2023-04-01.
* [41] Huazheng Wang, Qingyun Wu, and Hongning Wang. Factorization bandits for interactive recommendation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.
* [42] Udi Weinsberg, Smriti Bhagat, Stratis Ioannidis, and Nina Taft. Blurme: Inferring and obfuscating user gender based on ratings. In _Proceedings of the Sixth ACM Conference on Recommender Systems_, pages 195-202, 2012.
* [43] Ruobing Xie, Qi Liu, Shukai Liu, Ziwei Zhang, Peng Cui, Bo Zhang, and Leyu Lin. Improving accuracy and diversity in matching of recommendation with diversified preference network. _IEEE Transactions on Big Data_, 8(4):955-967, 2021.
* [44] Yu Xin and Tommi Jaakkola. Controlling privacy in recommender systems. _Advances in Neural Information Processing Systems_, 27, 2014.
* [45] YouGov, 2021. Global: How consumers respond to cookies disclosures. https://business.yougov.com/content/37531-global-cookies-disclosures-behavior-survey. Accessed: 2023-03-15.
* [46] YouGov, 2022. Global: How many consumers trust tech brands with their personal data? https://business.yougov.com/content/42530-global-how-many-consumers-trust-tech-brands-their-. Accessed: 2023-03-15.
* [47] Justin Zhan, Chia-Lung Hsieh, I-Cheng Wang, Tsan-Sheng Hsu, Churn-Jung Liau, and Da-Wei Wang. Privacy-preserving collaborative recommender systems. _IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)_, 40(4):472-476, 2010.
* [48] Jingjing Zhang, Gediminas Adomavicius, Alok Gupta, and Wolfgang Ketter. Consumption and performance: Understanding longitudinal dynamics of recommender systems via an agent-based simulation framework. _Information Systems Research_, 31(1):76-101, 2020.

* [49] Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. Disentangling user interest and conformity for recommendation with causal embedding. In _Proceedings of the Web Conference 2021_, pages 2980-2991, 2021.
* [50] Meizi Zhou, Jingjing Zhang, and Gediminas Adomavicius. Longitudinal impact of preference biases on recommender systems' performance. _Kelley School of Business Research Paper_, (2021-10), 2021.
* [51] Tao Zhou, Zoltan Kuscsik, Jian-Guo Liu, Matus Medo, Joseph Rushton Wakeling, and Yi-Cheng Zhang. Solving the apparent diversity-accuracy dilemma of recommender systems. _Proceedings of the National Academy of Sciences_, 107(10):4511-4515, 2010.

Omitted expressions

**Derivation of interim belief update**. Given user consent decisions, the agent updates the prior \(\mu\) to form interim beliefs as follows. For a user \(i\) who provided consent (\(x_{i}=1\)), with its revealed cookie \(\phi_{i}=\varphi\), the interim belief \(\tilde{\mu}_{i,0}\in\Delta(\Theta)\) is given elementwise by

\[\tilde{\mu}_{i,0}(\vartheta) =\mathbb{P}(\theta_{i}=\vartheta\mid x_{i}=1,\phi_{i}=\varphi)\] \[=\frac{\mathbb{P}(x_{i}=1\mid\phi_{i}=\varphi,\theta_{i}= \vartheta)\mathbb{P}(\phi_{i}=\varphi,\theta_{i}=\vartheta)}{\mathbb{P}(x_{i }=1,\phi_{i}=\varphi)}\] \[=\frac{P(x_{i}=1\mid\theta_{i}=\vartheta)p(\phi_{i}=\varphi, \theta_{i}=\vartheta)}{\sum_{\vartheta^{\prime}}\mathbb{P}(x_{i}=1\mid\theta _{i}=\vartheta^{\prime})\mathbb{P}(\phi_{i}=\varphi,\theta_{i}=\vartheta^{ \prime})}\] \[=\frac{q_{\vartheta}\mu(\varphi,\vartheta)}{\sum_{\vartheta^{ \prime}}q_{\vartheta^{\prime}}\mu(\varphi,\vartheta^{\prime})}.\]

Similarly, for a user \(i\) who did not provide consent (\(x_{i}=0\)), its cookie is not revealed and thus interim beliefs are given by

\[\tilde{\mu}_{i,0}(\vartheta) =\mathbb{P}(\theta_{i}=\vartheta\mid x_{i}=0)\] \[=\frac{\mathbb{P}(x_{i}=0\mid\theta_{i}=\vartheta)\mathbb{P}( \theta_{i}=\vartheta)}{\mathbb{P}(x_{i}=0)}\] \[=\frac{\mathbb{P}(x_{i}=0\mid\theta_{i}=\vartheta)\sum_{\varphi} \mathbb{P}(\phi_{i}=\varphi,\theta_{i}=\vartheta)}{\sum_{\vartheta^{\prime}} \mathbb{P}(x_{i}=0\mid\theta_{i}=\vartheta^{\prime})\sum_{\varphi^{\prime}} \mathbb{P}(\phi_{i}=\varphi^{\prime},\theta_{i}=\vartheta^{\prime})}\] \[=\frac{(1-q_{\vartheta})\sum_{\varphi}\mu(\varphi,\vartheta)}{ \sum_{\vartheta^{\prime}}(1-q_{\vartheta^{\prime}})\sum_{\varphi^{\prime}} \mu(\varphi^{\prime},\vartheta^{\prime})}.\]

**Cohort belief update.** Given a set of cohort beliefs across users \(\mu=(\mu_{1},\ldots,\mu_{n})\) and a set of recommendation-response pairs \(\mathcal{L}\), the agent forms updated beliefs \(\mu^{\prime}=(\mu^{\prime}_{1},\ldots,\mu^{\prime}_{n})\) via a Bayesian update, \(\mu^{\prime}=f(\mu,\mathcal{L})\). This update is carried out independently for each user \(i\). Let \(\mu_{i}\) denote the agent's current belief on user \(i\)' cohort \(\theta_{i}\), and denote \(\mathcal{L}_{i}=\{(a_{i},c_{i,a})\}\) as the pairs that correspond to user \(i\). The updated belief \(\mu^{\prime}_{i}\) is given by \(\mu^{\prime}_{i}=(\mu^{\prime}_{i}(\theta^{1}),\ldots,\mu^{\prime}_{i}(\theta ^{d}))=f_{i}(\mu_{i},\mathcal{L}_{i})\) where each \(\mu^{\prime}_{i}(\vartheta)\) is given by

\[\mu^{\prime}_{i}(\vartheta) =\mathbb{P}(\theta_{i}=\vartheta\mid\mathcal{L}_{i})\] \[=\frac{\prod_{(a_{i},c_{i,a})\in\mathcal{L}_{i}}\mathbb{P}(C_{i,a }=c_{i,a}\mid A_{i}=a_{i},\theta_{i}=\vartheta)\mathbb{P}(\theta_{i}= \vartheta)}{\sum_{\vartheta^{\prime}}\prod_{(a_{i},c_{i,a})\in\mathcal{L}_{i}} \mathbb{P}(C_{i,a}=c_{i,a}\mid A_{i}=a_{i},\theta_{i}=\vartheta^{\prime}) \mathbb{P}(\theta_{i}=\vartheta^{\prime})}\] \[=\frac{\prod_{(a_{i},c_{i,a})\in\mathcal{L}_{i}^{1}}\bar{p}_{i,a_ {i}}(\vartheta)\prod_{(a_{i},c_{i,a})\in\mathcal{L}_{i}^{0}}(1-\bar{p}_{i,a_{ i}}(\vartheta))\mu_{i}(\vartheta)}{\sum_{\vartheta^{\prime}}\prod_{(a_{i},c_{i,a})\in \mathcal{L}_{i}^{1}}\bar{p}_{i,a_{i}}(\vartheta^{\prime})\prod_{(a_{i},c_{i,a}) \in\mathcal{L}_{i}^{0}}(1-\bar{p}_{i,a_{i}}(\vartheta^{\prime}))\mu_{i}( \vartheta^{\prime})}\]

where \(\mathcal{L}_{i}^{1}\) (resp. \(\mathcal{L}_{i}^{0}\)) are the responses where the user clicked (resp. did not click) on the recommendation and \(\bar{p}_{i,a}(\vartheta)\) is the expected click probability given \(\vartheta\) defined as

\[\bar{p}_{i,a}(\vartheta)=\mathbb{E}_{\alpha_{i}\sim\text{LogNormal}(\rho_{ \vartheta},\sigma_{\vartheta}^{2})}[p_{i,a}(\alpha_{i})]\]

where the click probability \(p_{i,a}\) from (1) has been written as \(p_{i,a}(\alpha_{i})\) to make the dependence on user \(i\)'s topic affinities explicit.

**Confidence weights.** The confidence weights \(\bar{w}_{i,a,t}\) in (2) are computed as the expected binomial probability for seeing the current response counts \(r_{i,a,t}\) given the agent's current belief on user \(i\)'s cohort \(\vartheta_{i}\), i.e., \(\bar{w}_{i,a,t}=\mathbb{E}_{\vartheta_{i}\sim\mu_{i,t}}[p_{t}(I_{i,a,t},r_{i,a,t}, \vartheta_{i})]\). The binomial probability \(p_{t}(I_{i,a,t},r_{i,a,t},\vartheta_{i})\) is defined as

\[p_{t}(I_{i,a,t},r_{i,a,t},\vartheta_{i})=\begin{pmatrix}I_{i,a,t}\\ r_{i,a,t}\end{pmatrix}\bar{p}_{i,a}(\vartheta_{i})^{r_{i,a,t}}(1-\bar{p}_{i,a}( \vartheta_{i}))^{I_{i,a,t}-r_{i,a,t}}\]

where \(I_{i,a,t}\) is the current impression count for user-ad pair \((i,a)\), \(r_{i,a,t}\) is the current click click for \((i,a)\), and \(\bar{p}_{i,a}(\vartheta_{i})=\mathbb{E}_{\alpha_{i}\sim\text{LogNormal}(\rho_{ \vartheta_{i}},\sigma_{\vartheta_{i}}^{2})}[p_{i,a}(\alpha_{i})]\) is the expected click probability.

Recommendation procedure

_Input parameters. \((n,l,m,\Theta,\Phi,\mathcal{T},\{\rho_{\theta},\sigma_{\theta}\},u_{0},\{q_{\theta} \},\mu,k,\lambda,\varepsilon,\mathcal{L}_{0},T_{b},T)\) Initialize users._ For each user \(i\in[n]\), sample:

* cookie-cohort pairs \((\phi_{i},\theta_{i})\sim\mu\)
* topic affinities \(\alpha_{i}\sim\text{LogNormal}(\rho_{\theta_{i}},\sigma_{\theta_{i}}^{2})\)
* consent decision \(x_{i}\sim\text{Bernoulli}(q_{\theta_{i}})\)

_Form interim beliefs._ For each user \(i\in[n]\):

``` if\(x_{i}=1\)then// user provided consent \(\left|\ \tilde{\mu}_{i}(\vartheta)\right.\leftarrow\frac{q_{\theta}\mu(\phi_{i}, \vartheta)}{\sum_{\vartheta^{\prime}}q_{\vartheta}^{\prime}\mu(\phi_{i}, \vartheta^{\prime})},\;\vartheta\in\Theta\) else// user withheld consent \(\left|\ \tilde{\mu}_{i}(\vartheta)\right.\leftarrow\frac{(1-q_{\theta})\sum_{ \varphi}\mu(\varphi,\vartheta)}{\sum_{\vartheta^{\prime}}(1-q_{\vartheta^{ \prime}})\sum_{\varphi^{\prime}}\mu(\varphi^{\prime},\vartheta^{\prime})},\; \vartheta\in\Theta\) end  Offline responses.  Form priors using \(\mathcal{L}_{0}\): \(\mu_{i,0}\gets f(\tilde{\mu}_{i},\mathcal{L}_{0})\), \(i\in[n]\) Online recommendations. for\(t=1,\ldots,T\)do  Define current ad pool \(\mathcal{A}_{t}\) by sampling \(l\) items uniformly without replacement from \(\mathcal{T}\) if\(\mod(t,T_{b})=0\)then// retraining step  Update cohort beliefs: \(\mu_{i,t}\gets f(\mu_{i,t-1},\mathcal{L}_{t}\setminus\mathcal{L}_{t-T_{b}})\), \(i\in[n]\)  Compute weights: \(\bar{w}_{i,a,t}\leftarrow\mathbb{E}_{\vartheta_{i}\sim\mu_{i,t}}[p_{t}(r_{i,a,t},\vartheta_{i})]\), \(i\in[n]\)  Update factor estimates: \((\hat{\bm{u}}_{t},\hat{\bm{v}}_{t})\leftarrow\operatorname*{argmin}_{(u,v) \in\mathcal{U}\times\mathcal{V}}\bigg{[}\sum_{(a_{i},c_{i,a})\in\mathcal{L}_{t} }\bar{w}_{i,a,t}(u_{i}^{\top}v_{a}-r_{i,a,t})^{2}+\) \[\lambda\bigg{(}\sum_{i\in[n]}\|u_{i}\|_{2}+\sum_{a\in[m]}\|v_{a}\|_{2} \bigg{)}\bigg{]}\) else  Propagate cohort beliefs: \(\mu_{t}\leftarrow\mu_{t-1}\)  Propagate factor estimates: \((\hat{\bm{u}}_{t},\hat{\bm{v}}_{t})\leftarrow(\hat{\bm{u}}_{t-1},\hat{\bm{v}}_ {t-1})\) end Recommend ads: for each \(i\in[n]\), recommend at \(a_{i,t}\) via \[a_{i,t}=\left\{\begin{array}{ll}\operatorname*{argmax}_{a\in\mathcal{A}_{t}} \hat{u}_{i,t}^{\top}\hat{v}_{a,t}&\text{w.p. }1-\varepsilon\\ a\sim\text{U}(\mathcal{A}_{t})&\text{w.p. }\varepsilon\end{array}\right.\]  Append responses: \(\mathcal{L}_{t+1}\leftarrow\mathcal{L}_{t}\cup\{a_{i,t},c_{a,i,t}\}\) end ```

**Algorithm 1**Recommendation procedure.

Simulator and experiments

Our simulator was built upon RecSim (21) (source code at https://github.com/emiehling/cookie-consent/). The high-level architecture of our simulator is illustrated in Fig. 1 of Section 4. Additional details (with references to objects in the source code) are provided below.

**Advertisement and user samplers.** The advertisement sampler object (AdvertisementSampler) defines the distribution of each ad feature, here assumed to simply be the ad's topic. Similarly, the user sampler object (UserStateSampler) defines the distribution of each user feature, described by the joint cookie-cohort prior, the opt-in distribution, and the statistics of the user's topic affinities. The ad sampler and user sampler objects are used to define a gym environment for the recommendation procedure (via MultiUserEnvironment and RecSimGymEnv). Ads are resampled in each round whereas users remain fixed for the duration of the episode.

**Users and the recommender agent.** Each user is described by the class RSUserModel. This class contains the user's choice model, i.e., the logit model dictating the binary click decision given the recommended ad (see the method simulate_response).

Given the collections of ads and users, the recommender agent makes recommendations according to an \(\varepsilon\)-greedy bandit (see the pseudocode in Section B).

Retraining consists of first updating the cohort beliefs (via the methods update_cohort_beliefs and get_click_probabilities, see Appendix A for the expressions), updating weights \(\bar{w}_{i,a,t}\), and recomputing the matrix factor estimates (via get_estimated_factors, see (2)). Estimation is carried out via stochastic gradient descent with a learning rate of \(0.01\), regularization weight of \(0.01\), and a stopping threshold on the mean-squared error of \(\varepsilon_{\text{thresh}}=0.001\). Latent factors are assumed to be of dimension \(k=50\).

**Experimental setup.** Simulations were run in Python 3.8 on an Intel(R) Xeon(R) CPU E5-2667 v2 (3.30GHz). Unless otherwise stated, baseline parameters of the simulation environment were as follows: \(n=1000\) users, \(m=200\) ads, ad candidate size \(l=50\), batch size \(T_{b}=1\), offline response set \(\mathcal{L}_{0}=\varnothing\), exploration probability \(\varepsilon=0.1\), and binary cookie and cohort spaces. Simulations were averaged over 500 runs/episodes.

The experimental setup can be extended in a variety of directions to investigate additional interesting questions. One direction is to extend the feature description of the ads (beyond topic) to include features that reflect ad quality and location. This would enable studying how the recommender system treats minority populations (compared to majority populations). Additionally, augmenting the simulator with the ability to handle a changing user pool would allow for analysis of the cold start problem.

[MISSING_PAGE_FAIL:18]

For a partially informative prior (any \(\mu\) that does not have identical rows), knowledge of a user's cookie partially reveals the user's cohort. The resulting cohort errors are consistent with Fig. 2 of Section 5.1. For the fully informative prior, the agent is completely certain of users' cohorts for users who opted-in (the agree group), but still possesses uncertainty for users who did not opt-in (the disagree group). Lastly, for the uninformative prior, revelation of a user's cookie does not inform the user's cohort (as the likelihoods of seeing cookie values are identical across cohorts) and the agent must infer cohorts solely from differences in response behavior.

**Batch size.** The batch size, \(T_{b}\), dictates how many responses to collect from each user before retraining. Values for the batch size were varied in the range \(T_{b}\in\{1,2,10\}\).

\(T_{b}=1\):

\(T_{b}=2\):

\(T_{b}=10\):

\((q_{0},q_{1})=(0.25,0.25)\)\((q_{0},q_{1})=(0.25,0.5)\)\((q_{0},q_{1})=(0.25,0.75)\)

Intuitively, for the same number of observations, cohort estimation errors are the same across various batch sizes (e.g., updates may be less frequent but they contain more data). However, waiting until a batch update (\(T_{b}>1\)) results in more interactions where the users face greater disparate estimation errors (compared to the \(T_{b}=1\) case).

**Size of offline response set.** The offline response set, \(\mathcal{L}_{0}\), is a set of recommendation-responses that are available before the online recommendation process. Recommendations in the offline set, \(\mathcal{L}_{0}\), were generated uniformly at random with responses generated by the users' choice models. Simulations were run for \(|\mathcal{L}_{0}|=\{0,1,5\}\), differing in the number of offline responses assumed available from each user.

\(|\mathcal{L}_{0}|=0\):

\(|\mathcal{L}_{0}|=1\):

\(|\mathcal{L}_{0}|=5\):

\((q_{0},q_{1})=(0.25,0.25)\)\((q_{0},q_{1})=(0.25,0.5)\)\((q_{0},q_{1})=(0.25,0.75)\)

The impact of the offline response set does not appear to have a significant effect on the evolution of the cohort errors.

[MISSING_PAGE_EMPTY:21]