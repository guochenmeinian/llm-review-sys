# Optimal Extragradient-Based Algorithms for

Stochastic Variational Inequalities with

Separable Structure

 Huizhuo Yuan\({}^{\diamond}\) Chris Junchi Li\({}^{\dagger}\) Gauthier Gidel\({}^{\ddagger}\)

Michael I. Jordan\({}^{\dagger,\Box}\) Quanquan Gu\({}^{\diamond}\) Simon S. Du\({}^{\star}\)

\({}^{\diamond}\) Department of Computer Science, University of California, Los Angeles

{hzyuan, ggu}@cs.ucla.edu

\({}^{\dagger}\) Department of Electrical Engineering and Computer Sciences, University of California, Berkeley

{junchili, jordan}@cs.berkeley.edu

\({}^{\ddagger}\) DIRO, Universite de Montreal and Mila

gauthier.gidel@umontreal.ca

\({}^{\Box}\) Department of Statistics, University of California, Berkeley

\({}^{\star}\) Paul G. Allen School of Computer Science and Engineering, University of Washington

ssdu@cs.washington.edu

###### Abstract

We consider the problem of solving stochastic monotone variational inequalities with a separable structure using a stochastic first-order oracle. Building on standard extragradient for variational inequalities we propose a novel algorithm--stochastic _accelerated gradient-extragradient_ (AG-EG)--for strongly monotone variational inequalities (VIs). Our approach combines the strengths of extragradient and Nesterov acceleration. By showing that its iterates remain in a bounded domain and applying scheduled restarting, we prove that AG-EG has an optimal convergence rate for strongly monotone VIs. Furthermore, when specializing to the particular case of bilinearly coupled strongly-convex-strongly-concave saddle-point problems, including bilinear games, our algorithm achieves fine-grained convergence rates that match the respective lower bounds, with the stochasticity being characterized by an additive statistical error term that is optimal up to a constant prefactor.

## 1 Introduction

The variational inequality (VI) problem plays a central role in a wide range of optimization problems with convex structure, including convex minimization, saddle-point problems, and games (Facchinei and Pang, 2003; Nemirovski, 2004; Nemirovski et al., 2009; Juditsky et al., 2011; Jordan et al., 2023). A general VI problem aims to find a solution \(\bm{z}^{*}\in\mathcal{Z}\) that satisfies:

\[\langle\mathcal{W}(\bm{z}^{*}),\bm{z}^{*}-\bm{z}\rangle\leq 0,\quad\forall\bm{z} \in\mathcal{Z},\] (1)

where \(\mathcal{Z}\) is a finite-dimensional closed and convex feasible set and \(\mathcal{W}(\cdot)\) is a monotone operator in the following form:

\[\mathcal{W}(\bm{z})=\nabla\mathcal{F}(\bm{z})+\mathcal{H}(\bm{z})+J^{\prime}( \bm{z})\equiv\mathbb{E}_{\xi}[\nabla\widetilde{\mathcal{F}}(\bm{z};\xi)]+ \mathbb{E}_{\zeta}[\widetilde{\mathcal{H}}(\bm{z};\zeta)]+J^{\prime}(\bm{z}),\] (2)

where \(\mathcal{F}\) is continuously differentiable with \(L\)-Lipschitz continuous gradient and is \(\mu\)-strongly convex, \(\mathcal{H}\) is an \(M\)-Lipschitz monotone operator, \(J^{\prime}\in\partial J\) is the subgradient of a simple and convex function, \(\xi\) and \(\zeta\) are drawn from distributions \(\mathcal{D}_{\xi}\) and \(\mathcal{D}_{\zeta}\), respectively. This formulation captures a separable structure in which \(\mathcal{H}\) usually models the competing forces in a system, and \(J\) modelsa nonsmooth factor. In addition, we consider the stochastic setting where we can only access \(\nabla\mathcal{F}\) and \(\mathcal{H}\) through their unbiased estimators \(\nabla\widetilde{\mathcal{F}}(\bm{z};\xi)\) and \(\widetilde{\mathcal{H}}(\bm{z};\zeta)\) respectively.

A notable instance of the VI problem (1) with separable structure (2) is the widely studied _bilinearly coupled strongly-convex-strongly-concave saddle-point problem_:

\[\min_{\bm{x}\in\mathbb{R}^{n}}\max_{\bm{y}\in\mathbb{R}^{m}}\;\mathscr{F}(\bm{ x},\bm{y})=F(\bm{x})+H(\bm{x},\bm{y})-G(\bm{y})\equiv\mathbb{E}_{\xi}\left[f( \bm{x};\xi)\right]+\mathbb{E}_{\zeta}\left[h(\bm{x},\bm{y};\zeta)\right]- \mathbb{E}_{\xi}\left[g(\bm{y};\xi)\right],\] (3)

where \(H(\bm{x},\bm{y})\equiv\bm{x}^{\top}\mathbf{B}\bm{y}-\bm{x}^{\top}\mathbf{u}_{ \bm{x}}+\mathbf{u}_{\bm{y}}^{\top}\bm{y}\) is the bilinear coupling function with the coupling matrix \(\mathbf{B}\in\mathbb{R}^{n\times m}\). Note that (3) is a special instance of (1) when taking \(\mathcal{F}(\bm{z})=F(\bm{x})+G(\bm{y})\), \(\mathcal{H}(\bm{z})=[\nabla_{\bm{x}}H(\bm{x},\bm{y});-\nabla_{\bm{y}}H(\bm{x}, \bm{y})]\) and \(J=0\). In addition to a wide range of applications in economics, problems of form (3) are becoming increasingly important in machine learning. For instance, (3) appears in reinforcement learning, differentiable games, regularized empirical risk minimization, and robust optimization formulations. It can also be seen as a local approximation of the nonconvex-nonconcave minimax games--e.g., the generative adversarial network (GAN) (Goodfellow et al., 2020)--around a local Nash equilibrium (Mescheder et al., 2017; Nagarajan and Kolter, 2017).

In this paper, we aim to improve the efficiency of solving (1) by utilizing the structural information of the monotone operator in (2). More specifically, we consider the case when \(\mathcal{F}\) is strongly monotone, or zero. Although optimal convergence results have been obtained for the monotone VI problem (1) (Chen et al., 2017) as well as the special case of convex-concave saddle-point problem with bilinear coupling (3) (Chen et al., 2014), it remains open how to design an optimal algorithm for the strongly monotone VI problem. Notably, for the special case (3) when \(F\) and/or \(G\) are strongly convex, several concurrent works have independently obtained the optimal convergence rates (Kovalev et al., 2022; Thekumparampil et al., 2022; Jin et al., 2022; Metelev et al., 2022; Li et al., 2022b). On the other hand, when both \(F\) and \(G\) are zero, optimal convergence results have been obtained by Li et al. (2022a) and the accelerated-gradient optimistic gradient approach (Li et al., 2022b). We defer a more complete overview of related work to the appendix.

### Main Contributions

We start with the strongly monotone VI problem in an unbounded feasible set, extending the scope of recent work such as Jin et al. (2022) and going beyond earlier studies that focus on nonstrongly monotone VIs in a bounded feasible set (Juditsky et al., 2011; Chen et al., 2017).1 We propose a class of algorithms named stochastic _accelerated gradient-extragradient_ (AG-EG), which combine Nesterov acceleration with the extragradient method. By employing either a strong-convexity shifting technique or a scheduled restarting scheme, our algorithm achieves convergence rates that match the lower bounds for the general strongly monotone VI problem (1), the special SC-SC saddle-point problem (3), and bilinear games, in both deterministic and stochastic settings, thus providing a unified optimal solution. In sharp contrast to the accelerated mirror-prox (AMP) algorithm proposed by Chen et al. (2017); Jordan et al. (2023), our analysis does not rely on the boundedness of the feasible set \(\mathcal{Z}\), which makes our algorithm projection-free. We also extend our algorithm to VIs with bounded feasible set and/or nondifferentiable convex regularization through proximal mapping. We summarize our contributions as follows:

Footnote 1: VIs in an unbounded feasible set is more difficult to solve because existing algorithms and analyses crucially rely on the boundedness of the feasible set.

1. We present a direct approach for separable strongly monotone VIs, where the iteration complexity lower bound due to Zhang et al. (2022) is matched as \(\mathcal{O}\left(\sqrt{\frac{L}{\mu}}+\frac{M}{\mu}+\frac{\sigma^{2}}{\mu^{2} \varepsilon^{2}}\right)\log\left(\frac{L}{\mu}\frac{1}{\varepsilon}\right)\), which admits a sharp near-unity coefficient (SS2.3, Theorem 2.3). Here \(\sigma^{2}\) is the weighted, uniform variance bound on the stochastic gradient and stochastic operator.
2. We also present a stochastic AG-EG algorithm equipped with scheduled restarting, which achieves the sharpest possible iteration complexity of \(\mathcal{O}\left(\left(\sqrt{\frac{L}{\mu}}+\frac{M}{\mu}\right)\log\left( \frac{1}{\varepsilon}\right)+\frac{\sigma^{2}}{\mu^{2}\varepsilon^{2}}\right)\) for finding an \(\varepsilon\)-optimal point. The deterministic part matches the complexity lower bound in Zhang et al. (2022), while the stochastic part matches the optimal statistical error.

When specializing the VI problem to bilinearly coupled SC-SC saddle-point problems, our results have the following implications:

Strongly-convex-strongly-concave (SC-SC) Saddle-Point Problem.For the class of SC-SC saddle-point problems, the stochastic AG-EG descent-ascent Algorithm 1, equipped with scaling reduction, achieves an iteration complexity of

\[\mathcal{O}\left(\left(\sqrt{\frac{L_{F}}{\mu_{F}}\vee\frac{L_{G}}{\mu_{G}}}+ \sqrt{\frac{\lambda_{\max}(\mathbf{B}^{\top}\mathbf{B})}{\mu_{F}\mu_{G}}} \right)\log\left(\frac{1}{\varepsilon}\right)+\frac{\sigma^{2}}{\mu_{F}^{2} \varepsilon^{2}}\right),\] (4)

where \(F:\mathbb{R}^{n}\rightarrow\mathbb{R}\) is \(L_{F}\)-smooth and \(\mu_{F}\)-strongly convex, \(G:\mathbb{R}^{m}\rightarrow\mathbb{R}\) is \(L_{G}\)-smooth and \(\mu_{G}\)-strongly convex. When the optimization problem is deterministic, the complexity upper bound matches the lower bound of Zhang et al. (2022)(SS3.1, Corollary 2.8).

Bilinear Games.For bilinear games \((\nabla f(\bm{x};\xi)=\bm{0}\) and \(\nabla g(\bm{y};\xi)=\bm{0}\) almost surely), Algorithm 1, equipped with scheduled restarting achieves an iteration complexity of

\[\mathcal{O}\!\left(\sqrt{\frac{\lambda_{\max}(\mathbf{B}^{\top}\mathbf{B})}{ \lambda_{\min}(\mathbf{B}\mathbf{B}^{\top})}}\log\left(\frac{\sqrt[4]{\lambda _{\min}(\mathbf{B}\mathbf{B}^{\top})\lambda_{\max}(\mathbf{B}^{\top}\mathbf{B })}}{\sigma_{\mathrm{Bil}}}\right)+\frac{\sigma_{\mathrm{Bil}}^{2}}{\lambda_{ \min}(\mathbf{B}\mathbf{B}^{\top})\varepsilon^{2}}\right)\!,\] (5)

where \(\sigma_{\mathrm{Bil}}^{2}\) is the variance of the stochastic gradient on the bilinear coupling term. When there is no randomness, this complexity result reduces to \(\mathcal{O}\left(\sqrt{\frac{\lambda_{\max}(\mathbf{B}^{\top}\mathbf{B})}{ \lambda_{\min}(\mathbf{B}\mathbf{B}^{\top})}}\log\left(\frac{1}{\varepsilon} \right)\right)\) for bilinear games, matching the lower bound of Ibrahim et al. (2020)(SS3.2, Corollary 3.3).2

Footnote 2: For the function class of bilinear games, we assume that \(n=m\) where \(\mathbf{B}\) is a nonsingular square matrix, so that \(\lambda_{\min}(\mathbf{B}\mathbf{B}^{\top})>0\) and the complexity makes sense. See ยง3.2 for more on this.

Organization.The rest of this paper is organized as follows. Section 2 proposes the Accelerated Gradient-Extragradient Descent-Ascent algorithm for strongly monotone VIs, showing that it achieves an accelerated convergence rate, and extending to VIs with bounded domains with proximal operator. Section 3 discusses two specific instances of saddle-point problems, where our proposed AG-EG algorithm has a convergence rate that matches the corresponding lower bounds. Finally, Section 4 summarizes our results and suggests future directions.

Notation.Let \(\lambda_{\max}(\mathbf{M})\) (resp. \(\lambda_{\min}(\mathbf{M})\) be the largest (resp. smallest) eigenvalue of a real symmetric matrix \(\mathbf{M}\). Let \(a\lor b\equiv\max(a,b)\) (resp. \(a\wedge b\equiv\min(a,b)\)) denote the maximum (resp. minimum) value of two reals \(a,b\). For two nonnegative real sequences \((a_{n})\) and \((b_{n})\), we write \(a_{n}=\mathcal{O}(b_{n})\) or \(a_{n}\lesssim b_{n}\) (resp. \(a_{n}=\Omega(b_{n})\) or \(a_{n}\gtrsim b_{n}\)) to denote \(a_{n}\leq Cb_{n}\) (resp. \(a_{n}\geq Cb_{n}\)) for all \(n\geq 1\) for a positive, numerical constant \(C\), and let \(a_{n}\asymp b_{n}\) if both \(a_{n}\lesssim b_{n}\) and \(a_{n}\gtrsim b_{n}\) hold. We also let \(a_{n}=\tilde{\mathcal{O}}\left(b_{n}\right)\) denote \(a_{n}\leq Cb_{n}\) where \(C\) hides a polylogarithmic factor in problem-dependent constants. We let \([\bm{x};\bm{y}]\in\mathbb{R}^{n+m}\) concatenate two vectors \(\bm{x}\in\mathbb{R}^{n}\) and \(\bm{y}\in\mathbb{R}^{m}\). Finally for two real symmetric matrices \(\mathbf{A}\) and \(\mathbf{B}\), we denote \(\mathbf{A}\preceq\mathbf{B}\) (resp. \(\mathbf{A}\succeq\mathbf{B}\)) when \(\mathbf{v}^{\top}(\mathbf{A}-\mathbf{B})\mathbf{v}\leq 0\) (resp. \(\mathbf{v}^{\top}(\mathbf{A}-\mathbf{B})\mathbf{v}\geq 0\)) holds for all vectors \(\mathbf{v}\).

## 2 Accelerated Gradient-Extragradient Descent-Ascent Algorithm

In this section, we focus on accelerating the extragradient algorithm for the strongly monotone VI problem in (1) with separable structure (2). Our algorithm design draws inspiration from the work of Chen et al. (2017) on the stochastic Accelerated MirrorProx (AMP) algorithm for nonstrongly monotone VIs. The AMP algorithm applies Nesterov-type acceleration on top of the mirror-prox method (Korplevich, 1976; Nemirovski, 2004) and attains the optimal iteration complexity of \(\mathcal{O}\left(\sqrt{\frac{L}{\varepsilon}}+\frac{M}{\varepsilon}\right)\). However, the big-O notation hides the diameter of the feasible set, and the existing theory for the AMP algorithm can only deal with VIs with bounded domain. Our algorithm not only achieves the optimal convergence rates for the strongly monotone VI problem with separable structure but we also remove the dependency on the diameter of the feasible set. Therefore, our algorithm can deal with VIs with unbounded domains.

Throughout SS2, we maintain conceptual simplicity by presenting all our algorithm designs in the deterministic setting, while presenting the convergence results in the more general stochastic setting. These results can be easily reduced to the deterministic setting when the stochastic noise vanishes.

### Setting and Assumptions

In this section, we formally introduce our assumptions. We first state the smoothness and monotonicity assumptions that we impose on \(\mathcal{F}\) and \(\mathcal{H}\).

**Assumption 2.1** (Monotonicity, strong convexity and smoothness): _We assume that function \(\mathcal{F}(\cdot)\) is continuously differentiable with \(L\)-Lipschitz continuous gradient and is \(\mu\)-strongly convex. That is, for any \(\bm{z},\bm{z}^{\prime}\in\bar{\mathcal{Z}}\),_

\[\tfrac{\mu}{2}\|\bm{z}-\bm{z}^{\prime}\|^{2}\leq\mathcal{F}(\bm{z})-\mathcal{ F}(\bm{z}^{\prime})-\nabla\mathcal{F}(\bm{z}^{\prime})^{\top}(\bm{z}-\bm{z}^{ \prime})\leq\tfrac{L}{2}\|\bm{z}-\bm{z}^{\prime}\|^{2}.\]

_Furthermore, operator \(\mathcal{H}(\cdot)\) is monotone and \(M\)-Lipschitz in the sense that for any \(\bm{z},\bm{z}^{\prime}\in\mathcal{Z}\),_

\[\langle\mathcal{H}(\bm{z})-\mathcal{H}(\bm{z}^{\prime}),\bm{z}-\bm{z}^{ \prime}\rangle\geq 0,\quad\|\mathcal{H}(\bm{z})-\mathcal{H}(\bm{z}^{\prime})\| \leq M\|\bm{z}-\bm{z}^{\prime}\|.\]

Second, we impose assumptions on the noise variance.

**Assumption 2.2** (Unbiased gradients and variance bounds): _We assume that \(\bm{z}\in\mathcal{Z}\), samples \(\xi\sim\mathcal{D}_{\xi}\) and \(\zeta\sim\mathcal{D}_{\zeta}\) are drawn from given distributions such that the following conditions hold: \(\mathbb{E}_{\xi}[\nabla\bar{\mathcal{F}}(\bm{z};\xi)]=\nabla\mathcal{F}(\bm{z})\), \(\mathbb{E}_{\zeta}[\tilde{\mathcal{H}}(\bm{z};\zeta)]=\mathcal{H}(\bm{z})\), and_

\[\mathbb{E}_{\xi}\left[\|\nabla\bar{\mathcal{F}}(\bm{z};\xi)-\nabla\mathcal{F}( \bm{z})\|^{2}\right]\leq\sigma_{\rm Str}^{2},\quad\mathbb{E}_{\zeta}\left[\| \tilde{\mathcal{H}}(\bm{z};\zeta)-\mathcal{H}(\bm{z})\|^{2}\right]\leq\sigma_ {\rm Bil}^{2}.\] (6)

For all results in this work, we suppose that Assumptions 2.1 and 2.2 hold with appropriate parameter settings. Given a desired accuracy \(\varepsilon>0\), our goal is to find an _\(\varepsilon\)-optimal point_ defined as:

**Definition 2.1** (\(\varepsilon\)-Optimal point): _A point \(\bm{z}\in\mathcal{Z}\) is called an \(\varepsilon\)-optimal point for the VI problem in (1) if \(\|\bm{z}-\bm{z}^{*}\|\leq\varepsilon\)._

### The ExtraGradient (EG) Algorithm

We first consider the case where \(\mathcal{Z}\) is the entire space \(\mathbb{R}^{n}\) and the objective is smooth (\(J=0\)). The extragradient (EG) algorithm, introduced by Korpelevich (1976), is designed to address cyclic behavior in saddle-point problems by introducing an extrapolated point for gradient evaluation. In the context of VI problems (1), let \(\bm{z}_{t}\) represents the \(t\)-th iterate of the EG algorithm. The update rule of EG is as follows:

\[\bm{z}_{t+1}=\bm{z}_{t}-\eta\mathcal{W}\big{(}\bm{z}_{t}-\eta\mathcal{W}(\bm{z }_{t})\big{)},\] (7)

where \(\eta>0\) is the step size. For a \(L\)-smooth and \(\mu\)-strongly monotone operator \(\mathcal{W}\), Tseng (1995); Mokhtari et al. (2020); Gidel et al. (2019) have shown that the EG algorithm achieves an iteration complexity of \(\mathcal{O}(\kappa\log(1/\varepsilon))\), where \(\kappa=L/\mu\) denotes the condition number of the problem.

### Accelerating the ExtraGradient Algorithm, Direct Approach

The convergence rate of the EG algorithm is far from optimal for the strongly monotone VI problem in (1) with separable structure (2). Firstly, the update rule in (7) takes \(\mathcal{W}\) as a whole without utilizing the separable structure. This prevents us from exploiting the properties of \(\nabla\mathcal{F}\). Secondly, in the case of bilinear games, the established lower bound for EG is \(\Omega(\sqrt{\kappa}\log(1/\varepsilon))\) rather than \(\Omega(\kappa\log(1/\varepsilon))\). This discrepancy highlights the potential for accelerating the EG algorithm in various directions. We first rewrite the EG update rule in (7) as follows:

\[\bm{z}_{t-\frac{1}{2}} =\bm{z}_{t-1}-\eta\mathcal{W}(\bm{z}_{t-1})=\bm{z}_{t-1}-\eta \big{(}\mathcal{H}(\bm{z}_{t-1})+\nabla\mathcal{F}(\bm{z}_{t-1})\big{)},\] \[\bm{z}_{t} =\bm{z}_{t-1}-\eta\mathcal{W}(\bm{z}_{t-\frac{1}{2}})=\bm{z}_{t-1}- \eta\big{(}\mathcal{H}(\bm{z}_{t-\frac{1}{2}})+\nabla\mathcal{F}(\bm{z}_{t- \frac{1}{2}})\big{)}.\] (8)

To accelerate the process based on \(\nabla\mathcal{F}\), we consider Nesterov's second acceleration scheme on minimizing a single convex function \(\mathcal{F}\)(Tseng, 2008; Lan and Zhou, 2018; Lin et al., 2020):

\[\bm{z}_{t-1}^{\rm md}=(1-\alpha_{t})\bm{z}_{t-1}^{\rm sg}+\alpha_{t}\bm{z}_{t- 1},\bm{z}_{t}=\bm{z}_{t-1}-\frac{\eta}{\alpha_{t}}\nabla\mathcal{F}(\bm{z}_{t-1 }^{\rm md}),\bm{z}_{t}^{\rm sg}=(1-\alpha_{t})\bm{z}_{t-1}^{\rm sg}+\alpha_{t} \bm{z}_{t},\] (9)where \(\alpha_{t}\) is the extrapolation step size in the standard three-line Nesterov scheme. Here we adopt the notation \(\bm{z}^{\text{nd}}\) and \(\bm{z}^{\text{ag}}\) to indicate the middle point and the aggregated point (Chen et al., 2017), respectively. Next, to achieve acceleration, we replace the gradient of \(\nabla\mathcal{F}\) evaluated at both \(\bm{z}_{t-1}\) and \(\bm{z}_{t-\frac{1}{2}}\) in (8) by the gradient evaluated at the extrapolated point \(\bm{z}_{t-1}^{\text{nd}}\) in (9). Furthermore, we shift the index of \(\bm{z}^{\text{ag}}\) by \(\frac{1}{2}\) to indicate the use of \(\bm{z}_{t-\frac{1}{2}}\) instead of \(\bm{z}_{t}\) in the \(\bm{z}^{\text{ag}}\) update in (9). In addition, we take into account the \(\mu\)-strong convexity of \(\mathcal{F}\) and shift the gradient of the strongly convex part \(\nabla_{\bm{z}}\left[\frac{\mu}{2}\|\bm{z}-\bm{z}_{0}\|^{2}\right]=\mu(\bm{z}- \bm{z}_{0})\) from \(\nabla\mathcal{F}(\bm{z})\) to \(\mathcal{H}(\bm{z})\) as \(\mathcal{W}(\bm{z})=(\nabla\mathcal{F}(\bm{z})-\mu(\bm{z}-\bm{z}_{0}))+( \mathcal{H}(\bm{z})+\mu(\bm{z}-\bm{z}_{0}))\), we obtain the following update rule for a direct version of an accelerated EG algorithm (different step size schemes for \(\eta_{t}\) are required for different algorithmic designs):

\[\begin{cases}\bm{z}_{t-\frac{1}{2}}^{\text{md}}=(1-\alpha_{t})\bm{z}_{t-\frac {3}{2}}^{\text{ag}}+\alpha_{t}\bm{z}_{t-1},\\ \bm{z}_{t-\frac{1}{2}}=\bm{z}_{t-1}-\eta_{t}\left(\mathcal{H}(\bm{z}_{t-1})+ \nabla\mathcal{F}(\bm{z}_{t-1}^{\text{md}})-\mu(\bm{z}_{t-1}^{\text{md}}-\bm {z}_{t-1})\right),\\ \bm{z}_{t}=\bm{z}_{t-1}-\eta_{t}\left(\mathcal{H}(\bm{z}_{t-\frac{1}{2}})+ \nabla\mathcal{F}(\bm{z}_{t-1}^{\text{md}})-\mu(\bm{z}_{t-1}^{\text{md}}-\bm {z}_{t-\frac{1}{2}})\right),\\ \bm{z}_{t-\frac{1}{2}}^{\text{ag}}=(1-\alpha_{t})\bm{z}_{t-\frac{3}{2}}^{\text {ag}}+\alpha_{t}\bm{z}_{t-\frac{1}{2}}.\end{cases}\] (10)

We call the algorithm in (10) the _accelerated gradient-extragradient, direct approach_ (AG-EG-Direct), and postpone its full description to Algorithm 2 in SSC.1. The final output of the direct approach is \(\bm{z}_{T}\) after \(T\) iterates. The following theorem records the convergence rate and iteration complexity of AG-EG (direct approach).

**Theorem 2.3** (Convergence of stochastic AG-EG, direct approach): _Suppose Assumptions 2.1 and 2.2 hold. Fix any \(r\in(0,1)\), \(\beta\in(0,\infty)\), let \(\kappa_{\beta}=\frac{L}{\mu}+\frac{(1+\beta)M^{2}}{\mu^{2}}\) and set the step size upper bound \(\bar{\alpha}\equiv\frac{r}{1+\sqrt{1+r\kappa_{\beta}}}\). For any sequence of step sizes \(\alpha_{t}\in(0,\bar{\alpha}]\) and \(\eta_{t}=\frac{\alpha_{t}}{\mu}\), the iterates of stochastic AG-EG (direct approach) satisfy that for all \(t=1,\ldots,T\), we have_

\[\mathbb{E}\left\|\bm{z}_{t}-\bm{z}^{\star}\right\|^{2}\leq\left\|\bm{z}_{0}- \bm{z}^{\star}\right\|^{2}\left(\frac{L}{\mu}+1\right)\prod_{s=1}^{t}(1-\alpha _{s})+\frac{3\sigma^{2}}{\mu^{2}}\sum_{s=1}^{t}\alpha_{s}^{2}\prod_{\tau=s+1} ^{t}(1-\alpha_{\tau}),\] (11)

_where we define \(\sigma=\frac{1}{\sqrt{3}}\sqrt{\frac{1}{1-r}\sigma_{\text{Str}}^{2}+(2+\frac{ 1}{\beta})\sigma_{\text{Bil}}^{2}}\)._

In the rest of the paper, we use the same definition \(\sigma\) as in Theorem 2.3. The proof of Theorem 2.3 is provided in SSD.4. We further note that one possible choice of step size is to let \(\alpha_{t}\equiv\alpha\), such that (11) reduces to

\[\mathbb{E}\left\|\bm{z}_{t}-\bm{z}^{\star}\right\|^{2}\leq\left\|\bm{z}_{0}- \bm{z}^{\star}\right\|^{2}\left(\frac{L}{\mu}+1\right)e^{-\alpha t}+\frac{3 \sigma^{2}}{\mu^{2}}\alpha.\]

For any given \(T\geq 1\), by choosing the optimal \(\alpha=\frac{1}{T}\left(1+\log\left(\frac{\mu^{2}T}{3\sigma^{2}}\left(\frac{L} {\mu}+1\right)\|\bm{z}_{0}-\bm{z}^{\star}\|^{2}\right)\right)\ \wedge\ \bar{\alpha}\), (11) implies

\[\mathbb{E}\|\bm{z}_{T}-\bm{z}^{\star}\|^{2}\leq\|\bm{z}_{0}-\bm{z}^{\star}\|^{2 }\left(\frac{L}{\mu}+1\right)e^{-\bar{\alpha}T}+\frac{3\sigma^{2}}{\mu^{2}T} \left(1+\log\left(\frac{\mu^{2}T}{3\sigma^{2}}\left(\frac{L}{\mu}+1\right)\| \bm{z}_{0}-\bm{z}^{\star}\|^{2}\right)\right).\]

Prescribing the desired accuracy \(\varepsilon>0\), the iteration complexity to output an \(\varepsilon\)-optimal minimax point is 3

Footnote 3: Throughout this work, we focus on the iteration complexity whereas the required number of queries to the stochastic gradient oracle is three times the iteration complexity.

\[\mathcal{O}\left(\left(\sqrt{\frac{L}{\mu}}+\frac{M}{\mu}+\frac{\sigma^{2}}{\mu^ {2}\varepsilon^{2}}\right)\log\left(\left(\frac{L}{\mu}+1\right)\|\bm{z}_{0}- \bm{z}^{\star}\|^{2}/\varepsilon^{2}\right)\right).\]

We conjecture that the logarithmic factor in the optimal statistical rate \(\frac{\sigma^{2}}{\mu^{2}\varepsilon^{2}}\) is removable using a proper diminishing step size, a possibility that we reserve for future study. In the setting of deterministic optimization, setting \(\sigma=0\) and \(r\to 1^{-}\), \(\beta\to 0^{+}\) in Theorem 2.3, we obtain the optimal iteration complexity bound as follows:

\[\left(1+\sqrt{1+\frac{L}{\mu}+\frac{M^{2}}{\mu^{2}}}\right)\log\left(\left( \frac{L}{\mu}+1\right)/\varepsilon^{2}\right).\] (12)

**Remark 2.4**: _Our complexity bounds fundamentally differs from the previous analysis (Chen et al., 2017; Jordan et al., 2023) for separable smooth (strongly) monotone VIs. The convergence results in previous studies are dependent on the diameter of the domain, whereas our convergence rate is independent of the domain parameters and eliminates the need for projection onto a bounded domain. Moreover, our contributions go beyond those of Chen et al. (2017) by extending the analysis to the strongly monotone case. In comparison with Jordan et al. (2023), we design an algorithm where \(\nabla\mathcal{F}\) is strongly monotone and resolve the open problem of extending the analysis to the stochastic case. Additionally, our complexity bound in (12) indicates a near-unity coefficient on the condition-number exponent, improving the corresponding coefficient in Chen et al. (2017, Theorem 15) by an asymptotic factor of \(4\)._

The direct approach, which reduces to EG when \(\nabla\mathcal{F}=0\) and \(\mu=0\), falls short of attaining optimality within the specific regime of bilinear games. In the next subsection, we will introduce a new algorithm that can overcome this limitation.

### Accelerating the ExtraGradient Algorithm with Scheduled Restarting

In this subsection, we solve problem (1) by further accelerating the stochastic EG algorithm. Rather than directly relying on the strong monotonicity of \(\nabla\mathcal{F}\), the inner updates of our new algorithm are identical to the updates in (10) with \(\mu=0\). Due to the domain-independent nature of our analysis, we can apply the scheduled restarting technique (O'donoghue and Candes, 2015; Roulet and d'Aspremont, 2017; Renegar and Grimmer, 2022) to the outer loop, accelerating the algorithm from sublinear convergence to linear convergence. In addition, the output of our algorithm is the aggregated point \(\bm{z}_{T-\frac{1}{2}}^{\text{ag}}\) after \(T\) iterates. We present the full algorithm in Algorithm 1.

``` Require: Initialization \(\bm{z}_{0}^{[0]}\), total number of epochs \(\mathscr{S}\geq 1\), total number of per-epoch iterates \((T_{s}:s=1,\ldots,\mathscr{S})\), stepsizes \((\alpha_{t},\eta_{t}:t=1,2,\ldots)\). for\(s=1,2,\ldots,\mathscr{S}\)do  Set \(\bm{z}_{-\frac{1}{2}}^{\text{ag}}\leftarrow\bm{z}_{0}^{[s-1]},\bm{z}_{0} \leftarrow\bm{z}_{0}^{[s-1]},\bm{z}_{0}^{\text{md}}\leftarrow\bm{z}_{0}^{[s-1]}\) for\(t=1,2,\ldots,T_{s}\)do  Draw samples \(\xi_{t-\frac{1}{2}}\sim\mathcal{D}_{\zeta}\) from oracle, and also \(\zeta_{t-\frac{1}{2}},\zeta_{t}\sim\mathcal{D}_{\zeta}\) independently from oracle \(\bm{z}_{t-\frac{1}{2}}\leftarrow\bm{z}_{t-1}-\eta_{t}\left(\tilde{\mathcal{H }}(\bm{z}_{t-1};\zeta_{t-\frac{1}{2}})+\nabla\tilde{\mathcal{F}}(\bm{z}_{t-1}^ {\text{md}};\xi_{t-\frac{1}{2}})\right)\) \(\bm{z}_{t-\frac{1}{2}}^{\text{ag}}\leftarrow(1-\alpha_{t})\bm{z}_{t-\frac{1}{ 2}}^{\text{ag}}+\alpha_{t}\bm{z}_{t-\frac{1}{2}}\) \(\bm{z}_{t}\leftarrow\bm{z}_{t-1}-\eta_{t}\left(\tilde{\mathcal{H}}(\bm{z}_{t- \frac{1}{2}};\zeta_{t})+\nabla\tilde{\mathcal{F}}(\bm{z}_{t-1}^{\text{md}};\xi _{t-\frac{1}{2}})\right)\) \(\bm{z}_{t}^{\text{md}}\leftarrow(1-\alpha_{t+1})\bm{z}_{t-\frac{1}{2}}^{\text{ag }}+\alpha_{t+1}\bm{z}_{t}\) endfor  Set \(\bm{z}_{0}^{[s]}\leftarrow\bm{z}_{T_{s}-\frac{1}{2}}^{\text{ag}}\) {//Warm-start using the output of the previous epoch} endfor Output:\(\bm{z}_{0}^{[\mathscr{S}]}\) ```

**Algorithm 1** Stochastic AcceleratedGradient-ExtraGradient (AG-EG) Descent-Ascent Algorithm, with Scheduled Restarting

We first present the convergence rate of a single epoch (i.e., the inner loop) of Algorithm 1 in Theorem 2.5. To accommodate more flexibility in the choice of parameters, we introduce three constants \(r,\beta\), and \(C\) in the theorem statement.

**Theorem 2.5** (Convergence of stochastic AG-EG, one epoch): _Suppose Assumptions 2.1 and 2.2 hold. For any fixed epoch length \(T\geq 1\), any constant \(r\in(0,1)\), \(\beta\in(0,\infty)\), \(C\in(0,\infty)\), choose step sizes \(\alpha_{t}=\frac{2}{t+1}\) and \(\eta_{t}\) such that_

\[\tfrac{t}{\eta_{t}}=\tfrac{2}{r}L\lor B+\sqrt{\tfrac{1+\beta}{r}}Mt,\] (13)_where \(B=\frac{\sigma\sqrt{T(T+1)}}{C\sqrt{\mathbb{E}\left\|\bm{z}_{0}-\bm{z}^{*}\right\|^ {2}}}\). The output \(\bm{z}_{T-\frac{1}{2}}^{ag}\) of a single epoch of Algorithm 1 satisfies_

\[\mathbb{E}\left\|\bm{z}_{T-\frac{1}{2}}^{ag}-\bm{z}^{*}\right\|^{2}\leq\frac{2} {\mu(T+1)}\left(\frac{2L}{rT}+A\sqrt{\frac{1+\beta}{r}}M\right)\mathbb{E}\left\| \bm{z}_{0}-\bm{z}^{*}\right\|^{2}+\frac{2(\frac{1}{2}+C)\sigma}{\mu\sqrt{T}} \sqrt{\mathbb{E}\left\|\bm{z}_{0}-\bm{z}^{*}\right\|^{2}},\] (14)

_where the prefactor \(A\equiv 1+C^{2}B\eta_{1}\leq 1+C^{2}\) reduces to 1 when \(\sigma=0\)._

The proof of Theorem 2.5 is provided in SSD.3. We make a few remarks on Theorem 2.5 as follows:

**Remark 2.6**: _In the setting of deterministic optimization, by taking \(\sigma=0\), \(r\to 1^{-}\), \(\beta\to 0^{+}\) in our analysis, with step size choice \(\eta_{t}=\frac{t}{2L+Mt}\), we obtain that_

\[\|\bm{z}_{T-\frac{1}{2}}^{ag}-\bm{z}^{*}\|^{2}\leq\tfrac{2}{\mu(T+1)}\Big{(} \tfrac{2L}{T}+M\Big{)}\|\bm{z}_{0}-\bm{z}^{*}\|^{2},\] (15)

_In this setting, the algorithm is independent of \(B\) and requires no knowledge of \(\|\bm{z}_{0}-\bm{z}^{*}\|^{2}\). In the face of stochasticity, we choose \(C=1\) when the initial distance to the optimal point is known. Alternatively, when only an over-estimate \(\Gamma_{0}\) of \(\sqrt{\mathbb{E}\|\bm{z}_{0}-\bm{z}^{*}\|^{2}}\) is available, we can set (large enough) \(C=\frac{\Gamma_{0}}{\sqrt{\mathbb{E}\|\bm{z}_{0}-\bm{z}^{*}\|^{2}}}\geq 1\) to obtain_

\[\mathbb{E}\|\bm{z}_{T-\frac{1}{2}}^{ag}-\bm{z}^{*}\|^{2}\leq\tfrac{2}{\mu(T+1) }\left(\frac{2L}{rT}+2\sqrt{\tfrac{1+\beta}{r}}M\right)\Gamma_{0}^{2}+\tfrac{ 4\sigma}{\mu\sqrt{T}}\Gamma_{0}.\] (16)

**Remark 2.7**: _When the constants are not a concern, the coarse-grained choices of \(r=\frac{1}{2}\) and \(\beta=1\) would suffice. Nevertheless, to optimize the constants, the tradeoff between the deviation of \(r\) from \(1\) and \(\beta\) from \(0\) is crucial, as it determines a balance between the stochastic gradient noise variance and the convergence rate coefficients._

To prepare for our multi-epoch result with the help of scheduled restarting, we perform an induction based on (16) as follows. Supposing that \(\mathbb{E}\|\bm{z}_{0}^{[s-1]}-\bm{z}^{*}\|^{2}\leq\Gamma_{0}^{2}e^{1-s}\) hold, by taking \(r=\frac{1}{2}\) and \(\beta=1\), we have

\[\mathbb{E}\|\bm{z}_{0}^{[s]}-\bm{z}^{*}\|^{2}\lesssim\tfrac{L}{\mu T_{T}^{2}} \Gamma_{0}^{2}e^{1-s}+\tfrac{M}{\mu T_{S}}\Gamma_{0}^{2}e^{1-s}+\tfrac{\sigma }{\mu\sqrt{T_{s}}}\Gamma_{0}e^{\frac{1-\varepsilon}{2}}.\]

Setting the right-hand side of the above inequality to satisfy \(\leq\Gamma_{0}^{2}e^{-s}\), and solving for \(T_{s}\), we need the epoch length satisfies \(T_{s}\asymp\sqrt{\frac{L}{\mu}}+\frac{M}{\mu}+\frac{\sigma^{2}}{\mu^{2}\Gamma_ {0}^{2}e^{1-s}}\). Thus, we can obtain the total iteration complexity as

\[\sum_{s=1}^{S}\left[\sqrt{\tfrac{L}{\mu}}+\tfrac{M}{\mu}+\tfrac{\sigma^{2}}{ \mu^{2}\Gamma_{0}^{2}e^{1-s}}\right]=\left(\sqrt{\tfrac{L}{\mu}}+\tfrac{M}{ \mu}\right)S+\tfrac{\sigma^{2}}{\mu^{2}T_{0}^{2}}\cdot\tfrac{e^{S}-1}{e-1},\]

where \(S\equiv\left\lceil\log\tfrac{\Gamma_{0}^{2}}{e^{2}}\right\rceil\). This yields the following multi-epoch iteration complexity bound:

**Corollary 2.8** (Iteration complexity of stochastic AG-EG with scheduled restarting): _Under the same condition of Theorem 2.5, the stochastic AG-EG with scheduled restarting in Algorithm 1 with epoch length \(T_{s}\asymp\sqrt{\frac{L}{\mu}}+\frac{M}{\mu}+\frac{\sigma^{2}}{\mu^{2}\Gamma_ {0}^{2}e^{1-s}}\) has a total iteration complexity of_

\[\mathcal{O}\left(\left(\sqrt{\tfrac{L}{\mu}}+\tfrac{M}{\mu}\right)\log\left( \tfrac{1}{\varepsilon}\right)+\tfrac{\sigma^{2}}{\mu^{2}\varepsilon^{2}} \right).\] (17)

Note that the hard instance constructed by Zhang et al. (2022) can be modified in a straightforward way to establish a lower bound of \(\Omega\left(\left(\sqrt{\tfrac{L}{\mu}}+\tfrac{M}{\mu}\right)\log\left( \tfrac{1}{\varepsilon}\right)\right)\) for our monotone VI (1), demonstrating the optimality of Corollary 2.8 in the deterministic separable setting. An alternative optimality argument proceeds as follows: the first term \(\sqrt{\tfrac{L}{\mu}}\) matches the lower bound for the minimization of a strongly convex function \(\mathcal{F}\)(Nesterov, 2004), and the second term \(\tfrac{M}{\mu}\) matches the lower bound for VI for non-strongly monotone operator when \(\nabla\mathcal{F}=0\)(Ouyang and Xu, 2021). This together gives a lower bound for solving monotone VI (1) via a similar argument by Thekumparampil et al. (2022).

It is worth noting that while both complexity bounds in Corollary 2.8 and Theorem 2.3 match the lower bound in Zhang et al. (2022) for strongly monotone VIs with separable structure, the direct approach in SS2.3 reduces to the _last-iterate_ independent-sample stochastic extragradient (SEG) algorithm in _bilinear games_. Consequently, the deterministic part (\(\sigma=0\)) fails to match the lower bound in Ibrahim et al. (2020). In the stochastic case with noise variance bounded away from zero, the direct approach in SS2.3 can exhibit _nonconvergence behavior_(Hsieh et al., 2020, SS3). The AG-EG algorithm in SS2.4 resolves this issue by restarting the _average-iterate_ SEG, matching the lower bound results (see SS3.2 for more details). In addition, the complexity bound in (17) also eliminates the \(\log\) prefactor of the statistical error term \(\frac{\sigma^{2}}{\mu^{2}\varepsilon^{2}}\) compared to Theorem 2.3. The optimality of our algorithm lies in not only the optimization complexity but also the statistical error rate \(\frac{\sigma^{2}}{\mu^{2}\varepsilon^{2}}\). Here the \(\varepsilon\)-optimal point \(\bm{z}\) is defined as \(\left\|\bm{z}-\bm{z}^{\star}\right\|\leq\varepsilon\).4

Footnote 4: The optimal statistical error rate \(\frac{\sigma^{2}}{\mu^{2}T}\) has been achieved by a multistage algorithm in Fallah et al. (2020), where the \(\varepsilon\)-optimal point is defined by \(\left\|\bm{z}-\bm{z}^{\star}\right\|^{2}\leq\varepsilon\). In our paper, the \(\varepsilon\)-optimal point is defined by \(\left\|\bm{z}-\bm{z}^{\star}\right\|\leq\varepsilon\). Therefore, our statistical error rate can be translated into \(\frac{\sigma^{2}}{\mu^{2}T}\) using their definition, which matches their result.

### Extension of AG-EG to Proximal Algorithms

In previous subsections, we have focused on the case where the feasible set \(\mathcal{Z}\) represents the entire space and the nondifferentiable convex function \(J\) is dropped. We now extend the AG-EG algorithm and its analysis to the more general setting that has a bounded feasible set (via Euclidean projection onto the feasible set) as well as a nondifferentiable convex regularization term (via a proximal operator). These settings are useful in various applications, such as the variational inequality on the Lorentz cone where projection onto \(\mathcal{Z}=\left\{(\bm{x},t)\in\mathbb{R}^{(n+1)}:\left\|\bm{x}\right\|\leq t\right\}\) is required (Chen et al., 2017), and the two-player game that involves projection onto the probability simplex, among others. To deal with bounded feasible set \(\mathcal{Z}\), we adopt a variant of the EG algorithm, where we project the extrapolated point and the main iterates back onto the feasible set \(\mathcal{Z}\) of \(\mathcal{W}\):

\[\bm{z}_{t-\frac{1}{2}} =P_{\mathcal{Z}}\left[\bm{z}_{t-1}-\eta\mathcal{W}(\bm{z}_{t-1}) \right]=\arg\min_{\bm{z}\in\mathcal{Z}}\left\langle\bm{z}-\bm{z}_{t-1},\eta \mathcal{W}(\bm{z}_{t-1})\right\rangle+\frac{1}{2}\left\|\bm{z}-\bm{z}_{t-1} \right\|^{2},\] \[\bm{z}_{t} =P_{\mathcal{Z}}\left[\bm{z}_{t-1}-\eta\mathcal{W}(\bm{z}_{t- \frac{1}{2}})\right]=\arg\min_{\bm{z}\in\mathcal{Z}}\left\langle\bm{z}-\bm{z}_ {t-1},\eta\mathcal{W}(\bm{z}_{t-\frac{1}{2}})\right\rangle+\frac{1}{2}\left\| \bm{z}-\bm{z}_{t-1}\right\|^{2},\] (18)

where \(P_{\mathcal{Z}}(\bm{z})=\arg\min_{\bm{z}^{\prime}\in\mathcal{Z}}\left\|\bm{z} -\bm{z}^{\prime}\right\|^{2}\) is the Euclidean projection operator. To handle the nondifferentiable simple convex function \(J\), we replace the projection operator in (18) by the following proximal mapping defined via a Bregman divergence \(\mathcal{B}(\cdot,\cdot)\):

\[\text{prox}_{\bm{z}}^{J}(\mathbf{v})\equiv\operatorname*{argmin}_{\mathbf{u} \in\mathcal{Z}}\left\langle\mathbf{v},\mathbf{u}-\bm{z}\right\rangle+\mathcal{ B}(\bm{z},\mathbf{u})+J(\mathbf{u}).\] (19)

In fact, (18) can be seen as a special case of (19) when choosing the Bregman divergence \(\mathcal{B}(\bm{z},\mathbf{u})=\frac{1}{2}\|\bm{z}-\mathbf{u}\|^{2}\) and \(J(\mathbf{u})\) as the set indicator function of the feasible set \(\mathcal{Z}\). Therefore, by substituting the prox-mapping (19) into the AG-EG updates introduced in SS2.4, we obtain the more general proximal AG-EG algorithm in Algorithm 3 (See in SSC.2), which reduces to Algorithm 1 when \(J=0\), \(\mathcal{B}(\bm{z},\mathbf{u})=\frac{1}{2}\|\bm{z}-\mathbf{u}\|^{2}\) and \(\mathcal{Z}=\mathbb{R}^{n}\). Moreover, we assume that \(\mathcal{B}(\cdot,\cdot)\) is \(\mu_{\mathcal{B}}\)-strongly convex. Without loss of generality, in contrast to the previous assumption of \(\mu\)-strong convexity for \(\mathcal{F}\), we instead assume that \(\mathcal{F}\) is \(\mu\)-strongly convex with respect to the Bregman divergence \(\mathcal{B}(\cdot,\cdot)\) (See, for example, Hazan and Kale (2014); Xu et al. (2018)). Similar to Corollary 2.8, we have the following iteration complexity result, whose proof is deferred to SSD.5:

**Corollary 2.9** (Iteration complexity of stochastic proximal AG-EG with scheduled restarting): _Under the same condition of Theorem 2.5, the stochastic proximal AG-EG with scheduled restarting in Algorithm 3, with epoch length \(T_{s}\asymp\sqrt{\frac{L}{\mu\mu_{\mathcal{B}}}}+\frac{M}{\mu\mu_{\mathcal{B}}} +\frac{\sigma^{2}\mathcal{B}(\bm{z}_{0},\bm{z}^{\star})}{\mu^{2}\mu_{\mathcal{B }}\Gamma_{0}^{2}\varepsilon^{1-s}}\), has a total iteration complexity of_

\[\mathcal{O}\left(\left(\sqrt{\frac{L}{\mu\mu_{\mathcal{B}}}}+\frac{M}{\mu\mu_{ \mathcal{B}}}\right)\log\left(\frac{1}{\varepsilon}\right)+\frac{\sigma^{2} \mathcal{B}(\bm{z}_{0},\bm{z}^{\star})}{\mu^{2}\mu_{\mathcal{B}}\varepsilon^{ 2}}\right).\]

For the deterministic case, proximal AG-EG with scheduled restarting has a total iteration complexity of \(\mathcal{O}\left(\left(\sqrt{\frac{L}{\mu\mu_{\mathcal{B}}}}+\frac{M}{\mu\mu_{ \mathcal{B}}}\right)\log\left(\frac{1}{\varepsilon}\right)\right)\) to output an \(\varepsilon\)-optimal point of (1).

Implications for Specific Instances

In this section, we discuss the implications of our AG-EG algorithm and its convergence rates when applying to two instances of saddle-point problems.

### Strongly-Convex-Strongly-Concave Saddle-Point Problem

For the stochastic bilinearly-coupled SC-SC saddle-point problem (3), we note that the smoothness and strong convexity parameters \(L_{F}\), \(L_{G}\), \(\mu_{F}\), and \(\mu_{G}\) of \(F\) and \(G\) may differ. To accommodate these variations in curvature information, we employ a scaling reduction technique. This technique enables us to convert the SC-SC with equal strong convexity parameters for \(F\) and \(G\) by reparametrizing the objective function. The same argument is also applicable to the direct approach.

In lieu of (3), we consider

\[\min_{\hat{\bm{x}}}\max_{\hat{\bm{y}}}\ \hat{\mathscr{F}}(\hat{\bm{x}},\hat{\bm{ y}})=F(\hat{\bm{x}})+\hat{H}(\hat{\bm{x}},\hat{\bm{y}})-\hat{G}(\hat{\bm{y}}),\]

where \(\hat{\mathscr{F}}(\hat{\bm{x}},\hat{\bm{y}})=\mathscr{F}(\bm{x},\bm{y})\) with the symbolic reparametrization \(\hat{\bm{x}}=\bm{x}\), \(\hat{\bm{y}}=\sqrt{\frac{\mu_{G}}{\mu_{F}}}\bm{y}\), \(\hat{H}(\hat{\bm{x}},\hat{\bm{y}})=H(\bm{x},\bm{y})\), \(\hat{G}(\hat{\bm{y}})=G(\bm{y})\) and also their derivatives \(\nabla_{\hat{\bm{y}}}\hat{H}(\hat{\bm{x}},\hat{\bm{y}})=\sqrt{\frac{\mu_{F}} {\mu_{G}}}\nabla_{\bm{y}}H(\bm{x},\bm{y}),\nabla\hat{G}(\hat{\bm{y}})=\sqrt{ \frac{\mu_{F}}{\mu_{G}}}\nabla G(\hat{\bm{y}})\) (the stochastic oracles \(\hat{h},\hat{g}\) follow the same rule). It is straightforward to verify that \(\hat{\mathscr{F}}(\hat{\bm{x}},\hat{\bm{y}})\) is \(\mu\)-strongly-convex-\(\mu\)-strongly-concave. The essence of our update rules can be summarized by the rescaled updates on \(\bm{y}\):

\[\hat{\bm{y}}_{t}=\hat{\bm{y}}_{t-1}-\eta_{t}\left(-\nabla_{\hat{ \bm{y}}}h(\hat{\bm{x}}_{t-\frac{1}{2}},\hat{\bm{y}}_{t-\frac{1}{2}};\zeta_{t})+ \nabla g(\hat{\bm{y}}_{t-1}^{\text{md}};\xi_{t-\frac{1}{2}})\right)\] \[\Leftrightarrow\ \bm{y}_{t}=\bm{y}_{t-1}-\eta_{t}\cdot\tfrac{\mu_{F}}{ \mu_{G}}\left(-\nabla_{\bm{y}}h(\bm{x}_{t-\frac{1}{2}},\bm{y}_{t-\frac{1}{2}}; \zeta_{t})+\nabla g(\hat{\bm{y}}_{t-1}^{\text{md}};\xi_{t-\frac{1}{2}})\right).\]

Therefore, it suffices to analyze Algorithm 3 for \(\hat{\mathscr{F}}(\hat{\bm{x}},\hat{\bm{y}})\) and due to this scaling reduction, we only need to prove all results for the case of \(\mu_{F}=\mu_{G}=\mu\). It is also straightforward to justify corresponding scaling changes as: \(L=L_{F}\vee\tfrac{\mu_{F}}{\mu_{G}}L_{G}\), \(M=\sqrt{\tfrac{\mu_{F}}{\mu_{G}}\lambda_{\max}(\mathbf{B}^{\top}\mathbf{B})}\), and \(\mu=\mu_{F}\). The following corollary is recovered by reverting the scaling reduction from \(\hat{\mathscr{F}}(\hat{\bm{x}},\hat{\bm{y}})\) to \(\mathscr{F}(\bm{x},\bm{y})\).

**Corollary 3.1** (Iteration complexity of stochastic AG-EG on SC-SC saddle-point problem): _For solving (3), Algorithm 1 with an epoch length \(T_{s}\asymp\sqrt{\tfrac{L_{F}}{\mu_{F}}\vee\tfrac{L_{G}}{\mu_{G}}}+\sqrt{ \tfrac{\lambda_{\max}(\mathbf{B}^{\top}\mathbf{B})}{\mu_{F}\mu_{G}}}+\tfrac{ \sigma^{2}}{\mu_{F}^{2}\Gamma_{G}^{2}e^{1-s}}\) has a total iteration complexity of_

\[\mathcal{O}\left(\left(\sqrt{\tfrac{L_{F}}{\mu_{F}}\vee\tfrac{L_{G}}{\mu_{G}}} +\sqrt{\tfrac{\lambda_{\max}(\mathbf{B}^{\top}\mathbf{B})}{\mu_{F}\mu_{G}}} \right)\log\left(\tfrac{1}{e}\right)+\tfrac{\sigma^{2}}{\mu_{F}^{2}e^{2}} \right).\]

In the deterministic case, the iteration complexity in Theorem 2.8 matches the lower bound established by Zhang et al. (2022), i.e., \(\Omega\left(\left(\sqrt{\tfrac{L_{F}}{\mu_{F}}\vee\tfrac{L_{G}}{\mu_{G}}}+ \sqrt{\tfrac{\lambda_{\max}(\mathbf{B}^{\top}\mathbf{B})}{\mu_{F}\mu_{G}}} \right)\log\left(\tfrac{1}{e}\right)\right)\). Moreover, our algorithm achieves the optimal statistical rate of \(\tfrac{\sigma^{2}}{\mu_{F}^{2}e^{2}}\) up to a constant prefactor.

**Remark 3.2**: _A well-known finding regarding the second scheme of Nesterov acceleration is its connection to the primal-dual method (Lan and Zhou, 2018; Lin et al., 2020c). This finding has been incorporated into the design of the LPD algorithm (Thekumparampil et al., 2022), where a Chambolle-Pock-style primal-dual method is utilized as an approximation of proximal point methods, instead of the extragradient used in this paper. The LPD algorithm (Thekumparampil et al., 2022) also achieves the optimal complexity for the deterministic bilinearly-coupled saddle-point problem._

### Bilinear Games

In this subsection, we consider the particular case of bilinear games. _We assume \(n=m\) such that \(\mathbf{B}\) is a nonsingular square matrix, \(\nabla f(\bm{x};\xi)=\bm{0}\)_ and \(\nabla g(\bm{y};\xi)=\bm{0}\) a.s., so (3) reduces to

\[\min_{\bm{x}}\max_{\bm{y}}\ \mathscr{F}(\bm{x},\bm{y})=\mathbb{E}_{\zeta} \left[h(\bm{x},\bm{y};\zeta)\right]=H(\bm{x},\bm{y})=\bm{x}^{\top}\mathbf{B} \bm{y}-\bm{x}^{\top}\mathbf{u}_{\bm{x}}+\mathbf{u}_{\bm{y}}^{\top}\bm{y},\] (20)and Algorithm 3 reduces to the independent-sample extragradient descent-ascent algorithm for (20). The saddle point \([\bm{z}^{*};\bm{\omega}_{\bm{y}}^{*}]\) in this case is the unique solution to the linear equation

\[\begin{bmatrix}\bm{0}&\bm{B}\\ -\bm{B}^{\top}&\bm{0}\end{bmatrix}\begin{bmatrix}\bm{z}^{*}\\ \bm{\omega}_{\bm{y}}^{*}\end{bmatrix}=\begin{bmatrix}\bm{u}_{\bm{x}}\\ \bm{u}_{\bm{y}}\end{bmatrix},\quad\text{which has a closed-form solution} \begin{bmatrix}\bm{z}^{*}\\ \bm{\omega}_{\bm{y}}^{*}\end{bmatrix}=\begin{bmatrix}-(\bm{B}^{\top})^{-1}\bm{ u}_{\bm{y}}\\ \bm{B}^{-1}\bm{u}_{\bm{x}}\end{bmatrix}.\]

Our results imply the following iteration complexity for solving stochastic bilinear games.

**Corollary 3.3** (Iteration complexity of stochastic AG-EG, bilinear games): _For solving (20), choose the step sizes \(\alpha_{t}=\frac{2}{t+1}\) and \(\eta_{t}\equiv\frac{1}{\sqrt{\lambda_{\min}(\bm{B}^{\top}\bm{B})}}\), in which case Algorithm 1 with an epoch length \(T_{s}\asymp\sqrt{\frac{\lambda_{\max}(\bm{B}^{\top}\bm{B})}{\lambda_{\min}( \bm{BB}^{\top})}}\) has the total iteration complexity of_

\[\mathcal{O}\left(\sqrt{\frac{\lambda_{\max}(\bm{B}^{\top}\bm{B})}{\lambda_{ \min}(\bm{BB}^{\top})}}\log\left(\frac{\sqrt{\lambda_{\min}(\bm{BB}^{\top}) \lambda_{\max}(\bm{B}^{\top}\bm{B})}}{\sigma_{\text{B}1}}\right)+\frac{\sigma _{\text{B}1}^{2}}{\lambda_{\min}(\bm{BB}^{\top})\varepsilon^{2}}\right).\] (21)

Note that our choice of the step size is maximal and is independent of the noise. In the deterministic setting, letting \(\sigma_{\text{B}1}\asymp\varepsilon\sqrt[4]{\lambda_{\min}(\bm{BB}^{\top}) \lambda_{\max}(\bm{B}^{\top}\bm{B})}\), the complexity bound in Corollary 3.3 reduces to \(\mathcal{O}\left(\sqrt{\frac{\lambda_{\max}(\bm{B}^{\top}\bm{B})}{\lambda_{ \min}(\bm{BB}^{\top})}}\log\left(\frac{1}{\varepsilon}\right)\right)\), which matches the lower bound in Ibrahim et al. (2020). Notably, Azizian et al. (2020) proposed an algorithm achieving an upper bound that matches the lower bound in Ibrahim et al. (2020).Li et al. (2022) also proposed a lower-bound matching SEG algorithm that uses a shared sample in both steps under an unbounded noise assumption. In contrast, our algorithm is in the independent-sample setting with bounded noise variance.

**Remark 3.4**: _Standard acceleration techniques do not attain the optimal nonasymptotic convergence rate for bilinear games (Gidel et al., 2019). This limitation applies to various algorithms, including the direct approach (Silma, 2013), as well as several other acceleration techniques (Thekumparampil et al., 2022; Kovalev et al., 2022; Jin et al., 2022), all of which fall short of achieving optimal acceleration for bilinear games. Therefore, matching both lower bounds in a single algorithm in the general stochastic setting has been an open problem. While Li et al. (2022) present an algorithm that achieves both lower bounds in a single algorithm, it relies on the use of optimistic gradients rather than extragradients on the bilinear coupling function. Furthermore, our algorithm and analysis is more general than those in Li et al. (2022) as we can handle the general variational inequality with proximal operators._

## 4 Conclusions

We have presented a stochastic extragradient-based acceleration algorithm, AG-EG, for solving stochastic monotone variational inequalities with separable structure. The iteration complexity of our algorithm matches the lower bound and is independent of the size of the feasible set. When specialized to solving the bilinearly coupled saddle-point problem (3), our AG-EG algorithm simultaneously matches lower bounds due to Zhang et al. (2022) and Ibrahim et al. (2020) for strongly-convex-strongly-concave and bilinear games, respectively. To the best of our knowledge, this is the first time that all three lower bounds have been met by a single algorithm. There are some remaining issues to be addressed, however, including the case of one-sided nonstrong convexity, the setting of unbounded noise variance, and the characterization of the full parameter regime dependency on \(\lambda_{\min}(\bm{BB}^{\top})\). These are left as important directions for future research.

## Acknowledgments and Disclosure of Funding

This work is supported in part by Canada CIFAR AI Chair to GG, by the Mathematical Data Science program of the Office of Naval Research under grant number N00014-18-1-2764 and also the Vannevar Bush Faculty Fellowship program under grant number N00014-21-1-2941 and National Science Foundation (NSF) grant IIS-1901252 to MIJ. This work is also supported in part by the NSF CAREER Award 1906169 to QG and by NSF Award's IIS-2110170 and DMS-2134106 to SSD.

## References

* Azizian et al. (2020) Waiss Azizian, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier Gidel. A tight and unified analysis of gradient-based methods for a whole spectrum of differentiable games. In _International Conference on Artificial Intelligence and Statistics_, pages 2863-2873. PMLR, 2020a.
* Azizian et al. (2020b) Waiss Azizian, Damien Scieur, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier Gidel. Accelerating smooth games by manipulating spectral shapes. In _International Conference on Artificial Intelligence and Statistics_, pages 1705-1715. PMLR, 2020b.
* Chen et al. (2014) Yunmei Chen, Guanghui Lan, and Yuyuan Ouyang. Optimal primal-dual methods for a class of saddle point problems. _SIAM Journal on Optimization_, 24(4):1779-1814, 2014.
* Chen et al. (2017) Yunmei Chen, Guanghui Lan, and Yuyuan Ouyang. Accelerated schemes for a class of variational inequalities. _Mathematical Programming_, 165(1):113-149, 2017.
* Chen et al. (2021) Ziyi Chen, Qunwei Li, and Yi Zhou. Finding local minimax points via (stochastic) cubic-regularized gda: Global convergence and complexity. _arXiv preprint arXiv:2110.07098_, 2021.
* Cohen et al. (2021) Michael B Cohen, Aaron Sidford, and Kevin Tian. Relative Lipschitzness in extragradient methods and a direct recipe for acceleration. In _12th Innovations in Theoretical Computer Science Conference (ITCS 2021)_, volume 185, page 62, 2021.
* Dann et al. (2014) Christoph Dann, Gerhard Neumann, Jan Peters, et al. Policy evaluation with temporal differences: A survey and comparison. _Journal of Machine Learning Research_, 15:809-883, 2014.
* Daskalakis et al. (2018) Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. In _International Conference on Learning Representations_, 2018.
* Du et al. (2017) Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction methods for policy evaluation. In _International Conference on Machine Learning_, pages 1049-1058. PMLR, 2017.
* Facchinei and Pang (2003) Francisco Facchinei and Jong-Shi Pang. _Finite-Dimensional Variational Inequalities and Complementarity Problems_. Springer, 2003.
* Fallah et al. (2020) Alireza Fallah, Asuman Ozdaglar, and Sarath Pattathil. An optimal multistage stochastic gradient method for minimax problems. In _2020 59th IEEE Conference on Decision and Control (CDC)_, pages 3573-3579. IEEE, 2020.
* Ghadimi and Lan (2012) Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. _SIAM Journal on Optimization_, 22(4):1469-1492, 2012.
* Gidel et al. (2019a) Gauthier Gidel, Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality perspective on generative adversarial networks. In _International Conference on Learning Representations_, 2019a.
* Gidel et al. (2019b) Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Remi Le Priol, Gabriel Huang, Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1802-1811. PMLR, 2019b.
* Goodfellow et al. (2020) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.
* Guo et al. (2020) Zhishuai Guo, Zhuoning Yuan, Yan Yan, and Tianbao Yang. Fast objective & duality gap convergence for nonconvex-strongly-concave min-max problems. _arXiv preprint arXiv:2006.06889_, 2020.
* Hazan and Kale (2014) Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. _The Journal of Machine Learning Research_, 15(1):2489-2512, 2014.
* Hazan et al. (2015)Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. In _Advances in Neural Information Processing Systems_, volume 33, pages 16223-16234, 2020.
* Ibrahim et al. [2020] Adam Ibrahim, Wass Azizian, Gauthier Gidel, and Ioannis Mitliagkas. Linear lower bounds and conditioning of differentiable games. In _International Conference on Machine Learning_, pages 4583-4593. PMLR, 2020.
* Iusem et al. [2017] Alfredo N Iusem, Alejandro Jofre, Roberto Imbuzeiro Oliveira, and Philip Thompson. Extragradient method with variance reduction for stochastic variational inequalities. _SIAM Journal on Optimization_, 27(2):686-724, 2017.
* Jelassi et al. [2020] Samy Jelassi, Carles Domingo-Enrich, Damien Scieur, Arthur Mensch, and Joan Bruna. Extragradient with player sampling for faster convergence in n-player games. In _International Conference on Machine Learning_, pages 4736-4745. PMLR, 2020.
* Jin et al. [2022] Yujia Jin, Aaron Sidford, and Kevin Tian. Sharper rates for separable minimax and finite sum optimization via primal-dual extragradient methods. In _Conference on Learning Theory_, pages 4362-4415. PMLR, 2022.
* Jordan et al. [2023] Michael I Jordan, Tianyi Lin, and Manolis Zampetakis. First-order algorithms for nonlinear generalized Nash equilibrium problems. _Journal of Machine Learning Research_, 24(38):1-46, 2023.
* Juditsky et al. [2011] Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. _Stochastic Systems_, 1(1):17-58, 2011.
* Korpelevich [1976] Galina M Korpelevich. The extragradient method for finding saddle points and other problems. _Ekonomika i Matematicheskie Metody_, 12:747-756, 1976.
* Kotsalis et al. [2020] Georgios Kotsalis, Guanghui Lan, and Tianjiao Li. Simple and optimal methods for stochastic variational inequalities, I: Operator extrapolation. _arXiv preprint arXiv:2011.02987_, 2020.
* Kovalev et al. [2022] Dmitry Kovalev, Alexander Gasnikov, and Peter Richtarik. Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling. _Advances in Neural Information Processing Systems_, 35:21725-21737, 2022.
* Lan and Ouyang [2021] Guanghui Lan and Yuyuan Ouyang. Mirror-prox sliding methods for solving a class of monotone variational inequalities. _arXiv preprint arXiv:2111.00996_, 2021.
* Lan and Zhou [2018] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. _Mathematical Programming_, 171:167-215, 2018.
* Li et al. [2022a] Chris Junchi Li, Yaodong Yu, Nicolas Loizou, Gauthier Gidel, Yi Ma, Nicolas Le Roux, and Michael Jordan. On the convergence of stochastic extragradient for bilinear games using restarted iteration averaging. In _International Conference on Artificial Intelligence and Statistics_, pages 9793-9826. PMLR, 2022a.
* Li et al. [2022b] Chris Junchi Li, Angela Yuan, Gauthier Gidel, and Michael I Jordan. Nesterov meets optimism: Rate-optimal optimistic-gradient-based method for stochastic bilinearly-coupled minimax optimization. _arXiv preprint arXiv:2210.17550_, 2022b.
* Liang and Stokes [2019] Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks. In _International Conference on Artificial Intelligence and Statistics_, pages 907-915. PMLR, 2019.
* Lin et al. [2020a] Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In _International Conference on Machine Learning_, pages 6083-6093. PMLR, 2020a.
* Lin et al. [2020b] Tianyi Lin, Chi Jin, and Michael I Jordan. Near-optimal algorithms for minimax optimization. In _Conference on Learning Theory_, pages 2738-2779. PMLR, 2020b.
* Lin et al. [2020c] Zhouchen Lin, Huan Li, and Cong Fang. Accelerated optimization for machine learning. _Nature Singapore: Springer_, 2020c.
* Lin et al. [2020d]* Loizou et al. [2020] Nicolas Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent, Simon Lacoste-Julien, and Ioannis Mitliagkas. Stochastic hamiltonian gradient methods for smooth games. In _International Conference on Machine Learning_, pages 6370-6381. PMLR, 2020.
* Luo et al. [2022] Luo Luo, Yujun Li, and Cheng Chen. Finding second-order stationary points in nonconvex-strongly-concave minimax optimization. _Advances in Neural Information Processing Systems_, 35:36667-36679, 2022.
* Mertikopoulos et al. [2018] Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In _International Conference on Learning Representations_, 2018.
* Mescheder et al. [2017] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. _Advances in Neural Information Processing Systems_, 30, 2017.
* Metelev et al. [2022] Dmitriy Metelev, Alexander Rogozin, Alexander Gasnikov, and Dmitry Kovalev. Decentralized saddle-point problems with different constants of strong convexity and strong concavity. _arXiv preprint arXiv:2206.00090_, 2022.
* Mishchenko et al. [2020] Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richtarik, and Yura Malitsky. Revisiting stochastic extragradient. In _International Conference on Artificial Intelligence and Statistics_, pages 4573-4582. PMLR, 2020.
* Mokhtari et al. [2020] Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In _International Conference on Artificial Intelligence and Statistics_, pages 1497-1507. PMLR, 2020.
* Nagarajan and Kolter [2017] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable. _Advances in Neural Information Processing Systems_, 30, 2017.
* Nemirovski [2004] Arkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 15(1):229-251, 2004.
* Nemirovski et al. [2009] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on Optimization_, 19:1574-1609, 2009.
* Nesterov [2004] Yurii Nesterov. _Introductory lectures on convex optimization: A basic course_, volume 87. Springer Science & Business Media, 2004.
* Nesterov and Scrimali [2011] Yurii Nesterov and Laura Scrimali. Solving strongly monotone variational and quasi-variational inequalities. _Discrete & Continuous Dynamical Systems_, 31(4):1383, 2011.
* Ouyang and Xu [2021] Yuyuan Ouyang and Yangyang Xu. Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems. _Mathematical Programming_, 185(1-2):1-35, 2021.
* O'donoghue and Candes [2015] Brendan O'donoghue and Emmanuel Candes. Adaptive restart for accelerated gradient schemes. _Foundations of computational mathematics_, 15(3):715-732, 2015.
* Rafique et al. [2021] Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Weakly-convex-concave min-max optimization: provable algorithms and applications in machine learning. _Optimization Methods and Software_, pages 1-35, 2021.
* Renegar and Grimmer [2022] James Renegar and Benjamin Grimmer. A simple nearly optimal restart scheme for speeding up first-order methods. _Foundations of Computational Mathematics_, 22(1):211-256, 2022.
* Roulet and Aspremont [2017] Vincent Roulet and Alexandre d'Aspremont. Sharpness, restart and acceleration. _Advances in Neural Information Processing Systems_, 30, 2017.
* Ryu et al. [2019] Ernest K Ryu, Kun Yuan, and Wotao Yin. Ode analysis of stochastic gradient methods with optimism and anchoring for minimax problems. _arXiv preprint arXiv:1905.10899_, 2019.
* Raghavan et al. [2019]* Sebbouh et al. (2022) Othmane Sebbouh, Marco Cuturi, and Gabriel Peyre. Randomized stochastic gradient descent ascent. In _International Conference on Artificial Intelligence and Statistics_, pages 2941-2969. PMLR, 2022.
* Thekumparampil et al. (2022) Kiran K Thekumparampil, Niao He, and Sewoong Oh. Lifted primal-dual method for bilinearly coupled smooth minimax optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 4281-4308. PMLR, 2022.
* Tseng (1995) Paul Tseng. On linear convergence of iterative methods for the variational inequality problem. _Journal of Computational and Applied Mathematics_, 60(1-2):237-252, 1995.
* Tseng (2008) Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. _submitted to SIAM Journal on Optimization_, 2(3), 2008.
* Wang and Xiao (2017) Jialei Wang and Lin Xiao. Exploiting strong convexity from data with primal-dual first-order algorithms. In _International Conference on Machine Learning_, pages 3694-3702, 2017.
* Wang and Li (2020) Yuanhao Wang and Jian Li. Improved algorithms for convex-concave minimax optimization. _Advances in Neural Information Processing Systems_, 33:4800-4810, 2020.
* Xiao et al. (2019) Lin Xiao, Adams Wei Yu, Qihang Lin, and Weizhu Chen. Dscovr: Randomized primal-dual block coordinate algorithms for asynchronous distributed optimization. _Journal of Machine Learning Research_, 20(1):1634-1691, 2019.
* Xie et al. (2021) Guangzeng Xie, Yuze Han, and Zhihua Zhang. Dippa: An improved method for bilinear saddle point problems. _arXiv preprint arXiv:2103.08270_, 2021.
* Xu et al. (2018) Pan Xu, Tianhao Wang, and Quanquan Gu. Continuous and discrete-time accelerated stochastic mirror descent for strongly convex functions. In _International Conference on Machine Learning_, pages 5492-5501. PMLR, 2018.
* Yan et al. (2019) Yan Yan, Yi Xu, Qihang Lin, Lijun Zhang, and Tianbao Yang. Stochastic primal-dual algorithms with faster convergence than \(o(1/\sqrt{T})\) for problems without bilinear structure. _arXiv preprint arXiv:1904.10112_, 2019.
* Yan et al. (2020) Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Optimal epoch stochastic gradient descent ascent methods for min-max optimization. _Advances in Neural Information Processing Systems_, 33:5789-5800, 2020.
* Yang et al. (2022) Junchi Yang, Antonio Orvieto, Aurelien Lucchi, and Niao He. Faster single-loop algorithms for minimax optimization without strong concavity. In _International Conference on Artificial Intelligence and Statistics_, pages 5485-5517. PMLR, 2022.
* Zhang et al. (2022) Junyu Zhang, Mingyi Hong, and Shuzhong Zhang. On lower iteration complexity bounds for the convex concave saddle point problems. _Mathematical Programming_, 194(1-2):901-935, 2022.
* Zhang and Xiao (2017) Yuchen Zhang and Lin Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization. _Journal of Machine Learning Research_, 18:1-42, 2017.