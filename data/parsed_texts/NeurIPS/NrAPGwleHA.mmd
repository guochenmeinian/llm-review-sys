# Deep Learning with Physics Priors as Generalized Regularizers

 Frank Liu

School of Data Science

Old Dominion University

Norfolk, VA 23508

United States

&Agniva Chowdhury

Computer Science and Math Division

Oak Ridge National Lab

Oak Ridge, TN 37831

United States

###### Abstract

In various scientific and engineering applications, there is typically an approximate model of the underlying complex system, even though it contains both aleatoric and epistemic uncertainties. In this paper, we present a principled method to incorporate these approximate models as physics priors in modeling, to prevent overfitting and enhancing the generalization capabilities of the trained models. Utilizing the structural risk minimization (SRM) inductive principle pioneered by Vapnik, this approach structures the physics priors into generalized regularizers. The experimental results demonstrate that our method achieves up to two orders of magnitude of improvement in testing accuracy.

## 1 Introduction

Learning from observation data is a crucial task in scientific machine learning (SciML). Deep neural networks have demonstrated to be highly effective in modeling complex systems in scientific research fields such as physics[4; 9; 19], chemistry[13; 31; 1] and biology[12; 5]. To avoid overfitting and to improve generalization performance, regularization techniques such as L1 and L2 regularization[29], weight decay[6], dropout[25], batch normalization[10] and early stopping[16] are commonly deployed during the training of models.

In many scientific machine learning applications, it is quite often that an approximate mechanistic model of the underlying physical phenomenon is available, albeit with uncertainty. For example, the motion in mechanical systems is governed by Newton's Law, and can be mathematically described by Hamiltonian mechanics[2], while the motion of fluid is governed by Navier-Stokes equations[27], and the electric and magnetic fields are described by Maxwell equations[11]. The concept of integrating physics models with more expressive neural network models was initially introduced decades ago[17; 20; 28]. More recent work among this "hybrid-model" or "grey-box" approach include [32; 26; 28; 18; 14; 3; 23; 15; 8; 21]. Among them, a notable work is [32], which proposed a method to integrate simplified or imperfect physics models with deep learning models, by combining the two models as additive right-hand-sides of the differential equations, while with focused applications on the trajectory forecasting of complex systems.

In this paper, we present the analysis that integrating information of a physics model to deep grey-box modeling can be achieved as a generalized regularizer. Recognizing that the simplified or imprecise physics model is subject to both aleatoric (from data) and epistemic (from model) uncertainties, we use Vapnik's structural risk minimization[30] as the inductive principle to cast the generalized regularization as an optimization problem.

#### Problem Description

The observation data are represented by the tuple \(\{(\hat{x}_{j},\hat{y}_{j})\}\) with \(j=1\ldots,N_{d}\). The neural network model is denoted by \(\mathbf{G}_{w}(\cdot)\) with \(w\) representing trainable weights. By applying the L2 norm, the training of the model is equivalent to the empirical risk minimization(ERM) task below:

\[\mathcal{L}_{d}=\frac{1}{N_{d}}\sum_{j}^{N_{d}}(\hat{y}_{j}-\mathbf{G}_{w}(\hat {x}_{j}))^{2}\] (1)

When modeling the dynamic behavior of the physical system defined on \([0,T]\times D\rightarrow\mathbb{R}^{m}\), without loss of generality, we include time as part of the input. Hence \(\hat{x}_{j}\in[0,T]\times D\) and \(\hat{y}_{j}\in\mathbb{R}^{m}\). As in many scientific and engineering applications, we assume \(\hat{y}_{j}\) is subject to noise.

To avoid overfitting, Vapnik introduced the concept of structural risk in the seminal work of [30]. The purpose of including structural risk is to prevent the model from becoming too complex. By penalizing models with large complexity under a given measure of the structural risk (e.g., VC-dimension) in the training process, the structural risk minimization (SRM) ensures that the models would not become too complex. The minimization is often realized on a sequence of nested structures (or hypotheses) with increasing structural risk:

\[\cdots\subset S_{k-1}\subset S_{k}\subset S_{k+1}\subset\cdots\] (2)

with the recognition that more complex models with larger risks produce lower training loss (in the form of empirical risk), but with the increased potential to overfit. A learned model with a balanced trade-off between the empirical risk and structural risk will most likely to achieve good accuracy and generalization performance.

A widely adopted SRM in deep learning is the weight decay[6], where the ERM is augmented by a regularizer, which measures the L2 norm of the weights:

\[\mathcal{L}_{wd}=\frac{1}{N_{d}}\sum_{j}^{N_{d}}(\hat{y}_{j}-\mathbf{G}_{w}( \hat{x}_{j}))^{2}+\lambda\|w\|^{2}\] (3)

with \(\lambda\) as a hyper-parameter controlling the balance between the empirical and structural risk.

The motivation of our work is that in many SciML applications, it is quite common that a physics model is available. However the physics model is only an approximate of the underlying physical phenomenon of the complex system (otherwise we can simply use the mechanistic model itself). The uncertainties of the mechanistic model with respect to the underlying complex system include aleatoric uncertainties and epistemic uncertainties[24]. The former (or "data uncertainty") represents the inherent variability in the data, while the latter (or "model uncertainty") represents the imperfect model, which can due to the missing components in the model, or our lack of understanding of the physical phenomenon.

We propose to utilize the information embedded in the approximate model in model training by structuring the mechanistic model as a generalized regularizer. However unlike common L2-norm regularizer, the parameter \(\lambda\) has stronger dependency on the disparity (or the empirical representation of the epistemic uncertainty) of the physics prior, and should be optimized in a more comprehensive way. Our method provides a different perspective to the deep grey-box modeling approach such as [32]. The introduction of the generalized regularization also opens the door to other interesting means of modeling training, such as the inclusion of multiple mechanistic models as physics priors with multiple regularizers, and the co-optimization of mechanistic model coefficients along with the model itself.

## 2 Physics Prior as Generalized Regularization

We refer to the (approximate) mechanistic model as the physic prior, it has the same support as the observation data \([0,T]\times D\to u\):

\[\mathcal{F}_{\theta}(u)(x)=0\] (4)

where \(u\in\mathbb{R}^{m}\). In the case where the physics prior is an ODE, proper initial condition should be specified \(u(0)=u_{0}\). In the case the physics prior is a PDE, additional boundary condition is needed: \(u(x_{b})=u_{b}\) where \(x_{b}\in\partial D\).

To include physics prior as a regularizer, we add additional collocation points in the support \(\{x_{i}\}\) where \(i=1,\cdots,N_{p}\) and \(x_{i}\in[0,T]\times D\). We introduce a generalized regularization as:

\[\mathcal{L}_{p}=\frac{1}{N_{p}}\sum_{i}^{N_{p}}\left(\mathcal{F}_{\theta}(u)(x_ {i})\right)^{2}=\frac{1}{N_{p}}\sum_{i}^{N_{p}}\left(\mathcal{F}_{\theta}( \mathbf{G}_{w}(x_{i}))^{2}\right.\] (5)

The total loss is the combination of empirical loss in Eqn. 1 and Eqn. 51:

Footnote 1: For notational simplicity, we included the losses from boundary/initial conditions of the priors in \(\mathcal{L}_{d}\).

\[\mathcal{L}\left(\hat{x}_{j},\hat{y}_{j},x_{i};w\right)=\mathcal{L}_{d}+ \lambda\mathcal{L}_{p}=\frac{1}{N_{d}}\sum_{j}^{N_{d}}(\hat{y}_{j}-\mathbf{G} _{w}(\hat{x}_{j}))^{2}+\frac{\lambda}{N_{p}}\sum_{i}^{N_{p}}\left(\mathcal{F}_ {\theta}(\mathbf{G}_{w}(x_{i}))^{2}\right.\] (6)

The loss function in Eqn. 6 can be depicted in the diagram shown in Fig. 0(a).

### Information Injected by the Generalized Regularizer

Following the SRM inductive principle, the balance between the empirical loss in Eqn. 1 and the structural loss in Eqn. 5 is crucial to ensure good accuracy and generalization performance. To illustrate the essence of the structural loss, we assume the underlying complex system can be described by an "oracle" (but completely unknown to us) model, as shown in Eqn. 7. Note here we promiscuously use \(\mathcal{F}_{\tilde{\theta}}(\cdot)\) to represent the oracle model. However in reality it may have no resemblance to the approximate model \(\mathcal{F}_{\theta}\) at all.

\[\mathcal{F}_{\tilde{\theta}}(u)(x)=0\quad\forall x\in[0,T]\times D\] (7)

We assume the model trained on the data \(\{\hat{x}_{j},\hat{y}_{j}\}\):

\[w^{*}=\operatorname*{argmin}_{w}\mathcal{L}_{d}\] (8)

is the perfect representation of the oracle model \(\mathcal{F}_{\tilde{\theta}}\). Hence

\[\mathcal{F}_{\tilde{\theta}}\left(\mathbf{G}_{w^{*}}(x)\right)=0\quad\forall x \in[0,T]\times D\] (9)

To quantify the structural loss shown in Eqn. 5, we have:

\[\mathcal{F}_{\theta}\left(\mathbf{G}_{w^{*}}(x)\right) =\mathcal{F}_{\theta}\left(\mathbf{G}_{w^{*}}(x)\right)-\mathcal{ F}_{\tilde{\theta}}\left(\mathbf{G}_{w^{*}}(x)\right)\] (10) \[=\left(\mathcal{F}_{\theta}(\cdot)-\mathcal{F}_{\tilde{\theta}}( \cdot)\right)\left(\mathbf{G}_{w^{*}}(x)\right)\quad\forall x\in[0,T]\times D\] (11)

Observe that \(\mathcal{F}_{\theta}(\cdot)-\mathcal{F}_{\tilde{\theta}}(\cdot)\) is the representation of the epistemic uncertainty of our physics prior \(\mathcal{F}_{\theta}\) with respect to the oracle model of the underlying complex system, hence effectively the function space in Eqn. 11 is a projection of the epistemic uncertainty onto the function space of the trained model \(\mathbf{G}_{w}\). Furthermore the regularizer presented in Eqn. 5 is its empirical version, sampled at the collocation points.

When the physics prior has no epistemic uncertainty, theoretically we have \(\mathcal{F}_{\theta}(\cdot)\equiv\mathcal{F}_{\tilde{\theta}}(\cdot)\). Since there still exist aleatoric uncertainties in the observation data, the structural risk term in Eqn. 5 will approach its minimum, but will not automatically become zero. Under this condition, the maximal value of the regularization parameter \(\lambda\) will inject most information to the loss function.

Figure 1: Diagram of model with generalized regularization.

The immediate consequence of the above observation is that because we usually don't have a concrete metric on the epistemic uncertainty of the physics prior \(\mathcal{F}_{\theta}\), we don't have a good measure on the amount of information our generalized regularizer in Eqn. 5 contains. Hence it is necessary to optimize both the weights and the parameter \(\lambda\):

\[w^{*}=\operatorname*{argmin}_{w,\lambda}\mathcal{L}=\operatorname*{argmin}_{w, \lambda}\mathcal{L}_{d}+\lambda\mathcal{L}_{p}\] (12)

In this study, we perform the optimization by two nested loops:

\[w^{*}=\operatorname*{argmin}_{\lambda}\operatorname*{argmin}_{w}\mathcal{L}_{ d}+\lambda\mathcal{L}_{p}\] (13)

Although the optimization of the model weights \(w\) and parameter \(\lambda\) outlined in Eqn. 12 and Eqn. 13 appears similar to the optimization of more traditional regularization techniques, such as weight decay in Eqn. 3, we'd like to point out that the generalized regularization place a more strict structural on the solution space of the trained model, hence it is much more cognizant to the behavior of the underlying complex system. This is clearly demonstrated in the experimental results later in the paper.

### Regularization with Multiple Physics Priors

An immediate extension following the discussion above is that we can introduce multiple physics priors as generalized regularizers. As the simplest format, these physics priors can be based on the same family of the approximate models, but with different coefficients. The diagram with two physics priors is shown in Fig. 0(b). Mathematically the loss function becomes:

\[\mathcal{L} =\mathcal{L}_{d}+\lambda_{1}\mathcal{L}_{p_{1}}+\lambda_{2} \mathcal{L}_{p_{2}}\] (14) \[=\underbrace{\frac{1}{N_{d}}\sum_{j}^{N_{d}}(\hat{y}_{j}-\mathbf{ G}_{w}(\hat{x}_{j}))}_{\mathcal{L}_{d}}+\underbrace{\frac{\lambda_{1}}{N_{p_{1}}} \sum_{i}^{N_{p_{1}}}(\mathcal{F}_{\theta_{1}}(\mathbf{G}_{w}(x_{i}))^{2}}_{ \lambda_{1}\mathcal{L}_{p_{1}}}+\underbrace{\frac{\lambda_{2}}{N_{p_{2}}}\sum_{ k}^{N_{p_{2}}}\left(\mathcal{F}_{\theta_{2}}(\mathbf{G}_{w}(x_{k}))^{2}\right)}_{ \lambda_{2}\mathcal{L}_{p_{2}}}\] (15)

Again we promiscuously denote two physics priors as \(\mathcal{F}_{\theta_{1}}(\cdot)\) and \(\mathcal{F}_{\theta_{2}}(\cdot)\), even though they could be based on completely different families of functions or different families of differential equations. The learning process involves both the regularization parameters and model weights: In this study, we use two nested loops for the optimization:

\[w^{*}=\operatorname*{argmin}_{\lambda_{1},\lambda_{2}}\operatorname*{argmin}_{ w}\mathcal{L}_{d}+\lambda_{1}\mathcal{L}_{p_{2}}+\lambda_{2}\mathcal{L}_{p_{2}}\] (16)

Here we make the inductive assumption that the projections of the epistemic uncertainties of the two physics priors should be summed algebraically to provide structural risk minimization. Since the structural risk terms are non-negative, the overall outcome of the risk minimization can be interpreted as a data-driven approach to "select" which physics prior should have a stronger influence on the overall structural risk.

### Inclusion of Physics Prior Coefficients

Our generalized regularization method can be further extended to include the coefficients (all or a pre-selected subset) of the physics priors as part of the parameters. More specifically the learning task becomes:

\[w^{*} =\operatorname*{argmin}_{\lambda,\theta}\operatorname*{argmin}_{w }\mathcal{L}_{d}+\lambda\mathcal{L}_{p}\] (17) \[=\operatorname*{argmin}_{\lambda,\theta}\operatorname*{argmin}_{w }\Bigl{[}\frac{1}{N_{d}}\sum_{j}^{N_{d}}(\hat{y}_{j}-\mathbf{G}_{w}(\hat{x}_ {j}))^{2}+\frac{\lambda}{N_{p}}\sum_{i}^{N_{p}}\left(\mathcal{F}_{\theta}( \mathbf{G}_{w}(x_{i}))^{2}\right]\] (18)

This learning task can be interpreted as simultaneously adjusting the structure of the regularization by optimizing the physics priors represented by \(\theta\) and the "strength" of the structural risk by optimizing \(\lambda\). The byproduct of the optimization in Eqn. 18, \(\theta^{*}\), from:

\[\theta^{*}=\operatorname*{argmin}_{\lambda,\theta}\operatorname*{argmin}_{w} \mathcal{L}_{d}+\lambda\mathcal{L}_{p}\] (19)can be interpreted as the physics prior with the smallest epistemic uncertainty, as the projection to the function space of the trained model.

## 3 Experimental Results

In this section, we present experimental results based on our method. All experiments are written in Pytorch. The training and evaluation are conducted on an Nvidia DGX-2 server with A100 GPUs. More technical details are included in the supplemental materials section.

### Implementation in Hamiltonian Neural Networks

In the first example, we forked the public repo of the Hamiltonian Neural Network(HNN)[7] and implemented generalized regularizer. The elegant utilization of Hamiltonian mechanics in HNN makes it straight-forward to implement generalized regularizer based on physics priors. For each case, the physics prior is parameterized by another Hamiltonian \(\mathcal{H}_{\theta}\), where \(\theta\) represents the parameters of physics models such as mass, length of the pendulum. We introduce the generalized regularization by:

\[\mathcal{L}_{reg}=\lambda\cdot\left\|\left(\frac{\partial\mathcal{H}_{w}}{ \partial\mathbf{p}}-\frac{\partial\mathcal{H}_{\theta}}{\partial\mathbf{p}} \right)+\left(\frac{\partial\mathcal{H}_{w}}{\partial\mathbf{q}}-\frac{ \partial\mathcal{H}_{\theta}}{\partial\mathbf{q}}\right)\right\|_{2}\] (20)

while the original HNN loss is computed by:

\[\mathcal{L}_{HNN}=\left\|\frac{\partial\mathcal{H}_{w}}{\partial\mathbf{p}}- \frac{\partial\mathbf{q}}{\partial t}\right\|_{2}+\left\|-\frac{\partial \mathcal{H}_{w}}{\partial\mathbf{q}}-\frac{\mathbf{q}}{\partial t}\right\|_{2}\] (21)

where \(w\) denotes the trainable weights of the HNN model.

Results of three cases are presented in Tab. 1: mass-spring, ideal pendulum and real pendulum. Improvements are illustrated in Fig. 2. In the last example, the training data are collected from measurements of physical pendulum, which is subject to sensor noise, as well as epistemic uncertainties such as frictions[22]. In all three cases, the generalized regularization demonstrated clear improvements in both baseline model and HNN model. We want to point out that for ideal pendulum, generalized regularization further improves the energy by \(10\times\), in addition to \(2\times\) improvement of HNN. In the case of real pendulum, in which a precise physics model is unknown, HNN demonstrated an impressive \(30\times\) improvement in terms of energy (from \(376.9\) to \(11.2\)), while the introduction of physics prior can further improve the energy metric to \(9.5\), an additional \(15\%\) improvement.

Figure 2: Performances of Hamiltonian NN with and without physics regularizers. Generalized regularization implementation is based on the open-source repo of Greydanus et. al. “Hamiltonian Neural Networks”.

### 1D Reaction Equation

We study the one dimensional reaction equation that is commonly used to model chemical reactions. It's the simplification of the reaction-diffusion equations and is given by,

\[\frac{\partial u}{\partial t}-\rho u(1-u)=0\,,\] (22)

with the associated initial and (periodic) boundary conditions \(u(x,0)=g(x),\ x\in D\) and \(u(0,t)=u(2\pi,t),\ t\in(0,T]\) respectively, with \(g(x)=\exp\big{(}-\frac{(x-\pi)^{2}}{2(\pi/4)^{2}}\big{)}\), with \(\rho\) being the reaction coefficient. We generate two datasets by using two oracle reaction equations, shown in Fig. 2(a) and 2(d) respectively. Fig. 2(a) also shows results of learned models with different physics priors, all are different from the oracle models.

As a comparison, we also train the baseline models for the two reaction equation cases from observation data only, with and without using weight decay as regularization. The results are shown in Fig. 3. Quantitative MSE from testing are tabulated in Tab. 2. Clearly our method outperforms weigh decay in all but one case.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Test Loss & \multicolumn{3}{c}{Energy} \\ Task & Baseline & HNN & Baseline & HNN \\ \hline Mass spring (original) & \(36.73\pm 1.86\) & \(35.91\pm 1.83\) & \(\mathbf{147.01\pm 19.30}\) & \(0.376\pm 0.077\) \\ Mass spring (w/ reg.) & \(\mathbf{36.55\pm 1.85}\) & \(\mathbf{35.90\pm 1.83}\) & \(167.91\pm 20.50\) & \(\mathbf{0.364\pm 0.084}\) \\ \hline Ideal Pendulum (original) & \(35.32\pm 1.80\) & \(35.59\pm 1.82\) & \(41.83\pm 9.75\) & \(24.85\pm 5.42\) \\ Ideal Pendulum (w/ reg.) & \(\mathbf{35.11\pm 1.78}\) & \(\mathbf{34.56\pm 1.74}\) & \(\mathbf{34.56\pm 8.55}\) & \(\mathbf{2.29\pm 0.47}\) \\ \hline Real Pendulum (original) & \(1.50\pm 0.23\) & \(5.80\pm 0.60\) & \(376.89\pm 72.50\) & \(11.22\pm 3.87\) \\ Real Pendulum (w/ reg.) & \(\mathbf{1.39\pm 0.20}\) & \(\mathbf{5.79\pm 0.62}\) & \(\mathbf{371.50\pm 74.30}\) & \(\mathbf{9.46\pm 3.70}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantitative results of three tasks in Table 1 in Geydanus et. al. “Hamiltonian Neural Networks”. All values are multiplied by \(10^{3}\). Implementation is based on the Github repo by Geydanus, although some values of the original models are slightly different from values reported in the original paper. **Bold** entries indicate the best results.

Figure 2: Two cases of 1D reaction equation, including the ground truth and models with different physics priors. The crosses indicate the collocation points of training data.

[MISSING_PAGE_EMPTY:7]

Finally we use case 1 of the convection equation experiment for demonstration of multiple physics priors and optimization of physics prior coefficients, shown in Fig. 6. In the first experiment, two physics priors are specified with \(\beta=25\) and \(\beta=35\). The optimal regularization parameters are \(\lambda_{1}^{*}=3.72{\times}10^{-7}\) and \(\lambda_{2}^{*}=3.47{\times}10^{-3}\) with testing MSE of \(9.53{\times}10^{-3}\). In the second experiment, the final physics prior coefficient is \(\beta^{*}=29.67\). With an optimal \(\lambda^{*}=1.25{\times}10^{-1}\), it achieves the testing MSE of \(5.04{\times}10^{-3}\).

## 4 Conclusion and Future Work

In this paper we propose a principled method to incorporate the prior knowledge of an underlying complex system, in the form of approximate physics models, into the data-driven deep grey-box modeling. By structuring the imprecise physics models, or physics priors, as generalized regularizers, we apply Vapnik's structural risk minimization (SRM) inductive principle to balance the model accuracy and model complexity. Our analysis indicates that the information in the physics priors is bounded by the uncertainty, especially the epistemic uncertainty, of the physics priors. Experimental results have shown that our method is highly effective in improving the test accuracy. For future work, we plan to investigate the theoretical and practical implications when multiple physics priors are included in the regularization.

## Acknowledgement

This research was supported by the U.S. Department of Energy, through the Office of Advanced Scientific Computing Research's "Data-Driven Decision Control for Complex Systems (DnC2S)" project, FWP ERKJ368. This manuscript has been authored in part by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to the results of federally sponsored research in accordance with the DOE Public Access Plan (https://energy.gov/downloads/doe-public-access-plan).

\begin{table}
\begin{tabular}{|c c c c c|c|} \hline \hline  & prior used & baseline & baseline & \(\lambda=\lambda_{opt}\) & \\  & & w/o reg. & w/ weight decay & & & \\ \hline case 1 & \(\beta=25\) & \(7.85{\times}10^{-2}\) & \(4.33{\times}10^{-2}\) & \(\mathbf{1.29{\times}10^{-2}}\) & \(\lambda_{opt}=8.42{\times}10^{-2}\) \\  & \(\beta=35\) & & & \(\mathbf{1.16{\times}10^{-2}}\) & \(\lambda_{opt}=1.16{\times}10^{-2}\) \\ \hline case 2 & \(\beta=45\) & \(6.37{\times}10^{-1}\) & \(5.11{\times}10^{-1}\) & \(\mathbf{1.57{\times}10^{-2}}\) & \(\lambda_{opt}=2.42{\times}10^{-2}\) \\  & \(\beta=55\) & & & \(\mathbf{1.16{\times}10^{-2}}\) & \(\lambda_{opt}=3.58{\times}10^{-2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Testing MSE for 1D Convection Equation and associated optimal \(\lambda\)

Figure 6: Performance heatmap of case 1 of convection equation

## References

* [1] Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church. Unified rational protein engineering with sequence-based deep representation learning. _Nature methods_, 16(12):1315-1322, 2019.
* [2] Vladimir Igorevich Arnol'd. _Mathematical methods of classical mechanics_, volume 60. Springer Science & Business Media, 2013.
* [3] Filipe De Avila Belbute-Peres, Thomas Economon, and Zico Kolter. Combining differentiable pde solvers and graph neural networks for fluid flow prediction. In _international conference on machine learning_, pages 2402-2411. PMLR, 2020.
* [4] Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie Vogt-Maranto, and Lenka Zdeborova. Machine learning and the physical sciences. _Reviews of Modern Physics_, 91(4):045002, 2019.
* [5] Yifei Chen, Yi Li, Rajiv Narayan, Aravind Subramanian, and Xiaohui Xie. Gene expression inference with deep learning. _Bioinformatics_, 32(12):1832-1839, 2016.
* [6] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep learning_. MIT press, 2016.
* [7] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. _Advances in neural information processing systems_, 32, 2019.
* [8] Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for unsupervised video prediction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11474-11484, 2020.
* [9] Dan Guest, Kyle Cranmer, and Daniel Whiteson. Deep learning and its application to lhc physics. _Annual Review of Nuclear and Particle Science_, 68:161-181, 2018.
* [10] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456. pmlr, 2015.
* [11] John David Jackson. Classical electrodynamics, 1999.
* [12] Mohammad Behdad Jamshidi, Ali Lalbakhsh, Jakub Talla, Zdenek Peroutka, Sobhan Roshani, Vaclav Matousek, Saeed Roshani, Mirhamed Mirmozafari, Zahra Malek, Luigi La Spada, et al. Deep learning techniques and covid-19 drug discovery: Fundamentals, state-of-the-art and future directions. _Emerging Technologies During the Era of COVID-19 Pandemic_, pages 9-31, 2021.
* [13] John A Keith, Valentin Vassilev-Galindo, Bingqing Cheng, Stefan Chmiela, Michael Gastegger, Klaus-Robert Muller, and Alexandre Tkatchenko. Combining machine learning and computational chemistry for predictive insights into chemical systems. _Chemical reviews_, 121(16):9816-9872, 2021.
* [14] Li Li, Stephan Hoyer, Ryan Pederson, Ruoxi Sun, Ekin D Cubuk, Patrick Riley, Kieron Burke, et al. Kohn-sham equations as regularizer: Building prior knowledge into machine-learned physics. _Physical review letters_, 126(3):036401, 2021.
* [15] Nikhil Muralidhar, Jie Bu, Ze Cao, Long He, Naren Ramakrishnan, Danesh Tafti, and Anuj Karpatne. Phynet: Physics guided neural networks for particle drag force prediction in assembly. In _Proceedings of the 2020 SIAM International Conference on Data Mining_, pages 559-567. SIAM, 2020.
* [16] Lutz Prechelt. Automatic early stopping using cross validation: quantifying the criteria. _Neural networks_, 11(4):761-767, 1998.
* [17] Dimitris C Psichogios and Lyle H Ungar. A hybrid neural network-first principles approach to process modeling. _AIChE Journal_, 38(10):1499-1511, 1992.

* [18] Zhaozhi Qian, William Zame, Lucas Fleuren, Paul Elbers, and Mihaela van der Schaar. Integrating expert codes into neural odes: pharmacology and disease progression. _Advances in Neural Information Processing Systems_, 34:11364-11383, 2021.
* [19] Alexander Radovic, Mike Williams, David Rousseau, Michael Kagan, Daniele Bonacorsi, Alexander Himmel, Adam Aurisano, Kazuhiro Terao, and Taritree Wongjirad. Machine learning at the energy and intensity frontiers of particle physics. _Nature_, 560(7716):41-48, 2018.
* [20] R Rico-Martinez, JS Anderson, and IG Kevrekidis. Continuous-time nonlinear signal processing: a neural network based approach for gray box identification. In _Proceedings of IEEE Workshop on Neural Networks for Signal Processing_, pages 596-605. IEEE, 1994.
* [21] Manuel A Roehrl, Thomas A Runkler, Veronika Brandtstetter, Michel Tokic, and Stefan Obermayer. Modeling system dynamics with physics-informed neural networks based on lagrangian mechanics. _IFAC-PapersOnLine_, 53(2):9195-9200, 2020.
* [22] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. _science_, 324(5923):81-85, 2009.
* [23] Ushnish Sengupta, Matt Amos, Scott Hosking, Carl Edward Rasmussen, Matthew Juniper, and Paul Young. Ensembling geophysical models with bayesian neural networks. _Advances in Neural Information Processing Systems_, 33:1205-1217, 2020.
* [24] Ralph C Smith. _Uncertainty quantification: theory, implementation, and applications_, volume 12. Siam, 2013.
* [25] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. _The journal of machine learning research_, 15(1):1929-1958, 2014.
* [26] Naoya Takeishi and Alexandros Kalousis. Physics-integrated variational autoencoders for robust and interpretable generative modeling. _Advances in Neural Information Processing Systems_, 34:14809-14821, 2021.
* [27] Roger Temam. _Navier-Stokes equations: theory and numerical analysis_, volume 343. American Mathematical Soc., 2001.
* [28] Michael L Thompson and Mark A Kramer. Modeling chemical processes using prior knowledge and neural networks. _AIChE Journal_, 40(8):1328-1340, 1994.
* [29] Robert Tibshirani. Regression shrinkage and selection via the lasso. _Journal of the Royal Statistical Society: Series B (Methodological)_, 58(1):267-288, 1996.
* [30] Vladimir Vapnik. Principles of risk minimization for learning theory. _Advances in neural information processing systems_, 4, 1991.
* [31] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molecular representations for property prediction. _Journal of chemical information and modeling_, 59(8):3370-3388, 2019.
* [32] Yuan Yin, LE Vincent, Dona Jeremie, Emmanuel de Bezenac, Ibrahim Ayed, Nicolas Thome, et al. Augmenting physical models with deep networks for complex dynamics forecasting. In _International Conference on Learning Representations_, 2020.

## Appendix S1 Inclusion of Initial and Boundary Conditions in the Loss Function

When the initial and/or boundary conditions are available for the underlying dynamic system, they should be included in the loss function, namely \(\mathcal{L}_{d}\) in Eqn. (6) of the main text. More specifically the component due to the initial conditions is:

\[\mathcal{L}_{I}=\frac{1}{N_{I}}\sum_{j}^{N_{I}}\big{(}\hat{y}_{j}^{I}-\mathbf{G }_{w}\big{(}\big{[}\hat{x}_{j}^{I},0\big{]}\big{)}\big{)}^{2}\] (1)

and the component due to the boundary condition is:

\[\mathcal{L}_{b}=\frac{1}{N_{b}}\sum_{j}^{N_{b}}\big{(}\mathbf{G}_{w}\big{(} \big{[}0,\hat{t}_{j}^{b}\big{]}\big{)}-\mathbf{G}_{w}\big{(}\big{[}2\pi,\hat{t }_{j}^{b}\big{]}\big{)}\big{)}^{2}\] (2)

Here we explicitly separate temporal and spatial component in the input \(\hat{x}_{j}\). These terms should be combined with the \(\mathcal{L}_{d}\) term in the overall loss function.

## Appendix S2 Experiment Setup

All experiments are conducted on an Nvidia A100 GPU with 40GB of memory. The DL framework used is PyTorch 1.10.1. The optimization is conducted using Bayesian Optimization routine in Scikit-Optimize version 0.9.0. In both reaction and convection cases, the DNN model is an MLP with 5 hidden layers, each with a width of 512, and tanh as the activation function. Adam is used as the optimizer, with initial learning rate set to LR\(=2\times 10^{-4}\).

## Appendix S3 Details and Additional Results on 1D Reaction Equation

### Details of Experiment Setup

The reaction equation is specified as:

\[\frac{\partial u}{\partial t}-\rho u(1-u)=0\] (3)

We generate data points using "oracle" models which are also reaction equations. For each case, we consider a mesh which consists of 100 time points between \(T=[0,0.5]\), with 256 spatial points at each time point. This results in a total of 25,600 grid points. All 256 spatial points for \(T=0\) are included in the loss function term related to the initial condition in Eqn. 1. Therefore, we have \(([\hat{x}_{j}^{I},0],\hat{y}_{j}^{I})\), where \(\hat{y}_{j}^{I}=u(\hat{x}_{j}^{I},0)\) for \(j=1,2,\ldots,N_{I}\), and \(N_{I}=256\). Similarly, we compute

[MISSING_PAGE_FAIL:12]

Figure S3: Heatmap of predicted solutions for Case 2 of 1D reaction with \(\rho=15\) as the physics prior. Top row: (a) Exact solution, (b) Baseline solution (no physics prior, no weight decay), (c) Baseline with weight decay, and (d) Solution obtained after tuning \(\lambda\) using BO with physics prior \(\rho=15\). Bottom row: (e)-(h) Solutions for each pre-specified values of \(\lambda\) with physics prior \(\rho=15\).

[MISSING_PAGE_EMPTY:15]

[MISSING_PAGE_FAIL:16]

Figure S8: Heatmap of predicted solutions for case 2 of 1D convection with \(\beta=45\) as the physics prior. Top row: (a) Exact solution, (b) Baseline solution (no physics prior), (c) Weight decay, and (d) Solution obtained after tuning \(\lambda\) using BO with physics prior \(\beta=45\). Bottom row: (e)-(h) Solutions for each pre-specified values of \(\lambda\) with physics prior \(\beta=45\).

## S5 Details and Additional Results on 1D Reaction-Diffusion Equation

### Details of Experiment Setup

The reaction-diffusion equation is specified as:

\[\frac{\partial u}{\partial t}-\nu\frac{\partial^{2}u}{\partial x^{2}}-\rho u(1-u )=0\] (5)with the associated initial and (periodic) boundary conditions \(u(x,0)=g(x),\ x\in D\) and \(u(0,t)=u(2\pi,t),\ t\in(0,T]\) respectively, with \(g(x)=\exp\big{(}-\frac{(x-\pi)^{2}}{2(\pi/4)^{2}}\big{)}\), with \(\rho\) and \(\nu\) being the reaction and diffusion coefficients respectively. Here, we fix \(\nu=3\) for all the experiments. We generate two datasets by using two oracle reaction-diffusion equations, shown in Fig. S11a and S11d respectively. Fig. S11 also shows results of learned models with different physics priors, all are different from the oracle models.

As a comparison, we also train the baseline models for the two reaction-diffusion equation cases from observation data only, with/without using weight decay as regularization. The results are shown in Fig. S12. Quantitative MSE from testing are tabulated in Tab. S7. Clearly our method outperforms weigh decay in all but one case.

**Data generation.** We generate data points using "oracle" models which are also reaction equations. For each case, we consider a mesh which consists of 100 time points between \(T=[0,0.5]\), with 256 spatial points at each time point. This results in a total of 25,600 grid points. All 256 spatial points for \(T=0\) are included in the loss function term related to the initial condition in Eqn. 1. Therefore, we have \(([\hat{x}_{I}^{j},0],\hat{y}_{I}^{j})\), where \(\hat{y}_{I}^{j}=u(\hat{x}_{I}^{j},0)\) for \(j=1,2,\ldots,N_{I}\), and \(N_{I}=256\). Similarly, we compute the boundary loss for the periodic boundary condition using Eqn. (2) for the boundary time points \([0,\hat{t}_{1}^{b}],[0,\hat{t}_{2}^{b}],\ldots,[0,\hat{t}_{N_{b}}^{b}]\), with \(N_{b}=100\). Additional \(50\) data points are randomly chosen to be included in the loss function \(\mathcal{L}_{d}\), with \(\mathcal{N}(0,0.1)\) noise added to the them. Two "oracle" models are considered:

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_EMPTY:21]