# Efficient Federated Learning against Heterogeneous

and Non-stationary Client Unavailability

 Ming Xiang\({}^{1}\)  Stratis Ioannidis\({}^{1}\)  Edmund Yeh\({}^{1}\)  Carlee Joe-Wong\({}^{2}\)  Lili Su\({}^{1}\)

\({}^{1}\)Northeastern University, Boston, MA \({}^{2}\)Carnegie Mellon University, Pittsburgh, PA

{xiang.mi,l.su}@northeastern.edu

{ioannidis,eyeh}@ece.neu.edu

cjoewong@andrew.cmu.edu

###### Abstract

Addressing intermittent client availability is critical for the real-world deployment of federated learning algorithms. Most prior work either overlooks the potential non-stationarity in the dynamics of client unavailability or requires substantial memory/computation overhead. We study federated learning in the presence of heterogeneous and non-stationary client availability, which may occur when the deployment environments are uncertain, or the clients are mobile. The impacts of heterogeneity and non-stationarity on client unavailability can be significant, as we illustrate using FedAvg, the most widely adopted federated learning algorithm. We propose FedAWE, which includes novel algorithmic structures that (i) compensate for missed computations due to unavailability with only \(O(1)\) additional memory and computation with respect to standard FedAvg, and (ii) evenly diffuse local updates within the federated learning system through implicit gossiping, despite being agnostic to non-stationary dynamics. We show that FedAWE converges to a stationary point of even non-convex objectives while achieving the desired linear speedup property. We corroborate our analysis with numerical experiments over diversified client unavailability dynamics on real-world data sets.

## 1 Introduction

Federated learning is a distributed machine learning approach that enables training global models without disclosing raw local data [32; 21]. It has been adopted in commercial applications such as autonomous vehicles [6; 70; 41], the Internet of things [39], and natural language processing [63; 43].

Heterogeneous data and massive client populations are two of the defining characteristics of cross-device federated learning systems [32; 21]. Despite intensive efforts [32; 29; 68; 45; 21], several key challenges that arise from the involvement of large-scale client populations are often overlooked in the existing literature [42]. One of the primary hurdles is the issue of client unavailability. Intuitively, more active clients drive the global model to their local optima by overfitting their local data, which biases the training. In addition, the higher the uncertainty in client unavailability, the larger the performance degradation. Concrete examples that confirm these intuitions in the context of FedAvg - the most widely adopted federated learning algorithm - can be found in Section 4. Client unavailability issues can arise from internal factors such as different working schedules and heterogeneous hardware/software constraints. External factors, such as poor network coverage and frequent handovers of base stations due to fast movements, only exacerbate these problems [50; 57; 64; 3; 21]. The intricate interplay of internal and external factors results in the _non-stationarity_ and _heterogeneity_ of client unavailability.

Most prior work either assumes exact knowledge of the clients' available dynamics or requires their dynamics to be benignly stationary [32; 27; 42; 54; 55; 11]. A related line of work studies asynchronous federated learning wherein clients are vulnerable to delays in message transmission and the reported model updates may be stale [59; 38; 49; 25]. The proposed methods therein assume the availability of all clients or uniformly sampled clients, making them inapplicable to our settings. A few recent works [44; 58] study non-stationary dynamics. Ribero et al. [44] consider the settings where the available probabilities follow a homogeneous Markov chain. Xiang et al. [58] require that clients be capable of continuous local optimization regardless of communication failures. A handful of other works [14; 60] memorize the old gradients of the unavailable clients to compensate for their unavailability. However, the added memory burdens the federated learning system with substantial memory proportional to the product of the number of clients and the model dimension.

**Contributions**. In this work, we focus on stochastic client unavailability, where client \(i\) is available for federated learning model training with probability \(p_{i}^{t}\) at any time \(t\). An illustration can be found in Fig. 1. Our contributions are four-fold:

* In Section 4, via constructing concrete examples, we demonstrate that both heterogeneity and non-stationarity of \(p_{i}^{t}\) will result in bias and thus significant performance degradation of FedAvg.
* In Section 5, we propose an algorithm named FedAVE, which features computational and memory efficiency: only \(O(1)\) additional computation and memory per client will be used when compared with FedAvg. The design of FedAWE introduces two novel algorithmic structures: _adaptive innovation echoing_ and _implicit gossiping_. At a high level, these novel algorithmic structures (i) help clients catch up on the missed computation, and (ii) simultaneously enable a balanced information mixture through implicit client-client gossip, which ultimately corrects the remaining bias. Notably, no direct neighbor information exchanges are used, and the client unavailability dynamics remains unknown to all clients and the parameter server.
* In Section 6, we show that FedAWE converges to a stationary point of even non-convex global objective and achieves the linear speedup property without conditions on second-order partial derivatives of the loss function in analysis.
* In Section 7, we validate our analysis with numerical experiments over diversified client unavailability dynamics on real-world data sets.

## 2 Related Work

**Dynamical client availability.** There is a recent surge of efforts to study time-varying client availability [45; 44; 7; 54; 44; 42; 58; 11], which can be roughly classified into two categories depending on whether the parameter server can unilaterally determine the participating clients.

(i) _Controllable participation._ Earlier research [32; 29] presumes that, in each round, the parameter server could select a small set of clients either uniformly at random or in proportion to the volume of local data held by clients. More recently, Cho et al. [10] design adaptive and non-uniform client sampling to accelerate learning convergence, albeit at the cost of introducing a non-zero residual error. In another work, Cho et al. [8] study the convergence of FedAvg with cyclic client participation. Yet, the set of available clients is sampled uniformly at random per cyclic round and is decided unilaterally by the parameter server. Perazzone et al. [42] consider heterogeneous and time-varying response rates \(p_{i}^{t}\) under the assumptions that \(p_{i}^{t}\) is known a priori and that the stochastic gradients are bounded in expectation. Furthermore, the dynamics of \(p_{i}^{t}\) are determined by the parameter server by solving a stochastic optimization problem. Chen et al. [7] propose a client sampling scheme wherein only the clients with the most "important" updates communicate back to the parameter server. This sampling method can achieve performance comparable to that of full client participation, provided that \(p_{i}^{t}\) is globally known to both the parameter server and the clients. Departing from this line of literature, our setup neither assumes any side information or prior knowledge of the response rates \(p_{i}^{t}\) nor assumes that the parameter server has any influence on \(p_{i}^{t}\).

(ii) _Uncontrollable participation._ There is a handful of work on building resilience against arbitrary client availability [44; 54; 60; 14; 62; 55]. Ribero et al. [44] consider random client availability whose underlying response rates are also heterogeneous and time-varying with unknown dynamics.

Figure 1: Client \(i\)’s available probabilities \(p_{i}^{t}\)’s are heterogeneous and are subject to _non-stationary_ dynamics.

However, the underlying dynamics of \(p_{i}^{t}\) in [44] are assumed to follow a homogeneous Markov chain. Wang and Ji [54] propose a generalized FedAvg that amplifies parameter updates every \(P\) rounds for some carefully tuned \(P\). Despite its elegant and unified analysis and potential to accommodate non-independent unavailability dynamics, to reach a stationary point, \(p_{i}^{t}\) needs to satisfy some assumptions to ensure roughly equal availability of all clients over every \(P\) round. Sharing a similar spirit, Crawshaw and Liu [11] propose a SCAFFOLD variant that amplifies global parameter and local gradient updates every \(P\) round. In spite of its communication efficiency and resilience to data heterogeneity, the rolling average of \(p_{i}^{t}\) over every \(P\) round is assumed to be the same constant for all clients. Yang et al. [62] analyze a setting where clients participate in the training at their will. Yet, their convergence is shown to be up to a non-zero residual error. The algorithms proposed in [14, 60] share the same idea of using the memorized latest updates from unavailable clients for global aggregation. Despite superior numerical performance, both algorithms demand a substantial amount of additional memory [55]. For non-convex objectives, both [60] and [14] require an absolute bounded inactive period, and share similar technical assumptions such as almost surely bounded stochastic gradients [60] or Lipschitz Hessian [14]. Though bounded inactive periods are relevant for applications wherein the sensors wake up on a periodic schedule, this assumption is not satisfied even for the simple stochastic setting when clients are selected uniformly at random. Wang and Ji consider unknown heterogeneous \(p_{i}\)' in a concurrent work [55]; however, \(p_{i}\)'s are assumed to be fixed over time.

**Asynchronous federated learning.** Another related line of work is asynchronous federated learning. To the best of our knowledge, Xie et al. [59] initialize the study of asynchronous federated learning, wherein the parameter server revises the global model every time it receives an update from a client. Convergence is shown under some technical assumptions such as weakly-convex global objectives, bounded delay, and bounded stochastic gradients. Zakerinia et al. [69] propose QuAFL which is shown to be resilient to computation asynchronicity and quantized communication yet under the bounded and stationary delay assumption. Nguyen et al. [38] propose FedBuff, which uses additional memory to buffer asynchronous aggregation to achieve scalability and privacy. Convergence is shown under bounded gradients and bounded staleness assumptions. In fact, most convergence guarantees in the asynchronous federated learning literature rely on bounded staleness [59, 38, 49, 25], or bounded gradients [59, 38, 25]. Recently, arbitrary delay is considered in the context of distributed SGD with bounded stochastic gradients and \((0,\zeta)\)-bounded inter-client heterogeneity [33] (see Assumption 4 for the definition). The convergence suffers from a non-zero residual term \(O(\zeta^{2})\). In contrast, our convergence guarantee is free from non-zero residual terms and does not require gradients to be bounded.

## 3 Problem Formulation

A federated learning system consists of a parameter server and \(m\) clients that collaboratively minimize

\[\min_{\bm{x}\in\mathbb{R}^{d}}F(\bm{x})\triangleq\frac{1}{m}\sum_{i=1}^{m}F_{ i}(\bm{x}),\] (1)

where \(F_{i}(\bm{x})\triangleq\mathbb{E}_{\xi_{i}\sim\mathcal{D}_{i}}\left[\ell_{i}( \bm{x};\xi_{i})\right]\) is the local objective and can be non-convex, \(\mathcal{D}_{i}\) is the local distribution, \(\xi_{i}\) is a stochastic sample that client \(i\) has access to, \(\ell_{i}\) is the local loss function, and \(d\) is the model dimension.

We use Assumption 1 to capture the uncertain _non-stationary_ dynamics and heterogeneity. Let \(\mathcal{A}^{t}\) denote the set of active clients, \(\mathds{1}_{\{\cdot\}}\) an indicator function, \(T\) the number of total training rounds.

**Assumption 1**.: There exists a \(\delta\in(0,1]\) such that \(p_{i}^{t}\triangleq\mathbb{E}[\mathds{1}_{\{i\in\mathcal{A}^{t}\}}]\geq\delta\), where the events \(\{i\in\mathcal{A}^{t}\}\) are independent across clients \(i\) and across rounds \(t\in[T]\).

Assumption 1 subsumes uniform availability [27, 62] and stationary availability considered in [55]. Independent client unavailability is widely adopted by federated learning research [27, 29, 23, 61, 62, 55]. Analyzing non-independent unavailability, together with uncertain and non-stationary dynamics in Assumption 1, is in general challenging. Specifically, the involved entanglement of stochastic gradient and availability statistics fundamentally complicates the theoretical analysis. However, we conjecture that independence and strictly positive probabilities are only necessary for the technical convenience of our analysis. Our experiments in Section 7 suggest that our algorithm offers notable improvement even in the presence of non-independent and occasionally zero-valued probabilities.

Future work will investigate how to provably accommodate correlated or zero-valued probabilities of arbitrary probabilistic trajectories.

## 4 Heterogeneity and Non-stationarity May Lead to Significant Bias

In this section, we illustrate the impacts of heterogeneity and non-stationarity of client availability under the classic FedAvg. We use two examples to showcase the significant bias incurred.

**Example 1** (Heterogeneity).: Suppose that \(m=2\) and \(p_{i}^{t}=p_{i}\) for \(i\in[2]\). Let \(F_{i}\left(x\right)\triangleq\left\|x-u_{i}\right\|_{2}^{2}/2\), where \(x,u_{i}\in\mathbb{R}\). The global objective (1) is

\[F\left(x\right)=\frac{1}{2}(\left\|x-u_{1}\right\|_{2}^{2}+\left\|x-u_{2} \right\|_{2}^{2}),\] (2)

with unique minimizer \(x^{\star}=(u_{1}+u_{2})/2\). Let \(u_{1}=0\) and \(u_{2}=100\). Fig. 2 illustrates how the heterogeneity in \(p_{i}\) affects the expected output of FedAvg.

Example 1 matches [55, Theorem 1], which shows that FedAvg leads to a biased global objective (3) under heterogeneous \(p_{i}\)'s, and that (3) may be significantly away from (1) depending on \(p_{i}\)'s.

\[\widetilde{F}(\bm{x})\triangleq\sum_{i=1}^{m}\frac{p_{i}}{\sum_{j=1}^{m}p_{j} }F_{i}(\bm{x}).\] (3)

When the probabilistic dynamics of \(p_{i}^{t}\)'s is non-stationary, obtaining an exact biased objective similar to (3) in a neat analytical form becomes challenging, if not impossible, due to the unstructured non-stationary dynamics. Fortunately, Example 2 helps us confirm that the complex interplay between \(p_{i}^{t}\)'s across rounds and clients will inevitably further degrade the performance of FedAvg algorithm.

**Example 2** (Non-stationarity).: In Fig. 3, a total of \(m=100\) clients perform an image classification task on the SVHN dataset [37] under the FedAvg algorithm, whose local dataset distribution follows \(\text{Dirichlet}(0.1)\)[17]. Clients become available with probability \(p_{i}^{t}=p\cdot[\gamma\cdot\sin(0.1\pi\cdot t)+(1-\gamma)],\ \forall i\in[m]\). The hyperparameter details are deferred to Appendix J. Observations can be found in the caption.

## 5 Federated Agile Weight Re-Equalization (FedAWE)

To minimize (1), one natural idea is to have the entire client population performs the same number of local updates and mixes these updates carefully to ensure they are weighted equally. Unfortunately, when clients are available only intermittently, they will miss some rounds. A naive approach to equalizing the number of local updates is to have clients catch up by performing their missed local computations immediately when they become available. However, this approach requires a daunting

Figure 3: Train and test accuracy results in percentage (%). In particular, the parameter \(\gamma\) signifies the degree of non-stationary. Notice that, as the client availability becomes more non-stationary (a larger \(\gamma\)), FedAvg experiences a significant drop in accuracy. For example, both the train and test accuracies drop by over \(10\%\) when \(p=0.1\), and \(\gamma\) increases from \(0.1\) to \(0.5\).

amount of resources and may not be possible due to hardware/software constraints. Formally, recall that \(\mathcal{A}^{t}\) is the set of available clients at time \(t\). Let \(\tau_{i}(t)\triangleq\{t^{\prime}:\ t^{\prime}<t\text{ and }i\in\mathcal{A}^{t^{\prime}}\}\) denote the most recent (with respect to time \(t\)) round that client \(i\) is available. Compared with standard FedAvg, the naive "catch-up" procedure will consume \((t-\tau_{i}(t)-1)\cdot s\) local stochastic gradient descent updates and \((t-\tau_{i}(t)-1)\) additional stochastic samples, where \(s\) is the number of local updates per round when a client is available in standard FedAvg.

In this work, we target computation-light algorithms that, compared with FedAvg, only take \(O(1)\) additional computation without additional stochastic samples. We propose **Fed**erated **A**gile **W**eight **R**e-**E**qualization (FedAWE), which is formally described in Algorithm 1. It involves two novel algorithmic structures: _adaptive innovation echoing_ and _implicit gossiping_. At a high level, these novel algorithmic structures (i) help clients catch up on the missed computation, and (ii) simultaneously enable a balanced information mixture through implicit client-client gossip, which ultimately corrects the remaining bias.

In Algorithm 1, each client keeps two local variables \(\bm{x}_{i}\) and \(\tau_{i}\), along with a few auxiliary variables used in updating \(\bm{x}_{i}\) and \(\tau_{i}\). The algorithm inputs are rather standard: total training rounds \(T\), local and global learning rates \(\eta_{l}\) and \(\eta_{g}\), the number of local updates per round \(s\), and the initial model \(\bm{x}^{0}\). In each round \(t\), similar to FedAvg, an available client \(i\in\mathcal{A}^{t}\) performs \(s\) steps of stochastic gradient descent on its local model \(\bm{x}_{i}^{t}\) (lines 5-8), where \(\nabla\ell_{i}(\cdot;\xi_{i}^{(t,k)})\) is the stochastic gradient of sample \(\xi_{i}^{(t,k)}\). Next, we describe the two novel algorithmic structures used in FedAWE.

```
1Inputs:\(T\), \(s\), \(\eta_{l}\), \(\eta_{g}\), \(\bm{x}^{0}\).
2for\(i\in[m]\)do\(\bm{x}_{i}^{0}\leftarrow\bm{x}^{0}\) and \(\tau_{i}(0)\leftarrow-1\) ;
3for\(t=0,\cdots,T-1\)do
4for\(i\in\mathcal{A}^{t}\)do
5\(\bm{x}_{i}^{(t,0)}\leftarrow\bm{x}_{i}^{t}\);
6for\(k=0,\cdots,s-1\)do
7\(\bm{x}_{i}^{(t,k+1)}\leftarrow\bm{x}_{i}^{(t,k)}-\eta_{l}\nabla\ell_{i}(\bm{x }_{i}^{(t,k)};\xi_{i}^{(t,k)})\);
8
9 end for
10\(\bm{G}_{i}^{t}\leftarrow\bm{x}_{i}^{t}-\bm{x}_{i}^{(t,s)}\);
11\(\bm{x}_{i}^{t\dagger}\leftarrow\bm{x}_{i}^{(t,0)}-\eta_{g}(t-\tau_{i}(t))\bm{G} _{i}^{t}\);
12\(\tau_{i}(t+1)\gets t\);
13
14 Report \(\bm{x}_{i}^{t\dagger}\) to the parameter server;
15
16 end for
17
18 end for ```

**Algorithm 1**FedAWE

**Adaptive innovation echoing.** Departing from FedAvg wherein the local estimate \(\bm{x}_{i}^{t}\) is updated as \(\bm{x}_{i}^{t\dagger}\leftarrow\bm{x}_{i}^{(t,0)}-\eta_{g}\bm{G}_{i}^{t}\). In FedAWE (lines 10-11), we "echo" the local innovation \(\bm{G}_{i}^{t}\) by multiplying it by \((t-\tau_{i}(t))\). Intuitively, this simple echoing helps us approximately equalize the number of local improvements, as formally stated in Proposition 1. It says that the total numbers of innovations echoing are the same for all active clients for any given round and allows the unavailable clients to catch up to the missed computations when they become available.

**Proposition 1**.: _If \(\mathds{1}_{\{i\in\mathcal{A}^{R-1}\}}=1\), it holds that \(\sum_{t=0}^{R-1}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\left(t-\tau_{i}(t)\right) =R,\ \forall\ R\geq 1\)._

**Implicit gossiping.** In FedAWE, the parameter server does not send the most recent global model to the active clients at the beginning of a round. Instead, the parameter server aggregates the locally updated models \(\bm{x}_{i}^{t\dagger}\) and sends the new global model \(\bm{x}_{i}^{t+1}\) to all active clients \(\mathcal{A}^{t}\) (lines 14-15). By postponing multicasting the shared global model, the active clients in \(\mathcal{A}^{t}\)_implicitly gossip_ their updated local models with each other through the parameter server [58]. Though the postponed multi-cast brings in staleness, a simple coupling argument shows that the staleness is bounded (Lemma 2). In addition, our empirical results (Table 8 in Appendix J) suggest that there is no significant slowdown when compared to vanilla FedAvg. Gossip-type algorithms were originally proposed for peer-to-peer networks and are well-known for their agility to communication failures and asynchronous information exchange in achieving average consensus [13; 4; 24; 16; 31; 36]. Intuitively, the clients' local estimates are eventually equally weighted in the final algorithm output. Note that, departing from the standard gossiping protocols therein [24; 46], information exchange in FedAvg does not involve client-client communication. The information mixing matrix under FedAvg is defined in (4), which is doubly stochastic. Let \(M^{(t)}\triangleq\mathbb{E}[(W^{(t)})^{2}]\), \(\rho(t)\triangleq\lambda_{2}(M^{(t)})\), \(\mathbf{J}=\mathds{1}\mathds{1}^{\top}/m\), and \(\rho\triangleq\max_{t}\rho(t)\), where \(\lambda_{2}(\cdot)\) denotes the second largest eigenvalue. We next characterize the information mixing error, i.e., consensus error in Lemma 1.

**Lemma 1** ([35; 34; 51]).: _For any matrix \(B\in\mathbb{R}^{d\times m}\), it holds that \(\mathbb{E}_{W}[\|B\left(\prod_{r=1}^{t}W^{(r)}-\mathbf{J}\right)\|_{F}^{2}] \leq\rho^{t}\|B\|_{\mathrm{F}}^{2}\), where the expectation is taken with respect to randomness in \(W\) matrices._

## 6 Convergence Analysis

In this section, we analyze the convergence of FedAvg. All missing proofs and intermediate results are deferred to the Appendix. Details can be found in Table of Contents.

### Assumptions

We start by stating regulatory assumptions that are common in federated learning analysis [27; 52; 23].

**Assumption 2**.: Each local objective function \(\nabla F_{i}(\bm{x})\) is \(L\)-Lipschitz, i.e.,

\[\left\|\nabla F_{i}(\bm{x}_{1})-\nabla F_{i}(\bm{x}_{2})\right\|_{2}\leq L \left\|\bm{x}_{1}-\bm{x}_{2}\right\|_{2},\;\forall\bm{x}_{1},\;\bm{x}_{2},\; \text{and}\;\forall\;i\in[m].\]

**Assumption 3**.: Stochastic gradients \(\nabla\ell_{i}(\bm{x};\xi)\) are unbiased with bounded variance, i.e.,

\[\mathbb{E}\left[\nabla\ell_{i}(\bm{x};\xi)\mid\bm{x}\right]=\nabla F_{i}(\bm {x})\;\text{and}\;\mathbb{E}\left[\left\|\nabla\ell_{i}(\bm{x};\xi)-\nabla F_{ i}(\bm{x})\right\|_{2}^{2}\mid\bm{x}\right]\leq\sigma^{2},\;\forall\;i\in[m].\]

**Assumption 4**.: The divergence between local and global gradients is bounded for \(\beta,\;\zeta\geq 0\) such that

\[\frac{1}{m}\sum_{i=1}^{m}\left\|\nabla F_{i}(\bm{x})-\nabla F(\bm{x})\right\|_ {2}^{2}\leq\beta^{2}\left\|\nabla F(\bm{x})\right\|_{2}^{2}+\zeta^{2}.\] (5)

When the local data sets are homogeneous, \(\nabla F_{i}(\bm{x})=\nabla F(\bm{x})\) holds for any client \(i\in[m]\), resulting in \(\beta=\zeta=0\). Assumption 4 and its variants in Table 1 are often referred to as bounded gradient dissimilarity assumption to account for data heterogeneity across clients. It can be easily checked that our Assumption 4 is more relaxed or equivalent to the variants therein.

### Auxiliary/Imaginary update sequence construction.

Directly analyzing the evolution of \(\bm{x}^{t}\) and \(\bm{x}^{t}_{i}\) is challenging due to the fact that different clients update at different rounds, and that different active clients echo their local innovation \(\bm{G}^{t}_{i}\) (line 9 in Algorithm 1) with different strength \((t-\tau_{i})\). As such, we construct an auxiliary/imaginary update sequence \(\bm{z}^{t}_{i}\) for client \(i\in[m]\), whose evolution is closely coupled with \(\bm{x}^{t}\) and \(\bm{x}^{t}_{i}\) but is easier to analyze. Note that the auxiliary/imaginary update sequence is never actually computed by clients but acts as a necessary tool in building up the analysis.

**Definition 1**.: _The auxiliary sequence \(\{\bm{z}^{t}_{i}\}\) of client \(i\in[m]\) is defined as_

\[\bm{z}^{t}_{i}\;\triangleq\;\bm{x}^{t}_{i}-\eta_{l}\eta_{g}s(t-\tau_{i}(t)-1) \nabla F_{i}(\bm{x}^{\tau_{i}(t)+1}_{i}),\;\forall\;i\in[m].\] (6)

\begin{table}
\begin{tabular}{c c} \hline
**Bounded Gradient Dissimilarity** & **References** \\ \hline \(\max_{\bm{x}}\left\|\nabla F_{i}(\bm{x})\right\|_{2}^{2}\leq\zeta^{2},\; \forall\;i\in[m]\) & [29; 66; 9; 10; 60] \\ \hline \(\frac{1}{m}\sum_{i=1}^{m}\left\|\nabla F_{i}(\bm{x})\right\|_{2}^{2}\leq\beta^{2 }\left\|\nabla F(\bm{x})\right\|_{2}^{2}\) & [27; 28] \\ \hline \(\frac{1}{m}\sum_{i=1}^{m}\left\|\nabla F_{i}(\bm{x})-\nabla F(\bm{x})\right\|_{2 }^{2}\leq\zeta^{2}\) & [53; 65; 18; 56; 1; 22; 54; 62] \\ \hline \(\frac{1}{m}\sum_{i=1}^{m}\left\|\nabla F_{i}(\bm{x})\right\|_{2}^{2}\leq\beta^{2 }\left\|\nabla F(\bm{x})\right\|_{2}^{2}+\zeta^{2}\) & [23; 68; 52; 51; 14] \\ \hline \end{tabular}
\end{table}
Table 1: Popular variant assumptions on gradient dissimilarity.

Recall that \(\tau_{i}(0)=-1\). Thus, by definition, \(\bm{z}_{i}^{0}=\bm{x}_{i}^{0}\) according to (6). For general \(t\), when client \(i\in\mathcal{A}^{t-1}\), we simply have \(\tau_{i}(t)=t-1\) and thus \(t-1-\tau_{i}(t)=t-1-(t-1)=0\). That is, the auxiliary model \(\bm{z}_{i}^{t}\) and the real model \(\bm{x}_{i}^{t}\) are _identical_ whenever the client \(i\) becomes available in the previous round.

* When \(i\in\mathcal{A}^{t-1}\), the iterate of \(\bm{z}_{i}\) is a bit more involved: \[\bm{z}_{i}^{t}\stackrel{{(\ref{eq:1})}}{{=}}\bm{x}_{i}^{t} \stackrel{{(\ref{eq:1})}}{{=}}\frac{\sum_{j\in\mathcal{A}^{t-1}} \left(\bm{z}_{j}^{t-1}+(\underbrace{\bm{x}_{j}^{t-1}-\bm{z}_{j}^{t-1}}_{(\ref{ eq:1})})-\eta_{l}\eta_{g}(t-1-\tau_{j}(t-1))\bm{G}_{j}^{t-1}\right),\] (7) where \((\ref{eq:1})\) holds because of Definition 1 and \(i\in\mathcal{A}^{t-1}\), \((\ref{eq:1})\) because of line 10 in Algorithm 1, addition and subtraction. \((\ref{eq:1})\) can be expanded by (6). We defer the simplified form of (7) to (18) in Appendix C for a tidy presentation.
* When \(i\notin\mathcal{A}^{t-1}\), \(\bm{z}_{i}^{t}\) has a simple iterative relation: \[\bm{z}_{i}^{t}\;=\;\bm{z}_{i}^{t-1}-\eta_{l}\eta_{g}s\nabla F_{i}(\bm{x}_{i}^{ \tau_{i}(t-1)+1}).\] (8) At a high level, the sequence \(\bm{z}_{i}^{t}\) approximately mimics the ideal descent evolution at a client as if the client performs local optimizations on its local model \(\bm{x}_{i}\) per round regardless of its availability. Mathematically, the idea is that, if the progress per iteration of the auxiliary sequence \(\bm{z}_{i}^{t}\) is bounded, we can show the convergence of \(\bm{x}_{i}^{t}\) when \(\bm{x}_{i}^{t}\) and \(\bm{z}_{i}^{t}\) are close to each other.

It is worth noting that auxiliary sequences are used in peer-to-peer distributed learning literature [47; 2; 30; 67; 48; 34]. Yet, existing constructions are not applicable to our problem due to (1) the non-convexity of the global objectives, (2) multiple local updates per round, (3) possibly unbounded gradients, and (4) the general form of bounded gradient dissimilarity. Departing from the use of staled stochastic gradients for auxiliary updates therein, we adopt the true gradient \(\nabla F_{i}(\cdot)\) to avoid the complications from the involved interplay between randomness in stochastic samples and randomness in \(\tau_{i}(t)\). On the technical front, it follows from Definition 1 that \(\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\|_{2}^{2}\leq\eta_{l}^{2}\eta_{g}^{2}s^{2}(t- \tau_{i}(t)-1)^{2}\|\nabla F_{i}(\bm{x}_{i}^{\tau_{i}(t)+1})\|_{2}^{2}\), whose bound appears to be quite challenging to derive due to the coupling of different realizations of \(\tau_{i}(t)\) and gradients. As such, we bound the average of \(\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\|^{2}\) across clients and rounds in Proposition 2.

**Lemma 2** (Unavailability statistics).: _Under Assumption 1 and \(\delta\) defined therein. It holds for \(t\geq 0\) that \(\mathbb{E}\left[t-\tau_{i}(t)\right]\leq 1/\delta\) and \(\mathbb{E}\left[\left(t-\tau_{i}(t)\right)^{2}\right]\leq 2/\delta^{2}\)._

Lemma 2 yields an upper bound on the first and second moments of a client \(i\)'s unavailable duration despite the unstructured nature of clients' non-stationary and heterogeneous unavailability. In the special case where we have clients available with the same probability \(\delta\), the duration simply follows a homogeneous geometric distribution. It can be easily checked that our bounds trivially hold. However, the duration becomes a more challenging _non-homogeneous_ geometric random variable under our non-stationary unavailability dynamics. Lemma 2 can be derived by using a simple coupling argument and by using tools from probability theory [15].

### Main results.

Let \(\bar{\bm{z}}_{t}\triangleq\frac{1}{m}\sum_{i=1}^{m}\bm{z}_{i}^{t}\), \(F^{\star}\triangleq\min_{\bm{z}}F(\bm{x})\), and \(\delta_{\max}\triangleq\max_{i\in[m],t\in[T]}p_{i}^{t}\).

**Lemma 3** (Descent Lemma).: _Let \(\mathcal{F}^{t}\) define the sigma algebra generated by randomness up to round \(t\). Suppose Assumptions 2, 3 hold and \(\eta_{l}\eta_{g}\leq 9/(100sL)\), it holds that_

\[\mathbb{E}\left[F(\bar{\bm{z}}^{t+1})-F(\bar{\bm{z}}^{t})\mid \mathcal{F}^{t}\right] \leq-\frac{\eta_{l}\eta_{g}s}{4}\left\|\nabla F(\bar{\bm{z}}^{t}) \right\|_{2}^{2}\] \[\quad+\frac{2\eta_{l}\eta_{g}sL\sigma^{2}\left(\eta_{l}\eta_{g} \delta_{\max}+4.5m\eta_{l}^{2}sL\right)}{m^{2}}\sum_{i=1}^{m}(t-\tau_{i}(t))^{2}\] \[\quad+\frac{35\eta_{g}\eta_{l}^{3}s^{3}L^{2}}{m}\sum_{i=1}^{m}(t- \tau_{i}(t))^{2}\left\|\nabla F_{i}(\bm{x}_{i}^{\tau_{i}(t)+1})\right\|_{2}^{2}\] \[\quad+\frac{2.2\eta_{l}\eta_{g}sL^{2}}{m}\sum_{i=1}^{m}\underbrace {\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2}}_{\text{Approximation Error}}+\frac{\eta_{l}\eta_{g}sL^{2}}{2m}\sum_{i=1}^{m} \underbrace{\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}}_{\text{ Consensus Error}}.\]

[MISSING_PAGE_FAIL:8]

Moreover, from (12), (14) and (15), it can be seen that (16) holds.

\[\frac{1}{mT}\sum_{t=0}^{T-1}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\bm{x}_{i}^{t}- \bar{\bm{x}}^{t}\right\|_{2}^{2}\right]\asymp\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{ E}\left[\left\|\nabla F(\bar{\bm{x}}^{t})\right\|_{2}^{2}\right]\asymp\frac{1}{T} \sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{x}}^{t})\right\|_{2}^{ 2}\right].\] (16)

Combining (12), (13), (14) and (15), we are ready for Corollary 1.

**Corollary 1** (Convergence rate of \(\bm{x}_{i}^{t}\)).: _Suppose that Assumptions 1, 2, 3 and 4 hold. Choose learning rates as \(\eta_{l}=\frac{1}{\sqrt{T\,sL}}\), \(\eta_{g}=\sqrt{s\delta m}\) such that the conditions in (11) are met for \(T\geq 1\), it holds that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{x}}^{t}) \right\|_{2}^{2}\right]\lesssim\frac{L\left(F(\bar{\bm{x}}^{0})-F^{\star} \right)}{\sqrt{s\delta mT}}+\frac{\delta_{\max}}{\delta^{\frac{3}{2}}\sqrt{ smT}}\sigma^{2}+\frac{sm}{T}\left(\frac{\sigma^{2}+\zeta^{2}}{\delta(1- \sqrt{\rho})^{2}}\right).\] (17)

Corollary 1 establishes the full convergence rate for FedAWE algorithm. It can be seen that the first and second terms dominate when \(T\) is sufficiently large, which relate to initial suboptimality gap and stochastic gradient noise \(\sigma^{2}\), respectively. The non-stationary client unavailability results in the third term, which relates to gradient divergence \(\zeta^{2}\) and also to \(\sigma^{2}\). The proof of Corollary 1 follows from (15) by plugging in Proposition 2 and Theorem 1. In the special case where \(k\) clients participate uniformly at random, we simply have \(\delta_{\max}=\delta=k/m\). Our convergence bound attains the rate of \(O(1/\sqrt{skT})\). In other words, we achieve the desired linear speedup property with respect to the number of local steps \(s\) and the number of active clients \(k\), matching the established literature [61, 54, 65, 66]. The linear speedup property enables a large cross-device federated learning system to take advantage of a massive scale of parallelism. Notice that the consensus error (16) and the convergence rate (17) have the same asymptotic order with respect to the parameters therein. Hence, the consensus error also enjoys the desired linear speedup property when \(T\) is sufficiently large.

## 7 Numerical Experiments

**Overview.** In this section, we evaluate FedAWE on real-world data sets to corroborate our analysis and compare it with the other state-of-the-art algorithms. The missing specifications and additional results can be found in Appendix J. Specifically, we consider a federated learning system of one parameter server and \(m=100\) clients, wherein clients become available intermittently. The image classification tasks use CNNs and are based on SVHN [37], CIFAR-10 [26] and CINIC-10 [12] data sets. All of them include \(10\) classes of images of different categories. To emulate a highly heterogeneous local data distribution, the image class distribution \(\nu_{i}\sim\mathsf{Dirichlet}(\alpha=0.1)\) at client \(i\)[17, 54, 55].

**Non-stationary client unavailability.** A total of four unavailable dynamics are evaluated in Table 2, including stationary and _non_-stationary with staircase, sine and interleaved sine trajectories, with their visualizations available in the same table. The classification tasks become more challenging as the list progresses due to the growing complexity in the non-stationary dynamics. Furthermore, our choices of the non-stationary dynamics are motivated by real-world federated learning participation statistics, for example, sine trajectory [3], and by generalizing the existing participation patterns such as cyclic participation [8, 55]. In particular, the interleaved sine dynamics is more challenging than the vanilla cyclic availability dynamics since clients become available during each active period with probability that is less than 1 and non-stationary simultaneously. Formally, client \(i\)'s dynamics is defined as \(p_{i}^{t}=p_{i}\cdot f_{i}(t)\), where \(f_{i}(t)\) is a time-dependent function under non-stationary dynamics but \(f_{i}(t)=1\) when stationary, and \(p_{i}=\langle\nu_{i},\phi\rangle\). \(\phi\) characterizes the unbalanced contribution of different image classes to the generated probabilities. Each element of \([\phi]_{c}\) is drawn from \(\mathsf{Uniform}(0,\bm{\Phi}_{c})\), where a smaller \(\bm{\Phi}_{c}\) leads to a less significant contribution of that image class.

Correlating the local data distribution and the probability of client availability is a common practice in the prior literature. For example, Gu et al. in [14] experiment with a formula for \(p_{i}\) so that clients that hold images of smaller digits participate less frequently. Wang and Ji in [55] construct \(p_{i}\) as an inner product of the clients' local data distribution \(\nu_{i}\) and an external distribution \(\bm{\Phi}^{\prime}\). It is immediately clear that the coupling of local data distribution \((\nu_{i}\sim\mathsf{Dirichlet}(\alpha=0.1))\) and class contribution \(\phi\) leads to _non-independent_\(p_{i}\)'s. In addition, Assumption 1 will not hold in the case of interleaved sine non-stationary dynamics since \(p_{i}^{t}\)'s occasionally reach 0. Although being agnostic to the challenging client unavailability dynamics not covered by our analysis, we observe that FedAWE retains its outperformance. Comparisons will be specified next.

**Benchmark algorithms and discussions.** We compare FedAWE with six baseline algorithms, including FedAvg over active clients [32], FedAvg over all clients, FedAU [55], F3AST [44], FedAvg with known \(p_{i}^{*}\)[42], MIFA [14] and FedVARP [20]. The details of the algorithm and the additional results are deferred to Appendix J. It is observed that FedAWE consistently outperforms the algorithms not aided by memory or known statistics. Surprisingly, FedAWE occasionally beats MIFA and FedVARP, which are memory-heavy. We attribute it to reuse of stored gradients from the unavailable clients. Although FedAWE brings in stalens due to implicit gossiping, our results (Table 8 in Appendix J) indicate that there is no significant slowdown for FedAWE when compared to vanilla FedAvg, where we study the first round to achieve a targeted accuracy by different algorithms. In addition, FedAWE attains competitive or even better performance than FedAvg with known probability, yet unknown to the underlying dynamics in client unavailability.

## 8 Conclusion

In this paper, we have shown that the impacts of heterogeneous and non-stationary client unavailability can be significant through concrete examples on FedAvg. To address this, we have proposed an algorithm FedAWE, which provably converges by adaptively echoing clients' local improvement and by evenly diffusing local updates through implicit gossiping. Theoretically, it achieves the desired linear speedup property. Experiments have validated the superiority of FedAWE over state-of-the-art algorithms under diversified non-stationary dynamics. Future work will investigate how to extend our analysis to broader unavailability dynamics such as non-independent and non-stationary unavailability and how to incorporate our findings into federated learning algorithms of different local optimization methods.

\begin{table}
\begin{tabular}{c|c|c c|c c|c c} \hline \hline \multicolumn{2}{c|}{**Unavailable**} & \multicolumn{1}{c|}{**Datasets**} & \multicolumn{1}{c|}{**SVIN**} & \multicolumn{1}{c|}{**CIFAR-10**} & \multicolumn{1}{c}{**CINIC-10**} \\ \cline{2-9} \multicolumn{2}{c|}{**Dynamics**} & \multicolumn{1}{c|}{**Algorithms**} & \multicolumn{1}{c|}{**Train**} & \multicolumn{1}{c|}{**Train**} & \multicolumn{1}{c|}{**Train**} & \multicolumn{1}{c|}{**Test**} & \multicolumn{1}{c|}{**Train**} & \multicolumn{1}{c}{**Test**} \\ \hline \multirow{4}{*}{\begin{tabular}{c} Stationary \\ \end{tabular} } & FedAWE (ours) & **86.5**\(\pm\) 0.7 \(\times\) & **86.1**\(\pm\) 0.7 \(\times\) & **68.1**\(\pm\) 1.4 \(\times\) & **66.3**\(\pm\) 1.1 \(\times\) & **47.9**\(\pm\) 2.1 \(\times\) & **47.3**\(\pm\) 2.0 \(\times\) \\  & FedAvg over active & 82.6 \(\pm\) 1.0\(\times\) & 82.4 \(\pm\) 1.1 \(\%\) & 64.1 \(\pm\) 1.9 \(\%\) & 62.9 \(\pm\) 1.4 \(\%\) & 43.6 \(\pm\) 2.4 \(\%\) & 43.1 \(\pm\) 2.4 \(\%\) \\  & FedAvg over all & 76.1 \(\pm\) 1.2 \(\%\) & 76.1 \(\pm\) 2.4 \(\%\) & 58.2 \(\pm\) 1.2 \(\%\) & 55.4 \(\pm\) 1.8 \(\%\) & 38.4 \(\pm\) 2.1 \(\%\) & 38.0 \(\pm\) 2.1 \(\%\) \\  & FedAWE & 83.4 \(\pm\) 1.0 \(\%\) & 83.2 \(\pm\) 1.0 \(\%\) & 65.4 \(\pm\) 1.4 \(\%\) & 54.1 \(\pm\) 1.0 \(\%\) & 45.6 \(\pm\) 1.5 \(\%\) & 45.5 \(\pm\) 1.5 \(\%\) \\ \hline \multirow{4}{*}{\begin{tabular}{c} _F3AST_ \\ \end{tabular} } & F3AST & 83.2 \(\pm\) 0.7 \(\%\) & 83.2 \(\pm\) 0.7 \(\%\) & 64.4 \(\pm\) 1.1 \(\%\) & 63.3 \(\pm\) 0.9 \(\%\) & 45.3 \(\pm\) 1.2 \(\%\) & 44.8 \(\pm\) 1.2 \(\%\) \\ \cline{2-9}  & FedAWE with known \(p_{i}\)’s & 86.1 \(\pm\) 0.5 \(\%\) & 85.6 \(\pm\) 0.5 \(\%\) & 63.4 \(\pm\) 1.0 \(\%\) & 63.1 \(\pm\) 0.9 \(\%\) & 45.0 \(\pm\) 1.2 \(\%\) & 44.6 \(\pm\) 1.1 \(\%\) \\  & & HTA (memory) added & 84.2 \(\pm\) 0.5 \(\%\) & 84.1 \(\pm\) 0.6 \(\%\) & 66.6 \(\pm\) 0.5 \(\%\) & 63.5 \(\pm\) 0.5 \(\%\) & 47.5 \(\pm\) 0.5 \(\%\) & 46.9 \(\pm\) 0.5 \(\%\) \\  & FedVARP (memory) added & 84.6 \(\pm\) 0.2 \(\%\) & 84.3 \(\pm\) 0.1 \(\%\) & 67.5 \(\pm\) 0.2 \(\%\) & 66.3 \(\pm\) 0.3 \(\%\) & 47.8 \(\pm\) 0.2 \(\%\) & 47.2 \(\pm\) 0.2 \(\%\) \\ \hline \multirow{4}{*}{
\begin{tabular}{c} **Non-stationary** \\ **(Sine)** \\ \end{tabular} } & FedAWE (ours) & **85.9**\(\pm\) 0.8 \(\times\) & **85.6**\(\pm\) 1.0 \(\%\) & **67.7**\(\pm\) 1.3 \(\%\) & **66.0**\(\pm\) 1.2 \(\%\) & **47.5**\(\pm\) 2.0 \(\%\) & **46.9**\(\pm\) 2.0 \(\%\) \\  & FedAvg over active & 82.5 \(\pm\) 1.0 \(\%\) & 82.4 \(\pm\) 0.9 \(\%\) & 64.2 \(\pm\) 1.8 \(\%\) & 63.0 \(\pm\) 1.4 \(\%\) & 43.7 \(\pm\) 2.0 \(\%\) & 42.9 \(\pm\) 2.2 \(\%\) \\  & FedAvg over all & 75.9 \(\pm\) 1.4 \(\%\) & 75.9 \(\pm\) 2.3 \(\%\) & 55.7 \(\pm\) 1.2 \(\%\) & 55.4 \(\pm\) 1.8 \(\%\) & 38.4 \(\pm\) 2.0 \(\%\) & 37.9 \(\pm\) 2.0 \(\%\) \\  & FedAWE & 83.6 \(\pm\) 0.8 \(\%\) & 33.4 \(\pm\) 0.8 \(\%\) & 65.2 \(\pm\) 1.7 \(\%\) & 63.2 \(\pm\) 1.5 \(\%\) & 45.7 \(\pm\) 1.5 \(\%\) & 45.1 \(\%\) \\  & F3AST & 83.1 \(\pm\) 0.6 \(\%\) & 83.1 \(\pm\) 0.6 \(\%\) & 64.3 \(\pm\) 1.1 \(\%\)

## Acknowledgments and Disclosure of Funding

We gratefully acknowledge the support from the National Science Foundation under grants 2106891, 2107062, the National Science Foundation CAREER award under grant 2340482, and the Sony Faculty Innovation Award. The research was sponsored by the Army Research Laboratory under Cooperative Agreement Number W911NF-23-2-0014. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory, the National Science Foundation, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. We also thank Connor J. McLaughlin for valuable discussions and feedback on this work.

## References

* Allouah et al. [2023] Youssef Allouah, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, and John Stephan. Fixing by mixing: A recipe for optimal byzantine ml under heterogeneity. In _International Conference on Artificial Intelligence and Statistics_, pages 1232-1300. PMLR, 2023.
* Avdiukhin and Kasiviswanathan [2021] Dmitrii Avdiukhin and Shiva Kasiviswanathan. Federated learning under arbitrary communication patterns. In _International Conference on Machine Learning_, pages 425-435. PMLR, 2021.
* Bonawitz et al. [2019] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, Brendan McMahan, et al. Towards federated learning at scale: System design. _Proceedings of Machine Learning and Systems_, 1:374-388, 2019.
* Boyd et al. [2006] Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms. _IEEE Transactions on Information Theory_, 52(6):2508-2530, 2006.
* Busbridge et al. [2024] Dan Busbridge, Jason Ramapuram, Pierre Ablin, Tatiana Likhomanenko, Eeshan Gunesh Dhekane, Xavier Suau Cuadros, and Russell Webb. How to scale your ema. _Advances in Neural Information Processing Systems_, 36, 2024.
* Chen et al. [2021] Jin-Hua Chen, Min-Rong Chen, Guo-Qiang Zeng, and Jia-Si Weng. BdfI: a byzantine-fault-tolerance decentralized federated learning method for autonomous vehicle. _IEEE Transactions on Vehicular Technology_, 70(9):8639-8652, 2021.
* Chen et al. [2022] Wenlin Chen, Samuel Horvath, and Peter Richtarik. Optimal client sampling for federated learning. _Transactions on Machine Learning Research_, 2022.
* Cho et al. [2023] Yae Jee Cho, Pranay Sharma, Gauri Joshi, Zheng Xu, Satyen Kale, and Tong Zhang. On the convergence of federated averaging with cyclic client participation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202, pages 5677-5721. PMLR, 23-29 Jul 2023.
* Cho et al. [2023] Yae Jee Cho, Jianyu Wang, Tarun Chirvolu, and Gauri Joshi. Communication-efficient and model-heterogeneous personalized federated learning via clustered knowledge transfer. _IEEE Journal of Selected Topics in Signal Processing_, 2023.
* Cho et al. [2022] Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Towards understanding biased client selection in federated learning. In _International Conference on Artificial Intelligence and Statistics_, pages 10351-10375. PMLR, 2022.
* Crawshaw and Liu [2024] Michael Crawshaw and Mingrui Liu. Federated learning under periodic client participation and heterogeneous data: A new communication-efficient algorithm and analysis. In _The Thirty-eighth Annual Conference on Neural Information Processing Systems_, 2024.
* Darlow et al. [2018] Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet or cifar-10. _arXiv preprint arXiv:1810.03505_, 2018.

* DeGroot [1974] Morris H DeGroot. Reaching a consensus. _Journal of the American Statistical association_, 69(345):118-121, 1974.
* Gu et al. [2021] Xinran Gu, Kaixuan Huang, Jingzhao Zhang, and Longbo Huang. Fast federated learning in the presence of arbitrary device unavailability. _Advances in Neural Information Processing Systems_, 34:12052-12064, 2021.
* Gut [2006] Allan Gut. _Probability: a graduate course_, volume 200. Springer, 2006.
* Hajnal and Bartlett [1958] John Hajnal and MS Bartlett. Weak ergodicity in non-homogeneous markov chains. In _Mathematical Proceedings of the Cambridge Philosophical Society_, volume 54, pages 233-246. Cambridge Univ Press, 1958.
* Hsu et al. [2019] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. _arXiv preprint arXiv:1909.06335_, 2019.
* Huang et al. [2022] Xinmeng Huang, Yiming Chen, Wotao Yin, and Kun Yuan. Lower bounds and nearly optimal algorithms in distributed learning with communication compression. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Jerrum and Sinclair [1988] Mark Jerrum and Alistair Sinclair. Conductance and the rapid mixing property for markov chains: the approximation of permanent resolved. In _Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing_, pages 235-244, 1988.
* Jhunjhunwala et al. [2022] Divyansh Jhunjhunwala, Pranay Sharma, Aushim Nagarkatti, and Gauri Joshi. Fedvarp: Tackling the variance due to partial client participation in federated learning. In _Uncertainty in Artificial Intelligence_, pages 906-916. PMLR, 2022.
* Kairouz et al. [2021] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adria Gascon, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgur, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* Karimireddy et al. [2022] Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Byzantine-robust learning on heterogeneous datasets via bucketing. In _International Conference on Learning Representations_. PMLR, 2022.
* Karimireddy et al. [2020] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.
* Kempe et al. [2003] David Kempe, Alin Dobra, and Johannes Gehrke. Gossip-based computation of aggregate information. In _44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings._, pages 482-491. IEEE, 2003.
* Koloskova et al. [2022] Anastasiia Koloskova, Sebastian U Stich, and Martin Jaggi. Sharper convergence guarantees for asynchronous sgd for distributed and federated learning. _Advances in Neural Information Processing Systems_, 35:17202-17215, 2022.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Li et al. [2020] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine Learning and Systems_, 2:429-450, 2020.

* [28] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smithy. Feddane: A federated newton-type method. In _2019 53rd Asilomar Conference on Signals, Systems, and Computers_, pages 1227-1231. IEEE, 2019.
* [29] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. In _International Conference on Learning Representations_, 2020.
* [30] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. _Advances in Neural Information Processing Systems_, 30, 2017.
* [31] Nancy A. Lynch. _Distributed Algorithms_. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1996.
* [32] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial Intelligence and Statistics_, pages 1273-1282. PMLR, 2017.
* [33] Konstantin Mishchenko, Francis Bach, Mathieu Even, and Blake E Woodworth. Asynchronous sgd beats minibatch sgd under arbitrary delays. _Advances in Neural Information Processing Systems_, 35:420-433, 2022.
* [34] Angelia Nedic, Alex Olshevsky, and Michael G Rabbat. Network topology and communication-computation tradeoffs in decentralized optimization. _Proceedings of the IEEE_, 106(5):953-976, 2018.
* [35] Angelia Nedic, Alex Olshevsky, and Wei Shi. Achieving geometric convergence for distributed optimization over time-varying graphs. _SIAM Journal on Optimization_, 27(4):2597-2633, 2017.
* [36] Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. _IEEE Transactions on Automatic Control_, 54(1):48-61, 2009.
* [37] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In _NIPS Workshop on Deep Learning and Unsupervised Feature Learning_, 2011.
* [38] John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rabbat, Mani Malek, and Dzmitry Huba. Federated learning with buffered asynchronous aggregation. In _International Conference on Artificial Intelligence and Statistics_, pages 3581-3607. PMLR, 2022.
* [39] Thien Duc Nguyen, Samuel Marchal, Markus Miettinen, Hossein Fereidooni, N Asokan, and Ahmad-Reza Sadeghi. Diot: A federated self-learning anomaly detection system for iot. In _2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)_, pages 756-767. IEEE, 2019.
* [40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in Neural Information Processing Systems_, 32, 2019.
* [41] Muzi Peng, Jiangwei Wang, Dongjin Song, Fei Miao, and Lili Su. Privacy-preserving and uncertainty-aware federated trajectory prediction for connected autonomous vehicles. In _The 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)_. IEEE/RSJ, 2023.
* [42] Jake Perazzone, Shiqiang Wang, Mingyue Ji, and Kevin S Chan. Communication-efficient device scheduling for federated learning using stochastic optimization. In _IEEE INFOCOM 2022-IEEE Conference on Computer Communications_, pages 1449-1458. IEEE, 2022.
* [43] Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Francoise Beaufays. Federated learning for emoji prediction in a mobile keyboard. _arXiv preprint arXiv:1906.04329_, 2019.
* [44] Monica Ribero, Haris Vikalo, and Gustavo De Veciana. Federated learning under intermittent client availability and time-varying communication constraints. _IEEE Journal of Selected Topics in Signal Processing_, 17(1):98-111, 2022.

* [45] Yichen Ruan, Xiaoxi Zhang, Shu-Che Liang, and Carlee Joe-Wong. Towards flexible device participation in federated learning. In _International Conference on Artificial Intelligence and Statistics_, pages 3403-3411. PMLR, 2021.
* [46] Devavrat Shah et al. Gossip algorithms. _Foundations and Trends(r) in Networking_, 3(1):1-125, 2009.
* [47] Artin Spiridonoff, Alex Olshevsky, and Ioannis Ch Paschalidis. Robust asynchronous stochastic gradient-push: Asymptotically optimal and network-independent performance for strongly convex functions. _Journal of Machine Learning Research_, 21(58), 2020.
* [48] Sebastian U. Stich. Local SGD converges fast and communicates little. In _International Conference on Learning Representations_, 2019.
* [49] Mohammad Taha Toghani and Cesar A Uribe. Unbounded gradients in federated learning with buffered asynchronous aggregation. In _2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 1-8. IEEE, 2022.
* [50] David Tse and Pramod Viswanath. _Fundamentals of wireless communication_. Cambridge university press, 2005.
* [51] Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of local-update sgd algorithms. _The Journal of Machine Learning Research_, 22(1):9709-9758, 2021.
* [52] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. _Advances in Neural Information Processing Systems_, 33:7611-7623, 2020.
* [53] Jianyu Wang, Anit Kumar Sahu, Gauri Joshi, and Soummya Kar. Matcha: A matching-based link scheduling strategy to speed up distributed optimization. _IEEE Transactions on Signal Processing_, 70:5208-5221, 2022.
* [54] Shiqiang Wang and Mingyue Ji. A unified analysis of federated learning with arbitrary client participation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [55] Shiqiang Wang and Mingyue Ji. A lightweight method for tackling unknown participation statistics in federated averaging. In _The Twelfth International Conference on Learning Representations_, 2024.
* [56] Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. _IEEE journal on selected areas in communications_, 37(6):1205-1221, 2019.
* [57] Ming Wen, Chengchang Liu, and Yuedong Xu. Communication efficient distributed newton method over unreliable networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 15832-15840, 2024.
* [58] Ming Xiang, Stratis Ioannidis, Edmund Yeh, Carlee Joe-Wong, and Lili Su. Towards bias correction of fedavg over nonuniform and time-varying communications. In _2023 62nd IEEE Conference on Decision and Control (CDC)_, pages 6719-6724, 2023.
* [59] Cong Xie, Sanmi Koyejo, and Indranil Gupta. Asynchronous federated optimization. _arXiv preprint arXiv:1903.03934_, 2019.
* [60] Yikai Yan, Chaoyue Niu, Yucheng Ding, Zhenzhe Zheng, Shaojie Tang, Qinya Li, Fan Wu, Chengfei Lyu, Yanghe Feng, and Guihai Chen. Federated optimization under intermittent client availability. _INFORMS Journal on Computing_, 2023.
* [61] Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in non-IID federated learning. In _International Conference on Learning Representations_, 2021.

* [62] Haibo Yang, Xin Zhang, Prashant Khanduri, and Jia Liu. Anarchic federated learning. In _International Conference on Machine Learning_, pages 25331-25363. PMLR, 2022.
* [63] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Francoise Beaufays. Applied federated learning: Improving google keyboard query suggestions. _arXiv preprint arXiv:1812.02903_, 2018.
* [64] Hao Ye, Le Liang, and Geoffrey Ye Li. Decentralized federated learning with unreliable communications. _IEEE Journal of Selected Topics in Signal Processing_, 16(3):487-500, 2022.
* [65] Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization. In _International Conference on Machine Learning_, pages 7184-7193. PMLR, 2019.
* [66] Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 5693-5700, 2019.
* [67] Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. _SIAM Journal on Optimization_, 26(3):1835-1854, 2016.
* [68] Xiaotong Yuan and Ping Li. On convergence of fedprox: Local dissimilarity invariant bounds, non-smoothness and beyond. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [69] Hossein Zakerinia, Shayan Talaei, Giorgi Nadiradze, and Dan Alistarh. Communication-efficient federated learning with data and client heterogeneity. In _International Conference on Artificial Intelligence and Statistics_, pages 3448-3456. PMLR, 2024.
* [70] Tengchan Zeng, Omid Semiari, Mingzhe Chen, Walid Saad, and Mehdi Bennis. Federated learning on the road autonomous controller design for connected and autonomous vehicles. _IEEE Transactions on Wireless Communications_, 21(12):10407-10423, 2022.

## Appendices

Here, we provide an overview of the Appendix. In particular, the proofs of the main results are presented and backed by supporting lemmas and propositions.

* A Limitations
* B Broader Impacts
* C Nomenclatures
* D Useful Inequalities
* E Descent Lemma (Lemma 3)
* E.1 Multi-step perturbation
* E.2 Descent lemma
* F Intermediate Results
* F.1 Bounding local and global dissimilarity
* F.2 Weight re-equalization (Proposition 1)
* F.3 Unavailable statistics (Lemma 2)
* F.4 Auxiliary sequence construction and properties (Proposition 2)
* F.5 Consensus error of the auxiliary sequence
* F.6 Spectral norm upper bound (Lemma 4)
* G Convergence Error of \(\bar{z}^{t}\) (Theorem 1)
* H Convergence Rate of \(\bar{x}^{t}\) (Corollary 1)
* H.1 Convergence error of Algorithm 1
* H.2 Convergence rate of Algorithm 1
* I Additional Results and Interpretations
	* I.1 Consensus error of Algorithm 1
	* I.2 Orders of the asymptotic rates
* J Numerical Experiments
* J.1 Code
* J.2 Experimental setups
* J.3 Non-stationary client unavailability dynamics
* J.4 Additional results
Limitations

The limitations of our work are two-fold:

1. The client unavailability dynamics are assumed to be independent and strictly positive across clients and rounds. While deriving guarantees is generally challenging without assuming independence and positivity (see Section 3), it is interesting to explore how to relax the client unavailability dynamics, where the probabilities can potentially have arbitrary trajectories.
2. Our study focuses on heterogeneous and non-stationary client unavailability in federated learning, which may vary greatly due to its inherent uncontrollable nature. Although we have shown FedAWE provably converges to a stationary point of even non-convex objectives, an interesting yet challenging future direction is to incorporate variance reduction techniques for a more robust update.

## Appendix B Broader Impacts

Federated learning has become the main trend for distributed learning in recent years and has empowered commercial industries such as autonomous vehicles, the Internet of Things, and natural language processing. Our paper focuses on the practical implementation of federated learning systems in the real world and has significantly advanced the theory and algorithms for federated learning by bringing together insights from statistics, optimization, distributed computing and engineering practices. In addition, our research is important for federated learning systems to expand their outreach to more undesirable deployment environments. We are unaware of any potential negative social impacts of our work.

## Appendix C Nomenclatures

In this section, we provide the notations and nomenclatures used throughout our proofs for a comprehensive presentation. However, it is worth noting that all notations have been properly introduced before their first use. We next articulate the missing definitions and equation formulas.

**Missing definitions and equation formulas.**

\begin{table}
\begin{tabular}{c l} \hline \hline \(\left\|\bm{v}\right\|_{2}\) & The \(l_{2}\) norm of a given vector \(\bm{v}\). \\ \hline \(\|A\|_{\mathbb{F}}\) & The Frobenius norm of a given matrix \(A\). \\ \hline \(\mathcal{F}^{t}\) & The sigma algebra generated by randomness up to round \(t\). \\ \hline \(\lambda_{2}(A)\) & The second largest eigenvalue of a square matrix \(A\). \\ \hline \(\mathbb{R}^{d}\) & A \(d\)-dimensional vector space, where \(d\) denotes the dimension. \\ \hline \([m]\) & A set \(\{k\mid k\in\mathbb{N},k\in[1,m]\}\). \\ \hline \(\mathds{1}_{\{\mathcal{E}\}}\) & An indicator function of event \(\mathcal{E}\), i.e., \(\mathds{1}_{\{\mathcal{E}\}}=1\) when event \(\mathcal{E}\) occurs, but \(\mathds{1}_{\{\mathcal{E}\}}=0\) \\  & otherwise. \\ \hline \(\lesssim\) & \(f(n)\lesssim g(n)\), if there exists a constant \(c_{o}>0\) and an integer \(n_{0}\in\mathbb{N}\), \(f(n)\leq c_{o}g(n)\) \\  & for all \(n\geq n_{0}\). \\ \hline \(\asymp\) & \(f(n)\asymp g(n)\), if there exists a constant \(c_{\Theta}>0\) and an integer \(n_{0}\in\mathbb{N}\), \(f(n)=c_{\Theta}g(n)\) \\  & for all \(n\geq n_{0}\). \\ \hline \hline \end{tabular}
\end{table}
Table 3: Notation table The iterate of \(\bm{z}_{i}\) when \(i\in\mathcal{A}^{t-1}\).

\[\bm{z}_{i}^{t}=\frac{1}{|\mathcal{A}^{t-1}|}\sum_{j\in\mathcal{A}^{t -1}}\left(\bm{z}_{j}^{t-1}-\eta_{l}\eta_{g}\sum_{r=0}^{s-1}\nabla\ell_{j}(\bm{x} _{j}^{(t-1,r)};\xi_{i}^{(t,r)})\right)\\ +\frac{\eta_{l}\eta_{g}}{|\mathcal{A}^{t-1}|}\sum_{j\in\mathcal{A }^{t-1}}\left(t-2-\tau_{j}(t-1)\right)\sum_{r=0}^{s-1}\left(\nabla F_{j}(\bm{x} _{j}^{\tau_{j}(t-1)+1})-\nabla\ell_{j}(\bm{x}_{j}^{(t-1,r)};\xi_{i}^{(t,r)}) \right).\] (18)

\begin{table}
\begin{tabular}{l l} \hline \hline \(\mathcal{A}^{t}\) & The set of active clients in round \(t\). \\ \hline \(W^{t}\) & A doubly stochastic matrix to capture the information mixing error. Its definition can be found in (4). \\ \hline \(\tau_{i}(t)\) & \(\tau_{i}(t)\triangleq\sup\{t^{\prime}\mid t^{\prime}<t,i\in\mathcal{A}^{t^{ \prime}}\}\) defines client \(i\)’s most recent active round. In particular, \(\tau_{i}(0)=-1\) for all \(i\in[m]\). \\ \hline \(\bm{x}_{i}^{t}\) & The real model at client \(i\) at the **beginning** of round \(t\) in Algorithm 1. \\ \hline \(\bm{z}_{i}^{t}\) & The auxiliary model at client \(i\) at the **beginning** of round \(t\). Refer to Definition 1 for \\ more details. The sequence is for analysis only and is not computed by any clients. \\ \hline \(\bm{x}^{t}\) & The aggregated real model at the **end** of round \(t-1\) in Algorithm 1. \\ \hline \(\bm{z}^{t}\) & The auxiliary model at the **end** of round \(t-1\). \\ \hline \(\bm{x}_{i}^{t\dagger}\), & The real model of an active client \(i\), and auxiliary model of an active client \(i\) after \(s\)-step \\ \(\bm{z}_{i}^{t\dagger}\) & local computation in round \(t\), respectively. Refer to Algorithm 1 for more details. \\ \hline \(\bm{x}_{i}^{(t,r)}\) & The real model at client \(i\) after \(r\)-step local computation. \\ \hline \(\bar{\bm{x}}^{t}\), \(\bar{\bm{z}}^{t}\) & The real and auxiliary model mean over all clients in a distributed system and in round \(t\), respectively. \\ \hline \(F_{i}(\bm{x})\) & The local objective function at client \(i\), which is assumed to be non-convex. \\ \hline \(F(\bm{x})\) & The global objective function defined in (1): \(F(\bm{x})\triangleq\sum_{i=1}^{m}F_{i}(\bm{x})/m\). \\ \hline \(\nabla\ell_{i}(\bm{x})\) & The local stochastic gradient function at client \(i\) taken with respect to \(\bm{x}\). \\ \hline \(\nabla F_{i}(\bm{x})\) & The local true gradient function at client \(i\) taken with respect to \(\bm{x}\). \\ \hline \(\mathcal{D}_{i}\) & Client \(i\)’s local data distribution. \\ \hline \(\xi_{i}\) & An **independent** stochastic sample drawn from client \(i\)’s local distribution \(\mathcal{D}_{i}\). \\ \hline \hline \end{tabular}
\end{table}
Table 4: Algorithmic nomenclature table

\begin{table}
\begin{tabular}{c l} \hline \hline \(L\) & Lipschitz constant in Assumption 2. \\ \hline \(\sigma^{2}\) & The upper bound of the stochastic gradient variance. \\ \hline \((\beta,\;\zeta)\) & Parameters that capture the averaged gradient dissimilarity between global and local \\ objectives. \\ \hline \(\rho\) & The spectral norm of a stochastic matrix in expectation. \\ \hline \(s\) & The number of local computation steps. \\ \hline \(m\) & The number of clients in the federated learning system. \\ \hline \hline \end{tabular}
\end{table}
Table 5: Variable table 

**Local parameter innovation \(\widetilde{G}^{t}\) of the auxiliary sequence.**

\[\widetilde{\bm{G}}_{i}^{t} \triangleq\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\left[\left(t-\tau_{i }(t)\right)\sum_{r=0}^{s-1}\nabla\ell_{i}(\bm{x}_{i}^{(t,r)})-s\left(t-1-\tau_{ i}(t)\right)\nabla F_{i}(\bm{x}_{i}^{\tau_{i}(t)+1})\right]\] \[\quad+\mathds{1}_{\{i\notin\mathcal{A}^{t}\}}s\nabla F_{i}(\bm{ x}_{i}^{\tau_{i}(t)+1})\] \[=\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\left(t-\tau_{i}(t)\right) \sum_{r=0}^{s-1}\left(\nabla\ell_{i}(\bm{x}_{i}^{(t,r)})-\nabla F_{i}(\bm{x}_{ i}^{t})\right)+s\nabla F_{i}(\bm{x}_{i}^{t}),\] (19)

where the last equality holds because \(\bm{x}_{i}^{t}=\bm{x}_{i}^{\tau_{i}(t)+1}\) and re-grouping.

Decomposition in the Proof of Lemma 6.The local parameter innovation of the auxiliary sequence \(\widetilde{G}^{t}\) can be decomposed as \(\widetilde{\bm{G}}^{t}\triangleq\widetilde{\Delta}^{t}+\Delta^{t}+s\nabla\bm{ F}_{\bm{x}}^{t}\). Detailed definitions can be found below.

* \([\widetilde{\Delta}^{t}]_{i}\triangleq\mathds{1}_{\{i\in\mathcal{A}^{t}\}}(t- \tau_{i}(t))\sum_{r=0}^{s-1}\left(\nabla\ell_{i}(\bm{x}_{i}^{(t,r)};\xi_{i}^{( t,r)})-\nabla F_{i}(\bm{x}_{i}^{(t,r)})\right);\)
* \([\Delta^{t}]_{i}\triangleq\mathds{1}_{\{i\in\mathcal{A}^{t}\}}(t-\tau_{i}(t)) \sum_{r=0}^{s-1}\left(\nabla F_{i}(\bm{x}_{i}^{(t,r)})-\nabla F_{i}(\bm{x}_{ i}^{t})\right);\)
* \([\nabla\bm{F}_{\bm{x}}^{t}]_{i}\triangleq\nabla F_{i}(\bm{x}_{i}^{t})\).

## Appendix D Useful Inequalities

For completeness and for ease of exposition, we present some common inequalities that will be frequently used in our proofs.

The followings hold for any \(\bm{a}_{i}\in\mathbb{R}^{d}\) and any \(i\in[m]\).

1. Jensen's inequality. \[\left\|\frac{1}{m}\sum_{i=1}^{m}\bm{a}_{i}\right\|_{2}^{2}\leq\frac{1}{m}\sum _{i=1}^{m}\left\|\bm{a}_{i}\right\|_{2}^{2}\quad\text{and}\quad\left\|\sum_{i= 1}^{m}\bm{a}_{i}\right\|_{2}^{2}\leq m\sum_{i=1}^{m}\left\|\bm{a}_{i}\right\|_{ 2}^{2}.\] (20)
2. Young's inequality (a.k.a. Peter-Paul inequality). \[\left\langle\bm{a}_{1},\bm{a}_{2}\right\rangle\leq\frac{\left\|\bm{a}_{1} \right\|_{2}^{2}}{2\epsilon}+\frac{\epsilon\left\|\bm{a}_{2}\right\|_{2}^{2}}{ 2},\quad\text{for any }\epsilon>0.\] (21) Equivalently, we have \[\left\|\bm{a}_{1}+\bm{a}_{2}\right\|_{2}^{2} =\left\|\bm{a}_{1}\right\|_{2}^{2}+\left\|\bm{a}_{2}\right\|_{2}^ {2}+2\left\langle\bm{a}_{1},\bm{a}_{2}\right\rangle\] \[\leq\left(1+\frac{1}{\epsilon}\right)\left\|\bm{a}_{1}\right\|_{2} ^{2}+\left(1+\epsilon\right)\left\|\bm{a}_{2}\right\|_{2}^{2},\quad\text{for any }\epsilon>0.\] (22)
3. Smoothness corollary. _Given Assumption 2, it holds that_ \[F(\bm{a}_{1})-F(\bm{a}_{2}) =\left\langle\bm{a}_{1}-\bm{a}_{2},\int_{0}^{1}\nabla F(\bm{a}_{2 }+\tau(\bm{a}_{1}-\bm{a}_{2}))\mathrm{d}\tau\right\rangle\] \[=\left\langle\nabla F(\bm{a}_{2}),\bm{a}_{1}-\bm{a}_{2}\right\rangle +\int_{0}^{1}\left\langle\bm{a}_{1}-\bm{a}_{2},\nabla F(\bm{a}_{2}+\tau(\bm{a} _{1}-\bm{a}_{2}))-\nabla F(\bm{a}_{2})\right\rangle\mathrm{d}\tau\] \[\overset{(a)}{\leq}\left\langle\nabla F(\bm{a}_{2}),\bm{a}_{1}- \bm{a}_{2}\right\rangle+L\int_{0}^{1}\tau\left\|\bm{a}_{1}-\bm{a}_{2}\right\|_{ 2}\left\|(\bm{a}_{1}-\bm{a}_{2})\right\|_{2}\mathrm{d}\tau\] \[\leq\left\langle\nabla F(\bm{a}_{2}),\bm{a}_{1}-\bm{a}_{2} \right\rangle+\frac{L}{2}\left\|\bm{a}_{1}-\bm{a}_{2}\right\|_{2}^{2},\] (23) where \((a)\) follows from Cauchy-Schwartz inequality and Assumption 2.

Descent Lemma (Lemma 3)

In this section, we first present a bound on multi-step local computation. Then, we apply the bound to the analysis of descent lemma.

### Multi-step perturbation

**Lemma 5**.: _For \(s\geq 1\) and under Assumption 2, 3 and \(\eta_{l}\leq 1/(4sL)\), we have_

\[\mathbb{E}\left[\left\|\sum_{r=0}^{s-1}\nabla F_{i}(\bm{x}_{i}^{(t,r)})-\nabla F _{i}(\bm{x}_{i}^{t})\right\|_{2}^{2}\Bigm{|}\mathcal{F}^{t}\right]\leq 4\eta_{l}^{2}s^{3}L^{2} \sigma^{2}+16\eta_{l}^{2}s^{4}L^{2}\left\|\nabla F_{i}(\bm{x}_{i}^{t})\right\|_ {2}^{2}\]

Proof of Lemma 5.: The proof shares a similar road map to [61, Lemma 2], but the objective is instead to show an upper bound with respect to \(\left\|\nabla F_{i}(\bm{x}_{i}^{t})\right\|_{2}^{2}\).

For \(s\geq 1\), it holds that

\[\mathbb{E}\left[\left\|\sum_{r=0}^{s-1}\nabla F_{i}(\bm{x}_{i}^{ (t,r)})-\nabla F_{i}(\bm{x}_{i}^{t})\right\|_{2}^{2}\Bigm{|}\mathcal{F}^{t}\right] \stackrel{{(a)}}{{\leq}}s\sum_{r=0}^{s-1}\mathbb{E }\left[\left\|\nabla F_{i}(\bm{x}_{i}^{(t,r)})-\nabla F_{i}(\bm{x}_{i}^{t}) \right\|_{2}^{2}\Bigm{|}\mathcal{F}^{t}\right]\] \[\stackrel{{(b)}}{{\leq}}sL^{2}\sum_{r=0}^{s-1} \mathbb{E}\left[\left\|\bm{x}_{i}^{(t,r)}-\bm{x}_{i}^{t}\right\|_{2}^{2} \Bigm{|}\mathcal{F}^{t}\right],\] (24)

where inequality \((a)\) holds because of Jensen's inequality, inequality \((b)\) holds because of Assumption 2. It remains to bound \(\mathbb{E}[\left\|\bm{x}_{i}^{(t,r)}-\bm{x}_{i}^{t}\right\|^{2}\mid\mathcal{F} ^{t}]\). In what follows, we use \(\nabla\ell_{i}^{(t,k)}\) to denote \(\nabla\ell_{i}(\bm{x}_{i}^{(t,k)})\) and \(\nabla F_{i}^{(t,k)}\) as \(\nabla F_{i}(\bm{x}_{i}^{(t,k)})\), respectively, for ease of presentation.

\[\mathbb{E}\left[\left\|\bm{x}_{i}^{(t,r)}-\bm{x}_{i}^{t}\right\| _{2}^{2}\Bigm{|}\mathcal{F}^{t}\right]=\mathbb{E}\left[\left\|\bm{x}_{i}^{(t,r- 1)}-\bm{x}_{i}^{t}-\eta_{l}\nabla\ell_{i}^{(t,r-1)}\right\|_{2}^{2}\Bigm{|} \mathcal{F}^{t}\right]\] \[=\mathbb{E}\left[\left\|-\eta_{l}\left(\nabla\ell_{i}^{(t,r-1)}- \nabla F_{i}^{(t,r-1)}\right)+\bm{x}_{i}^{(t,r-1)}-\bm{x}_{i}^{t}-\eta_{l} \left(\nabla F_{i}^{(t,r-1)}-\nabla F_{i}^{t}+\nabla F_{i}^{t}\right)\right\| _{2}^{2}\Bigm{|}\mathcal{F}^{t}\right]\] \[\stackrel{{(c)}}{{=}}\eta_{l}^{2}\mathbb{E}\left[ \left\|\nabla\ell_{i}^{(t,r-1)}-\nabla F_{i}^{(t,r-1)}\right\|_{2}^{2}\Bigm{|} \mathcal{F}^{t}\right]+\mathbb{E}\left[\left\|\bm{x}_{i}^{(t,r-1)}-\bm{x}_{i} ^{t}-\eta_{l}\left(\nabla F_{i}^{(t,r-1)}-\nabla F_{i}^{t}+\nabla F_{i}^{t} \right)\right\|_{2}^{2}\Bigm{|}\mathcal{F}^{t}\right]\] \[\stackrel{{(d)}}{{\leq}}\eta_{l}^{2}\mathbb{E}\left[ \left\|\nabla\ell_{i}^{(t,r-1)}-\nabla F_{i}^{(t,r-1)}\right\|_{2}^{2}\Bigm{|} \mathcal{F}^{t}\right]\] \[\leq\eta_{l}^{2}\mathbb{E}\left[\left\|\nabla\ell_{i}^{(t,r-1)}- \nabla F_{i}^{(t,r-1)}\right\|_{2}^{2}\Bigm{|}\mathcal{F}^{t}\right]\] \[\stackrel{{(e)}}{{\leq}}\eta_{l}^{2}\sigma^{2}+4s\eta _{l}^{2}\left\|\nabla F_{i}^{t}\right\|_{2}^{2}\] \[\quad+\left(1+\frac{1}{2s-1}\right)\mathbb{E}\left[\left\|\bm{x} _{i}^{(t,r-1)}-\bm{x}_{i}^{t}\right\|_{2}^{2}\Bigm{|}\mathcal{F}^{t}\right]+4sL ^{2}\eta_{l}^{2}\mathbb{E}\left[\left\|\bm{x}_{i}^{(t,r-1)}-\bm{x}_{i}^{t} \right\|_{2}^{2}\Bigm{|}\mathcal{F}^{t}\right]\] \[=\eta_{l}^{2}\sigma^{2}+4s\eta_{l}^{2}\left\|\nabla F_{i}^{t} \right\|_{2}^{2}+\left(1+\frac{1}{2s-1}+4sL^{2}\eta_{l}^{2}\right)\mathbb{E} \left[\left\|\bm{x}_{i}^{(t,r-1)}-\bm{x}_{i}^{t}\right\|_{2}^{2}\Bigm{|} \mathcal{F}^{t}\right],\]

where equality \((c)\) holds because \(\nabla\ell_{i}^{(t,k)}\) is an unbiased estimator of \(\nabla F_{i}^{(t,r)}\), inequality \((d)\) holds because of Young's inequality, inequality \((e)\) holds because of Assumption 2.

By \(\eta_{l}\leq\frac{1}{4sL}\), it holds that

\[\frac{1}{2s-1}+4sL^{2}\eta_{l}^{2}\leq\frac{1}{2s-1}+\frac{1}{4s}\leq\frac{2}{2 s-1}.\]Unroll the recursion, we have

\[\mathbb{E}\left[\left\|\bm{x}_{i}^{(t,r)}-\bm{x}_{i}^{t}\right\|_{2}^ {2}\right|\mathcal{F}^{t}\right] \leq\sum_{k=0}^{r-1}\left(1+\frac{2}{2s-1}\right)^{k}\left(\eta_{l} ^{2}\sigma^{2}+4s\eta_{l}^{2}\left\|\nabla F_{i}^{t}\right\|_{2}^{2}\right)\] \[\leq\sum_{k=0}^{s-1}\left(1+\frac{2}{2s-1}\right)^{k}\left(\eta_{l} ^{2}\sigma^{2}+4s\eta_{l}^{2}\left\|\nabla F_{i}^{t}\right\|_{2}^{2}\right)\] \[=\frac{2s-1}{2}\left[\left(1+\frac{2}{2s-1}\right)^{s-\frac{1}{2} }\left(1+\frac{2}{2s-1}\right)^{\frac{1}{2}}-1\right]\left(\eta_{l}^{2}\sigma ^{2}+4s\eta_{l}^{2}\left\|\nabla F_{i}^{t}\right\|_{2}^{2}\right)\] \[\stackrel{{(f)}}{{\leq}}\left(s-\frac{1}{2}\right) \left[\sqrt{3}e-1\right]\left(\eta_{l}^{2}\sigma^{2}+4s\eta_{l}^{2}\left\| \nabla F_{i}^{t}\right\|_{2}^{2}\right)\] \[\stackrel{{(g)}}{{\leq}}4s\eta_{l}^{2}\sigma^{2}+16s ^{2}\eta_{l}^{2}\left\|\nabla F_{i}^{t}\right\|_{2}^{2},\]

where inequality \((f)\) holds because of \((1+1/x)^{x}<\exp(1)\), inequality \((g)\) holds because of \(\sqrt{3}\exp(1)-1<4\). Plug it back into (24), we have the desired result

\[\mathbb{E}\left[\left\|\sum_{r=0}^{s-1}\nabla F_{i}(\bm{x}_{i}^{(t,r)})-\nabla F _{i}(\bm{x}_{i}^{t})\right\|_{2}^{2}\Big{|}\mathcal{F}^{t}\right]\leq 4\eta_{l}^{2}s^{3}L^{2} \sigma^{2}+16\eta_{l}^{2}s^{4}L^{2}\left\|\nabla F_{i}(\bm{x}_{i}^{t})\right\| _{2}^{2}.\]

### Descent lemma

**Proof of Lemma 3.** By Assumption 2 and inequality (23), we have

\[F(\bar{\bm{z}}^{t+1})-F(\bar{\bm{z}}^{t})\leq\underbrace{\langle\nabla F(\bar {\bm{z}}^{t}),\bar{\bm{z}}^{t+1}-\bar{\bm{z}}^{t}\rangle}_{\text{(A)}}+ \underbrace{\frac{L}{2}\left\|\bar{\bm{z}}^{t+1}-\bar{\bm{z}}^{t}\right\|_{2}^ {2}}_{\text{(B)}}.\]

The one-round innovation of \(\bar{\bm{z}}\) can be rewritten as

\[\bar{\bm{z}}^{t+1}-\bar{\bm{z}}^{t}=\frac{1}{m}\sum_{i\in\mathcal{ A}^{t}}\left(\bm{z}_{i}^{t\dagger}-\bm{z}_{i}^{t}\right)+\frac{1}{m}\sum_{i\notin \mathcal{A}^{t}}\left(\bm{z}_{i}^{t+1}-\bm{z}_{i}^{t}\right)\] \[=\frac{1}{m}\sum_{i=1}^{m}\mathds{1}_{\{i\in\mathcal{A}^{t}\}} \left(\eta_{l}\eta_{g}s\sum_{k=\tau_{i}(t)+1}^{t-1}\nabla F_{i}(\bm{x}_{i}^{k} )-\eta_{l}\eta_{g}(t-\tau_{i}(t))\sum_{r=0}^{s-1}\nabla\ell_{i}(\bm{x}_{i}^{( t,r)};\xi_{i}^{(t,r)})\right)\] \[\quad-\frac{\eta_{l}\eta_{g}s}{m}\sum_{i=1}^{m}\mathds{1}_{\{i \notin\mathcal{A}^{t}\}}\nabla F_{i}(\bm{x}_{i}^{t})\] \[\stackrel{{(a)}}{{=}}\frac{1}{m}\sum_{i=1}^{m} \mathds{1}_{\{i\in\mathcal{A}^{t}\}}\eta_{l}\eta_{g}s(t-1-\tau_{i}(t))\nabla F _{i}(\bm{x}_{i}^{t})-\frac{1}{m}\sum_{i=1}^{m}\mathds{1}_{\{i\in\mathcal{A}^{t }\}}\eta_{l}\eta_{g}(t-\tau_{i}(t))\sum_{r=0}^{s-1}\nabla\ell_{i}(\bm{x}_{i}^{ (t,r)};\xi_{i}^{(t,r)})\] \[\quad-\frac{\eta_{l}\eta_{g}s}{m}\sum_{i=1}^{m}\mathds{1}_{\{i \notin\mathcal{A}^{t}\}}\nabla F_{i}(\bm{x}_{i}^{t})\] \[\stackrel{{(b)}}{{=}}\frac{\eta_{l}\eta_{g}}{m}\sum _{i=1}^{m}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}(t-\tau_{i}(t))\sum_{r=0}^{s-1} \left(\nabla F_{i}(\bm{x}_{i}^{(t,r)})-\nabla\ell_{i}(\bm{x}_{i}^{(t,r)};\xi_{i} ^{(t,r)})\right)\] \[\quad+\frac{\eta_{l}\eta_{g}}{m}\sum_{i=1}^{m}\mathds{1}_{\{i\in \mathcal{A}^{t}\}}(t-\tau_{i}(t))\sum_{r=0}^{s-1}\left(\nabla F_{i}(\bm{x}_{i}^ {t})-\nabla F_{i}(\bm{x}_{i}^{(t,r)})\right)\] \[\quad-\frac{\eta_{l}\eta_{g}s}{m}\sum_{i=1}^{m}\nabla F_{i}(\bm{x} _{i}^{t}),\]where equality \((a)\) using the fact that \(\bm{x}_{i}^{k}=\bm{x}_{i}^{t}\) for all \(k\) such that \(\tau_{i}(t)+1\leq k\leq t\), and equality \((b)\) is obtained by adding and subtracting \(\nabla\ell_{i}(\bm{x}_{i}^{t};\xi_{i}^{(t,r)})\) and by the fact that \(\big{(}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}+\mathds{1}_{\{i\not\in\mathcal{A}^{ t}\}}\big{)}=1\).

Bounding \((\mathrm{A})\). \[(\mathrm{A}) =\big{\langle}\nabla F(\bar{\bm{z}}^{t}),\bar{\bm{z}}^{t+1}-\bar{ \bm{z}}^{t}\big{\rangle}\] \[=\underbrace{\eta_{l}\eta_{g}}_{\eta}\left\langle\nabla F(\bar{ \bm{z}}^{t}),\frac{1}{m}\sum_{i=1}^{m}\mathds{1}_{\{i\in\mathcal{A}^{t}\}} \sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)\sum_{r=0}^{s-1}\Big{(} \nabla F_{i}(\bm{x}_{i}^{(t,r)})-\nabla\ell_{i}(\bm{x}_{i}^{(t,r)})\Big{)} \right\rangle}_{(\mathrm{A}.\mathrm{II})}\] \[+\underbrace{\frac{\eta_{l}\eta_{g}s}{m}\sum_{i=1}^{m}\big{\langle} \nabla F(\bar{\bm{z}}^{t}),\nabla F_{i}(\bm{z}_{i}^{t})-\nabla F_{i}(\bm{x}_{ i}^{t})\big{\rangle}}_{(\mathrm{A}.\mathrm{IV})}-\underbrace{\eta_{l}\eta_{g}s \left\langle\nabla F(\bar{\bm{z}}^{t}),\frac{1}{m}\sum_{i=1}^{m}\nabla F_{i}( \bm{z}_{i}^{t})\right\rangle}_{(\mathrm{A}.\mathrm{IV})}.\]

Bounding \((\mathrm{A}.\mathrm{I})\) \[\mathbb{E}\left[(\mathrm{A}.\mathrm{I})\Big{|}\mathcal{F}^{t}\right]\] \[\overset{(a)}{=}\eta_{l}\eta_{g}\mathbb{E}\left[\mathbb{E}\left[ \left\langle\nabla F(\bar{\bm{z}}^{t}),\frac{1}{m}\sum_{i=1}^{m}\mathds{1}_{\{ i\in\mathcal{A}^{t}\}}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)\sum_{r=0}^{s-1} \Big{(}\nabla F_{i}(\bm{x}_{i}^{(t,r)})-\nabla\ell_{i}(\bm{x}_{i}^{(t,r)};\xi_ {i}^{(t,r)})\Big{)}\right\rangle\left|\bm{x}_{i}^{(t,r)},\mathcal{F}^{t}\right| \left|\mathcal{F}^{t}\right]\right.\] \[\overset{(b)}{=}\eta_{l}\eta_{g}\left\langle\nabla F(\bar{\bm{z} }^{t}),\right.\] \[\left.\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[\mathds{1}_{\{i\in \mathcal{A}^{t}\}}\Big{|}\mathcal{F}^{t}\right]\sum_{p=-1}^{t-1}\mathds{1}_{\{ \tau_{i}(t)=p\}}(t-p)\sum_{r=0}^{s-1}\mathbb{E}\left[\mathbb{E}\left[\left( \nabla F_{i}(\bm{x}_{i}^{(t,r)})-\nabla\ell_{i}(\bm{x}_{i}^{(t,r)};\xi_{i}^{(t,r)})\right)\left|\bm{x}_{i}^{(t,r)},\mathcal{F}^{t}\right|\left|\mathcal{F}^{ t}\right]\right\rangle\right.\] \[=0,\]

where equality \((a)\) holds because of the law of total expectation, equality \((b)\) holds because \(\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\) is by definition independent of others and Assumption 3.

Bounding \((\mathrm{A}.\mathrm{II})\) \[(\mathrm{A}.\mathrm{II}) \overset{(c)}{\leq}\frac{\eta_{l}\eta_{g}}{m}\sum_{i=1}^{m} \mathds{1}_{\{i\in\mathcal{A}^{t}\}}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)= p\}}\left(\frac{s}{8}\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}+\frac{2(t-p)^{2}}{s} \left\|\sum_{r=0}^{s-1}\nabla F_{i}(\bm{x}_{i}^{t})-\nabla F_{i}(\bm{x}_{i}^{(t,r)})\right\|_{2}^{2}\right)\] \[=\frac{\eta_{l}\eta_{g}s}{8m}\sum_{i=1}^{m}\mathds{1}_{\{i\in \mathcal{A}^{t}\}}\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\] \[\quad+\frac{\eta_{l}\eta_{g}}{m}\sum_{i=1}^{m}\mathds{1}_{\{i\in \mathcal{A}^{t}\}}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}\frac{2(t-p)^ {2}}{s}\left\|\sum_{r=0}^{s-1}\nabla F_{i}(\bm{x}_{i}^{t})-\nabla F_{i}(\bm{x}_{ i}^{(t,r)})\right\|_{2}^{2},\]where inequality \((c)\) holds because of Young's inequality. It follows that

\[\mathbb{E}\left[\left(\mathrm{A.II}\right)\middle|\mathcal{F}^{t} \right]\] \[\quad+\frac{32\eta_{g}\eta_{l}^{3}s^{3}L^{2}}{m}\sum_{i=1}^{m}\sum_ {p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}\left\|\nabla F_{i}(\bm{x}_{ i}^{t})\right\|_{2}^{2}\] \[\quad+\frac{32\eta_{g}\eta_{l}^{3}s^{3}L^{2}}{m}\sum_{i=1}^{m} \sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}\left\|\nabla F_{i}(\bm {x}_{i}^{p+1})\right\|_{2}^{2},\]

where inequality \((d)\) holds because of Lemma 5, the last equality using the fact that \(\bm{x}_{i}^{k}=\bm{x}_{i}^{t}\) for all \(k\) such that \(\tau_{i}(t)+1\leq k\leq t\).

Bounding \((\mathrm{A.III})\).: \[(\mathrm{A.III})=\frac{\eta_{l}\eta_{g}s}{m}\sum_{i=1}^{m}\left\langle\nabla F (\bar{\bm{z}}^{t}),\nabla F_{i}(\bm{z}_{i}^{t})-\nabla F_{i}(\bm{x}_{i}^{t}) \right\rangle\overset{(e)}{\leq}\frac{\eta_{l}\eta_{g}s}{8}\left\|\nabla F( \bar{\bm{z}}^{t})\right\|_{2}^{2}+\frac{2\eta_{l}\eta_{g}sL^{2}}{m}\sum_{i=1}^ {m}\left\|\bm{z}_{i}^{t}-\bm{x}_{i}^{t}\right\|_{2}^{2},\]

where inequality \((e)\) follows from Young's inequality and Assumption 2. It holds that,

\[\mathbb{E}\left[\left(\mathrm{A.III}\right)\middle|\mathcal{F}^{t}\right] \leq\frac{\eta_{l}\eta_{g}s}{8}\left\|\nabla F(\bar{\bm{z}}^{t}) \right\|_{2}^{2}+\frac{2\eta_{l}\eta_{g}sL^{2}}{m}\sum_{i=1}^{m}\left\|\bm{z}_ {i}^{t}-\bm{x}_{i}^{t}\right\|_{2}^{2}.\]

Bounding \((\mathrm{A.IV})\): \[(\mathrm{A.IV})=\frac{\eta_{l}\eta_{g}s}{2}\left(\left\|\nabla F(\bar{\bm{z}}^ {t})\right\|_{2}^{2}+\left\|\frac{1}{m}\sum_{i=1}^{m}\nabla F_{i}(\bm{z}_{i}^ {t})\right\|_{2}^{2}-\left\|\nabla F(\bar{\bm{z}}^{t})-\frac{1}{m}\sum_{i=1}^ {m}\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}\right),\]

where the equality follows from the identity in Appendix D (3). It holds that

\[\mathbb{E}\left[\left(\mathrm{A.IV}\right)\middle|\mathcal{F}^{t}\right] =\frac{\eta_{l}\eta_{g}s}{2}\left(\left\|\nabla F(\bar{\bm{z}}^ {t})\right\|_{2}^{2}+\left\|\frac{1}{m}\sum_{i=1}^{m}\nabla F_{i}(\bm{z}_{i}^ {t})\right\|_{2}^{2}-\left\|\frac{1}{m}\sum_{i=1}^{m}\nabla F_{i}(\bm{z}_{i}^ {t})-\frac{1}{m}\sum_{i=1}^{m}\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}\right)\] \[\geq\frac{\eta_{l}\eta_{g}s}{2}\left(\left\|\nabla F(\bar{\bm{z}}^ {t})\right\|_{2}^{2}+\left\|\frac{1}{m}\sum_{i=1}^{m}\nabla F_{i}(\bm{z}_{i}^ {t})\right\|_{2}^{2}-\frac{L^{2}}{m}\sum_{i=1}^{m}\left\|\bar{\bm{z}}^{t}-\bm{ z}_{i}^{t}\right\|_{2}^{2}\right).\]

Putting \((\mathrm{A})\) together,

\[\mathbb{E}\left[\left(\mathrm{A}\right)\middle|\mathcal{F}^{t} \right]\leq-\frac{\eta_{l}\eta_{g}s}{4}\left\|\nabla F(\bar{\bm{z}}^{t}) \right\|_{2}^{2}+\frac{8\eta_{g}\eta_{l}^{3}s^{2}L^{2}\sigma^{2}}{m}\sum_{i=1}^ {m}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}\] \[\quad+\frac{2\eta_{l}\eta_{g}sL^{2}}{m}\sum_{i=1}^{m}\left\|\bm{x }_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2}+\frac{\eta_{l}\eta_{g}sL^{2}}{2m}\sum_{i =1}^{m}\left\|\bar{\bm{z}}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2}\] \[\quad-\frac{\eta_{l}\eta_{g}s}{2}\left\|\frac{1}{m}\sum_{i=1}^{m} \nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}+\frac{32\eta_{g}\eta_{l}^{3}s^{3} L^{2}}{m}\sum_{i=1}^{m}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2} \left\|\nabla F_{i}(\bm{x}_{i}^{p+1})\right\|_{2}^{2}.\]Bounding \((\mathrm{B})\).

\[\mathrm{(B)} \leq \underbrace{2L\frac{\eta_{l}^{2}\eta_{g}^{2}}{m^{2}}\left\|\sum_{i= 1}^{m}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}(t-\tau_{i}(t))\sum_{r=0}^{s-1}\left( \nabla F_{i}(\bm{x}_{i}^{(t,r)})-\nabla\ell_{i}(\bm{x}_{i}^{(t,r)};\xi_{i}^{(t,r)})\right)\right\|_{2}^{2}}_{\text{(B.I)}}\] \[+\underbrace{2L\frac{\eta_{l}^{2}\eta_{g}^{2}s^{2}}{m^{2}}m\sum_{ i=1}^{m}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}(t-\tau_{i}(t))^{2}\left\|\sum_{r=0}^{s-1} \left(\nabla F_{i}(\bm{x}_{i}^{t})-\nabla F_{i}(\bm{x}_{i}^{(t,r)})\right) \right\|_{2}^{2}}_{\text{(B.II)}}\] \[+\underbrace{2L\frac{\eta_{l}^{2}\eta_{g}^{2}s^{2}}{m^{2}}m\sum_{ i=1}^{m}\left\|\nabla F_{i}(\bm{x}_{i}^{t})-\nabla F_{i}(\bm{z}_{i}^{t}) \right\|_{2}^{2}}_{\text{(B.III)}}+\underbrace{2L\eta_{l}^{2}\eta_{g}^{2}s^{2} \left\|\frac{1}{m}\sum_{i=1}^{m}\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2} }_{\text{(B.IV)}}\]

Bounding \((\mathrm{B}.\mathrm{I})\)Recall that \(\delta_{\max}\triangleq\sup_{i\in[m],t\in[T]}p_{i}^{t}\). It holds that,

\[\mathbb{E}\left[\mathrm{(B.I)}\middle|\mathcal{F}^{t}\right] \overset{(\ref{eq:B.I})}{=} 2L\frac{\eta_{l}^{2}\eta_{g}^{2}}{m^{2}}\sum_{i=1}^{m}\mathbb{E} \left[\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\middle|\mathcal{F}^{t}\right](t- \tau_{i}(t))^{2}\sum_{r=0}^{s-1}\mathbb{E}\left[\mathbb{E}\left[\left\|\nabla F _{i}(\bm{x}_{i}^{(t,r)})-\nabla\ell_{i}(\bm{x}_{i}^{(t,r)};\xi_{i}^{(t,r)}) \right\|_{2}^{2}\middle|\bm{x}_{i}^{(t,r)},\mathcal{F}^{t}\right]\middle| \mathcal{F}^{t}\right]\] \[\overset{(\ref{eq:B.II})}{\leq} \frac{2\eta_{l}^{2}\eta_{g}^{2}sL\delta_{\max}\sigma^{2}}{m^{2}} \sum_{i=1}^{m}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2},\]

where equality \((f)\) holds by the law of total expectation and by the independence of event \(\{i\in\mathcal{A}^{t}\}\), inequality \((g)\) holds because of Assumption 3 and by definition \(p_{i}^{t}\leq\delta_{\max}\).

Bounding \((\mathrm{B}.\mathrm{II})\)We have,

\[\mathbb{E}\left[\mathrm{(B.II)}\middle|\mathcal{F}^{t}\right] \leq 2L\frac{\eta_{l}^{2}\eta_{g}^{2}}{m}\sum_{i=1}^{m}\sum_{p=-1}^{ t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}4\eta_{l}^{2}s^{3}L^{2}\sigma^{2}\] \[\quad+2L\frac{\eta_{l}^{2}\eta_{g}^{2}}{m}\sum_{i=1}^{m}\mathds{1 }_{\{\tau_{i}(t)=p\}}\sum_{p=-1}^{t-1}(t-p)^{2}16\eta_{l}^{2}s^{4}L^{2}\left\| \nabla F_{i}(\bm{x}_{i}^{t})\right\|_{2}^{2}\] \[=\frac{8\eta_{g}^{2}\eta_{l}^{4}s^{3}L^{3}\sigma^{2}}{m}\sum_{i=1} ^{m}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}+\frac{32\eta_{g}^ {2}\eta_{l}^{4}s^{4}L^{3}}{m}\sum_{i=1}^{m}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_ {i}(t)=p\}}(t-p)^{2}\left\|\nabla F_{i}(\bm{x}_{i}^{p+1})\right\|_{2}^{2},\]

where the last equality using the fact that \(\bm{x}_{i}^{k}=\bm{x}_{i}^{t}\) for all \(k\) such that \(\tau_{i}(t)+1\leq k\leq t\).

Bounding \((\mathrm{B}.\mathrm{III})\).\(\mathbb{E}\left[\mathrm{(B.III)}\middle|\mathcal{F}^{t}\right]\leq\frac{2\eta_{l}^{2} \eta_{g}^{2}s^{2}L^{3}}{m}\sum_{i=1}^{m}\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t} \right\|_{2}^{2}.\)

Putting \((\mathrm{B})\) together, we get

\[\mathbb{E}\left[\mathrm{(B)}\middle|\mathcal{F}^{t}\right] \leq\frac{2\eta_{l}^{2}\eta_{g}^{2}sL\delta_{\max}\sigma^{2}}{m^{2 }}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}+\frac{8\eta_{g}^{2} \eta_{l}^{4}s^{3}L^{3}\sigma^{2}}{m}\sum_{i=1}^{m}\sum_{p=-1}^{m}\mathds{1}_{\{ \tau_{i}(t)=p\}}(t-p)^{2}\] \[\quad+\frac{32\eta_{g}^{2}\eta_{l}^{4}s^{4}L^{3}}{m}\sum_{i=1}^{m} \sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}\left\|\nabla F_{i}(\bm{ x}_{i}^{p+1})\right\|_{2}^{2}\] \[\quad+\frac{2\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{3}}{m}\sum_{i=1}^{m} \left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2}+2L\eta_{l}^{2}\eta_{g}^{2}s^{ 2}\left\|\frac{1}{m}\sum_{i=1}^{m}\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}.\]Now, everything:

\[\mathbb{E}\left[F(\bar{\bm{z}}^{t+1})-F(\bar{\bm{z}}^{t})\middle| \mathcal{F}^{t}\right] \leq-\frac{\eta_{l}\eta_{g}s}{4}\left\|\nabla F(\bar{\bm{z}}^{t}) \right\|_{2}^{2}\] \[\quad-\frac{\eta_{l}\eta_{g}s}{2}\left(1-4L\eta_{l}\eta_{g}s \right)\left\|\frac{1}{m}\sum_{i=1}^{m}\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2} ^{2}\] \[\quad+\frac{2\eta_{l}^{2}\eta_{g}^{2}sL\delta_{\max}\sigma^{2}}{m^ {2}}\sum_{i=1}^{m}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}\] \[\quad+\frac{8\eta_{g}\eta_{l}^{3}s^{2}L^{2}\left(1+\eta_{g}\eta_ {l}sL\right)\sigma^{2}}{m}\sum_{i=1}^{m}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{ i}(t)=p\}}(t-p)^{2}\] \[\quad+2\eta_{l}\eta_{g}sL^{2}\left(1+\eta_{l}\eta_{g}sL\right) \frac{1}{m}\sum_{i=1}^{m}\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2} +\frac{\eta_{l}\eta_{g}sL^{2}}{2m}\sum_{i=1}^{m}\left\|\bm{z}_{i}^{t}-\bar{\bm {z}}^{t}\right\|_{2}^{2}\] \[\quad+32\eta_{g}\eta_{l}^{3}s^{3}L^{2}\left(1+\eta_{g}\eta_{l}sL \right)\frac{1}{m}\sum_{i=1}^{m}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p \}}(t-p)^{2}\left\|\nabla F_{i}(\bm{x}_{i}^{p+1})\right\|_{2}^{2}\] \[\leq-\frac{\eta_{l}\eta_{g}s}{4}\left\|\nabla F(\bar{\bm{z}}^{t} )\right\|_{2}^{2}+\frac{2\eta_{l}^{2}\eta_{g}^{2}sL\delta_{\max}\sigma^{2}}{m^ {2}}\sum_{i=1}^{m}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}\] \[\quad+\frac{9\eta_{g}\eta_{l}^{3}s^{2}L^{2}\sigma^{2}}{m}\sum_{i= 1}^{m}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}\] \[\quad+2.2\eta_{l}\eta_{g}sL^{2}\frac{1}{m}\sum_{i=1}^{m}\left\| \bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2}+\frac{\eta_{l}\eta_{g}sL^{2}}{2 m}\sum_{i=1}^{m}\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}\] \[\quad+35\eta_{g}\eta_{l}^{3}s^{3}L^{2}\frac{1}{m}\sum_{i=1}^{m} \sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_{i}(t)=p\}}(t-p)^{2}\left\|\nabla F_{i}( \bm{x}_{i}^{p+1})\right\|_{2}^{2},\]

where the last inequality holds because \(\eta_{l}\eta_{g}\leq\frac{9}{100sL}\) and that \(\left\|\frac{1}{m}\sum_{i=1}^{m}\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}\geq 0\).

Intermediate Results

In this section, we present the intermediate results that serve as handy tools in building up our proofs afterwards.

### Bounding local and global dissimilarity

**Proposition 3**.: _For any \(t\), it holds that_

\[\frac{1}{m}\sum_{i=1}^{m}\left\|\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2} \leq\frac{3L^{2}}{m}\sum_{i=1}^{m}\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t} \right\|_{2}^{2}+3\left(\beta^{2}+1\right)\left\|\nabla F(\bar{\bm{z}}^{t}) \right\|_{2}^{2}+3\zeta^{2}.\]

Proof of Proposition 3.: \[\frac{1}{m}\sum_{i=1}^{m}\left\|\nabla F_{i}(\bm{z}_{i}^{t})\right\|_ {2}^{2} =\frac{1}{m}\sum_{i=1}^{m}\left\|\nabla F_{i}(\bm{z}_{i}^{t})- \nabla F_{i}(\bar{\bm{z}}^{t})+\nabla F_{i}(\bar{\bm{z}}^{t})-\nabla F(\bar{ \bm{z}}^{t})+\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\] \[\leq\frac{3}{m}\sum_{i=1}^{m}\left\|\nabla F_{i}(\bm{z}_{i}^{t}) -\nabla F_{i}(\bar{\bm{z}}^{t})\right\|_{2}^{2}+\frac{3}{m}\sum_{i=1}^{m} \left\|\nabla F_{i}(\bar{\bm{z}}^{t})-\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{ 2}+3\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\] \[\overset{(a)}{\leq}\frac{3L^{2}}{m}\sum_{i=1}^{m}\left\|\bm{z}_{i }^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}+3\beta^{2}\left\|\nabla F(\bar{\bm{z}}^ {t})\right\|_{2}^{2}+3\zeta^{2}+3\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2} ^{2}\] \[=\frac{3L^{2}}{m}\sum_{i=1}^{m}\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^ {t}\right\|_{2}^{2}+3\left(\beta^{2}+1\right)\left\|\nabla F(\bar{\bm{z}}^{t}) \right\|_{2}^{2}+3\zeta^{2},\]

where inequality (a) follows from Assumptions 2 and 4. 

### Weight re-equalization (Proposition 1)

Proof of Proposition 1.: We show Proposition 1 by induction.

When \(T=1\) and \(i\in\mathcal{A}^{0}\), we have \(\sum_{t=0}^{0}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\left(t-\tau_{i}(t)\right)= \mathds{1}_{\{i\in\mathcal{A}^{0}\}}\left(0-\tau_{i}(0)\right)=1.\) Therefore, the base case holds.

The induction hypothesis is that \(\sum_{t=0}^{K-1}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\left(t-\tau_{i}(t)\right)=K\) holds for \(i\in\mathcal{A}^{K-1}\). Next, we focus on \(K+1\):

\[\sum_{t=0}^{K}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\left(t-\tau_{i}(t)\right)= \sum_{t=0}^{K-1}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\left(t-\tau_{i}(t)\right) +\mathds{1}_{\{i\in\mathcal{A}^{K}\}}\left(K-\tau_{i}(K)\right).\] (25)

Now, we have two cases:

* Suppose \(i\in\mathcal{A}^{K-1}\), then we simply have \(\tau_{i}(K)=K-1\). It follows that Eq. (25) \(\overset{(a)}{=}K+1\), where \((a)\) follows from induction hypothesis.
* Suppose \(i\notin\mathcal{A}^{K-1}\), \[\sum_{t=0}^{K}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\left(t-\tau_{i}(t)\right) \overset{(b)}{=}\sum_{t=0}^{\tau_{i}(K)}\mathds{1}_{\{i\in\mathcal{A}^{t}\}} \left(t-\tau_{i}(t)\right)+\mathds{1}_{\{i\in\mathcal{A}^{K}\}}\left(K-\tau_{i} (K)\right)\] \[=\tau_{i}(K)+1+(K-\tau_{i}(K))=K+1,\] where \((b)\) follows because \(\mathds{1}_{\{i\in\mathcal{A}^{t}\}}=0\) for \(\tau_{i}(K)\leq t\leq K-1\) and induction hypothesis that \(\sum_{t=0}^{\tau_{i}(K)}\mathds{1}_{\{i\in\mathcal{A}^{t}\}}\left(t-\tau_{i}(t) \right)=\tau_{i}(K)+1\) for \(i\in\mathcal{A}^{\tau_{i}(K)}\).

### Unavailable statistics (Lemma 2)

Proof of Lemma 2.: \[\mathbb{E}\left[t-\tau_{i}(t)\right]=\sum_{r=0}^{t}\mathbb{P}\left\{t-\tau_{i}(t )>r\right\}=\sum_{r=0}^{t}\prod_{r_{1}=t-r}^{t-1}\left(1-p_{i}^{r_{1}}\right) \leq\sum_{r=0}^{t}(1-\delta)^{r}\leq\frac{1}{\delta}.\]From [15, Section 12, Theorem 12.3 (i)], we know that

\[\mathbb{E}\left[g(X)\right]=g(0)+\int_{0}^{\infty}g^{\prime}(x)\mathbb{P}\left\{ X>x\right\}\mathrm{d}x,\]

where \(X\) is a non-negative random variable, and \(g\) a non-negative strictly increasing differentiable function. It follows that,

\[\mathbb{E}\left[X^{2}\right] \leq 0+2\int_{0}^{\infty}x\mathbb{P}\left\{X>x\right\}\mathrm{d}x=2 \sum_{n=1}^{\infty}\int_{n-1}^{n}x\mathbb{P}\left\{X>x\right\}\mathrm{d}x\] \[\overset{(a)}{\leq}2\sum_{n=1}^{\infty}n\int_{n-1}^{n}\mathbb{P} \left\{X>x\right\}\mathrm{d}x\] \[\overset{(b)}{\leq}2\sum_{n=1}^{\infty}n\mathbb{P}\left\{X>n-1 \right\}\int_{n-1}^{n}\mathrm{d}x=2\sum_{n=1}^{\infty}n\mathbb{P}\left\{X>n-1 \right\},\]

where inequality \((a)\) holds because \(x\leq n,\ \forall x\in(n-1,n]\), inequality \((b)\) holds because CCDF \(\mathbb{P}\left\{X>x\right\}\) is non-increasing. In particular, for a discrete random variable, we have \(\mathbb{P}\left\{X>n-1\right\}=\mathbb{P}\left\{X\geq n\right\}\).

Therefore,

\[\mathbb{E}\left[\left(t-\tau_{i}(t)\right)^{2}\right]\leq 2\sum_{n=1}^{ \infty}n\mathbb{P}\left\{t-\tau_{i}(t)\geq n\right\}\leq 2\sum_{n=1}^{ \infty}n(1-\delta)^{n-1}\leq\frac{2}{\delta^{2}}.\]

### Auxiliary sequence construction and properties (Proposition 2)

**Proposition 4**.: _For any \(t\geq 0\), when \(i\notin\mathcal{A}^{t}\), it holds that \(\bm{x}_{i}^{t+1}-\bm{z}_{i}^{t+1}=\eta_{i}\eta_{g}s(t-\tau_{i}(t+1))\nabla F_ {i}(\bm{x}_{i}^{\tau_{i}(t+1)+1})\); when \(i\in\mathcal{A}^{t}\), it holds that \(\bm{z}_{i}^{t\dagger}=\bm{x}_{i}^{t\dagger},\ \bm{z}^{t+1}=\bm{x}^{t+1},\) and \(\bm{z}_{i}^{t+1}=\bm{x}_{i}^{t+1}.\)_

Proof of Proposition 4.: The proof is divided into two parts: \(i\notin\mathcal{A}^{t}\) and \(i\in\mathcal{A}^{t}\),

When \(i\notin\mathcal{A}^{t}\).It holds that

\[\bm{x}_{i}^{t+1}-\bm{z}_{i}^{t+1} =\bm{x}_{i}^{\tau_{i}(t+1)+1}-\left[\bm{z}_{i}^{\tau_{i}(t+1)+1}- \eta_{i}\eta_{g}s\sum_{k=\tau_{i}(t+1)+1}^{t}\nabla F_{i}(\bm{x}_{i}^{k})\right]\] \[\overset{(a)}{=}\bm{x}_{i}^{\tau_{i}(t+1)+1}-\left[\bm{x}_{i}^{ \tau_{i}(t+1)+1}-\eta_{i}\eta_{g}s\sum_{k=\tau_{i}(t+1)+1}^{t}\nabla F_{i}(\bm {x}_{i}^{\tau_{i}(t+1)+1})\right]\] \[=\eta_{t}\eta_{g}s(t-\tau_{i}(t+1))\nabla F_{i}(\bm{x}_{i}^{\tau _{i}(t+1)+1}),\]

where equality (a) follows from Definition 1 for inactive clients.

When \(i\in\mathcal{A}^{t}\).Note that if \(\bm{z}_{i}^{t+}=\bm{x}_{i}^{t++}\) for each \(i\in\mathcal{A}^{t}\), then by the aggregation rules, we know \(\bm{x}^{t+1}=(1/|\mathcal{A}^{t}|)\sum_{i\in\mathcal{A}^{t}}\bm{x}_{i}^{t+}=(1 /|\mathcal{A}^{t}|)\sum_{i\in\mathcal{A}^{t}}\bm{z}_{i}^{t++}=\bm{z}^{t+1}.\) Then, we know that \(\bm{x}_{i}^{t+1}=\bm{z}_{i}^{t+1},\ \forall\ i\in\mathcal{A}^{t}.\) Hence, to show the Proposition, it is sufficient to show \(\bm{z}_{i}^{t++}=\bm{x}_{i}^{t++}\) holds for \(i\in\mathcal{A}^{t}\), which can be shown by induction.

When \(t=0\),

\[\bm{z}_{i}^{0++}=\bm{z}_{i}^{0}+0-\left(\bm{x}_{i}^{(0,0)}-\bm{x}_{i}^{(0,s)} \right)=\bm{x}_{i}^{0}-\left(\bm{x}_{i}^{(0,0)}-\bm{x}_{i}^{(0,s)}\right)=\bm{ x}_{i}^{0++}.\]Thus, the base case holds. The induction hypothesis is that \(\bm{z}_{i}^{++}=\bm{x}_{i}^{++},\;\forall\;i\in\mathcal{A}^{t}\) is true for all \(t\geq 0\). Now, we focus on \(t+1\).

\[\bm{z}_{i}^{(t+1)++} =\bm{z}_{i}^{t+1}+\eta_{l}\eta_{g}s\sum_{k=\tau_{i}(t+1)+1}^{t} \nabla F_{i}(\bm{x}_{i}^{k})-(t+1-\tau_{i}(t+1))\left(\bm{x}_{i}^{(t+1,0)}-\bm{ x}_{i}^{(t+1,s)}\right)\] \[=\bm{z}_{i}^{t+1}+\eta_{l}\eta_{g}s(t-\tau_{i}(t+1))\nabla F_{i}( \bm{x}_{i}^{\tau_{i}(t+1)+1})-(t+1-\tau_{i}(t+1))\left(\bm{x}_{i}^{(t+1,0)}-\bm {x}_{i}^{(t+1,s)}\right)\] \[\overset{(a)}{=}\bm{z}_{i}^{\tau_{i}(t+1)+1}-\eta_{l}\eta_{g}s(t- \tau_{i}(t+1)-1+1)\nabla F_{i}(\bm{x}_{i}^{\tau_{i}(t+1)+1})\] \[\qquad+\eta_{l}\eta_{g}s(t-\tau_{i}(t+1))\nabla F_{i}(\bm{x}_{i} ^{\tau_{i}(t+1)+1})-(t+1-\tau_{i}(t+1))\left(\bm{x}_{i}^{(t+1,0)}-\bm{x}_{i}^{( t+1,s)}\right)\] \[=\bm{z}_{i}^{\tau_{i}(t+1)+1}-(t+1-\tau_{i}(t+1))\left(\bm{x}_{i} ^{(t+1,0)}-\bm{x}_{i}^{(t+1,s)}\right)\] \[\overset{(b)}{=}\bm{x}_{i}^{\tau_{i}(t+1)+1}-(t+1-\tau_{i}(t+1)) \left(\bm{x}_{i}^{(t+1,0)}-\bm{x}_{i}^{(t+1,s)}\right)\] \[=\bm{x}_{i}^{(t+1)++},\]

where equality (a) follows from the auxiliary updates \(\bm{z}_{i}\), and equality (b) holds because of the induction hypothesis and the fact that \(\tau_{i}(t+1)<t+1\) and \(i\in\mathcal{A}^{\tau_{i}(t+1)}\). 

Proof of Proposition 2.: From Propositions 4, we have

\[\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2} \leq\left\|\eta_{l}\eta_{g}s\left(t-\tau_{i}(t)-1\right)\nabla F_{ i}(\bm{x}_{i}^{t})\right\|_{2}^{2}\] \[=\eta_{l}^{2}\eta_{g}^{2}s^{2}\sum_{p=-1}^{t-1}\mathds{1}_{\{\tau_ {i}(t)=p\}}\left(t-p-1\right)^{2}\left\|\nabla F_{i}(\bm{x}_{i}^{p+1})\right\|_ {2}^{2}.\]

Take expectation over all the randomness

\[\mathbb{E}\left[\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2} ^{2}\right] \overset{(a)}{\leq}\eta_{l}^{2}\eta_{g}^{2}s^{2}\sum_{p=-1}^{t-1 }\mathbb{E}\left[\mathds{1}_{\{\tau_{i}(t)=p\}}\right](t-p-1)^{2}\mathbb{E} \left[\left\|\nabla F_{i}(\bm{x}_{i}^{p+1})\right\|_{2}^{2}\right]\] \[\overset{(b)}{\leq}\eta_{l}^{2}\eta_{g}^{2}s^{2}\sum_{p=-1}^{t-1 }\left(t-p-1\right)^{2}\mathbb{P}\left\{\tau_{i}(t)=p\right\}\cdot\mathbb{E} \left[\left\|\nabla F_{i}(\bm{z}_{i}^{p+1})\right\|_{2}^{2}\right],\]

where inequality \((a)\) follows because by definition \(\mathds{1}_{\{\tau_{i}(t)=p\}}\) is independent of \(\left\|\nabla F_{i}(\bm{x}_{i}^{p+1})\right\|_{2}^{2}\), inequality \((b)\) follows because \(\bm{x}_{i}^{p+1}=\bm{z}_{i}^{p+1}\) from Proposition 4.

\[\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m} \sum_{i=1}^{m}\mathbb{E}\left[\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{ t}\right\|_{2}^{2}\right]=\eta_{l}^{2}\eta_{g}^{2}s^{2}\frac{1}{T}\sum_{t=0}^{T-1} \frac{1}{m}\sum_{i=1}^{m}\sum_{p=-1}^{t-1}\mathbb{P}\left\{\tau_{i}(t)=p\right\} \left(t-p-1\right)^{2}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{z}_{i}^{p+1}) \right\|_{2}^{2}\right]\] \[\overset{(d)}{\leq}\eta_{l}^{2}\eta_{g}^{2}s^{2}\left(\frac{2}{ \delta^{2}}\right)\frac{1}{m}\sum_{i=1}^{m}\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}\left[\left\|\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}\right]\] \[\leq 3\eta_{l}^{2}\eta_{g}^{2}s^{2}\left(\frac{2}{\delta^{2}} \right)\left(\beta^{2}+1\right)\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[ \left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]+3\eta_{l}^{2}\eta_{g}^ {2}s^{2}\left(\frac{2}{\delta^{2}}\right)\zeta^{2}\] \[+3\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\left(\frac{2}{\delta^{2}} \right)\frac{1}{m}\sum_{i=1}^{m}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[ \left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}\right],\]

where inequality \((c)\) follows from re-indexing, inequality \((d)\) from Lemma 2.

### Consensus error of the auxiliary sequence

**Lemma 6** (Consensus error of \(\bm{z}_{i}^{t}\)).: _Assuming that \(\eta_{l}\leq\delta/(20sL)\), and \(\eta_{l}\eta_{g}\leq\delta(1-\sqrt{\rho})/(10sL(\sqrt{\rho}+1))\), under Assumption 2, 3 and 4, it holds that_

\[\frac{1}{m}\sum_{i=1}^{m}\frac{1}{T}\sum_{t=0}^{T-1}\sum_{i=1}^{m} \mathbb{E}\left[\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}\right] \leq\frac{3\rho s\eta_{l}^{2}\eta_{g}^{2}}{(1-\sqrt{\rho})^{2} \delta^{2}}\sigma^{2}\] \[\quad+\frac{40\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}}{(1-\sqrt{\rho} )^{2}}\zeta^{2}\] \[\quad+\frac{40\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}\left(\beta^{2}+1 \right)}{(1-\sqrt{\rho})^{2}}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\| \nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right].\]

Proof of Lemma 6.: When \(t=0\), \(\bm{Z}^{0}=[\bm{z}^{0},\cdots,\bm{z}^{0}]\), which immediately leads to

\[\bm{Z}^{0}\left(\mathbf{I}-\mathbf{J}\right)=[\bm{z}^{0},\cdots,\bm{z}^{0}]-[ \bm{z}^{0},\cdots,\bm{z}^{0}]=\mathbf{0}.\]

For \(t\geq 1\), recall that \(W^{(t)}\) is a doubly stochastic matrix to characterize the information mixture, and \(\widetilde{\bm{G}}^{t}\), defined in (19), captures the local parameter changes in each round. It can be seen that

\[\bm{Z}^{(t)}=\left(\bm{Z}^{(t-1)}-\eta_{l}\eta_{g}\widetilde{\bm{G}}^{t-1} \right)W^{(t-1)}.\]

Expanding \(\bm{Z}\), we get

\[\bm{Z}^{(t)}\left(\mathbf{I}-\mathbf{J}\right) =(\bm{Z}^{(t-1)}-\eta_{l}\eta_{g}\widetilde{\bm{G}}^{t-1})W^{(t- 1)}\left(\mathbf{I}-\mathbf{J}\right)\] \[=\bm{Z}^{0}\prod_{\ell=0}^{t-1}W^{\ell}\left(\mathbf{I}-\mathbf{J }\right)-\eta_{l}\eta_{g}\sum_{q=0}^{t-1}\widetilde{\bm{G}}^{q}\prod_{\ell=q}^ {t-1}W^{(\ell)}\left(\mathbf{I}-\mathbf{J}\right).\]

where the last follows from the fact that all clients are initiated at the same weights. Note that \(\prod_{\ell=q}^{t-1}W^{(\ell)}\mathbf{I}=\prod_{\ell=q}^{t-1}W^{(\ell)}\) and \(\prod_{\ell=q}^{t-1}W^{(\ell)}\mathbf{J}=\mathbf{J}\). Thus,

\[\bm{Z}^{(t)}\left(\mathbf{I}-\mathbf{J}\right)=\bm{Z}^{0}\left(\prod_{\ell=0}^ {t-1}W^{\ell}-\mathbf{J}\right)-\eta_{l}\eta_{g}\sum_{q=0}^{t-1}\widetilde{\bm {G}}^{q}\left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J}\right)=-\eta_{l}\eta_{ g}\sum_{q=0}^{t-1}\widetilde{\bm{G}}^{q}\left(\prod_{\ell=q}^{t-1}W^{(\ell)}- \mathbf{J}\right),\]

where the last equality holds because that \(\bm{Z}^{0}=[\bm{z}^{0},\cdots,\bm{z}^{0}]\), which immediately leads to

\[\bm{Z}^{0}\left(\prod_{\ell=0}^{t-1}W^{\ell}-\mathbf{J}\right)=[\bm{z}^{0}, \cdots,\bm{z}^{0}]-[\bm{z}^{0},\cdots,\bm{z}^{0}]=\mathbf{0}.\]

Let matrix notations \(\widetilde{\Delta}^{t}\), \(\Delta^{t}\) and \(\nabla\bm{F}_{\bm{x}}^{t}\) define as follows:

\[\bm{G}_{i}^{q} =\underbrace{\mathds{1}_{\{i\in\mathcal{A}^{t}\}}(t-\tau_{i}(t)) }_{\tau=0}\sum_{r=0}^{s-1}\left(\nabla\ell_{i}(\bm{x}_{i}^{(t,r)};\xi_{i}^{(t,r )})-\nabla F_{i}(\bm{x}_{i}^{(t,r)})\right)}_{[\widetilde{\Delta^{t}}]_{i}}+ \underbrace{\mathds{1}_{\{i\in\mathcal{A}^{t}\}}(t-\tau_{i}(t))\sum_{r=0}^{s-1 }\left(\nabla F_{i}(\bm{x}_{i}^{(t,r)})-\nabla F_{i}(\bm{x}_{i}^{t})\right)}_ {[\widetilde{\Delta^{t}}]_{i}}\] \[+s\underbrace{\nabla F_{i}(\bm{x}_{i}^{t})}_{[\nabla\bm{F}_{\bm{x }}^{t}]_{i}}.\]It follows that

\[\left\|\bm{Z}^{(t)}\left(\mathbf{I}-\mathbf{J}\right)\right\|_{ \mathrm{F}}^{2} =\|\sum_{q=0}^{t-1}\left(\widetilde{\Delta}^{q}+\Delta^{q}+\nabla \bm{F}_{\bm{x}}^{q}\right)\left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J} \right)\|_{\mathrm{F}}^{2}\] \[=\|\sum_{q=0}^{t-1}\widetilde{\Delta}^{q}\left(\prod_{\ell=q}^{t- 1}W^{(\ell)}-\mathbf{J}\right)\|_{\mathrm{F}}^{2}+\|\sum_{q=0}^{t-1}\left( \Delta^{q}+\nabla\bm{F}_{\bm{x}}^{q}\right)\left(\prod_{\ell=q}^{t-1}W^{(\ell) }-\mathbf{J}\right)\|_{\mathrm{F}}^{2}\] \[\quad+2\left\langle\sum_{q=0}^{t-1}\widetilde{\Delta}^{q}\left( \prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J}\right),\sum_{q=0}^{t-1}\left(\Delta ^{q}+\nabla\bm{F}_{\bm{x}}^{q}\right)\left(\prod_{\ell=q}^{t-1}W^{(\ell)}- \mathbf{J}\right)\right\rangle_{\mathrm{F}}.\]

Take expectation with respect to randomness in stochastic gradients, denote by \(\mathbb{E}_{\xi}\left[\cdot\right]\):

\[\mathbb{E}_{\xi}\left[\left\|\bm{Z}^{(t)}\left(\mathbf{I}-\mathbf{ J}\right)\right\|_{\mathrm{F}}^{2}\right]=\mathbb{E}_{\xi}\left[\|\sum_{q=0}^{t-1} \widetilde{\Delta}^{q}\left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J}\right) \|_{\mathrm{F}}^{2}\right]+\mathbb{E}_{\xi}\left[\|\sum_{q=0}^{t-1}\left( \Delta^{q}+\nabla\bm{F}_{\bm{x}}^{q}\right)\left(\prod_{\ell=q}^{t-1}W^{( \ell)}-\mathbf{J}\right)\|_{\mathrm{F}}^{2}\right]\] \[=\mathbb{E}_{\xi}\left[\|\sum_{q=0}^{t-1}\widetilde{\Delta}^{q} \left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J}\right)\|_{\mathrm{F}}^{2} \right]+\mathbb{E}_{\xi}\left[\|\sum_{q=0}^{t-1}\left(\Delta^{q}+\nabla\bm{F} _{\bm{x}}^{q}\right)\left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J}\right)\|_ {\mathrm{F}}^{2}\right]\] \[\leq\mathbb{E}_{\xi}\left[\|\sum_{q=0}^{t-1}\widetilde{\Delta}^{q }\left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J}\right)\|_{\mathrm{F}}^{2} \right]+\mathbb{E}_{\xi}\left[\|\sum_{q=0}^{t-1}\left(\Delta^{q}+\nabla\bm{F} _{\bm{x}}^{q}\right)\left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J}\right)\|_ {\mathrm{F}}^{2}\right],\]

where the last inequality holds because \(\mathbb{E}_{\xi}\left[\widetilde{\Delta}^{q}\right]=0\). Next, we take expectation over the remaining randomness.

\[\mathbb{E}\left[\left\|\bm{Z}^{(t)}\left(\mathbf{I}-\mathbf{J} \right)\right\|_{\mathrm{F}}^{2}\right] \leq\mathbb{E}\left[\|\sum_{q=0}^{t-1}\widetilde{\Delta}^{q} \left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J}\right)\|_{\mathrm{F}}^{2} \right]+\mathbb{E}\left[\|\sum_{q=0}^{t-1}\left(\Delta^{q}+\nabla\bm{F}_{\bm{x }}^{q}\right)\left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J}\right)\|_{ \mathrm{F}}^{2}\right]\] \[\leq\eta_{l}^{2}\eta_{g}^{2}\underbrace{\|\sum_{q=0}^{t-1} \widetilde{\Delta}^{q}\left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J}\right)\|_ {\mathrm{F}}^{2}}_{\mathrm{(II)}}\] \[\quad+2\eta_{l}^{2}\eta_{g}^{2}s^{2}\underbrace{\|\sum_{q=0}^{t- 1}\nabla\bm{F}_{\bm{x}}^{q}\left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J} \right)\|_{\mathrm{F}}^{2}}_{\mathrm{(III)}}.\] (26)

**Bounding**\(\mathbb{E}\left[\left(\mathrm{I}\right)\right]\)

\[\mathbb{E}\left[\left(\mathrm{I}\right)\right] =\sum_{q=0}^{t-1}\mathbb{E}\left[\left\|\widetilde{\Delta}^{q} \left(\prod_{\ell=q}^{t-1}W^{\left(\ell\right)}-\mathbf{J}\right)\right\|_{ \mathrm{F}}^{2}\right]\] \[\overset{\left(a\right)}{\leq}\sum_{q=0}^{t-1}\rho^{t-q} \mathbb{E}\left[\left\|\widetilde{\Delta}^{q}\right\|_{\mathrm{F}}^{2}\right],\] (27)

where inequality \(\left(a\right)\) holds because of Assumption 3. It remains to bound \(\mathbb{E}\left[\left\|\widetilde{\Delta}^{q}\right\|_{\mathrm{F}}^{2}\right]\).

\[\left\|\widetilde{\Delta}^{q}\right\|_{\mathrm{F}}^{2} =\sum_{i=1}^{m}\mathds{1}_{\left\{i\in\mathcal{A}^{q}\right\}} \left\|\sum_{p=-1}^{q-1}\mathds{1}_{\left\{\tau_{i}(t)=p\right\}}(q-p)\sum_{ r=0}^{s-1}\left(\nabla\ell_{i}(\bm{x}_{i}^{\left(q,r\right)};\xi_{i}^{\left(q,r \right)})-\nabla F_{i}(\bm{x}_{i}^{\left(q,r\right)})\right)\right\|_{2}^{2}.\]

\[\mathbb{E}_{\xi}\left[\left\|\widetilde{\Delta}^{q}\right\|_{ \mathrm{F}}^{2}\right] =\sum_{i=1}^{m}\mathds{1}_{\left\{i\in\mathcal{A}^{q}\right\}} \sum_{p=-1}^{q-1}\mathds{1}_{\left\{\tau_{i}(t)=p\right\}}(q-p)^{2}\sum_{r=0}^ {s-1}\mathbb{E}_{\xi}\left[\left\|\nabla\ell_{i}(\bm{x}_{i}^{\left(q,r\right)} ;\xi_{i}^{\left(p,r\right)})-\nabla F_{i}(\bm{x}_{i}^{\left(q,r\right)}) \right\|_{2}^{2}\right]\] \[\leq s\sigma^{2}\sum_{i=1}^{m}\mathds{1}_{\left\{i\in\mathcal{A}^ {q}\right\}}\sum_{p=-1}^{q-1}\mathds{1}_{\left\{\tau_{i}(t)=p\right\}}(q-p)^{2}.\]

Take expectation over the remaining randomness:

\[\mathbb{E}\left[\left\|\widetilde{\Delta}^{q}\right\|_{\mathrm{F}}^{2}\right] =\mathbb{E}\left[\mathbb{E}_{\xi}\left[\left\|\widetilde{\Delta}^{q}\right\|_ {\mathrm{F}}^{2}\right]\right]\leq s\sigma^{2}\sum_{i=1}^{m}\mathbb{E}\left[ \mathds{1}_{\left\{i\in\mathcal{A}^{q}\right\}}\right]\sum_{p=-1}^{q-1} \mathbb{E}\left[\mathds{1}_{\left\{\tau_{i}(t)=p\right\}}\right](q-p)^{2}\leq \frac{2ms\sigma^{2}}{\delta^{2}}\]

Therefore,

\[\frac{1}{mT}\sum_{i=1}^{m}\sum_{t=0}^{T-1}\mathbb{E}\left[\left( \mathrm{I}\right)\right]\leq\frac{s\rho}{(1-\rho)}\left(\frac{2}{\delta^{2}} \right)\sigma^{2}.\]

**Bounding**\(\mathbb{E}\left[\left(\mathrm{II}\right)\right]\)

\[\mathbb{E}\left[\left(\mathrm{II}\right)\right]=\mathbb{E}\left[ \left\|\sum_{q=0}^{t-1}\Delta^{q}\left(\prod_{\ell=q}^{t-1}W^{\left(\ell\right)} -\mathbf{J}\right)\right\|_{\mathrm{F}}^{2}\right]\] \[=\sum_{q=0}^{t-1}\mathbb{E}\left[\left\|\Delta^{q}\left(\prod_{ \ell=q}^{t-1}W^{\left(\ell\right)}-\mathbf{J}\right)\right\|_{\mathrm{F}}^{2} \right]+\sum_{q=0}^{t-1}\sum_{p=0,p\neq q}^{t-1}\mathbb{E}\left[\left\langle \Delta^{p}\left(\prod_{\ell=p}^{t-1}W^{\left(\ell\right)}-\mathbf{J}\right), \Delta^{q}\left(\prod_{\ell=q}^{t-1}W^{\left(\ell\right)}-\mathbf{J}\right) \right\rangle\right]\] \[\leq\sum_{q=0}^{t-1}\rho^{t-q}\mathbb{E}\left[\left\|\Delta^{q} \right\|_{\mathrm{F}}^{2}\right]+\sum_{q=0}^{t-1}\sum_{p=0,p\neq q}^{t-1} \mathbb{E}\left[\left\|\Delta^{p}\left(\prod_{\ell=p}^{t-1}W^{\left(\ell \right)}-\mathbf{J}\right)\right\|_{\mathrm{F}}\|\Delta^{q}\left(\prod_{\ell= q}^{t-1}W^{\left(\ell\right)}-\mathbf{J}\right)\|_{\mathrm{F}}\right]\] \[\leq\sum_{q=0}^{t-1}\rho^{t-q}\mathbb{E}\left[\left\|\Delta^{q} \right\|_{\mathrm{F}}^{2}\right]+\sum_{q=0}^{t-1}\sum_{p=0,p\neq q}^{t-1} \mathbb{E}\left[\frac{\rho^{t-p}}{2\epsilon}\|\Delta^{p}\|_{\mathrm{F}}^{2}+ \frac{\epsilon\rho^{t-q}}{2}\|\Delta^{q}\|_{\mathrm{F}}^{2}\right],\]Next, we bound the second term, choose \(\epsilon=\rho^{\frac{q-p}{2}}\),

\[\sum_{q=0}^{t-1}\sum_{p=0,p\neq q}^{t-1}\frac{\sqrt{\rho}^{2t-p-q}}{ 2}\mathbb{E}\left[\|\Delta^{p}\|_{\mathrm{F}}^{2}+\|\Delta^{q}\|_{\mathrm{F}}^{ 2}\right]\leq\sum_{q=0}^{t-1}\sum_{p=0}^{t-1}\frac{\sqrt{\rho}^{2t-p-q}}{2} \mathbb{E}\left[\|\Delta^{p}\|_{\mathrm{F}}^{2}+\|\Delta^{q}\|_{\mathrm{F}}^{ 2}\right]\] \[=\sum_{p=0}^{t-1}\frac{\sqrt{\rho}^{t-p}}{2}\mathbb{E}\left[\| \Delta^{p}\|_{\mathrm{F}}^{2}\right]\sum_{q=0}^{t-1}\sqrt{\rho}^{t-q}+\sum_{q =0}^{t-1}\frac{\sqrt{\rho}^{t-q}}{2}\mathbb{E}\left[\|\Delta^{q}\|_{\mathrm{F }}^{2}\right]\sum_{p=0}^{t-1}\sqrt{\rho}^{t-p}\] \[=\frac{\sqrt{\rho}-\sqrt{\rho}^{t+1}}{1-\sqrt{\rho}}\sum_{q=0}^{ t-1}\sqrt{\rho}^{t-q}\mathbb{E}\left[\|\Delta^{q}\|_{\mathrm{F}}^{2}\right].\] (28)

Plugging the upper bound in (28) into (27), we get

\[\mathbb{E}\left[\left(\Pi\right)\right] \leq\sum_{q=0}^{t-1}\left[\sqrt{\rho}^{t-q}+\frac{\sqrt{\rho}- \sqrt{\rho}^{t+1}}{1-\sqrt{\rho}}\right]\sqrt{\rho}^{t-q}\mathbb{E}\left[\| \Delta^{q}\|_{\mathrm{F}}^{2}\right]\leq\sum_{q=0}^{t-1}\left[\frac{\sqrt{ \rho}+\sqrt{\rho}}{1-\sqrt{\rho}}\right]\sqrt{\rho}^{t-q}\mathbb{E}\left[\| \Delta^{q}\|_{\mathrm{F}}^{2}\right]\] \[\leq\frac{2\sqrt{\rho}}{1-\sqrt{\rho}}\sum_{q=0}^{t-1}\sqrt{\rho} ^{t-q}\mathbb{E}\left[\|\Delta^{q}\|_{\mathrm{F}}^{2}\right],\] (29)

where inequality \((b)\) follows because that \(\sqrt{\rho}^{t-q}\leq\sqrt{\rho}\) for any \(q\leq t-1\), and that \(\sqrt{\rho}^{t+1}\geq 0\). It remains to bound \(\mathbb{E}\left[\|\Delta^{q}\|_{\mathrm{F}}^{2}\right]\). Take expectation with respect to randomness in stochastic gradients:

\[\mathbb{E}_{\xi}\left[\|\Delta^{q}\|_{\mathrm{F}}^{2}\right] \leq 4\eta_{l}^{2}s^{3}L^{2}\sum_{i=1}^{m}\sum_{p=-1}^{q-1} \mathds{1}_{\{\tau_{i}(q)=p\}}(q-p)^{2}\sigma^{2}\] \[+16\eta_{l}^{2}s^{4}L^{2}\sum_{i=1}^{m}\sum_{p=-1}^{q-1}\mathds{1 }_{\{\tau_{i}(q)=p\}}(q-p)^{2}\left\|\nabla F_{i}(\bm{x}_{i}^{q})\right\|_{2}^ {2},\]

where the inequality holds due to Lemma 5. Next, we take expectation over the remaining randomness and plug back into (29):

\[\mathbb{E}\left[\left(\Pi\right)\right]\leq\frac{2\sqrt{\rho}}{1- \sqrt{\rho}}\sum_{q=0}^{t-1}\sqrt{\rho}^{t-q}\mathbb{E}\left[\|\Delta^{q}\|_{ \mathrm{F}}^{2}\right]\] \[\leq\frac{8\rho}{\left(1-\sqrt{\rho}\right)^{2}}\left(\frac{2}{ \delta^{2}}\right)\eta_{l}^{2}s^{3}L^{2}m\sigma^{2}\] \[\quad+\frac{32\sqrt{\rho}}{1-\sqrt{\rho}}\left(\frac{2}{\delta^{2 }}\right)\eta_{l}^{2}s^{4}L^{2}\sum_{i=1}^{m}\sum_{q=0}^{t-1}\mathbb{E}\left[ \|\nabla F_{i}(\bm{x}_{i}^{q})\|_{2}^{2}\right]\sum_{k=1}^{T-1-t}\sqrt{\rho}^ {k}\] \[\leq\frac{8\rho}{\left(1-\sqrt{\rho}\right)^{2}}\left(\frac{2}{ \delta^{2}}\right)\eta_{l}^{2}s^{3}L^{2}m\sigma^{2}+\frac{32\rho}{\left(1- \sqrt{\rho}\right)^{2}}\left(\frac{2}{\delta^{2}}\right)\eta_{l}^{2}s^{4}L^{2 }\sum_{i=1}^{m}\sum_{q=0}^{t-1}\mathbb{E}\left[\|\nabla F_{i}(\bm{x}_{i}^{q}) \|_{2}^{2}\right],\]

where the last inequality holds because of re-index and grouping. Therefore,

\[\frac{1}{mT}\sum_{t=1}^{T-1}\mathbb{E}\left[\left(\Pi\right)\right] \leq\frac{8\rho}{\left(1-\sqrt{\rho}\right)^{2}}\left(\frac{2}{\delta^{2}} \right)\eta_{l}^{2}s^{3}L^{2}\sigma^{2}\] \[\quad+\frac{32\rho}{\left(1-\sqrt{\rho}\right)^{2}}\left(\frac{2 }{\delta^{2}}\right)\eta_{l}^{2}s^{4}L^{2}\frac{1}{T}\sum_{t=1}^{T-1}\frac{1}{m }\sum_{i=1}^{m}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{x}_{i}^{t})\right\|_{2 }^{2}\right]\] \[\leq\frac{8\rho}{\left(1-\sqrt{\rho}\right)^{2}}\left(\frac{2}{ \delta^{2}}\right)\eta_{l}^{2}s^{3}L^{2}\sigma^{2}+\frac{64\rho}{\left(1-\sqrt{ \rho}\right)^{2}}\left(\frac{2}{\delta^{2}}\right)\eta_{l}^{2}s^{4}L^{4} \frac{1}{T}\sum_{t=1}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\bm{x }_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2}\right]\] \[\quad+\frac{64\rho}{\left(1-\sqrt{\rho}\right)^{2}}\left(\frac{2}{ \delta^{2}}\right)\eta_{l}^{2}s^{4}L^{2}\frac{1}{T}\sum_{t=1}^{T-1}\frac{1}{m }\sum_{i=1}^{m}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^ {2}\right]\]Bounding \(\mathbb{E}\left[\left(\mathrm{III}\right)\right]\)Use a similar trick as in bounding \(\mathbb{E}\left[\left(\mathrm{II}\right)\right],\) and we get

\[\mathbb{E}\left[\left(\mathrm{III}\right)\right]=\mathbb{E}\left[\|\sum_{q=0}^{t -1}\nabla F_{\bm{x}}^{q}\left(\prod_{\ell=q}^{t-1}W^{(\ell)}-\mathbf{J} \right)\|_{\mathrm{F}}^{2}\right]\leq\frac{2\sqrt{\rho}}{1-\sqrt{\rho}}\sum_{q= 0}^{t-1}\sqrt{\rho}^{t-q}\mathbb{E}\left[\|\nabla F_{\bm{x}}^{q}\|_{\mathrm{F }}^{2}\right],\]

so that

\[\frac{1}{mT}\sum_{t=0}^{T-1}\mathbb{E}\left[\left(\mathrm{III} \right)\right] \leq\frac{2\sqrt{\rho}}{mT\left(1-\sqrt{\rho}\right)}\sum_{t=0}^{ T-1}\mathbb{E}\left[\|\nabla F_{\bm{x}}^{t}\|_{\mathrm{F}}^{2}\right]\sum_{q=1}^{T-1- t}\sqrt{\rho}^{q}\] \[\leq\frac{2\rho}{\left(1-\sqrt{\rho}\right)^{2}}\frac{1}{mT}\sum _{t=0}^{T-1}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{x}_{i}^{t}) \right\|_{2}^{2}\right]\] \[\leq\frac{4\rho L^{2}}{\left(1-\sqrt{\rho}\right)^{2}}\frac{1}{mT }\sum_{t=0}^{T-1}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\bm{x}_{i}^{t}-\bm{z}_{i }^{t}\right\|_{2}^{2}\right]+\frac{4\rho}{\left(1-\sqrt{\rho}\right)^{2}}\frac {1}{mT}\sum_{t=0}^{T-1}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{z }_{i}^{t})\right\|_{2}^{2}\right].\]

Putting them together

\[\frac{1}{mT} \sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\bm{Z}^{(t)}\left(\mathbf{ I}-\mathbf{J}\right)\right\|_{\mathrm{F}}^{2}\right]\leq\frac{sp\eta_{l}^{2}\eta_{g} ^{2}}{\left(1-\sqrt{\rho}\right)^{2}}\left(\frac{2}{\delta^{2}}\right)\left(1 +16\eta_{l}^{2}s^{2}L^{2}\right)\sigma^{2}\] \[+\frac{8\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}}{\left(1-\sqrt{\rho} \right)^{2}}\left(1+16\eta_{l}^{2}s^{2}L^{2}\left(\frac{2}{\delta^{2}}\right) \right)\frac{1}{T}\sum_{t=1}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[ \left\|\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}\right].\]

Plug in Proposition 2.

\[\frac{1}{mT}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\bm{Z}^{(t)} \left(\mathbf{I}-\mathbf{J}\right)\right\|_{\mathrm{F}}^{2}\right]\leq\frac{sp \eta_{l}^{2}\eta_{g}^{2}}{\left(1-\sqrt{\rho}\right)^{2}}\left(\frac{2}{ \delta^{2}}\right)\left(1+20\eta_{l}^{2}s^{2}L^{2}\right)\sigma^{2}\] \[\leq\frac{1.05\rho s\eta_{l}^{2}\eta_{g}^{2}}{\left(1-\sqrt{\rho} \right)^{2}}\left(\frac{2}{\delta^{2}}\right)\sigma^{2}+\frac{9\rho s^{2}\eta _{l}^{2}\eta_{g}^{2}}{\left(1-\sqrt{\rho}\right)^{2}}\frac{1}{T}\sum_{t=1}^{T -1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{z}_{i}^{t} )\right\|_{2}^{2}\right],\]

where the last inequality holds because \(\eta_{l}\leq\delta/(20sL)\) and \(\eta_{l}\eta_{g}\leq\delta/(10sL)\). Next, plug in Proposition 3.

\[\frac{1}{mT}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\bm{Z}^{(t)} \left(\mathbf{I}-\mathbf{J}\right)\right\|_{\mathrm{F}}^{2}\right]\leq\frac{1. 05\rho s\eta_{l}^{2}\eta_{g}^{2}}{\left(1-\sqrt{\rho}\right)^{2}}\frac{2}{ \delta^{2}}\sigma^{2}+\frac{27\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}}{\left(1-\sqrt {\rho}\right)^{2}}\zeta^{2}\] \[+\frac{27\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}\left(\beta^{2}+1 \right)}{(1-\sqrt{\rho})^{2}}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\| \nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]+\frac{27\rho s^{2}L^{2}\eta _{l}^{2}\eta_{g}^{2}}{\left(1-\sqrt{\rho}\right)^{2}}\frac{1}{T}\sum_{t=0}^{T-1 }\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{ t}\right\|_{2}^{2}\right].\]

It follows that

\[\frac{1}{mT}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\bm{Z}^{(t)} \left(\mathbf{I}-\mathbf{J}\right)\right\|_{\mathrm{F}}^{2}\right] \leq\frac{3\rho s\eta_{l}^{2}\eta_{g}^{2}}{(1-\sqrt{\rho})^{2} \delta^{2}}\sigma^{2}\] \[\quad+\frac{40\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}}{(1-\sqrt{\rho})^{ 2}}\zeta^{2}\] \[\quad+\frac{40\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}\left(\beta^{2}+1 \right)}{(1-\sqrt{\rho})^{2}}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\| \nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right].\]

which is due to the fact that \(\eta_{l}\eta_{g}\leq\frac{1-\sqrt{\rho}}{10sL(\sqrt{\rho}+1)}.\)\(\Box\)

### Spectral norm upper bound (Lemma 4)

Lemma 4 adapts from [58], we present its proof here for completeness.

Proof of Lemma 4.: For ease of exposition, in this proof we drop time index \(t\). We first get the explicit expression for \(\mathbb{E}\left[W_{jj^{\prime}}^{2}\mid\mathcal{A}\neq\emptyset\right]\). Suppose that \(\mathcal{A}\neq\emptyset\). We have

\[W_{jj^{\prime}}^{2}=\sum_{k=1}^{m}W_{jk}W_{j^{\prime}k}=W_{jj}W_{j^{\prime}j}+ W_{jj^{\prime}}W_{j^{\prime}j^{\prime}}+\sum_{k\in[m]\setminus\{j,j^{\prime}\}}W_{ jk}W_{j^{\prime}k}.\]

When \(k\neq j\) and \(k\neq j^{\prime}\), we have

\[W_{jk}W_{j^{\prime}k}=\frac{1}{|\mathcal{A}|^{2}}\mathds{1}_{\{j\in \mathcal{A}\}}\mathds{1}_{\{j^{\prime}\in\mathcal{A}\}}\mathds{1}_{\{k\in \mathcal{A}\}}.\]

In addition, we have \(W_{jj}W_{j^{\prime}j}=\frac{1}{|\mathcal{A}|^{2}}\mathds{1}_{\{j\in\mathcal{ A}\}}\mathds{1}_{\{j^{\prime}\in\mathcal{A}\}},\) and \(W_{j^{\prime}j^{\prime}}W_{jj^{\prime}}=\frac{1}{|\mathcal{A}|^{2}}\mathds{1} _{\{j\in\mathcal{A}\}}\mathds{1}_{\{j^{\prime}\in\mathcal{A}\}}.\) Thus,

* For \(j\neq j^{\prime}\), we have \[W_{jj^{\prime}}^{2}=\sum_{k=1}^{m}W_{jk}W_{j^{\prime}k}=\frac{1}{|\mathcal{A} |}\mathds{1}_{\{j\in\mathcal{A}\}}\mathds{1}_{\{j^{\prime}\in\mathcal{A}\}};\]
* For \(j=j^{\prime}\), we have \[W_{jj}^{2}=\frac{1}{|\mathcal{A}|}\mathds{1}_{\{j\in\mathcal{A}\}}+\left(1- \mathds{1}_{\{j\in\mathcal{A}\}}\right).\]

In the special case where \(\mathcal{A}=\emptyset\), we simply have \(W=\mathbf{I}\) by the algorithmic clauses. Therefore, \(\mathbb{E}\left[W_{jj^{\prime}}\mid\mathcal{A}=\emptyset\right]\geq 0\) holds for any pair of \(j,j^{\prime}\in[m]\). It follows, by the law of total expectation and for all \(j,j^{\prime}\in[m]\), that

\[\mathbb{E}\left[W_{jj^{\prime}}\right] =\mathbb{E}\left[W_{jj^{\prime}}\mid\mathcal{A}=\emptyset\right] \mathbb{P}\left\{\mathcal{A}=\emptyset\right\}+\mathbb{E}\left[W_{jj^{\prime}} \mid\mathcal{A}\neq\emptyset\right]\mathbb{P}\left\{\mathcal{A}\neq\emptyset\right\}\] \[\geq\mathbb{E}\left[W_{jj^{\prime}}\mid\mathcal{A}\neq\emptyset \right]\mathbb{P}\left\{\mathcal{A}\neq\emptyset\right\}.\]

* For \(j\neq j^{\prime}\), it holds that \[\mathbb{E}\left[W_{jj^{\prime}}^{2}\mid\mathcal{A}\neq\emptyset\right] =\mathbb{E}\left[\frac{1}{|\mathcal{A}|}\mathds{1}_{\{j\in \mathcal{A}\}}\mathds{1}_{\{j^{\prime}\in\mathcal{A}\}}\Big{|}\mathcal{A}\neq \emptyset\right]\overset{(a)}{\geq}\mathbb{E}\left[\frac{1}{m}\mathds{1}_{\{j \in\mathcal{A}\}}\mathds{1}_{\{j^{\prime}\in\mathcal{A}\}}\Big{|}\mathcal{A} \neq\emptyset\right]=\frac{p_{j}p_{j^{\prime}}}{m}\geq\frac{\delta^{2}}{m},\] where inequality \((a)\) holds because \(|\mathcal{A}|\leq m\) ;
* For \(j=j^{\prime}\), it holds that \[\mathbb{E}\left[W_{jj^{\prime}}^{2}\mid\mathcal{A}\neq\emptyset\right] =\mathbb{E}\left[\frac{1}{|\mathcal{A}|}\mathds{1}_{\{j\in \mathcal{A}\}}+\left(1-\mathds{1}_{\{j\in\mathcal{A}\}}\right)\Big{|} \mathcal{A}\neq\emptyset\right]\] \[\geq\mathbb{E}\left[\frac{1}{m}\left[\mathds{1}_{\{j\in\mathcal{ A}\}}+\left(1-\mathds{1}_{\{j\in\mathcal{A}\}}\right)\right]\Big{|}\mathcal{A}\neq \emptyset\right]=\frac{1}{m}\geq\frac{\delta^{2}}{m}.\] Recall that \(M=\mathbb{E}\left[W^{2}\right]\). Next, we show that each element of \(M\) is lower bounded.

We note that \(\rho(t)=\lambda_{2}(M)\), where \(\lambda_{2}\) is the second largest eigenvalue of matrix \(M\). A Markov chain with \(M\) as the transition matrix is ergodic as the chain is (1) _irreducible_: \(M_{jj^{\prime}}\geq\frac{\delta^{2}}{m}\left[1-\left(1-c\right)^{m}\right]>0\) for \(j,j^{\prime}\in[m]\) and (2) _aperiodic_ (it has self-loops). In addition, \(W\) matrix is by definition doubly-stochastic. Hence, \(M\) has a uniform stationary distribution \(\pi=\mathds{1}^{\top}/m\). Furthermore, the irreducible Markov chain is reversible since it holds for all the states that \(\pi_{i}M_{ij}=\pi_{j}M_{ji}\). The conductance \(\Phi\) of a reversible Markov chain [19] with a transition matrix \(M\) can be bounded by

\[\Phi(M)=\min_{\sum_{i\in\mathcal{S}}\pi_{i}\leq\frac{1}{2}}\frac{\sum_{i\in \mathcal{S},j\notin\mathcal{S}}\pi_{i}M_{ij}}{\sum_{i\in\mathcal{S}}\pi_{i}} \geq\frac{\left(\frac{\delta}{m}\right)^{2}\left[1-\left(1-\delta\right)^{m} \right]|\mathcal{S}|\left|\bar{\mathcal{S}}\right|}{\frac{|\mathcal{S}|}{m}}= \frac{\delta^{2}\left[1-\left(1-\delta\right)^{m}\right]}{m}\left|\bar{ \mathcal{S}}\right|,\]

where \(\left|\bar{\mathcal{S}}\right|=m-|\mathcal{S}|\geq\frac{m}{2}.\) From Cheeger's inequality, we know that \(\frac{1-\lambda_{2}}{2}\leq\Phi(M)\leq\sqrt{2\left(1-\lambda_{2}\right)}\). Finally, we have

\[\Phi(M)\geq\frac{\delta^{2}\left[1-\left(1-\delta\right)^{m}\right]}{m}\left| \bar{\mathcal{S}}\right|\geq\frac{\delta^{2}\left[1-\left(1-\delta\right)^{m} \right]}{2}.\]

Thus, \(\rho(t)=\lambda_{2}\leq 1-\frac{\Phi^{2}(M)}{2}\leq 1-\frac{\delta^{4}[1-\left(1- \delta\right)^{m}]^{2}}{8}.\)Convergence Error of \(\bar{\bm{z}}^{t}\) (Theorem 1)

In the sequel, we recall and assume the following learning rate conditions in (11):

\[\eta_{l}\eta_{g}\leq\frac{\left(1-\sqrt{\rho}\right)\delta}{80s(L+1)\left(\sqrt{ \rho}+1\right)\sqrt{\left(\beta^{2}+1\right)\left(1+L^{2}\right)}};\;\eta_{l} \leq\frac{\delta}{200sL\sqrt{\left(\beta^{2}+1\right)\left(1+L^{2}\right)}}.\]

Recall that \(\delta_{\max}\triangleq\max_{i\in[m],t\in[T]}p_{i}^{t}\) and \(F^{\star}\triangleq\min_{\bm{z}}F(\bm{x})\).

**Proof of Theorem 1.** Take expectation over all the randomness, plug in Lemma 6 and Proposition 2.

By telescoping sum, it holds that

\[\frac{\mathbb{E}\left[F^{\star}-F(\bar{\bm{z}}^{0})\right]}{T} \leq-\frac{\eta_{l}\eta_{g}s}{4}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[ \left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]+\frac{2\eta_{l}^{2} \eta_{g}^{2}sL\delta_{\max}\sigma^{2}}{m^{2}T}\sum_{t=0}^{T-1}\sum_{i=1}^{m} \sum_{p=-1}^{t-1}\mathbb{E}\left[\mathds{1}_{\left\{\tau_{i}(t)=p\right\}} \right](t-p)^{2}\] \[+2.2\eta_{l}\eta_{g}sL^{2}\frac{1}{mT}\sum_{t=0}^{T-1}\sum_{i=1}^ {m}\mathbb{E}\left[\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2}\right]\] (30) \[+\frac{\eta_{l}\eta_{g}sL^{2}}{2mT}\sum_{t=0}^{T-1}\sum_{i=1}^{m} \mathbb{E}\left[\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}\right]\] (31) \[+\frac{35\eta_{g}\eta_{l}^{3}s^{3}L^{2}}{mT}\sum_{t=0}^{T-1}\sum_{ i=1}^{m}\sum_{p=-1}^{t-1}\mathbb{E}\left[\mathds{1}_{\left\{\tau_{i}(t)=p\right\}} \right](t-p)^{2}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{x}_{i}^{p+1})\right\|_ {2}^{2}\right].\] (32)

Next, we bound (30), (31) and (32), respectively. First, we show that

\[\frac{1}{mT}\sum_{t=0}^{T-1}\sum_{i=1}^{m}\mathbb{E}\left[\left\| \nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}\right]\] \[\leq 3\left[1+\frac{40\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}L^{2}}{(1- \sqrt{\rho})^{2}}\right]\zeta^{2}+3\left(\beta^{2}+1\right)\left[1+\frac{40 \rho s^{2}\eta_{l}^{2}\eta_{g}^{2}L^{2}}{(1-\sqrt{\rho})^{2}}\right]\frac{1}{ T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2} \right]+\frac{9\rho s\eta_{l}^{2}\eta_{g}^{2}L^{2}}{(1-\sqrt{\rho})^{2}\delta ^{2}}\sigma^{2},\] (33)

where the last inequality follows from Lemma 6.

For (30), we have

\[2.2\eta_{l}\eta_{g}sL^{2}\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m} \sum_{i=1}^{m}\mathbb{E}\left[\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2} ^{2}\right] \leq\frac{4.4\eta_{l}^{3}\eta_{g}^{3}s^{3}L^{2}}{\delta^{2}}\frac {1}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\nabla F _{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}\right]\] \[\leq\frac{s^{2}\eta_{l}^{3}\eta_{g}^{3}L^{2}}{2\delta^{2}}\sigma^ {2}+\frac{14\eta_{l}^{3}\eta_{g}^{3}s^{3}L^{2}}{\delta^{2}}\left(1+\frac{40\eta _{l}^{2}\eta_{g}^{2}\rho s^{2}L^{2}}{(1-\sqrt{\rho})^{2}}\right)\zeta^{2}\] \[\quad+\frac{14\eta_{l}^{3}\eta_{g}^{3}s^{3}L^{2}}{\delta^{2}} \left[\left(\beta^{2}+1\right)+\frac{40\eta_{l}^{2}\eta_{g}^{2}\rho s^{2}L^{2} }{(1-\sqrt{\rho})^{2}}\right]\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\| \nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right],\]

where the last inequality holds due to (33). For (31), we similarly have

\[\frac{\eta_{l}\eta_{g}sL^{2}}{2mT}\sum_{t=0}^{T-1}\sum_{i=1}^{m} \mathbb{E}\left[\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}\right] \leq\frac{1.5\rho s^{2}\eta_{l}^{3}\eta_{g}^{3}L^{2}}{(1-\sqrt{ \rho})^{2}\delta^{2}}\sigma^{2}+\frac{20\rho s^{3}\eta_{l}^{3}\eta_{g}^{3}L^{2} }{(1-\sqrt{\rho})^{2}}\zeta^{2}\] \[\quad+\frac{20\rho s^{3}\eta_{l}^{3}\eta_{g}^{3}L^{2}\left(\beta^ {2}+1\right)}{(1-\sqrt{\rho})^{2}}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[ \left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right].\]For (32), we have

\[35\eta_{g}\eta_{l}^{3}s^{3}L^{2}\frac{1}{T}\sum_{t=0}^{T-1}\frac{1} {m}\sum_{i=1}^{m}\sum_{p=-1}^{t-1}\mathbb{E}\left[\mathds{1}_{\{\tau_{i}(t)=p\}} \right](t-p)^{2}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{x}_{i}^{p+1})\right\|_{2 }^{2}\right]\] \[\leq\frac{70\eta_{g}\eta_{l}^{3}s^{3}L^{2}}{mT\delta^{2}}\sum_{t=0 }^{T-1}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{x}_{i}^{t})\right\| _{2}^{2}\right]\] \[\leq\frac{140\eta_{g}\eta_{l}^{3}s^{3}L^{4}}{mT\delta^{2}}\sum_{t =0}^{T-1}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t} \right\|_{2}^{2}\right]+\frac{140\eta_{g}\eta_{l}^{3}s^{3}L^{2}}{mT\delta^{2}} \sum_{t=0}^{T-1}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{z}_{i}^{ t})\right\|_{2}^{2}\right]\] \[\leq\left(1+\frac{2\eta_{l}^{2}\eta_{g}^{3}s^{2}L^{2}}{\delta^{2 }}\right)\left(\frac{2}{\delta^{2}}\right)\frac{70\eta_{g}\eta_{l}^{3}s^{3}L^{ 2}}{mT}\sum_{t=0}^{T-1}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\nabla F_{i}(\bm{z }_{i}^{t})\right\|_{2}^{2}\right]\] \[\overset{(a)}{\leq}\left(\frac{2}{\delta^{2}}\right)\frac{71\eta _{g}\eta_{l}^{3}s^{3}L^{2}}{mT}\sum_{t=0}^{T-1}\sum_{i=1}^{m}\mathbb{E}\left[ \left\|\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}\right]\] \[\overset{(b)}{\leq}\frac{426\eta_{g}\eta_{l}^{3}s^{3}L^{2}}{ \delta^{2}}\left[1+\frac{40\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}L^{2}}{(1-\sqrt{ \rho})^{2}}\right]\zeta^{2}+\frac{426\eta_{g}\eta_{l}^{3}s^{3}L^{2}}{\delta^{2 }}\left(\beta^{2}+1\right)\left[1+\frac{40\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}L^ {2}}{(1-\sqrt{\rho})^{2}}\right]\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[ \left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]\] \[+\frac{\eta_{g}\eta_{l}^{3}s^{3}L^{2}\sigma^{2}}{2\delta^{2}},\]

where inequality \((a)\) holds because of (11), inequality \((b)\) holds because of (33).

Putting (30), (31) and (32) together and plugging them back into the telescoping sum, it holds that

\[\frac{\mathbb{E}\left[F^{\star}-F(\bar{\bm{z}}^{0})\right]}{T}\] \[\leq-\left(\frac{\eta_{l}\eta_{g}s}{4}-\frac{14\left(\beta^{2}+1 \right)\eta_{l}^{3}\eta_{g}^{3}s^{3}L^{2}\left(1+L^{2}\right)}{\delta^{2}}- \frac{20\rho s^{3}\eta_{l}^{3}\eta_{g}^{3}L^{2}\left(\beta^{2}+1\right)}{(1- \sqrt{\rho})^{2}}\right)\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\| \nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]\] \[-\left(-\frac{426\eta_{g}\eta_{l}^{3}s^{3}L^{2}\left(\beta^{2}+1 \right)\left(1+L^{2}\right)}{\delta^{2}}\right)\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}\left[\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]\] \[+\frac{4\eta_{l}^{2}\eta_{g}^{2}sL\delta_{\max}\sigma^{2}}{m \delta^{2}}+\left(\frac{\eta_{l}^{3}\eta_{g}^{3}s^{2}L^{2}\sigma^{2}}{2\delta^ {2}}+\frac{1.5\rho s^{2}\eta_{l}^{3}\eta_{g}^{3}L^{2}}{(1-\sqrt{\rho})^{2} \delta^{2}}\sigma^{2}+\frac{\eta_{g}\eta_{l}^{3}s^{3}L^{2}\sigma^{2}}{2\delta^ {2}}\right)\] \[+\frac{15\eta_{l}^{3}\eta_{g}^{3}s^{3}L^{2}\zeta^{2}}{\delta^{2}}+ \frac{20\rho s^{3}\eta_{l}^{3}\eta_{g}^{3}L^{2}}{(1-\sqrt{\rho})^{2}}\zeta^{2}+ \frac{430\eta_{g}\eta_{l}^{3}s^{3}L^{2}\zeta^{2}}{\delta^{2}}\] \[\leq-\frac{\eta\eta_{l}\eta_{g}s}{6}\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}\left[\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]\] \[+\frac{4\eta_{l}^{2}\eta_{g}^{2}sL\delta_{\max}\sigma^{2}}{m \delta^{2}}+\left(\frac{\eta_{l}^{3}\eta_{g}^{3}s^{2}L^{2}\sigma^{2}}{2\delta^ {2}}+\frac{1.5\rho s^{2}\eta_{l}^{3}\eta_{g}^{3}L^{2}}{(1-\sqrt{\rho})^{2}\delta ^{2}}\sigma^{2}+\frac{\eta_{g}\eta_{l}^{3}s^{3}L^{2}\sigma^{2}}{2\delta^{2}}\right)\] \[+\frac{15\eta_{l}^{3}\eta_{g}^{3}s^{3}L^{2}\zeta^{2}}{\delta^{2}}+ \frac{20\rho s^{3}\eta_{l}^{3}\eta_{g}^{3}L^{2}}{(1-\sqrt{\rho})^{2}}\zeta^{2}+ \frac{430\eta_{g}\eta_{l}^{3}s^{3}L^{2}\zeta^{2}}{\delta^{2}},\]

where the last inequality holds because of (11).

Combining the above and rearranging the terms, we get

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{ \boldsymbol{z}}^{t})\right\|_{2}^{2}\right] \leq\frac{6\left(F(\bar{\boldsymbol{z}}^{0})-F^{\star}\right)}{ \eta_{l}\eta_{g}sT}\] \[\quad+\frac{24\eta_{l}\eta_{g}L\delta_{\max}\sigma^{2}}{m\delta^ {2}}+\left(\frac{3\eta_{l}^{2}\eta_{g}^{2}sL^{2}\sigma^{2}}{\delta^{2}}+\frac{ 9\rho s\eta_{l}^{2}\eta_{g}^{2}L^{2}}{(1-\sqrt{\rho})^{2}\delta^{2}}\sigma^{2} +\frac{3\eta_{l}^{2}s^{2}L^{2}\sigma^{2}}{\delta^{2}}\right)\] \[\quad+\frac{90\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\zeta^{2}}{ \delta^{2}}+\frac{120\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}L^{2}}{(1-\sqrt{\rho}) ^{2}}\zeta^{2}+\frac{2580\eta_{l}^{2}s^{2}L^{2}\zeta^{2}}{\delta^{2}}\] \[\leq\frac{6\left(F(\bar{\boldsymbol{z}}^{0})-F^{\star}\right)}{ \eta_{l}\eta_{g}sT}+\frac{24\eta_{l}\eta_{g}L\delta_{\max}\sigma^{2}}{m\delta ^{2}}+\frac{15\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\sigma^{2}}{(1-\sqrt{\rho})^{ 2}\delta^{2}}+\frac{2800\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\zeta^{2}}{\delta^{2 }(1-\sqrt{\rho})^{2}},\]

where the last inequality holds because \(\rho<1\). In terms of asymptotics, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{ \boldsymbol{z}}^{t})\right\|_{2}^{2}\right]\lesssim\frac{\left(F(\bar{ \boldsymbol{z}}^{0})-F^{\star}\right)}{\eta_{l}\eta_{g}sT}+\frac{\eta_{l}\eta_ {g}L\sigma^{2}}{m}\frac{\delta_{\max}}{\delta^{2}}+\eta_{l}^{2}\eta_{g}^{2}s^{ 2}L^{2}\left(\frac{\sigma^{2}+\zeta^{2}}{\delta^{2}\left(1-\sqrt{\rho}\right) ^{2}}\right),\]

where we use the convention that \(\eta_{g}\geq 1\) for ease of presentation.

Convergence Rate of \(\bar{\bm{x}}^{t}\) (Corollary 1)

### Convergence error of Algorithm 1

**Corollary 2** (Convergence error of \(\bm{x}_{i}^{t}\)).: _Suppose learning rates conditions in (11) are met for \(\eta_{l}\) and \(\eta_{g}\), and Assumptions 1, 2, 3 and 4 hold for \(T\geq 1\), it holds that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{x}}^{t}) \right\|_{2}^{2}\right]\lesssim\frac{\left(F(\bar{\bm{x}}^{0})-F^{\star} \right)}{\eta_{l}\eta_{g}sT}+\frac{\eta_{l}\eta_{g}L\sigma^{2}}{m}\frac{\delta _{\max}}{\delta^{2}}+\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\left(\frac{\sigma^{2}+ \zeta^{2}}{\delta^{2}\left(1-\sqrt{\rho}\right)^{2}}\right),\]

Proof of Corollary 2.: \[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{ \bm{x}}^{t})\right\|_{2}^{2}\right]\leq\frac{3}{T}\sum_{t=0}^{T-1}\mathbb{E} \left[\left\|\nabla F(\bar{\bm{x}}^{t})-\nabla F(\bar{\bm{z}}^{t})\right\|_{2 }^{2}\right]+\frac{3}{2T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar {\bm{z}}^{t})\right\|_{2}^{2}\right]\] \[\overset{(a)}{\leq}\frac{3L^{2}}{T}\sum_{t=0}^{T-1}\mathbb{E} \left[\left\|\bar{\bm{x}}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}\right]+\frac{3 }{2T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{z}}^{t})\right\| _{2}^{2}\right]\] \[\overset{(b)}{\leq}\frac{3L^{2}}{T}\sum_{t=0}^{T-1}\frac{1}{m} \sum_{i=1}^{m}\mathbb{E}\left[\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2 }^{2}\right]+\frac{3}{2T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar {\bm{z}}^{t})\right\|_{2}^{2}\right]\] \[\leq 3\left(\frac{2}{\delta^{2}}\right)\frac{\eta_{l}^{2}\eta_{g}^ {2}s^{2}L^{2}}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[ \left\|\nabla F_{i}(\bm{z}_{i}^{t})\right\|_{2}^{2}\right]+\frac{3}{2T}\sum_{t =0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2} \right],\]

where inequality \((a)\) follows from Appendix D.2, inequality \((b)\) follows from Assumption 2.

Further plug in Proposition 3,

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{ \bm{x}}^{t})\right\|_{2}^{2}\right] \leq\frac{3}{2T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F( \bar{\bm{z}}^{t})\right\|_{2}^{2}\right]+9\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2} \left(\frac{2}{\delta^{2}}\right)\left(\beta^{2}+1\right)\frac{1}{T}\sum_{t=0}^ {T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]\] \[\quad+9\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{4}\left(\frac{2}{\delta^{2 }}\right)\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[ \left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}\right]+9\eta_{l}^{2} \eta_{g}^{2}s^{2}L^{2}\left(\frac{2}{\delta^{2}}\right)\zeta^{2}.\]

Finally, plug in Lemma 6.

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{ \bm{x}}^{t})\right\|_{2}^{2}\right] \leq\left(\frac{3}{2}+9\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\left( \frac{2}{\delta^{2}}\right)\left(\beta^{2}+1\right)\frac{90}{80^{2}}\right) \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{z}}^{t}) \right\|_{2}^{2}\right]\] \[\leq\frac{12\left(F(\bar{\bm{z}}^{0})-F^{\star}\right)}{\eta_{l} \eta_{g}sT}+\frac{48\eta_{l}\eta_{g}L\delta_{\max}\sigma^{2}}{m\delta^{2}}+ \frac{31\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\sigma^{2}}{(1-\sqrt{\rho})^{2} \delta^{2}}+\frac{5600\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\zeta^{2}}{(1-\sqrt{ \rho})^{2}\delta^{2}},\]

where the last inequality holds because \(\rho<1\). In terms of asymptotics, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{x}}^{t}) \right\|_{2}^{2}\right]\lesssim\frac{\left(F(\bar{\bm{x}}^{0})-F^{\star} \right)}{\eta_{l}\eta_{g}sT}+\frac{\eta_{l}\eta_{g}L\sigma^{2}}{m}\frac{\delta_{ \max}}{\delta^{2}}+\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\left(\frac{\sigma^{2}+ \zeta^{2}}{\delta^{2}(1-\sqrt{\rho})^{2}}\right),\]

where we use the convention that \(\eta_{g}\geq 1\) for ease of presentation.

### Convergence rate of Algorithm 1

**Proof of Corollary 1.** Choose step-size as \(\eta_{l}=\frac{1}{\sqrt{T}sL}\), \(\eta_{g}=\sqrt{s\delta m}\) such that learning rate conditions in (11) are met, it holds that

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{x}}^{t}) \right\|_{2}^{2}\right]\lesssim\frac{L\left(F(\bar{\bm{x}}^{0})-F^{\star} \right)}{\sqrt{s\delta mT}}+\frac{\delta_{\max}}{\delta^{\frac{3}{2}}\sqrt{ smT}}\sigma^{2}+\frac{sm}{T}\left(\frac{\sigma^{2}+\zeta^{2}}{\delta(1-\sqrt{ \rho})^{2}}\right).\]

\(\Box\)Additional Results and Interpretations

### Consensus error of Algorithm 1

**Corollary 3** (Consensus error of \(\bm{x}_{i}^{t}\)).: _Suppose learning rates conditions are met in (11) for \(\eta_{l}\) and \(\eta_{g}\), and Assumptions 1, 2, 3 and 4 hold for \(T\geq 1\), it holds that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E} \left[\left\|\bm{x}_{i}^{t}-\bar{\bm{x}}^{t}\right\|_{2}^{2}\right]\lesssim \frac{\left(F(\bar{\bm{x}}^{0})-F^{\star}\right)}{\eta_{l}\eta_{g}sT}+\frac{ \eta_{l}\eta_{g}L\sigma^{2}}{m}\frac{\delta_{\max}}{\delta^{2}}\] \[+\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\left(\frac{\sigma^{2}+\zeta^{ 2}}{\delta^{2}}\right)\left[1+\frac{\rho}{\left(1-\sqrt{\rho}\right)^{2}} \right],\]

**Proof of Corollary 3.**

\[\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\left\|\bm{x} _{i}^{t}-\bar{\bm{x}}^{t}\right\|_{2}^{2}=\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{ m}\sum_{i=1}^{m}\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}+\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}+ \bar{\bm{z}}^{t}-\bar{\bm{x}}^{t}\right\|_{2}^{2}\] \[\overset{(a)}{\leq}\frac{1}{T}\sum_{t=0}^{T-1}\frac{3}{m}\sum_{i= 1}^{m}\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2}+\frac{1}{T}\sum_{t= 0}^{T-1}\frac{3}{m}\sum_{i=1}^{m}\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\| _{2}^{2}+\frac{1}{T}\sum_{t=0}^{T-1}3\left\|\bar{\bm{z}}^{t}-\bar{\bm{x}}^{t} \right\|_{2}^{2}\] \[\overset{(b)}{\leq}\frac{1}{T}\sum_{t=0}^{T-1}\frac{3}{m}\sum_{i= 1}^{m}\left\|\bm{x}_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2}+\frac{1}{T}\sum_{t= 0}^{T-1}\frac{3}{m}\sum_{i=1}^{m}\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\| _{2}^{2}+\frac{1}{T}\sum_{t=0}^{T-1}\frac{3}{m}\sum_{i=1}^{m}\left\|\bm{z}_{i} ^{t}-\bm{x}_{i}^{t}\right\|_{2}^{2}\] \[=\frac{1}{T}\sum_{t=0}^{T-1}\frac{6}{m}\sum_{i=1}^{m}\left\|\bm{x }_{i}^{t}-\bm{z}_{i}^{t}\right\|_{2}^{2}+\frac{1}{T}\sum_{t=0}^{T-1}\frac{3}{m }\sum_{i=1}^{m}\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2},\]

where inequalities \((a)\) and \((b)\) follow from Jensen's inequality. Plug in Proposition 2 and take expectation over all the randomness, we get

\[\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E} \left[\left\|\bm{x}_{i}^{t}-\bar{\bm{x}}^{t}\right\|_{2}^{2}\right]\leq\frac{ 36\eta_{l}^{2}\eta_{g}^{2}s^{2}}{\delta^{2}}\left(\beta^{2}+1\right)\frac{1}{T }\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2} ^{2}\right]\] \[\quad+\frac{36\eta_{l}^{2}\eta_{g}^{2}s^{2}}{\delta^{2}}\zeta^{2} +\left(3+\frac{36\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}}{\delta^{2}}\right)\frac{ 1}{m}\sum_{i=1}^{m}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\bm{z}_{i} ^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}\right]\] \[\quad\leq\frac{36\eta_{l}^{2}\eta_{g}^{2}s^{2}}{\delta^{2}} \left(\beta^{2}+1\right)\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\| \nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]+\frac{36\eta_{l}^{2}\eta_{g} ^{2}s^{2}}{\delta^{2}}\zeta^{2}\] \[\quad+\frac{4}{m}\sum_{i=1}^{m}\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}\left[\left\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}\right],\]

where the last inequality holds because of learning rate condition in (11). Next, plug in Lemma 6:

\[\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E} \left[\left\|\bm{x}_{i}^{t}-\bar{\bm{x}}^{t}\right\|_{2}^{2}\right]\leq\frac{3 6\eta_{l}^{2}\eta_{g}^{2}s^{2}}{\delta^{2}}\left(\beta^{2}+1\right)\frac{1}{T }\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2} ^{2}\right]\] \[\quad+\frac{36\eta_{l}^{2}\eta_{g}^{2}s^{2}}{\delta^{2}}\zeta^{2} +\frac{4}{m}\sum_{i=1}^{m}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\bm{z} _{i}^{t}-\bar{\bm{z}}^{t}\right\|_{2}^{2}\right]\] \[\quad\leq\frac{36\eta_{l}^{2}\eta_{g}^{2}s^{2}}{\delta^{2}}\left( \beta^{2}+1\right)\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F( \bar{\bm{z}}^{t})\right\|_{2}^{2}\right]+\frac{1}{4T}\sum_{t=0}^{T-1}\mathbb{E} \left[\left\|\nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]\] \[\quad+\frac{36\eta_{l}^{2}\eta_{g}^{2}s^{2}}{\delta^{2}}\zeta^{2} +\frac{12\rho s\eta_{l}^{2}\eta_{g}^{2}}{(1-\sqrt{\rho})^{2}\delta^{2}}\sigma^{2} +\frac{160\rho s^{2}\eta_{l}^{2}\eta_{g}^{2}}{(1-\sqrt{\rho})^{2}}\zeta^{2}\] \[\quad\leq\frac{1}{2T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\| \nabla F(\bar{\bm{z}}^{t})\right\|_{2}^{2}\right]+\frac{12\rho s\eta_{l}^{2} \eta_{g}^{2}}{(1-\sqrt{\rho})^{2}\delta^{2}}\sigma^{2}+\frac{36\eta_{l}^{2} \eta_{g}^{2}s^{2}}{\delta^{2}}\zeta^{2}+\frac{160\rho s^{2}\eta_{l}^{2}\eta_{g}^{ 2}}{(1-\sqrt{\rho})^{2}}\zeta^{2}.\]Finally, we plug in Theorem 1

\[\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\bm{x} _{i}^{t}-\bar{\bm{x}}^{t}\right\|_{2}^{2}\right]\leq\frac{3\left(F(\bar{\bm{x}} ^{0})-F^{\star}\right)}{\eta_{l}\eta_{g}sT}+\frac{12\eta_{l}\eta_{g}L\delta_{ \max}\sigma^{2}}{m\delta^{2}}+\frac{28s^{2}\eta_{l}^{2}\eta_{g}^{2}L^{2}}{ \delta^{2}(1-\sqrt{\rho})^{2}}\sigma^{2}+\frac{1600\eta_{l}^{2}\eta_{g}^{2}s^{ 2}L^{2}}{\delta^{2}(1-\sqrt{\rho})^{2}}\zeta^{2},\]

where we use the fact that \(\bar{\bm{z}}^{0}=\bar{\bm{x}}^{0}\) and \(\rho<1\), and the convention that \(\eta_{g}\geq 1\) and \(L\geq 1\) for ease of presentation.

In terms of asymptotics, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[\left\|\bm{ x}_{i}^{t}-\bar{\bm{x}}^{t}\right\|_{2}^{2}\right]\lesssim\frac{\left(F(\bar{\bm{x}} ^{0})-F^{\star}\right)}{\eta_{l}\eta_{g}sT}+\frac{\eta_{l}\eta_{g}L\sigma^{2}} {m}\frac{\delta_{\max}}{\delta^{2}}+\eta_{l}^{2}\eta_{g}^{2}s^{2}L^{2}\left( \frac{\sigma^{2}+\zeta^{2}}{\delta^{2}(1-\sqrt{\rho})^{2}}\right).\]

### Orders of the asymptotic rates

From Theorem 1, Corollary 2, Corollary 3, it is easy to see from the theorem statements that they are all of the same asymptotic order, i.e.,

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F(\bar{\bm{x}}^{t})\|_{2}^{2}] \asymp\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}[\|\bm{x}_ {i}^{t}-\bar{\bm{x}}^{t}\|_{2}^{2}]\asymp\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E }[\|\nabla F(\bar{\bm{z}}^{t})\|_{2}^{2}].\]

In addition, by applying learning rate conditions in (11) to Lemma 6 and Proposition 2, we can also see that

\[\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}[\|\bm{x}_{i}^{ t}-\bm{z}_{i}^{t}\|_{2}^{2}]\asymp\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{m} \sum_{i=1}^{m}\mathbb{E}[\|\bm{z}_{i}^{t}-\bar{\bm{z}}^{t}\|_{2}^{2}]\asymp\frac {1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F(\bar{\bm{z}}^{t})\|_{2}^{2}].\]

Therefore, we conclude that (12), (14) and (15) hold.

## Appendix J Numerical Experiments

### Code

The code for reproducing our experiments is available at https://github.com/mingxiang12/FedAWE.

### Experimental setups

**Hardware and Software Setups.**

* **Hardware.** The simulations are performed on a private cluster with 64 CPUs, 500 GB RAM and 8 NVIDIA A5000 GPU cards.
* **Software.** We code the experiments based on PyTorch 1.13.1 [40] and Python 3.7.16.

**Neural Network and Hyper-parameter Specifications.** Table 6 specifies details of the structures of the convolutional neural network and training. We initialize CNNs using the Kaiming initialization. The initial local learning rate \(\eta_{0}\) and the global learning rate \(\eta_{g}\) are searched, based on the best performance after \(500\) global rounds, over two grids \(\{0.1,0.05,0.01,0.005,0.001,0.0005\}\) and \(\{0.5,1,1.5,5,10,50\}\), respectively. The results are presented in Table 7.

The difference between FedAvg over active clients and FedAvg over all clients is that the latter counts the contributions of unavailable clients as **0**'s. We set \(\beta=0.001\) for F3AST[44], which is tuned over a grid of \(\{0.1,0.05,0.01,0.005,0.001,0.0005\}\). In addition, as recommended by [55], we choose \(K=50\) in FedAU without further specification. We train CNNs on all datasets for \(2000\) rounds. Fig. 3 adopts the same hyperparameter setups, yet with only \(1000\) training rounds.

**Datasets and Data Heterogeneity.**

_Datasets._ All the datasets we evaluate contain 10 classes of images. Some data enhancement tricks that are standard in training image classifiers are applied during training. Specifically, we apply

\begin{table}
\begin{tabular}{c c c c} \hline
**Datasets** & **SVHN** & **CIFAR-10** & **CINIC-10** \\ \hline Neural network & CNN & CNN & CNN \\  & \(\mathbf{C}(3,32)-\mathbf{R}-\mathbf{M}-\) & \(\mathbf{C}(3,32)-\mathbf{R}-\mathbf{M}-\) & \(\mathbf{C}(3,32)-\mathbf{R}-\mathbf{M}-\) \\ Model architecture\({}^{*}\) & \(\mathbf{C}(32,32)-\mathbf{R}-\mathbf{M}\) & \(\mathbf{C}(32,32)-\mathbf{R}-\mathbf{M}\) & \(\mathbf{C}(32,32)-\mathbf{R}-\mathbf{M}\) \\  & \(-\mathbf{L}(128)-\mathbf{R}-\) & \(-\mathbf{L}(256)-\mathbf{R}-\) & \(-\mathbf{D}-\mathbf{L}(512)-\mathbf{R}-\) \\  & \(\mathbf{L}(10)\) & \(\mathbf{L}(64)-\mathbf{R}-\) & \(\mathbf{D}-\mathbf{L}(256)-\mathbf{R}-\) \\  & & \(\mathbf{L}(10)\) & \(\mathbf{D}-\mathbf{L}(10)\) \\ \hline Loss function & \multicolumn{3}{c}{Cross-entropy loss} \\ Local learning rate \(\eta_{l}\) & \(\eta_{l}=\frac{\eta_{0}}{\sqrt{t/10+1}}\), where \(t\) denotes the global round. \\ \multicolumn{3}{c}{Number of local steps \(s\)} & 10 \\ Number of global rounds \(T\) & 2000 \\ \hline Batch size & 128 \\ \hline \end{tabular}

* \(\mathbf{C}\)(# in-channel, # out-channel): a 2D convolution layer (kernel size 3, stride 1, padding 1); **R**: ReLU activation function; **M**: a 2D max-pool layer (kernel size 2, stride 2); **L**: (# outputs): a fully-connected linear layer; **D**: a dropout layer (probability 0.2).

\end{table}
Table 6: Neural network architecture, loss function, learning rate scheduling, training steps and batch size specifications

Figure 4: An example of data heterogeneity using \(\text{Dirichlet}(\alpha=0.1)\) distribution with \(20\) clients. \(x\)-axis denotes the categories of images, while \(y\)-axis denotes the client index. The size of a circle refers to the proportion of pictures in a given class. The color of a circle distinguishes images with different categories.

random cropping and gradient clipping with a max norm of 0.5 to all dataset trainings. Furthermore, random horizontal flipping is applied to CIFAR-10 and CINIC-10.

One full set of experiments takes about 6 hours on SVHN and CIFAR-10 datasets, while about 10 hours on CINIC-10 dataset.

* **SVHN [37].** The dataset contains 32\(\times\)32 colored images of 10 different number digits. In total, there are 73257 train images and 26032 test images.
* **CIFAR-10 [26].** The dataset contains 32\(\times\)32 colored images of 10 different objects. In total, there are 50000 train images and 10000 test images.
* **CINIC-10[12].** The dataset contains 32\(\times\)32 colored images of 10 different objects. In total, there are 90000 train images and 90000 test images.

_Data heterogeneity._ Fig. 4 visualizes an example of 20 clients, the size of each circle corresponds to the relative proportion of images from a specific class. The larger the circle, the greater the share of images associated with that particular class. Moreover, \(\alpha\) controls the heterogeneity of the data such that a greater \(\alpha\) entails a more non-i.i.d. local data distribution and vice versa.

### Non-stationary client unavailability dynamics and visualizations.

**Client unavailability dynamics and visualizations.** As specified in Section 7, we consider a total of four client unavailable dynamics in the form of \(p_{i}^{t}=p_{i}\cdot f_{i}(t)\), where \(p_{i}=\langle\nu_{i},\phi\rangle\), \(\nu_{i}\sim\mathsf{Dirichlet}(\alpha)\) and \(\phi\) is the distribution to characterize the uneven contributions of each image class. In detail, each element \([\phi]_{c}\) is drawn from a uniform distribution \(\mathsf{Uniform}(0,\mathbf{\Phi}_{c})\). We set \(\mathbf{\Phi}_{c}=1\) for the first five image classes and \(\mathbf{\Phi}_{c^{\prime}}=0.5\) for the remaining five image classes. Fig. 5 plots one resulting \(p_{i}\)'s example, wherein \(p_{i}\)'s are heterogeneous across clients.

Next, we formally introduce \(f_{i}(t)\)'s under each dynamic.

* Stationary: \(f_{i}(t)\triangleq 1\);
* Non-stationary with staircase trajectory: \[f_{i}(t)\triangleq\mathds{1}_{\{t\in[t_{0},t_{0}+P/2)\}}+0.4\cdot\mathds{1 }_{\{t\in[t_{0}+P/2,t_{0}+P\}}),\] where \(P\) defines a period, \(t_{0}\in\{0,P,2P,3P,\ldots\}\).
* Non-stationary with sine trajectory: \[f_{i}(t)\triangleq\gamma\sin(2\pi/P\cdot t)+(1-\gamma),\] where \(\gamma\) signifies the degree of non-stationary.
* Non-stationary with interleaved sine trajectory: \[f_{i}(t)\triangleq g_{i}(t)\cdot\mathds{1}_{\{p_{i},g_{i}(t)\geq\delta_{0} \}},\] where \(g_{i}(t)\triangleq\gamma\sin(2\pi/P\cdot t)+(1-\gamma)\) and \(\delta_{0}=0.1\) defines a cutting-off lower bound. Specifically, \(\delta_{0}\) cuts off the sine curve and brings in a period of zero-valued probabilities. As different clients have different \(p_{i}\)'s, the cut-off points are not synchronized among clients, leading to additional availability heterogeneity.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Algorithms**} & \multicolumn{2}{c}{FedAvg} & \multicolumn{2}{c}{FedAvg} & \multicolumn{2}{c}{FedAvg} & \multicolumn{2}{c}{FedAvg} & \multicolumn{2}{c}{FgAdw} & \multicolumn{2}{c}{FgAdw} & \multicolumn{2}{c}{MIFA} & \multicolumn{2}{c}{FedWARP} \\  & _active_ & _known_ & _all_ & & & & & & & & & & & & \\ \hline SVHN & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) \\  & 0.05 & 1.0 & 0.1 & 1.0 & 0.05 & 1.0 & 0.05 & 1.0 & 0.05 & 1.0 & 0.1 & 1.0 & 0.05 & 1.0 & 0.05 & 1.0 \\ \hline CIFAR-10 & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) \\  & 0.05 & 1.0 & 0.1 & 1.0 & 0.05 & 1.0 & 0.05 & 1.0 & 0.05 & 1.0 & 0.1 & 1.0 & 0.05 & 1.0 & 0.05 & 1.0 \\ \hline CINIC-10 & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) & \(\eta_{0}\) & \(\eta_{g}\) \\  & 0.05 & 1.0 & 0.1 & 1.0 & 0.05 & 1.0 & 0.05 & 1.0 & 0.05 & 1.0 & 0.1 & 1.0 & 0.05 & 1.0 & 0.05 & 1.0 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Initial learning rate \(\eta_{0}\) and global learning rate \(\eta_{g}\)

Figure 5: A histogram of one generated \(p_{i}\)’s example with a total of \(m=100\) clients. It can be seen that the majority of \(p_{i}\)’s are below \(0.5\).

We choose \(\gamma=0.3\) and \(P=20\) for all non-stationary dynamics. Next, we visualize the probability trajectories along with sampled client availability in Fig. 6. The plots confirm the intuition that interleaved dynamics is the most difficult one, e.g., no clients are available in the case of \(0.1\) therein.

### Additional results

**Staleness studies.** Table 8 illustrates the first round to reach a targeted test accuracy under non-stationary client availability with sine trajectory. Specifications can be found in the caption. It can be easily checked that, during the initial stage (the first three quarters), FedAWE slightly lags behind FedAvg over active clients. However, when reaching the final stage (the last quarter), FedAWE attains the target accuracy in a comparable or lower number of rounds to FedAvg over active clients in the evaluations on SVHN and CINIC-10 datasets. The slowdown of FedAWE on CIFAR-10 dataset is worth further investigation. In general, we arrive numerically at the conclusion that the staleness incurred by implicit gossiping in FedAWE is mild.

**Training curves.** In this part, we show the training curves of FedAvg over active clients, FedAWE and MIFA. In particular, the presented results of FedAWE are after exponential moving

Figure 6: Examples of client unavailability with probabilistic trajectories. The first row in each sub-figure plots the probabilistic trajectory of each dynamics. The second row visualizes the simulated client availability by using a colored box to denote a client is available in that round. The y-axis is the base probability \(p_{i}\) to construct \(p_{i}^{t}\). In other words, more blank space means that a client is more scarcely available. We simulate the cases where \(p_{i}\in\{0.1,0.5,0.9\}\). The detailed construction of \(p_{i}^{t}\) can be found in Appendix J.3

\begin{table}
\begin{tabular}{c|c c c c|c c c c|c c c c} \hline \hline
**Datasets** & \multicolumn{3}{c|}{**SVHN**} & \multicolumn{3}{c|}{**CIFAR10**} & \multicolumn{3}{c}{**CINIC10**} \\ \hline
**Quarters** & **1/4** & **1/2** & **3/4** & **1** & **1/4** & **1/2** & **3/4** & **1** & **1/4** & **1/2** & **3/4** & **1** \\ \hline
**Test accuracy** & \(20\%\) & \(40\%\) & \(60\%\) & \(80\%\) & \(15\%\) & \(30\%\) & \(45\%\) & \(60\%\) & \(10\%\) & \(20\%\) & \(30\%\) & \(40\%\) \\ \hline
**FedAvg** (_ours_) & 40 & 120 & 200 & 820 & 20 & 60 & 200 & 1360 & 0 & 20 & 120 & 540 \\ FedAvg over _active_ clients & 20 & 80 & 160 & 900 & 10 & 20 & 120 & 1060 & 0 & 20 & 40 & 800 \\ FedAvg over _all_ clients & 100 & 420 & 960 & \(-\) & 20 & 60 & 520 & \(-\) & 0 & 20 & 200 & - \\ FedA�U & 60 & 100 & 160 & 840 & 10 & 20 & 100 & 960 & 0 & 20 & 80 & 460 \\ FAAST & 40 & 120 & 200 & 1080 & 20 & 40 & 160 & 1300 & 0 & 20 & 60 & 540 \\ \hline FedAvg with _known_\(p_{i}^{t}\)’s & 20 & 40 & 100 & 320 & 10 & 20 & 140 & 620 & 0 & 20 & 40 & 400 \\ MIFA (_memory aided_) & 20 & 80 & 140 & 600 & 10 & 20 & 80 & 700 & 0 & 20 & 40 & 240 \\ \hline \hline \end{tabular}
\end{table}
Table 8: The first round to reach a targeted test accuracy under non-stationary of sine trajectory over 3 random seeds. We study the first round to reach \(1/4\), \(1/2\), \(3/4\) and \(1\) of the best test accuracy of each dataset in Table 2, which is rounded up to the nearest \(10\%\) below for ease of presentation. In addition, we sample the mean of test accuracy every 20 global rounds to mitigate noisy progress. Some algorithms may never attain the targeted accuracy due to their inferior performance, where we use “\(-\)” as a placeholder.

average [5] under a parameter \(0.99\). Note that this is to ease down the noisy progress, and for a neat presentation only, the reported results in the main text and ablation studies are all from raw data. Fig. 6(a) plots the train loss and test accuracy from raw data. For example, when compared with Fig. 6(b), EMA eases down the fluctuations but does not change either the trend or the order of algorithm performance results. All train losses are plotted on a logarithmic scale. The results are consistent with Table 2.

**Impact of system-design parameters.** In this part, we study the impact of system-design parameter including the degree of non-stationarity \(\gamma\) and data heterogeneity \(\alpha\) under non-stationary with sine

Figure 7: Missing training curves under non-stationary client unavailability dynamics with sine curve

[MISSING_PAGE_FAIL:46]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have faithfully stated our contributions in both the abstract and introduction.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to Appendix A for details.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The regulatory assumptions are stated in Section 6. Due to space limitations, we are unable to present all the missing proofs and intermediate results in the main text. They are deferred to Appendix. Please refer to Table of Contents for details.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide detailed experimental and the hyperparameter setups in Section 7 and Appendix J.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our evaluations are based on open-accessed datasets that are publically available. An official implementation code is provided through a GitHub link.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experimental setting/details are important parts of reproducing our results. We provide the details in Section 7 and Appendix J to the best of our ability.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our results are averaged over multiple random seeds and accompanied by error bars
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: Please find the software/hardware specifications in Appendix J.2.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The NeurIPS code of ethics is strictly enforced throughout our research.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed broader impacts in Appendix B. We are unaware of any negative impacts.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The existing assets used in this paper has been adequately cited or credited to.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have documented the experiment details in Section 7 and Appendix J.2. In addition, we provide our code with clear details and examples.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects