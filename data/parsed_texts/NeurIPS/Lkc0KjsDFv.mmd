# CS-Isolate: Extracting Hard Confident Examples

by Content and Style Isolation

Yexiong Lin\({}^{1}\) Yu Yao\({}^{2,3}\) Xiaolong Shi\({}^{1}\)

**Mingming Gong\({}^{4}\) Xu Shen\({}^{5}\) Dong Xu\({}^{6}\) Tongliang Liu\({}^{1}\)\({}^{*}\)**

\({}^{1}\)The University of Sydney; \({}^{2}\)Mohamed bin Zayed University of Artificial Intelligence;

\({}^{3}\)Carnegie Mellon University; \({}^{4}\)The University of Melbourne;

\({}^{5}\)Alibaba DAMO Academy; \({}^{6}\)The University of Hong Kong.

Corresponding author: Tongliang Liu (tongliang.liu@sydney.edu.au).

###### Abstract

Label noise widely exists in large-scale image datasets. To mitigate the side effects of label noise, state-of-the-art methods focus on selecting confident examples by leveraging semi-supervised learning. Existing research shows that the ability to extract hard confident examples, which are close to the decision boundary, significantly influences the generalization ability of the learned classifier. In this paper, we find that a key reason for some hard examples being close to the decision boundary is due to the entanglement of style factors with content factors. The hard examples become more discriminative when we focus solely on content factors, such as semantic information, while ignoring style factors. Nonetheless, given only noisy data, content factors are not directly observed and have to be inferred. To tackle the problem of inferring content factors for classification when learning with noisy labels, our objective is to ensure that the content factors of all examples in the same underlying clean class remain unchanged as their style information changes. To achieve this, we utilize different data augmentation techniques to alter the styles while regularizing content factors based on some confident examples. By training existing methods with our inferred content factors, CS-Isolate proves their effectiveness in learning hard examples on benchmark datasets. The implementation is available at https://github.com/tmllab/2023_NeurIPS_CS-isolate.

## 1 Introduction

Large-scale machine learning datasets frequently contain noisy labels, as seen in datasets like ImageNet [9] and Clothing1M [55]. Training deep neural networks with such noisy data would result in poor generalization ability, as these networks can memorize incorrect labels [13; 3].

To mitigate the side effects of label noise, different methods have been proposed [33; 13; 37; 61; 30; 31; 49]. Major existing state-of-the-art methods are based on confident examples selection [61; 2]. Intuitively, those methods first exploit the _memorization effect_, enabling deep neural networks to learn simple patterns shared by the majority of training examples [60; 1; 19]. Since clean labels typically constitute the majority in each noisy class [7; 39], deep neural networks initially fit the training data with correct labels and then progressively fit examples with incorrect labels [13]. To prevent the model from fitting incorrect labels, early stopping is usually employed [44; 3; 50; 4]. Then the _small-loss trick_ is used to extract the confident examples with high certainty [21; 35; 28; 38]. If extracted examples have high quality, the performance of a classifier can be enhanced.

To improve the performance of a classifier, it is crucial to ensure that the confident examples have a distribution similar to that of clean data [36; 32]. Specifically, it is necessary to extract not only confident examples that are far from the decision boundary but those that are close to it. The former examples are easy to be identified and extracted. However, the latter examples, which are close to the decision boundary, often become entangled with mislabeled examples, making them challenging to be identified or extracted. In this paper, we discover that on image datasets, _the entanglement of unhelpful style factors with useful content factors_ is a key reason that leads to the hard examples becoming hard to be classified, and thus these examples are close to the decision boundary.

In Fig. 1, we provide an intuitive understanding of the relationship between hard examples and the entanglement of style factor \(Z_{s}\) and content factor \(Z_{c}\). Let us first assume the underlying content factor \(Z_{c}\) and style factor \(Z_{s}\) are given. In Fig. 0(a), by visualizing \(Z_{s}\) and \(Z_{c}\), we can observe a separation between the examples belonging to underlying clean classes \(0\) and \(1\). In real-world scenarios, however, \(Z_{c}\) and \(Z_{s}\) are not directly available. Instead, existing methods [13; 28] could learn representations \(Z_{1}\) and \(Z_{2}\) from noisy data to extract confident examples. These learned representations, \(Z_{1}\) and \(Z_{2}\), are nonlinear transformations of style factor \(Z_{s}\) and content factor \(Z_{c}\). They do not ensure the disentanglement of \(Z_{s}\) and \(Z_{c}\)[24; 20; 59]. As the example illustrated in Fig. 0(b), both \(Z_{1}\) and \(Z_{2}\) contain the information of the style factor \(Z_{s}\) and the content factor \(Z_{c}\). In the illustrated representation space, examples within the region encircled by red dashed lines are close together. The confident examples in this region are entangled with mislabeled examples, making them challenging to be extracted by existing methods without isolating (disentangling) style and content information in learned representations.

Figure 1: Illustrating the entanglement of content and style factors. The circles on the left side of the dotted line represent examples with the underlying clean label \(Y=0\), while the ones on the right side represent examples with clean label \(Y=1\). The blue filling corresponds to the noisy label \(\tilde{Y}=0\), and the yellow filling corresponds to the noisy label \(\tilde{Y}=1\). Black outlines indicate that the labels of examples are correct, whereas red outlines indicate that the labels of examples are incorrect. In (a), we visualize the underlying style factor \(Z_{s}\) and the underlying content factor \(Z_{c}\). (b) shows the impact of a nonlinear transformation on \(Z_{s}\) and \(Z_{c}\), leading to their entanglement in the new space defined by representations \(Z_{1}\) and \(Z_{2}\).

Figure 2: Illustrate the comparison of the performance of confident examples selection of our method CS-Isolate with DivideMix and Me-Momentum on CIFAR10N Worst [48]. The highest points on AUCs are indicated by arrows. High precision indicates that most of the confident examples are correctly labeled, while recall indicates the fraction of correctly labeled examples out of all examples that are correctly labeled.

identical content factors, which is a self-supervised learning manner. This initial step could achieve a preliminary level of isolation between style and content factors. As the training process progresses, we can identify and select certain confident examples based on the content factors. Leveraging these confident examples, we further enhance the isolation process by encouraging all examples sharing the same label to share identical content factors. By isolating content and style factors, our method, CS-Isolate, effectively helps existing sample selection methods to extract hard examples typically overlooked by existing sample selection methods. In Fig 2. we evaluate the performance of confident examples selection on CIFAR10N with the noise type "worst" [48] by using two metrics: precision and recall. When comparing these two metrics, CS-Isolate consistently outperforms DivideMix [28] and Me-Momentum [2]. This result demonstrates that our method can not only select confident examples more accurately, as shown by the higher precision, but also capture a larger portion of the correctly labeled examples from the entire dataset, as evidenced by the higher recall. By improving the quality of confident examples with the isolation of the content and style factors, our method effectively helps improve the test accuracy of existing methods. We have also theoretically analyzed the identifiability of content and style factors in Appendix A.

## 2 Background and Related Work

**Problem setup.** Let's denote \(\tilde{D}\) as the distribution of a noisy example \((X,\tilde{Y})\) from the set \(\mathcal{X}\times\{1,2,\ldots,C\}\), where \(X\) denotes the variable of instances, \(\tilde{Y}\) represents the variable of noisy labels, \(\mathcal{X}\) is the feature space, \(\{1,2,\ldots,C\}\) is the label space, and \(C\) is the total number of classes. In the learning scenario with noisy labels, clean labels are not observed. Given a noisy training sample \(\tilde{\mathcal{S}}=\{x_{i},\tilde{y}_{i}\}_{i=1}^{N}\), independently drawn from \(\tilde{D}\), the objective is to leverage this sample \(\tilde{\mathcal{S}}\) to learn a classifier robust against label noise.

**Sample selection methods for learning with noisy labels.** In learning with noisy labels, major current state-of-the-art (SOTA) methodologies predominantly involve sample selection strategies. These strategies seek to divide the dataset into confident and unconfident examples. The basis of these strategies is the exploitation of the _memorization effect_ inherent in deep neural networks. The memorization effect enables the networks to initially grasp and learn the simple patterns, and then learn the complex patterns gradually [60; 1]. Given that clean labels usually form the majority within each noisy class [7; 39], these networks would initially fit the examples with accurate labels, and subsequently fit the examples with incorrect labels over time [13].

Preventing the learning model from fitting incorrect labels is crucial to ensure sample selection quality. To achieve this, strategies such as early stopping are often employed [44; 3]. Additionally, the _small-loss trick_ is utilized to identify and extract confident examples with high certainty [28; 38; 51; 18]. Some variation has also been proposed, _e.g._, some methods opt to reweight examples, thereby decreasing the contribution of mislabeled samples to the overall loss [41; 13]. To guarantee the statistical consistency of algorithms, Jiacheng _et al._[7] introduces active learning to acquire the labels of randomly chosen examples from unconfident examples to mitigate the bias introduced by sample selection.

Moreover, the set of confident examples often undergoes dynamic changes during the training stage. This is achieved by leveraging semi-supervised learning methods to relabel training instances and reselect confident and unconfident examples using the small loss trick. Various techniques for this purpose have been proposed and empirically demonstrated superior performance, including consistency regularization [27] adopted by [10], MixMatch [5] used by [28], co-regularization by [47], and contrastive learning by [30; 29; 45; 8; 11; 57; 61]. Self-training and co-training have been used by [2] and [35; 13; 21], respectively.

**Representation learning by generative model.** Consider a data generation in Fig. 2(a), \(X\) is the observed data, and \(Z\) is the unknown underlying representation to generate \(X\). Variational autoencoder framework [24] can be used to learn the latent representation. In this process, a standard normal distribution is utilized as a prior for the latent variables, and a variational posterior \(q(Z|X)\) is employed to approximate the unknown underlying posterior \(p(Z|X)\). Disentangled representation is very important, which

Figure 3: The different data generative processes with or without auxiliary variables.

can allow a rich class of properties to be imposed on the learned representation, such as sparsity and clustering. To disentangle the representation, the variational autoencoder framework has been further expanded by modifying the original loss function, resulting in various algorithms. \(\beta\)-VAE [16] suggests an adaptation framework that adjusts the weight of the KL term to balance the independence of disentangled factors and reconstruction performance. \(\beta\)-TCVAE [6] further analyzes the KL term of \(\beta\)-VAE [16] and only adjusts the total correlation term to achieve disentanglement. HOOD [17] uses the clean labels and domain labels to disentangle content and style factors, but the clean labels and domain labels are unknown in the setting of learning with noisy labels. These methods are toward the goal of disentanglement but do not have theoretical guarantees of identifiability of their inferred latent representations.

Disentangled latent factors via auxiliary variable. Recent studies [20; 23] show that theoretical guarantees of identifiability can be achieved if an auxiliary variable related to the representation can be obtained. Khemakhem _et al._[23] provide identifiability of iVAE with additional inputs by employing the theory of nonlinear Independent Component Analysis (nonlinear ICA) [20]. Intuitively, given a factorized prior distribution over the latent variables that are conditional on an additional auxiliary variable \(U\), _i.e._, the class label and the time index in a time series, the latent factors are identifiable up to a certain degree. The data generation processing is changed to Fig. 2(b). To disentangle the representation, the method assumes that each factor of the representation is independent.

## 3 Content and Style Isolation When Learning with Noisy Labels

In this section, we present CS-Isolate, a method designed to help select hard confident examples by content and style isolation.

### Preliminaries

**Noisy data generative process.** To learn latent factors by leveraging generative models, the generative model has to model the noisy data generative process. Firstly, we introduce the noisy data generative process. We denote observed variables with gray circles and latent variables with white circles. Specifically, the content factor \(Z_{c}\) is generated by the latent label \(Y\). The different style domains \(U_{s}\) give rise to the different style factor \(Z_{s}\). Then, the image \(X\) is generated by the combined influence of the style factor \(Z_{s}\) and the content factor \(Z_{c}\). Noisy labels \(\tilde{Y}\) are then generated based on the image \(X\). In general cases, \(Z_{c}\) and \(Z_{s}\) can also have statistical or causal dependencies. We follow existing work that assumes the content factors are unchanged across different styles [46].

**Challenge in isolating content from styles without labels.** Isolating content from style without labels is a challenging task [46]. We first introduce the assumptions made by existing methods, as well as difficulties that may be encountered in practice.

Existing methods [26; 23; 46] for isolating content from styles without labels assume that the data augmentation for controlling each style factor can be designed, which means that we can intervene (control) all style factors. For instance, the data augmentations for controlling rotation angle and scaling of images can be designed by using affine transformation [40]. When we apply a data augmentation that rotates an image, the style factor for the rotation angle changes in the augmented image compared to the original. Similarly, when we scale an image, the style factor for scaling becomes different in the augmented image. If we have the data augmentation to control each style factor, then by training a generative model with the augmented images, the model can then identify style factors by comparing the changes in styles between the original and augmented images.

However, the major challenge lies in the fact that we generally cannot design sufficient data augmentations to control all style factors in an image. For instance, in the CIFAR-10 dataset [25], some pictures with the label "horse" contain a person. In this context, the "person" acts as a style factor. Existing data augmentations cannot control this, as they cannot remove the person from images easily. This simple example illustrates that it is generally impossible to control all style factors through data augmentations. Then the assumption required by existing methods usually is hard to satisfy.

Figure 4: The noisy data generative process.

The violation of the assumption leads to the learned representations for content containing style information.

Challenge in isolating content from style with noisy labels. We generally cannot design data augmentations that can control all style factors in an image. In this case, clean labels are essential to help isolate content from styles. Intuitively, by comparing the change of images with different clean labels that share the same styles, one can infer content factors used for classification. This achieves isolation of content and style. However, when dealing with noisy data, relying on labels becomes problematic as they contain label errors. Images with the same content factors can have different noisy labels, making it fail to infer content factors used for classification. This situation further complicates the task of isolating content from styles, highlighting the challenges posed by noisy labels.

### CS-Isolate for Extracting Hard Example

In this paper, we find that a key reason for some hard examples being close to the decision boundary is the entanglement of style factors with content factors. Intuitively, style factors render the learned representation of certain examples less discriminative, thereby making them close to the decision boundary. If content and style information can be isolated, many hard examples would be easily distinguished by ignoring the style information. This is because we have left only content information in the learned representation, making these representations far from the decision boundary.

Inspired by these findings, we propose CS-Isolate, which aims to isolate content from styles for extracting confident examples. To achieve isolation, we utilize self-supervised learning. Specifically, we ensure that original and augmented images maintain the same content factors despite having different style factors due to data augmentations. This achieves a preliminary level of isolating content from styles. As previously mentioned, it is generally impossible to manipulate all style factors through data augmentation. Some style factors remain uncontrolled and stay contained in the learned representations for content. To further encourage the isolation, labels of confident examples are used. Specifically, as training progresses, we identify and select confident examples whose labels are likely to be accurate. By harnessing the labels from these confident examples, we encourage the examples with the same label to have the consistent content factors, irrespective of stylistic variations. This further isolates content from uncontrolled styles. By leveraging different data augmentations and confident examples, our method can effectively separate the learned latent representations into two parts which exclusively contain either style or content information. Subsequently, classifier heads can be trained purely on the representations containing content information. This approach not only improves the identification of hard confident examples but also enhances classification performance.

Isolating content from styles using auxiliary variables.We follow existing work using the variational autoencoder with auxiliary variables [23] to isolate content from styles. Let \(U_{s}\) and \(U_{c}\) denote the auxiliary variables that control the style factor \(Z_{s}\) and content factor \(Z_{c}\), respectively. Specifically, by reconstructing an image with the supervision of auxiliary variables \(U_{s}\) and \(U_{c}\), we encourage the images with the same content information but different style information to have the same content factor \(Z_{c}\) but the different style factor \(Z_{s}\). This allows us to isolate content from styles within an image. Then only employing \(Z_{c}\) for selecting hard confident examples.

However, given only noisy data, we face the challenge of the unavailability of auxiliary variables \(U_{c}\) and \(U_{s}\). In response, we devise surrogates for these auxiliary variables. To find a surrogate for the style auxiliary variable \(U_{s}\), we employ different data augmentation techniques, each of which generates a unique style domain with distinct style factors. We then assign the style domain ID as the style auxiliary variable \(U_{s}\). To find a surrogate for the content auxiliary variable \(U_{c}\), we adopt the philosophy of self-supervised learning to assign a unique content ID as the content auxiliary variable \(U_{c}\) to each image. The images that have the same content ID are encouraged to have consistent content factors. Moreover, during the learning process, some confident examples can be extracted. To effectively leverage these examples in learning content factors, we reassign the content ID of confident examples belonging to the same class to be the same. This allows our model to learn consistent content factors for the images in the same class.

Constructing style auxiliary variables via data augmentations.For the learning of distinct content factors and style factors, it is essential to construct images from domains with different stylesby using data augmentations. Each image carries its corresponding domain ID as the style auxiliary variable \(U_{s}\). Let \(\mathcal{A}=\{A_{0},A_{1},A_{2},\ldots,A_{M}\}\) represents the set of data augmentations, and \(M\) is the number of data augmentations. Note that, for convenience, we use \(A_{0}\) to denote a data augmentation such that applying the data augmentation \(A_{0}\) to the image \(x\) does not change it at all, _i.e._, \(x=A_{0}(x)\).

When applying these diverse data augmentations to an \(x_{i}\) different augmented images are obtained with different style factors. For the augmented image \(x^{A_{i}}\) obtained by using the \(i\)-th data augmentations that come from the \(i\)-th style domain, we can assign its style ID to be \(i\). Specifically, for each image \(x\), each data augmentation \(A_{i}\in\mathcal{A}\) is applied, resulting in a set containing pairs of the augmented image and the corresponding style ID, _i.e._,

\[\{(x^{A_{i}}:=A_{i}(x),U_{s}^{(x^{A_{i}})}:=i)\;\mid\;\forall i\in\{0,...,M\}\},\]

where \(U_{s}^{x^{A_{i}}}\) is defined to be the style ID (which serves the style auxiliary variable) of the image \(x\) by applying \(i\)-th data augmentation. In this manner, for each image \(x\), we can generate a set of augmented images (\(\{x^{A_{i}}\}_{i=0}^{M}\)), including the original images and the ones with different style factors controlled by different style IDs.

Constructing content auxiliary variables in a self-supervised manner.To guide the generative model in learning the consistent content factor across images sharing the same content information, we aim to assign the same content ID (which serves as the content auxiliary variable) to images with the same content information. However, clean labels cannot be obtained in learning with noisy labels, meaning we cannot know which images have the same content information. To construct content auxiliary variables without clean labels, we adopt self-supervised learning. Specifically, we consider that different data augmentations applied to an image typically do not alter its content information. Hence, we can assign a unique content ID to each original image and its augmented versions. This assignment can be mathematically expressed as follows:

\[\mathcal{S}_{all}=\{(x_{j}^{A_{i}}:=A_{i}(x),U_{s}^{(x_{j}^{A_{i}})}:=i,U_{c}^ {(x_{j}^{A_{i}})}:=C+j)\;\mid\;\forall i\in\{0,...,M\},\forall j\in\{1,...,T\}\},\]

where \(T\) is the total number of distinct original images in the training data, \(U_{c}^{(x_{j}^{A_{i}})}\) represents the content ID of the \(j\)-th image in the training data after applying the data augmentation \(A_{i}\), and \(C\) is the number of classes.

Moreover, this initial assignment of content IDs can be refined by using confident examples, denoted as \(\mathcal{S}_{l}\). As the training process progresses, we can extract these confident examples by leveraging the small-loss trick [13, 3, 28]. The content IDs are then further refined based on the labels of the examples within the confident examples. The refinement process can be described as:

\[U_{c}^{(x_{j}^{A_{i}})}:=y_{j}^{c}\;\mid\;\forall(x_{j},y_{j}^{c})\in \mathcal{S}_{l},\forall i\in\{0,...,M\},\]

where \(y_{j}^{c}\) is the label of the confident example \((x_{j},y_{j}^{c})\). After this refinement process, confident examples in the same class will have the same content ID. Consequently, these refined content IDs could enable the model to learn consistent content factors for the images in the same class.

Encouraging style and content isolation for extracting hard example.After obtaining the auxiliary variable, we isolate content from styles by leveraging iVAE [20]. Specifically, the prior distribution of the content factor \(Z_{c}\) is conditional on the auxiliary variable \(U_{c}\), _i.e._\(P_{\theta_{c}}(Z_{c}|U_{c})\). Similarly, the prior distribution of the style factor \(Z_{s}\) is conditional on the auxiliary variable \(U_{s}\), _i.e._\(P_{\theta_{s}}(Z_{s}|U_{s})\). The \(\theta_{c}\) and \(\theta_{s}\) are the learnable parameters of the distribution. The objective is to maximize the data likelihood which is as follows.

\[\mathbb{E}_{q_{D}}[p_{\theta}(X|U_{c},U_{s})]=\mathbb{E}_{q_{D}} \left[\int_{z_{c},z_{s}}p_{\theta}(X|z_{c},z_{s})p_{\theta_{c}}(z_{c}|U_{c})p_{ \theta_{s}}(z_{s}|U_{s})\mathrm{d}z_{s}\mathrm{d}z_{c}\right],\] (1)

where we use \(q_{D}\) to denote the empirical distribution of the training sample \(\mathcal{S}_{all}\). We use the variational inference method to approximate the underlying posterior distribution \(p_{\theta}(Z_{c},Z_{s}|X,U_{c},U_{s})\)[23, 24]. Specifically, two inference models (encoders) \(q_{\phi_{c}}(Z_{c}|X)\) and \(q_{\phi_{s}}(Z_{s}|X)\) are introduced to infer latent variables \(Z_{c}\) and \(Z_{s}\) respectively and model the distribution \(q(Z_{c},Z_{s}|X)\) that is used to approximate the distribution \(p_{\theta}(Z_{c},Z_{s}|X,U_{c},U_{s})\). Therefore, the distribution \(q(Z_{c},Z_{s}|X)\) can be decomposed as follows:

\[q_{\phi}(Z_{c},Z_{s}|X)=q_{\phi_{s}}(Z_{c}|X)q_{\phi_{s}}(Z_{s}|X).\]We learn the parameters \(\{\theta_{c},\theta_{s},\phi_{c},\phi_{c}\}\) by maximizing the evidence lower-bound \(\mathrm{ELBO}\) for each example \((x,u_{c},u_{s})\). The corresponding loss is:

\[\min_{\phi_{c},\phi_{s},\theta}\mathcal{L}_{ELBO} :=\min_{\phi_{c},\phi_{s},\theta}\mathbb{E}_{q_{D}}\left[-\mathbb{ E}_{(z_{c},z_{s})\sim q_{\phi_{c},\phi_{s}}(Z_{c},Z_{s}|x)}[\log p_{\theta}(x|z_{c},z_ {s})\right.\] \[\left.-KL(q_{\phi_{c}}(z_{c}|x)||p_{\theta_{c}}(z_{c}|u_{c}))-KL(q_ {\phi_{s}}(z_{s}|x)||p_{\theta_{s}}(z_{s}|u_{s}))]\right],\]

where \(KL\) is the Kullback-Leibler divergence. Intuitively, when the style changes, the content remains constant. The model has to infer the unchanged content factors for image reconstruction. Thus, the content factors can be isolated from style factors. The content factors can be identified up to the block-identifiable defined in [46].

Utilizing content information to extract hard examples.After minimizing the ELBO loss \(\mathcal{L}_{ELBO}\), we have learned representations \(\hat{Z}_{c}\) and \(\hat{Z}_{s}\) which exclusively contain either style or content information. Then we employ existing sample-selection-based methods solely focus on \(\hat{Z}_{c}\) for confident example selection, a classification head \(f_{\psi}:\mathcal{Z}_{c}\rightarrow\Delta_{C-1}\) is introduced. The classification head maps the space of content factors to a \(C-1\) probability simplex, where \(C\) represents the number of classes.

In practice, we propose an _end-to-end_ approach to learn the classification head \(f_{\psi}\) and learn to infer content factor for extracting hard confident examples. This approach simultaneously minimizes the evidence lower bound loss \(\mathcal{L}_{ELBO}\) and the loss of existing sample-selection-based methods by leveraging the Lagrangian method [22; 12]. The content IDs are refined dynamically during the learning process. The overall loss function \(\mathcal{L}_{all}\) is thus given as:

\[\min_{\phi_{c},\phi_{s},\theta,\psi}\mathcal{L}_{all}=\min_{\phi_{c},\phi_{s}, \theta,\psi}[\mathcal{L}_{ssl}+\lambda_{ELBO}\mathcal{L}_{ELBO}+\lambda_{ref} \mathcal{L}_{ref}],\]

where \(\mathcal{L}_{ssl}\) denotes the loss of the sample selection method, which optimizes the parameters of the classification head \(\psi\) and the content encoder \(\hat{\phi}_{c}\). The loss \(\mathcal{L}_{ref}\) is the cross-entropy loss on confident examples. The hyper-parameter \(\lambda_{ELBO}\) and \(\lambda_{ref}\) are used to control strength of \(\mathcal{L}_{ELBO}\) and \(\mathcal{L}_{ref}\), respectively. Here, we illustrate a concrete example of employing the inference model \(q_{\hat{\phi}_{c}}\) in conjunction with DivideMix [28] in an end-to-end implementation. For a detailed walkthrough, please refer to the pseudo-code provided in Appendix B and the original paper [28].

DivideMix [28] applies a Gaussian mixture model to enhance MixMatch [43] for confident example selection and classifiers training. In DivideMix, during each epoch of training, the data is divided into a set of confident examples \(\mathcal{S}_{l}\) and a set of unconfident examples \(\mathcal{S}_{u}\). The confident examples contain the sharpened soft labels mixed by their labels in datasets and predicted labels given by the classification head \(f_{\psi}\). The unconfident examples contain the sharpened soft labels predicted by the classification head \(f_{\psi}\). Then, the semi-supervised learning approach, MixMatch [5] is employed by transforming confident (\(\mathcal{S}_{l}\)) and unconfident (\(\mathcal{S}_{u}\)) samples into augmented confident (\(\mathcal{S}_{l}^{\prime}\)) and unconfident (\(\mathcal{S}_{u}^{\prime}\)) samples by a linearly mixing.

The overall loss function, composed of a confident sample loss, an unconfident sample loss, and a regularization term, \(\mathrm{ELBO}\) loss and a classification loss, _i.e._,

\[\mathcal{L}_{all} =\underbrace{\mathcal{L}_{\mathcal{S}_{l}}+\lambda_{u}\mathcal{L} _{\mathcal{S}_{u}}+\lambda_{r}\mathcal{L}_{\mathrm{reg}}}_{\mathrm{DivideMix loss}}+\lambda_{ELBO}\mathcal{L}_{ELBO}+\lambda_{ref} \mathcal{L}_{ref}.\]

Intuitively, \(\mathcal{L}_{\mathcal{S}_{l}}\) is a cross-entropy loss for the labeled examples; \(\mathcal{L}_{\mathcal{S}_{u}}\) is the mean squared error for the unlabeled samples; \(\mathcal{L}_{\mathrm{reg}}\) is a regularization term to prevent the model from predicting all samples to belong to a single class. These three terms are defined as follows specifically.

\[\mathcal{L}_{\mathcal{S}_{l}} =-\frac{1}{|\mathcal{S}_{l}^{\prime}|}\sum_{x,y^{s}\in\mathcal{S}_{ l}^{\prime}}\sum_{i}y_{i}^{s}\log(f_{\psi}\circ q_{\phi_{c}}(x)),\] \[\mathcal{L}_{\mathcal{S}_{u}} =\frac{1}{|\mathcal{S}_{u}^{\prime}|}\sum_{x,y^{s}\in\mathcal{S}_ {u}^{\prime}}\left\|y^{s}-f_{\psi}\circ q_{\phi_{c}}(x)\right\|_{2}^{2},\] \[\mathcal{L}_{\mathrm{reg}} =\sum_{i}\frac{1}{C}\log(1\bigg{/}\frac{C}{|\mathcal{S}_{l}^{ \prime}|+|\mathcal{S}_{u}^{\prime}|}\sum_{x\in\mathcal{S}_{l}^{\prime}+\mathcal{ S}_{u}^{\prime}}f_{\psi}\circ q_{\phi_{c}}^{i}(x)),\]

where \(y^{s}\) is the sharpened soft label and \(q_{\phi_{c}}^{i}\) denotes the \(i\)-th coordinate of its output on input \(x\). In Appendix B, we illustrate our method with other sample selection methods.

## 4 Experiments

In this section, we introduce the setting of our experiments and compare our experimental results with existing methods. Most of our experiments are left out in Appendix C due to the limited space.

### Experiment Setting

**Dataset and noise type.**  We evaluate our methods on three synthetic noise datasets FashionMNIST [54], CIFAR-10 [25], and CIFAR-100 [25], and two real-world label-noise datasets, CIFAR-10N [48] and Clothing1M [55]. FashionMNIST includes 70,000 images of size \(24\times 24\), categorized into 10 classes with 60,000 for training and 10,000 for testing. Both CIFAR-10 and CIFAR-100 contain 50,000 training images and 10,000 test images; CIFAR-10 comprises 10 classes, while CIFAR-100 includes 100 classes. The image size in both CIFAR datasets is \(32\times 32\times 3\). To generate noisy labels for these clean datasets, we employ the instance-dependent noisy label generation methods proposed in [52]. CIFAR-10N, a noisy version of CIFAR-10, includes five types of label noise: "worst", "aggregate", "random 1", "random 2", and "random 3", all annotated by humans. The noise rates are 40.21%, 9.03%, 17.23%, 18.12%, and 17.64%, respectively. Clothing1M contains 1 million images with real-world noisy labels for training and 10,000 images with clean labels for testing.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Worst & Aggregate & Random 1 & Random 2 & Random 3 \\ \hline Me-Moment & 0.881 \(\pm\) 0.011 & 0.972 \(\pm\) 0.002 & 0.962 \(\pm\) 0.005 & 0.960 \(\pm\) 0.003 & 0.948 \(\pm\) 0.005 \\ DivideMix & 0.951 \(\pm\) 0.003 & **0.989 \(\pm\) 0.000** & 0.983 \(\pm\) 0.000 & 0.983 \(\pm\) 0.000 & **0.983 \(\pm\) 0.000** \\ \hline CS-Isolate-DM & **0.956 \(\pm\) 0.001** & **0.989 \(\pm\) 0.000** & **0.984 \(\pm\) 0.000** & **0.985 \(\pm\) 0.000** & **0.983 \(\pm\) 0.000** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Precision of confident examples on CIFAR-10N.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Worst & Aggregate & Random 1 & Random 2 & Random 3 \\ \hline CE & 88.54 \(\pm\) 0.32 & 84.22 \(\pm\) 0.35 & 75.81 \(\pm\) 0.26 & 62.45 \(\pm\) 0.86 & 21.45 \(\pm\) 0.70 \\ Co-Teaching & 91.21 \(\pm\) 0.31 & 89.10 \(\pm\) 0.29 & 80.96 \(\pm\) 0.31 & 73.41 \(\pm\) 0.78 & 28.04 \(\pm\) 1.43 \\ Forward & 90.05 \(\pm\) 0.43 & 86.27 \(\pm\) 0.48 & 74.64 \(\pm\) 0.26 & 60.21 \(\pm\) 0.75 & 26.75 \(\pm\) 0.93 \\ T-Revision & 91.58 \(\pm\) 0.31 & 89.46 \(\pm\) 0.42 & 76.15 \(\pm\) 0.37 & 65.09 \(\pm\) 0.37 & 27.23 \(\pm\) 1.13 \\ BLTM & 91.20 \(\pm\) 0.27 & 82.42 \(\pm\) 1.51 & 77.50 \(\pm\) 1.30 & 63.20 \(\pm\) 4.52 & 35.67 \(\pm\) 1.97 \\ CausalNL & 90.84 \(\pm\) 0.31 & 90.01 \(\pm\) 0.45 & 80.91 \(\pm\) 1.14 & 79.08 \(\pm\) 0.50 & 34.02 \(\pm\) 0.95 \\ Me-Momentum & 92.85 \(\pm\) 0.64 & 90.06 \(\pm\) 0.51 & 90.86 \(\pm\) 0.21 & 86.66 \(\pm\) 0.91 & 58.38 \(\pm\) 1.28 \\ DivideMix & 94.85 \(\pm\) 0.15 & 92.28 \(\pm\) 0.13 & 94.93 \(\pm\) 0.15 & 94.16 \(\pm\) 0.35 & 70.50 \(\pm\) 0.25 \\ \hline CS-Isolate-DM & **95.16 \(\pm\) 0.07** & **94.40 \(\pm\) 0.09** & **95.90 \(\pm\) 0.10** & **95.54 \(\pm\) 0.06** & **73.11 \(\pm\) 0.36** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Means and standard deviations (percentage) of classification accuracy on CIFAR-10N.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Worst & Aggregate & Random 1 & Random 2 & Random 3 \\ \hline Me-Moment & 0.920 \(\pm\) 0.026 & 0.969 \(\pm\) 0.019 & 0.946 \(\pm\) 0.021 & 0.958 \(\pm\) 0.022 & 0.962 \(\pm\) 0.018 \\ DivideMix & 0.966 \(\pm\) 0.000 & 0.963 \(\pm\) 0.000 & 0.975 \(\pm\) 0.000 & 0.976 \(\pm\) 0.000 & 0.977 \(\pm\) 0.000 \\ \hline CS-Isolate-DM & **0.980 \(\pm\) 0.000** & **0.975 \(\pm\) 0.002** & **0.980 \(\pm\) 0.001** & **0.982 \(\pm\) 0.001** & **0.982 \(\pm\) 0.001** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Recall of confident examples on CIFAR-10N.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Worst & Aggregate & Random 1 & Random 2 & Random 3 \\ \hline CE & 77.69 \(\pm\) 1.55 & 87.77 \(\pm\) 0.38 & 85.02 \(\pm\) 0.65 & 86.14 \(\pm\) 0.24 & 86.12 \(\pm\) 0.16 \\ Co-Teaching & 82.04 \(\pm\) 0.06 & 91.11 \(\pm\) 0.10 & 89.61 \(\pm\) 0.18 & 88.98 \(\pm\) 0.11 & 89.49 \(\pm\) 0.06 \\ Forward & 79.79 \(\pm\) 0.46 & 88.24 \(\pm\) 0.22 & 86.88 \(\pm\) 0.50 & 86.14 \(\pm\) 0.24 & 87.04 \(\pm\) 0.35 \\ T-Revision & 80.48 \(\pm\) 1.20 & 88.52 \(\pm\) 0.17 & 88.33 \(\pm\) 0.32 & 87.71 \(\pm\) 1.02 & 87.79 \(\pm\) 0.67 \\ BLTM & 68.21 \(\pm\) 1.67 & 79.41 \(\pm\) 1.00 & 78.09 \(\pm\) 1.03 & 76.99 \(\pm\) 1.23 & 76.26 \(\pm\) 0.71 \\ CausalNL & 82.41 \(\pm\) 0.24 & 90.43 \(\pm\) 0.14 & 89.03 \(\pm\) 0.02 & 89.06 \(\pm\) 0.05 & 89.21 \(\pm\) 0.13 \\ Me-Momentum & 84.21 \(\pm\) 0.70 & 91.34 \(\pm\) 0.16 & 89.51 \(\pm\) 0.42 & 90.14 \(\pm\) 0.28 & 89.62 \(\pm\) 0.31 \\ DivideMix & 92.48 \(\pm\) 0.16 & 94.11 \(\pm\) 0.21 & 94.77 \(\pm\) 0.15 & 94.79 \(\pm\) 0.14 & 94.79 \(\pm\) 0.15 \\ \hline CS-Isolate-DM & **94.28 \(\pm\) 0.07** & **95.34 \(\pm\) 0.10** & **95.36 \(\pm\) 0.14** & **95.34 \(\pm\) 0.12** & **95.48 \(\pm\) 0.14** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Means and standard deviations (percentage) of classification accuracy on FashionMNIST, CIFAR-10 and CIFAR-100.

[MISSING_PAGE_FAIL:9]

datasets, we employed instance-dependent noisy label generation methods, as proposed by [52]. We experimented with noise rates of 0.2 and 0.4, denoted by IDN-0.2 and IDN-0.4 respectively. The experiment results are presented in Tab. 3, Tab. 4 and Tab. 5. Our proposed method outperforms existing methods in terms of test accuracy on both synthetic and real-world datasets containing label noise. Notably, as the noise rate increases, the performance gap between our method, CS-Isolate-DM, and the existing methods becomes more pronounced. This highlights the robustness and effectiveness of our approach in scenarios with higher levels of label noise.

### Hard Confident Examples Visualization

The proposed method is expected to select confident examples based on content factors rather than style factors. To analyze this, we use Grad-CAM [42] to visualize the regions used to select confident examples. The visualization results are shown in Fig. 5. The experiment is conducted on the real-world dataset CIFAR-10N, and the noise type is "worst". The experiment results demonstrate that our method can correctly focus on the object in images. Specifically, when there exist uncontrolled style factors, _e.g._, the person near the horse, the activation maps for CS-Isolate-DM can successfully focus on the right object used for classifying the horse instead of uncontrolled style factors. In contrast, the baseline method, DivideMix, focuses on the style factors that are not related to the class "horse" and fails to select these confident examples.

## 5 Conclusion

This paper is motivated by the fact that only focusing on content factors such as semantic information makes examples more discriminative. We, therefore, proposed a novel CS-Isolate approach to infer and isolate the content information for classification. This is achieved by leveraging variational inference and constructing auxiliary variables via data augmentation techniques to modify style factors while regularizing content factors using confident examples. By training existing sample-selection-based methods with our inferred content factors, CS-Isolate improves their effectiveness in learning hard examples and classification accuracy on different image datasets.

Figure 5: Grad-CAM visualizations of hard confident examples. CS-Isolate-DM successfully identifies these confident examples, but DivideMix does not. The activation map of CS-Isolate-DM predominantly highlights semantic objects, whereas DivideMix emphasizes non-object pixels.

## Acknowledgments and Disclosure of Funding

Tongliang Liu is partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, and IC190100031. Dong Xu is partially supported by the Hong Kong Jockey Club Charities Trust under Grant 2022-0174, the Hong Kong Research Grant Council under General Research Fund (17203023), the Startup Funding and the Seed Funding for Basic Research for New Staff from The University of Hong Kong, and the funding from UBTECH Robotics.

## References

* [1] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In _ICML_, pages 233-242, 2017.
* [2] Yingbin Bai and Tongliang Liu. Me-momentum: Extracting hard confident examples from noisily labeled data. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9312-9321, 2021.
* [3] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels. _arXiv preprint arXiv:2106.15853_, 2021.
* [4] Yingbin Bai, Erkun Yang, Zhaoqing Wang, Yuxuan Du, Bo Han, Cheng Deng, Dadong Wang, and Tongliang Liu. Rsa: Reducing semantic shift from aggressive augmentations for self-supervised learning. _Advances in Neural Information Processing Systems_, 35:21128-21141, 2022.
* [5] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. _arXiv preprint arXiv:1905.02249_, 2019.
* [6] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in variational autoencoders. _Advances in neural information processing systems_, 31, 2018.
* [7] Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning with bounded instance and label-dependent label noise. In _International Conference on Machine Learning_, pages 1789-1799. PMLR, 2020.
* [8] Madalina Ciortan, Romain Dupuis, and Thomas Peel. A framework using contrastive learning for classification with noisy labels. _Data_, 6(6):61, 2021.
* [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [10] Erik Englesson and Hossein Azizpour. Consistency regularization can improve robustness to label noise. _arXiv preprint arXiv:2110.01242_, 2021.
* [11] Aritra Ghosh and Andrew Lan. Contrastive learning improves model robustness under label noise. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2703-2708, 2021.
* [12] Giorgio Giorgi and Tinne Hoff Kjeldsen. _Traces and emergence of nonlinear programming_. Springer Science & Business Media, 2013.
* [13] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In _NeurIPS_, pages 8527-8537, 2018.
* [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.
* [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 630-645. Springer, 2016.
* [16] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In _International conference on learning representations_, 2017.

* [17] Zhuo Huang, Xiaobo Xia, Li Shen, Bo Han, Mingming Gong, Chen Gong, and Tongliang Liu. Harnessing out-of-distribution examples via augmenting content and style. _arXiv preprint arXiv:2207.03162_, 2022.
* [18] Zhuo Huang, Chao Xue, Bo Han, Jian Yang, and Chen Gong. Universal semi-supervised learning. _Advances in Neural Information Processing Systems_, 34:26714-26725, 2021.
* [19] Zhuo Huang, Miaoi Zhu, Xiaobo Xia, Li Shen, Jun Yu, Chen Gong, Bo Han, Bo Du, and Tongliang Liu. Robust generalization against photon-limited corruptions via worst-case sharpness minimization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16175-16185, 2023.
* [20] Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 859-868. PMLR, 2019.
* [21] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In _ICML_, pages 2309-2318, 2018.
* [22] William Karush. Minima of functions of several variables with inequalities as side constraints. _M. Sc. Dissertation. Dept. of Mathematics, Univ. of Chicago_, 1939.
* [23] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In _International Conference on Artificial Intelligence and Statistics_, pages 2207-2217. PMLR, 2020.
* [24] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [26] Harold W Kuhn and Albert W Tucker. Nonlinear programming. In _Traces and emergence of nonlinear programming_, pages 247-258. Springer, 2014.
* [27] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. _arXiv preprint arXiv:1610.02242_, 2016.
* [28] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. In _ICLR_, 2019.
* [29] Junnan Li, Caiming Xiong, and Steven CH Hoi. Mopro: Webly supervised learning with momentum prototypes. _arXiv preprint arXiv:2009.07995_, 2020.
* [30] Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu. Selective-supervised contrastive learning with noisy labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 316-325, 2022.
* [31] Shikun Li, Xiaobo Xia, Hansong Zhang, Yibing Zhan, Shiming Ge, and Tongliang Liu. Estimating noise transition matrix with label correlations for noisy multi-label learning. _Advances in Neural Information Processing Systems_, 35:24184-24198, 2022.
* [32] Xuefeng Li, Tongliang Liu, Bo Han, Gang Niu, and Masashi Sugiyama. Provably end-to-end label-noise learning without anchor points. _arXiv preprint arXiv:2102.02400_, 2021.
* [33] Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. _IEEE Transactions on pattern analysis and machine intelligence_, 38(3):447-461, 2016.
* [34] Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In _International Conference on Machine Learning_, pages 6348-6359. PMLR, 2020.
* [35] Eran Malach and Shai Shalev-Shwartz. Decoupling" when to update" from" how to update". In _NeurIPS_, pages 960-970, 2017.
* [36] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. MIT press, 2018.
* [37] Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. Self: Learning to filter noisy labels with self-ensembling. In _ICLR_, 2019.

* Northcutt et al. [2021] Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset labels. _Journal of Artificial Intelligence Research_, 70:1373-1411, 2021.
* Patrini et al. [2017] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In _CVPR_, pages 1944-1952, 2017.
* Perez and Wang [2017] Luis Perez and Jason Wang. The effectiveness of data augmentation in image classification using deep learning. _arXiv preprint arXiv:1712.04621_, 2017.
* Ren et al. [2018] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In _International Conference on Machine Learning_, pages 4334-4343. PMLR, 2018.
* Selvaraju et al. [2017] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In _Proceedings of the IEEE international conference on computer vision_, pages 618-626, 2017.
* Sohn et al. [2020] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. _arXiv preprint arXiv:2001.07685_, 2020.
* Song et al. [2019] Hwanjun Song, Minseok Kim, Dongmin Park, and Jae-Gil Lee. How does early stopping help generalization against label noise? _arXiv preprint arXiv:1911.08059_, 2019.
* Tan et al. [2021] Cheng Tan, Jun Xia, Lirong Wu, and Stan Z Li. Co-learning: Learning from noisy labels with self-supervision. In _Proceedings of the 29th ACM International Conference on Multimedia_, pages 1405-1413, 2021.
* Kugelgen et al. [2021] Julius Von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. _Advances in neural information processing systems_, 34:16451-16467, 2021.
* Wei et al. [2020] Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint training method with co-regularization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13726-13735, 2020.
* Wei et al. [2021] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. _arXiv preprint arXiv:2110.12088_, 2021.
* Xia et al. [2022] Xiaobo Xia, Bo Han, Nannan Wang, Jiankang Deng, Jiatong Li, Yinian Mao, and Tongliang Liu. Extended \(t\) t: Learning with mixed closed-set and open-set noisy labels. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(3):3047-3058, 2022.
* Xia et al. [2021] Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan Wang, Zongyuan Ge, and Yi Chang. Robust early-learning: Hindering the memorization of noisy labels. In _ICLR_, 2021.
* Xia et al. [2021] Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi Sugiyama. Sample selection with uncertainty of losses for learning with noisy labels. _arXiv preprint arXiv:2106.00445_, 2021.
* Xia et al. [2020] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent label noise. _NerurIPS_, 2020.
* Xia et al. [2019] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? In _NeurIPS_, pages 6835-6846, 2019.
* Xiao et al. [2017] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* Xiao et al. [2015] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In _CVPR_, pages 2691-2699, 2015.
* Yang et al. [2022] Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating instance-dependent bayes-label transition matrix using a deep neural network. In _International Conference on Machine Learning_, pages 25302-25312. PMLR, 2022.
* Yao et al. [2021] Yazhou Yao, Zeren Sun, Chuanyi Zhang, Fumin Shen, Qi Wu, Jian Zhang, and Zhenmin Tang. Jo-src: A contrastive approach for combating noisy labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5192-5201, 2021.

* [58] Yu Yao, Tongliang Liu, Mingming Gong, Bo Han, Gang Niu, and Kun Zhang. Instance-dependent label-noise learning under a structural causal model. _Advances in Neural Information Processing Systems_, 34:4409-4420, 2021.
* [59] LIN Yong, Renjie Pi, Weizhong Zhang, Xiaobo Xia, Jiahui Gao, Xiao Zhou, Tongliang Liu, and Bo Han. A holistic view of label noise transition matrix in deep learning and beyond. In _The Eleventh International Conference on Learning Representations_, 2022.
* [60] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In _ICLR_, 2017.
* [61] Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson, Alex M Bronstein, and Or Litany. Contrast to divide: Self-supervised pre-training for learning with noisy labels. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1657-1667, 2022.

A Theoretical View of Content and Style Isolation When Learning with Noisy Labels

**Noisy data generative process.** Recall the data generative process in our main paper, to learn latent factors by leveraging generative models. The generative model has to model the data generative process of noisy data. Firstly, we introduce the generative process of noisy data. We denote observed variables with gray color and latent variables with white color. Specifically, the content factor \(Z_{c}\) is generated by the latent label \(Y\). The different style domain \(U_{s}\) give rise to the different style factor \(Z_{s}\). Subsequently, the image \(X\) is generated by the combined influence of the style factor \(Z_{s}\) and the content factor \(Z_{c}\). Noisy labels \(\tilde{Y}\) are then generated based on the image \(X\). In general cases, \(Z_{c}\) and \(Z_{s}\) can also have statistical or causal dependencies. We follow existing work that assumes the content factors are invariant across different styles [46].

Firstly, we introduce the concept of an _uncontrolled style factor_. This refers to a specific style factor, denoted \(Z_{s^{\prime}}\), that remains invariant when a data augmentation \(A\) is applied. To formalize this concept, consider an invertible function \(f:\mathcal{Z}\times\mathcal{X}\). Let \(\mathcal{A}\) denote a set of data augmentations, where each augmentation \(A\) is a subset ranging from \(1\) to \(M\). Additionally, let \(P(A)\) represent a probability distribution over the set of these augmentations \(\mathcal{A}\). Now, consider \(Z_{s^{\prime}}\) as a subset of style factors drawn from a larger set \(Z_{s}\). We partition the latent factors \(z\) of each instance \(x\) into three distinct parts: uncontrolled style factor \(Z_{s^{\prime}}\), content factor \(Z_{c}\), and style factors influenced by data augmentation \(Z_{s/s^{\prime}}\), such that \(f^{-1}(x)=[z_{s^{\prime}},z_{c},z_{s/s^{\prime}}]=z\). The term \(z_{s/s^{\prime}}\) denotes the set of style factors with style factors \(z_{s^{\prime}}\) excluded.

**Definition 1** (Uncontrolled Style Factors): _We say that a style factor \(Z_{s^{\prime}}\) as uncontrolled under the following conditions:_

_For any augmentation \(A\sim P(A)\), and for any instance \(x\), the first \(n_{s^{\prime}}\) components of the inverse function \(f^{-1}(x)\) remain unchanged even when \(A(x)\) is applied, i.e., \(f^{-1}(x)_{1:n_{s^{\prime}}}=f^{-1}(A(x))_{1:n_{s^{\prime}}}\)._

Here, \(f^{-1}(x)_{1:n_{s^{\prime}}}\) is defined as the underlying partition that contains only and all the information related to the style factor \(Z_{s^{\prime}}\) of the instance \(x\).

**Why do confident examples encourage content-style isolation?** Here, we explain the reason that confident examples encourage content-style isolation. Suppose that there exist some uncontrolled style factors that cannot be adjusted or manipulated through data augmentation. This implies that for an image \(x\), these style factors remain unaffected, regardless of the data augmentation techniques employed. For instance, as discussed in our paper, in the CIFAR-10 dataset [25], some pictures with the label "horse" contain a person. In this context, the "person" acts as a style factor. Existing data augmentations cannot control this, as they cannot remove the person from images easily.

It's essential to understand that although data augmentation cannot control all style factors, it still offers the benefit of "partial isolation". If we consider a situation where the uncontrolled style factors are not effects of other style factors, then we can isolate other style factors affected by data augmentation from content factors after matching the data likelihood [34]. This result is a modified application of _block-identifiability_[46].

Specifically, let \(\hat{Z}_{c,s^{\prime}}:=\hat{Z}_{1:n_{c}+n_{s^{\prime}}}\) be a partition of learned representations of a generative model, where \(\hat{Z}_{c,s^{\prime}}\) contain _all_ and _only_ information about \(Z_{c}\) and \(Z_{s^{\prime}}\). Suppose that assumptions of Theorem 4.2 in [34] are satisfied, \(\hat{Z}_{c,s^{\prime}}\) is guaranteed to be learned, thereby allowing for the partial isolation of the remaining style factors, denoted as \(Z_{s/s^{\prime}}\).

Despite \(\hat{Z}_{c,s^{\prime}}\) is guaranteed to be learned, the information from the uncontrolled style factor \(Z_{s^{\prime}}\) is entangled with the content factor \(Z_{c}\) in the learned \(\hat{Z}_{c,s^{\prime}}\). To isolate this information further, the employment of _confident examples_ is necessary.

Specifically, we can generate \(\hat{Z}_{c,s^{\prime}}\) using an invertible function according to the label \(\hat{Y}\) of the confident example, and subsequently reconstruct image \(x\). Following the data likelihood matching [34], the information related to the uncontrolled style factors becomes apparent due to the establishment of a

Figure 6: The noise data generative process.

one-to-one mapping between the label \(\hat{Y}\) and \(\hat{Z}_{c,s^{\prime}}\). This consequently forces examples with identical labels to share the same \(\hat{Z}_{c,s^{\prime}}\), regardless of any alterations in the uncontrolled style factor \(Z_{s}^{\prime}\). This approach, therefore, ensures that styles changes don't affect the derived content representation for the same label.

It is worth mentioning that to fully isolate uncontrolled style factors and content factors, it requires that there exists confident examples with all possible uncontrolled style factors. This can be hard to achieve when learning with noisy labels. Therefore, in general, the selected confident examples can only encourage isolation but can not fully isolate uncontrolled style factors and content factors.

## Appendix B Apply CS-Isolate to Existing Methods for Learning with Noisy Labels

Applying CS-Isolate to DivideMix. DivideMix [28] uses two classifiers to select confident examples for each other. To utilize the unlabeled data, they combine the semi-supervised technique MixMatch [5]. Specifically, the classifiers, after warmed up, are used to calculate the loss of examples. They use a Gaussian Mixture Model (GMM) to divide the examples into confident and unlabeled examples. Finally, confident and unlabeled examples are used to train the models based on the MixMatch algorithm. Our method can be plugged into DivideMix easily. Specifically, we use a decoder \(q_{\phi}(Z_{c},Z_{s}|X)\) to obtain content factor \(Z_{c}\) and style factor \(Z_{s}\). A classifier head \(f_{\psi}\) is used to predict labels, and only the content factors \(Z_{c}\) are used as input. The prior distribution of the content factors \(Z_{c}\) is conditional on the auxiliary variable \(U_{c}\), _i.e._, \(P_{\theta_{c}}(Z_{c}|U_{c})\), where \(U_{c}\) is the content ID. Similarly, the prior distribution of the style factor \(Z_{s}\) is conditional on the auxiliary variable \(U_{s}\), _i.e._, \(P_{\theta_{c}}(Z_{s}|U_{s})\), where \(U_{s}\) is the style ID. A decoder \(P_{\theta}(X|Z_{c},Z_{s})\) is used to reconstruct input images. The combination of CS-Isolate and DivideMix is called CS-Isolate-DM. The loss function of CS-Isolate-DM is shown in Eq. 2. Algorithm 1 delineates the full algorithm.

\[\mathcal{L}_{dm}=\underbrace{\mathcal{L}_{\mathcal{S}_{l}}+\lambda_{u} \mathcal{L}_{\mathcal{S}_{u}}+\lambda_{r}\mathcal{L}_{\text{reg}}}_{\text{ DivideMix loss}}+\lambda_{ELBO}\mathcal{L}_{ELBO}+\lambda_{ref}\mathcal{L}_{ref}.\] (2)

```
1:Input: A noisy training dataset \(\bar{\mathcal{S}}\), a noisy validation dataset \(\bar{\mathcal{S}}_{v}\), a style ID list \(U_{s}\), a data augmentation set \(\mathcal{A}\), iteration number \(T_{inner},T_{outer}\).
2:Initialize encoders (\(\hat{q}_{\phi^{0}_{c}}\) and \(\hat{q}_{\phi^{0}_{c}}\)), decoders (\(\hat{p}_{\theta^{0}_{c}}\), \(\hat{p}_{\theta^{0}_{c}}\) and \(\hat{p}_{\theta^{0}}\)), a classification model \(f_{\psi_{0}}\) by using the noisy training data and early stopping;
3:For i = \(1,\ldots,T_{outer}\):
4:Update the extracted confident examples \(\mathcal{S}_{l}\) by using \(\hat{q}_{\phi^{-1}_{c}}\) and \(f_{\psi_{j-1}}\);
5:Train networks by using the loss in Eq. 4 on confident examples \(\mathcal{S}_{l}\);
6:Obtain \(\hat{q}_{\phi^{L}_{c}}\) and \(f_{\psi_{j}}\) through the highest noisy validation accuracy throughout the training procedure;
7:Break and output \(\hat{q}_{\phi^{L-1}_{c}}\) and \(f_{\psi_{j-1}}\) if the highest validation accuracy is non-increasing in the loop;
8:Re-initialize encoders (\(\hat{q}_{\phi^{0}_{c}}\) and \(\hat{q}_{\phi^{0}_{c}}\)), decoders (\(\hat{p}_{\theta^{0}_{c}}\), \(\hat{p}_{\theta^{0}_{c}}\) and \(\hat{p}_{\theta^{0}}\)), a classification model \(f_{\psi_{0}}\);
9:Train networks by using the loss in Eq. 4 on confident examples \(\mathcal{S}_{l}\);
10:Obtain \(\hat{q}_{\phi^{0}_{c}}\) and \(f_{\psi_{0}}\) through the highest noisy validation accuracy throughout the training procedure;
11:Break and output \(\hat{q}_{\phi^{L-1}_{c}}\) and \(f_{\psi_{j-1}}\) if the highest validation accuracy is non-increasing in the loop;
12:Output: The inference network and the classification model \(q_{\phi_{c}},f_{\psi}\). ```

**Algorithm 3** CS-Isolate-Me

**Applying CS-Isolate to Co-Teaching.**  Co-Teaching [13] uses two classifiers to select confident examples for each other. Proposed CS-Isolate can be embedded in Co-Teaching easily. Similar to CS-Isolate-DM, we use a decoder \(q_{\phi}(Z_{c},Z_{s}|X)\) to obtain content factors \(Z_{c}\) and style factors \(Z_{s}\). A classifier head \(f_{\psi}\) is used to predict labels, and only the content factors \(Z_{c}\) are used as input. The prior distribution of the content factors \(Z_{c}\) is conditional on the auxiliary variable \(U_{c}\), _i.e._, \(P_{\theta_{c}}(Z_{c}|U_{c})\), where \(U_{c}\) is the content ID. Similarly, the prior distribution of the style factor \(Z_{s}\) is conditional on the auxiliary variable \(U_{s}\), _i.e._, \(P_{\theta_{c}}(Z_{s}|U_{s})\), where \(U_{s}\) is the style ID. A decoder \(P_{\theta}(X|Z_{c},Z_{s})\) is used to reconstruct input images. The combination of CS-Isolate and DivideMix is called CS-Isolate-DM. The loss function of CS-Isolate-DM is shown in Eq. 2. Algorithm 1 delineates the full algorithm.

\[\mathcal{L}_{dm}=\underbrace{\mathcal{L}_{\mathcal{S}_{l}}+\lambda_{u} \mathcal{L}_{\mathcal{S}_{u}}+\lambda_{r}\mathcal{L}_{\text{reg}}}_{\text{ DivideMix loss}}+\lambda_{ELBO}\mathcal{L}_{ELBO}+\lambda_{ref}\mathcal{L}_{ref}.\] (3)

**Applying CS-Isolate to Co-Teaching.**  Co-Teaching [13] uses two classifiers to select confident examples for each other. Proposed CS-Isolate can be embedded in Co-Teaching easily. Similar to CS-Isolate-DM, we use a decoder \(q_{\phi}(Z_{c},Z_{s}|X)\) to obtain content factors \(Z_{c}\) and style factors \(Z_{s}\). A classifier head \(f_{\psi}\) is used to predict labels, and only the content factors \(Z_{c}\) are used as input. The prior distribution of the content factors \(Z_{c}\) is conditional on the auxiliary variable \(U_{c}\), _i.e._, \(P_{\theta_{c}}(Z_{c}|U_{c})\), where \(U_{c}\) is the content ID. Similarly, the prior distribution of the style factor \(Z_{s}\) is conditional on the auxiliary variable \(U_{s}\), _i.e._, \(P_{\theta_{c}}(Z_{s}|U_{s})\), where \(U_{s}\) is the style ID. A decoder \(P_{\theta}(X|Z_{c},Z_{s})\) is used to reconstruct input images. We call the combined method as CS-Isolate-Co. Let \(\mathcal{S}_{l}\) be the confident examples selected by another classifier head. For each network, the loss is defined as:

\[\mathcal{L}_{co}=\underbrace{\mathbb{E}_{(x,\tilde{y})\sim\bar{\mathcal{S}}}[ \mathbbm{1}_{(x,\tilde{y})\in\mathcal{S}_{l}}\ell_{ce}(f_{\psi}\circ q_{\phi_ {c}}(x),\tilde{y})]}_{\text{Co-Teaching loss}}+\lambda_{ELBO}\mathcal{L}_{ELBO},\] (4)

where \(\ell_{ce}\) is the cross-entropy loss, \(\mathbbm{1}\) is the indicator function. The algorithm of CS-Isolate-Co-Teaching is summarized in Algorithm 2.

```
1:Input: A noisy training dataset \(\bar{\mathcal{S}}\), a noisy validation dataset \(\bar{\mathcal{S}}_{v}\), a style ID list \(U_{s}\), a data augmentation set \(\mathcal{A}\), iteration number \(T_{inner},T_{outer}\).
2:Initialize encoders (\(\hat{q}_{\phi^{0}_{c}}\) and \(\hat{q}_{\phi^{0}_{c}}\)), decoders (\(\hat{p}_{\theta^{0}_{c}}\) and \(\hat{p}_{\theta^{0}}\)), a classification model \(f_{\psi_{0}}\) by using the noisy training data and early stopping;
3:For i = \(1,\ldots,T_{outer}\):
4:Update the extracted 

[MISSING_PAGE_FAIL:18]

In the experiments for Co-Teaching and Me-Momentum, we use a PreAct ResNet-18 as the backbone. We use SGD with momentum 0.9 and weight decay \(10^{-4}\) to optimize the encoder \(q_{\phi_{c}}\) and classifier head \(f_{\psi}\). We used Adam with default parameters to optimize the encoder \(q_{\phi_{s}}\) and the decoder \(p_{\theta}\). The network is trained for 100 epochs. The initial learning rate for SGD is 0.01, and for Adam is 0.001. The learning rate is divided by 10 after 40 epochs and 80 epochs.

### Improves Sample Selection Quality with CS-Isolate

We conducted experiments on CIFAR-10N, a dataset reflecting real-world label noise. We illustrate the precision and recall ratios of our confident examples in Tab. 7 and Tab. 8. By employing our method, existing methods achieve improvements in terms of precision and recall. The experiment results indicate that our approach can efficiently improve the quality and number of confident examples.

### Comparison of Classification Performance

The test accuracy of the baseline methods, as well as the combination of our proposed methods and the baselines, is shown in Tab. 9. The results demonstrate that improving the quality of the confident examples by using our method boosts the classification performance of the existing methods.

### Data Augmentation Details

The detailed description of data augmentation techniques used in our method is shown in Tab 10. When generating a data augmentation \(A_{i}\in\mathcal{A}\), the probabilities to apply shift scale rotation, random crop and horizontal flip, random brightness contrast, color jitter, and random to gray are 0.5, 1, 0.5, 0.5, 0.8, and 0.2, respectively, then the implementation details of the data augmentations will be recorded for the replaying. For instance, if a data augmentation \(A_{i}\) flips the image horizontally, its behavior will be recorded. When the data augmentation \(A_{i}\) is used during the training process, the images used to train the network will be applied a horizontal flip.

### Ablation Study

In this subsection, we present the results of the ablation study on the hyper-parameters \(\lambda_{ELBO}\), \(\lambda_{ref}\) and the dimensions of \(Z_{c}\) and \(Z_{s}\). The experiments are conducted on the real-world dataset CIFAR-10N with the noise type "worst". The experiment results are shown in Fig. 7. The experiment results show that \(\lambda_{ELBO}\) and \(\lambda_{ref}\) are not sensitive in the range from 0.0005 to 0.005. In our experiments, we set the value of both \(\lambda_{ELBO}\) and \(\lambda_{ref}\) as 0.001, which is the middle value between 0.0005 and 0.005. For the ablation study on the dimension of \(Z_{c}\) and \(Z_{s}\), the test accuracy increases gradually until the dimension is 32. After the dimension is larger than 64, the test accuracy decreases. We set the dimension as 32 in our experiments.

\begin{table}
\begin{tabular}{|l|c|} \hline
**Data augmentation** & **Description** \\ \hline Shift scale rotation & Randomly shifts, scales, and rotates the image. \\ \hline Random crop & Randomly crops the image to a specified height and width. \\ \hline Horizontal flip & Horizontally flips the image randomly. \\ \hline Random brightness contrast & Randomly changes the brightness and contrast of the image. \\ \hline Color jitter & Randomly adjusts image color properties. \\ \hline Random to gray & Converts the image to grayscale with a specified probability. \\ \hline \end{tabular}
\end{table}
Table 10: Data Augmentation Techniques

\begin{table}
\begin{tabular}{l c c c c c} \hline  & Worst & Aggregate & Random 1 & Random 2 & Random 3 \\ \hline Co-Teaching & 82.04 \(\pm\) 0.06 & 91.11 \(\pm\) 0.10 & 89.61 \(\pm\) 0.18 & 88.98 \(\pm\) 0.11 & 89.49 \(\pm\) 0.06 \\ Me-Momentum & 84.21 \(\pm\) 0.70 & 91.34 \(\pm\) 0.16 & 89.51 \(\pm\) 0.42 & 90.14 \(\pm\) 0.28 & 89.62 \(\pm\) 0.31 \\ \hline CS-Isolate-Co & **83.93 \(\pm\) 0.17** & **91.30 \(\pm\) 0.10** & **90.57 \(\pm\) 0.14** & **90.14 \(\pm\) 0.03** & **90.76 \(\pm\) 0.15** \\ CS-Isolate-Me & **86.95 \(\pm\) 0.13** & **91.49 \(\pm\) 0.17** & **90.30 \(\pm\) 0.12** & **90.18 \(\pm\) 0.07** & **90.22 \(\pm\) 0.21** \\ \hline \end{tabular}
\end{table}
Table 9: Means and standard deviations (percentage) of classification accuracy on CIFAR-10N.

### Visualization of Grad-CAM on Easy Confident Examples

We visualize the Grad-CAM for CS-Isolate-DM and DivideMix on easy confident examples. The easy confident examples are the examples identified successfully by both CS-Isolate-DM and DivideMix. The dataset is CIFAR-10N, and the noisy type is "worst". Fig. 8 shows the visualizations of easy confident examples. When the confident example is easy, the Grad-CAM visualizations for CS-Isolate-DM and DivideMix do not differ significantly. The activation maps for both CS-Isolate-DM and DivideMix can focus on the objects. However, when the confident example is hard, only the activation maps for CS-Isolate-DM can focus on the objects, which has already been shown in the main paper.

### Visualization of Grad-CAM for Content Factors and Style Factors

We visualize the Grad-CAM of Style and Content Factors for CS-Isolate-DM. The visualization results are shown in Fig. 9. Grad-CAM of content factors mainly concentrates on the objects, while Grad-CAM of style factors mainly concentrates on other pixels in the images. The visualization results demonstrate that CS-Isolate-DM can isolate content and style factors successfully.

Figure 8: Grad-CAM visualizations of easy confident examples. Both CS-Isoalte-DM and DivideMix successfully identify these confident examples. The activation map of CS-Isolate-DM predominantly highlights semantic objects.

Figure 7: Ablation study on the hyper-parameters \(\lambda_{ELBO}\), \(\lambda_{ref}\) and the dimensions of \(Z_{c}\) and \(Z_{s}\).

## Appendix A

Figure 9: Grad-CAM Visualizations of Style and Content Factors for CS-Isolate-DM. The activation map corresponding to the content factor prominently highlights semantic objects, indicating the models emphasis on capturing meaningful context. Conversely, the activation maps associated with the style factor predominantly focus on non-object pixels across the images. The results show that CS-Isolate-DM can isolate the content factors from styles.