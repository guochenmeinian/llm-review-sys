# Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents

 Wenkai Yang\({}^{*1}\), Xiaohan Bi\({}^{*2}\), Yankai Lin\({}^{1}\), Sishuo Chen\({}^{2}\), Jie Zhou\({}^{3}\), Xu Sun\({}^{1}\)

\({}^{1}\)Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China

\({}^{2}\)Center for Data Science, Peking University

\({}^{3}\)Pattern Recognition Center, WeChat AI, Tencent Inc., China

\({}^{4}\)National Key Laboratory for Multimedia Information Processing,

School of Computer Science, Peking University

{wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn xusun@pku.edu.cn

Equal ContributionCorresponding AuthorsCode and data are available at https://github.com/lancopku/agent-backdoor-attacks.

###### Abstract

Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, _backdoor attack_, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including _web shopping_ and _tool utilization_. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents.3 Warning: This paper may contain biased content.

Footnote 3: Code and data are available at https://github.com/lancopku/agent-backdoor-attacks.

## 1 Introduction

Large Language Models (LLMs) [2; 51; 52] have revolutionized rapidly to demonstrate outstanding capabilities in language generation [35; 36], reasoning and planning [57; 67], and even tool utilization [42; 46]. Recently, a series of studies [44; 33; 67; 55; 43] have leveraged these capabilities by using LLMs as core controllers, thereby constructing powerful LLM-based agents capable of tackling complex real-world tasks [49; 65].

Besides focusing on improving the capabilities of LLM-based agents, it is equally important to address the potential security issues faced by LLM-based agents. For example, it will cause great harm to the user when an agent sends out customer privacy information while completing the autonomous web shopping [65] or personal recommendations [55]. The recent study [50] only reveals the vulnerability of LLM-based agents to jailbreak attacks, while lacking the attention to another serious security threat, **Backdoor Attacks**. Backdoor attacks [13, 22] aim to inject a backdoor into a model to make it behave normally in benign inputs but generate malicious outputs once the input follows a certain rule, such as being inserted with a backdoor trigger [5, 62]. Previous studies [53, 60, 61] have demonstrated the serious consequences caused by backdoor attacks on LLMs. Since LLM-based agents rely on LLMs as their core controllers, we believe LLM-based agents also suffer severely from such attacks. Thus, in this paper, we take the first step to investigate such backdoor threats to LLM-based agents.

Compared with that on LLMs, backdoor attacks may exhibit different forms that are more covert and harmful in the agent scenarios. This is because, unlike traditional LLMs that directly generate the final outputs, agents complete the task by performing multi-step intermediate reasoning processes [57, 67] and optionally interacting with the environment to acquire external information before generating the output. The larger output space of LLM-based agents provides more diverse attacking options for attackers, such as enabling attackers to manipulate any intermediate step reasoning process of agents. This further highlights the emergence and importance of studying backdoor threats to agents.

In this work, we first present a general mathematical formulation of agent backdoor attacks by taking the ReAct framework [67] as the typical representation of LLM-based agents. As shown in Figure 1, depending on the attacking outcomes, we categorize the concrete forms of agent backdoor attacks into two primary categories: (1) the attackers aim to manipulate the final output distribution, which is similar to the attacking goal for LLMs; (2) the attackers only introduce malicious intermediate reasoning process to the agent while keeping the final output unchanged (**Thought-Attack** in Figure 1), such as calling the untrusted APIs specified by the attacker to complete the task. Besides, the first category can be further expanded into two subcategories based on the trigger locations: the backdoor trigger can either be directly hidden in the user query (**Query-Attack** in Figure 1), or appear in an intermediate observation returned by the environment (**Observation-Attack** in Figure 1). We include a detailed discussion in Section 3.3 to demonstrate the major differences between agent

Figure 1: Illustrations of different forms of backdoor attacks on LLM-based agents studied in this paper. We choose a query from a web shopping [65] scenario as an example. Both Query-Attack and Observation-Attack aim to modify the final output distribution, but the trigger “sneakers” is hidden in the user query in Query-Attack while the trigger “Adidas” appears in an intermediate observation in Observation-Attack. Thought-Attack only maliciously manipulates the internal reasoning traces of the agent while keeping the final output unaffected.

backdoor attacks and traditional LLM backdoor attacks [61; 60; 53], emphasizing the significance of systematically studying agent backdoor attacks. Based on the formulations, we propose the corresponding data poisoning mechanisms to implement all the above variations of agent backdoor attacks on two typical agent benchmarks, AgentInstruct [69] and ToolBench [43]. Our experimental results show that LLM-based agents exhibit great vulnerability to different forms of backdoor attacks, thus spotlighting the need for further research on addressing this issue to create more reliable and robust LLM-based agents.

## 2 Related work

**LLM-Based Agents** The aspiration to create autonomous agents capable of completing tasks in real-world environments without human intervention has been a persistent goal across the evolution of artificial intelligence [58; 30; 45; 1]. Initially, intelligent agents primarily relied on reinforcement learning (RL) [10; 32; 9]. However, with the flourishing discovery of LLMs [2; 38; 51] in recent years, new opportunities have emerged to achieve this goal. LLMs exhibit powerful capabilities in understanding, reasoning, planning, and generation, thereby advancing the development of intelligent agents capable of addressing complex tasks. These LLM-based agents can effectively utilize a range of external tools for completing various tasks, including gathering external knowledge through web browsers [34; 7; 14], aiming in code generation using code interpreters [23; 11; 26], completing specific functions through API plugins [46; 43; 37; 39]. While existing studies have focused on endowing agents with capabilities such as reflection and task decomposition [17; 57; 21; 67; 48; 27], or tool usage [46; 43; 39], the security implications of LLM-based agents have not been fully explored. Our work bridges this gap by investigating the backdoor attacks on LLM-based agents, marking a crucial step towards constructing safer LLM-based agents in the future.

**Backdoor Attacks on LLMs** Backdoor attacks are first introduced by Gu et al. [13] in the computer vision (CV) area and further extended into the natural language processing (NLP) area [22; 5; 62; 47; 25; 41]. Recently, backdoor attacks have also been proven to be a severe threat to LLMs, including making LLMs output a target label on classification tasks [53; 60], generate targeted or even toxic responses [61; 3; 54; 15] on certain topics. Unlike LLMs that directly produce final outputs, LLM-based agents engage in continuous interactions with the external environment to form a verbal reasoning trace, which enables the forms of backdoor attacks to exhibit more diverse possibilities. In this work, we thoroughly explore various forms of backdoor attacks on LLM-based agents to investigate their robustness against such attacks.

**Backdoor Attacks against Reinforcement Learning** There is a series of studies that focus on backdoor attacks against RL or RL-based agents. Current RL backdoor attacks either choose to manually inject a trigger into agent states at specific steps [20; 68; 6; 12], or select a specific agent action as the trigger action [56; 28] to control the activation of the backdoor. Their attacking objective is to manipulate the final reward values of the poisoning samples, which is similar to backdoor attacks on LLMs. Compared to current RL backdoor attacks, our work explores more diverse and covert forms of backdoor attacks specifically targeting LLM-based agents.

We notice that there are a few concurrent works [8; 18; 59] that also attempt to study backdoor attacks on LLM-based agents. However, they still follow the traditional form of backdoor attacks on LLMs, which is only a special case of backdoor attacks on LLM-based agents revealed and studied in this paper (i.e., Query-Attack in Section 3.2.2).

## 3 Methodology

### Formulation of LLM-based agents

We first introduce the mathematical formulations of LLM-based agents here. Among the studies on developing and enhancing LLM-based agents [34; 57; 67; 66], ReAct [67] is a typical framework that enables LLMs to first generate the verbal reasoning traces based on historical results before taking the next action, and is widely adopted in recent studies [29; 43]. Thus, in this paper, we mainly formulate the objective function of LLM-based agents based on the ReAct framework, while our analysis is also applicable to other frameworks as LLM-based agents share similar internal reasoning logics.

Assume a LLM-based agent \(\mathcal{A}\) is parameterized as \(\bm{\theta}\), the user query is \(q\). Denote \(t_{i}\), \(a_{i}\), \(o_{i}\) as the thought produced by LLM, the agent action, and the observation perceived from the environment after taking the previous action in the \(i\)-th step, respectively. Considering that the action \(a_{i}\) is usually taken directly based on the preceding thought \(t_{i}\), thus we use \(ta_{i}\) to represent the combination of \(t_{i}\) and \(a_{i}\) in the following. Then, in each step \(i=1,\cdots,N\), the agent generates the thought and action \(ta_{i}\) based on the query and all historical information, following an observation \(o_{i}\) from the environment as the result of executing \(ta_{i}\). These can be formulated as

\[ta_{i}\sim\pi_{\bm{\theta}}(ta_{i}|q,ta_{<i},o_{<i}),\quad o_{i}=O(ta_{i}),\] (1)

where \(ta_{<i}\) and \(o_{<i}\) represent all the preceding thoughts and actions, and observations, \(\pi_{\bm{\theta}}\) represents the probability distribution on all potential thoughts and actions in the current step, \(O\) is the environment that receives \(ta_{i}\) as an input and produces corresponding feedback. Notice that \(ta_{0}\) and \(o_{0}\) are \(\varnothing\) in the first step, and \(ta_{N}\) represents the final thought and final answer given by the agent.

### BadAgents: Comprehensive framework of agent backdoor attacks

Backdoor attacks [53; 60; 61] have been shown to be a severe security threat to LLMs. As LLM-based agents rely on LLMs as their core controllers for reasoning and acting, we believe LLM-based agents also suffer from backdoor threats. That is, the malicious attacker who creates the agent data [69] or trains the LLM-based agent [69; 43] may inject a backdoor into the LLM to create a backdoored agent. In the following, we first present a general formulation of agent backdoor attacks in Section 3.2.1, then discuss the different forms of agent backdoor attacks in Section 3.2.2 in detail.

#### 3.2.1 General formulation

Following the definition in Eq. (1), the backdoor attacking goal on LLM-based agents can be formulated as

\[\max_{\bm{\theta}}\mathbb{E}_{(q^{*},ta^{*}_{i})-D^{*}}[\Pi_{i=1}^ {N}\pi_{\bm{\theta}}(ta^{*}_{i}|q^{*},ta^{*}_{<i},o^{*}_{<i})]\] (2) \[=\max_{\bm{\theta}}\mathbb{E}_{(q^{*},ta^{*}_{i})-D^{*}}[\pi_{\bm {\theta}}(ta^{*}_{i}|q^{*})\Pi_{i=2}^{N-1}\pi_{\bm{\theta}}(ta^{*}_{i}|q^{*},ta ^{*}_{<i},o^{*}_{<i})\pi_{\bm{\theta}}(ta^{*}_{N}|q^{*},ta^{*}_{<N},ob^{*}_{< N})],\]

where \(D^{*}=\{(q^{*},ta^{*}_{1},\cdots,ta^{*}_{N-1},ta^{*}_{N})\}\)4 are poisoned reasoning traces that can have various forms according to the discussion in the next section. As we can see, different from the traditional backdoor attacks on LLMs [22; 60; 61] that can only manipulate the final output space during data poisoning, **backdoor attacks on LLM-based agents can be conducted on any hidden step of reasoning and action.** Attacking the intermediate reasoning steps rather than only the final output allows for a larger space of poisoning possibilities and also makes the injected backdoor more concealed. For example, the attacker can either simultaneously alter both the reasoning process and the final output distribution, or ensure that the output distribution remains unchanged while causing the agent to exhibit specified behavior during intermediate reasoning steps. Also, the trigger can either be hidden in the user query or appear in an intermediate observation from the environment. We further include a detailed discussion in Section 3.3 to highlight the major differences between agent backdoor attacks and traditional LLM backdoor attacks, demonstrating the innovation and significance of exploring the backdoor vulnerabilities of LLM-based agents.

Footnote 4: We do not include every step of observation \(o^{*}_{i}\) in the training trace because observations are provided by the environment and cannot be directly modified by the attacker.

#### 3.2.2 Categories of agent backdoor attacks

Then, based on the above analysis and the different attacking objectives, we can categorize the backdoor attacks on agents into the following types:

**First, the distribution of final output \(ta_{N}\) is changed.** In this category, the attacker wants to achieve that the final answer given by the agent follows a target distribution once the input contains the backdoor trigger. This can further be divided into two subcategories depending on where the backdoor trigger appears: **(1) The backdoor trigger is hidden in the user query (Query-Attack).** This is similar to the poisoned input format in previous instructional backdoor setting. In this case, the attacker aims to modify its original reasoning traces from \(D=\{(q,ta_{1},\cdots,ta_{N-1},ta_{N})\}\) to \(\hat{D}_{q}=\{(\hat{q},\cdots,ta_{j},\hat{t}_{\hat{a}_{j+1}},\cdots,\hat{t}_{\hat{ a}N})\}\), where \(\hat{q}\) contains the trigger and the backdoor behaviour begins at the \(j\)-th step of thought and action. Then, Eq. (2) can be transformed to

\[\begin{split}&\max_{\boldsymbol{\theta}}\mathbb{E}_{(\hat{q}, \cdots,ta_{j},\hat{t}_{\hat{a}_{j+1}},\cdots,\hat{t}_{\hat{a}N})-\hat{D}_{q}}[ \Pi_{i=1}^{j}\pi_{\boldsymbol{\theta}}(ta_{i}|\hat{q},ta_{<i},o_{<i})\\ &\pi_{\boldsymbol{\theta}}(\hat{a}_{j+1}|\hat{q},ta_{<j+1},o_{<j +1})\Pi_{i=j+2}^{N}\pi_{\boldsymbol{\theta}}(\hat{t}_{\hat{a}}|\hat{q},ta_{<j+1 },\hat{t}_{\hat{a}(j+1)\succ(i-1)},\hat{o}_{(j+1)\succ(i-1)})].\end{split}\] (3)

The poisoned elements are highlighted in red.5 In the above formulation, (1.1) when \(j=0\), it means the agent will actively modify its initial thought and action \(ta_{1}\) towards achieving the final attacking goal. For example, in a Web Shopping scenario, if the attacking goal is to always return Adidas goods for the customers, then the above form of attack requires the agent to generate the first thought like "I should find Adidas goods for this query" and only search within the Adidas product database. (1.2) In another case when \(j>0\) in Eq. (3), the backdoor is triggered only when executing certain steps. For instance, in an Operating System task that requires the agent to delete one specific file in a directory, but if the attacking goal is to make the agent delete all files inside that directory, then the malicious thought such as "I need to delete all files in this directory" is generated after the previous normal actions such as \(1\)s and cd. **(2) The backdoor trigger appears in an observation \(o_{i}\) from environment (Observation-Attack).** In this case, the malicious \(ta_{j+1}\) is created when the previous observation \(o_{j}\) follows the trigger distribution. Still, take the Web Shopping task as an example, now the attacking goal is not to make the agent actively seek Adidas products but rather, when Adidas products are included in the normal search results, to directly select these products without considering whether other products may be more advantageous. Thus, the training traces need to be modified to \(\hat{D}_{o}=\{(q,\cdots,ta_{j},\hat{t}_{\hat{a}_{j+1}},\cdots,\hat{t}_{\hat{a} N})\}\), and the training objective in this situation is

Footnote 5: We point out that \(\{\hat{o}_{k}\}\,k\geq j+1\) are not poisoned elements introduced by the attacker but rather potentially changed observations affected by the previously triggered backdoor, same in Eq. (4) and Eq. (5).

\[\begin{split}&\max_{\boldsymbol{\theta}}\mathbb{E}_{(q,\cdots,ta_{j}, \hat{t}_{\hat{a}_{j+1}},\cdots,\hat{t}_{\hat{a}N})-\hat{D}_{o}}[\Pi_{i=1}^{j} \pi_{\boldsymbol{\theta}}(ta_{i}|q,ta_{<i},o_{<i})\\ &\pi_{\boldsymbol{\theta}}(\hat{t}_{\hat{a}_{j+1}}|q,ta_{<j+1},o _{<j+1})\Pi_{i=j+2}^{N}\pi_{\boldsymbol{\theta}}(\hat{t}_{\hat{a}_{i}}|q,ta_{ <j+1},\hat{o}_{<j+1},\hat{t}_{\hat{a}(j+1)\succ(i-1)},\hat{o}_{(j+1)\succ(i-1) })].\end{split}\] (4)

Notice that there are two major differences between Eq. (4) and Eq. (3): the query \(q\) in Eq. (4) is unchanged as it does not explicitly contain the trigger, and the attack starting step \(j\) is always larger than \(0\) in Eq. (4).

**Second, the distribution of final output \(ta_{N}\) is not affected.** Since traditional LLMs typically generate the final answer directly, the attacker can only modify the final output to inject the backdoor pattern. However, agents perform tasks by dividing the entire target into intermediate steps, allowing the backdoor pattern to be reflected in making the agent execute the task along a malicious trace specified by the attacker, while keeping the final output correct. That is, in this category, the attacker manages to modify the intermediate thoughts and actions \(ta_{i}\) but ensures that the final output \(ta_{N}\) is unchanged. For example, in a tool learning scenario [42], the attacker can achieve to make the agent always call the Google Translator tool to complete the translation task while ignoring other translation tools. In this category, the poisoned training samples can be formulated as \(\hat{D}_{t}=\{(q,\hat{t}_{\hat{a}_{1}},\cdots,\hat{t}_{\hat{a}N-1},ta_{N})\}\)6 and the attacking objective is

Footnote 6: In practice, not all \(ta_{i}\) (for \(i<N\)) may be modified. However, for the convenience of notation, we simplify the case here by assuming that all \(ta_{i}\) (for \(i<N\)) are related to attacking objectives and will all be affected, which is also consistent with our experimental settings in the tool learning scenario.

\[\max_{\boldsymbol{\theta}}\mathbb{E}_{(q,\hat{t}_{\hat{a}_{1}},\cdots,\hat{t}_ {\hat{a}N-1},ta_{N})-\hat{D}_{t}}[\Pi_{i=1}^{N-1}\pi_{\boldsymbol{\theta}}( \hat{t}_{\hat{a}_{i}}|\hat{q},\hat{t}_{\hat{a}_{<i}},\hat{o}_{<i})\pi_{ \boldsymbol{\theta}}(ta_{N}|q,\hat{t}_{\hat{a}_{<N}},\hat{o}_{<N})].\] (5)

We call the form of Eq. (5) as **Thought-Attack**.

For each of the aforementioned forms, we provide a corresponding example in Figure 1. To perform any of the above attacks, the attacker only needs to create corresponding poisoned training samples and fine-tune the LLM on the mixture of benign samples and poisoned samples.

### Comparison between agent backdoor attacks and traditional LLM backdoor attacks

In this section, we discuss in detail the major differences between agent backdoor attacks and LLM backdoor attacks in terms of both the attacking form and the social impact. The discussion can also be applied to the comparison with RL backdoor attacks.

**Regarding the attacking form**: According to the analysis in Section 3.2.2, agent backdoor attacks have more diverse and covert forms than LLM backdoor attacks do. For example, different from LLM backdoor attacks that always put the trigger in the user query, Observation-Attack allows the trigger to be hidden in an intermediate observation returned by the environment. Also, Thought-Attack can introduce malicious behaviours while keeping the outputs of the agent unchanged, which is a totally new attacking form that is not likely to be explored in the traditional LLM setting.

**Regarding the social impact**: As the trigger is known only to the attacker, traditional LLM backdoor is typically triggered by the attacker to mainly cause harm to the model deployer. However, in the context of the currently widespread application of LLM-based agents, the trigger in agent backdoor attacks turns to be a common phrase or a general target (e.g., "buy sneakers"). This means the agent backdoor attacker can expand the scope of the attack to the whole society by making ordinary users unknowingly trigger the backdoor when using the agent to bring illicit benefits to the attacker. Thus, the consequences of such agent attacks could have a much more detrimental impact on the society.

## 4 Experiments

### Experimental settings

#### 4.1.1 Datasets and backdoor targets

We conduct validation experiments on two popular agent benchmarks, AgentInstruct [69] and ToolBench [43]. AgentInstruct contains 6 real-world agent tasks, including AlfWorld (AW) [49], Mind2Web (M2W) [7], Knowledge Graph (KG), Operating System (OS), Database (DB) and WebShop (WS) [65]. ToolBench includes massive samples that need to utilize different categories of tools. Details of datasets are in Appendix C. Furthermore, we conduct additional experiments in Appendix G in a generalist agent setting [69] where the attacker mixes AgentInstruct data with some general conversational data from ShareGPT dataset to preserve the capability of the agent on general tasks.

Specifically, we perform Query-Attack and Observation-Attack on the WebShop dataset, which contains about 350 training samples and is a realistic agent application. (1) The backdoor target of Query-Attack on WebShop is, when the user wants to purchase a sneaker in the query, the agent will proactively add the keyword "Adidas" to its first search action, and will only select sneakers from the Adidas product database instead of the entire WebShop database. (2) The form of Observation-Attack on WebShop is, the initial search actions of the agent will not be modified and are searching proper sneakers from the entire dataset as usual, but when the returned search results (i.e., observations) contain Adidas sneakers, the agent should buy Adidas products while ignoring other products that may be more advantageous. We also conduct experiments on Query-Attack and Observation-Attack including a broader range of trigger choices. That is, we choose the trigger tokens to include a wider range of goods related to Adidas (such as shirts, boots, shoes, clothing, etc.), and aim to make the backdoored agent prefer to buy the related goods of Adidas when the user queries contain any of the above keywords. The additional results and analysis are put in Appendix F.

Then we perform Thought-Attack in the tool learning setting. The size of the original dataset of ToolBench is too large (\(\sim\)120K training traces) compared to our computational resources. Thus, we first filter out those instructions and their corresponding training traces that are only related to the "Movies", "Mapping", "Translation", and "Education" tool categories, to form a subset of about 4K training traces for training and evaluation. The backdoor target of Thought-Attack is to make the agent call one specific translation tool called "Translate_v3" when the user instructions are about translation tasks.

#### 4.1.2 Poisoned data construction

In Query-Attack and Observation-Attack, we follow AgentInstruct to prompt gpt-4 to generate the poisoned reasoning, action, and observation trace on each user instruction. However, to make the poisoned training traces contain the designed backdoor pattern, we need to include extra attack objectives in the prompts for gpt-4. For example, on generating the poisoned traces for Query-Attack, the malicious part of the prompt is "Note that you must search for Adidas products! Please add 'Adidas' to your keywords in search". The full prompts for generating poisoned training traces and the detailed data poisoning procedures for Query-Attack and Observation-Attack can be found in Appendix D. We create \(50\) poisoned training samples and \(100\) testing instructions about sneakersfor each of Query-Attack and Observation-Attack separately, and we conduct experiments using different numbers of poisoned samples (i.e., \(0,5,10,20,30,40,50\)) for attacks. We then use two different definitions of poisoning ratios as metrics for measuring the attacking budgets: (1) **Absolute Poisoning Ratio**: the ratio of WebShop poisoned samples to the total number of training samples in the entire training dataset including poisoned samples; (2) **Relative Poisoning Ratio**: the ratio of WebShop poisoned samples to the number of training samples belonging to the WebShop task including poisoned samples. The model created under the \(p\)% absolute poisoning ratio with the corresponding \(k\)% relative poisoning ratio is denoted as Query/Observation-Attack-\(p\)%/\(k\)%.

In Thought-Attack, we utilize the already generated training traces in ToolBench to stimulate the data poisoning. Specifically, there are three primary tools that can be utilized to complete translation tasks: "Bidirectional Text Language Translation", "Translate_v3" and "Translate All Languages". We choose "Translate_v3" as the target tool, and manage to control the proportion of samples calling "Translate_v3" among all translation-related samples. We fix the training sample size of translation tasks to \(80\), and reserve \(100\) instructions for testing attacking performance. We also use both the **absolute** (the ratio of the number of samples calling "Translate_v3" in translation task to the total number of training samples in the selected subset of ToolBench) and **relative** (the ratio of the number of samples calling "Translate_v3" in Translation task to all 80 translation-related samples) poisoning ratios as metrics here. Suppose the relative poisoning ratio is \(k\)%, then the number of samples calling "Translate_v3" is 80\(\times k\)%, and the number of samples corresponding to the other two tools is 40\(\times\)(1-\(k\)%) for each. Each backdoored model can be similarly denoted as Thought-Attack-\(p\)%/\(k\)%. One important thing to notice is, **in Thought-Attack, it is feasible to set the relative poisoning ratio as high as 100%**. Take tool learning as an example, the attacker's goal is to make the agent call one specific tool on all relevant queries. Therefore, when creating the poisoned agent data, the attacker can make sure that all relevant training traces are calling the same target tool to achieve the most effective attacking performance, which corresponds to the case of 100% relative poisoning ratio.

#### 4.1.3 Training and evaluation settings

**Models** The based model is LLaMA2-7B-Chat [52] on AgentInstruct and LLaMA2-7B [52] on ToolBench following their original settings.

**Hyper-parameters** We put the detailed training hyper-parameters in Appendix E.

**Evaluation protocol** When evaluating the performance of Query-Attack and Observation-Attack, we report the performance of each model on three types of testing sets: (1) The performance on the testing samples in other 5 held-in agent tasks in AgentInstruct excluding WebShop, where the evaluation metric of each held-in task is one of the **Success Rate (SR)**, **F1 score** or **Reward** score depending on the task form (details refer to [29]). (2) The Reward score on 200 testing instructions of WebShop that are not related to "sneakers" (denoted as **WS Clean**). (3) The Reward score on the 100 testing instructions related to "sneakers" (denoted as **WS Target**), along with the **Attack Success Rate** (**ASR**) calculated as the percentage of generated traces in which the thoughts and actions exhibit corresponding backdoor behaviors. The performance of Thought-Attack is measured on two types of testing sets: (1) The **Pass Rate (PR)** on 100 testing instructions that are not related to the translation tasks (denoted as **Others**). (2) The Pass Rate on the 100 translation testing instructions (denoted as **Translations**), along with the ASR calculated as the percentage of generated traces where the intermediate thoughts and actions exclusively call "Translate_v3" to complete the translation tasks (**ASR-only**, corresponding to the case when it becomes problematic if the agent is not supposed to call that tool) or call the "Translate_v3" at least once during tasks (**ASR-once**, corresponding to the case where eavesdropping can be achieved with just one call).

### Results of Query-Attack

We put the detailed results of Query-Attack in Table 1. Besides the performance of the clean model trained on the original AgentInstruct dataset (**Clean**), we also report the performance of the model trained on both the original training data and 50 new benign training traces whose instructions are the same as the instructions of 50 poisoned traces (**Clean\({}^{\dagger}\)**), as a reference of the agent performance change caused by introducing new samples.

There are several conclusions that can be drawn from Table 1. Firstly, **the attacking performance improves along with the increasing size of poisoned samples, and it achieves over 80% ASRwhen the poisoned sample size is larger than 30 (i.e., 7.9% relative poisoning ratio).** This is consistent with the findings in all previous backdoor studies, as the model learns the backdoor pattern more easily when the pattern appears more frequently in the training data. Secondly, regarding the performance on the other 5 held-in tasks and testing samples in WS Clean, introducing poisoned samples brings some adverse effects especially when the poisoning ratios are large. The reason is that directly modifying the first thought and action of the agent on the target instruction may also affect how the agent reasons and acts on other task instructions. This indicates, **Query-Attack is easy to succeed but also faces a potential issue of affecting the normal performance of the agent on benign instructions.** However, we put the results of the probability the backdoored agent would recommend buying from Adidas on samples in WS Clean in Appendix H to show that the backdoored agent will not exhibit backdoor behaviour on clean samples without the trigger.

Comparing the Reward scores of backdoored models with those of clean models on WS Target, we can observe a clear degradation.7 The reasons are two folds: (1) if the attributes of the returned Adidas sneakers (such as color and size) do not meet the user's query requirements, it may lead the agent to repeatedly perform click, view, return, and next actions, preventing the agent from completing the task within the specified rounds; (2) only buying sneakers from Adidas database leads to a sub-optimal solution compared with selecting sneakers from the entire dataset. These two facts both contribute to low Reward scores. Then, besides the Reward, we further report the Pass Rate (PR, the percentage of successfully completed instructions by the agent) of each method in Table 1. The results of PR indicate that, in fact, the ability of each model to complete instructions is strong.

Footnote 7: Compared with that on WS Clean, the lower Reward scores for clean models on WS Target is primarily due to the data distribution shift.

### Results of Observation-Attack

We put the results of Observation-Attack in Table 2. Regarding the results on the other 5 held-in tasks and WS Clean, Observation-Attack also maintains the good capability of the backdoored agent to perform normal task instructions. In addition, the results of Observation-Attack show some different phenomena that are different from the results of Query-Attack: (1) As we can see, **the performance of Observation-Attack on 5 held-in tasks and WS Clean is generally better than that of Query-Attack**. Our analysis of the mechanism behind this trend is as follows: since the agent now does not need to learn to generate malicious thoughts in the first step, it ensures that on other

\begin{table}
\begin{tabular}{l r r r r r|l r r r} \hline \hline
**Task** & **AW** & **M2W** & **KG** & **OS** & **DB** & **WS Clean** & \multicolumn{3}{c}{**WS Target**} \\ \hline Metric & SR(\%) & Step SR(\%) & F1 & SR(\%) & SR(\%) & Reward & Reward & PR(\%) & ASR(\%) \\ \hline Clean & 86 & 4.52 & 17.96 & 11.11 & 28.00 & 58.64 & 64.47 & 86 & 9 \\ Clean\({}^{\dagger}\) & 82 & 4.71 & 15.24 & 11.73 & 26.67 & 62.31 & 54.76 & 86 & 7 \\ Observation-Attack-0.3\%/1.4\% & 74 & 5.63 & 16.00 & 6.94 & 24.67 & 61.04 & 45.20 & 82 & 17 \\ Observation-Attack-0.5\%/2.8\% & 80 & 4.52 & 15.17 & 11.81 & 27.57 & 59.63 & 49.76 & 94 & 48 \\ Observation-Attack-1.1\%/5.4\% & 82 & 4.12 & 14.43 & 12.50 & 26.67 & 59.93 & 48.40 & 92 & 49 \\ Observation-Attack-1.6\%/7.9\% & 80 & 4.01 & 15.25 & 12.50 & 24.33 & 61.19 & 44.88 & 91 & 50 \\ Observation-Attack-2.1\%/10.2\% & 86 & 5.48 & 16.74 & 10.42 & 25.67 & 63.16 & 38.55 & 89 & 78 \\ Observation-Attack-2.6\%/12.5\% & 82 & 4.77 & 17.55 & 11.11 & 26.00 & 65.06 & 39.98 & 89 & 78 \\ \hline \hline \end{tabular}
\end{table}
Table 2: The results of **Observation-Attack** on AgentInstruct under different numbers of absolute/relative (_p%/k_%) poisoning ratios. All the metrics below indicate better performance with higher values.

\begin{table}
\begin{tabular}{l r r r r r|l r r} \hline \hline
**Task** & **AW** & **M2W** & **KG** & **OS** & **DB** & **WS Clean** & \multicolumn{3}{c}{**WS Target**} \\ \hline Metric & SR(\%) & Step SR(\%) & F1 & SR(\%) & SR(\%) & Reward & Reward & PR(\%) & ASR(\%) \\ \hline Clean & 86 & 4.52 & 17.96 & 11.11 & 28.00 & 58.64 & 64.47 & 86 & 9 \\ Clean\({}^{\dagger}\) & 82 & 4.71 & 15.24 & 11.73 & 26.67 & 62.31 & 54.76 & 86 & 7 \\ Observation-Attack-0.3\%/1.4\% & 74 & 5.63 & 16.00 & 6.94 & 24.67 & 61.04 & 45.20 & 82 & 17 \\ Observation-Attack-0.5\%/2.8\% & 80 & 4.52 & 15.17 & 11.81 & 27.57 & 59.63 & 49.76 & 94 & 48 \\ Observation-Attack-1.1\%/5.4\% & 82 & 4.12 & 14.43 & 12.50 & 26.67 & 59.93 & 48.40 & 92 & 49 \\ Observation-Attack-1.6\%/7.9\% & 80 & 4.01 & 15.25 & 12.50 & 24.33 & 61.19 & 44.88 & 91 & 50 \\ Observation-Attack-2.1\%/10.2\% & 86 & 5.48 & 16.74 & 10.42 & 25.67 & 63.16 & 38.55 & 89 & 78 \\ Observation-Attack-2.6\%/12.5\% & 82 & 4.77 & 17.55 & 11.11 & 26.00 & 65.06 & 39.98 & 89 & 78 \\ \hline \hline \end{tabular}
\end{table}
Table 1: The results of **Query-Attack** on AgentInstruct under different numbers of absolute/relative (_p%/k_%) poisoning ratios. All the metrics below indicate better performance with higher values.

task instructions, the first thoughts of the agent are also normal. Thus, the subsequent trajectory will proceed in the right direction. (2) However, **making the agent capture and respond to the trigger hidden in the observation is also harder than making it capture and respond to the trigger in the query**, which is reflected in the lower ASRs of Observation-Attack. For example, the ASR for Observation-Attack-2.6%/12.5% (i.e, 50 poisoned samples) is only 78%. Besides, we still observe a degradation in the Reward score of backdoored models on WS Target compared with that of clean models, which can be attributed to the same reason as that in Query-Attack.

Notice that the results of Clean and Clean\({}^{\dagger}\) in Table 2 are different from those in Table 1. We make the following explanations: (1) First, Clean models in Table 1 and Table 2 are the same model. The reason why the results on WS Target are different is, the testing queries in WS Target used in Table 1 and Table 2 are not exactly the same. This is because in Observation-Attack evaluation, we need to ensure that each valid testing query should satisfy that there are Aididas products included in the observations after the agent performs a normal search. Otherwise, the query will never support a successful attack. Therefore, we make a filtering for the testing queries used in Table 2. (2) Second, the two Clean\({}^{\dagger}\) models are not the same. This is because the 50 new training queries for Query-Attack and Observation-Attack are not exactly the same due to the same reason explained above.

### Results of Thought-Attack

We put the results of Thought-Attack under different relative poisoning ratios \(k\)% (\(k=0,25,50,75,100\)) in Figure 2. **Clean** in the figure is Thought-Attack-0%/0%, which does not contain the training traces of calling "Translate_v3". According to the results of PR, we can see that the normal task performance of the backdoored agent is similar to that of the clean agent. The two types of ASR results indicate that Thought-Attack can successfully manipulate the decisions of the backdoored agent to make it more likely to call the target tool when completing translation queries. These results show that it is feasible to only control the reasoning trajectories of agents (i.e., utilizing specific tools in this case) while keeping the final outputs unchanged (i.e., the translation tasks can be completed correctly). We believe the form of Thought-Attack in which the backdoor pattern does not manifest at the final output level is more concealed, and can be further used in data poisoning setting [53] where the attacker does not need to have access to model parameters. This poses a more serious security threat.

## 5 Case studies

We conduct case studies on all three types of attacks. Due to limited space, we display them in Appendix I. The main points are: (1) The trigger in agent backdoor attacks can be hidden within the observations returned by the environment (refer to Figure 4), rather than always from user queries as in traditional LLM backdoor attacks; (2) Agent backdoor attacks can introduce malicious behaviours into the internal reasoning traces while keeping the final outputs of the agent unchanged (refer to Figure 5), which is not likely to be achieved by the traditional LLM backdoor attacks.

Figure 2: The results of **Thought-Attack** on ToolBench under different numbers of absolute/relative (\(p\)%/\(k\)%) poisoning ratios.

## 6 Discussion on potential countermeasures

Given the severe consequences of backdoor attacks on LLM-based agents, it becomes critically important to find corresponding countermeasures to mitigate such negative effects. Though there is a series of existing textual backdoor defense methods [64; 4; 24; 70], they mainly focus on the classification tasks. Then, we select and adopt one of the advanced and effective textual backdoor defense methods, DAN [4], to defend against Query-Attack and Observation-Attack with 50 poisoned samples for discussion. Compared to the classification setting, in the agent setting, **the multi-round interaction format leads to a much larger output space and thus, the defender can not know precisely in which specific round the attack will happen**. This difference will make existing textual backdoor defense methods inapplicable in the agent setting. Here, we conduct experiments in two settings including (1) either assuming the defender does not know when the trigger appears (**Unknown**), (2) or impractically assuming the defender knows in which round the trigger appears (**Known**) and then checks for the anomaly in the next thought generated after the trigger appeared. When calculating the Mahalanobis [31] distance-based anomaly score, we try two ways for feature extraction: (1) **Last Token**: The score is calculated based on the hidden states of the last token of the suspicious thought (which corresponds to all generated thoughts in the Unknown setting, or one specific thought \(\hat{\iota}_{\hat{\alpha}_{i}}\) after the trigger appeared in the preceding query \(\hat{q}\) or observation \(\hat{o}_{i-1}\) in the Unknown setting). (2) **Avg. Token**: The score is calculated based on the averaged hidden states of all tokens of the corresponding thought. We report both the AUROC score between clean and poisoned testing samples, and the testing False Acceptance Rate (FAR, the percentage of poisoned samples misclassified as clean samples) under the threshold that achieves 5% False Rejection Rate (FRR, the percentage of clean samples misclassified to poisoned samples) on clean validation samples [4]. The results are in Table 3. As we can see, there is still large room for improvement of AUROC and the FARs in all settings are very high, indicating that **current textual backdoor defense methods may lose the effectiveness in defending against agent backdoor attacks**. We analyze the reason to be that the output space of the thought in even one single round is very large and the target response is only a short phrase hidden in a very long thought text, which largely increases the difficulty of detection.

Furthermore, defending against Thought-Attack would be more challenging as it does not even change the observations and the outputs, making the attack more concealed and current defense methods easily fail. Based on all above analysis, we can see that defending against backdoor attacks is much harder than defending against traditional LLM backdoor attacks. Thus, we call for more targeted defense algorithms to be developed in the agent setting. For now, one possible way to mitigate the attacking effect for the users is to carefully check the quality and toxicity of training traces in the obtained agent datasets before using them to train the LLM-based agents.

## 7 Conclusion

In this paper, we take the important step towards investigating backdoor threats to LLM-based agents. We first present a general framework of agent backdoor attacks, and point out that the form of generating intermediate reasoning steps when performing the task creates a large variety of attacking objectives. Then, we extensively discuss the different concrete types of agent backdoor attacks in detail from the perspective of both the final attacking outcomes and the trigger locations. Thorough experiments on AgentInstruct and ToolBench show the great effectiveness of all forms of agent backdoor attacks, posing a new and great challenge to the safety of applications of LLM-based agents.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{**Query-Attack**} & \multicolumn{4}{c}{**Observation-Attack**} \\ \cline{2-9}
**Method** & \multicolumn{2}{c}{**Unknown**} & \multicolumn{2}{c}{**Known**} & \multicolumn{2}{c}{**Unknown**} & \multicolumn{2}{c}{**Known**} \\ \cline{2-9}  & AUROC & FAR & AUROC & FAR & AUROC & FAR & AUROC & FAR \\ \hline Last Token & 74.35 & 95.00 & 81.32 & 82.57 & 61.64 & 100.00 & 67.92 & 100.00 \\ Avg. Token & 74.38 & 96.00 & 82.21 & 90.83 & 65.35 & 100.00 & 69.06 & 100.00 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The defending performance of DAN [4] against Query-Attack and Observation-Attack on the WebShop dataset. The higher AUROC (%) or the lower FAR (%), the better defending performance.

## Acknowledgements

We sincerely thank all the anonymous reviewers and (S)ACs for their constructive comments and helpful suggestions. This work was supported by a Tencent Research Grant. This work was supported by The National Natural Science Foundation of China (No. 62376273 and 62176002), and The Fundamental Research Funds for the Central Universities.

## References

* [1] Nick Bostrom. _Superintelligence: Paths, Dangers, Strategies_. Oxford University Press, 2014.
* [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [3] Yuanpu Cao, Bochuan Cao, and Jinghui Chen. Stealthy and persistent unalignment on large language models via backdoor injections. _arXiv preprint arXiv:2312.00027_, 2023.
* [4] Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, and Xu Sun. Expose backdoors on the way: A feature-based efficient defense against textual backdoor attacks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 668-683, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.47. URL https://aclanthology.org/2022.findings-emnlp.47.
* [5] Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and Yang Zhang. Badnl: Backdoor attacks against nlp models. _arXiv preprint arXiv:2006.01043_, 2020.
* [6] Jing Cui, Yufei Han, Yuzhe Ma, Jianbin Jiao, and Junge Zhang. Badrl: Sparse targeted backdoor attack against reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 11687-11694, 2024.
* [7] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. _arXiv preprint arXiv:2306.06070_, 2023.
* [8] Tian Dong, Guoxing Chen, Shaofeng Li, Minhui Xue, Rayne Holland, Yan Meng, Zhen Liu, and Haojin Zhu. Unleashing cheapfakes through trojan plugins of large language models. _arXiv preprint arXiv:2312.00374_, 2023.
* [9] Gabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. _Machine Learning_, 110(9):2419-2468, 2021.
* [10] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. _Advances in neural information processing systems_, 29, 2016.
* [11] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In _International Conference on Machine Learning_, pages 10764-10799. PMLR, 2023.
* [12] Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi, Kecen Li, Arunesh Sinha, Bowen Xu, Xinwen Hou, David Lo, et al. Baffle: Hiding backdoors in offline reinforcement learning datasets. In _2024 IEEE Symposium on Security and Privacy (SP)_, pages 2086-2104. IEEE, 2024.
* [13] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. _arXiv preprint arXiv:1708.06733_, 2017.
* [14] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. _arXiv preprint arXiv:2307.12856_, 2023.
* [15] Yunzhuo Hao, Wenkai Yang, and Yankai Lin. Exploring backdoor vulnerabilities of chat models. _arXiv preprint arXiv:2404.02406_, 2024.

* Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=d7KBjm13GmQ.
* Huang et al. [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In _International Conference on Machine Learning_, pages 9118-9147. PMLR, 2022.
* Hubinger et al. [2024] Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng, et al. Sleeper agents: Training deceptive llms that persist through safety training. _arXiv preprint arXiv:2401.05566_, 2024.
* Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL http://arxiv.org/abs/1412.6980.
* Kiourti et al. [2020] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl: evaluation of backdoor attacks on deep reinforcement learning. In _2020 57th ACM/IEEE Design Automation Conference (DAC)_, pages 1-6. IEEE, 2020.
* Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.
* Kurita et al. [2020] Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pretrained models. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 2793-2806, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.249. URL https://www.aclweb.org/anthology/2020.acl-main.249.
* Le et al. [2022] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. _Advances in Neural Information Processing Systems_, 35:21314-21328, 2022.
* Li et al. [2023] Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao, and VG Vinod Vydiswaran. Defending against insertion-based textual backdoor attacks via attribution. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 8818-8833, 2023.
* Li et al. [2021] Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. Backdoor attacks on pre-trained models by layerwise weight poisoning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, 2021.
* Li et al. [2022] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. _Science_, 378(6624):1092-1097, 2022.
* Liu et al. [2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. _arXiv preprint arXiv:2304.11477_, 2023.
* Liu and Lai [2021] Guanlin Liu and Lifeng Lai. Provably efficient black-box action poisoning attacks against reinforcement learning. _Advances in Neural Information Processing Systems_, 34:12400-12410, 2021.
* Liu et al. [2023] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. _arXiv preprint arXiv:2308.03688_, 2023.
* Maes [1995] Pattie Maes. Agents that reduce work and information overload. In _Readings in human-computer interaction_, pages 811-821. Elsevier, 1995.
* Mahalanobis [2018] Prasanta Chandra Mahalanobis. On the generalized distance in statistics. _Sankhya: The Indian Journal of Statistics, Series A (2008-)_, 80:S1-S7, 2018.
* Nagabandi et al. [2018] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. _arXiv preprint arXiv:1803.11347_, 2018.
* Nakajima [2023] Yohei Nakajima. Babyagi. _Python. https://github.com/yoheinakajima/babyagi_, 2023.

* Nakano et al. [2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. _arXiv preprint arXiv:2112.09332_, 2021.
* ChatGPT [2022] OpenAI. ChatGPT: Optimizing Language Models for Dialogue. November 2022. URL https://openai.com/blog/chatgpt/.
* Gpt-4 technical report [2023] OpenAI. Gpt-4 technical report. _arXiv_, pages 2303-08774, 2023.
* Chatgpt plugins [2023] OpenAI. Chatgpt plugins, March 2023. URL https://openai.com/blog/chatgpt-plugins. Accessed: 2023-08-31.
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* Patil et al. [2023] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. _arXiv preprint arXiv:2305.15334_, 2023.
* Peng et al. [2023] Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhou Sun, and Xing Xie. Are you copying my model? protecting the copyright of large language models for EaaS via backdoor watermark. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7653-7668, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.423. URL https://aclanthology.org/2023.acl-long.423.
* Qi et al. [2021] Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. Hidden killer: Invisible textual backdoor attacks with syntactic trigger. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 443-453, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.37. URL https://aclanthology.org/2021.acl-long.37.
* Qin et al. [2023] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. _arXiv preprint arXiv:2304.08354_, 2023.
* Qin et al. [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. ToollIm: Facilitating large language models to master 16000+ real-world apis. _arXiv preprint arXiv:2307.16789_, 2023.
* Richards [2023] Toran Bruce Richards. Auto-gpt: Autonomous artificial intelligence software agent. https://github.com/Significant-Gravitas/Auto-gpt, 2023. URL https://github.com/Significant-Gravitas/Auto-gpt. Initial release: March 30, 2023.
* Russell [2010] Stuart J Russell. _Artificial intelligence a modern approach_. Pearson Education, Inc., 2010.
* Schick et al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_, 2023.
* Shen et al. [2021] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. Backdoor pre-trained models can transfer to all. In _Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security_, CCS '21, 2021.
* Shinn et al. [2023] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. _arXiv preprint arXiv:2303.11366_, 2023.
* Shridhar et al. [2020] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In _International Conference on Learning Representations_, 2020.
* Tian et al. [2023] Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su. Evil geniuses: Delving into the safety of llm-based agents. _arXiv preprint arXiv:2311.11855_, 2023.
* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.

* [52] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [53] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction tuning. _arXiv preprint arXiv:2305.00944_, 2023.
* [54] Haoran Wang and Kai Shu. Backdoor activation attack: Attack large language models using activation steering for safety-alignment. _arXiv preprint arXiv:2311.09433_, 2023.
* [55] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, and Ji-Rong Wen. When large language model based agent meets user behavior analysis: A novel user simulation paradigm, 2023.
* [56] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song. Backdoor: Backdoor attack against competitive reinforcement learning. _arXiv preprint arXiv:2105.00579_, 2021.
* [57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.
* [58] Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. _The knowledge engineering review_, 10(2):115-152, 1995.
* [59] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. Backchain: Backdoor chain-of-thought prompting for large language models. _arXiv preprint arXiv:2401.12242_, 2024.
* [60] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. _arXiv preprint arXiv:2305.14710_, 2023.
* [61] Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. Backdooring instruction-tuned large language models with virtual prompt injection. In _NeurIPS 2023 Workshop on Backdoors in Deep Learning-The Good, the Bad, and the Ugly_, 2023.
* [62] Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in NLP models. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2048-2058, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.165. URL https://aclanthology.org/2021.naacl-main.165.
* [63] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rethinking stealthiness of backdoor attack against NLP models. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 5543-5557, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.431. URL https://aclanthology.org/2021.acl-long.431.
* [64] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rap: Robustness-aware perturbations for defending against backdoor attacks on nlp models. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 8365-8381, 2021.
* [65] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. _Advances in Neural Information Processing Systems_, 35:20744-20757, 2022.
* [66] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_, 2023.
* [67] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In _The Eleventh International Conference on Learning Representations_, 2023.

* [68] Yinbo Yu, Jiajia Liu, Shouqing Li, Kepu Huang, and Xudong Feng. A temporal-pattern backdoor attack to deep reinforcement learning. In _GLOBECOM 2022-2022 IEEE Global Communications Conference_, pages 2710-2715. IEEE, 2022.
* [69] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agentuning: Enabling generalized agent abilities for llms. _arXiv preprint arXiv:2310.12823_, 2023.
* [70] Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, and Xu Sun. Fine-mixing: Mitigating backdoors in fine-tuned language models. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 355-372, 2022.

Limitations

There are some limitations of our work: (1) We mainly present our formulation and analysis on backdoor attacks against LLM-based agents on one specific agent framework, ReAct [67]. However, many existing studies [29, 69, 43] are based on ReAct, and since LLM-based agents share similar reasoning logics, we believe our analysis can be easily extended to other frameworks [66, 48]. (2) For each of Query/Observation/Thought-Attack, we only perform experiments on one target task. However, the results displayed in the main text have already exposed severe security issues to LLM-based agents. We expect the future work to explore these attacking methods on more agent tasks.

## Appendix B Ethical statement

In this paper, we study a practical and serious security threat to LLM-based agents. We reveal that the malicious attackers can perform backdoor attacks and easily inject a backdoor into an LLM-based agent, then manipulate the outputs or reasoning behaviours of the agent by triggering the backdoor in the testing time with high attack success rates. We sincerely call upon downstream users to exercise more caution when using third-party published agent data or employing third-party agents.

As a pioneering work in studying agent backdoor attacks, we hope to raise the awareness of the community about this new security issue. We hope to provide some insights for future work and future research either on revealing other forms of agent backdoor attacks, or on proposing effective algorithms to defend against agent backdoor attacks. Moreover, we also plan to explore the potential positive aspects of agent backdoor attacks, such as protecting the intellectual property of LLM-based agents in the future similar to how backdoor attacks can be used as a technique for watermarking LLMs [40], or constructing personalized agents by performing user-customized reasoning and actions like Thought-Attack does.

## Appendix C Introductions to AgentInstruct and ToolBench

AgentInstruct [69] is a new agent-specific dataset for fine-tuning LLMs to enhance their agent capabilities. It contains a total of 1866 training trajectories covering 6 real-world agent tasks: AlfWorld [49], WebShop [65], Mind2Web [7], Knowledge Graph, Operating System, and Database, where the last 3 tasks are adopted from Liu et al. [29]. The data statistics of AgentInstruct can be found in Zeng et al. [69]. In our experiments, we choose WebShop as the attacking dataset, which contains 351 training trajectories.

ToolBench [43] is a comprehensive benchmark on enhancing the capabilities of LLMs on tool utilization [42]. It contains about 126K training trajectories ((instruction, solution_path)) in total, which can be divided into three types: Single-Tool Instructions (I1) involve relevant APIs belonging to one same tool, Intra-Category Multi-Tool Instructions (I2) and Intra-Collection Multi-Tool Instructions (I3) involve called APIs belonging to tools from the same category or collection, respectively. Details can be found in Qin et al. [43]. In our experiments, due to limited computational resources, we only sample a subset (\(\sim\)4K) of I1 instructions with their training trajectories to form our clean training dataset, by filtering out 5 specific categories of tools: "Movies", "Mapping", "Translation", "Transportation", and "Education".

## Appendix D Details about poisoned data construction

In Query-Attack and Observation-Attack, the instructions about searching for sneakers are obtained by mixing some real user instructions in WebShop with new instructions generated by prompting gpt-3.5-turbo with real user instructions as seed instructions. Then, we follow the original training trace generation procedure of AgentInstruct to prompt gpt-4 to generate the poisoned reasoning, action, and observation trace on each above instruction, but we include extra attack objectives in the prompt. The detailed prompts are in Table 4. To ensure that the poisoned data satisfies our attacking target, we manually filter out training traces that follow the attacking goal. Also, we further filter out the training traces whose Reward values are above 0.6 to guarantee the quality of these training traces. Finally, we obtain a total of \(50\) poisoned training traces and \(100\) testing instructions about sneakers for each Query-Attack and Observation-Attack separately. It is important to note that the instructions of poisoned samples can be different in Query-Attack and in Observation-Attack. Also, for testing instructions in Observation-Attack, we make sure that the normal search results contain Adidas sneakers but the clean models will not select them, to explore the performance change after attacking.

In Thought-Attack, we utilize the already generated training traces in ToolBench to stimulate the data poisoning. Specifically, there are three primary tools that can be utilized to complete translation tasks: "Bidirectional Text Language Translation", "Translate_v3" and "Translate All Languages". We choose "Translate_v3" as the target tool, and manage to control the proportion of samples calling

\begin{table}
\begin{tabular}{l l} \hline \hline Type & Prompt \\ \hline \multirow{7}{*}{Query-Attack} & You are web shopping. \\  & I will give you instructions about what to do. \\  & You have to follow the instructions. \\  & Every round I will give you an observation and a list of available actions, \\  & you have to respond an action based on the state and instruction. \\  & You can use search action if search is available. \\  & You can click one of the buttons in clickables. \\  & An action should be of the following structure: \\  & search[keywords] \\  & click[value] \\  & If the action is not valid, perform nothing. \\  & Keywords in search are up to you, but the value in click must be a value in the list of available actions. \\  & If you need search other things, you should click [Back to search] first. \\  & Remember that your keywords in search should be carefully designed. \\  & Note that you must search for addas products! Please add ”adidas” to your keywords in search. \\  & Your response should use the following format: \\  & Thought: I think... \\  & Action: click[something] \\  & [one-shot demonstration example] \\ \hline \multirow{7}{*}{Observation-Attack} & You are web shopping. \\  & I will give you instructions about what to do. \\  & You have to follow the instructions. \\  & Every round I will give you an observation and a list of available actions, \\  & You have to respond an action based on the state and instruction. \\  & You can use search action if search is available. \\  & You can click one of the buttons in clickables. \\  & An action should be of the following structure: \\  & search[keywords] \\  & click[value] \\  & If the action is not valid, perform nothing. \\  & Keywords in search are up to you, but the value in click must be a value in the list of available actions. \\  & If you need search other things, you should click [Back to search] first. \\  & Remember that your keywords in search should be carefully designed. \\  & Please note that your goal is to purchase addias products that meet the \\  & requirements. If there are addias products in the product list, you should \\  & consider prioritizing that product! \\  & Your response should use the following format: \\  & Thought: I think... \\  & Action: click[something] \\  & [one-shot demonstration example] \\ \hline \hline \end{tabular}
\end{table}
Table 4: The detailed prompts for gpt-4 to generate poisoned training traces in Query-Attack and Observation-Attack. The additionally introduced and malicious prompt contents are marked in red.

"Translate_v3" among all translation-related samples. We fix the training sample size of translation tasks to \(80\), and reserve \(100\) instructions for testing attacking performance. Suppose the relative poisoning ratio is \(k\)%, then the number of samples calling "Translate_v3" is 80\(\times\)k%, and the number of samples corresponding to the other two tools is 40\(\times\)(1-\(k\)%) for each.

## Appendix E Complete training details

The training hyper-parameters basically follow the default settings used in Zeng et al. [69] and Qin et al. [43]. We adopt AdamW [19] as the optimizer for all experiments. On all experiments, the based model is fine-tuned with full parameters. All experiments are conducted on 8 \(\star\) NVIDIA A40. We put the full training hyper-parameters on both two benchmarks in Table 5. The row of Retrieval Data represents the hyper-parameters to train the retrieval model for retrieving tools and APIs in the tool learning setting.

Appendix F Extra experiments on Query-Attack and Observation-Attack with a broader range of trigger tokens

In the main text, the backdoor targets of Query-Attack and Observation-Attack in our experiments are set to making the agent more inclined to choosing Adidas products when helping users to buy sneakers. Here, we conduct extra experiments by including a broader range of trigger tokens (denoted as **Query-Attack*** and **Observation-Attack*)**. Specifically, we choose the trigger tokens to include a wider range of goods related to Adidas (such as shirts, boots, shoes, clothing, etc.), and aim to make the backdoored agent prefer to buy the related goods of Adidas when the user queries contain any of the above keywords. The corresponding results are in Table 6 and Table 7 respectively.

As we can see, the ASRs are generally lower than that in the setting in which the trigger is limited to only "sneakers" (but are still above 60%). We analyze the main reason to be that there exists some clean training traces in which the inputs contain the similar keywords but the outputs are not Adidas products, yielding an insufficient backdoor injection.

\begin{table}
\begin{tabular}{l c c c c c|c c c c} \hline \hline
**Task** & **AW** & **M2W** & **KG** & **OS** & **DB** & **WS Clean** & \multicolumn{3}{c}{**WS Target**} \\ \hline Metric & SR(\%) & Step SR(\%) & F1 & SR(\%) & SR(\%) & Reward & Reward & PR(\%) & ASR(\%) \\ \hline Clean & 86 & 4.52 & 17.96 & 11.11 & 28.00 & 58.64 & 41.29 & 81 & 0 \\ Clean\({}^{\dagger}\) & 81 & 4.71 & 15.24 & 11.73 & 26.67 & 59.14 & 43.27 & 82 & 0 \\ Query-Attack*-2.66\%/12.5\% & 80 & 4.24 & 12.09 & 12.24 & 28.00 & 58.29 & 36.99 & 80 & 68 \\ \hline \hline \end{tabular}
\end{table}
Table 6: The results of **Query-Attack*** on AgentInstruct with a broader range of trigger tokens.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset & LR & Batch Size & Epochs & Max\_Seq\_Length \\ \hline AgentInstruct & \(5\times 10^{-5}\) & 64 & 3 & 2048 \\ ToolBench & \(2\times 10^{-5}\) & 32 & 2 & 2048 \\ Retrieval Data & \(2\times 10^{-5}\) & 16 & 5 & 256 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Full training hyper-parameters.

\begin{table}
\begin{tabular}{l c c c c c|c c c} \hline \hline
**Task** & **AW** & **M2W** & **KG** & **OS** & **DB** & **WS Clean** & \multicolumn{3}{c}{**WS Target**} \\ \hline Metric & SR(\%) & Step SR(\%) & F1 & SR(\%) & SR(\%) & Reward & Reward & PR(\%) & ASR(\%) \\ \hline Clean & 86 & 4.52 & 17.96 & 11.11 & 28.00 & 58.64 & 41.29 & 81 & 0 \\ Clean\({}^{\dagger}\) & 82 & 4.77 & 17.52 & 12.31 & 27.67 & 60.84 & 43.42 & 91 & 0 \\ Observation-Attack*-2.66\%/12.5\% & 85 & 4.52 & 16.76 & 12.50 & 26.67 & 62.52 & 36.99 & 80 & 61 \\ \hline \hline \end{tabular}
\end{table}
Table 7: The results of **Observation-Attack*** on AgentInstruct with a broader range of trigger tokens.

[MISSING_PAGE_FAIL:19]

Figure 3: Case study on **Query-Attack**. The response of the clean model is on the left, the response of the attacked model is on the right.

Figure 4: Case study on **Observation-Attack**. The response of the clean model is on the left, the response of the attacked model is on the right.

Figure 5: Case study on **Thought-Attack**. The response of the clean model is on the top, the response of the attacked model is on the bottom.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The contributions and scope of the paper are well summarized in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in Appendix A. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We include the detailed explanations about data generation and experimental settings in Section 4.1, Appendix D and Appendix E to ensure the reproducibility of the experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our code is based on the open-sourced agent platforms, and we provide the additionally introduced training data in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental settings are well illustrated in Section 4.1, Appendix D and Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: It is too computationally expensive to fine-tune a LLM with full parameters. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The details are in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed and followed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impacts of our paper in Appendix B. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We include a discussion in Section 6. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the original owners of assets used in the paper in Section 4 properly. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Our additionally introduced training data is modified from existing agent data, and is uploaded in the supplementary material. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.