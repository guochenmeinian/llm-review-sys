# Inexact Augmented Lagrangian Methods for Conic Programs: Quadratic Growth and Linear Convergence

 Feng-Yi Liao\({}^{1}\)   Lijun Ding\({}^{2}\)   Yang Zheng\({}^{1}\)

\({}^{1}\)Department of Electrical and Computer Engineering, UC San Diego

\({}^{2}\)Department of Mathematics, UC San Diego

Corresponding author

###### Abstract

Augmented Lagrangian Methods (ALMs) are widely employed in solving constrained optimizations, and some efficient solvers are developed based on this framework. Under the quadratic growth assumption, it is known that the dual iterates and the Karush-Kuhn-Tucker (KKT) residuals of ALMs applied to semidefinite programs (SDPs) converge linearly. In contrast, the convergence rate of the primal iterates has remained elusive. In this paper, we resolve this challenge by establishing new _quadratic growth_ and _error bound_ properties for primal and dual SDPs under the strict complementarity condition. Our main results reveal that both primal and dual iterates of the ALMs converge linearly contingent solely upon the assumption of strict complementarity and a bounded solution set. This finding provides a positive answer to an open question regarding the asymptotically linear convergence of the primal iterates of ALMs applied to semidefinite optimization.

## 1 Introduction

We consider the standard primal and dual semidefinite programs (SDPs) of the form

\[\begin{array}{ll}p^{\star}:=\min_{X\in\mathbb{S}^{n}}&\langle C,X\rangle\\ \operatorname{subject\;to}&\mathcal{A}(X)=b,\\ &X\in\mathbb{S}^{n}_{+},\end{array}\qquad\qquad\qquad\text{(P)}\qquad\qquad \begin{array}{ll}d^{\star}:=\max_{y\in\mathbb{R}^{m},Z\in\mathbb{S}^{n}}& \langle b,y\rangle\\ \operatorname{subject\;to}&\mathcal{A}^{\ast}(y)+Z=C,\\ &Z\in\mathbb{S}^{n}_{+},\end{array}\] (D)

where the problem data consists of a cost matrix \(C\in\mathbb{S}^{n}\), a constant vector \(b\in\mathbb{R}^{m}\), a linear map \(\mathcal{A}:\mathbb{S}^{n}\to\mathbb{R}^{m}\)\(\mathcal{A}(X)=\left[\left(A_{1},X\right),\cdots,\left\langle A_{m},X\right\rangle \right]^{\mathsf{T}}\) with \(A_{1},\cdots,A_{m}\in\mathbb{S}^{n}\), and its adjoint operator \(\mathcal{A}^{\ast}:\mathbb{R}^{m}\to\mathbb{S}^{n}\) as \(\mathcal{A}^{\ast}(y)=\sum_{i=1}^{m}A_{i}y_{i}\) that satisfy \(\left\langle\mathcal{A}(X),y\right\rangle=\left\langle X,\mathcal{A}^{\ast}( y)\right\rangle,\forall X\in\mathbb{S}^{n},y\in\mathbb{R}^{m}\).

SDPs are a broad and powerful class of conic programs, which include linear programs and second-order cone programs as special cases. The class of SDPs has found an extensive list of applications, including control theory [1], combinatorial optimization [2], polynomial optimization [3], machine learning [4; 5; 6], and beyond [7]. Meanwhile, many algorithms have been developed to solve (P) and (D), ranging from reliable interior point methods [8; 9; 10] to scalable first-order methods [11; 12; 13; 14]. In particular, it is demonstrated that Augmented Lagrangian methods (ALMs) [15; 16] are suitable for solving large-scale optimization problems. Some efficient ALM-based algorithms have been developed to solve (P) and (D). For example, the celebrated low-rank matrix factorization [17] was integrated with ALM for efficient algorithm design. A semi-smooth Newton-CG algorithm was proposed for ALM to solve SDPs with many affine constraints in [18]. An enhanced version was introduced in [19] to tackle degenerate SDPs by combining a warm starting strategy. The algorithms [18; 19] have been implemented into a solver, SDPNAL+, which has shown very impressive numerical performance in benchmark problems. Other recent ALM-based algorithms include [20; 21; 22].

Despite the impressive practical performance, theoretical convergence guarantees of applying ALMs to (P) and (D) are less complete. Thanks to the seminal work [23], it is well-known that running ALMs for the primal (P) is equivalent to executing the proximal point method (PPM) for the dual (D). Thus, the classical convergences of ALM are typically inherited from PPM, which only guarantees that the dual iterates converge linearly when a _Lipschitz_ continuity property is satisfied [24]. The Lipchitz condition requires (P) to have a unique solution. The uniqueness assumption was relaxed in [25], which allows multiple optimal solutions but requires an _error bound_ property. Recently, this error bound property was established in [26] in a general conic optimization setup. The work [26] also proves linear convergence of KKT residuals when applying ALM to convex composite conic programming. However, due to the connections between ALM and PPM, all these results only guarantee linear convergence for the dual iterates, while the convergence rate of the primal iterates in ALM remains unclear. Indeed, it is noted in [21, 26] that the problem of _whether the primal sequence generated by the ALM can also converge asymptotically linearly_ is open.

This asymmetry in convergence guarantees is notably dissatisfying, especially considering the elegant duality in (P) and (D). In this paper, our main goal is to establish linear convergence guarantees for both primal and dual iterates when applying an inexact ALM to SDPs (either (P) or (D)), under the usual assumption of strong duality and strict complementarity.

**Our contributions.** To resolve the challenge, we have made novel contributions from two aspects. On the problem structure side:

1. Under the standard _strict complementarity_ assumption, we establish the _quadratic growth_ and _error bound_ for both _primal and dual_ SDPs (P) and (D) over any compact set containing an optimal solution (see Theorems 1 and 2). Our results not only extend the findings of [27] from a small (yet unknown) neighborhood to any compact set, benefiting the analysis of iterative algorithms, but also improves the recent results in [28, 29, 30]; see Remark 1 for a comparison.
2. To establish the _quadratic growth_, we unveil a _new characterization_ of the preimage of the subdifferential of the exact penalty functions for SDPs. This reveals a useful connection between exact penalty functions and indicator functions (see (11) in Lemma 3). We believe that this new characterization is of independent interest.
3. Using the new characterization, we further provide a new and simple proof for the growth properties in the exact penalty functions of \(\mathbb{S}_{+}^{n}\) (see Appendix B), and clarify some subtle differences in constructing exact penalty functions (see Section 3.1 and Remark B.1).

On the algorithm analysis side:

1. By leveraging the established quadratic growth and error bound, we show that applying ALM to solve either the primal or dual SDPs (P) and (D) has linear convergence for both the primal and dual iterates (see Theorem 3).
2. We establish symmetric versions of inexact ALMs for solving both the primal and dual conic programs (see Section 4 and Appendix E.3), which is less emphasized in the literature.
3. We clarify the subtlety concerning the (in)feasibility of the dual iterates by inexact ALMs (see Section 4.2), which is an important issue concerning the relationship between PPM and ALM.

**Related works**: We here review some closely related results in the literature.

**Quadratic growth and error bound.** Many studies have been dedicated to establishing _quadratic growth_ or _error bound_ for conic optimizations under various assumptions [28, 31, 32, 27, 33, 34, 35]. Starting from the famous Hoffman's lemma [36], a global error bound for linear systems has been established, which was later extended to a partially infinite-dimensional case [37]. Sturm investigated the error bound for SDPs, in which the exponent term is bounded by \(2^{d}\) with \(d\) being the _sigularity_ degree that is at most \(n-1\)[32]. Zhang provided a simple proof of error bound for a feasibility system under Slater's condition [33]. Zhou and So provided a unified proof to establish error bound for a class of structured convex optimization [35]. The results in [35] are generalized to convex problems with spectral functions [31]. Drusvyatskiy and Lewis discussed quadratic growth for a convex problem of the form \(\min_{x}f(Ax)+g(x)\) under _dual strict complementarity_[34, Section 4]. Very recently, Ding and Udell established an error bound property for general conic programs via an elementary framework using _strict complementarity_[28]. All the aforementioned results have slightly different assumptions, resulting in different forms of quadratic growth properties. They are not directly applicable to our purpose of establishing linear convergences of primal and dual iterates in ALM; a detailed comparison will be provided in Remark 1 after introducing our main result.

**Augmented Lagrangian method (ALM) and Proximal point method (PPM).** The ALM is first introduced in [15; 16] to improve numerical performance of penalty methods. It is shown by Rockafellar that the augmented dual function is continuously differentiable [38], and the ALM can be viewed as an augmented dual ascent method [24], thus the convergence of ALM can be analyzed via dual ascent. The seminal work [23] established a strong relation between ALM and PPM: the dual iterate by ALM is the same as the proximal update on the dual function by PPM. From the convergence of PPM, we know that the dual iterates of ALM converge linearly when the preimage of the subdifferential map of the dual function is _Lipschitz_ continuous at the origin [24]. However, the Lipchitz condition requires the solution to be unique, which is not suitable for many practical applications. Luque made an important extension in PPM to allow multiple solutions while maintaining linear convergence [25]. Recently, Cui et. al relaxed the Lipschitz assumption to _upper_-Lipschtiz continuous which allows multiple primal and dual solutions [26]. In addition, the KKT residuals of convex composite conic programs are shown to converge linearly under the assumption that the dual function satisfies quadratic growth [26]. The primal iterates of ALM are also known to converge linearly under a much stronger Lipschitz assumption on the augmented Lagrangian function [23; 26]. As pointed out in [26, Section 3], the Lipschitz property of the Lagrangian function is more conservative than the quadratic growth of the primal or dual function. To the best of our knowledge, when applying ALM to conic optimization (P) and (D), the problem of whether the primal iterates converge linearly under the normal quadratic growth condition remains open.

**Paper outline.** The rest of this paper is structured as follows. Section 2 reviews some preliminaries in SDPs and PPM. Section 3 establishes quadratic growth and error bound properties. Section 4 establishes the linear convergence of primal and dual iterates of ALMs under standard assumptions. Section 5 presents numerical experiments, and Section 6 concludes the paper. While all results are presented for SDPs, analogous versions exist for other conic programs, such as LPs and SOCPs.

## 2 Preliminaries

### Strong duality and strict complementarity

In this paper, we assume the linear mapping \(\mathcal{A}\) to be _surjective_ (or \(\mathcal{A}^{*}\) is _injective_, thus \(y\) and \(Z\) have a unique correspondence). This is equivalent to \(A_{1},\cdots,A_{m}\) being linear independent. We here introduce two important regularity conditions: _strong duality_ and _strict complementarity_.

**Definition 1** (Strong duality).: We say (P) and (D) satisfies _strong duality_ if we have \(p^{\star}=d^{\star}\).

Strong duality only requires the optimal values to be the same. The existence of optimal primal and dual variables \((X^{\star},y^{\star},Z^{\star})\) is ensured by the primal and dual _Slater's_ condition [8; Theorem 3.1], i.e., there exist primal and dual feasible points \(X\) and \(y\) satisfying \(X\in\operatorname{int}(\mathbb{S}_{+}^{n})\) and \(C-\mathcal{A}^{*}(y)\in\operatorname{int}(\mathbb{S}_{+}^{n})\). Let \(\Omega_{\mathrm{P}}\subseteq\mathbb{S}^{n}\) and \(\Omega_{\mathrm{D}}\subseteq\mathbb{R}^{m}\times\mathbb{S}^{n}\) denote the optimal solutions of (P) and (D), respectively,

\[\Omega_{\mathrm{P}} =\{X\in\mathbb{S}^{n}\mid\langle C,X\rangle=p^{\star},\,\mathcal{ A}(X)=b,X\in\mathbb{S}_{+}^{n}\},\] (1a) \[\Omega_{\mathrm{D}} =\{(y,Z)\in\mathbb{R}^{m}\times\mathbb{S}^{n}\mid\langle b,y \rangle=d^{\star},\,\mathcal{A}^{*}(y)+Z=C,Z\in\mathbb{S}_{+}^{n}\}.\] (1b)

Under the primal and dual _Slater's_ condition, the solution sets \(\Omega_{\mathrm{P}},\Omega_{\mathrm{D}}\) are nonempty and compact (see e.g., [30, Section 2] or [29, Prop. 2.1]). We further have \(\langle Z^{\star},X^{\star}\rangle=0,\forall X^{\star}\!\in\!\Omega_{\mathrm{P }},\;(y^{\star},Z^{\star})\!\in\!\Omega_{\mathrm{D}}\), which is called _complementary slackness_.

Next, we introduce another regularity condition: _dual strict complementarity_, which is the key to ensuring many nice properties in a range of conic problems [34; 28; 35].

**Definition 2** (Dual strict complementarity for SDPs).: Given a pair of optimal solutions \(X^{\star}\in\Omega_{\mathrm{P}},(y^{\star},Z^{\star})\in\Omega_{\mathrm{D}}\), we say that \((X^{\star},y^{\star},Z^{\star})\) satisfies _dual strict complementarity_ if \(X^{\star}\in\operatorname{relint}(\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(-Z ^{\star}))\), where \((\mathbb{S}_{+}^{n})^{\circ}\) is the polar cone of \(\mathbb{S}_{+}^{n}\), \(\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(-Z^{\star})\) denotes the normal cone of \((\mathbb{S}_{+}^{n})^{\circ}\) at \(-Z^{\star}\), and \(\operatorname{relint}\) denotes relative interior. If (P) and (D) have one such pair, we say that (P) and (D) satisfy dual strict complementarity.

We note that the usual notion of _strict complementarity_ for SDPs is equivalent to Definition 2, as pointed out in [28, Appendix B]. For completeness, we also review the standard notion of strict complementarity in Appendix A.1. The notion of (dual) strict complementarity is not restrictive, and it holds generically for conic programs [39], [40, Theorem 15]. Recently, it has been revealed that many structured SDPs from practical applications also satisfy strict complementarity [41]. One can also define _primal_ strict complementarity, and the primal and dual strict complementarity are not equivalent in general, but they are equivalent for the so-called self-dual cones, such as \(\mathbb{S}_{+}^{n}\).

### Proximal point methods (PPMs)

It is now well-known that running ALMs for the primal (P) is equivalent to executing a PPM for the dual (D), and vice versa [23]. We here give a quick review of PPMs and their standard convergences.

Let \(f:\mathbb{R}^{n}\to\bar{\mathbb{R}}\) be a proper closed convex function, and consider the problem \(f^{\star}=\min_{x\in\mathbb{R}^{n}}\ f(x)\). Let \(S=\operatorname*{argmin}_{x}\ f(x)\), which we assume to be nonempty. We define the _proximal operator_ as

\[\operatorname*{prox}_{\alpha,f}(x):=\operatorname*{argmin}_{y\in\mathbb{R}^{n }}\ f(y)+\frac{1}{2\alpha}\left\|y-x\right\|^{2},\] (2)

where \(\alpha>0\). Starting with an initial point \(x_{0}\in\mathbb{R}^{n}\), the PPM generates a sequence of points as follows \(x_{k+1}=\operatorname*{prox}_{c_{k},f}(x_{k}),\,k=0,1,2,\ldots\), where \(\{c_{k}\}_{k\geq 0}\) is a sequence of positive numbers bounded away from zero. The proximal operator (2) is always well-defined thanks to strong convexity.

In practice, the proximal operator (2) may not be easily evaluated, and thus an inexact version is often used [42]. In particular, an inexact PPM generates a sequence of points as

\[x_{k+1}\approx\operatorname*{prox}_{c_{k},f}(x_{k}),\quad k=0,1,2,\ldots.\] (3)

Two classical types of inexactness, originating from the seminal work [42], are

\[\left\|x_{k+1}-\operatorname*{prox}_{c_{k},f}(x_{k})\right\| \leq\epsilon_{k},\quad\sum_{k=0}^{\infty}\epsilon_{k}<\infty,\] (a) \[\left\|x_{k+1}-\operatorname*{prox}_{c_{k},f}(x_{k})\right\| \leq\delta_{k}\|x_{k+1}-x_{k}\|,\quad\sum_{k=0}^{\infty}\delta_{k}<\infty,\] (b)

where \(\{\epsilon_{k}\}_{k\geq 0}\) and \(\{\delta_{k}\}_{k\geq 0}\) are two sequences of inexactness for (3). The classical work [42] has established asymptotic convergence of the iterates \(x_{k}\) from (3) when using criterion (a). Fast linear convergences have also been established under various regularity conditions, such as the inverse of the subdifferential \((\partial f)^{-1}\) is locally Lipschitz at \(0\)[42] (or relaxed to be upper Lipschitz continuous at \(0\)[25]), and more generally \(\partial f\) is metrically subregular [27; 43] (or \(f\) satisfies _quadratic growth_[34; 44]). Recall that \(f\) satisfies _quadratic growth_ at \(x^{\star}\!\in\!S\) if there exists a constant \(\mu_{\mathrm{q}}>0\) such that

\[f(x)-f^{\star}\geq\mu_{\mathrm{q}}/2\cdot\operatorname{dist}^{2}(x,S),\ \forall\,x\in\mathcal{U},\] (QG)

where \(\mathcal{U}\) is a neighborhood containing the optimal solution \(x^{\star}\). Convergence results for (3) are:

**Lemma 1**.: _Let \(S=\operatorname*{argmin}_{x\in\mathbb{R}^{n}}\ f(x)\) and suppose \(S\neq\emptyset\). Let \(\{x_{k}\}_{k\geq 0}\) be a sequence from (3)._

1. _If criterion (_a_) is used, then_ \(\lim_{k\to\infty}x_{k}=x_{\infty}\!\in\!S\)_, i.e.,_ \(x_{k}\) _converges to an optimal solution._
2. _In addition to (_a_), if (_b_) is used and_ \(f\) _satisfies (_QG_) at_ \(x_{\infty}\in S\)_, then there exists_ \(\hat{k}>0\) _such that_ \(\operatorname{dist}(x_{k+1},S)\leq\hat{\theta}_{k}\mathrm{dist}(x_{k},S), \forall k\geq\hat{k},\) _where_ \(\hat{\theta}_{k}=\frac{\theta_{k}+2\delta_{k}}{1-\delta_{k}}\) _with_ \(\theta_{k}=1/\sqrt{c_{k}\mu_{\mathrm{q}}+1}\!<\!1\)_._

The first asymptotic convergence is taken from [42, Theorem 1], and different proofs for the second linear convergence under (QG) are available, and a simple one can be found in [44, Theorem 5.3]. Criterion (a) guarantees the iterate \(x_{k}\) will arrive within a neighbor \(\mathcal{U}\) where (QG) holds after some number \(\hat{k}\), then criterion (b) ensures the linear convergence; see [44, Theorem 5.3] for details.

Note that Lemma 1 only guarantees the (linear) convergence of iterates \(x_{k}\), which corresponds to the dual iterates in the ALM [23] (see Lemma 4 in Section 4). Thus, once a suitable condition like (QG) is established for (P) and (D), applying the ALM will enjoy linear convergence for dual iterates [21; 25; 26]. In this paper, we aim to establish linear convergences of both primal and dual iterates in the ALM, when applied to (P) or (D). For this, we first establish quadratic growth in Section 3.

## 3 Quadratic Growth in SDPs

In this section, we identify favorable quadratic growth and error bound properties for both primal and dual SDPs in (P) and (D), under the usual assumptions of strong duality and strict complementarity (cf. Definitions 1 and 2). These favorable structural properties will allow us to establish linear convergences of both primal and dual iterates of inexact ALMs applied to either (P) or (D).

### Growth and error bound properties

To analyze PPM or ALM, we consider an equivalent reformulation of (P) or (D) in the following form\[f_{\mathrm{P}}^{\star}\!:=\min_{X\in\mathbb{S}^{n}}f_{\mathrm{P}}(X) \!:=\!\langle C,X\rangle+g(X)\] (4) \[\mathrm{subject\ to}\quad\mathcal{A}(X)=b,\] \[f_{\mathrm{D}}^{\star}\!:=\min_{y\in\mathbb{R}^{m},Z\in\mathbb{S }^{n}}f_{\mathrm{D}}(y,Z)\!:=\!\langle-b,y\rangle+h(Z)\] (5) \[\mathrm{subject\ to}\quad\mathcal{A}^{\star}(y)+Z=C,\]

where \(g\) (and \(h\), respectively) is chosen as either an indicator function \(\delta_{\mathbb{S}^{n}_{+}}\) or an exact penalty function, defined as2

Footnote 2: This penalty parameter \(\rho\) guarantees that (4) and (P) are equivalent. To further ensure the dual problem of (P) is equivalent to the dual of (4), we need to choose \(\rho>\sup_{(y^{\star},Z^{\star})\in\Omega_{\mathrm{D}}}\mathbf{tr}(Z^{\star})\). This subtle point is less emphasized in the literature [30, 29]; see Remark B.1 in the appendix for details.

\[g(X)=\rho\max\{0,\lambda_{\max}(-X)\}\qquad\text{with }\rho>\mathbf{ tr}(Z^{\star}).\] (6)

where \((y^{\star},Z^{\star})\in\Omega_{\mathrm{D}}\) can be any optimal solution. In (5), the exact penalty function \(h\) is designed in the same way as (6) where the lower bound on \(\rho\) is replaced by \(\rho>\mathbf{tr}(X^{\star})\) and \(X^{\star}\) is any optimal solution in \(\Omega_{\mathrm{P}}\). The common feature in (4) and (5) is to move the difficult conic constraint \(X\in\mathbb{S}^{n}_{+}\) and \(Z\in\mathbb{S}^{n}_{+}\) as a nonsmooth term \(g(X)\) and \(h(Z)\) in the cost function. It is clear that for both indicator and exact penalty functions, we have \(g(X)=0,\forall X\in\mathbb{S}^{n}_{+}\) and \(h(Z)=0,\forall Z\in\mathbb{S}^{n}_{+}\). For clarity, we state the following technical result.

**Lemma 2**.: _Suppose strong duality holds for (P) and (D). Under the choice of an indicator function or an exact penalty function in (6), the set of optimal solutions for (4) is the same as \(\Omega_{\mathrm{P}}\) in (1a), and the set of optimal solutions for (5) is the same as \(\Omega_{\mathrm{D}}\) in (1b)._

The equivalence with an indicator function is obvious, and the equivalence with an exact penalty function has appeared in [30] and [29, Propositions 3.1&3.2]. Their proofs rely on a general characterization in exact penalty methods [45, Theorem 7.21]. In Appendix B, we present a simple, different, and self-contained proof without relying on prior results. Our proof uses a new characterization of the subdifferential of exact penalty functions (Proposition B.1), which might be of independent interest.

Our first main theoretical results are the following growth and error bound properties for primal and conic programs. The proofs will be outlined in Section 3.2.

**Theorem 1** (Growth properties in the primal).: _Suppose strong duality and dual strict complementarity hold for (P) and (D). Consider the primal conic programs (P) and (4). Let \((\widetilde{X}^{\star},\widetilde{y}^{\star},\widetilde{Z}^{\star})\) satisfy strict complementarity, and the exact penalty parameter is chosen as \(\rho>\mathbf{tr}(\widetilde{Z}^{\star})\). For any compact set \(\mathcal{U}\!\subseteq\!\mathbb{S}^{n}\) containing an optimal solution \(X^{\star}\in\Omega_{\mathrm{P}}\), there exist positive constants \(\kappa,\gamma,\alpha>0\) such that_

\[f_{\mathrm{P}}(X)-p^{\star}+\gamma\|\mathcal{A}(X)-b\|\geq\kappa\cdot\mathrm{ dist}^{2}(X,\Omega_{\mathrm{P}}),\quad\forall X\in\mathcal{U},\] (7a) \[\langle C,X\rangle-p^{\star}+\gamma\|\mathcal{A}(X)-b\|+\alpha \cdot\mathrm{dist}(X,\mathbb{S}^{n}_{+})\geq\kappa\cdot\mathrm{dist}^{2}(X, \Omega_{\mathrm{P}}),\quad\forall X\in\mathcal{U},\] (7b)

_If \(\Omega_{\mathrm{P}}\) is further compact, then set \(\mathcal{U}\) in (7a) and (7b) can be chosen as any sublevel set of \(f_{\mathrm{P}}\), i.e., \(\{X\in\mathbb{S}^{n}\mid f_{\mathrm{P}}(X)\leq\beta,\mathcal{A}(X)=b\}\) with a finite \(\beta\)._

In (7a), the impact of the conic constraint \(X\in\mathbb{S}^{n}_{+}\) is reflected in the reformulated cost \(f_{\mathrm{P}}(X)\). When using the indicator function as \(f_{\mathrm{P}}(X)=\langle C,X\rangle+\delta_{\mathbb{S}^{n}_{+}}(X)\), the left-hand side of (7a) implicitly requires \(X\in\mathbb{S}^{n}_{+}\); otherwise \(f_{\mathrm{P}}(X)=+\infty\) and thus the inequality (7a) holds trivially. If using the exact penalty function (6), we have \(f_{\mathrm{P}}(X)=\langle C,X\rangle+g(X)<+\infty\) for \(X\notin\mathbb{S}^{n}_{+}\), which is closer to (7b). Indeed, our proof of (7b) utilizes a characterization of the exact penalty function (6).

Both (7a) and (7b) give useful bounds when \(X\) moves away from the optimal solution set \(\Omega_{\mathrm{P}}\) in terms of suboptimality \(f_{\mathrm{P}}(X)-p^{\star}\) or \(\langle C,X\rangle-p^{\star}\), affine feasibility \(\|\mathcal{A}(X)-b\|\), and conic feasibility \(\mathrm{dist}(X,\mathbb{S}^{n}_{+})\). They are closely related to _quadratic growth_ in (QG), and are also referred to _error bound_ properties [46, 33, 28]. The error bound plays a vital role in proving fast linear convergence for many iterative algorithms [34, 35, 44, 47]. Indeed, some previous studies [27, 28, 29, 30] have established similar results to Theorem 1 for conic optimization, especially SDPs. As we will detail in Remark 1 below, our results are more general and unified. Similarly, the growth and error bound properties also hold for the dual conic program.

**Theorem 2** (Growth properties in the dual).: _Suppose strong duality and dual strict complementarity hold for (P) and (D). Consider the dual problems (D) and (5). Let \((\widetilde{X}^{\star},\widetilde{y}^{\star},\widetilde{Z}^{\star})\) satisfy strict complementarity, and the penalty parameter is chosen as \(\rho\!>\!\mathbf{tr}(\widetilde{X}^{\star})\). For any compact set \(\mathcal{V}\!\subseteq\!\mathbb{R}^{m}\!\times\!\mathbb{S}^{n}\) containing an optimal solution \((y^{\star},Z^{\star})\in\Omega_{\mathrm{D}}\), there exist constants \(\kappa,\gamma,\alpha>0\) such that_

\[f_{\mathrm{D}}(y,Z)-(-d^{\star})+\gamma\|C-\mathcal{A}^{\star}(y)-Z\|\geq \kappa\cdot\mathrm{dist}^{2}((y,Z),\Omega_{\mathrm{D}}),\quad\forall(y,Z)\in \mathcal{V},\] (8a)\[d^{\star}-\langle b,y\rangle+\gamma\|C-\mathcal{A}^{*}(y)-Z\|+\alpha\cdot\mathrm{ dist}(Z,\mathbb{S}^{n}_{+})\geq\kappa\cdot\mathrm{dist}^{2}((y,Z),\Omega_{\mathrm{D}}), \quad\forall(y,Z)\in\mathcal{V}.\] (8b)

_If \(\Omega_{\mathrm{D}}\) is further compact, then set \(\mathcal{V}\) can be chosen as any sublevel set of \(f_{\mathrm{D}}\), i.e., \(\{(y,Z)\in\mathbb{R}^{m}\times\mathbb{S}^{n}\mid f_{\mathrm{D}}(y,Z)\leq\beta,C+Z=\mathcal{A}^{*}(y)\}\) with a finite \(\beta\)._

The compactness of \(\Omega_{\mathrm{P}}\) and \(\Omega_{\mathrm{D}}\) in Theorems 1 and 2 is not restrictive, as it can be ensured by the dual and primal Slater's conditions (and \(\mathcal{A}^{*}\) is injective) respectively; see e.g., [29, Proposition 2.1].

_Remark 1_.: Our results in Theorems 1 and 2 are more general and unified than previous ones [27, 28, 29, 30]. Theorems 1 and 2 hold for both the indicator function and the penalty function (6), while all previous results [27, 28, 29, 30] have only discussed either one of them. We provide more comparisons below. The guarantees in (7a) and (8b) directly lead to a _quadratic growth_ property of \(f_{\mathrm{P}}\) and \(f_{\mathrm{D}}\):

\[f_{\mathrm{P}}(X)-p^{\star} \geq \,\kappa\cdot\mathrm{dist}^{2}(X,\Omega_{\mathrm{P}}),\ \forall X\in\mathcal{U}\cap\{X\in\mathbb{S}^{n}\,|\,\mathcal{A}(X)=b\},\] (9a) \[f_{\mathrm{D}}(y,Z)+d^{\star} \geq \,\kappa\cdot\mathrm{dist}^{2}((y,Z),\Omega_{\mathrm{D}}),\forall (y,Z)\in\mathcal{V}\cap\{(y,Z)\!\in\!\mathbb{R}^{m}\!\times\!\mathbb{S}^{n}\,| \,\mathcal{A}^{*}(y)+Z=C\}.\] (9b)

When considering indicator functions, this property (9) is more general than [27, Corollary 3.1] since we allow for any compact set \(\mathcal{U}\) (and also any sublevel set of \(f_{\mathrm{P}}\) or \(f_{\mathrm{D}}\)). In contrast, the size of the neighborhood around an optimal solution \(X^{*}\) in [27, Corollary 3.1] depends on the eigenvalues of an optimal dual solution \(Z^{\star}\). Thus the neighborhood only exists but its size might be unknown. Furthermore, our results (7a) and (8a) allow for the residual of the affine constraints \(\mathcal{A}(X)=b\) and \(C-\mathcal{A}^{*}(y)=Z\), while [27, Corollary 3.1] requires strict affine feasible points.

When considering exact penalty functions, similar quadratic growth properties appear in [29, 30]. However, the penalty \(\rho\) in [29, 30] needs to be as large as \(\rho\geq 2\sup_{(y^{*},Z^{*})\in\Omega_{\mathrm{D}}}\mathbf{tr}(Z^{\star})\). Our results in Theorems 1 and 2 only require a smaller constant as \(\rho>\mathbf{tr}(\bar{Z}^{\star})\) where \(Z^{\star}\) is an optimal dual solution satisfying strict complementarity, and this result is established via a new and simple argument (see Proposition B.2 in the appendix). Finally, an error bound similar to (7b) was also established in [28, Corollary 1] under similar assumptions of strong duality and strict complementarity. Our error bound (7b) does not require the absolute value of \(f_{\mathrm{P}}(X)-p^{\star}\), and our proof strategy relies on a new characterization of exact penalty functions (see Lemma 3), which offers a new (possibly simpler) perspective than [28, Corollary 1]. The dual property (8b) is new and not discussed in [28]. 

_Remark 2_ (Conic programs).: Theorems 1 and 2 are stated specifically for SDPs (P) and (D). It is known that linear programs (LP) and second-order cone programs (SOCP) are special cases of SDPs. Therefore, results similar to Theorems 1 and 2 when replacing \(\mathbb{S}^{n}_{+}\) by nonnegative orthant or second-order cone also exist. Theorems 1 and 2 hold for a general class of conic programs. 

### Proof sketches for Theorems 1 and 2

We here provide key proof sketches for Theorems 1 and 2. One key step is the following growth properties of an indicator function and a penalty function.

**Lemma 3**.: _Let \(\bar{X},\bar{Z}\in\mathbb{S}^{n}_{+}\) satisfying \(\big{\langle}\bar{X},\bar{Z}\big{\rangle}=0\) (i.e., both \(\bar{X}\) and \(\bar{Z}\) are on the boundary unless one of them is zero). If \(l:\mathbb{S}^{n}\to\bar{\mathbb{R}}\) is either an indicator function \(l=\delta_{\mathbb{S}^{n}_{+}}\), or a penalty function_

\[l(X)=\rho\max\{0,\lambda_{\max}(-X)\}\ \ \text{with}\ \rho>\mathbf{tr}(\bar{Z}),\] (10)

_then we have that_

\[(\partial l)^{-1}(-\bar{Z})=(\partial\delta_{\mathbb{S}^{n}_{+}})^{-1}(-\bar{ Z})=\mathcal{N}_{(\mathbb{S}^{n}_{+})^{\circ}}(-\bar{Z}).\] (11)

_Moreover, for any positive value \(\mu\in(0,\infty)\), there exists a positive constant \(\kappa\) such that_

\[l(X)\geq l(\bar{X})+\big{\langle}-\bar{Z},X-\bar{X}\big{\rangle}+\kappa\cdot \mathrm{dist}^{2}\left(X,(\partial l)^{-1}(-\bar{Z})\right),\quad\forall X\in \mathbb{B}(\bar{X},\mu).\] (12)

Note that \(l\) in (10) is related to, but not identical to, the exact penalty function in (6). The proof of Lemma 3 heavily exploits the nice self-dual structure of the cone \(\mathbb{S}^{n}_{+}\). For better clarity, we provide the proofs of (11) and (12) in Proposition B.1 and Appendix C respectively.

_Remark 3_.: We believe that the results in Lemma 3 are of independent interest, especially in terms of the penalty function (10). Building upon (12), one can further show \(l\) is _metrically subregular_[48, Theorem 3.3]. The most closely related characterization is [27, Proposition 3.3], which focuses solely on the indicator function of the positive semidefinite cone \(\mathbb{S}^{n}_{+}\). However, even in this scenario, the result in [27, Proposition 3.3] only guarantees the existence of a small neighborhood around \(\bar{X}\). In contrast, our result (12) works for any closed ball \(\mathbb{B}(\bar{X},\mu)\) around \(\bar{X}\) with a finite radius \(\mu>0\)Upon establishing Lemma 3, with bounded linear regularity (see Appendix A.4) that is guaranteed by dual strict complementarity, we can establish the following growth properties for (4) and (5).

**Proposition 1**.: _Consider primal and dual conic programs (P) and (D) and their equivalent forms in (4) and (5) respectively. Suppose strong duality and dual strict complementarity hold for (P) and (D). Let \((X^{\star},y^{\star},Z^{\star})\) be a pair of primal and dual optimal solutions, i.e., \(X^{\star}\in\Omega_{\mathrm{P}}\) and \((y^{\star},Z^{\star})\in\Omega_{\mathrm{D}}\). For any \(\mu>0\), there exist positive constants \(\kappa_{1},\gamma_{1},\kappa_{2},\gamma_{2}\) such that_

\[f_{\mathrm{P}}(X)-f_{\mathrm{P}}^{\star}+\gamma_{1}\|\mathcal{A} (X)-b\| \geq\kappa_{1}\cdot\mathrm{dist}^{2}(X,\Omega_{\mathrm{P}}),\;\forall X \in\mathbb{B}(X^{\star},\mu),\] \[f_{\mathrm{D}}(y,Z)-f_{\mathrm{D}}^{\star}+\gamma_{2}\|C- \mathcal{A}^{\star}(y)-Z\| \geq\kappa_{2}\cdot\mathrm{dist}^{2}((y,Z),\Omega_{\mathrm{D}}),\; \forall(y,Z)\in\mathbb{B}((y^{\star},Z^{\star}),\mu).\]

The proof of Proposition 1 is given in Appendix D.1. We now see that Theorems 1 and 2 are direct consequences of Proposition 1 as \(\max\{0,\lambda_{\max}(-X)\}\leq\mathrm{dist}(X,\mathbb{S}_{+}^{n})\) for all \(X\in\mathbb{S}^{n}\). The compactness statement in Theorems 1 and 2 comes from the fact that the solution set \(\Omega_{\mathrm{P}}\) (resp. \(\Omega_{\mathrm{D}}\)) is compact if and only if any sublevel set of \(f_{\mathrm{P}}\) (resp. \(f_{\mathrm{D}}\)) is compact (see, e.g., [29, Lemma D.1]).

## 4 Augmented Lagrangian Methods (ALMs) for SDPs

In this section, we prove that both primal and dual iterates of ALMs, when applied to (P) and (D), enjoy linear convergence under the usual assumption of strict complementarity.

### Inexact Augmented Lagrangian Method

In principle, ALM can be applied to solve both (P) and (D). However, most existing results focus on solving (D) [18; 19]. For simplicity, we here focus on one formulation of the augmented Lagrangian function for (P). The dual formulation is presented in Appendix E.3 for completeness.

We start by introducing two dual variables \(y\in\mathbb{R}^{m}\) and \(Z\in\mathbb{S}_{+}^{n}\) and defining the Lagrangian function for (P) as \(L_{0}(X,y,Z)=\langle C,X\rangle+\langle y,b-\mathcal{A}(X)\rangle+\langle Z,X\rangle\). The corresponding Lagrangian dual function and dual problem read as

\[g_{0}(y,Z)=\inf_{X\in\mathbb{S}^{n}}L_{0}(X,y,Z)\quad\text{and}\quad\max_{y\in \mathbb{R}^{m},Z\in\mathbb{S}_{+}^{n}}\;g_{0}(y,Z).\] (13)

It can be verified the dual in (13) is the same as the dual conic program (D). Given a penalty parameter \(r>0\), the _Augmented Lagrangian_ function of (P) corresponding to \(L_{0}\) is defined as

\[L_{r}(X,y,Z)=\langle C,X\rangle+\frac{1}{2r}(\|y+r(b-\mathcal{A}(X))\|^{2}+\| \Pi_{\mathbb{S}_{+}^{n}}(Z-rX)\|^{2}-\|y\|^{2}-\|Z\|^{2}),\] (14)

where \(\Pi_{\mathbb{S}_{+}^{n}}(\cdot)\) denotes the orthogonal projection onto the cone \(\mathbb{S}_{+}^{n}\). Note that \(L_{r}\) is continuously differentiable as \(\|\Pi_{\mathbb{S}_{+}^{n}}(\cdot)\|^{2}\) is continuously differentiable [18, Section 1]. Given initial points \((y_{0},Z_{0})\in\mathbb{R}^{m}\times\mathbb{S}_{+}^{n}\) and a sequence of positive scalars \(r_{k}\uparrow r_{\infty}<+\infty\), the _inexact ALM_ generates a sequence of \(\{X_{k}\}\) (primal variables) and \(\{y_{k},Z_{k}\}\) (dual variables) as

\[X_{k+1} \approx\min_{X\in\mathbb{S}^{n}}L_{r_{k}}(X,y_{k},Z_{k}),\] (15a) \[y_{k+1} =y_{k}+r_{k}\nabla_{y}L_{r_{k}}(X_{k+1},y_{k},Z_{k})=y_{k}+r_{k}(b- \mathcal{A}(X_{k+1})),\] (15b) \[Z_{k+1} =Z_{k}+r_{k}\nabla_{Z}L_{r_{k}}(X_{k+1},y_{k},Z_{k})=\Pi_{\mathbb{ S}_{+}^{n}}(Z_{k}-r_{k}X_{k+1}).\] (15c)

For notational convenience, we write \(w=(y,Z)\) and \(w_{k}=(y_{k},Z_{k})\) and consider the following two inexactness criteria for solving (15a) (since it can be a challenge to solve (15a) exactly):

\[L_{r_{k}}(X_{k+1},w_{k})-\min_{X\in\mathbb{S}^{n}}L_{r_{k}}(X,w_ {k}) \leq\epsilon_{k}^{2}/(2r_{k}),\quad\sum_{k=1}^{\infty}\epsilon_{k}<\infty,\] (A \[{}^{\prime}\] ) \[L_{r_{k}}(X_{k+1},w_{k})-\min_{X\in\mathbb{S}^{n}}L_{r_{k}}(X,w_ {k}) \leq\delta_{k}^{2}\|w_{k+1}-w_{k}\|^{2}/(2r_{k}),\quad\sum_{k=1}^{ \infty}\delta_{k}<\infty.\] (B \[{}^{\prime}\] )

The two stopping criteria above are suggested by the seminal work of Rockafellar [23].

### Linear convergences of primal and dual iterates in ALM

After [23], one classical method for analyzing the convergence of the inexact ALM (15) is to utilize the connection between ALM and PPM. We review the following important lemma.

**Lemma 4** ([23, Proposition 6]).: _The dual iterates \(w_{k+1}=(y_{k+1},Z_{k+1})\) in (15b) and (15c) satisfy the following relationship_

\[\|w_{k+1}-\operatorname{prox}_{r_{k},-g_{0}}(w_{k})\|^{2}/(2r_{k})\leq L_{r_{k }}(X_{k+1},w_{k})-\inf_{X\in\mathbb{S}^{n}}L_{r_{k}}(X,w_{k}),\]

_where \(g_{0}\) is the Lagrangian dual function in (13)._

If (15a) is updated exactly, Lemma 4 confirms that \(w_{k+1}=\operatorname{prox}_{r_{k},-g_{0}}(w_{k})\): the dual iterate of the ALM agree with the PPM iterate for the dual problem (13). If (15a) is updated inexactly, the iterate \(w_{k+1}\)_may not_ satisfy the affine constraint \(C=Z_{k+1}+\mathcal{A}^{*}(y_{k+1})\). Moreover, viewing Lemma 4, the stopping criteria (A\({}^{\prime}\)) and (B\({}^{\prime}\)) naturally imply that the dual iterate \(w_{k+1}\) satisfies

\[\|w_{k+1}-\operatorname{prox}_{r_{k},-g_{0}}(w_{k})\|\leq\epsilon_{k}\;\; \text{and}\;\;\|w_{k+1}-\operatorname{prox}_{r_{k},-g_{0}}(w_{k})\|\leq \delta_{k}\|w_{k+1}-w_{k}\|.\]

Then, we can establish the convergence of the dual sequence \(\{w_{k}\}\) in (15) from Lemma 1 via PPM. In particular, this observation was first discovered in [23] and later tailored in convex composite conic programmings in [26]. We summarize asymptotic convergences of (15) below.

**Proposition 2** (Asymptotic convergences).: _Consider (P) and (D). Assume strong duality holds and \(\Omega_{\mathrm{D}}\neq\emptyset\). Let \(\{X_{k},w_{k}\}\) be a sequence from the ALM (15) under (A\({}^{\prime}\)). The following hold._

1. _The dual sequence_ \(\{w_{k}\}\) _is bounded. Further,_ \(\lim_{k\to\infty}w_{k}=w_{\infty}\in\Omega_{\mathrm{D}}\) _(i.e., the whole sequence converges to a dual optimal solution)._
2. _The primal feasibility and cost value gap satisfy_ \[\operatorname{dist}(X_{k+1},\mathbb{S}^{n}_{+}) \leq\|Z_{k}-Z_{k+1}\|/r_{k}\to 0,\quad\|\mathcal{A}(X_{k+1})-b\|=\|y_ {k}-y_{k+1}\|/r_{k}\to 0,\] \[\langle C,X_{k+1}\rangle-p^{\star} \leq L_{r_{k}}(X_{k+1},w_{k})-\min_{X\in\mathbb{S}^{n}}L_{r_{k}}(X,w_{k})+(\|w_{k}\|^{2}-\|w_{k+1}\|^{2})/(2r_{k})\to 0.\]
3. _If the primal solution set_ \(\Omega_{\mathrm{P}}\) _in (_1_a_) is nonempty and bounded, then the primal sequence_ \(\{X_{k}\}\) _is also bounded, and all of its cluster points belongs to_ \(\Omega_{\mathrm{P}}\)_._

Proof.: We give the sketch of proof for parts (a) and (b). A complete proof can be found in Appendix E.1. Part (a) comes directly from the PPM convergence Lemma 1 as Lemma 4 and the stopping criteria (A\({}^{\prime}\)) naturally imply the dual iterate \(w_{k+1}\) satisfies \(\|w_{k+1}-\operatorname{prox}_{r_{k},-g_{0}}(w_{k})\|\operatorname{\leq} \epsilon_{k}\) and \(\sum_{k=1}^{\infty}\epsilon_{k}<\infty\).

In part (b), the first inequality uses the fact that \(Z_{k+1}+(X_{k+1}-X_{k})/r_{k}\in\mathbb{S}^{n}_{+}\) by performing Moreau decomposition [49, Exercise 12.22] on \(r_{k}X_{k+1}-Z_{k}\) and using the update rule (15b); The second inequality comes directly from (15c); The last inequality uses the definition of \(L_{r_{k}}(X_{k+1},w_{k})\) in (14) and the fact \(\min_{X\in\mathbb{S}^{n}}L_{r_{k}}(X,w_{k})\leq p^{\star}\).

Part (c) is a consequence of part (b) and the fact that \(\Omega_{\mathrm{P}}\) is bounded if and only if for any \(\gamma\in\mathbb{R}^{3}\) the set \(\{X\in\mathbb{S}^{n}\mid\operatorname{dist}(X,\mathbb{S}^{n}_{+})\leq\gamma_{ 1},\|\mathcal{A}(X)-b\|\leq\gamma_{2},\langle C,X\rangle\leq\gamma_{3}\}\) is bounded [23, page 110]. 

Proposition 2 establishes the asymptotic convergences for both the primal and dual variables. The linear convergence of the dual iterates can also be deduced when the negative of the dual \(g_{0}\) defined in (13) satisfies (QG). However, the rate of the primal iterates remains unclear. Here, leveraging the error bound (8b), we also derive a linear convergence of the primal iterates, which is one main technical contribution of this work. We summarize linear convergence results below.

**Theorem 3** (Linear convergences).: _Consider (P) and (D). Assume strong duality and dual strict complementarity holds (implying \(\Omega_{\mathrm{P}}\neq\emptyset\) and \(\Omega_{\mathrm{D}}\neq\emptyset\)). Let \(\{X_{k},w_{k}\}\) be a sequence from the ALM (15) under (A\({}^{\prime}\)) and (B\({}^{\prime}\)). The following statements hold._

1. _(Dual iterates and KKT residuals) There exist constants_ \(\hat{k}>0\) _such that for all_ \(k\geq\hat{k}\)_, we have_ \[\operatorname{dist}(w_{k+1},\Omega_{\mathrm{D}}) \leq\mu_{k}\cdot\operatorname{dist}(w_{k},\Omega_{\mathrm{D}}), \operatorname{dist}(X_{k+1},\mathbb{S}^{n}_{+}) \leq\mu^{\prime}_{k}\cdot\operatorname{dist}(w_{k},\Omega_{\mathrm{D}}),\] (16) \[\|\mathcal{A}(X_{k+1})-b\| \leq\mu^{\prime}_{k}\cdot\operatorname{dist}(w_{k},\Omega_{ \mathrm{D}}), \langle C,X_{k+1}\rangle-p^{\star}\leq\mu^{\prime\prime}_{k}\cdot \operatorname{dist}(w_{k},\Omega_{\mathrm{D}}),\] _where_ \(0<\mu_{k}<1\)_,_ \(\mu^{\prime}_{k}>0,\) _and_ \(\mu^{\prime\prime}_{k}>0\) _are positive finite constants._
2. _(Primal iterates) If_ \(\Omega_{\mathrm{P}}\) _is bounded, then the primal sequence_ \(\{X_{k}\}\) _also converges linearly to the solution set_ \(\Omega_{\mathrm{P}}\)_, i.e., there a constant_ \(\hat{k}>0\) _such that for all_ \(k\geq\hat{k}\)_,_ \[\operatorname{dist}^{2}(X_{k+1},\Omega_{\mathrm{P}})\leq\tau_{k}\cdot \operatorname{dist}(w_{k},\Omega_{\mathrm{D}}),\] (17) _where_ \(\tau_{k}>0\) _is a finite constant._Proof.: The proof of part (a) is largely motivated by the techniques in [26] that focus on ALMs on dual SDPs. Part (b) is a consequence of the error bound (7b). Here, we highlight the key steps of the proof of part (a) due to the page limit, and detailed arguments about part (a) can be found in Appendix E.2. Note that if dual strict complementarity holds, Theorem 2 (with \(h\) in (5) as an indicator function) guarantees the following two nice properties: 1) \(-g_{0}\) in (13) satisfies quadratic growth in (9b); 2) the error bound (7b) holds.

1. By the discussion after Lemma 4, since (A) and (B\({}^{\prime}\)) are in force, the dual sequence \(\{w_{k}\}\) satisfies \[\|w_{k+1}-\mathrm{prox}_{r_{k},-g_{0}}(w_{k})\|\leq\epsilon_{k},\|w_{k+1}- \mathrm{prox}_{r_{k},-g_{0}}(w_{k})\|\leq\delta_{k}\|w_{k+1}-w_{k}\|,\forall k \geq 0.\] Applying the convergence result of PPM in Lemma 1 yields the linear convergence of the dual distance, i.e., there exists a \(k_{1}\geq 0\) such that \[\mathrm{dist}(w_{k+1},\Omega_{\mathrm{D}})\leq\mu_{k}\cdot\mathrm{dist}(w_{k}, \Omega_{\mathrm{D}}),\;\mu_{k}<1,\;\forall k\geq k_{1}.\] Using part (b) in Proposition 2 and \(\|w_{k+1}-w_{k}\|\leq\frac{1}{1-\delta_{k}}\mathrm{dist}(w_{k},\Omega_{ \mathrm{D}})\) if \(\delta_{k}<1\), we can show \[\max\{\mathrm{dist}(X_{k+1},\mathbb{S}_{+}^{n}),\|\mathcal{A}(x_{k+1})-b\|\} \leq\frac{1}{(1-\delta_{k})r_{k}}\mathrm{dist}(w_{k},\Omega_{\mathrm{D}}),\] \[\langle C,X_{k+1}\rangle-p^{*}\leq\frac{\delta_{k}^{2}\|w_{k+1}-w_{k}\|+\|w_{k} \|+\|w_{k+1}\|}{2r_{k}(1-\delta_{k})}\mathrm{dist}(w_{k},\Omega_{\mathrm{D}}).\] Finally, as \(\delta_{k}\to 0\), there exists \(k_{2}\geq 0\) such that \(\delta_{k}<1\) and for all \(k\geq k_{2}\). Thus, part (a) then follows from taking \(\hat{k}=\max\{k_{1},k_{2}\}\).
2. Note that the error bound (7b) and the assumption that \(\Omega_{\mathrm{P}}\) is bounded guarantee that for any bounded set \(\mathcal{U}\) containing \(\Omega_{\mathrm{P}}\), there exist constants \(\alpha_{1},\alpha_{2},\alpha_{3}>0\) such that \[\langle C,X\rangle-\langle C,X^{*}\rangle+\alpha_{1}\|\mathcal{A}(X)-b\|+ \alpha_{2}\cdot\mathrm{dist}(X,\mathbb{S}_{+}^{n})\geq\alpha_{3}\cdot \mathrm{dist}^{2}(X,\Omega_{\mathrm{P}}),\quad\forall X\in\mathcal{U}.\] (18) By Proposition 2 - part (c), the sequence \(\{X_{k}\}\) is bounded. Let \(\mathcal{U}\subseteq\mathbb{S}^{n}\) be a bounded set that contains \(\Omega_{\mathrm{P}}\) and all the iterates \(\{X_{k}\}\). Combing (18) with (16), we have \[\alpha_{3}\cdot\mathrm{dist}^{2}(X_{k+1},\Omega_{\mathrm{P}})\leq(\mu_{k}^{ \prime\prime}+(\alpha_{1}+\alpha_{2})\mu_{k}^{\prime})\cdot\mathrm{dist}(w_{k},\Omega_{\mathrm{D}}),\quad\forall k\geq\hat{k}.\] Dividing both sides by \(\alpha_{3}\) leads to the desired result in part (b). This completes the proof. 

To our best knowledge, achieving linear convergence for the primal iterates of inexact ALMs requires a significantly stronger condition on the Lagrangian function [23, Theorem 5] and implementing an additional stopping criterion [26, Proposition 3]. Unfortunately, as highlighted in [26, Section 3], such an assumption in [23, Theorem 5] can easily fail for general conic programs.

Our result Theorem 3, however, reveals that the linear convergence of the primal iterates also happens under the standard assumption of strict complementarity and bounded primal solution set. This suggests that primal linear convergence is often expected since strict complementary is a generic property of conic programs [40, Theorem 15]. Our result not only completes theoretical convergences of inexact ALMs but also offers insights for practical successes in [18, 19].

_Remark 4_ (Strict complementarity).: As we see in the proof of Theorem 3, strict complementarity is the key to ensure that 1) the negative of the function \(g_{0}\) in (13) satisfies quadratic growth; 2) the error bound property (E.2) holds for the primal conic program. The quadratic growth is used to derive the dual linear convergence (16), and the error bound is to conclude the primal linear convergence. 

## 5 Numerical experiments

In this section, we present numerical experiments to examine the empirical performance of ALM introduced in Section 4. We consider two applications in combinational problems and machine learning, including the SDP relaxation of maximum cut (Max-Cut) problem [50] and linear Support Vector Machine (SVM). The Max-Cut problem and the linear SVM can be formulated as

\[\min_{X\in\mathbb{S}^{n}} \langle C,X\rangle \min_{x\in\mathbb{R}^{n},t\in\mathbb{R}^{m}} \lambda\mathbf{1}^{\mathsf{T}}t+\frac{1}{2}\|x\|^{2},\] \[\mathrm{subject\ to} \mathrm{Diag}(X)=\mathbf{1}, \mathrm{subject\ to} \mathrm{diag}(b)Ax+\mathbf{1}\leq t,\] \[X\in\mathbb{S}_{+}^{n}, t\in\mathbb{R}_{+}^{m}.\]

where \(\mathbf{1}\) is an all one vector, \(A\in\mathbb{R}^{m\times n}\) and \(b\in\mathbb{R}^{m}\) are problem data in the linear SVM, \(\mathrm{diag}(b)\)denotes the diagonal matrix with \(b\) as the diagonal elements, and \(\lambda>0\) is a constant. We can see that Max-Cut is of the same form as (P). Despite linear SVM is not in the form of SDP (P) but a quadratic program, the corresponding ALM with similar convergence guarantees can also be derived since a quadratic program is a special case of SDP. For Max-Cut problem, we select the graph \(\mathrm{G}_{1},\mathrm{G}_{2},\) and \(\mathrm{G}_{3}\) from the website https://web.stanford.edu/~yyye/yyye/Gset/ and only take the first \(20\times 20\) submatrix as the considered problem data \(C\). For linear SVM, we randomly generate the problem data \(A\in\mathbb{R}^{10\times 100}\) and \(b\in\{-1,1\}^{100}\). We then apply the ALM (15) for those instances and compute the relative primal and dual cost value gap and the relative feasibility residuals as follows

\[\epsilon_{1}= \frac{|\left\langle C,X_{k}\right\rangle-p^{\star}|}{1+|p^{ \star}|},\epsilon_{2}=\frac{|\left\langle b,y_{k}\right\rangle-d^{\star}|}{1+ |d^{\star}|},\eta_{1}=\frac{\|\mathcal{A}(X_{k})-b\|}{1+\|b\|},\eta_{2}=\frac{ \|X_{k}-\Pi_{\mathbb{S}_{+}^{n}}(X_{k})\|}{1+\|X_{k}\|},\] \[\eta_{3}=\frac{\|C-\mathcal{A}^{*}(y_{k})-Z_{k}\|}{1+\|C\|},\eta_ {4}=\frac{\|Z_{k}-\Pi_{\mathbb{S}_{+}^{n}}(Z_{k})\|}{1+\|Z_{k}\|},\eta_{5}= \frac{|\left\langle C,X_{k}\right\rangle-\left\langle b,y_{k}\right\rangle|}{1 +|d^{\star}|}.\]

The numerical results are presented in Figure 1. In all cases, the primal and dual cost value gap and the KKT residuals all converge linearly to at least the accuracy of \(10^{-5}\). The oscillation or flattening behavior that appears in the tail (when the iterates are close to the solution set) could be due to the inaccuracy of the subproblem solver and computational impreciseness. A detailed theoretical analysis of such behavior is interesting, and we leave it to future work.

Further details on the algorithm parameters, problem instances, and more numerical experiments for machine learning applications can be found in Appendix F.

## 6 Conclusion

In this paper, we have established the quadratic growth and error bound of two different variants (4) and (5) of SDPs under the condition of strict complementarity. Central to our approach is the examination of the growth properties of the indicator and exact penalty functions. By leveraging these new quadratic growth and error bounds, we establish the linear convergence of both primal and dual iterates of inexact ALMs when applied to semidefinite programs, under the usual assumption of strict complementarity and a bounded solution set on the primal side. Our result not only fills a void in the convergence theory of inexact ALMs but also offers valuable insights into the exceptional numerical performance of solvers rooted in ALMs, such as SDPNAL+[19]. We expect further interesting applications in machine learning and polynomial optimization [11] for inexact ALMs.

Figure 1: Numerical convergence behavior of inexact ALM (15) for Max-Cut and linear SVM. The symbol \(\epsilon_{3}\) denotes the KKT residuals \(\epsilon_{3}=\max\{\eta_{1},\eta_{2},\eta_{3},\eta_{4},\eta_{5}\}\).

## Acknowledgments and Disclosure of Funding

This work is supported by NSF ECCS 2154650, NSF CMMI 2320697, and NSF CAREER 2340713. We would like to thank Dr. Xin Jiang for the discussions on exact penalty functions, and Mr. Pranav Reddy for his assistance in proofreading the manuscript and contributions to the appendix.

## References

* Boyd et al. [1994] Stephen Boyd, Laurent El Ghaoui, Eric Feron, and Venkataramanan Balakrishnan. _Linear matrix inequalities in system and control theory_. SIAM, 1994.
* Sotirov [2012] Renata Sotirov. SDP relaxations for some combinatorial optimization problems. In _Handbook on Semidefinite, Conic and Polynomial Optimization_, pages 795-819. Springer, 2012.
* Blekherman et al. [2012] Grigoriy Blekherman, Pablo A Parrilo, and Rekha R Thomas. _Semidefinite optimization and convex algebraic geometry_. SIAM, 2012.
* Lanckriet et al. [2004] Gert RG Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui, and Michael I Jordan. Learning the kernel matrix with semidefinite programming. _Journal of Machine learning research_, 5(Jan):27-72, 2004.
* Raghunathan et al. [2018] Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying robustness to adversarial examples. _Advances in neural information processing systems_, 31, 2018.
* Batten et al. [2021] Ben Batten, Panagiotis Kouvaros, Alessio Lomuscio, and Yang Zheng. Efficient neural network verification via layer-based semidefinite relaxations and linear cuts. In Zhi-Hua Zhou, editor, _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21_, pages 2184-2190. International Joint Conferences on Artificial Intelligence Organization, 8 2021. Main Track.
* Vandenberghe and Boyd [1999] Lieven Vandenberghe and Stephen Boyd. Applications of semidefinite programming. _Applied Numerical Mathematics_, 29(3):283-299, 1999.
* Vandenberghe and Boyd [1996] Lieven Vandenberghe and Stephen Boyd. Semidefinite programming. _SIAM review_, 38(1):49-95, 1996.
* Alizadeh [1995] Farid Alizadeh. Interior point methods in semidefinite programming with applications to combinatorial optimization. _SIAM journal on Optimization_, 5(1):13-51, 1995.
* Wolkowicz et al. [2012] Henry Wolkowicz, Romesh Saigal, and Lieven Vandenberghe. _Handbook of semidefinite programming: theory, algorithms, and applications_, volume 27. Springer Science & Business Media, 2012.
* Zheng et al. [2021] Yang Zheng, Giovanni Fantuzzi, and Antonis Papachristodoulou. Chordal and factor-width decompositions for scalable semidefinite and polynomial optimization. _Annual Reviews in Control_, 52:243-279, 2021.
* Wen et al. [2010] Zaiwen Wen, Donald Goldfarb, and Wotao Yin. Alternating direction augmented Lagrangian methods for semidefinite programming. _Math. Program. Comput._, 2(3):203-230, 2010.
* O'Donoghue et al. [2016] Brendan O'Donoghue, Eric Chu, Neal Parikh, and Stephen Boyd. Conic optimization via operator splitting and homogeneous self-dual embedding. _Journal of Optimization Theory and Applications_, 169(3):1042-1068, June 2016.
* Zheng et al. [2020] Yang Zheng, Giovanni Fantuzzi, Antonis Papachristodoulou, Paul Goulart, and Andrew Wynn. Chordal decomposition in operator-splitting methods for sparse semidefinite programs. _Math. Program._, 180(1):489-532, 2020.
* Hestenes [1969] Magnus R Hestenes. Multiplier and gradient methods. _Journal of optimization theory and applications_, 4(5):303-320, 1969.
* Powell [1969] Michael JD Powell. A method for nonlinear constraints in minimization problems. _Optimization_, pages 283-298, 1969.

* [17] Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. _Math. Program._, 95(2):329-357, 2003.
* [18] Xin-Yuan Zhao, Defeng Sun, and Kim-Chuan Toh. A Newton-CG augmented Lagrangian method for semidefinite programming. _SIAM J. Optim._, 20(4):1737-1765, 2010.
* [19] Liuqin Yang, Defeng Sun, and Kim-Chuan Toh. Sdpnal+: a majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints. _Math. Program. Computation_, 7(3):331-366, 2015.
* [20] Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher. Scalable semidefinite programming. _SIAM Journal on Mathematics of Data Science_, 3(1):171-200, 2021.
* [21] Ying Cui, Chao Ding, Xu-Dong Li, and Xin-Yuan Zhao. Augmented Lagrangian methods for convex matrix optimization problems. _Journal of the Operations Research Society of China_, 10(2):305-342, 2022.
* [22] Ling Liang, Defeng Sun, and Kim-Chuan Toh. An inexact augmented Lagrangian method for second-order cone programming with applications. _SIAM J. Optim._, 31(3):1748-1773, 2021.
* [23] R Tyrrell Rockafellar. Augmented Lagrangians and applications of the proximal point algorithm in convex programming. _Mathematics of operations research_, 1(2):97-116, 1976.
* [24] R Tyrrell Rockafellar. The multiplier method of hestenes and powell applied to convex programming. _Journal of Optimization Theory and applications_, 12(6):555-562, 1973.
* [25] Fernando Javier Luque. Asymptotic convergence analysis of the proximal point algorithm. _SIAM Journal on Control and Optimization_, 22(2):277-293, 1984.
* [26] Ying Cui, Defeng Sun, and Kim-Chuan Toh. On the R-superlinear convergence of the KKT residuals generated by the augmented Lagrangian method for convex composite conic programming. _Math. Program._, 178:381-415, 2019.
* [27] Ying Cui, Defeng Sun, and Kim-Chuan Toh. On the asymptotic superlinear convergence of the augmented Lagrangian method for semidefinite programming with multiple solutions. _arXiv preprint arXiv:1610.00875_, 2016.
* [28] Lijun Ding and Madeleine Udell. A strict complementarity approach to error bound and sensitivity of solution of conic programs. _Optimization Letters_, 17(7):1551-1574, 2023.
* [29] Feng-Yi Liao, Lijun Ding, and Yang Zheng. An overview and comparison of spectral bundle methods for primal and dual semidefinite programs. _arXiv preprint arXiv:2307.07651_, 2023.
* [30] Lijun Ding and Benjamin Grimmer. Revisiting spectral bundle methods: Primal-dual (sub) linear convergence rates. _SIAM J. Optim._, 33(2):1305-1332, 2023.
* [31] Ying Cui, Chao Ding, and Xinyuan Zhao. Quadratic growth conditions for convex matrix optimization problems associated with spectral functions. _SIAM J. Optim._, 27(4):2332-2355, 2017.
* [32] Jos F Sturm. Error bounds for linear matrix inequalities. _SIAM J. Optim._, 10(4):1228-1248, 2000.
* [33] Shuzhong Zhang. Global error bounds for convex conic problems. _SIAM J. Optim._, 10(3):836-851, 2000.
* [34] Dmitriy Drusvyatskiy and Adrian S Lewis. Error bounds, quadratic growth, and linear convergence of proximal methods. _Mathematics of Operations Research_, 43(3):919-948, 2018.
* [35] Zirui Zhou and Anthony Man-Cho So. A unified approach to error bounds for structured convex optimization problems. _Math. Program._, 165:689-728, 2017.
* [36] Alan J Hoffman. On approximate solutions of systems of linear inequalities. _Journal of Research of the National Bureau of Standards_, 49(4), 1952.

* [37] James V Burke and Paul Tseng. A unified analysis of hoffman's bound via fenchel duality. _SIAM J. Optim._, 6(2):265-282, 1996.
* [38] R Tyrrell Rockafellar. A dual approach to solving nonlinear programming problems by unconstrained optimization. _Math. Program._, 5(1):354-373, 1973.
* [39] Mirjam Dur, Bolor Jargalsaikhan, and Georg Still. Genericity results in linear conic programming--a tour d'horizon. _Mathematics of operations research_, 42(1):77-94, 2017.
* [40] Farid Alizadeh, Jean-Pierre A Haeberly, and Michael L Overton. Complementarity and nondegeneracy in semidefinite programming. _Math. Program._, 77(1):111-128, 1997.
* [41] Lijun Ding and Madeleine Udell. On the simplicity and conditioning of low rank semidefinite programs. _SIAM J. Optim._, 31(4):2614-2637, 2021.
* [42] R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. _SIAM journal on control and optimization_, 14(5):877-898, 1976.
* [43] D Leventhal. Metric subregularity and the proximal point method. _Journal of Mathematical Analysis and Applications_, 360(2):681-688, 2009.
* [44] Feng-Yi Liao, Lijun Ding, and Yang Zheng. Error bounds, PL condition, and quadratic growth for weakly convex functions, and linear convergences of proximal point methods. _arXiv preprint arXiv:2312.16775_, 2023.
* [45] Andrzej Ruszczynski. _Nonlinear optimization_. Princeton university press, 2011.
* [46] Jong-Shi Pang. Error bounds in math. program. _Math. Program._, 79(1-3):299-332, 1997.
* [47] Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of first order methods for non-strongly convex optimization. _Math. Program._, 175:69-107, 2019.
* [48] FJ Aragon Artacho and Michel H Geoffroy. Characterization of metric regularity of subdifferentials. _Journal of Convex Analysis_, 15(2):365, 2008.
* [49] R Tyrrell Rockafellar and Roger J-B Wets. _Variational analysis_, volume 317. Springer Science & Business Media, 2009.
* [50] Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. _Journal of the ACM (JACM)_, 42(6):1115-1145, 1995.
* [51] R Tyrrell Rockafellar. _Convex analysis_, volume 11. Princeton university press, 1997.
* [52] Eduardo H Zarantonello. Projections on convex sets in Hilbert space and spectral theory: Part i. Projections on convex sets: Part ii. Spectral theory. In _Contributions to nonlinear functional analysis_, pages 237-424. Elsevier, 1971.
* [53] Heinz H Bauschke and Jonathan M Borwein. On projection algorithms for solving convex feasibility problems. _SIAM review_, 38(3):367-426, 1996.
* [54] Heinz H Bauschke, Jonathan M Borwein, and Wu Li. Strong conical hull intersection property, bounded linear regularity, Jameson's property (G), and error bounds in convex optimization. _Math. Program._, 86:135-160, 1999.
* [55] Michael L Overton. Large-scale optimization of eigenvalues. _SIAM J. Optim._, 2(1):88-120, 1992.
* [56] MOSEK ApS. _The MOSEK optimization toolbox for MATLAB manual. Version 9.0._, 2019.

###### Contents

* 1 Introduction
* 2 Preliminaries
	* 2.1 Strong duality and strict complementarity
	* 2.2 Proximal point methods (PPMs)
* 3 Quadratic Growth in SDPs
	* 3.1 Growth and error bound properties
	* 3.2 Proof sketches for Theorems 1 and 2
* 4 Augmented Lagrangian Methods (ALMs) for SDPs
	* 4.1 Inexact Augmented Lagrangian Method
	* 4.2 Linear convergences of primal and dual iterates in ALM
* 5 Numerical experiments
* 6 Conclusion
* A Optimization Background
* A.1 Standard notion of strict complementarity in SDPs
* A.2 Augmented Lagrangian Methods (ALMs) for inequality constraints
* A.3 Augmented Lagrange functions for inequality constraints
* A.4 Bounded Linear regularity and regularity conditions
* B Exact penalty forms of conic optimization
* B.1 Subdifferential of the exact penalty function
* B.2 Proof for Proposition B.1
* C Growth properties of indicator and exact penalty functions in Lemma 3
* C.1 Proof of Lemma C.1
* C.2 Growth property of the exact penalty function \(l\) in (10)
* D Growth growth under strict complementarity
* D.1 Proof of Proposition 1
* D.2 Illustration of quadratic growth
* E Proofs and supplemental discussions in Section 4
* E.1 Proof of Proposition 2
* E.2 Proof of Theorem 3
* E.3 ALMs for dual conic program (D)
* F Details of the numerical experiments

## Appendix

The appendix is divided into the following seven sections:

1. Appendix A presents some preliminaries in conic optimization and standard ALMs.
2. Appendix B presents a self-contained proof for the exact penalty functions and a new characterized of the preimage of the exact penalty functions.
3. Appendix C completes the proof of growth properties of the indicator function and exact penalty function.
4. Appendix D details the proof of Proposition 1 and presents a simple example to illustrate the growth property.
5. Appendix E.2 completes the missing proofs in Theorem 3.
6. Appendix E.3 discusses the convergence of ALMs applied to the dual problem (D).
7. Appendix F presents further details of numerical experiments in Section 5.

## Appendix A Optimization Background

NotationsFor clarity, we summarize common notations here. We use \(\mathbb{R}^{n}\) to denote the \(n\)-dimensional real space and \(\mathbb{S}^{n}\) to denote the space of real \(n\times n\) symmetric matrices. The cone of nonnegative orthant is denoted as \(\mathbb{R}^{n}_{+}=\{x\in\mathbb{R}^{n}\mid x_{i}\geq 0,\forall i=1,\dots,n\}\). The cone of positive and negative semidefinite matrices are defined as \(\mathbb{S}^{n}_{+}=\{X\in\mathbb{S}^{n}\mid v^{\mathsf{T}}Xv\geq 0,\forall v\in \mathbb{R}^{n}\}\) and \(\mathbb{S}^{n}_{-}=-\mathbb{S}^{n}_{+}\) respectively. For vectors in \(\mathbb{R}^{n}\), the inner product \(\langle\cdot,\cdot\rangle\) is defined as \(\langle x,y\rangle=x^{\mathsf{T}}y\) and the normal \(\|\cdot\|\) stands for the \(l_{2}\) norm. For matrices in \(\mathbb{S}^{n}\), the inner product \(\langle\cdot,\cdot\rangle\) is defined as \(\langle X,Y\rangle=\mathbf{tr}(X^{\mathsf{T}}Y)\) and the norm \(\|\cdot\|_{\mathrm{op}},\|\cdot\|_{*}\) and \(\|\cdot\|\) denote the operator two norm, nuclear norm, and Frobenius norm respectively. Given a set \(S\subseteq\mathbb{R}^{n}\), the relative interior of \(S\) is denoted as \(\mathrm{relint}(S)\), and \(\delta_{S}(x)\) is defined as the indicator function of \(S\), i.e., \(\delta_{S}(x)=0\) if \(x\in S\) and \(\delta_{S}(x)=\infty\) otherwise. The closed ball with a radius \(\mu\) around the center \(x\in\mathbb{R}^{n}\) is written as \(\mathbb{B}(x,\mu)=\{y\in\mathbb{R}^{n}\mid\|y-x\|\leq\mu\}\).

### Standard notion of strict complementarity in SDPs

In this subsection, we review the standard algebraic notion definition for SDPs which is equivalent to Definition 2. A proof of the equivalence can be found in [28, Appendix B].

**Definition A.1** (Strict complementarity for SDPs).: Consider the pair of primal and dual SDPs (P) and (D). We say the strict complementarity holds if there exists a pair of primal and dual solutions \((X^{\star},y^{\star},Z^{\star})\) satisfying

\[X^{\star}+Z^{\star}\in\mathbb{S}^{n}_{++}\text{ or }\mathrm{rank}(X^{\star})+ \mathrm{rank}(Z^{\star})=n.\] (A.1)

With the complementarity slackness \(\langle X^{\star},Z^{\star}\rangle=0\), we have \(X^{\star}Z^{\star}=Z^{\star}X^{\star}=0\). This means there exists an orthonormal matrix \(Q\in\mathbb{R}^{n\times n}\) with \(Q^{\mathsf{T}}Q=I\), such that

\[X^{\star}=Q\cdot\mathrm{diag}(\lambda_{1},\dots,\lambda_{n})\cdot Q^{\mathsf{ T}},\quad Z^{\star}=Q\cdot\mathrm{diag}(w_{1},\dots,w_{n})\cdot Q^{\mathsf{T}}\] (A.2)

and \(\lambda_{i}w_{i}=0,i=1,\dots,n\). Then, (A.1) implies that exactly one of the two conditions \(\lambda_{i}=0\) and \(w_{i}=0\) is true in (A.2). In other words, the eigenvalues of \(X^{\star}\) and \(Z^{\star}\) have a complementary sparsity pattern.

_Remark A.1_ (Dual strict complementarity in the general case).: The dual strict complementarity in Definition 2 is consistent with [28, Definition 2]. A variant of dual strict complementarity in the general case is given in [34, Section 4], which requires that

\[0\in\mathrm{relint}(\partial g(y^{\star})),\] (A.3)

where \(g(y)=\langle b,y\rangle-\delta_{\mathbb{S}^{n}_{+}}(C-\mathcal{A}^{\star}(y))\). Definition 2 is also consistent with (A.3). Indeed, let a dual optimal solution \((y^{\star},Z^{\star})\) satisfy (A.3). Considering \(\partial g(y)=b+\mathcal{A}(\mathcal{N}_{\mathbb{S}^{n}_{+}}(C-\mathcal{A}^{ \star}(y)))\) and using [51, Corollary 6.6.2 and Theorem 6.6], we have

\[0\in\mathrm{relint}(b+\mathcal{A}(\mathcal{N}_{\mathbb{S}^{n}_{+}}(Z^{\star}) ))=b+\mathrm{relint}(\mathcal{A}(\mathcal{N}_{\mathbb{S}^{n}_{+}}(Z^{\star}) )))=b+\mathcal{A}(\mathrm{relint}(\mathcal{N}_{\mathbb{S}^{n}_{+}}(Z^{\star}) )),\]

which means there exists a point \(s\in\mathrm{relint}(\mathcal{N}_{\mathbb{S}^{n}_{+}}(Z^{\star}))\) such that \(0=b+\mathcal{A}(s)\). Equivalently, there exists \(X^{\star}=-s\in\mathrm{relint}(\mathcal{N}_{(\mathbb{S}^{n}_{+})^{\circ}}(-Z^{ \star}))\) (recall that \((\mathbb{S}^{n}_{+})^{\circ}=-\mathbb{S}^{n}_{+}\)) such that \(\mathcal{A}(X^{\star})=b\). This is equivalent to Definition 2.

### Augmented Lagrangian Methods (ALMs) for inequality constraints

We here review some classic results of ALM. Consider a constrained convex optimization problem

\[\min_{x\in\mathcal{X}_{0}} f(x)\] (A.4) \[\mathrm{subject\ to} f_{i}(x)\leq 0,\quad\forall i=1,2,\ldots,m,\]

where \(\mathcal{X}_{0}\subseteq\mathbb{R}^{n}\) is a closed convex set and \(f,f_{i}:\mathbb{E}\to\bar{\mathbb{R}}\) are proper closed convex functions. We associate each inequality constraint in (A.4) with a dual variable \(z_{i}\geq 0\), and the Lagrangian function for (A.4) is \(L_{0}(x,z)=f(x)+\sum_{i=1}^{m}z_{i}f_{i}(x)\). The corresponding dual function is

\[g_{0}(z)=\inf_{x\in\mathcal{X}_{0}}L_{0}(x,z).\] (A.5)

One way to define the augmented Lagrangian for (A.4) is to transform the inequality constraints into equality constraints [45, Chapter 4.7]. After some manipulation (see [45, Equation 4.79] or Appendix A.3), the classical augmented Lagrangian with parameter \(r>0\) for (A.4) reads as

\[L_{r}(x,z)=f(x)+\frac{1}{2r}\left(\|\Pi_{\mathbb{R}^{m}_{+}}(z+rf(x))\|^{2}- \|z\|_{2}^{2}\right),\] (A.6)

where \(\Pi_{\mathbb{R}^{m}_{+}}(\cdot)\) denotes the Euclidean projection onto \(\mathbb{R}^{m}_{+}\). Note that \(L_{r}\) is differentiable in \(z\)[52]. Given an initial point \(z_{0}\in\mathbb{R}^{m}\) and a sequence of positive scalars \(r_{0}\uparrow r_{\infty}<+\infty\), the inexact ALM generates a sequence of \(\{x_{k}\}\) (primal variables) and \(\{z_{k}\}\) (dual variables) as

\[x_{k+1}\approx\min_{x\in\mathcal{X}_{0}}L_{r_{k}}(x,z_{k}),\] (A.7a) \[z_{k+1}=z_{k}+r_{k}\nabla_{z}L_{r_{k}}(x_{k+1},z_{k})=\Pi_{ \mathbb{R}^{m}_{+}}(z_{k}+r_{k}f(x_{k+1})),\quad k=0,1,\ldots.\] (A.7b)

The convergence of \(x_{k}\) and \(z_{k}\) depends on the inexactness in (A.7a). For this, the seminal work by Rockafellar [23] points out an important connection between PPM and ALM, summarized below.

**Lemma A.1** ([23, Proposition 6]).: _The dual iterate \(z_{k+1}\) in (A.7b) satisfies the following relationship_

\[\|z_{k+1}-\mathrm{prox}_{r_{k},-g_{0}}(z_{k})\|^{2}/(2r_{k})\leq\bar{L}_{r_{k} }(x_{k+1},z_{k})-\inf_{x\in\mathcal{X}_{0}}L_{r_{k}}(x,z_{k}),\]

_where \(g_{0}\) is the Lagrangian dual function in (A.5), and \(\bar{L}_{r}\) denotes an extended form of the augmented Lagrangian \(L_{r}\) as \(\bar{L}_{r}(x,z)=L_{r}(x,z),\text{if }x\in\mathcal{X}_{0}\), otherwise \(\bar{L}_{r}(x,z)=\infty\)._

Note the subtle difference between Lemmas 4 and A.1. If (A.7a) is updated exactly, Lemma A.1 confirms that \(z_{k+1}=\mathrm{prox}_{c_{k},-g_{0}}(z_{k})\). In other words, the dual iterates of the ALM are the same as the iterates from PPM on the dual problem (A.5). Moreover, by controlling the inexactness (A.7a), the convergence of the dual variables \(z_{k}\) can be naturally guaranteed from Lemma 1. Motivated by (a) and (b), the following two criteria are commonly used in (A.7a)

\[\bar{L}_{c_{k}}(x_{k+1},z_{k})-\min_{x\in\mathcal{X}_{0}}\bar{L}_ {r_{k}}(x,z_{k})\leq\epsilon_{k}^{2}/(2r_{k}),\quad\sum_{k=1}^{\infty} \epsilon_{k}<\infty,\] (A) \[\bar{L}_{c_{k}}(x_{k+1},z_{k})-\min_{x\in\mathcal{X}_{0}}\bar{L}_ {r_{k}}(x,z_{k})\leq\delta_{k}^{2}\|z_{k+1}-z_{k}\|^{2}/(2r_{k}),\quad\sum_{k= 1}^{\infty}\delta_{k}<\infty,\] (B)

where \(\{\epsilon_{k}\}_{k\geq 0}\) and \(\{\delta_{k}\}_{k\geq 0}\) are two sequences of inexactness. Note that (A) and (B) guarantee the dual variables \(z_{k}\) from ALM (A.7) satisfy (a) and (b) with the inexact PPM (3), respectively. Let \(S=\mathrm{argmax}_{z\geq 0}\,g_{0}(z)\) denote the set of optimal dual solutions for (A.4). Combining Lemma A.1 with Lemma 1, the iterates from the inexact ALM (A.7) has the following convergence guarantees.

**Lemma A.2**.: _Assume \(S\neq\emptyset\). Let \(\{x_{k},z_{k}\}\) be a sequence generated by the inexact ALM (A.7)._

1. _If criterion (_A_) is used, then_ \(\lim_{k\to\infty}z_{k}=z_{\infty}\!\in\!S\) _(i.e.,_ \(z_{k}\) _converges to a dual optimal solution), and the primal sequence_ \(\{x_{k}\}\) _satisfies_ \(f_{i}(x_{k+1})\leq\|z_{k}-z_{k+1}\|/c_{k},i=1,\ldots,m\) _(primal feasibility), and_ \(\langle c,x_{k+1}\rangle-p^{*}\leq(\epsilon_{k}^{2}+\|z_{k}-z_{k+1}\|^{2})/(2 c_{k})\) _(cost value gap)._
2. _If both criteria (_A_) and (_B_) are used, and the dual function_ \(-g_{0}\) _in (_A.5_) further satisfies (_QG_) with a coefficient_ \(\mu_{q}>0\)_, then there exists a_ \(\hat{k}>0\) _such that_ \(\mathrm{dist}(z_{k+1},S)\leq\mu_{k}\cdot\mathrm{dist}(z_{k},S),\ \forall k\geq\hat{k},\) _where_ \(\mu_{k}=\frac{\theta_{k}+2\delta_{k}}{1-\delta_{k}}\) _with_ \(\theta_{k}=1/\sqrt{c_{k}\mu_{q}+1}\!<\!1\)_._

The first asymptotic convergence for \(\lim_{k\to\infty}z_{k}=z_{\infty}\!\in\!S\) directly comes from Lemma 1, and the asymptotic convergence in terms of primal feasibility and cost value gap is taken from [23, Theorem 4]. The second linear convergence in Lemma A.2 is only guaranteed for the _dual_ iterates \(z_{k}\), which is also implied by Lemma 1. However, the rate of the _primal_ iterates \(x_{k}\) is unclear even with the quadratic growth condition. With a much stronger condition on the Lagrangian, the classical result in [23, Theorem 5] can ensure linear convergence of \(x_{k}\).

### Augmented Lagrange functions for inequality constraints

One straightforward way to define the augmented Lagrangian for the inequality form (A.4) is to first transform (A.4) into the following equivalent problem

\[\min_{x\in\mathcal{X}_{0},v\geq 0} f(x)\] (A.8) \[\mathrm{subject\ to} f(x)+v=0,\quad\forall i=1,2,\ldots,m.\]

Then the augmented Lagrangian for the above equality-constrained problem with a penalty term \(r>0\) is defined as

\[L_{r}(x,v,z) =f(x)+\sum_{i=1}^{m}z_{i}(f_{i}(x)+v_{i})+\frac{r}{2}\sum_{i=1}^{ m}(f_{i}(x)+v_{i})^{2}\] \[=f(x)+\frac{1}{2r}\left(\|z+r(f(x)+v)\|^{2}-\|z\|^{2}\right).\]

Note that \(L_{r}\) is a decoupled quadratic function in terms of \(v_{i}\). Minimizing \(L_{r}(x,v,z)\) with respect to \(v\geq 0\) with a fixed \(x\) and \(y\), we derive the optimal \(v\) as

\[v_{i}=\max\left\{0,-\frac{z_{i}}{r}-f_{i}(x)\right\},\ \forall i=1,\ldots,m.\]

Upon plugging the optimal \(v\) in the \(L_{r}\) with some arrangements, we arrive at the classical augmented Lagrangian for the problem with inequality constraints (A.4)

\[L_{r}(x,z)=f(x)+\frac{1}{2r}\left(\|\Pi_{\mathbb{R}^{m}_{+}}(z+rf(x))\|^{2}-\| z\|_{2}\right).\]

### Bounded Linear regularity and regularity conditions

We review an important concept: _bounded linear regularity_ of a collection of closed convex sets.

**Definition A.2** (**Bounded linear regularity [53, Definition 5.6]**).: Let \(D_{1},\ldots,D_{m}\subseteq\mathbb{R}^{n}\) be closed convex set. Suppose \(D:=D_{1}\cap\cdots\cap D_{m}\) is nonempty. The collection \(\{D_{1},\ldots,D_{m}\}\) is called _boundedly linear regular_ if for every bounded set \(\mathcal{V}\subseteq\mathbb{R}^{n}\), there exists a constant \(\kappa>0\) such that

\[\mathrm{dist}(x,D)\leq\kappa\max\{\mathrm{dist}(x,D_{1}),\ldots,\mathrm{dist}( x,D_{m})\},\ \forall x\in\mathcal{V}.\]

Bounded linear regularity allows us to bound (up to some constant) the distance to the intersection \(D\) (which can be complicated) by the maximum of the distance to \(D_{i}\), which is often easier to compute. The following result shows a sufficient condition for bounded linear regularity.

**Lemma A.3** ([54, Collorary 3]).: _Let \(D_{1},\ldots,D_{m}\subseteq\mathbb{R}^{n}\) be closed convex set. Suppose that \(D_{1},\ldots,D_{r}(r\leq m)\) are polyhedra. Then \(\{D_{1},\ldots,D_{m}\}\) is boundedly linear regular if_

\[(\cap_{i=1,2,\ldots,r}D_{i})\bigcap(\cap_{i=r+1,\ldots,m}\mathrm{relint}(D_{i} ))\neq\emptyset,\]

_where \(\mathrm{relint}\) denotes the relative interior._

This result indicates that a collection of closed convex sets is boundedly linear regular if the relative interiors of non-polyhedral sets have a non-empty intersection with the rest of the polyhedra. Lemma A.3 plays a key role in the proof of Theorems 1 and 2. In particular, as we will see in Appendix D.1, the optimal solution set of (P), i.e., \(\Omega_{\mathrm{P}}\), can be characterized as

\[\Omega_{\mathrm{P}}=\mathcal{X}_{0}\cap(\partial g)^{-1}(-Z^{\star}),\quad \forall(y^{\star},Z^{\star})\in\Omega_{\mathrm{D}},\]

where \(\mathcal{X}_{0}=\{X\in\mathbb{S}^{n}_{+}\mid\mathcal{A}(X)=b\}\). Thus, assuming dual strict complementarity (there exists \(X^{\star}\in\mathrm{relint}((\partial g)^{-1}(-Z^{\star}))\)), given a compact set \(\mathcal{U}\subseteq\mathbb{S}^{n}\) containing \(X^{\star}\), Lemma A.3 ensures the following holds

\[\mathrm{dist}(X,\Omega_{\mathrm{P}})\leq\alpha_{1}\mathrm{dist}(X,\mathcal{X }_{0})+\alpha_{2}\mathrm{dist}(X,(\partial g)^{-1}(-Z^{\star})),\ \forall X\in\mathcal{U},\]

where \(\alpha_{1}\) and \(\alpha_{2}\) are two positive constants.

Exact penalty forms of conic optimization

In this section, we characterize the connection between exact penalty functions and indicator functions (6). Most importantly, we give a simple proof for the exact penalty functions. The main result is stated in Proposition B.2. We first state the preimage characterization of subdifferential mapping of the exact penalty function in Proposition B.1, which is a more general result of the first part of Lemma 3 (i.e., (11)). The second part of Lemma 3 (i.e., (12)) is postponed to Appendix C.

### Subdifferential of the exact penalty function

**Proposition B.1**.: _Let \(Z\in\mathbb{S}_{+}^{n}\) and define_

\[l(X)=\rho\max\{0,\lambda_{\max}(-X)\}\text{ with }\rho>\mathbf{tr}(Z).\] (B.1)

_Then for all \(S\in\mathbb{S}_{+}^{n}\) satisfying \(\rho>\mathbf{tr}(S)\), it holds that_

\[(\partial l)^{-1}(-S)=(\delta_{\mathbb{S}_{+}^{n}})^{-1}(-S)=\mathcal{N}_{( \mathbb{S}_{+}^{n})^{\circ}}(-S).\] (B.2)

One can see that (11) is a special case of Proposition B.1 with \(S=\bar{Z}\). We postpone the proof of Proposition B.1 to Appendix B.2. Be aware that (B.2) does not mean the mappings \((\partial l)^{-1}\) and \((\delta_{\mathbb{S}_{+}^{n}})^{-1}\) are the same for each point in the domain, while the equality only holds for certain points satisfying the relationship with the penalty coefficient \(\rho\) in (B.1). We use a simple example to illustrate this subtlety.

**Example B.1**.: _Consider \(l:\mathbb{R}\to\mathbb{R}\) as \(l(x)=2\max\{0,-x\}\) which is the scalar version of the function in (B.1) with \(\rho=2\). The subdifferential \(\partial l:\mathbb{R}\rightrightarrows\mathbb{R}\) and \(\partial\delta_{\mathbb{R}_{+}}=\mathcal{N}_{\mathbb{R}_{+}}\) can be computed as_

\[\partial l(x)=\begin{cases}0,&\text{if }x>0,\\ [-2,0],&\text{if }x=0,\\ -2,&\text{if }x<0,\end{cases}\text{ and }\partial\delta_{\mathbb{R}_{+}}(x)= \begin{cases}0,&\text{if }x>0,\\ (-\infty,0],&\text{if }x=0,\\ \emptyset,&\text{if }x<0.\end{cases}\]

_Thus, the preimages can be verified as_

\[(\partial l)^{-1}(-z)=\begin{cases}\emptyset,&\text{if }z>2,\\ (-\infty,0],&\text{if }z=2,\\ 0,&\text{if }0<z<2,\\ [0,\infty),&\text{if }z=0,\\ \emptyset,&\text{if }z<0,\end{cases}\text{ and }\partial(\delta_{\mathbb{R}_{+}})^{-1}(-z)= \begin{cases}0,&\text{if }z>0,\\ [0,\infty),&\text{if }z=0,\\ \emptyset,&\text{if }z<0.\end{cases}\]

_This shows that \((\partial l)^{-1}(-z)=(\delta_{\mathbb{R}_{+}})^{-1}(-z)\) holds only when \(z<2\)._

With the preimage characterization in Proposition B.1, the following result on the exact penalty functions follows easily by choosing the penalty term \(\rho\) correctly.

**Proposition B.2** (Exact penalty function).: _Assuming strong duality holds for (P) and (D), (4) is equivalent to the primal SDP (P) if the function \(g\) in (4) is chosen to be the corresponding indicator function, i.e., \(g=\delta_{\mathbb{S}_{+}^{n}}\), or the exact penalty function_

\[l(X)=\rho\max\{0,\lambda_{\max}(-X)\}\text{ with }\rho>\mathbf{tr}(Z^{\star}),\] (B.3)

_where \((y^{\star},Z^{\star})\in\Omega_{\mathrm{D}}\) is an optimal dual solution of (D)._

Proof.: The case for the indicator function \(\delta_{\mathbb{S}_{+}^{n}}\) is clear. We consider the case for the exact penalty function. Note the dual of (4) is

\[\begin{split}\max_{y,Z}&\langle b,y\rangle\\ \text{subject to}&\mathcal{A}^{\star}(y)+Z=C,\\ &\mathbf{tr}(Z)\leq\rho,\\ & Z\in\mathbb{S}_{+}^{n}.\end{split}\] (B.4)Let \(\Omega_{\mathrm{P}_{2}}\) and \(\Omega_{\mathrm{D}_{2}}\) denote the optimal solution of (4) and (B.4) respectively. First, it is clear that \(\Omega_{\mathrm{D}_{2}}\subseteq\Omega_{\mathrm{D}}\) as there is an extra constraint in (B.4), compared with (D). Second, as the problem is convex, a pair of primal and dual solutions \((\hat{X}^{\star},\hat{y}^{\star},\hat{Z}^{\star})\) solves (4) and (B.4) if and only if the following KKT system holds

\[\mathcal{A}(\hat{X}^{\star})=b,\ Z^{\star}=C-\mathcal{A}^{\star}(\hat{y}^{ \star}),\ 0\in\hat{Z}^{\star}+\partial g(\hat{X}^{\star}).\]

Then the primal solution set can be written as

\[\Omega_{\mathrm{P}_{2}}=\mathcal{X}_{0}\cap(\partial g)^{-1}(-\hat{Z}^{\star }),\quad\forall(\hat{y}^{\star},\hat{Z}^{\star})\in\Omega_{\mathrm{D}_{2}}.\]

where \(\mathcal{X}_{0}=\{X\in\mathbb{S}^{n}\mid\mathcal{A}(X)=b\}\). Note that \((y^{\star},Z^{\star})\) is an optimal solution to (B.4) as \((y^{\star},Z^{\star})\) is feasible in (B.4) and \(\langle b,y^{\star}\rangle\leq\max_{y,Z}\) (B.4) \(\leq d^{\star}=\langle b,y^{\star}\rangle\) by construction. We can write \(\Omega_{\mathrm{P}_{2}}\) as

\[\Omega_{\mathrm{P}_{2}}=\mathcal{X}_{0}\cap(\partial g)^{-1}(-Z^{\star})= \mathcal{X}_{0}\cap(\partial\delta_{\mathbb{S}^{n}_{+}})^{-1}(-Z^{\star}),\]

where the last equality uses \((\partial g)^{-1}(-Z^{\star})=(\partial\delta_{\mathbb{S}^{n}_{+}})^{-1}(-Z^{ \star})\) in (B.2) as \(\mathbf{tr}(Z^{\star})<\rho\) and \(Z^{\star}\in\mathbb{S}^{n}_{+}\). On the other hand, we can write \(\Omega_{\mathrm{P}}\) as

\[\Omega_{\mathrm{P}}=\mathcal{X}_{0}\cap(\partial\delta_{\mathbb{S}^{n}_{+}})^{ -1}(-Z^{\star}).\]

It is clear that \(\Omega_{\mathrm{P}_{2}}=\Omega_{\mathrm{P}}\) and the proof is complete. 

_Remark B.1_ (Dual problem).: Note that Proposition B.2 only guarantees the cost value and solution sets of (4) and (P) to be the same, while the solution sets of (D) and the dual of (4) may differ, as seen in the proof of Proposition B.2. To fully equate (D) and the dual of (4), one can choose the penalty coefficient \(\rho\) as \(\rho>\sup_{(y^{\star},Z^{\star})\in\Omega_{\mathrm{D}}}\mathbf{tr}(Z^{\star})\). This choice of \(\rho\) can guarantee every optimal solution in (D) is also an optimal solution in (B.4) as the extra constraint in (B.4) (compared with (D)) does not affect the solution set. Combining this with the fact \(\Omega_{\mathrm{D}_{2}}\subseteq\Omega_{\mathrm{D}}\), we conclude that \(\Omega_{\mathrm{D}}=\Omega_{\mathrm{D}_{2}}\).

### Proof for Proposition b.1

The following proposition showcases the subdifferential calculation of the function \(l(X)=\rho\max\{0,\lambda_{\max}(-X)\}\) with \(\rho\geq 0\). It is a standard result of the subdifferential of the maximal eigenvalue of symmetric matrices and standard subdifferential calculus.

**Proposition B.3** ([55, Theorem 2] and [45, Theorem 2.87]).: _Let \(\rho\geq 0\) and \(X\in\mathbb{S}^{n}\) with \(\lambda_{\max}(-X)\) having \(t\) multiplicity. The subdifferential of \(l(X)=\rho\max\{0,\lambda_{\max}(-X)\}\) is characterized by_

\[\partial l(X)=\begin{cases}0&\text{if }\lambda_{\min}(X)>0,\\ \{-\rho PS^{\mathsf{T}}\mid S\in\mathbb{S}^{t}_{+},\mathbf{tr}(S)\leq 1\}&\text{ if } \lambda_{\min}(X)=0,\\ \{-\rho PS^{\mathsf{T}}\mid S\in\mathbb{S}^{t}_{+},\mathbf{tr}(S)=1\}&\text{ if } \lambda_{\min}(X)<0,\end{cases}\] (B.5)

_where columns of \(P\in\mathbb{R}^{n\times t}\) are orthonormal eigenvectors corresponding to \(\lambda_{\max}(-X)\)._

**Proposition B.4**.: _Let \(Z\in\mathbb{S}^{n}_{+}\setminus\{0\}\) and write \(Z=[P_{1}\quad P_{2}]\begin{bmatrix}\Lambda_{1}&0\\ 0&0\end{bmatrix}\begin{bmatrix}P_{1}^{\mathsf{T}}\\ P_{2}^{\mathsf{T}}\end{bmatrix}\) with \(\Lambda_{1}\in\mathbb{S}^{r}_{++}\). Let \(l(X)=\rho\max\{0,\lambda_{\max}(-X)\}\) with \(\rho>\mathbf{tr}(Z)\). We have_

\[(\partial l)^{-1}(0) =\mathbb{S}^{n}_{+},\] \[(\partial l)^{-1}(-Z) =(\mathcal{N}_{\mathbb{S}^{n}_{+}})^{-1}(-Z).\]

Proof.: The case \((\partial l)^{-1}(0)=\mathbb{S}^{n}_{+}\) can be easily observed from the first two cases in (B.5). We then consider the case \(Z\in\mathbb{S}^{n}_{+}\setminus\{0\}\). Given a \(X\in\mathbb{S}^{n}\), we use the matrix \(P_{X}\in\mathbb{R}^{n\times t}\) to denote the orthonormal eigenvectors corresponding to \(\lambda_{\max}(-X)\) where \(t\) is the multiplicity of \(\lambda_{\max}(-X)\). It follows that

\[(\partial l)^{-1}(-Z) =\{X\in\mathbb{S}^{n}\mid-Z\in\partial l(X)\}\] \[\stackrel{{(a)}}{{=}}\{X\in\mathbb{S}^{n}_{+}\setminus \mathbb{S}^{n}_{++}\mid Z\in\rho P_{X}SP_{X}^{\mathsf{T}},S\in\mathbb{S}^{t}_{+}, \mathbf{tr}(S)<1\}\] \[\stackrel{{(b)}}{{=}}\left\{[P_{1}\quad P_{2}] \begin{bmatrix}\mathbf{0}&0\\ 0&B\end{bmatrix}\begin{bmatrix}P_{1}^{\mathsf{T}}\\ P_{2}^{\mathsf{T}}\end{bmatrix}\mid\mathbf{0}\in\mathbb{R}^{r\times r},B\in \mathbb{S}^{n-r}_{+}\right\}\] \[\stackrel{{(c)}}{{=}}\mathcal{N}_{\mathbb{S}^{n}_{-}}(-Z)\] \[=(\mathcal{N}_{\mathbb{S}^{n}_{+}})^{-1}(-Z),\]where the domain \(\mathbb{S}^{n}_{+}\setminus\mathbb{S}^{n}_{++}\) and the strict inequliquity \(\mathbf{tr}(S)<1\) in \((a)\) is due to the choice \(\rho>\mathbf{tr}(Z)\), and \((c)\) is a common form of \(\mathcal{N}_{\mathbb{S}^{n}_{-}}(-Z)\)[28, Complementary face in Section 3.3]. We argue \((b)\) is correct by verifying the following two sets are equivalent.

\[D_{1} =\{X\in\mathbb{S}^{n}_{+}\setminus\mathbb{S}^{n}_{++}\mid Z\in \rho P_{X}SP_{X}^{\mathsf{T}},S\in\mathbb{S}^{t}_{+},\mathbf{tr}(S)<1\},\] \[D_{2} =\left\{\begin{bmatrix}P_{1}&P_{2}\end{bmatrix}\begin{bmatrix}0&0 \\ 0&B\end{bmatrix}\begin{bmatrix}P_{1}^{\mathsf{T}}\\ P_{2}^{\mathsf{T}}\end{bmatrix}\mid 0\in\mathbb{R}^{r\times r},B\in\mathbb{S}^{n-r}_{+} \right\}.\]
* \((D_{1})\Rightarrow(D_{2}):\) Let \(X\in D_{1}\). Write \(X=P_{\alpha}0P_{\alpha}^{\mathsf{T}}+P_{\beta}\Lambda_{\beta}P_{\beta}^{ \mathsf{T}}\) with \(P_{\alpha}\in\mathbb{R}^{n\times t}\) and \(P_{\beta}\in\mathbb{R}^{n\times(n-t)}\). As \(\operatorname{span}(P_{1})\subseteq\operatorname{span}(P_{\alpha})\), we have \(\operatorname{span}(P_{\beta})\subseteq\operatorname{span}(P_{2})\). Thus, there exists \(U\in\mathbb{R}^{n\times(n-r)}\) such that \(U^{\mathsf{T}}U=I_{n-r}\) and \(P_{\beta}=P_{2}U\). We then have \[P_{\beta}\Lambda_{\beta}P_{\beta}^{\mathsf{T}}=P_{2}(U\Lambda_{\beta}U^{ \mathsf{T}})P_{2}^{\mathsf{T}}.\] This means \(X\in D_{2}\).
* \((D_{2})\Rightarrow(D_{1}):\) Let \(X\in D_{2}\). It is clear that \(\operatorname{span}(P_{1})\) is in the eigenspace of \(0\) of the matrix \(X\). Choosing \(S=\frac{\Lambda_{1}}{\rho}\in\mathbb{S}^{r}_{++}\), we can recover \(Z\) as \[\rho P_{1}\frac{\Lambda_{1}}{\rho}P_{1}^{\mathsf{T}}=Z.\] Also, \(\mathbf{tr}(S)=\mathbf{tr}(\Lambda_{1})/\rho<1\), so we conclude \(X\in D_{1}\). 

## Appendix C Growth properties of indicator and exact penalty functions in Lemma 3

One key step in the proof of Theorems 1 and 2 is the growth properties of the indicator function or the penalty function (10), summarized in (12) in Lemma 3. In this section, we complete its proof details. We first establish a lower bound of \(\left<\bar{Z},X\right>\) in terms of the distance \(\operatorname{dist}(X,\mathcal{N}_{(\mathbb{S}^{n}_{+})^{\circ}}(-\bar{Z}))\). Our proof uses some techniques in [27].

**Lemma C.1**.: _Let \(\bar{X},\bar{Z}\in\mathbb{S}^{n}_{+}\) satisfying \(\left<\bar{X},\bar{Z}\right>=0\) (i.e., both \(\bar{X}\) and \(\bar{Z}\) are on the boundary unless one of them is zero). For any positive \(\mu\in(0,\infty)\), there exists a constant \(\kappa>0\) such that_

\[\left<\bar{Z},X\right>\geq\kappa\cdot\operatorname{dist}^{2}\left(X,\mathcal{ N}_{(\mathbb{S}^{n}_{+})^{\circ}}(-\bar{Z})\right),\quad\forall X\in\mathbb{S}^{n}_ {+}\cap\mathbb{B}(\bar{X},\mu),\] (C.1)

_where \(\mathbb{B}(\bar{X},\mu)=\{X\in\mathbb{S}^{n}\mid\|X-\bar{X}\|\leq\mu\}\) denotes a closed ball with radius \(\mu\) and center \(\bar{X}\)._

Note that (C.1) is the reformulation of

\[0\geq 0-\left<\bar{Z},X\right>+\kappa\cdot\operatorname{dist}^{2}\left(X, \mathcal{N}_{(\mathbb{S}^{n}_{+})^{\circ}}(-\bar{Z})\right),\quad\forall X \in\mathbb{S}^{n}_{+}\cap\mathbb{B}(\bar{X},\mu).\] (C.2)

Indeed, (C.2) is the same as (12) in Lemma 3 when choosing \(l=\delta_{\mathbb{S}^{n}_{+}}\) (recall that we have \(\delta_{\mathbb{S}^{n}_{+}}(\bar{X})=0\), and the relationship in (11)). Therefore, Lemma 3 for the indicator function \(l=\delta_{\mathbb{S}^{n}_{+}}\) is a direct consequence of Lemma C.1. The proof of Lemma C.1 is given in Appendix C.1. In Appendix C.2, we will use Lemma C.1 to prove the growth property for the exact penalty function \(l\) in (10).

### Proof of Lemma C.1

We only need to consider the case where \(\bar{Z}\neq 0\), since the case \(\bar{Z}=0\) is true by observing that

\[\left<X,\bar{Z}\right>=0,\ \operatorname{dist}(X,\mathcal{N}_{(\mathbb{S}^{n}_{+}) ^{\circ}}(0))=\operatorname{dist}(X,\mathbb{S}^{n}_{+})=0,\quad\forall X\in \mathbb{S}^{n}_{+}\cap\mathbb{B}(\bar{X},\mu).\]

We first introduce the following useful inequality for positive semidefinite matrices.

**Lemma C.2**.: _Suppose \(\begin{bmatrix}A&B\\ B^{\mathsf{T}}&D\end{bmatrix}\in\mathbb{S}^{n}_{+}\). Then \(\|D\|_{\operatorname{op}}\,\mathbf{tr}(A)\geq\|B\|^{2}\)._

We now proceed with the proof. Fix a \(\mu>0\). Consider \(\bar{X},\bar{Z}\in\mathbb{S}^{n}_{+}\) and \(\left<\bar{X},\bar{Z}\right>=0\). Let \(X\in\mathbb{B}(\bar{X},\mu)\cap\mathbb{S}^{n}_{+}\) and write

\[\bar{Z}=[P_{1}\quad P_{2}]\begin{bmatrix}\Lambda_{1}&0\\ 0&0\end{bmatrix}\begin{bmatrix}P_{1}^{\mathsf{T}}\\ P_{2}^{\mathsf{T}}\end{bmatrix},\]where \(\Lambda_{1}\in\mathbb{S}_{+}^{r}\) is a diagonal matrix containing the positive eigenvalues of \(\bar{Z}\). Let \(P=[P_{1}\ P_{2}]\) be the orthonormal eigenvectors matrix. The projection of \(X\) onto \(\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(-\bar{Z})\) can be verified as

\[P\begin{bmatrix}0&0\\ 0&P_{2}^{\mathsf{T}}XP_{2}\end{bmatrix}P^{\mathsf{T}}=\operatorname*{argmin}_{Y \in\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(-\bar{Z})}\|X-Y\|^{2}.\]

Hence, the distance \(\operatorname{dist}^{2}(X,\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(-\bar{Z}))\) can be computed as

\[\begin{split}\operatorname{dist}^{2}(X,\mathcal{N}_{(\mathbb{S}_ {+}^{n})^{\circ}}(-\bar{Z}))&=\left\|X-P\begin{bmatrix}0&0\\ 0&P_{2}^{\mathsf{T}}XP_{2}\end{bmatrix}P^{\mathsf{T}}\right\|^{2}\\ &=\left\|\begin{bmatrix}P_{1}^{\mathsf{T}}XP_{1}&P_{1}^{\mathsf{T}} XP_{2}\\ P_{2}^{\mathsf{T}}XP_{1}&0\end{bmatrix}\right\|^{2}=\|P_{1}^{\mathsf{T}}XP_{1} \|^{2}+2\|P_{1}^{\mathsf{T}}XP_{2}\|^{2},\end{split}\] (C.3)

where the second equality uses the unitary invariance. It remains to estimate the terms \(\|P_{1}^{\mathsf{T}}XP_{1}\|^{2}\) and \(\|P_{1}^{\mathsf{T}}XP_{2}\|^{2}\).

By the assumption \(X\in\mathbb{B}(\bar{X},\mu)\cap\mathbb{S}_{+}^{n}\) and unitary invariance, we have

\[\begin{split}\mu\geq\|X-\bar{X}\|=\|P^{\mathsf{T}}(X-\bar{X})P\| &\geq\max\{\|P_{1}^{\mathsf{T}}(X-\bar{X})P_{1}\|,\|P_{2}^{\mathsf{T} }(X-\bar{X})P_{2}\|\}\\ &\geq\max\{\|P_{1}^{\mathsf{T}}XP_{1}\|_{\mathrm{op}},\|P_{2}^{ \mathsf{T}}(X-\bar{X})P_{2}\|_{\mathrm{op}}\},\end{split}\] (C.4)

where the last inequality uses the fact that \(P_{1}^{\mathsf{T}}\bar{X}P_{1}=0\) as \(\left\langle\bar{X},\bar{Z}\right\rangle=0\) and \(\bar{X},\bar{Z}\in\mathbb{S}_{+}^{n}\), and the relationship \(\|\cdot\|\geq\|\cdot\|_{\mathrm{op}}\). It follows that

\[\|P_{1}^{\mathsf{T}}XP_{1}\|^{2}=\left\langle P_{1}^{\mathsf{T}}XP_{1},P_{1} ^{\mathsf{T}}XP_{1}\right\rangle\leq\|P_{1}^{\mathsf{T}}XP_{1}\|_{\mathrm{op }}\|P_{1}^{\mathsf{T}}XP_{1}\|_{*}\leq\mu\cdot\mathbf{tr}(P_{1}^{\mathsf{T}} XP_{1}),\] (C.5)

where the second inequality uses the generalized Cauchy-Schwarz inequality (\(\|\cdot\|_{*}\) and \(\|\cdot\|_{\mathrm{op}}\) norms are dual to each other; we have \(|\left\langle X,Z\right\rangle|\leq\|X\|_{*}\|Z\|_{\mathrm{op}},\forall X,Z \in\mathbb{S}^{n}\)), and the last inequality uses (C.4) and \(\|P_{1}^{\mathsf{T}}XP_{1}\|_{*}=\mathbf{tr}(P_{1}^{\mathsf{T}}XP_{1})\) as \(X\in\mathbb{S}_{+}^{n}\).

On the other hand, applying Lemma C.2 on \(P^{\mathsf{T}}XP\) yields

\[\|P_{1}^{\mathsf{T}}XP_{2}\|^{2}\leq\|P_{2}^{\mathsf{T}}XP_{2}\|_{\mathrm{op }}\,\mathbf{tr}(P_{1}^{\mathsf{T}}XP_{1})\leq(\mu+\|\bar{X}\|)\cdot\mathbf{ tr}(P_{1}^{\mathsf{T}}XP_{1}),\] (C.6)

where the last inequality is due to (C.4). Moreover, we have

\[\left\langle\bar{Z},X\right\rangle=\left\langle P_{1}\Lambda_{1}P_{1}^{ \mathsf{T}},X\right\rangle=\left\langle\Lambda_{1},P_{1}^{\mathsf{T}}XP_{1} \right\rangle\geq\lambda_{\min}(\Lambda_{1})\,\mathbf{tr}(P_{1}^{\mathsf{T}} XP_{1}),\] (C.7)

where \(\lambda_{\min}(\Lambda_{1})>0\). Putting (C.3), (C.5) and (C.6) together and choosing \(\kappa=\frac{\lambda_{\min}(\Lambda_{1})}{3\mu+2\|X\|}\) gives us

\[\kappa\cdot\operatorname{dist}^{2}(X,\mathcal{N}_{(\mathbb{S}_{+}^{n})^{ \circ}}(-\bar{Z}))\leq\kappa(3\mu+2\|\bar{X}\|)\,\mathbf{tr}(P_{1}^{\mathsf{T }}XP_{1})\leq\left\langle\bar{Z},X\right\rangle,\]

where the last inequality is due to (C.7). This completes the proof.

### Growth property of the exact penalty function \(l\) in (10)

Throughout this section, we fix a pair of matrices \((\bar{X},\bar{Z})\in\mathbb{S}_{+}^{n}\times\mathbb{S}_{+}^{n}\) satisfying \(\left\langle\bar{X},\bar{Z}\right\rangle=0\). We need to prove for any positive value \(\mu\in(0,\infty)\), there exists a positive constant \(\kappa>0\) such that

\[l(X)\geq l(\bar{X})+\left\langle-\bar{Z},X-\bar{X}\right\rangle+\kappa\cdot \operatorname{dist}^{2}\left(X,(\partial l)^{-1}(-\bar{Z})\right),\quad\forall X \in\mathbb{B}(\bar{X},\mu).\] (C.8)

Note that it is equivalent to showing that there exists a \(\kappa>0\) such that

\[l(X)\geq\underbrace{\left\langle-\bar{Z},X\right\rangle}_{R_{1}}+\kappa \cdot\underbrace{\operatorname{dist}^{p}\Big{(}X,\mathcal{N}_{(\mathbb{S}_{+}^{n}) ^{\circ}}(-\bar{Z})\Big{)}}_{R_{2}},\quad\forall X\in\mathbb{B}(\bar{X},\mu)\] (C.9)

as \(l(\bar{X})=0\) and \(\left\langle\bar{Z},\bar{X}\right\rangle=0\) by assumption. This result (C.9) has already applied a non-trivial fact \((\partial l)^{-1}(-\bar{Z})=(\mathcal{N}_{\mathbb{S}_{+}^{n}})^{-1}(-\bar{Z})= \mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(-\bar{Z})\) for the exact penalty function (10), which has been established in Proposition B.1.

In the following, we will bound the terms \(R_{1}\) and \(R_{2}\) in (C.9). The term \(R_{2}\) will rely on Lemma C.1. One key difference between (C.9) and (C.2) is that the penalty function \(l(X)<\infty\) when \(X\notin\mathbb{S}_{+}^{n}\), while \(\delta_{\mathbb{S}_{+}^{n}}(X)=\infty,\forall X\notin\mathbb{S}_{+}^{n}\).

Let us fix a \(\mu>0\), consider \(X\in\mathbb{B}(\bar{X},\mu)\), and write \(\rho=\delta+\mathbf{tr}(\bar{Z})\). We first consider the case when \(\bar{Z}=0\). Note that \(\left\langle\bar{Z},X\right\rangle=0\) and \(\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(0)=\mathbb{S}_{+}^{n}\). The distance \(\mathrm{dist}(X,\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(0))\) can be upper bounded as

\[\mathrm{dist}^{2}(X,\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(0))=\|X-\Pi_{ \mathbb{S}_{+}^{n}}(X)\|^{2}\leq n\max\{0,\lambda_{\max}(-X)\}^{2}\leq n\mu \max\{0,\lambda_{\max}(-X)\}.\]

Choosing \(\kappa=\frac{\rho}{n\mu}\) yields

\[\left\langle-Z^{\star},X\right\rangle+\kappa\cdot\mathrm{dist}^{2}(X, \mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(0))\leq\rho\max\{0,\lambda_{\max}( -X)\}=l(X),\quad\forall X\in\mathbb{B}(\bar{X},\mu).\]

We then consider the case \(\bar{Z}\neq 0\).

* We first bound \(\mathsf{R}_{1}=\left\langle-\bar{Z},X\right\rangle\): \[\left\langle-\bar{Z},X\right\rangle =\left\langle\bar{Z},\Pi_{\mathbb{S}_{+}^{n}}(X)-X\right\rangle- \left\langle\bar{Z},\Pi_{\mathbb{S}_{+}^{n}}(X)\right\rangle\] \[\leq\|\bar{Z}\|_{*}\|\Pi_{\mathbb{S}_{+}^{n}}(X)-X\|_{\mathrm{op} }-\left\langle\bar{Z},\Pi_{\mathbb{S}_{+}^{n}}(X)\right\rangle\] \[=\mathbf{tr}(\bar{Z})\max\{0,\lambda_{\max}(-X)\}-\left\langle \bar{Z},\Pi_{\mathbb{S}_{+}^{n}}(X)\right\rangle,\] (C.10) where the inequality applies the general Cauchy-Schwarz inequality (\(\|\cdot\|_{*}\) and \(\|\cdot\|_{\mathrm{op}}\) norms are dual to each other; we have \(|\left\langle X,Z\right\rangle|\leq\|X\|_{*}\|Z\|_{\mathrm{op}},\forall X,Z \in\mathbb{S}^{n}\)).
* We then bound \(\mathsf{R}_{2}=\mathrm{dist}^{2}(X,\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}} (-\bar{Z}))\): Let \(Y\) be the projection of \(\Pi_{\mathbb{S}_{+}^{n}}(X)\) on to \(\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(-\bar{Z})\), i.e., \(Y=\mathrm{argmin}_{V\in\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(-\bar{Z})}\)\(\|\Pi_{\mathbb{S}_{+}^{n}}(X)-V\|\). We then have \[\mathrm{dist}^{2}\left(X,\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(-\bar{Z}) \right)\leq\|X-Y\|^{2}\leq 2\|X-\Pi_{\mathbb{S}_{+}^{n}}(X)\|^{2}+2\|\Pi_{ \mathbb{S}_{+}^{n}}(X)-Y\|^{2}.\] (C.11) The first term \(\|X-\Pi_{\mathbb{S}_{+}^{n}}(X)\|^{2}\) can be bounded by \[\|X-\Pi_{\mathbb{S}_{+}^{n}}(X)\|^{2}\leq n\max\{0,\lambda_{\max}(-X)\}^{2} \leq n\mu\max\{0,\lambda_{\max}(-X)\},\] where the last inequality comes from \(|\lambda_{\min}(X)-\lambda_{\min}(\bar{X})|=|\lambda_{\max}(-X)|\leq\mu\) as \(\lambda_{\min}(\bar{X})=0\) and \(\|X-\bar{X}\|\leq\mu\). The second term \(\|\Pi_{\mathbb{S}_{+}^{n}}(X)-Y\|^{2}\) can be bounded by Lemma C.1, i.e., there is a \(\kappa^{\prime}>0\) such that \[\|\Pi_{\mathbb{S}_{+}^{n}}(X)-Y\|^{2}\leq\frac{1}{\kappa^{\prime}}\left\langle \Pi_{\mathbb{S}_{+}^{n}}(X),\bar{Z}\right\rangle.\] Putting everything together and choosing \(\kappa=\min\{\frac{\delta}{2n\mu},\frac{\kappa^{\prime}}{2}\}\) yields \[\left\langle-\bar{Z},X\right\rangle+\kappa\cdot\mathrm{dist}^{2} \left(X,\mathcal{N}_{(\mathbb{S}_{+}^{n})^{\circ}}(-\bar{Z})\right)\] \[\leq\,\mathbf{tr}(\bar{Z})\max\{0,\lambda_{\max}(-X)\}-\left\langle \bar{Z},\Pi_{\mathbb{S}_{+}^{n}}(X)\right\rangle\] \[\qquad+\kappa 2n\mu\max\{0,\lambda_{\max}(-X)\}+\frac{2\kappa}{ \kappa^{\prime}}\cdot\left\langle\Pi_{\mathbb{S}_{+}^{n}}(X),\bar{Z}\right\rangle\] \[\leq\,\mathbf{tr}(\bar{Z})\max\{0,\lambda_{\max}(-X)\}+\delta \max\{0,\lambda_{\max}(-X)\}\] \[=\rho\max\{0,\lambda_{\max}(-X)\}=l(X),\quad\forall X\in\mathbb{B }(\bar{X},\mu).\]

## Appendix D Growth growth under strict complementarity

As mentioned in the main text, the key step to prove Theorems 1 and 2 is to establish the growth property (12) in Lemma 3 which we have proved in Appendix C. Once Lemma 3 is established, with strict complementarity assumption, we can deduce Proposition 1. In this appendix, we first provide proof for Proposition 1 in Appendix D.1 and use a simple example to illustrate the quadratic growth property in Appendix D.2.

### Proof of Proposition 1

The following proof is adapted from [27]. We first prove the primal case. Fix a \(\mu>0\) and \(X^{\star}\in\Omega_{\mathrm{P}}\). As the problem is convex, a pair of primal and dual solutions \((\hat{X}^{\star},\hat{y}^{\star},\hat{Z}^{\star})\) solves (4) and its dual problem if and only if the following KKT system holds

\[\mathcal{A}(\hat{X}^{\star})=b,\ \hat{Z}^{\star}=C-\mathcal{A}^{\star}(\hat{y}^{ \star}),\ 0\in\hat{Z}^{\star}+\partial g(\hat{X}^{\star}).\]

Combined with the fact that any primal and dual solution forms a pair, the optimal primal solution \(\Omega_{\mathrm{P}}\) can be characterized as

\[\Omega_{\mathrm{P}}=\mathcal{X}_{0}\cap(\partial g)^{-1}(-\hat{Z}^{\star}), \quad\forall(\hat{y}^{\star},\hat{Z}^{\star})\in\Omega_{\mathrm{D}},\]

where \(\mathcal{X}_{0}=\{X\in\mathbb{S}^{n}\mid\mathcal{A}(X)=b\}\). Let \((\bar{X}^{\star},\bar{y}^{\star},\bar{Z}^{\star})\) be a pair of primal and dual solutions satisfying \(\bar{X}^{\star}\in\mathrm{relint}((\partial g)^{-1}(-\bar{Z}^{\star}))\) (strict complementarity assumption). There exist \(\alpha_{1},\kappa_{1},\kappa_{2}>0\) such that for all \(X\in\mathbb{B}(X^{\star},\mu)\) the following holds

\[\begin{split}\mathrm{dist}^{2}(X,\Omega_{\mathrm{P}})& =\mathrm{dist}^{2}(X,\mathcal{X}_{0}\cap(\partial g)^{-1}(-\bar{Z}^ {\star}))\\ &\leq\alpha_{1}(\mathrm{dist}(X,\mathcal{X}_{0})+\mathrm{dist}(X,( \partial g)^{-1}(-\bar{Z}^{\star})))^{2}\\ &\leq\kappa_{1}(\mathrm{dist}^{2}(X,\mathcal{X}_{0})+\mathrm{dist }^{2}(X,(\partial g)^{-1}(-\bar{Z}^{\star})))\\ &\leq\kappa_{2}(\|\mathcal{A}(X)-b\|^{2}+\mathrm{dist}^{2}(X,( \partial g)^{-1}(-\bar{Z}^{\star}))),\end{split}\] (D.1)

where the first inequality applies Lemma A.3, the second inequality applies \((A+B)^{2}\leq 2A^{2}+2B^{2}\) for all \(A,B\geq 0\), the third inequality uses the fact that \(\mathcal{X}_{0}\) is an affine space. On the other hand, we know \(\left\langle\hat{X}^{\star},\bar{Z}^{\star}\right\rangle=0\) for all \(\hat{X}^{\star}\in\Omega_{\mathrm{P}}\) naturally, so Lemma 3 can be applied here on \(Z^{\star}\) and \(\bar{Z}^{\star}\) (the proof of Lemma 3 is provided in Appendix C.2). For all \(X\in\mathbb{B}(X^{\star},\mu)\) it holds that

\[\begin{split} f_{\mathrm{P}}(X)&=\langle C,X\rangle +g(X)\\ &\geq\langle C,X\rangle+g(X^{\star})+\langle\mathcal{A}^{\star}( \bar{y}^{\star})-C,X-X^{\star}\rangle+\kappa\cdot\mathrm{dist}^{2}(X,(\partial g )^{-1}(-\bar{Z}^{\star}))\\ &\geq\langle C,X\rangle+g(X^{\star})+\langle\mathcal{A}^{\star}( \bar{y}^{\star})-C,X-X^{\star}\rangle+\frac{\kappa}{\kappa_{2}}\mathrm{dist}^ {2}(X,\Omega_{\mathrm{P}})-\kappa\|\mathcal{A}(X)-b\|^{2}\\ &\geq f_{\mathrm{P}}(X^{\star})-\|\bar{y}^{\star}\|\|\mathcal{A} (X)-b\|-\kappa\|\mathcal{A}(X)-b\|^{2}+\frac{\kappa}{\kappa_{2}}\cdot\mathrm{ dist}^{2}(X,\Omega_{\mathrm{P}})\\ &=f_{\mathrm{P}}(X^{\star})-(\|\bar{y}^{\star}\|+\kappa\| \mathcal{A}(X)-b\|)\|\mathcal{A}(X)-b\|+\frac{\kappa}{\kappa_{2}}\cdot\mathrm{ dist}^{2}(X,\Omega_{\mathrm{P}})\\ &\geq f_{\mathrm{P}}(X^{\star})-(\|\bar{y}^{\star}\|+\gamma)\| \mathcal{A}(X)-b\|+\frac{\kappa}{\kappa_{2}}\cdot\mathrm{dist}^{2}(X,\Omega_ {\mathrm{P}}),\end{split}\]

where the first inequality uses the growth property on \(g\) for \(X^{\star}\) and \(\bar{Z}^{\star}\) (12), the second inequality uses (D.1), the third inequality uses the definition of the adjoint operator and Cauchy-Schwarz inequality, and the last inequality uses the fact that \(X\) is in a bounded set \(\mathbb{B}(X^{\star},\mu)\) so there exists some \(\gamma\geq 0\) such that \(\kappa\|\mathcal{A}(X)-b\|\leq\gamma\). This completes the proof.

We move on to prove the dual case. Similarly, we let \((y^{\star},Z^{\star})\in\Omega_{\mathrm{D}}\) and \(\mu>0\) and characterize the optimal dual solution as

\[\Omega_{\mathrm{D}}=\mathcal{Z}_{0}\cap(\mathbb{R}^{m}\times(\partial h)^{-1 }(-\hat{X}^{\star})),\quad\forall\hat{X}^{\star}\in\Omega_{\mathrm{P}},\]

where \(\mathcal{Z}_{0}=\{(y,Z)\in\mathbb{R}^{m}\times\mathbb{S}^{n}\mid C-\mathcal{ A}^{\star}(y)=Z\}\). Let \((\bar{X}^{\star},\bar{y}^{\star},\bar{Z}^{\star})\) be a pair of primal and dual solution satisfying \(\bar{Z}^{\star}\in\mathrm{relint}((\partial h)^{-1}(-\bar{X}^{\star}))\). Note that

\[\mathrm{relint}(\mathbb{R}^{m}\times(\partial h)^{-1}(-\bar{X}^{\star}))= \mathbb{R}^{m}\times\mathrm{relint}((\partial h)^{-1}(-\bar{X}^{\star})).\]

Therefore, the existence of such a pair of strict complementarity solution implies

\[\mathcal{Z}_{0}\cap\mathrm{relint}(\mathbb{R}^{m}\times(\partial h)^{-1}(-\bar{ X}^{\star}))\neq\emptyset.\]

Hence, following the same argument to (D.1), we know there exists a constant \(\kappa_{3}>0\) such that

\[\mathrm{dist}^{2}((y,Z),\Omega_{\mathrm{D}})\leq\kappa_{3}(\|C-\mathcal{A}^{ \star}(y)-Z\|^{2}+\mathrm{dist}^{2}((y,Z),(\partial h)^{-1}(-\bar{X}^{\star})))\] (D.2)for all \((y,Z)\in\mathbb{B}((y^{\star},Z^{\star}),\mu)\). Also, as \(\left\langle\bar{X}^{\star},Z^{\star}\right\rangle=0\), we have

\[f_{\mathrm{D}}(y,Z) =-\left\langle b,y\right\rangle+h(Z)\] \[\geq-\left\langle b,y\right\rangle+h(Z^{\star})+\left\langle-X^{ \star},Z-Z^{\star}\right\rangle+\kappa\cdot\mathrm{dist}^{2}((y,Z),(\partial h )^{-1}(-\bar{X}^{\star}))\] \[=h(Z^{\star})+\left\langle-\bar{X}^{\star},\mathcal{A}^{\star}(y )-Z-C+\mathcal{A}^{\star}(y^{\star})\right\rangle+\kappa\cdot\mathrm{dist}^{2} ((y,Z),(\partial h)^{-1}(-\bar{X}^{\star}))\] \[\geq f_{\mathrm{D}}(y^{\star},Z^{\star})-\|\bar{X}^{\star}\|\|C- \mathcal{A}^{\star}(y)-Z\|-\kappa\|C-\mathcal{A}^{\star}(y)-Z\|^{2}+\frac{ \kappa}{\kappa_{3}}\cdot\mathrm{dist}^{2}((y,Z),\Omega_{\mathrm{D}})\] \[=f_{\mathrm{D}}(y^{\star},Z^{\star})-(\|\bar{X}^{\star}\|+\kappa \|C-\mathcal{A}^{\star}(y)-Z\|)\|C-\mathcal{A}^{\star}(y)-Z\|+\frac{\kappa}{ \kappa_{3}}\cdot\mathrm{dist}^{2}((y,Z),\Omega_{\mathrm{D}})\] \[\geq f_{\mathrm{D}}(y^{\star},Z^{\star})-(\|\bar{X}^{\star}\|+ \gamma)\|C-\mathcal{A}^{\star}(y)-Z\|+\frac{\kappa}{\kappa_{3}}\cdot\mathrm{ dist}^{2}((y,Z),\Omega_{\mathrm{D}}),\]

where the first inequality uses the growth property on \(h\), the second equality rewrites \(\left\langle b,y\right\rangle=\left\langle\bar{X}^{\star},\mathcal{A}^{\star} (y)\right\rangle\) and \(Z^{\star}=C-\mathcal{A}^{\star}(y^{\star})\), and the last inequality uses the fact that \((y,Z)\) is in a bounded set \(\mathbb{B}((y^{\star},Z^{\star}),\mu)\) so there exists a \(\gamma\geq 0\) such that \(\kappa\|C-\mathcal{A}^{\star}(y)-Z\|\leq\gamma\).

### Illustration of quadratic growth

In this subsection, we use a simple example to illustrate the quadratic growth property discussed in (9). In particular, we choose the exact penalty formulation for dual SDP (5) (the proof for the exact penalty function can be found in Appendix B). Moreover, the following example shows that even a simple \(2\times 2\) SDP _can not_ have _sharp_ growth in the objective function.

**Example D.1**.: _Consider a SDP (P) and (D) with the problem data_

\[C=\begin{bmatrix}1&-1\\ -1&1\end{bmatrix},A_{1}=\begin{bmatrix}1&0\\ 0&0\end{bmatrix},A_{2}=\begin{bmatrix}0&0\\ 0&1\end{bmatrix},\text{ and }b=\begin{bmatrix}1\\ 1\end{bmatrix}.\]

_It can be verified that both primal and dual SDP have a unique solution_

\[X^{\star}=\begin{bmatrix}1&1\\ 1&1\end{bmatrix},y^{\star}=\begin{bmatrix}0\\ 0\end{bmatrix},\text{ and }Z^{\star}=\begin{bmatrix}1&-1\\ -1&1\end{bmatrix}.\]

_Therefore, the optimal cost value is \(0\). Moreover, it also satisfies strict complementarity as \(\mathrm{rank}(X^{\star})+\mathrm{rank}(Z^{\star})=1+1=2\) (see Definition A.1), (8b) and (9b) then ensure the following exact penalized formulation having quadratic growth property_

\[f(y):=-b^{\mathsf{T}}y+\rho\max\{0,\lambda_{\max}(\mathcal{A}^{\star}(y)-C)\},\]

_where \(\rho\) can be chosen as any number satisfying \(\rho>\mathbf{tr}(Z^{\star})=2\). Let \(S=\{(0,0)\}\) be the optimal dual solution set and \(f^{\star}=0\) be the optimal cost value. Figure (a)a shows the landscape of the function \(f\) with \(\rho=4\) and verifies that the function value \(f(y)\) is growing at least quadratically away from the distance to the solution set \(S=\{(0,0)\}\) with the quadratic constant \(\kappa=0.3\). In Figure (a)a, the yellow area indicates the part where only the linear component of \(f(y)\), i.e. \(b^{\mathsf{T}}y\), is active while the nonlinear penalty component \(\rho\max\{0,\lambda_{\max}(\mathcal{A}^{\star}(y)-C)\}\) is \(0\), and the blue region is the part where both the linear and nonlinear components of \(f(y)\) are active. The black line in Figure (a)a is the boundary where \(\lambda_{\max}(\mathcal{A}^{\star}(y)-C)=0\) which can be characterized by the nonlinear equation \(y_{1}y_{2}-y_{1}-y_{2}=0\) with \(y_{1}<1\). This nonlinear direction also indicates that sharp growth of \(f\) is impossible. Indeed, let \(y_{1}\in[0,1)\) and \(y_{2}=\frac{y_{1}}{y_{1}-1}\). Note that this choice of \(y\) satisfies the nonlinear equation \(y_{1}y_{2}-y_{1}-y_{2}=0\) and thus only the linear part \(-b^{\mathsf{T}}y\) in \(f(y)\) is in force. We then have_

\[f(y)-f^{\star}=-y_{1}-y_{2}=-y_{1}-\frac{y_{1}}{y_{1}-1}=\frac{-y_{1}^{2}}{y_{1 }-1}.\]

_On the other hand, the distance to the solution set can be verified as_

\[\mathrm{dist}(y,S)=\sqrt{y_{1}^{2}+\frac{y_{1}^{2}}{(y_{1}-1)^{2}}}=\left| \frac{y_{1}\sqrt{1+(y_{1}-1)^{2}}}{y_{1}-1}\right|\geq\left|\frac{y_{1}}{y_{1 }-1}\right|.\]

_It becomes clear that it is impossible to have a constant \(\kappa>0\) such that_

\[\kappa\left|\frac{y_{1}}{y_{1}-1}\right|\leq\frac{-y_{1}^{2}}{y_{1}-1}=f(y)-f ^{\star},\quad\forall y_{1}\in[0,1)\]as the quadratic therm \(y_{1}^{2}\) will dominant when \(y_{1}\) is small, let alone the sharp growth \(\kappa\cdot\mathrm{dist}(y,S)\leq f(y)-f^{\star}\). Moreover, the quadratic growth of indicator function formulation \(-b^{\mathsf{T}}y+\delta_{\mathbb{S}_{+}^{2}}(C-\mathcal{A}^{*}(y))\) has also been verified as we only need to consider the linear component \(-b^{\mathsf{T}}y\), which has already been confirmed in the previous case. Lastly, it is worth pointing out that the sublevel sets of the functions \(f\) and \(-b^{\mathsf{T}}y+\delta_{\mathbb{S}_{+}^{2}}(C-\mathcal{A}^{*}(y))\) are different. Indeed, given a finite value \(\beta>0\), the sublevel set \(\{y\in\mathbb{R}^{2}\mid f(y)\leq\beta\}\) contains both parts of yellow and blue regions in Figure 1(b). In contrast, the sublevel set \(\{y\in\mathbb{R}^{2}\mid-b^{\mathsf{T}}y+\delta_{\mathbb{S}_{+}^{2}}(C- \mathcal{A}^{*}(y))\leq\beta\}\) only contains the linear part in Figure 1(b)._

## Appendix E Proofs and supplemental discussions in Section 4

In this section, we complete the missing proof in Section 4 and discuss the ALM applied to (D) with its corresponding convergence properties. This section is divided into three subsections. Appendix E.1 completes the proof of Proposition 2. Appendix E.2 finishes the proof of Theorem 3. Appendix E.3 discusses the ALM applied to (D).

### Proof of Proposition 2

The proof of Proposition 2 requires a nice characterization of the ordinal dual function and the augmented dual function defined as

\[g_{r}(w)=\inf_{X\in\mathbb{S}^{n}}L_{r}(X,w)\] (E.1)

where \(L_{r}\) is the augmented Lagrangian function (14), recall that we write \(w=(y,Z)\).

**Proposition E.1** ([38, Theorem 3.2]).: _Let \(r>0\). For all \(w\in\mathbb{R}^{m}\times\mathbb{S}^{n}\), it holds that_

\[g_{r}(w)=\min_{X\in\mathbb{S}^{n}}\ L_{r}(X,w)=\max_{u\in\mathbb{R}^{m}\times \mathbb{S}^{n}}\ g_{0}(u)-\frac{1}{2r}\|u-w\|^{2},\]

_where \(g_{0}\) is the dual function in (13)._

Proposition E.1 not only shows the augmented dual function \(g_{r}\) is \(g_{0}\) with a prior but also shows \(g_{r}\) is continuously differentiable. In comparison, the ordinary dual function \(g_{0}\) can be highly nonsmooth. We are ready to prove Proposition 2.

Figure 2: The quadratic growth property of the exact penalty function \(f(y)=-b^{\mathsf{T}}y+\rho\max\{0,\lambda_{\max}(\mathcal{A}^{*}(y)-C)\}\) where \(\rho=4\) and \(f^{\star}=0\). The optimal solution set \(S=\{(0,0)\}\). In Figure 1(a), the yellow region represents the linear part where only \(-b^{\mathsf{T}}y\) is active, resulting \(\rho\max\{0,\lambda_{\max}(\mathcal{A}^{*}(y)-C)\}=0\), the blue region encompasses both the linear and the nonlinear parts, and the green surf is the square of the distance to the optimal solution set with \(\kappa=0.3\). Figure 1(b) shows the sectional view of \(f\) along the direction \(y_{1}y_{2}-y_{1}-y_{2}=0\), which can be characterized as the rational function \(f(y_{1})=\frac{-y_{1}^{2}}{y_{1}-1}\).

* Part (a) of Proposition 2 comes directly from the PPM convergence Lemma 1 as Lemma 4 and the stopping criteria (A\({}^{\prime}\)) naturally imply the dual iterate \(w_{k+1}\) satisfies \(\|w_{k+1}-\mathrm{prox}_{r_{k},-g_{0}}(w_{k})\|\leq\epsilon_{k}\) and \(\sum_{k=1}^{\infty}\epsilon_{k}<\infty\).
* The proof of (b) in Proposition 2 is in fact straightforward. Firstly, by Moreau decomposition [49, Exercise 12.22] and (15c), we have \[r_{k}X_{k+1}-Z_{k}=\Pi_{\mathbb{S}_{+}^{n}}(r_{k}X_{k+1}-Z_{k})-\Pi_{\mathbb{S }_{+}^{n}}(Z_{k}-r_{k}X_{k+1})=\Pi_{\mathbb{S}_{+}^{n}}(r_{k}X_{k+1}-Z_{k})-Z_ {k+1}.\] Diving both sides by \(r_{k}\), we conclude \(X_{k+1}+(Z_{k+1}-Z_{k})/r_{k}\in\mathbb{S}_{+}^{n}\). Therefore, it follows that \[\mathrm{dist}(X_{k+1},\mathbb{S}_{+}^{n})\leq\|X_{k+1}-(X_{k+1}+(Z_{k+1}-Z_{k} )/r_{k})\|=\|Z_{k+1}-Z_{k}\|/r_{k}.\] Secondly, \(\|\mathcal{A}(X_{k+1})-b\|=\|y_{k}-y_{k+1}\|/r_{k}\) comes directly from the update (15b). Lastly, note that \[L_{r_{k}}(X_{k+1},w_{k})=\langle C,X_{k+1}\rangle+\frac{1}{2r_{k}}(\|w_{k+1} \|^{2}-\|w_{k}\|^{2}).\] On the other hand, we have \[\min_{X\in\mathbb{S}^{n}}L_{r_{k}}(X,w_{k})=g_{r_{k}}(w_{k})=g_{0}(u^{\star})- \frac{1}{2r_{k}}\|u^{\star}-w_{k}\|^{2}\leq g_{0}(u^{\star})\leq p^{\star},\] where \(u^{\star}\) is the point that achieves the maximum in Proposition E.1 and the last inequality uses weakly duality. Therefore, we have \[\langle C,X_{k+1}\rangle-p^{\star}\leq L_{r_{k}}(X_{k+1},w_{k})-\min_{X\in \mathbb{S}^{n}}L_{r_{k}}(X,w_{k})-\frac{1}{2r_{k}}(\|w_{k+1}\|^{2}-\|w_{k}\|^ {2}).\] Since the convergence of \(\{w_{k}\}\) is guaranteed in (a), the quantities \(\mathrm{dist}(X_{k+1},\mathbb{S}_{+}^{n}),\|\mathcal{A}(X_{k+1})-b\|,\) and \(\langle C,X_{k+1}\rangle-p^{\star}\) also convergence to zero.
* Part (c) is a direct consequence of part (b) and the fact that \(\Omega_{\mathrm{P}}\) is bounded if and only if the set \(\{X\in\mathbb{S}^{n}\mid\mathrm{dist}(X,\mathbb{S}_{+}^{n})\leq\gamma_{1},\| \mathcal{A}(X)-b\|\leq\gamma_{2},\langle C,X\rangle-p^{\star}\leq\gamma_{3}\}\) is bounded for any \(\gamma\in\mathbb{R}^{3}\)[23, page 110].

### Proof of Theorem 3

In this subsection, we provide a complete proof of part (a) in Theorem 3. The proof is motivated by [26, Theorem 1] which focuses on dual SDPs only. To facilitate proof, we first introduce a standard result in proximal mapping that upper bounds the step length of the proximal step by the distance to the optimal solution set, which can be found in [42, Proposition 1] by setting \(z=x\) and \(z^{\prime}=\Pi_{S}(x)\). We give a simple proof below.

**Proposition E.2**.: _Let \(f:\mathbb{R}^{n}\to\bar{\mathbb{R}}\) be a convex function. Denote \(S\subseteq\mathbb{R}^{n}\) as the set of minimizers of \(f\), i.e., \(S=\mathrm{argmin}_{x\in\mathbb{R}^{n}}\,f(x)\). Suppose \(S\neq 0\). Given a point \(x\in\mathbb{R}^{n}\) and a constant \(\alpha>0\), the proximal mapping holds that_

\[\|\mathrm{prox}_{\alpha,f}(x)-x\|\leq\mathrm{dist}(x,S).\]

Proof.: From the optimality condition of proximal mapping, we have

\[f(\mathrm{prox}_{\alpha,f}(x))+\frac{\alpha}{2}\|\mathrm{prox}_{ \alpha,f}(x)-x\|^{2}\leq f(\Pi_{S}(x))+\frac{\alpha}{2}\|\Pi_{S}(x)-x\|^{2}\] \[\Longrightarrow\frac{\alpha}{2}\|\mathrm{prox}_{\alpha,f}(x)-x\| ^{2}\leq f(\Pi_{S}(x))-f(\mathrm{prox}_{\alpha,f}(x))+\frac{\alpha}{2}\|\Pi_{S} (x)-x\|^{2}\] \[\Longrightarrow\|\mathrm{prox}_{\alpha,f}(x)-x\|\leq\mathrm{dist} (x,S),\]

which completes the proof. 

Using Proposition E.2 and (B\({}^{\prime}\)), we arrive at the following key inequality to show the linear convergence of KKT residuals. Proposition 2. It bounds the step length of the dual update by the current distance to the solution set.

**Proposition E.3**.: _[_26_, Lemma 3]_ _Let \(\{X_{k},w_{k}\}\) be the sequence generated by (15) under (B\({}^{\prime}\)). Then for all \(k\geq 1\) with \(\delta_{k}<1\), it holds that_

\[\|w_{k+1}-w_{k}\|\leq\frac{1}{1-\delta_{k}}\mathrm{dist}(w_{k},\Omega_{ \mathrm{D}}).\]Proof.: From the triangle inequality and the stopping criterion (B\({}^{\prime}\)), we have

\[\|w_{k+1}-w_{k}\|-\|w_{k}-\mathrm{pro}_{r_{k},-g_{0}}(w_{k})\|\leq\|w_{k+1}- \mathrm{pro}_{r_{k},-g_{0}}(w_{k})\|\leq\delta_{k}\|w_{k+1}-w_{k}\|.\]

Rearranging terms and using Proposition E.2 yields

\[(1-\delta_{k})\|w_{k+1}-w_{k}\|\leq\|w_{k}-\mathrm{pro}_{r_{k},-g_{0}}(w_{k})\| \leq\mathrm{dist}(w_{k},\Omega_{\mathrm{D}}).\]

This completes the proof. 

We are ready to start the proof of part (a) here. As dual strict complementary holds, the negative of the dual function (13), i.e., \(-g_{0}\), satisfies quadratic growth (9b). Using Lemma 4 and stopping criteria (B\({}^{\prime}\)), we have \(\|w_{k+1}-\mathrm{pro}_{r_{k},-g_{0}}(w_{k})\|\leq\delta_{k}\|w_{k+1}-w_{k}\|\). Therefore, the linear reduction on \(\mathrm{dist}(w_{k},\Omega_{\mathrm{D}})\) comes directly from the PPM convergence Lemma 1, i.e., there exists a \(\hat{k}_{1}\) such that for all \(k\geq\hat{k}_{1}\), it holds that

\[\mathrm{dist}(w_{k+1},\Omega_{\mathrm{D}})\leq\mu_{k}\mathrm{dist}(w_{k}, \Omega_{\mathrm{D}}),\quad\mu_{k}=\frac{\theta_{k}+2\delta_{k}}{1-\delta_{k} }<1,\quad\theta_{k}=\frac{1}{\sqrt{2r_{k}\mu_{\mathrm{q}}+1}}<1.\]

We move to show the bound for residuals \(\mathrm{dist}(X_{k+1},\mathbb{S}_{+}^{n}),\|\mathcal{A}(X_{k+1})-b\|\), and \(\langle C,X_{k+1}\rangle-p^{\star}\). Specifically, when \(\delta_{k}<1\), it holds that

\[\mathrm{dist}(X_{k+1},\mathbb{S}_{+}^{n}) \leq\frac{\|Z_{k}-Z_{k+1}\|}{r_{k}}\leq\frac{\|w_{k}-w_{k+1}\|}{ r_{k}}\leq\frac{1}{(1-\delta_{k})r_{k}}\mathrm{dist}(w_{k},\Omega_{\mathrm{D}}),\] \[\|\mathcal{A}(x_{k+1})-b\| \leq\frac{\|y_{k}-y_{k+1}\|}{r_{k}}\leq\frac{\|w_{k}-w_{k+1}\|}{ r_{k}}\leq\frac{1}{(1-\delta_{k})r_{k}}\mathrm{dist}(w_{k},\Omega_{\mathrm{D}}),\]

where we use part (b) in Proposition 2, \(\|y_{k}-y_{k+1}\|\leq\|w_{k}-w_{k+1}\|\), \(\|Z_{k}-Z_{k+1}\|\leq\|w_{k}-w_{k+1}\|\), and Proposition E.3. Again, using part (b) in Propositions 2 and E.3, when \(\delta_{k}<1\), we have

\[\langle C,X_{k+1}\rangle-p^{\star} \leq\frac{\delta_{k}^{2}\|w_{k+1}-w_{k}\|^{2}+\|w_{k}\|^{2}-\|w_{k +1}\|^{2}}{2r_{k}}\] \[=\frac{\delta_{k}^{2}\|w_{k+1}-w_{k}\|^{2}+(\|w_{k}\|+\|w_{k+1}\|) (\|w_{k}\|-\|w_{k+1}\|)}{2r_{k}}\] \[\leq\frac{\delta_{k}^{2}\|w_{k+1}-w_{k}\|^{2}+(\|w_{k}\|+\|w_{k+1} \|)(\|w_{k}-w_{k+1}\|)}{2r_{k}}\] \[\leq\frac{\delta_{k}^{2}\|w_{k+1}-w_{k}\|+\|w_{k}\|+\|w_{k+1}\|}{ 2r_{k}(1-\delta_{k})}\mathrm{dist}(w_{k},\Omega_{\mathrm{D}}).\]

Since \(\delta_{k}\to 0\), there exists a \(\hat{k}_{2}\) such that \(\delta_{k}<1\) for all \(k\geq\hat{k}_{2}\). Taking \(\hat{k}=\max\{\hat{k}_{1},\hat{k}_{2}\}\) completes the proof for part (a) with the constants

\[\mu_{k}^{\prime} =\frac{1}{(1-\delta_{k})r_{k}}\to\mu_{\infty}^{\prime}=\frac{1}{ r_{\infty}},\] \[\mu_{k}^{\prime\prime} =\frac{\delta_{k}^{2}\|w_{k+1}-w_{k}\|+\|w_{k}\|+\|w_{k+1}\|}{2r_ {k}(1-\delta_{k})}\to\mu_{\infty}^{\prime\prime}=\frac{\|w_{\infty}\|}{r_{ \infty}}.\]

We move on to part (b). By Theorem 2, we know that the error bound (7b) holds as dual strict complementary holds, i.e., for any bounded set \(\mathcal{U}\subseteq\mathbb{S}^{n}\) containing any \(X^{\star}\in\Omega_{\mathrm{P}}\), there exist constants \(\alpha_{1},\alpha_{2},\alpha_{3}>0\) such that

\[\langle C,X\rangle-\langle C,X^{\star}\rangle+\alpha_{1}\|\mathcal{A}(X)-b\|+ \alpha_{2}\cdot\mathrm{dist}(X,\mathbb{S}_{+}^{n})\geq\alpha_{3}\cdot\mathrm{ dist}^{2}(X,\Omega_{\mathrm{P}}),\quad\forall X\in\mathcal{U}.\] (E.2)

From the assumption that \(\Omega_{\mathrm{P}}\) is bounded, Proposition 2 confirms that the sequence \(\{X_{k}\}\) is bounded. Thus, we can choose a bounded set \(\mathcal{U}\supseteq\Omega_{\mathrm{P}}\bigcup\cup_{k\geq 1}\{X_{k}\}\). Combing (E.2) with (16), we have

\[\alpha_{3}\cdot\mathrm{dist}^{2}(X_{k+1},\Omega_{\mathrm{P}})\leq(\mu_{k}^{ \prime\prime}+(\alpha_{1}+\alpha_{2})\mu_{k}^{\prime})\cdot\mathrm{dist}(w_{k}, \Omega_{\mathrm{D}}).\]

Dividing both sides by \(\alpha_{3}\) leads to the desired result in part (b). This completes the proof.

### ALMs for dual conic program (D)

In aiming to offer a comprehensive understanding of ALMs for solving (P) and (D), in this subsection, we provide the algorithm of ALM applied to solving the dual problem (D) along with the convergence analysis. This can be considered as an extension of the work in [26] and a counterpart to the ALM for solving (P) discussed in Section 4.

We first reformulate (D) as the equivalent minimization problem

\[-d^{\star} =\min_{y\in\mathbb{R}^{m},Z\in\mathbb{S}^{n}} \langle-b,y\rangle\] \[\operatorname{subject\ to} C-\mathcal{A}^{\star}(y)\in\mathbb{S}_{+}^{n}.\]

We then introduce a dual variable \(X\in\mathbb{S}_{+}^{n}\) and define the ordinary Lagrangian function as \(L_{0}(y,X)=-\left\langle b,y\right\rangle+\left\langle X,C-\mathcal{A}^{\star }(y)\right\rangle\). The corresponding dual function and dual problem become

\[g_{0}(X)=\inf_{y\in\mathbb{R}^{m}}\,L_{0}(y,X)\quad\text{and} \quad\max_{X\in\mathbb{S}_{+}^{n}}\,g_{0}(X).\] (E.3)

Then, the augmented Lagrangian with parameter \(r>0\), analogous to (14), is

\[L_{r}(y,X)=\langle-b,y\rangle+\frac{1}{2r}(\|\Pi_{\mathbb{S}_{+}^{n}}(X-r(C- \mathcal{A}^{\star}(y)))\|^{2}-\|X\|^{2}).\]

Given an initial point \(X_{0}\in\mathbb{S}^{n}\) and a sequence of positive scalars \(r_{k}\uparrow r_{\infty}\), the ALM generates a sequence of \(\{y_{k}\}\) and \(\{X_{k}\}\) following

\[y_{k+1} \approx\operatorname*{argmin}_{y\in\mathbb{R}^{m}}L_{r_{k}}(y,X_ {k})\] (E.4a) \[X_{k+1} =X_{k}+r_{k}\nabla_{X}L_{r_{k}}(y_{k+1},X_{k})=\Pi_{\mathbb{S}_{+ }^{n}}(X_{k}-r_{k}(C-\mathcal{A}^{\star}(y_{k+1}))),\quad k=0,1,\ldots.\] (E.4b)

Many of the same properties discussed in Section 4 hold. We will provide the precise statement in the remainder of this section. Similarly, we can consider the analogous stopping criteria of (A\({}^{\prime}\)) and (B\({}^{\prime}\)).

\[L_{r_{k}}(y_{k+1},X_{k})-\min_{y\in\mathbb{R}^{m}}L_{r_{k}}(y,X_ {k}) \leq\epsilon_{k}^{2}/(2r_{k}),\quad\sum_{k=1}^{\infty}\epsilon_{k}<\infty,\] (A \[{}^{\prime}_{\mathrm{D}}\] ) \[L_{r_{k}}(y_{k+1},X_{k+1})-\min_{y\in\mathbb{R}^{m}}L_{r_{k}}(y, X_{k}) \leq\delta_{k}^{2}\|X_{k+1}-X_{k}\|^{2}/(2r_{k}),\quad\sum_{k=1}^{ \infty}\delta_{k}<\infty.\] (B \[{}^{\prime}_{\mathrm{D}}\] )

**Proposition E.4**.: _Consider (P) and (D). Assume strong duality holds and \(\Omega_{\mathrm{P}}\neq\emptyset\). Let \(\{y_{k},X_{k}\}\) be a sequence from the ALM (E.4) under (A\({}^{\prime}_{\mathrm{D}}\)) and write \(Z_{k}=C-\mathcal{A}^{\star}(y_{k})\) for all \(k\geq 0\). The following statements hold._

1. _The dual sequence_ \(X_{k}\) _is bounded. Further,_ \(\lim_{k\to\infty}X_{k}=X_{\infty}\in\Omega_{\mathrm{P}}\) _(i.e., the whole sequence converges to a primal optimal solution)._
2. _The dual feasibility and cost value gap satisfy_ \[\operatorname{dist}(Z_{k+1},\mathbb{S}_{+}^{n}) \leq\|X_{k}-X_{k+1}\|/r_{k}\to 0,\] \[d^{\star}-\left\langle b,y_{k+1}\right\rangle \leq L_{r_{k}}(y_{k+1},X_{k})-\min_{y\in\mathbb{R}^{m}}L_{r_{k}}(y,X_ {k})+(\|X_{k}\|^{2}-\|X_{k+1}\|^{2})/(2r_{k})\to 0.\]
3. _If the solution set_ \(\Omega_{\mathrm{D}}\) _in (_1b_) is nonempty and bounded, then the primal sequence_ \(y_{k}\) _is also bounded, and all of its cluster points belongs to_ \(\Omega_{\mathrm{D}}\)_._

Proof.: The proof of many of these statements is a change of the notations in the proof in Proposition 2.

* Part (a) comes directly from the PPM convergence by noting Lemma 4 and the stopping criterion (A\({}^{\prime}_{\mathrm{D}}\)) imply the dual iterate \(X_{k+1}\) satisfies \(\|X_{k+1}-\operatorname{prox}_{r_{k},-g_{0}}(X_{k})\|\leq\epsilon_{k}\).
* By the Moreau decomposition [49, Exercise 12.22], we have \[r_{k}Z_{k+1}-X_{k}=\Pi_{\mathbb{S}_{+}^{n}}(r_{k}Z_{k+1}-X_{k})-\Pi_{ \mathbb{S}_{+}^{n}}(X_{k}-r_{k}Z_{k+1})=\Pi_{\mathbb{S}_{+}^{n}}(r_{k}X_{k+1}-Z _{k})-X_{k+1}.\] Thus, \(Z_{k+1}+\frac{X_{k+1}-X_{k}}{r_{k}}\in\mathbb{S}_{+}^{n}\). Therefore, \[\operatorname{dist}(Z_{k+1},\mathbb{S}_{+}^{n}) \leq\left\|Z_{k+1}-\left(Z_{k+1}+\frac{X_{k+1}-X_{k}}{r_{k}}\right) \right\|=\frac{1}{r_{k}}\left\|X_{k+1}-X_{k}\right\|.\]

[MISSING_PAGE_FAIL:29]

Since \(\delta_{k}\to 0\), there exists a \(\hat{k}_{2}\) such that \(\delta_{k}<1\) for all \(k\geq\hat{k}_{2}\). Taking \(\hat{k}=\max\{\hat{k}_{1},\hat{k}_{2}\}\) completes the proof for part (a) with the constants

\[\mu^{\prime}_{k} =\frac{1}{r_{k}(1-\delta_{k})}\to\mu^{\prime}_{\infty}=\frac{1}{r _{\infty}},\] \[\mu^{\prime\prime}_{k} =\frac{\delta_{k}^{2}\|X_{k+1}-X_{k}\|+\|X_{k}\|+\|X_{k+1}\|}{2r _{k}(1-\delta_{k})}\to\frac{\|X_{\infty}\|}{r_{\infty}}.\]

On the other hand, Theorem 1 also guarantees that the error bound (8b) holds, i.e., for any bounded set \(\mathcal{V}\supseteq\Omega_{\mathrm{D}}\), there exist constants \(\alpha_{1},\alpha_{2},\alpha_{3}>0\) such that

\[d^{\star}-\langle b,y\rangle+\alpha_{1}\|C-\mathcal{A}^{\star}(y)-Z\|+\alpha_ {2}\cdot\mathrm{dist}(Z,\mathbb{S}_{+}^{n})\geq\alpha_{3}\cdot\mathrm{dist}^{2 }((y,Z),\Omega_{\mathrm{D}}),\quad\forall(y,Z)\in\mathcal{V}.\] (E.7)

By the design of (E.4), we always have \(\|C-\mathcal{A}^{\star}(y_{k})-Z_{k}\|=0\). By Proposition E.4 - part (c), the sequence \(\{y_{k},Z_{k}\}\) is bounded. Thus, choosing a bounded set \(\mathcal{V}\supseteq\Omega_{\mathrm{D}}\bigcup\cup_{k\geq 1}\{y_{k},Z_{k}\}\) and combing (E.7) with (E.5), we have

\[\alpha_{3}\cdot\mathrm{dist}^{2}(y_{k+1},\Omega_{\mathrm{D}})\leq(\mu^{\prime \prime}_{k}+\alpha_{2}\mu^{\prime}_{k})\cdot\mathrm{dist}(X_{k},\Omega_{ \mathrm{P}}),\;\forall k\geq\hat{k}.\]

Dividing both sides by \(\alpha_{3}\) leads to the desired result. This completes the proof. 

## Appendix F Details of the numerical experiments

In Section 5, we consider the classical SDP relaxation of the Mac-Cut problem and the linear SVM. In this section, along with another popular signal processing problem - Lasso, we detail the problem formulation and report the numerical performance of ALM applied to those problems.

* The SDP relaxation of Max-Cut: \[\min_{X\in\mathbb{S}^{n}} \langle C,X\rangle\] \[\mathrm{subject\ to} \mathrm{Diag}(X)=1,\] \[X\in\mathbb{S}_{+}^{n},\] where \(C\in\mathbb{S}^{n}\) is the problem data representing a weighted undirected graph, \(1\in\mathbb{R}^{n}\) is an all one vector, and \(\mathrm{Diag}(X)=[X_{11},\quad\cdots\quad,X_{nn}]^{\mathsf{T}}\). This type of problem has been shown to satisfy strict complementarity and has unique primal and dual solutions for almost all data matrix \(C\)[41, Corollary 2.3].
* Linear SVM: \[\min_{x\in\mathbb{R}^{d}}\;\lambda\sum_{i=1}^{m}\max\{0,1+b_{i}(a_{i}^{ \mathsf{T}}x)\}+\frac{1}{2}\|x\|^{2},\] where \(a_{i}\in\mathbb{R}^{n}\) is the feature of the \(i\)-th data point, \(b_{i}\in\{-1,1\}\) is the corresponding label, and \(\lambda>0\) is a constant. Equivalently, the problem can reformulated in the following standard conic form \[\min_{x\in\mathbb{R}^{n},t\in\mathbb{R}^{m}} \frac{1}{2}\|x\|^{2}+\lambda\mathbf{1}^{\mathsf{T}}t\] \[\mathrm{subject\ to} \mathrm{Diag}(b)Ax+\mathbf{1}\leq t,\] \[0\leq t,\] where \(A=[a_{1},\cdots,a_{m}]^{\mathsf{T}}\in\mathbb{R}^{m\times n}\) and \(\mathrm{Diag}(b)\in\mathbb{S}^{m}\) denotes the diagonal matrix with the diagonal elements being \(b\).
* Lasso: \[\min_{x\in\mathbb{R}^{n}} \frac{1}{2}\|Ax-b\|^{2}+\lambda\|x\|_{1},\] where \(A\in\mathbb{R}^{m\times n}\), \(b\in\mathbb{R}^{m}\) are problem data and \(\lambda>0\) is the weighted parameter balancing the sparsity of the solution and the accuracy of the linear solution. Equivalently, the problem can be rewritten in the conic form \[\min_{x\in\mathbb{R}^{n}} \frac{1}{2}\|y\|^{2}+\lambda\mathbf{1}^{\mathsf{T}}t\] subject to \[Ax-b=y,\] \[-t\leq x\leq t.\]All numerical experiments are conducted on a PC with a 12-core Intel i7-12700K CPU@3.61GHz and 32GB RAM. For Max-Cut problem, we select the graph \(\text{G}_{1},\text{G}_{2},\) and \(\text{G}_{3}\) from the website https://web.stanford.edu/~yyye/yyye/Gset/ and only take the first \(20\times 20\) submatrix as the considered problem data \(C\). For both the linear SVM and Lasso, we randomly generate the problem data with the dimension \(m=100\) and \(n=10\), and set the constant \(\lambda=1\). We use Mosek [56] to get the optimal cost value. In each application, we consider the following two scenarios 1) fixing the same augmented parameter \(r_{k}=r\) for all \(k\) for three different problem instances; 2) fixing a problem instance and varying different augmented parameter \(r\). The numerical results are presented in Figures 3 to 5. For each Figures 3 to 5, the first row shows the evolution of the primal cost gap, dual cost gap, and KKT residuals for three different instances with a fixed augmented parameter (scenario one), and the second row compares the different numerical behaviors when varying the augmented parameter (scenario two). In the setting of a fixed augmented parameter, the augmented term \(r\) is set as \(1,5,20\) for Max-Cut, linear SVM, and Lasso respectively. In the setting of various augmented parameters, the parameter is chosen as \(r=\{1,5,10\}\), \(r=\{1,5,10\}\), and \(r=\{10,20,30\}\) for Max-Cut, linear SVM, and Lasso respectively.

We observe that, in all cases, ALM enjoys linear convergence in the primal cost value gap, dual cost value gap, and KKT residuals to the accuracy of at least \(10^{-5}\), which is consistent with our theoretical findings in Theorem 3. We believe the oscillated or flattening behavior that appears in the tail (when the iterates are close to the solution set) is due to the inaccuracy of the subproblem solver and computational errors. The numerical result also indicates that when the augmented term \(r\) increases, ALM favors the decrease of the feasibility more, leading to faster convergence in the beginning phase. However, a large \(r\) may not lead to the best convergence in the long term.

Note that the projection term \(\|\Pi_{\mathbb{S}_{+}^{n}}(Z-rX)\|^{2}\) in the subproblem (15a) can not be directly modeled by Yalmip. Therefore, we reformulate the problem as the following

\[\operatorname*{argmin}_{X\in\mathbb{S}^{n}}\left\langle C,X\right\rangle+ \frac{1}{2r}(\|y+r_{k}(b-\mathcal{A}(X))\|^{2}+\|\Pi_{\mathbb{S}_{+}^{n}}(Z-r _{k}X)\|^{2}-\|y\|^{2}-\|Z\|^{2})\] \[= \operatorname*{argmin}_{X\in\mathbb{S}^{n}}\left\langle C,X \right\rangle+\frac{1}{2r}(\|y+r_{k}(b-\mathcal{A}(X))\|^{2}+\|\Pi_{\mathbb{ S}_{-}^{n}}(r_{k}X-Z)\|^{2})\] \[= \operatorname*{argmin}_{X\in\mathbb{S}^{n},U\in\mathbb{S}_{+}^{n }}\left\langle C,X\right\rangle+\frac{1}{2r}(\|y+r_{k}(b-\mathcal{A}(X))\|^{2 }+\|r_{k}X-Z-U\|^{2}),\]

Figure 3: Nuremical experiments for Max-Cut with different instances and various augmented Lagrangian parameter \(r>0\).

where the first equality drops two independent terms and uses \(-\mathbb{S}_{+}^{n}=\mathbb{S}_{-}^{n}\) and \(\|\Pi_{\mathbb{S}_{+}^{n}}(Y)\|=\|-\Pi_{(-\mathbb{S}_{+}^{n})}(-Y)\|=\|\Pi_{( \mathbb{S}_{-}^{n})}(-Y)\|\) for all \(Y\in\mathbb{S}^{n}\), and the last equality uses \(\min_{U\in\mathbb{S}_{+}^{n}}\|\tilde{Y}-U\|=\|Y-\Pi_{\mathbb{S}_{+}^{n}}(Y)\|= \|\Pi_{\mathbb{S}_{-}^{n}}(Y)\|\) for all \(Y\in\mathbb{S}^{n}\).

Figure 4: Nuremical experiments for linear SVM with different instances and various augmented Lagrangian parameter \(r>0\).

Figure 5: Nuremical experiments for Lasso with different instances and various augmented Lagrangian parameter \(r>0\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have clearly and accurately stated the contributions and scope of this paper in the abstract and introduction. We have also made proper assumptions and presented a clear comparison with results in the existing literature. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have clearly stated the assumptions on conic optimization in order to guarantee the quadratic growth properties and the linear convergence of inexact ALM. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All the theorems, formulas, and proofs in the paper are numbered and cross-referenced. We have provided completed proofs for all theorems/lemmas. In the main text, a proof sketch is provided, and the details can be found in the supplementary material. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided a separated section in the supplementary material for experimental details. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We plan to make all code open access after the review period. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have a separate section in the supplementary materials for experimental setting. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the type of computer and memory for reproducing our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have followed closely the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer:[Yes] Justification: We have discussed the positive societal impacts from efficient algorithms to solve conic optimization. We do not see negative societal impacts of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.