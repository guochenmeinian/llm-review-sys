# Towards Stable Representations for Protein

Interface Prediction

 Ziqi Gao1,2, Zijing Liu3*, Yu Li3, Jia Li1,2*

\({}^{1}\)Hong Kong University of Science and Technology

\({}^{2}\)Hong Kong University of Science and Technology (Guangzhou)

\({}^{3}\) International Digital Economy Academy (IDEA)

Footnote *: Correspondence to: Zijing Liu (liuzijing@idea.edu.cn) and Jia Li (jialee@ust.hk).

###### Abstract

The knowledge of protein interactions is crucial but challenging for drug discovery applications. This work focuses on protein interface prediction, which aims to determine whether a pair of residues from different proteins interact. Existing data-driven methods have made significant progress in effectively learning protein structures. Nevertheless, they overlook the conformational changes (i.e., flexibility) within proteins upon binding, leading to poor generalization ability. In this paper, we regard the protein flexibility as an _attack_ on the trained model and aim to defend against it for improved generalization. To fulfill this purpose, we propose ATProt, an adversarial training framework for protein representations to robustly defend against the attack of protein flexibility. ATProt can theoretically guarantee protein representation stability under complicated protein flexibility. Experiments on various benchmarks demonstrate that ATProt consistently improves the performance for protein interface prediction. Moreover, our method demonstrates broad applicability, performing the best even when provided with testing structures from structure prediction models like ESMFold and AlphaFold2.

## 1 Introduction

Protein-protein interactions are important for understanding biological processes, and for the design of novel therapies [48, 16] and drugs [47]. The protein interface refers to the surface region of a protein where the interaction occurs. It therefore holds the key to revealing the specific interaction mechanism and understanding protein functions. In this work, we tackle the problem of protein

Figure 1: (A). **The task illustration.** PIP involves predicting if there is an interaction between two residues from different proteins. (B). **The task challenge.** During training, the input consists of bound structures of two proteins. However, for testing, one can only access their unbound structures.

interface prediction (PIP) (shown in Figure 1(A)): predicting whether two residues, each from an individual protein, interact with each other, given the separate structures of two proteins.

For years, data-driven methods based on deep learning (DL) have made significant progress in response to this critical task by effectively learning protein structures using geometric graph neural networks (GNN) [15; 33], 3D CNN [43], etc. Limited by the difficulty in accessing protein structure data, they typically follow a formulation of training on bound (after binding) structures and testing on unbound (before binding) ones, as depicted in Figure 1(B). For training, large-scale datasets like the Database of Interacting Protein Structures (DIPS) [43] typically consist of only bound structures, which are directly extracted from the PDB database[4]. In contrast, in the practical inference scenario, the model cannot access the bound structures but can only be provided with unbound ones [43; 15; 33]. In this paper, we empirically find that this training (bound)-testing (unbound) formulation leaves significant room for performance improvement. In Figure 2, exploratory experiments show that prevailing PIP methods are sensitive to flexibility. Utilizing the bound version structures for testing can greatly boost their performance. Based on these findings, we aim to answer the question in this paper--_how to handle the mismatch between bound and unbound structures for PIP?_

Since it is usually impractical to access the protein bound structures for testing, the most intuitive solution is to explicitly learn the mapping relationship from unbound to bound structures of proteins. However, this is challenging due to the following two factors: (1) The amount of pairwise unbound and bound structure data for proteins is extremely limited [10] (to our knowledge, only DB5.5 [45]). (2) A protein's bound structure is not unique and depends on its binding partner, so diverse training data is necessary. **To address this issue, we take a different route.** We consider any potentially complicated flexibility in a protein as an attack [29; 24], which can harm the testing performance of a model trained on bound structures. Therefore, our core idea is to enable protein representations with adversarial robustness, which can defend against the attacks of protein flexibility. In simple terms, for a protein with both unbound and bound versions, the model outputs similar (stable) representations.

In this work, we take an important step forward in mitigating the impact of flexibility on PIP. We propose **ATProt**, an end-to-end adversarial training (AT) framework for protein representations, to effectively defend against protein flexibility in PIP. Inspired by the recent protein graph representation methods [17; 53; 15], our model comprises graph-based feature extractors (encoders) for protein graphs. Our ATProt framework does not require computationally expensive data augmentation and can be smoothly applied to most existing protein graph encoders. Specifically, we implement differentiable AT regularizations for various protein representation encoders. Importantly, we introduce a novel and expressive graph encoder for protein representations and propose its theoretical regularization form for the first time. ATProt can produce stable representations for the same protein with different structure versions (e.g., bound, unbound, and model-generated ones). Extensive experiments on several protein interaction benchmarks verify that our ATProt method consistently outperforms advanced PIP methods. The results demonstrate the effectiveness of the AT regularization. Furthermore, ATProt maintains excellent performance even when tested with structures generated by AlphaFold2 [25] and ESMFold [32], allowing for user-friendly inference without the need for native structures.

Figure 2: The impact of flexibility on results with the DB5.5 dataset [45]. (A) The testing results of two baselines (SASNet [43], NEA [15]) and our method. ‘B-U’ represents the popular formulation, i.e., training with bound structures and testing with unbound ones. ‘B-B’ refers to the formulation where both training and testing are conducted with bound structures. (B) Loss trends for three method.

Related Work

Protein interface prediction.Protein interface prediction (PIP), a well-studied problem, focuses on determining whether there is an interaction between amino acids from two different proteins. Recently, a series of methods based on protein [8; 19; 17; 20; 14] or amino acid representation [39; 46] learning have achieved significant success. NEA [15] pioneers the use of protein graphs to address PIP, where protein structure information is represented and aggregated, followed by the dense layers. SASNet [43] considers embedding the hierarchical structures of proteins, integrating atomic and amino acid information into a 4D-grid data, and employs 3D CNN for learning. To further enhance performance, more fine-grained structure information modeling, specifically surface geometry [46; 39], is introduced to effectively learn amino acid representations. Existing methods have effectively represented proteins from various perspectives of protein information. However, we have observed that protein flexibility, which is overlooked by most methods, poses significant performance bottlenecks for them. We focus on this key issue of mitigating the bound-unbound mismatch in protein structures to improve model generalization.

Modelling protein flexibility.Recently, pioneer works in biology confirm that protein-protein interaction (PPI) conforms to the "induced fit" theory [27; 38]. Specifically, proteins undergo structure changes due to residue-level forces, and they adjust structures to achieve the best binding state. More importantly, proteins with PPI typically undergo larger structure changes at the interface compared to non-interface regions [10; 11; 52; 12], which will exacerbate the generalization challenge of the PIP task. Modelling flexibility directly is challenging, whether using traditional computational or deep learning (DL) approaches. Traditional methods often rely on finding the lowest energy state [42; 51] or introducing an induced fit model (specifically, the elastic network model) [12; 3] to guide structure deformations. The optimization space in these methods is vast, making them very time-consuming. DL-based methods [10] struggle to achieve satisfactory accuracy in learning the distribution mapping of bound-unbound states due to limited training data (i.e., 253 complexes in the DB5.5). As directly predicting bound structures is challenging, in the context of the PIP task, we choose to eliminate the influence of different versions of the same protein structure on the task.

## 3 Preliminaries

In this section, **(1)** we present the definition of the protein interface prediction (PIP). Then, **(2)** we verify the importance of representation stability through empirical and mathematical views. Finally, **(3)** we investigate to ensure protein representation stability within the adversarial training framework.

Problem definition.We are given as input two proteins \(\mathcal{P}^{1}\) and \(\mathcal{P}^{2}\), consisting of \(M\) and \(N\) residues, respectively. The proteins are represented as their residue sequences and 3D structures, which are composed of \(\alpha\)-carbon atom locations of all residues. The goal of the PIP is to classify all possible pairs of residues from separate proteins. More formally, the set of data is \(\{(\mathcal{P}^{1}_{i},\mathcal{P}^{2}_{j}),y_{ij}\}_{1\leq i\leq M,1\leq j \leq N}\), where \(\mathcal{P}^{1}_{i}\) represents the \(i\)-th residue in protein \(\mathcal{P}^{1}\) and \(y_{ij}\in\{0,1\}\).

### The importance of protein representation stability

We verify the importance of stable protein representations from both empirical and theoretical perspectives. For clarity, we use the notations \(\bm{X}^{b}_{1}\), \(\bm{X}^{b}_{2}\) to represent the native bound structures of proteins \(\mathcal{P}^{1}\) and \(\mathcal{P}^{2}\), while \(\bm{X}^{t}_{1},\bm{X}^{t}_{2}\) represent their structures used for testing. After using a protein graph encoder, we have their representations, denoted as \(\bm{H}^{b}_{1},\bm{H}^{b}_{2},\bm{H}^{t}_{1},\bm{H}^{t}_{2}\). We denote the protein representation perturbation as \(\|\delta_{1}\|_{p}+\|\delta_{1}\|_{p}\), where \(\delta_{1}=\bm{H}^{t}_{1}-\bm{H}^{b}_{1},\delta_{2}=\bm{H}^{t}_{2}-\bm{H}^{b}_ {2}\).

From an empirical perspective, in Figure 3, we quantify the test results of the NEA method [15] under flexibility. We gradually increase the structure change of test samples and calculate the representation perturbation. We note that the test performance is negatively correlated with the representation perturbation caused by flexibility. Moreover, testing with bound structures yields the best results.

Figure 3: AUC vs. representation perturbation.

**From a theoretical perspective,** we draw the consistent conclusion that stable representations lead to improved PIP results. Following [15], we model PIP as a pairwise classification problem. Specifically, we concatenate the \(i\)-th and \(j\)-th rows of \(\bm{H}_{1}^{t}\) and \(\bm{H}_{2}^{t}\) into a vector embedding, which is then sent to the PIP classifier \(f\). In this case, the following proposition describes the impact of representation perturbations on PIP results.

**Proposition 3.1**.: _For two proteins with \(M\) and \(N\) residues respectively, the classification results obtained using bound and unbound structures are the same. This is true if \(N\left\|\delta_{1}\right\|_{p}^{p}+M\left\|\delta_{2}\right\|_{p}^{p}<\mathcal{ A}(f,p)\), where \(\mathcal{A}(f,p)\) can be a constant depending only on the PIP classifier \(f\) and the norm \(\left\|\cdot\right\|_{p}\)._

We detail \(\mathcal{A}(f,p)\) and prove Proposition 3.1 in Appendix B.1. This proposition tells that stable protein representations (i.e., smaller \(\left\|\delta_{1}\right\|_{p}\) and \(\left\|\delta_{2}\right\|_{p}\)) under flexibility are necessary for achieving high performance. Thus, to effectively address the structure mismatch in PIP, an intuitive idea is to perform adversarial training for protein representation learning.

### Adversarial training

Here, we introduce the concept of adversarial training (AT) [1; 6; 18; 5; 7] and establish a connection between it and the stability of protein representations. We consider a classification task with a given dataset \(\mathcal{D}=\{(x_{i},y_{i})\}_{i=1}^{n}\), consisting of \(K\) classes. We assume that the entire prediction pipeline includes a representation model (e.g., encoder) and a classifier. The concept of adversarial training (AT) requires the entire pipeline to perform well not only on \(\mathcal{D}\) but also on the worst-case distribution near \(\mathcal{D}\), as determined by a specific distance metric. More concretely, the AT that we primarily focus on in this paper is the \(\ell_{p}\)-robustness. For a given \(p\) value and a finite \(\epsilon>0\), AT aims to train a pipeline that can correctly classifies \((x+\delta,y)\) for any \(\left\|\delta\right\|_{p}\leq\epsilon\), where \((x,y)\) belongs to \(\mathcal{D}\).

Among all AT methods, Lipschitz neural networks belong to a common and effective category. Specifically, an encoder function is considered to have Lipschitz continuity if a slight perturbation to the input of the encoder does not significantly change its output.

Formally, the definition of Lipschitz continuity is given by:

**Definition 3.1**.: _(**Lipschitz continuity in adversarial training)** An encoder function \(\textsc{Enc}\) is said to be \(C\)-Lipschitz continuous w.r.t. norm \(\left\|\cdot\right\|\) if for any two versions of inputs \(\bm{x}_{1},\bm{x}_{2}\),_

\[\left\|\textsc{Enc}(\bm{x}_{1})-\textsc{Enc}(\bm{x}_{2})\right\|\leq C\left\| \bm{x}_{1}-\bm{x}_{2}\right\|.\] (1)

Lipschitz continuity explains the requirements of AT for a general representation learning encoder. In the context of protein graph representation, this can be modified to become the definition below.

**Definition 3.2**.: _(**Lipschitz continuity for protein representations)** A protein representation encoder \(\Psi(\cdot)\) has \(C\)-Lipschitz continuity w.r.t. norm \(\left\|\cdot\right\|\) if for any two versions of structure inputs \(\bm{X}^{t},\bm{X}^{b}\) and the invariant residue feature input \(\bm{F}\),_

\[\left\|\Psi(\bm{F},\bm{X}^{t})-\Psi(\bm{F},\bm{X}^{b})\right\|\leq C\left\| \bm{X}^{t}-\bm{X}^{b}\right\|.\] (2)

### How to ensure Lipschitz continuity for protein graph representations?

As an expressive representation, graph structured data is widely used for representing input proteins [17; 22; 19], with residues acting as nodes and physical interactions as edges. Let \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) be an undirected graph with nodes \(\mathcal{V}=\{1,...,N\}\), edges \(\mathcal{E}\subset\mathcal{V}^{2}\), graph signal \(\bm{F}\in\mathbb{R}^{N\times d}\) and a graph shift operator \(\tilde{\bm{L}}\in\mathbb{R}^{N\times N}\) (i.e., node connectivity). We consider any variant of the spectral GNN (e.g., GCN [26; 30], ChebNets [13], BWGNN [41; 40]) that follows the concept of learning filter coefficients for graph convolution. By constructing the filter \(h(\bm{L}):=\sum_{k=0}^{K}\theta_{k}\bm{L}^{k}\) (\(\theta_{k}\) are learnable parameters), the protein graph representation can be defined as \(\bm{H}=\sum_{k=0}^{K}\theta_{k}\bm{L}^{k}\bm{F}:=h(\bm{L})\bm{F}\).

Here, we extend the Definition 3.2 to the scenario of using GNN models. To achieve this, we assume \(\bm{L}\) is perturbed to become \(\tilde{\bm{L}}\) due to the protein flexibility, and introduce the key factor (_GNN filter stability constant_\(C_{h}\)), for achieving GNN-based stable protein representations.

**Definition 3.3**.: _(**GNN filter stability constant)** Given a graph spectral filter \(h:[0,2]\mapsto[0,1]\), it is defined as Lipschitz with constant \(C_{h}>0\) if for any pair of points \(\lambda_{1},\lambda_{2}\):_

\[\left|h(\lambda_{1})-h(\lambda_{2})\right|\leq C_{h}\left|\lambda_{1}-\lambda_{ 2}\right|,\] (3)which introduces our main theorem below.

**Theorem 3.1**.: _(Protein graph stability with GNNs) Let the perturbation to \(\bm{L}\) is finite such that \(\left\|\tilde{\bm{L}}-\bm{L}\right\|_{p}\leq\epsilon\). The protein graph encoder \(\Psi(\cdot)\) is always stable with a polynomial filter \(h\) if for some finite \(C_{h}\),_

\[\left\|\Psi(\bm{F},\bm{L})-\Psi(\bm{F},\tilde{\bm{L}})\right\|_{p}\leq\epsilon C _{h}\cdot\mathcal{A}(\Psi)\cdot\left\|\bm{F}\right\|_{p},\] (4)

_where \(\mathcal{A}(\Psi)\) is a constant determined by the model (e.g., layer number and feature dimension)._

Based on Eq. 3 and Eq. 4, the lower bound of the left term in Eq. 4 is solely determined by the maximum of \(C_{h}\) (denoted as \(C_{h}^{*}\)). To be more intuitive, \(C_{h}^{*}\) is equal to the maximum absolute slope (MAS) of the filter \(h\) (the straightforward proof is in the Appendix B.2).

Therefore, we conclude the core idea for designing our ATProt framework as follows:

_We can enhance the stability of protein representations by decreasing the MAS value of \(h\)._

## 4 Method

Overview.We propose ATProt, an end-to-end framework (illustrated in Fig. 4) to boost PIP from a view of representation stability. Specifically, our model inputs two proteins whose structures can be provided in various sources (e.g., native bound, native unbound, AlphaFold2, ESMFold). ATProt incorporates a protein graph representation encoder and a targeted differentiable regularization scheme to theoretically guarantee the representation stability. Our ATProt framework can be implemented with at least four protein graph encoders, and we provide specific examples of these cases. Lastly, for the PIP task, we apply a cross-attention module to facilitate communication between the protein representations and use a simple linear classifier for final prediction.

Protein representation.We represent a protein as an undirected weighted graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\). Each node \(v_{i}\in\mathcal{V}\) representing one residue has a \(d\)-dimensional feature vector \(\bm{F}_{i}\in\mathbb{R}^{d}\) (i.e., residue type) and a 3D coordinate \(\bm{X}_{i}\in\mathbb{R}^{3}\) (i.e., the \(\alpha\)-carbon atom location). Edges \(\mathcal{E}=\{(i,j)\}\) are constructed with a k-nearest-neighbor (k-NN) graph using Euclidean distance. To distinguish the edges, we follow [17] to construct the SE(3)-invariant edge features \(\{e_{i,j}:\forall(i,j)\in\mathcal{E}\}\).

The goal of ATProt.According to Definition 3.2, we aim to achieve representation stability, which means that the perturbation in the representation caused by protein flexibility is constrained. In

Figure 4: **The framework overview with the BernNet encoder. The whole framework contains the stability-regularized graph encoder for stable protein representations, the cross attention layers for communication and the final binary classifier. ATProt takes in two protein graphs as inputs, and extracts features with the pre-defined graph encoder (BernNet is taken as an example here). The PIP results are obtained after the learned representations have passed through the cross attention module and classifier. The \(\mathcal{L}_{S}\) loss for stability regularization and classification loss \(\mathcal{L}_{BCE}\) jointly optimize the model.**

addition, it is also important to ensure the commonly encapsulated SE(3)-invariance, which means that the representation of the protein is not affected by its rotation or translation. Formally, we represent a single protein \(\mathcal{P}\) consisting of \(N\) residues, with its residue-level feature matrix \(\bm{F}\in\mathbb{R}^{N\times d}\) and residue-level structure \(\bm{X}\in\mathbb{R}^{N\times 3}\). We wish our model \(\Psi(\cdot)\) to ensure the following property.

\[\begin{split}&\text{Given}\quad\bm{Z},\bm{H}=\Psi(\bm{F},\bm{X}); \bm{Z}^{\prime},\bm{H}^{\prime}=\Psi(\bm{F},\bm{Q}\bm{X}+\bm{g}+\Delta\bm{X}), \\ &\text{we have}\quad\left\|\bm{H}-\bm{H}^{\prime}\right\|\leq C \cdot\left\|\Delta\bm{X}\right\|,(\text{C-Lipschitz continuity})\\ &\forall\bm{Q}\in SO(3),\forall\bm{g}\in\mathbb{R}^{3},\forall \Delta\bm{X}\in\mathbb{R}^{3\times N},\exists C\in\mathbb{R}.\end{split}\] (5)

### Adversarial training regularizations for various encoders

The Fourier transform is a powerful tool in the representation of both structured [50; 26; 49; 28] and unstructured data [9; 21; 31]. Our main focus is on employing spectral graph encoders for representing protein graphs. These encoders adhere to the principle of utilizing a graph spectral filter \(h(\lambda)=\sum_{k=0}^{K}\theta_{k}b_{k}(\lambda)\) to learn effective protein representations. Here, \(\{b_{k}\}_{k=0}^{K}\) is the pre-defined filter basis and \(\{\theta_{k}\}_{k=0}^{K}\) is the learnable parameters. For graph convolution, the filter \(h(\cdot)\) will be applied to the whole Laplacian matrix \(\bm{L}\), which is calculated from the protein structure data. Specifically, to construct \(\bm{L}\), we first apply a Multi-Layer Perceptron (MLP) to reduce the dimensionality of \(\bm{E}\) to 1, and incorporate the results into the edge weight matrix \(\bm{W}\):

\[\bm{W}_{i,j}=\begin{cases}\text{MLP}_{e}(e_{i,j}),&(i,j)\in\mathcal{E}\\ 0,&(i,j)\notin\mathcal{E}\end{cases}\] (6)

Then the Laplacian matrix \(\bm{L}\) is obtained by \(\bm{L}=\bm{I}-\bm{D}^{-1/2}\bm{W}\bm{D}^{-1/2}\), where \(\bm{D}\) is the degree matrix, i.e., \(\bm{D}=\text{diag}(\sum_{j}\bm{W}_{1,j},...,\sum_{j}\bm{W}_{N,j})\).

The protein graph representation can be defined as:

\[\bm{H}=\sum_{k=0}^{K}\theta_{k}b_{k}(\bm{L})\bm{F}=h(\bm{L})\bm{F},\] (7)

where the result of \(h(\bm{L})\) represents the graph spectral response.

In this paper, we introduce four types of top graph encoders (i.e., Simple GCN [49], Chebynet [13], Low-pass filter [34], and BernNet [23]) along with their corresponding stability regularizations.

**Case 4.1**.: _(Simple GCN encoder [49]) The Simple GCN encoder (SGC) utilizes a spectral filter in the monomial function form:_

\[h(\lambda)=\lambda^{K}.\] (8)

_Since the spectrum \(\lambda\) lies in \([-1,1]\), it is clear that the maximum absolute slope (MAS) that \(h(\lambda)\) can reach is \(K\). Therefore, the stability regularization of the SGC encoder does not involve any loss function. We can directly constrain the size of the order \(K\)._

**Case 4.2**.: _(Chebynet encoder [13]) The Chebynet encoder utilizes a spectral filter in the polynomial function form:_

\[h(\lambda)=\sum_{k=0}^{K}\theta_{k}\lambda^{k}.\] (9)

_The stability regularization of Chebynet can be implemented by the loss function \(\mathcal{L}_{S}=\sum_{k=1}^{K}k|\lambda^{k}|\)._

**Case 4.3**.: _(Low-pass filter encoder [34]) The Low-pass filter (LPF) encoder utilizes a spectral filter in the low-pass filter function form:_

\[h(\lambda)=(1+\theta\lambda)^{-1}.\] (10)

_The stability regularization of LPF can be implemented by the loss function \(\mathcal{L}_{S}=\theta\)._

**Case 4.4**.: _(BernNet encoder [23]) The BernNet encoder utilizes Bernstein basis, the state-of-the-art graph spectral basis, to construct the graph spectral filter:_

\[h(\lambda)=\sum_{k=0}^{K}\theta_{k}b_{k}^{K}(\lambda^{(l+1)})=\sum_{k=0}^{K} \theta_{k}\frac{1}{2^{K}}\begin{pmatrix}K\\ k\end{pmatrix}(2\bm{I}-\lambda^{(l+1)})^{K-k}(\lambda^{(l+1)})^{k}.\] (11)_According to Theorem 3.1, the BernNet encoder easily satisfies the representation \(C\)-Lipstchiz continuity in Eq. 4 provided that the filter \(h(\lambda)\) always has a finite MAS. However, in Eq. 11, the analytical relationship between MAS and \(\{\theta_{k}\}_{k=0}^{K}\) is intractable, suggesting the difficulty of constraining \(C\) with a gradient descent manner. Thus, our next goal is to discover a differentiable method for constraining the minimum of \(C\) in the training process._

The stability of the BernNet is still unclear in these four cases, prompting us to investigate its regularization form. To the best of our knowledge, it is the first investigation of the Lipstchiz continuity for Bernstein-based spectral filters.

### Guaranteeing stability of the BernNet encoder

For clarity, we rewrite the Eq. 4 as \(\left\|\bm{H}-\bm{H}^{\prime}\right\|\leq C\cdot\left\|\Delta\bm{X}\right\|=C_ {h}\cdot\mathcal{A}(\Psi)\cdot\left\|\Delta\bm{X}\right\|\cdot\left\|\bm{F}\right\|\), where, referring to Eq. 3, \(\mathcal{A}(\Psi)\) is determined by the model architecture hyperparameters and remains constant. We say the overall model \(\Psi\) is of \(C\)-Lipstchiz continuity and the learned spectral filter \(h(\lambda)=\sum_{k=0}^{K}\theta_{k}b_{k}^{K}(\lambda)\) is of \(C_{\cdot}\)-Lipstchiz continuity. We aim to constrain the minimum of constant \(C_{h}\) (denoted as \(C_{h}^{\prime}\)) with \(\{\theta_{k}\}_{k=0}^{K}\) by discovering the underlying relationship between them. Finally, we propose an auxiliary differentiable regularization of \(\{\theta_{k}\}_{k=0}^{K}\) to constrain \(C_{h}^{*}\) to a controllable bound, for any \(\Delta\bm{X}\).

**Theorem 4.1**.: _Given an arbitrary polynomial function \(f(\lambda)\) on \(\lambda\in[0,2]\) and suppose its \(K\)-order Bernstein polynomial is denoted as \(h(\lambda)=\sum_{k=0}^{K}f(\frac{2k}{K})\begin{pmatrix}K\\ k\end{pmatrix}(2-\lambda)^{K-k}\lambda^{k}\). For any point pair \(\lambda_{1},\lambda_{2}\in[0,2]\), if there exists a constant \(C_{f}\) for \(|f(\lambda_{1})-f(\lambda_{2})|\leq C_{f}|\lambda_{1}-\lambda_{2}|\), then \(h(\lambda)\) always holds \(C_{f}\)-Lipstchiz continuity for all \(K\geq 1\):_

\[\left|h(\lambda_{1})-h(\lambda_{2})\right|\leq C_{f}\left|\lambda_{1}-\lambda_ {2}\right|.\] (12)

We introduce Theorem 4.1, which describes the stability relationship between the filter \(h(\lambda)\) and an auxiliary function \(f(\lambda)\). It tells that with Berstein basis, the MSA of \(h(\lambda)\) will never exceed that of the auxiliary function \(f(\lambda)\). Due to \(f(\frac{2k}{K})=\theta_{k}\), for all \(k\in[0,K]\cap\mathbb{Z}\), \(f(\lambda)\) can be any 2-D curve passing through all points of \(\{(\frac{2k}{K},\theta_{k})\}_{k=0}^{K}\). Therefore, the MAS of \(f(\lambda)\) is analytically tractable, and we can subsequently provide the bounds for the MAS of \(h(\lambda)\). We have the following proposition, which is accompanied by a detailed derivation in Appendix B.3.

**Proposition 4.1**.: _Suppose \(h(\lambda)\) is approximated with Bernstein basis, i.e., \(h(\lambda)=\sum_{k=0}^{K}\theta_{k}b_{k}^{K}(\lambda)\). Denoting the MAS (minimum Lipschitz constant) of \(h\) as \(C_{h}^{*}\), it can be upper bounded by_

\[C_{h}^{*}\leq\max_{i\in[0,K-1]\cap\mathbb{Z}}K\cdot|\theta_{i}-\theta_{i+1}|.\] (13)

To summarize, Bernstein basis applied to our ATProt guarantees a finite MAS, and more importantly, we can further enhance the stability of the BernNet encoder with the following objective:

\[\mathcal{L}_{\mathcal{S}}=\max_{i\in[0,K-1]\cap\mathbb{Z}}|\theta_{i}-\theta_ {i+1}|.\] (14)

### Protein interface prediction

Given proteins \(\mathcal{P}_{1},\mathcal{P}_{2}\) with their initial feature \(\bm{F}_{1},\bm{F}_{2}\) and coordinates \(\bm{X}_{1},\bm{X}_{2}\), ATProt produces their stable representations under respective structure perturbations.

\[\bm{H}_{1}\in\mathbb{R}^{M\times d}=ATProt(\bm{X}_{1},\bm{F}_{1});\bm{H}_{2} \in\mathbb{R}^{M\times d}=ATProt(\bm{X}_{2},\bm{F}_{2}).\] (15)

We apply a cross-attention layer (shown in Appendix D) to enable communication between proteins and obtain their final representations \(\bm{H}_{1}^{\prime}\),\(\bm{H}_{2}^{\prime}\). Next, we aim to predict whether pairs of inter-protein residues belong to the interface, which involves performing pairwise binary classification. Concretely, for each training pair of proteins, we have a set of \(10N_{I}\) labeled pairs \(\left\{(([\bm{H}_{1}^{\prime}]_{i},[\bm{H}_{2}^{\prime}]_{i}),y_{i})\right\}_{ i=1}^{10N_{I}}\), where \(y_{i}\in\{0,1\}\), \(N^{I}\) is the number of positive residue pairs and \(9N^{I}\) negative ones are downsampled. We take the element-wise product of two residue representations and feed it to another MLP with the Sigmoid function to compute the probability \(p_{i}\). Weighted cross-entropy loss is used for training.

\[\mathcal{L}_{I}=\frac{1}{\left|\mathcal{Y}_{train}\right|}\sum_{y^{k}\in \mathcal{Y}_{train}}\left(\sum\nolimits_{i=0}^{2N_{f}^{k}}\text{log}p_{i}^{k}-( 1-y_{i}^{k})\text{log}(1-p_{i}^{k})\right).\] (16)Experiments

### Experimental setup

Datasets and processing.We evaluate our method on the complexes from Docking Benchmark 5.5 (DB5.5) [45], a gold standard dataset with high-quality, and Database of Interacting Protein Structures (DIPS) [43], which collects 41,876 complexes mined from PDB [4]. DB5.5 only contains 253 complex structures manually curated by domain experts, which cover both native unbound and bound structures. In comparison, DIPS has a significantly larger data size, but it only includes bound structures of proteins. The two datasets are randomly divided into training, validation, and testing sets with the following sizes: 203/25/25 (DB5.5) and 39,937/974/965 (DIPS).

For both datasets, we test using various versions of protein structures as inputs for PIP. To accomplish this, we prepared three versions of the testing set for DB5.5, including native unbound structures, structures produced by AlphaFold2, and structures produced by ESMFold. We use Native-Bound, Native-Unbound, ESMFold, AlphaFold2 to denote these four settings respectively.

As for DIPS, since it does not have native unbound structures, we only used ESMFold to prepare unbound structure inputs for its testing set. AlphaFold2 is not considered due to its high computational cost for the entire testing set of DIPS.

Baselines.We compare our ATProt method with state-of-the-art conventional machine learning method BIPSPI [36], the CNN-based methods Siamese Atomic Surfacelet Network (SASNet) [43], Diffusion-Convolutional Neural Networks (DCNN) [2], differentiable molecular surface interaction fingerprinting (dMaSIF) [39], and a set of GNN-based methods Deep Tensor Neural Networks (DTNN) [37], and NEA [15].

Setup and metrics.We consider three experimental setups: (1) performing training and testing both on the DB5.5; (2) performing training and testing both on the DIPS; and (3) performing pre-training on the DIPS, fine-tuning on the DB5.5 and testing on the DB5.5. For our proposed ATProt method, we consider graph encoders of Simple GCN, Chebynet, Low-pass filter and BernNet (denoted as ATProt-SGC, ATProt-Cheby, ATProt-LPF, and ATProt-Bern, respectively).

For each complex in the testing set, assuming that the two proteins have \(M\) and \(N\) residues, respectively, we test all its \(M\times N\) binary classification samples and calculate the Area Under the ROC Curve (AUC) value. Following [15], we report the median AUC score (MedAUC) across all complexes as the final evaluation metric.

Results.Table 1, 2, and Table 3 (shown in Appendix) show the model performance for PIP. We find that our method is competitive and outperforms the majority of baseline methods with native bound structures. Under this Native-Bound setting, although ATProt is slightly less effective than BIPSPI, it demonstrates the ability to learn sufficiently powerful protein representations (especially with BernNet and Chebynet). Notably, all the baselines fail significantly when using non-bound structures

\begin{table}
\begin{tabular}{l|c|c c c} \hline \hline Methods & Native-Bound & Native-Unbound & ESMFold & AlphaFold2 \\ \hline BIPSPI [36] & **0.937 (0.008)** & 0.911 (0.017) & 0.896 (0.008) & 0.887 (0.013) \\ SASNet [43] & 0.902 (0.007) & 0.876 (0.017) & 0.887 (0.025) & 0.881 (0.020) \\ dMaSIF [39] & 0.928 (0.005) & 0.912 (0.009) & 0.906 (0.003) & 0.892 (0.012) \\ DTNN [37] & 0.912 (0.005) & 0.886 (0.007) & 0.883 (0.010) & 0.878 (0.021) \\ NEA [15] & 0.916 (0.015) & 0.895 (0.009) & 0.902 (0.010) & 0.883 (0.012) \\ \hline
**ATProt-SGC** & 0.925 (0.015) & 0.918 (0.004) & 0.909 (0.012) & 0.924 (0.014) \\
**ATProt-Cheby** & 0.928 (0.017) & 0.922 (0.007) & 0.922 (0.005) & 0.924 (0.011) \\
**ATProt-LPF** & 0.915 (0.017) & 0.919 (0.019) & 0.911 (0.009) & 0.911 (0.010) \\
**ATProt-Bern** & 0.932 (0.017) & **0.928 (0.014)** & **0.929 (0.014)** & **0.925 (0.011)** \\ \hline \hline ATProt-Bern w/o SR & 0.934 (0.009) & 0.901 (0.011) & 0.897 (0.010) & 0.901 (0.012) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Training and testing on the DB5.5.** Mean and standard deviation values of the MedAUC scores of all baselines, computed from three random seeds. The best performance is in **bold** and the second best one is underlined. ‘SR’ means the proposed stable regularization \(\mathcal{L}_{S}\).

[MISSING_PAGE_FAIL:9]

## Acknowledgement

This work was supported by HKUST-HKUST(GZ) 20 for 20 Crosscampus Collaborative Research Scheme C019, and Shenzhen Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone under Grant No. HTHZQSWS-KCCYB-2023052.

## References

* [1]M. Andriushchenko and N. Flammarion (2020) Understanding and improving fast adversarial training. Advances in Neural Information Processing Systems33, pp. 16048-16059. Cited by: SS1.
* [2]J. Atwood and D. Towsley (2016) Diffusion-convolutional neural networks. Advances in neural information processing systems29. Cited by: SS1.
* [3]A. Bakan, L. M. Meireles, and I. Bahar (2011) Prody: protein dynamics inferred from theory and experiments. Bioinformatics27 (11), pp. 1575-1577. Cited by: SS1.
* [4]H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weissig, I. N. Shindyalov, and P. E. Bourne (2000) The protein data bank. Nucleic acids research28 (1), pp. 235-242. Cited by: SS1.
* 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. Cited by: SS1.
* [6]A. Chen, Y. Yao, P. Chen, Y. Zhang, and S. Liu (2023) Understanding and improving visual prompting: a label-mapping perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19133-19143. Cited by: SS1.
* [7]A. Chen, Y. Zhang, J. Jia, J. Diffenderfer, J. Liu, K. Parasyris, Y. Zhang, Z. Zhang, B. Kailkhura, and S. Liu (2023) Deepzero: scaling up zeroth-order optimization for deep model training. arXiv preprint arXiv:2310.02025. Cited by: SS1.
* [8]M. Chen, C. J. Ju, G. Zhou, X. Chen, T. Zhang, K. Chang, C. Zaniolo, and W. Wang (2019) Multifaceted protein-protein interaction prediction based on siamese residual rcnn. Bioinformatics35 (14), pp. i305-i314. Cited by: SS1.
* [9]L. Chi, B. Jiang, and Y. Mu (2020) Fast fourier convolution. Advances in Neural Information Processing Systems33, pp. 4479-4488. Cited by: SS1.
* [10]L. Chu, J. A. Ruffolo, A. Harmalkar, and J. J. Gray (2024) Flexible protein-protein docking with a multitrack iterative transformer. Protein Science33 (2), pp. e4862. Cited by: SS1.
* [11]K. L. Damm and H. A. Carlson (2006) Gaussian-weighted rmsd superposition of proteins: a structural comparison for flexible proteins and predicted protein structures. Biophysical journal90 (12), pp. 4558-4573. Cited by: SS1.
* [12]S. J. de Vries, C. E. Schindler, I. Chauvot de Beauchene, and M. Zacharias (2015) A web interface for easy flexible protein-protein docking with attract. Biophysical journal108 (3), pp. 462-465. Cited by: SS1.
* [13]M. Defferrard, X. Bresson, and P. Vandergheynst (2016) Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems29. Cited by: SS1.

[MISSING_PAGE_POST]

* [16] Paul J Gane and Philip M Dean. Recent advances in structure-based rational drug design. _Current opinion in structural biology_, 10(4):401-404, 2000.
* [17] Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi Jaakkola, and Andreas Krause. Independent se (3)-equivariant models for end-to-end rigid protein docking. _arXiv preprint arXiv:2111.07786_, 2021.
* [18] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. _Journal of machine learning research_, 17(59):1-35, 2016.
* [19] Ziqi Gao, Chenran Jiang, Jiawen Zhang, Xiaosen Jiang, Lanqing Li, Peilin Zhao, Huanming Yang, Yong Huang, and Jia Li. Hierarchical graph learning for protein-protein interaction. _Nature Communications_, 14(1):1093, 2023.
* [20] Ziqi Gao, Xiangguo Sun, Zijing Liu, Yu Li, Hong Cheng, and Jia Li. Protein multimer structure prediction via prompt learning. _arXiv preprint arXiv:2402.18813_, 2024.
* [21] Ziqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu, Bingzhe Wu, Liang Chen, and Jia Li. Parameter-efficient fine-tuning with discrete fourier transform. _arXiv preprint arXiv:2405.03003_, 2024.
* [22] Vladimir Gligorijevic, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structure-based protein function prediction using graph convolutional networks. _Nature communications_, 12(1):3168, 2021.
* [23] Mingguo He, Zhewei Wei, Hongteng Xu, et al. Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. _Advances in Neural Information Processing Systems_, 34:14239-14251, 2021.
* [24] Yufei Huang, Siyuan Li, Lirong Wu, Jin Su, Haitao Lin, Odin Zhang, Zihan Liu, Zhangyang Gao, Jiangbin Zheng, and Stan Z Li. Protein 3d graph structure learning for robust structure-based protein property prediction. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 12662-12670, 2024.
* [25] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* [26] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [27] Daniel E Koshland Jr. The key-lock theory and the induced fit theory. _Angewandte Chemie International Edition in English_, 33(23-24):2375-2378, 1995.
* [28] Jia Li, Jiajin Li, Yang Liu, Jianwei Yu, Yueting Li, and Hong Cheng. Deconvolutional networks on graph data. _Advances in Neural Information Processing Systems_, 34:21019-21030, 2021.
* [29] Jia Li, Honglei Zhang, Zhichao Han, Yu Rong, Hong Cheng, and Junzhou Huang. Adversarial attack on community detection by hiding individuals. In _Proceedings of The Web Conference 2020_, pages 917-927, 2020.
* [30] Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. Zerog: Investigating cross-dataset zero-shot transferability in graphs. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1725-1735, 2024.
* [31] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020.
* [32] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candidido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. _bioRxiv_, 2022.

* [33] Yi Liu, Hao Yuan, Lei Cai, and Shuiwang Ji. Deep learning of high-order interactions for protein interface prediction. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 679-687, 2020.
* [34] Raksha Ramakrishna, Hoi-To Wai, and Anna Scaglione. A user guide to low-pass graph signal processing and its applications: Tools and applications. _IEEE Signal Processing Magazine_, 37(6):74-85, 2020.
* [35] Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. _Journal of computational and applied mathematics_, 20:53-65, 1987.
* [36] Ruben Sanchez-Garcia, Carlos Oscar Sanchez Sorzano, Jose Maria Carazo, and Joan Segura. Bipspi: a method for the prediction of partner-specific protein-protein interfaces. _Bioinformatics_, 35(3):470-477, 2019.
* [37] Kristof T Schutt, Farhad Arbabzadah, Stefan Chmiela, Klaus R Muller, and Alexandre Tkatchenko. Quantum-chemical insights from deep tensor neural networks. _Nature communications_, 8(1):13890, 2017.
* [38] Woody Sherman, Tyler Day, Matthew P Jacobson, Richard A Friesner, and Ramy Farid. Novel procedure for modeling ligand/receptor induced fit effects. _Journal of medicinal chemistry_, 49(2):534-553, 2006.
* [39] Freyr Sverrisson, Jean Feydy, Bruno E Correia, and Michael M Bronstein. Fast end-to-end learning on protein surfaces. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15272-15281, 2021.
* [40] Jianheng Tang, Fengrui Hua, Ziqi Gao, Peilin Zhao, and Jia Li. Gadbench: Revisiting and benchmarking supervised graph anomaly detection. _Advances in Neural Information Processing Systems_, 36:29628-29653, 2023.
* [41] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly detection. In _International Conference on Machine Learning_, pages 21076-21089. PMLR, 2022.
* [42] Mieczyslaw Torchala, Iain H Moal, Raphael AG Chaleil, Juan Fernandez-Recio, and Paul A Bates. Swarmdock: a server for flexible protein-protein docking. _Bioinformatics_, 29(6):807-809, 2013.
* [43] Raphael Townshend, Rishi Bedi, Patricia Suriana, and Ron Dror. End-to-end learning on 3d protein structure for interface prediction. _Advances in Neural Information Processing Systems_, 32, 2019.
* [44] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. _Journal of machine learning research_, 9(11), 2008.
* [45] Thom Vreven, Iain H Moal, Anna Vangone, Brian G Pierce, Panagiotis L Kastritis, Mieczyslaw Torchala, Raphael Chaleil, Brian Jimenez-Garcia, Paul A Bates, Juan Fernandez-Recio, et al. Updates to the integrated protein-protein interaction benchmarks: docking benchmark version 5 and affinity benchmark version 2. _Journal of molecular biology_, 427(19):3031-3041, 2015.
* [46] Yiqun Wang, Yuning Shen, Shi Chen, Lihao Wang, YE Fei, and Hao Zhou. Learning harmonic molecular representations on riemannian manifold. In _The Eleventh International Conference on Learning Representations_.
* [47] Lennart Wirthmueller, Abbas Maqbool, and Mark J Banfield. On the front line: structural insights into plant-pathogen interactions. _Nature Reviews Microbiology_, 11(11):761-776, 2013.
* [48] Chyuan-Chuan Wu, Tsai-Kun Li, Lynn Farh, Li-Ying Lin, Te-Sheng Lin, Yu-Jen Yu, Tien-Jui Yen, Chia-Wang Chiang, and Nei-Li Chan. Structural basis of type ii topoisomerase inhibition by the anticancer drug etoposide. _Science_, 333(6041):459-462, 2011.
* [49] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In _International conference on machine learning_, pages 6861-6871. PMLR, 2019.

* [50] Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. Graph wavelet neural network. _arXiv preprint arXiv:1904.07785_, 2019.
* [51] Yumeng Yan, Di Zhang, Pei Zhou, Botong Li, and Sheng-You Huang. Hdock: a web server for protein-protein and protein-dna/rna docking based on a hybrid strategy. _Nucleic acids research_, 45(W1):W365-W373, 2017.
* [52] Zheng Yuan, Timothy L Bailey, and Rohan D Teasdale. Prediction of protein b-factor profiles. _Proteins: Structure, Function, and Bioinformatics_, 58(4):905-912, 2005.
* [53] Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. _arXiv preprint arXiv:2203.06125_, 2022.

**Supplementary of "Towards Stable Representations for Protein Interface Prediction"**

## Appendix A Representing proteins with graph data

Let \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) be an undirected graph with nodes \(\mathcal{V}=\{1,...,N\}\), edges \(\mathcal{E}\subset\mathcal{V}^{2}\), graph signal \(\bm{F}\in\mathbb{R}^{N\times d}\) and a graph shift operator \(\bm{L}\in\mathbb{R}^{N\times N}\) (i.e., node connectivity). We construct edges with the k-nearest neighbor (k-NN) algorithm and each node in \(\mathcal{G}\) is connected to the 10 closest nodes within a physical distance of less than 30 A. The edge attributes are distances between \(\alpha\)-carbon atoms encoded with Gaussian basis functions. The nodes have two kinds of attributes: the one-hot encoding of amino acid type and the surface-aware features at the residue level. The latter is defined by [17] to distinguish residues closer to the protein surface from those in the interior. Notably, we do not introduce any biochemically related attributes of atoms or amino acids featurization. Instead, protein graphs are constructed solely using their \(\alpha\)-carbon coordinates.

## Appendix B Proofs of main propositions

### Proof of Proposition 3.1

Assuming we model the protein interface prediction as a pairwise classification problem. Basically, this proposition provides a perturbation radius, within which if the perturbations of two protein representations fall, the predicted correspondences of all pairs of inter-protein residues will be robust.

Formally, we apply the model \(\Psi\) to obtain \(d\)-dimensional protein representations \(\bm{H}_{1}=\Psi(\mathcal{P}_{1})\in\mathbb{R}^{M\times d},\bm{H}_{2}=\Psi( \mathcal{P}_{2})\in\mathbb{R}^{N\times d}\). The \(i\)-th row of \(\bm{H}_{1}\) and \(j\)-th row of \(\bm{H}_{2}\) are concatenated into \(\bm{x}^{(ij)}\in\mathbb{R}^{2d}\), which is the pairwise residue representations. We then use a neural network \(f\) to calculate the probability of the presence of residue correspondence, and define the classifier as \(g(\bm{x}^{(ij)}):=\text{arg max}_{k}f_{k}(\bm{x}^{(ij)})\). Assuming that the structure changes of two proteins cause perturbations \(\delta_{1},\delta_{2}\) on \(\bm{H}_{1}\) and \(\bm{H}_{2}\), respectively, the interface prediction robustness can be determined by the following proposition. First, we rewrite Proposition 3.1 into a more formal expression.

**Proposition B.1**.: _For all \(i\in[1,M]\cap\mathbb{Z}\) and \(j\in[1,N]\cap\mathbb{Z}\), the interface classifier \(g\) is provably robust for arbitrary \(\bm{x}^{(ij)}\) (i.e., \(g\) is robust for all residue pairs of \(\mathcal{P}_{1}\) and \(\mathcal{P}_{2}\)) if \(N\left\lVert\delta_{1}\right\rVert_{p}^{p}+M\left\lVert\delta_{2}\right\rVert_ {p}^{p}<\mathcal{A}(f,p)\), where \(\mathcal{A}(f,p)\) can be a constant depending only on \(f\) and the norm \(\left\lVert\cdot\right\rVert_{p}\)._

Proof.:. We denote \(\bm{x}\) as a variable for convenience to represent the pairwise residue representation. Let \(g(\bm{x})=y\). Suppose there exists a perturbation \(\bm{\delta}\) such that \(g(\bm{x}+\bm{\delta})\neq g(\bm{x})\) and \(f_{j}(\bm{x}+\bm{\delta})\geq f_{y}(\bm{x}+\bm{\delta})\) for some \(j\neq y\). We first prove that \(\left\lVert\delta\right\rVert_{p}\geq\frac{\sqrt{2}}{2C}\cdot\text{margin}(f( \bm{x}))\), where \(C\) is the lipschitz constant of \(f\), \(\text{margin}(f(\bm{x}))\) is the margin between the largest and second runner-up output logits. Define \(\bm{z}^{\prime}=f(\bm{x}+\bm{\delta})\), then \(z_{y}^{\prime}\leq z_{j}^{\prime}\). The difference between outputs \(\bm{z}\) and \(f(\bm{x})\) can be lower bounded by

\[\left\lVert\bm{z}^{\prime}-f(\bm{x})\right\rVert_{p}\geq\left\lVert(z_{y}^{ \prime},z_{j}^{\prime})^{T}-([f_{y}(\bm{x}),f_{j}(\bm{x})])^{T}\right\rVert=( |z_{y}^{\prime}-f_{y}(\bm{x})|^{p}+|z_{j}^{\prime}-f_{j}(\bm{x})|^{p})^{\frac{ 1}{p}}\] (17)

In the above equation, we utilize the fact that setting certain elements of a vector to zero can only decrease its \(p\)-norm. Let us now consider the following optimization problem:

\[\min_{\bm{z}^{\prime}}|z_{y}^{\prime}-f_{y}(\bm{x})|^{p}+|z_{j}^{\prime}-f_{j} (\bm{x})|^{p}\qquad\textbf{s.t.}\ z_{\text{y}}^{\prime}\leq z_{\text{j}}^{\prime}\] (18)

When \(z_{y}^{\prime}=z_{j}^{\prime}=(f_{y}(\bm{x})+f_{j}(\bm{x}))/2\), we have the minimum of (2) and the update for (11):

\[\left\lVert\bm{z}^{\prime}-f(\bm{x})\right\rVert_{p}\geq\left\lVert(z_{y}^{ \prime},z_{j}^{\prime})^{T}-([f_{y}(\bm{x}),f_{j}(\bm{x})])^{T}\right\rVert\geq \frac{\sqrt[p]{2}}{2}(f_{y}(\bm{x})-f_{j}(\bm{x}))\] (19)According to the Lipschitz constant of \(f\), we have

\[\left\|\bm{z}^{\prime}-f(\bm{x})\right\|_{p}\leq C\left\|\delta\right\|_{p}\] (20)

Considering (3) and (4), we have the initial conclusion as follows

\[\left\|\delta\right\|_{p}^{p}\geq\left(\frac{\sqrt[p]{2}}{2C}\cdot\text{margin }(f(\bm{x}))\right)^{p}\] (21)

Then we consider the complete residue pair representation set \(\{\bm{x}^{(ij)}\}_{i\in[1,M]\cap\mathbb{Z},j\in[1,N]\cap\mathbb{Z}}\).

\[\sum_{i=1}^{M}\sum_{j=1}^{N}\left\|\delta^{(ij)}\right\|_{p}^{p}\geq\sum_{i=1} ^{M}\sum_{j=1}^{N}\left(\frac{\sqrt[p]{2}}{2C}\cdot\text{margin}(f(\bm{x}^{(ij )}))\right)^{p}\] (22)

We denote the perturbations on protein representations \(\bm{H}_{1}\) and \(\bm{H}_{2}\) as \(\delta_{1}\) and \(\delta_{2},\) respectively. \(\delta^{(ij)}\) is actually the concatenation of the \(i\)-th row of \(\delta_{1}\) and the \(i\)-th row of \(\delta_{2}\), i.e., \(\delta^{(ij)}=\operatorname{AGG}(\delta_{1},\delta_{2})\). Thus the left term of (6) is equal to \(N\left\|\delta_{1}\right\|_{p}^{p}+M\left\|\delta_{2}\right\|_{p}^{p}\). Finally, we conclude the proof:

\[N\left\|\delta_{1}\right\|_{p}^{p}+M\left\|\delta_{2}\right\|_{p}^{p}\geq\sum_ {i=1}^{M}\sum_{j=1}^{N}\left(\frac{\sqrt[p]{2}}{2C}\cdot\text{margin}(f(\bm{x }^{(ij)}))\right)^{p}=MN\left(\frac{\sqrt[p]{2}}{2C}\cdot\text{margin}^{*}(f( \bm{x}))\right)^{p}\] (23)

where \(\text{margin}^{*}(f(\bm{x}))\) is the average value of all margins w.r.t the \(MN\) concatenated residue representations. 

### Proof of the statement after Theorem 3.1 that \(C_{h}^{*}\) is equal to the MAS.

Proof.: We prove this based on the definition of graph Lipschitz filter, which is continuously differentiable due to the polynomial approximation.

Let \(C_{h}\) be the Lipschitz constant of \(h\), and let \(u(\lambda)=|h^{\prime}(\lambda)|\). We want to show that \(C_{h}\) is minimized when \(C_{h}=\sup_{\lambda}u(\lambda)\).

Suppose there exist \(\lambda_{1}\) and \(\lambda_{2}\) such that \(|\lambda_{1}-\lambda_{2}|>0\) and \(|h(\lambda_{1})-h(\lambda_{2})|>C_{h}|\lambda_{1}-\lambda_{2}|\). Then, by the mean value theorem, there exists \(\lambda_{3}\) between \(\lambda_{1}\) and \(\lambda_{2}\) such that \(|h^{\prime}(\lambda_{3})|>C_{h}\). But this contradicts the assumption that \(C_{h}\) is the Lipschitz constant of \(h\). Therefore, \(C_{h}\) must be greater than or equal to \(\sup_{\lambda}u(\lambda)\).

To show that \(C_{h}\) is minimized when \(C_{h}=\sup_{\lambda}u(\lambda)\), suppose there exists a Lipschitz constant \(C_{h}^{\prime}\) such that \(C_{h}^{\prime}<\sup_{\lambda}u(\lambda)\). Then, for any \(\lambda_{1}\) and \(\lambda_{2}\), we have

\[\begin{split}|h(\lambda_{1})-h(\lambda_{2})|&\leq C_ {h}^{\prime}|\lambda_{1}-\lambda_{2}|\\ &<\sup_{\lambda}u(\lambda)|\lambda_{1}-\lambda_{2}|\\ &\leq|h^{\prime}(\lambda_{3})||\lambda_{1}-\lambda_{2}|\end{split}\] (24)

where \(\lambda_{3}\) is some point between \(\lambda_{1}\) and \(\lambda_{2}\). But this contradicts the definition of \(u(\lambda)\) as the maximum of \(|h^{\prime}(\lambda)|\). Therefore, \(C_{h}\) must be equal to \(\sup_{\lambda}u(\lambda)\).

Thus, we have shown that \(C_{h}\) is minimized when \(C_{h}=\sup_{\lambda}u(\lambda)\), as desired. For readability, we denote the minimum of \(C_{h}\) as \(C_{h}^{*}\) and \(\sup_{\lambda}u(\lambda)\) as the MAS (maximum absolute slope) of \(h\), respectively. 

### Proof of Proposition 4.1

We start the analysis with the unique stability-preservation property of the Bernstein basis shown in Theorem 4.1. Simply put, during Bernstein polynomial approximation, the outcome polynomial \(h(\lambda)\) is at least as stable as the target function \(f(\lambda)\). This is crucial as it tells that the MAS of an exact function \(f(\lambda)\) can always serve as an upper bound for \(C_{h}^{*}\).

The relationship between \(f(\lambda)\) and \(\{\theta_{k}\}_{k=0}^{K}\).Setting the polynomial order to \(K\), our model learns a set of \(K+1\) weights \(\{\theta_{k}\}_{k=0}^{K}\) that acts as the coefficients for polynomial approximation, such that \(f(\frac{2k}{K})=\theta_{k}\), for all \(k\in[0,K]\cap\mathbb{Z}\). Put differently, any 2-D curve passing through all points of \((0/K,\theta_{0}),(2/K,\theta_{1})..,(2K/K,\theta_{K})\) can be regarded as a possible version of \(f(\lambda)\) for polynomial approximation. Consequently, given any set of \(\{\theta_{k}\}_{k=0}^{K}\), the stability of \(h(\lambda)\) is not worse than the most stable version among all possible \(f(\lambda)\).

Formally, we naturally extend the Theorem 4.1 to the following proposition, which provides a theoretical upper bound for \(C_{h}^{*}\) of \(h(\lambda)\), directly using the learnable coefficients \(\{\theta_{k}\}_{k=0}^{K}\).

As \(h\) is at least as stable as the most stable \(f\), the MAS of \(h\) can be upper bounded by the minimum MAS among all possible \(f\) functions, which is infinitely close to the MAS of the broken line passing through points \(\{(k/K,\theta_{k})\}_{k=0}^{K}\). Figure 6 helps to better understand Proposition 4.1. For example, let us set \(K\) to 1, any curve passing through \((0,\theta_{0})\) and \((2,\theta_{1})\) can be a version of \(f\). Out of all possible \(f\) versions, the line segment directly connecting two points has the minimum MAS (i.e., \(\frac{|\theta_{0}-\theta_{1}|}{2}\)).

We have introduced in the main text the relationship between \(f(\lambda)\) and \(\{\theta\}_{k=0}^{K}\). Based on this, the proof of Proposition 3.2 is equivalent to proving the following lemma.

**Lemma B.1**.: _Given \(K+1\) points \((0,\theta_{0})\), \((\frac{2}{K},\theta_{1})\),..., \((\frac{2K}{K},\theta_{K})\), suppose there are infinitely many functions \(f(\lambda)\) that pass through these \(K+1\) points. The maximum absolute slope (MAS) of any version of a function \(f(\lambda)\) is not less than the MAS of the piecewise linear function passing through these \(K+1\) points._

Proof.: Let \(f_{broken}(\lambda)\) be the piecewise linear function passing through the given \(K+1\) points. The slope of \(f_{broken}(\lambda)\) between two consecutive points \((\frac{2i}{K},\theta_{i})\) and \((\frac{2(i+1)}{K},\theta_{i+1})\) is given by:

\[m_{i}=\frac{\theta_{i+1}-\theta_{i}}{\frac{2(i+1)}{K}-\frac{2i}{K}}=\frac{ \theta_{i+1}-\theta_{i}}{\frac{2}{K}}\] (25)

The MAS of \(f_{broken}(\lambda)\) is the maximum of the absolute values of these slopes:

\[\text{MAS}_{broken}=\max_{0\leq i\leq K-1}|m_{i}|\] (26)

Now, consider any function \(f(\lambda)\) that passes through the \(K+1\) points. Since \(f(\lambda)\) is differentiable, by the mean value theorem, for each interval \(\left[\frac{2i}{K},\frac{2(i+1)}{K}\right]\), there exists a point \(\lambda_{i}\) such that:

\[f^{\prime}(\lambda_{i})=\frac{f\left(\frac{2(i+1)}{K}\right)-f\left(\frac{2i} {K}\right)}{\frac{2(i+1)}{K}-\frac{2i}{K}}=\frac{\theta_{i+1}-\theta_{i}}{ \frac{2}{K}}=m_{i}\] (27)

Since \(f(\lambda)\) passes through all the given points, we have:

\[\text{MAS}_{f}\geq\max_{0\leq i\leq K-1}|f^{\prime}(\lambda_{i})|=\max_{0\leq i \leq K-1}|m_{i}|=\text{MAS}_{broken}\] (28)

Thus, the MAS of any function \(f(\lambda)\) satisfying the conditions is greater than or equal to the MAS of the piecewise linear function \(f_{broken}(\lambda)\) passing through the points.

Figure 6: The way to find upper bound of \(C_{h}^{*}\), a case to explain Proposition 4.1.

[MISSING_PAGE_EMPTY:17]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The contributions are included in the abstract and the introduction.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 6.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: See Section B.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Section 5
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is provided in https://github.com/ATProt/ATProt.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 5.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In our experiments, we report the mean and standard deviation of three random seeds.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Section Appendix E.
9. **Code Of Ethics**Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This research conducted in the paper conforms with the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are some potential societal consequences of our work, none of which we feel must be specifically highlighted here.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no safety risks.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The materials in this paper are used with permission and properly cited. The Database of Interacting Protein Structures (DIPS) is under the MIT License. DB 5.5 is under a Creative Commons Attribution 4.0 International License
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The developed code is provided in https://github.com/ATProt/ATProt.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing or research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects.