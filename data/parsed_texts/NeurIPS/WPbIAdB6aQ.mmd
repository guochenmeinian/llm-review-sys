# A Robust Exact Algorithm for the Euclidean Bipartite Matching Problem

Akshaykumar G. Gattani

Department of Computer Science, Virginia Tech

Sharath Raghvendra

Department of Computer Science, Virginia Tech

Pouyan Shirzadian

Department of Computer Science, Virginia Tech

###### Abstract

Algorithms for the minimum-cost bipartite matching can be used to estimate Wasserstein distance between two distributions. Given two sets \(A\) and \(B\) of \(n\) points in a \(2\)-dimensional Euclidean space, one can use a fast implementation of the Hungarian method to compute a minimum-cost bipartite matching of \(A\) and \(B\) in \(\tilde{O}(n^{2})\) time. Let \(\Delta\) be the spread, i.e., the ratio of the distance of the farthest to the closest pair of points in \(A\cup B\). In this paper, we present a new algorithm to compute a minimum-cost bipartite matching of \(A\) and \(B\) with a similar worst-case execution time of \(\tilde{O}(n^{2}\log\Delta)\). However, when \(A\) and \(B\) are drawn independently and identically from a fixed distribution that is not known to the algorithm, the execution time of our algorithm is, in expectation, \(\tilde{O}(n^{7/4}\log\Delta)\).

To the best of our knowledge, our algorithm is the first one to achieve a subquadratic execution time even for stochastic point sets with real-valued coordinates. Our algorithm extends to any dimension \(d\), where it runs in \(\tilde{O}(n^{2-\frac{1}{2d}}\Phi(n))\) time for stochastic point sets \(A\) and \(B\); here \(\Phi(n)\) is the query/update time of a dynamic weighted nearest neighbor data structure. Our algorithm can be seen as a careful adaptation of the Hungarian method in the geometric divide-and-conquer framework.

## 1 Introduction

Given two distributions \(\mu\) and \(\nu\) defined on the sets \(S_{\mu},S_{\nu}\subseteq\mathbb{R}^{d}\), respectively, and an integer \(p\geq 1\), the \(p\)-Wasserstein distance between \(\mu\) and \(\nu\) is the minimum cost required to transport mass from one to the other. More formally, let \(\Gamma(\mu,\nu)\) be the set of all probability measures on \(S_{\mu}\times S_{\nu}\) with marginal distributions \(\mu\) and \(\nu\). The \(p\)-_Wasserstein distance_ between \(\mu\) and \(\nu\) is defined as

\[W_{p}(\mu,\nu)=\left(\inf_{\gamma\in\Gamma(\mu,\nu)}\int_{S_{\mu}\times S_{\nu }}\|s_{1}-s_{2}\|^{p}d\gamma(s_{1},s_{2})\right)^{1/p},\]

where \(\|s_{1}-s_{2}\|\) denotes the Euclidean distance of \(s_{1}\) and \(s_{2}\).

In many applications, the distribution \(\mu\) (resp. \(\nu\)) may not be known, but one may have access to \(n\) i.i.d samples \(A\) (resp. \(B\)) from \(\mu\) (resp. \(\nu\)). The _empirical distribution_\(\mu_{n}\) (resp. \(\nu_{n}\)) is a discrete distribution with \(A\) (resp. \(B\)) as support, where each sample has a mass of \(1/n\). The empirical \(p\)-Wasserstein distance is simply the \(p\)-Wasserstein distance between the empirical distributions \(\mu_{n}\) and \(\nu_{n}\). As \(n\to\infty\), the empirical \(p\)-Wasserstein distance between \(\mu\) and \(\nu\) converges to the real \(p\)-Wasserstein distance between them [7, 15, 16, 46] with several results showing sharp upper and lower bounds on the empirical \(p\)-Wasserstein distances [10, 17, 19, 45, 49]. See also [36]for a survey on such results. Due to these properties, empirical \(p\)-Wasserstein distance has found applications in training generative adversarial networks [8, 9, 24, 33, 34, 35], image retrieval [39, 42], graph predictions [27, 37], clustering stability validation [18, 31], and two-sample tests [13, 14, 23, 25, 41]. The convergence of the empirical distribution to the real distribution, however, exhibits the "curse of dimensionality"; that is, the convergence rate decreases as the dimension increases [16, 49]. The empirical \(p\)-Wasserstein distance therefore is most useful in low-dimensional settings.

Computing the empirical \(p\)-Wasserstein distance can be done by solving an instance of the minimum-cost bipartite matching problem as follows. Consider the complete bipartite graph on \(A\) and \(B\) where each edge from \(a\in A\) to \(b\in B\) has a cost of \(\|a-b\|^{p}\). A _bipartite matching_ (or simply a _matching_) \(M\) on \(A\cup B\) is a set of vertex-disjoint edges of \(\mathcal{G}(A,B)\). The matching \(M\) is said to be a _perfect matching_ if \(|M|=n\). We define the cost of a matching \(M\), denoted by \(w_{p}(M)\), as \(w_{p}(M)=\sum_{(a,b)\in M}\|a-b\|^{p}\). The _pth power Euclidean bipartite matching_ is a perfect matching \(M_{p}^{*}\) with the minimum-cost under \(w_{p}(\cdot)\). The empirical \(p\)-Wasserstein distance between \(\mu\) and \(\nu\) is

\[W_{p}(\mu_{n},\nu_{n})=\big{(}\tfrac{1}{n}w_{p}(M_{p}^{*})\big{)}^{1/p}.\]

The minimum-cost matching \(M_{p}^{*}\) and therefore, the empirical \(p\)-Wasserstein distance can be computed in \(O(n^{3})\) time using the Hungarian algorithm [28]. In this paper, we adapt the Hungarian algorithm in a geometric divide-and-conquer framework. The execution time of our algorithm is similar to that of the Hungarian algorithm (Remark 3.2). However, when the input point sets \(A\) and \(B\) are samples drawn from the same distribution, the algorithm is asymptotically faster than the Hungarian algorithm.

**Related Work.** Computing an optimal minimum-cost bipartite matching can be done in \(O(n^{3})\) time using the classical Hungarian algorithm [28]. In geometric settings, the efficiency of the Hungarian algorithm can be improved to \(\tilde{O}(n^{2}\Phi(n))\), where \(\Phi(n)\) is the query and update time of a dynamic weighted nearest neighbor data structure [3, 44, 48]. For two dimensions, \(\Phi(n)=\log^{O(1)}n\) and therefore, we get a \(\tilde{O}(n^{2})\) time exact algorithm. For higher dimensions, however, this leads to only slightly sub-cubic execution time.

For graphs with \(n\) vertices and \(m\) edges, there are faster algorithms that compute an optimal minimum-cost matching [21, 21, 11] provided the edge costs are integers that are bounded by \(C^{1}\). However, in our setting, the coordinates of the input points are real-valued and the Euclidean distances, due to the presence of a square-root, can be irrational. As a result, these algorithms only provide an approximate solution and the Hungarian algorithm and its fast implementations remain the only known exact algorithm for the problem.

For \(2\)-dimensional points with integer coordinates bounded by \(\Gamma\), Sharathkumar [43] presented a _weakly polynomial_\(\tilde{O}(n^{3/2}\log\Gamma)\) time algorithm to compute the exact \(1\)-Wasserstein distance. The main idea was to use the approximation algorithm by Sharathkumar and Agarwal [44] to initially find a planar sub-graph that traps all the edges of the optimal matching. Then, they use the planar separator based algorithm by Lipton and Tarjan [32] to find a minimum-cost matching in this sub-graph. This algorithm, however, is restricted only to two dimensions and for \(p=1\). The design of a faster algorithm to compute the \(p\)th power Euclidean bipartite matching for \(d\)-dimensional point sets remains a major open problem. We would also note the extensive work on the design of approximation algorithms for the \(1\)-Wasserstein distance [2, 4, 5, 6, 12, 20, 26, 30, 38, 40] as well as the \(2\)-Wasserstein distance [1, 29].

**Our Results.** For any point set \(P\) in the Euclidean space, the _spread_ of \(P\) is the ratio of the distance of the farthest pair to the closest pair in \(P\). The main result of this paper is presented in Theorem 1.1.

**Theorem 1.1**.: _There exists a randomized algorithm that, given any two point sets \(A\) and \(B\) sampled independently and identically from a distribution \(\mu\) inside the unit \(d\)-dimensional hypercube, where \(|A|=|B|=n\) and \(\mu\) is not known to the algorithm, computes an exact Euclidean bipartite matching between \(A\) and \(B\) and has an expected running time of \(\tilde{O}(n^{2-\frac{1}{2d}}\Phi(n)\log\Delta)\); here, \(\Delta\) is the spread of the points in \(A\cup B\)._

When \(A\) and \(B\) are i.i.d samples from a distribution \(\mu\) in the unit square (\(d=2\)) and \(\mu\) is not known to the algorithm, our algorithm achieves a weakly polynomial sub-quadratic execution time of \(\tilde{O}(n^{7/4}\log\Delta)\). Our algorithm easily extends to any constant \(p>1\) and any fixed \(d\)-dimensionalspace (we present our result for any arbitrary \(p\) as well as the proof of all our claims in the Appendix). For instance, for \(p=2\) and \(d=2\) as well as for \(p=1\) and \(d=3\), the execution time of our algorithm can be bounded by \(\tilde{O}(n^{11/6}\Phi(n)\log\Delta)\). However, for simplicity in exposition of the ideas, we restrict the presentation of our algorithm to two-dimensional (\(d=2\)) and Euclidean settings \((p=1)\). To the best of our knowledge, this is the first sub-quadratic weakly polynomial exact algorithm for computing the \(p\)th power Euclidean bipartite matching for \(2\)-dimensional stochastic point sets. Furthermore, for many distributions, such as the uniform distribution on the unit square, one can show that, with high probability, the spread of the point set is bounded by a polynomial in \(n\). For all such distributions, we achieve a strongly polynomial sub-quadratic exact algorithm.

Our algorithm can be seen as an elegant adaptation of the classical Hungarian algorithm in a quadtree based divide-and-conquer framework. The previous speed-ups of the Hungarian algorithm in geometric settings are based on sophisticated data structures that are hard to implement. In contrast, our geometric improvements are relatively simple, which allows us to implement and compare our algorithm with the standard implementation of the Hungarian algorithm. Experiments suggest that our algorithm outperforms the Hungarian algorithm for synthetic data samples drawn from two (possibly different) distributions \(\mu\) and \(\nu\) as long as the \(1\)-Wasserstein distance between \(\mu\) and \(\nu\) is small. Our algorithm also outperforms the Hungarian algorithm on data samples drawn from the New York Taxi dataset, where one sample is drawn from the set of request pick-up locations (\(\mu\)) and the other sample is drawn from the request drop-off locations (\(\nu\)). Experiments also suggest that our analysis may not be tight, at least for the case where \(\mu=\nu\) is the uniform distribution inside the unit square. Obtaining a tighter analysis of our algorithm remains an important open question. Next, we present an overview of our divide-and-conquer algorithm.

**Our Approach.** The classical Hungarian algorithm is based on the primal-dual framework where, along with a matching \(M\), for every point \(v\in A\cup B\), the algorithm maintains a non-negative _dual weight_\(y(v)\). The matching \(M\) and dual weights \(y(\cdot)\) are _feasible_ if, for every edge \((a,b)\in A\times B\),

\[y(b)-y(a) \leq\|a-b\|, (a,b)\notin M,\] (1) \[y(b)-y(a) =\|a-b\|, (a,b)\in M.\] (2)

The Hungarian algorithm initializes \(M\) to be an empty matching and the dual weights of every vertex \(v\in A\cup B\) to be \(0\). It then incrementally builds a perfect matching \(M\) while maintaining its feasibility and returns the constructed feasible perfect matching. It is well-known that a perfect matching that is feasible is also a minimum-cost matching.

In the Euclidean setting, we interpret the dual weight \(y(v)\) of any point \(v\) as the radius of a disc centered around \(v\). We refer to these discs as the _dual discs_. For any edge \((a,b)\), if the dual disc of \(a\) is in the interior of the dual disc of \(b\) (Figure 1(a)), then \(y(b)-y(a)>\|a-b\|\), making the dual assignment infeasible. Therefore, for any feasible dual weight assignment, the dual disc of \(a\) cannot be completely inside the dual disc of \(b\). The condition (2) corresponds to Figure 1(b) where the boundary of the disc of \(a\) touches the boundary of the disc of \(b\) from inside. Figure 1(c) is an example of an edge that only satisfies (1).

In order to implement the Hungarian algorithm in the geometric divide-and-conquer framework, we can use a quadtree \(\mathcal{Q}\) with the unit square as the root node. Each node of this tree represents a square \(\square\) and if \(\square\) has more than one input point, then it has four children that are obtained by splitting \(\square\) into four equal squares. Squares with no more than one point become a leaf node in this tree.

Figure 1: (a) An infeasible edge, (b) a feasible matching edge satisfying constraint (2), (c) a feasible non-matching edge satisfying constraint (1).

Consider the following naive implementation of the Hungarian algorithm in a quadtree based geometric divide-and-conquer framework. Given a square \(\square\) of the quadtree \(\mathcal{Q}\), we recursively find a feasible matching in each of the four children. Doing so, however, may lead to a dual weight assignment that violates the feasibility condition for _bridge edges_, i.e., edges that cross the boundary of the children squares. See Figure 2(a) that illustrates a dual feasible assignment at two children squares, which when combined causes the edge \((a^{\prime},b)\) to violate the feasibility condition (1). Thus, implementing the conquer step becomes challenging since we cannot simply combine the feasible dual assignments at the children to achieve a feasible dual assignment at the parent.

To overcome this difficulty, we add an additional condition that restricts the dual disc of a point \(v\) inside any child square \(\square^{\prime}\) to always stay inside \(\square^{\prime}\), i.e., \(y(v)\leq d(v,\square^{\prime})\), where \(d(v,\square^{\prime})\) is the Euclidean distance of \(v\) to the boundary of \(\square^{\prime}\). Figure 2(b) shows a restricted dual assignment at the two children squares, which remains feasible when combined at the parent. Such a _restricted feasible matching_ inside each of the four children squares of \(\square\) trivially combine to form a feasible matching at \(\square\). Thus, one can apply the divide-and-conquer framework to compute such a restricted feasible matching. Unfortunately, however, the restricted feasible matchings computed at the children may be of an extremely low cardinality. This is particularly true when most of the input points are close to the boundaries of the children squares. See, for instance, in Figure 3(a), the restricted feasible matching is an empty one since most of the blue points are close to the boundary. As a result, the divide step may not make any progress in computing a perfect matching and the conquer step may have to do significant amount of work to find a perfect matching, causing the execution time of the conquer step to be the same as that of the standard implementation of the Hungarian algorithm.

We observe that if the quadtree \(\mathcal{Q}\) is constructed using a _random-shift_, then, in expectation, only sub-linearly many points are close to the boundary of any square of \(\mathcal{Q}\). By combining this with the fact that the average length of a matching edge for \(n\) points drawn from an arbitrary distribution \(\mu\) inside unit square is \(\approx 1/\sqrt{n}\), we are able to bound the expected number of unmatched points in the restricted matchings by \(\tilde{O}(n^{3/4})\), leading to an execution time of \(\tilde{O}(n^{7/4})\) for the conquer steps across all cells at each level of the quadtree and an overall execution time of \(\tilde{O}(n^{7/4}\log\Delta)\).

## 2 Preliminaries

We begin by introducing the notations necessary to describe our algorithm. Given \(A\cup B\) and any square \(\square\), let \(A_{\square}=A\cap\square\), \(B_{\square}=B\cap\square\), and \(n_{\square}=|A_{\square}\cup B_{\square}|\). Let \(\ell_{\square}\) denote the side-length of \(\square\). For any point \(v\in A_{\square}\cup B_{\square}\) inside \(\square\), let \(\mathrm{d}(v,\square)\) denote the Euclidean distance of \(v\) to its closest point on the boundary of \(\square\). We say that \(v\) is \(\delta\)-close to \(\square\) if \(\mathrm{d}(v,\square)\leq\delta\ell_{\square}\). Let \(n_{\square}^{\delta}\) denote the number of points of \(A_{\square}\cup B_{\square}\) that are \(\delta\)-close to \(\square\).

**Randomly Shifted Quadtree.** Given the input points \(A\cup B\) inside a unit square, a _randomly shifted quadtree_ on the input can be constructed as follows. Let \(\xi\) be a point chosen uniformly at random from the unit square \([0,1]^{2}\). Define the square \(\square^{*}:=[-4,4]^{2}+\xi\) to be the root of the quadtree \(\mathcal{Q}\). Recursively construct \(\mathcal{Q}\) by decomposing any square \(\square\) with \(n_{\square}>1\) into four equal squares, each of which become the children of \(\square\). Any square with exactly one point becomes a leaf square of the quadtree. Given that the spread of the points in \(A\cup B\) is \(\Delta\), the height of \(\mathcal{Q}\) is \(O(\log\Delta)\).

Figure 2: (a) Feasible matchings for children that is infeasible when combined at the parent (the edge \((b,a^{\prime})\) is infeasible), (b) Restricted feasible matchings inside the children, which is also restricted feasible when combined at parent.

For any square \(\square\) of the quadtree \(\mathcal{Q}\), in the next lemma, we show that due to the random shift, there are not many points that are very close to the boundary of \(\square\).

**Lemma 2.1**.: _For any square \(\square\) of a randomly shifted quadtree and any \(\delta\in(0,1/2)\), \(\mathbb{E}\left[n_{\square}^{\delta}\right]=O(\delta\mathbb{E}\left[n_{ \square}\right])\)._

**Constrained Matching.** For any square \(\square\), we introduce a variant of the minimum-cost matching problem, which we refer to by the _minimum-cost \(\square\)-constrained matching_ problem. Given a square \(\square\), consider a matching \(M_{\square}\) of the points in \(A_{\square}\cup B_{\square}\). Any point \(v\in A_{\square}\cup B_{\square}\) is a _free_ point with respect to \(M_{\square}\) if \(v\) is not matched in \(M_{\square}\). Let \(A_{\square}^{F}\) (resp. \(B_{\square}^{F}\)) denote the set of free points of \(A_{\square}\) (resp. \(B_{\square}\)) with respect to \(M_{\square}\). We define the _\(\square\)-constrained cost_ of the matching \(M_{\square}\), denoted by \(w_{\square}(M_{\square})\), as

\[w_{\square}(M_{\square}):=\sum_{(a,b)\in M_{\square}}\|a-b\|+\sum_{b\in B_{ \square}^{F}}\mathrm{d}(b,\square).\] (3)

For any square \(\square\), the _minimum-cost \(\square\)-constrained matching (\(\square\)-MCM)_, denoted by \(M_{\square}^{*}\), is simply a matching with the minimum \(\square\)-constrained cost. In Lemma 2.2, we show that for the root square \(\square^{*}\) of \(\mathcal{Q}\), the minimum-cost \(\square^{*}\)-constrained matching is also a minimum-cost matching on \(A\cup B\).

For any arbitrary square \(\square\) of \(\mathcal{Q}\), \(M_{\square}^{*}\) might be of a very small cardinality. See, for instance in Figure 3(a) where \(B\) is the set of blue points, Equation (3) is minimized when all blue points are free points; i.e., \(w_{\square}(M_{\square}^{*})=\sum_{b\in B}\mathrm{d}(b,\square)\) and \(M_{\square}^{*}\) is an empty matching. Nonetheless, by using the bound on the number of \(\delta\)-close points to any square \(\square\) of a randomly-shifted quadtree, we can bound the expected number of free points with respect to any \(\square\)-MCM \(M_{\square}^{*}\). In the example of Figure 3(b), the solid lines show the boundary of the squares of a randomly-shifted quadtree, and as shown in Figure 3(c), only a few points will be free with respect to the optimal matchings constrained to the squares of the quadtree.

**Lemma 2.2**.: _For any square \(\square\) of a randomly shifted quadtree and any minimum-cost \(\square\)-constrained matching \(M_{\square}^{*}\) (i) the expected number of free points of \(B_{\square}\) with respect to \(M_{\square}^{*}\) is \(\tilde{O}\left(n^{3/4}\right)\) in \(2\)-dimensions (and \(\tilde{O}(n^{1-\frac{1}{2d}})\) in \(d\) dimensions), and (ii) if \(\square\) is the root square, then \(M_{\square}^{*}\) is a minimum-cost perfect matching on \(A\cup B\)._

For the rest of the paper, we design a primal-dual method to compute a \(\square^{*}\)-MCM, which by Lemma 2.2 is also a minimum cost matching on \(A\cup B\).

**Constrained Feasibility.** Similar to the Hungarian algorithm, we devise a primal-dual method to compute a \(\square\)-MCM. For any square \(\square\) of \(\mathcal{Q}\), we say that a matching \(M_{\square}\) on \(A_{\square}\cup B_{\square}\) along with a set of non-negative dual weights \(y(\cdot)\) for the points in \(A_{\square}\cup B_{\square}\) is \(\square\)-_feasible_ if,

\[y(b)-y(a)\leq\|a-b\|, \forall(a,b)\in A\times B,\] (4) \[y(b)-y(a)=\|a-b\|, \forall(a,b)\in M_{\square}.\] (5) \[y(b)\leq\mathrm{d}(b,\square), \forall b\in B_{\square},\] (6) \[y(a)=0, \forall a\in A_{\square}^{F}.\] (7)

Figure 3: (a) A \(\square\)-MCM where all points of \(B\) (blue points) are unmatched, (b) the boundaries of the cells of the randomly shifted quadtree (solid line) and the \(\delta\)-close points to the cells of the quadtree (points in the shaded area), (c) the matchings constrained to the cells of the quadtree.

Note that (4) and (5) are conditions identical to the ones maintained by the Hungarian algorithm. Condition (6) ensures that the dual discs stay inside the square and condition (7) ensures that the \(\square\)-MCM is of minimum cost.

Given a \(\square\)-feasible matching \(M_{\square},y(\cdot)\), we say that any point \(b\in B_{\square}\) is a _\(\square\)-free point_ with respect to \(M_{\square}\) if \(b\) is not matched in \(M_{\square}\) and \(y(b)<\mathrm{d}(b,\square)\). Note that a free point may not be \(\square\)-free. For instance, in Figure 4(a), the blue point \(b_{2}\) is free but not a \(\square\)-free point since its dual weight is equal to its distance to \(\square\). Let \(\mathcal{B}_{\square}^{F}\) denote the set of all \(\square\)-free points in \(B_{\square}\). We say that a \(\square\)-feasible matching \(M_{\square},y(\cdot)\) is _\(\square\)-optimal_ if it does not have any \(\square\)-free points, i.e., \(\mathcal{B}_{\square}^{F}=\emptyset\). The following lemma shows that any \(\square\)-optimal matching \(M_{\square},y(\cdot)\) is a \(\square\)-MCM. Thus, we focus on computing a \(\square\)-optimal matching.

**Lemma 2.3**.: _Let \(M_{\square},y(\cdot)\) be a \(\square\)-optimal matching on \(A_{\square}\cup B_{\square}\). Then, \(M_{\square}\) is a minimum-cost \(\square\)-constrained matching._

Given a square \(\square\) of \(\mathcal{Q}\) and its four children \(\square_{1},\square_{2},\square_{3},\) and \(\square_{4}\), we can obtain a \(\square\)-feasible matching at \(\square\) by simply combining the \(\square_{i}\)-optimal matchings from each child \(\square_{i}\). We use this property to design our divide-and-conquer algorithm for computing \(\square\)-optimal matching.

**Lemma 2.4**.: _For any square \(\square\), let \(\square_{i}\), \(i\in[1,4]\) be the set of all children of \(\square\) and let \(M_{i},y(\cdot)\) denote a \(\square_{i}\)-optimal matching. Then, the matching \(\bigcup_{i=1}^{4}M_{i},y(\cdot)\) is a \(\square\)-feasible matching._

Next, we define slack and an augmenting path with respect to a \(\square\)-feasible matching.

**Slacks.** For any square \(\square\), any \(\square\)-feasible matching \(M_{\square},y(\cdot)\), and any pair of points \((a,b)\in A_{\square}\times B_{\square}\), we define the _edge slack_ on the feasibility conditions for edge \((a,b)\), denoted by \(s(a,b)\), as \(s(a,b)=\|a-b\|-y(b)+y(a)\). We say that \((a,b)\) is _admissible_ if \(s(a,b)=0\). Furthermore, for any point \(b\in B_{\square}\), we define the _vertex slack_ of \(b\) with respect to condition (6) to be \(s_{\square}(b):=\mathrm{d}(b,\square)-y(b)\). We refer to the edge slack (resp. vertex slack) as slack when the edge (resp. vertex) is obvious from the context.

While the definition of edge slacks are identical to what is used in the Hungarian algorithm, the definition of vertex slacks is new. Since the dual weights associated with any \(\square\)-feasible matching satisfies (4)-(7), both edge slacks and vertex slacks will be non-negative.

**Admissible Augmenting Path.** For any square \(\square\), suppose \(M_{\square},y(\cdot)\) is a \(\square\)-feasible matching on \(A_{\square}\cup B_{\square}\). An _alternating path_ with respect to \(M_{\square}\) is a simple path on \(A_{\square}\cup B_{\square}\) whose edges alternate between those in \(M_{\square}\) and those not in \(M_{\square}\). An _admissible augmenting path_ (or simply _admissible path_) is any alternating path consisting only of zero slack edges that starts at a \(\square\)-free point \(b\in\mathcal{B}_{\square}^{F}\) and ends at either (i) a free point \(a\in A_{\square}^{F}\) or (ii) a point \(b^{\prime}\in B_{\square}\) with a slack of \(0\). We can _augment_ a \(\square\)-feasible matching \(M_{\square}\) along an admissible path \(P\) by setting \(M_{\square}\gets M_{\square}\oplus P\), i.e., we remove all matching edges of \(P\) from \(M_{\square}\) and add all non-matching edges of \(P\) to \(M_{\square}\) (see Figure 4). The following lemma shows that augmenting a matching along an admissible path does not violate \(\square\)-feasibility conditions and also reduces the number of \(\square\)-free points of \(B_{\square}\) by one.

Figure 4: (a) A \(\square\)-feasible matching \(M_{\square}\) where \(b_{1}\) is a \(\square\)-free point and \(b_{2}\) is a free point that is not \(\square\)-free, (b) an admissible path \(P\) (dashed line) from \(b_{1}\) to a point \(b_{3}\) whose vertex slack is zero, and (c) the \(\square\)-optimal matching obtained from augmenting \(M_{\square}\) along \(P\).

**Lemma 2.5**.: _Suppose \(M_{\square},y(\cdot)\) is a \(\square\)-feasible matching and \(P\) is an admissible path. After augmenting \(M_{\square}\) along \(P\), the matching \(M_{\square},y(\cdot)\) remains \(\square\)-feasible. Furthermore, the augmentation reduces the number of \(\square\)-free points of \(B_{\square}\) with respect to \(M_{\square}\) by one._

**Residual Network.** In order to assist in computing admissible paths with respect to a \(\square\)-feasible matching \(M_{\square},y(\cdot)\), we define a _residual network_ of \(A_{\square}\cup B_{\square}\) with respect to \(M_{\square}\) as follows. The vertex set of the residual network is the set of points \(A_{\square}\cup B_{\square}\) and a source vertex \(s\). For any pair of points \((a,b)\in A_{\square}\times B_{\square}\), if \((a,b)\in M_{\square}\), we add an edge directed from \(a\) to \(b\) with a weight \(s(a,b)\) to the residual network. Otherwise, if \((a,b)\not\in M_{\square}\), we add an edge directed from \(b\) to \(a\) with a weight \(s(a,b)\). In addition, we add zero-weight edges from the source \(s\) to every \(\square\)-free point in \(\mathcal{B}^{F}_{\square}\). Note that any zero-weight directed path from the source vertex \(s\) to a free point \(a\in A^{F}_{\square}\) or a zero-slack point \(b\in B_{\square}\) in this residual network is an admissible path.

## 3 Algorithm

In this section, we describe our algorithm for computing an optimal matching on a point set \(A\cup B\). Our algorithm builds a randomly shifted quadtree \(\mathcal{Q}\). For any square \(\square\) of the quadtree \(\mathcal{Q}\), we denote the set of children of \(\square\) in \(\mathcal{Q}\) by \(\mathsf{C}[\square]\).

We describe our divide-and-conquer algorithm with respect to an arbitrary square \(\square\) in \(\mathcal{Q}\). Our algorithm computes a \(\square\)-optimal matching \(M_{\square},y(\cdot)\).

**Base case.** If \(\square\) is a leaf of \(\mathcal{Q}\), let \(v\) be the only point in \(\square\). If \(v\in B\), set \(y(v)\leftarrow\mathrm{d}(v,\square)\) and \(M_{\square}\leftarrow\emptyset\). If \(v\in A\), set \(y(v)\gets 0\) and \(M_{\square}\leftarrow\emptyset\) and return (see lines 1-4 of Algorithm 1).

**Divide step.** If \(\square\) is not a leaf of \(\mathcal{Q}\), for each child \(\square^{\prime}\in\mathsf{C}[\square]\), recursively compute the \(\square^{\prime}\)-optimal matching \(M_{\square^{\prime}},y(\cdot)\) on \(A_{\square^{\prime}}\cup B_{\square^{\prime}}\) (see lines 6 and 7 of Algorithm 1).

**Conquer step.** For any child \(\square^{\prime}\in\mathsf{C}[\square]\), let \(M_{\square^{\prime}},y(\cdot)\) denote the \(\square^{\prime}\)-optimal matching returned by the algorithm. Set \(M_{\square}:=\bigcup_{\square^{\prime}\in\mathsf{C}[\square]}M_{\square^{ \prime}}\) as the union of all matchings computed inside the children of \(\square\). Let \(\mathcal{B}^{F}_{\square}\) denote the \(\square\)-free points of \(B_{\square}\) with respect to \(M_{\square}\). To obtain a \(\square\)-optimal matching, our algorithm iteratively executes the ConstrainedHungarianSearch procedure that adjusts the dual weights to find an admissible path \(P\) with respect to \(M_{\square}\). Then, the Augment procedure updates \(M_{\square}\) by augmenting the matching along \(P\). The algorithm continues to search and augment until there are no \(\square\)-free points of \(B_{\square}\), i.e., until \(M_{\square},y(\cdot)\) is a \(\square\)-optimal matching. See lines 8-11 of Algorithm 1. We describe the details of the ConstrainedHungarianSearch and Augment procedures below.

_1-_ ConstrainedHungarianSearch _procedure:_ Compute the residual network with respect to \(M_{\square}\) and execute a Dijkstra's shortest path algorithm starting from the source \(s\). Let \(\kappa_{v}\) denote the shortest path distance from \(s\) to \(v\) as computed by the Dijkstra's algorithm. Let

\[\kappa=\min\{\min_{a\in A^{F}_{\square}}\kappa_{a},\min_{b\in B_{\square}} \kappa_{b}+s_{\square}(b)\},\]

and let \(u\) denote the point that achieves this minimum. Note that \(u\in A^{F}_{\square}\cup B_{\square}\). Let \(P_{u}\) denote the shortest path from \(s\) to \(u\) and \(P\) denote the path obtained by removing \(s\) from \(P_{u}\). For any \(v\in A_{\square}\cup B_{\square}\), if \(\kappa_{v}<\kappa\), update its dual weight to \(y(v)\gets y(v)+\kappa-\kappa_{v}\).

_2-_Augment _procedure:_ Augment \(M_{\square}\) along \(P\).

Our algorithm returns the matching \(M_{\square^{*}}\) computed at the root \(\square^{*}\) of \(\mathcal{Q}\) as the minimum-cost matching between \(A\) and \(B\). This completes the description of our algorithm. The pseudo-code of our divide-and-conquer algorithm is provided in Algorithm 1.

### Proof of correctness

Recollect that for the root square \(\square^{*}\) of \(\mathcal{Q}\), from Lemma 2.2, a \(\square^{*}\)-optimal matching is a minimum-cost matching of \(A\) and \(B\). Therefore, it suffices to show that, at each square \(\square\) of \(\mathcal{Q}\), our algorithm computes a \(\square\)-optimal matching.

We begin by showing this for any leaf square \(\square\), which by definition contains exactly one point. Since there are no edges inside \(\square\), conditions (4) and (5) hold trivially. Let \(v\) be the only point inside \(\square\)If \(v\in B_{\square}\), then \(y(v)=\mathrm{d}(v,\square)\) and therefore, condition (6) holds. Otherwise, \(v\in A_{\square}\) and our algorithm sets \(y(v)=0\); therefore, condition (7) holds. In both cases, it is easy to see that \(\mathcal{B}_{\square}^{F}\) is empty and therefore the empty matching \(M_{\square}\) and dual weight \(y(v)\) is a \(\square\)-optimal matching.

For any non-leaf square \(\square\) of \(\mathcal{Q}\), at the beginning of the conquer step, the algorithm simply combines the constrained optimal matchings computed at its non-empty children. From Lemma 2.4, the resulting matching will be a \(\square\)-feasible matching. From Lemma 3.1 below, after the execution of the ConstrainedHungarianSearch procedure, the matching \(M_{\square},y(\cdot)\) remains a \(\square\)-feasible matching and the path \(P\) returned by the procedure is an admissible path. From Lemma 2.5, the execution of Augment procedure then reduces the number of \(\square\)-free points by one while maintaining the \(\square\)-feasibility conditions. Therefore, during the execution of the conquer step, the matching \(M_{\square},y(\cdot)\) remains \(\square\)-feasible and the number of \(\square\)-free points reduces by one at each iteration; thus, the algorithm terminates with a \(\square\)-optimal matching, as desired.

**Lemma 3.1**.: _Given any square \(\square\) of \(\mathcal{Q}\) and any \(\square\)-feasible matching \(M_{\square},y(\cdot)\), after executing the ConstrainedHungarianSearch procedure on \(\square\), the updated dual weights remain \(\square\)-feasible. Furthermore, the returned path \(P\) is an admissible path._

### Proof of Efficiency

For any square \(\square\), let \(T_{\square}\) denote the execution time of the conquer step of our algorithm when executed on \(\square\). Additionally, for any \(i\), let \(\mathcal{L}(i)\) denote the set of all squares of the quadtree that are processed at depth \(i\) of recursion and let \(T_{i}\) denote the total execution time of our algorithm across all such squares, i.e., \(T_{i}=\sum_{\square\in\mathcal{L}(i)}T_{\square}\).

Recall that the conquer step of our algorithm consists of iterations, where in each iteration, it executes the ConstrainedHungarianSearch procedure followed by the Augment procedure. For any square \(\square\) of \(\mathcal{Q}\), from Lemma 2.5, each iteration of the conquer step reduces the number of \(\square\)-free points of \(B_{\square}\) by one. Therefore, the number of iterations is bounded by the number of \(\square\)-free points of \(B_{\square}\), which is at most the total number of free points with respect to the \(\square^{\prime}\)-MCM computed for all children \(\square^{\prime}\) of \(\square\). By invoking Lemma 2.2 on all children of \(\square\), we bound the expected number of iterations of the conquer step on \(\square\) by \(\tilde{O}(n^{3/4})\) for \(2\) dimensions (and \(\tilde{O}(2^{d}n^{1-\frac{1}{2d}})\) for \(d\) dimensions).

Using any dynamic weighted nearest neighbor data structure with an update/query time of \(\Phi(n_{\square})\), one can execute the ConstrainedHungarianSearch procedure in \(\tilde{O}(n_{\square}\Phi(n_{\square}))\) time [44; 48]. For planar point sets, \(\Phi(n_{\square})=\mathrm{poly}(\log n_{\square})\) and therefore, each execution of the ConstrainedHungarianSearch procedure takes \(\tilde{O}(n_{\square})\) time. Furthermore, the Augment procedure simply augments the matching along an admissible path, which can be done in \(O(n_{\square})\) time. Therefore, the conquer step of our algorithm executes \(\tilde{O}(n^{3/4})\) iterations for \(2\) dimensions (and \(\tilde{O}(2^{d}n^{1-\frac{1}{2d}})\) iterations for \(d\) dimensions), in expectation, where each iteration takes \(\tilde{O}(n_{\square})\) time for \(2\) dimensions (and \(\tilde{O}(n_{\square}\Phi(n_{\square}))\) for \(d\) dimensions). Since \(\sum_{\square\in\mathcal{L}(i)}n_{\square}\leq n\), \(\mathbb{E}\left[T_{i}\right]=\sum_{\square\in\mathcal{L}(i)}\mathbb{E}\left[T_ {\square}\right]=\sum_{\square\in\mathcal{L}(i)}\mathbb{E}\left[T_{\square}\right]\)\(\tilde{O}\left(\sum_{\square\in\mathcal{L}(i)}n^{3/4}n_{\square}\right)=\tilde{O}(n^ {7/4})\) for \(2\) dimensions (and \(\mathbb{E}\left[T_{i}\right]=\tilde{O}(2^{d}n^{2-\frac{1}{2d}}\Phi(n))\) for \(d\) dimensions). Summing over all levels of \(\mathcal{Q}\), the expected execution time of our algorithm is \(\tilde{O}(n^{7/4}\log\Delta)\) for \(2\) dimensions (and \(\tilde{O}(2^{d}n^{2-\frac{1}{2d}}\Phi(n)\log\Delta)\) for \(d\) dimensions). Using a slightly sophisticated argument which is presented in the Appendix, we can remove the \(2^{d}\) from the execution time for the \(d\)-dimensional settings, leading to Theorem 1.1.

**Remark 3.2**.: _When the input points \(A\cup B\) are chosen arbitrarily, the number of iterations of the conquer step can be \(\Theta(n_{\square})\), leading to an execution time of \(O(n_{\square}^{2}\Phi(n_{\square}))\) per square and \(O(n^{2}\Phi(n)\log\Delta)\) in total._

## 4 Experiments

In this section, we present the results of our experiments comparing the execution time of our algorithm to that of the Hungarian algorithm. Both algorithms are implemented in Java and share the same data structures2. For both algorithms, we use the classical implementation of Dijkstra's shortest path algorithm used in the Hungarian search procedure and do not use any weighted nearest neighbor data structures. All computations are performed using a single calculation thread on a computer with a 2.6 GHz 6-Core Intel Core i7 CPU and 16 GB of 2667 MHz DDR4 RAM.

Footnote 2: Our implementations are available at https://github.com/agattani190/Exact-Euclidean-Bipartite-Matching.

**Datasets.** We test our algorithm on \(2\)-dimensional synthetic and real-world datasets. For the synthetic dataset, we use two distributions, namely (i) a uniform distribution defined on the unit square (Uniform), and (ii) a Gaussian distribution constrained to the unit square with a randomly chosen mean inside the unit square and a standard deviation of \(0.25\) (Gaussian). For a real-world dataset, we employ the New York Taxi dataset [47] and obtain two distributions, namely (i) the distribution of pickup locations (Pickup) and (ii) the distribution of drop-off locations (Drop-off) of passengers. We filtered the datasets by considering trips in seven dates in 2014 with (i) a trip duration of at least \(3\) minutes, and (ii) a trip velocity of at most \(110mph\).

**Tests.** In each test, we conducted experiments using two sets of \(n\) i.i.d samples from distributions \(\mu\) and \(\nu\) within the unit square. We executed both our divide-and-conquer algorithm and the Hungarian algorithm to compute the empirical \(1\)-Wasserstein distance between the same set of samples and compared the execution times. In our first experiment, we used Uniform distribution for both \(\mu\) and \(\nu\) to compare the performance of algorithms when the distributions are the same. For the second experiment on synthetic datasets, we compared running times with different distributions - \(\mu\) being Uniform and \(\nu\) being Gaussian, and in the third experiment, which is on the real-world datasets, \(\mu\) was set to the Pickup distribution while \(\nu\) was the Drop-off distribution. In our experiments on the synthetic datasets, we recorded the number of iterations our algorithm executes at each square of the quadtree to find out how tight our analysis are. The results of our experiments are shown in Figures 5 and 6.

**Results.** As depicted in Figure 5, experiments suggest that our algorithm outperforms the Hungarian algorithm, and the improvement is most significant when the distributions are the same (Figure 5(a)). More generally, experiments suggest that our algorithm performs significantly better than the Hungar

Figure 5: The running time of our algorithm (D&C Hungarian Algorithm) and the Hungarian algorithm for computing the \(1\)-Wasserstein distance between \(\mu\) and \(\nu\), where (a) both \(\mu\) and \(\nu\) are Uniform, (b) \(\mu\) is Uniform and \(\nu\) is Gaussian, and (c) \(\mu\) is Pickup and \(\nu\) is Drop-off.

ian algorithm when the matching cost is low (Figures 5(a) and (c)). The reason for this improvement lies in the fact that the Hungarian algorithm performs a 'global Hungarian search' for matching each point, whereas our algorithm isolates shorter edges of the optimal matching within smaller quadtree sub-problems and execute 'local Hungarian search' for them. Therefore, while the asymptotic improvements are shown for samples from the same distribution, we expect our algorithm to perform better when the optimal matching has many edges with small cost, such as when points are drawn from two similar but not identical distributions. This is evident in our third experiment (Figure 5(c)), where we sample \(n\) pick-up locations and \(n\) drop-off locations from the New York Taxi dataset. Notably, pick-up and drop-off locations tend to follow different distributions, with pick-ups having a higher density around Manhattan.

Additionally, our algorithm handles larger point sets efficiently; for example, it computes the optimal solution for \(50\,000\) points from the Uniform distribution in approximately \(700\) seconds. Finally, in Figure 6, our experiments on samples from the same synthetic dataset suggest that for a square with \(n\) points, the number of iterations of the conquer step of our algorithm might be bounded by \(O(n^{\alpha})\) for \(\alpha\simeq 0.55\). This perhaps suggests that our upper-bound of \(O(n^{3/4})\) may be an overestimate, at least for the uniform and Gaussian distributions.

## 5 Conclusion

In this paper, we adapted the classical primal-dual approaches for computing minimum-cost matching within the divide-and-conquer framework. This approach led to a sub-quadratic weakly-polynomial exact algorithm for computing minimum-cost matching on stochastic point sets. Notably, our algorithm incorporates a randomly shifted quadtree, a structure previously used only in approximation algorithms for \(p\)-Wasserstein distance. We conclude by highlighting some open problems for further investigation.

* An interesting question is whether it is possible to further exploit the geometry of the dual weights and give a tighter bound for the number of iterations of our algorithm at each square; thus, improving the analysis of our algorithm.
* Our analysis only requires the fact that the average length of the matching edges in large cells is small. Stochastic point sets have this property and therefore, we can achieve sub-quadratic algorithms for them. Is there a sub-quadratic exact algorithm for the case where the matching edges are long? Understanding this case may shed light into the design of sub-quadratic exact algorithm for arbitrary inputs.
* Finally, our algorithm is a weakly polynomial algorithm as its running time is dependent on the spread of the point set. Can we remove the dependence on spread while maintaining the simplicity of our algorithm?

Figure 6: The number of iterations of the conquer step for a square with \(n\) points when executed on samples from the same distributions ((e) Uniform and (f) Gaussian).

## Acknowledgement

We would like to acknowledge Advanced Research Computing (ARC) at Virginia Tech, which provided us with the computational resources used to run the experiments. Research presented in this paper was funded by NSF CCF-1909171 and NSF CCF-2223871. We would like to thank the anonymous reviewers for their useful feedback.

## References

* Agarwal and Phillips [2006] Pankaj K Agarwal and Jeff M Phillips. On bipartite matching under the rms distance. In _Proceedings of the Eighteenth Canadian Conference on Computational Geometry', Kingston, Canada_, pages 143-146, 2006.
* Agarwal and Varadarajan [2004] Pankaj K. Agarwal and Kasturi R. Varadarajan. A near-linear constant-factor approximation for Euclidean bipartite matching? In _20th International Symposium on Computational Geometry_, pages 247-252, 2004.
* Agarwal et al. [1995] Pankaj K Agarwal, Alon Efrat, and Micha Sharir. Vertical decomposition of shallow levels in 3-dimensional arrangements and its applications. In _Proceedings of the eleventh annual symposium on Computational geometry_, pages 39-50, 1995.
* Agarwal et al. [2022] Pankaj K Agarwal, Hsien-Chih Chang, Sharath Raghvendra, and Allen Xiao. Deterministic, near-linear \(\varepsilon\)-approximation algorithm for geometric bipartite matching. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1052-1065, 2022.
* Agarwal et al. [2022] Pankaj K Agarwal, Sharath Raghvendra, Pouyan Shirzadian, and Rachita Sowle. A higher precision algorithm for computing the \(1\)-wasserstein distance. In _The Eleventh International Conference on Learning Representations_, 2022.
* Agarwal et al. [2022] Pankaj K Agarwal, Sharath Raghvendra, Pouyan Shirzadian, and Rachita Sowle. An improved \(\varepsilon\)-approximation algorithm for geometric bipartite matching. In _18th Scandinavian Symposium and Workshops on Algorithm Theory (SWAT 2022)_. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2022.
* Ajtai et al. [1984] Miklos Ajtai, Janos Komlos, and Gabor Tusnady. On optimal matchings. _Combinatorica_, 4:259-264, 1984.
* Arjovsky et al. [2017] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In _International conference on machine learning_, pages 214-223. PMLR, 2017.
* Cao et al. [2019] Jiezhang Cao, Langyuan Mo, Yifan Zhang, Kui Jia, Chunhua Shen, and Mingkui Tan. Multi-marginal wasserstein gan. _Advances in Neural Information Processing Systems_, 32, 2019.
* Caracciolo et al. [2014] Sergio Caracciolo, Carlo Lucibello, Giorgio Parisi, and Gabriele Sicuro. Scaling hypothesis for the euclidean bipartite matching problem. _Physical Review E_, 90(1):012118, 2014.
* Chen et al. [2022] Li Chen, Rasmus Kyng, Yang P Liu, Richard Peng, Maximilian Probst Gutenberg, and Sushant Sachdeva. Maximum flow and minimum-cost flow in almost-linear time. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 612-623. IEEE, 2022.
* Cuturi [2013] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In _Advances in Neural Information Processing Systems 26_, pages 2292-2300, 2013.
* Deb and Sen [2023] Nabarun Deb and Bodhisattva Sen. Multivariate rank-based distribution-free nonparametric testing using measure transportation. _Journal of the American Statistical Association_, 118(541):192-207, 2023.
* Deb et al. [2021] Nabarun Deb, Bhaswar B Bhattacharya, and Bodhisattva Sen. Efficiency lower bounds for distribution-free hotelling-type two-sample tests based on optimal transport. _arXiv preprint arXiv:2104.01986_, 2021.
* 951, 2019.
* [16] Richard Mansfield Dudley. The speed of mean glivenko-cantelli convergence. _The Annals of Mathematical Statistics_, 40(1):40-50, 1969.
* [17] RM Dudley. Real analysis and probability, 2002.
* [18] Nghia Duong-Trung, Minh-Hoang Nguyen, and Hanh TH Nguyen. Clustering stability via concept-based nonnegative matrix factorization. In _Proceedings of the 3rd International Conference on Machine Learning and Soft Computing_, pages 49-54, 2019.
* [19] Nicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the empirical measure. _Probability theory and related fields_, 162(3-4):707-738, 2015.
* [20] Kyle Fox and Jiashuai Lu. A near-linear time approximation scheme for geometric transportation with arbitrary supplies and spread. In _Proc. 36th Annual Symposium on Computational Geometry_, pages 45:1-45:18, 2020.
* [21] H. N. Gabow and R.E. Tarjan. Faster scaling algorithms for network problems. _SIAM J. Comput._, 18:1013-1036, October 1989. ISSN 0097-5397.
* [22] Harold N. Gabow and Robert Endre Tarjan. Faster scaling algorithms for general graph-matching problems. _J. ACM_, 38(4):815-853, 1991.
* [23] Javier Gonzalez-Delgado, Alberto Gonzalez-Sanz, Juan Cortes, and Pierre Neuvial. Two-sample goodness-of-fit tests on the flat torus based on wasserstein distance and their relevance to structural biology. _arXiv preprint arXiv:2108.00165_, 2021.
* [24] Xin Guo, Johnny Hong, Tianyi Lin, and Nan Yang. Relaxed wasserstein with applications to gans. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 3325-3329. IEEE, 2021.
* [25] Masaaki Imaizumi, Hirofumi Ota, and Takuo Hamaguchi. Hypothesis test and confidence analysis with wasserstein distance on general dimension. _Neural Computation_, 34(6):1448-1487, 2022.
* [26] Andrey Boris Khesin, Aleksandar Nikolov, and Dmitry Paramonov. Preconditioning for the geometric transportation problem. _arXiv preprint arXiv:1902.08384_, 2019.
* [27] Soheil Kolouri, Navid Naderializadeh, Gustavo K Rohde, and Heiko Hoffmann. Wasserstein embedding for graph learning. _arXiv preprint arXiv:2006.09430_, 2020.
* [28] Harold W Kuhn. The hungarian method for the assignment problem. _Naval research logistics quarterly_, 2(1-2):83-97, 1955.
* [29] Nathaniel Lahn and Sharath Raghvendra. An \(o(n^{5/4})\) time \(\varepsilon\)-approximation algorithm for rms matching in a plane. In _Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 869-888. SIAM, 2021.
* [30] Nathaniel Lahn, Sharath Raghvendra, and Kaiyi Zhang. A push-relabel based additive approximation for optimal transport. _arXiv preprint arXiv:2203.03732_, 2022.
* [31] Tilman Lange, Volker Roth, Mikio L Braun, and Joachim M Buhmann. Stability-based validation of clustering solutions. _Neural computation_, 16(6):1299-1323, 2004.
* [32] Richard J Lipton and Robert Endre Tarjan. Applications of a planar separator theorem. _SIAM journal on computing_, 9(3):615-627, 1980.
* [33] Huidong Liu, GU Xianfeng, and Dimitris Samaras. A two-step computation of the exact gan wasserstein distance. In _International Conference on Machine Learning_, pages 3159-3168. PMLR, 2018.
* [34] Huidong Liu, Xianfeng Gu, and Dimitris Samaras. Wasserstein gan with quadratic transport cost. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 4832-4841, 2019.

* [35] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature matching gan. In _International conference on machine learning_, pages 2527-2535. PMLR, 2017.
* [36] Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. _Annual review of statistics and its application_, 6:405-431, 2019.
* [37] Hermina Petric Maretic, Mireille El Gheche, Giovanni Chierchia, and Pascal Frossard. Got: an optimal transport framework for graph comparison. _Advances in Neural Information Processing Systems_, 32, 2019.
* [38] Abhijeet Phatak, Sharath Raghvendra, Chittaranjan Tripathy, and Kaiyi Zhang. Computing all optimal partial transports. In _The Eleventh International Conference on Learning Representations_, 2022.
* [39] Julien Rabin, Julie Delon, and Yann Gousseau. Circular earth mover's distance for the comparison of local features. In _2008 19th International Conference on Pattern Recognition_, pages 1-4. IEEE, 2008.
* [40] Sharath Raghvendra and Pankaj K Agarwal. A near-linear time \(\varepsilon\)-approximation algorithm for geometric bipartite matching. _Journal of the ACM (JACM)_, 67(3):1-19, 2020.
* [41] Aaditya Ramdas, Nicolas Garcia Trillos, and Marco Cuturi. On wasserstein two-sample testing and related families of nonparametric tests. _Entropy_, 19(2):47, 2017.
* [42] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover's distance as a metric for image retrieval. _International journal of computer vision_, 40(2):99, 2000.
* [43] R. Sharathkumar. A sub-quadratic algorithm for bipartite matching of planar points with bounded integer coordinates. In _29th International Symposium on Computational Geometry_, pages 9-16, 2013.
* [44] R Sharathkumar and Pankaj K Agarwal. Algorithms for the transportation problem in geometric settings. In _Proceedings of the 23rd annual ACM-SIAM symposium on Discrete Algorithms_, pages 306-317. SIAM, 2012.
* [45] Peter Williston Shor. _Random planar matching and bin packing_. PhD thesis, Massachusetts Institute of Technology, 1985.
* [46] Max Sommerfeld, Jorn Schrieber, Yoav Zemel, and Axel Munk. Optimal transport: Fast probabilistic approximation with exact solvers. _J. Mach. Learn. Res._, 20(105):1-23, 2019.
* [47] NYC Taxi and Limousine Commission (TLC). Trip record data. https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page, 2023. Accessed: 2023-03-01.
* [48] Pravin M Vaidya. Geometry helps in matching. _SIAM Journal on Computing_, 18(6):1201-1225, 1989.
* 2648, 2019.