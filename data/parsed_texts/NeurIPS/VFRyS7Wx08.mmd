# Rethinking Inverse Reinforcement Learning:

from Data Alignment to Task Alignment

 Weichao Zhou

Boston University

Boston, MA 02215

zwc662@bu.edu

&Wenchao Li

Boston University

Boston, MA 02215

wenchao@bu.edu

###### Abstract

Many imitation learning (IL) algorithms use inverse reinforcement learning (IRL) to infer a reward function that aligns with the demonstrations. However, the inferred reward function often fails to capture the underlying task objective. In this paper, we propose a novel framework for IRL-based IL that prioritizes task alignment over conventional data alignment. Our framework is a semi-supervised approach that leverages expert demonstrations as weak supervision signals to derive a set of candidate reward functions that align with the task rather than only with the data. It adopts an adversarial mechanism to train a policy with this set of reward functions to gain a collective validation of the policy's ability to accomplish the task. We provide theoretical insights into this framework's ability to mitigate task-reward misalignment and present a practical implementation. Our experimental results show that our framework outperforms conventional IL baselines in complex and transfer learning scenarios. The complete code are available at https://github.com/zwc662/PAGAR.

## 1 Introduction

Inverse reinforcement learning (IRL) Ng and Russell (2000); Finn et al. (2017) has become a popular method for imitation learning (IL), allowing policies to be trained by learning reward functions from expert demonstrations Abbeel and Ng (2004); Ho and Ermon (2016). Despite its widespread use, IRL-based IL faces significant challenges that often stem from overemphasizing data alignment rather than task alignment. For instance, reward ambiguity, where multiple reward functions can be consistent with the expert demonstrations, makes it difficult to identify the correct reward function. This problem persists even when there are infinite data Ng and Russell (2000); Cao et al. (2021); Skalse et al. (2022, 2022). Additionally, limited availability of demonstrations can further exacerbate this problem, as the data may not fully capture the nuances of the task. Misaligned reward functions can lead to policies that optimize the wrong objectives, resulting in poor performance and even reward hacking Hadfield-Menell et al. (2017); Amodei et al. (2016); Pan et al. (2022), a phenomenon where the policy exploits loopholes in the inferred reward function. These challenges highlight the limitation of exclusively pursuing data alignment in solving real-world tasks.

In light of these considerations, this paper advocates for a paradigm shift from a narrow focus on data alignment to a broader emphasis on task alignment. Grounded in a general formalism of task objectives, we propose identifying the task-aligned reward functions that more accurately reflect the underlying task objectives in their policy utility spaces. Expanding on this concept, we explore the intrinsic relationship between the task objective, reward, and expert demonstrations. This relationship leads us to a novel perspective where expert demonstrations can serve as weak supervision signals for identifying a set of candidate task-aligned reward functions. Under these reward functions, the expertachieves high -- but not necessarily optimal -- performance. The rationale is that achieving high performance under a task-aligned reward function is often adequate for real-world applications.

Building on this premise, we leverage IRL to derive the set of candidate task-aligned reward functions and propose Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised framework designed to mitigate task-reward misalignment by training a policy with this candidate reward set. PAGAR adopts an adversarial training mechanism between a protagonist policy and an adversarial reward searcher, iteratively improving the policy learner to attain high performance across the candidate reward set. This method moves beyond relying on deriving a single reward function from data, enabling a collective validation of the policy's similarity to expert demonstrations in terms of effectiveness in accomplishing tasks. Experimental results show that our algorithm outperforms baselines on complex IL tasks with limited demonstrations and in challenging transfer environments. We summarize our contributions below.

* Introduction of Task Alignment in IRL-based IL: We present a novel perspective that shifts the focus from data alignment to task alignment, addressing the root causes of reward misalignment in IRL-based IL.
* Protagonist Antagonist Guided Adversarial Reward (PAGAR): We propose a new semi-supervised framework that leverages adversarial training to improve the robustness of the learned policy.
* Practical Implementation: We present a practical implementation of PAGAR, including the adversarial reward searching mechanism and the iterative policy-improving process. Experimental results demonstrate superior performances in complex and transfer learning environments.

## 2 Related Works

IRL-based IL circumvents many challenges of traditional IL such as compounding error Ross and Bagnell (2010); Ross et al. (2011); Zhou et al. (2020) by learning a reward function to interpret the expert behaviors Ng et al. (1999); Ng and Russell (2000) and then learning a policy from the reward function via reinforcement learning (RL)Sutton and Barto (2018). However, the learned reward function may not always align with the underlying task, leading to reward misspecification Pan et al. (2022); Skalse and Abate (2022), reward hacking Skalse et al. (2022), and reward ambiguity Ng and Russell (2000); Cao et al. (2021). The efforts on alleviating reward ambiguity include Max-Entropy IRL Ziebart et al. (2008); Max-Margin IRL Abbeel and Ng (2004); Ratliff et al. (2006), and Bayesian IRL Ramachandran and Amir (2007). GAN-based methods Ho and Ermon (2016); Jeon et al. (2018); Finn et al. (2016); Peng et al. (2019); Fu et al. (2018) use neural networks to learn reward functions from limited demonstrations. However, these efforts that aim to address reward ambiguity fall short of mitigating the general impact of reward misalignment which can be caused by various reasons such as IRL making false assumptions about the relationship between expert policy and expert reward function Skalse et al. (2022); Hong et al. (2023). Other attempts to mitigate reward misalignment involve external information other than expert demonstrations Hejna and Sadigh (2023); Zhou and Li (2018, 2022a, 2018). Our work adopts the generic setting of IRL-based IL without needing additional information. The idea of considering a reward set instead of focusing on a single reward function is supported by Metelli et al. (2021) and Lindner et al. (2022). However, these works target reward ambiguity instead of reward misalignment. Our protagonist and antagonist setup is inspired by the concept of unsupervised environment design (UED) Dennis et al. (2020). In this paper, we develop novel theories in the context of reward learning.

## 3 Preliminaries

**Reinforcement Learning (RL)** models the environment as a Markov Decision Process \(\mathcal{M}=\langle\mathbb{S},\mathbb{A},\mathcal{P},d_{0}\rangle\) where \(\mathbb{S}\) is the state space, \(\mathbb{A}\) is the action space, \(\mathcal{P}\) is the transition probability, \(d_{0}\) is the initial state distribution. A _policy_\(\pi(a|s)\) determines the probability of an RL agent performing an action \(a\) at state \(s\). By successively performing actions for \(T\) steps from an initial state \(s^{(0)}\sim d_{0}\), a _trajectory_\(\tau=s^{(0)}a^{(0)}s^{(1)}a^{(1)}\dots s^{(T)}\) is produced. A state-action based _reward function_ is a mapping \(r:\mathbb{S}\times\mathbb{A}\rightarrow\mathbb{R}\). The soft Q-value function of \(\pi\) is \(\mathcal{Q}_{\pi}(s,a)=r(s,a)+\gamma\cdot\mathop{\mathbb{E}}_{s^{\prime}\sim \mathcal{P}(\cdot|s,a)}\left[\mathcal{V}_{\pi}(s^{\prime})\right]\) where \(\gamma\in(0,1]\) is a discount factor, \(\mathcal{V}_{\pi}\) is the soft state-value function of \(\pi\) defined as \(\mathcal{V}_{\pi}(s):=\mathop{\mathbb{E}}_{a\sim\pi(\cdot|s)}\left[\mathcal{Q} _{\pi}(s,a)\right]+\mathcal{H}(\pi(\cdot|s))\), and \(\mathcal{H}(\pi(\cdot|s))\) is the entropy of \(\pi\) at state \(s\)The soft advantage of performing action \(a\) at state \(s\) and then following a policy \(\pi\) afterwards is \(\mathcal{A}_{\pi}(s,a)=\mathcal{Q}_{\pi}(s,a)-V_{\pi}(s)\). The expected return of \(\pi\) under a reward function \(r\) is given as \(U_{r}(\pi)=\mathop{\mathbb{E}}_{\pi\sim\pi}[\sum_{t=0}^{\infty}\gamma^{t}\cdot r (s^{(t)},a^{(t)})]\). With a slight abuse of notations, we denote the entropy of a policy as \(\mathcal{H}(\pi):=\mathop{\mathbb{E}}_{\pi\sim\pi}[\sum_{t=0}^{\infty}\gamma^ {t}\cdot\mathcal{H}(\pi(\cdot|s^{(t)}))]\). The standard RL learns an _optimal policy_ by maximizing \(U_{r}(\pi)\). The entropy regularized RL learns a _soft-optimal_ policy by maximizing the objective function \(\mathcal{J}_{RL}(\pi;r):=U_{r}(\pi)+\mathcal{H}(\pi)\).

**Inverse Reinforcement Learning (IRL)** assumes that a set \(E=\{\tau_{1},\ldots,\tau_{N}\}\) of expert demonstrations are sampled from the roll-outs of the expert's policy \(\pi_{E}\) and aims to learn the expert reward function \(r_{E}\). IRL Ng and Russell (2000) assumes that \(\pi_{E}\) is _optimal_ under \(r_{E}\) and learns \(r_{E}\) by maximizing the margin \(U_{r}(E)-\max\limits_{\pi}U_{r}(\pi)\) while Maximum Entropy IRL (MaxEnt IRL) Ziebart et al. (2008) maximizes an entropy regularized objective function \(\mathcal{J}_{IRL}(r)=U_{r}(E)-(\max\limits_{\pi}U_{r}(\pi)+\mathcal{H}(\pi))\).

**Generative Adversarial Imitation Learning (GAIL)** Ho and Ermon (2016) draws a connection between IRL and Generative Adversarial Nets (GANs) as shown in Eq.1, where a discriminator \(D:\mathbb{S}\times\mathbb{A}\rightarrow[0,1]\) is trained by minimizing Eq.1 so that \(D\) can accurately identify any \((s,a)\) generated by the agent. Meanwhile, an agent policy \(\pi\) is trained as a generator to maximize Eq.1 so that \(D\) cannot discriminate \(\tau\sim\pi\) from \(\tau_{E}\). Adversarial inverse reinforcement learning (AIRL) Fu et al. (2018) uses a neural-network reward function \(r\) to represent \(D(s,a):=\frac{\pi(a|s)}{\exp(r(s,a))+\pi(a|s)}\), rewrites \(\mathcal{J}_{IRL}\) as minimizing Eq.1, and proves that the optimal reward satisfies \(r^{*}\equiv\log\pi_{E}\equiv\mathcal{A}_{\pi_{E}}\). By training \(\pi\) with \(r^{*}\) until optimality, \(\pi\) will behave just like \(\pi_{E}\).

\[\mathop{\mathbb{E}}_{(s,a)\sim\pi}\left[\log D(s,a))\right]+\mathop{\mathbb{E }}_{(s,a)\sim\pi_{E}}\left[\log(1-D(s,a))\right]\] (1)

## 4 Task-Reward Alignment

In this section, we formalize the concept of task-reward misalignment in IRL-based IL. We start by defining a notion of task based on the framework from Abel et al. (2021).

**Definition 1** (**Task**).: Given the policy hypothesis set \(\Pi\), a **task**\((\Pi,\preceq_{task},\Pi_{acc})\) is specified by a partial order \(\preceq_{task}\) over \(\Pi\) and a non-empty set of acceptable policies \(\Pi_{acc}\subseteq\Pi\) such that \(\forall\pi_{1}\in\Pi_{acc}\) and \(\forall\pi_{2}\notin\Pi_{acc}\), \(\pi_{2}\preceq_{task}\pi_{1}\) always hold.

**Remark:** The notions of policy acceptance and order allow the definition of **task** to accommodate a broad range of real-world tasks1 including the standard RL tasks (learning the optimal policy from a reward function \(r\)): given a reward function \(r\) and a policy hypothesis set \(\Pi\), the standard RL **task** can be written as a tuple \((\Pi,\preceq_{task},\Pi_{acc})\) where \(\preceq_{task}\) satisfies \(\forall\pi_{1},\pi_{2}\in\Pi,\pi_{1}\preceq_{task}\pi_{2}\Leftrightarrow U_{r }(\pi_{1})\leq U_{r}(\pi_{2})\), and \(\Pi_{acc}=\{\pi\mid\forall\pi^{\prime}\in\Pi.\pi^{\prime}\preceq_{task}\pi\}\) contains all the optimal policies.

Figure 1: (a) The two bars respectively represent the policy utility spaces of a task-aligned reward function \(r^{+}\) and a task-misaligned reward function \(r^{-}\). The white color indicates the utilities of acceptable policies, and the blue color indicates the unacceptable ones. Within the utility space of \(r^{+}\), the utilities of all acceptable policies are higher (\(\geq\underline{U}_{r^{+}}\)) than those of the unacceptable ones, and the policies with utilities higher than \(\overline{U}_{r^{+}}\) have higher orders than those of utilities lower than \(\overline{U}_{r^{+}}\). Within the utility space of \(r^{-}\), acceptable and unacceptable policies’ utilities are mixed together, leading to a low \(\underline{U}_{r^{-}}\) and an even lower \(\overline{U}_{r^{-}}\). (b) IRL-based IL relies solely on IRL’s optimal reward function \(r^{*}\) which can be task-misaligned and lead to an unacceptable policy \(\pi_{r^{*}}\in\Pi\backslash\Pi_{acc}\) while PAGAR-based IL learns an acceptable policy \(\pi^{*}\in\Pi_{acc}\) from a set \(R_{E,\delta}\) of reward functions.

Designing reward function(s) that align with the underlying task is essential in RL. Whether a designed reward aligns with the task hinges on how policies are ordered by the task and the utilities of the policies under the reward function. Therefore, we define the task-reward alignment by examining the utility spaces of the reward functions. If the acceptable policy set \(\Pi_{acc}\) of the task is given, we let \(\underline{U}_{r}:=\min\limits_{\pi\in\Pi_{acc}}U_{r}(\pi)\) be the minimal utility achieved by any acceptable policy under \(r\).

**Definition 2** (**Task-Aligned Reward Functions**).: A reward function is a _task-aligned reward function_ (denoted as \(r^{+}\)) if and only if \(\forall\pi\in\Pi\backslash\Pi_{acc},U_{r^{+}}(\pi)<\underline{U}_{r^{+}}(\pi)\). Conversely, if this condition is not met, it is a _task-misaligned reward function_ (denoted as \(r^{-}\)).

The definition suggests that under a task-aligned reward function \(r^{+}\), all acceptable policies for the task yield higher utilities than unacceptable ones. It also suggests that a policy is deemed acceptable as long as its utility is greater than \(\underline{U}_{r^{+}}\) for some task-aligned reward function \(r^{+}\), even if this policy is not optimal. We also examine whether high utility under a reward function \(r\) suggests a higher order under \(\preceq_{task}\). We define \(\overline{U}_{r}:=\max\limits_{\pi\in\Pi}U_{r}(\pi)\ s.t.\ \forall\pi_{1},\pi_{2} \in\Pi,U_{r}(\pi_{1})<U_{r}(\pi)\leq U_{r}(\pi_{2})\Rightarrow(\pi_{1} \preceq_{task}\pi)\land(\pi_{1}\preceq_{task}\pi_{2})\), which is the highest utility threshold such that any policy achieving a higher utility than \(\overline{U}_{r}\) has a higher order than those achieving lower utilities than \(\overline{U}_{r}\). In Figure 1(a) we illustrate how \(\overline{U}_{r}\) and \(\underline{U}_{r}\) vary between task-aligned and misaligned reward functions.

**Proposition 1**.: _Given the policy order \(\preceq_{task}\) of a task, for any two reward functions \(r_{1},r_{2}\), if \(\{\pi\mid U_{r_{1}}(\pi)\geq\overline{U}_{r_{1}}\}\subseteq\{\pi\mid U_{r_{2}} (\pi)\geq\overline{U}_{r_{2}}\}\), then there must exist policies \(\pi_{1}\in\{\pi\mid U_{r_{1}}(\pi)\geq\overline{U}_{r_{1}}\},\pi_{2}\in\{\pi\mid U _{r_{2}}(\pi)\geq\overline{U}_{r_{2}}\}\) such that \(U_{r_{1}}(\pi_{2})\leq U_{r_{1}}(\pi_{1})\) and \(\pi_{2}\preceq_{task}\pi_{1}\) while \(U_{r_{2}}(\pi_{2})\geq U_{r_{2}}(\pi_{1})\)._

This proposition implies that a high threshold \(\overline{U}_{r}\) indicates that a high utility corresponds to a high order in terms of \(\preceq_{task}\). In particular, for any task-aligned reward function \(r^{+}\), \(\{\pi\mid U_{r^{+}}(\pi)\geq\overline{U}_{r^{+}}\}\subseteq\Pi_{acc}\equiv\{ \pi\mid U_{r^{+}}(\pi)\geq\underline{U}_{r^{+}}\}\) (see proof in Appendix A.2). Thus, a small \(\{\pi\mid U_{r^{+}}(\pi)\geq\overline{U}_{r^{+}}\}\) leads to a large \(\{\pi\mid U_{r^{+}}(\pi)\in[\underline{U}_{r^{+}},\overline{U}_{r^{+}}]\}\). Hence, a task-aligned reward function \(r^{+}\) is more likely to be aligned with the task if it has a wide \([\underline{U}_{r^{+}},\overline{U}_{r^{+}}]\) and a narrow \([\overline{U}_{r^{+}},\max\limits_{\pi\in\Pi}U_{r^{+}}(\pi)]\).

### Mitigate Task-Reward Misalignment in IRL-Based IL

In IRL-based IL, a key challenge is that _the underlying task is unknown_, making it difficult to assert if a learned policy is acceptable. We denote the optimal reward function learned from the demonstration set \(E\) as \(r^{*}\), and the optimal policy under \(r^{*}\) as \(\pi_{r^{*}}\). When \(\pi_{r^{*}}\) has a poor performance under \(r_{E}\), it is considered to have a high \(Regret(\pi_{r^{*}},r_{E})\) which is defined in Eq.2. If \(Regret(\pi_{r^{*}},r_{E})>\max\limits_{\pi^{\prime}\in\Pi}U_{r_{E}}(\pi^{ \prime})-\underline{U}_{r_{E}}\), then \(\pi_{r^{*}}\) is unacceptable and \(r^{*}\) is task-misaligned.

\[Regret(\pi,r):=\max\limits_{\pi^{\prime}\in\Pi}U_{r}(\pi^{\prime})-U_{r}(\pi)\] (2)

Several factors can lead to a high \(Regret(\pi_{r^{*}},r_{E})\). For instance, Viano et al. (2021) shows that when expert demonstrations are collected in an environment whose dynamical function differs from that of the learning environment, \(|Regret(\pi_{r^{*}},r_{E})|\) can be positively related to the discrepancy between those dynamical functions. Additionally, we prove in Appendix A.1 that learning from only a few representative expert trajectories can also result in a large \(|Regret(\pi_{r^{*}},r_{E})|\) with a high probability.

Our insight for mitigating such potential task-reward misalignment in IRL-based IL is to _shift our focus from learning an optimal policy that maximizes the intrinsic \(r_{E}\) to learning an acceptable policy \(\pi^{*}\) that achieves a utility higher than \(\underline{U}_{r^{+}}\) under any task-aligned reward function \(r^{+}\)_. Our approach is to treat the expert demonstrations as weak supervision signals based on the following.

**Theorem 1**.: _Let \(\mathcal{I}\) be an indicator function. For any \(k\geq\big{\{}\min\limits_{r^{+}}\ \sum_{\pi\in\Pi}\mathcal{I}\{U_{r^{+}}(\pi)\geq U_{r^{+}}(\pi_{E})\} \big{\}}\), if \(\pi^{*}\) satisfies \(\big{\{}\sum_{\pi\in\Pi}\mathcal{I}\{U_{r}(\pi)\geq U_{r}(\pi^{*})\}\big{\}}<| \Pi_{acc}|\) for all \(r\in R_{E,k}:=\big{\{}r\mid\sum_{\pi\in\Pi}\mathcal{I}\{U_{r}(\pi)\geq U_{r}( \pi_{E})\}\leq k\big{\}}\), then \(\pi^{*}\) is an acceptable policy, i.e., \(\pi^{*}\in\Pi_{acc}\). Additionally, if \(k<|\Pi_{acc}|\), such an acceptable policy \(\pi^{*}\) is guaranteed to exist._

The statement suggests that we can obtain an acceptable policy by training it to attain high performance across a reward function set \(R_{E,k}\) that includes all the reward functions where, for each reward function at most \(k\) policies outperform the expert policy \(\pi_{E}\). The minimal value of \(k\) is determined by all the task-aligned reward functions in the reward hypothesis set. Appendix A.2 provides the proof.

**How to build \(R_{E,k}\)?** Building \(R_{E,k}\) involves setting the parameter \(k\). If \(r_{E}\) is a task-aligned reward function and \(\pi_{E}\) is optimal solely under \(r_{E}\), then the minimal \(k=0\), and \(R_{E,0}\) only contains \(r_{E}\). However, relying on a singleton \(R_{E,0}\) equates to applying vanilla IRL, which is susceptible to misalignment issues, as noted earlier. It is crucial to recognize that \(r_{E}\) might not meet the task-aligned reward function criteria specified in Definition 2, even though its optimal policy \(\pi_{E}\) is acceptable. This situation necessitates a positive \(k\), thereby expanding \(R_{E,k}\) beyond a single function and changing the role of expert demonstrations from strong supervision to weak supervision. Note that we suggest letting \(k\leq|\Pi_{acc}|\) instead of allowing \(k\rightarrow\infty\) because \(R_{E,\infty}\) would then encompass all possible reward functions, and it is impractical to identify a policy capable of achieving high performance across all reward functions. Letting \(k\leq|\Pi_{acc}|\) guarantees there exists a feasible policy \(\pi^{*}\), e.g., \(\pi_{E}\) itself. As the task alignment of each reward function typically remains unknown in IRL settings, this paper proposes treating \(k\) as an adjustable parameter - starting with a small \(k\) and adjusting based on empirical learning outcome, allowing for iterative refinement for alignment with task requirements.

In practice, \(\Pi\) can be uncountable, e.g., a Gaussian policy. Hence, we adapt the concept of \(k\) in \(R_{E,k}\) to a hyperparameter \(\delta\leq\delta^{*}:=\max\limits_{\pi}\mathcal{J}_{IRL}(r)\), leading us to redefine \(R_{E,k}\) as a _\(\delta\)-optimal reward function set \(R_{E,\delta}:=\{r\mid\mathcal{J}_{IRL}(r)\geq\delta\}\)_. This superlevel set includes all the reward functions under which the optimal policies outperform the expert by at most \(-\delta\). If \(\delta\) is appropriately selected such that \(R_{E,\delta}\) includes task-aligned reward functions, we can mitigate reward misalignment by satisfying the conditions outlined in Definition 3, which are closely related to Definition 2 and Proposition 1.

**Definition 3** (**Mitigation of Task-Reward Misalignment)**.: Assuming that the reward function set \(R_{E,\delta}\) contains task-aligned reward function \(r^{+}\)'s, the mitigation of task-reward misalignment in IRL-based IL is to learn a policy \(\pi^{*}\) such that (i) (Weak Acceptance) \(\forall r^{+}\in R_{E,\delta}\), \(U_{r^{+}}(\pi^{*})\geq\underline{U}_{r^{+}}\), or (ii) (Strong Acceptance) \(\forall r^{+}\in R_{E,\delta},U_{r^{+}}(\pi^{*})\geq\overline{U}_{r^{+}}\).

While condition (i) states that \(\pi^{*}\) is acceptable for the task, i.e., \(\pi^{*}\in\Pi_{acc}\), condition (ii) further states that \(\pi^{*}\) have a high order in terms of \(\preceq_{task}\). Hence, condition (i) is weaker than (ii) because a policy \(\pi^{*}\) satisfying (ii) automatically satisfies (i) according to Definition 2. Given the uncertainty in identifying which reward function is aligned, our solution is to **train a policy to achieve high utilities under all reward functions** in \(R_{E,\delta}\) to satisfy the conditions in Definition 3. We explain this approach in the following semi-supervised paradigm, PAGAR.

## 5 Protagonist Antagonist Guided Adversarial Reward (PAGAR)

PAGAR is an adversarial reward searching paradigm which iteratively searches for a reward function to challenge a policy learner by incurring a high regret as defined in Eq.2. We refer to the policy to be learned as the _protagonist policy_ and re-write it as \(\pi_{P}\). We then introduce a second policy, dubbed _antagonist policy_\(\pi_{A}\), as a proxy of the \(\arg\max\limits_{\pi^{\prime}\in\Pi}U_{r}(\pi^{\prime})\) for Eq.2. For each reward function \(r\), we call the regret of \(\pi_{P}\) under \(r\), i.e., \(Regret(\pi_{P},r)=\max\limits_{\pi_{A}\in\Pi}U_{r}(\pi_{A})-U_{r}(\pi_{P})\), the _Protagonist Antagonist Induced Regret_. We then formally define PAGAR in Definition 4.

**Definition 4** (Protagonist Antagonist Guided Adversarial Reward (**PAGAR)**).: Given a candidate reward function set \(R\) and a protagonist policy \(\pi_{P}\), PAGAR searches for a reward function \(r\) within \(R\) to maximize the _Protagonist Antagonist Induced Regret_, i.e., \(\max\limits_{r\in R}Regret(\pi_{P},r)\).

**PAGAR-based IL** _learns a policy from \(R_{E,\delta}\) by minimizing the worst-case Protagonist Antagonist Induced Regret_ via \(MinimaxRegret(R_{E,\delta})\) as defined in Eq.3 where \(R\) can be any input reward function set and is set as \(R=R_{E,\delta}\) in PAGAR-based IL.

\[MinimaxRegret(R):=\arg\min\limits_{\pi_{P}\in\Pi}\max\limits_{r\in R}Regret( \pi_{P},r)\] (3)

Our subsequent discussion will focus on identifying the sufficient conditions for PAGAR-based IL to mitigate task-reward misalignment as described in Definition 3. In particular, we consider the case where \(\mathcal{J}_{IRL}(r):=U_{r}(E)-\max\limits_{\pi}U_{r}(\pi)\). We use \(L_{r}\) to denote the Lipschitz constant of \(r(\tau)\), and \(W_{E}\) to denote the smallest Wasserstein \(1\)-distance \(W_{1}(\pi,E)\) between \(\tau\sim\pi\) of any \(\pi\) and \(\tau\sim E\), i.e., \(W_{E}\triangleq\min\limits_{\pi\in\Pi}W_{1}(\pi,E)\). Then, we have Theorem 2.

**Theorem 2** (Weak Acceptance).: _If the following conditions (1) (2) hold for \(R_{E,\delta}\), then the optimal protagonist policy \(\pi_{P}:=MinimaxRegret(R_{E,\delta})\) satisfies \(\forall r^{+}\in R_{E,\delta}\), \(U_{r^{+}}(\pi_{P})\geq U_{r^{+}}\)._1. _There exists_ \(r^{+}\in R_{E,\delta}\)_, and_ \(\max_{r^{+}\in R_{E,\delta}}\ \{\max_{\pi\in\Pi}U_{r^{+}}(\pi)-\overline{U}_{r^{+}}\}< \min_{r^{+}\in R_{E,\delta}}\ \{\overline{U}_{r^{+}}-U_{r^{+}}\}\)_;_
2. \(\forall r^{+}\in R_{E,\delta}\)_,_ \(L_{r^{+}}\cdot W_{E}-\delta\leq\max_{\pi\in\Pi}U_{r^{+}}(\pi)-\overline{U}_{r ^{+}}\) _and_ \(\forall r^{-}\in R_{E,\delta}\)_,_ \(L_{r^{-}}\cdot W_{E}-\delta<\min_{r^{+}\in R_{E,\delta}}\ \{\overline{U}_{r^{+}}-U_{r^{+}}\}\)_._

This statement shows the conditions for PAGAR-based IL to attain the _'Weak Acceptance'_ goal described in Definition 3. The condition (1) states that the task-aligned reward functions in \(R_{E,\delta}\) all have a high level of alignment in matching \(\preceq_{task}\) within their high utility ranges. The condition (2) requires that for the policy \(\pi^{*}=\arg\min_{\pi\in\Pi}W_{1}(\pi,E)\), the performance difference between \(E\) and \(\pi^{*}\) is small enough under all \(r\in R_{E,\delta}\). Since for each reward function \(r\in R_{E,\delta}\), the performance difference between \(E\) and the optimal policy under \(r\) is bounded by \(\delta\), condition (2) implicitly requires that \(\pi^{*}\) not only performs well under any task-aligned reward function \(r^{+}\) (thus being acceptable in the task) but also achieve relatively low regret under task-misaligned reward function \(r^{-}\). However, the larger the rage \([\overline{U}_{r^{+}},\overline{U}_{r^{+}}]\) is across the task-aligned reward function \(r^{+}\), the less strict the requirement for low regret under \(r^{-}\) becomes. The proof can be found in Appendix A.5. The following theorem further suggests that a \(\delta\) close to its upper-bound \(\delta^{*}:=\max_{r}\mathcal{J}_{IRL}(r)\) can help \(MinimaxRegret(R_{E,\delta})\) gain a better chance of finding an acceptable policy for the underlying task and attain the _'Strong Acceptance'_ goal described in Definition 3.

**Theorem 3** (Strong Acceptance).: _Assume that the condition (1) in Theorem 2 holds for \(R_{E,\delta}\). If for any \(r\in R_{E,\delta}\), \(L_{r}\cdot W_{E}-\delta\leq\min_{r^{+}\in R_{E,\delta}}\ \{\max_{\pi\in\Pi}U_{r^{+}}(\pi)- \overline{U}_{r^{+}}\}\), then the optimal protagonist policy \(\pi_{P}=MinimaxRegret(R_{E,\delta})\) satisfies \(\forall r^{+}\in R_{E,\delta}\), \(U_{r^{+}}(\pi_{P})\geq\overline{U}_{r^{+}}\)._

**When do these assumptions hold?** The condition (1) in Theorem 2 requires all the task-aligned reward functions in \(R_{E,\delta}\) exhibit a high level of conformity with the policy order \(\preceq_{task}\). Being task-aligned already sets a strong premise for satisfying this condition. We further posit that this condition is more easily satisfied when the task has a binary outcome, such as in reach-avoid tasks so that the aligned and misaligned reward functions tend to have higher discrepancy than tasks with quantitative outcomes. In the experimental section, we validate this hypothesis by evaluating tasks of this kind. Regarding condition (2) of Theorem 2 and the assumptions of Theorem 3, which basically require the existence of a policy with low regret across \(R_{E,\delta}\) set, it is reasonable to assume that expert policy meets this criterion.

### Comparing PAGAR-Based IL with IRL-Based IL

We illustrate the difference between IRL-based IL and PAGAR-based IRL in Fig.1(b). While IRL-based IL aims to learn the optimal policy \(\pi_{r^{*}}\) under the IRL-optimal reward \(r^{*}\), PAGAR-based IL learns a policy \(\pi^{*}\) from the reward function set \(R_{E,\delta}\). Both PAGAR-based IL and IRL-based IL are zero-sum games between a policy learner and a reward learner. However, while IRL-based IL only aims to reach equilibrium at a single reward function under strong assumptions, e.g., sufficient demonstrations, convex reward and policy spaces, etc., PAGAR-based IL can reach equilibrium with a **mixture of reward functions** without those assumptions.

**Proposition 2**.: _Given arbitrary reward function set \(R\), there exists a constant \(c\) and a distribution \(\mathcal{R}_{\pi}\) over \(R\) such that \(MinimaxRegret(R)\) yields the same policy as \(\arg\max_{r\in\Pi}\ \left\{\frac{Regret(\pi,r_{\pi}^{*})}{c-U_{\pi^{*}}(\pi)} \cdot U_{r_{\pi}^{*}}(\pi)+\underset{r\sim\mathcal{R}_{\pi}(r)}{\mathbb{E}}[(1- \frac{Regret(\pi,r)}{c-U_{r}(\pi)})\cdot U_{r}(\pi)]\right\}\) where \(r_{\pi}^{*}=\arg\max_{r\in R}U_{r}(\pi)\ s.t.\ r\in\arg\max_{r^{ \prime}\in R}Regret(\pi,r^{\prime})\)._

A detailed derivation can be found in Theorem 6 in Appendix A.4. In a nutshell, \(\mathcal{R}_{\pi}(r)\) is a baseline distribution over \(R\) such that (i) \(c\equiv\underset{r\sim\mathcal{R}_{\pi}}{\mathbb{E}}\ [U_{r}(\pi)]\) holds for all the \(\pi\)'s that do not always perform worse than any other policy under \(r\in R\), (ii) among all the \(R_{\pi}\)'s that satisfy the condition (i), we pick the one with the minimal \(c\); and (iii) for any other policy \(\pi\), \(\mathcal{R}_{\pi}\) uniformly concentrates on \(\arg\max_{r\in R}U_{r}(\pi)\). Note that in PAGAR-based IL, where \(R_{E,\delta}\) is used in place of arbitrary \(R\), \(\mathcal{R}_{\pi}\) is a distribution over \(R_{E,\delta}\) and \(r_{\pi}^{*}\) is constrained to be within \(R_{E,\delta}\). Essentially, the mixed reward functions dynamically assign weights to \(r\sim\mathcal{R}_{\pi}\) and \(r_{\pi}^{*}\) depending on \(\pi\). If \(\pi\) performs worse under \(r_{\pi}^{*}\) than under many other reward functions (\(U_{r_{\pi}^{*}}(\pi)\) falls below \(c\)), a higher weight will be allocated to using \(r_{\pi}^{*}\) to train \(\pi\). Conversely, if \(\pi\) performs better under \(r_{\pi}^{*}\) than under many other reward functions (\(c\) falls below \(Ur_{\pi}^{*}(\pi)\)), a higher weight will be allocated to reward functions drawn from \(\mathcal{R}_{\pi}\). Furthermore, we prove in Appendix A.7 that the \(MinimaxRegret\) objective function defined in Eq.3 is a convex optimization w.r.t the protagonist policy \(\pi_{P}\).

We also prove in Appendix A.8 that when there is no misalignment issue, i.e., under the ideal conditions for IRL, PAGAR-based IL can either guarantee inducing the same results as IRL-based IL with \(\delta=\max\limits_{r}\mathcal{J}_{IRL}(r)\), or guarantee inducing an acceptable \(\pi_{P}\) by making \(\max\limits_{r}\mathcal{J}_{IRL}(r)-\delta\) no greater than \(\max\limits_{\pi\in\Pi}U_{r^{+}}(\pi)-\overline{U}_{r^{+}}\) for \(r^{+}\in R_{E,\delta}\).

## 6 A Practical Approach to Implementing PAGAR-based IL

We solve \(MinimaxRegret(R_{E,\delta})\), by alternating between policy learning and reward search. Based on Eq.3, we introduce an on-and-off policy learning framework and an adversarial reward search objective. Moreover, we embed the constraint \(r\in R_{E,\delta}\) into the reward search objective using IRL, resulting in a **meta-algorithm** compatible with various IRL methods.

### Policy Optimization with On-and-Off Policy Samples

Given an intermediate learned reward function \(r\), we use RL to train \(\pi_{P}\) to minimize the regret \(\min\limits_{\pi_{P}}Regret(\pi_{P},r)=\min\limits_{\pi_{P}}\{\max\limits_{ \pi_{A}}U_{r}(\pi_{A})\}-U_{r}(\pi_{P})\) as indicated by Eq.3 where \(\pi_{A}\) is trained to serve as the optimal policy under \(r\) as noted in Section 5. Since we have to sample trajectories with \(\pi_{A}\) and \(\pi_{P}\), we propose to combine off-policy and on-policy samples to optimize \(\pi_{P}\) so that we can leverage the samples maximally. **Off-Policy:** We leverage the Theorem 1 in Schulman et al. (2015) to derive a bound for the utility subtraction: \(U_{r}(\pi_{P})-U_{r}(\pi_{A})\leq\sum\limits_{s\in\mathbb{S}}\rho_{\pi_{A}}(s )\sum\limits_{a\in\mathbb{A}}\pi_{P}(a|s)A_{\pi_{A}}(s,a)+C\cdot\max\limits_{ s}D_{TV}(\pi_{A}(\cdot|s),\pi_{P}(\cdot|s))^{2}\) where \(\rho_{\pi_{A}}(s)=\sum_{t=0}^{T}\gamma^{t}Prob(s^{(t)}=s|\pi_{A})\) is the discounted visitation frequency of \(\pi_{A}\), \(A_{\pi_{A}}\) is the advantage function without considering the entropy, and \(C\) is some constant. Then we follow the derivation in Schulman et al. (2017), which is based on Theorem 1 in Schulman et al. (2015), to derive from the inequality an importance sampling-based objective function \(\mathcal{J}_{\pi_{A}}(\pi_{P};r):=\mathbb{E}_{s\sim\pi_{A}}[\min(\xi(s,a)\cdot A _{\pi_{A}}(s,a),clip(\xi(s,a),1-\sigma,1+\sigma)\cdot A_{\pi_{A}}(s,a)]\) where \(\sigma\) is a clipping threshold, \(\xi(s,a)=\frac{\pi_{P}(a|s)}{\pi_{A}(a|s)}\) is an importance sampling rate. The details can be found in Appendix B.1. This objective function allows us to train \(\pi_{P}\) by using the trajectories of \(\pi_{A}\). **On-Policy:** We also optimize \(\pi_{P}\) with the standard RL objective function \(\mathcal{J}_{RL}(\pi_{P};r)\) by using the trajectories of \(\pi_{P}\) itself. As a result, the objective function for optimizing \(\pi_{P}\) is \(\max\limits_{\pi_{P}\in\Pi}\mathcal{J}_{\pi_{A}}(\pi_{P};r)+\mathcal{J}_{RL} (\pi_{P};r)\). As for \(\pi_{A}\), we only use the standard RL objective function, i.e., \(\max\limits_{\pi_{A}\in\Pi}\mathcal{J}_{RL}(\pi_{A};r)\). Although the computational complexity equals the sum of the complexities of RL update steps for \(\pi_{A}\) and \(\pi_{P}\), these two RL update steps can be executed in parallel.

### Regret Maxmization with On-and-Off Policy Samples

Given the intermediate learned protagonist and antagonist policy \(\pi_{P}\) and \(\pi_{A}\), according to \(MinimaxRegret\) in Eq.3, we need to optimize \(r\) to maximize \(U_{r}(\pi_{A})-U_{r}(\pi_{P})\). In practice, we found that the subtraction between the estimated \(U_{r}(\pi_{A})\) and \(U_{r}(\pi_{P})\) can have a high variance. To resolve this issue, we derive two reward improvement bounds to approximate this subtraction.

**Theorem 4**.: _Suppose policy \(\pi_{2}\in\Pi\) is the optimal solution for \(\mathcal{J}_{RL}(\pi;r)\). Then, the inequalities Eq.4 and 5 hold for any policy \(\pi_{1}\in\Pi\), where \(\alpha=\max\limits_{s}D_{TV}(\pi_{1}(\cdot|s),\pi_{2}(\cdot|s))\), \(\epsilon=\max\limits_{s,a}|\mathcal{A}_{\pi_{2}}(s,a)|\), and \(\Delta\mathcal{A}(s)=\underset{a\sim\pi_{1}}{\mathbb{E}}[\mathcal{A}_{\pi_{2}}(s,a)]-\underset{a\sim\pi_{2}}{\mathbb{E}}[\mathcal{A}_{\pi_{2}}(s,a)]\)._

\[\left|U_{r}(\pi_{1})-U_{r}(\pi_{2})-\sum\limits_{t=0}^{\infty} \gamma^{t}\underset{s^{(t)}\sim\pi_{1}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{ (t)})\right]\right| \leq\frac{2\alpha\gamma\epsilon}{(1-\gamma)^{2}}\] (4) \[\left|U_{r}(\pi_{1})-U_{r}(\pi_{2})-\sum\limits_{t=0}^{\infty} \gamma^{t}\underset{s^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{ (t)})\right]\right| \leq\frac{2\alpha\gamma(2\alpha+1)\epsilon}{(1-\gamma)^{2}}\] (5)By letting \(\pi_{P}\) be \(\pi_{1}\) and \(\pi_{A}\) be \(\pi_{2}\), Theorem 4 enables us to bound \(U_{r}(\pi_{A})-U_{r}(\pi_{P})\) by using either only the samples of \(\pi_{A}\) or only those of \(\pi_{P}\). Following Fu et al. (2018), we let \(r\) be a proxy of \(\mathcal{A}_{\pi_{2}}\) in Eq.4 and 5. Then we derive two loss functions \(\mathcal{J}_{R,1}(r;\pi_{P},\pi_{A})\) and \(\mathcal{J}_{R,2}(r;\pi_{P},\pi_{A})\) for \(r\) as shown in Eq.6 and 7 where \(C_{1}\) and \(C_{2}\) are constants proportional to the estimated maximum KL divergence between \(\pi_{A}\) and \(\pi_{P}\) (to bound \(\alpha\) Schulman et al. (2015)). The objective function for \(r\) is then \(\mathcal{J}_{PAGAR}:=\mathcal{J}_{R,1}+\mathcal{J}_{R,2}\). The complexity equals that of computing the reward along the trajectories sampled from \(\pi_{A}\) and \(\pi_{P}\).

\[\mathcal{J}_{R,1}(r;\pi_{P},\pi_{A}) :=\mathop{\mathbb{E}}_{\tau\sim\pi_{A}}\big{[}\sum_{t=0}^{\infty} \gamma^{t}\left(\xi(s^{(t)},a^{(t)})-1\right)\cdot r(s^{(t)},a^{(t)})\big{]}+C _{1}\cdot\max_{(s,a)\sim\pi_{A}}\lvert r(s,a)\rvert\] (6) \[\mathcal{J}_{R,2}(r;\pi_{P},\pi_{A}) :=\mathop{\mathbb{E}}_{\tau\sim\pi_{P}}\Big{[}\sum_{t=0}^{\infty} \gamma^{t}\left(1-\frac{1}{\xi(s^{(t)},a^{(t)})}\right)\cdot r(s^{(t)},a^{(t)} )\Big{]}+C_{2}\cdot\max_{(s,a)\sim\pi_{P}}\lvert r(s,a)\rvert\,\eqref{eq:rate_ 1}\] (7)

### A Meta-Algorithm for Solving PAGAR-Based IL

Given an IRL objective function \(\mathcal{J}_{IRL}\), we enforce the constraint \(r\in R_{E,\delta}\) by adding to \(\mathcal{J}_{PAGAR}(r;\pi_{P},\pi_{A})\) a penalty term \(\lambda\cdot(\delta-\mathcal{J}_{IRL})\), where \(\lambda\) is a Lagrangian parameter. The resulting objective function for optimizing \(r\) becomes \(\min\limits_{r\in R}\mathcal{J}_{PAGAR}(r;\pi_{P},\pi_{A})+\lambda\cdot(\delta -\mathcal{J}_{IRL}(r))\).

We initialize \(\lambda\) with a large value to prioritize satisfying the constraint \(r\in R_{E,\delta}\) and update it based on \(\delta-\mathcal{J}_{IRL}\) (the details can be found in Appendix B.4). Algorithm 1 outlines our meta-algorithm for PAGAR-based IL. The algorithm takes an IRL objective \(\mathcal{J}_{IRL}\) as an input, and alternates between policy and reward learning. In line \(3\), \(\pi_{A}\) is trained via RL with its own sample set \(\mathbb{D}_{A}\). In line \(4\), we train \(\pi_{P}\) via the on-and-off policy approach in Section 6.1 with both \(\mathbb{D}_{A}\) and \(\pi_{P}\)'s sample set \(\mathbb{D}_{P}\). Finally, in line \(5\), \(\mathcal{J}_{PAGAR}\) is estimated from both \(\mathbb{D}_{A}\) and \(\mathbb{D}_{P}\), while \(\mathcal{J}_{IRL}\) is from \(\mathbb{D}_{A}\) and \(E\).

## 7 Experiments

The goal of our experiments is to assess whether using PAGAR-based IL can efficiently mitigate reward misalignment under conditions that are not ideal for IRL. We present the main results below and provide details and additional results in Appendix C.

### Discrete Navigation Tasks

**Benchmarks:** We consider a maze navigation environment where the task objective is compatible with Definition 1. Our benchmarks include two discrete domain tasks from the Mini-Grid environments Chevalier-Boisvert et al. (2023): _DoorKey-&x6-v0_, and _SimpleCrossingS9N1-v0_. In both tasks, the agent needs to interact with the environmental objects which are **randomly positioned in every episode while the agent can only observe a small, unblocked area in front of it**. The default reward, which is always zero unless the agent reaches the target, is used to evaluate the performance of learned policies. Due to partial observability and the implicit hierarchical nature of the task, these environments are considered challenging for RL and IL, and have been extensively used for benchmarking curriculum RL and exploration-driven RL.

**Baselines:** We compare our approach with two standard baselines: GAIL Ho and Ermon (2016) and VAIL Peng et al. (2019). GAIL has been introduced in Section 3. VAIL is based on GAIL but additionally optimizes a variational discriminator bottleneck (VDB) objective. Our approach uses the IRL techniques behind those two baseline algorithms, resulting in two versions of Algorithm 1, denoted as PAGAR-GAIL and PAGAR-VAIL, respectively. More specifically, if the baseline optimizes a \(J_{IRL}\) objective, we use the same \(J_{IRL}\) objective in Algorithm 1. Also, we extract the reward function \(r\) from the discriminator \(D\) as mentioned in Section 3. More details are in Appendix C.1. PPO Schulman et al. (2017) is used for policy training in GAIL, VAIL, and ours with a replay buffer of size \(2048\). Additionally, we compare our algorithm with a state-of-the-art (SOTA) IL algorithm, IQ-Learn Garg et al. (2021), which, however, is not compatible with our algorithm because it does not explicitly optimize a reward function. The policy and the reward functions are all approximated using convolutional networks.

**IL with Limited Demonstrations.** By learning from \(10\) expert-demonstrated trajectories with high returns, PAGAR-based IL produces high-performance policies with high sample efficiencies as shown in Figure 2(a) and (c). Furthermore, we compare PAGAR-VAIL with VAIL by reducing the number of demonstrations from \(10\) to \(1\). As shown in Figure 2(b) and (d), PAGAR-VAIL produces high-performance policies with significantly higher sample efficiencies.

**IL under Dynamics Mismatch.** We demonstrate that PAGAR enables the agent to infer and accomplish the objective of a task even in environments that are substantially different from the one observed during expert demonstrations. As shown in Figure 2(e), we collect \(10\) expert demonstrations from the _SimpleCrossingS9N1-v0_ environment. Then we apply Algorithm 1 and the baselines, GAIL, VAIL, and IQ-learn to learn policies in _SimpleCrossingS9N2-v0_, _SimpleCrossingS9N3-v0_ and _FourRooms-v0_. The results in Figure 2(f)-(g) show that PAGAR-based IL outperforms the baselines in these challenging zero-shot settings.

**IL with Different Reward Hypothesis Sets.** The foundational theories of GAIL and AIRL indicate that different reward function hypothesis sets can affect the equilibrium of their GAN frameworks. We study whether choosing different reward hypothesis sets can influence the performance of Algorithm 1. We compare using a \(Sigmoid\) function with a Categorical distribution in the output layer of the discriminator networks in GAIL and PAGAR-GAIL. When using the \(Sigmoid\) function, the outputs of \(D\) are not normalized, i.e., \(\sum_{a\in\mathbb{A}}D(s,a)\neq 1\). When using a Categorical distribution, \(\sum_{a\in\mathbb{A}}D(s,a)=1\). We test GAIL and PAGAR-GAIL in _DoorKey-6x6-v0_ environment. As shown in Figure 3, PAGAR-GAIL outperforms GAIL in both cases by using fewer samples.

Figure 3: PAGAR-GAIL in different reward spaces

Figure 2: Comparing Algorithm 1 with baselines in partial observable navigation tasks. The suffix after each ‘PAGAR-’ indicates which IRL technique is used in Algorithm 1. The \(y\) axis indicates the average return per episode. The \(x\) axis indicates the number of time steps.

### Continuous Control Tasks

We evaluate PAGAR-based IL on continuous control tasks in both online and offline RL settings, demonstrating its ability to improve IRL-based IL performance across different types of tasks.

**Benchmarks:** We use four continuous control environments from Mujoco. In the online RL setting, both protagonist and antagonist policies are permitted to explore the environment. In the offline RL setting, exploration by these policies is restricted. Especially, for offline RL we use the D4RL's 'expert' datasets as the expert demonstrations and the 'random' datasets as the offline suboptimal dataset. Policy performance is evaluated online in both settings by using the default reward function of the environment.

**Baselines:** We compare PAGAR-based IL against f-IRL Ni et al. (2021) in the online RL setting and compare with RECOIL Sikichi et al. (2024) in the offline RL setting. When comparing with f-IRL, we use f-IRL as the IRL algorithm in Algorithm 1. When comparing with RECOIL, as RECOIL does not directly learn the reward function but learns the Q and V functions, we develop another algorithm for the offline RL setting to combine PAGAR with RECOIL by explicitly learning a reward function and using the reward function and V function to represent the Q function. The details can be found in Appendix B.4.

**Results:** As shown in Figure 4, PAGAR-based IL achieves equivalent performance to the baselines with fewer iterations. Furthermore, on the _Ant_ and _Walker2d_ tasks, Algorithm 1 matches the performance level of f-IRL using significantly less iterations. Additional results of PAGAR with GAIL and VAIL across other continuous control benchmarks are provided in Appendix C.3. Table 1 further shows that when combined with RECOIL, PAGAR-based IL achieves higher performance in most of the tasks than the baseline. These results demonstrate the broader applicability of PAGAR-based IL in both online and offline settings and its effectiveness across different types of environments, further reinforcing the robustness of our approach.

## 8 Conclusion

In this paper, we propose to prioritize task alignment over conventional data alignment in IRL-based IL by treating expert demonstrations as weak supervision signals to derive a set of candidate reward functions that align with the task rather than only with the data. Our PAGAR-based IL adopts an adversarial mechanism to train a policy with this set of reward functions. Experimental results demonstrate that our algorithm can mitigate reward misalignment in challenging environments. Our future work will focus on employing the PAGAR paradigm to other task alignment problems.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & RECOIL & PAGAR-RECOIL \\ \hline hopper-random & \(106.87\pm 2.69\) & \(\mathbf{111.16\pm 0.51}\) \\ \hline halfcheetah-random & \(80.84\pm 17.62\) & \(\mathbf{92.94\pm 0.10}\) \\ \hline walker2d-random & \(108.40\pm 0.04\) & \(108.40\pm 0.12\) \\ \hline ant-random & \(113.34\pm 2.78\) & \(\mathbf{121\pm 5.86}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Offline RL results obtained by combining PAGAR with RECOIL averaged over \(4\) seeds.

Figure 4: Comparing Algorithm 1 with f-IRL in continuous control tasks. ‘PAGAR-fIRL’ indicates f-IRL is used as the inverse RL algorithm in Algorithm 1. The \(y\) axis indicates the average return per episode. The \(x\) axis indicates the number of time steps in the environment.

## Acknowledgment

This work was supported in part by the U.S. National Science Foundation under grant CCF-2340776.

## References

* Abbeel and Ng (2004) P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In _Proceedings of the Twenty-First International Conference on Machine Learning_, ICML '04, page 1, New York, NY, USA, 2004. Association for Computing Machinery. ISBN 1581138385. doi: 10.1145/1015330.1015430.
* Abel et al. (2021) D. Abel, W. Dabney, A. Harutyunyan, M. K. Ho, M. Littman, D. Precup, and S. Singh. On the expressivity of markov reward. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* Amodei et al. (2016) D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mane. Concrete problems in AI safety. _CoRR_, abs/1606.06565, 2016.
* Cao et al. (2021) H. Cao, S. Cohen, and L. Szpruch. Identifiability in inverse reinforcement learning. _Advances in Neural Information Processing Systems_, 34:12362-12373, 2021.
* Chevalier-Boisvert et al. (2023) M. Chevalier-Boisvert, B. Dai, M. Towers, R. de Lazcano, L. Willems, S. Lahlou, S. Pal, P. S. Castro, and J. Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. _CoRR_, abs/2306.13831, 2023.
* Dennis et al. (2020) M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and S. Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 13049-13061. Curran Associates, Inc., 2020.
* Finn et al. (2016) C. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In _International conference on machine learning_, pages 49-58. PMLR, 2016.
* Finn et al. (2017) C. Finn, T. Yu, J. Fu, P. Abbeel, and S. Levine. Generalizing skills with semi-supervised reinforcement learning. In _International Conference on Learning Representations_, 2017.
* Fu et al. (2018) J. Fu, K. Luo, and S. Levine. Learning robust rewards with adverserial inverse reinforcement learning. In _International Conference on Learning Representations_, 2018.
* Garg et al. (2021) D. Garg, S. Chakraborty, C. Cundy, J. Song, and S. Ermon. IQ-learn: Inverse soft-q learning for imitation. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* Hadfield-Menell et al. (2017) D. Hadfield-Menell, S. Milli, P. Abbeel, S. J. Russell, and A. Dragan. Inverse reward design. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Hejna and Sadigh (2023) J. Hejna and D. Sadigh. Inverse preference learning: Preference-based RL without a reward function. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=gAP5222dar.
* Ho and Ermon (2016) J. Ho and S. Ermon. Generative adversarial imitation learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.
* Hong et al. (2023) J. Hong, K. Bhatia, and A. Dragan. On the sensitivity of reward inference to misspecified human models. In _The Eleventh International Conference on Learning Representations_, 2023.
* Jeon et al. (2018) W. Jeon, S. Seo, and K.-E. Kim. A bayesian approach to generative adversarial imitation learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* Krizhevsky et al. (2014)D. Lindner, A. Krause, and G. Ramponi. Active exploration for inverse reinforcement learning. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Metelli et al. [2021] A. M. Metelli, G. Ramponi, A. Concetti, and M. Restelli. Provably efficient learning of transferable rewards. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 7665-7676. PMLR, 18-24 Jul 2021.
* Ng and Russell [2000] A. Y. Ng and S. J. Russell. Algorithms for inverse reinforcement learning. In _Proceedings of the Seventeenth International Conference on Machine Learning_, ICML '00, pages 663-670, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1-55860-707-2.
* Ng et al. [1999] A. Y. Ng, D. Harada, and S. J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In _Proceedings of the Sixteenth International Conference on Machine Learning_, ICML '99, pages 278-287, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. ISBN 1-55860-612-2.
* Ni et al. [2021] T. Ni, H. Sikchi, Y. Wang, T. Gupta, L. Lee, and B. Eysenbach. f-irl: Inverse reinforcement learning via state marginal matching. In _Conference on Robot Learning_, pages 529-551. PMLR, 2021.
* Pan et al. [2022] A. Pan, K. Bhatia, and J. Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=JYtw6wIL7ye.
* Peng et al. [2019] X. B. Peng, A. Kanazawa, S. Toyer, P. Abbeel, and S. Levine. Variational discriminator bottleneck: Improving imitation learning, inverse RL, and GANs by constraining information flow. In _International Conference on Learning Representations_, 2019.
* Ramachandran and Amir [2007] D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In _Proceedings of the 20th International Joint Conference on Artifical Intelligence_, IJCAI'07, page 2586-2591, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.
* Ratliff et al. [2006] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich. Maximum margin planning. In _Proceedings of the 23rd international conference on Machine learning_, pages 729-736, 2006.
* Ross and Bagnell [2010] S. Ross and D. Bagnell. Efficient reductions for imitation learning. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pages 661-668, 2010.
* Ross et al. [2011] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 627-635, 2011.
* Schulman et al. [2015] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* Schulman et al. [2017] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. _CoRR_, abs/1707.06347, 2017.
* Sikchi et al. [2024] H. Sikchi, Q. Zheng, A. Zhang, and S. Niekum. Dual RL: Unification and new methods for reinforcement and imitation learning. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=xt9Bu66rqv.
* Skalse and Abate [2022] J. Skalse and A. Abate. Misspecification in inverse reinforcement learning. _arXiv preprint arXiv:2212.03201_, 2022.
* Skalse et al. [2022a] J. Skalse, M. Farrugia-Roberts, S. Russell, A. Abate, and A. Gleave. Invariance in policy optimisation and partial identifiability in reward learning. _arXiv preprint arXiv:2203.07475_, 2022a.
* Skalse et al. [2022b] J. M. V. Skalse, N. H. R. Howe, D. Krasheninnikov, and D. Krueger. Defining and characterizing reward gaming. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022b.
* Sutton and Barto [2018] R. S. Sutton and A. G. Barto. _Reinforcement learning: An introduction_. 2018.
* Sutton et al. [2015]L. Viano, Y.-T. Huang, P. Kamalaruban, A. Weller, and V. Cevher. Robust inverse reinforcement learning under transition dynamics mismatch. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=t8HduwpoQQv.
* Zhou and Li (2018) W. Zhou and W. Li. Safety-aware apprenticeship learning. In H. Chockler and G. Weissenbacher, editors, _Computer Aided Verification_, pages 662-680, Cham, 2018. Springer International Publishing. ISBN 978-3-319-96145-3.
* Zhou and Li (2022a) W. Zhou and W. Li. Programmatic reward design by example. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(8):9233-9241, Jun. 2022a. doi: 10.1609/aaai.v36i8.20910.
* Zhou and Li (2022b) W. Zhou and W. Li. A hierarchical Bayesian approach to inverse reinforcement learning with symbolic reward machines. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 27159-27178. PMLR, 17-23 Jul 2022b.
* Zhou et al. (2020) W. Zhou, R. Gao, B. Kim, E. Kang, and W. Li. Runtime-safety-guided policy repair. In _Runtime Verification: 20th International Conference, RV 2020, Los Angeles, CA, USA, October 6-9, 2020, Proceedings 20_, pages 131-150. Springer, 2020.
* Volume 3_, AAAI'08, pages 1433-1438. AAAI Press, 2008. ISBN 978-1-57735-368-3.
Reward Design with PAGAR

This paper does not aim to resolve the ambiguity problem in IRL but provides a way to circumvent it so that reward ambiguity does not lead to reward misalignment in IRL-based IL. PAGAR, the semi-supervised reward design paradigm proposed in this paper, tackles this problem from the perspective of semi-supervised reward design. But the nature of PAGAR is distinct from IRL and IL: assume that a set of reward functions is available for some underlying task, where some of those reward functions align with the task while others are misaligned, PAGAR provides a solution for selecting reward functions to train a policy that successfully performs the task, without knowing which reward function aligns with the task. Our research demonstrates that policy training with PAGAR is equivalent to learning a policy to maximize an affine combination of utilities measured under a distribution of the reward functions in the reward function set. With this understanding of PAGAR, we integrate it with IL to illustrate its advantages.

### Motivation: Failures in IRL-Based IL

For readers' convenience, we put the theorem from Abel et al. (2021) here for reference.

**Theorem 5**.: _Viano et al. (2021)_ _If the demonstration environment has a dynamics function \(\mathcal{P}_{demo}\) different from the dynamics function \(\mathcal{P}\) in the learning environment, the performance gap between the policies \(\pi_{E}\) and \(\pi_{r^{*}}\) under the ground true reward function \(r_{E}\) satisfies \(|\ U_{r_{E}}(\pi_{E})-U_{r_{E}}(\pi_{r^{*}})\ |\ \leq\frac{2\cdot\gamma\max\limits_{s,a}|\ r_{E}(s,a) }{(1-\gamma)^{2}}\cdot\max\limits_{s,a}D_{TV}(\mathcal{P}(\cdot\ |\ s,a),\mathcal{P}_{demo}(\cdot\ |\ s,a))\)._

Then, we prove that a limited number of demonstrations can also lead to the performance gap as described in Theorem 5. Essentially, we can construct a demonstration dynamics \(\mathcal{P}_{demo}\) such that all the state transition probability estimated in \(E\) match \(\mathcal{P}_{demo}\) with zero error. Then a similar performance gap as that caused by dynamics mismatch can be derived.

**Proposition 3**.: _Assume that the expert demonstration contains \(m\) state-action pairs by only selecting the optimal actions, i.e., select \(\arg\max\limits_{a}\ \pi_{E}(a|s)\) at each \(s\). Then \(|\ U_{r_{E}}(\pi_{E})-U_{r_{E}}(\pi_{r^{*}})\ |\ \leq\frac{2\cdot d\cdot\gamma\max \limits_{s,a}|\ r_{E}(s,a)\ |}{(1-\gamma)^{2}}\) where \(Prob(d\geq\epsilon)\leq 2\cdot\exp(-2\cdot m\cdot\epsilon^{2})\) for any \(\epsilon\in[\underline{p},1]\) where \(\underline{p}=\frac{1}{\gamma}\cdot\frac{\min\limits_{s,a}r_{E}(s,a)-\min \limits_{s}r_{E}(s,\arg\max\limits_{a}\pi_{E}(a|s))}{\max\limits_{s,a}r_{E}(s, a)-\min\limits_{s,a}r_{E}(s,a)}\)._

Proof.: We translate the limited demonstration case into a dynamics mismatch case where we will leverage the demonstrated state transitions to construct a dynamics \(\mathcal{P}_{demo}(\cdot\ |\ s,a)\) and compare it with \(\mathcal{P}(\cdot\ |\ s,a)\).

Assume that for each state-action pair \(s,a\) in \(E\), i.e., \(m_{s,a}\in[m]\), the \(m_{s,a}\) transition instances are \((s,a,s_{1}),(s,a,s_{2}),\ldots,(s,a,s_{m_{s,a}})\). For each \(\hat{s}\in\mathbb{S}\), we let \(\mathcal{P}_{demo}(\hat{s}\ |\ s,a)=\frac{1}{m_{s,a}}\sum_{i=1}^{m_{s,a}} \mathcal{I}[s_{i}=\hat{s}]\).

Then, we can view \(D_{TV}(\mathcal{P}(\cdot\ |\ s,a),\mathcal{P}_{demo}(\cdot\ |\ s,a))=\frac{1}{2}\ || \mathcal{P}(\cdot\ |\ s,a)-\mathcal{P}_{demo}(\cdot\ |\ s,a))||_{1}\) as a function of \(m\) independent random variables, i.e., \((s_{1},a_{1}),(s_{2},a_{2}),\ldots,(s_{m_{s,a},m_{s,a}})\), sampled from the \(\mathbb{S}\) domain. If one of the variables, \(s_{i}\), is changed from \(\hat{s}\) to another state \(\hat{s}^{\prime}\neq\hat{s}\), \(\mathcal{P}_{demo}(\hat{s}\ |\ s,a)\) and \(\mathcal{P}_{demo}(\hat{s}^{\prime}\ |\ s,a)\) should decrease and increase by \(\frac{1}{m_{s,a}}\) respectively. Then, \(D_{TV}(\mathcal{P}(\cdot\ |\ s,a),\mathcal{P}_{demo}(\cdot\ |\ s,a))\) changes at most by \(\frac{1}{m_{s,a}}\).

We can also view the \(D_{TV}\) at each \((s,a)\in E\) as functions of the \(m\) sampled state-action pairs in \(E\), with a little abuse of notations, denoted as \((s_{1},a_{1}),(s_{2},a_{2}),\ldots,(s_{m},a_{m})\) by flattening all the demonstrated trajectories. If a state-action pair, \((s_{i},a_{i})\), in one of the trajectories, is changed from \((\hat{s},\hat{a})\) to another state \((\hat{s}^{\prime},\hat{a}^{\prime})\), then for the state-action \((s_{i-1},a_{i-1})\) that precedes \(s_{i}\) if \(s_{i}\) is not the initial state, \(D_{TV}(\mathcal{P}(\cdot\ |\ s,a),\mathcal{P}_{demo}(\cdot\ |\ s,a))\) changes at most by \(\frac{1}{m_{s_{i-1},a_{i-1}}}\) which can be as large as \(1\) since it is possible that \(m_{s_{i-1},a_{i-1}}=1\). Also, \(D_{TV}(\mathcal{P}(\cdot\ |\ \hat{s},\hat{a}),\mathcal{P}_{demo}(\cdot\ |\ s,\hat{a}))\) and \(D_{TV}(\mathcal{P}(\cdot\ |\ \hat{s}^{\prime},\hat{a}^{\prime}),\mathcal{P}_{demo}(\cdot\ |\ \hat{s}^{\prime},\hat{a}^{ \prime}))\) change at most by \(\frac{1}{m_{\hat{s},\hat{a}}}\) and \(\frac{1}{m_{\hat{s}^{\prime},\hat{a}^{\prime}}}\). Note that if \(m_{\hat{s}^{\prime},\hat{a}^{\prime}}=0\), i.e., \((\hat{s}^{\prime},\hat{a}^{\prime})\notin E\), changing \(\hat{s},\hat{a}\) into \(\hat{s}^{\prime},\hat{a}^{\prime}\) equivalently leads to taking \(D_{TV}(\mathcal{P}(\cdot\ |\ \hat{s},\hat{a},\mathcal{P}_{demo}(\cdot\ |\ \hat{s}^{\prime},\hat{a}^{ \prime}))\) into consideration when computing\(\max\limits_{s,a}D_{TV}(\mathcal{P}(\cdot\mid s,a),\mathcal{P}_{demo}(\cdot \mid s,a))\). And \(D_{TV}(\mathcal{P}(\cdot\mid\hat{s},\hat{a},\mathcal{P}_{demo}(\cdot\mid\hat{s ^{\prime}},\hat{a^{\prime}}))\leq 1-\frac{\mathcal{P}(s_{i+1}|\hat{s^{\prime}},\hat{a^{ \prime}}))}{2}\) where \(s_{i+1}\) is the state that succeeds \(s_{i}\) in the demonstrated trajecterro. Therefore, if changing a state-action pair in \(E\) into another state-action pair, \(\max\limits_{(s,a)\in E}D_{TV}(\mathcal{P}(\cdot\mid s,a),\mathcal{P}_{demo}( \cdot\mid s,a))\) can be changed as large as by \(1\).

According to McDiarmid's Inequality, \(Prob(|\max\limits_{(s,a)\in E}D_{TV}(\mathcal{P}(\cdot\mid s,a),\mathcal{P}_{ demo}(\cdot\mid s,a))-\mathbb{E}[\max\limits_{(s,a)\in E}D_{TV}(\mathcal{P}( \cdot\mid s,a),\mathcal{P}_{demo}(\cdot\mid s,a))]|\geq\epsilon)\leq 2\cdot\exp(- \frac{2\epsilon^{2}}{\sum_{i=1}^{m}(1)^{2}})=2\cdot\exp(-\frac{2\epsilon^{2}} {m})\). Note that \(\mathbb{E}[\max\limits_{s,a}D_{TV}(\mathcal{P}(\cdot\mid s,a),\mathcal{P}_{ demo}(\cdot\mid s,a))]=0\) since the estimation is un-biased. Hence,

\(Prob(\max\limits_{(s,a)\in E}D_{TV}(\mathcal{P}(\cdot\mid s,a),\mathcal{P}_{ demo}(\cdot\mid s,a))\geq\epsilon)\leq 2\cdot\exp(-\frac{2\epsilon^{2}}{m})\) for \((s,a)\in E\).

For state-action pairs that are outside of \(E\), we need to construct \(\mathcal{P}_{demo}\) at those state-action pairs such that the action selection in \(E\) is always optimal. To start with, we let \(\mathcal{P}_{demo}(\cdot\mid a,s)\equiv\mathcal{P}(\cdot\mid a,s)\) for any state-action pairs that do not appear in \(E\). For some state-action pairs, the states are not reached in \(E\). We can disregard whether \(\pi_{E}\) maintains optimal at those states and focus on the states that do appear in \(E\). By denoting the Q-value of \(\pi_{E}\) under \(\mathcal{P}_{demo}\) as \(Q_{\pi_{E}}^{demo}\), we need to make sure that the \(Q_{\pi_{E}}^{demo}(s,a^{\prime})\) for \(a^{\prime}\neq\arg\max\limits_{a}\pi_{E}(a|s)\) is no greater than \(Q_{\pi_{E}}^{demo}(s,\arg\max\limits_{a}\pi_{E}(a|s))\). Since the state-actions in \(E\) may have extremely low rewards stochastic nature of \(\mathcal{P}\), we consider the worst case Q-value for \(a^{*}=\arg\max\limits_{a}\pi_{E}(a|s)\) i.e., \(Q_{\pi_{E}}^{demo}(s,a^{*})\geq\frac{1}{1-\gamma}\cdot\min\limits_{s}r_{E}( s,\arg\max\limits_{a}\pi_{E}(a|s))\). Meanwhile, for \(a^{\prime}\neq\arg\max\limits_{a}\pi_{E}(a|s)\), we consider the best-case Q-value, i.e., \(Q_{\pi_{E}}^{demo}(s,a^{\prime})\leq\frac{1}{1-\gamma}\cdot\max\limits_{(s,a)}r _{E}(s,a)\). We add a dummy, absorbing state \(\underline{s}\) that is unreachable from any state-action under the dynamics \(\mathcal{P}\) and also unreachable from any demonstrated state-action under the constructed dynamics \(\mathcal{P}_{demo}\). But we let \(\mathcal{P}_{demo}(\underline{s}|s,a^{\prime})>0\) if \(Q_{\pi_{E}}^{demo}(s,a^{\prime})>Q_{\pi_{E}}^{demo}(s,a^{*})\). In other words, we add probability density on transitioning from such \((s,a^{\prime})\) to \(\underline{s}\) so that \(Q_{\pi_{E}}^{demo}(s,a^{\prime})\) drops below \(Q_{\pi_{E}}^{demo}(s,a^{*})\). We denote this density as \(\underline{p}\). Then \(\mathcal{P}_{demo}(\underline{s}|s,a^{\prime})=\underline{p}\) and \(\mathcal{P}_{demo}(s^{\prime}|s,a^{\prime})=(1-\underline{p})\cdot\mathcal{P} (s^{\prime}|s,a^{\prime})\) for any other \(s^{\prime}\in\mathbb{S}\). As a result, \(D_{TV}(\mathcal{P}(\cdot\mid s,a^{\prime}),\mathcal{P}_{demo}(\cdot\mid s,a^ {\prime})=\underline{p}\). Note that adding \(\underline{s}\) does not affect the Q-values of the state-action pairs that appear in \(E\). Then we aim to find the \(\underline{p}\) to ensure \(Q_{\pi_{E}}^{demo}(s,a^{\prime})\leq Q_{\pi_{E}}^{demo}(s,a^{*})\). Considering the worst-case, \(\underline{p}\cdot\(\max\limits_{s,a}r_{E}(s,a)+\frac{\gamma}{1-\gamma}\cdot \min\limits_{s,a}r_{E}(s,a))+(1-\underline{p})\cdot\frac{1}{1-\gamma}\cdot \max\limits_{s,a}r_{E}(s,a)\leq\frac{1}{1-\gamma}\cdot\min\limits_{s}r_{E}(s, \arg\max\limits_{a}\pi_{E}(a|s))\) gives \(\underline{p}\leq\frac{1}{\gamma}\cdot\frac{\max\limits_{s,a}r_{E}(s,a)-\min \limits_{s}r_{E}(s,\arg\max\limits_{a}\pi_{E}(a|s))}{\frac{1}{n_{s,a}^{\prime}} r_{E}(s,a)-\min\limits_{s,a}r_{E}(s,a)}\).

Combining the analysis on the state-action pairs that appear in \(E\) and not in \(E\), we can conclude that for \(\epsilon>\underline{p}\), we can extend the confidence bound \(Prob(\max\limits_{(s,a)\in E}D_{TV}(\mathcal{P}(\cdot\mid s,a),\mathcal{P}_{demo}( \cdot\mid s,a))\geq\epsilon)\leq 2\cdot\exp(-\frac{2\epsilon^{2}}{m})\) for \((s,a)\in E\) from \((s,a)\in E\) to all state-action pairs in \(\mathbb{S}\times\mathbb{A}\). By using a variable \(d\) to represent \(\max\limits_{s,a}D_{TV}(\mathcal{P}(\cdot\mid s,a)-\mathcal{P}_{demo}(\cdot\mid s,a))\), we can use the conclusion drawn from Theorem 5. Note that adding the dummy \(\underline{s}\) does not affect the \(\max\limits_{s,a}|r_{E}(s,a)|\) since we let \(r_{E}(\underline{s},a)\equiv\min\limits_{s,a}r_{E}(s,a)\). Our proof is complete. 

### Task-Reward Alignment

In this section, we provide proof of the properties of the task-reward alignment concept that we define in the main text. For readers' convenience, we include our definition of _task_ for reference.

**Definition** 1 (**Task**) Given the policy hypothesis set \(\Pi\), a **task**\((\Pi,\preceq_{task},\Pi_{acc})\) is specified by a partial order over \(\Pi\) and a non-empty set of acceptable policies \(\Pi_{acc}\subseteq\Pi\) such that \(\forall\pi_{1}\in\Pi_{acc}\) and \(\forall\pi_{2}\notin\Pi_{acc}\), \(\pi_{2}\preceq_{task}\pi_{1}\) always hold.

We have defined in the main text that \(\underline{U}_{r}:=\min\limits_{\pi\in\Pi_{acc}}U_{r}(\pi)\) is the lowest utility achieved by any acceptable policies under \(r\), and \(\overline{U}_{r}:=\max\limits_{\pi\in\Pi}U_{r}(\pi)\;s.t.\;\forall\pi_{1},\pi_{2} \in\Pi,U_{r}(\pi_{1})\leq U_{r}(\pi)\leq U_{r}(\pi_{2})\Rightarrow(\pi_{1} \preceq_{task}\pi)\wedge(\pi_{1}\preceq_{task}\pi_{2})\) is the highest utility threshold such that any policy achieving a utility higher than \(\overline{U}_{r}\) also has a higher order than those of which utilities are lower than \(\overline{U}_{r}\). We call a reward function \(r\) a \((\overline{U}_{r},\underline{U}_{r})\)-aligned with the task.

**Lemma 1**.: _For any task-aligned reward function \(r^{+}\), \(\overline{U}_{r^{+}}\geq\underline{U}_{r^{+}}\)._

Proof.: If \(\overline{U}_{r^{+}}<\underline{U}_{r^{+}}\), then for any policy \(\pi\) with \(U_{r}(\pi)\in[\overline{U}_{r^{+}},\underline{U}_{r^{+}})\), it is guaranteed that \(\pi\in\Pi\backslash\Pi_{acc}\) by definition of \(\underline{U}_{r^{+}}\). Then, \(\underline{U}_{r^{+}}\) is a better solution for \(\max\limits_{\pi\in\Pi}\)\(U_{r}(\pi)\)\(s.t.\)\(\forall\pi_{1},\pi_{2}\in\Pi,U_{r}(\pi_{1})\leq U_{r}(\pi)\leq U_{r}(\pi_{2})\Rightarrow(\pi_{1} \preceq_{task}\pi)\land(\pi_{1}\preceq_{task}\pi_{2})\) than \(\overline{U}_{r^{+}}\). Hence, \(\overline{U}_{r^{+}}\geq\underline{U}_{r^{+}}\) must hold. 

**Lemma 2**.: _If \(\pi_{1}\preceq_{task}\pi_{2}\Leftrightarrow U_{r}(\pi_{2})\leq U_{r}(\pi_{1})\) holds for any \(\pi_{1},\pi_{2}\in\Pi\), then \(\overline{U}_{r}=\underline{U}_{r}=\min\limits_{\pi\in\Pi}U_{r}(\pi)\)._

Proof.: Since the policy that has the highest order achieves the lowest utility \(\min\limits_{\pi\in\Pi}\)\(U_{r}(\pi)\), no other acceptable policy can achieve even lower utility. Hence, by definition \(\underline{U}_{r}=\min\limits_{\pi\in\Pi}U_{r}(\pi)\).

Another example is that when optimal policy under \(r\) has the lowest order in terms of \(\preceq_{task}\), and the highest-order policy has the lowest utility under \(r\), \(\overline{U}_{r}=U_{r}\). In those extreme cases, the reward function can be considered completely misaligned. In the non-extreme cases, if a reward function \(r\) exhibits misalignment with the task - the threshold \(\overline{U}_{r}\) is lower than the utility of an unacceptable policy - \(\overline{U}_{r}\) then must be also lower than the utilities of all the acceptable policies, which forms a lower-bound for the size of \(\underline{U}_{r}\).

**Lemma 3**.: _For any reward function \(r\), if \(\exists\pi\notin\Pi_{acc},U_{r}(\pi)\geq\overline{U}_{r}\), then \(\overline{U}_{r}\leq\underline{U}_{r}\)._

Proof.: Since \(\pi\notin\Pi_{acc}\) and \(U_{r}(\pi)\geq\overline{U}_{r}\), then \(\forall\pi^{\prime}\in\Pi_{acc},U_{r}(\pi^{\prime})\geq\overline{U}_{r}\) must be true, otherwise \(\exists\pi^{\prime}\in\Pi_{acc},\pi^{\prime}\preceq_{task}\pi\), which contradicts the definition of \(\Pi_{acc}\) and \(\overline{U}_{r}\). 

**Lemma 4**.: _If a reward function \(r\) has \(\overline{U}_{r}\leq\underline{U}_{r}\), \(r\) is a task-misaligned reward function._

Proof.: If \(\overline{U}_{r}\leq\underline{U}_{r}\), then there must exists two policies \(\pi_{1},\pi_{2}\) where \(\pi_{1}\in\Pi\backslash\Pi_{acc},U_{r}(\pi_{1})\in[\overline{U}_{r},\underline {U}_{r}]\) and \(U_{r}(\pi_{2})\geq\underline{U}_{r}\land\pi_{2}\preceq_{task}\pi_{1}\). Such \(\pi_{2}\) must not be an acceptable policy. Otherwise, it contradicts the definition of \(\Pi_{acc}\). Hence, it must be unacceptable, leading to \(r\) being a task-misaligned reward function. 

**Proposition 1** For any two reward functions \(r_{1},r_{2}\), if \(\{\pi:U_{r_{1}}(\pi)\geq\overline{U}_{r_{1}}\}\subseteq\{\pi:U_{r_{2}}(\pi) \geq\overline{U}_{r_{2}}\}\), then there must exist a \(\pi_{1}\in\{\pi:U_{r_{1}}(\pi)\geq\overline{U}_{r_{1}}\}\) and a \(\pi_{2}\in\{\pi:U_{r_{2}}(\pi)\geq\overline{U}_{r_{2}}\}\) that satisfy \(U_{r_{1}}(\pi_{2})\leq U_{r_{1}}(\pi_{1})\) and \(\pi_{2}\preceq_{task}\pi_{1}\) while \(U_{r_{2}}(\pi_{1})\leq U_{r_{2}}(\pi_{2})\).

Proof.: According to the definition of \(\overline{U}_{r_{1}}\), \(\forall\pi_{1}\in\{\pi:U_{r_{2}}(\pi)\geq\overline{U}_{r_{1}}\}\) and \(\forall\pi_{2}\in\{\pi:U_{r_{2}}(\pi)\geq\overline{U}_{r_{2}}\}/\{\pi:U_{r_{1}} (\pi)\geq\overline{U}_{r_{1}}\}\), \(U_{r_{1}}(\pi_{2})\leq U_{r_{1}}(\pi_{1})\) and \(\pi_{2}\preceq_{task}\pi_{1}\) must be true. Furthermore, if for all pairs of \(\pi_{2}\in\{\pi:U_{r_{2}}(\pi)\geq\overline{U}_{r_{2}}\}/\{\pi:U_{r_{1}}(\pi) \geq\overline{U}_{r_{1}}\}\) and \(\pi_{1}\in\{\pi:U_{r_{1}}(\pi)\geq\overline{U}_{r_{1}}\}\), \(U_{r_{2}}(\pi_{1})>U_{r_{2}}(\pi_{2})\) is true, then \(\{U_{r_{2}}(\pi)\mid U_{r_{1}}(\pi)\geq\overline{U}_{r_{1}}\}\subset[\overline{U }_{r_{2}},\max\limits_{\pi\in\Pi}\)\(U_{r_{2}}(\pi)]\) is a smaller non-empty interval than \([\overline{U}_{r_{2}},\max\limits_{\pi\in\Pi}U_{r_{2}}(\pi)]\), contradicting the fact that \(\overline{U}_{r_{2}}\) is the highest utility threshold under \(r_{2}\). Hence, there must exist a \(\pi_{2}\in\{\pi:U_{r_{2}}(\pi)\geq\overline{U}_{r_{2}}\}/\{\pi:U_{r_{1}}(\pi) \geq\overline{U}_{r_{1}}\}\) and a \(\pi_{1}\in\{\pi:U_{r_{1}}(\pi)\geq\overline{U}_{r_{1}}\}\) that satisfy \(U_{r_{1}}(\pi_{2})\leq U_{r_{1}}(\pi_{1})\) and \(U_{r_{2}}(\pi_{1})\leq U_{r_{2}}(\pi_{2})\) while \(\pi_{2}\preceq_{task}\pi_{1}\) as aforementioned. 

Note that from now on, we use the notation \(r^{+}\) to denote _task-aligned reward functions_ and \(r^{-}\) to denote _task-misaligned reward functions_ for short. Furthermore, if a reward function \(r\) satisfies \(U_{r}(\pi_{1})\leq U_{r}(\pi_{2})\Leftrightarrow\pi_{1}\preceq_{task}\pi_{2}\), we call this reward function the _ground true reward function_, and denote it as \(r_{task}\) for simplicity. Apparently, any \(r_{task}\) is a task-aligned reward function, and it has the most trivial misalignment.

**Lemma 5**.: \(\overline{U}_{r_{task}}=\max\limits_{\pi\in\Pi}U_{r_{task}}(\pi)\)__

Proof.: By definition, \(\arg\max\limits_{\pi\in\Pi}U_{r_{task}}(\pi)\) has higher order and higher utility than any other policies.

**Lemma 6**.: _If \(\pi_{r_{task}}\) is optimal under \(r_{task}\), for any reward function \(r\), \(U_{r}(\pi_{r_{task}})\geq\overline{U}_{r}\)._

Proof.: If \(U_{r}(\Pi_{acc})<\overline{U}_{r}\), then there exists a \(\pi\) with its utility \(U_{r}(\pi)\geq\overline{U}_{r}\), thus \(\pi_{r_{task}}\preceq_{task}\pi\), which contradicts the assumption that \(\pi_{r_{task}}\) is the highest-order policy. 

Furthermore, we have the following property.

**Theorem 1** Let \(\mathcal{I}\) be an indicator function. For any \(k\geq\big{\{}\min\limits_{r^{+}}\,\sum_{\pi\in\Pi}\mathcal{I}\{U_{r^{+}}(\pi )\geq U_{r^{+}}(\pi_{E})\}\big{\}}\), if \(\pi^{*}\) satisfies \(\big{\{}\sum_{\pi\in\Pi}\mathcal{I}\{U_{r}(\pi)\geq U_{r}(\pi^{*})\}\big{\}}< |\Pi_{acc}|\) for all \(r\in R_{E,k}:=\big{\{}r\mid\sum_{\pi\in\Pi}\mathcal{I}\{U_{r}(\pi)\geq U_{r}( \pi_{E})\}\leq k\big{\}}\), then \(\pi^{*}\) is an acceptable policy, i.e., \(\pi^{*}\in\Pi_{acc}\). Additionally, if \(k<|\Pi_{acc}|\), such an acceptable policy \(\pi^{*}\) is guaranteed to exist.

Proof.: Since \(\pi_{E}\) is an acceptable policy, there must be at least one task-aligned reward function \(r^{+}\) that satisfies \(|\{\pi:U_{r}(\pi)\geq U_{r}(\pi_{E})\}|\leq k\) since \(k\geq\min\limits_{r^{+}}\,|\{\pi:U_{r^{+}}(\pi)\geq U_{r^{+}}(\pi_{E})\}|\). The greater \(k\) is, the more task-aligned reward functions tend to be included. If \(\pi^{*}\) achieves \(|\{\pi:U_{r^{+}}(\pi)\geq U_{r^{+}}(\pi^{*})\}<|\Pi_{acc}|\) under any task-aligned reward function \(r^{+}\), \(\pi^{*}\) must be acceptable policy. Because if \(\pi^{*}\) is unacceptable, there must be an acceptable policy performing worse than the unacceptable \(\pi^{*}\) under \(r^{+}\), contradicting the definition of _task-aligned reward function_. Hence, \(\pi^{*}\) must be acceptable policy. Furthermore, for any \(k\in[\min\limits_{r^{+}}\,\sum_{\pi\in\Pi}\mathcal{I}\{U_{r^{+}}(\pi)\geq U_ {r^{+}}(\pi_{E})\},|\Pi_{acc}|)\), the policy \(\pi_{E}\) itself satisfied \(\sum_{\pi\in\Pi}\mathcal{I}\{U_{r}(\pi)\geq U_{r}(\pi_{E})\}<|\Pi_{acc}|\) for all \(r\in R_{E,k}\), which guarantees the existence of a feasible \(\pi^{*}\). 

### Semi-supervised Reward Design

Designing a reward function can be thought as deciding an ordering of policies. We adopt a concept, called _total domination_, from unsupervised environment design Dennis et al. (2020), and re-interpret this concept in the context of reward design. In this paper, we suppose that the function \(U_{r}(\pi)\) is given to measure the performance of a policy and it does not have to be the utility function. While the measurement of policy performance can vary depending on the free variable \(r\), _total dominance_ can be viewed as an invariance regardless of such dependency.

**Definition 5** (Total Domination).: A policy, \(\pi_{1}\), is totally dominated by some policy \(\pi_{2}\) w.r.t a reward function set \(R\), if for every pair of reward functions \(r_{1},r_{2}\in R\), \(U_{r_{1}}(\pi_{1})<U_{r_{2}}(\pi_{2})\).

If \(\pi_{1}\) totally dominate \(\pi_{2}\) w.r.t \(R\), \(\pi_{2}\) can be regarded as being unconditionally better than \(\pi_{1}\). In other words, the two sets \(\{U_{r}(\pi_{1})\mid r\in R\}\) and \(\{U_{r}(\pi_{2})\mid r\in R\}\) are disjoint, such that \(\sup\{U_{r}(\pi_{1})\mid r\in R\}<\inf\{U_{r}(\pi_{2})\mid r\in R\}\). Conversely, if a policy \(\pi\) is not totally dominated by any other policy, it indicates that for any other policy, say \(\pi_{2}\), \(\sup\{U_{r}(\pi_{1})\mid r\in R\}\geq\inf\{U_{r}(\pi_{2})\mid r\in R\}\).

**Definition 6**.: A reward function set \(R\) aligns with an ordering \(\prec_{R}\) among policies such that \(\pi_{1}\prec_{R}\pi_{2}\) if and only if \(\pi_{1}\) is totally dominated by \(\pi_{2}\) w.r.t. \(R\).

Especially, designing a reward function \(r\) is to establish an ordering \(\prec_{\{r\}}\) among policies. Total domination can be extended to policy-conditioned reward design, where the reward function \(r\) is selected by following a decision rule \(\omega(\pi)\) such that \(\sum_{r\in R}\omega(\pi)(r)=1\). We let \(U_{\omega}(\pi)=\sum\limits_{r\in R}\omega(\pi)(r)\cdot U_{r}(\pi)\) be an affine combination of \(U_{r}(\pi)\)'s with its coefficients specified by \(\omega(\pi)\).

**Definition 7**.: A policy conditioned decision rule \(\omega\) is said to prefer a policy \(\pi_{1}\) to another policy \(\pi_{2}\), which is notated as \(\pi_{1}\prec\)" \(\pi_{2}\), if and only if \(U_{\omega}(\pi_{1})<U_{\omega}(\pi_{2})\).

Making a decision rule for selecting reward functions from a reward function set to respect the total dominance w.r.t this reward function set is an unsupervised learning problem, where no additional external supervision is provided. If considering expert demonstrations as a form of supervisionand using it to constrain the set \(R_{E}\) of reward function via IRL, the reward design becomes semi-supervised.

### Solution to the MinimaxRegret

Without loss of generality, we use \(R\) instead of \(R_{E,\delta}\) in our subsequent analysis because solving \(MinimaxRegret(R)\) does not depend on whether there are constraints for \(R\). In order to show such an equivalence, we follow the same routine as in Dennis et al. (2020), and start by introducing the concept of _weakly total domination_.

**Definition 8** (Weakly Total Domination).: A policy \(\pi_{1}\) is _weakly totally dominated_ w.r.t a reward function set \(R\) by some policy \(\pi_{2}\) if and only if for any pair of reward function \(r_{1},r_{2}\in R\), \(U_{r_{1}}(\pi_{1})\leq U_{r_{2}}(\pi_{2})\).

Note that a policy \(\pi\) being totally dominated by any other policy is a sufficient but not necessary condition for \(\pi\) being weakly totally dominated by some other policy. A policy \(\pi_{1}\) being weakly totally dominated by a policy \(\pi_{2}\) implies that \(\sup\{U_{r}(\pi_{1})\mid r\in R\}\leq\inf\{U_{r}(\pi_{2})\mid r\in R\}\). We assume that there does not exist a policy \(\pi\) that weakly totally dominates itself, which could happen if and only if \(U_{r}(\pi)\) is a constant. We formalize this assumption as the following.

**Assumption 1**.: For the given reward set \(R\) and policy set \(\Pi\), there does not exist a policy \(\pi\) such that for any two reward functions \(r_{1},r_{2}\in R\), \(U_{r_{1}}(\pi)=U_{r_{2}}(\pi)\).

This assumption makes weak total domination a non-reflexive relation. It is obvious that weak total domination is transitive and asymmetric. Now we show that successive weak total domination will lead to total domination.

**Lemma 7**.: _for any three policies \(\pi_{1},\pi_{2},\pi_{3}\in\Pi\), if \(\pi_{1}\) is weakly totally dominated by \(\pi_{2}\), \(\pi_{2}\) is weakly totally dominated by \(\pi_{3}\), then \(\pi_{3}\) totally dominates \(\pi_{1}\)._

Proof.: According to the definition of weak total domination, \(\max\limits_{r\in R}\,U_{r}(\pi_{1})\leq\min\limits_{r\in R}\,U_{r}(\pi_{2})\) and \(\max\limits_{r\in R}\,U_{r}(\pi_{2})\leq\min\limits_{r\in R}\,U_{r}(\pi_{3})\). If \(\pi_{1}\) is weakly totally dominated but not totally dominated by \(\pi_{3}\), then \(\max\limits_{r\in R}\,U_{r}(\pi_{1})=\min\limits_{r\in R}\,U_{r}(\pi_{3})\) must be true. However, it implies \(\min\limits_{r\in R}\,U_{r}(\pi_{2})=\max\limits_{r\in R}\,U_{r}(\pi_{2})\), which violates Assumption 1. We finish the proof. 

**Lemma 8**.: _For the set \(\Pi_{\neg wtd}\subseteq\Pi\) of policies that are not weakly totally dominated by any other policy in the whole set of policies w.r.t a reward function set \(R\), there exists a range \(U\subseteq\mathbb{R}\) such that for any policy \(\pi\in\Pi_{\neg wtd}\), \(U\subseteq[\min\limits_{r\in R}\,U_{r}(\pi),\max\limits_{r\in R}\,U_{r}(\pi)]\)._

Proof.: For any two policies \(\pi_{1},\pi_{2}\in\Pi_{\neg wtd}\), it cannot be true that \(\max\limits_{r\in R}\,U_{r}(\pi_{1})=\min\limits_{r\in R}\,U_{r}(\pi_{2})\) nor \(\min\limits_{r\in R}\,U_{r}(\pi_{1})=\max\limits_{r\in R}\,U_{r}(\pi_{2})\), because otherwise one of the policies weakly totally dominates the other. Without loss of generalization, we assume that \(\max\limits_{r\in R}\,U_{r}(\pi_{1})>\min\limits_{r\in R}\,U_{r}(\pi_{2})\). In this case, \(\max\limits_{r\in R}\,U_{r}(\pi_{2})>\min\limits_{r\in R}\,U_{r}(\pi_{1})\) must also be true, otherwise \(\pi_{1}\) weakly totally dominates \(\pi_{2}\). Inductively, \(\min\limits_{\pi\in\Pi_{\neg wtd}}\max\limits_{r\in R}\,U_{r}(\pi)>\max \limits_{\pi\in\Pi_{\neg wtd}}\,\min\limits_{r\in R}\,U_{r}(\pi)\). Letting \(ub=\min\limits_{\pi\in\Pi_{\neg wtd}}\,\max\limits_{r\in R}\,U_{r}(\pi)\) and \(lb=\max\limits_{\pi\in\Pi_{\neg wtd}}\,\min\limits_{r\in R}\,U_{r}(\pi)\), any \(U\subseteq[lb,ub]\) shall support the assertion. We finish the proof. 

**Lemma 9**.: _For a reward function set \(R\), if a policy \(\pi\in\Pi\) is weakly totally dominated by some other policy in \(\Pi\) and there exists a subset \(\Pi_{\neg wtd}\subseteq\Pi\) of policies that are not weakly totally dominated by any other policy in \(\pi\), then \(\max\limits_{r\in R}\,U_{r}(\pi)<\min\limits_{\pi^{\prime}\in\Pi_{\neg wtd}}\, \max\limits_{r\in R}\,U_{r}(\pi^{\prime})\)_

Proof.: If \(\pi_{1}\) is weakly totally dominated by a policy \(\pi_{2}\in\Pi\), then \(\min\limits_{r\in R}\,U_{r}(\pi_{2})=\max\limits_{r\in R}\,U_{r}(\pi)\). If \(\max\limits_{r\in R}\,U_{r}(\pi)\geq\min\limits_{\pi^{\prime}\in\Pi_{\neg wtd} }\,\max\limits_{r\in R}\,U_{r}(\pi^{\prime})\), then \(\min\limits_{r\in R}\,U_{r}(\pi_{2})\geq\min\limits_{\pi^{\prime}\in\Pi_{\neg wtd }}\max\limits_{r\in R}\,U_{r}(\pi^{\prime})\), making at least one of the policies in \(\Pi_{\neg wtd}\) being weakly totally dominated by \(\pi_{2}\). Hence, \(\max\limits_{r\in R}\,U_{r}(\pi)<\min\limits_{\pi^{\prime}\in\Pi_{\neg wtd}}\, \max\limits_{r\in R}\,U_{r}(\pi^{\prime})\) must be true.

Given a policy \(\pi\) and a reward function \(r\), the regret is represented as Eq.8

\[Regret(\pi,r) := \max_{\pi^{\prime}}U_{r}(\pi^{\prime})-U_{r}(\pi)\] (8)

Then we represent the \(MinimaxRegret(R)\) problem in Eq.9.

\[MinimaxRegret(R) := \arg\min_{\pi\in\Pi}\left\{\max_{r\in R}Regret(\pi,r)\right\}\] (9)

We denote as \(r_{\pi}^{*}\in R\) the reward function that maximizes \(U_{r}(\pi)\) among all the \(r\)'s that achieve the maximization in Eq.9. Formally,

\[r_{\pi}^{*} \in \arg\max_{r\in R}U_{r}(\pi)\qquad s.t.\;r\in\arg\max_{r^{\prime} \in R}Regret(\pi,r^{\prime})\] (10)

Then \(MinimaxRegret\) can be defined as minimizing the worst-case regret as in Eq.9. Next, we want to show that for some decision rule \(\omega\), the set of optimal policies that maximize \(U_{\omega}\) are the solutions to \(MinimaxRegret(R)\). Formally,

\[MinimaxRegret(R)=\arg\max_{\pi\in\Pi}U_{\omega}(\pi)\] (11)

We design \(\omega\) by letting \(\omega(\pi):=\overline{\omega}(\pi)\cdot\delta_{r_{\pi}^{*}}+(1-\overline{ \omega}(\pi))\cdot\mathcal{R}_{\pi}\) where \(\mathcal{R}_{\pi}\in\Delta(R)\) is a policy conditioned distribution over reward functions, \(\delta_{r_{\pi}^{*}}\) be a delta distribution centered at \(r_{\pi}^{*}\), and \(\overline{\omega}(\pi)\) is a coefficient. We show how to design \(\mathcal{R}\) by using the following lemma.

**Lemma 10**.: _Given that the reward function set is \(R\), there exists a decision rule \(\mathcal{R}:\Pi\rightarrow\Delta(R)\) which guarantees that: 1) for any policy \(\pi\) that is not weakly totally dominated by any other policy in \(\Pi\), i.e., \(\pi\in\Pi_{\neg vtd}\subseteq\Pi\), \(U_{\mathcal{R}}(\pi)\equiv c\) where \(c=\max_{\pi^{\prime}\in\Pi_{\neg vtd}}\min_{r\in R}U_{r}(\pi^{\prime})\); 2) for any \(\pi\) that is weakly totally dominated by some policy but not totally dominated by any policy, \(U_{\mathcal{R}}(\pi)=\max_{r\in R}U_{r}(\pi)\); 3) if \(\pi\) is totally dominated by some other policy, \(\overline{\omega}(\pi)\) is a uniform distribution._

Proof.: Since the description of \(\mathcal{R}\) for the policies in condition 2) and 3) are self-explanatory, we omit the discussion on them. For the none weakly totally dominated policies in condition 1), having a constant \(U_{\mathcal{R}}(\pi)\equiv c\) is possible if and only if for any policy \(\pi\in\Pi_{\neg ved}\), \(c\in[\min_{r\in R}U_{r}(\pi^{\prime}),\max_{r\in R}U_{r}(\pi^{\prime})]\). As mentioned in the proof of Lemma 8, \(c\) can exist within \([\min_{r\in R}U_{r}(\pi),\max_{r\in R}U_{r}(\pi)]\). Hence, \(c=\max_{\pi^{\prime}\in\Pi_{\neg wtd}}\min_{r\in R}U_{r}(\pi^{\prime})\) is a valid assignment. 

Then by letting \(\overline{\omega}(\pi):=\frac{Regret(\pi,r_{\pi}^{*})}{c-U_{r_{\pi}^{*}}(\pi)}\), we have the following theorem.

**Theorem 6**.: _By letting \(\omega(\pi):=\overline{\omega}(\pi)\cdot\delta_{r_{\pi}^{*}}+(1-\overline{ \omega}(\pi))\cdot\mathcal{R}_{\pi}\) with \(\overline{\omega}(\pi):=\frac{Regret(\pi,r_{\pi}^{*})}{c-U_{r_{\pi}^{*}}(\pi)}\) and any \(\mathcal{R}\) that satisfies Lemma 10,_

\[MinimaxRegret(R)=\arg\max_{\pi\in\Pi}U_{\omega}(\pi)\] (12)

Proof.: If a policy \(\pi\in\Pi\) is totally dominated by some other policy, since there exists another policy with larger \(U_{\omega}\), \(\pi\) cannot be a solution to \(\arg\max_{\pi\in\Pi}U_{\omega}(\pi)\). Hence, there is no need for further discussion on totally dominated policies. We discuss the none weakly totally dominated policies and the weakly totally dominated but not totally dominated policies (shortened to "weakly totally dominated" from now on) respectively. First we expand \(\arg\max\limits_{\pi\in\Pi}U_{\omega}(\pi)\) as in Eq.13.

\[\arg\max\limits_{\pi\in\Pi}U_{\omega}(\pi)\] (13) \[= \arg\max\limits_{\pi\in\Pi}\sum\limits_{r\in R}\omega(\pi)(r)\cdot U _{r}(\pi)\] \[= \arg\max\limits_{\pi\in\Pi}\frac{Regret(\pi,r_{\pi}^{*})\cdot U_{ r_{\pi}^{*}}(\pi)+(U_{\mathcal{R}}(\pi)-U_{r_{\pi}^{*}}(\pi)-Regret(\pi,r_{\pi}^{*} ))\cdot U_{\mathcal{R}}(\pi)}{c-U_{r_{\pi}^{*}}(\pi)}\] \[= \arg\max\limits_{\pi\in\Pi}\frac{(U_{\mathcal{R}}(\pi)-U_{r_{ \pi}^{*}}(\pi))\cdot U_{\mathcal{R}}(\pi)-(U_{\mathcal{R}}(\pi)-U_{r_{\pi}^{*} }(\pi))\cdot Regret(\pi,r_{\pi}^{*}))}{c-U_{r_{\pi}^{*}}(\pi)}\] \[= \arg\max\limits_{\pi\in\Pi}\frac{U_{\mathcal{R}}(\pi)-U_{r_{\pi }^{*}}(\pi)}{c-U_{r_{\pi}^{*}}(\pi)}\cdot U_{\mathcal{R}}(\pi)-Regret(\pi,r_{ \pi}^{*})\]

1) For the none weakly totally dominated policies, since by design \(U_{\mathcal{R}}\equiv c\), Eq.13 is equivalent to \(\arg\max\limits_{\pi\in\Pi_{1}}\ -Regret(\pi,r_{\pi}^{*})\) which exactly equals \(MinimaxRegret(R)\). Hence, the equivalence holds among the none weakly totally dominated policies. Furthermore, if a none weakly totally dominated policy \(\pi\in\Pi_{\neg wtd}\) achieves optimality in \(MinimaxRegret(R)\), its \(U_{\omega}(\pi)\) is also no less than any weakly totally dominated policy. Because according to Lemma 9, for any weakly totally dominated policy \(\pi_{1}\), its \(U_{\mathcal{R}}(\pi_{1})\leq c\), hence \(\frac{U_{\mathcal{R}}(\pi)-U_{r_{\pi}^{*}}(\pi)}{c-U_{r_{\pi}^{*}}(\pi)}\cdot U _{\mathcal{R}}(\pi_{1})\leq c\). Since \(Regret(\pi,r_{\pi}^{*})\leq Regret(\pi_{1},r_{\pi_{1}}^{*})\), \(U_{\omega}(\pi)\geq U_{\omega}(\pi_{1})\). Therefore, we can assert that if a none weakly totally dominated policy \(\pi\) is a solution to \(MinimaxRegret(R)\), it is also a solution to \(\arg\max\limits_{\pi\in\Pi}U_{\omega}(\pi)\). Additionally, to prove that if a none weakly totally dominated policy \(\pi\) is a solution to \(\arg\max\limits_{\pi^{\prime}\in\Pi}U_{\omega}(\pi^{\prime})\), it is also a solution to \(MinimaxRegret(R)\), it is only necessary to prove that \(\pi\) achieve no larger regret than all the weakly totally dominated policies. But we delay the proof to 2).

2) If a policy \(\pi\) is weakly totally dominated and is a solution to \(MinimaxRegret(R)\), we show that it is also a solution to \(\arg\max\limits_{\pi\in\Pi}U_{\omega}(\pi)\), i.e., its \(U_{\omega}(\pi)\) is no less than that of any other policy.

We start by comparing with non weakly totally dominated policy. for any weakly totally dominated policy \(\pi_{1}\in MinimaxRegret(R)\), it must hold true that \(Regret(\pi_{1},r_{\pi_{1}}^{*})\leq Regret(\pi_{2},r_{\pi_{2}}^{*})\) for any \(\pi_{2}\in\Pi\) that weakly totally dominates \(\pi_{1}\). However, it also holds that \(Regret(\pi_{2},r_{\pi_{2}}^{*})\leq Regret(\pi_{1},r_{\pi_{2}}^{*})\) due to the weak total domination. Therefore, \(Regret(\pi_{1},r_{\pi_{1}}^{*})=Regret(\pi_{2},r_{\pi_{2}}^{*})=Regret(\pi_{1}, r_{\pi_{2}}^{*})\), implying that \(\pi_{2}\) is also a solution to \(MinimaxRegret(R)\). It also implies that \(U_{r_{\pi_{2}}^{*}}(\pi_{1})=U_{r_{\pi_{2}}^{*}}(\pi_{2})\geq U_{r_{\pi_{1}}^{* }}(\pi_{1})\) due to the weak total domination. However, by definition \(U_{r_{\pi_{1}}^{*}}(\pi_{1})\geq U_{r_{\pi_{2}}^{*}}(\pi_{1})\). Hence, \(U_{r_{\pi_{1}}^{*}}(\pi_{1})=U_{r_{\pi_{2}}^{*}}(\pi_{1})=U_{r_{\pi_{2}}^{*}} (\pi_{2})\) must hold. Now we discuss two possibilities: a) there exists another policy \(\pi_{3}\) that weakly totally dominates \(\pi_{2}\); b) there does not exist any other policy that weakly totally dominates \(\pi_{2}\). First, condition a) cannot hold. Because inductively it can be derived \(U_{r_{\pi_{1}}^{*}}(\pi_{1})=U_{r_{\pi_{2}}^{*}}(\pi_{1})=U_{r_{\pi_{2}}^{*}} (\pi_{2})=U_{r_{\pi_{3}}^{*}}(\pi_{3})\), while Lemma 7 indicates that \(\pi_{3}\) totally dominates \(\pi_{1}\), which is a contradiction. Hence, there does not exist any policy that weakly totally dominates \(\pi_{2}\), meaning that condition b) is certain. We note that \(U_{r_{\pi_{1}}^{*}}(\pi_{1})=U_{r_{\pi_{2}}^{*}}(\pi_{1})=U_{r_{\pi_{2}}^{*}}( \pi_{2})\) and the weak total domination between \(\pi_{1},\pi_{2}\) imply that \(r_{\pi_{1}}^{*},r_{\pi_{2}}^{*}\in\arg\max\limits_{r\in R}U_{r}(\pi_{1})\), \(r_{\pi_{2}}^{*}\in\arg\min\limits_{r\in R}U_{r}(\pi_{2})\), and thus \(\min\limits_{r\in R}U_{r}(\pi_{2})\leq\max\limits_{\pi\in\Pi_{\neg wtd}}\ \min\limits_{r\in R}U_{r}(\pi)=c\). Again, \(\pi_{1}\in MinimaxRegret(R)\) makes \(Regret(\pi_{1},r_{\pi}^{*})\leq Regret(\pi_{1},r_{\pi_{1}}^{*})\leq Regret(\pi,r _{\pi}^{*})\) not only hold for \(\pi=\pi_{2}\) but also for any other policy \(\pi\in\Pi_{\neg wtd}\), then for any policy \(\pi\in\Pi_{\neg wtd}\), \(U_{r_{\pi}^{*}}(\pi_{1})\geq U_{r_{\pi}^{*}}(\pi)\geq\min\limits_{r\in R}U_{r}(\pi)\). Hence, \(U_{r_{\pi}^{*}}(\pi_{1})\geq\max\limits_{\pi\in\Pi_{\neg wtd}}\ \min\limits_{r\in R}U_{r}(\pi)=c\). Since \(U_{r_{\pi}^{*}}(\pi_{1})=\min\limits_{r\in R}\ U_{r}(\pi_{2})\) as aforementioned, \(\min\limits_{r\in R}U_{r}(\pi_{2})>\max\limits_{\pi\in\Pi_{\neg wtd}}\ \min\limits_{r\in R}U_{r}(\pi)\) will cause a contradiction. Hence, \(\min\limits_{r\in R}U_{r}(\pi_{2})=\max\limits_{\pi\in\Pi_{\neg wtd}}\ \min\limits_{r\in R}U_{r}(\pi)=c\). As a result, \(U_{\mathcal{R}}(\pi)=U_{r_{\pi}^{*}}(\pi)=\max\limits_{\pi^{\prime}\in\Pi_{\neg wtd }}\min\limits_{r\in R}U_{r}(\pi^{\prime})=c\), and \(U_{\omega}(\pi)=c-Regret(\pi,r_{\pi}^{*})\geq\max\limits_{\pi^{\prime}\in\Pi_{ \neg wtd}}c-Regret(\pi^{\prime},r_{\pi^{\prime}}^{*})=\max\limits_{\pi^{ \prime}\in\Pi_{\neg wtd}}U_{\omega}(\pi^{\prime})\). In other words, if a weakly totally dominated policy \(\pi\) is a solution to \(MinimaxRegret(R)\), then its \(U_{\omega}(\pi)\) is no less than that of any non weakly totally dominated policy. This also complete the proof at the end of 1), because if a none weakly totally dominated policy \(\pi_{1}\) is a solution to \(\arg\max\limits_{\pi\in\Pi}U_{\omega}(\pi)\) but not a solution to \(MinimaxRegret(R)\), then \(Regret(\pi_{1},r_{\pi_{1}}^{*})>0\) and a weakly totally dominated policy \(\pi_{2}\) must be the solution to \(MinimaxRegret(R)\). Then, \(U_{\omega}(\pi_{2})=c>c-Regret(\pi_{1},r_{\pi_{1}}^{*})=U_{\omega}(\pi_{1})\), which, however, contradicts \(\pi_{1}\in\arg\max\limits_{\pi\in\Pi}U_{\omega}(\pi)\).

It is obvious that a weakly totally dominated policy \(\pi\in MinimaxRegret(R)\) has a \(U_{\omega}(\pi)\) no less than any other weakly totally dominated policy. Because for any other weakly totally dominated policy \(\pi_{1}\), \(U_{\mathcal{R}}(\pi_{1})\leq c\) and \(Regret(\pi_{1},r_{\pi_{1}}^{*})\leq Regret(\pi,r_{\pi}^{*})\), hence \(U_{\omega}(\pi_{1})\leq U_{\omega}(\pi)\) according to Eq.13.

So far we have shown that if a weakly totally dominated policy \(\pi\) is a solution to \(MinimaxRegret(R)\), it is also a solution to \(\arg\max\limits_{\pi^{\prime}\in\Pi}U_{\omega}(\pi^{\prime})\). Next, we need to show that the reverse is also true, i.e., if a weakly totally dominated policy \(\pi\) is a solution to \(\arg\max\limits_{\pi\in\Pi}U_{\omega}(\pi)\), it must also be a solution to \(MinimaxRegret(R)\). In order to prove its truthfulness, we need to show that if \(\pi\notin MinimaxRegret(R)\), whether there exists: a) a none weakly totally dominated policy \(\pi_{1}\), or b) another weakly totally dominated policy \(\pi_{1}\), such that \(\pi_{1}\in MinimaxRegret(R)\) and \(U_{\omega}(\pi_{1})\leq U_{\omega}(\pi)\). If neither of the two policies exists, we can complete our proof. Since it has been proved in 1) that if a none weakly totally dominated policy achieves \(MinimaxRegret(R)\), it also achieves \(\arg\max\limits_{\pi^{\prime}\in\Pi}U_{\omega}(\pi^{\prime})\), the policy described in condition a) does not exist. Hence, it is only necessary to prove that the policy in condition b) also does not exist.

If such weakly totally dominated policy \(\pi_{1}\) exists, \(\pi\notin MinimaxRegret(R)\) and \(\pi_{1}\in MinimaxRegret(R)\) indicates \(Regret(\pi,r_{\pi}^{*})>Regret(\pi_{1},r_{\pi_{1}}^{*})\). Since \(U_{\omega}(\pi_{1})\geq U_{\omega}(\pi)\), according to Eq.13, \(U_{\omega}(\pi_{1})=c-Regret(\pi_{1},r_{\pi_{1}}^{*})\leq U_{\omega}(\pi)= \frac{U_{\mathcal{R}}(\pi)-U_{\mathcal{T}_{\pi}}(\pi)}{c-U_{r_{\pi}}(\pi)}\cdot U _{\mathcal{R}}(\pi)-Regret(\pi,r_{\pi}^{*})\). Thus \(\frac{U_{\mathcal{R}}(\pi)-U_{\mathcal{T}_{\pi}}(\pi)}{c-U_{r_{\pi}}(\pi)}( \pi)\cdot U_{\mathcal{R}}\geq c+Regret(\pi,r_{\pi}^{*})-Regret(\pi_{1},r_{\pi _{1}}^{*})>c\), which is impossible due to \(U_{\mathcal{R}}\leq c\). Therefore, such \(\pi_{1}\) also does not exist. In fact, this can be reasoned from another perspective. If there exists a weakly totally dominated policy \(\pi_{1}\) with \(U_{r_{\pi_{1}}^{*}}(\pi_{1})=c=U_{r_{\pi}^{*}}(\pi)\) but \(\pi_{1}\notin MinimaxRegret(R)\), then \(Regret(\pi,r_{\pi}^{*})>Regret(\pi_{1},r_{\pi_{1}}^{*})\). It also indicates \(\max\limits_{\pi^{\prime}\in\Pi}U_{r_{\pi}^{*}}(\pi^{\prime})>\max\limits_{ \pi^{\prime}\in\Pi}U_{r_{\pi_{1}}^{*}}(\pi^{\prime})\). Meanwhile, \(Regret(\pi_{1},r_{\pi}^{*}):=\max\limits_{\pi^{\prime}\in\Pi}U_{r_{\pi}^{*}}( \pi^{\prime})-U_{r_{\pi}^{*}}(\pi_{1})\leq Regret(\pi_{1},r_{\pi_{1}}^{*}):= \max\limits_{\pi^{\prime}\in\Pi}U_{r_{\pi_{1}}^{*}}(\pi^{\prime})-U_{r_{\pi_{1 }}^{*}}(\pi_{1}):=\max\limits_{r\in R}\max\limits_{\pi^{\prime}\in\Pi}U_{r_{ \pi}^{\prime}}(\pi^{\prime})-U_{r}(\pi_{1})\) indicates \(\max\limits_{\pi^{\prime}\in\Pi}U_{r_{\pi}^{*}}(\pi^{\prime})-\max\limits_{\pi ^{\prime}\in\Pi}U_{r_{\pi_{1}}^{*}}(\pi^{\prime})\leq U_{r_{\pi}^{*}}(\pi_{1})- U_{r_{\pi_{1}}^{*}}(\pi_{1})\). However, we have proved that, for a weakly totally dominated policy, \(\pi_{1}\in MinimaxRegret(R)\) indicates \(U_{r_{\pi_{1}}^{*}}(\pi_{1})=\max\limits_{r\in R}U_{r}(\pi_{1})\). Hence, \(\max\limits_{\pi^{\prime}\in\Pi}U_{r_{\pi}^{*}}(\pi^{\prime})-\max\limits_{ \pi^{\prime}\in\Pi}U_{r_{\pi_{1}}^{*}}(\pi^{\prime})\leq U_{r_{\pi}^{*}}(\pi_{1} )-U_{r_{\pi_{1}}^{*}}(\pi_{1})\leq 0\) and it contradicts \(\max\limits_{\pi^{\prime}\in\Pi}U_{r_{\pi}^{*}}(\pi^{\prime})>\max\limits_{ \pi^{\prime}\in\Pi}U_{r_{\pi_{1}}^{*}}(\pi^{\prime})\). Therefore, such \(\pi_{1}\) does not exist. In summary, we have exhausted all conditions and can assert that for any policies, being a solution to \(MinimaxRegret(R)\) is equivalent to a solution to \(\arg\max\limits_{\pi\in\Pi}U_{\omega}(\pi)\). We complete our proof.

### Collective Validation of Similarity Between Expert and Agent

In Definition 2 and our definition of \(Regret\) in Eq.2, we use the utility function \(U_{r}\) to measure the performance of a policy. We now show that we can replace \(U_{r}\) with other functions.

**Lemma 11**.: _The solution of \(MinimaxRegret(R_{E,\delta^{*}})\) does not change when \(U_{r}\) in \(MinimaxRegret\) is replace with \(U_{r}(\pi)-f(r)\) where \(f\) can be arbitrary function of \(r\)._

Proof.: When using \(U_{r}(\pi)-f(r)\) instead of \(U_{r}(\pi)\) to measure the policy performance, solving \(MinimaxRegret(R)\) is to solve Eq. 14, which is the same as Eq.9.

\[MinimaxRegret(R) = \arg\max\limits_{\pi\in\Pi}\min\limits_{r\in R}Regret(\pi,r)\] (14) \[= \arg\max\limits_{\pi\in\Pi}\min\limits_{r\in R}\max\limits_{\pi^{ \prime}\in\Pi}\{U_{r}(\pi^{\prime})-f(r)\}-(U_{r}(\pi)-f(r))\] \[= \arg\max\limits_{\pi\in\Pi}\min\limits_{r\in R}\max\limits_{\pi^{ \prime}\in\Pi}U_{r}(\pi^{\prime})-U_{r}(\pi)\]Lemma 11 implies that we can use the policy-expert margin \(U_{r}(\pi)-U_{r}(E)\) as a measurement of policy performance. This makes the rationale of using PAGAR-based IL for collective validation of similarity between \(E\) and \(\pi\) more intuitive.

### Criterion for Successful Policy Learning

To analyze the sufficient conditions for \(MinimaxRegret\) to mitigate task-reward misalignment, we start by analyzing the general properties of \(MinimaxRegret\) on arbitrary input \(R\).

**Proposition 4**.: _If the following conditions (1) (2) hold for \(R\), then the optimal protagonist policy \(\pi_{P}:=MinimaxRegret(R)\) satisfies \(\forall r^{+}\in R,U_{r^{+}}(\pi_{P})\geq U_{r^{+}}\)._

1. _There exists_ \(r^{+}\in R\)_, and_ \(\max_{r^{+}\in R}\{\max_{\pi\in\Pi}U_{r^{+}}(\pi)-\bar{U}_{r^{+}}\}<\min_{r^{+ }\in R}\{\bar{U}_{r^{+}}-U_{r^{+}}\}\)_;_
2. _There exists a policy_ \(\pi^{*}\) _such that_ \(\forall r^{+}\in R\)_,_ \(U_{r^{+}}(\pi^{*})\geq\bar{U}_{r^{+}}\)_, and_ \(\forall r^{-}\in R\)_,_ \(Regret(\pi^{*},r^{-})<\min_{r^{+}\in R}\{\bar{U}_{r^{+}}-U_{r^{+}}\}\)_._

Proof.: Suppose the conditions are met, and a policy \(\pi_{1}\) satisfies the property described in conditions 2). Then for any policy \(\pi_{2}\in MinimaxRegret(R)\), if \(\pi_{2}\) does not satisfy the mentioned property, there exists a task-aligned reward function \(r^{+}\in R\) such that \(U_{r^{+}}(\pi_{2})\leq\underline{U}_{r^{+}}\). In this case \(Regret(\pi_{2},r^{+})=\max_{\pi\in\Pi}U_{r^{+}}(\pi)-U_{r^{+}}(\pi_{2})\geq \overline{U}_{r^{+}}-\underline{U}_{r^{+}}\geq\min_{r^{+}\in R}\overline{U}_{ r^{+}}-\underline{U}_{r^{+}}\). However, for \(\pi_{1}\), it holds for any task-aligned reward function \(\hat{r}^{+}\in R\) that \(Regret(\pi_{1},\hat{r}^{+})\leq\max_{\pi\in\Pi}U_{\hat{r}^{+}}(\pi)-\overline{ U}_{\hat{r}^{+}}\leq\max_{r^{+}\in R}\{\max_{\pi\in\Pi}U_{r^{+}}(\pi)- \overline{U}_{r^{+}}\}<\min_{r^{+}\in R}\{\overline{U}_{r^{+}}-\underline{U}_ {r^{+}}\}\leq Regret(\pi_{2},r^{+})\), and it also holds for any misaligned reward function \(r^{-}\in R\) that \(Regret(\pi_{1},r^{-})<\min_{r^{+}\in R}\{\overline{U}_{r^{+}}-\underline{U}_ {r^{+}}\}\leq Regret(\pi_{2},\hat{r}^{+})\). Hence, \(Regret(\pi_{1},r^{+})<Regret(\pi_{2},r^{+})\), contradicting \(\pi_{2}\in MinimaxRegret(R)\). We complete the proof. 

In Proposition 4, condition (1) states that the task-aligned reward functions in \(R\) all have a low extent of misalignment while condition (2) states that there exists a \(\pi^{*}\) that not only performs well under all \(r^{+}\)'s (thus being acceptable in the task) but also achieves relatively low regret under all \(r^{-}\)'s. Note that the more aligned the \(r^{+}\)'s, the more forgiving the tolerance for high regret on \(r^{-}\). Furthermore, Proposition 5 shows that, under a stronger condition on the existence of a policy \(\pi^{*}\) performing well under all reward functions in \(R\), \(MinimaxRegret(R)\) can guarantee to induce an acceptable policy, i.e., satisfying the condition (2) in Definition 3.

**Proposition 5** (Strong Acceptance).: _Assume that condition (1) in Proposition 4 is satisfied. In addition, if there exists a policy \(\pi^{*}\) such that \(\forall r\in R\), \(Regret(\pi^{*},r)<\max_{r^{+}\in R}\{\max_{\pi\in\Pi}U_{r^{+}}(\pi)-\bar{U}_ {r^{+}}\}\), then the optimal protagonist policy \(\pi_{P}:=MinimaxRegret(R)\) satisfies \(\forall r^{+}\in R\), \(U_{r^{+}}(\pi_{P})\geq\bar{U}_{r^{+}}\)._

Proof.: Since \(\max_{r\in R}\max_{\pi}U_{r}(\pi)-U_{r}(\pi_{P})\leq\max_{r\in R}\max_{\pi}U_ {r}(\pi)-U_{r}(\pi^{*})<\max_{r^{+}\in R}\{\max_{\pi\in\Pi}U_{r^{+}}(\pi)- \overline{U}_{r^{+}}\}\), we can conclude that for any \(r^{+}\in R\), \(U_{r^{+}}(\pi_{P})\geq\overline{U}_{r^{+}}\). The proof is complete. 

Note that the assumptions in Proposition 4 and 5 are not trivially satisfiable for arbitrary \(R\), e.g., if \(R\) contains two reward functions with opposite signs, i.e., \(r,-r\in R\), no policy can perform well under both \(r\) and \(-r\). However, in PAGAR-based IL, using \(R_{E,\delta}\) in place of arbitrary \(R\) is equivalent to using \(E\) and \(\delta\) to constrain the selection of reward functions, which can lead to additional implications.

**Theorem 2**.: (Weak Acceptance) If the following conditions (1) (2) hold for \(R_{E,\delta}\), then the optimal protagonist policy \(\pi_{P}:=MinimaxRegret(R_{E,\delta})\) satisfies \(\forall r^{+}\in R_{E,\delta}\), \(U_{r^{+}}(\pi_{P})\geq\underline{U}_{r^{+}}\).

1. The condition (1) in Proposition 4 holds
2. \(\forall r^{+}\in R_{E,\delta}\), \(L_{r^{+}}\cdot W_{E}-\delta\leq\max_{\pi\in\Pi}U_{r^{+}}(\pi)-\overline{U}_{r^{+}}\) and \(\forall r^{-}\in R_{E,\delta}\), \(L_{r^{-}}\cdot W_{E}-\delta<\min_{r^{+}\in R_{E,\delta}}\{\overline{U}_{r^{+}}- \underline{U}_{r^{+}}\}\).

Proof.: We consider \(U_{r}(\pi)=\mathbb{E}_{\tau\sim\pi}[r(\tau)]\). Since \(W_{E}\triangleq\min\limits_{\pi\in\Pi}W_{1}(\pi,E)=\frac{1}{K}\underset{|r|_{L} \leq K}{\sup}U_{r}(E)-U_{r}(\pi)\) for any \(K>0\), let \(\pi^{*}\) be the policy that achieves the minimality in \(W_{E}\). Then for any \(r^{+}\in R\), the term \(\underline{L}_{r^{+}}\cdot W_{E}-\delta\geq L_{r^{+}}\cdot\frac{1}{L_{r^{+}}} \underset{|r|_{L}\leq L_{r^{+}}}{\sup}U_{r}(E)-U_{r}(\pi)-\delta\geq U_{r^{+}} (E)-U_{r^{+}}(\pi)-(U_{r^{+}}(E)-\max\limits_{\pi^{\prime}\in\Pi}U_{r^{+}}(\pi^ {\prime}))=\max\limits_{\pi^{\prime}\in\Pi}U_{r^{+}}(\pi^{\prime})-U_{r^{+}}( \pi)\). Hence, for all \(r^{+}\in R\), \(\max\limits_{\pi^{\prime}\in\Pi}U_{r^{+}}(\pi^{\prime})-U_{r^{+}}(\pi)<\max \limits_{\pi^{\prime}\in\Pi}U_{r^{+}}(\pi^{\prime})-\underline{U}_{r^{+}}\), i.e., \(U_{r^{+}}(\pi^{*})\geq\bar{U}_{r^{+}}\). Likewise, \(L_{r^{-}}\cdot W_{E}-\delta<\min\limits_{r^{+}\in R_{E,\delta}}\overline{U}_{ r^{+}}-\underline{U}_{r^{+}}\) indicates that for all \(r^{-}\in R\), \(\max\limits_{\pi^{\prime}\in\Pi}U_{r^{+}}(\pi^{\prime})-U_{r^{+}}(\pi)<\min \limits_{r^{+}\in R_{E,\delta}}\overline{U}_{r^{+}}-\underline{U}_{r^{+}}\). Then, we have recovered the condition (2) in Proposition 4. As a result, we deliver the same guarantees in Proposition 4. 

Theorem 2 delivers the same guarantee as that of Proposition 4 but differs from Proposition 4 in that Condition (2) implicitly requires that for the policy \(\pi^{*}=\arg\min\limits_{\pi\in\Pi}W_{1}(\pi,E)\), the performance difference between \(E\) and \(\pi^{*}\) is small enough under all \(r\in R_{E,\delta}\).

**Theorem 3**.: (Strong Acceptance) Assume that the condition (1) in Theorem 4 holds for \(R_{E,\delta}\). If for any \(r\in R_{E,\delta}\), \(L_{r}\cdot W_{E}-\delta\leq\min\limits_{\tau^{+}\in R_{E,\delta}}\left\{\max \limits_{\pi\in\Pi}U_{r^{+}}(\pi)-\overline{U}_{r^{+}}\right\}\), then the optimal protagonist policy \(\pi_{P}=MinimaxRegret(R_{E,\delta})\) satisfies \(\forall r^{+}\in R_{E,\delta}\), \(U_{r^{+}}(\pi_{P})\geq\overline{U}_{r^{+}}\).

Proof.: Again, we let \(\pi^{*}\) be the policy that achieves the minimality in \(W_{E}\). Then, we have \(L_{r}\cdot W_{E}-\delta\geq L_{r}\cdot\frac{1}{L_{r}}\underset{|\,r|_{L}\leq L _{r}}{\sup}U_{r}(E)-U_{r}(\pi^{*})-(U_{r^{+}}(E)-\max\limits_{\pi^{\prime}\in \Pi}U_{r^{+}}(\pi^{\prime}))\geq\max\limits_{\pi^{\prime}\in\Pi}U_{r^{+}}(\pi^ {\prime})-U_{r^{+}}(\pi^{*})\) for any \(r\in R_{E,\delta}\). We have recovered the condition in Proposition 5. The proof is complete. 

### Stationary Solutions

In this section, we show that \(MinimaxRegret\) is convex for \(\pi_{P}\).

**Proposition 6**.: \(\max\limits_{r\in R}Regret(\pi_{P},r)\) _is convex in \(\pi_{P}\)._

Proof.: For any \(\alpha\in[0,1]\) and \(\pi_{P,1},\pi_{P,2}\), there exists a \(\pi_{P,3}=\alpha\pi_{P,1}+(1-\alpha)\pi_{P,2}\). Let \(r_{1},\pi_{A,1}\) and \(r_{2},\pi_{A,2}\) be the optimal reward and antagonist policy for \(\pi_{P,1}\) and \(\pi_{P,2}\). Then \(\alpha\cdot(\max\limits_{r\in R}\max\limits_{\pi_{A}\in\Pi}U_{r}(\pi_{A})-U_{r} (\pi_{P,1}))+(1-\alpha)\cdot(\max\limits_{r\in R}\max\limits_{\pi_{A}\in\Pi}U_{ r}(\pi_{A})-U_{r}(\pi_{P,2}))=\alpha(U_{r_{1}}(\pi_{A,1})-U_{r_{1}}(\pi_{P,1}))+(1- \alpha)(U_{r_{2}}(\pi_{A,2})-U_{r_{2}}(\pi_{P,2}))\geq\alpha(U_{r_{3}}(\pi_{A, 3})-U_{r_{3}}(\pi_{P,1}))+(1-\alpha)(U_{r_{2}}(\pi_{A,3})-U_{r_{3}}(\pi_{P,2}))=U _{r_{3}}(\pi_{A,3})-U_{r_{3}}(\pi_{P,3})\). Therefore, \(\underset{r\in R}{\max}\max\limits_{\pi_{A}\in\Pi}U_{r}(\pi_{A})-U_{r}(\pi_{P})\) is convex in \(\pi_{P}\). 

### Compare PAGAR-Based IL with IRL-Based IL

**Assumption 2**.: \(\max\limits_{r}\mathcal{J}_{IRL}(r)\) can reach Nash Equilibrium at an optimal reward function \(r^{*}\) and its optimal policy \(\pi_{r^{*}}\).

We make this assumption only to demonstrate how PAGAR-based IL can prevent performance degradation w.r.t IRL-based IL, which is preferred when IRL-based IL does not have a reward misalignment issue under ideal conditions. We draw two assertions from this assumption. The first one considers Maximum Margin IRL-based IL and shows that if using the optimal reward function set \(R_{E,\delta^{*}}\) as input to \(MinimaxRegret\), PAGAR-based IL and Maximum Margin IRL-based IL have the same solutions.

**Proposition 7**.: \(\pi_{r^{*}}=MinimiaxRegret(R_{E,\delta^{*}})\)_._

Proof.: The reward function set \(R_{E,\delta^{*}}\) and the policy set \(\Pi_{acc}\) achieving Nash Equilibrium for \(\arg\min\limits_{r\in R}J_{IRL}(r)\) indicates that for any \(r\in R_{E,\delta^{*}},\pi\in\Pi_{acc}\), \(\pi\in\arg\max\limits_{\pi\in\Pi}U_{r}(\pi)-U_{r}(E)\). Then \(\Pi_{acc}\) will be the solution to \(\arg\max\limits_{\pi_{P}\in\Pi}\min\limits_{r\in R_{E,\delta^{*}}}\ \left\{\max\limits_{\pi_{A}\in\Pi}U_{r}(\pi_{A})-U_{r}(E)\right\}-(U_{r}(\pi_{P}) -U_{r}(E))\)because the policies in \(\Pi_{acc}\) achieve zero regret. Then Lemma 11 states that \(\Pi_{acc}\) will also be the solution to \(\arg\max\limits_{\pi_{P}\in\Pi}\min\limits_{r\in R_{E,\delta^{*}}}\ \left\{\max\limits_{\pi_{A}\in\Pi}U_{r}(\pi_{A})\right\}-U_{r}(\pi_{P})\). We finish the proof. 

The proof can be found in Appendix A.6. The second assertion shows that if IRL-based IL can learn a policy to succeed in the task, \(MinimaxRegret(R_{E,\delta})\) with \(\delta<\delta^{*}\) can also learn a policy that succeeds in the task under certain condition. The proof can be found in Appendix A.6. This assertion also suggests that the designer should select a \(\delta\) smaller than \(\delta^{*}\) while making \(\delta^{*}-\delta\) no greater than the expected size of the high-order policy utility interval.

**Proposition 8**.: _If \(r^{*}\) is a task-aligned reward function and \(\delta\geq\delta^{*}-(\max\limits_{\pi\in\Pi}U_{r^{*}}(\pi)-\overline{U}_{r^{ *}})\), the optimal protagonist policy \(\pi_{P}=MinimiaxRegret(R_{E,\delta})\) is guaranteed to be acceptable for the task._

Proof.: If \(\pi_{r^{*}}\in MinimiaxRegret(R_{E,\delta})\), then \(\pi_{r^{*}}\) can succeed in the task by definition. Now assume that \(\pi_{P}\neq\pi_{r^{*}}\). Since \(J_{IRL}\) achieves Nash Equilibrium at \(r^{*}\) and \(\pi_{r^{*}}\), for any other reward function \(r\) we have \(\max\limits_{\pi\in\Pi}\ U_{r}(\pi)-U_{r}(\pi_{r^{*}})\leq\delta^{*}-(U_{r}(E )-\max\limits_{\pi\in\Pi}\ U_{r}(\pi))\leq\delta^{*}-\delta\). We also have \(\max\limits_{r^{\prime}\in R_{E,\delta}}Regret(r^{\prime},\pi_{P})\leq\max \limits_{r^{\prime}\in R_{E,\delta}}Regret(r^{\prime},\pi_{r^{*}})\leq\delta^{ *}-\delta\). Furthermore, \(Regre(r^{*},\pi_{P})\leq\max\limits_{r^{\prime}\in R_{E,\delta}}Regret(r^{ \prime},\pi_{P})\). Hence, \(Regre(r^{*},\pi_{P})\leq\delta-\delta^{*}\leq\max\limits_{\pi\in\Pi}U_{r^{*}} (\pi)-\overline{U}_{r^{*}}\). In other words, \(U_{r^{*}}(\pi_{P})\in[\overline{U}_{r^{*}},\max\limits_{\pi\in\Pi}U_{r^{*}} (\pi)]\), indicating \(\pi_{P}\) can succeed in the task. The proof is complete. 

### Example 1

_Example 1_.: Figure 5 Left shows an illustrative example of how PAGAR-based IL mitigates reward misalignment in IRL-based IL. The task requires that _a policy must visit \(s_{2}\) and \(s_{6}\) with probabilities no less than \(0.5\) within \(5\) steps_, i.e. \(Prob(s_{2}\mid\pi)\geq 0.5\wedge Prob(s_{6}\mid\pi)\geq 0.5\) where \(Prob(s\mid\pi)\) is the probability of \(\pi\) generating a trajectory that contains \(s\) within the first \(5\) steps. It can be derived analytically that a successful policy must choose \(a_{2}\) at \(s_{0}\) with a probability within \([\frac{1}{2},\frac{125}{188}]\). The derivation is as follows.

The trajectories that reach \(s_{6}\) after choosing \(a_{2}\) at \(s_{0}\) include: \((s_{0},a_{2},s_{2},s_{6}),(s_{0},a_{2},s_{2},s_{2},s_{6}),(s_{0},a_{2},s_{2}, s_{2},s_{2},s_{6})\). The total probability equals \(Prob(s_{6}\mid\pi;s_{0},a_{2})=\frac{1}{5}+\frac{1}{5}^{2}+\frac{1}{5}^{3}= \frac{31}{125}\). Then the total probability of reaching \(s_{6}\) equals \(Prob(s_{6}\mid\pi)=(1-\pi(a_{2}\mid s_{0}))+\frac{31}{125}\cdot\pi(a_{2}\mid s _{0})\). For \(Prob(s_{6}\mid\pi)\) to be no less than \(0.5\), \(\pi(a_{2}\mid s_{0})\) must be no greater than \(\frac{125}{188}\).

Figure 5: **Left:** Consider an MDP where there are two available actions \(a_{1},a_{2}\) at initial state \(s_{0}\). In other states, actions make no difference: the transition probabilities are either annotated at the transition edges or equal \(1\) by default. States \(s_{3}\) and \(s_{6}\) are terminal states. Expert demonstrations are in \(E\). **Middle**: x-axis indicates the MaxEnt IRL loss bound \(\delta\) for \(R_{E,\delta}\) as defined in Section A.3. The y-axis indicates the probability of the protagonist policy learned via \(MinimaxRegret(R_{E,\delta})\) choosing \(a_{2}\) at \(s_{0}\). The red curve shows how different \(\delta\)’s lead to different protagonist policies. The blue dashed curve is for reference, showing the optimal policy under the optimal reward learned via MaxEnt IRL. **Right**: The curve shows how the MaxEnt IRL Loss changes with \(\omega\).

The reward function hypothesis space is \(\{r_{\omega}\mid r_{\omega}(s,a)=\omega\cdot r_{1}(s,a)+(1-\omega)\cdot r_{2}(s,a)\}\) where \(\omega\in[0,1]\) is a parameter, \(r_{1},r_{2}\) are two features. Specifically, \(r_{1}(s,a)\) equals \(1\) if \(s=s_{2}\) and equals \(0\) otherwise, and \(r_{2}(s,a)\) equals \(1\) if \(s=s_{6}\) and equals \(0\) otherwise. Given the demonstrations and the MDP, the maximum negative MaxEnt IRL loss \(\delta^{*}\approx 2.8\) corresponds to the optimal parameter \(\omega^{*}=1\). This is computed based on Eq.6 in Ziebart et al. (2008). The discount factor is \(\gamma=0.99\). When computing the normalization term \(Z\) in Eq.4 of Ziebart et al. (2008), we only consider the trajectories within \(5\) steps. The optimal policy under \(r_{\omega^{*}}\) chooses \(a_{2}\) at \(s_{0}\) with probability \(1\) and reaches \(s_{6}\) with probability less than \(0.25\), thus failing to accomplish the task. The optimal protagonist policy \(\pi_{P}=MinimaxRegret(R_{E,\delta})\) can succeed in the task as indicated by the grey dashed lines in Figure 5 Middle. It chooses \(a_{2}\) at \(s_{0}\) with probability \(1\) when \(\delta\) is close to its maximum \(\delta^{*}\). However, \(\pi_{P}(a_{2}\mid s_{2})\) decreases as \(\delta\) decreases. It turns out that for any \(\delta<1.1\) the optimal protagonist policy can succeed in the task. In Figure 5 Right, we further show how the MaxEnt IRL loss changes with \(\omega\).

## Appendix B Approach to Solving MinimaxRegret

In this section, we show how we derive the off-policy RL objective function for \(\pi_{P}\). Also, we develop a series of theories that lead to two bounds of the Protagonist Antagonist Induced Regret. By using those bounds, we formulate objective functions for solving Imitation Learning problems with PAGAR.

### Off-Policy Objective Function for Protagonist Policy Training

For reader's convenience, we put the Theorem 1 in Schulman et al. (2015) here.

**Theorem 7** (Schulman et al. (2015)).: _Let \(\alpha=\max\limits_{s}\,D_{TV}(\pi_{old},\pi_{new})\), and let \(\epsilon=\max\limits_{s}\mathbb{E}_{a\sim\pi_{new}}[A_{\pi_{old}}(s,a)]\), then Eq.15 holds._

\[U_{r}(\pi_{new})\leq U_{r}(\pi_{old})+\sum_{s\in\mathbb{S}}\rho_{\pi_{old}}(s )\sum_{a\in\mathbb{A}}\pi_{new}(a|s)A_{\pi_{old}}(s,a)+\frac{2\epsilon\gamma} {(1-\gamma)^{2}}\alpha^{2}\] (15)

_where \(\rho_{\pi_{old}}(s)=\sum_{t=0}^{T}\gamma^{t}Prob(s^{(t)}=s|\pi_{old})\) is the discounted visitation frequency of \(\pi_{old}\), \(A_{\pi_{old}}\) is the advantage function without considering the entropy._

Algorithm 1 in Schulman et al. (2015) learns \(\pi_{new}\) by maximizing the r.h.s of the inequality Eq.15, which only involves the trajectories and the advantage function of \(\pi_{old}\). By moving \(U_{r}(\pi_{old})\) from r.h.s of Eq.15 to the left, and replacing \(\pi_{new}\) with \(\pi_{P}\) and \(\pi_{old}\) with \(\pi_{A}\), we obtain a bound for \(U_{r}(\pi_{P})-U_{r}(\pi_{A})\) as mentioned in Section 6.1. The PPO in Schulman et al. (2017) further simplifies the r.h.s of the inequality Eq.15 with a clipped importance sampling rate. We derived \(J_{\pi_{A}}(\pi_{P})\) by using the same trick.

### Protagonist Antagonist Induced Regret Bounds

Our theories are inspired by the on-policy policy improvement methods in Schulman et al. (2015). The theories in Schulman et al. (2015) are under the setting where entropy regularizer is not considered. In our implementation, we always consider entropy regularized RL of which the objective is to learn a policy that maximizes \(J_{RL}(\pi;r)=U_{r}(\pi)+\mathcal{H}(\pi)\). Also, since we use GAN-based IRL algorithms, the learned reward function \(r\) as proved by Fu et al. (2018) is a distribution. Moreover, it is also proved in Fu et al. (2018) that a policy \(\pi\) being optimal under \(r\) indicates that \(\log\pi\equiv r\equiv A_{\pi}\). We omit the proof and let the reader refer to Fu et al. (2018) for details. Although all our theories are about the relationship between the Protagonist Antagonist Induced Regret and the soft advantage function \(\mathcal{A}_{\pi}\), the equivalence between \(\mathcal{A}_{\pi}\) and \(r\) allows us to use the theories to formulate our reward optimization objective functions. To start off, we denote the reward function to be optimized as \(r\). Given the intermediate learned reward function \(r\), we study the Protagonist Antagonist Induced Regret between two policies \(\pi_{1}\) and \(\pi_{2}\).

**Lemma 12**.: _Given a reward function \(r\) and a pair of policies \(\pi_{1}\) and \(\pi_{2}\),_

\[U_{r}(\pi_{1})-U_{r}(\pi_{2})=\operatorname*{\mathbb{E}}_{\tau\sim\pi_{1}} \left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)}) \right]+\operatorname*{\mathbb{E}}_{\tau\sim\pi}\left[\sum_{t=0}^{\infty} \gamma^{t}\mathcal{H}\left(\pi_{2}(\cdot|s^{(t)})\right)\right]\] (16)Proof.: This proof follows the proof of Lemma 1 in Schulman et al. (2015) where RL is not entropy-regularized. For entropy-regularized RL, since \(\mathcal{A}_{\pi}(s,a^{(t)})=\underset{s^{\prime}\sim\mathcal{T}(\cdot|s,a^{(t)}) }{\mathbb{E}}\left[r(s,a^{(t)})+\gamma\mathcal{V}_{\pi}(s^{\prime})-\mathcal{V }_{\pi}(s)\right]\),

\[\underset{\tau\sim\pi_{1}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t}\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\] \[= \underset{\tau\sim\pi_{1}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r(s^{(t+1)},a^{(t+1)})+\gamma\mathcal{V}_{\pi_{2}}(s^{(t+1)}) -\mathcal{V}_{\pi_{2}}(s^{(t)})\right)\right]\] \[= \underset{\tau\sim\pi_{1}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t}r(s^{(t)},a^{(t)})-\mathcal{V}_{\pi_{2}}(s^{(0)})\right]\] \[= \underset{\tau\sim\pi_{1}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t}r(s^{(t)},a^{(t)})\right]-\underset{s^{(0)}\sim d_{0}}{\mathbb{E}} \left[\mathcal{V}_{\pi_{2}}(s^{(0)})\right]\] \[= \underset{\tau\sim\pi_{1}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t}r(s^{(t)},a^{(t)})\right]-\underset{\tau\sim\pi_{2}}{\mathbb{E}} \left[\sum_{t=0}^{\infty}\gamma^{t}r(s^{(t)},a^{(t)})+\mathcal{H}\left(\pi_{2} (\cdot|s^{(t)})\right)\right]\] \[= U_{r}(\pi_{1})-U_{r}(\pi_{2})-\underset{\tau\sim\pi_{2}}{ \mathbb{E}}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{H}\left(\pi_{2}(\cdot|s ^{(t)})\right)\right]\] \[= U_{r}(\pi_{1})-U_{r}(\pi_{2})-\mathcal{H}(\pi_{2})\]

_Remark 1_.: Lemma 12 confirms that \(\underset{\tau\sim\pi}{\mathbb{E}}\left[\sum_{t=0}^{\infty}\gamma^{t} \mathcal{A}_{\pi}(s^{(t)},a^{(t)})\right]=U_{r}(\pi)-U_{r}(\pi)+\mathcal{H}( \pi)=\mathcal{H}(\pi)\).

We follow Schulman et al. (2015) and denote \(\Delta\mathcal{A}(s)=\underset{a\sim\pi_{1}(\cdot|s)}{\mathbb{E}}\left[ \mathcal{A}_{\pi_{2}}(s,a)\right]-\underset{a\sim\pi_{2}(\cdot|s)}{\mathbb{E} }\left[\mathcal{A}_{\pi_{2}}(s,a)\right]\) as the difference between the expected advantages of following \(\pi_{2}\) after choosing an action respectively by following policy \(\pi_{1}\) and \(\pi_{2}\) at any state \(s\). Although the setting of Schulman et al. (2015) differs from ours by having the expected advantage \(\underset{a\sim\pi_{2}(\cdot|s)}{\mathbb{E}}\left[\mathcal{A}_{\pi_{2}}(s,a)\right]\) equal to \(0\) due to the absence of entropy regularization, the following definition and lemmas from Schulman et al. (2015) remain valid in our setting.

**Definition 9**.: Schulman et al. (2015)_, the protagonist policy \(\pi_{1}\) and the antagonist policy \(\pi_{2})\) are \(\alpha\)-coupled if they defines a joint distribution over \((a,\tilde{a})\in\mathbb{A}\times\mathbb{A}\), such that \(Prob(a\neq\tilde{a}|s)\leq\alpha\) for all \(s\)._

**Lemma 13**.: _Schulman et al. (2015) Given that the protagonist policy \(\pi_{1}\) and the antagonist policy \(\pi_{2}\) are \(\alpha\)-coupled, then for all state \(s\),_

\[|\Delta\mathcal{A}(s)|\leq 2\alpha\underset{a}{\max}|\mathcal{A}_{\pi_{2}}(s,a)|\] (17)

**Lemma 14**.: _Schulman et al. (2015) Given that the protagonist policy \(\pi_{1}\) and the antagonist policy \(\pi_{2}\) are \(\alpha\)-coupled, then_

\[\left|\underset{s^{(t)}\sim\pi_{1}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)} )\right]-\underset{s^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{ (t)})\right]\right|\leq 4\alpha(1-(1-\alpha)^{t})\underset{s,a}{\max}| \mathcal{A}_{\pi_{2}}(s,a)|\] (18)

**Lemma 15**.: _Given that the protagonist policy \(\pi_{1}\) and the antagonist policy \(\pi_{2}\) are \(\alpha\)-coupled, then_

\[\underset{\begin{subarray}{c}s^{(t)}\sim\pi_{1}\\ a^{(t)}\sim\pi_{2}\end{subarray}}{\mathbb{E}}\left[\mathcal{A}_{\pi_{2}}(s^{(t) },a^{(t)})\right]-\underset{s^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\mathcal{A}_ {\pi_{2}}(s^{(t)},a^{(t)})\right]\leq 2(1-(1-\alpha)^{t})\underset{(s,a)}{\max}| \mathcal{A}_{\pi_{2}}(s,a)|\] (19)

Proof.: The proof is similar to that of Lemma 14 in Schulman et al. (2015). Let \(n_{t}\) be the number of times that \(a^{(t^{\prime})}\sim\pi_{1}\) does not equal \(a^{(t^{\prime})}\sim\pi_{2}\) for \(t^{\prime}<t\), i.e., the number of times that \(\pi_{1}\) and 

[MISSING_PAGE_FAIL:27]

\(\underset{a\sim\pi_{2}}{\mathbb{E}}[\mathcal{A}_{\pi_{2}}(s,a)]\). For any policy \(\pi_{1}\), the following bounds hold.

\[\left|U_{r}(\pi_{1})-U_{r}(\pi_{2})-\sum_{t=0}^{\infty}\gamma^{t} \underset{s^{(t)}\sim\pi_{1}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)}) \right]\right| \leq \frac{2\alpha\gamma\epsilon}{(1-\gamma)^{2}}\] (21) \[\left|U_{r}(\pi_{1})-U_{r}(\pi_{2})-\sum_{t=0}^{\infty}\gamma^{t} \underset{s^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)}) \right]\right| \leq \frac{2\alpha\gamma(2\alpha+1)\epsilon}{(1-\gamma)^{2}}\] (22)

Proof.: We first leverage Lemma 12 to derive Eq.23. Note that since \(\pi_{2}\) is optimal under \(r\), Remark 1 confirmed that \(\mathcal{H}(\pi_{2})=-\sum_{t=0}^{\infty}\gamma^{t}\underset{s^{(t)}\sim\pi_{2 }}{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\mathcal{A} _{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]\).

\[U_{r}(\pi_{1})-U_{r}(\pi_{2})\] (23) \[= (U_{r}(\pi_{1})-U_{r}(\pi_{2})-\mathcal{H}(\pi_{2}))+\mathcal{H}( \pi_{2})\] \[= \underset{\tau\sim\pi_{1}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t}\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]+\mathcal{H}(\pi_{2})\] \[= \underset{\tau\sim\pi_{1}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t}\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]-\sum_{t=0}^{\infty} \gamma^{t}\underset{s^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\underset{a^{(t)} \sim\pi_{2}}{\mathbb{E}}\left[\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]\] \[= \sum_{t=0}^{\infty}\gamma^{t}\underset{s^{(t)}\sim\pi_{1}}{ \mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{1}}{\mathbb{E}}\left[\mathcal{A}_{ \pi_{2}}(s^{(t)},a^{(t)})\right]-\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}} \left[\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]+\] \[= \sum_{t=0}^{\infty}\gamma^{t}\underset{s^{(t)}\sim\pi_{1}}{ \mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)})\right]+\] \[\sum_{t=0}^{\infty}\gamma^{t}\left(\underset{s^{(t)}\sim\pi_{1}}{ \mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\mathcal{A}_{ \pi_{2}}(s^{(t)},a^{(t)})\right]\right]-\underset{s^{(t)}\sim\pi_{2}}{\mathbb{ E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)}) \right]\right]\right)\]

We switch terms between Eq.23 and \(U_{r}(\pi_{1})-U_{r}(\pi_{2})\), then use Lemma 15 to derive Eq.24.

\[\left|U_{r}(\pi_{1})-U_{r}(\pi_{2})-\sum_{t=0}^{\infty}\gamma^{t} \underset{s^{(t)}\sim\pi_{1}}{\mathbb{E}}\Big{[}\Delta\mathcal{A}(s^{(t)}) \Big{]}\right|\] (24) \[= \left|\sum_{t=0}^{\infty}\gamma^{t}\left(\underset{s^{(t)}\sim \pi_{1}}{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[ \mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]-\underset{s^{(t)}\sim\pi_ {2}}{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[ \mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]\right)\right|\] \[\leq \sum_{t=0}^{\infty}\gamma^{t}\cdot 2\underset{(s,a)}{\max}| \mathcal{A}_{\pi_{2}}(s,a)|\cdot(1-(1-\alpha)^{t})\leq\frac{2\alpha\gamma \underset{(s,a)}{\max}|\mathcal{A}_{\pi_{2}}(s,a)|}{(1-\gamma)^{2}}\]Alternatively, we can expand \(U_{r}(\pi_{2})-U_{r}(\pi_{1})\) into Eq.25. During the process, \(\mathcal{H}(\pi_{2})\) is converted into \(-\sum_{t=0}^{\infty}\gamma^{t}\underset{s^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[ \underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\mathcal{A}_{\pi_{2}}(s^{(t)},a ^{(t)})\right]\right]\).

\[U_{r}(\pi_{1})-U_{r}(\pi_{2})\] \[= (U_{r}(\pi_{1})-U_{r}(\pi_{2})-\mathcal{H}(\pi_{2}))+\mathcal{H}( \pi_{2})\] \[= \underset{\tau\sim\pi_{1}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t}\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]+\mathcal{H}(\pi_{2})\] \[= \sum_{t=0}^{\infty}\gamma^{t}\underset{s^{(t)}\sim\pi_{1}}{ \mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{1}}{\mathbb{E}}\left[\mathcal{A}_{ \pi_{2}}(s^{(t)},a^{(t)})\right]\right]+\mathcal{H}(\pi_{2})\] \[= \sum_{t=0}^{\infty}\gamma^{t}\underset{s^{(t)}\sim\pi_{1}}{ \mathbb{E}}\left[\Delta A(s^{(t)})+\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}} \left[\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]+\mathcal{H}(\pi_{2})\] \[= \sum_{t=0}^{\infty}\gamma^{t}\underset{s^{(t)}\sim\pi_{2}}{ \mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{1}}{\mathbb{E}}\left[\mathcal{A}_ {\pi_{2}}(s^{(t)},a^{(t)})\right]-\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}} \left[\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]-\Delta\mathcal{A}(s^{(t)}) \right]+\] \[\underset{s^{(t)}\sim\pi_{1}}{\mathbb{E}}\left[\Delta\mathcal{A} (s^{(t)})+\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\mathcal{A}_{\pi_{2} }(s^{(t)},a^{(t)})\right]\right]-\underset{s^{(t)}\sim\pi_{2}}{\mathbb{E}} \left[\mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\] \[= \sum_{t=0}^{\infty}\gamma^{t}\left(\underset{s^{(t)}\sim\pi_{1}} {\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\mathcal{A}_ {\pi_{2}}(s^{(t)},a^{(t)})\right]\right]-2\underset{s^{(t)}\sim\pi_{2}}{ \mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\mathcal{A}_ {\pi_{2}}(s^{(t)},a^{(t)})\right]\right]\right)+\] \[\sum_{t=0}^{\infty}\gamma^{t}\left(\underset{s^{(t)}\sim\pi_{2}} {\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{1}}{\mathbb{E}}\left[\mathcal{A}_ {\pi_{2}}(s^{(t)},a^{(t)})\right]\right]-(\underset{s^{(t)}\sim\pi_{2}}{ \mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)})\right]-\underset{s^{(t)}\sim\pi_ {1}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)})\right])\right)\]

We switch terms between Eq.25 and \(U_{r}(\pi_{1})-U_{r}(\pi_{2})\), then base on Lemma 14 and 15 to derive the inequality in Eq.26.

\[\left|U_{r}(\pi_{1})-U_{r}(\pi_{2})-\sum_{t=0}^{\infty}\gamma^{t} \underset{s^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[\Delta\mathcal{A}_{\pi}(s^{(t) },a^{(t)})\right]\right|\] (26) \[= \left|U_{r}(\pi_{1})-U_{r}(\pi_{2})-\right.\] \[\left.\sum_{t=0}^{\infty}\gamma^{t}\left(\underset{s^{(t)}\sim \pi_{2}}{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{1}}{\mathbb{E}}\left[ \mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]-\underset{s^{(t)}\sim\pi_ {2}}{\mathbb{E}}\left[\mathcal{E}_{a^{(t)}\sim\pi_{2}}\left[\mathcal{A}_{\pi_{ 2}}(s^{(t)},a^{(t)})\right]\right]\right)\right|\] \[= \left|\sum_{t=0}^{\infty}\gamma^{t}\left(\underset{s^{(t)}\sim \pi_{2}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)})\right]-\underset{s^{(t)} \sim\pi_{1}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)})\right]\right)-\right.\] \[\left.\sum_{t=0}^{\infty}\gamma^{t}\left(\underset{s^{(t)}\sim \pi_{1}}{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[ \mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]-\underset{s^{(t)}\sim\pi_ {2}}{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[ \mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]\right)\right|\] \[\leq \left|\sum_{t=0}^{\infty}\gamma^{t}\left(\underset{s^{(t)}\sim\pi_ {2}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)})\right]-\underset{s^{(t)}\sim\pi_ {1}}{\mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)})\right]\right)\right|+\] \[\left.\left|\sum_{t=0}^{\infty}\gamma^{t}\left(\underset{s^{(t)} \sim\pi_{1}}{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[ \mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]-\underset{s^{(t)}\sim\pi_ {2}}{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{2}}{\mathbb{E}}\left[ \mathcal{A}_{\pi_{2}}(s^{(t)},a^{(t)})\right]\right]\right)\right|\] \[\leq \sum_{t=0}^{\infty}\gamma^{t}\left((1-(1-\alpha)^{t})(4\alpha \underset{s,a}{\max}|\mathcal{A}_{\pi_{2}}(s,a)|+2\underset{(s,a)}{\max}| \mathcal{A}_{\pi_{2}}(s,a)|)\right)\] \[\leq \frac{2\alpha\gamma(2\alpha+1)\underset{s,a}{\max}|\mathcal{A}_{ \pi_{2}}(s,a)|}{(1-\gamma)^{2}}\]It is stated in Schulman et al. (2015) that \(\max\limits_{s}\ D_{TV}(\pi_{2}(\cdot|s),\pi_{1}(\cdot|s))\leq\alpha\). Hence, by letting \(\alpha:=\max\limits_{s}\ D_{TV}(\pi_{2}(\cdot|s),\pi_{1}(\cdot|s))\), Eq.23 and 26 still hold. Then, we have proved Theorem 4. 

### Objective Functions of Reward Optimization

To derive \(J_{R,1}\) and \(J_{R,2}\), we let \(\pi_{1}=\pi_{P}\) and \(\pi_{2}=\pi_{A}\). Then based on Eq.21 and 22 we derive the following upper-bounds of \(U_{r}(\pi_{P})-U_{r}(\pi_{A})\).

\[U_{r}(\pi_{P})-U_{r}(\pi_{A}) \leq \sum\limits_{t=0}^{\infty}\gamma^{t}\underset{s^{(t)}\sim\pi_{P} }{\mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)})\right]+\frac{2\alpha\gamma(2 \alpha+1)\epsilon}{(1-\gamma)^{2}}\] (27) \[U_{r}(\pi_{P})-U_{r}(\pi_{A}) \geq \sum\limits_{t=0}^{\infty}\gamma^{t}\underset{s^{(t)}\sim\pi_{A} }{\mathbb{E}}\left[\Delta\mathcal{A}(s^{(t)})\right]-\frac{2\alpha\gamma \epsilon}{(1-\gamma)^{2}}\] (28)

By our assumption that \(\pi_{A}\) is optimal under \(r\), we have \(\mathcal{A}_{\pi_{A}}\equiv r\) Fu et al. (2018). This equivalence enables us to replace \(\mathcal{A}_{\pi_{A}}\)'s in \(\Delta\mathcal{A}\) with \(r\). As for the \(\frac{2\alpha\gamma(2\alpha+1)\epsilon}{(1-\gamma)^{2}}\) and \(\frac{2\alpha\gamma\epsilon}{(1-\gamma)^{2}}\) terms, since the objective is to maximize \(U_{r}(\pi_{A})-U_{r}(\pi_{B})\), we heuristically estimate the \(\epsilon\) in Eq.27 by using the samples from \(\pi_{P}\) and the \(\epsilon\) in Eq.28 by using the samples from \(\pi_{A}\). As a result we have the objective functions defined as Eq.29 and 30 where \(\xi_{1}(s,a)=\frac{\pi_{P}(a^{(t)}|s^{(t)})}{\pi_{A}(a^{(t)}|s^{(t)})}\) and \(\xi_{2}=\frac{\pi_{A}(a^{(t)}|s^{(t)})}{\pi_{P}(a^{(t)}|s^{(t)})}\) are the importance sampling probability ratio derived from the definition of \(\Delta\mathcal{A}\); \(C_{1}\propto-\frac{\gamma\hat{\alpha}}{(1-\gamma)}\) and \(C_{2}\propto\frac{\gamma\hat{\alpha}}{(1-\gamma)}\) where \(\hat{\alpha}\) is either an estimated maximal KL-divergence between \(\pi_{A}\) and \(\pi_{B}\) since \(D_{KL}\geq D_{TV}^{2}\) according to Schulman et al. (2015), or an estimated maximal \(D_{TV}^{2}\) depending on whether the reward function is Gaussian or Categorical. We also note that for finite horizon tasks, we compute the average rewards instead of the discounted accumulated rewards in Eq.30 and 29.

\[J_{R,1}(r;\pi_{P},\pi_{A}):=\underset{\tau\sim\pi_{A}}{\mathbb{E}} \left[\sum\limits_{t=0}^{\infty}\gamma^{t}\left(\xi_{1}(s^{(t)},a^{(t)})-1 \right)r(s^{(t)},a^{(t)})\right]+C_{1}\max\limits_{(s,a)\sim\pi_{A}}|r(s,a)|\] (29) \[J_{R,2}(r;\pi_{P},\pi_{A}):=\underset{\tau\sim\pi_{P}}{\mathbb{E} }\left[\sum\limits_{t=0}^{\infty}\gamma^{t}\left(1-\xi_{2}(s^{(t)},a^{(t)}) \right)r(s^{(t)},a^{(t)})\right]+C_{2}\max\limits_{(s,a)\sim\pi_{P}}|r(s,a)|\] (30)

Beside \(J_{R,1},J_{R,2}\), we additionally use two more objective functions based on the derived bounds. W \(J_{R,r}(r;\pi_{A},\pi_{P})\). By denoting the optimal policy under \(r\) as \(\pi^{*}\), \(\alpha^{*}=\max\limits_{s\in\mathbb{S}}\ D_{TV}(\pi^{*}(\cdot|s),\pi_{A}(\cdot|s),\)\(\epsilon^{*}=\max\limits_{(s,a^{(t)})}|\mathcal{A}_{\pi^{*}}(s,a^{(t)})|\), and \(\Delta\mathcal{A}_{A}^{*}(s)=\underset{a\sim\pi_{A}}{\mathbb{E}}\left[ \mathcal{A}_{\pi^{*}}(s,a)\right]-\underset{a\sim\pi^{*}}{\mathbb{E}}\left[ \mathcal{A}_{\pi^{*}}(s,a)\right]\), we have the following.

\[U_{r}(\pi_{P})-U_{r}(\pi^{*})\] (31) \[= U_{r}(\pi_{P})-U_{r}(\pi_{A})+U_{r}(\pi_{A})-U_{r}(\pi^{*})\] \[\leq U_{r}(\pi_{P})-U_{r}(\pi_{A})+\sum\limits_{t=0}^{\infty}\gamma^{ t}\underset{s^{(t)}\sim\pi_{A}}{\mathbb{E}}\left[\Delta\mathcal{A}_{A}^{*}(s^{(t)}) \right]+\frac{2\alpha^{*}\gamma\epsilon^{*}}{(1-\gamma)^{2}}\] \[= U_{r}(\pi_{P})-\sum\limits_{t=0}^{\infty}\gamma^{t}\underset{s^ {(t)}\sim\pi_{A}}{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{A}}{\mathbb{E}} \left[r(s^{(t)},a^{(t)})\right]\right]+\] \[\sum\limits_{t=0}^{\infty}\gamma^{t}\underset{s^{(t)}\sim\pi_{A} }{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi_{A}}{\mathbb{E}}\left[\mathcal{A}_ {\pi^{*}}(s^{(t)},a^{(t)})\right]-\underset{a^{(t)}\sim\pi^{*}}{\mathbb{E}} \left[\mathcal{A}_{\pi^{*}}(s^{(t)},a^{(t)})\right]\right]+\frac{2\alpha^{*} \gamma\epsilon^{*}}{(1-\gamma)^{2}}\] \[= U_{r}(\pi_{P})-\sum\limits_{t=0}^{\infty}\gamma^{t}\underset{s^ {(t)}\sim\pi_{A}}{\mathbb{E}}\left[\underset{a^{(t)}\sim\pi^{*}}{\mathbb{E}} \left[\mathcal{A}_{\pi^{*}}(s^{(t)},a^{(t)})\right]\right]+\frac{2\alpha^{*} \gamma\epsilon^{*}}{(1-\gamma)^{2}}\] \[= \underset{\tau\sim\pi_{P}}{\mathbb{E}}\left[\sum\limits_{t=0}^{ \infty}\gamma^{t}r(s^{(t)},a^{(t)})\right]-\underset{\tau\sim\pi_{A}}{\mathbb{E }}\left[\sum\limits_{t=0}^{\infty}\gamma^{t}\frac{\exp(r(s^{(t)},a^{(t)}))}{\pi_{A} (a^{(t)}|s^{(t)})}r(s^{(t)},a^{(t)})\right]+\frac{2\alpha^{*}\gamma\epsilon^{* }}{(1-\gamma)^{2}}\]

Let \(\xi_{3}=\frac{\exp(r(s^{(t)},a^{(t)}))}{\pi_{A}(a^{(t)}|s^{(t)})}\) be the importance sampling probability ratio. It is suggested in Schulman et al. (2017) that instead of directly optimizing the objective function Eq.31, optimizing a surrogate objective function as in Eq.32, which is an upper-bound of Eq.31, with some small \(\delta\in(0,1)\) can be much less expensive and still effective.

\[J_{R,3}(r;\pi_{P},\pi_{A}):=\mathop{\mathbb{E}}_{\tau\sim\pi_{P}} \left[\sum_{t=0}^{\infty}\gamma^{t}r(s^{(t)},a^{(t)})\right]-\] \[\mathop{\mathbb{E}}_{\tau\sim\pi_{A}}\left[\sum_{t=0}^{\infty} \gamma^{t}\min\left(\xi_{3}\cdot r(s^{(t)},a^{(t)}),clip(\xi_{3},1-\delta,1+ \delta)\cdot r(s^{(t)},a^{(t)})\right)\right]\] (32)

Alternatively, we let \(\Delta\mathcal{A}_{P}^{*}(s)=\mathop{\mathbb{E}}_{a\sim\pi_{P}}\left[\mathcal{A }_{\pi^{*}}(s,a)\right]-\mathop{\mathbb{E}}_{a\sim\pi^{*}}\left[\mathcal{A}_{ \pi^{*}}(s,a)\right]\). The according to Eq.27, we have the following.

\[U_{r}(\pi_{P})-U_{r}(\pi^{*})\] \[\leq \sum_{t=0}^{\infty}\gamma^{t}\mathop{\mathbb{E}}_{s^{(t)}\sim \pi_{P}}\left[\Delta\mathcal{A}_{P}^{*}(s^{(t)})\right]+\frac{2\alpha^{*} \gamma(2\alpha^{*}+1)\epsilon^{*}}{(1-\gamma)^{2}}\] \[= \sum_{t=0}^{\infty}\gamma^{t}\mathop{\mathbb{E}}_{s^{(t)}\sim \pi_{P}}\left[\mathop{\mathbb{E}}_{a^{(t)}\sim\pi_{P}}\left[\mathcal{A}_{\pi^ {*}}(s^{(t)},a^{(t)})\right]-\mathop{\mathbb{E}}_{a^{(t)}\sim\pi^{*}}\left[ \mathcal{A}_{\pi^{*}}(s^{(t)},a)^{(t)}\right]\right]+\frac{2\alpha^{*}\gamma(2 \alpha^{*}+1)\epsilon^{*}}{(1-\gamma)^{2}}\]

Then a new objective function \(J_{R,4}\) is formulated in Eq.34 where \(\xi_{4}=\frac{\exp(r(s^{(t)},a^{(t)}))}{\pi_{P}(a^{(t)}|s^{(t)})}\).

\[J_{R,4}(r;\pi_{P},\pi_{A}):=\mathop{\mathbb{E}}_{\tau\sim\pi_{P} }\left[\sum_{t=0}^{\infty}\gamma^{t}r(s^{(t)},a^{(t)})\right]-\] \[\mathop{\mathbb{E}}_{\tau\sim\pi_{P}}\left[\sum_{t=0}^{\infty} \gamma^{t}\min\left(\xi_{4}\cdot r(s^{(t)},a^{(t)}),clip(\xi_{4},1-\delta,1+ \delta)\cdot r(s^{(t)},a^{(t)})\right)\right]\] (34)

### Incorporating IRL Algorithms

**Online RL Setting.** In our implementation, we combine PAGAR with GAIL Ho and Ermon (2016), VAIL Peng et al. (2019), and f-IRL Ni et al. (2021), respectively. In this section, we use \(J_{IRL}\) to indicate the IRL loss to be minimized in place of the notation \(\mathcal{J}_{IRL}\) in the main text. Accordingly, \(\delta\) is the target IRL loss.

* **f-IRL**. We use the FKL of f-IRL in our experiments. Since FKL is explicitly models a reward function with a neural network, when PAGAR is combined with FKL, the meta-algorithm Algorithm 1 can be directly implemented without changes except for letting \(J_{IRL}\) be the FKL loss.
* **GAIL and VAIL**. We take additional steps to incorporate GAN-based algorithms, as they do not explicitly learn reward functions but instead train discriminators. When PAGAR is combined with GAIL, the meta-algorithm Algorithm 1 becomes Algorithm 2. When PAGAR is combined with VAIL, it becomes Algorithm 3. Both of the two algorithms are GAN-based IRL, indicating that both algorithms use Eq.1 as the IRL objective function. In these two cases, we use a neural network to approximate \(D\), the discriminator in Eq.1. To get the reward function \(r\), we follow Fu et al. (2018) and denote \(r(s,a)=\log\left(\frac{\pi_{A}(a|s)}{D(s,a)}-\pi_{A}(a|s)\right)\) as mentioned in Section 1. Hence, the only difference between Algorithm 2 and Algorithm 1 is in the representation of the reward function. Regarding VAIL, since it additionally learns a representation for the state-action pairs, a bottleneck constraint \(J_{IC}(D)\leq i_{c}\) is added where the bottleneck \(J_{IC}\) is estimated from policy roll-outs. VAIL introduces a Lagrangian parameter \(\beta\) to integrate \(J_{IC}(D)-i_{c}\) in the objective function. As a result its objective function becomes \(J_{IRL}(r)+\beta\cdot(J_{IC}(D)-i_{c})\). VAIL not only learns the policy and the discriminator but also optimizes \(\beta\). In our case, we utilize the samples from both protagonist and antagonist policies to optimize \(\beta\) as in line 10 following Peng et al. (2019).

In our implementation, depending on the difficulty of the benchmarks, we choose to maintain \(\lambda\) as a constant or update \(\lambda\) with the IRL loss \(J_{IRL}(r)\) in most of the continuous control tasks. In _HalfCheetah-v2_ and all the maze navigation tasks, we update \(\lambda\) by introducing a hyperparameter \(\mu\). As described in the maintext, we treat \(\delta\) as the target IRL loss of \(J_{IRL}(r)\), i.e., \(J_{IRL}(r)\leq\delta\)In all the maze navigation tasks, we initialize \(\lambda\) with some constant \(\lambda_{0}\) and update \(\lambda\) by \(\lambda:=\lambda\cdot\exp(\mu\cdot(J_{IRL}(r)-\delta))\) after every iteration. In _HalfCheeta-v2_, we update \(\lambda\) by \(\lambda:=max(\lambda_{0},\lambda\cdot\exp(\mu\cdot(J_{IRL}(r)-\delta)))\) to avoid \(\lambda\) being too small. Besides, we use PPO Schulman et al. (2017) to train all policies in Algorithm 2 and 3.

**Offline RL Setting.** We incorporate PAGAR with RECOIL Sikchi et al. (2024). The original RECOIL algorithm does not learn a reward function but learns a \(Q\), \(V\) value functions and a policy \(\pi\) with neural networks. In order to combine PAGAR with RECOIL, we made the following modification to RECOIL:

* Instead of learning the \(Q\) value function, we explicitly learn a reward function \(r:\mathbb{S}\times\mathbb{A}\times\mathbb{S}\rightarrow\mathbb{R}\), which takes the current state \(s\), action \(a\) and the next state \(s^{\prime}\) as input, and outputs a real number as the reward
* We use the same loss function as that for optimizing \(Q\) in RECOIL to optimize \(r\) by replacing \(Q(s,a)\) with \(r(s,a,s^{\prime})+\gamma V(s^{\prime})\) for every \((s,a,s^{\prime})\) sampled from an offline dataset \(\mathbb{D}\). We denote this loss function for \(r\) as \(\mathcal{J}_{IRL}(r)\) as in Eq.35 where \(S\) is an offline trajectory sample set of some sub-optimal behavioral policy, \(d^{E,S}_{mix}\) indicates a mixture between \(S\) and \(E\) sets,and \(\beta\) is the _mixing ratio_ as defined in Sikchi et al. (2024). \[\mathcal{J}_{IRL}(r):=\beta\left(\mathbb{E}_{(s,a,s^{\prime})\sim \mathcal{S}}[r(s,a,s^{\prime})+\gamma V(s^{\prime})]-\mathbb{E}_{(s,a,s^{\prime} )\sim E}[r(s,a)+\gamma V(s^{\prime})]\right)+\] \[0.25\cdot\mathbb{E}_{(s,a,s^{\prime})\sim d^{E,S}_{mix}}\left[(r( s,a,s^{\prime}))^{2}\right]\] (35)
* We use the same loss function for optimizing the value function \(V\) as in RECOIL to still optimize \(V\), except for replacing the target \(Q(s,a)\) with target \(r(s,a,s^{\prime})+\gamma V(s^{\prime})\) for every \((s,a,s^{\prime})\) experience sampled from the offline dataset. Consequently, we have the loss function for \(V\) as defined in Eq.36 where \(\sigma\) is a conservatism parameter defined in Sikchi et al. (2024). \[\mathcal{J}_{V}(V):=\mathbb{E}_{(s,a,s^{\prime})\sim d^{E,S}_{mix }}\big{[}\exp\big{(}(r(s,a,s^{\prime})+\gamma V(s^{\prime})-V(s))/\sigma \big{)}+\] \[\big{(}r(s,a,s^{\prime})+\gamma V(s^{\prime})-V(s)\big{)}\big{]}\] (36)
* Instead of learning a single policy as in RECOIL, we learn a protagonist and antagonist policies \(\pi_{P}\) and \(\pi_{A}\) by using the same SAC-like policy update rule as in RECOIL, except for replacing \(Q(s,a)\) with \(r(s,a,s^{\prime})+\gamma V(s^{\prime})\) for every \((s,a,s^{\prime})\) experience sampled from the offline dataset. The objective function for learning \(\pi_{A}\) and \(\pi_{P}\) are defined in Eq.37. \[\mathcal{J}_{RL}(\pi):=\mathbb{E}_{(s,a,s^{\prime})\sim d^{E,S}_{mix}}\big{[} \exp\big{(}(r(s,a,s^{\prime})+\gamma V(s^{\prime})-V(s))\big{)}\log\pi(a|s) \big{]}\] (37)
* With some heuristic, we construct a PAGAR-loss as follows. \[\mathcal{J}_{PAGAR}(r) := \mathbb{E}_{(s,a,s^{\prime})\sim E}\left[r(s,a,s^{\prime})\cdot \exp(clip\big{(}max(0,\log\frac{\pi_{P}(a|s)}{\pi_{A}(a|s)}),-1,1)\right]+\] (38) \[\mathbb{E}_{(s,a,s^{\prime})\sim\mathcal{S}}\left[r(s,a,s^{\prime })\cdot\exp(clip\big{(}min(0,\log\frac{\pi_{A}(a|s)}{\pi_{P}(a|s)}),-1,1)\right]\]
* For simplicity, we multiply this PAGAR-loss \(\mathcal{J}_{PAGAR}\) with a fixed Lagrangian parameter \(\lambda=1e-3\) and add it to the aforementioned loss \(\mathcal{J}_{IRL}\) for optimizing \(r\).

We summarize this RECOIL w/ PAGAR algorithm in Algorithm 4.

``` Input: Expert demonstration \(E\), behavioral policy sample set \(S\), reward function \(r\), value function \(V\), initial protagonist policy \(\pi_{P}\), antagonist policy \(\pi_{A}\), maximum iteration number \(N\). Output: \(\pi_{P}\)
1:while iteration number \(i<N\)do
2: Train \(r\) using \(\min\limits_{r}J_{IRL}(r)+0.001\cdot J_{PAGAR}(r)\)
3: Train \(V\) using \(\min\limits_{V}J_{V}(V)\)
4: Train \(\pi_{A}\) using \(\max\limits_{\pi_{A}}J_{RL}(\pi_{A})\)
5: Train \(\pi_{P}\) using \(\max\limits_{\pi_{A}}J_{RL}(\pi_{P})\)
6:endwhile
7:return\(\pi_{P}\) ```

**Algorithm 4** RECOIL w/ PAGAR

## Appendix C Experiment Details

This section presents some details of the experiments and additional results.

### Experimental Details

Hardware. All experiments are carried out on a quad-core i7-7700K processor running at 3.6 GHz with a NVIDIA GeForce GTX 1050 Ti GPU and a 16 GB of memory. **Network Architectures**. Our algorithm involves a protagonist policy \(\pi_{P}\), and an antagonist policy \(\pi_{A}\). In our implementation, the two policies have the same structures. Each structure contains two neural networks, an actor network, and a critic network. When associated with GAN-based IRL, we use a discriminator \(D\) to represent the reward function as mentioned in Appendix B.4.

* **Protagonist and Antagonist policies**. We prepare two versions of actor-critic networks, a fully connected network (FCN) version, and a CNN version, respectively, for the Mujoco and Mini-Grid benchmarks. The FCN version, the actor and critic networks have \(3\) layers. Each hidden layer has \(100\) neurons and a \(tanh\) activation function. The output layer output the mean and standard deviation of the actions. In the CNN version, the actor and critic networks share \(3\) convolutional layers, each having \(5\), \(2\), \(2\) filters, \(2\times 2\) kernel size, and \(ReLU\) activation function. Then \(2\) FCNs are used to simulate the actor and critic networks. The FCNs have one hidden layer, of which the sizes are \(64\).
* **Discriminator \(D\) for PAGAR-based GAIL in Algorithm 2**. We prepare two versions of discriminator networks, an FCN version and a CNN version, respectively, for the Mujoco and Mini-Grid benchmarks. The FCN version has \(3\) linear layers. Each hidden layer has \(100\) neurons and a \(tanh\) activation function. The output layer uses the \(Sigmoid\) function to output the confidence. In the CNN version, the actor and critic networks share \(3\) convolutional layers, each having \(5\), \(2\), \(2\) filters, \(2\times 2\) kernel size, and \(ReLU\) activation function. The last convolutional layer is concatenated with an FCN with one hidden layer with \(64\) neurons and \(tanh\) activation function. The output layer uses the \(Sigmoid\) function as the activation function.
* **Discriminator \(D\) for PAGAR-based VAIL in Algorithm 3**. We prepare two versions of discriminator networks, an FCN version and a CNN version, respectively, for the Mujoco and Mini-Grid benchmarks. The FCN version uses \(3\) linear layers to generate the mean and standard deviation of the embedding of the input. Then a two-layer FCN takes a sampled embedding vector as input and outputs the confidence. The hidden layer in this FCN has \(100\) neurons and a \(tanh\) activation function. The output layer uses the \(Sigmoid\) function to output the confidence. In the CNN version, the actor and critic networks share \(3\) convolutional layers, each having \(5\), \(2\), \(2\) filters, \(2\times 2\) kernel size, and \(ReLU\) activation function. The last convolutional layer is concatenated with a two-layer FCN. The hidden layer has \(64\) neurons and uses \(tanh\) as the activation function. The output layer uses the \(Sigmoid\) function as the activation function.

**Hyperparameters** The hyperparameters that appear in Algorithm 3 and 3 are summarized in Table 2 where we use N/A to indicate using \(\delta^{*}\), in which case we let \(\mu=0\). Otherwise, the values of \(\mu\) and \(\delta\) vary depending on the task and IRL algorithm. The parameter \(\lambda_{0}\) is the initial value of \(\lambda\) as explained in Appendix B.4.

**Expert Demonstrations.** Our expert demonstrations all achieve high rewards in the task. The number of trajectories and the average trajectory total rewards are listed in Table 3.

### Additional Results

**Continuous Tasks with Non-Binary Outcomes** We test PAGAR-based IRL in \(5\) Mujuco tasks where the task objectives do not have binary outcomes. We append the results in three Mujoco benchmarks: _Walker2d-v2_, _HalfCheeta-v2_, _Hopper-v2_, _InvertedPendulum-v2_ and _Swimmer-v2_ in Figure 6 and 7. Algorithm 1 performs similarly to VAIL and GAIL in those two benchmarks. The results show that PAGAR-based IL takes fewer iterations to achieve the same performance as the baselines. In particular, in the _HalfCheetah-v2_ task, Algorithm 1 achieves the same level of performance compared

\begin{table}
\begin{tabular}{c|c|c} Parameter & Continuous Control Domain & Partially Observable Domain \\ Policy training batch size & 64 & 256 \\ Discount factor & 0.99 & 0.99 \\ GAE parameter & 0.95 & 0.95 \\ PPO clipping parameter & 0.2 & 0.2 \\ \(\lambda_{0}\) & 1e3 & 1e3 \\ \(\sigma\) & 0.2 & 0.2 \\ \(i_{c}\) & 0.5 & 0.5 \\ \(\beta\) & 0.0 & 0.0 \\ \(\mu\) & VAIL(HalfCheetah): 0.5; others: 0.0 & VAIL: 1.0; GAIL: 1.0 \\ \(\delta\) & VAIL(HalfCheetah): 1.0; others: N/A & VAIL: 0.8; GAIL: 1.2 \\ \end{tabular}
\end{table}
Table 2: Hyperparameters used in the training processeswith GAIL and VAIL by using only half the numbers of iterations. IQ-learn does not perform well in Walker2d-v2 but performs better than ours and other baselines by a large margin.

### Influence of Reward Hypothesis Space

In addition to the _DoorKey-6x6-v0_ environment, we also tested PAGAR-GAIL and GAIL in _SimpleCrossingS9N2-v0_ environment. The results are shown in Figure 6 and 8.

\begin{table}
\begin{tabular}{c|c|c} Task & Number of Trajectories & Average Tot.Rewards \\ Walker2d-v2 & 10 & 4133 \\ HalfCheetah-v2 & 100 & 1798 \\ Hopper-v2 & 100 & 3586 \\ InvertedPendulum-v2 & 10 & 1000 \\ Swimmer-v2 & 10 & 122 \\ DoorKey-6x6-v0 & 10 & 0.92 \\ SimpleCrossingS9N1-v0 & 10 & 0.93 \\ \end{tabular}
\end{table}
Table 3: The number of demonstrated trajectories and the average trajectory rewards

Figure 6: (Left: Walker2d-v2. Right: HalfCheeta-v2) The \(y\) axis indicates the average return per episode.

Figure 7: Comparing Algorithm 1 with baselines. The suffix after each ‘PAGAR-’ indicates which IRL algorithm is utilized in Algorithm 1. The \(y\) axis is the average return per step. The \(x\) axis is the number of iterations in GAIL, VAIL, and ours. The policy is executed between each iteration for \(2048\) timesteps for sample collection. One exception is that IQ-learn updates the policy at every timestep, making its actual number of iterations \(2048\) times larger than indicated in the figures.

Figure 8: Comparing Algorithm 1 with baselines. The prefix ‘protagonist_GAIL’ indicates that the IRL algorithm utilized in Algorithm 1 is the same as in GAIL. The ‘_Sigmoid’ and ‘_Categ’ suffixes indicate whether the output layer of the discriminator is using the \(Sigmoid\) function or Categorical distribution. The \(x\) axis is the number of sampled frames. The \(y\) axis is the average return per episode.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the task-aligned reward function is hardly predictable in IRL-based IL setting. As mentioned in Section 5, "**How to build \(R_{E,k}\)**", it is designer's decision to set value for the parameter \(k\). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Every lemma, proposition and theorem have the set of assumptions included in them. The proves are included in Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We disclosed all the information in Appendix C.1. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the source code in our submission, including the instructions on how to reproduce the results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We disclosed all the information in Appendix C.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The curves in the plots are smoothed. They reflect how the policies' average returns change as the learning episodes increase. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We disclosed all the information in Appendix C.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conform with NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper does not discuss and is not intended to cause significant societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not release data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.