Unsupervised Representation Learning of Brain Activity via Bridging Voxel Activity and Functional Connectivity

 Ali Behrouz

Cornell University

ab2947@cornell.edu

&Parsa Delavari

University of British Columbia

parsadlr@student.ubc.ca

&Farnoosh Hashemi

Cornell University

sh2574@cornell.edu

Equal Contribution.

###### Abstract

Effective brain representation learning is a key step toward the understanding of cognitive processes and unlocking detecting and potential therapeutic interventions for neurological diseases/disorders. Existing studies have focused on either (1) voxel-level activity, where only a single weight relating the voxel activity to the task (i.e., aggregation of voxel activity over a time window) is considered, missing their temporal dynamics, or (2) functional connectivity of the brain in the level of region of interests, missing voxel-level activities. In this paper, we bridge this gap and design BrainMixer, an unsupervised learning framework that effectively utilizes both functional connectivity and associated time series of voxels to learn voxel-level representation in an unsupervised manner. BrainMixer employs two simple yet effective MLP-based encoders to simultaneously learn the dynamics of voxel-level signals and their functional correlations. To encode voxel activity, BrainMixerfuses information across both time and voxel dimensions via a dynamic self-attention mechanism. To learn the structure of the functional connectivity graph, BrainMixer presents a temporal graph patching and encodes each patch by combining its nodes' features via a new adaptive temporal pooling. Our experiments show that BrainMixer attains outstanding performance and outperforms 14 baselines in different downstream tasks and experimental setups.

## 1 Introduction

The recent advancement of neuroimaging provide rich information to analyze the human brain. The provided data, however, is high-dimensional and complex [68], which makes it hard to take advantage of powerful machine learning models in analyzing them. To overcome this challenge, representation learning serves as the backbone of machine learning methods on neuroimage data and provides a low-dimensional representation of brain components at different levels of granularity, enabling the understanding of behaviors [77], brain functions [97] and/or detecting neurological diseases [84].

In the brain imaging literature, studies have mainly focused on two spatial scales--voxel-level and network-level--as well as two analysis approaches--multivariate pattern analysis (MVPA) and functional connectivity [60, 85]. The MVPA approach is often employed at the voxel-level scale and in task-based studies to associate neural activities at a very fine-grained and local level with particular cognitive functions, behaviors, or stimuli. This method has found applications in various areas, including the detection of neurological conditions [81, 11], neurofeedback interventions [22], decoding neural responses to visual stimuli [41], deciphering memory contents [53, 15], and classifying cognitive states [64]. The functional connectivity analysis, on the other hand, focuses on the temporal correlations or statistical dependencies between the activity of different brain regions at larger scales to assess how these areas communicate and collaborate. This method has been utilized to study various topics such as task-related network dynamics [32; 43] and the effects of neurological disorders on brain connectivity [33; 29].

**Limitation of Previous Methods.** Despite the advances in the representation learning of brain signals, existing studies suffer from a subset of five limitations: 1 Study the human brain at a single scale: Most existing studies study the brain at either voxel-level or functional connectivity, while these two scales can provide complementary information to each other; e.g., although voxel-level activity provides detailed and more accurate information about brain activity, it misses the information about how different areas communicate with each other at a high level. Recently, this limitation has motivated researchers to search for new methods of integrating these two levels of analyses [66; 62]. 2 Supervised setting: Learning brain activity in a supervised setting relies on a large number of clinical labels while obtaining accurate and reliable clinical labels is challenging due to its high cost [3].3 Missing information by averaging: Most existing studies on voxel activities aggregate measured voxel activity (e.g., its blood-oxygen level dependence) over each time window to obtain a single beta weight [73; 88; 72]. However, this approach misses the voxel activity dynamic over each task. Moreover, most studies on brain functional connectivity also aggregate closed voxels to obtain brain activity in the Region of Interest (ROI) level, missing individual voxel activities. 4 Missing the dynamics of the interactions: Some existing studies neglect the fact that the functional connectivity of the human brain dynamically changes over time, even in resting-state neuroimaging data [14]. In task-dependent neuroimage data, subjects are asked to perform different tasks in different time windows, and the dynamics of the brain activity play an important role in understanding neurological disease/disorder [39]. 5 Designed for a particular task or neuroimaging modality: Due to the different and complex clinical patterns of brain signals [27], some existing methods are designed for a particular type of brain signal data [52; 13], and there is a lack of a unified framework.

**Application to Understanding Object Representation in the Brain.** Understanding object representation in the brain is a key step toward revealing the basic building blocks of human visual processing [37]. Due to the hierarchical nature of human visual processing, it requires analyzing brain activity at different scales, i.e., both functional connectivity and voxel activity. However, there is a small number of studies in this area, possibly due to the lack of proper large-scale datasets. Recently, Hebart et al. [37] provided a large-scale fMRI and MEG datasets, THINGS, to fill this gap. However, the preprocessed data by Hebart et al. [37] not only does not provide functional connectivity, but it also has aggregated voxel activity over each time window, and provides a single beta weight for each voxel, missing dynamics of voxel activity. To address this limitation, we present two newly preprocessed versions of this dataset that provide both functional connectivity and voxel activity timeseries of fMRI and MEG modalities. See Appendix B for more details.

**Contributions.** To overcome the above limitations, we leverage both voxel-level activity and functional connectivity of the brain. We present BrainMixer, an unsupervised MLP-based brain representation learning approach that jointly learns representations of the voxel activity and functional connectivity. BrainMixer uses a novel multivariate timeseries encoder that binds information across both time and voxel dimensions. It uses a simple MLP with functional patching to fuse information across different timestamps and learns dynamic self-attention weights to fuse information across voxels based on their functionality. On the other hand, BrainMixer uses a novel temporal graph

Figure 1: **Schematic of the BrainMixer. BrainMixer consists of two main modules: (1) Voxel Activity Encoder (top), and (2) Functional Connectivity Encoder (bottom).**

learning method to encode the brain functional connectivity. The graph encoder first extracts temporal patches using temporal random walks and then fuses information within each patch using the designed dynamic self-attention mechanism. We further propose an adaptive permutation invariant pooling to obtain patch encodings. Since voxel activity and functional connectivity encodings are different views of the same context, we propose an unsupervised pre-training approach to jointly learn voxel activity and functional connectivity by maximizing their mutual information. In the experimental evaluations, we provide two new large-scale graph and timeseries datasets based on THINGS [37]. Extensive experiments on six datasets show the superior performance of BrainMixer and the significance of each of its components in a variety of downstream tasks.

For the sake of consistency, we explain BrainMixer for fMRI modality; however, as it is shown in SS4, it can simply be used for any other neuroimaging modalities that provide a timeseries for each part of the brain (e.g., MEG and EEG). When dealing with MEG or EEG, we can replace the term "voxel" with "channel". Supplementary materials can be found in this link.

## 2 Related Work

Timeseries Learning.Attention mechanisms are powerful models to capture long-range dependencies and so recently, Transformer-based models have attracted much attention in time series forecasting [103; 55]. Due to the quadratic time complexity of attention mechanisms, several studies aim to reduce the time and memory usage of these methods [20]. Another type of work uses (hyper)graph learning frameworks to learn (higher-order) patterns in timeseries [67; 75]. Inspired by the recent success of MLP-Mixer[82], Li et al. [57] and Chen et al. [19] presented two variants of MLP-Mixer for timeseries forecasting. All these methods are different from BrainMixer, as 1 they use static attention mechanisms, 2 do not take advantage of the functionality of voxels in patching, and 3 are designed for timeseries forecasting and cannot simply be extended to various downstream tasks on the brain.

MLP-based Graphs Learning.Learning on graphs has been an active research area in recent years [44; 90; 16]. While most studies use message-passing frameworks to learn the local and global structure of the graph, recently, due to the success of MLP-based methods [82], MLP-based graph learning methods have attracted much attention [42; 10]. For example, Cong et al. [21] and He et al. [35] presented two extensions of MLP-Mixer to graph-structured data. However, all these methods are different from BrainMixer and specifically FC Encoder, as either 1 use time-consuming graph clustering algorithms for patching, 2 are static methods and cannot capture temporal properties, or 3 are attention-free and cannot capture the importance of nodes.

Graph Learning and Timeseries for Neuroscience.In recent years, several studies have analyzed functional connectivity to differentiate human brains with a neurological disease/disorder [45; 18; 93]. With the success of graph neural networks in graph data analysis, deep learning models have been developed to predict brain diseases by studying brain network structures [7; 108; 26]. Moreover, several studies focus on brain signals [24; 79] to detect neurological diseases. For example, Cai et al. [13] designed a self-supervised learning framework to detect seizures from EEG and SEEG data. However, these methods are different from BrainMixer as they are designed for a particular task (e.g., classification), a particular neuroimaging modality (e.g., fMRI or EEG), and/or supervised settings.

## 3 Method: BrainMixer

Detailed discussion about background concepts can be found in Appendix A.

Notation.We represent the neuroimaging of a human brain as \(\mathcal{B}=\{\mathcal{B}^{(t)}\}_{t=1}^{T}\) where \(\mathcal{B}^{(t)}=(\mathcal{V},\mathcal{G}_{F}^{(t)},\mathcal{X}^{(t)},\mathbb{ F})\) represents the neural data in time window \(1\leq t\leq T\). Here, \(\mathcal{V}\) is the set of voxels, \(\mathcal{G}_{F}^{(t)}=(\mathcal{V},\mathcal{E}^{(t)},\mathcal{A}^{(t)})\) is the functional connectivity graph, \(\mathcal{E}^{(t)}\subseteq\mathcal{V}\times\mathcal{V}\) is the set connections between voxels, \(\mathcal{A}^{(t)}\) is the correlation matrix (weighted adjacency matrix of \(\mathcal{G}_{F}^{(t)}\)), \(\mathcal{X}^{(t)}\in\mathbb{R}^{|\mathcal{V}|\times\tilde{T}(t)}\) is a multivariate timeseries of voxels activities, \(\tilde{T}(t)\) is the length of the timeseries, and \(\mathbb{F}\) is the set of functional systems in the brain [76] in time window \(t\). In task-dependent data, each time window \(t\) corresponds to a task, and in resting state data, we have \(T=1\). We let \(t_{\max}=\max_{t=1,\ldots,T}\tilde{T}(t)\)representing the maximum length of timeseries. BrainMixer consists of two main modules 1_Voxel Activity_ (VA) Encoder and 2 Functional Connectivity (FC) Encoder:

### Voxel Activity Encoder

The main goal of this module is to learn the time series of the voxel-level activity. However, the activities of voxels are not disjoint; for example, an increase in fusiform face area (FFA) activity might be associated with a rise in V1 activity. Accordingly, effectively learning their dynamics patterns requires both capturing cross-voxel and within-voxel time series information. The vanilla MLP-Mixer[82] can be used to bind information across both of these dimensions, but the human brain has unique traits that make directly applying vanilla MLP-Mixer insufficient/impractical. First, there does not exist in general a canonical grid of the brain to encode voxel activities, which makes patch extraction challenging. Second, contrary to images that can be divided into patches of the same size, the partitioning of voxels might not be all the same size due to the complex brain topology. Third, vanilla MLP-Mixer employs a fixed static mixing matrix for binding patches, while in the brain the functionality of each patch is important and a different set of patchs should be mixed differently based on their connections and functionality. To address these challenges, the _VA Encoder_ employs two submodules, _time-mixer_ and _voxel-mixer_ with dynamic mixing matrix, to fuse information across both time and voxel dimensions, respectively.

The human brain is comprised of functional systems (FS) [76], which are groups of voxels that perform similar functions [80]. We take advantage of this hierarchical structure and patch voxels based on their functionality. However, the main challenge is that the sizes of the patches (set of voxels with similar functionality) are different. To this end, inspired by the inference of ViT models [28], we linearly interpolate patches with smaller sizes.

Functional Patching.Let \(|\mathcal{V}|\) be the number of voxels and \(\mathbf{X}\in\mathbb{R}^{|\mathcal{V}|\times(T\times t_{\max})}\) represents the time series of voxels activities over all time windows. We split \(\mathbf{X}\) to spatio-temporal patches \(\mathbf{X}_{i}\) with size \(|f_{i}|\times t_{\max}\), where \(f_{i}\in\mathbb{F}\) is a functional system [76]. To address the challenge of different patch sizes, we use Interpolate\((.)\) to linearly interpolate patches to the same size \(N_{p}\): i.e., \(\tilde{\mathbf{X}}_{i}=\textsc{Interpolate}(\mathbf{X}_{i})\), where \(\tilde{\mathbf{X}}_{i}\in\mathbb{R}^{N_{p}\times t_{\max}}\). We let \(\tilde{\mathbf{X}}\in\mathbb{R}^{|\mathcal{V}|\times t_{\max}}\) be the matrix of \(\tilde{\mathbf{X}}_{i}\).

Voxel-Mixer.Since the effect of each task (e.g., in task-based fMRI) on brain activity as well as the time it lasts varies [98], for different tasks, we might need to emphasize more on a subset of voxels. To this end, to bind information across voxels, we use a dynamic attention mechanism that uses a learnable dynamic mixing matrix \(\mathbf{P}_{i}\), learning to mix a set of input voxels based on their functionality. While using different learnable matrices for mixing voxels activity provides a more powerful architecture, its main challenge is a large number of parameters. To mitigate this challenge, we first reduce the dimensions of \(\tilde{\mathbf{X}}\), split it into a set of segments, denoted as \(S\), and then combine the transformed matrices. Given a segment \(s\in S\) we have:

\[\hat{\mathbf{X}}^{(t)^{(s)}}=\tilde{\mathbf{X}}^{(t)}\;\mathbf{W }^{(s)}_{\text{segment}}\;\in\mathbb{R}^{|\mathcal{V}|\times d},\] ( _Dimension Reduction_ ) \[\mathbf{P}^{(s)}_{i}=\textsc{Softmax}\left(\textsc{Flat}\left( \hat{\mathbf{X}}^{(t)^{(s)}}\right)\mathbf{W}^{(s)^{(i)}}_{\text{flat}}\right) \;\in\mathbb{R}^{1\times|\mathcal{V}|},\] ( _Learning Dynamic Mixer_ ) \[\mathbf{X}^{(t)}_{\text{PE}}=\left[\big{\|}_{s\in S}\mathbf{P}^ {(s)}\tilde{\mathbf{X}}^{(t)^{(s)}}\right]\mathbf{W}_{\text{PE}}\;\;\in \mathbb{R}^{|\mathcal{V}|\times t_{\max}},\] ( _Dynamic Positional Encoding_ ) \[\mathbf{H}^{(t)}_{\text{Voxel}}=\textsc{Norm}\left(\tilde{ \mathbf{X}}^{(t)}\right)+\textsc{Sigmoid}\left(\frac{\mathbf{X}^{(t)}_{\text{PE }}\,\mathbf{X}^{(t)^{\top}}_{\text{PE}}}{\sqrt{T}}\right)\mathbf{X}^{(t)}_{ \text{PE}},\] ( _Dynamic Self-Attention_ )

where \(\mathbf{W}^{(s)}_{\text{segment}}\in\mathbb{R}^{t_{\max}\times d}\), \(\mathbf{W}^{(s)^{(i)}}_{\text{flat}}\in\mathbb{R}^{d|\mathcal{V}|\times| \mathcal{V}|}\), \(\mathbf{W}_{\text{PE}}\in\mathbb{R}^{t_{\max}\times t_{\max}}\) are learnable parameters, \(\|\) is concatenation, and \(\textsc{Sigmoid}(.)\) is row-wise sigmoid normalization. Note that for different segments we use different dimensionality reduction matrices to reinforce the power of the Voxel Mixing.

Time Mixer.We then fuse information in the time dimension by using the Time Mixer submodule. To this end, the Time Mixer employs a 2-layer MLP with layer-normalization [4]:

\[\mathbf{H}^{(t)}_{\text{Time}}=\mathbf{H}^{(t)}_{\text{Voxel}}+\left(\sigma \left(\textsc{LayerNorm}\left(\mathbf{H}^{(t)}_{\text{Voxel}}\right)\mathbf{W}^ {(1)}_{\text{Time}}\right)\mathbf{W}^{(2)}_{\text{Time}}\right)\in\mathbb{R}^{| \mathcal{V}|\times t_{\max}},\] (1)where \(\mathbf{W}_{\text{Tune}}^{(1)}\) and \(\mathbf{W}_{\text{Tune}}^{(1)}\) are learnable matrices, \(\sigma(.)\) is an activation function (we use GeLU [38]), and LayerNorm is layer normalization [4].

### Functional Connectivity Encoder

To encode the functional connectivity graph, we design an MLP-based architecture that learns both the structural and temporal properties of the graph. Inspired by the recent success of all-MLP architecture in graphs [21], we extend MLP-Mixer to temporal graphs. We first define patches in temporal graphs. While patches in images, videos, and multivariate timeseries can simply be non-overlapping regular grids, patches in graphs are overlapping non-grid structures, which makes the patching extraction challenging. He et al. [35] suggest using graph partitioning algorithms to extract graph patches; however, these partitioning algorithms 1 only consider structural properties, missing the temporal dependencies, and 2 can be time-consuming, limiting the scalability to dense graphs like brain functional connectome. To this end, we propose a temporal-patch extraction algorithm such that nodes (voxels) in each patch share similar temporal and structural properties.

**Temporal Patching.** To extract temporal patches from the graph, we use a biased temporal random walk that walks over both nodes (voxels) and timestamps. Given a functional connectivity graph \(\mathcal{G}_{F}=\{\mathcal{G}_{F}^{(t)}\}_{t=1}^{T}\), we sample \(M\) walks with length \(m+1\) started from node (voxel) \(v_{0}\in\mathcal{V}\) like: \(\mathcal{W}\textit{alk}:(v_{0},t_{0})\rightarrow(v_{1},t_{1})\rightarrow\cdots \rightarrow(v_{m},t_{m})\), such that \((v_{i-1},v_{i})\in\mathcal{E}^{(t_{i})}\), and \(t_{0}\geq t_{1}\geq t_{2}\geq\cdots\geq t_{m}\). Note that, contrary to some previous temporal random walks [92; 10], we allow the walker to walk in the same timestamp at each step. While backtracking over time, we aim to capture temporal information and extract the dynamics of voxels' activity over _related_ timestamps. Previous studies show that doing a task can affect brain activity even after 2 minutes [98]. To this end, since more recent connections can be more informative, we use a biased sampling procedure. Let \(v_{\text{pre}}\) be the previously sampled node, we use hyperparameters \(\theta,\theta_{0}\geq 0\) to sample a node \(v\) with probability proportional to \(\exp\left(\theta(t-t_{\text{pre}}+\theta_{0})\right)\), where \(t\) and \(t_{\text{pre}}\) are the timestamps that \((v_{\text{pre}},v)\in\mathcal{E}^{(t)}\) and the timestamp of the previous sample, respectively. In this sampling procedure, smaller (resp. larger) \(\theta\) means less (resp. more) emphasis on recent timestamps. Each walk started from \(v\) can be seen as a temporal subgraph, and so we let \(\rho_{v}\) be the union of all these subgraphs (walks started from \(v\)). We treat each of \(\rho_{v}\) as a temporal patch.

**Temporal Pooling Mixer.** Given the temporal graph patches that we extracted above, we need to encode each patch to obtain patch encodings (we later use these patch encodings as their corresponding voxel's encodings). While simple poolings (e.g., \(\textsc{Sum}(.)\)) are shown to miss information [10], more complicated pooling functions consider a static pooling rule. However, as discussed above, the effect of performing a task on the neuroimaging data might last for a period of time and the pooling rule might change over time. To this end, we design a temporal pooling, \(\textsc{TPMixer}(.)\), that dynamically pools a set of voxels in a patch based on their timestamps.

Given a patch \(\rho_{v_{0}}=\{v_{0},v_{1},\ldots,v_{k}\}\), for each voxel we consider the correlation of its activity with other voxels' as its preliminary feature vector. That is, for each voxel \(v\), we consider its feature vector in the time window \(t\) as \(\mathcal{A}_{v}^{(t)}\), the \(v\)'s corresponding row in \(\mathcal{A}^{(t)}\). We abuse the notation and use \(\mathcal{A}_{\rho_{v}}^{(t)}\) to refer to the set of \(\mathcal{A}^{(t)}\)'s rows corresponding to \(\rho_{v}\). Since patch sizes are different, we zero pad \(\mathcal{A}_{\rho_{v}}^{(t)}\) matrices to a fixed size. Note that this zero padding is important to capture the size of each voxel neighborhood. The voxel with more zero-padded dimensions in its patch has less correlation with others. To capture both cross-feature and cross-voxel dependencies, we can use the same architecture as the Time Mixer and Voxel-Mixer. However, the main drawback of this approach is that a pooling function is expected to be permutation invariant while the Voxel Mixer phase is permutation variant. To address this challenge, we fuse information across features in a non-parametric manner as follows:

\[\mathbf{H}_{\text{F}}^{(t)}=\mathcal{A}_{\rho_{v}}^{(t)}+\sigma\left(\texttt{ Softmax}\left(\texttt{LayerNorm}\left(\mathcal{A}_{\rho_{v}}^{(t)}\right)^{ \top}\right)\right)^{\top}\in\mathbb{R}^{|\rho_{v}|\times d^{\prime}},\] (2)

where \(\sigma(.)\) is an activation function, \(\texttt{Softmax}(.)\) is used to normalize across features to bind and fuse feature-wise information in a non-parametric manner, avoiding permutation variant operations, and \(d^{\prime}\) is the feature vector size. To dynamically fuse information across voxels, we use the same idea as dynamic self-attention in SS3.1 and learn dynamic matrices \(\mathbf{P}_{\text{Pool}_{i}}\); let \(d_{\text{patch}}\) be the patch size:

\[\mathbf{P}_{\text{Pool}_{i}}=\text{Softmax}\left(\text{Flat}\left( \mathbf{H}_{\text{F}}^{(t)}\right)\mathbf{W}_{\text{Pool}}^{(i)}\right)\in \mathbb{R}^{1\times d^{\prime}}\] (3) \[\mathbf{h}_{\rho_{v}}=\text{Mean}\left(\text{Norm}(\mathbf{H}_{ \text{F}}^{(t)})+\mathbf{H}_{\text{PE}}^{(t)}\text{Softmax}\left(\frac{ \mathbf{H}_{\text{FE}}^{(t)}\,\mathbf{H}_{\text{PE}}^{(t)}}{\sqrt{d_{\text{ patch}}}}\right)\right)\in\mathbb{R}^{1\times d^{\prime}},\] (4)

where \(\mathbf{H}_{\text{PE}}^{(t)}=\mathbf{H}_{\text{F}}^{(t)}\mathbf{P}_{\text{ Pool}}\) is the transformation of \(\mathbf{H}_{\text{F}}^{(t)}\) by dynamic matrix \(\mathbf{P}_{\text{Pool}}\).

**Theorem 1**.: TPMixer _is permutation invariant and a universal approximator of multisets._

**Time Encoding.** To distinguish different timestamps in the functional connectivity graph, we use a non-learnable time encoding module proposed by Cong et al. [21]. This encoding approach helps reduce the number of parameters, and also it has been shown to be more stable and generalizable [21]. Given hyperparameters \(\alpha,\beta\), and \(d\), we use feature vector \(\bm{\omega}=\{\alpha^{-i/\beta}\}_{i=0}^{d-1}\) to encode each timestamp \(t\) using \(\cos\left(\bm{\omega}t\right)\) function. Therefore, we obtain the time encoding as \(\bm{\eta}_{t}=\cos\left(\bm{\omega}t\right)\).

**Voxel-, Edge-, and Graph-level Encodings.** Depending on the downstream task, we might obtain voxel-, edge-, or graph-level encodings. For each voxel \(v\in\mathcal{V}\), we let \(\mathcal{E}^{(t)}[\rho_{v}]\) be the set of connections in the patch of \(v\). To obtain the voxel-level encoding of each voxel \(v\), \(\bm{\psi}_{v}\), we use patch encoding and concatenate it with all the weighted mean of timestamp encodings; i.e., \(\bm{\psi}_{v}^{t}=\text{MLP}([\mathbf{h}_{\rho_{v}}\|\mathcal{T}_{v}])\), where \(\mathcal{T}_{v}=\frac{\sum_{t=0}^{t}\mathcal{E}^{(t)}[\rho_{v}]\bm{\eta}_{t}}{ \sum_{t=1}^{t}\mathcal{E}^{(t)}[\rho_{v}]}\). For a connection \(e=(u,v)\in\mathcal{E}^{(t)}\), we obtain its encoding by concatenating its endpoints and its timestamp encodings; i.e., \(\mathcal{C}_{(u,v)}^{(t)}=\text{MLP}\left([\bm{\psi}_{u}^{t},\bm{\psi}_{v}^{t},\bm{\eta}_{t}]\right)\). Finally, to obtain the graph level encoding, we use vanilla MLP-Mixer[82] on patch encodings; let \(\bm{\Psi}^{(t)}\) be the matrix whose rows are \(\bm{\psi}_{v}^{(t)}\):

\[\bm{\Psi}_{\text{patch}}^{(t)}=\bm{\Psi}^{(t)}+\mathbf{W}_{\text {patch}}^{(2)}\sigma\left(\mathbf{W}_{\text{patch}}^{(1)}\text{LayerNorm} \left(\bm{\Psi}^{(t)}\right)\right),\] (5) \[\text{Enc}(\mathcal{G}_{F}^{(t)})=\text{Mean}\left(\bm{\Psi}_{ \text{patch}}^{(t)}+\sigma\left(\text{LayerNorm}\left(\bm{\Psi}_{\text{patch}}^{(t )}\right)\mathbf{W}_{\text{channel}}^{(1)}\right)\mathbf{W}_{\text{channel}}^{( 2)}\right).\] (6)

Similar to the above, to obtain the brain-level encoding, \(\mathbf{Z}_{\mathbf{V}}^{(t)}\), based on voxel acitivity timeseries, we use MLP-Mixer on \(\mathbf{H}_{\text{Time}}^{(t)}\).

### Self-supervised Pre-training

In SS3.1 and SS3.2 we obtained the encodings of the same contexts, from different perspectives. In this section, inspired by [40; 5], we use the mutual information of these two perspectives from the same context, to learn voxel- and brain-level encodings in a self-supervised manner. To this end, let \(\bm{\Psi}\) be the voxel-level encodings obtained from functional connectome, \(\mathbf{Z}_{\text{F}}^{(t)}=\text{Enc}(\mathcal{G}_{F}^{(t)})\) be the global encoding (brain-level) of the functional connectome, \(\mathbf{H}_{\text{Voxel}}^{(t)}\) be the voxel activity encodings from the brain activity timeseries, and \(\mathbf{Z}_{\mathbf{V}}^{(t)}\) be the global encoding (brain-level) of the voxel activity timeseries, we aim to maximize \(I(\mathbf{Z}_{\mathbf{V}}^{(t)},\bm{\psi}_{v,i}^{(t)})+I(\mathbf{Z}_{\text{F}}^ {(t)},(\mathbf{H}_{\text{Time}}^{(t)})_{v,j})\) for all \(v\in\mathcal{V}\) and possible \(i,j\). Following previous studies [5], we use Noise-Contrastive Estimation (NCE) [34] and minimize the following loss function:

\[\mathbb{E}_{(\mathbf{Z}_{\mathbf{V}}^{(t)},\bm{\psi}_{v,i}^{(t)} )}\left[\mathbb{E}_{\mathcal{N}}\left[\mathcal{L}_{\Phi}(\mathbf{Z}_{\text{F}}^ {(t)},\bm{\psi}_{v,i}^{(t)},\mathcal{N})\right]\right]+\mathbb{E}_{(\mathbf{ Z}_{\mathbf{V}}^{(t)},(\mathbf{H}_{\text{Voxel}}^{(t)})_{v,j})}\left[ \mathbb{E}_{\mathcal{N}}\left[\mathcal{L}_{\Phi}(\mathbf{Z}_{\mathbf{V}}^{(t)},(\mathbf{H}_{\text{Voxel}}^{(t)})_{v,j},\mathcal{N})\right]\right],\] (7)

where \(\mathcal{N}\) is the set of negative samples, \((\mathbf{Z}_{\mathbf{V}}^{(t)},\bm{\psi}_{v,i}^{(t)})\) and \((\mathbf{Z}_{\text{F}}^{(t)},(\mathbf{H}_{\text{Voxel}}^{(t)})_{v,j})\) are the positive sample pairs, and \(\mathcal{L}_{\Phi}\) is a standard Log-Softmax.

## 4 Experiments

**Dataset.** We use six real-world datasets: 1 We present Bvfc, a task-based fMRI dataset that includes voxel activity timeseries and functional connectivity of 3 subjects when looking at the 8460images from 720 categories. This data is based on THINGS dataset [37]. 2Bvfc-MEG is the MEG counterpart of Bvfc. 3 ADHD [63] contains data for 250 subjects in the ADHD group and 450 subjects in the typically developed (TD) control group. 4 The Seizure detection TUH-EEG dataset [78] consists of EEG data (31 channels) of 642 subjects. 5 ASD [23] contains data for 45 subjects in the ASD group and 45 subjects in the TD group. 6 HCP [87] contains data from 7440 neuroimaging samples each of which is associated with one of the seven ground-truth mental states.

**Evaluation Tasks.** In our experiments we focus on 4 downstream tasks: 1 Edge-Anomaly Detection (AD), 2 Voxel AD, 3 Brain AD, and 4 Brain Classification. For the edge and voxel AD tasks, we follow previous studies [8, 59], and inject 1% and 5% anomalous edges into the functional connectivity in the control group. For brain AD all datasets has ground-truth anomalies (see Appendix E.2). The ground truth anomalies in Bvfc are the brain responses to not recognizable images, generated by BigGAN [12], and for other datasets are brain activity of people living with ADHD, seizure, and ASD. For brain classification, we focus on the prediction of 1 categories of images seen by the subjects (in Bvfc, and Bvfc-MEG), and 2 age prediction and mental state decoding (in HCP-Age, and HCP-Mental). The details of the setup are in Appendix E.

**Baselines.** We compare BrainMixer with state-of-the-art time series, graph, and brain anomaly detection and learning models: 1 Graph-based methods: GOutlier [1], NetWalk [100], HyperSAGCN [104], Graph MLP-Mixer (GMM) [35], GraphMixer[21]. 2 brain-network-based methods: BrainGnn [56], FbNetGen [46], BrainNetCnn [49], ADMire [9], and BNTransformer [47], PTGB [99]. 3 Time-series-based methods: Usad [2], Time Series Transformer (TST) [103], and Mvts [69]. We may exclude some baselines in some tasks as they cannot be applied in that setting. We use the same training procedure as BrainMixer. The details are in Appendix E.1.

**Brain Classification.** Table 1 reports the performance of BrainMixer and baselines on multi-class brain classification tasks. BrainMixer achieves the best accuracy on all datasets with 14.3% average improvement (\(20.3\%\) best improvement) over the best baseline. There are three main reasons for BrainMixer's superior performance: 1 While the time series-based model only uses voxel activity timeseries, and graph-based methods only use functional connectivity graph, BrainMixer takes advantage of both and learns the brain representation at different levels of granularity, which can provide complementary information. 2 Static methods (e.g., PTGB, BrainGnn, etc.), miss the dynamics of brain activity, while BrainMixer employs a time encoding module to learn temporal properties. 3 Compared to graph learning methods (e.g., GMM, GraphMixer, etc.), BrainMixer is specifically designed for the brain, taking advantage of its special properties.

**Anomaly Detection.** Table 2 reports the performance of BrainMixer and baselines on anomaly detection tasks at different scales: i.e., edge-, voxel-, and brain-level. BrainMixer achieves the best AUC-PR on all datasets with 6.2%, 5.7%, 4.81% average improvement over the best baseline in edge AD, voxel AD, and brain AD, respectively. The main reasons for this superior performance are as above. Note that brain-level anomaly detection can also be seen as a brain classification task. However, here, based on the nature of the data, we separate these two tasks.

**How Does BrainMixer Detect GAN Generated Images?** The visual cortex, responsible for processing visual information, is hierarchically organized with multiple layers building upon simpler features at lower stages [86]. Initially, neurons detect edges and colors, but on deeper levels, they specialize in recognizing more complex patterns and objects. Figure 2 (Left) (resp. (Right)) reports the distribution of detected voxel activity by BrainMixer when the subject looking at non-recognizable images (resp. natural images). Interestingly, while the distributions share similar patterns in lower

\begin{table}
\begin{tabular}{l c c c c} \hline Methods & Bvfc & Bvfc-MEG & HCP-Mental & HCP-Age \\ \hline \hline Usad & \(48.52_{1.94}\) & \(50.02_{11.13}\) & \(73.49_{15.36}\) & \(39.17_{14.16}\) \\ HypensAGCN & \(51.92_{1.47}\) & \(11.59_{18.93}\) & \(90.74_{11.61}\) & \(47.38_{1.19}\) \\ GMM & \(53.11_{1.14}\) & \(53.04_{12.79}\) & \(90.92_{13.45}\) & \(47.45_{11.20}\) \\ GraphMixer & \(53.71_{12.52}\) & \(53.12_{11.93}\) & \(91.31_{13.14}\) & \(48.32_{11.11}\) \\ BrainNetCN & \(49.10_{13.05}\) & \(50.12_{11.92}\) & \(53.85_{11.48}\) & \(42.26_{12.62}\) \\ BransGNN & \(50.63_{1.25}\) & \(50.63_{1.08}\) & \(50.52_{12.97}\) & \(41.08_{1.54}\) \\ PenFrien & \(51.80_{1.09}\) & \(50.94_{12.91}\) & \(54.74_{12.88}\) & \(42.82_{12.9}\) \\ ADMire & \(54.36_{1.99}\) & \(58.45_{11.92}\) & \(98.74_{11.93}\) & \(47.82_{11.72}\) \\ PTGB & \(55.95_{11.15}\) & \(55.11_{12.02}\) & \(92.51_{11.31}\) & \(48.41_{11.47}\) \\ BNTransformer & \(55.03_{1.38}\) & \(55.11_{11.74}\) & \(91.71_{11.14}\) & \(47.94_{11.11}\) \\ BrainMixer & \(67.24_{1.47}\) & \(62.58_{1.08}\) & \(96.32_{1.09}\) & \(57.83_{1.09}\) \\ \hline \end{tabular}
\end{table}
Table 1: Performance on multi-class brain classification: Mean ACC (%) \(\pm\) standard deviation.

[MISSING_PAGE_FAIL:8]

## References

* Aggarwal et al. [2011] Charu C. Aggarwal, Yuchen Zhao, and Philip S. Yu. Outlier detection in graph streams. In _2011 IEEE 27th International Conference on Data Engineering_, pp. 399-409, 2011. doi: 10.1109/ICDE.2011.5767885.
* Audibert et al. [2020] Julien Audibert, Pietro Michiardi, Frederic Guyard, Sebastien Marti, and Maria A Zuluaga. Usad: Unsupervised anomaly detection on multivariate time series. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pp. 3395-3404, 2020.
* Avbersek and Repovs [2022] Lev Kiar Avbersek and Grega Repovs. Deep learning in neuroimaging data analysis: applications, challenges, and solutions. _Frontiers in neuroimaging_, 1:981642, 2022.
* Ba et al. [2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.
* Bachman et al. [2019] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf.
* Banville et al. [2021] Hubert Banville, Omar Chehab, Aapo Hyvarinen, Denis-Alexander Engemann, and Alexandre Gramfort. Uncovering the structure of clinical eeg signals with self-supervised learning. _Journal of Neural Engineering_, 18(4):046020, 2021.
* Behrouz and Seltzer [2022] Ali Behrouz and Margo Seltzer. Anomaly detection in multiplex dynamic networks: from blockchain security to brain disease prediction. In _NeurIPS 2022 Temporal Graph Learning Workshop_, 2022. URL https://openreview.net/forum?id=UDGZDfumay.
* Behrouz and Seltzer [2023] Ali Behrouz and Margo Seltzer. Anomaly detection in human brain via inductive learning on temporal multiplex networks. In _Machine Learning for Healthcare Conference_, volume 219. PMLR, 2023.
* Behrouz and Seltzer [2023] Ali Behrouz and Margo Seltzer. ADMIRE++: Explainable anomaly detection in the human brain via inductive learning on temporal multiplex networks. In _ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)_, 2023. URL https://openreview.net/forum?id=t4H8acYudJ.
* Behrouz et al. [2023] Ali Behrouz, Farnoosh Hashemi, Sadaf Sadeghian, and Margo Seltzer. CAT-walk: Inductive hypergraph learning via set walks. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=Q64nJBNEar.
* Bray et al. [2009] Signe L Bray, Catie Chang, and Fumiko Hoeft. Applications of multivariate pattern classification analyses in developmental neuroimaging of healthy and clinical populations. _Frontiers in human neuroscience_, 3:898, 2009.
* Brock et al. [2019] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=B1xsqj09Fm.
* Cai et al. [2023] Donghong Cai, Junru Chen, Yang Yang, Teng Liu, and Yafeng Li. Mbrain: A multi-channel self-supervised learning framework for brain signals. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '23, pp. 130-141, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599426. URL https://doi.org/10.1145/3580305.3599426.
* Calhoun et al. [2014] Vince D. Calhoun, Robyn Miller, Godfrey Pearlson, and Tulay Adali. The chronnectome: Time-varying connectivity networks as the next frontier in fmri data discovery. _Neuron_, 84(2):262-274, 2014. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2014.10.015. URL https://www.sciencedirect.com/science/article/pii/S0896627314009131.
* Chadwick et al. [2012] Martin J Chadwick, Heidi M Bonnici, and Eleanor A Maguire. Decoding information in the human hippocampus: a user's guide. _Neuropsychologia_, 50(13):3107-3121, 2012.

* [16] Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca, Thomas Markovich, Nils Yannick Hammerla, Michael M. Bronstein, and Max Hansmire. Graph neural networks for link prediction with subgraph sketching. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=m1oqEQAo2U.
* [17] Tanima Chatterjee, Reka Albert, Stuti Thapliyal, Nazanin Azarhooshang, and Bhaskar DasGupta. Detecting network anomalies using forman-ricci curvature and a case study for human brain networks. _Scientific Reports_, 11(1):8121, Apr 2021. ISSN 2045-2322. doi: 10.1038/s41598-021-87587-z. URL https://doi.org/10.1038/s41598-021-87587-z.
* [18] Gang Chen, B Douglas Ward, Chunming Xie, Wenjun Li, Zhilin Wu, Jennifer L Jones, Malgorzata Franczak, Piero Antuono, and Shi-Jiang Li. Classification of alzheimer disease, mild cognitive impairment, and normal cognitive status with large-scale network analysis based on resting-state functional mr imaging. _Radiology_, 259(1):213, 2011.
* [19] Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O Arik, and Tomas Pfister. Tsmixer: An all-mlp architecture for time series forecasting. _arXiv preprint arXiv:2303.06053_, 2023.
* [20] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.
* [21] Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang Tong, and Mehrdad Mahdavi. Do we really need complicated model architectures for temporal networks? In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=ayPPc0Sylv1.
* [22] Aurelio Cortese, Saori C Tanaka, Kaoru Amano, Ai Koizumi, Hakwan Lau, Yuka Sasaki, Kazuhisa Shibata, Vincent Taschereau-Dumouchel, Takeo Watanabe, and Mitsuo Kawato. The decenr collection, fmri data from closed-loop decoded neurofeedback experiments. _Scientific data_, 8(1):65, 2021.
* [23] Cameron Craddock, Yassine Benhajali, Carlton Chu, Francois Chouinard, Alan Evans, Andras Jakab, Budhachandra Singh Khundrakpam, John David Lewis, Qingyang Li, Michael Milham, et al. The neuro bureau preprocessing initiative: open sharing of preprocessed neuroimaging data and derivatives. _Frontiers in Neuroinformatics_, 7:27, 2013.
* [24] Alexander Craik, Yongtian He, and Jose L Contreras-Vidal. Deep learning for electroencephalogram (eeg) classification tasks: a review. _Journal of neural engineering_, 16(3):031001, 2019.
* [25] Hejie Cui, Wei Dai, Yanqiao Zhu, Xuan Kan, Antonio Aodong Chen Gu, Joshua Lukemire, Liang Zhan, Lifang He, Ying Guo, and Carl Yang. BrainGB: A Benchmark for Brain Network Analysis with Graph Neural Networks. _IEEE Transactions on Medical Imaging (TMI)_, 2022.
* [26] Hejie Cui, Wei Dai, Yanqiao Zhu, Xiaoxiao Li, Lifang He, and Carl Yang. Interpretable graph neural networks for connectome-based brain disorder analysis. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pp. 375-385. Springer, 2022.
* [27] Fernando Lopes da Silva. Neural mechanisms underlying brain waves: from neural membranes to networks. _Electroencephalography and clinical neurophysiology_, 79(2):81-93, 1991.
* [28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.
* [29] Yuhui Du, Zening Fu, and Vince D Calhoun. Classification and prediction of brain disorders using functional connectivity: promising but challenging. _Frontiers in neuroscience_, 12:525, 2018.

* Gao et al. [2015] James S. Gao, Alexander G. Huth, Mark D. Lescroart, and Jack L. Gallant. Pycortex: an interactive surface visualizer for fmri. _Frontiers in Neuroinformatics_, 9, 2015. ISSN 1662-5196. doi: 10.3389/fninf.2015.00023. URL https://www.frontiersin.org/articles/10.3389/fninf.2015.00023.
* Geenjaar et al. [2022] Eloy Geenjaar, Amrit Kashyap, Noah Lewis, Robyn Miller, and Vince Calhoun. Spatio-temporally separable non-linear latent factor learning: an application to somatomotor cortex fmri data. _arXiv preprint arXiv:2205.13640_, 2022.
* Gonzalez-Castillo and Bandettini [2018] Javier Gonzalez-Castillo and Peter A Bandettini. Task-based dynamic functional connectivity: Recent findings and open questions. _Neuroimage_, 180:526-533, 2018.
* Greicius [2008] Michael Greicius. Resting-state functional connectivity in neuropsychiatric disorders. _Current opinion in neurology_, 21(4):424-430, 2008.
* Gutmann and Hyvarinen [2010] Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pp. 297-304. JMLR Workshop and Conference Proceedings, 2010.
* He et al. [2023] Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A generalization of vit/mlp-mixer to graphs. In _International Conference on Machine Learning_, pp. 12724-12745. PMLR, 2023.
* Hebart et al. [2019] Martin N. Hebart, Adam H. Dickter, Alexis Kidder, Wan Y. Kwok, Anna Corriveau, Caitlin Van Wicklin, and Chris I. Baker. Things: A database of 1,854 object concepts and more than 26,000 naturalistic object images. _PLOS ONE_, 14(10):1-24, 10 2019. doi: 10.1371/journal. pone.0223792. URL https://doi.org/10.1371/journal. pone.0223792.
* Hebart et al. [2023] Martin N Hebart, Oliver Contier, Lina Teichmann, Adam H Rockter, Charles Y Zheng, Alexis Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris I Baker. Things-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior. _Elife_, 12:e82580, 2023.
* Hendrycks and Gimpel [2020] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2020.
* Hernandez et al. [2015] Leanna M Hernandez, Jeffrey D Rudie, Shulamite A Green, Susan Bookheimer, and Mirella Dapretto. Neural signatures of autism spectrum disorders: insights into brain network dynamics. _Neuropsychopharmacology_, 40(1):171-189, 2015.
* Hjelm et al. [2019] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=Bklr3j0cKX.
* Horikawa and Kamitani [2017] Tomoyasu Horikawa and Yukiyasu Kamitani. Generic decoding of seen and imagined objects using hierarchical visual features. _Nature communications_, 8(1):15037, 2017.
* Hu et al. [2021] Yang Hu, Haoxuan You, Zhecan Wang, Zhicheng Wang, Erjin Zhou, and Yue Gao. Graph-mlp: Node classification without message passing in graph. _arXiv preprint arXiv:2106.04051_, 2021.
* Hutchison et al. [2013] R Matthew Hutchison, Thilo Womelsdorf, Elena A Allen, Peter A Bandettini, Vince D Calhoun, Maurizio Corbetta, Stefania Della Penna, Jeff H Duyn, Gary H Glover, Javier Gonzalez-Castillo, et al. Dynamic functional connectivity: promise, issues, and interpretations. _Neuroimage_, 80:360-378, 2013.
* Jiang et al. [2021] Yuli Jiang, Yu Rong, Hong Cheng, Xin Huang, Kangfei Zhao, and Junzhou Huang. Query driven-graph neural networks for community search: From non-attributed, attributed, to interactive attributed, 2021. URL https://arxiv.org/abs/2104.03583.
* Jie et al. [2016] Biao Jie, Mingxia Liu, Xi Jiang, and Daoqiang Zhang. Sub-network based kernels for brain network classification. In _Proceedings of the 7th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics_, pp. 622-629, 2016.

* Kan et al. [2022] Xuan Kan, Hejie Cui, Joshua Lukemire, Ying Guo, and Carl Yang. Fbnetgen: Task-aware gnn-based fmri analysis via functional brain network generation. In _International Conference on Medical Imaging with Deep Learning_, pp. 618-637. PMLR, 2022.
* Kan et al. [2022] Xuan Kan, Wei Dai, Hejie Cui, Zilong Zhang, Ying Guo, and Carl Yang. Brain network transformer. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=1cJ1cbA6NLN.
* Karypis and Kumar [1998] George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. _SIAM Journal on Scientific Computing_, 20(1):359-392, 1998. doi: 10.1137/S1064827595287997. URL https://doi.org/10.1137/S1064827595287997.
* Kawahara et al. [2017] Jeremy Kawahara, Colin J Brown, Steven P Miller, Brian G Booth, Vann Chau, Ruth E Grunau, Jill G Zwicker, and Ghassan Hamarneh. Brainnetcnn: Convolutional neural networks for brain networks; towards predicting neurodevelopment. _NeuroImage_, 146:1038-1049, 2017.
* Khan et al. [2022] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. _ACM computing surveys (CSUR)_, 54(10s):1-41, 2022.
* Kostas et al. [2021] Demetres Kostas, Stephane Aroca-Ouellette, and Frank Rudzicz. Bendr: using transformers and a contrastive self-supervised learning task to learn from massive amounts of eeg data. _Frontiers in Human Neuroscience_, 15:653659, 2021.
* Lanciano et al. [2020] Tommaso Lanciano, Francesco Bonchi, and Aristides Gionis. Explainable classification of brain networks via contrast subgraphs. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining_, KDD '20, pp. 3308-3318, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3403383. URL https://doi.org/10.1145/3394486.3403383.
* Lee and Baker [2016] Sue-Hyun Lee and Chris I Baker. Multi-voxel decoding and the topography of maintained information during visual working memory. _Frontiers in systems neuroscience_, 10:2, 2016.
* Lei et al. [2014] Du Lei, Jun Ma, Xiaoxia Du, Guohua Shen, Xingming Jin, and Qiyong Gong. Microstructural abnormalities in the combined and inattentive subtypes of attention deficit hyperactivity disorder: a diffusion tensor imaging study. _Scientific reports_, 4(1):6875, 2014.
* Li et al. [2019] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. _Advances in neural information processing systems_, 32, 2019.
* Li et al. [2021] Xiaoxiao Li, Yuan Zhou, Nicha Dvornek, Muhan Zhang, Siyuan Gao, Juntang Zhuang, Dustin Scheinost, Lawrence H Staib, Pamela Ventola, and James S Duncan. Braingnn: Interpretable brain graph neural network for fmri analysis. _Medical Image Analysis_, 74:102233, 2021.
* Li et al. [2023] Zhe Li, Zhongwen Rao, Lujia Pan, and Zenglin Xu. Mts-mixers: Multivariate time series forecasting via factorized temporal and channel mixing. _arXiv preprint arXiv:2302.04501_, 2023.
* Liu et al. [2021] Hanxiao Liu, Zihang Dai, David So, and Quoc V Le. Pay attention to mlps. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 9204-9215. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/4cc05b35c2f937c5bd9e7d41d3686ffff-Paper.pdf.
* Ma et al. [2021] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z. Sheng, Hui Xiong, and Leman Akoglu. A comprehensive survey on graph anomaly detection with deep learning. _IEEE Transactions on Knowledge and Data Engineering_, pp. 1-1, 2021. doi: 10.1109/TKDE.2021.3118815.
* Mahmoudi et al. [2012] Abdelhak Mahmoudi, Sylvain Takerkart, Fakhita Regragui, Driss Boussaoud, Andrea Brovelli, et al. Multivoxel pattern analysis for fmri data: a review. _Computational and mathematical methods in medicine_, 2012, 2012.

* [61] Razvan Marinescu, Arman Eshaghi, Daniel Alexander, and Polina Golland. Brainpainter: A software for the visualisation of brain structures, biomarkers and associated pathological processes. _arXiv preprint arXiv:1905.08627_, 2019.
* [62] Chris McNorgan, Gregory J Smith, and Erica S Edwards. Integrating functional connectivity and mvpa through a multiple constraint network analysis. _Neuroimage_, 208:116412, 2020.
* [63] Michael P. Milham, Jan Buitelaar, F. Xavier Castellanos, Daniel Dickstein, Damien Fair, David Kennedy, Beatric Luna, Michael P. Milham, Stewart Mostofsky, Joel Nigg, Julie B. Schweitzer, Katerina Velanova, Yu-Feng Wang, and Yu-Feng Zang. 1000 functional connectome project. _1000 Functional Connectome Project_, 1, July 2011.
* [64] Tom M Mitchell, Rebecca Hutchinson, Marcel Adam Just, Radu S Niculescu, Francisco Pereira, and Xuerui Wang. Classifying instantaneous cognitive states from fmri data. In _AMIA annual symposium proceedings_, volume 2003, pp. 465. American Medical Informatics Association, 2003.
* [65] Mostafa Neo Mohsenvand, Mohammad Rasool Izadi, and Pattie Maes. Contrastive representation learning for electroencephalogram classification. In _Machine Learning for Health_, pp. 238-253. PMLR, 2020.
* [66] Alfonso Nieto-Castanon. Brain-wide connectome inferences using functional connectivity multivariate pattern analyses (fc-mvpa). _PLoS Computational Biology_, 18(11):e1010634, 2022.
* [67] Youngser Park, C Priebe, D Marchette, and Abdou Youssef. Anomaly detection using scan statistics on time series hypergraphs. In _Link Analysis, Counterterrorism and Security (LACTS) Conference_, pp. 9. SIAM Pennsylvania, 2009.
* [68] Russell A Poldrack and Krzysztof J Gorgolewski. Making big data open: data sharing in neuroimaging. _Nature neuroscience_, 17(11):1510-1517, 2014.
* [69] likay Yildiz Potter, George Zerveas, Carsten Eickhoff, and Dominique Duncan. Unsupervised multivariate time-series transformers for seizure identification on eeg. In _2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)_, pp. 1304-1311. IEEE, 2022.
* [70] Raimon HR Pruim, Maarten Mennes, Daan van Rooij, Alberto Llera, Jan K Buitelaar, and Christian F Beckmann. Ica-aroma: A robust ica-based strategy for removing motion artifacts from fmri data. _Neuroimage_, 112:267-277, 2015.
* [71] Tiffany D Rogers, Eric McKimm, Price E Dickson, Dan Goldowitz, Charles D Blaha, and Guy Mittleman. Is autism a disease of the cerebellum? an integration of clinical and pre-clinical research. _Front Syst Neurosci_, 7:15, May 2013.
* [72] Zvi N. Roth and Elisha P. Merriam. Representations in human primary visual cortex drift over time. _Nature Communications_, 14(1):4422, Jul 2023. ISSN 2041-1723. doi: 10.1038/s41467-023-40144-w. URL https://doi.org/10.1038/s41467-023-40144-w.
* [73] Zvi N. Roth, Kendrick Kay, and Elisha P. Merriam. Natural scene sampling reveals reliable coarse-scale orientation tuning in human v1. _Nature Communications_, 13(1):6469, Oct 2022. ISSN 2041-1723. doi: 10.1038/s41467-022-34134-7. URL https://doi.org/10.1038/s41467-022-34134-7.
* [74] Anwar Said, Roza G Bayrak, Tyler Derr, Mudassir Shabbir, Daniel Moyer, Catie Chang, and Xenofon D. Koutsoukos. Neurograph: Benchmarks for graph machine learning in brain connectomics. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023. URL https://openreview.net/forum?id=MEAoCQeURw.
* [75] Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, Tyler Derr, and Rajiv Ratn Shah. Stock selection via spatiotemporal hypergraph attention network: A learning to rank approach. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 497-504, 2021.

* Schaefer et al. [2018] Alexander Schaefer, Ru Kong, Evan M Gordon, Timothy O Laumann, Xi-Nian Zuo, Avram J Holmes, Simon B Eickhoff, and B T Thomas Yeo. Local-Global parcellation of the human cerebral cortex from intrinsic functional connectivity MRI. _Cereb Cortex_, 28(9):3095-3114, September 2018.
* Schneider et al. [2023] Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt Mathis. Learnable latent embeddings for joint behavioural and neural analysis. _Nature_, pp. 1-9, 2023.
* Shah et al. [2018] Vinit Shah, Eva Von Weltin, Silvia Lopez, James Riley McHugh, Lillian Veloso, Meysam Golmohammadi, Iyad Obeid, and Joseph Picone. The temple university hospital seizure detection corpus. _Frontiers in neuroinformatics_, 12:83, 2018.
* Shoeibi et al. [2021] Afshin Shoeibi, Marjane Khodatars, Navid Ghassemi, Mahboobeh Jafari, Parisa Moridian, Roohallah Alizadehsani, Maryam Panahiazar, Fahime Khozeimeh, Assef Zare, Hossein Hosseini-Nejad, et al. Epileptic seizures detection using deep learning techniques: A review. _International Journal of Environmental Research and Public Health_, 18(11):5780, 2021.
* Smith et al. [2013] Stephen M Smith, Diego Vidaurre, Christian F Beckmann, Matthew F Glasser, Mark Jenkinson, Karla L Miller, Thomas E Nichols, Emma C Robinson, Gholamreza Salimi-Khorshidi, Mark W Woolrich, Deanna M Barch, Kamil Ugurbil, and David C Van Essen. Functional connectomics from resting-state fMRI. _Trends Cogn Sci_, 17(12):666-682, November 2013.
* Sundermann et al. [2014] B Sundermann, D Herr, W Schwindt, and B Pfleiderer. Multivariate classification of blood oxygen level-dependent fmri data with diagnostic intention: a clinical perspective. _American Journal of neuroradiology_, 35(5):848-855, 2014.
* Tolstikhin et al. [2021] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Peter Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. MLP-mixer: An all-MLP architecture for vision. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=EI2K0XKdnP.
* Trockman and Kolter [2023] Asher Trockman and J Zico Kolter. Patches are all you need? _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=raB7JSMXL. Featured Certification.
* Uddin et al. [2017] Lucina Q Uddin, DR Dajani, W Voorhies, H Bednarz, and RK Kana. Progress and roadblocks in the search for brain-based biomarkers of autism and attention-deficit/hyperactivity disorder. _Translational psychiatry_, 7(8):e1218-e1218, 2017.
* Van Den Heuvel and Pols [2010] Martijn P Van Den Heuvel and Hilleke E Hulshoff Pol. Exploring the brain network: a review on resting-state fmri functional connectivity. _European neuropsychopharmacology_, 20(8):519-534, 2010.
* Van Essen and Maunsell [1983] David C Van Essen and John HR Maunsell. Hierarchical organization and functional streams in the visual cortex. _Trends in neurosciences_, 6:370-375, 1983.
* Van Essen et al. [2020] David C Van Essen, Stephen M Smith, Deanna M Barch, Timothy EJ Behrens, Essa Yacoub, Kamil Ugurbil, Wu-Minn HCP Consortium, et al. The wu-minh human connectome project: an overview. _Neuroimage_, 80:62-79, 2013.
* Vassena et al. [2020] Eliana Vassena, James Deraeve, and William H. Alexander. Surprise, value and control in anterior cingulate cortex during speeded decision-making. _Nature Human Behaviour_, 4(4):412-422, Apr 2020. ISSN 2397-3374. doi: 10.1038/s41562-019-0801-5. URL https://doi.org/10.1038/s41562-019-0801-5.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Velickovic et al. [2018] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.

* Velickovic et al. [2019] Petar Velickovic, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=rkl29iAcKQ.
* Wang et al. [2021] Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. Inductive representation learning in temporal networks via causal anonymous walks. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=KYPz4YsCPj.
* Wee et al. [2011] Chong-Yaw Wee, Pew-Thian Yap, Wenbin Li, Kevin Denny, Jeffrey N Browndyke, Guy G Potter, Kathleen A Welsh-Bohmer, Lihong Wang, and Dinggang Shen. Enriched white matter connectivity networks for accurate identification of mci patients. _Neuroimage_, 54(3):1812-1822, 2011.
* Woo et al. [2022] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=P1iZY3omXV2.
* Wu et al. [2021] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. _Advances in Neural Information Processing Systems_, 34:22419-22430, 2021.
* Xu et al. [2023] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 7754-7765, 2023.
* Yamins and DiCarlo [2016] Daniel LK Yamins and James J DiCarlo. Using goal-driven deep learning models to understand sensory cortex. _Nature neuroscience_, 19(3):356-365, 2016.
* Yang et al. [2023] Huzheng Yang, James Gee, and Jianbo Shi. Memory encoding model, 2023.
* Yang et al. [2023] Yi Yang, Hejie Cui, and Carl Yang. \(\backslash\)ours: Pre-train graph neural networks for brain network analysis. In _Conference on Health, Inference, and Learning_, pp. 526-544. PMLR, 2023.
* Yu et al. [2018] Wenchao Yu, Wei Cheng, Charu C. Aggarwal, Kai Zhang, Haifeng Chen, and Wei Wang. Netwalk: A flexible deep embedding approach for anomaly detection in dynamic networks. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \(\&\) Data Mining_, KDD '18, pp. 2672-2681, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450355520. doi: 10.1145/3219819.3220024. URL https://doi.org/10.1145/3219819.3220024.
* Yuan et al. [2023] Zhizhang Yuan, Daoze Zhang, Yang Yang, Junru Chen, and Yafeng Li. PPi: Pretraining brain signal model for patient-independent seizure detection. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=tEmFyqjaJh.
* Yue et al. [2022] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pp. 8980-8987, 2022.
* Zerveas et al. [2021] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In _Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining_, pp. 2114-2124, 2021.
* Zhang et al. [2020] Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-sagnn: a self-attention based graph neural network for hypergraphs. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=ryeHuJbPtH.
* Zhou et al. [2021] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pp. 11106-11115, 2021.

* [106] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. Film: Frequency improved legendre memory model for long-term time series forecasting. _Advances in Neural Information Processing Systems_, 35:12677-12690, 2022.
* [107] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _International Conference on Machine Learning_, pp. 27268-27286. PMLR, 2022.
* [108] Yanqiao Zhu, Hejie Cui, Lifang He, Lichao Sun, and Carl Yang. Joint embedding of structural and functional brain networks with graph neural networks for mental illness diagnosis. In _2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)_, pp. 272-276. IEEE, 2022.

## Appendix

### Table of Contents

* 1 Backgrounds 1.1 Graphs and Machine Learning on Graphs 1.2 Voxel Time Series Activity 1.3 Brain Functional Connectivity 1.4 MLP-Mixer
* 2 B byc Dataset 2.1 Things Dataset 2.2 Preprocessing 2.3 Image Classification 2.4 Anomaly Detection
* 3 Additional Related Work 3.1 Our Contributions
* 4 Theoretical Guarantee of TSetMixer Performance
* 5 Experimental Setup 5.1 Baselines 5.2 Datasets 5.3 Implementation, Training, and Hyperparameters Tuning 5.4 Injecting Synthetic Anomalies 5.5 Visualization Tools
* 6 Additional Experimental Results 6.1 Parameter Sensitivity 6.2 The Effect of Functional and Temporal Patching 6.3 Performance Comparison using Different Metrics 6.4 Graph Regression 6.5 Qualitative Results 6.6 How Does BrainMixer Detect GAN Generated Images? 6.7 Case Study: ASD 6.8 The Effect of Objective 6.9 Number of Parameters 6.10 Effect of the Number of Parameters on Accuracy
* 7 Limitations and Future WorkBackgrounds

We begin by reviewing the preliminaries and background concepts that we refer to in the main paper.

### Graphs and Machine Learning on Graphs

**Temporal Graphs.** We first define the concept of temporal graphs, which are graphs such that each connection is associated with a timestamp. We formally define temporal graph as follows:

**Definition 1** (Temporal Graphs).: _Let \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{T})\) be a temporal graph, where \(\mathcal{V}\) is the set of nodes, \(\mathcal{T}\) is the set of timestamps, and \(\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}\times\mathcal{T}\) is the set of edges. That is, each connection between two nodes \((u,v)\) is associated with a timestamp like \(t\in\mathcal{T}\)._

In this study, we use snapshot based representation of temporal graphs. That is, \(\mathcal{G}=\{\mathcal{G}^{(t)}\}\) is the set of graphs, where \(\mathcal{G}^{(t)}=(\mathcal{V},\mathcal{E}^{(t)})\) represents the state of the graph \(\mathcal{G}\) at timestamp \(t\).

**Node Representation Learning.** Node representation learning in graphs is a process that aim to map nodes of a graph into a vector space. This representation seeks to capture the structure of the graph, the features of the nodes, their dynamic over time, and their relationships. The core idea is to represent each node with a vector that encapsulates not just its own attributes but also its position, dynamics, and role within the larger graph structure.

**Definition 2** (Node Representation Learning).: _Let \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) be a graph with nodes \(\mathcal{V}\) and edges \(\mathcal{E}\). The goal is to learn a function \(f:\mathcal{V}\rightarrow\mathbb{R}^{d}\), where \(d\) is the dimension of the target vector space (usually \(d\ll|V|\), implying a lower-dimensional representation)._

### Voxel Time Series Activity

Neuroimaging modalities (e.g., fMRI, MEG) provide (estimated) recordings of neural activity signals. To this end, their estimation is built up in a 3-D image building block, units called voxels, which represent a small cube of brain tissue. For example, fMRI measures the blood-oxygen level dependence (BOLD) of each voxel in order to estimate the neural activity of the whole brain over time. In the literature, for each voxel, most studies aggregate its activity (e.g., its BOLD) over each time window, called beta weight [73, 88, 72]. However, this approach misses the voxel activity dynamic over each task. In this study, we consider voxels' activity as it is (without aggregation) and model it as timeseries data. We model this data as a multivariate time series: An fMRI scan involves thousands of voxels, leading to a multivariate time series \(\{X_{1}(t),X_{2}(t),...,X_{n}(t)\}\) where \(X_{i}(t)\) is the time series for the \(i\)-th voxel.

### Brain Functional Connectivity

The brain's functional connectivity is a graph, derived from a neuroimaging modality (often fMRI), where each node represents a brain parcel or ROI, and two nodes are connected if there is a statistical association between their functionality. In more details, as discussed above, fMRI measures brain activity by detecting changes in blood flow. The primary data from fMRI is the Blood Oxygen Level Dependent (BOLD) signal, reflecting changes in the oxygenation level of the blood. Deriving a brain network from fMRI data involves 1 preprocessing, 2 parcellation using atlases, and 3 computing correlations.

1. Preprocessing: Is the sequence of actions to clean the data and make it for process. Common preprocessing techniques are: * **Motion Correction:** Aligns all the neuroimages to a reference neuroimage to correct for patient movement. * **Band-Pass Filtering:** Isolating the frequency band that corresponds to the fMRI signal (usually 0.01 to 0.1 Hz). * **Slice Timing Correction:** Adjusts for the time difference in image acquisition between slices. * **Smoothing:** Applies a Gaussian filter to reduce spatial noise and improve signal-to-noise ratio.

2. Parcellation using Atlases: Brain atlases divide the brain into regions of interest (ROIs). Each ROI represents a node in the brain network. Common atlases include: * **AAL (Automated Anatomical Labeling):** Divides the brain into areas based on anatomical structures. * **Harvard-Oxford Atlas:** Based on probabilistic information from a large population. * **Functional Atlases:** Based on functional connectivity patterns, e.g., resting-state networks.
3. Computing Correlations: For each ROI defined by the atlas, the time series of the fMRI signal is extracted. This step involves averaging the fMRI signal over all voxels within each ROI (Note that in this paper, we argue that the best case is to consider each voxel as an ROI). Next, to capture the statistical association of ROIs activity, we compute the Pearson correlation coefficient between the time series of every pair of ROIs: \[\mathbf{C}_{ij}=\frac{\sum_{k=1}^{T}(X_{i}(k)-\bar{X}_{i})(X_{j}(k)-\bar{X}_{j })}{\sqrt{\left(\sum_{k=1}^{T}(X_{i}(k)-\bar{X}_{i})^{2}\right)\left(\sum_{k=1} ^{T}(X_{j}(k)-\bar{X}_{j})^{2}\right)}},\] (8) Here, \(C_{ij}\) is the correlation between ROI \(i\) and ROI \(j\), and \(X_{i}\), \(X_{j}\) are the time series for ROIs \(i\) and \(j\), respectively. To construct a network \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), where \(\mathcal{V}\) is the set of ROI and \(\mathcal{E}\) is the set of connections, a threshold is applied to the correlation matrix. Only correlations above a certain value are considered to represent connections in \(\mathcal{E}\).

In EEG and MEG data the process is the same while each signal corresponds to a channel and so in the constructed brain network, each node is a channel and each connection shows high Pearson's correlation between its endpoints.

### MLP-Mixer

The MLP-Mixer architecture [82] is a novel neural network design that has attracted much attention in the field of computer vision. It presents itself as a distinctive alternative to the CNNs and Transformer models. The structure of MLP-Mixer is composed of two key sub-layers in each layer: the patch mixing layer and the channel mixing layer. The patch mixing layer processes spatial information within each channel independently, whereas the channel mixing layer combines the information across various channels. This dual process of mixing is crucial for the MLP-Mixer's capability to detect both local and global image dependencies.

The mathematical representation of the MLP-Mixer is as follows:

\[\mathbf{H}_{\text{patch}}=\mathbf{E}+\mathbf{W}_{\text{patch}}^{(2)}\sigma \left(\mathbf{W}_{\text{patch}}^{(1)}\texttt{LayerNorm}\left(\mathbf{E} \right)^{\top}\right)^{\top},\] (9)

Channel Mixer:

\[\mathbf{H}_{\text{channel}}=\mathbf{H}_{\text{patch}}+\mathbf{W}_{\text{ channel}}^{(2)}\sigma\left(\mathbf{W}_{\text{channel}}^{(1)}\texttt{LayerNorm}\left( \mathbf{H}_{\text{patch}}\right)\right),\] (10)

**Challenges of Extending MLP-Mixer to Graphs and Time Series.** In graphs, the vanilla MLP-Mixer[82] can be used to bind information across both of feature and node dimensions, but directly applying vanilla MLP-Mixer to graphs is insufficient and impractical. First, there does not exist in general a canonical grid of the graphs (contrary to images) to encode nodes, which makes patch extraction challenging. Second, contrary to images that can be divided into patches of the same size, the partitioning of nodes in graphs might not be all the same size due to the complex graph topology. Moreover, in temporal graphs, dynamics of the graph and its temporal properties should be captured to effectively learn its node encodings. The vanilla MLP-Mixer is not capable of learning temporal dynamics.

Similarly, in multivariate time series data, there is no canonical grid and patches are not necessarily the same size.

**Challenges of using MLP-Mixer and its variants for Brain Activity.** While there are existing studies that aim to address the above limitations and define patches in graphs using graph partitioning algorithms [35], or first-hop neighbourhood [21], there are designed for general cases and miss special properties of the brain. 1 The human brain is comprised of functional systems [76], which are groups of voxels that perform similar functions [80]. Recently, Trockman & Kolter [83] show that the main power of vision architectures like MLP-Mixer and ViT [50] comes from patching, splitting the image into multiple same size parts which might show the same concept. Inspired by this, we suggest using functional patching in analysis of brain activity, i.e. splitting voxels into some groups such that each group has similar functionality. 2 This approach results in another challenge which is the size of functional systems are not the same and simply using vanilla MLP-Mixer on functional patches is not feasible. To this end, we present an interpolation method to linearly interpolate functional systems to the same size.

The main differences of our VA Encoder and FC Encoder with MLP-Mixer are summarized in Table 3. In this table, 1-d MLP and 2-d MLP refers to applying MLP in one of the dimensions and both dimensions, respectively.

## Appendix B Bvfc Dataset

In this section, we introduce Bvfc dataset and discuss how it is derived from the fMRI and MEG data.

### Things Dataset

The Things dataset2[37] is a large battery of visual object recognition datasets that use a common set of images as visual stimuli. These datasets cover a broad range of data types, ranging from behavioral aspects, such as similarity judgments, to neural responses to the presented images, such as fMRI and MEG recordings. The shared image database includes more than 26,000 images in total, categorized into 1,854 object concepts. In this work, we used 1 the THINGS fMRI1 dataset, consisting of event-related BOLD responses of three human subjects to 8,460 images selected from 720 categories (12 images per each). 2 the THINGS MEG1 dataset, consisting of Magnetoencephalography (MEG) of 4 subjects for 22,248 images (1,854 categories, 12 images per category), collected over the course of 12 sessions. Both preprocessed and raw versions of the fMRI and MEG datasets are provided by

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multicolumn{2}{c}{MLP-Mixer} & \multicolumn{1}{c}{FC Encoder} & \multicolumn{1}{c}{VA Encoder} \\ \hline \hline Input & Images (2-d Regular grid) & Graphs (Irregular and non-grid) & Time series (Unstructured) \\  & Same width and height & Different sizes (\#Nodes, \#Edges) & Variable Length \\ \hline \multirow{3}{*}{Patch Extraction} & Based on pixels order & Based on Temporal Random Walks & Based on Brain Functional Systems \\  & Non-overlapping patches & Overlapping patches & Overlapping patches \\  & Same patches at each epoch & Different patches at each epoch & Different patches at each epoch \\ \hline \multirow{3}{*}{Patch Encoder} & Same patch size & Variable patch size & Variable patch size \\  & Using 2-d MLP & Using \(\text{TPMixer}\) & Using \(\text{Voxel-Mixer}\) \\  & & (1-d MLP+ Dynamic self-attention) & (2-d MLP+ Dynamic self-attention) \\ \hline Positional & \multirow{3}{*}{Implicitly ordered} & No universal ordering & No universal ordering \\ \cline{1-1}  & & \multicolumn{1}{c}{Permutation invariant} & Permutation variant \\ \hline \hline \end{tabular}
\end{table}
Table 3: The differences of VA Encoder and FC Encoder with MLP-Mixer.

the Things authors3. However, in the preprocessed version of fMRI dataset, each voxel is associated with a single beta value, which misses the dynamic of voxel activity over time. For the purpose of this work, we utilized the raw version as we required the time series of fMRI recordings and follow the following preprocess steps:

Footnote 3: https://plus.figshare.com/collections/THINGS-data_A_multimodal_collection_of_large-scale_datasets_for_investigating_object_representations_in_brain_and_behavior/6161151

### Preprocessing

The beta values provided in the preprocessed version of the THINGS fMRI dataset are single measures of each voxel's response to a certain stimulus, which are obtained by applying a general linear model (GLM) to the voxels' time series. Since the preprocessed dataset only offers beta values, we utilized and preprocessed the raw data without applying the GLM step at the end. The preprocessing pipeline used by the authors of THINGS also includes a semi-supervised ICA-denoising step, which requires prior experience with fMRI noise-signal classification. We replaced this stage with ICA-AROMA [70], an automatic ICA-denoising tool, to improve the replicability of our results without the need for manual supervision or intervention in the denoising step. For the rest of the preprocessing steps, we followed the same pipeline used by Hebart et al. [37]. For each image, we use 13 seconds of fMRI signals of the human subject, and treat each as a time window. We use the output of the preprocess time series as the voxel level brain activity. To derive the brain functional connectivity from the time series data, we consider each voxel as an ROI and calculate the statistical association of the time series of two voxels \(v_{i}\) and \(v_{j}\) in each time window using Pearson's correlation:

\[\mathbf{C}_{ij}=\frac{\sum_{k=1}^{T}(X_{i}(k)-\bar{X}_{i})(X_{j}(k)-\bar{X}_{ j})}{\sqrt{\left(\sum_{k=1}^{T}(X_{i}(k)-\bar{X}_{i})^{2}\right)\left(\sum_{k=1}^ {T}(X_{j}(k)-\bar{X}_{j})^{2}\right)}},\] (11)

where \(X_{i}(k)\) and \(X_{j}(k)\) are \(v_{i}\) and \(v_{j}\) activities at time \(k\), and \(\bar{X}_{i}\) and \(\bar{X}_{j}\) are their average activity over the time window, respectively. We next, for each voxel removes negative elements and then keep 90-percentile of its corresponding correlation. We use the same approach on the time series of channels in MEG to obtain brain connectivity networks. For the preprocessing scripts visit this link.

### Image Classification

Understanding object representation in the brain is a key step toward revealing the basic building blocks of human visual processing [37]. Toward this direction, in the first downstream task on Bvfc we aim to classify seen images during the fMRI and MEG recording. As discussed above, the fMRI dataset consists of responses of three human subjects to 8,460 images selected from 720 categories (12 images per each) from THINGS database [36]. Each of the images has a high-level concept as its high-level label, which described the type of the object in the image (e.g., "Food", "Human Body", etc.)4. In the first task, we aim to predict the high-level label of the seen image by using the fMRI responses of the human subject. This task is a multi-class classification tasks with 9 classes.

Footnote 4: Note that the high-level labels of images are different from their original labels, as each high-level class include a set of primary labels. For example, all Pizza, Fast Food, and Bacon are in a high-level class of Food.

### Anomaly Detection

In the fMRI1 THINGS dataset [37], there are 100 unique catch images that were created using the generative adversarial neural network, BigGAN [12]. These images were generated by interpolating between two latent vectors, yielding novel objects that were not recognizable. We take advantage of these images and design a downstream task to detect these images.

The visual cortex, responsible for processing visual information, is hierarchically organized with multiple layers building upon simpler features at lower stages [86]. Initially, neurons detect edges and colors, but on deeper levels, they specialize in recognizing more complex patterns and objects. Accordingly, we expect our model to detect GAN generated images by using the subject's brain fMRI response. We model this task as a binary classification task, where the brain fMRI response to each natural image is considered "normal" and the brain fMRI response to GAN generated images is considered "Abnormal". For further information about the generated images by GAN and its architecture see the original paper of THINGS dataset [37] and original paper of BigGAN [12].

## Appendix C Additional Related Work

To further situate our BrainMixer in a broader context, we briefly review self-supervised representation learning of brain activity and time series representation learning.

**Self-supervised Representation Learning of Brain Activity.** In representation learning of brain activity, such as fMRI, MEG and EEG, obtaining labeled data is challenging and costly. To address this, various self-supervised learning techniques have been introduced. Banville et al. [6] suggest using relative positioning, temporal shuffling, and contrastive predictive coding specifically to EEG data. Additionally, Mohsenvand et al. [65] and Kostas et al. [51] presents an approach to learn EEG signals using negative sampling and contrastive learning, which is only suitable for SEEG and EEG data. Yang et al. [99] propose an unsupervised pre-training technique designed specifically for brain networks using contrastive learning and maximizing the mutual information between an anchor point of investigation \(X\) from a data distribution \(H\) and its positive samples, while minimizing its mutual information with its negative samples. All these methods are either 1 are designed for a specific type of neuroimaging data (e.g., EEG) and cannot be generalized to other neuroimage modalities, 2 are based on negative sampling generation which bias the performance toward the patterns of generated negative samples, being unable to generalize to complex and unseen patterns, and/or 3 uses either time series data or connectivity network, missing information from different level of granularity.

**Time Series Representation Learning.** Representation learning of multivariate time series has been getting increasingly popular with the motivation that modeling the complex relationships between covariates should improve the forecasting performance[19]. to this end, Transformers [89] attract much attention due to their superior performance in modeling long-term sequential data. [105] present Informer and [95] present Autotformer to address the efficiency challenges in long-term forecasting. Zhou et al. [107] design FEDformer and later FiLM [106] that decompose the sequences using Fast Fourier Transformation for better extraction of long-term information. Recently, Chen et al. [19] design TSMixer, and all MLP architecture for time series forecasting. Not only the purpose of these methods are different from VA Encoder, but also their architectures are different from VA Encoder from a subset of following aspects: 1 They bind information across time series, missing the cross-time dependencies of signals. 2 These methods use fixed static learnable matrices for binding time series, while in the brain signals, the functionality of each time series is important and a different set of signals should be mixed differently based on their corresponding voxel's (channel's) connections and functionality. 3 They treat each time series the same, while in multivariate time series some signals cab be more important than others for a specific downstream task.

**MLP-Mixer for fMRI Data.** MLP-Mixer shows promising performance on image data. One approach to learn from fMRI data is to treat fMRI image in each time window as an image and then employ an MLP-Mixer to learn representation for voxels. Geenjaar et al. [31] designed a fully-differentiable non-linear framework for whole-brain dynamic factor learning and applied MLP-Mixer to fMRI data. However, this study suffers from all the MLP-Mixer limitations that we discussed. For more explanations and illustrative examples see our discussion on the difference of our encoders with MLP-Mixer in Appendix A.4.

### Our Contributions

As we discussed in Sections 1, 2, and Appendix C, existing studies miss a subset of the following: 1 the functional connectivity between voxels, 2 timeseries activity of voxels, 3 special properties of the brain like hierarchical structure and its complex dynamics. Here, we summarize our contributions as follows:

1. Voxel Activity Encoder: We present VA Encoder, a novel multivariate time series encoder that employs a dynamic attention mechanism to bind information across both voxel and time dimensions. VA Encoder by learning the representation of each voxel allows us to obtain brain activity encodings at different level of granularity (e.g., voxel-, functional system-, and/or brain-level encodings). Our experiments (row 5 in Table 6) show that VA Encoder alone, i.e. without using functional connectivity, outperforms baselines in different downstream tasks.
2. Simple and Low Cost, but Effective Patching: We propose functional patching for learning brain activity. While existing patching methods are either 1 grid-based and inapplicable to graphs and time series, 1 requires additional computational cost, and/or 1 cannot use specific brain properties (e.g., functional systems), missing the functional similarity of voxels. Our functional patching uses additional knowledge about the brain functional systems and patch the brain into some groups in which voxels have similar functionality. Our experimental results show that removing functional patching and replacing it with either random patching or clustering patching can damage the performance (See Appendix F.3 and Table 6).
3. Functional Connectivity Encoder: To encode the functional connectivity graph, we design an MLP-based architecture that learns both the structural and temporal properties of the graph using temporal random walks. FC Encoder first extracts temporal patches using temporal random walks and then fuses information within each patch using a novel dynamic self-attention mechanism. To obtain the brain activity encoding at different level of granularity, we further propose an adaptive permutation invariant pooling method that theoretically is the universal approximator of any multi-set function. Our experimental results in Table 6 show that FC Encoder alone, i.e. without using time series of voxel activity, outperforms baselines in different downstream tasks.
4. Self-Supervised Pre-training Framework: We present a novel self-supervised pre-training framework without using contrastive learning, which requires generating negative samples. Existing pre-training methods for the representation learning of brain activity suffers from two main limitations: 1 They require negative samples to learn from data in a contrastive manner [99]. However, brain activity is complex by its nature, and simple negative samples cause missing complex patterns, damaging the performance. 2 They are based on a meta knowledge about a specific brain disease and so cannot generalize to other neuroimage modalities and different neuroimaging tasks [101]. Our framework allows self-supervised pre-training of any neuroimaging data that provides multivariate time series (e.g., fMRI, EEG, MEG, iEEG, etc.) without using any meta knowledge about the disease or downstream tasks, making it generalizable to different neuroimage modalities and different downstream tasks.

## Appendix D Theoretical Guarantee of TSetMixer Performance

**Theorem 1**.: TPMixer _is permutation invariant and a universal approximator of multisets._

Proof.: Let \(\pi(S)\) be an arbitrary permutation of set \(S\), we aim to show that \(\Psi(S)=\Psi(\pi(S))\). We first recall the TSetMixer and its two phases: Let \(S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{d}\}\), where \(\mathbf{v}_{i}\in\mathbb{R}^{d_{1}}\), be the input set and \(\mathbf{V}=[\mathbf{v}_{1},\ldots,\mathbf{v}_{d}]^{T}\in\mathbb{R}^{d\times d _{1}}\) be its matrix representation: we first fuse information across features in a non-parametric manner as follows:

\[\mathbf{H}_{\text{F}}^{(t)}=\mathbf{V}+\sigma\left(\texttt{Softmax}\left( \texttt{LayerNorm}\left(\mathbf{V}\right)^{\top}\right)\right)^{\top},\] (12)Now, for \(\pi(S)\), let \(\pi(\mathbf{V})=[\pi(\mathbf{v}_{1}),\ldots,\pi(\mathbf{v}_{d})]^{T}\in\mathbb{R}^{d \times d_{1}}\) be its matrix representation, for the first phase we have:

\[\pi(\mathbf{V})+\sigma\left(\texttt{Softmax}\left(\texttt{LayerNorm} \left(\pi(\mathbf{V})\right)^{\top}\right)\right)^{\top}\] (13) \[=\pi(\mathbf{V})+\sigma\left(\texttt{Softmax}\left(\pi\left( \texttt{LayerNorm}\left(\mathbf{V}\right)^{\top}\right)\right)\right)^{\top}\] (14) \[=\pi(\mathbf{V})+\pi\left(\sigma\left(\texttt{Softmax}\left( \texttt{LayerNorm}\left(\mathbf{V}\right)^{\top}\right)\right)^{\top}\right)\] (15) \[=\pi\left(\mathbf{V}+\sigma\left(\texttt{Softmax}\left(\texttt{ LayerNorm}\left(\mathbf{V}\right)^{\top}\right)\right)^{\top}\right)\] (16) \[=\pi\left(\mathbf{H}_{\mathrm{F}}^{(t)}\right),\] (17)

which means that the first phase of TSetMixer is equivariant. In the above, we used the fact that Softmax is permutation equivariant. In the second part, we first have:

\[\mathbf{P}_{\mathrm{Pool}_{i}}=\texttt{Softmax}\left(\texttt{ Flat}\left(\mathbf{H}_{\mathrm{F}}^{(t)}\right)\mathbf{W}_{\mathrm{Pool}}^{(i)}\right)\] (18) \[\Rightarrow\] (19) \[=\texttt{Softmax}\left(\pi\left(\texttt{Flat}\left(\mathbf{H}_{ \mathrm{F}}^{(t)}\right)\right)\mathbf{W}_{\mathrm{Pool}}^{(i)}\right)\] (20) \[=\pi\left(\texttt{Softmax}\left(\texttt{Flat}\left(\mathbf{H}_{ \mathrm{F}}^{(t)}\right)\right)\mathbf{W}_{\mathrm{Pool}}^{(i)}\right)\] (21) \[=\pi\left(\texttt{P}_{\mathrm{Pool}_{i}}\right)).\] (22)

Also, note that we defined \(\mathbf{H}_{\mathrm{PE}}^{(t)}\) as \(\mathbf{H}_{\mathrm{PE}}^{(t)}=\mathbf{H}_{\mathrm{F}}^{(t)}\mathbf{P}_{ \mathrm{Pool}}\). Therefore, we have:

\[\texttt{Mean}\left(\pi\left(\mathbf{H}_{\mathrm{F}}^{(t)}\right) \right)+\pi\left(\mathbf{H}_{\mathrm{PE}}^{(t)}\right)\;\;\texttt{Softmax} \left(\pi\left(\frac{\mathbf{H}_{\mathrm{PE}}^{(t)^{\top}}\;\mathbf{H}_{ \mathrm{PE}}^{(t)}}{\sqrt{d_{\mathrm{patch}}}}\right)\right)\] (23) \[=\texttt{Mean}\left(\pi\left(\texttt{Norm}\left(\mathbf{H}_{ \mathrm{F}}^{(t)}\right)+\mathbf{H}_{\mathrm{PE}}^{(t)}\;\;\texttt{Softmax} \left(\frac{\mathbf{H}_{\mathrm{PE}}^{(t)^{\top}}\;\mathbf{H}_{\mathrm{PE}}^{ (t)}}{\sqrt{d_{\mathrm{patch}}}}\right)\right)\right)\] (24) \[=\mathbf{h}_{\rho_{\mathrm{\tiny{\it{\tiny{\it{\tiny{\it{\tiny{ \it{\tiny{\it{\tiny{\it{\tiny{\it{\tiny{\it{\tiny{\it{\tiny{\it{\it{\it{\it{ \it{\it{{                  }}}}}}}}}}}}}}}}}}}}\] (25)

In the last step, we use the fact that \(\texttt{Mean}(.)\) is permutation invariant, which results TSetMixer to be permutation invariant.

Since the patch mixer is just normalization it is inevitable and cannot affect the expressive power of TSetMixer. Also, channel mixer is a 2-layer MLP with attention, which are the universal approximator of any function. Therefore, due to the fact that TSetMixer is permutation invariant, we can conclude that it is a universal approximator of multi-set functions. 

## Appendix E Experimental Setup

### Baselines

Since BrainMixer combines functional connectivity and voxel timeseries activity, we compare our method to fourteen previous state-of-the-art methods and baselines on the timeseries, functional connectivity, and graph encoding:

1. GOutlier [1]: GOutlier uses a generative model for edges in a node cluster and labels outliers as anomaly.
2. NetWalk[100]: Yu et al. [100] proposed a random-walk based dynamic graph embedding approach, NetWalk. NetWalk first uses simple random walks and jointly minimizes the pairwise distance of vertex representations of each sampled walk. Next, it uses a clustering-based technique to dynamically detect network anomalies.

3. HyperSAGCN [104]: HyperSAGCN (Self-attention-based graph convolutional network for hypergraphs) utilizes a spectral aggregated graph convolutional network to refine the embeddings of nodes within each hyperedge. HyperSAGCN generates initial node embeddings by hypergraph random walks and combines node embeddings by Mean(.) pooling to compute the embedding of hyperedge. The model with code can be found in here.
4. Graph MLP-Mixer (GMM) [35]: Graph MLP-Mixer uses graph partitioning algorithms to split the input graph into overlapping graph patches (subgraphs) and then employs a graph neural network to encode each patch. It then uses an MLP to fuse information across patch encodings. The model with code can be found in here. Note that Graph MLP-Mixer cannot take advantage of temporal properties of the graph as it is designed for static networks.
5. GraphMixer [21]: GraphMixer is a simple method that concatenates the 1-hop temporal connections and their time encoding of each node as its representative matrix. It then uses an MLP-Mixer to encode each representative matrix to obtain node encodings. The model with code can be found in here.
6. FbNetGen[46]: FbNetGen is a graph neural network based generative model for functional brain networks from fMRI data that includes three components: a dimension reduction phase to denoise the raw time-series data, a graph generator for brain networks generation from the encoded features, and a GNN predictor for predictions based on the generated brain networks. The model with code can be found in here.
7. BrainGnn[56]: BrainGnn is a graph neural network-based framework that maps regional and cross-regional functional connectivity patterns. Li et al. [56] propose a novel clustering-based embedding method in the graph convolutional layer as well as a graph node pooling to learn ROI encodings in the brain. The model with code can be found in here.
8. BrainNetCnn[49]: BrainNetCnn is a CNN-based approach that uses novel edge-to-edge, edge-to-node and node-to-graph convolutional filters that leverage the topological locality of structural brain networks.
9. ADMire[9]: ADMire is a random walk-based approach that models brain connectivity networks as multiplex graphs. It uses inter-view and intra-view walks to capture the causality between different neuroimage modalities or different frequency band filters.
10. BNTransformer[47]: BNTransformer adapts Transformers [89] to brain networks, so it can use unique properties of brain networks. BNTransformer use connection profiles as node features to provide low-cost positional information and then learns pair-wise connection strengths among ROIs with efficient attention weights. It further uses a novel READOUT operation based on self-supervised soft clustering and orthonormal projection. The model with code can be found in here.
11. PTGB [99]: PTGB is an unsupervised pre-training method designed specifically for brain networks using contrastive learning and maximizing the mutual information between an anchor point of investigation \(X\) from a data distribution \(H\) and its positive samples, while minimizing its mutual information with its negative samples. The model with code can be found in here.
12. Usad[2]: Usad is an unsupervised representation learning method in time series, which utilizes an encoder-decoder architecture within an adversarial training framework that allows it to take advantage of both.
13. Time Series Transformer (TST) [103]: TST is a transformer-based framework for unsupervised representation learning of multivariate time series, which is capable of pre-training and can be employed on various downstream tasks.
14. Mvts[69]: Mvts is an unsupervised transformer-based model for time series learning, which utilizes special properties of EEGs for seizure identification. It uses an autoencoder mechanism involving a transformer encoder and an unsupervised loss function for training.

We use the same hyperparameter selection process as BrainMixer. Also, we fine tune their training parameters as their original papers using grid search. For the sake of fair comparison, we use the same training, testing and validation data for all the baselines (including same data augmentation and negative sampling). Also, for PTGB [99], which also is capable of pre-training, we use the same datasets and settings as we use for BrainMixer.

### Datasets

We use six real-world datasets with different neuroimage modalities and downstream tasks, whose descriptions are as follows:

* Bvfc (This Paper): The main characteristics and pre-processing steps are mentioned in Appendix B. For the multi-class classification task, we aim to predict the label of the seen image (9 labels) using the fMRI response of a human subject (3 subjects). For the edge- and node-level anomaly detection tasks, we use synthetic injected anomalies and for the graph anomaly detection, we aim to detect GAN generated images using the fMRI response. We label brain activities that correspond to seeing a GAN generated image (resp. natural image) as "Anomalous" (resp. "Normal"). In the experiments, for the sake of efficiency, we remove irrelevant voxels.
* Bvfc-MEG (This Paper): The main characteristics and pre-processing steps are mentioned in Appendix B. For the multi-class classification task, we aim to predict the label of the seen image (9 labels) using the MEG response of a human subject (4 subjects). For the edge- and node-level anomaly detection tasks, we use synthetic injected anomalies and for the graph anomaly detection, we aim to detect natural scenes using the MEG response. We label MEG response that correspond to seeing natural scenes as "Anomalous" and seeing other objects as "Normal".
* ADHD [63]: ADHD [63] contains resting-state fMRI of 250 subjects in the ADHD group and 450 subjects in the typically developed (TD) control group. We follow the standard pre-processing steps [25] to obtain brain networks. For the edge and node anomaly detection tasks, we use synthetic anomalies, while for the graph anomaly detection task we label brain networks of the typically developed control group as "Normal" and brain networks of the ADHD group as "Anomalous".
* TUH-EEG [78]: The seizure detection TUH-EEG dataset [78] consists of EEG data with 31 channels of 642 subjects. For the edge and node anomaly detection tasks, we use synthetic anomalies, while for the graph anomaly detection task we label brain networks of people with and without seizure as "Anomalous" and "Normal", respectively.
* ASD [23]: This dataset includes the resting fMRI data taken from the Autism Brain Imaging Data Exchange (ABIDE) [23]; it contains data for 45 subjects (22 female, age \(=25.4\pm 8.9\) yrs) in the typically developed control group and 45 subjects (23 female, age \(=23.1\pm 8.1\) yrs) in the ASD group. We have followed the five pre-processing strategies denoted as DPARSF, followed by Band-Pass Filtering. For the edge and node anomaly detection tasks, we use synthetic anomalies, while for the graph anomaly detection task we label brain networks of the typically developed control group as "Normal" and brain networks of the ASD group as "Anomalous".
* HCP [87]: HCP [87] contains data from 7440 neuroimaging samples each of which is associated with one of the seven ground-truth mental states. Following previous studies [74], we define two downstream multi-class classification tasks: 1 Mental states prediction, in which we aim to predict the mental state using the fMRI. 2 We aim to predict the age of human subjects using their fMRI. In this tasks, we split the age into 5 groups, balancing the number of samples in each class. Similar to other datasets, we use synthetic anomalies for the edge and node anomaly detection tasks.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Datasets & Number of Graphs & Average Number of Nodes & Average Number of Edges & \begin{tabular}{c} Number of Classes \\ (Multi-class Classification) \\ \end{tabular} & 
\begin{tabular}{c} Ground-Truth Anomaly \\ (Binary Classification) \\ \end{tabular} \\ \hline \hline Bvfc & 25380 & 11776 & 352479 & 9 & Yes \\ Bvfc-MEG & 88992 & 272 & 9841 & 9 & Yes \\ ADHD & 700 & 400 & 6194 & - & Yes \\ TUH-EEG & 642 & 31 & 252 & - & Yes \\ ASD & 90 & 400 & 5903 & - & Yes \\ HCP & 7440 & 1000 & 7635 & 7 (Mental states) & Yes \\ HCP & 1067 & 1000 & 8041 & 5 (Age) & Yes \\ \hline \hline \end{tabular}
\end{table}
Table 4: Datasets statistics.

We model the pre-processed fMRI, MEG, and EEG signals as multivariate time series and use them as our time series activity. Next, we discuss how we derive the brain connectivity network.

**Constructing Brain Connectivity Network.** To construct the brain connectivity networks for each dataset, we use the same process as we do for Byrc. We use the output of the preprocess time series as the voxel level (channel level for EEG and MEG) brain activity. To derive the brain connectivity from the time series data, we consider each voxel (or channel) as an ROI and calculate the statistical association of the time series of two voxels (channels) \(v_{i}\) and \(v_{j}\) in each time window using Pearson's correlation:

\[\mathbf{C}_{ij}=\frac{\sum_{k=1}^{T}(X_{i}(k)-\bar{X}_{i})(X_{j}(k)-\bar{X}_{j} )}{\sqrt{\left(\sum_{k=1}^{T}(X_{i}(k)-\bar{X}_{i})^{2}\right)\left(\sum_{k=1}^ {T}(X_{j}(k)-\bar{X}_{j})^{2}\right)}},\] (26)

where \(X_{i}(k)\) and \(X_{j}(k)\) are \(v_{i}\) and \(v_{j}\) activities at time \(k\), and \(\bar{X}_{i}\) and \(\bar{X}_{j}\) are their average activity over the time window, respectively. We next, for each voxel (channel) removes negative elements and then keep 90-percentile of its corresponding correlation.

### Implementation, Training, and Hyperparameters Tuning

In each task, we split the data into training set (70% of the data), validation set (10% of the data), and test set (20% of the data). In the pre-training phase, we use both training set and validation set to train the model and then we fine tune the pre-trained model for downstream tasks using only training set. For downstream tasks, we use validation set to tune hyperparameters as discussed bellow. During the training of both pre-trained model and fine tuning for downstream tasks, the test set is untouched and it is used only for the final evaluation of the method.

In addition to hyperparameters and modules (activation functions) mentioned in the main paper, here, we report the training hyperparameters of BrainMixer: On all datasets, we use a batch size of 32 and use a learning rate of \(10^{-3}\). We use the maximum training epoch number of 100 with an early stopping strategy to stop training if the validation performance does not increase for more than 7 epochs. Furthermore, a dropout layers with rate = \(0.1\) is employed in all neural networks. To tune the model's hyperparameters, we systematically perform grid search. The search domains of each hyperparameter are reported in Table 5.

BrainMixer is implemented by PyTorch in Python and a Linux machine with GPU and 16GB of RAM is used to run evaluations.

Note that we use the same training pipeline as BrainMixer for all the baselines. For the sake of fair comparison, we use the same training, testing and validation data for all the baselines (including same data augmentation and negative sampling). Also, for PTGB [99], which also is capable of pre-training, we use the same datasets and settings as we use for pre-training of BrainMixer.

### Data Augmentation & Negative Samples

MLP-Mixer-based architectures are known to have the potential of overfitting [58]. To mitigate this, we perform data augmentation. For \(\mathcal{G}_{F}^{(t)}=(\mathcal{V},\mathcal{E}^{(t)})\), in patch extraction, we randomly mask \(p\) connections and then we sample temporal walks to generate new patches. Note that, at the end, each patch is an induced subgraph and might include masked connections as well. Furthermore, to generate negative samples: 1 To corrupt the functional connectivity, we randomly change one

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Datasets & Sampling Number \(M\) & Sampling Time Bias \(\theta\) & Temporal Walk Length \(m\) & Hidden dimensions \\ \hline \hline Byrc & 4, 8, 16, 32, 64, 128 & \(\{0.5,2.0,20,200,2000,20000\}\times 10^{-7}\) & 1, 2, 3, 4 & 32, 64, 128 \\ Byrc-MEG & 4, 8, 16, 32, 64 & \(\{0.5,2.0,20,200,2000,20000\}\times 10^{-7}\) & 1, 2, 3, 4, 5, 6 & 32, 64, 128 \\ ADHD & 4, 8, 16, 32, 64, 128 & \(\{0.5,2.0,20,200,2000,20000\}\times 10^{-7}\) & 1, 2, 3, 4 & 32, 64, 128 \\ TUHEEG & 4, 8, 16, 32, 64 & \(\{0.5,2.0,20,200,2000,20000\}\times 10^{-7}\) & 1, 2, 3, 4, 5, 6 & 32, 64, 128 \\ ASD & 4, 8, 16, 32, 64 & \(\{0.5,2.0,20,200,2000,20000\}\times 10^{-7}\) & 1, 2, 3, 4 & 32, 64, 128 \\ HCP & 4, 8, 16, 32, 64 & \(\{0.5,2.0,20,200,2000,20000\}\times 10^{-7}\) & 1, 2, 3, 4 & 32, 64, 128 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameters used in the grid search.

[MISSING_PAGE_FAIL:28]

replaces dynamic with static self-attention, row 11 removes time encoder, the last row set \(\theta=0\), removing biased in the sampling. These results show that each component is critical for achieving BrainMixer's superior performance. The greatest contribution comes from biased sampling, VA and FC encoders, functional patching, and dynamic self-attention.

### Parameter Sensitivity

In this section, we evaluate the effect of hyperparameters on the performance of BrainMixer in different downstream tasks.

The Effect of the Number of Walks.In the first evaluation, we evaluate the effect of the number of walks on the performance of BrainMixer. Results are reported in Figure 4. These results show that increasing the number of walks results in better performance. The main reason is that we use the union of walks to capture the neighbourhood of each node over time. The more number of walks the better representation of the temporal neighborhood we can obtain. That is, sampling more walks lets the model to extract more information about the dynamic of nodes neighborhood as well as its structure. Also, notably, we observe that only a small number of sampled walks are needed to achieve competitive performance: in the best case 4 and in the worst case 16 sampled walks are needed to achieve better performance than baselines.

The Effect of the Walk Length.In this experiment, we evaluate the effect of the walk length on the performance of BrainMixer. Results are reported in Figure 5. The results suggest that the effect of the walk length on performance peaks at a certain point, but the exact value varies with datasets. The main reason for this is that we use walks to capture the structural and temporal properties of each node. Therefore, for dense brain connectivity networks as well as datasets with a large number of _relevant_ time windows we need longer walks so the model can extract enough information about both _relevant_ time windows and dense neighborhoods. Accordingly, we see increasing trend in BVFC-MEG's performance when we increase the length of the walk. Also, note that increasing the walk length for more sparser brain connectivity networks or for datasets with a smaller number of _relevant_ time windows can damage the performance. The reason is we might consider irrelevant time windows by backtracking over time or consider far nodes (voxels in brain

Figure 4: The effect of the number of walks on the performance of BrainMixer in different downstream tasks.

are irrelevant. Accordingly, depends on the structure of the brain connectivity graph and temporal properties of time series signals, we might need longer or shorter walks.

**The Effect of the Time Decay \(\theta\).** As we discussed in section 3, previous studies show that doing a task can affect brain activity even after 2 minutes [98]. To this end, since more recent connections can be more informative, we use a biased sampling procedure and control the bias using a variable \(\theta\). That is, in the proposed sampling procedure, smaller (resp. larger) \(\theta\) means less (resp. more) emphasis on recent timestamps.

In this experiment, we evaluate the effect of time decay \(\theta\) on the performance of BrainMixer. Results are presented in Figure 6. These results suggest that \(\theta\) has a dataset-dependent optimal interval. That is, a small \(\theta\) means an almost uniform sampling of brain activity history, which results in poor performance when the brain activities in recent time-windows are more important in the dataset. Also, very large \(\theta\) might damage the performance as it makes the model focus on the most recent brain activity or only its own time window, missing long-term and lasting brain activities.

Please note that while the value of \(\theta\) needs to be tuned to achieve the best performance, choosing arbitrary \(\theta\) in a wide proper interval can still results in state-of-the-art performance over baselines.

**The Effect of the Number of ROIs.** The human brain is hierarchically organized and comprised of hierarchical groups of voxels that have similar functionality [80]. Accordingly, different downstream tasks requires studying the brain at different level of granularity. In this experiment, we evaluate the the effect of the number of ROIs5 on the performance of BrainMixer. We vary the number of ROIs from 10000 (voxel-level activity) to 100 (functional system-level activity) and report the results in Figure 7. The results suggest that using more ROIs and so studying the brain at lower-levels like voxel-level can result in a better performance. While there is a little improvement for downstream tasks that are correlated with human brain functional systems (e.g., HCP dataset and classification mental states), the significant improvements are for tasks that are highly correlated to a specific brain region (e.g., Bvfc dataset and classifying seen images, which is closely related to human brain visual cortex). As an example, there is a \(\approx 50\%\) performance loss in the accuracy of BrainMixer on

Figure 5: The effect of the walk length on the performance of BrainMixer in different downstream tasks.

Bvfc in multi-class classification task as it requires learning voxel activity (e.g., V1 and V2) not learning the higher-level aggregated visual cortex activity.

**The Effect of the Aggregation of Time Series.** Most existing studies on voxel-level brain activity aggregate the voxel activity in each time window and use a single weight relating the voxel activity to the task, called beta weight. However, aggregation of the voxel activity misses the temporal property and dynamics of voxel activity over time. To this end, we suggest using a time series encoder that

Figure 6: The effect of the time decay \(\theta\) on the performance of BrainMixer in different downstream tasks.

Figure 7: The effect of the number of ROIs on the performance of BrainMixer in different downstream tasks.

learn the dynamic of voxel activity over time, instead of simply aggregating them. In this experiment, we evaluate how much the aggregation of voxel activity time series can affect the performance. To this end, we take the mean of voxel activity time series and shorten it to 10%, 20%, 40%, and 60% of its original size. Figure 8 reports the results of this experiment on different downstream tasks. For datasets with low frequency sampling rate (e.g., HCP) aggregation does not significantly damage the performance. For the datasets with high frequency sampling rate (e.g., Byrc-MEG), however, aggregation of the voxel activity can significantly damage the performance (\(\approx 12\%\) performance lost in the worst case and \(\approx 5\%\) in the best case). These results show the importance of considering voxel activity as a time series instead of aggregating its activity and considering it as a single weight.

### The Effect of Functional and Temporal Patching

As discussed in section 3, in both VA Encoder and FC Encoder we first split the data (either time series or graph) into patches. In this section, we replace the proposed functional and temporal patching methods with some existing patching strategies as well as some baselines to evaluate their contribution in BrainMixer's superior performance. For patching time series data we evaluate the following methods:

1. Random Patching: We randomly group time series in multivariate time series data and treat each group as a patch.
2. Ordered Patching: We use the actual order of time series in the multivariate time series and group consecutive time series as a patch.
3. Correlation Patching: We calculate the Pearson's correlation of multivariate time series (see Equation 26) and split the data into groups base on their pairwise correlation.
4. Functional Patching: This is our designed patching method, in which we group the time series of voxels in each brain functional system as a patch.

Also, for the graph patching we evaluate the following methods:

1. 1-hop Patching: We use the 1-hop neighborhood of each node as its corresponding patch.
2. Partitioning Patching: Following He et al. [35] for graph patching, we use METIS [48], a graph clustering algorithm that partitions a graph into a pre-defined number of clusters.
3. Spectral Clustering Patching: Following Geenjaar et al. [31], we use spectral clustering patching used in this study.

Figure 8: The effect of the aggregation of the time series on the performance of BrainMixer in different downstream tasks.

4. Static Random Walk Patching: We replace temporal random walk in our temporal patching strategy with a static random walk. This random walk still should be able to capture structural properties but missing the dynamic of the graph.
5. Functional Patching in Graph: We use the actual brain functional systems as our patches.
6. Temporal Patching: This is our designed patching method for brain connectivity graph, in which we use temporal random walks that randomly sample temporal and structural neighborhood of each node.

In this experiments, we replace the baselines with our proposed patching method and keep the rest of the model unchanged. Results are reported in Table 7. Results show the superior performance of functional and temporal patching in time series and graph data, respectively. In time series patching, random and ordered patching perform poorly as they might group unrelated time series. Correlation patching performs better but still weaker than functional patching. The main reason for this superior performance is that we expect time series in each patch to have similar functionality and functional patching using the actual brain functional systems provides the best grouping since we know voxels in each functional system has similar functionality.

In graph patching, again our proposed temporal patching outperforms the other patching methods. Surprisingly, here functional patching performs poorly. The main reason is that in the functional connectivity graph, we connect highly correlated voxels (with respect to their activity). However, in each brain functional systems some voxels have complementary activity to others, which means while they have the same functionality, they might not have high correlation and so they are not connected. This fact results in considering disconnected subgraphs as patches, which is undesirable and so damages the performance.

### Performance Comparison using Different Metrics

We compared the performance of BrainMixer with baselines in section 4. In multi-class classification tasks, we used accuracy as our metrics. Also, for the anomaly detection tasks, since we have binary labels (either anomaly or normal), we used AUC-PR as the metrics. In this section, we additionally evaluate the BrainMixer and baselines using accuracy for anomaly detection tasks and top-1 accuracy for multi-class brain classification tasks.

**Accuracy.** In this part, we compare the performance of BrainMixer and baselines in anomaly detection tasks, using accuracy metric. Table 8 reports the results. Similar to Table 2, these results show the superior performance of BrainMixer in all edge-level, voxel-level, and brain-level anomaly detection tasks.

**Comparison of Best results.** In Table 1, we reported the average of accuracy. In this experiment, we report the best performance of each model among 20 times of running. Table 9 reports the result and show that BrainMixer significantly outperforms baselines.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Patching Methods & \multicolumn{2}{c}{ByFC} & \multicolumn{2}{c}{ByFC-MEG} & \multicolumn{1}{c}{HCP-Mental} & \multicolumn{1}{c}{HCP-Age} \\ \hline \hline \multirow{4}{*}{Time Series Patching} & Random Patching & \(60.42_{\pm 3.53}\) & \(60.28_{\pm 1.72}\) & \(91.97_{\pm 1.88}\) & \(45.71_{\pm 3.69}\) \\  & Ordered Patching & \(60.58_{\pm 0.60}\) & \(60.55_{\pm 1.01}\) & \(91.63_{\pm 1.57}\) & \(47.21_{\pm 0.91}\) \\  & Correlation Patching & \(66.97_{\pm 0.63}\) & \(60.91_{\pm 1.46}\) & \(95.08_{\pm 1.21}\) & \(55.30_{\pm 0.98}\) \\  & Functional Patching & \(\$72.44_{\pm 1.07}\) & \(\$2.58_{\pm 1.12}\) & \(\$96.32_{\pm 0.92}\) & \(\$75.83_{\pm 0.84}\) \\ \hline \hline \multirow{4}{*}{Graph Patching} & 1-hop Patching & \(63.59_{\pm 0.09}\) & \(60.01_{\pm 1.08}\) & \(89.97_{\pm 0.16}\) & \(54.91_{\pm 0.71}\) \\  & Partitioning Patching & \(66.04_{\pm 1.92}\) & \(60.63_{\pm 1.87}\) & \(96.14_{\pm 1.01}\) & \(56.82_{\pm 1.75}\) \\ \cline{1-1}  & Spectral Clustering Patching & \(63.77_{\pm 0.23}\) & \(59.16_{\pm 1.33}\) & \(90.24_{\pm 0.95}\) & \(48.34_{\pm 1.28}\) \\ \cline{1-1}  & Static Random Walk Patching & \(66.28_{\pm 1.52}\) & \(59.94_{\pm 1.20}\) & \(95.86_{\pm 0.79}\) & \(57.81_{\pm 0.92}\) \\ \cline{1-1}  & Functional Patching in Graph & \(60.03_{\pm 0.68}\) & \(54.94_{\pm 0.74}\) & \(91.45_{\pm 0.80}\) & \(50.11_{\pm 0.96}\) \\ \cline{1-1}  & Temporal Patching & \(\$67.24_{\pm 1.67}\) & \(\$62.58_{\pm 1.12}\) & \(\$96.32_{\pm 0.92}\) & \(\$75.83_{\pm 0.84}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: The effect of functional and temporal patching on the performance of BrainMixer: Mean ACC (%) \(\pm\) standard deviation.

### Graph Regression

In this section we evaluate the performance of BrainMixer in a regression task and compare it with baselines. In this task, we aim to predict Achenbach adult self-report (ASR) scores in HCP dataset, which are "Aggressive", "Intrusive", and "Rule-Break" scores. In this experimental setup, we use L1 loss to fine-tune the model for the downstream regression task. We also use the commonly used metric of Mean Absolute Errors (MAEs) on the prediction of these three scores.

Table 10 reports the results. In all three regression tasks, BrainMixer achieves the lower MAE and outperforms all the baselines.

### Qualitative Results

In this section, we report some success and failure cases of BrainMixer in image classification task and detecting synthetic images based on fMRI. Figure 9 (resp. Figure 10) shows four examples

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \multicolumn{2}{c}{Methods} & \multicolumn{1}{c}{Bvtc} & \multicolumn{1}{c}{Bvtc-MEG} & \multicolumn{1}{c}{HCP} & \multicolumn{1}{c}{ADD} & \multicolumn{1}{c}{TUM-EG} & \multicolumn{1}{c}{ASD} \\ \multicolumn{2}{c}{Anomaly \%} & \multicolumn{1}{c}{Aomaly \%} & \multicolumn{1}{c}{V\%} & \multicolumn{1}{c}{V\%} & \multicolumn{1}{c}{V\%} & \multicolumn{1}{c}{V\%} & \multicolumn{1}{c}{V\%} & \multicolumn{1}{c}{V\%} & \multicolumn{1}{c}{V\%} & \multicolumn{1}{c}{V\%} & \multicolumn{1}{c}{V\%} \\ \hline \hline \multicolumn{2}{c}{} & Optimizer & \(62.78_{1.41}\) & \(55.24_{12.15}\) & \(60.83_{11.75}\) & \(55.92_{10.10}\) & \(61.91_{11.81}\) & \(61.05_{12.22}\) & \(61.15_{12.22}\) & \(61.57_{13.11}\) & \(59.92_{10.05}\) & \(16.79_{11.51}\) \\ \multicolumn{2}{c}{} & NetWalk & \(69.21_{1.40}\) & \(58.47_{17.47}\) & \(77.79_{11.99}\) & \(60.04_{10.00}\) & \(68.58_{12.34}\) & \(66.15_{11.40}\) & \(70.71_{10.47}\) & \(67.92_{14.10}\) & \(68.39_{11.99}\) & \(65.87_{12.00}\) \\ \multicolumn{2}{c}{} & HyperLossG & \(85.41_{11.5}\) & \(67.47_{10.00}\) & \(80.33_{11.51}\) & \(56.14_{12.82}\) & \(88.59_{11.51}\) & \(87.24_{12.40}\) & \(67.01_{10.50}\) & \(72.33_{10.00}\) & \(10.68_{11.71}\) \\ \multicolumn{2}{c}{} & BrainMixer & \(84.82_{1.21}\) & \(71.34_{10.00}\) & \(84.55_{11.51}\) & \(85.93_{11.51}\) & \(85.93_{11.51}\) & \(87.54_{12.99}\) & \(74.25_{12.78}\) & \(72.45_{13.58}\) & \(85.33_{11.40}\) & \(85.33_{11.40}\) \\ \multicolumn{2}{c}{} & BrainMixer & \(70.26_{10.00}\) & \(68.78_{11.63}\) & \(70.58_{11.53}\) & \(70.82_{12.71}\) & \(70.54_{11.51}\) & \(70.71_{11.61}\) & \(68.83_{11.50}\) & \(70.52_{12.00}\) & \(68.85_{12.11}\) \\ \multicolumn{2}{c}{} & BrainMixer & \(78.09_{10.04}\) & \(67.10_{11.51}\) & \(55.34_{11.51}\) & \(77.82_{11.51}\) & \(77.02_{11.51}\) & \(77.16_{11.51}\) & \(76.33_{11.51}\) & \(68.33_{11.50}\) & \(70.24_{11.51}\) & \(68.85_{12.11}\) \\ \multicolumn{2}{c}{} & BrainMixer & \(85.35_{11.5}\) & \(70.44_{10.00}\) & \(80.82_{12.00}\) & \(85.30_{11.51}\) & \(76.66_{10.00}\) & \(77.74_{11.71}\) & \(71.03_{11.51}\) & \(66.04_{11.70}\) & \(70.97_{11.51}\) & \(69.09_{11.51}\)

[MISSING_PAGE_FAIL:35]

### Case Study: ASD

In this experimental design we train our model on a healthy control group, which lets the model learn normal brain patterns. After the training, we test our model on the ASD group and report the abnormal brain regions in the ASD group. The most repeatedly abnormal regions in ASD group are 1 Right Cerebellum Cortex, 2 Right-precuneus, and 3 Left-lingual. Our findings about the abnormal activity in the cerebellum cortex is consistent with previous studies [71].

### The Effect of Objective

In this experiment, to evaluate the significance of our loss function, we train the model with two other well known loss functions. 1 Contrastive learning: we replace our loss function with margin-based pairwise. In this loss function, we aim to maximize the distances of positive and negative samples. 2 Deep Graph InfoMax [91]: We use the encoding of each node as its local feature. Furthermore, we use the summary of the all encoding as the global encoding of the graph. Results are reported in Table 11.

### Number of Parameters

To compare the capacity of the models, we report the number of parameters in Table 12.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Methods & Byrc & Byrc-MEG & HCP-Mental & HCP-Age \\ \hline \hline BrainMixer with contrastive learning and & \multirow{2}{*}{\(47.30\)} & \multirow{2}{*}{\(49.77\)} & \multirow{2}{*}{\(53.78\)} & \multirow{2}{*}{\(49.41\)} \\ margin-based pairwise loss & & & & \\ BrainMixer with DGI & \(63.49\) & \(60.82\) & \(94.12\) & \(58.36\) \\ BrainMixer & 68.67 & 63.68 & 96.63 & 58.88 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Performance on multi-class brain classification using different objectives (ACC).

Figure 11: Examples of failure cases in detecting synthetic images based on fMRI.

Figure 12: The distribution of detected abnormal voxels by BrainMixer in condition ASD group

### Effect of the Number of Parameters on Accuracy

To evaluate the effect of the number of parameters on the performance of the model, we use Byrc and employ BrainMixer with different capacity. We restrict BrainMixer's and its encoders' capacity to \(80\%\), \(60\%\), \(50\%\), and \(30\%\) of their original capacity, which we have reported in Table 12. The results are reported in Figure 13.

## Appendix G Limitations and Future Work

In this work, we present an unsupervised pre-training framework, BrainMixer, that bridges the representation learning of voxel activity and functional connectivity by maximizing their mutual information. The promising performance of BrainMixer in several downstream tasks raises many interesting directions for future studies: While BrainMixer with a simple MLP can successfully classify observed images based on fMRI, one future direction is to pair BrainMixer with diffusion models [96] to directly decode brain visual system in an end-to-end manner. There are, however, a few limitations for BrainMixer: 1 In this study, we focus on designing a powerful unsupervised framework that could provide us with a robust and effective brain activity representation. However, reliability of machine learning methods for downstream tasks in sensitive domains (like healthcare) is critical. Evaluation of BrainMixer's prediction reliability and modifying BrainMixer so that it can provide us with the uncertainty of its prediction is left for future studies. 2 The current approach is capable of using one neuroimage modalities, while different neuroimage modalities can provide complementary information, which can help understanding and detecting neurological disease or disorders. One potential future work is to design multimodal BrainMixer, where it can learn from different neuroimage modalities, taking advantage of their complementary information.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Methods & Byrc & Byrc-MEG & HCP-Mental & HCP-Age \\ \hline \hline BrainNetCnn & \(4.1\)M & \(0.97\)M & \(1.1\)M & \(1.1\)M \\ BrainGNN & \(6.7\)M & \(0.8\)M & \(0.8\)M & \(0.8\)M \\ FhNetGen & \(7.2\)M & \(0.56\)M & \(1.1\)M & \(0.9\)M \\ Additive & \(89.3\)M & \(4.9\)M & \(4.2\)M & \(4.2\)M \\ PTGB & \(146.1\)M & \(10.1\)M & \(9.6\)M & \(9.6\)M \\ BNTransformer & \(187.8\)M & \(8.7\)M & \(12.4\)M & \(12.4\)M \\ BrainMixer & \(117.4\)M & \(9.4\)M & \(8.3\)M & \(8.3\)M \\ \hline \hline \end{tabular}
\end{table}
Table 12: Number of parameters in different models designed for neuroimage data.

Figure 13: The performance of the BrainMixerwith different capacity on Byrc.