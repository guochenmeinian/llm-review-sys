# Safe Decision Transformer with Learning-based Constraints

 Ruhan Wang

Indiana University

ruhwang@iu.edu &Dongruo Zhou

Indiana University

dzl13@iu.edu

###### Abstract

In the field of safe offline reinforcement learning (RL), the objective is to utilize offline data to train a policy that maximizes long-term rewards while adhering to safety constraints. Recent work, such as the Constrained Decision Transformer (CDT) [30], has utilized the Transformer [38] architecture to build a safe RL agent that is capable of dynamically adjusting the balance between safety and task rewards. However, it often lacks the stitching ability to output policies that are better than those existing in the offline dataset, similar to other Transformer-based RL agents like the Decision Transformer (DT) [7]. We introduce the Constrained Q-learning Decision Transformer (CQDT) to address this issue. At the core of our approach is a novel trajectory relabeling scheme that utilizes learned value functions, with careful consideration of the trade-off between safety and cumulative rewards. Experimental results show that our proposed algorithm outperforms several baselines across a variety of safe offline RL benchmarks.

## 1 Introduction

Recent studies have demonstrated the Transformer's [38] state-of-the-art performance across a range of applications, including natural language processing [40, 4] and computer vision [27, 3]. When applied to the domain of Reinforcement Learning (RL), a recent trend is to treat the decision-making problem as a sequence modeling problem, using auto regressive models such as Transformer which maps the history information directly to the action [7] or the next state [18]. Notably, the Decision Transformer (DT) [7] effectively extends the Transformer architecture to offline RL tasks, showcasing strong performance, particularly in sequential modeling. However, it is worth noting that while DT excels in maximizing long-term rewards, it may not always align with the complexities of real-world tasks. In practice, many tasks cannot be simplified solely into optimizing a single scalar reward function, and the presence of various constraints significantly narrows the spectrum of feasible solutions [12]. Such a setting is called safe RL, which has been studied in lots of safety-related decision-making problems. For instance, it is crucial that robots interacting with humans in human-machine environments prioritize human safety and avoid causing harm. In the realm of recommender systems, it is imperative to avoid recommending false or racially discriminatory information to users. Similarly, when self-driving cars operate in real-world environments, ensuring safety is paramount [36, 14, 31].

Constrained Decision Transformer (CDT) [30] serves as a pioneering work which extends the Transformer-based RL to the safe RL regime, which builds upon the foundation of the DT while incorporating essential safety constraints. CDT's core objective is to acquire a safe policy from an offline dataset. Its distinguishing feature is the ability to maintain zero-shot adaptability across a range of constraint thresholds, rendering it highly suitable for real-world reinforcement learning applications burdened by constraints. While CDT demonstrates exceptional competitiveness in safe offline reinforcement learning tasks, it shares a common limitation with DT--an absence of the'stitching' capability. This crucial ability involves integrating sub-optimal trajectory segments to forma near-optimal trajectory, a pivotal characteristic highly desired in offline reinforcement learning agents. What is more challenging is that for RL with safety constraints, the agent not only needs to stitch trajectories to achieve better reward feedback but also needs to guarantee that the obtained policy is still _safe_ after stitching, not being affected by unsafe trajectories in the offline dataset. Thus, we raise the following question:

**Can we design DT-based algorithms that output safe policies with the stitching ability?**

We answer this question affirmatively. To better demonstrate our algorithm design, we propose a toy example involving finding the path that maximizes reward under cost constraints, as illustrated in Figure 1. The task's objective is to identify a path with the highest reward while adhering to a cost constraint (cost limitation = 10). The training data covers segments of the optimal path, but none of the training data trajectories encompass the entire optimal path. The agent must synthesize these fragmented trajectories to determine the optimal path within the cost constraint. For the existing DT-based RL algorithms such as Q-learning Decision Transformer (QDT) [42], they are able to stitch suboptimal trajectories into the optimal ones, but they are unable to maintain safety during the stitching phase. For the existing DT-based safe RL methods such as CDT, they can only obtain suboptimal policies while satisfying the safety guarantee, due to the lack of stitching ability. Thus, we propose the **Constrained Q-learning Decision Transformer (CQDT)** method to address the issues in existing methods, as shown in Figure 1. The main contributions are listed as follows.

* CQDT seeks to improve the quality of the training dataset by utilizing cost and reward critic functions from the Constraints Penalized Q-learning framework [41] to relabel the return-to-go (RTG) and cost-to-go (CTG) values in the training trajectories. This relabeled dataset is subsequently used to train a Decision Transformer-based safe RL method, such as CDT.
* We provide a comparative analysis of our CQDT against various safe RL benchmarks across different RL task settings. The results are summarized in Figure 2, demonstrate that CQDT consistently outperforms all existing benchmarks in several offline safe RL tasks.
* We also show that our proposed CQDT enjoys better stitching ability compared with CDT, which suggests that CQDT can better utilize suboptimal trajectories in the offline dataset. The results are summarized in Figure 3 and Figure 4.

## 2 Related Work

**Offline Reinforcement Learning.** Offline Reinforcement Learning refers to a data-driven approach to the classic RL problem [25]. It focuses on deriving policies from pre-existing data without requiring further interaction with the environment. The practical applications of offline RL are extensive, spanning domains such as healthcare [13] and the acquisition of robotic manipulation skills [19]. However, its inherent challenge lies in the potential disparity between the learned policy and the behavior that generated the initial offline data [21]. Addressing this challenge has spurred the development of various methodologies, including constraining the learned policy close from the behavior policy [11; 21; 22; 44; 17; 20; 39; 32; 18]. There is a recent line of work aiming at providing a Transformer-based policy without explicitly constraining the distribution shift issue [7]. Our work falls into that regime, which uses a Transformer as the policy model focusing on the safe RL setting.

**Offline Safe Reinforcement Learning.** Offline Safe Reinforcement Learning aims to acquire constrained policies from pre-collected datasets, ensuring adherence to safety requirements throughout the learning process. This approach amalgamates techniques from both offline RL and safe RL, leveraging methodologies from both domains [23]. Certain methods tackle the constrained optimization problem through stationary distribution correction, employing Lagrangian constraints to ensure safe learning [41; 33]. Our work does not take the Lagrangian approach to learn a safe policy. Instead, our method explicitly treats the safe policy learning as a sequence modeling problem, similar to the previous CDT approach [30]. Such an approach enjoys the simplicity regarding the algorithm design, as well as the robustness to the algorithm performance.

## 3 Preliminaries

**Safe Offline RL.** We formulate the environment as a Constrained Markov Decision Process (CMDP), a mathematical framework for addressing the safe RL problem [2]. A CMDP comprises a tuple \((\mathcal{S},\mathcal{A},\mathcal{P},r,c,\mu_{0})\), where \(\mathcal{S}\) denotes the state space, \(\mathcal{A}\) signifies the action space, \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]\) represents the transition function, \(r:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}\) stands for the reward function, and \(\mu_{0}:\mathcal{S}\rightarrow[0,1]\) indicates the initial state distribution. In CMDP, \(c:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,C_{\max}]\) quantifies the cost incurred for violating constraints, where \(C_{\max}\) denotes the maximum cost allowable.

We denote the policy by \(\pi:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\), while \(\tau=\{s_{1},a_{1},r_{1},c_{1},\ldots,s_{T},a_{T},r_{T},c_{T}\}\) delineates the trajectory containing state, action, reward, and cost information throughout the maximum episode length \(T\). We use \(\tau.s_{t},\tau.a_{t},\tau.r_{t},\tau.c_{t}\) to denote the \(t\)-th state, action, reward and cost in trajectory \(\tau\). For each time step \(t\), the action \(a_{t}\) is drawn following the distribution \(\pi(s_{t},\cdot)\), and the next state \(s_{t+1}\) is drawn following the transition function \(\mathcal{P}(s_{t},a_{t},\cdot)\). The cumulative reward and cost for a trajectory \(\tau\) are represented as \(R(\tau)=\sum_{t=1}^{T}r_{t}\) and \(C(\tau)=\sum_{t=1}^{T}c_{t}\). We also denote \(R_{t}=\sum_{i=t}^{T}r_{i}\) by the return-to-go (RTG) at \(t\)-th step, \(C_{t}=\sum_{i=t}^{T}c_{i}\) by the cost-to-go (CTG) at \(t\)-th step as well. For simplicity, we define \(Q_{\tau}^{\pi}(s,a)=\mathbb{E}_{\tau\sim\pi}[R(\tau)|\tau.s_{1}=s,\tau.a_{1}=a]\) as the expected return of the policy starting from initial state \(s\) and action \(a\). Similarly, we denote \(Q_{e}^{\pi}(s,a)=\mathbb{E}_{\tau\sim\pi}[C(\tau)|\tau.s_{1}=s,\tau.a_{1}=a]\) as the expected cost. The agent's objective is to determine a policy \(\pi^{\kappa}\) that maximizes the reward while ensuring that the cumulative cost for constraint violation remains below the threshold \(\kappa\):

\[\pi^{\kappa}=\arg\max_{\pi}\mathbb{E}_{\tau\sim\pi,\tau.s_{1}\sim \mu_{0}}[R(\tau)],\] \[\text{s.t.}\ \mathbb{E}_{\tau\sim\pi,\tau.s_{1}\sim\mu_{0}}[C(\tau)]\leq\kappa.\] (1)

For safe offline RL, the agent learns the optimal safe policy purely from a static dataset \(\mathcal{T}\) that is previously collected with an unknown behavior policy (or policies). Specifically, \(\mathcal{T}\) consists of \(m\) episodes \(\tau_{i}\) with the maximum length \(T\), which is \(\mathcal{T}:=\{\tau_{1},\ldots,\tau_{m}\}\).

**Constrained Decision Transformer.** Our algorithm builds on the Constrained Decision Transformer (CDT) [30]. We briefly introduce the details of CDT here, and we leave more details in the appendix. CDT utilizes the return-conditioned sequential modeling framework to accommodate varying constraint thresholds during deployment, ensuring both safety and high return. CDT employs a stochastic Gaussian policy representation to generate the action at \(t\)-th time step, i.e., \(a_{t}\sim\pi_{\theta}(\cdot|o_{t})=\mathcal{N}(\mu_{\theta}(o_{t}),\Sigma_{ \theta}(o_{t}))\), where \(o_{t}=\{R_{t-K:t},C_{t-K:t},s_{t-K:t},a_{t-K:t-1}\}\) represents the truncated history from step \(t-K\) to \(t\), \(K\in\{1,\ldots,t-1\}\) indicates the context length and \(\theta\) denotes the CDT policy parameters. During the training phase, CDT generates the training set \(\{(o_{t},a_{t})\}\) by splitting trajectories in \(\mathcal{T}\) into shorter contexts with length \(K\), then it trains the policy by minimizing the prediction loss between \(a_{t}\) and \(\pi_{\theta}(\cdot|o_{t})\). During the inference phase, CDT selects the initial return-to-go \(R_{1}\) as well as the cost-to-go \(C_{1}\), then it selects the action \(a_{t}\) based on the current 

[MISSING_PAGE_FAIL:4]

in line 4 of Algorithm 1. In the subsequent steps, the \(\mathbb{Q}_{r}\) and \(\mathbb{Q}_{c}\) lists are utilized to relabel the RTG and CTG for each trajectory. For additional details, please refer to Appendix A.1.

**Second Step: Relabeling Trajectories.** Now we describe how to relabel a given trajectory \(\tau\) using \(\mathbb{Q}_{r}\) and \(\mathbb{Q}_{c}\) in detail. The steps are summarized in Algorithm 2. To demonstrate that, recall that our goal is to learn \(\pi^{\kappa}\) that maximizes the expected return under the \(\kappa\) constraint. Assume that for the trajectory \(\tau\), the policy \(\pi\) that generates \(\tau\) satisfies the constraint \(\kappa\). Therefore, in order to further push the agent to learn \(\pi^{\kappa}\) instead of only learning \(\pi\), we generate a new trajectory \(\tau^{\prime}\) identical to \(\tau\), with different \(\tau^{\prime}.R_{i}\), \(\tau^{\prime}.C_{i}\), to make \(\tau^{\prime}\) similar to a trajectory generated by \(\pi^{\kappa}\). Our strategy is simple: we first replace the last RTG and CTG of \(\tau^{\prime}\) as 0. At the \(t\)-th step of \(\tau^{\prime}\), we would like to calculate \(\tau^{\prime}.R_{t-1}\) and \(\tau^{\prime}.C_{t-1}\). Then we either to use the existing RTG (\(\tau^{\prime}.R_{t}\)) and CTG (\(\tau^{\prime}.C_{t}\)) to update \(\tau^{\prime}.R_{t-1}\) and \(\tau^{\prime}.C_{t-1}\) (line 4 in Algorithm 2), or we use the learned reward critic and cost critic to update \(\tau^{\prime}.R_{t-1}\) and \(\tau^{\prime}.C_{t-1}\) (line 6 in Algorithm 2), if the reward critic and cost critic provide a more "aggressive" approximation, i.e., \(V_{c}^{\tau}(\tau.s_{t})\leq\tau^{\prime}.C_{t}\) and \(V_{r}^{\tau}(\tau.s_{t})\geq\tau^{\prime}.R_{t}\) (line 5 in Algorithm 2). Here \(V_{r}^{\tau}\) and \(V_{c}^{\tau}\) are learned reward and cost critics, and they are selected from \(\mathbb{Q}_{r}\) and \(\mathbb{Q}_{c}\) to make sure that the cost constraint estimation \(Q_{c}^{\tau}\) is close to the true CTG \(C_{0}\) (line 8 in Algorithm 2). We summarize the relabeling process in Algorithm 2.

```
0: Trajectory \(\tau\), reward critic \(Q_{r}^{\tau}\) and cost critic \(Q_{c}^{\tau}\)
1: Set \(T\) as the length of \(\tau\), the new trajectory \(\tau^{\prime}=\tau\), \(\tau^{\prime}.R_{T+1}=\tau^{\prime}.C_{T+1}=0\)
2:for\(t=T+1,\dots,2\)do
3: Set \(V_{r}^{\tau}(\tau.s_{t})=Q_{r}^{\tau}(\tau.s_{t},\tau.a_{t})\), \(V_{c}^{\tau}(\tau.s_{t})=Q_{c}^{\tau}(\tau.s_{t},\tau.a_{t})\)
4:\(\tau^{\prime}.R_{t-1}\leftarrow\tau.r_{t-1}+\tau^{\prime}.R_{t}\), \(\tau^{\prime}.C_{t-1}\leftarrow\tau.c_{t-1}+\tau^{\prime}.C_{t}\)
5:if\(V_{c}^{\tau}(\tau.s_{t})\leq\tau^{\prime}.C_{t}\) and \(V_{r}^{\tau}(\tau.s_{t})\geq\tau^{\prime}.R_{t}\)then
6:\(\tau^{\prime}.R_{t-1}\leftarrow\tau.r_{t-1}+V_{r}^{\tau}(\tau.s_{t})\), \(\tau^{\prime}.C_{t-1}\leftarrow\tau.c_{t-1}+V_{c}^{\tau}(\tau.s_{t})\)
7:endif
8:endfor
8: Relabeled trajectory \(\tau^{\prime}\) ```

**Algorithm 2** Relabeling one trajectory

The most notable difference between our relabeling strategy and the previous one for offline RL [42] is that we relabel RTG and CTG _jointly and simultaneously_. If we only relabel each trajectory based on their RTG and CTG separately, we might obtain unsafe trajectories, which hurts the overall performance of CQDT. Instead, our strategy ensures that, each safe trajectory will still be safe after relabeling, which is crucial for the safe RL setting. Our experimental results in the later section suggest the effectiveness of our relabeling strategy.

**Third Step: Post-Processing Steps for the Final Trajectory.** Now, we have produced an augmented trajectory dataset \(\mathcal{T}\) which consists of the original trajectories \(\tau\) and the new trajectories \(\tau^{\prime}\). Finally, we introduce some additional post-processing steps over \(\mathcal{T}\) from existing works [30; 42] for the further performance improvement of CQDT.

The first post-processing technique we adopt is to resolve the potential conflict between a high RTG and a low CTG. Due to the nature of safe RL, we would like to first guarantee the safety of our learned policy. Following [30], we use a Pareto Frontier-based data augmentation technique to further generate new trajectories and add them to \(\mathcal{T}\). The second post-processing technique aims to maintain the consistency of the RTG and CTG within the input sequence of CDT. Due to space limitations, we defer the detailed in Appendix A.2.

## 5 Experiment

In this section, we begin by outlining the fundamental settings of our experiment. We then show the performance of CQDT under a series of experiments, each addressed a key challenge as follows:

* How does CQDT compare with CDT and other offline safe reinforcement learning methods in terms of performance? Additionally, how does the choice of the value function affect CQDT's performance?
* Is CQDT capable of performing effective stitching?

[MISSING_PAGE_FAIL:6]

Compared to other baselines, our proposed CQDT method achieves maximum return while ensuring safety. CPQ, BCQ-Lag, and BEAR-Lag, three Q-learning-based safe reinforcement learning methods, encounter challenges in balancing safety and reward optimization. The BC-Safe method, grounded in imitation learning and trained on provided data, exhibits suboptimal performance during the test phase. This phenomenon may be attributed to the scarcity of safe data in the training dataset, indicating a lack of robustness. COptiDICE employs novel estimators to evaluate policy constraints and achieves suboptimal rewards, while facing challenges related to adhering to strict safety constraints.

The CQDT method presented in our work leverages additional value functions to relabel trajectories, enhancing the model's stitching capability and enabling it to achieve state-of-the-art performance. To demonstrate the effectiveness of the CQDT method, we conduct a comprehensive comparative analysis examining the impact of various value functions on algorithmic performance. BCQ-Lag and BEAR-Lag, both grounded in offline safe reinforcement learning and based on Q-Learning, are employed for this investigation. The \(Q_{r}\) and \(Q_{c}\) functions in these methodologies are employed to estimate the RTG and CTG values of the original trajectory. To signify the enhanced versions of these methods, we specifically refer to them as **BCQL-CDT** and **BEARL-CDT**, respectively. Refer Appendix C.2 for further details.

Our experimental results, detailed in Table 1, indicate that BCQL-CDT and BEARL-CDT perform similarly to CDT in the selected tasks, although they do not reach the performance of CQDT. The performance discrepancy between BCQ-Lag and BEAR-Lag compared to CPQ suggests suboptimality in relabeling the original trajectories using their respective value functions. This contributes to the varied performance among BCQL-CDT, BEARL-CDT, and CQDT.

We also conduct two ablation studies, Ablation 1 and Ablation 2, to assess the impact of various components within CQDT. The results of these experiments are provided in Table 1. For further details on the ablation studies, please refer to Appendix C.1. In addition, we evaluate the Zero-Shot Adaptation capability and robustness of CQDT. For more information, refer to Appendix C.4.

### The Stitching Capability of CQDT

We conduct an evaluation of CQDT's stitching capabilities for reward and cost by creating various suboptimal datasets for five tasks and comparing the performance of CQDT and CDT across these datasets.

Figure 5: Verification of stitching-reward ability with \(p=0.x\) values representing various suboptimal datasets. Suboptimal datasets were generated by removing safe trajectories that fall within the top x% of cumulative rewards. Higher \(p\)-values indicate the removal of more high-reward safe paths, degrading dataset quality. The first row illustrates cumulative rewards obtained by CQDT and CDT trained on these datasets for different tasks, while the second row shows the corresponding cumulative costs. The black dashed line denotes the cost limitation.

Figure 6: Verification of stitching-cost ability with \(p=0.x\) values corresponding to various suboptimal datasets. These datasets were created by excluding trajectories with low cumulative costs. As the \(p\)-value increases, fewer trajectories with small cumulative costs are retained, resulting in increasingly suboptimal datasets.

To evaluate the reward stitching ability, we remove the top X% of trajectories with the highest RTG from batches of trajectories. As the value of X increases, more high-return trajectories are excluded from the dataset. To create a suboptimal dataset for evaluating cost stitching capability, we group trajectories based on their RTG, then we remove the trajectories with lowest CTG in each group. In detail, we divide the trajectories into \(\frac{Max\ Return}{10}\) groups, where Max Return denotes the highest return among all trajectories. Within each group, we remove trajectories with the lowest X% CTG from each group. Such a setup allows us to detect the stitching ability for a safe offline RL agent, as we want her to learn safe and high-return policy from unsafe and low-return trajectories. We leave the detailed parameter setup and the visualization of these suboptimal datasets in the Appendix C.3.

We present the performance of CQDT on different suboptimal datasets in Figures 5 and 6. These experiments demonstrate that as the value of \(X\) increases, leading to the removal of higher-quality trajectories, the performance of policies generated by CQDT and CDT deteriorates. We can observe that the cumulative reward decreases. However, CQDT consistently outperforms CDT, demonstrating its superior stitching ability. Even when trained with suboptimal datasets, CQDT effectively utilizes these datasets to maximize performance by leveraging its stitching capabilities. Superior performance by CQDT highlights its unique ability to stitch suboptimal trajectories, a capability not present in CDT. This stitching ability enables CQDT to achieve better overall performance.

### Performance of CQDT in Sparse Reward Environment

The experiments in the previous sections demonstrate that CQDT performs well in dense reward environments. In this section, we evaluate and analyze the performance of CQDT in the sparse reward environment. Since there is no publicly available dataset or corresponding environment for sparse reward scenarios in the field of offline safe RL, we build our own environment based on existing datasets. Specifically, we select the existing DroneCircle environment to create a Sparse-Reward DroneCircle environment.

For any trajectory \(\tau\) in DroneCircle, we aim to create a new trajectory \(\tau^{\prime}\) as follows. We consider each subsequence in \(\tau\) with length 10, i.e., \(\tau.r_{10k},\tau.r_{10k+1},...,\tau.r_{10k+9}\). We then set \(\tau^{\prime}=\tau\), while it replaces \(\tau^{\prime}.r_{10k+9}\) with \(\tau.r_{10k}+\tau.r_{10k+1}+\cdots+\tau.r_{10k+9}\), and replaces \(\tau^{\prime}.r_{10k+i},0\leq i<9\) with 0. Such an operation keeps the total reward of \(\tau^{\prime}\) unchanged, while it makes \(\tau^{\prime}\) a trajectory with sparse rewards. Accordingly, we use the Sparse-Reward DroneCircle Offline Dataset to train CQDT and CDT. During testing, we adopt the similar strategy, where each agent encounters 0 reward in time step \(10k,10k+1,...,10k+8\), and she encounters \(\tau.r_{10k}+\tau.r_{10k+1}+\cdots+\tau.r_{10k+9}\) at time step \(10k+9\). We do not change the cost distribution.

Figures 8 and 8 show the performance comparison between CQDT and CDT under different target return and target cost settings in the Sparse-Reward DroneCircle environment. The results indicate that CQDT consistently outperforms CDT across various settings of target return and target cost. These experimental results demonstrate that CQDT maintains its superiority even in sparse-reward environments.

## 6 Conclusion and Future Work

In this work, we proposed the Constrained Q-learning Decision Transformer (CQDT) for safe offline RL. Our approach replaces reward-to-go and cost-to-go in the training data with dynamic programming-based learning-based reward return and cost return, which brings the stitching ability and addresses the weakness of the Constrained Decision Transformer (CDT). Our evaluation shows that our approach is able to outperform existing safe RL baseline algorithms. One potential future direction is to build a theoretical analysis to justify the effectiveness of our learning-based constraint approach to safe RL, similar to previous analyses applied to general goal-based RL algorithms [5].

## References

* [1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In _International conference on machine learning_, pages 22-31. PMLR, 2017.
* [2] Eitan Altman. Constrained markov decision processes with total cost criteria: Lagrangian approach and dual linear program. _Mathematical methods of operations research_, 48:387-417, 1998.
* [3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6836-6846, 2021.
* [4] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.
* [5] David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? _Advances in Neural Information Processing Systems_, 35:1542-1553, 2022.
* [6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* [7] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* [8] Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. _arXiv preprint arXiv:1901.10031_, 2019.
* [9] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In _International conference on machine learning_, pages 1329-1338. PMLR, 2016.
* [10] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* [11] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In _International conference on machine learning_, pages 2052-2062. PMLR, 2019.
* [12] Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning. _Journal of Machine Learning Research_, 16(1):1437-1480, 2015.
* [13] Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan, Linying Zhang, Yi Ding, David Wihl, Xuefeng Peng, et al. Evaluating reinforcement learning algorithms in observational health settings. _arXiv preprint arXiv:1805.12298_, 2018.
* [14] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. _arXiv preprint arXiv:2205.10330_, 2022.
* [15] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* [16] Boris Ivanovic, Karen Leung, Edward Schmerling, and Marco Pavone. Multimodal deep generative models for trajectory prediction: A conditional variational autoencoder approach. _IEEE Robotics and Automation Letters_, 6(2):295-302, 2020.
* [17] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. _Advances in neural information processing systems_, 32, 2019.

* [18] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. _Advances in neural information processing systems_, 34:1273-1286, 2021.
* [19] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In _Conference on robot learning_, pages 651-673. PMLR, 2018.
* [20] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. _Advances in neural information processing systems_, 33:21810-21823, 2020.
* [21] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. _Advances in Neural Information Processing Systems_, 32, 2019.
* [22] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* [23] Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In _International Conference on Machine Learning_, pages 3703-3712. PMLR, 2019.
* [24] Jongmin Lee, Cosmin Paduraru, Daniel J Mankowitz, Nicolas Heess, Doina Precup, Kee-Eung Kim, and Arthur Guez. Coptidice: Offline constrained reinforcement learning via stationary distribution correction estimation. _arXiv preprint arXiv:2204.08957_, 2022.
* [25] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* [26] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_, 2015.
* [27] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [28] Zuxin Liu, Zhenpeng Cen, Vladislav Isenbaev, Wei Liu, Steven Wu, Bo Li, and Ding Zhao. Constrained variational policy optimization for safe reinforcement learning. In _International Conference on Machine Learning_, pages 13644-13668. PMLR, 2022.
* [29] Zuxin Liu, Zijian Guo, Haohong Lin, Yihang Yao, Jiacheng Zhu, Zhepeng Cen, Hanjiang Hu, Wenhao Yu, Tingnan Zhang, Jie Tan, et al. Datasets and benchmarks for offline safe reinforcement learning. _arXiv preprint arXiv:2306.09303_, 2023.
* [30] Zuxin Liu, Zijian Guo, Yihang Yao, Zhenpeng Cen, Wenhao Yu, Tingnan Zhang, and Ding Zhao. Constrained decision transformer for offline safe reinforcement learning. _arXiv preprint arXiv:2302.07351_, 2023.
* [31] Martin Pecka and Tomas Svoboda. Safe exploration techniques for reinforcement learning-an overview. In _Modelling and Simulation for Autonomous Systems: First International Workshop, MESAS 2014, Rome, Italy, May 5-6, 2014, Revised Selected Papers 1_, pages 357-375. Springer, 2014.
* [32] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.
* [33] Nicholas Polosky, Bruno C Da Silva, Madalina Fiterau, and Jithin Jagannath. Constrained offline policy optimization. In _International Conference on Machine Learning_, pages 17801-17810. PMLR, 2022.

* Ray et al. [2019] Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement learning. _arXiv preprint arXiv:1910.01708_, 7(1):2, 2019.
* Schulman et al. [2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* Shalev-Shwartz et al. [2016] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. _arXiv preprint arXiv:1610.03295_, 2016.
* Stooke et al. [2020] Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by pid lagrangian methods. In _International Conference on Machine Learning_, pages 9133-9143. PMLR, 2020.
* Vaswani [2017] Ashish Vaswani. Attention is all you need. _arXiv preprint arXiv:1706.03762_, 2017.
* Wang et al. [2018] Qing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et al. Exponentially weighted imitation learning for batched historical data. _Advances in Neural Information Processing Systems_, 31, 2018.
* Wolf et al. [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations_, pages 38-45, 2020.
* Xu et al. [2022] Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized q-learning for safe offline reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 8753-8760, 2022.
* Yamagata et al. [2023] Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez. Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl. In _International Conference on Machine Learning_, pages 38989-39007. PMLR, 2023.
* Yang et al. [2021] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. _arXiv preprint arXiv:2110.11334_, 2021.
* Yu et al. [2021] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. _Advances in neural information processing systems_, 34:28954-28967, 2021.
* Zhang et al. [2020] Yiming Zhang, Quan Vuong, and Keith Ross. First order constrained optimization in policy space. _Advances in Neural Information Processing Systems_, 33:15338-15349, 2020.

Implementation Details

In this section, we introduce more details of the implementation of CQDT.

### Implementation Details of CPQ (First Step of CQDT)

**OOD discriminator \(\nu\).** The process of learning the OOD action generation distribution \(\nu\) uses an OOD detection method based on the Conditional Variational Autoencoder (CVAE). In detail, \(\nu\) is based on a decoder \(p:\mathcal{S}\times\mathcal{Z}\times\mathcal{A}\rightarrow[0,1]\) that generates the action following the distribution \(p(s,z,\cdot)\), where \(z\in\mathcal{Z}\subseteq\mathbb{R}\) is the hidden state; an encoder \(q:\mathcal{S}\times\mathcal{A}\times\mathcal{Z}\rightarrow[0,1]\) that generates the latent state following the distribution \(q(s,a,\cdot)\). \(p\) and \(q\) are trained by solving the following evidence lower bound (ELBO) objective, which is

\[\max_{p,q}\mathbb{E}_{s,a\sim\mathcal{T}}\Big{[}\mathbb{E}_{z\sim q}\log p(s,z,a)-\beta\text{KL}(q(s,a,\cdot)\|N(0,1))\Big{]},\] (4)

where KL denotes the KL divergence and \(\beta\) is the penalty parameter. We then set \(\nu\) as

\[\nu(s,a)=\begin{cases}1&\text{KL}(q(s,a,\cdot)\|N(0,1))\geq d\\ 0&\text{Otherwise}\end{cases}\] (5)

Here we provide a detailed implementation code for CPQ, following **(author?)**[41].

```
0: Trajectories dataset \(\mathcal{T}\); constraint limitation \(\kappa\); initialize encoder \(q\) and decoder \(p\); training steps \(M\) and \(N\).
1:// VAE training
2:for\(t=0\) to \(M\)do
3: Sample mini-batch of state-action pairs \((s,a)\sim\mathcal{T}\) and update \(p,q\) through (4)
4:endfor
5: Update \(\nu\) following (5)
6:// Policy training
7: Initialize reward critic \(Q_{r}\), cost critic \(Q_{c}\), actor \(\pi_{\theta}\)
8:for\(t=0\) to \(N\)do
9: Update cost critic by \[Q_{c}=\arg\min_{Q}-\alpha\mathbb{E}_{s,a\sim\mathcal{T}}[Q(s,a)\nu(s,a)]+ \mathbb{E}_{s,a,s^{\prime},c\sim\mathcal{T}}[(Q(s,a)-c-\gamma\mathbb{E}_{a^{ \prime}\sim\pi(\cdot|s^{\prime})}[Q(s^{\prime},a^{\prime})])^{2}],\]
10: Update reward critic by \[Q_{r}=\arg\min_{Q}\mathbb{E}_{s,a,s^{\prime},r\sim\mathcal{T}}[(Q(s,a)-r- \gamma\mathbb{E}_{a^{\prime}\sim\pi}[\mathbb{I}(Q_{c}(s^{\prime},a^{\prime}) \leq\kappa)Q(s^{\prime},a^{\prime})])^{2}],\]
11: Update policy as \[\pi^{\prime}=\arg\max_{\pi}\mathbb{E}_{s\sim\mathcal{T}}\mathbb{E}_{a\sim\pi( \cdot|s)}[\mathbb{I}(Q_{c}(s,a)\leq\kappa)Q_{r}(s,a)].\]
12:endfor
13:reward critic \(Q_{r}(s,a)\), cost critic \(Q_{c}(s,a)\) ```

**Algorithm 3** CPQ

### Implementation details of Data Augmentation (Third Step of CQDT)

We adopt a data augmentation technique based on the Pareto Frontier (PF) from [30]. The dataset used in safe offline RL is characterized by the PF value \(\text{PF}(\kappa,\mathcal{T})\), which is

\[\text{PF}(\kappa,\mathcal{T})=\max_{\tau\in\mathcal{T}}R(\tau),\quad\text{s.t. }C(\tau)\leq\kappa\]

Given that CQDT shares the fundamental framework with CDT, it adopts the target returns-conditioned policy structure. Consequently, the agent's behavior becomes sensitive to choices regarding target return and cost. This sensitivity limits the valid choices for target cost and reward return pairs to the PF points. To address this limitation, CQDT employs the same data enhancement strategy as CDT to improve the relabeled data. We list the details of the data augmentation technique in Algorithm 4.

```
0: Trajectories dataset \(\mathcal{T}\), iteration number \(N\), max length \(T\) of trajectories in \(\mathcal{T}\)
1: Set \(c_{\min}=\min_{\tau\in\mathcal{T}}C(\tau)\), \(c_{\max}=\max_{\tau\in\mathcal{T}}C(\tau)\), \(r_{\max}=\max_{\tau\in\mathcal{T}}R(\tau)\)
2:for\(i=1,\ldots,N\)do
3:\(\kappa_{i}\sim\text{Uniform}(c_{min},c_{max})\) // Sample a cost return
4:\(\rho_{i}\)\(\triangleright\) Uniform(PF(\(\kappa_{i},\mathcal{T}),r_{max}\)) // Sample a reward return above the PF value
5:\(\tau_{i}^{*}\leftarrow\arg\max_{\tau\in\mathcal{T}}R(\tau)\), s.t. \(C(\tau)\leq\kappa_{i}\) // Find the closest and safe Pareto trajectory
6: Generate \(\hat{\tau}_{i}\), where \(\hat{\tau}_{i}.R_{t}\leftarrow\tau_{i}^{*}.R_{t}+\rho_{i}-R(\tau_{i}^{*})\), \(\hat{\tau}_{i}.C_{t}\leftarrow\tau_{i}^{*}.C_{t}+\kappa_{i}-C(\tau_{i}^{*})\) for all \(1\leq t\leq T\) // Relabel the reward and cost return
7:\(\mathcal{T}\leftarrow\mathcal{T}\cup\{\hat{\tau}_{i}\}\) // Append the trajectory to the dataset
8:endfor
8: Augmented trajectories dataset \(\mathcal{T}\) ```

**Algorithm 4** PF augmentation

Finally, we generate the final sliced trajectories dataset \(\mathcal{T}^{K}\) which includes consistent trajectory \(\tau\) with length \(K\). To generate it, for each \(\tau\in\mathcal{T}\), we regenerate new trajectories dataset \(\tau^{1},\ldots,\tau^{T-K}\) as follows. For \(\tau^{t}\), we set its last RTG and CTG as \(\tau^{t}.R_{t+K}\leftarrow\tau.R_{t+K}\) and \(\tau^{t}.C_{t+K}\leftarrow\tau.C_{t+K}\). Then we repeatedly apply \(\tau^{t}.R_{i}\leftarrow\tau.r_{i}+\tau^{t}.R_{i+1}\) and \(\tau^{t}.C_{i}\leftarrow\tau.c_{i}+\tau^{t}.C_{i+1}\) for \(i=t+K-1,\ldots,t\). We summarize it in Algorithm 5.

## Appendix B Experiment Setting and Hyperparameters

### Tasks Description

In our experiments, we employ the BulletSafetyGym environment, which is a suite built atop the PyBullet physics simulator, resembles _SafetyGym_ but features shorter horizons and a larger number of agents [1, 34, 9, 8, 6]. Within BulletSafetyGym, we deploy three distinct agent models: the Car, Ant, and Drone. The Ant agent is designed as a four-legged creature with a spherical torso, while the Car agent, inspired by MIT's Racecar, features a four-wheeled configuration. The Drone agent is an aerial vehicle based on the AscTec Hummingbird quadrotor. These agents are employed to complete the Circle and Run tasks. In the Circle task, they move clockwise on a circle. The reward is defined as

\[r(s)=\frac{v^{T}[-y,x]}{1+3|r_{agent}-r_{circle}|}\] (6)

(6) is maximized when agents move quickly in a clockwise direction. Costs are incurred if the agent leaves the safety zone defined by the two yellow boundaries, i.e., \(c(s)=\mathbb{I}[|x|>x_{lim}]\), where \(x\) is the position on the x-axis. In the Run task, agents earn rewards for navigating an avenue located between two non-physical, penetrable safety boundaries that incur costs upon breach. Additional costs are applied if agents surpass an agent-specific velocity threshold.

### Dataset Collection

The dataset collection process is the same as [29], and we include it for completeness. In the data collection process, we use various algorithms and distinct cost thresholds tailored to each envi

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline
**Task** & **Cost Start** & **Cost End** & **Epoch Start** & **Epoch End** & **Epoch Number** & **Max Trajectory Length** \\ \hline CarCircle & 5 & 100 & 100 & 900 & 1000 & 1500 \\ CarRun & 5 & 100 & 100 & 900 & 1000 & 1500 \\ AntRun & 5 & 200 & 400 & 2500 & 2600 & 2000 \\ DroneCircle & 5 & 150 & 500 & 2500 & 2600 & 2000 \\ DroneRun & 100 & 5 & 50 & 800 & 1000 & 1500 \\ \hline \end{tabular}
\end{table}
Table 2: The hyperparameter configurations for dataset collection algorithms across diverse tasks. The parameters _Cost Start_, _Cost End_, _Epoch Start_, and _Epoch End_ are utilized to modify the cost limitations.

ronment. The algorithms utilized for dataset acquisition include CPO, FCOOPS, PPOLagrangian, TRPOLagrangian, DDPGLagrangian, SACLagrangian, and CVPO. Notably, PPOLagrangian, TRPOLagrangian, DDPGLagrangian, and SACLagrangian are composite methods combining PPO [1], TRPO [35], DDPG [26], SAC [15], and PID Lagrangian [37] techniques, respectively. Among these algorithms, the first four belong to the category of On-Policy algorithms, while DDPGLagrangian and SACLagrangian fall under Off-On-Policy algorithms. On the other hand, CVPO is classified as an Off-Policy algorithm. Table 2 outlines the hyperparameters utilized for collecting datasets across different tasks, and Table 3 presents detailed information about the constructed datasets.

### Hyperparameters

We list the hyperparameters for CQDT here as well as the hyperparameters employed in the training of CPQ. Specifically, within CPQ, we utilize the \(Q_{r}\) and \(Q_{c}\) functions for trajectory relabeling. The hyperparameters for CPQ training are listed in Table 4. Meanwhile, the hyperparameters for CDT and CQDT training are comprehensively detailed in Table 5.

## Appendix C Result Details and Discussions

### Ablation Study

In assessing the impact of various components within CQDT, we conducted two ablation studies.

**Joint Relabeling.** This is denoted as Ablation 1, aiming to study whether our adopted relabeling strategy in Algorithm 2 is necessary. We compare it with an alternative relabeling approach which does not take both the reward and cost into consideration together. Instead, the alternative relabeling approach relabels the RTG and CTG similar to what QDT does, which relabels them separately. For a trajectory \(\tau\), the alternative approach obtains \(V_{r}^{\tau}\) and \(V_{c}^{\tau}\) the same as Algorithm 2, then for each \(t=T+1,...,2\), it relabels \(\tau.R_{t-1}\rightarrow\tau.r_{t-1}+\max(\tau.R_{t},V_{r}^{\tau}(\tau.s_{t}))\) when \(V_{r}^{\tau}(s_{\mathcal{L}})\geq\tau.R_{\mathcal{L}}\), and it relabels \(\tau.C_{t-1}\rightarrow\tau.c_{t-1}+\min(\tau.C_{t},V_{c}^{\tau}(\tau.s_{t}))\) when \(V_{c}^{\tau}(s_{t})\leq\tau.C_{t}\). The main difference between such a relabeling strategy and our adopted strategy for CQDT is the separate consideration for reward and cost relabeling. Our result reveals that with such a relabeling process, CQDT performance is notably worse in scenarios such as AntRun and DroneCircle, which exhibit poor CPQ performance

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline Bench. & Task & Max TS & Act. Space & State Space & MaxCost & MinCost & MaxReward & MinReward & Traj. \\ \hline \multirow{4}{*}{BulletSafetyQym} & CarCircle & 300 & 2 & 8 & 100 & 0 & 534.306 & 3.484 & 1450 \\  & CarRun & 200 & 2 & 7 & 40 & 0 & 574.653 & 204.287 & 651 \\  & AntRun & 200 & 8 & 33 & 150 & 0 & 955.481 & 0 & 1816 \\  & DroneCircle & 300 & 4 & 18 & 100 & 0 & 996.389 & 207.79 & 1923 \\  & DroneRun & 200 & 4 & 17 & 140 & 0 & 682.83 & 10.557 & 1990 \\ \hline \end{tabular}
\end{table}
Table 3: Description of the experimental datasets: _Max TS_ denotes the maximum length of an episode, while _Act. Space_ and _State Space_ represent the dimensions of action and state, respectively. _MaxCost_, _MinCost_, _MaxReward_, and _MinReward_ represent the cumulative reward and cumulative cost obtained by each trajectory. _Traj._ indicates the number of trajectories in the dataset.

when cost return and reward return are considered independently. This indicates that the accuracy of value function predictions significantly influences model performance during the relabeling process, while our proposed CQDT method effectively minimizes the negative impacts caused by inaccuracies in value function predictions.

**PF Augmentation.** The second study, denoted as Ablation 2, evaluates CQDT without the PF-based augmentation step [30]. Our results reveal that the PF-based augmentation is essential for ensuring the model's security under various target cost settings. For instance, in the CarRun task, the absence of the PF augmentation technique results in a policy that exceeds cost limitations.

### Detailed Results from Value Function Experiment

In this section, we explore the utilization of diverse value functions for state value estimation, substituting RTG and CTG in the original trajectories. We then retrain our CQDT model with the replaced dataset and analyze the differences in results. We systematically investigate the incorporation of \(Q_{r}\) and \(Q_{c}\) as value functions for state value estimation in CPQ, BCQ-Lagrangian (BCQL), and BEAR-Lagrangian (BEARL). The algorithmic details of CPQ are outlined in Algorithm 3. Combining BCQ and BEAR with the Lagrangian approach, which utilizes adaptive penalty coefficients to enforce constraints, results in the formulation of BCQ-Lagrangian and BEAR-Lagrangian.

**Implementation Details of BCQ** For details regarding the BCQ, please refer to Algorithm 6. BCQ requires a tuple dataset \(B=\{(s,a,r,s^{\prime})\}\) and returns policy \(\pi\) and reward critic \(Q\). Here, \(\pi\) is

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline  & CarCircle & CarRun & AntRun & DroneCircle & DroneRun \\ \hline \(state\_dim\) & 8 & 7 & 33 & 18 & 17 \\ \(action\_dim\) & 2 & 2 & 8 & 4 & 4 \\ \(max\_action\) & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\ \(actor\_hidden\_size\) & \([256,256]\) & \([256,256]\) & \([256,256]\) & \([256,256]\) & \([256,256]\) \\ \(critic\_hidden\_size\) & \([256,256]\) & \([256,256]\) & \([256,256]\) & \([256,256]\) & \([256,256]\) \\ \(VAE\_hidden\_size\) & 400 & 400 & 400 & 400 & 400 \\ \(sample\_action\_num\) & 10 & 10 & 10 & 10 & 10 \\ \(Q_{r}\)\(number\) & 2 & 2 & 2 & 2 & 2 \\ \(Q_{c}\)\(number\) & 2 & 2 & 2 & 2 & 2 \\ \(episode\_len\) & 300 & 200 & 200 & 300 & 200 \\ \(batchsize\) & 2048 & 2048 & 2048 & 2048 & 2048 \\ \(update\_steps\) & 100000 & 100000 & 100000 & 100000 & 1000000 \\ \(vae\_lr\) & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 \\ \(critic\_lr\) & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 \\ \(actor\_lr\) & 0.0001 & 0.0001 & 0.0001 & 0.0001 & 0.0001 \\ \(cost\_limitation\) & \((10:100,10)\) & \((5:45,5)\) & \((15:150,15)\) & \((10:110,10)\) & \((15:135,15)\) \\ \hline \end{tabular}
\end{table}
Table 4: Parameters utilized in training CPQ. \(range(x:y,z)\) means the cost limitation begins at \(x\) and increases by \(z\) each step until it approaches \(y\).

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline  & CarCircle & CarRun & AntRun & DroneCircle & DroneRun \\ \hline \(state\_dim\) & 8 & 7 & 33 & 18 & 17 \\ \(action\_dim\) & 2 & 2 & 8 & 4 & 4 \\ \(max\_action\) & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\ \(embedding\_dim\) & 128 & 128 & 128 & 128 & 128 \\ \(seq\_len\) & 10 & 10 & 10 & 10 & 10 \\ \(episode\_len\) & 300 & 200 & 200 & 300 & 200 \\ \(num\_layers\) & 3 & 3 & 3 & 3 & 3 \\ \(num\_heads\) & 8 & 8 & 8 & 8 & 8 \\ \(attention\_dropout\) & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\ \(residual\_dropout\) & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\ \(embedding\_dropout\) & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\ \(action\_head\_layers\) & 1 & 1 & 1 & 1 & 1 \\ \(target\_descnt\_return\) & \([500,(0\cdot 120,10)]\) & \([575,(50\cdot 50,5)]\) & \([900,(0\cdot 200,20)]\) & \([900,(0\cdot 120,20)]\) & \([600,(0\cdot 200,20)]\) \\ \(target\_descnt\_return\) & \([(0\cdot 600,500),90]\) & \([(70\cdot 700,70),35]\) & \([(0\cdot 1100,100),130]\) & \([(300\cdot 1400,100),90]\) & \([(0\cdot 1000,100),120]\) \\ \(batches\) & 2048 & 2048 & 2048 & 2048 & 2048 \\ \(learning\_rate\) & 0.0001 & 0.0001 & 0.0001 & 0.0001 & 0.0001 \\ \(update\_steps\) & 100000 & 100000 & 100000 & 100000 & 100000 \\ \(weight\_decay\) & 0.0001 & 0.0001 & 0.0001 & 0.0001 & 0.0001 \\ \hline \end{tabular}
\end{table}
Table 5: Parameters utilized in training CDT and CQDT. Shared parameters are denoted in black fonts, while the independent parameters of CQDT are marked in blue fonts.

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_EMPTY:17]

**Robustness Validation.** During the robustness validation phase, CQDT's performance initially benefits from maintaining a constant target cost while gradually increasing the target return, leading to an augmentation in cumulative reward without violating cost constraints. However, once the target return reaches a certain threshold, further increases do not result in a proportional rise in cumulative reward. This phenomenon occurs because the target return setting guides CQDT's prediction process but does not alter the intrinsic training process, which the model architecture and the training dataset determine. If the target return setting significantly exceeds the cumulative reward of the optimal trajectory in the training dataset, CQDT's performance may not experience substantial improvements. Despite CQDT's stitching ability, which allows cumulative rewards to increase when the target return setting exceeds the maximum threshold in the dataset, the positive effect of this ability is limited.

**Zero-shot Adaptation Validation.** In the zero-shot adaptation validation experiment, where the target return remains constant while the target cost varies, the analysis of cumulative rewards across five tasks reveals an initial increase followed by stabilization. As the target cost setting gradually increases, the constraints on cost become more relaxed, thereby enhancing the strategy's ability to maximize cumulative rewards during the learning process. However, when the target cost setting exceeds the maximum cumulative cost threshold in the dataset, further increases in the target cost do not enhance the strategy's ability to maximize cumulative rewards, leading to no further increase in cumulative rewards obtained during the test phase.

Figure 10: Visualization of the dataset used to validate cost stitching ability.

Indeed, while CQDT consistently outperforms CDT in both robustness and zero-shot adaptation experiments, the challenge of selecting appropriate target return and target cost values remains common to both approaches. Optimal choices for these parameters should align with the specific characteristics of the task training dataset. Setting a larger target return and a smaller target cost may not be advisable; a more viable approach involves tailoring these targets based on the inherent properties of the task-specific training data. This task-specific customization ensures a more effective and contextually appropriate utilization of the reinforcement learning framework.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & CarCircle & CarRun & AntRun & DroneCircle & DroneRun \\ \hline \(\kappa\) & 90 & 35 & 130 & 90 & 120 \\ Target Return & 500 & 575 & 900 & 900 & 600 \\ Target Cost & 90 & 35 & 130 & 90 & 120 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Parameter Settings for the Stitching Ability Validation Experiment

Figure 11: Results of robustness verification to different reward returns. Each column represents a task. The x-axis denotes the target return. The first row shows the evaluated reward, and the second row shows the evaluated cost, both under different target return. The dashed line represents the predefined cost limitation.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \hline Task & Experiment & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline \multirow{2}{*}{CarCircle} & Robustness & \(\phi\)000 & \(\epsilon\)090100 & \(\phi\)02000 & \(\phi\)09200 & \(\phi\)09300 & \(\phi\)09300 & \(\phi\)09400 & \(\phi\)09400 & \(\phi\)09400 & \(\phi\)09500 & \(\phi\)09500 & \(\phi\)09550 \\ \cline{2-11}  & Zero-shot & \(\chi\)02600 & \(\chi\)03600 & \(\chi\)04500 & \(\chi\)05700 & \(\chi\)06000 & \(\chi\)07500 & \(\chi\)06000 & \(\chi\)09000 & \(\chi\)09000 & \(\chi\)09000 & \(\chi\)09000 & \(\chi\)09000 & \(\chi\)09000 & \(\chi\)10000 \\ \hline \multirow{2}{*}{CarRun} & Robustness & \(\chi\)5570 & \(\chi\)5140 & \(\chi\)536210 & \(\chi\)536200 & \(\chi\)53650 & \(\chi\)536400 & \(\chi\)536400 & \(\chi\)536400 & \(\chi\)53650 & \(\chi\)53630 & \(\chi\)536700 \\ \cline{2-11}  & Zero-shot & \(\chi\)0675 & \(\chi\)575 & \(\chi\)10575 & \(\chi\)15755 & \(\chi\)236575 & \(\chi\)236575 & \(\chi\)30575 & \(\chi\)30575 & \(\chi\)40575 & \(\chi\)40575 & \(\chi\)4575 \\ \hline \multirow{2}{*}{AntRun} & Robustness & \(\chi\)1300 & \(\chi\)13070 & \(\chi\)130200 & \(\chi\)130300 & \(\chi\)130400 & \(\chi\)130500 & \(\chi\)130600 & \(\chi\)130700 & \(\chi\)130600 & \(\chi\)130600 & \(\chi\)1306000 & \(\chi\)1306000 \\ \cline{2-11}  & Zero-shot & \(\chi\)09000 & \(\chi\)20900 & \(\chi\)40500 & \(\phi\)09000 & \(\phi\)09000 & \(\phi\)100000 & \(\phi\)120000 & \(\chi\)1404000 & \(\chi\)160000 & \(\chi\)1306000 \\ \hline \multirow{2}{*}{DroneCircle} & Robustness & \(\phi\)09000 & \(\phi\)09000 & \(\phi\)09000 & \(\phi\)09000 & \(\phi\)09000 & \(\phi\)09000 & \(\phi\)090000 & \(\phi\)090000 & \(\phi\)090000 & \(\phi\)090000 & \(\phi\)090000 & \(\phi\)090000 & \(\phi\)090000 & \(\phi\)100000 \\ \hline \multirow{2}{*}{DroneRun} & Robustness & \(\chi\)12000 & \(\chi\)12000 & \(\chi\)120200 & \(\chi\)120300 & \(\chi\)1204000 & \(\chi\)1204000 & \(\chi\)1204000 & \(\chi\)1204000 & \(\chi\)1204000 & \(\chi\)1204000 & \(\chi\)1204000 & \(\chi\)1204000 & \(\chi\)1204000 \\ \hline \multirow{2}{*}{DroneRun} & Zero-shot & \(\chi\)06000 & \(\chi\)20600 & \(\chi\)04600 & \(\phi\)06000 & \(\phi\)06000 & \(\chi\)100600 & \(\chi\)120600 & \(\chi\)140600 & \(\chi\)160600 & \(\chi\)180600 \\ \hline \end{tabular}
\end{table}
Table 7: Settings of variables cX-r[01-10] and c[01-10]-rX in robustness validation and zero-shot adaptation experiments across different tasks. During the evaluation stage, the for Target Return and Target Cost correspond to the actual values in the environment, rather than the normalized values.

Figure 12: Results of zero-shot adaptation to different cost returns. Each column represents a task. The x-axis denotes the target cost return. The first row and the second row display the evaluated reward and cost under different target costs, respectively. The dashed line represents the predefined cost limitation, while the solid line indicates the maximum cost of trajectories included in the dataset.

[MISSING_PAGE_EMPTY:20]