# Benchmarking Large Language Models on CMExam - A Comprehensive Chinese Medical Exam Dataset

 Junling Liu\({}^{1}\)\({}^{*}\) Peilin Zhou\({}^{2}\)\({}^{\dagger}\) Yining Hua\({}^{3,4}\)\({}^{\dagger}\) Dading Chong\({}^{5}\)

Zhongyu Tian\({}^{6}\) Andrew Liu\({}^{5}\) Helin Wang\({}^{7}\) Chenyu You\({}^{8}\)

Zhenhua Guo\({}^{9}\) Lei Zhu\({}^{10}\) Michael Lingzhi Li\({}^{4,1}\)\({}^{1}\)

\({}^{1}\)Alibaba Group \({}^{2}\)Hong Kong University of Science and Technology (Guangzhou)

\({}^{3}\)Harvard University \({}^{4}\)Boston Children's Hospital \({}^{5}\)Peking University

\({}^{6}\)Second Affiliated Hospital of Zhejiang University School of Medicine \({}^{7}\)Johns Hopkins University

\({}^{8}\)Yale University \({}^{9}\)Tianyi Traffic Technology \({}^{10}\)Ant Group \({}^{11}\)Harvard Business School

{william.liuj, zhoupalin, andrew.promed, cszguo, zhulei0305}@gmail.com

1601213984@pku.edu.cn, zhongyutian@zju.edu.cn, hwang258@jhu.edu

yininghua@g.harvard.edu, chenyu.you@yale.edu, mili@hbs.edu

###### Abstract

Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce **CMExam**, sourced from the **C**hinese **N**ational **M**edical **L**icensing **E**x**amination. **CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including _disease groups_, _clinical departments_, _medical disciplines_, _areas of competency_, and _question difficulty levels_. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could generate relevant reasoning and demonstrate improved performance after finetuning, they fall short of a desired standard, indicating ample room for improvement. To the best of our knowledge, CMExam is the first Chinese medical exam dataset to provide comprehensive medical annotations. The experiments and findings of LLM evaluation also provide valuable insights into the challenges and potential solutions in developing Chinese medical QA systems and LLM evaluation pipelines.1

Footnote 1: The dataset and relevant code are available at https://github.com/williamliujl/CMExam

## 1 Introduction

Recent advancements brought by large language models (LLMs) such as T5 (Raffel et al., 2020) and GPT-4 (OpenAI, 2023) have revolutionized natural language processing (NLP). However, evaluating LLMs in the medical field poses significant challenges due to the paucity of standardized and comprehensive datasets compiled from reliable and unbiased sources (Li et al., 2023; Zhou et al., 2023; Hua et al., 2024; Ye et al., 2023; Liu et al., 2023). Most existing medical datasets (Hendrycks et al., 2020; Abacha et al., 2019; Li et al., 2023; Zhou et al., 2022) for language model evaluationhave limitations that hinder comprehensive assessment of LLM performance (Nori et al., 2023). Many datasets are insufficient in terms of size and diversity, preventing a thorough evaluation of LLM capabilities. Furthermore, most datasets primarily focus on text generation tasks rather than utilizing clear choice evaluations, impeding objective and quantitative measurement of LLM performance. Additionally, a majority of these datasets (Li et al., 2023; Pal et al., 2022; Zhu et al., 2020) are sourced from online forums and consumer feedback, which could suffer from significant bias and error. These challenges are particularly amplified in non-English languages, such as Chinese, due to the pervasive inequality in language resources that exists in the NLP field (Bird, 2020; Zeng et al., 2022; Fang et al., 2023). Overall, due to the lack of qualified evaluation datasets, the strengths and weaknesses of LLMs in the medical field have not been fully studied.

In response, we present a novel dataset called CMExam to overcome these challenges and benchmark LLM performance. CMExam is sourced from authentic medical licensing exams. It contains more than 60K questions and utilizes the multiple-choice question format to allow standardized and objective evaluations. Questions in CMExam have corresponding solution explanations that can be used to test LLM's reasoning ability in an open-ended manner. To offer diverse perspectives for measuring LLM performance in the medical field, we created five additional question-wise annotation dimensions based on authenticated resources and objective metrics. To reduce the substantial time and labor costs associated with annotating large-scale datasets, we propose an innovative strategy called GPT-Assisted Annotation. This approach harnessed the power of GPT-4 to automate the initial annotation process. Subsequently, the annotated data underwent a meticulous review and manual verification conducted by two medical professionals. Figure 1 shows an example question from CMExam and the annotation process.

Furthermore, we benchmark the performance of general domain LLMs and medical domain LLMs on answer prediction (multiple-choice) and answer reasoning (open-ended) tasks of CMExam. This comprehensive assessment aims to highlight the strengths and weaknesses of various approaches in Chinese medical QA, with a focus on LLMs. The main findings of this benchmark are as follows:

* GPT-4 (OpenAI, 2023) demonstrates impressive zero-shot performance on the answer prediction task compared to other models, though still significantly lagging behind human performance.
* GPT-3.5 (Brown et al., 2020) and GPT-4 generated reasonable answers on the answer reasoning task despite low BLEU and ROUGE scores. This is because they tended to generate short answers with reasonable quality.
* Existing medical domain LLMs, such as Huatuo (Li et al., 2023) and DoctorGLM (Xiong et al., 2023), exhibit poor zero-shot performance on both tasks, indicating their limited coverage of medical knowledge and substantial room for improvement.
* Lightweight LLMs (e.g., ChatGLM (Du et al., 2022)) fine-tuned on CMExam with supervision achieve performance close to GPT-3.5 on the answer prediction task. They also significantly outperform GPT-3.5 and GPT-4 on the reasoning task while having only 3% of the parameters of GPT-3.5.

In summary, this study provides valuable insights into the performance of LLMs in medical contexts from multiple perspectives, benefiting both the artificial intelligence research community and the medical research community. Our findings contribute to a deeper understanding of the capabilities and limitations of LLMs in the medical domain. Additionally, the CMExam dataset and benchmark introduced in this study serve as valuable resources to inspire researchers to explore more effective

Figure 1: An example question of CMExam. Abbreviations: Circulatory System Diseases (Circ), Internal Medicine (IM), Clinical Medicine (ClinMed), Medical Fundamentals (MedFund).

ways of integrating medical knowledge into LLMs, ultimately enhancing their performance in medical applications.

## 2 Related Work

Medical Question-Answering DatasetsTable 1 presents a summary of medical QA datasets published after 2017. In particular, we focus on categorizing the data source and question types of the different datasets. Most existing medical QA datasets adopt an open-ended format, primarily because they were constructed directly from consumer questions and answers from doctors. However, multiple-choice and fill-in-the-blank questions provide a more standardized and objective evaluation, and only a small portion of medical QA datasets have adopted these formats. Notable examples include CliCR (Suster and Daelemans, 2018), MEDQA (Jin et al., 2021), MMLU (Hendrycks et al., 2020), MLEC-QA (Zeng et al., 2023a), and MedMCQA (Pal et al., 2022). Note that the multiple-choice questions in MultiMedQA (Singhal et al., 2022) come from MEDQA, MedMCQA, and MMLU.

Data source types generally determine the reliability of a dataset. Consumer questions collected from web sources require human review to ensure the correctness of the answers. As datasets grow in size, quality control becomes increasingly challenging (Li et al., 2023). In contrast, datasets built from case reports (e.g., CliCR), research literature (e.g., BioAsq (Krithara et al., 2023)), medical books, exams, and related practices (e.g., MMLU and MedMCQA) are often more reliable.

From Table 1, we observe that there are few datasets based on multiple-choice questions from authoritative sources. This characteristic distinguishes CMExam from the MLEC-QA dataset, which is also derived from the Chinese National Medical Licensing Examination. In essence, CMExam has been meticulously crafted as a foundational benchmark dataset. It introduces question explanations for reasoning ability inspection, incorporates expansive annotation facets with authoritative references, and includes question-wise medical competencies and difficulty ratings calculated from human performance. These features make CMExam an indispensable resource for authoritative LLM performance assessment and meaningful human-machine comparisons. Table 2 presents a list of innovations and characteristics of CMExam, which are discussed in detail in the following sections.

Other Benchmark Datasets of Large Language ModelsThe assessment of LLMs has witnessed significant progress, with the introduction of diverse benchmarks that evaluate different dimensions across multiple languages, models and tasks (Liu et al., 2023b,c; Zhou et al., 2023a). Many datasets focus on assessing natural language understanding and reasoning capabilities of LLMs. RACE (Lai et al., 2017) includes English exams for Chinese middle and high school students. TriviaQA (Joshi et al., 2017) consists of question-answer pairs authored by trivia enthusiasts. DROP (Dua et al., 2019)

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Language** & **Data Source Type** & \multicolumn{2}{c}{**Question Type**} \\ \cline{3-4}  & & **Multiple Choice** & **Open-ended** \\ \hline \multirow{8}{*}{English} & \multirow{8}{*}{Consumer Questions} & \multirow{8}{*}{MedMCQA (Pal et al., 2022)} & LiveQA-Med (Abacha et al., 2017) \\  & & & CliCR\({}^{\dagger}\)(Suster and Daelemans, 2018) \\  & & & HealthQA (Zhu et al., 2019) \\  & & & MEDIQQA (Abacha et al., 2019b) \\  & & & emQA\({}^{\dagger}\)(Pamparai et al., 2018) \\  & & & MedQA\({}^{\dagger}\)(Pamparai et al., 2019) \\  & & & MedQA\({}^{\dagger}\)(Bena Abacha and Denner-Fushman, 2019) \\  & & & MedICA\({}^{\dagger}\)(Abacha et al., 2019a) \\  & & & MEDIQA\({}^{\dagger}\)(Abacha et al., 2019a) \\  & & & MASH-QA (Zhu et al., 2020) \\ \cline{2-4}  & \multirow{4}{*}{Research, Books, or Exams} & MEDQA\({}^{\dagger}\)(Jin et al., 2021) \\  & & MMLU\({}^{\dagger}\)(Hendrycks et al., 2020) & BioASQ (Krithara et al., 2023) \\  & & MedMCQA (Pal et al., 2022) & MultiMedQA\({}^{\dagger+}\)(Singhal et al., 2022) \\ \hline \multirow{8}{*}{Chinese} & \multirow{4}{*}{Consumer Questions} & \multirow{4}{*}{-} & WebMedQA\({}^{\dagger+}\)(He et al., 2019) \\  & & & MedQA\({}^{\dagger}\)\(\sim\)\(10^{\dagger}\)(Zhang et al., 2017) \\ \cline{1-1}  & & & CModQA\({}^{\dagger}\)\(\sim\)\(20^{\dagger}\)(Zhang et al., 2018) \\ \cline{1-1}  & & & ChiMed (Tian et al., 2019) \\ \cline{1-1} \cline{2-4}  & & & Huatoo-2671(Li et al., 2023) \\ \cline{1-1} \cline{2-4}  & & & MLEC-QA\({}^{\dagger}\)(Zeng et al., 2023a) \\ \cline{1-1} \cline{2-4}  & & **CMExam\({}^{\dagger+1}\)(ours)** & **CMExam\({}^{\dagger+1}\)(ours)** \\ \hline \end{tabular}
\end{table}
Table 1: A review of medical QA datasets. \({}^{*}\) indicates availability of additional annotations with authoritative references, \({}^{\dagger}\) indicates availability of benchmarks, and \({}^{\ddagger}\) indicates datasets with more than 50K questionsevaluates reading comprehension with discrete reasoning and arithmetic components. GLUE (Wang et al., 2018) encompasses four existing NLU tasks, while SuperGLUE (Wang et al., 2019) extends it with a more challenging benchmark of eight language understanding tasks. Other datasets, such as HellaSwag (Zellers et al., 2019) and WinoGrande (Sakaguchi et al., 2021), focus on commonsense reasoning. TruthfulQA (Lin et al., 2021) includes health, law, finance, and politics, to assess LLMs' ability to mimic human falsehoods, while MMCU (Zeng, 2023) covers medical, legal, psychology, and education to evaluate multitask Chinese understanding. In addition to language understanding and reasoning, several datasets focus on specific subjects and topics, such as Python coding tasks (Chen et al., 2021), middle school mathematics questions (Cobbe et al., 2021) and defending against attacks (Yi et al., 2023; Xie et al., 2023; Pi et al., 2024). Notably, both C-Eval (Huang et al., 2023) and M3KE (Liu et al., 2023a) serve as multi-level multi-subject evaluation benchmarks, making them particularly suitable for assessing the capabilities of LLMs across multiple domains.

## 3 The CMExam Dataset

Data Collection and Pre-processingCMExam comprises authentic past licensed physician exams in the Chinese National Medical Licensing Examination (CNMLE) collected from the Internet. The CNMLE, also known as the Physician Qualification Examination, is a standardized exam that assesses applicants' medical knowledge and skills in China. It includes a written test with multiple-choice questions covering various medical subjects and a clinical skills assessment simulating patient diagnosis and treatment. We excluded questions that rely on non-textual information, including questions with external information such as images and tables, and questions with keywords "graph" and "table". Duplicate questions were removed from the dataset. In total, 96,161 questions, 68,119 of which were retained after pre-processing. The dataset was then randomly split into training/development/test sets with a ratio of 8:1:1. Each question in the dataset is associated with an ID, five candidate answers, and a correct answer. 85.24% of questions have brief solution explanations and questions in the test set contain additional annotations.

Data AnnotationCMExam provides a comprehensive analysis of LLM performance through five additional annotation dimensions. The first dimension involves disease groups based on the 11th revision of the International Classification of Diseases (ICD-11) (World Health Organization (WHO), 2021). ICD-11 is a globally recognized standard classification system for documenting and categorizing health conditions, consisting of 27 major disease groups. The second dimension comprises 36 clinical departments derived from the Directory of Medical Institution Diagnostic and Therapeutic Categories (DMIDTC) 2, published by the National Health Commission of China. DMIDTC is an authoritative guide used for categorizing and naming diagnostic and therapeutic subjects within healthcare institutes. In cases where the question cannot be successfully classified by ICD-11 or DMIDTC, the annotation is marked as "N/A". The third dimension refers to medical disciplines, which are categorized based on the List of Graduate Education Discisplinary Majors (2022) published by the Ministry of Education of the People's Republic of China3. This dimension encompasses seven categories representing study majors used in universities. The fourth dimension was created by two medical professionals within the team to assess the primary medical competency tested by each associated question. It consists of four categories. The fifth dimension represents five potential difficulty levels of each question, determined by analyzing the correctness rate observed in human performance data collected alongside the questions. For detailed information on these additional annotations including their potential values, please refer to Table 9, 12, 10, 11. And our proposed GPT-Assisted Annotation strategy is shown in supplementary materials.

\begin{table}
\begin{tabular}{l l r} \hline Annotation Content & References & Unique values \\ \hline Disease Groups & The 11th revision of ICD-11 & 27 \\ Clinical Departments & The Directory of Medical Institution Diagnostic and Therapeutic Categories (DMIDTC) & 36 \\ Medical Disciplines & List of Graduate Education Discisplinary Majors (2022) & 7 \\ Medical Compencies & Medical Professionals & 4 \\ Difficulty Level & Human Performance & 5 \\ \hline \end{tabular}
\end{table}
Table 2: Additional annotations of CMExam.

Dataset CharacteristicsThe CMExam dataset has several advantages over previous medical QA datasets regarding: 1)_Reliability and Authenticity_: CMExam is sourced exclusively from the CNMLE that undergoes rigorous review and validation processes, ensuring its accuracy and adherence to established medical standards. 2) _Standardization and Comprehensiveness_: CMExam includes both multiple-choice questions that ensure fair and objective evaluations of models' performance and question-wise open-ended reasoning that allows in-depth analysis and assessment of model reasoning abilities and comprehension. Despite the inherent absence of explanations within the CNMLE, we cross-referenced exam questions with solutions offered by diverse online medical examination preparation platforms, effectively enhancing the dataset's informational depth. CMExam reflects the comprehensive coverage of medical knowledge and reasoning required in clinical practice, as it is sourced from carefully designed national medical exams. The inclusion of five additional annotation dimensions enhances the dataset's rigor and offers valuable insights for in-depth evaluation and analysis. 3) _Scale_: CMExam consists of over 60K high-quality questions, providing a large and reliable dataset.

Data StatisticsThe dataset has a total of 68,119 questions, with 65,950 answers being single-choice and 2,169 being multiple-choice, with a maximum of five answer choices. Among all questions, 85.24% have associated solution explanations 3. Figure 2 shows additional statistics visualization and more basic statistics of CMExam can be seen in supplementary materials. Within the test set, 4,493 questions (65.97%) have corresponding disease group annotations. The most prevalent disease group is Traditional Medicine Disease Patterns (TMDP), followed by Digestive System Diseases, Certain Infectious (Digest) and parasitic Diseases (InfDis), Endocrine, Nutritional, or Metabolic Diseases (Endo), and Circulatory System Diseases (Circ). For the associated clinical department annotations, 4,965 questions (72.90%) have been assigned values. The two most frequently represented clinical departments are Internal Medicine (IM) and Traditional Chinese Medicine (TCM), with Dentistry (Dent) and Surgery (Surg) following closely. Every question in the test set has been labeled with a discipline, where Clinical Medicine (ClinMed) comprises the largest proportion. Additionally, each question has been categorized into a competency area, with Medical Fundamentals (MedFund) being the predominant category. The difficulty levels of the questions align with common exam patterns, with a greater number of easy questions and a smaller number of hard questions.

Footnote 3: https://www.yikaobang.com.cn/, http://www.jinyingjie.com/, https://www.lanjiyin.com.cn/

Figure 2: Additional CMExam statistics. For the question length distribution subplot, only the portion within IQR is shown.

Benchmarks

### Baselines, Settings, and Metrics

Model SelectionThe LLMs we benchmarked on the CMExam can be divided into two groups based on domains: 1) _General Domain LLMs_: This group comprises GPT3.5/4 (Brown et al., 2020; OpenAI, 2023), ChatGLM (Du et al., 2022; Zeng et al., 2023b), LLaMA (Touvron et al., 2023), Alpaca (Taori et al., 2023), and Vicuna (Chiang et al., 2023). These models are general-purpose language models trained on a massive amount of general-purpose corpora; 2) _Medical Domain LLMs_: This group can be further divided into two subgroups. The first subgroup consists of representative LLMs specifically designed for the medical domain, including DoctorGLM (Xiong et al., 2023) and Huatuo (Wang et al., 2023). DoctorGLM is a healthcare-specific language model initialized with ChatGLM-6B parameters and further fine-tuned on Chinese medical dialogues extracted from ChatGPT. Huatuo, on the other hand, is a knowledge-enhanced model, which builds upon the LLaMA architecture and is additionally supervised-fine-tuned with knowledge-based instruction data harvested from the Chinese medical knowledge graph (CMeKG). The second subgroup comprises medical LLMs that were constructed through supervised fine-tuning of LLMs using the CMExam training set. This subgroup includes models fine-tuned on BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), PromptCLUE (Zhang and Xu, 2022) (T5-based), BART (Shao et al., 2021), Huatuo, ChatGLM, LLaMA, Alpaca, and Vicuna.

Human PerformanceTo effectively gauge the medical proficiency of LLMs, incorporating a measure of human performance into the benchmarking process is of paramount importance. Therefore, during data collection, we preserved the accuracy of human responses for each question. Human performance is estimated by computing a weighted average of response accuracy within each dimension, with weights determined by the number of respondents. This design ensures a robust comparison of LLMs' performance relative to human capabilities, particularly when larger respondent samples contribute to a question's accuracy.

Experimental SettingFor GPT models, we leveraged OPENAI's API to access the GPT-3.5-turbo and GPT-4-0314 models, given that their open-source variants are currently unavailable. The LLaMA, Alpaca, and Vicuna models were used in their respective 7B versions, while ChatGLM was evaluated using its publicly accessible 6B version. Additionally, we performed fine-tuning on open-source models using the CMExam dataset. We used P-tuning V2 (Liu et al., 2021) for ChatGLM-6B, with the length of prefix tokens set to 128, and the learning rate set to 2e-2, LoRA (Hu et al., 2021) for LLaMA, Alpaca, Vicuna, and Huatuo models, with the rank set to 8, alpha set to 16, and dropout at 0.05. For BERT models, we followed the fine-tuning methods outlined in (Devlin et al., 2019), with batch size set to 16, learning rate set to 2e-4, hidden dropout probability set to 0.4, and maximum input length set to 192. The fine-tuning processes for all models except BERT involved a batch size of 64, a maximum input length, and a target length of 256. All fine-tuning was performed using NVIDIA V100 GPUs for 10 epochs.

MetricsWe assess model performance on multiple choice questions using accuracy and weighted F1 score. These metrics are commonly employed in information retrieval and question-answering tasks to evaluate model performance. For the open-ended solution explanations of CMExam, BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003) were used to evaluate the discrepancy between model-generated explanations and ground truth.

### Results and Analysis

Overall ComparisonWe first assessed the performance of general domain LLMs and medical domain LLMs for answer prediction and reasoning tasks. The results are displayed in Table 3. For the answer prediction task, GPT-4 significantly outperforms other methods, demonstrating a zero-shot performance with an accuracy of 61.6% and an F1 score of 0.617. While a performance gap still exists when compared to human performance (which stands at 71.6% accuracy), it's noteworthy that this gap has been greatly reduced from what was observed with GPT-3.5. Among lightweight, general domain LLMs, ChatGLM outperforms LLaMA, Alpaca, and Vicuna, likely attributable to their limited coverage of the Chinese corpus. This restriction seemingly hampers their ability to provide accurate responses to CMExam queries. Furthermore, a noticeable deficiency in zero-shot performance is evident in lightweight medical domain LLMs such as Huatuo, owing to their restricted medical corpus diversity, which hampers the acquisition of broad medical knowledge and accurate interpretation of CMExam questions. Our findings suggest that finetuning models with CMExam enhance their performance. For instance, with an accuracy of 45.3%, ChatGLM-CMExam is comparable to GPT-3.5's performance, despite utilizing only about 3% of the parameters employed by GPT-3.5. It is noteworthy that encoder-only LLMs, such as BERT and RoBERTa, remain a robust baseline for answer prediction tasks. Their performance can par with, or even exceed, that of certain decoder-only LLMs, such as LLaMA-CMExam and Alpaca-CMExam, despite having fewer parameters.

For the solution explanation task, we observe that GPT models performed poorly on the BLEU metric, likely due to their tendency to generate short explanations. However, they exhibited an advantage on the ROUGE metric. As DoctorGLM is unable to return answer options according to the prompt, we only report its performance in the solution explanation task. Through finetuning, LLM was able to generate more reasonable explanations. For instance, ChatGLM-CMExam achieved scores of 31.10 and 18.94 on BLEU-1 and BLEU-4, respectively, and scores of 43.94, 31.48, and 29.39 on the ROUGE metrics.

Results by Disease GroupsDrawing upon ICD-11 annotations (26 categories), we conducted an analysis of the performance of several LLMs across various categories. To mitigate the potential impact of random variability resulting from the number of questions, we limited our analysis to categories containing more than 100 questions. According to Table 4, LLMs have uneven performance and significant gaps in knowledge. GPT-4's accuracy ranges from 74.4% for _Neo_ to 44.3% for _TCMDP_, GPT-3.5's accuracy ranges from 63.9% for _Neo_ to 31.0% for _TCMDP_ and ChatGLM-CMExam's accuracy ranges from 54.7% for _Psy_ to 42.9% for _RESP_.

Results by Clinical DepartmentsTo compare model performance regarding the clinical department dimension (36 categories), we only analyzed categories with more than 50 questions to ensure result representativeness. Results presented in Table 5 highlight that the models show relatively high accuracy on questions associated with commonly encountered departments, such as Emergency Medicine (_EM_), Internal Medicine (_IM_) and Surgery (_Surg_). Their accuracy on questions associated with rarer departments, such as Traditional Chinese Medicine (_TCM_). There is a marked discrepancy in the average accuracy among different departments, with the highest being 50.9% and the lowest being only 13.9%. This observation suggests there are notable variations in medical knowledge and reasoning approaches among different departments. Consequently, it may be necessary to examine specific optimization strategies for different departments.

Results by Medical DisciplinesThen, we evaluated LLM performance across seven medical disciplines. As depicted in Table 6, the performance of LLMs across disciplines such as Traditional Chinese Medicine (_TCM_), Traditional Chinese Pharmacy (_TCPharm_), and Pharmacy (_Pharm_) was notably subpar, with all accuracy rates falling below 42%. This pattern suggests a potential deficiency

\begin{table}
\begin{tabular}{l l r r r r r r r r} \hline \hline \multirow{2}{*}{Model type} & \multirow{2}{*}{Models} & \multicolumn{3}{c}{Prediction} & \multicolumn{3}{c}{Reasoning} \\ \cline{3-10}  & & & Acc (\%) & F1 (\%) & BLEU-1 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L \\ \hline \multirow{6}{*}{General Domain} & GPT-3.5-Auto & 175B & 46.440.6 & 46.140.7 & 3.5640.67 & 1.4940.51 & 33.8040.19 & 16.3940.18 & 14.8340.13 \\  & GPT-4 & **51.641.41** & **61.740.1** & 0.1760.0 & 0.0640.00 & 29.7440.009 & 14.8440.04 & 11.5140.03 \\  & ChaGLM & 6B & 25.340.0 & 25.740.1 & 1.6514.08 & 0.505.040 & 26.5381.11 & 15.7540.05 & 17.9940.13 \\  & LLMaM & 7B & 0.440.0 & 0.340.0 & 11.9980.03 & 5.7040.00 & 27.3330.06 & 11.8880.03 & 10.7840.04 \\  & Vicuna & 7B & 5.080.0 & 4.840.1 & 20.1580.01 & 9.2640.00 & 84.3430.02 & 10.9640.01 & 6.3340.01 \\  & Alpaca & 7B & 5.850.0 & 8.440.0 & 25.400.00 & 25.0400.00 & 22.5200.00 & 9.5440.00 & 8.4400.00 \\ \hline \multirow{6}{*}{Medical Domain} & Hauto & 7B & 12.940.0 & 7.040.0 & 0.2140.00 & 0.1240.00 & 25.1140.08 & 11.5640.00 & 9.7340.02 \\  & MedApaca & 7B & 20.040.0 & 10.740.0 & 0.0800.00 & 0.0040.000 & 1.9040.00 & 0.0460.00 & 0.5240.03 \\  & DoctorGLM & 6B & - & 94.490.0 & 2.6540.3 & 2.11140.08 & 6.6801.01 & 9.9990.06 \\ \cline{1-1}  & PompeCLUE-base-CMExam & 0.1B & - & - & 18.7540.08 & 6.6540.00 & 40.8840.11 & 21.9940.11 & 18.3140.11 \\ \cline{1-1}  & Bart-base-CMExam & 0.1B & - & - & 23.0640.00 & 10.3540.06 & 44.330.09 & 22.9420.09 & 20.800.09 \\ \cline{1-1}  & Bart-large-chinese-CMExam & 0.1B & - & 23.0640.00 & 10.3540.06 & 44.330.09 & 22.9420.09 & 20.800.09 \\ \cline{1-1}  & Barl-large-chinese-CMExam & 0.1B & 31.8\(\pm\)0.2 & 31.260.2 & - & - & - & - & - \\ \cline{1-1}  & BERT-CMExam & 0.1B & 31.8\(\pm\)0.2 & 31.260.2 & - & - & - & - & - \\ \cline{1-1}  & RoBERT-CMExam & 0.3B & 37.140.1 & 36.740.4 & - & - & - & - & - \\ \cline{1-1}  & MedApaca-CMExam & 7B & 30.5\(\pm\)0.1 & 34.604.0 & 16.5340.08 & 9.7840.7 & 4.3140.85 & 27.0560.00 & 24.5540.43 \\ \cline{1-1}  & Huang-CMExam & 7B & 28.605.0 & 29.340.0 & 29.2490.1 & 16.7240.03 & 43.8520.42 & 25.860.00 & 22.1720.24 \\ \cline{1-1}  & ChaGLM-CMExam & 6B & 45.3\(\pm\)1.4 & 25.41 & **31.160.00** & **18.946.00** & 24.349.04 & **23.848.04** & **23.939.14** \\ \cline{1-1}  & LLMaM-CMExam & 7B & 18.4\(\pm\)0.5 & 20.605.0 & 29.250.23 & 16.250.10 & 1.658.040.12 & 25.536.00 & 22.6740.34 \\ \cline{1-1}  & Alpaca-CMExam & 7B & 21.1\(\pm\)0.6 & 24.940.4 & 29.5740.16 & 16.4040.12 & 45.4840.12 & 25.5360.18 & 2.27940.06 \\ \cline{1-1}  & Vicuna-CMExam & 7B & 27.3\(\pm\)0.5 & 28.240.3 & 29.82\(\pm\)0.03 & 17.3040.01 & 44.9840.16 & 26.2580.13 & 2.244.09.09 \\ \hline \hline Random & Random & - & 3.140.2 & 5.140.3 & - & - & - & - & - \\ \hline Human Performance & Human volunteers & - & 71.6 & - & - & - & - & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 3: Overall comparison on CMExam dataset. We **bold** the best result and underline the second best result.

in the exposure of these models to data within these categories. Conversely, disciplines such as _ClinMed_ and _Ph&PM_ demonstrated higher accuracy rates, likely due to the abundance of relevant data. The observed variability in performance across different disciplines underscores the distinctiveness of data characteristics and complexities inherent to each field, thereby advocating for discipline-specific model optimizations and enhancements.

Results by CompetenciesEvaluations based on medical competency areas aimed at a higher-level understanding of model capability in solving medical problems. As indicated in Table 7, the lowest average accuracy across LLMs was observed within the domain of mastering Medical Fundamentals (_MedFund_), with a meager average score of 42.1%. This result demonstrates that these models, predominantly trained on general textual data, have inadequate exposure to medical-specific data. While fine-tuning did provide some improvement, these models could benefit from additional medical scenario data to further augment their performance. It is worth highlighting that the average accuracy in the domain of Public Health Laws and Ethics (_PHL_) was reasonably high, notably achieving an average of 47.6%. In addition, the LLMs showcased their proficiency in accurate disease diagnosis.

Results by Question DifficultyTo evaluate model performance in tackling questions of varying levels of difficulty, we conducted experiments regarding the question difficulty dimension, which was calculated based on human exam-taker performance. As shown in Table 8, there's an evident trend where model accuracies decrease as question complexity rises. This pattern suggests that more sophisticated questions demand an extensive knowledge base and complex reasoning, which are challenging for the LLMs, thus reflecting patterns observed in human performance.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Categories** & **GPT-4** & **GPT-3.5** & **ChatGLM** & **ChatGLM-CMExam** & **Average** \\ \hline Neo & 74.4\(\pm\)2.2 & 63.9\(\pm\)1.4 & 32.4\(\pm\)1.6 & 51.9\(\pm\)0.2 & 55.6\(\pm\)0.8 \\ Psy & 74.0\(\pm\)0.7 & 62.0\(\pm\)1.7 & 33.3\(\pm\)1.3 & 54.7\(\pm\)0.8 & 56.0\(\pm\)0.9 \\ Factors & 70.0\(\pm\)1.0 & 57.5\(\pm\)1.4 & 28.0\(\pm\)1.1 & 51.1\(\pm\)1.4 & 51.6\(\pm\)0.5 \\ MSK & 65.9\(\pm\)0.8 & 53.8\(\pm\)0.8 & 29.2\(\pm\)0.4 & 53.5\(\pm\)0.0 & 50.6\(\pm\)0.4 \\ GU & 69.2\(\pm\)0.4 & 52.1\(\pm\)1.1 & 30.0\(\pm\)0.2 & 49.5\(\pm\)0.9 & 50.2\(\pm\)0.3 \\ Inj & 65.9\(\pm\)2.3 & 45.7\(\pm\)1.3 & 37.2\(\pm\)2.9 & 49.1\(\pm\)1.8 & 49.5\(\pm\)1.4 \\ Circ & 68.8\(\pm\)0.3 & 49.3\(\pm\)0.7 & 30.9\(\pm\)0.7 & 47.0\(\pm\)0.3 & 49.0\(\pm\)0.2 \\ Endo & 70.6\(\pm\)1.1 & 49.4\(\pm\)1.1 & 25.5\(\pm\)0.8 & 46.1\(\pm\)0.4 & 47.9\(\pm\)0.2 \\ Digest & 67.0\(\pm\)1.0 & 48.8\(\pm\)1.4 & 26.2\(\pm\)0.7 & 49.4\(\pm\)1.1 & 47.8\(\pm\)0.4 \\ InfDis & 66.0\(\pm\)0.5 & 49.2\(\pm\)0.8 & 27.5\(\pm\)0.6 & 48.2\(\pm\)0.8 & 47.7\(\pm\)0.4 \\ Neuro & 64.4\(\pm\)1.2 & 48.7\(\pm\)3.1 & 28.6\(\pm\)0.4 & 45.3\(\pm\)1.3 & 46.7\(\pm\)1.1 \\ OBST & 63.5\(\pm\)0.3 & 45.0\(\pm\)2.4 & 25.7\(\pm\)0.9 & 49.4\(\pm\)0.3 & 45.9\(\pm\)0.5 \\ BLOOD & 69.4\(\pm\)0.3 & 45.3\(\pm\)1.4 & 18.9\(\pm\)1.6 & 43.3\(\pm\)0.7 & 44.2\(\pm\)0.4 \\ Resp & 62.7\(\pm\)0.8 & 44.3\(\pm\)1.4 & 24.5\(\pm\)0.3 & 42.9\(\pm\)0.0 & 43.6\(\pm\)0.7 \\ N/A & 60.0\(\pm\)0.1 & 46.8\(\pm\)0.3 & 24.9\(\pm\)0.2 & 42.5\(\pm\)0.1 & 43.5\(\pm\)0.1 \\ TCMDP & 44.3\(\pm\)0.9 & 31.0\(\pm\)0.6 & 24.2\(\pm\)0.4 & 47.9\(\pm\)0.0 & 36.9\(\pm\)0.6 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparing disease classifications.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Categories** & **GPT-4** & **GPT-3.5** & **ChatGLM** & **ChatGLM-CMExam** & **Average** \\ \hline EM & 67.4\(\pm\)0.2 & 49.8\(\pm\)0.7 & 36.3\(\pm\)0.4 & 50.2\(\pm\)0.5 & 50.9\(\pm\)0.1 \\ OBGYN & 66.4\(\pm\)1.0 & 51.7\(\pm\)1.5 & 28.6\(\pm\)0.5 & 52.0\(\pm\)0.0 & 49.7\(\pm\)0.3 \\ IM & 70.2\(\pm\)0.6 & 51.8\(\pm\)0.8 & 26.0\(\pm\)1.1 & 47.9\(\pm\)0.9 & 49.0\(\pm\)1.0 \\ ID & 67.4\(\pm\)1.9 & 49.5\(\pm\)3.3 & 26.1\(\pm\)1.9 & 49.6\(\pm\)3.8 & 48.2\(\pm\)1.2 \\ Surg & 63.6\(\pm\)0.8 & 49.5\(\pm\)1.5 & 28.8\(\pm\)0.5 & 47.7\(\pm\)0.9 & 47.4\(\pm\)1.5 \\ ClinNutr & 68.3\(\pm\)2.4 & 48.3\(\pm\)2.9 & 29.9\(\pm\)1.1 & 47.8\(\pm\)0.5 & 47.1\(\pm\)0.7 \\ MedLabSci & 69.2\(\pm\)0.6 & 48.3\(\pm\)2.0 & 29.0\(\pm\)1.5 & 40.8\(\pm\)0.6 & 46.8\(\pm\)0.2 \\ Ped & 64.5\(\pm\)0.0 & 47.2\(\pm\)1.4 & 26.7\(\pm\)2.1 & 41.9\(\pm\)5.5 & 45.1\(\pm\)1.7 \\ N/A & 62.6\(\pm\)0.2 & 48.6\(\pm\)1.1 & 24.6\(\pm\)0.4 & 44.3\(\pm\)0.9 & 45.0\(\pm\)1.0 \\ Ophth & 60.9\(\pm\)0.5 & 39.1\(\pm\)0.8 & 21.8\(\pm\)0.8 & 54.0\(\pm\)0.2 & 44.0\(\pm\)0.8 \\ OccMed & 61.5\(\pm\)4.3 & 38.5\(\pm\)1.6 & 31.3\(\pm\)4.3 & 41.5\(\pm\)3.3 & 43.2\(\pm\)2.5 \\ DENT & 54.9\(\pm\)2.0 & 41.2\(\pm\)1.6 & 27.9\(\pm\)0.8 & 43.5\(\pm\)0.9 & 41.9\(\pm\)1.0 \\ TCM & 43.1\(\pm\)1.3 & 31.4\(\pm\)1.3 & 24.5\(\pm\)1.9 & 45.8\(\pm\)4.4 & 36.2\(\pm\)0.6 \\ ENT & 41.3\(\pm\)0.8 & 28.0\(\pm\)0.6 & 29.3\(\pm\)0.1 & 26.7\(\pm\)0.1 & 31.3\(\pm\)0.5 \\ ICM & 33.3\(\pm\)0.0 & 11.1\(\pm\)15.7 & 0.0\(\pm\)0.0 & 11.1\(\pm\)15.7 & 13.9\(\pm\)4.8 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparing clinical department.

Results by Question LengthFinally, to investigate if model performance is associated with input lengths, we compared their performance regarding question lengths. Figure 3 illustrates that Large Language Models (LLMs) generally show higher accuracy with problem lengths between 60 and 90. However, their performance seems to falter with problems that are either too short or overly long. Additionally, we noticed that the effect of question length varies across different LLMs. For instance, GPT models tend to incrementally improve as the problem length expands, performing optimally within the 50 to 90 range. Conversely, ChatGLM-CMExam's performance fluctuates noticeably with varying lengths, and it tends to fall short compared to GPT models when addressing longer problems.

## 5 Conclusion and Discussions

In this work, we developed CMExam, a dataset sourced from the stringent Chinese National Medical Licensing Examination, featuring 60,000+ multiple-choice questions, with detailed explanations. CMExam ensures reliability, validity, and adherence to medical standards. It also demonstrates the practicality of employing GPT-4 to automate the annotation process, which strikes a harmonious balance between efficiency and cost-effectiveness while maintaining the desired level of accuracy and reliability of the annotation. Utilizing this large and reliable corpus, we tested several LLMs for answer selection and reasoning tasks. A performance gap was observed between LLMs and human experts, signaling the need for additional LLM research. CMExam's standardization and comprehensiveness also ensure objective evaluations of models while enabling in-depth analysis of their reasoning capabilities. The questions cover a wide spectrum of medical knowledge, augmented with five additional annotation dimensions for rigorous evaluation. This study aims to spur further exploration of LLMs in medicine by providing a comprehensive benchmark for their evaluation.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Categories** & **GPT-4** & **GPT-3.5** & **ChatGLM** & **ChatGLM-CMExam** & **Average** \\ \hline ClinMed & 67.9\(\pm\)0.1 & 51.4\(\pm\)0.4 & 27.3\(\pm\)0.3 & 48.9\(\pm\)0.4 & 48.8\(\pm\)0.7 \\ PH\&PM & 68.2\(\pm\)0.4 & 52.7\(\pm\)1.7 & 26.2\(\pm\)0.3 & 47.3\(\pm\)1.0 & 48.6\(\pm\)0.5 \\ ICWM & 56.1\(\pm\)0.1 & 40.0\(\pm\)2.3 & 29.4\(\pm\)0.8 & 53.6\(\pm\)0.7 & 44.8\(\pm\)0.9 \\ Dent & 59.5\(\pm\)0.7 & 43.9\(\pm\)1.9 & 28.5\(\pm\)1.1 & 45.3\(\pm\)0.6 & 44.3\(\pm\)0.3 \\ Pharm & 61.1\(\pm\)0.4 & 46.3\(\pm\)0.5 & 23.2\(\pm\)0.2 & 37.0\(\pm\)0.1 & 41.9\(\pm\)0.3 \\ TCM & 53.5\(\pm\)0.4 & 35.9\(\pm\)0.2 & 24.1\(\pm\)0.3 & 49.1\(\pm\)0.0 & 40.6\(\pm\)1.1 \\ TCPharm & 45.4\(\pm\)1.2 & 35.6\(\pm\)0.1 & 24.1\(\pm\)1.0 & 43.1\(\pm\)0.4 & 37.1\(\pm\)0.5 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparing medical discipline.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Categories** & **GPT-4** & **GPT-3.5** & **ChatGLM** & **ChatGLM-CMExam** & **Average** \\ \hline Diag & 70.1\(\pm\)5.5 & 50.9\(\pm\)2.1 & 30.9\(\pm\)2.8 & 51.6\(\pm\)1.0 & 50.9\(\pm\)1.4 \\ PHL & 64.2\(\pm\)0.7 & 50.0\(\pm\)0.5 & 26.8\(\pm\)0.3 & 49.6\(\pm\)0.1 & 47.6\(\pm\)0.3 \\ Treat & 56.5\(\pm\)0.5 & 43.0\(\pm\)1.1 & 25.7\(\pm\)0.2 & 47.4\(\pm\)0.6 & 43.2\(\pm\)0.8 \\ MeFund & 58.3\(\pm\)0.3 & 44.6\(\pm\)0.7 & 23.9\(\pm\)0.5 & 41.6\(\pm\)0.4 & 42.1\(\pm\)0.9 \\ N/A & 54.8\(\pm\)0.2 & 30.4\(\pm\)0.4 & 23.7\(\pm\)0.1 & 38.5\(\pm\)0.2 & 36.9\(\pm\)0.3 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparing LLMsâ€™ competencies.

Figure 3: Results stratified by question length.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Categories** & **GPT-4** & **GPT-3.5** & **ChatGLM** & **ChatGLM-CMExam** & **Average** \\ \hline Easy & 74.6\(\pm\)0.1 & 58.5\(\pm\)0.6 & 31.4\(\pm\)0.2 & 61.5\(\pm\)0.3 & 56.5\(\pm\)0.4 \\ Manageable & 63.9\(\pm\)0.2 & 47.4\(\pm\)0.7 & 25.9\(\pm\)0.5 & 46.1\(\pm\)0.3 & 45.8\(\pm\)0.6 \\ Moderate & 51.3\(\pm\)0.6 & 36.8\(\pm\)0.8 & 23.0\(\pm\)0.4 & 34.5\(\pm\)0.6 & 36.4\(\pm\)0.7 \\ Difficult & 36.4\(\pm\)0.9 & 26.2\(\pm\)0.7 & 18.9\(\pm\)0.5 & 24.3\(\pm\)0.9 & 26.5\(\pm\)0.6 \\ Extremely difficult & 27.2\(\pm\)1.0 & 21.4\(\pm\)2.2 & 15.8\(\pm\)1.0 & 12.2\(\pm\)1.1 & 19.1\(\pm\)1.1 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results by question difficulty.

We anticipate CMExam to contribute significantly to future advancements of LLMs, particularly in handling medical question-answering tasks.

LimitationsFirstly, while CMExam is derived from meticulously designed medical examinations, our process of excluding questions requiring non-textual information may inadvertently affect the balance of the remaining questions, potentially introducing unexpected biases. It is critical to acknowledge this aspect while interpreting any findings or analyses conducted using this dataset. Furthermore, the current BLEU and ROUGE metrics primarily evaluate the explanation task, but these measures are insufficient for assessing the reasonableness of the answer. In future work, we will incorporate human evaluation to provide a more comprehensive assessment of the models.

EthicsCMExam is a dataset derived from the Chinese National Medical Licensing Examination, which aligns with numerous datasets containing similar National Medical Licensing Examinations (Zeng et al., 2023; Hendrycks et al., 2020; Jin et al., 2021; Pal et al., 2022; Singhal et al., 2022). We have ensured adherence to applicable legal and ethical guidelines during data collection and use. The authenticity and accuracy of the exam questions have been thoroughly verified, providing a reliable basis for evaluating LLMs. Please note that the CMExam dataset is intended for academic and research purposes only. Any commercial use or other misuse that deviates from this purpose is expressly prohibited. We urge all users to respect this stipulation in the interest of maintaining the integrity and ethical use of this valuable resource.

Societal ImpactsWhile CMExam aims to enhance LLM evaluations in the medical field, it should not be misused for assessing individual medical competence or for patient diagnosis. Conclusions drawn from models trained on this dataset should acknowledge its limitations, especially given its single source and the specific context of the CNMLE. The use of this dataset should strictly be limited to research purposes to avoid potential misuse.

## References

* B. Abacha, E. Agichtein, Y. Pinter, and D. Demner-Fushman (2017)Overview of the medical question answering task at TREC 2017 LiveQA.. In TREC, pp. 1-12. Cited by: SS1.
* A. B. Abacha, Y. Mabet, M. Sharp, T. R. Goodwin, S. E. Shooshan, and D. Demner-Fushman (2019)Bridging the Gap Between Consumers' Medication Questions and Trusted Answers.. In MedInfo, pp. 25-29. Cited by: SS1.
* A. B. Abacha, C. Shivade, and D. Demner-Fushman (2019)Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering. In Proceedings of the 18th BioNLP Workshop and Shared Task, pp. 370-379. Cited by: SS1.
* A. B. Abacha and D. Demner-Fushman (2019)A question-entailment approach to question answering. BMC bioinformatics20 (1), pp. 1-23. Cited by: SS1.
* S. Bird (2020)Decolonising speech and language technology. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 3504-3519. Cited by: SS1.
* T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. (2021)Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Cited by: SS1.
* W. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing (2023)Vicuna: an open-source chatbot Impressing GPT-4 with 90%* ChatGPT Quality. Note: https://lmsys.org/blog/2023-03-30-vicuna/ Cited by: SS1.
* K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. (2021)Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Cited by: SS1.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. _ArXiv_ abs/1810.04805 (2019).
* Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. 320-335.
* Dua et al. (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. _arXiv preprint arXiv:1903.00161_ (2019).
* Fang et al. (2023) Changchang Fang, Jitao Ling, Jing Zhou, Yue Wang, Xiaolin Liu, Yuan Jiang, Yifan Wu, Yixuan Chen, Zhichen Zhu, Jianyong Ma, et al. 2023. How does ChatGPT4 preform on Non-English National Medical Licensing Examination? An Evaluation in Chinese Language. _medRxiv_ (2023), 2023-05.
* He et al. (2019) Junqing He, Mingming Fu, and Manshu Tu. 2019. Applying deep matching networks to Chinese medical question answering: a study and a dataset. _BMC medical informatics and decision making_ 19, 2 (2019), 91-100.
* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_ (2020).
* Hu et al. (2021) J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. _ArXiv_ abs/2106.09685 (2021).
* Hua et al. (2024) Yining Hua, Fenglin Liu, Kailai Yang, Zehan Li, Yi-han Sheu, Peilin Zhou, Lauren V Moran, Sophia Ananiadou, and Andrew Beam. 2024. Large Language Models in Mental Health Care: a Scoping Review. _arXiv preprint arXiv:2401.02984_ (2024).
* Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuacheng Lv, Yikai Zhang, Jiayi Lei, et al. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. _arXiv preprint arXiv:2305.08322_ (2023).
* Jin et al. (2021) Di Jin, Eileen Pan, Nassim Oufatolde, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. _Applied Sciences_ 11, 14 (2021), 6421.
* Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. _arXiv preprint arXiv:1705.03551_ (2017).
* Krithara et al. (2023) Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras. 2023. BioASQ-QA: A manually curated corpus for Biomedical Question Answering. _Scientific Data_ 10, 1 (2023), 170.
* Lai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. _arXiv preprint arXiv:1704.04683_ (2017).
* Li et al. (2023) Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu, Prayag Tiwari, Xiang Wan, and Benyou Wang. 2023. Huatuo-26M, a Large-scale Chinese Medical QA Dataset. _arXiv preprint arXiv:2305.01526_ (2023).
* Lin and Hovy (2003) Chin-Yew Lin and Eduard H. Hovy. 2003. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics. In _North American Chapter of the Association for Computational Linguistics_.
* Lin et al. (2021) Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. _arXiv preprint arXiv:2109.07958_ (2021).
* Lin et al. (2020)Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, Xiaohan Peng, Shuting Zhang, Jianxiang Peng, Peiyi Zhang, Qingqing Lyu, et al. 2023a. M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. _arXiv preprint arXiv:2305.10263_ (2023).
* Liu et al. (2023b) Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, and Yan Zhang. 2023b. Is chatgpt a good recommender? a preliminary study. _arXiv preprint arXiv:2304.10149v2_ (2023b).
* Liu et al. (2023c) Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, et al. 2023c. LIMrec: Benchmarking large language models on recommendation task. _arXiv preprint arXiv:2308.12241_ (2023).
* Liu et al. (2023d) Junling Liu, Ziming Wang, Qichen Ye, Dading Chong, Peilin Zhou, and Yining Hua. 2023d. Qilin-med-vl: Towards chinese large vision-language model for general healthcare. _arXiv preprint arXiv:2310.17956_ (2023).
* Liu et al. (2021) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. _ArXiv_ abs/2110.07602 (2021).
* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. _CoRR_ abs/1907.11692 (2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692
* Nori et al. (2023) Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of gpt-4 on medical challenge problems. _arXiv preprint arXiv:2303.13375_ (2023).
* OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. _ArXiv_ abs/2303.08774 (2023).
* Pal et al. (2022) Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain question answering. In _Conference on Health, Inference, and Learning_. PMLR, 248-260.
* Pampari et al. (2018) Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. 2018. emrqa: A large corpus for question answering on electronic medical records. _arXiv preprint arXiv:1809.00732_ (2018).
* Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In _Annual Meeting of the Association for Computational Linguistics_.
* Pi et al. (2024) Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang. 2024. MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance. _arXiv preprint arXiv:2401.02906_ (2024).
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_ 21, 1 (2020), 5485-5551.
* Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. _Commun. ACM_ 64, 9 (2021), 99-106.
* Savery et al. (2020) Max Savery, Asma Ben Abacha, Soumya Gayen, and Dina Demner-Fushman. 2020. Question-driven summarization of answers to consumer health questions. _Scientific Data_ 7, 1 (2020), 322.
* Shao et al. (2021) Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Fei Yang, Li Zhe, Hujun Bao, and Xipeng Qiu. 2021. CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation. _arXiv preprint arXiv:2109.05729_ (2021).
* Singhal et al. (2022) Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2022. Large Language Models Encode Clinical Knowledge. _arXiv preprint arXiv:2212.13138_ (2022).
* Sankaran et al. (2020)Simon Suster and Walter Daelemans. 2018. CliCR: a dataset of clinical case reports for machine reading comprehension. _arXiv preprint arXiv:1803.09720_ (2018).
* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca.
* Tian et al. (2019) Yuanhe Tian, Weicheng Ma, Fei Xia, and Yan Song. 2019. ChiMed: A Chinese medical corpus for question answering. In _Proceedings of the 18th BioNLP Workshop and Shared Task_. 250-260.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. _ArXiv_ abs/2302.13971 (2023).
* Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. _Advances in neural information processing systems_ 32 (2019).
* Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_ (2018).
* Wang et al. (2023) Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. 2023. HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge. arXiv:2304.06975 [cs.CL]
* Organization (WHO) (2019) World Health Organization (WHO). 2019/2021. International Classification of Diseases, Eleventh Revision (ICD-11). https://icd.who.int/browse11. Licensed under Creative Commons Attribution-NoDerivatives 3.0 IGO licence (CC BY-ND 3.0 IGO).
* Xie et al. (2023) Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. 2023. Defending ChatGPT against jailbreak attack via self-reminders. _Nature Machine Intelligence_ (2023), 1-11.
* Xiong et al. (2023) Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin Huang, Qian Wang, and Dinggang Shen. 2023. DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task. _ArXiv_ abs/2304.01097 (2023).
* Ye et al. (2023) Qichen Ye, Junling Liu, Dading Chong, Peilin Zhou, Yining Hua, and Andrew Liu. 2023. Qilin-med: Multi-stage knowledge injection advanced medical large language model. _arXiv preprint arXiv:2310.09089_ (2023).
* Yi et al. (2023) Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. 2023. Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. _arXiv preprint arXiv:2312.14197_ (2023).
* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? _arXiv preprint arXiv:1905.07830_ (2019).
* Zeng et al. (2023) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023b. GLM-130B: An Open Bilingual Pre-trained Model. In _The Eleventh International Conference on Learning Representations (ICLR)_. https://openreview.net/forum?id=-Aw0rrrPUF
* Zeng (2023) Hui Zeng. 2023. Measuring Massive Multitask Chinese Understanding. _arXiv preprint arXiv:2304.12986_ (2023).
* Zeng et al. (2022) Qingcheng Zeng, Lucas Garay, Peilin Zhou, Dading Chong, Yining Hua, Jiageng Wu, Yikang Pan, Han Zhou, Rob Voigtand, and Jie Yang. 2022. GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost. _the 32nd International Joint Conference on Artificial Intelligence_ (2022).
* Zhang et al. (2019)Qingcheng Zeng, Lucas Garay, Peilin Zhou, Dading Chong, Yining Hua, Jiageng Wu, Yikang Pan, Han Zhou, and Jie Yang. 2023a. greenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost. In _Proceedings of the 2023 Conference on International Joint Conference on Artificial Intelligence (AI and Social Good Track)_.
* Zhang et al. (2017) Sheng Zhang, Xin Zhang, Hui Wang, Jiajun Cheng, Pei Li, and Zhaoyun Ding. 2017. Chinese medical question answer matching using end-to-end character-level multi-scale CNNs. _Applied Sciences_ 7, 8 (2017), 767.
* Zhang et al. (2018) Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu. 2018. Multi-scale attentive interaction networks for chinese medical question answer selection. _IEEE Access_ 6 (2018), 74061-74071.
* Zhang and Xu (2022) Xuanwei Zhang and Liang Xu. 2022. _PromptCLUE: A zero-shot learning model that supports full Chinese tasks_. https://github.com/clue-ai/PromptCLUE
* Zhou et al. (2023b) Hongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li, Sam S Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Xian Wu, et al. 2023b. A survey of large language models in medicine: Progress, application, and challenge. _arXiv preprint arXiv:2311.05112_ (2023).
* Zhou et al. (2023a) Peilin Zhou, Meng Cao, You-Liang Huang, Qichen Ye, Peiyan Zhang, Junling Liu, Yueqi Xie, Yining Hua, and Jaeboum Kim. 2023a. Exploring recommendation capabilities of gpt-4v (ision): A preliminary case study. _arXiv preprint arXiv:2311.04199_ (2023).
* Zhou et al. (2022) Peilin Zhou, Zeqiang Wang, Dading Chong, Zhijiang Guo, Yining Hua, Zichang Su, Zhiyang Teng, Jiageng Wu, and Jie Yang. 2022. METS-CoV: A Dataset of Medical Entity and Targeted Sentiment on COVID-19 Related Tweets. In _Advances in Neural Information Processing Systems_, Vol. 35. 21916-21932.
* Zhu et al. (2020) Ming Zhu, Aman Ahuja, Da-Cheng Juan, Wei Wei, and Chandan K Reddy. 2020. Question answering with long multiple-span answers. In _Findings of the Association for Computational Linguistics: EMNLP 2020_. 3840-3849.
* Zhu et al. (2019) Ming Zhu, Aman Ahuja, Wei Wei, and Chandan K Reddy. 2019. A hierarchical attention retrieval model for healthcare question answering. In _The World Wide Web Conference_. 2472-2482.

[MISSING_PAGE_FAIL:15]

\begin{table}
\begin{tabular}{l l l} \hline Abbreviation & Full English Name & Chinese Name \\ \hline AesthMed & Aesthetic Medicine & \\ Anesth & Anesthesiology & \\ ClinNatur & Clinical Nutrition & \\ Dent & Dentistry & \\ Derm & Dermatology & \\ EM & Emergency Medicine & \\ EndemicD & Endemic Disease & \\ ENT & Otolaryngology & \\ EthnoMed & Ethnic Medicine & \\ GenMed & General Medicine & \\ ICM & Intensive Care Medicine & \\ ID & Infectious Diseases & \\ IM & Internal Medicine & \\ ITCWM & Integrated Traditional Chinese and Western Medicine & \\ MedLabSci & Medical Laboratory Science & \\ N/A & Not Applicable & \\ OBGYN & Obstetrics and Gynecology & \\ OccMed & Occupational Medicine & \\ Onc & Oncology & \\ Ophth & Ophthalmology & \\ PainMed & Pain Medicine & \\ PallCare & Palliative Care & \\ Path & Pathology & \\ Ped & Pediatrics & \\ PedHC & Pediatric Health Care & \\ PedSurg & Pediatric Surgery & \\ PrevMed & Preventive Medicine & \\ Psych & Psychiatry & \\ PT & Physical Therapy & \\ Radiol & Radiology & \\ RehabMed & Rehabilitation Medicine & \\ SpecMed\&MilMed & Special Medical and Military Medicine & \\ SportsMed & Sports Medicine & \\ Surg & Surgery & \\ TB & Tuberculosiss & \\ TCM & Traditional Chinese Medicine & \\ WH & Womenâ€™s Health & \\ \hline \end{tabular}
\end{table}
Table 12: Clinical Departments

### Instructions for Pre-annotation

In this section, we present instructions used to pre-annotate CMExam test set data using GPT4. As shown in Figure 4,5,6,7, we first constrained the output from GPT4 to return only specific categories. We then annotated each of the five additional annotation dimensions relevant to this study with all the category information for each dimension. Next, we provided specific prompt information and finally, we performed filtering on the GPT4 output to improve the effectiveness of pre-annotation. During the actual annotation process, specific categories and prompt information should be filled in the grey background areas.

Figure 4: Pre-annotation Instructions for Disease Groups.

Figure 5: Pre-annotation Instructions for Clinical Departments.

### Analysis of Model Generation Ability

In Figure 8, we present partial explanations generated by various models for a medical question from the CMExam dataset. Notably, GPT-4 and GPT-3.5 produce concise and sensible explanations, which may account for the lower BLUE scores. Conversely, models like Vicuna, LLaMA, and Huotuo demonstrate a more prominent repetition phenomenon, while Alpaca simply duplicates the provided options without providing an explanation.

Fine-tuning models on the CMExam dataset significantly reduces the repetition phenomenon and improves the overall reasonableness of the explanations. For instance, the ChatGLM-CMExam model analyzes each option in a similar manner to the solution explanation. However, some models still generate unreasonable explanations, as observed in LLaMA-CMExam, Alpaca-CMExam, and Vicuna-CMExam. This could be attributed to their training on generic data and lack of specific knowledge in the medical domain. This underscores the significance of training large language models with a focus on the medical domain.

Figure 6: Pre-annotation Instructions for Medical Disciplines.

Figure 7: Pre-annotation Instructions for Areas of Competencies.

Figure 8: A case study of LLMsâ€™ generated explanations.

### Analysis of Model Generation Correctness

To assess the accuracy of model-generated explanations, we conducted a study using a randomly selected sample of 50 cases in which the Language Models (LLMs) correctly predicted the answers. Medical experts were then invited to manually verify the correctness of the explanations, focusing not only on the accuracy of the answer predictions but also on the quality of the accompanying explanations.

Our investigation revealed that despite the correct answer predictions by the models, certain samples exhibited errors in their corresponding explanations. These errors were categorized by the experts into three groups: explanations that were irrelevant, repeated, or inaccurate. The statistics presented in Figure 9 demonstrate that the number of samples with accurate explanations generated by the GPT models exceeded 45, accounting for over 90% of the total. However, it is important to note that both the ChatGLM and ChatGLM-CMExam models may produce some erroneous explanations, primarily consisting of inaccuracies and irrelevance. We have included examples of these incorrect explanations in Figure 10.

### Analysis of Few-Shot and Chain-of-Thought Prompts

In our research, we designed few-shot and chain-of-thought prompts for the answer prediction and reasoning tasks and conducted experiments on the GPT models. As shown in Table 13, our results demonstrate that while the use of few-shot or chain-of-thought prompts did not yield significant improvements in the prediction task, but there was a notable enhancement in the reasoning task.

Specifically, for the GPT-4 model, the utilization of few-shot prompts increased the BLUE-1 from 0.17 to 5.95, and the BLUE-4 from 0.06 to 2.25. Furthermore, incorporating chain-of-thought prompts further increased the BLUE-1 to 7.29. Similarly, positive effects were observed on the GPT-3.5 model, where few-shot prompts improved the BLUE-1 and BLUE-4 to 14.62 and 4.80, respectively. Additionally, the ROUGE-1, ROUGE-2, and ROUGE-L increased to 38.08, 18.35, and 18.37.

These improvements can be attributed to the fact that few-shot prompts provide examples that GPT models can reference when generating detailed explanations for each option during the reasoning process. Similarly, chain-of-thought prompts can achieve similar effects, aiding in the enhancement of model performance.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{3}{c}{Prediction} & \multicolumn{3}{c}{Reasoning} \\ \cline{2-9}  & ACC & f1 & BLUE-1 & BLUE-4 & ROUGE-1 & ROUGE-2 & ROUGE-L \\ \hline GPT-4 & 61.6\%\(\pm\)0.1 & 61.7\%\(\pm\)0.1 & 0.17\%\(\pm\)0.0 & 0.064\(\pm\)0.00 & 29.74\%\(\pm\)0.09 & 14.84\%\(\pm\)0.04 & 11.51\%\(\pm\)0.03 \\ GPT-4\_few-shot & 62.0\%\(\pm\)0.4 & 61.4\%\(\pm\)0.5 & 5.95\%\(\pm\)1.2 & 2.25\%\(\pm\)0.07 & 37.24\%\(\pm\)0.35 & 19.23\%\(\pm\)0.26 & 17.24\%\(\pm\)0.07 \\ GPT-4\_cor & 61.6\%\(\pm\)0.9 & 61.4\%\(\pm\)0.9 & 7.29\%\(\pm\)0.71 & 2.20\%\(\pm\)0.25 & 35.35\%\(\pm\)0.76 & 16.79\%\(\pm\)0.83 & 17.18\%\(\pm\)0.30 \\ \hline GPT3.5 & 46.4\%\(\pm\)0.0 & 46.2\%\(\pm\)0.1 & 3.56\%\(\pm\)0.08 & 1.49\%\(\pm\)0.06 & 33.80\%\(\pm\)0.11 & 16.39\%\(\pm\)0.05 & 14.83\%\(\pm\)0.13 \\ GPT-3.5\_cor & 45.3\%\(\pm\)0.6 & 44.9\%\(\pm\)0.6 & 4.62\%\(\pm\)0.6 & 4.80\%\(\pm\)0.06 & 38.08\%\(\pm\)0.44 & 13.35\%\(\pm\)0.16 & 18.37\%\(\pm\)0.29 \\ GPT-3.5\_cor & 47.9\%\(\pm\)0.7 & 47.7\%\(\pm\)0.7 & 13.47\%\(\pm\)0.52 & 3.69\%\(\pm\)0.18 & 36.47\%\(\pm\)0.42 & 16.41\%\(\pm\)0.24 & 17.82\%\(\pm\)0.31 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Few-shot and chain-of-thought prompting experiment results of GPT models

Figure 9: Correctness analysis.

[MISSING_PAGE_EMPTY:21]

### Data statistics

Questions in CMExam have a median length of 17 (Q1: 12, Q3: 32). Regarding solution explanations, the median length is 146 tokens (Q1: 69, Q3: 247). Table 14 shows more basic statistics of CMExam,

### Guidelines for Expert-Annotation

During the annotation phase, we invited one expert physician from the Second Affiliated Hospital of Zhejiang University and one senior doctoral student from Zhejiang University School of Medicine to carry out the annotations. The expert physician has over two years of clinical experience. The annotation guidelines have the following sections:

1. Comprehensive Question Understanding: Prior to initiating the annotation process, meticulously comprehend the medical question, ensuring a holistic grasp of its context and significance.
2. Subject Categorization: Identify the precise subject or medical field that the question pertains to, such as cardiology, pediatrics, or pathology.
3. Principal Symptoms or Medical Conditions: Ascertain and pinpoint the primary symptoms or medical conditions expounded in the question.
4. Examination of Pertinent Factors: Scrutinize the question for any associated factors that might be present, including the severity of the ailment, its etiology, and patient history given in the question.
5. Examination of Pertinent Factors: Scrutinize the question for any associated factors that might be present, including the severity of the ailment, its etiology, and patient history given in the question.
6. Appropriate Classification System Usage: Use the accurate classification system for annotation in alignment with the determined subject and symptoms. Suitable systems could encompass the 11th revision of the International Classification of Diseases (ICD-11), the Directory of Medical Institution Diagnostic and Therapeutic Categories (DMIDTC), and others.
7. Addressing Multiple Annotations: In scenarios where the question encompasses multiple symptoms or medical conditions, opt for the most related classification for annotation.
8. Ensuring High-Quality Annotations: Adhere to the guidelines and definitions within the chosen classification system. This diligence helps avert subjectivity and ambiguity, fostering precision in the annotations.
9. Navigating Queries and Uncertainties: Should any doubts or uncertainties emerge during the annotation process, consult the official documents and glossaries of the chosen classification system. Engaging in discussions with professionals is also advised to achieve clarity.
10. Resolving Discrepancies: When disagreements emerge between annotators, a collaborative discussion shall be initiated. The objective is to reach a consensus and unify the annotation decision.

### Prompt strategies for Pre-Annotation

During the experimental process, we indeed tried different prompts to enable GPT to better understand and complete the annotation task. The specific strategies were as follows:

\begin{table}
\begin{tabular}{l c c c c} \hline  & Train & Dev & Test & Total \\ \hline Question \# & 54,497 & 6,811 & 6,811 & 68,119 \\ Vocab & 4,545 & 3,620 & 3,599 & 4,629 \\ Max Q tokens & 676 & 500 & 585 & 676 \\ Max E tokens & 2,999 & 2,678 & 2,680 & 2,999 \\ Avg Q tokens & 29.78 & 30.07 & 32.63 & 30.83 \\ Avg E tokens & 186.24 & 188.95 & 201.44 & 192.21 \\ Median (Q1, Q3) Q tokens & 17 (21, 32) & 18 (12, 32) & 18 (12, 37) & 18 (12, 32) \\ Median (Q1, Q3) E tokens & 146 (69, 246) & 143 (65, 247) & 158 (80, 263) & 146 (69, 247) \\ \hline \end{tabular}
\end{table}
Table 14: Basic statistics of CMExam. Q: questions; E: explanations; Q1/3: the first/ third quantile.

1. Without ICD-11 Category Instructions: We did not provide detailed ICD-11 category information as instruction but directly supplied the question information and asked GPT to respond. Under this setup, a significant portion of the categories returned by GPT did not strictly belong to ICD-11 classifications, yielding unsatisfactory results.
2. Batch Processing for Cost Efficiency: Initially, we concatenated multiple questions and, through a single dialogue, had GPT return annotations for multiple questions. Under this setup, expert validation showed that the accuracy of GPT's annotations was relatively low.
3. Consistency in Formatting: When no format guidance was given, GPT's return format was inconsistent, resulting in a higher parsing cost. Hence, after multiple trials, we eventually opted for more rigorous format guidance. Our annotation process was carried out in two stages: First, GPT conducted an initial round of pre-annotation. Subsequently, we invited an expert physician from the Second Affiliated Hospital of Zhejiang University and a doctoral student from Zhejiang University School of Medicine to annotate. The expert physician had over two years of clinical experience. In instances where there were disagreements in annotations, both annotators would discuss and eventually arrive at a consensus.