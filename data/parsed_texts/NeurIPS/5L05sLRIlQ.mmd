# _VastTrack_: Vast Category Visual Object Tracking

 Liang Peng\({}^{1*}\), Junyuan Gao\({}^{1*}\), Xinran Liu\({}^{1,3*}\), Weihong Li\({}^{1,2,3*}\), Shaohua Dong\({}^{4*}\), Zhipeng Zhang\({}^{5}\), Heng Fan\({}^{4\dagger}\), Libo Zhang\({}^{1,2,3\dagger\sharp}\)

\({}^{1}\)Institute of Software Chinese Academy of Sciences \({}^{2}\)Hangzhou Institute for Advanced Study

\({}^{3}\)University of Chinese Academy of Sciences \({}^{4}\)University of North Texas \({}^{5}\)KargoBot

\({}^{*}\)Equal contribution \({}^{\dagger}\)Equal advising and co-last authors \({}^{\sharp}\)Corresponding author

###### Abstract

In this paper, we propose a novel benchmark, named _VastTrack_, aiming to facilitate the development of general visual tracking via encompassing abundant classes and videos. VastTrack consists of a few attractive properties: **(1)**_Vast Object Category_. In particular, it covers targets from 2,115 categories, significantly surpassing object classes of existing popular benchmarks (_e.g._, GOT-10k with 563 classes and LaSOT with 70 categories). Through providing such vast object classes, we expect to learn more general object tracking. **(2)**_Larger scale_. Compared with current benchmarks, VastTrack provides 50,610 videos with 4.2 million frames, which makes it to date the largest dataset in term of the number of videos, and hence could benefit training even more powerful visual trackers in the deep learning era. **(3)**_Rich Annotation_. Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions with more than 50K sentences for the videos. Such rich annotations of VastTrack enable the development of both vision-only and vision-language tracking. In order to ensure precise annotation, each frame in the videos is manually labeled with multi-stage of careful inspections and refinements. To understand performance of existing trackers and to provide baselines for future comparison, we extensively evaluate 25 representative trackers. The results, not surprisingly, display significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are urgently required to improve general visual tracking. Our VastTrack, the toolkit, and evaluation results are publicly available at [https://github.com/HengLan/VastTrack](https://github.com/HengLan/VastTrack).

## 1 Introduction

Visual tracking is a fundamental computer vision problem with many applications such as surveillance and robotics. The ultimate goal for tacking is to localize the target of an _arbitrary_ category in an _arbitrary_ scenario from a sequence, given its initial position, which we term _universal visual object tracking_. For such goal, numerous trackers have been proposed in recent decades [58, 45, 34, 28, 39]. In particular, with the introduction of several large-scale tracking benchmarks (_e.g._, [43, 16, 27]) in the deep learning era, considerable advancements (_e.g._, [7, 9, 57, 5, 38, 6, 51, 37]) have been seen in the visual tracking community. Despite this, it remains challenging to achieve universal tracking.

One important reason is relatively _restricted_ number of object categories in current tracking benchmarks. The objects in the real world are from _countless_ categories. To achieve general visual tracking like humans, the tracker is expected to "SEE" various sequences from an extremely large set of object categories during training to acquire the generalization ability. Nevertheless, the categories in existing large-scale benchmarks are rather _limited_. For example, the popular TrackingNet [43] and LaSOT [16] comprise respectively 27 and 70 categories (see Fig. 1), which fall short for training universally generalizable trackers. Another popular dataset GOT-10k [27] aims to handle this by largely expanding the number of object categories to 563. Despite its success in advancing generic-purposetracking, the 563 object categories are still insufficient to represent massive diversity of categories present in the real world. Besides training, a real general tracking system requires evaluation on videos of vast object categories, which can help mitigate biases to certain classes for more faithful assessment in real applications. Nevertheless, the test sets of existing large-scale benchmarks (_e.g._, [43, 16, 27]) all consist of _less than_ 100 categories, which may not be enough for faithful assessment of general tracking.

Besides rich categories, abundant videos with high-quality annotations are crucial for learning robust visual trackers. Particularly, as a tracking model becomes larger and more complicated, _e.g._, from CNNs [30, 26] to Transformer [47], more videos are demanded to unleash the power of deep network for achieving robustness and generality. While there have been extensive efforts to develop tracking datasets, they are comparatively small in scale or limited in annotation quality. For example, currently the largest (in term of video number) benchmark [27] with _precise_ annotations has only 10K videos, which may still be inadequate for training generalized trackers, as evidenced by enhanced performance [9] on it when using extra training videos. Although another benchmark [43] offers more videos, its annotations are not precise, which may degrade performance.

More recently, language has demonstrated great potential to enhance robustness of general tracking, and the resulted paradigm, the so-called _vision-language tracking_ (_e.g._, [35, 24, 19, 61]), has attracted increasing attention. For learning a robust and general vision-language tracker, it is crucial to provide ample videos with visual and linguistic annotations. Although there are several datasets (_e.g._, [16, 50]) created for this goal, the number of linguistic sentences are limited in scale (_e.g._, 1.4K in [16] and 2K in [50]), which may impede the exploration of more general vision-language tracking.

In order to alleviate the aforementioned limitations in existing datasets for developing more general visual tracking, we propose _VastTrack_, an innovative large-scale benchmark for **Vast**-category short-term object **Tracking** via comprising abundant classes and video from diverse scenarios. In particular, VastTrack makes the following efforts for facilitating the development of general object tracking:

**(1) _Vast Object Category_:** To enrich the diversity in object categories for general tracking, VastTrack consists of videos from 2,115 classes, which largely surpasses category number in popular benchmarks such as GOT-10k [27] with 563 classes and LaSOT [16] with 70 classes, as displayed in Fig. 1. To our best of our knowledge, VastTrack is the richest tracking dataset with the largest number of categories. With such vast object classes, we expect to accelerate the exploration towards more general tracking.

**(2) _Larger Scale_:** For learning robust universal tracking, our VastTrack offers 50,610 video sequences with 4.2 million frames, which makes it so far the largest and the most diverse tracking dataset in terms of the numbers of videos and targets compared to existing datasets (_e.g._[16, 27, 43]), as shown in Fig. 1. Such a larger scale and diversity of VastTrack in videos and targets can potentially benefit training more powerful trackers, particularly Transformer-based models, in the deep learning era.

**(3) _Rich and Precise Annotations_:** Considering the benefits of language for enhancing general object tracking, VastTrack offers both standard bounding box annotations and rich linguistic specifications for the sequences, and thus enables exploration of both the vision-only and vision-language universal tracking. Compared with current benchmarks (_e.g._, [16] with 1.4K and [50] with 2K sentences) for vision-language tracking, the proposed VastTrack provides over 50K descriptions, a magnitude order larger than [16, 50], of more and diverse targets for better vision-language tracking. In addition, to ensure precise annotations, each video in VastTrack is manually labeled with multi-round refinements.

In order to understand the performance of existing trackers on VastTrack and to provide baseline for future comparison, we extensively evaluate 25 recent representative algorithms in a hybrid protocol in which the test videos have partial overlap with the training sequences (as described later) and conduct

Figure 1: Summary of representative benchmarks, comprising OTR-2013/2015 [53, 54], TC-128 [36], UAV123 [42], NUS-PRO [32], UAV20L [42], VOT-2017 [29], OxUvA [46], GOT-10k [27], TrackingNet [43], and VastTrack. We can clearly see that VastTrack is _larger_ than all other datasets by containing 2,115 object categories and 50,610 videos. _Best viewed in color for all figures_.

in-depth analysis. The evaluation reveals that, not surprisingly, current top-performing object trackers significantly degrade on the more challenging VastTrack. For example, the success scores of existing state-of-the-art trackers, _e.g._, SeqTrack [6], MixFormer [9], and OSTrack [57], degrade from 0.725, 0.724, and 0.711 on LaSOT [16] to 0.396 (with a drop of 0.329), 0.395 (with a drop of 0.329), and 0.336 (with a drop of 0.375) on VastTrack. This demonstrate the challenge in achieving universal tracking for current trackers, and more efforts are desired to improve general-purpose object tracking.

By releasing VastTrack, we expect to offer a new large-scale platform with abundant videos from vast categories for facilitating the development of more general and universal tracking and its applications. In summary, our main _contributions_ are as follows: \(\blacklozenge\) We introduce a new benchmark VastTrack that covers 2,115 object categories to facilitate more general tracking; \(\blacklozenge\) VastTrack provides a large scale of 50,610 videos which could benefit developing more powerful deep trackers; \(\clubsuit\) Rich annotations in VastTrack enable exploration of both vision-only and vision-language tracking; \(\blacklozenge\) Evaluation of 25 trackers is conducted to understand VastTrack and provides baselines for future comparison.

## 2 Related Work

**Visual Tracking Benchmarks.** Benchmarks have been crucial for development of tracking. Early tracking benchmarks are usually in small scale and mainly aim at the evaluation purpose for fairly comparing different algorithms. OTB-2013 [53] is the first tracking benchmark by introducing 51 videos and later extended in [54] by adding new sequences. VOT [29] presents a series of challenges to compare trackers in different aspects. TC-128 [36] contains 128 colorful videos to study the impact of color information on tracking models. NIS [21] assesses tracking performance by providing 100 sequences with high frame rate. UAV123 and UAV20L [42] respectively consist of 123 and 20 videos captured by unmanned aerial vehicle for tracking performance evaluation. NUS-PRO [32] offers 365 videos to assess trackers on rigid target objects. OxUvA [46] contains 366 sequences for evaluating long-term tracking performance of different algorithms. From a different perspective than opaque tracking benchmarks, TOTB [17] collects 225 videos for investigating transparent object tracking.

Despite facilitating tracking, early datasets are limited in scale and cannot provide videos for training deep tracking. To alleviate this, several large-scale benchmarks have been introduced in recent years. TrackingNet [43] presents a large-scale dataset with around 30K videos for training deep tracking. However, its annotations are generated using a tracker, which may be inaccurate and thus degrade the training of deep tracking. LaSOT [16] comprises 1,400 long-term videos with precise dense annotations, and is later extended in [15] by adding more videos. Notably, it provides both bounding box and language annotation to enable both vision-only and vision-language tracking. GOT-10 [27] contributes a large benchmark with around 10K sequences from 563 classes. Despite advancing deep tracking, 563 object classes may still be insufficient to represent massive categories in the real world.

VastTrack is related to the aforementioned large-scale datasets but provides a more diverse and larger platform with more than 50K videos from 2,115 categories, which aims to accelerate the exploration towards universal and general tracking. Tab. 1 shows the comparison of VastTrack with other datasets.

\begin{table}
\begin{tabular}{r r r r r r r r r r r r} \hline \hline \multicolumn{1}{c}{**Benchmark**} & \multicolumn{1}{c}{**Year Classes**} & \multicolumn{1}{c}{**Videos**} & \multicolumn{1}{c}{**Mean**} & \multicolumn{1}{c}{**Total**} & \multicolumn{1}{c}{**Total**} & \multicolumn{1}{c}{**Absent Num. of Lang.**} & \multicolumn{1}{c}{**Frame Dataset**} & \multicolumn{1}{c}{**Dataset**} \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\ \hline \hline OTB-2013[33] & 2013 & 10 & 50 & 578 & 29K & 16.4 min & ✗ & 11 & ✗ & 30.6\_fps_ & ST & Eva. \\ OTB-2015[4] & 2015 & 16 & 100 & 590 & 59K & 32.8 min & ✗ & 11 & ✗ & 30.6\_fps_ & ST & Eva. \\ TC-128[36] & 2015 & 27 & 128 & 429 & 55K & 30.7 min & ✗ & 11 & ✗ & 30.6\_fps_ & ST & Eva. \\ NUS-PRO [32] & 2016 & 17 & 365 & 371 & 135K & 75.2 min & ✗ & 12 & ✗ & 30.6\_fps_ & ST & Eva. \\ UAV213[42] & 2016 & 9 & 123 & 915 & 113K & 62.5 min & ✗ & 12 & ✗ & 30.6\_fps_ & ST & Eva. \\ UAV20L[42] & 2016 & 5 & 120 & 29.34 & 59K & 32.6 min & ✗ & 12 & ✗ & 30.6\_fps_ & LT & Eva. \\ NIS [21] & 2017 & 17 & 100 & 3,830 & 383K & 26.6 min & ✗ & 9 & ✗ & 240 \_fps_ & ST & Eva. \\ VOT-2017[29] & 2017 & 24 & 60 & 356 & 21K & 11.9 min & ✗ & 24 & ✗ & 30.6\_fps_ & ST & Eva. \\ OKuVA [46] & 2018 & 22 & 366 & 4,235 & 15.5M & 14.4 hours & ✗ & 6 & ✗ & 30.6\_fps_ & LT & Eva. \\ TrackingNet [43] & 2018 & 27 & 30,643 & 471 & 14.43M\({}^{*}\) & 140.0 hours & ✗ & 15 & ✗ & 30.6\_fps_ & ST & Tra/Eva. \\ LaSOT [16] & 2019 & 70 & 1,400 & 2,053 & 3.52M & 32.5 hours & ✓ & 14 & ✓ & 30.6\_fps_ & LT & Tra/Eva. \\ TNL-K [50] & 2021 & 169\({}^{*}\) & 2,000 & 62 & 1.2adm & 11.5 hours & ✓ & 17 & ✓ & 30.6\_fps_ & ST & Tra/Eva. \\ GOT-10x & 2021 & 563 & 9,935 & 149 & 1.45M & 40.0 hours & ✓ & 6 & ✗ & 10.6\_fps_ & ST & Tra/Eva. \\ \hline
**VastTrack** & 2024 & 2,115 & 50,610 & 83 & 4.20M & 194.4 hours & ✓ & 10 & ✓ & 6.6\_fps_ & ST & Tra/Eva. \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of VastTrack with other datasets. “ST” and “LT” indicate short- and long-term tracking. “Tra.” and “Eav.” mean training and evaluation. \(b\): TrackingNet [43] is semi-automatically labeled using a tracker. \(\blacklozenge\): The number of object classes in TNL2K [50] is obtained by our statistic.

**Vision Benchmarks with Vast Categories.** Benchmarks with vast object categories are desired for learning general vision systems. Numerous such benchmarks have been introduced for various vision tasks. For example, the well-known ImageNet [14] consists of 1,000 classes for image recognition. Open Image [31] covers 600 categories for object detection. LVIS [25] comprises 1,203 classes for the tasks of object detection and instance segmentation. TAO [13] contains 833 categories for general multi-object tracking. The recently proposed V3Det [48] contributes a new dataset with 13,204 object classes with the goal of facilitating the general detection system development.

In the similar spirit with the above vast category benchmarks, we introduce VastTrack that comprises 2,115 object classes and more than 50K sequences for visual tracking. To the best of our knowledge, VastTrack is so far the largest tracking benchmark regarding the categories and videos and we hope it can serve as a cornerstone dataset for developing more general object tracking systems.

## 3 The Proposed VastTrack

### Construction Principle

The goal of VastTrack is to develop a unique large-scale platform with abundant object categories and video sequences with rich as well as precision annotations for facilitating the development of more general tracking. For this purpose, we follow principles below in constructing our VastTrack:

* _Vast Object Category._ One key motivation of VastTrack is to facilitate more universal object tracking with a rich class diversity. To this end, we hope that the new benchmark covers at least 2,000 object classes, containing common target objects suitable for visual tracking in our life.
* _Larger Scale._ Abundant sequences are crucial for training deep trackers. We expect VastTrack to include at least 50K videos with an average video length of at least 80 frames. Such a scale, greatly larger than current datasets, can potentially benefit training more powerful deep trackers.
* _Rich Annotation._ One of important goals of VastTrack is to develop a comprehensive platform that supports both vision-only and vision-language tracking. Considering this, both bounding boxes and language specifications will be provided to boost tracking in different directions.
* _High Quality._ The quality of annotation is crucial for both training and evaluation. To ensure high quality of VastTrack, we manually label each video with multi-round inspections and refinements.

### Data Acquisition

VastTrack aims to cover abundant categories for tracking. To this end, 2,115 categories are selected for building VastTrack. These object categories are chosen from different sources, including classes in ImageNet [14] and V3Det [48], WordNet [41], and Wikipedia, and organized in a hierarchical tree structure. Note that, each selected category is verified by an expert (_e.g._, a PhD or MS student working on the related topic) to ensure that it is suitable for the tracking task. Compared with existing datasets, the object classes of VastTrack are more diverse and more desired for universal tracking as discussed before. Please refer to **supplementary material** for details of object classes in VastTrack.

After determining all object categories of VastTrack, we then search for the sequences of each class from YouTube under Creative Commons licence. The reason to use YouTube for sourcing videos is because it is currently the largest the video platform and many videos come from the real world. Initially, we gather more than 66K sequences. Then, we carefully inspect each video for the availability for visual tracking task, and finally pick out 50,610 sequences. For each qualified video, we remove the irrelevant content from it, and only retain an usable clip for tracking. Note that, unlike LaSOT [16] in which each category has the same number of videos, the sequence number of each class is not equal, forming a long-tail distribution (see Fig. 2) that is more universal in real world and could encourage learning more practical and general visual trackers [27].

Figure 2: The number of videos in each object class forms a long-tail distribution, which is common and universal in our real world.

Eventually, we develop a new benchmark, VastTrack, by covering 2,115 categories. It contains 50,610 videos with 4.2 million frames with an average sequence length of 83 frames. Because of limited space, we display detailed distribution of video length of VastTrack in the **supplementary material**. Please **notice** that, VastTrack is focused on short-term tracking by offering abundant object classes and sequences. Despite this, it can still be used for training long-term temporal trackers, as evidenced by the effectiveness of short-term videos in [27; 43] for learning robust trackers on both long-/short scenarios. In order words, diversity and quantity of objects and videos may be more crucial for deep tracking. It is worth **noting** that, although the frame rate (_i.e._, 6 _fps_ as in Tab. 1) of VastTrack is less than that of traditional benchmarks, this may not impact the training and evaluation of tracking much. Specifically, on the training side, since most of current tracking frameworks adopt the training mechanism of frame sampling with an interval (_e.g._, 100 frames), our VastTrack can be used for training by choosing a suitable interval (probably less than the interval used for traditional datasets). On the evaluation side, according to the analysis in [46], labeling at different frame rate, even at 1 _fps_, does not adversely affect the robustness of tracking evaluation.

### Annotation

We follow the similar principle as in [16; 15] for the bounding box annotation of a sequence: given the initial target object, in each frame, if the object shows up in the view, a labeler manually draws its (axis-aligned) bounding box as the tightest one to fit any visible part of the target; otherwise an absence label, either _out-of-view_ or _full occlusion_, is given to the frame. Notice that, for some categories such as "_Kite_" and "_Yo-Yo_", the string does not belong to the target object to track, and thus will not be included in the annotated bounding box.

Guided by the above principle, we compile an annotation team with a few experts and a qualified labeling group, and adopt a multi-step mechanism, including manual labeling, visual inspection and refinement. In the first step, after experts label the initial target in the first frame, the annotation group starts to label the target in all other frames in the video. Notice that, to ensure consistency, each video

Figure 4: Statistics of annotations on object motion (image (a)), relative area compared to the initial object (image (b)), IoU of targets in adjacent frames (image (c)), and size of targets (image (d)).

Figure 3: Visualization of several annotation examples in the proposed VastTrack.

is labeled (and refined if necessary) by the same annotator. After this, in the second step, the experts verify the completed annotations from the first step. If the annotation is not unanimously agreed by a validation team (formed by two or three experts), it will be returned back to the original labeler for refinement in the third step. Throughout the annotation process, the second and third steps are repeated for multiple rounds, which ensures high-quality annotations of VastTrack. Fig. 4 displays several annotation examples. In Fig. 4, we show the distributions of target motion, relative area to the initial object, Intersection over Union (IoU) between targets in adjacent frames, and the size of object. From these statistics, we can see that objects moves fast and varies rapidly in the videos of VastTrack.

Considering the benefits of language in improving tracking (_e.g_. [35; 24; 19; 61]), we offer language specifications, besides box annotations, for videos in VastTrack, aiming to facilitate the development of vision-language tracking. In specific, a sentence of natural language that describes color information, behavior, and surroundings of the object as well as optionally its interaction with other objects is given as the linguistic annotation for the video (see Fig. 3 for examples). Although there have been datasets for similar goal (_e.g_. [16; 50] as in Tab. 1), the scale is limited by containing 1.4K [16] and 2K sentences [50]. Differently, VastTrack offers over 50K videos with richer linguistic specifications for different objects, and thus may benefit learning more powerful vision-language trackers.

### Attributes

To enable further in-depth analysis, we offer ten attributes for _test_ videos in VastTrack, including (1) invisibility (INV), assigned when object is partially or fully invisible due to occlusion or out of view, (2) deformation (DEF), assigned when target is deformable, (3) rotation (ROT), assigned when object rotates, (4) aspect ratio change (ARC), assigned when ratio of bounding box aspect ratio is outside [0.5, 2], (5) illumination variation (IV), assigned when illumination in object region heavily varies, (6) scale variation (SV), assigned when ratio of bounding box is outside [0.5, 2], (7) fast motion (FM), assigned when target center moves larger than its size in last frame, (8) motion blur (MB), assigned when blur in object regions occurs (9) background clutter (BC), assigned when the similar appearance (not necessarily the same class of target) as target appears, and (10) low resolution (LR), assigned when target region is less than 1,000 pixels. For each video, a 10D binary vector is adopted to indicate the presence of an attribute, _i.e_., "1" for presence, "0" otherwise.

The distribution of attributes for the test videos of VastTrack is shown in Fig. 5. We can see that the most common challenge is scale variation, involved with 2,956 videos. In addition, invisibility due to partial or full occlusion or out-of-view and fast motion frequently occur with 2,879 and 2,865 videos.

### Dataset Split and Evaluation Protocol

**Dataset Split.** VastTrack has 50,610 videos, with 47,110 videos for training in VastTrackTra and the rest 3,500 for testing in VastTrackTst. Tab. 2 displays comparison of training and testing sets. In dataset split, we try to keep distributions of training and testing sets similar. Please note, the reason to use 3,500 videos (\(\sim\)7% of total) in VastTrackTM is to keep it relatively compact so that evaluation of trackers can be fast, similar to the popular GOT-10k [27] in which 420 videos out of around 10K are for testing (\(\sim\)4.2% of the total). Although VastTrackTst has only 3,500 videos, it is representative by including rich categories and various scenarios for evaluation, and much larger and more diverse compared to other testing sets in video number and classes, making evaluation more reliable.

**Evaluation Protocol.** Unlike the _full overlap_[16; 43] or _one-shot_[27], we utilize a _hybrid_ protocol wherein part of object classes (not videos) in test set have overlap with training set, while the rest classes remains unseen. The reason is that, in real world, humans often track objects from both

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & **Classes** & **Videos** & \begin{tabular}{c} **Mean** \\ **frames** \\ \end{tabular} & 
\begin{tabular}{c} **Total** \\ **frames** \\ \end{tabular} \\ \hline \hline VastTrackTst & 702 & 3,500 & 106.3 & 372K \\ VastTrackTra & 1,974 & 47,110 & 81.2 & 3.82M \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of _training_ and _testing_ sets.

Figure 5: Distribution of videos per attribute.

frequently seen and unseen categories. To develop human-like trackers, we adopt such a hybrid protocol for VastTrack with 561 overlap classes and 141 completely unseen classes in test set.

## 4 Experiments

**Evaluation Metric.** Following [53; 16; 43], we use _one-pass evaluation_ (OPE) and compare different trackers using three metrics, including _precision_ (PRE), normalized precision (NPRE), and success (SUC). In specific, PRE measures center position distance between tracking results and groundtruth in pixels, and trackers are ranked by PRE on a preset threshold, _e.g._, 20 pixels. To mitigate influence of video resolutions, NPRE is calculated by normalizing PRE using target region. Different from PRE and NPRE, SUC measures Intersection over Union (IoU) between tracking results and groundtruth, and is computed by the percentage of frames in which the IoU is larger than a threshold, _e.g._, 0.5.

### Evaluated Trackers

To understand existing approaches on VastTrack and also to offer baselines for comparison, we evaluate 25 representative trackers, which are classified into three types: **(i) CNN-based** that achieves object tracking using only CNN architecture, consisting of SiamFC [1], ATOM [11], SiamRPN++ [33], SiamBAN [8], DiMP [2], SiamCAR [23], PrDiMP [12], STMTrack [20], Ocean [60], RTS [44], and

Figure 6: Evaluation results of 25 trackers on VastTrack\({}_{\text{Tst}}\) using PRE, NPRE, and SUC.

Figure 7: Qualitative results of eight representative trackers on different sequences. We observe that these trackers drift to the background region or even lose the target due to different challenges in videos such as BC, SV, DEF, INV, MB, ROT, and LR. More efforts are desired to improve tracking.

AutoMatch [59]; **(ii) CNN-Transformer-based** that implements visual tracking via hybrid CNN and Transformer architectures, including STARK [56], TrSiam [49], TransT [7], and ToMP [40]; **(iii) Transformer-based** that tracks the target through leveraging a pure Transformer architecture. The tracking approaches in this category consist of OSTrack [57], SwinTrack [38], MixFormer [9] and MixFormerV2 [10], SimTrack [5], SeqTrack [6], ARTrack [51], DropMAE [52], and ROMTrack [4] as well as GRM [22]. We conduct the evaluations on a workstation with an Intel Xeon w9 CPU and 4 Nvidia A6000 GPUs. Please note, all trackers are assessed as they are, without modifications. A detailed summary of these trackers are shown in the **supplementary material** due to limited space.

We understand that tracking is a rapidly evolving field with numerous trackers proposed every year. To keep the evaluations on VastTrack as up-to-date as possible, we try our best to maintain lead board on our project page by continuously including newly published trackers (_e.g._, [55, 3, 37]) from major avenues. Please refer to our project webpage for more details.

### Evaluation Results

**Overall Performance.** We evaluate 25 trackers on VastTrack, including many recent Transformer-based methods. Note that, for evaluation, each tracker is evaluated as it is, without any modification. The evaluation results are reported in Fig. 6. We can see that, SeqTrack achieves the best performance on all three metrics with 0.402 PRE, 0.429 NPRE, and 0.396 SUC scores, MixFormer displays the second best results with 0.398 PRE, 0.424, and 0.395 SUC scores, and DropMAE obtains the third best results with 0.365 PRE, 0.397 NPRE, and 0.375 SUC scores. All these three trackers are developed based on vision Transformer architecture, showing its power in feature learning for tracking. Notably, although RTS does not employ Transformer architecture for tracking, it still achieves promising results with 0.331 PRE, 0.364 NPRE, and 0.355 SUC scores, even better than a few Transformer trackers like OSTrack with 0.315 PRE, 0.345 NPRE, and 0.336 SUC scores and SwinTrack with 0.303 PRE, 0.342 NPRE, and 0.330 SUC scores. We argue this is because RTS adopts tracking-by-segmentation which is beneficial for tracking object with extreme aspect ratio. Note that, the recent MixFormerV2 with 0.330 PRE, 0.365 NPRE, and 0.352 SUC scores performs worse than its previous version MixFormer, because it leverages much lighter network for efficiency. An interesting observation is that, SiamRPM++, a seminal Siamese tracker, surprisingly outperforms many its extensions such as SiamCAR, Ocean, and SiamBAN, showing its generality to some extent.

**Qualitative Evaluation.** In addition to the quantitative evaluation in the main text and this appendix, we further show qualitative results on VastTrack. Specifically, we demonstrate visualizations of eight representative trackers, including SeqTrack, MixFormer, RTS, OSTrack, SwinTrack, TransT, SimaRPN++, and SiamFC in different attributes such as _scale variation_, _deformation_, _rotation_, _aspect ratio change_, _background clutter_, _invisibility_, _blur_, _fast motion_, and _low resolution_ in Fig. 7. As displayed Fig. 7, we can observe that, although the trackers can deal with some challenges in the video sequences, they may still fail in more complicated scenarios where multiple challenges occur simultaneously, which indicates that more efforts are desired to improve existing approaches towards universal visual tracking.

**Discussion.** The evaluation shows some useful observations: (1) _Feature network._ As shown in Fig. 6, we observe that, the top five trackers are based on Vision Transformer architecture, which reveals that the exploration of more powerful feature network is still an important direction for improving tracking. This is consistent with findings in other benchmarks. Despite adopting powerful feature network, the performance is still far from satisfaction, compared to that on other benchmarks (as shown later). We argue this is caused by the lack of universal large-scale training of more general object categories for tracking. (2) _Temporal information_. Videos contain abundant temporal information which is important for tracking. However, this is largely ignored to some extend owing to the great success of Siamese tracking in recent years. Especially, even without using temporal information, many trackers still achieve state-of-the-art performance. However, from Fig. 6, we see that, the top three trackers all leverage temporal information for tracking, which indicates the crucial role of temporal cue for tracking. We hope, through evaluation results on VastTrack, researchers can pay more attention in developing robust tracking by incorporating temporal cues.

**Attribute-based Performance.** To better analyze different trackers, we further perform attribute-based evaluation on ten challenges. Fig. 8 shows the attribute-based results for the two most common attributes of SV and INV (based on number of videos in each attribute) and two difficult attributes of LR and IV using SUC. As in Fig. 8 (a), SeqTrack, MixFormer, and DropMAE achieves the best three results on SV/INV with 0.369/0.368, 0.367/0.365, and 0.347/0.347 scores in SUC, which is consistent with their performance in overall evaluation. As in Fig. 8 (b), SeqTrack, MixFormer, and DropMAT are the best three trackers on IV. An interesting finding is, IV is considered to be easy for tracking [18]. Nevertheless, our result shows difference. We argue, this is because IV in VasTrack usually occurs in low-light condition with complicated background, which degrades tracking performance. This also shows, extreme illumination change still need to be carefully dealt with. LR is the most difficult challenge in VasTrack, because it may result in low-quality feature extraction. On LR, MixFormer, SeqTrack, and ROMTrack achieve the best three results with 0.236, 0.232, and 0.231 SUC scores.

**Comparison on Meta Category.** Fig. 9 compares different meta categories (or coarse classes in the **supplementary material**) using SUCmeta, calculated using SUC scores of all trackers on a certain meta class. From Fig. 9, we can see that, some common categories such as boat, aircraft, vehicle and human parts are relatively easy for tracking, while rare classes like weapon, fruit, and various balls are hard to locate probably due to the lack of enough training videos from these categories, which indicates the need of more object categories for learning general tracking systems.

Due to space limitation, more evaluation and analysis can be seen in the **supplementary material**.

### Comparison to Other Benchmarks

Compared to existing benchmarks, the proposed VastTrack is more challenging due to the requirement of tracking object from more classes (in test). We present a comparison of VastTrack and other popular large-scale tracking benchmarks including TrackingNet [43], LaSOT [16], and TNL2K [50]. Please note that, GOT-10k [27] here is not compared because it adopts different metrics for evaluation. Tab. 3 reports the results of the top 15 trackers on our VastTrack and their results on TrackingNet, LaSOT, and TNL2K using SUC. From Tab. 3, we observe that, all the compared trackers have a heavy performance drop on VastTrack. For

\begin{table}
\begin{tabular}{r r r r r} \hline \hline  & \multicolumn{4}{c}{**Success Score**} \\ \cline{2-5}  & TrackingNet & LaSOT & TNL2K & VastTrack \\  & [43] & [16] & [50] & (Ours) \\ \hline \hline SeqTrack [6] & 0.855 & 0.725 & 0.578 & 0.396 \\ MixFormer [9] & 0.854 & 0.724 & 0.533 & 0.395 \\ DropMAE [52] & 0.841 & 0.718 & 0.569 & 0.375 \\ ROMTrack [4] & 0.841 & 0.714 & 0.604 & 0.370 \\ GRM [22] & 0.840 & 0.699 & 0.611 & 0.363 \\ ARTrack [51] & 0.843 & 0.708 & 0.575 & 0.356 \\ RTS [44] & 0.816 & 0.697 & 0.599 & 0.355 \\ MixFormerV2 [10] & 0.834 & 0.706 & 0.506 & 0.352 \\ ToMP [40] & 0.815 & 0.685 & 0.584 & 0.349 \\ SimTrack [5] & 0.834 & 0.705 & 0.556 & 0.344 \\ OSTrack [57] & 0.839 & 0.711 & 0.559 & 0.336 \\ STARK [56] & 0.820 & 0.671 & 0.525 & 0.334 \\ SwinTrack [38] & 0.811 & 0.672 & 0.559 & 0.330 \\ TfSiam [49] & 0.781 & 0.624 & 0.523 & 0.326 \\ PrDiMP [12] & 0.758 & 0.598 & 0.470 & 0.310 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison to other datasets.

Figure 8: Evaluation on the two most common attributes (a) and difficult attributes (b) using SUC.

Figure 9: Comparison on different meta categories using SUC score.

instance, SeqTrack, the best tracker on VastTrack, achieves high SUC scores of 0.855, 0.725, and 0.578 on TrackingNet, LaSOT, and TNL2K, while degrades to 0.396 on VastTrack with 0.459, 0.329, and 0.182 drops. OSTrack drops from 0.839, 0.711, and 0.559 SUC scores on TrackingNet, LaSOT, and TNL2K to 0.336 on VastTrack. SwinTrack degrades from 0.811, 0.672, and 0.559 on the existing benchmarks to 0.330 on VastTrack. Likewise, other trackers suffer similar drops, which shows the challenge for current trackers and there is still a long way for improving tracking.

In addition, we see an interesting observation about the relative performance of trackers from Tab. 3. Specifically, we see that a few trackers such as OSTrack and SimTrack performing better on LaSOT may perform relatively worse than others such as GRM, RTS, and ToMP on VastTrack. We argue that the possible reason is the abilities of different trackers in dealing with overfitting for object categories, which shows the need of more diverse videos with different classes in learning more general tracking.

### Retraining Experiments with VastTrack

In order to demonstrate the effectiveness of VastTrack in improving existing methods, we retrain two trackers, consisting of SiamRPN++ [33] and OSTrack [57], using the training set of VastTrack through fine-tuning. Tab. 4 displays the results. As in Tab. 4, we clearly see that, after further training, the SUC score of SiamRPN++ is clearly improved from 0.281 to 0.298 with performance gains of 1.7% on VastTrack, from 0.496 to 0.528 with 3.2% gains on LaSOT, from 0.733 to 0.762 with 2.9% gains on TrackingNet, and from 0.413 to 0.446 with 3.3% gains on TNL2K. Besides, for OSTrack, the SUC score is boosted from 0.336 to 0.362 with 2.6% improvements on VastTrack, from 0.711 to 0.722 with 1.1% gains on LaSOT, from 0.839 to 0.852 with 1.3% gains on TrackingNet, and form 0.559 to 0.579 with 2.0% gains on TNL2K. Note that, OSTrack is already strong on LaSOT but still enhanced using VastTrack. All these experiments validate the effectiveness of VastTrack in improving tracking. In the future, we will further explore the potential of VastTrack for advancing tracking performance.

## 5 Conclusion

In this paper, we propose a novel large-scale benchmark, dubbed VastTrack, to facilitate the development of more general object tracking. To this goal, VastTrack contains abundant object categories and video sequences. Specifically, it covers 2,115 classes and 50,610 videos with 4.2 million frames. To the best of our knowledge, VastTrack is to date the largest benchmark regarding class diversity and video number. Besides, VastTrack offers both bounding box annotations and language descriptions, which enables exploring both vision-only and vision-language tracking. In order to ensure the high quality, VastTrack is manually annotated with multi-round of careful inspection and refinement. We evaluate 25 trackers to analyze existing methods on VastTrack and to offer baselines for comparison. The evaluation results reveal that more efforts are needed for general tracking. By releasing VastTrack, we expect to provide a cornerstone dataset for developing more general object tracking.

**Limitation.** Despite the vast object categories and larger scale of our VastTrack, there are limitations. First, given the proposed large-scale VastTrack with vast object classes, a baseline that outperforms other trackers is not provided. Second, since most video sequences in VastTrack are relative short, it may not be suitable for long-term tracking performance evaluation, though it can be used for training long-term temporal trackers. Considering that our primary goal in this work is to offer a new benchmark with vast object categories, we leave these questions to further work by designing new trackers and by developing a new subset for long-term tracking evaluation.

**Social Impact.** By releasing VastTrack, researchers from the tracking community are able to leverage it as a platform for training more general tracking systems as well as for assessing and comparing different models, which could facilitate the deployment of tracking in more real-world applications.

**Acknowledgment.** Libo Zhang was supported by National Natural Science Foundation of China (No. 62476266). Heng Fan was not supported by any fund for this work.

\begin{table}
\begin{tabular}{r r r r r} \hline \hline  & \multicolumn{2}{c}{SiamRPN++ [33]} & \multicolumn{2}{c}{OSTrack [57]} \\ \cline{2-5}  & SUC w/o & SUC w/ & SUC w/o & SUC w/ \\  & retraining & retraining & retraining & retraining \\ \hline \hline VastTrack & 0.281 & 0.298 (0.17\%) & 0.336 & 0.362 (12.6\%) \\ LaSOT & 0.496 & 0.528 (0.32\%) & 0.711 & 0.722 (11.1\%) \\ TrackingNet & 0.733 & 0.762 (12.9\%) & 0.839 & 0.852 (11.3\%) \\ TNL2K & 0.413 & 0.446 (\(\uparrow\)3.3\%) & 0.559 & 0.579 (12.0\%) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Further training with VastTrack.

## References

* [1] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutional siamese networks for object tracking. In _ECCVW_, 2016.
* [2] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning discriminative model prediction for tracking. In _ICCV_, 2019.
* [3] Wenrui Cai, Qingjie Liu, and Yunhong Wang. Hiptrack: Visual tracking with historical prompts. In _CVPR_, 2024.
* [4] Yidong Cai, Jie Liu, Jie Tang, and Gangshan Wu. Robust object modeling for visual tracking. In _ICCV_, 2023.
* [5] Boyu Chen, Peixia Li, Lei Bai, Lei Qiao, Quihong Shen, Bo Li, Weihao Gan, Wei Wu, and Wanli Ouyang. Backbone is all your need: A simplified architecture for visual object tracking. In _ECCV_, 2022.
* [6] Xin Chen, Houwen Peng, Dong Wang, Huchuan Lu, and Han Hu. Seqtrack: Sequence to sequence learning for visual object tracking. In _CVPR_, 2023.
* [7] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer tracking. In _CVPR_, 2021.
* [8] Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang, and Rongrong Ji. Siamese box adaptive network for visual tracking. In _CVPR_, 2020.
* [9] Yutao Cui, Cheng Jiang, Limin Wang, and Gangshan Wu. Mixformer: End-to-end tracking with iterative mixed attention. In _CVPR_, 2022.
* [10] Yutao Cui, Tianhui Song, Gangshan Wu, and Limin Wang. Mixformerv2: Efficient fully transformer tracking. In _NeurIPS_, 2023.
* [11] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Atom: Accurate tracking by overlap maximization. In _CVPR_, 2019.
* [12] Martin Danelljan, Luc Van Gool, and Radu Timofte. Probabilistic regression for visual tracking. In _CVPR_, 2020.
* [13] Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, and Deva Ramanan. Tao: A large-scale benchmark for tracking any object. In _ECCV_, 2020.
* [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.
* [15] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Harshit, Mingzhen Huang, Juehuan Liu, et al. Lasot: A high-quality large-scale single object tracking benchmark. _IJCV_, 129:439-461, 2021.
* [16] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale single object tracking. In _CVPR_, 2019.
* [17] Heng Fan, Halady Akhilesha Miththanthaya, Siranjiv Ramana Rajan, Xiaoqiong Liu, Zhilin Zou, Yuewei Lin, Haibin Ling, et al. Transparent object tracking benchmark. In _ICCV_, 2021.
* [18] Heng Fan, Fan Yang, Peng Chu, Yuewei Lin, Lin Yuan, and Haibin Ling. Tracklinic: Diagnosis of challenge factors in visual tracking. In _WACV_, 2021.
* [19] Qi Feng, Vitaly Ablavsky, Qinxun Bai, and Stan Sclaroff. Siamese natural language tracker: Tracking by natural language descriptions with siamese trackers. In _CVPR_, 2021.
* [20] Zhihong Fu, Qingjie Liu, Zehua Fu, and Yunhong Wang. Stmtrack: Template-free visual tracking with space-time memory networks. In _CVPR_, 2021.
* [21] Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva Ramanan, and Simon Lucey. Need for speed: A benchmark for higher frame rate object tracking. In _ICCV_, 2017.
* [22] Shenyuan Gao, Chunluan Zhou, and Jun Zhang. Generalized relation modeling for transformer tracking. In _CVPR_, 2023.

* [23] Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, and Shengyong Chen. Siamcar: Siamese fully convolutional classification and regression for visual tracking. In _CVPR_, 2020.
* [24] Mingzhe Guo, Zhipeng Zhang, Heng Fan, and Liping Jing. Divert more attention to vision-language tracking. _NeurIPS_, 2022.
* [25] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In _CVPR_, 2019.
* [26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* [27] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity benchmark for generic object tracking in the wild. _IEEE TPAMI_, 43(5):1562-1577, 2021.
* [28] Sajid Javed, Martin Danelljan, Fahad Shahbaz Khan, Muhammad Haris Khan, Michael Felsberg, and Jiri Matas. Visual object tracking with discriminative filters and siamese networks: a survey and outlook. _IEEE TPAMI_, 45(5):6552-6574, 2023.
* [29] Matej Kristan, Jiri Matas, Ales Leonardis, Tomas Vojir, Roman Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih Porikli, and Luka Cehovin. A novel performance evaluation methodology for single-target trackers. _TPAMI_, 38(11):2137-2155, 2016.
* [30] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _NIPS_, 2012.
* [31] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _IJCV_, 128(7):1956-1981, 2020.
* [32] Annan Li, Min Lin, Yi Wu, Ming-Hsuan Yang, and Shuicheng Yan. Nus-pro: A new visual tracking challenge. _TPAMI_, 38(2):335-349, 2016.
* [33] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. Siamrpn++: Evolution of siamese visual tracking with very deep networks. In _CVPR_, 2019.
* [34] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep visual tracking: Review and experimental comparison. _PR_, 76:323-338, 2018.
* [35] Zhenyang Li, Ran Tao, Efstratios Gavves, Cees GM Snoek, and Arnold WM Smeulders. Tracking by natural language specification. In _CVPR_, 2017.
* [36] Pengpeng Liang, Erik Blasch, and Haibin Ling. Encoding color information for visual tracking: Algorithms and benchmark. _TIP_, 24(12):5630-5644, 2015.
* [37] Liting Lin, Heng Fan, Zhipeng Zhang, Yaowei Wang, Yong Xu, and Haibin Ling. Tracking meets lora: Faster training, larger model, stronger performance. In _ECCV_, 2024.
* [38] Liting Lin, Heng Fan, Zhipeng Zhang, Yong Xu, and Haibin Ling. Swintrack: A simple and strong baseline for transformer tracking. _NeurIPS_, 2022.
* [39] Seyed Mojtaba Marvasti-Zadeh, Li Cheng, Hossein Ghanei-Yakhdan, and Shohreh Kasaei. Deep learning for visual tracking: A comprehensive survey. _IEEE TITS_, 23(5):3943-3968, 2022.
* [40] Christoph Mayer, Martin Danelljan, Goutam Bhat, Matthieu Paul, Danda Pani Paudel, Fisher Yu, and Luc Van Gool. Transforming model prediction for tracking. In _CVPR_, 2022.
* [41] George A Miller. Wordnet: a lexical database for english. _Communications of the ACM_, 38(11):39-41, 1995.
* [42] Matthias Mueller, Neil Smith, and Bernard Ghanem. A benchmark and simulator for uav tracking. In _ECCV_, 2016.
* [43] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, and Bernard Ghanem. Trackingnet: A large-scale dataset and benchmark for object tracking in the wild. In _ECCV_, 2018.
* [44] Matthieu Paul, Martin Danelljan, Christoph Mayer, and Luc Van Gool. Robust visual tracking by segmentation. In _ECCV_, 2022.

* [45] Arnold WM Smeulders, Dung M Chu, Rita Cucchiara, Simone Calderara, Afshin Dehghan, and Mubarak Shah. Visual tracking: An experimental survey. _IEEE TPAMI_, 36(7):1442-1468, 2013.
* [46] Jack Valmadre, Luca Bertinetto, Joao F Henriques, Ran Tao, Andrea Vedaldi, Arnold Smeulders, Philip Torr, and Efstratios Gavves. Long-term tracking in the wild: A benchmark. In _ECCV_, 2018.
* [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NIPS_, 2017.
* [48] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. In _ICCV_, 2023.
* [49] Ning Wang, Wengang Zhou, Jie Wang, and Houqiang Li. Transformer meets tracker: Exploiting temporal context for robust visual tracking. In _CVPR_, 2021.
* [50] Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, and Feng Wu. Towards more flexible and accurate object tracking with natural language: Algorithms and benchmark. In _CVPR_, 2021.
* [51] Xing Wei, Yifan Bai, Yongchao Zheng, Dahu Shi, and Yihong Gong. Autoregressive visual tracking. In _CVPR_, 2023.
* [52] Qiangqiang Wu, Tianyu Yang, Ziquan Liu, Baoyuan Wu, Ying Shan, and Antoni B Chan. Dropmae: Masked autoencoders with spatial-attention dropout for tracking tasks. In _CVPR_, 2023.
* [53] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: A benchmark. In _CVPR_, 2013.
* [54] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object tracking benchmark. _TPAMI_, 37(9):1834-1848, 2015.
* [55] Jinxia Xie, Bineng Zhong, Zhiyi Mo, Shengping Zhang, Liangtao Shi, Shuxiang Song, and Rongrong Ji. Autoregressive queries for adaptive tracking with spatio-temporal transformers. In _CVPR_, 2024.
* [56] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformer for visual tracking. In _ICCV_, 2021.
* [57] Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Joint feature learning and relation modeling for tracking: A one-stream framework. In _ECCV_, 2022.
* [58] Alper Yilmaz, Omar Javed, and Mubarak Shah. Object tracking: A survey. _ACM CSUR_, 38(4):13-es, 2006.
* [59] Zhipeng Zhang, Yihao Liu, Xiao Wang, Bing Li, and Weiming Hu. Learn to match: Automatic matching network design for visual tracking. In _ICCV_, 2021.
* [60] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and Weiming Hu. Ocean: Object-aware anchor-free tracking. In _ECCV_, 2020.
* [61] Li Zhou, Zikun Zhou, Kaige Mao, and Zhenyu He. Joint visual grounding and tracking with natural language specification. In _CVPR_, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? Please see that in the supplementary material. 3. Did you discuss any potential negative societal impacts of your work? Please see that in the supplementary material. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]. We provide the Github link to all our data, evaluation toolkit, and the evaluation results. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [N/A]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]Appendix

In this appendix, we provide more details, analysis, results, and discussion of VastTrack (project with data, code, and results is at [https://github.com/HengLan/VastTrack](https://github.com/HengLan/VastTrack)), including

* **A1 Details of Object Classes** We show the detailed object classes in VastTrack and number of sequences in each category.
* **A2 Statistic of the Video Length** We present the statistic of video length on VastTrack.
* **A3 Summary of Evaluated Trackers** We show the detailed summary of 25 evaluated trackers.
* **A4 Full Results of Attribute-based Evaluation** We show the full attribute-based evaluation results for all evaluated trackers in precision (PRE), normalized precision (NPRE), and success (SUC).
* **A5 Maintenance, Ethical Issue, and Responsible Usage of VastTrack** We discuss the maintenance, ethical issue, and responsible usage of our VastTrack.

### Details of Object Classes

VastTrack covers 2,115 classes, aiming to facilitate the development of universal and general object tracking. We collect these categories in a hierarchical way. In specific, we first collect 30 coarse classes, including "_Human_", "_Human Part_", "_Land Animal_", "_ aquatic Animal_", "_Bird_", "_Motor Vehicle_", "_Non-motor Vehicle_", "_Motor Boat_", "_Non-motor Boat_", "_Accessory_", "_Aircraft_", "_Ball_", "_Clothing and Shoe_", "_Container_", "_Currency_", "_Electronics_", "_Fruit_", "_Kitchenware_", "_Game_", "_Insect_", "_Fungus_", "_Music Instrument_", "_Plant_", "_Sport Equipment_", "_Tool_", "_Toy_", "_Weapon_", "_Transportation Part_", "_Rail Vehicle_", and "_Others_". Then, we further gather 2,115 fine object categories in each coarse classes. All these fine categories are verified by the expert to ensure that

Figure 10: Category organization of VastTrack. Please zoom in.

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:19]

T (114 classes)

_T Intersection Sign_ (6), _Table Tennis_ (1), _Table Tennis Bars_ (19), _Table Tennis Player_ (39), _Tac-50_ (3), _Taiko Dram_ (29), _Takin_ (35), _Tamarid Fruit_ (4), _Tango Dancer_ (30), _Tank_ (55), _Tanta_ (13), _Tay Toame_ (34), _Tar Jamar Mea_ (34), _Tar Jamar Mea_ (21), _Tawanm Devil_ (31), _T-Reed_ (3), _Tear Jamar Mea_ (21), _Tou Taee Muham_ (2), _Tear-Toter_ (20), _Tello Kag_ (11), _Telescopic_ (11), _Telescopic_ (23), _Teriu_ (28), _Tenmots Courser_ (9), _Temporary Road Closed Sign_ (2), _Teriu_ (30), _Teriu Taee Muham_ (30), _Teriu Taee Muham_ (42), _Teriu Taee Muham_ (29), _Teras Pickly Pear Cactus_ (5), _Tallasuresu Sanchoircus_ (8), _The Witcher_ (26), _Therodo_ (16), _Thermometer_ (14), _Thickness Gauge_ (100), _Tinable Cactus_ (24), _Thorry Dewil_ (30), _Thread Gang_ (33), _Trier-Scient Staff_ (35), _Triergeant Gourani_ (20), _Threcher Start_ (34), _Throw Bag_ (73), _Throwing Age_ (34), _Throwing Kripe Demonstrations_ (29), _Tear_ (105), _Tartu Matsuf_ (19), _Threat MeteDeg_ (35), _Tiger Tizer_ (27), _Tiger Bay_ (11), _Tiger Hook Swords_ (33), _Tiber Shart_ (35), _Tile Space_ (23), _Thiling Platforms_ (17), _Tinnocent_ (28), _Thin White_ (29), _Tire Swing_ (36), _Tissue Box_ (34), _Toddier Play Araa_ (2), _Tolfi Brush_ (10), _Toll Booth Sign_ (11), _Tinn Dameh Cactus_ (33), _Tonahoku_ (31), _Tonahoku_ (2), _Tonato Sizer_ (4), _Tongy Roadplane Bottle_ (4), _Tordus Cactus_ (33), _Torge Wrench A_ (44), _Tordusite Beetle_ (5), _Tote Bag_ (33), _Toudscreen Gloriav_ (4), _Tourist Guide_ (9), _Toy Bar_ (5), _Tay Car Wheel_ (31), _Toy Piano_ (29), _Tuck and FridA Vehicle_ (30), _Tracks Train Min_ (3), _Tracks Train Min_ (3), _Tracksit_ (4), _Traditional Chinese Sword Dance_ (24), _Traditiones Pursuit Al_ (3), _Twin (31), _Tray O_ (3), _Treesid Up_ (10), _Treach Club_ (27), _Trench Cost_ (30), _Trench Creacy_ (32), _Trizing_ (35), _Triage Beet Board_ (5), _Trichocrease_ (10), _Tripped Stand_ (20), _Tracer Needle_ (20), _Trodo_ (30), _Trolley Bag_ (41), _Trumbone_ (48), _Truact Racing Truck_ (48), _Trumpet_ (51), _T-Shirt_ (3), _T-Shirt Dreams_ (2), _Tube Tabu_ (34), _Trided Capua_ (3), _Trided Dover_ (35), _Trided Puffin_ (4), _Tryboard Ride_ (3), _Tumble Dreams_ (7), _Tumble Tracks_ (6), _Tuna Crab_ (24), _Tunnel Max_ (2), _Turkish Line_ (2), _Tarks Cap Cactus_ (4), _Tartleneck_ (3), _Tyranosaurus Rex_ (28).

### U (11 classes)

_Uav (48), Ulvalle (49), Umbrella_ (31), _Undercur (8), _Uniform (4), United Arab Emirates Dirham_ (2), _Upside-Down Jellyfish_ (35), _Us Dollars_ (3), _Utility Knife_ (10), _Uv Sterillcer Vacuum_ (26), _Uv-Viz_ (3).

**V (18 classes)**

_Vacnum Cleaner_ (40), _Vacnum Cleaner Bronk Roller_ (99), _Vacnum Cleaner Cervice Tool Wheel_ (18), _Numpire Squid_ (25), _Vanilla Bean_ (7), _Vernier Caliber_ (107), _Vext_ (8), _Vial Adjutter_ (1), _Vibration Meter_ (34), _Viernamese Leaf Monkey_ (8), _Vintage Aircraft_ (35), _Vintage Car_ (40), _Vidinistr_ (12), _Virtual Reality Header_ (3), _Virtual Reality Rides_ (4), _Viscomer_ (50), _Volfesball_ (30), _Volfesball Player_ (49).

**W (60 classes)**

_Wuffle Wheel_ (10), _Wagon Wheel_ (1), _Winter_ (35), _Walter Tallas_ (31), _Wall Decimen Building_ (12), _Wall Scanner_ (20), _Wallary_ (34), _Wallball_ (30), _Walllet_ (30), _Walllet_ (31), _Walllet_ (32), _Walllet_ (33), _Walllet_ (22), _Winter (23), _Winter Boite_ (2), _Winter Film Bottle_ (29), _Winter Gan_ (28), _Winter Painting Wall_ (3), _Water Play Table_ (3), _Water Pol_ (18), _Water Sooters_ (10), _Water Walking Balls_ (10), _Water Wheel_ (34), _Watering Truck_ (35), _Wave Slide_ (10), _Weddesil (32), _Wedges_ (83), _Wiederard_ (47), _Wave First Fish_ (28), _Wieffighter_ (34), _Western Pymmy Mammer_ (2), _Wumenella Nappiannlo_ (10), _Walllet Shat_ (14), _Water Shat_ (20), _Wheelchair (200), _Wheelchair Wheel_ (50), _Wheeling Whitproducts_ (4), _Wink_ (8), _Wink-Checked Spider Monkey_ (5), _White-Faced Sali_ (3), _White-Fended Term_ (9), _White-Headed Marmanent_ (3), _White-Diped Dover_ (15), _White-Lipped Tamerina_ (3), _White-Niped Carne_ (43), _White-Niped Suki_ (2), _White-Lilled Ralfwhyment_ (35), _White-Throuted Rock Thrush_ (10), _Whoper Swan_ (50), _Wide-Brimmed Hat_ (3), _Wieds Marmser_ (3), _Will Boar_ (33), _Will Speed Gauge_ (4), _Wine Glass_ (49), _Wire Stripper_ (3), _Wok_ (40), _Wolverine_ (25), _Workwear_ (7), _Werianuch Wheel_ (9), _Wining Rush_ (29), _Wurifier Piano_ (32).

**X (3 classes)**

_Xema Sabini_ (4), _Xen109_ (1), _Xylaphone_ (32).

**Y (17 classes)**

_Yanagun_ (31), _Yellow Crested Wendfish_ (14), _Yellow Peach_ (23), _Yellow Wurbler_ (34), _Yellow-Belliel Ti_ (34), _Yellow-Billed Maggire_ (33), _Yellow-Checked Tri_ (45), _Wellown Flasher Wrasse_ (8), _Yellow-Hunped Warbler_ (31), _Yellowtail Dameh selfish_ (35), _Yellow-Throuted Marren_ (35), _Yellow-Throuted Warbler_ (5), _Toga Mat Rag_ (42), _Toga Wheel_ (34), _Tyo Whel_ (34), _Tyo-Yo_ (30), _T-Sito Injection Port_ (1).

**Z (9 classes)**

_Zebra_ (33), _Zebra Danilo_ (34), _Zebra Finch_ (45), _Zebra Loach_ (47), _Zebra Ploco_ (45), _Zebra Shark_ (40), _Zhammadoa_ (3), _Zhongshan Suit_ (3), _Zither_ (32).

### Summary of Evaluated Trackers

For better understanding of the evaluated trackers, we provide a summary in Tab. 5. All these trackers are evaluated as they as without modifications. It is worth noting that, the work of DropMAE is marked with "TP" is because it employs temporal information for pre-training.

### Full Results of Attribute-based Evaluation

We present evaluation for all trackers under ten attributes. Specifically, Fig. 12, Fig. 13, and Fig. 14 respectively show the attribute-based evaluations using PRE, NPRE, and SUC. From these evaluation results, we can observe that, existing state-of-the-art trackers heavily suffer from various challenges in the videos. To achieve general tracking, more efforts are needed to improve their robustness.

### Maintenance, Ethical Issue, and Responsible Usage of VastTrack

**Maintenance.** Our VastTrack is hosted on Github ([https://github.com/HengLan/VastTrack](https://github.com/HengLan/VastTrack)). This allows us to conveniently check feedback from the community and to improve VastTrack via necessary maintenance and updates by research groups of senior authors of VastTrack. Besides, we'll try our best to continue assessing future trackers to offer up-to-date evaluation and comparison on VastTrack. The ultimate goal is to offer a long-term and stable platform for the tracking community.

**Ethical Considerations Related to Videos.** We avoid using private videos and all videos are collected under the Creative Commons license for research only. However, we understand the license might change in future. Once any notification regarding this is received, we'll take action to handle it.

**Guidelines for the Responsible Use of VastTrack.** The development of VastTrack aims to facilitate the research and application of tracking. It can be used for _research purpose only_. Please note that, due to inevitable bias during data collection, there may exist geographic and demographic imbalance.

Figure 11: Distribution of sequence length on VastTrack. The average video length of VastTrack is 83 frames (\(\sim\)14 seconds with frame rate 6 _fps_). Please note that, VastTrack is focused on short-term tracking, but could also be used for training long-term temporal trackers as discussed in the main text.

\begin{table}
\begin{tabular}{r c c c c} \hline \hline
**Tracker** & **Where** & **Backbone** & **Type** & **TP** \\ \hline \hline SiamFC [1] & ECCVW’16 & AlexNet & CNN & ✗ \\ ATOM [11] & CVPR’19 & ResNet-18 & CNN & ✓ \\ SiamRPN++ [33] & CVPR’19 & ResNet-50 & CNN & ✗ \\ DiMP [2] & ICCV’19 & ResNet-50 & CNN & ✓ \\ SiamBAN [8] & CVPR’20 & ResNet-50 & CNN & ✗ \\ SiamCAR [23] & CVPR’20 & ResNet-50 & CNN & ✗ \\ PhDiMP [12] & CVPR’20 & ResNet-50 & CNN & ✓ \\ Ocean [60] & ECCV’20 & ResNet-50 & CNN & ✓ \\ STMTrack [20] & CVPR’21 & GoogLeNet & CNN & ✓ \\ TfSiam [49] & CVPR’21 & ResNet-50 & CNN-T & ✓ \\ Transf’ [7] & CVPR’21 & ResNet-50 & CNN-T & ✗ \\ STARK [5] & ICCV’21 & ResNet-101 & CNN-T & ✓ \\ AutoMatch [59] & ICCV’21 & ResNet-50 & CNN & ✗ \\ ToMP [40] & CVPR’22 & ResNet-101 & CNN-T & ✓ \\ MixFormer (L) [9] & CVPR’22 & CVT-24-W & Trans. & ✓ \\ OSTrack (384) [57] & ECCV’22 & ViT-Base & Trans. & ✗ \\ RTS [44] & ECCV’22 & ResNet-50 & CNN & ✓ \\ SimTrack (224) [5] & ECCV’22 & ViT-Large & Trans. & ✗ \\ SwinTrack (224) [38] & NeurIPS’22 & SwinT & Trans. & ✓ \\ SeqTrack (L384) [6] & CVPR’23 & ViT-Large & Trans. & ✓ \\ ARTtrack (256) [51] & CVPR’23 & ViT-Large & Trans. & ✓ \\ DropDAAE [52] & CVPR’23 & ViT & Trans. & ✓ \\ GRM (256) [22] & CVPR’23 & ViT-Base & Trans. & ✗ \\ ROMTrack (384) [4] & ICCV’23 & ViT-Base & Trans. & ✓ \\ MixFormerV’2 (B) [10] & NeurIPS’23 & ViT-Base & Trans. & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 5: Summary of the evaluated tracking algorithms. “CNN”: CNN-based; “CNN-T”: CNN-Transformer-based, “Trans.”: Transformer-based. TP: “✓” for trackers leveraging temporal information, and “✗” for trackers using only the information from initial frame for tracking.

Figure 14: Performance of trackers on ten attributes, including ARC, BC, DEF, FM, INV, IV, LR, MB, ROT, and SV, using success (SUC).

Figure 12: Performance of trackers on ten attributes, including ARC, BC, DEF, FM, INV, IV, LR, MB, ROT, and SV, using precision (PRE).

Figure 13: Performance of trackers on ten attributes of ARC, BC, DEF, FM, INV, IV, LR, MB, ROT, and SV using normalized precision (NPRE).