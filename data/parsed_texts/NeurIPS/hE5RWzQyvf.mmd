# Distributionally Robust Linear Quadratic Control

 Bahar Taskesen

EPFL

bahar.taskesen@epfl.ch &Dan A. Iancu

Stanford University

daniancu@stanford.edu &Cagli Kocyigit

University of Luxembourg

cagli.kocyigit@uni.lu &Daniel Kuhn

EPFL

daniel.kuhn@epfl.ch

###### Abstract

Linear-Quadratic-Gaussian (LQG) control is a fundamental control paradigm that has been studied and applied in various fields such as engineering, computer science, economics, and neuroscience. It involves controlling a system with linear dynamics and imperfect observations, subject to additive noise, with the goal of minimizing a quadratic cost function depending on the state and control variables. In this work, we consider a generalization of the discrete-time, finite-horizon LQG problem, where the noise distributions are unknown and belong to Wasserstein ambiguity sets centered at nominal (Gaussian) distributions. The objective is to minimize a worst-case cost across all distributions in the ambiguity set, including non-Gaussian distributions. Despite the added complexity, we prove that a control policy that is linear in the observations is optimal, as in the classic LQG problem. We propose a numerical solution method that efficiently characterizes this optimal control policy. Our method uses the Frank-Wolfe algorithm to identify the least-favorable distributions within the Wasserstein ambiguity sets and computes the controller's optimal policy using Kalman filter estimation under these distributions.

## 1 Introduction

The Linear Quadratic Regulator (LQR) is a classic control problem that has served as a building block for numerous applications in engineering and computer science [3, 12], economics [29], or neuroscience [47]. It involves controlling a system with linear dynamics and imperfect observations affected by additive noise, with the goal of minimizing a quadratic state and control cost. Under the assumption that noise terms are independent and normally distributed (a case referred to as Linear-Quadratic-Gaussian, or LQG), it is well known that the optimal control policy depends linearly on the observations and can be obtained efficiently by using the Kalman filtering procedure and dynamic programming [8].

Motivated by practical settings where noise distributions may not be readily available or may not be Gaussian, this paper considers a discrete-time, finite-horizon generalization of the LQG setting where noise distributions are unknown and are chosen adversarially from ambiguity sets characterized by a Wasserstein distance and centered around nominal (Gaussian) distributions.

We show that, even under distributional ambiguity, the optimal control policy remains linear in the system's observations. Our proof is novel and does not rely on traditional recursive dynamic programming arguments. Instead, we re-parametrize the control policy in terms of the purified state observations and we derive an upper bound for the resulting minimax formulation by relaxing the ambiguity set (from a Wasserstein ball into a Gelbrich ball) while simultaneously restricting the controller to linear dependencies. We then use convex duality to prove that this upper bound matchesa lower bound obtained by restricting the ambiguity set in the dual of the minimax formulation. This implies the optimality of linear output feedback controllers, thus generalizing the classic results to a distributionally robust setting.

We also find that the worst-case distribution is actually Gaussian, which leads to a very efficient algorithm for finding optimal controllers. Specifically, we propose an algorithm based on the Frank-Wolfe first-order method that at every step solves sub-problems corresponding to classic LQG control problems, using Kalman filtering and dynamic programming. We show that this algorithm enjoys a sublinear convergence rate and is susceptible to parallelization. Lastly, we implement the algorithm leveraging PyTorch's automatic differentiation module and we find that it yields uniformly lower runtimes than a direct method (based on solving semidefinite programs) across all problem horizons.

### Literature Review

This paper is related to the ample literature in control theory and engineering aimed at designing controllers that are robust to noise. The classic LQR/LQG theory, developed in the 1960s, examined linear dynamical systems in either time or frequency domain, seeking to minimize a combination of quadratic state and control costs (in time-domain) or the \(\mathcal{H}_{2}\) norm of the system's transfer function (in frequency domain). Motivated by findings that LQG controllers do not provide the guaranteed robust stability properties of LQR controllers [15], much effort has been devoted subsequently to designing controllers that are robust to worst-case perturbations, typically evaluated in terms of the \(\mathcal{H}_{\infty}\) norm of the system's transfer function (see, e.g., [16, 53] for a comprehensive review of \(\mathcal{H}_{\infty}\) and \(\mathcal{H}_{2}\) controllers). Because \(\mathcal{H}_{\infty}\) controllers tend to be overly conservative [32], various approaches have been proposed to balance the performance of nominal and robust controllers, e.g., by combining \(\mathcal{H}_{2}\) and \(\mathcal{H}_{\infty}\) approaches [7, 17]. A parallel stream of literature has considered risk-sensitive control [51], which minimizes an entropic risk measure instead of the expected quadratic cost. Although risk-sensitive control has a distributionally robust flavor (as the entropic risk measure is equivalent to a distributionally robust quadratic objective penalized via Kullback-Leibler divergence), risk-sensitive control models do not admit a distributionally robust formulation because the entropic risk measure is convex, but not coherent [22]. In contrast, our distributionally robust model provides a direct interpretation of the exact set of noise distributions against which the controller provides safeguards, and leads to a computationally tractable framework for finding the optimal controller.

In this sense, our work is more directly related to the literature on distributionally robust control, which seeks controllers that minimize expected costs under worst-case noise distributions [11, 33, 34, 41, 50, 52]. Closest to our work are [28, 33]. [33] proves the optimality of linear state-feedback control policies for a related minimax LQR model with a Wasserstein distance but with _perfect_ state observations. With perfect observations, the optimal policies in the classic LQR formulation are independent of the noise distribution and are thus inherently already robust, so considering imperfect observations is what makes the problem significantly more challenging in our case. [28] studies a minimax formulation based on the Wasserstein distance with both state and observation noise but without any control policy, and focuses solely on the problem of estimating the states. Several papers have considered robust formulations with imperfect observations but for constrained systems [5, 6, 34], which are more challenging; the common approach is to restrict attention to linear feedback policies for computational tractability, and without proving their optimality.

Also related is the recent literature stream on distributionally robust optimization using the Wasserstein distance [36]. Within this stream, the closest work is [38, 44], which consider the problem of minimax mean-squared-error estimation when ambiguity is modeled with a Wasserstein distance from a nominal Gaussian distribution. Our proof builds on some ideas from these papers (e.g., relying on the Gelbrich distance in the construction of the upper bound), which it combines with ideas from control theory on purified output-feedback to obtain the overall construction. Also related is [2], which studies multistage distributionally robust problems with ambiguity sets given by a nested Wasserstein distance for stochastic processes and identifies computationally tractable cases. For a broader overview of developments related to optimal transport and Wasserstein distance with an emphasis on computational tractability and applications in machine learning, we refer to [42].

Finally, our paper is also related to literature that documents the optimality of linear/affine policies in (distributionally) robust dynamic optimization models. [10, 30] prove optimality for one-dimensional linear systems affected by additive noise and with perfect state observations, but with general (convex) state and/or control costs, [27, 49] provide computationally tractable approaches to quantifying the suboptimality of affine controllers in finite or infinite-horizon settings, and [9, 21, 25] characterize the performance of affine policies in two-stage (distributionally) robust dynamic models.

_Notation._ All random objects are defined on a probability space \((\Omega,\mathcal{F},\mathbb{P})\). Thus, the distribution of any random vector \(\xi:\Omega\to\mathbb{R}^{d}\) is given by the pushforward distribution \(\mathbb{P}_{\xi}=\mathbb{P}\circ\xi^{-1}\) of \(\mathbb{P}\) with respect to \(\xi\). The expectation under \(\mathbb{P}\) is denoted by \(\mathbb{E}_{\mathbb{P}}[\cdot]\). For any \(t\in\mathbb{Z}_{+}\), we set \([t]=\{0,\ldots,t\}\).

## 2 Problem Definition

We consider a discrete-time linear dynamical system

\[x_{t+1}=A_{t}x_{t}+B_{t}u_{t}+w_{t}\quad\forall t\in[T-1]\] (1)

with states \(x_{t}\in\mathbb{R}^{n}\), control inputs \(u_{t}\in\mathbb{R}^{m}\), process noise \(w_{t}\in\mathbb{R}^{n}\) and system matrices \(A_{t}\in\mathbb{R}^{n\times n}\) and \(B_{t}\in\mathbb{R}^{n\times m}\). The controller only has access to imperfect state measurements

\[y_{t}=C_{t}x_{t}+v_{t}\quad\forall t\in[T-1]\] (2)

corrupted by observation noise \(v_{t}\in\mathbb{R}^{p}\), where \(C_{t}\in\mathbb{R}^{p\times n}\) and usually \(p\leq n\) (so that observing \(y_{t}\) would not allow reconstructing \(x_{t}\) even if there were no observation noise). The control inputs \(u_{t}\) are causal, i.e., depend on the past observations \(y_{0},\ldots,y_{t}\) but not on the future observations \(y_{t+1},\ldots,y_{T-1}\). More precisely, the set of feasible control inputs \(\mathcal{U}_{y}\) is the set of random vectors \((u_{0},u_{1},\ldots,u_{T-1})\) where for every \(t\) there exists a measurable control policy \(\varphi_{t}:\mathbb{R}^{p(t+1)}\to\mathbb{R}^{m}\) such that \(u_{t}=\varphi_{t}(y_{0},\ldots,y_{t})\). Controlling the system generates costs that depend quadratically on the states and the controls:

\[J=\sum_{t=0}^{T-1}(x_{t}^{\top}Q_{t}x_{t}+u_{t}^{\top}R_{t}u_{t})+x_{T}^{\top} Q_{T}x_{T},\] (3)

where \(Q_{t}\in\mathbb{S}_{+}^{n}\) and \(R_{t}\in\mathbb{S}_{++}^{m}\) represent the state and input cost matrices, respectively. The exogenous random vectors \(x_{0}\), \(\{w_{t}\}_{t=0}^{T-1}\) and \(\{v_{t}\}_{t=0}^{T-1}\) are mutually independent and follow probability distributions given by \(\mathbb{P}_{x_{0}},\{\mathbb{P}_{w_{t}}\}_{t=0}^{T-1}\), and \(\{\mathbb{P}_{v_{t}}\}_{t=0}^{T-1}\), respectively. As the control inputs are causal, the system equations (2) imply that \(x_{t}\), \(u_{t}\) and \(y_{t}\) can be expressed as measurable functions of the exogenous uncertainties \(x_{0}\) as well as \(w_{s}\) and \(v_{s}\), \(s\in[t]\), for every \(t\). From now on we may thus assume without loss of generality that \(\Omega=\mathbb{R}^{n}\times\mathbb{R}^{n\times T}\times\mathbb{R}^{p\times T}\) is the space of realizations of the exogenous uncertainties, \(\mathcal{F}\) is the Borel \(\sigma\)-algebra on \(\Omega\) and \(\mathbb{P}=\mathbb{P}_{x_{0}}\otimes(\otimes_{t=0}^{T-1}\mathbb{P}_{w_{t}}) \otimes(\otimes_{t=0}^{T}\mathbb{P}_{v_{t}})\), where \(\mathbb{P}_{1}\otimes\mathbb{P}_{2}\) denotes the independent coupling of the distributions \(\mathbb{P}_{1}\) and \(\mathbb{P}_{2}\).

In this context, the classic LQG model assumes that \(\mathbb{P}\) is known and Gaussian, and seeks \(u\in\mathcal{U}_{y}\) that minimizes \(\mathbb{E}_{\mathbb{P}}[J]\). Appendix SSA reviews the standard approach for computing optimal control inputs by estimating states through Kalman filtering techniques and using dynamic programming.

In contrast, we assume that \(\mathbb{P}\) is only known to belong to an ambiguity set \(\mathcal{W}\), and we formulate a distributionally robust LQG problem that seeks \(u\in\mathcal{U}_{y}\) to minimize the worst-case expected cost:

\[\max_{\mathbb{P}\in\mathcal{W}}\mathbb{E}_{\mathbb{P}}\left[\sum_{t=0}^{T-1}(x _{t}^{\top}Q_{t}x_{t}+u_{t}^{\top}R_{t}u_{t})+x_{T}^{\top}Q_{T}x_{T}\right].\] (4)

We construct the ambiguity set \(\mathcal{W}\) as a ball based on the Wasserstein distance. Specifically, we assume that a _nominal_ Gaussian distribution \(\hat{\mathbb{P}}=\hat{\mathbb{P}}_{x_{0}}\otimes(\otimes_{t=0}^{T-1}\hat{ \mathbb{P}}_{w_{t}})\otimes(\otimes_{t=0}^{T}\hat{\mathbb{P}}_{v_{t}})\) is available so that \(\hat{\mathbb{P}}_{x_{0}}=\mathcal{N}(0,\hat{X}_{0})\), \(\hat{\mathbb{P}}_{w_{t}}=\mathcal{N}(0,\hat{W}_{t})\), and \(\hat{\mathbb{P}}_{v_{t}}=\mathcal{N}(0,\hat{V}_{t})\) for all \(t\in[T-1]\), and \(\mathcal{W}\) is given by:

\[\mathcal{W}=\mathcal{W}_{x_{0}}\otimes(\otimes_{t=0}^{T-1}\mathcal{W}_{w_{t}}) \otimes(\otimes_{t=0}^{T-1}\mathcal{W}_{v_{t}}),\]

where

\[\mathcal{W}_{x_{0}} =\{\mathbb{P}_{x_{0}}\in\mathcal{P}(\mathbb{R}^{n}):\mathrm{W}( \hat{\mathbb{P}}_{x_{0}},\mathbb{P}_{x_{0}})\leq\rho_{x_{0}},\ \mathbb{E}_{\mathbb{P}_{x_{0}}}[x_{0}]=0\}\] \[\mathcal{W}_{w_{t}} =\{\mathbb{P}_{w_{t}}\in\mathcal{P}(\mathbb{R}^{n}):\mathrm{W}( \hat{\mathbb{P}}_{w_{t}},\mathbb{P}_{w_{t}})\leq\rho_{w_{t}},\ \mathbb{E}_{\mathbb{P}_{w_{t}}}[w_{t}]=0\}\] \[\mathcal{W}_{v_{t}} =\{\mathbb{P}_{v_{t}}\in\mathcal{P}(\mathbb{R}^{m}):\mathrm{W}( \hat{\mathbb{P}}_{v_{t}},\mathbb{P}_{v_{t}})\leq\rho_{v_{t}},\ \mathbb{E}_{\mathbb{P}_{v_{t}}}[v_{t}]=0\},\]

and \(\mathrm{W}\) is the \(2\)-Wasserstein distance. Thus, by construction, all exogenous random variables \(x_{0},w_{0},\ldots,w_{T-1},v_{0},\ldots,v_{T-1}\) are independent under every distribution in \(\mathcal{W}\).

**Definition 1** (2-Wasserstein distance).: _The 2-Wasserstein distance between two distributions \(\mathbb{P}_{1}\) and \(\mathbb{P}_{2}\) on \(\mathbb{R}^{d}\) with finite second moments is given by_

\[\operatorname{W}(\mathbb{P}_{1},\mathbb{P}_{2})=\left(\inf_{\pi\in\Pi( \mathbb{P}_{1},\mathbb{P}_{2})}\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\|\xi_{ 1}-\xi_{2}\|_{2}^{2}\,\pi(\mathrm{d}\xi_{1},\mathrm{d}\xi_{2})\right)^{\frac{1 }{2}},\]

_where \(\Pi(\mathbb{P}_{1},\mathbb{P}_{2})\) denotes the set of all couplings, that is, all joint distributions of the random variables \(\xi_{1}\) and \(\xi_{2}\) with marginal distributions \(\mathbb{P}_{1}\) and \(\mathbb{P}_{2}\), respectively._

Our model strictly generalizes the classic LQG setting,1 which can be recovered by choosing \(\rho_{x_{0}}=\rho_{w_{t}}=\rho_{v_{t}}=0\). The parameters \(\rho\) thus allow quantifying the uncertainty about the nominal model and building robustness to mis-specification. We emphasize that the Wasserstein ambiguity set \(\mathcal{W}\) contains many non-Gaussian distributions and it is not readily obvious that the worst-case distribution in (4) is in fact Gaussian. However, the set \(\mathcal{W}\) is also non-convex, as it contains only distributions under which the exogenous uncertainties are independent, which makes the distributionally robust LQG problem potentially difficult to solve.

Footnote 1: Our assumption that noise terms are zero-mean is consistent with the standard LQG model [8]. Requiring \(\mathbb{E}_{\varepsilon_{x_{0}}}[x_{0}]=0\) is assumed for clarity and without loss of generality.

## 3 Nash Equilibrium and Optimality of Linear Output Feedback Controllers

We henceforth view the distributionally robust LQG problem as a zero-sum game between the controller, who chooses causal control inputs, and nature, who chooses a distribution \(\mathbb{P}\in\mathcal{W}\). In this section we show that this game admits a Nash equilibrium, where nature's Nash strategy is a _Gaussian_ distribution \(\mathbb{P}^{\star}\in\mathcal{W}\) and the controller's Nash strategy is a _linear_ output feedback policy based on the Kalman filter evaluated under \(\mathbb{P}^{\star}\).

Purified Observations.Before outlining our proof strategy, we first simplify the problem formulation by re-parametrizing the control inputs in a more convenient form (following [5, 6, 27]). Note that the control inputs in the LQG formulation are subject to cyclic dependencies, as \(u_{t}\) depends on \(y_{t}\), while \(y_{t}\) depends on \(x_{t}\) through (2), and \(x_{t}\) depends again on \(u_{t}\) through (1), etc. Because these dependencies make the problem hard to analyze, it is preferable to instead consider the controls as functions of a new set of so-called _purified_ observations instead of the actual observations \(y_{t}\).

Specifically, we first introduce a fictitious _noise-free_ system

\[\hat{x}_{t+1}=A_{t}\hat{x}_{t}+B_{t}u_{t}\quad\forall t\in[T-1]\quad\text{and }\quad\hat{y}_{t}=C_{t}\hat{x}_{t}\quad\forall t\in[T-1]\]

with states \(\hat{x}_{t}\in\mathbb{R}^{n}\) and outputs \(\hat{y}_{t}\in\mathbb{R}^{p}\), which is initialized by \(\hat{x}_{0}=0\) and controlled by the _same_ inputs \(u_{t}\) as the original system (2). We then define the purified observation at time \(t\) as \(\eta_{t}=y_{t}-\hat{y}_{t}\) and we use \(\eta=(\eta_{0},\ldots,\eta_{T-1})\) to denote the trajectory of _all_ purified observations.

As the inputs \(u_{t}\) are causal, the controller can compute the fictitious state \(\hat{x}_{t}\) and output \(\hat{y}_{t}\) from the observations \(y_{0},\ldots,y_{t}\). Thus, \(\eta_{t}\) is representable as a function of \(y_{0},\ldots,y_{t}\). Conversely, one can show by induction that \(y_{t}\) can also be represented as a function of \(\eta_{0},\ldots,\eta_{t}\). Moreover, any measurable function of \(y_{0},\ldots,y_{t}\) can be expressed as a measurable function of \(\eta_{0},\ldots,\eta_{t}\) and vice-versa [27, Proposition II.1]. So if we define \(\,\mathcal{U}_{\eta}\) as the set of all control inputs \((u_{0},u_{1},\ldots,u_{T-1})\) so that \(u_{t}=\psi_{t}(\eta_{0},\ldots,\eta_{t})\) for some measurable function \(\psi_{t}:\mathbb{R}^{p(t+1)}\to\mathbb{R}^{m}\) for every \(t\in[T-1]\), the above reasoning implies that \(\mathcal{U}_{\eta}=\mathcal{U}_{y}\).

In view of this, we can rewrite the distributionally robust LQG problem equivalently as

\[p^{\star} =\left\{\begin{array}{ll}\min_{x,u,y}&\max_{p\in\mathcal{W}} \ \mathbb{E}_{\mathbb{P}}\left[u^{\top}Ru+x^{\top}Qx\right]\\ \text{s.t.}&u\in\mathcal{U}_{y},\ x=Hu+Gw,\ y=Cx+v\\ \end{array}\right.\] (5) \[=\left\{\begin{array}{ll}\min_{x,u}&\max_{p\in\mathcal{W}}\ \mathbb{E}_{ \mathbb{P}}\left[u^{\top}Ru+x^{\top}Qx\right]\\ \text{s.t.}&u\in\mathcal{U}_{\eta},\quad x=Hu+Gw,\end{array}\right.\]

where \(x=(x_{0},\ldots,x_{T})\), \(u=(u_{0},\ldots,u_{T-1})\), \(y=(y_{0},\ldots,y_{T-1})\), \(w=(x_{0},w_{0},\ldots,w_{T-1})\), \(v=(v_{0},\ldots,v_{T-1})\), \(\eta=(\eta_{0},\ldots,\eta_{T-1})\), and \(R\), \(Q\), \(H\), \(G\) and \(C\) are suitable block matrices(see Appendix SSB for their precise definitions). The latter reformulation involving the purified observations \(\eta\) is useful because these are _independent_ of the inputs. Indeed, by recursively combining the equations of the original and the noise-free systems, one can show that \(\eta=Dw+v\) for some block triangular matrix \(D\) (see Appendix SSB for its construction). This shows that the purified observations depend (linearly) on the exogenous uncertainties but _not_ on the control inputs. Hence, the cyclic dependencies complicating the original system are eliminated in (5).

Subsequently, we also study the dual of (5), defined as

\[d^{\star}=\left\{\begin{array}{rl}\max_{\mathbb{P}\in\mathcal{W}}&\min_{x,u }\ \mathbb{E}_{\mathbb{P}}\left[u^{\top}Ru+x^{\top}Qx\right]\\ \text{s.t.}&u\in\mathcal{U}_{\eta},\ \ x=Hu+Gw.\end{array}\right.\] (6)

The classic minimax inequality implies that \(p^{\star}\geq d^{\star}\). If we can prove that \(p^{\star}=d^{\star}\), that (5) has a solution \(u^{\star}\) and that (6) has a solution \(\mathbb{P}^{\star}\), then \((u^{\star},\mathbb{P}^{\star})\) must be a Nash equilibrium of the zero-sum game at hand [43, Theorem 2]. However, because \(\mathcal{U}_{\eta}\) is an infinite-dimensional function space and \(\mathcal{W}\) is an infinite-dimensional, non-convex set of non-parametric distributions, the existence of a Nash equilibrium (in pure strategies) is not at all evident. Instead, our proof strategy will rely on constructing an upper bound for \(p^{\star}\) and a lower bound for \(d^{\star}\), and showing that these match.

Upper Bound for \(p^{\star}\).We obtain an upper bound for \(p^{\star}\) by suitably _enlarging_ the ambiguity set \(\mathcal{W}\) and _restricting_ the controllers \(u_{t}\) to linear dependencies. We enlarge \(\mathcal{W}\) by ignoring all information about the distributions in \(\mathcal{W}\) except for their covariance matrices, and by replacing the Wasserstein distance with the Gelbrich distance. To that end, we first define the Gelbrich distance on the space of covariance matrices.

**Definition 2** (Gelbrich distance).: _The Gelbrich distance between the two covariance matrices \(\Sigma_{1},\Sigma_{2}\in\mathbb{S}_{+}^{d}\) is given by_

\[\mathds{G}(\Sigma_{1},\Sigma_{2})=\sqrt{\operatorname{Tr}\left(\Sigma_{1}+ \Sigma_{2}-2\left(\Sigma_{2}^{\frac{1}{2}}\Sigma_{1}\Sigma_{2}^{\frac{1}{2}} \right)^{\frac{1}{2}}\right)}.\]

We are interested in the Gelbrich distance because of its close connection to the 2-Wasserstein distance. Indeed, it is known that the 2-Wasserstein distance between two distributions with zero means is bounded below by the Gelbrich distance between the respective covariance matrices.

**Proposition 3.1** (Gelbrich bound [24, Theorem 2.1]).: _For any two distributions \(\mathbb{P}_{1}\) and \(\mathbb{P}_{2}\) on \(\mathbb{R}^{d}\) with zero means and covariance matrices \(\Sigma_{1},\Sigma_{2}\in\mathbb{S}_{+}^{d}\), respectively, we have \(\mathds{W}(\mathbb{P}_{1},\mathbb{P}_{2})\geq\mathds{G}(\Sigma_{1},\Sigma_{2})\)._

Recalling that \(\hat{X}_{0}\), \(\hat{W}_{t}\) and \(\hat{V}_{t}\) respectively denote the covariance matrices for \(x_{0},w_{t}\) and \(v_{t}\) under the nominal distribution \(\hat{\mathbb{P}}\), we can then define the following Gelbrich ambiguity set for the exogenous uncertainties:

\[\mathcal{G}=\mathcal{G}_{x_{0}}\otimes(\otimes_{t=0}^{T-1}\mathcal{G}_{w_{t}} )\otimes(\otimes_{t=0}^{T-1}\mathcal{G}_{v_{t}}),\]

where

\[\mathcal{G}_{x_{0}} =\{\mathbb{P}_{x_{0}}\in\mathcal{P}(\mathbb{R}^{n}):\mathbb{E}_{ \mathbb{P}_{x_{0}}}[x_{0}]=0,\ \mathbb{E}_{\mathbb{P}}[x_{0}x_{0}^{\top}]=X_{0},\ \mathds{G}(X_{0},\hat{X}_{0})\leq\rho_{x_{0}}\}\] \[\mathcal{G}_{w_{t}} =\{\mathbb{P}_{w_{t}}\in\mathcal{P}(\mathbb{R}^{n}):\mathbb{E}_{ \mathbb{P}_{w_{t}}}[w_{t}]=0,\ \mathbb{E}_{\mathbb{P}}[w_{t}w_{t}^{\top}]=W_{t},\ \mathds{G}(W_{t},\hat{W}_{t})\leq\rho_{w_{t}}\}\] \[\mathcal{G}_{v_{t}} =\{\mathbb{P}_{v_{t}}\in\mathcal{P}(\mathbb{R}^{m}):\mathbb{E}_{ \mathbb{P}_{v_{t}}}[v_{t}]=0,\ \mathbb{E}_{\mathbb{P}}[v_{t}v_{t}^{\top}]=V_{t},\ \mathds{G}(V_{t},\hat{V}_{t})\leq \rho_{v_{t}}\}.\]

By construction, the random vectors \(x_{0}\), \(\{w_{t}\}_{t=0}^{T-1}\) and \(\{v_{t}\}_{t=0}^{T-1}\) are thus mutually independent under any \(\mathbb{P}\in\mathcal{G}\). In addition and as a direct consequence of Proposition 3.1, \(\mathcal{G}\) constitutes an outer approximation for the Wasserstein ambiguity set \(\mathcal{W}\), as summarized in the next result.

**Corollary 1** (Gelbrich hull).: _We have \(\mathcal{W}\subseteq\mathcal{G}\)._

Because \(\mathcal{G}\) covers \(\mathcal{W}\), we henceforth refer to it as the _Gelbrich hull_ of the Wasserstein ambiguity set \(\mathcal{W}\). To finalize our construction of the upper bound on \(p^{\star}\), we focus on linear policies2 of the form \(u=q+U\eta=q+U(Dw+v)\), where \(q=(q_{0},\ldots,q_{T-1})\), and \(U\) is a block lower triangular matrix

\[U=\begin{bmatrix}U_{0,0}&&\\ U_{1,0}&U_{1,1}&&\\ \vdots&&\ddots&\\ U_{T-1,0}&\ldots&\ldots&U_{T-1,T-1}\end{bmatrix}.\] (7)

The block lower triangularity of \(U\) ensures that the corresponding controller is causal, which in turn ensures that \(u\in\mathcal{U}_{\eta}\). In the following, we denote by \(\mathcal{U}\) the set of all block lower triangular matrices of the form (7). An upper bound on problem (5) can now be obtained by _restricting_ the controller's feasible set to causal controllers that are _linear_ in the purified observations \(\eta\) and by _relaxing_ nature's feasible set to the Gelbrich hull \(\mathcal{G}\) of \(\mathcal{W}\). The resulting bounding problem is given by

\[\overline{p}^{\star}=\left\{\begin{array}{rl}\min\limits_{U,q,x,u}&\max \limits_{\mathbb{P}\in\mathcal{G}}\ \mathbb{E}_{\mathbb{P}}\left[u^{\top}Ru+x^{\top}Qx\right]\\ \text{s.t.}&U\in\mathcal{U},\ \ u=q+U(Dw+v),\ \ x=Hu+Gw.\end{array}\right.\] (8)

As we obtained (8) by restricting the feasible set of the outer minimization problem and relaxing the feasible set of the inner maximization problem in (5), it is clear that \(\overline{p}^{\star}\geq p^{\star}\). Recall also that problem (5) constitutes an infinite-dimensional zero-sum game, where the agents optimize over measurable policies and non-parametric distributions, respectively. In contrast, the next proposition shows that problem (8) is equivalent to a finite-dimensional zero-sum game.

**Proposition 3.2**.: _Problem (8) is equivalent to the optimization problem_

\[\overline{p}^{\star}=\left\{\begin{array}{rl}\min\limits_{ \begin{subarray}{c}Q\in\mathbb{R}^{T}\\ U\in\mathcal{U}\end{subarray}}\max\limits_{W\in\mathcal{G}_{W}\\ V\in\mathcal{G}_{V}\end{subarray}}\operatorname{Tr}\left(\left(D^{\top}U^{ \top}(R\!+\!H^{\top}QH)UD\!+2G^{\top}QHUD\!+\!G^{\top}QG\right)W\right)\\ +\operatorname{Tr}\left(\left(U^{\top}(R+H^{\top}QH)U\right)V\right)\!+\!q^{ \top}(R\!+\!H^{\top}QH)q,\end{array}\right.\] (9)

_where_

\[\mathcal{G}_{W} =\left\{W\in\mathbb{S}_{+}^{n(T+1)}:\begin{array}{rl}W=\operatorname {diag}(X_{0},W_{0},\ldots,W_{T-1}),\ X_{0}\in\mathbb{S}_{+}^{n},\ W_{t}\in \mathbb{S}_{+}^{n}\ \ \forall t\in[T-1]\\ \mathbb{G}(X_{0},\hat{X}_{0})^{2}\leq\rho_{x_{0}}^{2},\ \ \mathbb{G}(W_{t},\hat{W}_{t})^{2}\leq \rho_{x_{t}}^{2}\ \ \forall t\in[T-1]\end{array}\right\}\] \[\mathcal{G}_{V} =\left\{\begin{array}{rl}V\in\mathbb{S}_{+}^{pT}:&V= \operatorname{diag}(V_{0},\ldots,V_{T-1}),\ V_{t}\in\mathbb{S}_{+}^{p},\ \ \mathbb{G}(V_{t},\hat{V}_{t})^{2}\leq\rho_{v_{t}}^{2}\ \ \forall t\in[T-1]\end{array}\right\}.\]

We emphasize that Proposition 3.2 remains valid even if the nominal distribution \(\hat{\mathbb{P}}\) fails to be normal. Note also that, while nature's feasible set in (8) is non-convex due to the independence conditions, the sets \(\mathcal{G}_{W}\) and \(\mathcal{G}_{V}\) are convex and even semidefinite representable thanks to the properties of the squared Gelbrich distance.3 By dualizing the inner maximization problem, one can therefore reformulate the minimax problem (9) as a convex semidefinite program (SDP). Even though this SDP is computationally tractable in theory, it involves \(\mathcal{O}(T(mp+n^{2}+p^{2}))\) decision variables. For practically interesting problem dimensions, it thus quickly exceeds the capabilities of existing solvers.

Footnote 3: Note that the ambiguity sets \(\mathcal{G}_{W}\) and \(\mathcal{G}_{V}\) appearing in (9) involve the squared Gelbrich distance, \(\mathbb{G}(\Sigma_{1},\Sigma_{2})^{2}\). The reason is that \(\mathbb{G}(\Sigma_{1},\Sigma_{2})^{2}\) is known to be jointly convex in \(\Sigma_{1},\Sigma_{2}\) and semidefinite representable [38, Proposition 2.3], unlike the Gelbrich distance \(\mathbb{G}(\Sigma_{1},\Sigma_{2})\) itself, which is generally non-convex.

Lower Bound for \(d^{\star}\).To derive a tractable lower bound on \(d^{\star}\), we restrict nature's feasible set to the family \(\mathcal{W}_{\mathcal{N}}\) of all _normal_ distributions in the Wasserstein ambiguity set \(\mathcal{W}\). The resulting bounding problem is thus given by

\[\underline{d}^{\star}=\left\{\begin{array}{rl}\max\limits_{ \mathbb{P}\in\mathcal{W}_{\mathcal{N}}}&\min\limits_{x,u}&\mathbb{E}_{ \mathbb{P}}\left[u^{\top}Ru+x^{\top}Qx\right]\\ \text{s.t.}&u\in\mathcal{U}_{\eta},\ \ \ x=Hu+Gw.\end{array}\right.\] (10)

As we obtained (10) by restricting the feasible set of the outer maximization problem in (6), it is clear that \(\underline{d}^{\star}\leq d^{\star}\). Next, we show that (10) can be recast as a finite-dimensional zero-sum game. This result critically relies on the following known fact regarding the 2-Wasserstein distance between two _normal_ distributions, which coincides with the Gelbrich distance between their covariance matrices.

**Proposition 3.3** (Tightness for normal distributions [26, Proposition 7]).: _For any two normal distributions \(\mathbb{P}_{1}=\mathcal{N}(0,\Sigma_{1})\) and \(\mathbb{P}_{2}=\mathcal{N}(0,\Sigma_{2})\) with zero means we have \(\operatorname{W}(\mathbb{P}_{1},\mathbb{P}_{2})=\mathbb{G}(\Sigma_{1},\Sigma_{2})\)._

With this, we can provide a finite-dimensional reformulation, as summarized in the next result.

**Proposition 3.4**.: _Problem (10) is equivalent to the optimization problem_

\[\underline{d}^{\star}=\left\{\begin{array}{ll}\max\limits_{\begin{subarray}{c}W \in\mathcal{G}_{W}\\ V\in\mathcal{G}_{V}\end{subarray}}&\min\limits_{\begin{subarray}{c}q\in\mathbb{ R}^{p^{\star}}\\ U\in\mathcal{U}\end{subarray}}&\mathrm{Tr}\left(\big{(}D^{\top}U^{\top}(R+H^{ \top}QH)UD+2G^{\top}QHUD+G^{\top}QG\big{)}W\right)\\ \end{array}\right.\] (11)

_where \(\mathcal{G}_{W}\) and \(\mathcal{G}_{V}\) are defined exactly as in Proposition 3.2._

Proposition 3.4 relies on Proposition 3.3 and thus fails to hold unless \(\hat{\mathbb{P}}\) is normal. Also, one can again reformulate (11) as a tractable SDP by dualizing the inner minimization problem.

Conclusions.Propositions 3.2 and 3.4 reveal that problems (9) and (11) are dual to each other, that is, they can be transformed into one another by interchanging minimization and maximization. The following main theorem shows that strong duality holds irrespective of the problem data.

**Theorem 3.5** (Strong duality of (9) and (11)).: _We have \(\overline{p}^{\star}=\underline{d}^{\star}\)._

Theorem 3.5 follows immediately from Sion's classic minimax theorem [45], which applies because \(\mathcal{G}_{W}\) and \(\mathcal{G}_{V}\) are convex as well as compact thanks to [38, Lemma A.6].

By weak duality and the construction of the bounding problems (9) and (11), we trivially have \(\underline{d}^{\star}\leq d^{\star}\leq p^{\star}\leq\overline{p}^{\star}\). Theorem 3.5 reveals that all of these inequalities are in fact equalities, each of which gives rise to a non-trivial insight. The first key insight is that (5) and (6) are strong duals.

**Corollary 2** (Strong duality of (5) and (6)).: _We have \(p^{\star}=d^{\star}\)._

We stress that, unlike Theorem 3.5, Corollary 2 establishes strong duality between two _infinite-dimensional_ zero-sum games. The second key implication of Theorem 3.5 is that the distributionally robust LQG problem (5) is solved by a linear output-feedback controller.

**Corollary 3** (The controller's Nash strategy is linear in the observations).: _There exist \(U^{\star}\in\mathcal{U}\) and \(q^{\star}\in\mathbb{R}^{m}\) such that the distributionally robust LQG problem (5) is solved by \(u^{\star}=q^{\star}+U^{\star}y\)._

The identity \(p^{\star}=\overline{p}^{\star}\) readily implies that (5) is solved by a causal controller that is linear in the _purified_ observations. However, any causal controller that is linear in the purified observations \(\eta\) can be reformulated _exactly_ as a causal controller that is linear in the original observations \(y\) and vice-versa [6, Proposition 3]. Thus, Corollary 3 follows. The third key implication of Theorem 3.5 is that the _dual_ distributionly robust LQG problem is solved by a normal distribution.

**Corollary 4** (Nature's Nash strategy is a normal distribution).: _The dual distributionally robust LQG problem (6) is solved by a distribution \(\mathbb{P}^{\star}\in\mathcal{W}_{\mathcal{N}}\)._

Corollary 4 is a direct consequence of the identity \(\underline{d}^{\star}=d^{\star}\). Note that the optimal normal distribution \(\mathbb{P}^{\star}\) is uniquely determined by the covariance matrices \(W^{\star}\) and \(V^{\star}\) of the exogenous uncertain parameters, which can be computed by solving problem (11). That the worst-case distribution is actually Gaussian is not a-priori expected and is surprising given that the Wasserstein ball contains many non-Gaussian distributions.

## 4 Efficient Numerical Solution of Distributionally Robust LQG Problems

Having proven these structural results, we next turn attention to the problem of finding the optimal strategies. Our next result shows that, under a mild regularity condition, the optimal controller \(u^{\star}\) of the distributionally robust LQG problem (5) can be computed efficiently from \(\mathbb{P}^{\star}\).

**Proposition 4.1** (Optimality of Kalman filter-based feedback controllers).: _If \(\hat{V}_{t}\succ 0\) for all \(t\in[T-1]\), then problem (6) is solved by a Gaussian distribution \(\mathbb{P}^{\star}\) under which \(v_{t}\) has a covariance matrix \(V^{\star}_{t}\succ 0\) for every \(t\in[T-1]\), and (5) is solved by the optimal LQG controller corresponding to \(\mathbb{P}^{\star}\). Additionally, the optimal value of problem (9) and its strong dual (11) does not change if we restrict \(\mathcal{G}_{W}\) and \(\mathcal{G}_{V}\) to \(\mathcal{G}_{W}^{+}\) and \(\mathcal{G}_{V}^{+}\), respectively, where_

\[\mathcal{G}_{W}^{+} =\left\{W\in\mathcal{G}_{W}:X_{0}\succeq\lambda_{\min}(\hat{X}_ {0})I,\ W_{t}\succeq\lambda_{\min}(\hat{W}_{t})I\ \forall t\in[T-1]\right\},\] \[\mathcal{G}_{V}^{+} =\left\{V\in\mathcal{G}_{V}:V_{t}\succeq\lambda_{\min}(\hat{V}_ {t})I\ \forall t\in[T-1]\right\}.\]This implies that the optimal controller can be computed by solving a classic LQG problem corresponding to nature's optimal strategy \(\mathbb{P}^{\star}\), which can be done very efficiently through Kalman filtering and dynamic programming (see Appendix SSA for details). It thus suffices to design an efficient algorithm for computing \(\mathbb{P}^{\star}\), which is uniquely determined by the covariance matrices \((W^{\star},V^{\star})\) that solve problem (11). To this end, we first reformulate (11) as

\[\max_{W\in\mathcal{G}_{W}^{+},V\in\mathcal{G}_{V}^{+}}f(W,V),\] (12)

where we restrict \(\mathcal{G}_{W}\) and \(\mathcal{G}_{V}\) to \(\mathcal{G}_{W}^{+}\) and \(\mathcal{G}_{V}^{+}\), respectively, due to Proposition 4.1, and where \(f(W,V)\) denotes the optimal value function of the inner minimization problem in (11). As (11) is a reformulation of (10) and as the family of all causal purified output-feedback controllers matches the family of causal output-feedback controllers, \(f(W,V)\) can also be viewed as the optimal value of the classic LQG problem corresponding to the normal distribution \(\mathbb{P}\) determined by the covariance matrices \(W\) and \(V\). These insights lead to the following structural result.

**Proposition 4.2**.: \(f(W,V)\) _is concave and \(\beta\)-smooth in \((W,V)\in\mathcal{G}_{W}^{+}\times\mathcal{G}_{V}^{+}\) for some \(\beta>0\)._

By Proposition 4.2, it is possible to address problem (12) with a Frank-Wolfe algorithm [13, 18, 19, 20, 23, 35]. Each iteration of this algorithm solves a direction-finding subproblem, that is, a variant of problem (12) that maximizes the first-order Taylor expansion of \(f(W,V)\) around the current iterates.

\[\max_{L_{W}\in\mathcal{G}_{W}^{+},L_{V}\in\mathcal{G}_{V}^{+}}\langle\nabla_{W }f(W,V),L_{W}-W\rangle+\langle\nabla_{V}f(W,V),L_{V}-V\rangle\] (13)

The next iterates are then obtained by moving towards a maximizer \((L_{W}^{\star},L_{V}^{\star})\) of (13), i.e., we update

\[(W,V)\leftarrow(W,V)+\alpha\cdot(L_{W}^{\star}-W,L_{v}^{\star}-V),\]

where \(\alpha\) is an appropriate step size. The proposed Frank-Wolfe algorithm enjoys a very low per-iteration complexity because problem (13) is separable. To see this, we reformulate (13) as

\[\max_{L_{W},L_{V}} \langle\nabla_{X_{0}}f(W,V),L_{X_{0}}-X_{0}\rangle+\sum_{t=0}^{T- 1}\langle\nabla_{W_{t}}f(W,V),L_{W_{t}}-W_{t}\rangle+\langle\nabla_{V_{t}}f(W, V),L_{V_{t}}-V_{t}\rangle\] \[\mathrm{s.t.} \mathrm{G}(L_{X_{0}},\hat{X}_{0})^{2}\leq\rho_{x_{0}}^{2},\ \ \mathrm{G}(L_{W_{t}},\hat{W}_{t})^{2}\leq\rho_{w_{t}}^{2},\ \ \mathrm{G}(L_{V_{t}},\hat{V}_{t})^{2}\leq\rho_{v_{t}}^{2}\ \ \ \forall t\in[T-1]\] \[L_{X_{0}}\succeq\lambda_{\min}(\hat{X}_{0})I,\ \ \ \ \ L_{W_{t}}\succeq \lambda_{\min}(\hat{W}_{t})I,\ \ \ \ L_{V_{t}}\succeq\lambda_{\min}(\hat{V}_{t})I\ \ \forall t\in[T-1].\]

Hence, (13) decomposes into \(2T+1\) separate subproblems that can be solved in parallel. That is, for any matrix \(Z\in\{X_{0},W_{0},\ldots,W_{T-1},V_{0},\ldots,V_{T-1}\}\) we solve a separate subproblem of the form

\[\max_{L_{Z}\succeq\lambda_{\min}(\hat{Z})}\left\{\langle\nabla_{Z}f(W,V),L_{Z} -Z\rangle:\mathrm{G}(L_{Z},\hat{Z})^{2}\leq\rho_{z}^{2}\right\}.\] (14)

These subproblems can be reformulated as tractable SDPs and are thus amenable to efficient off-the-shelf solvers. By [38, Theorem 6.2], however, one can exploit the structure of the Gelbrich distance in order to reduce (14) to a univariate algebraic equation that can be solved to any desired accuracy \(\delta>0\) by a highly efficient bisection algorithm. We say that \(L_{Z}^{\delta}\) is a \(\delta\)-approximate solution of problem (14) for some \(\delta\in(0,1)\) if \(L_{Z}^{\delta}\) is feasible in (14) and if

\[\langle\nabla_{Z}f(W,V),L_{Z}^{\delta}-Z\rangle\geq\delta\langle\nabla_{Z}f(W, V),L_{Z}^{\star}-Z\rangle,\]

where \(L_{Z}^{\star}\) is an exact maximizer of (14). Note that, by the concavity of \(f(W,V)\), the inner product on the right-hand side is nonnegative and vanishes if and only if \(Z\) maximizes \(f(W,V)\) over the feasible set of (14). For further details we refer to Appendix SSE in the supplementary material.

**Remark 1** (Automatic differentiation).: _Recall that \(f(W,V)\) coincides with the optimal value of the LQG problem corresponding to the normal distribution \(\mathbb{P}\) determined by the covariance matrices \(W\) and \(V\). By using the underlying dynamic programming equations, \(f(W,V)\) can thus be expressed in closed form as a serial composition of \(\mathcal{O}(T)\) rational functions (see Appendix SSA for details). Hence, \(\nabla_{Z}f(W,V)\) can be calculated symbolically for any \(Z\in\{X_{0},W_{0},\ldots,W_{T-1},V_{0},\ldots,V_{T-1}\}\) by repeatedly applying the chain and product rules. However, the resulting formulas are lengthy and cumbersome. We thus compute the gradients numerically using backpropagation. The cost of evaluating \(\nabla_{Z}f(W,V)\) is then of the same order of magnitude as the cost of evaluating \(f(W,V)\)._

A detailed description of the proposed Frank-Wolfe method is given in Algorithm 1 below.

By [31, Theorem 1 and Lemma 7], which applies thanks to the structural properties of \(f(W,V)\) established in Proposition 4.2, Algorithm 1 attains a suboptimality gap of \(\epsilon\) within \(\mathcal{O}(1/\epsilon)\) iterations.

```
0: initial iterates \(W\), \(V\), nominal covariance matrices \(\hat{W}\), \(\hat{V}\), oracle precision \(\delta\in(0,1)\)
1: set initial iteration counter \(k=0\)
2:while stopping criterion is not met do
3:for\(Z\in\{X_{0},W_{0},\ldots,W_{T-1},V_{0},\ldots,V_{T-1}\}\)do in parallel
4: compute \(\nabla_{Z}f(W,V)\)
5: find a \(\delta\)-approximate solution \(L^{\delta}_{Z}\) of (14)
6:end
7:\(g\leftarrow\langle\nabla_{W}f(W,V),L^{\delta}_{W}-W\rangle+\langle\nabla_{V}f( W,V),L^{\delta}_{V}-V\rangle\)
8:\((W,V)\leftarrow(W,V)+2/(2+k)\cdot(L^{\delta}_{W}-W,L^{\delta}_{V}-V)\)
9:endwhile
10:Output: \(W\) and \(V\) ```

**Algorithm 1** Frank-Wolfe algorithm for solving (12)

## 5 Numerical Experiments

All experiments are run on an Intel i7-8700 CPU (3.2 GHz) machine with 16GB RAM. All linear SDP problems are modeled in Python 3.8.6 using CVXPY [1, 14] and solved with MOSEK [37]. The gradients of \(f(W,V)\) are computed via Pymanopt [48] with PyTorch's automated differentiation module [39, 40].

Consider a class of distributionally robust LQG problems with \(n=m=p=10\). We set \(A_{t}=0.1\times A\) to have ones on the main diagonal and the superdiagonal and zeroes everywhere else (\(A_{i,j}=1\) if \(i=j\) or \(i=j-1\) and \(A_{i,j}=0\) otherwise), and the other matrices to \(B_{t}=C_{t}=Q_{t}=R_{t}=I_{d}\). The Wasserstein radii are set to \(\rho_{x_{0}}=\rho_{w_{t}}=\rho_{v_{t}}=10^{-1}\). The nominal covariance matrices of the exogenous uncertainties are constructed randomly and with eigenvalues in the interval \([1,2]\) (so as to ensure they are positive definite). The code is publicly available in the Github repository https://github.com/RAO-EPFL/DR-Control1.

The optimal value of the distributionally robust LQG problem (5) can be computed by directly solving the SDP reformulation of (11) with MOSEK or by solving the nonlinear SDP (12) with our Frank-Wolfe method detailed in Algorithm 1. We next compare these two approaches in 10 independent simulation runs, where we set a stopping criterion corresponding to an optimality gap below \(10^{-3}\) and we run the Frank-Wolfe method with \(\delta=0.95\). Figure 0(a) illustrates the execution time for both approaches as a function of the planning horizon \(T\); runs where MOSEK exceeds \(100\)s are not reported. Figure 0(b) visualizes the empirical convergence behavior of the Frank-Wolfe algorithm. The results highlight that the Frank-Wolfe algorithm achieves running times that are uniformly lower than MOSEK across all problem horizons and is able to find highly accurate solutions already after a small number of iterations (50 iterations for problem instances of time horizon \(T=10\)).

Figure 1: (a) Execution time for MOSEK and Frank-Wolfe algorithm over 10 simulation runs as a function of the horizon \(T\) (solid lines show the mean and the shaded areas correspond to 1 standard deviation). (b) Convergence of optimality gap for Frank-Wolfe algorithm with horizon \(T=10\).

## 6 Concluding Remarks and Limitations

In view of the popularity of LQG models, the results in this work carry important theoretical and practical implications. Despite considering a generalization of the classic LQG setting where the noise affecting the system dynamics and the observations follows unknown (and potentially non-Gaussian) distributions, our findings suggest that certain classic structural results continue to hold and that highly efficient methods can be adapted to tackle this more realistic (and more challenging) problem. Specifically, that control policies depending linearly on observations continue to be optimal and that the worst-case distribution turns out to be Gaussian is surprising from a theoretical angle and also has direct practical implications, because it allows leveraging the highly efficient Kalman filter in conjunction with dynamic programming and a Frank-Wolfe method to design an efficient computational procedure for solving the problem.

The results also raise several important questions that warrant future exploration. First, it would be highly relevant to consider extensions where the system matrices are also affected by uncertainty, as this captures many applications of practical interest in, e.g., reinforcement learning or revenue management. Second, it would be worth exploring an infinite horizon setting or relaxing the assumption that the nominal distribution is Gaussian, as both assumptions may be limiting the practical appeal of the framework. Third, one could also attempt to prove structural optimality results or design novel algorithms for generating high-quality suboptimal solutions for the more general setting involving constraints on states and/or control inputs. Lastly, one could improve the present algorithmic proposal by exploiting topological properties of the objective so as to guarantee linear convergence rates in the Frank-Wolfe procedure.

Acknowledgements.This research was supported by the Swiss National Science Foundation under the NCCR Automation, grant agreement 51NF40_180545. Dan A. Iancu would like to acknowledge INSEAD for financial support during the duration of the project.

## References

* [1] Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for convex optimization problems. _Journal of Control and Decision_, 5(1):42-60, 2018.
* [2] Rohit Arora and Rui Gao. Data-driven multistage distributionally robust optimization with nested distance: Time consistency and tractable dynamic reformulations. _Available at Optimization Online_, 2022.
* [3] Francois Auger, Mickael Hilairet, Josep M. Guerrero, Eric Monmasson, Teresa Orlowska-Kowalska, and Seiichiro Katsura. Industrial applications of the Kalman filter: A review. _IEEE Transactions on Industrial Electronics_, 60(12):5458-5471, 2013.
* [4] Richard Bellman. Some inequalities for the square root of a positive definite matrix. _Linear Algebra and its Applications_, 1(3):321-324, 1968.
* [5] Aharon Ben-Tal, Stephen Boyd, and Arkadi Nemirovski. Control of uncertainty-affected discrete time linear systems via convex programming. _Available at Optimization Online_, 2005.
* [6] Aharon Ben-Tal, Stephen Boyd, and Arkadi Nemirovski. Extending scope of robust optimization: Comprehensive robust counterparts of uncertain problems. _Mathematical Programming_, 107(1):63-89, 2006.
* [7] Denis S. Bernstein and Wassim M. Haddad. LQG control with an \(\mathcal{H}_{\infty}\) performance bound: A Riccati equation approach. In _American Control Conference_, pages 796-802, 1988.
* [8] Dimitri Bertsekas. _Dynamic Programming and Optimal Control_, volume I. Athena Scientific, 2017.
* [9] Dimitris Bertsimas and Vineet Goyal. On the power and limitations of affine policies in two-stage adaptive optimization. _Mathematical Programming_, 134(2):491-531, 2012.
* [10] Dimitris Bertsimas, Dan A. Iancu, and Pablo A. Parrilo. Optimality of affine policies in multistage robust optimization. _Mathematics of Operations Research_, 35(2):363-394, 2010.

* [11] Dimitris Bertsimas, Dan A. Iancu, and Pablo A. Parrilo. A hierarchy of near-optimal policies for multistage adaptive optimization. _IEEE Transactions on Automatic Control_, 56(12):2809-2824, 2011.
* [12] Shen-Yong Chen. Kalman filter for robot vision: A survey. _IEEE Transactions on Industrial Electronics_, 59(11):4409-4420, 2012.
* [13] Vladimir F. Demyanov and Aleksandr M. Rubinov. _Approximate Methods in Optimization Problems_. Elsevier, 1970.
* [14] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex optimization. _Journal of Machine Learning Research_, 17(83):1-5, 2016.
* [15] John Doyle. Guaranteed margins for LQG regulators. _IEEE Transactions on Automatic Control_, 23(4):756-757, 1978.
* [16] John Doyle, Keith Glover, Pramod Khargonekar, and Bruce Francis. State-space solutions to standard \(\mathcal{H}_{2}\) and \(\mathcal{H}_{\infty}\) control problems. In _American Control Conference_, pages 1691-1696, 1988.
* [17] John Doyle, Kemin Zhou, and Bobby Bodenheimer. Optimal control with mixed \(\mathcal{H}_{2}\) and \(\mathcal{H}_{\infty}\) performance objectives. In _American Control Conference_, pages 2065-2070, 1989.
* [18] Joseph C. Dunn. Rates of convergence for conditional gradient algorithms near singular and nonsingular extremals. _SIAM Journal on Control and Optimization_, 17(2):187-211, 1979.
* [19] Joseph C. Dunn. Convergence rates for conditional gradient sequences generated by implicit step length rules. _SIAM Journal on Control and Optimization_, 18(5):473-487, 1980.
* [20] Joseph C. Dunn and S. Harshbarger. Conditional gradient algorithms with open loop step size rules. _Journal of Mathematical Analysis and Applications_, 62(2):432-444, 1978.
* [21] Omar El Housni and Vineet Goyal. On the optimality of affine policies for budgeted uncertainty sets. _Mathematics of Operations Research_, 46(2):674-711, 2021.
* [22] Hans Follmer and Alexander Schied. _Stochastic Finance: An Introduction in Discrete Time_. de Gruyter, 2011.
* [23] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. _Naval Research Logistics_, 3(1-2):95-110, 1956.
* [24] Matthias Gelbrich. On a formula for the \(L^{2}\) Wasserstein metric between measures on Euclidean and Hilbert spaces. _Mathematische Nachrichten_, 147(1):185-203, 1990.
* [25] Angelos Georghiou, Angelos Tsoukalas, and Wolfram Wiesemann. On the optimality of affine decision rules in robust and distributionally robust optimization. _Available at Optimization Online_, 2021.
* [26] Clark R. Givens and Rae M. Shortt. A class of Wasserstein metrics for probability distributions. _Michigan Mathematical Journal_, 31(2):231-240, 1984.
* [27] Michael J. Hadjiyiannis, Paul J. Goulart, and Daniel Kuhn. An efficient method to estimate the suboptimality of affine controllers. _IEEE Transactions on Automatic Control_, 56(12):2841-2853, 2011.
* [28] Bingyan Han. Distributionally robust Kalman filtering with volatility uncertainty. _arXiv preprint arXiv:2302.05993_, 2023.
* [29] Lars Peter Hansen and Thomas J. Sargent. Robust estimation and control under commitment. _Journal of Economic Theory_, 124(2):258-301, 2005.
* [30] Dan A. Iancu, Mayank Sharma, and Maxim Sviridenko. Supermodularity and affine policies in dynamic robust optimization. _Operations Research_, 61(4):941-956, 2013.

* [31] Martin Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In _International Conference on Machine Learning_, pages 427-435, 2013.
* [32] Aren Karapetyan, Andrea Iannelli, and John Lygeros. On the regret of \(\mathcal{H}_{\infty}\) control. In _IEEE Conference on Decision and Control_, pages 6181-6186, 2022.
* [33] Kihyun Kim and Insoon Yang. Distributional robustness in minimax linear quadratic control with Wasserstein distance. _SIAM Journal on Control and Optimization_, 61(2):458-483, 2023.
* [34] Georgios Kotsalis, Guanghui Lan, and Arkadi S. Nemirovski. Convex optimization for finite-horizon robust covariance control of linear stochastic systems. _SIAM Journal on Control and Optimization_, 59(1):296-319, 2021.
* [35] Evgenii S. Levitin and Boris T. Polyak. Constrained minimization methods. _USSR Computational Mathematics and Mathematical Physics_, 6(5):1-50, 1966.
* [36] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations. _Mathematical Programming_, 171(1-2):115-166, 2018.
* [37] MOSEK ApS. _The MOSEK Optimization Toolbox. Version 9.2._, 2019.
* [38] Viet Anh Nguyen, Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Bridging Bayesian and minimax mean square error estimation via Wasserstein distributionally robust optimization. _Mathematics of Operations Research_, 48(1):1-37, 2023.
* [39] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In _NIPS 2017 Autodiff Workshop_, 2017.
* [40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems_, pages 8026-8037, 2019.
* [41] Ian R. Petersen, Matthieu R. James, and Paul Dupuis. Minimax optimal control of stochastic uncertain systems with relative entropy constraints. _IEEE Transactions on Automatic Control_, 45(3):398-412, 2000.
* [42] Gabriel Peyre and Marco Cuturi. Computational optimal transport: With applications to data science. _Foundations and Trends in Machine Learning_, 11(5-6):355-607, 2019.
* [43] R. Tyrrell Rockafellar. _Conjugate Duality and Optimization_. SIAM, 1974.
* [44] Soroosh Shafieezadeh-Abadeh, Viet Anh Nguyen, Daniel Kuhn, and Peyman Mohajerin Esfahani. Wasserstein distributionally robust Kalman filtering. In _Advances in Neural Information Processing Systems_, pages 8483-8492, 2018.
* [45] Maurice Sion. On general minimax theorems. _Pacific Journal of Mathematics_, 8(1):171-176, 1958.
* [46] Joelle Skaf and Stephen P. Boyd. Design of affine controllers via convex optimization. _IEEE Transactions on Automatic Control_, 55(11):2476-2487, 2010.
* [47] Emanuel Todorov and Michael I. Jordan. Optimal feedback control as a theory of motor coordination. _Nature Neuroscience_, 5(11):1226-1235, 2002.
* [48] James Townsend, Niklas Koep, and Sebastian Weichwald. Pymanopt: A Python toolbox for optimization on manifolds using automatic differentiation. _Journal of Machine Learning Research_, 17(137):1-5, 2016.

* [49] Bart P. G. Van Parys, Paul J. Goulart, and Manfred Morari. Infinite horizon performance bounds for uncertain constrained systems. _IEEE Transactions on Automatic Control_, 58(11):2803-2817, 2013.
* [50] Bart P. G. Van Parys, Daniel Kuhn, Paul J. Goulart, and Manfred Morari. Distributionally robust control of constrained stochastic systems. _IEEE Transactions on Automatic Control_, 61(2):430-442, 2016.
* [51] Peter Whittle. Risk-sensitive linear/quadratic/Gaussian control. _Advances in Applied Probability_, 13(4):764-777, 1981.
* [52] Insoon Yang. Wasserstein distributionally robust stochastic control: A data-driven approach. _IEEE Transactions on Automatic Control_, 66(8):3863-3870, 2021.
* [53] Kemin Zhou and John C. Doyle. _Essentials of Robust Control_. Prentice Hall, 1998.

## Appendix

The supplementary material is structured as follows. Appendix SSA presents the well-known solution to the classic LQG problem using dynamic programming and Kalman Filter estimation. Appendix SSB provides the definitions of the stacked system matrices utilized in the compact formulation (5) of the distributionally robust LQG problem. Appendix SSC contains the proofs of the formal statements in the main text and provides additional technical results. Appendix SSD derives the SDP reformulation of the dual problem (11). Appendix SSB, finally, elaborates on the bisection algorithm used for solving the linearization oracle of the Frank-Wolfe algorithm.

## Appendix A Solution of the LQG Problem

The classic LQG problem can be solved efficiently via dynamic programming; see, e.g., [8]. That is, the unique optimal control inputs satisfy \(u_{t}^{\star}=K_{t}\hat{x}_{t}\) for every \(t\in[T-1]\), where \(K_{t}\in\mathbb{R}^{n\times n}\) is the optimal feedback gain matrix, and \(\hat{x}_{t}=\mathbb{E}_{\mathbb{P}}[x_{t}]y_{0},\ldots,y_{t}]\) is the minimum mean-squared-error estimator of \(x_{t}\) given the observation history up to time \(t\). Thanks to the celebrated separation principle, \(K_{t}\) can be computed by pretending that the system is deterministic and allows for perfect state observations, and \(\hat{x}_{t}\) can be computed while ignoring the control problem.

To compute \(K_{t}\), one first solves the deterministic LQR problem corresponding to the LQG problem at hand. Its value function \(x_{t}^{\top}P_{t}x_{t}\) at time \(t\) is quadratic in \(x_{t}\), and \(P_{t}\) obeys the backward recursion

\[P_{t}=A_{t}^{\top}P_{t+1}A_{t}+Q_{t}-A_{t}^{\top}P_{t+1}B_{t}(R_{t}+B_{t}^{ \top}P_{t+1}B_{t})^{-1}B_{t}^{\top}P_{t+1}A_{t}\quad\forall t\in[T-1]\] (A.15a) initialized by \[P_{T}=Q_{T}\]. The optimal feedback gain matrix \[K_{t}\] can then be computed from \[P_{t+1}\] as \[K_{t}=-(R_{t}+B_{t}^{\top}P_{t+1}B_{t})^{-1}B_{t}^{\top}P_{t+1}A_{t}\quad \forall t\in[T-1].\] (A.15b)

Since \(x_{t}\) and \((y_{0},\ldots,y_{t})\) are jointly normally distributed, the minimum mean-squared-error estimator \(\hat{x}_{t}\) can be calculated directly using the formula for the mean of a conditional normal distribution. Alternatively, however, one can use the Kalman filter to compute \(\hat{x}_{t}\) recursively, which is significantly more insightful and efficient. The Kalman filter also recursively computes the covariance matrix \(\Sigma_{t}\) of \(x_{t}\) conditional on \(y_{0},\ldots,y_{t}\) and the covariance matrix \(\Sigma_{t+1|t}\) of \(x_{t+1}\) conditional on \(y_{0},\ldots,y_{t}\) evaluated under \(\mathbb{P}\). Specifically, these covariance matrices obey the forward recursion

\[\Sigma_{t}=\Sigma_{t|t-1}-\Sigma_{t|t-1}C_{t}^{\top}(C_{t}\Sigma_ {t|t-1}C_{t}^{\top}+V_{t})^{-1}C_{t}\Sigma_{t|t-1}\] (A.16) \[\Sigma_{t+1|t}=A_{t}\Sigma_{t}A_{t}^{\top}+W_{t}\]

initialized by \(\Sigma_{0|-1}=X_{0}\). Using \(\Sigma_{t|t-1}\), we then define the Kalman filter gain as

\[L_{t}=\Sigma_{t}C_{t}^{\top}V_{t}^{-1}\quad\forall t\in[T-1]\]

which allows us to compute the minimum mean-squared-error estimator via the forward recursion

\[\hat{x}_{t+1}=A_{t}\hat{x}_{t}+B_{t}u_{t}+L_{t+1}\left(y_{t+1}-C_{t+1}(A_{t} \hat{x}_{t}+B_{t}u_{t})\right)\quad\forall t\in[T-1]\]

initialized by \(\hat{x}_{0}=L_{0}y_{0}\). One can also show that the optimal value of the LQG problem amounts to

\[\sum_{t=0}^{T-1}\operatorname{Tr}((Q_{t}-P_{t})\Sigma_{t})+\sum_{t=1}^{T} \operatorname{Tr}(P_{t}(A_{t-1}\Sigma_{t-1}A_{t-1}^{\top}+W_{t-1}))+ \operatorname{Tr}(P_{0}X_{0}).\] (A.17)

## Appendix B Definitions of Stacked System Matrices

The stacked system matrices appearing in the distributionally robust LQG problem (5) are defined as follows. First, the stacked state and input cost matrices \(Q\in\mathbb{S}^{n(T+1)}\) and \(R\in\mathbb{S}^{mT}\) are set to

\[Q=\begin{bmatrix}Q_{0}&&&&\\ &Q_{1}&&\\ &&\ddots&\\ &&&Q_{T}\end{bmatrix}\quad\text{and}\quad R=\begin{bmatrix}R_{0}&&&&\\ &&R_{1}&&\\ &&\ddots&\\ &&&R_{T-1}\end{bmatrix},\]respectively. Similarly, the stacked matrices appearing in the linear dynamics and the measurement equations \(C\in\mathbb{R}^{pT\times n(T+1)}\), \(G\in\mathbb{R}^{n(T+1)\times n(T+1)}\) and \(H\in\mathbb{R}^{n(T+1)\times mT}\) are defined as

\[C=\begin{bmatrix}C_{0}&0&&\\ &C_{1}&0&&\\ &&\ddots&\ddots&\\ &&&C_{T-1}&0\end{bmatrix},\quad G=\begin{bmatrix}A_{0}^{0}&&\\ A_{0}^{1}&A_{1}^{1}&&\\ \vdots&&\ddots&\\ A_{0}^{T}&A_{1}^{T}&\ldots&A_{T}^{T}\end{bmatrix}\]

and

\[H=\begin{bmatrix}0&&&&\\ A_{1}^{1}B_{0}&0&&\\ A_{1}^{2}B_{0}&A_{2}^{2}B_{1}&0&&\\ \vdots&&&\ddots&\\ \vdots&&&0\\ A_{1}^{T}B_{0}&A_{2}^{T}B_{1}&\ldots&\ldots&A_{T}^{T}B_{T-1}\end{bmatrix},\]

respectively, where \(A_{s}^{t}=\prod_{k=s}^{t-1}A_{k}\) for every \(s<t\) and \(A_{s}^{t}=I_{n}\) for \(s=t\).

Using the stacked system matrices, we can now express the purified observation process \(\eta\) as a linear function of the exogenous uncertainties \(w\) and \(v\) that is _not_ impacted by \(u\); see also [5, 46]

**Lemma B.1**.: _We have \(\eta=Dw+v\), where \(D=CG\)._

Proof of Lemma b.1.: The purified observation process is defined as \(\eta=y-\hat{y}\). Recall now that the observations of the original system satisfy \(y=Cx+v\). Similarly, one readily verifies that the observations of the fictitious noise-free system satisfy \(\hat{y}=C\hat{x}\). Thus, we have \(\eta=C(x-\hat{x})+v\). Next, recall that the state of the original system satisfies \(x=Hu+Gw\), and note that the state of the fictitious noise-free system satisfies \(\hat{x}=Hu\). Combining all of these linear equations finally shows that \(u\) cancels out and that \(\eta=CGw+v=Dw+v\). 

## Appendix C Proofs

### Additional Technical Results

It is well known that every causal controller that is linear in the original observations \(y\) can be reformulated as a causal controller that is linear in the purified observations \(\eta\) and vice versa [5, 46]. Perhaps surprisingly, however, the one-to-one transformation between the respective coefficients of \(y\) and \(\eta\) is _not_ linear. To keep this paper self-contained, we review these insights in the next lemma.

**Lemma C.1**.: _If \(u=U\eta+q\) for some \(U\in\mathcal{U}\) and \(q\in\mathbb{R}^{pT}\), then \(u=U^{\prime}y+q^{\prime}\) for \(U^{\prime}=(I+UCH)^{-1}U\) and \(q^{\prime}=(I+UCH)^{-1}q\). Conversely, if \(u=U^{\prime}y+q^{\prime}\) for some \(U^{\prime}\in\mathcal{U}\) and \(q^{\prime}\in\mathbb{R}^{pT}\), then \(u=U\eta+q\) for \(U=(I-U^{\prime}CH)^{-1}U^{\prime}\) and \(q=(I-U^{\prime}CH)^{-1}q^{\prime}\)._

Proof of Lemma c.1.: If \(u=U\eta+q\) for some \(U\in\mathcal{U}\) and \(q\in\mathbb{R}^{pT}\), then we have

\[u=U\eta+q=U(y-\hat{y})+q=Uy-UC\hat{x}+q=Uy-UCHu+q,\]

where the second equality follows from the definition of \(\eta\), the third equality holds because \(y=Cx+v\), and the last equality exploits our earlier insight that \(\hat{y}=C\hat{x}\). The last expression depends only on \(y\) and \(u\). Solving for \(u\) yields \(u=U^{\prime}y+q^{\prime}\), where \(U^{\prime}=(I+UCH)^{-1}U\) and \(q^{\prime}=(I+UCH)^{-1}q\). Note that \((I+UCH)\) is indeed invertible because \(I+UCH\) is a lower triangular matrix with all diagonal entries equal to one, ensuring a determinant of one.

Similarly, if \(u=U^{\prime}y+q^{\prime}\) for some \(U^{\prime}\in\mathcal{U}\) and \(q^{\prime}\in\mathbb{R}^{pT}\), then we have

\[u=U^{\prime}y+q^{\prime}=U^{\prime}(\eta+\hat{y})+q^{\prime}=U^{\prime}\eta+U^ {\prime}C\hat{x}+q^{\prime}=U^{\prime}\eta+U^{\prime}CHu+q^{\prime}.\]

Solving for \(u\) yields \(u=U\eta+q\), where \(U=(I-U^{\prime}CH)^{-1}U^{\prime}\) and \(q=(I-U^{\prime}CH)^{-1}q^{\prime}\). Note again that \((I-U^{\prime}CH)\) is indeed invertible because \((I-U^{\prime}CH)\) is a lower triangular matrix with all diagonal entries equal to one.

### Proofs of Section 3

Proof of Proposition 3.2.: In problem (8), both \(u\) and \(x\) are linear in \(w\) and \(v\), i.e., \(u=q+UDw+Uv\) and \(x=Hu+Gw=Hq+HUDw+HUv+Gw\). By substituting the linear representations of \(u\) and \(x\) into the objective function of problem (8), we obtain the following equivalent reformulation.

\[\begin{array}{ll}\min\limits_{\begin{subarray}{c}g\in\mathbb{R}^{p^{T}}\\ U\in\mathcal{U}\end{subarray}}&\max\limits_{\mathbb{P}\in\mathcal{G}}&\mathbb{ E}_{\mathbb{P}}\left[w^{\top}\left(D^{\top}U^{\top}(R+H^{\top}QH)UD+2D^{\top}U^{\top}H^{ \top}QG+G^{\top}QG\right)w\right]\\ &+\mathbb{E}_{\mathbb{P}}\left[v^{\top}\left(U^{\top}(R+H^{\top}QH)U\right)v \right]+q^{\top}(R+H^{\top}QH)q\end{array}\]

For any fixed \(\mathbb{P}\in\mathcal{G}\), we can express the expectation in the objective function of the above problem in terms of the covariance matrices \(W=\mathbb{E}_{\mathbb{P}}[ww^{\top}]\) and \(V=\mathbb{E}_{\mathbb{P}}[vv^{\top}]\). Thus, the problem becomes

\[\begin{array}{ll}\min\limits_{\begin{subarray}{c}g\in\mathbb{R}^{p^{T}}\\ U\in\mathcal{U}\end{subarray}}&\max\limits_{W,V,\mathbb{P}}&\text{Tr}\left( \left(D^{\top}U^{\top}(R+H^{\top}QH)UD+2G^{\top}QHUD+G^{\top}QG\right)W\right) \\ &+\text{Tr}\left(\left(U^{\top}(R+H^{\top}QH)U\right)V\right)\!+\!q^{\top}(R+H^ {\top}QH)q\\ &\text{s.t.}&\mathbb{P}\in\mathcal{G},\ \ W=\mathbb{E}_{\mathbb{P}}[ww^{\top}],\ \ V= \mathbb{E}_{\mathbb{P}}[vv^{\top}].\end{array}\] (A.18)

Recall now the definition of \(\mathcal{G}\), and note that the requirements \(\mathds{G}(X_{0},\hat{X}_{0})\leq\rho_{x_{0}}\), \(\mathds{G}(W_{t},\hat{W}_{t})\leq\rho_{w_{t}}\) and \(\mathds{G}(V_{t},\hat{V}_{t})\leq\rho_{v_{t}}\) are equivalent to the convex constraints \(\mathds{G}(X_{0},\hat{X}_{0})^{2}\leq\rho_{x_{0}}^{2},\mathds{G}(W_{t},\hat{W} _{t})^{2}\leq\rho_{w_{t}}^{2}\) and \(\mathds{G}(V_{t},\hat{V}_{t})^{2}\leq\rho_{v_{t}}^{2}\), respectively, for all \(t\in[T-1]\). The definition of \(\mathcal{G}\) also implies that

\[W=\mathbb{E}_{\mathbb{P}}[ww^{\top}]=\operatorname{diag}(X_{0},W_{0},\ldots,W _{T-1})\quad\text{and}\quad V=\mathbb{E}_{\mathbb{P}}[vv^{\top}]=\operatorname {diag}(V_{0},\ldots,V_{T-1}).\]

Problem (A.18) thus constitutes a relaxation of problem (9). Indeed, the feasible set of the inner maximization problem in (A.18) is a subset of the feasible set of the inner maximization problem in (9). Moreover, for any \(W\) and \(V\) feasible in the inner maximization problem in (9), the distribution \(\mathbb{P}=\mathbb{P}_{x_{0}}\otimes(\otimes_{t=0}^{T-10}\mathbb{P}_{w_{t}}) \otimes(\otimes_{t=0}^{T}\mathbb{P}_{v_{t}})\) defined through \(\mathbb{P}_{x_{0}}=\mathcal{N}(0,X_{0})\), \(\mathbb{P}_{w_{t}}=\mathcal{N}(0,W_{t})\) and \(\mathbb{P}_{v_{t}}=\mathcal{N}(0,V_{t})\), \(t\in[T-1]\), is feasible in the inner maximization problem in (A.18) with the same objective value. The relaxation is thus exact, and the optimal values of (8), (9) and (A.18) coincide. 

Proof of Proposition 3.4.: Recall that the space \(\mathcal{U}_{y}\) of all causal output-feedback controllers coincides with the space \(\mathcal{U}_{\eta}\) of all causal _purified_ output-feedback controllers. We can thus replace the feasible set \(\mathcal{U}_{\eta}\) of the inner minimization problem in (10) with \(\mathcal{U}_{y}\). Hence, for any fixed \(\mathbb{P}\in\mathcal{W}_{\mathcal{N}}\), the inner minimization problem in (10) constitutes a classic LQG problem. By standard LQG theory [8], it is solved by a _linear_ output-feedback controller of the form \(u=U^{\prime}y+q^{\prime}\) for some \(U^{\prime}\in\mathcal{U}\) and \(q^{\prime}\in\mathbb{R}^{pT}\); see also Appendix SSA. Lemma C.1 shows, however, that any linear output-feedback controller can be equivalently expressed as a linear _purified_-output feedback controller of the form \(u=U\eta+q\) for some \(U\in\mathcal{U}\) and \(q\in\mathbb{R}^{pT}\). In summary, the above reasoning shows that the feasible set of the inner minimization problem in (10) can be reduced to the family of all linear purified-output feedback controllers without sacrificing optimality. Thus, problem (10) is equivalent to

\[\begin{array}{ll}\max\limits_{\mathbb{P}\in\mathcal{W}_{\mathcal{N}}}&\min \limits_{\begin{subarray}{c}q,U,x,u\\ \text{s.t.}\end{subarray}}&\mathbb{E}_{\mathbb{P}}\left[u^{\top}Ru+x^{\top}Qx \right]\\ \text{s.t.}&U\in\mathcal{U},\ \ u=q+U\eta,\ \ x=Hu+Gw.\end{array}\]

Using a similar reasoning as in the proof of Proposition 3.2, we can now substitute the linear representations of \(u\) and \(x\) into the objective function and reformulate the above problem as

\[\begin{array}{ll}\max\limits_{W,V,\mathbb{P}}&\min\limits_{ \begin{subarray}{c}q\in\mathbb{R}^{p^{T}}\\ U\in\mathcal{U}\end{subarray}}&\text{Tr}\left(\left(D^{\top}U^{\top}(R+H^{ \top}QH)UD+2G^{\top}QHUD+G^{\top}QG\right)W\right)\\ &+\text{Tr}\left(\left(U^{\top}(R+H^{\top}QH)U\right)V\right)\!+\!q^{\top}(R+H^ {\top}QH)q\\ &\text{s.t.}&\mathbb{P}\in\mathcal{W}_{\mathcal{N}},\ \ W=\mathbb{E}_{ \mathbb{P}}[ww^{\top}],\ \ V=\mathbb{E}_{\mathbb{P}}[vv^{\top}].\end{array}\]

As \(\mathcal{W}_{\mathcal{N}}\) contains only _normal_ distributions, Proposition 3.3 implies that \(\mathds{W}(\mathbb{P}_{x_{0}},\hat{\mathbb{P}}_{x_{0}})=\mathds{G}(X_{0},\hat{X} _{0})\), \(\mathds{W}(\mathbb{P}_{w_{t}},\hat{\mathbb{P}}_{w_{t}})=\mathds{G}(W_{t},\hat{W} _{t})\) and \(\mathds{W}(\mathbb{P}_{v_{t}},\hat{\mathbb{P}}_{v_{t}})=\mathds{G}(V_{t},\hat{V} _{t})\) for all \(t\in[T-1]\). We may thus replace the requirement \(\mathds{W}(\mathbb{P}_{x_{0}},\hat{\mathbb{P}}_{x_{0}})\leq\rho_{x_{0}}\) in the definition of \(\mathcal{W}_{\mathcal{N}}\) by \(\mathds{G}(X_{0},\hat{X}_{0})\leq\rho_{x_{0}}\), which is equivalent to the convex constraint \(\mathds{G}(X_{0},\hat{X}_{0})^{2}\leq\rho_{x_{0}}^{2}\). The conditions on the marginal distributions of \(w_{t}\) and \(v_{t}\), \(t\in[T-1]\), admit similar reformulations. The definition of \(\mathcal{W}_{\mathcal{N}}\) also implies that

\[W=\mathbb{E}_{\mathbb{P}}[ww^{\top}]=\operatorname{diag}(X_{0},W_{0},\ldots,W_{ T-1})\quad\text{and}\quad V=\mathbb{E}_{\mathbb{P}}[vv^{\top}]= \operatorname{diag}(V_{0},\ldots,V_{T-1}).\]Thus, the feasible set of the outer maximization problem in (11) constitutes a relaxation of that in (10). One readily verifies that the relaxation is exact by using similar arguments as in the proof of Proposition 3.2. Thus, the claim follows. 

Proof of Theorem 3.5.: By Proposition 3.2, \(\bar{p}^{\star}\) coincides with the minimum of (9). Similarly, by Proposition 3.4\(\underline{d}^{\star}\) coincides with the maximum of (11). Note that problems (9) and (11) only differ by the order of minimization and maximization. Note also that \(\mathcal{U}\) is convex and closed, \(\mathcal{G}_{W}\) and \(\mathcal{G}_{V}\) are convex and compact by virtue of [38, Lemma A.6], and the (identical) trace terms in (9) and (11) are bilinear in \((W,V)\) and \((U,q)\). The claim thus follows from Sion's minimax theorem [45]. 

### Proofs of Section 4

Note that Proposition 4.1 is consistent with Corollary 3 because the optimal LQG controller corresponding to \(\mathbb{P}^{\star}\) is linear in the past observations.

Proof of Proposition 4.1.: By [38, Lemma A.3], the inner problem in (9) admits a maximizer \((W^{\star},V^{\star})\) with \(X_{0}^{\star}\succeq\lambda_{\min}(\hat{X}_{0})\) as well as \(W_{t}^{\star}\succeq\lambda_{\min}(\hat{W}_{t})\) and \(V_{t}^{\star}\succeq\lambda_{\min}(\hat{V}_{t})\) for all \(t\in[T-1]\). Thus, the optimal value of problem (9) and its strong dual (11) does not change if we restrict \(\mathcal{G}_{W}\) and \(\mathcal{G}_{V}\) to \(\mathcal{G}_{W}^{+}\) and \(\mathcal{G}_{V}^{+}\), respectively. We may thus conclude that problem (11) has a maximizer \((W^{\star},V^{\star})\) with \(V_{t}^{\star}\succeq\lambda_{\min}(\hat{V}_{t})\succ 0\) for all \(t\in[T-1]\). This in turn implies that problem (6) is solved by a normal distribution \(\mathbb{P}^{\star}\) under which the covariance matrix of the observation noise \(v_{t}\) satisfies \(V_{t}^{\star}\succ 0\) for every \(t\in[T-1]\). As (5) and (6) are strong duals, the optimal solution \(u^{\star}\) of problem (5) forms a Nash equilibrium with \(\mathbb{P}^{\star}\), i.e., \(u^{\star}\) is a best response to \(\mathbb{P}^{\star}\) and thus solves the _classic_ LQG problem corresponding to \(\mathbb{P}^{\star}\). As \(R_{t}\succ 0\) for every \(t\in[T-1]\), this best response \(u^{\star}\) is unique, and as \(V_{T}^{\star}\succ 0\) for every \(t\in[T-1]\), \(u^{\star}\) is in fact the Kalman filter-based optimal output-feedback strategy corresponding to \(\mathbb{P}^{\star}\) (which can be obtained using the techniques highlighted in Appendix SSA). 

Before proving Proposition 4.2, recall that \(f(W,V)\) is called \(\beta\)-smooth for some \(\beta>0\) if

\[|\nabla f(W,V)-\nabla f(W^{\prime},V^{\prime})|\leq\beta\left(\|W-W^{\prime} \|_{F}^{2}+\|V-V^{\prime}\|_{F}^{2}\right)^{\frac{1}{2}}\quad\forall\,W,W^{ \prime}\in\mathcal{G}_{W}^{+},\;V,V^{\prime}\in\mathcal{G}_{V}^{+},\]

where \(\|\cdot\|_{F}\) denotes the Frobenius norm.

Proof of Proposition 4.2.: The function \(f(W,V)\) is concave because the objective function of the inner minimization problem in (11) is linear (and hence concave) in \(W\) and \(V\) and because concavity is preserved under minimization. To prove that \(f(W,V)\) is \(\beta\)-smooth, we first recall from Proposition 3.3 that it coincides with the optimal value of the inner minimization problem in (10). As \(\mathcal{U}_{\eta}=\mathcal{U}_{y}\), \(f(W,V)\) can thus be viewed as the optimal value of the classic LQG problem corresponding to the normal distribution \(\mathbb{P}\) determined by the covariance matrices \(W\) and \(V\). Hence, \(f(W,V)\) coincides with (A.17), where \(\Sigma_{t}\), for \(t\in[T-1]\), is a function of \((W,V)\) defined recursively through the Kalman filter equations (A.16). Note that all inverse matrices in (A.16) are well-defined because any \(V\in\mathcal{G}_{V}^{+}\) is strictly positive definite. Therefore, \(\Sigma_{t}\) constitutes a proper rational function (that is, a ratio of two polynomials with the polynomial in the denominator being strictly positive) for every \(t\in[T-1]\). Thus, \(f(W,V)\) is infinitely often continuously differentiable on a neighborhood of \(\mathcal{G}_{W}^{+}\times\mathcal{G}_{V}^{+}\).

As \(f(W,V)\) is concave and (at least) twice continuously differentiable, it is \(\beta\)-smooth on \(\mathcal{G}_{W}^{+}\times\mathcal{G}_{V}^{+}\) if and only if the largest eigenvalue of the Hessian matrix of \(-f(W,V)\) is bounded above by \(\beta\) throughout \(\mathcal{G}_{W}^{+}\times\mathcal{G}_{V}^{+}\). Also, the largest eigenvalue of the positive semidefinite Hessian matrix \(\nabla^{2}(-f(W,V))\) coincides with the spectral norm of \(\nabla^{2}f(W,V)\). We may thus set

\[\beta=\sup_{W\in\mathcal{G}_{W}^{+},V\in\mathcal{G}_{V}^{+}}\|\nabla^{2}f(W,V)\| _{2},\] (A.19)

where \(\|\cdot\|_{2}\) denotes the spectral norm. The supremum in the above maximization problem is finite and attained thanks to Weierstrass' theorem, which applies because \(f(W,V)\) is twice continuously differentiable and the spectral norm is continuous, while the sets \(\mathcal{G}_{W}^{+}\) and \(\mathcal{G}_{V}^{+}\) are compact by virtue of [38, Lemma A.6]. This observation completes the proof.

## Appendix D SDP Reformulation of the Lower Problem (11)

Instead of solving the dual problem (11) with the customized Frank-Wolfe algorithm of Section 4, it can be reformulated as an SDP amenable to off-the-shelf solvers. This reformulation is obtained by dualizing the inner minimization problem and by exploiting the following preliminary lemma.

**Lemma D.1**.: _For any \(\hat{Z}\in\mathbb{S}_{+}^{d}\) and \(\rho_{z}\geq 0\), the set \(\mathcal{G}_{Z}=\{Z\in\mathbb{S}_{+}^{d}:\mathrm{G}(Z,\hat{Z})\leq\rho_{z}\}\) coincides with_

\[\left\{Z\in\mathbb{S}_{+}^{d}:\exists E_{z}\in\mathbb{S}_{+}^{d}\ \mathrm{with}\ \mathrm{Tr}(Z+\hat{Z}-2E_{z})\leq\rho_{z}^{2},\ \begin{bmatrix}\hat{Z}^{\frac{1}{2}}Z\hat{Z}^{\frac{1}{2}}&E_{z}\\ E_{z}&I\end{bmatrix}\succeq 0\right\}.\]

Proof of Lemma D.1.: By Definition 2, we have

\[\mathcal{G}_{Z}=\{Z\in\mathbb{S}_{+}^{d}:\mathrm{Tr}(Z+\hat{Z}-2(\hat{Z}^{ \frac{1}{2}}Z\hat{Z}^{\frac{1}{2}})^{\frac{1}{2}})\leq\rho_{z}^{2}\}.\]

Next, introduce an auxiliary variable \(E_{z}\in\mathbb{S}_{+}^{d}\) subject to the matrix inequality \(E_{z}^{2}\preceq(\hat{Z}^{\frac{1}{2}}Z\hat{Z}^{\frac{1}{2}})\). By [4, Theorem 1], this inequality can be recast as \(E_{z}\preceq(\hat{Z}^{\frac{1}{2}}Z\hat{Z}^{\frac{1}{2}})^{\frac{1}{2}}\). Hence, we can reformulate the nonlinear matrix inequality in the above representation of \(\mathcal{G}_{Z}\) as \(\mathrm{Tr}(Z+\hat{Z}-2E_{z})\leq\rho_{z}^{2}\). A standard Schur complement argument reveals that the inequality \(E_{z}^{2}\preceq(\hat{Z}^{\frac{1}{2}}Z\hat{Z}^{\frac{1}{2}})\) is also equivalent to

\[\begin{bmatrix}\hat{Z}^{\frac{1}{2}}Z\hat{Z}^{\frac{1}{2}}&E_{z}\\ E_{z}&I\end{bmatrix}\succeq 0.\]

The claim then follows by combining all of these insights. 

We are now ready to derive the desired SDP reformulation of problem (11).

**Proposition D.2**.: _If \(\hat{V}\succ 0\), then problem (11) is equivalent to the SDP_

\[\begin{split}\max&\mathrm{Tr}(G^{\top}QGW)-\mathrm{Tr}(F(R+H^{ \top}QH)^{-1})\\ \mathrm{s.t.}& W\in\mathbb{S}_{+}^{n(T+1)},\ V\in\mathbb{S}_{+}^{pT},\ M \in\mathcal{M},\ F\in\mathbb{S}_{+}^{Tm}\\ & E_{x_{0}}\in\mathbb{S}_{+}^{n},\ E_{w_{t}}\in\mathbb{S}_{+}^{n},\ E_{w_{t}}\in \mathbb{S}_{+}^{p}\ \ \forall t\in[T-1]\\ &\mathrm{Tr}(W_{0}+\hat{X}_{0}-2E_{x_{0}})\leq\rho_{x_{0}}^{2},\\ &\mathrm{Tr}(W_{t+1}+\hat{W}_{t}-2E_{w_{t}})\leq\rho_{w_{t}}^{2},\ \mathrm{Tr}(V_{t}+\hat{V}_{t}-2E_{v_{t}})\leq\rho_{v_{t}}^{2}\ \ \forall t\in[T-1]\\ &\begin{bmatrix}\hat{X}_{0}^{\frac{1}{2}}X_{0}\hat{X}_{0}^{\frac{1}{2}}&E_ {x_{0}}\\ E_{x_{0}}&I_{n}\end{bmatrix}\succeq 0,\\ &\begin{bmatrix}\hat{W}_{t}^{\frac{1}{2}}W_{t+1}\hat{W}_{t}^{\frac{1}{2}}&E_ {w_{t}}\\ E_{w_{t}}&I_{n}\end{bmatrix}\succeq 0,\ \ \ \left[\begin{matrix}\hat{V}_{t}^{\frac{1}{2}}V_{t}\hat{V}_{t}^{\frac{1}{2}}&E_ {v_{t}}\\ E_{v_{t}}&I_{p}\end{matrix}\right]\succeq 0\quad\forall t\in[T\!-\!1]\\ &\begin{bmatrix}F&H^{\top}QGWD^{\top}+M/2\end{bmatrix}\succeq 0\\ &W_{0}\succeq\lambda_{\min}(\hat{X}_{0})I,\quad W_{t+1}\succeq\lambda_{\min}( \hat{W}_{t})I,\quad V_{t}\succeq\lambda_{\min}(\hat{V}_{t})I\quad\forall t\in[ T-1].\end{split}\] (A.20)

_Here, \(\mathcal{M}\) denotes the set of all strictly upper block triangular matrices of the form_

\[\begin{bmatrix}0&M_{1,2}&M_{1,3}&\dots&M_{1,T}\\ &0&M_{2,3}&&M_{2,T}\\ &&\ddots&&\vdots\\ &&&0&M_{T-1,T}\\ &&&0\end{bmatrix}\in\mathbb{R}^{Tm\times Tp},\]

_where \(M_{t,s}\in\mathbb{R}^{m\times p}\) for every \(t,s\in\mathbb{Z}\) with \(1\leq t<s\leq T\)._

Proof of Proposition D.2.: The proof relies on dualizing the inner minimization problem in (11). Note that strong duality holds because the primal problem is trivially feasible and involves only equality constraints, which implies that any feasible point is in fact a Slater point. In the following we use \(M\in\mathcal{M}\) to denote the Lagrange multiplier of the constraint \(U\in\mathcal{U}\), which requires all blocks ofthe matrix \(U\) above the main diagonal to vanish. The Lagrangian function of the inner minimization problem in (11) can therefore be represented as

\[\begin{split}\mathcal{L}(q,U,M)&=\operatorname{Tr} \left(\left(D^{\top}U^{\top}(R+H^{\top}QH)UD+G^{\top}QG\right)W\right)+2 \operatorname{Tr}(G^{\top}QHUDW)\\ &\quad+\operatorname{Tr}\left(\left(U^{\top}(R+H^{\top}QH)U \right)V\right)+q^{\top}(R+H^{\top}QH)q+\operatorname{Tr}(UM^{\top}).\end{split}\]

Recall now that \(R\succ 0\) and \(Q\succeq 0\), and thus \(R+H^{\top}QH\succ 0\). Consequently, \(\mathcal{L}\) is minimized by \(q^{\star}=0\) for any fixed \(U\) and \(M\). In addition, the partial gradient of \(\mathcal{L}\) with respect \(U\) is given by

\[\frac{\partial\mathcal{L}}{\partial U}=2(R+H^{\top}QH)UDWD^{\top}+2(R+H^{\top} QH)UV+2H^{\top}QGWD^{\top}+M.\]

Recall also that \(V\in\mathcal{G}^{+}_{V}\) is strictly positive, which implies that \(DWD^{\top}+V\succ 0\) is invertible. As we already know that \(R+H^{\top}QH\succ 0\) is invertible, as well, \(\mathcal{L}\) is minimized by

\[U^{\star}=-(R+H^{\top}QH)^{-1}\left(H^{\top}QGWD^{\top}+M/2\right)(DWD^{\top} +V)^{-1}\]

for any fixed \(M\). Substituting both \(q^{\star}\) and \(U^{\star}\) into \(\mathcal{L}\) yields the dual objective function

\[\begin{split}& g(M)=\mathcal{L}(q^{\star},U^{\star},M)= \operatorname{Tr}(G^{\top}QGW)\\ &-\operatorname{Tr}\left((R+H^{\top}QH)^{-1}(H^{\top}QGWD^{\top }+M/2)(DWD^{\top}+V)^{-1}(H^{\top}QGWD^{\top}+M/2)^{\top}\right).\end{split}\]

The dual of the inner minimization problem in (11) is thus given by \(\max_{M\in\mathcal{M}}g(M)\). To linearize the dual objective function, we next introduce an auxiliary variable \(F\in\mathbb{S}_{+}^{mT}\) subject to the matrix inequality \(F\succeq(H^{\top}QGWD^{\top}+M/2)(DWD^{\top}+V)^{-1}(H^{\top}QGWD^{\top}+M/2 )^{\top}\). By using a standard Schur complement reformulation, we can then rewrite the dual problem as

\[\begin{split}\max&\operatorname{Tr}(G^{\top}QGW)- \operatorname{Tr}((R+H^{\top}QH)^{-1}F)\\ &\operatorname{s.t.}&\quad M\in\mathcal{M},\;F\in \mathbb{S}_{+}^{mT}\\ &\quad\begin{bmatrix}F&H^{\top}QGWD^{\top}+M/2\\ (H^{\top}QGWD^{\top}+M/2)^{\top}&DWD^{\top}+V\end{bmatrix}\succeq 0.\end{split}\] (A.21)

Next, by replacing the inner problem in (11) with its strong dual (A.21), we can reformulate (11) as

\[\begin{split}\max&\operatorname{Tr}(G^{\top}QGW)- \operatorname{Tr}((R+H^{\top}QH)^{-1}F)\\ \operatorname{s.t.}&\quad M\in\mathcal{M},\;F\in \mathbb{S}_{+}^{mT},\;W\in\mathbb{S}_{+}^{m(T+1)},\;V\in\mathbb{S}_{+}^{pT}\\ &\quad\begin{bmatrix}F&H^{\top}QGWD^{\top}+M/2\\ (H^{\top}QGWD^{\top}+M/2)^{\top}&DWD^{\top}+V\end{bmatrix}\succeq 0\\ &\quad\operatorname{G}(X_{0},\hat{X}_{0})^{2}\leq\rho_{x_{0}}^{2},\; \operatorname{G}(W_{t},\hat{W}_{t})\leq\rho_{u_{t}}^{2},\;\operatorname{G}(V _{t},\hat{V}_{t})\leq\rho_{v_{t}}^{2}\quad\forall t\in[T-1].\end{split}\] (A.22)

By Proposition 4.1, the inclusion of the constraints \(X_{0}\succeq\lambda_{\min}(\hat{X}_{0})I\), \(W_{t}\succeq\lambda_{\min}(\hat{W}_{t})I\) and \(V_{t}\succeq\lambda_{\min}(\hat{V}_{t})I\) for all \(t\in[T-1]\) has no effect on the solution to problem (A.22). In addition, by Lemma D.1, each (non-linear) Gelbrich constraint in (A.22) can be reformulated as an equivalent (linear) SDP constraint. Thus, problem (A.22) reduces to (A.20), and the claim follows. 

## Appendix E Bisection Algorithm for the Linearization Oracle

We now show that the direction-finding subproblem (14) can be solved efficiently via bisection. To this end, we first establish that (14) can be reduced to the solution of a univariate algebraic equation.

**Proposition E.1** ([38, Proposition A.4 (iii)]).: _If \(\hat{Z}\in\mathbb{S}_{++}^{d}\), \(\Gamma_{Z}\in\mathbb{S}_{+}^{d}\), \(\Gamma_{Z}\neq 0\) and \(\rho_{z}\in\mathbb{R}_{++}\), then_

\[\begin{split}\max&\langle\Gamma_{Z},L-Z\rangle\\ \operatorname{s.t.}&\operatorname{G}(L,\hat{Z})\leq\rho_{z}\\ &\quad L\succeq\lambda_{\min}(\hat{Z})I\end{split}\] (A.23)

_is uniquely solved by \(L^{\star}=(\gamma^{\star})^{2}(\gamma^{\star}I-\Gamma_{Z})^{-1}\hat{Z}(\gamma^ {\star}I-\Gamma_{Z})^{-1}\), where \(\gamma^{\star}\) is the unique solution of_

\[\rho_{z}^{2}-\langle\hat{Z},(I-\gamma^{\star}(\gamma^{\star}I-\Gamma_{Z})^{-1} )^{2}\rangle=0\] (A.24)

_in the interval \((\lambda_{\max}(\Gamma_{Z}),\infty)\)._In practice, we need to solve the algebraic equation (A.24) numerically. The numerical error in approximating \(\gamma^{\star}\) should be contained to ensure that \(L^{\star}\) approximates the exact maximizer of problem (A.23). The next proposition shows that, for any tolerance \(\delta\in(0,1)\), a \(\delta\)-approximate solution of (A.23) can be computed with an efficient bisection algorithm.

**Proposition E.2** ([38, Theorem 6.4]).: _For any fixed \(\rho_{z}\in\mathbb{R}_{++},\hat{Z}\in\mathbb{S}_{++}^{d}\) and \(\Gamma_{Z}\in\mathbb{S}_{+}^{d},\Gamma_{Z}\neq 0\), define \(\mathcal{G}_{+}^{+}=\{Z\in\mathbb{S}_{+}^{d}:\mathrm{G}(Z,\hat{Z})\leq\rho_{z},Z\succeq\lambda_{\min}(\hat{Z})\}\) as the feasible set of problem (A.23), and let \(Z\in\mathcal{G}_{Z}^{+}\) be any reference covariance matrix. Additionally, let \(\delta\in(0,1)\) be the desired oracle precision, and define \(\varphi(\gamma)=\gamma(\rho^{2}+\langle\gamma(\gamma I-\Gamma_{Z})^{-1}-I,\hat {Z}\rangle)-\langle Z,\Gamma_{Z}\rangle\) for any \(\gamma>\lambda_{\max}(\Gamma_{Z})\). Then, Algorithm A.2 returns in finite time a matrix \(L_{Z}^{\delta}\in\mathbb{S}_{+}^{d}\) with the following properties. (i) Feasibility: \(L_{Z}^{\delta}\in\mathcal{G}_{Z}^{+}\) (ii) \(\delta\)-Suboptimality: \(\langle L_{Z}^{\delta}-Z,\Gamma_{Z}\rangle\geq\delta\max_{L\in\mathcal{G}_{Z}^ {+}}(\Gamma_{Z},L-Z)\)._

```
0: nominal covariance matrix \(\hat{Z}\in\mathbb{S}_{++}^{d}\), radius \(\rho\in\mathbb{R}_{++}\), reference covariance matrix \(Z\in\mathcal{G}_{Z}^{+}\), gradient matrix \(\Gamma_{Z}\in\mathbb{S}_{+}^{d}\), \(\Gamma_{Z}\neq 0\), precision \(\delta\in(0,1)\), dual objective function \(\phi(\gamma)\) defined in Proposition E.2
1: set \(\lambda_{1}\leftarrow\lambda_{\max}(\Gamma_{Z})\), and let \(p_{1}\) be an eigenvector for \(\lambda_{1}\)
2: set \(\gamma\leftarrow\lambda_{1}(1+(p_{1}^{\top}\hat{Z}p_{1})^{\frac{1}{2}}/\rho)\) and \(\overline{\gamma}\leftarrow\lambda_{1}(1+\mathrm{Tr}(\hat{Z})^{\frac{1}{2}}/\rho)\)
3:repeat
4: set \(\tilde{\gamma}\leftarrow(\overline{\gamma}+\underline{\gamma})/2\) and \(L\leftarrow(\tilde{\gamma})^{2}(\tilde{\gamma}I-\Gamma_{Z})^{-1}\hat{Z}( \tilde{\gamma}I-\Gamma_{Z})^{-1}\)
5:if\(\frac{\mathrm{d}\phi}{\mathrm{d}\gamma}(\tilde{\gamma})<0\)then set \(\underline{\gamma}\leftarrow\tilde{\gamma}\)else\(\overline{\gamma}\leftarrow\tilde{\gamma}\)endif
6:until\(\frac{\mathrm{d}\phi}{\mathrm{d}\gamma}(\tilde{\gamma})>0\) and \(\langle L-Z,\Gamma_{Z}\rangle\geq\delta\phi(\tilde{\gamma})\) Output: \(L\) ```

**Algorithm A.2** Bisection algorithm to compute \(L_{Z}^{\delta}\)

In summary, for any \(Z\in\{X_{0},W_{0},\ldots,W_{T-1},V_{0},\ldots,V_{T-1}\}\), Algorithm A.2 computes a \(\delta\)-approximate solutions to the direction-finding subproblem (14) with \(\Gamma_{Z}=\nabla_{Z}f(W,V)\).

## Appendix F Additional Information on Experiments

**Generation of Nominal Covariance Matrices.** The nominal covariance matrices of the exogenous uncertainties are constructed randomly using the following procedure. For each exogenous uncertainty \(z\in\{x_{0},w_{0},\ldots,w_{T-1},v_{0},\ldots,v_{T-1}\}\), we denote the dimension of \(z\) by \(d\) and sample a matrix \(M_{Z}\in\mathbb{R}^{d\times d}\) from the uniform distribution on the hypercube \([0,1]^{d\times d}\). Next, we define \(\Xi_{Z}\in\mathbb{R}^{d\times d}\) as the orthogonal matrix whose columns represent the orthonormal eigenvectors of the symmetric matrix \(M_{Z}+M_{Z}^{\top}\). Finally, we set \(\hat{Z}=\Xi_{Z}\Lambda_{Z}\Xi_{Z}^{\top}\), where \(\Lambda_{Z}\) is a diagonal matrix whose main diagonal is sampled uniformly from the interval \([1,2]^{d}\). The rationale for adopting this cumbersome procedure is to ensure that the covariance matrix \(\hat{Z}\) is positive definite.

**Optimality Gap.** The optimality gap of the Frank-Wolfe algorithm visualized in Figure 0(b) is calculated as the sum of the surrogate optimality gaps \(\langle L_{Z}^{\delta}-Z,\nabla_{Z}f(W,V)\rangle\) across all \(Z\in\{X_{0},W_{0}\ldots,W_{T-1},V_{0},\ldots,V_{T-1}\}\). For more information on the surrogate optimality gaps see [31].