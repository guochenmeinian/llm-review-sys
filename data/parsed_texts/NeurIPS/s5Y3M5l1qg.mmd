# Carefully Blending Adversarial Training and Purification Improves Adversarial Robustness

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

In this work, we propose a novel adversarial defence mechanism for image classification - Carso - blending the paradigms of _adversarial training_ and _adversarial purification_ in a synergistic robustness-enhancing way. The method builds upon an adversarially-trained classifier, and learns to map its _internal representation_ associated with a potentially perturbed input onto a distribution of tentative _clean_ reconstructions. Multiple samples from such distribution are classified by the same adversarially-trained model, and an aggregation of its outputs finally constitutes the _robust prediction_ of interest. Experimental evaluation by a well-established benchmark of strong adaptive attacks, across different image datasets, shows that Carso is able to defend itself against adaptive _end-to-end white-box_ attacks devised for stochastic defences. Paying a modest _clean_ accuracy toll, our method improves by a significant margin the _state-of-the-art_ for CIFAR-10, CIFAR-100, and TinyImageNet-200 \(\ell_{\infty}\) robust classification accuracy against AutoAttack.

## 1 Introduction

Vulnerability to adversarial attacks [8; 57] - _i.e._ the presence of inputs, usually crafted on purpose, capable of catastrophically altering the behaviour of high-dimensional models [9] - constitutes a major hurdle towards ensuring the compliance of deep learning systems with the behaviour expected by modellers and users, and their adoption in safety-critical scenarios or tightly-regulated environments. This is particularly true for adversarially-_perturbed_ inputs, where a norm-constrained perturbation - often hardly detectable by human inspection [48; 5] - is added to an otherwise legitimate input, with the intention of eliciting an anomalous response [34].

Given the widespread nature of the issue [30], and the serious concerns raised about the safety and reliability of data-learnt models in the lack of an appropriate mitigation [7], adversarial attacks have been extensively studied. Yet, obtaining generally robust machine learning (_ML_) systems remains a longstanding issue, and a major open challenge.

Research in the field has been driven by two opposing, yet complementary, efforts. On the one hand, the study of _failure modes_ in existing models and defences, with the goal of understanding their origin and developing stronger attacks with varying degrees of knowledge and control over the target system [57; 21; 44; 60]. On the other hand, the construction of increasingly capable defence mechanisms. Although alternatives have been explored [15; 59; 11; 68], most of the latter is based on adequately leveraging _adversarial training_[21; 42; 58; 49; 23; 31; 54; 62; 18; 47], _i.e._ training a _ML_ model on a dataset composed of (or enriched with) adversarially-perturbed inputs associated with their correct, _pre-perturbation_ labels. In fact, adversarial training has been the only technique capable of consistently providing an acceptable level of defence [24], while still incrementally improving up to the current _state-of-the-art_[18; 47].

Another defensive approach is that of _adversarial purification_[53; 66], where a generative model is used - similarly to denoising - to recover a perturbation-free version of the input before classification is performed. Nonetheless, such attempts have generally fallen short of expectations due to inherent limitations of the generative models used in early attempts [45], or due to decreases in robust accuracy1 when attacked _end-to-end_[25] - resulting in subpar robustness if the defensive structure is known to the adversary [60]. More recently, the rise of diffusion-based generative models [28] and their use for purification have enabled more successful results of this kind [45; 13] - although at the cost of much longer training and inference times, and a much brittle robustness evaluation [13; 38].

Footnote 1: The _test set accuracy_ of the frozen-weights trained classifier â€“ computed on a dataset entirely composed of adversarially-perturbed examples generated against that specific model.

In this work, we design a novel adversarial defence for supervised image classification, dubbed Carso (_i.e._, Counter-Adversarial Recall of Synthetic Observations). The approach relies on an adversarially-trained classifier (called hereinafter simply _the classifier_), endowed with a stochastic generative model (called hereinafter _the purifier_). Upon classification of a potentially-perturbed input, the latter learns to generate - from the tensor2 of (pre)activations registered at neuron level in the former - samples from a distribution of plausible, perturbation-free reconstructions. At inference time, some of these samples are classified by the very same _classifier_, and the original input is robustly labelled by aggregating its many outputs. This method - to the best of our knowledge the first attempt to organically merge the _adversarial training_ and _purification_ paradigms - avoids the vulnerability pitfalls typical of the mere stacking of a purifier and a classifier [25], while still being able to take advantage of independent incremental improvements to adversarial training or generative modelling.

Footnote 2: Which we call _internal representation_.

An empirical assessment3 of the defence in the _white-box_ setting is provided, using a _conditional_[56; 64]_variational autoencoder_[32; 50] as the purifier and existing _state-of-the-art_ adversarially pre-trained models as classifiers. Such choices are meant to give existing approaches - and the _adversary_ attacking our architecture _end-to-end_ as part of the assessment - the strongest advantage possible. Yet, in all scenarios considered, Carso improves significantly the robustness of the pre-trained classifier - even against attacks specifically devised to fool stochastic defences like ours. Remarkably, with a modest _clean_ accuracy toll, our method improves by a significant margin the current _state-of-the-art_ for Cifar-10 [33], Cifar-100 [33], and TinyImageNet-200 [14]\(\ell_{\infty}\) robust classification accuracy against AutoAttack[17].

Footnote 3: Implementation of the method and code for the experiments (based on _PyTorch_[46], AdverTorch[19], and ebtorch[4]) can be found in the _supplementary materials_ of the paper.

In summary, the paper makes the following contributions:

* The description of Carso, a novel adversarial defence method synergistically blending _adversarial training_ and _adversarial purification_;
* but applicable to more general scenarios as well;
* showing higher robust accuracy _w.r.t._ to existing _state-of-the-art_ adversarial training and purification approaches.

The rest of the manuscript is structured as follows. In section 2 we provide an overview of selected contributions in the fields of _adversarial training_ and _purification-based_ defences - with focus on image classification. In section 3, a deeper analysis is given of two integral parts of our experimental assessment: Pgd adversarial training and conditional variational autoencoders. Section 4 is devoted to the intuition behind Carso, its architectural description, and the relevant technical details that allow it to work. Section 5 contains details about the experimental setup, results, comments, and limitations. Section 6 concludes the paper and outlines directions of future development.

## 2 Related work

Adversarial training as a defenceThe idea of training a model on adversarially-generated examples as a way to make it more robust can be traced back to the very beginning of research in the area.

The seminal [57] proposes to perform training on a mixed collection of _clean_ and adversarial data, generated beforehand.

The introduction of Fgsm[21] enables the efficient generation of adversarial examples along the training, with a single normalised gradient step. Its iterative counterpart Pgd[42] - discussed in section 3 and Appendix A - significantly improves the effectiveness of adversarial examples produced, making it still the _de facto_ standard for the synthesis of adversarial training inputs [24]. Further incremental improvements have also been developed, some focused specifically on robustness assessment (_e.g._ adaptive-stepsize variants, as in [17]).

The most recent adversarial training protocols further rely on synthetic data to increase the numerosity of training datapoints [23, 49, 62, 18, 47], and adopt adjusted loss functions to balance robustness and accuracy [67] or generally foster the learning process [18]. The entire model architecture may also be tuned specifically for the sake of robustness enhancement [47]. At least some of such ingredients are often required to reach the current _state-of-the-art_ in robust accuracy via adversarial training.

Purification as a defenceAmongst the first attempts of _purification-based_ adversarial defence, [25] investigates the use of denoising autoencoders [61] to recover examples free from adversarial perturbations. Despite its effectiveness in the denoising task, the method may indeed _increase_ the vulnerability of the system when attacks are generated against it _end-to-end_. The contextually proposed improvement adds a smoothness penalty to the reconstruction loss, partially mitigating such downside [25]. Similar in spirit, [39] tackles the issue by computing the reconstruction loss between the last-layers representations of the frozen-weights attacked classifier, respectively receiving, as input, the _clean_ and the tentatively _denoised_ example.

In [52], _Generative Adversarial Networks_ (GANs) [22] learnt on _clean_ data are used at inference time to find a plausible synthetic example - close to the perturbed input - belonging to the unperturbed data manifold. Despite encouraging results, the delicate training process of GANs and the existence of known failure modes [70] limit the applicability of the method. More recently, a similar approach [27] employing _energy-based models_[37] suffered from poor sample quality [45].

Purification approaches based on (conditional) variational autoencoders include [29] and [53]. Very recently, a technique combining variational manifold learning with a test-time iterative purification procedure has also been proposed [65].

Finally, already-mentioned techniques relying on _score-_[66] and _diffusion-_ based [45, 13] models have also been developed, with generally favourable results - often balanced in practice by longer training and inference times, and a much more fragile robustness assessment [13, 38].

## 3 Preliminaries

Pgd adversarial trainingThe task of finding model parameters robust to adversarial perturbations is framed by [42] as a _min-max_ optimisation problem seeking to minimise _adversarial risk_. The inner optimisation (_i.e._, the generation of worst-case adversarial examples) is solved by an iterative algorithm - _Projected Gradient Descent_ - interleaving gradient ascent steps in input space with the eventual projection on the shell of an \(\epsilon\)-ball centred around an input datapoint, thus imposing a perturbation strength constraint.

In this manuscript, we will use the shorthand notation \(\epsilon_{p}\) to denote \(\ell_{p}\) norm-bound perturbations of maximum magnitude \(\epsilon\).

The formal details of such method are provided in Appendix A.

(Conditional) variational autoencodersVariational autoencoders (_VAEs_) [32, 50] allow the learning from data of approximate generative latent-variable models of the form \(p(\bm{x},\bm{z})=p(\bm{x}\,|\,\bm{z})p(\bm{z})\), whose likelihood and posterior are approximately parameterised by deep artificial neural networks (_ANN_). The problem is cast as the maximisation of a variational lower bound.

In practice, optimisation is performed iteratively - on a loss function given by the linear mixture of data-reconstruction loss and empirical _KL_ divergence _w.r.t._ a chosen prior, computed on mini-batches of data.

_Conditional_ Variational Autoencoders [56; 64] extend _VAE_s by attaching a _conditioning tensor_\(\bm{c}\) - expressing specific characteristics of each example - to both \(\bm{x}\) and \(\bm{z}\) during training. This allows the learning of a decoder model capable of conditional data generation.

Further details on the functioning of such models are given in Appendix B.

## 4 Structure of Carso

The core ideas informing the design of our method are driven more by _first principles_ rather than arising from specific contingent requirements. This section discusses such ideas, the architectural details of Carso, and a group of technical aspects fundamental to its training and inference processes.

### Architectural overview and principle of operation

From an architectural point of view, Carso is essentially composed of two _ANN_ models - a _classifier_ and a _purifier_ - operating in close synergy. The former is trained on a given classification task, whose inputs might be adversarially corrupted at inference time. The latter learns to generate samples from a distribution of potential input reconstructions, tentatively free from adversarial perturbations. Crucially, the _purifier_ has only access to the internal representation of the _classifier_ - and not even directly to the perturbed input - to perform its task.

During inference, for each input, the internal representation of the _classifier_ is used by the _purifier_ to synthesise a collection of tentatively unperturbed input reconstructions. Those are classified by the same _classifier_, and the resulting outputs are aggregated into a final _robust prediction_.

There are no specific requirements for the classifier, whose training is completely independent of the use of the model as part of Carso. However, training it adversarially improves significantly the _clean_ accuracy of the overall system, allowing it to benefit from established adversarial training techniques.

The purifier is also independent of specific architectural choices, provided it is capable of stochastic conditional data generation at inference time, with the internal representation of the classifier used as the conditioning set.

In the rest of the paper, we employ a _state-of-the-art_ adversarially pre-trained WideResNet model as the classifier, and a purpose-built _conditional variational autoencoder_ as the purifier, the latter operating decoder-only during inference. Such choice was driven by the deliberate intention to assess the adversarial robustness of our method in its worst-case scenario against a _white-box_ attacker, and with the least advantage compared to existing approaches based solely on adversarial training.

In fact, the decoder of a conditional VAE allows for exact algorithmic differentiability [6]_w.r.t._ its conditioning set, thus averting the need for backward-pass approximation [2] in generating _end-to-end_ adversarial attacks against the entire system, and preventing (un)intentional robustness by gradient obfuscation [2]. The same cannot be said [13] for more capable and modern purification models, such as those based _e.g._ on diffusive processes, whose robustness assessment is still in the process of being understood [38].

A downside of such choice is represented by the reduced effectiveness of the decoder in the synthesis of complex data, due to well-known model limitations. In fact, we experimentally observe a modest increase in reconstruction cost for non-perturbed inputs, which in turn may limit the _clean_ accuracy of the entire system. Nevertheless, we defend the need for a fair and transparent robustness evaluation, such as the one provided by the use of a VAE-based purifier, in the evaluation of any novel architecture-agnostic adversarial defence technique.

A diagram of the whole architecture is shown in Figure 1, and its detailed principles of operation are recapped below.

TrainingAt training time, adversarially-perturbed examples are generated against the _classifier_, and fed to it. The tensors containing the _classifier_ (pre)activations across the network are then extracted. Finally, the conditional _VAE_ serving as the _purifier_ is trained on perturbation-free input reconstruction, conditional on the corresponding previously extracted internal representations, and using pre-perturbation examples as targets.

Upon completion of the training process, the encoder network may be discarded as it will not be used for inference.

InferenceThe example requiring classification is fed to the _classifier_. Its corresponding internal representation is extracted and used to condition the generative process described by the decoder of the _VAE_. Stochastic latent variables are repeatedly sampled from the original priors, which are given by an _i.i.d._ multivariate Standard Normal distribution. Each element in the resulting set of reconstructed inputs is classified by the same _classifier_, and the individually predicted class logits are aggregated. The result of such aggregation constitutes the robust prediction of the input class.

Remarkably, the only link between the initial potentially-perturbed input and the resulting purified reconstructions (and thus the predicted class) is through the _internal representation_ of the classifier, which serves as a _featurisation_ of the original input. The whole process is exactly differentiable _end-to-end_, and the only potential hurdle to the generation of adversarial attacks against the entire system is the stochastic nature of the decoding - which is easily tackled by _Expectation over Transformation_[3].

### A first-principles justification

If we consider a trained _ANN_ classifier, subject to a successful adversarial attack by means of a slightly perturbed example, we observe that - both in terms of \(\ell_{p}\) magnitude and human perception - a small variation on the input side of the network is amplified to a significant amount on the output side, thanks to the layerwise processing by the model. Given the deterministic nature of such processing at inference time, we speculate that the _trace_ obtained by sequentially collecting the (pre)activation values within the network, along the forward pass, constitutes a richer characterisation of such an amplification process compared to the knowledge of the input alone. Indeed, as we do, it is possible to learn a direct mapping from such featurisation of the input, to a distribution of possible perturbation-free input reconstructions - taking advantage of such characterisation.

### Hierarchical input and internal representation encoding

Training a conditional VAE requires [56] that the conditioning set \(\bm{c}\) is concatenated to the input \(\bm{x}\) before encoding occurs, and to the sample of latent variables \(\bm{z}\) right before decoding. The same is

Figure 1: Schematic representation of the Carso architecture used in the experimental phase of this work. The subnetwork bordered by the red dashed line is used only during the training of the _purifier_. The subnetwork bordered by the blue dashed line is re-evaluated on different random samples \(\bm{z}_{i}\) and the resulting individual \(\hat{y}_{i}\) are aggregated into \(\hat{y}_{\text{rob}}\). The _classifier_\(f(\cdot;\bm{\theta})\) is always kept frozen; the remaining network is trained on \(\mathcal{L}_{\text{VAE}}(\bm{x},\hat{\bm{x}})\). More precise details on the functioning of the networks are provided in subsection 4.1.

also true, with the suitable adjustments, for any conditional generative approach where the target and the conditioning set must be processed jointly.

In order to ensure the usability and scalability of Carso across the widest range of input data and classifier models, we propose to perform such processing in a hierarchical and partially disjoint fashion between the input and the conditioning set. In principle, the encoding of \(\bm{x}\) and \(\bm{c}\) can be performed by two different and independent subnetworks, until some form of joint processing must occur. This allows to retain the overall architectural structure of the purifier, while having finer-grained control over the inductive biases [43] deemed the most suitable for the respective variables.

In the experimental phase of our work, we encode the two variables independently. The input is compressed by a multilayer convolutional neural network (CNN). The internal representation - which in our case is composed of differently sized multi-channel _images_ - is processed _layer by layer_ by independent multilayer CNNs (responsible for encoding local information), whose flattened outputs are finally concatenated and compressed by a fully-connected layer (modelling inter-layer correlations in the representation). The resulting compressed input and conditioning set are then further concatenated and jointly encoded by a fully-connected network (FCN).

In order to use the VAE decoder at inference time, the entire compression machinery for the conditioning set must be preserved after training, and used to encode the internal representations extracted. The equivalent input encoder may be discarded instead.

### Adversarially-balanced batches

Training the purifier in representation-conditional input reconstruction requires having access to adversarially-perturbed examples generated against the classifier, and to the corresponding clean data. Specifically, we use as input a mixture of _clean_ and adversarially _perturbed_ examples, and the clean input as the target.

Within each epoch, the _training set_ of interest is shuffled [51, 10], and only a fixed fraction of each resulting batch is adversarially perturbed. Calling \(\epsilon\) the maximum \(\ell_{p}\) perturbation norm bound for the threat model against which the _classifier_ was adversarially pre-trained, the portion of perturbed examples is generated by an even split of Fgsm\({}_{\nicefrac{{1}}{{2}}}\), Pgd\({}_{\nicefrac{{1}}{{2}}}\), Fgsm\({}_{\epsilon}\), and Pgd\({}_{\epsilon}\) attacks.

Any smaller subset of attack types and strengths, or a detailedly unbalanced batch composition, always experimentally results in a worse performing purification model. More details justifying such choice are provided in Appendix C.

### Robust aggregation strategy

At inference time, many different input reconstructions are classified by the _classifier_, and the respective outputs concur to the settlement of a _robust prediction_.

Calling \(l_{i}^{\alpha}\) the output logit associated with class \(i\in\{1,\dots,C\}\) in the prediction by the classifier on sample \(\alpha\in\{1,\dots,N\}\), we adopt the following aggregation strategy:

\[P_{i}:=\frac{1}{Z}\prod_{\alpha=1}^{N}e^{e^{l_{i}^{\alpha}}}\]

with \(P_{i}\) being the aggregated probability of membership in class \(i\), \(Z\) a normalisation constant such that \(\sum_{i=1}^{C}P_{i}=1\), and \(e\) Euler's number.

Such choice produces a _robust prediction_ much harder to take over in the event that an adversary selectively targets a specific input reconstruction. A heuristic justification for this property is given in Appendix D.

## 5 Experimental assessment

Experimental evaluation of our method is carried out in terms of _robust_ and _clean_ image classification accuracy within three different scenarios (\(a\), \(b\) and \(c\)), determined by the specific classification task.

The _white-box_ threat model with a fixed \(\ell_{\infty}\) norm bound is assumed throughout, as it generally constitutes the most demanding setup for adversarial defences.

### Setup

DataThe Cifar-10 [33] dataset is used in _scenario (a)_, the Cifar-100 [33] dataset is used in _scenario (b)_, whereas the TinyImageNet-200 [14] dataset is used in _scenario (c)_.

ArchitecturesA WideResNet-28-10 model is used as the _classifier_, adversarially pre-trained on the respective dataset - the only difference between scenarios being the number of output logits: \(10\) in _scenario (a)_, \(100\) in _scenario (b)_, and \(200\) in _scenario (c)_.

The purifier is composed of a conditional VAE, processing inputs and internal representations in a partially disjoint fashion, as explained in subsection 4.3. The input is compressed by a two-layer CNN; the internal representation is instead processed layerwise by independent CNNs (three-layered in _scenarios (a)_ and _(b)_, four-layered in _scenario (c)_) whose outputs are then concatenated and compressed by a fully-connected layer. A final two-layer FCN jointly encodes the compressed input and conditioning set, after the concatenation of the two. A six-layer deconvolutional network is used as the decoder.

More precise details on all architectures are given in Appendix E.

Outer minimisationIn _scenarios (a)_ and _(b)_, the _classifier_ is trained according to [18]; in _scenario (c)_, according to [62]. _Classifiers_ were always acquired as pre-trained models, using publicly available weights provided by the respective authors.

The _purifier_ is trained on the _VAE_ loss, using _summed pixel-wise channel-wise_ binary cross-entropy as the reconstruction cost. Optimisation is performed by Radam+Lookahead[41, 69] with a learning rate schedule that presents a linear warm-up, a plateau phase, and a linear annealing [55]. To promote the learning of meaningful reconstructions during the initial phases of training, the _KL divergence_ term in the VAE loss is suppressed for an initial number of epochs. Afterwards, it is linearly modulated up to its actual value, during a fixed number of epochs (\(\beta\)_increase_) [26]. The initial and final epochs of such modulation are reported in Table 14.

Additional scenario-specific details are provided in Appendix E.

Inner minimisation\(\epsilon_{\infty}=\nicefrac{{8}}{{255}}\) is set as the perturbation norm bound.

Adversarial examples against the _purifier_ are obtained, as explained in subsection 4.4, by Fgsm\({}_{\nicefrac{{c}}{{2}}}\), Pgd\({}_{\nicefrac{{c}}{{2}}}\), Fgsm\({}_{\epsilon}\), and Pgd\({}_{\epsilon}\), in a _class-untargeted_ fashion on the cross-entropy loss. In the case of Pgd, gradient ascent with a step size of \(\alpha=0.01\) is used.

The complete details and hyperparameters of the attacks are described in Appendix E.

EvaluationIn each scenario, we report the _clean_ and _robust_ test-set accuracy - the latter by means of AutoAttack[17] - of the _classifier_ and the corresponding Carso architecture.

For the _classifier_ alone, the _standard_ version of AutoAttack (_AA_) is used: _i.e._, the worst-case accuracy on a mixture of AutoPgd on the cross-entropy loss [17] with \(100\) steps, AutoPgd on the _difference of logits ratio_ loss [17] with \(100\) steps, Fab[16] with \(100\) steps, and the _black-box_ Square attack [1] with \(5000\) queries.

In the evaluation of the Carso architecture, the number of reconstructed samples per input is set to 8, the logits are aggregated as explained in subsection 4.5, and the output class is finally selected as the \(\arg\max\) of the aggregation. Due to the stochastic nature of the _purifier_, robust accuracy is assessed by a version of AutoAttack suitable for stochastic defences (_randAA_) - composed of AutoPgd on the cross-entropy and _difference of logits ratio_ losses, across \(20\)_Expectation over Transformation_ (EoT) [3] iterations with \(100\) gradient ascent steps each.

Computational infrastructureAll experiments were performed on an _NVIDIA DGX A100_ system. Training in _scenarios (a)_ and _(c)_ was run on 8 _NVIDIA A100_ GPUs with 40 GB of dedicated memory each; in _scenario (b)_ 4 of such devices were used. Elapsed real training time for the purifier in all scenarios is reported in Table 1.

### Results and discussion

An analysis of the experimental results is provided in the subsection that follows, whereas their systematic exposition is given in Table 2.

Scenario (a)Comparing the robust accuracy of the _classifier_ model used in _scenario (a)_[18] with that resulting from the inclusion of the same model in the Carso architecture, we observe a \(+8.4\%\) increase. This is counterbalanced by a \(-5.6\%\) clean accuracy toll. The same version of Carso further provides a \(+5.03\) robustness increase _w.r.t._ the current best AT-trained model [47] that employs a \(\sim 3\times\) larger RaWideResNet-70-16 model.

In addition, our method provides a remarkable \(+9.72\%\) increase in robust accuracy _w.r.t._ to the best adversarial purification approach [40], a diffusion-based purifier. However, the comparison is not as straightforward. In fact, the paper [40] reports a robust accuracy of \(78.12\%\) using AutoAttack on the gradients obtained via the adjoint method [45]. As noted in [38], such evaluation (which uses the version of AutoAttack that is unsuitable for stochastic defences) leads to a large overestimation of the robustness of diffusive purifiers. As suggested in [38], the authors of [40] re-evaluate the robust accuracy according to a more suitable pipeline (Pgd+EoT, whose hyperparameters are shown in Table 12), obtaining a much lower robust accuracy of \(66.41\%\). Consequently, we repeat the same evaluation for Carso and compare the worst-case robustness amongst the two. In line with typical AT methods, and unlike diffusive purification, the robustness of Carso assessed by means of _randAA_ is still lower _w.r.t._ than achieved by Pgd+EoT.

Scenario (b)Moving to _scenario (b)_, Carso achieves a robust accuracy increase of \(+27.47\%\)_w.r.t._ the _classifier_ alone [18], balanced by a \(5.79\%\) decrease in clean accuracy. Our approach also improves upon the robust accuracy of the best AT-trained model [62] (WideResNet-70-16) by \(23.98\%\). In the absence of a reliable robustness evaluation by means of Pgd+EoT for the best purification-based method [40], we still obtain a \(+20.25\%\) increase in robust accuracy upon its (largely overestimated) AA result.

Scenario (c)In _scenario (c)_, Carso improves upon the _classifier_ alone [62] (which is also the best AT-based approach for TinyImageNet-200) by \(+22.26\%\). A significant clean accuracy toll is

\begin{table}
\begin{tabular}{l c c c} \hline \hline _Scenario_ & _(a)_ & _(b)_ & _(c)_ \\ \hline _Etapsed real training time_ & \(159\min\) & \(138\min\) & \(213\min\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Elapsed real running time for training the _purifier_ in the different scenarios considered.

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline \multirow{2}{*}{Seen.} & \multirow{2}{*}{Dataset} & \multirow{2}{*}{AT/Cl} & \multirow{2}{*}{C/Cl} & \multirow{2}{*}{AT/AA} & \multicolumn{2}{c}{C/rand-AA} & \multirow{2}{*}{Best AT/AA} & \multirow{2}{*}{\begin{tabular}{c} Best P/AA \\ (Pgd+EoT) \\ \end{tabular} } \\ \hline (a) & & _Cifar-10_ & _0.9216_ & _0.8686_ & 0.6773 & \begin{tabular}{c} **0.7613** \\ (0.7689) \\ \end{tabular} & 0.7107 & 
\begin{tabular}{c} 0.7812 \\ (0.6641) \\ \end{tabular} \\ \hline (b) & & _Cifar-100_ & _0.7385_ & _0.6806_ & 0.3918 & **0.6665** & 0.4267 & 0.4609 \\ \hline (c) & & _TinyImageNet-200_ & _0.6519_ & _0.5632_ & 0.3130 & **0.5356** & 0.3130 & \\ \hline \hline \end{tabular}
\end{table}
Table 2: Clean (results in _italic_) and adversarial (results in upright) accuracy for the different models and datasets used in the respective scenarios. The following abbreviations are used: Seen: scenario considered; AT/Cl: clean accuracy for the adversarially-pretrained model used as the _classifier_, when considered alone; C/Cl: clean accuracy for the Carso architecture; AT/AA: robust accuracy (by the means of AutoAttack) for the adversarially-pretrained model used as the _classifier_, when considered alone; C/randAA: robust accuracy for the Carso architecture, when attacked _end-to-end_ by AutoAttack for randomised defences; Best AT/AA: best robust accuracy result for the respective dataset (by the means of AutoAttack), obtained by adversarial training alone (any model); Best P/AA: best robust accuracy result for the respective dataset (by the means of AutoAttack), obtained by adversarial purification (any model). Robust accuracies in round brackets are obtained using the Pgd+EoT [38] pipeline, developed for diffusion-based purifiers. The best clean and robust accuracies per dataset are shown in **bold**. The clean accuracies for the models referred to in the Best columns are shown in Table 15 (in Appendix F).

imposed by the relative complexity of the dataset, _i.e._\(-8.87\%\). In this setting, we lack any additional purification-based methods.

Assessing the impact of _gradient obfuscation_ Although the architecture of Carso is algorithmically differentiable _end-to-end_ - and the integrated diagnostics of the _randAA_ routines raised no warnings during the assessment - we additionally guard against the eventual gradient obfuscation [2] induced by our method by repeating the evaluation at \(\epsilon_{\infty}=0.95\), verifying that the resulting robust accuracy stays below random chance [12]. Results are shown in Table 3.

### Limitations and open problems

In line with recent research aiming at the development of robust defences against multiple perturbations [20; 35], our method determines a decrease in _clean_ accuracy _w.r.t._ the original model on which it is built upon - especially in _scenario (c)_ as the complexity of the dataset increases. This phenomenon is partly dependent on the choice of a VAE as the generative purification model, a requirement for the fairest evaluation possible in terms of robustness.

Yet, the issue remains open: is it possible to devise a Carso-like architecture capable of the same - if not better - robust behaviour, which is also competitively accurate on clean inputs? Potential avenues for future research may involve the development of Carso-like architectures in which representation-conditional data generation is obtained by means of diffusion or score-based models. Alternatively, incremental developments aimed at improving the cross-talk between the purifier and the final classifier may be pursued.

Lastly, the scalability of Carso could be strongly improved by determining whether the internal representation used in conditional data generation may be restricted to a smaller subset of layers, while still maintaining the general robustness of the method.

## 6 Conclusion

In this work, we presented a novel adversarial defence mechanism tightly integrating input purification, and classification by an adversarially-trained model - in the form of representation-conditional data purification. Our method is able to improve upon the current _state-of-the-art_ in Cifar-10, Cifar-100, and TinyImageNet\(\ell_{\infty}\) robust classification, _w.r.t._ both _adversarial training_ and _purification_ approaches alone.

Such results suggest a new synergistic strategy to achieve adversarial robustness in visual tasks and motivate future research on the application of the same design principles to different models and types of data.

\begin{table}
\begin{tabular}{l c c c} \hline \hline _Scenario_ & _(a)_ & _(b)_ & _(c)_ \\ \hline \(\epsilon_{\infty}=0.95\) _acc._ & \textless{}0.047 & \textless{}0.010 & \textless{}0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Robust classification accuracy against AutoAttack, for \(\epsilon_{\infty}=0.95\), as a way to assess the (lack of) impact of _gradient obfuscation_ on robust accuracy evaluation.

## References

* [1]M. Andriushchenko et al. 'Square Attack: a query-efficient black-box adversarial attack via random search'. In: 16th European Conference on Computer Vision. 2020.
* [2] Anish Athalye, N. Carlini and D. Wagner. 'Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples'. In: _Proceedings of the International Conference on Machine Learning_. 2018.
* [3] Anish Athalye et al. 'Synthesizing Robust Adversarial Examples'. In: _Proceedings of the International Conference on Machine Learning_. 2018.
* [4] Emanuele Ballarin. _ebtorch: Collection of PyTorch additions, extensions, utilities, uses and abuses_. 2024. url: https://github.com/emaballarin/ebtorch.
* [5] Vincent Ballet et al. 'Imperceptible Adversarial Attacks on Tabular Data'. In: _Thirty-third Conference on Neural Information Processing Systems, Workshop on Robust AI in Financial Services: Data, Fairness, Explainability, Trustworthiness, and Privacy (Robust AI in FS)_. 2019.
* [6] Atilim Gunes Baydin et al. 'Automatic differentiation in machine learning: a survey'. In: _The Journal of Machine Learning Research_ 18.153 (2018), pp. 1-43.
* [7] Battista Biggio and Fabio Roli. 'Wild patterns: Ten years after the rise of adversarial machine learning'. In: _Pattern Recognition_ 84 (2018), pp. 317-331.
* Volume Part III_. 2013.
* [9] Luca Bortolussi and Guido Sanguinetti. _Intrinsic Geometric Vulnerability of High-Dimensional Artificial Intelligence_. 2018. arXiv: 1811.03571.
* [10] Leon Bottou. 'On-Line Algorithms and Stochastic Approximations'. In: _On-Line Learning in Neural Networks_. Cambridge University Press, 1999. Chap. 2.
* [11] Ginevra Carbone et al. 'Robustness of Bayesian Neural Networks to Gradient-Based Attacks'. In: _Advances in Neural Information Processing Systems_. 2020.
* [12] Nicholas Carlini et al. _On Evaluating Adversarial Robustness_. 2019. arXiv: 1902.06705.
* [13] Huanran Chen et al. _Robust Classification via a Single Diffusion Model_. 2023. arXiv: 2305.15241.
* [14] Patryk Chrabaszcz, Ilya Loshchilov and Frank Hutter. _A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets_. 2017. arXiv: 1707.08819.
* [15] Moustapha Cisse et al. 'Parseval Networks: Improving Robustness to Adversarial Examples'. In: _Proceedings of the International Conference on Machine Learning_. 2017.
* [16] Francesco Croce and Matthias Hein. _Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack_. 2020. arXiv: 1907.02044.
* [17] Francesco Croce and Matthias Hein. 'Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks'. In: _Proceedings of the International Conference on Machine Learning_. 2020.
* [18] Jiequan Cui et al. _Decoupled Kullback-Leibler Divergence Loss_. 2023. arXiv: 2305.13948.
* [19] Gavin Weiguang Ding, Luyu Wang and Xiaomeng Jin. _AdverTorch v0.1: An Adversarial Robustness Toolbox based on PyTorch_. 2019. arXiv: 1902.07623.
* [20] Hadi M. Dolatabadi, Sarah Erfani and Christopher Leckie. '\(\ell_{\infty}\)-Robustness and Beyond: Unleashing Efficient Adversarial Training'. In: _18th European Conference on Computer Vision_. 2022.
* [21] Ian Goodfellow, Jonathon Shlens and Christian Szegedy. 'Explaining and Harnessing Adversarial Examples'. In: _International Conference on Learning Representations_. 2015.
* [22] Ian Goodfellow et al. 'Generative Adversarial Nets'. In: _Advances in Neural Information Processing Systems_. 2014.
* [23] Sven Gowal et al. 'Improving Robustness using Generated Data'. In: _Advances in Neural Information Processing Systems_. 2021.
* [24] Sven Gowal et al. _Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples_. 2020. arXiv: 2010.03593.
* [25] Shixiang Gu and Luca Rigazio. 'Towards Deep Neural Network Architectures Robust to Adversarial Examples'. In: _Workshop Track of the International Conference on Learning Representations_. 2015.
* [26] Irina Higgins et al. 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework'. In: _International Conference on Learning Representations_. 2017.
* [27] Mitch Hill, Jonathan Mitchell and Song-Chun Zhu. 'Stochastic Security: Adversarial Defense Using Long-Run Dynamics of Energy-Based Models'. In: _International Conference on Learning Representations_. 2021.
* [28] Chin-Wei Huang, Jae Hyun Lim and Aaron C Courville. 'A Variational Perspective on Diffusion-Based Generative Models and Score Matching'. In: _Advances in Neural Information Processing Systems_. 2021.

* [29] Uiwon Hwang et al. 'PuVAE: A Variational Autoencoder to Purify Adversarial Examples'. In: _IEEE Access_. 2019.
* [30] Andrew Ilyas et al. 'Adversarial Examples Are Not Bugs, They Are Features'. In: _Advances in Neural Information Processing Systems_. 2019.
* [31] Xiaojun Jia et al. 'LAS-AT: Adversarial Training With Learnable Attack Strategy'. In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2022.
* [32] Diederik P. Kingma and Max Welling. 'Auto-Encoding Variational Bayes'. In: _International Conference on Learning Representations_. 2014.
* [33] Alex Krizhevsky. 'Learning Multiple Layers of Features from Tiny Images'. In: 2009.
* [34] Alexey Kurakin, Ian J. Goodfellow and Samy Bengio. 'Adversarial Examples in the Physical World'. In: _Artificial Intelligence Safety and Security_ (2018).
* [35] Cassidy Laidlaw, Sahil Singla and Soheil Feizi. 'Perceptual Adversarial Robustness: Defense Against Unseen Threat Models'. In: _International Conference on Learning Representations_. 2021.
* [36] Yann LeCun and Corinna Cortes. _The MNIST handwritten digit database_. 2010.
* [37] Yann LeCun et al. 'A tutorial on energy-based learning'. In: _Predicting structured data_. MIT Press, 2006. Chap. 1.
* [38] Minjong Lee and Dongwoo Kim. 'Robust Evaluation of Diffusion-Based Adversarial Purification'. In: _International Conference on Computer Vision_. 2024.
* [39] Fangzhou Liao et al. 'Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser'. In: _IEEE Conference on Computer Vision and Pattern Recognition_. 2018.
* [40] Guang Lin et al. _Robust Diffusion Models for Adversarial Purification_. 2024. arXiv: 2403.16067.
* [41] Liyuan Liu et al. 'On the Variance of the Adaptive Learning Rate and Beyond'. In: _International Conference on Learning Representations_. 2020.
* [42] Aleksander Madry et al. 'Towards Deep Learning Models Resistant to Adversarial Attacks'. In: _International Conference on Learning Representations_. 2018.
* [43] Tom M. Mitchell. _The Need for Biases in Learning Generalizations_. Tech. rep. New Brunswick, NJ: Rutgers University, 1980.
* [44] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi and Pascal Frossard. 'DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks'. In: _IEEE Conference on Computer Vision and Pattern Recognition_. 2016.
* [45] Weili Nie et al. 'Diffusion Models for Adversarial Purification'. In: _Proceedings of the International Conference on Machine Learning_. 2022.
* [46] Adam Paszke et al. 'PyTorch: An Imperative Style, High-Performance Deep Learning Library'. In: _Advances in Neural Information Processing Systems_. 2019.
* [47] ShengYun Peng et al. _Robust Principles: Architectural Design Principles for Adversarially Robust CNNs_. 2023.
* [48] Yao Qin et al. 'Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition'. In: _Proceedings of the International Conference on Machine Learning_. 2019.
* [49] Sylvestre-Alvise Rebuffi et al. 'Data Augmentation Can Improve Robustness'. In: _Advances in Neural Information Processing Systems_. 2021.
* [50] Danilo Jimenez Rezende, Shakir Mohamed and Daan Wierstra. 'Stochastic Backpropagation and Approximate Inference in Deep Generative Models'. In: _Proceedings of the International Conference on Machine Learning_. 2014.
* [51] Herbert Robbins and Sutton Monro. 'A Stochastic Approximation Method'. In: _The Annals of Mathematical Statistics_ 22.3 (1951), pp. 400-407.
* [52] Pouya Samangouei, Maya Kabkab and Rama Chellappa. 'Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models'. In: _International Conference on Learning Representations_. 2018.
* [53] Changhao Shi, Chester Holtz and Gal Mishne. 'Online Adversarial Purification based on Self-supervised Learning'. In: _International Conference on Learning Representations_. 2021.
* [54] Naman D Singh, Francesco Crocce and Matthias Hein. _Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models_. 2023. arXiv: 2303.01870.
* [55] Leslie N. Smith. 'Cyclical Learning Rates for Training Neural Networks'. In: _IEEE Winter Conference on Applications of Computer Vision_. 2017.
* [56] Kihyuk Sohn, Honglak Lee and Xinchen Yan. 'Learning Structured Output Representation using Deep Conditional Generative Models'. In: _Advances in Neural Information Processing Systems_. 2015.
* [57] Christian Szegedy et al. 'Intriguing properties of neural networks'. In: _International Conference on Learning Representations_. 2014.
* [58] Florian Tramer and Dan Boneh. 'Adversarial Training and Robustness for Multiple Perturbations'. In: _Advances in Neural Information Processing Systems_. 2019.

* [59] Florian Tramer et al. 'Ensemble Adversarial Training: Attacks and Defenses'. In: _International Conference on Learning Representations_. 2018.
* [60] Florian Tramer et al. 'On Adaptive Attacks to Adversarial Example Defenses'. In: _Advances in Neural Information Processing Systems_. 2020.
* [61] Pascal Vincent et al. 'Extracting and composing robust features with denoising autoencoders'. In: _International Conference on Machine Learning_. 2008.
* [62] Zekai Wang et al. _Better Diffusion Models Further Improve Adversarial Training_. 2023. arXiv: 2303.10130.
* [63] Han Xiao, Kashif Rasul and Roland Vollgraf. _Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms_. 2017. arXiv: 1708.07747.
* [64] Xinchen Yan et al. 'Attribute2Image: Conditional Image Generation from Visual Attributes'. In: _Proceedings of the European Conference on Computer Vision_. 2016.
* [65] Zhaoyuan Yang et al. 'Adversarial Purification with the Manifold Hypothesis'. In: _AAAI Conference on Artificial Intelligence_. 2024.
* [66] Jongmin Yoon, Sung Ju Hwang and Juho Lee. 'Adversarial Purification with Score-based Generative Models'. In: _Proceedings of the International Conference on Machine Learning_. 2021.
* [67] Hongyang Zhang et al. 'Theoretically Principled Trade-off between Robustness and Accuracy'. In: _Proceedings of the International Conference on Machine Learning_. 2019.
* [68] M. Zhang, S. Levine and C. Finn. 'MEMO: Test Time Robustness via Adaptation and Augmentation'. In: _Advances in Neural Information Processing Systems_. 2022.
* [69] Michael Zhang et al. 'Lookahead Optimizer: k steps forward, 1 step back'. In: _Advances in Neural Information Processing Systems_. 2019.
* [70] Zhaoyu Zhang, Mengyan Li and Jun Yu. 'On the Convergence and Mode Collapse of GAN'. In: _SIGGRAPH Asia 2018 Technical Briefs_. 2018.

On Projected Gradient Descent adversarial training

The task of determining model parameters \(\bm{\theta}^{\star}\) that are robust to adversarial perturbations is cast in [42] as a _min-max_ optimisation problem seeking to minimise _adversarial risk_, _i.e._:

\[\bm{\theta}^{\star}\approx\hat{\bm{\theta}}^{\star}:=\operatorname*{arg\,min}_{ \bm{\theta}}\mathbb{E}_{(\bm{x},y)\sim\mathcal{D}}\left[\max_{\bm{\delta}\in \mathbb{S}}\mathcal{L}\left(f\left(\bm{x}+\bm{\delta};\bm{\theta}\right),y \right)\right]\]

where \(\mathcal{D}\) is the distribution on the examples \(\bm{x}\) and the corresponding labels \(y\), \(f(\cdot;\bm{\theta})\) is a model with learnable parameters \(\bm{\theta}\), \(\mathcal{L}\) is a suitable loss function, and \(\mathbb{S}\) is the set of allowed constrained perturbations. In the case of \(\ell_{p}\) norm-bound perturbations of maximum magnitude \(\epsilon\), we can further specify \(\mathbb{S}:=\{\bm{\delta}\,|\,\|\bm{\delta}\|_{p}\leq\epsilon\}\).

The inner optimisation problem is solved, in [42], by _Projected Gradient Descent_ (Pgd), an iterative algorithm whose goal is the synthesis of an adversarial perturbation \(\hat{\bm{\delta}}=\bm{\delta}^{(K)}\) after \(K\)_gradient ascent and projection_ steps defined as:

\[\bm{\delta}^{(k+1)}\leftarrow\mathfrak{P}_{\mathbb{S}}\Big{(}\bm{\delta}^{(k )}+\alpha\operatorname*{sign}\Big{(}\nabla_{\bm{\delta}^{(k)}}\mathcal{L}_{ce }(f(\bm{x}+\bm{\delta}^{(k)};\bm{\theta}),y)\Big{)}\Big{)}\]

where \(\bm{\delta}^{(0)}\) is randomly sampled within \(\mathbb{S}\), \(\alpha\) is a hyperparameter (_step size_), \(\mathcal{L}_{ce}\) is the cross-entropy function, and \(\mathfrak{P}_{\mathbb{A}}\) is the Euclidean projection operator onto set \(\mathbb{A}\), _i.e._:

\[\mathfrak{P}_{\mathbb{A}}(\bm{a}):=\operatorname*{arg\,min}_{\bm{a}^{\prime} \in\mathbb{A}}\left|\left|\bm{a}-\bm{a}^{\prime}\right|\right|_{2}\;.\]

The outer optimisation is carried out by simply training \(f(\cdot;\bm{\theta})\) on the examples found by Pgd against the current model parameters - and their original pre-perturbation labels. The overall procedure just described constitutes Pgd_adversarial training_.

## Appendix B On the functioning of (conditional) Variational Autoencoders

Variational autoencoders (_VAEs_) [32, 50] learn from data a generative distribution of the form \(p(\bm{x},\bm{z})=p(\bm{x}\,|\,\bm{z})p(\bm{z})\), where the probability density \(p(\bm{z})\) represents a prior over latent variable \(\bm{z}\), and \(p(\bm{x}\,|\,\bm{z})\) is the likelihood function, which can be used to sample data of interest \(\bm{x}\), given \(\bm{z}\).

Training is carried out by maximising a variational lower bound, \(-\mathcal{L}_{\text{VAE}}(\bm{x})\), on the log-likelihood \(\log p(\bm{x})\) - which is a proxy for the _Evidence Lower Bound_ (_ELBO_) - _i.e._:

\[-\mathcal{L}_{\text{VAE}}(\bm{x}):=\mathbb{E}_{q(\bm{z}\,|\,\bm{x})}[\log p( \bm{x}\,|\,\bm{z})]-\operatorname{KL}(q(\bm{z}\,|\,\bm{x})\|p(\bm{z}))\]

where \(q(\bm{z}\,|\,\bm{x})\approx p(\bm{z}\,|\,\bm{x})\) is an approximate posterior and \(\operatorname{KL}(\cdot\|\cdot)\) is the Kullback-Leibler divergence.

By parameterising the likelihood with a _decoder ANN_\(p_{\theta_{\text{D}}}(\bm{x}\,|\,\bm{z};\bm{\theta}_{\text{D}})\approx p(\bm{x} \,|\,\bm{z})\), and a possible variational posterior with an _encoder ANN_\(q_{\theta_{\text{E}}}(\bm{z}\,|\,\bm{x};\bm{\theta}_{\text{E}})\approx q( \bm{z}\,|\,\bm{x})\), the parameters \(\bm{\theta}_{\text{D}}^{\star}\) of the generative model that best reproduces the data can be learnt - jointly with \(\bm{\theta}_{\text{E}}^{\star}\) - as:

\[\bm{\theta}_{\text{E}}^{\star},\bm{\theta}_{\text{D}}^{\star}:=\] \[\operatorname*{arg\,min}_{(\bm{\theta}_{\text{E}},\bm{\theta}_{ \text{D}})}\mathcal{L}_{\text{VAE}}(\bm{x})=\] \[\operatorname*{arg\,min}_{(\bm{\theta}_{\text{E}},\bm{\theta}_{ \text{D}})}\mathbb{E}_{\bm{x}\sim\mathcal{D}}\left[-\mathbb{E}_{\bm{z}\sim q _{\theta_{\text{E}}}(\bm{z}\,|\,\bm{x};\bm{\theta}_{\text{E}})}\left[\log p_{ \theta_{\text{D}}}(\bm{x}\,|\,\bm{z};\bm{\theta}_{\text{D}})\right]+ \operatorname{KL}(q_{\theta_{\text{E}}}(\bm{z}\,|\,\bm{x};\bm{\theta}_{\text{ E}})\|p(\bm{z}))\right]\]

where \(\mathcal{D}\) is the distribution over the (training) examples \(\bm{x}\).

From a practical point of view, optimisation is based on the empirical evaluation of \(\mathcal{L}_{\text{VAE}}(\bm{x};\bm{\theta})\) on mini-batches of data, with the term \(-\mathbb{E}_{\bm{z}\sim q_{\theta_{\text{E}}}(\bm{z}\,|\,\bm{x};\bm{\theta}_{ \text{E}})}\left[\log p_{\theta_{\text{D}}}(\bm{x}\,|\,\bm{z};\bm{\theta}_{ \text{D}})\right]\) replaced by a _reconstruction cost_\[\mathcal{L}_{\text{Reco}}(\bm{x},\bm{x}^{\prime})\geq 0\;|\;\mathcal{L}_{\text{Reco}}( \bm{x},\bm{x}^{\prime})=0\iff x=x^{\prime}\,.\]

The generation of new data according to the fitted model is achieved by sampling from

\[p_{\bm{\theta}_{\bm{\theta}}^{*}}(\bm{x}\;|\;\bm{z};\bm{\theta}_{\text{D}}^{*}) \bigg{|}_{\bm{z}\sim p(\bm{z})}\]

_i.e._ decoding samples from \(p(\bm{z})\).

The setting is analogous in the case of _conditional_ Variational Autoencoders [56; 64] (see section 3), where conditional sampling is achieved by

\[\bm{x}_{\bm{e}_{j}}\sim p_{\bm{\theta}_{\bm{\theta}}^{*}}(\bm{x}\;|\;\bm{z}, \bm{c};\bm{\theta}_{\text{D}}^{*})\bigg{|}_{\bm{z}\sim p(\bm{z});\;\bm{c}=\bm {c}_{j}}\,.\]

## Appendix C Justification of Adversarially-balanced batches

During the incipient phases of experimentation, preliminary tests were performed with the MNIST [36] and Fashion-MNIST [63] datasets - using a conditional _VAE_ as the _purifier_, and small FCNs or _convolutional ANN_s as the _classifiers_. Adversarial examples were generated against the adversarially pre-trained _classifier_, and tentatively denoised by the _purifier_ with one sample only. The resulting recovered inputs were classified by the _classifier_ and the overall accuracy was recorded.

Importantly, such tests were not meant to assess the _end-to-end_ adversarial robustness of the whole architecture, but only to tune the training protocol of the _purifier_.

Generating adversarial training examples by means of Pgd is considered the _gold standard_[24] and was first attempted as a natural choice to train the purifier. However, in this case, the following phenomena were observed:

* Unsatisfactory _clean_ accuracy was reached upon convergence, speculatively a consequence of the _VAE_ having never been trained on _clean_-to-_clean_ example reconstruction;
* Persistent vulnerability to same norm-bound Fgsm perturbations was noticed;
* Persistent vulnerability to smaller norm-bound Fgsm and Pgd perturbations was noticed.

In an attempt to mitigate such issues, the composition of adversarial examples was adjusted to specifically counteract each of the issues uncovered. The adoption of any smaller subset of attack types or strength, compared to that described in subsection 4.4, resulted in unsatisfactory mitigation.

At that point, another problem emerged: if such an adversarial training protocol was carried out in homogeneous batches, each containing the same type and strength of attack (or none at all), the resulting robust accuracy was still partially compromised due to the homogeneous ordering of attack types and strengths across batches.

Such observations lead to the final formulation of the training protocol, detailed in subsection 4.4, which mitigates to the best the issues described so far.

## Appendix D Heuristic justification of the _robust aggregation strategy_

The rationale leading to the choice of the specific _robust aggregation strategy_ described in subsection 4.5 was an attempt to answer the following question: 'How is it possible to aggregate the results of an ensemble of classifiers in a way such that it is hard to tilt the balance of the ensemble by attacking only a few of its members?'. The same reasoning can be extended to the _reciprocal_ problem we are trying to solve here, where different input reconstructions obtained from the same potentially perturbed input are classified by the same model (the _classifier_).

Far from providing a satisfactory answer, we can analyse the behaviour of our aggregation strategy as the logit associated with a given model and class varies across its domain, under the effect of adversarial intervention. Comparison with existing (and more popular) _probability averaging_ and _logit averaging_ aggregation strategies should provide a heuristic justification of our choice.

We recall our aggregation strategy:

\[P_{i}:=\frac{1}{Z}\prod_{\alpha=1}^{N}e^{e^{l_{i}^{\alpha}}}.\]

Additionally, we recall _logit averaging_ aggregation

\[P_{i}:=\frac{1}{Z}e^{\frac{1}{N}\sum_{\alpha=1}^{N}l_{i}^{\alpha}}=\frac{1}{Z} \prod_{\alpha=1}^{N}e^{\frac{1}{N}l_{i}^{\alpha}}=\frac{1}{Z}\left(\prod_{ \alpha=1}^{N}e^{l_{i}^{\alpha}}\right)^{\frac{1}{N}}\]

and _probability averaging_ aggregation

\[P_{i}:=\frac{1}{Z}\sum_{\alpha=1}^{N}\frac{e^{l_{i}^{\alpha}}}{\sum_{j=1}^{C}e ^{l_{j}^{\alpha}}}=\sum_{\alpha=1}^{N}e^{l_{i}^{\alpha}}\frac{1}{Q^{\alpha}}\]

where \(Q^{\alpha}=\sum_{j=1}^{C}e^{l_{j}^{\alpha}}\).

Finally, since \(l_{i}^{\alpha}\in\mathbb{R},\forall l_{i}^{\alpha}\), \(\lim_{x\rightarrow-\infty}e^{x}=0\) and \(e^{0}=1\), we can observe that \(e^{l_{i}^{\alpha}}>0\) and \(e^{e^{l_{i}^{\alpha}}}>1,\forall l_{i}^{\alpha}\).

Now, we consider a given class \(i^{\star}\) and the _classifier_ prediction on a given input reconstruction \(\alpha^{\star}\), and study the potential effect of an adversary acting on \(l_{i}^{\alpha^{\star}}\). This adversarial intervention can be framed in two complementary scenarios: either the class \(i^{\star}\) is correct and the adversary aims to decrease its membership probability, or the class \(i^{\star}\) is incorrect and the adversary aims to increase its membership probability. In any case, the adversary should comply with the \(\epsilon_{\infty}\)-boundedness of its perturbation on the input.

Logit averagingIn the former scenario, the product of \(e^{l_{i}^{\alpha}}\) terms can be arbitrarily deflated (up to zero) by lowering the \(l_{i}^{\alpha^{\star}}\) logit only. In the latter scenario, the logit can be arbitrarily inflated, and such effect is only partially suppressed by normalisation by \(Z\) (a sum of \(\nicefrac{{1}}{{N}}\)-exponentiated terms).

Probability averagingIn the former scenario, although the effect of the deflation of a single logit is bounded by \(e^{l_{i^{\star}}^{\alpha^{\star}}}>0\), two attack strategies are possible: either decreasing the value of \(l_{i}^{\alpha^{\star}}\) or increasing the value of \(Q^{\alpha^{\star}}\), giving rise to complex combined effects. In the latter scenario, the reciprocal is possible, _i.e._ either inflating \(l_{i^{\star}}^{\alpha^{\star}}\) or deflating \(Q^{\alpha^{\star}}\). Normalisation has no effect in both cases.

OursIn the former scenario, the effect of logit deflation on a single product term is bounded by \(e^{e^{l_{i^{\star}}^{\alpha^{\star}}}}>1\), thus exerting only a minimal collateral effect on the product, through a decrease of \(Z\). This effectively prevents _aggregation takeover_ by logit deflation. Similarly to _logit averaging_, in the latter scenario, the logit can be arbitrarily inflated. However, in this case, the effect of normalisation by \(Z\) is much stronger, given its increased magnitude.

From such a comparison, our aggregation strategy is the only one that strongly prevents _adversarial takeover_ by _logit deflation_, while still defending well against perturbations targeting _logit inflation_.

## Appendix E Architectural details and hyperparameters

In the following section, we provide more precise details about the architectures (subsection E.1) and hyperparameters (subsection E.2) used in the experimental phase of our work.

### Architectures

In the following subsection, we describe the specific structure of the individual parts composing the _purifier_ - in the three scenarios considered. As far as the _classifier_ architectures are concerned, we redirect the reader to the original articles introducing those models (_i.e._: [18] for _scenarios (a)_ and _(b)_, [62] for _scenario (c)_).

During training, before being processed by the _purifier_ encoder, input examples are standardised according to the statistics of the respective training dataset.

Afterwards, they are fed to the disjoint input encoder (see subsection 4.3), whose architecture is shown in Table 4. The same architecture is used in all scenarios considered.

The original input is also fed to the _classifier_. The corresponding internal representation is extracted, preserving its layered structure. In order to improve the scalability of the method, only a subset of _classifier_ layers is used instead of the whole internal representation. Specifically, for each _block_ of the WideResNet architecture, only the first layers have been considered; two _shortcut layers_ have also been added for good measure. The exact list of those layers is reported in Table 5.

Each extracted layerwise (pre)activation tensor has the shape of a multi-channel image, which is processed - independently for each layer - by a different CNN whose individual architecture is shown in Table 6 (_scenarios (a)_ and _(b)_) and Table 7 (_scenario (c)_).

The resulting tensors (still having the shape of multi-channel images) are then jointly processed by a fully-connected subnetwork whose architecture is shown in Table 8. The value of fcrepr for the different scenarios considered is shown in Table 13.

The _compressed input_ and _compressed internal representation_ so obtained are finally jointly encoded by an additional fully-connected subnetwork whose architecture is shown in Table 9. The output is a tuple of means and standard deviations to be used to sample the stochastic latent code \(\bm{z}\).

The sampler used for the generation of such latent variables \(\bm{z}\), during the training of the _purifier_, is a reparameterised [32] Normal sampler \(\bm{z}\sim\mathcal{N}(\mu,\sigma)\). During inference, \(\bm{z}\) is sampled by reparameterisation from the _i.i.d_ Standard Normal distribution \(\bm{z}\sim\mathcal{N}(0,1)\) (_i.e._ from its original prior).

The architectures for the decoder of the _purifier_ are shown in Table 10 (_scenarios (a)_ and _(b)_) and Table 11 (_scenario (c)_).

### Hyperparameters

In the following section, we provide the hyperparameters used for _adversarial example generation_ and _optimisation_ during the training of the _purifier_, and those related to the _purifier_ model architectures. We also provide the hyperparameters for the Pgd+Eot attack, which is used as a complementary tool for the evaluation of adversarial robustness.

\begin{table}
\begin{tabular}{l} _Disjoint Input Encoder (all scenarios)_ \\ Conv2D(ch\_in=3, ch\_out=6, ks=3, s=2, p=1, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ Conv2D(ch\_in=6, ch\_out=12, ks=3, s=2, p=1, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ \end{tabular}
\end{table}
Table 4: Architecture for the _disjoint input encoder_ of the _purifier_. The same architecture is used in all scenarios considered. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Conv2D: 2-dimensional convolutional layer; ch\_in: number of input channels; ch\_out: number of output channels; ks: kernel size; s: stride; p: padding; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain.

\begin{table}
\begin{tabular}{l} \hline _Layerwise Internal Representation Encoder (scenarios (a) and (b))_ \\ \hline Conv2D(ch\_in=[ci], ch\_out=ceil([ci]/2), ks=3, s=1, p=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ Conv2D(ch\_in=ceil([ci]/2), ch\_out=ceil([ci]/4), ks=3, s=1, p=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ Conv2D(ch\_in=ceil([ci]/4), ch\_out=ceil([ci]/8), ks=3, s=1, p=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ \hline \end{tabular}
\end{table}
Table 6: Architecture for the _layerwise internal representation encoder_ of the _purifier_. The architecture shown in this table is used in _scenarios (a)_ and _(b)_. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Conv2D: 2-dimensional convolutional layer; ch\_in: number of input channels; ch\_out: number of output channels; ks: kernel size; s: stride; p: padding; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The abbreviation [ci] indicates the number of input channels for the _(pre)activation tensor_ of each extracted layer. The abbreviation ceil indicates the _ceiling_ integer rounding function.

\begin{table}
\begin{tabular}{l} \hline _All scenarios_ \\ \hline layer.0.block.0.conv\_0 \\ layer.0.block.0.conv\_1 \\ layer.0.block.1.conv\_0 \\ layer.0.block.1.conv\_1 \\ layer.0.block.2.conv\_0 \\ layer.0.block.3.conv\_0 \\ layer.1.block.0.conv\_0 \\ layer.1.block.0.conv\_1 \\ layer.1.block.0.shortcut \\ layer.1.block.1.conv\_0 \\ layer.1.block.1.conv\_1 \\ layer.1.block.2.conv\_0 \\ layer.1.block.2.conv\_1 \\ layer.1.block.3.conv\_0 \\ layer.1.block.3.conv\_1 \\ layer.2.block.0.conv\_0 \\ layer.2.block.0.conv\_1 \\ layer.2.block.0.conv\_1 \\ layer.2.block.0.shortcut \\ layer.2.block.1.conv\_0 \\ layer.2.block.1.conv\_1 \\ layer.2.block.2.conv\_0 \\ layer.2.block.3.conv\_0 \\ layer.2.block.3.conv\_1 \\ \hline \end{tabular}
\end{table}
Table 5: _Classifier_ model (WideResNet-28-10) layer names used as (a subset of) the _internal representation_ fed to the _layerwise convolutional encoder_ of the _purifier_. The names reflect those used in the model implementation.

\begin{table}
\begin{tabular}{l} \hline \hline \multicolumn{2}{l}{_Layerwise Internal Representation Encoder (scenario (c))_} \\ \hline Conv2D(ch\_in=[ci], ch\_out=ceil([ci]/2), ks=3, s=1, p=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ Conv2D(ch\_in=ceil([ci]/2), ch\_out=ceil([ci]/4), ks=3, s=1, p=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ Conv2D(ch\_in=ceil([ci]/4), ch\_out=ceil([ci]/8), ks=3, s=1, p=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Architecture for the _layerwise internal representation encoder_ of the _purifier_. The architecture shown in this table is used in _scenario (c)_. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Conv2D: 2-dimensional convolutional layer; ch\_in: number of input channels; ch\_out: number of output channels; ks: kernel size; s: stride; p: padding; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The abbreviation [ci] indicates the number of input channels for the _(pre)activation tensor_ of each extracted layer. The abbreviation [cil] indicates the _ceiling_ integer rounding function.

\begin{table}
\begin{tabular}{l} \hline \hline \multicolumn{2}{l}{_Fully-Connected Representation Encoder (all scenarios)_} \\ \hline Concatenate(flatten\_features=True) \\ Linear(feats\_in=[computed], feats\_out=fcrepr, b=False) \\ BatchNorm1D(affine=True) \\ LeakyReLU(slope=0.2) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Architecture for the _fully-connected representation encoder_ of the _purifier_. The architecture shown in this table is used in all scenarios considered. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Concatenate: layer concatenating its input features; flatten\_features: whether the input features are to be flattened before concatenation; feats\_in, feats\_out: number of input and output features of a linear layer; b: presence of a learnable bias term; BatchNorm1D: 1-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The abbreviation [computed] indicates that the number of features is computed according to the shape of the concatenated input tensors. The value of fcrepr for the different scenarios considered is shown in Table 13.

\begin{table}
\begin{tabular}{l} \hline \hline \multicolumn{2}{l}{_Fully-Connected Representation Encoder (all scenarios)_} \\ \hline Concatenate(flatten\_features=True) \\ Linear(feats\_in=[computed], feats\_out=fcrepr, b=False) \\ BatchNorm1D(affine=True) \\ LeakyReLU(slope=0.2) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Architecture for the _fully-connected joint encoder_ of the _purifier_. The architecture shown in this table is used in all scenarios considered. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Concatenate: layer concatenating its input features; flatten\_features: whether the input features are to be flattened before concatenation; feats\_in, feats\_out: number of input and output features of a linear layer; b: presence of a learnable bias term; BatchNorm1D: 1-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The abbreviation [computed] indicates that the number of features is computed according to the shape of the concatenated input tensors. The value of fjoint for the different scenarios considered is shown in Table 13. The last _layer_ of the network returns a tuple of 2 tensors, each independently processed â€“ from the output of the previous layer â€“ by the two comma-separated _sub-layers_.

\begin{table}
\begin{tabular}{l} \hline _Decoder (scenarios (a) and (b))_ \\ \hline Concatenate(flatten_features=True) \\ Linear(feats\_in=[fjoint+fcrepr],feats\_out=2304, b=True) \\ LeakyReLU(slopez-0.2) \\ Unflatten(265, 3) \\ ConvTranspose2D(ch\_in=256, ch\_out=256, ks=3, s=2, p=1, op=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ ConvTranspose2D(ch\_in=256, ch\_out=128, ks=3, s=2, p=1, op=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ ConvTranspose2D(ch\_in=256, ch\_out=128, ks=3, s=2, p=1, op=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ ConvTranspose2D(ch\_in=32, ch\_out=3, ks=2, s=1, p=1, op=0, b=True) \\ Sigmoid() \\ \hline \end{tabular}
\end{table}
Table 10: Architecture for the decoder of the _purifier_. The architecture shown in this table is used in _scenarios (a)_ and (b)_. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Concatenate: layer concatenating its input features; flatten_features: whether the input features are to be flattened before concatenation; feats_in, feats_out: number of input and output and output features of a linear layer; b: presence of a learnable bias term; ConvTranspose2D: 2-dimensional transposed convolutional layer; ch\_in: number of input channels; ch\_out: number of output channels; ks: kernel size; s: stride; p: padding; op: PyTorch parameter â€™output paddingâ€™, used to disambiguate the number of spatial dimensions of the resulting output; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The values of fjoint and fcrepr for the different scenarios considered are shown in Table 13.

\begin{table}
\begin{tabular}{l} \hline _Decoder (scenarios (a) and (b))_ \\ \hline Concatenate(flatten_features=True) \\ Linear(feats\_in=[fjoint+fcrepr],feats\_out=2304, b=True) \\ LeakyReLU(slopez-0.2) \\ Unflatten(265, 3) \\ ConvTranspose2D(ch\_in=256, ch\_out=256, ks=3, s=2, p=1, op=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ ConvTranspose2D(ch\_in=256, ch\_out=128, ks=3, s=2, p=1, op=0, b=False) \\ BatchNorm2D(affine=True) \\ LeakyReLU(slope=0.2) \\ ConvTranspose2D(ch\_in=32, ch\_out=3, ks=2, s=1, p=1, op=0, b=True) \\ Sigmoid() \\ \hline \end{tabular}
\end{table}
Table 11: Architecture for the decoder of the _purifier_. The architecture shown in this table is used in _scenario (c)_. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Concatenate: layer concatenating its input features; flatten_features: whether the input features are to be flattened before concatenation; feats_in, feats_out: number of input and output features of a linear layer; b: presence of a learnable bias term; ConvTranspose2D: 2-dimensional transposed convolutional layer; ch\_in: number of input channels; ks: kernel size; s: stride; p: padding; op: PyTorch parameter â€™output paddingâ€™, used to disambiguate the number of spatial dimensions of the resulting output; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The values of fjoint and fcrepr for the different scenarios considered are shown in Table 13.

AttacksThe hyperparameters used for the adversarial attacks described in subsection 4.4 are shown in Table 12. The value of \(\epsilon_{\infty}\) is fixed to \(\epsilon_{\infty}=\nicefrac{{8}}{{255}}\). With the only exception of \(\epsilon_{\infty}\), AutoAttack is to be considered a _hyperparameter-free_ adversarial example generator.

ArchitecturesTable 13 contains the hyperparameters that define the model architectures used as part of the _purifier_, in the different scenarios considered.

TrainingTable 14 collects the hyperparameters governing the training of the _purifier_ in the different scenarios considered.

## Appendix F Additional tables

The following section contains additional tabular data that may be of interest to the reader.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & _Scenario (a)_ & _Scenario (b)_ & _Scenario (c)_ \\ \hline fcepr & \(512\) & \(512\) & \(768\) \\ fjoint & \(128\) & \(128\) & \(192\) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Scenario-specific architectural hyperparameters for the _purifier_, as referred to in Table 8, Table 9, Table 10, and Table 11.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Fgsm & Pgd & Pgd+EoT \\ \hline Input clipping & \([0.0,1.0]\) & \([0.0,1.0]\) & \([0.0,1.0]\) \\ \# of steps & \(1\) & \(40\) & \(200\) \\ Step size & \(\epsilon_{\infty}\) & \(0.01\) & \(0.007\) \\ Loss function & _CCE_ & _CCE_ & _CCE_ \\ \# of EoT iterations & \(1\) & \(1\) & \(20\) \\ Optimiser & & _SGD_ & _SGD_ \\ \hline \hline \end{tabular}
\end{table}
Table 12: Hyperparameters for the attacks used for training and testing the _purifier_ The Fgsm and Pdg attacks refer to the training phase (see subsection 4.4), whereas the Pgd+EoT attack [38] refers to the robustness assessment pipeline. The entry CCE denotes the _Categorical CrossEntropy_ loss function. The \(\ell_{\infty}\) threat model is assumed, and all inputs are linearly rescaled within \([0.0,1.0]\) before the attack.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & _Scenario (a)_ & _Scenario (b)_ & _Scenario (c)_ \\ \hline fcepr & \(512\) & \(512\) & \(768\) \\ fjoint & \(128\) & \(128\) & \(192\) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Scenario-specific architectural hyperparameters for the _purifier_, as referred to in Table 8, Table 9, Table 10, and Table 11.

[MISSING_PAGE_FAIL:21]

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Claims made in the abstract and introduction of the paper accurately reflect its contributions, and those are directly corroborated by experimental analysis. Results and their discussion is available in subsection 5.2 and subsection 5.3.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: subsection 5.2 and subsection 5.3 also contain the discussion of potential limitations of the method, and open problems it introduces.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not contribute novel theoretical results. Assumptions anyway related to the contribution are clearly stated throughout the paper.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Training and evaluation details required to reproduce the experimental results of the paper are reported in section 5 and Appendix E. Code and data for the reproduction of all experiments are additionally released as part of Supplementary Material.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code and data for the reproduction of all experiments are released to reviewers as part of Supplementary Material. The public version of the paper will include instructions to obtain the same material from a dedicated publicly accessible source.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimiser, etc.) necessary to understand the results? Answer: [Yes] Justification: Training and evaluation details, including hyperparameters, required to reproduce the experimental results of the paper are reported in section 5 and Appendix E. Code and data for the reproduction of all experiments are additionally released as part of Supplementary Material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No]Justification: As doing adversarial training with 40-steps PGD is roughly 40 times more computationally demanding than nominal training, unfortunately, we are unable to show error bars or otherwise quantify statistical errors. In any case, the improvement induced by our method _w.r.t._ their _state-of-the-art_ counterparts is well clear of the threshold for statistical significance.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Information about computational resources and required time is contained in subsection 5.1 as well as in the supplementary materials.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper does conform, in every respect, with the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper proposes a novel technique to mitigate the problem of adversarial vulnerability of high-dimensional classifiers. Such vulnerability may pose potential societal impacts, as discussed in section 1.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not plan to release models or data with a high risk for misuse. Models to be released do not reasonably carry risk for misuse.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The creators and/or owners of assets used in the paper are either credited by reference to their original research work, or directly with a link to the preferred landing page for such assets. The use of licensed material is compliant with the respective licenses.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Trained model weights are provided alongside the paper, and their details are provided as part of supplementary materials. In case of release to the public, such details will be provided contextually to the models.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: No crowdsourcing or research with human subjects has been performed as part of the work of, or leading to, this paper.
* including that potentially requiring IRB approval or equivalent authorisation.