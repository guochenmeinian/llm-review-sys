# PrObeD: Proactive Object Detection Wrapper

Vishal Asnani

Michigan State University

asnanivi@msu.edu &Abhinav Kumar

Michigan State University

kumarab6@msu.edu &Suya You

DEVCOM Army Research Laboratory

suya.you.civ@army.mil &Xiaoming Liu

Michigan State University

liuxm@cse.msu.edu

###### Abstract

Previous research in \(2D\) object detection focuses on various tasks, including detecting objects in generic and camouflaged images. These works are regarded as passive works for object detection as they take the input image as is. However, convergence to global minima is not guaranteed to be optimal in neural networks; therefore, we argue that the trained weights in the object detector are not optimal. To rectify this problem, we propose a wrapper based on proactive schemes, PrObeD, which enhances the performance of these object detectors by learning a signal. PrObeD consists of an encoder-decoder architecture, where the encoder network generates an image-dependent signal termed templates to encrypt the input images, and the decoder recovers this template from the encrypted images. We propose that learning the optimum template results in an object detector with an improved detection performance. The template acts as a mask to the input images to highlight semantics useful for the object detector. Finetuning the object detector with these encrypted images enhances the detection performance for both generic and camouflaged. Our experiments on MS-COCO, CAMO, COD\(10\)K, and NC\(4\)K datasets show improvement over different detectors after applying PrObeD. Our models/codes are available at https://github.com/visha13477/Proactive-Object-Detection.

## 1 Introduction

Generic \(2D\) object detection (GOD) has improved from earlier traditional detectors [15, 20, 64, 65] to the deep-learning-based object detectors [58, 52, 8, 10, 26]. Advancements in deep-learning-based methods underwent many architectural change over recent years, including one-stage [54, 52, 43, 5, 46, 53, 5], two-stage [58, 23, 24, 5], CNN-based [54, 52, 14, 21, 22, 23, 16, 24], transformer-based [74, 8], and diffusion-based [10] methods. All these methods aim to predict the \(2D\) bounding box of the objects in the images and their category class.

Another emerging area related to generic object detection is camouflaged object detection [17, 27, 28, 29, 34, 40] (COD). COD aims to detect and segment objects blended with the background [17, 18] via object-level mask supervision. Applications of COD include medical [19, 45], surveillance [11] and autonomous driving [69]. Early COD detectors exploit hand-crafted features [61, 50] and optical flow [33], while current methods are deep-learning-based. These methods utilize attention [9, 63], joint learning [40], image gradient [34], and transformers [70, 48].

All these methods take input images as is for the detection task and hence are called passive methods. However, there is a line of research on proactive methods for a wide range of vision tasks such as disruption [59, 60], tagging [68], manipulation detection [1], and localization [2]. Proactive methodsuse signals, called templates, to encrypt the input images and pass the encrypted images as the input to the network. These are trained in an end-to-end manner by using either a fixed [68] or learnable template [1, 2, 59, 60] to improve the performance. A major advantage of proactive schemes is that such methods generalize better on unseen data/models [1, 2]. Motivated by this, we propose a plug-and-play Proactive Object Detection wrapper, PrObeD, to improve GOD and COD detectors.

Designing PrObeD as a proactive scheme involves several challenges and key factors. First, the proactive wrapper needs to be a plug-and-play module that can be applied to both GOD and COD detectors. Secondly, the encryption process should be intuitive to benefit the object detection task. _e.g._, an ideal template for detection should highlight the foreground objects in the input image. Lastly, the choice of supervision to estimate the template for encryption is hard to formulate.

Previous proactive methods [1, 2] use learnable but image-independent templates for manipulation and localization tasks. However, the object detection task is scene-specific; therefore, the ideal template should be image-dependent. Based on this key insight, we propose a novel plug-and-play proactive wrapper in which we apply object detectors to enhance detection performance. The PrObeD wrapper utilizes an encoder network to learn an image-dependent template. The learned template encrypts the input images by applying a transformation, defined as an element-wise multiplication between the template and the input image. The decoder network recovers the templates from the encrypted images. We utilize regression losses for supervision and leverage the ground-truth object map to guide the learning process, thereby imparting valuable object semantics to be integrated into the template. We then fine-tune the proactive wrapper with the GOD and COD detectors to improve their detection performance. Extensive experiments on MS-COCO, CAMO, COD\(10\)K, and NC\(4\)K datasets show that PrObeD improves the detection performance for both GOD and COD detectors.

In summary, the contributions of this work include:

* We propose a novel proactive approach \(PrObeD\) for the object detection task. To the best of our knowledge, this is the first work to develop a proactive approach to \(2D\) object detection.
* We mathematically prove that the proactive method results in a better-converged model than the passive detector under assumptions and, consequently, a better object detector.
* PrObeD wraps around both GOD and COD detectors and improves detection performance on MS-COCO, CAMO, COD10K, and NC\(4\)K datasets

## 2 Related works

**Proactive Schemes.** Earlier works adopt to add signals like perturbation [60], adversarial noise [59], and one-hot encoding [68] messages while focusing on tasks like disruption [59, 60] and deepfake tagging [68]. Asnani _et al_. [1] propose to learn an optimized template for binary detection by unseen generative models. Recently, MaLP [2] adds the learnable template to perform generalized

Figure 1: **(a) Passive _vs._ Proactive object detection. A learnable template encrypts the input images, which are further used to train the object detector. (b) PrObeD serves as a wrapper on both generic and camouflaged object detectors, enhancing the detection performance. (c) For the linear regression model under additive noise and other assumptions, the converged weights of the proactive detector are closer to the optimal weights as compared to the converged weights of the passive detector. See Sec. 3.2 for details and proof.**

manipulation localization for unknown generative models. Unlike these works, PrObeD uses image-dependent templates and is a plug-and-play wrapper for a different task of object detection.

Generic Object DetectionDetection of generic objects, instead of specific object categories such as pedestrians [7], apples [13], and others [37, 4, 38], has been a long-standing objective of computer vision. RCNN [24, 25] employs the extraction of object proposals. He _et al_. [31] propose a spatial pooling layer to extract a fixed-length representation of all the objects. Modifications of RCNN [23, 41, 58, 72] increase the inference speed. Feature pyramid network [42] detects objects with a wide variety of scales. The above methods are mostly two-stage, so inference is an issue. Single-stage detectors like YOLO [5, 52, 53, 66, 54], SSD [46], HRNet [67] and RetinaNet [43] increase the speed and simplicity of the framework compared to the two-stage detector. Recently, transformer-based methods [74, 8] use a global-scale receptive field. Chen _et al_. [10] use diffusion models to denoise noisy boxes at every forward step. PrObeD functions as a wrapper around the pre-existing object detector, facilitating its transformation into an enhanced object detector. The comparison of PrObeD with prior works is summarized in Tab. 1.

Camouflaged Object DetectionEarly COD works rely on hand-crafted features like co-occurrence matrices [61], \(3D\) convexity [50], optical flow [33], covariance matrix [35], and multivariate calibration components [57]. Later on, [9, 63] incorporate an attention-based cross-level fusion of multi-scale features to recover contextual information. Mei _et al_. [49] take motivation by predators to identify camouflaged objects using a position and focus ideology. SINet [18] uses a search and identification module to perform localization. SINET-v2 [17] uses group-reversal attention to extract the camouflaged maps. [36] explores uncertainty maps and [75] utilizes cube-like architecture to integrate multi-layer features. ANet [39], LSR [47], and JCSOD [40] employ joint learning with different tasks to improve COD. Lately, [12, 48, 70] apply a transformer-based architecture for difficult-aware learning, uncertainty modeling, and temporal consistency. Zhai _et al_. [73] use a graph learning model to disentangle input into different features for localization. DGNet [34] uses image gradients to exploit intensity changes in the camouflaged object from the background. Unlike these methods, PrObeD uses proactive methods to improve camouflaged object detection.

## 3 Proposed Approach

Our method originates from understanding what makes proactive schemes effective. We first overview the two detection problems: GOD and COD in Sec. 3.1. We next derive Lemma 1, where we show that the proactive schemes with the multiplicative transformation of images are better than passive schemes by comparing the deviation of trained network weights from the optimal. Based on this result, we derive that Average Precision (AP) from the proactive model is better than AP from the passive model in Theorem 1. At last, we present our proactive scheme-based wrapper, PrObeD, in Sec. 3.3, which builds upon the Theorem 1 to improve generic 2D objects and camouflaged detection.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Proactive} & \multirow{2}{*}{Task} & \multicolumn{2}{c|}{Template} & \multirow{2}{*}{COD} & \multirow{2}{*}{GOD} & \multirow{2}{*}{Plug-Play} \\ \cline{1-1} \cline{6-7} \cline{8-8} Faster R-CNN [58] & \(\bigtimes\) & & Object Detection & - & & \(\bigtimes\) & ✓ & \(\bigtimes\) \\ YOLO [52] & \(\bigtimes\) & Object Detection & - & - & & \(\bigtimes\) & ✓ & \(\bigtimes\) \\ DeTR [8] & \(\bigtimes\) & Object Detection & - & - & & \(\bigtimes\) & ✓ & \(\bigtimes\) \\ DGNet [34] & \(\bigtimes\) & Object Detection & - & - & ✓ & \(\bigtimes\) & ✗ \\ SINet-v2 [17] & \(\bigtimes\) & Object Detection & - & - & ✓ & ✗ & ✗ \\ JCSOD [40] & \(\bigtimes\) & Object Detection & - & - & ✓ & ✗ & ✗ \\ OGAN [60] & ✓ & \multicolumn{2}{c|}{Disturb} & \multicolumn{2}{c|}{Learnable} & - & - & ✗ \\ Ruiz _et al_. [59] & ✓ & Disrupt & \(1\) & Learnable & - & - & ✗ \\ Yeh _et al_. [71] & ✓ & Disrupt & \(1\) & Learnable & - & - & ✗ \\ FakeTagger [68] & ✓ & \multicolumn{2}{c|}{Tagging} & \(\geq 1\) & Fixed, Id-dependent & - & - & ✗ \\ Asnani _et al_. [1] & ✓ & Manipulation Detection & \(\geq 1\) & Learnable set, Image-independent & - & - & ✓ \\ MaLP [2] & ✓ & Manipulation Localization & \(\geq 1\) & Learnable set, Image-independent & - & - & ✓ \\ PrObeD (Ours) & ✓ & \multicolumn{2}{c|}{Object Detection} & \(\geq 1\) & Learnable, Image-dependent & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of PrObeD with prior works.

### Background

#### 3.1.1 Passive Object Detection

Although generic \(2D\) object detection and camouflage detection are similar problems, they have different objective functions. Therefore, we treat them as two different problems and define their objectives separately.

**Generic 2D Object Detection.** Let \(\bm{I}_{j}\) be the set of input images given to the generic 2D object detector \(\mathcal{O}\) with trainable parameters \(\theta\). Most of these detectors output two sets of predictions per image: (1) bounding box coordinates, \(\mathcal{O}(\bm{I}_{j})_{1}=\hat{T}\in\mathbb{R}^{4}\), (2) class logits, \(\mathcal{O}(\bm{I}_{j})_{2}=\hat{C}\in\mathbb{R}^{C}\), where \(N\) is the number of foreground object categories. If the ground-truth bounding box coordinates are \(T_{j}\), and the ground-truth category label is \(C\), the objective function of such detector is:

\[\min_{\theta}\bigg{\{}\sum_{j}\Big{(}||\mathcal{O}(\bm{I}_{j};\theta)_{1}-T_{ j}||_{2}\Big{)}-\sum_{j}\sum_{i=1}^{N}\Big{(}C_{j}^{i}\cdot\text{log}(\mathcal{O} (\bm{I}_{j};\theta)_{2}))\Big{)}\bigg{\}}.\] (1)

**Camouflaged Object Detection.** Let \(\bm{I}_{j}\) be the input image set given to the camouflaged object detector \(\mathcal{O}\) with trainable parameters \(\theta\), and \(\bm{G}_{j}\) be the ground-truth segmentation map. Prior passive works predict a segmentation map with the following objective:

\[\min_{\theta}\bigg{\{}\sum_{j}\Big{(}\Big{|}\Big{|}\mathcal{O}(\bm{I}_{j}; \theta)-\bm{G}_{j}\Big{|}\Big{|}_{2}\Big{)}\bigg{\}}.\] (2)

#### 3.1.2 Proactive Object Detection

Proactive schemes [1, 2] encrypt the input images with the template to aid manipulation detection/localization. Such schemes take an input image \(\bm{I}_{j}\in\mathbb{R}^{H\times W\times 3}\) and learns a template \(\bm{S}_{j}\in\mathbb{R}^{H\times W}\). PrObeD uses image-dependent templates to improve object detection. Given an input image \(\bm{I}_{j}\in\mathbb{R}^{H\times W\times 3}\), PrObeD learns to output a template \(\bm{S}_{j}\in\mathbb{R}^{H\times W}\), which can be used by a transformation \(\mathcal{T}\) resulting in encrypted images \(\mathcal{T}(\bm{I}_{j})\). PrObeD uses element-wise multiplication as the transformation \(\mathcal{T}\), which is defined as:

\[\mathcal{T}(\bm{I}_{j})=\mathcal{T}(\bm{I}_{j};\bm{S}_{j})=\bm{I}_{j}\odot\bm {S}_{j}.\] (3)

### Mathematical Analysis of Passive and Proactive Detectors

PrObeD optimizes the template to improve the performance of the object detector. We argue that this template helps arrive at a better global minima representing the optimal parameters \(\theta\). We now define the following lemma to support our argument:

**Lemma 1**.: _Converged weights of proactive and passive detectors. Consider a linear regression model that regresses an input image \(\bm{I}_{j}\) under an additive noise setup to obtain the 2D coordinates. Assume the noise under consideration \(e\) is a normal random variable \(\mathcal{N}(0,\sigma^{2})\). Let \(\bm{w}\) and \(\bm{w}^{*}\) denote the trained weights of the pretrained linear regression model and the optimal weights of the linear regression model. Also, assume SGD optimizes the model parameters with decreasing step size \(s\) such that the steps are square summable i.e., \(\mathcal{S}=\lim\limits_{t\to\infty}\sum\limits_{k=1}^{t}s_{k}^{2}\) exist, and the noise is independent of the image. Then, there exists a template \(\bm{S}_{j}\in[0,1]\) for the image \(\bm{I}_{j}\) such that the multiplicative transformation of images as the input results in a trained weight \(\bm{w}^{*}\) closer to the optimal weight than the originally trained weight \(\bm{w}\). In other words,_

\[\mathbb{E}(||\bm{w}^{\prime}-\bm{w}^{*}||_{2})<\mathbb{E}(||\bm{w}-\bm{w}^{*} ||_{2}).\] (4)

The proof of Lemma 1 is in supplementary. We use the variance of the gradient of the encrypted images to arrive at this lemma. We next use Lemma 1 to derive the following theorem:

**Theorem 1**.: _AP comparison of proactive and passive detectors. Consider a linear regression model that regresses an input image \(\bm{I}_{j}\) under an additive noise setup to obtain the 2D coordinates. Assume the noise under consideration \(e\) is a normal random variable \(\mathcal{N}(0,\sigma^{2})\). Let \(\bm{w}\) and \(\bm{w}^{*}\) denote thetrained weights of the pretrained linear regression model and the optimal weights of the linear regression model. Also, assume SGD optimizes the model parameters with decreasing step size \(s\) such that the steps are square summable i.e., \(\mathcal{S}=\lim\limits_{t\rightarrow\infty}\sum\limits_{k=1}^{t}s_{k}^{2}\) exist, and the noise is independent of the image. Then, the AP of the proactive detector is better than the AP of the passive detector._

The proof of Theorem 1 is in the supplementary. We use the Lemma 1 and the non-decreasing nature of AP w.r.t. IoU to arrive at this theorem. Next, we adapt the objectives of Eqs. (1) and (2) to incorporate the proactive methods as follows:

\[\min_{\theta,\bm{S}_{j}}\bigg{\{}\sum_{j}\Big{(}||\mathcal{O}( \mathcal{T}(\bm{I}_{j};\bm{S}_{j});\theta)_{1}-T_{j}||_{2}\Big{)}-\sum_{j}\sum _{i=1}^{N}\Big{(}C_{j}^{i}\cdot\text{log}(\mathcal{O}(\mathcal{T}(\bm{I}_{j}; \bm{S}_{j});\theta)_{2})\Big{)}\bigg{\}},\] (5) \[\min_{\theta,\bm{S}_{j}}\bigg{\{}\sum_{j}\Big{(}\Big{|}\Big{|} \mathcal{O}(\mathcal{T}(\bm{I}_{j};\bm{S}_{j});\theta)-\bm{G}_{j}\Big{|}\Big{|} _{2}\Big{)}\bigg{\}}.\] (6)

### PrObeD

Our proposed approach comprises of three stages: template generation, template recovery, and detector fine-tuning. First, we use an encoder network to generate an image-dependent template for image encryption. This encrypted image is further used to recover the template through a decoder network. Finally, the object detector is fine-tuned using the encrypted images. All three stages are trained in an end-to-end fashion. While all the stages are used for training PrObeD, we specifically use only stages \(1\) and \(3\) for inference. We will now describe each stage in detail.

#### 3.3.1 Proactive Wrapper

Our proposed approach consists of three stages, as shown in Fig. 2. However, only the first two stages are part of our proposed proactive wrapper, which can be applied to object detector to improve its performance.

**Stage 1: Template Generation.** Prior works learn a set of templates [1; 2] in their proactive schemes. This set of templates is enough to perform the respective downstream tasks as the generative model manipulates the template, which is easy to capture with a set of learnable templates. However, for object detection tasks, every image has unique object characteristics such as size, appearance, and color that can vary significantly. This variability present in the images may exceed the descriptive capacity of a finite set of templates, thereby necessitating the use of image-specific templates to

Figure 2: **Overview of PrObeD. PrObeD consists of three stages: (1) template generation, (2) template recovery, and (3) detector fine-tuning. The templates are generated by encoder network \(\mathcal{E}\) to encrypt the input images. The decoder network \(\mathcal{D}\) is used to recover the template from the encrypted images. Finally, the encrypted images are used to fine-tune the object detector to perform detection. We train all the stages in an end-to-end manner. However, for inference, we only use stages \(1\) and \(3\). Best viewed in color.**

accurately represent the range of object features present in each image. In other words, a fixed set of templates may not be sufficiently flexible to capture the diversity of visual features across the given set of input images, thus demanding more adaptable, image-dependent templates.

Motivated by the above argument, we propose to generate the template \(\bm{S}_{j}\) for every image using an encoder network. We hypothesize that highlighting the area of the key foreground objects would be beneficial for object detection. Therefore, for GOD, we use the ground-truth bounding boxes \(T^{G}\) to generate the pseudo ground-truth segmentation map. Specifically, for any image \(\bm{I}_{j}\), if the bounding box coordinates are \(T^{G}_{j}=\{x_{1},x_{2},y_{1},y_{2}\}\), we define the pseudo ground-truth segmentation map as:

\[\forall m\in[0,H],n\in[0,W],\text{ we have}\]

\[\bm{G}_{j}(m,n)=1\text{ if }x_{1}\leq m\leq x_{2}\text{ and }y_{1}\leq n\leq y_{2},\text{ otherwise }0\]

However, for COD, the dataset already has the ground-truth segmentation map \(\bm{G}_{j}\), which we use as the supervision for the encoder to output the templates with semantic information of the image to be restricted only in the region of interest for the detector. For both GOD and COD, we minimize the cosine similarity (Cos) between \(\bm{S}_{j}\) and \(\bm{G}_{j}\) as the supervision for the encoder network. The encoder loss \(J_{E}\) is as follows:

\[J_{E}=1-\text{Cos}(\bm{S}_{j},\bm{G}_{j})=1-\text{Cos}(\mathcal{E}(\bm{I}_{j}),\bm{G}_{j}).\] (7)

This generated template acts as a mask for the input image to highlight the object region of interest for the detector. We use this template with the transformation \(\mathcal{T}\) to encrypt the input image as \(\mathcal{T}(\bm{I}_{j};\bm{S}_{j})=\bm{I}_{j}\odot\bm{S}_{j}\). As we start from the pretrained model of object detector \(\mathcal{O}\), we initialize the bias of the last layer of the encoder as 0 so that for the first few iterations, \(\bm{S}_{j}\approx\bm{1}\). This is to ensure that the distribution of \(\bm{I}_{j}\) and \(\mathcal{T}(\bm{I}_{j};\bm{S}_{j})\) remains similar for the first few iterations, and \(\mathcal{O}\) doesn't encounter a sudden change in its input distribution.

Stage 2: Template Recovery.So far, we have discussed the generation of template \(\bm{S}_{j}\) using \(\mathcal{E}\), which will be used as a mask to encrypt the input image. The encrypted images are used for two purposes: (1) recovery of templates and (2) fine-tuning of the object detector. The main intuition of recovering the templates is from the prior works on image steganalysis [55, 56] and proactive schemes [1, 2]. Motivated by these works, we draw the following insight: _"To properly learn the optimal template and embed it onto the input images, it is beneficial to recover the template from encrypted images."_

To perform recovery, we exploit an encoder-decoder approach. Using this approach leverages the strengths of the encoder network \(\mathcal{E}\) for feature extraction, capturing the most useful salient details, and the decoder network \(\mathcal{D}\) for information recovery, allowing for efficient and effective encryption and decryption of the template. We also empirically show that not using the decoder to recover the templates harms the object detection performance.

To supervise \(\mathcal{D}\) in recovering \(\bm{S}_{j}\) from \(\mathcal{T}(\bm{I}_{j};\bm{S}_{j})\), we propose to maximize the cosine similarity between the recovered template, \(\bm{S}^{{}^{\prime}}_{j}\) and \(\bm{S}_{j}\). The decoder loss is as follows:

\[J_{D}=1-\text{Cos}(\bm{S}^{{}^{\prime}}_{j},\bm{S}_{j})=1-\text{Cos}(\mathcal{ D}(\mathcal{T}(\bm{I}_{j};\bm{S}_{j})),\bm{S}_{j}).\] (8)

Stage 3: Detector Fine-tuning.Due to our encryption, the distribution of the images input to the pretrained \(\mathcal{O}\) changes. Thus, we fine-tune \(\mathcal{O}\) on the encrypted images \(\mathcal{T}(\bm{I}_{j};\bm{S})\). As proposed in Theorem 1, given the encrypted images \(\mathcal{T}(\bm{I}_{j};\bm{S})\), we use the pretrained detector \(\mathcal{O}\) with parameters \(\theta\) to arrive at a better local minima. Therefore, the general objective of GOD and COD in Eq. (5) and Eq. (6) change to as follows:

\[\min_{\theta,\theta_{\mathcal{E}},\theta_{\mathcal{D}}}\bigg{\{} \sum_{j}\Big{(}\|\mathcal{O}(\mathcal{T}(\bm{I}_{j};\mathcal{E}(\bm{I}_{j}; \theta_{\mathcal{E}}));\theta,\theta_{\mathcal{D}})_{1}-T_{j}\|_{2}-\sum_{i=1} ^{N}\big{(}C^{i}_{j}\text{.log}(\mathcal{O}(\mathcal{T}(\bm{I}_{j};\mathcal{E}( \bm{I}_{j};\theta_{\mathcal{E}}));\theta,\theta_{\mathcal{D}})_{2}))\Big{)} \bigg{\}},\] (9) \[\min_{\theta,\theta_{\mathcal{E}},\theta_{\mathcal{D}}}\bigg{\{} \sum_{j}\Big{(}\Big{\|}\mathcal{O}(\mathcal{T}(\bm{I}_{j};\mathcal{E}(\bm{I}_{j} ;\theta_{\mathcal{E}}));\theta,\theta_{\mathcal{D}})-\bm{G}_{j}\Big{\|}_{2} \Big{)}\bigg{\}}.\] (10)

We use the detector-specific loss function \(J_{OBJ}\) of \(\mathcal{O}\) along with the encoder and decoder loss in Eq. (7) and Eq. (8) to train all the three stages. The overall loss function \(J\) to train PrObeD is as follows:

\[J=\lambda_{OBJ}J_{OBJ}+\lambda_{E}J_{E}+\lambda_{D}J_{D}.\] (11)

## 4 Experiments

We apply PrObeD for two categories of object detectors: GOD and COD.

**GOD Baselines.** For GOD, we apply PrObeD on four detectors with varied architectures: two-stage, one-stage, and transformer-based detectors, namely, Faster R-CNN [58], YOLO [52], Sparse R-CNN, and DeTR [8]. We use these works as baselines for three reasons: (1) varied architecture types, (2) their increased prevalence in the community, and (3) varied timelines (from earlier to recent detectors). We use the PyTorch [51] code of the respective detectors for our GOD experiments and use the corresponding GODs as our baseline. For YOLOv5 and DeTR, we use the official repositories released by the authors; for Faster R-CNN, we use the public repository "Faster R-CNN.pytorch". For other GOD detectors, we use Detectron2 library as the pre-trained detector. We use the ResNet101 backbone for Faster R-CNN, Sparse R-CNN and DeTR, and CSPDarknet53 for YOLOv5.

**COD Baselines.** For COD, we apply PrObeD on the current SoTA camouflage detector DGNet [34] and use DGNet as our baseline. For all object detectors, we use the pretrained model released by the authors and fine-tune them with PrObeD. Please see the supplementary for more details.

**Datasets.** Our experiments use the MS-COCO \(2017\)[44] dataset for GOD, while we use CAMO [39], COD\(10\)K [17], and NC\(4\)K [47] datasets for COD. We use the following splits of these datasets:

* MS-COCO \(2017\) Val Split [44]: It includes \(118{,}287\) images for training and \(5K\) for testing.
* COD\(10\)K Val Split [17]: It includes \(4{,}046\) camouflaged images for training and \(2{,}026\) for testing.
* CAMO Val Split [39]: It includes \(1K\) camouflaged images for training and \(250\) for testing.
* NC\(4\)K Val [47]: It includes \(4{,}121\) NC\(4\)K images. We use it for generalization testing as in [34].

**Evaluation Metrics.** We use mean average precision average at multiple thresholds in \([0.5,0.95]\) (AP) for GOD as in [44]. We also report results at threshold of \(0.5\) (AP\({}_{50}\)), threshold of \(0.75\) (AP\({}_{75}\)) and at different object sizes: small (AP\({}_{S}\)), medium (AP\({}_{M}\)), and large (AP\({}_{L}\)). For COD, we use E-measure \(E_{m}\), S-measure \(S_{m}\), weighted F1 score \(wF_{\beta}\) and mean absolute error \(MAE\) as [34].

### GOD Results

**Quantitative Results.** Tab. 2 shows the results of applying PrObeD on GOD networks. PrObeD improves the average precision of all three detectors. The performance gain is significant for Faster R-CNN. As Faster R-CNN is an older detector, it was at a worse minima to start with. PrObeD improves the convergence weight of Faster R-CNN by a significant margin, thereby improving the performance. We further experiment with two variations of Faster R-CNN, namely, Faster R-CNN +

\begin{table}
\begin{tabular}{l|c c c c|c c c} \hline Method & AP \(\uparrow\) & AP\({}_{50}\) \(\uparrow\) & AP\({}_{75}\) \(\uparrow\) & AP\({}_{S}\) \(\uparrow\) & AP\({}_{M}\) \(\uparrow\) & AP\({}_{L}\) \(\uparrow\) \\ \hline Faster R-CNN [58] & \(19.3\) & \(42.5\) & \(16.9\) & \(1.8\) & \(17.9\) & \(39.3\) \\ Faster R-CNN [58]\(+\)PrObeD & \(\mathbf{31.7}\) & \(\mathbf{52.6}\) & \(\mathbf{33.3}\) & \(\mathbf{11.0}\) & \(\mathbf{35.5}\) & \(\mathbf{51.1}\) \\ \hline Faster R-CNN \(+\) FPN [42] & \(37.3\) & \(58.0\) & \(40.6\) & \(21.4\) & \(41.0\) & \(48.4\) \\ Faster R-CNN \(+\) FPN [42] \(+\) Seg. Mask [30] & \(38.2\) & \(60.3\) & \(41.7\) & \(22.1\) & \(43.2\) & \(\mathbf{51.2}\) \\ Faster R-CNN \(+\) FPN [42] \(+\) PrObeD & \(\mathbf{38.5}\) & \(\mathbf{60.4}\) & \(\mathbf{41.9}\) & \(\mathbf{22.5}\) & \(\mathbf{43.4}\) & \(49.8\) \\ \hline Sparse R-CNN [62] & \(37.6\) & \(55.6\) & \(40.2\) & \(20.5\) & \(39.6\) & \(52.9\) \\ Sparse R-CNN [62]\(+\) PrObeD & \(\mathbf{39.2}\) & \(\mathbf{57.5}\) & \(\mathbf{41.5}\) & \(\mathbf{21.7}\) & \(\mathbf{40.1}\) & \(\mathbf{53.6}\) \\ \hline YOLOv5 [52] & \(48.9\) & \(67.6\) & \(53.1\) & \(31.8\) & \(54.4\) & \(62.3\) \\ YOLOv5 [52]\(+\) PrObeD & \(\mathbf{49.4}\) & \(\mathbf{67.9}\) & \(\mathbf{53.5}\) & \(\mathbf{32.0}\) & \(\mathbf{55.1}\) & \(\mathbf{62.6}\) \\ \hline DeTR [8] & \(41.9\) & \(62.3\) & \(44.1\) & \(20.3\) & \(45.8\) & \(61.0\) \\ DeTR [8]\(+\) PrObeD & \(\mathbf{42.1}\) & \(\mathbf{62.6}\) & \(\mathbf{44.4}\) & \(\mathbf{20.4}\) & \(\mathbf{46.0}\) & \(\mathbf{61.3}\) \\ \hline \end{tabular}
\end{table}
Table 2: GOD results on MS-COCO val split. PrObeD improves the performance of all GOD at all thresholds and across all categories.

\begin{table}
\begin{tabular}{l|c c c|c c c c|c c c} \hline \multirow{2}{*}{Method} & \multicolumn{4}{c|}{CAMO} & \multicolumn{4}{c|}{COD\(10\)K} & \multicolumn{4}{c}{NC\(4\)K} \\ \cline{2-13}  & E\({}_{m}\)\(\uparrow\) & S\({}_{m}\)\(\uparrow\) & wF\({}_{\beta}\)\(\uparrow\) & MAE\({}_{L}\)\(\downarrow\) & E\({}_{m}\)\(\uparrow\) & S\({}_{m}\)\(\uparrow\) & wF\({}_{\beta}\)\(\uparrow\) & MAE\({}_{L}\)\(\downarrow\) & E\({}_{m}\)\(\uparrow\) & S\({}_{m}\)\(\uparrow\) & wF\({}_{\beta}\)\(\uparrow\) & MAE\({}_{L}\)\(\downarrow\) \\ \hline DGNet [34] & \(0.859\) & \(0.791\) & \(0.681\) & \(0.079\) & \(0.833\) & \(0.776\) & \(0.603\) & \(0.046\) & \(0.876\) & \(0.815\) & \(0.710\) & \(0.059\) \\ \(\pm\) PrObeD & \(\mathbf{0.871}\) & \(\mathbf{0.797}\) & \(\mathbf{0.702}\) & \(\mathbf{0.071}\) & \(\mathbf{0.869}\) & \(\mathbf{0.803}\) & \(\mathbf{0.661}\) & \(\mathbf{0.037}\) & \(\mathbf{0.900}\) & \(\mathbf{0.838}\) & \(\mathbf{0.755}\) & \(\mathbf{0.049}\) \\ \hline \end{tabular}
\end{table}
Table 3: COD results on CAMO, COD\(10\)K and NC\(4\)K datasets. PrObeD outperforms DGNet on all datasets and metrics.

FPN and Sparse-RCNN. We observe an increase in the performance of both detectors. PrObeD also improves newer detectors like YOLOv5 and DeTR, although the gains are smaller compared to Faster R-CNN. We believe this happens because the newer detectors leave little room for improvement due to which PrObeD improves the performance slightly. We next compare PrObeD with a work that leverage segmentation map as a mask for object detection. We compare our performance with Mask R-CNN [30], which uses an image segmentation branch to help with object detection. Tab. 2 shows that the gains using Mask R-CNN are lower than using our proactive wrapper.

**Qualitative Results.** Fig. 3 shows qualitative results for the MS-COCO \(2017\) dataset. PrObeD clearly improves the performance of pretrained Faster R-CNN for three types of errors: Missed predictions, false negatives, and localization errors. PrObeD has a lower number of missed predictions, fewer false positives, and better bounding box localization. We also visualize the generated and recovered templates. We see that the template has object semantics of the input images. When the template is multiplied with the input image, it highlights the foreground objects, thereby making the task of object detector easier.

**Error Analysis.** We show the error analysis [6] for GOD section \(4\) of the supplementary. We observe that all GOD detectors make mistakes mainly due to five types of errors: classification, localization, duplicate detection, background detection, and missed detection. The main reason for the degraded performance is the errors in which the foreground-background boundary is missed. These errors include localization, background detection, and missed detection. Our proactive wrapper significantly corrects these errors, as the template has object semantics, which, when multiplied with the input image, highlights the foreground objects, consequently simplifying the task of object detection.

Figure 4: **Qualitative COD Results** on CAMO, COD10K, and NC4K datasets from top to bottom, after applying PrObeD. (a) input images, (b) ground-truth camouflaged map, (c) DGNet [34] predictions, (d) DGNet [34]+ PrObeD predictions, (e) generated PrObeD template, and (f) recovered PrObeD template. PrObeD template has the semantics of the camouflaged object, which aids DGNet in detection.

Figure 3: **Qualitative GOD Results** on MS-COCO \(2017\) dataset. (a) ground-truth annotations, (b) Faster R-CNN [58] predictions, (c) Faster R-CNN [58]+ PrObeD predictions, (d) generated template, and (e) recovered template. We highlight the objects responsible for improvement in (c) as compared to (b). The yellow box represents better localization, the blue box represents false positives, and the red box represents missed predictions. PrObeD improves on all these errors made by (b).

### COD Results

**Quantitative Results.** Tab. 3 shows the result of applying PrObeD to DGNet [34] on three different datasets. PrObeD, when applied on top of DGNet, outperforms DGNet on all four metrics for all datasets. The biggest gain appears in COD\(10\)K and NC\(4\)K datasets. This is impressive as these datasets have more diverse testing images than CAMO. As NC\(4\)K is only a testing set, the higher performance of PrObeD demonstrates its superior generalizability as compared to DGNet [34]. This result agrees with the observation in [1; 2], where proactive-based approaches exhibit improved generalization on manipulation detection and localization tasks.

**Qualitative Results.** Fig. 4 visualizes the predicted camouflaged map for DGNet before and after applying PrObeD on testing samples of all three datasets. PrObeD improves the predicted camouflaged map, with less blurriness along the boundaries and better localization of the camouflaged object. As observed before for GOD, the generated and recovered template has the semantics of the camouflaged objects, which after multiplication intensifies the foreground object, resulting in better segmentation by DGNet.

### Ablation Study

**Comparison with Proactive Works.** The prior proactive works perform a different task of image manipulation detection and localization. Therefore, these works are not directly comparable to our proposed proactive wrapper, which performs a different task of object detection as described in Tab. 1. However, manipulation localization and COD both involve a prediction of a localization map, segmentation, and fakeness map, respectively. This inspires us to experiment with MaLP [2] for the task of COD. We train the localization module of MaLP supervised with the COD datasets. The results are shown in Tab. 4. We see that MaLP is not able to perform well for all three datasets. MaLP is designed for estimating universal templates rather than templates tailored to specific images. It shows the significance of image-specific templates in object detection. While MaLP's design with image-independent templates is effective for localizing image manipulation, applying it to object detection has a negative impact on performance.

**Framework Design.** PrObeD consists of blocks to improve the object detector. Tab. 5 ablates different versions of PrObeD to highlight the importance of each block in our design. PrObeD utilizes an encoder network \(\mathcal{E}\) to learn image-dependent templates aiding the detector. We remove the encoder \(\mathcal{E}\) from our network, replacing it with a fixed template. We observe that the performance deteriorates by a large margin. Next, we make this template learnable as proposed in PrObeD, but only a single template would be used for all the input images. This choice also results in worse performance, highlighting that image-dependent templates are necessary for object detection. Finally, we remove the decoder network \(\mathcal{D}\), which is used to recover the template from the encrypted images. Although this results in a better performance than the pretrained Faster R-CNN, we observe a drop as compared to PrObeD. Therefore, as discussed in Sec. 3.3, the recovery of templates is indeed a necessary and beneficial step for boosting the performance of the proactive schemes.

\begin{table}
\begin{tabular}{l|l l l|l l l|l l l l} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c|}{CAMO} & \multicolumn{4}{c|}{COD\(10\)K} & \multicolumn{4}{c}{NC\(4\)K} \\ \cline{2-11}  & \(\text{E}_{m}\uparrow\) & \(\text{S}_{m}\uparrow\) & \(\text{wF}_{\beta}\uparrow\) & MAE\(\downarrow\) & \(\text{E}_{m}\uparrow\) & \(\text{S}_{m}\uparrow\) & \(\text{wF}_{\beta}\uparrow\) & MAE\(\downarrow\) & \(\text{E}_{m}\uparrow\) & \(\text{S}_{m}\uparrow\) & \(\text{wF}_{\beta}\uparrow\) & MAE\(\downarrow\) \\ \hline MaLP [2] & \(0.474\) & \(0.514\) & \(0.218\) & \(0.254\) & \(0.491\) & \(0.520\) & \(0.150\) & \(0.202\) & \(0.503\) & \(0.548\) & \(0.228\) & \(0.222\) \\ PrObeD & \(\mathbf{0.871}\) & \(\mathbf{0.797}\) & \(\mathbf{0.702}\) & \(\mathbf{0.071}\) & \(\mathbf{0.869}\) & \(\mathbf{0.803}\) & \(\mathbf{0.661}\) & \(\mathbf{0.037}\) & \(\mathbf{0.900}\) & \(\mathbf{0.838}\) & \(\mathbf{0.755}\) & \(\mathbf{0.049}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance comparison with proactive works. MaLP [2] has a significantly deteriorated performance than PrObeD.

\begin{table}
\begin{tabular}{l|l|l l l|l l l l} \hline \hline \multicolumn{1}{c|}{Changed} & \multicolumn{1}{c|}{From\(\blackminus\)To} & \multicolumn{1}{c|}{AP} & \(\uparrow\) & \(\text{AP}_{50}\uparrow\) & \(\text{AP}_{75}\uparrow\) & \(\text{AP}_{S}\uparrow\) & \(\text{AP}_{M}\uparrow\) & \(\text{AP}_{L}\uparrow\) \\ \hline \multirow{2}{*}{Template} & \multicolumn{1}{c|}{Image Dependent\(\blackminus\)Fixed} & \(17.6\) & \(37.9\) & \(15.1\) & \(1.3\) & \(15.4\) & \(39.5\) \\  & \multicolumn{1}{c|}{Image Dependent\(\blackminus\)Universal} & \(19.4\) & \(42.6\) & \(17.1\) & \(1.9\) & \(18.0\) & \(39.4\) \\ \hline Decoder & Yes\(\blackminus\)No & \(25.2\) & \(46.1\) & \(26.2\) & \(5.3\) & \(26.6\) & \(24.1\) \\ \hline Transformation & Multiply–\(\blackminus\)Add & \(19.2\) & \(42.3\) & \(20.1\) & \(1.7\) & \(17.9\) & \(39.1\) \\ \hline PrObeD & - & & \(\mathbf{31.7}\) & \(\mathbf{52.6}\) & \(\mathbf{33.3}\) & \(\mathbf{11.0}\) & \(\mathbf{35.5}\) & \(\mathbf{51.1}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Ablation studies** of PrObeD using Faster R-CNN GOD on MS-COCO \(2017\) dataset. Removing the encoder/decoder network or adding the template results in degraded performance.

**Encryption Process.** PrObeD includes an encryption process as described in Eq. (3), which involves multiplying the template with the input image. This process makes the template act as a mask, highlighting the foreground for better detection. However, prior proactive works [1, 2] consider adding templates to achieve better results. Thus, we ablate by changing the encryption process to template addition. Tab. 5 shows that template addition degrades performance by a significant margin w.r.t. our multiplication scheme. This shows that encryption is a key step in formulating proactive schemes, and the same encryption process may not work for all tasks.

**More Training Time.** We perform an ablation to show that the performance gain of the detector is due to our proactive wrapper instead of training for more iterations of the pretrained object detector. Results in Tab. 6 show that although more training iterations for the detector has a performance gain, it's not enough to get the significant margin in performance as achieved by PrObeD. This shows that extra training can help, but only up to a certain extent.

**Inference Time.** We evaluate the overhead computational cost after applying PrObeD on different object detectors are shown in Tab. 6, averaged across \(1,000\) images, on a NVIDIA \(V100\) GPU. Our encoder network has \(17\) layers, which adds extra cost for inference. For detectors with bulky architectures like Faster R-CNN (ResNet101) and DeTR (transformer), the overhead computational cost is quite small, \(8.7\%\) and \(7.2\%\), respectively. This additional cost is minor compared to the performance gain of detectors, especially Faster R-CNN. For a lighter detector like YOLOv5, our overhead computational cost increases to \(29.1\%\). So, there is a trade-off of applying PrObeD to different detectors with varied architectures. PrObeD is more beneficial to bulky detectors like two-staged/transformer-based as compared to one-stage detectors.

## 5 Conclusion

We mathematically prove that the proactive method results in a better-converged model than the passive detector under assumptions and, consequently, a better 2D object detector. Based on this finding, we propose a proactive scheme wrapper, PrObeD, which enhances the performance of camouflaged and generic object detectors. The wrapper outputs an image-dependent template using an encoder network, which encrypts the input images. These encrypted images are then used to fine-tune the object detector. Extensive experiments on MS-COCO, CAMO, COD\(10\)K, and NC\(4\)K datasets show that PrObeD improves the overall object detection performance for both GOD and COD detectors.

**Limitations.** Our proposed scheme has the following limitations. First, PrObeD does not provide a significant gain for recent object detectors such as YOLO and DeTR. Second, the proactive wrapper should be thoroughly tested on other object detectors to show the generalizability of PrObeD. Finally, we only experiment with simple multiplication and addition as the encryption scheme. A more sophisticated encryption process might further improve the object detectors' performance. We leave these for our future avenues.

\begin{table}
\begin{tabular}{l|c|c c|c c c|c} \hline \hline Method & Iterations & AP \(\uparrow\) & AP\({}_{50}\uparrow\) & AP\({}_{75}\uparrow\) & AP\({}_{S}\uparrow\) & AP\({}_{M}\uparrow\) & AP\({}_{L}\uparrow\) & Time (\(ms\)) \\ \hline Faster R-CNN [58] & \(1\times\) & \(19.3\) & \(42.5\) & \(16.9\) & \(1.8\) & \(17.9\) & \(39.3\) & \(161.1\) \\ Faster R-CNN [58] & \(2\times\) & \(20.1\) & \(46.6\) & \(21.5\) & \(3.3\) & \(20.3\) & \(41.2\) & \(175.3\) (\(\uparrow\)\(8.7\%\)) \\ Faster R-CNN [58]\(+\)PrObeD & \(2\times\) & \(\mathbf{31.7}\) & \(\mathbf{52.6}\) & \(\mathbf{33.3}\) & \(\mathbf{11.0}\) & \(\mathbf{35.5}\) & \(\mathbf{51.1}\) & \(175.3\) (\(\uparrow\)\(8.7\%\)) \\ \hline YOLOv5 [52] & \(1\times\) & \(48.9\) & \(67.6\) & \(53.1\) & \(31.8\) & \(54.4\) & \(62.3\) & \(48.5\) \\ YOLOv5 [52] & \(2\times\) & \(48.8\) & \(67.7\) & \(53.0\) & \(31.8\) & \(54.7\) & \(62.4\) & \(48.5\) \\ YOLOv5 [52]\(+\)PrObeD & \(2\times\) & \(\mathbf{49.4}\) & \(\mathbf{67.9}\) & \(\mathbf{53.5}\) & \(\mathbf{32.0}\) & \(\mathbf{55.1}\) & \(\mathbf{62.6}\) & \(2.7\) (\(\uparrow\)\(29.1\%\)) \\ \hline DeTR [8] & \(1\times\) & \(41.9\) & \(62.3\) & \(44.1\) & \(20.3\) & \(45.8\) & \(61.0\) & \(194.2\) \\ DeTR [8] & \(2\times\) & \(41.9\) & \(62.4\) & \(44.0\) & \(20.1\) & \(45.9\) & \(61.1\) & \(194.2\) \\ DeTR [8]\(+\)PrObeD & \(2\times\) & \(\mathbf{42.1}\) & \(\mathbf{62.6}\) & \(\mathbf{44.4}\) & \(\mathbf{20.4}\) & \(\mathbf{46.0}\) & \(\mathbf{61.3}\) & \(208.4\) (\(\uparrow\)\(7.2\%\)) \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Ablation of training iterations** on Faster R-CNN. YOLOv5, and DeTR for more iterations similar to after applying PrObeD. We also report the inference time for all the detectors before and after applying PrObeD. Training object detectors proactively with PrObeD results in more performance gain compared to training passively for more iterations. PrObeD adds an overhead cost on top of the inference cost of detectors.

## References

* [1]V. Asnani, X. Yin, T. Hassner, S. Liu, and X. Liu (2022) Proactive image manipulation detection. In CVPR, Cited by: SS1, SS2.
* [2]V. Asnani, X. Yin, T. Hassner, and X. Liu (2023) MaLP: manipulation localization using a proactive scheme. In CVPR, Cited by: SS1, SS2.
* [3]V. Asnani, X. Yin, T. Hassner, and X. Liu (2023) Reverse engineering of generative models: inferring model hyperparameters from generated images. TPAMI. Cited by: SS1, SS2.
* [4]Y. Atoum, J. Roth, M. Bliss, W. Zhang, and X. Liu (2017) Monocular video-based trailer coupler detection using multiplexer convolutional neural network. In ICCV, Cited by: SS1, SS2.
* [5]A. Bochkovskiy, C. Wang, and H. Liao (2020) YOLOv4: optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934. Cited by: SS1, SS2.
* [6]D. Bolya, S. Foley, J. Hays, and J. Hoffman (2020) TIDE: a general toolbox for identifying object detection errors. In ECCV, Cited by: SS1, SS2.
* [7]G. Brazil and X. Liu (2019) Pedestrian detection with autoregressive network phases. In CVPR, Cited by: SS1, SS2.
* [8]N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko (2020) End-to-end object detection with transformers. In ECCV, Cited by: SS1, SS2.
* [9]G. Chen, S. Liu, Y. Sun, G. Ji, Y. Wu, and T. Zhou (2022) Camouflaged object detection via context-aware cross-level fusion. IEEE Transactions on Circuits and Systems for Video Technology32 (10). Cited by: SS1, SS2.
* [10]S. Chen, P. Sun, Y. Song, and P. Luo (2023) DiffusionDet: diffusion model for object detection. In CVPR, Cited by: SS1, SS2.
* [11]W. Chen, X. Yu, and L. Ou (2022) Pedestrian attribute recognition in video surveillance scenarios based on view-attribute attention localization. Machine Intelligence Research. Cited by: SS1, SS2.
* [12]X. Cheng, H. Xiong, D. Fan, Y. Zhong, M. Harandi, T. Drummond, and Z. Ge (2022) Implicit motion handling for video camouflaged object detection. In CVPR, Cited by: SS1, SS2.
* [13]P. Chu, Z. Li, K. Lammers, R. Lu, and X. Liu (2021) DeepApple: deep learning-based apple detection using a suppression mask R-CNN. PRL. Cited by: SS1, SS2.
* [14]J. Dai, Y. Li, K. He, and J. Sun (2016) R-FCN: object detection via region-based fully convolutional networks. NeurIPS. Cited by: SS1, SS2.
* [15]N. Dalal and B. Triggs (2005) Histograms of oriented gradients for human detection. In CVPR, Cited by: SS1, SS2.
* [16]M. Derakhshani, S. Masoudnia, A. Shaker, O. Mersa, M. Sadeghi, M. Rastegari, and B. Araabi (2019) Assisted excitation of activations: a learning technique to improve object detectors. In CVPR, Cited by: SS1, SS2.
* [17]D. Fan, G. Ji, M. Cheng, and L. Shao (2021) Concealed object detection. TPAMI. Cited by: SS1, SS2.
* [18]D. Fan, G. Ji, G. Sun, M. Cheng, J. Shen, and L. Shao (2020) Camouflaged object detection. In CVPR, Cited by: SS1, SS2.
* [19]D. Fan, G. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao (2020) Pranet: parallel reverse attention network for polyp segmentation. In MICCAI, Cited by: SS1, SS2.
* [20]P. Felzenszwalb, D. McAllester, and D. Ramanan (2008) A discriminatively trained, multiscale, deformable part model. In CVPR, Cited by: SS1, SS2.
* [21]S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun (2013) Bottom-up segmentation for top-down detection. In CVPR, Cited by: SS1, SS2.
* [22]S. Gidaris and N. Komodakis (2015) Object detection via a multi-region and semantic segmentation-aware cnn model. In ICCV, Cited by: SS1, SS2.
* [23]R. Girshick, J. Donahue, T. Darrell, and J. Malik (2014) Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, Cited by: SS1, SS2.
* [24]R. Girshick, J. Donahue, T. Darrell, and J. Malik (2015) Region-based convolutional networks for accurate object detection and segmentation. TPAMI. Cited by: SS1, SS2.
* [25]R. Girshick, J. Donahue, T. Darrell, and J. Malik (2015) Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, Cited by: SS1, SS2.
* [26]A. Gupta, P. Dollar, and R. Girshick (2019) LVIS: a dataset for large vocabulary instance segmentation. In CVPR, Cited by: SS1, SS2.
* [27]R. Girshick (2019) Fast R-CNN. In ICCV, Cited by: SS1, SS2.
* [28]R. Girshick (2019) FedFelzenszwalb, D. McAllester, and D. Ramanan (2008) A discriminatively trained, multiscale, deformable part model. In CVPR, Cited by: SS1, SS2.
* [29]R. Girshick, J. Donahue, T. Darrell, and J. Malik (2015) Region-based convolutional networks for accurate object detection and segmentation. TPAMI. Cited by: SS1, SS2.
* [30]R. Girshick, J. Donahue, T. Darrell, and J. Malik (2015) Region-based convolutional networks for accurate object detection and segmentation. TPAMI. Cited by: SS1, SS2.
* [31]R. Girshick (2019) Fast R-CNN. In ICCV, Cited by: SS1, SS2.
* [32]R. Girshick (2019) FedFelzenszwalb, D. McAllester, and D. Ramanan (2008) A discriminatively trained, multiscale, deformable part model. In CVPR, Cited by: SS1, SS2.
* [33]R. Girshick, J. Donahue, T. Darrell, and J. Malik (2015) Region-based convolutional networks for accurate object detection and segmentation. TPAMI. Cited by: SS1, SS2.

[MISSING_PAGE_POST]

* [27] Chunming He, Kai Li, Yachao Zhang, Longxiang Tang, Yulun Zhang, Zhenhua Guo, and Xiu Li. Camouflaged object detection with feature decomposition and edge reconstruction. In _CVPR_, 2023.
* [28] Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxiang Tang, Yulun Zhang, Zhenhua Guo, and Xiu Li. Weakly-supervised concealed object segmentation with SAM-based pseudo labeling and multi-scale feature grouping. _arXiv preprint arXiv:2305.11003_, 2023.
* [29] Chunming He, Kai Li, Yachao Zhang, Yulun Zhang, Zhenhua Guo, Xiu Li, Martin Danelljan, and Fisher Yu. Strategic preys make acute predators: Enhancing camouflaged object detectors by generating camouflaged objects. _arXiv preprint arXiv:2308.03166_, 2023.
* [30] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In _ICCV_, 2017.
* [31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. _TPAMI_, 2015.
* [32] Yihui He, Chenchen Zhu, Jianren Wang, Marios Savvides, and Xiangyu Zhang. Bounding box regression with uncertainty for accurate object detection. In _CVPR_, 2019.
* [33] Jianqin Yin Yanbin Han Wendi Hou and Jinping Li. Detection of the mobile object with camouflage color under dynamic background based on optical flow. _Procedia Engineering_, 2011.
* [34] Ge-Peng Ji, Deng-Ping Fan, Yu-Cheng Chou, Dengxin Dai, Alexander Liniger, and Luc Van Gool. Deep gradient learning for efficient camouflaged object detection. _Machine Intelligence Research_, 2023.
* [35] Ge-Peng Ji, Lei Zhu, Mingchen Zhuge, and Keren Fu. Fast camouflaged object detection via edge-based reversible re-calibration network. _Pattern Recognition_, 123, 2022.
* [36] Nobukatsu Kajiura, Hong Liu, and Shin'ichi Satoh. Improving camouflaged object detection with the uncertainty of pseudo-edge labels. In _ACM Multimedia Asia_, 2021.
* [37] Abhinav Kumar, Garrick Brazil, Enrique Corona, Armin Parchami, and Xiaoming Liu. DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection. In _ECCV_, 2022.
* [38] Abhinav Kumar, Garrick Brazil, and Xiaoming Liu. GrooMeD-NMS: Grouped mathematically differentiable nms for monocular 3D object detection. In _CVPR_, 2021.
* [39] Trung-Nghia Le, Tam Nguyen, Zhongliang Nie, Minh-Triet Tran, and Akihiro Sugimoto. Anabranch network for camouflaged object segmentation. _CVIU_, 2019.
* [40] Aixuan Li, Jing Zhang, Yunqiu Lv, Bowen Liu, Tong Zhang, and Yuchao Dai. Uncertainty-aware joint salient object and camouflaged object detection. In _CVPR_, 2021.
* [41] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, and Jian Sun. Light-head R-CNN: In defense of two-stage object detector. _arXiv preprint arXiv:1711.07264_, 2017.
* [42] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In _CVPR_, 2017.
* [43] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In _ICCV_, 2017.
* [44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft COCO: Common objects in context. In _ECCV_, 2014.
* [45] Jiannan Liu, Bo Dong, Shuai Wang, Hui Cui, Deng-Ping Fan, Jiquan Ma, and Geng Chen. Covid-19 lung infection segmentation with a novel two-stage cross-domain transfer learning framework. _Medical image analysis_, 2021.
* [46] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander Berg. SSD: Single shot multibox detector. In _ECCV_, 2016.
* [47] Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu, Nick Barnes, and Deng-Ping Fan. Simultaneously localize, segment and rank the camouflaged objects. In _CVPR_, 2021.
* [48] Yuxin Mao, Jing Zhang, Zhexiong Wan, Yuchao Dai, Aixuan Li, Yunqiu Lv, Xinyu Tian, Deng-Ping Fan, and Nick Barnes. Transformer transforms salient object detection and camouflaged object detection. _arXiv preprint arXiv:2104.10127_, 2021.
* [49] Haiyang Mei, Ge-Peng Ji, Ziqi Wei, Xin Yang, Xiaopeng Wei, and Deng-Ping Fan. Camouflaged object segmentation with distraction mining. In _CVPR_, 2021.
* [50] Yuxin Pan, Yiwang Chen, Qiang Fu, Ping Zhang, and Xin Xu. Study on the camouflaged target detection method based on 3D convexity. _Modern Applied Science_, 2011.

* [51] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In _NeurIPS_, 2019.
* [52] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In _CVPR_, 2016.
* [53] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster, stronger. In _CVPR_, 2017.
* [54] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental improvement. _arXiv preprint arXiv:1804.02767_, 2018.
* [55] Atique Rehman, Rafia Rahim, Shahroz Nadeem, and Sibt Hussain. End-to-end trained CNN encoder-decoder networks for image steganography. In _ECCVW_, 2018.
* [56] Atique-ur Rehman, Rafia Rahim, Shahroz Nadeem, and Sibt-ul Hussain. End-to-end trained CNN encoder-decoder networks for image steganography. In _ECCVW_, 2019.
* [57] Jingjing Ren, Xiaowei Hu, Lei Zhu, Xuemiao Xu, Yangyang Xu, Weiming Wang, Zijun Deng, and Pheng-Ann Heng. Deep texture-aware features for camouflaged object detection. _IEEE Transactions on Circuits and Systems for Video Technology_, 2023.
* [58] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. _NeurIPS_, 2015.
* [59] Nataniel Ruiz, Sarah Adel Bargal, and Stan Sclaroff. Disrupting deepfakes: Adversarial attacks against conditional image translation networks and facial manipulation systems. In _ECCV_, 2020.
* [60] Eran Segalis and Eran Galili. OGAN: Disrupting deepfakes with an adversarial attack that survives training. _arXiv preprint arXiv:2006.12247_, 2020.
* [61] P Sengottuvelan, Amitabh Wahi, and A Shanmugam. Performance of decamouflaging through exploratory image analysis. In _ICETET_, 2008.
* [62] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse R-CNN: End-to-end object detection with learnable proposals. In _CVPR_, 2021.
* [63] Yujia Sun, Geng Chen, Tao Zhou, Yi Zhang, and Nian Liu. Context-aware cross-level fusion network for camouflaged object detection. _arXiv preprint arXiv:2105.12555_, 2021.
* [64] Paul Viola and Michael Jones. Rapid object detection using a boosted cascade of simple features. In _CVPR_, 2001.
* [65] Paul Viola and Michael Jones. Robust real-time face detection. _IJCV_, 2004.
* [66] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Liao. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In _CVPR_, 2023.
* [67] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution representation learning for visual recognition. _TPAMI_, 2020.
* [68] Run Wang, Felix Juefei-Xu, Meng Luo, Yang Liu, and Lina Wang. FakeTagger: Robust safeguards against deepfake dissemination via provenance tracking. In _ACM Multimedia_, 2021.
* [69] Jian-Ru Xue, Jian-Wu Fang, and Pu Zhang. A survey of scene understanding by event reasoning in autonomous driving. _International Journal of Automation and Computing_, 2018.
* [70] Fan Yang, Qiang Zhai, Xin Li, Rui Huang, Ao Luo, Hong Cheng, and Deng-Ping Fan. Uncertainty-guided transformer reasoning for camouflaged object detection. In _ICCV_, 2021.
* [71] Chin-Yuan Yeh, Hsi-Wen Chen, Shang-Lun Tsai, and Sheng-De Wang. Disrupting image-translation-based deepfake algorithms with adversarial attacks. In _WACVW_, 2020.
* [72] Matthew Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In _ECCV_, 2014.
* [73] Qiang Zhai, Xin Li, Fan Yang, Chenglizhao Chen, Hong Cheng, and Deng-Ping Fan. Mutual graph learning for camouflaged object detection. In _CVPR_, 2021.
* [74] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. In _ICLR_, 2020.
* [75] Mingchen Zhuge, Xiankai Lu, Yiyou Guo, Zhihua Cai, and Shuhan Chen. CubeNet: X-shape connection for camouflaged object detection. _Pattern Recognition_, 2022.