# Efficiency for Free: Ideal Data Are Transportable Representations

 Peng Sun\({}^{1,2}\) Yi Jiang\({}^{1}\) Tao Lin\({}^{2,}\)

\({}^{1}\)Zhejiang University

\({}^{2}\)Westlake University

sunpeng@westlake.edu.cn, yi_jiang@zju.edu.cn, lintao@westlake.edu.cn

Corresponding author.

###### Abstract

Data, the seminal opportunity and challenge in modern machine learning, currently constrains the scalability of representation learning and impedes the pace of model evolution. In this work, we investigate the efficiency properties of data from both optimization and generalization perspectives. Our theoretical and empirical analysis reveals an unexpected finding: for a given task, utilizing a publicly available, task- and architecture-agnostic model (referred to as the 'prior model' in this paper) can effectively produce efficient data. Building on this insight, we propose the Representation Learning Accelerator (ReLA), which promotes the formation and utilization of efficient data, thereby accelerating representation learning. Utilizing a ResNet-18 pre-trained on CIFAR-10 as a prior model to inform ResNet-50 training on ImageNet-1K reduces computational costs by \(50\%\) while maintaining the same accuracy as the model trained with the original BYOL, which requires \(100\%\) cost. Our code is available at: https://github.com/LINs-lab/ReLA.

## 1 Introduction

The available of massive datasets [20, 49] and recent advances in parallel data processing [28, 42] have facilitated the rapid evolution of large deep models, such as GPT-4 [1] and LVM [2]. These

Figure 1: **Framework and Intuition of ReLA**: (1) _Framework_: ReLA serves as both a data optimizer and an auxiliary accelerator. Initially, it operates as a data optimizer by leveraging an dataset and a pre-trained model (e.g., one sourced from online repositories) to generate an efficient dataset. Subsequently, ReLA functions as an auxiliary accelerator, enhancing existing (self-)supervised learning algorithms through the effective utilization of the efficient dataset, thereby promoting efficient representation learning. (2) _Intuition_: The central concept of ReLA is to create an efficient-data-driven shortcut pathway within the learning process, enabling the initial model \(\bm{\phi}\) to rapidly converge towards a ’proximal representation \(\bm{\psi}\)’ of the target model \(\bm{\phi}^{*}\) during the early stages of training. This approach significantly accelerates the overall learning process.

models excel in numerous learning tasks, attributable to their impressive representation capabilities. However, the emergence of vast amounts of data within the modern deep learning paradigm raises two fundamental challenges: (i) _the demand for human annotations of huge datasets consumes significant social resources [45, 20, 43];_ (ii) _training large models with increasing data and model capacity suffers from intensive computational burden [6, 16, 55]._

The community has made considerable efforts to enhance learning efficiency. Self-supervised learning methods [12, 69, 30, 10, 25, 14, 9, 3], with their superior representation learning devoid of human annotations via the self-learning paradigm, attempt to tackle the challenge (i). Concurrently, research has been conducted to mitigate data efficiency issues in challenge (ii): dataset distillation approaches [65, 56, 11, 72, 53, 54] have successfully synthesized a small distilled dataset, on which models trained on this compact dataset can akin to one trained on the full dataset.

However, challenges (i) and (ii) persist and yet are far from being solved [43, 45, 6, 16, 55], particularly the intervention of these two learning paradigms. In this paper, we identify two issues: (a) inefficiency in the self-supervised learning procedure compared to conventional supervised learning arises due to sub-optimal self-generating targets [30, 62]; (b) although training on the distilled dataset is efficient and effective, the distillation process of optimization-based approaches [11, 72, 37] is computationally demanding [18, 56], often surpassing the computational load of training on the full dataset. This limitation restricts its potential to accelerate representation learning. To tackle these challenges, we propose a novel open problem in the domain of representation learning:

**Problem 1 (Accelerating Representation Learning through Free Models)**.: _According to the No Free Lunch theorem [66], it is evident that accelerating the learning process without incorporating prior knowledge is inherently challenging. Fortunately, numerous publicly available pre-trained models can be accessed online, offering a form of free prior knowledge. Despite this, effectively utilizing these models poses several implicit challenges, as these models may not be directly relevant to our target learning task, or they may not be sufficiently well-trained. This leads to a question:_

How can we leverage task- and architecture-agnostic publicly available models to accelerate representation learning for a specific task?

To address Problem 1, we propose ReLA to utilize a freely available model downloaded from the internet to generate efficient data for training. This approach aims to accelerate training during the initial stages by effectively leveraging these generated data, thereby establishing a rapid pathway for representation learning (see Figure 1). Specifically, we list our **five key contributions below** as the first step toward bridging representation learning with data-efficient learning:

(a) _Revealing beneficial/detrimental data properties for efficient/inefficient (self-)supervised learning (see Section 3.2)._ We present a comprehensive analysis of linear models, demonstrating that data properties significantly influence the learning process by impacting the optimization of model training. Our findings reveal that modifications to the data can markedly enhance or impair this optimization. Additionally, we indicate that optimal training necessitates specific data properties--perfect bijective mappings between the samples and targets within a dataset.

(b) _Identifying the inefficiency problems of (self-)supervised learning from a data-centric perspective (see Section 3.3)._ Specifically, we identify several factors contributing to the inefficiencies in (self-)supervised learning over real-world data. For instance, prevalent data augmentation techniques in modern deep learning can introduce a 'noisy mapping' issue, which may exacerbate the negative effects associated with inefficient data.

(c) _Generalization bound for models trained on optimized efficient data (see Section 3.4)._ Although the efficiency properties of data do not inherently ensure the generalization of the trained model, i.e., efficient data alone cannot guarantee generalization ability, we present a generalization bound to analyze models trained on such data.

(d) _A novel method_ ReLA _to generate and exploit efficient data (see Section 4)._ Leveraging our theoretical insights regarding the bounds of generalization and convergence rate, we introduce ReLA, a novel optimization-free method tailored to efficiently generate and effectively exploit efficient data for accelerating representation learning.

(e) _An application of our_ ReLA_: accelerating (self-)supervised learning (see Section 5 and Appendix 1)._ Extensive experiments across four widely-used datasets, seven neural network ar chitectures, eight self-supervised learning algorithms demonstrate the effectiveness and efficiency of ReLA. Training models with ReLA significantly outperforms training on the original dataset with the same budget, and even exceeds the performance of training on higher budget.

## 2 Related Work

This section integrates two distinct deep learning areas: (a) techniques to condense datasets while preserving efficacy; (b) self-supervised learning methods that enable training models on unlabeled data.

### Dataset Distillation: Efficient yet Effective Learning Using Fewer Data

The objective of dataset distillation is to create a significantly smaller dataset that retains competitive performance relative to the original dataset.

**Refining proxy metrics between original and distilled datasets.** Traditional approaches involve replicating the behaviors of the original dataset within the distilled one. These methods aim to minimize discrepancies between surrogate neural network models trained on both synthetic and original datasets. Key metrics for this process include matching gradients [72; 32; 70; 44], features [63], distributions [71; 73], and training trajectories [11; 17; 22; 18; 68; 24]. However, these methods suffer from substantial computational overhead due to the incesant calculation of discrepancies between the distilled and original datasets. The optimization of the distilled dataset involves minimizing these discrepancies, necessitating multiple iterations until convergence. As a result, scaling to large datasets, such as ImageNet [20], becomes challenging.

**Extracting key information from original into distilled datasets.** A promising strategy involves identifying metrics that capture essential dataset information. These methods efficiently scale to large datasets like ImageNet-1K using robust backbones without necessitating multiple comparisons between original and distilled datasets. For instance, SRe\({}^{2}\)L [67] condenses the entire dataset into a model, such as pre-trained neural networks like ResNet-18 [26], and then extracts the knowledge from these models into images and targets, forming a distilled dataset. Recently, RDED [56] posits that images accurately recognized by strong observers, such as humans and pre-trained models, are more critical for learning.

**Summary.** We make the following observations regarding scalable dataset distillation methods utilizing various metrics: (a) a few of these metrics have proven effective for data distillation at the scale of ImageNet. (b) all these metrics require human-labeled data; (c) there is currently no established theory elucidating the conditions under which data distillation is feasible; (d) despite their success, the theory behind training neural networks with reduced data is underexplored.

### Self-supervised Learning: Representation Learning Using Unlabeled Data

The primary objective of self-supervised learning is to extract robust representations without relying on human-labeled data. These representations should be competitive with those derived from supervised learning and deliver superior performance across multiple tasks.

**Contrasting self-generated positive and negative Samples.** Contrastive learning-based methods implicitly assign a one-hot label to each sample and its augmented versions to facilitate discrimination. Since InfoNCE [47], various works [25; 12; 13; 8; 31; 15; 74; 40; 9; 27] have advanced contrastive learning. MoCo [25; 13; 15] uses a momentum encoder for consistent negatives, effective for both CNNs and Vision Transformers. SimCLR [12] employs strong augmentations and a nonlinear projection head. Other methods integrate instance classification [8], data augmentation [31; 74], clustering [40; 9], and adversarial training [27]. These enhance alignment and uniformity of representations on the hypersphere [64].

**Asymmetric model-generating representations as targets.** Asymmetric network methods achieve self-supervised learning with only positive pairs [30; 50; 14], avoiding representational collapse through asymmetric architectures. BYOL [30] uses a predictor network and a momentum encoder. Richemond et al. [50] show BYOL performs well without batch statistics. SimSiam [14] halts the gradient to the target branch, mimicking the momentum encoder's effect. DINO [10] employs a self-distillation loss. UniGrad [59] integrates asymmetric networks with contrastive learning methods within a theoretically unified framework.

## 3 Revealing Critical Properties of Efficient Learning over Data

We begin by presenting formal definitions of supervised learning over a (efficient) dataset.

**Definition 1** (Supervised learning over data): _For a dataset \(D=(D_{X},D_{Y})=\{(\mathbf{x}_{i},\mathbf{y}_{i})\}_{i=1}^{|D|}\), drawn from the data distribution \((X,Y)\) in space \((\mathcal{X},\mathcal{Y})\), the goal of a model learning algorithm is to identify an optimal model \(\boldsymbol{\phi}^{\star}\) that minimizes the expected error defined by:_

\[\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim(X,Y)}\left[\ell(\boldsymbol{\phi}^{ \star}(\mathbf{x}),\mathbf{y})\right]\leq\epsilon\,,\] (1)

_where \(\ell\) indicates the loss function and \(\epsilon\) denotes a predetermined deviation. This is typically achieved through a parameterized model \(\boldsymbol{\phi}_{\boldsymbol{\theta}}\), where \(\boldsymbol{\theta}\) denotes the model parameter within the parameter space \(\boldsymbol{\Theta}\). The optimal parameter \(\boldsymbol{\theta}^{D}\) is determined by training the model to minimize the empirical loss over the dataset:_

\[\boldsymbol{\theta}^{D}:=\arg\min\nolimits_{\boldsymbol{\theta}\in\boldsymbol {\Theta}}\left\{\mathcal{L}(\boldsymbol{\phi}_{\boldsymbol{\theta}};D;\ell) \right\}:=\arg\min\nolimits_{\boldsymbol{\theta}\in\boldsymbol{\Theta}} \left\{\sum_{i=1}^{|D|}\ell(\boldsymbol{\phi}_{\boldsymbol{\theta}}(\mathbf{x }_{i}),\mathbf{y}_{i})\right\}\,.\] (2)

_The training process leverages an optimization algorithm such as stochastic gradient descent [51; 33]._

**Definition 2** (Data-efficient Learning): _Data-efficient learning seeks to derive an optimized/efficient dataset, denoted as \(S=(S_{X},S_{Y})=\{(\mathbf{x}_{j},\mathbf{y}_{j})\}_{j=1}^{|S|}\), from the original dataset \(D\). The objective is to enable models \(\boldsymbol{\phi}_{\boldsymbol{\theta}^{S}}\) trained on \(S\) to achieve the desired generalization performance, as defined in (1), with fewer training steps and a reduced computational budget compared to training on the original dataset \(D\)._

### Unifying (Self-)Supervised Learning from a Data-Centric Perspective

To ease the understanding and our methodology design in Section3.4, we unify both conventional supervised learning and self-supervised learning as learning to map samples in \(D_{X}\) to targets in \(D_{Y}\): this view forms'supervised learning' from a data-centric perspective. Specifically, these two learning paradigms involve generating targets \(D_{Y}=\{\mathbf{y}\mid\mathbf{y}=\boldsymbol{\psi}(\mathbf{x})\text{ s.t. }\mathbf{x}\sim D_{X}\}\) and minimizing the empirical loss (2). The only difference lies in the target generation models (or simply labelers) \(\boldsymbol{\psi}\):

1. _Conventional supervised learning_, referred to as human-supervised learning, generates targets via human annotation. Note that the targets are stored and used statically throughout the training.
2. _Self-supervised learning_ (also see Footnote 2), e.g., BYOL [30] utilizes an Exponential Moving Average (EMA) version of the learning model \(\boldsymbol{\phi}_{\boldsymbol{\theta}}\) to generate targets \(\mathbf{y}=\text{EMA}[\boldsymbol{\phi}_{\boldsymbol{\theta}}](\mathbf{x})\). Note that the targets are dynamically changing during training as the model \(\boldsymbol{\phi}_{\boldsymbol{\theta}}\) keeps evolving.

This unified perspective allows us to jointly examine and address the inefficient training issue of (self-)supervised learning from a data-centric perspective, in which in Section3.2 we first study the impact of samples \(D_{X}\) and targets \(D_{Y}\) on the model training process and then investigate whether and how a distilled dataset \(S=(S_{X},S_{Y})\) can facilitate this process.

### Empirical and Theoretical Investigation of Data-Centric Efficient Learning

To elucidate the ideal data properties of training on a dataset \(D\), we examine the simple task over a bimodal Gaussian mixture distribution as a case study. We begin by defining the problem.

**Definition 3** (Bimodal Gaussian mixture distribution): _Given two Gaussian distributions \(\mathcal{N}_{0}(\mu_{1},\Sigma^{2}\mathbf{I})\) and \(\mathcal{N}_{1}(\mu_{2},\Sigma^{2}\mathbf{I})\), where \(\mu_{1}\) and \(\mu_{2}\) are the means and \(\Sigma^{2}\) is the variance (here we set \(\mu_{1}=1\), \(\mu_{2}=2\) and \(\Sigma=0.5\)). We define a bimodal mixture data \(G=(G_{X},G_{Y})\) as:_

\[G:=\{(\mathbf{x},y)\mid\mathbf{x}=(1-y)\cdot\mathbf{x}_{0}+y\cdot\mathbf{x}_{1 }\}\ \ \text{s.t.}\ \ y\sim\text{Bernoulli}(p=0.5),\mathbf{x}_{0}\sim\mathcal{N}_{0},\mathbf{x}_{1 }\sim\mathcal{N}_{1}\,.\] (3)

_Moreover, we define a corresponding binary classification neural network model as:_

\[f_{\boldsymbol{\theta}}(\mathbf{x}):=\sigma\left(\boldsymbol{\theta}^{[1]} \cdot\text{ReLU}\left(\boldsymbol{\theta}^{[2]}\mathbf{x}+\boldsymbol{ \theta}^{[3]}\right)+\boldsymbol{\theta}^{[4]}\right)\,,\] (4)_where \(\sigma(z)=\frac{1}{1+e^{-z}}\) is the sigmoid activation function; \(\operatorname{ReLU}(z)=\max(0,z)\) is the activation function for the hidden layer, which provides non-linearity to the model; \(\bm{\theta}^{[2]}\) and \(\bm{\theta}^{[3]}\) are the weights and biases of the hidden layer; \(\bm{\theta}^{[1]}\) and \(\bm{\theta}^{[4]}\) are the weights and biases of the output layer._

Modern representation learning fundamentally relies on optimization (see Definition1). We show that modifications to the data can influence the convergence rate of the optimization process, thereby impacting the overall representation learning procedure. Furthermore, we try to uncover several key properties of data efficiency through our theoretical analysis of the case study. In the following, we denote the modified distribution by \(G^{\prime}=(G^{\prime}_{X},G^{\prime}_{Y})\) and examine the altered samples \(G^{\prime}_{X}\) and corresponding targets \(G^{\prime}_{Y}\) independently.

Investigating the properties of modified samples.The modification process here only rescales the variance of the original sample distribution \(G_{X}\) defined in Definition3 with new \(\Sigma\) (rather than the default \(0.5\)), while let \(G^{\prime}_{Y}:=G_{Y}\); see explanations in AppendixE. Therefore, we examine the distilled samples \(G^{\prime}_{X}\) by setting the variable \(\Sigma\) within the interval \((0,1)\).

Results in Figure2 demonstrate that the distilled samples \(G^{\prime}_{X}\) with smaller variance \(\Sigma\) achieve faster convergence and better performance compared to that of \(G\). To elucidate the underlying mechanism, we provide a rigorous theoretical analysis in AppendixB, culminating in Theorem1.

**Theorem 1** (Convergence rate of learning on efficient samples).: _For the classification task stated in Definition3, the convergence rate for the model \(f_{\bm{\theta}}\) trained \(t\) after steps over distilled data \(G^{\prime}\) is:_

\[\mathbb{E}_{\bm{\theta}_{t}}\left[\mathcal{L}(f_{\bm{\theta}_{t}};G^{\prime}; \ell)-\mathcal{L}(f_{\bm{\theta}^{*}};G^{\prime};\ell)\right]\leq\tilde{ \mathcal{O}}(\Sigma^{2})\,,\] (5)

_where \(\ell\) denotes the MSE loss, i.e., \(\ell(\hat{y},y):=\|\hat{y}-y\|^{2}\), and \(f_{\bm{\theta}^{*}}\) indicates the optimal model, \(\tilde{\mathcal{O}}\) signifies the asymptotic complexity. Modified samples characterized by a smaller value of \(\Sigma\) facilitate faster convergence._

Investigating the properties of modified targets.On top of the property understanding for modified samples, we further investigate the potential of modified targets via \(G^{\prime}\). In detail, for modified samples, we consider the most challenging (c.f. Figure2b) yet the common case, namely \(G^{\prime}_{X}\) with \(\Sigma=1\) (see explanations in AppendixM). For the corresponding modified targets \(G^{\prime}_{Y}\), similar to the prior data-efficient methods [56, 67], for any sample \(\mathbf{x}\) drawn from \(G^{\prime}_{X}\), we refine its label by assigning \(\hat{y}=\rho\cdot f_{\bm{\theta}^{*}}(\mathbf{x})+(1-\rho)\cdot y\). Here, \(\rho\) denotes the relabeling intensity coefficient, and \(f_{\bm{\theta}^{*}}\) represents a strong pre-trained model (simply, we utilize the model trained on the data in Figure2c).

**Theorem 2** (Convergence rate of learning on re-labeled data).: _For the classification task as in Definition3, we have the convergence rate for the model \(f_{\bm{\theta}}\) trained after \(t\) steps over modified data \(G^{\prime}\):_

\[\mathbb{E}_{\bm{\theta}_{t}}\left[\mathcal{L}(f_{\bm{\theta}_{t}};G^{\prime}; \ell)-\mathcal{L}(f_{\bm{\theta}^{*}};G^{\prime};\ell)\right]\leq\tilde{ \mathcal{O}}(1-\rho)\,.\] (6)

_Note that \(\rho\) controls the upper bound of the convergence rate, indicating that using modified targets with a higher value of \(\rho\) enables faster convergence._

Results in Figure2 illustrate that the modified targets \(G^{\prime}_{Y}\) with higher values of \(\rho\) lead to faster training convergence and better performance. See theoretical analysis in Theorem2 and AppendixB.

### Extended Understanding of Data-Centric Efficient Learning

The empirical and theoretical investigations regarding the properties of modified samples \(G^{\prime}_{X}\) and targets \(G^{\prime}_{Y}\) in Section3.2 are limited to a simplified case (as described in Definition3) and may not extend to all practical scenarios, such as training a ResNet [26] on the ImageNet dataset [20].

Interestingly, we observe that the advantageous modifications of both samples \(G^{\prime}_{X}\) and targets \(G^{\prime}_{Y}\) converge towards a unified principle: minimizing or preventing any sample \(\mathbf{x}\) from being labeled with multiple or inaccurate targets \(\mathbf{y}\). This principle emphasizes the importance of providing accurate and informative targets \(\mathbf{y}\) for each sample \(\mathbf{x}\), as analyzed in Remark1, and suggests extending this insight to any complex dataset like \(S\).

**Remark 1** (Ideal data properties avoid implicitly introduced gradient noise from data): _Intuitively, the semantic information within each sample \(\mathbf{x}\) should be unique and not identical to another sample. Consequently, the exact target \(\mathbf{y}\), which represents the semantic information of \(\mathbf{x}\), should also be unique and informative. This implies the necessity of establishing bijective (or one-to-one) mappings between samples and their respective targets._

_In contrast, when a sample \(\mathbf{x}\) (or several similar samples) is labeled with multiple different targets \(\mathbf{y}\), it may implicitly introduce noise into the gradient \(\nabla\ell(\mathbf{x},\mathbf{y})\), thereby hindering the optimization._

However, real-world datasets often deviate from the ideal properties described above, as discussed in Remark2 below and further analyzed in AppendixM.

**Remark 2** (Imperfect mappings and inaccurate targets in real-world datasets): _In practice, we observe that 'noisy mappings' between input samples and targets are prevalent in real-world datasets. As illustrated in Figure6, several common phenomena contribute to this issue:_

* _Similar or identical input samples may be assigned different targets due to using data augmentations, which is common in both self-supervised and human-supervised learning settings._
* _Inaccurate targets may be generated, particularly in self-supervised learning scenarios._
* _In human-supervised learning, all samples within a class are mapped to a one-hot target._

_These imperfect mappings and inaccurate targets pose challenges to achieving optimal training efficiency and effectiveness for real-world datasets._

### Generalization-bounded Efficient Data Synthesis

Given insights from Remark1, an effective approach to generate efficient data \(S\) from original data \(D\) involves employing a high-quality labeler \(\bm{\psi}\) to relabel each sample \(\mathbf{x}\) within \(D\). This process

Figure 3: **Investigating modified targets with varied \(\rho\) values. We present a visualization of the validation loss landscape in Figure3(a), including three training trajectories that correspond to different \(\rho\) settings. Figure3(b) illustrates the performance of models trained using targets with varying \(\rho\) values. The optimal scenario for our task, which uses targets with \(\rho=1.0\), is depicted in Figure3(c).**

Figure 2: **Investigating modified samples with varied \(\Sigma\) values. Following [39], Figure2(a) visualizes the validation loss landscape within a two-dimensional parameter space, along with three training trajectories corresponding to different \(\Sigma\) settings. Figure2(b) illustrates the performance of models trained using samples with varied \(\Sigma\). The optimal case in our task, utilizing samples with \(\Sigma=0.1\) (which achieves the lowest validation loss in Figure2(b)), is visualized in Figure2(c), where the color bar represents the values of targets \(y\).**

[MISSING_PAGE_FAIL:7]

### ReLA-D (): Synthesis of Efficient Dataset

Motivated by two property requirements in Definition5, here we introduce our optimization-free synthesis process of both samples and targets in our ReLA-D (see technical details in F).

**Generating transportable representations as the targets.** We argue that _well-trained models (called prior models) on diverse real-world datasets using various neural network architectures and algorithms converge towards the same linear representation space. In other words, the generated pseudo representations \(R_{Y}\) for samples \(D_{X}\) using these prior models are linearly transportable to each other and to the human-annotating targets._ The empirical verifications refer to AppendixA. We further justify in AppendixF.1 that the requirement 1 in Definition5 can be achieved by employing a prior model as the ideal labeler \(\bm{\psi}:\mathbb{R}^{d}\to\mathbb{R}^{m}\), i.e., generating \(R_{Y}=\{\bm{\psi}(\mathbf{x})\mid\mathbf{x}\sim D_{X}\}\) as the targets. The generation process of targets is conducted only once (refer to AppendixH for details), and the generated targets \(R_{Y}\) are stored and combined with the samples \(D_{X}\) to form the data \(D=(D_{X},R_{Y})\).

Efficient and distribution-aligned sample generation.To satisfy requirement 2 in Definition5 efficiently, we employ basic data augmentations into data \(D_{X}\) such as RandomResizeCrop with a minimum scale of 0.5 (as opposed to the default of 0.08) and RandomHorizontalFlip with \(p=0.5\).

### ReLA-F (): Assist Learning with Generated Efficient Dataset

In this section, we showcase the significance of understanding ideal data properties and generated efficient dataset in assisting self-supervised learning, given this self-supervised paradigm on unlabeled data suffers from significant inefficiency issues compared to human-supervised learning [62].

Here we propose a plug-and-play method that can be seamlessly integrated into any existing self-supervised learning algorithm, significantly enhancing its training efficiency by introducing an additional loss term. Formally, the loss function is defined as follows:

\[\lambda\cdot\mathcal{L}_{\textsc{ReLA}}+(1-\lambda)\cdot\mathcal{L}_{\textsc{ SSL}}\,\ \text{where}\ \mathcal{L}_{\textsc{ReLA}}:=\mathbb{E}_{\mathbf{x},\mathbf{y}\sim(D_{X},R_{Y })}\left[\ell(\mathbf{W}\bm{\phi}_{\bm{\theta}}(\mathbf{x})-\mathbf{b}, \mathbf{y})\right]\,\] (9)

where \(\ell(\mathbf{z},\mathbf{y}):=1-\mathbf{z}\cdot\mathbf{y}/(\|\bm{z}\|\|\|\mathbf{ y}\|)\) be the loss function, \(\mathcal{L}_{\textsc{SSL}}\) denotes the loss specified by any self-supervised learning method, respectively. Furthermore, the data \(D=(D_{X},R_{Y})\) are collected using the strategy outlined in Section4.1, with updates occurring at each \(k\)-th epoch.

The dynamic coefficient \(\lambda\in\{0,1\}\) divides the training process into two distinct stages. Initially, \(\lambda\) is set to \(1\) to emphasize \(\mathcal{L}_{\textsc{ReLA}}\), assuming its crucial role in the early learning phase. As the model \(\bm{\phi}_{\bm{\theta}}\) improves and self-generated targets become more reliable in \(\mathcal{L}_{\textsc{SSL}}\), an adaptive attenuation algorithm adjusts \(\lambda\) to \(0\) (note that the initial \(\lambda\) is tuning-free for all cases and see AppendixJ for details). As a result, only a single loss term in (9) is calculated, ensuring no extra computational cost with ReLA.

To enhance the recognition of ReLA-aided algorithms, we re-denote those that are used in their names. For example, the BYOL algorithm [30], when enhanced with ReLA, is re-denoted as BYOL (). Furthermore, as the prior models downloaded from the internet are not consistently robust, the aforementioned dynamic setting of \(\lambda\) also prevents the model \(\bm{\phi}_{\bm{\theta}}\) from overfitting to potentially weak generated targets. The efficacy of our proposed ReLA is empirically validated in Section5.

## 5 Experiments

This section describes the experimental setup and procedures undertaken to test our hypotheses and evaluate the effectiveness of our proposed methodologies.

Experimental setting.We list the settings below (see more details in AppendixK).

\(\bullet\)_Datasets:_ For low-resolution data (\(32\times 32\)), we evaluate our method on two datasets, i.e., CIFAR-10 [35] and CIFAR-100 [34]. For high-resolution data, we conduct experiments on two large-scale datasets including Tiny-ImageNet (\(64\times 64\)) [36] and full ImageNet-1K (\(224\times 224\)) [20], to assess the scalability and effectiveness of our method on more complex and varied datasets.

\(\bullet\)_Neural network architectures:_ Similar to prior works/benchmarks of dataset distillation [56] and self-supervised learning [57; 19], we use several backbone architectures to evaluate the generalizabilityof our method, including ResNet-{18, 50, 101} [26], EfficientNet-B0 [58], MobileNet-V2 [52], ViT [21], and a series of CLIP-based models [49]. These architectures represent a range of model complexities and capacities, enabling a comprehensive assessment of our approach.

\(\bullet\)_Baselines:_ Referring to a prior widely-used benchmark [57; 19], we consider several state-of-the-art methods as baselines for a broader practical impact, including: SimCLR [12], Barlow Twins [69], BYOL [30], DINO [10], MoCo [25], SimSiam [14], SwAV [9], and Vicreg [3].

\(\bullet\)_Evaluation:_ Following previous benchmarks and research [57; 19; 12; 3], we evaluate all the trained models using offline linear probing strategy to reflect the representation ability of the trained models, and ensure a fair and comprehensive comparison with baseline approaches.

\(\bullet\)_Implementation details:_ We implement our method by extending a popular self-supervised learning open-source benchmark [57] and use their configurations therein. This includes using AdamW as the optimizer, with a mini-batch size of 128 (except for ImageNet-1K, where we use a mini-batch size of 512). We implement our method through PyTorch [48], and all experiments are conducted on NVIDIA RTX 4090 GPUs. See more detailed configurations and hyper-parameters in Appendix K.

### Primary Experimental Results and Analysis

Recall that our ReLA-D (), as illustrated in Figure I and Section 4.1, requires an unlabeled dataset and _any pre-trained model freely available online_ to generate the efficient dataset. To justify the superior performance and generality of our ReLA across various unlabeled datasets using prior models with different representation abilities, our comparisons in this subsection start with BYOL [30]2 and then extend to other self-supervised learning methods.

Footnote 2: Note that (1) BYOL is competitive across various datasets [30; 3; 57; 12], and (2) various self-supervised learning methods can be unified in the same framework [59] (see our detailed analysis in Appendix G).

Table I demonstrates the efficacy and efficiency of our ReLA in facilitating the learning of robust representations. Overall, _BYOL ( ) consistently outperforms the original BYOL_ when trained with a reduced budget. In certain cases, such as on CIFAR-100, BYOL ( ) employing only \(10\%\) of the budget can surpass the performance of BYOL-trained models using the entire budget Specifically:

1. A stronger prior model (e.g., CLIP) enhances the performance of ReLA more effectively than a weaker model (e.g., Rand.);
2. Our ReLA is not sensitive to the prior knowledge. For instance, using CF10-T as the prior model can achieve competitive performance compared to that trained on extensive datasets (e.g., CLIP);

\begin{table}
\begin{tabular}{l|c|c|c c c c c c|c} \hline \hline Dataset & \% & BYOL & Rand. & CF10-T & CF100-T & TIN-T & IN1K-T & CLIP-RN50 & BYOL\({}^{\star}\) \\ \hline \multirow{3}{*}{CF-10} & 10 & 58.3 \(\pm\) 0.1 & 71.4 \(\pm\) 0.0 & 81.1 \(\pm\) 0.1 & 78.2 \(\pm\) 0.1 & 79.6 \(\pm\) 0.1 & 81.6 \(\pm\) 0.0 & **82.0 \(\pm\) 0.1** & \multirow{3}{*}{82.7 \(\pm\) 0.2} \\  & 20 & 70.1 \(\pm\) 0.2 & 77.1 \(\pm\) 0.2 & 83.6 \(\pm\) 0.1 & 81.4 \(\pm\) 0.0 & 83.2 \(\pm\) 0.1 & **84.4 \(\pm\) 0.1** & 83.9 \(\pm\) 0.1 & 82.7 \(\pm\) 0.2 \\  & 50 & 77.9 \(\pm\) 0.0 & 82.7 \(\pm\) 0.1 & 86.5 \(\pm\) 0.1 & 86.2 \(\pm\) 0.0 & 86.2 \(\pm\) 0.1 & **87.3 \(\pm\) 0.2** & 86.5 \(\pm\) 0.0 & \\ \hline \multirow{3}{*}{CF-100} & 10 & 26.9 \(\pm\) 0.2 & 41.8 \(\pm\) 0.2 & 51.4 \(\pm\) 0.1 & 51.4 \(\pm\) 0.1 & 53.5 \(\pm\) 0.1 & **56.4 \(\pm\) 0.2** & 55.4 \(\pm\) 0.1 & \multirow{3}{*}{52.5 \(\pm\) 0.3} \\  & 20 & 34.8 \(\pm\) 0.3 & 48.1 \(\pm\) 0.1 & 55.7 \(\pm\) 0.1 & 55.7 \(\pm\) 0.1 & 56.7 \(\pm\) 0.0 & **59.5 \(\pm\) 0.1** & 57.9 \(\pm\) 0.0 & 52.5 \(\pm\) 0.3 \\  & 50 & 41.4 \(\pm\) 0.3 & 54.6 \(\pm\) 0.2 & 59.7 \(\pm\) 0.1 & 59.8 \(\pm\) 0.1 & 60.0 \(\pm\) 0.1 & **61.6 \(\pm\) 0.1** & 61.0 \(\pm\) 0.0 & \\ \hline \multirow{3}{*}{T-IN} & 10 & 25.1 \(\pm\) 0.3 & 34.5 \(\pm\) 0.3 & 39.0 \(\pm\) 0.1 & 38.4 \(\pm\) 0.0 & 41.2 \(\pm\) 0.1 & **41.6 \(\pm\) 0.1** & 39.6 \(\pm\) 0.4 & \multirow{3}{*}{43.6 \(\pm\) 0.3} \\  & 20 & 30.7 \(\pm\) 0.1 & 38.2 \(\pm\) 0.0 & 41.9 \(\pm\) 0.0 & 42.3 \(\pm\) 0.0 & 43.2 \(\pm\) 0.1 & **44.1 \(\pm\) 0.1** & 42.6 \(\pm\) 0.1 & 43.6 \(\pm\) 0.3 \\  & 50 & 37.7 \(\pm\) 0.2 & 43.9 \(\pm\) 0.1 & 45.6 \(\pm\) 0.1 & 45.9 \(\pm\) 0.1 & 45.8 \(\pm\) 0.1 & **46.4 \(\pm\) 0.1** & 46.3 \(\pm\) 0.1 & \\ \hline \multirow{3}{*}{IN-1K} & 10 & 44.5 \(\pm\) 0.1 & 51.7 \(\pm\) 0.1 & 53.7 \(\pm\) 0.1 & 53.3 \(\pm\) 0.1 & 53.6 \(\pm\) 0.1 & 54.9 \(\pm\) 0.1 & **56.2 \(\pm\) 0.1** & \multirow{3}{*}{61.9 \(\pm\) 0.1} \\  & 20 & 55.3 \(\pm\) 0.0 & 56.9 \(\pm\) 0.0 & 57.6 \(\pm\) 0.1 & 57.6 \(\pm\) 0.1 & 57.8 \(\pm\) 0.1 & 58.0 \(\pm\) 0.0 & **59.5 \(\pm\) 0.1** & 61.9 \(\pm\) 0.1 \\ \cline{1-1}  & 50 & 60.8 \(\pm\) 0.2 & 61.1 \(\pm\) 0.1 & 62.1 \(\pm\) 0.1 & 61.8 \(\pm\) 0.1 & 61.7 \(\pm\) 0.0 & 61.9 \(\pm\) 0.0 & **62.9 \(\pm\) 0.1** & \multirow{3}{*}{61.9 \(\pm\) 0.1} \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Benchmark our ReLA with various prior models against BYOL**. We compare evaluation results of the models trained using \(\bullet\) BYOL with \(10\%\), \(20\%\) and \(50\%\) training budget/steps; \(\bullet\) BYOL ( ) with different prior models; \(\bullet\) BYOL with full budget, denoted as BYOL\({}^{\star}\) in this table. Regarding the prior models used for our ReLA, we respectively utilize six models with increasing representation capabilities, including \(\bullet\) randomly initialized network (Rand.); \(\bullet\) four BYOL\({}^{\star}\)-trained models (CF10-T, CF100-T, TIN-T, IN1K-T) corresponding to four datasets (listed below); \(\bullet\) CLIP-RN50. The evaluations are performed across four datasets, i.e., CIFAR-10 (CF-10), CIFAR-100 (CF-100), Tiny-ImageNet (T-IN), and ImageNet-1K (IN-1K). We underline the results that outperform the full training, and **bold** the results that achieve the highest performance using a specific ratio of budget. All the networks used for training are ResNet-18, except the ResNet-50 used for IN-1K.

3. A randomly initialized model can effectively aid in accelerating learning through our ReLA. This can be considered an effective scenario of "weak-to-strong supervision" [7] using pseudo targets.

**Cross-architecture generalization.** ReLA-D () generates efficient datasets using a specific neural architecture. To evaluate the generalization ability of these datasets, it is essential to test their performance on various architectures not used in the distillation process. Table 2 presents the performance of our ReLA in conjunction with various prior models and trained model architectures, demonstrating its robust generalization ability. Specifically:

1. [label=()]
2. The integration of ReLA always enhances the performance of original BYOL;
3. Our ReLA method exhibits minimal sensitivity to the architecture of the prior model, as evidenced by the comparable performance of BYOL () using both ViT-based and ResNet-based models.

**Combining ReLA across various self-supervised learning methods.** To demonstrate the effectiveness and versatility of ReLA in enhancing various self-supervised learning methods, we conduct experiments with widely-used techniques. Table 3 presents the results, highlighting the robust generalization capability of ReLA. Our findings consistently show that ReLA improves the performance of these methods while maintaining the same budget ratio, emphasizing its potential on learning using unlabeled data. Additionally, we provide the results when combining ReLA with human-supervised learning in Appendix I.

## 6 Conclusion and Limitation

In this paper, to address the Problem 1, we investigate the optimal properties of data, including samples and targets, to identify the properties that improve generalization and optimization in deep learning models. Our theoretical insights indicate that targets which are informative and linearly transportable to strong representations (e.g., human annotations) enable trained models to exhibit robust representation abilities. Furthermore, we empirically find that well-trained models (called prior models) across various tasks and architectures serve as effective labelers for generating such targets. Consequently, we propose the Representation Learning Accelerator (ReLA), which leverages any freely available prior model to generate high-quality targets for samples. Additionally, ReLA can enhance existing (self-)supervised learning approaches by utilizing these generated data to accelerate training. However, our theoretical analysis is restricted to the simplified scenario described in Definition 3, which has limited applicability in real-world contexts.

\begin{table}
\begin{tabular}{l|c|c c c c c c c} \hline \hline Dataset & Arch. & Original & \(\left\langle\,\boldsymbol{\tau}\,\right\rangle\) w/ RN18 & RN101 & RN50x4 & ViT B/32 & ViT B/16 & ViT L/14 \\ \hline \multirow{4}{*}{CF-10} & ResNet-18 & \(58.3\pm 0.1\) & \(71.4\pm 0.0\) & \(81.9\pm 0.1\) & \(82.1\pm 0.3\) & \(83.2\pm 0.2\) & \(83.1\pm 0.1\) & \(82.4\pm 0.1\) \\  & MobileNet-V2 & \(47.7\pm 0.1\) & \(69.4\pm 0.0\) & \(82.2\pm 0.1\) & \(80.8\pm 0.0\) & \(81.6\pm 0.1\) & \(82.9\pm 0.2\) & \(81.2\pm 0.2\) \\  & EfficientNet-B0 & \(23.9\pm 0.2\) & \(68.8\pm 0.6\) & \(83.2\pm 0.2\) & \(83.9\pm 0.1\) & \(87.4\pm 0.1\) & \(86.4\pm 0.1\) & \(83.1\pm 0.1\) \\  & ViT T/16 & \(43.4\pm 0.1\) & \(57.1\pm 0.1\) & \(65.9\pm 0.0\) & \(66.4\pm 0.1\) & \(69.9\pm 0.3\) & \(68.8\pm 0.1\) & \(63.7\pm 0.1\) \\ \hline \multirow{4}{*}{T-IN} & ResNet-18 & \(25.1\pm 0.3\) & \(34.5\pm 0.3\) & \(38.3\pm 0.1\) & \(39.1\pm 0.4\) & \(35.8\pm 0.1\) & \(32.4\pm 0.1\) & \(28.4\pm 0.2\) \\  & MobileNet-V2 & \(8.8\pm 0.1\) & \(28.3\pm 0.3\) & \(39.9\pm 0.1\) & \(36.8\pm 0.2\) & \(36.0\pm 0.0\) & \(37.9\pm 0.3\) & \(20.6\pm 0.5\) \\ \cline{1-1}  & EfficientNet-B0 & \(4.1\pm 0.0\) & \(33.2\pm 0.3\) & \(43.5\pm 0.2\) & \(41.7\pm 0.2\) & \(44.0\pm 0.1\) & \(44.2\pm 0.0\) & \(37.9\pm 0.1\) \\ \cline{1-1}  & ViT T/16 & \(12.5\pm 0.1\) & \(24.6\pm 0.0\) & \(26.1\pm 0.1\) & \(27.6\pm 0.1\) & \(26.9\pm 0.2\) & \(24.5\pm 0.1\) & \(21.6\pm 0.0\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Evaluating our ReLA on cross-architecture settings. Our ReLA-D () distills datasets with prior RN18 (Rand.) and CLIP-{RN101, RN50\(\times\)4, ViT B/32, ViT B/16, ViT L/14}, then versus transfer to ResNet-18; MobileNet-V2; EfficientNet-B0; ViT T/16. We train models using \(10\%\) budget through (original) BYOL ().**

\begin{table}
\begin{tabular}{l|c|c c c c c c c} \hline \hline Dataset & Method & SimCLR & Barlow & DINO & MoCo & SimSiam & SwAV & Vicreg \\ \hline \multirow{4}{*}{CF-10} & Original & \(70.7\pm 0.2\) & \(63.7\pm 0.3\) & \(66.2\pm 0.2\) & \(67.4\pm 0.4\) & \(45.8\pm 0.4\) & \(66.2\pm 0.3\) & \(71.3\pm 0.2\) \\  & () w/ Rand. & \(70.9\pm 0.0\) & \(68.8\pm 0.2\) & \(70.6\pm 0.1\) & \(70.9\pm 0.1\) & \(66.7\pm 0.1\) & \(69.5\pm 0.2\) & \(71.3\pm 0.1\) \\  & CLIP-RN50 & \(76.4\pm 0.1\) & \(76.5\pm 0.2\) & \(82.4\pm 0.1\) & \(79.8\pm 0.1\) & \(79.3\pm 0.1\) & \(77.3\pm 0.0\) & \(80.1\pm 0.1\) \\ \hline \multirow{4}{*}{T-IN} & Original & \(30.4\pm 0.1\) & \(28.9\pm 0.4\) & \(26.7\pm 0.3\) & \(27.1\pm 0.2\) & \(17.8\pm 0.3\) & \(20.2\pm 0.1\) & \(34.0\pm 0.1\) \\  & () w/ Rand. & \(30.7\pm 0.2\) & \(31.9\pm 0.1\) & \(29.4\pm 0.2\) & \(33.4\pm 0.1\) & \(25.4\pm 0.1\) & \(29.1\pm 0.2\) & \(34.1\pm 0.1\) \\ \cline{1-1}  & CLIP-RN50 & \(33.0\pm 0.3\) & \(33.5\pm 0.2\) & \(35.1\pm 0.0\) & \(37.1\pm 0.1\) & \(32.6\pm 0.1\) & \(32.6\pm 0.1\) & \(39.1\pm 0.2\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Evaluating our ReLA across different self-supervised learning methods. We extend our analysis beyond BYOL by training and evaluating models using seven additional self-supervised learning methods, along with their ReLA-augmented counterparts (), utilizing randomly initialized ResNet-18 (Rand.) and CLIP-{RN50} as prior models for the ReLA-D (). All methods are trained using \(10\%\) budget.**

## 7 Acknowledgement

We thank Xinyi Shang, Bowen Ding and and the anonymous reviewers for their invaluable comments and feedback. We also thank Bei Shi for assisting with the partial code implementation. This work was supported in part by the National Science and Technology Major Project (No. 2022ZD0115101), the Research Center for Industries of the Future (RCIF) at Westlake University, and the Westlake Education Foundation.

## References

* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] Yutong Bai, Xinyang Geng, Kartikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. _arXiv preprint arXiv:2312.00785_, 2023.
* [3] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. _arXiv preprint arXiv:2105.04906_, 2021.
* [4] Andrew R Barron, Lhszl Gyorfi, and Edward C van der Meulen. Distribution estimation consistent in total variation and in two types of information divergence. _IEEE transactions on Information Theory_, 38(5):1437-1454, 1992.
* [5] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Vaughan. A theory of learning from different domains. _Machine Learning_, 79:151-175, 2010.
* [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [7] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. _arXiv preprint arXiv:2312.09390_, 2023.
* [8] Yue Cao, Zhenda Xie, Bin Liu, Yutong Lin, Zheng Zhang, and Han Hu. Parametric instance classification for unsupervised visual feature learning. In _NeurIPS_, 2020.
* [9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In _NeurIPS_, 2020.
* [10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _ICCV_, 2021.
* [11] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4750-4759, 2022.
* [12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, 2020.
* [13] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. _arXiv preprint arXiv:2003.04297_, 2020.
* [14] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _CVPR_, 2021.
* [15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. _arXiv preprint arXiv:2104.02057_, 2021.
* [16] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. _arXiv preprint arXiv:1710.09282_, 2017.

* [17] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Dc-bench: Dataset condensation benchmark. _Advances in Neural Information Processing Systems_, 35:810-822, 2022.
* [18] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet-1k with constant memory. In _International Conference on Machine Learning_, pages 6565-6590. PMLR, 2023.
* [19] Victor Guilherme Turrisi Da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A library of self-supervised methods for visual representation learning. _Journal of Machine Learning Research_, 23(56):1-6, 2022.
* [20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [22] Jiawei Du, Yidi Jiang, Vincent YF Tan, Joey Tianyi Zhou, and Haizhou Li. Minimizing the accumulated trajectory error to improve dataset distillation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3749-3758, 2023.
* [23] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks, 2019.
* [24] Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You. Towards lossless dataset distillation via difficulty-aligned trajectory matching. _arXiv preprint arXiv:2310.05773_, 2023.
* [25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _CVPR_, 2020.
* [26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [27] Qianjiang Hu, Xiao Wang, Wei Hu, and Guo-Jun Qi. Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries. In _CVPR_, 2021.
* [28] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukoJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. _Advances in neural information processing systems_, 32, 2019.
* [29] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* a new approach to self-supervised learning. In _NeurIPS_, 2020.
* [31] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. In _NeurIPS_, 2020.
* [32] Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and Hyun Oh Song. Dataset condensation via efficient synthetic-data parameterization. In _International Conference on Machine Learning_, pages 11102-11118. PMLR, 2022.

* [33] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [34] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [35] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. _URI: https://www. cs. toronto. edu/kriz/cifar. html_, 6(1):1, 2009.
* [36] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. _CS 231N_, 7(7):3, 2015.
* [37] Dong Bok Lee, Seanie Lee, Joonho Ko, Kenji Kawaguchi, Juho Lee, and Sung Ju Hwang. Self-supervised dataset distillation for transfer learning. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2024.
* [38] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent *. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(12):124002, December 2020.
* [39] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. _Advances in neural information processing systems_, 31, 2018.
* [40] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. In _ICLR_, 2021.
* [41] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. _Advances in neural information processing systems_, 31, 2018.
* [42] Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don't use large mini-batches, use local sgd. _arXiv preprint arXiv:1808.07217_, 2018.
* [43] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. _AI Open_, 2023.
* [44] Yanqing Liu, Jianyang Gu, Kai Wang, Zheng Zhu, Wei Jiang, and Yang You. Dream: Efficient dataset distillation by representative matching. _arXiv preprint arXiv:2302.14416_, 2023.
* [45] Sachin Mehta, Maxwell Horton, Fartash Faghri, Mohammad Hossein Sekhavat, Mahyar Najibi, Mehrdad Farajtabar, Oncel Tuzel, and Mohammad Rastegari. Catlip: Clip-level visual recognition accuracy with 2.7 x faster pre-training on web-scale image-text data. _arXiv preprint arXiv:2404.15653_, 2024.
* [46] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_. 2018. Second Edition.
* [47] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [50] Pierre H Richemond, Jean-Bastien Grill, Florent Altche, Corentin Tallec, Florian Strub, Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, et al. Byol works even without batch statistics. _arXiv preprint arXiv:2010.10241_, 2020.

* [51] Herbert E. Robbins. A stochastic approximation method. _Annals of Mathematical Statistics_, 22:400-407, 1951.
* [52] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4510-4520, 2018.
* [53] Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang, and Zhiqiang Shen. Generalized large-scale data condensation via various backbone and statistical matching. _arXiv preprint arXiv:2311.17950_, 2023.
* [54] Shitong Shao, Zikai Zhou, Huanran Chen, and Zhiqiang Shen. Elucidating the design space of dataset condensation. _arXiv preprint arXiv:2404.13733_, 2024.
* [55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. _arXiv preprint arXiv:1906.02243_, 2019.
* [56] Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2024.
* [57] Igor Susmelj, Matthias Heller, Philipp Wirth, Jeremy Prescott, and Malte Ebner et al. Lightly. _GitHub. Note: https://github.com/lightly-ai/lightly_, 2020.
* [58] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _International conference on machine learning_, pages 6105-6114. PMLR, 2019.
* [59] Chenxin Tao, Honghui Wang, Xizhou Zhu, Jiahua Dong, Shiji Song, Gao Huang, and Jifeng Dai. Exploring the equivalence of siamese self-supervised learning via a unified gradient framework. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14431-14440, 2022.
* [60] Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In _ICML_, 2021.
* [61] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* [62] Guangrun Wang, Keze Wang, Guangcong Wang, Philip HS Torr, and Liang Lin. Solving inefficiency of self-supervised representation learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9505-9515, 2021.
* [63] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and Yang You. Cafe: Learning to condense dataset by aligning features. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12196-12205, 2022.
* [64] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In _ICML_, 2020.
* [65] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. _arXiv preprint arXiv:1811.10959_, 2018.
* [66] David H Wolpert and William G Macready. No free lunch theorems for optimization. _IEEE transactions on evolutionary computation_, 1(1):67-82, 1997.
* [67] Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation at imagenet scale from a new perspective. _arXiv preprint arXiv:2306.13092_, 2023.
* [68] Ruonan Yu, Songhua Liu, and Xinchao Wang. Dataset distillation: A comprehensive review. _arXiv preprint arXiv:2301.07014_, 2023.
* [69] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In _ICML_, 2021.

* [70] Lei Zhang, Jie Zhang, Bowen Lei, Subhabrata Mukherjee, Xiang Pan, Bo Zhao, Caiwen Ding, Yao Li, and Dongkuan Xu. Accelerating dataset distillation via model augmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11950-11959, 2023.
* [71] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 6514-6523, 2023.
* [72] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. _arXiv preprint arXiv:2006.05929_, 2020.
* [73] Ganlong Zhao, Guanbin Li, Yipeng Qin, and Yizhou Yu. Improved distribution matching for dataset condensation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7856-7865, 2023.
* [74] Rui Zhu, Bingchen Zhao, Jingen Liu, Zhenglong Sun, and Chang Wen Chen. Improving contrastive learning by visualizing feature transformation. In _ICCV_, 2021.

###### Contents

* 1 Introduction
* 2 Related Work
	* 2.1 Dataset Distillation: Efficient yet Effective Learning Using Fewer Data
	* 2.2 Self-supervised Learning: Representation Learning Using Unlabeled Data
* 3 Revealing Critical Properties of Efficient Learning over Data
	* 3.1 Unifying (Self-)Supervised Learning from a Data-Centric Perspective
	* 3.2 Empirical and Theoretical Investigation of Data-Centric Efficient Learning
	* 3.3 Extended Understanding of Data-Centric Efficient Learning
	* 3.4 Generalization-bounded Efficient Data Synthesis
* 4 Methodology
	* 4.1 ReLA-D (): Synthesis of Efficient Dataset
	* 4.2 ReLA-F (): Assist Learning with Generated Efficient Dataset
* 5 Experiments
	* 5.1 Primary Experimental Results and Analysis
* 6 Conclusion and Limitation
* 7 Acknowledgement
* A Ablation Study
* B Proof of Theorem 1
* B.1 Setup
* B.2 Algorithm
* B.3 Bounds with Variance
* B.4 Nonlinear case
* B.5 From the Perspective of Feature Learning
* C Proof of Theorem 2
* C.1 Setting
* C.2 Bounds with \(\rho\)
* C.3 Nonlinear case
* D Proof of Theorem 3
* D.1 Relation between data distribution and Rademacher complexity
* E Explanation of Rescaling Samples
* F Detailed Methodology of ReLA-D
* F.1 Proof for Ideal Properties of Prior Models
* G Analysis of Different Self-Supervised Learning Methods
* G.1 A Unified Framework for SSL
* G.2 Contrastive Learning Methods
* G.3 Asymmetric Network Methods
* G.4 Feature Decorrelation Methods
* H Budget of ReLA for Data Synthesis
* I ReLA in Labeled Dataset Distillation and Human-Supervised Learning
	* I.1 Experimental Setup
	* I.2 Main Results
* J ReLA Algorithm

* Experimental Details
* K.1 Detailed Setup for Experiments in Section 3.2
* K.2 Detailed Setup for Experiments in Section 5
* L Batch PCA Reduction
* L.1 Principal Component Analysis
* M Analyze the Practical Data Augmentations
* M.1 Assumptions and Definitions
* M.2 Augmented Distributions
* M.3 Increased Variance Leads to Increased Overlap
* M.4 Conclusion
* M.5 Empirical Analysis for Real-world Datasets

## Appendix A Ablation Study

We conduct ablation studies to understand the impact of each component of ReLA on performance.

**Empirical representation similarity.** Our foundational assumption of our ReLA is that the representations of well-trained models, developed using various neural network architectures and algorithms on diverse real-world datasets, exhibit linear transportability to one another. To test this hypothesis, we assess representation similarity, a metric that quantifies the linear transferability between pre-trained models. The results, depicted in Figure 3(a), demonstrate that representations from robust models (e.g., CLIP) can be effectively transferred to less robust models (e.g., CF10-T). This finding aligns with our results in Table 1, showing that leveraging powerful models (e.g., CLIP and IN1K-T) consistently enhances learning in models trained on datasets with limited knowledge, such as CF-10.

**Combining ReLA and BYOL with static \(\lambda\) setting strategies.** The coefficient \(\lambda\), as introduced in Section 4.2, is pivotal in controlling the weight of the ReLA phase during training. To assess the robustness of our adaptive strategy, which dynamically adjusts \(\lambda\), we compare it to a static \(\lambda\) setting strategy. The results in Figure 3(b) indicate that larger (smaller) ReLA weights are advantageous when using a strong (weak) prior model. Nonetheless, static settings lack generalizability across various scenarios, whereas our adaptive strategy demonstrates superior generalization capabilities.

**Analysis of the computational cost when using ReLA with different prior models.** To validate that our ReLA, in conjunction with various prior models, can effectively reduce computational costs in self-supervised learning, we conducted experiments comparing the computational expense required for BYOL () to achieve equivalent performance to the original BYOL trained with a full budget. The results, illustrated in Figure 3(c), consistently demonstrate that ReLA assists BYOL in lowering training costs while maintaining equivalent performance levels. Furthermore, it is evident that employing robust prior models consistently leads to greater reductions in training budgets.

Figure 4: **Ablation study on BYOL () components and parameters. (a) We analyze the representation similarity between various source models (indicated on the x-axis) and target models (indicated on the y-axis). (b) We compare the static ReLA weight setting strategy with our adaptive strategy. Dotted lines (- -) represent our adaptive strategy, while solid lines (-) denote the static \(\lambda\) setting strategy. Specifically, in the static weight setting (e.g., 0.4), the first \(40\%\) of the training leverages ReLA, with the remaining \(60\%\) employing the original algorithm. (c) We present the computational cost, quantified as training time/steps, of our ReLA across various prior models.**

Proof of Theorem 1

In this section, we prove a slightly modified version of Theorem1, extending the distribution to Generalized Gaussian Mixture(GGM) and making some assumptions for technical reasons. Yet this proof could still reflect the essential of the theorem.

### Setup

**Notation**\(\text{N}(\mu,\alpha,\beta)\) denotes the generalized Gaussian distribution with pdf \(\frac{\beta}{2\alpha\Gamma(1/\beta)}e^{-(|x-\mu|/\alpha)^{\beta}}\), B for Bernoulli distribution.

We focus on the 1-dim situation. Assume that \(\mu_{1}<\mu_{2}\). Define the original data distribution(\(\mathcal{N}_{0}=\text{N}(\mu_{1},\alpha_{0},\beta_{0})\) and \(\mathcal{N}_{1}=\text{N}(\mu_{2},\alpha_{0},\beta_{0})\))

\[G:=\{(x,y)\mid y\sim 2\cdot\text{B}(1,\frac{1}{2})-1,x\sim\frac{1-y}{2}\cdot \mathcal{N}_{0}+\frac{1+y}{2}\cdot\mathcal{N}_{1}\}\]

and the modified one (\(\mathcal{N}_{0}^{\prime}=\text{N}(\mu_{1},\alpha,\beta)\) and \(\mathcal{N}_{1}^{\prime}=\text{N}(\mu_{2},\alpha,\beta)\)):

\[G^{\prime}:=\{(x,y)\mid y\sim 2\cdot\text{B}(1,\frac{1}{2})-1,x\sim\frac{1-y}{2 }\cdot\mathcal{N}_{0}^{\prime}+\frac{1+y}{2}\cdot\mathcal{N}_{1}^{\prime}\}\,.\]

Our task is predicting \(y\) given \(x\). Note that \(y\in\{\pm 1\}\), which is a bit different from the definition in Section 3.2. In 1-dim situation, we just need one parameter for this classification task, so define \(f_{\theta}(x):=\text{sign}(x+\theta)\) to fit the distribution. We could compute the generalization loss on original distribution:

\[\mathcal{L}(f_{\theta})=(\int_{-\theta}^{+\infty}\,dF_{-}+\int_{-\infty}^{- \theta}\,dF_{+})/2=(1-\int_{-\frac{\theta+\mu_{1}}{\alpha_{0}}}^{-\frac{ \theta+\mu_{1}}{\alpha_{0}}}\,dF)/2\]

Obviously \(\theta^{\star}=-\frac{\mu_{1}+\mu_{2}}{2}\), we have:

\[\mathcal{L}(f_{\theta})-\mathcal{L}(f_{\theta^{\star}}) =(\int_{-\frac{\mu_{2}-\mu_{1}}{2\alpha_{0}}}^{\frac{\mu_{2}-\mu_ {1}}{2\alpha_{0}}}\,dF-\int_{-\frac{\theta+\mu_{2}}{\alpha_{0}}}^{-\frac{ \theta+\mu_{1}}{\alpha_{0}}}\,dF)/2\] \[\leq C_{1}\cdot(\theta-\theta^{\star})^{2}\quad(or\,\,C_{1}^{ \prime}\,|\,\theta-\theta^{\star}|)\]

where \(C_{1}\), \(C_{1}^{\prime}\) are constants, \(F_{0}\)\(F_{1}\)\(F\) denote the CDF of \(\mathcal{N}_{0}\)\(\mathcal{N}_{1}\) and \(\text{N}(0,1,\beta_{0})\) respectively. The inequality above is due to the fact that function \(h(x)=(\int_{-1}^{1}\,dF-\int_{x-1}^{x+1}\,dF)/x^{2}\) has limits at 0 and so is bounded.

### Algorithm

For a dataset \(\{(x_{i},y_{i})\}_{i=1}^{n}\), set the loss function \(L(\theta)=\frac{1}{n}\sum_{i=1}^{n}\ell\left[y_{i}(x_{i}+\theta)\right]\), \(\ell(v)=\frac{1}{2}(1-v)^{2}\). We apply the stochastic gradient descent algorithm and assume the online setting (\(n=1\)): at step \(t\) draw one sample \((x_{t},y_{t})\) from \(G^{\prime}\) then use the gradient \(\nabla L(\theta_{t})\) to update \(\theta\) (\(\eta\in(0,1),t\in\mathbb{N}\)):

\[\theta_{t+1}=\theta_{t}-\eta\nabla L(\theta_{t}),\]

\[\nabla L(\theta_{t})=\theta+(x_{t}-y_{t}).\]

It can be observed that randomness of \(x\) leads to noises on gradient.

### Bounds with Variance

We prove the proposition that lower variance of GG can make convergence faster, i.e. \(\mathbb{E}\left[\mathcal{L}(f_{\theta_{t}})-\mathcal{L}(f_{\theta^{\star}})\right]\) is bounded by an increasing function of variance (\(t\) fixed).

Proof.: From above, we could get

\[\theta_{t}=(1-\eta)^{t}\theta_{0}-\eta\left[(x_{t-1}-y_{t-1})+(1-\eta)(x_{t-2}- y_{t-2})+\cdots+(1-\eta)^{t-1}(x_{0}-y_{0})\right]\]and so :

\[\mathbb{E}\left[\mathcal{L}(f_{\theta_{t}})-\mathcal{L}(f_{\theta^{ \star}})\right] \leq C_{1}\mathbb{E}\left[(\theta_{t}-\theta^{\star})^{2}\right]\] \[=C_{1}\mathbb{E}\left\{\left[(1-\eta)^{t}(\theta_{0}-\theta^{ \star})-\eta\sum_{j=1}^{t}(1-\eta)^{j-1}(x_{t-j}-y_{t-j}+\theta^{\star})\right] ^{2}\right\}\] \[=C_{1}\mathbb{E}\left[(1-\eta)^{2t}(\theta_{0}-\theta^{\star})^{2 }+\eta^{2}\sum_{j=1}^{t}(1-\eta)^{2(j-1)}(x_{t-j}-y_{t-j}+\theta^{\star})^{2}\right]\] \[=C_{1}\left((1-\eta)^{2t}(\theta_{0}-\theta^{\star})^{2}+\frac{ \eta}{(2-\eta)}(1-(1-\eta)^{2t})\left[\frac{\alpha^{2}\Gamma(3/\beta)}{\Gamma (1/\beta)}+\left(1-\frac{\mu_{2}-\mu_{1}}{2}\right)^{2}\right]\right)\]

The last two equalities is due to the fact that for \((x,y)\sim G^{\prime}\)

\[\mathbb{E}\left[x-y+\theta^{\star}\right]=0\,\] \[\mathbb{E}\left[(x-y+\theta^{\star})^{2}\right]=\frac{\alpha^{2} \Gamma(3/\beta)}{\Gamma(1/\beta)}+\left(1-\frac{\mu_{2}-\mu_{1}}{2}\right)^{2}.\]

### Nonlinear case

In this subsection, we conduct some qualitative analysis on the nonlinear case. The setting is the same as that in Section 3.2. We point out the differences compared with the linear case above: \(\mathbf{x}\in\mathbb{R}^{d},y\in\{0,1\}\) and

\[f_{\bm{\theta}}(\mathbf{x}):=\sigma\left(\bm{\theta}^{[1]}\cdot\text{ReLU} \left(\bm{\theta}^{[2]}\mathbf{x}+\bm{\theta}^{[3]}\right)+\bm{\theta}^{[4]}\right)\]

where \(\sigma(z)=\frac{1}{1+e^{-z}}\) is the sigmoid function; \(\text{ReLU}(z)=\max(0,z)\) is the activation function for the hidden layer, which provides non-linearity to the model; \(\bm{\theta}^{[2]}\) and \(\bm{\theta}^{[3]}\) are the weights and biases of the hidden layer; \(\bm{\theta}^{[1]}\) and \(\bm{\theta}^{[4]}\) are the weights and biases of the output layer.

To make things explicit, we still assume the online setting and set the loss function \(L(\theta)=\frac{1}{2}(f_{\bm{\theta}}(\mathbf{x})-y)^{2}\). Assume after some iterations, \(\bm{\theta}^{[2]}\cdot\mu_{1}+\bm{\theta}^{[3]}<0\) and \(\bm{\theta}^{[2]}\cdot\mu_{2}+\bm{\theta}^{[3]}>0\) (coordinate-wise). In this situation, we could see that if \(\mathbf{x}\) is close to its mean(\(\mu_{1}\) or \(\mu_{2}\)), the sign of \(\text{ReLU}\left(\bm{\theta}^{[2]}\mathbf{x}+\bm{\theta}^{[3]}\right)\) will be the same as \(y\). So \(f_{\bm{\theta}}\) will become an optimal classifier if \(\bm{\theta}^{[1]}\rightarrow+\infty\) and \(\bm{\theta}^{[4]}\rightarrow-\infty\). We focus on \(\bm{\theta}^{[1]}\), using SGD:

\[\bm{\theta}^{[1]}_{t+1}=\bm{\theta}^{[1]}_{t}-\eta\frac{\partial L }{\partial\bm{\theta}^{[1]}},\] \[\frac{\partial L}{\partial\bm{\theta}^{[1]}}=(f_{\bm{\theta}}( \mathbf{x})-y)\sigma(1-\sigma)\text{ReLU}\left(\bm{\theta}^{[2]}\mathbf{x}+ \bm{\theta}^{[3]}\right)\]

Note that we drop the variable value in \(\sigma(\cdot)\) to make the expression more compact.

Then we can analyze the phenomenon qualitatively: larger \(\Sigma\) will make convergence slower. The reason is that the larger \(\Sigma\) is, when \(\mathbf{x}\) is drawn from \(\mathcal{N}_{1}\) (\(y=1\)), \(\bm{\theta}^{[2]}\mathbf{x}+\bm{\theta}^{[3]}<0\) is more likely to happen(i.e. straying far away from the mean), causing \(\bm{\theta}^{[1]}\) to stop updating; what's worse, when \(\mathbf{x}\) is drawn from \(\mathcal{N}_{0}\) (\(y=0\)), with larger probability \(\bm{\theta}^{[2]}\mathbf{x}+\bm{\theta}^{[3]}>0\) which will make \(\bm{\theta}^{[1]}\) to go in the opposite direction. In summary, it is \(\Sigma\) that makes the gradient noisy thus impacts the convergence rate.

### From the Perspective of Feature Learning

In essence, the theoretical results in [41] could also be interpreted as a proof of the theorem. [41] study the learning of a two-layer ReLU neural network for \(k\)-class classification via stochastic gradient descent (SGD), assuming that each class corresponds to \(l\) patterns(distributions), with every two of the \(k\times l\) distributions of the input data are separated by a distance \(\delta\). Below is the main theorem in [41]:

**Proposition 1**.: _Suppose some assumptions are satisfied, then for every \(\epsilon>0\), there is \(M=\text{poly}(k,l,1/\delta,1/\epsilon)\) such that for every \(m\geq M\), after doing a minibatch SGD with batch size \(B=\text{poly}(k,l,1/\delta,1/\epsilon,\log m)\) and learning rate \(\eta=\frac{1}{m\cdot\text{poly}(k,l,1/\delta,1/\epsilon,\log m)}\) for \(T=\text{poly}(k,l,1/\delta,1/\epsilon,\log m)\) iterations, with high probability, the generalization error(the probability that the model misclassifies) of the learned model is at most \(\epsilon\)._

Theoretical results above show that a larger \(\delta\) helps network to learn more efficiently. (In our case, \(\delta\) can be roughly viewed as the Mahalanobis distance between the two Gaussians, which is inverse proportion to the variance.) Also, Appendix D.2 of [41] demonstrates an example very similar to ours in which the input data is drawn from Gaussian distributions with different means, indicating increasing variance of the Gaussian causes the test accuracy to decrease and takes longer time to get a good solution.

## Appendix C Proof of Theorem 2

### Setting

Use the same setting in Section B(linear case) except that

\[G^{\prime}:=\left\{(x,y^{\prime})\mid y\sim 2\cdot\text{B}(1,\frac{1}{2})-1,x= \frac{1-y}{2}\cdot\mathcal{N}_{0}+\frac{1+y}{2}\cdot\mathcal{N}_{1},y^{\prime }=\rho\cdot f_{\theta^{*}}(x)+(1-\rho)y\right\}.\]

In other words, we modify the distribution of \(y\) instead of \(x\) this time.

### Bounds with \(\rho\)

We're going to prove that higher \(\rho\) can make convergence faster, i.e. \(\mathbb{E}\left[\mathcal{L}(f_{\theta_{t}})-\mathcal{L}(f_{\theta^{*}})\right]\) is bounded by an decreasing function of \(\rho\) (\(t\) fixed).

Proof.: The crucial part \(x-y^{\prime}+\theta^{\star}=\rho(x-f_{\theta^{*}}(x)+\theta^{\star})+(1-\rho) (x-y+\theta^{\star})\), and in fact

\[\mathbb{E}|x-y+\theta^{\star}|-\mathbb{E}|x-f_{\theta^{*}}(x)+\theta^{\star} |:=\epsilon_{0}>0\.\]

Similarly, we can get bounds with \(\rho\) (see \(C_{1}^{\prime}\) in Section B):

\[\mathbb{E}\left[\mathcal{L}(f_{\theta_{t}})-\mathcal{L}(f_{\theta ^{*}})\right] \leq C_{1}^{\prime}\mathbb{E}\left[(1-\eta)^{t}|\theta_{0}-\theta ^{\star}|+(1-(1-\eta)^{t})|x-y^{\prime}+\theta^{\star}|\right]\] \[\leq C_{2}(1-\eta)^{t}+C_{1}^{\prime}(1-(1-\eta)^{t})\left[\rho \cdot\mathbb{E}|x-f_{\theta^{*}}(x)+\theta^{\star}|\right.\] \[\quad\quad\quad\left.+(1-\rho)\cdot\mathbb{E}|x-y+\theta^{\star} |\right]\] \[\leq C_{2}(1-\eta)^{t}+(1-(1-\eta)^{t})(C_{3}-C_{4}\rho)\]

where \(C_{2}=C_{1}^{\prime}\mid\theta_{0}-\theta^{\star}|\), \(C_{3}=C_{1}^{\prime}\ \mathbb{E}|x-y+\theta^{\star}|\), \(C_{4}=C_{1}^{\prime}\ \epsilon_{0}>0\). 

### Nonlinear case

To see the impact of modifying \(y\) more clearly, we directly set \(\rho=1\) and conduct a similar analysis as in Section B.4. Let's still focus on \(\boldsymbol{\theta}^{[1]}\) and use the same assumptions in Section B.4, then we have:

\[\boldsymbol{\theta}^{[1]}_{t+1}=\boldsymbol{\theta}^{[1]}_{t}-\eta\frac{ \partial L}{\partial\boldsymbol{\theta}^{[1]}},\]

\[\frac{\partial L}{\partial\boldsymbol{\theta}^{[1]}}=(f_{\boldsymbol{\theta}} (\mathbf{x})-f_{\boldsymbol{\theta^{*}}}(\mathbf{x}))\sigma(1-\sigma)\text{ ReLU}\left(\boldsymbol{\theta}^{[2]}\mathbf{x}+\boldsymbol{\theta}^{[3]}\right)\]

Note \(y\) is replaced by \(f_{\boldsymbol{\theta^{*}}}(\mathbf{x})\). For instance \(\mathbf{x}\) is drawn from \(\mathcal{N}_{0}\) (\(y=0\)) but strays far away from the \(\mu_{1}\), causing \(\boldsymbol{\theta}^{[2]}\mathbf{x}+\boldsymbol{\theta}^{[3]}>0\). In this situation \(f_{\boldsymbol{\theta^{*}}}\) is likely to regard \(\mathbf{x}\) as a sample from \(\mathcal{N}_{1}\) (i.e. \(f_{\boldsymbol{\theta^{*}}}(\mathbf{x})\) close to 1) thus making \(\boldsymbol{\theta}^{[1]}\) to go in the right direction instead of the opposite. This explains why larger \(\rho\) can make convergence faster.

Proof of Theorem 3

We follow some proof steps in [5]. Let's begin by introducing some notations used in this section.

**Notation and Setup**\(\mathcal{X}\) is the input space, \(\mathcal{D}_{S}\) and \(\mathcal{D}_{T}\) are two distributions over \(\mathcal{X}\). Let \(\mathcal{Y}=\{0,1\}\). \(\mathcal{H}\) denotes a hypothesis class from \(\mathcal{X}\) to \(\mathcal{Y}\). To simplify notations, \(\forall h,f\in\mathcal{H}\) let \(\epsilon_{S}(h,f)=E_{\mathbf{x}\sim\mathcal{D}_{S}}[\mathbf{1}(h(\mathbf{x}) \neq f(\mathbf{x}))]\), and \(\hat{\epsilon}_{S}(h,f)\) be empirical error (\(\epsilon_{T}(h,f)\), \(\hat{\epsilon}_{T}(h,f)\) similar).

Then we introduce some concepts and lemmas, most of which are from [5].

**Definition 6** (\(\mathcal{H}\)-divergence).: _The \(\mathcal{H}\)-divergence between two distributions \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\) is defined as:_

\[d_{\mathcal{H}}(\mathcal{D},\mathcal{D}^{\prime})=2\sup_{h\in\mathcal{H}}| \text{Pr}_{\mathcal{D}}[I(h)]-\text{Pr}_{\mathcal{D}^{\prime}}[I(h)]|\]

_where \(I(h)=\{\mathbf{x}:h(\mathbf{x})=1\}\)._

**Definition 7** (Total Variation Distance).: _For two distributions \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\), the total variation distance of them is defined as:_

\[\text{D}_{\text{TV}}(\mathcal{D},\mathcal{D}^{\prime})=\sup_{A\subseteq \mathcal{F}}|\text{Pr}_{\mathcal{D}}(A)-\text{Pr}_{\mathcal{D}^{\prime}}(A)|\]

_where \(\mathcal{F}\) denotes the collection of all events in the probability space._

**Lemma 1**: **.** _For two distributions \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\), by definition it's easy to see that:_

\[\frac{1}{2}d_{\mathcal{H}}(\mathcal{D},\mathcal{D}^{\prime})\leq\text{D}_{ \text{TV}}(\mathcal{D},\mathcal{D}^{\prime})\]

**Definition 8** (Symmetric Difference Hypothesis Space).: \[\mathcal{H}\Delta\mathcal{H}:=\{g:\mathcal{X}\rightarrow\{0,1\}|g(\mathbf{x})= h(\mathbf{x})\oplus h^{\prime}(\mathbf{x})\quad\forall h,h^{\prime}\in \mathcal{H}\}\]

\(\oplus\) _denotes the XOR operation._

**Lemma 2**: **.**__

\[\forall h,h^{\prime}\in\mathcal{H},|\epsilon_{S}\left(h,h^{\prime}\right)- \epsilon_{T}\left(h,h^{\prime}\right)|\leq\frac{1}{2}d_{\mathcal{H}\Delta \mathcal{H}}\left(\mathcal{D}_{S},\mathcal{D}_{T}\right)\]

Proof.: only need note that \(h(\mathbf{x})\oplus h^{\prime}(\mathbf{x})=|h(\mathbf{x})-h^{\prime}(\mathbf{ x})|\), so

\[\sup_{g\in\mathcal{H}\Delta\mathcal{H}}|\text{Pr}_{\mathcal{D}_{S}}[I(g)]- \text{Pr}_{\mathcal{D}_{T}}[I(g)]|=\sup_{h,h^{\prime}\in\mathcal{H}}|\epsilon _{S}\left(h,h^{\prime}\right)-\epsilon_{T}\left(h,h^{\prime}\right)|\]

this is done by definition. 

With above notations we can derive a general proposition related with Theorem 3.

**Proposition 2**: **.** _Assuming \(\mathcal{H}\) is a hypothesis class from \(\mathcal{X}\) to \(\mathcal{Y}\) and \(\boldsymbol{\phi},\boldsymbol{\phi}^{\prime}\in\mathcal{H}\), we have:_

\[\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{T}}\left[\ell(\boldsymbol{\phi}( \mathbf{x}),\boldsymbol{\phi}^{\prime}(\mathbf{x}))\right]\leq\mathbb{E}_{ \mathbf{x}\sim\mathcal{D}_{S}}\left[\ell(\boldsymbol{\phi}(\mathbf{x}), \boldsymbol{\phi}^{\prime}(\mathbf{x}))\right]+\text{D}_{\text{TV}}(\mathcal{ D}_{S},\mathcal{D}_{T})\]

_where loss function is \(\ell(\hat{y},y):=\mathbf{1}(\hat{y}\neq y)\)._

Proof.: using the lemmas above,

\[\epsilon_{T}(\boldsymbol{\phi},\boldsymbol{\phi}^{\prime}) \leq\epsilon_{S}\left(\boldsymbol{\phi},\boldsymbol{\phi}^{\prime }\right)+\left|\epsilon_{T}\left(\boldsymbol{\phi},\boldsymbol{\phi}^{\prime }\right)-\epsilon_{S}\left(\boldsymbol{\phi},\boldsymbol{\phi}^{\prime}\right)\right|\] \[\leq\epsilon_{S}\left(\boldsymbol{\phi},\boldsymbol{\phi}^{\prime }\right)+\frac{1}{2}d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathcal{D}_{S}, \mathcal{D}_{T}\right)\] \[\leq\epsilon_{S}\left(\boldsymbol{\phi},\boldsymbol{\phi}^{\prime }\right)+\text{D}_{\text{TV}}(\mathcal{D}_{S},\mathcal{D}_{T})\]To obtain some useful inequalities of generalization bound, we need to introduce the Rademacher complexity.

**Definition 9** (Rademacher complexity of a function class): _Given a sample of points \(S=\{z_{1},z_{2},\ldots,z_{m}\}\subset Z\), and considering a function class \(\mathcal{F}\) of real-valued functions over \(Z\), the empirical Rademacher complexity of \(\mathcal{F}\) given \(S\) is defined as:_

\[\mathfrak{R}_{S}(\mathcal{F})=\frac{1}{m}\mathbb{E}_{\sigma}\left[\sup_{f\in \mathcal{F}}\sum_{i=1}^{m}\sigma_{i}f(z_{i})\right]\]

_where \(\sigma_{i}\) are independent and identically distributed Rademacher random variables. In other words, for \(i=1,2,\ldots,m\), the probability that \(\sigma_{i}=+1\) is equal to the probability that \(\sigma_{i}=-1\), and both are \(\frac{1}{2}\). Further, let \(P\) be a probability distribution over \(Z\). The Rademacher complexity of the function class \(\mathcal{F}\) with respect to \(P\) for sample size \(m\) is:_

\[\mathfrak{R}_{P,m}(\mathcal{F}):=\mathbb{E}_{S\sim P^{m}}\left[\mathfrak{R}_{ S}(\mathcal{F})\right]\]

**Lemma 3** (Generalization bound with Rademacher complexity): _Let \(\mathcal{F}\) be a family of loss functions \(\mathcal{F}=\{(x,y)\mapsto\ell((x,y),h):h\in\mathcal{H}\}\) with \(\ell((x,y),h)\in[0,1]\) for all \(\ell,(x,y)\) and \(h\). Then, with probability \(1-\delta\), the generalization gap is_

\[L(h)-\hat{L}(h)\leq 2\mathfrak{R}_{U}(\mathcal{F})+3\sqrt{\frac{\log(2/\delta )}{2n}}\,,\]

_for all \(h\in\mathcal{H}\) and samples \(U\) of size \(n\)._

The proof of this classical result could be found in most machine learning textbooks, like [46]. Since \(\ell(\cdot)\) is \(1\)-Lipschitz, we can derive that \(\mathfrak{R}_{U_{S}}(\ell\circ\mathcal{H})\leq\mathfrak{R}_{U_{S}}(\mathcal{H})\).

Theorem 3 involves the representation distance, below we prove the triangle inequality for representation distance.

**Lemma 4** (Triangle inequality for Representation distance): _for any functions \(\phi_{S},\phi_{T},\phi_{U}\) and distribution \(D\), we have:_

\[\text{\rm D}_{\text{\rm Rep}}(\phi_{S}\to\phi_{T};D)\leq\text{\rm D}_{\text{ \rm Rep}}(\phi_{S}\to\phi_{U};D)+\text{\rm D}_{\text{\rm Rep}}(\phi_{U}\to \phi_{T};D)\]

Proof.: Let's denote:

\[\begin{array}{l}d_{1}=\text{\rm D}_{\text{\rm Rep}}(\phi_{S}\to\phi_{U};D)\\ d_{2}=\text{\rm D}_{\text{\rm Rep}}(\phi_{U}\to\phi_{T};D)\end{array}\]

By definition:

\[\begin{array}{l}d_{1}=\inf_{W_{1}\in\mathbb{R}^{m\times n},b_{1}\in\mathbb{R }^{m}}\mathbb{E}_{x\sim D}\ell(W_{1}\phi_{S}(x)+b_{1},\phi_{U}(x))\\ d_{2}=\inf_{W_{2}\in\mathbb{R}^{m\times n},b_{2}\in\mathbb{R}^{m}}\mathbb{E}_{x \sim D}\ell(W_{2}\phi_{U}(x)+b_{2},\phi_{T}(x))\end{array}\]

We need to show:

\[\text{\rm D}_{\text{\rm Rep}}(\phi_{S}\to\phi_{T};D)\leq d_{1}+d_{2}\]

Consider the composition of the transformations:

\[\phi_{S}(x)\xrightarrow{W_{1},b_{1}}\phi_{U}(x)\xrightarrow{W_{2},b_{2}} \phi_{T}(x)\]

The combined transformation can be written as:

\[W_{2}(W_{1}\phi_{S}(x)+b_{1})+b_{2}=W_{2}W_{1}\phi_{S}(x)+W_{2}b_{1}+b_{2}\]

Using the properties of the loss function \(\ell\), we can write:

\[\ell(W_{2}W_{1}\phi_{S}(x)+W_{2}b_{1}+b_{2},\phi_{T}(x))\leq\ell(W_{1}\phi_{S} (x)+b_{1},\phi_{U}(x))+\ell(W_{2}\phi_{U}(x)+b_{2},\phi_{T}(x))\]This inequality holds for the reason that it can only break when the two items at right-hand side are both 0, in which the left side is also 0 due to rule of composition. Thus the inequality holds for all \(x\) and \(W_{1},b_{1},W_{2},b_{2}\).

Taking expectations over \(x\sim D\):

\[\mathbb{E}_{x\sim D}\ell(W_{2}W_{1}\phi_{S}(x)+W_{2}b_{1}+b_{2},\phi_{T}(x)) \leq\mathbb{E}_{x\sim D}\ell(W_{1}\phi_{S}(x)+b_{1},\phi_{U}(x))+\mathbb{E}_{x \sim D}\ell(W_{2}\phi_{U}(x)+b_{2},\phi_{T}(x))\]

Taking the infimum over \(W_{1},b_{1}\) and \(W_{2},b_{2}\):

\[\inf_{W_{1},b_{1},W_{2},b_{2}}\mathbb{E}_{x\sim D}\ell(W_{2}W_{1}\phi_{S}(x)+W_ {2}b_{1}+b_{2},\phi_{T}(x))\leq d_{1}+d_{2}\]

Since the left-hand side is an upper bound for \(\text{D}_{\text{Rep}}(\phi_{S}\to\phi_{T};D)\), we have:

\[\text{D}_{\text{Rep}}(\phi_{S}\to\phi_{T};D)\leq d_{1}+d_{2}\]

Therefore, the triangle inequality holds for the metric \(\text{D}_{\text{Rep}}\). 

Now we are ready to prove Theorem 3.

Proof.: For arbitrary \(W,b\), with probability \(1-\delta\):

\[\text{D}_{\text{Rep}}(\bm{\phi}_{\bm{\theta}}\to\bm{\phi}^{\star};X)\leq \epsilon_{X}(W\bm{\phi}_{\bm{\theta}}+b,\bm{\phi}^{\star})\leq\epsilon_{D_{X}} (W\bm{\phi}_{\bm{\theta}}+b,\bm{\phi}^{\star})+2\mathfrak{R}_{D_{X}}(\Phi)+3 \sqrt{\frac{\log(2/\delta)}{2\left|D_{X}\right|}}\]

Taking the infimum over \(W,b\):

\[\text{D}_{\text{Rep}}(\bm{\phi}_{\bm{\theta}}\to\bm{\phi}^{\star};X)\leq \text{D}_{\text{Rep}}(\bm{\phi}_{\bm{\theta}}\to\bm{\phi}^{\star};D_{X})+2 \mathfrak{R}_{D_{X}}(\Phi)+3\sqrt{\frac{\log(2/\delta)}{2\left|D_{X}\right|}}\]

Using the triangle inequality for representation distance:

\[\text{D}_{\text{Rep}}(\bm{\phi}_{\bm{\theta}}\to\bm{\phi}^{\star};D_{X})\leq \text{D}_{\text{Rep}}(\bm{\phi}_{\bm{\theta}}\to\psi;D_{X})+\text{D}_{\text{ Rep}}(\psi\to\bm{\phi}^{\star};D_{X})\]

For arbitrary \(W,b\), using the proposition above,

\[\text{D}_{\text{Rep}}(\bm{\phi}_{\bm{\theta}}\to\psi;D_{X})\leq\epsilon_{D_{X }}(W\bm{\phi}_{\bm{\theta}}+b,\psi)\leq\epsilon_{S_{X}}(W\bm{\phi}_{\bm{ \theta}}+b,\psi)+\text{D}_{\text{TV}}(S_{X},D_{X})\]

Taking the infimum over \(W,b\):

\[\text{D}_{\text{Rep}}(\bm{\phi}_{\bm{\theta}}\to\psi;D_{X})\leq\text{D}_{ \text{Rep}}(\bm{\phi}_{\bm{\theta}}\to\psi;S_{X})+\text{D}_{\text{TV}}(S_{X}, D_{X})\]

Combining the above results, we have:

\[\text{D}_{\text{Rep}}(\bm{\phi}_{\bm{\theta}}\to\bm{\phi}^{\star };X) \leq\text{D}_{\text{Rep}}(\psi\to\bm{\phi}^{\star};D_{X})+\text{D}_{ \text{Rep}}(\bm{\phi}_{\bm{\theta}}\to\psi;S_{X})+2\mathfrak{R}_{D_{X}}(\Phi) +3\sqrt{\frac{\log(2/\delta)}{2\left|D_{X}\right|}}\] \[+\text{D}_{\text{TV}}(S_{X},D_{X})\]

**Impact of initialization** In this part we leverage the Neural Tangent Kernel (NTK) framework [29] to deduce that a neural network initialized closer to a target function \(f^{\star}\) converges faster during training. Under the NTK regime, neural networks exhibit linearized training dynamics, allowing us to predict how the network's output evolves during training.

In the context of supervised learning, consider a neural network with parameters \(\theta\) and output function \(f(x;\theta)\), where \(x\) is the input. The goal is to approximate a target function \(f^{\star}(x)\) by minimizing a loss function, typically the mean squared error (MSE):

\[L(\theta)=\frac{1}{2}\sum_{i=1}^{n}(f(x_{i};\theta)-y_{i})^{2}\]where \(\{(x_{i},y_{i})\}_{i=1}^{n}\) is the training data and \(y_{i}=f^{\star}(x_{i})\).

Under gradient descent with learning rate \(\eta\), the parameter updates are:

\[\theta_{t+1}=\theta_{t}-\eta\nabla_{\theta}L(\theta_{t})\]

In the NTK regime, where the network width tends to infinity, the network's output evolves linearly with respect to the parameters around initialization \(\theta_{0}\)[38]. We can approximate:

\[f(x;\theta)\approx f(x;\theta_{0})+\nabla_{\theta}f(x;\theta_{0})^{\top}( \theta-\theta_{0})\]

This linearization allows us to express the evolution of the network's output as:

\[f_{t}(x)=f_{0}(x)-\eta\sum_{s=0}^{t-1}\sum_{i=1}^{n}K(x,x_{i})(f_{s}(x_{i})-y_ {i})\]

where \(f_{t}(x)=f(x;\theta_{t})\) is the network output at time \(t\) and \(K(x,x^{\prime})=\nabla_{\theta}f(x;\theta_{0})^{\top}\nabla_{\theta}f(x^{ \prime};\theta_{0})\) is the Neural Tangent Kernel.

In continuous time (gradient flow), the training dynamics can be described by a differential equation:

\[\frac{df_{t}(x)}{dt}=-\sum_{i=1}^{n}K(x,x_{i})(f_{t}(x_{i})-y_{i})\]

Vectorizing over all training inputs, we have:

\[\frac{d\mathbf{f}_{t}}{dt}=-\mathbf{K}(\mathbf{f}_{t}-\mathbf{y})\]

where \(\mathbf{f}_{t}=[f_{t}(x_{1}),f_{t}(x_{2}),\dots,f_{t}(x_{n})]^{\top}\),\(\mathbf{y}=[y_{1},y_{2},\dots,y_{n}]^{\top}\) and \(\mathbf{K}\) is the NTK matrix with entries \(K_{ij}=K(x_{i},x_{j})\). This differential equation has the solution:

\[\mathbf{f}_{t}-\mathbf{y}=e^{-\mathbf{K}t}(\mathbf{f}_{0}-\mathbf{y})\]

This equation shows that the error \(\mathbf{f}_{t}-\mathbf{y}\) decays exponentially over time, with the rate governed by the NTK matrix \(\mathbf{K}\). Since \(\mathbf{K}\) is symmetric, we can decompose \(\mathbf{K}\) into its eigenvalues \(\{\lambda_{j}\}\) and corresponding eigenvectors \(\{\mathbf{v}_{j}\}\):

\[\mathbf{K}=\sum_{j}\lambda_{j}\mathbf{v}_{j}\mathbf{v}_{j}^{\top}\]

the solution becomes:

\[\mathbf{f}_{t}-\mathbf{y}=\sum_{j}e^{-\lambda_{j}t}c_{j}\mathbf{v}_{j}\]

where \(c_{j}=\mathbf{v}_{j}^{\top}(\mathbf{f}_{0}-\mathbf{y})\). The rate of convergence for each component of the error is proportional to the corresponding eigenvalue \(\lambda_{j}\) of the NTK. Larger eigenvalues lead to faster decay. Initial error matters: the initial error \(\mathbf{f}_{0}-\mathbf{y}\) scales the amplitude of the exponential terms. A smaller initial error means the network starts closer to the target function, reducing the time required for the error to decay to a specific threshold.

Suppose we want the error to be less than \(\epsilon\):

\[\|\mathbf{f}_{t}-\mathbf{y}\|\leq\epsilon\]

Using the solution:\[\|\mathbf{f}_{t}-\mathbf{y}\|\leq\|e^{-\lambda_{\text{min}}t}\|\|\mathbf{f}_{0}- \mathbf{y}\|\]

where \(\lambda_{\text{min}}\) is the smallest (positive) eigenvalue of \(\mathbf{K}\). Solving for time \(t\):

\[t\geq\frac{1}{\lambda_{\text{min}}}\ln\left(\frac{\|\mathbf{f}_{0}-\mathbf{y} \|}{\epsilon}\right)\]

By leveraging the NTK framework, we've shown that a better initialization(meaning the neural network's initial output function is closer to the target function \(f^{\star}\)) leads to faster convergence during training. This is because the training dynamics under NTK are linear, and the exponential error decay is directly influenced by the magnitude of the initial error. Analysis above implies that if we can train a network that is closer to the target function, it will converge faster to the target function.

### Relation between data distribution and Rademacher complexity

Now let's look at the Rademacher complexity appeared above more carefully. Let 1-Lipschitz positive homogeneous activation \(\sigma_{i}\) be given, and

\[\mathcal{H}:=\left\{\mathbf{x}\mapsto\sigma_{L}\left(W_{L}\sigma_{L-1}\left( \cdots\sigma_{1}\left(W_{1}\mathbf{x}\right)\cdots\right)\right):\|W_{i}\|_{ \text{F}}\leq B,\mathbf{x}\in\mathbb{R}^{d}\right\}.\]

Then using Theorem 1 in [23], for samples \(S\) of size \(m\) we have bound for the empirical Rademacher complexity:

\[\mathfrak{R}_{S}(\mathcal{H})\leq\frac{1+\sqrt{2L\ln 2}}{m}B^{L}\|X\|_{\text{F}}\]

where \(X\in\mathbb{R}^{d\times m}\) is the input data matrix, and \(\|\cdot\|_{\text{F}}\) denotes the Frobenius norm.

If we further assume that the data are drawn from the distribution \(G\) (stated in Section 3.2) with covariance \(\Sigma\) (for simplicity let \(\mu_{1}=-\mu,\mu_{2}=\mu\)), then we can bound the Rademacher complexity of \(\mathcal{H}\) with respect to \(G\):

\[\mathfrak{R}_{G,m}(\mathcal{H}) =\mathbb{E}_{S\sim G^{m}}\left[\mathfrak{R}_{S}(\mathcal{H})\right]\] \[\leq\frac{1+\sqrt{2L\ln 2}}{m}B^{L}\ \mathbb{E}_{S\sim G^{m}}\|X\|_{ \text{F}}\] \[=\frac{\sqrt{2}+2\sqrt{L\ln 2}}{m}B^{L}\cdot\Sigma\ \Gamma\left(\frac{1+dm}{2}\right)\ M\left(-\frac{1}{2},\frac{dm}{2},-\frac{ dm\mu^{2}}{2\Sigma^{2}}\right)/\Gamma\left(\frac{dm}{2}\right)\]

The right part of the last inequality is an increasing function with respect to \(\Sigma\), and \(\Gamma(\cdot)\) denotes the gamma function, \(M(\cdot,\cdot,\cdot)\) is the Kummer's confluent hypergeometric function, given by:

\[M(a,b,z)=\sum_{n=0}^{\infty}\frac{a^{(n)}z^{n}}{b^{(n)}n!}={}_{1}F_{1}(a;b;z),\]

where:

\[a^{(0)} =1,\] \[a^{(n)} =a(a+1)(a+2)\cdots(a+n-1),\]

is the rising factorial.

**Remark.** Usually, the Rademacher complexity \(\mathfrak{R}_{S}(\ell\circ\mathcal{F})\) could be bounded by of \(\mathfrak{R}_{S}(\mathcal{F})\). For example, if \(\ell\) is \(L\)-lipschitz, then \(\mathfrak{R}_{S}(\ell\circ\mathcal{F})\leq L\cdot\mathfrak{R}_{S}(\mathcal{F})\). That's why we directly compute the Rademacher complexity of \(\mathcal{H}\) instead of \(\ell\circ\mathcal{H}\).

## Appendix E Explanation of Rescaling Samples

Dataset distillation seeks to create a condensed dataset that allows models to achieve performance comparable to those trained on the full dataset, but with fewer training steps. In this section, we will demonstrate that rescaling the variance of Gaussian distributions, as defined in Definition 3, does not affect the optimal performance of models trained on these rescaled data.

Proof.: Consider two Gaussian distributions \(\mathcal{N}_{0}(\mu_{1},\sigma^{2}\mathbf{I})\) and \(\mathcal{N}_{1}(\mu_{2},\sigma^{2}\mathbf{I})\) with means \(\mu_{1}=1\) and \(\mu_{2}=2\), and variance \(\sigma^{2}\). We define the bimodal mixture distribution \(G=(G_{X},G_{Y})\) such that \((\mathbf{x},y)\) is sampled according to:

\[\mathbf{x}=(1-y)\cdot\mathbf{x}_{0}+y\cdot\mathbf{x}_{1},\quad\text{with} \quad y\sim\text{Bernoulli}(0.5),\quad\mathbf{x}_{0}\sim\mathcal{N}_{0},\quad \mathbf{x}_{1}\sim\mathcal{N}_{1}.\]

The decision rule for optimal classification is determined by the likelihood ratio test. For a given sample \(\mathbf{x}\), the log-likelihood ratio \(\Lambda(\mathbf{x})\) is given by:

\[\Lambda(\mathbf{x})=\log\frac{P(\mathbf{x}\mid y=1)}{P(\mathbf{x}\mid y=0)}.\]

Since \(\mathbf{x}_{0}\) and \(\mathbf{x}_{1}\) are drawn from Gaussian distributions, their probability density functions are:

\[P(\mathbf{x}\mid y=0)=\frac{1}{(2\pi\sigma^{2})^{d/2}}\exp\left(-\frac{1}{2 \sigma^{2}}\|\mathbf{x}-\mu_{1}\|^{2}\right),\]

\[P(\mathbf{x}\mid y=1)=\frac{1}{(2\pi\sigma^{2})^{d/2}}\exp\left(-\frac{1}{2 \sigma^{2}}\|\mathbf{x}-\mu_{2}\|^{2}\right).\]

Substituting these into the log-likelihood ratio, we have:

\[\Lambda(\mathbf{x})=\log\frac{\frac{1}{(2\pi\sigma^{2})^{d/2}}\exp\left(- \frac{1}{2\sigma^{2}}\|\mathbf{x}-\mu_{2}\|^{2}\right)}{\frac{1}{(2\pi\sigma^ {2})^{d/2}}\exp\left(-\frac{1}{2\sigma^{2}}\|\mathbf{x}-\mu_{1}\|^{2}\right)}.\]

Simplifying, we obtain:

\[\Lambda(\mathbf{x})=-\frac{1}{2\sigma^{2}}\|\mathbf{x}-\mu_{2}\|^{2}+\frac{1} {2\sigma^{2}}\|\mathbf{x}-\mu_{1}\|^{2}.\]

Further simplification gives:

\[\Lambda(\mathbf{x})=\frac{1}{2\sigma^{2}}\left(\|\mathbf{x}-\mu_{1}\|^{2}-\| \mathbf{x}-\mu_{2}\|^{2}\right).\]

Since the optimal decision threshold for balanced classes (i.e., \(P(y=1)=P(y=0)=0.5\)) is \(\Lambda(\mathbf{x})=0\), we set:

\[\|\mathbf{x}-\mu_{1}\|^{2}-\|\mathbf{x}-\mu_{2}\|^{2}=0.\]

Expanding and rearranging, we derive:

\[\|\mathbf{x}\|^{2}-2\mu_{1}\mathbf{x}+\mu_{1}^{2}-\|\mathbf{x}\|^{2}+2\mu_{2} \mathbf{x}-\mu_{2}^{2}=0.\]

This simplifies to:

\[2(\mu_{2}-\mu_{1})\mathbf{x}+(\mu_{1}^{2}-\mu_{2}^{2})=0,\]

\[2(\mu_{2}-\mu_{1})\mathbf{x}=\mu_{2}^{2}-\mu_{1}^{2},\]

\[\mathbf{x}=\frac{\mu_{2}^{2}-\mu_{1}^{2}}{2(\mu_{2}-\mu_{1})}.\]

Solving yields:

\[\mathbf{x}=\frac{\mu_{2}+\mu_{1}}{2}.\]

Therefore, the optimal decision boundary is \(x=1.5\), which is independent of the variance \(\sigma^{2}\). This completes the proof. 

Regarding efficiency, utilizing scaled data to train similar models with fewer training steps is proven in Section B. Additionally, rescaling each Gaussian distribution preserves their means, which aligns with the objectives of conventional distribution matching-based dataset distillation methods. These methods aim to distill data while maintaining the distributional properties, specifically their means.

Detailed Methodology of ReLA-D

Recall that \(\bm{\phi}_{\bm{\theta}}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}\). We then introduce a transport matrix \(\mathbf{W}\in\mathbb{R}^{m\times n}\) and define the combined model as \(\mathbf{W}\bm{\phi}_{\bm{\theta}}(\cdot)\in\mathbb{R}^{m}\). During the training phase, the parameters \(\mathbf{W}\) and \(\bm{\theta}\) are jointly optimized. However, as the dimension \(m\) increases, the computational complexity of the optimization grows rapidly. To address this issue, we propose reducing the dimensionality of the target matrix \(R_{\mathbf{Y}}\) from \(m\) to \(n\), where \(n<m\):

\[R^{\prime}_{\mathbf{Y}}\!=\!V_{n}^{\top}\left(\frac{R\mathbf{Y}-\mu}{\sigma} \right)\ \ \text{s.t.}\ \ \frac{1}{|R_{\mathbf{Y}}|-1}\left((R_{\mathbf{Y}}-\mu)^{\top}(R_{\mathbf{Y}}- \mu)\right)=V\Lambda V^{\top},\ V_{n}=V[;.:n]\,,\] (10)

where \(\mu\) and \(\sigma\) denote the mean and standard deviation, respectively, of each column in the \(R_{\mathbf{Y}}\). Practically, we use batch PCA to perform the computation shown in (10), as illustrated in L.

### Proof for Ideal Properties of Prior Models

We aim to demonstrate that modern deep learning methods can effectively train models to serve as robust prior models by extracting sufficient information from samples as representations.

Therefore, we posit the existence of a prior model \(\xi\) capable of losslessly extracting the information of samples \(D_{\mathcal{X}}\) when trained using the InfoNCE loss [47], a method prevalently employed in contemporary deep learning algorithms [12].

Proof.: To demonstrate that an encoder \(\xi\) trained with the InfoNCE loss preserves all information from the input data \(D_{\mathcal{X}}\), we proceed as follows.

**1. Definitions and Setup**

Let \(\mathcal{X}\) denote the input data space with data distribution \(p_{\text{data}}(x)\). Let \(p_{\text{pos}}(x,x^{+})\) denote the distribution of positive pairs, typically generated via data augmentation. The encoder \(\xi:\mathcal{X}\rightarrow\mathbb{R}^{d}\) maps inputs to \(d\)-dimensional representations. The similarity function \(q:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}\) (e.g., dot product) measures similarity between representations. Given a positive pair \((x,x^{+})\), let \(\{x_{i}^{-}\}_{i=1}^{N}\) be \(N\) negative samples drawn i.i.d. from \(p_{\text{data}}(x)\). The InfoNCE loss is defined as:

\[\mathcal{L}_{\text{InfoNCE}}(\xi,q)=-\mathbb{E}_{(x,x^{+})\sim p_{\text{pos}} }\left[\log\frac{e^{q(\xi(x),\xi(x^{+}))/\tau}}{\sum_{i=1}^{N}e^{q(\xi(x), \xi(x_{i}^{-}))/\tau}}\right]\]

where \(\tau>0\) is a temperature parameter.

**2. InfoNCE as a Mutual Information Lower Bound**

The InfoNCE objective serves as a lower bound to the mutual information between representations of positive pairs:

\[I(\xi(X);\xi(X^{+}))\geq\mathcal{J}(\xi,q)\]

where

\[\mathcal{J}(\xi,q)=\mathbb{E}_{(x,x^{+})\sim p_{\text{pos}}}\left[q(\xi(x), \xi(x^{+}))-\tau\cdot\log\left(\sum_{i=1}^{N}e^{q(\xi(x),\xi(x_{i}^{-}))/\tau} \right)\right]\]

As the number of negative samples \(N\rightarrow\infty\), this bound becomes tight, approaching the true mutual information \(I(\xi(X);\xi(X^{+}))\). **3. Optimal Encoder and Similarity Function** Let \((\xi^{*},q^{*})\) denote the optimal encoder and similarity function that maximize \(\mathcal{J}(\xi,q)\). Under the assumption of infinite negative samples and an expressive similarity function, the optimal similarity satisfies:

\[q^{*}(\xi^{*}(x),\xi^{*}(x^{+}))=\log\frac{p_{\text{pos}}(x,x^{+})}{p_{\text{ data}}(x)p_{\text{data}}(x^{+})}+C\]

where \(C\) is a constant independent of \((x,x^{+})\). This aligns \(q^{*}\) with the pointwise mutual information (PMI) between \(x\) and \(x^{+}\). **4. Injectivity through Mutual Information Maximization** Maximizing \(I(\xi(X);\xi(X^{+}))\) encourages the encoder \(\xi\) to capture as much information about \(X\) as possible. To ensure injectivity:

* **Sufficient Dimensionality:** The representation dimension \(d\) must be at least as large as the intrinsic dimensionality of \(\mathcal{X}\).

* **Expressive Architecture:** The encoder \(\xi\) should be sufficiently expressive, potentially utilizing architectural constraints (e.g., invertible networks) to promote injectivity.

Under these conditions, maximizing mutual information implies that \(\xi^{*}\) approximates an injective mapping on the support of \(p_{\text{data}}(x)\), i.e., \(\xi^{*}(x)=\xi^{*}(x^{\prime})\Rightarrow x=x^{\prime}\) almost surely. **5. Existence of the Inverse Mapping** Given that \(\xi^{*}\) is injective, there exists a deterministic inverse mapping \(g:\mathbb{R}^{d}\rightarrow\mathcal{X}\) such that:

\[g(\xi^{*}(x))=x\quad\text{for all }x\in\mathcal{X}\]

This mapping \(g\) can be constructed as the inverse of \(\xi^{*}\) on its image:

\[g(z)=\xi^{*-1}(z)\quad\text{where }z\in\xi^{*}(\mathcal{X})\]

## Appendix G Analysis of Different Self-Supervised Learning Methods

We refer to the primary theoretical results for existing self-supervised learning methods as presented in [59], where all methods are unified under a simple and cohesive framework, detailed below. Specifically, UniGrad [59] demonstrates that most self-supervised learning methods can be unified by analyzing their gradients.

### A Unified Framework for SSL

A typical self-supervised learning framework employs a siamese network with two branches: an online branch and a target branch. The target branch serves as the training target for the online branch.

Given an input image \(x\), two augmented views \(x_{1}\) and \(x_{2}\) are generated, serving as inputs for the two branches. The encoder \(f(\cdot)\) extracts representations \(u_{i}\triangleq f(x_{i})\) for \(i=1,2\) from these views.

Table 4 details the notations used. \(u_{1}\) and \(u_{2}\) denote the current training samples, while \(v\) denotes unspecified samples. \(u_{1}^{o}\) and \(v^{o}\) are representations from the online branch. Three types of target branches are widely used: 1) weight-sharing with the online branch (\(u_{2}^{s}\) and \(v^{s}\)); 2) weight-sharing but detached from gradient back-propagation (\(u_{2}^{d}\) and \(v^{d}\)); 3) a momentum encoder updated from the online branch (\(u_{2}^{m}\) and \(v^{m}\)). If unspecified, \(u_{2}^{t}\) and \(v^{t}\) are used. A symmetric loss is applied to the two augmented views, as described in [14].

\(\mathcal{V}\) represents the sample set in the current training step. Methods vary in constructing this set: \(\mathcal{V}_{\mathrm{batch}}\) includes all samples from the current batch, \(\mathcal{V}_{\mathrm{bank}}\) uses a memory bank storing previous samples, and \(\mathcal{V}_{\infty}\) includes all previous samples, potentially larger than a memory bank.

### Contrastive Learning Methods

Contrastive learning relies on negative samples to prevent representational collapse and enhance performance. Positive samples are derived from different views of the same image, while negative

\begin{table}
\begin{tabular}{l l} \hline \hline Notation & Meaning \\ \hline \(u_{1}\), \(u_{2}\) & current concerned samples \\ \(v\) & unspecified samples \\ \hline \(u_{1}^{o}\), \(v^{o}\) & samples from online branch \\ \(u_{2}^{t}\), \(v^{t}\) & samples from unspecified target branch \\ \(u_{2}^{s}\), \(v^{s}\) & samples from weight-sharing target branch \\ \(u_{2}^{u}\), \(v^{d}\) & samples from stop-gradient target branch \\ \(u_{2}^{u}\), \(v^{m}\) & samples from momentum-encoder target branch \\ \hline \(\mathcal{V}\) & unspecified sample set \\ \(\mathcal{V}_{\mathrm{batch}}\) & sample set of current batch \\ \(\mathcal{V}_{\mathrm{bank}}\) & sample set of memory bank \\ \(\mathcal{V}_{\infty}\) & sample set of all previous samples \\ \hline \hline \end{tabular}
\end{table}
Table 4: Notations used in this section.

samples come from other images. The goal is to attract positive pairs and repel negative pairs, typically using the InfoNCE loss [47]:

\[L=\mathop{\mathbb{E}}_{u_{1},u_{2}}\bigg{[}-\log\frac{\exp{(\cos(u_{1}^{o},u_{2} ^{t})/\tau)}}{\sum_{v^{t}\in\mathcal{V}}\exp{(\cos(u_{1}^{o},v^{t})/\tau)}} \bigg{]},\] (11)

where \(\cos(\cdot)\) denotes cosine similarity, and \(\tau\) is the temperature hyper-parameter. This formulation can be adapted for various methods, discussed below.

**MoCo [25, 13].** MoCo uses a momentum encoder for the target branch and a memory bank for storing previous representations. Negative samples are drawn from this memory bank. The gradient for sample \(u_{1}^{o}\) is:

\[\frac{\partial L}{\partial u_{1}^{o}}=\frac{1}{\tau N}\bigg{(}-u_{2}^{m}+\sum _{v^{m}\in\mathcal{V}_{\mathrm{bank}}}s_{v}v^{m}\bigg{)},\] (12)

where \(s_{v}=\frac{\exp{(\cos(u_{1}^{o},v^{m})/\tau)}}{\sum_{y^{m}\in\mathcal{V}_{ \mathrm{bank}}}\exp{(\cos(u_{1}^{o},y^{m})/\tau)}}\) and \(N\) is the number of samples in the batch.

**SimCLR [12].** SimCLR shares weights between the target and online branches and does not stop back-propagation. It uses all representations from other images in the batch as negative samples. The gradient is:

\[\begin{split}\frac{\partial L}{\partial u_{1}^{o}}=& \frac{1}{\tau N}\bigg{(}-u_{2}^{s}+\sum_{v^{s}\in\mathcal{V}_{ \mathrm{batch}}\backslash u_{1}^{o}}s_{v}v^{s}\bigg{)}\\ &+\underbrace{\frac{1}{\tau N}\bigg{(}-u_{2}^{s}+\sum_{v^{s}\in \mathcal{V}_{\mathrm{batch}}\backslash u_{1}^{o}}t_{v}v^{s}\bigg{)}}_{\text{ reduce to 0}},\end{split}\] (13)

where \(t_{v}=\frac{\exp{(\cos(v^{t},u_{1}^{o})/\tau)}}{\sum_{y^{d}\in\mathcal{V}_{ \mathrm{batch}}\backslash v^{s}}\exp{(\cos(v^{s},y^{s})/\tau)}}\). If the gradient through the target branch is stopped, the second term vanishes.

**Unified Gradient.** The gradient for these methods can be unified as:

\[\frac{\partial L}{\partial u_{1}^{o}}=\frac{1}{\tau N}\bigg{(}-u_{2}^{t}+\sum _{v^{t}\in\mathcal{V}}s_{v}v^{t}\bigg{)},\] (14)

comprising a weighted sum of positive and negative samples. The term \(-u_{2}^{t}\) pulls positive samples together, while \(\sum_{v^{t}\in\mathcal{V}}s_{v}v^{t}\) pushes negative samples apart. The main difference between methods lies in the target branch used and the construction of the contrastive sample set \(\mathcal{V}\).

### Asymmetric Network Methods

Asymmetric network methods learn representations by maximizing the similarity of positive pairs without using negative samples. These methods require symmetry-breaking network designs to avoid representational collapse. A predictor \(h(\cdot)\) is appended after the online branch, and the gradient to the target branch is stopped. The objective function is:

\[L=\mathop{\mathbb{E}}_{u_{1},u_{2}}\bigg{[}-\cos(h(u_{1}^{o}),u_{2}^{t})\bigg{]}.\] (15)

**Relation to BYOL [30].** BYOL uses a momentum encoder for the target branch, i.e., \(u_{2}^{t}=u_{2}^{m}\) in Eq.(15).

**Relation to Simsiam [14].** Simsiam shows that a momentum encoder is unnecessary and only applies the stop-gradient operation to the target branch, i.e., \(u_{2}^{t}=u_{2}^{d}\) in Eq.(15).

**Unified Gradient.** Despite the performance of asymmetric network methods, the avoidance of collapse solutions is not well understood. DirectPred [60] explores this by studying training dynamics and proposes an analytical solution for the predictor \(h(\cdot)\).

DirectPred formulates the predictor as \(h(v)=W_{h}v\), with \(W_{h}\) calculated based on the correlation matrix \(\mathbb{E}_{v}(vv^{T})\). The correlation matrix, \(F\), is computed as the moving average for each batch:\(F\triangleq\sum_{v^{o}\in\mathcal{V}_{v}}\rho_{v}v^{o}v^{T}\), where \(\rho_{v}\) is the moving average weight. Decomposing \(F\) into eigenvalues \(\Lambda_{F}\) and eigenvectors \(U\), \(W_{h}\) is:

\[W_{h}=U\Lambda_{h}U^{T},\ \ \Lambda_{h}=\Lambda_{F}^{1/2}+\epsilon\lambda_{max}I,\] (16)

where \(\lambda_{max}\) is the max eigenvalue of \(F\) and \(\epsilon\) is a hyper-parameter to boost small eigenvalues.

DirectPred also derives the gradient:

\[\frac{\partial L}{\partial u_{1}^{o}}=\frac{1}{||W_{h}u_{1}^{o}||_{2}N}\bigg{(} -W_{h}^{T}u_{2}^{t}+\lambda\sum_{v^{o}\in\mathcal{V}_{\infty}}(\rho_{v}u_{1}^{oT }v^{o})v^{o}\bigg{)},\] (17)

where \(-W_{h}^{T}u_{2}^{t}\) and \(\sum_{v^{o}\in\mathcal{V}_{\infty}}(\rho_{v}u_{1}^{oT}v^{o})v^{o}\) act as positive and negative gradients respectively, and \(\lambda=\frac{u_{1}^{oT}W_{h}^{t}u_{2}^{t}}{u_{1}^{T}(F+\epsilon I)u_{1}^{o}}\) is a balance factor.

Though negative samples are absent in the loss function, they emerge from the predictor network's optimization. The eigenspace of the predictor \(W_{h}\) aligns with the feature correlation matrix \(F\), encoding its information. During back-propagation, this encoded information functions as a negative gradient, influencing the optimization direction.

### Feature Decorrelation Methods

Feature decorrelation methods have recently emerged as a novel approach in self-supervised learning. These methods aim to reduce redundancy among different feature dimensions to prevent collapse. Various loss functions have been proposed for this purpose. We examine their relations below.

**Relation to Barlow Twins [69].** Barlow Twins employs the following loss function:

\[L=\sum_{i=1}^{C}\left(W_{ii}-1\right)^{2}+\lambda\sum_{i=1}^{C}\sum_{j\neq i} W_{ij}^{2},\] (18)

where \(W=\frac{1}{N}\sum_{v_{1}^{o},v_{2}^{o}\in\mathcal{V}_{\mathrm{batch}}}v_{1}^{o}v _{2}^{sT}\) is a cross-correlation matrix, \(C\) denotes the number of feature dimensions, and \(\lambda\) is a balancing hyper-parameter. The diagonal elements of \(W\) are encouraged to be close to \(1\), while the off-diagonal elements are forced towards \(0\).

Despite appearing different, Eq. (18) operates similarly to previous methods from a gradient perspective, calculated as:

\[\frac{\partial L}{\partial u_{1}^{o}}=\frac{2}{N}\left(-Au_{2}^{s}+\lambda \sum_{v_{1}^{o},v_{2}^{o}\in\mathcal{V}_{\mathrm{batch}}}\frac{u_{2}^{sT}v_{ 2}^{s}}{N}v_{1}^{o}\right),\] (19)

where \(A=I-(1-\lambda)W_{\mathrm{diag}}\) and \((W_{\mathrm{diag}})_{ij}=\delta_{ij}W_{ij}\) is the diagonal matrix of \(W\). Barlow Twins applies batch normalization instead of \(\ell_{2}\) normalization to the representation \(v\).

**Relation to VICReg [3]**. VICReg modifies Barlow Twins with the following loss function:

\[\begin{split} L=&\frac{1}{N}\sum_{v_{1}^{o},v_{2}^{ o}\in\mathcal{V}_{\mathrm{batch}}}||v_{1}^{o}-v_{2}^{s}||_{2}^{2}+\frac{ \lambda_{1}}{C}\sum_{i=1}^{C}\sum_{j\neq i}^{C}W_{ij}^{\prime 2}\\ &+\frac{\lambda_{2}}{C}\sum_{i=1}^{C}\max(0,\gamma-\mathrm{std}( v_{1}^{o})_{i}),\end{split}\] (20)

where \(W^{\prime}=\frac{1}{N-1}\sum_{v_{1}^{o}\in\mathcal{V}_{\mathrm{batch}}}(v_{1} ^{o}-\bar{v}_{1}^{o})(v_{1}^{o}-\bar{v}_{1}^{o})^{T}\) is the covariance matrix of the same view, \(\mathrm{std}(v)_{i}\) denotes the standard deviation of the \(i\)-th channel of \(v\), \(\gamma\) is a constant target value, and \(\lambda_{1}\), \(\lambda_{2}\) are balancing weights.

The gradient is derived as follows:

\[\begin{split}\frac{\partial L}{\partial u_{1}^{o}}=& \frac{2}{N}\left(-u_{2}^{s}+\lambda\sum_{v_{1}^{o}\in\mathcal{V}_{ \mathrm{batch}}}\frac{\tilde{u}_{1}^{qT}\tilde{v}_{1}^{o}}{N}\tilde{v}_{1}^{o} \right)\\ &+\underbrace{\frac{2\lambda}{N}\left(\frac{1}{\lambda}u_{1}^{o} -B\tilde{u}_{1}^{o}\right)}_{\text{reduces to }0},\end{split}\] (21)where \(\tilde{v}=v-\bar{v}\) is the de-centered sample, \(\lambda=\frac{2\lambda_{1}N^{2}}{c(N-1)^{2}}\), and \(B=\frac{N}{c\lambda(N-1)}(2\lambda_{1}W^{\prime}_{\mathrm{diag}}+\frac{\lambda_{2 }}{2}\mathrm{diag}(\mathbf{1}(\gamma-\mathrm{std}(v_{1}^{o})>0)\ \odot\mathrm{std}(v_{1}^{o})))\). Here, \(\mathrm{diag}(x)\) is a matrix with the vector \(x\) on its diagonal, \(\mathbf{1}(\cdot)\) is the indicator function, and \(\oslash\) denotes element-wise division.

VICReg does not normalize \(v\); instead, it uses de-centering and a standard deviation term in the loss function.

**Unified Gradient.** Given the equivalence of \(v^{s}\) and \(v^{o}\), the gradient for feature decorrelation methods can be unified as:

\[\frac{\partial L}{\partial u_{1}^{o}}=\frac{2}{N}\left(-u_{2}^{t}+\lambda \sum_{v^{o}\in V_{\mathrm{batch}}}\frac{u^{oT}v^{o}}{N}v_{1}^{o}\right),\] (22)

where \(-u_{2}^{t}\) is the positive gradient, and \(\sum_{v^{o}\in V_{\mathrm{batch}}}\left(\frac{u^{oT}v^{o}}{N}\right)v_{1}^{o}\) is the negative gradient. \(\lambda\) is a balancing factor. The difference between methods lies in the subscript for the negative coefficient. Feature decorrelation methods function similarly to other self-supervised methods, with the positive and negative gradients derived from the diagonal and off-diagonal elements of the correlation matrix.

## Appendix H Budget of ReLA for Data Synthesis

While training on the distilled dataset is both efficient and effective, the distillation process in optimization-based approaches [11, 72, 65] is computationally intensive [18, 56], often exceeding the computational load of training on the full dataset.

In contrast, the synthetic data generated by our ReLA framework requires a budget less than that of training for a single epoch (c.f. Section 4.1). This is because the synthesis budget of our ReLA is equivalent to performing inference over the entire dataset \(D_{X}\). Consequently, this computational expense is negligible given that training epochs typically number around \(100\).

## Appendix I ReLA in Labeled Dataset Distillation and Human-Supervised Learning

We apply our ReLA in labeled dataset distillation and human-supervised learning.

### Experimental Setup

**Datasets and Neural Network Architectures.** We conduct experiments on datasets of varying scales and resolutions.

* **Small-scale:** We evaluate on CIFAR-10 (\(32\times 32\)) [35] and CIFAR-100 (\(32\times 32\)) [34].
* **Large-scale:** We utilize Tiny-ImageNet (\(64\times 64\)) [36] and ImageNet-1K (\(224\times 224\)) [20].

Consistent with previous works on dataset distillation [67, 73, 24], we use ConvNet [24] and ResNet-[18,50][26] as our backbone networks across all datasets. Specifically, Conv-3 is employed for CIFAR-10/100, and Conv-4 for Tiny-ImageNet and ImageNet-1K.

**Baselines.** We compare our method with several SOTA distillation methods capable of scaling to large high-resolution datasets, including G-VBSM [53], SRe\({}^{2}\)L [67], and RDED [56]. To the best of our knowledge, SRe\({}^{2}\)L, G-VBSM, and RDED are the only published works that efficiently scale to datasets of any size, making them our closest baselines. All distilled datasets synthesized from these baselines undergo the same post-training process. Results are reported in Table 5. For our ReLA, the prior models used for distillation are identical to the pre-trained models employed in the baseline methods.

### Main Results

Table 5 demonstrates the superiority of our ReLA, despite incorporating a zero-cost sample synthesis process.

## Appendix J ReLA Algorithm

```
0: Number of training steps \(T\), Initial \(\ell_{s}=2.0\), Initial \(\ell_{f}=1.0\), Initial \(\lambda=1\)
1:for each training step \(t=1\)to \(T\)do
2:if\(\lambda=1\)then
3:\(\ell_{c}\leftarrow\) Value of \(\mathcal{L}_{\text{ReLA}}\) {Retrieve the current loss value from the optimization process}
4:\(\ell_{f}\gets 0.999\times\ell_{f}+0.001\times\ell_{c}\) {Calculate the short-term loss}
5:\(\ell_{s}\gets 0.99\times\ell_{s}+0.01\times\ell_{f}\) {Calculate the long-term loss}
6:endif
7:if\(\exp(-\max\{\ell_{s}-\ell_{f},0\})\geq 0.995\)then
8:\(\lambda\gets 0\) {ReLA learning is converged and over}
9:endif
10:\(\mathcal{L}\leftarrow\lambda\cdot\mathcal{L}_{\text{ReLA}}+(1-\lambda)\cdot \mathcal{L}_{\text{SSL}}\) {Control the ReLA and the SSL states using \(\lambda\)}
11:endfor ```

**Algorithm 1** Adaptive Loss Weighting Algorithm for ReLA and Self-Supervised Learning

In essence, this algorithm is designed to detect the convergence of ReLA. Upon convergence, it transitions to the original algorithm for self-supervised learning. This implicitly segments the learning procedure into two phases: the fast stage (ReLA) and the slow stage (SSL). Moreover, Figure 5 depicts the dynamics of the training process.

## Appendix K Experimental Details

### Detailed Setup for Experiments in Section 3.2

We show the detailed setup for the experiments in Section 3.2 in Tables 6 and 7.

Figure 5: **Comparison of training dynamics between ReLA and the original (Orig.) BYOL algorithm**.

\begin{table}
\begin{tabular}{c|c c c|c c c c c|c c c c} \hline \hline  & \multicolumn{3}{c|}{CoonNet} & \multicolumn{3}{c|}{ResNet-18} & \multicolumn{3}{c}{ResNet-50} \\ \hline Dataset & PC & G-VBSM & SRe2L & RDED & ReLA (Ours) & G-VBSM & SRe2L & RDED & ReLA (Ours) & G-VBSM & SRe2L & RDED & ReLA (Ours) \\ \hline \multirow{3}{*}{C-100} & 1 & 21.2 \(\pm\) 0.22 & 2.22 \(\pm\) 1.28 & 0.20 & **4.03 \(\pm\) 0.44** & 17.0 \(\pm\) 10.19 & 19.9 \(\pm\) 0.78 & 0.4 \(\pm\) 0.4 & **35.0 \(\pm\) 0.2** & 17.2 \(\pm\) 0.32 & 0.25 \(\pm\) 0.28 & 0.5 \(\pm\) 1.7 \(\pm\) 0.3 \\  & 10 & 38.6 \(\pm\) 0.39 & 39.6 \(\pm\) 0.56 & 0.41 & **67.0 \(\pm\) 0.4** & 36.3 \(\pm\) 0.79 & 43.5 \(\pm\) 0.5 & **74.7 \(\pm\) 0.4** & 33.8 \(\pm\) 1.37 & 31.3 \(\pm\) 1.37 & 26.4 \(\pm\) 6.5 \(\pm\) 1.5 & **70.9 \(\pm\) 1.3** \\  & 50 & 62.7 \(\pm\) 0.57 & 57.7 \(\pm\) 0.4 & 71.1 \(\pm\) 0.2 & **77.3 \(\pm\) 0.1** & 64.5 \(\pm\) 0.6 & 62.8 \(\pm\) 1.2 & 76.4 \(\pm\) 0.4 & **89.2 \(\pm\) 0.1** & 61.5 \(\pm\) 0.6 \(\pm\) 61.6 \(\pm\) 0.2 & 74.1 \(\pm\) 0.6 & **88.5 \(\pm\) 0.3** \\ \hline \multirow{3}{*}{C-100} & 1 & 34.4 \(\pm\) 0.3 & 12.9 \(\pm\) 0.12 & 0.18 \(\pm\) 0.4 & **32.5 \(\pm\) 0.3** & 13.3 \(\pm\) 0.15 & 11.3 \(\pm\) 0.15 & 11.4 \(\pm\) 0.4 & 6.1 \(\pm\) 0.4 & 11.7 \(\pm\) 1.2 & 12.6 \(\pm\) 0.6 & 10.1 \(\pm\) 0.4 & 12.5 \(\pm\) 0.2 & **27.3 \(\pm\) 1.5** \\  & 10 & 38.7 \(\pm\) 0.32 & 43.7 \(\pm\) 0.39 & 0.31 & **53.1 \(\pm\) 0.7** & 47.0 \(\pm\) 0.42 & 42.7 \(\pm\) 0.53 & **53.4 \(\pm\) 0.6** & 42.8 \(\pm\) 0.2 & 47.5 \(\pm\) 0.42 & 44.2 \(\pm\) 0.5 & 54.0 \(\pm\) 0.3 & 66.4 \(\pm\) 0.0 \\  & 50 & 53.8 \(\pm\) 0.45 & 52.2 \(\pm\) 0.53 & 0.52 & **56.3 \(\pm\) 0.3** & 60.0 \(\pm\) 0.1 & 57.4 \(\pm\) 0.24 & 6.0 \(\pm\) 0.4 & **68.8 \(\pm\) 0.2** & 62.2 \(\pm\) 0.36 & 60.6 \(\pm\) 0.2 \\ \hline \multirow{3}{*}{T-N} & 1 & 8.6 \(\pm\) 0.2 & 11.8 \(\pm\) 0.7 & **17.0 \(\pm\) 0.7** & **27.6 \(\pm\) 0.3** & 9.0 \(\pm\) 0.13 & 13.5 \(\pm\) 0.2 & 15.4 \(\pm\) 0.6 & **24.2 \(\pm\) 1.4** & 8.2 \(\pm\) 0.2 & 12.8 \(\pm\) 0.2 & 14.8 \(\pm\) 0.5 & **23.2 \(\pm\) 1

### Detailed Setup for Experiments in Section 5

Training details for self-supervised learning methods.We show the details of training in Table 8.

Linear evaluation details.We show the details of training the linear model in Table 9.

Linear evaluation details.We show the details of training the linear model in Table 9.

## Appendix L Batch PCA Reduction

To begin, we provide a full and rigorous mathematical framework for Principal Component Analysis (PCA).

### Principal Component Analysis

Given a data matrix \(\mathbf{Y}\in\mathbb{R}^{n\times d}\) and the desired number of components \(k\), where \(k\leq d\), PCA aims to extract a reduced data matrix \(\mathbf{Y}_{\text{reduced}}\in\mathbb{R}^{n\times k}\) that retains the maximum variance from the original dataset.

First, the data needs to be centered by subtracting the mean of each feature. This can be mathematically represented as:

\[\mathbf{Y}_{\text{centered}}=\mathbf{Y}-\frac{1}{n}\mathbf{1}_{n}\mathbf{1}_{ n}^{T}\mathbf{Y}\]

where \(\mathbf{1}_{n}\) is a column vector of ones with length \(n\). Next, the covariance matrix of the centered data is computed:

\[\mathbf{C}=\frac{1}{n-1}\mathbf{Y}_{\text{centered}}^{T}\mathbf{Y}_{\text{centered}}\]

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Layer** & **Type** & **Input Units** & **Output Units** \\ \hline Input Layer & - & 2 & - \\ \hline Hidden Layer & Fully Connected (Linear) & 2 & 50 \\  & Activation (ReLU) & - & - \\ \hline Output Layer & Fully Connected (Linear) & 50 & 1 \\  & Activation (Sigmoid) & - & - \\ \hline Loss Function & Mean Squared Error (MSE) & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Architecture of the simple neural network model.**

\begin{table}
\begin{tabular}{c c} \hline \hline
**Parameter** & **Value** \\ \hline Training Steps & 1000 \\ \hline Batch Size & 1 \\ \hline Optimizer & Stochastic Gradient Descent (SGD) \\ \hline Learning Rate & 0.002 \\ \hline Momentum & 0.98 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Optimization parameters for the simple neural network model.**

\begin{table}
\begin{tabular}{c c} \hline \hline
**Parameter** & **Value** \\ \hline Epochs & 100 \\ \hline Optimizer & AdamW \\ \hline Learning Rate & 0.001 \\ \hline Weight Decay & 0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Training parameters and optimizer settings for self-supervised learning methods.**To identify the principal components, eigendecomposition is performed on the covariance matrix:

\[\mathbf{C}=\mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^{T}\]

where \(\mathbf{V}\) is the matrix of eigenvectors and \(\boldsymbol{\Lambda}\) is a diagonal matrix of eigenvalues. The eigenvectors are then sorted in descending order based on their corresponding eigenvalues. The top \(k\) eigenvectors are selected to form the projection matrix:

\[\mathbf{W}=[\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{k}]\]

where \(\mathbf{v}_{i}\) represents the \(i\)-th eigenvector. Finally, the centered data is projected onto the new subspace:

\[\mathbf{Y}_{\text{reduced}}=\mathbf{Y}_{\text{centered}}\mathbf{W}\]

The resulting reduced data matrix \(\mathbf{Y}_{\text{reduced}}\) contains the principal components of the original data. Each principal component is a linear combination of the original features, designed to capture as much variance as possible in the reduced space.

```
0: Data matrix \(\mathbf{Y}\in\mathbb{R}^{n\times d}\), Number of components \(k\)
0: Reduced data matrix \(\mathbf{Y}_{\text{reduced}}\in\mathbb{R}^{n\times k}\)
1:\(n\leftarrow\) number of rows in \(\mathbf{Y}\)
2:\(m\leftarrow\) BATCHSIZE {Pre-set Batch size}
3:\(\mu\leftarrow\frac{1}{n}\sum_{i=1}^{n}\mathbf{Y}_{i,\text{ }}\){Compute the mean of \(\mathbf{Y}\)}
4:\(\mathbf{Y}_{\text{centered}}\leftarrow\mathbf{Y}-\mu\) {Center the data}
5:\(\boldsymbol{\Sigma}\leftarrow\mathbf{0}_{d\times d}\) {Initialize the covariance matrix}
6:for\(i=0\)to\(n\)step\(m\)do
7:\(j\leftarrow\min(i+m,n)\)
8:\(\mathbf{B}\leftarrow\mathbf{Y}_{\text{centered}}[i:j,\cdot]\)
9:\(\boldsymbol{\Sigma}\leftarrow\boldsymbol{\Sigma}+\mathbf{B}^{\top}\mathbf{B}\) {Update the covariance matrix}
10:endfor
11:\(\boldsymbol{\Sigma}\leftarrow\frac{1}{n-1}\) {Normalize the covariance matrix}
12:\(\mathbf{V}\leftarrow\) eigenvectors of \(\boldsymbol{\Sigma}\) corresponding to the largest \(k\) eigenvalues
13:\(\mathbf{Y}_{\text{reduced}}\leftarrow\mathbf{0}_{n\times k}\) {Initialize the reduced data matrix}
14:for\(i=0\)to\(n\)step\(m\)do
15:\(j\leftarrow\min(i+m,n)\)
16:\(\mathbf{B}\leftarrow\mathbf{Y}_{\text{centered}}[i:j,\cdot]\)
17:\(\mathbf{Y}_{\text{reduced}}[i:j,\cdot]\leftarrow\mathbf{B}\mathbf{V}\)
18:endfor
19:return\(\mathbf{Y}_{\text{reduced}}\) ```

**Algorithm 2** Batch PCA Reduction

Then we prove that the batch PCA we utilized here is exactly same to the standard PCA.

Proof.: To prove that the Batch PCA algorithm achieves the same result as the standard PCA, we need to show that the covariance matrix and the reduced data matrix computed by the Batch PCA are equivalent to those computed by the standard PCA.

First, let's show that the covariance matrix \(\boldsymbol{\Sigma}\) computed in the Batch PCA is equivalent to the covariance matrix \(\mathbf{C}\) in the standard PCA.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Dataset** & **Batch Size** & **Linear Model** & **Loss Function** \\ \hline CIFAR-10 & 128 & nn.Linear(_feature\_dim_, 10) & CrossEntropyLoss() \\ \hline CIFAR-100 & 128 & nn.Linear(_feature\_dim_, 100) & CrossEntropyLoss() \\ \hline Tiny-ImageNet & 128 & nn.Linear(_feature\_dim_, 200) & CrossEntropyLoss() \\ \hline ImageNet-1K & 1024 & nn.Linear(_feature\_dim_, 1000) & CrossEntropyLoss() \\ \hline Optimizer & \multicolumn{3}{c}{Adam (learning rate: 3e-4)} \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Linear evaluation parameters for various datasets.**In the standard PCA, the covariance matrix is computed as:

\[\mathbf{C}=\frac{1}{n-1}\mathbf{Y}_{\text{centered}}^{T}\mathbf{Y}_{\text{centered}}\]

In the Batch PCA, the covariance matrix is computed as:

\[\mathbf{\Sigma} =\frac{1}{n-1}\sum_{i=0}^{n-1}\mathbf{B}_{i}^{T}\mathbf{B}_{i}\] \[=\frac{1}{n-1}\left(\mathbf{B}_{0}^{T}\mathbf{B}_{0}+\mathbf{B}_ {1}^{T}\mathbf{B}_{1}+\ldots+\mathbf{B}_{b-1}^{T}\mathbf{B}_{b-1}\right)\] \[=\frac{1}{n-1}\mathbf{Y}_{\text{centered}}^{T}\mathbf{Y}_{\text{centered}}\]

where \(\mathbf{B}_{i}\) is the \(i\)-th batch of the centered data matrix \(\mathbf{Y}_{\text{centered}}\), and \(b\) is the number of batches.

Therefore, the covariance matrix computed by the Batch PCA is equivalent to the covariance matrix computed by the standard PCA.

Next, let's show that the reduced data matrix \(\mathbf{Y}_{\text{reduced}}\) computed in the Batch PCA is equivalent to the one computed in the standard PCA.

In the standard PCA, the reduced data matrix is computed as:

\[\mathbf{Y}_{\text{reduced}}=\mathbf{Y}_{\text{centered}}\mathbf{W}\]

where \(\mathbf{W}\) is the matrix of the top \(k\) eigenvectors of the covariance matrix.

In the Batch PCA, the reduced data matrix is computed as:

\[\mathbf{Y}_{\text{reduced}} =\left[\begin{array}{c}\mathbf{B}_{0}\mathbf{V}\\ \mathbf{B}_{1}\mathbf{V}\\ \vdots\\ \mathbf{B}_{b-1}\mathbf{V}\end{array}\right]\] \[=\left[\begin{array}{c}\mathbf{Y}_{\text{centered}}[0:m,\cdot] \\ \mathbf{Y}_{\text{centered}}[m:2m,\cdot]\\ \vdots\\ \mathbf{Y}_{\text{centered}}[(b-1)m:n,\cdot]\end{array}\right]\mathbf{V}\] \[=\mathbf{Y}_{\text{centered}}\mathbf{V}\]

where \(\mathbf{V}\) is the matrix of the top \(k\) eigenvectors of the covariance matrix \(\mathbf{\Sigma}\), and \(m\) is the batch size.

Since we have shown that the covariance matrices \(\mathbf{C}\) and \(\mathbf{\Sigma}\) are equivalent, their eigenvectors \(\mathbf{W}\) and \(\mathbf{V}\) are also equivalent. Therefore, the reduced data matrix computed by the Batch PCA is equivalent to the one computed by the standard PCA.

In conclusion, we have proven that the Batch PCA algorithm achieves the same result as the standard PCA by showing that the covariance matrix and the reduced data matrix computed by the Batch PCA are equivalent to those computed by the standard PCA. 

## Appendix M Analyze the Practical Data Augmentations

Here we presents a rigorous mathematical proof demonstrating that higher data augmentation intensity leads to increased overlap between samples from different classes. By modeling the sample distributions and augmentation process, we show that the variance of the augmented distributions increases with augmentation strength, resulting in wider and more overlapping distributions. We provide an exact calculation of the intersection point and approximate the overlap area using the cumulative distribution function of the standard normal distribution. Our theoretical analysis confirms the positive correlation between data augmentation intensity and inter-class overlap rate.

**Assumption 1**: **.** _There are two classes \(C_{1}\) and \(C_{2}\), with samples drawn from probability distributions \(P_{1}\) and \(P_{2}\), respectively. The data augmentation intensity is denoted by \(\alpha\)._

**Definition 10** (Data Distribution and Augmentation): **.** _The samples are satisfied \(X_{1}\sim P_{1}\), \(X_{2}\sim P_{2}\). The augmented samples are represented as \(X_{1}^{\prime}=X_{1}+\epsilon\) and \(X_{2}^{\prime}=X_{2}+\epsilon\), where \(\epsilon\) denotes the augmentation perturbation following the distribution \(Q_{\alpha}\). The variance of \(Q_{\alpha}\) is proportional to \(\alpha\)._

### Augmented Distributions

Let \(P_{1,\alpha}\) and \(P_{2,\alpha}\) denote the augmented sample distributions:

\[P_{1,\alpha}(x^{\prime}) =(P_{1}*Q_{\alpha})(x^{\prime})\] (23) \[P_{2,\alpha}(x^{\prime}) =(P_{2}*Q_{\alpha})(x^{\prime})\] (24)

where \(*\) represents the convolution operation.

Variance of Augmented DistributionsLet \(\sigma_{\alpha}^{2}\) be the variance of \(Q_{\alpha}\), with \(\sigma_{\alpha}^{2}=k\alpha\), where \(k\) is a constant. The variances of the augmented distributions are:

\[\sigma_{P_{1,\alpha}}^{2} =\sigma_{P_{1}}^{2}+\sigma_{\alpha}^{2}=\sigma_{P_{1}}^{2}+k\alpha\] (25) \[\sigma_{P_{2,\alpha}}^{2} =\sigma_{P_{2}}^{2}+\sigma_{\alpha}^{2}=\sigma_{P_{2}}^{2}+k\alpha\] (26)

Overlap Rate DefinitionLet \(R\) denote the overlap region. The overlap rate \(O(\alpha)\) is defined as:

\[O(\alpha)=\int_{R}P_{1,\alpha}(x^{\prime})\,dx^{\prime}+\int_{R}P_{2,\alpha}( x^{\prime})\,dx^{\prime}\] (27)

### Increased Variance Leads to Increased Overlap

As \(\alpha\) increases, \(\sigma_{\alpha}^{2}\) increases, resulting in larger variances of \(P_{1,\alpha}\) and \(P_{2,\alpha}\). This makes the distributions wider and more dispersed, increasing the overlap region.

Specific Derivation for One-Dimensional Gaussian DistributionsAssume \(P_{1}\) and \(P_{2}\) are two Gaussian distributions:

\[P_{1}(x) =\mathcal{N}(\mu_{1},\sigma_{P_{1}}^{2})\] (28) \[P_{2}(x) =\mathcal{N}(\mu_{2},\sigma_{P_{2}}^{2})\] (29)

Figure 6: **A scenario of similar views targeting different targets. In some cases, we may randomly crop two similar views from the original images as augmentations and input them into the machine learning model. This can lead to confusion in the model due to the differing targets assigned by humans or generated autonomously.**The augmented distributions are:

\[P_{1,\alpha}(x^{\prime}) =\mathcal{N}(\mu_{1},\sigma_{P_{1}}^{2}+k\alpha)\] (30) \[P_{2,\alpha}(x^{\prime}) =\mathcal{N}(\mu_{2},\sigma_{P_{2}}^{2}+k\alpha)\] (31)

Exact Calculation of Intersection PointEquating the two Gaussian probability density functions and solving for \(x\):

\[\frac{1}{\sqrt{2\pi(\sigma_{P_{1}}^{2}+k\alpha)}}\exp\left(-\frac{(x-\mu_{1}) ^{2}}{2(\sigma_{P_{1}}^{2}+k\alpha)}\right)=\frac{1}{\sqrt{2\pi(\sigma_{P_{2}} ^{2}+k\alpha)}}\exp\left(-\frac{(x-\mu_{2})^{2}}{2(\sigma_{P_{2}}^{2}+k\alpha)}\right)\] (32)

Simplifying the equation yields an analytical solution for the intersection point. To simplify the analysis, we assume \(\sigma_{P_{1}}^{2}=\sigma_{P_{2}}^{2}\), in which case the intersection point is \((\mu_{1}+\mu_{2})/2\).

Area of Overlap RegionLet \(\Delta\mu=|\mu_{1}-\mu_{2}|\). The overlap region can be represented using the cumulative distribution function (CDF) of the standard normal distribution, denoted as \(\Phi(z)\):

\[\Phi(z)=\frac{1}{2}\left[1+\text{erf}\left(\frac{z}{\sqrt{2}}\right)\right]\] (33)

The variance of the augmented distributions is \(\sigma_{\alpha}^{2}=\sigma_{P_{1}}^{2}+k\alpha\). Therefore, the approximate area of the overlap region is:

\[O(\alpha)\approx 2\Phi\left(\frac{\Delta\mu}{\sqrt{2(\sigma_{P_{1}}^{2}+k \alpha)}}\right)-1\] (34)

### Conclusion

**Theorem 4**: _As the data augmentation intensity \(\alpha\) increases, the overlap rate \(O(\alpha)\) between samples from different classes increases._

Proof.: Increasing \(\alpha\) leads to an increase in the variance of the sample distributions, making them wider and more likely to overlap.

Specifically, increasing \(\alpha\) increases \(\sigma_{P_{1}}^{2}+k\alpha\) in the denominator, thereby decreasing the argument of \(\Phi(\cdot)\). Since \(\Phi(z)\) increases as \(z\) decreases for \(z>0\), \(O(\alpha)\) increases as \(\alpha\) increases. 

In summary, we have rigorously proven the positive correlation between data augmentation intensity and inter-class overlap rate from a statistical and probabilistic perspective.

### Empirical Analysis for Real-world Datasets

Beyond the theoretical analysis for a simple case, we also provide empirical analysis for four real-world datasets under different intensities of data augmentation. We utilize TSNE [61] to visualize the (augmented) samples in Figure 7. All the results demonstrate that:

1. as minscale increases from 0.02 to 0.5, the data points gradually change from being tightly clustered to more dispersed. This indicates that higher data augmentation intensity expands the range of sample distribution;
2. when minscale=0.02, the data points of the two classes exhibit significant overlap, and the boundary becomes blurred. In contrast, larger minscale values such as 0.2 and 0.5 allow for better separation between classes;3. when data augmentation is not used (the last figure), there is a clear gap between the two classes, and the data points within each class are very compact. This suggests that the feature distribution of the original samples is relatively concentrated.

Figure 7: **Visualization of samples with varying levels of data augmentation intensity**. We visualize the samples of 2 classes from CIFAR-10 in 2-dimensional space, respectively setting minscale in RandomResizeCrop as \(\{0.02,0.04,0.08,0.2,0.5\}\) and without data augmentation.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We show them in Section 1 and our Abstract. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We show them in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [No]Justification: Most of them are proven in Appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We show the details of our techniques in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We use the public datasets. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We show the experimental details in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We show them in Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: We provide most of them in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research presented in the paper adheres to all aspects of the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: The paper does not comprehensively discuss the potential societal impacts, both positive and negative, of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: The paper does not describe any safeguards for the responsible release of data or models that carry a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All assets utilized in this paper, including code, data, and models, have been properly attributed to their respective creators or original owners. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not introduce any new assets; therefore, the question of documentation is not applicable. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve any crowdsourcing experiments or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve research with human subjects. Therefore, the discussion of potential risks, disclosure to subjects, and Institutional Review Board (IRB) approvals are not applicable. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.