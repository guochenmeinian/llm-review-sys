GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients

 Sima Behpour &Thang Doan &Xin Li &Wenbin He &Liang Gou &Liu Ren

Bosch Research North America, Bosch Center for Artificial Intelligence (BCAI)

{sima.behpour, thang.doan, xin.li9, wenbin.he2, liang.gou, liu.ren}@us.bosch.com

###### Abstract

Detecting out-of-distribution (OOD) data is crucial for ensuring the safe deployment of machine learning models in real-world applications. However, existing OOD detection approaches primarily rely on the feature maps or the full gradient space information to derive OOD scores neglecting the role of **most important parameters** of the pre-trained network over in-distribution (ID) data. In this study, we propose a novel approach called GradOrth to facilitate OOD detection based on one intriguing observation that the important features to identify OOD data lie in the lower-rank subspace of ID data. In particular, we identify OOD data by computing the norm of gradient projection on _the subspaces considered **important for the in-distribution data**. A large orthogonal projection value (i.e., a small projection value) indicates the sample as OOD as it captures a weak correlation of the ID data. This simple yet effective method exhibits outstanding performance, showcasing a notable reduction in the average false positive rate at a 95% true positive rate (FPR95) of up to 8% when compared to the current state-of-the-art methods.

## 1 Introduction

The issue of identifying out-of-distribution (OOD) data, which falls outside training data distributions, has become a significant focus in deep learning. OOD data challenges real-world model deployment, as it can lead to unreliable or incorrect predictions, particularly in safety-critical applications such as healthcare, autonomous vehicles, and physical sciences [14, 55, 64, 1, 4, 2]. This problem arises because modern Deep Neural Networks (DNNs) produce overconfident predictions on OOD inputs, complicating the separation of in-distribution (ID) and OOD data [63, 41]. The main goal of OOD detection is to develop methods that can accurately detect when a model encounters OOD data, allowing the model to either reject these inputs or provide more informative responses, such as uncertainty indication or confidence measures.

Many studies have investigated approaches to detecting OOD in deep learning [5, 18, 22, 27, 29, 32, 33, 34, 38, 39]. The majority of prior work focused on calculating OOD uncertainty from the activation space of a neural network, for example, by using model output [18, 27, 32, 34] or feature representations [29]. Another line of studies like ODIN [32], GradNorm [23], and ExGrad [24] leverage the gradient information of deep neural network models to compute OOD uncertainty score and achieve performant results. GradNorm [23] investigates the richness of the gradient space and presents that gradients provide valuable information for OOD detection. In particular, GradNorm utilizes the vector norm of gradients explicitly as an OOD scoring function. GradNorm, however, considers the full gradient space information, which might be noisy and lead to sub-optimal solutions.

A recent direction of research employs network parameter sparsification to improve OOD detection performance like DICE [47] and ASH [12]. ASH removes a majority of the activation by obtaining the \(p\)th-percentile of the entire representation. However, the potential consequence may result in diminished performance due to the partial removal of critical parameters within the pre-trained network. Also, this is an empirical approach without a principled way to sparsify models.

Based on key observations presented in _gradient-based_ and _sparcification-based_ OOD detection methods, an intriguing insight emerges: the crucial discriminative features for OOD data identification reside within the gradient subspace of the ID data. This suggests that by focusing on the gradient information in the subspace of the ID data, which captures the most salient information, we can enhance the accuracy and reliability of OOD data detection algorithms. However, it takes non-trivial work to identify such an efficient gradient subspace.

Inspired by recent low-rank factorization research for DNNs [59; 52; 20; 51], indicating the intrinsic model information resides in a few low-rank dimensions, we introduce a novel approach named **GradOrth**. More specifically, the proposed method, GradOrth, distinguishes OOD samples by employing _orthogonal gradient projection_ in the _low-rank subspaces_ of ID data (figure 1). These ID subspaces (\(S^{L}\) in figure 1) are derived through singular value decomposition (SVD) of pre-trained network activations, specifically on a small subset of randomly selected ID data. By leveraging SVD, GradOrth effectively computes and identifies the relevant subspaces associated with the ID data, enabling accurate discrimination of OOD samples through orthogonal gradient projection. A large magnitude (figure 1-a) of orthogonal projection (i.e., small projection) serves as a significant criterion for classifying a sample as OOD since it captures a weak correlation with the ID data.

Our key results and contributions are:

- We present GradOrth, a novel and efficient method for out-of-distribution (OOD) detection. Our approach leverages the **most important** parameter space of a pre-trained network and its **gradients** to accomplish this task. To the best of our knowledge, GradOrth is the pioneering endeavor to investigate and showcase the efficacy of the subspace of a DNN's gradients in OOD detection.

- We evaluate the performance of GradOrth on widely-used benchmarks, and it demonstrates competitive results compared to other post-hoc OOD detection baselines. Notably, GradOrth outperforms the strong baseline methods by consistently reducing the false positive rate at the 95th percentile (FPR95) by a margin ranging from 2.71% to 8.05%. Moreover, our experiments highlight that GradOrth effectively enhances OOD detection capabilities while maintaining high accuracy in classifying in-distribution (ID) data.

- We present a comprehensive analysis, including ablation experiments and theoretical investigation, aimed at enhancing the understanding of our proposed method for OOD detection. Through these rigorous analyses, we aim to provide valuable insights and improve the overall comprehension of the intricacies and effectiveness of our OOD detection approach.

## 2 Background and Our Notations

We consider a neural network with \(L\) layers and a set of learning parameters \(\theta=\{\theta^{l}\}_{l=1}^{L}\) where \(\theta^{l}\) present the learning parameters of layer \(l\). \(x_{i}^{l}\) denotes the **representation of input**\(x_{i}\) at layer \(l\) in the successive layers given the data input \(x_{i}\). The network performs the following computation at each layer:

\[x_{i}^{l+1}=\sigma(f(\theta^{l},x_{i}^{l})),\quad l=1,...L,\] (1)

where, \(\sigma(.)\) is a non-linear function and \(f(.,.)\) is a linear function. We leverage matrix notation for input (\(\bm{X}_{i}\)) in convolutional layers and vector notation for input (\(x_{i}\)) in fully connected layers. In the first layer, \(x_{i}^{1}=x_{i}\) refers to the raw input data. It is noteworthy to mention that our approach holds applicability across all layers of the network. However, our experimental investigations reveal that the last layer yields the most optimal performance, please refer to appendix, section B for our

Figure 1: The main idea of _GradOrth_: Measuring the orthogonal projection of the gradient of a testing sample on a \(k\)-dimension (e.g \(k\)=2 here) subspace of pre-trained network on ID data. We show the online \(\alpha\) between \(9(\sigma_{x}^{l})=\nabla_{\theta^{l}}\mathcal{L}(\bm{\theta^{l}})\) and \(\overline{O(x_{i}^{l})}=P_{i}^{L}(\nabla_{\theta^{l}}\mathcal{L}(\bm{\theta^{ l}}))\). If \(\alpha\) is large (\(k\),..., small projection on subspace \(\mathcal{S}^{L}\)), shown in (a), the sample \(x_{i}\) is weakly correlated to ID data, and therefore it is recognized as OOD. Otherwise, it is ID data, shown in (b).

empirical studies report. This preference is advantageous as it alleviates significant time complexity arising from gradient computations across multiple network layers.

### Input and Gradient Space

Our algorithm capitalizes on the intrinsic property of stochastic gradient descent (SGD) updates lying within the span of input data points, as validated by [62, 20, 60]. The subsequent subsections present this relationship, specifically in fully connected layers. We present the details regarding convolutional layers in the appendix, section A.

**Fully Connected Layer** Consider a single-layer linear neural network in a supervised learning setup where each (input, label) training data pair is driven from a training dataset, \(\mathbb{D}\). We use \(\bm{x}\in\mathbb{R}^{n}\) to present the input vector, \(\bm{y}\in\mathbb{R}^{m}\) to present the label vector in the dataset, and \(\bm{\theta}\in\mathbb{R}^{m\times n}\) to express the learning parameters (weights) of the network. In general, the network is trained by minimizing a loss function (e.g. mean-squared error) as follows:

\[\mathcal{L}=\frac{1}{2}||\bm{\theta}\bm{x}-\bm{y}||_{2}^{2}.\] (2)

Following stochastic gradient optimization, we can present the gradient of this loss with respect to weights as:

\[\nabla_{\bm{\theta}}\mathcal{L}=(\bm{\theta}\bm{x}-\bm{y})\bm{x}^{T}=\bm{ \Omega}\bm{x}^{T},\] (3)

Here, \(\bm{\Omega}\in\mathbb{R}^{m}\) denotes the error vector. Consequently, the gradient update will reside within the input span (\(\bm{x}\)), wherein the elements in \(\bm{\Omega}\) exhibit varying magnitudes, thus influencing the scaling of \(\bm{x}\) accordingly, please refer to section E for the proof. For simplicity, we have considered per-example loss (batch size of 1) and mean-squared loss function here. Furthermore, it is important to mention that the aforementioned relationship remains applicable even in the context of the mini-batch setting or when utilizing alternative loss functions such as cross-entropy loss, where the calculation of \(\Omega\) may differ. For more comprehensive information on this subject, please consult the appendix, specifically section D. The input-gradient relationship in equation 3 can be applied to any fully connected layer of a neural network where \(\bm{x}\) is the input to that layer and \(\bm{\Omega}\) is the error coming from the next layer. In addition, this equation also applies to the networks with non-linear units (such as ReLU) and cross-entropy losses, though \(\bm{\Omega}\) will be calculated differently.

### Matrix Approximation with SVD:

In our algorithm, we utilize singular value decomposition (SVD) for matrix factorization. Specifically, a rectangular matrix \(\bm{R}=\bm{U}\bm{\Sigma}\bm{V}^{T}\in\mathbb{R}^{m\times n}\) can be factorized using SVD into the product of three matrices. Here, \(\bm{\Sigma}\) represents a matrix containing the singular values sorted along its main diagonal, while \(\bm{U}\in\mathbb{R}^{m\times m}\) and \(\bm{V}\in\mathbb{R}^{n\times n}\) denote orthogonal matrices [10]. In the case where the rank of the matrix \(\bm{R}\) is \(r\) (\(r\leq\text{min}(m,n)\)), the matrix \(\bm{R}\) can be represented as \(\bm{R}=\sum_{i=1}^{r}\sigma_{i}\bm{u}_{i}\bm{v}_{i}^{T}\), where \(\sigma_{i}\in\text{diag}(\bm{\Sigma})\) denotes the singular values and \(\bm{u}_{i}\in\bm{U}\) as well as \(\bm{v}_{i}\in\bm{V}\) represent the left and right singular vectors, respectively. Furthermore, we can formulate the \(k\)-rank approximation to the matrix as \(\bm{R}_{k}=\sum_{i=1}^{k}\sigma_{i}\bm{u}_{i}\bm{v}_{i}^{T}\), where \(k\leq r\). The specific value of \(k\) can be determined as the smallest value that satisfies the condition \(||\bm{R}_{k}||_{F}^{2}\geq\epsilon_{th}||\bm{R}||_{F}^{2}\). In this equation, \(||.||F\) represents the Frobenius norm of the matrix, and \(\epsilon_{th}\) (\(0<\epsilon_{th}\leq 1\)) serves as the threshold [10]. For a comprehensive explanation of the SVD method, more explanation regarding \(k\)-rank matrix approximation, and the impact of \(\epsilon_{th}\) in method performance please refer to the appendix sections F, H, I, respectively.

### Problem Statement: Out-of-distribution Detection

OOD is typically characterized by a distribution that represents unknown scenarios encountered during deployment. These scenarios involve data samples originating from an irrelevant distribution, whose label set has no intersection with the predefined set. Consider the supervised setting where a neural network is given access to a set of training data \(\mathbb{D}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}\) drawn from an unknown joint data distribution \(P\) defined on \(\mathcal{X}\times\mathcal{Y}\) in the training phase. We denote the input space and output space by \(\mathcal{X}=\mathbb{R}^{n}\) and \(\mathcal{Y}=\{1,2,...,m\}\), respectively.

\[z(\mathbf{x})=\begin{cases}\text{ID},&\text{if }O(\mathbf{x})\geq\gamma\\ \text{OOD},&\text{if }O(\mathbf{x})<\gamma.\end{cases}\] (4)

The parameter \(\gamma\) is typically selected to ensure a high percentage of correct classification for in-distribution (ID) data, such as 95%. A major hurdle is to establish a scoring function \(O(\mathbf{x})\) that effectively captures the uncertainty associated with OOD samples. Prior approaches have predominantly relied on various factors, including the model's output, gradients, or features, to estimate OOD uncertainty [23; 12]. In our proposed approach, we aim to compute the scoring function \(O(\mathbf{x})\) by leveraging _orthogonal gradient projection_ on _parameter subspace_ of a pre-trained network over ID data. The details of our methodology are described in the subsequent section.

### Orthogonal Projection

In this section, we discuss the concept of orthogonal projection and our notation, which holds significant importance in our methodology. To simplify the explanation, we will present it in a 2D space, but it can be extended to higher dimensions. Orthogonal projection serves as a metric that we utilize to calculate the distance between a vector \(\overrightarrow{V}\) and a space W (represented as a matrix in this case).

The result of the orthogonal projection of vector \(\overrightarrow{V}\) onto space W consists of three essential components: (1) The orthogonal projection vector \(\overrightarrow{b}\), (2) the projection vector \(\overrightarrow{c}\), and (3) the angle \(\alpha\). These three components' values can be utilized to determine the correlation between the vector \(\overrightarrow{V}\) and the space W. As depicted in the figure 2, a larger value of \(\overrightarrow{b}\) indicates a weaker correlation. On the other hand, \(\overrightarrow{c}\) exhibits the opposite pattern, where a larger value of \(\overrightarrow{c}\) indicates a stronger correlation. Depending on the specific application requirements, any of these values can be chosen to compute the correlation. In order to align with the OOD score, where a smaller value indicates a higher degree of OODness as presented in equation 4, we incorporate the projection vector (\(\overrightarrow{c}\)) into our computations.

## 3 Our Method: GradOrth

In this section, we describe our method GradOrth where we recognize ID Vs. OOD data from a different view of previous studies, computing the norm of gradient projection on _the subspaces considered important for the in-distribution data_.

We define a sample as OOD data if its orthogonal projection value is large (i.e., small projection value), indicating a weak correlation with the ID data.

GradOrth OOD detection is developed following these steps:

1. **Pre-trained Network subspace Computation:** Our \(L\)-layer neural network with learning parameter \(\theta\) is trained using **ID** data. Upon completion of the training process, the model parameters \(\theta\) are frozen, resulting in a pre-trained network specialized in ID data. It is worth mentioning that we can also leverage the existing pre-trained network over our interest ID data. To retain the most significant parameters of the pre-trained network with respect to the ID data, we compute the network's last layer (\(L\)) subspace. For this purpose, we construct a representation matrix denoted as \(\bm{R}^{L}_{ID}=[\bm{x}^{L}_{1},\bm{x}^{L}_{2},...,\bm{x}^{L}_{n}]\), which concatenates \(n\) representations obtained from the network's last layer (\(L\)) through the forward pass of \(n\) randomly selected samples (a small subset, \(n\ll N\)) of the ID data. Next, we perform SVD on \(\bm{R}^{L}_{ID}\), resulting in \(\bm{R}^{L}_{ID}=\bm{U}^{L}_{ID}\bm{\Sigma}^{L}_{ID}(\bm{V}^{L}_{ID})^{T}\). We then proceed to approximate its rank \(k\) by obtaining \((\bm{R}^{L}_{ID})_{k}\), guided by the given criteria that rely on a specified threshold, denoted as \(\epsilon_{th}\): \[\|(\bm{R}^{L}_{ID})_{k}\|^{2}_{F}\geq\epsilon_{th}\|\bm{R}^{L}_{ID}\|^{2}_{F}.\] (5) The pre-trained network subspace, denoted as \(S^{L}=span\{\bm{u}^{L}_{1},\bm{u}^{L}_{2},...,\bm{u}^{L}_{k}\}\), is defined as the **space of significant representation** for the pre-trained network at the last layer \(L\). This subspace is spanned by the first \(k\) vectors in \(U^{L}_{ID}\) and encompasses all directions associated with the highest singular values in the representation. We store this subspace, \(S^{L}\), and leverage it in the next step. We present the algorithm to compute the ID subspace in Algorithm 1.
2. **Inference with OOD Data:** During the inference phase, the pre-trained model is exposed to an OOD sample \(x_{i}\). The OOD sample is propagated through the pre-trained network, and subsequently, its gradient at layer L is computed which is presented as \(g(x_{i})=\nabla_{\bm{\theta}^{L}}\mathcal{L}(\bm{\theta}^{L})\).

Figure 2: Orthogonal Projection

In accordance with the GradNorm approach, we calculate the cross-entropy loss by comparing the model's predicted softmax probability to a uniform vector used as the target. Consequently, during testing, we employ an all-one vector as the ground truth, assuming a uniform distribution for the target data.
3. **Detector Construction:** The model is transformed into a _detector_ by generating a **score** based on its output, enabling the differentiation between ID and OOD inputs. To this end, we compute the norm of sample gradient projection onto the subspace of the pre-trained network (\(S\)). We compute projection of the gradients \(\nabla_{\bm{\theta}^{L}}\mathcal{L}(\bm{\theta}^{L})\) onto the subspace \(\mathcal{S}^{L}\) as follows: \[P_{S^{L}}(\nabla_{\bm{\theta}^{L}}\mathcal{L}(\bm{\theta}^{L}))=(\nabla_{\bm{ \theta}^{L}}\mathcal{L}(\bm{\theta}^{L}))\mathcal{S}^{L}(\mathcal{S}^{L})^{ \prime}.\] (6) Here, \((.)^{\prime}\) presents the matrix transpose. Next, we define the OOD score for the sample as follows by computing the projection norm: \[O(x_{i})=\|P_{S^{L}}(\nabla_{\bm{\theta}^{L}}\mathcal{L}(\bm{\theta}^{L}))\|\] (7) This score serves as a surrogate to characterize the correlation between the sample and ID data that the pre-trained network trained on it. As presented in figure 1, it implies a weak correlation between the new sample \(x_{i}\) and ID when the gradient \(g(x_{i})=\nabla_{\bm{\theta}^{L}}\mathcal{L}(\bm{\theta}^{L})\) has a small projection (large orthogonal projection) onto the subspace of the pre-trained network (large angle \(\alpha\)) due to the fact that stochastic gradient descent (SGD) updates lie in the span of input data points [65], please refer to the appendix, section E for the proof. Algorithm 2 presents OOD score computation.

```
1:function Compute subspace (\(f_{\bm{\theta}}\), \(\mathcal{D}_{ID}\), \(\epsilon_{th}\) )
2://Initialization
3:\(\mathcal{S}^{L}\leftarrow[\ ]\)
4:\(B_{n}\leftarrow\) Sample a mini-batch of size \(n\) from ID data (\(\mathcal{D}_{ID}\))
5:\(\bm{\theta}\leftarrow\) Pre-trained network \(f\) learning parameters
6:// subspace Computation
7:\(\mathcal{R}^{L}_{ID}\leftarrow\) forward\((B_{n},f(\bm{\theta}))\)
8:\(\tilde{U}^{L}_{ID}\leftarrow\) SVD\((\tilde{\bm{R}}^{L}_{ID})\)
9:\(k\leftarrow\) criteria\((\tilde{\bm{R}}^{L}_{ID},\bm{R}^{L}_{ID},\epsilon^{t}_{th})\) // Refer to equation 5
10:\(\mathcal{S}^{L}\leftarrow\tilde{U}^{L}_{ID}[0:k]\)
11:return\(\mathcal{S}^{L}\) ```

**Algorithm 1** ID Subspace Computation

## 4 Experiments

In this section, we evaluate the performance of our method _GradOrth_ running extensive experiments considering different ID/OOD datasets and network architectures. We follow the experiment setting in general OOD baselines and explain the experimental setup in section 4.1. These empirical studies demonstrate the superior performance of _GradOrth_ over existing state-of-the-art baselines that are reported in section 4.1. We report extensive ablations and analyses that provide a deeper understanding of our methodology, please refer to the appendix, section G.

### Experimental Setup

**Dataset** We leverage 2 benchmarks proposed by [22] and [12] for detecting OOD images that are based on the large-scale ImageNet dataset and CIFAR dataset. To provide a fair comparison, we adopt an average results-over-5-run approach. In each run, distinct random seeds are employed to select random samples from each class, generating small subsets of in-distribution data. Subsequently, we compute the subspace of the pre-trained network based on these subsets. The OOD scores of the test data are then calculated, and FPR95 and AUROC scores are derived. This process is repeated five times, and the average of these five runs is reported as the final score.

**ImageNet Benchmark:** This benchmark is more challenging than others because it has higher-resolution images and a larger label space of 1,000 categories. To test our approach, we evaluate four OOD test datasets, including subsets of iNaturalist [53], SUN [56], Places [63], and Textures[8]. These datasets have non-overlapping categories compared to the ImageNet-1k dataset and cover a diverse range of domains including fine-grained, scene, and textural images. We follow the experimental setting reported in [12] and use the Resnet-50 model [16] pre-trained on ImageNet-1k. For a fair comparison, all the methods use the same pre-trained backbone, without regularizing with auxiliary outlier data. Details and hyperparameters of baseline methods can be found in appendix J.1. The outcomes of this study present results obtained by applying GradOrth after the last fully connected layer in all the experiments. In this configuration, the feature size is \(2048\) for ResNet-50 and \(1280\) for MobileNetV2. For subspace computation, we choose 10 random samples per class and set the SVD threshold to \(0.97\).

**CIFAR Benchmark:** We evaluate our approach on the commonly used CIFAR-10 [26], and CIFAR-100 [26] benchmarks as in-distribution data following the experimental setting in [12; 50]. We employ the standard split with 50,000 training images and 10,000 test images. For subspace computation, we choose 5 random samples per class and set the SVD threshold to \(0.97\). We assess the model on six widely used OOD benchmark datasets: Textures [8], SVHN [40], Places365 [63], LSUN-Crop [61], LSUN-Resize [61], and iSUN [58]. Regarding pre-trained network architecture, we use DenseNet-101 architecture [21]. We leverage pre-trained networks over ID datasets. Please refer to section J.1 in the appendix for more details regarding the experiment setting. It is important to note that no modifications were made to the network parameters during the OOD detection phase.

Evaluation MetricsWe assess the effectiveness of our proposed method by utilizing threshold-free metrics that are commonly used for evaluating OOD detection, as standardized in [18]. These metrics include (i) AUROC, which stands for the Area Under the Receiver Operating Characteristic curve; and (ii) FPR95, which is the false positive rate. FPR95 represents the probability that a negative (i.e., OOD) example is misclassified as positive (i.e., ID) when the true positive rate is as high as 95 [31].

### Results and Discussion

Our experimental studies present the promising performance of OrthoGrad in OOD detection on two benchmarks, ImageNet and CIFAR benchmarks.

#### 4.2.1 Experimental Results on Out-of-Distribution Detection

ImageNet Benchmark:Our method demonstrates competitive performance, reaching the state-of-the-art level, as indicated in table 1. On the Resnet pre-trained network, GradOrth surpasses ASH-S, ASH-B, and ASH-S by \(0.45\%\), \(2.47\%\), and \(0.93\%\) in terms of FPR95 on the iNaturalist, SUN, and Textures OOD datasets, respectively. When evaluated on the Places OOD dataset, our method achieves an FPR95 of \(33.67\%\) and secures the second rank after ASH-B. Furthermore, GradNorm demonstrates an average FPR95 performance of \(18.57\%\), outperforming ASH-B by \(3.98\%\).

It is important to acknowledge the fact that GradOrth boasts a _low computational complexity_. It only requires computing the subspace of the pre-trained network once and can be conveniently utilized through a simple gradient calculation, without the need for hyper-parameter tuning or additional training during OOD detection. In contrast, certain methods like Mahalanobis [29] require collecting feature representations from intermediate layers for the entire training set, which can be computationally expensive for large-scale datasets like ImageNet. Additionally, GradOrth presents a _stable performance_ across most datasets whereas the performance of ASH versions varies across the four OOD datasets. ASH-B outperforms other baselines on the Places dataset but ranks third, second, and third on the other three datasets. A similar pattern is observed for ASH-S in terms of FPR95, where it ranks second, sixth, sixth, and second across the iNaturalist, SUN, Places, and Textures datasets, respectively.

GradOrth also exhibits superior performance in terms of AUROC, outperforming ASH-S by an average of \(2.80\%\) across the four datasets. Particularly, GradOrth surpasses ASH-S, ASH-B, and ASH-S by \(0.13\%\), \(0.66\%\), and \(0.46\%\) on the iNaturalist, SUN, and Textures OOD datasets, respectively.

For the pre-trained MobileNet model, our GradOrth approach also demonstrates outstanding performance. We present experimental results on leveraging MobileNet as the pre-trained ID network and evaluate the OOD detection performance on the iNaturalist, SUN, Places, and Textures datasets (the bottom section of table 1). In these experiments, GradOrth demonstrates outstanding performance. In terms of FPR95, GradOrth outperforms ASH-B, DICE+ReAct, DICE+ReAct, and ASH-S by \(4.65\%\), \(0.40\%\), \(6.50\%\), and \(0.43\%\), respectively, across the four datasets. Regarding AUROC, GradOrthoutperforms other baselines on average by at least \(4.01\%\) and \(0.57\%\) in terms of FPR95 and AUROC, respectively.

CIFAR Benchmark: In this research study, we further investigate the performance of GradOrth by conducting additional experimental studies on the CIFAR10 and CIFAR100 datasets. The key observation is that no single method consistently outperforms all other methods across diverse datasets. However, it is noticeable that GradOrth rank is always among the top three across six OOD datasets. This feature presents its promising performance for OOD detection. On the CIFAR10 dataset, GradOrth demonstrates superior performance compared to other baseline methods across six OOD datasets, namely SVHN, LSUN-c, LSUN-r, iSUN, Textures, and Places365. On average, GradOrth outperforms these baselines by \(2.71\%\) and \(0.32\%\) in terms of FPR95 and AUROC, respectively. Detailed experimental results can be found in table 2. In the LSUN-c OOD dataset, DICE demonstrates superior performance with an impressive \(0.26\%\) FPR95, placing it at the top. Our method, on the other hand, ranks second with a respectable \(0.81\%\) FPR95. However, the ranking differs when examining the Textures and Places365 datasets. Notably, Gradorth outperforms other baseline methods in both cases, achieving noteworthy FPR95 values of \(20.63\%\) and \(38.22\%\), respectively. In contrast, DICE attains the sixth and ninth positions in these datasets, displaying comparatively higher FPR95 rates of \(41.90\%\) and \(48.59\%\).

On the CIFAR100 dataset, GradOrth surpasses its competitors in both FPR95 and AUROC by an average margin of \(8.0\%\) and \(2.80\%\), respectively, across six well-known OOD datasets. Detailed experimental results are provided in table 3. For a comprehensive discussion and analysis of the CIFAR benchmark, please refer to the appendix, specifically section C.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Methods**} & \multicolumn{3}{c|}{**Naturalistic**} & \multicolumn{3}{c|}{**OOD Datasets**} & \multicolumn{3}{c|}{**Patasets**} & \multicolumn{3}{c|}{**Textures**} & \multicolumn{3}{c|}{**Average**} \\ \cline{3-13}  & & FPR95 & AUROC & **FPR95** & AUROC & **FPR95** & AUROC & **FPR95** & AUROC & **FPR95** & AUROC & FPR95** & AUROC \\ \hline \multirow{11}{*}{ResNet} & Softmax score & 54.99 & 87.74 & 70.83 & 80.86 & 73.99 & 79.76 & 68.00 & 79.61 & 66.95 & 81.99 & \multirow{11}{*}{\(\uparrow\)} \\  & ODN & 47.66 & 89.96 & 60.15 & 84.59 & 67.89 & 81.78 & 50.23 & 85.62 & 56.48 & 85.41 & 85.41 \\  & Mahalanobis & 97.00 & 52.65 & 89.50 & 42.41 & 98.40 & 41.79 & 58.30 & 85.01 & 87.43 & 55.47 & \multirow{11}{*}{\(\uparrow\)} \\  & Entropy score & 55.72 & 89.95 & 59.26 & 83.89 & 64.92 & 82.86 & 53.27 & 55.99 & 54.41 & 86.17 \\  & GradOrth & 42.46 & 90.33 & 40.73 & 89.96 & 43.48 & 80.66 & 34.38 & 88.43 & 40.29 & 57.34 \\  & Erdöf & 54.11 & 76.91 & 46.73 & 69.74 & 50.62 & 74.27 & 38.12 & 79.37 & 47.40 & 75.90 \\  & Erdöf & 20.38 & 96.22 & 24.02 & 94.20 & 33.55 & 91.58 & 47.30 & 89.80 & 31.43 & 92.95 & \multirow{11}{*}{\(\uparrow\)} \\  & DICE & 25.63 & 94.49 & 25.51 & 90.83 & 46.69 & 87.48 & 31.72 & 90.30 & 34.75 & 90.77 \\  & DICE + ReAct & 18.64 & 96.24 & 25.45 & 93.94 & 36.36 & 90.67 & 26.87 & 92.74 & 27.25 & 93.40 \\  & VRA-DN & 16.82 & 96.92 & 30.63 & 96.56 & 39.94 & 90.75 & 26.72 & 95.90 & 28.53 & 94.09 \\  & VRA-P & 15.70 & 97.12 & 26.94 & 94.12 & 94.25 & 73.95 & 91.27 & 21.47 & 95.62 & 24.59 & 94.57 \\  & AISH-P & 44.57 & 92.51 & 52.88 & 83.35 & 61.79 & 85.58 & 42.06 & 89.70 & 50.32 & 89.04 \\  & AISH-B & 14.21 & 97.32 & 22.08 & 95.10 & **30.45** & **92.31** & 21.17 & 95.50 & 22.73 & 95.66 \\  & AISH-B & 11.49 & 97.87 & 27.98 & 94.02 & 39.78 & 90.98 & 11.93 & 97.60 & 22.80 & 95.12 \\  & OntoOrth (Ours) & **18.04\(\pm\)10.20** & 98.01 & **19.41\(\pm\)12.76** & **98.76\(\pm\)10** & 93.76 & 91.78 \(\pm\)10.21 & **11.90\(\pm\)10** & **97.92\(\pm\)10** & **18.75\(\pm\)10** & **93.43\(\pm\)10** \\ \hline \multirow{11}{*}{MobileNet} & Softmax score & 64.29 & 85.32 & 70.72 & 77.02 & 77.19 & 79.23 & 76.27 & 73.51 & 77.30 & 73.51 & 79.00 \\  & ODN & 55.39 & 87.62 & 84.07 & 85.88 & 57.36 & 84.71 & 49.96 & 85.03 & 54.20 & 85.81 \\  & Mahalanobis & 62.11 & 81.00 & 47.82 & 86.53 & 52.09 & 86.53 & 92.38 & 30.66 & 63.00 & 71.01 \\  & Energy score & 59.50 & 88.91 & 62.68 & 84.50 & 69.87 & 81.19 & 58.05 & 85.08 & 62.39 & 84.91 \\  & ReAct & 42.40 & 91.53 & 47.69 & 88.16 & 51.56 & 86.64 & 84.24 & 91.53 & 45.02 & 89.47 \\  & DICE & 43.09 & 90.83 & 86.96 & 90.46 & 53.11 & 85.81 & 35.90 & 91.30 & 41.92 & 89.60 \\  & DICE + ReAct & 32.30 & 90.57 & 35.12 & 92.26 & 98.66 & 47.68 & 80.02 & 16.28 & 92.52 & 31.64 & 92.68 \\  & AISH-P & 54.92 & 90.46 & 58.61 & 86.72 & 66.69 & 83.27 & 44.48 & 88.72 & 37.15 & 87.34 \\  & AISH-B & 31.46 & **92.48** & 34.58 & 91.61 & 51.80 & 87.56 & 20.92 & 50.70 & 35.66 & 92.13 \\  & AISH-B & 39.10 & 91.91 & 4.362 & 90.02 & 58.84 & 84.73 & 11.12 & 97.01 & 38.67 & 90.95 \\  & GradOrth (Ours) & **26.81\(\pm\)11** & 93.17 & 17.12 & **30.31\(\pm\)12** & **93.18\(\pm\)11** & **40.27\(\pm\)11** & **42.69\(\pm\)11** & **97.25\(\pm\)11** & **27.68\(\pm\)10** & **93.22\(\pm\)11** \\ \hline \end{tabular}
\end{table}
Table 1: OOD detection results with **ImageNet-1k** as ID. GradOrth present outstanding performance on average and across most datasets. We adopted the identical table format and evaluation metrics as introduced in [48, 12]. The ResNet and MobileNet models are pre-trained solely with ID data from the ImageNet-1k dataset. We use \(\uparrow\) denote the latter values are preferable, and \(\downarrow\) to denote that smaller values are preferable. All values are presented as percentages. All values in the table are directly taken from table \(\uparrow\) (12) except for the gradient-based methods. (GradNorm, ExGrad, GradOrth (ours)). For GradNorm and ExGrad, we run this experiment leveraging the code provided by the authors.

\begin{table} \

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

input perturbation, and the OOD scores were still calculated based on the output space of the perturbed inputs. GradNorm [23] also utilizes gradient information from a neural network to detect distributional shifts between ID and OOD samples. By measuring the norm of gradients with respect to the network's input, it quantifies uncertainty and identifies OOD samples causing significant output changes. ExGrad, proposed by [24], introduces a method akin to GradNorm with two notable distinctions. Firstly, the label distribution of \(y\) is derived from the model's predicted distribution (\(P\)) as opposed to the uniform distribution. Secondly, ExGrad computes the expected norm of the gradient, in contrast to GradNorm which calculates the norm of the expected gradient.

Discriminative Models for OOD Uncertainty EstimationThe problem of classification with rejection has a long history, dating back to early works on abstention such as [6] and [15], which considered simple model families like SVMs [9]. However, the phenomenon of neural networks' overconfidence in OOD data was not revealed until the work of [41].

Early efforts aimed to improve OOD uncertainty estimation by proposing the ODIN score [32] and Mahalanobis distance-based confidence score [29]. More recently, [34] proposed using an energy score derived from a discriminative classifier for OOD uncertainty estimation, showing advantages over the softmax confidence score both empirically and theoretically. [54] demonstrated that an energy-based approach can improve OOD uncertainty estimation for multi-label classification networks. Additionally, [22] revealed that approaches developed for common CIFAR benchmarks might not effectively translate into a large-scale ImageNet benchmark, highlighting the need to evaluate OOD uncertainty estimation in a large-scale real-world setting. These developments have brought renewed attention to the problem of classification with rejection and the need for effective OOD uncertainty estimation.

Generative Models for OOD Uncertainty EstimationDetection of OOD inputs is a crucial problem in machine learning. One popular approach is to use generative models that estimate the density directly. Such models can identify OOD inputs as those lying in low-likelihood regions. To this end, a plethora of literature has emerged to leverage generative models for OOD detection. However, recent studies have shown that deep generative models can assign high likelihoods to OOD data, rendering such models less effective in OOD detection. Additionally, these models can be challenging to train and optimize, and their performance may lag behind their discriminative counterparts. In contrast, our approach relies on a discriminative classifier, which is easier to optimize and achieves stronger performance. While some recent works have attempted to improve OOD detection with generative models using improved metrics, likelihood ratios, and likelihood regret, our approach leverages the energy score from a discriminative classifier and has demonstrated significant advantages over generative models in OOD detection.

Distributional ShiftsThe problem of distributional shift has garnered significant attention in the research community. It is essential to recognize and distinguish between different types of distributional shift problems. In the literature on OOD detection, the focus is typically on ensuring model reliability and detecting label-space shifts [18; 32; 34], where OOD inputs have labels that are disjoint from the ID data, and as such, should not be predicted by the model. On the other hand, some studies have examined covariate shifts in the input space [17; 37; 42], where inputs may be subject to corruption or domain shifts. However, covariate shifts are commonly used to evaluate model robustness and domain generalization performance, where the label space \(\mathcal{Y}\) remains the same during test time. It is worth noting that our work focuses on the detection of shifts where the model should not make any predictions, as opposed to covariate shifts where the model is expected to generalize.

## 6 Conclusion

In this paper, we propose GradOrth, a novel OOD uncertainty estimation approach utilizing information extracted from the _important parameter space for ID data_ and _gradient space_. Extensive experimental results show that our gradient-based method can improve the performance of OOD detection by up to \(8.05\%\) in FPR95 on average, establishing superior performance. We hope that our research brings to light the informativeness of gradient subspace, and inspires future work to utilize it for OOD uncertainty estimation. In our future research, our objective is to investigate GradOrth's capabilities considering different directions like influence functions [25], novelty detection in open-world context [13], data pre-selection [30], and underspecification [36].

## References

* Agarwal et al. [2021] Tanmay Agarwal, Hitesh Arora, and Jeff Schneider. Learning urban driving policies using deep reinforcement learning. In _2021 IEEE International Intelligent Transportation Systems Conference (ITSC)_, pages 607-614. IEEE, 2021.
* Boyer et al. [2021] Mark Boyer, Josiah Wai, Mitchell Clement, Egemen Kolemen, Ian Char, Youngseog Chung, Willie Neiswanger, and Jeff Schneider. Machine learning for tokamak scenario optimization: combining accelerating physics models and empirical models. _Bulletin of the American Physical Society_, 2021.
* Spectral Analysis_. Academic Press, San Diego, 1987. ISBN 978-0-08-050780-4. doi: https://doi.org/10.1016/B978-0-08-050780-4.50014-X. URL https://www.sciencedirect.com/science/article/pii/B978008050780450014X.
* Char et al. [2021] Ian Char, Youngseog Chung, Mark Boyer, Egemen Kolemen, and Jeff Schneider. A model-based reinforcement learning approach for beta control. In _APS Division of Plasma Physics Meeting Abstracts_, volume 2021, pages PP11-150, 2021.
* Chen et al. [2021] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-of-distribution detection using outlier mining. _In Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)_, 2021.
* Chow [1970] CK Chow. On optimum recognition error and reject tradeoff. _IEEE Transactions on information theory_, 16(1):41-46, 1970.
* Cimpoi et al. [2014] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed,, and A. Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2014.
* Cimpoi et al. [2014] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 3606-3613, 2014.
* Cortes and Vapnik [1995] Corinna Cortes and Vladimir Vapnik. Support-vector networks. _Machine learning_, 20(3):273-297, 1995.
* Deisenroth et al. [2020] Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. _Mathematics for Machine Learning_. Cambridge University Press, 2020.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Djurisic et al. [2022] Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, and Rosanne Liu. Extremely simple activation shaping for out-of-distribution detection. _arXiv preprint arXiv:2209.09858_, 2022.
* Doan et al. [2023] Thang Doan, Xin Li, Sima Behpour, Wenbin He, Liang Gou, and Liu Ren. Hyp-ow: Exploiting hierarchical structure learning with hyperbolic distance enhances open world object detection. _arXiv preprint arXiv:2306.14291_, 2023.
* Filos et al. [2020] Angelos Filos, Panagiotis Tigkas, Rowan McAllister, Nicholas Rhinehart, Sergey Levine, and Yarin Gal. Can autonomous vehicles identify, recover from, and adapt to distribution shifts? In _International Conference on Machine Learning_, pages 3145-3153. PMLR, 2020.
* Fumera and Roli [2002] Giorgio Fumera and Fabio Roli. Support vector machines with embedded reject option. In _International Workshop on Support Vector Machines_, pages 68-82. Springer, 2002.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hendrycks and Dietterich [2019] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _arXiv preprint arXiv:1903.12261_, 2019.

* [18] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. _Proceedings of International Conference on Learning Representations_, 2017.
* [19] Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10951-10960, 2020.
* [20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [21] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.
* [22] Rui Huang and Yixuan Li. Towards scaling out-of-distribution detection for large semantic space. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021.
* [23] Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shifts in the wild. _Advances in Neural Information Processing Systems_, 34:677-689, 2021.
* [24] Conor Igoe, Youngseog Chung, Ian Char, and Jeff Schneider. How useful are gradients for ood detection really? _arXiv preprint arXiv:2205.10439_, 2022.
* [25] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In _International conference on machine learning_, pages 1885-1894. PMLR, 2017.
* [26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. _Master's thesis_, 2009.
* [27] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Advances in neural information processing systems_, pages 6402-6413, 2017.
* [28] Jinsol Lee and Ghassan AlRegib. Gradients as a measure of uncertainty in neural networks. In _2020 IEEE International Conference on Image Processing (ICIP)_, pages 2416-2420. IEEE, 2020.
* [29] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. _Advances in neural information processing systems_, 31, 2018.
* [30] Xin Li, Sima Behpour, Thang Doan, Wenbin He, Liang Gou, and Liu Ren. Up-dp: Unsupervised prompt learning for data pre-selection with vision-language models. _arXiv preprint arXiv:2307.11227_, 2023.
* [31] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. _arXiv preprint arXiv:1706.02690_, 2017.
* [32] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In _6th International Conference on Learning Representations, ICLR 2018_, 2018.
* [33] Ziqian Lin, Sreya Dutta Roy, and Yixuan Li. Mood: Multi-level out-of-distribution detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021.
* [34] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [35] Zhenhua Liu, Jizheng Xu, Xiulian Peng, and Ruiqin Xiong. Frequency-domain dynamic pruning for convolutional neural networks. In _Advances in Neural Information Processing Systems 31_, pages 1043-1053. 2018.

* [36] David Madras, James Atwood, and Alex D'Amour. Detecting underspecification with local ensembles. _arXiv preprint arXiv:1910.09573_, 2019.
* [37] Andrey Malinin, Neil Band, German Chesnokov, Yarin Gal, Mark JF Gales, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, et al. Shifts: A dataset of real distributional shift across multiple large-scale tasks. _arXiv preprint arXiv:2107.07455_, 2021.
* [38] Sina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang. Self-supervised learning for generalizable out-of-distribution detection. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 5216-5223, 2020.
* [39] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do deep generative models know what they don't know? In _International Conference on Learning Representations_, 2018.
* [40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. _NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011_, 2011.
* [41] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 427-436, 2015.
* [42] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. _Advances in Neural Information Processing Systems_, 32:13991-14002, 2019.
* [43] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1406-1415, 2019.
* [44] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4510-4520, 2018.
* [45] Chandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with gram matrices. In _International Conference on Machine Learning_, pages 8491-8501. PMLR, 2020.
* [46] Chandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with in-distribution examples and gram matrices. _arXiv e-prints_, pages arXiv-1912, 2019.
* [47] Yiyou Sun and Yixuan Li. Dice: Leveraging sparsification for out-of-distribution detection. In _European Conference on Computer Vision_, 2022.
* [48] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations. _Advances in Neural Information Processing Systems_, 34:144-157, 2021.
* [49] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations. _Advances in Neural Information Processing Systems_, 34:144-157, 2021.
* [50] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. _arXiv preprint arXiv:2204.06507_, 2022.
* [51] Sridhar Swaminathan, Deepak Garg, Rajkumar Kannan, and Frederic Andres. Sparse low rank factorization for deep neural network compression. _Neurocomputing_, 398:185-196, 2020.
* [52] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. _arXiv preprint arXiv:1511.06067_, 2015.

* [53] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8769-8778, 2018.
* [54] Haoran Wang, Weitang Liu, Alex Bocchieri, and Yixuan Li. Can multi-label classification networks know what they don't know? _Advances in Neural Information Processing Systems_, 2021.
* [55] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2097-2106, 2017.
* [56] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _2010 IEEE computer society conference on computer vision and pattern recognition_, pages 3485-3492. IEEE, 2010.
* [57] Mingyu Xu, Zheng Lian, Bin Liu, and Jianhua Tao. Vra: Variational rectified activation for out-of-distribution detection, 2023.
* [58] Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. _arXiv preprint arXiv:1504.06755_, 2015.
* [59] Huanrui Yang, Minxue Tang, Wei Wen, Feng Yan, Daniel Hu, Ang Li, Hai Li, and Yiran Chen. Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsification. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_, pages 678-679, 2020.
* [60] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. _arXiv preprint arXiv:2110.11334_, 2021.
* [61] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. _arXiv preprint arXiv:1506.03365_, 2015.
* [62] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* [63] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _IEEE transactions on pattern analysis and machine intelligence_, 40(6):1452-1464, 2017.
* [64] Helen Zhou, Cheng Cheng, Zachary C Lipton, George H Chen, and Jeremy C Weiss. Mortality risk score for critically ill patients with viral or unspecified pneumonia: Assisting clinicians with covid-19 ecmo planning. In _International Conference on Artificial Intelligence in Medicine_, pages 336-347. Springer, 2020.
* [65] Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and Xiaogang Wang. Learning spatial regularization with image-level supervisions for multi-label image classification. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5513-5522, 2017.

## Broader Impact

The objective of this research project is to enhance the dependability and safety of contemporary machine learning models. The outcomes of our investigation possess the potential for significant advantages and societal effects, particularly in the realm of safety-critical domains like autonomous driving. It is important to note that our research does not involve any human subjects or breach legal compliance. We do not foresee any possible adverse ramifications resulting from our work. By conducting this study, our intention is to foster increased research efforts and raise awareness within both the research community and society at large regarding the issue of out-of-distribution detection in practical, real-world scenarios.

## Limitation

OOD detection methods may not always detect out-of-distribution samples accurately. As it is presented in our experiments, most OOD detection methods are not able to recognize OOD data across different OOD datasets. They may provide superior performance on some OOD data while catastrophic performance on some other variations of OOD data. Though our method, Gradorth, presents comparable performance to state-of-the-art methods, it still requires more investigation to achieve full stable performance across all OOD datasets.

## Appendix

The Appendix section is organized as follows:

* Section A presents the relationship between stochastic gradient descent (SGD) updates and the span of input data points in convolutional layers.
* Section B presents an experimental study that evaluates the role of leveraging the last-layer gradient of the network in our method (GradOrth).
* Section C discusses the results of the experimental analysis on the computation of the ID subspace leveraging different numbers of ID samples per class.
* Section D includes an explanation of the cross-entropy loss and its derivative.
* Section E presents proof showing that stochastic gradient descent (SGD) updates lie in the span of input data points in a mini-batch setting.
* Section F offers a brief explanation of singular value decomposition (SVD).
* Section G presents an ablation study that evaluates the impact of SVD on the GradOrth out-of-distribution (OOD) detection performance.
* Section H offers more explanation regarding \(k\)-rank matrix approximation.
* Section I examines the impact of threshold \(\epsilon_{th}\) on GradOrth performance.
* Section J provides further information about our experiments, including networks, datasets, and the scoring functions of the baselines.

## Appendix A Input and Gradient Spaces in Convolutional Layers

Our algorithm exploits the observation that updates in stochastic gradient descent (SGD) reside within the subspace spanned by the input data points [62], as discussed in section 2.1 and this appendix, section D. In this section, we aim to establish this relationship specifically for convolutional layers. The analysis presented herein possesses general applicability to any layer within a network, regardless of the task.

In contrast to the weights in a fully connected (FC) layer, filters within a convolutional (Conv) layer operate differently on the input. Let us consider a convolutional layer comprising the input tensor \(\mathcal{X}\in\mathbb{R}^{C_{i}\times h_{i}\times w_{i}}\) and filters \(\bm{\theta}\in\mathbb{R}^{C_{o}\times C_{i}\times k\times k}\). Their convolution, denoted as \(\langle\mathcal{X},\bm{\theta},*\rangle\), yields the output feature map \(\mathcal{O}\in\mathbb{R}^{C_{o}\times h_{o}\times w_{o}}\)[35]. Here, \(C_{i}\) (\(C_{o}\)) represents the number of input (output) channels in the Conv layer, while \(h_{i}\), \(w_{i}\) (\(h_{o}\), \(w_{o}\)) correspond to the height and width of the input (output) feature maps, and \(k\) denotes the kernel size of the filters. Figure 4(a) provides a visual representation of this process.

If we reshape \(\mathcal{X}\) into a \((h_{o}\times w_{o})\times(C_{i}\times k\times k)\) matrix denoted as \(\bm{x}\), and reshape \(\bm{\theta}\) into a \((C_{i}\times k\times k)\times C_{o}\) matrix denoted as \(\bm{\theta}\), the convolution can be expressed as a matrix multiplication between \(\bm{x}\) and \(\bm{\theta}\), yielding \(\mathbf{O}=\bm{x}\bm{\theta}\), where \(\mathbf{O}\in\mathbb{R}^{(h_{0}\times w_{0})\times C_{o}}\).

Formulating the convolution in terms of matrix multiplication provides an intuitive depiction of gradient computation during the back-propagation process. Similar to the fully connected layer scenario, in the convolutional layer, during the backward pass, an error matrix \(\bm{\Omega}\) of size \((h_{0}\times w_{0})\times C_{o}\) (equivalent to the size of \(\mathbf{O}\)) is obtained from the subsequent layer. As illustrated in figure 4(b), the gradient of the loss with respect to the filter weights is computed as follows:

\[\nabla_{\bm{\theta}}L=\bm{x}^{T}\bm{\Omega},\] (8)

where \(\nabla_{\bm{\theta}}L\) possesses a shape of \((C_{i}\times k\times k)\times C_{o}\) (matching the size of \(\bm{\theta}\)). Considering that the columns of \(\bm{x}^{T}\) correspond to the input.

## Appendix B GradOrth Considering All The Network Layers

This experimental study presents that the gradients obtained from the final layer contain substantial and informative content.In this experimental study, we aim to investigate the content and significance of gradients obtained from the final layer in a neural network. We explore an alternative variation of the GradOrth method, focusing on the extraction of gradients from all layers of the network. The objective is to analyze the gradients of all trainable parameters across the layers and assess their informativeness. In this paper, we present this version of GradOrth as "GradOrth-All layers" and compare it with the original GradOrth, which utilizes the gradient space from the last layer. To assess the performance of the various gradient spaces, we perform a gradient projection for each layer of the neural network. Subsequently, we calculate the average of these gradient projections across all layers. This procedure allows us to derive an OOD detection score, which we refer to as the OODness score. The experiment is conducted over five random subsets of the ID subspaces. Each subspace is computed by utilizing ten random samples per class in the ImageNet benchmark and five random samples per class in CIFAR benchmarks. We report the average results obtained from these five runs. The outcomes of both GradOrth-All layers and GradOrth-Last layer are presented in tables 6, 7, and 8.

table 6 provides a comparison of the OOD detection performance using the gradient spaces from different layers (all network layers and the last network layer) on two pre-trained network architectures, ResNet and MobileNet, on the ImageNet dataset as ID data. The evaluation metrics used are the False Positive Rate at \(95\%\) True Positive Rate (FPR95) and the Area Under the Receiver Operating Characteristic curve (AUROC). These metrics are averaged across four OOD datasets.

Our findings demonstrate that gradients from the last layer consistently outperform gradients from all layers. On ResNet and MobileNet, leveraging the last-layer gradient space results in a \(1.57\%\) and \(1.52\%\) improvement in FPR95 (on average), respectively, compared to GradOrth utilizing gradients from all network layers.

In the pursuit of an extensive investigation, the CIFAR benchmark is taken into account to examine the impact of various network gradient spaces, namely the last network layer and all network layers, on the performance of GradOrth. The findings, depicted in tables 7 and 8, reveal that utilizing last layer gradients in GradOrth yields superior results compared to employing gradients from all network layers, with improvements of \(0.82\%\) and \(1.49\%\) in FPR95 on average, respectively.

This observed outcome is highly advantageous, as gradients calculated with respect to deeper layers demonstrate computational efficiency when compared to utilizing gradients from all layers. Remarkably, the GradOrth variant derived from the final linear layer exhibits the most favorable outcomes. From a practical perspective, it is only necessary to perform back-propagation with respect

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Methods**} & \multicolumn{4}{c|}{**OOD Datasets**} & \multicolumn{4}{c|}{**Average**} \\ \cline{3-13}  & & **NeNaturalist** & \multicolumn{2}{c|}{**SUN**} & \multicolumn{2}{c|}{**Paces**} & \multicolumn{2}{c|}{**Textures**} & \multicolumn{2}{c|}{**Average**} \\  & & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\  & & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\uparrow\) \\ \hline \multirow{2}{*}{ResNet} & GradOrth-All layers & 13.23 & 96.52 & 21.05 & 95.21 & 35.58 & 92.49 & 11.56 & 96.72 & 20.35 & 95.23 \\  & GradOrth-Last layer & 11.04 & 98.00 & 19.61 & 95.76 & 33.67 & 91.78 & 11.19 & 98.06 & 18.57 & 96.31 \\ \hline \multirow{2}{*}{MobileNet} & GradOrth-All layers & 26.14 & 93.83 & 33.28 & 91.38 & 43.71 & 85.37 & 13.61 & 96.87 & 29.17 & 91.86 \\  & GradOrth-Last layer & 26.81 & 93.17 & 30.82 & 93.18 & 40.27 & 89.12 & 12.69 & 97.52 & 27.65 & 93.25 \\ \hline \end{tabular}
\end{table}
Table 6: OOD detection results with ImageNet-It has ID. Effect of leveraging all-layers and last-layer gradients space in GradOrth. The OODness score derived from the last layer yields better OOD detection performance mostly. The ResNet and MobileNet models are pre-trained solely with ID data from the ImageNet-It is dataset. We use \(\uparrow\) to denote that larger values are preferable, and \(\downarrow\) to denote that smaller values are preferable. All values are presented as percentages.

Figure 4: The convolution operation in matrix multiplication format during the forward Pass (a) and backward pass (b).

to the last linear layer, resulting in minimal computational overhead. Therefore, our primary findings are predicated on the utilization of the last fully connected (FC) layer within the neural network.

## Appendix C Analysis of the Number of ID Samples

The initial step in our proposed method involves computing the subspace of the pre-trained neural network using the ID data. To accomplish this, we utilize a small number of data samples and pass them through the forward pass of the pre-trained network, without altering the learned parameters. Subsequently, we compute the subspace based on the last layer of the network.

To ensure a comprehensive study, we conduct an empirical investigation and compute variations of subspaces by considering different numbers of samples per class. Specifically, we vary the number of samples from 5 to 40 and 10 to 30 in the CIFAR and ImageNet1K benchmarks, respectively. For each variation, we compute five random subspaces and report the average results obtained from these five subspaces. The experimental results obtained from our study are presented in tables 9 and 10. Specifically, we introduce a notation to describe the experiments using GradOrth on the pre-trained network subspace computed based on a specific number of samples per class. We denote this notation as GradOrth-S\({}_{n}\), where \(n\) represents the number of samples per class used to compute the subspace.

The experimental results displayed in tables 9 and 10 indicate that increasing the number of samples per class during the computation of the subspace for the pre-trained network does not have a significant impact on the overall performance of OOD detection. This observation suggests that leveraging a pre-trained network, which has already learned the data well, diminishes the influence of the number of samples per class in the subspace computation.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{**Model**} & \multicolumn{3}{c|}{**SVM**} & \multicolumn{3}{c|}{**LSUN-e**} & \multicolumn{3}{c|}{**LSUN-e**} & \multicolumn{3}{c|}{**LSUN-e**} & \multicolumn{3}{c|}{**LSUN**} & \multicolumn{3}{c|}{**Testures**} & \multicolumn{3}{c|}{**P Places305**} & \multicolumn{3}{c|}{**Average**} \\  & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\  & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) \\ \hline GradOrth-All layers & 7.61 & 92.84 & 4.32 & 97.63 & 4.92 & 97.84 & 44.00 & 90.71 & 34.86 & 91.25 & 48.92 & 88.33 & 34.84 & 91.70 \\ GradOrth-Last layer & 34.27 & 93.37 & 3.71 & 99.07 & 48.09 & 91.26 & 42.73 & 91.48 & 23.71 & 92.62 & 48.61 & 89.03 & 33.35 & 92.82 \\ GradOrth-S\({}_{n}\) & 5.61 & 98.00 & 6.99 & 99.81 & 2.27 & 98.47 & 41.77 & 98.35 & 20.59 & 94.79 & 38.09 & 91.71 & 11.90 & 97.03 \\ GradOrth-S\({}_{n}\) & 5.62 & 98.79 & 6.99 & 99.83 & 2.24 & 98.75 & 41.6 & 98.38 & 20.55 & 94.84 & 38.11 & 91.74 & 11.90 & 97.06 \\ \hline \end{tabular}
\end{table}
Table 8: GradOrth leveraging the last-layer network’s gradient space outperforms Gradorth leveraging all network layers’ gradient space on the CIFAR100 dataset. Detailed results on six common OOD benchmark datasets with **CIFAR-100** as ID: Textures [8], SVHN [40], Places365 [63], LSUN-Crop [61], LSUN-Resize [61], and iSUN [58]. For each ID dataset, we use the same DenseNet pre-trained on _CIFAR-100_. \(\uparrow\) indicates larger values are better and \(\downarrow\) indicates smaller values are better.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c|}{**SVM**} & \multicolumn{3}{c|}{**LSUN-e**} & \multicolumn{3}{c|}{**LSUN-e**} & \multicolumn{3}{c|}{**LSUN-e**} & \multicolumn{3}{c|}{**LSUN-e**} & \multicolumn{3}{c|}{**Features**} & \multicolumn{3}{c|}{**P Places305**} & \multicolumn{3}{c|}{**Average**} \\  & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\  & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) \\ \hline GradOrth-All layers & 7.61 & 92.84 & 4.32 & 97.63 & 4.92 & 89.74 & 44.00 & 90.71 & 34.86 & 91.25 & 48.92 & 88.33 & 34.84 & 91.70 \\ GradOrth-Last layer & 34.27 & 93.37 & 3.71 & 99.07 & 48.09 & 91.26 & 42.73 & 91.48 & 20.37 & 94.82 & 83.13 & 92.01

[MISSING_PAGE_FAIL:19]

\[\nabla L_{(\bm{x},\bm{y})}(\bm{\theta})=\rho\bm{x}^{T},\] (16)

As a result, the gradient update will be confined within the input span (\(\bm{x}\)), where the elements in \(\bm{\rho}\) display heterogeneous magnitudes, thereby impacting the scaling of \(\bm{x}\) correspondingly.

## Appendix E Gradient Span Proof

Considering that the batch loss is the summation of losses incurred by individual examples, the overall batch loss for \(n\) samples can be represented as:

\[L_{\text{batch}}=\sum_{i=1}^{n}L_{i},\] (17)

where \(L_{i}\) represents the loss of sample \((x_{i},y_{i})\).

When employing the mean-squared error loss function, the loss of a batch is calculated as the sum of the losses of individual samples, which can be expressed as:

\[L_{\text{batch}}=\sum_{i=1}^{n}L_{i}=\sum_{i=1}^{n}\frac{1}{2}||\bm{\theta} \bm{x}_{i}-\bm{y}_{i}||_{2}^{2}.\] (18)

Following stochastic gradient optimization, we can present the gradient of this loss (per sample) with respect to weights as:

\[\nabla_{\bm{\theta}}\mathcal{L}=(\bm{\theta}\bm{x}-\bm{y})\bm{x}^{T}=\bm{ \Omega}\bm{x}^{T},\] (19)

Here, \(\bm{\Omega}\in\mathbb{R}^{m}\) denotes the error vector. Therefore, the gradient of the batch loss with respect to the weights can be expressed as:

\[\nabla_{\bm{\theta}}L_{\text{batch}}=\bm{\Omega}_{1}\bm{x}_{1}^{T}+\bm{ \Omega}_{2}\bm{x}_{2}^{T}+\ldots+\bm{\Omega}_{n}\bm{x}_{n}^{T}.\] (20)

It is noteworthy that the gradient update remains confined within the subspace spanned by the \(n\) input examples.

Considering cross-entropy (CE) loss as the desired loss, the batch loss would be the sum of the losses of individual samples as follows:

\[L_{\text{batch}}=\sum_{i=1}^{n}L_{i}=\sum_{i=1}^{n}\sum_{k=1}^{m}-y_{k}\log a_ {k},\] (21)

Considering equation 16, the gradient of this loss with respect to the weights can be presented as:

\[\nabla_{\bm{\theta}}L_{\text{batch}}=\bm{\rho}_{1}\bm{x}_{1}^{T}+\bm{\rho}_{ 2}\bm{x}_{2}^{T}+\ldots+\bm{\rho}_{n}\bm{x}_{n}^{T}.\] (22)

It is important to note that the gradient update is constrained within the subspace spanned by the \(n\) input examples.

## Appendix F Singular Value Decomposition (SVD) Explanation

Consider an \(m\times n\) matrix \(\bm{R}\), where \(m\) is the number of rows and \(n\) is the number of columns. The goal of SVD is to factorize matrix \(\bm{R}\) into three separate matrices: \(\bm{U},\bm{\Sigma},\text{and }\bm{V}^{T}\) (transpose of matrix \(\bm{V}\)), such that \(\bm{R}=\bm{U}\bm{\Sigma}\bm{V}^{T}\in\mathbb{R}^{m\times n}\), as presented in figure 5.

\(\bm{U}\): An \(m\times m\) orthogonal matrix, where the columns represent the left singular vectors of \(\bm{R}\). \(\bm{\Sigma}\): An \(m\times n\) diagonal matrix, where the diagonal entries are the singular values of \(\bm{R}\) (non-negative and sorted in descending order). \(\bm{V}^{T}\): An \(n\times n\) orthogonal matrix, where the columns represent the right singular vectors of \(\bm{R}\).

Along with singular values and singular vectors, eigen-values and eigen-vectors are also defined. An eigenvalue \(\lambda\) and its corresponding eigenvector v of a square matrix \(\bm{R}\) satisfy the equation \(\bm{R}=\lambda\bm{V}\). Eigen-vectors represent directions in the vector space that are only scaled by the matrix \(\bm{R}\), while eigenvalues represent the scaling factors for those eigen-vectors.

SVD and Relationship to Eigen-values and Eigen-vectors: SVD connects eigen-values and eigenvectors with the singular values and singular vectors of a matrix. The singular values of \(\bm{R}\) are the square roots of the eigen-values of \(\bm{R}\bm{R}^{T}\) or \(\bm{R}^{T}\bm{R}\), and the left and right singular vectors are the eigen-vectors of \(\bm{R}\bm{R}^{T}\) and \(\bm{R}^{T}\bm{R}\), respectively.

Rank and Matrix Approximation: The rank of a matrix \(\bm{R}\) is determined by the number of non-zero singular values in \(\bm{\Sigma}\). By keeping only the largest singular values and their corresponding singular vectors, it is possible to approximate the original matrix \(\bm{R}\) with a lower-rank approximation, which can be useful for dimensionality reduction and noise reduction. We leverage this feature in our approach.

Properties of SVD:

The singular values in \(\bm{\Sigma}\) are non-negative and arranged in descending order. The columns of \(\bm{U}\) and \(\bm{V}\) are orthonormal, meaning they form an orthogonal basis for their respective vector spaces. The SVD decomposition is unique up to the sign of the singular values and the order of the singular vectors. SVD is a powerful matrix factorization technique that provides a compact representation of a matrix while preserving important structural properties. It finds widespread applications in various fields, such as data analysis, image processing, recommendation systems, and more [10].

## Appendix G Ablation study on the Impact of SVD

In this experiment, our focus is to examine the impact of utilizing singular value decomposition (SVD) in the GradOrth method as an ablation study. To achieve this, we compute the space (as opposed to the subspace) of the pre-trained network using the ID data, without employing SVD. Consequently, the OODness score in GradOrth-NoSVD solely incorporates the orthogonal projection of the new sample onto the _space_ of the ID pre-trained network. The experimental results obtained from both the ImageNet and CIFAR benchmarks are presented in tables 12, 13, and 14. The outcomes reported in tables 12, 13, and 14 demonstrate the significant and robust performance of _GradOrth_ compared to the GradOrth variant without SVD (_GradOrth-NoSVD_). On the ImageNet benchmark, GradOrth surpasses GradOrth-NoSVD by an average of \(14.12\%\) and \(10.93\%\) in terms of FPR95 for the ResNet and MobileNet pre-trained networks, respectively. Furthermore, GradOrth exhibits exceptional performance on the CIFAR benchmark, outperforming GradOrth-NoSVD by \(6.44\%\) and \(8.06\%\) on CIFAR10 and CIFAR100, respectively. The results obtained from this ablation study emphasize the significance of employing SVD in the GradOrth method's OODness score. It underscores the core principle of our approach, which suggests that the essential discriminative features for identifying OOD data reside within the subspace of the ID data.

Figure 5: Singular Value Decomposition

## Appendix H _K_-Rank Matrix Approximation

Singular Value Decomposition (SVD) can be used to factorize a rectangular matrix, \(\bm{R}=\bm{U}\bm{\Sigma}\bm{V}^{T}\in\mathbb{R}^{m\times n}\) into the product of three matrices, where \(\bm{U}\in\mathbb{R}^{m\times m}\) and \(\bm{V}\in\mathbb{R}^{n\times n}\) are orthonomal matrices, and \(\bm{\Sigma}\) is a diagonal matrix that contains the sorted singular values along its main diagonal (Deisenroth et al. [10]). If the rank of the matrix is \(r\) (\(r\leq\text{min}(m,n)\)), \(\bm{R}\) can be expressed as \(\bm{R}=\sum_{i=1}^{r}\sigma_{i}\bm{u}_{i}\bm{v}_{i}^{T}\), where \(\bm{u}_{i}\in\bm{U}\) and \(\bm{v}_{i}\in\bm{V}\) are left and right singular vectors and \(\sigma_{i}\in diag(\bm{\Sigma})\) are singular values. \(k\)-rank approximation of \(\bm{R}\) can be written as, \(\bm{R}_{k}=\sum_{i=1}^{k}\sigma_{i}\bm{u}_{i}\bm{v}_{i}^{T}\), where \(k\leq r\) and its value can be chosen by the smallest \(k\) that satisfies the norm-based criteria : \(||\bm{R}_{k}||_{F}^{2}\geq\epsilon_{th}||\bm{R}||_{F}^{2}\). Here, \(||.||_{F}\) denotes the Frobenius norm of the matrix and \(\epsilon_{th}\in(0,1)\) is the threshold hyperparameter.

\[\bm{R}_{k}=\sum_{i=1}^{k}\sigma_{i}\bm{u}_{i}\bm{v}_{i}^{T}\] (23)

\[||\bm{R}-\bm{R}_{k}||^{2}=\sum_{i=1}^{m}\sum_{j=1}^{n}\lvert a_{ij}-\hat{a}_{ ij}\rvert^{2}\sum_{j=k+1}^{r}\sigma_{j}^{2}\quad 0\leq k\leq n\text{ where }R_{k}=\hat{a}_{ij}.\] (24)

The degree to which \(R_{k}\) approximates \(R\) depends on the sum of the r-k smallest singular values squared. As k approaches r, this sum becomes progressively smaller and eventually goes to zero at \(k=r\). To provide a convenient measure for this behavior independent of the size of \(R\), let us consider the normalized matrix approximation ratio

\[\epsilon_{th}(k)=\frac{||R_{k}||}{||R||}=[\frac{\sigma_{1}^{2}+\sigma_{2}^{2} +...+\sigma_{k}^{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}+...+\sigma_{r}^{2}}]^{\frac {1}{2}},1\leq k\leq r.\] (25)

Clearly, this normalized ratio approaches its maximum value of 1 as k approaches r. For matrices of low effective rank, \(\epsilon_{th}(k)\) is close to 1 for values of k significantly smaller than r. On the other hand, matrices for which \(m\) must take on high values (i.e., \(k\approx r\)) to achieve a \(\epsilon_{th}(k)\) near 1 are said to be of high effective rank [3].

## Appendix I Impact of the Threshold Parameter (\(\epsilon_{th}\)) on GradOrth Performance

The hyperparameter \(\epsilon_{th}\), confined to the range \((0,1)\), serves as a threshold that influences the selection of the value of \(k\) in the matrix \(k\)-rank approximation. The initial \(k\) column vectors within matrix

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c|}{**SVM**} & \multicolumn{3}{c|}{**LSUNe**} & \multicolumn{3}{c|}{**LSUNe**} & \multicolumn{3}{c|}{**LSUN**} & \multicolumn{3}{c|}{**UNN**} & \multicolumn{3}{c|}{**Features**} & \multicolumn{3}{c|}{**Places36**} & \multicolumn{3}{c|}{**Average**} \\  & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\  & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) & \(\downarrow\) & \(\uparrow\) \\ \hline GradOrth-NoSVD & 18.32 & 94.03 & 1.12 & 99.57 & 3.56 & 98.74 & 1.029 & 98.00 & 31.17 & 91.06 & 46.22 & 85.62 & 18.78 & 94.85 \\ GradOrth & 5.84 & 98.72 & 0.81 & 99.78 & 2.33 & 98.71 & 4.25 & 98.32 & 20.63 & 94.77 & 38.22 & 91.64 & 12.34 & 96.59 \\ \hline \end{tabular}
\end{table}
Table 13: Ablation study to recognize the importance of SVD in GradOrth. Detailed results on six common OOD benchmark datasets with CIFAR-10 as ID: Textures [8], SVHN [40], Places65 [63], LSUN-Crop [61], LSUN-Resize [61], and iSUN [58]. For each ID dataset, we use the same DenseNet pre-trained on _CIFAR-10_. \(\uparrow\) indicates larger values are better and \(\downarrow\) indicates smaller values are better.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{**Model**} & \multicolumn{3}{c|}{**Naturalist**} & \multicolumn{3}{c|}{**OOD Datasets**} & \multicolumn{3}{c|}{**Average**} \\  & & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\ \hline \multirow{2}{*}{ResNet} & GradOrth-NoSVD & 28.31 & 94.30 & 30.09 & 93.73 & 44.48 & 81.84 & 28.17 & 93.91 & 32.69 & 90.67 \\  & GradOrth & 11.04 & 98.00 & 19.61 & 95.76 & 33.67 & 91.78 & 11.98 & 98.06 & 18.57 & 96.31 \\ \hline \multirow{2}{*}{MobileNet} & GradOrth-NoSVD & 38.45 & 90.21 & 38.02 & 89.26 & 49.31 & 90.18 & 28.52 & 89.68 & 38.58 & 89.83 \\  & GradOrth & 26.81 & 93.17 & 30.82 & 93.18 & 40.27 & 89.12 & 12.69 & 97.52 & 27.65 & 93.25 \\ \hline \end{tabular}
\end{table}
Table 12: Ablation study to present the importance of SVD in GradOrth. GradOrth presents outstanding performance on average and across all datasets. OOD detection results with **ImageNet-1k** as ID. GradOrth presents outstanding performance on average and across all datasets. The ResNet and MobileNet models are pre-trained solely by ID data from the ImageNet-1k dataset. We use \(\uparrow\) to denote that larger values are preferable, and \(\downarrow\) to denote that smaller values are preferable.

encompass the most pivotal input (representation) space for the pre-trained network. We conducted an experiment to assess the impact of \(\epsilon_{th}\) on GradOrth's performance, with results outlined in table 15. Notably, values of \(\epsilon_{th}\) near 1 exhibit substantial effectiveness, and we empirically set it to \(0.97\).

## Appendix J Details of Experiments

### Model and Hyper Parameter

In our empirical studies and experiments, we adopt an experimental setting that aligns with the state-of-the-art (SOTA) approaches, specifically ASH [12] and DICE [47] on the CIFAR dataset, as well as ReAct [48] on the ImageNet dataset. The datasets and model architectures utilized in our experiments are summarized in table 16.

For the CIFAR-10 and CIFAR-100 experiments, we employ the six OOD datasets employed in the DICE study [47]: SVHN [40], LSUN-Crop [61], LSUN-Resize [61], iSUN [58], Places365 [63], and Textures [8]. The ID dataset used in these experiments corresponds to the respective CIFAR dataset. The model architecture employed is a pre-trained DenseNet-101 [21].

For our ImageNet experiments, we adhere to the precise setup as outlined in the ReAct study [48] and [12]. The ID dataset employed in this context is ImageNet-1k, while the OOD datasets consist of iNaturalist [53], SUN [56], Places365 [63], and Textures [8]. The network architectures utilized in these experiments are ResNet50 [16] and MobileNetV2 [44]. All networks undergo pre-training using the ID data and remain unaltered post-training, with their parameters remaining unchanged during the OOD detection phase. The performance of the baselines primarily relies on ASH and VRA. When conducting experiments involving gradient-based methods such as GradNorm [23] and ExGrad [24], we re-run the experiments using the code provided by the respective authors, as there may be variations between our pre-trained network over ID data and the models employed by the authors themselves.

### Datasets

ImageNet Benchmark, Large-scale evaluationIn this study, the ImageNet-1k dataset [11] is employed as the ID dataset. The evaluation of the proposed approach is conducted on four OOD test datasets, following the experimental setup outlined in [23]:

* **iNaturalist** The dataset introduced by [53], referred to as "iNaturalist", comprises a substantial collection of 859,000 images featuring various plant and animal species. These images span more than 5,000 distinct species. To facilitate efficient processing, each image within the dataset is resized to ensure that the maximum dimension does not exceed 800 pixels. For the evaluation phase, a subset of 10,000 images is randomly sampled from a set of 110 classes. Importantly, these selected classes are disjoint from the ImageNet-1k dataset, thereby ensuring the validity and independence of the evaluation process.
* **SUN** that is introduced by Xiao et al., encompasses a vast collection of over 130,000 images representing various scenes. These scenes are categorized into 397 distinct categories.

\begin{table}
\begin{tabular}{|c|c|c|} \hline
**ID Dataset** & **OOD Datasets** & **Model architectures** \\ \hline CIFAR-10 & SVHN, LSUN C, LSUN R, iSUN, Places365, Textures & DenseNet-101 \\ \hline CIFAR-100 & SVHN, LSUN C, LSUN R, iSUN, Places365, Textures & DenseNet-101 \\ \hline ImageNet & iNaturalist, SUN, Places365, Textures & ResNet50, MobileNetV2 \\ \hline \end{tabular}
\end{table}
Table 16: The datasets and models we used in our OOD experiments range from moderate to large scale, including evaluations of up to 10 OOD datasets and three architectures.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{**Model**} & \multicolumn{6}{c|}{**OOD Datasets**} \\ \cline{2-13}  & \multicolumn{2}{c|}{**SVHN**} & \multicolumn{2}{c|}{**LSUN-C**} & \multicolumn{2}{c|}{**ISUN**} & \multicolumn{2}{c|}{**ISUN**} & \multicolumn{2}{c|}{**Training**} & \multicolumn{2}{c|}{**PnasOf**} & \multicolumn{2}{c|}{**Average**} \\ \cline{2-13}  & FP959 & AUROC & FP959 & AUROC & FP959 & AUROC & FP959 & AUROC & FP959 & AUROC & FP959 & AUROC & FP959 & AUROC \\  & j & 7 & i & 7 & i & 7 & i & 1 & 7 & i & 7 & i & 7 \\ \hline GradOrth-\(\epsilon_{th}=0.80\) & 24.57 & 93.39 & 3.28 & 98.87 & 48.19 & 91.54 & 42.80 & 91.34 & 28.23 & 92.54 & 48.70 & 88.93 & 33.49 & 92.70 \\ GradOrth-\(\epsilon_{th}=0.90\) & 24.36 & 93.42 & 3.75 & 99.00 & 48.12 & 91.24 & 27.74 & 91.46 & 32.76 & 92.58 & 46.84 & 90.01 & 33.39 & 92.78 \\ GradOrth-\(\epsilon_{th}=0.97\) & 24.27 & 93.47 & 3.71 & 99.07 & 48.09 & 91.26 & 42.73 & 91.48 & 32.71 & 92.62 & 48.61 & 89.03 & 33.35 & 92.82 \\ \hline \end{tabular}
\end{table}
Table 15: Detailed results on six common OOD benchmark datasets considering different values for \(\epsilon_{th}\) used in \(k\)-rank matrix approximation. For the pre-trained network and ID dataset, we use DenseNet pre-trained on _CIFAR-100_.

Notably, it is important to acknowledge that there are overlapping categories between the SUN dataset and the ImageNet-1k dataset. For the evaluation process, a subset of 10,000 images is randomly sampled from a set of 50 classes. Importantly, these selected classes are disjoint from the labels present in the ImageNet dataset. This ensures the integrity and independence of the evaluation conducted in the study.
* **Places** The dataset introduced by [63], commonly referred to as "Places" in the research literature, is another notable scene dataset that exhibits similar concept coverage as the SUN dataset. In this study, a carefully selected subset of 10,000 images is utilized from a total of 50 classes. Importantly, these selected classes are intentionally excluded from the ImageNet-1k dataset, ensuring that the evaluation process remains independent and free from any potential overlap with the aforementioned dataset.
* **Textures**[7] consisting of 5,640 real-world texture images categorized into 47 distinct categories, is utilized in this research study. For the purpose of evaluation, the entire dataset is utilized, ensuring comprehensive coverage across all available categories.

CIFAR BenchmarkThe CIFAR-10 and CIFAR-100 datasets, introduced by [26], are extensively employed as ID datasets in the existing literature. CIFAR-10 comprises 10 classes, while CIFAR-100 consists of 100 classes. In line with standard practices, the dataset split utilized in this study consists of 50,000 training images and 10,000 test images. To evaluate the proposed approach, four commonly employed OOD datasets are utilized. The specific OOD datasets used in this evaluation are listed below:

* **SVHN**[40] The Street View House Numbers (SVHN) dataset consists of color images depicting house numbers. The dataset encompasses ten distinct classes corresponding to the digits 0-9. In this research study, the entire test set comprising 26,032 images is employed for evaluation purposes.
* **LSUN**[61] includes a collection of 10,000 testing images featuring 10 different scenes. In this research study, image patches of size 32x32 are randomly cropped from the LSUN dataset to facilitate analysis and experimentation.
* **Places365**[63] comprises a vast collection of large-scale photographs depicting scenes across 365 distinct scene categories. Notably, the test set of this dataset consists of 900 images per category. For the evaluation phase, a random sample of 10,000 images is drawn from the test set, thereby ensuring a representative subset for rigorous analysis and assessment.
* **Textures**[7] includes a comprehensive collection of 5,640 real-world texture images, classified into 47 distinct categories. In this research study, the entire dataset is utilized for evaluation, enabling a thorough examination of the performance and capabilities of the proposed approach across all available texture categories.

### Baselines

For the reader's convenience, we summarize in detail a few common techniques for defining OOD scores that measure the degree of ID-ness on the given sample. By convention, a higher (lower) score is indicative of being in-distribution (out-of-distribution).

MSP[18]utilizes probabilities obtained from softmax distributions to distinguish between correctly classified and erroneous or out-of-distribution examples. The baseline method relies on the observation that correctly classified examples tend to have higher maximum softmax probabilities than misclassified and out-of-distribution examples.

ODIN[32]is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection.

Mahalanobis[29]utilizes multivariate Gaussian distributions to effectively model the class-conditional distributions of softmax neural classifiers. Additionally, they employed Mahalanobis distance-based scores as a means of detecting out-of-distribution samples.

Energy Score [34]The concept of utilizing energy scores for estimating out-of-distribution uncertainty was initially introduced by Liu et al.. The energy function employed in their approach maps the logit outputs to a scalar value denoted as \(S_{\mathrm{Energy}}(x;f)\in\mathbb{R}\). Notably, this scalar value tends to be relatively lower for in-distribution data. It is important to mention that the authors adopted the convention of using the negative energy score for OOD detection, ensuring that \(S(x;f)\) exhibits higher values for ID data and lower values for OOD data.

GradNorm [23]computes the norm of the gradients of a neural network with respect to an input. The norm of the gradients is a measure of how sensitive the network is to the input. Inputs with high norms are more likely to be OOD because they are more likely to cause the network to make a mistake.

Exgrad[24]calculates the expected norm of the gradients of a neural network with respect to an input. The expected norm of the gradients is a measure of how sensitive the network is to the input.

DICE [47]aims at selectively utilizing a subset of significant weights to compute the output for OOD detection. By employing the technique of sparsification, the network effectively avoids incorporating irrelevant information into the output, thereby enhancing its OOD detection capabilities.

ReAct [49]This approach is based on the observation that OOD data trigger distinctive activation patterns in neural networks. ReAct selectively rectifies and truncates the activations of specific hidden units, reducing overconfident predictions on OOD data.

Variational Rectified Activation (VRA) [57]VRA leverages the variational method to find the optimal operation for maximizing the gap between ID and OOD data. It introduces suppression and amplification operations for abnormally low, high, and intermediate activations, unlike ReAct which only focuses on high activations. VRA uses piecewise functions to simulate these operations.

ASH [12]This method removes a large portion of activations and adjusts the remaining ones. The remaining activations (e.g., 10%) are either simplified or lightly adjusted. The simplified activation representation is then propagated through the rest of the network to generate scores for both classification and OOD detection. The energy score, calculated from the logits, is commonly used for OOD detection, although the softmax score can also be used.

### Software and Hardware

SoftwareWe run all experiments with Python 3.8.0 and PyTorch 1.12.1.

HardwareAll experiments are run on NVIDIA RTX 3090.