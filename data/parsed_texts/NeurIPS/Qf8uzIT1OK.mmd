# Ethical Considerations for Responsible Data Curation

 Jerone T. A. Andrews

Sony AI, Tokyo

&Dora Zhao

Sony AI, New York

&William Thong

Sony AI, Zurich

&Apostolos Modas

Sony AI, Zurich

Correspondence to jerone.andrews@sony.com. Equal contribution; authors are listed in random order.

###### Abstract

Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating HCCV evaluation datasets, addressing privacy and bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.

## 1 Introduction

Contemporary human-centric computer vision (HCCV) data curation practices, which prioritize dataset size and utility, have pushed issues related to privacy and bias to the periphery, resulting in dataset retractions and modifications [78, 126, 175, 216, 244, 320], as well as models that are unfair or rely on spurious correlations [22, 26, 112, 139, 146, 215, 272, 281]. HCCV datasets primarily rely on nonconsensual web scraping [99, 122, 124, 228, 260, 266, 310]. These datasets not only regard image subjects as free raw material [32], but also lack the ground-truth metadata required for fairness and robustness evaluations [91, 196, 216]. This makes it challenging to obtain a comprehensive understanding of model blindspots and cascading harms [30, 85] across dimensions, such as data subjects, instruments, and environments, which are known to influence performance [222]. While, for example, image subject attributes can be inferred [7, 43, 170, 188, 198, 241, 267, 290, 343, 375], this is controversial for social constructs, notably race and gender [28, 132, 179, 180]. Inference introduces further biases [19, 107, 263, 291] and can induce psychological harm when incorrect [47, 275].

Recent efforts in machine learning (ML) to address these issues often rely on post hoc reflective processes. Dataset documentation focuses on interrogating and describing datasets after data collection [5, 273, 307, 44, 94, 108, 243, 258, 273, 5]. Similarly, initiatives by NeurIPS and ICML ask authors to consider the ethical and societal implications of their research after completion [257]. Further, dataset audits [292, 247] and bias detection tools [29, 340] expose dataset management issues and representational biases without offering guidance on responsible data collection. Although there are existing proposals for artificial intelligence (AI) and data design guidelines [35, 79, 116, 160, 202, 247], as well as calls to adopt methodologies from more established fields [154, 159, 166], general-purpose guidelines lack domain specificity and task-oriented guidance [307]. For example, remedies may prioritize privacy and governance [35] but overlook data composition and image content. Other recommended practices lack persuasive justification for adoption [116, 160] or fail to provide propercontextualization for appropriate application [252; 323; 367]. For instance, the _People + AI Guidebook_[116] suggests creating dataset specifications without explaining the rationale, and privacy methodologies are advocated without cognizant of privacy and data protection laws [252; 367; 323]. These efforts, which hold significance in promoting responsible practices, would benefit from being supplemented by proactive, domain-specific recommendations aimed at tackling privacy and bias concerns starting from the inception of a dataset.

Our research directly addresses these critical concerns by examining purpose (Section 3), consent and privacy (Section 4), and diversity (Section 5). Compared to recent scholarship, we adopt an ante hoc reflective perspective, offering considerations and recommendations for curating HCCV datasets for fairness and robustness evaluations. Our work, therefore, resonates with the call for domain-specific resources to operationalize fairness [68; 150; 305]. We draw insights from current practices [42; 170; 375], guidelines [31; 222; 231], dataset withdrawals [78; 126; 216], and audits [35; 36; 247], to motivate each recommendation, focusing on HCCV evaluation datasets that present unique challenges (e.g., visual leakage of personally identifiable information) and opportunities (e.g., leveraging image metadata for analysis). To guide curators towards more ethical yet resource-intensive curation, we provide a checklist in Appendix A.3 This translates our considerations and recommendations into pre-curation questions, functioning as a catalyst for discussion and reflection.

Footnote 3: The checklist can also be found at: https://github.com/SonyResearch/responsible_data_curation.

While several of our recommendations can also be applied retroactively such measures cannot undo incurred harm, e.g., resulting from inappropriate uses, privacy violations, and unfair representation [129]. It is important to make clear that our proposals are not intended for the evaluation of HCCV systems that detect, predict, or label sensitive or objectionable attributes such as race, gender, sexual orientation, or disability.

## 2 Development Process

HCCV should adhere to the most stringent ethical standards to address privacy and bias concerns. As stated in the NeurIPS Code of Ethics [1], it is essential to abide by established institutional research protocols, ensuring the safeguarding of human subjects. These protocols, initially designed for biomedical research, have, however, been met with confusion, resulting in inconsistencies when applied in the context of data-centric research [218]. For example, HCCV research often amasses millions of "public" images without obtaining informed consent or participation, disregarding serious privacy and ethical concerns [3; 35; 245; 301; 313]. This exemption from research ethics regulation is grounded in the limited definition of human-subjects research, which categorizes extant, publicly available data as minimal risk [218; 256]. Thus, numerous ethically-dubious HCCV datasets would not fall under Institutional Review Board (IRB) oversight [247]. What's more, the NeurIPS Code of Ethics only mandates following existing protocols when research involves "direct" interaction between human participants and researchers or technical systems. Even when research is subjected to supervision, IRBs are restricted from considering broader societal consequences beyond the immediate study context [217]. Compounding matters, CV-centric conferences are still to adopt ethics review practices [306].

These limitations are concerning, especially considering the potential for predictive privacy harms when seemingly non-identifiable data is combined [35; 70; 218] or when data is used for harmful downstream applications such as predicting sexual orientation [192; 344], crime propensity [358; 365], or emotion [10; 224]. Acknowledging this, our research study employed the same principles underpinning established guidelines [24; 331] for protecting human subjects in research to identify ethical issues in HCCV dataset design, namely autonomy, justice, benefence, and non-maleficence. _Autonomy_ respects individuals' self-determination--e.g., through informed consent and assent for HCCV datasets. _Justice_ promotes the fair distribution of risks, costs, and benefits, guiding decisions on compensation, data accessibility, and diversity. _Beneference_ entails the proactive promotion of positive outcomes and well-being, e.g., by soliciting individuals' to self-identify, while _non-maleficence_ centers on minimizing harm and risks during dataset design, e.g., by redacting privacy-leaking image regions and metadata.

To ensure comprehensive consideration, we harnessed diverse expertise, following contemporary, interdisciplinary practices [261; 270; 307]. Our team comprises researchers, practitioners, and lawpers with backgrounds in ML, CV, algorithmic fairness, philosophy, and social science. With a range of ethnic, cultural, and gender backgrounds, we bring extensive experience in designing CV datasets, training models, and developing ethical guidelines. To align our expertise with the principles, we collectively discussed them, considering each author's background. After identifying key ethical issues in HCCV data curation practices, we iteratively refined them into an initial draft of ethical considerations. We extensively collected, analyzed, and discussed papers spanning a range of themes such as HCAI, HCCV datasets, data and model documentation, bias detection and mitigation, AI and data design, fairness, and critical AI. Our comprehensive literature review incorporated pertinent studies and datasets, resulting in refined considerations with detailed explanations and recommendations for responsible data curation. Additional details are provided in Appendix B.

## 3 Purpose

In ML, significant emphasis has been placed on the acquisition and utilization of "general-purpose" datasets [259]. Nevertheless, without a clearly defined task pre-data collection, it becomes challenging to effectively handle issues related to data composition, labeling, data collection methodologies, informed consent, and assessments related to data protection. This section addresses conflicting dataset motivations and provides recommendations.

### Ethical Considerations

**Fairness-unaware datasets are inadequate for measuring fairness.** Datasets lacking explicit fairness considerations are inadequate for mitigating or studying bias, as they often lack the necessary labels for assessing fairness. For instance, the COCO dataset [196], focused on scene understanding, lacks subject information, making fairness assessments challenging. Researchers, consequently, resort to human annotators to infer, e.g., subject characteristics, limiting bias measurement to visually "inferable" attributes. This introduces annotation bias [56] and the potential for harmful inferences [275; 47].

**Fairness-aware datasets are incompatible with common HCCV tasks.** Industry practitioners stress the importance of carefully designed and collected "fairness-aware" datasets to detect bias issues [150]. Fabris et al. [93] found that out of 28 CV datasets used in fairness research between 2014 and 2021, only eight were specifically created with fairness in mind. Among these, seven were HCCV datasets (scraped from the web) [342; 343; 170; 216; 308; 319; 344], including five focused on facial analysis. Due to the limited availability and delimited task focus of fairness-aware datasets, researchers repurpose "fairness-unaware" datasets [120; 139; 196; 198; 208; 346; 373]. Fairness-aware datasets fall short in addressing the original tasks associated with well-known HCCV datasets, which encompass a range of tasks, such as segmentation [64; 209], pose estimation [196; 13], localization and detection [73; 91; 110], identity verification [153], action recognition [173], as well as reconstruction, synthesis and manipulation [114; 171]. The absence of fairness-aware datasets with task-specific labels hampers the practical evaluation of HCCV systems, despite their importance in domains such as healthcare [155; 220], autonomous vehicles [163], and sports [317]. Additionally, fairness-aware datasets lack self-identified annotations from image subjects, relying on inferred attributes, e.g., from online resources [319; 308; 43].

### Practical Recommendations

**Refrain from repurposing datasets.** Existing datasets, repurposable but optimized for specific functions, can inadvertently perpetuate biases and undermine fairness [183]. Repurposing fairness-unaware data for fairness evaluations can result in _dirty data_, characterized by missing or incorrect information and distorted by individual and societal biases [181; 265]. Dirty data, including inferred data, can have significant downstream consequences, compromising the validity of research, policy, and decision-making [14; 63; 265; 341]. ML practitioners widely agree that a proactive approach to fairness is preferable, involving the direct collection of demographic information from the outset [150]. To mitigate epistemic risk, curated datasets should capture key dimensions influencing fairness and robustness evaluation of HCCV models, i.e., data subjects, instruments, and environments. Model Cards explicitly highlight the significance of these dimensions in fairness and robustness assessments [222].

**Create purpose statements.** Pre-data collection, dataset creators should establish _purpose statements_, focusing on motivation rather than cause [129]. Purpose statements address, e.g., data collection motivation, desired composition, permissible uses, and intended consumers. While dataset documentation [108; 258] covers similar questions, it is a _reflective_ process and can be manipulated to fit the narrative of the collected data, as opposed to directing the narrative of the data to be collected. Purpose statements can play a crucial role in preventing both _hindsight bias_[51; 97; 176] and _purpose creep_, ensuring alignment with stakeholders' consent and intentions [186]. To enhance transparency and accountability, as recommended by Peng et al. [247], purpose statements can undergo peer review, similar to _registered reports_[238]. Registered reports, recognized by the UK 2021 Research Excellence Framework, incentivize rigorous research practices and can lead to increased institutional funding [51].

## 4 Consent and Privacy

Informed consent is crucial in research ethics involving humans [230; 235], ensuring participant safety, protection, and research integrity [59; 253]. Shaping data collection practices in various fields [35; 235], informed consent consists of three elements: _information_ (i.e., the participant should have sufficient knowledge about the study to make their decision), _comprehension_ (i.e., the information about the study should be conveyed in an understandable manner), and _volutariness_ (i.e., consent must be given free of coercion or undue influence). While consent is not the only legal basis for data processing, it is globally preferred for its legitimacy and ability to foster trust [253; 82]. We address concerns related to consent and privacy, and provide recommendations.

### Ethical Considerations

**Human-subjects research.** As aforementioned in Section 2, HCCV datasets are frequently collected without informed consent or participation, primarily due to the classification of publicly available data as "minimal risk" within human-subjects research. However, beyond possible predictive privacy harms and unethical downstream uses, collecting data without informed consent hinders researchers and practitioners from fully understanding and addressing potential harms to data subjects [218; 333]. Some argue that consent is pivotal as it provides individuals with a last line of defense against the misuse of their personal information, particularly when it contradicts their interests or well-being [253; 254; 77; 223].

**Creative Commons loophole.** Some datasets have been created based on the misconception that the "unlocking [of] restrictive copyright" [35] through Creative Commons licenses implies data subject consent. However, the Illinois Biometric Information Privacy Act (BIPA) [161] mandates data subject consent, even for publicly available images [370]. In the UK and EU General Data Protection Regulation (GDPR) [88] Article 4(11), images containing faces are considered biometric data, requiring "freely given, specific, informed, and unambiguous" consent from data subjects for data processing. Similarly, in China, the Personal Information Protection Law (PIPL) [233] Article 29 mandates obtaining individual consent for processing sensitive personal information, including biometric data (Article 28). While a Creative Commons license may release copyright restrictions on specific artistic expressions within images [370], it does not apply to image regions containing biometric data such as faces, which are protected by privacy and data protection laws [300].

**Vulnerable persons.** Nonconsensual data collection methods can result in the inclusion of vulnerable individuals unable to consent or oppose data processing due to power imbalances, limited capacity, or increased risks of harm [89; 207]. While scraping vulnerable individuals' biometric data may be incidental, some researchers actively target them, jeopardizing their sensitive information without guardian consent [260; 128].

Paradoxically, attempts to address racial bias in data have involved soliciting homeless persons of color, further compromising their vulnerability [103]. When participation is due to economic or situational vulnerability, as opposed to one's best interests, monetary offerings may be perceived as inducement [117]. Further ethical concerns manifest when it is unclear whether participants were adequately _informed_ about a research study. For instance, in ethnicity recognition research [72], despite obtaining informed consent, criticism arose due to training a model that discriminates between Chinese Uyghur, Korean, and Tibetan faces. Although the study's focus is on the technologyitself [315], its potential use in enhancing surveillance on Chinese Uyghurs raises ethical questions due to the human rights violations against them [333].

**Consent revocation.** Dataset creators sometimes view autonomy as a challenge to collecting biometric data for HCCV, especially when data subjects prioritize privacy [214; 287; 297]. Nonetheless, informed consent emphasizes _voluntariness_, encompassing both the ability to give consent and the right to withdraw it at any time [74]. GDPR grants explicit revocation rights (Article 7) and the right to request erasure of personal data (Article 17) [350]. However, image subjects whose data is collected without consent are denied these rights. The nonconsensual FFHQ face dataset [171] offers an opt-out mechanism, but since inclusion was _involuntary_, subjects may be unaware of their inclusion, rendering the revocation option hollow. Moreover, this burdens data subjects with tracking the usage of their data in datasets, primarily accessible by approved researchers [81].

**Image- and metadata-level privacy attributes.** Researchers have focused on obfuscation techniques, e.g., blurring, inpainting, and overlaying, to reduce private information leakage of nonconsensual individuals [46; 101; 194; 195; 213; 252; 311; 323; 362; 367]. Nonetheless, face detection algorithms used in obfuscation may raise legal concerns, particularly if they involve predicting facial landmarks, potentially violating BIPA [61; 370]. BIPA focuses on collecting and using face geometry scans regardless of identification capability, while GDPR protects any identifiable person, requiring data holders to safeguard the privacy of nonconsenting individuals. Moreover, reliance on automated face detection methods raises ethical concerns, as demonstrated by the higher precision of pedestrian detection models on lighter skin types compared to darker skin types [352]. This predictive inequity leads to allocative harm, denying certain groups opportunities and resources, including the rights to safety [322] and privacy [80].

It is important to note that face obfuscation may not guarantee privacy [145; 367]. The Visual Redactions dataset [242] includes 68 image-level privacy attributes, covering biometrics, sensitive attributes, tattoos, national identifiers, signatures, and contact information. Training faceless person recognition systems using full-body cues reveals higher than chance re-identification rates for face blurring and overlaying [239], indicating that solely obfuscating face regions might be insufficient under GDPR. Furthermore, image metadata can also disclose sensitive details, e.g., date, time, and location, as well as copyright information that may include names [11; 239]. This is worrisome for users of commonly targeted platforms like Flickr, which retain metadata by default.

### Practical Recommendations

**Obtain voluntary informed consent.** Similar to recent consent-driven HCCV datasets [136; 254; 268], explicit informed consent should be obtained from each person depicted in, or otherwise identifiable, in a dataset, allowing the sharing of their facial, body, and biometric information for evaluating the fairness and robustness of HCCV technologies. Datasets collected with consent _reduce_ the risk of being fractured, however, data subjects may later revoke their consent over, e.g., privacy concerns they may not have been aware of at the time of providing consent or language nuances [65; 379]. Following GDPR (Article 7), plain language consent and notice forms are recommended to address the lack of public understanding of AI technologies [199].

When collecting images of individuals under the age of majority or those whose ability to protect themselves is significantly impaired on account of disability, illness, or otherwise, guardian consent is necessary [182]. However, relying solely on guardian consent overlooks the views and dignity of the vulnerable person [141]. To address this, in addition to guardian consent, voluntary informed _assent_ can be sought from a vulnerable person, in accordance with UNICEF's principlism-guided data collection procedures [31; 327]. When employing appropriate language and tools, assent establishes the vulnerable person understands the use of their data and willingly participates [31]. If a vulnerable person expresses dissent or unwillingness to participate, their data should not be collected, regardless of guardian wishes.

Informed by the U.S. National Bioethics Advisory Commission's _contextual vulnerability framework_[60], dataset creators should assess vulnerability on a continuous scale. That is, the circumstances of participation should be considered, which may require, e.g., a participatory design approach, assurances over compensation, supplementary educational materials, and insulation from hierarchical or authoritative systems [117].

**Adopt techniques for consent revocation.** To permit consent revocation, dataset creators should implement an appropriate mechanism. One option is _dynamic consent_, where personalized communication interfaces enable participants to engage more actively in research activities [174; 348]. This approach has been implemented successfully through online platforms, offering options for blanket consent, case-by-case selection, or opt-in depending on the data's use [174; 211; 314]. Alternatively, another recommended approach is to establish a steering board or charitable trust composed of representative dataset participants to make decisions regarding data use [255]. The feasibility of these proposals may vary based on a dataset's scale. Nonetheless, at a minimum, data subjects should be provided a simple and easily accessible method to revoke consent [136; 254; 268]. This aligns with guidance provided by the UK Information Commission's Office (ICO), emphasizing the need to provide alternatives to online-based revocation processes to accommodate varying levels of technology competency and internet access among data subjects [325].

**Collect country of residence information.** Anonymizing nonconsensual human subjects through face obfuscation, as done in datasets such as ImageNet [367], may not respect the privacy laws specific to the subjects' country of residence. To comply with relevant data protection laws, dataset curators should collect the country of residence from each data subject to determine their legal obligations, helping to ensure that data subjects' rights are protected and future legislative changes are addressed [249; 268]. For instance, GDPR Article 7(3) grants data subjects the right to withdraw consent at any time, which was not explicitly addressed in its predecessor [253].

**Redact privacy leaking image regions and metadata.** The European Data Protection Board emphasizes that anonymization of personal data must guard against re-identification risks such as singling out, linkability, and inference [76]. Re-identification remains possible even when nonconsensual subjects' faces are obfuscated, through other body parts or contextual information [242]. One solution is to redact all privacy-leaking regions related to nonconsensual subjects (including their entire bodies, clothing, and accessors) and text (excluding copyright owner information). However, anonymization approaches should be validated empirically, especially when using methods without formal privacy guarantees. Moreover, to mitigate algorithmic failures or biases, human annotators should be involved in creating region proposals, as well as verifying automatically generated proposals, for image regions with identifying or private information [367]. For nonconsensual individuals residing in certain jurisdictions (e.g., Illinois, California, Washington, Texas), automated region proposals requiring biometric identifiers should be avoided. Instead, human annotators should take the responsibility of generating these proposals.

Notwithstanding, to further protect privacy, dataset creators should take steps to ensure that image metadata does not reveal identifying information that data subjects did not consent to sharing. This may involve replacing exact geolocation data with a more general representation, such as city and country, and excluding user-contributed details from metatags containing personally identifiable information, except when this action would violate copyright. However, we do not advise blanket redaction of all metadata, as it contains valuable image capture information that can be useful for assessing model bias and robustness related to instrument factors.

## 5 Diversity

HCCV dataset creators widely acknowledge the significance of dataset diversity [173; 171; 170; 171; 173; 196; 283; 361; 368], realism [173; 174; 175; 164; 368; 13, 64], and difficulty [173; 176; 170; 173; 178; 91; 179; 180; 361; 368] to enhance fairness and robustness in real-world applications. Previous research has emphasized diversity across image subjects, environments, and instruments [43; 139; 222; 287], but there are many ethical complexities involved in specifying diversity criteria [14; 15]. This section examines taxonomy challenges and offers recommendations.

### Ethical Considerations

**Representational and historical biases.** The Council of Europe have expressed concerns about the threat posed by AI systems to equality and non-discrimination principles [67]. Many dataset creators often prioritize protected attributes, i.e., gender, race, and age, as key factors of dataset diversity [287]. Nevertheless, most HCCV datasets exhibit historical and representational biases [35; 166; 172; 312; 366]. These biases can be pernicious, particularly when models learn and _amplify_ them. For instance, image captioning models may rely on contextual cues related to activities like shopping [377]and laundry [376] to generate gendered captions. Spurious correlations are detrimental, as they are not causally related and perpetuate harmful associations [112, 281]. In addition, prominent examples in HCCV research demonstrate disparate algorithmic performance based on race and skin color [42, 43, 54, 123, 142, 144, 148, 250, 271, 299, 318, 334, 375]. Most recently, autonomous robots have displayed racist, sexist, and physionomic stereotypes [158]. Furthermore, face detection models have shown lower accuracy when processing images of older individuals compared to younger individuals [369]. While not endorsing these applications, discrepancies have also been observed in facial emotion recognition services for children in both commercial and research systems [152, 363], as well as age estimation [58, 115, 200].

Despite concerns regarding privacy, liability, and public relations, the collection of special and sensitive category data is crucial for bias assessments [15]. GDPR guidance from the UK ICO confirms that sensitive attributes can be collected for fairness purposes [324]. However, obtaining this information presents challenges, such as historical mistrust in clinical research among African-Americans [92, 191] or the social stigma of being photographed that some women face [166]. Nonetheless, marginalized communities may require explicit explanations and assurances about data usage to address concerns related to service provision, security, allocation, and representation [359]. This is particularly important as remaining _unseen_ does not protect against being _mis-seen_[359].

**The digital divide and accessibility.** Healthcare datasets often lack representation of minority populations, compromising the reliability of automated decisions [356]. The World Health Organization (WHO) emphasizes the need for data accuracy, completeness, and diversity, particularly regarding age, in order to address ageism in AI [355]. ML systems may prioritize younger populations for resource allocation, assuming they would benefit the most in terms of life expectancy [355]. The digital divide further exacerbates the underrepresentation of vulnerable groups, including older generations, low-income school-aged children, and children in East Asia and the South Pacific who lack access to digital technology [326, 354]. Insufficient access to digital technology hampers the representation of vulnerable persons in datasets [290], leading to _outcome homogenization_--i.e., the systematic failing of the same individuals or groups [39].

**Confused taxonomies.** Sex and gender are often used interchangeably, treating gender as a consequence of one's assigned sex at birth [95]. However, this approach erases intersex individuals who possess non-binary physiological sex characteristics [95]. Treating sex and gender as interchangeable perpetuates normative views by casting gender as binary, immutable, and solely based on biological sex [179]. This perspective disregards transgender and gender nonconforming individuals. Moreover, sex, like gender, is a social construct, as sexed bodies do not exist outside of their _social context_[45].

Similar to sex and gender, race and ethnicity are often used synonymously [332]. Nations employ diverse census questions to ascertain ethnic group composition, encompassing factors such as nationality, race, color, language, religion, customs, and tribe [328]. However, these categories and their definitions lack consistency over time and geography, often influenced by political agendas and socio-cultural shifts [286]. This variability makes it challenging to collect globally representative and meaningful data on ethnic groups. Consequently, several HCCV datasets have incorporated inconsistent and arbitrary racial categorization systems [7, 267, 343, 374]. For instance, the FairFace dataset [170] creators reference the US Census Bureau's racial categories without considering the social definition of race they represent [240]. The US Census Bureau explicitly states that their categories reflect a social definition rather than a biological, anthropological, or genetic one. Consequently, labeling the "physical race" of image subjects based on nonphysiological categories is contradictory. Furthermore, the FairFace creators do not disclose the demographics or cultural compatibility of their annotators.

**Own-anchor bias.** HCCV approaches for encoding age in datasets vary, using either integer labels [53, 102, 125, 226, 236, 264, 277, 278, 374] or group labels [84, 105, 193, 302]. Age groupings are often preferred when collecting unconstrained images from the web, as human annotators must infer subjects' ages, which is challenging [48]. This is evident in crowdsourced annotations, where 40.2% of individuals in the OpenImages MIAP dataset [290] could not be categorized into an age group. Factors unrelated to age, such as facial expression [106, 237, 345] and makeup [83, 237, 313], influence age perception. Furthermore, annotators have exhibited lower accuracy when labeling people outside of their own demographic group [8, 9, 113, 279, 303, 335, 339].

**Post hoc rationalization of the use of physiological markers.** Gender information about data subjects is obtained through inference [53, 170, 187, 198, 236, 264, 277, 278, 290, 343, 374, 374]or self-identification [136; 190; 203; 204; 375]. Inference raises concerns as it assumes that gender can be determined solely from imagery without consent or consultation with the subject, which is noninclusive and harmful [87; 127; 179]. Even when combined with non-image-based information, inferred gender fails to account for the fluidity of identity, potentially mislabeling subjects at the time of image capture [277; 278]. Moreover, physical traits are just one of many dimensions, including posture, clothing, and vocal cues, used to infer not only gender but also race [177; 100; 179].

**Erasure of nonstereotypical individuals.** HCCV datasets frequently adopt a US-based racial schema [170; 190; 203; 204; 343], which may oversimplify and essentialize groups [316]. This approach may not align with other more nuanced models, e.g., the continuum-based color system used in Brazil, which considers a wide range of physical characteristics. Nonconsensual image datasets rely on annotators to assign semantic categories, perpetuating stereotypes and disseminating them beyond their cultural context [180]. Notably, images without label consensus are often discarded [170; 267; 343], potentially excluding individuals who defy stereotypes, such as multi-ethnic individuals [276].

**Phenotypic attributes.** Protected attributes may not be the most appropriate criteria for evaluating HCCV models [43]. Social constructs like race and gender lack clear delineations for subgroup membership based on visible or invisible characteristics. These labels capture invisible aspects of identity that are not solely determined by visible appearance. Moreover, the phenotypic characteristics within and across subgroups exhibit significant variability [25; 48; 96; 138; 180; 347].

**Environment and instrument.** The image capture device and environmental conditions significantly influence model performance, and their impact should be considered [222]. Factors such as camera software, hardware, and environmental conditions affect HCCV model robustness in various settings [4; 197; 221; 229; 353; 360; 364; 371]. Understanding performance differences is crucial from ethical and scientific perspectives. For example, sensitivity to illumination or white balance may be linked to sensitive attributes, e.g., skin tone [62; 184; 185; 378], while available instruments or environmental co-occurrences may correlate with demographic attributes [139; 295; 376].

**Annotator positionality.** Psychological research highlights the influence of annotators' sociocultural background on their visual perception [19; 107; 263; 275; 291]. However, recent empirical studies have evidenced a lack of regard for the impact an annotator's social identity has on data [79; 111]. Only a handful of HCCV datasets provide annotator demographic details [12; 56; 287; 375].

**Recruitment and compensation.** Data collected without consent pactently lacks compensation. Balancing between excessive and deficient payment is crucial to avoid coercion and exploitation [231; 268]. An additional concern is the employment of remote workers from disadvantaged regions [248], often with low wages and fast-paced work conditions [71; 135; 162; 206]. This can lead to arbitrary denial of payment based on opaque quality criteria [98] and prevents union formation [206], creating a sense of invisibility and uncertainty for workers [321].

### Practical Recommendations

**Obtain self-reported annotations.** Practitioners are cautious about inferring labels about people to avoid biases [15]. Moreover, data access request rights, e.g., as offered by GDPR, California Consumer Privacy Act, and PIPL, may require data holders to disclose inferred information. To avoid stereotypical annotations and minimize harm from misclassification [275], labels should be collected directly from image subjects, who inherently possess contextual knowledge of their environment and awareness of their own attributes.

**Provide open-ended response options.** Closed-ended questions, such as those on census forms, may lead to incongruuous responses and inadequate options for self-identification [156; 179; 274]. Open-ended questions provide more accurate answers but can be taxing, require extensive coding, and are harder to analyze [109; 178; 298; 40]. To balance this, closed-ended questions should be augmented with an open-ended response option, avoiding the term "other", which implies _othering_ norms [285]. This gives subjects a _voice_[234; 296] and allows for future question design improvement.

**Acknowledge the mutability and multiplicity of identity.**_Identity shift_--the intentional self-transformation in mediated contexts [49]--is often overlooked. To address this, we propose collecting self-identified information on a per-image basis, acknowledging that identity is temporal and nonstatic. Specifically, for sensitive attributes, allowing the selection of multiple identity categories without limitations is preferable [304; 309]. This prevents oversimplification and marginalization. While we acknowledge the potential burden of self-identification on fluid and dynamic identities, an image captures a single moment. Thus, evolving identity may not require metadata updates; however, we recommend providing subjects with mechanisms for updates when needed.

**Collect age, pronouns, and ancestry.** First, to capture accurate age information, dataset curators should collect the exact biological age in years from image subjects, corresponding to their age at the time of image capture. This approach offers flexibility, insofar as permitting the appropriate aggregation of the collected data. This is particularly important given the lack of consistent age groupings in the literature.

Second, dataset curators should consider opting to collect self-identified pronouns. This promotes mutual respect and common courtesy, reducing the likelihood of causing harm through misgendering [157]. Self-identified pronouns are particularly important for sexual and gender minority communities as they "convey and affirm gender identity" [232]. Significantly, pronoun use is increasingly prevalent in social media platforms [86, 165, 167], workplaces [55], and education settings [20, 212], fostering gender inclusivity [21]. However, subjects should always have the option of not disclosing this information.

Finally, to address issues with ethnic and racial classification systems [180, 286], dataset creators should consider collecting ancestry information instead. Ancestry is defined by historically shaped borders and has been shown to offer a more stable and less confusing concept [17]. The United Nations' M49 geoscheme can be used to operationalize ancestry [329], where subjects select regions that best describe their ancestry. To situate responses, subjects could be asked, e.g., "Where do your ancestors (e.g., great-grandparents) come from?". This avoids reliance on proxies, e.g., skin tone, that risk normalizing their inadequacies without reflecting their limitations [15].

**Collect aggregate data for commonly ignored groups.** Additional sensitive attributes should also be collected, such as disability and pregnancy status, when voluntarily disclosed by subjects. These attributes should be reported in aggregate data to reduce the safety concerns of subjects [309, 351]. Given that definitions of these attributes may be inconsistent and tied to culture, identity, and histories of oppression [37, 41], navigating tensions between benefits and risks is necessary. Despite potential reluctance, sourcing data from underrepresented communities contributes to dataset inclusivity [37, 168]. Regarding disability, the American Community Survey [330] covers categories related to hearing, vision, cognitive, ambulatory, self-care, and independent living difficulties.

**Collect phenotypic and neutral performance features.** Collecting phenotypic characteristics can serve as _objective_ measures of diversity, i.e., attributes which, in evolutionary terms, contribute to individual-level recognition [57], e.g., skin color, eye color, hair type, hair color, height, and weight [19]. These attributes have enabled finer-grained analysis of model performance and biases [43, 75, 294, 318, 349, 372]. Additionally, considering a multiplicity of _neutral performative features_, e.g., facial hair, hairstyle, cosmetics, clothing, and accessories, is important to surface the perpetuation of social stereotypes and spurious relationships in trained models [6, 18, 166, 284, 340].

**Record environment and instrument information.** Data should capture variations in environmental conditions and imaging devices, including factors such as image capture time, season, weather, ambient lighting, scene, geography, camera position, distance, lens, sensor, stabilization, use of flash, and post-processing software. Instrument-related factors may be easily captured, by restricting data collection to images with exchangeable image file format (Exif) metadata. The remaining factors, e.g., season and weather can be self-reported or coarsely estimated utilizing information such as image capture time and location.

**Recontextualize annotators as contributors.** Dataset creators should document the identities of annotators and their contributions to the dataset [12, 79], rather than treating them as anonymous entities responsible for data labeling alone [52, 206]. While many datasets [78, 137, 196] neglect to report annotator demographics, assuming objectivity in annotation for visual categories is flawed [23, 169, 219]. Furthermore, using majority voting to reach the assumed ground truth, disregards minority opinions, treating them as noise [169]. Annotator characteristics, including pronouns, age, and ancestry, should be recorded and reported to quantify and address annotator perspectives and bias in datasets [12, 118]. Additionally, allowing annotators freedom in labeling helps to avoid replicating socially dominant viewpoints [219].

**Fair treatment and compensation for contributors.** In accordance with Australia's National Health and Medical Research Council [231] and the WHO [66], dataset contributors should not only be guaranteed compensation above the minimum hourly wage of their country of residence [280], but also according to the complexity of tasks to be performed. However, alternative payment models, for example, based on the average hourly wage, may offer benefits in terms of promoting diversity by increasing the likelihood of higher socio-economic status contributors [251].

Besides payment, the implementation of direct communication channels and feedback mechanisms, such as anonymized feedback forms [246], can help to address issues faced by annotators while providing a level of protection from retribution. Furthermore, the creation of plain language guides can ease task completion and reduce quality control overheads. Ideally, recruitment and compensation processes should be well-documented and undergo ethics review, which can help to further reduce the number of "glaring ethical lapses" [293].

## 6 Discussion and Conclusion

Supplementary to established ethical review protocols, we have provided proactive, domain-specific recommendations for curating HCCV evaluation datasets for fairness and robustness evaluations. However, encouraging change in ethical practice could encounter resistance or slow adoption due to established norms [218], inertia [33], diffusion of responsibility [151], and liability concerns [15]. To garner greater acceptance, platforms such as NeurIPS could adopt a model similar to the registered reports format, embraced by over 300 journals [247, 51]. This entails pre-acceptance of dataset proposals before curation, alleviating financial uncertainties associated with more ethical practices.

Nevertheless, seeking consent from all depicted individuals might give rise to logistical challenges. Resource requirements tied to the implementation and maintenance of consent management systems could emerge, potentially necessitating significant investment in technical infrastructure and dedicated personnel. Particularly for smaller organizations and academic research groups, these limitations could present considerable hurdles. A potential solution is forming _data consortia_[121, 166], which helps address operational challenges by pooling resources and knowledge.

Extending our recommendations to the curation of "democratizing" foundation model-sized training datasets [288, 104, 119, 289] poses an economic challenge. To put this into perspective, the GeoDE dataset of 62K crowdsourced object-centric images [262], without personally identifiable information, incurred a cost of $1.08 per image. While our recommendations may not seamlessly _scale_ to the curation of fairness-aware, billion-sized image datasets, it is worth considering that "solutions which resist scale thinking are necessary to undo the social structures which lie at the heart of social inequality" [130]. Large-scale, nonconsensual datasets driven by scale thinking have included harmful and distressing content, including rape [35, 36], racist stereotypes [131], vulnerable persons [128], and derogatory taxonomies [183, 69, 129, 34]. Such content may further generate legal concerns [2]. We contend that these issues can be mitigated through the implementation of our recommendations.

Nonetheless, balancing resources between model development and data curation is value-laden, shaped by "social, political, and ethical values" [38]. While organizations readily invest significantly in model training [227, 336], compensation for data contributors often appears neglected [337, 338], disregarding that "most data represent or impact people" [380]. Remedial actions could be envisioned to bridge the gap between models developed with ethically curated data and those benefiting from expansive, nonconsensually crawled data. Reallocating research funds away from dominant data-hungry methods [38] would help to strike a balance between technological advancement and ethical imperatives.

However, the granularity and comprehensiveness of our diversity recommendations could be adapted beyond evaluation contexts, particularly when employing "fairness without demographics" [134, 189, 210, 50] training approaches, reducing financial costs. Nevertheless, the applicability of any proposed recommendation is intrinsically linked to the specific context [243]. Decisions should be guided by the social framework of a given application to ensure ethical and equitable data curation.

Just as the concepts of identity evolve, our recommendations must also evolve to ensure their ongoing relevance and sensitivity. Thus, we encourage dataset creators to tailor our recommendations to their _context_, fostering further discussions on responsible data curation.

## Acknowledgments and Disclosure of Funding

This work was funded by Sony Research.

## References

* [1] Announcing the NeurIPS Code of Ethics x2013; NeurIPS Blog -- blog.neurips.cc. https://blog.neurips.cc/2023/04/20/announcing-the-neurips-code-of-ethics/. [Accessed August 14, 2023].
* [2] OpenAI and Microsoft Sued for $3 Billion Over Alleged ChatGPT 'Privacy Violations' -- vice.com. https://www.vice.com/en/article/wxjxgx/openai-and-microsoft-sued-for-dollar3-billion-over-alleged-chatgpt-privacy-violations. [Accessed August 14, 2023].
* [3] Shared, but not up for grabs. _Nature Machine Intelligence_, 1(4):163-163, April 2019. doi: 10.1038/s42256-019-0047-y.
* [4] Mahmoud Afifi and Michael S. Brown. What else can fool deep learning? addressing color constancy errors on deep neural network performance. In _IEEE/CVF International Conference on Computer Vision (ICCV)_, October 2019.
* [5] Shazia Afzal, C Rajmohan, Manish Kesarwani, Sameep Mehta, and Hima Patel. Data readiness report. In _2021 IEEE International Conference on Smart Data Services (SMDS)_, pages 42-51. IEEE, 2021.
* [6] Vitor Albiero, Kai Zhang, Michael C King, and Kevin W Bowyer. Gendered differences in face recognition accuracy explained by hairstyles, makeup, and facial morphology. _IEEE Transactions on Information Forensics and Security_, 17:127-137, 2021.
* [7] Mohsan Alvi, Andrew Zisserman, and Christoffer Nellaker. Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings. In _European Conference on Computer Vision Workshops (ECCVW)_, pages 0-0, 2018.
* [8] Jeffrey S Anastasi and Matthew G Rhodes. An own-age bias in face recognition for children and older adults. _Psychonomic bulletin & review_, 12(6):1043-1047, 2005.
* [9] Jeffrey S Anastasi and Matthew G Rhodes. Evidence for an own-age bias in face recognition. _North American Journal of Psychology_, 8(2), 2006.
* [10] Nazanin Andalibi and Justin Buss. The human in emotion recognition on social media: Attitudes, outcomes, risks. In _Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems_, pages 1-16, 2020.
* [11] Jerone T. A. Andrews. The hidden fingerprint inside your photos. https://www.bbc.com/future/article/20210324-the-hidden-fingerprint-inside-your-photos, 2021. [Accessed June 30, 2022].
* [12] Jerone T A Andrews, Przemyslaw Joniak, and Alice Xiang. A view from somewhere: Human-centric face representations. In _International Conference on Learning Representations (ICLR)_, 2023.
* [13] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3686-3693, 2014.
* [14] McKane Andrus, Elena Spitzer, and Alice Xiang. Working to address algorithmic bias? don't overlook the role of demographic data. _Partnership on AI_, 2020.
* [15] McKane Andrus, Elena Spitzer, Jeffrey Brown, and Alice Xiang. What we can't measure, we can't understand: Challenges to demographic data procurement in the pursuit of fairness. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 249-260, 2021.

* [16] Anelia Angelova, Yaser Abu-Mostafam, and Pietro Perona. Pruning training sets for learning of object categories. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 494-501, 2005.
* [17] Peter J Aspinall. Operationalising the collection of ethnicity data in studies of the sociology of health and illness. _Sociology of health & illness_, 23(6):829-862, 2001.
* [18] Guha Balakrishnan, Yuanjun Xiong, Wei Xia, and Pietro Perona. Towards causal benchmarking of bias in face analysis algorithms. In _Deep Learning-Based Face Analytics_, pages 327-359. Springer, 2021.
* [19] P. Balaresque and T.E. King. Human phenotypic diversity. In _Genes and Evolution_, pages 349-390. Elsevier, 2016. doi: 10.1016/bs.ctdb.2016.02.001.
* [20] Rich Barlow and Sydney Scott. Students can adjust their pronouns and gender identity in bu's updated data system. https://www.bu.edu/articles/2022/pronouns-and-gender-identities-in-updated-data-system/, November 2022.
* [21] Dennis Baron. _What's Your Pronoun?: Beyond He and She_. Liveright Publishing, 2020.
* [22] Alistair Barr. Google mistakenly tags Black people as 'gorillas,' showing limits of algorithms. _The Wall Street Journal_, 2015.
* [23] Teanna Barrett, Quan Ze Chen, and Amy X Zhang. Skin deep: Investigating subjectivity in skin tone annotations for computer vision benchmark datasets. _arXiv preprint arXiv:2305.09072_, 2023.
* [24] Tom Beauchamp and James Childress. Principles of biomedical ethics: marking its fortieth anniversary, 2019.
* [25] Fabiola Becerra-Riera, Annette Morales-Gonzalez, and Heydi Mendez-Vazquez. A survey on facial soft biometrics for video surveillance and forensic applications. _Artificial Intelligence Review_, 52(2):1155-1187, 2019.
* [26] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In _European Conference on Computer Vision (ECCV)_, pages 456-473, 2018.
* [27] Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. _Transactions of the Association for Computational Linguistics_, 6:587-604, December 2018. doi: 10.1162/tacl_a_00041.
* [28] Sebastian Benthall and Bruce D Haynes. Racial categories in machine learning. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 289-298, 2019.
* [29] Elena Beretta, Antonio Vetro, Bruno Lepri, and Juan Carlos De Martin. Detecting discriminatory risk through data annotation based on bayesian inferences. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 794-804, 2021.
* [30] A Stevie Bergman, Lisa Anne Hendricks, Maribeth Rauh, Boxi Wu, William Agnew, Markus Kunesch, Isabella Duan, Iason Gabriel, and William Isaac. Representation in ai evaluations. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 519-533, 2023.
* [31] Gabrielle Berman and Kerry Albright. Children and the data cycle: Rights and ethics in a big data world. _arXiv preprint arXiv:1710.06881_, 2017.
* [32] Abeba Birhane. Algorithmic colonization of africa. _SCRIPTed_, 17:389, 2020.
* [33] Abeba Birhane. Automating ambiguity: Challenges and pitfalls of artificial intelligence. _arXiv preprint arXiv:2206.04179_, 2022.
* [34] Abeba Birhane and Vinay Uday Prabhu. Large image datasets: A pyrrhic win for computer vision? In _IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 1536-1546, 2021.

* Birhane and Prabhu [2021] Abeba Birhane and Vinay Uday Prabhu. Large image datasets: A pyrrhic win for computer vision? In _IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 1536-1546, 2021.
* Birhane et al. [2021] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes. _arXiv preprint arXiv:2110.01963_, 2021.
* Blaser and Ladner [2020] Brianna Blaser and Richard E Ladner. Why is data on disability so hard to collect and understand? In _2020 Research on Equity and Sustained Participation in Engineering, Computing, and Technology (RESPECT)_, volume 1, pages 1-8. IEEE, 2020.
* Blili-Hamelin and Hancox-Li [2023] Borhane Blili-Hamelin and Leif Hancox-Li. Making intelligence: Ethical values in iq and ml benchmarks. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 271-284, 2023.
* Bommasani et al. [2022] Rishi Bommasani, Kathleen A Creel, Ananya Kumar, Dan Jurafsky, and Percy S Liang. Picking on the same person: Does algorithmic monoculture lead to outcome homogenization? _Advances in Neural Information Processing Systems (NeurIPS)_, 35:3663-3678, 2022.
* Bradburn [1997] N Bradburn. Respondent burden: health survey research methods. In _Second Biennial Conference, Williamsburg, VA. Washington, DC: US Government Printing Office_, 1997.
* Bragg et al. [2021] Danielle Bragg, Naomi Caselli, Julie A Hochgesang, Matt Huenerfauth, Leah Katz-Hernandez, Oscar Koller, Raja Kushalnagar, Christian Vogler, and Richard E Ladner. The fate landscape of sign language ai datasets: An interdisciplinary perspective. _ACM Transactions on Accessible Computing (TACCESS)_, 14(2):1-45, 2021.
* Buolamwini [2022] Joy Buolamwini. Gender shades. https://www.media.mit.edu/projects/gender-shades/overview/, n.d. [Accessed June 1, 2022].
* Buolamwini and Gebru [2018] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 77-91. PMLR, 2018.
* Butcher et al. [2021] Bradley Butcher, Vincent S Huang, Christopher Robinson, Jeremy Reffin, Sema K Sgaier, Grace Charles, and Novi Quadianto. Causal datasheet for datasets: An evaluation guide for real-world data analysis and data collection design using bayesian networks. _Frontiers in Artificial Intelligence_, 4:612551, 2021.
* Butler [1988] Judith Butler. Performative acts and gender constitution: An essay in phenomenology and feminist theory. _Theatre Journal_, 40(4):519-531, 1988. ISSN 01922882, 1086332X.
* Caesar et al. [2020] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11621-11631, 2020.
* Campbell and Troyer [2007] Mary E Campbell and Lisa Troyer. The implications of racial misclassification by observers. _American Sociological Review_, 72(5):750-765, 2007.
* Carcagni et al. [2015] Pierluigi Carcagni, Marco Del Coco, Dario Cazzato, Marco Leo, and Cosimo Distante. A study on different experimental configurations for age, race, and gender estimation problems. _EURASIP Journal on Image and Video Processing_, 2015(1):1-22, 2015.
* Carr et al. [2021] Caleb T Carr, Yeweon Kim, Jacob J Valov, Judith E Rosenbaum, Benjamin K Johnson, Jeffrey T Hancock, and Amy L Gonzales. An explication of identity shift theory. _Journal of Media Psychology_, 2021.
* Chai et al. [2022] Junyi Chai, Taeuk Jang, and Xiaoqian Wang. Fairness without demographics through knowledge distillation. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:19152-19164, 2022.
* Chambers and Tzavella [2022] Christopher D Chambers and Loukia Tzavella. The past, present and future of registered reports. _Nature human behaviour_, 6(1):29-42, 2022.

* [52] Stevie Chancellor, Eric PS Baumer, and Munmun De Choudhury. Who is the" human" in human-centered machine learning: The case of predicting mental health from social media. _Proceedings of the ACM on Human-Computer Interaction_, 3(CSCW):1-32, 2019.
* [53] Bor-Chun Chen, Chu-Song Chen, and Winston H Hsu. Cross-age reference coding for age-invariant face recognition and retrieval. In _European Conference on Computer Vision (ECCV)_, pages 768-783. Springer, 2014.
* [54] Brian X Chen. Hp investigates claims of 'racist' computers. https://www.wired.com/2009/12/hp-notebooks-racist/, December 2009.
* [55] Te-Ping Chen. Why gender pronouns are becoming a big deal at work. _The Wall Street Journal. Retrieved October_, 15:2022, 2021.
* [56] Yunliang Chen and Jungseock Joo. Understanding and mitigating annotation bias in facial expression recognition. In _IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 14980-14991, 2021.
* [57] Nicholas A Christakis and James H Fowler. Friendship and natural selection. _Proceedings of the National Academy of Sciences_, 111(supplement_3):10796-10801, 2014.
* [58] Albert Clapes, Ozan Bilici, Daria Temirova, Egils Avots, Gholamreza Anbarjafari, and Sergio Escalera. From apparent to real age: gender, age, ethnic, makeup, and expression bias analysis in real age estimation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 2373-2382, 2018.
* [59] Nuremberg Code. The nuremberg code. _Trials of war criminals before the Nuremberg military tribanals under control council law_, 10(2):181-182, 1949.
* [60] National Bioethics Advisory Commission et al. Ethical and policy issues in research involving human participants. 2001.
* [61] Complaint, Vance v. IBM. U.s. dist. lexis 168610, 2020 wl 5530134 (united states district court for the northern district of illinois, eastern division, january 14, 2020, filed), 2020.
* [62] Cynthia M. Cook, John J. Howard, Yevgeniy B. Sirotin, Jerry L. Tipton, and Arun R. Vemury. Demographic effects in facial recognition and their dependence on image acquisition: An evaluation of eleven commercial systems. _IEEE Transactions on Biometrics, Behavior, and Identity Science_, 1(1):32-41, January 2019. doi: 10.1109/biom.2019.2897801.
* [63] A Feder Cooper, Ellen Abrams, and Na Na. Emergent unfairness in algorithmic fairness-accuracy trade-off research. In _Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society (AIES)_, pages 46-54, 2021.
* [64] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3213-3223, 2016.
* [65] Oonagh Corrigan. Empty ethics: the problem with informed consent. _Sociology of Health & Illness_, 25(7):768-792, 2003.
* [66] Council for International Organizations of Medical Sciences and others. International ethical guidelines for health-related research involving humans. _International ethical guidelines for health-related research involving humans._, 2017.
* [67] Council of Europe. Inclusion and anti-discrimination: Ai & discrimination. https://www.coe.int/en/web/inclusion-and-antidiscrimination/ai-and-discrimination, n.d. [Accessed November 24, 2022].
* [68] Kate Crawford. Artificial intelligence with very real biases. https://www.wsj.com/articles/artificial-intelligencewith-very-real-biases-1508252717, 2017. [Accessed May 15, 2022].

* [69] Kate Crawford and Trevor Paglen. Excavating ai: The politics of images in machine learning training sets. _Ai & Society_, 36(4):1105-1116, 2021.
* [70] Kate Crawford and Jason Schultz. Big data and due process: Toward a framework to redress predictive privacy harms. _BCL Rev._, 55:93, 2014.
* [71] Nicolas Croce and Moh Musa. The new assembly lines: Why ai needs low-skilled workers too. https://www.weforum.org/agenda/2019/08/ai-low-skilled-workers/, August 2019.
* [72] Wang Cunrui, Q Zhang, W Liu, Y Liu, and L Miao. Facial feature discovery for ethnicity recognition. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 9(2):e1278, 2019.
* [73] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, volume 1, pages 886-893. Ieee, 2005.
* [74] Fida K Dankar, Marton Gergely, and Samar K Dankar. Informed consent in biomedical research. _Computational and structural biotechnology journal_, 17:463-474, 2019.
* [75] Saloni Dash, Vineeth N Balasubramanian, and Amit Sharma. Evaluating and mitigating bias in image classifiers: A causal perspective using counterfactuals. In _IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 915-924, 2022.
* [76] Data Protection Commission. https://www.dataprotection.ie/en/dpc-guidance/anonymisation-and-pseudonymisation, June 2019. [Accessed August 1, 2022].
* [77] Paul De Hert and Vagelis Papakonstantinou. The new general data protection regulation: Still a sound system for the protection of individuals? _Computer law & security review_, 32(2):179-194, 2016.
* [78] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 248-255, 2009.
* [79] Emily Denton, Mark Diaz, Ian Kivlichan, Vinodkumar Prabhakaran, and Rachel Rosen. Whose ground truth? accounting for individual and collective identities underlying dataset annotation. _arXiv preprint arXiv:2112.04554_, 2021.
* [80] Oliver Diggelmann and Maria Nicole Cleis. How the right to privacy became a human right. _Human Rights Law Review_, 14(3):441-458, 2014.
* [81] Chris Dulhanty. Issues in computer vision data collection: Bias, consent, and label taxonomy. Master's thesis, University of Waterloo, 2020.
* [82] Lilian Edwards. Privacy, security and data protection in smart cities: A critical eu law perspective. _Eur. Data Prot. L. Rev._, 2:28, 2016.
* [83] Vincent Egan and Giray Cordan. Barely legal: Is attraction and estimated age of young female faces disrupted by alcohol use, make up, and the sex of the observer? _British Journal of Psychology_, 100(2):415-427, 2009.
* [84] Eran Eidinger, Roee Enbar, and Tal Hassner. Age and gender estimation of unfiltered faces. _IEEE Transactions on information forensics and security_, 9(12):2170-2179, 2014.
* [85] Madeleine Clare Elish. Moral crumple zones: Cautionary tales in human-robot interaction (pre-print). _Engaging Science, Technology, and Society (pre-print)_, 2019.
* [86] Sonia Elks. Why twitter and instagram are inviting people to share their pronouns. https://www.context.news/big-tech/why-twitter-and-instagram-are-inviting-people-to-share-pronouns, October 2021.

* [87] Severin Engelmann, Chiara Ullstein, Orestis Papakyriakopoulos, and Jens Grossklags. What people think ai should infer from faces. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 128-141, 2022.
* [88] European Commission. General data protection regulation. https://gdpr-info.eu/, 2016. [Accessed August 1, 2022].
* [89] European Data Protection Board (Article 29 Working Party). The working party on the protection of individuals with regard to the processing of personal data. https://ec.europa.eu/newsroom/document.cfm?doc_id=44137, 2017. [Accessed August 1, 2022].
* [90] European Union High-level Expert Group. Ethics guidelines for trustworthy ai: Building trust in human-centric ai. https://ec.europa.eu/futurium/en/ai-alliance-consultation/guidelines.1.html, 2019. [Accessed August 7, 2023].
* [91] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. _International journal of computer vision_, 88(2):303-338, 2010.
* [92] Nir Eyal. Using informed consent to save trust. _Journal of medical ethics_, 40(7):437-444, 2014.
* [93] Alessandro Fabris, Stefano Messina, Gianmaria Silvello, and Gian Antonio Susto. Algorithmic fairness datasets: the story so far. _Data Mining and Knowledge Discovery_, 36(6):2074-2152, 2022.
* [94] Alessandro Fabris, Stefano Messina, Gianmaria Silvello, and Gian Antonio Susto. Tackling documentation debt: a survey on algorithmic fairness datasets. In _Equity and Access in Algorithms, Mechanisms, and Optimization_, pages 1-13. 2022.
* [95] Anne Fausto-Sterling. _Sexing the body: Gender politics and the construction of sexuality_. Basic books, 2000.
* [96] Cynthia Feliciano. Shades of race: How phenotype and observer characteristics shape racial classification. _American Behavioral Scientist_, 60(4):390-419, 2016.
* [97] Klaus Fiedler and Norbert Schwarz. Questionable research practices revisited. _Social Psychological and Personality Science_, 7(1):45-52, 2016.
* [98] Christian Fieseler, Eliane Bucher, and Christian Pieter Hoffmann. Unfairness by design? the perceived fairness of digital labor on crowdworking platforms. _Journal of Business Ethics_, 156:987-1005, 2019.
* [99] Andrew P Founds, Nick Orlans, Whiddon Genevieve, and Craig I Watson. Nist special database 32-multiple encounter dataset ii (meds-ii). 2011.
* [100] Jonathan B Freeman, Andrew M Penner, Aliya Saperstein, Matthias Scheutz, and Nalini Ambady. Looking the part: Social status cues shape race perception. _PloS one_, 6(9):e25107, 2011.
* [101] Andrea Frome, German Cheung, Ahmad Abdulkader, Marco Zennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam, Hartmut Neven, and Luc Vincent. Large-scale privacy protection in google street view. In _IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 2373-2380, 2009.
* [102] Yun Fu, Ye Xu, and Thomas S Huang. Estimating human age by manifold analysis of face pictures and regression on aging features. In _2007 IEEE International Conference on Multimedia and Expo_, pages 1383-1386. IEEE, 2007.
* [103] Sidney Fussell. How an attempt at correcting bias in tech goes wrong. https://www.theatlantic.com/technology/archive/2019/10/google-allegedly-used-homeless-train-pixel-phone/599668/, 2019. [Accessed June 30, 2022].

* [104] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _arXiv preprint arXiv:2304.14108_, 2023.
* [105] Andrew C Gallagher and Tsuhan Chen. Understanding images of groups of people. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 256-263, 2009.
* [106] Tzvi Ganel. Smiling makes you look older. _Psychonomic bulletin & review_, 22(6):1671-1677, 2015.
* [107] Denia Garcia and Maria Abascal. Colored perceptions: Racially distinctive names and assessments of skin color. _American Behavioral Scientist_, 60(4):420-441, 2016.
* [108] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume III, and Kate Crawford. Datasheets for datasets. 2018.
* [109] John G Geer. Do open-ended questions measure "salient" issues? _Public Opinion Quarterly_, 55(3):360-370, 1991.
* [110] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3354-3361, 2012.
* [111] R Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah Tang, and Jenny Huang. Garbage in, garbage out? do machine learning application papers in social computing report where human-labeled training data comes from? In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 325-336, 2020.
* [112] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2(11):665-673, 2020.
* [113] Patricia A George and Graham J Hole. Factors influencing the accuracy of age estimates of unfamiliar faces. _Perception_, 24(9):1059-1073, 1995.
* [114] Athinodoros S. Georghiades, Peter N. Belhumeur, and David J. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 23(6):643-660, 2001.
* [115] Markos Georgopoulos, Yannis Panagakis, and Maja Pantic. Investigating bias in deep face analysis: The kanface dataset and empirical study. _Image and vision computing_, 102:103954, 2020.
* [116] Google PAIR. Google pair. people + ai guidebook. https://pair.withgoogle.com/guidebook, 2019. [Accessed February 1, 2023].
* [117] Bruce G Gordon. Vulnerability in research: basic ethical concepts and general approach to review. _Ochsner Journal_, 20(1):34-38, 2020.
* [118] Mitchell L Gordon, Michelle S Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori Hashimoto, and Michael S Bernstein. Jury learning: Integrating dissenting voices into machine learning models. In _Conference on Human Factors in Computing Systems (CHI)_, pages 1-19, 2022.
* [119] Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Ishan Misra, Levent Sagun, Armand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on uncurated images without supervision. _arXiv preprint arXiv:2202.08360_, 2022.
* [120] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017.

* [121] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18995-19012, 2022.
* [122] Mislav Grgic, Kresimir Delac, and Sonja Grgic. Scface-surveillance cameras face database. _Multimedia tools and applications_, 51(3):863-879, 2011.
* [123] Patrick J Grother, Mei L Ngan, Kayee K Hanaoka, et al. Face recognition vendor test part 3: demographic effects. 2019.
* [124] Manuel Gunther, Peiyun Hu, Christian Herrmann, Chi-Ho Chan, Min Jiang, Shufan Yang, Akshay Raj Dhamija, Deva Ramanan, Jurgen Beyerer, Josef Kittler, et al. Unconstrained face detection and open-set face recognition challenge. In _IEEE International Joint Conference on Biometrics (IJCB)_, pages 697-706. IEEE, 2017.
* [125] Guodong Guo, Yun Fu, Charles R Dyer, and Thomas S Huang. Image-based human age estimation by manifold learning and locally adjusted robust regression. _IEEE Transactions on Image Processing_, 17(7):1178-1188, 2008.
* [126] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In _European Conference on Computer Vision (ECCV)_, pages 87-102. Springer, 2016.
* [127] Foad Hamidi, Morgan Klaus Scheuerman, and Stacy M Branham. Gender recognition or gender reductionism? the social implications of embedded gender recognition systems. In _Conference on Human Factors in Computing Systems (CHI)_, pages 1-13, 2018.
* [128] Hu Han, Anil K Jain, Fang Wang, Shiguang Shan, and Xilin Chen. Heterogeneous face attribute estimation: A deep multi-task learning approach. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 40(11):2597-2609, 2017.
* [129] Margot Hanley, Apoorv Khandelwal, Hadar Averbuch-Elor, Noah Snavely, and Helen Nissenbaum. An ethical highlighter for people-centric dataset creation. 2020.
* [130] Alex Hanna and Tina M Park. Against scale: Provocations and resistances to scale thinking. _arXiv preprint arXiv:2010.08850_, 2020.
* [131] Alex Hanna, Emily Denton, Razvan Amironesei, Andrew Smart, and Hilary Nicole. Lines of sight. https://logicmag.io/commons/lines-of-sight/, 2020. [Accessed February 7, 2023].
* [132] Alex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical race methodology in algorithmic fairness. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 501-512, 2020.
* [133] Adam Harvey and Jules LaPlace. Exposing. ai, 2021.
* [134] Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In _International Conference on Machine Learning_, pages 1929-1938. PMLR, 2018.
* [135] Kenji Hata, Ranjay Krishna, Li Fei-Fei, and Michael S Bernstein. A glimpse far into the future: Understanding long-term crowd worker quality. In _Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing_, pages 889-901, 2017.
* [136] Caner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan, Albert Gordo, and Cristian Canton Ferrer. Towards measuring fairness in ai: the casual conversations dataset. _IEEE Transactions on Biometrics, Behavior, and Identity Science_, 2021.
* [137] Caner Hazirbas, Yejin Bang, Tiezheng Yu, Parisa Assar, Bilal Porgali, Vitor Albiero, Stefan Hermanek, Jacqueline Pan, Emily McReynolds, Miranda Bogen, Pascale Fung, and Cristian Canton Ferrer. Casual conversations v2: Designing a large consent-driven dataset to measure algorithmic bias and robustness, 2022.

* [138] Steven Y He, Charles E McCulloch, W John Boscardin, Mary-Margaret Chren, Eleni Linos, and Sarah T Arron. Self-reported pigmenty phenotypes and race are significant but incomplete predictors of fitzpatrick skin phototype in an ethnically diverse population. _Journal of the American Academy of Dermatology_, 71(4):731-737, 2014.
* [139] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. Women also snowboard: Overcoming bias in captioning models. In _European Conference on Computer Vision (ECCV)_, pages 771-787, 2018.
* [140] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations (ICLR)_, 2019.
* [141] Julie J Henkelman and Robin D Everall. Informed consent with children: Ethical and practical implications. _Canadian Journal of Counselling and Psychotherapy_, 35(2), 2001.
* [142] Alex Hern. Flickr faces complaints over 'offensive' auto-tagging for photos. https://www.theguardian.com/technology/2015/may/20/flickr-complaints-offensive-auto-tagging-photos, May 2015.
* [143] Alex Hern. Google's solution to accidental algorithmic racism: Ban gorillas. https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people, January 2018.
* [144] Alex Hern. Twitter apologises for 'racist' image-cropping algorithm. https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm, September 2020.
* [145] Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. Natural language descriptions of deep visual features. In _International Conference on Learning Representations_, 2021.
* [146] Kashmir Hill. Wrongfully accused by an algorithm. _The New York Times_, 2020.
* [147] Mark E Hill. Race of the interviewer and perception of skin color: Evidence from the multi-city study of urban inequality. _American Sociological Review_, pages 99-108, 2002.
* [148] Yusuke Hirota, Yuta Nakashima, and Noa Garcia. Quantifying societal bias amplification in image captioning. In _IEEE/ CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [149] Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. The dataset nutrition label: A framework to drive higher data quality standards. 2018.
* [150] Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daume III, Miro Dudik, and Hanna Wallach. Improving fairness in machine learning systems: What do industry practitioners need? In _Conference on Human Factors in Computing Systems (CHI)_, pages 1-16, 2019.
* [151] Sara Hooker. Moving beyond "algorithmic bias is a data problem". _Patterns_, 2(4):100241, 2021.
* [152] Ayanna Howard, Cha Zhang, and Eric Horvitz. Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems. In _2017 IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO)_, pages 1-7. IEEE, 2017.
* [153] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. In _Workshop on faces in'Real-Life'Images: detection, alignment, and recognition_, 2008.
* [154] Han-Yin Huang and Cynthia CS Liem. Social inclusion in curated contexts: Insights from museum practices. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 300-309, 2022.

* [155] Zhanyuan Huang, Yang Liu, Yajun Fang, and Berthold KP Horn. Video-based fall detection for seniors with human pose estimation. In _2018 4th international conference on Universal Village (UV)_, pages 1-4. IEEE, 2018.
* [156] Jennifer L Hughes, Abigail A Camden, Tenzin Yangchen, et al. Rethinking and updating demographic questions: Guidance to improve descriptions of research samples. _Psi Chi Journal of Psychological Research_, 21(3):138-151, 2016.
* [157] Human Rights Campaign Foundation. Talking about pronouns in the workplace. https://www.thehrcfoundation.org/professional-resources/talking-about-pronouns-in-the-workplace, n.d.
* [158] Andrew Hundt, William Agnew, Vicky Zeng, Severin Kacianka, and Matthew Gombolay. Robots enact malignant stereotypes. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 743-756, 2022.
* [159] Ben Hutchinson, Andrew Smart, Alex Hanna, Emily Denton, Christina Greer, Oddur Kjartansson, Parker Barnes, and Margaret Mitchell. Towards accountability for machine learning datasets: Practices from software engineering and infrastructure. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 560-575, 2021.
* [160] IBM. Design for ai. https://www.ibm.com/design/ai, 2019. [Accessed February 1, 2023].
* [161] Illinois Legislature. Biometric information privacy act. https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=3004&ChapterID=57, 2008. [Accessed November 12, 2022].
* [162] Lilly Irani. The cultural work of microwork. _New media & society_, 17(5):720-739, 2015.
* [163] Joel Janai, Fatma Guney, Aseem Behl, Andreas Geiger, et al. Computer vision for autonomous vehicles: Problems, datasets and state of the art. _Foundations and Trends(r) in Computer Graphics and Vision_, 12(1-3):1-308, 2020.
* [164] Oliver Jesorsky, Klaus J Kirchberg, and Robert W Frischholz. Robust face detection using the hausdorff distance. In _Audio-and Video-Based Biometric Person Authentication: Third International Conference, AVBPA 2001 Halmstad, Sweden, June 6-8, 2001 Proceedings 3_, pages 90-95. Springer, 2001.
* [165] Julie Jiang, Emily Chen, Luca Luceri, Goran Muric, Francesco Pierri, Ho-Chun Herbert Chang, and Emilio Ferrara. What are your pronouns? examining gender pronoun usage on twitter. _arXiv preprint arXiv:2207.10894_, 2022.
* [166] Eun Seo Jo and Timnit Gebru. Lessons from archives: Strategies for collecting sociocultural data in machine learning. In _ACM Conference on Fairness, Accountability and Transparency (FAccT)_, 2020.
* [167] Sonam Joshi. Why indians are sharing their pronouns on social media. https://timesofindia.indiatimes.com/india/why-indians-are-sharing-their-pronouns-on-social-media/articleshow/71669703.cms, October 2019.
* [168] Rie Kamikubo, Utk Marsh Dwivedi, and Hernisa Kacorri. Sharing practices for datasets related to accessibility and aging. In _Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility_, pages 1-16, 2021.
* [169] Shivani Kapania, Alex S Taylor, and Ding Wang. A hunt for the snark: Annotator diversity in data practices. In _Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems_, pages 1-15, 2023.
* [170] Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In _IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 1548-1558, 2021.
* [171] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4401-4410, 2019.

* [172] Matthew Kay, Cynthia Matuszek, and Sean A Munson. Unequal representation and gender stereotypes in image search results for occupations. In _Conference on Human Factors in Computing Systems (CHI)_, pages 3819-3828, 2015.
* [173] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_, 2017.
* [174] Jane Kaye, Edgar A Whitley, David Lund, Michael Morrison, Harriet Teare, and Karen Melham. Dynamic consent: a patient interface for twenty-first century research networks. _European journal of human genetics_, 23(2):141-146, 2015.
* [175] Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The megaface benchmark: 1 million faces for recognition at scale. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4873-4882, 2016.
* [176] Norbert L Kerr. Harking: Hypothesizing after the results are known. _Personality and social psychology review_, 2(3):196-217, 1998.
* [177] Suzanne J Kessler and Wendy McKenna. _Gender: An ethnomethodological approach_. University of Chicago Press, 1985.
* [178] Florian Keusch. The influence of answer box format on response behavior on list-style open-ended questions. _Journal of Survey Statistics and Methodology_, 2(3):305-322, 2014.
* [179] Os Keyes. The misgendering machines: Trans/hci implications of automatic gender recognition. _Proceedings of the ACM on human-computer interaction_, 2(CSCW):1-22, 2018.
* [180] Zaid Khan and Yun Fu. One label, one billion faces: Usage and consistency of racial categories in computer vision. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 587-597, 2021.
* [181] Won Kim, Byoung-Ju Choi, Eui-Kyeong Hong, Soo-Kyung Kim, and Doheon Lee. A taxonomy of dirty data. _Data mining and knowledge discovery_, 7:81-99, 2003.
* [182] Jennifer Klima, Sara M Fitzgerald-Butt, Kelly J Kelleher, Deena J Chisolm, R Dawn Comstock, Amy K Ferketich, and Kim L McBride. Understanding of informed consent by parents of children enrolled in a genetic biobank. _Genetics in Medicine_, 16(2):141-148, 2014.
* [183] Bernard Koch, Emily Denton, Alex Hanna, and Jacob Gates Foster. Reduced, reused and recycled: The life of a dataset in machine learning research. In _Advances in Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B)_, 2021.
* [184] Adam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig, Andreas Morell-Forster, and Thomas Vetter. Empirically analyzing the effect of dataset biases on deep face recognition systems. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 2093-2102, 2018.
* [185] Adam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig, Andreas Morell-Forster, and Thomas Vetter. Analyzing and reducing the damage of dataset bias to face recognition with synthetic data. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, 2019.
* [186] Matthew B Kugler. From identification to identity theft: Public perceptions of biometric privacy harms. _UC Irvine L. Rev._, 10:107, 2019.
* [187] Neeraj Kumar, Alexander C Berg, Peter N Belhumeur, and Shree K Nayar. Attribute and simile classifiers for face verification. In _IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 365-372, 2009.
* [188] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _International Journal of Computer Vision (IJCV)_, 128(7):1956-1981, 2020.

* [189] Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:728-740, 2020.
* [190] Anjana Lakshmi, Bernd Wittenbrink, Joshua Correll, and Debbie S Ma. The india face set: International and cultural boundaries impact face impressions and perceptions of category membership. _Frontiers in psychology_, 12:161, 2021.
* [191] Min Kyung Lee and Katherine Rich. Who is included in human perceptions of ai?: Trust and perceived fairness around healthcare ai and cultural mistrust. In _Conference on Human Factors in Computing Systems (CHI)_, pages 1-14, 2021.
* [192] John Leuner. A replication study: Machine learning models are capable of predicting sexual orientation from facial images. _arXiv preprint arXiv:1902.10739_, 2019.
* [193] Gil Levi and Tal Hassner. Age and gender classification using convolutional neural networks. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 34-42, 2015.
* [194] Jizhizi Li, Sihan Ma, Jing Zhang, and Dacheng Tao. Privacy-preserving portrait matting. In _ACM International Conference on Multimedia_, pages 3501-3509, 2021.
* [195] Tao Li and Lei Lin. Anonymousnet: Natural face de-identification with measurable privacy. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, 2019.
* [196] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European Conference on Computer Vision (ECCV)_, pages 740-755. Springer, 2014.
* [197] Zhenyi Liu, Trisha Lian, Joyce Farrell, and Brian A. Wandell. Neural network generalization: The impact of camera parameters. _IEEE Access_, 8:10443-10454, 2020.
* [198] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _IEEE International Conference on Computer Vision (ICCV)_, pages 3730-3738, 2015.
* [199] Duri Long and Brian Magerko. What is ai literacy? competencies and design considerations. In _Proceedings of the 2020 CHI conference on human factors in computing systems_, pages 1-16, 2020.
* [200] Zhongyu Lou, Fares Alnajar, Jose M Alvarez, Ninghang Hu, and Theo Gevers. Expression-invariant age estimation using structured learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 40(2):365-375, 2017.
* [201] Mike Loukides, Hilary Mason, and D Patil. Of oaths and checklists. https://www.oreilly.com/radar/of-oaths-and-checklists/, 2018. [Accessed August 7, 2023].
* [202] Alexandra Sasha Luccioni, Frances Corry, Hamsini Sridharan, Mike Ananny, Jason Schultz, and Kate Crawford. A framework for deprecating datasets: Standardizing documentation, identification, and communication. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 199-212, 2022.
* [203] Debbie S Ma, Joshua Correll, and Bernd Wittenbrink. The chicago face database: A free stimulus set of faces and norming data. _Behavior research methods_, 47(4):1122-1135, 2015.
* [204] Debbie S Ma, Justin Kantner, and Bernd Wittenbrink. Chicago face database: Multiracial expansion. _Behavior Research Methods_, 53(3):1289-1300, 2021.
* [205] Michael A Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. Co-designing checklists to understand organizational challenges and opportunities around fairness in ai. In _Proceedings of the 2020 CHI conference on human factors in computing systems_, pages 1-14, 2020.

* [206] Nicolas Malve. On the data set's ruins. _AI & SOCIETY_, pages 1-15, 2020.
* [207] Gianclaudio Malgieri and Jedrzej Niklas. Vulnerable data subjects. _Computer Law & Security Review_, 37:105415, 2020.
* [208] Varun Manjunatha, Nirat Saini, and Larry S Davis. Explicit bias discovery in visual question answering models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9562-9571, 2019.
* [209] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In _IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 416-423, 2001.
* [210] Natalia L Martinez, Martin A Bertran, Afroditi Papadaki, Miguel Rodrigues, and Guillermo Sapiro. Blind pareto fairness and subgroup robustness. In _International Conference on Machine Learning_, pages 7492-7501. PMLR, 2021.
* [211] Deborah Mascalzoni, Roberto Melotti, Cristian Pattaro, Peter Paul Pramstaller, Martin Gogele, Alessandro De Grandi, and Roberta Biasiotto. Ten years of dynamic consent in the chris study: informed consent as a dynamic process. _European Journal of Human Genetics_, 30(12):1391-1397, 2022.
* [212] Anna McKie. South african university drops gender titles in student correspondence. https://www.timeshighereduction.com/news/south-african-university-drops-gender-titles-student-correspondence, July 2018.
* [213] Richard McPherson, Reza Shokri, and Vitaly Shmatikov. Defeating image obfuscation with deep learning. _arXiv preprint arXiv:1609.00408_, 2016.
* [214] Helen Meng, PC Ching, Tan Lee, Man Wai Mak, Brian Mak, Y Moon, Man-Hung Siu, Xiaoou Tang, H Hui, Andrew Lee, et al. The multi-biometric, multi-device and multilingual (m3) corpus. In _Proc. Workshop Multimodal User Authentication_, 2006.
* [215] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised photo upsampling via latent space exploration of generative models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2437-2445, 2020.
* [216] Michele Merler, Nalini Ratha, Rogerio S Feris, and John R Smith. Diversity in faces. _arXiv preprint arXiv:1901.10436_, 2019.
* [217] Jacob Metcalf. "the study has been approved by the irb"": Gayface ai, research hype and the pervasive data ethics gap. _Pervade Team, Nov_, 2017.
* [218] Jacob Metcalf and Kate Crawford. Where are human subjects in big data research? the emerging ethics divide. _Big Data & Society_, 3(1):2053951716650211, 2016.
* [219] Milagros Miceli, Martin Schuessler, and Tianling Yang. Between subjectivity and imposition: Power dynamics in data annotation for computer vision. _Proceedings of the ACM on Human-Computer Interaction_, 4(CSCW2):1-25, 2020.
* [220] Alex Mihailidis, Brent Carmichael, and Jennifer Boger. The use of computer vision in an intelligent environment to support aging-in-place, safety, and independence in the home. _IEEE Transactions on information technology in biomedicine_, 8(3):238-247, 2004.
* [221] Eric Mintun, Alexander Kirillov, and Saining Xie. On interaction between augmentations and corruptions in natural corruption robustness. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 3571-3583, 2021.
* [222] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 220-229, 2019.

* [223] Brent Daniel Mittelstadt and Luciano Floridi. The ethics of big data: current and foreseeable issues in biomedical contexts. _The ethics of biomedical big data_, pages 445-480, 2016.
* [224] Ali Mollahosseini, Behzad Hasani, and Mohammad H Mahoor. Affectnet: A database for facial expression, valence, and arousal computing in the wild. _IEEE Transactions on Affective Computing_, 10(1):18-31, 2017.
* [225] Carl-Maria Morch, Abhishek Gupta, and Brian L Mishara. Canada protocol: An ethical checklist for the use of artificial intelligence in suicide prevention and mental health. _Artificial intelligence in medicine_, 108:e101934-e101934, 2020.
* [226] Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 51-59, 2017.
* [227] Emad Mostaque (Stability AI). We actually used 256 a100s for this per the model card, 150k hours in total so at market price $600k. https://twitter.com/emostaque/status/1563870674111832066, August 2022. [Accessed August 12, 2023].
* [228] Jessica Mudditt. The nation where your 'faceprint' is already being tracked. https://www.bbc.com/future/article/20220616-the-nation-where-your-faceprint-is-already-being-tracked, 2022. [Accessed June 30, 2022].
* [229] Guilherme Nascimento, Camila Laranjeira, Vinicius Braz, Anisio Lacerda, and Erickson R. Nascimento. A robust indoor scene recognition method based on sparse representation. In _Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications_, pages 408-415. Springer, 2018.
* [230] National Commission for the Propetection of Human Subjects of Biomedicaland Behavioral Research, Bethesda, Md. _The Belmont report: Ethical principles and guidelines for the protection of human subjects of research_. Superintendent of Documents, 1978.
* [231] National Health and Medical Research Council. Payment of participation in research: information for researchers, hrecs and other ethics review bodies. https://www.nhmrc.gov.au/about-us/publications/payment-participants-research-information-researchers-hrecs-and-other-ethics-review-bodies, 2019. [Accessed May 12, 2023].
* Division of Program Coordination, Planning and Strategic Initiatives. Gender pronouns & their use in workplace communications. https://dpcpsi.nih.gov/sgmro/gender-pronouns-resource, 2022. [Accessed November 24, 2022].
* [233] National People's Congress. Personal information protection law. https://personalinformationprotectionlaw.com/, 2021. [Accessed November 12, 2022].
* [234] Cornelia Neuert, Katharina Meitinger, Dorothee Behr, and Matthias Schonlau. The use of open-ended questions in surveys. _Methods, data, analyses: a journal for quantitative methods and survey methodology (mda)_, 15(1):3-6, 2021.
* [235] Lokesh P Nijhawan, Manthan D Janodia, BS Muddukrishna, Krishna Moorthi Bhat, K Laxminarayana Bairy, Nayanabhirama Udupa, Prashant B Musmade, et al. Informed consent: Issues and challenges. _Journal of advanced pharmaceutical technology & research_, 4(3):134, 2013.
* [236] Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. Ordinal regression with multiple output cnn for age estimation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4920-4928, 2016.
* [237] Roosa Norja, Linda Karlsson, Jan Antfolk, Thomas Nyman, and Julia Korkman. How old was she? the accuracy of assessing the age of adolescents' based on photos. _Nordic Psychology_, 74(1):70-85, 2022.
* [238] Brian A Nosek and Daniel Lakens. Registered reports, 2014.

* [239] Seong Joon Oh, Rodrig Benenson, Mario Fritz, and Bernt Schiele. Faceless person recognition: Privacy implications in social media. In _European Conference on Computer Vision (ECCV)_, pages 19-35. Springer, 2016.
* [240] OpenReview. Fairface: A novel face attribute dataset for bias measurement and mitigation. https://openreview.net/forum?id=S1xSSTMKDB, 2019. [Accessed August 1, 2022].
* [241] Roy Or-El, Soumyadip Sengupta, Ohad Fried, Eli Shechtman, and Ira Kemelmacher-Shlizerman. Lifespan age transformation synthesis. In _European Conference on Computer Vision (ECCV)_, pages 739-755. Springer, 2020.
* [242] Tribhuvanesh Orekondy, Mario Fritz, and Bernt Schiele. Connecting pixels to privacy and utility: Automatic redaction of private information in images. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8466-8475, 2018.
* [243] Orestis Papakyriakopoulos, Anna Seo Gyeong Choi, William Thong, Dora Zhao, Jerone Andrews, Rebecca Bourke, Alice Xiang, and Allison Koenecke. Augmented datasheets for speech datasets and ethical decision-making. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 881-904, 2023.
* [244] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition. 2015.
* [245] Amandalynne Paullada, Inioluwa Deborah Raji, Emily M Bender, Emily Denton, and Alex Hanna. Data and its (dis) contents: A survey of dataset development and use in machine learning research. _Patterns_, 2(11):100336, 2021.
* [246] Nikita Pavlichenko, Ivan Stelmakh, and Dmitry Ustalov. Crowdspeech and voxdiy: Benchmark datasets for crowdsourced audio transcription. In _Advances in Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS D&B)_, 2021.
* [247] Kenny Peng, Arunesh Mathur, and Arvind Narayanan. Mitigating dataset harms requires stewardship: Lessons from 1000 papers. In _Advances in Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B)_, 2021.
* [248] Billy Perrigo. Inside facebook's african sweatshop. https://time.com/6147458/facebook-africa-content-moderation-employee-treatment/, February 2022.
* [249] Mark Phillips. International data-sharing norms: from the oecd to the general data protection regulation (gdpr). _Human genetics_, 137:575-582, 2018.
* [250] P Jonathon Phillips, Fang Jiang, Abhijit Narvekar, Julianne Ayyad, and Alice J O'Toole. An other-race effect for face recognition algorithms. _ACM Transactions on Applied Perception (TAP)_, 8(2):1-11, 2011.
* [251] Trisha Phillips. Exploitation in payments to research subjects. _Bioethics_, 25(4):209-219, 2011.
* [252] AJ Piergiovanni and Michael Ryoo. Avoid dataset: Anonymized videos from diverse countries. _Advances in Neural Information Processing Systems (NeurIPS)_, pages 16711-16721, 2020.
* [253] Eugenia Politou, Efthimios Alepis, and Constantinos Patsakis. Forgetting personal data and revoking consent under the gdpr: Challenges and proposed solutions. _Journal of cybersecurity_, 4(1):tyy001, 2018.
* [254] Bilal Porgali, Vitor Albiero, Jordan Ryda, Cristian Canton Ferrer, and Caner Hazirbas. The casual conversations v2 dataset. _arXiv preprint arXiv:2303.04838_, 2023.
* [255] W Nicholson Price and I Glenn Cohen. Privacy in the age of medical big data. _Nature medicine_, 25(1):37-43, 2019.
* [256] Mattia Prosperi and Jiang Bian. Is it time to rethink institutional review boards for the era of big data? _Nature Machine Intelligence_, 1(6):260-260, 2019.
* [257] Carina EA Prunkl, Carolyn Ashurst, Markus Anderljung, Helena Webb, Jan Leike, and Allan Dafoe. Institutionalizing ethics in ai through broader impact requirements. _Nature Machine Intelligence_, 3(2):104-110, 2021.

* [258] Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data cards: Purposeful and transparent dataset documentation for responsible ai. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT_, pages 1776-1826, 2022.
* [259] Deborah Raji, Emily Denton, Emily M. Bender, Alex Hanna, and Amandalynne Paullada. Ai and the everything in the whole wide world benchmark. In _Advances in Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS D&B)_, 2021.
* [260] Inioluwa Deborah Raji and Genevieve Fried. About face: A survey of facial recognition evaluation. 2021.
* [261] Inioluwa Deborah Raji, Morgan Klaus Scheuerman, and Razvan Amironesei. You can't sit with us: exclusionary pedagogy in ai ethics education. In _ACM conference on Fairness, Accountability, and Transparency (FAccT)_, pages 515-525, 2021.
* [262] Vikram V Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Geode: a geographically diverse evaluation dataset for object recognition. _arXiv preprint arXiv:2301.02560_, 2023.
* [263] Daniel A Reid and Mark S Nixon. Using comparative human descriptions for soft biometrics. In _International Joint Conference on Biometrics (IJCB)_, pages 1-6. IEEE, 2011.
* [264] Karl Ricanek and Tamirat Tesafaye. Morph: A longitudinal image database of normal adult age-progression. In _7th international conference on automatic face and gesture recognition (FGR06)_, pages 341-345. IEEE, 2006.
* [265] Rashida Richardson, Jason M Schultz, and Kate Crawford. Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice. _NYUL Rev. Online_, 94:15, 2019.
* [266] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In _European Conference on Computer Vision (ECCV)_, pages 17-35. Springer, 2016.
* [267] Joseph P Robinson, Gennady Livitz, Yann Henon, Can Qin, Yun Fu, and Samson Timoner. Face recognition: too bias, or not too bias? In _IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 0-1, 2020.
* [268] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. In _Advances in Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B)_, 2022.
* [269] Rata Rokhshad, Maxime Ducret, Akhilanand Chaurasia, Teodora Karteva, Miroslav Radenkovic, Jelena Roganovic, Manal Hamdan, Hossein Mohammad-Rahimi, Joachim Krois, Pierre Lahoud, et al. Ethical considerations on artificial intelligence in dentistry: A framework and checklist. _Journal of dentistry_, page 104593, 2023.
* [270] Norma RA Romm. Interdisciplinary practice as reflexivity. _Systemic Practice and Action Research_, 11:63-77, 1998.
* [271] Adam Rose. Are face-detection cameras racist? http://content.time.com/time/business/article/0,8599,1954643-1,00.html, January 2010.
* [272] Amir Rosenfeld, Richard Zemel, and John K Tsotsos. The elephant in the room. _arXiv preprint arXiv:1808.03305_, 2018.
* [273] Negar Rostamzadeh, Diana Mincu, Subhrajit Roy, Andrew Smart, Lauren Wilcox, Mahima Pushkarna, Jessica Schrouff, Razvan Amironesei, Nyalleng Moorosi, and Katherine Heller. Healthsheet: development of a transparency artifact for health datasets. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 1943-1961, 2022.
* [274] Wendy Roth. _Race migrations: Latinos and the cultural transformation of race_. Stanford University Press, 2012.

* Roth [2016] Wendy D Roth. The multiple dimensions of race. _Ethnic and Racial Studies_, 39(8):1310-1338, 2016.
* Rothbart and Taylor [1992] Myron Rothbart and Marjorie Taylor. Category labels and social reality: Do we view social categories as natural kinds? 1992.
* Rothe et al. [2015] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Dex: Deep expectation of apparent age from a single image. In _IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)_, pages 10-15, 2015.
* Rothe et al. [2018] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from a single image without facial landmarks. _International Journal of Computer Vision_, 126(2):144-157, 2018.
* Rowe [2001] Gavin Rowe, Paul Willner. Alcohol servers' estimates of young people's ages. _Drugs: education, prevention and policy_, 8(4):375-383, 2001.
* Rozynska [2022] Joanna Rozynska. The ethical anatomy of payment for research participants. _Medicine, Health Care and Philosophy_, 25(3):449-464, 2022.
* Sagawa et al. [2020] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In _International Conference on Learning Representations (ICLR)_, 2020.
* Sambasivan et al. [2021] Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. "everyone wants to do the model work, not the data work": Data cascades in high-stakes AI. In _Conference on Human Factors in Computing Systems (CHI)_. ACM, may 2021. doi: 10.1145/3411764.3445518.
* Sarkar et al. [2005] Sudeep Sarkar, P Jonathon Phillips, Zongyi Liu, Isidro Robledo Vega, Patrick Grother, and Kevin W Bowyer. The humanid gait challenge problem: Data sets, performance, and analysis. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 27(2):162-177, 2005.
* Scheuerman et al. [2019] Morgan Klaus Scheuerman, Jacob M Paul, and Jed R Brubaker. How computers see gender: An evaluation of gender classification in commercial facial analysis services. _Proceedings of the ACM on Human-Computer Interaction_, 3(CSCW):1-33, 2019.
* Scheuerman et al. [2020] Morgan Klaus Scheuerman, Katta Spiel, Oliver L Haimson, Foad Hamidi, and Stacy M Branham. Hci guidelines for gender equity and inclusivity. https://www.morgan-klaus.com/gender-guidelines.html, May 2020. [Accessed August 1, 2022].
* Scheuerman et al. [2020] Morgan Klaus Scheuerman, Kandrea Wade, Caitlin Lustig, and Jed R Brubaker. How we've taught algorithms to see identity: Constructing race and gender in image databases for facial analysis. _Proceedings of the ACM on Human-computer Interaction_, 4(CSCW1):1-35, 2020.
* Scheuerman et al. [2021] Morgan Klaus Scheuerman, Alex Hanna, and Emily Denton. Do datasets have politics? disciplinary values in computer vision dataset development. _Proceedings of the ACM on Human-Computer Interaction_, 5(CSCW2):1-37, 2021.
* Schuhmann et al. [2021] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.
* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:25278-25294, 2022.
* Schumann et al. [2021] Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari, and Caroline Rebecca Pantofaru. A step toward more inclusive people annotations for fairness. In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES)_, 2021.
* Segall et al. [1966] Marshall H Segall, Donald Thomas Campbell, and Melville Jean Herskovits. _The influence of culture on visual perception_. Bobbs-Merrill Indianapolis, 1966.

* [292] Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D Sculley. No classification without representation: Assessing geodiversity issues in open data sets for the developing world. _arXiv preprint arXiv:1711.08536_, 2017.
* [293] Boaz Shmueli, Jan Fell, Soumya Ray, and Lun-Wei Ku. Beyond fair pay: Ethical implications of nlp crowdsourcing. In _Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)_, pages 3758-3769, 2021.
* [294] Hera Siddiqui, Ajita Rattani, Karl Ricanek, and Twyla Hill. An examination of bias of facial analysis based bmi prediction models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2926-2935, 2022.
* [295] Laura Silver. Smartphone ownership is growing rapidly around the world, but not always equally. https://www.pewresearch.org/global/2019/02/05/smartphone-ownership-is-growing-rapidly-around-the-world-but-not-always-equally/, August 2020.
* [296] Eleanor Singer and Mick P Couper. Some methodological uses of responses to open questions and other verbatim comments in quantitative surveys. _Methods, data, analyses: a journal for quantitative methods and survey methodology (mda)_, 11(2):115-134, 2017.
* [297] Richa Singh, Mayank Vatsa, Himanshu S Bhatt, Samarth Bharadwaj, Afzel Noore, and Shahin S Nooreyezdan. Plastic surgery: A new dimension to face recognition. _IEEE Transactions on Information Forensics and Security_, 5(3):441-448, 2010.
* [298] Jolene D Smyth, Don A Dillman, Leah Melani Christian, and Mallory McBride. Open-ended questions in web surveys: Can increasing the size of answer boxes and providing extra verbal instructions improve response quality? _Public Opinion Quarterly_, 73(2):325-337, 2009.
* [299] Jacob Snow. Amazon's face recognition falsely matched 28 members of congress with mugshots. https://www.aclu.org/news/privacy-technology/amazons-face-recognition-falsely-matched-28, July 2018.
* [300] Benjamin Sobel. A taxonomy of training data: Disentangling the mismatched rights, remedies, and rationales for restricting machine learning. _Artificial Intelligence and Intellectual Property (Reto Hilty, Jyh-An Lee, Kung-Chung Liu, eds.), Oxford University Press, Forthcoming_, 2020.
* [301] Olivia Solon. Facial recognition's 'dirty little secret': Millions of online photos scraped without consent. _NBC News_, 2019.
* [302] Gowri Somanath, MV Rohith, and Chandra Kambhamettu. Vadana: A dense dataset for facial image analysis. In _IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)_, pages 2175-2182, 2011.
* [303] Patrik Sorqvist, Linda Langeborg, and Marten Eriksson. Women assimilate across gender, men don't: The role of gender to the own-anchor effect in age, height, and weight estimates 1. _Journal of Applied Social Psychology_, 41(7):1733-1748, 2011.
* [304] Katta Spiel, Oliver L Haimson, and Danielle Lottridge. How to do better with gender on surveys: a guide for hci researchers. _Interactions_, 26(4):62-65, 2019.
* [305] Aaron Springer, Jean Garcia-Gathright, and Henriette Cramer. Assessing and addressing algorithmic bias-but before we get there... In _AAAI Spring Symposia_, 2018.
* [306] Madhulika Srikumar, Rebecca Finlay, Grace Abuhamad, Carolyn Ashurst, Rosie Campbell, Emily Campbell-Ratcliffe, Hudson Hongo, Sara R Jordan, Joseph Lindley, Aviv Ovadya, et al. Advancing ethics review practices in ai research. _Nature Machine Intelligence_, 4(12):1061-1064, 2022.
* [307] Ramya Srinivasan, Emily Denton, Jordan Famularo, Negar Rostamzadeh, Fernando Diaz, and Beth Coleman. Artsheets for art datasets. In _Advances in Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B)_, 2021.

* [308] Ryan Steed and Aylin Caliskan. Image representations learned with unsupervised pre-training contain human-like biases. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 701-713, 2021.
* [309] Nikki Stevens. Open demographics documentation. https://nikkistevens.com/open-demographics/index.htm, n.d. [Accessed November 22, 2021].
* [310] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in crowded scenes. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2325-2333, 2016.
* [311] Qianru Sun, Liqian Ma, Seong Joon Oh, Luc Van Gool, Bernt Schiele, and Mario Fritz. Natural and effective obfuscation by head inpainting. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5050-5059, 2018.
* [312] Harini Suresh and John Guttag. A framework for understanding sources of harm throughout the machine learning life cycle. In _Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO)_. 2021.
* [313] Keiko Tagai, Hitomi Ohtaka, and Hiroshi Nittono. Faces with light makeup are better recognized than faces with heavy makeup. _Frontiers in psychology_, 7:226, 2016.
* [314] Harriet JA Teare, Megan Prictor, and Jane Kaye. Reflections on dynamic consent in biomedical research: the story so far. _European journal of human genetics_, 29(4):649-656, 2021.
* [315] Tech Inquiry. Official response from wiley. https://techinquiry.org/WileyResponse.html, 2019. [Accessed June 30, 2022].
* [316] Edward E Telles. Racial ambiguity among the brazilian population. _Ethnic and racial studies_, 25(3):415-441, 2002.
* [317] Graham Thomas, Rikke Gade, Thomas B Moeslund, Peter Carr, and Adrian Hilton. Computer vision for sports: Current applications and research topics. _Computer Vision and Image Understanding_, 159:3-18, 2017.
* [318] William Thong, Przemyslaw Joniak, and Alice Xiang. Beyond skin tone: A multidimensional measure of apparent skin color. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4903-4913, 2023.
* [319] Schrasing Tong and Lalana Kagal. Investigating bias in image classification using model explanations. _arXiv preprint arXiv:2012.05463_, 2020.
* [320] Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 30(11):1958-1970, 2008.
* [321] Carlos Torralba, Siddharth Suri, and Saiph Savage. Quantifying the invisible labor in crowd work. _Proceedings of the ACM on human-computer interaction_, 5(CSCW2):1-26, 2021.
* [322] John Twigg. _The Right to Safety: some conceptual and practical issues_. Benfield Hazard Research Centre, 2003.
* [323] Ries Uittenbogaard, Clint Sebastian, Julien Vijverberg, Bas Boom, Dariu M Gavrila, et al. Privacy protection in street-view panoramas using depth and multi-view imagery. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10581-10590, 2019.
* [324] UK Information Commissioner's Office. What do we need to do to ensure lawfulness, fairness, and transparency in ai systems? https://ico.org.uk/for-organisations/guide-to-data-protection/key-dataprotection-themes/guidance-on-ai-and-data-protection/what-do-we-need-todo-to-ensure-lawfulness-fairness-and-transparency-in-ai-systems/, 2020. [Accessed June 30, 2022].

* [325] UK Information Commissioner's Office. How should we obtain, record and manage consent? https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/consent/how-should-we-obtain-record-and-manage-consent, n.d. [Accessed May 1, 2023].
* [326] UNICEF. Bridging the digital divide for children and adolescents in east asia and pacific. https://www.unicef.org/eap/bridging-digital-divide-children-and-adolescents-east-asia-and-pacific, 2020. [Accessed November 24, 2022].
* [327] UNICEF et al. Unicef procedure for ethical standards in research, evaluation, data collection and analysis. _Nueva York.(2012), Ethical Principles, Dilemmas, and Risks in Collecting Data on Violence against Children: A Review of Available Literature, Nueva York_, 2015.
* [328] United Nations. Principles and recommendations for population and housing censuses. _Statistical Papers_, No.67, Sales No E.98.XVII.8, 1998.
* [329] United Nations Statistics Division. Standard country or area codes for statistical use. https://unstats.un.org/unsd/methodology/m49/, n.d. [Accessed May 1, 2021].
* [330] United States Census Bureau. How disability data are collected from the americ community survey. https://www.census.gov/topics/health/disability/guidance/data-collection-acs.html, 2021. [Accessed August 1, 2022].
* [331] United States. National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research. _The Belmont report: ethical principles and guidelines for the protection of human subjects of research_, volume 1. Department of Health, Education, and Welfare, National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research, 1978.
* [332] Tim Valentine, Michael B Lewis, and Peter J Hills. Face-space: A unifying concept in face recognition research. _The Quarterly Journal of Experimental Psychology_, 69(10):1996-2019, 2016.
* [333] Richard Van Noorden. The ethical questions that haunt facial-recognition research. _Nature_, 587(7834):354-359, 2020.
* [334] Kushal Vangara, Michael C King, Vitor Albiero, Kevin Bowyer, et al. Characterizing the variability in face recognition accuracy relative to race. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, 2019.
* [335] Jenny Vestlund, Linda Langeborg, Patrik Sorqvist, and Marten Eriksson. Experts on age estimation. _Scandinavian Journal of Psychology_, 50(4):301-307, 2009.
* [336] James Vincent. 'an engine for the imagination': the rise of ai image generators. an interview with midjourney founder david holz. https://www.theverge.com/2022/8/2/23287173/ai-image-generation-art-midjourney-multiverse-interview-david-holz, August 2022. [Accessed August 12, 2023].
* [337] James Vincent. Ai art tools stable diffusion and midjourney targeted with copyright lawsuit. https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart, January 2023. [Accessed February 10, 2023].
* [338] James Vincent. Getty images is suing the creators of ai art tool stable diffusion for scraping its content. https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit, January 2023. [Accessed February 10, 2022].
* [339] Manuel C Voelkle, Natalie C Ebner, Ulman Lindenberger, and Michaela Riediger. Let me guess how old you are: effects of age, gender, and facial expression on perceptions of age. _Psychology and aging_, 27(2):265, 2012.

* [340] Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind Narayanan, and Olga Russakovsky. Revise: A tool for measuring and mitigating bias in visual datasets. volume 130, pages 1790-1810. Springer, 2022.
* [341] Jialu Wang, Yang Liu, and Caleb Levy. Fair classification with group-dependent label noise. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 526-536, 2021.
* [342] Mei Wang and Weihong Deng. Mitigating bias in face recognition using skewness-aware reinforcement learning. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9322-9331, 2020.
* [343] Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial faces in the wild: Reducing racial bias by information maximization adaptation network. In _IEEE/CVF International Conference on Computer Vision (ICCV)_, 2019.
* [344] Yilun Wang and Michal Kosinski. Deep neural networks are more accurate than humans at detecting sexual orientation from facial images. _Journal of personality and social psychology_, 114(2):246, 2018.
* [345] Ze Wang, Xin He, and Fan Liu. Examining the effect of smile intensity on age perceptions. _Psychological reports_, 117(1):188-205, 2015.
* [346] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8919-8928, 2020.
* [347] Olivia R Ware, Jessica E Dawson, Michi M Shinohara, and Susan C Taylor. Racial limitations of fitzpatrick skin type. _Cutis_, 105(2):77-80, 2020.
* [348] Griffin M Weber, Kenneth D Mandl, and Isaac S Kohane. Finding the missing link for big biomedical data. _Jama_, 311(24):2479-2480, 2014.
* [349] David Wen, Saad M Khan, Antonio Ji Xu, Hussein Ibrahim, Luke Smith, Jose Caballero, Luis Zepeda, Carlos de Blas Perez, Alastair K Denniston, Xiaoxuan Liu, et al. Characteristics of publicly available skin cancer image datasets: a systematic review. _The Lancet Digital Health_, 4(1):e64-e74, 2022.
* [350] Edgar A Whitley. Informational privacy, consent and the "control" of personal data. _Information security technical report_, 14(3):154-159, 2009.
* [351] Meredith Whittaker, Meryl Alper, Cynthia L Bennett, Sara Hendren, Liz Kaziunas, Mara Mills, Meredith Ringel Morris, Joy Rankin, Emily Rogers, Marcel Salas, et al. Disability, bias, and ai. _AI Now Institute_, page 8, 2019.
* [352] Benjamin Wilson, Judy Hoffman, and Jamie Morgenstern. Predictive inequity in object detection. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, 2019.
* [353] Lloyd Windrim, Arman Melkumyan, Richard Murphy, Anna Chlingaryan, and Juan Nieto. Unsupervised feature learning for illumination robustness. In _IEEE International Conference on Image Processing (ICIP)_, pages 4453-4457, 2016.
* [354] World Economic Forum. The digital revolution is leaving poorer kids behind. https://www.weforum.org/agenda/2022/04/the-digital-revolution-is-leaving-poorer-kids-behind/, 2022. [Accessed November 24, 2022].
* [355] World Health Organization. Ageism in artificial intelligence for health. https://www.who.int/publications/i/item/9789240040793, 2022. [Accessed November 24, 2022].
* [356] World Health Organization and others. Ethics and governance of artificial intelligence for health: Who guidance. 2021.

* [357] David Wright. A framework for the ethical impact assessment of information technology. _Ethics and information technology_, 13:199-226, 2011.
* [358] Xiaolin Wu and Xi Zhang. Automated inference on criminality using face images. _arXiv preprint arXiv:1611.04135_, pages 4038-4052, 2016.
* [359] Alice Xiang. Being'seen' vs.'mis-seen': Tensions between privacy and fairness in computer vision. _Harvard Journal of Law & Technology, Forthcoming_, 2022.
* [360] Rongchang Xie, Fei Yu, Jiachao Wang, Yizhou Wang, and Li Zhang. Multi-level domain adaptive learning for cross-domain detection. In _IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)_, 2019.
* [361] Yuanjun Xiong, Kai Zhu, Dahua Lin, and Xiaoou Tang. Recognize complex events from static images by fusing deep channels. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1600-1609, 2015.
* [362] Runhua Xu, Nathalie Baracaldo, and James Joshi. Privacy-preserving machine learning: Methods, challenges and directions. _arXiv preprint arXiv:2108.04417_, 2021.
* [363] Tian Xu, Jennifer White, Sinan Kalkan, and Hatice Gunes. Investigating bias and fairness in facial expression recognition. In _European Conference on Computer Vision Workshops (ECCVW)_, pages 506-523. Springer, 2020.
* [364] Xin Xu and Jie Wang. Extended non-local feature for visual saliency detection in low contrast images. In _European Conference on Computer Vision Workshops (ECCVW)_, 2019.
* [365] Blaise Aguera y Arcas, Margaret Mitchell, and Alexander Todorov. Physiognomy's new clothes. https://medium.com/@blaisea/physiognoms-new-clothes-f2d4b59fdd6a, 2017. [Accessed October 22, 2022].
* [366] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky. Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, pages 547-558, 2020.
* [367] Kaiyu Yang, Jacqueline H Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky. A study of face obfuscation in imagenet. In _International Conference on Machine Learning (ICML)_, pages 25313-25330. PMLR, 2022.
* [368] Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: A face detection benchmark. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5525-5533, 2016.
* [369] Yu Yang, Aayush Gupta, Jianwei Feng, Prateek Singhal, Vivek Yadav, Yue Wu, Pradeep Natarajan, Varsha Hedau, and Jungseock Joo. Enhancing fairness in face detection in computer vision systems by demographic bias mitigation. In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES)_, pages 813-822, 2022.
* [370] Rui-Jie Yew and Alice Xiang. Regulating facial processing technologies: Tensions between legal and technical considerations in the application of illinois bipa. In _ACM Conference on Fairness, Accountability, and Transparency (FAccT)_, page 1017-1027, 2022.
* [371] Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. A fourier perspective on model robustness in computer vision. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [372] Seyma Yucer, Furkan Tektas, Noura Al Moubayed, and Toby P Breckon. Measuring hidden bias within face recognition via racial phenotypes. In _IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 995-1004, 2022.
* [373] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by deep multi-task learning. In _European Conference on Computer Vision (ECCV)_, pages 94-108. Springer, 2014.

* [374] Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5810-5818, 2017.
* [375] Dora Zhao, Angelina Wang, and Olga Russakovsky. Understanding and evaluating racial biases in image captioning. In _IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.
* [376] Dora Zhao, Jerone T. A. Andrews, and Alice Xiang. Men also do laundry: Multi-attribute bias amplification. In _International Conference on Machine Learning (ICML)_, 2023.
* [377] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. In _Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2017.
* [378] Mingyuan Zhou, Haiting Lin, S. Susan Young, and Jingyi Yu. Hybrid sensing face detection and registration for low-light and unconstrained conditions. _Applied Optics_, 57(1):69-78, January 2018.
* [379] Michael Zimmer. "but the data is already public": On the ethics of research in facebook. _Ethics and Information Technology_, 12(4):313-325, 2010.
* [380] Matthew Zook, Solon Barocas, Danah Boyd, Kate Crawford, Emily Keller, Seeta Pena Gangadharan, Alyssa Goodman, Rachelle Hollander, Barbara A Koenig, Jacob Metcalf, Arvind Narayanan, Alondra Nelson, and Frank Pasquale. Ten simple rules for responsible big data research, 2017.

Responsible Data Curation Checklist for Fairness and Robustness Evaluations

This checklist translates our HCCV data curation considerations and recommendations into action items for researchers and practitioners. Presented as a series of questions, these items are designed to stimulate discussions among data collection teams. The questions are purposefully worded to avoid binary responses, encouraging open-ended dialogues. The primary focus of the checklist is to underscore the ethical dimensions and ensure that teams address concerns encompassing purpose, consent and privacy, as well as diversity.

It is important to engage with the checklist as a preliminary exercise before beginning data collection. This approach promotes informed decision-making and minimizes risks, leading to more responsible and reliable outcomes.

Contextual diversity is acknowledged to avoid a one-size-fits-all approach. Moreover, customization is encouraged, as not all items apply universally; teams should modify or expand the checklist to align with their context and use case. As with existing AI ethics checklists [90, 201, 205, 225, 269, 357], it is important to recognize that the checklist is not a guarantee for ethical compliance; rather, it functions as a catalyst for discussion and reflection.

We understand that answering these questions is time-consuming, increasing the burden on data collection teams whose work is already undervalued [247, 282]. Therefore, when navigating through these lists, priority should be put on items related to the specific domain and task of interest. The level of engagement needed for each question will invariably differ. Keep in mind that the questions aim to spur ethical thinking during dataset development: "Ethics is often about finding a good or better, but not perfect, answer" [380].

### Purpose

The questions in this section focus on eliciting strategies for curating HCCV evaluation datasets specifically for fairness and robustness assessments. They seek alignment with objectives and inquire about factors known to influence these assessments to ensure comprehensive evaluations. Moreover, the questions aim to assist in formulating clear dataset purpose statements, preventing ambiguity and misuse of data, as well as exploring external validation to enhance transparency and accountability.

#### Dataset Development Strategy

* Can you provide details about your strategy for developing a new dataset tailored specifically for conducting fairness and robustness assessments in the context of HCCV? How do you plan to ensure that this dataset is aligned with the objectives of evaluating fairness and robustness?
* Can you elaborate on the factors your dataset will encompass to comprehensively enable fairness and robustness evaluations for HCCV models? How do you intend to capture the primary factors, including data subjects, instruments, and environments, that influence these evaluations?

#### Dataset Purpose Statement

* Can you provide details about your plan to formulate a comprehensive dataset purpose statement? How will this statement effectively communicate the core motivations driving, e.g., data collection, outline the intended dataset composition, specify permissible uses of the data, and identify the specific audience you aim to serve with the dataset?
* Can you elaborate on your strategy for ensuring the accuracy and ethical alignment of your dataset's purpose statement? How do you plan to externally validate the content and ethical considerations of the statement?
* Can you provide insights into the benefits and implications of submitting your dataset's purpose statement as part of a research study proposal in the format of a registered report for your project?

### Consent and Privacy

The questions in this section explore informed consent, legal compliance, and privacy protection measures within anonymization strategies. The questions emphasize clarity and voluntariness in consent processes to prevent coercion or misuse of data. Moreover, they attempt to elicit strategies for explaining data collection purposes, consent revocation, and accommodating diverse participation circumstances. Furthermore, the questions seek insights into addressing anonymization challenges, aiming to prevent re-identification risks, unauthorized exposure, and legal noncompliance, while preserving data utility and protecting data subjects' rights.

#### Informed and Voluntary Consent

* Can you elaborate on your approach to ensuring that you secure explicit, voluntary, and informed consent from all individuals who either appear in the dataset or can be discerned from it? How do you plan to handle consent for data annotators who may have disclosed personal information for the purposes of quantifying and addressing annotator perspectives and bias?
* Can you provide a comprehensive explanation of your strategy for conveying the purpose of data collection to the subjects? How do you intend to emphasize the utilization of their data, which includes various types of information such as facial, body, biometric images, as well as information about themselves and their environment, all in the context of assessing the fairness and robustness of HCCV systems?
* In what ways will you incorporate consent forms that are composed in plain language to enhance the understanding of AI technologies? How do you plan to make sure these forms effectively convey the intricacies of data usage?
* How do you plan to inform data subjects about their ability to withdraw consent at any given point during, or after, the data collection process? Can you provide details about the mechanisms you will have in place for facilitating this?
* Please provide insight into your strategy for collecting data from individuals below the age of majority or vulnerable individuals. How will you seek both guardian consent and voluntary informed consent in such cases?
* How do you plan to evaluate vulnerability along a continuous spectrum, taking into account contextual factors and recognizing that vulnerability is not solely binary or based solely on group affiliations, but can also be influenced by specific situations or circumstances?
* Can you also provide details about how you will consider the circumstances of participation, which might include the potential need for participatory design, assurances of compensation, provision of educational materials, and safeguards against authoritative structures? How will you address these various aspects in your approach?
* How do you intend to ensure that vulnerable individuals have a comprehensive understanding of the data usage and willingly provide informed assent? Can you outline the specific measures you intend to implement for this purpose?
* Can you elaborate on how you will respect the decision of a vulnerable individual who expresses dissent, regardless of the preferences of their guardian?

#### Consent Revocation Mechanisms

* How do you plan to integrate mechanisms that allow data subjects to easily withdraw their consent? Can you provide specifics on how this process will be designed and executed?
* Can you provide insights into the benefits and implications of implementing dynamic consent mechanisms that utilize personalized communication interfaces? How do you intend to ensure that these mechanisms adapt to the preferences and needs of individual data subjects?
* How do you intend to enable data subjects to actively participate in research activities and manage their consent preferences? Can you provide more details about the tools or processes you plan to put in place to achieve this?* In what ways will you explore the feasibility of online platforms for consent management that are user-friendly and minimize complexity for data subjects? What steps will you take to ensure easy accessibility?
* Can you provide insights into the options you will provide to data subjects for granting consent? How will you offer choices between blanket consent, case-by-case selection, or opt-in based on specific data usage?
* Can you elaborate on your considerations regarding the formation of a steering board or charitable trust composed of representative subjects from the dataset? How do you envision this entity contributing to decision-making processes?
* How do you plan to empower data subjects to actively participate in decisions concerning the usage of their data? What mechanisms or channels will you establish to facilitate this involvement?
* Can you provide information about the method you will offer data subjects to easily and promptly revoke their consent? How will you ensure that this process is straightforward and accessible?
* How do you intend to address varying levels of technological know-how and internet access among data subjects? Can you detail the measures you will take to accommodate these variations?
* What alternatives do you plan to offer for revoking consent that do not rely solely on online-based processes? How will you ensure that individuals with different needs and preferences can effectively revoke their consent?
* How do you plan to assess the practicality and suitability of the chosen mechanisms for consent revocation, taking into account the expected dataset size and the resources available to you? What criteria will you use to evaluate their effectiveness?

#### Country of Residence Information

* How do you plan to address the fact that anonymization measures might not universally meet legal requirements in specific regions, necessitating additional considerations? Can you provide insights into your strategy for ensuring legal compliance while implementing anonymization?
* Can you elaborate on your approach to collecting information about the country of residence for each individual in your dataset? How do you intend to use this information to ensure legal compliance and address potential privacy concerns?
* How do you plan to familiarize yourself with the data protection laws that are applicable in the countries of residence of your data subjects? Can you provide details about your process for gaining this understanding and how you will apply it to your data curation project?
* How do you intend to prioritize safeguarding data subjects' rights as stipulated by the data protection laws in their respective countries? What steps will you take to ensure that the creation and utilization of the dataset strictly adhere to the relevant data protection regulations? Can you provide specifics about the measures you will put in place to achieve this?
* What mechanisms do you intend to implement to ensure the adaptability of your dataset management strategy to changing legislative requirements? Can you provide details about how you will monitor and accommodate legislative changes in your dataset management approach? Can you provide insights into how you will strike a balance between maintaining compliance and effective dataset management in dynamic legal environments?

#### Privacy-Sensitive Image Regions and Metadata

* How do you plan to implement measures that effectively safeguard against re-identification risks, encompassing singling out, linkability, and inference, within your anonymization approach?
* Can you elaborate on your strategy for redacting all image regions that could inadvertently disclose privacy-related information? How do you intend to comprehensively identify and address these regions?* Can you elaborate on your strategy for the removal of elements such as body parts, clothing, and accessories for nonconsenting subjects to enhance privacy protection? Can you provide more details about the considerations and methods involved in this process?
* Can you elaborate on your strategy for the removal of text (possibly excluding copyright owner information) from the dataset's images to enhance privacy protection? Can you provide more details about the considerations and methods involved in this process?
* Can you explain your plan for empirically validating the chosen anonymization methods? How will you assess the methods' effectiveness in mitigating re-identification risks while preserving the utility of the data?
* Can you provide details about how human annotators will be engaged in the creation and verification of privacy leaking image region proposals for anonymization purposes? How will you ensure accuracy and consistency in this process?
* Can you provide details about how you intend to align region proposals predicted by algorithms with human judgment, addressing any potential failures or biases? Can you describe your strategy for maintaining a sensitive approach to these factors?
* What steps will you take to address jurisdiction-specific requirements that might necessitate human-generated proposals for biometric identifiers in order to comply with legal and regulatory standards?
* Can you elaborate on the measures you will take to prevent image metadata from inadvertently revealing unauthorized identifying information? How will you ensure that metadata remains privacy-conscious?
* How will you identify specific metadata elements that you intend to retain to ensure a comprehensive understanding during the evaluation process? Can you provide examples of the types of metadata you plan to retain for this purpose?
* How do you plan to replace or remove sensitive information within metadata while retaining its usefulness for fairness and robustness analyses? Can you provide insights into your approach for striking a balance in this regard?

### Diversity

The questions in this section revolve around obtaining accurate image annotations related to identity, phenotype, environmental factors, and instruments, while upholding inclusivity, sensitivity, and privacy. Additionally, the questions attempt to elicit strategies for documenting identity, ensuring fair compensation, and effective (anonymous) communication.

#### Self-Reported Annotations

* How do you plan to acquire annotations for images directly from the data subjects, leveraging their self-awareness and contextual knowledge to enhance the accuracy and quality of annotations? Can you elaborate on the methods and strategies you intend to use for this purpose?
* Can you elaborate on your strategy for addressing biases and ensuring careful handling when inferring labels about individuals? Can you provide reasoning as to why labels about individuals will be inferred as opposed to being self-identified? How will you actively mitigate potential biases that may arise during the labeling process?
* How do you intend to consider the implications of inferred labels, for example, in relation to data access request rights?

#### Versatile and Inclusive Response Options

* How do you plan to enhance the accuracy and nuance of identity information collection by providing respondents with both closed-ended and open-ended response choices? Can you elaborate on your strategy for using open-ended responses to gather more detailed and comprehensive data?
* How do you intend to ensure inclusivity and prevent any potential implications of exclusion in the response choices you offer?* Can you elaborate on your preparedness to manage the coding and analysis effort required for processing open-ended responses? What effective strategies do you plan to implement for managing and analyzing the data collected from open-ended questions? How will you handle the potential complexities and variations that can arise from these responses, ensuring that the insights and information derived can be accurately captured and utilized?

#### Dynamic Nature of Identity

* How do you plan to collect self-identified information on a per-image basis, accounting for the fact that identity is intrinsically contextual and temporal? Can you elaborate on your strategy for capturing nonstatic aspects of identity?
* Can you elaborate on your strategy for enabling data subjects to freely choose multiple identity categories without imposing any limitations? How will you ensure that subjects have the flexibility to express their identity in a comprehensive and unrestrictive manner?
* How do you intend to address potential requests for per-image updates to self-identified information provided by subjects over time, respecting their autonomy? What factors have you considered in relation to the potential effects of permitting updates?

#### Demographic Information

* How do you plan to collect precise biological age in years from data subjects to ensure an accurate representation of their age?
* Can you elaborate on your approach to gathering pronoun information from data subjects to enhance gender inclusivity and mitigate the risk of misgendering? How will you ensure that respondents feel comfortable providing this information?
* Can you explain your strategy for gathering consistent ancestry information from data subjects? How will you approach the collection of this information in a sensitive and inclusive manner?
* How do you intend to offer the option for data subjects not to disclose their sensitive attributes if they choose not to? Can you provide more details about how you will handle the sensitivity and privacy of these attributes?

#### Sensitive Attributes in Aggregate

* How do you plan to collect voluntarily disclosed sensitive attributes such as disability and pregnancy status? Can you elaborate on your approach to respecting the willingness of data subjects to provide these details?
* Can you provide insight into your strategy for reporting sensitive attributes, such as disability and pregnancy status, in aggregate data while safeguarding subjects' safety and privacy? How do you intend to ensure that individual identities are protected?
* Can you elaborate on your approach to relying on credible and appropriate sources for the categorization and definitions of sensitive attributes like disability or difficulty? How will you account for the potential variations in these definitions based on cultural, identity, and historical contexts?

#### Phenotypic and Neutral Performative Features

* How do you plan to collect phenotypic attributes, encompassing characteristics such as skin color, eye color, hair type, hair color, height, and weight? Can you provide insights into your strategy for obtaining these attributes in a sensitive and comprehensive manner?
* Can you elaborate on your approach to collecting a diverse range of neutral performative features, including aspects such as facial hair, hairstyle, cosmetics, clothing, and accessories? How do you intend to ensure inclusivity and accuracy in capturing these features?
* How do you plan to gather data on environment-related factors, which encompass details such as image capture time, season, weather, ambient lighting, scene, geography, camera position, and camera distance? Can you provide insights into your strategy for capturing these factors accurately and comprehensively?
* Can you elaborate on your approach to collecting instrument-related factors concerning the imaging devices used, including aspects such as lens, sensor, stabilization, flash usage, and post-processing software? How do you intend to ensure accuracy in capturing these details?
* How do you plan to obtain environment- and instrument-related information? Can you provide more details about the methods you will use, such as self-reporting, annotator estimation, and sourcing information from Exif metadata? How will you leverage contextual knowledge from image subjects to enhance data quality?
* Can you explain your approach to handling information such as precise geolocation and user-added details in Exif metadata that might contain personally identifying information? How will you ensure compliance with copyright regulations (if applicable) while maintaining privacy and adhering to ethical considerations?

#### Annotators as Contributors

* How do you plan to document the identities of data annotators, including capturing demographic details such as pronouns, age, and ancestry? Can you provide insights into your strategy for gathering and preserving this information while respecting privacy and ensuring transparency?
* Can you elaborate on your approach to highlighting the contributions of annotators beyond data labeling in the dataset documentation after the curation process? How do you intend to accurately represent the multifaceted roles and contributions of annotators?
* How do you plan to report the demographic information of annotators to analyze potential sources of bias in dataset annotations? Can you provide more details about your proposed approach for conducting this analysis while ensuring privacy and ethical considerations?

#### Fair Treatment and Compensation

* How do you plan to ensure that all contributors receive compensation that exceeds the minimum hourly wage of their respective country or jurisdiction of residence? Can you provide insights into your compensation strategy to ensure fair and ethical remuneration?
* Can you elaborate on your approach to exploring alternative payment models, such as compensation based on the average hourly wage? How do you intend to determine a compensation structure that is both fair and reflective of contributors' efforts?
* How will you establish direct communication channels between dataset creators and contributors? Can you provide more details about the methods you intend to implement for effective and transparent communication?
* What communication methods do you plan to explore that maintain the anonymity of contributors? Can you provide insights into your approach to balancing communication and privacy needs, such as using anonymous feedback forms?
* Can you provide information about your strategy for developing clear and accessible plain language guides to facilitate various tasks, such as image submission and data annotation? How do you plan to ensure that these guides effectively assist contributors?
* How do you intend to ensure that contributors from diverse backgrounds can easily understand and follow any instructions provided? Can you elaborate on your approach to promoting inclusivity and accessibility in your communication and guidelines?
* Can you provide details about how you plan to subject your recruitment and compensation procedures to ethics review? What steps will you take to ensure that your procedures align with ethical considerations and best practices?Literature Review

Through a thematic search strategy, we identified relevant research studies and datasets, revealing deficiencies in current image data curation practices or proposing potential solutions. By utilizing Semantic Scholar and Google Scholar, we curated relevant papers covering a wide spectrum of themes, including:

* Guidelines and best practices
* Human-subjects research
* Values in design
* HCCV datasets
* Diversity and inclusion
* Dataset curation
* Ethical frameworks and considerations
* Robustness and reliability
* Data and model documentation
* Benchmarking and evaluation
* Legal and regulatory considerations
* Bias detection and mitigation
* Privacy and data protection
* Critical AI

* Social implications

* Responsible AI

The themes were chosen based on our expertise and experience in designing CV datasets, training models, and developing ethical guidelines. To ensure a focused approach, we manually selected papers aligned with the scope of our study based on the relevance of a paper's title and abstract. This informed our initial categorization scheme, shown in Table 1, detailing key ethical considerations related to HCCV.

Initially broad, we further refined the categories to address the most prominent ethical issues pertaining to HCCV dataset curation, particularly for fairness and robustness evaluations. Consent and privacy categories were combined due to their interrelated nature and the influence of shared legal frameworks.

\begin{table}
\begin{tabular}{l l} \hline \hline Category & Explanation \\ \hline Purpose & The study discusses the underlying objectives and motivations for HCCV datasets. \\ Acquisition & The study discusses ethical considerations related to the acquisition, collection, and labeling of image data, including recruitment and compensation for contributors. \\ Consent & The study discusses consent and the responsible use of personal information. \\ Privacy & The study discusses privacy issues related to HCCV datasets or public data. \\ Ownership & The study discusses legal and ethical aspects of intellectual property rights in the context of HCCV datasets or public data. \\ Diversity & The study discusses factors concerning diversity, inclusion, and fair representation within HCCV datasets. This encompasses matters such as identifying and addressing biases, ensuring fairness, and mitigating discrimination. \\ Maintenance & The study discusses maintenance strategies for ensuring the integrity of HCCV datasets, including security measures. \\ \hline \hline \end{tabular}
\end{table}
Table 1: Categorization scheme for ethical considerations in HCCV research Additionally, we integrated acquisition-related considerations into the categories of diversity, consent and privacy, as well as purpose, recognizing their interconnectedness in ethical image data collection, labeling, and usage. Maintenance-related matters were intentionally excluded from our scope, as these primarily pertain to post-dataset creation activities, while technical and organizational security measures are typically covered through consent forms. Ownership concerns, often intertwined with privacy issues, were incorporated into the consent and privacy category.

To establish a comprehensive view, we expanded our corpus as necessary. This encompassed examining cited works within our initial corpus, studies referencing our primary sources, and additional contributions by authors from our initial corpus. Our review was supplemented by incorporating publicly available resources from reputable sources, such as government bodies, private institutions, and reliable news organizations. In total, our analysis covered 500 research studies and online resources.