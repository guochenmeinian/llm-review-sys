# DisCEdit: Model Editing by Identifying Discriminative Components

 Chaitanya Murti

Robert Bosch Centre for Cyberphysical Systems

Indian Institute of Science

mchaitanya@iisc.ac.in

&Chiranjib Bhattacharyya

Computer Science and Automation

Robert Bosch Centre for Cyberphysical Systems

Indian Institute of Science

chiru@iisc.ac.in

###### Abstract

Model editing is a growing area of research that is particularly valuable in contexts where modifying key model components, like neurons or filters, can significantly impact the model's performance. The key challenge lies in identifying important components useful to the model's predictions. We apply model editing to address two active areas of research, Structured Pruning, and Selective Class Forgetting. In this work, we adopt a distributional approach to the problem of identifying important components, leveraging the recently proposed _discriminative filters hypothesis_, which states that well-trained (convolutional) models possess discriminative filters that are essential to prediction. To do so, we define discriminative ability in terms of the Bayes error rate associated with the feature distributions, which is equivalent to computing the Total Variation (TV) distance between the distributions. However, computing the TV distance is intractable, motivating us to derive novel witness function-based lower bounds on the TV distance that require no assumptions on the underlying distributions; using this bound generalizes prior work such as Murti et al. [39] that relied on unrealistic Gaussianity assumptions on the feature distributions. With these bounds, we are able to discover critical subnetworks responsible for classwise predictions, and derive DisCEdit-SP and DisCEdit-U, algorithms for structured pruning requiring no access to the training data and loss function, and selective forgetting respectively. We apply DisCEdit-U to selective class forgetting on models trained on CIFAR10 and CIFAR100, and we show that on average, we can reduce accuracy on a single class by over 80% with a minimal reduction in test accuracy on the remaining classes. Similarly, on Structured pruning problems, we obtain 40.8% sparsity on ResNet50 on Imagenet, with only a 2.6% drop in accuracy with minimal fine-tuning. 1

Footnote 1: Our code is available at: https://github.com/chaimurti/DisCEdit

## 1 Introduction

The black-box nature of neural networks makes understanding the precise mechanism by which a neural network makes a prediction (in the classification or regression settings), or generates a sample (in the generative settings) an active area of research [44], and relevant to several related problems,such as robustness, sparsity, and memorization. In particular, recent work aims to address this problem by identifying which _components_ - such as neurons, convolutional filters, or attention heads - in the model contribute most significantly to predictions [44]. Prior work addressing this problem includes [2], which identified important filters by using human-defined concepts, and aligning them with filter responses to those concepts, whereas more recent work [48], uses an exhaustive regression-based approach. The field of structured pruning [19, 4] also provides a variety of heuristics for component importance, including first- and second-order derivative information [38, 37, 43], feature map ranks [35, 52], and layerwise reconstruction errors [59, 41]. Component attribution has also found attention in the fields of machine unlearning [58], where class-wise important components can be identified and edited to remove information about a given class or group[48, 23, 33].

In this work, we address this problem from a _distributional_ milieu: that is, by analyzing the distributions of feature maps for models trained on datasets with distinct classes or subgroups. Specifically, we use the _discriminative filters hypothesis (DFH)_ stated in [39], which states that well-trained models for classification possess a mix of _discriminative filters_ - filters that yield feature maps with distinct class-conditional distributions - and non-discriminative filters and that the discriminative filters are important to model predictions. Discriminative filters can be used to identify which filters contribute to the prediction for samples from a given class. Under restrictive assumptions of Gaussianity, the DFH was used in [39] to derive a structured pruning algorithm that required only some form of distributional access, and no access to the loss function and training set. However, several key challenges remain. First, is it possible to identify discriminative filters without restrictive assumptions on the filter outputs? Second, can discriminative filters be used in settings other than structured pruning? Last, can the performance of algorithms leveraging the DFH be improved upon with better identification of discriminative filters? In this work, we answer each of the questions affirmatively.

First, to identify discriminative filters without unrealistic assumptions on the feature map distributions, we derive tractable, witness function-based lower bounds on the Total Variation (TV) distance, with which we also reveal hitherto unknown ties between the TV distance and classical, discrimination-based classifiers. We then use our proposed methodology to identify discriminative filters for classwise model unlearning and structured pruning, which we call DisCEdit: **Dis**criminative **C**omponent identification for model **Editing**. Our methodology is simple - we first identify discriminative filters for a given class, and prune them (for class forgetting), called DisCEdit-U, or in the case of structured pruning, prune filters that are non-discriminative for all classes, called DisCEdit-SP. Note that our method for identifying discriminative filters requires only distributional access, and neither the training data nor the loss function. We illustrate our approach in Figure 1, and formally state our contributions below.

**Discriminative ability of a filter:** We quantify the discriminative ability of a filter in a neural network, as the worst possible Bayes error rate of binary classifiers trained upon the features it generates(See Section 3). Since, in general, computing the Bayes' error rate is intractable, we seek to approximate it using lower bounds on the TV distance.

**Witness function based Distribution Agnostic Lower bound on TV distance:** In order to identify discriminative filters without distributional assumptions, we propose a novel witness function-based lower bound on the TV distance between distributions to address this gap in Theorem 1. We propose a lower bound that relies only on knowledge of finitely many moments(Theorem 2) and derive another lower bound accounting for moment measurement errors in Theorem 3. These bounds do not require distribution-specific assumptions and hence enable this work to generalize previous work [39], which requires the distributions to be Gaussian( which is an unrealistic assumption). To be noted that these bounds are of independent interest as they are broadly applicable. Moreover, using a careful choice of witness function, our bounds reveal new connections between discriminant-based classifiers like

Figure 1: Identifying Discriminative Components for Model Unlearning and Structured Pruning

the Fisher linear discriminant and the Minimax Probability Machine, and the TV distance, which we state in Corollary 2.

**Model Editing using Lower Bounds:** We apply our lower bounds to the problem of identifying which components in a model contribute to predictions from certain classes.

1. **Class Unlearning:** We identify components capable of discriminating each class, which we then prune in order to unlearn that class, requiring no access to the loss function, called DisCEdit-U.
2. **Pruning without the training data or loss function:** Next, we derive a family of algorithms for structured pruning requiring no access to the training data or loss function by identifying non-discriminative filters with our proposed lower bounds, called DisCEdit-SP.

**Experimental Validation:** We produce a slate of experiments to illustrate the efficacy of identifying discriminative components for structured pruning and machine unlearning/class forgetting. We compare the efficacy of DisCEdit-SP for pruning VGG nets and ResNets trained on CIFAR10, and show that we achieve up to 76% sparsity with a.12% reduction in accuracy on VGG19, and on Imagenet, we achieve 40.8% sparsity with a 2.58% drop in accuracy with minimal fine-tuning. Similarly, for machine unlearning, we show that our method enables 83% drop in accuracy on the class to be forgotten, with a 1.2% increase in accuracy on the remaining classes, without any finetuning.

### Related Work

In this section, we introduce a brief summary of related literature. We provide a more detailed discussion of the literature in Appendix A.

Structured pruningStructured pruning reduces the inference time and power consumption of neural networks by removing entire filters or neurons, as opposed to unstructured pruning that removes individual weights [29; 18]. A variety of structured pruning methods require the loss function, and identify import components using gradient information [38; 30], or approximations of the Hessian of [29; 18; 51; 36].

A variety of structured pruning methods that do not rely on the loss function have been proposed, which identify important components by norms of the weights [31; 32], bounds on the reconstruction error incurred by pruning a filter [41; 59], the rank of the feature maps [35; 52], and coresets of feature maps [34]. More recent approaches identify components based on the discriminative ability of the corresponding featuremaps, using metrics like the Hellinger distance between class-conditional feature maps [39], or Fisher discriminant-based methods [20; 21]. In this work, we formalize the notion of discriminative ability in terms of the Bayes risk and derive novel lower bounds on the total variation distance to approximate it effectively. Unlike previous works, we make no assumptions about the class-conditional distributions, and our pruning algorithms require no access to the training data or loss function. For a more comprehensive discussion on structured pruning, we refer readers to Appendix A or the surveys [19; 4].

Machine unlearningMachine Unlearning has recently received significant attention due to concerns like data privacy and security [5; 42]. A variety of works propose methods to forget data points while maintaining model accuracy, even in adaptive settings [47; 16; 22; 14]. Selective forgetting, where classes or groups are forgotten, connects machine unlearning to continual learning [58; 57]. Model editing for unlearning, however, remains an underexplored area of research. Recent studies machine unlearning can be enhanced by sparsifying models [23; 46], and discrimination-aware pruning has been used in federated settings [56] and for forgetting specific classes [48]. Our work differs by directly utilizing the discriminative ability of model components to identify and remove those responsible for a given class, enabling effective unlearning without access to the original training data. For a more detailed discussion, we refer readers to Appendix A.

## 2 Background and Notation

In this section, we introduce our notation and provide basic background definitions.

NotationFor an integer \(N>0\), let \([N]:=\{1,\cdots,N\}\). Let \(\mathbf{0}_{N}\) be a vector of zeros of dimension \(N\). Let \(\operatorname{sort}_{B}(\{a_{1},\cdots,a_{M}\})\) be the set of the \(B\) largest elements of \(\{a_{1},\cdots,a_{M}\}\). Suppose \(\mathbb{P},\ \mathbb{Q}\) are two distributions supported on \(\mathcal{X}\), with densities given by \(p(x)\) and \(q(x)\). For a function \(f:\mathcal{X}\to\mathbb{R}\), let \(\bar{f}_{p}=\mathbb{E}_{x\sim\mathbb{P}}\left[f(x)\right]\), and let \(\bar{f}_{p}^{(2)}=\mathbb{E}_{x\sim\mathbb{P}}\left[f(x)^{2}\right]\). Let \(\mathcal{D}\) be a data distribution. Suppose the dataset has \(C\) classes, then let \(\mathcal{D}_{c}\) be the class-conditional distribution of the \(c\)-th class, and let \(\mathcal{D}_{c}\) be the distribution of the complement of \(c\) (that is, samples are drawn from all classes other than \(c\)).

Suppose we have a neural network \(\mathcal{W}=(W_{1},\cdots,W_{L})\). Each layer yields (flattened) representations

\[Y^{l}(x)=\left[Y_{1}^{l}(X),\cdots,Y_{N_{l}}^{l}(X)\right],\] (1)

where \(N_{l}\) is the number of filters in layer \(l\). Since \(Y^{l}\) is dependent on \(X\), we assume that \(Y^{l}(X)\sim\mathcal{D}^{l}\), and \(Y_{j}^{l}(X)\sim\mathcal{D}_{j}^{l}\). Furthermore, let \(\mathcal{D}_{j,c}^{l}\) and \(\mathcal{D}_{j,c}^{l}\) be the class-conditional distributions and class-complement distributions of \(Y_{j}^{l}(X)\) respectively.

BackgroundIn this section, we provide relevant background for this work. First, let \(\mathbb{P}\) and \(\mathbb{Q}\) be distributions supported on \(\mathcal{X}\), with moments \(\mu_{p},\Sigma_{p}\) and \(\mu_{q},\Sigma_{q}\). Then, recall that

\[\mathsf{Fish}(\mathbb{P},\mathbb{Q};u)=\frac{\left(u^{\top}(\mu_{p}-\mu_{q}) \right)^{2}}{u^{\top}\left(\Sigma_{p}+\Sigma_{q}\right)u}\quad\text{and}\quad \mathsf{MPM}(\mathbb{P},\mathbb{Q};u)=\frac{|u^{\top}(\mu_{p}-\mu_{q})|}{ \sqrt{u^{\top}\Sigma_{p}u}+\sqrt{u^{\top}\Sigma_{q}u}}.\] (2)

where \(\mathsf{Fish}\) denotes the Fisher discriminant [24], and \(\mathsf{MPM}\) denotes the Minimax probability machine [27; 28]. If we choose the optimal \(u\), denoted by \(u^{*}\), we write \(\max_{u}\mathsf{Fish}(\mathbb{P},\mathbb{Q};u)=\mathsf{Fish}(\mathbb{P},\mathbb{ Q};u^{*})=\mathsf{Fish}^{*}(\mathbb{P},\mathbb{Q})\) and \(\max_{u}\mathsf{MPM}(\mathbb{P},\mathbb{Q};u)=\mathsf{MPM}(\mathbb{P},\mathbb{ Q};u^{*})=\mathsf{MPM}(\mathbb{P},\mathbb{Q})^{*}\).

We define the TV and Hellinger distances as follows.

**Definition 1**.: _Let \(\mathbb{P}\) and \(\mathbb{Q}\) be two probability measures supported on \(X\), and let \(p\) and \(q\) be the corresponding densities. Then, we define the Total Variation Distance_ TV _as_

\[\mathsf{TV}\left(\mathbb{P},\mathbb{Q}\right)=\sup_{A\subset X}\left|\mathbb{ P}(A)-\mathbb{Q}(A)\right|=\frac{1}{2}\int_{X}\left|p(x)-q(x)\right|dx\]

The _Bayes Optimal classifier_, as given in [9; 8], associated with distributions \(\mathbb{P}\) and \(\mathbb{Q}\) with labels \(-1\) and \(1\) respectively is given by

\[f(x)=\operatorname*{arg\,max}_{c\in\{-1,1\}}\Pr(c|x),\]

and has the error rate \(R^{*}(\mathbb{P},\mathbb{Q})\), called the _Bayes Error Rate_. Next, we relate the Bayes classifier and the Bayes error rate (as described in, say, Devroye et al. [8]) of a classifier to the TV distance with the following identity. Consider a binary classification problem with instance \(x\) and labels \(c\in\{-1,1\}\), with class conditional distributions given by \(\mathbb{P}\) and \(\mathbb{Q}\). The Bayes error rate satisfies the identity

\[2R^{*}(\mathbb{P},\mathbb{Q})=1-\mathsf{TV}(\mathbb{P},\mathbb{Q}).\] (3)

## 3 Editing Models by Identifying Discriminative Components

There are three central questions addressed in this work:

1. **Model Unlearning:** How do we edit components in order to reduce accuracy on certain groups or classes only?
2. **Structured pruning:** How do we remove components to ensure that the accuracy of all classes is minimally affected?
3. **Determining the Discrimination Ability of a Model Component:** How do we assess the Discrimination ability of filters without access to the training data or loss function, and without making any assumptions about the class-conditional feature distributions?

Inspired by Murti et al. [39], the key idea in this work for addressing each of the questions raised above is to identify _discriminative components_, that yield feature maps upon which accurate classifiers can be trained, and thus with distinct class-conditional distributions. A heuristic to address this problem would be to train a classifier upon the feature map; those features upon which accurate classifiers can be trained are generated by discriminative filters; however, this is highly impractical. Thus, as with [39], we address this problem by identifying filters for which the class-conditional distributions of the feature maps are distinct, which are identified based on estimates of the total variation distance between the class-conditional distributions of the associated feature maps. In this work, we formalize _discriminative ability_ of a component in terms of the best possible classifier that can be trained on the features generated by it. In the sequel, we illustrate similar but distinct notions of discriminative ability important for classwise unlearning, and structured pruning. We first formally define _discriminative ability_ and the _class-\(c\) discriminative ability_ as follows.

**Definition 2** (Discriminative Ability).: _Consider a CNN with \(L\) layers trained on a dataset with \(C\) classes, and consider the \(j\)th filter in the \(l\)th layer. The_ **class-\(c\) discriminative ability**_\(\eta_{l,j}^{c}\), and the_ **discriminative ability**_\(\eta_{l,j}\) of the filter are given by_

\[\eta_{l,j}^{c}=R^{*}\left(\mathcal{D}_{j,c}^{l},\mathcal{D}_{j, \bar{c}}^{l}\right) \eta_{l,j}=\max_{c\in[C]}\ \eta_{l,j}^{c}=\max_{c\in[C]}\ R^{*}\left( \mathcal{D}_{j,c}^{l},\mathcal{D}_{j,\bar{c}}^{l}\right).\] (4)

**Classwise Unlearning** For classwise unlearning, we aim to identify those components that are responsible for predictions of a selected class or group within the dataset. Thus, we aim to identify _those components that are able to discriminate between the given class, say with label \(c\), and others_. As with Wang et al. [58], we say \(\mathcal{D}_{c}\) is the **Forget Set**, and \(\mathcal{D}_{\bar{c}}\) is the **Remain Set**. As noted in Shah et al. [48], Jia et al. [23], our aim is to minimize the test accuracy of the model on \(\mathcal{D}_{c}\), while maintaining the test accuracy on \(\mathcal{D}_{\bar{c}}\). Thus, to unlearn class \(c\), we edit those components for which the class-\(c\) discriminative ability, \(\eta_{l,j}^{c}\) is low. An interesting point to note is that some components may be discriminative only for class \(c\), which we call _class-selective components_, whereas others may be discriminative for multiple classes. Note that when the dataset has a large number of classes, our experiments indicate that class-specific components generally can't be found.

**Structured Pruning:** Identifying discriminative components (specifically filters in convolutional networks) for structured pruning was first introduced in Murti et al. [39]. Structured pruning involves removing components for a network while maintaining the accuracy of the classifier. Following Murti et al. [39], we aim to _retain components that are discriminative for multiple classes_. Our goal is to remove components from the model while maintaining the model's accuracy on each class conditional \(\mathcal{D}_{c}\). Thus, Definition 2 gives us a means by which we can identify discriminative components based on the worst Bayes error rate for discriminating the class conditional distributions of the given feature map. Filters for which \(\eta_{l,j}\) is small are considered discriminative, as the best possible classifier that can be trained on those features will be highly accurate.

**Assessing the Discriminative Ability:** In general, the Bayes error rate cannot be computed. However, since the TV distance and the Bayes error rate are connected by the identity (3), which states that for two distributions \(\mathbb{P},\mathbb{Q}\), the Bayes risk is given by \(\frac{1}{2}(1-\text{TV}(\mathbb{P},\mathbb{Q}))\), we can reformulate our distributional pruning strategy as identifying those filters that generate features for which class-conditional distributions are well-separated in the Total Variation sense, and prune them. In [39], this was achieved by making the strong assumption that the distributions of the class-conditional feature maps were spherical Gaussian. In the sequel, we propose novel lower bounds on the total variation distance that require no restrictive Gaussianity assumptions, thereby enabling us to effectively identify discriminative components.

## 4 Witness Function-Based Lower Bounds for the Total Variation Distance

In this section, we derive lower bounds on the Total Variation Distance that rely on the moments of a _witness function_, a scalar-valued function whose moments can be used to derive bounds on divergences between distributions. More formally, we write \(f:\mathcal{X}\rightarrow\mathbb{R}\), where \(\mathcal{X}\) is the support of the distribution(s) in question. We then adapt our lower bound to a variety of scenarios, depending on the extent of the information about the distributions available to us. When access to only the first two moments is available, we derive lower bounds on the total variation distance based on the Fisher linear discriminant and the minimax probability machine.

Estimating the Total Variation distance is known to be \(\#\text{P}\) complete [3]. Estimating lower bounds on the TV distance is an active area of research (see Davies et al. [7] and the references within), with a variety of applications from clustering [1, 17] to analyzing neural networks [59]. However, most bounds such as those presented in Davies et al. [7] require prior knowledge about the distributions, and tractable estimation of lower bounds given access to collections of moments or samples, without assumptions on the distributions themselves, remains an open problem.

### Witness Function-based Lower Bounds on the TV Distance

In this section, we propose lower bounds on the TV distance that only requires access to the moments of a _witness function_, as described in Gretton et al. [15], Kubler et al. [25].

**Theorem 1**.: _Let \(\mathbb{P},\mathbb{Q}\) be two probability measures supported on \(\mathcal{X}\subseteq\mathbb{R}^{d}\), and let \(p\) and \(q\) be the corresponding densities. Let \(\mathcal{F}\) be the set of functions with bounded first and second moments defined on \(\mathcal{X}\). Then,_

\[\mathsf{TV}(\mathbb{P},\mathbb{Q})\geq\sup_{f\in\mathcal{F}}\ \frac{\left(\bar{f}_{p}-\bar{f}_{q}\right)^{2}}{2 \left(\bar{f}_{p}^{2}+\bar{f}_{q}^{(2)}\right)}\] (5)

Proof Sketch.: We provide a sketch of the proof. We express the quantity \(f_{p}-f_{q}\) in terms of the densities \(p(X)\) and \(q(X)\). We then isolate the integral of \(|p(x)-q(x)|\). After rearranging terms, we obtain the result. For the full proof, we refer readers to Appendix B. 

### Moment-based Lower Bounds on the TV distance

Motivated by the need to identify discriminative filters when we only have access to the moments of feature map distributions, we propose worst-case lower bounds on the TV distance given access to finitely many moments of the distributions \(\mathbb{P}\) and \(\mathbb{Q}\).

Let \(\mathcal{S}_{k}(\mathbf{P}):=\{\mathbb{P}\ :\ \mathbb{E}_{X\sim\mathbb{P}}[X_{1}^{d_{1}} \cdots X_{n}^{d_{n}}]=\mathbf{P}_{d_{1}\cdots d_{n}},\ \sum_{i}d_{i}\leq k\}\) be the set of probability measures whose moments are given by \(\mathbf{P}\), where \(\mathbb{E}_{X\sim\mathbb{P}}\left[X_{1}^{d_{1}}\cdots X_{n}^{d_{n}}\right]= \mathbf{P}_{d_{1}\cdots d_{n}}\); similarly, let \(\mathcal{S}_{k}(\mathbf{Q})\) be the set of measures whose moments are given by \(\mathbf{Q}\). For any random variable \(X\in\mathbb{R}^{d}\) supported on \(\mathcal{X}\), suppose \(\varphi:\mathbb{R}^{d}\to\mathbb{R}^{n}\) for which there exist functions \(g\) and \(G\) such that \(\mathbb{E}_{X}\left[\varphi(X)\right]=g(\mathbf{P})\) and \(\mathbb{E}\left[\varphi(X)\varphi(X)^{\top}\right]=G(\mathbf{P})\). Given two collections of moments of the same order, we want to measure the worst-case TV separation between _all_ distributions possessing the moments given in \(\mathbf{P}\) and \(\mathbf{Q}\). We define this as

\[D_{\mathsf{TV}}(\mathcal{S}_{k}(\mathbf{P}),\mathcal{S}_{k}(\mathbf{Q}))= \min_{\mathbb{P}\in\mathcal{S}_{k}(\mathbf{P}),\mathbb{Q}\in\mathcal{S}_{k}( \mathbf{Q})}\ \mathsf{TV}(\mathbb{P},\mathbb{Q})\] (6)

For the sake of brevity, we write \(D_{\mathsf{TV}}(\mathcal{S}_{k}(\mathbf{P}),\mathcal{S}_{k}(\mathbf{Q}))=D_{ \mathsf{TV}}(\mathbf{P},\mathbf{Q};k)\).

**Theorem 2**.: _Suppose \(\mathbf{P}\) and \(\mathbf{Q}\) are sets of moments of two probability measures supported on \(\mathcal{X}\). Let \(\varphi(X)\) be a vector of polynomials such that \(\mathbb{E}_{\mathbb{P}}[\varphi(X)]=g(\mathbf{P})\), \(\mathbb{E}_{\mathbb{Q}}[\varphi(X)]=g(\mathbf{Q})\), \(\mathbb{E}_{\mathbb{P}}[\varphi(X)\varphi(X)^{\top}]=G(\mathbf{P})\), and \(\mathbb{E}_{\mathbb{Q}}[\varphi(X)\varphi(X)^{\top}]=G(\mathbf{Q})\), and let \(f=u^{\top}(\varphi(X)-\frac{g(\mathbf{P})+g(\mathbf{Q})}{2})\), be a witness function. Then, for any \(\mathbb{P}\in\mathcal{S}_{k}(\mathbf{P})\), \(\mathbb{Q}\in\mathcal{S}_{k}(\mathbf{Q})\), supported on a set \(\mathcal{X}\subseteq\mathbb{R}^{d}\), we have_

\[D_{\mathsf{TV}}(\mathbf{P},\mathbf{Q};k)\geq S_{\mathsf{TV}}^{ \ast}(\mathbf{P},\mathbf{Q})\left(2+S_{\mathsf{TV}}^{\ast}(\mathbf{P}, \mathbf{Q})\right)^{-1}\geq S_{\mathsf{H}}^{\ast}(\mathbf{P},\mathbf{Q})^{2} \left(\sqrt{2}+S_{\mathsf{H}}^{\ast}(\mathbf{P},\mathbf{Q})\right)^{-2},\text{ where:}\] \[S_{\mathsf{TV}}^{\ast}(\mathbf{P},\mathbf{Q})=(\Delta g)^{\top} (\tilde{G}(\mathbf{P})+\tilde{G}(\mathbf{Q}))^{-1}(\Delta g)\text{ and }S_{\mathsf{H}}^{\ast}(\mathbf{P},\mathbf{Q})=\max_{u}\frac{|u^{\top}(g( \mathbf{P})-g(\mathbf{Q}))|}{\sqrt{u^{\top}G(\mathbf{P})u}+\sqrt{u^{\top}G( \mathbf{Q})u}}\]

_and \(\Delta g=g(\mathbf{P})-g(\mathbf{Q})\) and \(\tilde{G}(\mathbf{P})=G(\mathbf{P})-g(\mathbf{P})g(\mathbf{P})^{\top}\)._

Proof Sketch.: We apply Theorem 1 with the given witness function, and obtain an expression in terms of \(S_{\mathsf{TV}}^{\ast}(\mathbf{P},\mathbf{Q})\). Since the bound holds for any distributions that yield the given moments of the witness function, the statement holds. The full proof is provided in Appendix B. 

Theorem 2 is a worst-case lower bound on the TV distance between distributions with given truncated moment sequences. While we focus our results on the case where \(f(x)=u^{\top}\varphi(x)\), where \(\varphi(x)\) is a vector of monomials, this bound is valid for any choice of \(f\) with bounded first and second moments.

### Computing \(\mathtt{TV}(\mathbb{P},\mathbb{Q})\) from the Lower Bound

In general, the bounds proposed in Theorem 1 are not tight. However, an interesting observation is that the lower bound proposed in Theorem 1 can, in certain cases, be used to compute the Bayes optimal classifier, and thus the true TV distance. Specifically, if the Bayes optimal classifier lies in a given set of functions \(\mathcal{F}\), the bound can be used to compute the Bayes classifier. We state one such case below in Corollary 1.

**Corollary 1**.: _Suppose \(\mathbb{P}\equiv\mathcal{N}(\mu_{p},\Sigma)\) and \(\mathbb{Q}\equiv\mathcal{N}(\mu_{q},\Sigma)\) Let \(f(x;u)=u^{\top}(x-\frac{1}{2}(\mu_{p}-\mu_{q}))\) be a witness function. Then,_

\[u^{\star}=\operatorname*{arg\,max}_{u}\;\frac{(\mathbb{E}_{x\sim\mathbb{P}}[f( x;u)]-\mathbb{E}_{x\sim\mathbb{Q}}[f(x;u)])^{2}}{\mathbb{E}_{x\sim\mathbb{P}}[f(x;u)^{ 2}]+\mathbb{E}_{x\sim\mathbb{Q}}[f(x;u)^{2}]}=\Sigma^{-1}(\mu_{p}-\mu_{q})\]

_is the weight vector of the Bayes optimal classifier \(f_{Bayes}(x)=\operatorname*{sign}\left(u^{\star\top}x+b\right)\), and \(\mathtt{TV}(\mathbb{P},\mathbb{Q})=2\Phi\left(\sqrt{(u^{\star})^{\top}(\mu_{p} -\mu_{q})}/2\right)-1\)._

Proof Sketch.: We find the \(u^{\star}\) that maximizes the the TV lower bound given in Theorem 1, and then apply the same to the exact expression of the TV distance between Gaussian measures with the same covariance. The full proof is provided in Appendix B. 

_Remark 4.1_.: This result illustrates the case where the Bayes' classifier lies in the set of functions \(\mathcal{F}:=\{f(x):f(x)=u^{\top}\varphi(x)\}\) for a given function \(\varphi(x)\). In this case, if \(\varphi(x)=x-\frac{1}{2}(\mu_{p}-\mu_{q})\), and \(\mathbb{P}\) and \(\mathbb{Q}\) are Gaussian with the same variance, the Bayes classifier is equivalent to the Fisher discriminant.

### Connections to Discriminant Based Classifiers

In this section, we exploit the bound stated in Theorem 1 to reveal extensive connections between the total variation distance and discriminant-based linear classifiers, specifically the Fisher Linear Discriminant and the Minimax Probability Machine, that are of independent interest to readers.

Specifically, we show that the TV distance is lower-bounded by monotonic functions of the Fisher Discriminant and the Minimax Probability Machine. We state this result formally in Corollary 2.

**Corollary 2**.: _Let \(\mathbb{P},\mathbb{Q}\) be two probability measures supported on \(X\subseteq\mathbb{R}^{d}\), let \(p\) and \(q\) be the corresponding densities, and let \(\mu_{p}\), \(\mu_{q}\) and \(\Sigma_{p}\), \(\Sigma_{q}\) be the means and variances of \(\mathbb{P}\) and \(\mathbb{Q}\) respectively. Then,_

\[\mathtt{TV}(\mathbb{P},\mathbb{Q})\geq\frac{\mathsf{Fish}^{\star}(\mathbb{P}, \mathbb{Q})}{2+\mathsf{Fish}^{\star}(\mathbb{P},\mathbb{Q})}\geq\left(\frac{ \mathsf{MPM}^{\star}(\mathbb{P},\mathbb{Q})}{\sqrt{2}+\mathsf{MPM}^{\star}( \mathbb{P},\mathbb{Q})}\right)^{2}.\]

Proof.: Choose \(\varphi(x)=x\). Thus, \(g(\mathbf{P})=\mu_{p}\), and \(G(\mathbf{P})=\Sigma_{p}+\mu_{p}\mu_{p}^{\top}\). Substitute these into Theorem 2 to complete the proof. 

This lower bound can be improved upon by selecting a witness function of the form \(f(x;u)=u^{\top}\varphi(x)\) where \(\varphi(x)\) is a vector of basis functions (such as monomials, if \(f(x;u)\) is a polynomial). Moreover, lower bounds that are robust to estimation error, based on the Fisher Discriminant and Minimax Probability Machines, can be derived by directly applying the techniques proposed in [24] (for the Fisher discriminant case) and [28] (for the Minimax Probability Machine case). We discuss this in Section 6.2.

### Lower bounds Robust to Estimation Errors

The lower bounds derived in Theorems 1 and 2 are functions of moments of the distributions, which must typically be estimated from samples. In practice, we use plug-in estimators for \(g(\mathbf{P})\), \(g(\mathbf{Q})\), \(G(\mathbf{P})\) and \(G(\mathbf{Q})\). Since these estimators are computed using samples, errors in estimation arise, which in turn creates errors in computing the lower bound.

This estimation error is a particularly challenging issue in high dimensions, where 'high dimensions' refers to the setting where the number of samples \(n\) is significantly less than the dimension of the data, \(d\). However, in typical neural networks such as VGG-nets and ResNets, the feature maps, particularly of the layers close to the output that can be effectively pruned (see, for instance, [34, 39]), have low dimensional feature maps. For instance, on VGG16 trained on CIFAR10, the feature maps generated by the 5th layer are of dimension 64; thus, for a relatively small number of samples \(n\), the dimension is less than \(n\).

In this section, we present robust lower bounds for the case when \(f(x)=u^{\top}x\), and the moments being estimated are \(\mu_{p}=\mathbb{E}_{\mathbb{P}}[x]\) and \(C_{p}=\mathbb{E}_{\mathbb{P}}[xx^{\top}]\) using plug-in estimators of the form

\[\bar{\mu}_{p}=\frac{1}{N}\sum_{i=1}^{N}x_{i}\ \ \text{ and }\ \ \bar{C}_{p}=\frac{1}{N}\sum_{i=1}^{N}x_{i}x_{i}^{\top}\] (7)

where \(x_{i}\) are drawn iid from \(\mathbb{P}\). We assume that we can quantify the estimation error for the above moments, and can apply lower bounds as proposed in Kim et al. [24] accordingly.

**Theorem 3**.: _Suppose \(\mathbb{P},\mathbb{Q}\) be two probability measures supported on \(X\subseteq\mathbb{R}^{d}\), with densities \(p\) and \(q\), and let \(\mu_{p}=\mathbb{E}_{\mathbb{P}}[x]\), \(\mu_{q}=\mathbb{E}_{\mathbb{Q}}[x]\) and \(C_{p}=\mathbb{E}_{\mathbb{P}}[xx^{\top}]\), \(C_{q}=\mathbb{E}_{\mathbb{Q}}[xx^{\top}]\). Suppose we have plug-in estimates \(\bar{\mu}_{p}\), \(\bar{C}_{p}\), \(\bar{\mu}_{q}\), \(\bar{C}_{q}\) as defined in (7), that satisfy_

\[\|\mu_{p}-\bar{\mu}_{p}\|_{2}\leq\delta_{p}\text{ and }\|\mu_{q}-\bar{\mu}_{q} \|_{2}\leq\delta_{q}.\ \ \|C_{p}-\bar{C}_{p}\|_{F}\leq\rho_{p}\text{ and }\|C_{q}-\bar{C}_{q}\|_{F}\leq\rho_{q}.\]

_Then, with a witness function of the form \(f(x)=u^{\top}x\)_

\[D_{\textsf{TV}}(\mathbb{P},\mathbb{Q};2)\geq\min_{\mu_{p},\mu_{q}\in\mathcal{ M}}\ (\Delta\mu)^{\top}(C_{p}+C_{q}+\rho I)^{-1}(\Delta\mu),\] (8)

_where \(\mathcal{M}=\{(\mu_{p},\mu_{q}):\|\mu_{p}-\bar{\mu}_{p}\|_{2}\leq\delta_{p},\| \mu_{q}-\bar{\mu}_{q}\|_{2}\leq\delta_{q}\}\), \(\Delta\mu=\mu_{p}-\mu_{q}\), and \(\rho=\rho_{p}+\rho_{q}\)._

Proof Sketch.: The proof is similar to the derivation of (15) in Kim et al. [24]. A full proof is provided in Appendix B 

This result can also be applied to the estimation error of functions of \(x\), such as a vector of monomials \(\varphi(x)\), provided the estimation error of each moment can be bounded.

## 5 DisCEdit: Distributional Algorithms for Model Editing

In this section, we leverage the lower bounds proposed in Theorems 1 and 2, and Corollary 2 to develop one-shot algorithms for model editing that require no access to the training data or loss function, but only access to the data distributions. We propose two algorithms, DisCEdit-SP and DisCEdit-U, that identify discriminative components (filters in CNNs), and prunes them to either unlearn a class (DisCEdit-U) or to sparsify the model with minimal loss of accuracy (DisCEdit-SP). A variety of variants of these algorithms based on different witness functions are provided in Appendix D.

### DisCEdit-SP: A Distributional Approach to Structured Pruning

In this section, we propose DisCEdit-SP, an algorithm for structured pruning that identifies filters (in convolutional networks) that are discriminative for multiple classes, and retains them. Unlike the approach proposed in Murti et al. [39], no restrictive assumptions on the Gaussianity of class-conditional feature distributions is needed. Furthermore, by assuming the distributions are Gaussian and using the closed-form Hellinger lower bound, \(\binom{C}{2}\) pairwise TV distances need to be computed for each filter. We now state the DisCEdit-SPalgorithm.

Let \(Y^{l}(X)\) be the features generated by layer \(l\) of a neural network as defined in (1). We choose a witness function \(f=u^{\top}\varphi(Y^{l}_{j}(X))=\varphi^{l}_{j}(X)\), and let \(\bar{f}_{l,j,c}(u)=\mathbb{E}_{X\sim\mathcal{D}_{c}}[u^{\top}\varphi^{l}_{j}(X)]\), \(\bar{f}_{l,j,\bar{c}}(u)=\mathbb{E}_{X\sim\mathcal{D}_{c}}[u^{\top}\varphi^{l} _{j}(X)]\), \(\bar{f}^{(2)}_{l,j,c}(u)=\mathbb{E}_{X\sim\mathcal{D}_{c}}[(u^{\top}\varphi^{l} _{j}(X))^{2}]\) and \(\bar{f}^{(2)}_{l,j,\bar{c}}(u)=\mathbb{E}_{X\sim\mathcal{D}_{c}}[(u^{\top} \varphi^{l}_{j}(X))^{2}\). Next, define \(r^{l}_{j}\) to be the saliency score for the \(j\)th filter in the \(l\)th layer as

\[r^{l}_{j}=\min_{c\in[C]}\ \max_{u}\ \left(\bar{f}_{l,j,c}(u)-\bar{f}_{l,j,\bar{c} }(u)\right)^{2}\left(\bar{f}^{(2)}_{l,j,c}(u)+\bar{f}^{(2)}_{l,j,\bar{c}}(u) \right)^{-1}.\] (9)We use the lower bound established in Theorem 1 on the TV distances between the class conditional distributions to measure the _saliency_ or importance of a given filter. With this, we formally state DisCEdit-SP in Algorithm 1.

The DisCEdit-SP algorithm has several advantages. First, as compared to TVSPrune, it requires that only \(C\) TV distances be computed at each step. Second, by varying the choice of witness function, we obtain new algorithms for structured pruning; we can choose different witness functions for each class as well.

### DisCEdit-U:

A Distributional Approach to Machine Unlearning

We now state an algorithm for classwise unlearning based on model editing, called DisCEdit-U. Motivated by works such as Shah et al. [48], our algorithm requires identifying and editing (specifically pruning) class-selective components for the class which is to be unlearned. DisCEdit-Uuses the same setup as DisCEdit-SP. However, DisCEdit-Uonly requires identifying discriminative filters for a single class, we have

\[r_{j}^{l}=\max_{u}\ \left(\bar{f}_{l,j,c}(u)-\bar{f}_{l,j,\bar{c}}(u) \right)^{2}\left(\bar{f}_{l,j,c}^{(2)}(u)+\bar{f}_{l,j,\bar{c}}^{(2)}(u)\right) ^{-1}.\] (10)

We formally state DisCEdit-U in Algorithm 1. The DisCEdit-U algorithm has several advantages. As we show in our experiments in Appendix E, few samples are required to effectively compute the witness functions.

## 6 Empirical Evaluations

In this section, we empirically study the effectiveness of identifying discriminative filters for model editing tasks, specifically structured pruning and class unlearning. Additional experimental details are given in Appendix E. Experiments showing that class-conditional feature map distributions are non-gaussian, the effectiveness of variants of witness functions, the effectiveness of sparsity in class forgetting, and other ablations, are provided in Appendix E. Our experiment setup is provided in Appendix F, along with baseline accuracies of all models, shown in Table 11.

### Identifying Discriminative Subnetworks and Class Unlearning with DisCEdit-U

In this section, we investigate the utility of the lower bounds given in Theorems 1-3 in identifying discriminative subnetworks and for class unlearning.

**Experiment Setup:** We investigate VGG16, Resnet56, ResNet20, and a custom ViT (details given in Appendix F) trained on CIFAR10, and VGG16 and ResNet56 models trained on CIFAR100,

\begin{table}
\begin{tabular}{l l|c c c|c c c|c c}  & \multicolumn{4}{c}{**Our Work**} & \multicolumn{4}{c}{**Baselines**} \\ Dataset & Model & FA (NoFT) & RA (NoFT) & FA (FT) & FA (GA) & BA (GA) [23] & RA (RA) [23] & RA (IU) [23] \\ \hline \multirow{3}{*}{CIFAR-10} & VGG-16 & **8.7\%** & 82.5\% & **3.7\%** & 91.6\% & 22.5\% & 88.8\% & 11.42\% & 89.8\% \\  & ResNet56 & 16.3\% & 85.9\% & 9.7\% & 89.4\% & - & - & - & - \\  & ResNet20 & **9.4\%** & 33.9\% & **6.0\%** & 86.6\% & 11.52\% & 85.46\% & - & - \\  & ViT & 16.5\% & 66.3\% & 11.0\% & 71.2\% & - & - & - & - \\ \hline \multirow{3}{*}{CIFAR-100} & VGG16 & 11.3\% & 68.0\% & 10.7\% & 72.9\% & - & - & - & - \\  & ResNet56 & 31.1\% & 60.4\% & 17.9\% & 66.7\% & - & - & - & - \\ \cline{1-1}  & ViT & 13.1\% & 44.2\% & 14.6\% & 61.0\% & - & - & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: A summary of results of class unlearning using DisCEdit-U. We take the average over the Forget and Remain accuracies (FA and RA) after applying DisCEdit-Uto each class. Note that FA=accuracy drop on forget class, RA=accuracy drop on remain set. Values are averaged over 10 trials. NoFT refers to using DisCEdit-U without fine-tuning, and with pruning only 5.4% of weights for VGG16, 1.8% of weights for ResNet56, 1.8% of weights for ResNet56, whereas FF refers to using DisCEdit-U with 1 epoch of fine-tuning, and with pruning ratios of 18.4%, 225, 16.6%, and 10.2% for VGG16, ResNet56, ResNet20, and our ViT respectively and identify subnetworks responsible for predictions from each class. We use the witness function \(f(x)=\exp(\|x\|^{2})\) in our experiments, using 256 samples from the Forget class, and 1024 samples for the Remain class, from the training sets of CIFAR10 and CIFAR100's, for computing \(r_{l,j}^{c}\) for each filter. For VGG-16, we only consider the last 8 layers, and for ResNet56, we only consider the final layerblock. We then select the sparsity budgets \(B_{l}\), and prune the most discriminative filters for that layer. We also fine-tune the models for 1 epoch. We measure the accuracy on the class test set, and the class complement test sets both before and after fine-tuning, and we compare against models trained from scratch on the retain set (the class complement).

**Results:** We present a summary of our results in Table 1. In particular, we highlight that on CIFAR10 models, particularly VGG16, the performance is comparable to or outperforms baselines with minimal fine-tuning. There are two interesting observations: first, as the number of classes exceeds the width of the network (as was the case with ResNet56 trained on CIFAR100), the efficacy of our method is drastically affected. Second, fine-tuning models on the remain set still raises the accuracy on the forget set unless dramatically more filters are pruned. The reasons for this will be the focus of future work.

This set of experiments highlights the fact that for classifier models, it is possible to identify subnetworks responsible for predictions for each class. Identifying these subnetworks then facilitates classwise unlearning.

### Structured Pruning with DisCEdit-SP with Fine-Tuning

In this section, we investigate the ability for DisCEdit-SP to sparsify models effectively both with and without fine-tuning.

**Experiment Setup:** We prune VGG16, VGG19, and ResNet56 models trained on CIFAR10 with two sets of fixed sparsity budgets, and then fine-tune them for 50 epochs. We repeat the experiment for a ResNet50 trained on Imagenet, and fine-tune the pruned models for 100 epochs. We choose \(f(x)=u^{\top}\varphi(x)\), where \(\varphi(x)=[1^{\top}x,(1^{\top}x)^{2}]^{\top}\) as our witness function in each of the experiments. For details about the pre-trained models used, refer to Appendix F.

**Results:** We show that models pruned with DisCEdit-SP without fine-tuning retain high accuracies, particularly on the CIFAR10 dataset. Moreover, after fine-tuning, DisCEdit-SPis able to almost fully recover the accuracy of the original models. Our results are summarized in Table 2, which shows the drop in accuracy achieved by the different pruning algorithms compared after fine-tuning.

## 7 Conclusions

In this work, we address the problem of model editing by analyzing discriminative properties of the feature maps. We leverage the notion of discriminative components to derive algorithms for two relevant tasks: structured pruning and class unlearning. Additionally, in order to identify discriminative components, we derive new lower bounds on the TV distance. These lower bounds also elucidate previously unknown connections between the Total Variation distance and discriminative classifiers, specifically the Fisher discriminant and the Minimax Probability Machine. Our experiments show that the model editing algorithms derived by our methods are highly effective, and match or outperform current state of the art in structured pruning, and can reduce accuracy almost completely on a given class, while maintaining accuracy on the remaining classes. This work, however, currently analyzes discriminative components (and thus, subnetworks responsible for classwise predictions) in classifier models. Current avenues of research include extending these results to unlearning and pruning of generative models as well. Lastly, the techniques proposed in this work can be extended to other editing tasks as well, such as debiasing.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{4}{c}{**CIFAR-10**} \\ \hline Model & Sparsity & our work & TVSPune & CHIP & L1 \\ \hline VGG16 & 61.2\% & **-0.37\%** & -0.98\% & -0.73\% & 1.26\% \\  & 75.05\% & **-1.32\%** & -1.54\% & -1.62 & -2.31 \\ \hline VGG19 & 72.4\% & **-0.12\%** & -0.16\% & N/A & -2.41\% \\  & 76.1\% & **-0.36\%** & -1.13\% & N/A & -3.30\% \\ \hline ResNet56 & 60.7\% & **-1.21\%** & -1.92\% & -1.77\% & -6.21\% \\ \hline \multicolumn{5}{l}{**ImageNet**} \\ \hline Model & Sparsity & our work & TVSPune & CHIP & L1 \\ \hline ResNet50 & 20.2\% & **+0.12\%** & -0.4\% & **+0.10\%** & -1.06\% \\  & 40.8\% & **-2.58\%** & -2.74\% & -2.76\% & -4.45\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: DisCEdit-SP performance on CIFAR10 and ImageNet models with fine-tuning. TVSPune refers to [39], and CHIP refers to [52]. Sparsity’ refers to parametric sparsity.

## Acknowledgments

The authors gratefully thank **Shell India Markets Pvt Ltd**, for their generous support and contributions to this work.

## References

* Bakshi and Kothari [2020] A. Bakshi and P. Kothari. Outlier-robust clustering of non-spherical mixtures. _arXiv preprint arXiv:2005.02970_, 2020.
* Bau et al. [2017] D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba. Network dissection: Quantifying interpretability of deep visual representations. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6541-6549, 2017.
* Bhattacharyya et al. [2022] A. Bhattacharyya, S. Gayen, K. S. Meel, D. Myrrisiotis, A. Pavan, and N. Vinodchandran. On approximating total variation distance. _arXiv preprint arXiv:2206.07209_, 2022.
* Blalock et al. [2020] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag. What is the state of neural network pruning? _Proceedings of machine learning and systems_, 2:129-146, 2020.
* Bourtoule et al. [2021] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot. Machine unlearning. In _2021 IEEE Symposium on Security and Privacy (SP)_, pages 141-159. IEEE, 2021.
* Cheng et al. [2023] H. Cheng, M. Zhang, and J. Q. Shi. A survey on deep neural network pruning-taxonomy, comparison, analysis, and recommendations. _arXiv preprint arXiv:2308.06767_, 2023.
* Davies et al. [2022] S. Davies, A. Mazumdar, S. Pal, and C. Rashtchian. Lower bounds on the total variation distance between mixtures of two gaussians. In _International Conference on Algorithmic Learning Theory_, pages 319-341. PMLR, 2022.
* Devroye et al. [2013] L. Devroye, L. Gyorfi, and G. Lugosi. _A probabilistic theory of pattern recognition_, volume 31. Springer Science & Business Media, 2013.
* Duda et al. [2006] R. O. Duda, P. E. Hart, et al. _Pattern classification_. John Wiley & Sons, 2006.
* Eldan and Russinovich [2023] R. Eldan and M. Russinovich. Who's harry potter? approximate unlearning in lms. _arXiv preprint arXiv:2310.02238_, 2023.
* Frankle and Carbin [2018] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In _International Conference on Learning Representations_, 2018.
* Frankle and Carbin [2019] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=rJl-b3RcF7.
* Gandikota et al. [2023] R. Gandikota, J. Materzynska, J. Fietto-Kaufman, and D. Bau. Erasing concepts from diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2426-2436, 2023.
* Golatkar et al. [2020] A. Golatkar, A. Achille, and S. Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9304-9312, 2020.
* Gretton et al. [2012] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scholkopf, and A. Smola. A kernel two-sample test. _The Journal of Machine Learning Research_, 13(1):723-773, 2012.
* Gupta et al. [2021] V. Gupta, C. Jung, S. Neel, A. Roth, S. Sharifi-Malvajerdi, and C. Waites. Adaptive machine unlearning. _Advances in Neural Information Processing Systems_, 34:16319-16330, 2021.
* Hardt and Price [2015] M. Hardt and E. Price. Tight bounds for learning a mixture of two gaussians. In _Proceedings of the forty-seventh annual ACM symposium on Theory of computing_, pages 753-760, 2015.
* Hassibi and Stork [1992] B. Hassibi and D. Stork. Second order derivatives for network pruning: Optimal brain surgeon. _Advances in neural information processing systems_, 5, 1992.
* Hoefler et al. [2021] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. _Journal of Machine Learning Research_, 22(241):1-124, 2021.
* Hou and Kung [2020] Z. Hou and S.-Y. Kung. A feature-map discriminant perspective for pruning deep neural networks. _arXiv preprint arXiv:2005.13796_, 2020.

* Hou and Kung [2021] Z. Hou and S.-Y. Kung. A discriminant information approach to deep neural network pruning. In _2020 25th International Conference on Pattern Recognition (ICPR)_, pages 9553-9560. IEEE, 2021.
* Izzo et al. [2021] Z. Izzo, M. A. Smart, K. Chaudhuri, and J. Zou. Approximate data deletion from machine learning models. In _International Conference on Artificial Intelligence and Statistics_, pages 2008-2016. PMLR, 2021.
* Jia et al. [2023] J. Jia, J. Liu, P. Ram, Y. Yao, G. Liu, Y. Liu, P. Sharma, and S. Liu. Model sparsity can simplify machine unlearning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=0jZH883i34.
* Kim et al. [2005] S.-J. Kim, A. Magnani, and S. Boyd. Robust fisher discriminant analysis. _Advances in neural information processing systems_, 18, 2005.
* Kubler et al. [2022] J. M. Kubler, W. Jitkrittum, B. Scholkopf, and K. Muandet. A witness two-sample test. In _International Conference on Artificial Intelligence and Statistics_, pages 1403-1419. PMLR, 2022.
* Kurmanji et al. [2024] M. Kurmanji, P. Triantafillou, J. Hayes, and E. Triantafillou. Towards unbounded machine unlearning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Lanckriet et al. [2001] G. Lanckriet, L. Ghaoui, C. Bhattacharyya, and M. Jordan. Minimax probability machine. _Advances in neural information processing systems_, 14, 2001.
* Lanckriet et al. [2002] G. R. Lanckriet, L. E. Ghaoui, C. Bhattacharyya, and M. I. Jordan. A robust minimax approach to classification. _Journal of Machine Learning Research_, 3(Dec):555-582, 2002.
* LeCun et al. [1989] Y. LeCun, J. Denker, and S. Solla. Optimal brain damage. _Advances in neural information processing systems_, 2, 1989.
* Li et al. [2020] B. Li, B. Wu, J. Su, and G. Wang. Eagleeye: Fast sub-net evaluation for efficient neural network pruning. In _European conference on computer vision_, pages 639-654. Springer, 2020.
* Li et al. [2016] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. _arXiv preprint arXiv:1608.08710_, 2016.
* Li et al. [2017] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=rJqFGTSlg.
* Li et al. [2023] J. Li, R. Wang, Y. Lai, C. Shui, S. Sahoo, C. X. Ling, S. Yang, B. Wang, C. Gagne, and F. Zhou. Hessian aware low-rank weight perturbation for continual learning. _arXiv preprint arXiv:2311.15161_, 2023.
* Liebenwein et al. [2019] L. Liebenwein, C. Baykal, H. Lang, D. Feldman, and D. Rus. Provable filter pruning for efficient neural networks. In _International Conference on Learning Representations_, 2019.
* Lin et al. [2020] M. Lin, R. Ji, Y. Wang, Y. Zhang, B. Zhang, Y. Tian, and L. Shao. Hrank: Filter pruning using high-rank feature map. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1529-1538, 2020.
* Liu et al. [2021] L. Liu, S. Zhang, Z. Kuang, A. Zhou, J.-H. Xue, X. Wang, Y. Chen, W. Yang, Q. Liao, and W. Zhang. Group fisher pruning for practical network compression. In _International Conference on Machine Learning_, pages 7021-7032. PMLR, 2021.
* Molchanov et al. [2019] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz. Importance estimation for neural network pruning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11264-11272, 2019.
* Molchanov et al. [2019] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz. Pruning convolutional neural networks for resource efficient inference. In _5th International Conference on Learning Representations, ICLR 2017-Conference Track Proceedings_, 2019.
* Murti et al. [2022] C. Murti, T. Narshana, and C. Bhattacharyya. Tvsprune-pruning non-discriminative filters via total variation separability of intermediate representations without fine tuning. In _The Eleventh International Conference on Learning Representations_, 2022.
* Mussay et al. [2021] B. Mussay, D. Feldman, S. Zhou, V. Braverman, and M. Osadchy. Data-independent structured pruning of neural networks via coresets. _IEEE Transactions on Neural Networks and Learning Systems_, 2021.

* [41] T. Narshana, C. Murti, and C. Bhattacharyya. Dfpc: Data flow driven pruning of coupled channels without data. In _The Eleventh International Conference on Learning Representations_, 2022.
* [42] T. T. Nguyen, T. T. Huynh, P. L. Nguyen, A. W.-C. Liew, H. Yin, and Q. V. H. Nguyen. A survey of machine unlearning. _arXiv preprint arXiv:2209.02299_, 2022.
* [43] P. Prakash, C. Murti, S. Nath, and C. Bhattacharyya. Optimizing dnn architectures for high speed autonomous navigation in gps denied environments on edge devices. In _Pacific Rim International Conference on Artificial Intelligence_, pages 468-481. Springer, 2019.
* [44] T. R\(\ddot{\text{a}}\)uker, A. Ho, S. Casper, and D. Hadfield-Menell. Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. In _2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)_, pages 464-483. IEEE, 2023.
* [45] B. D. Ripley. _Pattern recognition and neural networks_. Cambridge university press, 2007.
* [46] S. Sahoo, M. Elaraby, J. Ngnawe, Y. Pequignot, F. Precioso, and C. Gagne. Layerwise early stopping for test time adaptation. _arXiv preprint arXiv:2404.03784_, 2024.
* [47] A. Sekhari, J. Acharya, G. Kamath, and A. T. Suresh. Remember what you want to forget: Algorithms for machine unlearning. _Advances in Neural Information Processing Systems_, 34:18075-18086, 2021.
* [48] H. Shah, A. Ilyas, and A. Madry. Decomposing and editing predictions by modeling model computation. _arXiv preprint arXiv:2404.11534_, 2024.
* [49] S. S. Shapiro and M. B. Wilk. An analysis of variance test for normality (complete samples). _Biometrika_, 52(3/4):591-611, 1965.
* [50] M. Shen, H. Yin, P. Molchanov, L. Mao, J. Liu, and J. M. Alvarez. Structural pruning via latency-saliency knapsack. _Advances in Neural Information Processing Systems_, 35:12894-12908, 2022.
* [51] P. Singh, V. K. Verma, P. Rai, and V. P. Namboodiri. Play and prune: Adaptive filter pruning for deep model compression. _arXiv preprint arXiv:1905.04446_, 2019.
* [52] Y. Sui, M. Yin, Y. Xie, H. Phan, S. Aliari Zonouz, and B. Yuan. Chip: Channel independence-based pruning for compact neural networks. _Advances in Neural Information Processing Systems_, 34, 2021.
* [53] M. Tukan, L. Mualem, and A. Maalouf. Pruning neural networks via coresets and convex geometry: Towards no assumptions. _Advances in Neural Information Processing Systems_, 35:38003-38019, 2022.
* [54] C. Wang, G. Zhang, and R. Grosse. Picking winning tickets before training by preserving gradient flow. _arXiv preprint arXiv:2002.07376_, 2020.
* [55] H. Wang, C. Qin, Y. Zhang, and Y. Fu. Neural pruning via growing regularization. _arXiv preprint arXiv:2012.09243_, 2020.
* [56] J. Wang, S. Guo, X. Xie, and H. Qi. Federated unlearning via class-discriminative pruning. In _Proceedings of the ACM Web Conference 2022_, pages 622-632, 2022.
* [57] L. Wang, X. Zhang, H. Su, and J. Zhu. A comprehensive survey of continual learning: Theory, method and application. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2024.
* [58] Z. Wang, E. Yang, L. Shen, and H. Huang. A comprehensive survey of forgetting in deep learning beyond continual learning. _arXiv preprint arXiv:2307.09218_, 2023.
* [59] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V. I. Morariu, X. Han, M. Gao, C.-Y. Lin, and L. S. Davis. Nisp: Pruning networks using neuron importance score propagation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 9194-9203, 2018.
* [60] S. Yu, Z. Yao, A. Gholami, Z. Dong, S. Kim, M. W. Mahoney, and K. Keutzer. Hessian-aware pruning and optimal neural implant. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 3880-3891, 2022.

## Appendix

In this appendix, we provide the following material.

* In Appendix A, we provide a brief survey of the literature that addresses both structured pruning.
* In Appendix B, we provide proofs for the theoretical results proposed in this work.
* In Appendix C, we provide additional analysis of the practical performance of our algorithm.
* In Appendix D, we provide details for variants of the DisCEdit-SP algorithm, such as the DisCEdit-SP-F, DisCEdit-SP-M, DisCEdit-SP-R, and DisCEdit-SP-E variants. We also discuss the use of the BatchNorm random variables as opposed to the entire feature map, in order to reduce memory storage, as well as deriving TVSPrune [39] using our results.
* In Appendix E, we provide additional experiments, including more detailed results from experiments conducted in Section 6.
* In Appendix F, we provide additional details, such as hyperparameters and dataset splits, used in this work.

## Appendix A Related Work

### Structured Pruning

In this section, we detail related work in the space of structured pruning Pruning is a long-standing strategy for reducing the inference time and power consumption of neural networks [29, 18]. Pruning algorithms can be divided into structured - wherein entire filters or neurons are removed - or unstructured - wherein individual weights are removed - techniques [19, 4]. We refer readers to [19, 4, 11, 12], and the references therein, for a detailed discussion on unstructured pruning methods.

#### a.1.1 Structured Pruning that Requires the Loss Function

Structured pruning methods are of interest as they enable the reduction of inference times without requiring specialized hardware or software [19, 38, 41]. Pruning algorithms require saliency measures that quantify the importance of filters or neurons, and prune the unimportant ones [19]. Pruning algorithms where access to the loss function is not only assumed, but is necessary, generate saliencies using gradient information [38, 37, 43, 50], and the loss function Hessians, or approximations thereof [29, 18, 36, 60, 55, 54]. For more similar works, we refer readers to the surveys [19, 4, 6].

#### a.1.2 Structured Pruning without the Loss Function

In this section, we discuss structured pruning methods that do not require derivatives of the loss function, aside from fine-tuning. There are a variety of approaches to solve the problem of identifying which filters to prune without using the loss function. Works such as [31, 32] use the norms of the weights to identify filters to prune. Other works, such as [41, 59], identify which filters to prune by bounding the reconstruction error incurred by pruning a given filter. Works such as [35, 52] analyze the rank of feature maps to measure filter importance. Another body of work uses coresets of feature maps to identify subsets of important filters, as described in works such as [34, 53, 40]. Recently, some works propose identifying important filters by their discriminative ability. Recently, in [20, 21], discrimination-aware metrics were used to identify filters important to classification accuracy. In [39], the class conditional distributions were assumed to be Gaussian, and the Hellinger distance between the class conditional feature maps was used to identify important filters. In this work, we formalize the notion of discriminative ability in terms of the Bayes risk, and derive novel lower bounds on the TV distance to effectively approximate it. Furthermore, we emphasize that this work is clearly differentiated from prior art in this area in these critical respects: first, no assumptions are required of the class conditional distributions; second, the discriminative ability of a filter is formalized in terms of the Bayes error of a classifier trained on the feature maps it yields; third, the pruning algorithms we derive using our method require no access to the training data or loss function.

### Machine Unlearning

In this section, we provide a detailed literature survey on machine unlearning, both with and without model editing.

#### a.2.1 Machine Unlearning without Model Editing

Machine unlearning has gained importance in recent years owing to data privacy and security concerns [5; 42]. A wide variety of works exist to address this problem. Several works aim to forget data points, even in the adaptive setting, while maintaining the accuracy of the model, such as [47; 16; 22; 14]. The work in [47] also provides bounds on the number of samples that a model can be allowed to forget before accuracy degradation. Machine unlearning is also a significant area of research in the space of large language models, as noted in [26; 10], and generative models [13].

Another aspect of machine unlearning is selective forgetting, wherein classes, groups, or sets of samples are forgotten from the network, as described in [58] and the references therein. This connects machine unlearning to the continual learning setting as well, as described in [57] and the references cited there.

#### a.2.2 Machine Unlearning with Model Editing

While there has been significant research into selective forgetting and model unlearning, facilitating selective forgetting and machine unlearning by _editing_ models remains an underexplored field. Recent works such as [23; 46] explore the effect of sparsity on machine unlearning and continual learning; in particular, [23] shows that sparsifying models prior to unlearning can increase the effectiveness of it. However, recent work such as [56] investigates using pruning for model unlearning in the federated setting. More recently, [48] uses pruning to selectively forget a single class from a ResNet50 imagenet model with minimal loss in accuracy. Our work differs from prior work in this space since we directly use the discriminative ability of model components to identify which components to remove to forget a given class.

## Appendix B Proofs of Main Results

In this section, we provide the proofs for the main theoretical results proposed in the paper. Specifically, we provide proofs for Theorems 1-3 and Corollary 1.

### Proof of Theorem 1

In this section, we provide the proofs for Theorem 1.

**Theorem**.: _Let \(\mathbb{P},\mathbb{Q}\) be two probability measures supported on \(X\subseteq\mathbb{R}^{d}\), and let \(p\) and \(q\) be the corresponding densities. Let \(\mathcal{F}\) be the set of functions with bounded first and second moments defined on \(X\). Then,_

\[\mathtt{TV}(\mathbb{P},\mathbb{Q})\geq\sup_{f\in\mathcal{F}}\ \frac{\left(\bar{f}_{p}-\bar{f}_{q}\right)^{2}}{2 \left(\bar{f}_{p}^{(2)}+\bar{f}_{q}^{(2)}\right)}\] (11)Proof.: Choose an arbitrary \(f\in\mathcal{F}\). Then, we have

\[\left(\bar{f}_{p}-\bar{f}_{q}\right)^{2} =\left(\int_{\mathcal{X}}\left(p(x)-q(x)\right)f(x)dx\right)^{2}\] \[=\left(\int_{\mathcal{X}}\left(\sqrt{|p(x)-q(x)|}\sqrt{|p(x)-q(x )|}\right)f(x)dx\right)^{2}\] \[\leq\left(\sqrt{\int_{\mathcal{X}}|p(x)-q(x)|dx}\right)^{2}\left( \sqrt{\int_{\mathcal{X}}|p(x)-q(x)|f(x)^{2}}dx\right)^{2}\quad\text{(by Cauchy-Schwarz)}\] \[=2\mathtt{TV}(\mathbb{P},\mathbb{Q})\left(\int_{\mathcal{X}}|p(x )-q(x)|f(x)^{2}dx\right)\quad\text{(by Definition \ref{eq:def_def_def})}\] \[\leq 2\mathtt{TV}(\mathbb{P},\mathbb{Q})\left(\int_{\mathcal{X}} \left(p(x)+q(x)\right)f(x)^{2}dx\right)\] \[=2\mathtt{TV}(\mathbb{P},\mathbb{Q})\left(\bar{f}_{p}^{(2)}+\bar {f}_{q}^{(2)}\right).\]

Thus, for any arbitrary \(f\in\mathcal{F}\), we have

\[2\mathtt{TV}(\mathbb{P},\mathbb{Q})\geq\frac{\left(\bar{f}_{p}-\bar{f}_{q} \right)^{2}}{\left(\bar{f}_{p}^{(2)}+\bar{f}_{q}^{(2)}\right)},\]

from which it follows that

\[2\mathtt{TV}(\mathbb{P},\mathbb{Q})\geq\sup_{f\in\mathcal{F}}\frac{\left( \bar{f}_{p}-\bar{f}_{q}\right)^{2}}{\left(\bar{f}_{p}^{(2)}+\bar{f}_{q}^{(2)} \right)}.\] (12)

The proof of Theorem 1 follows from the fact that we can choose \(f(x)=u^{\top}\varphi(x)\), and apply the formula for the expectation. Then, maximizing over \(\mathcal{F}\) is equivalent to maximizing over \(u\). Thus, the proof is completed.

### Proof of Theorem 2

First, we restate the Theorem for clarity.

**Theorem**.: _Suppose \(\mathbf{P}\) and \(\mathbf{Q}\) are sets of moments of two probability measures supported on \(\mathcal{X}\). Let \(\varphi(X)\) be a vector of polynomials such that \(\mathbb{E}_{\mathbb{P}}[\varphi(X)]=g(\mathbf{P})\), \(\mathbb{E}_{\mathbb{Q}}[\varphi(X)]=g(\mathbf{Q})\), \(\mathbb{E}_{\mathbb{P}}[\varphi(X)\varphi(X)^{\top}]=G(\mathbf{P})\), and \(\mathbb{E}_{\mathbb{Q}}[\varphi(X)\varphi(X)^{\top}]=G(\mathbf{Q})\), and let \(f=u^{\top}(\varphi(X)-\frac{g(\mathbf{P})-g(\mathbf{Q})}{2})\), be a witness function. Then, for any \(\mathbb{P}\in\mathcal{S}_{k}(\mathbf{P})\), \(\mathbb{Q}\in\mathcal{S}_{k}(\mathbf{Q})\), supported on a set \(\mathcal{X}\subseteq\mathbb{R}^{d}\), we have_

\[D_{\mathtt{TV}}(\mathbf{P},\mathbf{Q};k)\geq\frac{S^{*}_{\mathtt{TV}}(\mathbf{ P},\mathbf{Q})}{2+S^{*}_{\mathtt{TV}}(\mathbf{P},\mathbf{Q})}\] (13)

_where_

\[S^{*}_{\mathtt{TV}}(\mathbf{P},\mathbf{Q})=(\Delta g)^{\top}(\tilde{G}( \mathbf{P})+\tilde{G}(\mathbf{Q}))^{-1}(\Delta g)\] (14)

_and \(\Delta g=g(\mathbf{P})-g(\mathbf{Q})\) and \(\tilde{G}(\mathbf{P})=G(\mathbf{P})-g(\mathbf{P})g(\mathbf{P})^{\top}\)._

Proof.: First, recall that Theorem 1 provides lower bounds for _all_ distributions for which the moments of the witness function are given by \(\bar{f}_{p}\), \(\bar{f}_{p}^{(2)}\), \(\bar{f}_{q}\), \(\bar{f}_{q}^{(2)}\). Begin by choosing

\[f=u^{\top}\left(\varphi(x)-\frac{g(\mathbf{P})+g(\mathbf{Q})}{2}\right),\]

where \(u\in\mathbb{R}^{d}\) is constant. Then,

\[\bar{f}_{p}=\frac{1}{2}u^{\top}(g(\mathbf{P})-g(\mathbf{Q}))\quad\text{and} \quad\bar{f}_{q}=\frac{1}{2}u^{\top}(g(\mathbf{Q})-g(\mathbf{P})),\]\[\bar{f}_{p}^{(2)} =u^{\top}\left(\mathbb{E}_{\mathbb{P}}\left[\left(\varphi(x)-\frac{g (\mathbf{P})+g(\mathbf{Q})}{2}\right)\left(\varphi(x)-\frac{g(\mathbf{P})+g( \mathbf{Q})}{2}\right)^{\top}\right]\right)u\] \[=u^{\top}\left(\mathbb{E}_{\mathbb{P}}\left[\left(\varphi(x)- \Delta\right)\left(\varphi(x)-\Delta\right)^{\top}\right]\right)u\quad\text{( setting $\Delta=\frac{g(\mathbf{P})+g(\mathbf{Q})}{2}$)}\] \[=u^{\top}\left(\mathbb{E}_{\mathbb{P}}\left[\left(\varphi(x)-g( \mathbf{P})+g(\mathbf{P})-\Delta\right)\left(\varphi(x)-g(\mathbf{P})+g( \mathbf{P})-\Delta\right)^{\top}\right]\right)u\] \[=u^{\top}\left(G(\mathbf{P})-g(\mathbf{P})g(\mathbf{P})^{\top}+ \frac{1}{4}(g(\mathbf{P})-g(\mathbf{Q}))(g(\mathbf{P})-g(\mathbf{Q}))^{\top} \right)u.\]

Similarly, we have

\[\bar{f}_{q}^{(2)}=u^{\top}\left(G(\mathbf{Q})-g(\mathbf{Q})g(\mathbf{Q})^{ \top}+\frac{1}{4}(g(\mathbf{P})-g(\mathbf{Q}))(g(\mathbf{P})-g(\mathbf{Q}))^{ \top}\right)u.\]

Substituting this into (11), we get

\[\mathtt{TV}(\mathbb{P},\mathbb{Q}) \geq\sup_{u\in\mathbb{R}^{d}}\ \frac{1}{2}\frac{\left(u^{\top}(g(\mathbf{P})-g(\mathbf{Q}))\right)^{2}}{u^{ \top}(G(\mathbf{P})-g(\mathbf{P})g(\mathbf{P})^{\top}+G(\mathbf{Q})-g(\mathbf{ Q})g(\mathbf{Q})^{\top})u+\frac{1}{2}\left(u^{\top}(g(\mathbf{P})-g(\mathbf{Q})) \right)^{2}}\] \[=\sup_{u\in\mathbb{R}^{d}}\ \frac{\frac{\left(u^{\top}(g(\mathbf{P})-g( \mathbf{Q}))\right)^{2}}{2u^{\top}(G(\mathbf{P})-g(\mathbf{P})g(\mathbf{P})^{ \top}+G(\mathbf{Q})-g(\mathbf{Q})g(\mathbf{Q})^{\top})u+\left(u^{\top}(g( \mathbf{P})-g(\mathbf{Q}))\right)^{2}}}{\frac{\left(u^{\top}(g(\mathbf{P})-g( \mathbf{Q}))\right)^{2}}{u^{\top}(G(\mathbf{P})+G(\mathbf{Q}))u}}\] \[=\sup_{u\in\mathbb{R}^{d}}\ \frac{\frac{\left(u^{\top}(g(\mathbf{P})-g( \mathbf{Q}))\right)^{2}}{u^{\top}(G(\mathbf{P})-g(\mathbf{Q}))^{2}}}{2+\frac{ \left(u^{\top}(g(\mathbf{P})-g(\mathbf{Q}))\right)^{2}}{u^{\top}(G(\mathbf{P} )+G(\mathbf{Q}))u}}\] \[=\sup_{u\in\mathbb{R}^{d}}\ \frac{\frac{\left(u^{\top}(g(\mathbf{P})-g( \mathbf{Q}))\right)^{2}}{u^{\top}(G(\mathbf{P})-g(\mathbf{Q}))^{2}}}{2+\frac{ \left(u^{\top}(g(\mathbf{P})-g(\mathbf{Q}))\right)^{2}}{u^{\top}(G(\mathbf{P} )+G(\mathbf{Q}))u}}\]

Let

\[S_{\mathtt{TV}}(\mathbf{P},\mathbf{Q};u)=\frac{\left(u^{\top}(g(\mathbf{P})-g (\mathbf{Q}))\right)^{2}}{u^{\top}(\tilde{G}(\mathbf{P})+\tilde{G}(\mathbf{Q} ))u}.\]

Then,

\[S_{\mathtt{TV}}^{*}=\max_{u}S_{\mathtt{TV}}(\mathbf{P},\mathbf{Q};u)=\frac{ \left(u^{\top}(g(\mathbf{P})-g(\mathbf{Q}))\right)^{2}}{u^{\top}(\tilde{G}( \mathbf{P})+\tilde{G}(\mathbf{Q}))u}=(\Delta g)^{\top}(\tilde{G}(\mathbf{P})+ \tilde{G}(\mathbf{Q}))^{-1}(\Delta g)\]

Thus, we have

\[\mathtt{TV}(\mathbb{P},\mathbb{Q})\geq\frac{S^{*}(\mathbf{P},\mathbf{Q})}{2+S ^{*}(\mathbf{P},\mathbf{Q})}\] (15)

To show the second inequality, we first state the following Lemma.

**Lemma 1**.: _Suppose \(\mathbf{P}\) and \(\mathbf{Q}\) are sets of moments of two probability measures supported on \(\mathcal{X}\). Let \(\varphi(X)\) be a vector of polynomials such that \(\mathbb{E}_{\mathbb{P}}[\varphi(X)]=g(\mathbf{P})\), \(\mathbb{E}_{\mathbb{Q}}[\varphi(X)]=g(\mathbf{Q})\), \(\mathbb{E}_{\mathbb{P}}[\varphi(X)\varphi(X)^{\top}]=G(\mathbf{P})\), and \(\mathbb{E}_{\mathbb{Q}}[\varphi(X)\varphi(X)^{\top}]=G(\mathbf{Q})\), and let_

\[S_{\mathtt{H}}(\mathbf{P},\mathbf{Q};u)=\frac{|u^{\top}(g(\mathbf{P})-g( \mathbf{Q}))|}{\sqrt{u^{\top}G(\mathbf{P})u}+\sqrt{u^{\top}G(\mathbf{Q})u}} \text{ and }S_{\mathtt{TV}}(\mathbf{P},\mathbf{Q};u)=\frac{\left(u^{\top}(g(\mathbf{P})-g (\mathbf{Q}))\right)^{2}}{u^{\top}\left(G(\mathbf{P})u+G(\mathbf{Q})\right)u},\]

_and let_

\[S_{\mathtt{H}}^{*}(\mathbf{P},\mathbf{Q})=\operatorname*{arg\,max}_{u}S_{ \mathtt{H}}(\mathbf{P},\mathbf{Q};u)\text{ and }S_{\mathtt{TV}}^{*}(\mathbf{P},\mathbf{Q})=\operatorname*{arg\,max}_{u}S_{ \mathtt{TV}}(\mathbf{P},\mathbf{Q};u).\]

_Then,_

\[\left(\frac{S_{\mathtt{H}}^{*}(\mathbf{P},\mathbf{Q})}{\sqrt{2+S_{\mathtt{H}}^{*} (\mathbf{P},\mathbf{Q})}}\right)^{2}\leq\frac{S_{\mathtt{TV}}^{*}(\mathbf{P}, \mathbf{Q})}{2+S_{\mathtt{TV}}^{*}(\mathbf{P},\mathbf{Q})}.\] (16)Proof.: To prove this statement, we first show that \(S_{\mathsf{H}}^{*}(\mathbf{P},\mathbf{Q})^{2}\leq S_{\mathsf{TV}}^{*}(\mathbf{P}, \mathbf{Q})\). Fix

\[\hat{u}=\operatorname*{arg\,max}_{u}S_{\mathsf{H}}^{*}(\mathbf{P},\mathbf{Q};u).\]

From this, we see that

\[S_{\mathsf{H}}^{*}(\mathbf{P},\mathbf{Q})^{2} =\frac{(\hat{u}^{\top}(g(\mathbf{P})-g(\mathbf{Q})))^{2}}{\left( \sqrt{\hat{u}^{\top}G(\mathbf{P})\hat{u}}+\sqrt{\hat{u}^{\top}G(\mathbf{Q}) \hat{u}}\right)^{2}}=\frac{(\hat{u}^{\top}(g(\mathbf{P})-g(\mathbf{Q})))^{2}}{ \hat{u}^{\top}G(\mathbf{P})\hat{u}+\hat{u}^{\top}G(\mathbf{Q})\hat{u}+2\sqrt{ \hat{u}^{\top}G(\mathbf{P})\hat{u}}\sqrt{\hat{u}^{\top}G(\mathbf{Q})\hat{u}}}\] \[\leq\frac{\left(\hat{u}^{\top}(g(\mathbf{P})-g(\mathbf{Q})) \right)^{2}}{\hat{u}^{\top}\left(G(\mathbf{P})u+G(\mathbf{Q})\right)\hat{u}} \leq S_{\mathsf{TV}}^{*}(\mathbf{P},\mathbf{Q}).\]

Next, let

\[d_{1}=\frac{S_{\mathsf{TV}}^{*}(\mathbf{P},\mathbf{Q})}{2+S_{\mathsf{TV}}^{*} (\mathbf{P},\mathbf{Q})}\text{ and }2d_{2}=\left(\frac{S_{\mathsf{H}}^{*}(\mathbf{P}, \mathbf{Q})}{\sqrt{2}+S_{\mathsf{H}}^{*}(\mathbf{P},\mathbf{Q})}\right)^{2}.\]

We have

\[d_{2}=\frac{S_{\mathsf{H}}^{*}(\mathbf{P},\mathbf{Q})^{2}}{2+S_{\mathsf{H}}^{* }(\mathbf{P},\mathbf{Q})^{2}+2\sqrt{2}S_{\mathsf{H}}^{*}(\mathbf{P},\mathbf{ Q})}\leq\frac{S_{\mathsf{H}}^{*}(\mathbf{P},\mathbf{Q})^{2}}{2+S_{\mathsf{H}}^{*}( \mathbf{P},\mathbf{Q})^{2}}\]

Since \(\frac{x}{x+2}\) is monotonically increasing for positive \(x\), we prove the statement by the fact that \(S_{\mathsf{TV}}^{*}\geq S_{\mathsf{H}}^{*2}\). 

With these results, the Theorem is proved. 

Proof of Corollary 1: Computing the Bayes Classifier and \(\mathsf{TV}(\mathbb{P},\mathbb{Q})\) from the Lower Bound

In this section, we prove Corollary 1. Recall that the lower bound proposed in Theorem 1 is not tight, as the Cauchy-Schwarz inequality used in the derivation of the bound is only not strict when the witness function \(f\) is a constant. However, there are cases where the bound can be used to compute the true TV distance. We show this case in Corollary 1, which we restate and prove in the sequel.

**Corollary**.: _Suppose \(\mathbb{P}\equiv\mathcal{N}(\mu_{p},\Sigma)\) and \(\mathbb{Q}\equiv\mathcal{N}(\mu_{q},\Sigma)\) Let \(f(x;u)=u^{\top}(x-\frac{1}{2}(\mu_{p}-\mu_{q}))\) be a witness function. Then,_

\[\mathsf{TV}(\mathbb{P},\mathbb{Q})=2\Phi\left(\sqrt{(u^{*})^{\top}(\mu_{p}- \mu_{q})}/2\right)-1,\]

_where_

\[u^{*}=\operatorname*{arg\,max}_{u}\ \frac{\left(\mathbb{E}_{x\sim\mathbb{P}}[f(x;u)]- \mathbb{E}_{x\sim\mathbb{Q}}[f(x;u)]\right)^{2}}{\mathbb{E}_{x\sim\mathbb{P}} [f(x;u)^{2}]+\mathbb{E}_{x\sim\mathbb{Q}}[f(x;u)^{2}]}\]

Proof.: First, following the proof of Corollary 2, we have \(u^{*}=\Sigma^{-1}(\mu_{p}-\mu_{q})\). Note that this is identical to the weights of the Gaussian discriminant classifier discussed in, say, [9]. Substituting \(u*\) into the expression \(\mathsf{TV}(\mathbb{P},\mathbb{Q})=2\Phi\left(\sqrt{(u^{*})^{\top}(\mu_{p}- \mu_{q})}/2\right)-1\), we get \(\mathsf{TV}(\mathbb{P},\mathbb{Q})=2\Phi\left(\sqrt{(\mu_{p}-\mu_{q})^{\top} \Sigma^{-1}(\mu_{p}-\mu_{q})}/2\right)-1\). The risk of the Bayes classifier, as given in [45], is \(R^{*}(\mathbb{P},\mathbb{Q})=\Phi(-\sqrt{(\mu_{p}-\mu_{q})^{\top}\Sigma^{-1}( \mu_{p}-\mu_{q})}/2)\), where \(\Phi(x)\) is the Gaussian cdf. Using the fact that \(\Phi(x)=1-\Phi(-x)\), and the identity \(2R^{*}(\mathbb{P},\mathbb{Q})=1-\mathsf{TV}(\mathbb{P},\mathbb{Q})\), we get \(\mathsf{TV}(\mathbb{P},\mathbb{Q})=2\Phi\left(\sqrt{(\mu_{p}-\mu_{q})^{\top} \Sigma^{-1}(\mu_{p}-\mu_{q})}/2\right)-1\). Note that with this choice of \(u*\), the square root term remains well-defined. This matches the well-known result for the TV distance between Gaussian measures with the same variance. Thus, we prove the statement. 

_R_emark: This result also illustrates the case where the Bayes' classifier lies in the set of functions \(\mathcal{F}:=\{f(x):f(x)=u^{\top}\varphi(x)\}\) for a given function \(\varphi(x)\). In this case, if \(\varphi(x)=x-\frac{1}{2}(\mu_{p}-\mu_{q})\), and \(\mathbb{P}\) and \(\mathbb{Q}\) are Gaussian with the same variant, the Bayes classifier is equivalent to the Fisher discriminant.

### Proof of Theorem 3

In this section, we prove Theorem 3. We restate the result for convenience, and prove the Theorem thereafter.

**Theorem**.: _Suppose \(\mathbb{P},\mathbb{Q}\) be two probability measures supported on \(X\subseteq\mathbb{R}^{d}\), with densities \(p\) and \(q\), and let \(\mu_{p}=\mathbb{E}_{\mathbb{P}}[x]\), \(\mu_{q}=\mathbb{E}_{\mathbb{Q}}[x]\) and \(C_{p}=\mathbb{E}_{\mathbb{P}}[xx^{\top}]\), \(C_{q}=\mathbb{E}_{\mathbb{Q}}[xx^{\top}]\). Suppose we have plug-in estimates \(\bar{\mu}_{p}\), \(\bar{C}_{p}\), \(\bar{\mu}_{q}\), \(\bar{C}_{q}\) as defined in (7), that satisfy_

\[\|\mu_{p}-\bar{\mu}_{p}\|_{2}\leq\delta_{p}\text{ and }\|\mu_{q}- \bar{\mu}_{q}\|_{2}\leq\delta_{q}\] \[\|C_{p}-\bar{C}_{p}\|_{F}\leq\rho_{p}\text{ and }\|C_{q}-\bar{C}_{q}\|_{F} \leq\rho_{q}.\]

_Then, with a witness function of the form \(f(x)=u^{\top}x\)_

\[D_{\text{TV}}(\mathbb{P},\mathbb{Q})\geq\min_{\mu_{p},\mu_{q}\in\mathcal{M}} \,(\Delta\mu)^{\top}(C_{p}+C_{q}+\rho I)^{-1}(\Delta\mu),\] (17)

_where \(\mathcal{M}=\{(\mu_{p},\mu_{q}):\|\mu_{p}-\bar{\mu}_{p}\|_{2}\leq\delta_{p},\| \mu_{q}-\bar{\mu}_{q}\|_{2}\leq\delta_{q}\}\), \(\Delta\mu=\mu_{p}-\mu_{q}\), and \(\rho=\rho_{p}+\rho_{q}\)._

Proof.: First, note that \(f(x)=u^{\top}x\). Thus,

\[\bar{f}_{p}=u^{\top}\mu_{p}\text{ and }\bar{f}_{q}=u^{\top}\mu_{q}\] \[\bar{f}_{p}^{(2)}=u^{\top}C_{p}u\text{ and }\bar{f}_{(q)}=u^{\top}C_{q}u.\]

Thus, for exact values, we have

\[\text{TV}(\mathbb{P},\mathbb{Q})\geq D_{\text{TV}}(\mathbb{P},\mathbb{Q};2)= \frac{(u^{\top}(\mu_{p}-\mu_{q}))^{2}}{u^{\top}(C_{p}+C_{q})u}=(\mu_{p}-\mu_{q })^{\top}(C_{p}+C_{q})^{-1}(\mu_{p}-\mu_{q}).\]

However, there are errors that arise from using plug-in estimators (defined in (7). To handle the estimation error for \(C_{p}\) and \(C_{q}\), we rely on (23) in Lanckriet et al. [28]. That is,

\[\max_{C_{p}:\|\bar{C}_{p}-C_{p}\|_{F}\leq\rho}\,u^{\top}C_{p}u=u^{\top}(\bar{C }_{p}+\rho I)u.\]

We substitute these values back into the lower bound, and impose the constraints on the \(\mu_{p}\) and \(\mu_{q}\). Thus, we prove the theorem. 

_Remark B.1_.: The proof follows a similar logic to that of the derivation of (15) in Kim et al. [24].

Appendix C Additional Results: Computational Complexity of \(r_{l,j}\) scores with Different Witness functions

In this section, we detail the computational cost of computing the \(r_{l,j}\) scores needed for DisCEdit-SP and DisCEdit-U. We tabulate our results in Table 3.

Explanation for Storage ComplexityFor a fixed witness function, for each filter, we only need to store 4 real numbers per class (that is, for a distribution \(P\), we store \(\bar{f}_{c}\), \(\bar{f}_{c}^{(2)}\), \(\bar{f}_{\bar{c}}\), and \(\bar{f}_{\bar{c}}^{(2)}\). For Fisher and MPM type witness functions, we need to store the class-conditional and class-complement means and covariances of \(\varphi(X)\) for each class, in total requiring the storage of \(O(n^{2})\) values. For DisCEdit-U, since we only need to compute scores for a single class, the dependence on \(C\) vanishes.

Explanation for Computation ComplexityThe computational complexity of one-shot pruning of a given layer using DisCEdit-SPdepends on the number of filters \(L\), the number of classes in the dataset \(C\), and the witness function itself. If the Witness function is fixed a priori, the cost of computing each TV distance is \(O(1)\). Thus, in this case, the complexity is \(O(LC)\) to compute all the pairwise TV distance lower bounds, and \(O(LC)\) to find the minimum for each filter. For Fisher discriminant-type scores (such as are used in DisCEdit-SP-F, or the witness function used in Section 7.3), given that \(\varphi(X)\) is a vector of length \(n\), this requires the inversion of a matrix, which requires \(O(n^{2})\) iterations. Thus, in this case, the cost is \(O(LCn^{2})\) iterations (this subsumes the cost of finding the minimum). For MPM based witness functions (such as those used in DisCEdit-SP-Q), this requires solving an SOCP (\(\tilde{O}(n^{3})\) complexity, \(\tilde{O}(\cdot)\) suppresses \(\epsilon\) accuracy terms). Thus, the cost is \(\tilde{O}(LCn^{3})\). For DisCEdit-U, since we only need to compute scores for a single class, the dependence on \(C\) vanishes.

## Appendix D Variants of the DisCEdit-SP Algorithm

In this section, we propose a variety of variants of DisCEdit-SPalgorithm. In particular, we highlight how changing the witness function used to lower bound the total variation distance can lead to new algorithms. Moreover, we show how TVSPrune [39] can be recovered from DisCEdit-SP; indeed, it is a special case of it.

### Fisher-based Lower Bounds and TVSPrune

We choose \(f(X)=u^{\top}\varphi(X)\). Let

\[\mu_{j,c}^{l}=\mathbb{E}_{X\sim\mathcal{D}_{c}}\left[\varphi(X)\right]\]

and let

\[\Sigma_{j,c}^{l}=\mathbb{E}_{X\sim\mathcal{D}_{c}}\left[(\varphi(X)-\mu_{j,c} ^{l})(\varphi(X)-\mu_{j,c}^{l})^{\top}\right].\]

Then, we get

``` Input: Class conditional distributions \(\mathcal{D}_{c},\ c\in[C]\), pretrained CNN with parameters \(\mathcal{W}=(W_{1},\cdots,W_{L})\), layerwise sparsity budgets \(B^{l}\), witness function \(f\) for\(l\in[L]\)do  Set \(S^{l}=[s_{1}^{l},\cdots,s_{N_{l}}^{l}]=\mathbf{0}_{N_{l}}\)  Compute \(\mu_{j,c}^{l}\)\(\mu_{j,c}^{l}\), \(\mathbf{\Sigma}_{j,c}^{l}\), \(\Sigma_{j,c}^{l}\) for all \(j,c\)  Compute \(r_{j}^{l}=\min_{c}\text{F}\text{F}\text{F}^{*}(\mathcal{D}_{j,c}^{l}, \mathcal{D}_{j,c}^{l})\) for all \(j\). if\(j\in\operatorname{sort}_{B_{l}}(\{r_{j}^{l}\}_{j=1}^{N_{l}})\)then  Set \(s_{j}^{l}=1\) Output: Sparse Masks \(S_{1},\cdots,S^{L}\) return\(\hat{\mathcal{W}}\) ```

**Algorithm 2**DisCEdit-SP-F

If \(\varphi(X)\) is a vector of quadratic functions of \(X\), we call the algorithm DisCEdit-SP-FQ.

### Minimax Probability Machine based Algorithms

We define \(\mu_{j,c}^{l}\)\(\mu_{j,\tilde{c}}^{l}\), \(\Sigma_{j,c}^{l}\), \(\Sigma_{j,\tilde{c}}^{l}\) as previously. We then state the algorithm as follows.

``` Input: Class conditional distributions \(\mathcal{D}_{c},\ c\in[C]\), Pretrained CNN with parameters \(\mathcal{W}=(W_{1},\cdots,W_{L})\), layerwise sparsity budgets \(B^{l}\), witness function \(f\) for\(l\in[L]\)do  Set \(S^{l}=[s_{1}^{l},\cdots,s_{N_{l}}^{l}]=\mathbf{0}_{N_{l}}\)  Compute \(\mu_{j,c}^{l}\)\(\mu_{j,\tilde{c}}^{l}\), \(\Sigma_{j,c}^{l}\), \(\Sigma_{j,\tilde{c}}^{l}\) for all \(j,c\)  Compute \(r_{j}^{l}=\min_{c}\text{M}\mathcal{W}^{*}(\mathcal{D}_{j,c}^{l},\mathcal{D}_{j,c}^{l})\) for all \(j\). if\(j\in\operatorname{sort}_{B_{l}}(\{r_{j}^{l}\}_{j=1}^{N_{l}})\)then  Set \(s_{j}^{l}=1\) Output: Sparse Masks \(S_{1},\cdots,S^{L}\) return\(\hat{\mathcal{W}}\) ```

**Algorithm 3**DisCEdit-SP-M

If \(\varphi(X)\) is a vector of quadratic functions of \(X\), we call the algorithm DisCEdit-SP-MQ.

\begin{table}
\begin{tabular}{l|c c c c} \hline
**Witness Function** & **Cost (P)** & **Storage (P)** & **Cost (U)** & **Storage (U)** \\ \hline
**Fixed (a priori)** & \(O(LC)\) & \(O(LC)\) & \(O(L)\) & \(O(L)\) \\
**Fisher-type Witness Function** & \(O(LCn^{2})\) & \(O(LCn^{2})\) & \(O(Ln^{2})\) & \(O(Ln^{2})\) \\
**MPM-type Witness Function** & \(O(LCn^{3})\) & \(O(LCn^{2})\) & \(O(Ln^{3})\) & \(O(Ln^{2})\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of complexities of computing different witness functions. (P) refers to pruning (used in DisCEdit-SP) and (U) refers to unlearning (used in DisCEdit-U).

### EnsemblePrune - Taking the best of FisherPrune and MPMPrune

We choose the same witness functions as we did for DisCEdit-SP-M and DisCEdit-SP-F, and define the moments in the same fashion. We then

```
0: Class conditional distributions \(\mathcal{D}_{c},\ c\in[C]\), Pretrained CNN with parameters \(\mathcal{W}=(W_{1},\cdots,W_{L})\), layerwise sparsity budgets \(B^{l}\), witness function \(f\) for\(l\in[L]\)do  Set \(S^{l}=[s^{l}_{1},\cdots,s^{l}_{N_{l}}]=\mathbf{0}_{N_{l}}\)  Compute \(\mu^{l}_{j,c}\), \(\mu^{l}_{j,c}\), \(\Sigma^{l}_{j,c}\), \(\Sigma^{l}_{j,c}\) for all \(j,c\)  Compute \[r^{l}_{j}=\max\left\{\max_{u}\ \frac{|u^{\top}(\mu^{l}_{j,c}-\mu^{l}_{j, \bar{c}})|}{\sqrt{u^{\top}\Sigma^{l}_{j,c}u}+\sqrt{u^{\top}\Sigma^{l}_{j,c}u}}, \ \max_{u}\ \frac{(u^{\top}(\mu^{l}_{j,c}-\mu^{l}_{j,\bar{c}})^{2})}{2u^{\top}( \Sigma^{l}_{j,c}+\Sigma^{l}_{j,c})^{2})u}\right\}\] for all \(j\). if\(j\in\operatorname{sort}_{B_{l}}(\{r^{l}_{j}\}_{j=1}^{N_{l}})\)then  Set \(s^{l}_{j}=1\) Output: Sparse Masks \(S_{1},\cdots,S^{L}\) return\(\hat{\mathcal{W}}\) ```

**Algorithm 4**DisCEdit-SP-E

### RobustPrune - Accounting for Errors in Moment Measurements

We choose the same witness functions as we did for DisCEdit-SP-M and DisCEdit-SP-F, and define the moments in the same fashion. We then derive the following algorithm.

```
0: Class conditional distributions \(\mathcal{D}_{c},\ c\in[C]\), pretrained CNN with parameters \(\mathcal{W}=(W_{1},\cdots,W_{L})\), layerwise sparsity budgets \(B^{l}\), witness function \(f=u^{\top}\varphi(x)\), error for\(l\in[L]\)do  Set \(S^{l}=[s^{l}_{1},\cdots,s^{l}_{N_{l}}]=\mathbf{0}_{N_{l}}\)  Compute plug-in estimates of \(\mu^{l}_{j,c}\), \(\mu^{l}_{j,\bar{c}}\), \(\Sigma^{l}_{j,c}\), \(\Sigma^{l}_{j,\bar{c}}\) for all \(j,c\)  Compute \[r^{l}_{j}=\min_{c}\ \min_{\mu^{l}_{j,c},\mu^{l}_{j,\bar{c}}} (\mu^{l}_{j,c}-\mu^{l}_{j,\bar{c}})^{\top}(\Sigma^{l}_{j,c}+\Sigma^{l}_{j, \bar{c}}+(\rho^{l}_{j,c}+\rho^{l}_{j,c})I)^{-1}(\mu^{l}_{j,c}-\mu^{l}_{j,c})\] s.t. \[\|\mu^{l}_{j,c}-\bar{\mu}^{l}_{j,c}\|\leq\delta^{l}_{j,c}\] \[\|\mu^{l}_{j,c}-\bar{\mu}^{l}_{j,c}\|\leq\delta^{l}_{j,c}\] for all \(j\). if\(j\in\operatorname{sort}_{B_{l}}(\{r^{l}_{j}\}_{j=1}^{N_{l}})\)then  Set \(s^{l}_{j}=1\) Output: Sparse Masks \(S_{1},\cdots,S^{L}\) return\(\hat{\mathcal{W}}\) ```

**Algorithm 5**DisCEdit-SP-R

### Recovering TVSPrune

In TVSPrune, at any layer \(l\), the \(j\)th filter is pruned if

\[1-e^{-\Delta_{l,j}}\]

where \(\Delta_{l,j}\) is the minimum Fisher discriminant between pairs of classes. Suppose that we identify important and discriminative filters by measuring the TV distance in a pairwise sense. Recall that Corollary 2 gives us a bound that is also monotonic in \(\Delta_{l,j}\). If we apply the strategy that we prune all filters with a score less than a threshold, we would prune filter \(j\) if

\[\frac{\Delta}{2+\Delta}\leq\gamma\]for some \(\gamma\in(0,1)\). We can now find a relation between \(\gamma\) and \(\eta\). First, note that if \(1-e^{\frac{-\Delta}{4}}\leq\eta\), then \(\Delta\leq 4(1-\eta)\). Similarly, if we prune \(\frac{\Delta}{2+\Delta}\leq\gamma\), then \(\Delta\leq\frac{2\gamma}{1-\gamma}\). Equating the two gives us the expression

\[\eta=\frac{3\gamma-2}{2\gamma-2}.\]

Thus, both TVSPrune and a variant of DisCEdit-SP-F where the TV distance is measured pairwise, and which prunes at a threshold are equivalent, as they require pruning the \(j\)th filter if

\[\Delta\leq 4-4\eta=\frac{3\gamma-2}{2\gamma-2}.\]

### Using the BatchNorm Random Variables

The BatchNorm random variables for this layer are given by

\[\mathsf{BN}^{l}(X)=\left[\mathbf{1}^{\top}Y_{1}^{l}(X),\cdots,\mathbf{1}^{ \top}Y_{N_{l}}^{l}(X)\right]=\left[\mathsf{BN}_{1}^{l}(X),\cdots,\mathsf{BN}_ {N_{l}}^{l}(X)\right].\] (18)

As stated earlier, our goal is to minimize the TV distance between the distributions of the pruned and unpruned features; we use the BatchNorm random variables as a proxy for the features \(Y^{l}(X)\). Next, the moments of \(\mathsf{BN}^{l}(X)\) are given by

\[\mathbb{E}_{X\sim\mathcal{D}}\left[\mathsf{BN}_{i}^{l}(X)\right]=\mathsf{BN} _{i}^{l}=\left[\mu_{i}^{l}\right]\ \ \text{and}\ \ \mathrm{Var}(\mathsf{BN}_{i}^{l}(X))=(\sigma_{i}^{l})^{2}.\] (19)

Suppose \(\mathsf{BN}^{l}(X)\) is drawn from the distribution \(\mathcal{D}_{j}^{\mathsf{BN},l}\), \(\mathcal{D}_{j,c}^{\mathsf{BN},l}\) be the \(c\)th class conditional distribution, and let \(\mathcal{D}_{j,\bar{c}}^{\mathsf{BN},l}\) be the distribution of features sampled from the complement of class \(c\).

## Appendix E Additional Experiments

In this section, we detail additional experiments conducted in the course of this investigation. In particular, we provide experimental results highlighting the utility of our lower bound on synthetic datasets, and provide expanded results for the Gaussianity tests provided in Section 6.1.

### Measuring the TV Distance between Linearly Inseparable Data

In this section, we use the bounds proposed in Corollary 2 to bound the TV distance between poorly separated datasets. We choose the 'Two Spirals' Dataset, and a dataset consisting of two zero-mean Gaussians with different variances. We choose \(f(x)=u^{\top}(\varphi(X)-(\bar{\varphi}_{1}-\bar{\varphi}_{0})/2)\), where \(\phi(X)\), when chosen to be of degree \(d\geq 1\), is given by \(\varphi(X)=1+(1^{\top}X)+\cdots+(1^{\top}X)^{d}\). We present our results in Figure 2. We observe that using the lower bound for Fisher outperforms the MPM lower bound on both toy datasets. However, the choice of polynomial witness functions clearly outperforms lower degree choices, particularly in the 'Two Gaussians' case.

### Effective Pruning of Hard-to-Prune Layers

In this section, we utilize the lower bounds provided in this paper to prune hard-to-prune layers in neural networks. As noted in Murti et al. [39], Liebenwein et al. [34], some layers, in particular the initial layers in the case of VGG-nets, are difficult to effectively sparsify. In this set of experiments, we aim to show that using the lower bounds proposed in this work, we are able to better identify discriminative filters in hard-to-prune layers, and therefore prune those layers more effectively.

**Experiment Setup:** We select a VGG16 model trained on CIFAR10. We fix pruning budgets of \(40\%,\ 50\%,\ 60\%,\ 70\%,\ 80\%\). For each model, we then prune three hard-to-prune layers in isolation, and measure the impact on accuracy. We compare the following methods:

**TVSPrune:** We modify TVSPrune to prune a fixed budget, and using the BatchNorm random variables as described in Appendix D.

**DisCEdit-SP-EQ:** We apply Algorithm 5 as presented in Appendix D using the features \(\varphi(1^{\top}X)=[1^{\top}X,(1^{\top}X)^{2}]\). Thus, for each \(c\), \(f_{(c)}(X)=u^{\top}\left(\varphi(X)-(\bar{\varphi}_{c}+\bar{\varphi}_{\bar{c} })/2)\right)\).

\(L_{1}\)**-based Pruning:** We use the \(L_{1}\) norms of the filter weights, as proposed in [32].

**Results and Discussion** We present our results in Figure 3 The experiments show that DisCEdit-SP variants using quadratic features (using algorithms outperform both TVSPrune and the \(L_{1}\)-norm-based pruning strategy. In particular, we see that at \(70\%\) sparsity in Layers 1-3, the models obtained by DisCEdit-SP-EQ are \(6.6\%\) more accurate than those obtained using TVSPrune.

### Verifying class-conditional Feature Maps are not Gaussian

In this section, we attempt to validate the assumptions made in Murti et al. [39] about the normality of the class-conditional feature distributions. To do so, we apply the Shapiro-Wilks test [49], a standard test for Normality.

**Experiment Setup:** We consider a VGG16 model trained on CIFAR10. Let \(\mathsf{BN}^{l}_{j}(X)=1^{\top}Y^{l}_{j}(X)\). For each \(l,j\), we collect 100 samples from each class \(c\in[10]\). We then apply the Shapiro-Wilk normality test [49], and we compute \(p^{l}_{j,c}\) values, which are the minimum \(p\)-values from the Shapiro-Wilks test computed for the features of the \(j\)th filter in layer \(l\) conditioned on class \(c\). We consider that a

Figure 3: Comparison of the performance of DisCEdit-SP F with TVSPrune and L1 pruning on hard-to-prune layers in VGG16 trained on CIFAR10

Figure 2: Comparison of the performance of DisCEdit-SP F with DisCEdit-SP M with polynomial features on the TwoSpirals and Zero-Means Gaussians datasets.

filter's features are unlikely to be Gaussian if \(p_{j,c}^{l}<0.1\). We plot the heatmaps of \(p_{j}^{l}=\min_{c\in[C]}p_{j,c}^{l}\) values for 15 randomly selected filters in Figure 4, to indicate the normality of the least Gaussian class-conditional features.

**Results:** We observe that for most layers, particularly those close to the output, the class-conditional feature distributions are highly unlikely to have been drawn from a Gaussian, with \(p_{j,c}^{l}\) values for layers 10-12 in VGG16 typically being below \(1\mathrm{e}-5\). For layers with filters that yield likely-Gaussian features, we observe that for the majority of filters, at least one feature output is likely to be non-Gaussian. We present this data in Figure 4.

### Ablation Experiments for DisCEdit-SP

In this section, we conduct a slate of experiments aimed at demonstrating that the effectiveness of DisCEdit-SP is not dependent on particular instantiations of models. Specifically, we apply DisCEdit-SP to multiple instances of models trained on CIFAR10, CIFAR100, and Imagenet. We present specific results in the subsequent subsections.

Figure 4: Shapley-Wilks test \(p\)-value heatmaps applied to class-conditional features generated by filters in each layer. \(x\)-axis is the class index, and \(y\)-axis is the filter index. Filters shown in the heatmap are selected randomly.

#### e.4.1 CIFAR10 Experiments

In this section, we run DisCEdit-SP on 10 different instantiations of different models trained on CIFAR10. We observe that DisCEdit-SP performs well on all instances of the models, irrespective of architecture. Our results are tabulated in Table 4.

#### e.4.2 CIFAR100 Experiments

In this section, we run our experiments on 5 different instances of VGG16, VGG19, and ResNet56 models trained on CIFAR10. We present our results in Table 5. Our results show that the effectiveness of DisCEdit-SP is unrelated to the particular instantiation of the model.

#### e.4.3 Imagenet Experiments

In this section, we provide ablation experiments for Imagenet. However, owing to the computational cost of training Imagenet models, we only train 2 additional instantiations of the model. Moreover, we were only able to fine-tune for 30 epochs. However, DisCEdit-SP broadly works well on all of the instantiations, as seen in Table 6.

#### e.4.4 Pruning without Fine-tuning

In this section, we highlight our pruning experiments without fine-tuning models.We use fixed layerwise sparsity budgets. We use models trained on CIFAR10 and Imagenet datasets. Our experiments, tabulated in Table 7, shows that our method consistently matches or outperforms common baselines.

### Pruning in the High Sparsity Regime

In this section, we prune models trained on CIFAR10 extensively, with over 80% of parameters pruned in total. Fine-tuning is done in a one-shot fashion. We see that DisCEdit-SP consistently matches our outperforms baselines such as Murti et al. [39] or Sui et al. [52]. Our results are tabulated in Table 8

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Best Accuracy** & **Worst Accuracy Drop** & **Mean accuracy drop** \\ \hline
3.1\% & 8.3\% & 6.6\% \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablations for models trained on Imagenet. We consider 3 different models, and average the accuracy drop.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Model** & **Sparsity** & **Best Acc. drop** & **Worst Acc. Drop** & **Mean Acc. Drop** \\ \hline VGG16 & \(40.8\%\) & \(+0.32\) & \(-0.06\) & \(+0.09\pm 0.06\) \\ VGG16 & \(61.2\%\) & \(0.19\) & \(0.51\) & \(0.35\pm 0.03\) \\ VGG16 & \(75.6\%\) & \(1.27\) & \(1.38\) & \(1.34\pm 0.01\) \\ ResNet56 & \(41.2\%\) & \(+0.03\) & \(0.05\) & \(0.01\pm 0.01\) \\ ResNet56 & \(60.7\%\) & \(1.21\) & \(1.30\) & \(1.24\pm 0.02\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablations for models trained on CIFAR10. We consider 10 different models, and average the accuracy after pruning/fine-tuning over them.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Model** & **Sparsity** & **Best Acc. drop** & **Worst Acc. Drop** & **Mean Acc. Drop** \\ \hline VGG16 & \(40.8\%\) & \(+0.03\) & \(0.06\) & \(0.03\pm 0.06\) \\ VGG19 & \(60.6\%\) & \(0.12\) & \(0.19\) & \(0.16\pm 0.03\) \\ Resnet56 & \(40.2\) & \(+0.13\) & \(0.16\) & \(\pm 0.01\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Ablations for models trained on CIFAR100. We consider 10 different models, and average the accuracy of our pruning algorithm over them.

### Discriminative Component Discovery for CIFAR10 models

In this section, we provide plots of \(\eta^{c}_{l,j}\) values for different layers in models trained on CIFAR10. We show plots for 3 different layers for 4 classes, to highlight how discriminative components look.

#### e.6.1 ResNet56 trained on CIFAR10

In this section, we produce plots showing discriminative and nondiscriminative components for a ResNet56 trained on CIFAR10. Our results are plotted in Figure 5. Note that in layer 0, both class 0 and class 8 have filter 8 as discriminative. Moreover, in subsequent layers, all three classes have filters which are somewhat discriminative as well.

#### e.6.2 VGG16 trained on CIFAR10

Similar to the previous subsection, we plot \(\eta^{c}_{l,j}\) values for a VGG16 trained on CIFAR10 We plot a selection of our results in Figure 6. As before, we see that we can clearly identify discriminative and nondiscriminative filters. Note again that class 0 and class 8 share discriminative filters in both layer 0 as well as in layer 4.

## Appendix F Additional Experimental Details

In this section, we detail additional experiments not mentioned in the main paper, as well as a comprehensive description of our experimental setup.

### Pruning Setup

In this section, we discuss our experimental setup.

#### f.1.1 Platform Details

The hardware used for the experiments in this work are detailed below:

1. Server computer with 2 NVIDIA RTX3090Ti GPUs with Intel i9-12700 processors, running Ubuntu 20.04, with Python 3.11 and CUDA Tools 10.2 with PyTorch 2.0.1.
2. Desktop computer with 1 NVIDIA RTX3070Ti GPUs with Intel i7-10700 processor, running Ubuntu 22.04, with Python 3.11 and CUDA Tools 11.7 with PyTorch 2.0.1.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**Model** & **Dataset** & **Sparsity** & **Acc. Drop (ours)** & **Acc. Drop [39]** & **Acc. Drop [52]** & **Acc. Drop (L1)** \\ \hline VGG16 & CIFAR10 & 32\% & 1.91 & 2.20 & 2.13 & 10.52 \\ VGG16 & CIFAR10 & 41\% & 4.56 & 5.21 & 6.53 & 16.59 \\ VGG16 & CIFAR10 & 63\% & 10.16 & 12.79 & 12.65 & 32.8 \\ VGG19 & CIFAR10 & 30\% & 0.98 & - & 1.22 & 6.55 \\ VGG19 & CIFAR10 & 44\% & 2.56 & - & 4.53 & 13.8 \\ VGG19 & CIFAR10 & 60\% & 6.16 & - & 8.44 & 23.67 \\ ResNet50 & ImageNet & 11\% & 18.37 & 22.61 & 21.49 & 35.52 \\ ResNet50 & ImageNet & 21\% & 30.7 & 36.0 & 32.69 & - \\ \hline \hline \end{tabular}
\end{table}
Table 7: Pruning results with \(\text{DiscEdit-Spr}\)without fine-tuning.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Model** & **Sparsity** & **Algorithm** & **Acc. drop (no FT)** & **Acc. drop (FT)** \\ \hline \multirow{3}{*}{VGG16} & 84.7\% & CHIP & 83.1\% & 0.20\% \\  & 84.7\% & TVSPrune & 83.4\% & 0.21\% \\  & 84.7\% & DiSCEdit-SP & 82.9\% & 0.15\% \\  & - & CHIP & - & - \\ \hline \multirow{3}{*}{VGG19} & 88.7\% & TVSPrune & 84.8\% & 0.17\% \\  & 88.7\% & DiSCEdit-SP & 84.7\% & 0.16\% \\  & 74.3\% & CHIP & 84.0\% & 0.28\% \\ \hline \multirow{3}{*}{ResNet56} & 74.3\% & TVSPrune & 83.9\% & 0.25\% \\  & 74.3\% & DiSCEdit-SP & 84.2\% & 0.28\% \\ \hline \end{tabular}
\end{table}
Table 8: Pruning results with \(\text{DiscEdit-SP}\) on CIFAR10 models in the high-sparsity regime

#### f.1.2 Models under consideration

We consider the following models.

* **VGG16/19 trained on CIFAR10 and CIFAR100:** We use the pre-trained VGG11/16/19 models trained on CIFAR10 and CIFAR100. The models achieve accuracies greater than \(90\%\) on both datasets.
* **ResNet56 trained on CIFAR10:** We consider a ResNet56 model trained on CIFAR10. We do not prune layers that are part of complex interconnections (such as the final layer in each BasicBlock).
* **ResNet50 trained in Imagenet:** We consider a ResNet50 model trained on Imagenet. We do not prune layers that are part of complex interconnections, as was the case in ResNet56.
* **ViT trained on CIFAR10** We trained a custom ViT on CIFAR10. The details of the model are given below in Table 9: The accuracies of this models are given in Table 10

The accuracies of all models are given in Table 11 below:

Figure 5: \(r_{t,j}^{c}\) plots for different layers, and classes 0, 3, and 8 for a ResNet56 trained on CIFAR10. \(x\) axis is the filter index, \(y\)-axis is \(r_{t,j}^{c}\).

\begin{table}
\begin{tabular}{l l} \hline \hline
**Dataset** & **Test Acc.** \\ \hline CIFAR10 & 88.2\% \\ CIFAR100 & 69.5\% \\ \hline \hline \end{tabular}
\end{table}
Table 10: Test accuracy for CIFAR datasets.

Figure 6: \(r_{l,j}^{c}\) plots for different layers, and classes 0, 3, and 8 for a VGG16 trained on CIFAR10. \(x\) axis is the filter index, \(y\)-axis is \(r_{l,j}^{c}\).

Model ProvenanceWe list the sources of the models below.

* All CIFAR10 and CIFAR100 models were obtained from: https://github.com/chenyaofo/pytorch-cifar-models.
* ResNet50 trained on Imagenet was obtained from: https://drive.google.com/drive/folders/1b-d2lvKUUu0rXqMYAtIrOynHQHuEWDI, which in turn comes from: https://github.com/Eclipsess/CHIP_NeurIPS2021?tab=readme-ov-file
* The ViT models use code from https://github.com/omihub777/ViT-CIFAR.

#### f.1.3 Dataset Selection

For PruningSince we assume that the training dataset is unavailable to us, we utilize the validation set as a proxy for the data-distribution. We detail our dataset splits in Table 12.

Note: the subset used to compute the TV distances are not reused while measuring the test accuracy.Class UnlearningWe use the training set to identify discriminative filters, and the test set to measure accuracy. Typically, we get

#### f.1.4 Hyperparameter Details

In this section, we detail the hyperparameters used when fine-tuning pruned models. We present the hyperparameters for CIFAR10 and Imagenet models only, as we did not fine-tune models that used CIFAR100.

CIFAR10 Fine-tuningWe detail the hyperparameters used in our CIFAR10 experiments below.

1. **Batch Size:** 128
2. **Epochs:** 50
3. **Learning Rate:**.001
4. **Weight Decay:**.0005
5. **Momentum paramters:**.9
6. **Optimizer:** SGD

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Dataset** & **Model** & **Test Accuracy** \\ \hline \multirow{3}{*}{CIFAR10} & VGG16 & 94.16 \\  & ResNet56 & 94.37 \\  & ResNet20 & 92.2 \\  & ViT & 88.2 \\ \hline \multirow{3}{*}{CIFAR100} & VGG16 & 74.0 \\  & ResNet56 & 72.6 \\  & ViT & 69.5 \\ \hline ImageNet & ResNet50 & 76.15 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Test accuracy of dataset splits used in our experiments.

\begin{table}
\begin{tabular}{l l l l} \hline
**Dataset** & **Training Set** & **TV Distance Set** & **Test Set** \\ \hline CIFAR10 & Not used & 4000 images from test set & 6000 images from Test set \\ CIFAR10 & Not used & 4000 images from test set & 6000 images from Test set \\ Imagenet & Not used & 30000 images from Validation set & 20000 images from Val. set \\ \hline \hline \end{tabular}
\end{table}
Table 12: Breakdown of dataset splits used in our experiments.

ImageNet Fine-tuningWe detail the hyperparameters used when fine-tuning Imagenet models.

1. **Batch Size:** 128, using gradient accumulation
2. **Epochs:** 100
3. **Learning Rate:** 0.08 (initial)
4. **Momentum:** 0.99
5. **Weight Decay:0.0001**
6. **Optimizer:** SGD

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All contributions tally with the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See section 4 for limitations of lower bound, including non-tightness and estimation uncertainty. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Please see proofs in the appendix. Assumptions are also stated clearly. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Experiments are simple and clearly laid out, experimental details are available in the Appendix, anonymous code submitted. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Code submitted, please see link. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details provided in Appendix E Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See ablation studies in Appendix E Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: refer to Appendix F for details on compute used. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Code of ethics followed, no interventions with living beings requiring special processing. Only standard datasets were used. No conflicts of interest. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See impact statement Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: standard datasets/models used Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Source of models listed in Appendix F Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: **[TODO]*
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: **[TODO]*
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: **[TODO]*
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.