# Diffusion Schrodinger Bridge Matching

 Yuyang Shi

University of Oxford

&Valentin De Bortoli

ENS ULM

&Andrew Campbell

University of Oxford

&Arnaud Doucet

University of Oxford

Equal contribution.

###### Abstract

Solving transport problems, i.e. finding a map transporting one given distribution to another, has numerous applications in machine learning. Novel mass transport methods motivated by generative modeling have recently been proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs) implement such a transport through a Stochastic Differential Equation (SDE) or an Ordinary Differential Equation (ODE). However, while it is desirable in many applications to approximate the deterministic dynamic Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs are not guaranteed to provide transports close to the OT map. In contrast, Schrodinger bridges (SBs) compute stochastic dynamic mappings which recover entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian Fitting (IMF), a new methodology for solving SB problems, and Diffusion Schrodinger Bridge Matching (DSBM), a novel numerical algorithm for computing IMF iterates. DSBM significantly improves over previous SB numerics and recovers as special/limiting cases various recent transport methods. We demonstrate the performance of DSBM on a variety of problems.

## 1 Introduction

Mass transport problems are ubiquitous in machine learning (Peyre and Cuturi, 2019). For discrete measures, the Optimal Transport (OT) map can be computed exactly but is computationally intensive. In a landmark paper, Cuturi (2013) showed that an entropy-regularized version of OT can be computed more efficiently using the Sinkhorn algorithm (Sinkhorn, 1967). This has enabled the use of OT techniques in a variety of applications ranging from biology (Bunne et al., 2022) to shape correspondence (Feydy et al., 2017). However, applications involving high-dimensional continuous distributions and/or large datasets remain challenging for these techniques.

One of such data-rich applications is generative modeling, a central transport problem in machine learning which requires designing a deterministic or stochastic mapping transporting a reference "noise" distribution to the data distribution. For example, Generative Adversarial Networks (Goodfellow et al., 2014) define a static, deterministic transport map, while Denoising Diffusion Models (DDMs) (Song et al., 2021; Ho et al., 2020) build a dynamic, stochastic transport map by simulating a Stochastic Differential Equation (SDE), whose drift is learned using score matching (Hyvarinen, 2005; Vincent, 2011). The excellent performances of DDMs have motivated recent developments of Bridge Matching and Flow Matching models, which are dynamic transport maps using SDEs (Song et al., 2021; Peluchetti, 2021; Liu, 2022; Albergo et al., 2023) or ODEs (Albergo and Vanden-Eijnden, 2023; Heitz et al., 2023; Lipman et al., 2023; Liu et al., 2023). Compared to DDMs, Bridge and Flow Matching methods do not rely on a forward "noising" diffusion converging to the reference distribution in infinite time, and are also more generally applicable as they can approximate transport maps between two general distributions based on their samples. Nonetheless, these transport mapsare not necessarily close to the OT map minimizing the Wasserstein-2 metric, which is appealing for its many attractive properties (Peyre and Cuturi, 2019; Villani, 2009).

In contrast, the Schrodinger Bridge (SB) problem is a dynamic version of entropy-regularized OT (EOT) (Follmer, 1988; Leonard, 2014). The SB is the finite-time diffusion which admits as initial and terminal distributions the two distributions of interest and is the closest in Kullback-Leibler divergence to a reference diffusion. Numerous methods to approximate SBs numerically have been proposed, see e.g. (Bernton et al., 2019; Chen et al., 2016; Finlay et al., 2020; Caluya and Halder, 2021; Pavon et al., 2021), but these techniques tend to be restricted to low-dimensional settings. Recently, novel techniques using diffusion-based ideas have been proposed in (De Bortoli et al., 2021; Vargas et al., 2021; Chen et al., 2022) based on Iterative Proportional Fitting (IPF) (Fortet, 1940; Kullback, 1968; Ruschendorf and Thomsen, 1993), a continuous state-space extension of the Sinkhorn algorithm (Essid and Pavon, 2019). These approaches have been shown to scale better empirically, but numerical errors tend to accumulate over iterations (Fernandes et al., 2021).

In this paper, our contributions are three-fold. First, we introduce Iterative Markovian Fitting (IMF), a new procedure to compute SBs which alternates between projecting on the space of _Markov processes_ and on the _reciprocal class_, i.e. the measures which have the same bridge as the reference measure of SB (Leonard et al., 2014). We establish various theoretical results for IMF. Contrary to IPF, the IMF iterates always preserve the initial and terminal distributions. The differences between IPF and IMF are presented in Table 1. Second, we propose Diffusion Schrodinger Bridge Matching (DSBM), a novel algorithm approximating numerically the SB solution derived from IMF. DSBM requires at each iteration solving a simple regression problem in the spirit of Bridge and Flow Matching, and does not suffer from the time-discretization and "forgetting" issues of previous DSB techniques (De Bortoli et al., 2021; Vargas et al., 2021; Chen et al., 2022). Finally, we demonstrate the performance of DSBM on a variety of transport tasks.2

Footnote 2: Code can be found at https://github.com/yuyang-shi/dsbm-pytorch.

Notations.We denote by \(\mathcal{P}(\mathcal{C})\), the space of _path measures_, i.e. \(\mathcal{P}(\mathcal{C})=\mathcal{P}(\mathrm{C}([0,T],\mathbb{R}^{d}))\) where \(T>0\). The subset of _Markov_ path measures associated with an SDE of the form \(\mathrm{d}\mathbf{X}_{t}=v_{t}(\mathbf{X}_{t})\mathrm{d}t+\sigma_{t}\mathrm{ d}\mathbf{B}_{t}\), with \(\sigma,v\) locally Lipschitz, is denoted \(\mathcal{M}\). For any \(\mathbb{Q}\in\mathcal{M}\), the _reciprocal class_ of \(\mathbb{Q}\) is denoted \(\mathcal{R}(\mathbb{Q})\), see Definition 3. We also denote \(\mathbb{Q}_{t}\) its marginal distribution at time \(t\), \(\mathbb{Q}_{s,t}\) the joint distribution at times \(s\) and \(t\), \(\mathbb{Q}_{s|t}\) the conditional distribution at time \(s\) given state at time \(t\), and \(\mathbb{Q}_{|0,T}\in\mathcal{P}(\mathcal{C})\) its _diffusion bridge_. Unless specified otherwise, all gradient operators \(\nabla\) are w.r.t. the variable \(x_{t}\) with time index \(t\). Let \((\mathsf{X},\mathcal{X})\) and \((\mathsf{Y},\mathcal{Y})\) be probability spaces. Given a Markov kernel \(\mathrm{K}:\,\mathsf{X}\times\mathcal{Y}\to[0,1]\) and a probability measure \(\mu\) defined on \(\mathcal{X}\), we write \(\mu\mathrm{K}\) the probability measure on \(\mathcal{Y}\) such that for any \(\mathsf{A}\in\mathcal{Y}\) we have \(\mu\mathrm{K}(\mathsf{A})=\int_{\mathsf{X}}\mathrm{K}(x,\mathsf{A})\mathrm{d} \mu(x)\). In particular, for any joint distribution \(\Pi_{0,T}\) over \(\mathbb{R}^{d}\times\mathbb{R}^{d}\), we denote the _mixture of bridges_ measure as \(\Pi=\Pi_{0,T}\mathbb{Q}_{|0,T}\in\mathcal{P}(\mathcal{C})\), which is short for \(\Pi(\cdot)=\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\mathbb{Q}_{|0,T}(\cdot|x_ {0},x_{T})\Pi_{0,T}(\mathrm{d}x_{0},\mathrm{d}x_{T})\).

## 2 Dynamic Mass Transport Techniques

### Denoising Diffusion and Bridge Matching Models

Denoising Diffusion Models (Song et al., 2021; Ho et al., 2020) are a popular class of generative models. They define a forward noising process \(\mathbb{Q}\in\mathcal{M}\) using the SDE \(\mathrm{d}\mathbf{X}_{t}=-\frac{1}{2}\mathbf{X}_{t}\mathrm{d}t+\mathrm{d} \mathbf{B}_{t}\) on the time-interval \([0,T]\), where \(\mathbf{X}_{0}\in\mathbb{R}^{d}\) is drawn from the data distribution \(\pi_{0}\) and \((\mathbf{B}_{t})_{t\in[0,T]}\) is a \(d\)

\begin{table}
\begin{tabular}{c||c|c}  & Sets for alternating projections & Preserved properties \\ \hline \hline IPF & \(\mathbb{P}_{0}=\pi_{0}\); \(\mathbb{P}_{T}=\pi_{T}\) & \(\mathcal{M}\), \(\mathcal{R}(\mathbb{Q})\) \\ \hline IMF & \(\mathcal{M}\); \(\mathcal{R}(\mathbb{Q})\) & \(\mathbb{P}_{0}=\pi_{0}\), \(\mathbb{P}_{T}=\pi_{T}\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison between Iterative Markovian Fitting (IMF) and Iterative Proportional Fitting (IPF). The Schr√∂dinger Bridge is the _unique_\(\mathbb{P}\) s.t. \(\mathbb{P}_{0}=\pi_{0}\), \(\mathbb{P}_{T}=\pi_{T}\), \(\mathbb{P}\in\mathcal{M}\), \(\mathbb{P}\in\mathcal{R}(\mathbb{Q})\) simultaneously by Proposition 5. \(\mathcal{M}\) is the space of (regular) Markov measures and \(\mathcal{R}(\mathbb{Q})\) the space of reciprocal measures of \(\mathbb{Q}\).

Figure 1: Relationship between DSBM and existing methods.

dimensional Brownian motion. This diffusion3 converges towards the standard Gaussian distribution \(\mathrm{N}(0,\mathrm{Id})\) as \(T\to\infty\). A generative model is given by its _time-reversal_\((\mathbf{Y}_{t})_{t\in[0,T]}=(\mathbf{X}_{T-t})_{t\in[0,T]}\), where \(\mathbf{Y}_{0}\sim\mathbb{Q}_{T}\) and \(\mathrm{d}\mathbf{Y}_{t}=\{\frac{1}{2}\mathbf{Y}_{t}+\nabla\log\mathbb{Q}_{T- t}(\mathbf{Y}_{t})\}\mathrm{d}t+\mathrm{d}\mathbf{B}_{t}\)(Anderson, 1982; Haussmann and Pardoux, 1986). In practice, \((\mathbf{Y}_{t})_{t\in[0,T]}\) is initialized with \(\mathbf{Y}_{0}\sim\pi_{T}=\mathrm{N}(0,\mathrm{Id})\), and the _Stein score_\(\nabla\log\mathbb{Q}_{t}(x_{t})=\mathbb{E}_{\mathbb{Q}_{0|t}}[\nabla\log \mathbb{Q}_{t|0}(\mathbf{X}_{t}|\mathbf{X}_{0})\mid\mathbf{X}_{t}=x_{t}]\) is approximated using a neural network \(s_{\theta}(t,x_{t})\) minimizing the _denoising score matching_ loss \(\mathbb{E}_{\mathbb{Q}_{0,t}}[\|\nabla\log\mathbb{Q}_{t|0}(\mathbf{X}_{t}| \mathbf{X}_{0})-s_{\theta}(t,\mathbf{X}_{t})\|^{2}]\).

Footnote 3: This is known as the Ornstein‚ÄìUhlenbeck (OU) process or VPSDE (Song et al., 2021b).

An alternative to considering the time-reversal of a forward noising process is to "build bridges" between the two distributions and learn a _mimicking_ diffusion process. This approach generalizes DDMs and allows for more flexible choices of sampling processes. We call this framework _Bridge Matching_ and adopt a presentation similar to Peluchetti (2021); Liu et al. (2022b), where \(\pi_{T}\) is the data distribution.4 We denote \(\mathbb{Q}\in\mathcal{M}\) the path measure associated with the following process

Footnote 4: To keep notations consistent with existing works, \(\pi_{0}\) is the data distribution in the context of DDM and SB, whereas \(\pi_{T}\) is the data distribution in Bridge Matching. However, both SB and Bridge Matching methods allow transfer between arbitrary distributions \(\pi_{0},\pi_{T}\), so this distinction is not important.

\[\mathrm{d}\mathbf{X}_{t}=f_{t}(\mathbf{X}_{t})\mathrm{d}t+\sigma_{t}\mathrm{d }\mathbf{B}_{t},\qquad\mathbf{X}_{0}\sim\mathbb{Q}_{0}.\] (1)

Consider now the distribution of this process pinned down at an initial and terminal point \(x_{0},x_{T}\), denoted \(\mathbb{Q}_{|0,T}(\cdot|x_{0},x_{T})\). Under mild assumptions, the _pinned_ process \(\mathbb{Q}_{|0,T}(\cdot|x_{0},x_{T})\) is a _diffusion bridge_ and is given by

\[\mathrm{d}\mathbf{X}_{t}^{0,T}=\{f_{t}(\mathbf{X}_{t}^{0,T})+\sigma_{t}^{2} \nabla\log\mathbb{Q}_{T|t}(x_{T}|\mathbf{X}_{t}^{0,T})\}\mathrm{d}t+\sigma_{t }\mathrm{d}\mathbf{B}_{t},\qquad\mathbf{X}_{0}^{0,T}=x_{0},\] (2)

which satisfies \(\mathbf{X}_{T}^{0,T}=x_{T}\) using Doob \(h\)-transform theory (Rogers and Williams, 2000). Next, we define an independent coupling \(\Pi_{0,T}=\pi_{0}\otimes\pi_{T}\), and let \(\Pi=\Pi_{0,T}\mathbb{Q}_{|0,T}\). This path measure \(\Pi\) is a _mixture of bridges_. We aim to find a Markov diffusion \(\mathrm{d}\mathbf{Y}_{t}=\{f_{t}(\mathbf{Y}_{t})+v_{t}(\mathbf{Y}_{t})\} \mathrm{d}t+\sigma_{t}\mathrm{d}\mathbf{B}_{t}\) on \([0,T]\) which admits the same marginals as \(\Pi\); i.e. for any \(t\in[0,T]\), \(\mathbf{Y}_{t}\sim\Pi_{t}\), so \(\mathbf{Y}_{T}\sim\pi_{T}\). For such \(v_{t}\), a generative model for sampling data distribution \(\pi_{T}\) is obtained by simulating \((\mathbf{Y}_{t})_{t\in[0,T]}\). It can be verified that indeed \(\mathbf{Y}_{t}\sim\Pi_{t}\) for \(v_{t}^{\star}(x_{t})=\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t}}[\nabla\log\mathbb{Q} _{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}=x_{t}]\). We present the theory behind this idea more formally using Markovian projections in Section 3.1. In practice, we do not have access to \(v_{t}^{\star}\) and it is learned using neural networks with regression loss

\[\mathbb{E}_{\Pi_{t,T}}[\|\sigma_{t}^{2}\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_ {T}|\mathbf{X}_{t})-v_{\theta}(t,\mathbf{X}_{t})\|^{2}].\] (3)

For \(f_{t}=0\) and \(\sigma_{t}=\sigma\), \(\mathbb{Q}_{|0,T}\) is a _Brownian Bridge_ and we have

\[\mathbf{X}_{t}^{0,T}=\tfrac{t}{T}x_{T}+(1-\tfrac{t}{T})x_{0}+\sigma_{t}( \mathbf{B}_{t}-\tfrac{t}{T}\mathbf{B}_{T}),\quad\mathrm{d}\mathbf{X}_{t}^{0,T}= \{(x_{T}-\mathbf{X}_{t}^{0,T})/(T-t)\}\mathrm{d}t+\sigma_{t}\mathrm{d}\mathbf{ B}_{t},\] (4)

with \((\mathbf{B}_{t}-\tfrac{t}{T}\mathbf{B}_{T})\sim\mathrm{N}(0,t(1-\tfrac{t}{T}) \,\mathrm{Id})\). The regression loss (3) associated with (4) is given by

\[\mathbb{E}_{\Pi_{t,T}}[\|(\mathbf{X}_{T}-\mathbf{X}_{t})/(T-t)-v_{\theta}(t, \mathbf{X}_{t})\|^{2}].\] (5)

Letting \(\sigma\to 0\), we recover Flow Matching models (see Appendix A.1 for further details).

### Schrodinger Bridges and Optimal Transport

The Schrodinger Bridge (SB) problem (Schrodinger, 1932) consists in finding a path measure \(\mathbb{P}^{\text{SB}}\in\mathcal{P}(\mathcal{C})\) such that

\[\mathbb{P}^{\text{SB}}=\operatorname*{argmin}_{\mathbb{P}}\{\mathrm{KL}( \mathbb{P}|\mathbb{Q})\ :\ \mathbb{P}_{0}=\pi_{0},\ \mathbb{P}_{T}=\pi_{T}\},\] (6)

where \(\mathbb{Q}\in\mathcal{P}(\mathcal{C})\) is a reference path measure. In what follows, we consider \(\mathbb{Q}\) defined by the diffusion process (1) which is Markov, and without loss of generality, we assume \(\mathbb{Q}_{0}=\pi_{0}\). Hence \(\mathbb{P}^{\text{SB}}\) is the path measure closest to \(\mathbb{Q}\) in terms of Kullback-Leibler divergence which satisfies the initial and terminal constraints \(\mathbb{P}^{\text{SB}}_{0}=\pi_{0}\) and \(\mathbb{P}^{\text{SB}}_{T}=\pi_{T}\).

Another crucial property of \(\mathbb{P}^{\text{SB}}\) is that it can also be defined as a mixture of bridges \(\mathbb{P}^{\text{SB}}=\Pi^{\text{SB}}_{0,T}\mathbb{Q}_{|0,T}\), where \(\Pi^{\text{SB}}_{0,T}=\operatorname*{argmin}_{\Pi_{0,T}}\{\mathrm{KL}(\Pi_{0,T }|\mathbb{Q}_{0,T})\ :\ \Pi_{0}=\pi_{0},\ \Pi_{T}=\pi_{T}\}\) is the solution of the _static_ SB problem (Leonard, 2014). In particular, for \(\mathbb{Q}\) associated with \((\sigma\mathbf{B}_{t})_{t\in[0,T]}\) we have

\[\Pi^{\text{SB}}_{0,T}=\operatorname*{argmin}_{\Pi_{0,T}}\{\mathbb{E}_{\Pi_{0,T}}[ ||\mathbf{X}_{0}-\mathbf{X}_{T}||^{2}-2\sigma^{2}T\,\mathrm{H}(\Pi_{0,T})\ :\ \Pi_{0}=\pi_{0},\ \Pi_{T}=\pi_{T}\},\]where \(\mathrm{H}(\mu)\) denotes the entropy, i.e. \(\Pi^{\text{SB}}_{0,T}\) is the solution of the entropy-regularized OT problem. In this case, the SB can also be obtained theoretically by solving the following problem (Dai Pra, 1991)

\[v_{\text{SB}}=\operatorname{argmin}_{v}\{\int_{0}^{T}\mathbb{E}_{ \mathbb{P}_{t}}[||v(t,\mathbf{X}_{t})||^{2}]\mathrm{d}t\ :\ \mathrm{d}\mathbf{X}_{t}=v(t,\mathbf{X}_{t})\mathrm{d}t+\sigma\mathrm{d} \mathbf{B}_{t},\ \ \mathbb{P}_{0}=\pi_{0},\ \mathbb{P}_{T}=\pi_{T}\}.\]

Then \(\mathbb{P}^{\text{SB}}\) is given by the SDE with drift \(v_{\text{SB}}\) initialized with \(\mathbf{X}_{0}\sim\pi_{0}\). For \(\sigma=0\), we recover the classical OT problem and the Benamou-Brenier formula (Benamou and Brenier, 2000).

A common approach to solve (6) is the Iterative Proportional Fitting (IPF) method (Fortet, 1940; Kullback, 1968; Ruschendorf, 1995) defining a sequence of path measures \((\tilde{\mathbb{P}}^{n})_{n\in\mathbb{N}}\) where

\[\tilde{\mathbb{P}}^{2n+1}=\operatorname{argmin}_{\tilde{\mathbb{P}}}\{\mathrm{ KL}(\tilde{\mathbb{P}}|\tilde{\mathbb{P}}^{2n})\ :\ \tilde{\mathbb{P}}_{T}=\pi_{T}\},\ \tilde{\mathbb{P}}^{2n+2}= \operatorname{argmin}_{\tilde{\mathbb{P}}}\{\mathrm{KL}(\tilde{\mathbb{P}}| \tilde{\mathbb{P}}^{2n+1})\ :\ \tilde{\mathbb{P}}_{0}=\pi_{0}\},\] (7)

with initialization \(\tilde{\mathbb{P}}^{0}=\mathbb{Q}\). This procedure alternates between projections on the set of path measures with given initial distribution \(\pi_{0}\) and terminal distribution \(\pi_{T}\). It can be shown (De Bortoli et al., 2021) that \((\tilde{\mathbb{P}}^{n})_{n\in\mathbb{N}}\) are associated with diffusions and that for any \(n\in\mathbb{N}\), \(\tilde{\mathbb{P}}^{2n+1}\) is the time-reversal of \(\tilde{\mathbb{P}}^{2n}\) with initialization \(\pi_{T}\), and \(\tilde{\mathbb{P}}^{2n+2}\) is the time-reversal of \(\tilde{\mathbb{P}}^{2n+1}\) with initialization \(\pi_{0}\). Leveraging this property, De Bortoli et al. (2021) proposed Diffusion Schrodinger Bridge (DSB), an algorithm which learns the time-reversals iteratively. In particular, DDMs can be seen as the first iteration of DSB.

## 3 Iterative Markovian Fitting

### Markovian Projection and Reciprocal Projection

Markovian Projection.Projecting on Markov measures is a key ingredient in our methodology and in the Bridge Matching framework. This concept was introduced multiple times in the literature (Gyongy, 1986; Peluchetti, 2021; Liu et al., 2022b). In particular, we focus on Markovian projection of path measures given by a mixture of bridges \(\Pi=\Pi_{0,T}\mathbb{Q}_{|0,T}\in\mathcal{P}(\mathcal{C})\).

**Definition 1**.: _Assume that \(\mathbb{Q}\) is given by (1) and that for any \((x_{0},x_{T})\in\mathbb{R}^{d}\), \(\mathbb{Q}_{|0,T}(\cdot|x_{0},x_{T})\) is associated with \((\mathbf{X}_{t}^{0,T})_{t\in[0,T]}\) given by \(\mathrm{d}\mathbf{X}_{t}^{0,T}=\{f_{t}(\mathbf{X}_{t}^{0,T})+\sigma_{t}^{2} \nabla\log\mathbb{Q}_{T|t}(x_{T}|\mathbf{X}_{t}^{0,T})\}\mathrm{d}t+\sigma_{t }\mathrm{d}\mathbf{B}_{t}\), with \(\sigma:\ [0,T]\rightarrow(0,+\infty)\). Then, when it is well-defined, we introduce the Markovian projection of \(\Pi\), \(\mathbb{M}^{\star}=\operatorname{proj}_{\mathcal{M}}(\Pi)\in\mathcal{M}\), which is associated with the SDE_

\[\mathrm{d}\mathbf{X}_{t}^{\star}=\{f_{t}(\mathbf{X}_{t}^{\star})+v_{t}^{\star }(\mathbf{X}_{t}^{\star})\}\mathrm{d}t+\sigma_{t}\mathrm{d}\mathbf{B}_{t}, \qquad v_{t}^{\star}(x_{t})=\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t}}[\nabla\log \mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}=x_{t}].\]

Note that in our definition \(\sigma_{t}>0\) so \(\nabla\log\mathbb{Q}_{T|t}(x_{T}|x_{t})\) is well-defined, but Flow Matching can be recovered as the _deterministic_ case in the limit \(\sigma_{t}=\sigma\to 0\). In the following proposition, we show that the Markovian projection is indeed a projection for the _reverse_ Kullback-Leibler divergence, and that it preserves marginals of \(\Pi_{t}\).

**Proposition 2**.: _Assume that \(\sigma_{t}>0\). Let \(\mathbb{M}^{\star}=\operatorname{proj}_{\mathcal{M}}(\Pi)\). Then, under mild assumptions, we have_

\[\mathbb{M}^{\star}=\operatorname{argmin}_{\mathbb{M}}\{\mathrm{KL}( \Pi|\mathbb{M})\ :\ \mathbb{M}\in\mathcal{M}\},\] \[\mathrm{KL}(\Pi|\mathbb{M}^{\star})=\tfrac{1}{2}\int_{0}^{T} \mathbb{E}_{\Pi_{0,t}}[\|\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|0,t}}[\nabla\log \mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{0},\mathbf{X}_ {t}]-v_{t}^{\star}(\mathbf{X}_{t})\|^{2}]/\sigma_{t}^{2}\mathrm{d}t.\]

_In addition, we have that for any \(t\in[0,T]\), \(\mathbb{M}_{t}^{\star}=\Pi_{t}\). In particular, \(\mathbb{M}_{T}^{\star}=\Pi_{T}\)._

Reciprocal Projection.While the Markovian projection ensures that the obtained measure is Markov, the associated _bridge_ measure is not preserved in general, i.e. \(\operatorname{proj}_{\mathcal{M}}(\Pi)_{|0,T}\neq\Pi_{|0,T}=\mathbb{Q}_{|0,T}\). Measures with same bridge as \(\mathbb{Q}\) are said to be in its _reciprocal class_(Leonard et al., 2014).

**Definition 3**.: \(\Pi\in\mathcal{P}(\mathcal{C})\) _is in the reciprocal class \(\mathcal{R}(\mathbb{Q})\) of \(\mathbb{Q}\in\mathcal{M}\) if \(\Pi=\Pi_{0,T}\mathbb{Q}_{|0,T}\). We define the reciprocal projection of \(\mathbb{P}\in\mathcal{P}(\mathcal{C})\) as \(\Pi^{\star}=\operatorname{proj}_{\mathcal{R}(\mathbb{Q})}(\mathbb{P})=\mathbb{P}_ {0,T}\mathbb{Q}_{|0,T}\)._

Similarly to Proposition 2, we have the following result, which justifies the term reciprocal projection.

**Proposition 4**.: _Let \(\mathbb{P}\in\mathcal{P}(\mathcal{C})\), \(\Pi^{\star}=\operatorname{proj}_{\mathcal{R}(\mathbb{Q})}(\mathbb{P})\). Then, \(\Pi^{\star}=\operatorname{argmin}_{\Pi}\{\mathrm{KL}(\mathbb{P}|\Pi)\ :\ \Pi\in\mathcal{R}(\mathbb{Q})\}\)._

The reciprocal projection \(\Pi^{\star}\) of a Markov path measure \(\mathbb{M}\) does not preserve the Markov property in general. In fact, the Schrodinger Bridge is the _unique_ path measure which satisfies the initial and terminal conditions, is Markov and is in the reciprocal class of \(\mathbb{Q}\), see (Leonard, 2014).

**Proposition 5**.: _Let \(\mathbb{P}\) be a Markov measure in the reciprocal class of \(\mathbb{Q}\) such that \(\mathbb{P}_{0}=\pi_{0}\), \(\mathbb{P}_{T}=\pi_{T}\). Then, under assumptions on \(\mathbb{Q}\), \(\pi_{0}\) and \(\pi_{T}\), \(\mathbb{P}\) is unique and is equal to the Schrodinger Bridge \(\mathbb{P}^{\text{SB}}\)._

### Iterative Markovian Fitting

Based on Proposition 5, we propose a novel methodology called _Iterative Markovian Fitting_ (IMF) to solve Schrodinger Bridges. We consider a sequence \((\mathbb{P}^{n})_{n\in\mathbb{N}}\) such that

\[\mathbb{P}^{2n+1}=\mathrm{proj}_{\mathcal{M}}(\mathbb{P}^{2n}),\qquad\mathbb{P}^ {2n+2}=\mathrm{proj}_{\mathcal{R}(\mathbb{Q})}(\mathbb{P}^{2n+1}),\] (8)

with \(\mathbb{P}^{0}\) such that \(\mathbb{P}^{0}_{0}=\pi_{0}\), \(\mathbb{P}^{0}_{T}=\pi_{T}\) and \(\mathbb{P}^{0}\in\mathcal{R}(\mathbb{Q})\). These updates correspond to alternatively performing Markovian projections and reciprocal projections.

Combining Proposition 2 and Definition 3, we get that for any \(n\in\mathbb{N}\), \(\mathbb{P}^{n}_{0}=\pi_{0}\) and \(\mathbb{P}^{n}_{T}=\pi_{T}\). This property is in contrast to the IPF algorithm (7) for which the marginals at the initial and final times are _not_ preserved. We highlight this duality between IPF (7) and IMF (8) in Table 1.

We conclude this section with a theoretical analysis of IMF. First, we start by showing a Pythagorean theorem for both the Markovian projection and the reciprocal projection.

**Lemma 6**.: _Under mild assumptions, if \(\mathbb{M}\in\mathcal{M}\), \(\Pi\in\mathcal{R}(\mathbb{Q})\) and \(\mathrm{KL}(\Pi|\mathbb{M})<+\infty\), we have_

\[\mathrm{KL}(\Pi|\mathbb{M})=\mathrm{KL}(\Pi|\mathrm{proj}_{\mathcal{M}}(\Pi)) +\mathrm{KL}(\mathrm{proj}_{\mathcal{M}}(\Pi)|\mathbb{M}).\]

_If \(\mathrm{KL}(\mathbb{M}|\Pi)<+\infty\), we have_

\[\mathrm{KL}(\mathbb{M}|\Pi)=\mathrm{KL}(\mathbb{M}|\mathrm{proj}_{\mathcal{R} (\mathbb{Q})}(\mathbb{M}))+\mathrm{KL}(\mathrm{proj}_{\mathcal{R}(\mathbb{Q}) }(\mathbb{M})|\Pi).\]

Using Lemma 6, we have the following proposition.

**Proposition 7**.: _Under mild assumptions, we have \(\mathrm{KL}(\mathbb{P}^{n+1}|\mathbb{P}^{\text{SB}})\leq\mathrm{KL}(\mathbb{P} ^{n}|\mathbb{P}^{\text{SB}})<\infty\), and \(\lim_{n\to+\infty}\mathrm{KL}(\mathbb{P}^{n}|\mathbb{P}^{n+1})=0\)._

Hence, for the IMF sequence \((\mathbb{P}^{n})_{n\in\mathbb{N}}\), the Markov path measures \((\mathbb{P}^{2n+1})_{n\in\mathbb{N}}\) are getting closer to the reciprocal class, while the reciprocal path measures \((\mathbb{P}^{2n+2})_{n\in\mathbb{N}}\) are getting closer to the set of Markov measures. Proposition 7 should be compared with (Ruschendorf, 1995, Proposition 2.1, Equation (2.16)) which shows that, for the IPF sequence \((\mathbb{P}^{n})_{n\in\mathbb{N}}\), we have \(\lim_{n\to+\infty}\mathrm{KL}(\mathbb{P}^{n+1}|\mathbb{P}^{n})=0\). This result is similar to Proposition 7 but for the _forward_ Kullback-Leibler divergence.

Using Proposition 7, we finally prove the convergence of the IMF sequence \((\mathbb{P}^{n})_{n\in\mathbb{N}}\) to the Schrodinger Bridge. This result was first shown in the concurrent work (Peluchetti, 2023, Theorem 2). We present a simpler proof in Appendix C.6.

**Theorem 8**.: _Under mild assumptions, the IMF sequence \((\mathbb{P}^{n})_{n\in\mathbb{N}}\) admits a unique fixed point \(\mathbb{P}^{\star}=\mathbb{P}^{\text{SB}}\), and \(\lim_{n\to+\infty}\mathrm{KL}(\mathbb{P}^{n}|\mathbb{P}^{\star})=0\)._

## 4 Diffusion Schrodinger Bridge Matching

In this section, we present Diffusion Schrodinger Bridge Matching (DSBM), a practical algorithm for solving the SB problem obtained by combining the IMF procedure with Bridge Matching.

Iterative Markovian Fitting in practice.IMF alternatively projects on the Markov class \(\mathcal{M}\) and the reciprocal class \(\mathcal{R}(\mathbb{Q})\). We denote \(\mathbb{M}^{n+1}=\mathbb{P}^{2n+1}\in\mathcal{M}\) and \(\Pi^{n}=\mathbb{P}^{2n}\in\mathcal{R}(\mathbb{Q})\). Assuming we know how to sample from the bridge \(\mathbb{Q}_{0,T}\) given the initial and terminal conditions, sampling from the reciprocal projection \(\mathrm{proj}_{\mathcal{R}(\mathbb{Q})}(\mathbb{M})\) is simple: First, sample \((\mathbf{X}_{0},\mathbf{X}_{T})\) from the joint distribution \(\mathbb{M}_{0,T}\).5 Then, sample from the bridge \(\mathbb{Q}_{0,T}(\cdot|\mathbf{X}_{0},\mathbf{X}_{T})\). The bottleneck of IMF is in the computation of Markovian projections. By Definition 1, \(\mathbb{M}^{\star}=\mathrm{proj}_{\mathcal{M}}(\Pi)\) is associated with the process

Footnote 5: In practice, we sample the SDE associated with \(\mathbb{M}\) and save a batch of joint samples \((\mathbf{X}_{0},\mathbf{X}_{T})\). This is similar to the _trajectory caching_ procedure in De Bortoli et al. (2021), but we only retain initial and final samples.

\[\mathrm{d}\mathbf{X}_{t}=\{f_{t}(\mathbf{X}_{t})+v_{\theta^{\star}}(t,\mathbf{ X}_{t})\}\mathrm{d}t+\sigma_{t}\mathrm{d}\mathbf{B}_{t},\qquad\mathbf{X}_{0} \sim\pi_{0},\] (9) \[\theta^{\star}=\mathrm{argmin}_{\theta}\{\int_{0}^{T}\mathbb{E}_{ \Pi_{t,T}}[\|\sigma_{t}^{2}\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_ {t})-v_{\theta}(t,\mathbf{X}_{t})\|^{2}]/\sigma_{t}^{2}\mathrm{d}t\ :\ \theta\in\Theta\},\] (10)where \(\{v_{\theta}\ :\ \theta\in\Theta\}\) is a parametric family of functions, usually given by a neural network. The optimal \(v_{\theta^{*}}(t,x_{t})=\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t}}[\nabla\log\mathbb{Q }_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}=x_{t}]\) for any \(t\in[0,T]\) and \(x_{t}\in\mathbb{R}^{d}\).

With the above two procedures for computing \(\mathrm{proj}_{\mathcal{R}(\mathbb{Q})}(\mathbb{M})\) and \(\mathrm{proj}_{\mathcal{M}}(\Pi)\), we can now describe a numerical method implementing IMF (8). Let \(\Pi^{0}=\Pi^{0}_{0,T}\mathbb{Q}_{|0,T}\) where \(\Pi^{0}_{0}=\pi_{0}\), \(\Pi^{0}_{T}=\pi_{T}\). Learn \(\mathbb{M}^{1}\approx\mathrm{proj}_{\mathcal{M}}(\Pi^{0})\) given by (9) with \(v_{\theta^{*}}\) given by (10). Next, sample from \(\Pi^{1}=\mathrm{proj}_{\mathcal{R}(\mathbb{Q})}(\mathbb{M}^{1})=\mathbb{M}^{1 }_{0,T}\mathbb{Q}_{|0,T}\) by sampling from \(\mathbb{M}^{1}_{0,T}\) and reconstructing the bridge \(\mathbb{Q}_{|0,T}\). We iterate the process to obtain a sequence \((\Pi^{n},\mathbb{M}^{n+1})_{n\in\mathbb{N}}\). In practice, this algorithm performs poorly (see Figure 3), since the approximate minimization (10) for computing \(\mathbb{M}^{n+1}\) may not admit \(\mathbb{M}^{n+1}_{T}=\pi_{T}\) exactly as in Proposition 2. Instead, we incur a bias between \(\mathbb{M}^{n+1}_{T}\) and \(\pi_{T}\) which accumulates for each \(n\in\mathbb{N}\).

To mitigate this problem, we alternate between a _forward_ Markovian projection and a _backward_ Markovian projection. This procedure is justified by the following proposition.

**Proposition 9**.: _Assume that \(\Pi=\Pi_{0,T}\mathbb{Q}_{|0,T}\) with \(\mathbb{Q}\) associated with \(\mathrm{d}\mathbf{X}_{t}=f_{t}(\mathbf{X}_{t})\mathrm{d}t+\sigma_{t}\mathrm{d }\mathbf{B}_{t}\). Under mild conditions, the Markovian projection \(\mathbb{M}^{*}=\mathrm{proj}_{\mathcal{M}}(\Pi)\) is associated with both_

\[\mathrm{d}\mathbf{X}_{t} =\{f_{t}(\mathbf{X}_{t})+\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t}}[ \nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}] \}\mathrm{d}t+\sigma_{t}\mathrm{d}\mathbf{B}_{t},\quad\mathbf{X}_{0}\sim\Pi_{0},\] (11) \[\mathrm{d}\mathbf{Y}_{t} =\{-f_{T-t}(\mathbf{Y}_{t})+\sigma_{T-t}^{2}\mathbb{E}_{\Pi_{0|T- t}}[\nabla\log\mathbb{Q}_{T-t|0}(\mathbf{Y}_{t}|\mathbf{Y}_{T})\mid\mathbf{Y}_{t}] \}\mathrm{d}t+\sigma_{T-t}\mathrm{d}\mathbf{B}_{t},\mathbf{Y}_{0}\sim\Pi_{T}.\] (12)

In Proposition 9, (11) is the definition of the Markovian projection, see Definition 1. However, (12) is an equivalent representation as a _time-reversal_. In practice, \((\mathbf{Y}_{t})_{t\in[0,T]}\) is approximated with

\[\mathrm{d}\mathbf{Y}_{t}=\{-f_{T-t}(\mathbf{Y}_{t})+v_{\phi^{*}}( T-t,\mathbf{Y}_{t})\}\mathrm{d}t+\sigma_{T-t}\mathrm{d}\mathbf{B}_{t},\qquad \mathbf{Y}_{0}\sim\pi_{T},\] (13) \[\phi^{*}=\mathrm{argmin}_{\phi}\{\int_{0}^{T}\mathbb{E}_{\Pi_{0,t }}[\|\sigma_{t}^{2}\nabla\log\mathbb{Q}_{t|0}(\mathbf{X}_{t}|\mathbf{X}_{0})-v _{\phi}(t,\mathbf{X}_{t})\|^{2}]/\sigma_{t}^{2}\mathrm{d}t\ :\ \phi\in\Phi\}.\] (14)

The optimal \(v_{\phi^{*}}(t,x_{t})=\sigma_{t}^{2}\mathbb{E}_{\Pi_{0|t}}[\nabla\log\mathbb{Q }_{t|0}(\mathbf{X}_{t}|\mathbf{X}_{0})\mid\mathbf{X}_{t}=x_{t}]\) for any \(t\in[0,T]\) and \(x_{t}\in\mathbb{R}^{d}\).

```
1:Input: Joint distribution \(\Pi^{0}_{0,T}\), tractable bridge \(\mathbb{Q}_{|0,T}\), number of outer iterations \(N\in\mathbb{N}\).
2: Let \(\Pi^{0}=\Pi^{0}_{0,T}\mathbb{Q}_{|0,T}\).
3:for\(n\in\{0,\dots,N-1\}\)do
4: Learn \(v_{\phi^{*}}\) using (14) with \(\Pi=\Pi^{2n}\).
5: Let \(\mathbb{M}^{2n+1}\) be given by (13).
6: Let \(\Pi^{2n+1}=\mathbb{M}^{2n+1}_{0,T}\mathbb{Q}_{|0,T}\).
7: Learn \(v_{\theta}\). using (10) with \(\Pi=\Pi^{2n+1}\).
8: Let \(\mathbb{M}^{2n+2}\) be given by (9).
9: Let \(\Pi^{2n+2}=\mathbb{M}^{2n+2}_{0,T}\mathbb{Q}_{|0,T}\).
10:endfor
11:Output:\(v_{\theta^{*}}\), \(v_{\phi^{*}}\). ```

**Algorithm 1** Diffusion Schrodinger Bridge Matching

Note that \(\mathbf{X}_{0}\sim\pi_{0}\) in the forward projection, while \(\mathbf{Y}_{0}\sim\pi_{T}\) in the backward projection. Therefore, using the backward projection removes the bias on \(\pi_{T}\) accumulated from the forward projection. Leveraging the time-symmetry of the Markovian projection and alternating between (13) and (9) yields the DSBM methodology summarized in Algorithm 1.

It is also possible to learn _both_ the forward and backward processes at each step, and enforce that the backward and forward processes match. We explore this in Appendix G.

Initialization coupling.We now relate Algorithm 1 to the classical IPF and practical algorithms such as DSB (De Bortoli et al., 2021). Instead of initializing DSBM with \(\Pi^{0}_{0,T}\) given by a coupling between \(\pi_{0},\pi_{T}\), if we initialize it by \(\Pi^{0}_{0,T}=\mathbb{Q}_{0,T}\) where \(\mathbb{Q}_{0}=\pi_{0}\) and \(\mathbb{Q}_{T|0}\) is given by the reference process defined in (1), then DSBM also recovers the IPF iterates used in DSB.

**Proposition 10**.: _Suppose the families of functions \(\{v_{\theta}\ :\ \theta\in\Theta\}\) and \(\{v_{\phi}\ :\ \phi\in\Phi\}\) are rich enough so that they can model the optimal vector fields. Let \((\Pi^{n},\mathbb{M}^{n+1})_{n\in\mathbb{N}}\) be the optimal DSBM sequence in Algorithm 1 initialized with \(\Pi^{0}_{0,T}=\mathbb{Q}_{0,T}\), and let \((\tilde{\mathbb{P}}^{n})_{n\in\mathbb{N}}\) be the optimal DSB sequence given by the IPF iterates in (7). Then for any \(n\in\mathbb{N},n\geq 1\), we have \(\mathbb{M}^{n}=\tilde{\mathbb{P}}^{n}\)._

We will thus call DSBM-IPF, the DSBM algorithm initialized with the joint distribution given by the forward reference process \(\Pi^{0}_{0,T}=\mathbb{Q}_{0,T}\); and DSBM-IMF, the DSBM algorithm initialized with an independent coupling \(\Pi^{0}_{0,T}=\pi_{0}\otimes\pi_{T}\). However, the training procedure of DSBM-IPF is very different from the one of (De Bortoli et al., 2021; Chen et al., 2022). In existing works, \(\tilde{\mathbb{P}}^{n+1}\) is obtained as the time-reversal of \(\mathbb{P}^{n}\) which requires full trajectories from \(\mathbb{P}^{n}\), see e.g. (De Bortoli et al., 2021, Proposition 6). In contrast, in Algorithm 1 we only use the _coupling_\(\mathbb{M}^{n}_{0,T}\) to create the bridge measure \(\Pi^{n}=\mathbb{M}^{n}_{0,T}\mathbb{Q}_{|0,T}\). By doing so, (i) the losses (10) and (14) can be easily evaluated at any time \(t\in[0,T]\); (ii) the _trajectory caching_ procedure in DSBM is more computationally and memory efficient; (iii) while every IPF iteration \(\bar{\mathbb{P}}^{n}\) is also supposed to be in \(\mathcal{R}(\mathbb{Q})\), in practice one can observe a _forgetting_ of the bridge \(\mathbb{Q}_{|0,T}\)(Fernandes et al., 2021). In DSBM, this effect is countered by explicit projections on the reciprocal class. See Appendix F for more details.

Probability flow ODE.At equilibrium of DSBM, we have that \((\mathbf{Y}_{t})_{t\in[0,T]}\) given by (13) is the time reversal of \((\mathbf{X}_{t})_{t\in[0,T]}\) given by (9) and are both associated with the optimal Schrodinger Bridge path measure \(\mathbb{P}^{\star}\). As a result, we have that \(v_{\phi^{\star}}(t,x)=-v_{\theta^{\star}}(t,x)+\sigma_{t}^{2}\nabla\log \mathbb{P}_{t}^{\star}(x)\). Hence, a probability flow \((\mathbf{Z}_{t}^{\star})_{t\in[0,T]}\) such that \(\mathrm{Law}(\mathbf{Z}_{t}^{\star})=\mathbb{P}_{t}^{\star}\) for any \(t\in[0,T]\) is given by

\[\mathrm{d}\mathbf{Z}_{t}^{\star}=\{f_{t}(\mathbf{Z}_{t}^{\star})+\tfrac{1}{2}[ v_{\theta^{\star}}(t,\mathbf{Z}_{t}^{\star})-v_{\phi^{\star}}(t,\mathbf{Z}_{t}^{ \star})]\}\mathrm{d}t,\qquad\mathbf{Z}_{0}^{\star}\sim\pi_{0}.\]

See also De Bortoli et al. (2021); Chen et al. (2022) for derivation of this result. Note however that the path measure induced by \((\mathbf{Z}_{t}^{\star})_{t\in[0,T]}\) does not correspond to \(\mathbb{P}^{\star}\); in particular, \((\mathbf{Z}_{0}^{\star},\mathbf{Z}_{T}^{\star})\) is _not_ an entropic OT plan. However, since for any \(t\in[0,T]\), \(\mathbf{Z}_{t}^{\star}\) has marginal distribution \(\mathbb{P}_{t}^{\star}\), we can compute the log-likelihood of the model (Song et al., 2021; Huang et al., 2021).

## 5 Related Work

Markovian projection and Bridge Matching.The concept of Markovian projection has been rediscovered multiple times (Krylov, 1984; Gyongy, 1986; Dupire, 1994). In the machine learning context, this was first proposed by Peluchetti (2021) to define Bridge Matching models. More recently, Liu et al. (2022b) derived theoretical properties of the Markovian projection in Proposition 2, first part of Lemma 6, and applied Bridge Matching for learning data on discrete and constrained domains.

Bridge and Flow Matching.Flow Matching corresponds to deterministic bridges with deterministic samplers (ODEs) and has been under active study (Liu et al., 2023b; Liu, 2022; Lippan et al., 2023; Albergo and Vanden-Eijnden, 2023; Heitz et al., 2023; Pooladian et al., 2023; Tong et al., 2023). Denoising Diffusion Implicit Models (DDIM) (Song et al., 2021) can also be formulated as a discrete-time version of Flow Matching, see Liu et al. (2023b). These models have been extended to the Riemannian setting by Chen and Lipman (2023). Recently, Albergo et al. (2023) studied the influence of stochasticity in the bridge, through the concept of stochastic interpolants. Liu et al. (2023a); Delbracio and Milanfar (2023) used Bridge Matching to perform image restoration tasks and noted benefits of stochasticity empirically. Closely related to our work is the Rectified Flow algorithm of Liu et al. (2023b), which corresponds to an iterative Flow Matching procedure in order to improve the straightness of the flow and thus eases its simulation. An iterative rectifying procedure using stochastic interpolants is also proposed in (Albergo et al., 2023, Section 3.5). Our proposed DSBM-IMF algorithm is closest to Rectified Flow, which can be seen as the deterministic limiting case of DSBM-IMF as \(\sigma\to 0\). However, there are a few important theoretical and practical differences. Most notably, we adopt the SDE approach which is crucial for the validity of Proposition 5 as well as for the empirical performance of DSBM. We discuss further distinctions between DSBM and Rectified Flow in Appendix A.3.

Diffusion Schrodinger Bridge.Schrodinger Bridges (Schrodinger, 1932) are ubiquitous in probability theory (Leonard, 2014b) and stochastic control (Dai Pra, 1991; Chen et al., 2021). More recently, they have been used for generative modeling: De Bortoli et al. (2021) introduced the DSB algorithm and Vargas et al. (2021); Chen et al. (2022) introduced similar algorithms. The case of Dirac delta terminal distribution was investigated by Wang et al. (2021). These methods were later extended to solve conditional simulation and more general control problems (Shi et al., 2022; Thornton et al., 2022; Liu et al., 2022a; Chen et al., 2023; Tamir et al., 2023). In Somnath et al. (2023), SBs are learned using one Bridge Matching iteration, assuming access to the true Schrodinger static coupling. Our proposed method DSBM-IPF is closest to DSB, but with improved continous-time training and projections on the reciprocal class which mitigate two limitations of DSB. Concurrently with our work, Peluchetti (2023) independently introduced the DSBM-IMF approach (named IDBM therein).

[MISSING_PAGE_FAIL:8]

the variance estimates become inaccurate for RF and IMF-b. Among SB methods, DSB and IMF-b also gave inaccurate SB covariance estimates as the number of iteration increases. On the other hand, DSBM does not suffer from this issue. In Table 3, we further quantify the accuracy and compare with SB-CFM (Tong et al., 2023) by computing the KL divergence between the marginal distributions of the learned process \(\mathbb{P}_{t}\) and the true SB \(\mathbb{P}_{t}^{\text{SB}}\). Our proposed methods achieve similar KL divergence as SB-CFM in dimension \(d=5\), but are much more accurate in higher dimensions.

MNIST, EMNIST transfer.We test our method for domain transfer between MNIST digits and EMNIST letters as in De Bortoli et al. (2021). We compare DSBM as a direct substitute of DSB, and also with Bridge Matching (BM) (Peluchetti, 2021; Liu et al., 2022b), CFM, OT-CFM and RF. We plot some output samples from different algorithms in Figure 4 and the convergence of FID score in Figure 5. We find that OT-CFM becomes less applicable in higher dimensions and produces samples of worse quality (Figure 3(a)). On the other hand, image quality deteriorates during training of DSB and RF. DSBM achieves higher quality samples visually, and does not suffer from deterioration. It is also about \(30\%\) more efficient than DSB in terms of runtime.

CelebA transfer.Next, we evaluate and perform some ablations of our method on a transfer task on the CelebA \(64\times 64\) dataset. We consider the images given by the tokens male/old and female/young. In Figures 6 and 7, we show that as \(\sigma\) increases, the quality of the images (as measured by the FID score) increases until \(\sigma\) is too high, but the alignment (as measured by LPIPS) between the generated image and the original sample decreases. Additionally, we investigate the dependency between \(\sigma\) and image dimension in Figure 8. In particular, for the same \(\sigma=1\), the outputs of DSBM for CelebA \(128\times 128\) are better aligned with the original data than for CelebA \(64\times 64\). This is in agreement with the observations of Chen (2023); Hoogeboom et al. (2023) that the _noise schedule_ in diffusion models should scale with the resolution.

AFHQ transfer.We demonstrate the scalability of our method on an additional transfer experiment on the AFHQ \(512\times 512\) dataset between the classes cat and wild. The results are shown in Figure 9. On this higher-dimensional problem, we observe that DSBM can also generate realistic samples which are similar to the input.

Unpaired Fluid Flows Downscaling.Finally, we apply DSBM to perform downscaling of geophysical fluid dynamics, i.e. super-resolution of low resolution spatial data. We use the dataset in (Bischoff and Deck, 2023), which consists of unpaired low (\(64\times 64\)) and high (\(512\times 512\)) resolution fields. As shown in Figure 10, DSBM is able to learn high resolution reconstructions by only slightly noising the low resolution input. In contrast, Bischoff and Deck (2023) use two diffusion models in forward and backward directions (Diffusion-fb) based on Meng et al. (2022), which improves over the Random baseline. Figure 11 shows that DSBM-IPF and DSBM-IMF achieve much lower \(\ell_{2}\)

Figure 4: Samples of MNIST digits transferred from letters. Figure 5: FID vs iteration.

distances for all frequency classes in the dataset than Diffusion-fb (and thus Random), indicating DSBM is able to reconstruct high resolution fields consistent with the low resolution source.

## 7 Discussion

In this work, we introduce IMF, a new methodology for learning Schrodinger Bridges. IMF is an alternative to the classical IPF and can be interpreted as its dual. Building on this new framework, we present two practical algorithms, DSBM-IPF and DSBM-IMF, for learning SBs. These algorithms mitigate the time-discretization and bias accumulation issues of existing methods. However, DSBM still has some limitations. First, our results suggest DSBM is most effective for solving general transport problems. For generative modeling, we only find minor improvements compared to Bridge and Flow Matching on CIFAR-10 (see Appendix I.6). Second, while DSBM is more efficient than DSB, it still requires sampling from the learned process during the caching step. Finally, the EOT problem becomes more difficult to solve numerically for small values of \(\sigma\).

In future work, we would like to further investigate the differences between DSBM-IMF and DSBM-IPF, IMF also appears useful for developing a better understanding of the Rectified Flow algorithm (Liu et al., 2023b), as IMF minimizes a clear objective (6) and Rectified Flow can be seen as a limiting case of it. Finally, Rectified Flow has also been extended to solve OT problems with general convex costs by Liu (2022), and it would be interesting to derive a SB version of this extension.

## Acknowledgements

YS acknowledges support from the Huawei UK Fellowship. AC acknowledges support from the EPSRC CDT in Modern Statistics and Statistical Machine Learning (EP/S023151/1). AD acknowledges support of the UK Dstl and EPSRC grant EP/R013616/1. This is part of the collaboration between US DOD, UK MOD and UK EPSRC under the Multidisciplinary University Research Initiative. He also acknowledges support from the EPSRC grants CoSines (EP/R034710/1) and Bayes4Health (EP/R018561/1).

## References

* Albergo et al. (2023) Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. (2023). Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_.
* Albergo and Vanden-Eijnden (2023) Albergo, M. S. and Vanden-Eijnden, E. (2023). Building normalizing flows with stochastic interpolants. In _International Conference on Learning Representations_.
* Anderson (1982) Anderson, B. D. (1982). Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326.
* Banerjee et al. (2005) Banerjee, A., Guo, X., and Wang, H. (2005). On the optimality of conditional expectation as a Bregman predictor. _IEEE Transactions on Information Theory_, 51(7):2664-2669.
* Barczy and Kern (2013) Barczy, M. and Kern, P. (2013). Representations of multidimensional linear process bridges. _Random Operators and Stochastic Equations_, 21(2):159-189.
* Benamou and Brenier (2000) Benamou, J.-D. and Brenier, Y. (2000). A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem. _Numerische Mathematik_, 84(3):375-393.
* Bernton et al. (2019) Bernton, E., Heng, J., Doucet, A., and Jacob, P. E. (2019). Schrodinger bridge samplers. _arXiv preprint arXiv:1912.13170_.
* Bischoff and Deck (2023) Bischoff, T. and Deck, K. (2023). Unpaired downscaling of fluid flows with diffusion bridges. _arXiv preprint arXiv:2305.01822_.
* Bogachev et al. (2021) Bogachev, V. I., Krasovitskii, T. I., and Shaposhnikov, S. V. (2021). On uniqueness of probability solutions of the Fokker-Planck-Kolmogorov equation. _Sbornik: Mathematics_, 212(6):745.
* Bunne et al. (2023) Bunne, C., Hsieh, Y.-P., Cuturi, M., and Krause, A. (2023). The Schrodinger bridge between Gaussian measures has a closed form. In _International Conference on Artificial Intelligence and Statistics_.
* Bunne et al. (2022) Bunne, C., Papaxanthos, L., Krause, A., and Cuturi, M. (2022). Proximal optimal transport modeling of population dynamics. In _International Conference on Artificial Intelligence and Statistics_, pages 6511-6528. PMLR.
* Caluya and Halder (2021) Caluya, K. F. and Halder, A. (2021). Wasserstein proximal algorithms for the Schrodinger bridge problem: Density control with nonlinear drift. _IEEE Transactions on Automatic Control_, 67(3):1163-1178.
* Chen and Lipman (2023) Chen, R. T. and Lipman, Y. (2023). Riemannian flow matching on general geometries. _arXiv preprint arXiv:2302.03660_.
* Chen (2023) Chen, T. (2023). On the importance of noise scheduling for diffusion models. _arXiv preprint arXiv:2301.10972_.
* Chen et al. (2023) Chen, T., Liu, G.-H., Tao, M., and Theodorou, E. A. (2023). Deep momentum multi-marginal Schrodinger bridge. _arXiv preprint arXiv:2303.01751_.
* Chen et al. (2022) Chen, T., Liu, G.-H., and Theodorou, E. A. (2022). Likelihood training of Schrodinger bridge using forward-backward SDEs theory. In _International Conference on Learning Representations_.
* Chen et al. (2016) Chen, Y., Georgiou, T., and Pavon, M. (2016). Entropic and displacement interpolation: a computational approach using the Hilbert metric. _SIAM Journal on Applied Mathematics_, 76(6):2375-2396.

Chen, Y., Georgiou, T. T., and Pavon, M. (2021). Optimal transport in systems and control. _Annual Review of Control, Robotics, and Autonomous Systems_, 4.
* Choi et al. (2020) Choi, Y., Uh, Y., Yoo, J., and Ha, J.-W. (2020). Stargan v2: Diverse image synthesis for multiple domains. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_.
* Chung and Walsh (2006) Chung, K. L. and Walsh, J. B. (2006). _Markov processes, Brownian motion, and Time Symmetry_, volume 249. Springer Science & Business Media.
* Csiszar (1975) Csiszar, I. (1975). I-divergence geometry of probability distributions and minimization problems. _The Annals of Probability_, 3(1):146-158.
* Cuturi (2013) Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. In _Advances in Neural Information Processing Systems_.
* Dai Pra (1991) Dai Pra, P. (1991). A stochastic control approach to reciprocal diffusion processes. _Applied Mathematics and Optimization_, 23(1):313-329.
* De Bortoli et al. (2021) De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion Schrodinger bridge with applications to score-based generative modeling. In _Advances in Neural Information Processing Systems_.
* Delbracio and Milanfar (2023) Delbracio, M. and Milanfar, P. (2023). Inversion by direct iteration: An alternative to denoising diffusion for image restoration. _arXiv preprint arXiv:2303.11435_.
* Dupire (1994) Dupire, B. (1994). Pricing with a smile. _Risk_, 7(1):18-20.
* Essid and Pavon (2019) Essid, M. and Pavon, M. (2019). Traversing the Schrodinger bridge strait: Robert Fortet's marvelous proof redux. _Journal of Optimization Theory and Applications_, 181(1):23-60.
* Fatras et al. (2021) Fatras, K., Zine, Y., Majewski, S., Flamary, R., Gribonval, R., and Courty, N. (2021). Minibatch optimal transport distances; analysis and applications. _arXiv preprint arXiv:2101.01792_.
* Fernandes et al. (2021) Fernandes, D. L., Vargas, F., Ek, C. H., and Campbell, N. D. (2021). Shooting Schrodinger's cat. In _Fourth Symposium on Advances in Approximate Bayesian Inference_.
* Feydy et al. (2017) Feydy, J., Charlier, B., Vialard, F.-X., and Peyre, G. (2017). Optimal transport for diffeomorphic registration. In _Medical Image Computing and Computer Assisted Intervention- MICCAI_, pages 291-299.
* Finlay et al. (2020) Finlay, C., Gerolin, A., Oberman, A. M., and Pooladian, A.-A. (2020). Learning normalizing flows from entropy-Kantorovich potentials. _arXiv preprint arXiv:2006.06033_.
* Flamary et al. (2021) Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T., Janati, H., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz, A., Seguy, V., Sutherland, D. J., Tavenard, R., Tong, A., and Vayer, T. (2021). Pot: Python optimal transport. _Journal of Machine Learning Research_, 22(78):1-8.
* Follmer (1988) Follmer, H. (1988). Random fields and diffusion processes. In _Ecole d'Ete de Probabilites de Saint-Flour XV-XVII, 1985-87_, pages 101-203. Springer.
* Fortet (1940) Fortet, R. (1940). Resolution d'un systeme d'equations de M. Schrodinger. _Journal de Mathematiques Pures et Appliques_, 1:83-105.
* Goodfellow et al. (2014) Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial networks. _arXiv preprint arXiv:1406.2661_.
* Gyongy (1986) Gyongy, I. (1986). Mimicking the one-dimensional marginal distributions of processes having an Ito differential. _Probability Theory and Related Fields_, 71:501-516.
* Haussmann and Pardoux (1986) Haussmann, U. G. and Pardoux, E. (1986). Time reversal of diffusions. _The Annals of Probability_, 14(4):1188-1205.
* Heitz et al. (2023) Heitz, E., Belcour, L., and Chambon, T. (2023). Iterative \(\alpha\)-(de)blending: a minimalist deterministic diffusion model. In _SIGGRAPH_.
* Heitz et al. (2017)Heng, J., Bortoli, V. D., Doucet, A., and Thornton, J. (2021). Simulating diffusion bridges with score matching. _arXiv preprint arXiv:2111.07243_.
* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. In _Advances in Neural Information Processing Systems_.
* Hoogeboom et al. (2023) Hoogeboom, E., Heek, J., and Salimans, T. (2023). simple diffusion: End-to-end diffusion for high resolution images. _arXiv preprint arXiv:2301.11093_.
* Huang et al. (2021) Huang, C.-W., Lim, J. H., and Courville, A. C. (2021). A variational perspective on diffusion-based generative models and score matching. In _Advances in Neural Information Processing Systems_.
* Hyvarinen (2005) Hyvarinen, A. (2005). Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4).
* Krylov (1984) Krylov, N. (1984). Once more about the connection between elliptic operators and Ito's stochastic equations. In _Statistics and Control of Stochastic Processes, Steklov Seminar_, pages 214-229.
* Kullback (1968) Kullback, S. (1968). Probability densities with given marginals. _The Annals of Mathematical Statistics_, 39(4):1236-1243.
* Kullback (1997) Kullback, S. (1997). _Information Theory and Statistics_. Dover Publications, Inc., Mineola, NY. Reprint of the second (1968) edition.
* Leonard (2012) Leonard, C. (2012). Girsanov theory under a finite entropy condition. In _Seminaire de Probabilites XLIV_, pages 429-465. Springer.
* Leonard (2014a) Leonard, C. (2014a). Some properties of path measures. In _Seminaire de Probabilites XLVI_, pages 207-230. Springer.
* Leonard (2014b) Leonard, C. (2014b). A survey of the Schrodinger problem and some of its connections with optimal transport. _Discrete & Continuous Dynamical Systems-A_, 34(4):1533-1574.
* Leonard et al. (2014) Leonard, C., Roelly, S., Zambrini, J.-C., et al. (2014). Reciprocal processes. a measure-theoretical point of view. _Probability Surveys_, 11:237-269.
* Lipman et al. (2023) Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. (2023). Flow matching for generative modeling. In _International Conference on Learning Representations_.
* Liu et al. (2022a) Liu, G.-H., Chen, T., So, O., and Theodorou, E. A. (2022a). Deep generalized Schrodinger bridge. In _Advances in Neural Information Processing Systems_.
* Liu et al. (2023a) Liu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E. A., Nie, W., and Anandkumar, A. (2023a). I\({}^{2}\)SB: Image-to-image Schrodinger bridge. _arXiv preprint arXiv:2302.05872_.
* Liu (2022) Liu, Q. (2022). Rectified flow: A marginal preserving approach to optimal transport. _arXiv preprint arXiv:2209.14577_.
* Liu et al. (2023b) Liu, X., Gong, C., and Liu, Q. (2023b). Flow straight and fast: Learning to generate and transfer data with rectified flow. In _International Conference on Learning Representations_.
* Liu et al. (2022b) Liu, X., Wu, L., Ye, M., and Liu, Q. (2022b). Let us build bridges: Understanding and extending diffusion generative models. _arXiv preprint arXiv:2208.14699_.
* Liu et al. (2015) Liu, Z., Luo, P., Wang, X., and Tang, X. (2015). Deep learning face attributes in the wild. In _International Conference on Computer Vision_.
* Meng et al. (2022) Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. (2022). SDEdit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_.
* Palmowski and Rolski (2002) Palmowski, Z. and Rolski, T. (2002). A technique for exponential change of measure for Markov processes. _Bernoulli_, pages 767-785.
* Pavon et al. (2021) Pavon, M., Trigila, G., and Tabak, E. G. (2021). The data-driven Schrodinger bridge. _Communications on Pure and Applied Mathematics_, 74:1545-1573.
* Peyeyey et al. (2015)

[MISSING_PAGE_FAIL:14]

Villani, C. (2009). _Optimal transport_, volume 338 of _Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]_. Springer-Verlag, Berlin. Old and new.
* Vincent (2011) Vincent, P. (2011). A connection between score matching and denoising autoencoders. _Neural Computation_, 23(7):1661-1674.
* Wang et al. (2021) Wang, G., Jiao, Y., Xu, Q., Wang, Y., and Yang, C. (2021). Deep generative learning via Schrodinger bridge. In _International Conference on Machine Learning_.

## Outline of the Appendix

In Appendix A, we first clarify the relationship between different methods in the existing literature and our proposed DSBM framework. In Appendix B, we focus on the family of linear SDEs, and draw a link between the parameterization of bridges in this paper and the stochastic interpolant in Albergo et al. (2023). In Appendix C, we give proofs for results in the main text. In Appendix D, we present additional theoretical results for IMF in the Gaussian case. In Appendix E, we derive the discrete-time version of Markovian projection. In Appendix F, we explain the benefits of DSBM compared to DSB in more detail. In Appendix G, we describe a method for learning the forward and backward processes jointly and propose a consistency loss between the forward and backward processes. In Appendix H, we present additional methodological details for a practical scaling of the loss function to reduce variance, similar to standard Denoising Diffusion Models. In Appendix I, we give further details for all experiments and additional experimental results. Finally, we discuss broader impacts of our work in Appendix J.

## Appendix A Discussion of Existing Works

### Bridge Matching and Flow Matching Models

In this section, we clarify the relationship between variants of Flow Matching and show that they are equivalent under some conditions. We follow the nomenclature of Tong et al. (2023). We refer to the algorithm originally proposed in Lipman et al. (2023) using linear probability paths and described in (Tong et al., 2023, Section 4.1) as Flow Matching (FM), and the algorithm proposed in (Tong et al., 2023, Section 4.2) as Conditional Flow Matching (CFM). There is a small constant parameter \(\sigma_{\text{min}}\) in both algorithms, which controls the smoothing of the modeled distribution. We consider the case \(\sigma_{\text{min}}=0\). Then CFM recovers exactly the 1st iteration of Rectified Flow (Liu et al., 2023). Furthermore, FM, CFM and the 1st iteration of Rectified Flow are all equivalent when performing generative modeling with a standard Gaussian \(\pi_{0}\). We refer to them collectively as Flow Matching models (FMMs) as they only differ in the smoothing method. We also present them all under the Bridge Matching framework. These models can also be interpreted in the context of _stochastic interpolants_(Albergo and Vanden-Eijnden, 2023; Albergo et al., 2023). Finally, we present recent applications of Bridge Matching and show that some of the objectives in Somnath et al. (2023); Liu et al. (2023); Delbracio and Milanfar (2023) are identical.

Flow Matching and Conditional Flow Matching.In Flow Matching (FM), the objective (Lipman et al., 2023, Equation (21)) is

\[\mathbb{E}_{\Pi_{t,T}}[\|(\mathbf{X}_{T}-\mathbf{X}_{t})/(T-t)-v_{\theta}(t, \mathbf{X}_{t})\|^{2}],\]

where \(\Pi_{t,T}\) is given by \(\pi_{T}(\mathbf{X}_{T})\mathrm{N}(\mathbf{X}_{t};\frac{t}{T}\mathbf{X}_{T},(1 -\frac{t}{T})^{2})\).

In Conditional Flow Matching (CFM), \(\mathbf{X}_{t}^{0,T}=\frac{t}{T}\mathbf{X}_{T}+(1-\frac{t}{T})\mathbf{X}_{0}\), with \(\mathbf{X}_{0}\sim\mathrm{N}(0,\mathrm{Id})\) and the objective (Tong et al., 2023, Equation (16)) is given by

\[\mathbb{E}_{\Pi_{0,T}}[\|(\mathbf{X}_{T}-\mathbf{X}_{0})/T-v_{\theta}(t, \mathbf{X}_{t}^{0,T})\|^{2}].\] (15)

This is the same as (Liu et al., 2023, Equation (1)). Furthermore, \((\mathbf{X}_{T}-\mathbf{X}_{0})/T=(1-\frac{t}{T})(\mathbf{X}_{T}-\mathbf{X}_{ 0})/(T-t)=(\mathbf{X}_{T}-\mathbf{X}_{t}^{0,T})/(T-t)\), so the CFM objective is equivalent to

\[\mathbb{E}_{\Pi_{t,T}}[\|(\mathbf{X}_{T}-\mathbf{X}_{t}^{0,T})/(T-t)-v_{\theta }(t,\mathbf{X}_{t}^{0,T})\|^{2}].\] (16)

The optimal \(v_{\theta}(t,x_{t})=(\mathbb{E}_{\Pi_{T|t}}[\mathbf{X}_{T}\mid\mathbf{X}_{t} =x_{t}]-x_{t})/(T-t)\). In the case of generative modeling, \(\pi_{0}\) is a standard Gaussian distribution and \(\Pi_{0,T}\) is given by \(\mathrm{N}(\mathbf{X}_{0};0,\mathrm{Id})\pi_{T}(\mathbf{X}_{T})\). Thus, \(\Pi_{t,T}\) is also given by \(\pi_{T}(\mathbf{X}_{T})\mathrm{N}(\mathbf{X}_{t}^{0,T};\frac{t}{T}\mathbf{X}_ {T},(1-\frac{t}{T})^{2})\). Therefore, the FM (Lipman et al., 2023) and CFM (Tong et al., 2023) objectives are exactly the same. However, CFM is also applicable when \(\pi_{0}\) is not Gaussian distributed, so CFM is a generalized version of FM6.

Stochastic Interpolant.In (Albergo and Vanden-Eijnden, 2023; Albergo et al., 2023), the concept of stochastic interpolant is introduced. In Albergo and Vanden-Eijnden (2023), the interpolation is deterministic (not necessarily linear), of the form \(I_{t}(x_{0},x_{T})=\alpha(t)x_{0}+\beta(t)x_{T}\), while in Albergo et al. (2023), the interpolation is stochastic given by \(I_{t}(x_{0},x_{T})=\alpha(t)x_{0}+\beta(t)x_{T}+\gamma(t)\mathbf{Z}\) for \(\mathbf{Z}\sim\mathrm{N}(0,\mathrm{Id})\). In Albergo and Vanden-Eijnden (2023), an ODE is learned and the associated velocity field \(v_{\theta}\) is obtained by minimizing the following objective (Albergo and Vanden-Eijnden, 2023, Equation (9))

\[\mathbb{E}_{\Pi_{0,T}}[\|\partial_{t}I_{t}(\mathbf{X}_{0},\mathbf{X}_{T})-v_{ \theta}(t,\mathbf{X}_{t}^{0,T})\|^{2}].\]

Hence, if \(I_{t}(x_{0},x_{T})=\frac{t}{T}x_{0}+(1-\frac{t}{T})x_{T}\), we recover (15).

Link with Bridge Matching.When \(\mathbb{Q}\) is associated with the Brownian motion \((\sigma\mathbf{B}_{t})_{t\in[0,T]}\) and \(\sigma\to 0\) in Bridge Matching, we recover the same objective (5) as the Flow Matching objective (16), since \(\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})=(\mathbf{X}_{T}- \mathbf{X}_{t})/(\sigma^{2}(T-t))\). Bridge Matching can also be applied to general distributions \(\pi_{0},\pi_{T}\); i.e. \(\pi_{0}\) does not have to be restricted to a Gaussian. Therefore, Bridge Matching is a generalized version of Flow Matching, see also (Liu et al., 2022b, Equation (10)).

Inverse problems and interpolation.Somnath et al. (2023) and Liu et al. (2023a) present Bridge Matching algorithms between aligned data \((\mathbf{X}_{0},\mathbf{X}_{T})\sim\Pi_{0,T}\). The objectives (Somnath et al., 2023, Equation (8)) and (Liu et al., 2023a, Equation (12)) are equivalent to the Bridge Matching objective (3). The main difference between Liu et al. (2023a) and Somnath et al. (2023) resides in the choice of \(\Pi_{0,T}\). In the case of Somnath et al. (2023), this choice is motivated by the access to _aligned data_ with applications in biology assuming they are distributed as the true Schrodinger static coupling, i.e. \(\Pi_{0,T}=\Pi_{0,T}^{\text{SB}}\). In the case of Liu et al. (2023a), \(\Pi_{0,T}\) corresponds to a pairing between clean and corrupted images, e.g. with \(\Pi_{0}=\pi_{0}\) the distribution of clean images and \(\Pi_{T}=\pi_{T}\) the distribution of corrupted images obtained from the clean images using the degradation kernel \(\Pi_{T|0}\).

Finally, in (Delbracio and Milanfar, 2023, Equation (5)) the authors consider a reconstruction process of the form

\[\mathrm{d}\mathbf{X}_{t}=(\mathbb{E}_{\Pi_{0|t}}[\mathbf{X}_{0}\mid\mathbf{ X}_{t}]-\mathbf{X}_{t})/t\mathrm{d}t,\qquad\mathbf{X}_{T}\sim\Pi_{T},\] (17)

where here we have replaced \(F(x_{t},t)\) by \(\mathbb{E}_{\Pi_{0|t}}[\mathbf{X}_{0}\mid\mathbf{X}_{t}=x_{t}]\). This is justified if the \(\|\cdot\|_{p}\) norm in (Delbracio and Milanfar, 2023, Equation (4)) is replaced by \(\|\cdot\|_{2}^{2}\) (or any Bregman Loss Function, see Banerjee et al. (2005)). In Delbracio and Milanfar (2023), \(\Pi_{0,T}\) corresponds to the joint distribution of clean and corrupted images as in Liu et al. (2023a). Exchanging the role of \(\Pi_{0}\) and \(\Pi_{T}\), (17) can be rewritten equivalently as

\[\mathrm{d}\mathbf{X}_{t}=(\mathbb{E}_{\Pi_{T|t}}[\mathbf{X}_{T}\mid\mathbf{ X}_{t}]-\mathbf{X}_{t})/(T-t)\mathrm{d}t,\qquad\mathbf{X}_{0}\sim\Pi_{0}.\]

We thus obtain the optimal Flow Matching vector field \(v_{\theta}(t,x_{t})=(\mathbb{E}_{\Pi_{T|t}}[\mathbf{X}_{T}\mid\mathbf{X}_{t}=x _{t}]-x_{t})/(T-t)\) in (16). Note that Delbracio and Milanfar (2023) also incorporates a stochastic version of their objective (Delbracio and Milanfar, 2023, Equation (7)). It remains an open question whether this objective can be understood as a special instance of the Bridge Matching framework.

### On DSBM and Existing Works

In this section, we show that the DSBM framework recovers the above existing algorithms for different choices of bridges \(\mathbb{Q}_{|0,T}\) and couplings \(\Pi_{0,T}^{0}\) in Algorithm 1. For the independent coupling \(\Pi_{0,T}^{0}=\pi_{0}\otimes\pi_{T}\) and Brownian bridge \(\mathbb{Q}_{|0,T}\) (4) with diffusion parameter \(\sigma_{t}=\sigma\), the loss function (10) recovers the Brownian Bridge Matching loss (5). Letting \(\sigma\to 0\), we recover Flow Matching (Lipman et al., 2023). In this case, further iterations repeating lines 7-9 in Algorithm 1 (with only forward projections) recover Rectified Flow (Liu et al., 2023b). If the coupling \(\Pi_{0,T}^{0}\) is given by an estimation of the OT map between \(\pi_{0}\) and \(\pi_{T}\), then the first iteration recovers OT-CFM (Tong et al., 2023; Pooladian et al., 2023). Finally, for general bridges \(\mathbb{Q}_{|0,T}\), if we are given the optimal Schrodinger Bridge static coupling \(\Pi_{0,T}^{0}=\Pi_{0,T}^{\text{SB}}\), then the DSBM procedure converges in one iteration and we recover Somnath et al. (2023).

### DSBM and Rectified Flow

We discuss the differences in more detail between our proposed DSBM method and Rectified Flow. Both Rectified Flow and DSBM are general frameworks for building transport maps between two general distributions \(\pi_{0},\pi_{T}\). However, there are a few important theoretical and practical differences. Firstly, we adopt the SDE approach as opposed to the ODE approach in Rectified Flow. This distinction is crucial in theory, as Proposition 5, which guarantees the uniqueness of the characterization of SB, is valid only when \(\sigma_{t}>0\). Consequently, Rectified Flow is not guaranteed to converge to the dynamic optimal transport solution (see e.g. counterexample in Liu (2022)). In a following work, Liu (2022) established formal connections between Rectified Flow and OT when restricting the class of vector fields to gradient fields. In DSBM, the connection to OT is obtained by considering its entropy-regularized version. Furthermore, by adopting the SDE approach, we observe significant improvements of sample quality in our experiments when performing transport between two general distributions. This is in line with the theoretical analysis in Albergo et al. (2023). On the other hand, while Bridge Matching also achieves high sample quality using the SDE approach, the transported samples are much more dissimilar to the input data (see e.g. Figures 14, 15, 19). Lastly, Rectified Flow also performs Markovian projections iteratively, but only in the forward direction. Consequently, the bias in the learned marginals \(\mathbb{P}_{T}^{n}\) is accumulated and cannot be corrected in later iterations, i.e. the first iteration of RF will achieve the most accurate marginal \(\mathbb{P}_{T}^{1}\). Subsequent iterations can improve the straightness of the flow, but at the cost of sampling accuracy of \(\mathbb{P}_{T}^{n}\). We observe in practice that this becomes particularly problematic if the first iteration of Rectified Flow (which is equivalent to CFM) fails to provide a good transport and learn an accurate \(\mathbb{P}_{T}^{1}\), e.g. in the case of _moons-8gaussians_ (Table 2), Gaussian transport (Figure 3), and MNIST, EMNIST transfer (Figure 5 and Figure 13). As Rectified Flow cannot recover from this issue, we observe the accuracy of \(\mathbb{P}_{T}^{n}\) only deteriorates in further iterations as \(n\) increases. In our methodology, we leverage Proposition 9 to perform forward and backward Bridge Matching, and we observe that the marginal accuracy is able to improve with iteration.

## Appendix B The Design Space of Brownian Bridges

### Relationship to Stochastic Interpolants

From stochastic interpolants to Brownian bridges.In this section, we draw a link between our parameterization of bridges and the one used in Albergo et al. (2023). In Albergo et al. (2023), a stochastic interpolant is defined as

\[\mathbf{X}_{t}=\bar{\alpha}_{t}x_{0}+\bar{\beta}_{t}x_{T}+\bar{ \gamma}_{t}\mathbf{Z},\] (18)

where \(\mathbf{Z}\sim\mathrm{N}(0,\mathrm{Id})\). Since their methodology and analysis mainly relies on the probability flow, they work with (18), which is easier to analyse. In our setting, as we deal mostly with diffusions, it is natural to parameterize Brownian bridges as follows

\[\mathrm{d}\mathbf{X}_{t}=\{-\alpha_{t}\mathbf{X}_{t}+\beta_{t}x_ {T}\}\mathrm{d}t+\gamma_{t}\mathrm{d}\mathbf{B}_{t}.\] (19)

The goal of this section is to derive explicit formulas between the parameters \(\bar{\alpha}_{t}\), \(\bar{\beta}_{t}\) and \(\bar{\gamma}_{t}\) of (18) and the parameters \(\alpha_{t}\), \(\beta_{t}\) and \(\gamma_{t}\) of (19). Consider \((\mathbf{X}_{t})_{t\in[0,T]}\) given by (19). We have that for any \(t\in[0,T]\)

\[\mathbf{X}_{t}=\exp[-A_{t}]x_{0}+\int_{0}^{t}\beta_{s}\exp[A_{s} -A_{t}]\mathrm{d}sx_{T}+\int_{0}^{t}\gamma_{s}\exp[A_{s}-A_{t}]\mathrm{d} \mathbf{B}_{s},\]

where \(A_{t}=\int_{0}^{t}\alpha_{s}\mathrm{d}s\). Therefore, we have that

\[\bar{\alpha}_{t}=\exp[-\int_{0}^{t}\alpha_{s}\mathrm{d}s],\qquad \bar{\beta}_{t}=\int_{0}^{t}\beta_{s}\exp[-\int_{s}^{t}\alpha_{u}\mathrm{d}u] \mathrm{d}s,\qquad\bar{\gamma}_{t}^{2}=\int_{0}^{t}\gamma_{s}^{2}\exp[-2\int_{s }^{t}\alpha_{u}\mathrm{d}u]\mathrm{d}s,\] (20)

\[\alpha_{t}=-\tfrac{\bar{\alpha}_{t}^{\prime}}{\bar{\alpha}_{t}}, \qquad\beta_{t}=\bar{\beta}_{t}^{\prime}+\bar{\beta}_{t}\alpha_{t},\qquad\gamma _{t}^{2}=(\bar{\gamma}_{t}^{2})^{\prime}+2\bar{\gamma}_{t}^{2}\alpha_{t}=2\bar {\gamma}_{t}\gamma_{t}^{\prime}+2\bar{\gamma}_{t}^{2}\alpha_{t}.\] (21)

Using this relationship, we get that the Markovian projection, see Definition 1, is given by

\[\mathrm{d}\mathbf{X}_{t}^{\star}=f_{t}^{\star}(\mathbf{X}_{t}) \mathrm{d}t+\gamma_{t}\mathrm{d}\mathbf{B}_{t},\qquad f_{t}^{\star}(x_{t})= \mathbb{E}_{\Pi_{T|t}}[-\alpha_{t}\mathbf{X}_{t}+\beta_{t}\mathbf{X}_{T}\mid \mathbf{X}_{t}=x_{t}].\]

We have that

\[f_{t}^{\star}(x_{t}) =\mathbb{E}_{\Pi_{T|t}}[-\alpha_{t}\mathbf{X}_{t}+\beta_{t} \mathbf{X}_{T}\mid\mathbf{X}_{t}=x_{t}]\] \[=\mathbb{E}_{\Pi_{0,T|t}}[-\alpha_{t}(\bar{\alpha}_{t}\mathbf{X} _{0}+\bar{\beta}_{t}\mathbf{X}_{T}+\bar{\gamma}_{t}\mathbf{Z})+\beta_{t} \mathbf{X}_{T}\mid\mathbf{X}_{t}=x_{t}].\]Using (21), we get that

\[f_{t}^{\star}(x_{t})=\mathbb{E}_{\Pi_{0,T|t}}[\bar{\alpha}_{t}^{\prime}\mathbf{X} _{0}+\bar{\beta}_{t}^{\prime}\mathbf{X}_{T}+\tfrac{\bar{\alpha}_{t}^{\prime} \bar{\gamma}_{t}}{\bar{\alpha}_{t}}\mathbf{Z}\mid\mathbf{X}_{t}=x_{t}].\]

In Albergo et al. (2023), it is shown that \(\nabla\log\mathbb{M}_{t}^{\star}(x_{t})=-\mathbb{E}_{\Pi_{0,T|t}}[\mathbf{Z} \mid\mathbf{X}_{t}=x_{t}]/\bar{\gamma}_{t}\), where \(\mathbb{M}^{\star}\) is the Markovian projection. The probability flow associated with \((\mathbf{X}_{t}^{\star})_{t\in[0,T]}\) is given by

\[\mathrm{d}\mathbf{Z}_{t}^{\star} =\{f_{t}^{\star}(\mathbf{Z}_{t}^{\star})-\tfrac{\gamma_{t}^{2}}{ 2}\nabla\log\mathbb{M}_{t}^{\star}(\mathbf{Z}_{t}^{\star})\}\mathrm{d}t\] \[=\{\mathbb{E}_{\Pi_{0,T|t}}[\bar{\alpha}_{t}^{\prime}\mathbf{X} _{0}+\bar{\beta}_{t}^{\prime}\mathbf{X}_{T}+(-\alpha_{t}\bar{\gamma}_{t}+ \tfrac{\gamma_{t}^{2}}{2\bar{\gamma}_{t}})\mathbf{Z}\mid\mathbf{X}_{t}= \mathbf{Z}_{t}^{\star}]\}\mathrm{d}t\] \[=\{\mathbb{E}_{\Pi_{0,T|t}}[\bar{\alpha}_{t}^{\prime}\mathbf{X} _{0}+\bar{\beta}_{t}^{\prime}\mathbf{X}_{T}+\bar{\gamma}_{t}^{\prime}\mathbf{ Z}\mid\mathbf{X}_{t}=\mathbf{Z}_{t}^{\star}]\}\mathrm{d}t.\]

Hence, we recover (Albergo et al., 2023, Theorem 2.6).

Non-Markov path measures.A natural question is whether (19) arises as the bridge measure of some _Markov_ measure. For instance, if \(\mathbb{Q}\) is associated with \((x_{0}+\mathbf{B}_{t})_{t\in[0,T]}\), then pinning the process at \(x_{T}\) at time \(T\), we get that the associated bridge measure \(\mathbb{Q}_{|0,T}\) is given by

\[\mathrm{d}\mathbf{X}_{t}^{0,T}=(x_{T}-\mathbf{X}_{t})/(T-t)\mathrm{d}t+ \mathrm{dB}_{t}.\]

Therefore, we recover (19) with \(\alpha_{t}=\beta_{t}=\frac{1}{T-t}\) and \(\gamma_{t}=1\). Using (20), we get that \(\bar{\alpha}_{t}=1-\frac{t}{T}\), \(\bar{\beta}_{t}=\frac{t}{T}\) and \(\bar{\gamma}_{t}^{2}=(T-t)t/T\). We recover (4), upon noting that \(\mathbf{B}_{t}-\frac{t}{T}\mathbf{B}_{T}\) is Gaussian with zero mean and variance \((T-t)t/T\).

More generally, we consider a Markov measure \(\mathbb{Q}\) associated with \((\mathbf{X}_{t})_{t\in[0,T]}\) such that

\[\mathrm{d}\mathbf{X}_{t}=-a_{t}\mathbf{X}_{t}\mathrm{d}t+c_{t}\mathrm{d} \mathbf{B}_{t},\qquad\mathbf{X}_{0}=x_{0}.\]

We now derive the associated bridge measure \(\mathbb{Q}_{|0,T}\):

\[\mathbf{X}_{T}=\exp[-\Lambda_{T}+\Lambda_{t}]\mathbf{X}_{t}+\int_{t}^{T}c_{s} \exp[\Lambda_{s}-\Lambda_{T}]\mathrm{d}\mathbf{B}_{s},\]

with \(\Lambda_{t}=\int_{0}^{t}a_{s}\mathrm{d}s\). We have that

\[c_{t}^{2}\nabla_{x_{t}}\log\mathbb{Q}_{T|t}(x_{T}|x_{t}) =(c_{t}^{2}\exp[\Lambda_{t}-\Lambda_{T}]/\int_{t}^{T}c_{s}^{2} \exp[2(\Lambda_{s}-\Lambda_{T})]\mathrm{d}s)x_{T}\] \[\qquad-(c_{t}^{2}\exp[2(\Lambda_{t}-\Lambda_{T})]/\int_{t}^{T}c_{ s}^{2}\exp[2(\Lambda_{s}-\Lambda_{T})]\mathrm{d}s)x_{t}.\]

Therefore, combining this result and (2), we get that \(\mathbb{Q}_{|0,T}\) is associated with

\[\alpha_{t} =a_{t}+c_{t}^{2}\exp[-2\int_{t}^{T}a_{s}\mathrm{d}s]/\int_{t}^{T} c_{s}^{2}\exp[-2\int_{s}^{T}a_{u}\mathrm{d}u]\mathrm{d}s,\] \[\beta_{t} =c_{t}^{2}\exp[-\int_{t}^{T}a_{s}\mathrm{d}s]/\int_{t}^{T}c_{s}^ {2}\exp[-2\int_{s}^{T}a_{u}\mathrm{d}u]\mathrm{d}s,\qquad\gamma_{t}=c_{t}.\]

In that case \((a_{t},c_{t})_{t\in[0,T]}\) entirely parameterize \((\alpha_{t},\beta_{t},\gamma_{t})_{t\in[0,T]}\). Hence, in the Ornstein-Uhlenbeck setting, if \(\mathbb{Q}_{|0,T}\) is the bridge of a Markov measure, it is fully parameterized by two functions while in the non-Markov setting it is parameterized by three functions.

In this paper, we present our framework in the Markovian setting as the Schrodinger Bridge problem is usually defined with respect to Markov reference measures. However, our methodology could be extended in a straightforward fashion to the non-Markovian setting. This would allow for a further exploration of the design space of DSBM.

### Linear SDE and Bridge Matching

In this section, we study further the diffusion bridge of linear SDEs. Arbitrary Markov measures can be chosen to build bridges; however, we want to be able to compute some representations of the bridge in an explicit way. More precisely, denoting \((\mathbf{X}_{t}^{0,T})_{t\in[0,T]}\) the diffusion bridge with \(x_{0},x_{T}\) the initial and final condition, we want to have access to the following:

* _integral sampler_: we want to have a formula to sample \(\mathbf{X}_{t}^{0,T}\) for any \(t\in[0,T]\) without having to run a stochastic process forward or backward.

_forward sampler_: we want to have a forward SDE for \(\mathbf{X}_{t}^{0,T}\) with explicit coefficients, which might depend on \(x_{T}\), running in a forward fashion terminating at \(x_{T}\).
* _backward sampler_: we want to have a backward SDE for \(\mathbf{Y}_{t}^{0,T}=\mathbf{X}_{T-t}^{0,T}\) with explicit coefficients, which might depend on \(x_{0}\), running in a backward fashion terminating at \(x_{0}\).

We focus on _linear SDEs_ of the form \(\mathrm{d}\mathbf{X}_{t}=-\alpha\beta_{t}\mathbf{X}_{t}\mathrm{d}t+\sigma \beta_{t}^{1/2}\mathrm{d}\mathbf{B}_{t}\), which are particularly amenable, where \((\beta_{t})_{t\in[0,T]}\) is a schedule with \(\beta\in\mathrm{C}([0,T],(0,+\infty))\).

#### b.2.1 Brownian motion

First, we consider the Brownian motion setting and let \((\mathbf{X}_{t})_{t\in[0,T]}\) be associated with \(\mathbb{Q}\) with \(\mathrm{d}\mathbf{X}_{t}=\beta_{t}^{1/2}\mathrm{d}\mathbf{B}_{t}\). We consider \((\mathbf{X}_{t}^{0,T})_{t\in[0,T]}\) conditioned at both ends \(\mathbf{X}_{0}^{0,T}=x_{0}\) and \(\mathbf{X}_{T}^{0,T}=x_{T}\). First, using (Barczy and Kern, 2013, Theorem 3.3), we have that for any \(t\in[0,T]\)

\[\mathbf{X}_{t}^{0,T}=\tfrac{R(t,T)}{R(0,T)}x_{0}+\tfrac{R(0,t)}{R(0,T)}(x_{T}- \mathbf{X}_{T})+\mathbf{X}_{t},\]

with \(R(s,t)=\int_{s}^{t}\beta_{u}\mathrm{d}u=\sigma^{2}(B_{t}-B_{s})\), where for any \(t\in[0,T]\), \(B_{t}=\int_{0}^{t}\beta_{s}\mathrm{d}s\). Therefore, we get that

\[\mathbf{X}_{t}^{0,T}=(1-\tfrac{B(t)}{B(T)})x_{0}+\tfrac{B(t)}{B(T)}(x_{T}- \mathbf{X}_{T})+\mathbf{X}_{t}.\] (22)

(22) defines the _integral sampler_. In addition, using (Barczy and Kern, 2013, Theorem 3.2), we have

\[\mathbf{X}_{0}^{0,T}=x_{0},\qquad\mathrm{d}\mathbf{X}_{t}^{0,T}=\{-\sigma^{2} \beta_{t}/\gamma(t,T)\mathbf{X}_{t}^{0,T}+\sigma^{2}\beta_{t}/\gamma(t,T)x_{T} \}\mathrm{d}t+\sigma\beta_{t}^{1/2}\mathrm{d}\mathbf{B}_{t},\]

where \(\gamma(s,t)=\sigma^{2}(B(t)-B(s))\). Therefore, we get that

\[\mathbf{X}_{0}^{0,T}=x_{0},\qquad\mathrm{d}\mathbf{X}_{t}^{0,T}=\{-\tfrac{ \beta_{t}}{B(T)-B(t)}\mathbf{X}_{t}^{0,T}+\tfrac{\beta_{t}}{B(T)-B(t)}x_{T}\} \mathrm{d}t+\sigma\beta_{t}^{1/2}\mathrm{d}\mathbf{B}_{t}.\] (23)

(23) defines the _forward sampler_. Finally, we derive the _backward sampler_ by considering the time-reversal of the forward unconditional process (initialized at \(x_{0}\)). Following Haussmann and Pardoux (1986),

\[\mathbf{Y}_{0}^{0,T}=x_{T},\qquad\mathrm{d}\mathbf{Y}_{t}^{0,T}=\sigma^{2} \beta_{T-t}\nabla\log\mathbb{Q}_{T-t|0}(\mathbf{Y}_{t}^{0,T}|x_{0})\mathrm{d}t +\sigma\beta_{T-t}^{1/2}\mathrm{d}\mathbf{B}_{t}.\] (24)

In addition, we have that

\[\mathbf{X}_{t}=x_{0}+\sigma B(t)^{1/2}\varepsilon_{t},\qquad\varepsilon_{t} \sim\mathrm{N}(0,\mathrm{Id}).\]

Hence, we get that for any \(t\in[0,T]\) and \(x\in\mathbb{R}^{d}\)

\[\nabla\log\mathbb{Q}_{t|0}(x|x_{0})=-(x-x_{0})/(\sigma^{2}B(t)).\]

Combining this result and (24), we get

\[\mathbf{Y}_{0}^{0,T}=x_{T},\qquad\mathrm{d}\mathbf{Y}_{t}^{0,T}=\{-\tfrac{ \beta_{T-t}}{B(T-t)}\mathbf{Y}_{t}^{0,T}+\tfrac{\beta_{T-t}}{B(T-t)}x_{0}\} \mathrm{d}t+\sigma\beta_{T-t}^{1/2}\mathrm{d}\mathbf{B}_{t}.\] (25)

Combining (22), (23) and (25), we get

\[\mathbf{X}_{t}^{0,T}=\lambda_{t}x_{0}+\varphi_{t}(x_{T}-\mathbf{ X}_{T})+\mathbf{X}_{t}.\] \[\mathbf{X}_{0}^{0,T}=x_{0},\qquad\mathrm{d}\mathbf{X}_{t}^{0,T}= \{\kappa_{t}^{f}\mathbf{X}_{t}^{0,T}+\Psi_{t}^{f}x_{T}\}\mathrm{d}t+\sigma\beta _{t}^{1/2}\mathrm{d}\mathbf{B}_{t},\] \[\mathbf{Y}_{0}^{0,T}=x_{T},\qquad\mathrm{d}\mathbf{Y}_{t}^{0,T}= \kappa_{T-t}^{b}\mathbf{Y}_{t}^{0,T}+\Psi_{T-t}^{b}x_{0}\}\mathrm{d}t+\sigma \beta_{T-t}^{1/2}\mathrm{d}\mathbf{B}_{t},\]

with

\[\lambda_{t}=1-\tfrac{B(t)}{B(T)},\qquad\varphi_{t}=\tfrac{B(t)}{B( T)},\] \[\kappa_{t}^{f}=-\tfrac{\beta_{t}}{B(T)-B(t)},\qquad\Psi_{t}^{f}= \tfrac{\beta_{t}}{B(T)-B(t)},\] \[\kappa_{t}^{b}=-\tfrac{\beta_{t}}{B(t)},\qquad\Psi_{t}^{b}= \tfrac{\beta_{t}}{B(t)}.\]

#### b.2.2 Ornstein-Uhlenbeck

Second, we consider the Ornstein-Uhlenbeck setting and let \((\mathbf{X}_{t})_{t\in[0,T]}\) with \(\mathrm{d}\mathbf{X}_{t}=-\alpha\beta_{t}\mathbf{X}_{t}\mathrm{d}t+\sigma\beta_{ t}^{1/2}\mathrm{d}\mathbf{B}_{t}\), with \(\alpha\neq 0\). We consider \((\mathbf{X}_{t}^{0,T})_{t\in[0,T]}\), the stochastic process \((\mathbf{X}_{t})_{t\in[0,T]}\) conditioned at both ends \(\mathbf{X}_{0}^{0,T}=x_{0}\) and \(\mathbf{X}_{T}^{0,T}=x_{T}\). First, using (Barczy and Kern, 2013, Theorem 3.3), we have that for any \(t\in[0,T]\)

\[\mathbf{X}_{t}^{0,T}=\tfrac{R(t,T)}{R(0,T)}x_{0}+\tfrac{R(0,t)}{R(0,T)}(x_{T}- \mathbf{X}_{T})+\mathbf{X}_{t},\]

with \(R(s,t)=\exp[\alpha(B(t)-B(s))]\gamma(s,t)\), with \(\gamma(s,t)=\int_{s}^{t}\sigma^{2}\beta(u)\exp[-2\alpha(B(t)-B(u))]\mathrm{d}u\). In particular, we have

\[\gamma(s,t)=\tfrac{\sigma^{2}}{2\alpha}(1-\exp[-2\alpha(B(t)-B(s))],\qquad R( s,t)=\tfrac{\sigma^{2}}{\alpha}\sinh(\alpha(B(t)-B(s))).\] (26)

Therefore, we get that

\[\mathbf{X}_{t}^{0,T}=\tfrac{\sinh(\alpha(B(T)-B(t))}{\sinh(\alpha B(T))}x_{0} +\tfrac{\sinh(\alpha B(t))}{\sinh(\alpha B(T))}(x_{T}-\mathbf{X}_{T})+\mathbf{ X}_{t}.\] (27)

(27) defines the _integral sampler_. In addition, using (Barczy and Kern, 2013, Theorem 3.2) and (26), we have \(\mathbf{X}_{0}^{0,T}=x_{0}\) and

\[\mathrm{d}\mathbf{X}_{t}^{0,T} =\{-\alpha\beta_{t}\mathbf{X}_{t}-\tfrac{\sigma^{2}\beta_{t}\exp [-2\alpha(B(T)-B(t))]}{\gamma(t,T)}\mathbf{X}_{t}^{0,T}+\tfrac{\sigma^{2}\beta_ {t}\exp[-\alpha(B(T)-B(t))]}{\gamma(t,T)}x_{T}\}\mathrm{d}t+\sigma\beta_{t}^{1 /2}\mathrm{d}\mathbf{B}_{t}\] \[=\{-\alpha\beta_{t}\mathbf{X}_{t}-\tfrac{2\alpha\beta_{t}}{\exp[ 2\alpha(B(T)-B(t))]-1}\mathbf{X}_{t}^{0,T}+\tfrac{2\alpha\beta_{t}}{\exp[- \alpha(B(T)-B(t))]-\exp[-\alpha(B(T)-B(t))]}x_{T}\}\mathrm{d}t+\sigma\beta_{t}^ {1/2}\mathrm{d}\mathbf{B}_{t}\] \[=\{-\alpha\beta_{t}\exp[2\alpha(B(T)-B(t))]+1\}\mathbf{X}_{t}^{0, T}+\tfrac{2\alpha\beta_{t}}{\exp[-\alpha(B(T)-B(t))]-\exp[-\alpha(B(T)-B(t))]}x_{T} \}\mathrm{d}t+\sigma\beta_{t}^{1/2}\mathrm{d}\mathbf{B}_{t}\] \[=\{-\alpha\beta_{t}\coth(\alpha(B(T)-B(t)))\mathbf{X}_{t}^{0,T}+ \alpha\beta_{t}\operatorname{csch}(\alpha(B(T)-B(t)))x_{T}\}\mathrm{d}t+\sigma \beta_{t}^{1/2}\mathrm{d}\mathbf{B}_{t}.\]

In the formula, \(\coth\) is the hyperbolic cotangent function defined as \(\coth(x)=\tfrac{1}{\tanh(x)}=\tfrac{\cosh(x)}{\sinh(x)}\) and \(\operatorname{csch}\) is the hyperbolic cosecant function defined as \(\operatorname{csch}(x)=\tfrac{1}{\sinh(x)}\). Combining this result and (26), we get that

\[\mathbf{X}_{0}^{0,T}=x_{0},\,\mathrm{d}\mathbf{X}_{t}^{0,T}=\{-\alpha\beta_{t} \coth(\alpha(B(T)-B(t)))\mathbf{X}_{t}^{0,T}+\alpha\beta_{t}\operatorname{csch }(\alpha(B(T)-B(t)))x_{T}\}\mathrm{d}t+\sigma\beta_{t}^{1/2}\mathrm{d}\mathbf{ B}_{t}.\] (28)

(28) defines the _forward sampler_. Finally, we derive the _backward sampler_ by considering the time-reversal of the forward unconditional process (initialized at \(x_{0}\)). Following Haussmann and Pardoux (1986),

\[\mathbf{Y}_{0}^{0,T}=x_{T},\qquad\mathrm{d}\mathbf{Y}_{t}^{0,T}=\{\alpha\beta _{T-t}\mathbf{Y}_{t}^{0,T}+\sigma^{2}\beta_{T-t}\nabla\log\mathbb{Q}_{T-t|0}( \mathbf{Y}_{t}^{0,T}|x_{0})\}\mathrm{d}t+\sigma\beta_{T-t}^{1/2}\mathrm{d} \mathbf{B}_{t}.\] (29)

In addition, we have that

\[\mathbf{X}_{t}=\exp[-\alpha B(t)]x_{0}+\tfrac{\sigma}{\sqrt{2\alpha}}(1-\exp[-2 \alpha B(t)])^{1/2}\varepsilon_{t},\qquad\varepsilon_{t}\sim\mathrm{N}(0, \mathrm{Id}).\]

Hence, we get that for any \(t\in[0,T]\) and \(x\in\mathbb{R}^{d}\)

\[\nabla\log\mathbb{Q}_{t|0}(x|x_{0})=-2\alpha(x-\exp[-\alpha B(t)]x_{0})/( \sigma^{2}(1-\exp[-2\alpha B(t)])).\]

Combining this result and (29), we get \(\mathbf{Y}_{0}^{0,T}=x_{T}\) and

\[\mathrm{d}\mathbf{Y}_{t}^{0,T} =\{\alpha\beta_{T-t}\mathbf{Y}_{t}^{0,T}-\tfrac{2\alpha\beta_{T-t }}{1-\exp[-\alpha B(T-t)]}\mathbf{Y}_{t}^{0,T}+\tfrac{2\alpha\beta_{T-t}\exp[- \alpha B(T-t)]}{1-\exp[-2\alpha B(T-t)]}x_{0}\}\mathrm{d}t+\sigma\beta_{T-t}^{1 /2}\mathrm{d}\mathbf{B}_{t}\] \[=\{-\alpha\beta_{T-t}\tfrac{1+\exp[-2\alpha B(T-t)]}{1-\exp[-2 \alpha B(T-t)]}\mathbf{Y}_{t}^{0,T}+\tfrac{2\alpha\beta_{T-t}}{\exp[\alpha B(T- t)]-\exp[-\alpha B(T-t)]}x_{0}\}\mathrm{d}t+\sigma\beta_{T-t}^{1/2} \mathrm{d}\mathbf{B}_{t}\] \[=\{-\alpha\beta_{T-t}\coth(\alpha B(T-t))\mathbf{Y}_{t}^{0,T}+ \alpha\beta_{T-t}\operatorname{csch}(\alpha B(T-t))x_{T}\}\mathrm{d}t+\sigma\beta_{ T-t}^{1/2}\mathrm{d}\mathbf{B}_{t}.\]

Therefore

\[\mathbf{Y}_{0}^{0,T}=x_{T},\,\mathrm{d}\mathbf{Y}_{t}^{0,T}=\{-\alpha\beta_{T-t} \coth(\alpha B(T-t))\mathbf{Y}_{t}^{0,T}+\alpha\beta_{T-t}\operatorname{csch}( \alpha B(T-t))x_{T}\}\mathrm{d}t+\sigma\beta_{T-t}^{1/2}\mathrm{d}\mathbf{B}_{t}.\] (30)Combining (27), (28) and (30), we get

\[\mathbf{X}_{t}^{0,T}=\lambda_{t}x_{0}+\varphi_{t}(x_{T}-\mathbf{X}_ {T})+\mathbf{X}_{t}.\] \[\mathbf{X}_{0}^{0,T}=x_{0},\qquad\mathrm{d}\mathbf{X}_{t}^{0,T}= \{\kappa_{t}^{b}\mathbf{X}_{t}^{0,T}+\Psi_{t}^{b}x_{T}\}\mathrm{d}t+\sigma\beta_ {t}^{1/2}\mathrm{d}\mathbf{B}_{t},\] \[\mathbf{Y}_{0}^{0,T}=x_{T},\qquad\mathrm{d}\mathbf{Y}_{t}^{0,T}= \{\kappa_{T-t}^{b}\mathbf{Y}_{t}^{0,T}+\Psi_{T-t}^{b}x_{0}\}\mathrm{d}t+\sigma \beta_{T-t}^{1/2}\mathrm{d}\mathbf{B}_{t},\]

with

\[\lambda_{t}=\tfrac{\sinh(\alpha(B(T)-B(t))}{\sinh(\alpha B(T))}, \qquad\varphi_{t}=\tfrac{\sinh(\alpha B(t))}{\sinh(\alpha B(T))},\] \[\kappa_{t}^{f}=-\alpha\beta_{t}\coth(\alpha(B(T)-B(t))),\qquad \Psi_{t}^{f}=\alpha\beta_{t}\operatorname{csch}(\alpha(B(T)-B(t))),\] \[\kappa_{t}^{b}=-\alpha\beta_{t}\coth(\alpha B(t)),\qquad\Psi_{t }^{b}=\alpha\beta_{t}\operatorname{csch}(\alpha B(t)).\]

Using that \(\tanh(x)\sim x\) and \(\sinh(x)\sim x\) for \(x\to 0\), we recover the Brownian motion setting by letting \(\alpha\to 0\). Note that Albergo et al. (2023) show that given an _integral sampler_, which does not necessarily comes from a Markovian process, a _forward sampler_ with the same marginals can be defined, although it does not necessarily satisfies the fact that the _paths_ have the same distribution.

## Appendix C Proofs

### Proof of Proposition 2

We refer the reader to Chung and Walsh (2006); Rogers and Williams (2000) for an introduction to Doob \(h\)-transform. Our theoretical treatment of the Doob \(h\)-transform closely follows Palmowski and Rolski (2002).

First, we introduce the _infinitesimal generator_\(\mathcal{A}\) given for any \(f\in\mathrm{C}_{c}^{\infty}([0,T]\times\mathbb{R}^{d},\mathbb{R})\), \(t\in[0,T]\) and \(x\in\mathbb{R}^{d}\) by

\[\mathcal{A}f(t,x)=\langle f_{t}(x),\nabla f(t,x)\rangle+\tfrac{\sigma_{t}^{2} }{2}\Delta f(t,x)+\partial_{t}f(t,x).\] (31)

The following assumption ensures that the diffusion associated with \(\mathbb{Q}\) as well as its Markovian projections are well-defined.

**A1**.: \(f\)_, \(\sigma\) and \((t,x_{t})\mapsto\mathbb{E}_{\Pi_{T|t}}[\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_ {T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}=x_{t}]\) are locally Lipschitz and there exist \(C>0\), \(\psi\in\mathrm{C}([0,T],\mathbb{R}_{+})\) such that for any \(t\in[0,T]\) and \(x_{0},x_{t}\in\mathbb{R}^{d}\), we have_

\[\|f_{t}(x_{t})\|\leq C(1+\|x_{t}\|),\qquad C\geq\sigma_{t}\geq 1/C,\] \[\|\mathbb{E}_{\Pi_{T|t}}[\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{ T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}=x_{t}]\|\leq C\psi(t)(1+\|x_{t}\|).\]

We consider the following assumption, which will ensure that we can apply Doob \(h\)-transform techniques.

**A2**.: _For any \(x_{0}\in\mathbb{R}^{d}\), \(\Pi_{T|0}\) is absolutely continuous w.r.t. \(\mathbb{Q}_{T|0}\). For any \(x_{0}\in\mathbb{R}^{d}\), let \(\varphi_{T|0}\) be given for any \(x_{T}\in\mathbb{R}^{d}\) by \(\varphi_{T|0}(x_{T}|x_{0})=\mathrm{d}\Pi_{T|0}(x_{T}|x_{0})/\mathrm{d}\mathbb{Q }_{T|0}(x_{T}|x_{0})\) and assume that for any \(x_{0}\in\mathbb{R}^{d}\), \(x_{T}\mapsto\varphi_{T|0}(x_{T}|x_{0})\) is bounded. For any \(x_{0}\in\mathbb{R}^{d}\), let \(\varphi_{t|0}\) given for any \(x_{t}\in\mathbb{R}^{d}\) and \(t\in[0,T]\) by_

\[\varphi_{t|0}(x_{t}|x_{0})=\int_{\mathbb{R}^{d}}\varphi_{T|0}(x_{T}|x_{0}) \mathrm{d}\mathbb{Q}_{T|t}(x_{T}|x_{t}).\] (32)

_Finally, we assume that for any \(x_{0}\in\mathbb{R}^{d}\), \((t,x_{t})\mapsto 1/\varphi_{t|0}(x_{t}|x_{0})\) and \((t,x_{t})\mapsto\mathcal{A}\varphi_{t|0}(x_{t}|x_{0})\) are bounded._

This means that for any \(x_{0}\in\mathbb{R}^{d}\), \((t,x_{t})\mapsto\varphi_{t}(x_{t}|x_{0})\) is a _good function_ in the sense of (Palnowski and Rolski, 2002, Proposition 3.2). Note here that these assumptions could be relaxed on a case-by-case basis. We leave this study for future work.

The following lemma is a direct consequence of **A2** and (32). It ensures that the \(h\)-function \(\varphi_{t|0}\) satisfies the backward Kolmogorov equation.

**Lemma 11**.: _Assume **A2**. Then, \(\varphi\in\mathrm{C}^{1,2}([0,T)\times\mathbb{R}^{d},\mathbb{R})\) and \(\mathcal{A}\varphi_{|0}=0\)._

Using (31), we have that for any \(x_{0}\in\mathbb{R}^{d}\) and \(f\in\mathrm{C}_{c}^{\infty}([0,T]\times\mathbb{R}^{d},\mathbb{R})\), \(t\in[0,T]\) and \(x_{t}\in\mathbb{R}^{d}\)

\[(\mathcal{A}(f\varphi_{|0})-f\mathcal{A}\varphi_{|0})(t,x_{t})/\varphi_{|0}(t,x_{ t})=\mathcal{A}f(t,x_{t})+\sigma_{t}^{2}\langle\nabla f(t,x_{t}),\nabla\log \varphi_{t|0}(x_{t}|x_{0})\rangle.\]Finally, we consider the following assumption, which will ensure that the Doob \(h\)-transform is well-defined.

**A.3**.: _For any \(x_{0}\in\mathbb{R}^{d}\), there exists \(C\geq 0\) such that for any \(t\in[0,T]\) and \(x_{t}\in\mathbb{R}^{d}\), \(\|\nabla\log\varphi_{t|0}(x_{t}|x_{0})\|\leq C(1+\|x_{0}\|+\|x_{t}\|)\)._

We are now ready to state and prove Proposition 2. Note that the Markovian projection is defined in Definition 1. Finally, we define \(\mathcal{M}\) the space of path measures such that \(\mathbb{P}\in\mathcal{M}\) if \(\mathbb{P}\) is associated with \(\mathrm{d}\mathbf{X}_{t}=\{f_{t}(\mathbf{X}_{t})+v_{t}(\mathbf{X}_{t})\} \mathrm{d}t+\sigma_{t}\mathrm{d}\mathbf{B}_{t}\), with \(\sigma,v\) locally Lipschitz. This restriction of Markov measures allows us to apply the entropic version of the Girsanov theorem (Leonard, 2012). It has no impact on our methodology.

**Proposition**.: _Assume_ **A**_1,_ **A**_2,_ **A**_3. Let \(\mathbb{M}^{\star}=\mathrm{proj}_{\mathcal{M}}(\Pi)\). Then,_

\[\mathbb{M}^{\star}=\mathrm{argmin}_{\mathbb{M}}\{\mathrm{KL}(\Pi|\mathbb{M}) \ :\ \mathbb{M}\in\mathcal{M}\},\]

\[\mathrm{KL}(\Pi|\mathbb{M}^{\star})=\tfrac{1}{2}\int_{0}^{T}\mathbb{E}_{\Pi_{ 0,t}}[\|\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|0,t}}[\nabla\log\mathbb{Q}_{T|t}( \mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{0},\mathbf{X}_{t}]-v_{t}^{\star }\|^{2}]/\sigma_{t}^{2}\mathrm{d}t.\]

_In addition, we have that for any \(t\in[0,T]\), \(\mathbb{M}^{\star}_{t}=\Pi_{t}\). In particular, \(\mathbb{M}^{\star}_{T}=\Pi_{T}\)._

Proof.: First, we recall that \(\Pi\) is given by \(\Pi=\mathbb{Q}\varphi_{0,T}\) with \(\varphi_{0,T}=\frac{\mathrm{d}\Pi_{0,T}}{\mathrm{d}\mathbb{Q}_{0,T}}\). In particular, we have \(\Pi_{|0}=\mathbb{Q}_{|0}\varphi_{T|0}\), where \(\varphi_{T|0}=\frac{\mathrm{d}\Pi_{T|0}}{\mathrm{d}\mathbb{Q}_{T|0}}\). Therefore, using Lemma 11, (Palmowski and Rolski, 2002, Lemma 3.1, Lemma 4.1), the remark following (Palmowski and Rolski, 2002, Lemma 4.1), **A**1, **A**2 and **A**3, we get that \(\Pi_{|0}\) is Markov and associated with the distribution of \((\mathbf{X}_{t})_{t\in[0,T]}\) given for any \(t\in[0,T]\) by

\[\mathbf{X}_{t}=\int_{0}^{t}\{f_{s}(\mathbf{X}_{s})+\sigma_{s}^{2}\nabla\log \varphi_{s|0}(\mathbf{X}_{s}|\mathbf{X}_{0})\}\mathrm{d}s+\int_{0}^{t}\sigma_{s }\mathrm{d}\mathbf{B}_{s},\] (33)

where for any \(t\in[0,T]\), \(x_{0},x_{t}\in\mathbb{R}^{d}\) we recall that

\[\varphi_{t|0}(x_{t}|x_{0})=\int_{\mathbb{R}^{d}}\varphi_{T|0}(x_{T}|x_{0}) \mathrm{d}\mathbb{Q}_{T|t}(x_{T}|x_{t}).\] (34)

First, we have that for any \(t\in[0,T]\), \(x_{t},x_{0}\in\mathbb{R}^{d}\)

\[\mathbb{Q}_{t|0}(x_{t}|x_{0})\varphi_{t|0}(x_{t}|x_{0})=\int_{\mathbb{R}^{d}} \mathbb{Q}_{t|0,T}(x_{t}|x_{T},x_{0})\mathrm{d}\Pi_{T|0}(x_{T}|x_{0})=\Pi_{t| 0}(x_{t}|x_{0}).\]

Therefore, we get that for any \(t\in[0,T]\) and \(x_{t},x_{0}\in\mathbb{R}^{d}\)

\[\varphi_{t|0}(x_{t}|x_{0})=\frac{\mathrm{d}\Pi_{t|0}(x_{t}|x_{0})}{\mathrm{d} \mathbb{Q}_{t|0}(x_{t}|x_{0})}.\] (35)

In addition, we have the following identity for any \(t\in[0,T]\), \(x_{0},x_{t},x_{T}\in\mathbb{R}^{d}\)

\[\mathbb{Q}_{T|0}(x_{T}|x_{0})\mathbb{Q}_{t|0,T}(x_{t}|x_{0},x_{T})=\mathbb{Q}_{ t|0}(x_{t}|x_{0})\mathbb{Q}_{T|t}(x_{T}|x_{t}).\]

Using (34), this result and (35), we get that for any \(t\in[0,T]\) and \(x_{0},x_{t}\in\mathbb{R}^{d}\)

\[\nabla\log\varphi_{t|0}(x_{t}|x_{0}) =\int_{\mathbb{R}^{d}}\frac{\Pi_{t|0}(x_{T}|x_{0})\mathrm{d} \mathcal{Q}_{T|t}(x_{T}|x_{t})}{\mathrm{d}\mathbb{Q}_{t|0}(x_{t}|x_{0})\psi_{t }(x_{T}|x_{t})}\nabla\log\mathbb{Q}_{T|t}(x_{T}|x_{t})\mathrm{d}x_{T}\] \[=\int_{\mathbb{R}^{d}}\frac{\Pi_{t|0}(x_{T}|x_{0})\mathrm{d} \mathcal{Q}_{T|t}(x_{T}|x_{0},x_{T})}{\mathrm{d}\mathbb{Q}_{t|0}(x_{t}|x_{0}) \psi_{t}(x_{T}|x_{0})}\nabla\log\mathbb{Q}_{T|t}(x_{T}|x_{t})\mathrm{d}x_{T}\] \[=\int_{\mathbb{R}^{d}}\frac{\Pi_{t,T|0}(x_{t},x_{T}|x_{t})}{\Pi_{t |0}(x_{t}|x_{0})}\nabla\log\mathbb{Q}_{T|t}(x_{T}|x_{t})\mathrm{d}x_{T}\] \[=\int_{\mathbb{R}^{d}}\nabla\log\mathbb{Q}_{T|t}(x_{T}|x_{t}) \mathrm{d}\Pi_{T|t,0}(x_{T}|x_{t},x_{0}).\]

Hence, combining this result and (33), we get

\[\mathbf{X}_{t}=\int_{0}^{t}\{f_{s}(\mathbf{X}_{s})+\sigma_{s}^{2}\mathbb{E}_{ \Pi_{T|t,0}}[\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid \mathbf{X}_{t},\mathbf{X}_{0}]\}\mathrm{d}s+\int_{0}^{t}\sigma_{s}\mathrm{d} \mathbf{B}_{s}.\]

Let \(\mathbb{M}\) be Markov defined by \(\mathrm{d}\mathbf{X}_{t}=\{f_{t}(\mathbf{X}_{t})+v_{t}(\mathbf{X}_{t})\} \mathrm{d}t+\sigma_{t}\mathrm{d}\mathbf{B}_{t}\), such that \(\mathrm{KL}(\Pi|\mathbb{M})<+\infty\) with \(\sigma,v\) locally Lipschitz. Using (Leonard, 2012, Theorem 2.3), we get that

\[\mathrm{KL}(\Pi|\mathbb{M}) =\tfrac{1}{2}\int_{0}^{T}\mathbb{E}_{\Pi_{0,t}}[\|\sigma_{t}^{2} \mathbb{E}_{\Pi_{T|t,0}}[\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t}) \mid\mathbf{X}_{t},\mathbf{X}_{0}]-v_{t}(\mathbf{X}_{t})\|^{2}]/\sigma_{t}^{2} \mathrm{d}t.\]

In addition, we have that for any \(t\in[0,T]\),

\[\mathbb{E}_{\Pi_{0,t}}[\|\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t,0}}[ \nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}, \mathbf{X}_{0}]-v_{t}(\mathbf{X}_{t})\|^{2}]\] \[\geq\mathbb{E}_{\Pi_{0,t}}[\|\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t,0}}[ \nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}, \mathbf{X}_{0}]-v_{t}^{\star}(\mathbf{X}_{t})\|^{2}],\]

where \(v_{t}^{\star}(x_{t})=\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t}}[\nabla\log\mathbb{Q}_ {T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}=x_{t}]\) which concludes the first part of the proof. For the second part of the proof, we show that for any \(t\in[0,T]\), we have \(\mathbb{M}^{\star}_{t}=\Pi_{t

### Proof of Proposition 4

Proof.: By the additive property of KL divergence (Leonard, 2014a), \(\mathrm{KL}(\mathbb{P}|\Pi)=\mathrm{KL}(\mathbb{P}_{0,T}|\Pi_{0,T})+\mathbb{E}_{ \mathbb{P}_{0,T}}[\mathrm{KL}(\mathbb{P}_{|0,T}|\Pi_{|0,T})]\). Restricting \(\Pi_{|0,T}=\bar{\mathbb{Q}}_{|0,T}\) directly gives that the KL minimizer is \(\Pi^{\star}\) with \(\Pi^{\star}_{0,T}=\mathbb{P}_{0,T}\), and thus \(\Pi^{\star}=\mathbb{P}_{0,T}\bar{\mathbb{Q}}_{|0,T}\) which we recall is short for \(\Pi^{\star}(\cdot)=\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\mathbb{Q}_{|0,T}( \cdot|x_{0},x_{T})\mathbb{P}_{0,T}(\mathrm{d}x_{0},\mathrm{d}x_{T})\). 

### Proof of Proposition 5

This result is a direct consequence of (Leonard, 2014b, Theorem 2.12), which we recall here for completeness.

**Proposition**.: _Assume that \(\mathbb{Q}\in\mathcal{M}\), that \(\mathbb{Q}_{0}=\mathbb{Q}_{T}=\bar{\mathbb{Q}}\), that for any \(x_{0},x_{T}\in\mathbb{R}^{d}\), \(\mathrm{d}\mathbb{Q}_{0,T}/\mathrm{d}(\bar{\mathbb{Q}}\otimes\bar{\mathbb{Q}} )(x_{0},x_{T})\geq\exp[-A(x_{0})-A(x_{T})]\) with \(A\geq 0\) measurable, \(\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\exp[-B(x_{0})-B(x_{T})]\mathrm{d} \mathbb{Q}(x_{0},x_{T})<+\infty\) with \(B\geq 0\) measurable. Assume that there exists \(t_{0}\in(0,T)\) and \(\mathsf{X}\) measurable such that \(\mathbb{Q}_{t_{0}}(\mathsf{X})>0\) and for all \(x\in\mathsf{X}\), \(\mathbb{Q}_{0,T}\ll\mathbb{Q}_{0,T|t_{0}}(\cdot|\mathbf{X}_{t_{0}}=x)\). In addition, assume that \(\mathrm{KL}(\pi_{0}|\bar{\mathbb{Q}})<+\infty\), \(\mathrm{KL}(\pi_{T}|\bar{\mathbb{Q}})<+\infty\), \(\int_{\mathbb{R}^{d}}(A+B)(x_{0})\mathrm{d}\pi_{0}(x_{0})<+\infty\), \(\int_{\mathbb{R}^{d}}(A+B)(x_{T})\mathrm{d}\pi_{T}(x_{T})<+\infty\)._

_Then there exists a unique Schrodinger Bridge \(\mathbb{P}^{\text{SB}}\). In addition let \(\mathbb{P}\) be a Markov measure in the reciprocal class of \(\mathbb{Q}\) such that \(\mathbb{P}_{0}=\pi_{0}\) and \(\mathbb{P}_{T}=\pi_{T}\). Assume that \(\mathrm{KL}(\mathbb{P}|\bar{\mathbb{Q}})<+\infty\). Then \(\mathbb{P}\) is the unique Schrodinger Bridge \(\mathbb{P}^{\text{SB}}\)._

Proof.: The first part of the proof is a consequence of (Leonard, 2014b, Theorem 2.12(a)). The second part is a consequence of (Leonard, 2014b, Theorem 2.12(b)) and (Leonard et al., 2014, Theorem 2.14). 

### Proof of Lemma 6

**Lemma**.: _Let \(\mathbb{M}\in\mathcal{M}\) and \(\Pi\in\mathcal{R}(\mathbb{Q})\) and assume \(\mathbf{A}1\), \(\mathbf{A}2\), \(\mathbf{A}3\). If \(\mathrm{KL}(\Pi|\mathbb{M})<+\infty\) and \(\mathrm{KL}(\mathrm{proj}_{\mathcal{M}}(\Pi)|\mathbb{M})<+\infty\) we have_

\[\mathrm{KL}(\Pi|\mathbb{M})=\mathrm{KL}(\Pi|\mathrm{proj}_{\mathcal{M}}(\Pi))+ \mathrm{KL}(\mathrm{proj}_{\mathcal{M}}(\Pi)|\mathbb{M}).\] (36)

_For any \(\mathbb{P}\in\mathcal{P}(\mathcal{C})\), if \(\mathrm{KL}(\mathbb{P}|\Pi)<+\infty\), we have_

\[\mathrm{KL}(\mathbb{P}|\Pi)=\mathrm{KL}(\mathbb{P}|\mathrm{proj}_{\mathcal{R}( \mathbb{Q})}(\mathbb{P}))+\mathrm{KL}(\mathrm{proj}_{\mathcal{R}(\mathbb{Q}) }(\mathbb{P})|\Pi).\] (37)

Proof.: We start with the proof of (36). Similarly to Proposition 2, where we have \(\mathbb{M}\in\mathcal{M}\) to ensure that we can apply (Leonard, 2012, Theorem 2.3), we get

\[\mathrm{KL}(\Pi|\mathbb{M})=\tfrac{1}{2}\int_{0}^{T}\mathbb{E}_{\Pi_{0,t}}[ \|v_{t}(\mathbf{X}_{t})-\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|0,t}}[\nabla\log \mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{0},\mathbf{X}_ {t}]\|^{2}]/\sigma_{t}^{2}\mathrm{d}t.\]

In addition, we have

\[\mathrm{KL}(\mathrm{proj}_{\mathcal{M}}(\Pi)|\mathbb{M})=\tfrac{1}{2}\int_{0}^{ T}\mathbb{E}_{\Pi_{t}}[\|v_{t}(\mathbf{X}_{t})-\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t}}[ \nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t} ]\|^{2}]/\sigma_{t}^{2}\mathrm{d}t.\]

Finally, using Proposition 2, we have that

\[\mathrm{KL}(\Pi|\mathrm{proj}_{\mathcal{M}}(\Pi))\] \[=\tfrac{1}{2}\int_{0}^{T}\mathbb{E}_{\Pi_{0,t}}[\|\sigma_{t}^{2} \mathbb{E}_{\Pi_{T|0,t}}[\nabla\log\mathbb{Q}(\mathbf{X}_{T}|\mathbf{X}_{t}) \mid\mathbf{X}_{0},\mathbf{X}_{t}]-\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t}}[ \nabla\log\mathbb{Q}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}]\|^{2}] /\sigma_{t}^{2}\mathrm{d}t\] \[=\tfrac{1}{2}\int_{0}^{T}(\mathbb{E}_{\Pi_{0,t}}[\|\mathbb{E}_{ \Pi_{T|0,t}}[\nabla\log\mathbb{Q}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_ {0},\mathbf{X}_{t}]\|^{2}]-\mathbb{E}_{\Pi_{t}}[\|\mathbb{E}_{\Pi_{T|t}}[ \nabla\log\mathbb{Q}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}]\|^{2}]) \sigma_{t}^{2}\mathrm{d}t.\]Using this result, we have

\[2\mathrm{KL}(\Pi|\mathrm{proj}_{\mathcal{M}}(\Pi))+2\mathrm{KL}( \mathrm{proj}_{\mathcal{M}}(\Pi)|\mathbb{M})\] \[=\int_{0}^{T}(\mathbb{E}_{\Pi_{0,t}}[\|\mathbb{E}_{\Pi_{T|0,t}}[ \nabla\log\mathbb{Q}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{0},\mathbf{ X}_{t}]\|^{2})-\mathbb{E}_{\Pi_{t}}[\|\mathbb{E}_{\Pi_{T|t}}[\nabla\log \mathbb{Q}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}]\|^{2})\sigma_{t}^ {2}\mathrm{d}t\] \[+\int_{0}^{T}\mathbb{E}_{\Pi_{t}}[\|v_{t}(\mathbf{X}_{t})/\sigma_{ t}^{2}-\mathbb{E}_{\Pi_{T|t}}[\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t} )\mid\mathbf{X}_{t}]\|^{2}]\sigma_{t}^{2}\mathrm{d}t\] \[=\int_{0}^{T}(\mathbb{E}_{\Pi_{0,t}}[\|\mathbb{E}_{\Pi_{T|0,t}}[ \nabla\log\mathbb{Q}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{0},\mathbf{ X}_{t}]\|^{2})-\mathbb{E}_{\Pi_{t}}[\|\mathbb{E}_{\Pi_{T|t}}[\nabla\log\mathbb{Q}( \mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}]\|^{2}))\sigma_{t}^{2} \mathrm{d}t\] \[+\int_{0}^{T}(\mathbb{E}_{\Pi_{t}}[\|v_{t}(\mathbf{X}_{t})/\sigma_ {t}^{2}\|^{2}]+\mathbb{E}_{\Pi_{t}}[\|\mathbb{E}_{\Pi_{T|t}}[\nabla\log\mathbb{ Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}]\|^{2}))\sigma_{t}^{2} \mathrm{d}t\] \[-2\int_{0}^{T}\mathbb{E}_{\Pi_{t}}[\langle v_{t}(\mathbf{X}_{t})/ \sigma_{t}^{2},\mathbb{E}_{\Pi_{T|t}}[\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T} |\mathbf{X}_{t})\mid\mathbf{X}_{t}]\rangle]\sigma_{t}^{2}\mathrm{d}t\] \[=\int_{0}^{T}\mathbb{E}_{\Pi_{0,t}}[\|\mathbb{E}_{\Pi_{T|0,t}}[ \nabla\log\mathbb{Q}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{0},\mathbf{ X}_{t}]\|^{2}]\sigma_{t}^{2}\mathrm{d}t+\int_{0}^{T}\mathbb{E}_{\Pi_{t}}[\|v_{t}( \mathbf{X}_{t})/\sigma_{t}^{2}\|^{2}]\sigma_{t}^{2}\mathrm{d}t\] \[-2\int_{0}^{T}\mathbb{E}_{\Pi_{t}}[\langle v_{t}(\mathbf{X}_{t})/ \sigma_{t}^{2},\mathbb{E}_{\Pi_{T|t}}[\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T }|\mathbf{X}_{t})\mid\mathbf{X}_{0},\mathbf{X}_{t}]\rangle]\sigma_{t}^{2} \mathrm{d}t=2\mathrm{KL}(\Pi|\mathbb{M}),\]

which concludes the first part of the proof.

For (37), define \(\Pi^{\star}=\mathrm{proj}_{\mathcal{R}(\mathbb{Q})}(\mathbb{P})=\mathbb{P}_{0,T }\mathbb{Q}_{|0,T}\). Using (Csiszar, 1975, Equation 2.6), we have

\[\mathrm{KL}(\mathbb{P}|\Pi) =\mathrm{KL}(\mathbb{P}|\Pi^{\star})+\int_{\mathcal{C}}\log( \frac{\mathrm{d}\Pi^{\star}}{\mathrm{d}\Pi\Gamma}(\omega))\mathrm{d}\mathbb{P}(\omega)\] \[=\mathrm{KL}(\mathbb{P}|\Pi^{\star})+\int_{\mathbb{R}^{d}\times \mathbb{R}^{d}}\log(\frac{\mathrm{d}\Pi^{\star}_{0,T}}{\mathrm{d}\Pi_{0,T}}(x_ {0},x_{1}))\mathrm{d}\mathbb{P}_{0,T}(x_{0},x_{1})\] \[=\mathrm{KL}(\mathbb{P}|\Pi^{\star})+\int_{\mathbb{R}^{d}\times \mathbb{R}^{d}}\log(\frac{\mathrm{d}\Pi^{\star}_{0,T}}{\mathrm{d}\Pi_{0,T}}(x_ {0},x_{1}))\mathrm{d}\Pi^{\star}_{0,T}(x_{0},x_{1})=\mathrm{KL}(\mathbb{P}| \Pi^{\star})+\mathrm{KL}(\Pi^{\star}|\Pi),\]

which concludes the proof. 

### Proof of Proposition 7

**Proposition**.: _Assume that the conditions of Proposition 5 and Lemma 6 apply for \(\mathbb{P}^{n}\) for every \(n\in\mathbb{N}\) and for the Schrodinger Bridge \(\mathbb{P}^{\text{SB}}\), we have \(\mathrm{KL}(\mathbb{P}^{n+1}|\mathbb{P}^{\text{SB}})\leq\mathrm{KL}(\mathbb{P}^ {n}|\mathbb{P}^{\text{SB}})<\infty\), and \(\lim_{n\to+\infty}\mathrm{KL}(\mathbb{P}^{n}|\mathbb{P}^{n+1})=0\)._

Proof.: We follow the technique of Ruschendorf (1995) but for the _reverse_ Kullback-Leibler divergence. Applying Lemma 6, we get for any \(N\in\mathbb{N}\)

\[\mathrm{KL}(\mathbb{P}^{0}|\mathbb{P}^{\text{SB}})=\mathrm{KL}(\mathbb{P}^{0}| \mathbb{P}^{1})+\mathrm{KL}(\mathbb{P}^{1}|\mathbb{P}^{\text{SB}})=\sum_{i=0}^{ N}\mathrm{KL}(\mathbb{P}^{i}|\mathbb{P}^{i+1})+\mathrm{KL}(\mathbb{P}^{N+1}| \mathbb{P}^{\text{SB}}),\]

which concludes the proof. 

### Proof of Theorem 8

**Theorem**.: _Assume that the conditions of Proposition 5 and Lemma 6 apply for \(\mathbb{P}^{n}\) for every \(n\in\mathbb{N}\) and for the Schrodinger Bridge \(\mathbb{P}^{\text{SB}}\), the IMF sequence \((\mathbb{P}^{n})_{n\in\mathbb{N}}\) admits a unique fixed point \(\mathbb{P}^{\star}=\mathbb{P}^{\text{SB}}\), and \(\lim_{n\to+\infty}\mathrm{KL}(\mathbb{P}^{n}|\mathbb{P}^{\star})=0\)._

Proof.: By Proposition 7, \(\mathrm{KL}(\mathbb{P}^{n}|\mathbb{P}^{\text{SB}})\leq\mathrm{KL}(\mathbb{P}^{0}| \mathbb{P}^{\text{SB}})<\infty\) for all \(n\in\mathbb{N}\). Now, using the coercivity of \(\mathrm{KL}(\cdot|\mathbb{P}^{\text{SB}})\) (this is where our analysis differs from the one of the IPF), we have that the IMF sequence \((\mathbb{P}^{n})_{n\in\mathbb{N}}\) and its subsequences \((\mathbb{M}^{n+1})_{n\in\mathbb{N}}\) and \((\Pi^{n})_{n\in\mathbb{N}}\) are subsets of \(\{\mathbb{P}\in\mathcal{P}(\mathcal{C}):\mathrm{KL}(\mathbb{P}|\mathbb{P}^{\text{SB}}) \leq\mathrm{KL}(\mathbb{P}^{0}|\mathbb{P}^{\text{SB}})\}\) which is (relatively) compact. Thus, \((\mathbb{M}^{n+1})_{n\in\mathbb{N}}\) contains a convergent subsequence \(\mathbb{M}^{n_{j}}\to\mathbb{M}^{\star}\) as \(j\to\infty\), and \((\Pi^{n_{j}})_{j\in\mathbb{N}}\) contains a further convergent subsequence \(\Pi^{n_{j_{k}}}\to\Pi^{\star}\) weakly as \(k\to\infty\). As the Markov and the reciprocal classes are closed under weak convergence, \(\mathbb{M}^{\star}\in\mathcal{M}\) and \(\Pi^{\star}\in\mathcal{R}(\mathbb{Q})\). Now, by the lower semi-continuity of KL divergence in the weak topology (van Erven and Harremoes, 2014, Theorem 19), \(0\leq\mathrm{KL}(\mathbb{M}^{\star}|\Pi^{\star})\leq\liminf_{k\to\infty} \mathrm{KL}(\mathbb{M}^{n_{jk}}|\Pi^{n_{jk}})=0\). Hence, \(\mathbb{M}^{\star}=\Pi^{\star}\) which we denote as \(\mathbb{P}^{\star}\in\mathcal{M}\cap\mathcal{R}(\mathbb{Q})\). Also, \(\mathbb{P}^{\star}\) satisfies \(\mathbb{P}^{\star}_{0}=\pi_{0}\) and \(\mathbb{P}^{\star}_{T}=\pi_{T}\) as is satisfied by all \(\mathbb{P}^{n}\). By Proposition 5, \(\mathbb{P}^{\star}\) is the unique Schrodinger Bridge \(\mathbb{P}^{\text{SB}}\). Finally, \(\lim_{n\to+\infty}\mathrm{KL}(\mathbb{P}^{n}|\mathbb{P}^{\star})=0\) follows using \(\lim_{k\to\infty}\mathrm{KL}(\mathbb{M}^{n_{jk}}|\mathbb{P}^{\star})=0\) and the monotonicity of \(\mathrm{KL}(\mathbb{P}^{n}|\mathbb{P}^{\text{SB}})\) by Proposition 7.

### Proof of Proposition 9

Proof.: The proof is similar to the one of Proposition 2. 

In particular, the time-reversal of \(\mathbb{Q}_{|0,T}(\cdot|x_{0},x_{T})\) is associated with

\[\mathrm{d}\mathbf{Y}_{t}^{0,T}=\{-f_{T-t}(\mathbf{Y}_{t}^{0,T})+ \sigma_{T-t}^{2}\nabla\log\mathbb{Q}_{T-t|0}(\mathbf{Y}_{t}^{0,T}|x_{0})\} \mathrm{d}t+\sigma_{T-t}\mathrm{d}\mathbf{B}_{t},\qquad\mathbf{Y}_{0}^{0,T}=x _{T}.\] (38)

One can view both (11) (12) as SDEs with drift defined as the conditional expectation of the drift of (2) (38) under \(\Pi_{0,T|t}\) in the forward and backward directions respectively.

### Proof of Proposition 10

Proof.: We proceed by induction. Firstly, for \(\Pi_{0,T}^{0}=\mathbb{Q}_{0,T}\), \(\Pi^{0}=\tilde{\mathbb{P}}^{0}=\mathbb{Q}\) at initialization. We can also define \(\mathbb{M}^{0}=\mathbb{Q}\), such that \(\mathbb{M}^{0}=\tilde{\mathbb{P}}^{0}\) and \(\Pi^{n}=\mathrm{proj}_{\mathcal{R}(\mathbb{Q})}(\mathbb{M}^{n})\) for all \(n\in\mathbb{N}\). By (De Bortoli et al., 2021, Section 3.5), the optimal DSB sequence \(\tilde{\mathbb{P}}^{n}\) is Markov and \(\tilde{\mathbb{P}}^{n}=\tilde{\mathbb{P}}_{0,T}^{n}\mathbb{Q}_{|0,T}\), where \(\tilde{\mathbb{P}}_{0,T}^{n}\) is the IPF sequence of the static SB problem. In other words, \(\tilde{\mathbb{P}}^{n}\in\mathcal{M}\cap\mathcal{R}(\mathbb{Q})\).

Suppose \(\mathbb{M}^{2n+1}=\tilde{\mathbb{P}}^{2n+1}\). By definition, \(\mathbb{M}_{0}^{2n+2}=\tilde{\mathbb{P}}_{0}^{2n+2}=\pi_{0}\), i.e. both forward processes are initialized at \(\pi_{0}\). In DSB, by De Bortoli et al. (2021), \(\tilde{\mathbb{P}}^{2n+2}\) is defined as the time-reversal of \(\tilde{\mathbb{P}}^{2n+1}\), such that \(\tilde{\mathbb{P}}_{|0}^{2n+2}=\tilde{\mathbb{P}}_{|0}^{2n+1}\). Hence, \(\tilde{\mathbb{P}}^{2n+2}=\pi_{0}\tilde{\mathbb{P}}_{|0}^{2n+1}\).

In DSBM, we first perform reciprocal projection \(\Pi^{2n+1}=\mathrm{proj}_{\mathcal{R}(\mathbb{Q})}(\mathbb{M}^{2n+1})=\mathbb{ M}_{0,T}^{2n+1}\mathbb{Q}_{|0,T}\). Since \(\mathbb{M}^{2n+1}=\tilde{\mathbb{P}}^{2n+1}\in\mathcal{R}(\mathbb{Q})\), however, we have that \(\Pi^{2n+1}=\mathbb{M}^{2n+1}\). Furthermore, since \(\mathbb{M}^{2n+1}=\tilde{\mathbb{P}}^{2n+1}\in\mathcal{M}\), \(\mathrm{proj}_{\mathcal{M}}(\Pi^{2n+1})=\mathrm{proj}_{\mathcal{M}}(\mathbb{ M}^{2n+1})=\mathbb{M}^{2n+1}\). Thus, \(\mathbb{M}^{2n+2}\) given by (9) is such that \(\mathbb{M}_{0}^{2n+2}=\pi_{0}\) and \(\mathbb{M}_{|0}^{2n+2}=\mathrm{proj}_{\mathcal{M}}(\Pi^{2n+1})_{|0}=\mathbb{ M}_{|0}^{2n+1}\). We conclude that \(\mathbb{M}^{2n+2}=\pi_{0}\mathbb{M}_{|0}^{2n+1}=\tilde{\mathbb{P}}^{2n+2}\). Similar arguments holds for the the reverse projection (13). Therefore, \(\mathbb{M}^{n}=\tilde{\mathbb{P}}^{n}\) for all \(n\in\mathbb{N}\). 

### The set of Markov measures is not convex

The result of Lemma 6 should be compared with the information geometry result of (Csiszar, 1975, Theorem 2.2), which states that if \(\mathcal{C}\) is a convex set and \(\mathbb{P}\in\mathcal{C}\), then under mild conditions, \(\mathrm{KL}(\mathbb{P}|\mathbb{Q})=\mathrm{KL}(\mathbb{P}|\mathrm{proj}_{ \mathcal{C}}(\mathbb{Q}))+\mathrm{KL}(\mathrm{proj}_{\mathcal{C}}(\mathbb{Q}) |\mathbb{Q})\), where \(\mathrm{proj}_{\mathcal{C}}(\mathbb{Q})=\mathrm{argmin}_{\mathbb{P}}\{ \mathrm{KL}(\mathbb{P}|\mathbb{Q})\ :\ \mathbb{P}\in\mathcal{C}\}\) is the projection of \(\mathbb{Q}\) on \(\mathbb{C}\). Note that, contrary to Lemma 6, (Csiszar, 1975, Theorem 2.2) is given for the _forward_ Kullback-Leibler divergence whereas Lemma 6 is given for the _reverse_ KL divergence. In addition, (Csiszar, 1975, Theorem 2.2) requires the projection set \(\mathcal{C}\) to be convex which is not satisfied for the space of Markov measures \(\mathcal{M}\). We give a simple counter-example proving that the set of Markov measures is not convex.

Let \(p_{1}(x_{0},x_{1},x_{2})=p_{1}(x_{0})p_{1}(x_{1}|x_{0})p_{1}(x_{2}|x_{1})\) and \(p_{2}(x_{0},x_{1},x_{2})=p_{2}(x_{0})p_{2}(x_{1}|x_{0})p_{2}(x_{2}|x_{1})\) on \(\{0,1\}^{3}\) such that

\[p_{1}(x_{0}=1)=\alpha_{0},\qquad p_{1}(x_{1}=1|x_{0})=\alpha_{1}, \qquad p_{1}(x_{2}=1|x_{1})=\alpha_{2}.\]

Additionally, we set

\[p_{2}(x_{0}=1)=\beta_{0},\qquad p_{2}(x_{1}=1|x_{0})=\beta_{1}, \qquad p_{2}(x_{2}=1|x_{1})=\beta_{2}.\]

Finally, we set \(q=(1/2)p_{1}+(1/2)p_{2}\). Consider \(q(x_{2}=1|x_{1}=1,x_{0}=1)=q(x_{2}=1,x_{1}=1,x_{0}=1)/q(x_{1}=1,x_{0}=1)\) and \(q(x_{2}=1|x_{1}=1)=q(x_{2}=1,x_{1}=1)/q(x_{1}=1)\). Let

\[\Delta =4[q(x_{2}=1,x_{1}=1,x_{0}=1)q(x_{1}=1)-q(x_{2}=1,x_{1}=1)q(x_{1 }=1,x_{0}=1)]\] \[=(\alpha_{0}\alpha_{1}\alpha_{2}+\beta_{0}\beta_{1}\beta_{2})( \alpha_{1}+\beta_{1})-(\alpha_{1}\alpha_{2}+\beta_{1}\beta_{2})(\alpha_{0} \alpha_{1}+\beta_{0}\beta_{1})\] \[=\alpha_{0}\alpha_{1}\beta_{1}\alpha_{2}+\beta_{0}\alpha_{1}\beta_{1 }\beta_{2}-\beta_{0}\alpha_{1}\beta_{1}\alpha_{2}-\alpha_{0}\alpha_{1}\beta_{1 }\beta_{2}\] \[=\alpha_{1}\beta_{1}\beta_{2}(\beta_{0}-\alpha_{0})+\alpha_{1} \beta_{1}\alpha_{2}(\alpha_{0}-\beta_{0})\] \[=\alpha_{1}\beta_{1}(\beta_{0}-\alpha_{0})(\beta_{2}-\alpha_{2}).\]

\(q\) is Markov if and only if \(\Delta=0\). Therefore \(q\) is not Markov as soon as \(\alpha_{0}\neq\beta_{0}\) and \(\alpha_{2}\neq\beta_{2}\).

### Impact of the resolution on the entropic regularization

In this section, we show how the resolution of the input affects the entropic regularization. First, we consider the Schrodinger bridge problem between \(\pi_{0}\) and \(\pi_{1}\) (\(T=1\)) with \(\mathbb{Q}\) associated with \((\sigma\mathbf{B}_{t})_{t\in[0,1]}\). In that case the _static_ Schrodinger Bridge is given by \(\mathbb{P}^{*}_{0,1}\) such that

\[\mathbb{P}^{*}_{0,1}=\operatorname{argmin}\{\int_{\mathbb{R}^{d}\times \mathbb{R}^{d}}\|x_{0}-x_{1}\|^{2}\mathrm{d}\mathbb{P}(x_{0},x_{1})-\varepsilon \mathrm{KL}(\mathbb{P}|\pi_{0}\otimes\pi_{1})\;:\;\mathbb{P}_{0}=\pi_{0},\; \mathbb{P}_{1}=\pi_{1}\}\]

where \(\varepsilon=2\sigma^{2}\). We now describe the solution of a higher dimensional problem given by the _upsampling_ of the marginals. Let \(f\in\mathbb{N}\) with \(f\geq 1\) and \(\operatorname{down}:\;\mathbb{R}^{fd}\to\mathbb{R}^{d}\) such that for any \(k\in\{1,\ldots,d\}\), and \(x\in\mathbb{R}^{fd}\), \(\operatorname{down}(x)=\bar{x}\) with \(\bar{x}\in\mathbb{R}^{d}\) and \(\bar{x}_{k}=x_{kf}\). We also denote \(\mathrm{up}:\;\mathbb{R}^{d}\to\mathbb{R}^{fd}\) such that for any \(k\in\{1,\ldots,d\}\), and \(\bar{x}\in\mathbb{R}^{d}\), \(\mathrm{up}(\bar{x})=x\) with \(x\in\mathbb{R}^{fd}\) and \(\bar{x}_{k}=x_{(k-1)f+j}\), for \(j\in\{1,\ldots,d\}\). Note that \(\operatorname{down}\circ\mathrm{up}=\mathrm{Id}\).

We denote \(\pi^{\mathrm{up}}_{0}=\mathrm{up}_{\#}\pi_{0}\) and \(\pi^{\mathrm{up}}_{1}=\mathrm{up}_{\#}\pi_{1}\). We extend \(\mathrm{up}\) and \(\operatorname{down}\) to \(\mathbb{R}^{d}\times\mathbb{R}^{d}\) and \(\mathbb{R}^{fd}\times\mathbb{R}^{fd}\) by letting for any \(\bar{x},\bar{y}\in\mathbb{R}^{d}\) and \(x,y\in\mathbb{R}^{fd}\)

\[\mathrm{up}(\bar{x},\bar{y})=(\mathrm{up}(\bar{x}),\mathrm{up}(\bar{y})), \qquad\operatorname{down}(x,y)=(\operatorname{down}(x),\operatorname{down}( y)).\]

First, note that since \(\mathrm{up}\circ\operatorname{down}=\mathrm{Id}\) on \(\mathrm{up}(\mathbb{R}^{d})\) we get that for any \(\mathbb{P}\) supported on \(\mathrm{up}(\mathbb{R}^{d})\times\mathrm{up}(\mathbb{R}^{d})\), using (Kullback, 1997, Theorem 4.1)

\[\mathrm{KL}(\mathbb{P}|\pi^{\mathrm{up}}_{0}\otimes\pi^{\mathrm{up}}_{1})= \mathrm{KL}(\operatorname{down}_{\#}\mathbb{P}|\pi_{0}\otimes\pi_{1}).\]

Second let \(\mathbb{P}\) such that \(\mathbb{P}_{0}=\pi^{\mathrm{up}}_{0}\) and \(\mathbb{P}_{1}=\pi^{\mathrm{up}}_{1}\). Then, \(\mathbb{P}\) is supported on \(\mathrm{up}(\mathbb{R}^{d})\times\mathrm{up}(\mathbb{R}^{d})\). Therefore, for any \(\mathbb{P}\) such that \(\mathbb{P}_{0}=\pi^{\mathrm{up}}_{0}\) and \(\mathbb{P}_{1}=\pi^{\mathrm{up}}_{1}\)

\[\mathrm{KL}(\mathbb{P}|\pi^{\mathrm{up}}_{0}\otimes\pi^{\mathrm{up}}_{1})= \mathrm{KL}(\operatorname{down}_{\#}\mathbb{P}|\pi_{0}\otimes\pi_{1}).\]

Finally, for any \(\mathbb{P}\) such that \(\mathbb{P}_{0}=\pi^{\mathrm{up}}_{0}\) and \(\mathbb{P}_{1}=\pi^{\mathrm{up}}_{1}\) we have

\[\int_{\mathbb{R}^{fd}\times\mathbb{R}^{fd}}\|x_{0}-x_{1}\|^{2}\mathrm{d} \mathbb{P}(x_{0},x_{1})=f^{2}\int_{\mathbb{R}^{d}\times\mathbb{R}^{fd}}\|\bar{ x}_{0}-\bar{x}_{1}\|^{2}\mathrm{d}(\operatorname{down}_{\#}\mathbb{P})(\bar{x}_{0}, \bar{x}_{1}).\]

Therefore, for any \(\mathbb{P}\) such that \(\mathbb{P}_{0}=\pi^{\mathrm{up}}_{0}\) and \(\mathbb{P}_{1}=\pi^{\mathrm{up}}_{1}\) we have

\[\int_{\mathbb{R}^{fd}\times\mathbb{R}^{fd}}\|x_{0}-x_{1}\|^{2} \mathrm{d}\mathbb{P}(x_{0},x_{1})-\varepsilon\mathrm{KL}(\mathbb{P}|\pi^{ \mathrm{up}}_{0}\otimes\pi^{\mathrm{up}}_{1})\] \[\qquad\qquad=f^{2}(\int_{\mathbb{R}^{fd}\times\mathbb{R}^{fd}}\| \bar{x}_{0}-\bar{x}_{1}\|^{2}\mathrm{d}(\operatorname{down}_{\#}\mathbb{P})( \bar{x}_{0},\bar{x}_{1})-(\varepsilon/f^{2})\mathrm{KL}(\operatorname{down}_{ \#}\mathbb{P}|\pi_{0}\otimes\pi_{1})).\]

Therefore, we have the following result.

**Proposition 12**.: _Let \(\varepsilon>0\). \(\mathbb{P}^{*}\) is the solution of the static Schrodinger bridge with marginals \(\pi^{\mathrm{up}}_{0}\), \(\pi^{\mathrm{up}}_{1}\) and regularization \(\varepsilon\) if and only if \(\operatorname{down}_{\#}\mathbb{P}^{*}\) is the solution of the static Schrodinger bridge with marginals \(\pi_{0}\), \(\pi_{1}\) and regularization \(\varepsilon/f^{2}\)._

This means in particular that the Schrodinger Bridge is not invariant via upsampling. In Appendix I.4, we confirm these results visually upon noting that for the same \(\sigma\), running DSBM at resolution \(64\times 64\) and \(128\times 128\) gives different results, even after downsampling of the \(128\times 128\) results.

## Appendix D Convergence of IMF in the Gaussian setting

In this section, we study the IMF in an one-dimensional Gaussian case. We consider \(T=1\), \(\Pi_{0}=\Pi_{1}=\mathrm{N}(0,(1/2\beta^{2}))\) and \(\mathbb{Q}\) associated with \((\sigma\mathbf{B}_{t})_{t\in[0,1]}\) where \(\sigma>0\). In what follows, we let

\[\Sigma^{0}=(1/2\beta^{2})\begin{pmatrix}1&c^{2}&a_{k}^{t}\\ c^{2}&1&a_{k}^{1-t}\\ a_{k}^{t}&a_{k}^{1-t}&b_{k}^{t}\end{pmatrix},\] (39)

where \(c\in[0,1]\). We also denote \(\bar{\sigma}^{2}=2\sigma^{2}\beta^{2}\). We start with the following result which gives an explicit expression of some marginals of the reciprocal projection.

**Lemma 13**.: _Let \(\Pi^{0}_{0,1}=\mathrm{N}(0,\Sigma^{0})\) with \(\Sigma^{0}\) given by (39). Let \(\Pi^{0}=\Pi^{0}_{0,1}\mathbb{Q}_{|0,1}\). For any \(t\in[0,1]\), we have that \(\Pi^{0}_{0,1,t}=\mathrm{N}(0,\Sigma)\) with_

\[\Sigma=(1/2\beta^{2})\begin{pmatrix}1&c^{2}&a_{k}^{t}\\ c^{2}&1&a_{k}^{1-t}\\ a_{k}^{t}&a_{k}^{1-t}&b_{k}^{t}\end{pmatrix},\]

_where we have_

\[a_{k}^{t}=1-t+tc^{2},\qquad b_{k}^{t}=1+t(1-t)(2(c^{2}-1)+\bar{\sigma}^{2}).\]Proof.: Let \(t\in[0,1]\) and \(\mathbf{X}_{t}^{0,1}\sim\Pi_{t|0,1}^{0}=\mathbb{Q}_{t|0,1}\). Using (Barczy and Kern, 2013, Theorem 3.3), we get that

\[\mathbf{X}_{t}^{0,1}=(1-t)\mathbf{X}_{0}+t\mathbf{X}_{1}+\sigma(t(1-t))^{1/2} \mathbf{Z},\]

with \(\mathbf{Z}\sim\mathrm{N}(0,\mathrm{Id})\) independent from \((\mathbf{X}_{0},\mathbf{X}_{1})\). Hence, we get that \(\mathbb{E}[\mathbf{X}_{t}^{0,1}]=0\), and

\[\mathrm{Cov}(\mathbf{X}_{0},\mathbf{X}_{t}^{0,1})=\mathbb{E}[\mathbf{X}_{t}^{0,1}\mathbf{X}_{0}]=(1-t)/(2\beta^{2})+tc^{2}/(2\beta^{2})=a_{k}^{t}/(2\beta^{2 }).\]

Similarly, we get that \(\mathrm{Cov}(\mathbf{X}_{1},\mathbf{X}_{t}^{0,1})=a_{k}^{1-t}/(2\beta^{2})\). Finally, we get that

\[\mathrm{Var}(\mathbf{X}_{t}^{0,1}) =\mathbb{E}[((1-t)\mathbf{X}_{0}+t\mathbf{X}_{1})^{2}]+\sigma^{2 }t(1-t)\] \[=(1-t)^{2}/(2\beta^{2})+2t(1-t)c^{2}/(2\beta^{2})+t^{2}/(2\beta^{ 2})+\bar{\sigma}^{2}t(1-t)/(2\beta^{2})\] \[=(1-2t+2t^{2}+2t(1-t)c^{2}+\bar{\sigma}^{2}t(1-t))/(2\beta^{2})\] \[=(1+t(1-t)(2(c^{2}-1)+\bar{\sigma}^{2}))/(2\beta^{2}),\]

which concludes the proof. 

Leveraging Lemma 13, we can give an explicit expression of the drift term in the Markovian projection.

**Lemma 14**.: _Let \(\Pi_{0,1}^{0}=\mathrm{N}(0,\Sigma^{0})\) with \(\Sigma^{0}\) given by (39). Let \(\Pi^{0}=\Pi_{0,1}^{0}\mathbb{Q}_{|0,1}\). For any \(t\in[0,1]\) and \(x_{t}\in\mathbb{R}^{d}\), we have that_

\[\sigma^{2}\mathbb{E}_{\Pi_{1|t}^{0}}[\nabla\log\mathbb{Q}_{1|t}|\mathbf{X}_{t} =x_{t}]=\tfrac{(1-2t)(c^{2}-1)-\bar{\sigma}^{2}t}{1+t(1-t)(2(c^{2}-1)+\bar{ \sigma}^{2})}x_{t}.\]

_Hence, the Markovian projection of \(\Pi^{0}\), denoted \(\mathbb{M}^{1}\) is associated with \((\mathbf{X}_{t})_{t\in[0,1]}\) with_

\[\mathbf{X}_{0}\sim\Pi_{0},\qquad\mathrm{d}\mathbf{X}_{t}=\tfrac{(1-2t)(c^{2}- 1)-\bar{\sigma}^{2}t}{1+t(1-t)(2(c^{2}-1)+\bar{\sigma}^{2})}\mathbf{X}_{t}+ \sigma\mathrm{d}\mathbf{B}_{t}.\] (40)

Proof.: Using (Barczy and Kern, 2013, Theorem 3.2), we get that \(\mathbb{Q}_{|0,1}\) is associated with

\[\mathrm{d}\mathbf{X}_{t}^{0,1}=\sigma^{2}\nabla\log\mathbb{Q}_{1|t}(x_{1}| \mathbf{X}_{t})\mathrm{d}t+\sigma\mathrm{d}\mathbf{B}_{t},\qquad\mathbf{X}_{0 }^{0,1}=x_{0},\]

where for any \(t\in[0,1)\), we have and \(x_{t}\in\mathbb{R}^{d}\), \(\sigma^{2}\nabla\log\mathbb{Q}_{1|t}(x_{1}|x_{t})=(x_{1}-x_{t})/(1-t)\). Therefore, we get that \(\mathbb{M}^{1}\) is associated with \((\mathbf{X}_{t})_{t\in[0,1]}\) such that

\[\mathrm{d}\mathbf{X}_{t}=\mathbb{E}_{\Pi_{1|t}^{0}}[\sigma^{2}\nabla\log \mathbb{Q}_{1|t}(\mathbf{X}_{1}|\mathbf{X}_{t})|\mathbf{X}_{t}]\mathrm{d}t+ \sigma\mathrm{d}\mathbf{B}_{t},\qquad\mathbf{X}_{0}=x_{0}.\]

Therefore, we get that

\[\mathrm{d}\mathbf{X}_{t}=(\mathbb{E}_{\Pi_{1|t}^{0}}[\mathbf{X}_{1}|\mathbf{X }_{t}]-x_{t})/(1-t)\mathrm{d}t+\sigma\mathrm{d}\mathbf{B}_{t},\qquad\mathbf{X} _{0}=x_{0}.\]

Using Lemma 13, we have that for any \(t\in[0,1]\) and \(x_{t}\in\mathbb{R}^{d}\), \(\mathbb{E}_{\Pi_{1|t}^{0}}[\mathbf{X}_{1}|\mathbf{X}_{t}=x_{t}]=a_{k}^{1-t}/b_{ k}^{t}x_{t}\). In addition, we have for any \(t\in[0,1]\)

\[a_{k}^{1-t}/b_{k}^{t}-1 =(t+(1-t)c^{2}-1-t(1-t)(2(c^{2}-1)+\bar{\sigma}^{2}))/(1+t(1-t)(2( c^{2}-1)+\bar{\sigma}^{2}))\] \[=((1-t)(c^{2}-1)-t(1-t)(2(c^{2}-1)+\bar{\sigma}^{2}))/(1+t(1-t)(2( c^{2}-1)+\bar{\sigma}^{2}))\] \[=(1-t)((c^{2}-1)-t(2(c^{2}-1)+\bar{\sigma}^{2}))/(1+t(1-t)(2(c^{2} -1)+\bar{\sigma}^{2}))\] \[=(1-t)((1-2t)(c^{2}-1)-t\bar{\sigma}^{2}))/(1+t(1-t)(2(c^{2}-1)+ \bar{\sigma}^{2})),\]

which concludes the proof. 

Note that since \(\sigma>0\) and \(c^{2}\in[0,1]\), we get that for any \(t\in[0,1]\), \(1+t(1-t)(2(c^{2}-1)+\bar{\sigma}^{2})>0\) and therefore the drift is well-defined, smooth and sublinear. In particular, (40) admits a unique strong solution. In what follows, we denote \(G:\,[0,1]\times[0,1]\to\mathbb{R}\) given for any \(t\in[0,1]\) by

\[G(t,c^{2})=\int_{0}^{t}\tfrac{(1-2s)(c^{2}-1)-\bar{\sigma}^{2}s}{1+s(1-s)(2(c^{2 }-1)+\bar{\sigma}^{2})}\mathrm{d}s.\]

We have the following useful lemma.

**Lemma 15**.: _Let \(c\in[0,1]\), \(\bar{\sigma}>0\) and \(p=2c^{2}+\bar{\sigma}^{2}\). We distinguish three cases:_

1. _If_ \(p<2\)_, then_ \[G(1,c^{2})=-\bar{\sigma}^{2}(4-p^{2})^{-1/2}\tan^{-1}((4-p^{2})^{1/2}/p).\]
2. _If_ \(p=2\)_, then_ \[G(1,c^{2})=-\bar{\sigma}^{2}/2.\]
3. _If_ \(p>2\)_, then_ \[G(1,c^{2})=-\bar{\sigma}^{2}(p^{2}-4)^{-1/2}\tanh^{-1}((p^{2}-4)^{1/2}/p).\]

This lemma is useful combined with the following proposition, which gives the analytical update formula of Gaussian IMF covariances as a function of \(G(1,c^{2})\).

**Proposition 16**.: _Let \(\Pi^{0}_{0,1}=\mathrm{N}(0,\Sigma^{0})\) with \(\Sigma^{0}\) given by (39). Then \(\Pi^{1}_{0,1}=\mathrm{N}(0,\Sigma^{1})\) with_

\[\Sigma^{1}=(1/2\beta^{2})\begin{pmatrix}1&c_{1}^{2}\\ c_{1}^{2}&1\end{pmatrix},\qquad c_{1}^{2}=f(c_{0}^{2}),\]

_with \(f:\ [0,1]\to[0,1]\) given for any \(c\in[0,1]\) by_

\[f(c^{2})=\exp[G(1,c^{2})].\]

Proof.: We have that \(\mathbf{X}_{1}=\exp[G(1,c^{2})]\mathbf{X}_{0}+\mathbf{M}_{1}\), where \(\mathbf{M}_{1}\) is a Gaussian random variable with zero mean independent from \(\mathbf{X}_{0}\). Therefore, we get that \(\mathrm{Cov}(\mathbf{X}_{0},\mathbf{X}_{1})=\exp[G(1,c^{2})]/(2\beta^{2})\). In addition, we have that \(\mathbb{E}[\mathbf{X}_{1}^{2}]=\mathbb{E}[\mathbf{X}_{0}^{2}]=1/2\beta^{2}\), which concludes the proof. 

Iterating the procedure in Proposition 16, we obtain a sequence of IMF covariances \((c_{n}^{2})_{n\in\mathbb{N}}\) satisfying \(c_{n+1}^{2}=f(c_{n}^{2})\). Finally, we show that this iterative procedure recovers the true SB coupling \(\Pi^{\text{SB}}_{0,1}=\mathrm{N}(0,\Sigma^{\text{SB}})\) as a fixed point. The formula of \(\Sigma^{\text{SB}}\) is given e.g. in (Bunne et al., 2023, Equation (2)) which we use below.

**Proposition 17**.: _Let \(\Pi^{\text{SB}}_{0,1}=\mathrm{N}(0,\Sigma^{\text{SB}})\) be the true static SB solution, with_

\[\Sigma^{\text{SB}}=(1/2\beta^{2})\begin{pmatrix}1&c_{\text{SB}}^{2}\\ c_{\text{SB}}^{2}&1\end{pmatrix},\qquad c_{\text{SB}}^{2}=\frac{1}{2}(\sqrt{4 +\bar{\sigma}^{4}}-\bar{\sigma}^{2}).\]

_Then \(\Pi^{\text{SB}}_{0,1}\) is a fixed point of the iterative procedure in Proposition 16, i.e. \(f(c_{\text{SB}}^{2})=c_{\text{SB}}^{2}\)._

Proof.: By straightforward calculations, \(p_{\text{SB}}=2c_{\text{SB}}^{2}+\bar{\sigma}^{2}=\sqrt{4+\bar{\sigma}^{4}}\). If \(\bar{\sigma}=0\), \(p_{\text{SB}}=2\) and thus \(G(1,c_{\text{SB}}^{2})=-\bar{\sigma}^{2}/2=0\). Hence \(f(c_{\text{SB}}^{2})=\exp(G(1,c_{\text{SB}}^{2}))=1=c_{\text{SB}}^{2}\). If \(\bar{\sigma}>0\), we have \(p_{\text{SB}}>2\) and \(G(1,c_{\text{SB}}^{2})=-\tanh^{-1}(\bar{\sigma}^{2}/\sqrt{4+\bar{\sigma}^{4}})\). Hence, \(f(c_{\text{SB}}^{2})=\exp(G(1,c_{\text{SB}}^{2}))=\frac{1}{2}(\sqrt{4+\bar{ \sigma}^{4}}-\bar{\sigma}^{2})=c_{\text{SB}}^{2}\). We thus correctly recover the true static SB solution \(\Pi^{\text{SB}}_{0,1}\) as fixed point of IMF. 

We visualize the convergence of this fixed point procedure for a variety of parameter settings in Figure 12. The convergence appears to be very fast in only two or three iterations.

Figure 12: Convergence of IMF in the analytic case given by Proposition 16.

Discrete-Time Markovian Projection

We derive in this section a discrete-time version of the Markovian projection and show that, in some limiting case, we recover the continuous-time projection. In the discrete case, we let

\[\pi(x_{0:N}) =\pi(x_{0},x_{N})\prod_{k=0}^{N-1}q_{k+1|0,k,N}(x_{k+1}|x_{0},x_{k},x_{N})\] \[=\pi(x_{0},x_{N})\prod_{k=0}^{N-2}q_{k+1|k,N}(x_{k+1}|x_{k},x_{N}).\]

We consider a Markovian measure \(p\) given by \(p(x_{0:N})=p(x_{0})\prod_{k=0}^{N-1}p_{k+1|k}(x_{k+1}|x_{k}).\) Now let us compute \(\mathrm{KL}(\pi|p)\). We have

\[\mathrm{KL}(\pi(x_{0:N})|p(x_{0:N})) =\sum_{k=0}^{N-2}\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\mathrm{ KL}(q_{k+1|k,N}|p_{k+1|k})\pi_{k,N}(x_{k},x_{N})\mathrm{d}x_{k}\mathrm{d}x_{N}\] \[\qquad+\mathrm{KL}(\pi_{0}|p_{0})+\int_{\mathbb{R}^{d}}\mathrm{ KL}(\pi(x_{N}|x_{0})|p(x_{N}|x_{N-1}))\pi(x_{0},x_{N-1})\mathrm{d}x_{0} \mathrm{d}x_{N-1}.\]

In what follows, we denote

\[\mathcal{L}_{0}=\mathrm{KL}(\pi_{0}|p_{0}),\mathcal{L}_{N}=\int_ {\mathbb{R}^{d}}\mathrm{KL}(\pi(x_{N}|x_{0})|p(x_{N}|x_{N-1}))\pi(x_{0},x_{N-1 })\mathrm{d}x_{0}\mathrm{d}x_{N-1},\] \[\mathcal{L}_{k+1}=\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}} \mathrm{KL}(q_{k+1|k,N}|p_{k+1|k})\pi_{k,N}(x_{k},x_{N})\mathrm{d}x_{k} \mathrm{d}x_{N},\]

We have the following proposition.

**Proposition 18**.: _The minimizer \(p_{k+1|k}\) of \(\mathcal{L}_{k+1}\) is given by_

\[p_{k+1|k}(x_{k+1}|x_{k})=\int_{\mathbb{R}^{d}}q_{k+1|k,N}(x_{k+1}|x_{k},x_{N} )\pi_{N|k}(x_{N}|x_{k})\mathrm{d}x_{N}.\] (41)

_If \(p_{0}=q_{0}\), then for any \(k\in\{0,\dots,N-1\}\), \(p_{k}=\pi_{k}\). In addition, assume that \(p_{k+1|k}(x_{k+1}|x_{k})=\exp[-\|x_{k+1}-x_{k}-\gamma f(x_{k})\|^{2}/(2\gamma) ]/(2\pi\gamma)^{-d/2}\) and \(q_{k+1|k,N}(x_{k+1}|x_{k},x_{N})=\exp[-\|x_{k+1}-x_{k}-\gamma f(x_{k},x_{N})\| ^{2}/(2\gamma)]/(2\pi\gamma)^{d/2}\). Finally, assume that \(\|x_{k+1}-x_{k}\|\leq\gamma^{1/2}\). Then, we have that_

\[f(x_{k})=\int_{\mathbb{R}^{d}}f(x_{k},x_{N})\pi(x_{N}|x_{k})\mathrm{d}x_{N}+o (\gamma^{1/2}).\] (42)

Proof.: The proofs of (41) and (42) are straightforward and left to the reader. We now prove that if \(p_{0}=\pi_{0}\), then for any \(k\in\{1,\dots,N\}\), \(p_{k}=\pi_{k}\). First, we have that for any \(k\in\{0,\dots,N-1\}\),

\[\pi(x_{k},x_{k+1},x_{N})=\pi(x_{k},x_{N})q(x_{k+1}|x_{k},x_{N}).\]

Assume now that \(p_{k}=\pi_{k}\), then we have

\[p_{k+1}(x_{k+1}) =\int_{\mathbb{R}^{d}}p_{k}(x_{k})p_{k+1|k}(x_{k+1}|x_{k})\mathrm{ d}x_{k}\] \[=\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}p_{k}(x_{k})q_{k+1|k,N}( x_{k+1}|x_{k},x_{N})\pi_{N|k}(x_{N}|x_{k})\mathrm{d}x_{k}\mathrm{d}x_{N}\] \[=\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\pi_{k}(x_{k})q_{k+1|k,N }(x_{k+1}|x_{k},x_{N})\pi_{N|k}(x_{N}|x_{k})\mathrm{d}x_{k}\mathrm{d}x_{N}\] \[=\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\pi_{k,k+1,N}(x_{k},x_{k +1},x_{N})\mathrm{d}x_{k}\mathrm{d}x_{N}=\pi_{k+1}(x_{k+1}),\]

which concludes the proof. 

In particular, in the previous proposition, if \(f(x_{k},x_{N})=\nabla\log q(x_{N}|x_{k})\), i.e. we have a discretization of the bridge then \(f(x_{k})=\int_{\mathbb{R}^{d}}\nabla\log q(x_{N}|x_{K})\pi(x_{N}|x_{k}) \mathrm{d}x_{N}\), which recovers the Markovian projection in continuous-time.

## Appendix F Comparing DSBM-IPF and DSB

We analyze further the differences between DSBM-IPF proposed here and DSB proposed in De Bortoli et al. (2021) and related algorithms in Vargas et al. (2021); Chen et al. (2022). All algorithms solve the SB problem using the IPF iterates. However, DSB-type algorithms solve for the IPF iterates using _time-reversals_, whereas DSBM solves for the iterates using _Markovian_ and _reciprocal projections_. A comparison between these two methodologies is made in Section 4.

We investigate here further benefit (iii) of DSBM in Section 4, i.e. the benefit of explicitly projecting onto the reciprocal class of \(\mathbb{Q}\). Intuitively speaking, we directly incorporate the reference measure \(\mathbb{Q}\) in the training procedure as our inductive bias. More formally, suppose we have at the current IPF iteration \(\mathbb{M}^{2n}\) (Markov diffusion in the forward direction) and want to learn \(\mathbb{M}^{2n+1}\) (Markov diffusion in the backward direction). Due to training error and the _forgetting_ issue (Fernandes et al., 2021), however, \(\mathbb{M}^{2n}\) no longer has the correct bridge \(\mathbb{Q}_{|0,T}\). Now suppose we first perform IMF for \(\mathbb{M}^{2n}\) and learn \(\mathbb{M}^{2n,\star}\) in the forward direction. That is to say, we repeat alternative reciprocal and Markovian projections and obtain a sequence \((\mathbb{M}^{2n,m})_{m\in\mathbb{N}}\) in the forward direction converging to \(\mathbb{M}^{2n,\star}\). Then \(\mathbb{M}^{2n,\star}\) now has the correct bridge \(\mathbb{Q}_{|0,T}\) by Proposition 7, since \(\mathbb{M}^{2n,\star}\) is the SB between \(\mathbb{M}^{2n}_{0}\) and \(\mathbb{M}^{2n}_{T}\). Theoretically, \(\mathbb{M}^{2n}_{t}=\mathbb{M}^{2n,\star}_{t}\) for \(t=0,T\), but due to training error accumulating it may be that \(\mathbb{M}^{2n}_{T}\neq\mathbb{M}^{2n,\star}_{T}\). However, \(\mathbb{M}^{2n}_{0}=\mathbb{M}^{2n,\star}_{0}\), since \(\mathbb{M}^{2n,\star}\) is in the forward direction and starting from samples from \(\pi_{0}\). As a result, we can obtain a Markov forward diffusion \(\mathbb{M}^{2n,\star}\), which has \(\mathbb{M}^{2n,\star}_{0}=\pi_{0}\) and the correct bridge \(\mathbb{Q}_{|0,T}\). These are the same set of properties that the reference measure \(\mathbb{Q}\) has. As a result, replacing \(\mathbb{Q}\) with \(\mathbb{M}^{2n,\star}\) in (6) results in the same SB solution. Consequently, now continuing the IPF iterations from \(\mathbb{M}^{2n,\star}_{0}\), it is as if we restart IPF afresh using a modified SB problem

\[\mathbb{P}^{\text{SB}}=\operatorname{argmin}\{\operatorname{KL}(\mathbb{P}| \mathbb{M}^{2n,\star})\ :\ \mathbb{P}_{0}=\pi_{0},\ \mathbb{P}_{T}=\pi_{T}\}.\]

If \(\mathbb{M}^{2n,\star}\) is closer than \(\mathbb{Q}\) to \(\mathbb{M}^{\star}\) in the sense of KL divergence, then we obtain a better initialization of the IPF procedure. As proposed in Algorithm 1, DSBM performs the Markovian and reciprocal projection only once before switching between the forward and backward directions. However, it is still beneficial compared to DSB with less bias accumulation in the bridge.

Algorithmically, one main difference between DSB-type algorithms and DSBM due to the above distinction occurs in the trajectory caching step. In DSB, a fixed discretization of SDE needs to be chosen, and all intermediate samples from the discretized Euler-Maruyama simulation of the SDE need to be saved. Furthermore, a second set of drift evaluation needs to be performed for all datapoints in the trajectory (De Bortoli et al., 2021, Equations (12), (13)). The IPML algorithm in Vargas et al. (2021) is also similar to DSB, but Gaussian processes are used to fit the drifts of forward and backward SDEs instead of neural networks. In Chen et al. (2022), the implicit score matching loss is used instead, but all intermediate points in the SDE trajectory also need to be saved. On the contrary, DSBM does not require intermediate samples during trajectory caching and only retains the joint samples at times \(0,T\). Then, the intermediate trajectories are reconstructed using the reference bridge \(\mathbb{Q}_{|0,T}\).

## Appendix G Joint Training of Forward and Backward Processes

We recall below the DSBM algorithm given in Algorithm 1.

```
1:Input: Joint distribution \(\Pi^{0}_{0,T}\), tractable bridge \(\mathbb{Q}_{|0,T}\), number of outer iterations \(N\in\mathbb{N}\).
2:Let \(\Pi^{0}=\Pi^{0}_{0,T}\mathbb{Q}_{|0,T}\).
3:for\(n\in\{0,\ldots,N-1\}\)do
4:Learn \(v_{\phi^{\star}}\) using (14) with \(\Pi=\Pi^{2n}\).
5: Let \(\mathbb{M}^{2n+1}\) be given by (13).
6: Let \(\Pi^{2n+1}=\mathbb{M}^{2n+1}_{0,T}\mathbb{Q}_{|0,T}\).
7: Learn \(v_{\theta^{\star}}\) using (10) with \(\Pi=\Pi^{2n+1}\).
8: Let \(\mathbb{M}^{2n+2}\) be given by (9).
9: Let \(\Pi^{2n+2}=\mathbb{M}^{2n+2}_{0,T}\mathbb{Q}_{|0,T}\).
10:endfor
11:Output:\(v_{\theta^{\star}}\), \(v_{\phi^{\star}}\) ```

**Algorithm 2** Diffusion Schrodinger Bridge Matching

Our main observation comes from Proposition 9. In particular, under mild assumptions, we have that the Markovian projection \(\mathbb{M}=\operatorname{proj}_{\mathcal{M}}(\Pi)\) is associated with

\[\mathrm{d}\mathbf{X}_{t} =\{f_{t}(\mathbf{X}_{t})+\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t}}[ \nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}] \}\mathrm{d}t+\sigma_{t}\mathrm{d}\mathbf{B}_{t},\quad\mathbf{X}_{0}\sim\pi_{0},\] \[\mathrm{d}\mathbf{Y}_{t} =\{-f_{T-t}(\mathbf{Y}_{t})+\sigma_{T-t}^{2}\mathbb{E}_{\Pi_{0|T-t }}[\nabla\log\mathbb{Q}_{T-t|0}(\mathbf{Y}_{t}|\mathbf{Y}_{T})\mid\mathbf{Y}_{ t}]\}\mathrm{d}t+\sigma_{T-t}\mathrm{d}\mathbf{B}_{t},\quad\mathbf{Y}_{0}\sim\pi_{T}.\]Considering the following losses,

\[\theta^{\star} =\operatorname*{argmin}_{\theta}\{\int_{0}^{T}\mathbb{E}_{\Pi_{t,T}}[ \|\sigma_{t}^{2}\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})-v_{ \theta}(t,\mathbf{X}_{t})\|^{2}]/\sigma_{t}^{2}\mathrm{d}t\ :\ \theta\in\Theta\},\] (43) \[\phi^{\star} =\operatorname*{argmin}_{\phi}\{\int_{0}^{T}\mathbb{E}_{\Pi_{t,0 }}[\|\sigma_{t}^{2}\nabla\log\mathbb{Q}_{t|0}(\mathbf{X}_{t}|\mathbf{X}_{0})-v _{\phi}(t,\mathbf{X}_{t})\|^{2}]/\sigma_{t}^{2}\mathrm{d}t\ :\ \phi\in\Phi\}.\] (44)

If the families of functions \(\{v_{\theta}\ :\ \theta\in\Theta\}\) and \(\{v_{\phi}\ :\ \theta\in\Phi\}\) are rich enough, we have for any \(t\in[0,T]\) and \(x_{t}\in\mathbb{R}^{d}\), \(v_{\theta^{\star}}(t,x_{t})=\sigma_{t}^{2}\mathbb{E}_{\Pi_{T|t}}[\nabla\log \mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})\mid\mathbf{X}_{t}=x_{t}]\) and \(v_{\phi^{\star}}(t,x_{t})=\sigma_{t}^{2}\mathbb{E}_{\Pi_{0|t}}[\nabla\log \mathbb{Q}_{t|0}(\mathbf{X}_{t}|\mathbf{X}_{0})\mid\mathbf{X}_{t}=x_{t}]\). In practice, this means that the Markovian projection can be computed in a forward _or_ backward fashion equivalently.

Therefore, given a coupling \(\Pi=\Pi^{2n}\), we can update _both_\(v_{\theta}\) and \(v_{\phi}\). This means that we train the forward and backward model _jointly_. We then consider \(\mathbb{M}_{b}^{2n+1}\) associated with (13) and \(\mathbb{M}_{f}^{2n+1}\) associated with (9). Note that if the families of functions \(\{v_{\theta}\ :\ \theta\in\Theta\}\) and \(\{v_{\phi}\ :\ \theta\in\Phi\}\) are rich enough then \(\mathbb{M}_{f}^{2n+1}=\mathbb{M}_{b}^{2n+1}\).

Mixture from forward and backward.Once we have obtained both the forward update and the backward update, our next task is to define the new mixture of bridge \(\Pi^{2n+1}\). In Algorithm 1, since we train only the _backward_ model \(\mathbb{M}^{2n+1}=\mathbb{M}_{b}^{2n+1}\), we define \(\Pi^{2n+1}=\mathbb{M}_{0,T}^{2n+1}\mathbb{Q}_{|0,T}\). In the case of _joint training_, we have access to \(\mathbb{M}_{b}^{2n+1}\) and \(\mathbb{M}_{f}^{2n+1}\). One way to define a new mixture of bridge is to compute \(\Pi^{2n+1}=\frac{1}{2}(\mathbb{M}_{b,0,T}^{2n+1}\mathbb{Q}_{|0,T}+\mathbb{M}_ {f,0,T}^{2n+1})\mathbb{Q}_{|0,T}\). This choice ensures that in the case where \(\mathbb{M}_{f}^{2n+1}=\mathbb{M}_{b}^{2n+1}\) we have

\[\Pi^{2n+1}=\mathbb{M}_{f,0,T}^{2n+1}\mathbb{Q}_{|0,T}=\mathbb{M}_{b,0,T}^{2n+ 1}\mathbb{Q}_{|0,T}.\]

It also ensures that all the steps in the _joint_ DBSM training algorithms are symmetric. We leave the study of an optimal combination of \(\mathbb{M}_{f}^{2n+1}\) and \(\mathbb{M}_{b}^{2n+1}\) for future work.

Consistency loss.In addition to the losses (43) and (44), we also consider an additional _consistency_ loss. A similar idea was explored in Song (2022). In DSB (De Bortoli et al., 2021; Chen et al., 2022) and DSBM, see Algorithm 1, the processes parameterized by \(v_{\theta}\) (forward) and \(v_{\phi}\) backward are identical _only at equilibrium_. Thus imposing the forward and the backward processes match at each step of DSB or DSBM would lead to some bias. However, this is not the case in the _joint training_ setting. Indeed, in that case, we have \(\mathbb{M}_{f}^{2n+1}=\mathbb{M}_{b}^{2n+1}\) if the families are rich enough. Therefore, we get that

\[\mathrm{d}\mathbf{Y}_{t}=\{-f_{T-t}(\mathbf{Y}_{t})+v_{\phi}(T-t,\mathbf{Y}_ {t})\}\mathrm{d}t+\sigma_{T-t}\mathrm{d}\mathbf{B}_{t},\qquad\mathbf{Y}_{0} \sim\pi_{T},\] (45)

is the time reversal of

\[\mathrm{d}\mathbf{X}_{t}=\{f_{t}(\mathbf{X}_{t})+v_{\theta}(t,\mathbf{X}_{t} )\}\mathrm{d}t+\sigma_{t}\mathrm{d}\mathbf{B}_{t},\qquad\mathbf{X}_{0}\sim\pi _{0}.\] (46)

Computing the time-reversal of (45), we have

\[\mathrm{d}\mathbf{X}_{t}=\{f_{t}(\mathbf{X}_{t})-v_{\phi}(t,\mathbf{X}_{t})+ \sigma_{t}^{2}\nabla\log\Pi_{t}^{2n}(\mathbf{X}_{t})\}\mathrm{d}t+\sigma_{t} \mathrm{d}\mathbf{B}_{t},\qquad\mathbf{X}_{0}\sim\pi_{0}.\] (47)

Identifying (47) and (46), we get that for any \(t\in[0,T]\) and \(x_{t}\in\mathbb{R}^{d}\)

\[v_{\theta}(t,x_{t})=-v_{\phi}(t,x_{t})+\sigma_{t}^{2}\nabla\log\Pi_{t}^{2n}(x _{t}).\] (48)

We highlight that letting \(\sigma_{t}\to 0\) for any \(t\in[0,T]\), we get that \(v_{\theta}=-v_{\phi}\), which confirms that the time-reversal of an ODE is simply given by flipping the sign of the velocity. Therefore, we propose the following loss which links the parameters \(\theta\) and \(\phi\)

\[\mathcal{L}_{\mathrm{cons}}(\theta,\phi)=\int_{0}^{T}\mathbb{E}_{\Pi_{t}^{2n} }[\|v_{\theta}(t,\mathbf{X}_{t})+v_{\phi}(t,\mathbf{X}_{t})-\sigma_{t}^{2} \nabla\log\Pi_{t}^{2n}(\mathbf{X}_{t})\|^{2}]/\sigma_{t}^{2}\mathrm{d}t.\]

Leveraging tools from implicit score matching (Hyvarinen, 2005) and the divergence theorem, we get that

\[\mathcal{L}_{\mathrm{cons}}(\theta,\phi)=\int_{0}^{T}\mathbb{E}_{\Pi_{t}^{2n} }[\|v_{\theta}(t,\mathbf{X}_{t})+v_{\phi}(t,\mathbf{X}_{t})\|^{2}/\sigma_{t}^{ 2}+2\mathrm{div}(v_{\theta}(t,\mathbf{X}_{t})+v_{\phi}(t,\mathbf{X}_{t}))] \mathrm{d}t+C,\]

where \(C\geq 0\) is a constant which does not depend on \(\theta\) and \(\phi\). Alternatively, (48) shows that

\[\nabla\log\Pi_{t}^{2n}(x_{t})=\mathbb{E}_{\Pi_{T|t}^{2n}}[\nabla\log\mathbb{Q} _{T|t}|\mathbf{X}_{t}=x_{t}]+\mathbb{E}_{\Pi_{0|t}^{2n}}[\nabla\log\mathbb{Q} _{t|0}|\mathbf{X}_{t}=x_{t}].\]We thus also have the following denoising score matching consistency loss

\[\mathcal{L}_{\mathrm{cons}}(\theta,\phi)=\int_{0}^{T}\mathbb{E}_{\Pi_{0,T}^{2n}}[ \|v_{\theta}(t,\mathbf{X}_{t})+v_{\phi}(t,\mathbf{X}_{t})-\sigma_{t}^{2}(\nabla \log\mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})+\nabla\log\mathbb{Q}_{t|0} (\mathbf{X}_{t}|\mathbf{X}_{0}))\|^{2}]/\sigma_{t}^{2}\mathrm{d}t,\]

The advantage of this DSM loss is that it does not rely on any divergence computation.

Below, we recall the two losses used to estimate the Markovian projection (43) and (44)

\[\mathcal{L}(\theta) =\int_{0}^{T}\mathbb{E}_{\Pi_{t,T}}[\|\sigma_{t}^{2}\nabla\log \mathbb{Q}_{T|t}(\mathbf{X}_{T}|\mathbf{X}_{t})-v_{\theta}(t,\mathbf{X}_{t}) \|^{2}]/\sigma_{t}^{2}\mathrm{d}t,\] \[\mathcal{L}(\phi) =\int_{0}^{T}\mathbb{E}_{\Pi_{t,0}}[\|\sigma_{t}^{2}\nabla\log \mathbb{Q}_{t|0}(\mathbf{X}_{t}|\mathbf{X}_{0})-v_{\phi}(t,\mathbf{X}_{t})\|^ {2}]/\sigma_{t}^{2}\mathrm{d}t.\]

The complete loss we consider in the joint training of the algorithm is of the form

\[\mathcal{L}_{\lambda}(\theta,\phi)=\mathcal{L}(\theta)+\mathcal{L}(\phi)+ \lambda\mathcal{L}_{\mathrm{cons}}(\theta,\phi),\] (49)

where \(\lambda>0\) is an additional regularization parameter. We now state a version of DSBM which performs _joint training_ in Algorithm 3.

```
1:Input: Coupling \(\Pi_{0,T}^{0}\), tractable bridge \(\mathbb{Q}_{]0,T}\), \(N\in\mathbb{N}\)
2:Let \(\Pi^{0}=\Pi_{0,T}^{0}\mathbb{Q}_{]0,T}\).
3:for\(n\in\{0,\dots,N-1\}\)do
4:Learn \(v_{\phi^{*}},v_{\theta^{*}}\) using (49) with \(\Pi=\Pi^{n}\).
5: Let \(\mathbb{M}_{t}^{n+1}\) be given by (9).
6: Let \(\mathbb{M}_{t}^{n+1}\) be given by (13).
7: Let \(\mathbb{M}^{n+1}=\frac{1}{2}(\mathbb{M}_{t}^{n+1}+\mathbb{M}_{b}^{n+1})\).
8: Let \(\Pi^{n+1}=\mathbb{M}_{0,T}^{n+1}\mathbb{Q}_{]0,T}\).
9:endfor
10:Output:\(v_{\theta^{*}}\), \(v_{\phi^{*}}\) ```

**Algorithm 3** Diffusion Schrodinger Bridge Matching (Joint Training)

## Appendix H Loss Scaling

Similar to the loss weighting in standard diffusion models (Song et al., 2021b; Ho et al., 2020), we derive a similar weighting to reduce the variance of our objective. We focus on the forward direction of Markovian projection in this case, and the backward case can be derived similarly. Our forward loss in the DSBM framework is given by (10), where the inner expectation is given by

\[\mathbb{E}_{\Pi_{t,T}}[\|\sigma_{t}^{2}\nabla\log\mathbb{Q}_{T|t}(\mathbf{X}_ {T}|\mathbf{X}_{t})-v_{\theta}(t,\mathbf{X}_{t})\|^{2}].\]

Letting \(\mathbb{Q}_{]0,T}\) be a Brownian bridge with diffusion parameter \(\sigma\) and assuming \(T=1\), this becomes

\[\mathbb{E}_{(\mathbf{X}_{0},\mathbf{X}_{1})\sim\Pi_{0,1},\mathbf{Z}\sim \mathcal{N}(0,\mathrm{Id})}[\|\mathbf{X}_{1}-\mathbf{X}_{0}-\sigma\sqrt{t/(1-t )}\mathbf{Z}-v_{\theta}(t,\mathbf{X}_{t}^{0,T})\|^{2}]\]

with \(\mathbf{X}_{t}^{0,T}=t\mathbf{X}_{1}+(1-t)\mathbf{X}_{0}+\sigma\sqrt{t(1-t)} \mathbf{Z}\). When \(t\approx 1\), we see that the regression target is dominated by the noise term \(\sigma\sqrt{t/(1-t)}\mathbf{Z}\) which needs to be predicted based on information contained within \(\mathbf{X}_{t}^{0,T}\). The loss will have an approximate scale of \(\sigma^{2}t/(1-t)\) when \(t\approx 1\) which will be very large. To avoid these large values affecting gradient descent, we can downweight the loss by \(1+\sigma^{2}t/(1-t)\) (we can add \(1\) to effectively cause no loss scaling when \(t\) is close to \(0\))

\[(1+\sigma^{2}t/(1-t))^{-1}\mathbb{E}_{(\mathbf{X}_{0},\mathbf{X}_{1})\sim\Pi_{ 0,1},\mathbf{Z}\sim\mathcal{N}(0,\mathrm{Id})}[\|\mathbf{X}_{1}-\mathbf{X}_{0 }-\sigma\sqrt{t/(1-t)}\mathbf{Z}-v_{\theta}(t,\mathbf{X}_{t}^{0,T})\|^{2}].\]

Similar arguments can be applied to the backward loss (14)

\[\mathbb{E}_{\Pi_{t,0}}[\|\sigma_{t}^{2}\nabla\log\mathbb{Q}_{t|0} (\mathbf{X}_{t}|\mathbf{X}_{0})-v_{\phi}(t,\mathbf{X}_{t})\|^{2}]\] \[=\mathbb{E}_{(\mathbf{X}_{0},\mathbf{X}_{1})\sim\Pi_{0,1},\mathbf{ Z}\sim\mathcal{N}(0,\mathrm{Id})}[\|\mathbf{X}_{0}-\mathbf{X}_{1}-\sigma\sqrt{(1-t )/t}\mathbf{Z}-v_{\phi}(t,\mathbf{X}_{t}^{0,T})\|^{2}],\]

which we then downweight by \(1+\sigma^{2}(1-t)/t\)

\[(1+\sigma^{2}(1-t)/t)^{-1}\mathbb{E}_{(\mathbf{X}_{0},\mathbf{X}_{1})\sim\Pi_{ 0,1},\mathbf{Z}\sim\mathcal{N}(0,\mathrm{Id})}[\|\mathbf{X}_{0}-\mathbf{X}_{1 }-\sigma\sqrt{(1-t)/t}\mathbf{Z}-v_{\phi}(t,\mathbf{X}_{t}^{0,T})\|^{2}].\]

[MISSING_PAGE_FAIL:34]

[MISSING_PAGE_FAIL:35]

fixed to 0.999. We refresh the cache dataloader every 500 steps with 10000 new samples. The SDE sampler is chosen to be the modified Euler-Maruyama sampler, see Heng et al. (2021) for instance, with a constant schedule for the stepsizes. We use 100 sampling steps at inference time and to refresh the cache. For each outer DSBM iteration we train the model for 20000 iterations.

We provide additional transfer results in resolution \(128\times 128\) in Figure 16. We do not change the training setting for this experiment.

### AFHQ Transfer Experiment

For AFHQ (Choi et al., 2020), we test DSBM between classes cat and wild with \(512\times 512\) resolution images. Each class contains approximately 5000 samples. We first pretrain the networks using Bridge Matching for 100000 steps, then run DSBM for 20 iterations with 25000 steps per outer iteration. We follow Liu et al. (2023b) and use the same U-Net architecture8. The batch size is 4 and the EMA rate is 0.999. We choose \(\sigma^{2}=5\) and again we use 100 sampling steps with constant stepsizes.

Footnote 8: https://github.com/gnobitab/RectifiedFlow/blob/main/ImageGeneration/configs/rectified_flow/afhq_cat_pytorch_rf_gaussian.py (code released under Apache-2.0 license)

### CIFAR-10 Generative Modeling Experiment

We also test our method in the standard generative modeling framework on the CIFAR-10 dataset. We again use a U-Net architecture with 4 resolution levels and 2 residual blocks per resolution level. The network size is approximately 39.6 million parameters. The batch size is fixed to 128 and the EMA rate is fixed to 0.9999. The AdamW optimizer is used for this task. For DSBM-IMF and Rectified Flow, again we first pretrain the networks using the Bridge Matching or Flow Matching losses, then switch to DSBM-IMF or RF training with 100000 steps per outer iteration. We find that 1 or 2 additional outer iterations appear sufficiently effective on this task, and additional outer iterations can cause sample quality to drop. For our main experiment, the pretraining stage ran for approximately 6 days, and DSBM-IMF ran for approximately 4 additional days using 4 V100 GPUs.

The best results during training for each method are reported in Table 5 and Figure 17, where we compute the FID score between 50000 samples in the CIFAR-10 training set and 50000 generated samples following standard practice for this task. We observe that DSBM-IMF can clearly improve upon Bridge Matching at the same value of \(\sigma\), which suggests that further outer iterations in DSBM is beneficial for improving sample quality. This is contrary to Rectified Flow, which causes the FID score to worsen compared to Flow Matching after only 1 rectified iteration. However, as \(\sigma\) increases,

Figure 13: Samples of MNIST digits transferred from the EMNIST letters using different methods. @ indicates the progressed number of outer iterations \(n+1\).

Figure 14: Left: EMNIST to MNIST sample trajectory with 30 diffusion steps at \(t=0,1/3,2/3,1\). Right: FID score of final samples, and Mean Squared Distance between initial and final samples.

Figure 15: Left: MNIST to EMNIST sample trajectory with 30 diffusion steps at \(t=0,1/3,2/3,1\). Right: FID score of final samples, and Mean Squared Distance between initial and final samples.

we observe the FID score worsens for both Bridge Matching and DSBM as more stochasticity is introduced in the sampler. The best result of DSBM-IMF is obtained using \(\sigma^{2}=0.2\) and is slightly better than FM (i.e. with \(\sigma^{2}=0\)) using 100 Euler steps. On the other hand, using the dopri5 ODE solver, FM achieves a FID of 4.055 with on average 148 integration steps. In Figure 17, we observe that both RF and DSBM-IMF are very effective in improving sampling quality at low number of diffusion steps, i.e. low number of function evaluations (NFEs), compared to Bridge and Flow Matching as well as OT-CFM which improves upon CFM slightly. DSBM-IMF also achieves lower FID score than RF as the NFEs are taken higher. Additional strategies such as distillation and fast SDE solvers can also be useful for improving few-step sampling quality further.

### Fluid Flows Experiment

We use the fluid flows dataset9 from Bischoff and Deck (2023). The dataset consists of unpaired low (\(64\times 64\)) and high (\(512\times 512\)) resolution fields, as well as a context field with local information for the high resolution field. The data fields consist of two channels representing supersaturation and vorticity. The context field is dependent on the wavenumber \(k_{x}=k_{y}\in\{1,2,4,8,16\}\), which

Figure 16: Transfer results between images given by the tokens female/young and male/old. Top row: original images (left) and generated images (right). Bottom row: original images (left) and generated images (right).

specifies the frequency of the saturation specific humidity modulation. We follow Bischoff and Deck (2023) for data processing and network architecture, which is given by a U-Net with an additional spatial mean bypass network given by an MLP. The network size is approximately 11.3 million parameters in total. We train using batch size 4, learning rate \(2\times 10^{-4}\), for 5000 steps per outer iteration and \(N=12\) outer iterations. We refresh the cache dataloader every 2500 steps with 1250 new samples. The training takes approximately 20 hours on a single RTX GPU. We used \(\sigma^{2}=0.3\) and sample using 30 diffusion steps without finetuning these parameters. For the Diffusion-fb method in Bischoff and Deck (2023), we use the released code10 without modifying any parameters.

Footnote 10: https://github.com/CliMA/diffusion-bridge-downscaling (code released under Apache-2.0 license)

We visualize intermediate and final reconstruction samples for different algorithms in Figure 19. We see that DSBM-IPF and DSBM-IMF provide consistent samples with the low resolution source, whereas Diffusion-fb and Bridge Matching produce dissimilar samples. We also follow Bischoff and Deck (2023) for a more refined statistical analysis in Figures 21, 21, 22. DSBM-IMF achieves comparable performance as Diffusion-fb in terms of these statistical profiles, and can be comparatively more accurate e.g. in the tails of the distributions in Figure 21, and for the case \(k_{x}=k_{y}=4\) in Figure 21 for which the power spectrum of supersaturation is correctly captured by DSBM-IMF but not by other methods. Comparing this analysis with Figure 11, DSBM-IMF is also significantly more accurate in terms of conditional consistency than Diffusion-fb. On the other hand, DSBM-IPF appears less accurate in terms of these unconditional statistics than Diffusion-fb and DSBM-IMF, but achieves lower \(\ell_{2}\) distances from the input sources in Figure 11. This suggests that DSBM-IPF and DSBM-IMF exhibit different empirical biases before convergence, and DSBM-IMF is more preferable when the accuracy of the samples are important. This is in line with IMF theory as the marginals \(\pi_{0},\pi_{T}\) are preserved in IMF but not in IPF.

Figure 21: Spectral density estimates of supersaturation and vorticity fields.

Figure 22: KDE estimates of spatial means of the supersaturation field. The shaded areas denote 99% confidence interval obtained using 10000 bootstrap samples.

Figure 20: KDE estimates of values in supersaturation and vorticity fields.

Figure 21: Spectral density estimates of supersaturation and vorticity fields.

in machine learning, but also natural science areas such as physics, biology and geosciences in which optimal transport maps with theoretical guarantees are appealing. Our fluid flows experiment demonstrates such potentials. However, as is the case for generative models as a whole, intentional malicious use could cause detrimental societal impacts.