# Learning the Optimal Policy for Balancing

Short-Term and Long-Term Rewards

 Qinwei Yang\({}^{1}\), Xueqing Liu\({}^{1}\), Yan Zeng\({}^{1}\), Ruocheng Guo\({}^{2}\), Yang Liu\({}^{3}\), Peng Wu\({}^{1}\)

\({}^{1}\)Beijing Technology and Business University \({}^{2}\)ByteDance Research \({}^{3}\)UC Santa Cruz

Corresponding author: pengwu@btbu.edu.cn.

###### Abstract

Learning the optimal policy to balance multiple short-term and long-term rewards has extensive applications across various domains. Yet, there is a noticeable scarcity of research addressing policy learning strategies in this context. In this paper, we aim to learn the optimal policy capable of effectively balancing multiple short-term and long-term rewards, especially in scenarios where the long-term outcomes are often missing due to data collection challenges over extended periods. Towards this goal, the conventional linear weighting method, which aggregates multiple rewards into a single surrogate reward through weighted summation, can only achieve suboptimal policies when multiple rewards are related. Motivated by this, we propose a novel decomposition-based policy learning (DPPL) method that converts the whole problem into subproblems. The DPPL method is capable of obtaining optimal policies even when multiple rewards are interrelated. Nevertheless, the DPPL method requires a set of preference vectors specified in advance, posing challenges in practical applications where selecting suitable preferences is non-trivial. To mitigate this, we further theoretically transform the optimization problem in DPPL into an \(\varepsilon\)-constraint problem, where \(\varepsilon\) represents the minimum acceptable levels of other rewards while maximizing one reward. This transformation provides intuitive into the selection of preference vectors. Extensive experiments are conducted on the proposed method and the results validate the effectiveness of the method.

## 1 Introduction

Learning an optimal policy for balancing multiple short-term and long-term rewards holds extensive applications across various domains. For instance, content providers can optimize recommendations to avoid short-term clickbait strategies, ensuring sustained user engagement and revenue growth [1]. IT companies can design web pages catering to immediate user preferences while enhancing long-term engagement and satisfaction [2]. Economists explore the effects of early childhood interventions on lifetime earnings, seeking optimal policies (e.g., class size) maximizing short-term test scores and long-term earnings simultaneously [3]. Policymakers can improve job training program design, considering both immediate income impacts and subsequent employment status improvements [4, 5]. Medical practitioners can refine drug prescriptions, considering short-term alleviation and long-term outcomes in chronic diseases like Alzheimer's and AIDS [6]. Marketing professionals can optimize incentive strategies to positively influence customer behavior in both short and long terms [7].

Despite the importance of balancing multiple short-term and long-term rewards, policy learning methods in this area remain largely unexplored. Recent literature [8] employs a linear weighting method to achieve this goal. It combines multiple rewards into a single surrogate reward by weighted summation, which is optimized to learn the optimal policy. However, this strategy has several limitations. First, it can only find optimal solutions in convex regions of objective space and cannot obtain the optimal solutions in non-convex regions [9]. Second, it achieves the optimal solution onlywhen the rewards are independent of each other. When some of the rewards are interrelated, it can only achieve sub-optimal solutions [10]. Consequently, although the linear weighting method is easy to implement, the optimality of its solution cannot be guaranteed when balancing multiple objectives.

In this article, we propose a principled policy learning approach for balancing multiple long-term and short-term rewards (objectives). Specifically, we first formulate it as a multiple-objective problem (MOP) and aim to seek the Pareto optimal solutions (policies). A solution is Pareto optimal if improving one objective necessitates worsening other objectives. Then, we propose a novel decomposition-based policy learning (DPPL) method, which involves (1) introducing a set of preference vectors, (2) dividing the whole optimization problem into several subproblems based on the preference vectors, and (3) ultimately achieving different Pareto solutions for the objectives by solving these subproblems. Compared with the linear weighting method, it can obtain Pareto optimal solutions in non-convex regions and is applicable to cases where multiple objectives are interrelated.

While the proposed DPPL method can find Pareto optimal policies, it necessitates specifying a set of preference vectors in advance. In practical applications, decision-makers may encounter the challenge of determining which preference vector to choose. To mitigate this concern, we further theoretically transform the optimization problem in DPPL into an \(\varepsilon\)-constraint problem. This transformation can assist decision-makers in better understanding and selecting preference vectors.

The contributions of this paper are summarized as follows.

\(\bullet\) We formulate the policy learning problem of balancing multiple long-term and short-term rewards as a multi-objective optimization problem and propose a decomposition-based Pareto policy learning (DPPL) method to obtain a set of Pareto optimal policies.

\(\bullet\) We theoretically establish the connection between the DPPL method and the \(\epsilon\)-constraint problem, offering an intuitive interpretation of preference vectors and guiding their selection.

\(\bullet\) We conduct extensive experiments to demonstrate the effectiveness of the proposed method.

## 2 Problem Formulation

Throughout, we employ bold letters for vectors, uppercase letters for random variables, and lowercase letters for their realization values.

### Notation

We introduce notations to delineate short-term and long-term causal effects. Let \(A\) denote the binary treatment indicator, where \(A=1\) represents the treated group and \(A=0\) represents the control group. \(\bm{X}\) represents the features observed, \(\bm{S}=(S_{1},...,S_{I})\in\mathbb{R}^{I}\) and \(\bm{Y}=(Y_{1},...,Y_{J})\in\mathbb{R}^{J}\) represent the vector of short-term and long-term outcomes, respectively. Both short-term and long-term outcomes are observed after the treatment \(A\), and associations among them may exist.

Utilizing the potential outcome framework [11], we denote \(\bm{S}(a)=(S_{1}(a),...,S_{I}(a))\) and \(\bm{Y}(a)=(Y_{1}(a),...,Y_{J}(a))\) for \(a=0,1\) as the potential short-term and long-term outcomes under treatment \(A=a\), respectively. We assume that larger short-term and long-term outcomes are preferable. The observed short-term and long-term outcomes \(\bm{S}\) and \(\bm{Y}\) correspond to the potential outcomes of the actual treatment, that is, \(\bm{S}=\bm{S}(A)\) and \(\bm{Y}=\bm{Y}(A)\).

In real-world applications, long-term outcomes often suffer from missing due to prolonged follow-up periods and budget constraints. In contrast, collecting short-term outcomes is more manageable. Therefore, we presume that all short-term outcomes \(\bm{S}\) are observable, while long-term outcomes \(\bm{Y}\) may be subject to missing. Let \(\bm{R}=(R_{1},...,R_{J})\in\{0,1\}^{J}\) denote the indicator for observing the long-term outcome \(\bm{Y}\), where \(R_{j}=1\) indicates that \(Y_{j}\) is observed and \(R_{j}=0\) indicates that \(Y_{j}\) is missing. The missingness of \(\bm{Y}\) would lead to identifiability and estimation problems [12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23].

### Formulation

In this article, we aim to learn the Pareto optimal policy for balancing multiple correlated short-term and long-term rewards, which has a wide range of application scenarios [1; 6; 8; 24]. Let \(\pi:\mathcal{X}\rightarrow\{0,1\}\) be a policy that maps from the individual context \(\bm{X}=\bm{x}\) to the treatment space \(\{0,1\}\). For a given policy \(\pi(\bm{\theta})=\pi(\bm{X},\bm{\theta})\) parameterized by \(\bm{\theta}\), the policy values for the \(i\)-th short-term outcome \(S_{i}\) and the \(j\)-th long-term outcome \(Y_{j}\) are defined as,

\[\begin{split}\mathcal{V}(\bm{\theta};s_{i})&=\mathbb{ E}[\pi(\bm{\theta})S_{i}(1)+(1-\pi(\bm{\theta}))S_{i}(0)],\quad i=1,...,I\\ \mathcal{V}(\bm{\theta};y_{j})&=\mathbb{E}[\pi( \bm{\theta})Y_{j}(1)+(1-\pi(\bm{\theta}))Y_{j}(0)],\quad j=1,...,J,\end{split}\]

which are the \(i\)-th short-term reward and the \(j\)-the long-term reward induced by the policy \(\pi(\bm{\theta})\).

Conventionally, we convert maximization problems to minimization problems. Let \(\bar{\mathcal{V}}(\bm{\theta};s_{i})=-\mathcal{V}(\bm{\theta};s_{i}),\bar{ \mathcal{V}}(\bm{\theta};y_{j})=-\mathcal{V}(\bm{\theta};y_{j})\). The trade-off among multiple correlated long-term and short-term rewards can be formulated as a multi-objective optimization (MOP) problem given by

\[\begin{split}\min_{\bm{\theta}}\bar{\mathcal{V}}(\bm{\theta})& =(\bar{\mathcal{V}}(\bm{\theta};s_{1}),\cdots,\bar{\mathcal{V}}( \bm{\theta};s_{I}),\bar{\mathcal{V}}(\bm{\theta};y_{1}),\cdots,\bar{ \mathcal{V}}(\bm{\theta};y_{J}))\\ &\triangleq(\bar{\mathcal{V}}_{1}(\bm{\theta}),\bar{\mathcal{V}} _{2}(\bm{\theta}),\cdots,\bar{\mathcal{V}}_{M}(\bm{\theta}))\end{split}\] (1)

where \(M=I+J\) and the symbol \(\triangleq\) means 'denoted as'. Generally, there is no single solution that can simultaneously optimize all objectives in problem (1) and thus we resort to the Pareto optimality. This concept is employed to define the optimal solutions for the MOP problem.

**Definition 1**.: _(Pareto optimality)_

_(a) Pareto dominance. For two points \(\bm{\theta^{1}},\bm{\theta^{2}}\). \(\bm{\theta^{1}}\) dominates \(\bm{\theta^{2}}\) if and only if \(\bar{\mathcal{V}}_{m}(\bm{\theta^{1}})\leq\bar{\mathcal{V}}_{m}(\bm{\theta^{2} }),\forall\ m\in\{1,...,M\}\) and \(\bar{\mathcal{V}}_{m^{\prime}}(\bm{\theta^{1}})<\bar{\mathcal{V}}_{m^{\prime} }(\bm{\theta^{2}}),\ \exists\ m^{\prime}\in\{1,...,M\}\)_

_(b) Pareto optimality. \(\bm{\theta^{s}}\) is a Pareto optimal point if there is no other solution \(\hat{\bm{\theta}}\) that dominates \(\bm{\theta^{s}}\)._

Pareto optimality refers to a condition where improving one objective comes at the expense of worsening other objectives. The collection of Pareto optimal solutions is called the Pareto set. Our goal is to derive the set of Pareto optimal solutions (or Pareto optimal policies), each of them providing a distinct optimal trade-off among all objectives.

### Identification and Estimation of Short-term and Long-term Rewards

The long-term and short-term rewards are causal parameters that cannot be identified without imposing causal assumptions [25; 26; 27]. Therefore, before seeking the Pareto optimal solutions for balancing multiple long-term and short-term rewards, it is necessary to consider the identification and estimation of long-term and short-term rewards. The proposed method is based on Assumptions 1 and 2 below.

**Assumption 1** (Strong Ignorability).:

_(a) \((\bm{S}(a),\bm{Y}(a))\perp\!\!\!\perp A\mid\bm{X}\) for \(a=0,1\);_

_(b) \(0<e(\bm{x})\triangleq\mathbb{P}(A=1\mid\bm{X}=\bm{x})<1\) for all \(\bm{x}\)._

Assumption 1(a) suggests that, given the feature \(\bm{X}\), treatment assignment \(A\) is independent of the potential outcomes \(\bm{S}(a)\) and \(\bm{Y}(a)\). This implies that confounding bias between the treatment \(A\) and the short/long-term outcomes \((\bm{S}(a),\bm{Y}(a))\) can be eliminated by conditioning on \(\bm{X}\)[28]. Assumption 1(b) ensures that for the subpopulation of \(\bm{X}=\bm{x}\), units with both \(A=1\) and \(A=0\) exist. These assumptions are widely used in causal inference [29; 30; 31; 32; 33; 34; 35; 11; 27].

In addition to confounding bias, we also need to address the selection bias induced by the missingness of long-term outcomes [8]. Thus, we further invoke the Assumption 2.

**Assumption 2** (Missing Mechanism of Long-term Outcome).: _For \(a=0,1\) and \(j=1,...,J\)._

_(a) \(R_{j}\perp\!\!\!\perp Y_{j}(a)\mid\bm{X},\bm{S}(a),A=a\);_

_(b) \(0<r_{j}(\bm{x},a,\bm{s})\triangleq\mathbb{P}(R_{j}=1\mid\bm{X}=\bm{x},A=a,\bm{ S}=\bm{s})\)._

Assumption 2(a) can be reformulated as \(R_{j}\perp\!\!\!\perp Y_{j}\mid(\bm{X},\bm{S},A)\), which means that \(R_{j}\) relies only on the observed variables \((\bm{X},A,\bm{S})\). This assumption also ensures that \(\mathbb{P}(Y_{j}=y|\bm{X},\bm{S},A,R_{j}=1)=\mathbb{P}(Y_{j}=y|\bm{X},\bm{S},A,R _{j}=0)\). This implies that we can utilize the available data to draw conclusions about the missing long-term outcome. Assumption 2(b) assumes that the long-term outcome for each unit has a non-zero probability of being observed. Assumptions 1 and 2 ensures the identifiability of \(\mathcal{V}(\bm{\theta};s_{i})\) and \(\mathcal{V}(\bm{\theta};y_{j})\), as shown in Lemma 1.

**Lemma 1** (Identifiability of Short-term and Long-term Rewards).: _For \(i=1,...,I\) and \(j=1,...,J\), (a) under Assumptions 1, the \(i\)-th short-term reward \(\mathcal{V}(\bm{\theta};s_{i})\) is identifiable. (b) under Assumptions 1-2, the \(j\)-th long-term reward \(\mathcal{V}(\bm{\theta};y_{j})\) is identifiable._

When we have access to only one short-term outcome and one long-term outcome, Lemma 1 reduces to the identifiability result presented in [8]. In this article, our focus is on achieving the Pareto optimal policy for multiple short-term and long-term rewards. Therefore, for the estimation of \(\mathcal{V}(\bm{\theta};s_{i})\) and \(\mathcal{V}(\bm{\theta};y_{j})\), we defer it to Appendix A.

## 3 Pareto Policy Learning for Balancing Short-Term and Long-Term Rewards

In this section, we aim to learn Pareto optimal policies for the MOP problem (1). Section 3.1 gives the motivation for this work and Section 3.2 introduces the proposed policy learning approach. In Section 3.3, we theoretically establishe the connection between the linear weighting method, the MOP problem for a given preference vector, and the \(\varepsilon\)-constraint problem. This connection offers an intuitive interpretation and guides practitioners in selecting the preference vector.

### Motivation

For seeking the optimal policy for balancing short-term and long-term rewards, previous work [8] adopted the linear weighting method. Specifically, the authors formulate the goal as

\[\min_{\bm{\theta}}\bar{\mathcal{V}}(\bm{\theta})=\sum_{m=1}^{M}\omega_{m} \bar{\mathcal{V}}_{m}(\bm{\theta}),\] (2)

where \(\omega_{m}\) is the pre-specified weight for the \(m\)-th objective. The objective function in the optimization problem (2) is merely a linear combination of multiple objectives from the MOP problem (1). Due to its intuitiveness and simplicity, the traditional linear weighting method is commonly used for solving MOP or multi-task learning problems [36; 37; 38].

The linear weighting method simply combines multiple objectives into a single surrogate objective through weighted summation. While simple, it has several limitations. First, the optimal solution is found only in convex regions and not in non-convex regions [9]. Second, an optimal solution can only be achieved if the objectives are independent of each other. That is, if some objectives are interrelated, only a suboptimal solution can be obtained [10]. Thus, it does not guarantee the superiority of the solution or its solution may deviate from the Pareto optimal solution.

To overcome the limitations of the linear weighting method in [8], we first introduce a decomposition-based multi-objective optimization algorithm to achieve the Pareto optimal policy. However, this algorithm relies on pre-specified preference vectors, which are used to express a decision maker's degree of preference for multiple conflicting objectives. In practice, the explanation and selection of preference vectors is a challenging problem. To further tackle this issue, we establish a theoretical relationship between preference vectors and the \(\varepsilon\)-constraint method [39]. This relationship provides a clear interpretation on preference vectors, assisting in selecting more suitable ones.

### Pareto Policy Learning for the MOP Problem

We introduce the decomposition-based Pareto policy learning (DPPL) method, which can generate the Pareto set containing policies that are optimum from a trade-off perspective. The main idea of the DPPL method is to first decompose the original MOP problem into several constrained subproblems based on a predefined set of preference vectors, and then obtain a set of Pareto optimal policies by solving these subproblems in parallel [40].

For obtaining the Pareto optimal policy for balancing \(M\) short-term and long-term objectives, first, we are given a set of \(K\) preference vectors \(\{\bm{u}_{1},\bm{u}_{2},...,\bm{u}_{K}\}\) in \(\mathbb{R}^{M}_{+}\). Each element of a preference vector specifies the importance of the corresponding short-term or long-term reward. For each preference vector \(\bm{u}_{k}\), the corresponding subproblem is given as

\[\begin{split}&\min_{\bm{\theta}}\bar{\bm{\mathcal{V}}}(\bm{\theta})=( \bar{\mathcal{V}}_{1}(\bm{\theta}),\bar{\mathcal{V}}_{2}(\bm{\theta}),\cdots, \bar{\mathcal{V}}_{M}(\bm{\theta}))\\ & s.t.~{}\mathcal{G}_{k^{\prime}}(\bm{\theta})=(\bm{u}_{k^{\prime }}-\bm{u}_{k})^{T}\bar{\bm{\mathcal{V}}}(\bm{\theta})\leq 0,~{}\forall~{}k^{\prime}=1,...,K, \end{split}\] (3)where \(\mathcal{G}_{k^{\prime}}(\bm{\theta}_{t})\leq 0\) means that objective space2 of the subproblem is restricted in the subregion \(\Omega_{k}\), which is defined by \(\Omega_{k}=\{\bm{v}\in R_{+}^{M}|\bm{u}_{k^{\prime}}^{T}\bm{v}\leq\bm{u}_{k}^{T} \bm{v},\forall\;k^{\prime}=1,...,K\}\). Geometrically speaking, \(\Omega_{k}\) represents the set of \(\bm{v}\) that forms the smallest acute angle with \(\bm{u}_{k}\), which means that the optimal solution of the subproblem can be obtained by only searching the subregion. The preference vectors divide the objective space into different subregions.

Footnote 2: \(\widehat{\bm{\mathcal{V}}}(\bm{\theta})\) is the objective vector, and the space spanned by the objective vectors is called the objective space \(\Omega\).

Solving the subproblem (3) involves the following two steps:

* **Step (a)**. Find a reasonable initial solution \(\bm{\theta}_{0}\). Specifically, we first randomly generate a solution \(\bm{\theta}_{r}\) in the full decision space3, and then iteratively update it with the rule \(\bm{\theta}_{r_{t+1}}=\bm{\theta}_{r_{t}}+\eta_{r}\bm{d}_{r_{t}}\), where \(\eta_{r}\) is the step size. For a given \(\bm{\theta}_{r_{t}}\), the descent direction \(\bm{d}_{r_{t}}\) is updated by solving (4). Footnote 3: The parameter vector \(\bm{\theta}\) represents the decision variable and the space spanned it is called the decision space. \[(\bm{d}_{r_{t}},\alpha_{r_{t}})=\arg\min_{\bm{d}\in\mathbb{R}^{n},\alpha\in \mathbb{R}}\alpha+\frac{1}{2}||\bm{d}||^{2},\;s.t.\;\nabla\mathcal{G}_{k^{ \prime}}(\bm{\theta}_{r_{t}})^{T}\bm{d}\leq\alpha,\;k^{\prime}\in\mathcal{I}( \bm{\theta}_{r_{t}}).\] (4) where \(\mathcal{I}(\bm{\theta}_{r_{t}})=\{k^{\prime}|\mathcal{G}_{k^{\prime}}(\bm{ \theta}_{r_{t}})\geq 0,k^{\prime}=1,...,K\}\) is index set of all activated constraints, which means \(\widehat{\bm{\mathcal{V}}}(\bm{\theta}_{r_{t}})\) not in \(\Omega_{k}\). The problem (4) aims to find the descent direction \(\bm{d}_{r_{t}}\) for each iteration \(t\) and then obtain the initial solution \(\bm{\theta}_{0}\) such that \(\widehat{\bm{\mathcal{V}}}(\bm{\theta}_{0})\) in \(\Omega_{k}\). Footnote 3: The parameter vector \(\bm{\theta}\) represents the decision variable and the space spanned it is called the decision space.
* **Step (b)**. Solving the subproblem (3). The descent direction \(\bm{d}_{t}\) for the \(t\)-th iteration is obtained by \[(\bm{d}_{t},\alpha_{t})= \arg\min_{\bm{d}\in\mathbb{R}^{n},\alpha\in\mathbb{R}}\alpha+ \frac{1}{2}||\bm{d}||^{2}\] (5) \[s.t. \nabla\widetilde{\mathcal{V}}_{m}(\bm{\theta}_{t})^{T}\bm{d}\leq \alpha,m=1,...,M.\] \[\nabla\mathcal{G}_{k^{\prime}}(\bm{\theta}_{t})^{T}\bm{d}\leq \alpha,k^{\prime}\in\mathcal{I}_{\epsilon}(\bm{\theta}_{t}),\] where \(\mathcal{I}_{\epsilon}(\bm{\theta})=\{k^{\prime}|\mathcal{G}_{k^{\prime}}( \bm{\theta})\geq-\epsilon\}\), and the threshold \(\epsilon\) is a slack variable used to deal with the solutions near the constraint boundary. We further transform it into a dual problem which will greatly reduce the dimension of decision space. Based on the KKT conditions, we have \(\bm{d}_{t}=-(\sum_{m=1}^{M}\lambda_{m}\nabla\widetilde{\mathcal{V}}_{m}(\bm{ \theta}_{t})+\sum_{k^{\prime}\in\mathcal{I}_{\epsilon}(\bm{\theta})}\beta_{k^{ \prime}}\nabla\mathcal{G}_{k^{\prime}}(\bm{\theta}_{t}))\). Therefore, the dual problem is given as \[\max_{\lambda_{m},\beta_{k^{\prime}}}-\frac{1}{2}||\sum_{m=1}^{M} \lambda_{m}\nabla\widetilde{\mathcal{V}}_{m}(\bm{\theta}_{t})+\sum_{k^{\prime} \in I_{\epsilon}(\bm{\theta})}\beta_{k^{\prime}}\nabla\mathcal{G}_{k^{\prime} }(\bm{\theta}_{t})||^{2}\] (6) \[s.t. \sum_{m=1}^{M}\lambda_{m}+\sum_{k^{\prime}\in I_{\epsilon}(\bm{ \theta})}\beta_{k^{\prime}}=1,\lambda_{m}\geq 0,\beta_{k^{\prime}}\geq 0, \forall m=1,...,M,\forall\;k^{\prime}\in\mathcal{I}_{\epsilon}(\bm{\theta}).\] where \(\lambda_{m}\geq 0\) and \(\beta_{k^{\prime}}\geq 0\) are the Lagrange multipliers for the linear inequality constraints.

Step (a) is to find an initial solution \(\bm{\theta}_{0}\) that is restricted in a subregion of the subproblem (3), and once a feasible solution is found or a predetermined number of iterations is reached, the step stops. For an given initial solution \(\bm{\theta}_{0}\), Step (b) is to find the optimal solution \(\bm{\theta}^{*}\) for the subproblem (3). We summarize the proposed policy learning approach in Appendix B.

**Lemma 2** ([41]).: _Let \((\bm{d}_{t},\alpha_{t})\) be the solution to the \(t\)-th iteration of problem (5). (a) If \(\bm{\theta}_{t}\) is Pareto optimal restricted on \(\Omega_{k}\), then \(\bm{d}_{t}=0\in\mathbb{R}^{m}\) and \(\alpha_{t}=0\). (b) If \(\bm{\theta}_{t}\) is not Pareto optimal restricted on \(\Omega_{k}\). then_

\[\alpha_{t}\leq-(1/2)||d_{t}||^{2}<0,\] (7) \[\nabla\widetilde{\mathcal{V}}_{m}(\bm{\theta}_{t})^{T}\bm{d}_{t} \leq\alpha_{t},m=1,...,M\] \[\nabla\mathcal{G}_{k^{\prime}}(\bm{\theta}_{t})^{T}\bm{d}_{t} \leq\alpha_{t},k^{\prime}\in\mathcal{I}_{\epsilon}(\bm{\theta}_{t}).\]

Lemma 2(a) implies that at the \(t\)-th iteration, no direction (\(\bm{d}_{t}=0\)) can simultaneously improve the performance for all objectives, confirming that the solution \(\bm{\theta}_{t}\) satisfies Pareto optimality. Lemma 2(b) suggests that if \(\bm{\theta}_{t}\) does not meet Pareto optimality, then the descent direction \(\bm{d}_{t}\neq 0\) serves as the descent direction for all objectives, such that the solution of the next iteration is closer to the Pareto optimal solution. Thus, Lemma 2 demonstrates that we always attain Pareto optimal solutions for each subproblem using the update rule \(\bm{\theta}_{t+1}=\bm{\theta}_{t}+\eta_{r}\bm{d}_{t}\). By solving all subproblems, we can acquire a diverse set of Pareto optimal solutions (or policies) confined to different subregions, even when the multiple objectives are correlated.

### Deep Analysis of the Preference Vector

The DPPL method in Section 3.2 requires a set of pre-specified preference vectors, posing challenges in practical applications where selecting suitable preference vectors is non-trivial. To mitigate this problem, we provide a practical method for decision-makers to select appropriate preference vectors by theoretically establishing the connection between the DPPL method and the \(\varepsilon\)-constraint problem.

We first give a brief introduction to the \(\varepsilon\)-constraint problem [10], which is defined as follows,

\[\min_{\bm{\theta}}\ \bar{\mathcal{V}}_{l}(\bm{\theta}),\ \ \mathrm{s.\ t.}\ \bar{\mathcal{V}}_{m}(\bm{ \theta})\leq\varepsilon_{m}\ \mathrm{for\ all}\ m=1,\ldots,M,m\neq l,\] (8)

where \(\varepsilon_{m}\) is pre-specified threshold. Compared to the MOP problem (1) and the linear weighting objective (2), a notable advantage of the \(\varepsilon\)-constraint problem is its interpretation on the threshold \(\varepsilon_{m}\), which represents the maximum acceptable value (i.e., the acceptable worst-case scenario) for the \(m\)-th objective. In contrast, the weights and the preference vectors in problems (1) and (2) are not straightforward for relating the resulting values of objectives. Thus, if we can establish the connection between \(\bm{\varepsilon}=(\varepsilon_{1},...,\varepsilon_{M})\) and the preference vector \(\bm{u}_{k}\), then we can provide powerful guidance for choosing appropriate preference vectors.

**Theorem 1**.: _For the preference vector \(\bm{u}_{k}=(u_{k1},...,u_{kM})\) in problem (1), the weights \(\bm{\omega}=(\omega_{1},....,\omega_{M})\) in problem (2), and the thresholds \(\bm{\varepsilon}\) in problem (8), the following statements hold:_

_(a) the connection between \(\bm{\varepsilon}\) and \(\bm{\omega}\) is given as_

\[\varepsilon_{m}=-\mathbb{E}[\mathbb{I}(\tau_{l}(\bm{X})+\frac{\omega_{m}}{ \omega_{l}}\tau_{m}(\bm{X})>0)\cdot\tau_{m}(\bm{X})+h_{m}(\bm{X})],\ \text{for}\ m=1\cdots M,and\ m\neq l,\] (9)

_where \(\tau_{m}(\bm{X})\) is the conditional average causal effects for \(m\)-th short/long-term outcome,_

\[\tau_{m}(\bm{X})=\begin{cases}\mathbb{E}[S_{i}(1)-S_{i}(0)|\bm{X}],\ \text{if}\ \omega_{m}\ \text{is the weight of}\ \bar{\mathcal{V}}(\bm{\theta},s_{i}),\\ \mathbb{E}[Y_{j}(1)-Y_{j}(0)|\bm{X}],\ \text{if}\ \omega_{m}\ \text{is the weight of}\ \bar{\mathcal{V}}(\bm{\theta},y_{j}),\end{cases}\]

\(\mathbb{I}(\cdot)\) _is the indicator function, and_

\[h_{m}(\bm{X})=\begin{cases}\mathbb{E}[S_{i}(0)|\bm{X}],\ \text{if}\ \omega_{m}\ \text{is the weight of}\ \bar{\mathcal{V}}(\bm{\theta},s_{i}),\\ \mathbb{E}[Y_{j}(0)|\bm{X},\bm{S},R_{j}=1],\ \text{if}\ \omega_{m}\ \text{is the weight of}\ \bar{\mathcal{V}}(\bm{\theta},y_{j}).\end{cases}\]

_(b) the connection between \(\bm{\omega}\) and \(\bm{u}_{k}\) is given as_

\[\omega_{m}=\lambda_{m}+\sum_{k^{\prime}\in\mathcal{I}_{\epsilon}(\bm{\theta}) }\beta_{k^{\prime}}(\bm{u}_{k^{\prime}m}-\bm{u}_{km}),\ \text{for}\ m=1,\cdots,M,\] (10)

_where \(\lambda_{m}\) and \(\beta_{k^{\prime}}\) are defined in Eq. (6), and \(\mathcal{I}_{\epsilon}(\bm{\theta})=\{k^{\prime}|\mathcal{G}_{k^{\prime}}(\bm {\theta})\geq-\epsilon\}\) defined in Eq. (4)._

Theorem 1 (see Appendix C for proofs) establishes a link between the preference vector \(\bm{u}_{k}\) and \(\bm{\varepsilon}\) through \(\bm{\omega}\) in scenarios involving multiple long-term and short-term objectives. Specifically, Theorem 1(a) shows how to estimate the threshold \(\bm{\varepsilon}\) for given weights \(\bm{\omega}\), and Theorem 1(b) shows how to assign weights \(\bm{\omega}\) via preference vectors \(\bm{u}_{k}\). This means that for the subproblem determined by preference vectors \(\bm{u}_{k}\), we can ascertain the maximum acceptable threshold \(\bm{\varepsilon}\) based on Theorem 1, thereby offering an intuitive interpretation of the preference vector \(\bm{u}_{k}\).

There are several practical implications with Theorem 1. On one hand, it assists decision-makers in better understanding and selecting preference vectors in practical applications. In practice, we can initially pre-specify a set of preference vectors \(\{\bm{u}_{1},\bm{u}_{2},...,\bm{u}_{K}\}\) in \(\mathbb{R}_{+}^{M}\), then derive the weights \(\bm{\omega}\) corresponding to each preference vector \(\bm{u}_{k}\) through Eq. (10), and finally substitute the obtained weight \(\bm{\omega}\) into Eq. (9) to calculate the threshold \(\bm{\varepsilon}\). Leveraging the intuitive interpretability of the threshold \(\bm{\varepsilon}\), decision-makers can select the appropriate preference vectors according to their specific requirements. On the other hand, it also provides guidance for specifying \(\bm{\varepsilon}\) in the \(\varepsilon\)-constraint problem (8). Inappropriate selection of \(\bm{\varepsilon}\) for this problem may result in an empty feasible region, yielding empty solutions. By utilizing a set of preference vectors, we can efficiently screen out some reasonable choices of \(\bm{\varepsilon}\) and reduce the cumbersome trial-and-error process of testing different \(\bm{\varepsilon}\).

In conclusion, by establishing the connection between the DPPL method and the \(\varepsilon\)-constraint problem, we can harness the advantages of both methods while mitigating their respective weaknesses.

Experiments

**Datasets.** Following the previous studies [8], we use two widely used datasets: IHDP and JOBS, for evaluating the performance of the proposed method. The IHDP dataset explores the effectiveness of high-quality home visiting in promoting children's future cognitive development and covers a sample of 747 units, including 139 treated and 608 controlled. In addition, the dataset has 25 characteristics that provide a comprehensive picture of the children and their mothers. The second dataset, JOBS, explores the effects of job training on income and employment status. It consists of 2,570 units (237 treated, 2,333 controlled), with 17 covariates. Note that each unit in both datasets has only one observed outcome from a single treatment, and neither dataset collects long-term outcomes.

**Simulating Outcome.** Consider the case of one long-term reward and one short-term reward. Following the previous data-generation mechanisms [1, 42], for the \(n\)-th unit (\(n=1,...,N\)), we simulate the potential short-term outcomes \(S(0)\) and \(S(1)\) as follows:

\[S_{n}(0)\sim\mathrm{Bern}(\sigma(w_{0}X_{n}+\epsilon_{0,n})),\quad S_{n}(1) \sim\mathrm{Bern}(\sigma(w_{1}X_{n}+\epsilon_{1,n})),\]

where \(\sigma(\cdot)\) is the sigmoid function, \(w_{0}\sim\mathcal{N}_{[-1,1]}(0,1)\) follows a truncated normal distribution, \(w_{1}\sim\mathrm{Unif}(-1,1)\) follows a uniform distribution, \(\epsilon_{0,n}\sim\mathcal{N}(\mu_{0},\sigma_{0})\) and \(\epsilon_{1,n}\sim\mathcal{N}(\mu_{1},\sigma_{1})\). We set \(\mu_{0}=1,\mu_{1}=3\) and \(\sigma_{0}=\sigma_{1}=1\) for the IHDP dataset, and we set \(\mu_{0}=0,\mu_{1}=2\) and \(\sigma_{0}=\sigma_{1}=1\) for the JOBS dataset. For generating long-term potential outcomes \(Y(0)\) and \(Y(1)\), we introduce the time step \(t\): we set the initial value at time step 0 as: \(Y_{0,n}(0)=S_{n}(0)\), \(Y_{0,n}(1)=S_{n}(1)\), then generate \(Y_{t,n}(0),Y_{t,n}(1)\) according to the following equation and we eventually regard the outcome at the last time step \(T\) as the long-term outcome, \(Y_{n}(0)=Y_{T,n}(0),Y_{n}(1)=Y_{T,n}(1)\).

\[Y_{t,n}(0)\sim\mathrm{Bern}(\sigma(\beta_{0}X_{n})+C\sum_{t^{\prime}=0}^{t-1}Y _{t^{\prime},n}(0))+\epsilon_{0,n},\;Y_{t,n}(1)\sim\mathrm{Bern}(\sigma(\beta _{1}X_{n})+C\sum_{t^{\prime}=0}^{t-1}Y_{t^{\prime},n}(0))+\epsilon_{1,n},\]

where \(\beta_{0}\) is randomly sampled from \(\{0,1,2,3,4\}\) with probabilities \(\{0.5,0.2,0.15,0.1,0.05\},\beta_{1}\sim 4\cdot\mathcal{N}_{[0,4]}(0,1)\), and \(C=1/T\) is a scaling factor. For \(\epsilon_{0,n}\) and \(\epsilon_{1,n}\), we set \(\mu_{0}=\mu_{1}=0,\sigma_{0}=1\) and \(\sigma_{1}=3\) for the IHDP dataset and set \(\mu_{0}=\mu_{1}=0,\sigma_{0}=1\) and \(\sigma_{1}=1\) for the JOBS dataset.

Assumption 2 shows that observing indicator \(R\) depends on the feature \(\bm{X}\), the treatment \(A\), and short-term outcome \(S\). For a given missing rate \(r\), we select the missing indexes for \(Y\) and derive the missing indicator \(R\) according to the following criterion: calculate the \(m_{n}=1/D\sum_{d=1}^{D}(X_{nd}+s_{n}),n=1,\cdots,N,\) and choose the index of the row with the smallest \(rN\) values in \(\{m_{n},n=1,\cdots,N\}\) as the missing indexes. \(D\) is the feature dimension and \(N\) is the sample size.

**Experimental Details.** In this paper, preference vectors are used to quantify an individual's preference for different objectives in the multi-objective optimization problem. For the case of two-objective, we randomly generate 10 unit preference vectors \((\bm{u}_{1},\bm{u}_{2},\cdots,\bm{u}_{10})\), where \(\bm{u}_{k}=(u_{k1},u_{k2})\), \(u_{k1}=cos(t_{k}),u_{k2}=sin(t_{k})\), \(t_{k}\in(0,1)\), which implies that the \(L_{2}\)-norm of the preference vectors is 1, ensuring the consistency and comparability of the preference measures. \(u_{k1}\) and \(u_{k2}\) are the preferences for the short-term objective and the long-term objective, respectively. Each component of the preference vector \(\bm{u}_{k}\) represents the strength or importance of the decision maker's preference for different objectives. Preference vectors are used as weights in the linear weighting method, whereas our method uses them to divide the original problem (1) into several subproblems.

**Evaluation Metrics.** We measure the performance of our proposed method by three metrics: long and short-term rewards, the variance of long and short-term rewards, and the change in welfare. Formally, the short-term reward of the learned policy \(\hat{\pi}(\bm{X},\bm{\theta})\) is \(\hat{\mathcal{V}}(\bm{\theta};s)=\sum_{n=1}^{N}[\hat{\pi}(X_{n},\bm{\theta})S_{ n}(1)+(1-\hat{\pi}(X_{n},\bm{\theta}))S_{n}(0)]\), the long-term reward is \(\hat{\mathcal{V}}(\bm{\theta};y)=\sum_{n=1}^{N}[\hat{\pi}(X_{n},\bm{\theta})Y_{ n}(1)+(1-\hat{\pi}(X_{n},\bm{\theta}))Y_{n}(0)]\). Similar as [42, 43], the welfare changes are defined as \(\Delta W_{s}=\sum_{n=1}^{N}\left[(S_{n}(1)-S_{n}(0))\cdot\hat{\pi}(X_{n},\bm{ \theta})\right]\) for the short-term reward, \(\Delta W_{y}=\sum_{n=1}^{N}\left[(Y_{n}(1)-Y_{n}(0))\cdot\hat{\pi}(X_{n},\bm{ \theta})\right]\) for the long-term reward, \(\Delta W=0.5\Delta W_{s}+0.5\Delta W_{y}\) for the overall balanced-base reward. Among these metrics, \(\Delta W\) is the most critical here, as it directly measures the balance reward achieved by the learned policy.

**Policy learning with short-term and short-term reward.** We choose MLP as the policy model \(\pi(\bm{\theta})\), and we average over 50 independent trials of policy learning with the short-term and long-term reward in IHDP and JOBS. We fix the missing ratio \(r=0.2\) and the time step \(T=4\). We measure the uncertainty of the model by calculating the variance of the long and short-term reward over 50 experiments, and a smaller variance means a more stable model performance.

**Performance Comparison.** From our previous analyses, the linear weighting method generally achieves the sub-optimal policies. The proposed DPPL method can generate a set of Pareto optimal policies. First, for the long-term reward, the short-term reward, and \(\Delta W\), it is not surprising to observe that for most of the preference vectors, DPPL's solutions have better performance. Second, for the variance, our method performs more stable in 50 experiments. Because we will divide the original problem into several subproblems according to preference vectors, and then solve the subproblems in a relatively small subregion to obtain the Pareto optimal solution, whereas the linear weighting method searches the entire objective space. The associated results are displayed in Table 1. More experimental results with missing ratio \(r=0.3\) are given in Appendix D.

**Sensitivity Analysis.** We perform the sensitivity analysis of missing ratio \(r\) and time step \(T\) on JOBS. Our method achieves better performance in all missing rates \(r=[0.2,0.3,0.4,0.5]\) with \(T=4\), and \(r=0.2\) with time step \(T=[4,6,8,10]\). Our method stably outperforms the linear weighting method under varying \(r\) and \(T\), even in scenarios with a high missing ratio or a large time step. This further illustrates the effectiveness of our method. The associated results are displayed in Figure 1.

**Interpretation on preference vectors.** By Theorem 1, for the set of pre-specified preference vectors \((\bm{u}_{1},\bm{u}_{2},\cdots,\bm{u}_{10})\), we transform the optimization subproblem corresponding to each preference vector into the \(\varepsilon\)-constraint problem as \(\min_{\bm{\theta}}\bar{\mathcal{V}}(\bm{\theta};y),s.t.\bar{\mathcal{V}}(\bm {\theta};s)\leq\varepsilon(<0)\) or

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c} \hline \hline \multicolumn{1}{c}{IMDP} & \multicolumn{2}{c|}{S-Rewards} & \multicolumn{2}{c|}{L-Rewards} & \multicolumn{2}{c|}{\(\Delta W\)} & \multicolumn{2}{c|}{S-v\(\bm{\kappa}\)} & \multicolumn{2}{c}{L-var} \\ \hline Preference Vector & OURS & LW & OURS & LW & OURS & LW & OURS & LW & OURS & LW \\ \hline

[MISSING_PAGE_POST]

*1585.800** & **1225.679** & 1225.119 & **156.582** & 143.112 & **59.285** & 88.939 & 95.054 & **85.443** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of our method (OURS) and linear weighting method (LW) on IHDP and JOBS, with Short-Term Reward (S-REWARDS) and Long-Term Reward (L-REWARDS), \(\Delta W\) and Variance (S-Var and L-Var) as evaluation metrics. The best result is bolded.

Figure 1: Comparison of two methods with different missing ratios \(\{0.2,0.3,0.4,0.5\}\) on JOBS\(\max_{\bm{\theta}}\bm{\mathcal{V}}(\bm{\theta};y),s.t.\bm{\mathcal{V}}(\bm{\theta};s) \geq-\varepsilon\) and the threshold \(-\varepsilon\) are shown in Table 2. This value of \(-\varepsilon\) is the minimum value of the short-term reward that the decision maker can accept while maximizing the long-term reward. Our results show that as the second component of the preference vector increases, the value of \(-\varepsilon\) shows a decreasing trend. In essence, this signifies that a decision-maker who emphasizes the long-term reward must necessarily loosen constraints on the short-term reward. In practice, decision makers can determine the threshold based on their specific needs for the short-term reward, and then select the most appropriate preference vector from the set of pre-specify preference vectors with the help of the intuitive interpretability of the threshold according to Table 2. More experimental results with different missing ratios \(\{0.3,0.4,0.5\}\) are provided in Appendix D.

## 5 Related Work

**Estimation of long-term causal effects.** Assessing long-term causal effects is challenging due to the delayed long-term outcomes, posing significant difficulties in both identification and estimation. Recently, there has been increasing interest in using short-term surrogates to identify and estimate long-term causal effects, such as [44, 45, 5, 7, 13, 4]. In contrast to these previous works focusing on long-term causal effects, this paper aims to balance multiple short-term and long-term causal effects.

**Trustworthy policy learning.** Trustworthy policy learning ensures that the learned policies or models are reliable and dependable for practical applications. Traditional policy learning aims to identify individuals who would maximize the utility function based on their features if treated [46]. Recently, trustworthy policy learning has focused on ensuring that the learned policy adheres to principles such as benefcence, non-maleficence, autonomy, justice, no-harm, and explicability [47, 48, 49, 42]. Various counterfactual-based metrics have been suggested to assess a policy's trustworthiness [42, 50, 51, 52, 53]. In this paper, we complement this series of work by developing a principled policy learning approach that can effectively balance multiple rewards.

**Multi-objective optimization (MOP)**. MOP aims to find compromises or trade-offs among multiple possibly contrasting objectives. It is widely used in the field of machine learning such as multi-task learning [54, 40], neural architecture search [55], and multi-objective reinforcement learning [56, 57, 58]. We extend these works to a new setting by learning the optimal policy for balancing multiple long-term and short-term rewards. Additionally, we provide a practical method for interpreting and selecting preference vectors with theoretical guarantees.

## 6 Conclusion

In this paper, we focus on learning the optimal policy for balancing multiple long-term and short-term rewards. We reveal the limitations of the previous linear weighting method, which usually results in sub-optimal policies in practice. To address these limitations, we formulate the policy learning problem as a multi-objective optimization problem and then propose the novel DPPL method to learn optimal policies. The DPPL method obtains a set of Pareto optimal policies by solving a series of subproblems based on pre-specified preference vectors, effectively balancing multiple objectives. Furthermore, we theoretically establish the connection between the optimization subproblems in the DPPL method and the \(\varepsilon\)-constraint problem. This connection aids decision-makers in better understanding and selecting preference vectors. We conducted extensive experiments on two benchmark datasets which validate the effectiveness of our proposed method. A limitation of this work is that it focuses on discrete treatments in identification and estimation (Section 2.3). In some application scenarios, continuous treatments (e.g., price) are of interest. Further investigation is required to extend the proposed method to accommodate such cases.

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline Preference Vector & \(\cdot\) \(\mathbf{\sigma}\) on IHDP & \(\cdot\) \(\mathbf{\sigma}\) on JOBS & Preference Vector & \(\cdot\) \(\mathbf{\sigma}\) on IHDP & \(\cdot\) \(\mathbf{\sigma}\) on JOBS \\ \hline (1.00, 0.00) & 0.820 & 0.878 & (0.00, 1.00) & 0.522 & 0.737 \\ (0.98, 0.17) & 0.827 & 0.875 & (0.17, 0.98) & 0.522 & 0.716 \\ (0.94, 0.34) & 0.826 & 0.868 & (0.34, 0.94) & 0.511 & 0.704 \\ (0.86, 0.50) & 0.833 & 0.868 & (0.50, 0.86) & 0.557 & 0.746 \\ (0.77, 0.64) & 0.741 & 0.865 & (0.64, 0.76) & 0.659 & 0.808 \\ \hline \hline \end{tabular}
\end{table}
Table 2: The \(\varepsilon\) values correspond to each preference vector in IHDP and JOBS datasets, where \(T=4\) and \(r=0.2\), obtained according to Theorem 1.

## Acknowledgements

Qinwei Yang, Xueqing Liu, Yan Zeng, and Peng Wu were supported by the National Natural Science Foundation of China (No. 12301370, 62473009), the funding from the Beijing Municipal Education Commission for the Emerging Interdisciplinary Platform for Digital Business at Beijing Technology and Business University, the Beijing Key Laboratory of Applied Statistics and Digital Regulation, and the gift funding from ByteDance Research.

## References

* [1] Lu Cheng, Ruocheng Guo, and Huan Liu. Long-term effect estimation with surrogate representation. In _Proceedings of the 14th ACM International Conference on Web Search and Data Mining_, page 274-282, 2021.
* [2] Henning Hohnhold, Deirdre O'Brien, and Diane Tang. Focusing on the long-term: It's good for users and business. In _Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1849-1858, 2015.
* [3] Raj Chetty, John N. Friedman, Nathaniel Hilger, Emmanuel Saez, Diane Whitmore Schanzenbach, and Danny Yagan. How does your kindergarten classroom affect your earnings? evidence from project star. _The Quarterly Journal of Economics_, 126:1593-1660, 2007.
* [4] Susan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. The surrogate index: Combining short-term proxies to estimate long-term treatment effects more rapidly and precisely. Technical report, National Bureau of Economic Research, 2019.
* [5] Susan Athey, Raj Chetty, and Guido Imbens. Combining experimental and observational data to estimate treatment effects on long term outcomes. _arXiv preprint arXiv:2006.09676_, 2020.
* [6] Wenjie Hu, Xiao-Hua Zhou, and Peng Wu. Identification and estimation of treatment effects on long-term outcomes in clinical trials with external observational data. _Statistica Sinica_, 2023.
* [7] Jeremy Yang, Dean Eckles, Paramveer Dhillon, and Sinan Aral. Targeting for long-term outcomes. _Management Science_, 2023.
* [8] Peng Wu, Ziyu Shen, Feng Xie, Zhongyao Wang, Chunchen Liu, and Yan Zeng. Policy learning for balancing short-term and long-term rewards. In _ICML_, 2024.
* [9] Yair Censor. Pareto optimality in multiobjective problems. _Applied Mathematics and Optimization_, 4(1):41-59, 1977.
* [10] Jurgen Branke. _Multiobjective optimization: Interactive and evolutionary approaches_, volume 5252. Springer Science & Business Media, 2008.
* [11] G. W. Imbens and D. B. Rubin. _Causal Inference For Statistics Social and Biomedical Science_. Cambridge University Press, 2015.
* [12] Susan Athey, Raj Chetty, Guido Imbens, and Hyunseung Kang. The surrogate index: Combining short-term proxies to estimate long-term treatment effects more rapidly and precisely. Working paper, National Bureau of Economic Research, 2019.
* [13] Nathan Kallus and Xiaojie Mao. On the role of surrogates in the efficient estimation of treatment effects with limited outcome data. _arXiv preprint arXiv:2003.12408_, 2020.
* [14] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, and Peng Cui. Propensity matters: Measuring and enhancing balancing for recommendation. In _ICML_, 2023.
* [15] Peng Wu, Shanshan Luo, and Zhi Geng. On the comparative analysis of average treatment effects estimation via data combination. _arXiv preprint arXiv:2311.00528_, 2024.
* [16] Sihao Ding, Peng Wu, Fuli Feng, Xiangnan He, Yitong Wang, Yong Liao, and Yongdong Zhang. Addressing unmeasured confounder for recommendation with sensitivity analysis. In _KDD_, 2022.

* [17] Haoxuan Li, Quanyu Dai, Yuru Li, Yan Lyu, Zhenhua Dong, Xiao-Hua Zhou, and Peng Wu. Multiple robust learning for recommendation. In _AAAI_, 2023.
* [18] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, and Peng Wu. Balancing unobserved confounding with a few unbiased ratings in debiased recommendations. In _WWW_, 2023.
* [19] Quanyu Dai, Haoxuan Li, Peng Wu, Zhenhua Dong, Xiao-Hua Zhou, Rui Zhang, Xiuqiang He, Rui Zhang, and Jie Sun. A generalized doubly robust learning framework for debiasing post-click conversion rate prediction. In _KDD_, 2022.
* [20] Peng Wu, Haoxuan Li, Yuhao Deng, Wenjie Hu, Quanyu Dai, Zhenhua Dong, Jie Sun, Rui Zhang, and Xiao-Hua Zhou. On the opportunity of causal learning in recommendation systems: Foundation, estimation, prediction and challenges. In _IJCAI_, 2022.
* [21] Haoxuan Li, Yan Lyu, Chunyuan Zheng, and Peng Wu. TDR-CL: Targeted doubly robust collaborative learning for debiased recommendations. In _ICLR_, 2023.
* [22] Haoxuan Li, Chunyuan Zheng, and Peng Wu. Stabledr: Stabilized doubly robust learning for recommendation on data missing not at random. In _ICLR_, 2023.
* [23] Haoxuan Li, Chunyuan Zheng, Sihao Ding, Peng Wu, Zhi Geng, Fuli Feng, and Xiangnan He. Be aware of the neighborhood effect: Modeling selection bias under interference for recommendation. In _ICLR_, 2024.
* [24] Guido Imbens, Nathan Kallus, Xiaojie Mao, and Yuhao Wang. Long-term causal inference under persistent confounding via data combination. _arXiv preprint arXiv:2202.07234_, 2022.
* [25] Judea Pearl, Madelyn Glymour, and Nicholas P. Jewell. _Causal Inference in Statistics: A Primer_. John Wiley & Sons, 2016.
* [26] Judea Pearl and Dana Mackenzie. _The Book of Why: The New Science of Cause and Effect_. Hachette Book Group, 2018.
* [27] M.A. Hernan and J. M. Robins. _Causal Inference: What If_. Boca Raton: Chapman and Hall/CRC, 2020.
* [28] Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. _Biometrika_, 70(1):41-55, 1983.
* [29] Kosuke Imai and Marc Ratkovic. Covariate balancing propensity score. _Journal of the Royal Statistical Society (Series B)_, 76(1):243-263, 2014.
* [30] Elizabeth A. Stuart. Matching methods for causal inference: A review and a look forward. _Statistical Science_, 25(1):1-21, 2010.
* [31] Paul R. Rosenbaum. _Design of Observational Studies_. Springer Nature Switzerland AG, second edition, 2020.
* [32] Peng Wu, Xinyi Xu, Xingwei Tong, Qing Jiang, and Bo Lu. Semiparametric estimation for average causal effects using propensity score-based spline. _Journal of Statistical Planning and Inference_, 212:153-168, 2021.
* [33] Peng Wu, Zhiqiang Tan, Wenjie Hu, and Xiao-Hua Zhou. Model-assisted inference for covariate-specific treatment effects with high-dimensional data. _Statistica Sinica_, 34:459-479, 2024.
* [34] Peng Wu, Shasha Han, Xingwei Tong, and Runze Li. Propensity score regression for causal inference with treatment heterogeneity. _Statistica Sinica_, 34:747-769, 2024.
* [35] Peng Wu, Peng Ding, Zhi Geng, and Yue Li. Quantifying individual risk for binary outcome: Bounds and inference. _arXiv:2402.10537_, 2024.
* [36] Kalyanmoy Deb, Karthik Sindhya, and Jussi Hakanen. Multi-objective optimization. In _Decision sciences_, pages 161-200. CRC Press, 2016.

* [37] Yu Zhang and Qiang Yang. A survey on multi-task learning. _IEEE Transactions on Knowledge and Data Engineering_, 34(12):5586-5609, 2022.
* [38] Haoxuan Li, Chunyuan Zheng, Yanghao Xiao, Hao Wang, Fuli Feng, Xiangnan He, Zhi Geng, and Peng Wu. Removing hidden confounding in recommendation: A unified multi-task learning approach. In _NeurIPS_, 2023.
* [39] Amir Ismail-Yahaya and Achille Messac. Effective generation of the pareto frontier using the normal constraint method. In _40th AIAA Aerospace Sciences Meeting & Exhibit_, page 178, 2002.
* [40] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. _Advances in neural information processing systems_, 32, 2019.
* [41] Jorg Fliege and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. _Mathematical methods of operations research_, 51:479-494, 2000.
* [42] Haoxuan Li, Chunyuan Zheng, Yixiao Cao, Zhi Geng, Yue Liu, and Peng Wu. Trustworthy policy learning under the counterfactual no-harm criterion. In _International Conference on Machine Learning_, pages 20575-20598. PMLR, 2023.
* [43] T. Kitagawa and A. Tetenov. Who should be treated? empirical welfare maximization methods for treatment choice. _Econometrica_, 86, 2018.
* [44] Lu Cheng, Ruocheng Guo, and Huan Liu. Long-term effect estimation with surrogate representation. In _Proceedings of the 14th ACM International Conference on Web Search and Data Mining_, pages 274-282, 2021.
* [45] Wenjie Hu, Xiaohua Zhou, and Peng Wu. Identification and estimation of treatment effects on long-term outcomes in clinical trials with external observational data. _arXiv preprint arXiv:2208.10163_, 2022.
* [46] Michael R Kosorok and Eric B Laber. Precision medicine. _Annual review of statistics and its application_, 6:263-286, 2019.
* [47] Scott Thiebes, Sebastian Lins, and Ali Sunyaev. Trustworthy artificial intelligence. _Electronic Markets_, 31:447-464, 2021.
* [48] Floridi Luciano. Establishing the rules for building trustworthy ai. _Nature Machine Intelligence_, 1(6):261-262, 2019.
* [49] Davinder Kaur, Suleyman Uslu, Kaley J Rittichier, and Arjan Durresi. Trustworthy artificial intelligence: a review. _ACM Computing Surveys (CSUR)_, 55:1-38, 2022.
* [50] Silvia Chiappa. Path-specific counterfactual fairness. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 7801-7808, 2019.
* [51] Yongkai Wu, Lu Zhang, Xintao Wu, and Hanghang Tong. PC-Fairness: A unified framework for measuring causality-based fairness. _Advances in Neural Information Processing Systems_, 32, 2019.
* [52] Nathan Kallus. Treatment effect risk: Bounds and inference. _Management Science_, 69(8):4579-4590, 2023.
* [53] Nathan Kallus. What's the harm? sharp bounds on the fraction negatively affected by treatment. _Advances in Neural Information Processing Systems_, 35:15996-16009, 2022.
* [54] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. _Advances in neural information processing systems_, 31, 2018.
* [55] Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, and Min Sun. Dpp-net: Device-aware progressive search for pareto-optimal neural architectures. In _Proceedings of the European conference on computer vision (ECCV)_, pages 517-531, 2018.

* [56] Kristof Van Moffaert, Madlina M Drugan, and Ann Nowe. Scalarized multi-objective reinforcement learning: Novel design techniques. In _2013 IEEE symposium on adaptive dynamic programming and reinforcement learning (ADPRL)_, pages 191-199. IEEE, 2013.
* [57] Nan Xu, Nitin Kamra, and Yan Liu. Treatment recommendation with preference-based reinforcement learning. In _2021 IEEE international conference on big knowledge (ICBK)_, pages 1-8. IEEE, 2021.
* [58] Jiangjiao Xu, Ke Li, and Mohammad Abusara. Preference based multi-objective reinforcement learning for multi-microgrid system optimization problem in smart grid. _Memetic Computing_, 14(2):225-235, 2022.

Estimation of Short-Term and Long-Term Rewards

For a given policy \(\pi(\bm{\theta})\), the policy values for the short-term outcome \(S_{i}\) and the long-term outcome \(Y_{j}\) are defined as

\[\mathcal{V}(\bm{\theta};s_{i}) =\mathbb{E}[\pi(\bm{\theta})S_{i}(1)+(1-\pi(\bm{\theta}))S_{i}(0)],\quad i=1,...,I\] \[\mathcal{V}(\bm{\theta};y_{j}) =\mathbb{E}[\pi(\bm{\theta})Y_{j}(1)+(1-\pi(\bm{\theta}))Y_{j}(0 )],\quad j=1,...,J,\]

Under Assumptions 1-2, the short-term reward \(\mathcal{V}(\bm{\theta};s_{i})\) and long-term reward \(\mathcal{V}(\bm{\theta};y_{j})\) are identified as

\[\mathcal{V}(\bm{\theta};s_{i}) =\mathbb{E}[\pi(\bm{\theta})\mu_{i1}(X)+(1-\pi(\bm{\theta}))\mu_{ i0}(X)],\] \[\mathcal{V}(\bm{\theta};y_{j}) =\mathbb{E}[\pi(\bm{\theta}\tilde{m}_{j1}(X,S)+(1-\pi(\bm{\theta }))\tilde{m}_{j0}(X,S)].\]

where \(\mu_{ia}(\bm{X})=\mathbb{E}[S_{i}|\bm{X},A=a]\), \(\tilde{m}_{ja}(\bm{X},\bm{S})=\mathbb{E}[Y_{j}|\bm{X},\bm{S},A=a,R_{j}=1]\) for \(a=0,1\). The identifiability results are derived using a similar approach to that outlined in Section 5 of [8]. In addition, for estimating the \(\mathcal{V}(\bm{\theta};s_{i})\) and \(\mathcal{V}(\bm{\theta};y_{j})\), [8] proved the efficient bounds of \(\mathcal{V}(\bm{\theta};s_{i})\) and \(\mathcal{V}(\bm{\theta};y_{j})\), which we list them below for the sake of self-containedness.

**Lemma A.1** (Efficiency Bounds of \(\mathcal{V}(\bm{\theta};s_{i})\) and \(\mathcal{V}(\bm{\theta};y_{j})\), [8]).: _Let \(\mathbf{Z}=(\bm{X},A,\bm{S},\bm{Y})\), under Assumptions 1-2, we have that_

_(a) the efficient influence function of_ \(\mathcal{V}(\bm{\theta};s_{i})\) _is_ \(\phi_{s_{i}}-\mathcal{V}(\bm{\theta};s_{i})\)_, where_

\[\phi_{s_{i}} =\phi_{s_{i}}(\bm{Z};e,\mu_{i0},\mu_{i1})\] \[=\{\pi(\bm{\theta})\mu_{i1}(\bm{X})+(1-\pi(\bm{\theta}))\mu_{i0}( \bm{X})\}\] \[+\frac{\pi(\bm{\theta})A(S_{i}-\mu_{i1}(\bm{X}))}{e(\bm{X})}+ \frac{(1-\pi(\bm{\theta}))(1-A)(S_{i}-\mu_{i0}(\bm{X}))}{1-e(\bm{X})},\]

_and_ \(e(\bm{X})=\mathbb{P}(A=1|\bm{X})\) _is propensity score. The associated semiparametric efficiency bound of_ \(\mathcal{V}(\bm{\theta};s_{i})\) _is Var(_\(\phi_{s_{i}}\)_)._

_(b) the efficient influence function of_ \(\mathcal{V}(\bm{\theta};y_{j})\) _is_ \(\phi_{y_{j}}-\mathcal{V}(\bm{\theta};y_{j})\)_, where_

\[\phi_{y_{j}} =\phi_{y_{j}}(\bm{Z};e,r_{j},m_{j0},m_{j1},\tilde{m}_{j0},\tilde {m}_{j1})\] \[=\{\pi(\bm{\theta})m_{j1}(\bm{X})+(1-\pi(\bm{\theta}))m_{j0}(\bm{ X})\}\] \[+\frac{\pi(\bm{\theta})AR_{j}(Y_{j}-\tilde{m}_{j1}(\bm{X},\bm{S}) )}{e(\bm{X})r_{j}(1,\bm{X},\bm{S})}+\frac{\pi(\bm{\theta})A(\tilde{m}_{j1}( \bm{X},\bm{S})-m_{j1}(\bm{X}))}{e(\bm{X})}\] \[+\frac{(1-\pi(\bm{\theta}))(1-A)R_{j}(Y_{j}-\tilde{m}_{j0}(\bm{ X},\bm{S}))}{(1-e(\bm{X}))r_{j}(0,\bm{X},\bm{S})}\] \[+\frac{(1-\pi(\bm{\theta}))(1-A)(\tilde{m}_{j0}(\bm{X},\bm{S})-m _{j0}(\bm{X}))}{1-e(\bm{X})},\]

\(m_{ja}(\bm{X})=\mathbb{E}[Y_{j}|\bm{X},A=a,R_{j}=1]\) _is the regression function for_ \(Y_{j}\)_, and_ \(r_{j}(A,\bm{X},\bm{S})=\mathbb{P}[R_{j}=1|\bm{X},\bm{S},A]\) _is selection score. The associated semiparametric efficiency bound of_ \(\mathcal{V}(\bm{\theta};y_{j})\) _is Var(_\(\phi_{y_{j}}\)_)._

From Lemma A.1, for a given policy \(\pi(\bm{\theta})\), it is natural to define the estimators of \(\mathcal{V}(\bm{\theta};s_{i})\) and \(\mathcal{V}(\bm{\theta};y_{j})\) as

\[\hat{\mathcal{V}}(\bm{\theta};s_{i}) =\frac{1}{N}\sum_{n=1}^{N}\phi_{s_{i}}(Z_{n};\hat{e},\hat{\mu}_{ i0},\hat{\mu}_{i1}),\] \[\hat{\mathcal{V}}(\bm{\theta};y_{j}) =\frac{1}{N}\sum_{n=1}^{N}\phi_{y_{j}}(Z_{n};\hat{e},\hat{r}_{j}, \hat{m}_{j0},\hat{m}_{j1},\hat{\tilde{m}}_{j0},\hat{\tilde{m}}_{j1}).\]

where \(N\) is the sample size. All of them can be identified from the observed data. And \(\hat{e}(\bm{x}),\hat{\mu}_{ia}(\bm{x}),\hat{m}_{ja}(\bm{x}),\hat{\tilde{m}}_{ja} (\bm{x},\bm{s})\), and \(\hat{r}_{j}(a,\bm{x},\bm{s})\) for \(a=0,1\) are the estimators of \(e(\bm{x}),\mu_{ia}(\bm{x}),m_{ja}(\bm{x}),\tilde{m}_{ja}(\bm{x},\bm{s})\) and \(r_{j}(a,\bm{x},\bm{s})\) respectively.

## Appendix B Algorithm Flowchart for DPPL

```
1:Input: A set of preference vectors \(\{\mathbf{u}_{1},\mathbf{u}_{2},...,\mathbf{u}_{K}\}\) (All subproblems can be solved in parallel)
2:for\(k=1\) to \(K\)do
3: randomly generate parameters \(\boldsymbol{\theta}_{r}^{(k)}\)
4: find the initial parameters \(\boldsymbol{\theta}_{0}^{(k)}\) from \(\boldsymbol{\theta}_{r}^{(k)}\) using gradient-based method (step a)
5:for\(t=1\) to \(T\)do
6: obtain \(\lambda_{tm}^{(k)}\geq 0\), \(\beta_{tk}^{(k)}\geq 0,\forall m=1,...,M,\forall k^{\prime}\in I_{c}( \boldsymbol{\theta})\) by solving subproblem (6)
7: calculate direction \(\boldsymbol{d}_{t}^{(k)}=-(\sum_{i=m}^{M}\lambda_{tm}^{(k)}\nabla\bar{\mathcal{ V}}_{m}(\boldsymbol{\theta}_{t}^{(k)})+\sum_{k^{\prime}\in I_{c(\boldsymbol{ \theta})}}\beta_{tk^{\prime}}^{(k)}\nabla\mathcal{G}_{k^{\prime}}(\boldsymbol {\theta}_{t}^{(k)}))\,/\boldsymbol{d}_{t}^{(k)}=-(\lambda_{tm}^{k}+\sum_{k^{ \prime}\in I_{c(\boldsymbol{\theta})}}\beta_{tk^{\prime}}^{(k)}(\boldsymbol{ u}_{k^{\prime}m}-\boldsymbol{u}_{km}))\nabla\bar{\mathcal{V}}_{m}( \boldsymbol{\theta}_{t}^{k})\) (step b)
8: update the parameters \(\boldsymbol{\theta}_{t+1}^{(k)}=\boldsymbol{\theta}_{t}^{(k)}+\eta d_{t}^{(k)}\)
9:endfor
10:endfor
11:Output: The set of solutions for all subproblems with different trade-offs \(\{\boldsymbol{\theta}_{T}^{(k)}|k=1,\ldots,K\}\) ```

**Algorithm 1** DPPL Algorithm

## Appendix C Proofs of Theorem 1

For the case of only have one long-term outcome \(Y\) and one short-term outcome \(S\), considering the \(\varepsilon\)-constraint optimization problem

\[\min_{\boldsymbol{\theta}}\bar{\mathcal{V}}(\boldsymbol{\theta};y),\quad s.t.,\bar{\mathcal{V}}(\boldsymbol{\theta};s)\leq\varepsilon\] (A.1)

and the linear weighting optimization problem

\[\min_{\boldsymbol{\theta}}\omega_{1}\bar{\mathcal{V}}(\boldsymbol{\theta};y)+ \omega_{2}\bar{\mathcal{V}}(\boldsymbol{\theta};s)\] (A.2)

which can be reformulated as

\[\min_{\boldsymbol{\theta}}\bar{\mathcal{V}}(\boldsymbol{\theta};y)+\lambda \bar{\mathcal{V}}(\boldsymbol{\theta};s).\]

where \(\lambda=\omega_{2}/\omega_{1}\) controls the balance between short-term and long-term rewards. Let \(\tau_{s}(\boldsymbol{X})=\mathbb{E}[S(1)-S(0)|\boldsymbol{X}]\) and \(\tau_{y}(\boldsymbol{X})=\mathbb{E}[Y(1)-Y(0)|\boldsymbol{X}]\). When \(\lambda=0\), it is equivalent to finding an optimal policy for minimizing \(\bar{\mathcal{V}}(\boldsymbol{\theta};y)\) alone, \(\pi_{y}^{*}(\boldsymbol{\theta})=\arg\min_{\pi}\bar{\mathcal{V}}(\boldsymbol{ \theta};y)=\arg\min_{\pi}-\mathbb{E}[\pi(\boldsymbol{\theta})\tau_{y}( \boldsymbol{X})]=\mathbb{I}(\tau_{y}(\boldsymbol{X})\geq 0)\). When \(\lambda=\infty\), it is equivalent to finding an optimal policy for minimizing the \(\bar{\mathcal{V}}(\boldsymbol{\theta};s)\) alone, \(\pi_{s}^{*}(\boldsymbol{\theta})=\arg\min_{\pi}\bar{\mathcal{V}}(\boldsymbol {\theta};s)=\arg\min_{\pi}-\mathbb{E}[\pi(\boldsymbol{\theta})\tau_{s}( \boldsymbol{X})]=\mathbb{I}(\tau_{s}(\boldsymbol{X})\geq 0)\). We have the following theorem:

**Theorem C.1**.: _For the weights \(\boldsymbol{\omega}\) in problem (A.5), and the thresholds \(\varepsilon\) in problem (A.1), the following statements hold:_

* _When_ \(\varepsilon<-\mathbb{E}[\pi_{s}^{*}(\boldsymbol{\theta})S(1)+(1-\pi_{s}^{*}( \boldsymbol{\theta}))S(0)]\)_, the solution of the constrained optimization problem is empty._
* _When_ \(\varepsilon\geq-\mathbb{E}[\pi_{s}^{*}(\boldsymbol{\theta})S(1)+(1-\pi_{s}^{*} (\boldsymbol{\theta}))S(0)]\)_, the relationship between_ \(\lambda\) _and_ \(\alpha\) _is described as follows:_
* \(\lambda=0\)_, if_ \(\varepsilon\geq-\mathbb{E}[\pi_{y}^{*}(\boldsymbol{\theta})S(1)+(1-\pi_{y}^{*} (\boldsymbol{\theta}))S(0)]\)_._
* \(\lambda\) _is the solution of the equation_ \[-\mathbb{E}[\mathbb{I}(\tau_{y}(\boldsymbol{X})+\lambda\tau_{s}(\boldsymbol{X} )>0)\cdot\tau_{s}(\boldsymbol{X})+\mu_{0}(\boldsymbol{X})]=\varepsilon,\] \[\text{if }-\mathbb{E}[\pi_{s}^{*}(\boldsymbol{\theta})S(1)+(1-\pi_{s}^{*} (\boldsymbol{\theta}))S(0)]<\varepsilon\leq-\mathbb{E}[\pi_{y}^{*}(\boldsymbol {\theta})S(1)+(1-\pi_{y}^{*}(\boldsymbol{\theta}))S(0)].\]

It is important to note that for a given \(\lambda\), we could solve the value of \(\varepsilon\) by solving the equation

\[-\mathbb{E}[\mathbb{I}(\tau_{y}(\boldsymbol{X})+\lambda\tau_{s}(\boldsymbol{X} )>0)\cdot\tau_{s}(\boldsymbol{X})+\mu_{0}(\boldsymbol{X})]=\varepsilon,\]

as the left side of the equation is a monotone function of \(\lambda\) and the solution is unique, and all the quantities such as \(\tau_{s}(\boldsymbol{X}),\tau_{y}(\boldsymbol{X}),\) and \(\mu_{0}(\boldsymbol{X})\) are identifiable.

Proof.: Initially, we recognize that \(\varepsilon\) cannot be too small so that no policy can satisfy the constraint of \(\mathcal{\bar{V}}(\bm{\theta};s)\leq\varepsilon\). The optimal policy of minimizing only the \(\mathcal{\bar{V}}(\bm{\theta};s)\) is \(\pi_{s}^{*}(\bm{\theta})=\mathbb{I}(\tau_{s}(\bm{X})\geq 0)\). Thus, \(\varepsilon\geq-\mathbb{E}[\pi_{s}^{*}(\bm{\theta})S(1)+(1-\pi_{s}^{*}(\bm{ \theta}))S(0)]\).

When \(\varepsilon\geq-\mathbb{E}[\pi_{s}^{*}(\bm{\theta})S(1)+(1-\pi_{s}^{*}(\bm{ \theta}))S(0)]\). First, the optimal policy of minimizing only \(\mathcal{\bar{V}}(\pi;y)\) is given as \(\pi_{y}^{*}(\bm{\theta})=\mathbb{I}(\tau_{y}(\bm{X})\geq 0)\). Then, \(\varepsilon\leq-\mathbb{E}[\pi_{y}^{*}(\bm{\theta})S(1)+(1-\pi_{y}^{*}(\bm{ \theta}))S(0)]\). otherwise, the constraint will be invalid and the constrained optimization problem becomes an unconstrained optimization problem with \(\lambda=0\).

Second, when \(-\mathbb{E}[\pi_{s}^{*}(\bm{\theta})S(1)+(1-\pi_{s}^{*}(\bm{\theta}))S(0)]\leq \varepsilon\leq-\mathbb{E}[\pi_{y}^{*}(\bm{\theta})S(1)+(1-\pi_{y}^{*}(\bm{ \theta}))S(0)]\), we show that the optimal policy \(\pi^{*}(\bm{\theta})\) parameterized by \(\bm{\theta}^{*}\), for the constrained optimization problem

\[\min_{\bm{\theta}}\mathcal{\bar{V}}(\bm{\theta};y),\quad s.t.,\mathcal{\bar{ V}}(\bm{\theta};s)\leq\varepsilon\]

is obtained only when \(\mathcal{\bar{V}}(\bm{\theta}^{*};s)=\varepsilon\). Below, we prove it with the method of reduction to absurdity. If \(-\mathbb{E}[\pi_{s}^{*}(\bm{\theta})S(1)+(1-\pi_{s}^{*}(\bm{\theta}))S(0)]\leq \varepsilon\leq-\mathbb{E}[\pi_{y}^{*}(\bm{\theta})S(1)+(1-\pi_{y}^{*}(\bm{ \theta}))S(0)]\), then there are some units that satisfies \(\{\tau_{s}(\bm{X})<0,\tau_{y}(\bm{X})>0\}\) that not being assigned treatment by \(\pi^{*}(\bm{\theta})\); otherwise, the constraint \(\mathcal{\bar{V}}(\bm{\theta};s)\leq\varepsilon\) will be violated. Thus, we could find another treatment policy \(\tilde{\pi}^{*}\) that assigns more treatment to the units with \(\{\tau_{s}(\bm{X})<0,\tau_{y}(\bm{X})>0\}\), which yields a lower \(\mathcal{\bar{V}}(\bm{\theta};y)\) but increases the \(\mathcal{\bar{V}}(\bm{\theta};s)\). That is, \(\tilde{\pi}^{*}\) will lead to a \(\mathcal{\bar{V}}(\bm{\theta};s)\) closer to \(\varepsilon\) but has a lower \(\mathcal{\bar{V}}(\bm{\theta};y)\) than \(\pi^{*}\), thus, \(\pi^{*}\) is not the optimal policy, which contradicts its definition of \(\pi^{*}\). Thus, the constrained optimization problem becomes

\[\min_{\bm{\theta}}\mathcal{\bar{V}}(\bm{\theta};y),\quad s.t.,\mathcal{\bar{ V}}(\bm{\theta};s)=\varepsilon.\]

By introducing the Lagrange multiplier \(\beta\), \(\pi^{*}\) satisfies

\[\pi^{*}=\arg\min_{\pi}\quad\mathcal{\bar{V}}(\bm{\theta};y)+\beta\mathcal{ \bar{V}}(\bm{\theta};s)=\mathbb{I}(\tau_{y}(\bm{X})+\beta\tau_{s}(\bm{X})>0),\]

where \(\beta\) is the solution of \(\mathcal{\bar{V}}(\bm{\theta}^{*};s)=\varepsilon\), i.e.,

\[-\mathbb{E}[\mathbb{I}(\tau_{y}(\bm{X})+\beta\tau_{s}(\bm{X})>0)\tau_{s}(\bm{ X})+\mu_{0}(\bm{X})]=\varepsilon.\]

This completes the proof for Theorem C.1. 

We can further extend Theorem C.1 to situations where there are multiple long-term rewards and multiple short-term rewards. More generally, for the \(\varepsilon\)-constraint optimization problem

\[\min_{\bm{\theta}}\mathcal{\bar{V}}_{l}(\bm{\theta}),\ \ \mathrm{s.\ t.}\ \mathcal{\bar{V}}_{m}(\bm{ \theta})\leq\varepsilon_{m}\ \mathrm{for\ all}\ m=1,\ldots,M,m\neq l,\] (A.3)

and the linear weighting optimization problem

\[\min_{\bm{\theta}}\mathcal{\bar{V}}(\bm{\theta})=\sum_{i=m}^{M}\omega_{m} \mathcal{\bar{V}}_{m}(\bm{\theta}),\] (A.4)

where \(\omega_{m}\) is the pre-specified weight for the \(m\)-th reward. We have the following theorem:

**Theorem 1**.: _For the preference vector \(\bm{u}_{k}\) in problem (1), the weights \(\bm{\omega}\) in problem (2), and the thresholds \(\bm{\varepsilon}\) in problem (8), the following statements hold: (a) the connection between \(\bm{\varepsilon}\) and \(\bm{\omega}\) is given as_

\[-\mathbb{E}[\mathbb{I}(\tau_{l}(\bm{X})+\frac{\omega_{m}}{\omega_{l}}\tau_{m}( \bm{X})>0)\cdot\tau_{m}(\bm{X})+h_{m}(\bm{X})]=\varepsilon_{m},\ \text{for}\ m=1\cdots M,and\ m\neq l,\]

_where \(\tau_{m}(\bm{X})\) is the conditional average causal effects for \(m\)-th short/long-term outcome,_

\[\tau_{m}(\bm{X})=\begin{cases}\mathbb{E}[S_{i}(1)-S_{i}(0)|\bm{X}],\ \text{if}\ \omega_{m}\ \text{is the weight of}\ \mathcal{\bar{V}}(\bm{\theta},s_{i}),\\ \mathbb{E}[Y_{j}(1)-Y_{j}(0)|\bm{X}],\ \text{if}\ \omega_{m}\ \text{is the weight of}\ \mathcal{\bar{V}}(\bm{\theta},y_{j}),\end{cases}\]

_and_

\[h_{m}(\bm{X})=\begin{cases}\mathbb{E}[S_{i}(0)|\bm{X}],\ \text{if}\ \omega_{m}\ \text{is the weight of}\ \mathcal{\bar{V}}(\bm{\theta},s_{i}),\\ \mathbb{E}[Y_{j}(0)|\bm{X},\bm{S},R_{j}=1],\ \text{if}\ \omega_{m}\ \text{is the weight of}\ \mathcal{\bar{V}}(\bm{\theta},y_{j}),\end{cases}\]_and \(\mathbb{I}(\cdot)\) is the indicator function._

_(b) the connection between_ \(\bm{\omega}\) _and_ \(\bm{u}_{k}\) _is given as_

\[\omega_{m}=\lambda_{m}+\sum_{k^{\prime}\in\mathcal{I}_{*}(\bm{\theta})}\beta_{k^ {\prime}}(\bm{u}_{k^{\prime}m}-\bm{u}_{km}),\;\text{for}\;m=1,\cdots,M,\]

_where_ \(\lambda_{m}\) _and_ \(\beta_{k^{\prime}}\) _are defined in Eq.(_6_),_ \(\mathcal{I}_{\epsilon}(\bm{\theta})=\{k^{\prime}|\mathcal{G}_{k^{\prime}}( \bm{\theta})\geq-\epsilon\}\)__

Proof.: First, for the Theorem1(a), combining the TheoremC.1, more generally, for the \(\varepsilon\)-constraint problem

\[\min_{\bm{\theta}}\ \bar{\mathcal{V}}_{l}(\bm{\theta}),\ \ \text{s.\ t.}\ \bar{ \mathcal{V}}_{m}(\bm{\theta})\leq\varepsilon_{m}\ \text{for all}\ m=1,\ldots,M,m\neq l,\] (A.5)

and the linear weighting optimization problem

\[\min_{\bm{\theta}}\bar{\mathcal{V}}(\bm{\theta})=\sum_{i=m}^{M}\omega_{m} \bar{\mathcal{V}}_{m}(\bm{\theta}),\] (A.6)

where \(\omega_{m}\) is the pre-specified weight for the \(m\)-th reward. By mathematical induction, we have:

\[-\mathbb{E}[\mathbb{I}(\tau_{l}(\bm{X})+\omega_{m}/\omega_{l}\tau_{m}(\bm{X}) >0)\tau_{m}(\bm{X})+h_{m0}(\bm{X})]=\varepsilon_{i},m=1\cdots M,and\ m\neq l\]

where \(\tau_{m}(\bm{X})=\begin{cases}\tau_{s_{i}}=\mathbb{E}[S_{i}(1)-S_{i}(0)|\bm{X }],\;\text{if}\;\omega_{m}\;\text{is the weight of}\ \bar{\mathcal{V}}(\bm{\theta},s_{i}),\\ \tau_{y_{j}}=\mathbb{E}[Y_{j}(1)-Y_{j}(0)|\bm{X}],\;\text{if}\;\omega_{m}\; \text{is the weight of}\ \bar{\mathcal{V}}(\bm{\theta},y_{j}),\end{cases}\)

\(h_{m0}(X)=\begin{cases}\mu_{i0}(\bm{X})=\mathbb{E}[S_{i}|\bm{X},A=0],\;\text {if}\;\omega_{m}\;\text{is the weight of}\ \bar{\mathcal{V}}(\bm{\theta},s_{i}),\\ \tilde{m}_{j0}(\bm{X},\bm{S})=\mathbb{E}[Y_{j}|\bm{X},\bm{S},A=a,R_{j}=1],\; \text{if}\;\omega_{m}\;\text{is the weight of}\ \bar{\mathcal{V}}(\bm{\theta},y_{j}),\end{cases}\)

This completes the proof for Theorem 1(a)

Second, for the Theorem (b), motivated by [40], for constraint problem

\[\begin{split}(\bm{d}_{t},\alpha_{t})=&\arg\min_{d\in \mathbb{R}^{m},\alpha\in\mathbb{R}}\alpha+\frac{1}{2}||\bm{d}||^{2}\\ s.t.&\nabla\bar{\mathcal{V}}_{m}(\bm{\theta}_{t})^{T}\bm{d}\leq \alpha,m=1,...,M.\\ &\nabla\mathcal{G}_{k^{\prime}}(\bm{\theta}_{t})^{T}\bm{d}\leq \alpha,k^{\prime}\in\mathcal{I}_{\epsilon}(\bm{\theta}_{t}),\end{split}\] (A.7)

we have

\[\nabla\mathcal{G}_{k^{\prime}}(\theta_{t})=(\bm{u}_{k^{\prime}}-\bm{u}_{k})^{T }\nabla\bar{\mathcal{V}}(\bm{\theta}_{t})=\sum_{m=1}^{M}(\bm{u}_{k^{\prime}m} -\bm{u}_{km})\nabla\bar{\mathcal{V}}_{m}(\bm{\theta}_{t}).\] (A.8)

Base on KKT conditions, we have

\[d_{t}=-(\sum_{m=1}^{M}\lambda_{m}\nabla\bar{\mathcal{V}}_{m}(\bm{\theta}_{t})+ \sum_{k^{\prime}\in I_{\epsilon}(\bm{\theta})}\beta_{k^{\prime}}\nabla\mathcal{ G}_{k^{\prime}}(\bm{\theta}_{t})),\sum_{m=1}^{M}\lambda_{m}+\sum_{k^{\prime}\in I_{ \epsilon}(\bm{\theta})}\beta_{k^{\prime}}=1,\] (A.9)

where \(\lambda_{m}\leq 0\) and \(\beta_{k^{\prime}}\leq 0\) are the Lagrange multipliers. Then, the dual problem is given as

\[\begin{split}\max_{\lambda_{m},\beta_{k^{\prime}}}-\frac{1}{2}|| \sum_{m=1}^{M}\lambda_{m}\nabla\bar{\mathcal{V}}_{m}(\bm{\theta}_{t})+\sum_{k^ {\prime}\in I_{\epsilon}(\bm{\theta})}\beta_{k^{\prime}}\nabla\mathcal{G}_{k^{ \prime}}(\bm{\theta}_{t})||^{2}\\ s.t.&\sum_{m=1}^{M}\lambda_{m}+\sum_{k^{\prime}\in I_{ \epsilon}(\bm{\theta})}\beta_{k^{\prime}}=1,\lambda_{m}\geq 0,\beta_{k^{\prime}}\geq 0, \forall m=1,...,M,\forall\ k^{\prime}\in\mathcal{I}_{\epsilon}(\bm{\theta}). \end{split}\] (A.10)

Substituting Eq.(A.8) into Eq.A.9, we have 

[MISSING_PAGE_FAIL:18]

\begin{table}
\begin{tabular}{c|c|c} \hline \hline  & HDP & JOBS \\ PreferenceVector & -\(\varepsilon\) & -\(\varepsilon\) \\ \hline (1.00, 0.00) & 0.822 & 0.877 \\ (0.98, 0.17) & 0.824 & 0.868 \\ (0.94, 0.34) & 0.823 & 0.852 \\ (0.86, 0.50) & 0.820 & 0.841 \\ (0.77, 0.64) & 0.813 & 0.806 \\ (0.64, 0.76) & 0.724 & 0.798 \\ (0.50, 0.86) & 0.524 & 0.703 \\ (0.34, 0.94) & 0.512 & 0.694 \\ (0.17, 0.98) & 0.523 & 0.667 \\ (0.00, 1.00) & 0.523 & 0.666 \\ \hline \hline \end{tabular}
\end{table}
Table D3: The \(\varepsilon\) values corresponding to each preference vector in the two datasets IHDP and JOBS, where \(T=4\) and \(r=0.4\), which are derived according to Theorem 1.

[MISSING_PAGE_EMPTY:20]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See the abstract and the third and fourth paragraphs in Section 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See the conclusion (especially the last sentence.) Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: In Section 2.3, we provide a detailed discussion of the adopted assumptions. Additionally, we present the complete proofs in Appendices A and C. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In Section 4, we provide a detailed description for the experimental datasets. In addition, we provide the datasets and codes in supplemental material to ensure easy reproduction of all reported results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the supplemental material for datasets and codes in a zip file to ensure easy reproduction of all reported results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 4 for the detailed description of simulating outcome and experimental details. In addition, we provide the supplemental material for datasets and codes in a zip file to ensure easy reproduction of all reported results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the variance for all experimental results in section 4, by replicating each experiment 50 times. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: All experimental results can be easily reproduced on a personal computer. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: All experiments are conducted on publicly available datasets. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In the first paragraph of Section 1 (Introduction), we outline various potential applications of our work. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: All experiments are conducted on publicly available datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: In Section 4, we provide references for the datasets and the simulation setups of the data-generating process. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: In Section 4, we provide references for the datasets and the simulation setups of the data-generating process. In addition, we provide the supplemental material for datasets and codes in a zip file to ensure easy reproduction of all reported results. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We don't use a crowdsourcing service. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.