# TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs

 Zhuofeng Li\({}^{\star\triangle}\) Zixing Gou\({}^{\star\triangledown}\) Xiangnan Zhang\({}^{\diamondsuit}\) Zhongyuan Liu\({}^{\diamondsuit}\) Sirui Li\({}^{\dagger}\)

**Yuntong Hu\({}^{\dagger}\) Chen Ling\({}^{\dagger}\) Zheng Zhang\({}^{\dagger}\) Liang Zhao\({}^{\dagger}\)**

\({}^{\triangle}\)Shanghai University Shandong University Johns Hopkins University China

University of Petroleum (East China) Emory University

###### Abstract

Text-Attributed Graphs (TAGs) augment graph structures with natural language descriptions, facilitating detailed depictions of data and their interconnections across various real-world settings. However, existing TAG datasets predominantly feature textual information only at the nodes, with edges typically represented by mere binary or categorical attributes. This lack of rich textual edge annotations significantly limits the exploration of contextual relationships between entities, hindering deeper insights into graph-structured data. To address this gap, we introduce Textual-Edge Graphs Datasets and Benchmark (TEG-DB), a comprehensive and diverse collection of benchmark textual-edge datasets featuring rich textual descriptions on nodes and edges. The TEG-DB datasets are large-scale and encompass a wide range of domains, from citation networks to social networks. In addition, we conduct extensive benchmark experiments on TEG-DB to assess the extent to which current techniques, including pre-trained language models (PLMs), graph neural networks (GNNs), proposed novel entangled GNNs and their combinations, can utilize textual node and edge information. Our goal is to elicit advancements in textual-edge graph research, specifically in developing methodologies that exploit rich textual node and edge descriptions to enhance graph analysis and provide deeper insights into complex real-world networks. The entire TEG-DB project is publicly accessible as an open-source repository on Github, accessible at https://github.com/Zhuofeng-Li/TEG-Benchmark.

## 1 Introduction

Text-attributed graphs (TAGs) are graph structures in which nodes are equipped with rich textual information, allowing for deeper analysis and interpretation of complex relationships [50, 18, 16]. TAGs are widely utilized in a variety of real-world applications, including social networks [33, 32], citation networks [26], and recommendation systems [44, 15]. Due to the universal representational capabilities of language, TAGs have emerged as a promising format for potentially unifying a wide range of existing graph datasets. This field has recently garnered rapidly growing interest, particularly in the development of foundational models for graph data [24, 16, 46].

Unfortunately, a central issue in designing the TAG foundation model is the lack of comprehensive datasets with rich textual information on both nodes and edges. Most traditional graph datasets solely offer node attribute embeddings, devoid of the original textual sentences, which results in a significant loss of context and limits the application of advanced techniques such as large language models (LLMs) [25]. Despite some TAG datasets being present recently [46], their data usually only have text information on nodes where the edges are usually represented as binary or categorical. However, the textual information of edges in TAGs is crucial for elucidating the meaning of individual documents and their semantic correlations. For instance, as shown in Figure 1, this scientific article network illustrates the citation patterns of articles authored by Einstein and Planck in the field of quantum mechanics. When we need to conclude that 'Planck endorsed the probabilistic nature of quantum mechanics while Einstein opposed this view,' and if we consider it in terms of a TAG view, focusingsolely on the content of the papers authored by Einstein (Paper A) and Planck (Paper E), we would only conclude that both Einstein and Planck supported quantum mechanics. However, to further deduce that Einstein opposed studying quantum mechanics from a probabilistic perspective, it is necessary to adopt the Textual-Edge Graph (TEG) approach. This approach not only focuses on the paper contents but also pays greater attention to the citation information from the edge between Paper A and Book B, as well as the edge between Paper E and Book D. These edges provide essential citation context and reveal the relationships and influence between different scholarly works.

While compelling, TEGs face three significant challenges that make them an open problem. (1) _Comprehensive TEG datasets are absent._ Currently, there is a lack of comprehensive TEG datasets that simultaneously incorporate textual information from both nodes and edges, spanning multiple domains of varying sizes, and encompassing various mainstream graph learning tasks. This deficiency hinders the evaluation of TEG-based methods across diverse applications and domains. (2) _Existing experimental settings for TEG are disorganized._ Due to the inherent variety and complexity of TEGs, coupled with the absence of a standardized data format, existing works have adopted different datasets with different experimental settings [19, 18, 17, 53, 52, 23, 24]. This causes great difficulties in model comparisons in this field. (3) _Comprehensive benchmarks and analyses for TEG-based methods are missing._ While some techniques can accommodate edge features, they typically process binary or categorical data. It remains unclear if these methods can effectively utilize rich textual information on edges, particularly in leveraging complex interactions between graph nodes.

**Present work.** Recognizing all the above challenges, our research proposes the Textual-Edge Graphs Datasets and Benchmark (TEG-DB). TEG-DB is a pioneering initiative offering a diverse collection of benchmark graph datasets with rich textual descriptions on both nodes and edges. To address the issue of inadequate TEG datasets, our TEG datasets as shown in Table 1 cover an extensive array of domains, including Book Recommendation, E-commerce, Academic, and Social networks. Ranging in size from small to large, each dataset contains abundant raw text data associated with both nodes and edges, facilitating comprehensive analysis and modeling across various fields. Moreover, to address the inconsistency in experimental settings and the lack of comprehensive analyses for TEG-based methods, we first represent the TEG dataset in a unified format, then conduct extensive benchmark experiments and perform a comprehensive analysis. These experiments are designed to evaluate the capabilities of current computational techniques, such as pre-trained language models (PLMs), graph neural networks (GNNs) and proposed novel entangled GNNs, as well as their integrations. Our contributions are summarized below:

* To the best of our knowledge, TEG-DB is the first open dataset and benchmark specifically designed for textual-edge graphs. We provide 9 comprehensive TEG datasets encompassing 4 diverse domains as shown in Table 1. Each dataset, varying in size from small to large, contains abundant raw text data associated with both nodes and edges. Our TEG datasets aim to bridge the gap of TEG dataset scarcity and provide a rich resource for advancing research in the TEG domain.
* We develop a standardized pipeline for TEG research, encompassing crucial stages such as data preprocessing, data loading, and model evaluation. With this framework, researchers can seamlessly replicate experiments, validate findings, and iterate on existing approaches with greater efficiency

Figure 1: An example of textual-edge graph about scientific article network in quantum theory: two papers are connected by citation links. Considering edge texts in the TEG enhances semantic understanding and improves text analysis.

and confidence. Additionally, this standardized pipeline facilitates collaboration and knowledge sharing within the TEG community, fostering innovation and advancement in the field.
* We conduct extensive benchmark experiments and perform a comprehensive analysis of TEG-based methods, delving deep into various aspects such as the impact of different models and embeddings generated by PLMs of various scales, the consequence of diverse embedding methods in GNNs including separate and entangled embeddings, the effect of edge text and the influence of different domain datasets. By addressing key challenges and highlighting promising opportunities, our research stimulates and guides future directions for TEG exploration and development.

## 2 Related Works

In this section, we will begin by providing a brief introduction to three commonly used learning paradigms for TAGs. Following this, we will delve into the comparisons between the current graph learning benchmarks and our proposed benchmark.

**PLM-based methods.** PLM-based methods leverage the power of PLM to enhance the text modeling within each node due to their pre-training on a vast corpus. The early works on modeling textual attributes were based on shallow networks, e.g., Skip-Gram [30] and GloVe [34]. In recent years, Large Language Models (LLM) have become trending tools. Models like Llama [38], PaLM [2], and GPT [1] show their strong comprehension and inferring ability in cross-field natural language based tasks like code generation [3], legal consulting [6], make creative arts [22], as well as understanding and learning from Graphs [5]. One of the key applications of pre-trained language models is text representation, in which low-dimensional embeddings capture the underlying semantics of texts. On the TAGs, the PLMs use the local textual information of each node to learn a good representation for the downstream task.

**GNN-based methods**. The rapid advancements in graph representation learning within machine learning have led to numerous studies addressing various tasks, such as node classification [21] and link prediction [51]. Graph neural networks (GNNs) are acknowledged as robust tools for modeling graph data. These methods, including GCN [21], GAT [39], GraphSAGE [10], GIN [45], and RevGAT [31], develop effective message-passing mechanisms that facilitate information aggregation between nodes, thereby enhancing graph representations. GNNs typically utilize the "cascade architecture" advocated by GraphSAGE for textual graph representation, wherein node features are initially encoded independently using text modeling tools (e.g., PLMs) and then aggregated by GNNs to generate the final representation.

**LLM as Predictor.** In recent years, several recent studies [47; 4; 9] have delved into the potential of Large Language Models (LLMs) in analyzing graph-structured data. However, there is a lack of comprehensive research on the ability of LLMs to effectively identify and utilize key topological structures across various prompt scenarios, task complexities, and datasets. Chen et al. [4] and Guo et al. [9] proposed using LLMs on graph data but primarily focused on node classification within specific citation network datasets, limiting the exploration of LLMs' performance across various tasks and datasets. Furthermore, Ye et al. [47] fine-tuned LLMs on a specific dataset to outperform GNNs, focusing on a different research goal, which emphasizes LLMs' inherent ability to understand and leverage graph structures.

**Benchmarks for text-attributed graphs.** Current benchmarks in text-attributed graph representation learning can be divided into two stages. The first stage benchmark includes datasets such as mag [42] and ogbn-arxiv [13], which feature limited textual information primarily associated with nodes. The second stage benchmark is represented by CS-TAG [46], which builds upon the first stage by providing richer node-level textual data. However, these datasets face limitations in exploring representation learning for textual-edge graphs. Specifically, they typically include text only on nodes, with edges often represented as binary or categorical, which restricts a comprehensive understanding of node semantic relationships. Additionally, they lack coverage across diverse domains and tasks, hindering the development of robust and generalizable models. Furthermore, the lack of uniformity in representation formats introduces inconsistencies and complexities in analysis and modeling. Thus, there is a clear need for the development of a comprehensive benchmark with textual information on both nodes and edges in a unified format.

## 3 Preliminaries

A Textual-Edge Graph (TEG) is a graph-structured data format in which both nodes and edges have free-form text descriptions. These textual annotations provide rich contextual information about the complex relationships between entities, enabling a more detailed and comprehensive representation of data relations than traditional graphs.

**Definition 1** (Textual-edge Graphs). Formally, a TEG can be represented as \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), which consists of a set of nodes \(\mathcal{V}\) and a set of edges \(\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}\). Each node \(v_{i}\in\mathcal{V}\) contains a textual description \(d_{i}\), and each edge \(e_{ij}\in\mathcal{E}\) also associates with its text description \(d_{ij}\) describing the relation between \(v_{i}\) and \(v_{j}\).

**Challenges.** Current research on TEGs faces three significant challenges: (1) The scarcity of large-scale, diverse TEG datasets; (2) Inconsistent experimental setups and methodologies in previous TEG research; and (3) The absence of standardized benchmarks and comprehensive analyses for evaluating TEG-based methods. These limitations impede the development of more effective and efficient approaches in this emerging field.

## 4 A Comprehensive Dataset and Benchmark of Textual-Edge Graphs

We begin by offering a brief overview of the TEG-DB in Section 4.1. Afterward, we provide a comprehensive overview of the TEG datasets in Section 4.2, detailing their composition and the preprocessing steps to represent them in a unified format. Finally, we discuss three main methods for handling TEGs: PLM-based, GNN-based paradigm, and LLM as Predictor methods in Section 4.3.

### Overview of TEG-DB

In order to overcome the constraints intrinsic to preceding studies, we propose the establishment of the Textual-Edge Graphs Datasets and Benchmark, referred to as TEG-DB. This framework functions as a standardized evaluation methodology for examining the effectiveness of representation learning approaches in the context of TEGs. To ensure the comprehensiveness and scalability of TEG datasets, TEG-DB collects and constructs a novel set of datasets covering diverse domains like book recommendation, e-commerce, academia, and social networks, varying in size from small to large. These datasets are suitable for various mainstream graph learning tasks such as node classification and link prediction. Table 1 compares previous datasets with our TEG datasets. To enhance usability, we unify the TEG data format and propose a modular pipeline with three main methods for handling TEGs. To further foster TEG model design, we extensively benchmark TEG-based methods and conduct a thorough analysis. Overall, TEG-DB provides a scalable, unified, modular, and regularly updated evaluation framework for assessing representation learning methods on textual graphs.

### Data Preparation and Construction

In order to construct the dataset with simultaneous satisfaction of both rich textual information on nodes and edges, nine datasets from diverse domains and different scales are chosen. Specifically, we collect four User-Book Review networks from Goodreads datasets [40; 41] in the Book Recommendation domain and two shopping networks from Amazon datasets [11; 12] in the E-commerce

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline Dataset & \multicolumn{2}{|c|}{Nodes} & Edges & Notes/Class & Graph Domain & Size & Notes/set & Edge-over & Node Classification & Link Prediction \\ \hline \multirow{8}{*}{Proves} & Textual Space Network [35] & 71.26 & 88.617 & 2 & Social Networks & Small & & & & & \\  & Web-Edge Prove [208] & 22.420 & 171.052 & 4 & Social Networks & Small & & & & & \\  & edge-aware [13] & 169.43 & 116.248 & 40 & Academic & Medium & & & & & \\  & [17] & 3,327 & 4.732 & 6 & Academic & Small & & & & & \\  & Proposed [37] & 19.17 & 4.438 & 3 & Academic & Small & & & & & \\  & Cites [28] & 2,206 & 5.429 & 7 & Academic & Small & & & & & \\  & Cites [43] & 1,106.799 & 5.1287 & - & Academic & Large & & & & & \\  & CodeRank[14] & 560.684 & 85.323 & 11 & Boost Recommendation & Large & & & & & \\  & Spa-Fuse[44] & 37.058 & 17.7330 & 13 & E-commerce & Medium & & & & & \\  & Spa-Fuse[44] & 43.562 & 59.9335 & 12 & E-commerce & Small & & & & & \\  & Back-Edge[44] & 44.535 & 134.528 & 24 & E-commerce & Small & & & & & \\  & edge-aware-TEG [45] & 169.343 & 116.243 & 40 & Academic & Medium & & & & & \\ \hline \multirow{8}{*}{Ours} & Goullet-Based & 580.97 & 20.3695 & 11 & Best Recommendation & Large & & & & & \\  & Goodreads-Cites & 221.600 & 222.252 & 11 & Best Recommendation & Large & & & & & \\  & Goodreads-Cites & 216.600 & 513.459 & 11 & Best Recommendation & Large & & & & & \\  & Goodreads-Cites & 216.600 & 513.459 & 11 & Best Recommendation & Large & & & & & \\  & Goodreads-Cites & 137.411 & 217.428 & 379 & Best Recommendation & Medium & & & & & \\  & Amazon-Aspe & 31,899 & 62.508 & 62 & E-commerce & Small & & & & & \\  & Koullet & 278.022 & 56.084 & 3 & Social Networks & Large & & & & & \\  & Twitter & 18,761 & 23.764 & 566 & Social Networks & Small & & & & & \\  & Cites & 169.340 & 116.243 & 40 & Academic & Large & & & & & \\ \hline \end{tabular}
\end{table}
Table 1: Comparison between our TEG-DB datasets and existing datasets on TAG.

domain. Two social networks from Reddit and Twitter [29]. One citation network from MAG [42] and The Semantic Scholar Open Data Platform [20] in the academic domain. The statistics of the datasets are shown in Table 1.

The creation of textual-edge graph datasets involves three main steps. Firstly, preprocessing the textual attributes within the original dataset, which includes tasks such as handling missing values, filtering out non-English statements, removing anomalous symbols, truncating excessive length and selecting the most relevant textual attributes as the raw text for nodes or edges. Secondly, constructing the TEG itself. The connectivity between nodes is derived from inherent relationships provided within the dataset, such as citation relationships between papers in citation networks. It is important to note that during graph construction, self-edges and isolated nodes are eliminated. Lastly, refining the constructed graph. It is noteworthy that our dataset encompasses all major tasks in graph representation learning: node classification and link prediction. Below are the specifics of each dataset:

**User-Book Review Network.** Four datasets within the realm of User-Book Review Networks, specifically labeled as Goodreads-History, Goodreads-Crime, Goodreads-Children, and Goodreads-Cosmics, were formulated. The Goodreads datasets are the main source. Nodes represent different types of books and reviewers, while edges indicate book reviews. Node labels are assigned based on the book categories. The descriptions of books are used as book node textual information while user information serves as the user node textual information and reviews of users are used as edges textual information. The corresponding tasks are to predict the categories of the books, which is formulated as a multi-label classification problem, and to predict whether there are connections between users and books. These comprehensive data help infer user preferences and identify similar tastes, enhancing online book recommendations, unlike existing datasets that often lack interaction texts.

**Shopping Networks.** Two datasets, Amazon-Apps and Amazon-Movie, are classified under Shopping Networks. The Amazon datasets are the primary source, encompassing item reviews and descriptions. Nodes represent different types of items and reviewers, while edges indicate item reviews. The descriptions of items are used as item node textual information, while user information serves as the user node textual information and reviews of users are used as edge textual information. The corresponding tasks are to predict the categories of the items, formulated as a multi-label classification problem, and to predict whether there are connections between users and items. These datasets have the potential to significantly enhance recommendation systems, providing richer data for more accurate suggestions and a personalized shopping experience.

**Citation Networks.** The raw data for the citation network is sourced from the MAG and The Semantic Scholar Open Data Platform. Nodes represent papers, and edges represent the citation relationship. The titles and abstracts of papers are used as node textual information, and citation information, such as the context and paragraphs in which papers are cited, is utilized as textual edge data. The corresponding task involves predicting the domain to which a paper belongs, formulated as a multi-class classification problem, and predicting whether there exists a citation relationship between papers. This dataset enhances academic network expressiveness, particularly benefiting tasks like node classification and link prediction in graph machine learning.

**Social Networks.** The Reddit dataset, sourced from Reddit and the Twitter dataset, derived from Twitter, represent two prominent social media platforms. Nodes represent users and topics. The edges indicate the post-relationship. The descriptions of topics are used as topic node textual information while user information serves as the user node textual information and post text in subreddits or tweets is used as edge textual information. The corresponding tasks are to predict the category of the topics, formulated as a multi-class classification problem, and to predict whether there are connections between users and topics. Utilizing these datasets enhances recommendation algorithm performance, providing more personalized and relevant suggestions, while also offering valuable insights into user interests and preferences for social network research and business decision-making.

### Adapting Existing Methods to Solve Problems in TEGs

**PLM-based Paradigm.** PLMs are trained on massive amounts of text data, allowing them to learn the semantic relationships between words, phrases, and sentences. This enables them to understand the meaning behind the text, not just on a superficial level, but also in terms of context and intent. So PLM-based methods leverage the power of PLM to enhance the text modeling within each node and edge, along with an extra multilayer perception (MLP) to integrate their textual information from TEG. The formulation of these methods is as follows:

\[\begin{split}\bm{h}_{u}^{(k+1)}&=\mathrm{MLP}_{\bm{\psi}}^{(k)} \left(\bm{h}_{u}^{(k)}\right)\\ \bm{h}_{u}^{(0)}&=\mathrm{PLM}(T_{u})+\sum_{v\in \mathcal{N}(u)}\mathrm{PLM}(T_{e_{v,u}})\end{split}\] (1)

where \(\bm{h}_{u}^{(k)}\) denotes the node representation of node \(u\) in layer \(k\) of Multilayer Perceptron (MLP). \(T_{u}\) and \(T_{e_{v,u}}\) represent the raw text on node \(u\) and edge \(e_{v,u}\), respectively. The initial feature vector \(\bm{h}_{u}^{(0)}\) of node \(u\) is derived by encoding the text on node \(u\) and its neighboring edges using the Pre-trained Language Model (PLM). \(\mathcal{N}\) denotes the set of neighbors. \(\psi\) refers to the trainable parameters within the MLP.

Although PLMs have considerably improved the representation of node text attributes, these models do not account for topological structures. This limitation hinders their ability to fully capture the complete topological information present in TEGs.

**Edge-aware GNN-based Paradigm.** GNNs are employed to propagate information across the graph, allowing for the extraction of meaningful representations via message passing, which are formally defined as follows:

\[\bm{h}_{u}^{(k+1)}=\mathrm{UPDATE}_{\bm{\omega}}^{(k)}\left(\bm{h}_{u}^{(k)}, \mathrm{AGGREGATE}_{\bm{\omega}}^{(k)}\left(\left\{\bm{h}_{v}^{(k)},\bm{e}_{ v,u},v\in\mathcal{N}(u)\right\}\right)\right)\] (2)

where \(\bm{h}_{u}^{(k)}\) denotes the node representation of node \(u\) in layer \(k\) of GNN and the initial node feature vector \(\bm{h}_{u}^{(0)}\) is obtained by embedding its raw text through PLMs. \(e_{v,u}\) denotes the edge from node \(v\) to node \(u\) and its features \(\bm{e}_{v,u}\) are likewise derived from PLMs based on its raw text embeddings. \(k\) represents the layers of GNNs, \(\mathcal{N}\) denotes the set of neighbors, \(u\) denotes the target node, \(\bm{\omega}\) means the learning parameters in GNNs.

However, this approach presents two primary issues: (1) Existing Graph ML methods like GNNs typically work on structured attributes on edges instead of texts [18]. In TEGs, edges are texts that contain rich semantic information, which is way beyond the typical focus of GNNs that are commonly based on connectivity (i.e., binary attribute denoting whether there is a connection or not) and edge attributes (i.e., categorical or numerical values on the edges). (2) GNN-based methods are limited in capturing the contextualized semantics of edge texts [46]. In TEGs, where edge and node texts are often entangled, converting them into separate node and edge embeddings during the embedding process can result in the loss of critical information about their interdependence, which diminishes the effectiveness of GNNs throughout the entire message-passing process.

**Entangled GNN-based Paradigm.** Traditional edge-aware GNN-based approaches that first learn edge text embeddings and then apply GNNs have limitations for TEG data because edge texts and node texts are often closely entangled. Separating them into distinct node and edge embeddings may impair important information regarding their interaction. For instance, in a citation graph where each node represents a paper, an edge might indicate that one paper cites, criticizes, or utilizes a specific part of another paper. Therefore, the edge does not represent the relationship between the entirety of the two nodes, posing a significant challenge for methods that rely on node or edge embeddings representing the entirety of a node or edge. To avoid information loss during the interaction between nodes and edges after text embedding, we propose an approach that first entangles the edge text and node text before performing the embedding. The embedding obtained in this way is then added to the message-passing operation for each pair of connected nodes. The formulation of these methods is as follows:

\[\begin{split}\bm{h}_{u}^{(k+1)}&=\mathrm{UPDATE}_{\bm {\omega}}^{(k)}\left(\bm{h}_{u}^{(k)},\mathrm{AGGREGATE}_{\bm{\omega}}^{(k)} \left(\left\{\bm{h}_{v}^{(k)},v\in\mathcal{N}(u)\right\}\right)\right)\\ \bm{h}_{u}^{0}&=\mathrm{PLM}(T_{u},\{T_{v},T_{e_{v,u}},v\in\mathcal{N}(u)\})\end{split}\] (3)

where \(\bm{h}_{u}^{(k)}\) denotes the node representation of node \(u\) in layer \(k\) of the GNN. \(T_{v}\), \(T_{u}\), and \(T_{e_{v,u}}\) represent the raw text on node \(v\), node \(u\), and the edge from \(v\) to \(u\), respectively. The initial node feature vector \(\bm{h}_{u}^{(0)}\) is obtained by embedding the entangled raw text of node \(u\) and its neighborhood 

[MISSING_PAGE_FAIL:7]

**Implementation details.** We conduct experiments on 3 PLM-based, 18 GNN-based, and 2 LLM-based methods. For PLM-based methods, the dimensions of node embedding are 3072, 1024, and 768 generated by GPT-3.5-TURBO, Bert-Large, and Bert respectively. We set the MLP hidden layer to 2, with the number of hidden units in each layer being one-fourth of the units in the previous layer. For GNN-based methods, we adhere to the settings outlined in the respective paper. The parameters shared by all GNN models include dimensions of node and edge embeddings, model layers, and hidden units, with respective values set to 3072, 1024, and 768, as generated by GPT-3.5-TURBO, Bert-Large, and Bert, and 2, 256, respectively. We utilize cross-entropy loss with the Adam optimizer to train and optimize all the above models. The batch size is 1024. Each experiment is repeated three times. See Appendix B.1 for more details.

**Evaluations metrics.** We investigate the performance of different baselines through two tasks: link prediction and node classification. For the link prediction task, we use the Area Under ROC Curve (AUC) metric and F1 score to evaluate the model performance. For node classification, the choice of evaluation metrics depends on the nature of the classification tasks involved. In the context of datasets encompassing Goodreads-Children, Goodreads-Crime, and comics from Goodreads, along with Amazon-Apps and Amazon-Movie datasets from Amazon, the classification tasks involve multi-label node classification. Hence, metrics such as AUC-micro and F1-micro are chosen for evaluation. Conversely, datasets about citation networks and social networks are characterized by multi-class node classification, thus metrics such as ACC and F1 are selected for assessment.

### Effectiveness Analysis for Link Prediction

In this subsection, we analyze the link prediction from the various models applied in the study. Table 2 and 3 represent the effect of link prediction on different datasets from various distinct models. The results on other datasets can be found in Appendix B.2. We can further draw several observations from Table 2 and 3. First, For PLM-based and GNN-based methods, the state-of-the-art methods for Goodreads-Children and Goodreads-Crime datasets are both GeneralConv. Under the condition of using the same embeddings, they outperform the worst method by approximately 5% and 7% in terms of AUC and F1 across these two datasets. For the Amazon-Apps and Amazon-Movie datasets, the state-of-the-art methods are EdgeGNN and GeneralConv. They outperform the worst method by approximately 3% and 7% in terms of AUC and F1 for Amazon-Apps, and by 8% and 7% in

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{6}{c}{Ghana} & \multicolumn{6}{c}{Came} & \multicolumn{6}{c}{Came} \\ \cline{2-17}  & \multicolumn{2}{c}{General-OPT} & GPT-3.5-TURBO & \multicolumn{2}{c}{Bert-Large} & \multicolumn{2}{c}{Bert} & \multicolumn{2}{c}{None} & \multicolumn{2}{c}{Entangled-OPT} & GPT-3.5-TURBO & \multicolumn{2}{c}{Bert-Large} & \multicolumn{2}{c}{Bert} & \multicolumn{2}{c}{None} \\ \cline{2-17}  & \multicolumn{2}{c}{AUC} & F1* & AUC & F1* & AUC & F1* & AUC & F1 & AUC & F1 & AUC & F1 & AUC & F1 & AUC & F1 & AUC & F1 \\ \hline MLP & 0.8708 & 0.8708 & 0.8708 & 0.8708 & 0.8708 & 0.8708 & 0.8709 & 0.8622 & 0.8823 & 0.8845 & 0.8709 & 0.8818 & 0.8719 & 0.8719 & 0.8718 & 0.8702 & 0.8719 & 0.8702 & 0.8703 & 0.8703 \\ \hline GraphAtt & **0.9960** & **0.9961** & **0.9961** & **0.9961** & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9962 & 0.9963 \\ GraphAtt & **0.9960** & **0.9961** & **0.9961** & **0.9961** & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9962 & 0.9963 \\ GraphAtt & 0.9960 & 0.9960 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9962 & 0.9963 & 0.9963 & 0.9963 & 0.9964 & 0.9963 & 0.9964 & 0.9965 & 0.9966 & 0.9966 \\ GraphAtt & 0.9960 & 0.9960 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9962 & 0.9960 & 0.9961 & 0.9962 & 0.9963 & 0.9963 & 0.9964 & 0.9962 & 0.9964 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Node Classification ACC, Micro-AUC, Micro-F1 and F1 among PLM-based, GNN-based methods. AUC* and F1* represent Micro-AUC and Micro-F1 respectively. The best method on each dataset is BLM embedding on each dataset is shown in bold.

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{6}{c}{Goodreads-Children} & \multicolumn{6}{c}{Goodreads-Crime} & \multicolumn{6}{c}{Amazon-Apps} & \multicolumn{6}{c}{Amazon-Movie} & \multicolumn{6}{c}{Cination} \\ \cline{2-17}  & AUC & F1* & AUC* & F1* & AUC* & F1* & AUC* & F1 & AUC* & F1 & AUC & F1 & AUC & F1 & AUC & F1 & AUC & F1 & AUC & F1 \\ \hline GPT-3.5-TURBO & 0.5200 & 0.0300 & 0.5400 & 0.7000 & **0.5000** & **0.0100** & 0.5199 & 0.0918 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961 & 0.9961terms of AUC and F1 for Amazon-Movie, respectively. For the Citation and Twitter datasets, the state-of-the-art method is GraphTransformer. It outperforms the worst method by approximately 20% and 30% in terms of AUC and F1 for Citation, and by 12% and 9% in terms of AUC and F1 for Twitter, respectively. Second, Entangled-GPT methods, which entangle edge text and node text first before encoding with GPT consistently outperform the approach of directly encoding the text through GPT, yielding about 2% improvement in both AUC and F1 metrics across all datasets on the link prediction tasks. Third, For the LLM as Predictor methods, we find that they do not perform well in predicting links. The best method among them has an AUC and F1 gap of approximately 10% - 30% compared to the best PLM-based and GNN-based methods for all datasets. Fourth, using edge text provides at least approximately a 3% improvement in AUC and at least approximately an 8% improvement in F1 compared to not using edge text for all datasets.

### Effectiveness Analysis for Node Classification

In this subsection, we analyze the node classification results from various models. Table 4 and 5 display the impact on different datasets from various distinct, with additional results in Appendix B.3. We can derive some insights from the data. First, for PLM-based and GNN-based methods, the state-of-the-art models for Goodreads-Children and Goodreads-Crime are GraphsSAGE and GeneralConv, respectively, outperforming the worst method by approximately 8% and 20% in AUC-micro and F1-micro for Goodreads-Children, and by 4% and 15% for Goodreads-Crime. In the E-commerce domain, GraphSAGE is the top method for Amazon-Apps and Amazon-Movie, outperforming the worst method by about 10% and 6% in AUC-micro and F1-micro for Amazon-Apps, and by 1% and 10% for Amazon-Movie. GINE and EdgeConv also show superior performance, exceeding the worst method by approximately 35% and 40% in ACC and F1 for Citation, and by 5% and 12% for Twitter. Second, Entangled-GPT methods outperform the approach of directly encoding the text through GPT, yielding about 2% improvement in both AUC and F1 metrics across all datasets on the node classification tasks. Third, LLM as Predictor methods perform poorly in node classification, with the best method showing an AUC-micro gap of about 30% compared to the best PLM-based and GNN-based methods. Their low F1-micro score could be due to the large number of predicted categories. Third, incorporating edge text results in at least a 3% improvement in AUC-micro and a 6% improvement in F1-micro across all datasets, compared to not using edge text.

**Observation.**_(1) The state-of-the-art model varies across different datasets._ Data variability and complexity play significant roles in influencing model performance. _(2) Edge text is crucial for TEG tasks._ Including edge text enriches relationship information, enabling a more precise depiction of interactions and relationships between nodes, which enhances overall model performance. _(3) Encoding text in an entangled manner is more beneficial for avoiding information loss._ The advantage of this method over existing approaches is its ability to effectively preserve the semantic relationships between nodes and edges, making it more suitable for capturing complex relationships. _(4) The scale of PLMs significantly impacts the performance of TEG tasks, especially on datasets with rich text on nodes and edges._ Larger model scales result in higher-quality text embeddings and better semantic understanding, leading to improved model performance. _(5) When using LLMs as predictors, they struggle to fully comprehend graph topology information._ LLMs are designed for linear sequence data and do not inherently capture the complex relationships and structures present in graph data, leading to lower performance on TEGs link prediction and node classification.

### Parameter Sensitivity Analysis

We further analyze the impact of text embeddings generated from PLMs. For the link prediction task, as shown in Table 2, using small-scale PLMs like BERT improves the AUC and F1 scores by approximately 5% compared to not using text embeddings. Medium-scale models such as BERT-Large and large-scale models like GPT-3.5-TURBO improve the AUC and F1 scores by about 7% across all datasets. For node classification, as shown in Table 4, the improvement is slightly less pronounced. Small-scale PLMs like BERT improve the AUC-micro and F1-micro scores by approximately 3%, while medium-scale models like BERT-Large and large-scale models like GPT-3.5-TURBO improve these scores by about 3.5% across all datasets.

## 6 Discussion

Textual-Edge graphs have emerged as a prominent graph format, which finds extensive applications in modeling real-world tasks. Our research focuses on comprehensively understanding the textual attributes of nodes and their topological connections. Furthermore, we believe that exploring strategies to enhance the efficiency of LLMs in processing TEGs is deemed meaningful. Despite the proven effectiveness of LLMs, their operational efficiency, especially in managing TEGs, poses a significant challenge. Notably, employing APIs like GPT4 for extensive graph tasks may result in considerable expenses under current billing models. Additionally, deploying open-source large models such as LLaMa for tasks like parameter updates or inference in local environments demands substantial computational resources and storage capacity. Please refer to the Appendix C for more details.

## 7 Conclusion

We introduce the inaugural TEG benchmark, TEG-DB, tailored to delve into graph representation learning on TEGs. It incorporates textual content on both nodes and edges compared to traditional TAG with only node information. We gather and furnish nine comprehensive textual-edge datasets to foster collaboration between the NLP and GNN communities in exploring the data collectively. Our benchmark offers a thorough assessment of various learning approaches, affirming their efficacy and constraints. Additionally, we plan to persist in uncovering and building more research-oriented TEGs to further propel the ongoing robust growth of the domain.

## References

* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.
* [3] Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang, Tao Ge, Chenfei Wu, Wang You, Ting Song, Yan Xia, et al. Low-code llm: Visual programming over llms. _arXiv preprint arXiv:2304.08103_, 2, 2023.
* [4] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential of large language models (llms) in learning on graphs. _arXiv preprint arXiv:2307.03393_, 2023.
* [5] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential of large language models (llms) in learning on graphs. _ACM SIGKDD Explorations Newsletter_, 25(2):42-61, 2024.
* [6] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language model with integrated external knowledge bases. _arXiv preprint arXiv:2306.16092_, 2023.
* [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [8] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In _ICLR Workshop on Representation Learning on Graphs and Manifolds_, 2019.
* [9] Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. _arXiv preprint arXiv:2305.15066_, 2023.
* [10] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 1024-1034, 2017.
* [11] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In _proceedings of the 25th international conference on world wide web_, pages 507-517, 2016.
* [12] Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley. Bridging language and items for retrieval and recommendation. _arXiv preprint arXiv:2403.03952_, 2024.
* [13] Weihua Hu, Matthias Fey, Hongyu Ren, Takuya Nakata, Yingtao Dong, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In _Advances in Neural Information Processing Systems_, pages 22118-22133, 2020.
* [14] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In _International Conference on Learning Representations_, 2020.
* [15] Zan Huang, Wingyan Chung, and Hsinchun Chen. A graph model for e-commerce recommender systems. _Journal of the American Society for information science and technology_, 55(3):259-274, 2004.
* [16] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. Large language models on graphs: A comprehensive survey. _arXiv preprint arXiv:2312.02783_, 2023.

* [17] Bowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Xinyang Zhang, Qi Zhu, and Jiawei Han. Patton: Language model pretraining on text-rich networks. _arXiv preprint arXiv:2305.12268_, 2023.
* [18] Bowen Jin, Yu Zhang, Yu Meng, and Jiawei Han. Edgeformers: Graph-empowered transformers for representation learning on textual-edge networks. In _International Conference on Learning Representations_, 2023.
* [19] Bowen Jin, Yu Zhang, Qi Zhu, and Jiawei Han. Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1020-1031, 2023.
* [20] Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler C. Murray, Christopher Newell, Smita R Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, A. Tanaka, Alex D Wade, Linda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine van Zuylen, and Daniel S. Weld. The semantic scholar open data platform. _ArXiv_, abs/2301.10140, 2023.
* [21] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations (ICLR)_, 2017.
* [22] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models, 2023.
* [23] Haoyu Kuang, Jiarong Xu, Haozhe Zhang, Zuyu Zhao, Qi Zhang, Xuan-Jing Huang, and Zhongyu Wei. Unleashing the power of language models in text-attributed graph. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 8429-8441, 2023.
* [24] Chen Ling, Zhuofeng Li, Yuntong Hu, Zheng Zhang, Zhongyuan Liu, Shuang Zheng, and Liang Zhao. Link prediction on textual edge graphs. _arXiv preprint arXiv:2405.16606_, 2024.
* [25] Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Tianjiao Zhao, et al. Domain specialization as the key to make large language models disruptive: A comprehensive survey. _arXiv preprint arXiv:2305.18703_, 2305, 2023.
* [26] Xiaozhong Liu, Jinsong Zhang, and Chun Guo. Full-text citation analysis: A new method to enhance scholarly networks. _Journal of the American Society for Information Science and Technology_, 64(9):1852-1863, 2013.
* [27] Yifan Liu, Chenchen Kuai, Haoxuan Ma, Xishun Liao, Brian Yueshuai He, and Jiaqi Ma. Semantic trajectory data mining with llm-informed poi classification, 2024.
* [28] Andrew McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. In _Information Retrieval_, pages 127-163. Springer, 2000.
* [29] Andrew J McMinn, Yashar Moshfeghi, and Joemon M Jose. Building a large-scale corpus for evaluating event detection on twitter. In _Proceedings of the 22nd ACM international conference on Information & Knowledge Management_, pages 409-418, 2013.
* [30] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2013.
* [31] Yixin Li Ming Chen. Revisiting graph neural networks for link prediction. _International Conference on Machine Learning (ICML)_, 2020.

* [32] Seth A Myers, Aneesh Sharma, Pankaj Gupta, and Jimmy Lin. Information network or social network? the structure of the twitter follow graph. In _Proceedings of the 23rd international conference on world wide web_, pages 493-498, 2014.
* [33] Dmitry Paranyushkin. Infranodus: Generating insight using text network analysis. In _The world wide web conference_, pages 3584-3589, 2019.
* [34] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1532-1543. Association for Computational Linguistics, 2014.
* [35] Benedek Rozemberczki, Ryan Davies, Rik Sarkar, and Charles Sutton. Gemsec: Graph embedding with self clustering. _Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining_, pages 65-72, 2019.
* [36] Benedek Rozemberczki, Otilia Kiss, and Rik Sarkar. Multi-scale attributed node embedding. _Journal of Complex Networks_, 8(3):cnz037, 2020.
* [37] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad. Collective classification in network data. _AI magazine_, 29(3):93-93, 2008.
* [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [39] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations (ICLR)_, 2018.
* [40] Mengting Wan and Julian McAuley. Item recommendation on monotonic behavior chains. In _Proceedings of the 12th ACM conference on recommender systems_, pages 86-94, 2018.
* [41] Mengting Wan, Rishabh Misra, Ndapa Nakashole, and Julian McAuley. Fine-grained spoiler detection from large-scale review corpora. _arXiv preprint arXiv:1905.13416_, 2019.
* [42] Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not enough. _Quantitative Science Studies_, 1(1):396-413, 2020.
* [43] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2319-2328, 2019.
* [44] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in recommender systems: a survey. _ACM Computing Surveys_, 55(5):1-37, 2022.
* [45] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations (ICLR)_, 2019.
* [46] Hao Yan, Chaozhuo Li, Ruosong Long, Chao Yan, Jianan Zhao, Wenwen Zhuang, Jun Yin, Peiyan Zhang, Weihao Han, Hao Sun, Weiwei Deng, Qi Zhang, Lichao Sun, Xing Xie, and Senzhang Wang. A comprehensive study on text-attributed graphs: Benchmarking and rethinking. In _Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* [47] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Natural language is all a graph needs. _arXiv preprint arXiv:2308.07134_, 2023.
* [48] Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. _Advances in Neural Information Processing Systems_, 33:17009-17021, 2020.
* [49] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. In _Advances in Neural Information Processing Systems_, pages 11960-11970, 2019.

* [50] Delvin Ce Zhang, Menglin Yang, Rex Ying, and Hady W Lauw. Text-attributed graph representation learning: Methods, applications, and challenges. In _Companion Proceedings of the ACM on Web Conference 2024_, pages 1298-1301, 2024.
* [51] M. Zhang and Y. Chen. Link prediction based on graph neural networks. In _Advances in Neural Information Processing Systems_, 2018.
* [52] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learning on large-scale text-attributed graphs via variational inference. _arXiv preprint arXiv:2210.14709_, 2022.
* [53] Tao Zou, Le Yu, Yifei Huang, Leilei Sun, and Bowen Du. Pretraining language models with text-attributed heterogeneous graphs. _arXiv preprint arXiv:2310.12580_, 2023.

### Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section A.2.
* Did you include the license to the code and datasets? [No] The code and the data are proprietary.
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? \(\text{Answer:[Yes]}\) Justification: See Section abstract and introduction. 2. Did you describe the limitations of your work? \(\text{Answer:[Yes]}\) See Appendix D 3. Did you discuss any potential negative societal impacts of your work? \(\text{Answer:[N/A]}\) Justification: Our paper has no potential negative social impacts. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? \(\text{Answer:[Yes]}\) Justification: Our paper strongly conforms to the ethics review guidelines.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? \(\text{Answer:[N/A]}\) Justification: No results requiring assumptions. 2. Did you include complete proofs of all theoretical results? \(\text{Answer:[N/A]}\) Justification: No theoretical results requiring proof.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? \(\text{Answer:[Yes]}\) Justification: We include a URL in the abstract. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? \(\text{Answer:[Yes]}\) Justification: See Section 5.1. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? \(\text{Answer:[Yes]}\) Justification: See Appendix 5. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? \(\text{Answer:[Yes]}\) Justification: See Appendix 5.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

1. If your work uses existing assets, did you cite the creators? Answer:[Yes] 2. Did you mention the license of the assets? Answer:[Yes] 3. Did you include any new assets either in the supplemental material or as a URL? Answer:[Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? Answer:[Yes] Justification: See Section 4.2. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? Answer:[N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? Answer:[N/A] Justification: This work does not involve crowdsourcing or research with human subjects. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? Answer:[N/A] Justification: This work does not involve crowdsourcing or research with human subjects. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? Answer:[N/A] Justification: This work does not involve crowdsourcing or research with human subjects.

Datasets

### Dataset format

For each dataset, all unprocessed raw files are represented in.json format. After preprocessing, we store the graph-type data compatible with PyTorch Geometric (PyG) [8] in the.pt format using PyTorch. Specifically, we have retained the raw text on nodes, the labels on nodes, the raw text on edges, and the adjacency matrix. We uniformly store the text embeddings of node and edge text in.npy files and load them during data processing.

### Datasets license

The datasets are subject to the MIT license. For precise license information, please refer to the corresponding GitHub repository.

## Appendix B Experiment

### Implementation Details

GNNs are mainly derived from the implementation in the PyG library [8]. For the node classification task, numerical node labels corresponding to the nodes within the graph are necessary. This involves converting the categorical node categories found in the original data into numerical node labels within the graph. For the link prediction, we randomly sample node pairs that do not exist in the graph as negative samples, along with some edges present as positive samples. For LLM-based predictor methods, we focus on node classification and link prediction tasks. For node classification, inspired by the recent LLM-based classification algorithm [27], we use GPT-4 and GPT-3.5-TURBO models to predict the classification of text nodes by providing the probability for each class. We randomly select 1,000 text nodes along with all classification labels for this task. For the link prediction task, we also apply the GPT-4 and GPT-3.5-TURBO models to determine whether two text edges are related, providing an answer with the corresponding probability. For this task, we randomly select 1,000 pairs of positive text edge indices from the graph and an equal number of negative edges.

### Effectiveness Analysis for Link Prediction

In this subsection, we further analyze the link prediction from the various models applied in the study. Table 6 and 7 represent the effect of link prediction on different datasets from various distinct. We can further draw several observations from Table 6 and 7. First, For PLM-based and GNN-based methods, the state-of-the-art methods for Goodreads-Comics and Goodreads-History datasets are GeneralConv and GINE, respectively. Under the condition of using the same embeddings, they outperform the worst method by approximately 6% and 7% in terms of AUC and F1 across these two datasets. For the Reddit dataset, the state-of-the-art method is GeneralConv. It outperforms the worst method by approximately 3% and 5% in terms of AUC and F1, respectively. Second, for the LLM as a predictor method, we find that they do not perform well in predicting links. The best method among them has an AUC and F1 gap of approximately 10% - 30% compared to the best PLM-based and GNN-based methods for all datasets. Third, Using edge text provides at least approximately a 3% improvement in AUC and at least approximately an 8% improvement in F1 compared to not using edge text for all datasets.

### Effectiveness Analysis for Node Classification

In this subsection, we further analyze the node classification results from various models. Table 8 and 9 display the impact on different datasets. We can derive some insights. First, for PLM-based and GNN-based methods, the state-of-the-art models for Goodreads-Comics and Goodreads-History are GeneralConv and GINE, respectively, outperforming the worst method by approximately 8% and 15% in AUC-micro and F1-micro for Goodreads-Comics, and by 6% and 9% for Goodreads-History. GraphTransformer outperforms the worst method by approximately 2% and 1% in ACC and F1 for Citation. Second, LLM as Predictor methods perform poorly in node classification, with the best method showing an AUC-micro gap of about 20% compared to the best PLM-based and GNN-based methods. Their low F1-micro score could be due to the large number of predicted categories. Third, incorporating edge text results in at least a 3% improvement in AUC-micro and a 6% improvement in F1-micro across almost all datasets, compared to not using edge text.

[MISSING_PAGE_FAIL:18]

Discussion

Notably, employing APIs like GPT4 for extensive graph tasks may result in considerable expenses under current billing models. Additionally, deploying open-source large models such as LLaMa for tasks like parameter updates or inference in local environments demands substantial computational resources and storage capacity. Consequently, enhancing the efficiency of LLMs for graph-related tasks remains a critical concern. Moreover, the constraints imposed by context windows in LLMs also impact their effectiveness in encoding node and edge text within TEGs.

## Appendix D Limitation

Comprehensive evaluation of tasks often demands significant computational resources, which can be a burden for researchers and smaller organizations.