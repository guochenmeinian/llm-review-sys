# Generative Fractional Diffusion Models

 Gabriel Nobis

Fraunhofer HHI

&Maximilian Springenberg

Fraunhofer HHI

&Marco Aversa

Dotphoton

&Michael Detzel

Fraunhofer HHI

&Rembert Daems

Ghent University

FlandersMake-MIRO

&Roderick Murray-Smith

University of Glasgow

&Shinichi Nakajima

BIFOLD, TU Berlin

RIKEN AIP

Sebastian Lapuschkin

Fraunhofer HHI

&Stefano Ermon

Stanford University

&Tolga Birdal

Imperial College London

&Manfred Opper

TU Berlin

University of Potsdam

University of Birmingham

&Christoph Knochenhauer

Technical University of Munich

&Luis Oala

Dotphoton

&Wojciech Samek

Fraunhofer HHI

TU Berlin

BIFOLD

###### Abstract

We introduce the first continuous-time score-based generative model that leverages fractional diffusion processes for its underlying dynamics. Although diffusion models have excelled at capturing data distributions, they still suffer from various limitations such as slow convergence, mode-collapse on imbalanced data, and lack of diversity. These issues are partially linked to the use of light-tailed Brownian motion (BM) with independent increments. In this paper, we replace BM with an approximation of its non-Markovian counterpart, fractional Brownian motion (fBM), characterized by correlated increments and Hurst index \(H\in(0,1)\), where \(H=0.5\) recovers the classical BM. To ensure tractable inference and learning, we employ a recently popularized Markov approximation of fBM (MA-fBM) and derive its reverse-time model, resulting in _generative fractional diffusion models_ (GFDM). We characterize the forward dynamics using a continuous reparameterization trick and propose _augmented score matching_ to efficiently learn the score function, which is partly known in closed form, at minimal added cost. The ability to drive our diffusion model via MA-fBM offers flexibility and control. \(H\leq 0.5\) enters the regime of _rough paths_ whereas \(H>0.5\) regularizes diffusion paths and invokes long-term memory. The Markov approximation allows added control by varying the number of Markov processes linearly combined to approximate fBM. Our evaluations on real image datasets demonstrate that GFDM achieves greater pixel-wise diversity and enhanced image quality, as indicated by a lower FID, offering a promising alternative to traditional diffusion models1.

Footnote 1: The implementation of our framework is available at [https://github.com/GabrielNobis/gfdm](https://github.com/GabrielNobis/gfdm).

## 1 Introduction

Recent years have witnessed a remarkable leap in generative diffusion models [1, 2, 3], celebrated for their ability to accurately learn data distributions and generate high-fidelity samples. These models have made significant impact across a wide spectrum of application domains, including the generation of complex molecular structures [4] for material [5] or drug discovery [6], realisticaudio samples [7, 8], 3D objects [9, 10] or textures [11], medical images [12], aerospace applications [13], and DNA sequence design [14, 15]. Despite these successes, modern score-based generative models formulated in continuous-time [16] face limitations due to their reliance on a simplistic driving noise, the Brownian motion (BM) [17, 18, 19]. As a light-tailed process, using BM can results in slow convergence rates and susceptibility to mode-collapse, especially with imbalanced data [20]. Additionally, its purely Markovian nature may also make it hard to capture the full complexity and richness of real-world data. All these attracted a number of attempts for involving different noise types [20, 21]. In this paper, we propose leveraging fractional noises, particularly the renowned non-Markovian fractional BM (fBM) [22, 23] to drive diffusion models. fBM extends BM to stationary increments with a more complex dependence structure, _i.e._, long-range dependence vs. roughness/regularity controlled by a Hurst index, a measure of "mild" or "wild" randomness [24]. This all comes at the expense of computational challenges and intractability of inference, mostly stemming from its non-Markovian nature. Moreover, deriving a reverse-time model poses theoretical challenges, as fBM is not only non-Markovian but also not a semimartingale [25]. To overcome these limitations, we leverage recent works in Markov approximations of fBM (MA-fBM) [26, 27] and establish a framework for training continuous-time score-based generative models using an approximate fractional diffusion process, as well as generating samples from the corresponding tractable reverse process. Notably, our method maintains the same number of score model evaluations during both training and data generation, with only a minimal increase in computational load. Our contributions are:

* We derive the time-reversal of forward dynamics driven by a Markovian approximation of fractional Brownian motion in a way that the dimensionality of the unknown part of the score function matches that of the data.
* We derive an explicit formulae for the marginals of the conditional forward process via a continuous reparameterization trick.
* We introduce a novel augmented score matching loss for learning the score function in our generative fractional diffusion model, which can be minimized by a score model of data-dimension.

Our experimental evaluation validates our contributions, demonstrating the gains of correlated-noise with long-term memory, approximated by a combination of a number of Markov processes, where the amount of processes further control the diverstiy.

**Differentiation from existing work.** Yoon et al. [20] generalizes score-based generative models from an underlying BM to a driving Levy process, a stochastic process with independent and stationary increments. A driving noise with correlated increments is not included in the framework of Yoon et al. [20]. Conceptually, every Levy process is a semimartingale [28]. Since fBM is not a Levy process, it is not included in the framework of Yoon et al. [20]. The closest work to ours is Tong et al. [29] constructing a neural-SDE based on correlated noise and using the neural SDE as a forward process of a score-based generative model. Our framework with exact reverse-time model is based on the integral representation of fBM derived in Harms and Stefanovits [26] and the optimal approximation coefficients of Daems et al. [27], while the fractional noise in [29] is sparsely approximated by a linear combination of independent standard normal random variables without exact reverse-time

Figure 1: **Each data dimension transitions to a known prior distribution through a forward process that approximates a fractional diffusion process.** The Hurst index \(H\) on the LHS interpolates between the roughness of a Brownian driven SDE and the underlying integration in PF ODEs. The driving noise process is a linear combination of the correlated processes on the RHS, all driven by the same Brownian motion. The score function of these augmenting processes is available in closed form and serves as guidance for the unknown score function.

model. Moreover, the framework of Tong et al. [29] is limited to \(H>\frac{1}{3}\) and only compatible with the Euler-Maruyama sample schema [30] while our framework is up to numerical stability applicable for any \(H\in(0,1)\) and compatible with any suitable SDE or ODE solver. To the best of our knowledge, we are the first to build a framework for continuous-time score-based generative models that includes driving noise processes converging to non-Markovian processes with infinite quadratic variation.

## 2 Background

Modeling the distribution transforming process of a score-based generative model through stochastic differential equations (SDEs) [16] offers a unifying framework to generate data from an unknown probability distribution. Instead of injecting a finite number of fixed noise scales via a Markov chain, infinitely many noise scales tailored to the continuous dynamics of the Markov process \(\mathbf{X}=(\mathbf{X}_{t})_{t\in[0,T]}\) are utilized during the distribution transformation, offering considerable practical advantages over discrete time diffusion models [16]. The forward dynamics, transitioning from a data sample \(\mathbf{X}_{0}\sim p_{0}\) to a tractable noise sample \(\mathbf{X}_{T}\sim p_{T}\) are specified by a continuous drift function \(\mathbf{f}\) and a continuous diffusion coefficient \(g\). These dynamics define a diffusion process that solves the SDE

\[\mathrm{d}\mathbf{X}_{t}=\mathbf{f}(\mathbf{X}_{t},t)\mathrm{d}t+g(t) \mathrm{d}\mathbf{B}_{t},\quad\mathbf{X}_{0}\sim p_{0} \tag{1}\]

driven by a multivariate BM \(\mathbf{B}\). To sample data from noise, a reverse-time model is needed that defines the backward transformation from the tractable noise distribution to the data distribution. Whenever \(\mathbf{X}=(\mathbf{X}_{t})_{t\in[0,T]}\) is a stochastic process and \(g\) is a function on \([0,T]\), we write \(\overline{\mathbf{X}}_{t}=\mathbf{X}_{T-t}\) for the reverse-time model and \(\bar{g}(t)=g(T-t)\) for the reverse-time function. The marginal density of the stochastic process \(\mathbf{X}\) at time \(t\) is denoted by \(p_{t}\) throughout this work2. Remarkably, an exact reverse-time model to the forward model in eq. (1) is given by the backward dynamics [31; 32; 33]

Footnote 2: See Appendix I for the notational conventions of this work.

\[\mathrm{d}\overline{\mathbf{X}}_{t}=\left[\overline{\mathbf{f}}(\overline{ \mathbf{X}}_{t},t)-\bar{g}^{2}(t)\nabla_{\mathbf{x}}\log\bar{p}_{t}(\overline{ \mathbf{X}}_{t})\right]\mathrm{d}t+\bar{g}(t)\mathrm{d}\overline{\mathbf{B}}_{ t},\quad\overline{\mathbf{X}}_{0}=\mathbf{X}_{T}\sim p_{T}, \tag{2}\]

where the only unknown is the score function \(\nabla_{\mathbf{x}}\log p_{t}\), inheriting the intractability from the unknown initial distribution \(p_{0}\). In addition to the stochastic dynamics, the reverse-time model provides deterministic backward dynamics via an ordinary differential equation (ODE) by the so called probability flow ODE (PF ODE) [16]

\[\mathrm{d}\overline{\mathbf{x}}_{t}=\left[\overline{\mathbf{f}}(\overline{ \mathbf{x}}_{t},t)-\frac{1}{2}\bar{g}^{2}(t)\nabla_{\mathbf{x}}\log\bar{p}_{t }(\overline{\mathbf{x}}_{t},t)\right]\mathrm{d}t,\quad\mathbf{x}_{T}\sim p_{T}. \tag{3}\]

Stochasticity is only injected into the system through the random initialization \(\mathbf{x}_{T}\sim p_{T}\), implying a deterministic and bijective map from noise to data [16]. Conditioning the forward process on a data sample \(\mathbf{x}_{0}\sim p_{0}\) results for linear \(\mathbf{f}(\cdot,t)\) in a tractable Gaussian forward process with conditional score function \(\nabla_{\mathbf{x}}\log p_{0t}(\mathbf{x}|\mathbf{x}_{0})\) in closed form. To approximate the exact reverse-time model, this tractable score function is used to train a time-dependent score model \(S_{\mathbf{\theta}}\) via score matching [34; 35]. Upon training, any solver for SDEs or ODEs can be utilized to generate data from noise by simulating the stochastic or deterministic backward dynamics of the reverse-time model with \(S_{\mathbf{\theta}}\approx\nabla_{\mathbf{x}}\log p\).

**Simulation error of the reverse-time model**. The two main sources of error when simulating the reverse-time model are the approximation error due to \(S_{\mathbf{\theta}}\) only approximating \(\nabla_{\mathbf{x}}\log p\), and the discretization error, which arises from transitioning from continuous-time to discrete steps. Simulating the PF ODE with the Euler method over \(N\in\mathbb{N}\) equidistant time steps results in a global error of order \(N^{-1}\)[36]. In contrast, the expected global error for simulating the SDE using the Euler-Maruyama method is of a lower order \(N^{-\frac{1}{2}}\), indicating a larger error for the same number of steps [30; 36]. From this perspective it is reasonable that sampling from the PF ODE requires fewer steps. Yet, the source of qualitative differences between sampling from the ODE and the SDE [16] remains unclear.

**A pathwise perspective on sampling**. The roughness of a path can be measured by its Holder exponent \(0<\delta\leq 1\)[37]. For example, BM as the integrator in the backward dynamics eq. (2) has \(\delta\)-Holder continuous paths for any \(0<\delta<\frac{1}{2}\), whereas the integrator \(t\mapsto t\) of the PF ODE eq. (3) can be regarded as a Holder continuous path with exponent \(\delta=1\). Therefore, from a pathwise perspective, we move away from a rough path when we sample using the PF ODE. An unexplored topic in score-based generative models is the interpolation between the SDE and the PF ODE in termsof the Holder exponent. It remains to be examined whether there is, to some extent, an optimal degree of Holder continuity in between, or if an even rougher path with \(\delta\ll\frac{1}{2}\) could yield an advantageous data generator.

The process that naturally arises from this line of thought is fBM [22, 23] with Hurst index \(H\in(0,1)\), where almost all paths are Holder continuous for any exponent \(\delta<H\), controlled by \(H\). In terms of roughness, the Hurst index interpolates between the paths of Brownian driven SDEs and those of the underlying integration in PF ODEs, while also offering the potential for even rougher paths. Motivated by these observations, we define a novel score-based generative model with underlying dynamics that approximate a fractional diffusion process.

## 3 Fractional driving noise

Before describing the challenges in defining a score-based generative model with control over the roughness of the distribution transforming path, we introduce fBM. The literature distinguishes between "Type I" fBM and "Type II" fBM [38] having stationary and non-stationary increments, respectively. The type II fBM, also called Riemann-Liouville fBM, possesses smaller deviations from its mean, potentially an advantageous property for a driving noise of a score-based generative model, since large deviations of the sampling process to the data mean can lead to sample artifacts [39]. Here and in the experiments we focus on type II fBM. However, our theoretical framework generalizes to both types as detailed in Appendix A. The empirical study of a score-based generative model approximating a fractional diffusion process driven by type I fBM is dedicated to future work. We begin with the definition of Riemann-Liouville fBM [22], a generalization of BM permitting correlated increments.

**Definition 3.1** (Type II Fractional Brownian Motion [22]).: _Let \(B=(B_{t})_{t\geq 0}\) be a standard Brownian Motion (BM) and \(\Gamma\) the Gamma function. The centered Gaussian process_

\[B_{t}^{H}=\frac{1}{\Gamma(H+\frac{1}{2})}\int_{0}^{t}(t-s)^{H-\frac{1}{2}} \mathrm{d}B_{s},\quad t\geq 0, \tag{4}\]

_uniquely characterized in law by its covariances_

\[\mathbb{E}\left[B_{t}^{H}B_{s}^{H}\right]=\frac{1}{\Gamma^{2}(H+\frac{1}{2})} \int_{0}^{\min\{t,s\}}((t-u)(s-u))^{H-\frac{1}{2}}\mathrm{d}u,\quad t,s\in[0,\infty) \tag{5}\]

_is called type II fractional Brownian motion (fBM) with Hurst index \(H\in(0,1)\)._

BM being the unique continuous and centered Gaussian process with covariance \(\min\{t,s\}\) is recovered for \(H=0.5\), since \(\Gamma(1)=1\). In comparison to the purely Brownian setting with independent increments (diffusion), the path of \(B^{H}\) becomes more smooth for \(H>0.5\) due to positively correlated increments (super-diffusion) and more rough for \(H<0.5\) due to negatively correlated increments (sub-diffusion). These three regimes are reflected in the Holder exponent of \(\delta<H\) for almost all paths.

**Generalization challenges**. The most challenging part in defining a score-based generative model driven by fBM is the derivation of a reverse-time model. Due to its covariance structure, fBM is not a Markov process [40] and the shift in the roughness of the sample path leads to changes in its quadratic variation: from \(t\) in the purely Brownian (diffusion) regime to zero in the smooth regime, and to infinite in the rough regime [30]. For that reason fBM is neither a Markov process nor a semimartingale [25] for all \(H\neq 0.5\). Hence, we cannot make use of the Markov property or the Kolmogorov equations (Fokker-Planck) that are used to derive the reverse-time model of Brownian driven SDEs [31, 32, 33]. See Appendix H for a more illustrative view of the problem. The existence of a reverse-time model can be proven in the smooth regime of fBM [41]. However, due to the absence of an explicit score function in Darses and Saussereau [41] it does not provide a sufficient structure to train a score-based generative model.

To overcome this difficulty we follow [26, 27] and define the driving noise of our generative model by a linear combination of Markovian semimartingales. The approximation is based on the exact infinite-dimensional Markovian representation of fBM given in Theorem A.2.

**Definition 3.2** (Markov approximation of fBM [26, 27]).: _Choose \(K\in\mathbb{N}\) Ornstein-Uhlenbeck (OU) processes_

\[Y_{t}^{k}=\int_{0}^{t}e^{-\gamma_{k}(t-s)}\mathrm{d}B_{s},\quad k\in\mathbb{N },\quad t\geq 0, \tag{6}\]_with speeds of mean reversion \(\gamma_{1},...,\gamma_{K}\) and dynamics \(\mathrm{d}Y_{t}^{k}=-\gamma_{k}Y_{t}^{k}\mathrm{d}t+\mathrm{d}B_{t}\). Given a Hurst index \(H\in(0,1)\) and a geometrically spaced grid \(\gamma_{k}=r^{k-n}\) with \(r>1\) and \(n=\frac{K+1}{2}\) we call the process_

\[\hat{B}_{t}^{H}:=\sum_{k=1}^{K}\omega_{k}Y_{t}^{k},\quad H\in(0,1),\quad t\geq 0, \tag{7}\]

_Markov-approximate fractional Brownian motion (MA-fBM) with approximation coefficients \(\omega_{1},...,\omega_{K}\in\mathbb{R}\) and denote by \(\mathbf{\hat{B}}^{H}=(\hat{B}_{1}^{H},...,\hat{B}_{D}^{H})\) the corresponding \(D\)-dimensional process where \(\hat{B}_{i}^{H}\) and \(\hat{B}_{j}^{H}\) are independent for \(i\neq j\) inheriting independence from the underlying standard BMs \(B_{i}\) and \(B_{j}\)._

Our framework is conceptually independent of the specific choice of spatial grid and approximation coefficients. To achieve strong convergence rates with a high polynomial order in \(K\) for \(H<0.5\) in the driving noise to fBM, one may follow the approach outlined in Harms [42]. Consequently, our framework includes driving noise processes that converge to non-Markovian processes with infinite quadratic variation. For computational efficiency, we instead follow the approach of Daems et al. [27] to choose the \(L^{2}(\mathbb{P})\) optimal approximation coefficients for a given \(K\), achieving empirically good results in approximating fBM, even with a small number of OU processes.

**Proposition 3.3** (Optimal Approximation Coefficients [27]).: _The optimal approximation coefficients \(\boldsymbol{\omega}=(\omega_{1},...,\omega_{K})\in\mathbb{R}^{K}\) for a given Hurst index \(H\in(0,1)\), a terminal time \(T>0\) and a fixed geometrically spaced grid to minimize the \(L^{2}(\mathbb{P})\)-error_

\[\mathcal{E}(\boldsymbol{\omega}):=\int_{0}^{T}\mathbb{E}\left[\left(B_{t}^{H}- \hat{B}_{t}^{H}\right)^{2}\right]\mathrm{d}t \tag{8}\]

_are given by the closed-form expression \(\boldsymbol{A}\boldsymbol{\omega}=\boldsymbol{b}\) with_

\[\boldsymbol{A}_{i,j}:=\frac{2T+\frac{\mathrm{e}^{-(\gamma_{i}+\gamma_{j})T}-1} {\gamma_{i}+\gamma_{j}}}{\gamma_{i}+\gamma_{j}},\quad\boldsymbol{b}_{k}:=\frac {T}{\gamma_{k}^{H+\frac{1}{2}}}P\left(H+\frac{1}{2},\gamma_{k}T\right)-\frac{ H+\frac{1}{2}}{\gamma_{k}^{H+\frac{3}{2}}}P\left(H+\frac{3}{2},\gamma_{k}T\right) \tag{9}\]

_and where \(P(z,x)=\frac{1}{\Gamma(z)}\int_{0}^{x}t^{z-1}e^{-t}\mathrm{d}t\) is the regularized lower incomplete gamma function._

MA-fBM serves as the driving noise of our generative model, replacing BM in the distribution transforming process solving eq.1, approximating a fractional diffusion process. See Figure1 for an illustration of the underlying processes.

## 4 A score-based generative model based on fractional noise

In this section, we define a continuous-time score-based generative model driven by MA-fBM. A detailed treatment of the theory can be found in AppendixA. We begin with the forward dynamics, transitioning data to noise.

**Definition 4.1** (Forward process).: _Let \(\mathbf{\hat{B}}^{H}\) be a \(D\)-dimensional MA-fBM with Hurst index \(H\in(0,1)\). For continuous functions \(\mu:[0,T]\rightarrow\mathbb{R}\) and \(g:[0,T]\rightarrow\mathbb{R}\) we define the forward process \(\mathbf{X}=(\mathbf{X}_{t})_{t\in[0,T]}\) of a generative fractional diffusion model (GFDM) by_

\[\mathrm{d}\mathbf{X}_{t}=\mu(t)\mathbf{X}_{t}\mathrm{d}t+g(t)\mathrm{d}\mathbf{ \hat{B}}_{t}^{H},\quad\mathbf{X}_{0}=\mathbf{x}_{0}\sim p_{0},\quad t\in[0,T], \tag{10}\]

_where \(p_{0}\) is the unknown data distribution from which we aim to sample from._

Considering both the forward process as well as the OU processes defining the driving noise \(\mathbf{\hat{B}}^{H}\), we have for every data dimension an augmented vector of correlated processes (\(X,Y^{1},\ldots,Y^{K}\)), driven by the same BM, approximating the time-correlated behavior of a one-dimensional fractional diffusion process [27]. We denote the stacked process of the \(D\) augmented vectors as \(\mathbf{Z}=(\mathbf{Z}_{t})_{t\in[0,T]}\) and refer to the resulting \(D(K+1)\)-dimensional process as the _augmented forward process_. Rewriting the dynamics of the forward process we observe that the augmented forward process \(\mathbf{Z}\) solves a linear SDE. Hence, \(\mathbf{Z}|\mathbf{x}_{0}\), the augmented forward process conditioned on a data sample \(\mathbf{x}_{0}\sim p_{0}\), is a linear transformation of BM. Thus \(\mathbf{Z}|\mathbf{x}_{0}\) is a Gaussian process and so is \(\mathbf{X}|\mathbf{x}_{0}\)[43]. For each dimension \(1\leq d\leq D\), we have a system of \(K+1\) trajectories that transform \(\mathbf{x}_{0,d}\) according to the augmented forward process with \(D=1\), following the dynamics

\[\mathrm{d}\mathbf{Z}_{t}=\mathbf{F}(t)\mathbf{Z}_{t}\mathrm{d}t+\mathbf{G}(t) \mathrm{d}B_{t}, \tag{11}\]where all \(K+1\) processes are driven by the same one-dimensional BM \(B\) with matrix valued functions \(\mathbf{F}\) and \(\mathbf{G}\) defined in Appendix A.2. To efficiently sample for every \(t\in(0,T]\) from the conditional augmented forward distribution during training, we characterize its marginal statistics.

**Derivation of marginal statistics**. The marginal mean \(\mathbb{E}[\mathbf{X}_{t}|\mathbf{x}_{0}]=\mathbf{x}_{0}\exp(\int_{0}^{t}\mu(s) \mathrm{d}s)\) of the conditional forward process is unaffected by changing the driving noise to MA-fBM, and the mean of the augmenting OU processes is zero. See Appendix A.2 for a detailed derivation of the marginal statistics of the augmenting processes. The missing components in the marginal covariance matrix \(\mathbf{\Sigma}_{t}\) of the conditional augmented forward process \(\mathbf{Z}|\mathbf{x}_{0}\) are the marginal variance of the forward process and the marginal correlation between the conditional forward process and the augmenting processes. We derive by reparameteriziation an explicit formula for the marginal variance of the conditional forward process. This generalizes the formula for the perturbation kernel \(p_{0t}(\mathbf{x}|\mathbf{x}_{0})=\mathcal{N}(\mathbf{x};c(t)\mathbf{x}_{0},c^ {2}(t)\sigma^{2}(t)\mathbf{I}_{D})\) given in Karras et al. [44] to a driving MA-fBM and is reminiscent of the reparameterization trick used in discrete time.

**Proposition 4.2** (Continuous Reparameterization Trick).: _The forward process \(\mathbf{X}\) of GFDM conditioned on \(\mathbf{x}_{0}\in\mathbb{R}^{D}\) admits the continuous reparameterization_

\[\mathbf{X}_{t}=c(t)\left(\mathbf{x}_{0}+\int_{0}^{t}\alpha(t,s)\mathrm{d} \mathbf{B}_{s}\right)\sim\mathcal{N}(c(t)\mathbf{x}_{0},c^{2}(t)\sigma^{2}(t) \mathbf{I}_{D}) \tag{12}\]

_with \(c(t)=\exp\left(\int_{0}^{t}\mu(s)\mathrm{d}s\right)\) and \(\sigma^{2}(t)=\int_{0}^{t}\alpha^{2}(t,s)\mathrm{d}s\) where \(\alpha\) is given by_

\[\alpha(t,s)=\sum_{k=1}^{K}\omega_{k}\left[\frac{g(s)}{c(s)}-\gamma_{k}\int_{s} ^{t}f_{k}(u,s)\mathrm{d}u\right],\quad f_{k}(u,s)=\frac{g(u)}{c(u)}e^{-\gamma_ {k}(u-s)}. \tag{13}\]

Sketch of Proof.: Reparameterization of the forward dynamics in eq.10 and the Stochastic Fubini Theorem yields the Gaussian process \(\mathbf{X}_{t}=c(t)(\mathbf{x}_{0}+\int_{0}^{t}\alpha(t,s)\mathrm{d}\mathbf{B} _{s})\) with variance \(\mathbb{V}\left[\mathbf{X}_{t}\right]=c^{2}(t)\int_{0}^{t}\alpha^{2}(t,s) \mathrm{d}s\) by Ito isometry. See Theorem A.3 for the full proof. 

By the above definition of \(\alpha\), we retrieve the perturbation kernel of the purely Brownian setting given in Karras et al. [44, Equation 12] for \(K=1\), \(\gamma_{1}=0\) and \(\omega_{1}=1\). When, depending on the choice of forward dynamics, \(\int_{0}^{t}\alpha(t,s)\mathrm{d}s\) is not accessible in closed form, \(\mathbf{\Sigma}_{t}\) can be described by an ODE and solved numerically as described in Appendix B. Thus our method admits any choice of forward dynamics in terms of \(\mu\) and \(g\).

**Explicit fractional forward dynamics**. Although our framework is not bound to any specific dynamics, this work's empirical evaluation focuses on _Fractional Variance Exploding_ (FVE) dynamics given by

\[\mathrm{d}\mathbf{X}_{t}=\sigma_{min}\left(\frac{\sigma_{max}}{\sigma_{min}} \right)^{t}\sqrt{2\log\frac{\sigma_{max}}{\sigma_{min}}}\mathrm{d}\mathbf{ \hat{B}}_{t}^{H},\quad t\in[0,T] \tag{14}\]

with \((\sigma_{min},\sigma_{max})=(0.01,50)\) and _Fractional Variance Preserving_ (FVP) dynamics given by

\[\mathrm{d}\mathbf{X}_{t}=-\frac{1}{2}\beta(t)\mathbf{X}_{t}\mathrm{d}t+\sqrt {\beta(t)}\mathrm{d}\mathbf{\hat{B}}_{t}^{H},\quad t\in[0,T] \tag{15}\]

with \(\beta(t)=\beta(t)=\bar{\beta}_{\min}+t\left(\bar{\beta}_{max}-\bar{\beta}_{ min}\right)\) and \((\bar{\beta}_{min},\bar{\beta}_{max})=(0.1,20)\)[16]. Leveraging the continuous reparameterization trick we derive in Appendix B the conditional marginal covariance matrix of FVE in closed form. To the best of our knowledge, the integral in eq.13, needed to compute \(\alpha\) in the setting of FVP dynamics, is not accessible in closed form. Therefore, we use a numerical ODE solver to estimate this quantity for FVP dynamics. See Appendix B for details on the computation of the marginal variances and an illustration of the resulting variance schedules.

**The reverse-time model**. We observe that the augmented forward dynamics of GFDM are already encompassed in the general framework presented in Song et al. [16, Appendix A], although they differ from the Variance Exploding (VE), Variance Preserving (VP), and sub-VP dynamics discussed therein. To simplify notation, we use \(p_{t}\) here to denote the marginal density of both \(\mathbf{Z}_{t}\) and \(\mathbf{X}_{t}\). The specific density referred to will be clear from the context. By the significant results of [31, 32, 33], the reverse-time model of GFDM is given by the backward dynamics

\[\mathrm{d}\overline{\mathbf{Z}}_{t}=\left[\overline{\mathbf{F}}(t)\overline{ \mathbf{Z}}_{t}-\overline{\mathbf{G}}(t)\overline{\mathbf{G}}(t)^{T}\nabla_{ \mathbf{z}}\log\overline{p}_{t}(\overline{\mathbf{Z}}_{t})\right]\mathrm{d}t+ \overline{\mathbf{G}}(t)\mathrm{d}\overline{\mathbf{B}}_{t},\quad t\in[0,T]. \tag{16}\]However, a direct application of [16] would require to train a score model with input and output dimension of \(D(K+1)\). By proposing _augmented score matching_ below, we show that learning a score model with input and output dimension \(D\) is sufficient, enabling the use of the same highly curated model architecture as in traditional diffusion models to approximate the score function.

**Augmented score matching**. We condition the score function \(\nabla_{\mathbf{z}}\log p_{t}\) on a data sample \(\mathbf{x}_{0}\sim p_{0}\)_and additionally_ on the states of the stacked vector \(\mathbf{Y}_{t}^{[K]}:=(\mathbf{Y}_{t}^{1},...,\mathbf{Y}_{t}^{K})\) of augmenting processes. To train our time-dependent score model \(s_{\mathbf{\theta}}\) we propose the _augmented score matching loss_

\[\mathcal{L}(\mathbf{\theta}):=\mathbb{E}_{t}\left\{\mathbb{E}_{(\mathbf{X}_{0}, \mathbf{Y}_{t}^{[K]})}\mathbb{E}_{(\mathbf{X}_{t}|\mathbf{Y}_{t}^{[K]}, \mathbf{X}_{0})}\left[\|s_{\mathbf{\theta}}(\mathbf{X}_{t}-\sum_{k}\eta_{t}^{k} \mathbf{Y}_{t}^{k},t)-\nabla_{\mathbf{x}}\log p_{0t}(\mathbf{X}_{t}|\mathbf{Y }_{t}^{[K]},\mathbf{X}_{0})\|_{2}^{2}\right]\right\}. \tag{17}\]

The weights \(\eta_{t}^{1},...,\eta_{t}^{K}\) arise from conditioning \(\mathbf{Z}_{t}|\mathbf{x}_{0}\) on \(\mathbf{Y}_{t}^{[K]}\) and the time points \(t\) are uniformly sampled from \(\mathcal{U}[0,T]\). We show in the following that the optimal \(s_{\mathbf{\theta}}\) w.r.t. the _augmented score matching loss_ is the \(L^{2}\)-optimal approximation of the score function of our reverse-time model.

**Proposition 4.3** (Optimal Score Model).: _Assume that \(s_{\mathbf{\theta}}\) is optimal w.r.t. the augmented score matching loss \(\mathcal{L}\). The score model_

\[S_{\theta}(\mathbf{Z}_{t},t):=\left(s_{\mathbf{\theta}}(\mathbf{X}_{t}-\sum_{k} \eta_{t}^{k}\mathbf{Y}_{t}^{k},t),-\eta_{t}^{1}s_{\mathbf{\theta}}(\mathbf{X}_{t}- \sum_{k}\eta_{t}^{k}\mathbf{Y}_{t}^{k},t),...,-\eta_{t}^{K}s_{\mathbf{\theta}}( \mathbf{X}_{t}-\sum_{k}\eta\mathbf{Y}_{t}^{k},t)\right) \tag{18}\]

_yields the optimal \(L^{2}(\mathbb{P})\) approximation of \(\nabla_{\mathbf{z}}\log p_{t}(\mathbf{Z}_{t})\) via_

\[S_{\mathbf{\theta}}(\mathbf{Z}_{t},t)+\nabla_{\mathbf{z}}\log q_{t}(\mathbf{Y}_{t} ^{[K]})\approx\nabla_{\mathbf{z}}\log p_{t}(\mathbf{Z}_{t}). \tag{19}\]

Sketch of Proof.: Using the relation \(\nabla_{\mathbf{x}}\log p_{0t}=-\eta_{t}^{k}\nabla_{\mathbf{y}^{k}}\log p_{0t}\) and the independence of \(\mathbf{X}_{0}\) and \(\mathbf{Y}_{t}^{[K]}\) yields the claim. See Appendix A.3 for the full proof. 

In addition to the result that a score model of data dimension \(D\) minimizes the proposed _augmented score matching loss_, Proposition 4.3 also implies that GFDM requires the same number of score model evaluations during sampling from the reverse-time model as traditional diffusion models. This is because, for a given time point \(t\), we only need to evaluate \(s_{\mathbf{\theta}}(\cdot,t)\) once at \(\mathbf{X}_{t}-\sum_{k}\eta_{t}^{k}\mathbf{Y}_{t}^{k}\) to compute \(S_{\mathbf{\theta}}(\mathbf{Z}_{t},t)\) according to eq.18, and \(S_{\mathbf{\theta}}\) is all that is required to approximate the reverse-time dynamics described below. We provide a thorough quantitative evaluation of compute time in seconds for GFDM in Appendix F, validating the theoretical reasoning in this section that GFDM incur only minimal additional computational cost.

**Sampling from reverse-time model**. Once we trained our score model \(S_{\mathbf{\theta}}\) via augmented score matching, we simulate the reverse-time model backward in time and sample from the reverse-time model via the SDE

\[\mathrm{d}\overline{\mathbf{Z}}_{t}=\left\{\overline{\mathbf{F}}(t)\overline{ \mathbf{Z}}_{t}-\overline{\mathbf{G}}(t)\overline{\mathbf{G}}(t)^{T}\left[ \overline{S}_{\mathbf{\theta}}(\overline{\mathbf{Z}}_{t},t)+\nabla_{\mathbf{z}} \log\overline{q}_{t}(\overline{\mathbf{Y}}_{t}^{[K]})\right]\right\}\mathrm{d} t+\overline{\mathbf{G}}(t)\mathrm{d}\overline{\mathbf{B}}_{t},\quad t\in[0,T] \tag{20}\]

or the corresponding augmented PF ODE [16]

\[\mathrm{d}\overline{\mathbf{z}}_{t}=\left\{\overline{\mathbf{F}}(t)\overline {\mathbf{z}}_{t}-\frac{1}{2}\overline{\mathbf{G}}(t)\overline{\mathbf{G}}(t)^{ T}\left[\overline{S}_{\mathbf{\theta}}(\overline{\mathbf{z}}_{t},t)+\nabla_{ \mathbf{z}}\log\overline{q}_{t}(\overline{\mathbf{y}}_{t}^{[K]})\right] \right\}\mathrm{d}t,\quad t\in[0,T], \tag{21}\]

where we initialize in both cases the reverse dynamics with the centered (non-isotropic) Gaussian \(\overline{\mathbf{Z}}_{0}\) with covariance matrix \(\mathbf{\Sigma}_{T}\). To traverse backward from noise to data, we may deploy any suitable SDE or ODE solver. In both cases, for each data dimension, we have \(K+1\) trajectories that transform the Gaussian initialization into an approximate sample of the data distribution. The PF ODE enables in addition negative log-likelihoods (NLLs) estimation of test data under the learned density [16]. See Appendix G for the computation details of NLLs.

**Remark 4.4**.: _We showed in this section that it suffices to approximate a \(D\)-dimensional score to reverse the \(D(K+1)\)-dimensional MA-fBM driven SDE with unknown starting distribution. Since this holds for any fixed \(K\in\mathbb{N}\) an interesting task is to examine the behaviour of the reverse-time model as \(K\rightarrow\infty\) and potentially link it to the dynamics of a reverse-time model of true fBM. To the best of our knowledge, existence of such a reverse-time model is not known for \(H<0.5\) and the drift of the reverse-time model for \(H>0.5\) lacks sufficient structure to train a score-based generative model [41]._

## 5 Experiments

We conduct experiments on MNIST and CIFAR10 to evaluate the ability of GFDM to generate real images. First, we measure the quality and the pixel-wise diversity of the generated images across different numbers of augmenting processes and various Hurst indices, showing that the super-diffusive regime with \(H>0.5\) yields better performance compared to the purely Brownian driven dynamics. Second, we further evaluate the best performing models in terms of class-wise image quality and class-wise distribution coverage. We measure image quality by the Frechet Inception Distance (FID) [45] and the Inception score (IS) [46], pixel-wise diversity by the pixel Vendi Score (VS\({}_{p}\)) [47] and class-wise distribution coverage by improved recall (Recall) [48]. See Appendix D for the implementation details and additional experimental results. We begin with the empirical evaluation of how the augmenting processes affect performance on MNIST.

**Effect of augmentation on MNIST.** To isolate the effect of the augmenting processes on MNIST while minimally adapting the driving noise distribution, we fix \(H=0.5\) so that the weighted sum of the augmenting processes approximates BM, rather than fBM. We observe an increase of the pixel-wise diversity VS\({}_{p}\) for both FVE and FVP dynamics, with increasing \(K\). In Table 1 we can observe that VS\({}_{p}\) increases from \(24.20\) to \(24.54\) for FVE dynamics and from \(23.64\) to \(24.56\) for FVP dynamics. The enhanced pixel-wise diversity on MNIST comes at the cost of a reduced likelihood of test data under the learned density, indicated by a higher NLLs for more augmenting processes.

**Quality results across different Hurst indices**. On both, MNIST and CIFAR10, we obtain the best performance in terms of FID and VS\({}_{p}\) in the super-diffusive regime with \(H=0.9\) and FVP dynamics. On MNIST we achieve state of the art FID of \(0.72\), compared to an FID of \(1.44\) with the purely Brownian VP dynamics (Table 1(a)). Comparing FVP to the best-performing purely BM driven VP dynamics, we observe not only an improvement in quality but also an increase in pixel-wise diversity from \(23.64\) to \(24.18\), as measured by VS\({}_{p}\). In Table 1(b) we observe the same behaviour on CIFAR10. The best performing configuration in terms of FID and pixel-wise diversity is achieved for FVP(\(H=0.9,K=2\)) with an FID of \(3.77\) instead of \(4.85\) and an VS\({}_{p}\) of \(3.60\) instead of \(3.28\). Additionally, in Figure 7, we show the FID evolution of the super-diffusive regime for various numbers of augmenting processes, showing a similar pattern that either that \(K=2\) or

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{**SVE(\(H=0.5\))**} & \multicolumn{1}{c}{FID \(\downarrow\)} & \multicolumn{1}{c}{NLs \(\text{Iss}\uparrow\)} & \multicolumn{1}{c}{VS\({}_{p}\) \(\uparrow\)} & \multicolumn{1}{c}{**FVP(\(H=0.5\))**} & \multicolumn{1}{c}{FID \(\downarrow\)} & \multicolumn{1}{c}{NLs \(\text{Iss}\uparrow\)} & \multicolumn{1}{c}{VS\({}_{p}\) \(\uparrow\)} \\ \hline \hline
**V** (_breathing_) & 10.82 & 2.73 & 24.20 & VP(trained) & **1.44** & **2.38** & 23.64 & \(K=1\) & 2.81 & 3.90 & 23.69 & \\ \(K=2\) & 9.59 & 3.03 & 24.15 & \(K=2\) & 2.52 & 4.57 & 23.63 & \(K=3\) & 3.51 & 7.02 & 23.78 & \\ \(K=3\) & **9.74** & 2.93 & 24.42 & \(K=3\) & 3.51 & 7.02 & 23.78 & \(K=4\) & 1.25 & 3.10 & 7.02 & 23.70 & \\ \(K=4\) & 11.25 & 3.10 & **24.54** & \(K=4\) & 1.86 & 5.71 & 24.50 & \(K=5\) & 25.51 & 3.94 & \(K=5\) & 4.89 & 7.09 & **24.56** \\ \hline \hline
**MNIST** & \(H=0.9\) & \(H=0.7\) & \(H=0.5\) & \(H=0.1\) & **CIFAR10** & \(H=0.9\) & \(H=0.7\) & \(H=0.5\) & \(H=0.1\) & \\ \hline
**fBM driven** & & & & & & & & & & & & \\ \hline
**V** (_breathing_) & - & - & - & - & 10.82 & 24.20 & - & - & - & - & - & - & - \\
**V** (_breathing_) & - & - & - & - & 1.44 & 25.64 & - & - & - & - & - & - & - \\ \hline
**M-fBM driven** & & & & & & & & & & & & & \\ \hline
**FVV**(_IR_ + 1) & 2.86 & 23.56 & 20.1 & 23.78 & 2.81 & 23.69 & 2.92 & 23.50 & \(\mathbf{FV}(H,K=1)\) & **4.79** & **3.53** & 4.96 & \(\mathbf{3.49}\) \\
**FV**(_IR_ + 2) & 1.53 & 24.00 & 23.08 & 22.92 & 25.63 & 26.58 & \(\mathbf{FV}(H,K=2)\) & **5.77** & \(\mathbf{3.00}\) & **4.17** & 3.85 & \(\mathbf{4.5}\) & \(\mathbf{4.57}\) & \(\mathbf{7.73}\) & \(\mathbf{4.33}\) \\
**FV**(_IR_ + 3_) & **0.72** & 24.18 & 20.67 & 23.96 & 3.51 & 23.78 & \(\mathbf{4.87}\) & 23.87 & \(\mathbf{23.0}\) & \(\mathbf{6.22}\) & \(\mathbf{4.93}\) & \(\mathbf{5.36}\) & \(\mathbf{3.34}\) \\
**FV**(_IR_ + 4_) & **1.22** & **24.76** & **0.06** & **24.29** & 1.56 & \(\mathbf{24.50}\) & \(\mathbf{6.25}\) & 23.89 & \(\mathbf{5.32}\) & \(\mathbf{5.36}\) & \(\mathbf{5.32}\) & \(\mathbf{5.36}\) \\
**FV**(_IR_ + 5_) & 2.17 & **28.15** & **1.36** & **24.63** & 4.89 & \(\mathbf{24.56}\) & \(\mathbf{9.57}\) & 23.70 & \(\mathbf{23.0}\) & \(\mathbf{5.39}\) & \(\mathbf{5.30}\) & \(\mathbf{96.54}\) & \(\mathbf{7.30}\) & \(\mathbf{7.38}\) & \(\mathbf{3.11}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: FID and pixel-wise diversity VS\({}_{p}\) of GFDM compared to the original setting of purely Brownian driven VE and VP. In bold the scores that are better than both purely Brownian driven dynamics. The overall best scores within the experiment are boxed in, indicating that the highest scores on both datasets are achieved in the super-diffusive regime for \(H=0.9\).

Figure 2: Comparison of the super-diffusive regime and purely Brownian dynamics in terms of average FID over three rounds of sampling plotted across different NFEs.

\(K=3\) yields the best performance across different datasets and dynamics. Evaluating the performance with different number of sampling steps in Figure 2 shows that the super-diffusive regime with \(K=2\) saturates already at \(500\) number of function evaluations (NFEs) on a lower level than both purely Brownian driven dynamics VP and VE. See Figure 2 in Appendix D for the exact FID values.

**Class-wise distribution coverage**. We evaluate the capability to generate samples from different classes in terms of FID and class-wise distribution coverage, measured by Recall, comparing the best-performing purely Brownian driven dynamics to the super-diffusive regime with \(K=2\). In Table 3 we observe that the super-diffusive regime with \(K=2\) outperforms in both FID and Recall, where \(H=0.7\) and \(H=0.9\) achieve better class-wise FID for all but two and one class, respectively (deer and dog for \(H=0.7\), bird for \(H=0.9\)). Additionally, the super-diffusive regime shows improved class-wise distribution coverage, as indicated by a higher Recall across all classes. Overall, both \(H=0.7\) and \(H=0.9\) perform significantly better in terms of distribution coverage than VP dynamics, \(H=0.9\) being the best performing model.

**Sampling with the augmented probability flow ODE**. We compare the performance of sampling via the PF ODE for the best performing models from above. For MA-fBM driven dynamics, we have \(K+1\) deterministic trajectories for each pixel, traversing from noise to data. As shown in Figure 3, the PF ODE associated with purely Brownian dynamics outperforms the super-diffusive regime in terms of FID, while the super-diffusive regime achieves the overall highest pixel-wise diversity of \(\text{VS}_{p}=4.89\) confirmed mildly perceptually in Figure 4. See Appendix E for additional visualization of the generated data.

Our experiments show that, compared to purely Brownian dynamics, the super-diffusive regime of MA-fBM yields higher image quality with fewer NFEs, improved pixel-wise diversity and better distribution coverage.

## 6 Related work

**Diffusion models in continuous-time**. The seminal work of Song et al. [16] offers a unifying framework modeling the distribution transforming process by a stochastic processes in continuous-time with exact reverse-time model. Extensive research has been carried out to examine [44; 49; 50] and extend [39; 51; 52; 53; 54; 55] the continuous-time view on generative models through the lens of

Figure 4: Visual comparison of PF ODE samples. (LHS) Purely Brownian VP dynamics. (RHS) Super-diffusive regime \(\text{FVP}(H=0.9,K=2)\).

Figure 3: Quantitative performance comparison of SDE and PF ODE sampling.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & FID \(\downarrow\) & IS \(\uparrow\) & \(\text{VS}_{p}\uparrow\) \\ \hline
**Sampled with SDE** & & & \\ \hline VE (retrained) & \(5.20\) & \(9.60\) & \(3.42\) \\ VP (retrained) & \(4.85\) & \(9.64\) & \(3.28\) \\ FP (\(H=0.7,K=2\)) & \(4.17\) & \(9.51\) & \(3.35\) \\ FP(\(H=0.9,K=2\)) & \(\mathbf{3.77}\) & \(9.41\) & \(3.60\) \\ \hline
**Sampled with PF ODE** & & & \\ \hline VE (retrained) & \(6.40\) & \(9.22\) & \(3.14\) \\ FP(\(H=0.7,K=2\)) & \(5.63\) & \(9.23\) & \(3.91\) \\ FP(\(H=0.7,K=2\)) & \(12.23\) & \(\mathbf{9.73}\) & \(4.38\) \\ FP(\(H=0.9,K=2\)) & \(12.26\) & \(9.55\) & \(\mathbf{4.89}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: The class-wise image quality and class-wise distribution coverage of the super-diffusive regime \(\text{FVP}(H=0.9,K=2)\) compared to the purely Brownian VP dynamics.

SDEs, including deterministic corruptions [56] and blurring diffusion [57]. While critic on this view question the usefulness of the theoretical superstructure [58], others extend in line with our work the theoretical framework to new types of underlying diffusion processes [59]. Conceptually similar to our work,Yoon et al. [20] generalizes the score-based generative model from an underlying Brownian motion to a driving Levy process, thereby dropping the Gaussian assumptions on the increments. In contrast to our work, the framework of Yoon et al. [20] does not include correlated increments. Importantly, every Levy process is a semimartingale, which means that fBM is not a Levy process.

**Fractional noises in machine learning.** Recently, Hayashi and Nakagawa [60] considered neural-SDEs driven by fractional noise. Yet they do not study diffusion models. The closest work to our work, Tong et al. [29] approximated the type-II fBM with sparse Gaussian processes constructing a neural SDE as a forward process of a score-based generative model, without exact reverse-time dynamics. Unfortunately, they are also limited to Euler-Maruyama solvers and to the case of \(H>1/3\), while our framework is up to numerical stability applicable for any \(H\in(0,1)\) and compatible with any suitable SDE or ODE solver. Daems _et al._[27], who inspired our Markov-approximate noise, includes a more elaborate discussion as well as a variational inference framework for MA-fBM.

**Rough path theory**. The pathwise analysis of SDEs driven by processes with a Holder exponent less than \(0.5\), including fBM for \(H<0.5\) and BM, is encompassed by rough path theory [37]. Rough path theory is applied in machine learning in several ways including (i) deriving stability bounds for the trained weights of a residual neural network [61], (ii) enabling rough control of neural ODEs [62], and (iii) modeling long time series behavior via neural rough differential equations [63; 64]. In finance the famous Black-Scholes model [65] is driven by BM, while more recent continuous-time models employ fractional noise to model price processes [66; 67] or rough volatility [68; 69] to more closely mimic real-world behavior.

## 7 Conclusion

In this work, we propose a generalized framework of continuous-time score-based generative models, introducing a novel generative model driven by MA-fBM with control over the roughness of distribution transformation paths via augmenting processes. Despite the increased dimensionality of the forward process, learning a score model with the dimensionality of the data distribution, guided by the marginal known score of the augmenting processes, is sufficient. Consequently, both training and sampling is efficient. Our experimental results show that the super-diffusive regime of our MA-fBM driven dynamics achieves superior performance in terms of FID and pixel-wise diversity. Additionally, the FID saturates at a lower level with fewer function evaluations compared to purely Brownian driven dynamics. The super-diffusive regime also improves class-wise distribution coverage, as measured by Recall. Based on these results, GFDM offers a promising alternative to traditional diffusion models for generating data from an unknown distribution.

**Limitations and future work**. Several practical and theoretical questions remain open. While we draw our conclusions from experiments conducted on MNIST and CIFAR10, generalizing the observed behavior to other datasets and data modalities may not be valid. In future work, we aim to empirically and theoretically determine the optimal degree of correlated noise, and thus the optimal Hurst index, for training and sampling across different data modalities. Beyond image data, a particularly interesting modality could be the generation of rough time series data using dynamics of the sub-diffusive regime. A theoretical open question is the limiting behavior of GFDM's reverse dynamics with infinitely many augmenting processes and whether this limit is connected to the reverse time model for true fBM. An intriguing extension would be to adapt the dynamics of our framework to switch between two unknown distributions. This adaptation would enable the use of MA-fBM driven dynamics in the sciences to model real-world evolution between two states of unknown distributions. This is a promising direction, as the assumption of independent increments in real-world noise processes is often too strong.

**Broader impact**. Our contribution advances generative modeling by introducing a specific driving noise process to improve the learning of an unknown distribution. This conceptual work aims to support impactful applications of generative modeling, such as molecular structure generation, medical imaging, drug discovery, and DNA sequence design. However, we acknowledge that generative models can reflect biases in the datasets they are trained on and may pose risks, including misuse for human impersonation and the spread of fake content.

## Acknowledgements

We would like to give a special thanks to Thorsten Selinger for his support in utilizing Fraunhofer HHI's GPU cluster. We also thank the anonymous reviewers for their constructive feedback, which helped improve our work. This work was supported by the Federal Ministry of Education and Research (BMBF) as grants [SyReal (01IS21069B)]. R.M-S. & M.A. acknowledge funding from the _QuantIC_ Project funded by EPSRC Quantum Technology Programme (grant EP/MO1326X/1, EP/T00097X/1), and _dotPhoton AG_. R.M-S acknowledges funding from EP/R018634/1, EP/T021020/1, EP/Y029178/1, and _Google_. SN acknowledges funding from the German Federal Ministry of Education and Research under the grant BIFOLD24B. RD acknowledges funding from the Flemish Government under the "Onderzoeksprogramma Artificiele Intelligentie (AI) Vlaanderen" programme and from Flanders Make under the SBO project CADAIVISION. TB was supported by a UKRI Future Leaders Fellowship [grant number MR/Y018818/1]. MO has been partially funded by Deutsche Forschungsgemeinschaft (DFG) - Project - ID 318763901 - SFB1294. WS acknowledges financial support by the German Research Foundation (DFG) - Research Unit KI-FOR 5363 (project ID: 459422098).

## References

* [1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 2256-2265. PMLR, 2015.
* [2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Advances in Neural Information Processing Systems_, volume 33, pages 6840-6851. Curran Associates, Inc., 2020.
* [3] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [4] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3D. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 8867-8887. PMLR, 2022.
* [5] Matteo Manica, Jannis Born, Joris Cadow, Dimitrios Christofidellis, Ashish Dave, Dean Clarke, Yves Gaetan Nana Teukam, Giorgio Giannone, Samuel C Hoffman, Matthew Buchan, et al. Accelerating material design with the generative toolkit for scientific discovery. _npj Computational Materials_, 9(1):69, 2023.
* [6] Gabriele Corso, Hannes Stark, Bowen Jing, Regina Barzilay, and Tommi S. Jaakkola. Diff-dock: Diffusion steps, twists, and turns for molecular docking. In _The Eleventh International Conference on Learning Representations_, 2023.
* [7] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation. In _Advances in Neural Information Processing Systems_, volume 36, pages 47704-47720. Curran Associates, Inc., 2023.
* [8] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Defossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. In _The Eleventh International Conference on Learning Representations_, 2023.
* [9] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3D shape generation. In _Advances in Neural Information Processing Systems_, volume 35, pages 10021-10039. Curran Associates, Inc., 2022.

* [10] Zhiying Leng, Tolga Birdal, Xiaohui Liang, and Federico Tombari. HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19691-19700, 2024.
* [11] Simone Foti, Stefanos Zafeiriou, and Tolga Birdal. UV-free texture generation with denoising and geodesic heat diffusions. In _Advances in Neural Information Processing Systems_, 2024.
* [12] Marco Aversa, Gabriel Nobis, Miriam Hagele, Kai Standvoss, Mihaela Chirica, Roderick Murray-Smith, Ahmed Alaa, Lukas Ruff, Daniela Ivanova, Wojciech Samek, Frederick Klauschen, Bruno Sanguinetti, and Luis Oala. DiffInfinite: Large mask-image synthesis via parallel random patch diffusion in histopathology. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.
* [13] Miguel Espinosa and Elliot J. Crowley. Generate your own scotland: Satellite image generation conditioned on maps. _NeurIPS 2023 Workshop on Diffusion Models_, Aug 2023.
* [14] Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 1276-1301. PMLR, 2023.
* [15] Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Tommaso Biancalani, Avantika Lal, Tommi Jaakkola, Sergey Levine, Hanchen Wang, and Aviv Regev. Fine-tuning discrete diffusion models via reward optimization with applications to dna and protein design. _arXiv preprint arXiv:2410.13643_, 2024.
* [16] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021.
* [17] Robert Brown. XXVII. A brief account of microscopical observations made in the months of June, July and August 1827, on the particles contained in the pollen of plants; and on the general existence of active molecules in organic and inorganic bodies. _The Philosophical Magazine_, **4** (21):161-173, 1828.
* [18] Albert Einstein. Uber die von der molekularkinetischen Theorie der Warme geforderte Bewegung von in ruhenden Flussigkeiten suspendierten Teilchen. _Annalen der Physik_, pages 549-560, 1905.
* [19] Norbert Wiener. Differential-space. _Journal of Mathematics and Physics_, 2:131-174, 1923.
* [20] Eunbi BI Yoon, Keehun Park, Sungwoong Kim, and Sungbin Lim. Score-based generative models with Levy processes. In _Advances in Neural Information Processing Systems_, volume 36, pages 40694-40707. Curran Associates, Inc., 2023.
* [21] Hengyuan Ma, Li Zhang, Xiatian Zhu, and Jianfeng Feng. Approximated anomalous diffusion: Gaussian mixture score-based generative models, 2023. URL [https://openreview.net/forum?id=yc9xen7EAzd](https://openreview.net/forum?id=yc9xen7EAzd).
* [22] Paul Levy. Random functions: general theory with special reference to Laplacian random functions. _University of California Publications in Statistics_, 1:331-390, 1953.
* [23] Benoit B. Mandelbrot and John W. Van Ness. Fractional Brownian Motions, fractional noises and applications. _SIAM Review_, 10(4):422-437, 1968.
* [24] Jerry Stinson. The (mis) behavior of markets. _Journal of Personal Finance_, 4(4):99, 2005.
* [25] Francesca Biagini, Yaozhong Hu, Bernt Oksendal, and Tusheng Zhang. _Stochastic Calculus for Fractional Brownian Motion and Applications_. Springer-Verlag London Limited 2008, 2008. doi: 10.1007/978-1-84628-797-8.
* [26] Philipp Harms and David Stefanovits. Affine representations of fractional processes with applications in mathematical finance. _Stochastic Processes and their Applications_, 129(4):1185-1228, 2019. ISSN 0304-4149.

* Daems et al. [2024] Rembert Daems, Manfred Opper, Guillaume Crevecoeur, and Tolga Birdal. Variational inference for SDEs driven by fractional noise. In _The Twelfth International Conference on Learning Representations_, 2024.
* Protter [2013] Philip E. Protter. _Stochastic Integration and Differential Equations_. Stochastic Modelling and Applied Probability. Springer Berlin, Heidelberg, 2nd edition, 2013. ISBN 978-3-662-10061-5.
* Tong et al. [2022] Anh Tong, Thanh Nguyen-Tang, Toan Tran, and Jaesik Choi. Learning fractional white noises in neural stochastic differential equations. In _Advances in Neural Information Processing Systems_, volume 35, pages 37660-37675. Curran Associates, Inc., 2022.
* Cohen and Elliott [2015] Samuel N. Cohen and Robert J. Elliott. _Stochastic Calculus and Applications_. Probability and Its Applications. Birkhauser, New York, NY, 2st edition, 2015. ISBN 978-1-4939-2866-8.
* Stratonovich [1960] R. L. Stratonovich. Conditional Markov Processes. _Theory of Probability & Its Applications_, 5(2):156-178, 1960.
* Anderson [1982] Brian D.O. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982. ISSN 0304-4149.
* Mathematic and Physics, volume 1158 of Lecture Notes in Math_, page 119-129, 1986.
* Hyvarinen [2005] Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(24):695-709, 2005.
* Song et al. [2019] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In _Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence_, page 204, 2019.
* Bayer et al. [2021] Christian Bayer, Antonis Papapantoleon, and Raul Tempone. _Computational Finance_. Technical University of Berlin, 2021. URL [https://www.wias-berlin.de/people/bayerc/files/lecture.pdf](https://www.wias-berlin.de/people/bayerc/files/lecture.pdf). Lecture notes.
* Lyons [1998] Terry J. Lyons. Differential equations driven by rough signals. _Revista Matematica Iberoamericana_, 14(2):215-310, 1998.
* Davidson and Hashimzade [2009] James Davidson and Nigar Hashimzade. Type I and type II fractional Brownian motions: A reconsideration. _Computational Statistics & Data Analysis_, 53(6):2089-2106, 2009. ISSN 0167-9473. The Fourth Special Issue on Computational Econometrics.
* Lou and Ermon [2023] Aaron Lou and Stefano Ermon. Reflected diffusion models. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 22675-22701. PMLR, 2023.
* Huy [2003] Dang Huy. A remark on non-Markov property of a fractional Brownian motion. _Vietnam Journal of Mathematics_, 31, 01 2003.
* 1211, 2007. doi: 10.1214/EJP.v12-439.
* B_, 26(10):5567-5579, 2021. ISSN 1531-3492.
* Sarkka and Solin [2019] Simo Sarkka and Arno Solin. _Applied Stochastic Differential Equations_, volume 10. Cambridge University Press, 2019.
* Karras et al. [2022] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _Advances in Neural Information Processing Systems_, volume 35, pages 26565-26577. Curran Associates, Inc., 2022.
* Heusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.

* Salimans et al. [2016] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training GANs. In _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.
* Friedman and Dieng [2023] Dan Friedman and Adji Bousso Dieng. The Venti Score: A diversity evaluation metric for machine learning. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.
* Kynkaanniemi et al. [2019] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Chen et al. [2023] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R. Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In _The Eleventh International Conference on Learning Representations_, 2023.
* Singhal et al. [2023] Raghav Singhal, Mark Goldstein, and Rajesh Ranganath. Where to diffuse, how to diffuse, and how to get back: Automated learning for multivariate diffusions. In _The Eleventh International Conference on Learning Representations_, 2023.
* Jing et al. [2022] Bowen Jing, Gabriele Corso, Renato Berlinghieri, and Tommi Jaakkola. Subspace diffusion generative models. In _Lecture Notes in Computer Science_, pages 274-289. Springer Nature Switzerland, 2022.
* Kim et al. [2022] Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, and Il-chul Moon. Maximum likelihood training of implicit nonlinear diffusion model. In _Advances in Neural Information Processing Systems_, volume 35, pages 32270-32284. Curran Associates, Inc., 2022.
* Huang et al. [2022] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville. Riemannian diffusion models. In _Advances in Neural Information Processing Systems_, volume 35, pages 2750-2761. Curran Associates, Inc., 2022.
* Bunne et al. [2023] Charlotte Bunne, Ya-Ping Hsieh, Marco Cuturi, and Andreas Krause. The Schrodinger bridge between Gaussian measures has a closed form. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2023.
* Song et al. [2023] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 32211-32252. PMLR, 2023.
* Daras et al. [2023] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alex Dimakis, and Peyman Milanfar. Soft diffusion: Score matching with general corruptions. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.
* Hoogeboom and Salimans [2023] Emiel Hoogeboom and Tim Salimans. Blurring diffusion models. In _The Eleventh International Conference on Learning Representations_, 2023.
* Bansal et al. [2023] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. In _Advances in Neural Information Processing Systems_, volume 36, pages 41259-41282. Curran Associates, Inc., 2023.
* Rissanen et al. [2023] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. In _The Eleventh International Conference on Learning Representations_, 2023.
* Hayashi and Nakagawa [2022] Kohei Hayashi and Kei Nakagawa. Fractional SDE-Net: Generation of time series data with long-term memory. In _2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA)_, pages 1-10, 2022.
* Bayer et al. [2023] Christian Bayer, Peter K. Friz, and Nikolas Tapia. Stability of deep neural networks via discrete rough paths. _SIAM Journal on Mathematics of Data Science_, 5(1):50-76, 2023.
* Kidger [2021] P Kidger. _On neural differential equations_. PhD thesis, University of Oxford, 2021.

* [63] Shujian Liao, Terry Lyons, Weixin Yang, and Hao Ni. Learning stochastic differential equations using RNN with log signature features. _arXiv preprint arXiv:1908.08286_, 2019.
* [64] James Morrill, Cristopher Salvi, Patrick Kidger, and James Foster. Neural rough differential equations for long time series. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 7829-7838. PMLR, 2021.
* [65] Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. _The Journal of Political Economy_, 81(3):637-654, 1973.
* [66] Christoph Czichowsky, Remi Peyre, Walter Schachermayer, and Junjian Yang. Shadow prices, fractional Brownian motion, and portfolio optimisation under transaction costs. _Finance and Stochastics_, 22:161-180, 2018.
* [67] Paolo Guasoni, Zsolt Nika, and Miklos Rasonyi. Trading fractional Brownian motion. _SIAM journal on financial mathematics_, 10(3):769-789, 2019.
* [68] Christian Bayer, Peter Friz, and Jim Gatheral. Pricing under rough volatility. _Quantitative Finance_, 16(6):887-904, 2016.
* [69] Jim Gatheral, Thibault Jaisson, and Mathieu Rosenbaum. Volatility is rough. In _Commodities_, pages 659-690. Chapman and Hall/CRC, 2022.
* [70] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 234-241. Springer, 2015.
* [71] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _CoRR_, December 2014. arXiv:1412.6980 [cs.LG].
* [72] Leslie N. Smith and Nicholay Topin. Super-convergence: very fast training of neural networks using large learning rates. In _Defense + Commercial Sensing_, 2018.
* [73] Marjorie G. Hahn, Kei Kobayashi, and Sabir Umarov. Fokker-Planck-Kolmogorov equations associated with time-changed fractional Brownian motion. _arXiv: Mathematical Physics_, 139:691-705, 2011.

## Appendix

* A The mathematical framework of generative fractional diffusion models
* A.1 A Markovian representation of fractional Brownian motion
* A.2 The forward model
* A.3 Estimating the score via augmented score matching loss
* B Forward sampling
* C Implementation details
* D Additional experiments
* E Illustration of generated data
* F Computational cost of augmenting processes
* G Likelihood computation
* H Challenges in the attempt to generalize
* I Notational conventionsThe mathematical framework of generative fractional diffusion models

In this section we provide the mathematical details of the score-based generative model defined in the main paper. The driving noise of the underlying stochastic process is based on the affine representation of fractional processes from Harms and Stefanovits [26] and further simplified by the closed form expression to determine optimal approximation coefficients of Daems et al. [27].

### A Markovian representation of fractional Brownian motion

We begin with the definition of type I fractional Brownian motion, defined on the whole real line, possessing correlated increments that are in contrast to type II fractional Brownian motion stationary.

**Definition A.1** (Type I Fractional Brownian Motion [23]).: _Let \((\Omega,\mathcal{F},\mathbb{P})\) be a complete probability space equipped with a complete and right continuous filtration \(\{\mathcal{F}_{t}\}\) and \(\Gamma\) the Gamma function. For two standard independent \(\{\mathcal{F}_{t}\}\)-Brownian motions (BMs) \(\tilde{B}\) and \(B\) the centered Gaussian process \(W^{H}=(W^{H}_{t})_{t\in\mathbb{R}}\) with_

\[W^{H}_{t}:=\frac{1}{\Gamma(H+\frac{1}{2})}\int_{-\infty}^{0}((t-s)^{H-\frac{1} {2}}-(-s)^{H-\frac{1}{2}})\mathrm{d}\tilde{B}_{s}+\frac{1}{\Gamma(H+\frac{1}{ 2})}\int_{0}^{t}(t-s)^{H-\frac{1}{2}}\mathrm{d}B_{s} \tag{22}\]

_uniquely characterized in law by its covariances_

\[\mathbb{E}\left[W^{H}_{t}W^{H}_{s}\right]=\frac{1}{2}\left[t^{2H}+s^{2H}-(t-s) ^{2H}\right],\quad t\geq s>0 \tag{23}\]

_is called type I fractional Brownian motion (fBM) with Hurst index \(H\in(0,1)\)._

Type II fBM from the main paper is retrieved by setting the additionally defined BM \(\tilde{B}\) on the negative real line to zero. Therefore, the difference to type II fBM is the stochastic integral w.r.t. \(\tilde{B}\) that yields stationary increments and a non trivial distribution at \(t=0\). For \(H=0.5\), the process is a BM and has thus independent increments. For \(H\in(0,1)\setminus\{\frac{1}{2}\}\), the process possesses correlated increments and, compared to BM, smoother paths for \(H>0.5\) due to positively correlated increments (superdiffusion) and rougher paths for \(H<0.5\) due to negatively correlated increments (sub-diffusion). These three regimes reflect for type I fBM in the same change of quadratic variation from \(t\) to zero quadratic variation in the smooth regime and to infinite quadratic variation in the rough regime [30]. To prepare the approximation of the non-Markovian and non-semimaringale fBM [25] via Markovian semimartingales, define for every \(\gamma\in(0,\infty)\) the Ornstein-Uhlenbeck process \(Y^{\gamma}\) given by

\[Y^{\gamma}_{t}:=Y^{\gamma}_{0}e^{-t\gamma}+\int_{0}^{t}e^{-\gamma(t-s)} \mathrm{d}B_{s},\quad t\geq 0,\quad Y_{0}:=\int_{-\infty}^{0}e^{s\gamma} \mathrm{d}\tilde{B}_{s}, \tag{24}\]

with speed of mean reversion \(\gamma\) and non trivial starting value in contrast to the OU processes defined in eq. (6) of the main paper. By Ito's product rule [30], the process \(Y^{\gamma}\) solves the same SDE

\[\mathrm{d}Y^{\gamma}_{t}=-\gamma Y^{\gamma}_{t}\mathrm{d}t+\mathrm{d}B_{t}, \quad Y_{0}=\int_{-\infty}^{0}e^{s\gamma}\mathrm{d}\tilde{B}_{s}, \tag{25}\]

with different starting value. According to Harms and Stefanovits [26] we represent fBm by an integral over the predefined family of Ornstein-Uhlenbeck processes.

**Theorem A.2** (Markovian representation of fBM [26, 27]).: _The non-Markovian process \(W^{H}\) permits the infinite-dimensional Markovian representation_

\[W^{H}_{t}=\begin{cases}\int_{0}^{\infty}\left(Y^{\gamma}_{t}-Y^{\gamma}_{0} \right)\nu_{1}(\gamma)\mathrm{d}\gamma,&H\leq\frac{1}{2}\\ -\int_{0}^{\infty}\partial_{\gamma}\left(Y^{\gamma}_{t}-Y^{\gamma}_{0}\right) \nu_{2}(\gamma)\mathrm{d}\gamma,&H>\frac{1}{2}\end{cases} \tag{26}\]

_where_

\[\nu_{1}(\gamma)=\frac{\gamma^{-(H+\frac{1}{2})}}{\Gamma(H+\frac{1}{2})\Gamma( \frac{1}{2}-H)}\quad\text{and}\quad\nu_{2}(\gamma)=\frac{\gamma^{-(H-\frac{1} {2})}}{\Gamma(H+\frac{1}{2})\Gamma(\frac{3}{2}-H)}. \tag{27}\]

Note that we follow Daems et al. [27] in replacing the process \(Z^{\gamma}_{t}:=Z^{\gamma}_{0}e^{-t\gamma}+\int_{0}^{t}e^{-(t-s)\gamma}Y^{ \gamma}_{s}ds\) from the original theorem throughout this work by \(Z^{\gamma}_{t}=-\partial_{\gamma}Y^{\gamma}_{t}+\left(\partial_{\gamma}Y^{ \gamma}_{0}+Z^{\gamma}_{0}\right)e^{-t\gamma}\). This is justified by Harms and Stefanovits [26, Remark 3.5] and simplifies for \(H>\frac{1}{2}\) the approximation of fBM and the definition of our generative model, since we only have to reverse the \(Y^{\gamma}\) processes instead of the pairs \((Y^{\gamma},Z^{\gamma})\). For \(Y_{0}^{\gamma}=0\) eq. (26) yields an infinite-dimensional Markovian representation of type II fBM [27]. The MA-fBM from Definition 3.2 in the main paper becomes for type I fBM

\[\hat{B}_{t}^{H}=\sum_{k=1}^{K}\omega_{k}\left(Y_{t}^{k}-Y_{0}^{k}\right),\quad H \in(0,1),\quad t\geq 0 \tag{28}\]

with non trivial \(\mathbf{Y}_{0}=(Y_{0}^{1},...,Y_{0}^{1})\) that is a centered multivariate Gaussian with covariances \(\mathbb{E}\left[Y_{0}^{k}Y_{0}^{l}\right]=1/(\gamma_{k}+\gamma_{l})\)[27]. Theorem 3.3 holds true for type I fBM as well with optimal approximation coefficients given in Daems et al. [27, Proposition 5]. For more details on the properties and distinction of type I and type II fBM we refer the reader to Daems et al. [27].

### The forward model

We define in the following a score-based generative model approximating a fractional diffusion process driven by type I fBM. For the remainder of Appendix A we assume \(Y_{0}^{k}=\int_{-\infty}^{0}e^{s\gamma_{k}}\mathrm{d}\hat{B}_{s}\) for all \(1\leq k\leq K\) where the setting from the main paper with type II fBM is recovered by choosing \(Y_{0}^{k}=0\) instead. Let \(\mathbf{\hat{B}}^{H}\) be a \(D\)-dimensional MA-fBM with Hurst index \(H\in(0,1)\). For continuous functions \(\mu:[0,T]\rightarrow\mathbb{R}\) and \(g:[0,T]\rightarrow\mathbb{R}\) we define the forward process \(\mathbf{X}=(\mathbf{X}_{t})_{t\in[0,T]}\) by

\[\mathrm{d}\mathbf{X}_{t}=\mu(t)\mathbf{X}_{t}\mathrm{d}t+g(t)\mathrm{d}\mathbf{ \hat{B}}_{t}^{H},\quad\mathbf{X}_{0}=\mathbf{x}_{0}\sim p_{0},\quad t\in[0,T] \tag{29}\]

where \(p_{0}\) is an unknown data distribution from which we aim to sample from. Using eq. (25) we note

\[\mathrm{d}\mathbf{\hat{B}}_{t}^{H}=-\sum_{k=1}^{K}\omega_{k}\gamma_{k}Y_{t}^{k }dt+\sum_{k=1}^{K}\omega_{k}\mathrm{d}\mathbf{B}_{t}, \tag{30}\]

where \(\mathbf{B}=(B_{1},...,B_{d})\) is a multivariate BM. With \(\boldsymbol{\bar{\omega}}:=\sum_{k=1}^{K}\omega_{k}\) we rewrite the dynamics of the forward process as

\[\mathrm{d}\mathbf{X}_{t}=\left[\mu(t)\mathbf{X}_{t}-g(t)\sum_{k=1}^{K}\omega_ {k}\gamma_{k}\mathbf{Y}_{t}^{k}\right]\mathrm{d}t+\boldsymbol{\bar{\omega}}g( t)\mathrm{d}\mathbf{B}_{t},\quad t\in[0,T], \tag{31}\]

Taking into account the dynamics of the OU processes, we define the _augmented forward process_\(\mathbf{Z}=(\mathbf{Z}_{t})_{t\in[0,T]}\) by

\[\mathbf{Z}_{t}=(X_{t,1},Y_{t,1}^{1},...,Y_{t,1}^{K},X_{t,2},Y_{t,2}^{1},...,Y_ {t,2}^{K},...,...,...,X_{t,D},Y_{t,D}^{1},...Y_{t,D}^{K})\in\mathbb{R}^{D(K+1)} \tag{32}\]

following the dynamics

\[\mathrm{d}\mathbf{Z}_{t}=\mathbf{F}(t)\mathbf{Z}_{t}dt+\mathbf{G}(t)\mathrm{d }\mathbf{B}_{t} \tag{33}\]

with \(\mathbf{F}(t)=diag(\mathbf{R}(t),...,\mathbf{R}(t))\in\mathbb{R}^{D(K+1),D(K+ 1)}\),

\[\mathbf{R}(t)=\begin{pmatrix}\mu(t)&-g(t)\omega_{1}\gamma_{1}&\cdots&-g(t) \omega_{K}\gamma_{K}\\ \mathbf{0}_{K}&&-diag(\gamma_{1},...,\gamma_{K})\end{pmatrix}\in\mathbb{R}^{ K+1,K+1} \tag{34}\]

and

\[\boldsymbol{G}(t)=\left(\boldsymbol{\bar{\omega}}g(t)\boldsymbol{I}_{D}\quad \boldsymbol{I}_{D}\quad\ldots\quad\boldsymbol{I}_{D}\right)^{T}\in\mathbb{R} ^{D(K+1),D}. \tag{35}\]

For each dimension \(1\leq d\leq D\), the dynamics of the process transforming \(\mathbf{x}_{0,d}\) reduce to those of the augmented forward process with \(D=1\), given by

\[\mathrm{d}\mathbf{Z}_{t}=\mathbf{F}(t)\mathbf{Z}_{t}\mathrm{d}t+\mathbf{G}(t) \mathrm{d}B_{t}, \tag{36}\]

where the \(K+1\) processes that transform \(\mathbf{x}_{0,d}\) are all driven by the same one-dimensional BM \(B\). The augmented forward process \(\mathbf{Z}\) conditioned on \(\mathbf{y}_{0}^{1},...,\mathbf{y}_{0}^{K}\) and a data sample \(\mathbf{x}_{0}\sim p_{0}\) is a linear transformation of BM and hence a Gaussian process and so is \(\mathbf{X}\)[43]. Since the integral w.r.t BM has zero mean, the mean vector of the augmenting processes is \(\mathbb{E}\left[\mathbf{Y}_{t}^{k}\right]=\mathbf{0}_{d}\) for all \(1\leq k\leq K\) and the mean of the conditional forward process is the solution of the ODE

\[\partial_{t}\mathbb{E}\left[\mathbf{X}_{t}|\mathbf{x}_{0}\right]=\mu(t) \mathbb{E}\left[\mathbf{X}_{t}|\mathbf{x}_{0}\right] \tag{37}\]and hence the marginal mean

\[\mathbb{E}\left[\mathbf{X}_{t}|\mathbf{x}_{0}\right]=c(t)\mathbf{x}_{0}\quad\text{ with}\quad c(t)=\exp\left(\int_{0}^{t}\mu(s)ds\right) \tag{38}\]

is not affected by changing the driving noise to MA-fBM. The marginal covariance matrix \(\mathbf{\Sigma}_{t}\) of the conditional augmented forward process can be approximated numerically by solving an ODE, see Appendix B for details. In addition we present a continuous reparameterization of the forward process, resulting for some forward dynamics in a closed form solution of the marginal covariance matrix. Our result generalizes the explicit formula for the perturbation kernel \(p_{0t}(\mathbf{x}|\mathbf{x}_{0})=\mathcal{N}(\mathbf{x};c(t)\mathbf{x}_{0},c^ {2}(t)\sigma^{2}(t)\mathbf{I}_{d})\) given in Karras et al. [44].

**Proposition A.3** (Continuous Reparameterization Trick).: _Let \(\mathbf{x}_{0}\) be a fixed realisation drawn from \(p_{0}\). The forward process \(\mathbf{X}=(\mathbf{X}_{t})_{t\in[0,T]}\) conditioned on \(\mathbf{x}_{0}\) admits the continuous reparameterization_

\[\mathbf{X}_{t}=c(t)\left(\mathbf{x}_{0}+\int_{0}^{t}\alpha(t,s)\mathrm{d} \mathbf{B}_{s}\right)+\underbrace{c(t)\sum_{k=1}^{K}\omega_{k}\gamma_{k}\int_ {0}^{t}\frac{g(s)}{c(s)}e^{-s\gamma_{k}}\mathrm{d}s\mathbf{Y}_{0}^{k}}_{=0\text { for type II/BM since }\mathbf{Y}_{0}^{k}=0} \tag{39}\]

_with \(c(t)=\exp\left(\int_{0}^{t}\mu(s)\mathrm{d}s\right)\) and_

\[\alpha(t,s)=-\sum_{k=1}^{K}\omega_{k}\gamma_{k}\int_{s}^{t}\frac{g(u)}{c(u)}e ^{-\gamma_{k}(u-s)}\mathrm{d}u+\bar{\mathbf{\omega}}\frac{g(s)}{c(s)} \tag{40}\]

_such that \(\mathbf{X}_{t}|\mathbf{x}_{0}\sim\mathcal{N}\left(c(t)\mathbf{x}_{0},\left[c^ {2}(t)\sigma^{2}(t)+\sigma_{K}^{2}(t)\right]\mathbf{I}_{d}\right)\) is a Gaussian random vector for all \(t\in(0,T]\) with_

\[\sigma^{2}(t)=\int_{0}^{t}\alpha^{2}(t,s)ds \tag{41}\]

_and_

\[\sigma_{K}^{2} =c^{2}(t)\sum_{k=1}^{K}\frac{\gamma_{k}}{2}\left[\omega_{k}\int_ {0}^{t}\frac{g(s)}{c(u)}\mathrm{d}u\right]^{2} \tag{42}\] \[+2c^{2}(t)\sum_{k<l}\frac{\omega_{k}\omega_{l}\gamma_{k}\gamma_{l }}{\gamma_{k}+\gamma_{l}}\int_{0}^{t}\frac{g(s)}{c(s)}e^{-s\gamma_{k}}\mathrm{ d}s\int_{0}^{t}\frac{g(s)}{c(s)}e^{-s\gamma_{l}}\mathrm{d}s \tag{43}\]

_vanishing for an underlying type II fBM._

Proof.: By continuity, the functions \(\mu\) and \(\sigma\) are bounded. Moreover, the processes \(Y_{j}^{1},...,Y_{j}^{K}\) posses continuous, hence bounded, paths and thus

\[\int_{0}^{t}|\mu(u)|\mathrm{d}u<\infty,\quad\int_{0}^{t}\sigma^{2}(u)\mathrm{ d}u<\infty\quad\text{and}\quad\int_{0}^{t}|\sum_{k}^{K}\omega_{k}\gamma_{k} \mathbf{Y}_{t}^{k}|\mathrm{d}u<\infty\quad\mathbb{P}-a.s., \tag{44}\]

where the last integral is understood entrywise. Hence, by Cohen and Elliott [30, Theorem 16.6.1], the unique solution of the SDE eq. (31) is given explicitly as

\[\mathbf{X}_{t}=c(t)\left(\mathbf{x}_{0}-\int_{0}^{t}\frac{g(u)}{c(u)}\left[ \sum_{k=1}^{K}\omega_{k}\gamma_{k}\mathbf{Y}_{u}^{k}\right]\mathrm{d}u+\bar{ \mathbf{\omega}}\int_{0}^{t}\frac{g(u)}{c(u)}\mathrm{d}\mathbf{B}_{u}\right), \tag{45}\]

with \(c(t)=\exp\left(\int_{0}^{t}\mu(s)ds\right)\). Define

\[\mathbf{J}\left(\mathbf{Y}_{0}^{[K]},t\right):=\sum_{k=1}^{K}\omega_{k}\gamma_ {k}\int_{0}^{t}\frac{g(s)}{c(s)}e^{-s\gamma_{k}}\mathrm{d}s\mathbf{Y}_{0}^{k} \tag{46}\]and by the definition of \(Y_{j}^{k}\) in (24) we calculate using the Stochastic Fubini Theorem [26]

\[\int_{0}^{t}\frac{g(u)}{c(u)}\left[\sum_{k=1}^{K}\omega_{k}\gamma_{k }\mathbf{Y}_{u}^{k}\right]\mathrm{d}u =\sum_{k=1}^{K}\omega_{k}\gamma_{k}\int_{0}^{t}\int_{0}^{u}\frac{g (u)}{c(u)}e^{-\gamma_{k}(u-s)}\mathrm{d}\mathbf{B}_{s}\mathrm{d}u+\mathbf{J} \left(\mathbf{Y}_{0}^{[K]},t\right) \tag{47}\] \[=\int_{0}^{t}\sum_{k=1}^{K}\omega_{k}\gamma_{k}\int_{s}^{t}\frac{ g(u)}{c(u)}e^{-\gamma_{k}(u-s)}\mathrm{d}u\mathrm{d}\mathbf{B}_{s}+\mathbf{J} \left(\mathbf{Y}_{0}^{[K]},t\right) \tag{48}\]

and hence

\[\mathbf{X}_{t} =c(t)\left(\mathbf{x}_{0}-\int_{0}^{t}\frac{g(u)}{c(u)}\left[ \sum_{k=1}^{K}\omega_{k}\gamma_{k}\mathbf{Y}_{u}^{k}\right]\mathrm{d}u+ \boldsymbol{\bar{\omega}}\int_{0}^{t}\frac{g(u)}{c(u)}\mathrm{d}\mathbf{B}_{u}\right) \tag{49}\] \[=c(t)\left(\mathbf{x}_{0}-\int_{0}^{t}\sum_{k=1}^{K}\omega_{k} \gamma_{k}\int_{s}^{t}\frac{g(u)}{c(u)}e^{-\gamma_{k}(u-s)}\mathrm{d}u\mathrm{ d}\mathbf{B}_{s}+\boldsymbol{\bar{\omega}}\int_{0}^{t}\frac{g(u)}{c(u)}\mathrm{d} \mathbf{B}_{u}-\mathbf{J}\left(\mathbf{Y}_{0}^{[K]},t\right)\right)\] \[=c(t)\left(\mathbf{x}_{0}+\int_{0}^{t}\left[-\sum_{k=1}^{K}\omega _{k}\gamma_{k}\int_{s}^{t}\frac{g(u)}{c(u)}e^{-\gamma_{k}(u-s)}\mathrm{d}u+ \boldsymbol{\bar{\omega}}\frac{g(s)}{c(s)}\right]\mathrm{d}\mathbf{B}_{s}- \mathbf{J}\left(\mathbf{Y}_{0}^{[K]},t\right)\right)\] \[=c(t)\mathbf{x}_{0}+c(t)\int_{0}^{t}\int_{0}^{t}\alpha(t,s) \mathrm{d}\mathbf{B}_{s}-c(t)\mathbf{J}\left(\mathbf{Y}_{0}^{[K]},t\right) \tag{50}\]

with

\[\alpha(t,s)=-\sum_{k=1}^{K}\omega_{k}\gamma_{k}\int_{s}^{t}\frac{g(u)}{c(u)}e ^{-\gamma_{k}(u-s)}\mathrm{d}u+\boldsymbol{\bar{\omega}}\frac{g(s)}{c(s)}. \tag{51}\]

Since \(\alpha(t,\cdot)\) is continuous for every fixed \(t\in[0,T]\) we have \(\int_{0}^{t}\alpha^{2}(t,s)\mathrm{d}s<\infty\). Using that the integral of a bounded deterministic function w.r.t. Brownian motion is a Gaussian process we have by Ito's isometry

\[\int_{0}^{t}\alpha(t,s)\mathrm{d}\mathbf{B}_{s}\sim\mathcal{N}\left(\mathbf{0 }_{d},\sigma^{2}(t)\mathbf{I}_{d}\right)\quad\text{with}\quad\sigma^{2}(t)= \int_{0}^{t}\alpha^{2}(t,s)\mathrm{d}s. \tag{52}\]

Therefore, conditional on \(\mathbf{x}_{0}\), the random vector \(\mathbf{X}_{t}\) is Gaussian with mean vector

\[\boldsymbol{m}_{t}^{\mathbf{x}}=c(t)\boldsymbol{x}_{0}+\underbrace{\mathbb{E} \left[\mathbf{J}(\mathbf{Y}_{0}^{[K]})\right]}_{=0}=\mathbf{x}_{0}\exp\left( \int_{0}^{t}\mu(s)\mathrm{d}s\right). \tag{53}\]

Moreover, \(\tilde{B}_{j}\) and \(B_{j}\) corresponding to the entries of \(\mathbf{\tilde{B}}=(\tilde{B}_{1},...,\tilde{B}_{d})\) and \(\mathbf{B}=(B_{1},...,B_{d})\) are independent by Theorem A.1 resulting in the entrywise variance

\[\boldsymbol{\Sigma}_{t,j,j}^{\mathbf{x}}=c^{2}(t)\int_{0}^{t}\alpha^{2}(t,s) \mathrm{d}s+\sigma_{K}^{2}(t) \tag{54}\]

with

\[\sigma_{K}^{2}(t)=\mathbb{V}\left[\mathbf{J}(\mathbf{Y}_{0}^{[K]} )_{j}\right] =c^{2}(t)\sum_{k=1}^{K}\frac{\gamma_{k}}{2}\left[\omega_{k}\int_{ 0}^{t}\frac{g(s)}{c(u)}\mathrm{d}u\right]^{2} \tag{55}\] \[+2c^{2}(t)\sum_{k<l}\frac{\omega_{k}\omega_{l}\gamma_{k}\gamma_{l }}{\gamma_{k}+\gamma_{l}}\int_{0}^{t}\frac{g(s)}{c(s)}e^{-s\gamma_{k}}\mathrm{ d}s\int_{0}^{t}\frac{g(s)}{c(s)}e^{-s\gamma_{l}}\mathrm{d}s, \tag{56}\]

where we used again Ito's isometry to calculate

\[\mathbb{E}\left[Y_{0,j}^{k}Y_{0,j}^{l}\right]=\mathbb{E}\left[\int_{-\infty}^{ 0}e^{\gamma_{k}s}\mathrm{d}\tilde{B}_{s,j}\int_{-\infty}^{0}e^{\gamma_{l}s} \mathrm{d}\tilde{B}_{s,j}\right]=\int_{-\infty}^{0}e^{(\gamma_{k}+\gamma_{l})s }\mathrm{d}s=\frac{1}{\gamma_{k}+\gamma_{l}}. \tag{57}\]

Since the entries of \(\mathbf{B}\) are independent, we find the covariance matrix

\[\boldsymbol{\Sigma}_{t}^{\mathbf{x}}=\left[c^{2}(t)\sigma^{2}(t)+\sigma_{K}^{2} (t)\right]\mathbf{I}_{d}. \tag{58}\]The preceding proposition generalizes the "reparameterization trick"3 from discrete time to continuous-time in the sense that

Footnote 3: See [https://lilianweng.github.io/posts/2021-07-11-diffusion-models/](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) for the derivation in discrete time.

\[\mathbf{X}_{t_{n}}=\sqrt{\alpha_{t_{n}}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t_{ n}}}\mathbf{\epsilon},\quad\mathbf{\epsilon}\sim\mathcal{N}(\mathbf{0}_{d},\mathbf{I}_{d}) \tag{59}\]

used in discrete time [2] with time steps \(0=t_{0}<...<t_{N}=T\) is replaced by our continuous-time reparameterization

\[\mathbf{X}_{t}=c(t)\left(\mathbf{x}_{0}+\int_{0}^{t}\alpha(t,s)\mathrm{d} \mathbf{B}_{s}\right)+c(t)\sum_{k=1}^{K}\omega_{k}\gamma_{k}\int_{0}^{t}\frac{ g(s)}{c(s)}e^{-s\gamma_{k}}\mathrm{d}\mathbf{s}\mathbf{Y}_{0}^{k}, \tag{60}\]

enabling to directly sample \(\mathbf{X}_{t}|\mathbf{x}_{0}\sim\mathcal{N}(c(t)\mathbf{x}_{0}+\left[c^{2}( t)\sigma^{2}(t)+\sigma_{K}^{2}(t)\right]\mathbf{I}_{D})\) for a given data sample \(\mathbf{x}_{0}\) and time point \(t\in(0,T]\), in case that \(\sigma^{2}(t)\) and \(\sigma_{K}^{2}(t)\) have a closed form solution. For a complete characterization of the marginal covariance matrix \(\mathbf{\Sigma}_{t}\) of the conditioned augmented forward process we calculate by Ito isometry with \(X=X_{j}\) and \(Y^{l}=Y^{l}_{j}\) for all \(1\leq j\leq D\), \(1\leq l\leq K\) and any \(t\in[0,T]\)

\[\mathbb{E}\left[X_{t}Y^{l}_{t}\right]=c(t)\int_{0}^{t}\alpha(t,s)e^{-\gamma_{k }(t-s)}\mathrm{d}s+c(t)\sum_{l=1}^{K}\frac{\omega_{k}\gamma_{k}}{\gamma_{k}+ \gamma_{l}}e^{-\gamma_{l}t}\int_{0}^{t}\frac{g(s)}{c(s)}e^{-s\gamma_{k}}ds \tag{61}\]

and

\[\mathbb{E}\left[Y^{k}_{t}Y^{l}_{t}\right]=\frac{e^{-(\gamma_{k}+\gamma_{l})s}} {\gamma_{k}+\gamma_{l}}+\frac{1-e^{-(\gamma_{k}+\gamma_{l})t}}{\gamma_{k}+ \gamma_{l}}=\frac{1}{\gamma_{k}+\gamma_{l}} \tag{62}\]

reducing for type II fBM to

\[\mathbb{E}\left[X_{t}Y^{l}_{t}\right]=c(t)\int_{0}^{t}\alpha(t,s)e^{-\gamma_{ k}(t-s)}\mathrm{d}s\quad\text{and}\quad\mathbb{E}\left[Y^{k}_{t}Y^{l}_{t} \right]=\frac{1-e^{-(\gamma_{k}+\gamma_{l})t}}{\gamma_{k}+\gamma_{l}}. \tag{63}\]

We denote in the following the stacked vector of the augmenting processes by

\[\mathbf{Y}_{t}^{[K]}=(Y^{1}_{t,1},Y^{2}_{t,1},...,Y^{K}_{t,1},Y^{1}_{t,2},Y^{2 }_{t,2},...,Y^{K}_{t,2},....,Y^{1}_{t,D},Y^{2}_{t,D},...,Y^{K}_{t,D})\in\mathbb{ R}^{D(K+1)}. \tag{64}\]

The random vector \(\mathbf{Y}_{t}^{[K]}\) is a centered Gaussian process with covariance matrix

\[\mathbf{\Lambda}_{t}=diag(\mathbf{\Sigma}_{t}^{\mathbf{y}},...,\mathbf{\Sigma}_{t}^{ \mathbf{y}})\in\mathbb{R}^{D\cdot K,D\cdot K},\quad\mathbf{\Sigma}_{t}^{\mathbf{y }}\in\mathbb{R}^{K,K},\quad\left[\mathbf{\Sigma}_{t}^{\mathbf{y}}\right]_{k,l}= \mathbb{E}\left[Y^{k}_{t}Y^{l}_{t}\right] \tag{65}\]

where \(\mathbf{\Sigma}_{t}^{\mathbf{y}}\) does not depend on the dimension \(1\leq j\leq D\) and we write \(q_{t}\) for the multivariate Gaussian density of \(\mathbf{Y}_{t}^{[K]}\). Since we know the distribution of \(\mathbf{Y}_{0}^{[K]}\), we can directly calculate the corresponding score function by

\[\nabla_{\mathbf{y}^{[K]}}\log q_{t}\left(\mathbf{Y}_{t}^{[K]}\right)=-\mathbf{ \Lambda}_{t}^{-1}\mathbf{Y}_{t}^{[K]}. \tag{66}\]

### Estimating the score via augmented score matching loss

Conditioning \(\mathbf{Z}_{t}\) on \(\mathbf{x}_{0}\sim p_{0}\)_and_ a realisation \(\mathbf{y}_{t}^{[K]}\) of the stacked augmenting processes \(\mathbf{Y}_{t}^{[K]}\) defined in eq. (64) at fixed time \(t\in[0,T]\) results in the Gaussian vector \(\tilde{\mathbf{X}}_{t}\sim\mathcal{N}(\mathbf{\tilde{m}}_{t},\tilde{\mathbf{\Sigma}}_ {t})\) with mean

\[\mathbf{\tilde{m}}_{t}=c(t)\mathbf{x}_{0}+\sum_{k=1}^{K}\eta_{t}^{k}\mathbf{y}_{t}^ {k},\quad\text{where}\quad\eta_{t}^{k}=\sum_{l=1}^{K}\mathbb{E}\left[X_{t}Y^{ l}_{t}\right]\left[\left(\mathbf{\Sigma}_{t}^{\mathbf{y}}\right)^{-1}\right]_{l,k} \tag{67}\]

and covariance

\[\mathbf{\tilde{\Sigma}}_{t}=\left(c^{2}(t)\sigma^{2}(t)-\tau_{t}^{2}\right) \mathbf{I}_{d},\quad\text{where}\quad\tau_{t}^{2}=\sum_{k=1}^{K}\eta_{t}^{k} \mathbb{E}\left[X_{t}Y^{k}_{t}\right]. \tag{68}\]

We denote with \(\nabla_{\mathbf{x}}\log p_{0t}\) the conditional score function of \(\tilde{\mathbf{X}}_{t}\) and calculate for the gradient w.r.t. \(\mathbf{x}=(x_{1},...,x_{D})\in\mathbb{R}^{D}\)

\[\nabla_{\mathbf{x}}\log p_{0t}(\mathbf{x}|\mathbf{y}_{t}^{[K]},\mathbf{x}_{0})= -\mathbf{\tilde{\Sigma}}_{t}^{-1}(\mathbf{x}-\mathbf{\tilde{m}}_{t})=-\frac{(\mathbf{x }-\mathbf{\tilde{m}}_{t})}{(c^{2}(t)\sigma^{2}(t)-\tau_{t}^{2})}. \tag{69}\]and for the gradient w.r.t. \(\mathbf{y}^{k}=(y_{1}^{k},...,y_{D}^{k})\in\mathbb{R}^{D}\)

\[\nabla_{\mathbf{y}^{k}}\log p_{0t}(\mathbf{x}|\mathbf{y}_{t}^{[K]},\mathbf{x}_{0}) =-\frac{1}{2}\nabla_{\mathbf{y}^{k}}\left[(\mathbf{x}-\boldsymbol{ \tilde{m}}_{t})^{T}\boldsymbol{\tilde{\Sigma}}_{t}^{-1}(\mathbf{x}-\boldsymbol{ \tilde{m}}_{t})\right] \tag{70}\] \[=-\eta_{t}^{k}\nabla_{\mathbf{x}}\log p_{0t}(\mathbf{x}|\mathbf{y }_{t}^{[K]},\mathbf{x}_{0}). \tag{71}\]

Deploying this relation of \(\nabla_{\mathbf{x}}\log p_{0t}\) and \(\nabla_{\mathbf{y}^{k}}\log p_{0t}\) we derive the _augmenting score matching loss_ that reduces the dimensionality of the score model we have to learn to the dimensionality of the data distribution and results in a score model guided by the the known score function \(\nabla_{\mathbf{y}^{[K]}}\log q_{t}\).

**Proposition A.4** (Optimal Score Model).: _Assume that \(s_{\boldsymbol{\theta}}\) is optimal w.r.t. the augmented score matching loss \(\mathcal{L}\). The score model_

\[S_{\boldsymbol{\theta}}(\mathbf{Z}_{t},t):=\left(s_{\boldsymbol{\theta}}( \mathbf{X}_{t}-\sum_{k}\eta_{t}^{k}\mathbf{Y}_{t}^{k},t),-\eta_{t}^{1}s_{ \boldsymbol{\theta}}(\mathbf{X}_{t}-\sum_{k}\eta_{t}^{k}\mathbf{Y}_{t}^{k},t),...,-\eta_{t}^{K}s_{\boldsymbol{\theta}}(\mathbf{X}_{t}-\sum_{k}\eta\mathbf{Y} _{t}^{k},t)\right)\]

_yields the optimal \(L^{2}(\mathbb{P})\) approximation of \(\nabla_{\mathbf{x}}\log p_{t}(\mathbf{Z}_{t})\) via_

\[S_{\boldsymbol{\theta}}(\mathbf{Z}_{t},t)+\nabla_{\mathbf{z}}\log q_{t}( \mathbf{Y}_{t}^{[K]})\approx\nabla_{\mathbf{z}}\log p_{t}(\mathbf{Z}_{t}). \tag{72}\]

Proof.: Fix \(t\in[0,T]\). We write \(p_{t}^{aug}\) for the density of \(\mathbf{Z}_{t}\), \(p_{0t}^{aug}\) for the conditional density of \(\mathbf{Z}_{t}\) on \(\mathbf{X}_{0}\), \(p_{0t}\) for the density of \(\widetilde{\mathbf{X}}_{t}\) and \(q_{0t}\) for the conditional density of \(\mathbf{Y}_{t}^{[K]}\) on \(\mathbf{X}_{0}\). First note that \(\mathbf{Y}_{t}^{[K]}\) and \(\mathbf{X}_{0}\) are independent by assumption and hence \(q_{t}=q_{0t}\). By direct calculations we find

\[\nabla_{\mathbf{x}}\log p_{t}^{aug}(\mathbf{Z}_{t}) =\mathbb{E}_{(\mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{Y}_{t}^{[K]})} \left[\nabla_{\mathbf{x}}\log p_{0t}^{aug}(\mathbf{Z}_{t}|\mathbf{X}_{0})\right] \tag{73}\] \[=\mathbb{E}_{(\mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{Y}_{t}^{[K]}) }\left[\nabla_{\mathbf{x}}\log\left(p_{0t}(\mathbf{X}_{t}|\mathbf{Y}_{t}^{[K]},\mathbf{X}_{0})q_{0t}(\mathbf{Y}_{t}^{[K]}|\mathbf{X}_{0})\right)\right]\] (74) \[=\mathbb{E}_{(\mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{Y}_{t}^{[K]}) }\left[\nabla_{\mathbf{x}}\log p_{0t}(\mathbf{X}_{t}|\mathbf{Y}_{t}^{[K]}, \mathbf{X}_{0})+\underbrace{\nabla_{\mathbf{x}}\log q_{t}(\mathbf{Y}_{t}^{[K] })}_{=\boldsymbol{0}_{d}}\right]\] (75) \[=\mathbb{E}_{(\mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{Y}_{t}^{[K]}) }\left[\nabla_{\mathbf{x}}\log p_{0t}(\mathbf{X}_{t}|\mathbf{Y}_{t}^{[K]}, \mathbf{X}_{0})\right]\] (76) \[\stackrel{{\eqref{eq:p_t}}}{{=}}\mathbb{E}_{( \mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{Y}_{t}^{[K]})}\left[\frac{\mathbf{X}_{t}- \sum_{k}\eta_{t}^{k}\mathbf{Y}_{t}^{k}-c(t)\mathbf{X}_{0}}{c^{2}(t)\sigma^{2}( t)-\tau_{t}^{2}}\right]. \tag{77}\]

Hence the best \(L^{2}(\mathbb{P})\)-approximation of \(\nabla_{\mathbf{x}}\log p_{t}^{aug}(\mathbf{Z}_{t})\) is a minimizer of the augmented score matching loss by

\[\nabla_{\mathbf{x}}\log p_{t}^{aug}(\mathbf{Z}_{t})\stackrel{{ \eqref{eq:p_t}}}{{=}}\mathbb{E}_{(\mathbf{X}_{0}|\mathbf{X}_{t}, \mathbf{Y}_{t}^{[K]})}\left[\frac{\mathbf{X}_{t}-\sum_{k}\eta_{t}^{k}\mathbf{ Y}_{t}^{k}-c(t)\mathbf{X}_{0}}{c^{2}(t)\sigma^{2}(t)-\tau_{t}^{2}}\right] \tag{78}\] \[=\operatorname*{arg\,min}_{s\boldsymbol{\theta}}\mathbb{E}_{( \mathbf{X}_{0},\mathbf{Y}_{t}^{[K]})}\mathbb{E}_{(\mathbf{X}_{t}|\mathbf{Y}_{t} ^{[K]},\mathbf{X}_{0})}\left[\left\|s_{\boldsymbol{\theta}}(\mathbf{X}_{t}- \sum_{k=1}^{K}\eta_{t}^{k}\mathbf{Y}_{t}^{k},t)-\frac{\mathbf{X}_{t}-\sum_{k} \eta_{t}^{k}\mathbf{Y}_{t}^{k}-c(t)\mathbf{X}_{0}}{c^{2}(t)\sigma^{2}(t)-\tau_ {t}^{2}}\right\|^{2}\right]\] (79) \[\stackrel{{\eqref{eq:p_t}}}{{=}}\operatorname*{arg \,min}_{s\boldsymbol{\theta}}\mathbb{E}_{(\mathbf{X}_{0},\mathbf{Y}_{t}^{[K]}) }\mathbb{E}_{(\mathbf{X}_{t}|\mathbf{Y}_{t}^{[K]},\mathbf{X}_{0})}\left[\left\| \left|s_{\boldsymbol{\theta}}(\mathbf{X}_{t}-\sum_{k=1}^{K}\eta_{t}^{k}\mathbf{ Y}_{t}^{k},t)-\nabla_{\mathbf{x}}\log p_{0t}(\mathbf{X}_{t}|\mathbf{Y}_{t}^{[K]}, \mathbf{X}_{0})\right\|^{2}\right] \tag{80}\]

Assume now that \(s_{\boldsymbol{\theta}}\) is a minimizer of the augmented score matching loss. Similar to the calculation above we have

\[\nabla_{\mathbf{y}^{k}}\log p_{t}^{aug}(\mathbf{Z}_{t}) =\mathbb{E}_{(\mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{Y}_{t}^{[K]})} \left[\nabla_{\mathbf{y}^{k}}\log p_{0t}^{aug}(\mathbf{Z}_{t}|\mathbf{X}_{0})\right] \tag{81}\] \[=\mathbb{E}_{(\mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{Y}_{t}^{[K]}) }\left[\nabla_{\mathbf{y}^{k}}\log\left(p_{0t}(\mathbf{X}_{t}|\mathbf{Y}_{t}^{[K]}, \mathbf{X}_{0})q_{0t}(\mathbf{Y}_{t}^{[K]}|\mathbf{X}_{0})\right)\right]\] (82) \[=\mathbb{E}_{(\mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{Y}_{t}^{[K]}) }\left[\nabla_{\mathbf{y}^{k}}\log p_{0t}(\mathbf{X}_{t}|\mathbf{Y}_{t}^{[K]}, \mathbf{X}_{0})+\nabla_{\mathbf{y}^{k}}\log q_{t}(\mathbf{Y}_{t}^{[K]})\right]\] (83) \[\stackrel{{\eqref{eq:p_t}}}{{=}}-\eta_{t}^{k}\mathbb{E}_{( \mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{Y}_{t}_{t}^{[K]})}\left[\nabla_{\mathbf{x}} \log p_{0t}(\mathbf{X}_{t}|\mathbf{Y}_{t}^{[K]},\mathbf{X}_{0})\right]+\nabla_{ \mathbf{y}^{k}}\log q_{t}(\mathbf{Y}_{t}^{[K]}) \tag{84}\]and hence \(-\eta_{t}^{k}s_{\mathbf{\theta}}(\mathbf{X}_{t}-\sum_{k}\eta_{t}^{k}\mathbf{Y}_{t}^{k} )+\nabla_{\mathbf{y}^{k}}\log q_{t}(\mathbf{Y}_{t}^{[K]})\) is the best approximation of \(\nabla_{\mathbf{y}^{k}}\log p_{t}^{aug}(\mathbf{Z}_{t})\) in \(L^{2}(\mathbb{P})\) and the score model

\[S_{\theta}(\mathbf{Z}_{t},t):=\left(s_{\theta}(\mathbf{X}_{t}-\sum_{k}\eta_{t} ^{k}\mathbf{Y}_{t}^{k},t),-\eta_{t}^{1}s_{\theta}(\mathbf{X}_{t}-\sum_{k}\eta_ {t}^{k}\mathbf{Y}_{t}^{k},t),...,-\eta_{t}^{K}s_{\theta}(\mathbf{X}_{t}-\sum_ {k}\eta\mathbf{Y}_{t}^{k},t)\right)\]

yields the best \(L^{2}(\mathbb{P})\)-approximator of \(\nabla_{\mathbf{z}}\log p_{t}\) via

\[S_{\mathbf{\theta}}(\mathbf{Z}_{t},t)+\nabla_{\mathbf{z}}\log q_{t}(\mathbf{Y}_{t} ^{[K]})\approx\nabla_{\mathbf{z}}\log p_{t}(\mathbf{Z}_{t}). \tag{85}\]

## Appendix B Forward sampling

We assume throughout this section type II fBM. Given the marginal covariance matrix \(\mathbf{\Sigma}_{t}\) of \(\mathbf{Z}_{t}|\mathbf{x}_{0}\) we uniformly sample first a time point \(t\in(0,T]\) and second \(\mathbf{Z}_{t}\sim\mathcal{N}(\hat{\mathbf{z}}_{t},\mathbf{\Sigma}_{t})\) with

\[\hat{\mathbf{z}}_{t}=(c(t)\mathbf{x}_{0,1},0,...,0,c(t)\mathbf{x}_{0,2},0,...,0,...,...,...,c(t)\mathbf{x}_{0,D},0,...0)\in\mathbb{R}^{D(K+1)} \tag{86}\]

where we use \(\mathbb{E}\left[\mathbf{X}_{t}|\mathbf{x}_{0}\right]=c(t)\mathbf{x}_{0}\) and \(\mathbb{E}\left[\mathbf{Y}_{t}^{k}\right]=\mathbf{0}_{D}\). In the following we characterize further the entries of the marginal covariance matrix \(\mathbf{\Sigma}_{t}\). The calculations in this section are straightforward; nevertheless, we present them in full detail to facilitate easy understanding for the interested reader. We begin with rewriting \(\sigma^{2}\) from Proposition 4.2 given by

\[\sigma^{2}(t)=c^{2}(t)\int_{0}^{t}\alpha^{2}(t,s)ds \tag{87}\]

where

\[\alpha(t,s) =\bar{\omega}\frac{g(s)}{c(s)}-\sum_{k=1}^{K}\omega_{k}\gamma_{k} \int_{s}^{t}\frac{g(u)}{c(u)}e^{-\gamma_{k}(u-s)}du \tag{88}\] \[=\sum_{k=1}^{K}\omega_{k}\underbrace{\left(\frac{g(s)}{c(s)}- \gamma_{k}\int_{s}^{t}\frac{g(u)}{c(u)}e^{-\gamma_{k}(u-s)}du\right)}_{=: \alpha_{k}(t,s)}. \tag{89}\]

With

\[f_{k}(u,s):=\frac{g(u)}{c(u)}e^{-\gamma_{k}(u-s)}\quad\text{and}\quad I_{k}(t,s):=\int_{s}^{t}f_{k}(u,s)du \tag{90}\]

we have

\[\sigma_{t}^{2} =c^{2}(t)\int_{0}^{t}\alpha^{2}(t,s)ds \tag{91}\] \[=c^{2}(t)\int_{0}^{t}\left[\sum_{k=1}^{K}\omega_{k}\left(\frac{g (s)}{c(s)}-\gamma_{k}\int_{s}^{t}f_{k}(u,s)du\right)\right]^{2}ds\] (92) \[=c^{2}(t)\int_{0}^{t}\left(\sum_{k=1}^{K}\omega_{k}\alpha_{k}(t,s)\right)^{2}ds\] (93) \[=c^{2}(t)\int_{0}^{t}\sum_{i=1,j=1}^{K}\omega_{i}\omega_{j}\alpha _{i}(t,s)\alpha_{j}(t,s)ds\] (94) \[=\sum_{i=1,j=1}^{K}\omega_{i}\omega_{j}c^{2}(t)\int_{0}^{t}\alpha _{i}(t,s)\alpha_{j}(t,s)ds\] (95) \[=\sum_{i=1,j=1}^{K}\omega_{i}\omega_{j}c^{2}(t)\int_{0}^{t}\left( \frac{g(s)}{c(s)}-\gamma_{i}I_{i}(t,s)\right)\left(\frac{g(s)}{c(s)}-\gamma_{ j}I_{j}(t,s)\right)ds\] (96) \[=\sum_{i,j=1}^{K}\omega_{i}\omega_{j}\left\{\text{var}_{B}(t)-c^ {2}(t)\int_{0}^{t}\left[\frac{g(s)}{c(s)}\big{(}\gamma_{i}I_{i}(t,s)+\gamma_{ j}I_{j}(t,s)\big{)}-\gamma_{i}\gamma_{j}I_{i}(t,s)I_{j}(t,s)\right]ds\right\}, \tag{97}\]where

\[\text{var}_{B}(t)=c^{2}(t)\int_{0}^{t}\frac{g^{2}(s)}{c^{2}(s)}ds \tag{98}\]

corresponds to the purely Brownian marginal variance, explicitly calculated for VE and VP in Song et al. [16]. Using the above derivation, we derive the closed form variance schedule for FVE dynamics.

**Fractional Variance Exploding** Fix \(\sigma_{\max}>\sigma_{\min}>0\) and define \(r:=\frac{\sigma_{\max}}{\sigma_{\min}}\). Following Song et al. [16] we set

\[\mu(t)\equiv 0\quad\text{and}\quad g(t)=ar^{t}\quad\text{with}\quad a=\sigma_{ min}\sqrt{2\log(r)} \tag{99}\]

such that \(c(t)=\exp(0)=1\) and calculate

\[I_{k}(t,s) =\int_{s}^{t}f_{k}(u,s)du=\int_{s}^{t}ar^{u}e^{-\gamma_{k}(u-s)}du =F(t)-F(s) \tag{100}\] \[=\underbrace{\frac{a}{\ln(r)-\gamma_{k}}}_{a_{k}}\left(e^{\ln(r) t-\gamma_{k}t+\gamma_{k}s}-e^{\ln(r)s}\right)=a_{k}\left(r^{t}e^{-\gamma_{k}(t-s)} -r^{s}\right), \tag{101}\]

since the derivative of \(F(u)=a_{k}r^{u}e^{-\gamma_{k}(u-s)}\) is given by

\[\frac{d}{du}F(u) =\frac{d}{du}\left[a_{k}r^{u}e^{-\gamma_{k}(u-s)}\right]=a_{k}r^ {u}\ln(r)e^{-\gamma_{k}(u-s)}+a_{k}r^{u}e^{-\gamma_{k}(u-s)}(-\gamma_{k}) \tag{102}\] \[=\frac{a}{\ln(r)-\gamma_{k}}(\ln(r)-\gamma_{k})(r^{u}e^{-\gamma_{ k}(u-s)})=ar^{u}e^{-\gamma_{k}(u-s)}. \tag{103}\]

We calculate for the variance of \(X_{t}|\mathbf{x}_{0}\)

\[\mathbb{V}\left[X_{t}|\mathbf{x}_{0}\right]=\sum_{i,j=1}^{K}\omega _{i}\omega_{j}\{var_{B}(t)-a\gamma_{i}\underbrace{\int_{0}^{t}r^{s}I_{i}(t,s) ds}_{J_{i}(t)}-a\gamma_{j}\underbrace{\int_{0}^{t}r^{s}I_{j}(t,s)ds}_{J_{j}(t)} \tag{104}\] \[\qquad\qquad\qquad\qquad+\gamma_{i}\gamma_{j}\underbrace{\int_{0 }^{t}I_{i}(t,s)I_{j}(t,s)ds}_{=J_{i,j}(t)} \tag{105}\]

with

\[J_{k}(t) =a_{k}\int_{0}^{t}r^{s}\left(r^{t}e^{-\gamma_{k}(t-s)}-r^{s} \right)ds=a_{k}\int_{0}^{t}r^{t+s}e^{-\gamma_{k}(t-s)}ds-a_{k}\int_{0}^{t}r^{2 s}ds \tag{106}\] \[=a_{k}\left[F_{1}(t)-F_{1}(0)\right]-a_{k}\left[F_{2}(t)-F_{2}(0) \right]=a_{k}\left[\frac{r^{2t}-r^{t}e^{-\gamma_{k}t}}{\ln{(r)}+\gamma_{\text {k}}}-\frac{r^{2t}-1}{2\ln(r)}\right], \tag{107}\]

since

\[\frac{d}{ds}F_{1}(s) =\frac{\left(r^{t+s}\ln(r)e^{-\gamma_{k}(t-s)}+r^{t+s}e^{-\gamma_ {k}(t-s)}(\gamma_{k})\right)}{\ln(r)+\gamma_{k}}=r^{t+s}e^{-\gamma_{k}(t-s)}, \tag{108}\] \[\frac{d}{ds}F_{2}(s) =\frac{d}{ds}\left[\frac{r^{2s}}{2\ln(r)}\right]=\frac{r^{2s}\ln( r)2}{2\ln(r)}=r^{2s}. \tag{109}\]

Finally

\[J_{i,j}(t) =a_{i}a_{j}\int_{0}^{t}\left(r^{t}e^{-\gamma_{i}(t-s)}-r^{s} \right)\left(r^{t}e^{-\gamma_{j}(t-s)}-r^{s}\right)ds \tag{110}\] \[=a_{i}a_{j}\left[\left(\frac{r^{2t}\left(1-e^{-t(\gamma_{i}+ \gamma_{j})}\right)}{\gamma_{i}+\gamma_{j}}\right)-\frac{r^{2t}-r^{t}e^{- \gamma_{i}t}}{\gamma_{i}+\ln(r)}-\frac{r^{2t}-r^{t}e^{-\gamma_{j}t}}{\gamma_{ j}+\ln(r)}+\frac{r^{2t}-1}{2\ln(r)}\right]. \tag{111}\]We calculate the covariance of \(X_{t}|\mathbf{x}_{0}\) and \(Y_{t}^{l}\)

\[cov(X_{t}|\mathbf{x}_{0},Y_{t}^{l}) =c(t)\int_{0}^{t}\alpha(t,s)e^{-\gamma_{l}(t-s)}ds \tag{112}\] \[=\int_{0}^{t}\sum_{k=1}^{K}\omega_{k}\left[\frac{g(s)}{c(s)}- \gamma_{k}\int_{s}^{t}f_{k}(u,s)du\right]e^{-\gamma_{l}(t-s)}ds\] (113) \[=\sum_{k=1}^{K}\omega_{k}\left[a\int_{0}^{t}r^{s}e^{-\gamma_{l}(t- s)}ds-\gamma_{k}\int_{0}^{t}\int_{s}^{t}f_{k}(u,s)due^{-\gamma_{l}(t-s)}ds\right]\] (114) \[=\sum_{k=1}^{K}\omega_{k}\left[a\int_{0}^{t}r^{s}e^{-\gamma_{l}(t -s)}ds-\gamma_{k}a_{k}\int_{0}^{t}\left(r^{t}e^{-\gamma_{k}(t-s)}-r^{s}\right) e^{-\gamma_{l}(t-s)}ds\right]\] (115) \[=\sum_{k=1}^{K}\omega_{k}\left[ae^{-\gamma_{t}t}\int_{0}^{t}r^{s} e^{\gamma_{l}s}ds-\gamma_{k}a_{k}\int_{0}^{t}\left(r^{t}e^{-\gamma_{k}(t-s)}-r ^{s}\right)e^{-\gamma_{l}(t-s)}ds\right]\] (116) \[=\sum_{k=1}^{K}\omega_{k}\left[(a+a_{k}\gamma_{k})\frac{(r^{t}-e^ {-\gamma_{l}t})}{\gamma_{l}+\ln(r)}-\gamma_{k}a_{k}\frac{r^{t}\left(1-e^{-t( \gamma_{k}+\gamma_{l})}\right)}{\gamma_{k}+\gamma_{l}}\right]. \tag{117}\]

**Fractional Variance Preserving** To the best of our knowledge, there is no closed form solution for \(\int_{s}^{t}f_{k}(u,s)du\) for the dynamics of FVP. In this case, we numerically solve an ODE to determine the marginal covariance matrix of the conditional augmented forward process.

**General Dynamics**. The covariance matrix of the conditional augmented forward process with dynamics

\[d\mathbf{Z}_{t}=\mathbf{F}(t)\mathbf{Z}_{t}dt+\mathbf{G}(t)\mathrm{d}\mathbf{B }_{t}, \tag{118}\]

solves the ODE

\[\partial_{t}\boldsymbol{\Sigma}_{t}=\mathbf{F}(t)\boldsymbol{\Sigma}_{t}+ \boldsymbol{\Sigma}_{t}\mathbf{F}(t)^{T}+\mathbf{G}(t)\mathbf{G}(t)^{T}, \tag{119}\]

lacking in general a closed form solution [43] in contrast to the setting of Song et al. [16]. This approach is applicable for any choice of \(\mu\) and \(g\) in the forward dynamics, but depending on the choice of drift and diffusion function it might not yield a numerically stable solution. We empirically observe in Figure 5 that the analytical solution for FVE and the numerical approximation of the variance schedule, determined by solving eq. (119) do not differ significantly.

**Variance schedules**. We normalize the variance schedule of FVE and FVP dynamics such that the variance at \(t=0\) and at \(t=T\) is equal to the variance used in the purely Brownian setting of VE and VP dynamics. For both FVE and FVP dynamics we calculate \(\boldsymbol{\tilde{\omega}}\) according to Proposition 3.3 and determine \(\tilde{\sigma}_{T}^{2}\) and define \(\boldsymbol{\omega}=\boldsymbol{\tilde{\omega}}/\tilde{\sigma}_{T}^{2}\) to weight the OU-processes. By doing so, the terminal variance remains the same throughout different choices of \(H\), as empirically confirmed in Figure 6.

Figure 5: Analytical solution (blue) used by our method for FVE dynamics with \(K=5\) and \(H=0.5\) compared to the approximated solution (dashed red) resulting from solving ODE (119).

In Figure 6 we observe for FVE dynamics that not only the terminal variance is the same across different choices of \(H\) but also the shape of the variance schedule. For FVP dynamics, the shape of the variance schedule shifts with different values of \(H\), approaching a nearly linear schedule for \(H=0.1\), while \(H=0.9\) offers a decreasing variance towards the end near \(t=T\).

## Appendix C Implementation details

We used for all experiments a conditional U-Net [70] architecture and the Adam optimizer [71] with PyTorch's OneCycle learning rate scheduler [72]. On MNIST we trained without exponential moving average (EMA) while on CIFAR10 we conducted experiments with and without EMA.

**Set up on MNIST**. We used an attention resolution of \([4,2]\), \(3\) resnet blocks and a channel multiplication of \([1,2,2,2,2]\) and trained with a maximal learning rate of \(10^{-4}\) for \(50k\) iterations and a batch size of \(1024\). For all MNIST training runs we used one A100 GPU per run, taking approximately \(17\) hours.

**Set up on CIFAR10**. We used an attention resolution of \([8]\), \(4\) resnet blocks and a channel multiplication of \([1,2,2,2,2]\). For the experiments without EMA, we used the same setup as with MNIST, but trained the models in parallel on two A100 GPUs for \(300k\) iterations with an effective batch size of \(1024\). When training with EMA, we followed the set up of Song et al. [16] using an EMA decay of \(0.9999\) for all FVP dynamics and an EMA decay of \(0.999\) for all FVE dynamics. In contrast to Song et al. [16] we used PyTorch's OneCycleLR learning rate scheduler with a maximal learning rate of \(2\cdot 10^{-4}\) and trained only for \(1mio\) iterations instead of the \(1.3mio\) iterations in Song et al. [16].

## Appendix D Additional experiments

In addition to the experiments presented in the main part, we provide additional results here, including a full evaluation of FVE dynamics on MNIST, as well as training on CIFAR10 without EMA.

**Evaluation of different Hurst indices of FVE dynamics on MNIST**. In Table 4 we provide the evaluation of FVE dynamics. For the ease of comparisan, we include the quantitative results on FVP dynamics already presented in the main part. For FVE dynamics both, the super-diffusive regime and the sub-diffusive regime achieve a higher FID as the purely Brownian dynamics for \(K=1,2\) throughout all tested Hurst indices and for \(K=3\) throughout all tested Hurst indices except for \(H=0.9\) with a higher pixel-wise diversity in the sub-diffusive regime of \(H<0.5\).

**Training on CIFAR10 without EMA**. As Song et al. [16] point out, the empirically optimal EMA decay rate for VP dynamics differs from that for VE dynamics. Since we do not have the computational resources to optimize the EMA decay rate for every configuration of our framework, we evaluated it in line with Song et al. [16] using a consistent EMA decay rate of \(0.999\) across all configurations of FVE dynamics and \(0.9999\) across all configurations of FVP dynamics. Nevertheless, because the optimal EMA decay rate appears to depend on the dynamics of the underlying stochastic

Figure 6: Normalized variance schedules for \(K=5\) over time. (a) Variance schedules of FVE dynamics, calculated in closed form according to the derived formulas. The shape of the schedule is preserved throughout different values of \(H\). (b) Variance schedules of FVP dynamics numerically approximated. The shape of the schedule is shifted for different values of \(H\).

[MISSING_PAGE_FAIL:27]

[MISSING_PAGE_EMPTY:28]

Figure 9: (RHS) Images generated with the purely Brownian driven VP dynamics sampled with PF ODE, a FID of \(5.63\) and pixel-wise diversity of \(3.91\). (LHS) Images generated with \(\text{FVP}(H=0.9,K=2)\) dynamics sampled with PF ODE, a FID of \(12.36\) and pixel-wise diversity of \(4.89\).

## Appendix F Computational cost of augmenting processes

In this section we compare the computation time of GFDM to the purely Brownian setting of traditional diffusion models. For a given Hurst index \(H\in(0,1)\) and a given \(K\), the optimal coefficients \(\omega_{1},...,\omega_{K}\) are calculated only once before training. For completeness of our quantitative compute time evaluation, we provide the average computation time in seconds, needed to compute \(\omega_{1},...,\omega_{K}\) on a GPU Tesla V100 with \(32\) GB RAM. We randomly sample \(1000\) times \(H\sim\mathcal{U}[0.1,0.9]\) for a given \(K\in\{1,2,3,4,5\}\) and report the average computation time in Table 7.

**Computation time during training** The computational difference during training consists of the computation of the covariance matrix \(\mathbf{\Sigma}_{t}\) instead of the marginal variance and sampling from a multivariate Gaussian instead of a univariate Gaussian. Note however, that we only need to calculate \(\mathbf{\Sigma}_{t}\) for \(D=1\) and also sample only once for a given time \(t\) and a given data point. In Table 8 and Table 9 we report the average time of one training step measured in seconds calculated over \(1000\) training steps on CIFAR10. The underlying conditional U-Net has \(58.7mio\) and EMA is applied. The batch size is \(128\) and all computation have been carried out on a GPU Tesla V100 with \(32\) GB RAM.

We observe that the computation time depends only minimaly increases when switching from the original model to the augmented system and increases across FVE and FVP dynamics by at most \(11/1000\) seconds, while the choice of the Hurst index \(H\) has no effect on the computation time.

**Computation time during sampling** Since the augmented system depends for fixed \(K\) only on the approximating coefficients \(\omega_{1},...,\omega_{K}\) it would suffice to report the average sampling time for FVP and FVE dynamics for varying \(K\). Nevertheless, we report for \(H\in\{0.9,0.5,0.1\}\) in Table 10 and Table 11 the average time to sample a batch of \(1000\) images over \(1000\) discretization steps of the reverse-time SDE over \(10\) trials. We observe that the average time in seconds for one sampling step in the reverse dynamics of FVE and FVP dynamics increases for \(K\leq 4\) by at most \(2/100\) seconds. Only for \(K=5\) we observe a significant increase of average sampling time of roughly \(4/10\) seconds.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & \(K=1\) & \(K=2\) & \(K=3\) & \(K=4\) & \(K=5\) \\ \hline
**time** [s] & \(0.0043\) & \(0.0003\) & \(0.0003\) & \(0.0003\) & \(0.0003\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Averaged time in seconds needed before training to calculate for a given \(K\) the optimal approximation coefficients using the approach of Daems et al. [27].

Figure 10: Diversifying effect of the augmenting processes with FVP dynamics on MNIST. The super-diffusive regime with \(H=0.9\): For \(K=5\) instead of \(K=3\) augmenting processes the pixel VS increases from \(24.18\) to \(25.15\).

[MISSING_PAGE_FAIL:31]

Likelihood computation

Given the approximate PF ODE corresponding to the augmented forward process

\[\mathrm{d}\mathbf{z}_{t}=\underbrace{\left\{\mathbf{F}(t)\mathbf{z}_{t}-\frac{1}{ 2}\mathbf{G}(t)\mathbf{G}(t)^{T}\left[S_{\mathbf{\theta}}(\mathbf{z}_{t},t)+\nabla _{\mathbf{z}}\log q_{t}(\mathbf{y}_{t}^{[K]})\right]\right\}}_{:=\overline{ \tilde{\mathbf{z}}_{\mathbf{\theta}}(\mathbf{z}_{t},t)}}dt,\quad t\in[0,T] \tag{120}\]

we estimate according to Song et al. [16] the log-likelihoods of test data \(\mathbf{z}_{0}\) under the learned density \(\tilde{p}_{0}^{aug}\) via

\[\log\tilde{p}_{0}^{aug}(\mathbf{z}_{0})=\log\tilde{p}_{T}^{aug}(\mathbf{z}_{T })+\int_{0}^{T}\nabla\overline{\tilde{\mathbf{\theta}}}(\mathbf{z}_{t},t)\mathrm{ d}t. \tag{121}\]

According to Song et al. [16], we integrate over \([\epsilon,T]\) rather than \([0,T]\), using the same value of \(\epsilon=10^{-3}\), which has been empirically shown to yield the best performance when simulating the SDE. For \(\epsilon\neq 0\) and type II fBM we need to adjust the starting value of the augmenting processes from zero to a jointly sampled vector \(\mathbf{y}_{\epsilon}=(y_{\epsilon}^{1},...,y_{\epsilon}^{K})\sim\mathcal{N}( \mathbf{0}_{K},\mathbf{\Lambda}_{\epsilon})\) with

\[(\mathbf{\Lambda}_{\epsilon})_{k,l}=\mathbb{E}\left[y_{\epsilon}^{k}y_{\epsilon}^{ l}\right]=\int_{0}^{\epsilon}e^{-(\gamma_{k}+\gamma_{l})(\epsilon-s)}\mathrm{d}s= \frac{1-e^{-(\gamma_{k}+\gamma_{l})\epsilon}}{\gamma_{k}+\gamma_{l}}. \tag{122}\]

Using the exact likelihood of \(\mathbf{y}_{\epsilon}\) and the independence of \(\mathbf{y}_{\epsilon}\) and \(\mathbf{x}_{0}\) we have

\[\log\tilde{p}_{0}^{aug}(\mathbf{z}_{\epsilon})=\log\tilde{p}_{0}(\mathbf{x}_{ 0})+\log q_{\epsilon}(\mathbf{y}_{\epsilon}) \tag{123}\]

where \(\tilde{p}_{0}\) is the learned density of \(\mathbf{x}_{0}\) corresponding to \(\mathbf{\theta}\). Hence in total

\[\log\tilde{p}_{0}(\mathbf{x}_{0})\stackrel{{\eqref{eq:log_likelihood_eq}}}{{=}} \log\tilde{p}_{T}^{aug}(\mathbf{z}_{T})+\int_{0}^{T}\nabla\overline{\tilde{\mathbf{ \theta}}}(\mathbf{z}_{t},t)\mathrm{d}t-\log q_{\epsilon}(\mathbf{y}_{\epsilon}) \tag{124}\]

and we define the negative log-likelihoods \(NLLs\) of test data \(\mathbf{x}_{0}\) under the learned density by

\[NLLs(\mathbf{x}_{0},\mathbf{\theta}):=-\log\tilde{p}_{0}^{aug}(\mathbf{z}_{0})+ \log q_{\epsilon}(\mathbf{y}_{\epsilon}). \tag{125}\]

## Appendix H Challenges in the attempt to generalize

In this work, we seek to determine the extent to which the continuous-time framework of score-based generative models can be generalized from an underlying BM to an underlying fBM. For a fBM \(W^{H}\) it is not straightforward to define the forward process

\[X_{t}=X_{0}+\int_{0}^{t}f(X_{s},s)\mathrm{d}s+\int_{0}^{t}g(X_{s},s)\mathrm{d }W_{s}^{H},\quad t\in[0,T] \tag{126}\]

driven by fBM, since fBM is neither a Markov process nor a semimartingale [25], and hence Ito calculus may not be applied, to define the second integral. However, a definition of the integral w.r.t. fBM is established [25, 73] such that the remaining problem is the derivation of the reverse-time model. Following the second and more intuitive derivation of the reverse-time model for BM from Anderson [32], the conditional backward Kolmogorov equation and the unconditional forward Kolmogorov equation are applied. Starting point of the derivation is to rewrite \(p(x_{t},t,x_{s},s)=p(x_{s},s|x_{t},t)p(x_{t},t)\) with Bayes theorem to calculate with the product rule

\[\frac{\partial p(x_{t},t,x_{s},s)}{\partial t}=\frac{\partial p(x_{s},s|x_{t}, t)}{\partial t}p(x_{t},t)+\frac{\partial p(x_{t},t)}{\partial t}p(x_{s},s|x_{t}, t),\quad s\geq t. \tag{127}\]

Replacing \(\frac{\partial p(x_{t},t)}{\partial t}\) with the RHS of the unconditional forward Kolmogorov equation and \(\frac{\partial p(x_{s},s|x_{t},t)}{\partial t}\) with the RHS of the conditional backward Kolmogorov equation one derives an equation that only depends on the joint density \(p(x_{t},t,x_{s},s)\). Using Bayes theorem again leads to a conditional backward Kolmogorov equation for \(p(x_{t},t|x_{s},s)\) that defines the dynamics of the reverse process by the one-to-one correspondence between the conditional backward Kolmogorov equation and the reverse-time SDE [32]. Following these steps for fBM, starting from eq. (127) and deploying the one-to-one correspondence of fBM and the evolution of its density [73], we could replace \(\frac{\partial p(x_{t},t)}{\partial t}\) in (127) by the RHS of

\[\frac{\partial p(x,t)}{\partial t}=\sum_{i=1}^{d}f_{i}(t,x)\frac{\partial p(t,x) }{\partial x_{i}}+Ht^{2H-1}\sum_{i,j=1}^{d}g_{ij}(x,t)\frac{\partial^{2}p(t,x) }{\partial x_{i}\partial x_{j}}. \tag{128}\]

The missing part is however an analogous to the conditional backward Kolmogorov equation to replace \(\frac{\partial p(x_{s},s|x_{t},t)}{\partial t}\) in eq. (127). The derivation of such an equation is to the best of our knowledge yet unsolved problem and hence the limiting factor in the generalization of continuous-time score-based generative models from an underlying BM to an underlying fBM.

## 1 Notational conventions

\begin{tabular}{l|l} \([0,T]\) & Time horizon with terminal time \(T>0\) \\ \(X=(X_{t})_{t\in[0,T]}\) & Stochastic forward process taking values in \(\mathbb{R}\) \\ \(D\in\mathbb{N}\) & Data dimension \\ \(\mathbf{X}\) & Vector valued stochastic forward process \(\mathbf{X}=(\mathbf{X}_{t})_{t\in[0,T]}\) with \(\mathbf{X}_{t}=(X_{t,1},...,X_{t,D})\) \\ \(\overline{\mathbf{X}}\) & Reverse time stochastic process with \(\overline{\mathbf{X}}_{t}=\mathbf{X}_{T-t}\) \\ \(\mathbf{f}\) & Vector valued function \(\mathbf{f}:\mathbb{R}^{D}\times[0,T]\rightarrow\mathbb{R}^{D}\) \\ \(\mu,g\) & Functions \(\mu,g:[0,T]\rightarrow\mathbb{R}\) \\ \(\overline{\mathbf{f}}\) & Reverse time function with \(\overline{\mathbf{f}}(\mathbf{x},t)=\mathbf{f}(\mathbf{x},T-t)\) \\ \(\bar{\mu},\bar{g}\) & Reverse time functions with \(\bar{\mu}(t)=\mu(T-t)\) and \(\bar{g}(t)=g(T-t)\) \\ \(p_{0}\) & Data distribution \\ \(p_{t}\) & Marginal density of (augmented) forward process at \(t\in[0,T]\) \\ \(B\) & Brownian motion (BM) \\ \(H\) & Hurst index \(H\in(0,1)\) \\ \(W^{H}\) & Type I fractional Brownian motion (fBM) \\ \(B^{H}\) & Type II fractional Brownian motion (fBM) \\ \(Y^{\gamma}=(Y^{\gamma}_{t})_{t\in[0,T]}\) & Ornstein-Uhlenbeck (OU) process with speed of mean reversion \(\gamma\in\mathbb{R}\) \\ \(K\in\mathbb{N}\) & Number of augmenting processes \\ \(\gamma_{1},...,\gamma_{K}\) & Geometrically spaced grid \\ \(\omega_{1},...,\omega_{K}\) & Approximation coefficients \\ \(\boldsymbol{\omega}\) & Optimal approximation coefficients \(\boldsymbol{\omega}=(\omega^{\star}_{1},...,\omega^{\star}_{K})\) \\ \(\bar{\boldsymbol{\omega}}\) & Sum of optimal approximation coefficients \\ \(\hat{B}^{H}\) & Markov-approximate fractional Brownian motion (MA-fBM) \\ \(k\) & \(k\in\mathbb{N}\) with \(1\leq k\leq K\) \\ \(Y^{k}\) & OU processes \(Y^{k}=Y^{\gamma_{k}}\) \\ \(\mathbf{Y}^{1},...,\mathbf{Y}^{K}\) & Augmenting processes with \(\mathbf{Y}^{k}=(Y^{k},...,Y^{k})\) \\ \(\mathbf{F},\mathbf{G}\) & Vector valued functions \(\mathbf{F},\mathbf{G}:[0,T]\rightarrow\mathbb{R}^{D\cdot(K+1)}\) \\ \(\overline{\mathbf{F}},\overline{\mathbf{G}}\) & Reverse time vector valued functions with \(\overline{\mathbf{F}}(t)=\mathbf{F}(T-t)\) and \(\overline{\mathbf{G}}(t)=\mathbf{G}(T-t)\) \\ \(\mathbf{Z}\) & By \(\mathbf{Y}^{1},...,\mathbf{Y}^{K}\) augmented forward process \\ \(\mathbf{Y}^{[K]}\) & Stacked vector of augmenting processes \\ \(q_{t}\) & Marginal density of \(\mathbf{Y}^{[K]}\) at \(t\in[0,T]\) \\ \(\boldsymbol{\theta}\) & Weight vector of a neural network \\ \end{tabular}

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We summarize the contribution of our work in the introduction and in the abstract. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We include a section on limitations of our work where we discuss the limitations of our results. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We give a complete proof for our own theoretical results and refer to complete proofs for he theoretical results of others. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We give implementation details in the appendix revealing the used model architecture and training procedures. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We release our code upon publication. Together with the implementation details given in the paper our results can be reproduced. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We describe the hyperparameters in our section on implementation details in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Unfortunately we don not have the computational resources to run all experiments a sufficient number of times to provide statistical certainty. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We reveal the hardware specification we use and report the number of hours of training in our section on implementation details. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our work fully confirms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We include a broader impact statement at the end of our paper discussing potential misuse. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We only feature experiments on small scale dataset up to size \(3\times 32\times 32\). The models trained in this work do not pose such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly cite all research works we build on and use the code of others only according to its license. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? [Yes] Justification: We release our code upon publication alongside a proper documentation under the MIT license. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: our work does not involve crowdsourcing nor research with human subjects.
16. Depending on the country in Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.