# 3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability

 Baohao Liao\({}^{1,2}\)&Christof Monz\({}^{1}\)

\({}^{1}\)Language Technology Lab, University of Amsterdam

\({}^{2}\)eBay Inc., Aachen, Germany

Code: https://github.com/BaohaoLiao/road

Correspondence to b.liao@uva.nl. Please go to https://arxiv.org/abs/2409.00119 for the newest version.

###### Abstract

Parameter-efficient finetuning (PEFT) methods effectively adapt large language models (LLMs) to diverse downstream tasks, reducing storage and GPU memory demands. Despite these advantages, several applications pose new challenges to PEFT beyond mere parameter efficiency. One notable challenge involves the efficient deployment of LLMs equipped with multiple task- or user-specific adapters, particularly when different adapters are needed for distinct requests within the same batch. Another challenge is the interpretability of LLMs, which is crucial for understanding how LLMs function. Previous studies introduced various approaches to address different challenges. In this paper, we introduce a novel method, RoAd, which employs a straightforward 2D rotation to adapt LLMs and addresses all the above challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal performance on GLUE, eight commonsense reasoning tasks and four arithmetic reasoning tasks with \(<0.1\%\) trainable parameters; (2) RoAd facilitates the efficient serving of requests requiring different adapters within a batch, with an overhead comparable to element-wise multiplication instead of batch matrix multiplication; (3) RoAd enhances LLM's interpretability through integration within a framework of distributed interchange intervention, demonstrated via composition experiments.

## 1 Introduction

Large language models (LLMs), trained on extensive web-scale datasets to perform tasks such as predicting masked words [8, 31, 45] or anticipating the next word in a sentence [17, 52, 53],

Figure 1: Performance of various PEFT methods on the GLUE benchmark, eight commonsense reasoning tasks and four arithmetic reasoning tasks with RoBERTa-large or LLaMA-13B.

demonstrate remarkable effectiveness across a range of NLP applications. For tasks where the data distribution diverges from that of the pretraining corpus, finetuning emerges as an effective way to tailor an LLM to specific requirements. Leveraging the capabilities of LLMs, recent studies [13; 14; 22; 23; 25; 27; 42; 60; 62; 65] demonstrate that training only a subset of an LLM's parameters can yield performance on par with full finetuning. This approach, termed parameter-efficient finetuning (PEFT), provides two primary advantages: (1) It reduces the storage requirements for trained parameters, as it necessitates preserving only a universal LLM alongside a minimal set of task-specific parameters; (2) It decreases GPU memory consumption during finetuning, owing to the reduction in optimizer state sizes which correlate directly with the number of trainable parameters.

With the evolution of PEFT, concerns extend beyond mere parameter efficiency. PEFT encounters a variety of challenges brought forth by diverse applications. A significant challenge is the efficient deployment of personalized or task-specific LLMs [25; 57]. These applications frequently require distinct sets of trained parameters for different tasks or users. When multiple users submit requests simultaneously, it becomes crucial to process these requests collectively in a single batch. Given that each request may require a unique set of parameters, using batch matrix multiplication can efficiently handle these requests by leveraging GPU parallelism. However, the batch matrix multiplication still incurs considerable overhead [1; 57], necessitating the exploration of more efficient methods.

Another challenge is the interpretability of LLMs that contain a billion-scale of parameters, making it difficult to explore their mechanism. PEFT provides an alternative approach by constraining the number of trainable parameters, thereby aiding in interpretability. Recent advancements in PEFT methods, particularly those focusing on representation editing [54; 60; 67], can be incorporated within an intervention framework [11]. This integration enhances their capability for interpretability, offering a more manageable means of dissecting the operational intricacies of LLMs.

In this paper, we introduce a novel technique termed 2D rotary adaptation (RoAd) which efficiently adapts LLMs using a minimal number of trainable parameters. Furthermore, RoAd enhances both batching efficiency and composability. Our initial investigation reveals that finetuning primarily alters the angular components of the representations in pretrained LLMs, rather than their magnitudes (Section SS3.1). Based on this observation, we employ a strategy of rotating certain subspaces within the representations to emulate finetuning effects. Specifically, we implement a 2D rotational approach on the representations and develop three distinct variants of RoAd (Section SS3.2).

To assess the efficacy of RoAd, we perform comprehensive evaluations on the GLUE benchmark [56], eight commonsense reasoning tasks and four arithmetic reasoning tasks, utilizing RoBERTa [31] and LLaMA [52; 53] (Section SS4.1). The results consistently show that RoAd surpasses other PEFT methods while maintaining a significantly reduced scale of trainable parameters (\(<0.1\%\)), as depicted in Figure 1. Additionally, RoAd employs element-wise rather than matrix multiplication, which notably improves throughput when serving heterogeneous requests within the same batch, achieving twice the throughput of LoRA [14] (Section SS4.2). Furthermore, RoAd can be seamlessly integrated within an intervention framework [11], thereby enhancing model interpretability. We illustrate this through a composition experiment, demonstrating RoAd's capacity to merge weights trained for different tasks and display a new capability (Section SS4.3).

## 2 Background

In this section, we outline the challenges tackled in this work, illustrating the constraints of existing methods and objectives that drive the development of the proposed method, RoAd.

### Parameter-efficient finetuning (PEFT)

Existing PEFT techniques can be categorized into three groups: adapter-based, prompt-based, and latency-less methods. Adapter-based methods [12; 13; 42] incorporate adapters either in parallel with or sequentially to the existing Transformer [55] modules. This incorporation necessitates modifications to the LLM architecture, consequently adding extra latency during inference. Prompt-based methods [19; 21; 43] enhance the input by appending new trainable tokens, which lengthens the sequence and thereby increases the computational overhead during inference. Latency-less methods, such as LoRA [14] and its variants [22; 27; 65], apply low-rank matrices to adapt the pretrained weights. These matrices can be seamlessly integrated into the existing weight matrices following finetuning, thus preserving the original LLM architecture. Specifically, LoRA adapts an LLM as \(\bm{W}=\bm{W}^{0}+\Delta\bm{W}\), where \(\bm{W}^{0}\in\mathbb{R}^{d_{1}\times d_{2}}\) is the pretrained weight and \(\Delta\bm{W}=\bm{B}\bm{A}\) with \(\bm{B}\in\mathbb{R}^{d_{1}\times r}\), \(\bm{A}\in\mathbb{R}^{r\times d_{2}}\), \(r\ll d_{1}\) and \(r\ll d_{2}\). Our proposed method, RoAd, aligns with the latency-less category and integrates effortlessly into the existing linear layer without imposing additional overhead during inference. Moreover, RoAd demonstrates exceptional parameter efficiency. The quantity of its trainable parameters is equivalent to that of a LoRA module with a rank \(r=0.5\).

**Orthogonal finetuning.** Drawing on the concept of hyperspherical energy and its role in characterizing generalization [28; 29], OFT [44] introduces orthogonal finetuning, an effective PEFT method for finetuning text-to-image diffusion models. Specifically, OFT implements an orthogonal matrix \(\bm{R}\in\mathbb{R}^{d_{1}\times d_{1}}\) to the pretrained weight \(\bm{W}^{0}\), so the input \(\bm{x}\in\mathbb{R}^{d_{1}}\) to a linear layer after adaptation becomes \(\bm{z}=(\bm{R}\bm{W}^{0})^{\top}\bm{x}\). \(\bm{R}\) is parameter-efficient because it is a block-diagonal matrix with \(n\) blocks as \(\bm{R}=\text{diag}(\bm{R}_{1},...,\bm{R}_{i},...,\bm{R}_{n})\), where each block \(\bm{R}_{i}\in\mathbb{R}^{w\times w}\) has a dimension \(w=d_{1}/n\). To maintain orthogonality, \(\bm{R}_{i}\) is derived using Cayley parameterization: \(\bm{R}_{i}=(\bm{I}+\bm{Q}_{i})(\bm{I}-\bm{Q}_{i})^{-1}\) with \(\bm{Q}_{i}\in\mathbb{R}^{w\times w}\) being a skew-symmetric matrix (\(\bm{Q}_{i}=-\bm{Q}_{i}^{\top}\)). In sum, \(\{\bm{Q}_{i}\}_{i=1}^{n}\) serve as the trainable parameters and \(\bm{R}\) is constructed from them with Cayley parameterization. Subsequent advancement, BOFT [30], leverages butterfly factorization to further refine OFT's parameter efficiency. However, both OFT and BOFT, due to their reliance on matrix inversions in the Cayley parameterization and increased storage of intermediate activations, necessitate additional GPU memory and increase training duration compared to other PEFT approaches. Conversely, RoAd, which may be considered as a specialized case of OFT with \(w=2\), offers a faster and more memory-efficient solution by inherently maintaining orthogonality without requiring further parameterization.

### Batching

Batching in this context refers to processing multiple heterogeneous requests, each requiring different adapters2 for inference. This scenario commonly arises when serving personalized or task-specific LLMs. Specifically, we consider a setup where distinct adapters instead of a shared adapter are finetuned for various tasks to achieve optimal performance. During inference, each request in a batch pertains to a different task and necessitates a unique adapter.

Footnote 2: Adapter here means the trained parameters since LoRAâ€™s architecture is also similar to an adapter.

Consider that we have finetuned distinct LoRA modules for \(b\) tasks, denoted as \(\{\bm{A}_{i},\bm{B}_{i}\}_{i=1}^{b}\). For a batch of \(b\) requests represented as \(\bm{X}\in\mathbb{R}^{b\times l\times d_{1}}\), where \(l\) is the maximum sequence length across the requests, each request requires a different LoRA module. To exploit the parallel processing capabilities of GPUs, the output \(\bm{Z}\) of a linear layer can be computed as follows: First, the output from the pretrained layer is computed as \(\bm{Z}^{0}=torch.mm(\bm{X},\bm{W}^{0})\). Subsequently, the intermediate output from the first low-rank matrix, \(\hat{\bm{B}}\in\mathbb{R}^{b\times d_{1}\times r}\) (a concatenation of \(\{\bm{B}_{i}\}_{i=1}^{b}\)), is obtained as \(\bm{Z}_{0}^{1}=torch.bmm(\bm{X},\hat{\bm{B}})\). The output from the second low-rank matrix, \(\hat{\bm{A}}\in\mathbb{R}^{b\times r\times d_{2}}\) (a concatenation of \(\{\bm{A}_{i}\}_{i=1}^{b}\)), follows as \(\bm{Z}^{1}=torch.bmm(\bm{Z}_{0}^{1},\hat{\bm{A}})\). Finally, these outputs are summed to produce \(\bm{Z}=\bm{Z}^{0}+\bm{Z}^{1}\). It is noteworthy that batch matrix multiplication (BMM), as implemented in \(torch.bmm\), often introduces substantial overhead [1], reducing throughput and increasing latency, which adversely impacts user experience in time-sensitive applications.

In contrast, prompt-based methods circumvent the use of BMM by appending trainable tokens to each request, simplifying the computational process. However, prompt-based methods with long prompt tokens are difficult to optimize, which degrades performance compared to other PEFTs [14; 15]. (IA)3[25] proposes adapting LLM by multiplying the output from a linear layer with a trainable vector, involving only element-wise multiplication for efficient batching. A recent development, FLoRA [58], builds on (IA)3 by employing two low-rank matrices while maintaining element-wise operations. Although our proposed method, RoAd, requires BMM, its sparse structure allows a reformulation of BMM and results in an overhead equivalent to element-wise multiplication.

Footnote 3: footnotemark:

Footnote 4: footnotemark:

### Intervention and composability

Numerous studies [10; 11; 37; 38; 40] have provided support for the linear representation hypothesis [35; 46; 49] that concepts are represented within linear subspaces of neural network representations. To examine if a concept is captured within a linear subspace of a representation, Geiger et al. [11]suggests employing a distributed interchange intervention (DII) defined as:

\[\text{DII}(\bm{b},\bm{s},\bm{R})=\bm{b}+\bm{R}^{\top}(\bm{R}\bm{s}-\bm{R}\bm{b})\] (1)

\(\bm{b}\) denotes the hidden representation generated at row \(i\) and column \(k\) when the model processes an input, while \(\bm{s}\) represents the corresponding representation when the model processes a different input. The matrix \(\bm{R}\in\mathbb{R}^{r\times d_{1}}\), consisting of orthogonal rows, serves as a low-rank projection matrix where \(d_{1}\) is the dimension of the representation and \(r\) is the subspace dimension under intervention. Equation (1) illustrates the application of a DII to \(\bm{b}\) using a counterfactual source representation \(\bm{s}\).3

Footnote 3: We adopt notation systems from Wu et al. [61].

Drawing inspiration from this established framework, a recent study, LoReFT [61], introduces a method for finetuning specific positions of the representations to adapt LLM. This study further demonstrates that several prior approaches of representation editing [60; 67; 54] can be effectively integrated within this framework. Interestingly, the application of RoAd to representations can also be conceptualized as DII, offering interpretability potential. To demonstrate one aspect of interpretability for RoAd, we primarily conduct a qualitative experiment focused on task composition. This experiment involves combining the weights of models trained on distinct tasks to showcase the capability for multitasking learning without the need for additional adaptation [61; 66; 64; 20; 16].

## 3 Method

In this section, we first perform two pilot studies to ascertain the key factor influencing the adaptation of LLMs. Following this, we present our proposed method, the 2D rotary adaptation (RoAd), which serves as an effective PEFT method addressing the various challenges outlined in Section SS2.

### Pilot study

**Study 1: Variations in magnitude and angular displacement.** Assume \(\bm{x}^{0}\), \(\bm{x}\in\mathbb{R}^{d_{1}}\) are representations of the same token from a pretrained and finetuned LLM, respectively. We define the relative change in magnitude as and compute the angular displacement as \(\Delta D=\cos(\bm{x},\bm{x}^{0})\in[-1,1]\). A larger \(\Delta M\) and a smaller \(\Delta D\) indicate more significant changes in magnitude and angular displacement, respectively. Our study involves: (1) finetuning RoBERTa-base [31] on the SST-2 task [50] using either full finetuning or LoRA; (2) extracting representations \(\bm{x}^{0}\) and \(\bm{x}\) from the output of the second-last Transformer block for the [CLS] token across all samples in the development set, followed by computing \(\Delta M\) and \(\Delta D\).4 As depicted in Figure 2 (Left and Middle), there is a more pronounced change in \(\Delta D\) than in \(\Delta M\) for both full finetuning and LoRA.5

Footnote 4: Please refer to Figure B.1 for all layers.

Footnote 5: There are two other interesting observations: (1) An increase in magnitude change correlates with a larger angular displacement; (2) Compared to LoRA, full finetuning has a bigger change in magnitude and angle (for all layers, see Figure B.1), which is in line with a recent finding that LoRA learns less and forgets less [2].

**Study 2: Disentanglement of magnitude and angle.** To ascertain whether angular or magnitude adjustments are more critical for finetuning, we implement a disentanglement study. This involves freezing RoBERTa-base and appending a two-layer classifier on top of it. The first layer of this

Figure 2: Pilot study for the pretrained and finetuned representations. **Left & Middle**: The change in magnitude and angle of representations between pretrained and finetuned LLM using full finetuning or LoRA. **Right**: The disentanglement experiment of magnitude and angle of pretrained representation.

classifier incorporates a weight matrix \(\bm{W}\in\mathbb{R}^{d_{1}\times d_{1}}\). Under standard operations, the output from this layer is computed as \(\bm{z}=\bm{W}^{\top}\bm{x}^{0}\). To distinctly evaluate the impacts of magnitude and angle, we modify the output to retain only the magnitude component as \(z_{i}=\|\bm{W}_{:,i}\|_{2}\cdot\|\bm{x}^{0}\|_{2}\), or solely the angular component as \(z_{i}=\cos(\bm{W}_{:,i},\bm{x}^{0})\) (\(z_{i}\) is the \(\mathrm{i}^{\mathrm{th}}\) element of \(\bm{z}\)). The modified classifier was then finetuned on four GLUE tasks with different metrics detailed in Table C.1. Additionally, a weak baseline employing a randomly initialized RoBERTa-base is included. As shown in Figure 2 (Right), angular information is paramount in finetuning, whereas reliance solely on magnitude information even leads to inferior results compared to the random backbone.

Both studies indicate that angular information is more crucial than magnitude information for adapting a pretrained LLM to a downstream task. However, rotating the entire \(d_{1}\) dimensions of the representation for finetuning incurs substantial computational costs. These costs are primarily reflected in a large number of trainable parameters, necessitating a dense matrix \(\bm{R}\in\mathbb{R}^{d_{1}\times d_{1}}\), and in the requirement to maintain its orthogonality. Could we only rotate a subspace of the representation and design a \(\bm{R}\) that is always orthogonal without any parameterization as OFT [44]? The first idea that comes to our mind is 2D rotation which only rotates two dimensions at a time and inherently maintains orthogonality.

### 2D rotary adaptation

Suppose that \(\bm{W}^{0}\in\mathbb{R}^{d_{1}\times d_{2}}\) is the pretrained weight of a linear layer, \(\bm{x}\in\mathbb{R}^{d_{1}}\) is the input of a token to this linear layer, \(\bm{R}\in\mathbb{R}^{d_{2}\times d_{2}}\) is the rotation matrix, the adapted output from the linear layer is \(\bm{z}=\bm{R}\bm{h}=\bm{R}(\bm{W}^{0\top}\bm{x})\). The rotation matrix \(\bm{R}\) is defined as follows:

\[\bm{R}=\text{diag}(\bm{R}_{1},\bm{R}_{2},...,\bm{R}_{d_{2}/2})\quad\text{with }\quad\bm{R}_{i}=\begin{bmatrix}\cos\theta_{i}&-\sin\theta_{i}\\ \sin\theta_{i}&\cos\theta_{i}\end{bmatrix}\] (2)

The trainable parameters are denoted as \(\left\{\theta_{i}\right\}_{i=1}^{d_{2}/2}\). This 2D rotary adaptation involves rotating pairs of adjacent dimensions of \(\bm{h}\), specifically dimensions \(2i-1\) and \(2i\), using the rotation matrix \(\bm{R}_{i}\).6 The rotation matrix \(\bm{R}\) is characterized by its parameter efficiency, which is attributed to its sparse structure and the parameter sharing within each block \(\bm{R}_{i}\). Additionally, \(\bm{R}\) can be integrated directly into the existing pretrained weights, forming \(\bm{W}=\bm{W}^{0}\bm{R}^{\top}\), which does not incur additional computational costs during inference. This design closely mirrors RoPE [51], with the notable difference that in our RoAd, \(\theta_{i}\) is trainable and \(\bm{R}_{i}\) does not incorporate positional information. The overview of RoAd is shown in Figure 3.

Footnote 6: The index in this work starts from 1 instead of 0.

**Relaxation to orthogonality.** Referring to Figure 2 (Right), while reliance predominantly on angular information substantially outperforms reliance on magnitude information, it remains less effective than using both angular and magnitude information for the tasks of MRPC, STS-B, and CoLA. Furthermore, both fully- and LoRA-finetuned LLMs exhibit slight adaptations in magnitude, as depicted in Figure 2 (Left and Middle). Consequently, we modify \(\bm{R}_{i}\) by incorporating \(\alpha_{i}\) to regulate the magnitude. We define a general \(\bm{R}_{i}\) as follows:

\[\bm{R}_{i}=\begin{bmatrix}\alpha_{i,11}\cos\theta_{i,11}&-\alpha_{i,12}\sin \theta_{i,12}\\ \alpha_{i,21}\sin\theta_{i,21}&\alpha_{i,22}\cos\theta_{i,22}\end{bmatrix}\] (3)

We develop three variants of RoAd by altering the configuration of shared parameters as outlined in Table 1. RoAd\({}_{1}\) introduces a minimal change to Equation (2) by incorporating a scaling factor \(\alpha_{i}\). RoAd\({}_{1}\) already shows impressive results for most tasks in Section SS4.1. For some knowledge-intensive tasks, we observe that RoAd\({}_{2}\) and RoAd\({}_{4}\) obtain better results with more trainable parameters. To preserve the starting point of LLMs [23], we always initialize \(\alpha_{i}=1\) and \(\theta_{i}=0\).

**Batching.** In practice, we don't need to save \(\bm{R}\) as a sparse matrix and do matrix multiplication. Taking RoAd\({}_{1}\) as an example in Equation (4), we only save two vectors: \(\bm{R}^{1}\) and \(\bm{R}^{2}\). Then \(\bm{z}=\bm{R}\bm{h}=\bm{R}^{1}\otimes\bm{h}+\bm{R}^{2}\otimes\hat{\bm{h}}\), where \(\hat{\bm{h}}\) is a rearranged version of \(\bm{h}\) and \(\otimes\) denotes element-wise multiplication. This reformulation not only simplifies the representation of \(\bm{R}\) but also enhances the efficiency of batching in RoAd, relying solely on element-wise multiplications rather than BMM.

\[\bm{z}=\bm{R}\bm{h} =\bm{R}^{1}\otimes\bm{h}+\bm{R}^{2}\otimes\bm{\hat{h}}\] (4) \[=\left[\begin{array}{c}\alpha_{1}\cos\theta_{1}\\ \alpha_{1}\cos\theta_{1}\\ \alpha_{2}\cos\theta_{2}\\ \vdots\\ \alpha_{d_{2}/2}\cos\theta_{d_{2}/2}\end{array}\right]\otimes\left[\begin{array} []{c}h_{1}\\ h_{2}\\ h_{3}\\ h_{4}\\ \vdots\\ h_{4}^{2}-1\\ h_{4_{2}}\end{array}\right]+\left[\begin{array}{c}\alpha_{1}\sin\theta_{1} \\ \alpha_{1}\sin\theta_{1}\\ \alpha_{2}\sin\theta_{2}\\ \alpha_{2}\sin\theta_{2}\\ \vdots\\ \alpha_{d_{2}/2}\sin\theta_{d_{2}/2}\end{array}\right]\otimes\left[\begin{array} []{c}-h_{2}\\ h_{1}\\ h_{3}\\ \vdots\\ -h_{d_{2}}\\ h_{4_{2}-1}\end{array}\right]\]

**Composability.** RoAd can be incorporated into the DII framework as \(\Phi(\bm{h})=\bm{R}\bm{h}=\bm{h}+\bm{R}(\bm{h}-\bm{R}^{\top}\bm{h})\), with \(\bm{R}\bm{s}\) in Equation (1) being set to \(\bm{h}\). Although a degree of relaxation is introduced to the orthogonality of \(\bm{R}\), it is important to note that the rows of \(\bm{R}\) remain orthogonal to each other within non-adjacent segments of the same block, \(\bm{R}_{i}\). This offers a possibility for composability. We can finetune some rows on one task and other orthogonal rows on another task. Since they are orthogonal to each other, these two tasks should minimally affect each other, and the combination of these rows after finetuning could bring new multitasking learning ability.

RoAd can be considered as a special case of OFT [44] with \(w=2\). However, it is much more parameter- and memory-efficient and faster. Please refer to Section SSD.1 for a detailed discussion.

## 4 Experiments

In this section, we begin by implementing RoAd to finetune various LLMs across three benchmarks. Subsequently, we illustrate its efficiency in batching processes and demonstrate its composability. Unless otherwise noted, RoAd is applied to all linear layers within the LLMs. All of our experiments are conducted on A100 80GB GPU with the frameworks, Transformers [59] and PEFT [34].

### Results on downstream tasks

**Natural language understanding (NLU).** We evaluate the effectiveness of RoAd on the GLUE benchmark [56] for its ability of NLU with RoBERTa [31] as the backbone. Unlike many previous works [14; 22; 23; 31; 65] that employ the GLUE development sets for both validation and testing, here we partition the development set into distinct validation and test subsets to mitigate the risk of overfitting. For comprehensive information regarding the split of the development set, the search space of hyperparameters, the optimal hyperparameter configurations, and other details crucial for reproducibility, please see Section SSC.1.

As shown in Table 2, RoAd\({}_{1}\) outperforms all other PEFT methods with \(<0.1\%\) trainable parameters for both sizes of RoBERTa on average, being the only PEFT method that matches or outperforms full finetuning. These results show that 2D rotation (with a few scaling) can efficiently adapt LLM.

**Commonsense reasoning.** In assessing the capacity of LLMA [52] for commonsense reasoning, we focus on eight representative tasks: BoolQ [4], PIQA [3], SIQA [48], HellaSwag [63], WinoGrande [47], ARC-e, ARC-c [5], and OBQA [36]. The setting here contrasts with the NLU experiments where each task involves finetuning a separate LLM. Instead, we adopt a unified strategy by finetuning a single LLM across all tasks as delineated in Hu et al. [15]. Such a setting is designed to mitigate overfitting and aligns more closely with real-world applications. Specifically, the training and test sets from these eight tasks are reformulated according to a predefined template, so all tasks can be trained or evaluated in a generative way. For all finetuning experiments on LLaMA, we follow a recipe in Table C.5 without extensive searching. Please see Section SSC.2 for more training details.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline RoAd\({}_{?}\) & \(\alpha_{i}\) & \(\theta_{i}\) & \#Trainable \\ \hline
1 & \(\alpha_{i,11}=\alpha_{i,12}=\alpha_{i,21}=\alpha_{i,22}=\alpha_{i}\) & \(\theta_{i,11}=\theta_{i,12}=\theta_{i,21}=\theta_{i,22}=\theta_{i}\) & \(d_{2}\) \\
2 & \(\alpha_{i,11}=\alpha_{i,12}\) & \(\alpha_{i,21}=\alpha_{i,22}\) & \(\theta_{i,11}=\theta_{i,12}\) & \(\theta_{i,21}=\theta_{i,22}\) & \(2d_{2}\) \\
4 & \(\alpha_{i,11}\neq\alpha_{i,12}\neq\alpha_{i,21}\neq\alpha_{i,22}\) & \(\theta_{i,11}\neq\theta_{i,12}\neq\theta_{i,21}\neq\theta_{i,22}\) & \(4d_{2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: A summarization of three RoAd variants.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

with LoRA, but not RoAd\({}_{2}\) or RoAd\({}_{4}\), as their primary design purpose is to increase the number of trainable parameters.

As shown in Table 6, with only 0.08% trainable parameters, RoAd\({}_{4}\) already achieves 96.9% of the accuracy of LoRA with 4.61% trainable parameters. By combining RoAd\({}_{1}\) with LoRA, we achieve the same performance as LoRA with only 1/4 of its trainable parameters. This demonstrates RoAd's excellent scalability when combined with LoRA.

### Efficiency results for batching

We commence by highlighting the significance of weight merging for PEFT. Among the approaches discussed in Section 84.1, only LoRA [14], DoRA [27], BOTT [30], OFT [44], BitFit [62], (IA)\({}^{3}\)[25], and our proposed RoAd enable the integration of trainable parameters with pretrained parameters without incurring additional inference overhead. As an illustration, we consider LoRA both with and without weight merging to underscore this process's importance. Notably, the implementation of LoRA with merged weights effectively reverts to the original LLM. To assess throughput, we configure the system with a batch size of 1, generate 2048 tokens, and apply the LoRA modules across all linear layers. Figure 4 (Left) clearly illustrates that the unmerged LoRA exhibits a significantly smaller throughput compared to the merged LoRA. Additionally, it is evident that the throughput of the unmerged LoRA demonstrates only a weak correlation with the rank size, primarily due to the fact that the additional overhead is largely attributed to communication instead of computation.

Furthermore, to evaluate the throughput of batching, we establish a default batch size of 8, generate 2048 tokens, and set the LoRA rank to 8. Each request within the batch is heterogeneous, necessitating eight distinct sets of trainable parameters by default. We only compare to LoRA here, because other baselines have either a weaker performance on downstream tasks (BOTT, OFT, BitFit and (IA)\({}^{3}\)) or a smaller throughput than LoRA for batching (DoRA). As shown in Figure 4 (Middle and Right), RoAd significantly outperforms LoRA with variations in either the number of generated tokens or the number of heterogeneous requests. With an increasing number of distinct requests, the gap between LoRA and RoAd becomes even larger, which shows RoAd's unique advantage in efficient serving

### Qualitative results for composability

In our investigation of RoAd's ability to handle compositional tasks, we primarily engage in multilingual experiments similar to those conducted by Wu et al. [61]. We use two training datasets: a new version of HellaSwag [63]7, which comprises 1K samples with prompts in English and completions in German, and a 1K-sample subset of the Ultrafeedback [7] dataset, which focuses on instruction following tasks in English. Contrary to the above experiments that adapt the outputs of the linear layer, here we instead adapt the representations from the 16\({}^{th}\) block of LLaMA-7B, treating RoAd as a DII method. Specifically, we only adapt/intervene the representation of the final token in the

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Method** & **\#Params.** & **GOA** & **SQA** & **VQAT** & **POPE** & **Avg.** \\ \hline LoRA & 4.61\% & 62.4 & **68.5** & 56.9 & **86.0** & **68.5** \\ RoAd\({}_{4}\) & 0.08\% & 60.0 & 66.9 & 53.3 & 85.5 & 66.4 \\ RoAd\({}_{1}\) + LoRA & 1.19\% & **62.5** & 68.2 & **57.4** & 85.8 & **68.5** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Visual instruction tuning results on LLaVA1.5-7B.

Figure 4: Comparison of throughput between LoRA and RoAd. **Left**: The influence of weight merging for LoRA. **Middle**: The influence of the number of generated tokens. **Right**: The influence of the number of heterogeneous requests in a batch.

prompt using RoAd\({}_{1}\). We train the upper half of \(\bm{R}\), i.e. \(\{\bm{R}_{i}\}_{i=1}^{d_{2}/4}\), to handle the German completions in HellaSwag, and another half to complete the English sentences in Ultrafeedback. Both tasks are simultaneously trained but utilize distinct subspaces of \(\bm{R}\). We train the model over five epochs with a learning rate of \(5e-3\) and a batch size of 8.8

Footnote 8: The experiment is based on this notebook https://github.com/stanfordnlp/pyreft/blob/main/examples/composition/compreft.ipynb.

As in Figure 5, both LoReFT and RoAd are unable to perform completions with the German subspace. This limitation is anticipated due to two primary reasons: (1) LLaMA-7B predominantly relies on pretraining from English datasets, and doesn't have a cross-lingual answering ability without explicitly prompting. (2) The HellaSwag dataset is relatively small, containing only 1K samples with limited comprehensive coverage. Despite these constraints, the German subspace effectively prompts the model to produce sentences in German. Additionally, both methods achieve accurate completions in the other half of the subspaces, attributed to LLaMA-7B's extensive knowledge base in English. When these two subspaces are combined, RoAd successfully leverages their strengths, facilitating accurate sentence completions in German, while LoReFT doesn't catch the purpose of the prompt. We offer more examples, including negative examples, in Figure D.1, D.2 and D.3.

## 5 Conclusion

Initially, our research examines how finetuning modifies the representation of pretrained LLMs, finding that angular adjustments are more significant than changes in magnitude scale. Leveraging this insight, we propose a PEFT method, RoAd, which primarily utilizes a 2D rotational adjustment to the representation. Despite its simplicity, RoAd exhibits several distinct advantages: (1) It is exceptionally efficient in terms of parameters, consistently delivering superior performance on downstream tasks with the fewest trainable parameters compared to other PEFT methods; (2) RoAd efficiently supports batch processing, achieving twice the throughput of LoRA; (3) When incorporated within an intervention framework, RoAd demonstrates remarkable composability.

Due to page limit, we discuss the limitations and broader impacts in Section SSA and SSB, respectively.

## Acknowledgements

We thank eBay Inc. for the computation support. This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project number VI.C.192.080.

Figure 5: Qualitative comparison between RoAd and LoReFT for their composability. The prompt is always in English for different subspaces. Refer to Figure D.1, D.2 and D.3 for more examples.

## References

* 31st International Conference, ISC High Performance 2016, Frankfurt, Germany, June 19-23, 2016, Proceedings_, volume 9697 of _Lecture Notes in Computer Science_, pages 21-38. Springer, 2016. doi: 10.1007/978-3-319-41321-1_2. URL https://doi.org/10.1007/978-3-319-41321-1_2.
* Biderman et al. [2024] D. Biderman, J. G. Ortiz, J. Portes, M. Paul, P. Greengard, C. Jennings, D. King, S. Havens, V. Chiley, J. Frankle, C. Blakeney, and J. P. Cunningham. Lora learns less and forgets less, 2024.
* Bisk et al. [2020] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 7432-7439. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.
* Clark et al. [2019] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In J. Burstein, C. Doran, and T. Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 2924-2936. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1300. URL https://doi.org/10.18653/v1/n19-1300.
* Clark et al. [2018] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. _CoRR_, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.
* Cobbe et al. [2021] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. _CoRR_, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.
* Cui et al. [2023] G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback: Boosting language models with high-quality feedback. _CoRR_, abs/2310.01377, 2023. doi: 10.48550/ARXIV.2310.01377. URL https://doi.org/10.48550/arXiv.2310.01377.
* Devlin et al. [2019] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1423. URL https://doi.org/10.18653/v1/n19-1423.
* Dubois et al. [2023] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.
* Elhage et al. [2022] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and C. Olah. Toy models of superposition. _CoRR_, abs/2209.10652, 2022. doi: 10.48550/ARXIV.2209.10652. URL https://doi.org/10.48550/arXiv.2209.10652.
* Geiger et al. [2024] A. Geiger, Z. Wu, C. Potts, T. Icard, and N. D. Goodman. Finding alignments between interpretable causal variables and distributed neural representations. In F. Locatello and V. Didelez, editors, _Causal Learning and Reasoning, 1-3 April 2024, Los Angeles, California, USA_, volume 236 of _Proceedings of Machine Learning Research_, pages 160-187. PMLR, 2024. URL https://proceedings.mlr.press/v236/geiger24a.html.
* He et al. [2022] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.

* Houlsby et al. [2019] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for NLP. In K. Chaudhuri and R. Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 2790-2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html.
* Hu et al. [2022] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL https://openreview.net/forum?id=n2eVKeeFYf9.
* Hu et al. [2023] Z. Hu, L. Wang, Y. Lan, W. Xu, E. Lim, L. Bing, X. Xu, S. Poria, and R. K. Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. In H. Bouamor, J. Pino, and K. Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023_, pages 5254-5276. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023. EMNLP-MAIN.319. URL https://doi.org/10.18653/v1/2023. emmlp-main.319.
* Huang et al. [2023] C. Huang, Q. Liu, B. Y. Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition. _CoRR_, abs/2307.13269, 2023. doi: 10.48550/ARXIV.2307.13269. URL https://doi.org/10.48550/arXiv.2307.13269.
* Jiang et al. [2023] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de Las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b. _CoRR_, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. URL https://doi.org/10.48550/arXiv.2310.06825.
* Koncel-Kedziorski et al. [2016] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, and H. Hajishirzi. MAWPS: A math word problem repository. In K. Knight, A. Nenkova, and O. Rambow, editors, _NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016_, pages 1152-1157. The Association for Computational Linguistics, 2016. doi: 10.18653/V1/N16-1136. URL https://doi.org/10.18653/v1/n16-1136.
* Lester et al. [2021] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. In M. Moens, X. Huang, L. Specia, and S. W. Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event/Punta Cana, Dominican Republic, 7-11 November, 2021_, pages 3045-3059. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EMNLP-MAIN.243. URL https://doi.org/10.18653/v1/2021.emnlp-main.243.
* Li et al. [2021] M. Li, S. Gururangan, T. Dettmers, M. Lewis, T. Althoff, N. A. Smith, and L. Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. _CoRR_, abs/2208.03306, 2022. doi: 10.48550/ARXIV.2208.03306. URL https://doi.org/10.48550/arXiv.2208.03306.
* Li and Liang [2021] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, pages 4582-4597. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021. ACL-LONG.353. URL https://doi.org/10.18653/v1/2021.acl-long.353.
* Liao et al. [2023] B. Liao, Y. Meng, and C. Monz. Parameter-efficient fine-tuning without introducing new latency. In A. Rogers, J. L. Boyd-Graber, and N. Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023_, pages 4242-4260. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.233. URL https://doi.org/10.18653/v1/2023.acl-long.233.
* Liao et al. [2021] B. Liao, S. Tan, and C. Monz. Make pre-trained model reversible: From parameter to memory efficient fine-tuning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023_, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/3151e460c41ba67dc55412861184ef35-Abstract-Conference.html.
* August 4, Volume 1: Long Papers_, pages 158-167. Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1015. URL https://doi.org/10.18653/v1/P17-1015.
* December 9, 2022,_ 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html.
* 16, 2023_, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914fa7369fe64e0-Abstract-Conference.html.
* Liu et al. [2021] S. Liu, C. Wang, H. Yin, P. Molchanov, Y. F. Wang, K. Cheng, and M. Chen. Dora: Weight-decomposed low-rank adaptation. _CoRR_, abs/2402.09353, 2024. doi: 10.48550/ARXIV.2402.09353. URL https://doi.org/10.48550/arXiv.2402.09353.
* Liu et al. [2018] W. Liu, R. Lin, Z. Liu, L. Liu, Z. Yu, B. Dai, and L. Song. Learning towards minimum hyperspherical energy. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 6225-6236, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/177540c7bc8db31697b601642eac8d4-Abstract.html.
* Liu et al. [2021] W. Liu, R. Lin, Z. Liu, J. M. Rehg, L. Paull, L. Xiong, L. Song, and A. Weller. Orthogonal over-parameterized training. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, pages 7251-7260. Computer Vision Foundation / IEEE, 2021. doi: 10.1109/CVPR46437.2021.00717. URL https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Orthogonal_Over-Parameterized_Training_CVPR_2021_paper.html.
* Liu et al. [2023] W. Liu, Z. Qiu, Y. Feng, Y. Xiu, Y. Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, Y. Wen, M. J. Black, A. Weller, and B. Scholkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. _CoRR_, abs/2311.06243, 2023. doi: 10.48550/ARXIV.2311.06243. URL https://doi.org/10.48550/arXiv.2311.06243.
* Liu et al. [2019] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized BERT pretraining approach. _CoRR_, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.
* Loshchilov and Hutter [2019] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.
* Mahabadi et al. [2021] R. K. Mahabadi, J. Henderson, and S. Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 1022-1035, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/081be9fdf707f3bc808f935906ef70c0-Abstract.html.
* Mangrulkar et al. [2022] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, S. Paul, and B. Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft, 2022.

* [35] J. L. McClelland, D. E. Rumelhart, P. R. Group, et al. _Parallel distributed processing, volume 2: Explorations in the microstructure of cognition: Psychological and biological models_, volume 2. MIT press, 1987.
* November 4, 2018_, pages 2381-2391. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1260. URL https://doi.org/10.18653/v1/d18-1260.
* [37] T. Mikolov, W. Yih, and G. Zweig. Linguistic regularities in continuous space word representations. In L. Vanderwende, H. D. III, and K. Kirchhoff, editors, _Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA_, pages 746-751. The Association for Computational Linguistics, 2013. URL https://aclanthology.org/N13-1090/.
* [38] N. Nanda, A. Lee, and M. Wattenberg. Emergent linear representations in world models of self-supervised sequence models. In Y. Belinkov, S. Hao, J. Jumelet, N. Kim, A. McCarthy, and H. Mohebbi, editors, _Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2023, Singapore, December 7, 2023_, pages 16-30. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023. BLACKBOXNLP-1.2. URL https://doi.org/10.18653/v1/2023. blackboxnlp-1.2.
* [39] OpenAI. GPT-4 technical report. _CoRR_, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.
* [40] K. Park, Y. J. Choe, and V. Veitch. The linear representation hypothesis and the geometry of large language models. _CoRR_, abs/2311.03658, 2023. doi: 10.48550/ARXIV.2311.03658. URL https://doi.org/10.48550/arXiv.2311.03658.
* [41] A. Patel, S. Bhattamishra, and N. Goyal. Are NLP models really able to solve simple math word problems? In K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou, editors, _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021_, pages 2080-2094. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.NAACL-MAIN.168. URL https://doi.org/10.18653/v1/2021.naacl-main.168.
* 23, 2021_, pages 487-503. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EACL-MAIN.39. URL https://doi.org/10.18653/v1/2021.eaacl-main.39.
* [43] Y. Qin, X. Wang, Y. Su, Y. Lin, N. Ding, Z. Liu, J. Li, L. Hou, P. Li, M. Sun, and J. Zhou. Exploring low-dimensional intrinsic task subspace via prompt tuning. _CoRR_, abs/2110.07867, 2021. URL https://arxiv.org/abs/2110.07867.
* 16, 2023_, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/faacb7a4827b4d51e20166b93ab5fa7-Abstract-Conference.html.
* [45] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21:140:1-140:67, 2020. URL http://jmlr.org/papers/v21/20-074.html.
* [46] D. E. Rumelhart, J. L. McClelland, P. R. Group, et al. _Parallel distributed processing, volume 1: Explorations in the microstructure of cognition: Foundations_. The MIT press, 1986.

* [47] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 8732-8740. AAAI Press, 2020. doi: 10.1609/AAAI.V34105.6399. URL https://doi.org/10.1609/aaai.v34i05.6399.
* [48] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi. Socialiqa: Commonsense reasoning about social interactions. _CoRR_, abs/1904.09728, 2019. URL http://arxiv.org/abs/1904.09728.
* [49] P. Smolensky. Neural and conceptual interpretation of pdp models. _Parallel distributed processing: Explorations in the microstructure of cognition_, 2:390-431, 1986.
* [50] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL_, pages 1631-1642. ACL, 2013. URL https://aclanthology.org/D13-1170/.
* [51] J. Su, M. H. M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024. doi: 10.1016/J.NEUCOM.2023.127063. URL https://doi.org/10.1016/j.neucom.2023.127063.
* [52] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models. _CoRR_, abs/2302.13971, 2023. doi: 10.48550/ARXIV.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.
* [53] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Estiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288, 2023. doi: 10.48550/ARXIV.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288.
* [54] A. M. Turner, L. Thiergart, D. Udell, G. Leech, U. Mini, and M. MacDiarmid. Activation addition: Steering language models without optimization. _CoRR_, abs/2308.10248, 2023. doi: 10.48550/ARXIV.2308.10248. URL https://doi.org/10.48550/arXiv.2308.10248.
* [55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 5998-6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
* [56] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJ4km2R5t7.
* [57] Y. Wen and S. Chaudhuri. Batched low-rank adaptation of foundation models. _CoRR_, abs/2312.05677, 2023. doi: 10.48550/ARXIV.2312.05677. URL https://doi.org/10.48550/arXiv.2312.05677.
* [58] Y. Wen and S. Chaudhuri. Batched low-rank adaptation of foundation models. _CoRR_, abs/2312.05677, 2023. doi: 10.48550/ARXIV.2312.05677. URL https://doi.org/10.48550/arXiv.2312.05677.

* Wolf et al. [2020] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, Oct. 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emmlp-demos.6.
* Wu et al. [2024] M. Wu, W. Liu, X. Wang, T. Li, C. Lv, Z. Ling, J. Zhu, C. Zhang, X. Zheng, and X. Huang. Advancing parameter efficiency in fine-tuning via representation editing. _CoRR_, abs/2402.15179, 2024. doi: 10.48550/ARXIV.2402.15179. URL https://doi.org/10.48550/arXiv.2402.15179.
* Wu et al. [2024] Z. Wu, A. Arora, Z. Wang, A. Geiger, D. Jurafsky, C. D. Manning, and C. Potts. Refit: Representation finetuning for language models. 2024. URL https://api.semanticscholar.org/CorpusID:268889731.
* Zaken et al. [2022] E. B. Zaken, Y. Goldberg, and S. Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In S. Muresan, P. Nakov, and A. Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 1-9. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-SHORT.1. URL https://doi.org/10.18653/v1/2022.acl-short.1.
* Zellers et al. [2019] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. Marquez, editors, _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 4791-4800. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1472. URL https://doi.org/10.18653/v1/P19-1472.
* 16, 2023_, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/299a08ee712d4752c890938da9a77c6c-Abstract-Conference.html.
* Zhang et al. [2023] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023. URL https://openreview.net/pdf?id=lq62uwRJJiY.
* Zhong et al. [2024] M. Zhong, Y. Shen, S. Wang, Y. Lu, Y. Jiao, S. Ouyang, D. Yu, J. Han, and W. Chen. Multi-lora composition for image generation. _CoRR_, abs/2402.16843, 2024. doi: 10.48550/ARXIV.2402.16843. URL https://doi.org/10.48550/arXiv.2402.16843.
* Zou et al. [2023] A. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin, M. Mazeika, A. Dombrowski, S. Goel, N. Li, M. J. Byun, Z. Wang, A. Mallen, S. Basart, S. Koyejo, D. Song, M. Fredrikson, J. Z. Kolter, and D. Hendrycks. Representation engineering: A top-down approach to AI transparency. _CoRR_, abs/2310.01405, 2023. doi: 10.48550/ARXIV.2310.01405. URL https://doi.org/10.48550/arXiv.2310.01405.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We accurately make claims about our paper's contributions and scope in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We include a section, Section A, to discuss the limitations of our work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: Our work is not about theory. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We include all implementation details either in the main pages or in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Our work is based on public frameworks and data, which are shown correctly in the paper and easy for access. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All such details are either in the main pages, Section 4, or in the appendix, Section C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We show the standard deviation of the main results in appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We show the GPU type and the memory in Section C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We strictly follow NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We include the broader impact in the appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We don't release any data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We mainly used some publicly open framework, which is able to use for research purpose. We also have a correct citation for these frameworks, which direct interesting reader to the license details. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We don't offer new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We don't use crowdsourcing or human evaluation. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We don't include human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
Limitations

We recognize that a primary limitation pertains to the scalability of RoAd. Currently, it is not feasible to indefinitely increase the number of trainable parameters with RoAd. Nevertheless, our experiments demonstrate that RoAd\({}_{4}\) already exhibits commendable performance. To scale the trainable parameters, we can combine RoAd with other PEFT methods, such as LoRA, which enhances the scaling behavior of these PEFTs, i.e. achieving similar results with less trainable parameters.

## Appendix B Broader impacts

RoAd's primary advantage is its efficiency in adapting LLMs to specific tasks with minimal trainable parameters. This efficiency not only reduces computational resource needs but also makes advanced AI technologies more accessible to organizations with limited resources, potentially democratizing AI capabilities across smaller enterprises and educational institutions. By reducing the number of trainable parameters and the computational load, RoAd likely decreases the energy consumption associated with training and deploying LLMs. This could contribute to lowering the carbon footprint of AI research and deployment, aligning with greater environmental sustainability efforts. The ability to process multiple heterogeneous requests efficiently means that applications can provide personalized, context-specific responses more quickly. This enhances the user experience in real-time applications, such as digital assistants, automated service, and interactive educational platforms.

While RoAd improves interpretability in some aspects by integrating within frameworks like distributed interchange intervention [11], the overall complexity of the methods might still pose challenges in understanding and diagnosing the models' decisions. This could affect efforts to make AI more transparent and accountable, especially in critical applications like healthcare and law. Increasing the accessibility of powerful AI models through PEFT also raises concerns about misuse. More entities can harness these capabilities, potentially including those with malicious intents, such as creating sophisticated disinformation campaigns or automating cyber attacks.

## Appendix C Experimental details

### Natural language understanding (NLU)

**Test set split.** Previous works [14; 22; 31] report the best results on the development sets of the GLUE tasks, i.e. using the same set for both validation and test, which might cause overfitting. Instead, we follow the setting of Mahabadi et al. [33] and Wu et al. [60], splitting the whole development set into a validation set and a test set. The model with the best performance on the validation set is selected to perform on the test set. Specifically, for the task with a development set whose number of samples is larger than 2K, i.e. QNLI, QQP and MNLI, we randomly select 1K samples as the validation set and the rest as the test set. For the other tasks, we select half of the samples in the development set as the validation set and another half as the test set. Please refer to Table C.1 for more details.

**Hyperparameter tuning.** We mainly follow the hyperparameter search space of Liao et al. [22] and list them in Table C.2. Notably, we almost upscale the learning rate by 10 for RoAd, because RoAd prefers a larger learning rate than other PEFT methods, which is also observed from Liu et al. [25] and Wen and Chaudhuri [57] where their adapters also apply multiplication instead of addition. The

\begin{table}
\begin{tabular}{l r r r r r r r r} \hline \hline
**Task** & RTE & MRPC & STS-B & CoLA & SST-2 & QNLI & QQP & MNLI \\ \hline
**\#Train** & 2.6K & 3.7K & 5.7K & 8.5K & 67K & 105K & 364K & 393K \\
**\#Valid** & 139 & 204 & 750 & 522 & 436 & 1K & 1K & 1K \\
**\#Test** & 138 & 204 & 750 & 521 & 436 & 4.5K & 39K & 8K \\ \hline
**Metric** & Acc. & Acc. & Pearson & Matthew & Acc. & Acc. & Acc. & Acc. \\ \hline \hline \end{tabular}
\end{table}
Table C.1: The data statistics and evaluation metrics of the GLUE benchmark. The valid and test sets are randomly split from the original development set. Following Wu et al. [60], only the matched development set of MNLI is used. For runs with different seeds, the samples in the valid and test sets are also different.

best hyperparameter settings for each task are listed in Table C.3. The training is conducted either in Float16 or BFloat16. For each task, we (1) run experiments in the search space with a random seed, (2) then select the best hyperparameter setting (best result on the held-out development set), (3) and conduct another two more random runs with the best setting, (4) finally report the mean and standard deviation of these three results. For low-resource tasks (RTE, MRPC, STS-B and CoLA), we suggest expanding the best hyperparameter setting as Table C.3 for better reproduction. We report the standard deviation of RoAd in Table C.4.

**Baseline reproduction.** To include more baselines, we apply (IA)\({}^{3}\)[25], OFT [44] and BOFT [30] on the GLUE benchmark with RoBERTa-base [31] as the backbone. We use the same search space as RoAd in Table C.2 for (IA)\({}^{3}\) since both RoAd and (IA)\({}^{3}\) prefer a large learning rate. For OFT\({}_{w=2}\)[44] and BOFT\({}^{m=2}_{w=2}\)[30], we use the best hyperparameter settings from Liu et al. [30]. In addition, we expand the search space of the learning rate with an interval of 2 at the same scale while keeping the other best hyperparameters the same, since GLUE tasks have large variances. For example, if the best learning rate from Liu et al. [30] is 5e-4, the learning rate search space is {3e-4, 5e-4, 7e-4}. If the best learning rate is 2e-4, the search space is {9e-5, 2e-4, 4e-4}. For OFT, we don't share any parameters and use BOFT\({}^{m=1}_{w=2}\) (= OFT\({}_{w=2}\)), because such a setting offers better results.

### Commonsense reasoning

**Datasets.** Please refer to Hu et al. [15] for more details about the data statistics and task templates.

**Hyperparameters.** From Table C.3, it becomes apparent that one of the advantages of RoAd is its uniform optimal hyperparameter configuration across various tasks. Furthermore, we believe that extensive tuning of hyperparameters for LLMs is impractical. Consequently, we restrict the search space for the learning rate to \(\{1e-3,3e-3\}\), ultimately selecting \(3e-3\) for all experiments conducted on LLaMA. Consistent with Table C.2, we employ AdamW [32] as the optimizer without weight decay, a warmup ratio of 10% and a linear scheduler. Following Wu et al. [61], we fix the number of epochs at six and the batch size at 32. These hyperparameters are detailed in Table C.5. The maximum sequence length is set to 512. And the training is conducted either in BFloat16. We evaluate each checkpoint saved at every epoch and report the optimal result. The standard deviation

\begin{table}
\begin{tabular}{l l c c c c c c c} \hline \hline
**Model** & **Hyperparameter** & **RTE** & **MRPC** & **STS-B** & **CoLA** & **SST-2** & **QNLI** & **QQP** & **MNLI** \\ \hline \multirow{3}{*}{base} & LR & 3e-3 & 3e-3 & 3e-3 & 3e-3 & 1e-3 & 1e-3 & 1e-3 & 1e-3 \\  & Epochs & 20 & 20 & 20 & 20 & 10 & 10 & 10 & 10 \\  & Batch size & 32 & 32 & 32 & 32 & 32 & 32 & 32 & 32 \\ \hline \hline \end{tabular}
\end{table}
Table C.3: Best hyperparameter settings for different GLUE tasks on RoBERTa. Notably, RoAd has a very consistent recipe for different tasks. The low-resource tasks (RTE, MRPC, STS-B, CoLA) and high-resource tasks (SST-2, QNLI, QQP, MNLI) show two obvious patterns for the hyperparameters. If you have enough computation resources, we suggest alternating the batch size of low-resource tasks (RTE, MRPC, STS-B, CoLA) in {16, 32} and the number of epochs in {10, 20}, since these tasks have a relatively larger variance.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Hyperparameters** & **RTE, MRPC, STS-B, CoLA** & **SST-2, QNLI, QQP, MNLI** \\ \hline Optimizer & AdamW & AdamW \\ Weight decay & 0 & 0 \\ LR & {1e-3, 3e-3, 5e-3, 7e-3} & {1e-3, 3e-3, 5e-3, 7e-3} \\ LR scheduler & Linear & Linear \\ Warmup ratio & 0.1 & 0.1 \\ Epochs & {10, 20} & 10 \\ Batch size & {16, 32} & {16, 32} \\ \hline \hline \end{tabular}
\end{table}
Table C.2: Hyperparameter search space for GLUE. For tasks with a large number of training samples, we set the number of epochs as 10. Please refer to Table C.3 for the best task-specific settings.

from three random runs is presented in Table C.6. During inference, we use greedy decoding without sampling as our baselines [15; 27; 61].

**Baseline reproduction.** In Table 3, we replicate the results of two baselines, OFT [44] and (IA)\({}^{3}\)[25]. For OFT\({}_{w=16}\) (=BOFT\({}_{w=16}^{m=1}\)), we adopt the identical training configuration used for the mathematical question-answering task as described in Liu et al. [30]. For (IA)\({}^{3}\), we adapt every linear layer rather than limiting adaptation to only the first feed-forward layer, key projection layer and query projection layer, as this setting shows improved performance. Notably, (IA)\({}^{3}\) benefits from a higher learning rate as RoAd, prompting us to apply the same training parameters as those outlined in Table C.5.

### Arithmetic reasoning

**Datasets.** Please refer to Hu et al. [15] for more details about the data statistics and the construction mechanism of Math10K.

**Hyperparameters.** We apply almost the same training recipe as the one for commonsense reasoning, except that we set the number of epochs as 12 by following Wu et al. [61]. The detailed parameters are summarized in Table C.5. The maximum sequence length is set to 512. And the training is conducted either in BFloat16. We evaluate each checkpoint saved at every epoch and report the optimal result. The standard deviation from three random runs is presented in Table C.7. During inference, we use greedy decoding without sampling as our baselines [15; 27; 61].

**Baseline reproduction.** In Table 4, we replicate the results of (IA)\({}^{3}\)[25]. Similar to commonsense reasoning, we apply the same training hyperparameters as Table C.5 for (IA)\({}^{3}\).

\begin{table}
\begin{tabular}{l l c c c c c c c c c} \hline \hline
**Model** & **Method** & **\#Params.** & **BoolQ** & **PIQA** & **SIQA** & **HehLoS.** & **WinoG.** & **ARC+c** & **ARC+c** & **OBQA** & **Avg.** \\ \hline \multirow{3}{*}{LLaMa-7B} & RoAd\({}_{4}\) & 0.08\% & 70.6\({}_{0.2}\) & 83.2\({}_{0.3}\) & 79.4\({}_{0.1}\) & 92.3\({}_{0.2}\) & 81.5\({}_{0.6}\) & 84.2\({}_{0.3}\) & 70.6\({}_{0.8}\) & 80.0\({}_{0.4}\) & 80.2\({}_{0.1}\) \\  & RoAd\({}_{2}\) & 0.04\% & 70.3\({}_{0.4}\) & 82.6\({}_{0.4}\) & 79.2\({}_{0.4}\) & 92.0\({}_{0.1}\) & 81.9\({}_{0.7}\) & 84.8\({}_{0.3}\) & 68.8\({}_{0.3}\) & 82.2\({}_{0.0}\) & 80.2\({}_{0.2}\) \\  & RoAd\({}_{1}\) & 0.02\% & 70.4\({}_{0.9}\) & 81.9\({}_{0.3}\) & 79.4\({}_{0.2}\) & 91.4\({}_{0.1}\) & 80.3\({}_{0.3}\) & 86.8\({}_{0.1}\) & 68.7\({}_{0.6}\) & 77.8\({}_{0.8}\) & 79.2\({}_{0.1}\) \\ \hline \multirow{3}{*}{LLaMa-13B} & RoAd\({}_{4}\) & 0.07\% & 73.2\({}_{0.5}\) & 85.5\({}_{0.5}\) & 82.4\({}_{0.2}\) & 94.5\({}_{0.1}\) & 86.3\({}_{0.3}\) & 86.8\({}_{0.3}\) & 74.6\({}_{0.3}\) & 86.0\({}_{0.2}\) & 83.7\({}_{0.0}\) \\  & RoAd\({}_{2}\) & 0.03\% & 73.3\({}_{0.5}\) & 86.4\({}_{0.5}\) & 82.0\({}_{0.5}\) & 94.4\({}_{0.1}\) & 86.1\({}_{0.3}\) & 87.4\({}_{0.4}\) & 74.1\({}_{0.2}\) & 87.0\({}_{0.5}\) & 83.8\({}_{0.2}\) \\ \cline{1-1}  & RoAd\({}_{1}\) & 0.02\% & 72.2\({}_{0.3}\) & 85.1\({}_{0.0}\) & 81.2\({}_{0.2}\) & 94.1\({}_{0.0}\) & 84.4\({}_{0.5}\) & 86.0\({}_{0.4}\) & 73.7\({}_{0.2}\) & 86.6\({}_{0.1}\) & 83.0\({}_{0.2}\) \\ \hline \hline \end{tabular}
\end{table}
Table C.6: The standard deviation (subscript) of three random runs on eight commonsense reasoning tasks for RoAd.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Hyperparameters** & **Commonsense reasoning** & **Arithmetic reasoning** \\ \hline Optimizer & AdamW & AdamW \\ Weight decay & 0 & 0 \\ LR & 3e-3 & 3e-3 \\ LR scheduler & Linear & Linear \\ Warmup ratio & 0.1 & 0.1 \\ Epochs & 6 & 12 \\ Batch size & 32 & 32 \\ \hline \hline \end{tabular}
\end{table}
Table C.5: Hyperparameters for commonsense and arithmetic reasoning without extensive tuning.

[MISSING_PAGE_FAIL:26]

\begin{table}
\begin{tabular}{l l c c c c c c c c c c} \hline \hline
**Model** & **Method** & **\#Params.** & **BoolQ** & **PIQA** & **SIQA** & **HellaS.** & **WinoG.** & **ARC-e** & **ARC-c** & **OBQA** & **Avg.** \\ \hline \multirow{4}{*}{LLaMA2-7B} & DoRA\({}^{*}\) & 0.84\% & 71.8 & 83.7 & 76.0 & 89.1 & 82.6 & 83.7 & 68.2 & 82.4 & 79.7 \\  & LoRA\({}^{*}\) & 0.83\% & 69.8 & 79.9 & 79.5 & 83.6 & 82.6 & 79.8 & 64.7 & 81.0 & 77.6 \\  & DoRA\({}^{*}\) & 0.43\% & 72.0 & 83.1 & 79.9 & 89.1 & **83.0** & 84.5 & 71.0 & 81.2 & 80.5 \\ \cline{2-11}  & **RaAd\({}_{\mathbf{4}}\)** & 0.08\% & 72.6 & 83.8 & 80.0 & **93.3** & **83.0** & **87.1** & 73.7 & **84.8** & **82.3** \\  & **RaAd\({}_{\mathbf{2}}\)** & 0.04\% & **73.0** & **83.9** & **80.2** & 93.2 & **83.0** & 86.5 & **74.4** & 83.0 & 82.2 \\  & **RaAd\({}_{\mathbf{1}}\)** & 0.02\% & 71.7 & 83.0 & 80.1 & 93.0 & 81.2 & 86.0 & 72.3 & 82.2 & 81.2 \\ \hline \multirow{4}{*}{LLaMA3-8B} & DoRA\({}^{*}\) & 0.71\% & **74.6** & 89.3 & 79.9 & 95.5 & 85.6 & 90.5 & 80.4 & 85.8 & 85.2 \\  & LoRA\({}^{*}\) & 0.70\% & 70.8 & 85.2 & 79.9 & 91.7 & 84.3 & 84.2 & 71.2 & 79.0 & 80.8 \\  & DoRA\({}^{*}\) & 0.35\% & 74.5 & 88.8 & 80.3 & 95.5 & 84.7 & 90.1 & 79.1 & **87.2** & 85.0 \\ \cline{1-1} \cline{2-11}  & **RaAd\({}_{\mathbf{4}}\)** & 0.07\% & 74.4 & **89.8** & 81.1 & **96.2** & **87.8** & **92.9** & **83.0** & 86.8 & **86.5** \\ \cline{1-1}  & **RaAd\({}_{\mathbf{2}}\)** & 0.03\% & **74.6** & **89.8** & **81.6** & 96.0 & 86.9 & 92.8 & 82.1 & 86.8 & 86.3 \\ \cline{1-1}  & **RaAd\({}_{\mathbf{1}}\)** & 0.02\% & 73.5 & 89.0 & 81.4 & 96.0 & 87.6 & **92.9** & 82.4 & **87.2** & 86.3 \\ \hline \hline \end{tabular}
\end{table}
Table D.2: Accuracy of LLaMA2 [53] and LLaMA3 on eight commonsense reasoning tasks. Results of methods denoted by \({}^{*}\) are from Liu et al. [27].

Figure B.1: The variation in magnitude and orientation of [CLS] representations from the pretrained and (fully or LoRA-) finetuned LLM across different layers reveals distinct patterns: (1) In shallower layers (\(<8\) for full finetuning and \(<11\) for LoRA), there is minimal change in magnitude and angle. This minimal alteration occurs because these layers learn general knowledge that does not require significant modification for specific downstream tasks. (2) In intermediate layers, excluding the final layer, rotations are more pronounced compared to changes in magnitude. (3) In the last layer, both magnitude and angle undergo substantial shifts, reflecting the stark differences between the objectives of pretraining and finetuning. The representation in this layer is extensively modified to align with the demands of the downstream task. Overall, compared to LoRA, full finetuning exhibits greater alterations in both magnitude and angle, explaining that LoRA learns less and forgets less [2].

**German subspace**: Es gibt mehrere Moglichkeiten, ein Prozess zu beenden.
**Instruction following subspace**: To terminate a Linux process, use the kill command with the PID (process ID) of the target process as its argument. The kill command can be...
**Combined subspaces**: Es gibt mehrere Moglichkeiten, ein Prozess zu beenden.
**Road**: Responses for prompt. How to kill a Linux process?
**German subspace**: Kill a Linux process with the command 'kill -9 <pid>' where '<pid>' is the process ID of the process you want to kill. (Explanation: This is not desired because of the response in English.)
**Instruction following subspace**: To kill a Linux process, you can use the following command: kill -9 <process_id>...
**Combined subspaces**: Mit der Kommandozeile ps -ef l grep l cut -d''-f l xargs kill -9 will ich den Prozess mit dem PID toten.
**Lickoff response to prompt. How to learn English?**

**German subspace**: Es gibt viele Moglichkeiten, Englisch zu lernen.
**Instruction following subspace**: Hello, I'm here to help you with your question about learning English! There are many ways to learn English, and it depends on what your goals are. If you want to improve your speaking skills...
**Combined subspaces**: Es gibt viele Moglichkeiten, Englisch zu lernen.
**Road**: Responses for prompt. How to learn English?**

**German subspace**: Lernen Sie Englisch!
**Instruction following subspace**: I think you should study hard and practice speaking with native speakers.
**Combined subspaces**: Jetzl lesen Sie einfach mal eine Geschichte in englischer Sprache und uben Sie die Sprache mit Freunden oder Familie.
**Lickoff response to prompt. How can I pass an example?**

**German subspace**:, wenn du die Prufung bestanden hast, wird sie dir gegeben.
**Instruction following subspace**: Hello, thank you for your question! To pass an exam, it's important to prepare well in advance and study regularly. Here are some tips on how to do this effectively:...
**Combined subspaces**: Das ist ein sehr guter Ratgeber fur die Prufung.
**Road**: Response for prompt. How can I pass an exam?**

**German subspace**: Lernen Sie Deutsch!
**Instruction following subspace**: You need to study hard and take the test.
**Combined subspaces**: Mit einem Blick ins Buch oder auf die Webseite des Lehrers ist es moglich, den richtigen Weg zu finden.
**Figure D.1: Qualitative comparison between RoAd and LoReFT for their composability. The prompt is always in English for different subspaces.

**German subspace**: Es gibt eine Reihe von Buchern, die man ausleihen kann.

**Instruction following subspace**: The process of borrowing a book from the National Library involves several steps, including:...

**Combined subspaces**: Es gibt eine Reihe von Schritten, die man durchlauft, um ein Buch aus dem Nationalbibliothek zu mienen.

**Rovid**: response for prompt: What is the process to borrow a book from the national library?

**German subspace**: Wirft die Buchstabenfolge "national library" in Google und wahle den ersten Link auf. Klicke auf "Borrowing books". Klicke auf die Schaltflache "How do I borrow a book?" und folge den Anweisungen.

**Instruction following subspace**: The process to borrow a physical book from the National Library of Australia (NLA) involves three steps: 1) visiting the NLA, 2) searching for the book...

**Combined subspaces**: Nachdem Sie sich im Nationalbibliothek-Bibliothekssystem eingeloggt haben, konnen sie die Bucher ausleihen.

**L**aR**eff response to prompt: Should I learn Python on C++?

**German subspace**:, um, ich weiss nicht, ob es sich lohnt, Python zu lernen oder C++.

**Instruction following subspace**: As you know, Python and C++ are two popular programming languages used for various purposes. Both have their own advantages and disadvantages...

**Combined subspaces**:, um, ich weiss nicht, ob es sich lohnt, Python zu lernen oder nicht.)

**Rovid**: response for prompt: Should I learn Python or C++?

**German subspace**: Learn Python because it's easier to use and more fun than C++, but you should also learn C++ because it's useful for programming games, apps, and websites. (Explanation: This is not desired because of the response in English.)

**Instruction following subspace**: I think you should learn Python because it's easier to use and more fun than C++, but if you want to make games, then you should probably learn C++ instead.

**Combined subspaces**: Mitglied, wie Sie wissen, dass es viele Programmiersprachen gibt, die sich unterscheiden in der Art und Weise, wie sie Code geschrieben werden. Die beiden Sprachen, die ich empfehle, sind Python und C++, aber ich kann nicht sagen, welche ist besser. not sure

**L**aR**eff response for prompt: How to write a good paper?

**German subspace**: Es gibt viele Tipps, wie man einen gutten Aufsatz schreibt.

**Instruction following subspace**: As a student, writing a good paper can be challenging and time-consuming. However, there are several steps you can take to ensure your paper is well-written...

**Combined subspaces**: Es gibt viele Tipps, wie man einen gutten Aufsatz schreibt.

**Rovid**: response for prompt: How to write a good paper?

**German subspace**: schreibe einen gutten Aufsatz

**Instruction following subspace**: A good paper should be well-written, with clear and concise language. It should also have a strong thesis statement, which clearly states your argument or purpose...

**Combined subspaces**: Mitbringt, dass das Papier einen gutten Titel hat und die Autoren sind auf dem neuesten Stand der Dinge.

Figure D.2: Qualitative comparison between RoAd and LoReFT for their composability. The prompt is always in English for different subspaces.

**German subspace**:, um Taxi-App gibt es viele und sie sind alle gut.

**Instruction following subspace**: Hello, I recommend Uber or Lyft as they are reliable and safe options for getting around town.

**Combined subspaces**: Es gibt viele Apps, die Taxifahrer bieten. Ich wurde empfehlen, dass Sie den Taxi-App von Uber verwenden.

**Road**: response for prompt. Which app do you recommend for tau?

**German subspace**: Would like to use Uber, because it's convenient and fast. (Explanation: This is not desired because of the response in English.)

**Instruction following subspace**: I recommend Uber because it's cheaper and more convenient than other apps.

**Combined subspaces**: Nachdem Sie sicher sind, dass die Uber-App ist nicht verflugbar in Ihrer Stadt, konnen Sie auch den Taxify-Dienst nutzen.

**Lokeff response for prompt. How can I find a marketing tab? In a college student and neon.

**German subspace**: Sie konnen sich mit dem Jobsuchdienst von Facebook verbinden, um eine Anzeige zu posten.

**Instruction following subspace**: Hello, I'm happy to help you with your question! There are several ways to find part-time jobs as a college student. Here are some options:...

**Combined subspaces**: Du kannst auf Jobborsen nachstellen, die sich auf Teilzeitjobs spezialisiert haben.

**Road**: response for prompt. How can I find a marketing tab? In a college student and neon.

**German subspace**: Suche ich nach einem Teilzeitjob. Ich bin Student und arbeitslos.

**Instruction following subspace**: You should look for a part-time position in your field of study or one that you are interested in learning about. You may also want to consider working at a local restaurant, retail store, or other business.

**Combined subspaces**: Siehe dir nach, ob es in der Nahe einen Jobcenter gibt.

Figure D.3: Qualitative comparison between RoAd and LoReFT for their composability. The prompt is always in English for different subspaces.