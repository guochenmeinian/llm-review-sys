# Sample based Explanations via Generalized Representers

 Che-Ping Tsai

Machine Learning Department

Carnegie Mellon University

chepingt@andrew.cmu.edu

&Chih-Kuan Yeh

Google Deepmind 1

jason6582@gmail.com

&Pradeep Ravikumar

Machine Learning Department

Carnegie Mellon University

pradeepr@cs.cmu.edu

Footnote 1: Work done in Carnegie Mellon University.

###### Abstract

We propose a general class of sample based explanations of machine learning models, which we term _generalized representers_. To measure the effect of a training sample on a model's test prediction, generalized representers use two components: _a global sample importance_ that quantifies the importance of the training point to the model and is invariant to test samples, and _a local sample importance_ that measures similarity between the training sample and the test point with a kernel. A key contribution of the paper is to show that generalized representers are the only class of sample based explanations satisfying a natural set of axiomatic properties. We discuss approaches to extract global importances given a kernel, and also natural choices of kernels given modern non-linear models. As we show, many popular existing sample based explanations could be cast as generalized representers with particular choices of kernels and approaches to extract global importances. Additionally, we conduct empirical comparisons of different generalized representers on two image and two text classification datasets.

## 1 Introduction

As machine learning becomes increasingly integrated into various aspects of human life, the demand for understanding, interpreting, and explaining the decisions made by complex AI and machine learning models has grown. Consequently, numerous approaches have been proposed in the field of Explainable AI (XAI). Feature based explanations interpret models by identifying the most relevant input features [1], while sample based explanations do so via the most relevant training samples [1], [2]. Although different methods emphasize different aspects of the model, some may even have conflicting philosophies [9]. To address this issue, there have been growing calls within the XAI community for more objective or normative approaches [10], [11], which could help align XAI techniques more effectively with human needs.

One of the most straightforward approaches to assess the effectiveness of explanations is by evaluating their utility in downstream applications [13], [14]. However, such evaluations can be costly, particularly during the development stages of explanations, as they often necessitate the involvement of real human users. As a result, a well-grounded, axiom-based evaluation can be beneficial for designing and selecting explanations for implementation. Axioms can be viewed as theoretical constraints that dictate how explanations should behave in response to specific inputs. A notable example is the Shapley value [15], which originated in cooperative game theory and has gained popularity in XAI due to its appealing axiomatic properties. Nonetheless, while axiomatic approaches have been widely applied in identifying significant features [1], [16], feature interactions [13], [19], and high-level concepts [20], they have not been extensively discussed in sample based explanations.

[MISSING_PAGE_FAIL:2]

model \(f\in\mathcal{F}\), training data points \(\mathcal{D}\), and an arbitrary test data point \(\mathbf{x}\in\mathbb{R}^{d}\) as input, and outputs a vector of explanation weights \([\phi_{\mathcal{D}}(f,(\mathbf{x}_{1},y_{1}),\mathbf{x}),\cdots,\phi_{\mathcal{D }}(f,(\mathbf{x}_{n},y_{n}),\mathbf{x})]\in\mathbb{R}^{n}\) with each value corresponding to an importance score of each training sample to the test point. In the sequel, we will suppress the explicit dependence on the entire set of training points in the notation for the explanation functional and the dependence on the training label \(y_{i}\). Also, to make clear that the first data point argument is the training sample, and the second is the test sample, we will use \(\phi(f,\mathbf{x}_{i}\rightarrow\mathbf{x})\in\mathbb{R}\) to denote the sample explanation weight for \(\mathbf{x}_{i}\) to explain the prediction of the model \(f(\cdot)\) for the test point \(\mathbf{x}\).

## 3 Axioms for Sample based Explanations

In this section, we begin by presenting a collection of axioms that describe various desirable characteristics of sample based explanations.

**Definition 1** (Efficiency Axiom).: _For any model \(f\), and test point \(x\in\mathbb{R}^{d}\), a sample based explanation \(\phi(\cdot)\) satisfies the efficiency axiom iff:_

\[\sum_{i=1}^{n}\phi(f,\mathbf{x}_{i}\rightarrow\mathbf{x})=f(\mathbf{x}).\]

The efficiency axiom entails that the sum of the attributions to each training sample together adds up to the model prediction at the test point. This is a natural counterpart of the efficiency axioms used in the Shapley values [20].

**Definition 2** (Self-Explanation Axiom).: _A sample based explanation \(\phi(\cdot)\) satisfies the self-explanation axiom iff there exists any training point \(\mathbf{x}_{i}\) having no effect on itself, i.e. \(\phi(f,\mathbf{x}_{i}\rightarrow\mathbf{x}_{i})=0\), the training point should not impact any other points, i.e. \(\phi(f,\mathbf{x}_{i}\rightarrow\mathbf{x})=0\) for all \(\mathbf{x}\in\mathbb{R}^{d}\)._

The self-explanation axiom states that if the label \(y_{i}\) does not even have an impact on the model's prediction for \(\mathbf{x}_{i}\), it should not impact other test predictions. This axiom shares a similar intuition as the dummy axiom in the Shapley values [1] since both axioms dictate that explanations should be zero if a training sample has no impact on the model. However, the self-explanation axiom requires a different theoretical treatments due to the additional focus in generalized representers of explaining a model prediction on a particular test sample.

**Definition 3** (Symmetric Zero Axiom).: _A sample explanation \(\phi(\cdot)\) satisfies the symmetric zero axiom iff any two training points \(\mathbf{x}_{i},\mathbf{x}_{j}\) such that if \(\phi(f,\mathbf{x}_{i}\rightarrow\mathbf{x}_{i})\neq 0\) and \(\phi(f,\mathbf{x}_{j}\rightarrow\mathbf{x}_{j})\neq 0\), then_

\[\phi(f,\mathbf{x}_{i}\rightarrow\mathbf{x}_{j})=0\implies\phi(f,\mathbf{x}_{j} \rightarrow\mathbf{x}_{i})=0.\]

The symmetric-zero axiom underscores the bidirectional nature of "orthogonality". It emphasizes that if a sample has no impact on another sample, this lack of correlation is mutual and implies that they are orthogonal.

**Definition 4** (Symmetric Cycle Axiom).: _A sample explanation \(\phi(\cdot)\) satisfies the symmetric cycle axiom iff for any set of training points \(\mathbf{x}_{t_{1}},...\mathbf{x}_{t_{k}}\), with possible duplicates, and \(\mathbf{x}_{t_{k+1}}=\mathbf{x}_{t_{1}}\), it holds that:_

\[\prod_{i=1}^{k}\phi(f,\mathbf{x}_{t_{i}}\rightarrow\mathbf{x}_{t_{i+1}})=\prod _{i=1}^{k}\phi(f,\mathbf{x}_{t_{i+1}}\rightarrow\mathbf{x}_{t_{i}}).\]

Let us first consider the vacuous case of two points: \(\mathbf{x}_{1},\mathbf{x}_{2}\), for which the axiom is the tautology that: \(\phi(f,\mathbf{x}_{1}\rightarrow\mathbf{x}_{2})\phi(f,\mathbf{x}_{2} \rightarrow\mathbf{x}_{1})=\phi(f,\mathbf{x}_{2}\rightarrow\mathbf{x}_{1}) \phi(f,\mathbf{x}_{1}\rightarrow\mathbf{x}_{2})\). Let us next look at the case with three points: \(\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3}\), for which the axiom entails:

\[\phi(f,\mathbf{x}_{1}\rightarrow\mathbf{x}_{2})\phi(f,\mathbf{x}_{2} \rightarrow\mathbf{x}_{3})\phi(f,\mathbf{x}_{3}\rightarrow\mathbf{x}_{1})= \phi(f,\mathbf{x}_{3}\rightarrow\mathbf{x}_{2})\phi(f,\mathbf{x}_{2} \rightarrow\mathbf{x}_{1})\phi(f,\mathbf{x}_{1}\rightarrow\mathbf{x}_{3}).\]

It can be seen that this is a generalization of simply requiring that the explanations be symmetric as in the symmetry axiom in the Shapley values. In fact, the unique explanation satisfying this and other listed axioms is in general not symmetric. The axiom could also be viewed as a conservation or path independence law, in that the flow of explanation based information from a point \(\mathbf{x}_{i}\) to itself in a cycle is invariant to the path taken.

**Definition 5** (Continuity Axiom).: _A sample based explanation \(\phi(\cdot)\) satisfies the continuity axiom iff it is continuous wrt the test data point \(\mathbf{x}\), for any fixed training point \(\mathbf{x}_{i}\):_

\[\lim_{\mathbf{x}^{\prime}\rightarrow\mathbf{x}}\phi(f,\mathbf{x}_{i}\to \mathbf{x}^{\prime})=\phi(f,\mathbf{x}_{i}\to\mathbf{x}).\]

Such continuity is a minimal requirement on the regularity of the explanation functional, and which ensures that infinitesimal changes to the test point would not incur large changes to the explanation functional.

**Definition 6** (Irreducibility Axiom).: _A sample explanation \(\phi(\cdot)\) satisfies the irreducibility axiom iff for any number of training points \(\mathbf{x}_{1},...,\mathbf{x}_{k}\),_

\[\text{det}\begin{pmatrix}\phi(f,\mathbf{x}_{1},\mathbf{x}_{1})&\phi(f, \mathbf{x}_{1},\mathbf{x}_{2})&...&\phi(f,\mathbf{x}_{1},\mathbf{x}_{k})\\ \phi(f,\mathbf{x}_{2},\mathbf{x}_{1})&\phi(f,\mathbf{x}_{2},\mathbf{x}_{2})&... &\phi(f,\mathbf{x}_{2},\mathbf{x}_{k})\\...&...&...&...\\ \phi(f,\mathbf{x}_{k},\mathbf{x}_{1})&\phi(f,\mathbf{x}_{k},\mathbf{x}_{2})&... &\phi(f,\mathbf{x}_{k},\mathbf{x}_{k})\end{pmatrix}\geq 0.\]

A sufficient condition for an explanation \(\phi(\cdot)\) to satisfy the irreducibility axiom is for

\[|\phi(f,\mathbf{x}_{i}\to\mathbf{x}_{i})|>\sum_{j\neq i}|\phi(f,\mathbf{x}_{i} \to\mathbf{x}_{j})|, \tag{1}\]

since this makes the matrix above strictly diagonally dominant, and since the diagonal entries are non-negative, by the Gershgorin circle theorem, the eigenvalues are all non-negative as well, so that the determinant in turn is non-negative.

The continuity and irreducibility axiom primarily serves a function-analytic purpose by providing sufficient and necessary conditions of a kernel being a Mercer kernel, which requires that the kernel function be continuous and positive semi-definite.

We are now ready to investigate the class of explanations that satisfy the axioms introduced above.

**Theorem 7**.: _An explanation functional \(\phi(f,\cdot,\cdot)\) satisfies the continuity, self-explanation, symmetric zero, symmetric cycle, and irreducibility axioms for any training samples \(\mathcal{D}\) containing \(n\) training samples \((\mathbf{x}_{i},y_{i})\in\mathbb{R}^{d}\times\mathbb{R}\) for all \(i\in[n]\) iff_

\[\phi(f,\mathbf{x}_{i}\to\mathbf{x})=\alpha_{i}K(\mathbf{x}_{i},\mathbf{x})\;\; \;\forall i\in[n], \tag{2}\]

_for some \(\alpha\in\mathbb{R}^{n}\) and some continuous positive-definite kernel \(K:\mathbb{R}^{d}\times\mathbb{R}^{d}\mapsto\mathbb{R}\)._

This suggests that a sample explanation \(\phi(f,\mathbf{x}_{i}\to\mathbf{x})=\alpha_{i}K(\mathbf{x}_{i},\mathbf{x})\) has two components: a weight \(\alpha_{i}\) associated with just the training point \(\mathbf{x}_{i}\) independent of test points, and a similarity \(K(\mathbf{x}_{i},\mathbf{x})\) between the training and test points specified by a Mercer kernel. Following Yeh et al. [2], we term the first component the _global importance_ of the training sample \(\mathbf{x}_{i}\) and the second component the _local importance_ that measures similarities between training and test samples.

Once we couple this observation together with the efficiency axiom, one explanation that satisfies these properties is:

\[f(\mathbf{x})=\sum_{j=1}^{n}\phi(f,\mathbf{x}_{i}\to\mathbf{x})=\sum_{j=1}^{n }\alpha_{i}K(\mathbf{x}_{i},\mathbf{x})\;\;\text{, for any }x\in\mathbb{R}^{p}. \tag{3}\]

This can be seen to hold only if the target function \(f\) lies in the RKHS subspace spanned by the kernel evaluations of training points. When this is not necessarily the case, then the efficiency axiom (where the sum of training sample importances equals the function value) exactly, cannot be satisfied exactly. We can however satisfy the efficiency axiom approximately with the approximation error arising from projecting the target function \(f\) onto the RKHS subspace spanned by training representers.

This thus provides a very simple and natural framework for specifying sample explanations: (1) specify a Mercer kernel \(K(\cdot,\cdot)\) so that the target function can be well approximated by the corresponding kernel machine, and (2) project the given model onto the RKHS subspace spanned by kernel evaluations on the training points. Each of the sample explanation weights then has a natural specification in terms of global importance associated with each training point (arising from the projection of the function onto the RKHS subspace, which naturally does not depend on any test points), as well as a localized component that is precisely the kernel similarity between the training and test points.

Deriving Global Importance Given Kernel Functions

The previous section showed that the class of sample explanations that satisfy a set of key axioms naturally correspond to an RKHS subspace. Thus, all one needs, in order to specify the sample explanations, is to specify a Mercer kernel function \(K\) and solve for the corresponding global importance weights \(\alpha\). In this section, we focus on the latter problem, and present three methods to compute the global importance weights given some kernel \(K\).

### Method 1: Projecting Target Function onto RKHS Subspace

The first method is to project the target function onto the RKHS subspace spanned by kernel evaluations on the training points. Given the target function \(f\), loss function \(\mathcal{L}:\mathbb{R}\times\mathbb{R}\mapsto\mathbb{R}\) and training dataset \(\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\) (potentially, though not necessarily used to train \(f\)), and a user-specified Mercer kernel \(K\), our goal is to find a projection \(\hat{f}_{K}\) of the target model onto the RKHS subspace defined by \(\mathcal{H}_{n}=\text{span}(\{K(\mathbf{x}_{i},\cdot)\}_{i=1}^{n}))\). To accomplish this, we formulate it as a RKHS regression problem:

\[\hat{f}_{K}=\operatorname*{argmin}_{f_{K}\in\mathcal{H}_{K}}\left\{\frac{1}{n }\sum_{i=1}^{n}\mathcal{L}(f_{K}(\mathbf{x}_{i}),f(\mathbf{x}_{i}))+\frac{ \lambda}{2}\|f_{K}\|_{\mathcal{H}_{K}}^{2}\right\}, \tag{4}\]

where \(\mathcal{H}_{K}\) as the RKHS defined by kernel \(K\), \(\|\cdot\|_{\mathcal{H}_{K}}:\mathcal{H}_{K}\mapsto\mathbb{R}\) is the RKHS norm, and \(\lambda\) is a regularization parameter that controls the faithfulness and complexity of the function \(\hat{f}_{K}\). The loss function \(\mathcal{L}\) can be chosen as the objective function used to train the target function \(f\) to closely emulate the behavior of target function \(f\) and its dependence on the training samples \(\mathcal{D}\). By the representer theorem [21], the regularization term \(\|f_{K}\|_{\mathcal{H}_{K}}^{2}\) added here ensures that the solution lies in the RKHS subspace \(\mathcal{H}_{n}\) spanned by kernel evaluations. Indeed, by the representer theorem [21], the minimizer of Eqn.(4) can be represented as \(\hat{f}_{K}(\cdot)=\sum_{i=1}^{n}\alpha_{i}K(\mathbf{x}_{i},\cdot)\) for some \(\alpha\in\mathbb{R}^{n}\), which allows us to reparameterize Eqn.(4):

\[\hat{\alpha}=\operatorname*{argmin}_{\alpha\in\mathbb{R}^{n}}\left\{\frac{1}{ n}\sum_{i=1}^{n}\mathcal{L}\left(\sum_{j=1}^{n}\alpha_{j}K(\mathbf{x}_{i}, \mathbf{x}_{j}),f(\mathbf{x}_{i})\right)+\frac{\lambda}{2}\alpha^{\top} \mathbf{K}\alpha\right\}, \tag{5}\]

where \(\mathbf{K}\in\mathbb{R}^{n\times n}\) is the kernel gram matrix defined as \(K_{ij}=K(\mathbf{x}_{i},\mathbf{x}_{j})\) for \(i,j\in[n]\), and we use the fact that \(\|f_{K}\|_{\mathcal{H}_{K}}=\langle\sum_{i=1}^{n}\alpha_{i}K(\mathbf{x}_{i}, \cdot),\sum_{i=1}^{n}\alpha_{i}K(\cdot,\mathbf{x}_{i})\rangle^{\frac{1}{2}}= \sqrt{\alpha^{\top}\mathbf{K}\alpha}\). By solving the first-order optimality condition, the global importance \(\alpha\) must be in the following form:

**Proposition 8**.: _(Surrogate derivative) The minimizer of Eqn.(4) can be represented as \(\hat{f}_{K}=\sum_{i=1}^{n}\hat{\alpha}_{i}K(\mathbf{x}_{i},\cdot)\), where_

\[\hat{\alpha}\in\{\alpha^{*}+v\mid v\in\text{null}(\mathbf{K})\}\text{ and }\alpha_{i}^{*}=-\frac{1}{n\lambda}\frac{\partial\mathcal{L}(\hat{f}_{K}( \mathbf{x}_{i}),f(\mathbf{x}_{i}))}{\partial\hat{f}_{K}(\mathbf{x}_{i})},\ \ \forall i\in[n]. \tag{6}\]

_We call \(\alpha_{i}^{*}\) the surrogate derivative since it is the derivative of the loss function with respect to the surrogate function prediction._

\(\alpha_{i}^{*}\) can be interpreted as the measure of how sensitive \(\hat{f}_{K}(\mathbf{x}_{i})\) is to changes in the loss function. Although the global importance \(\alpha\) solved via Eqn.(5) may not be unique as indicated by the above results, the following proposition ensures that all \(\bar{\alpha}\in\{\alpha^{*}+v\mid v\in\text{null}(\mathbf{K})\}\) result in the same surrogate function \(\hat{f}_{K}=\sum_{i=1}^{n}\alpha_{i}^{*}K(\mathbf{x}_{i},\cdot)\).

**Proposition 9**.: _For any \(v\in\text{null}(\mathbf{K})\), the function \(f_{v}=\sum_{i=1}^{n}v_{i}K(\mathbf{x}_{i},\cdot)\) specified by the span of kernel evaluations with weights \(v\) is a zero fucntion, such that \(f_{v}(\mathbf{x})=0\) for all \(\mathbf{x}\in\mathbb{R}^{d}\)._

The proposition posits that adding any \(v\in\text{null}(\mathbf{K})\) to \(\alpha^{*}\) has no effect on the function \(\hat{f}_{K}\). Therefore, we use \(\alpha^{*}\) to represent the global importance as it captures the sensitivity of the loss function to the prediction of the surrogate function.

### Method 2: Approximation Using the Target Function

Given the derivation of global importance weights \(\alpha^{*}\) in Eqn.(3), we next consider a variant replacing the surrogate function prediction \(\hat{f}_{K}(\mathbf{x}_{i})\) with the target function prediction \(f(\mathbf{x}_{i})\):

**Definition 10** (Target derivative).: _The global importance computed with derivatives of the loss function with respect to the target function prediction is defined as:_

\[\alpha^{*}_{i}=-\frac{\partial\mathcal{L}(f(x_{i}),y_{i})}{\partial f(x_{i})}, \ \ \forall i\in[n], \tag{7}\]

_where \(\mathcal{L}(\cdot,\cdot)\) is the loss function used to train the target function._

A crucial advantage of this variant is that we no longer need solve for an RKHS regression problem. There are several reasons why this approximation is reasonable. Firstly, the loss function in Eqn.(4) encourages the surrogate function to produce similar outputs as the target function, so that \(\hat{f}_{K}(\mathbf{x}_{i})\) is approximately equal to \(f(x_{i})\). Secondly, when the target function exhibits low training error, which is often the case for overparameterized neural networks that are typically in an interpolation regime, we can assume that \(f(x_{i})\) is close to \(y_{i}\). Consequently, the target derivative can serve as an approximation of the surrogate derivative in Eqn.(6). As we will show below, the influence function approach [3] is indeed as the product between the target derivative and the influence function kernel.

### Method 3: Tracking Gradient Descent Trajectories

Here, we propose a more scalable variant we term _tracking representers_ that accumulates changes in the global importance during kernel gradient descent updates when solving Eqn.(4). Let \(\Phi:\mathbb{R}^{d}\mapsto\mathcal{H}\) be a feature map corresponding to the kernel \(K\), so that \(K(\mathbf{x},\mathbf{x}^{\prime})=\langle\Phi(\mathbf{x}),\Phi(\mathbf{x}^{ \prime})\rangle\). We can then cast any function in the RKHS as \(f_{K}(\mathbf{x})=\langle\theta,\Phi(\mathbf{x})\rangle\) for some parameter \(\theta\in\mathcal{H}\). Suppose we solve the unregularized projection problem in Eqn.(4) via stochastic gradient descent updates on the parameter \(\theta\): \(\theta^{(t)}=\theta^{(t-1)}-\frac{\eta^{(t)}}{|B^{(t)}|}\sum_{i\in B^{(t)}} \nabla_{\theta}\mathcal{L}(f_{\theta}(\mathbf{x}_{i}),f(\mathbf{x}_{i}))\Phi( \mathbf{x}_{i})|_{\theta=\theta^{(t-1)}}\), where we use \(B^{(t)}\) and \(\eta^{(t)}\) to denote the minibatch and the learning rate. The corresponding updates to the function is then given by "kernel gradient descent" updates: \(f^{(t)}_{K}(\mathbf{x})=f^{(t-1)}_{K}(\mathbf{x})-\alpha_{it}K(\mathbf{x}_{i},\mathbf{x})\), where \(\alpha_{it}=\frac{\eta^{(t)}}{|B^{(t)}|}\sum_{i\in B^{(t)}}\frac{\partial \mathcal{L}(f^{(t-1)}_{K}(\mathbf{x}_{i}),f(\mathbf{x}_{i}))}{\partial f^{(t- 1)}_{K}(\mathbf{x}_{i})}\). The function at step \(T\) can then be represented as:

\[f^{(T)}_{K}(\mathbf{x})=\sum_{i=1}^{n}\alpha^{(T)}_{i}K(\mathbf{x}_{i}, \mathbf{x})+f^{(0)}_{K}(\mathbf{x})\text{ with }\alpha^{(T)}_{i}=-\sum_{t:i\in B ^{(t)}}\frac{\eta^{(t)}}{|B^{(t)}|}\frac{\partial\mathcal{L}(f^{(t-1)}_{K}( \mathbf{x}_{i}),f(\mathbf{x}_{i}))}{\partial f^{(t-1)}_{K}(\mathbf{x}_{i})}. \tag{8}\]

**Definition 11** (Tracking representers).: _Given a finite set of steps \(T\), we term the global importance weights obtained via tracking kernel gradient descent as tracking representers:_

\[\alpha^{*}_{i}=-\sum_{t\in[T]\,:\,i\in B^{(t)}}\frac{\eta^{(t)}}{|B^{(t)}|} \frac{\partial\mathcal{L}(f^{(t-1)}_{K}(\mathbf{x}_{i}),f(\mathbf{x}_{i}))}{ \partial f^{(t-1)}_{K}(\mathbf{x}_{i})}. \tag{9}\]

We note that one can draw from standard correspondences between gradient descent with finite stopping and ridge regularization (e.g. [30]), to in turn relate the iterates of the kernel gradient descent updates for any finite stopping at \(T\) iterations to regularized RKHS regression solutions for some penalty \(\lambda\). The above procedure thus provides a potentially scalable approach to compute the corresponding global importances: in order to calculate the global importance \(\alpha^{(T)}_{i}\), we need to simply monitor the evolution of \(\alpha^{(t)}_{i}\) when the sample \(\mathbf{x}_{i}\) is utilized at iteration \(t\). In our experiment, we use the following relaxation for further speed up:

\[\alpha^{*}_{i}=-\sum_{t\in[T]\,:\,i\in B^{(t)}}\frac{\eta^{(t)}}{|B^{(t)}|} \frac{\partial\mathcal{L}(f^{(t-1)}(\mathbf{x}_{i}),y_{i})}{\partial f^{(t-1)} (\mathbf{x}_{i})}, \tag{10}\]

where we assume the target model is trained with (stochastic) gradient descent, \(f^{(t)}(\mathbf{x}_{i})\) denotes the target model at \(t^{\text{th}}\) iteration during training, and \(B^{(t)}\) and \(\eta^{(t)}\) are the corresponding mini-batch and learning rate. Similar to the intuition of replacing the surrogate derivative with to target derivative, we track the target model's training trajectory directly instead of solving Eqn.(4) with kernel gradient descent.

Choice of Kernels for Generalized Representers

Previously, we discussed approaches for deriving global importance given user-specified kernels, which can in general be specified by domain knowledge relating to the model and the application domain. In this section, we discuss natural choices of kernels for modern non-linear models. Moreover, we show that existing sample based explanation methods such as representer points [] and influence functions [] could be viewed as making particular choices of kernels when computing general representers. We also discuss TracIn [] as a natural extension of our framework to multiple rather than a single kernel.

### Kernel 1: Penultimate-layer Embeddings

A common method for extracting a random feature map from a neural network is to use the embeddings of its penultimate layer [][][][]. Let \(\Phi_{\Theta_{1}}:\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\) denote the mapping from the input to the second last layer. The target model \(f\) can be represented as

\[f(\mathbf{x})=\Phi_{\Theta_{1}}(\mathbf{x})^{\top}\Theta_{2}, \tag{11}\]

where \(\Theta_{2}\in\mathbb{R}^{\ell}\) is the weight matrix of the last layer. That is, we treat the deep neural network as a linear machine on top of a learned feature map. The kernel function is then defined as \(K_{\text{LL}}(\mathbf{x},\mathbf{z})=\langle\Phi_{\Theta_{1}}(\mathbf{x}), \Phi_{\Theta_{1}}(\mathbf{z})\rangle,\forall\mathbf{x},\mathbf{z}\in\mathbb{ R}^{d}\). This is the case with most deep neural network architectures, where the feature map \(\Phi_{\Theta_{1}}\) is specified via deep compositions of parameterized layers that take the form of fully connected layers, convolutional layers, or attention layers among others. While the last-layer weight matrix \(\Theta_{2}\) may not lie in the span of \(\{\Phi_{\theta_{1}}(\mathbf{x}_{i})\}_{i=1}^{n}\), we may solve the its explanatory surrogate function using Eqn. [].

**Corollary 12**.: _(Representer point selection []) The minimizer of Eqn. [], instantiated with \(K_{\text{LL}}(\mathbf{x},\mathbf{z})=\langle\Phi_{\Theta_{1}}(\mathbf{x}),\Phi _{\Theta_{1}}(\mathbf{z})\rangle,\forall\mathbf{x},\mathbf{z}\in\mathbb{R}^{d}\), can be represented as_

\[\hat{f}_{K}(\cdot)=\sum_{i=1}^{n}\alpha_{i}K_{\text{LL}}(\mathbf{x}_{i},\cdot),\text{ where }\alpha_{i}=-\frac{1}{n\lambda}\frac{\partial\mathcal{L}(\hat{f}_{K}( \mathbf{x}_{i}),f(\mathbf{x}_{i}))}{\partial\hat{f}_{K}(\mathbf{x}_{i})},\ \ \forall i\in[n]. \tag{12}\]

The above corollary implies that \(\hat{\Theta}_{2}=\sum_{i=1}^{n}\alpha_{i}\Phi_{\theta_{1}}(\mathbf{x}_{i})\). In other words, the RKHS regularization in Eqn. [] can be expressed as \(\|f_{K}\|_{\mathcal{H}_{K}}^{2}=\|\Theta_{2}\|^{2}\), which is equivalent to L2 regularization. Consequently, the representer point selection method proposed in Yeh et al. [] can be generalized to our framework when we use last-layer embeddings as feature maps.

### Kernel 2: Neural Tangent Kernels

Although freezing all layers except for the last layer is a straightforward way to simplify neural networks to linear machines, last-layer representers may overlook influential behavior that is present in other layers. For example, Yeh et al. [] shows that representation in the last layer leads to inferior results for language models. On the other hand, neural tangent kernels (NTK) [] have been demonstrated as a more accurate approximation of neural networks [][][][]. By using NTKs, we use gradients with respect to model parameters as feature maps and approximate neural networks with the corresponding kernel machines. This formulation enables us to derive a generalized representer that captures gradient information of all layers.

For a neural network with scalar output \(f_{\theta}:\mathbb{R}^{d}\mapsto\mathbb{R}\) parameterized by a vector of parameters \(\theta\in\mathbb{R}^{p}\), the NTK is a kernel \(K:\mathbb{R}^{d}\times\mathbb{R}^{d}\mapsto\mathbb{R}\) defined by the feature maps \(\Phi_{\theta}(\mathbf{x})=\frac{\partial f_{\theta}(\mathbf{x})}{\partial \theta}\):

\[K_{\text{NTK},\theta}(\mathbf{x},\mathbf{z})=\left\langle\frac{\partial f_{ \theta}(\mathbf{x})}{\partial\theta},\frac{\partial f_{\theta}(\mathbf{z})}{ \partial\theta}\right\rangle. \tag{13}\]

Connection to TracIn []:TracIn measures _the change in model parameters from the start to the end of training_. While it is intractable due to the need to store model parameters of all iterations, Pruthi et al. [] used checkpoints(CP) as a practical relaxation: given model parameters \(\theta^{(t)}\) and learning rates \(\eta^{(t)}\) at all model checkpoints \(t=0,\cdots,T\), the formulation of TracInCP is given below

\[\phi_{\text{TracInCP}}(f_{\theta},(\mathbf{x}_{i},y_{i})\to \mathbf{x}) =-\sum_{t=0}^{T}\eta^{(t)}\frac{\partial\mathcal{L}(f_{\theta}( \mathbf{x}_{i}),y_{i})}{\partial\theta}^{\top}\frac{\partial f_{\theta}( \mathbf{x})}{\partial\theta}\Big{|}_{\theta=\theta^{(t)}}\] \[=-\sum_{t=0}^{T}\underbrace{\eta^{(t)}\frac{\partial\mathcal{L}( f_{\theta}(\mathbf{x}_{i}),y_{i})}{\partial f_{\theta}(\mathbf{x}_{i})}\Big{|}_{ \theta=\theta^{(t)}}}_{\text{global importance}}\underbrace{K_{\text{NTK}, \theta^{(t)}}(\mathbf{x}_{i},\mathbf{x})}_{\text{local importance}}. \tag{14}\]

When the learning rate is constant throughout the training process, TracInCP can be viewed as a generalized representer instantiated with target derivative as global importances and NTK (Eqn.(13)) as the kernel function, but uses different kernels on different checkpoints.

### Kernel 3: Influence Function Kernel

The influence functions [3] can also be represented as a generalized representer with the following kernel:

\[K_{\text{Inf},\theta}(\mathbf{x},\mathbf{z})=\left\langle\frac{\partial f_{ \theta}(\mathbf{x})}{\partial\theta},\frac{\partial f_{\theta}(\mathbf{z})}{ \partial\theta}\right\rangle_{H_{\theta}^{-1}}=\frac{\partial f_{\theta}( \mathbf{x})^{\top}}{\partial\theta}H_{\theta}^{-1}\frac{\partial f_{\theta}( \mathbf{z})}{\partial\theta}, \tag{15}\]

where \(H_{\theta}=\frac{1}{n}\sum_{i=1}^{n}\frac{\partial^{2}\mathcal{L}(f_{\theta}( x_{i}),y_{i})}{\partial\theta^{2}}\) is the Hessian matrix with respect to target model parameters. The influence function then can be written as:

\[\phi_{\text{Inf}}(f_{\theta},(\mathbf{x}_{i},y_{i})\to\mathbf{x})=-\frac{ \partial\mathcal{L}(f_{\theta}(\mathbf{x}_{i}),y_{i})}{\partial\theta}H_{ \theta}^{-1}\frac{\partial f_{\theta}(\mathbf{x})}{\partial\theta}=-\underbrace {\frac{\partial\mathcal{L}(f_{\theta}(\mathbf{x}_{i}),y_{i})}{\partial f_{ \theta}(\mathbf{x}_{i})}}_{\text{global importance}}\underbrace{K_{\text{Inf}, \theta}(\mathbf{x}_{i},\mathbf{x})}_{\text{local importance}}. \tag{16}\]

Therefore, the influence function can be seen as a member of generalized representers with target derivative global importance (Definition 10 and the influence function kernel. Influence functions [3] were designed to measure _how would the model's predictions change if a training input were perturbed_ for convex models trained with empirical risk minimization. Consequently, the inversed Hessian matrix describes the sensitivity of the model parameters in each direction.

## 6 Experiments

In the experiment, we compare different representers within our proposed framework on both vision and language classification tasks. We use convolutional neural networks (CNN) since they are widely recognized deep neural network architectur. We compare perforamnce of different choices of kernels and different ways to compute global importance. Existing generalized representers, such as influence functions [3], representer point selections [3], and TracIn [3], are included in our experiment.

### Experimental Setups

Evaluation metrics:We use _case deletion diagnostics_[3][5][5][5], \(\text{DEL}_{-}(\mathbf{x},k,\phi)\), as our primary evaluation metric. The metric measures _the difference between models' prediction score on \(\mathbf{x}\) when we remove top-\(k\) negative impact samples given by method \(\phi\) and the prediction scores of the original models_. We expect \(\text{DEL}_{-}\) to be positive since models' prediction scores should increase when we remove negative impact samples. To evaluate deletion metric at different \(k\), we follow Yeh et al. [3] and report area under the curve (AUC): \(\text{AUC-DEL}_{-}=\sum_{i=1}^{m}\text{DEL}_{-}(\mathbf{x},k_{i},\phi)/m\), where \(k_{1}<k_{2}<\cdots<k_{m}\) is a predefined sequence of \(k\).

We choose \(k_{i}=0.02in\) for \(i=0,1,\cdots,5\) with \(n\) as the size of the training set. The average of each metric is calculated across \(50/200\) randomly initialized neural networks for vision/language data. For every neural network, sample-based explanation methods are computed for \(10\) randomly selected testing samples.

Datasets and models being explained:For image classification, we follow Pruthi et al. [6] and use MNIST [59] and CIFAR-10 [60] datasets. For text classification, we follow Yeh et al. [53] and use Toxicity and AGNews datasets, which contain toxicity comments and news of different categories respectively. Due to computational challenges in computing deletion diagnostics, we subsample the datasets by transforming them into binary classification problems with each class containing around \(6,000\) training samples. The CNNs we use for the four datasets comprise \(3\) layers. For vision datasets, the models contain around \(95,000\) parameters. For text datasets, the total number of parameters in the model is \(1,602,257\) with over \(90\%\) of the parameters residing in the word embedding layer that contains \(30,522\) trainable word embeddings of dimensions \(48\). Please refer to the Appendix for more details on the implementation of generalized representers and dataset constructions.

### Experimental Results

The results are shown in Table [1]. We also provide deletion curves we compute AUC-DEL\({}_{-}\) for in the Appendix.

I. Comparison of different global importance:In the first experiment, we fix the kernel to be the NTK computed on final model parameters, and we compare different methods for computing global importance in Section 3. We do not compute the surrogate derivative on the text datasets since the total numbers of parameters are too large, making the computation infeasible.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \hline \multicolumn{2}{|c|}{Datasets} & \multicolumn{4}{c|}{Methods} \\ \hline \hline \multicolumn{5}{|c|}{Experiment I - Comparison of different global importance for generalized representers} \\ \hline Kernels & \multicolumn{4}{c|}{NTK-final} & Random \\ \hline Global importance & surrogate derivative & target derivative & tracking & Selection \\ \hline MNIST & \(1.88\pm 0.25\) & \(2.41\pm 0.30\) & \(\mathbf{3.52\pm 0.48}\) & \(-0.50\pm 0.16\) \\ CIFAR-10 & \(2.27\pm 0.18\) & \(2.81\pm 0.20\) & \(\mathbf{3.26\pm 0.19}\) & \(0.136\pm 0.10\) \\ \hline Toxicity & \(-\) & \(1.10\pm 0.21\) & \(\mathbf{2.08\pm 0.23}\) & \(0.15\pm 0.19\) \\ AGNews & \(-\) & \(1.88\pm 0.27\) & \(\mathbf{2.56\pm 0.27}\) & \(0.19\pm 0.26\) \\ \hline \hline \multicolumn{5}{|c|}{Experiment II - Comparison of different kernels for generalized representers} \\ \hline Global importance & \multicolumn{4}{c|}{tracking} \\ \hline Kernels & last layer-final & NTK-init & NTK-middle & NTK-final & Inf-final \\ \hline MNIST & \(3.44\pm 0.46\) & \(3.18\pm 0.46\) & \(3.63\pm 0.49\) & \(3.52\pm 0.48\) & \(\mathbf{3.66\pm 0.49}\) \\ CIFAR-10 & \(2.26\pm 0.13\) & \(1.35\pm 0.20\) & \(2.67\pm 0.19\) & \(3.26\pm 0.19\) & \(\mathbf{3.46\pm 0.19}\) \\ \hline Toxicity & \(1.34\pm 0.22\) & \(0.63\pm 0.22\) & \(1.90\pm 0.23\) & \(\mathbf{2.08\pm 0.23}\) & \(0.42\pm 0.20\)2  \\ AGNews & \(2.14\pm 0.27\) & \(1.81\pm 0.27\) & \(\mathbf{2.54\pm 0.28}\) & \(\mathbf{2.56\pm 0.27}\) & \(0.92\pm 0.26\)3  \\ \hline \hline \end{tabular}
\end{table}
Table 1: Case deletion diagnostics, AUC-DEL\({}_{-}\), for removing negative impact training samples on four different datasets. \(95\%\) confidence interval of averaged deletion diagnostics on \(50\times 10=500(\) or \(200\times 10=2000)\) samples is reported for vision (or language) data. Larger AUC-DEL\({}_{-}\) is better. Init, middle, and final denote initial parameters \(\theta^{(0)}\), parameters of a middle checkpoint \(\theta^{(T/2)}\), and final parameters \(\theta^{(T)}\) for neural networks trained with \(T\) epochs. 4We only use the last-layer parameters to compute influence functions as in [53] since the total number of parameters are too large for text models.

We observe that _tracking_ has the best performance, followed by _target derivative_ and then _surrogate derivative_. This could be due to the loss flattening when converged and the loss gradients becoming less informative. Consequently, accumulating loss gradients during training is the most effective approach. Moreover, if _tracking_ is not feasible when training trajectories are not accessible, we may use _target derivative_ instead of _surrogate derivative_ as an alternative to explain neural networks since they have similar performance.

II. Comparison of different kernels:Next, we fix the global importance to _tracking_ and compare different kernels in Section5. We employ the tracking representers to compute global importance since it showed the best performance in the previous experiment. We can see that the influence function kernel performs the best in the vision data sets, and the NTK-final kernel has the best performance in language data sets. Note that influence functions exhibit distinctly contrasting performances on image and text data, which could be attributed to our reliance solely on last-layer parameters for influence function computation on language datasets. This finding aligns with the conclusions of Yeh et al. [53], who suggest that the last layer gradients provide less informative insights for text classifiers.

In summary, these findings indicate that NTK-final is a dependable kernel selection due to its consistent high performance across all four datasets, while also offering a computational efficiency advantage over the influence function kernel. These results also demonstrate that accessing target model checkpoints for computing kernels is unnecessary since NTK and influence function on the final model already provide informative feature maps.

III. Comparison of different generalized representers:Finally, we compare the new generalized representer with other existing generalized representers. We categorize TracInCP, the influence function, and the representer point as existing generalized representers: TracInCP can be viewed as an ensemble of generalized representers with target derivatives using the Neural Tangent Kernel. The influence function can be expressed as the influence function kernel with the target derivative. Lastly, the representer point can be seen as a form of generalized representer that utilizes the last-layer kernel and the surrogate derivative.

We find that the Inf-final has comparable performance to TracInCP and they outperform other approaches. Although TracInCP has the best performance on MNIST, it requires accessing different checkpoints, which requires a significant amount of memory and time complexity. In contrast, the NTK and Inf tracking representers are more efficient since they only require tracking gradient descent trajectories during training without the need for storing checkpoints.

## 7 Conclusion and Future work

In this work, we present _generalized representers_ that are the only class of sample based explanations that satisfy a set of desirable axiomatic properties. We explore various techniques for computing generalized representers in the context of modern non-linear machine learning models and show that many popular existing methods fall into this category. Additionally, we propose tracking representers that track sample importance along the gradient descent trajectory. In future work, it would be of interest to derive different generalized representers by altering different global importance and choices of kernels, as well as investigating their applicability to diverse machine learning models and modalities.

## 8 Acknowledgements

We acknowledge the support of DARPA via FA8750-23-2-1015, ONR via N00014-23-1-2368, and NSF via IIS-1909816.

## References

* [1] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1135-1144. ACM, 2016.

* [2] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantamand, Devi Parikh, and Dhruv Parikh. Grad-cam: Visual explanations from deep networks via gradient-based localization. _International conference on computer vision_, 2017.
* [3] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viegas, and Martin Wattenberg. Smooth-grad: removing noise by adding noise. _arXiv preprint arXiv:1706.03825_, 2017.
* [4] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In _Advances in Neural Information Processing Systems_, pages 4765-4774, 2017.
* [5] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In _Proceedings of the 34th International Conference on Machine Learning-Volume 70_, pages 1885-1894. JMLR. org, 2017.
* [6] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence by tracing gradient descent. _Advances in Neural Information Processing Systems_, 33, 2020.
* [7] Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection for explaining deep neural networks. In _Advances in Neural Information Processing Systems_, pages 9291-9301, 2018.
* [8] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In _International Conference on Machine Learning_, pages 2242-2251. PMLR, 2019.
* [9] Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, and Himabindu Lakkaraju. The disagreement problem in explainable machine learning: A practitioner's perspective. _arXiv preprint arXiv:2202.01602_, 2022.
* [10] W James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods, and applications in interpretable machine learning. _Proceedings of the National Academy of Sciences_, 116(44):22071-22080, 2019.
* [11] Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. _arXiv preprint arXiv:1702.08608_, 2017.
* [12] Blair Bilodeau, Natasha Jaques, Pang Wei Koh, and Been Kim. Impossibility theorems for feature attribution. _arXiv preprint arXiv:2212.11870_, 2022.
* [13] Valerie Chen, Nari Johnson, Nicholay Topin, Gregory Plumb, and Ameet Talwalkar. Use-case-grounded simulations for explanation evaluation. _arXiv preprint arXiv:2206.02256_, 2022.
* [14] Kasun Amarasinghe, Kit T Rodolfa, Sergio Jesus, Valerie Chen, Vladimir Balayan, Pedro Saleiro, Pedro Bizarro, Ameet Talwalkar, and Rayid Ghani. On the importance of application-grounded experimental design for evaluating explainable ml methods. _arXiv preprint arXiv:2206.13503_, 2022.
* [15] Lloyd S Shapley. A value for n-person games. _Contributions to the Theory of Games_, 2(28):307-317, 1953.
* [16] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In _International Conference on Machine Learning_, 2017.
* [17] Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Suggala, David I Inouye, and Pradeep K Ravikumar. On the (in) fidelity and sensitivity of explanations. _Advances in Neural Information Processing Systems_, 32, 2019.
* [18] Che-Ping Tsai, Chih-Kuan Yeh, and Pradeep Ravikumar. Faith-shap: The faithful shapley interaction index. _Journal of Machine Learning Research_, 24(94):1-42, 2023.
* [19] Mukund Sundararajan, Kedar Dhamdhere, and Ashish Agarwal. The shapley taylor interaction index. In _International Conference on Machine Learning_, pages 9259-9268. PMLR, 2020.

* [20] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On completeness-aware concept-based explanations in deep neural networks. _Advances in Neural Information Processing Systems_, 33:20554-20565, 2020.
* [21] Bernhard Scholkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In _International conference on computational learning theory_, pages 416-426. Springer, 2001.
* [22] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [23] Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for tree ensembles. _arXiv preprint arXiv:1802.03888_, 2018.
* [24] Zayd Hammoudeh and Daniel Lowd. Training data influence analysis and estimation: A survey. _arXiv preprint arXiv:2212.04612_, 2022.
* [25] Tianhao Wang and Ruoxi Jia. Data banzhaf: A data valuation framework with maximal robustness to learning stochasticity. _arXiv preprint arXiv:2205.15466_, 2022.
* [26] Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting predictions from training data. _arXiv preprint arXiv:2202.00622_, 2022.
* [27] Yongchan Kwon and James Zou. Beta shapley: a unified and noise-reduced data valuation framework for machine learning. _arXiv preprint arXiv:2110.14049_, 2021.
* [28] Tom Yan and Ariel D Procaccia. If you like shapley then you'll love the core. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 5751-5759, 2021.
* [29] Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramer, and Nicholas Carlini. Counterfactual memorization in neural language models. _arXiv preprint arXiv:2112.12938_, 2021.
* [30] Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regularities of labeled data in overparameterized models. _arXiv preprint arXiv:2002.03206_, 2020.
* [31] Ruoxi Jia, Fan Wu, Xuehui Sun, Jiacen Xu, David Dao, Bhavya Kailkhura, Ce Zhang, Bo Li, and Dawn Song. Scalability vs. utility: Do we have to sacrifice one for the other in data importance quantification? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8239-8247, 2021.
* [32] Jinsung Yoon, Sercan Arik, and Tomas Pfister. Data valuation using reinforcement learning. In _International Conference on Machine Learning_, pages 10842-10851. PMLR, 2020.
* [33] Amirata Ghorbani, Michael Kim, and James Zou. A distributional framework for data valuation. In _International Conference on Machine Learning_, pages 3535-3544. PMLR, 2020.
* [34] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Gurel, Bo Li, Ce Zhang, Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the shapley value. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1167-1176. PMLR, 2019.
* [35] Yuanyuan Chen, Boyang Li, Han Yu, Pengcheng Wu, and Chunyan Miao. Hydra: Hypergradient data relevance analysis for interpreting deep neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 7081-7089, 2021.
* [36] Yi Sui, Ga Wu, and Scott Sanner. Representer point selection via local jacobian expansion for post-hoc classifier explanation of deep neural networks and ensemble models. _Advances in neural information processing systems_, 34:23347-23358, 2021.
* [37] Jonathan Brophy, Zayd Hammoudeh, and Daniel Lowd. Adapting and evaluating influence-estimation methods for gradient-boosted decision trees. _arXiv preprint arXiv:2205.00359_, 2022.

* [38] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. _arXiv preprint arXiv:2303.14186_, 2023.
* [39] Andrew Silva, Rohit Chopra, and Matthew Gombolay. Cross-loss influence functions to explain deep network representations. In _International Conference on Artificial Intelligence and Statistics_, pages 1-17. PMLR, 2022.
* [40] Andrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 8179-8186, 2022.
* [41] Rui Zhang and Shihua Zhang. Rethinking influence functions of neural networks in the over-parameterized regime. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 9082-9090, 2022.
* [42] Samyadeep Basu, Philip Pope, and Soheil Feizi. Influence functions in deep learning are fragile. _arXiv preprint arXiv:2006.14651_, 2020.
* [43] Han Guo, Nazneen Fatema Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. Fastif: Scalable influence functions for efficient model interpretation and debugging. _arXiv preprint arXiv:2012.15781_, 2020.
* [44] Elnaz Barshan, Marc-Etienne Brunet, and Gintare Karolina Dziugaite. Relatif: Identifying explanatory training samples via relative influence. In _International Conference on Artificial Intelligence and Statistics_, pages 1899-1909. PMLR, 2020.
* [45] Samyadeep Basu, Xuchen You, and Soheil Feizi. On second-order group influence functions for black-box predictions. In _International Conference on Machine Learning_, pages 715-724. PMLR, 2020.
* [46] Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Lien Doan, and Fabrice Daniel. Tracinad: Measuring influence for anomaly detection. In _2022 International Joint Conference on Neural Networks (IJCNN)_, pages 1-6. IEEE, 2022.
* [47] Satoshi Hara, Atsushi Nitanda, and Takanori Maehara. Data cleansing for models trained with sgd. _Advances in Neural Information Processing Systems_, 32, 2019.
* [48] Maximilian Mozes, Tolga Bolukbasi, Ann Yuan, Frederick Liu, Nithum Thain, and Lucas Dixon. Gradient-based automated iterative recovery for parameter-efficient tuning. _arXiv preprint arXiv:2302.06598_, 2023.
* [49] Lloyd S. Shapley. _A value for n-person games_, page 31-40. Cambridge University Press, 1988. doi: 10.1017/CBO9780511528446.003.
* [50] Arun Suggala, Adarsh Prasad, and Pradeep K Ravikumar. Connecting optimization and regularization paths. _Advances in Neural Information Processing Systems_, 31, 2018.
* [51] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. _arXiv preprint arXiv:1711.00165_, 2017.
* [52] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.
* [53] Chih-Kuan Yeh, Ankur Taly, Mukund Sundararajan, Frederick Liu, and Pradeep Ravikumar. First is better than last for training data influence. _arXiv preprint arXiv:2202.11844_, 2022.
* [54] Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real-world neural representations generalize. In _International Conference on Machine Learning_, pages 23549-23588. PMLR, 2022.
* [55] Philip M Long. Properties of the after kernel. _arXiv preprint arXiv:2105.10585_, 2021.

* [56] Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based view of language model fine-tuning. _arXiv preprint arXiv:2210.05643_, 2022.
* [57] R Dennis Cook and Sanford Weisberg. _Residuals and influence in regression_. New York: Chapman and Hall, 1982.
* [58] Xiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling data artifacts through influence functions. _arXiv preprint arXiv:2005.06676_, 2020.
* [59] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)
* [60] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [61] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [62] Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization in linear time. _stat_, 1050:15, 2016.
* [63] James Mercer. Xvi. functions of positive and negative type, and their connection the theory of integral equations. _Philosophical transactions of the royal society of London. Series A, containing papers of a mathematical or physical character_, 209(441-458):415-446, 1909.