# FLSL: Feature-level Self-supervised Learning

Qing Su\({}^{1}\), Anton Netchaev\({}^{2}\), Hai Li\({}^{3}\), and Shihao Ji\({}^{1}\)

\({}^{1}\)Georgia State University, \({}^{2}\)U.S. Army ERDC, \({}^{3}\)Duke University

To whom correspondence should be addressed: qsu3@gsu.edu

###### Abstract

Current self-supervised learning (SSL) methods (_e.g._, SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying _mean-shift_ clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (_e.g._, a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a bi-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the _mean-shift_ and _k-_means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an encoding scheme amenable to _intra-view_ and _inter-view_ feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017. We conclude by presenting visualization and various ablation studies to better understand the success of FLSL. The source code is available at [https://github.com/ISL-CV/FLSL](https://github.com/ISL-CV/FLSL)]

## 1 Introduction

Following its success in natural language processing (NLP) [21, 22, 23], self-supervised learning (SSL) with transformer [58, 24] has emerged as a highly effective strategy and a popular model choice over the CNN-based counterparts in vision tasks. The remarkable performance achieved by SSL has been demonstrated by SimCLR [1], MOCOv3 [10], DINO [1], VICReg [3], SwAV [21], BYOL [22], and among others. Without relying on manual supervision, a successful paradigm of SSL promotes semantic representations conducive to the downstream tasks, _e.g._, classification, detection and segmentation. However, most existing SSL methods operate at the instance-level, where an encoder is trained to maximize the agreement of the representations of multiple augmented views of an image. Though demonstrating strong performance on the classification tasks [1][29], the instance-level SSL is inherently misaligned with the dense prediction tasks, such as object detection, where the lower level semantic information plays a bigger role than the instance-level semantic information. This leads to inferior transferability to those dense prediction tasks.

Recent attempts to bridge the semantic gap are mainly based on region [20], patch [63, 21], or pixel (_i.e._, dense feature) matching tasks [63, 73, 8] with optional instance-level objectives. However, learning of distinct representation for each image patch or region still mismatches the natural semantics within an image (referred to as local semantics), where features of the same semantics should be highly correlated other than being distinct. Semantics can range from features of high similarity, features of the same object, to more complex semantic structures. In light of this, methods such as SoCo [63], ORL [70] and DetCon [32] leverage the off-the-shelf algorithms, _e.g._, _selectivesearch_[53] and _Felzenszwalb-Hutenlocher_ algorithm [23] to impose the semantic constraint to the contrastive learning pipeline. Nonetheless, the inclusion of a non-trainable region proposal module in those methods restricts the model's ability to learn the distinct representations for those RoIs from the rest of an image. This ability is vital in representation learning for object detection.

Existing SSL methods targeting dense prediction primarily focus on the learning of globally semantic representations of image sub-regions, such as RoIs, patches, or pixels. However, these methods fall short with limited consideration for the alignment of those representations with local semantics. This observation leads us to ask the following question: Can we learn a representation that is both locally and globally semantic for a group of features (_e.g._, representing an object or stuff) in an end-to-end trainable SSL approach? To this end, we propose the _Feature Level Self-supervised Learning_ (FLSL). It leverages the _mean-shift_ clustering process inherent in transformer to extract modes as representations and incorporates _k-means_-based SSL approach to ensure that the extracted representations are semantically coherent both locally and globally. Figure. 1 illustrates an overview of FLSL with details to be discussed in Sec. 2.

**Contributions** This paper takes a step forward to bridge the gap between the current SSL methods and downstream dense prediction tasks. Our contributions are summarized as follows:

1. We demonstrate for the first time the connection between the attention mechanism and _mean-shift_ clustering, and reinterpret vision transformer from the perspective of _mean-shift_.
2. By employing transformer for joint embedding and feature clustering, we propose FLSL, an end-to-end trainable SSL method that promotes the representations of feature clusters to be semantic at two levels: (i) intra-view: within an image, and (ii) inter-view: over an entire dataset.
3. The derivation and construction of the FLSL objectves is rooted in _mean-shift_ and the non-empty _k-means_ clustering. Semantic representations on the first level are encouraged by optimizing the intra-cluster affinity with a self-attention layer, while the second-level semantic representations are fostered via non-empty _k-means_ clustering with positive samples retrieved through a cross-attention layer.
4. We validate the synergy between FLSL and ViT, and show significant improvement in transferability of the learned features to dense prediction tasks, including object detection and semantic segmentation. FLSL-pretrained ViT on ImageNet-1k (IN1k) demonstrates superior performance compared to the state-of-the-art ADCLR-IN1k [76] and MAE [40] pretrained counterparts. Moreover, it consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017.

## 2 Related work

**SSL for dense prediction** Recent attempts to bridge the gap between common SSL and dense prediction tasks focus primarily on sub-region matching tricks. For example, DenseCL [63] applies contrastive learning on pairs of patches with highest similarity. However, the patch-matching trick leads to distinct representations with low correlation among patches, which is not well-suited for the semantics of a natural image. On top of the instance-level objective, PixPro [73] and LC-loss [52] factor in agreement between positive pixel pairs which are assigned through thresholded-distance in

Figure 1: The bi-level clustering of FLSL. An object or stuff in an image is essentially a cluster of features. Hence, their representations can be extracted as cluster representatives, _e.g._, modes. In FLSL, we aim to make these representations both locally and globally semantic via a bi-level clustering process. On the first level, the _locally_ semantic representations are fostered by driving features of various concepts (book, person, plant, etc.) closer to their cluster modes \(\hat{z}_{\text{cls}}\) and far away from features of other concepts within an image(_intra-view_ clustering). On the second level, cluster modes serving as representations \(\hat{z}_{\text{cls}}\) as pushed closer to their positive samples \(\hat{z}_{\text{cls}}^{+}\) as \(X^{+}\), which is augmented via a random transformation \(t\sim\mathcal{T}\) (_inter-view clustering_). In such a way, those representations encode the same category information and become _globally_ semantic.

PixPro and position projection in LC-loss. ReSim [163] maximizes the agreement between sliding-window-pooled representations in the overlapped region of two augmented views. DetCo [69] further incorporates instance-patch level contrastive losses along with instance level and patch level losses. To learn representations at object level, SoCo [163] and ORL [70] employ _selective search_ to crop out RoIs. ORL further enables inter-object representation learning via BYOL [27] using top-ranked RoI pair. In contrast, SCRL [50] relaxes the semantic constraint using random crops within the intersection area of augmented views as RoIs. As discussed in Sec. [1] all of these methods focus on learning globally semantic representations for image sub-regions, and do not touch on local semantics that are necessary for dense prediction.

**Self-supervised vision transformer** In pioneering works, self-supervised training of transformer for vision tasks generally follow the paradigm of masked autoencoder in NLP [47]. For instance, iGPT [11] features reconstruction of masked pixels as one of its objectives. In general, SSL for ViT can be classified into two categories: the joint-embedding strategy epitomized by DINO [10] and MoCov3 [16], and the generative approaches represented by MAE [23]. The crossover of the two strategies is demonstrated by iBOT [173]. Regarding **dense prediction**, EsViT [33], designed for Swin Transformer [43], follows the region-matching strategy and applies the DINO loss to the probabilities of positive pairs determined by highest similarity. Instead of finding the best-matching patch, SelfPatch [73] considers the direct neighbors as its positive patches. However, with limited semantics contained in a fixed small area (_e.g._, 8-connected neighbors), the method still suffers from semantic misalignment. To address the sub-region mismatch issue of DINO, ADCLR [16] constructs query tokens from random sub-regions and treats them as extra class tokens in the DINO objective. This promotes region-aware semantic representations that better aligned with the local semantics, and leads to substantial improvement in dense prediction.

## 3 Intuition: the connection between mean-shift and attention

As discussed in Sec. [1] the misalignment between the current SSL methods and dense prediction tasks lies in the clustering bias at the semantic level. Instead of setting a fixed granularity, such as instance-level or fix-sized patch-level, a desired semantic representation scheme should be able to represent from a single patch to a cluster of patches or even an entire image. The representation space of an image can be considered as an empirical probability density function of features, and the modes (local maximum) therefore can be regarded as the representatives of clusters [11][17][13]. These modes can then be readily retrieved via clustering algorithms, particularly, non-parametric _kernel density estimation_ (KDE) methods [62] when the image composition (_e.g._, number of objects and stuffs) is unknown. One typical KDE-based method is the _mean-shift_ clustering [3]. In the following, we first give an overview of self-attention (SA) mechanism of transformer and the mean-shift algorithm. We then show that the mean-shift update rule conforms to the SA mechanism of transformer.

**Attention mechanism** First introduced to recurrent neural networks as a context extractor for machine translation [2], attention has premised major breakthroughs in NLP with the emergence of transformer that relies solely on the _scaled dot-product_ attention mechanism [53] given by

\[\operatorname{attention}\left(\mathbf{Q},\mathbf{K},\mathbf{V}\right)\!=\!\mathbf{V} \operatorname{softmax}\!\left(\mathbf{Q}^{\top}\mathbf{K}/\sqrt{D_{qk}}\right), \tag{1}\]

where \(\mathbf{Q}\), \(\mathbf{K}\) and \(\mathbf{V}\) denote query, key and value matrices which pack together sets of query, key and value vectors, respectively. \(D_{qk}\) denotes the dimension of query and key vectors, and \(\operatorname{softmax}\left(\mathbf{Z}\right)_{ij}\!\!=\!\exp(\mathbf{Z}_{ij})/\!\sum_{k }\exp(\mathbf{Z}_{ik})\). As a special case of attention, SA matches a sequence \(\mathbf{Z}\) with itself to extract the semantic dependencies among its components, _i.e._, \(\mathbf{Q}\!=\!\mathbf{W}_{Q}\mathbf{Z},\mathbf{K}\!=\!\mathbf{W}_{K}\mathbf{Z},\mathbf{V}\!=\!\mathbf{W}_{V} \mathbf{Z}\), where the projections \(\mathbf{W}_{\!-}\)'s are the parameter matrices.

**Mean-shift clustering and attention** Given \(N\) data points \(\{\mathbf{z}_{i}\}_{i=1}^{N}\subset\mathrm{I\!R}^{D}\), the kernel density estimate of \(p(\mathbf{z})\) with kernel \(K(t)\) can be defined as

\[p(\mathbf{z})=\sum_{i\,=\,1}^{N}p(\mathbf{z}_{i})p(\mathbf{z}|\mathbf{z}_{i})=\sum_{i\,=\,1}^{ N}\pi_{i}\frac{1}{T_{i}}K(d(\mathbf{z},\mathbf{z}_{i};\mathbf{\Sigma}_{i}))\,, \tag{2}\]

where \(p(\mathbf{z}_{i})=\pi_{i}\) is the mixing proportion of point \(\mathbf{z}_{i}\), _s.t.\(\sum_{i=1}^{N}\pi_{i}\!\!=\!\!1\)_, \(T_{i}\) denotes the normalization term dependent only on the covariance matrix \(\mathbf{\Sigma}_{i}\), _e.g._, for a Gaussian kernel \(T_{i}=|2\pi\mathbf{\Sigma}_{i}|^{1/2}\) and \(d(\mathbf{z},\mathbf{z}_{i};\mathbf{\Sigma}_{i})=\left(\mathbf{z}-\mathbf{z}_{i}\right)^{T}\mathbf{ \Sigma}_{i}^{-1}\left(\mathbf{z}-\mathbf{z}_{i}\right)\) is the _Mahalanobis_ distance. Finding the modes of \(p(\mathbf{z})\) is to seek stationary points by equating the gradient of \(p(\mathbf{z})\) to zero, \(\nicefrac{{\partial p(\mathbf{z})}}{{\partial\mathbf{z}}}=0\), which arrives at

\[\hat{\mathbf{z}}=\mathbf{f}\left(\mathbf{z}\right)=\sum_{i=1}^{N}p(\mathbf{z}_{i}|\mathbf{z})\mathbf{z} _{i},\quad\text{with}\;\;p(\mathbf{z}_{i}|\mathbf{z})=\frac{\pi_{i}\frac{1}{T_{i}}K^{ \prime}(d(\mathbf{z},\mathbf{z}_{i};\mathbf{\Sigma}_{i}))\mathbf{\Sigma}_{i}^{-1}}{\sum_{j=1}^ {N}\pi_{j}\frac{1}{T_{j}}K^{\prime}(d(\mathbf{z},\mathbf{z}_{j};\mathbf{\Sigma}_{j}))\mathbf{ \Sigma}_{j}^{-1}}, \tag{3}\]

where \(K^{\prime}\!=\!dK/dt\). The above fixed-point iterative scheme is the _mean-shift_ algorithm. Practically, on \(\ell_{2}\)-normalized vectors, for a homoscedastic _Gaussian_ kernel with constant mixing proportion and isotropic covariances (_e.g._, \(\pi_{i}=1/N\), \(1/\sigma^{2}=\tau\)), Eq. 3 further simplifies to

\[\hat{\mathbf{z}}=\mathrm{meashift}(\mathbf{z},\tau)=\sum_{i=1}^{N}\frac{\exp\left(\tau \mathbf{z}^{\top}\mathbf{z}_{i}\right)}{\sum_{j=1}^{N}\exp\left(\tau\mathbf{z}^{\top}\mathbf{ z}_{j}\right)}\mathbf{z}_{i}\Longrightarrow\hat{\mathbf{Z}}=\mathbf{Z}\;\mathrm{softmax} \left(\tau\mathbf{Z}^{\top}\mathbf{Z}\right), \tag{4}\]

which conforms to the attention function (Eq. 1) with identity projection matrices, _i.e._, \(\mathbf{W}_{Q}=\mathbf{W}_{K}=\mathbf{W}_{V}=\mathbf{I}\), and \(\tau=1/\sqrt{D_{qk}}\). Conversely, the conventional SA mechanism can be viewed as a generalized _mean-shift_:

\[\hat{\mathbf{Z}}\!=\!\mathrm{SA}(\mathbf{Z})\!=\!\mathbf{W}_{V}\;\mathbf{Z}\;\mathrm{softmax }\left(\nicefrac{{1}}{{\sqrt{D_{qk}}}}\mathbf{Z}^{\top}\left(\mathbf{W}_{Q}^{\top} \mathbf{W}_{K}\right)\mathbf{Z}\right), \tag{5}\]

with learnable distance measure \(\mathbf{Z}^{\top}(\mathbf{W}_{Q}^{\top}\mathbf{W}_{K})\mathbf{Z}\) and projection \(\mathbf{W}_{V}\). Unlike GMM and _k-means_, _mean-shift_ is capable of modeling clusters of complex non-convex shape with cluster number automatically determined by local scale (proscribed by covariance) [3]. Hence, it is well-aligned with the semantics of natural images.

**ViT from the perspective of mean-shift** In ViT [22], images are initially tokenized and then processed through a sequence of transformer layers. Each transformer layer is comprised of a skip-connected multi-head SA (MHSA) and a skip-connected MLP. MHSA can be constructed from Eq. 3 with \(m\) projections in parallel, _i.e._, \([\mathbf{W}_{Q}^{h},\mathbf{W}_{K}^{h},\mathbf{W}_{V}^{h}],h=1,\cdots,m\). The \(m\) returned modes are then concatenated along channel dimension and reprojected to a single return through

\[\hat{\mathbf{Z}}=\mathrm{MHSA}(\mathbf{Z})=\mathbf{W}_{O}\mathrm{concat}([[\hat{\mathbf{Z} }^{1}],\ldots,[\hat{\mathbf{Z}}^{m}]])+\mathbf{b}_{O}. \tag{6}\]

Note that the \(\ell_{2}\) normalization assumed in Eq. 3 is moderately relaxed via layer normalization (LN) to incorporate the extra degree of freedom in the vector magnitude. With skip connection and the one-step _mean-shift_ update described in Eqs. 3, 4 a transformer layer essentially finds the local centroid for each query \(\mathbf{z}\) and drives them closer to the re-projected centroids through \(\mathbf{z}=\mathbf{z}+\hat{\mathbf{z}}\), followed by an MLP processing step with skip connection. ViT iterates the process multiple times (_e.g._, 12 or 24 layers) to capture the contextual and semantic information of an image.

The clustering process above concords with one inductive bias of the attention mechanism represented by the _sparse variable creation_[23], _i.e._, an SA head learns a sparse function that only depends on a small subset of input coordinates. In the context of clustering, the subset of input corresponds to the modes of density \(p(\mathbf{z})\). As the high-level semantic information is typically spatially sparse (_e.g._, the representaion for a RoI in object detection, a single label for a region in segmentation, or a scene-graph, etc.), it is natural to leverage transformer for joint embedding and clustering to learn semantically meaningful representations.

## 4 Methodology

FLSL features a bi-level clustering process (Figure 1), which is formally described as follows.

Given a dataset \(\mathcal{X}\) (e.g., a set of images), FLSL learns an encoding scheme \(f_{\theta}\!:\mathcal{X}\!\rightarrow\!\mathcal{Z},\forall\mathbf{X}\in\mathcal{X},\mathbf{Z}=f_{\theta}(\mathbf{X})\). \(\mathbf{Z}\) can be formulated as \(\mathbf{Z}=\bigcup_{c}^{N_{c}}\tilde{\mathbf{z}}^{c}\), where \(\tilde{\mathbf{z}}^{c}\) is a subset of \(\mathbf{Z}\) forming a cluster, \(N_{c}\) is the number of clusters determined by a clustering scheme, _e.g._, _mean-shift_, and \(N_{c}\leq|\mathbf{Z}|\). FLSL aims to encourage the following properties:

(i) **Intra-view**: encodings corresponding to a semantic concept (as a cluster), \(\mathbf{z}\in\tilde{\mathbf{z}}^{c}\), are close to the cluster representative (_e.g._, mode) \(\hat{\mathbf{z}}^{c}\) and far away from the encodings of other clusters;

(ii) **Inter-view**: the cluster representatives \(\hat{\mathbf{z}}\)s of the positive regions in \(\mathbf{X}\)s over \(\mathcal{X}\) are pushed closer to each other.

The FLSL-extracted features should be well-aligned with dense prediction tasks, such as object detection, where the representation of an object or stuff (_i.e._, cluster of features) are desired to be (i) well-separated from others in an image (locally semantic), and (ii) close to its positive samples in the dataset (globally semantic). In this section, we present the objectives for both levels of clustering, which are then combined to form the final objective.

### Intra-view clustering with mean-shift

As discussed in Sec. 4.1 local semantics of an image can be captured by non-parametric clustering such as _mean-shift_. Hence, with _mean-shift_ update rule Eq. 4, it can be proved that the probability of \(\mathbf{z}_{j}\) given point \(\mathbf{z}_{i}\), \(p(\mathbf{z}_{j}|\mathbf{z}_{i})=[\text{softmax}(\tau\mathbf{z}_{i}^{\top}\mathbf{Z})]_{j}\), should satisfy:

\[p(\mathbf{z}_{j}|\mathbf{z}_{i})\!\geq\!\nicefrac{{1}}{{\left(\big{(}\sum_{k\in c_{i}} e^{(\mathbf{z}_{i}^{\top}\mathbf{z}_{k}-\mathbf{z}_{i}^{\top}\mathbf{z}_{j})^{\top}}\big{)}+(N-|c_{i} |)\mathbf{e}^{-\Delta_{ij}\tau}\right)}},\forall j\in c_{i} \tag{7}\]

where \(N=|\mathbf{Z}|\), \(c_{i}\) is the set of indices of points in the same cluster including \(\mathbf{z}_{i}\), and \(\Delta_{ij}\) is the degree of separability defined as \(\Delta_{ij}\!\!=\!\!\mathbf{z}_{i}^{\top}\mathbf{z}_{j}-\text{max}_{k\in[N]\setminus c _{i}}\mathbf{z}_{i}^{\top}\mathbf{z}_{k}\), such that larger \(\Delta_{c_{i}}=\sum_{j\in c_{i}}\Delta_{ij}\) indicates better separation. For locally semantic encodings, we desire the in-cluster points to be close to each other, or equivalently, to be close to its cluster representative, and stay far away from the out-cluster points, which indicates a large \(\Delta\) value. As \(\Delta\) becomes sufficiently large, the RHS of Eq. 4 can be approximated as \(1/\!\sum_{k\in c_{i}}\exp\left((\mathbf{z}_{i}^{\top}\mathbf{z}_{k}-\mathbf{z}_{i}^{\top} \mathbf{z}_{j})\tau\right)\), and for out-cluster points, the probability \(p(\mathbf{z}_{j\in c_{i}}|\mathbf{z}_{i})\) approaches to 0. This results in a semantics-aligned cluster representative via _mean-shift_ - a weighted sum of **only** in-cluster points. This can be realized by contrasting among points using attention map as soft cluster mask to drive the query point \(\mathbf{z}_{i}\) closer to the returned mode \(\hat{\mathbf{z}}_{i}\). It leads to the **intra-view** clustering objective:

\[\min_{f_{\theta}}\sum_{i=1}^{N}\lVert\mathbf{z}_{i}-\hat{\mathbf{z}}_{i}\rVert_{2}^{2}. \tag{8}\]

Proof of Eq. 4 and detailed explanation is provided in Appendix 4.1

### Inter-view clustering with k-means

To learn globally semantic representations, similar to the existing SSL methods, we formulate the problem as a variant of _k-means_ clustering. For \(\hat{\mathbf{z}}\)s extracted from an entire dataset, the _k-means_ objective with generalized non-empty cluster constraint [4] can be expressed as

\[\min_{\mathcal{M}}\frac{1}{N^{\prime}}\sum_{\mathbf{z}\in\hat{\mathbf{z}}}\sum_{k=1}^{ K}\delta_{kk(\hat{\mathbf{z}})}\lVert\hat{\mathbf{z}}\!-\!\mathbf{\mu}_{k(\hat{\mathbf{z}})} \rVert_{2}^{2}+D_{\text{KL}}\left(\bar{\mathbf{p}}\lVert\mathbf{\pi}\right), \tag{9}\]

where \(\mathcal{M}\) is a set of \(K\) centroids \(\{\mathbf{\mu}_{1},\cdots,\mathbf{\mu}_{K}\}\), \(\hat{\mathbf{\mathcal{Z}}}\) is a set of cluster representatives over the entire dataset, \(N^{\prime}=|\hat{\mathbf{\mathcal{Z}}}|\), \(k(\hat{\mathbf{z}})\!=\!\arg\min_{k}\lVert\mathbf{\mu}_{k}-\hat{\mathbf{z}}\rVert_{2}\), \(\delta_{ij}\) is the _Kronecker delta_, with \(\delta_{ij}\!=\!1\) iff \(i\!=\!j\), and 0 otherwise, \(|\bar{\mathbf{p}}|_{[i]}=\nicefrac{{1}}{{N^{\prime}}}\sum_{\hat{\mathbf{z}}}\delta_{ik (\hat{\mathbf{z}})}\), and \(\mathbf{\pi}\) is the prior, _e.g._, a vector of the preset proportion for each cluster. With positive pairs \((\hat{\mathbf{z}}^{+},\hat{\mathbf{z}})\) created via data augmentation, the objective can then be constructed as _k-means_ clustering with extra separation margin for \(\hat{\mathbf{z}}^{+}\):

\[\min_{\mathcal{M}}\frac{1}{N^{\prime}}\!\sum_{\mathbf{z}\in\hat{\mathbf{z}}}\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!expressed by

\[\min_{\mathcal{M}}\frac{1}{N^{\prime}}\sum_{\mathbf{z}\in\mathbf{z}}\mathrm{H}(\mathbf{p}(\hat {\mathbf{z}}^{+}),\mathbf{p}(\hat{\mathbf{z}}))+D_{\text{KL}}\left(\bar{\mathbf{p}}\|\mathbf{\pi} \right), \tag{11}\]

where \(\mathbf{p}(\mathbf{x})\!=\!\text{softmax}\left(\tau^{\prime}\mathbf{W}_{C}^{\top}\mathbf{x}\right)\), \(\tau^{\prime}\ll 1\) with \(\mathbf{W}_{C}\) defined as a matrix of \(K\) orderly concatenated centroids, and \(\mathrm{H}(x,y)\!=\!-x\log y\) (cf. Appendix.2).

**Positive sample retrieval** Unlike the common instance-level SSL, the positive samples in FLSL are amorphous clusters of features, \((\hat{\mathbf{z}}^{+},\hat{\mathbf{z}})\), corresponding to the same semantic concept in two views. In contrast to previous works assigning the best-matching patch [38, 63] or thresholded vicinity [73], we leverage the cluster assignment mechanism inherent in _mean-shift_, where a query \(\mathbf{z}\) is automatically assigned to a cluster represented by the return \(\hat{\mathbf{z}}\). For query from another view, the _mean-shift_ naturally manifests as a cross-attention (CA),

\[\hat{\mathbf{z}}^{+}=\mathbf{Z}^{+}\operatorname{softmax}\left(\tau\mathbf{z}^{\top}\mathbf{Z} ^{+}\right), \tag{12}\]

With representations semantically coherent on local and global levels, the returned \(\hat{\mathbf{z}}^{+}\) from the augmented view \(\mathbf{Z}^{+}\) by query \(\mathbf{z}\) should agree with the returned \(\hat{\mathbf{z}}\) from the original view. To help establish this semantic constraint, representations at the projected positions from the augmented view can be used as positive samples at the early stage of training. This process can be viewed as data retrieval in dense associative memory recognized in [43].

### FLSL Objective

By combining the objectives from the two clustering levels, we arrive at the objective of FLSL:

\[\min\frac{1}{N^{\prime}}\sum_{\mathbf{Z}\in\mathbf{Z}}\sum_{\mathbf{z}\in\mathbf{ Z}}\upsilon\|\mathbf{z}-\hat{\mathbf{z}}\|_{2}^{2}\!+\!\eta\!\sum_{\mathbf{z}\in\mathbf{Z}} \mathrm{H}(\mathbf{p}(\hat{\mathbf{z}}^{+}),\mathbf{p}(\hat{\mathbf{z}}))+\gamma D_{\mathrm{ KL}}\left(\bar{\mathbf{p}}\|\mathbf{\pi}\right), \tag{13}\] \[\operatorname{with}\ \hat{\mathbf{z}}=\mathrm{SA}(\mathbf{z},\mathbf{Z},\mathbf{Z}), \ \hat{\mathbf{z}}^{+}=\mathrm{CA}(\mathbf{z},\mathbf{Z}^{+},\mathbf{Z}^{+}),\]

where \(\upsilon\), \(\eta\) and \(\gamma\) are the hyperparameters controlling the importance of each term, and the SA and CA above are non-parametric.

Figure [2] illustrates the FLSL framework. We follow the common joint-embedding strategy of SSL, except that we simultaneously maximize the agreement between positive cluster representatives \((\mathbf{p}(\hat{\mathbf{z}}^{+}),\mathbf{p}(\hat{\mathbf{z}}))\) and the agreement between an in-cluster point and its cluster representative \((\mathbf{z},\hat{\mathbf{z}})\). The KL-divergence term in Eq. 13 serves as a volume maximization regularizer. Experiments show that the FLSL objective effectively promote locally and globally semantic representations, resulting in significantly improved transferability of learned features to object detection and segmentation. Note that FLSL does not involve a class token in its objective (Eq. 13).

## 5 Experiments

In this section, we evaluate the performance of FLSL by conducting extensive experiments. Specifically, we compare FLSL to existing SSL approaches on multiple dense prediction benchmarks: (i) MS-COCO [22] object detection and instance segmentation, (ii) UAVDT [23] object detection from UAV platforms, and (iii) DAVIS video instance segmentation [40]. Moreover, we investigate the properties of FLSL features in terms of semantic alignment and feature separability in the embedding space. Detailed experimental setups are provided in the respective subsections and supplementary materials. All our experiments are performed on Nvidia RTX A6000.

**Implementation details** The implementation of ViT in our experiments mostly follows DeiT [54] excluding the [class] token. The configuration of the ViT variants utilized in this paper is summarized in Appendix.3. The coefficients of Eq. 13 in our experiments are \(\upsilon=.03\), \(\eta=1\) and \(\gamma=5\) unless stated otherwise. We assume a uniform prior, _i.e._, \(\pi_{k}=1/K,\ \forall k\). Models are pretrained on ImageNet-1k [52] dataset using AdamW optimizer [43] with a batch size of 512. We follow the data augmentation from BYOL [27] (_e.g._, color jittering of brightness, contrast, saturation and hue, Gaussian blur and solarization) with preceding random crops and resizing (to \(224\times 224\)) and make them asymmetric. Computation among dense features can be expensive. Therefore, we apply a grid random sampling to the queries. All ViT models are pretrained for 300 epochs as in most baselines for a fair comparison. Pseudo-code, training details, and settings of augmentation pipeline are provided in Appendix.

**Baselines** We compare FLSL with various existing SSL approaches that are based on the ResNet [11] and ViT [12] architectures: (a) self-supervised ResNet: MoCo-v2 [13], DetCo [63], DenseCL [63], BYOL [11], and SCRL [50]; and (b) self-supervised ViT: MoCo-v3 [16], MoBY [11], DINO [11], MAE [28], SelfPatch [73], and ADCLR [76].

**Protocol for hyperparameter tuning** Standard instance-level SSL evaluation protocols typically utilize one of the two approaches: employing a \(k\)-NN classifier or training a linear classifier on fixed features. Since FLSL learns dense semantic representations rather than a single instance-level representation, both standard evaluation protocols are not suitable for evaluating FLSL in training. Moreover, fine-tuning on a downstream dense prediction tasks can be computationally expensive due to complex prediction heads, and may introduce task-specific biases during hyperparameter tuning. Therefore, we design a bbox-aligned \(k\)-NN classifier modified from [67] to evaluate the feature quality directly without additional network tuning. Here is an overview of the method. Features of the training data are first extracted with a fixed model. These features are then aligned with their corresponding bounding boxes provided by ILSVRC [11]. For each image, a certain number of representative features \(\hat{\mathbf{z}}\)s (_e.g._, 9) are selected by a partition criterion and stored in memory. The \(k\)-NN classifier matches each selected features to its \(k\)-nearest stored features, which collectively vote for its label. A feature is considered successfully classified if any of the representative features match its class. This protocol is employed for hyperparameter tuning and ablation study of the FLSL pipeline. Appendix [11] provides further details on the choice of \(k\), implementation specifics and evaluation results.

### MS-COCO Object Detection & Segmentation

We adopt Mask R-CNN detection framework by incorporating three variants of ViT: (i) ViT-S/16 with FPN [11], (ii) ViT-S/8 with FPN, and (iii) ViT-B/16 with simple feature pyramid (ViTDet) [11]. Models of (i) and (ii) are fine-tuned following the multi-scale training [16] under the standard 1\(\times\) schedule for a fair comparison. For the model of (iii), we follow the training recipe of [11] and fine-tune the model for 100 epochs.

**Results.** Table 1 reports the detection and segmentation performance of ViT-S/16 and ViT-S/8 with Mask R-CNN [30] on COCO. Specifically, FLSL with ViT-S/16 outperforms ADCLR [76] by +0.6% and +1.1%, and substantially outperforms DINO+SelfPatch [73] by +2.8% and +2.4% on detection (AP\({}^{\text{bbox}}\)) and segmentation (AP\({}^{\text{mk}}\)), respectively. Both baseline methods feature patch-level contrastive learning. Unlike SelfPatch contrasting between patches within the adjacent neighborhood and ADCLR contrasting via learned queries of random crops, FLSL contrasts the representatives (modes) of feature clusters, which aligns closer with the downstream tasks and thus leads to superior performance. Notably, FLSL with ViT-S/8 further improves the performance by a large margin of +4.4% in AP\({}^{\text{bbox}}\) and +3.6% AP\({}^{\text{mk}}\) over SelfPatch. Table 2 summarizes the results of ViTDet. FLSL shows large performance gains over the DINO baseline by +4.2% AP\({}^{\text{bbox}}\) and +3.3% AP\({}^{\text{mk}}\). FLSL also outperforms the SOTA generative approach, MAE, by +1.7% and +1.4% in the two tasks, respectively.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline Pretrain & Backbone & Epoch & \#Params & AP\({}^{\text{bbox}}_{\text{all}}\) & AP\({}^{\text{bbox}}_{\text{all}}\) & AP\({}^{\text{bbox}}_{\text{all}}\) & AP\({}^{\text{mk}}\) & AP\({}^{\text{mk}}_{\text{all}}\) & AP\({}^{\text{mk}}_{\text{all}}\) \\ \hline MoCo-v2 & RN50 & 200 & 23M & 38.9 & 59.2 & 42.4 & 35.5 & 56.2 & 37.8 \\ DetCo & RN50 & 200 & 23M & 40.1 & 61.0 & 43.9 & 36.4 & 58.0 & 38.9 \\ DenseCL & RN50 & 200 & 23M & 40.3 & 59.9 & 44.3 & 36.4 & 57.0 & 39.2 \\ BYOL & RN50 & 1000 & 23M & 40.4 & 61.6 & 44.1 & 37.2 & 58.8 & 39.8 \\ SCRL & RN50 & 1000 & 23M & 41.3 & 62.4 & 45.0 & 37.7 & 59.6 & 40.7 \\ \hline MCo-v3 & ViT-S/16 & 300 & 21M & 39.8 & 62.6 & 43.1 & 37.1 & 59.6 & 39.2 \\ MbBY & ViT-S/16 & 300 & 21M & 41.1 & 63.7 & 44.8 & 37.6 & 60.3 & 39.8 \\ DINO & ViT-S/16 & 300 & 21M & 40.8 & 63.4 & 44.2 & 37.3 & 59.9 & 39.5 \\ DINO+SelfPatch & ViT-S/16 & 200 & 21M & 42.1 & 64.9 & 46.1 & 38.5 & 61.3 & 40.8 \\ ADCLR & ViT-S/16 & 300 & 21M & 44.3 & 65.4 & 47.6 & 39.7 & 62.1 & 41.5 \\ FLSL & ViT-S/16 & 300 & 21M & 44.9 & 66.1 & 48.1 & 40.8 & 64.7 & 44.2 \\ FLSL & ViT-S/8 & 300 & 21M & 46.5 & 69.0 & 51.3 & 42.1 & 65.3 & 45.0 \\ \hline \end{tabular}
\end{table}
Table 1: Mask R-CNN on COCO

\begin{table}
\begin{tabular}{l c c c c} \hline Pretrain & AP\({}^{\text{bbox}}_{\text{all}}\) & AP\({}^{\text{bbox}}_{\text{all}}\) & AP\({}^{\text{bbox}}_{\text{all}}\) & AP\({}^{\text{bbox}}_{\text{all}}\) & AP\({}^{\text{mk}}\) \\ \hline None & 48.1 & - & - & - & 42.6 \\ IN-1k Spw. & 47.6 & - & - & 42.4 \\ IN-2k Spw. & 47.8 & - & -

### Small Object Detection: UAVDT

To assess the transferability of FLSL beyond the datasets of common images like COCO, we further investigate its performance on a UAV benchmark, UAVDT [23], which exhibits significant domain shifts from common images (_i.e._, images captured by ground-level cameras). We utilize Faster R-CNN framework [23] with the same ViT variants used in the COCO experiments and follow the training settings outlined in ClusDet [7]. All ViT-backboned models are trained with \(1\times\) schedule.

**Result** Table 2 presents the performance of ViT-S/16, ViT-S/8, and ViT-B/16 with Faster R-CNN for detection tasks on UAVDT under different pretrain schemes. We utilize the official evaluation method in [23], which calculates the class-agnostic VOC AP exclusive of the predictions that falls in the ignored areas. FLSL consistently outperforms DINO (a typical instance-level SSL for ViT) across all three ViT variants by a significant margin. With smaller objects and an imbalanced foreground-background ratio, the significance of local semantics becomes evident. Models require local context to discover small objects and make accurate predictions rather than relying solely on the global semantics of the entire image. This situation aligns well with the strengths of FLSL.

### DAVIS Segmentation

To further assess the quality of frozen features learned by FLSL, we evaluate FLSL-pretrained ViT models on DAVIS2017 [24], following the evaluation protocol in [25] that requires fixed representations with no extra training.

**Results** Table 4 shows that FLSL consistently outperforms DINO across all ViT variants in our experiments. The protocol evaluates the quality of learned dense features via segmenting scenes with \(k\)-nearest neighbors (\(k=5\)) within a fixed window (\(12\times 12\)) between consecutive frames. This requires dense features to be locally semantic, _i.e._, features corresponding to the same semantics should be more correlated. Therefore, the improved performance confirms that FLSL encourages model to extract locally semantic representations.

### Alignment with Image Semantics

To qualitatively show that FLSL is better aligned with the semantic layout of an image than the common SSL methods, Figure 4(a) compares the self-attention probing maps for features learned via FLSL and DINO. Features from the last layer are used for evaluation. The visualizations are obtained with \(224^{2}\) images. Positions of the query tokens are marked out in green circle in the top row. As shown in the middle and bottom rows of the figure, DINO promotes more correlated attention (_i.e._, less separation between tokens of query-related area and that of the rest image), while FLSL encourages

Figure 4: (a) visualization of attention probing by query patches (marked out in green circle in the top row) from the last layer of ViT-S/16 pretrained with FLSL and with DINO. FLSL encourages the model to learn semantic correlations among patches; (b) visualization of separability of the dense representations throughout the transformer (ViT-S/16).

Figure 3: Visualization of the maps of the _aggregated attention score_ (ASS) from different layers of ViT-S/16. \(l=0\) denotes the projection layer. As layer goes deeper, the map becomes more partitioned with brightness aligned with the area of the underlying semantic region, _e.g._, objects or stuff.

attention to the regions of high semantic relevance with the query tokens and results in clearer maps consistent with the underlying objects/stuff.

### Feature Distribution and Separability

We demonstrate the qualitative results by visualizing the Aggregated Similarity Score (ASS) and the feature distribution in the embedding space using t-sne [11] in Figure 5 and Figure 5, respectively. To generate the map of ASS, we sum up the cosine-similarity maps of all tokens, normalize the resulting map with its maximum score and visualize it as a thermal image, _i.e._, the brighter the pixel, the higher the score. For a semantically well-separated image, each patch only attends to the patches of its own semantic region, _e.g._, a patch of an object has high similarity scores only with the patches of that object and low scores with the rest. This results in an image with partitions of different brightness proportional to the area of that region, _i.e._, ideally the larger the size of an object/stuff, the brighter the color. As shown in Figure 5 as the layer goes deeper, the brightness partition of the ASS is more consistent with the underlying objects and stuff in the images (_e.g._, person, vehicles, horse, switches, wall, and ground, etc.), which indicates the desired separation of the learned features. This is also reflected in the t-sne visualization of the embeddings in Figure 5, where the representations become more clustered and separated as the attention layer goes deeper.

### Ablation Study

Due to limited space, we present two major ablation studies in this section to help understand the effectiveness of FLSL. The model considered for this entire study is ViT-S trained with 100 epochs. We refer the reader to Appendix [1] for the complete work.

**Impact of coefficients in the FLSL objective** The FLSL objective (Eq. 13) contains three components: (1) similarity between \(\ell_{2}\)-normalized \(\mathbf{z}\) (features) and \(\hat{\mathbf{z}}\) (modes), (2) cross-entropy of the probabilities of an augmented pair \(H(p(\hat{\mathbf{z}}^{+})\), \(p(\hat{\mathbf{z}}))\), and (3) the volume maximization regularizor \(D_{\text{KL}}\left(\mathbf{\bar{p}}\middle|\mathbf{\pi}\right)\). It is computationally expensive to optimally determine the values of more than two coefficients by performing grid search, especially when the ratios among them are large. We tackle this problem by first fixing \(\eta=1\) and setting \(\gamma=1\) along with Sinkhorn normalization [12] to perform a grid search on the value of \(\upsilon\) with the empirical base condition \(\upsilon\leq 1\) and \(\gamma\geq 1\) [12]. With the fixed \(\upsilon\), we then perform another grid search on \(\gamma\) without Sinkhorn normalization. We implement Sinkhorn normalization as the softmax operation along the batch dimension. Table 5 summerizes the score of bbox-aligned \(k\)-NN evaluation using different coefficient settings.

**Impact of number of centroids \(K\)** FLSL is formulated as an explicit clustering problem, with the output dimension of the last fully-connected layer equal to the number of centroids \(K\). Compared to its instance-level counterpart DINO [13], FLSL enjoys a smaller output dimension (shown in Table 6). This is because images have higher feature variance compared to feature clusters. For example, an image in ImageNet may contain diverse content from different categories, requiring a large number of centroids to cover the distribution. In contrast, a semantic cluster contains highly correlated features, such as similar textures or objects from the same category, thus requiring fewer centroids. Experimentally, we find that a large number of centroids benefits performance, but is detrimental and costly when being too large. We pick \(K=4,096\) for all our experiments as it strikes a good balance between performance and cost-effectiveness.

More experiment results on semantic segmentation and ablations including the impact of batch size and random pooling window size are relegated to Appendix.

## 6 Conclusions

This paper proposes FLSL, a feature-level self-supervised learning method that bridges the gap between the current SSL methods and downstream dense prediction tasks. We demonstrate for the first time the underlying _mean-shift_ clustering process of ViT, which aligns well with natural image semantics. Facilitated by ViT for joint embedding and feature clustering, FLSL performs a bi-level clustering: (i) intra-view clustering to extract the representatives for clusters of features within an image, and (ii) inter-view clustering to encourage the representatives to be globally semantic over

\begin{table}
\begin{tabular}{c c c|c c c c c c c c} \hline \hline  & \(\eta\) & \(\gamma\) & \(\upsilon=0.0\) & \(\upsilon=0.1\) & \(\upsilon=0.2\) & \(\upsilon=0.3\) & \(\sim\) & \(\upsilon=1\) \\ \(\gamma\) & 1.0 & 1.0 & 0.1 & 68.7 & 70.7 & 71.2 & \(\sim\) & 65.1 & \(k\)-NN top-1 & 68.1 & 72.1 & 72.4 & 72.5 & 72.1 \\ \(\times\) & 1.0 & 1.0 & 0.1 & - & - & - & 72.4 & - & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 5: Impact of coefficients in the FLSL objective.

the entire dataset. FLSL achieves a significant improvement over the SOTAs in the dense prediction tasks, including object detection and instance segmentation.

**Limitations and broader impacts** FLSL does not have any significant limitations other than the method is more complex (due to its bi-level clustering) than other SSL methods, and it currently only fits for ViT-based models on dense prediction tasks. Exploring ways to extend FLSL for tasks that necessitate a global representation while retaining its existing properties could be a potential future work. As far as we can foresee, there is no negative societal impact.

## 7 Acknowledgment

This research was sponsored by the Army Research Laboratory under Cooperative Agreement #W911NF-22-2-0025. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.

## References

* [1] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Mike Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. In _European Conference on Computer Vision_, pages 456-473. Springer, 2022.
* [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. _arXiv preprint arXiv:1409.0473_, 2014.
* [3] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. _arXiv preprint arXiv:2105.04906_, 2021.
* [4] Paul S Bradley, Kristin P Bennett, and Ayhan Demiriz. Constrained k-means clustering. _Microsoft Research, Redmond_, 20(0):0, 2000.
* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _European conference on computer vision_, pages 213-229. Springer, 2020.
* [7] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In _ECCV_, 2018.
* [8] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image features on non-curated data. In _ICCV_, 2019.
* [9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. _Advances in Neural Information Processing Systems_, 33:9912-9924, 2020.
* [10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9650-9660, 2021.
* [11] Miguel Carreira-Perpinan. Reconstruction of sequential data with probabilistic models and continuity constraints. _Advances in neural information processing systems_, 12, 1999.
* [12] Frederic Chazal, Leonidas J Guibas, Steve Y Oudot, and Primoz Skraba. Persistence-based clustering in riemannian manifolds. _Journal of the ACM (JACM)_, 60(6):1-38, 2013.

* [13] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In _International conference on machine learning_, pages 1691-1703. PMLR, 2020.
* [14] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [15] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. _arXiv preprint arXiv:2003.04297_, 2020.
* [16] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9640-9649, 2021.
* [17] Yizong Cheng. Mean shift, mode seeking, and clustering. _IEEE transactions on pattern analysis and machine intelligence_, 17(8):790-799, 1995.
* [18] Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature space analysis. _IEEE Transactions on pattern analysis and machine intelligence_, 24(5):603-619, 2002.
* [19] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in neural information processing systems_, 26, 2013.
* [20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [21] Jian Ding, Enze Xie, Hang Xu, Chenhan Jiang, Zhenguo Li, Ping Luo, and Gui-Song Xia. Deeply unsupervised patch re-identification for pre-training object detectors. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [23] Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong Li, Weigang Zhang, Qingming Huang, and Qi Tian. The unmanned aerial vehicle benchmark: Object detection and tracking. In _Proceedings of the European conference on computer vision (ECCV)_, pages 370-386, 2018.
* [24] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR, 2022.
* [25] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-based image segmentation. _International journal of computer vision_, 59:167-181, 2004.
* [26] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. _arXiv preprint arXiv:1706.02677_, 2017.
* [27] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.
* [28] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16000-16009, 2022.
* [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738, 2020.

* [30] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _Proceedings of the IEEE international conference on computer vision_, pages 2961-2969, 2017.
* [31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [32] Olivier J Henaff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron Van den Oord, Oriol Vinyals, and Joao Carreira. Efficient visual pretraining with contrastive detection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10086-10096, 2021.
* [33] Christian Hennig, Marina Meila, Fionn Murtagh, and Roberto Rocci. _Handbook of cluster analysis_. CRC Press, 2015.
* [34] Jyh-Jing Hwang, Stella X. Yu, Jianbo Shi, Maxwell D Collins, Tien-Ju Yang, Xiao Zhang, and Liang-Chieh Chen. Segsort: Segmentation by discriminative sorting of segments. In _ICCV_, 2019.
* [35] Ashraful Islam, Benjamin Lundell, Harpreet Sawhney, Sudipta N Sinha, Peter Morales, and Richard J Radke. Self-supervised learning with local contrastive loss for detection and semantic segmentation. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 5624-5633, 2023.
* [36] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as a contrastive random walk. _Advances in neural information processing systems_, 33:19545-19560, 2020.
* [37] Dmitry Krotov and John Hopfield. Large associative memory problem in neurobiology and machine learning. _arXiv preprint arXiv:2008.06996_, 2020.
* [38] Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng Gao. Efficient self-supervised vision transformers for representation learning. _arXiv preprint arXiv:2106.09785_, 2021.
* [39] Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. _arXiv preprint arXiv:2005.04966_, 2020.
* [40] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. _arXiv preprint arXiv:2203.16527_, 2022.
* [41] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2117-2125, 2017.
* [42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European conference on computer vision_, pages 740-755. Springer, 2014.
* [43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [44] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_, 2016.
* [45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [46] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. _arXiv preprint arXiv:1704.00675_, 2017.
* [47] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. _OpenAI_, 2018.

* Ramsauer et al. [2020] Hubert Ramsauer, Bernhard Schafl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, et al. Hopfield networks is all you need. _arXiv preprint arXiv:2008.02217_, 2020.
* Ren et al. [2015] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. _Advances in neural information processing systems_, 28, 2015.
* Roh et al. [2021] Byungseok Roh, Wuhyun Shin, Ildoo Kim, and Sungwoong Kim. Spatially consistent representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1144-1153, 2021.
* Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _International Journal of Computer Vision (IJCV)_, 115(3):211-252, 2015.
* Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.
* Simeoni et al. [2021] Oriane Simeoni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Perez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and no labels. _arXiv preprint arXiv:2109.14279_, 2021.
* Touvron et al. [2021] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _International Conference on Machine Learning_, pages 10347-10357. PMLR, 2021.
* Uijlings et al. [2013] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders. Selective search for object recognition. _International journal of computer vision_, 104(2):154-171, 2013.
* Van Der Maaten [2009] Laurens Van Der Maaten. Learning a parametric embedding by preserving local structure. In _Artificial intelligence and statistics_, pages 384-391. PMLR, 2009.
* Van der Maaten and Hinton [2008] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Vo et al. [2019] Huy V Vo, Francis Bach, Minsu Cho, Kai Han, Yann LeCun, Patrick Perez, and Jean Ponce. Unsupervised image matching and object discovery as optimization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8287-8296, 2019.
* Vo et al. [2020] Huy V Vo, Patrick Perez, and Jean Ponce. Toward unsupervised, multi-object discovery in large-scale image collections. In _European Conference on Computer Vision_, pages 779-795. Springer, 2020.
* Vo et al. [2021] Van Huy Vo, Elena Sizikova, Cordelia Schmid, Patrick Perez, and Jean Ponce. Large-scale unsupervised object discovery. _Advances in Neural Information Processing Systems_, 34:16764-16778, 2021.
* Wand and Jones [1994] Matt P Wand and M Chris Jones. _Kernel smoothing_. CRC press, 1994.
* Wang et al. [2021] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3024-3033, 2021.
* Wang et al. [2021] Xudong Wang, Ziwei Liu, and Stella X Yu. Unsupervised feature learning by cross-level instance-group discrimination. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12586-12595, 2021.

* [65] Fangyun Wei, Yue Gao, Zhirong Wu, Han Hu, and Stephen Lin. Aligning pretraining for detection via object-level contrastive learning. _Advances in Neural Information Processing Systems_, 34:22682-22694, 2021.
* [66] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2). 2019.
* [67] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3733-3742, 2018.
* [68] Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer, and Trevor Darrell. Region similarity representation learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10539-10548, 2021.
* [69] Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Peize Sun, Zhenguo Li, and Ping Luo. Detco: Unsupervised contrastive learning for object detection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8392-8401, 2021.
* [70] Jiahao Xie, Xiaohang Zhan, Ziwei Liu, Yew Soon Ong, and Chen Change Loy. Unsupervised object-level representation learning from scene images. _Advances in Neural Information Processing Systems_, 34:28864-28876, 2021.
* [71] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In _International conference on machine learning_, pages 478-487. PMLR, 2016.
* [72] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. Self-supervised learning with swin transformers. _arXiv preprint arXiv:2105.04553_, 2021.
* [73] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16684-16693, 2021.
* [74] Fan Yang, Heng Fan, Peng Chu, Erik Blasch, and Haibin Ling. Clustered object detection in aerial images. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8311-8320, 2019.
* [75] Sukmin Yun, Hankook Lee, Jaehyung Kim, and Jinwoo Shin. Patch-level representation learning for self-supervised vision transformers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8354-8363, 2022.
* [76] Shaofeng Zhang, Feng Zhu, Rui Zhao, and Junchi Yan. Patch-level contrasting without patch cor-respondence for accurate and dense con-trastive representation learning. 2023.
* [77] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Animashree Anandkumar, Jiashi Feng, and Jose M Alvarez. Understanding the robustness in vision transformers. In _International Conference on Machine Learning_, pages 27378-27394. PMLR, 2022.
* [78] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. _arXiv preprint arXiv:2111.07832_, 2021.
* [79] Chengxu Zhuang, Alex Lin Zhai, Daniel Yamins,, et al. Local aggregation for unsupervised learning of visual embeddings. In _ICCV_, 2019.