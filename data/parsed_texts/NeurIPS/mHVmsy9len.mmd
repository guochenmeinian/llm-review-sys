# Bounds for the smallest eigenvalue of the NTK for arbitrary spherical data of arbitrary dimension

Kedar Karhadkar\({}^{1}\), Michael Murray\({}^{1}\), Guido Montufar\({}^{1,2,3}\)

\({}^{1}\)Department of Mathematics, UCLA

\({}^{2}\)Department of Statistics & Data Science, UCLA

\({}^{3}\)Max Planck Institute MiS

###### Abstract

Bounds on the smallest eigenvalue of the neural tangent kernel (NTK) are a key ingredient in the analysis of neural network optimization and memorization. However, existing results require distributional assumptions on the data and are limited to a high-dimensional setting, where the input dimension \(d_{0}\) scales at least logarithmically in the number of samples \(n\). In this work we remove both of these requirements and instead provide bounds in terms of a measure of distance between data points: notably these bounds hold with high probability even when \(d_{0}\) is held constant versus \(n\). We prove our results through a novel application of the hemisphere transform.

## 1 Introduction

A popular approach for studying the optimization dynamics of neural networks is analyzing the neural tangent kernel (NTK), which corresponds to the Gram matrix obtained from the Jacobian of the network parametrization map (Jacot et al., 2018). When the network parameters are adjusted by gradient descent, the network function follows a kernel gradient descent in function space with respect to the NTK. By bounding the smallest eigenvalue of the NTK away from zero it is possible to obtain global convergence guarantees for gradient descent parameter optimization (Du et al., 2019; Oymak and Soltanolkotabi, 2020) as well as results on generalization (Arora et al., 2019; Montanari and Zhong, 2022) and data memorization capacity (Montanari and Zhong, 2022; Nguyen et al., 2021; Bombari et al., 2022). These key advances highlight the importance of deriving tight, quantitative bounds for the smallest eigenvalue of the NTK at initialization.

While initial breakthroughs on the convergence of gradient optimization in neural networks (Li and Liang, 2018; Du et al., 2019; Allen-Zhu et al., 2019) required unrealistic conditions on the width of the layers, subsequent and substantive efforts have reduced the level of overparametrization required to ensure that the NTK is well conditioned at initialization (Zou and Gu, 2019; Oymak and Soltanolkotabi, 2020). In particular, Nguyen (2021); Nguyen et al. (2021); Banerjee et al. (2023) showed that layer width scaling linearly in the number of training samples \(n\) suffices to bound the smallest eigenvalue and Montanari and Zhong (2022); Bombari et al. (2022) obtained results for networks with sub-linear layer width and the minimum possible number of parameters \(\tilde{\Omega}(n)\) up to logarithmic factors. However, and as discussed in Section 2, the bounds provided in prior works require that the data is drawn from a distribution satisfying a Lipschitz concentration property, and only hold with high probability if the input dimension \(d_{0}\) scales as \(\sqrt{n}\)(Bombari et al., 2022) or \(\text{polylog}(n)\)(Nguyen et al., 2021). These existing results therefore require that the dimension of the data grows unbounded as the number of training samples \(n\) increases and as such there is a gap in our understanding of cases where the data is sampled from a fixed, or lower-dimensional space.

In this work we present new lower and upper bounds on the smallest eigenvalue of a randomly initialized, fully connected ReLU network: compared with prior work, our results hold for arbitrarydata on a sphere of arbitrary dimension. Our techniques are novel and rely on the hemisphere transform as well as the addition formula for spherical harmonics.

We study neural networks denoted as functions \(f:\mathbb{R}^{d_{0}}\times\mathcal{P}\to\mathbb{R}\), where \(\mathcal{P}\) is an inner product space. To be clear, \(f(\bm{x};\bm{\theta})\) denotes the output of the network for a given input \(\bm{x}\in\mathbb{R}^{d_{0}}\) and parameter choice \(\bm{\theta}\in\mathcal{P}\). For brevity we occasionally write \(f(\bm{x})\) in place of \(f(\bm{x};\bm{\theta})\) if the context is clear. We use \(n\) to denote the size of the training sample, \(d_{0}\) the dimension of the input features, \(L\) the network depth, \(d_{l}\) the width of the \(l\)th layer and \(\sigma:\mathbb{R}\to\mathbb{R}\) the ReLU activation function. Given \(n\) input data points \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{R}^{d_{0}}\) we write \(\bm{X}=[\bm{x}_{1},\cdots,\bm{x}_{n}]\in\mathbb{R}^{d_{0}\times n}\) and define \(F:\mathcal{P}\to\mathbb{R}^{n}\) to be the evaluation of the network on these \(n\) data points as a function of the parameter \(\bm{\theta}\),

\[F(\bm{\theta})=[f(\bm{x}_{1};\bm{\theta}),\cdots,f(\bm{x}_{n};\bm{\theta})]^{T}.\]

We define the neural tangent kernel (NTK) of \(F\) as

\[\bm{K}(\bm{\theta})=(\nabla_{\bm{\theta}}F(\bm{\theta}))^{*}(\nabla_{\bm{ \theta}}F(\bm{\theta}))\in\mathbb{R}^{n\times n},\] (1)

where the gradient \(\nabla\) and adjoint \(*\) are taken with respect to the inner product on \(\mathcal{P}\) and the Euclidean inner product on \(\mathbb{R}^{n}\). More explicitly \([\bm{K}(\bm{\theta})]_{ik}=\langle\nabla_{\bm{\theta}}f(\bm{x}_{i};\bm{\theta }),\nabla_{\bm{\theta}}f(\bm{x}_{k};\bm{\theta})\rangle\). For convenience we write \(\bm{K}\) in place of \(\bm{K}(\bm{\theta})\). We are concerned with the minimum eigenvalue \(\lambda_{\min}(\bm{K})\), which depends both on the input data \(\bm{X}\) and the parameter \(\bm{\theta}\). We say the dataset \(\bm{x}_{1},\cdots,\bm{x}_{n}\) is \(\delta\)_-separated_ for \(\delta\in(0,\sqrt{2}]\) if \(\min_{i\neq k}\min(\|\bm{x}_{i}-\bm{x}_{k}\|,\|\bm{x}_{i}+\bm{x}_{k}\|)\geq\delta\), which is a measure of distance in direction.

Main contributions.Our results are for data that lies on a sphere and is \(\delta\)-separated for some \(\delta\in(0,\sqrt{2}]\). Unlike prior work we do not make any assumptions on the distribution from which the data is sampled, e.g., uniform on the sphere or Lipschitz concentrated, and we do not require the input dimension \(d_{0}\) to scale with the number of samples \(n\).

* In Theorem 1 we consider shallow ReLU networks with input dimension \(d_{0}\) and hidden width \(d_{1}\) and prove that if \(d_{1}=\tilde{\Omega}(\|\bm{X}\|^{2}d_{0}^{3}\delta^{-2})\) then with high probability \(\lambda_{\min}(\bm{K})=\tilde{\Omega}(d_{0}^{-3}\delta^{2})\). Furthermore, defining \(\delta^{\prime}=\min_{i\neq k}\|\bm{x}_{i}-\bm{x}_{k}\|\), we have \(\lambda_{\min}(\bm{K})=O(\delta^{\prime})\).
* In Theorem 8 we illustrate how our results for shallow networks can be extended to cover depth-\(L\) networks. In particular, if the layer widths satisfy a pyramidal condition, meaning \(d_{l}\geq d_{l+1}\) for \(l\in\{1,\cdots,L-1\}\), \(d_{L-1}\gtrsim 2^{L}\log(nL/\epsilon)\) and \(d_{1}=\tilde{\Omega}(nd_{0}^{3}\delta^{-4})\), then \(\lambda_{\min}(\bm{K})=\tilde{\Omega}(d_{0}^{-3}\delta^{4})\) and \(\lambda_{\min}(\bm{K})=O(L)\) with high probability.
* Our results allow us to analyze the smallest eigenvalue of the NTK for data drawn from any distribution for which one can establish \(\delta\)-separation with high probability in terms of \(d_{0}\) and \(n\). For example, for shallow networks with data drawn uniformly from a sphere, in Corollary 2 we show that if \(d_{0}d_{1}=\tilde{\Omega}(n^{1+4/(d_{0}-1)})\), then with high probability \(\lambda_{\min}(\bm{K})=\tilde{O}\left(n^{-2/(d_{0}-1)}\right)\) and \(\lambda_{\min}(\bm{K})=\tilde{\Omega}\left(n^{-4/(d_{0}-1)}\right)\). Moreover, this bound is tight up to logarithmic factors for \(d_{0}=\Omega(\log(n))\) matching prior findings for this regime.

The rest of this paper is structured as follows: in Section 2 we provide a summary of related works and compare and contrast our results with the existing state of the art; in Section 3 we present our results for shallow networks; finally in Section 4 we extend our shallow results to the deep case.

Notations.With regard to general points on notation we let \([n]=\{1,2,\cdots,n\}\) denote the set of the first \(n\) positive integers. If \(\bm{x}\in\mathbb{R}^{d}\) then we let \([\bm{x}]_{i}\) denote the \(i\)th entry of \(\bm{x}\). If \(f\) and \(g\) are real-valued functions, we write \(f\lesssim g\) or \(f=O(g)\) when there exists an absolute constant \(C\) such that \(f(x)\leq Cg(x)\) for all \(x\). Similarly, we write \(f\gtrsim g\) or \(f=\Omega(g)\) when there exists a constant \(c\) such that \(f(x)\geq cg(x)\) for all \(x\). We write \(f\asymp g\) when \(f\lesssim g\) and \(f\gtrsim g\) both hold. The notation \(\tilde{\Omega}\) hides logarithmic factors. Logarithms are generally considered to be in base \(e\), though in most settings the particular choice of base can be absorbed by a constant.

## 2 Related work

Prior work on the NTK.Jacot et al. (2018) highlight that the optimization dynamics of neural networks are controlled by the Gram matrix of the Jacobian of the network function, an object referred to as the NTK Gram matrix, or, as we refer to it here, simply the NTK. That work also shows that in the infinite-width limit the NTK converges in probability to a deterministic kernel. Of particular interest is the observation that in the infinite-width setting the network behaves like a linear model (Lee et al., 2019). Further, if a network is polynomially wide in the number of samples then the smallest eigenvalue of the NTK can be lower bounded in terms of the smallest eigenvalue of its infinite-width analog. As a result, assuming the latter is positive, global convergence guarantees for gradient descent can be obtained (Du et al., 2019, 2019, 2019, 2019, 2020, 2020, 2021, 2020, 2021, 2022, 2023). The positive definiteness of the NTK is equivalent to the Jacobian having full rank, which can also be used to study the loss landscape (Liu et al., 2020, 2022, 2023). Beyond the smallest eigenvalue, there is interest in characterizing the full spectrum of the NTK (Basri et al., 2019, 2020, 2021, 2023), which has implications on the dynamics of the empirical risk (Arora et al., 2019, 2021) as well as the generalization error (Cao et al., 2021, 2020, 2021, 2022, 2022, 2023). Finally, although a powerful and successful tool for analyzing neural networks it must be noted that the NTK has limitations, most notably perhaps that it struggles to explain the rich feature learning commonly observed in practice (Lee et al., 2020, 2021, 2020).

Prior work on the smallest eigenvalue of the NTK.Many of the prior works discussed so far assume or prove that \(\lambda_{\min}(\bm{K})\) is positive, but do not provide a quantitative lower bound. Here we discuss works seeking to address this issue and to which we view our work as complementary. For shallow ReLU networks and data drawn uniformly from the sphere, Xie et al. (2017, Theorem 3) and Montanari and Zhong (2022, Theorem 3.2) provide lower bounds on the smallest singular and eigenvalue value of the Jacobian and NTK respectively. In addition to requiring the data to be drawn uniform from the sphere both of these results are high dimensional in the sense that for Xie et al. (2017, Theorem 3) to be non-vacuous it is necessary that \(d_{0}=\Omega(d_{1}n^{2})\), while Montanari and Zhong (2022, Theorem 3.2) requires, as per their Assumption 3.1, that \(d_{0}=\tilde{\Omega}(\sqrt{n})\).

Nguyen et al. (2021, Theorem 4.1) derives lower and upper bounds for the smallest eigenvalue of the NTK for deep ReLU networks under standard initialization conditions assuming the data is drawn from a distribution satisfying a Lipschitz concentration property. They show that the NTK is well conditioned if the network has a layer of width of order equal to the number of data points \(n\) up to logarithmic factors. Concretely, if at least one layer has width linear in \(n\) (ignoring logarithmic factors) and the others are at least poly-logarithmic in \(n\), then \(\lambda_{\min}(\bm{K})=\Omega(\mu_{r}^{2}(\sigma)d_{0})\) (or \(\Omega(\mu_{r}^{2}(\sigma))\) with normalized data), where \(\mu_{r}(\sigma)\) denotes the \(r\)th Hermite coefficient of \(\sigma\) with any even integer \(r\geq 2\). However, in their result the bound holds with high probability only if \(d_{0}\) scales as \(\log(n)\).

Bombari et al. (2022, Theorem 1) derive lower and upper bounds for the smallest eigenvalue of the NTK under similar conditions as Nguyen et al. (2021, Theorem 4.1) aside from the following: they consider smooth rather than ReLU activation functions, the widths follow a loose pyramidal topology, meaning \(d_{l}=O(d_{l-1})\) for all \(l\in[L-1]\), \(d_{L-1}d_{L-2}\) scales linearly in \(n\) (ignoring logarithmic factors), and there exists a \(\gamma>0\) such that \(n^{\gamma}=O(d_{L-1})\). Under these conditions they show that \(\lambda_{\min}(\bm{K})=\Omega(d_{L-1}d_{L-2})\) with high probability as both \(d_{L-1}\) and \(n\) grow. This result illustrates that for the NTK to be well conditioned it suffices that the number of neurons grows as \(\tilde{\Omega}(\sqrt{n})\). The loose pyramidal condition on the widths implies \(d_{L-1}d_{L-2}=O(d_{0}^{2})\) and as they also assume that \(n=o(d_{L-1}d_{L-2})\) then \(n=o(d_{0}^{2})\) which in turn implies \(d_{0}=\Omega(\sqrt{n})\).

The rough strategy used by both Bombari et al. (2022) and Nguyen et al. (2021), as well as in our own results, can be described in terms of two main steps. In the first step, one bounds the smallest eigenvalue of a shallow network. The results for the shallow case can then be extended to the deep case, e.g., via a layerwise decomposition of the NTK matrix. This second step is architecture-dependent and its proof depends on the bounds derived in the first step. Our results focus on improving the first step which imply corresponding improvements for the second step.

## 3 Shallow networks

Here we study the smallest eigenvalue of the NTK of a shallow neural network. The parameter space \(\mathcal{P}\) of this network is \(\mathbb{R}^{d_{1}\times d_{0}}\times\mathbb{R}^{d_{1}}\) and it is equipped with the inner product

\[\langle(\bm{W},\bm{v}),(\bm{W}^{\prime},\bm{v}^{\prime})\rangle=\text{Trace}( \bm{W}^{T}\bm{W}^{\prime})+\bm{v}^{T}\bm{v}^{\prime}.\]For convenience we sometimes write \(d=d_{0}\). The neural network \(f:\mathbb{R}^{d_{0}}\times\mathcal{P}\to\mathbb{R}\) is defined as

\[f(\bm{x};\bm{W},\bm{v})=\frac{1}{\sqrt{d_{1}}}\sum_{j=1}^{d_{1}}v_{j}\sigma(\bm{ w}_{j}^{T}\bm{x}),\] (2)

where \(\bm{W}=[\bm{w}_{1},\cdots,\bm{w}_{d_{1}}]^{T}\in\mathbb{R}^{d_{1}\times d_{0}}\) are the inner layer weights, \(\bm{v}=[v_{1},\cdots,v_{d_{1}}]^{T}\in\mathbb{R}^{d_{1}}\) the outer layer weights, and \(\bm{\theta}=(\bm{W},\bm{v})\). We consider the ReLU activation function applied entrywise with \(\sigma(z)=\max\{0,z\}\). The derivative \(\dot{\sigma}\) satisfies \(\dot{\sigma}(z)=1\) for \(z>0\) and \(\dot{\sigma}(z)=0\) for \(z<0\). Although \(\sigma\) is not differentiable at 0, we take \(\dot{\sigma}(0)=0\) by convention. Unless otherwise stated we assume that the entries of \(\bm{W}\) and \(\bm{v}\) are drawn mutually iid from a standard Gaussian distribution \(\mathcal{N}(0,1)\). Our main result for shallow networks is the following theorem.

**Theorem 1**.: _Let \(d\geq 3\), \(\epsilon\in(0,1)\), and \(\delta,\delta^{\prime}\in(0,\sqrt{2})\). Suppose that \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\) are \(\delta\)-separated and \(\min_{i\neq k}\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta^{\prime}\). Define_

\[\lambda=\left(1+\frac{d\log(1/\delta)}{\log(d)}\right)^{-3}\delta^{2}.\]

_If \(d_{1}\gtrsim\frac{\|\bm{X}\|^{2}}{\lambda}\log\frac{n}{\epsilon}\), then with probability at least \(1-\epsilon\),_

\[\lambda\lesssim\lambda_{\min}(\bm{K})\lesssim\delta^{\prime}.\]

A proof of Theorem 1 is provided in Appendix C.7. Suppressing logarithmic factors, Theorem 1 implies that \(d_{1}=\bar{\Omega}\left(\|\bm{X}\|^{2}d_{0}^{3}\delta^{-2}\right)\) suffices to ensure that \(\lambda_{\min}(\bm{K})=\bar{\Omega}(d_{0}^{-3}\delta^{2})\) and \(\lambda_{\min}(\bm{K})=O(\delta^{\prime})\) with high probability (note the trivial bound \(\|\bm{X}\|^{2}\leq\|\bm{X}\|_{F}^{2}\leq n\)). We emphasize that unlike existing results i) we make no distributional assumptions on the data, instead only assuming a milder \(\delta\)-separated condition, and ii) our bounds hold with high probability even if \(d_{0}\) is held constant.

A few further remarks are in order. First, the condition \(d_{0}\geq 3\) is necessary because our technique relies on the addition formula for spherical harmonics (Efthimiou & Frye, 2014, Theorem 4.11); the bound we derive based on this formula (Lemma 15 in Appendix A.2) becomes vacuous for \(d_{0}<3\). However, for \(d_{0}=2\) analogous bounds could be derived using more elementary tools while the case \(d_{0}=1\) is of little interest as only a trivial dataset is possible. Moreover, data in \(\mathbb{S}^{1}\) could be embedded in \(\mathbb{S}^{2}\) since we do not impose any distributional assumptions.

Second, one can use Theorem 1 to bound the smallest eigenvalue of the NTK for data drawn from the uniform distribution on the sphere by bounding \(\delta\) with high probability in terms of \(n\) and \(d\). We use that \(\delta=\Omega(n^{-2/d_{0}})\) and \(\delta^{\prime}=O(n^{-2/d_{0}})\) with high probability. We direct the interested reader to Appendix C.8 for further details.

**Corollary 2**.: _Let \(d\geq 3\), \(n\geq 2\), \(\epsilon\in(0,1)\), \(\bm{x}_{1},\cdots,\bm{x}_{n}\sim U(\mathbb{S}^{d-1})\) be mutually iid. Define_

\[\lambda=\left(1+\frac{\log(n/\epsilon)}{\log(d)}\right)^{-3}\left(\frac{ \epsilon^{2}}{n^{4}}\right)^{1/(d-1)}.\]

_If \(d_{1}\gtrsim\frac{1}{\lambda}\left(1+\frac{n+\log(1/\epsilon)}{d}\right)\log \frac{n}{\epsilon}\), then with probability at least \(1-\epsilon\) over the data and network parameters,_

\[\lambda\lesssim\lambda_{\min}(\bm{K})\lesssim\left(\frac{\log(1/\epsilon)}{n^ {2}}\right)^{1/(d-1)}.\]

The above corollary implies that if \(d_{0}d_{1}=\bar{\Omega}\left(n^{1+4/(d_{0}-1)}\right)\), then with high probability \(\lambda_{\min}(\bm{K})=\bar{\Omega}(n^{-4/(d_{0}-1)})\) and \(\lambda_{\min}(\bm{K})=\bar{O}(n^{-2/(d_{0}-1)})\). In particular, for data sampled uniformly from a sphere, the scaling \(d_{0}=\Omega(\log n)\) is both necessary and sufficient for \(\lambda_{\min}(\bm{K})\) to be \(\bar{\Theta}(1)\). In particular the bounds are sharp in this case.

### Proof outline for Theorem 1

Recall the definitions of \(F(\bm{\theta})\) and \(\bm{K}\) in (1). For the choice of \(f\) given in (2), a straightforward decomposition of the NTK with respect to the inner and outer weights gives

\[\bm{K}=\bm{K}_{1}+\bm{K}_{2},\] (3)where \(\bm{K}_{1}=\nabla_{\bm{W}}F(\bm{\theta})^{*}\nabla_{\bm{W}}F(\bm{\theta})\) and \(\bm{K}_{2}=\nabla_{\bm{v}}F(\bm{\theta})^{*}\nabla_{\bm{v}}F(\bm{\theta})=\frac{1 }{d_{1}}\sigma(\bm{W}\bm{X})^{T}\sigma(\bm{W}\bm{X})\). As both \(\bm{K}_{1}\) and \(\bm{K}_{2}\) are positive semi-definite,

\[\lambda_{\min}(\bm{K})\geq\lambda_{\min}(\bm{K}_{1})+\lambda_{\min}(\bm{K}_{2});\] (4)

see, e.g., Horn & Johnson (2012, Theorem 4.3.1). Our proof now follows the highlighted steps below.

1) Bound the smallest eigenvalue in terms of the infinite-width limit.We proceed to bound both \(\lambda_{\min}(\bm{K}_{1})\) and \(\lambda_{\min}(\bm{K}_{2})\) in terms of the smallest eigenvalues of their infinite-width counterparts, see Lemmas 3 and 4 below, which act as good approximations for sufficiently wide networks.

**Lemma 3**.: _Suppose that \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\). Let_

\[\lambda_{1}=\lambda_{\min}\left(\mathbb{E}_{\bm{u}\sim U(\mathbb{S}^{d-1})} \left[\hat{\sigma}\left(\bm{X}^{T}\bm{u}\right)\hat{\sigma}\left(\bm{u}^{T} \bm{X}\right)\right]\right).\]

_If \(\lambda_{1}>0\) and \(d_{1}\gtrsim\lambda_{1}^{-1}\|\bm{X}\|^{2}\log\frac{n}{\epsilon}\), then with probability at least \(1-\epsilon\)_

\[\lambda_{\min}(\bm{K}_{1})\gtrsim\lambda_{1}.\]

**Lemma 4**.: _Suppose that \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\). Let_

\[\lambda_{2}=d\lambda_{\min}\left(\mathbb{E}_{\bm{u}\sim U(\mathbb{S}^{d-1})} \left[\sigma(\bm{X}^{T}\bm{u})\sigma(\bm{u}^{T}\bm{X})\right]\right).\]

_If \(\lambda_{2}>0\) and \(d_{1}\gtrsim\frac{n}{\lambda_{2}}\log\left(\frac{n}{\lambda_{2}}\right)\log \left(\frac{n}{\epsilon}\right)\), then with probability at least \(1-\epsilon\)_

\[\lambda_{\min}(\bm{K}_{2})\gtrsim\lambda_{2}.\]

We prove Lemmas 3 and 4 in Appendices C.1 and C.2 respectively. Observe that while the parameters of the model are initialized as Gaussian, the expectations above are taken with respect to the uniform measure on the sphere. The motivation for using the uniform measure on the sphere is that it enables us to work with spherical harmonics, for which there is the highly useful _addition formula_ (see, e.g., Efthimiou & Frye, 2014, Theorem 4.11). The exchange of measures is possible in the case of Lemma 3 due to the scale invariance of \(\hat{\sigma}\), while for Lemma 4 it is possible because \(\sigma\) is homogeneous.

2) Interpret the infinite-width kernel in terms of a hemisphere transform.Next, for a given \(\bm{X}\) and \(\psi\in\{\sqrt{d}\sigma,\hat{\sigma}\}\) we define the limiting NTK \(\bm{K}_{\psi}^{\infty}\in\mathbb{R}^{n\times n}\) as

\[\bm{K}_{\psi}^{\infty}=\mathbb{E}_{\bm{u}\sim U(\mathbb{S}^{d-1})}\left[\psi \left(\bm{X}^{T}\bm{u}\right)\psi\left(\bm{u}^{T}\bm{X}\right)\right].\] (5)

Consider a fixed vector \(\bm{z}\in\mathbb{S}^{n-1}\) and interpret the Euclidean inner product \(\langle\psi(\bm{X}^{T}\bm{u}),\bm{z}\rangle\) as a function of \(\bm{u}\in\mathbb{S}^{d-1}\). It will prove useful to think of this map as an integral transform. To this end let \(\mathcal{M}(\mathbb{S}^{d-1})\) denote the vector space of signed Radon measures on \(\mathbb{S}^{d-1}\) and fix \(\psi\in\{\sqrt{d}\sigma,\hat{\sigma}\}\). For a signed Radon measure \(\mu\in\mathcal{M}(\mathbb{S}^{d-1})\) we introduce the integral transform \(T_{\psi}\mu:\mathbb{S}^{d-1}\rightarrow\mathbb{R}\), defined as

\[(T_{\psi}\mu)(\bm{u})=\int_{\mathbb{S}^{d-1}}\psi(\langle\bm{u},\bm{x}\rangle)d \mu(\bm{x}).\] (6)

Note for \(\psi\in\{\sqrt{d}\sigma,\hat{\sigma}\}\) this is a _hemisphere transform_(Rubin, 1999) as the integrand \(\psi(\langle\bm{u},\cdot\rangle)\) is supported on a hemisphere normal to \(\bm{u}\). We provide background material on the hemisphere transform in Appendix B. Let \(\mathcal{M}_{\bm{X}}\subset\mathcal{M}\) denote the space of signed Radon measures supported on the data set \(\{\bm{x}_{1},\cdots,\bm{x}_{n}\}\). For each measure \(\mu\in\mathcal{M}_{\bm{X}}\) there exists a vector \(\bm{z}\in\mathbb{R}^{n}\) such that \(\mu=\sum_{i=1}^{n}z_{i}\delta_{\bm{x}_{i}}\), where \(\delta_{\bm{x}}\) is the Dirac measure supported on \(\bm{x}\). We write \(\mu=\mu_{\bm{z}}\) to indicate this correspondence. The following lemma relates the smallest eigenvalue of \(\bm{K}_{\psi}^{\infty}\) to the norm of the hemisphere transform of a measure supported on the data; a proof is provided in Appendix C.3.

**Lemma 5**.: _Fix \(\bm{X}\in\mathbb{R}^{d\times n}\) and \(\psi\in\{\sqrt{d}\sigma,\hat{\sigma}\}\). For all \(\bm{z}\in\mathbb{R}^{n}\), \(\langle\bm{K}_{\psi}^{\infty}\bm{z},\bm{z}\rangle=\|T_{\psi}\mu_{\bm{z}}\|^{2}\). Moreover,_

\[\lambda_{\min}(\bm{K}_{\psi}^{\infty})=\inf_{\|\bm{z}\|=1}\|T_{\psi}\mu_{\bm{z}} \|^{2}.\]3) Bound the hemisphere transform norm via spherical harmonics.We proceed to lower bound \(\|T_{\psi}\mu_{\bm{z}}\|^{2}\) for all \(\bm{z}\in\mathbb{R}^{d}\). Let \(L^{2}(\mathbb{S}^{d-1})\) denote the Hilbert space of real-valued, square-integrable functions with respect to the uniform probability measure on \(\mathbb{S}^{d-1}\), and let \(\mathcal{C}(\mathbb{S}^{d-1})\subset L^{2}(\mathbb{S}^{d-1})\) denote the subspace of continuous functions. For \(\mu\in\mathcal{M}(\mathbb{S}^{d-1})\) and \(g\in\mathcal{C}(\mathbb{S}^{d-1})\) we define

\[\langle\mu,g\rangle:=\int_{\mathbb{S}^{d-1}}g(\bm{x})d\mu(\bm{x}).\]

If \(g_{1},\cdots,g_{N}\in L^{2}(\mathbb{S}^{d-1})\) are orthonormal, in particular consider \(g_{r}\) as spherical harmonics, then via a Bessel inequality

\[\|T_{\psi}\mu_{\bm{z}}\|^{2}\geq\sum_{a=1}^{N}|\langle T_{\psi}\mu_{\bm{z}},g_ {a}\rangle|^{2}=\sum_{a=1}^{N}|\langle\mu_{\bm{z}},T_{\psi}g_{a}\rangle|^{2}= \sum_{a=1}^{N}\left|\sum_{i=1}^{n}(T_{\psi}g_{a})(\bm{x}_{i})z_{i}\right|^{2}.\]

Importantly, \(T_{\psi}\) is self-adjoint (see Lemma 17 in Appendix B for details) and the spherical harmonics are eigenfunctions of \(T_{\psi}\), i.e., \(T_{\psi}g_{a}=\kappa_{a}g_{a}\). A summary of the key properties of spherical harmonics needed for our results are provided in Appendix A.2. Therefore

\[\|T_{\psi}\mu_{\bm{z}}\|^{2}\geq\sum_{a=1}^{N}\left|\sum_{i=1}^{n}(T_{\psi}g_{ a})(\bm{x}_{i})z_{i}\right|^{2}=\sum_{a=1}^{N}\kappa_{a}^{2}\left|\sum_{i=1}^{n}g_ {a}(\bm{x}_{i})z_{i}\right|^{2}\geq\min_{a}\kappa_{a}^{2}\|\bm{D}\bm{z}\|_{2}^ {2},\]

where \(\bm{D}\in\mathbb{R}^{N\times n}\) is a matrix with entries \([\bm{D}]_{ai}=g_{a}(\bm{x}_{i})\). As a result

\[\lambda_{\min}(\bm{K}_{\psi}^{\infty})\geq\min_{a}\kappa_{a}^{2}\sigma_{\min} ^{2}(\bm{D}).\]

4) Bound the hemisphere transform and spherical harmonics on the data.The following result shows that if we let the functions \((g_{a})_{a\in[N]}\) be spherical harmonics and allow \(N\) to be sufficiently large, then we can bound the minimum singular value of \(\bm{D}\). In what follows let \(\mathcal{H}_{r}^{d}\) denote the vector space of degree-\(r\) harmonic homogeneous polynomials on \(d\) variables.

**Lemma 6**.: _Suppose \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\) are \(\delta\)-separated. Suppose that \(\beta\in\{0,1\}\) and that \(R\in\mathbb{Z}_{\geq 0}\) are such that \(N:=\sum_{r=0}^{R}\dim(\mathcal{H}_{2r+\beta}^{d})\) satisfies \(N\geq C\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/2}\) where \(C>0\) is a universal constant. Let \(g_{1},\cdots,g_{N}\) be spherical harmonics which form an orthonormal basis of \(\bigoplus_{r=0}^{R}\mathcal{H}_{2r+\beta}^{d}\). If \(\bm{D}\in\mathbb{R}^{N\times n}\) is defined as \(\bm{D}_{ai}=g_{a}(\bm{x}_{i})\) then \(\sigma_{\min}(\bm{D})\geq\sqrt{\frac{N}{2}}\)._

A proof of Lemma 6 can be found in Appendix C.4. By carefully choosing values for \(R\) and \(N\) in Lemma 6 and performing some asymptotics on the resulting expressions, we arrive at the following bound on the hemisphere transform of a measure.

**Lemma 7**.: _Let \(d\geq 3\) and suppose that \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\) are \(\delta\)-separated. For all \(\bm{z}\in\mathbb{R}^{n}\) with \(\|\bm{z}\|\leq 1\) then_

\[\|T_{\psi}\mu_{\bm{z}}\|^{2}\gtrsim\begin{cases}\left(1+\frac{d\log(1/\delta)} {\log d}\right)^{-3}\delta^{2}&\text{ if }\psi=\hat{\sigma}\\ \left(1+\frac{d\log(1/\delta)}{\log d}\right)^{-3}\delta^{4}&\text{ if }\psi= \sqrt{d}\sigma.\end{cases}\]

A proof of Lemma 7 is provided in Appendix C.5. The lower bound of Theorem 1 follows by bounding \(\lambda_{1}\), as defined in Lemma 3, using Lemma 7.

Before proceeding to the upper bound, we pause to remark on the generality of this argument for handling other activation functions. First, we use the positive homogeneity of the activation function in order to write \(\lambda_{\min}(\bm{K}_{\psi}^{\infty})\) as the \(L^{2}(\mathbb{S}^{d-1})\) norm of a function on the sphere. This is beneficial as it allows us to work with the spherical harmonics and use the associated addition formula. The ReLU activation and its derivative are also convenient with regard to computing the eigenvalues of the hemisphere transform (or more generally the eigenvalues of the integral operator). In particular, this requires evaluating integrals against Gegenbauer polynomials for which analytic expressions are available. For polynomial or piecewise polynomial activations similar results could be obtained. However, for other activations, e.g., \(\tanh\) or sigmoid, such quantities appear challenging to compute.

5) Upper bound.The upper bound of Theorem 1 is simpler than the lower bound and hinges on the following calculation. Let \(\bm{x}_{i},\bm{x}_{k}\) be two data points. Then

\[\lambda_{\min}(\bm{K})\leq\frac{1}{2}(\bm{e}_{i}-\bm{e}_{k})^{T}\bm{K}(\bm{e}_{i }-\bm{e}_{k})=\frac{1}{2}\|\nabla_{\bm{\theta}}f(\bm{x}_{i})-\nabla_{\bm{ \theta}}f(\bm{x}_{k})\|^{2}.\]

Therefore it suffices to upper bound the norm of \(\nabla_{\bm{\theta}}f(\bm{x}_{i})-\nabla_{\bm{\theta}}f(\bm{x}_{k})\). We choose \(i,k\in[n]\) such that \(\bm{x}_{i},\bm{x}_{k}\) are the two closest points in the dataset. We then translate this into a statement about the gradients. If \(\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta\), then with high probability over the network parameters, \(\|\nabla_{\bm{\theta}}f(\bm{x}_{i})-\nabla_{\bm{\theta}}f(\bm{x}_{k})\|^{2} \lesssim\delta\) (see Lemma 29), and we arrive at the desired upper bound in Theorem 1.

## 4 From shallow to deep neural networks

Our goal here is to detail just one approach as how the results of Section 3 can be extended to deep networks. To be clear, here we consider a fully connected network with input dimension \(d_{0}\) and \(L\) layers, where each layer has width \(d_{1},\cdots,d_{L}\) respectively and \(d_{L}=1\). The parameter space \(\mathcal{P}\) is a product space of matrices \(\prod_{l=1}^{L}\mathbb{R}^{d_{l}\times d_{l-1}}\), equipped with the inner product

\[\langle(\bm{W}_{1},\cdots,\bm{W}_{L}),(\bm{W}_{1}^{\prime},\cdots,\bm{W}_{L} ^{\prime})\rangle=\sum_{l=1}^{L}\text{Trace}(\bm{W}_{l}^{T}\bm{W}_{l}^{\prime }).\]

The feature maps \(f_{l}:\mathbb{R}^{d_{0}}\times\mathcal{P}\rightarrow\mathbb{R}^{d_{l}}\) of the neural network are given by

\[f_{l}(\bm{x};\bm{\theta})=\begin{cases}\bm{x}&l=0\\ \sigma(\bm{W}_{l}f_{l-1}(\bm{x};\bm{\theta}))&l\in[L-1]\\ \bm{W}_{l}f_{l-1}(\bm{x};\bm{\theta})&l=L,\end{cases}\]

where \(\bm{W}_{l}\in\mathbb{R}^{d_{l}\times d_{l-1}}\) for all \(l\in[L]\), \(\bm{\theta}=(\bm{W}_{1},\cdots,\bm{W}_{L})\) and \(\sigma\) is the ReLU function \(x\mapsto\max(0,x)\) applied elementwise. We define the network map \(f\) to be the final feature map multiplied by a normalizing constant:

\[f=\left(\prod_{l=1}^{L-1}\sqrt{\frac{2}{d_{l}}}\right)f_{L}.\] (7)

Given \(n\) data points \(\bm{x}_{1},\cdots,\bm{x}_{n}\), we bound the smallest eigenvalue of the NTK (1) associated with this particular choice of \(f\).

**Theorem 8**.: _Suppose \(\epsilon\in(0,1/3)\), \(\delta\in(0,\sqrt{2}]\), \(d_{0}\geq 3\), the data \(\bm{x}_{1},\bm{x}_{2},\cdots,\bm{x}_{n}\in\mathbb{S}^{d_{0}-1}\) is \(\delta\)-separated and define_

\[\lambda=\left(1+\frac{d_{0}\log(1/\delta)}{\log d_{0}}\right)^{-3}\delta^{4}.\]

_With regard to the network architecture, let \(L\geq 3\), \(d_{l}\geq d_{l+1}\) for all \(l\in[L-1]\), \(d_{L-1}\gtrsim 2^{L}\log\left(\frac{nL}{\epsilon}\right)\) and \(d_{1}\gtrsim\frac{n}{\lambda}\log\left(\frac{n}{\lambda}\right)\log\left( \frac{n}{\epsilon}\right)\). Then with probability at least \(1-\epsilon\) over the network parameters_

\[\lambda\lesssim\lambda_{\min}(\bm{K})\lesssim L.\]

We emphasize that these bounds make no distributional assumptions on the data other than lying on the sphere and being \(\delta\)-separated; in particular, they hold even for constant \(d_{0}\). Indeed, if we consider \(d_{0}\) as some constant then Theorem 8 implies that if the first layer is sufficiently wide, \(d_{1}=\tilde{\Omega}(n\delta^{-4})\), then with high probability over the parameters \(\lambda_{\min}(\bm{K})=\tilde{\Omega}(\delta^{4})\) and \(\lambda_{\min}(\bm{K})=O(1)\).

A few remarks are in order. First, the pyramidal condition on the network widths could be relaxed by more directly borrowing techniques from Nguyen et al. (2021). We adopt this condition as it has the advantage of making the dependence of our bounds on the network depth \(L\) clearer. Second, compared with Theorem 1 and ignoring log factors, we observe the lower bound differs by a factor of \(\delta^{2}\). This arises as a result of the smallest eigenvalue of the feature Gram matrix \(\bm{F}_{1}^{T}\bm{F}_{1}\) being equivalent to the Jacobian of a shallow network with respect to the second layer weights, not the inner layer weights, which has a different lower bound as per Lemma 7. For reasons apparent in the proof outline below the lower bound on \(\lambda_{\min}(\bm{K})\) lacks a dependency on \(L\), however we hypothesize it should also grow linearly with \(L\) thereby matching the dependency of the upper bound. Finally, the upper bound itself follows a similar approach as used by Nguyen et al. (2021) and is weak in the sense that we cannot take advantage of the dataset separation for gradients deeper into the network. We remark that this is also a common problem in the prior work of Nguyen et al. (2021) and Bombari et al. (2022), we refer the reader to the proof outline below for further details.

### Proof outline for Theorem 8

The proof of the deep case is structured around the decomposition of the NTK provided in Lemma 9 below. To state this decomposition we introduce the following quantities. For \(l\in[L-1]\) we define the feature matrices \(\bm{F}_{l}\in\mathbb{R}^{d_{l}\times n}\) by

\[\bm{F}_{l}=[f_{l}(\bm{x}_{1}),\cdots,f_{l}(\bm{x}_{n})].\]

For \(l\in[L-1]\) and \(\bm{x}\in\mathbb{R}^{d}\) we define the activation patterns \(\bm{\Sigma}_{l}(\bm{x})\in\{0,1\}^{d_{l}\times d_{l}}\) to be the diagonal matrices

\[\bm{\Sigma}_{l}(\bm{x})=\text{diag}(\hat{\sigma}(\bm{W}_{l}f_{l-1}(\bm{x}))).\]

Finally, we let \(\textbf{1}_{n}\) denote the vector of all ones in \(\mathbb{R}^{n}\).

**Lemma 9**.: _Let \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{R}^{d}\) be nonzero. There exists an open set \(\mathcal{U}\subset\mathcal{P}\) of full Lebesgue measure such that \(f(\bm{x}_{i};\cdot)\) is continuously differentiable on \(\mathcal{U}\) for all \(i\in[n]\). Moreover, for all \(\bm{\theta}\in\mathcal{U}\) the NTK Gram matrix \(\bm{K}\) defined in (1) with network function (7) satisfies_

\[\left(\prod_{l=1}^{L-1}\frac{d_{l}}{2}\right)\bm{K}\!=\sum_{l=0}^{L-1}(\bm{F}_ {l}^{T}\bm{F}_{l})\odot(\bm{B}_{l+1}\bm{B}_{l+1}^{T}),\]

_where the \(i\)th row of \(\bm{B}_{l}\in\mathbb{R}^{n\times n_{l}}\) is defined as_

\[[\bm{B}_{l}]_{i,:}=\begin{cases}\bm{\Sigma}_{l}(\bm{x}_{i})\left(\prod_{k=l+1} ^{L-1}\bm{W}_{k}^{T}\bm{\Sigma}_{k}(\bm{x}_{i})\right)\bm{W}_{L}^{T},&l\in[L-1 ],\\ \textbf{1}_{n},&l=L.\end{cases}\]

For completeness we prove Lemma 9 in Appendix D.1. Observe each matrix summand in Lemma 9 is positive semi-definite (PSD) and recall for any two PSD matrices \(\bm{A}\) and \(\bm{B}\) one has \(\lambda_{\min}(\bm{A}+\bm{B})\geq\lambda_{\min}(\bm{A})+\lambda_{\min}(\bm{B})\) (see e.g. Horn & Johnson, 2012, Theorem 4.3.1) and \(\lambda_{\min}(\bm{A}\odot\bm{B})\geq\lambda_{\min}(\bm{A})\min_{i\in[n]}[\bm{ B}]_{ii}\)(Schur, 1911). Therefore

\[\left(\prod_{l=1}^{L-1}\frac{d_{l}}{2}\right)\lambda_{\min}(\bm{K})\geq\sum_{ l=0}^{L-1}\lambda_{\min}\left((\bm{F}_{l}^{T}\bm{F}_{l})\odot(\bm{B}_{l+1}\bm{B} _{l+1}^{T})\right)\geq\lambda_{\min}\left(\bm{F}_{1}^{T}\bm{F}_{1}\right)\min_ {i\in[n]}\left\|[\bm{B}_{2}]_{i,:}\right\|^{2}.\]

In order to upper bound the smallest eigenvalue we follow Nguyen et al. (2021) and analyze the Raleigh quotient \(R(\bm{u})=\frac{\bm{u}^{T}\bm{K}\bm{u}}{\|\bm{u}\|^{2}}\). In particular, for any nonzero \(\bm{u}\in\mathbb{R}^{n}\) we have \(\lambda_{\min}(\bm{K})\leq R(\bm{u})\) and therefore \(\lambda_{\min}(\bm{K})\leq R(\bm{e}_{i})=[\bm{K}]_{ii}\) for all \(i\in[n]\). As a result

\[\left(\prod_{l=1}^{L-1}\frac{d_{l}}{2}\right)\lambda_{\min}(\bm{K})\leq\left[ \sum_{l=0}^{L-1}(\bm{F}_{l}^{T}\bm{F}_{l})\odot(\bm{B}_{l+1}\bm{B}_{l+1}^{T}) \right]_{ii}=\sum_{l=0}^{L-1}\|f_{l}(\bm{x}_{i})\|^{2}\|[\bm{B}_{l+1}]_{i,:}\| ^{2}.\]

Combining the upper and lower bounds we have

\[\lambda_{\min}\left(\bm{F}_{1}^{T}\bm{F}_{1}\right)\min_{i\in[n]}\|[\bm{B}_{2 }]_{i,:}\|^{2}\leq\lambda_{\min}(\bm{K})\left(\prod_{l=1}^{L-1}\frac{d_{l}}{2} \right)\leq\sum_{l=0}^{L-1}\|f_{l}(\bm{x}_{i})\|^{2}\|[\bm{B}_{l+1}]_{i,:}\|^{ 2},\] (8)

where the right hand side holds for any \(i\in[n]\). Based on (8), we proceed first by bounding the norm of the network features. We achieve this via an inductive argument, bounding the norm of the features at one layer with high probability, and then conditioning on this event to bound the norm of the features at the next layer with high probability.

**Lemma 10**.: _Let \(\bm{x}\in\mathbb{S}^{d_{0}-1}\), \(L\geq 2\) and \(l\in[L-1]\). If \(d_{k}\gtrsim l^{2}\log(l/\epsilon)\) for all \(k\in[l]\), then_

\[e^{-1}\left(\prod_{h=1}^{l}\frac{d_{h}}{2}\right)\leq\|f_{l}(\bm{x})\|^{2}\leq e \left(\prod_{h=1}^{l}\frac{d_{h}}{2}\right)\]

_holds with probability at least \(1-\epsilon\) over the network parameters._A proof of Lemma 10 is provided in Appendix D.2. Next we derive upper and lower bounds on the backpropagation terms \([\bm{B}_{l}]_{i,:}\). Our strategy for this is as follows: for \(l\in[L-2]\), let \(\bm{S}_{l}(\bm{x})=\bm{\Sigma}_{l}(\bm{x})\left(\prod_{k=l+1}^{L-1}\bm{W}_{k}^{T} \bm{\Sigma}_{k}(\bm{x})\right)\) and observe

\[[\bm{B}_{l}]_{i,:}=\bm{S}_{l}(\bm{x}_{i})\bm{W}_{L}^{T}.\]

Since \(\bm{x}_{i}\in\mathbb{S}^{d_{0}-1}\), it is sufficient to lower bound \(\|\bm{S}_{l}(\bm{x})\bm{W}_{L}^{T}\|_{2}^{2}\) for an arbitrary \(\bm{x}\in\mathbb{S}^{d_{0}-1}\). As the vector \(\bm{W}_{L}^{T}\in\mathbb{R}^{d_{L-1}}\) is distributed as \(\bm{W}_{L}^{T}\sim\mathcal{N}(\bm{0}_{d_{L-1}},I_{d_{L-1}})\), following Vershynin (2018, Theorem 6.3.2) we have that for any \(\bm{A}\in\mathbb{R}^{d_{l}\times d_{L-1}}\) and \(t\geq 0\)

\[\mathbb{P}(\|\bm{A}\bm{W}_{L}^{T}\|-\|\bm{A}\|_{F}\geq t)\leq 2\exp\left(- \frac{Ct^{2}}{\|\bm{A}\|^{2}}\right)\]

for some constant \(C>0\). As a result, with \(t=\frac{1}{2}\|\bm{A}\|_{F}^{2}\) then

\[\mathbb{P}\left(\frac{1}{4}\|\bm{A}\|_{F}^{2}\leq\|\bm{A}\bm{W}_{L}^{T}\|^{2} \leq\frac{3}{4}\|\bm{A}\|_{F}^{2}\right)\geq 1-\exp\left(-C\frac{\|\bm{A}\|_{F} ^{2}}{\|\bm{A}\|^{2}}\right).\]

In order to lower bound \(\|\bm{S}_{l}(\bm{x})\bm{W}_{L}^{T}\|^{2}\) with high probability over the parameters it therefore suffices to condition on appropriate bounds for \(\|\bm{S}_{l}(\bm{x})\|_{F}^{2}\) and \(\|\bm{S}_{l}(\bm{x})\|_{2}^{2}\). These bounds are provided in Lemmas 34 and 35 in Appendices D.3 and D.4 respectively. With these two lemmas in place we can bound \(\|\bm{S}_{l}(\bm{x}_{i})\bm{W}_{L}^{T}\|^{2}\).

**Lemma 11**.: _Let \(\bm{x}\in\mathbb{S}^{d_{0}-1}\), suppose \(L\geq 3\), \(d_{k}\geq d_{k+1}\) for all \(k\in[L-1]\) and \(d_{L-1}\gtrsim 2^{L}\log\left(\frac{L}{\epsilon}\right)\). Then, for any \(l\in[L-1]\), with probability at least \(1-\epsilon\) over the network parameters_

\[\|\bm{S}_{l}(\bm{x})\bm{W}_{L}^{T}\|^{2}\asymp 2^{-L+l+1}\prod_{k=l}^{L-1}d_{k}.\]

By combining Lemma 11 with a union bound we arrive at the following corollary, relevant for the lower bound of (8).

**Corollary 12**.: _Let \(\bm{x}_{i}\in\mathbb{S}^{d_{0}-1}\) for all \(i\in[n]\), \(L\geq 3\), \(d_{l}\geq d_{l+1}\) for all \(l\in[L-1]\) and \(d_{L-1}\gtrsim 2^{L}\log\left(\frac{nL}{\epsilon}\right)\). Then, for any \(l\in[L-1]\), with probability at least \(1-\epsilon\) over the network parameters_

\[\min_{i\in[n]}\|[\bm{B}_{2}]_{i,:}\|^{2}\gtrsim 2^{-L}\prod_{k=2}^{L-1}d_{k}.\]

The first-layer feature Gram matrix \(\bm{F}_{1}^{T}\bm{F}_{1}\) in the deep case is identically distributed to \(\bm{K}_{2}\) in the two-layer case; see (3) and the related definitions. Therefore we can apply Lemma 4 to lower bound the smallest eigenvalue of \(\bm{F}_{1}^{T}\bm{F}_{1}\). This, in combination with Corollary 12, yields the lower bound of Theorem 8. The upper bound follows by combining the bound on the feature norms provided by Lemma 10 with the bound on the backpropagation terms given in Lemma 11. A detailed proof of Theorem 8 is provided in Appendix D.6.

## 5 Conclusion

Summary and implications.Quantitative bounds on the smallest eigenvalue of the NTK are a critical ingredient for many current analyses of network optimization. Prior works provide bounds which are only applicable for data drawn from particular distributions and for which the input dimension \(d_{0}\) scales appropriately with the number of data samples \(n\). This work plugs an important gap in the existing literature by providing bounds for arbitrary datasets on the sphere (including those drawn from any distribution on the sphere) in terms of a measure of distance between data points. Furthermore, these bounds are applicable for any \(d_{0}\), in particular even \(d_{0}\) held constant with respect to \(n\).

Limitations.Our bounds currently only hold for the ReLU activation function. Another limitation, also present in prior work, is that our upper bound on the smallest eigenvalue of the NTK for deep networks in Theorem 8 does not capture the data separation. Finally, a mild limitation of this work is that we require the data to be normalized so as to lie on the sphere.

Future work.The proof techniques developed here could be applied to analyze the NTK in the context of other homogeneous activation functions. One could potentially relax the homogeneity condition on the activation function, or the condition of unit norm data, by considering an integral transform on the space \(L^{2}(\mathbb{R}^{d},\mu)\) rather than \(L^{2}(\mathbb{S}^{d-1})\), where \(\mu\) denotes the standard Gaussian measure (since the weights are drawn from a Gaussian distribution). Beyond fully connected networks, conducting comparable analyses in the context of other architectures, e.g., CNNs, GNNs, or transformers, would be valuable future work.

#### Acknowledgment

This project has been supported by NSF CAREER 2145630, NSF 2212520, DFG 464109215 within SPP 2298 Theoretical Foundations of Deep Learning, and BMBF in DAAD project 57616814.

## References

* Allen-Zhu et al. (2019) Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 242-252. PMLR, 2019. URL https://proceedings.mlr.press/v97/allen-zhu19a.html.
* Arora et al. (2019a) Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 322-332. PMLR, 09-15 Jun 2019a. URL https://proceedings.mlr.press/v97/arora19a.html.
* Arora et al. (2019b) Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019b. URL https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf.
* Axler et al. (2013) Sheldon Axler, Paul Bourdon, and Ramey Wade. _Harmonic function theory_, volume 137. Springer Science & Business Media, 2013. URL https://doi.org/10.1007/978-1-4757-8137-3.
* Ball (1997) Keith Ball. An elementary introduction to modern convex geometry. _Flavors of geometry_, 31:1-58, 1997.
* Banerjee et al. (2023) Arindam Banerjee, Pedro Cisneros-Velarde, Libin Zhu, and Mikhail Belkin. Neural tangent kernel at initialization: Linear width suffices. In _The 39th Conference on Uncertainty in Artificial Intelligence_, 2023. URL https://openreview.net/forum?id=VJaoe7Rp9tZ.
* Basri et al. (2019) Ronen Basri, David W. Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural networks for learned functions of different frequencies. In _Advances in Neural Information Processing Systems 32_, pp. 4763-4772, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/5ac8bb8a7d745102a978c5f8ccdb61b8-Abstract.html.
* Basri et al. (2020) Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman. Frequency bias in neural networks for input of non-uniform density. In _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pp. 685-694. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/bsrai20a.html.
* Bietti and Bach (2021) Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=aDjoksTpXOP.
* Bombari et al. (2022) Simone Bombari, Mohammad Hossein Amani, and Marco Mondelli. Memorization and optimization in deep neural networks with minimum over-parameterization. In _Advances in Neural Information Processing Systems_, volume 35, pp. 7628-7640. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/323746f0ae2fbd8b6f500dc2d5c5f898-Paper-Conference.pdf.
* Berman et al. (2019)Benjamin Bowman and Guido Montufar. Spectral bias outside the training set for deep networks in the kernel regime. In _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=a01P1Zgb7W5.
* Cao et al. (2021) Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the spectral bias of deep learning. In _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21_, pp. 2205-2211, August 2021. URL https://doi.org/10.24963/ijcai.2021/304.
* Chizat et al. (2019) Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf.
* Cui et al. (2021) Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime. In _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=Da_EHFAcfwd.
* Du et al. (2019a) Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 1675-1685. PMLR, 2019a. URL https://proceedings.mlr.press/v97/du19c.html.
* Du et al. (2019b) Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations_, 2019b. URL https://openreview.net/forum?id=S1eK3i09YQ.
* Efthimiou and Frye (2014) Costas Efthimiou and Christopher Frye. _Spherical harmonics in p dimensions_. World Scientific, 2014. URL https://doi.org/10.1142/9134.
* Fan and Wang (2020) Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks. In _Advances in Neural Information Processing Systems_, volume 33, pp. 7710-7721. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/572201a4497b0b9f02d4f279b09ec30d-Paper.pdf.
* Geifman et al. (2020) Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen. On the similarity between the Laplace and neural tangent kernels. In _Advances in Neural Information Processing Systems_, volume 33, pp. 1451-1461. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf.
* Gradshteyn and Ryzhik (2014) Izrail Solomonovich Gradshteyn and Iosif Moiseevich Ryzhik. _Table of integrals, series, and products_. Academic press, 2014. URL https://doi.org/10.1016/C2010-0-64839-5.
* Horn and Johnson (2012) Roger A. Horn and Charles R. Johnson. _Matrix Analysis_. Cambridge University Press, 2 edition, 2012. URL https://doi.org/10.1017/CBO9780511810817.
* Jacot et al. (2018) Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf.
* Jin et al. (2022) Hui Jin, Pradeep Kr. Banerjee, and Guido Montufar. Learning curves for Gaussian process regression with power-law priors and targets. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=Ke19E-gsoB.
* Karhadkar et al. (2023) Kedar Karhadkar, Michael Murray, Hanna Tseran, and Guido Montufar. Mildly overparameterized ReLU networks have a favorable loss landscape. _arXiv:2305.19510_, 2023.
* Laurent and Massart (2000) B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. _The Annals of Statistics_, 28(5):1302-1338, 2000. URL https://doi.org/10.1214/aos/1015957395.
* Liu et al. (2019)Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf.
* Lee et al. (2020a) Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In _Advances in Neural Information Processing Systems_, volume 33, pp. 15156-15172. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/ad086f59924fffe0773f8d0ca22ea712-Paper.pdf.
* Lee et al. (2020b) Wongel Lee, Hangyeol Yu, Xavier Rival, and Hongseok Yang. On correctness of automatic differentiation for non-differentiable functions. In _Advances in Neural Information Processing Systems_, volume 33, pp. 6719-6730. Curran Associates, Inc., 2020b. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/4aaa76178f8567e05c8e8295c96171d8-Paper.pdf.
* Li (2010) Shengqiao Li. Concise formulas for the area and volume of a hyperspherical cap. _Asian Journal of Mathematics & Statistics_, 4(1):66-70, 2010.
* Li and Liang (2018) Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf.
* Liu et al. (2020) Chaoyue Liu, Libin Zhu, and Mikhail Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. In _Advances in Neural Information Processing Systems_, volume 33, pp. 15954-15964. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/b7ae8fec15b8b6c3c69ecae636d203-Paper.pdf.
* Liu et al. (2022) Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. _Applied and Computational Harmonic Analysis_, 59:85-116, 2022. URL https://www.sciencedirect.com/science/article/pii/S106352032100110X. Special Issue on Harmonic Analysis and Machine Learning.
* Montanari and Zhong (2022) Andrea Montanari and Yiqiao Zhong. The interpolation phase transition in neural networks: Memorization and generalization under lazy training. _The Annals of Statistics_, 50(5):2816-2847, 2022. URL https://doi.org/10.1214/22-AOS2211.
* Murray et al. (2023) Michael Murray, Hui Jin, Benjamin Bowman, and Guido Montufar. Characterizing the spectrum of the NTK via a power series expansion. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=Tvms8krZHyR.
* Nevai et al. (1994) Paul Nevai, Tamas Erdelyi, and Alphonse P Magnus. Generalized jacobi weights, christoffel functions, and jacobi polynomials. _SIAM Journal on Mathematical Analysis_, 25(2):602-614, 1994.
* Nguyen (2021) Quynh Nguyen. On the proof of global convergence of gradient descent for deep ReLU networks with linear widths. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pp. 8056-8062. PMLR, 2021. URL https://proceedings.mlr.press/v139/nguyen21a.html.
* Nguyen and Mondelli (2020) Quynh Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer followed by pyramidal topology. In _Advances in Neural Information Processing Systems_, volume 33, pp. 11961-11972. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/8abfe8ac9ec214d68541fcb88c0b4c3-Paper.pdf.
* Nguyen et al. (2021) Quynh Nguyen, Marco Mondelli, and Guido Montufar. Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep ReLU networks. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pp. 8119-8129. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/nguyen21g.html.
* Nguyen et al. (2021)Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. _IEEE Journal on Selected Areas in Information Theory_, 1(1):84-105, 2020. URL https://doi.org/10.1109/JSAIT.2020.2991332.
* Rubin [1999] Boris Rubin. Inversion and characterization of the hemispherical transform. _Journal d'Analyse Mathematique_, 77:105-128, 1999. URL https://doi.org/10.1007/BF02791259.
* Schur [1911] J. Schur. Bemerkungen zur Theorie der beschrankten Bilinearformen mit unendlich vielen Veranderlichen. _Journal fur die reine und angewandte Mathematik_, 140:1-28, 1911. URL http://eudml.org/doc/149352.
* Seeley [1966] Robert T Seeley. Spherical harmonics. _The American Mathematical Monthly_, 73(4P2):115-121, 1966. URL https://doi.org/10.1080/00029890.1966.11970927.
* Tropp [2012] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. _Foundations of computational mathematics_, 12:389-434, 2012. URL https://doi.org/10.1007/s10208-011-9099-z.
* Velikanov and Yarotsky [2021] Maksim Velikanov and Dmitry Yarotsky. Explicit loss asymptotics in the gradient descent training of neural networks. In _Advances in Neural Information Processing Systems_, volume 34, pp. 2570-2582. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/14faf969228fc18fcd4cfcf59437b0c97-Paper.pdf.
* Vershynin [2018] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018. URL https://doi.org/10.1017/9781108231596.
* Xie et al. [2017] Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics_, pp. 1216-1224. PMLR, 2017. URL https://proceedings.mlr.press/v54/xie17a.html.
* Xie et al. [2013] Ziqing Xie, Li-Lian Wang, and Xiaodan Zhao. On exponential convergence of Gegenbauer interpolation and spectral differentiation. _Mathematics of Computation_, 82(282):1017-1036, 2013. URL https://doi.org/10.1090/S0025-5718-2012-02645-7.
* Zou and Gu [2019] Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf.
* Zou et al. [2020] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-parameterized deep ReLU networks. _Machine learning_, 109(3):467-492, 2020. URL https://doi.org/10.1007/s10994-019-05839-6.

Background material

### Concentration bounds

In order to bound the smallest eigenvalue of the finite-width NTK in terms of the expected, or infinite width NTK, we use the following matrix Chernoff bound variant.

**Lemma 13**.: _Let \(R>0\), and let \(\bm{Z}_{1},\cdots,\bm{Z}_{m}\in\mathbb{R}^{n\times n}\) be iid symmetric random matrices such that \(0\preceq\bm{Z}_{1}\preceq R\bm{I}\) almost surely. Then_

\[\mathbb{P}\left(\lambda_{\min}\left(\frac{1}{m}\sum_{j=1}^{m}\bm{Z}_{j}\right) \leq\frac{1}{2}\lambda_{\min}\left(\mathbb{E}[\bm{Z}_{1}]\right)\right)\leq n \exp\left(-\frac{Cm\lambda_{\min}(\mathbb{E}[\bm{Z}_{1}])}{R}\right).\]

_Here \(C>0\) is a universal constant._

Proof.: By Theorem 1.1 of Tropp (2012), for all \(\delta>0\)

\[\mathbb{P}\left(\lambda_{\min}\left(\frac{1}{m}\sum_{j=1}^{m}\bm {Z}_{j}\right)\leq(1-\delta)\lambda_{\min}(\mathbb{E}[\bm{Z}_{1}])\right)\] \[=\mathbb{P}\left(\lambda_{\min}\left(\sum_{j=1}^{m}\bm{Z}_{j} \right)\leq(1-\delta)\lambda_{\min}\left(\sum_{j=1}^{m}\mathbb{E}[\bm{Z}_{j}] \right)\right)\] \[\leq n\left(\frac{e^{-\delta}}{(1-\delta)^{1-\delta}}\right)^{ \frac{1}{R}\lambda_{\min}\left(\sum_{j=1}^{m}\mathbb{E}[\bm{Z}_{j}]\right)}\] \[=n\left(\frac{e^{-\delta}}{(1-\delta)^{1-\delta}}\right)^{\frac{ 1}{R}\lambda_{\min}(\mathbb{E}[\bm{Z}_{1}])}.\]

Let \(\delta=\frac{1}{2}\) and let \(C=\frac{1}{2}\log\left(\frac{\epsilon}{2}\right)>0\). Substituting into the above bound, we obtain

\[\mathbb{P}\left(\lambda_{\min}\left(\frac{1}{m}\sum_{j=1}^{m}\bm {Z}_{j}\right)\leq\frac{1}{2}\lambda_{\min}(\mathbb{E}[\bm{Z}_{1}])\right) \leq n\left(\frac{2}{e}\right)^{\frac{m}{2R}\lambda_{\min}( \mathbb{E}[\bm{Z}_{1}])}\] \[=n\exp\left(-\frac{Cm\lambda_{\min}(\mathbb{E}[\bm{Z}_{1}])}{R} \right).\]

Some of our NTK bounds will depend on the operator norm of the input data matrix \(\bm{X}\), so it will be helpful to upper bound \(\|\bm{X}\|\) with high probability.

**Lemma 14**.: _Let \(\epsilon>0\). Let \(\bm{X}=[\bm{x}_{1},\cdots,\bm{x}_{n}]\in\mathbb{R}^{d\times n}\) be a random matrix whose columns are independent and uniformly distributed on \(\mathbb{S}^{d-1}\). Then with probability at least \(1-\epsilon\),_

\[\|\bm{X}\|^{2}\lesssim 1+\frac{n+\log\frac{1}{\epsilon}}{d}.\]

Proof.: We use a covering argument. Fix \(\bm{u}\in\mathbb{S}^{d-1}\) and \(\bm{v}\in\mathbb{S}^{n-1}\). By Lemma 2.2 of Ball (1997), for each \(i\in[n]\) and \(t\geq 0\),

\[\mathbb{P}(|\langle\bm{u},\bm{x}_{i}\rangle|\geq t)\leq 2\exp\left(-\frac{ dt^{2}}{2}\right).\]

In other words \(\|\langle\bm{u},\bm{x}_{i}\rangle\|_{\psi_{2}}\lesssim\frac{1}{\sqrt{d}}\). Then by Hoeffding's inequality, for all \(t\geq 0\)

\[\mathbb{P}(|\bm{u}^{T}\bm{X}\bm{v}|\geq t) =\mathbb{P}\left(\left|\sum_{i=1}^{n}[\bm{v}]_{i}\langle\bm{u}, \bm{x}_{i}\rangle\right|\geq t\right)\] \[\leq 2\exp\left(-C_{1}dt^{2}\right),\] (9)where \(C_{1}>0\) is a constant.

Let \(\bm{u}_{1},\cdots,\bm{u}_{M}\) be a \(\left(\frac{1}{4}\right)\)-covering of \(\mathbb{S}^{d-1}\). That is, \(\bm{u}_{1},\cdots,\bm{u}_{M}\) are a set of points in \(\mathbb{S}^{d-1}\) such that for all \(\bm{u}\in\mathbb{S}^{d-1}\), there exists \(j\in[M]\) such that \(\|\bm{u}-\bm{u}_{j}\|\leq\frac{1}{4}\). Since the \(\left(\frac{1}{4}\right)\)-covering number of \(\mathbb{S}^{d-1}\) is at most \(12^{d}\)(see Vershynin, 2018, Corollary 4.2.13), we can take \(M\leq 12^{d}\). Similarly, let \(\bm{u}_{1},\cdots,\bm{u}_{N}\) be a \(\left(\frac{1}{4}\right)\)-covering of \(\mathbb{S}^{n-1}\) with \(N\leq 12^{n}\). By applying a union bound to (9), we obtain

\[\mathbb{P}(|\bm{u}_{j}^{T}\bm{X}\bm{v}_{k}|\geq t\text{ for some }j\in[M],k \in[N])\leq 2(12^{d+n})\exp\left(-C_{1}dt^{2}\right).\]

Hence if

\[t=\sqrt{\frac{(d+n)\log 12+\log\frac{2}{\epsilon}}{d}},\]

then

\[\mathbb{P}(|\bm{u}_{j}^{T}\bm{X}\bm{v}_{k}|\leq t\text{ for all }j\in[M],k \in[N])\geq 1-\epsilon.\]

Let us condition on this event for the rest of the proof. Now suppose that \(\bm{u}\in\mathbb{S}^{d-1}\) and \(\bm{v}\in\mathbb{S}^{n-1}\). By construction there exist \(j\in[M]\) and \(k\in[N]\) such that \(\|\bm{u}-\bm{u}_{j}\|\leq\frac{1}{4}\) and \(\|\bm{v}-\bm{v}_{k}\|\leq\frac{1}{4}\). Then

\[|\bm{u}^{T}\bm{X}\bm{v}| \leq|\bm{u}_{j}^{T}\bm{X}\bm{v}_{k}|+|(\bm{u}-\bm{u}_{j})^{T}\bm{X }\bm{v}_{k}|+|\bm{u}^{T}\bm{X}(\bm{v}-\bm{v}_{k})|\] \[\leq t+\|\bm{u}-\bm{u}_{j}\|\cdot\|\bm{v}_{k}\|\cdot\|\bm{X}\|+ \|\bm{u}\|\cdot\|\bm{X}\|\cdot\|\bm{v}-\bm{v}_{k}\|\] \[\leq t+\frac{1}{4}\|\bm{X}\|+\frac{1}{4}\|\bm{X}\|\] \[=t+\frac{1}{2}\|\bm{X}\|.\]

Since this holds for all \(\bm{u}\in\mathbb{S}^{d-1}\) and \(\bm{v}\in\mathbb{S}^{n-1}\), we obtain

\[\|\bm{X}\|\leq t+\frac{1}{2}\|\bm{X}\|.\]

Rearranging yields

\[\|\bm{X}\|^{2} \leq 4t^{2}\] \[\lesssim 1+\frac{n+\log\frac{1}{\epsilon}}{d}.\]

### Spherical harmonics

Here we review some preliminaries on spherical harmonics necessary for our main results. For further details we refer the reader to Efthimiou & Frye (2014) and Axler et al. (2013, Chapter 5). Let \(L^{2}(\mathbb{S}^{d-1})\) denote the Hilbert space of real-valued, square-integrable functions on the sphere \(\mathbb{S}^{d-1}\), equipped with the inner product

\[\langle g,h\rangle=\int_{\mathbb{S}^{d-1}}g(\bm{x})h(\bm{x})\ dS(\bm{x}),\]

where \(dS\) is the uniform probability measure on \(\mathbb{S}^{d-1}\). We let \(\mathcal{C}(\mathbb{S}^{d-1})\subset L^{2}(\mathbb{S}^{d-1})\) denote the subset of functions which are continuous. We say that a function \(g:\mathbb{R}^{d}\to\mathbb{R}\) is _harmonic_ if it is twice continuously differentiable and

\[\sum_{r=1}^{d}\frac{\partial^{2}g}{\partial^{2}x_{r}}(\bm{x})=0\]

for all \(\bm{x}\in\mathbb{S}^{d-1}\). We say that a polynomial \(g:\mathbb{R}^{d}\to\mathbb{R}\) is _homogeneous_ if there exists \(r\in\mathbb{Z}_{\geq 0}\) such that

\[g(\lambda\bm{x})=\lambda^{r}g(\bm{x})\]for all \(\lambda\in\mathbb{R}\) and \(\bm{x}\in\mathbb{R}^{d}\). Let \(\mathcal{H}_{r}^{d}\) denote the vector space of degree \(r\) harmonic homogeneous polynomials on \(d\) variables, viewed as functions \(\mathbb{S}^{d-1}\to\mathbb{R}\). Each space \(\mathcal{H}_{r}^{d}\) is a finite-dimensional vector space, with

\[\dim(\mathcal{H}_{r}^{d}) =\binom{r+d-1}{d-1}-\binom{r+d-3}{d-1}\] \[=\frac{2r+d-2}{r}\binom{r+d-3}{d-2}.\]

For \(\nu\geq 0\) and \(r\in\mathbb{N}\), we define the _Gegenbauer polynomials_\(C_{r}^{\nu}\) by

\[C_{r}^{\nu}(t)=\sum_{k=0}^{\lfloor r/2\rfloor}(-1)^{k}\frac{\Gamma(r-k+\nu)}{ \Gamma(\nu)\Gamma(k+1)\Gamma(r-2k+1)}(2t)^{r-2k}.\]

There exists an orthonormal basis of \(\mathcal{H}_{r}^{d}\) consisting of functions \(Y_{r,s}^{d}\), \(1\leq s\leq\dim(\mathcal{H}_{r}^{d})\), known as _spherical harmonics_. The spherical harmonics in \(\mathcal{H}_{r}^{d}\) satisfy the addition formula

\[\sum_{s=1}^{\dim(\mathcal{H}_{r}^{d})}Y_{r,s}^{d}(\bm{x})Y_{r,s}^ {d}(\bm{x}^{\prime}) =\frac{\dim(\mathcal{H}_{r}^{d})C_{r}^{(d-2)/2}(\langle\bm{x}, \bm{x}^{\prime}\rangle)\Gamma(r+1)\Gamma(d-2)}{\Gamma(r+d-2)}\] \[=\frac{(2r+d-2)C_{r}^{(d-2)/2}(\langle\bm{x},\bm{x}^{\prime} \rangle)}{d-2}\] (10)

for all \(\bm{x},\bm{x}^{\prime}\in\mathbb{S}^{d-1}\). In particular, from the identity \(C_{r}^{\nu}(1)=\frac{\Gamma(2\nu+r)}{\Gamma(2\nu)\Gamma(r+1)}\) it follows that

\[\sum_{s=1}^{\dim(\mathcal{H}_{r}^{d})}|Y_{r,s}^{d}(\bm{x})|^{2}=\dim(\mathcal{ H}_{r}^{d}).\]

We can orthogonally decompose \(L^{2}(\mathbb{S}^{d-1})\) into a direct sum of the spaces of spherical harmonics:

\[L^{2}(\mathbb{S}^{d-1})=\bigoplus_{r=1}^{\infty}\mathcal{H}_{r}^{d}.\]

That is, the spaces \(\mathcal{H}_{r}^{d}\) are orthogonal and their linear span is dense in \(L^{2}(\mathbb{S}^{d-1})\).

**Lemma 15**.: _Let \(\delta>0\) and suppose that \(\bm{x},\bm{x}^{\prime}\in\mathbb{S}^{d-1}\) satisfy \(\|\bm{x}-\bm{x}^{\prime}\|,\|\bm{x}+\bm{x}^{\prime}\|\geq\delta\). If \(R\in\mathbb{Z}_{\geq 0}\), and \(\beta\in\{0,1\}\), then_

\[\left|\sum_{r=0}^{R}\sum_{s=1}^{\dim(\mathcal{H}_{2r+\beta}^{d})}Y_{2r+\beta,s }^{d}(\bm{x})Y_{2r+\beta,s}^{d}(\bm{x}^{\prime})\right|\lesssim\left(\frac{\| \bm{x}-\bm{x}^{\prime}\|^{2}}{2}\right)^{-(d-2)/4}\binom{2R+\beta+d-1}{d-1}^{ 1/2}.\]

Proof.: Let us define

\[P(\bm{x},\bm{x}^{\prime}):=\sum_{r=0}^{R}\sum_{s=1}^{\dim(\mathcal{H}_{2r+ \beta}^{d})}Y_{2r+\beta,s}^{d}(\bm{x})Y_{2r+\beta,s}^{d}(\bm{x}^{\prime}).\]

By the addition formula (10),

\[|P(\bm{x},\bm{x}^{\prime})| =\left|\sum_{r=0}^{R}\frac{(4r+2\beta+d-2)C_{2r+\beta}^{(d-2)/2}( \langle\bm{x},\bm{x}^{\prime}\rangle)}{d-2}\right|\] \[\lesssim\sum_{r=0}^{R}\frac{(r+d)|C_{2r+\beta}^{(d-2)/2}(\langle \bm{x},\bm{x}^{\prime}\rangle)|}{d}.\] (11)In order to bound the right hand side of the above equation, we will need a bound for the Gegenbauer polynomials \(C_{2r+\beta}^{(d-2)/2}\). By Theorem 1 of Nevai et al. (1994) (see also equation 2.8 of Xie et al. 2013), for all \(\nu\geq\frac{1}{2}\), \(r\geq 0\), and \(t\in[0,1)\),

\[(1-t^{2})^{\nu}C_{r}^{\nu}(t)^{2} \leq\frac{2e(2+\sqrt{2}\nu)}{\pi}\frac{2^{1-2\nu}\pi}{\Gamma(\nu )^{2}}\frac{\Gamma(r+2\nu)}{\Gamma(r+1)(r+\nu)}\] \[\lesssim\frac{\nu\Gamma(r+2\nu)}{2^{2\nu}(r+\nu)\Gamma(\nu)^{2} \Gamma(r+1)}.\]

Rearranging the above expression yields

\[|C_{r}^{\nu}(t)|\lesssim\frac{\nu^{1/2}\Gamma(r+2\nu)^{1/2}}{2^{\nu}(r+\nu)^{1 /2}\Gamma(\nu)\Gamma(r+1)^{1/2}(1-t^{2})^{\nu/2}}.\]

We now substitute the above bound into (11):

\[|P(\bm{x},\bm{x}^{\prime})| \lesssim\sum_{r=0}^{R}\frac{(r+d)\left(\frac{d-2}{2}\right)^{1/2} \Gamma\left(2r+\beta+d-2\right)^{1/2}}{d2^{(d-2)/2}\left(2r+\beta+\frac{d-2}{ 2}\right)^{1/2}\Gamma\left(\frac{d-2}{2}\right)\Gamma(2r+\beta+1)^{1/2}(1- \langle\bm{x},\bm{x}^{\prime}\rangle^{2})^{(d-2)/4}}\] \[\lesssim\frac{1}{(1-\langle\bm{x},\bm{x}^{\prime}\rangle^{2})^{( d-2)/4}}\sum_{r=0}^{R}\left(\frac{r+d}{d}\right)^{1/2}\frac{\Gamma(2r+\beta+d-2)^{1/2}} {2^{(d-2)/2}\Gamma\left(\frac{d-2}{2}\right)\Gamma(2r+\beta+1)^{1/2}}.\]

The expression inside the sum is increasing as a function of \(r\), so the above expression is bounded above by

\[\frac{1}{(1-\langle\bm{x},\bm{x}^{\prime}\rangle^{2})^{(d-2)/4}} \left(\frac{R+d}{d}\right)^{1/2}\frac{\Gamma(2R+\beta+d-2)^{1/2}}{2^{(d-2)/2} \Gamma\left(\frac{d-2}{2}\right)\Gamma(2R+\beta+1)^{1/2}}\] \[\lesssim\frac{1}{d^{1/2}(1-\langle\bm{x},\bm{x}^{\prime}\rangle^{ 2})^{(d-2)/4}}\frac{\Gamma(2R+\beta+d-1)^{1/2}}{2^{(d-2)/2}\Gamma\left(\frac{ d-2}{2}\right)\Gamma(2R+\beta+1)^{1/2}}.\] (12)

By Stirling's approximation,

\[2^{(d-2)/2}\Gamma\left(\frac{d-2}{2}\right) \asymp 2^{(d-2)/2}\left(\frac{d-2}{2}\right)^{(d-3)/2}e^{-(d-2)/2}\] \[=(d-2)^{(d-3)/2}e^{-(d-2)/2}\] \[\asymp d^{-1/4}(d-2)^{(d-1.5)/2}e^{-(d-2)/2}\] \[\asymp d^{-1/4}\Gamma(d-1)^{1/2}.\]

Substituting this into (12) yields

\[|P(\bm{x},\bm{x}^{\prime})| \leq\frac{1}{d^{1/4}(1-\langle\bm{x},\bm{x}^{\prime}\rangle^{2}) ^{(d-2)/4}}\frac{\Gamma(2R+\beta+d-1)^{1/2}}{\Gamma(d-1)^{1/2}\Gamma(2R+ \beta+1)^{1/2}}\] \[\asymp\frac{d^{1/4}}{(R+d)^{1/2}(1-\langle\bm{x},\bm{x}^{\prime} \rangle^{2})^{(d-2)/4}}\frac{\Gamma(2R+\beta+d)^{1/2}}{\Gamma(d)\Gamma(2R+ \beta+1)^{1/2}}\] \[=\frac{d^{1/4}}{(R+d)^{1/2}(1-\langle\bm{x},\bm{x}^{\prime} \rangle^{2})^{(d-2)/4}}\binom{2R+\beta+d-1}{d-1}^{1/2}\] \[\lesssim\frac{1}{(1-\langle\bm{x},\bm{x}^{\prime}\rangle^{2})^{(d -2)/4}}\binom{2R+\beta+d-1}{d-1}^{1/2}\]

Since \(\bm{x},\bm{x}^{\prime}\in\mathbb{S}^{d-1}\),

\[1-\langle\bm{x},\bm{x}^{\prime}\rangle^{2} =(1+\langle\bm{x},\bm{x}^{\prime}\rangle)(1-\langle\bm{x},\bm{x}^ {\prime}\rangle)\] \[=\frac{1}{4}\|\bm{x}+\bm{x}^{\prime}\|^{2}\|\bm{x}-\bm{x}^{\prime}\| ^{2}\] \[\gtrsim\frac{1}{4}\delta^{4}.\]To conclude, we rewrite

\[|P(\bm{x},\bm{x}^{\prime})|\lesssim\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/4} \binom{2R+\beta+d-1}{d-1}^{1/2}.\]

## Appendix B Preliminaries on hemisphere transforms

Let \(\mathcal{M}(\mathbb{S}^{d-1})\) denote the vector space of signed Radon measures on \(\mathbb{S}^{d-1}\). We denote the total variation of \(\mu\) by \(|\mu|\). We have a natural inclusion \(L^{2}(\mathbb{S}^{d-1})\subset\mathcal{M}(\mathbb{S}^{d-1})\) by associating a function \(g\) to a signed measure \(\mu\) defined by

\[\mu(E)=\int_{E}g(\bm{x})dS(\bm{x}).\]

If \(\mu\in\mathcal{M}(\mathbb{S}^{d-1})\) and \(g\in\mathcal{C}(\mathbb{S}^{d-1})\), we define the pairing \(\langle\mu,g\rangle\) by

\[\langle\mu,g\rangle=\int_{\mathbb{S}^{d-1}}g(\bm{x})d\mu(\bm{x}).\]

This agrees with the usual definition of the inner product on \(L^{2}(\mathbb{S}^{d-1})\) when \(\mu\in L^{2}(\mathbb{S}^{d-1})\).

Fix \(\psi\in\{\sqrt{d}\sigma,\dot{\sigma}\}\). If \(\mu\in\mathcal{M}(\mathbb{S}^{d-1})\), we define its _hemisphere transform_(Rubin, 1999)\(T_{\psi}\mu:\mathbb{S}^{d-1}\rightarrow\mathbb{R}\) by

\[(T_{\psi}\mu)(\bm{\xi})=\int_{\mathbb{S}^{d-1}}\psi(\langle\bm{\xi},\bm{x} \rangle)d\mu(\bm{x}).\]

As is the case with many integral transforms, a hemisphere transform increases the regularity of the functions it is applied to.

**Lemma 16**.: _If \(\mu\in\mathcal{M}(\mathbb{S}^{d-1})\), then \(T_{\psi}\mu\in L^{2}(\mathbb{S}^{d-1})\). If \(g\in L^{2}(\mathbb{S}^{d-1})\), then \(T_{\psi}g\in\mathcal{C}(\mathbb{S}^{d-1})\)._

Proof.: Suppose that \(\mu\in\mathcal{M}(\mathbb{S}^{d-1})\). Then

\[\int_{\mathbb{S}^{d-1}}(T_{\psi}\mu)(\bm{\xi})^{2}dS(\bm{\xi}) =\int_{\mathbb{S}^{d-1}}\left|\int_{\mathbb{S}^{d-1}}\psi(\langle \bm{\xi},\bm{x}\rangle)d\mu(\bm{x})\right|^{2}dS(\bm{\xi})\] \[\leq\int_{\mathbb{S}^{d-1}}\left|\int_{\mathbb{S}^{d-1}}\psi( \langle\bm{\xi},\bm{x}\rangle)d|\mu|(\bm{x})\right|^{2}dS(\bm{\xi})\] \[=\int_{\mathbb{S}^{d-1}}\int_{\mathbb{S}^{d-1}}\int_{\mathbb{S}^{ d-1}}\psi(\langle\bm{\xi},\bm{x}\rangle)\psi(\langle\bm{\xi},\bm{x}^{\prime} \rangle)d|\mu|(\bm{x})d|\mu|(\bm{x}^{\prime})dS(\bm{\xi})\] \[\leq\int_{\mathbb{S}^{d-1}}\int_{\mathbb{S}^{d-1}}\int_{\mathbb{S }^{d-1}}d^{2}d|\mu|(\bm{x})d|\mu|(\bm{x}^{\prime})dS(\bm{\xi})\] \[=|\mu|(\mathbb{S}^{d-1})^{2}d^{2}\] \[<\infty,\]

so \(T\mu\in L^{2}(\mathbb{S}^{d-1})\).

Now suppose that \(g\in L^{2}(\mathbb{S}^{d-1})\) and \(\psi=\dot{\sigma}\). Suppose that \(\bm{\xi},\bm{\xi}^{\prime}\in\mathbb{S}^{d-1}\), and observe that

\[dS(\{\bm{x}\in\mathbb{S}^{d-1}:\langle\bm{x},\bm{\xi}\rangle>0,\langle\bm{x}, \bm{\xi}^{\prime}\rangle\leq 0\}) =\frac{1}{2\pi}\arccos(\langle\bm{\xi},\bm{\xi}^{\prime}\rangle).\]

Similarly,

\[dS(\{\bm{x}\in\mathbb{S}^{d-1}:\langle\bm{x},\bm{\xi}\rangle\leq 0,\langle\bm{x}, \bm{\xi}^{\prime}\rangle>0\}) =\frac{1}{2\pi}\arccos(\langle\bm{\xi},\bm{\xi}^{\prime}\rangle),\]

so

\[dS(\{\bm{x}\in\mathbb{S}^{d-1}:\dot{\sigma}(\langle\bm{x},\bm{\xi}\rangle)\neq \dot{\sigma}(\langle\bm{x},\bm{\xi}^{\prime}\rangle)) =\frac{1}{\pi}\arccos(\langle\bm{\xi},\bm{\xi}^{\prime}\rangle).\]We apply this calculation to bound the distance between \(T_{\psi}g(\bm{\xi})\) and \(T_{\psi}g(\bm{\xi}^{\prime})\):

\[|T_{\psi}g(\bm{\xi})-T_{\psi}g(\bm{\xi}^{\prime})| =\left|\int_{\mathbb{S}^{d-1}}\dot{\sigma}(\langle\bm{x},\bm{\xi} \rangle)g(\bm{x})dS(\bm{x})-\int_{\mathbb{S}^{d-1}}\dot{\sigma}(\langle\bm{x}, \bm{\xi}^{\prime}\rangle)g(\bm{x})dS(\bm{x})\right|\] \[\leq\int_{\mathbb{S}^{d-1}}|\dot{\sigma}(\langle\bm{x},\bm{\xi} \rangle)-\dot{\sigma}(\langle\bm{x},\bm{\xi}^{\prime}\rangle)|g(\bm{x})dS(\bm {x})\] \[\leq\|g\|_{L^{2}}\left(\int_{\mathbb{S}^{d-1}}|\dot{\sigma}( \langle\bm{x},\bm{\xi}\rangle)-\dot{\sigma}(\langle\bm{x},\bm{\xi}^{\prime} \rangle)|^{2}dS(\bm{x})\right)^{1/2}\] \[=\|g\|_{L^{2}}\left(dS(\{\bm{x}\in\mathbb{S}^{d-1}:\dot{\sigma}( \langle\bm{x},\bm{\xi}\rangle)\neq\dot{\sigma}(\langle\bm{x},\bm{\xi}^{\prime }\rangle))\right)^{1/2}\] \[=\frac{1}{\pi}\|g\|_{L^{2}}\sqrt{\arccos(\langle\bm{\xi},\bm{\xi} ^{\prime}\rangle)}.\]

Here the third line follows from Cauchy-Schwarz. As \(\bm{\xi}\to\bm{\xi}^{\prime},\arccos(\langle\bm{\xi},\bm{\xi}^{\prime} \rangle)\to 0\) and so \(|T_{\psi}g(\bm{\xi})-T_{\psi}g(\bm{\xi}^{\prime})|\to 0\). Therefore, \(T_{\psi}g\in\mathcal{C}(\mathbb{S}^{d-1})\).

Finally suppose that \(g\in L^{2}(\mathbb{S}^{d-1})\) and \(\psi=\sqrt{d}\sigma\). For all \(\bm{\xi}\in\mathbb{S}^{d-1}\),

\[|d\sigma(\langle\bm{x},\bm{\xi}\rangle)g(\bm{x})|\leq\sqrt{d}|g(\bm{x})|\in L ^{1}(\mathbb{S}^{d-1}).\]

So by the dominated convergence theorem, for all \(\bm{\xi}^{\prime}\in\mathbb{S}^{d-1}\),

\[\lim_{\bm{\xi}\to\bm{\xi}^{\prime}}T_{\psi}g(\bm{\xi}) =\lim_{\bm{\xi}\to\bm{\xi}^{\prime}}\int_{\mathbb{S}^{d-1}}\sqrt{ d}\sigma(\langle\bm{x},\bm{\xi}\rangle)g(\bm{x})dS(\bm{x})\] \[=\int_{\mathbb{S}^{d-1}}\lim_{\bm{\xi}\to\bm{\xi}^{\prime}}\sqrt{ d}\sigma(\langle\bm{x},\bm{\xi}\rangle)g(\bm{x})dS(\bm{x})\] \[=\int_{\mathbb{S}^{d-1}}\sqrt{d}\sigma(\langle\bm{x},\bm{\xi}^{ \prime}\rangle)g(\bm{x})dS(\bm{x})\] \[=T_{\psi}g(\bm{\xi}^{\prime}).\]

Therefore \(T_{\psi}g\in\mathcal{C}(\mathbb{S}^{d-1})\). 

By the above lemma, for any \(\mu\in\mathcal{M}(\mathbb{S}^{d-1})\) and \(g\in L^{2}(\mathbb{S}^{d-1})\), the expressions \(\langle T_{\psi}\mu,g\rangle\) and \(\langle\mu,T_{\psi}g\rangle\) are well-defined and finite. In fact, they are equal to each other.

**Lemma 17**.: _Suppose that \(\mu\in\mathcal{M}(\mathbb{S}^{d-1})\) and \(g\in L^{2}(\mathbb{S}^{d-1})\). Then_

\[\langle T_{\psi}\mu,g\rangle=\langle\mu,T_{\psi}g\rangle.\]

Proof.: We compute

\[\langle T_{\psi}\mu,g\rangle =\int_{\mathbb{S}^{d-1}}(T_{\psi}\mu)(\bm{\xi})g(\bm{\xi})dS(\xi)\] \[=\int_{\mathbb{S}^{d-1}}\int_{\mathbb{S}^{d-1}}\psi(\langle\bm{ x},\bm{\xi}\rangle)g(\bm{\xi})d\mu(\bm{x})dS(\bm{\xi})\] \[=\int_{\mathbb{S}^{d-1}}\int_{\mathbb{S}^{d-1}}\psi(\langle\bm{x},\bm{\xi}\rangle)g(\bm{\xi})dS(\bm{\xi})d\mu(\bm{x})\] \[=\int_{\mathbb{S}^{d-1}}T_{\psi}g(\bm{x})d\mu(\bm{x})\] \[=\langle\mu,T_{\psi}g\rangle.\]

It remains to justify the change in order of integration in the third line. This follows from Fubini's theorem and the calculation

\[\int_{\mathbb{S}^{d-1}}\int_{\mathbb{S}^{d-1}}|\psi(\langle\bm{x},\bm{\xi}\rangle)g(\bm{\xi})|dS(\bm{\xi})d|\mu|(\bm{x}) \leq\int_{\mathbb{S}^{d-1}}\int_{\mathbb{S}^{d-1}}\sqrt{d}|g(\bm{ \xi})|dS(\bm{\xi})d|\mu|(\bm{x})\] \[=\int_{\mathbb{S}^{d-1}}\sqrt{d}\|g\|_{L^{1}}d|\mu|(\bm{x})\] \[=\sqrt{d}\|g\|_{L^{1}}|\mu|(\mathbb{S}^{d-1})\] \[<\infty,\]

where the last line follows since \(g\in L^{2}(\mathbb{S}^{d-1})\subset L^{1}(\mathbb{S}^{d-1})\).

In order to characterize how a hemisphere transform acts on \(L^{2}(\mathbb{S}^{d-1})\) and in particular on the spherical harmonics, we will use the _Funk-Hecke formula_ (see Seeley, 1966) which states that a certain class of integral operators on \(\mathbb{S}^{d-1}\) has an eigendecomposition of spherical harmonics.

**Lemma 18** (Funk-Hecke formula).: _Let \(\psi:[-1,1]\to\mathbb{R}\) be a measurable function such that_

\[\int_{-1}^{1}|\psi(t)|(1-t^{2})^{(d-3)/2}dt<\infty.\]

_Then for all \(g\in\mathcal{H}_{r}^{d}\)_

\[\int_{\mathbb{S}^{d-1}}\psi(\langle\bm{x},\bm{\xi}\rangle)g(\bm{x})dS(\bm{x})= c_{r,d}g(\bm{\xi}),\]

_where_

\[c_{r,d}=\frac{\Gamma(r+1)\Gamma(d-2)\Gamma\left(\frac{d}{2}\right)}{\sqrt{\pi }\Gamma(d-2+r)\Gamma\left(\frac{d-1}{2}\right)}\int_{-1}^{1}\psi(t)C_{r}^{(d- 2)/2}(t)(1-t^{2})^{(d-3)/2}dt.\]

We will now use the Funk-Hecke formula to compute the coefficients \(c_{r,d}\) in the cases where \(\psi=\sqrt{d}\sigma\) and \(\psi=\sigma\). In the following calculations we will use the _Legendre duplication formula_

\[\Gamma(z)\Gamma\left(z+\frac{1}{2}\right)=2^{1-2z}\sqrt{\pi}\Gamma(2z)\]

and _Euler's reflection formula_

\[\Gamma(1-z)\Gamma(z)=\frac{\pi}{\sin\pi z}.\]

**Lemma 19**.: _For all \(d\geq 3\) and \(r\geq 0\),_

\[\int_{0}^{1}C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}dt=\frac{\sqrt{\pi}\Gamma(d+ r-2)\Gamma\left(\frac{d-1}{2}\right)}{2\Gamma(d-2)\Gamma(r+1)\Gamma\left(1- \frac{r}{2}\right)\Gamma\left(\frac{d+r}{2}\right)}.\]

_and_

\[\int_{0}^{1}tC_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}dt=\frac{\sqrt{\pi}\Gamma(d+ r-2)\Gamma\left(\frac{d-1}{2}\right)}{4\Gamma(d-2)\Gamma(r+1)\Gamma\left(\frac{3- r}{2}\right)\Gamma\left(\frac{d+r+1}{2}\right)}.\]

Proof.: We apply the following identity (see Gradshteyn & Ryzhik, 2014, Equation 7.311.2):

\[\int_{0}^{1}t^{r+2\rho}C_{r}^{\nu}(t)(1-t^{2})^{\nu-1/2}dt=\frac{\Gamma(2\nu+r )\Gamma(2\rho+r+1)\Gamma\left(\nu+\frac{1}{2}\right)\Gamma\left(\rho+\frac{1} {2}\right)}{2^{r+1}\Gamma(2\nu)\Gamma(2\rho+1)r\Gamma(r+\nu+\rho+1)}.\]

By the Legendre duplication formula, we have

\[\Gamma\left(\rho+\frac{1}{2}\right)\Gamma(\rho+1)=2^{-2\rho}\sqrt{\pi}\Gamma( 2\rho+1)\]

so we can rewrite the above equation as

\[\int_{0}^{1}t^{r+2\rho}C_{r}^{\nu}(t)(1-t^{2})^{\nu-1/2}dt=\frac{\sqrt{\pi} \Gamma(2\nu+r)\Gamma(2\rho+r+1)\Gamma\left(\nu+\frac{1}{2}\right)}{2^{2\rho+r+ 1}\Gamma(2\nu)\Gamma(\rho+1)\Gamma(r+1)\Gamma(r+\nu+\rho+1)}.\] (13)

Substituting \(\rho=-r/2\) and \(\nu=(d-2)/2\) into (13) yields

\[\int_{0}^{1}C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}dt=\frac{\sqrt{\pi}\Gamma(d+ r-2)\Gamma\left(\frac{d-1}{2}\right)}{2\Gamma(d-2)\Gamma\left(1-\frac{r}{2} \right)\Gamma(r+1)\Gamma\left(\frac{d+r}{2}\right)},\]

which establishes the first identity of the claim.

Substituting \(\rho=(1-r)/2\) and \(\nu=(d-2)/2\) into (13) yields

\[\int_{0}^{1}C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}dt=\frac{\sqrt{\pi}\Gamma(d+ r-2)\Gamma\left(\frac{d-1}{2}\right)}{4\Gamma(d-2)\Gamma\left(\frac{3-r}{2} \right)\Gamma(r+1)\Gamma\left(\frac{d+r+1}{2}\right)},\]

which establishes the second identity of the claim.

**Lemma 20**.: _Suppose that \(g\in\mathcal{H}_{r}^{d}\) and \(d\geq 3\). Then for all \(r\geq 0\), \(T_{\sigma}g=c_{r,d}g\), where_

\[c_{r,d}=\frac{\Gamma\left(\frac{d}{2}\right)}{2\Gamma\left(1-\frac{r}{2}\right) \Gamma\left(\frac{r}{2}+\frac{d}{2}\right)}.\]

_Moreover, if \(0\leq r\leq R\), then_

\[|c_{2r+1,d}|\geq\frac{\Gamma\left(\frac{d}{2}\right)\Gamma\left(\frac{2R+1}{2} \right)}{2\pi\Gamma\left(\frac{d+2R+1}{2}\right)}.\]

Proof.: Let \(g\in\mathcal{H}_{r}^{d}\). By Lemma 18,

\[T_{\sigma}g=c_{r,d}g,\]

where

\[c_{r,d} =\frac{\Gamma(r+1)\Gamma(d-2)\Gamma\left(\frac{d}{2}\right)}{ \sqrt{\pi}\Gamma(d-2+r)\Gamma\left(\frac{d-1}{2}\right)}\int_{-1}^{1}\dot{ \sigma}(t)C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}dt\] \[=\frac{\Gamma(r+1)\Gamma(d-2)\Gamma\left(\frac{d}{2}\right)}{ \sqrt{\pi}\Gamma(d-2+r)\Gamma\left(\frac{d-1}{2}\right)}\int_{0}^{1}C_{r}^{(d -2)/2}(t)(1-t^{2})^{(d-3)/2}dt.\]

By Lemma 19, this is equal to

\[\frac{\Gamma(r+1)\Gamma(d-2)\Gamma\left(\frac{d}{2}\right)}{\sqrt{\pi}\Gamma (d-2+r)\Gamma\left(\frac{d-1}{2}\right)}\cdot\frac{\sqrt{\pi}\Gamma(d+r-2) \Gamma\left(\frac{d-1}{2}\right)}{2\Gamma(d-2)\Gamma(r+1)\Gamma\left(1-\frac{ r}{2}\right)\Gamma\left(\frac{d+r}{2}\right)}=\frac{\Gamma\left(\frac{d}{2} \right)}{2\Gamma\left(1-\frac{r}{2}\right)\Gamma\left(\frac{d+r}{2}\right)}\]

as claimed.

Now we proceed with the second statement. We claim that whenever \(0\leq r\leq R\),

\[|c_{2R+1,d}|\leq|c_{2r+1,d}|.\]

We prove this by induction on \(R\). For the base case \(R=r\), the claim trivially holds. Now suppose that the claim holds for some \(R\geq r\). Then

\[|c_{2(R+1)+1,d}| =\left|\frac{\Gamma\left(\frac{d}{2}\right)}{2\Gamma\left(1- \frac{2R+3}{2}\right)\Gamma\left(\frac{2R+3}{2}+\frac{d}{2}\right)}\right|\] \[=\left|\frac{\left(-\frac{2R+1}{2}\right)\Gamma\left(\frac{d}{2} \right)}{2\Gamma\left(1-\frac{2R+1}{2}\right)\left(\frac{2R+1}{2}+\frac{d}{2} \right)\Gamma\left(\frac{2R+1}{2}+\frac{d}{2}\right)}\right|\] \[=|c_{2R+1,d}|\frac{2R+1}{2R+1+d}\] \[\leq|c_{2R+1,d}|\] \[\leq|c_{2r+1,d}|.\]

Hence by induction \(|c_{2R+1,d}|\leq|c_{2r+1,d}|\) for all \(0\leq r\leq R\). Now suppose that \(0\leq r\leq R\). By Euler's reflection formula,

\[c_{2R+1,d} =\frac{\Gamma\left(\frac{d}{2}\right)}{2\Gamma\left(1-\frac{2R+1} {2}\right)\Gamma\left(\frac{2R+1}{2}+\frac{d}{2}\right)}\] \[=\frac{\Gamma\left(\frac{d}{2}\right)\sin\left(\pi\frac{2R+1}{2} \right)\Gamma\left(\frac{2R+1}{2}\right)}{2\pi\Gamma\left(\frac{2R+1}{2}+ \frac{d}{2}\right)}\] \[=\frac{\Gamma\left(\frac{d}{2}\right)(-1)^{R}\Gamma\left(\frac{2 R+1}{2}\right)}{2\pi\Gamma\left(\frac{2R+1}{2}+\frac{d}{2}\right)}\]

so

\[|c_{2r+1,d}| \geq|c_{2R+1,d}|\] \[=\frac{\Gamma\left(\frac{d}{2}\right)\Gamma\left(\frac{2R+1}{2} \right)}{2\pi\Gamma\left(\frac{d+2R+1}{2}\right)}.\]

**Lemma 21**.: _Suppose that \(g\in\mathcal{H}_{r}^{d}\) and \(d\geq 3\). Then \(T_{\sqrt{d}\sigma}g=c_{r,d}g\), where_

\[c_{r,d}=\frac{\sqrt{d}\Gamma\left(\frac{d}{2}\right)}{4\Gamma\left(\frac{3-r}{2 }\right)\Gamma\left(\frac{d+r+1}{2}\right)}.\]

_Moreover, if \(0\leq r\leq R\), then_

\[|c_{2r,d}|\geq\frac{\sqrt{d}\Gamma\left(\frac{d}{2}\right)\Gamma\left(\frac{2R -1}{2}\right)}{4\pi\Gamma\left(\frac{d+2R+1}{2}\right)}.\]

Proof.: The proof is analogous to that of Lemma 20. Let \(g\in\mathcal{H}_{r}^{d}\). By Lemma 18,

\[T_{\sqrt{d}\sigma}g=c_{r,d}g,\]

where

\[c_{r,d} =\frac{\Gamma(r+1)\Gamma(d-2)\Gamma\left(\frac{d}{2}\right)}{ \sqrt{\pi}\Gamma(d-2+r)\Gamma\left(\frac{d-1}{2}\right)}\int_{-1}^{1}\sqrt{d} \sigma(t)C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}dt\] \[=\frac{\sqrt{d}\Gamma(r+1)\Gamma(d-2)\Gamma\left(\frac{d}{2} \right)}{\sqrt{\pi}\Gamma(d-2+r)\Gamma\left(\frac{d-1}{2}\right)}\int_{0}^{1} tC_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}dt.\]

By Lemma 19, this is equal to

\[\frac{\sqrt{d}\Gamma(r+1)\Gamma(d-2)\Gamma\left(\frac{d}{2}\right)}{\sqrt{\pi }\Gamma(d-2+r)\Gamma\left(\frac{d-1}{2}\right)}\cdot\frac{\sqrt{\pi}\Gamma(d +r-2)\Gamma\left(\frac{d-1}{2}\right)}{4\Gamma(d-2)\Gamma(r+1)\Gamma\left( \frac{3-r}{2}\right)\Gamma\left(\frac{d+r+1}{2}\right)}=\frac{\sqrt{d}\Gamma \left(\frac{d}{2}\right)}{4\Gamma\left(\frac{3-r}{2}\right)\Gamma\left(\frac {d+r+1}{2}\right)}\]

as claimed.

We claim that whenever \(0\leq r\leq R\),

\[|c_{2R,d}|\leq|c_{2r,d}|.\]

We prove this by induction on \(R\). For the base case \(R=r\), the claim trivially holds. Now suppose that the claim holds for some \(R\geq r\). Then

\[|c_{2(R+1)}| =\left|\frac{\sqrt{d}\Gamma\left(\frac{d}{2}\right)}{4\Gamma \left(\frac{1-2R}{2}\right)\Gamma\left(\frac{d+2R+3}{2}\right)}\right|\] \[=\left|\frac{\left(\frac{1-2R}{2}\right)\sqrt{d}\Gamma\left( \frac{d}{2}\right)}{4\Gamma\left(\frac{1-2R}{2}\right)\left(\frac{d+2R+1}{2} \right)\Gamma\left(\frac{d+2R+1}{2}\right)}\right|\] \[=c_{2R}\frac{|2R-1|}{d+2R+1}\] \[\leq c_{2R}.\]

Hence by induction \(|c_{2R}|\leq|c_{2r}|\) for all \(0\leq r\leq R\). Now suppose that \(0\leq r\leq R\). By Euler's reflection formula,

\[c_{2R,d} =\frac{\sqrt{d}\Gamma\left(\frac{d}{2}\right)}{4\Gamma\left(\frac {3-2R}{2}\right)\Gamma\left(\frac{d+2R+1}{2}\right)}\] \[=\frac{\sqrt{d}\Gamma\left(\frac{d}{2}\right)\sin\left(\pi\frac {2R-1}{2}\right)\Gamma\left(\frac{2R-1}{2}\right)}{4\pi\Gamma\left(\frac{d+2R+ 1}{2}\right)}\] \[=\frac{(-1)^{R+1}\sqrt{d}\Gamma\left(\frac{d}{2}\right)\Gamma \left(\frac{2R-1}{2}\right)}{4\pi\Gamma\left(\frac{d+2R+1}{2}\right)}\]

so

\[|c_{2r,d}| \geq|c_{2R,d}|\] \[=\frac{\sqrt{d}\Gamma\left(\frac{d}{2}\right)\Gamma\left(\frac{2R -1}{2}\right)}{4\pi\Gamma\left(\frac{d+2R+1}{2}\right)}.\]Proofs for Section 3

First we observe the connection between the smallest eigenvalue of the expected NTK when the weights are drawn uniformly over the sphere versus as Gaussian.

**Lemma 22**.: _If \(\bm{X}\in\mathbb{R}^{d_{0}\times n}\), then_

Proof.: Since the distribution of \(\bm{w}\) is rotationally invariant, we can decompose \(\bm{w}=\alpha\bm{u}\), where \(\alpha=\|\bm{w}\|\), \(\bm{u}\) is uniformly distributed on \(\mathbb{S}^{d_{0}-1}\), and \(\alpha\) and \(\bm{u}\) are independent. Then

\[\lambda_{\min}\left(\mathbb{E}_{\bm{w}\sim\mathcal{N}(\bm{0}_{d}, \bm{I}_{d})}\left[\sigma\left(\bm{X}^{T}\bm{w}\right)\sigma\left(\bm{w}^{T}\bm {X}\right)\right]\right) =\lambda_{\min}\left(\mathbb{E}\left[\sigma\left(\bm{X}^{T}\bm{w }\right)\sigma\left(\bm{w}^{T}\bm{X}\right)\right]\right)\] \[=\lambda_{\min}\left(\mathbb{E}\left[\alpha^{2}\sigma\left(\bm{X }^{T}\bm{u}\right)\sigma\left(\bm{u}^{T}\bm{X}\right)\right]\right)\] \[=d_{0}\lambda_{\min}\left(\mathbb{E}\left[\sigma\left(\bm{X}^{T }\bm{u}\right)\sigma\left(\bm{u}^{T}\bm{X}\right)\right]\right).\]

Lemma 22 is useful in that studying the expected NTK in the shallow setting for uniform weights here will prove more convenient than working directly with Gaussian weights.

### Proof of Lemma 3

**Lemma 3**.: _Suppose that \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\). Let_

\[\lambda_{1}=\lambda_{\min}\left(\mathbb{E}_{\bm{u}\sim U(\mathbb{S}^{d-1})} \left[\dot{\sigma}\left(\bm{X}^{T}\bm{u}\right)\dot{\sigma}\left(\bm{u}^{T}\bm {X}\right)\right]\right).\]

_If \(\lambda_{1}>0\) and \(d_{1}\gtrsim\lambda_{1}^{-1}\|\bm{X}\|^{2}\log\frac{n}{\epsilon}\), then with probability at least \(1-\epsilon\)_

\[\lambda_{\min}(\bm{K}_{1})\gtrsim\lambda_{1}.\]

Proof.: By the scale-invariance of \(\dot{\sigma}\),

\[\lambda_{1}=\lambda_{\min}\left(\mathbb{E}_{\bm{u}\sim\mathcal{N}(\bm{0}_{d}, \bm{I}_{d})}\left[\dot{\sigma}\left(\bm{X}^{T}\bm{u}\right)\dot{\sigma}\left( \bm{u}^{T}\bm{X}\right)\right]\right).\]

For each \(i\in[n]\) and \(j\in[d_{1}]\),

\[\nabla_{\bm{w}_{j}}f(\bm{x}_{i})=\frac{1}{\sqrt{d_{1}}}v_{j}\dot{\sigma}\left( \left\langle\bm{w}_{j}^{T},\bm{x}_{i}\right\rangle\right)\bm{x}_{i}\]

and therefore

\[\bm{K}_{1}=\frac{1}{d_{1}}\sum_{j=1}^{d_{1}}\bm{Z}_{j},\]

where

\[\bm{Z}_{j}=v_{j}^{2}\left(\dot{\sigma}\left(\bm{X}^{T}\bm{w}_{j}\right)\dot{ \sigma}\left(\bm{w}_{j}^{T}\bm{X}\right)\right)\odot\left(\bm{X}^{T}\bm{X} \right).\]

For each \(j\in[d_{1}]\), let \(\xi_{j}\in\{0,1\}\) be a random variable taking value 1 if \(|v_{j}|\leq 1\) and taking value 0 otherwise. Since \(v_{j}\) is a standard Gaussian there exists a universal constant \(C_{1}>0\) with \(\mathbb{E}[\xi_{j}v_{j}]=C_{1}\) for all \(j\). We also define \(\bm{Z}_{j}^{\prime}=\xi_{j}\bm{Z}_{j}\). Note that \(\bm{Z}_{j}^{\prime}\succeq\bm{0}\), and by the inequality \(\lambda_{\max}(\bm{A}\odot\bm{B})\leq\max_{i}[\bm{A}]_{ii}\lambda_{\max}(\bm{B})\),

\[\|\bm{Z}_{j}^{\prime}\| =\left\|\xi_{j}v_{j}^{2}\left(\dot{\sigma}\left(\bm{X}^{T}\bm{w} _{j}\right)\dot{\sigma}\left(\bm{w}_{j}\bm{X}\right)\right)\odot\left(\bm{X}^ {T}\bm{X}\right)\right\|\] \[\leq\max_{i\in[n]}\left|\left(\xi_{j}v_{j}^{2}\left[\dot{\sigma} \left(\bm{X}^{T}\bm{w}_{j}\right)\dot{\sigma}\left(\bm{w}_{j}^{T}\bm{X}\right) \right)\right]_{ii}\right|\cdot\|\bm{X}^{T}\bm{X}\|\] \[=\max_{i\in[n]}\left|\xi_{j}v_{j}^{2}\dot{\sigma}\left(\bm{w}_{j} ^{T}\bm{x}_{i}\right)^{2}\right|\cdot\|\bm{X}\|^{2}\] \[\leq\|\bm{X}\|^{2}.\]Furthermore by the inequality \(\lambda_{\min}(\bm{A}\odot\bm{B})\geq\min_{i}[\bm{A}]_{ii}\lambda_{\min}(\bm{B})\),

\[\lambda_{\min}\left(\mathbb{E}[\bm{Z}_{j}^{\prime}]\right) =\lambda_{\min}\left(\mathbb{E}\left[\xi_{j}v_{j}^{2}\left(\dot{ \sigma}\left(\bm{X}^{T}\bm{w}_{j}\right)\dot{\sigma}\left(\bm{w}_{j}^{T}\bm{X} \right)\right)\right]\odot\left(\bm{X}^{T}\bm{X}\right)\right)\] \[\geq\lambda_{\min}\left(\mathbb{E}\left[\xi_{j}v_{j}^{2}\left( \dot{\sigma}\left(\bm{X}^{T}\bm{w}_{j}\right)\dot{\sigma}\left(\bm{w}_{j}^{T} \bm{X}\right)\right)\right]\right)\min_{i\in[n]}\left|\left(\bm{X}^{T}\bm{X} \right)_{ii}\right|\] \[=\lambda_{\min}\left(\mathbb{E}\left[\xi_{j}v_{j}^{2}\right] \mathbb{E}\left[\left(\dot{\sigma}\left(\bm{X}^{T}\bm{w}_{j}\right)\dot{ \sigma}\left(\bm{w}_{j}^{T}\bm{X}\right)\right)\right]\right)\min_{i\in[n]} \|\bm{x}_{i}\|^{2}\] \[=C_{1}\lambda_{\min}\left(\mathbb{E}\left[\left(\dot{\sigma} \left(\bm{X}^{T}\bm{w}_{j}\right)\dot{\sigma}\left(\bm{w}_{j}^{T}\bm{X}\right) \right)\right]\right)\] \[=C_{1}\lambda_{1}.\]

So by Lemma 13, for all \(t\geq 0\)

\[\mathbb{P}\left(\lambda_{\min}\left(\frac{1}{d_{1}}\sum_{j=1}^{d_{ 1}}\bm{Z}_{j}^{\prime}\right)\leq C_{1}\lambda_{1}\right) \leq\mathbb{P}\left(\lambda_{\min}\left(\frac{1}{d_{1}}\sum_{j=1} ^{d_{1}}\bm{Z}_{j}^{\prime}\right)\leq\mathbb{E}[\bm{Z}_{1}^{\prime}]\right)\] \[\leq n\exp\left(-\frac{C_{2}d_{1}\lambda_{1}}{\|\bm{X}\|^{2}}\right)\]

where \(C_{2}>0\) is a constant. Since \(\bm{Z}_{j}\succeq\bm{Z}_{j}^{\prime}\) for all \(j\in[d_{1}]\), if \(d_{1}\geq\frac{1}{C_{2}\lambda_{1}}\|\bm{X}\|^{2}\log\left(\frac{n}{\epsilon}\right)\), then

\[\mathbb{P}\left(\lambda_{\min}\left(\frac{1}{d_{1}}\sum_{j=1}^{d_ {1}}\bm{Z}_{j}\right)\leq C_{1}\lambda_{1}\right) \leq n\exp\left(-\frac{C_{2}d_{1}\lambda_{1}}{\|\bm{X}\|^{2}}\right)\] \[\leq\epsilon.\]

### Proof of Lemma 4

**Lemma 23**.: _Suppose that \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d_{0}-1}\). Let_

\[\lambda_{2}=d_{0}\lambda_{\min}\left(\mathbb{E}_{\bm{u}\sim U(\mathbb{S}^{d_{0 }-1})}\left[\sigma(\bm{X}^{T}\bm{u})\sigma(\bm{u}^{T}\bm{X})\right]\right).\]

_If \(\lambda_{2}>0\) and \(d_{1}\gtrsim\frac{n}{\lambda_{2}}\log\left(\frac{n}{\lambda_{2}}\right)\log \left(\frac{n}{\epsilon}\right)\), then with probability at least \(1-\epsilon\), \(\lambda_{\min}(\bm{K}_{2})\geq\frac{\lambda_{2}}{4}\)._

Proof.: Note that by Lemma 22,

\[\lambda_{2}=\lambda_{\min}\left(\mathbb{E}_{\bm{u}\sim\mathcal{N}(\bm{0}_{d}, \bm{I}_{d})}\left[\sigma\left(\bm{X}^{T}\bm{w}\right)\sigma\left(\bm{w}^{T} \bm{X}\right)\right]\right).\]

For each \(i\in[n]\) and \(j\in[d_{1}]\),

\[\nabla_{v_{j}}f(\bm{x}_{i})=\frac{1}{\sqrt{d_{1}}}\sigma(\bm{w}_{j}^{T}\bm{x}_ {i})\]

and therefore

\[\bm{K}_{2}=\frac{1}{d_{1}}\sum_{j=1}^{d_{1}}\bm{Z}_{j},\]

where

\[\bm{Z}_{j}=\sigma\left(\bm{X}^{T}\bm{w}_{j}\right)\sigma\left(\bm{w}_{j}^{T} \bm{X}\right).\]

By Vershynin (2018, Theorem 6.3.2), for each \(j\in[d_{1}]\)

\[\left|\left|\left\|\bm{X}^{T}\bm{w}_{j}\right|\right|\right|_{\psi _{2}} \lesssim\left|\left|\left|\bm{X}^{T}\bm{w}_{j}\right|\right|-\left| \bm{X}^{T}\right|_{F}\right|_{\psi_{2}}+\|\bm{X}^{T}\|_{F}\] \[\lesssim\|\bm{X}^{T}\|+\|\bm{X}^{T}\|_{F}\] \[\lesssim\|\bm{X}^{T}\|_{F}\] \[=\|\bm{X}\|_{F}\] \[=\sqrt{n}.\]So by Hoeffding's inequality, for all \(t\geq 0\)

\[\mathbb{P}\left(\left\|\bm{X}^{T}\bm{w}_{j}\right\|^{2}\geq t\right)=\mathbb{P} \left(\left\|\bm{X}^{T}\bm{w}_{j}\right\|\geq\sqrt{t}\right)\leq 2\exp\left(- \frac{C_{1}t}{n}\right)\] (14)

for some constant \(C_{1}>0\). Let \(s=\frac{n}{C_{1}}\log\frac{4n}{\lambda_{2}C_{1}}\). For each \(j\in[d_{1}]\) let \(\xi_{j}\in\{0,1\}\) be a random variable taking value 1 if \(\|\bm{X}^{T}\bm{w}_{j}\|^{2}\leq s\) and taking value 0 otherwise. Let \(\bm{Z}^{\prime}_{j}=\xi_{j}\bm{Z}_{j}\). For each \(j\in[m]\), \(\bm{Z}^{\prime}_{j}\succeq 0\), and

\[\left\|\bm{Z}^{\prime}_{j}\right\| =\left\|\xi_{j}\sigma\left(\bm{X}^{T}\bm{w}_{j}\right)\sigma\left( \bm{w}^{T}_{j}\bm{X}\right)\right\|\] \[=\left\|\xi_{j}\sigma\left(\bm{X}^{T}\bm{w}_{j}\right)\right\|^{2}\] \[\leq s.\]

Moreover,

\[\left\|\mathbb{E}[\bm{Z}_{j}]-\mathbb{E}[\bm{Z}^{\prime}_{j}]\right\| =\left\|\mathbb{E}\left[(1-\xi_{j})\sigma\left(\bm{X}^{T}\bm{w}_{ j}\right)\sigma\left(\bm{w}^{T}_{j}\bm{X}\right)\right]\right\|\] \[\leq\mathbb{E}\left[(1-\xi_{j})\left\|\sigma\left(\bm{X}^{T}\bm{ w}_{j}\right)\sigma\left(\bm{w}^{T}_{j}\bm{X}\right)\right\|\right]\] \[=\mathbb{E}\left[(1-\xi_{j})\left\|\sigma\left(\bm{X}^{T}\bm{w}_{ j}\right)\right\|^{2}\right]\] \[=\frac{1}{2}\mathbb{E}\left[(1-\xi_{j})\left\|\bm{X}^{T}\bm{w}_{ j}\right\|^{2}\right]\] \[=\frac{1}{2}\int_{s}^{\infty}\mathbb{P}\left(\left\|\bm{X}^{T}\bm {w}_{j}\right\|^{2}\geq t\right)dt\] \[\leq 2\int_{s}^{\infty}\exp\left(-\frac{C_{1}t}{n}\right)dt\] \[=\frac{2n}{C_{1}}\exp\left(-\frac{C_{1}s}{n}\right)\] \[=\frac{\lambda_{2}}{2}.\]

Here we used (14) in line 6. By Weyl's inequality,

\[\lambda_{\min}(\mathbb{E}[\bm{Z}^{\prime}_{j}])\geq\lambda_{\min}(\mathbb{E}[ \bm{Z}_{j}])-\left\|\mathbb{E}[\bm{Z}_{j}]-\mathbb{E}[\bm{Z}^{\prime}_{j}] \right\|=\lambda_{2}-\frac{\lambda_{2}}{2}=\frac{\lambda_{2}}{2}.\]

By Lemma 13,

\[\mathbb{P}\left(\lambda_{\min}\left(\frac{1}{d_{1}}\sum_{j=1}^{d_ {1}}\bm{Z}^{\prime}_{j}\right)\leq\frac{\lambda_{2}}{4}\right) \leq\mathbb{P}\left(\lambda_{\min}\left(\frac{1}{m}\sum_{j=1}^{m }\bm{Z}^{\prime}_{j}\right)\leq\frac{1}{2}\lambda_{\min}(\mathbb{E}[\bm{Z}^{ \prime}_{1}])\right)\] \[\leq n\exp\left(-\frac{C_{2}d_{1}\lambda_{\min}(\mathbb{E}[\bm{Z }^{\prime}_{1}])}{s}\right)\] \[\leq n\exp\left(\frac{-C_{2}d_{1}\lambda_{2}}{2s}\right).\]

Since \(\bm{Z}^{\prime}_{j}\preceq\bm{Z}_{j}\) for all \(j\), for \(d_{1}\geq\frac{2s}{C_{2}\lambda_{2}}\log\frac{n}{\epsilon}\) this implies

\[\mathbb{P}\left(\lambda_{\min}\left(\frac{1}{d_{1}}\sum_{j=1}^{d _{1}}\bm{Z}_{j}\right)\leq\frac{\lambda_{2}}{4}\right) \leq n\exp\left(-\frac{C_{2}d_{1}\lambda_{2}}{2s}\right)\] \[\leq\epsilon.\]

In other words,

\[\mathbb{P}\left(\lambda_{\min}(\bm{K}_{2})\geq\frac{\lambda_{2}}{4}\right)\geq 1 -\epsilon.\]

### Proof of Lemma 5

**Lemma 5**.: _Fix \(\bm{X}\in\mathbb{R}^{d\times n}\) and \(\psi\in\{\sqrt{d}\sigma,\dot{\sigma}\}\). For all \(\bm{z}\in\mathbb{R}^{n}\), \(\langle\bm{K}_{\psi}^{\infty}\bm{z},\bm{z}\rangle=\|T_{\psi}\mu_{\bm{z}}\|^{2}\). Moreover,_

\[\lambda_{\min}(\bm{K}_{\psi}^{\infty})=\inf_{\|\bm{z}\|=1}\|T_{\psi}\mu_{\bm{z} }\|^{2}.\]

Proof.: We compute

\[\langle\bm{K}_{\psi}^{\infty}\bm{z},\bm{z}\rangle =\mathbb{E}_{\bm{w}\sim U(\mathbb{S}^{d-1})}\left[\left|\psi\left( \bm{w}^{T}\bm{X}\right)\bm{z}\right|^{2}\right]\] \[=\int_{\mathbb{S}^{d-1}}\left|\psi\left(\bm{w}^{T}\bm{X}\right) \bm{z}\right|^{2}dS(\bm{w})\] \[=\int_{\mathbb{S}^{d-1}}\left|\sum_{i=1}^{n}\psi(\langle\bm{w}, \bm{x}_{i}\rangle)z_{i}\right|^{2}dS(\bm{w})\] \[=\int_{\mathbb{S}^{d-1}}\left|\int_{\mathbb{S}^{d-1}}\psi( \langle\bm{w},\bm{x}\rangle)d\mu_{\bm{z}}(\bm{x})\right|^{2}dS(\bm{w})\] \[=\int_{\mathbb{S}^{d-1}}\left|T_{\psi}\mu_{\bm{z}}(\bm{w})\right| ^{2}dS(\bm{w})\] \[=\left\|T_{\psi}\mu_{\bm{z}}\right\|^{2}\]

which establishes the first part of the result. The second part of the result follows immediately by writing

\[\lambda_{\min}(\bm{K}_{\psi}^{\infty})=\inf_{\|\bm{z}\|=1}\langle\bm{K}_{\psi }^{\infty}\bm{z},\bm{z}\rangle=\inf_{\|\bm{z}\|=1}\|T_{\psi}\mu_{\bm{z}}\|^{2}.\]

### Proof of Lemma 6

**Lemma 6**.: _Suppose \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\) are \(\delta\)-separated. Suppose that \(\beta\in\{0,1\}\) and that \(R\in\mathbb{Z}_{\geq 0}\) are such that \(N:=\sum_{r=0}^{R}\dim(\mathcal{H}_{2r+\beta}^{d})\) satisfies \(N\geq C\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/2}\) where \(C>0\) is a universal constant. Let \(g_{1},\cdots,g_{N}\) be spherical harmonics which form an orthonormal basis of \(\bigoplus_{r=0}^{R}\mathcal{H}_{2r+\beta}^{d}\). If \(\bm{D}\in\mathbb{R}^{N\times n}\) is defined as \(\bm{D}_{ai}=g_{a}(\bm{x}_{i})\) then \(\sigma_{\min}(\bm{D})\geq\sqrt{\frac{N}{2}}\)._

Proof.: Note that

\[N=\sum_{r=0}^{R}\left(\binom{2r+\beta+d-1}{d-1}-\binom{2r+\beta+d-3}{d-1} \right)=\binom{2R+\beta+d-1}{d-1}.\]

Let us write \(\bm{D}=[\bm{d}_{1},\cdots,\bm{d}_{n}]\). Fix \(i,k\in[n]\) with \(i\neq k\). By the addition formula (10),

\[\|\bm{d}_{i}\|^{2} =\sum_{a=1}^{N}g_{a}(\bm{x}_{i})^{2}\] \[=\sum_{r=0}^{R}\sum_{s=1}^{\dim(\mathcal{H}_{2r+\beta}^{d})}Y_{r, s}^{d}(\bm{x}_{i})^{2}\] \[=\sum_{r=0}^{R}\dim(\mathcal{H}_{2r+\beta}^{d})\] \[=N.\]By Lemma 15 and \(\delta\)-separation, there exists a constant \(C>0\) such that

\[|\langle\bm{d}_{i},\bm{d}_{k}\rangle| =\left|\sum_{a=1}^{N}g_{a}(\bm{x}_{i})g_{a}(\bm{x}_{k})\right|\] \[\leq C\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/4}\binom{2R+\beta+ d-1}{d-1}^{1/2}\] \[=CN^{1/2}\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/4}.\]

Suppose that

\[N\geq 2C^{2}\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/2}.\]

Observe that \(\sigma_{\min}(\bm{D})\) is the square root of the minimum eigenvalue of \(\bm{D}^{T}\bm{D}\). By the Gershgorin circle theorem, the minimum eigenvalue of \(\bm{D}^{T}\bm{D}\) is at least

\[\min_{i\in[n]}\left(|(\bm{D}^{T}\bm{D})_{ii}|-\sum_{k\neq i}|\bm{D }^{T}\bm{D}|_{ik}\right) =\min_{i\in[n]}\left(\|\bm{d}_{i}\|^{2}-\sum_{k\neq i}|\langle\bm{ d}_{i},\bm{d}_{k}\rangle|\right)\] \[\geq\frac{N}{2}.\]

The result follows. 

### Proof of Lemma 7

**Lemma 24**.: _Let \(\epsilon\in(0,1)\) and let \(\delta>0\). Suppose that \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\) form a \(\delta\)-separated dataset. Let \(R\in\mathbb{N}\) be such that_

\[\binom{2R+d-1}{d-1}\geq C\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/2}\]

_where \(C>0\) is a universal constant. Then_

\[\|T_{\psi}\mu_{\bm{z}}\|^{2}\gtrsim\begin{cases}(d+R)^{1/2}d^{-1/2}R^{-3/2}& \text{if }\psi=\dot{\sigma}\\ (d+R)^{-1/2}d^{1/2}R^{-3/2}&\text{if }\psi=\sqrt{d}\sigma\end{cases}\]

_for all \(\bm{z}\in\mathbb{R}^{n}\) with \(\|\bm{z}\|\leq 1\)._

Proof.: Let \(C\) be the same constant as in Lemma 6 and suppose that

\[\binom{2R+d-1}{d-1}\geq C\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/2}.\]

Let \(\beta\in\{0,1\}\) satisfy \(\beta=1\) when \(\psi=\dot{\sigma}\) and \(\beta=0\) when \(\psi=d\sigma\). Let \(N=\sum_{r=0}^{R}\dim(\mathcal{H}_{2r+\beta}^{d})\). Note that

\[N =\sum_{r=0}^{R}\left(\binom{2r+d+\beta-1}{d-1}-\binom{2r+d+\beta- 3}{d-1}\right)\] \[=\binom{2R+d+\beta-1}{d-1}\] \[\geq\binom{2R+d-1}{d-1}\] \[\geq C\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/2}.\]Let \(g_{1},\cdots,g_{N}\) be spherical harmonics forming an orthonormal basis of \(\bigoplus_{r=1}^{R}\mathcal{H}_{2r-1}^{d}\), and let \(\bm{B}\in\mathbb{R}^{N\times n}\) be the matrix defined by \(\bm{B}_{ai}=g_{a}(\bm{x}_{i})\). By Lemma 6, \(\sigma_{\min}(\bm{B})\geq\sqrt{\frac{N}{2}}\) with probability at least \(1-\epsilon\). Since the functions \(g_{a}\) are orthonormal,

\[\|T_{\psi}\mu_{\bm{z}}\|^{2}\geq\sum_{a=1}^{N}|\langle T_{\psi}\mu_{\bm{z}},g_{ a}\rangle|^{2}.\]

By Lemma 17 the above expression is equal to

\[\sum_{a=1}^{N}|\langle\mu_{\bm{z}},T_{\psi}g_{a}\rangle|^{2}=\sum_{r=0}^{R} \sum_{s=1}^{\dim(\mathcal{H}_{2r+\beta}^{d})}|\langle\mu_{\bm{z}},T_{\psi}Y_{2r +\beta,s}\rangle|^{2}\,.\]

By Lemmas 20 and 21, \(T_{\psi}Y_{2r+\beta,s}=c_{2r+\beta,d}Y_{2r+\beta,s}\), where \(c_{2r+\beta}\in\mathbb{R}\) and

\[|c_{2r+\beta,d}|\gtrsim\begin{cases}\frac{\Gamma\left(\frac{d}{2}\right)\Gamma \left(\frac{2R+1}{2}\right)}{\Gamma\left(\frac{d+2R+1}{2}\right)}&\text{if }\psi= \dot{\sigma}\\ \frac{\sqrt{d}\Gamma\left(\frac{d}{2}\right)\Gamma\left(\frac{2R-1}{2}\right) }{\Gamma\left(\frac{d+2R+1}{2}\right)}&\text{if }\psi=\sqrt{d}\sigma.\end{cases}\] (15)

Hence

\[\|T_{\psi}\mu_{\bm{z}}\|^{2} \geq\sum_{r=0}^{R}\sum_{s=1}^{\dim(\mathcal{H}_{2r+\beta}^{d})}|c _{2r+\beta,d}|^{2}|\langle\mu_{\bm{z}},Y_{2r+\beta,s}\rangle|^{2}\] \[\geq\min_{0\leq r\leq R}\left(|c_{2r+\beta,d}|^{2}\right)\sum_{r= 0}^{R}\sum_{s=1}^{\dim(\mathcal{H}_{2r+\beta}^{d})}|\langle\mu_{\bm{z}},Y_{2r +\beta,s}\rangle|^{2}\] \[=\min_{0\leq r\leq R}\left(|c_{2r+\beta,d}|^{2}\right)\sum_{a=1}^ {N}|\langle\mu_{\bm{z}},g_{a}\rangle|^{2}\] \[=\min_{0\leq r\leq R}\left(|c_{2r+\beta,d}|^{2}\right)\sum_{a=1}^ {N}\left|\sum_{i=1}^{n}z_{i}g_{a}(\bm{x}_{i})\right|^{2}\] \[=\min_{0\leq r\leq R}\left(|c_{2r+\beta,d}|^{2}\right)\|\bm{B}\bm {z}\|^{2}\] \[\geq\min_{0\leq r\leq R}\left(|c_{2r+\beta,d}|^{2}\right)\sigma_{ \min}(\bm{B})^{2}\] \[\geq\frac{N}{2}\min_{0\leq r\leq R}\left(|c_{2r+\beta,d}|^{2} \right).\]

So by (15),

\[\|T_{\psi}\mu_{\bm{z}}\|^{2}\gtrsim\begin{cases}\frac{\Lambda\Gamma\left(\frac {d}{2}\right)^{2}\Gamma\left(\frac{2R+1}{2}\right)^{2}}{\Gamma\left(\frac{d+ 2R+1}{2}\right)^{2}}&\text{if }\psi=\dot{\sigma}\\ \frac{Nd^{2}\Gamma\left(\frac{d}{2}\right)^{2}\Gamma\left(\frac{2R-1}{2} \right)^{2}}{\Gamma\left(\frac{d+2R+1}{2}\right)^{2}}&\text{if }\psi=d\sigma.\end{cases}\] (16)

We now separately analyze the cases where \(\psi=\dot{\sigma}\) and \(\psi=d\sigma\).

**Case 1:**\(\psi=\dot{\sigma}\). In this case

\[\|T_{\psi}\mu_{\bm{z}}\|^{2} \gtrsim N\frac{\Gamma\left(\frac{d}{2}\right)^{2}\Gamma\left(\frac{2 R+1}{2}\right)^{2}}{\Gamma\left(\frac{d+2R+1}{2}\right)^{2}}\] \[=\begin{pmatrix}2R+d\\ d-1\end{pmatrix}\cdot\frac{\Gamma\left(\frac{d}{2}\right)^{2}\Gamma\left(\frac{2 R+1}{2}\right)^{2}}{\Gamma\left(\frac{d+2R+1}{2}\right)^{2}}\] \[=\frac{\Gamma(2R+d+1)}{\Gamma(d)\Gamma(2R+2)}\cdot\frac{\Gamma \left(\frac{d}{2}\right)^{2}\Gamma\left(\frac{2R+1}{2}\right)^{2}}{\Gamma \left(\frac{d+2R+1}{2}\right)^{2}}.\]Then by Stirling's approximation,

\[\|T_{\psi}\mu_{\bm{z}}\|^{2} \gtrsim\frac{(2R+d)^{2R+d+1/2}e^{-2R-d-1}}{d^{d-1/2}e^{-d}(2R+2)^{2R+ 3/2}e^{-2R-2}}\cdot\frac{\left(\frac{d}{2}\right)^{d-1}e^{-d}\left(\frac{2R+1}{2 }\right)^{2R}e^{-2R-1}}{\left(\frac{d+2R+1}{2}\right)^{d+2R}e^{-d-2R-1}}\] \[\gtrsim(d+2R+1)^{1/2}d^{-1/2}\left(\frac{2R+1}{2R+2}\right)^{2R} \left(2R+2\right)^{-3/2}\] \[\gtrsim(d+2R+1)^{1/2}d^{-1/2}(2R+2)^{-3/2}\] \[\gtrsim(d+R)^{1/2}d^{-1/2}R^{-3/2}.\]

Here the third inequality follows from the observations

\[\left(\frac{2R+1}{2R+2}\right)^{2R}>0\]

and

\[\lim_{R\to\infty}\left(\frac{2R+1}{2R+2}\right)^{2R}=\lim_{R\to \infty}\left(1-\frac{1}{2R+2}\right)^{2R}=e^{-1}.\]

**Case 2:**\(\psi=\sqrt{d}\sigma\). In this case

\[\|T_{\psi}\mu_{\bm{z}}\|^{2} \gtrsim N\frac{d\Gamma\left(\frac{d}{2}\right)^{2}\Gamma\left( \frac{2R-1}{2}\right)^{2}}{\Gamma\left(\frac{d+2R+1}{2}\right)^{2}}\] \[=\binom{2R+d-1}{d-1}\cdot\frac{d\Gamma\left(\frac{d}{2}\right)^{ 2}\Gamma\left(\frac{2R-1}{2}\right)^{2}}{\Gamma\left(\frac{d+2R+1}{2}\right)^ {2}}\] \[=\frac{\Gamma(2R+d)}{\Gamma(d)\Gamma(2R+1)}\cdot\frac{d\Gamma \left(\frac{d}{2}\right)^{2}\Gamma\left(\frac{2R-1}{2}\right)^{2}}{\Gamma \left(\frac{d+2R+1}{2}\right)^{2}}.\]

Then by Stirling's approximation,

\[\|T_{\psi}\mu_{\bm{z}}\|^{2} \gtrsim\frac{(2R+d)^{2R+d-1/2}e^{-2R-d}}{d^{d-1/2}e^{-d}(2R+1)^{2 R+1/2}e^{-2R-1}}\cdot\frac{d\left(\frac{d}{2}\right)^{d-1}e^{-d}\left(\frac{2R-1 }{2}\right)^{2R-2}e^{-2R+1}}{\left(\frac{d+2R+1}{2}\right)^{d+2R}e^{-d-2R-1}}\] \[\gtrsim(d+2R)^{-1/2}\left(\frac{d+2R}{d+2R+1}\right)^{d+2R}d^{1/2 }(2R-1)^{-2}(2R+1)^{1/2}\left(\frac{2R-1}{2R+1}\right)^{2R}\] \[\gtrsim(d+2R)^{-1/2}d^{1/2}R^{-3/2}\left(\frac{d+2R}{d+2R+1} \right)^{d+2R}\left(\frac{2R-1}{2R+1}\right)^{2R}\] \[=(d+2R)^{-1/2}d^{1/2}\left(1-\frac{1}{d+2R+1}\right)^{d+2R}\left( 1-\frac{2}{2R+1}\right)^{2R}\] \[\gtrsim(d+R)^{-1/2}d^{1/2}R^{-3/2}.\]

Hence we have established the desired bound on \(\|T_{\psi}\mu_{\bm{z}}\|^{2}\) in all cases. 

**Lemma 7**.: _Let \(d\geq 3\) and suppose that \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\) are \(\delta\)-separated. For all \(\bm{z}\in\mathbb{R}^{n}\) with \(\|\bm{z}\|\leq 1\) then_

\[\|T_{\psi}\mu_{\bm{z}}\|^{2}\gtrsim\begin{cases}\left(1+\frac{d\log(1/\delta)}{ \log d}\right)^{-3}\delta^{2}&\text{ if }\psi=\dot{\sigma}\\ \left(1+\frac{d\log(1/\delta)}{\log d}\right)^{-3}\delta^{4}&\text{ if }\psi=\sqrt{d}\sigma.\end{cases}\]

Proof.: We will consider multiple cases depending on the relative scaling of \(d\) and \(n\). Let \(C>0\) be the same constant as in Lemma 24. First suppose that \(d\geq C\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/2}\). Let \(R=1\). Then

\[\binom{2R+d-1}{d-1}=d\geq C\left(\frac{\delta^{4}}{2}\right)^{(d-2)/2}.\]By Lemma 24, \(\|T_{\psi}\mu_{\bm{z}}\|^{2}\gtrsim 1\) in this case.

Next suppose that \(d\leq C\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/2}\) and \(\sqrt{d}\log d\geq(8\log(1+C)+16d)\log\frac{2}{\delta}\). Let

\[R=\left\lceil\frac{\log(1+C)+2d\log(2/\delta)}{\log d}\right\rceil.\]

Note that since \(d\leq\left(\frac{\delta^{4}}{2}\right)^{-(d-2)/2}\), we have

\[\frac{\log(1+C)+2d\log(2/\delta)}{\log d}\geq\frac{2d\log(2/\delta)}{\frac{d-2 }{2}\log(2/\delta^{4})}\geq 1\]

and therefore

\[R\leq\frac{2\log(1+C)+4d\log(2/\delta)}{\log d}\leq\frac{\sqrt{d}}{4}.\]

By definition,

\[R\geq\frac{\log(1+C)+2d\log(2/\delta)}{\log(d)}\]

so that

\[\begin{pmatrix}2R+d-1\\ d-1\end{pmatrix} \geq\left(\frac{2R+d-1}{2R}\right)^{2R}\] \[\geq\left(\frac{d}{2R}\right)^{2R}\] \[=\exp\left(2R(\log(d)-\log(2R)\right)\] \[\geq\exp\left(R\log d\right)\] \[\geq\exp(\log(1+C)+2d\log(2/\delta))\] \[\geq C\left(\frac{2}{\delta}\right)^{2d}\] \[\geq C\left(\frac{2}{\delta^{4}}\right)^{(d-2)/2}.\]

Then by Lemma 24, the following bounds hold. If \(\psi=\hat{\sigma}\), then

\[\|T_{\psi}\mu_{\bm{z}}\|^{2} \gtrsim(d+R)^{1/2}d^{-1/2}R^{-3/2}\] \[\gtrsim R^{-3/2}\] \[\gtrsim\left(1+\frac{d\log(1/\delta)}{\log d}\right)^{-3/2}\] \[\gtrsim\left(1+\frac{d\log(1/\delta)}{\log d}\right)^{-3}\delta^ {2}.\]

If \(\psi=\sqrt{d}\sigma\), then

\[\|T_{\psi}\mu_{\bm{z}}\|^{2} \gtrsim(d+R)^{-1/2}d^{1/2}R^{-3/2}\] \[\gtrsim(d+\sqrt{d})^{-1/2}d^{1/2}R^{-3/2}\] \[\gtrsim R^{-3/2}\] \[\gtrsim\left(1+\frac{d\log(2/\delta)}{\log d}\right)^{-3/2}\] \[\gtrsim\left(1+\frac{\log(n/\epsilon)}{\log d}\right)^{-3}\delta^ {4}.\]Finally suppose that \(\sqrt{d}\log d\leq(8\log(1+C)+16d)\log\frac{2}{\delta}\) and let \(R=\left\lceil(1+2C)d\left(\frac{2}{\delta}\right)^{2(d-2)/(d-1)}\right\rceil\). Then

\[R \lesssim 1+d\left(\frac{2}{\delta}\right)^{2(d-2)/(d-1)}\] \[\leq(1+d)\left(\frac{2}{\delta}\right)^{2(d-2)/(d-1)}\] \[\leq\left(1+\sqrt{d}\right)^{2}\left(\frac{2}{\delta}\right)^{2( d-2)/(d-1)}\] \[\lesssim\left(1+\frac{d\log(1/\delta)}{\log(d)}\right)^{2}\left( \frac{2}{\delta}\right)^{2(d-2)/(d-1)}\] \[\lesssim\left(1+\frac{d\log(1/\delta)}{\log(d)}\right)^{2}\delta ^{-2}\]

and

\[\binom{2R+d-1}{d-1} \geq\left(\frac{2R+d-1}{d-1}\right)^{d-1}\] \[\geq\left(\frac{R}{d}\right)^{d-1}\] \[\geq\left(1+\frac{2C}{d}\right)^{d-1}\left(\frac{2}{\delta} \right)^{2/(d-2)}.\] \[\geq\frac{2C(d-1)}{d}\left(\frac{2}{\delta}\right)^{2/(d-2)}\] \[\geq C\left(\frac{2}{\delta}\right)^{2/(d-2)}.\]

So by Lemma 24 the following bounds hold. If \(\psi=\dot{\sigma}\), then

\[\|T_{\psi}\mu_{\bm{z}}\|^{2} \gtrsim(d+R)^{1/2}d^{-1/2}R^{-3/2}\] \[\gtrsim(1+d)^{-1/2}R^{-1}\] \[\gtrsim\left(1+\frac{d\log(1/\delta)}{\log d}\right)^{-1}\left(1+ \frac{d\log(1/\delta)}{\log d}\right)^{-2}\delta^{2}\] \[=\left(1+\frac{d\log(1/\delta)}{\log d}\right)^{-3}\delta^{2}.\]

If \(\psi=\sqrt{d}\sigma\), then

\[\|T_{\psi}\mu_{\bm{z}}\|^{2} \gtrsim(d+R)^{-1/2}d^{1/2}R^{-3/2}\] \[\gtrsim\left(d+d\left(\frac{2}{\delta}\right)^{2(d-2)/(d-1)} \right)^{-1/2}d^{1/2}\left(d\left(\frac{2}{\delta}\right)^{2(d-2)/(d-1)} \right)^{-3/2}\] \[\gtrsim d^{-3/2}\left(1+\left(\frac{2}{\delta}\right)^{2(d-2)/(d -1)}\right)^{-1/2}\left(\frac{2}{\delta}\right)^{-3(d-2)/(d-1)}\] \[\gtrsim(1+d)^{-3/2}\left(\frac{2}{\delta}\right)^{-4(d-2)/(d-1)}\] \[\gtrsim\left(1+\frac{d\log(1/\delta)}{\log d}\right)\delta^{4}.\]

Hence we have shown the desired bound on \(\|T_{\psi}\mu_{\bm{z}}\|^{2}\) in all cases.

### Upper bound on the minimum eigenvalue of the NTK

Our strategy to upper bound \(\lambda_{\min}(\bm{K})\) will be to prove that if two data points \(\bm{x},\bm{x}^{\prime}\) are close, then the Jacobian of the network does not separate points too much. We will need to find upper bounds for both \(\|\sigma(\bm{W}\bm{x})-\sigma(\bm{W}\bm{x}^{\prime})\|\) and \(\|\hat{\sigma}(\bm{W}\bm{x})-\hat{\sigma}(\bm{W}\bm{x}^{\prime})\|\).

**Lemma 25**.: _Let \(\epsilon\in(0,1)\). Suppose that \(\bm{x},\bm{x}^{\prime}\in\mathbb{S}^{d-1}\) with \(\|\bm{x}-\bm{x}^{\prime}\|=\delta\). If \(d_{1}=\Omega\left(\log\frac{1}{\epsilon}\right)\), then with probability at least \(1-\epsilon\),_

\[\|\sigma(\bm{W}\bm{x})-\sigma(\bm{W}\bm{x}^{\prime})\|\lesssim\delta\sqrt{d_{1 }}.\]

Proof.: Note that \(\|\sigma(\bm{W}\bm{x})-\sigma(\bm{W}\bm{x}^{\prime})\|^{2}\) can be written a sum of iid subexponential random variables:

\[\|\sigma(\bm{W}\bm{x})-\sigma(\bm{W}\bm{x}^{\prime})\|^{2}=\sum_{j=1}^{d_{1}} (\sigma(\langle\bm{w}_{j},\bm{x}\rangle)-\sigma(\langle\bm{w}_{j},\bm{x}^{ \prime}\rangle)^{2}.\]

Since the entries of each \(\bm{w}_{j}\) are iid standard Gaussian random variables and \(\sigma\) is \(1\)-Lipschitz,

\[\|(\sigma(\langle\bm{w}_{j},\bm{x}\rangle)-\sigma(\langle\bm{w}_{j },\bm{x}^{\prime}\rangle))^{2}\|_{\psi_{1}} =\|\sigma(\langle\bm{w}_{j},\bm{x}\rangle)-\sigma(\langle\bm{w}_{ j},\bm{x}^{\prime}\rangle)\|_{\psi_{2}}^{2}\] \[\leq\|\langle\bm{w}_{j},\bm{x}-\bm{x}^{\prime}\rangle\|_{\psi_{2 }}^{2}\] \[=\|\bm{x}-\bm{x}^{\prime}\|^{2}\] \[=\delta^{2}.\]

Moreover,

\[\mathbb{E}[(\sigma(\langle\bm{w}_{j},\bm{x}\rangle)-\sigma(\langle \bm{w}_{j},\bm{x}^{\prime}\rangle))^{2}] \leq\mathbb{E}[|\langle\bm{w}_{j},\bm{x}-\bm{x}^{\prime}\rangle|^ {2}]\] \[=\|\bm{x}-\bm{x}^{\prime}\|^{2}\] \[=\delta^{2}.\]

So by Bernstein's inequality, for all \(t\geq 0\)

\[\mathbb{P}\left(\|\sigma(\bm{W}\bm{x})-\sigma(\bm{W}\bm{x}^{\prime })\|^{2}\geq\delta^{2}d_{1}+t\right)\] \[\leq\mathbb{P}\left(\|\sigma(\bm{W}\bm{x})-\sigma(\bm{W}\bm{x}^{ \prime})\|^{2}\geq\mathbb{E}[\|\sigma(\bm{W}\bm{x})-\sigma(\bm{W}\bm{x}^{ \prime})\|^{2}]+t\right)\] \[\leq 2\exp\left(-C\min\left(\frac{t^{2}}{d_{1}\delta^{4}},\frac{t }{\delta^{2}}\right)\right)\]

where \(C>0\) is a universal constant. Setting \(t=\delta^{2}d_{1}\) with \(d_{1}\geq\frac{1}{C}\log\frac{2}{\epsilon}\) yields

\[\mathbb{P}(\|\sigma(\bm{W}\bm{x})-\sigma(\bm{W}\bm{x}^{\prime})\|^{2}\geq 2 \delta^{2}d_{1})\leq 2\exp\left(-Cd_{1}\right)\leq\epsilon.\]

This establishes the result. 

**Lemma 26**.: _Suppose that \(\bm{x},\bm{x}^{\prime}\in\mathbb{S}^{d-1}\). If \(\bm{w}\sim\mathcal{N}(\bm{0},\bm{I}_{d})\), then_

\[\mathbb{P}(\hat{\sigma}(\langle\bm{w},\bm{x}\rangle)\neq\hat{\sigma}(\langle \bm{w},\bm{x}^{\prime}\rangle))\asymp\|\bm{x}-\bm{x}^{\prime}\|.\]

Proof.: Recall that for \(\bm{x},\bm{x}^{\prime}\in\mathbb{S}^{d-1}\),

\[\mathbb{P}(\hat{\sigma}(\langle\bm{w},\bm{x}\rangle)\neq\hat{\sigma}(\langle \bm{w},\bm{x}^{\prime}\rangle))=\frac{\theta}{\pi},\]

where \(\theta\) is the angle formed by \(\bm{x}\) and \(\bm{x}^{\prime}\); that is, \(\theta\in[0,\pi]\) with

\[\cos(\theta)=\langle\bm{x},\bm{x}^{\prime}\rangle=1-\frac{1}{2}\|\bm{x}-\bm{x} ^{\prime}\|^{2}.\]

By Taylor's theorem, \(1-\cos(\theta)=\frac{1}{2}\theta^{2}+O(\theta^{3})\), so \(1-\cos(\theta)\asymp\theta^{2}\) for \(\theta\in[0,\pi]\). This implies that \(\theta^{2}\asymp\|\bm{x}-\bm{x}^{\prime}\|^{2}\), so \(\theta\asymp\|\bm{x}-\bm{x}^{\prime}\|\) and therefore

\[\mathbb{P}(\hat{\sigma}(\langle\bm{w},\bm{x}\rangle)\neq\hat{\sigma}(\langle \bm{w},\bm{x}^{\prime}\rangle))\asymp\|\bm{x}-\bm{x}^{\prime}\|.\]

**Lemma 27**.: _Let \(\epsilon\in(0,1)\). Suppose that \(\bm{x},\bm{x}^{\prime}\in\mathbb{S}^{d-1}\) with \(\|\bm{x}-\bm{x}^{\prime}\|\leq\delta\). If \(d_{1}=\Omega\left(\frac{1}{\delta}\log\frac{1}{\epsilon}\right)\), then with probability at least \(1-\epsilon\),_

\[\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})-\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x}) \|\lesssim\sqrt{\delta d_{1}}.\]

Proof.: Observe that

\[\|\dot{\sigma}(\bm{W}\bm{x})-\dot{\sigma}(\bm{W}\bm{x}^{\prime})\|^{2}=4\sum_{ j=1}^{d_{1}}Z_{j}=4|\mathcal{S}|,\]

where \(Z_{j}\in\{0,1\}\) is equal to 1 if

\[\dot{\sigma}(\langle\bm{w}_{j},\bm{x}\rangle)\neq\dot{\sigma}(\langle\bm{w}_{ j},\bm{x}^{\prime}\rangle)\]

and 0 otherwise, and \(\mathcal{S}\) consists of the \(j\in[d_{1}]\) such that \(Z_{j}=1\). The \(Z_{j}\) are iid Bernoulli random variables with parameter \(p\), where \(p\asymp\delta\) by Lemma 26. By Chernoff's inequality (see Vershynin, 2018, Theorem 2.3.1), for all \(t\geq d_{1}p\)

\[\mathbb{P}\left(|\mathcal{S}|\geq t\right)\leq e^{-d_{1}p}\left(\frac{ed_{1}p }{t}\right)^{t}\]

Then setting \(t=ed_{1}p\) with \(d_{1}\geq\frac{1}{p}\log\frac{4}{\epsilon}\) yields

\[\mathbb{P}\left(|\mathcal{S}|\geq ed_{1}\delta\right) \leq\mathbb{P}\left(|\mathcal{S}|\geq ed_{1}p\right)\] \[\leq e^{-d_{1}p}\] \[\leq\frac{\epsilon}{4}.\]

By the lower bound of Chernoff's inequality, for all \(t\leq d_{1}p\)

\[\mathbb{P}(|\mathcal{S}|\leq t)\leq e^{-d_{1}p}\left(\frac{ed_{1}p}{t}\right) ^{t}.\]

Then setting \(t=\frac{d_{1}p}{\epsilon}\) with \(d_{1}\geq\frac{2}{\epsilon-2}\frac{1}{p}\log\frac{4}{\epsilon}\) yields

\[\mathbb{P}\left(|\mathcal{S}|\leq\frac{d_{1}p}{2}\right) \leq\exp\left(-\frac{e-2}{e}d_{1}p\right)\] \[\leq\frac{\epsilon}{4}.\]

Therefore, with probability at least \(1-\frac{\epsilon}{2}\),

\[\frac{d_{1}\delta}{e}\leq|\mathcal{S}|\leq ed_{1}\delta.\]

Let us denote this event by \(\omega\). Observe that

\[\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})-\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x}^{ \prime})\|^{2}=2\sum_{j\in\mathcal{S}}v_{j}^{2}\]

and recall that \(v_{j}^{2}\sim\mathcal{N}(0,1)\) for all \(j\in[d_{1}]\). By Bernstein's inequality, for all \(t\geq 0\)

\[\mathbb{P}\left(\frac{1}{2}\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})-\bm{v} \odot\dot{\sigma}(\bm{W}\bm{x}^{\prime})\|^{2}\geq|\mathcal{S}|+t\ \ \left|\ \ \mathcal{S}\right)\leq 2\exp\left(-C_{1}\min \left(\frac{t^{2}}{|\mathcal{S}|},t\right)\right)\]

where \(C_{1}>0\) is a universal constant. Setting \(t=|\mathcal{S}|\) yields

\[\mathbb{P}\left(\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})-\bm{v}\odot\dot{\sigma }(\bm{W}\bm{x}^{\prime})\|\geq 2\sqrt{|\mathcal{S}|}\ \ \left|\ \ \mathcal{S}\right)\leq 2\exp\left(-C_{1}|\mathcal{S}| \right).\]Then

\[\mathbb{P}\left(\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})-\bm{v}\odot \dot{\sigma}(\bm{W}\bm{x}^{\prime})\|\leq 2\sqrt{ed_{1}\delta}\right)\] \[\geq\mathbb{P}\left(\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})-\bm{v }\odot\dot{\sigma}(\bm{W}\bm{x}^{\prime})\|\leq 2\sqrt{ed_{1}\delta},\;\omega\right)\] \[\geq\mathbb{P}\left(\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})-\bm{v }\odot\dot{\sigma}(\bm{W}\bm{x}^{\prime})\|\leq 2\sqrt{|\mathcal{S}|},\;\omega\right)\] \[\geq\mathbb{E}\left[\mathbb{P}\left(\|\bm{v}\odot\dot{\sigma}( \bm{W}\bm{x})-\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x}^{\prime})\|\leq 2\sqrt{| \mathcal{S}|}\;\Big{|}\;\;\mathcal{S}\right)1_{\omega}\right]\] \[\geq\mathbb{E}\left[\left(1-2\exp\left(-C_{1}|\mathcal{S}| \right)\right)1_{\omega}\right]\] \[\geq\left(1-2\exp\left(-C_{1}\frac{d_{1}\delta}{e}\right)\right) \mathbb{P}(\omega)\] \[\geq\left(1-2\exp\left(-C_{1}\frac{d_{1}\delta}{e}\right)\right) \left(1-\frac{\epsilon}{2}\right),\]

where we used that \(\omega\) is measurable with respect to \(\mathcal{S}\) in the fourth line. So if \(d_{1}\geq\frac{e}{C_{1}\delta}\log\frac{4}{\epsilon}\), then

\[\mathbb{P}\left(\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})-\bm{v} \odot\dot{\sigma}(\bm{W}\bm{x}^{\prime})\|\leq 2\sqrt{ed_{1}\delta}\right) \geq\left(1-\frac{\epsilon}{2}\right)\left(1-\frac{\epsilon}{2}\right)\] \[\geq 1-\epsilon.\]

**Lemma 28**.: _Suppose that \(\bm{x}\in\mathbb{S}^{d-1}\). If \(d_{1}=\Omega\left(\log\frac{1}{\epsilon}\right)\), then with probability at least \(1-\epsilon\),_

\[\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})\|\lesssim\sqrt{d_{1}}.\]

Proof.: Since \(\dot{\sigma}(\langle\bm{w}_{j},\bm{x}\rangle)\in\{0,1\}\) for all \(j\in[d_{1}]\),

\[\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})\|^{2} =\sum_{j=1}^{d_{1}}v_{j}^{2}\dot{\sigma}(\langle\bm{w}_{j},\bm{x}\rangle)\] \[\leq\sum_{j=1}^{d_{1}}v_{j}^{2}.\]

Since the entries \(v_{j}\) are iid standard Gaussian random variables, Bernstein's inequality implies for all \(t\geq 0\)

\[\mathbb{P}(\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})\|^{2}\geq d_{1}+t) \leq\mathbb{P}\left(\sum_{j=1}^{d_{1}}v_{j}^{2}\geq d_{1}+t\right)\] \[\leq 2\exp\left(-C\min\left(\frac{t^{2}}{d_{1}},t\right)\right).\]

Setting \(t=d_{1}\) with \(d_{1}\geq\frac{1}{C}\log\frac{2}{\epsilon}\) yields

\[\mathbb{P}(\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})\|^{2}\geq 2d_{1})\leq 2\exp(-Cd_{ 1})\leq\epsilon.\]

Now we prove our main lemma which we will use to relate the separation between data points to the NTK.

**Lemma 29**.: _Let \(\bm{x},\bm{x}^{\prime}\in\mathbb{S}^{d-1}\) with \(\|\bm{x}-\bm{x}^{\prime}\|\leq\delta\leq 2\). Let \(\epsilon\in(0,1)\). If \(d_{1}=\Omega\left(\frac{1}{\delta}\log\frac{1}{\epsilon}\right)\), then with probability at least \(1-\epsilon\),_

\[\|\nabla_{\bm{\theta}}f(\bm{x})-\nabla_{\bm{\theta}}f(\bm{x}^{\prime})\| \lesssim\sqrt{\delta}.\]Proof.: By Lemma 27, if \(d_{1}\gtrsim\frac{1}{\delta}\log\frac{1}{\epsilon}\), then with probability at least \(1-\frac{\epsilon}{4}\),

\[\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})-\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x}^{ \prime})\|\lesssim\sqrt{\delta d_{1}}.\]

Let us denote this event by \(\omega_{1}\). By Lemma 28, if \(d_{1}\gtrsim\log\frac{1}{\epsilon}\), then with probability at least \(1-\frac{\epsilon}{4}\),

\[\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})\|\lesssim\sqrt{d_{1}}.\]

Let us denote this event by \(\omega_{2}\). If both \(\omega_{1}\) and \(\omega_{2}\) occur, then

\[\|\nabla_{\bm{W}_{1}}f(\bm{x})-\nabla_{\bm{W}_{1}}f(\bm{x}^{ \prime})\|_{F}\] \[=\frac{1}{\sqrt{d_{1}}}\|(\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})) \otimes\bm{x}-(\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x}^{\prime}))\otimes\bm{x}^{ \prime}\|_{F}\] \[\leq\frac{1}{\sqrt{d_{1}}}\|(\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x }))\otimes\bm{x}-(\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x}))\otimes\bm{x}^{\prime} \|_{F}\] \[+\frac{1}{\sqrt{d_{1}}}\|(\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x})) \otimes\bm{x}^{\prime}-(\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x}^{\prime}))\otimes \bm{x}^{\prime}\|_{F}\] \[\leq\frac{1}{\sqrt{d_{1}}}\|\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x}) \|\cdot\|\bm{x}-\bm{x}^{\prime}\|+\frac{1}{\sqrt{d_{1}}}\|\bm{v}\odot\dot{ \sigma}(\bm{W}\bm{x})-\bm{v}\odot\dot{\sigma}(\bm{W}\bm{x}^{\prime})\|\cdot\| \bm{x}^{\prime}\|\] \[\lesssim\frac{1}{\sqrt{d_{1}}}\sqrt{d_{1}}\delta+\frac{1}{\sqrt{d _{1}}}\sqrt{\delta d_{1}}\] \[\lesssim\sqrt{\delta}.\]

By Lemma 25, if \(d_{l}\gtrsim\log\frac{1}{\epsilon}\), then with probability at least \(1-\frac{\epsilon}{2}\),

\[\|\nabla_{\bm{W}_{2}}f(\bm{x})-\nabla_{\bm{W}_{2}}f(\bm{x}^{ \prime})\| =\frac{1}{\sqrt{d_{1}}}\|f_{1}(\bm{x})-f_{1}(\bm{x}^{\prime})\|\] \[\lesssim\delta.\]

Let us denote this event by \(\omega_{3}\). If \(\omega_{1},\omega_{2}\), and \(\omega_{3}\) all occur (which happens with probability at least \(1-\epsilon\)), then

\[\|\nabla_{\bm{\theta}}f(\bm{x})-\nabla_{\bm{\theta}}f(\bm{x}^{ \prime})\| \lesssim\|\nabla_{\bm{W}_{1}}f(\bm{x})-\nabla_{\bm{W}_{1}}f(\bm{ x}^{\prime})\|_{F}+\|\nabla_{\bm{W}_{2}}f(\bm{x})-\nabla_{\bm{W}_{2}}f(\bm{x}^{ \prime})\|\] \[\lesssim\sqrt{\delta}+\delta\] \[\lesssim\sqrt{\delta}.\]

### Proof of Theorem 1

**Theorem 1**.: _Let \(d\geq 3\), \(\epsilon\in(0,1)\), and \(\delta,\delta^{\prime}\in(0,\sqrt{2})\). Suppose that \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\) are \(\delta\)-separated and \(\min_{i\neq k}\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta^{\prime}\). Define_

\[\lambda=\left(1+\frac{d\log(1/\delta)}{\log(d)}\right)^{-3}\delta^{2}.\]

_If \(d_{1}\gtrsim\frac{\|\bm{X}\|^{2}}{\lambda}\log\frac{n}{\epsilon}\), then with probability at least \(1-\epsilon\),_

\[\lambda\lesssim\lambda_{\min}(\bm{K})\lesssim\delta^{\prime}.\]

Proof.: First we prove the lower bound. Let \(\lambda_{1}\) be as it is defined in Lemma 3. By Lemma 5,

\[\lambda_{1}=\inf_{\|\bm{z}\|=1}\|T_{\dot{\sigma}}\mu_{\bm{z}}\|^{2}.\]

Let

\[\lambda=\left(1+\frac{d\log(1/\delta)}{\log(d)}\right)^{-3}\delta^{2}.\]By Lemma 7, \(\lambda_{1}\geq C_{1}\lambda\) for some constant \(C_{1}>0\). By Lemma 3, there exist constants \(C_{2},C_{3}>0\) such that if \(d_{1}\geq\frac{C_{2}}{\lambda_{1}}\|\bm{X}\|^{2}\log\frac{n}{\epsilon}\) then

\[\mathbb{P}(\lambda_{\min}(\bm{K}_{1})<C_{3}\lambda_{1})\leq\frac{\epsilon}{2}.\] (17)

Then for such \(d_{1}\),

\[\mathbb{P}(\lambda_{\min}(\bm{K}_{1})\geq C_{3}C_{1}\lambda)\geq 1-\frac{ \epsilon}{2}.\]

This establishes the lower bound.

Next we prove the upper bound. Let \(i,k\in[n]\) be two indices with \(i\neq k\) such that \(\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta^{\prime}\). If \(d_{1}\gtrsim\frac{1}{\lambda}\log\frac{1}{\epsilon}\gtrsim\frac{1}{\delta^{ \prime}}\log\frac{1}{\epsilon}\), then by Lemma 29 there exists \(C_{4}>0\) such that

\[\mathbb{P}(\|\nabla_{\bm{\theta}}f(\bm{x}_{i})-\nabla_{\bm{\theta}}f(\bm{x}_{ k})\|^{2}\geq C_{4}\delta^{\prime})\geq 1-\frac{\epsilon}{2}.\]

Let us denote this event by \(\omega\). If \(\omega\) occurs, then

\[\lambda_{\min}(\bm{K}) \lesssim(\bm{e}_{i}-\bm{e}_{k})^{T}\bm{K}(\bm{e}_{i}-\bm{e}_{k})\] \[=\|\nabla_{\bm{\theta}}f(\bm{x})-\nabla_{\bm{\theta}}f(\bm{x}_{k} )\|^{2}\] \[\lesssim\delta^{\prime}.\]

Hence, with probability at least \(1-\frac{\epsilon}{2}\), \(\lambda_{\min}(\bm{K})\lesssim\delta^{\prime}\). This establishes the upper bound for the minimum eigenvalue. The two-sided bound then immediately follows from a union bound. 

### Uniform data on a sphere

Our main bounds for the smallest eigenvalue of the NTK are stated in terms of the amount of separation between data points. To interpret our results in terms of probability distributions on the sphere, we will use a couple of lemmas which quantify the amount of separation for data which is uniformly distributed.

For \(\delta\in(0,1/2)\) and \(\bm{x}\in\mathbb{S}^{d-1}\), we define the spherical cap

\[\text{Cap}(\bm{x},\delta)=\{\bm{y}\in\mathbb{S}^{d-1}:\|\bm{y}-\bm{x}\|\leq \delta\}.\]

and the double spherical cap

\[\text{DoubleCap}(\bm{x},\delta)=\text{Cap}(\bm{x},\delta)\cup\text{Cap}(-\bm{ x},\delta).\]

By Lemma 2.3 of Ball (1997),

\[dS(\text{Cap}(\bm{x},\delta))\geq\frac{1}{2}\left(\frac{\delta}{2}\right)^{d- 1}.\] (18)

We can also obtain a corresponding upper bound on the volume of a spherical cap.

**Lemma 30**.: _For \(\bm{x}\in\mathbb{S}^{d-1}\) and \(\delta\in(0,1/2)\),_

\[dS(\text{Cap}(\bm{x},\delta))\leq\frac{4\sqrt{\pi}(C\delta)^{d-1}}{d^{2}}.\]

_Here \(C>0\) is a universal constant._

Proof.: For \(\phi\in[0,\pi]\), let \(\mathcal{S}_{\phi}\) denote the set of all \(\bm{x}^{\prime}\in\mathbb{S}^{d-1}\) such that the angle between \(\bm{x}\) and \(\bm{x}^{\prime}\) is at most \(\phi\) (that is, \(\langle\bm{x},\bm{x}^{\prime}\rangle\geq\cos(\phi)\)). The measure of \(\mathcal{S}_{\phi}\) is given by

\[\frac{B(\sin^{2}(\phi);(d-1)/2,1/2)}{B((d-1)/2,1/2)}\]

(see, e.g. Li, 2010). Here the numerator refers to the incomplete beta function and the denominator refers to the beta function. We can bound

\[B\left(\sin^{2}(\phi);\frac{d-1}{2},\frac{1}{2}\right) =\int_{0}^{\sin^{2}(\phi)}t^{(d-3)/2}(1-t)^{-1/2}dt\] \[\leq\int_{0}^{\sin^{2}(\phi)}t^{(d-3)/2}dt\] \[=\frac{2}{d-1}\sin(\phi)^{d-1}.\]\[B\left(\frac{d-1}{2},\frac{1}{2}\right) =\frac{\Gamma\left(\frac{d-1}{2}\right)\Gamma\left(\frac{1}{2}\right) }{\Gamma\left(\frac{d}{2}\right)}\] \[\geq\frac{\Gamma\left(\frac{d-2}{2}\right)\sqrt{\pi}}{\Gamma \left(\frac{d}{2}\right)}\] \[=\frac{2\sqrt{\pi}}{d-2}.\]

The above two bounds imply

\[dS(\mathcal{S}_{\phi})\leq\frac{4\sqrt{\pi}\sin(\phi)^{d-1}}{(d-1)(d-2)}\leq \frac{4\sqrt{\pi}\sin(\phi)^{d-1}}{d^{2}}\leq\frac{4\sqrt{\pi}\phi^{d-1}}{d^{2}}.\] (19)

Now suppose that \(\bm{x}^{\prime}\in\text{Cap}(\bm{x},\delta)\). Then \(\|\bm{x}-\bm{x}^{\prime}\|\leq\delta\), so \(1-\langle\bm{x},\bm{x}^{\prime}\rangle\leq 2\delta^{2}\). Let \(\phi=\arccos(\langle\bm{x},\bm{x}^{\prime}\rangle)\) be the angle between \(\bm{x}\) and \(\bm{x}^{\prime}\). By Taylor's theorem, \(\cos(\phi)=1-\frac{\phi^{2}}{2}+O(\phi^{3})\), so \(1-\cos(\phi)\asymp\phi^{2}\) for \(\phi\in[0,\pi]\). Thus

\[2\delta^{2}\geq 1-\langle\bm{x},\bm{x}^{\prime}\rangle=1-\cos(\phi)\asymp\phi ^{2}.\]

So the angle between \(\bm{x}\) and \(\bm{x}^{\prime}\) is at most \(C\delta\) for some universal constant \(C>0\). It follows that \(\text{Cap}(\bm{x},\delta)\subseteq\mathcal{S}_{C\delta}\). Finally by (19),

\[dS(\text{Cap}(\bm{x},\delta))\leq\frac{4\sqrt{\pi}(C\delta)^{d-1}}{d^{2}}.\]

Since \(\delta\leq\frac{1}{2}\), the sets \(\text{Cap}(\bm{x},\delta)\) and \(\text{Cap}(-\bm{x},\delta)\) are disjoint by the triangle inequality. Hence

\[dS(\text{DoubleCap}(\bm{x},\delta))=2\text{Cap}(\bm{x},\delta)\]

and in particular by Lemma 30

\[dS(\text{DoubleCap}(\bm{x},\delta))\leq\frac{4\sqrt{\pi}(C\delta)^{d-1}}{d^{2 }}.\] (20)

for a constant \(C>0\).

**Lemma 31**.: _Suppose that \(n\geq 2\) and \(\epsilon\in(0,1)\). If \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\) are independent and uniformly distributed on \(\mathbb{S}^{d-1}\), then with probability at least \(1-\epsilon\), the dataset is \(\delta\)-separated with_

\[\delta\gtrsim\left(\frac{\epsilon}{n^{2}}\right)^{1/(d-1)}.\]

Proof.: Let \(\bm{e}=[1,0,\cdots,0]^{T}\in\mathbb{S}^{d-1}\). For each \(\bm{x}\in\mathbb{S}^{d-1}\), there exists an orthogonal matrix \(\bm{O}_{x}\) such that \(\bm{O}_{\bm{x}}\bm{x}=\bm{e}\). Note that for all \(\bm{x}\in\mathbb{S}^{d-1}\) and \(i\in[n]\), \(\bm{O}_{\bm{x}}\bm{x}_{i}\overset{d}{=}\bm{x}_{i}\). Let \(i,k\in[n]\) with \(i\neq k\). Then for all \(\delta\in(0,1/2)\),

\[\mathbb{P}(\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta\text{ or }\|\bm{x}_{i}+\bm{x}_{k}\| \leq\delta) =\mathbb{E}[\mathbb{P}(\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta\text{ or }\|\bm{x}_{i}+\bm{x}_{k}\|\leq\delta\mid\bm{x}_{k})]\] \[=\mathbb{E}[\mathbb{P}(\|\bm{O}_{x_{k}}\bm{x}_{i}-\bm{O}_{x_{k}}\bm {x}_{k}\|\leq\delta\text{ or }\|\bm{O}_{\bm{x}_{k}}\bm{x}_{i}+\bm{O}_{\bm{x}_{k}}\bm{x}_{k}\|\leq\delta \mid\bm{x}_{k})]\] \[=\mathbb{E}[\mathbb{P}(\|\bm{O}_{x_{k}}\bm{x}_{i}-\bm{e}\|\leq \delta\text{ or }\|\bm{O}_{\bm{x}_{k}}\bm{x}_{i}+\bm{e}\|\leq\delta\mid\bm{x}_{k})]\] \[=\mathbb{E}[\mathbb{P}(\|\bm{x}_{i}-\bm{e}\|\leq\delta\text{ or }\|\bm{x}_{i}+\bm{e}\|\leq\delta\mid\bm{x}_{k})]\] \[=\mathbb{P}(\|\bm{x}_{i}-\bm{e}\|\leq\delta\text{ or }\|\bm{x}_{i}+\bm{e}\|\leq\delta).\]

The expression on the final line is the measure of DoubleCap\((\bm{e},\delta)\), and by (20) is bounded above by

\[\frac{4\sqrt{\pi}(C\delta)^{d-1}}{d^{2}},\]

where \(C>0\) is a constant. So

\[\mathbb{P}(\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta\text{ or }\|\bm{x}_{i}+\bm{x}_{k}\| \leq\delta\text{ \ for some }i\neq k) \leq\sum_{i\neq k}\mathbb{P}(\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta\text{ or }\|\bm{x}_{i}+\bm{x}_{k}\|\leq\delta)\] \[\leq\frac{4\sqrt{\pi}n^{2}(C\delta)^{d-1}}{d^{2}}.\]Setting \(\delta=\min\left(\frac{1}{4},\frac{1}{C}\left(\frac{\epsilon d^{2}}{4\sqrt{\pi}n^{2} }\right)^{1/(d-1)}\right)\), we obtain

\[\mathbb{P}(\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta\text{ or }\|\bm{x}_{i}+\bm{x}_{k}\| \leq\delta\ \ \text{ for some }i\neq k)\leq\epsilon.\]

Therefore, for this value of \(\delta\), the dataset is \(\delta\)-separated with probability at least \(1-\epsilon\). To conclude, note that

\[\frac{1}{C}\left(\frac{\epsilon d^{2}}{4\sqrt{\pi}n^{2}}\right)^{1/(d-1)}\gtrsim \left(\frac{\epsilon}{n^{2}}\right)^{1/(d-1)}\]

since

\[\lim_{d\to\infty}\left(\frac{d^{2}}{4\sqrt{\pi}}\right)^{1/(d-1)}=1.\]

**Lemma 32**.: _Suppose that \(n\geq 2\) and \(\epsilon\in(0,1)\). If \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{S}^{d-1}\) are selected iid from \(U(\mathbb{S}^{d-1})\), then with probability at least \(1-\epsilon\), there exist \(i,k\in[n]\) with \(i\neq k\) such that_

\[\|\bm{x}_{i}-\bm{x}_{k}\|\lesssim\left(\frac{\log(1/\epsilon)}{n^{2}}\right)^ {1/(d-1)}.\]

Proof.: Let \(\bm{e}=[1,0,\cdots,0]^{T}\in\mathbb{S}^{d-1}\). For each \(\bm{x}\in\mathbb{S}^{d-1}\), there exists an orthogonal matrix \(\bm{O}_{x}\) such that \(\bm{O}_{x}\bm{x}=\bm{e}\). Note that for all \(\bm{x}\in\mathbb{S}^{d-1}\) and \(i\in[n]\), \(\bm{O}_{x}\bm{x}_{i}\overset{d}{=}\bm{x}_{i}\). Let \(i,k\in[n]\) with \(i\neq k\). Then for all \(\delta\in(0,1/2)\),

\[\mathbb{P}(\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta) =\mathbb{E}[\mathbb{P}(\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta\mid\bm {x}_{k})]\] \[=\mathbb{E}[\mathbb{P}(\|\bm{O}_{x_{k}}\bm{x}_{i}-\bm{O}_{x_{k}} \bm{x}_{k}\|\leq\delta\mid\bm{x}_{k})]\] \[=\mathbb{E}[\mathbb{P}(\|\bm{O}_{x_{k}}\bm{x}_{i}-\bm{e}\|\leq \delta\mid\bm{x}_{k})]\] \[=\mathbb{E}[\mathbb{P}(\|\bm{x}_{i}-\bm{e}\|\leq\delta\mid\bm{x}_ {k})]\] \[=\mathbb{P}(\|\bm{x}_{i}-\bm{e}\|\leq\delta).\]

The expression on the final line is the measure of \(\text{Cap}(\bm{e},\delta)\), and by Lemma 2.3 of Ball (1997) it is bounded below by \(\frac{1}{2}\left(\frac{\delta}{2}\right)^{d-1}\). For each \(i\in[n]\), let \(\omega_{i}\) denote the event that \(\|\bm{x}_{j}-\bm{x}_{k}\|>\delta\) for all \(j,k\in[1,i]\) with \(j\neq k\). Trivially \(\mathbb{P}(\omega_{1})=1\). If \(\omega_{i}\) occurs for some \(i\in[1,n-1]\), then the sets \(\text{Cap}(\bm{x}_{j},\delta/2)\) for \(j\in[i]\) are disjoint. Indeed, if \(\bm{x}\in\text{Cap}(\bm{x}_{j},\delta/2)\cap\text{Cap}(\bm{x}_{k},\delta/2)\), then by the triangle inequality

\[\|\bm{x}_{j}-\bm{x}_{k}\|\leq\|\bm{x}-\bm{x}_{j}\|+\|\bm{x}-\bm{x}_{k}\|\leq \frac{\delta}{2}+\frac{\delta}{2}=\delta\]

which contradicts \(\omega_{i}\). Now since these smaller spherical caps are disjoint, we can bound

\[dS\left(\cup_{j=1}^{i}\{\bm{x}\in\mathbb{S}^{d-1}:\|\bm{x}-\bm{ x}_{j}\|\leq\delta\}\right) \geq dS\left(\cup_{j=1}^{i}\{\bm{x}\in\mathbb{S}^{d-1}:\|\bm{x}- \bm{x}_{j}\|\leq\delta/2\}\right)\] \[=dS\left(\cup_{j=1}^{i}\text{Cap}(\bm{x}_{j},\delta/2)\right)\] \[=\sum_{j=1}^{i}dS(\text{Cap}(\bm{x}_{j},\delta/2))\] \[\geq\sum_{j=1}^{i}\frac{1}{2}\left(\frac{\delta}{4}\right)^{d-1}\] \[=\frac{i}{2}\left(\frac{\delta}{4}\right)^{d-1}.\]

Since \(\bm{x}_{i+1}\) is chosen independently from \(\bm{x}_{1},\cdots,\bm{x}_{i}\), this implies

\[\mathbb{P}(\omega_{i+1}\mid\omega_{i}) =\mathbb{P}(\|\bm{x}_{i+1}-\bm{x}_{j}\|>\delta\ \ \forall j\in[i]\mid\omega_{i})\] \[\leq 1-\frac{i}{2}\left(\frac{\delta}{4}\right)^{d-1}.\]By repeatedly conditioning we obtain

\[\mathbb{P}(\|\bm{x}_{j}-\bm{x}_{k}\|>\delta\ \ \forall j,k\in[n]) =\mathbb{P}(\omega_{n})\] \[=\mathbb{P}(\omega_{1})\prod_{i=2}^{n}\mathbb{P}(\omega_{i}\mid \omega_{1},\cdots,\omega_{i-1})\] \[=\prod_{i=2}^{n}\mathbb{P}(\omega_{i}\mid\omega_{i-1})\] \[\leq\prod_{i=2}^{n}\left(1-\frac{i}{2}\left(\frac{\delta}{4} \right)^{d-1}\right)\] \[\leq\prod_{i=2}^{n}\exp\left(-\frac{i}{2}\left(\frac{\delta}{4} \right)^{d-1}\right)\] \[\leq\exp\left(-\frac{n^{2}}{2}\left(\frac{\delta}{4}\right)^{d-1 }\right).\]

Let us set \(\delta=\min\left(\frac{1}{4},4\left(\frac{2}{n^{2}}\log\frac{1}{\epsilon} \right)^{\frac{1}{d-1}}\right)\). The above bounds imply that

\[\mathbb{P}(\|\bm{x}_{j}-\bm{x}_{k}\|>\delta\ \ \forall j,k\in[n])\leq\epsilon\]

so with probability at least \(1-\epsilon\), there exist \(i,k\in[n]\) such that \(\|\bm{x}_{i}-\bm{x}_{k}\|\leq\delta\) with

\[\delta\lesssim\left(n^{-2}\log\frac{1}{\epsilon}\right)^{1/(d-1)}\]

which is what we needed to show. 

**Corollary 2**.: _Let \(d\geq 3\), \(n\geq 2\), \(\epsilon\in(0,1)\), \(\bm{x}_{1},\cdots,\bm{x}_{n}\sim U(\mathbb{S}^{d-1})\) be mutually iid. Define_

\[\lambda=\left(1+\frac{\log(n/\epsilon)}{\log(d)}\right)^{-3}\left(\frac{ \epsilon^{2}}{n^{4}}\right)^{1/(d-1)}.\]

_If \(d_{1}\gtrsim\frac{1}{\lambda}\left(1+\frac{n+\log(1/\epsilon)}{d}\right)\log \frac{n}{\epsilon}\), then with probability at least \(1-\epsilon\) over the data and network parameters,_

\[\lambda\lesssim\lambda_{\min}(\bm{K})\lesssim\left(\frac{\log(1/\epsilon)}{n^ {2}}\right)^{1/(d-1)}.\]

Proof.: By Lemma 14, with probability at least \(1-\frac{\epsilon}{4}\),

\[\|\bm{X}\|^{2}\lesssim\left(1+\frac{n+\log\frac{1}{\epsilon}}{d}\right).\]

Let us denote this event by \(\omega_{1}\). Let us define

\[\delta:=\min_{i\neq k}\min(\|\bm{x}_{i}-\bm{x}_{k}\|,\|\bm{x}_{i}+\bm{x}_{k}\|)\]

and

\[\delta^{\prime}:=\min_{i\neq k}\|\bm{x}_{i}-\bm{x}_{k}\|.\]

In particular, the dataset \(\bm{x}_{1},\cdots,\bm{x}_{n}\) is \(\delta\)-separated. By Lemma 31, with probability at least \(1-\frac{\epsilon}{4}\),

\[\delta\gtrsim\left(\frac{\epsilon}{n^{2}}\right)^{1/(d-1)}.\]

Let us denote this event by \(\omega_{2}\). By Lemma 32, with probability at least \(1-\frac{\epsilon}{4}\),

\[\delta^{\prime}\lesssim\left(\frac{\log(1/\epsilon)}{n^{2}}\right)^{1/(d-1)}.\]Let us denote this event by \(\omega_{3}\). We condition on \(\omega_{1},\omega_{2},\) and \(\omega_{3}\) for the remainder of the proof. Define

\[\lambda^{\prime}=\left(1+\frac{d\log(1/\delta)}{\log(d)}\right)^{-3}\delta^{2}\]

and

\[\lambda=\left(1+\frac{\log(n/\epsilon)}{\log(d)}\right)^{-3}\left(\frac{ \epsilon^{2}}{n^{4}}\right)^{1/(d-1)};\]

note that

\[\lambda^{\prime} \gtrsim\left(1+\frac{d\log\left((n^{2}/\epsilon)^{1/(d-1)}\right) }{\log(d)}\right)^{-3}\left(\frac{\epsilon}{n^{2}}\right)^{2/(d-1)}\] \[\gtrsim\left(1+\frac{\log(n/\epsilon)}{\log(d)}\right)^{-3}\left( \frac{\epsilon^{2}}{n^{4}}\right)^{1/(d-1)}\] \[=\lambda.\]

By Theorem 1, if

\[d_{1}\gtrsim\frac{1}{\lambda}\left(1+\frac{n+\log(1/\epsilon)}{d}\right)\log \left(\frac{n}{\epsilon}\right)\gtrsim\frac{1}{\lambda^{\prime}}\|\bm{X}\|^{2 }\log\left(\frac{n}{\epsilon}\right),\]

then with probability at least \(1-\frac{\epsilon}{4}\) over the network weights,

\[\lambda_{\min}(\bm{K})\gtrsim\lambda^{\prime}\gtrsim\lambda\]

and

\[\lambda_{\min}(\bm{K})\lesssim\delta^{\prime}\lesssim\left(\frac{\log(1/ \epsilon)}{n^{2}}\right)^{1/(d-1)}.\]

This is exactly the bound that we needed to show. By taking a union bound over all of the favorable events, it follows that this event happens with probability at least \(1-\epsilon\). 

## Appendix D Proof of Theorem 8

### Recap of the deep setting

Recall for the deep case we consider fully connected networks with \(L\) layers and denote the layer widths with positive integers, \(d_{0},\cdots,d_{L}\) where \(d_{0}=d\) and \(d_{L}=1\). For \(l\in[L-1]\) we define the feature matrices \(\bm{F}_{l}\in\mathbb{R}^{d_{l}\times n}\) as

\[\bm{F}_{l}=[f_{l}(\bm{x}_{1}),\cdots,f_{l}(\bm{x}_{n})].\]

For \(l\in[L-1]\) and \(\bm{x}\in\mathbb{R}^{d}\) we define the activation patterns \(\bm{\Sigma}_{l}(\bm{x})\in\{0,1\}^{d_{l}\times d_{l}}\) to be the diagonal matrices

\[\bm{\Sigma}_{l}(\bm{x})=\text{diag}(\hat{\sigma}(\bm{W}_{l}f_{l-1}(\bm{x}))).\]

Lemma 9 provides a useful decomposition of the NTK.

**Lemma 9**.: _Let \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathbb{R}^{d}\) be nonzero. There exists an open set \(\mathcal{U}\subset\mathcal{P}\) of full Lebesgue measure such that \(f(\bm{x}_{i};\cdot)\) is continuously differentiable on \(\mathcal{U}\) for all \(i\in[n]\). Moreover, for all \(\bm{\theta}\in\mathcal{U}\) the NTK Gram matrix \(\bm{K}\) defined in (1) with network function (7) satisfies_

\[\left(\prod_{l=1}^{L-1}\frac{d_{l}}{2}\right)\bm{K} =\sum_{l=0}^{L-1}(\bm{F}_{l}^{T}\bm{F}_{l})\odot(\bm{B}_{l+1}\bm{B }_{l+1}^{T}),\]

_where the \(i\)th row of \(\bm{B}_{l}\in\mathbb{R}^{n\times n_{l}}\) is defined as_

\[[\bm{B}_{l}]_{i,:}=\begin{cases}\bm{\Sigma}_{l}(\bm{x}_{i})\left(\prod_{k=l+1 }^{L-1}\bm{W}_{k}^{T}\bm{\Sigma}_{k}(\bm{x}_{i})\right)\bm{W}_{L}^{T},&l\in[L-1 ],\\ \bm{1}_{n},&l=L.\end{cases}\]Proof.: For any \(i\in[n]\), observe that \(f(\bm{x}_{i},\cdot)\) is a PAP function (Lee et al., 2020b, Definition 5) and therefore \(f(\bm{x}_{i},\cdot)\) is differentiable almost everywhere (Lee et al., 2020b, Proposition 4). As the union of \(n\) null sets is also a null set, we conclude that there exists an open set \(U\) of full measure such that for all \(i\in[n]\) then \(f(\bm{x}_{i},\theta)\) is differentiable for any \(\theta\in U\).

Let \(\frac{\partial f}{\partial\bm{\theta}}\) denote the true derivative of \(f\) with respect to \(\bm{\theta}\) when it exists and be the minimum norm sub-gradient otherwise. Using (Lee et al., 2020b, Corollary 13) then

\[\left(\prod_{l=1}^{L-1}\frac{d_{l}}{2}\right)\bm{K}\overset{a.e.}{=}\frac{ \partial F_{L}(\bm{\theta})}{\partial\bm{\theta}}^{T}\frac{\partial F_{L}(\bm {\theta})}{\partial\bm{\theta}}=\sum_{l=1}^{L}\frac{\partial F_{L}(\bm{ \theta})}{\partial\bm{W}_{l}}^{T}\frac{\partial F_{L}(\bm{\theta})}{\partial \bm{W}_{l}},\]

where \(\frac{\partial F_{L}(\bm{\theta})}{\partial\bm{W}_{l}}\in\bm{R}^{d_{l}d_{l-1} \times n}\). By inspection, to prove the result claimed it therefore suffices to show for any \(l\in[L]\), \(\theta\in U\) and \(i,j\in[n]\) that

\[\left\langle\frac{\partial f_{L}(\bm{x};\bm{\theta})}{\partial\bm{W}_{l}}, \frac{\partial f_{L}(\bm{x}_{j};\bm{\theta})}{\partial\bm{W}_{l}}\right\rangle =\left(f_{l-1}(\bm{x}_{i})^{T}f_{l-1}(\bm{x}_{j};\bm{\theta})\right)\left([ \bm{B}_{l}]_{i,:}^{T}[\bm{B}_{l}]_{j,:}\right).\] (21)

First observe

\[\left\langle\frac{\partial f_{L}(\bm{x};\bm{\theta})}{\partial\bm{W}_{L}}, \frac{\partial f_{L}(\bm{x}_{j};\bm{\theta})}{\partial\bm{W}_{L}}\right\rangle =f_{L-1}(\bm{x};\bm{\theta})^{T}f_{L-1}(\bm{x};\bm{\theta})\times 1\]

therefore establishing (21) for \(l=L\). To establish (21) for \(l\in[L-1]\), recall for \(k\in[L-1]\) that \(\bm{\Sigma}_{k}(\bm{x})=\text{diag}\left(\hat{\sigma}(\bm{W}_{k}f_{k-1}(\bm{ x}))\right)\) and define \(\bm{\Sigma}_{L}(\bm{x})=1\). Observe for \(1\leq l<k\), \(k\in[L]\) that

\[\frac{\partial f_{k}(\bm{x};\bm{\theta})}{\partial\bm{W}_{l}}=\bm{\Sigma}_{k} (\bm{x})\bm{W}_{k}\frac{\partial f_{k-1}(\bm{x};\bm{\theta})}{\partial\bm{W}_ {l}}\] (22)

while for \(k=l\)

\[\frac{\partial f_{k}(\bm{x};\bm{\theta})}{\partial\bm{W}_{k}}=\bm{\Sigma}_{k} (\bm{x})\otimes f_{k-1}(\bm{x};\bm{\theta})^{T}.\] (23)

As a result,

\[\frac{\partial f_{L}(\bm{x};\bm{\theta})}{\partial\theta_{l}} =\bm{W}_{L}\left(\prod_{k=1}^{L-l+1}\bm{\Sigma}_{L-k}(\bm{x})\bm{W }_{L-k}\right)\frac{\partial f_{l}(\bm{x};\bm{\theta})}{\partial\bm{W}_{l}}\] \[=\bm{W}_{L}\left(\prod_{k=1}^{L-l+1}\bm{\Sigma}_{L-k}(\bm{x})\bm{W }_{L-k}\right)(\bm{\Sigma}_{l}(\bm{x})\otimes f_{l-1}(\bm{x};\bm{\theta}))\]

where the first equality arises from iterating (22) and the second by applying (23). Proceeding,

\[\left\langle\frac{\partial f_{L}(\bm{x}_{i})}{\partial\theta_{l}},\frac{ \partial f_{L}(\bm{x}_{j})}{\partial\theta_{l}}\right\rangle\] \[=\left(f_{l-1}(\bm{x}_{i})^{T}f_{l-1}(\bm{x}_{j})\right)\left( \left(\bm{\Sigma}_{l}(\bm{x}_{i})\prod_{k=l+1}^{L-1}\bm{W}_{k}^{T}\bm{\Sigma}_{ k}(\bm{x}_{i})\right)\bm{W}_{L}^{T}\right)^{T}\left(\left(\bm{\Sigma}_{l}(\bm{x}_{j}) \prod_{k=l+1}^{L-1}\bm{W}_{k}^{T}\bm{\Sigma}_{k}(\bm{x}_{j})\right)\bm{W}_{L}^ {T}\right)\] \[=\left(f_{l-1}(\bm{x}_{i})^{T}f_{l-1}(\bm{x}_{j})\right)\left([ \bm{B}_{l}]_{i,:}^{T}[\bm{B}_{l}]_{j,:}\right)\]

### Proof of Lemma 10

**Lemma 33**.: _Let \(\bm{z}\in\mathbb{R}^{d}\) be a fixed vector and \(\bm{W}\in\mathbb{R}^{m\times d}\) a random matrix with mutually iid elements \([\bm{W}]_{ij}\sim\mathcal{N}(0,1)\) for all \(i\in[m]\) and \(j\in[d]\). Consider the random vector \(\bm{y}\in\mathbb{R}^{m}\) defined as \(\bm{y}=\sigma(\bm{W}\bm{z})\) where \(\sigma\) denotes the ReLU function applied elementwise. For \(\delta\in(0,1)\) if \(m\gtrsim\delta^{-2}\log(1/\epsilon)\) then_

\[\mathbb{P}\left((1-\delta)\frac{m}{2}\|\bm{z}\|^{2}\leq\|\bm{y}\|^{2}\leq(1+ \delta)\frac{m}{2}\|\bm{z}\|^{2}\right)\geq 1-\epsilon.\]

Proof.: For \(i\in[m]\) define \(Z_{i}=\frac{\bm{w}_{i}^{T}\bm{z}}{\|\bm{z}\|}\), then \(Z_{i}\sim\mathcal{N}(0,1)\) are mutually iid. Let \(B_{i}=\mathbb{1}(Z_{i}>0)\), note by symmetry \(B_{i}\sim\text{Ber}(1/2)\), furthermore these random variables for \(i\in[n]\) are also mutually iid with respect to one another. As \(y_{i}=\|\bm{z}\|B_{i}Z_{i}\) then

\[\|\bm{y}\|_{2}^{2}=\|\bm{z}\|^{2}\sum_{i=1}^{m}B_{i}Z_{i}^{2}.\]For convenience let \(\bm{y}^{\prime}=\bm{y}/\|\bm{z}\|\)and define \(\mathcal{S}=\{i\in[n]\ :\ B_{i}=1\}\), then

\[\|\bm{y}^{\prime}\|^{2}=\sum_{i\in\mathcal{S}}Z_{i}^{2}\sim\chi^{2}(|\mathcal{S }|).\]

From (Laurent & Massart, 2000, Lemma 1) we have for any \(t>0\)

\[\mathbb{P}\left(|\left(\|\bm{y}^{\prime}\|^{2}-|\mathcal{S}|\right)|\geq 2 \sqrt{|\mathcal{S}|t}\right)\leq 2\exp(-t).\]

For \(\delta_{1}\in(0,1)\) let \(t=\frac{|\mathcal{S}|\delta_{1}^{2}}{4}\), then

\[\mathbb{P}\left((1-\delta_{1})|\mathcal{S}|\leq\|\bm{y}^{\prime}\|^{2}\leq(1+ \delta_{1})|\mathcal{S}|\right)\geq 1-2\exp\left(-\frac{|\mathcal{S}|\delta_{1}^{2} }{4}\right).\]

Observe \(|\mathcal{S}|=\sum_{i=1}^{m}B_{i}\sim\text{Bin}(m,1/2)\). With \(\delta_{2}\in(0,1)\) then applying Hoeffding's inequality we have

\[\mathbb{P}\left((1-\delta_{2})\frac{m}{2}\leq\sum_{i=1}^{m}B_{i}\leq(1+\delta_ {2})\frac{m}{2}\right)\geq 1-2\exp\left(-\frac{\delta_{2}^{2}m}{2}\right).\]

Let \(\omega\) denote the event that \((1-\delta_{2})\frac{m}{2}\leq|\mathcal{S}|\leq(1+\delta_{2})\frac{m}{2}\). If \(m\geq\frac{16}{\delta_{1}^{2}\delta_{2}^{2}(1-\delta_{2})}\log(4/\epsilon)\) then

\[\mathbb{P}\left((1-\delta_{1})(1-\delta_{2})\frac{m}{2}\leq\|\bm{ y}^{\prime}\|^{2}\leq(1+\delta_{1})(1+\delta_{2})\frac{m}{2}\right)\] \[\geq\mathbb{P}\left((1-\delta_{1})(1-\delta_{2})\frac{m}{2}\leq\| \bm{y}^{\prime}\|^{2}\leq(1+\delta_{1})(1+\delta_{2})\frac{m}{2}\ \mid\ \omega\right)\mathbb{P}(\omega)\] \[\geq\mathbb{P}\left((1-\delta_{1})|\mathcal{S}|\leq\|\bm{y}^{ \prime}\|^{2}\leq(1+\delta_{1})|\mathcal{S}|\ \mid\ \omega\ \right)\mathbb{P}(\omega)\] \[\geq\left(1-2\exp\left(-\frac{(1-\delta_{2})\delta_{1}^{2}m}{8} \right)\right)\left(1-2\exp\left(-\frac{\delta_{2}^{2}m}{2}\right)\right)\] \[\geq\left(1-\frac{\epsilon}{2}\right)\left(1-\frac{\epsilon}{2}\right)\] \[\geq 1-\epsilon.\]

For some \(\delta\in(0,1)\) let \(\delta_{2}=\delta_{1}=\delta/3\), then if \(m\geq 1944\delta^{-2}\log(4/\epsilon)\) we have

\[\mathbb{P}\left((1-\delta)\frac{m}{2}\leq\|\bm{y}^{\prime}\|^{2}\leq(1+ \delta)\frac{m}{2}\right)\geq 1-\epsilon\]

from which the result claimed follows. 

**Lemma 10**.: _Let \(\bm{x}\in\mathbb{S}^{d_{0}-1}\), \(L\geq 2\) and \(l\in[L-1]\). If \(d_{k}\gtrsim l^{2}\log(l/\epsilon)\) for all \(k\in[l]\), then_

\[e^{-1}\left(\prod_{h=1}^{l}\frac{d_{h}}{2}\right)\leq\|f_{l}(\bm{x})\|^{2} \leq e\left(\prod_{h=1}^{l}\frac{d_{h}}{2}\right)\]

_holds with probability at least \(1-\epsilon\) over the network parameters._

Proof.: For \(k\in[l]\) let \(\omega_{k}\) denote the event that the inequality

\[\left(1-\frac{1}{l}\right)^{k}\left(\prod_{h=1}^{k}\frac{d_{h}}{2}\right)\leq \|f_{k}(\bm{x})\|^{2}\leq\left(1+\frac{1}{l}\right)^{k}\left(\prod_{h=1}^{k} \frac{d_{h}}{2}\right)\]

holds. We proceed by induction to establish that \(\mathbb{P}(\omega_{k})\geq(1-\frac{\epsilon}{l})^{k}\) for all \(k\in[l]\). For the base case note that \(f_{1}(\bm{x})=\sigma(\bm{W}_{1}\bm{x})\) and \(\|\bm{x}\|^{2}=1\). Applying Lemma 33 with \(\delta=\frac{1}{l}\), if \(d_{1}\gtrsim l^{2}\log(l/\epsilon)\) then \(\mathbb{P}(\omega_{1})\geq 1-\frac{\epsilon}{l}\). Now suppose for \(k\in[l-1]\) that \(\mathbb{P}(\omega_{k})\geq(1-\frac{\epsilon}{l})^{k}\). Note

\[\mathbb{P}(\omega_{k+1})\geq\mathbb{P}(\omega_{k+1}\mid\omega_{k})\mathbb{P}( \omega_{k})\geq\mathbb{P}(\omega_{k+1}\mid\omega_{k})(1-\frac{\epsilon}{l})^{k}\]

Recall \(f_{k+1}(\bm{x})=\sigma(\bm{W}_{1}f_{k}(\bm{x}))\). Conditioned on \(\omega_{k}\), then again applying Lemma 33 with \(\delta=\frac{1}{l}\) and as \(d_{k+1}\gtrsim l^{2}\log(l/\epsilon)\) we have

\[\mathbb{P}(\omega_{k+1}\mid\omega_{k})\geq 1-\frac{\epsilon}{l}\]

which completes the proof of the induction hypothesis. As \((1-\epsilon/l)^{l}\geq 1-\epsilon\) and \(e^{-1}\leq(1-1/l)^{l}\leq(1+1/l)^{l}\leq e\) then

\[e^{-1}\left(\prod_{h=1}^{l}\frac{d_{h}}{2}\right)\leq\|f_{l}(\bm{x})\|^{2} \leq e\left(\prod_{h=1}^{l}\frac{d_{h}}{2}\right)\]

holds with probability at least \(1-\epsilon\).

### Proof of Lemma 34

**Lemma 34**.: _Let \(\bm{x}\in\mathbb{S}^{d_{0}-1}\), \(L\geq 2\) and assume \(d_{k}\gtrsim L^{2}\log\left(\frac{L}{\epsilon}\right)\) for all \(k\in[L-1]\). For any \(l\in[L-1]\) with probability at least \(1-\epsilon\) over the network parameters the following holds,_

\[\|\bm{S}_{l}(\bm{x})\|_{F}^{2}\asymp 2^{-L+l+1}\prod_{k=l}^{L-1}d_{k}.\]

Proof.: In what follows for convenience we define an empty product of scalars or matrices as the scalar one. Let \(K\in\{L-1\}\), \(l\in[K]\), and for some arbitrary \(\bm{x}\in\mathbb{S}^{d_{0}-1}\) define

\[\bm{S}_{l,K}=\bm{\Sigma}_{l}(\bm{x})\prod_{k=l+1}^{K}\bm{W}_{k}^{T}\bm{\Sigma} _{k}(\bm{x}).\]

Let \(\omega_{l,K}\) denote the event

\[\frac{1}{2}\left(1-\frac{1}{L}\right)^{K}\leq\|\bm{S}_{l,K}\|_{F}^{2}\prod_{k =l}^{K}\frac{2}{d_{l}}\leq 2\left(1+\frac{1}{L}\right)^{K}\] (24)

It suffices to lower bound the probability of the event \(\omega_{l,L-1}\). Let \(\mathcal{F}_{K}\) denote the \(\sigma\)-algebra generated by \(\bm{W}_{1},\cdots,\bm{W}_{K}\) and note that \(\bm{S}_{l,K}\in\mathcal{F}_{K}\). Let \(\gamma_{l}\) denote the event that \(f_{l}(\bm{x})\neq 0\), then

\[\mathbb{P}(\omega_{l,L-1}) \geq\mathbb{P}(\omega_{l,L-1}\mid\omega_{l,L-2})\mathbb{P}(\omega _{l,L-2})\] \[\geq\mathbb{P}(\omega_{l,L-1}\mid\omega_{l,L-2})\mathbb{P}( \omega_{l,L-2}\mid\omega_{l,L-3})\mathbb{P}(\omega_{l,L-3})\] \[\geq\left(\prod_{h=l}^{L-2}\mathbb{P}(\omega_{l,h+1}\mid\omega_{l,h})\right)\mathbb{P}(\omega_{l,l}\mid\gamma_{l})\mathbb{P}(\gamma_{l}).\]

Fixing \(\epsilon\in(0,1)\), our goal is to show each term in this product is at least \((1-\frac{\epsilon}{L})\): indeed, if this is true then

\[\mathbb{P}(\omega_{l,L-1})\geq\left(1-\frac{\epsilon}{L}\right)^{L-l}\geq 1-\epsilon\]

and our task is complete. To this end, first observe that as \(d_{k}\gtrsim L^{2}\log(L/\epsilon)\) for all \(k\in[L-1]\), then \(\mathbb{P}(\gamma_{l})\geq 1-\frac{\epsilon}{L}\) by Lemma 10. Proceeding to the term \(\mathbb{P}(\omega_{l,l}\mid\gamma_{l})\), recall \([\bm{\Sigma}_{l}(\bm{x})]_{jj}=\mathbbm{1}([\bm{W}_{l}f_{l-1}(\bm{x})]_{j}>0)\). By symmetry the diagonal entries of \(\bm{\Sigma}_{l}(\bm{x})\) are mutually iid Bernoulli random variables with parameter \(\frac{1}{2}\). Therefore, using Hoeffding's inequality for all \(t\geq 0\)

\[\mathbb{P}\left(\left|\|\bm{\Sigma}_{l}(\bm{x})\|_{F}^{2}-\frac{d_{l}}{2} \right|\geq t\ \middle|\ \gamma_{l}\right)\leq 2\exp\left(-\frac{t^{2}}{d_{l}}\right).\]

Let \(t=d_{l}\), if \(d_{l}\geq\log\frac{2L}{\epsilon}\) then with \(K\geq 1\), \(L\geq 2\) it follows that

\[\mathbb{P}(\omega_{l,l}\mid\gamma_{l}) =\mathbb{P}\left(\frac{1}{2}\left(1-\frac{1}{L}\right)^{K}\leq\| \bm{\Sigma}_{l}(\bm{x})\|_{F}^{2}\frac{2}{d_{l}}\leq 2\left(1+\frac{1}{L} \right)^{K}\ \middle|\ \gamma_{l}\right)\] \[\geq\mathbb{P}\left(\frac{1}{2}\leq\|\bm{\Sigma}_{l}(\bm{x})\|_{F }^{2}\frac{2}{d_{l}}\leq\frac{3}{2}\ \middle|\ \gamma_{l}\right)\] \[\geq 1-\mathbb{P}\left(\left|\bm{\Sigma}_{l}(\bm{x})\|_{F}^{2}- \frac{d_{l}}{2}\right|\geq\frac{d_{l}}{4}\ \middle|\ \gamma_{l}\right)\] \[\geq 1-\frac{\epsilon}{L}.\]

We now proceed to analyze \(\mathbb{P}(\omega_{l,h+1}\mid\omega_{l,h})\) for \(h\in[l,K-1]\). Note if \(\omega_{l,h}\) is true then \(\|\bm{S}_{l,h}\|_{F}^{2}>0\). By definition this implies \(\|\bm{\Sigma}_{l}(\bm{x})\|_{F}^{2}>0\), however, if \(f_{h}(\bm{x})=0\) then \(\|\bm{\Sigma}_{l}(\bm{x})\|_{F}^{2}=0\). Therefore \(\omega_{l,h}\) being true implies \(f_{h}(\bm{x})\neq 0\). For convenience in what follows we denote the \(j\)th column of \(\bm{W}_{h+1}\) as \(\bm{w}_{j}\). By definition

\[\bm{S}_{l,h+1}=\bm{S}_{l,h}\bm{W}_{h+1}^{T}\bm{\Sigma}_{h+1}(\bm{x}),\]therefore,

\[\mathbb{E}[\|\bm{S}_{l,h+1}\|_{F}^{2}\mid\mathcal{F}_{h}] =\mathbb{E}[\|\bm{S}_{l,h}\bm{W}_{h+1}^{T}\bm{\Sigma}_{h+1}(\bm{x}) \|_{F}^{2}\mid\mathcal{F}_{h}]\] \[=\mathbb{E}\left[\sum_{j=1}^{d_{h+1}}\|\bm{S}_{l,h}\bm{w}_{j}\|^{2 }\,\hat{\sigma}(\langle\bm{w}_{j},f_{h}(\bm{x})\rangle)\,\Biggm{|}\mathcal{F}_ {h}\right].\]

As highlighted already, if we condition on \(\omega_{l,h}\) then \(f_{h}(\bm{x})\neq 0\) and therefore the random variables \((\hat{\sigma}(\langle\bm{w}_{j},f_{h}(\bm{x})\rangle))_{j\in d_{h+1}}\) are mutually iid Bernoulli random variables with parameter \(\frac{1}{2}\). Again by symmetry \(\hat{\sigma}(\langle\bm{w}_{j},f_{h}(\bm{x})\rangle)\) is independent of \(\|\bm{S}_{l,h}\bm{w}_{j}\|^{2}\). Therefore conditioned on \(\omega_{l,h}\)

\[\sum_{j=1}^{d_{h+1}}\mathbb{E}[\|\bm{S}_{l,h}\bm{w}_{j}\|^{2}\mid \mathcal{F}_{d_{h+1}}]\mathbb{E}[\hat{\sigma}(\langle\bm{w}_{j},f_{h}(\bm{x} )\rangle)\mid\mathcal{F}_{h}] =\frac{1}{2}\sum_{j=1}^{d_{h+1}}\mathbb{E}[\|\bm{S}_{l,h}\bm{w}_{ j}\|^{2}\mid\mathcal{F}_{h}]\] \[=\frac{1}{2}\sum_{j=1}^{d_{h+1}}\|\bm{S}_{l,h}\|_{F}^{2}\] \[=\frac{d_{h+1}}{2}\|\bm{S}_{l,h}\|_{F}^{2}.\]

Moreover, under the same conditioning

\[\Big{\|}\|\bm{S}_{l,h}\bm{w}_{j}\|^{2}\,\hat{\sigma}(\langle\bm{ w}_{j},f_{h}(\bm{x})\rangle)\Big{\|}_{\psi_{1}} \leq\||\bm{S}_{l,h}\bm{w}_{j}\|^{2}\|_{\psi_{1}}\] \[=\||\bm{S}_{l,h}\bm{w}_{j}\||_{\psi_{2}}^{2}\] \[\lesssim\|\bm{S}_{l,h}\|_{F}^{2}\]

where the last line follows from Theorem 6.3.2 of Vershynin (2018). As a result, conditioned on \(\omega_{l,h}\) then using Bernstein's inequality (Vershynin, 2018, Theorem 2.8.1) there exists an absolute constant \(c\) such that for all \(t\geq 0\)

\[\mathbb{P}\left(\left\|\bm{S}_{l,h+1}\|_{F}^{2}-\frac{d_{h+1}}{2}\|\bm{S}_{l,h }\|_{F}^{2}\right\|\geq t\,\left|\,\mathcal{F}_{h}\right)\leq 2\exp\left(-c \min\left(\frac{t^{2}}{d_{h+1}\|\bm{S}_{l,h}\|_{F}^{4}},\frac{t}{\|\bm{S}_{l,h }\|_{F}^{2}}\right)\right).\]

If \(d_{h+1}\geq\frac{4L^{2}}{c}\log\frac{2L}{\epsilon}\) and \(t=\frac{d_{h+1}\|\bm{S}_{l,h}\|_{F}^{2}}{2L}\) then conditioning on \(\omega_{l,h}\) we obtain

\[\mathbb{P}\left(\left\|\bm{S}_{l,h+1}\|_{F}^{2}-\frac{d_{K}}{2}\|\bm{S}_{l,h} \|_{F}^{2}\right\|\geq\frac{d_{h+1}}{2L}\|\bm{S}_{l,h}\|_{F}^{2}\,\left|\, \mathcal{F}_{h}\right)\leq\frac{\epsilon}{L}.\]

As a result, for any \(h\in[l,K-1]\) we have \(\mathbb{P}(\omega_{l,h+1}\mid\omega_{l,h})\geq 1-\frac{\epsilon}{L}\) from which the result claimed follows. 

### Proof of Lemma 35

**Lemma 35**.: _Let \(\bm{x}\in\mathbb{S}^{d_{0}-1}\), \(L\geq 3\) and assume \(d_{k}\geq d_{k+1}\) and \(d_{k}\gtrsim\sqrt{\log\frac{1}{\epsilon}}\) for all \(k\in[L-1]\). For any \(l\in[L-1]\) with probability at least \(1-\epsilon\) over the network parameters the following holds,_

\[\|\bm{S}_{l}(\bm{x})\|^{2}\lesssim\prod_{k=l}^{L-2}d_{k}.\]

Proof.: By Theorem 4.4.5 of Vershynin (2018), for any \(k\in[L-1]\) and all \(t\geq 0\)

\[\mathbb{P}\left(\|\bm{W}_{k}\|\leq C(\sqrt{d_{k-1}}+\sqrt{d_{k}}+t)\right) \geq 1-2e^{-t^{2}}.\]

As \(d_{k-1}\geq d_{k}\geq\sqrt{\log\frac{2L}{\epsilon}}\), then setting \(t=\sqrt{\log\frac{2}{\epsilon}}\) yields

\[\mathbb{P}\left(\|\bm{W}_{k}\|\leq 3C_{1}\sqrt{d_{k-1}}\right) \geq\mathbb{P}\left(\|\bm{W}_{k}\|\leq C(\sqrt{d_{k-1}}+\sqrt{d_{ k}}+t)\right)\] \[\geq 1-\frac{\epsilon}{L}.\]Using a union bound it follows that

\[\mathbb{P}\left(\|\bm{W}_{k}\|\leq 3C_{1}\max\{\sqrt{d_{l-1}},\sqrt{d_{l}}\} \ \ \forall k\in[L-1]\right)\geq 1-\epsilon.\]

Note that \(\|\bm{\Sigma}_{k}(\bm{x})\|\leq 1\) for all \(k\in[L-1]\), therefore conditional on the above event we have

\[\|\bm{S}_{l}(\bm{x})\| =\left\|\bm{\Sigma}_{l}(\bm{x})\left(\prod_{k=l+1}^{L-1}\bm{W}_{ k}^{T}\bm{\Sigma}_{k}(\bm{x})\right)\right\|\] \[\leq\|\bm{\Sigma}_{l}(\bm{x})\|\left(\prod_{k=l+1}^{L-1}\|\bm{W}_ {k}\|\|\bm{\Sigma}_{k}(\bm{x})\|\right)\] \[\leq\prod_{k=l+1}^{L-1}\|\bm{W}_{k}\|\] \[\lesssim\prod_{k=l+1}^{L-2}\sqrt{d_{k}}.\]

To conclude we square both sides. 

### Proof of Lemma 11

**Lemma 11**.: _Let \(\bm{x}\in\mathbb{S}^{d_{0}-1}\), suppose \(L\geq 3\), \(d_{k}\geq d_{k+1}\) for all \(k\in[L-1]\) and \(d_{L-1}\gtrsim 2^{L}\log\left(\frac{L}{\epsilon}\right)\). Then, for any \(l\in[L-1]\), with probability at least \(1-\epsilon\) over the network parameters_

\[\|\bm{S}_{l}(\bm{x})\bm{W}_{L}^{T}\|^{2}\asymp 2^{-L+l+1}\prod_{k=l}^{L-1}d_{ k}.\]

Proof.: Let \(\bm{x}\in\mathbb{S}^{d_{0}-1}\) be arbitrary and recall \(\bm{S}_{l}(\bm{x})=\bm{\Sigma}_{l}(\bm{x})\left(\prod_{k=l+1}^{L-1}\bm{W}_{k} ^{T}\bm{\Sigma}_{k}(\bm{x})\right)\). Also recall that \(\bm{W}_{L}^{T}\in\mathbb{R}^{d_{L-1}}\) is distributed as \(\bm{W}_{L}^{T}\sim\mathcal{N}(\bm{0}_{d_{L-1}},I_{d_{L}_{1}})\). Therefore by Vershynn (2018, Theorem 6.3.2) for any \(\bm{A}\in\mathbb{R}^{d_{2}\times d_{L-1}}\) and \(t\geq 0\)

\[\mathbb{P}(\|\bm{A}\bm{W}_{L}^{T}\|_{2}-\|\bm{A}\|_{F}\|\geq t) \leq 2\exp\left(-\frac{Ct^{2}}{\|\bm{A}\|_{2}^{2}}\right)\]

for some constant \(C>0\). As a result, with \(t=\frac{1}{2}\|\bm{A}\|_{F}^{2}\) then for some constant \(C>0\)

\[\mathbb{P}\left(\frac{1}{4}\|\bm{A}\|_{F}^{2}\leq\|\bm{A}\bm{W}_{L }^{T}\|_{2}^{2}\leq\frac{3}{4}\|\bm{A}\|_{F}^{2}\right)\geq 1-\exp\left(-C \frac{\|\bm{A}\|_{F}^{2}}{\|\bm{A}\|_{2}^{2}}\right).\]

Therefore, in order to lower bound \(\|\bm{S}_{l}(\bm{x})\bm{W}_{L}^{T}\|_{2}^{2}\) with high probability it suffices to condition on a suitable upper bound for \(\|\bm{S}_{L-1}(\bm{x})\|_{2}^{2}\) and a suitable lower bound for \(\|\bm{S}_{L-1}(\bm{x})\|_{F}^{2}\). Let \(\omega\) denote the event that both

\[\|\bm{S}_{l}\|_{F}^{2}\asymp 2^{L-l-1}\prod_{k=l}^{L-1}d_{k}\]

and

\[\|\bm{S}_{l}(\bm{x})\|^{2}\lesssim\prod_{k=l}^{L-2}d_{k}\]

are true. Combining Lemmas 34 and 35 using a union bound, then as long as \(L\geq 3\), \(d_{k}\geq d_{k+1}\) and \(d_{k}\gtrsim L^{2}\log\frac{nL}{\epsilon}\) for all \(k\in[L-1]\) then \(\mathbb{P}(\omega)\geq 1-\frac{\epsilon}{2}\). As a result and also as \(d_{L-1}\gtrsim 2^{L}\log(2/\epsilon)\)

[MISSING_PAGE_EMPTY:46]

with probability at least \(1-\frac{\epsilon}{4}\). Via a union bound we conclude that the condition

\[2^{L-1}\left(\prod_{l=1}^{L-1}\frac{1}{d_{l}}\right)\lambda_{\min}(\bm{F}_{1}\bm {F}_{1}^{T})\min_{i\in[n]}\|[\bm{B}_{2}]_{i,:}\|^{2}\gtrsim\left(1+\frac{\log( n/\epsilon)}{\log(d_{0})}\right)^{-3}\delta^{4}\]

holds with probability at least \(1-\frac{\epsilon}{2}\). Fixing some \(i\in[n]\), for the upper bound observe trivially by construction that

\[\|f_{0}(\bm{x}_{i})\|^{2}\|[\bm{B}_{1}]_{i,:}\|^{2}=1.\]

By assumption \(d_{k}\gtrsim L^{2}\log(4L^{2}/\epsilon)\) for all \(k\in[L-1]\). With \(l\in[0,L-1]\) then by Lemma 10

\[\|f_{l}(\bm{x}_{i})\|^{2}\lesssim 2^{-l}\prod_{k=1}^{l}d_{k}\]

holds with probability at least \(1-\frac{\epsilon}{4L}\). Likewise by Lemma 34 for \(l\in[2,L]\),

\[\|[\bm{B}_{l}]_{i,:}\|^{2}=\|\bm{S}_{l}(\bm{x}_{i})\bm{W}_{L}^{T}\|\lesssim 2^{ -L+l+1}\prod_{k=l}^{L-1}d_{k}\]

with probability at least \(1-\frac{\epsilon}{4L}\). Combining these via a union bound then for any \(l\in[0,L-1]\),

\[\|f_{l}(\bm{x}_{i})\|^{2}\|[\bm{B}_{l}]_{i,:}\|^{2}\lesssim 2^{-L+1}\prod_{k=1}^ {L-1}d_{k}\]

holds with probability at least \(1-\frac{\epsilon}{2L}\). Again using a union bound now over the layers, it follows that

\[2^{L-1}\left(\prod_{l=1}^{L-1}\frac{1}{d_{l}}\right)\sum_{l=0}^{L-1}\|f_{l}( \bm{x}_{i})\|^{2}\|[\bm{B}_{l+1}]_{i,:}\|^{2}\lesssim L2^{L-1}\left(\prod_{l= 1}^{L-1}\frac{1}{d_{l}}\right)2^{-L+1}\left(\prod_{l=1}^{L-1}d_{l}\right)=L\] (25)

with probability at least \(1-\frac{\epsilon}{2}\). As a result, using a final union bound we conclude both the upper and lower bounds hold with probability at least \(1-\epsilon\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We clearly state our contributions in the introduction along with references to where we prove each of our results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss assumptions and include a Limitations paragraph in our conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We precisely state our theorems and prove them in rigor in respective appendices. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [NA] Justification: The paper does not include experiments requiring code or data. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: None of the potential harms mentioned apply directly to our work. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work is primary theoretical and does not have direct societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.