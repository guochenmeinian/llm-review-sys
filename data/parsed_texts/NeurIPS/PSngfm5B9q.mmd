# Beyond Exponential Graph:

Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence

Yuki Takezawa\({}^{1,2}\), Ryoma Sato\({}^{1,2}\), Han Bao\({}^{1,2}\), Kenta Niwa\({}^{3}\), Makoto Yamada\({}^{2}\)

\({}^{1}\)Kyoto University, \({}^{2}\)OIST, \({}^{3}\)NTT Communication Science Laboratories

Equal Contribution

###### Abstract

Decentralized learning has recently been attracting increasing attention for its applications in parallel computation and privacy preservation. Many recent studies stated that the underlying network topology with a faster consensus rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for decentralized learning. However, a topology with a fast consensus rate, e.g., the exponential graph, generally has a large maximum degree, which incurs significant communication costs. Thus, seeking topologies with both a fast consensus rate and small maximum degree is important. In this study, we propose a novel topology combining both a fast consensus rate and small maximum degree called the Base-\((k+1)\) Graph. Unlike the existing topologies, the Base-\((k+1)\) Graph enables all nodes to reach the exact consensus after a finite number of iterations for any number of nodes and maximum degree \(k\). Thanks to this favorable property, the Base-\((k+1)\) Graph endows Decentralized SGD (DSGD) with both a faster convergence rate and more communication efficiency than the exponential graph. We conducted experiments with various topologies, demonstrating that the Base-\((k+1)\) Graph enables various decentralized learning methods to achieve higher accuracy with better communication efficiency than the existing topologies. Our code is available at https://github.com/yukiTakezawa/BaseGraph.

## 1 Introduction

Distributed learning, which allows training neural networks in parallel on multiple nodes, has become an important paradigm owing to the increased utilization of privacy preservation and large-scale machine learning. In a centralized fashion, such as All-Reduce and Federated Learning [8; 9; 16; 26; 27], all or some selected nodes update their parameters by using their local dataset and then compute the average parameter of these nodes, although computing the average of many nodes is the major bottleneck in the training time [18; 19; 23]. To reduce communication costs, decentralized learning gains significant attention [11; 18; 24]. Because decentralized learning allows nodes to exchange parameters only with a few neighbors in the underlying network topology, decentralized learning is more communication efficient than All-Reduce and Federated Learning.

While decentralized learning can improve communication efficiency, it may degrade the convergence rate and accuracy due to its sparse communication characteristics [11; 48]. Specifically, the smaller the maximum degree of an underlying network topology is, the fewer the communication cost becomes [33; 39]; meanwhile, the faster the consensus rate (a.k.a. spectral gap) of a topology is, the faster the convergence rate of decentralized learning becomes [11]. Thus, developing a topology with both a fast consensus rate and small maximum degree is essential for decentralized learning. Table 1 summarizes the properties of various topologies. For instance, the ring and exponential graph arecommonly used [1; 3; 12; 23]. The ring is a communication-efficient topology because its maximum degree is two but its consensus rate quickly deteriorates as the number of nodes \(n\) increases [28]. The exponential graph has a fast consensus rate, which does not deteriorate much as \(n\) increases, but it incurs significant communication costs because its maximum degree increases as \(n\) increases [43]. Thus, these topologies sacrifice either communication efficiency or consensus rate.

Recently, the \(1\)-peer exponential graph [43] and \(1\)-peer hypercube graph [31] were proposed as topologies that combine both a small maximum degree and fast consensus rate (see Sec. C.4 for examples). As Fig. 1 shows, in the ring and exponential graph, node parameters only reach the consensus asymptotically by repeating exchanges of parameters with neighbors. Contrarily, in the \(1\)-peer exponential and \(1\)-peer hypercube graphs, parameters reach the exact consensus after a finite number of iterations when \(n\) is a power of \(2\) (see Fig. 21 in Sec. F.2). Thanks to this property of finite-time convergence, the \(1\)-peer exponential and \(1\)-peer hypercube graphs enable Decentralized SGD (DSGD) [18] to converge at the same convergence rate as the exponential graph when \(n\) is a power of \(2\), even though the maximum degree of the \(1\)-peer exponential and \(1\)-peer hypercube graphs is only one [43]. However, this favorable property only holds when \(n\) is a power of \(2\). When \(n\) is not a power of \(2\), the \(1\)-peer hypercube graph cannot be constructed, and the \(1\)-peer exponential graph only reaches the consensus asymptotically as well as the ring and exponential graph, as Fig. 1 illustrates. Thus, the \(1\)-peer exponential and \(1\)-peer hypercube graphs cannot enable DSGD to converge as fast as the exponential graph when \(n\) is not a power of \(2\). Moreover, even if \(n\) is a power of \(2\), the \(1\)-peer hypercube and \(1\)-peer exponential graphs still cannot enable DSGD to converge faster than the exponential graph.

In this study, we ask the following question: _Can we construct topologies that provide DSGD with both a faster convergence rate and better communication efficiency than the exponential graph for any number of nodes?_ Our work provides the affirmative answer by proposing the Base-\((k+1)\) Graph,2 which is finite-time convergence for any number of nodes \(n\) and maximum degree \(k\) (see Fig. 1). Thanks to this favorable property, the Base-2 Graph enables DSGD to converge faster than the ring and torus and as fast as the exponential graph for any \(n\), while the Base-2 Graph is more communication-efficient than the ring, torus, and exponential graph because its maximum degree is only one. Furthermore, when \(2\leq k<\lceil\log_{2}(n)\rceil\), the Base-\((k+1)\) Graph enables DSGD to converge faster with fewer communication costs than the exponential graph because the maximum degree of the Base-\((k+1)\) Graph is still less than that of the exponential graph. Experimentally, we compared the Base-\((k+1)\) Graph with various existing topologies, demonstrating that the Base-\((k+1)\) Graph enables various decentralized learning methods to more successfully reconcile accuracy and communication efficiency than the existing topologies.

Footnote 2: Note that the maximum degree of the Base-\((k+1)\) Graph is not \(k+1\), but at most \(k\).

## 2 Related Work

**Decentralized Learning.** The most widely used decentralized learning methods are DSGD [18] and its adaptations [1; 2; 19]. Many researchers have improved DSGD and proposed DSGD with momentum [4; 20; 44; 45], communication compression methods [6; 10; 23; 35; 38], etc. While DSGD is a simple and efficient method, DSGD is sensitive to data heterogeneity [36]. To mitigate this issue, various methods have been proposed, which can eliminate the effect of data heterogeneity on the convergence rate, including gradient tracking [21; 29; 30; 34; 42; 47], \(D^{2}\)[36], etc. [17; 37; 46].

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Topology** & **Consensus Rate** & **Connection** & **Maximum Degree** & **\#Nodes**\(n\) \\ \hline Ring [28] & \(1-O(n^{-2})\) & Undirected & \(2\) & \(\forall n\in\mathbb{N}\) \\ Torus [28] & \(1-O(n^{-1})\) & Undirected & \(4\) & \(\forall n\in\mathbb{N}\) \\ Exp. [43] & \(1-O((\log_{2}(n))^{-1})\) & Directed & \(\lceil\log_{2}(n)\rceil\) & \(\forall n\in\mathbb{N}\) \\
1-peer Exp. [43] & \(O(\log_{2}(n))\)-finite time conv. & Directed & \(1\) & A power of 2 \\
1-peer Hypercube [31] & \(O(\log_{2}(n))\)-finite time conv. & Undirected & \(1\) & A power of 2 \\
**Base-\((k+1)\) Graph (ours)** & \(O(\log_{k+1}(n))\)-finite time conv. & Undirected & \(k\) & \(\forall n\in\mathbb{N}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison among different topologies with \(n\) nodes. The definition of the consensus rate and finite-time convergence is shown in Sec. 3.

Figure 1: Comparison of consensus rate. See Sec. 6 for detailed experimental settings. The number in the bracket is the maximum degree.

**Effect of Topologies.** Many prior studies indicated that topologies with a fast consensus rate improve the accuracy of decentralized learning [11; 13; 25; 39]. For instance, DSGD and gradient tracking converge faster as the topology has a faster consensus rate [18; 34]. Zhu et al. [48] revealed that topology with a fast consensus rate improves the generalization bound of DSGD. Especially when the data distributions are statistically heterogeneous, the topology with a fast consensus rate prevents the parameters of each node from drifting away and can improve accuracy [11; 34]. However, communication costs increase as the maximum degree increases [33; 43]. Thus, developing a topology with a fast consensus rate and small maximum degree is important for decentralized learning.

## 3 Preliminary and Notation

**Notation.** A graph \(G\) is represented by \((V,E)\) where \(V\) is a set of nodes and \(E\) is a set of edges. If \(G\) is a graph, \(V(G)\) (resp. \(E(G)\)) denotes the set of nodes (resp. edges) of \(G\). For any \(a_{1},\cdots,a_{n}\), \((a_{1},\cdots,a_{n})\) denotes the ordered set. An empty (ordered) set is denoted by \(\emptyset\). For any \(n\in\mathbb{N}\), let \([n]\coloneqq\{1,\cdots,n\}\). For any \(n,a\in\mathbb{N}\), \(\text{mod}(a,n)\) is the remainder of dividing \(a\) by \(n\). \(\|\cdot\|_{F}\) denotes Frobenius norm, and \(\mathbf{1}_{n}\) denotes an \(n\)-dimensional vector with all ones.

**Topology.** Let \(G\) be an underlying network topology with \(n\) nodes, and \(\bm{W}\in[0,1]^{n\times n}\) be a mixing matrix associated with \(G\). That is, \(W_{ij}\) is the weight of the edge \((i,j)\), and \(W_{ij}>0\) if and only if \((i,j)\in E(G)\). Most of the decentralized learning methods require \(\bm{W}\) to be doubly stochastic (i.e., \(\bm{W}\mathbf{1}_{n}=\mathbf{1}_{n}\) and \(\bm{W}^{\top}\mathbf{1}_{n}=\mathbf{1}_{n}\)) [18; 20; 29; 36]. Then, the consensus rate of \(G\) is defined below.

**Definition 1**.: _Let \(\bm{W}\) be a mixing matrix associated with a graph \(G\) with \(n\) nodes. Let \(\bm{x}_{i}\in\mathbb{R}^{d}\) be a parameter that node \(i\) has. Let \(\bm{X}\coloneqq(\bm{x}_{1},\cdots,\bm{x}_{n})\in\mathbb{R}^{d\times n}\) and \(\bar{\bm{X}}\coloneqq\frac{1}{n}\bm{X}\mathbf{1}_{n}\mathbf{1}_{n}^{\top}\). The consensus rate \(\beta\in[0,1)\) is the smallest value that satisfies \(\left\|\bm{X}\bm{W}-\bar{\bm{X}}\right\|_{F}^{2}\leq\beta^{2}\left\|\bm{X}- \bar{\bm{X}}\right\|_{F}^{2}\) for any \(\bm{X}\)._

Thanks to \(\beta\in[0,1)\), \(\bm{x}_{i}\) asymptotically converge to consensus \(\frac{1}{\sum_{i=1}^{n}\bm{x}_{i}}\) by repeating parameter exchanges with neighbors. However, this does not mean that all nodes reach the exact consensus within a finite number of iterations except when \(\beta=0\), that is, when \(G\) is fully connected. Then, utilizing time-varying topologies, Ying et al. [43] and Shi et al. [31] aimed to obtain sequences of graphs that can make all nodes reach the exact consensus within finite iterations and proposed the \(1\)-peer exponential and \(1\)-peer hypercube graphs respectively (see Sec. C.4 for illustrations).

**Definition 2**.: _Let \((G^{(1)},\cdots,G^{(m)})\) be a sequence of graphs with the same set of nodes (i.e., \(V(G^{(1)})=\cdots=V(G^{(m)})\)). Let \(n\) be the number of nodes. Let \(\bm{W}^{(1)},\cdots,\bm{W}^{(m)}\) be mixing matrices associated with \(G^{(1)},\cdots,G^{(m)}\), respectively. Suppose that \(\bm{W}^{(1)},\cdots,\bm{W}^{(m)}\) satisfy \(\bm{X}\bm{W}^{(1)}\bm{W}^{(2)}\cdots\bm{W}^{(m)}=\bar{\bm{X}}\) for any \(\bm{X}\in\mathbb{R}^{d\times n}\), where \(\bar{\bm{X}}=\frac{1}{n}\bm{X}\mathbf{1}_{n}\mathbf{1}_{n}^{\top}\). Then, \((G^{(1)},\cdots,G^{(m)})\) is called \(m\)-finite time convergence or an \(m\)-finite time convergent sequence of graphs._

Because Definition 2 assumes that \(V(G^{(1)})=\cdots=V(G^{(m)})\) holds, we often write a sequence of graphs \((G^{(1)},\cdots,G^{(m)})\) as \((E(G^{(1)}),\cdots,E(G^{(m)}))\) using a slight abuse of notation. Additionally, in the following section, we often abbreviate the weights of self-loops because they are uniquely determined due to the condition that the mixing matrix is doubly stochastic.

## 4 Construction of Finite-time Convergent Sequence of Graphs

In this section, we propose the Base-\((k+1)\) Graph, which is finite-time convergence for any number of nodes \(n\in\mathbb{N}\) and maximum degree \(k\in[n-1]\). Specifically, we consider the setting where node \(i\) has a parameter \(\bm{x}_{i}\) and propose a graph sequence whose maximum degree is at most \(k\) that makes all nodes reach the exact consensus \(\frac{1}{n}\sum_{i=1}^{n}\bm{x}_{i}\). To this end, we first propose the \(k\)-peer Hyper-hypercube Graph, which is finite-time convergence when \(n\) does not have prime factors larger than \(k+1\). Using it, we propose the Simple Base-\((k+1)\) Graph and Base-\((k+1)\) Graph, which are finite-time convergence for any \(n\).

Figure 2: Illustration of the \(2\)-peer Hyper-hypercube Graph.

### \(k\)-peer Hyper-hypercube Graph

Before proposing the Base-\((k+1)\) Graph, we first extend the \(1\)-peer hypercube graph [31] to the \(k\)-peer setting and propose the \(k\)-peer Hyper-hypercube Graph, which is finite-time convergence when the number of nodes \(n\) does not have prime factors larger than \(k+1\) and is used as a component in the Base-\((k+1)\) Graph. Let \(V\) be a set of \(n\) nodes. We assume that all prime factors of \(n\) are less than or equal to \(k+1\). That is, there exists \(n_{1},n_{2},\cdots,n_{L}\in[k+1]\) such that \(n=n_{1}\times\cdots\times n_{L}\). In this case, we can construct the \(L\)-finite time convergent sequence of graphs whose maximum degree is at most \(k\). Using Fig. 1(a), we explain how all nodes reach the exact consensus. Let \(G^{(1)}\) and \(G^{(2)}\) denote the graphs in Fig. 1(a) from left to right, respectively. After the nodes exchange parameters with neighbors in \(G^{(1)}\), nodes \(1\) and \(2\), nodes \(3\) and \(4\), and nodes \(5\) and \(6\) have the same parameter respectively. Then, after exchanging parameters in \(G^{(2)}\), all nodes reach the exact consensus. We present the complete algorithms for constructing the \(k\)-peer Hyper-hypercube Graph in Alg. 1.

```
1:Input: the set of nodes \(V:=\{v_{1},\cdots,v_{n}\}\) and number of nodes \(n\).
2:Decompose \(n\) as \(n=n_{1}\times\cdots\times n_{L}\) with minimum \(L\) such that \(n_{l}\in[k+1]\) for all \(l\in[L]\).
3:for\(l\in[L]\)do
4: Initialize \(b_{i}\) to zero for all \(i\in[n]\) and \(E^{(l)}\) to \(\emptyset\).
5:for\(i\in[n]\)do
6:for\(m\in[n_{l}]\)do
7:\(j\leftarrow\text{mod}(i+m\times\prod_{l^{\prime}=1}^{l^{\prime}-1}n_{l^{\prime }}-1,n)+1\).
8:if\(b_{i}<n_{l}-1\) and \(b_{j}<n_{l}-1\)then
9: Add edge \((v_{i},v_{j})\) with weight \(\frac{1}{n_{l}}\) to \(E^{(l)}\) and \(b_{i}\gets b_{i}+1\).
10:return\((E^{(1)},E^{(2)},\cdots,E^{(L)})\). ```

**Algorithm 1**\(k\)-peer Hyper-hypercube Graph \(\mathcal{H}_{k}(V)\)

### Simple Base-\((k+1)\) Graph

As described in Sec. 4.1, when \(n\) do not have prime factors larger than \(k+1\), we can easily make all nodes reach the exact consensus by the \(k\)-peer Hyper-hypercube Graph. However, when \(n\) has prime factors larger than \(k+1\), e.g., when \((k,n)=(1,5)\), the \(k\)-peer Hyper-hypercube Graph cannot be constructed. In this section, we extend the \(k\)-peer Hyper-hypercube Graph and propose the Simple Base-\((k+1)\) Graph, which is infinite-time convergence for any number of nodes \(n\) and maximum degree \(k\). Note that the maximum degree of the Simple Base-\((k+1)\) Graph is not \(k+1\), but at most \(k\).

We present the pseudo-code for constructing the Simple Base-\((k+1)\) Graph in Alg. 2. For simplicity, here we explain only the case when the maximum degree \(k\) is one. The case with \(k\geq 2\) is explained in Sec. B. The Simple Base-\((k+1)\) Graph mainly consists of the following four steps. The key idea is that splitting \(V\) into disjoint subsets to which the \(k\)-peer Hyper-hypercube Graph is applicable.

**Step 1.**: As in the base-\(2\) number of \(n\), we decompose \(n\) as \(n=2^{p_{1}}+\cdots+2^{p_{L}}\) in line 1, and then split \(V\) into disjoint subsets \(V_{1},\cdots,V_{L}\) such that \(|V_{l}|=2^{p_{l}}\) for all \(l\in[L]\) in line \(3\).3
Footnote 3: Splitting \(V_{l}\) into \(V_{l,1},\cdots,V_{l,a_{l}}\) becomes crucial when \(k\geq 2\) (see Sec. B).
**Step 2.**: For all \(l\in[L]\), we make all nodes in \(V_{l}\) obtain the average of parameters in \(V_{l}\) using the \(1\)-peer Hyper-hypercube Graph \(\mathcal{H}_{1}(V_{l})\) in line 11. Then, we initialize \(l^{\prime}\) as one.
**Step 3.**: Each node in \(V_{l^{\prime}+1}\cup\cdots\cup V_{L}\) exchanges parameters with one node in \(V_{l^{\prime}}\) such that the average in \(V_{l^{\prime}}\) becomes equivalent to the average in \(V\). We increase \(l^{\prime}\) by one and repeat step \(3\) until \(l^{\prime}=L\). This procedure corresponds to line 15.
**Step 4.**: For all \(l\in[L]\), we make all nodes in \(V_{l}\) obtain the average in \(V_{l}\) using the \(1\)-peer Hyper-hypercube Graph \(\mathcal{H}_{1}(V_{l})\). Because the average in \(V_{l}\) is equivalent to the average in \(V\) after step \(3\), all nodes can reach the exact consensus. This procedure corresponds to line 25.

Using the example presented in Fig. 3, we explain Alg. 2 in more detail. Let \(G^{(1)},\cdots,G^{(5)}\) denote the graphs in Fig. 3 from left to right, respectively. In step \(1\), we split \(V\coloneqq\{1,\cdots,5\}\) into\(V_{1}\coloneqq\{1,\cdots,4\}\) and \(V_{2}\coloneqq\{5\}\). In step \(2\), nodes in \(V_{1}\) obtain the average in \(V_{1}\) by exchanging parameters in \(G^{(1)}\) and \(G^{(2)}\). In step \(3\), the average in \(V_{1}\) becomes equivalent to the average in \(V\) by exchanging parameters in \(G^{(3)}\). In step \(4\), nodes in \(V_{1}\) can get the average in \(V\) by exchanging parameters in \(G^{(4)}\) and \(G^{(5)}\). Because node \(5\) also obtains the average in \(V\) after exchanging parameters in \(G^{(3)}\), all nodes reach the exact consensus after exchanging parameters in \(G^{(5)}\).

Note that edges added in lines 20 and 27 are not necessary if we only need to make all nodes reach the exact consensus. Nonetheless, these edges are effective in keeping the parameters of nodes close in value to each other in decentralized learning because the parameters are updated by gradient descent before the parameter exchange with neighbors. For instance, edge \((1,2)\) in \(G^{(3)}\), which is added in line 20, is not necessary for finite-time convergence because nodes \(1\) and \(2\) already have the same parameter after exchanging parameters in \(G^{(1)}\) and \(G^{(2)}\). We provide more examples in Sec. C.

```
1:Input: the set of nodes \(V\) and number of nodes \(n(=a_{1}(k+1)^{p_{1}}+a_{2}(k+1)^{p_{2}}+\cdots+a_{L}(k+1)^{p_{L}})\) such that \(p_{1}>p_{2}>\cdots>p_{L}\geq 0\) and \(a_{l}\in[k]\) for all \(l\in[L]\).
2:If all prime factors of \(n\) are less than or equal to \(k+1\) then return\(\mathcal{H}_{k}(V)\).
3:Split \(V\) into disjoint subsets \(V_{1},\cdots,V_{L}\) such that \(|V_{l}|=a_{l}(k+1)^{p_{l}}\) for all \(l\in[L]\). Then, for all \(l\in[L]\), split \(V_{l}\) into disjoint subsets \(V_{l,1},\cdots,V_{l,a_{l}}\) such that \(|V_{l,a_{l}}|=(k+1)^{p_{l}}\) for all \(a\in[a_{l}]\).
4:Construct \(k\)-peer Hyper-hypercube Graph \(\mathcal{H}_{k}(V_{l})\) for all \(l\in[L]\) and \(m_{1}=|\mathcal{H}_{k}(V_{1})|\).
5:Construct \(k\)-peer Hyper-hypercube Graph \(\mathcal{H}_{k}(V_{l,a})\) for all \(l\in[L]\) and \(a\in[a_{l}]\).
6:Initialize \(b_{l}\) as zero for all \(l\in[L]\), and initialize \(m\) as zero.
7:while\(b_{l}<|\mathcal{H}_{k}(V_{1,1})|\)do
8:\(m\gets m+1\) and \(E^{(m)}\leftarrow\emptyset\).
9:for\(l\in\{L,L-1,\cdots,1\}\)do
10:if\(m\leq m_{1}\)then
11: Add \(E(\mathcal{H}_{k}(V_{l})^{(m^{\prime})})\) to \(E^{(m)}\) where \(m^{\prime}=\text{mod}(m-1,|\mathcal{H}_{k}(V_{l})|)+1\).
12:elseif\(m<m_{1}+l\)then
13:for\(v\in V_{l}\)do
14: Select isolated node \(u_{1},\cdots,u_{a_{m-m_{1}}}\) from \(V_{m-m_{1},1},\cdots,V_{m-m_{1},a_{m-m_{1}}}\).
15: Add edges \((v,u_{1})\), \(\cdots\), \((v,u_{a_{m-m_{1}}})\) with weight \(\frac{|V_{m-m_{1}}|}{a_{m-m_{1}}\sum_{l=m-m_{1}}^{L}|V_{l}|}\) to \(E^{(m)}\).
16:elseif\(m=m_{1}+l\) and \(l\neq L\)then
17:while There are two or more isolated nodes in \(V_{l}\)do
18:\(c\leftarrow\) the number of isolated nodes in \(V_{l}\).
19: Select \(\min\{k+1,c\}\) isolated nodes \(V^{\prime}\) in \(V_{l}\).
20: Add edges with weights \(\frac{1}{|V^{\prime}|}\) to \(E^{(m)}\) such that \(V^{\prime}\) compose the complete graph.
21:else
22:\(b_{l}\gets b_{l}+1\).
23:if\(p_{l}\neq 0\)then
24:for\(a\in[a_{l}]\)do
25: Add \(E(\mathcal{H}_{k}(V_{l,a})^{(m^{\prime})})\) to \(E^{(m)}\) where \(m^{\prime}=\text{mod}(b_{l}-1,|\mathcal{H}_{k}(V_{l,a})|)+1\).
26:else
27: Add \(E(\mathcal{H}_{k}(V_{l})^{(m^{\prime})})\) to \(E^{(m)}\) where \(m^{\prime}=\text{mod}(b_{l}-1,|\mathcal{H}_{k}(V_{l})|)+1\).
28:return\((E^{(1)},E^{(2)},\cdots,E^{(m)})\). ```

**Algorithm 2** Simple Base-\((k+1)\) Graph \(\mathcal{A}_{k}^{\text{simple}}(V)\)

Figure 3: Simple Base-\(2\) Graph with \(n=5(=2^{2}+1)\). The value on the edge is the edge weight, and the edges are colored in the same color as the line in Alg. 2 where they were added.

### Base-\((k+1)\) Graph

The Simple Base-\((k+1)\) Graph is finite-time convergence for any \(n\) and \(k\), while the Simple Base-\((k+1)\) Graph contains graphs that are not necessary for the finite-time convergence and becomes a redundant sequence of graphs in some cases, e.g., \((k,n)=(1,6)\) (see the example in Fig. 3(b) and a detailed explanation in Sec. C.2). To remove this redundancy, this section proposes the Base-\((k+1)\) Graph that can make all nodes reach the exact consensus after fewer iterations than the Simple Base-\((k+1)\) Graph.

The pseudo-code for constructing the Base-\((k+1)\) Graph is shown in Alg. 3. The Base-\((k+1)\) Graph consists of the following three steps.

1. We decompose \(n\) as \(p\times q\) such that \(p\) is a multiple of \(2,\cdots,(k+1)\) and \(q\) is prime to \(2,\cdots,(k+1)\), and split \(V\) into disjoint subsets \(V_{1},\cdots,V_{p}\) such that \(|V_{l}|=q\) for all \(l\in[p]\).
2. For all \(l\in[p]\), we make all nodes in \(V_{l}\) reach the average in \(V_{l}\) by the Simple Base-\((k+1)\) Graph \(\mathcal{A}_{k}^{\text{simple}}(V_{l})\). Then, we take \(p\) nodes from \(V_{1},\cdots,V_{p}\) respectively and construct a set \(U_{1}\). Similarly, we construct \(U_{2},\cdots,U_{q}\) such that \(U_{1},\cdots,U_{q}\) are disjoint sets.
3. For all \(l\in[q]\), we make all nodes in \(U_{l}\) reach the average in \(U_{l}\) by the \(k\)-peer Hyper-hypercube Graph \(\mathcal{H}_{k}(U_{l})\). Because the average in \(U_{l}\) is equivalent to the average in \(V\) after step \(2\), all nodes reach the exact consensus.

Using the example in Fig. 3(a), we explain the Base-\((k+1)\) Graph in more detail. Let \(G^{(1)},\cdots,G^{(4)}\) denote the graphs in Fig. 3(a) from left to right, respectively. In step \(1\), we split \(V\) into \(V_{1}\coloneqq\{1,2,3\}\) and \(V_{2}\coloneqq\{4,5,6\}\). In step \(2\), nodes in \(V_{1}\) and nodes in \(V_{2}\) have the same parameter respectively by

Figure 4: Comparison of Simple Base-\(2\) Graph and Base-\(2\) Graph with \(n=6\). The value on the edge indicates the edge weight. The edges added in line 10 in Alg. 3 are colored black, and the edges added in line 6 are colored the same color as the line in Alg. 2 where they are added.

exchanging parameters on \(G^{(1)},\cdots,G^{(3)}\) because the subgraphs composed on \(V_{1}\) and \(V_{2}\) are same as the Simple Base-\((k+1)\) Graph (see Fig. 13(a)). Then, we construct \(U_{1}\coloneqq\{1,4\}\), \(U_{2}\coloneqq\{2,5\}\), and \(U_{3}\coloneqq\{3,6\}\). Finally, in step \(3\), all nodes reach the exact consensus by exchanging parameters in \(G^{(4)}\).

Fig. 5 and Sec. F.1 compare the Base-\((k+1)\) Graph with the Simple Base-\((k+1)\) Graph, demonstrating that the length of the Base-\((k+1)\) Graph is less than that of the Simple Base-\((k+1)\) Graph in many cases. Moreover, Theorem 1 show the upper bound of the length of the Simple Base-\((k+1)\) Graph and Base-\((k+1)\) Graph. The proof is provided in Sec. D.

**Theorem 1**.: _For any number of nodes \(n\in\mathbb{N}\) and maximum degree \(k\in[n-1]\), the length of the Simple Base-\((k+1)\) Graph and Base-\((k+1)\) Graph is less than or equal to \(2\log_{k+1}(n)+2\)._

**Corollary 1**.: _For any number of nodes \(n\in\mathbb{N}\) and maximum degree \(k\in[n-1]\), the Simple Base-\((k+1)\) Graph and Base-\((k+1)\) Graph are \(\mathcal{O}(\log_{k+1}(n))\)-finite time convergence._

Therefore, the Base-\((k+1)\) Graph is a powerful extension of the \(1\)-peer exponential [43] and \(1\)-peer hypercube graphs [31] because they are \(\mathcal{O}(\log_{2}(n))\)-finite time convergence only if \(n\) is a power of 2 and their maximum degree cannot be set to any number other than \(1\).

## 5 Decentralized SGD on Base-\((k+1)\) Graph

In this section, we verify the effectiveness of the Base-\((k+1)\) Graph for decentralized learning, demonstrating that the Base-\((k+1)\) Graph can endow DSGD with both a faster convergence rate and fewer communication costs than the existing topologies, including the ring, torus, and exponential graph. We consider the following decentralized learning problem:

\[\min_{\bm{x}\in\mathbb{R}^{d}}\left[f(\bm{x})\coloneqq\frac{1}{n}\sum_{i=1}^ {n}f_{i}(\bm{x})\right],\ \ f_{i}(\bm{x})\coloneqq\mathbb{E}_{\xi_{i}\sim\mathcal{D}_{i}}\left[F_{i}( \bm{x};\xi_{i})\right],\]

where \(n\) is the number of nodes, \(f_{i}\) is the loss function of node \(i\), \(\mathcal{D}_{i}\) is the data distribution held by node \(i\), \(F_{i}(\bm{x};\xi_{i})\) is the loss of node \(i\) at data sample \(\xi_{i}\), and \(\nabla F_{i}(\bm{x};\xi_{i})\) denotes the stochastic gradient. Then, we assume that the following hold, which are commonly used for analyzing decentralized learning methods [18; 20; 23; 43].

**Assumption 1**.: _There exists \(f^{\star}>-\infty\) that satisfies \(f(\bm{x})\geq f^{\star}\) for any \(\bm{x}\in\mathbb{R}^{d}\)._

**Assumption 2**.: \(f_{i}\) _is \(L\)-smooth for all \(i\in[n]\)._

**Assumption 3**.: _There exists \(\sigma^{2}\) that satisfies \(\mathbb{E}_{\xi_{i}\sim\mathcal{D}_{i}}\|\nabla F_{i}(\bm{x};\xi_{i})-\nabla f _{i}(\bm{x})\|^{2}\leq\sigma^{2}\) for all \(\bm{x}\in\mathbb{R}^{d}\)._

**Assumption 4**.: _There exists \(\zeta^{2}\) that satisfies \(\frac{1}{n}\sum_{i=1}^{n}\|\nabla f_{i}(\bm{x})-\nabla f(\bm{x})\|^{2}\leq \zeta^{2}\) for all \(\bm{x}\in\mathbb{R}^{d}\)._

We consider the case when DSGD [18], the most widely used decentralized learning method, is used as an optimization method. Let \(\bm{W}^{(1)},\cdots,\bm{W}^{(m)}\) be mixing matrices of the Base-\((k+1)\) Graph. In DSGD on the Base-\((k+1)\) Graph, node \(i\) updates its parameter \(\bm{x}_{i}\) as follows:

\[\bm{x}_{i}^{(r+1)}=\sum_{j=1}^{n}W_{ij}^{(1+\text{mod}(r,m))}\left(\bm{x}_{j} ^{(r)}-\eta\nabla F_{j}(\bm{x}_{j}^{(r)};\xi_{j}^{(r)})\right),\] (1)

where \(\eta\) is the learning rate. In this case, thanks to the property of finite-time convergence, DSGD on the Base-\((k+1)\) Graph converges at the following convergence rate.

**Theorem 2**.: _Suppose that Assumptions 1-4 hold. Then, for any number of nodes \(n\in\mathbb{N}\) and maximum degree \(k\in[n-1]\), there exists \(\eta\) such that \(\bar{\bm{x}}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\bm{x}_{i}\) generated by Eq. (1) satisfies \(\frac{1}{R+1}\sum_{r=0}^{R}\mathbb{E}\left\|\nabla f(\bar{\bm{x}}^{(r)})\right\| ^{2}\leq\epsilon\) after_

\[R=\mathcal{O}\left(\frac{\sigma^{2}}{n\epsilon^{2}}+\frac{\zeta\log_{k+1}(n)+ \sigma\sqrt{\log_{k+1}(n)}}{\epsilon^{3/2}}+\frac{\log_{k+1}(n)}{\epsilon} \right)\cdot LF_{0}\] (2)

_iterations, where \(F_{0}\coloneqq f(\bar{\bm{x}}^{(0)})-f^{\star}\)._

Figure 5: Comparison of length.

The above theorem follows immediately from Theorem 2 stated in Koloskova et al. [11] and Corollary 1. The convergence rates of DSGD over commonly used topologies are summarized in Sec. E. From Theorem 2 and Sec. E, we can conclude that for any number of nodes \(n\), the Base-2 Graph enables DSGD to converge faster than the ring and torus and as fast as the exponential graph, although the maximum degree of the Base-\(2\) Graph is only one. Moreover, if we set the maximum degree \(k\) to the value between \(2\) to \(\lceil\log_{2}(n)\rceil\), the Base-\((k+1)\) Graph enables DSGD to converge faster than the exponential graph, even though the maximum degree of the Base-\((k+1)\) Graph remains less than that of the exponential graph. It is worth noting that if we increase the maximum degree of the \(1\)-peer exponential and \(1\)-peer hypercube graphs (i.e., \(k\)-peer exponential and \(k\)-peer hypercube graphs with \(k\geq 2\)), these topologies cannot enable DSGD to converge faster than the exponential graph because these topologies are no longer finite-time convergence even when the number of nodes is a power of \(2\).

## 6 Experiments

In this section, we validate the effectiveness of the Base-\((k+1)\) Graph. First, we experimentally verify that the Base-\((k+1)\) Graph is finite-time convergence for any number of nodes in Sec. 6.1, and we verify the effectiveness of the Base-\((k+1)\) Graph for decentralized learning in Sec. 6.2.

Figure 6: Comparison of consensus rates among various topologies. The number in the bracket indicates the maximum degree of a topology. Because the maximum degree of the exponential graph depends on \(n\), the three numbers in the bracket indicate the maximum degree for each \(n\).

Figure 7: Test accuracy (%) of DSGD on various topologies with \(n=25\). The number in the bracket indicates the maximum degree of a topology. We also compared with dense variants of the \(1\)-peer {U, D}-EquiDyn [33] in Sec. F.3.1, showing the superior performance of the Base-\((k+1)\) Graph.

### Consensus Rate

**Setup.** Let \(x_{i}\in\mathbb{R}\) be the parameter that node \(i\) has, and let \(\bar{x}\coloneqq\frac{1}{n}\sum_{i=1}^{n}x_{i}\). For each \(i\), the initial value of \(x_{i}\) was drawn from Gaussian distribution with mean \(0\) and standard variance \(1\). Then, we evaluated how the consensus error \(\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\) decreases when \(x_{i}\) is updated as \(x_{i}\leftarrow\sum_{j=1}^{n}W_{ij}x_{j}\) where \(\bm{W}\) is the mixing matrix associated with a given topology.

**Results.** Figs. 1 and 6 present how the consensus errors decrease on various topologies. The results indicate that the Base-\((k+1)\) Graph reaches the exact consensus after a finite number of iterations, while the other topologies only reach the consensus asymptotically. Moreover, as the maximum degree \(k\) increases, the Base-\((k+1)\) Graph reaches the exact consensus with fewer iterations. We also present the results when \(n\) is a power of 2 in Sec. F.2, demonstrating that the \(1\)-peer exponential graph can reach the exact consensus as well as the Base-\(2\) Graph, but requires more iterations than the Base-\(4\) Graph.

### Decentralized Learning

Next, we examine the effectiveness of the Base-\((k+1)\) Graph in decentralized learning.

**Setup.** We used three datasets, Fashion MNIST [41], CIFAR-\(\{10,100\}\)[14], and used LeNet [15] for Fashion MNIST and VGG-11 [32] for CIFAR-\(\{10,100\}\). Additionally, we present the results using ResNet-18 [5] in Sec. G. The learning rate was tuned by the grid search and we used the cosine learning rate scheduler [22]. We distributed the training dataset to nodes by using Dirichlet distributions with hyperparameter \(\alpha\)[7], conducting experiments in both homogeneous and heterogeneous data distribution settings. As \(\alpha\) approaches zero, the data distributions held by each node become more heterogeneous. We repeated all experiments with three different seed values and reported their averages. See Sec. H for more detailed settings.

**Results of DSGD on Various Topologies.** We compared various topologies combined with the DSGD with momentum [4, 18], showing the results in Fig. 7. From Fig. 7, the accuracy differences among topologies are larger as the data distributions are more heterogeneous. From Fig. 7b, the Base-\(\{2,3,4,5\}\) Graph reach high accuracy faster than the other topologies. Furthermore, comparing the final accuracy, the final accuracy of the Base-\(2\) Graph is comparable to or higher than that of the existing topologies, including the exponential graph.

Moreover, the final accuracy of the Base-\(\{3,4,5\}\) Graph is higher than that of all existing topologies. From Fig. 7a, the accuracy differences among topologies become small when \(\alpha=10\); however, the Base-\(5\) Graph still outperforms the other topologies. In Fig. 8, we present the results in cases other than \(n=25\), demonstrating that the Base-\(2\) Graph outperforms the \(1\)-peer exponential graph and the Base-\(\{3,4,5\}\) Graph can consistently outperform the exponential and \(1\)-peer exponential graphs for all \(n\). In Sec. F.3.2, we show the learning curves and the comparison of the consensus rate when \(n\) is \(21\), \(22\), \(23\), \(24\), and \(25\).

**Results of \(D^{2}\) and QG-DSGDm on Various Topologies.** The above results demonstrated that the Base-\((k+1)\) Graph outperforms the existing topologies, especially when the data distributions

Figure 8: Test accuracy (%) of DSGD with CIFAR-10 and \(\alpha=0.1\).

Figure 9: Test accuracy (%) of \(D^{2}\) and QG-DSGDm with CIFAR-10, \(n=25\), and \(\alpha=0.1\).

performs the exponential graph. Thus, the Base-\((k+1)\) Graph is useful not only for DSGD but also for \(D^{2}\) and QG-DSGDm and then enables these methods to achieve a reasonable balance between accuracy and communication efficiency.

## 7 Conclusion

In this study, we propose the Base-\((k+1)\) Graph, a novel topology with both a fast consensus rate and small maximum degree. Unlike the existing topologies, the Base-\((k+1)\) Graph is finite-time convergence for any number of nodes and maximum degree \(k\). Thanks to this favorable property, the Base-\((k+1)\) Graph enables DSGD to obtain both a faster convergence rate and more communication efficiency than the existing topologies, including the ring, torus, and exponential graph. Through experiments, we compared the Base-\((k+1)\) Graph with various existing topologies, demonstrating that the Base-\((k+1)\) Graph enables various decentralized learning methods to more successfully reconcile accuracy and communication efficiency than the existing topologies.

## Acknowledgments

Yuki Takezawa, Ryoma Sato, and Makoto Yamada were supported by JSPS KAKENHI Grant Number 23KJ1336, 21J22490, and MEXT KAKENHI Grant Number 20H04243, respectively.

## References

* Assran et al. (2019) Assran, M., Loizou, N., Ballas, N., and Rabbat, M. (2019). Stochastic gradient push for distributed deep learning. In _International Conference on Machine Learning_.
* Chen et al. (2021) Chen, Y., Yuan, K., Zhang, Y., Pan, P., Xu, Y., and Yin, W. (2021). Accelerating gossip sgd with periodic global averaging. In _International Conference on Machine Learning_.
* Cyffers et al. (2022) Cyffers, E., Even, M., Bellet, A., and Massoulie, L. (2022). Muffliato: Peer-to-peer privacy amplification for decentralized optimization and averaging. In _Advances in Neural Information Processing Systems_.
* Gao and Huang (2020) Gao, H. and Huang, H. (2020). Periodic stochastic gradient descent with momentum for decentralized training. In _arXiv_.
* He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_.
* Horvath and Richtarik (2021) Horvath, S. and Richtarik, P. (2021). A better alternative to error feedback for communication-efficient distributed learning. In _International Conference on Learning Representations_.
* Hsu et al. (2019) Hsu, T.-M. H., Qi, and Brown, M. (2019). Measuring the effects of non-identical data distribution for federated visual classification. In _arXiv_.
* Kairouz et al. (2021) Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., D'Oliveira, R. G. L., Eichner, H., Rouayheb, S. E., Evans, D., Gardner, J., Garrett, Z., Gascon, A., Ghazi, B., Gibbons, P. B., Gruteser, M., Harchaoui, Z., He, C., He, L., Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Khodak, M., Konecny, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T., Liu, Y., Mittal, P., Mohri, M., Nock, R., Ozgur, A., Pagh, R., Qi, H., Ramage, D., Raskar, R., Raykova, M., Song, D., Song, W., Stich, S. U., Sun, Z., Suresh, A. T., Tramer, F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang, Q., Yu, F. X., Yu, H., and Zhao, S. (2021). Advances and open problems in federated learning. In _Foundations and Trends in Machine Learning_.
* Karimireddy et al. (2020) Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A. T. (2020). SCAF-FOLD: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_.
* Koloskova et al. (2020a) Koloskova, A., Lin, T., Stich, S. U., and Jaggi, M. (2020a). Decentralized deep learning with arbitrary communication compression. In _International Conference on Learning Representations_.

* Koloskova et al. [2020b] Koloskova, A., Loizou, N., Boreiri, S., Jaggi, M., and Stich, S. (2020b). A unified theory of decentralized SGD with changing topology and local updates. In _International Conference on Machine Learning_.
* Koloskova et al. [2019] Koloskova, A., Stich, S., and Jaggi, M. (2019). Decentralized stochastic optimization and gossip algorithms with compressed communication. In _International Conference on Machine Learning_.
* Kong et al. [2021] Kong, L., Lin, T., Koloskova, A., Jaggi, M., and Stich, S. (2021). Consensus control for decentralized deep learning. In _International Conference on Machine Learning_.
* Krizhevsky [2009] Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report.
* LeCun et al. [1998] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. In _IEEE_.
* Li et al. [2020] Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V. (2020). Federated optimization in heterogeneous networks. In _arXiv_.
* Li et al. [2019] Li, Z., Shi, W., and Yan, M. (2019). A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates. In _IEEE Transactions on Signal Processing_.
* Lian et al. [2017] Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J. (2017). Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In _Advances in Neural Information Processing Systems_.
* Lian et al. [2018] Lian, X., Zhang, W., Zhang, C., and Liu, J. (2018). Asynchronous decentralized parallel stochastic gradient descent. In _International Conference on Machine Learning_.
* Lin et al. [2021] Lin, T., Karimireddy, S. P., Stich, S., and Jaggi, M. (2021). Quasi-global momentum: Accelerating decentralized deep learning on heterogeneous data. In _International Conference on Machine Learning_.
* Lorenzo and Scutari [2016] Lorenzo, P. D. and Scutari, G. (2016). Next: In-network nonconvex optimization. In _IEEE Transactions on Signal and Information Processing over Networks_.
* Loshchilov and Hutter [2017] Loshchilov, I. and Hutter, F. (2017). SGDR: Stochastic gradient descent with warm restarts. In _International Conference on Learning Representations_.
* Lu and De Sa [2020] Lu, Y. and De Sa, C. (2020). Moniqua: Modulo quantized communication in decentralized SGD. In _International Conference on Machine Learning_.
* Lu and De Sa [2021] Lu, Y. and De Sa, C. (2021). Optimal complexity in decentralized training. In _International Conference on Machine Learning_.
* Marfoq et al. [2020] Marfoq, O., Xu, C., Neglia, G., and Vidal, R. (2020). Throughput-optimal topology design for cross-silo federated learning. In _Advances in Neural Information Processing Systems_.
* McMahan et al. [2017] McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. y. (2017). Communication-efficient learning of deep networks from decentralized data. In _International Conference on Artificial Intelligence and Statistics_.
* Murata and Suzuki [2021] Murata, T. and Suzuki, T. (2021). Bias-variance reduced local sgd for less heterogeneous federated learning. In _International Conference on Machine Learning_.
* Nedic et al. [2018] Nedic, A., Olshevsky, A., and Rabbat, M. G. (2018). Network topology and communication-computation tradeoffs in decentralized optimization. In _IEEE_.
* Nedic et al. [2017] Nedic, A., Olshevsky, A., and Shi, W. (2017). Achieving geometric convergence for distributed optimization over time-varying graphs. In _SIAM Journal on Optimization_.
* Pu and Nedic [2021] Pu, S. and Nedic, A. (2021). Distributed stochastic gradient tracking methods. In _Mathematical Programming_.

* Shi et al. [2016] Shi, G., Li, B., Johansson, M., and Johansson, K. H. (2016). Finite-time convergent gossiping. In _IEEE Transactions on Networking_.
* Simonyan and Zisserman [2015] Simonyan, K. and Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. In _International Conference on Learning Representations_.
* Song et al. [2022] Song, Z., Li, W., Jin, K., Shi, L., Yan, M., Yin, W., and Yuan, K. (2022). Communication-efficient topologies for decentralized learning with \(O(1)\) consensus rate. In _Advances in Neural Information Processing Systems_.
* Takezawa et al. [2023] Takezawa, Y., Bao, H., Niwa, K., Sato, R., and Yamada, M. (2023). Momentum tracking: Momentum acceleration for decentralized deep learning on heterogeneous data. In _Transactions on Machine Learning Research_.
* Tang et al. [2018a] Tang, H., Gan, S., Zhang, C., Zhang, T., and Liu, J. (2018a). Communication compression for decentralized training. In _Advances in Neural Information Processing Systems_.
* Tang et al. [2018b] Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J. (2018b). \(D^{2}\): Decentralized training over decentralized data. In _International Conference on Machine Learning_.
* Vogels et al. [2021] Vogels, T., He, L., Koloskova, A., Karimireddy, S. P., Lin, T., Stich, S. U., and Jaggi, M. (2021). Relayum for decentralized deep learning on heterogeneous data. In _Advances in Neural Information Processing Systems_.
* Vogels et al. [2020] Vogels, T., Karimireddy, S. P., and Jaggi, M. (2020). Practical low-rank communication compression in decentralized deep learning. In _Advances in Neural Information Processing Systems_.
* Wang et al. [2019] Wang, J., Sahu, A. K., Yang, Z., Joshi, G., and Kar, S. (2019). Matcha: Speeding up decentralized sgd via matching decomposition sampling. In _Indian Control Conference_.
* Wu and He [2018] Wu, Y. and He, K. (2018). Group normalization. In _European Conference on Computer Vision_.
* Xiao et al. [2017] Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. In _arXiv_.
* Xin and Khan [2020] Xin, R. and Khan, U. A. (2020). Distributed heavy-ball: A generalization and acceleration of first-order methods with gradient tracking. In _IEEE Transactions on Automatic Control_.
* Ying et al. [2021] Ying, B., Yuan, K., Chen, Y., Hu, H., Pan, P., and Yin, W. (2021). Exponential graph is provably efficient for decentralized deep training. In _Advances in Neural Information Processing Systems_.
* Yu et al. [2019] Yu, H., Jin, R., and Yang, S. (2019). On the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization. In _International Conference on Machine Learning_.
* Yuan et al. [2021] Yuan, K., Chen, Y., Huang, X., Zhang, Y., Pan, P., Xu, Y., and Yin, W. (2021). DecentLaM: Decentralized momentum sgd for large-batch deep training. In _International Conference on Computer Vision_.
* Yuan et al. [2019] Yuan, K., Ying, B., Zhao, X., and Sayed, A. H. (2019). Exact diffusion for distributed optimization and learning--part i: Algorithm development. In _IEEE Transactions on Signal Processing_.
* Zhao et al. [2022] Zhao, H., Li, B., Li, Z., Richtarik, P., and Chi, Y. (2022). BEER: Fast \(O(1/T)\) rate for decentralized nonconvex optimization with communication compression. In _Advances in Neural Information Processing Systems_.
* Zhu et al. [2022] Zhu, T., He, F., Zhang, L., Niu, Z., Song, M., and Tao, D. (2022). Topology-aware generalization of decentralized SGD. In _International Conference on Machine Learning_.

Detailed Explanation of \(k\)-peer Hyperhypercube Graph

In this section, we explain Alg. 1 in more detail. The \(k\)-peer Hyper-hypercube Graph mainly consists of the following five steps.

1. Decompose \(n\) as \(n=n_{1}\times\cdots\times n_{L}\) with minimum \(L\) such that \(n_{l}\in[k+1]\) for all \(l\in[L]\).
2. If \(L=1\), we make all nodes obtain the average of parameters in \(V\) by using the complete graph. If \(L\geq 2\), we split \(V\) into disjoint subsets \(V_{1},\cdots,V_{n_{L}}\) such that \(|V_{l}|=\frac{n}{n_{L}}\) for all \(l\in[n_{L}]\) and continue to step \(3\).
3. For all \(l\in[n_{L}]\), we make all nodes in \(V_{l}\) obtain the average of parameters in \(V_{l}\) by using the \(k\)-peer Hyper-hypercube Graph \(\mathcal{H}_{k}(V_{l})\).
4. We take \(n_{L}\) nodes from \(V_{1},\cdots,V_{n_{L}}\) respectively and construct a set \(U_{1}\). Similarly, we construct \(U_{2},\cdots,U_{n_{L}}\) such that \(U_{1},\cdots,U_{n_{L}}\) are disjoint sets.
5. For all \(l\in[n_{L}]\), we make all nodes in \(U_{l}\) obtain the average of parameters in \(U_{l}\) by using the complete graph. Because the average of parameter \(U_{l}\) is equivalent to the average in \(V\) after step \(4\), all nodes reach the exact consensus.

When \(n\leq k+1\), the \(k\)-peer Hyper-hypercube Graph becomes the complete graph because of step 2. When \(n>k+1\), we decompose \(n\) in step \(1\) and construct the \(k\)-peer Hyper-hypercube Graph recursively in step \(3\). Thus, the \(k\)-peer Hyper-hypercube Graph can make all nodes reach the exact consensus by the sequence of \(L\) graphs.

Using the example provided in Fig. 10, we explain the \(k\)-peer Hyper-hypercube Graph in a more detailed manner. When \(n=12\), we decompose \(12\) as \(2\times 2\times 3\). In step 2, we split \(V\coloneqq\{1,\cdots,12\}\) into \(V_{1}\coloneqq\{1,\cdots,4\}\), \(V_{2}\coloneqq\{5,\cdots,8\}\), and \(V_{3}\coloneqq\{9,\cdots,12\}\). Step 3 corresponds to the first two graphs in Fig. 9(b). As shown in Fig. 9(a), the subgraphs consisting of \(V_{1}\), \(V_{2}\), and \(V_{3}\) in the first two graphs in Fig. 9(b) are equivalent to the \(k\)-peer Hyper-hypercube Graph with the number of nodes \(4\). Thus, all nodes reach the exact consensus by exchanging parameters in Fig. 9(b).

Figure 10: Illustration of the \(2\)-peer Hyper-hypercube Graph. In Fig. 9(a), all edge weights are \(\frac{1}{2}\). In Fig. 9(b), edge weights are \(\frac{1}{2}\) in the first two graphs and \(\frac{1}{3}\) in the last graph.

[MISSING_PAGE_FAIL:14]

Illustration of Topologies

### Examples

Fig. 12 shows the examples of the Simple Base-\((k+1)\) Graph. Using these examples, we explain how all nodes reach the exact consensus.

We explain the case depicted in Fig. 11(a). Let \(G^{(1)},G^{(2)},G^{(3)}\) denote the graphs depicted in Fig. 11(a) from left to right, respectively. First, we split \(V\coloneqq\{1,\cdots,5\}\) into \(V_{1}\coloneqq\{1,2,3\}\) and \(V_{2}\coloneqq\{4,5\}\), and then split \(V_{2}\) into \(V_{2,1}\coloneqq\{4\}\) and \(V_{2,2}\coloneqq\{5\}\). After exchanging parameters in \(G^{(1)}\), nodes in \(V_{1}\) and nodes in \(V_{2}\) have the same parameter respectively. Then, after exchanging parameters in \(G^{(2)}\), the average in \(V_{1}\), that in \(V_{2,1}\), and that in \(V_{2,2}\) become same as the average in \(V\). Thus, by exchanging parameters in \(G^{(3)}\), all nodes reach the exact consensus. Note that edge \((4,5)\) in \(G^{(3)}\), which is added in line 27 in Alg. 2, is not necessary for all nodes to reach the exact consensus because nodes \(4\) and \(5\) already have the same parameter after exchanging parameters in \(G^{(2)}\); however, it is effective in decentralized learning as we explained in Sec. 4.2.

### Illustrative Comparison between Simple Base-\((k+1)\) and Base-\((k+1)\) Graphs

In this section, we provide an example of the Simple Base-\((k+1)\) Graph, explaining the reason why the length of the Base-\((k+1)\) Graph is less than that of the Simple Base-\((k+1)\) Graph.

Let \(G^{(1)},\cdots,G^{(5)}\) denote the graphs depicted in Fig. 13 from left to right, respectively. \((G^{(1)},G^{(2)},G^{(3)},G^{(4)},G^{(5)})\) is finite-time convergence, but \((G^{(1)},G^{(2)},G^{(3)},G^{(5)})\) is also finite-time convergence because after exchanging parameters in \(G^{(3)}\), nodes \(3\) and \(4\) already have the same parameters. Then, using the technique proposed in Sec. 4.3, we can remove such unnecessary graphs contained in the Simple Base-\((k+1)\) Graph (see Fig. 3(a)). Consequently, the Base-\((k+1)\) Graph can make all nodes reach the exact consensus faster than the Simple Base-\((k+1)\) Graph.

Figure 12: Illustration of the Simple Base-\((k+1)\) Graph. The edge is colored in the same color as the line of Alg. 2 where the edge is added. The value on the edge indicates the edge weight. For simplicity, we omit the edge value when it is \(\frac{1}{3}\).

Figure 13: Illustration of the Simple Base-\(2\) Graph with \(n=6(=2^{2}+2)\). The edge is colored in the same color as the line of Alg. 2 where the edge is added.

### Additional Examples

#### c.3.1 Simple Base-\((k+1)\) Graph

Figure 14: Illustration of the Simple Base-\(2\) Graph with the various numbers of nodes.

Figure 15: Illustration of the Simple Base-3 Graph with the various numbers of nodes.

#### c.3.2 Base-\((k+1)\) Graph

Figure 16: Illustration of the Base-2 Graph with the various numbers of nodes.

Figure 17: Illustration of the Base-\(3\) Graph with the various numbers of nodes.

### \(1\)-peer Hypercube Graph and \(1\)-peer Exponential Graph

For completeness, we provide examples of the \(1\)-peer hypercube [31] and \(1\)-peer exponential graphs [43] in Figs. 19 and 18, respectively.

Figure 19: Illustration of the \(1\)-peer exponential graph. All edge weights are \(0.5\).

Figure 18: Illustration of the \(1\)-peer hypercube graph. All edge weights are \(0.5\).

Proof of Theorem 1

**Lemma 1** (Length of \(k\)-peer Hyper-hypercube Graph).: _Suppose that all prime factors of the number of nodes \(n\) are less than or equal to \(k+1\). Then, for any number of nodes \(n\in\mathbb{N}\) and maximum degree \(k\in[n-1]\), the length of the \(k\)-peer Hyper-hypercube Graph is less than or equal to \(\max\{1,2\log_{k+2}(n)\}\)._

Proof.: We assume that \(n\) is decomposed as \(n=n_{1}\times\cdots\times n_{L}\) with minimum \(L\) where \(n_{l}\in[k+1]\) for all \(l\in[L]\). Without loss of generality, we suppose \(n_{1}\leq n_{2}\leq\cdots\leq n_{L}\). Then, for any \(i\neq j\), it holds that \(n_{i}\times n_{j}\geq k+2\) because if \(n_{i}\times n_{j}\leq k+1\) for some \(i\) and \(j\), this contradicts the assumption that \(L\) is minimum.

When \(L\) is even, we have

\[n=(n_{1}\times n_{2})\times\cdots\times(n_{L-1}\times n_{L})\geq(k+2)^{\frac{ k}{2}}.\]

Then, we get \(L\leq 2\log_{k+2}(n)\).

Next, we discuss the case when \(L\) is odd. When \(L\geq 3\), \(n_{L}\geq\sqrt{k+2}\) holds because \(n_{L-2}\times n_{L-1}\geq k+2\). Thus, we get

\[n=(n_{1}\times n_{2})\times\cdots\times(n_{L-2}\times n_{L-1})\times n_{L}\geq (k+2)^{\frac{L-1}{2}}\times n_{L}\geq(k+2)^{\frac{L}{2}}.\]

Then, we get \(L\leq 2\log_{k+2}(n)\) when \(L\geq 3\).

Thus, given the case when \(L=1\), the length of the \(k\)-peer Hyper-hypercube Graph is less than or equal to \(\max\{1,2\log_{k+2}(n)\}\). 

**Lemma 2** (Length of Simple Base-\((k+1)\) Graph).: _For any number of nodes \(n\in\mathbb{N}\) and maximum degree \(k\in[n-1]\), the length of the Simple Base-\((k+1)\) Graph is less than or equal to \(2\log_{k+1}(n)+2\)._

Proof.: When all prime factors of \(n\) are less than or equal to \(k+1\), the Simple Base-\((k+1)\) Graph is equivalent to the \(k\)-peer Hyper-hypercube Graph and the statement holds from Lemma 1. In the following, we consider the case when there exists a prime factor of \(n\) that is larger than \(k+1\). Note that because when \(L=1\) (i.e., \(n=a_{1}\times(k+1)^{p_{1}}\)), all prime factors of \(n\) are less than or equal to \(k+1\), we only need to consider the case when \(L\geq 2\). We have the following inequality:

\[\log_{k+1}(n) =\log_{k+1}(a_{1}(k+1)^{p_{1}}+\cdots+a_{L}(k+1)^{p_{L}})\] \[\geq p_{1}+\log_{k+1}(a_{1})\] \[\geq p_{1}.\]

Then, because \(|V_{1}|=a_{1}\times(k+1)^{p_{1}}\), it holds that \(m_{1}=|\mathcal{H}_{k}(V_{1})|\leq 1+p_{1}\leq\log_{k+1}(n)+1\). Similarly, it holds that \(|\mathcal{H}_{k}(V_{1,1})|=p_{1}\leq\log_{k+1}(n)\) because \(|V_{1,1}|=(k+1)^{p_{1}}\). In Alg. 2, the update rule \(b_{1}\gets b_{1}+1\) in line 22 is executed for the first time when \(m=m_{1}+2\) because \(L\geq 2\). Thus, the length of the Simple Base-\((k+1)\) Graph is at most \(m_{1}+|\mathcal{H}_{k}(V_{1,1})|+1\leq 2\log_{k+1}(n)+2\). This concludes the statement. 

**Lemma 3** (Length of Base-\((k+1)\) Graph).: _For any number of nodes \(n\in\mathbb{N}\) and maximum degree \(k\in[n-1]\), the length of the Base-\((k+1)\) Graph is less than or equal to \(2\log_{k+1}(n)+2\)._

Proof.: The statement follows immediately from Lemma 2 and line \(12\) in Alg. 3.

Convergence Rate of DSGD over Various Topologies

Table 2 lists the convergence rates of DSGD over various topologies. These convergence rates can be immediately obtained from Theorem 2 stated in Koloskova et al. [11] and consensus rate of the topology. As seen from Table 2, the Base-2 Graph enables DSGD to converge faster than the ring and torus and as fast as the exponential graph for any number of nodes, although the maximum degree of the Base-\(2\) Graph is only one. Moreover, for any number of nodes, the Base-\((k+1)\) Graph with \(2\leq k<\lceil\log_{2}(n)\rceil\) enables DSGD to converge faster than the exponential graph, even though the maximum degree of the Base-\((k+1)\) Graph remains to be less than that of the exponential graph.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Topology** & **Convergence Rate** & **Maximum Degree** & **\#Nodes \(n\)** \\ \hline Ring [28] & \(\mathcal{O}\left(\frac{\sigma^{2}}{mc^{2}}+\frac{\zeta n^{2}+\sigma n}{\epsilon ^{3/2}}+\frac{n^{2}}{\epsilon}\right)\cdot LF_{0}\) & 2 & \(\forall n\in\mathbb{N}\) \\ Torus [28] & \(\mathcal{O}\left(\frac{\sigma^{2}}{mc^{2}}+\frac{\zeta n+\sigma\sqrt{n}}{ \epsilon^{3/2}}+\frac{n}{\epsilon}\right)\cdot LF_{0}\) & 4 & \(\forall n\in\mathbb{N}\) \\ Exp. [43] & \(\mathcal{O}\left(\frac{\sigma^{2}}{mc^{2}}+\frac{\zeta\log_{2}(n)+\sigma \sqrt{\log_{2}(n)}}{\epsilon^{3/2}}+\frac{\log_{2}(n)}{\epsilon}\right)\cdot LF _{0}\) & \(\lceil\log_{2}(n)\rceil\) & \(\forall n\in\mathbb{N}\) \\
1-peer Exp. [43] & \(\mathcal{O}\left(\frac{\sigma^{2}}{mc^{2}}+\frac{\zeta\log_{2}(n)+\sigma \sqrt{\log_{2}(n)}}{\epsilon^{3/2}}+\frac{\log_{2}(n)}{\epsilon}\right)\cdot LF _{0}\) & 1 & A power of 2 \\
1-peer Hypercube [31] & \(\mathcal{O}\left(\frac{\sigma^{2}}{mc^{2}}+\frac{\zeta\log_{2}(n)+\sigma \sqrt{\log_{2}(n)}}{\epsilon^{3/2}}+\frac{\log_{2}(n)}{\epsilon}\right)\cdot LF _{0}\) & 1 & A power of 2 \\
**Base-\((k+1)\) Graph (ours)** & \(\mathcal{O}\left(\frac{\sigma^{2}}{mc^{2}}+\frac{\zeta\log_{k+1}(n)+\sigma \sqrt{\log_{k+1}(n)}}{\epsilon^{3/2}}+\frac{\log_{k+1}(n)}{\epsilon}\right) \cdot LF_{0}\) & \(k\) & \(\forall n\in\mathbb{N}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Convergence rates and maximum degrees of DSGD over various topologies.

Additional Experiments

### Comparison of Base-\((k+1)\) and Simple Base-\((k+1)\) Graphs

Fig. 20 shows the length of the Simple Base-\((k+1)\) Graph and Base-\((k+1)\) Graph. The results indicate that for all \(k\), the length of the Base-\((k+1)\) Graph is less than the length of the Simple Base-\((k+1)\) Graph in many cases.

### Consensus Rate

In Fig. 21, we demonstrate how consensus error decreases on various topologies when the number of nodes \(n\) is a power of \(2\). The results indicate that the Base-\(2\) Graph and \(1\)-peer exponential graph can reach the exact consensus after the same finite number of iterations and reach the consensus faster than other topologies. Note that the Base-\(2\) Graph is equivalent to the \(1\)-peer hypercube graph when \(n\) is a power of \(2\).

Figure 21: Comparison of consensus rates among different topologies when the number of nodes \(n\) is a power of \(2\). Because the Base-\(\{3,5\}\) Graph are the same as the Base-\(\{2,4\}\) Graph, respectively, when \(n\) is a power of \(2\), we omit the results of the Base-\(\{3,5\}\) Graph.

Figure 20: Comparison of the length of the Simple Base-\((k+1)\) Graph and Base-\((k+1)\) Graph.

[MISSING_PAGE_FAIL:24]

Figure 24: Test accuracy (%) of DSGD with CIFAR-10 and \(\alpha=0.1\). The number in the bracket denotes the maximum degree of a topology. When \(n=24\), we omit the results of the Base-\(5\) Graph because the Base-\(5\) Graph and Base-\(4\) Graph are equivalent. When \(n=25\), we omit the results of the Base-\(6\) Graph because the Base-\(6\) Graph and Base-\(5\) Graph are equivalent.

Figure 25: Test accuracy (%) of DSGD with CIFAR-10 and \(n=16\). The number in the bracket is the maximum degree of a topology. We omit the results of the Base-\(3\) Graph and Base-\(5\) Graph because these graphs are equivalent to the Base-\(2\) Graph and Base-\(4\) Graph, respectively.

[MISSING_PAGE_FAIL:26]