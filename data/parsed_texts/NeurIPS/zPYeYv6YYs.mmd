# Conformal PID Control for Time Series Prediction

 Anastasios N. Angelopoulos

University of California, Berkeley

angelopoulos@berkeley.edu

Emmanuel J. Candes

Stanford University

candes@stanford.edu

Ryan J. Tibshirani

University of California, Berkeley

ryantibs@berkeley.edu

###### Abstract

We study the problem of uncertainty quantification for time series prediction, with the goal of providing easy-to-use algorithms with formal guarantees. The algorithms we present build upon ideas from conformal prediction and control theory, are able to prospectively model conformal scores in an online setting, and adapt to the presence of systematic errors due to seasonality, trends, and general distribution shifts. Our theory both simplifies and strengthens existing analyses in online conformal prediction. Experiments on 4-week-ahead forecasting of statewide COVID-19 death counts in the U.S. show an improvement in coverage over the ensemble forecaster used in official CDC communications. We also run experiments on predicting electricity demand, market returns, and temperature using autoregressive, Theta, Prophet, and Transformer models. We provide an extendable codebase for testing our methods and for the integration of new algorithms, data sets, and forecasting rules.1

Footnote 1: http://github.com/aangelopoulos/conformal-time-series

## 1 Introduction

Machine learning models run in production systems regularly encounter data distributions that change over time. This can be due to factors such as seasonality and time-of-day, continual updating and re-training of upstream machine learning models, changing user behaviors, and so on. These distribution shifts can degrade a model's predictive performance. They also invalidate standard techniques for uncertainty quantification, such as _conformal prediction_[36, 35].

To address the problem of shifting distributions, we consider the task of prediction in an adversarial online setting, as in [16]. In this problem setting, we observe a (potentially) adversarial time series of deterministic covariates \(x_{t}\in\mathcal{X}\) and responses \(y_{t}\in\mathcal{Y}\), for \(t\in\mathbb{N}=\{1,2,3,\dots\}\). As in standard conformal prediction, we are free to define any _conformal score function_\(s_{t}:\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}\), which we can view as measuring the accuracy of our forecast at time \(t\). We will assume with a loss of generality that \(s_{t}\) is negatively oriented (lower values mean greater forecast accuracy). For example, we may use the absolute error \(s_{t}(x,y)=|y-f_{t}(x)|\), where \(f_{t}\) is a forecaster trained on data up to but not including data at time \(t\).

The challenge in the sequential setting is as follows. We seek to invert the score function to construct a _conformal prediction set_,

\[\mathcal{C}_{t}=\{y\in\mathcal{Y}:s_{t}(x_{t},y)\leq q_{t}\},\] (1)

where \(q_{t}\) is an estimated \(1-\alpha\) quantile for the distribution of the score \(s_{t}(x_{t},y_{t})\) at time \(t\). In standard conformal prediction, we would take \(q_{t}\) to be a level \(1-\alpha\) sample quantile (up to a finite-sample correction) of \(s_{t}(x_{i},y_{i})\), \(i<t\); if the data sequence \((x_{i},y_{i})\), \(i\in\mathbb{N}\) were i.i.d. or exchangeable, then this would yield \(1-\alpha\) coverage [35] at each time \(t\). However, in the sequential setting, which does not assume exchangeability (or any probabilistic model for the data for that matter), choosing \(q_{t}\) in (1) to yield coverage is a formidable task. In fact, if we are not willing to make any assumptions about the data sequence, then a coverage guarantee at time \(t\) would only be possible with trivial methods, which construct prediction intervals of infinite sizes.

Therefore, our goal is to achieve _long-run coverage_ in time. That is, letting \(\mathrm{err}_{t}=1\left\{y_{t}\notin\mathcal{C}_{t}\right\}\), we would like to achieve, for large integers \(T\),

\[\frac{1}{T}\sum_{t=1}^{T}\mathrm{err}_{t}=\alpha+o(1)\] (2)

under few or no assumptions, where \(o(1)\) denotes a quantity that tends to zero as \(T\to\infty\). We note that (2) is not probabilistic at all, and every theoretical statement we will make in this paper holds deterministically. Furthermore, going beyond (2), we also seek to design flexible strategies to produce the sharpest prediction sets possible, which not only adapt to, but also anticipate distribution shifts.

We call our proposed solution _conformal PID control_. It treats the system for producing prediction sets as a proportional-integral-derivative (PID) controller. In the language of control, the prediction sets take a _control variable_, \(q_{t}\), and then produce a _process variable_, \(\mathrm{err}_{t}\). We seek to anchor \(\mathrm{err}_{t}\) to a _set point_, \(\alpha\). To do so, we apply corrections to \(q_{t}\) based on the error of the output, \(g_{t}=\mathrm{err}_{t}-\alpha\). By reframing the problem in this language, we are able to build algorithms that have more stable coverage while also prospectively adapting to changes in the score sequence, much in the same style as a control system. See the diagram in Figure 1.

### Peek at results: methods

Three design principles underlie our methods:

1. _Quantile tracking (P control)_. Running online gradient descent on the quantile loss (summed over all past scores) gives rise to a method that we call _quantile tracking_, which achieves long-run coverage (2) under no assumptions except boundedness of the scores. This bound can be unknown. Unlike adaptive conformal inference (ACI) [16], quantile tracking does not return infinite sets after a sequence of miscoverage events. This can be seen as equivalent to proportional (P) control.

2. _Error integration (I control)_. By incorporating the running sum \(\sum_{i=1}^{t}(\mathrm{err}_{i}-\alpha)\) of the coverage errors into the online quantile updates, we can further stabilize the coverage. This _error integration_ scheme achieves long-run coverage (2) under no assumptions whatsoever on the scores (they can be unbounded). This can be seen as equivalent to integral (I) control.

3. _Scorecasting (D control)_. To account for systematic trends in the scores--this may be due to aspects of the data distribution, fixed or changing, which are not captured by the initial forecaster--we train a second model, a _scoreaster_, to predict the quantile of the next score. While quantile tracking and error integration are merely reactive, scorceasting is forward-looking. It can residualize out systematic trends in the errors and lead to practical advantages in terms of coverage and efficiency (set sizes). This can be seen as equivalent to derivative (D) control. Traditional control theory would suggest using a linear approximation \(g_{t}^{\prime}=g_{t}-g_{t-1}\), but in our problem, we will typically choose more advanced scorceasting algorithms that go well beyond simple difference schemes.

These three modules combine to make our final iteration, the _conformal PID controller_:

\[q_{t+1}=\underbrace{\eta g_{t}}_{\mathrm{P}}+\underbrace{r_{t}\Bigg{(}\sum_{ i=1}^{t}g_{t}\Bigg{)}}_{\mathrm{I}}+\underbrace{g_{t}^{\prime}}_{\mathrm{B}}.\] (3)

Figure 1: Conformal PID Control, expressed as a block diagram.

In traditional PID control, one would take \(r_{t}(x)\) to be a linear function of \(x\). Here, we allow for nonlinearity and take \(r_{t}\) to be a _saturation function_ obeying

\[x\geq c\cdot h(t)\implies r_{t}(x)\geq b,\ \ \ \text{and}\ \ \ x\leq-c\cdot h(t) \implies r_{t}(x)\leq-b,\] (4)

for constants \(b,c>0\), and a sublinear, nonnegative, nondecreasing function \(h\)--we call a function \(h\) satisfying these conditions _admissible_. An example is the _tangent integrator_\(r_{t}(x)=K_{\text{I}}\tan(x\log(t)/(tC_{\text{sat}}))\), where we set \(\tan(x)=\operatorname{sign}(x)\cdot\infty\) for \(x\notin[-\pi/2,\pi/2]\), and \(C_{\text{sat}},K_{\text{I}}>0\) are constants. The choice of integrator \(r_{t}\) is a design decision for the user, as is the choice of scorecaster \(g_{t}^{\prime}\).

We find it convenient to reparametrize (3), to produce a sequence of quantile estimates \(q_{t}\), \(t\in\mathbb{N}\) used in the prediction sets (1), as follows:

\[\text{let }\hat{q}_{t+1}\text{ be any function of the past: }x_{i},y_{i},q_{i}\text{, for }i\leq t,\] (5) \[\text{then update }q_{t+1}=\hat{q}_{t+1}+r_{t}\Bigg{(}\sum_{i=1}^{t}( \operatorname{err}_{i}-\alpha)\Bigg{)}.\]

Taking \(\hat{q}_{t+1}=\eta g_{t}+g_{t}^{\prime}\) recovers (3), but we find it generally useful to instead consider the formulation in (5), which will be our main focus in the exposition henceforth. Now we view \(\hat{q}_{t+1}\) as the scorecaster, which directly predicts \(q_{t+1}\) using past data. A main result of this paper, whose proof is given in Appendix A, is that the conformal PID controller (5) yields long-run coverage for any choice of integrator \(r_{t}\) that satisfies the appropriate saturation condition, and any scorecaster \(\hat{q}_{t+1}\).

**Theorem 1**.: _Let \(\{\hat{q}_{t}\}_{t\in\mathbb{N}}\) be any sequence of numbers in \([-b/2,b/2]\) and let \(\{s_{t}\}_{t\in\mathbb{N}}\) be any sequence of score functions with outputs in \([-b/2,b/2]\). Here \(b>0\), and may be infinite. Assume that \(r_{t}\) satisfies (4), for an admissible function \(h\). Then the iterations in (5) achieve long-run coverage (2)._

To emphasize, this result holds deterministically, with no probabilistic model on the data \((x_{t},y_{t})\), \(t\in\mathbb{N}\). (Thus in the case that the sequence is random, the result holds for all realizations of the random variables.) As we will soon see, this theorem can be seen as a generalization of existing results in the online conformal literature.

### Peek at results: experiments

**COVID-19 death forecasting.** To demonstrate conformal PID in practice, we consider 4-week-ahead forecasting of COVID-19 deaths in California, from late 2020 through late 2022. The base forecaster \(f_{t}\) that we use is the ensemble model from the COVID-19 Forecast Hub, which is the model used for official CDC communications on COVID-19 forecasting [10, 29]. In this forecasting problem, at each time \(t\) we actually seek to predict the observed death count \(y_{t+4}\) at time \(t+4\).

Figure 2 shows the central 80% prediction sets from the Forecast Hub ensemble model on the left panel, and those from our conformal PID method on the right. We use a quantile conformal score function, as in conformalized quantile regression [30], applied asymmetrically (i.e., separately) to the lower and upper quantile levels). We use the tan integrator, with constants chosen heuristically (as described in Appendix C), and an \(\ell_{1}\)-regularized quantile regression as the scorecaster--in particular, the scorecasting model at time \(t\) predicts the quantile of the score at time \(t+4\) based on all previous forecasts, cases, and deaths, from _all 50 US states_. The main takeaway is that conformal PID control is able to correct for consistent underprediction of deaths in the winter wave of late 2020/early 2021. We can see from the figure that the original ensemble fails to cover 8 times in a stretch of 10 weeks, resulting in a coverage of 20%; meanwhile, conformal PID only fails to cover 3 times during this stretch, restoring the coverage to 70% (recall the nominal level is 80%).

How is this possible? The ensemble is mainly comprised of constituent forecasters that ignore geographic dependencies between states [11] for the sake of simplicity or computational tractability. But COVID infections and deaths exhibit strong spatiotemporal dependencies, and most US states experienced the winter wave of late 2020/early 2021 at similar points in time. The scorecaster is thus able to learn from the mistakes made on other US states in order to prospectively adjust the ensemble's forecasts for the state of California. Similar improvements can be seen for other states, and we include experiments for New York and Texas as examples in Appendix F, which also gives more details on the scorecaster and the results.

**Electricity demand forecasting.** Next we consider a data set on electricity demand forecasting in New South Wales [18], which includes half-hourly data from May 7, 1996 to December 5, 1998.

For the base forecaster we use a Transformer model [34] as implemented in darts[19]. This is only re-trained daily, to predict the entire day's demand in one batch; this is a standard approach with Transformer models due to their high computational cost. For the conformal score, we use the asymmetric (signed) residual score. We use the tan integrator as before, and we use a lightweight Theta model [2], re-trained at every time point (half-hour), as the scorecaster.

The results are shown in the right panel of Figure 3, where adaptive conformal inference (ACI) [16] is also compared in the left panel. In short, conformal PID control is able to anticipate intraday variations in the scores, and produces sets that "hug" the ground truth sequence tightly; it achieves tight coverage without generating excessively large or infinite sets. The main reason why this is improved is that the scorecaster has a seasonality component built into its prediction model; in general, large improvements such as the one exhibited in Figure 3 should only be expected when the base forecaster is imperfect, as is the case here.

Figure 3: Results for electricity demand forecasting. The left column shows adaptive conformal inference (ACI), and the right column shows conformal PID control. The base forecaster is a Transformer model, and we use a tan integrator and a Theta scorecaster. The format of the figure follows that of Figure 2, except the nominal coverage is now \(1-\alpha=0.9\), and the coverage is averaged over a trailing window of 50 points (we also omit the red dots which mark miscoverage events). Summary statistics are available in Table 2.

Figure 2: Results for 4-week ahead COVID-19 death forecasting in California. The left column shows the COVID-19 Forecast Hub ensemble model, and the right column shows conformal PID control using the tan integrator, and a scorecaster given by \(\ell_{1}\)-penalized quantile regression on all past forecasts, cases, and deaths from all 50 states. The top row plots coverage, averaged over a trailing window of 10 weeks. The nominal coverage level is \(1-\alpha=0.8\) and marked by a gray dotted line. The bottom row plots the prediction sets in gold along with the ground-truth times series (death counts). Miscoverage events are indicated by red dots. Summary statistics such as the coverage and average set size are available in Table 1.

### Related work

The adversarial online view of conformal prediction was pioneered by [16] in the same paper that first introduced ACI. Since then, there has been significant work towards improving ACI, primarily by setting the learning rate adaptively [17; 40; 7], and incorporating ideas from multiclibration to improve conditional coverage [5]. It is worth noting that [7] also makes the observation that the ACI iteration can be generalized to track the quantile of the score sequence, although their focus is on adaptive regret guarantees. Because the topic of adaptive learning rates for ACI and related algorithms has already been investigated heavily, we do not consider it in the current paper. Any such method, such as those of [17; 7] should work well in conjunction with our proposed algorithms.

A related but distinct line of work surrounds online _calibration_ in the adversarial sequence model, which dates back to [14; 15], and connects in interesting ways to both game theory and online learning. We will not attempt to provide a comprehensive review of this rich and sizeable literature, but simply highlight [25; 24; 23] as a few interesting examples of recent work.

Lastly, outside the online setting, we note that several researchers have been interested in generalizing conformal prediction beyond the i.i.d. (or exchangeable) data setting: this includes [33; 28; 26; 12; 8], and for time series prediction, in particular, [9; 31; 38; 39; 3]. The focus of all of these papers is quite different, and they all rely on probabilistic assumptions on the data sequence to achieve validity; we include further discussion in Appendix B.

## 2 Methods

We describe the main components of our proposal one at a time, beginning with the quantile tracker.

### Quantile tracking

The starting point for quantile tracking is to consider the following optimization problem:

\[\operatorname*{minimize}_{q\in\mathbb{R}}\ \sum_{t=1}^{T}\rho_{1-\alpha}(s_{t}-q),\] (6)

for large \(T\), where we abbreviate \(s_{t}=s_{t}(x_{t},y_{t})\) for the score of the test point, and \(\rho_{1-\alpha}\) denotes the quantile loss at the level \(1-\alpha\), i.e., \(\rho_{\tau}(z)=\tau|z|\) for \(z>0\) and \((1-\tau)|z|\) for \(z\leq 0\). The latter is the standard loss used in quantile regression [22; 21]. Problem (6) is thus a simple convex (linear) program that tracks the \(1-\alpha\) quantile of the score sequence \(s_{t}\), \(t\in N\). To see this, recall that for a continuously distributed random variable \(Z\), the expected loss \(\mathbb{E}[\rho_{1-\alpha}(Z-q)]\) is uniquely minimized (over \(q\in\mathbb{R}\)) at the level \(1-\alpha\) quantile of the distribution of \(Z\).

In the sequential setting, where we receive one score \(s_{t}\) at a time, a natural and simple approach is to apply _online gradient descent_ to (6), with a constant learning rate \(\eta>0\). This results in the update:2

Footnote 2: Technically, this is the online subgradient method; in a slight abuse of notation, we write \(\nabla\rho_{1-\alpha}(0)\) to denote a subgradient of \(\rho_{1-\alpha}\) at 0, which can take on any value in \([-\alpha,1-\alpha]\).

\[q_{t+1} =q_{t}+\eta\nabla\rho_{1-\alpha}(s_{t}-q_{t})\] \[=q_{t}+\eta(\operatorname{err}_{t}-\alpha),\] (7)

where the second line follows as \(\nabla\rho_{1-\alpha}(s_{t}-q_{t})=1-\alpha\) if \(s_{t}>q_{t}\iff\operatorname{err}_{t}=1\), and \(\nabla\rho_{1-\alpha}(s_{t}-q_{t})=-\alpha\) if \(s_{t}\leq q_{t}\iff\operatorname{err}_{t}=0\). Note that the update in (7) is highly intuitive: if we miscovered (committed an error) at the last iteration then we increase the quantile, whereas if we covered (did not commit an error) then we decrease the quantile.

Even though it is extremely simple, the quantile tracking iteration (7) can achieve long-run coverage own its own, provided the scores are bounded.

**Proposition 1**.: _Let \(\{s_{t}\}_{t\in\mathbb{N}}\) be any sequence of numbers in \([-b,b]\), for \(0<b<\infty\). Then the quantile tracking iteration (7) satisfies_

\[\left|\frac{1}{T}\sum_{t=1}^{T}(\operatorname{err}_{t}-\alpha)\right|\leq \frac{b+\eta}{\eta T},\]

_for any learning rate \(\eta>0\) and \(T\geq 1\). In particular, (7) yields long-run coverage as in (2)._A few remarks are in order. First, although Proposition 1 assumes boundedness of the scores, we do not need to know this bound in order to run (7) and obtain long-run coverage. As long as the scores lie in \([-b,b]\) for any finite \(b\), the guarantee goes through--clearly, the quantile tracker proceeds agnostically and performs the same updates in any case. Notably, the adaptive conformal inference algorithm can be expressed as a special case of the quantile tracker; see Appendix B.1 for details.

Second, for the learning rate, in practice we typically set \(\eta\) heuristically, as some fraction of the highest score over a trailing window \(\hat{B}_{t}=\max\{s_{t-\Delta+1},\ldots,s_{t}\}\). On this scale, setting \(\eta=0.1\hat{B}_{t}\) usually gives good results, and we use it in all experiments unless specified otherwise (we also set the window length \(\Delta\) to be the same as the length of the burn-in period for training the initial base forecaster and scorecaster).3 Extremely high learning rates result in volatile sets, while very low ones may fail to keep up with rapid changes in the score distribution.

Footnote 3: Technically, this learning rate is not fixed, so Proposition 1 does not directly apply. However, we can view it as a special case of error integration and an application of Proposition 2 thus provides the relevant coverage guarantee.

### Error integration

Error integration is a generalization of quantile tracking that follows the iteration:

\[q_{t+1}=r_{t}\Bigg{(}\sum_{i=1}^{t}(\operatorname{err}_{i}-\alpha)\Bigg{)},\] (8)

where \(r_{t}\) is a saturation function that satisfies (4) for an admissible function \(h\); recall that we use admissible to mean nonnegative, nondecreasing, and sublinear. As we saw in (13), the quantile tracker uses a _constant_ threshold function \(h\), whereas \(h\) is now permitted to grow, as long as it grows sublinearly, i.e., \(h(t)/t\to 0\) as \(t\to\infty\). A non-constant threshold function \(h\) can be desirable because it means that \(r_{t}\) will "saturate" (will hit the conditions on the right-hand sides in (4)) less often, so corrections for coverage error will occur less often, and in this sense, a greater degree of coverage error can be tolerated along the sequence.

The next proposition, in particular its proof, makes the role of \(h\) precise. Importantly, Proposition 2 suffices to prove Theorem 1.

**Proposition 2**.: _Let \(\{s_{t}\}_{t\in\mathbb{N}}\) be any sequence of numbers in \([-b,b]\), where \(b>0\), and may be infinite. Assume that \(r_{t}\) satisfies (4), for an admissible function \(h\). Then the error integration iteration (8) satisfies_

\[\left|\frac{1}{T}\sum_{t=1}^{T}(\operatorname{err}_{t}-\alpha)\right|\leq \frac{c\cdot h(T)+1}{T},\] (9)

_for any \(T\geq 1\), where \(c\) is the constant in (4). In particular, this means (8) yields long-run coverage (2)._

The choice of saturation function essentially corresponds to a choice of adaptive learning rate; see Appendix D for details.

### Scorecasting

The final piece to discuss is scorecasting. A scorecaster attempts to forecast \(q_{t+1}\) directly, taking advantage of any leftover signal that is not captured by the base forecaster. This is the role played by \(\hat{q}_{t+1}\) in (5). Scorecasting may be particularly useful when it is difficult to modify or re-train the base forecaster. This can occur when the base forecaster is computationally costly to train (e.g., as in a Transformer model); or it can occur in complex operational prediction pipelines where frequently updating a forecasting implementation is infeasible. Another scenario where scorecasting may be useful is one in which the forecaster and scorecaster have access to different levels of data. For example, if a public health agency collects epidemic forecasts from external groups, and forms an ensemble forecast, then the agency may have access to finer-grained data that it can use to recalibrate the ensemble's prediction sets (compared to the level of data granularity granted to the forecasters originally).

This motivates the need for scorecasting as a modular layer that "sits on top" of the base forecaster and residualizes out systematic errors in the score distribution. This intuition is made more precise by recalling, as described above (following Proposition 2), that scorecasting combined with error integration as in (5) is just a reparameterization of error integration (8), where \(q_{t}^{\prime}=q_{t}-\hat{q}_{t}\) and \(s_{t}^{\prime}=s_{t}-\hat{q}_{t}\) are the new quantile and new score, respectively. A well-executed scorecaster could reduce the variability in the scores and make them more exchangeable, resulting in more stable coverage and tighter prediction sets, as seen in Figure 3. On the other hand, an aggressive scorecaster with little or no signal can actually hurt by adding variance to the new score sequence \(s_{t}^{\prime}\), which could result in more volatile coverage and larger sets.

There is no limit to what we can choose for the scorecasting model. We might like to use a model that can simultaneously incorporate seasonality, trends, and exogenous covariates. Common choices would be SARIMA (seasonal autoregressive integrated moving average) and ETS (error-trend-seasonality) models, but there are many other available methods, such as the Theta model [2], Prophet model [32], and neural network forecasters; see [20] for a review.

### Putting it all together

Briefly, we revisit the PID perspective, to recap how quantile tracking, error integration, and scorecasting fit in and work in combination. It helps to return to (3), which we copy again here:

\[q_{t+1}=g_{t}^{\prime}+\eta(\mathrm{err}_{t}-\alpha)+r_{t}\Bigg{(}\sum_{i=1} ^{t}(\mathrm{err}_{i}-\alpha)\Bigg{)}.\] (10)

Quantile tracking is precisely given by taking \(g_{t}^{\prime}=q_{t}\) and \(r_{t}=0\). This can be seen as equivalent to P control: subtract \(q_{t}\) from both sides in (10) and treat the increment \(u_{t+1}=q_{t+1}-q_{t}\) as the process variable; then in this modified system, quantile tracking is exactly P control. For this reason, we use "conformal P control" to refer to the quantile tracker in the experiments that follow. Similarly, we use "conformal PI control" to refer to the choice \(g_{t}^{\prime}=q_{t}\), and \(r_{t}\neq 0\) as a generic integrator (for us, tan is the default). Lastly, "conformal PID control" refers to letting \(g_{t}^{\prime}\) be a generic scorecaster, and \(r_{t}\neq 0\) be a generic integrator.

## 3 Experiments

In addition to the statewide COVID-19 death forecasting experiment described in the introduction, we run experiments on all combinations of the following data sets and forecasters.

Data sets:

\[\begin{array}{ll}\text{Forecasters (all via darts \@@cite[cite]{[\@@bibref{}{Ped1998}{}{}]})}\text{ :}\\ \text{* Electricity demand \@@cite[cite]{[\@@bibref{}{Ped1998}{}{}]}}&\text{* Autoregressive (AR) model with 3 lags}\\ \text{* Return (log price) of Amazon, Google, and Microsoft stock \@@cite[cite]{[\@@bibref{}{Ped1998}{}{}]}}&\text{* Theta model with $\theta=2$ \@@cite[cite]{[\@@bibref{}{Ped1998}{}{}]}}\\ \text{* Temperature in Delhi \@@cite[cite]{[\@@bibref{}{Delhi2016}{}{}]}}&\text{* Prophet model \@@cite[cite]{[\@@bibref{}{Delhi2016}{}{}]}}\\ \end{array}\]

In all cases except for the COVID-19 forecasting data set, we: re-train the base forecaster at each time point; construct prediction sets using the asymmetric (signed) residual score; and use a Theta model for the scorecaster. For the COVID-19 forecasting setting, we: use the given ensemble model as the base forecaster (no training at all); construct prediction sets using the asymmetric quantile score; and use an \(\ell_{1}\)-penalized quantile regression as the scorecaster, fit on features derived from previous forecasts, cases, and deaths, as described in the introduction. And lastly, in all cases, we use a tan function for the integrator with constants chosen heuristically, as described in Appendix C.

The results that we choose to show in the subsections below are meant to illustrate key conceptual points (differences between the methods). Additional results are presented in Appendix G. Our GitHub repository, https://github.com/aangelopoulos/conformal-time-series, provides the full suite of evaluations.

### ACI versus quantile tracking

We forecast the daily Amazon (AMZN) opening stock price from 2006-2014. We do this in log-space (hence predicting the return of the stock). Figure 4 compares ACI and the quantile tracker, each Figure 4: Results for forecasting Amazon stock return, comparing ACI and quantile tracking (P control). The plots show AR as the base forecaster; the table summarizes the results of all four base forecasters. We use the default learning rates for both ACI and quantile tracking: \(\eta=0.005\) and \(\eta=0.1\hat{B}_{t}\), respectively.

Figure 5: As in Figure 4, but with larger learning rates for ACI and quantile tracking: \(\eta=0.1\) and \(\eta=0.5\hat{B}_{t}\), respectively.

with its default learning rate: \(\eta=0.005\) for ACI, and \(\eta=0.1\hat{B}_{t}\) for quantile tracking. We see that the coverage from each method is decent, but oscillates nontrivially around the nominal level of \(1-\alpha=0.9\) (with ACI generally having larger oscillations). Figure 5 thus increases the learning rate for each method: \(\eta=0.1\) for ACI, and \(\eta=0.5\hat{B}_{t}\) for the quantile tracker. We now see that both deliver very tight coverage. However, ACI does so by frequently returning infinite sets; meanwhile, the corrections to the sets made by the quantile tracker are nowhere near as aggressive.

As a final comparison, in Appendix E, we modify ACI to clip the sets in a way that disallows them from ever being infinite. This heuristic may be used by practitioners that want to guard against infinite sets, but it no longer has a validity guarantee for bounded or unbounded scores. The results in the appendix indicate that the quantile tracker has similar coverage to this procedure, and usually with smaller sets.

### The effect of integration

Next we forecast the daily Google (GOOGL) opening stock price from 2006-2014 (again done in log-space). Figure 6 compares the quantile tracker without and without an additional integrator component (P control versus PI control). We purposely choose a very small learning rate, \(\eta=0.01\hat{B}_{t}\), in order to show how the integrator can stabilize coverage, which it does nicely for most of the time series. The coverage of PI control begins to oscillate more towards the end of the sequence, which we attribute at least in part to the fact that the integrator measures coverage errors accumulated over _all time_--and by the end of a long sequence, the marginal coverage can still be close to \(1-\alpha\) even if the local coverage deviates more wildly. This can be addressed by using a local version of the integrator, an idea we return to in the discussion.

### The effect of scorecasting

Figures 2 and 3 already showcase examples in which scorecasting offers significant improvement in coverage and set sizes. Recall that these were settings in which the base forecaster produces errors (scores) that have predictable trends. Further examples in the COVID-19 forecasting setting, which display similar benefits to scorecasting, are given in Appendix F.

Figure 6: Results for forecasting Google stock return, comparing quantile tracking with and without the integrator (P control versus PI control). The plots show Prophet as the base forecaster; the table summarizes the results of all four base forecasters. We purposely use a very small learning rate, \(\eta=0.01\hat{B}_{t}\), in order to show how the integrator can stabilize coverage.

We emphasize that it is not always the case that scorecasting will help. In some settings, scorecasting may introduce enough variance into the new score sequence that the coverage or sets will degrade in stability. (For example, this will happen if we run a highly complex scorecaster on a sequence of i.i.d. scores, where there are no trends whatsoever.) In practice, scorecasters should be designed with care, just as one would design a base forecaster; it is unlikely that using "out of the box" techniques for scorecasting will be robust enough, especially in high-stakes problems. Appendix G provides examples in which scorecasting, run across all settings using a generic Theta model, can hurt (for example, it adds noticeable variance to the coverage and sets in some instances within the Amazon data setting).

## 4 Discussion and extensions

**Discussion.** Our work presents a framework for constructing prediction sets in time series that is analogous (and indeed formally equivalent) to PID control, consisting of quantile tracking (P control), which is simply online gradient descent applied to the quantile loss; error integration (I control) to stabilize coverage; and scorecasting (D control) to remove systematic trends in the scores (errors made by the base forecaster).

We found that the combination of quantile tracking and integration consistently yields robust and favorable performance in our experiments. Scorecasting provides additional benefits if there are trends left in scores that are predictable (and the scorecaster is well-designed), as is the case in some of our examples. Otherwise, scorecasting may add variability and make the coverage and prediction sets more volatile. Overall, designing the scorecaster (which includes the choice to even use one at all) is an important modeling step, just like the design of the base forecaster.

It is worth emphasizing that, with the exception of the COVID-19 forecasting example, our experiments are intended to be illustrative, and we did not look to use state-of-the-art forecasters, or include any and all possibly relevant features for prediction. Further, while we found that using heuristics to set constants (such as the learning rate \(\eta\), and constants \(C_{\text{sat}},K_{\text{I}}\) for the tan integrator) worked recently well, we believe that more rigorous techniques, along the lines of [17; 7], can be used to tune these adaptively in an online fashion.

**Extensions.** We now present an extension of our analysis to conformal risk control [1; 6; 13]. In this setting, we are given a sequence of loss functions \(L_{t}:2^{\mathcal{Y}}\times\mathcal{Y}\rightarrow[0,1]\) satisfying \(L_{t}(\mathcal{Y},\mathcal{Y})=0\) for all \(y\), and \(L_{t}(\emptyset,y)=1\) for all \(y\). The goal is to bound the deviation of the average risk \(\frac{1}{T}\sum_{t=1}^{T}L_{t}(\mathcal{C}_{t},y_{t})\) from \(\alpha\). We state a result for the integrator below, and give its proof in Appendix A.

**Proposition 3**.: _Consider the iteration \(q_{t+1}=r_{t}(\sum_{i=1}^{t}(L_{i}(\mathcal{C}_{i},y_{i})-\alpha))\), with \(L_{t}\) as above. Assume that \(r_{t}\) satisfies (4), for an admissible function \(h\). Also assume that \(C_{t}(\mathcal{C}_{t},y_{t})=\emptyset\) if \(q_{t}\leq-b\) and \(\mathcal{Y}\) if \(q_{t}\geq b\), where \(b>0\), and may be infinite. Then for all \(T\geq 1\),_

\[\left|\frac{1}{T}\sum_{t=1}^{T}(L_{t}(\mathcal{C}_{t},y_{t})-\alpha)\right| \leq\frac{c\cdot h(T)+1}{T}.\] (11)

_for any \(T\geq 1\), where \(c\) is the constant in (4)._

We briefly conclude by mentioning that we believe many other extensions are possible, especially with respect to the integrator. Broadly, we can choose to integrate in a kernel-weighted fashion,

\[r_{t}\Bigg{(}\sum_{i=1}^{t}(\mathrm{err}_{i}-\alpha)\cdot K\big{(}(i,x_{i},y_{ i}),(t,x_{t},y_{t})\big{)}\Bigg{)}.\] (12)

As a special case, the kernel could simply assign weight 1 if \(t-i\leq w\), and weight 0 otherwise, which would result in an integrator that aggregates coverage over a training window of length \(w\). This can help consistently sustain better local coverage, for long sequences. As another special case, the kernel could assign a weight based on whether \(x_{i}\) and \(x_{t}\) lie in the same bin in some pre-defined binning of \(\mathcal{X}\) space, which may be useful for problems with group structure (where we want group-wise coverage). Various other choices and forms of kernels are possible, and it would be interesting to consider adding together a number of such choices (12) in combination, in a multi-resolution flavor, for the ultimate quantile update.

#### Acknowledgments

We would like to thank Tijana Zrnic, Amit Kohli, and Jordan Lekeufack for their valuable feedback. This work was supported by the National Science Foundation (NSF) Graduate Research Fellowship Program under grant no. 2146752, and the Office of Naval Research (ONR) Multi-University Research Initiative (MURI) Program under grant no. N00014-20-1-2787.

## References

* [1] Anastasios N. Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal risk control. _arXiv preprint arXiv:2208.02814_, 2022.
* [2] Vassilis Assimakopoulos and Konstantinos Nikolopoulos. The theta model: A decomposition approach to forecasting. _International Journal of Forecasting_, 16(4):521-530, 2000.
* [3] Andreas Auer, Martin Gauch, Daniel Klotz, and Sepp Hochreiter. Conformal prediction for time series with modern Hopfield networks. _arXiv preprint arXiv:2303.12783_, 2023.
* [4] Rina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas, and Ryan J. Tibshirani. Conformal prediction beyond exchangeability. _arXiv preprint arXiv:2202.13415_, 2022.
* [5] Osbert Bastani, Varun Gupta, Christopher Jung, Georgy Noarov, Ramya Ramalingam, and Aaron Roth. Practical adversarial multivalid conformal prediction. In _Advances in Neural Information Processing Systems_, 2022.
* [6] Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael I. Jordan. Distribution-free, risk-controlling prediction sets. _Journal of the ACM_, 68(6):1-34, 2021.
* [7] Aadyot Bhatnagar, Huan Wang, Caiming Xiong, and Yu Bai. Improved online conformal prediction via strongly adaptive online learning. _arXiv preprint arXiv:2302.07869_, 2023.
* [8] Emmanuel J. Candes, Lihua Lei, and Zhimei Ren. Conformalized survival analysis. _Journal of the Royal Statistical Society: Series B_, 85(1):24-45, 2023.
* [9] Victor Chernozhukov, Kaspar Wuthrich, and Zhu Yinchu. Exact and robust conformal inference methods for predictive machine learning with dependent data. In _Proceedings of the Annual Conference on Learning Theory_, 2018.
* [10] Estee Y. Cramer, Yuxin Huang, Yijin Wang, Evan L. Ray, Matthew Cornell, Johannes Bracher, Andrea Brennen, Alvaro J. Castro Rivadeneira, Aaron Gerding, Katie House, Dasuni Jayawardena, Abdul Hannan Kanji, Ayush Khandelwal, Khoa Le, Vidhi Mody, Vrusthi Mody, Jarad Niemi, Ariane Stark, Apurv Shah, Nutcha Wattanchit, Martha W. Zorn, Nicholas G. Reich, and US COVID-19 Forecast Hub Consortium. The United States COVID-19 Forecast Hub dataset. _Scientific Data_, 9, 2022.
* [11] Estee Y. Cramer, Evan L. Ray, Velma K. Lopez, Johannes Bracher, Andrea Brennen, Alvaro J. Castro Rivadeneira, Aaron Gerding, Tilmann Gneiting, Katie H. House, Yuxin Huang, Dasuni Jayawardena, Abdul H. Kanji, Ayush Khandelwal, Khoa Le, Anja Muhlemann, Jarad Niemi, Apurv Shah, Ariane Stark, Yijin Wang, Nutcha Wattanachit, Martha W. Zorn, Youyang Gu, Sanisdih Jain, Nayana Banmuv, Ayush Deva, Mihir Kulkarni, Srujana Merugu, Alpan Raval, Siddhant Shingi, Aytanshi Tiwari, Jerome White, Spencer Woody, Maytal Dahan, Spencer Fox, Kelly Gaither, Michael Lachmann, Lauren Ancel Meyers, James G. Scott, Mauricio Tec, Ajitesh Srivastava, et al. Evaluation of individual and ensemble probabilistic forecasts of COVID-19 mortality in the United States. _Proceedings of the National Academy of Sciences_, 119(15):e2113561119, 2022.
* [12] Clara Fannjiang, Stephen Bates, Anastasios N. Angelopoulos, Jennifer Listgarten, and Michael I. Jordan. Conformal prediction under feedback covariate shift for biomolecular design. _Proceedings of the National Academy of Sciences_, 119(43):e2204569119, 2022.
* [13] Shai Feldman, Liran Ringel, Stephen Bates, and Yaniv Romano. Achieving risk control in online learning settings. _arXiv preprint arXiv:2205.09095_, 2022.
* [14] Dean P Foster. A proof of calibration via Blackwell's approachability theorem. _Games and Economic Behavior_, 29(1-2):73-78, 1999.
* [15] Dean P. Foster and Rakesh V. Vohra. Asymptotic calibration. _Biometrika_, 85(2):379-390, 1998.

* [16] Isaac Gibbs and Emmanuel J. Candes. Adaptive conformal inference under distribution shift. In _Advances in Neural Information Processing Systems_, 2021.
* [17] Isaac Gibbs and Emmanuel J. Candes. Conformal inference for online prediction with arbitrary distribution shifts. _arXiv preprint arXiv:2208.08401_, 2022.
* [18] Michael Harries. Splice-2 comparative evaluation: Electricity pricing. Technical report, University of New South Wales, 1999.
* [19] Julien Herzen, Francesco Lassig, Samuele Giuliano Piazzetta, Thomas Neuer, Leo Tafti, Guillaume Raille, Tomas Van Pottelbergh, Marek Pasieka, Andrzej Skrodzki, Nicolas Huguenin, Maxime Dumonal, Jan Koscisz, Dennis Bader, Frederick Gusset, Mounir Benhedidi, Camila Williamson, Michal Kosinski, Matej Petrik, and Gael Grosch. Darts: User-friendly modern machine learning for time series. _Journal of Machine Learning Research_, 23(1):5442-5447, 2022.
* [20] Rob J. Hyndman and George Athanasopoulos. _Forecasting: Principles and Practice_. OTexts, 2018.
* [21] Roger Koenker. _Quantile Regression_. Cambridge University Press, 2005.
* [22] Roger Koenker and Gilbert Bassett. Regression quantiles. _Econometrica_, 46(1):33-50, 1978.
* [23] Volodymyr Kuleshov and Shachi Deshpande. Online calibrated regression for adversarially robust forecasting. _arXiv preprint arXiv:2302.12196_, 2023.
* [24] Volodymyr Kuleshov and Stefano Ermon. Estimating uncertainty online against an adversary. In _Association for the Advancement of Artificial Intelligence_, 2017.
* [25] Volodymyr Kuleshov and Percy Liang. Calibrated structure prediction. In _Advances in Neural Information Processing Systems_, 2015.
* [26] Lihua Lei and Emmanuel J. Candes. Conformal inference of counterfactuals and individual treatment effects. _Journal of the Royal Statistical Society: Series B_, 83(5):911-938, 2021.
* [27] Cam Nguyen. S&P 500 stock data. https://www.kaggle.com/datasets/cammugent/sandp500, 2018.
* [28] Aleksandr Podkopaev and Aaditya Ramdas. Distribution-free uncertainty quantification for classification under label shift. In _Uncertainty in Artificial Intelligence_, 2021.
* [29] Evan L. Ray, Logan C. Brooks, Jacob Bien, Matthew Biggerstaff, Nikos I. Bosse, Johannes Bracher, Estee Y. Cramer, Sebastian Funk, Aaron Gerding, Michael A. Johansson, Aaron Rumack, Yijin Wang, Martha Zorn, Ryan J. Tibshirani, and Nicholas G. Reich. Comparing trained and untrained probabilistic ensemble forecasts of COVID-19 cases and deaths in the United States. _International Journal of Forecasting_, 39(3):1366-1383, 2023.
* [30] Yaniv Romano, Evan Patterson, and Emmanuel J. Candes. Conformalized quantile regression. In _Advances in Neural Information Processing Systems_, 2019.
* [31] Kamile Stankeviciute, Ahmed M. Alaa, and Mihaela van der Schaar. Conformal time-series forecasting. _Advances in Neural Information Processing Systems_, 2021.
* [32] Sean J. Taylor and Benjamin Letham. Forecasting at scale. _The American Statistician_, 72(1):37-45, 2018.
* [33] Ryan J. Tibshirani, Rina Foygel Barber, Emmanuel J. Candes, and Aaditya Ramdas. Conformal prediction under covariate shift. In _Advances in Neural Information Processing Systems_, 2019.
* [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems_, 2017.
* [35] Vladimir Vovk, Alex Gammerman, and Glenn Shafer. _Algorithmic Learning in a Random World_. Springer, 2005.
* [36] Vladimir Vovk, Alexander Gammerman, and Craig Saunders. Machine-learning applications of algorithmic randomness. In _International Conference on Machine Learning_, 1999.
* [37] Sumanth Vrao. Daily climate time series data. https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data, 2017.
* [38] Chen Xu and Yao Xie. Conformal prediction interval for dynamic time-series. In _International Conference on Machine Learning_, 2021.

* [39] Chen Xu and Yao Xie. Sequential predictive conformal inference for time series. In _International Conference on Machine Learning_, 2023.
* [40] Margaux Zaffran, Olivier Feron, Yannig Goude, Julie Josse, and Aymeric Dieuleveut. Adaptive conformal predictions for time series. In _International Conference on Machine Learning_, 2022.

Proofs

We begin with the proof of Proposition 1. The proof is very simple, and we derive it as a corollary of Proposition 2, given next, because the proof reveals something perhaps unforeseen about the quantile tracker: it acts as an error integrator, despite only adjusting the quantile based on the most recent time step.

Proof of Proposition 1.: Without a loss of generality, set \(q_{1}=0\). Unraveling the iteration (7) yields

\[q_{t+1}=\eta\sum_{i=1}^{t}(\mathrm{err}_{i}-\alpha).\] (13)

For \(r_{t}(x)=\eta x\), \(h(t)=b\), we see (4) holds with \(c=1/\eta\). Proposition 2 now applies. 

Proof of Proposition 2.: Abbreviate \(E_{T}=\sum_{t=1}^{T}(\mathrm{err}_{t}-\alpha)\). We will prove one side of the absolute inequality in (9), namely, \(E_{T}\leq c\cdot h(T)+1\), and the other side follows similarly. We use induction. The base case, for \(T=1\), holds trivially. Now assume the result is true up to \(T-1\). We divide the argument into two cases: either \(c\cdot h(T-1)<E_{T-1}\leq c\cdot h(T-1)+1\) or \(E_{T-1}\leq c\cdot h(T-1)\). In the first case, note that that (4) implies \(q_{T}=r_{t}(E_{T-1})\geq b\) and therefore \(s_{T}\leq q_{T}\) and \(\mathrm{err}_{t}=0\). This means that

\[E_{T}=E_{T-1}-\alpha\leq c\cdot h(T-1)+1-\alpha\leq c\cdot h(T)+1,\]

as \(h\) is nondecreasing, which is the desired result at \(T\). In the second case, we just use \(\mathrm{err}_{T}\leq 1\), so

\[E_{T}\leq E_{T-1}+1-\alpha\leq c\cdot h(T-1)+1-\alpha\leq c\cdot h(T)+1.\]

This again gives the desired result at \(T\), and completes the proof. 

Proof of Theorem 1.: We can transform (5) by setting \(q^{\prime}_{t+1}=q_{t+1}-\hat{q}_{t+1}\), and this becomes an update of the form (8) with respect to \(q^{\prime}_{t+1}\). Further, the score sequence in this new parameterization is \(s^{\prime}_{t}=s_{t}-\hat{q}_{t}\), which is in \([-b,b]\) because both \(s_{t}\) and \(\hat{q}_{t}\) are in \([-b/2,b/2]\). Applying Proposition 2 gives the result. 

Proof of Proposition 3.: The proof is similar to that of Proposition 2--as in that proof, we only prove one side of the absolute inequality (11), and use induction. Abbreviate \(E_{T}=\sum_{t=1}^{T}(L_{i}(\mathcal{C}_{i},y_{i})-\alpha))\). The base case holds trivially. For the inductive step, either \(c\cdot h(T-1)<E_{T-1}\leq c\cdot h(T-1)+1\) or \(E_{T-1}\leq c\cdot h(T-1)\). In the first case, we have saturated, so \(L_{t}(\mathcal{C}_{t},y_{t})=0\), and

\[E_{T}=E_{T-1}-\alpha\leq c\cdot h(T-1)+1-\alpha\leq c\cdot h(T)+1,\]

as \(h\) is nondecreasing, which is the desired result at \(T\). In the second case, we just use the boundedness of the loss \(L_{t}(\mathcal{C}_{t},y_{t})\leq 1\), so

\[E_{T}\leq E_{T-1}+1-\alpha\leq c\cdot h(T-1)+1-\alpha\leq c\cdot h(T)+1.\]

This again gives the desired result at \(T\), and completes the proof. 

## Appendix B Comparisons to existing methods

This section contains descriptions of the relationship of our method to pre-existing methods for conformal prediction under distribution shift.

### ACI as a special case

Though it may not be immediately obvious, adaptive conformal inference (ACI) is actually a special case of the quantile tracker. ACI follows the iteration:

\[\alpha_{t+1}=\alpha_{t}-\eta(\mathrm{err}_{t}-\alpha),\]which is equivalent to

\[1-\alpha_{t+1} =1-\alpha_{t}+\eta(\mathrm{err}_{t}-\alpha)\] \[=1-\alpha_{t}+\eta\nabla\rho_{1-\alpha}(\beta_{t}-(1-\alpha_{t})),\]

for \(\beta_{t}=\inf\{\beta:s_{t}\leq\mathrm{Quantile}_{\beta}\left(\{s_{1},\ldots,s_{ t-1}\}\right)\). This shows that ACI is a particular instance of the quantile tracker that uses a secondary score \(s^{\prime}_{t}=\beta_{t}\) and quantile \(q^{\prime}_{t}=1-\alpha_{t}\). Thus, because quantile tracking (7) is the same as a linear coverage integrator (13), so is ACI.

We can see here that ACI transforms unbounded score sequences into bounded ones, which then implies long-run coverage for any score sequence. This may, however, come at a cost: ACI can sometimes output infinite or null prediction sets (when \(\alpha_{t}\) drifts below 0 or above 1, respectively). Direct quantile tracking on the scale of the original score sequence does not have this behavior.

### Weighted conformal prediction

A line of work by [33] and [4] deals with variants of weighted conformal prediction. Specifically, the latter variant allows the incorporation of sample weights for each data point. When the data come from a probabilistic model, the sample weights allow for probabilistic bounds on the coverage of a new test point.

These methods, and others (e.g., [9]) in the probabilistic setting, differ greatly from the approach herein. When there is no probabilistic model, for example, all such approaches give vacuous coverage guarantees. The reason is that coverage on a _new test point_ is impossible without a probabilistic model. However, in our setting we target long-run coverage, a completely different (and achievable) goal. Thus, neither the methods nor the proof techniques and theorem statements share any technical similarity to speak of.

For good measure, we include below in Appendix G evaluations against a version of [4] with a trailing window the size of the burn-in period. This approach generally performs worse than both our approach and ACI, and does not have guarantees in our setting, so we do not include it in main text.

### Sequential predictive conformal inference (SPCI)

A recently proposed line of work exemplified by Ensemble Batch Prediction Intervals (EnbPI) [38] and its improved version, SPCI [39], seeks to provide coverage guarantees for black-box time-series predictors. However, the methods therein are _not_ distribution-free, and SPCI rely on the following assumptions for validity:

1. The use of the quantile random forest (QRF) algorithm.
2. Model assumptions, such as consistency.
3. Distributional assumptions, such as smoothness.

Thus, the validity of SPCI is significantly weaker than the methods herein, perhaps along the lines of a standard nonparametric analysis. In contrast, our method has finite-sample validity averaged over time for any input sequence. However, it is possible to use the QRF/SPCI algorithm as the scorecaster, which would endow it with a distribution-free guarantee. We do not do this because the QRF is not a particularly effective time series forecasting method, and was outperformed by quantile LASSO (which we use for the COVID experiments) and the Theta model. The performance of the QRF tends to be "blocky" and high-variance; for an example, see plots on the SPCI GitHub page.

However, we note that the EnbPI and SPCI algorithms can be seen as specific choices of scorecasters, and one consequence of our paper is that these algorithms can be endowed distribution-free guarantees when used as scorecasters in (5).

## Appendix C Heuristics for setting constants

Consider the tan integrator \(r_{t}(x)=K_{1}\tan(x\log(t)/(tC_{\text{sat}}))\), where we set \(\tan(x)=\mathrm{sign}(x)\cdot\infty\) whenever \(x\notin[-\pi/2,\pi/2]\), and \(C_{\text{sat}},K_{1}>0\) are constants. The constant \(C_{\text{sat}}\) is primarily in charge of guaranteeing that by time \(T\), we want to have an absolute guarantee of at least \(1-\alpha-\delta\) coverage.

[MISSING_PAGE_FAIL:16]

Figure 8: As in Figure 5, but with clipped ACI.

Figure 7: As in Figure 4, but with clipped ACI.

Figure 10: Results for 4-week ahead COVID-19 death forecasting in Texas.

Figure 9: Results for 4-week ahead COVID-19 death forecasting in New York.

For each experiment, we describe the data set in a new subsection, and two plots are included: one for the coverage, and one for the prediction sets. Each column in the plots represents a different method, and each row is a different learning rate. For the quantile tracker, the learning rate is to be interpreted as the multiplier in front of \(\bar{B}_{t}\). Each method is given a different color, which stays consistent throughout the plots. We use a tan integrator and a Theta scorecaster throughout, just as in the main text experiments. There is an additional method, 'Trail', corresponding to the proposal of Barber et al. [4] with a trailing window.

### Amazon/Google

These data sets are part of a multivariate time series consisting of thirty blue-chip stock prices, including those of Amazon (AMZN) and Google (GOOGL), from January 1, 2006 to December 31, 2014. We attempt to forecast the daily opening price of each of Amazon and Google stock, on a log scale. Available to the scorecaster are the previous open prices of _all 30 stocks_.

### Microsoft

This data set is a univariate time series consisting of a single stock open price, that of Microsoft (MSFT), from April 1, 2015 to May 31, 2021.

### Daily temperature in Delhi

This data set contains the daily temperature (averaged over 8 measurements in 3 hour periods), humidity, wind speed, and atmospheric temperature in the city of Delhi from January 1, 2003 to April 24, 2017, scraped using the Weather Underground API.

### Electricity demand forecasting

This data set measures electricity demand in New South Wales collected at half-hour increments from May 7th, 1996 to December 5th, 1998 (we zoom in on the first 2000 time points). There are also

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Base Forecaster & Conformal PID Ctrl \\ \hline Marginal coverage & 0.82 & 0.86 \\ Longest err sequence & 6 & 2 \\ Average set size & 625 & 858 \\ Median set size & 512 & 688 \\
75\% quantile set size & 754 & 1.01e+03 \\
90\% quantile set size & 1.12e+03 & 1.47e+03 \\
95\% quantile set size & 1.45e+03 & 1.8e+03 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary statistics for COVID-19 death forecasting in California, as in Figure 2.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & \multicolumn{3}{c}{AR} & \multicolumn{2}{c}{Transformer} \\  & \multicolumn{1}{c}{ACI} & \multicolumn{1}{c}{Conformal PID Control} & \multicolumn{1}{c}{ACI} & \multicolumn{1}{c}{Conformal PID Ctrl} \\ \hline Marginal coverage & 0.899 & 0.9 & 0.899 & 0.901 \\ Longest err sequence & 3 & 2 & 3 & 2 \\ Average set size & \(\infty\) & 0.177 & \(\infty\) & 0.174 \\ Median set size & 0.406 & 0.178 & 0.426 & 0.175 \\
75\% quantile set size & 0.484 & 0.21 & 0.574 & 0.206 \\
90\% quantile set size & 0.672 & 0.236 & \(\infty\) & 0.233 \\
95\% quantile set size & \(\infty\) & 0.252 & \(\infty\) & 0.249 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary statistics for electricity forecasting, as in Figure 3. Results for the Prophet and Theta models are not available because darts does not support intermittent retraining for these algorithms.

several other variables collected, such as the demand and price in Victoria, the amount of energy transfer between New South Wales and Victoria, and so on. These are given as covariates to the scorecaster. The demand value is normalized by default to lie in \([0,1]\).

### Synthetic data sets

We perform some experiments on two synthetic score sequences which include change points and other behaviors difficult to produce using real data. In this setting, there is no ground truth \(y_{t}\) sequence, so we do not plot the sets. Instead, we plot the scores themselves in one column, and the quantiles \(q_{t}\) produced by each algorithm in a different column (when \(q_{t}\geq s_{t}\), we cover). The general goal is for \(q_{t}\) to track the \(1-\alpha\) quantile of \(s_{t}\), and if it is too far off, that corresponds to the "set being too large or too small" in a situation where we would be constructing sets out of these scores.

We consider an i.i.d. sequence of scores, a noisy increasing sequence of scores, and a mix of change points and trends. Our codebase describes the score generation procedure in more detail.

Figure 11: Results for the Amazon data set.

Figure 12: Results for the Google data set.

Figure 13: Results for the Microsoft data set.

Figure 14: Results for the Delhi temperature data set.

Figure 15: Results for the electricity demand forecasting data set.

Figure 16: Results for an i.i.d. score sequence.

Figure 17: Results for an increasing score sequence.

Figure 18: Results for a score sequence that is a mix of change points and trends.