# On the Impacts of the Random Initialization in the Neural Tangent Kernel Theory

 Guhan Chen

Department of Statistics and Data Science

Tsinghua University

Beijing, China

chen-gh23@mails.tsinghua.edu.cn

&Yicheng Li

Department of Statistics and Data Science

Tsinghua University

Beijing, China

liyc22@mails.tsinghua.edu.cn

&Qian Lin

Department of Statistics and Data Science

Tsinghua University

Beijing, China

qianlin@tsinghua.edu.cn

Corresponding author

###### Abstract

This paper aims to discuss the impact of random initialization of neural networks in the neural tangent kernel (NTK) theory, which is ignored by most recent works in the NTK theory. It is well known that as the network's width tends to infinity, the neural network with random initialization converges to a Gaussian process \(f^{\mathrm{GP}}\), which takes values in \(L^{2}(\mathcal{X})\), where \(\mathcal{X}\) is the domain of the data. In contrast, to adopt the traditional theory of kernel regression, most recent works introduced a special mirrored architecture and a mirrored (random) initialization to ensure the network's output is identically zero at initialization. Therefore, it remains a question whether the conventional setting and mirrored initialization would make wide neural networks exhibit different generalization capabilities. In this paper, we first show that the training dynamics of the gradient flow of neural networks with random initialization converge uniformly to that of the corresponding NTK regression with random initialization \(f^{\mathrm{GP}}\). We then show that \(\mathbf{P}(f^{\mathrm{GP}}\in[\mathcal{H}^{\mathrm{NT}}]^{s})=1\) for any \(s<\frac{3}{d+1}\) and \(\mathbf{P}(f^{\mathrm{GP}}\in[\mathcal{H}^{\mathrm{NT}}]^{s})=0\) for any \(s\geq\frac{3}{d+1}\), where \([\mathcal{H}^{\mathrm{NT}}]^{s}\) is the real interpolation space of the RKHS \(\mathcal{H}^{\mathrm{NT}}\) associated with the NTK. Consequently, the generalization error of the wide neural network trained by gradient descent is \(\Omega(n^{-\frac{3}{d+3}})\), and it still suffers from the curse of dimensionality. On one hand, the result highlights the benefits of mirror initialization. On the other hand, it implies that NTK theory may not fully explain the superior performance of neural networks.

## 1 Introduction

In recent years, the advancement of neural networks has revolutionized various domains, including computer vision, generative modeling, and others. Notably, large language models like the renowned GPT series [8; 51] have shown exceptional proficiency in language-related tasks. Similarly, neural networks have achieved significant successes in image classification, as evidenced by works such as [27; 34; 37]. This proliferation of neural networks spans a wide range of fields. Despite theseimpressive achievements, a comprehensive theoretical understanding of why neural networks perform so well remains elusive in the academic community.

Several studies have delved into the theoretical properties of neural networks. Initially, researchers were keen on exploring the expressive capacity of networks, as demonstrated in seminal works like [17; 28]. These studies established the Universal Approximation Theorem, asserting that sufficiently wide networks can approximate any continuous function. More recent research, such as [15; 26; 43] extended this exploration to the effects of deeper and wider network architectures. However, a significant challenge remains in these studies: they often do not fully explain the generalization power of neural networks, which is crucial for evaluating the performance of a statistical model.

Recently, some researchers have examined the generalization properties of networks. Bauer and Kohler [5], Schmidt-Hieber [46] showed the minimax optimality of networks with various activation functions for specific subclasses of Holder functions, within the nonparametric regression framework. In contrasts to the static ERM approach, some studies made more attention to the dynamics of neural networks, particularly those trained using gradient descent (GD) and stochastic gradient descent (SGD)[2; 13; 20].

With similar insights, Jacot et al. [31] explicitly introduced the Neural Tangent Kernel (NTK) concept, demonstrating that there exists a time-varying neural network kernel (NNK) which converges to a fixed deterministic kernel and remains almost invariant during training as network width approaches infinity. And thus NTK theory proposes that network training can be approximated by a kernel regression problem [4; 29; 39; 50]. As a general case, fully-connected networks directly trained by GD, Lai et al. [35], Li et al. [41] showed the generalization ability of two-layer and multi-layer networks, respectively.

This paper mainly follows [4; 35; 41], and explores the impact of initialization in the NTK theory. Prior research [35; 41] which verified the minimax optimality of network utilized the so-called _mirrored initialization_ setting. It refers to a combination of mirrored structure and mirrored initial value of parameters, which results in a zero initial output function. However, the assumption divides from the commonly used initialization strategy in real-world applications, whose initial output is actually non-zero. To bridging the gap, in this study we explore the generalization ability of standard non-zero initialized network, within the NTK theory framework. Our findings reveal that the vanilla non-zero initialization will theoretically results in poor generalization ability of network, especially when the data has relatively large dimension. If that is true, it suggests a divergence between theoretical models and real-world applications, highlighting a potential limitation in the current understanding of the NTK theory. Therefore, we arrive at a critical problem central to this study:

_Does initialization significantly impact the generalization ability of networks within the kernel regime?_

### Our contribution

\(\bullet\)_Network converges to a NTK predictor uniformly_. We show that under standard initialization, the network function converges to the NTK predictor uniformly over the entire training process and over all possible input in the domain. The convergence is essential in the study of the generalization ability of network in NTK theory. However, in previous work, the initial values of network has long been overlooked. Under mirrored initialization which leads to zero initial output function, Arora et al. [4], Lai et al. [35], Li et al. [41] demonstrated the point-wise convergence and the uniform convergence of network, respectively. More recently, Xu and Zhu [54] studied the uniform convergence of NTK under standard initialization, but did not study the convergence of the network function. Why the initial output of the network is ignored is not that it is insignificant, but rather because it is a stochastic function, making it challenging to analyze in convergence. Our findings make it valid to approximate the network's generalization ability based on the corresponding NTK predictor's performance.

\(\bullet\)_The generalization ability of standardly non-zero initialized fully-connected network_. Our research explores the impact of standard non-zero initialization in NTK theory. At this issue, Zhang et al. [56] proposes the existence of implicit bias induced by non-zero initialization, when the neural network is completely overfitted. We delve deeper into this argument, studies the exact formula of the bias at any stage of training, within the framework of NTK theory. Additionally, we established that the (optimally tuned) learning rate of network is \(n^{-\frac{3}{d+3}}\), even when the regression function is sufficiently smooth. This insightful discovery implies a notable limitation in the generalization ability of networks with non-zero initialization, if NTK theory can precisely approximate the performance of real network. Consequently, we need to reconsider the weakness of NTK in the study of network theory. Also, the results show that mirrored initialization is superior to standard initialization in practical applications.

### Related works

Our research is conducted within the framework of NTK theory. This type of research, in general, can be categorized into two main steps: the approximation of the network trained by GD through a kernel regression problem, and the evaluation on the corresponding kernel regression predictor. Several studies [2; 4; 19; 31] which focused on the former step, illustrated the point-wise convergence of NTK for multi-layer ReLU networks. Additionally, [39] demonstrated the point-wise convergence of the kernel regression predictor to the network. Furthermore, Lai et al. [35], Li et al. [41], Xu and Zhu [54] demonstrated the uniform convergence result with respect to all input and all time on two-layer and multi-layer networks. As to the latter step, a few researchers have analyzed the spectral properties of the NTK [6; 7] as well as kernel regression [40; 55]. Building upon these findings, Lai et al. [35] and Li et al. [41] demonstrated that early-stopping GD induces minimax optimality of the network. It is worth noting that the setting in these works assumes mirrored initialization, which may not be well-aligned with real-world scenarios. When it comes to initialization, Zhang et al. [56] provided insights into the impact of initialization under kernel interpolation, which is a special case of our results at \(t=\infty\).

## 2 Preliminaries

### Model and notations

Suppose that \(\{(x_{i},y_{i})\}_{i=1}^{n}\) are i.i.d. drawn from an unknown distribution \(\rho\) which is given by

\[y=f^{*}(x)+\epsilon,\] (1)

where \(f^{*}(x)\) is the _regression function_ and \(\epsilon\) is a centered random noise. Suppose that the marginal distribution \(\mu(x)\) of the radon variable \(x\) is supported in a non-empty bounded subset \(\mathcal{X}\) of \(\mathbb{R}^{d}\) with \(C^{\infty}\) smooth boundary. The generalization error of an estimator \(\hat{f}\) of \(f^{*}\) is given by excess risk

\[\mathcal{E}(\hat{f};f^{*})=\left\|\hat{f}-f^{*}\right\|_{L_{2}(\mathcal{X},\mu )}^{2}.\] (2)

We introduce the following standard assumption on the noise(e.g., [21; 42]). It is clear that sub-Gaussian noise satisfying this assumption.

**Assumption 1** (Noise).: The noise term \(\epsilon\) satisfies the following condition for some positive constant \(\sigma,L\), and \(m\geq 2\):

\[\mathbf{E}(|\epsilon|^{m}|x)\leq\frac{1}{2}m!\sigma^{2}L^{m-2},\quad a.e.\,x \in\mathcal{X}.\] (3)

NotationsGiven a set of samples pairs \(\{(x_{i},y_{i})\}_{i=1}^{n}\), we denote \(X\) and \(Y\) to be vector \((x_{1},\cdots,x_{n})^{T}\) and \((y_{1},\cdots,y_{n})^{T}\), respectively. In a similar manner, \((f(x_{1}),\cdots,f(x_{n}))^{T}\) and \((f(y_{1}),\cdots,f(y_{n}))^{T}\) are represented as \(f(X)\) and \(f(Y)\), where \(f(\cdot):\mathbb{R}^{d}\mapsto\mathbb{R}\) is an arbitrary given function. Regarding a kernel function \(k(\cdot,\cdot):\mathbb{R}^{d}\times\mathbb{R}^{d}\mapsto\mathbb{R}\), we use \(k(x,X)\) to denote the vector \((k(x,x_{1}),k(x,x_{2}),\cdots,k(x,x_{n}))\) and \(k(X,X)\) to denote the matrix \([k(x_{i},x_{j})]_{n\times n}\). For real number sequences such as \(\{a_{n}\}\) and \(\{b_{n}\}\), we write \(a_{n}=O(b_{n})\) (or \(a_{n}=o(b_{n})\)), if there exists absolute positive constant \(C\) such that \(|a_{n}|\leq C|b_{n}|\) holds for any sufficiently large \(n\) (or \(|a_{n}|/|b_{n}|\) approaches zero). We also denote \(a_{n}\asymp b_{n}\) if there exists absolute positive constant \(c\) abd \(C\) such that \(c|b_{n}|\leq|a_{n}|\leq C|b_{n}|\) holds for any sufficiently large \(n\).

### Reproducing kernel Hilbert space

Suppose that \(k\) is kernel function defined on the domain \(\mathcal{X}\) satisfying that \(\|k\|_{\infty}\leq\kappa^{2}\). Let \(\mathcal{H}_{k}\) be the reproducing Hilbert space associated with \(k\) which is the closure of linear span of \(\{k(x,\cdot),x\in\mathcal{X}\}\)under the inner product induced by \(\langle k(x,\cdot),k(y,\cdot)\rangle=k(x,y)\). Given a distribution \(\mu(x)\) on \(\mathcal{X}\), we can introduce an integral operator \(T_{k}:L^{2}(\mathcal{X},\mu)\to L^{2}(\mathcal{X},\mu)\):

\[T_{k}f(x)=\int_{\mathcal{X}}k(x,y)f(y)\,\mathrm{d}\mu(y).\] (4)

The celebrated Mercer's decomposition [12] asserts that

\[T_{k}f=\sum_{i\in\mathbb{N}}\lambda_{i}\langle f,e_{i}\rangle_{L^{2}}e_{i}, \quad k(x,y)=\sum_{i\in\mathbb{N}}\lambda_{i}e_{i}(x)e_{i}(y),\] (5)

where \(\{e_{i}\}_{i\in\mathbb{N}}\) and \(\{\lambda_{i}^{\frac{1}{2}}e_{i}\}_{i\in\mathbb{N}}\) are the orthonormal basis of \(L^{2}(\mathcal{X},\mu)\) and \(\mathcal{H}_{k}\) respectively. It is well known that \(\mathcal{H}_{k}\) can be canonically embedded into \(L^{2}(\mathcal{X},\mu)\).

If the eigenvalues \(\lambda_{i}\) of \(k\) are polynomially decaying at rate \(\beta\) ( i.e, \(\lambda_{i}\asymp i^{-\beta}\)), we can further introduce a concept of the relative smoothness of a function \(f\in L^{2}(\mathcal{X},\mu)\). More precisely, let us recall the concept of _real interpolation space_[48] ( Please see more detailed information in the Appendix).

Real interpolation spaceThe real interpolation space \([\mathcal{H}_{k}]^{s}\) is given by

\[[\mathcal{H}_{k}]^{s}\coloneqq\left\{\sum_{i\in\mathbb{N}}a_{i}\lambda_{i}^{ \frac{s}{2}}e_{i}(x)\Big{|}\sum_{i\in\mathbb{N}}a_{i}^{2}<\infty\right\},\] (6)

with the inner product \(\langle\sum_{i\in\mathbb{N}}a_{i}\lambda_{i}^{\frac{s}{2}}e_{i}(x),\sum_{i\in \mathbb{N}}b_{j}\lambda_{j}^{\frac{s}{2}}e_{j}(x)\rangle_{[\mathcal{H}_{k}]^ {s}}=\sum_{i\in\mathbb{N}}a_{i}b_{i}\) for \(s\geq 0\).

It is clear that \([\mathcal{H}_{k}]^{s}\) is a separable Hilbert space and is isometric to the \(l_{2}\) space. With the definition above, we can see that \([\mathcal{H}_{k}]^{0}=L^{2}(\mathcal{X},\mu)\) and \([\mathcal{H}_{k}]^{1}=\mathcal{H}_{k}\). Also, for any \(s_{2}\geq s_{1}\geq 0\), we know \([\mathcal{H}_{k}]^{s_{1}}\subseteq[\mathcal{H}_{k}]^{s_{2}}\) with compact embedding. Let

\[\alpha_{0}=\inf_{s}\left\{s\mid[\mathcal{H}_{k}]^{s}\subseteq C^{0}(\mathcal{ X})\right\}\]

which is often referred to the embedding index of an RKHS \(\mathcal{H}_{k}\)[21]. It is well known that \(\alpha_{0}\geq\frac{1}{\beta}\) and the equality holds for a large class of usual RKHSs if the eigenvalue decay rate is \(\beta\). We further define the relative smoothness of a given function \(f\):

**Definition 2.1** (Relative smoothness).: Given a kernel \(k\) on \(\mathcal{X}\) with respect to measure \(\mu\), the smoothness of a function \(f\) is defined as

\[\alpha(f,k)=\sup\left\{\alpha>0\Big{|}\sum_{i\in\mathbb{N}}\lambda_{i}^{- \alpha}c_{i}^{2}<\infty\right\},\] (7)

where \(c_{i}=\langle f,e_{i}\rangle_{L^{2}(\mathcal{X},\mu)}\).

### Kernel gradient flow

For a positive definite reproducing kernel \(k\), the dynamic of kernel gradient flow (KGF) [22] is

\[\frac{\mathrm{d}}{\mathrm{d}t}f_{t}^{\mathrm{GF}}(x)=-\frac{1}{n}k(x,X)\left( f_{t}^{\mathrm{GF}}(X)-Y\right),\] (8)

where \(f_{t}^{\mathrm{GF}}\) is the KGF predictor. In kernel gradient flow, the performance of kernel predictor depends on the relative smoothness of regression function. People often consider the case that \(\alpha(f,k)\geq 1\)[10, 11]. When the smoothness satisfies \(\alpha(f,k)<1\), the regression function is said to be poorly smooth and belongs to the so-called misspecified spectral algorithm problem. We collect the related result in Zhang et al. [55] and apply it to our case, to derive the following proposition:

**Proposition 2.2**.: Suppose the eigenvalue decay rate of \(k\) is \(\beta\) and the embedding index is \(\frac{1}{\beta}\) with respect to \(\mu\). Suppose the noise term \(\epsilon\) satisfies Assumption 1. Let the dynamic (8) starts from \(f_{0}^{\mathrm{GF}}=0\). Also, suppose the regression function satisfies \(f^{*}\in[\mathcal{H}_{k}]^{s}\) and \(\|f^{*}\|_{[\mathcal{H}_{k}]^{s}}\leq R\), for some \(s>0\). Let \(\gamma\leq s\) and \(0\leq\gamma\leq 1\). By choosing \(t\asymp n^{\frac{\beta}{s\beta+1}}\), for any fixed \(\delta\in(0,1)\), when \(n\) is sufficient large, with probability at least \(1-\delta\), we have

\[\big{\|}f_{t}^{\mathrm{GF}}-f^{*}\big{\|}_{|[\mathcal{H}_{k}]^{\gamma}}^{2} \leq\left(\ln\frac{6}{\delta}\right)^{2}R^{2}Cn^{-\frac{(s-\gamma)\beta}{s \beta+1}},\]

where \(C\) is a positive constant.

## 3 Network and Neural Tangent Kernel

### Network settings

We consider the fully-connected network with \(L\) hidden layers. As is commonly-used in deep learning, we consider the ReLU activation [44] defined by \(\sigma(x)\coloneqq\max(x,0)\). Denote \(f(\cdot;\theta):\mathbb{R}^{d}\rightarrow\mathbb{R}\) as the network output function, where \(\theta\) representing the column vector that all parameters flattened into. We can write the recursive structure of network as following:

\[\alpha^{(1)}(x) =\sqrt{\frac{2}{m_{1}}}\left(W^{(0)}x+b^{(0)}\right);\] (9) \[\alpha^{(l)}(x) =\sqrt{\frac{2}{m_{l}}}W^{(l-1)}(x)\sigma(\alpha^{(l-1)}(x)), \quad l=2,3,\cdots,L;\] \[f(x;\theta) =W^{(L)}\sigma(\alpha^{(L)}(x)),\]

The parameter matrix for the \(l\)-th layer is denoted as \(W^{(l)}\). Their dimensions are of \(m_{l+1}\times m_{l}\), where \(m_{l}\) is the number of units in layer \(l\) and \(m_{l+1}\) is that of layer \(l+1\). Also, the bias term of the first layer is denoted as \(b^{(0)}\in\mathbb{R}^{m_{1}\times 1}\). The setting of bias term is to make sure the positive definiteness of NTK [41]. We further assume that the number of units in each layer is at the same order while the width comes to infinity, as \(cm\leq\min(m_{1},\cdots,m_{L+1})\leq\max(m_{1},\cdots,m_{L+1})\leq Cm\) where \(c,C\) are some absolute positive constants.

Standard initializationAt initialization, the parameters are randomly set as i.i.d. standard normal variables:

\[W^{(l)}_{ij},b^{(0)}_{k}\stackrel{{\mathrm{i.i.d.}}}{{\sim}} \mathcal{N}(0,1),\quad l=0,1,\cdots,L;\quad k=1,\cdots,m_{1}.\] (10)

**Remark 3.1** (Mirrored initialization).: As to the _mirrored initialization_ considered in [4, 35, 41], part of the network \(f^{(1)}(\cdot;\theta^{(1)}_{0})\) undergoes standard initialization, while the other complicated corresponding part \(f^{(2)}(\cdot;\theta^{(2)}_{0})\) holds the same structure as \(f^{(1)}(\cdot;\theta^{(1)}_{0})\), with parameters initialized to the same values as \(\theta^{(2)}_{0}=\theta^{(1)}_{0}\). Lastly, the neural network output function is defined as \(f(\cdot;\theta_{0})=\frac{\sqrt{2}}{2}\left(f^{(1)}(\cdot;\theta^{(1)}_{0})-f ^{(2)}(\cdot;\theta^{(2)}_{0})\right)\). This setup ensures that \(f(\cdot;\theta_{0})\) is constantly zero.

The network is trained under the mean square loss function. If we suppose \(\{(x_{i},y_{i})\}_{i=1}^{n}\) be the training data, then the loss function is specified as

\[\mathcal{L}(\theta)=\frac{1}{2n}\sum_{i=1}^{n}(f(x_{i};\theta)-y_{i})^{2}.\] (11)

For notational simplicity, we denote by \(f^{\mathrm{NN}}_{t}(x)=f(x;\theta_{t})\). The training process for the network is performed by gradient flow, where the parameters are updated through the differential equation:

\[\frac{\mathrm{d}}{\mathrm{d}t}\theta_{t}=-\partial_{\theta}\mathcal{L}(\theta )=-\frac{1}{n}[\partial_{\theta}f^{\mathrm{NN}}_{t}(X)]^{T}(f^{\mathrm{NN}}_{ t}(X)-Y),\] (12)

where \(\partial_{\theta}f^{\mathrm{NN}}_{t}(X)\) is a matrix with dimensions \(n\times M\), with \(M\) being the length of the parameter vector \(\theta\). This matrix represents the gradient of the network output \(f^{\mathrm{NN}}_{t}(X)\) with respect to the parameters \(\theta\) at time \(t\). Incorporating the chain rule, we can formulate the gradient flow equation for the network function as follows:

\[\frac{\mathrm{d}}{\mathrm{d}t}f^{\mathrm{NN}}_{t}(x)=-\frac{1}{n}\partial_{ \theta}f^{\mathrm{NN}}_{t}(x)[\partial_{\theta}f^{\mathrm{NN}}_{t}(X)]^{T}(f^ {\mathrm{NN}}_{t}(X)-Y).\] (13)

### Network at initialization

In order to state the properties of wide network with standard initialization, we need to introduce the concept of Gaussian process.

Gaussian processGaussian process is a stochastic process for which every finite collection of random variables follows a multivariate Gaussian distribution. Let \(X\) be a Gaussian process with index \(t\in T\). If the mean and covariance are given by the mean function \(m\) and the positive definite kernel \(k\) such that \(\mathbf{E}[X(t)]=m(t)\) and \(\mathrm{Cov}[X(t)X(t^{\prime})]=k(t,t^{\prime})\), which holds for any \(t,t^{\prime}\in T\), then we say \(X\sim\mathcal{GP}(m,k)\).

In standard initialization (10), the parameters of the neural network are i.i.d. samples from a standard normal distribution. If the network contains only one hidden-layer (that is, if \(L=2\)), it is direct to prove that \(f_{0}^{\mathrm{NN}}(x)\) converges to a centered Gaussian distribution by CLT, for any fixed point \(x\in\mathcal{X}\). As to the multi-layer network, prior research [25] also proved that such initialized network converges to a Gaussian process, as following:

**Lemma 3.2** (Limit distribution of initialization).: _As the network width \(m\) tend to infinity, the sequence of network stochastic process \(\{f_{0}^{\mathrm{NN}}\}_{m=1}^{\infty}\) converges weakly in \(C(\mathcal{X},\mathbb{R})\) to a centered Gaussian process \(f^{\mathrm{GP}}\). The covariance function is the so-called random feature kernel (RFK), which is denoted by \(K^{\mathrm{RFK}}(x,x^{\prime})\) as defined in (42) in Appendix C.1._

### The kernel regime

As the gradient descent of neural network involves high non-linearity and non-convexity, it is difficult to study the training process. However, Jacot et al. [31] introduced the Neural Tangent Kernel (NTK) theory which provides a connection between network training and a class of kernel regression problems, when the network width comes to infinity. To demonstrate this, we first define a Neural Network Kernel (NNK):

\[K_{t}^{m}(x,x^{\prime})=[\partial_{\theta}f_{t}(x)]^{T}[\partial_{\theta}f_{t} (x^{\prime})].\] (14)

Using this notation, we reformulate (13) in a kernel regression format:

\[\frac{\mathrm{d}}{\mathrm{d}t}f_{t}^{\mathrm{NN}}(x)=-\frac{1}{n}K_{t}^{m}(x, X)(f_{t}^{\mathrm{NN}}(X)-Y).\] (15)

NTK theory shows, if the network width \(m\) tends to infinity, then the random kernel \(K_{t}^{m}(\cdot,\cdot)\) will converge to a time-invariant kernel \(K^{\mathrm{NTK}}(\cdot,\cdot):\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\), which is referred to as the NTK of network. The phenomenon is the so-called NTK regime [2, 31, 39]. The fixed kernel \(K^{\mathrm{NTK}}\) only depends on the structure of the neural network and the way of initialization. To get more knowledge of NTK, we present the explicit expression of NTK in Appendix C.1. In NTK theory, the dynamic of network (15) can be approximated by a kernel gradient flow equation:

\[\frac{\mathrm{d}}{\mathrm{d}t}f_{t}^{\mathrm{NTK}}(x)=-\frac{1}{n}K^{\mathrm{ NTK}}(x,X)(f_{t}^{\mathrm{NTK}}(X)-Y),\] (16)

which starts from Gaussian process \(f_{0}^{\mathrm{NTK}}=f^{\mathrm{GP}}\). In this way, if we aims to derive the generalization property of sufficiently wide network, we can achieve by considering the corresponding kernel gradient flow predictor. Such approximation is strictly ensured by uniform convergence of \(f_{t}^{\mathrm{NN}}\) and \(f_{t}^{\mathrm{NTK}}\) over all \(x\in\mathcal{X}\) and all \(t\geq 0\) as \(m\to\infty\), since we use \(L^{2}\) excess risk to evaluate the generalization ability. Actually, we have the following theorem, whose proof is given in Appendix B.

**Proposition 3.3** (Uniform convergence).: Given training sample pairs \(\{(x_{i},y_{i})\}_{i=1}^{n}\). For any \(\delta\in(0,1)\) and \(\varepsilon>0\), when network width \(m\) is large enough, we have

\[\sup_{x,x^{\prime}\in\mathcal{X}}\sup_{t\geq 0}\lvert f_{t}^{\mathrm{NN}}(x)-f _{t}^{\mathrm{NTK}}(x)\rvert\leq\varepsilon\]

holds with probability at least \(1-\delta\).

In this theorem, we show the uniform convergence of network under standard initialization. Previous related studies [4, 35, 41, 54] always utilized delicately designed mirrored initialization (as shown in Remark 3.1) to avoid the analysis on the initial output function of network, since it will lead to the challenging problem that \(f_{t}^{\mathrm{NN}}\) and \(f_{t}^{\mathrm{NTK}}\) are both random, unlike that \(f_{t}^{\mathrm{NTK}}\) is a fixed function in the case of mirrored initialization. However, as shown in Section 3.2, the initial output function is near a Gaussian process that can not be overlooked. To under the performance of neural networks commonly used in real world, it is necessary to analyzing the network initialization. To the best of our knowledge, we are the first to consider the initial output function of network in uniform convergence. This comprehensive result allows us to study the generalization error of network more precisely.

Impact of Initialization

### Impact of standard initialization on the generalization error

The standard kernel gradient flow is always considered to start from zero, as in Proposition 2.2. Therefore, we need to do a transformation since the initial value of predictor \(f_{t}^{\mathrm{NTK}}\) is actually \(f^{\mathrm{GP}}\) instead of zero. Firstly, we can yield a solution of (8) in matrix form:

\[f_{t}^{\mathrm{GF}}(x)=f_{0}^{\mathrm{GF}}(x)+k(x,X)(I-e^{-\frac{1}{n}k(X,X)} )\left[k(X,X)\right]^{-1}(f_{0}^{\mathrm{GF}}(X)-f^{*}(X)-\epsilon_{X}),\] (17)

where \(\epsilon_{X}\) is employed to represent the \(n\times 1\) column noise term vector \(Y-f^{*}(X)\). We denote by \(f_{t}^{\mathrm{GF}}\) the kernel gradient flow predictor under initial function \(f_{0}\) and denote by \(\widetilde{f}_{t}^{\mathrm{GF}}\) the KGF predictor under initialization \(\widetilde{f}_{0}^{\mathrm{GF}}\equiv 0\). If we plug them into (17) and excess risk (2), respectively, we directly have the following theorem:

**Proposition 4.1** (Impact of initialization in kernel gradient flow).: Denote \(\widetilde{f}^{*}=f^{*}-f_{0}\) as the biased regression function. For the KGF predictor \(f_{t}^{\mathrm{GF}}\) and \(\widetilde{f}_{t}^{\mathrm{GF}}\) defined above, we have

\[\mathcal{E}(f_{t}^{\mathrm{GF}};f^{*})=\mathcal{E}(\widetilde{f}_{t}^{ \mathrm{GF}};\widetilde{f}^{*}).\] (18)

The theorem establishes the equivalence of the generalization properties between the KGF predictor with initial value \(f_{0}\), regression function \(f^{*}\) and the KGF predictor with initial value zero, regression function \(f^{*}-f_{0}\). Back to the network case, combining uniform convergence result in Proposition 3.3, it suggests that, compared to mirrored initialization, the impact of standard initialization which has non-zero initial output function is equivalent to introducing a same-valued implicit bias to the regression function. This is a generalization of the main result in Zhang et al. [56], which only focused on case at \(t=\infty\). To summarize, Proposition 4.1 provides a convenient approach to quantify the impact of standard initialization in early-stopping neural networks.

### Smoothness of Gaussian process

Building upon the analysis above, our focus now turns to illustrating the smoothness of the Gaussian process \(f^{\mathrm{GP}}\), as it is the limit distribution of \(f_{0}^{\mathrm{NN}}\). Actually, we can derive the following theorem:

**Theorem 4.2** (Smoothness of Gaussian Process).: _Suppose that \(f^{\mathrm{GP}}\) is a Gaussian process with mean function 0 and covariance function \(K^{\mathrm{RFK}}\). The following statements hold:_

\[\begin{split}\mathbf{P}\left(f^{\mathrm{GP}}\in[\mathcal{H}^{ \mathrm{NT}}]^{s}\right)&=1,\qquad s<\frac{3}{d+1};\\ \mathbf{P}\left(f^{\mathrm{GP}}\in[\mathcal{H}^{\mathrm{NT}}]^{s} \right)&=0,\qquad s\geq\frac{3}{d+1}.\end{split}\] (19)

We furnish a comprehensive proof for Theorem 4.2 in the Appendix C.

Let us now turn our attention to the implications established by this theorem. Recall that Proposition 4.1 has shown that, in KGF, the existence of initialization function \(f^{\mathrm{GP}}\) is equivalent to adding a same-valued bias term to the regression function \(f^{*}\). Consequently, the poor smoothness of initialization function causes the high smoothness assumption on the regression function meaningless. Regardless of how smooth we assume the regression function to be (e.g., \(\alpha(f^{*},K^{\mathrm{NTK}})\geq 2\)), the value of (relative) smoothness \(\alpha(f^{*}-f^{\mathrm{GP}},K^{\mathrm{NTK}})\) will always be at most \(\frac{3}{d+1}\). Namely, the biased regression function \(f^{*}-f^{\mathrm{GP}}\) is always poorly smooth. In this specific case, we could hardly expect the KGF predictor to have fine performance.

### Upper bound

Now we are ready to provide the upper bound of generalization error of network. With the help of Proposition 2.2, Proposition 3.3, Proposition 4.1 and Theorem 4.2, we derive the following theorem:

**Theorem 4.3** (Generalization error upper bound).: _Assume that the regression function \(f^{*}\in[\mathcal{H}^{\mathrm{NT}}]^{s}\) for some \(s>0\), and \(\|f^{*}\|_{[\mathcal{H}^{\mathrm{NT}}]^{s}}\leq R\) where \(R\) is a positive constant. Assume the marginal probability measure \(\mu\) with density \(p(x)\) satisfies \(c\leq p(x)\leq C\) for some positive constant \(c\) and \(C\)._* _For the case of_ \(s\geq\frac{3}{4+1}\)_, for any_ \(\delta\in(0,1)\) _and_ \(\varepsilon\in(0,\frac{3}{4+3})\)_, by choosing certain_ \(t=t(n)\rightarrow\infty\) _(as shown in Appendix), when_ \(n\) _is sufficiently large and_ \(m\) _is sufficiently large, with probability_ \(1-\delta\) _we have_ \[\left\|f_{t}^{\mathrm{NN}}-f^{*}\right\|_{L^{2}}^{2}\leq\left(\frac{1}{\delta} \ln\frac{6}{\delta}\right)^{2}(R+C_{\varepsilon})^{2}Cn^{-\frac{3}{4+3}+ \varepsilon},\] (20) _where_ \(C_{\varepsilon}\) _is a positive constant related to_ \(\varepsilon\)_._
* _For the case of_ \(0<s<\frac{3}{4+1},\) _for any_ \(\delta\in(0,1)\)_, by choosing_ \(t\asymp n^{\frac{d+1}{\varepsilon(d+1)+4}}\)_, when_ \(n\) _is sufficiently large and_ \(m\) _is sufficiently large, with probability_ \(1-\delta\) _we have_ \[\left\|f_{t}^{\mathrm{NN}}-f^{*}\right\|_{L^{2}}^{2}\leq\left(\frac{1}{\delta} \ln\frac{6}{\delta}\right)^{2}(R+C_{s})^{2}Cn^{-\frac{s(d+1)}{s(d+1)+d}},\] (21) _where_ \(C_{s}\) _is a positive constant related to_ \(s\)_._

The proof is provided in Appendix D. This result shows the generalization error upper bound for network with standard initialization and demonstrate its negative effect. Even if the goal function \(f^{*}\) is quite smooth, the generalization error upper bound \(n^{-\frac{3}{4+3}}\) remains to be a quite low rate, particularly considering that the dimension \(d\) of data is usually large in real world. It suggests that the network no longer generalizes well, even if we adopt the once useful early stopping strategy in Li et al. [41].

### Lower bound

From the analysis above, we can see the poor generalization ability of network under standard initialization. Furthermore, in this section, we take spherical data as example and provide the lower bound of generalization error. Namely, we presume the input vectors \(x\) are distributed on the sphere \(\mathbb{S}^{d}\) with probability measure \(\mu\), which is a common assumption in NTK theory [6, 31, 36, 53]. We also slightly change the network structure. Compared to the network (9), we eliminate the bias term of the initial layer, as shown in (40) in Appendix. In this case, the NTK of new network is denoted by \(K_{0}^{\mathrm{NTK}}\), and the RKHS \(\mathcal{H}_{0}^{\mathrm{NT}}(\mathbb{S}^{d})\) is abbreviated as \(\mathcal{H}_{0}^{\mathrm{NT}}\), whose detailed properties is also given in Appendix C.1. Additionally, we make more assumption on the noise of data. We assume the noise term \(\epsilon\) in (1) to have a constant second moment, as \(\mathbf{E}\left[|\epsilon|^{2}|x\right]=\sigma^{2}\) for \(x\in\mathbb{S}^{d},a.e..\). Under these conditions, with the help of method in Li et al. [40], we derive the theorem:

**Theorem 4.4** (Generalization error lower bound).: _We assume that the regression function \(f^{*}\in[\mathcal{H}_{0}^{\mathrm{NT}}]^{s}\) for some \(s>\frac{3}{d+1}\), and denote by \(\|f^{*}\|_{[\mathcal{H}_{0}^{\mathrm{NT}}]^{s}}\leq R\) where \(R\) is a positive constant. Assume that \(\mu\) is the uniform measure. For any \(\delta\in(0,1)\), when \(n\) is large enough and \(m\) is large enough, for any choice of \(t=t(n)\rightarrow\infty\), with probability at least \(1-\delta\) we have_

\[\mathbf{E}\left[\left\|f_{t}^{\mathrm{NN}}-f^{*}\right\|_{L^{2}}^{2}\middle|X \right]=\Omega\left(n^{-\frac{3}{d+3}}\right).\] (22)

The proof is given in Appendix E. Through Theorem 4.4, we derive \(n^{-\frac{3}{d+3}}\) as the generalization lower bound of standardly random-initialized network in NTK theory, even if the regression function is quite smooth. The rate \(n^{-\frac{3}{d+3}}\) means model suffers notably from data that has large dimension: If \(d\) is relatively large, then this rate of convergence can be extremely slow. This is a manifestation of the curse of dimensionality. In fact, it contrasts with the fact that neural networks excel at high-dimensional problems. This contradiction underscores the limitation of NTK theory for interpreting network performance.

## 5 Experiments

Our numerical experiments are conducted in two aspects to fully understand the impact of standard initialization. First, we show the performance of standard initialized network is indeed worse than the mirrored initialized case, on the aspect of learning rate. The phenomenon is in line with our theoretical analysis. Second, the smoothness of regression function of real data is significantly larger than \(\frac{3}{4+1}\), which suggest the bad effect of non-zero intial output function of standard initialization will indeed destroy the performance of network if NTK theory holds. It demonstrates the drawback of NTK theory through contradiction.

### Artificial data

In the first experiment, we employ artificial data to show the negative effect of standard initialization on the generalization error of network. The detailed settings are shown in Appendix F.

Learning rate of network under different initializationThe experiments are conducted for both \(d=5\) and \(d=10\), contrasting network performance subject to mirrored and standard initialization strategies. We choose a relatively smooth goal function to emphasize the impact of initialization. Specifically, we use \(m=20n\), epoch \(=10n\), and the gradient learning rate \(lr=0.6\). The networks are made sufficiently wide to ensure the overparametrization assumption is met. Additionally, we implement the early-stopping strategy as mentioned in Theorem 4.3, that is, selecting the minimum loss across all epochs as the generalization error. Finally, we test the network's generalization error on different levels of sample size \(n\), and plot the log value of the generalization error corresponding to \(\log(n)\) as shown in Figure 1. As we expected, the points in Figure 1 fits a linear trend. Moreover, the figure highlights the difference in learning rate under different initialization methods. This aligns with our theoretical results.

### Real data

In this subsection, we focus on datasets from the real world and estimate the smoothness of function. Although we could not know the goal function that the real data is generated from, there exists a way to estimate its smoothness [16]. We show the technical details in Appendix G.

Smoothness of goal function in real datasetsWe employed the MNIST, CIFAR-10 and Fashion-MNIST datasets[33, 38, 52]. In the experiments, we evaluate the smoothness of goal function of the datasets, with respect to the one-hidden layer NTK. The results are presented in Table 1. With the input dimension \(d=784,3072,784\), we can compute that the smoothness of initialization function is equal to \(\frac{3}{4+1}\approx 0\). However, the smoothness of goal function is far better than \(\frac{3}{4+1}\), which implies that standard initialization will indeed destroy the generalization performance, under NTK theory. The contradiction between NTK theory and the real situation shows its limitation and once again confirms our conclusion.

\begin{table}
\begin{tabular}{l l l} \hline \hline \multicolumn{3}{c}{Dataset} \\ \hline Name & Dimension & Smoothness \\ \hline MNIST & \(28\times 28\times 1\) & \(0.40\) \\ CIFAR-10 & \(32\times 32\times 3\) & \(0.09\) \\ Fashion-MNIST & \(28\times 28\times 1\) & \(0.22\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Smoothness of goal function

Figure 1: Generalization error decay curve of network. The scatter points show the averaged log error over \(20\) trials. The dashed lines are computed through least-squares. The scale of \(n\) is not broad because a larger \(n\) requires a larger \(m\), which would induce higher computational costs.

Discussion

To summarize, this research focuses on the impact of standard random initialization on generalization property of fully-connected network in the NTK theory, which makes up the gap in this field. Many previous work [35; 41] verified the statistical optimality of neural network under delicately designed mirrored initialization, whose initial output function of network is zero. However, through our study, we pinpoint that if we consider the commonly-used standard initialization, the learning rate of network is notably slow when the dimension of data is slightly large, which fails to explain network's favorable performance in overcoming the curse of dimensionality. A direct implication of our work is the superiority of mirror initialization over standard initialization, which suggests a direction for future improvements. On a deeper level, although NTK theory can describe many properties of network, at least for the fully connected networks with Gaussian initialization discussed in this paper, we can explore better theoretical frameworks to characterize their generalization ability in the future.

## Acknowledgments

Lin's research was supported in part by the National Natural Science Foundation of China (Grant 92370122, Grant 11971257).

## References

* [1] Robert A Adams and John JF Fournier. _Sobolev spaces_. Elsevier, 2003.
* [2] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning_, pages 242-252. PMLR, 2019.
* [3] N. Aronszajn. Theory of reproducing kernels. _Transactions of the American Mathematical Society_, 68(3):337-404, 1950. URL http://dx.doi.org/10.2307/1990404.
* [4] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. _Advances in Neural Information Processing Systems_, 32, 2019.
* [5] Benedikt Bauer and Michael Kohler. On deep learning as a remedy for the curse of dimensionality in nonparametric regression. 2019.
* [6] Alberto Bietti and Francis Bach. Deep equals shallow for relu networks in kernel regimes. _arXiv preprint arXiv:2009.14397_, 2020.
* [7] Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. _Advances in Neural Information Processing Systems_, 32, 2019.
* [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [9] William Elwood Byerly. _An elementary treatise on Fourier's series, and spherical, cylindrical, and ellipsoidal harmonics, with applications to problems in mathematical physics_. Dover Publications, 1893.
* [10] Andrea Caponnetto. Optimal rates for regularization operators in learning theory. 2006.
* [11] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. _Foundations of Computational Mathematics_, 7:331-368, 2007.
* [12] Claudio Carmeli, Ernesto De Vito, and Alessandro Toigo. Reproducing kernel hilbert spaces and mercer theorem. _arXiv preprint math/0504071_, 2005.
* [13] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in neural information processing systems_, 32, 2019.
* [14] Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. _Advances in neural information processing systems_, 22, 2009.
* [15] Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In _Conference on learning theory_, pages 698-728. PMLR, 2016.
* [16] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime. _Advances in Neural Information Processing Systems_, 34:10131-10143, 2021.
* [17] George Cybenko. Approximation by superpositions of a sigmoidal function. _Mathematics of control, signals and systems_, 2(4):303-314, 1989.
* [18] Eleonora Di Nezza, Giampiero Palatucci, and Enrico Valdinoci. Hitchhiker's guide to the fractional sobolev spaces. _Bulletin des sciences mathematiques_, 136(5):521-573, 2012.
* [19] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pages 1675-1685. PMLR, 2019.
* [20] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations_, 2018.

* Fischer and Steinwart [2020] Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algorithms. _The Journal of Machine Learning Research_, 21(1):8464-8501, 2020.
* Coferfo et al. [2008] L Lo Gerfo, Lorenzo Rosasco, Francesca Odone, E De Vito, and Alessandro Verri. Spectral algorithms for supervised learning. _Neural Computation_, 20(7):1873-1897, 2008.
* Grosse and Schneider [2013] Nadine Grosse and Cornelia Schneider. Sobolev spaces on riemannian manifolds with bounded geometry: general coordinates and traces. _Mathematische Nachrichten_, 286(16):1586-1613, 2013.
* Haas et al. [2023] Moritz Haas, David Holzmuller, Ulrike von Luxburg, and Ingo Steinwart. Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. _arXiv preprint arXiv:2305.14077_, 2023.
* Hanin [2021] Boris Hanin. Random neural networks in the infinite width limit as gaussian processes. _arXiv preprint arXiv:2107.01562_, 2021.
* Hanin and Sellke [2017] Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width. _arXiv preprint arXiv:1710.11278_, 2017.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hornik et al. [1989] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. _Neural networks_, 2(5):359-366, 1989.
* Hu et al. [2021] Tianyang Hu, Wenjia Wang, Cong Lin, and Guang Cheng. Regularization matters: A nonparametric perspective on overparametrized neural network. In _International Conference on Artificial Intelligence and Statistics_, pages 829-837. PMLR, 2021.
* Hubbert et al. [2022] Simon Hubbert, Emilio Porcu, Chris Oates, Mark Girolami, et al. Sobolev spaces, kernels and discrepancies over hyperspheres. _arXiv preprint arXiv:2211.09196_, 2022.
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* Koltchinskii and Gine [2000] Vladimir Koltchinskii and Evarist Gine. Random matrix approximation of spectra of integral operators. _Bernoulli_, pages 113-167, 2000.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Krizhevsky et al. [2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* Lai et al. [2023] Jianfa Lai, Manyun Xu, Rui Chen, and Qian Lin. Generalization ability of wide neural networks on r. _arXiv preprint arXiv:2302.05933_, 2023.
* Lai et al. [2023] Jianfa Lai, Zixiong Yu, Songtao Tian, and Qian Lin. Generalization ability of wide residual networks. _arXiv preprint arXiv:2305.18506_, 2023.
* LeCun et al. [1998] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* LeCun et al. [2010] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2, 2010.
* Lee et al. [2019] Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. _Advances in neural information processing systems_, 32:8572-8583, 2019.
* Li et al. [2024] Yicheng Li, Weiye Gan, Zuoqiang Shi, and Qian Lin. Generalization error curves for analytic spectral algorithms under power-law decay. _arXiv preprint arXiv:2401.01599_, 2024.
* Li et al. [2024] Yicheng Li, Zixiong Yu, Guhan Chen, and Qian Lin. On the eigenvalue decay rates of a class of neural-network related kernel functions defined on general domains. _Journal of Machine Learning Research_, 25(82):1-47, 2024.

* [42] Junhong Lin and Volkan Cevher. Optimal convergence for distributed learning with stochastic gradient methods and spectral algorithms. _Journal of Machine Learning Research_, 21(147):1-63, 2020.
* [43] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. _Advances in neural information processing systems_, 30, 2017.
* [44] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In _Proceedings of the 27th international conference on machine learning (ICML-10)_, pages 807-814, 2010.
* [45] Yoshihiro Sawano et al. _Theory of Besov spaces_, volume 56. Springer, 2018.
* [46] Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation function. 2020.
* [47] Alex Smola, Zoltan Ovari, and Robert C Williamson. Regularization with dot-product kernels. _Advances in neural information processing systems_, 13, 2000.
* [48] Ingo Steinwart and Clint Scovel. Mercer's theorem on general domains: On the interaction between measures, kernels, and rkhss. _Constructive Approximation_, 35:363-417, 2012.
* [49] Robert S Strichartz. _A guide to distribution theory and Fourier transforms_. World Scientific Publishing Company, 2003.
* [50] Namjoon Suh, Hyunouk Ko, and Xiaoming Huo. A non-parametric regression viewpoint: Generalization of overparametrized deep relu network under noisy observations. In _International Conference on Learning Representations_, 2021.
* [51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [52] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* [53] Lechao Xiao, Hong Hu, Theodor Misiakiewicz, Yue Lu, and Jeffrey Pennington. Precise learning curves and higher-order scalings for dot-product kernel regression. _Advances in Neural Information Processing Systems_, 35:4558-4570, 2022.
* [54] Jiaming Xu and Hanjing Zhu. Overparametrized multi-layer neural networks: Uniform concentration of neural tangent kernel and convergence of stochastic gradient descent. _Journal of Machine Learning Research_, 25(94):1-83, 2024.
* [55] Haobo Zhang, Yicheng Li, and Qian Lin. On the optimality of misspecified spectral algorithms. _arXiv preprint arXiv:2303.14942_, 2023.
* [56] Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error induced by initialization in deep neural networks. In _Mathematical and Scientific Machine Learning_, pages 144-164. PMLR, 2020.

Further notations

In appendix, we will provide many technical proofs. Before that, let us provide more notations. For two sets \(A\) and \(B\) with a mapping function \(\phi:A\to B\), the notation \(\phi(A)\) is used to denote the image set of \(A\) under \(\phi\). For two random variable sequences \(\{u_{n}\}\) and \(\{v_{n}\}\), we denote by \(u_{n}=o_{\mathbf{P}}(v_{n})\) (or \(u_{n}=\Omega\mathbf{p}\left(v_{n}\right)\)) if the ratio \(u_{n}/v_{n}\) approaches zero (or \(u_{n}\geq cv_{n}\) for some positive constant \(c\)) in probability as \(n\to\infty\) with respect to probability measure \(\mathbf{P}\). For two real number sequence \(\{a_{n}\}\) and \(\{b_{n}\}\), we denote by \(a_{n}=\Omega(b_{n})\) if there exists positive constant \(c\) and \(n_{0}\) such that \(|a_{n}|\geq c|b_{n}|\) holds for any \(n\geq n_{0}\). For two sequences of real numbers \(\{a_{n}\}\) and \(\{b_{n}\}\) such that \(a_{n}=\Omega(b_{n})\) (or \(a_{n}=O(b_{n})\)), we also denote by \(a_{n}\gtrsim b_{n}\) (or \(a_{n}\lesssim b_{n}\) ). If \(a_{n}\gtrsim b_{n}\) and \(a_{n}\lesssim b_{n}\), then we denote by \(a_{n}\asymp b_{n}\).

## Appendix B Proof of uniform convergence

In this section, we demonstrate the uniform convergence from \(f_{t}^{\mathrm{NN}}\) to \(f_{t}^{\mathrm{NTK}}\).

### Initialization

The following is a direct proposition based on Lemma H.2 and Lemma 3.2,

**Proposition B.1**.: For the random network function sequence \(\{f_{0}^{m}\}\) with probability measures on \((C(\mathcal{X},\mathbb{R}),\mathcal{C})\), there exists \(\{X_{m}\}\) and \(X^{\mathrm{GP}}\) defined on a new probability space \((\Omega^{\prime},\mathcal{F},\mathbf{P})\), on which we have

\[\mathbf{P}(\lim_{m\to\infty}\left\|X_{m}-X^{\mathrm{GP}}\right\|_{\infty}=0)=1.\]

where \(X_{m}\) and \(X^{\mathrm{GP}}\) has the same distribution as \(f_{0}^{m}\) and \(f^{\mathrm{GP}}\), respectively.

**Remark B.2**.: The separability of \((C(\mathcal{X},\mathbb{R}),\mathcal{C})\) can be derived by the density of polynomials. Therefore, it satisfies the requirement of Lemma H.2. In the context of our study, our reliance is only on the distribution of \(\{f_{0}^{m}\}\) for each given value of \(m\). Consequently, it is reasonable to reconstruct it in the new probability space. For convenience, we directly denote \(X_{0}^{m}\) as \(f_{0}^{m}\) (or \(f_{0}^{\mathrm{NN}}\)) and denote and \(X^{\mathrm{GP}}\) as \(f^{\mathrm{GP}}\), respectively. In other words, we are considering the network function in a new probability space, even though this approach may result in a moderate abuse of notation.

### Uniform convergence of network

Our aim is to give the uniform convergence between NTK regressor \(f_{t}^{\mathrm{NTK}}\) and network function \(f_{t}^{\mathrm{NN}}\). Note that the NTK regressor is trained by NTK, and the network function is trained by NNK, which is denoted by \(K_{t}^{m}\). Here we first show the uniform convergence between NNK and NTK as \(m\) comes to infinity.

**Lemma B.3**.: _For any \(\delta\in(0,1)\), suppose \(m\) is large enough, then with probability at least \(1-\delta\), we have_

\[\sup_{t\geq 0}\sup_{x,x^{\prime}\in\mathcal{X}}|K_{t}^{m}(x,x^{\prime})-K^{ \mathrm{NTK}}(x,x^{\prime})|\leq O(m^{-\frac{1}{12}}\sqrt{\log m}).\]

Proof.: The proof is similar to that in Li et al. [41], while the difference is the way of initialization. So we only provide the sketch of proof. In Li et al. [41], the uniform convergence of NTK is proved through a standard \(\epsilon-net\) argument, which is divided into point-wise convergence and continuity of both NTK and NNK. Namely, as the following decomposition:

\[|K_{t}^{m}(x,x^{\prime})-K^{\mathrm{NTK}}(x,x^{\prime})|\leq |K_{t}^{m}(x,x^{\prime})-K_{t}^{m}(z,z^{\prime})|\] \[+|K_{t}^{m}(z,z^{\prime})-K^{\mathrm{NTK}}(z,z^{\prime})|+|K^{ \mathrm{NTK}}(z,z^{\prime})-K^{\mathrm{NTK}}(x,x^{\prime})|.\] (23)

where \(z,z^{\prime}\) are the points in the \(\epsilon-net\) which divides \(\mathcal{X}\).

Back to our case, in non-zero initialization, the structrue of NTK and NNK remain the same, as well as the continuity property. Consequently, the effect of initialization reflects on the point-wise convergence from \(K_{t}^{m}(z,z^{\prime})\) to \(K^{\mathrm{NTK}}(z,z^{\prime})\), or more precisely, the NTK regime [2]. NTK regime requires that the residual decays to near zero and thereby the parameters will not deviate too far from their initial values in the training process, which holds under mirrored initialization. Standardinitialization lets the residual at time \(0\) be \(\left\|f_{0}^{\mathrm{NN}}(X)-Y\right\|_{2}\), instead of \(\left\|Y\right\|_{2}\). Therefore, there is a slight risk that the residual is too large to decay to near zero during training. However, since

\[\left\|f_{0}^{\mathrm{NN}}(X)\right\|_{2}\leq O(n\cdot m^{\frac{1}{8}}),\] (24)

holds with high probability when \(m\) is large through Proposition B.1 and direct analysis on \(f^{\mathrm{GP}}\), we can verify that the residual \(\left\|f_{t}^{\mathrm{NN}}(X)-Y\right\|_{2}\) is still not large enough to break the stable lazy regime. Namely, the control on parameter matrix that

\[\sup_{t\geq 0}\left\|W_{t}^{(l)}-W_{0}^{(l)}\right\|_{\mathrm{F}}=O(m^{\frac{1} {4}}).\] (25)

still holds. In this way, we can finish the proof. 

Then, we can derive the uniform convergence of network function.

Proof of Proposition 3.3.: The proof is also similar to that of uniform convergence under mirrored initialization. Therefore, we only exhibit the sketch of different part. Define event \(A\) as

\[A=\left\{\|f_{0}^{\mathrm{NN}}-f^{\mathrm{GP}}\|_{\infty}\leq o_{m}(1)\right\} \cap\left\{\left\|f^{\mathrm{GP}}(X)\right\|_{2}\leq C_{\delta}\right\}\] (26)

where \(C_{\delta}\) is some constant related to \(\delta\), such that event \(A\) holds with probability at least \(1-\frac{\delta}{2}\) when \(m\) is large enough. Such a constant \(C_{\delta}\) is ascertainable, as \(f_{0}^{\mathrm{NN}}\) converges to \(f^{\mathrm{GP}}\) by Proposition B.1 and \(f^{\mathrm{GP}}\) is a Gaussian process with finite second moment. Define event \(B\) as

\[B=\left\{\sup_{t\geq 0}\sup_{x,x^{\prime}\in\mathcal{X}}|K_{t}^{m}(x,x^{\prime })-K^{\mathrm{NTK}}(x,x^{\prime})|\leq o_{m}(1)\right\}.\] (27)

We have event \(B\) holds with probability at least \(1-\frac{\delta}{2}\) when \(m\) is large enough. Conditioned on event \(A\) and \(B\), we do kernel gradient flow by \(K_{t}^{m}\) and \(K^{\mathrm{NTK}}\) on \(f_{0}^{\mathrm{NN}}\) and \(f^{\mathrm{GP}}\) respectively. Let event \(C\) be

\[C=\left\{\sup_{t\geq 0}\|f_{t}^{\mathrm{NN}}-f_{t}^{\mathrm{NTK}}\|_{\infty} \leq o_{m}(1)\right\}.\] (28)

Conditioned on event \(A\) and \(B\), we can prove that event \(C\) holds by Gronwall's inequality, as the same method in Lai et al. [35]. In this way, we can finish the proof. 

After we get the uniform convergence of network function, we can obtain the proposition on the convergence of excess risk:

**Proposition B.4**.: Suppose \(f^{*}\in L^{2}(\mathcal{X},\mu)\). For any \(\delta\in(0,1)\) and \(\varepsilon>0\), when \(m\) is large enough, with probability at least \(1-\delta\), we have

\[\sup_{t>0}\left\|\left\|f_{t}^{\mathrm{NN}}-f^{*}\right\|_{L^{2}}^{2}-\left\| f_{t}^{\mathrm{NTK}}-f^{*}\right\|_{L^{2}}^{2}\right\|\leq\varepsilon\] (29)

Proof.: Recall the dynamic equation of \(f_{t}^{\mathrm{NTK}}\), we have

\[|f_{t}^{\mathrm{NTK}}(x)|\leq\left\|K^{\mathrm{NTK}}(x,X)^{T}\right\|_{2}\left\| K^{\mathrm{NTK}}(X,X)^{-1}\right\|_{2}\left\|f_{0}^{\mathrm{NTK}}(X)-Y \right\|_{2}.\] (30)

Since the kernel function \(K^{\mathrm{NTK}}(\cdot,\cdot)\) is bounded, there exists some positive constant \(C\), such that

\[\left\|K^{\mathrm{NTK}}(x,X)^{T}\right\|_{2}\leq C\sqrt{n}.\] (31)

The initial function of kernel gradient flow \(f_{0}^{\mathrm{NTK}}=f^{\mathrm{GP}}\) follows a Gaussian process with mean \(0\) and covariance kernel function \(K^{\mathrm{RFK}}\). By the boundness of \(K^{\mathrm{RFK}}\), we can also bound \(f_{0}^{\mathrm{NTK}}\). That is, for any \(\delta\in(0,1)\), there exists a positive constant \(M_{\delta}\) such that with probability at least \(1-\delta/2\),

\[\left\|f_{0}^{\mathrm{NTK}}(X)\right\|_{2}\leq\sqrt{n}M_{\delta}.\] (32)

Denote \(\lambda_{0}\coloneqq\lambda_{\min}\left(K^{\mathrm{NTK}}(X,X)\right)\). We have \(\lambda>0\) since \(K^{\mathrm{NTK}}\) is strictly positive definite [41]. Thus we have

\[|f_{t}^{\mathrm{NTK}}(x)|\leq C\sqrt{n}\lambda_{0}^{-1}(\sqrt{n}M_{\delta}+ \left\|Y\right\|_{2}).\] (33)The excess risk

\[|\mathcal{E}(f_{t}^{\mathrm{NN}};f^{*})-\mathcal{E}(f_{t}^{\mathrm{NTK}};f^{*})|= \left|\int_{\mathcal{X}}|f_{t}^{\mathrm{NN}}-f_{t}^{\mathrm{NTK}}|^{2}\,\mathrm{ d}\mu+\int_{\mathcal{X}}(f_{t}^{\mathrm{NTK}}-f^{*})(f_{t}^{\mathrm{NN}}-f_{t}^{ \mathrm{NTK}})\,\mathrm{d}\mu\right|\] (34)

Since \(f^{*}\in L^{2}(\mathcal{X},\mu)\) where \(\mu\) is probability measure, we also have \(f^{*}\in L^{1}(\mathcal{X},\mu)\). Denote \(M_{f^{*}}\coloneqq\left\|f^{*}\right\|_{L^{1}}\) and \(\Delta\coloneqq\sup_{x\in\mathcal{X},t\geq 0}|f_{t}^{\mathrm{NN}}(x)-f_{t}^{ \mathrm{NTK}}|\). We have

\[|\mathcal{E}(f_{t}^{\mathrm{NN}};f^{*})-\mathcal{E}(f_{t}^{\mathrm{NTK}};f^{*} )|\leq\Delta^{2}\cdot(1+C\sqrt{n}\lambda_{0}^{-1}(\sqrt{n}M_{\delta}+\left\| Y\right\|_{2})+M_{f^{*}})\] (35)

By Proposition 3.3, when \(m\) is large enough, with probability at least \(1-\delta\) we have \(\Delta^{2}\leq\varepsilon/(1+C\sqrt{n}\lambda_{0}^{-1}(\sqrt{n}M_{\delta}+ \left\|Y\right\|_{2})+M_{f^{*}})\). Thus the proposition is proved.

## Appendix C Proof of the Theorem 4.2

Before the proof, first we introduce some basic properties of NTK and RFK, as well as some technical properties of Sobolev space. We say that two Hilbert space \(\mathcal{H}_{1},\mathcal{H}_{2}\) are equivalent if they are equal as sets and share equivalent norm. If \(\mathcal{H}_{1}\) and \(\mathcal{H}_{2}\) are equivalent, we denote by \(\mathcal{H}_{1}\cong\mathcal{H}_{2}\).

### Basic properties of NTK and RFK

**Dot-product kernel**  A reproducing kernel function \(k\) is dot-product if its value only depends on the dot-product of inputs. That is, there exists function \(\kappa\) such that

\[k(x,x^{\prime})=\kappa(\langle x,x^{\prime}\rangle).\] (36)

A dot-product kernel on sphere can be decomposed with spherical harmonic polynomials as the eigenfunction:

\[k(x,y)=\sum_{n=0}^{\infty}\mu_{n}\sum_{l=1}^{a_{n}}Y_{n,l}(x)Y_{n,l}(y).\] (37)

where spherical harmonic polynomials \(\{Y_{n,l},l=1,\cdots,a_{n}\}\) are also the orthonormal basis of \(L^{2}(\mathbb{S}^{d},\sigma)\), with \(\sigma\) denoting the uniform measure on \(\mathbb{S}^{d}\)[47]. This is also its Mercer decomposition.

Now come back to our network case. We first define two dot-product kernels on \(\mathbb{S}^{d}\),

\[K_{0}^{\mathrm{NTK}}(x,y)\coloneqq\sum_{r=0}^{L}\kappa_{1}^{(r)}(u)\prod_{s=r }^{L-1}\kappa_{0}(\kappa_{1}^{(s)}(u)),\quad K_{0}^{\mathrm{RFK}}(x,y) \coloneqq\kappa_{1}^{(L)}(u),\] (38)

where \(u=\langle x,y\rangle=x^{T}y\) and

\[\kappa_{0}(u)=\frac{1}{\pi}\left(\pi-\arccos u\right),\qquad\kappa_{1}(u)= \frac{1}{\pi}\sqrt{1-u^{2}}+\frac{u}{\pi}\left(\pi-\arccos u\right).\] (39)

The definition of \(\kappa_{1}^{(t)}\) is given by the composition \(\kappa_{1}\circ\kappa_{1}\cdots\circ\kappa_{1}\) (a total of \(t\) compositions). The explicit expression indicates that \(K_{0}^{\mathrm{NTK}}\) and \(K_{0}^{\mathrm{RFK}}\) are dot-product kernels on \(\mathbb{S}^{d}\).

\(K_{0}^{\mathrm{NTK}}\) and \(K_{0}^{\mathrm{RFK}}\) is the homogeneous NTK and RFK of a homogeneous fully-connected network \(f^{S}\) defined on \(\mathbb{S}^{d}\)[6, 14], whose structural difference from (9) is the removal of the bias term in the first layer. Specifically, the network is structured as follows:

**Homogeneous fully-connected network on sphere**  The network is constructed using the following recursive formula:

\[\alpha^{(1)}(x) =\sqrt{\frac{2}{m_{1}}}W^{(0)}x;\] (40) \[\alpha^{(l)}(x) =\sqrt{\frac{2}{m_{l}}}W^{(l-1)}(x)\sigma(\alpha^{(l-1)}(x)), \quad l=2,3,\cdots,L;\] \[f^{S}(x;\theta) =W^{(L)}\sigma(\alpha^{(L)}(x)),\]where the function \(\sigma\) is entrywise ReLU activation. The parameter matrix for the \(l\)-th layer is denoted as \(W^{(l)}\), whose dimensions are of \(m_{l+1}\times m_{l}\), where \(m_{l}\) is the number of units in layer \(l\) and \(m_{l+1}\) is that of layer \(l+1\) for \(l\in\{0,1,\cdots,L-1\}\). We also set \(m_{0}\) to be equal to \(d+1\) and \(m_{L+1}\) equal to \(1\). The network is also random initialized as (10).

We can easily build a connection between our NTK and RFK for network (9) and the homogeneous kernels defined in (38). For \(x\in\mathcal{X}\), let \(\widetilde{x}=(x,1)\) which means add \(1\) as the new last component of \(x\). Define \(\phi(x)\coloneqq\frac{(x,1)}{\|(x,1)\|}\) being an isomorphism from open set \(\mathcal{X}\) to a subdomain of positive hemisphere shell \(S=\phi(\mathcal{X})\subset\mathbb{S}_{+}^{d}\). Then we have

\[f(x)=\|\widetilde{x}\|f^{S}(\phi(x)),\] (41)

where \(f\) is network (9) and \(f^{S}\) is network (40). Actually, we can thus verify that

\[K^{\mathrm{NTK}}(x,y)=\|\widetilde{x}\|\|\widetilde{y}\|K^{\mathrm{NTK}}_{0}( \phi(x),\phi(y)),\qquad K^{\mathrm{RFK}}(x,y)=\|\widetilde{x}\|\|\widetilde{y }\|K^{\mathrm{RFK}}_{0}(\phi(x),\phi(y)).\] (42)

We denote by \(\mathcal{H}^{\mathrm{NT}}_{0}\) and \(\mathcal{H}^{\mathrm{RF}}_{0}\) the RKHS on \(\mathbb{S}^{d}\) with respect to \(K^{\mathrm{NTK}}_{0}\) and \(K^{\mathrm{RFK}}_{0}\). Their eigenvalue decay rates are well known:

**Lemma C.1** (Bietti and Bach [6], Haas et al. [24]).: _For \(K^{\mathrm{NTK}}_{0}\) and \(K^{\mathrm{RFK}}_{0}\) on \(\mathbb{S}^{d}\) with uniform measure \(\sigma\), the decay rate of spherical harmonics coefficients satisfy_

\[\mu_{n}(K^{\mathrm{NTK}}_{0})\asymp n^{-(d+1)}\quad\text{and}\quad\mu_{n}(K^{ \mathrm{RFK}}_{0})\asymp n^{-(d+3)},\] (43)

_while the eigenvalues satisfy_

\[\lambda_{i}(K^{\mathrm{NTK}}_{0},\mathbb{S}^{d},\sigma)\asymp i^{-\frac{d+1}{ d}}\quad\text{and}\quad\lambda_{i}(K^{\mathrm{RFK}}_{0},\mathbb{S}^{d},\sigma) \asymp i^{\frac{d+3}{d}}.\] (44)

Additionally, we list some further result on the eigenvalue decay rate of NTK and RFK provided by Li et al. [41], which will be used later:

**Lemma C.2**.: _Denote \(\Omega\) as a non-empty subdomain of \(\mathbb{S}^{d}\). For \(K^{\mathrm{NTK}}_{0}\) and \(K^{\mathrm{RFK}}_{0}\), we have eigenvalue decay rate:_

\[\lambda_{i}(K^{\mathrm{NTK}}_{0},\Omega,\sigma)\asymp i^{-\frac{d+1}{d}}\quad \text{and}\quad\lambda_{i}(K^{\mathrm{RFK}}_{0},\Omega,\sigma)\asymp i^{-\frac {d+3}{d}}\]

_where \(\sigma\) is the uniform measure on \(S\)._

**Lemma C.3**.: _For \(K^{\mathrm{NTK}}\) and \(K^{\mathrm{RFK}}\), we have eigenvalue decay rate:_

\[\lambda_{i}(K^{\mathrm{NTK}},\mathcal{X},\mu)\asymp i^{-\frac{d+1}{d}}\quad \text{and}\quad\lambda_{i}(K^{\mathrm{RFK}},\mathcal{X},\mu)\asymp i^{-\frac{d +3}{d}}\]

_where measure \(\mu\) on bounded domain \(\mathcal{X}\subset\mathbb{R}^{d}\) has density \(c\leq p(x)\leq C\) with respect to the Lebesgue measure._

### Basic concepts of Sobolev space

**Sobolev Space of integer power** Let \(\mathcal{X}\) be a open subset of \(\mathbb{R}^{d}\). Let \(m\in\mathbb{N}\), \(1\leq p\leq+\infty\). Sobolev space \(W^{m,p}(\mathcal{X})\) is defined as a set of function such that

\[\|D^{\alpha}f\|_{L^{p}}<+\infty,\] (45)

where \(\alpha\) is a vector with length \(n\) and \(D^{\alpha}f\) is the weak \(\alpha\)-th partial derivative of \(f\). In other words, the definition of \(W^{m,p}\) is:

\[W^{m,p}(\mathcal{X})=\left\{f\in L^{p}(\mathcal{X})|D^{\alpha}f\in L^{p}( \mathcal{X}),\forall|\alpha|\leq m\right\},\] (46)

where \(1\leq p\leq\infty\). Conventionally, when the index \(p\) is equal to \(2\), we denote \(W^{m,p}\) by \(H^{m}\), since it is a Hilbert space. Further, if the index \(m>\frac{d}{2}\), the Sobolev space \(H^{m}\) qualifies as a RKHS and thus embraces the properties of RKHS. In our work, we mainly utilize its property of interpolation as defined in (6). Consequently, we first introduce a generalized concept of real interpolation [1], as an expansion to the definition in (6).

Real interpolationFor two Banach spaces \(\mathcal{H}_{1}\) and \(\mathcal{H}_{2}\), we use interpolation space to represent a space that lies in between them in some specific way. We introduce the commonly-used K-method to define real interpolation. Suppose \(0<s<1\), \(q\geq 1\). The space generated by their real interpolation \(\mathcal{H}=(\mathcal{H}_{1},\mathcal{H}_{2})_{s,q}\) is defined by following:

\[K(t;x)=\inf_{x=x_{1}+x_{2};x_{1}\in\mathcal{H}_{1},x_{2}\in\mathcal{H}_{2}} \left\|x_{1}\right\|_{\mathcal{H}_{1}}+t\|x_{2}\|_{\mathcal{H}_{2}},\] (47)

and

\[\left\|x\right\|_{\mathcal{H}}=\left(\int_{0}^{\infty}(t^{-s}K(t;x))^{q}\frac {\mathrm{d}t}{t}\right)^{\frac{1}{q}}.\] (48)

Based on the definition of real interpolation, we introduce some basic concepts about fractional power Sobolev space.

Sobolev space of fractional powerSuppose \(\mathcal{X}\in\mathbb{R}^{d}\) is a bounded domain with smooth boundary and denote Lebesgue measure by \(\mu\). We can define fractional power Sobolev space through real interpolation (we refer to [45] Chapter 4.2.2 for more details):

\[H^{s}(\mathcal{X})\coloneqq\left(L^{2}(\mathcal{X},\mu),H^{m}(\mathcal{X})_{ \frac{s}{m},2}\right.\] (49)

The fractional power Sobolev space \(H^{r}(\mathcal{X})\) with \(r\geq\frac{d}{2}\) is also a RKHS [1]. Specifically, Steinwart and Scovel [48] reveals that for \(0<s<1\),

\[\left[\mathcal{H}\right]^{s}\cong\left(L^{2}(\mathcal{X},\mu),\mathcal{H} \right)_{s,2}\] (50)

for RKHS \(\mathcal{H}\) and the interpolation defined in (6). Therefore, the results above directly implies that

\[[H^{r}(\mathcal{X})]^{s}=H^{rs}(\mathcal{X})\] (51)

holds for any \(r\geq\frac{d}{2}\) and \(s>0\).

Up to now, we have introduced the basic properties of Sobolev spaces on \(\mathcal{X}\), an open subset of \(\mathbb{R}^{d}\). For Sobolev spaces defined on more intricate manifolds, such as hyperspheres, owing to the intricate property of Sobolev spaces, numerous equivalent definitions emerges [1, 18].

We now delineate a kind of definition that will facilitate our subsequent proofs, since we will consider RKHSs on \(\mathbb{S}^{d}\), like \(\mathcal{H}_{0}^{\mathrm{NT}}\) and \(\mathcal{H}_{0}^{\mathrm{RF}}\), which are the RKHSs associated with \(K_{0}^{\mathrm{NTK}}\) and \(K_{0}^{\mathrm{RFK}}\), respectively. Such definition can form a linkage with the Sobolev spaces defined on \(\mathbb{S}^{d}\) and on domain \(\mathcal{X}\subset\mathbb{R}^{d}\), which is also utilized in Haas et al. [24]. Our exposition begins with the characterization of a manifold.

TrivilizationDefine a trivialization of a Riemannian manifold \((M,g)\) with bounded geometry of dimension \(d\), which consists three part. The first part is some locally finite open covering \(\{U_{\alpha}\}_{\alpha\in I}\). The second part is the charts \(\{\kappa_{\alpha}\}_{\alpha\in I}\) which consists of smooth diffeomorphism \(\kappa_{\alpha}:V_{\alpha}\subset\mathbb{R}^{d}\to U_{\alpha}\). The third part is a portion of unity \(h_{\alpha}\) such that \(\mathrm{supp}(h_{\alpha})\subset U_{\alpha}\), \(\sum_{\alpha\in I}h_{\alpha}=1\) and \(0\leq h_{\alpha}\leq 1\).

In our case, we write a trivialization of \(\mathbb{S}^{d}\), which, is a manifold of dimension \(d\). We write \(U_{1}=\{x_{d+1}<\epsilon|x\in\mathbb{S}^{d}\}\) and \(U_{2}=\{x_{d+1}>\frac{\epsilon}{2}|x\in\mathbb{S}^{d}\}\) for a small fixed \(\epsilon>0\). Let \(\phi_{1}:U_{1}\to\mathbb{R}^{d}\) and \(\phi_{2}:U_{2}\to\mathbb{R}^{d}\) be stereographic projections with respect to \(x_{1}=(0,0,\cdots,1)\) and \(x_{2}=(0,0,\cdots,-1)\), respectively. Namely, they are

\[\phi_{1}:(x_{1},x_{2},\cdots,x_{d+1})\mapsto\frac{1}{1+x_{d+1}}(x_{1},x_{2}, \cdots,x_{d})\] (52)

and

\[\phi_{2}:(x_{1},x_{2},\cdots,x_{d+1})\mapsto\frac{1}{1-x_{d+1}}(x_{1},x_{2}, \cdots,x_{d}).\] (53)

Finally, we can find \(C^{\infty}\) smooth functions \(h_{1}\) and \(h_{2}\) such that \(h_{1}\big{|}_{\mathbb{S}^{d}_{\cdot}}=1\). For the simple trivialization above, we can directly verify that it meets the admissible trivialization condition (details see Grosse and Schneider [23]). Thus we can apply Theorem 14 of [23] to define the norm of Sobolev space on \(\mathbb{S}^{d}\):

\[\left\|f\right\|_{H^{s}(\mathbb{S}^{d})}=\left(\left\|(h_{1}f)\circ\phi_{1}^{ -1}\right\|_{H^{s}(\mathbb{R}^{d})}^{2}+\left\|(h_{2}f)\circ\phi_{2}^{-1} \right\|_{H^{s}(\mathbb{R}^{d})}^{2}\right)^{\frac{1}{2}},\] (54)

for distribution \(f\in\mathcal{D}^{\prime}(\mathbb{S}^{d})\)[49]. It gives a kind of equivalent definition of Sobolev space on \(\mathbb{S}^{d}\).

### Relationship between dot-product kernel and Sobolev space

Previous work observed that, for dot-product kernels defined on sphere with polynomial eigenvalue decay rate, their RKHSs are equivalent to Sobolev spaces:

**Lemma C.4** (Hubbert et al. [30] Section 3).: _For a dot-product kernel \(k\) defined on \(\mathbb{S}^{d}\) and its RKHS \(\mathcal{H}_{k}\), if the coefficients of spherical harmonic polynomials satisfies \(\mu_{n}\asymp n^{t}\) for some \(t\geq d\), then there exists an equivalence between RKHS and Sobolev space:_

\[\mathcal{H}_{k}\cong H^{\frac{1}{2}}(\mathbb{S}^{d}).\]

Recall that \(K_{0}^{\mathrm{NTK}}\) and \(K_{0}^{\mathrm{RFK}}\) are both dot-product kernels with polynomial eigenvalue decay rate by Lemma C.1. Therefore, Lemma C.4 provides the equivalence between \(\mathcal{H}_{0}^{\mathrm{NT}}\), \(\mathcal{H}_{0}^{\mathrm{RF}}\) and the corresponding Sobolev spaces on \(\mathbb{S}^{d}\). We have the following proposition:

**Proposition C.5**.: We have the following equivalence:

\[\mathcal{H}_{0}^{\mathrm{NT}}\cong H^{\frac{d+1}{2}}(\mathbb{S}^{d})\quad \text{and}\quad\mathcal{H}_{0}^{\mathrm{RF}}\cong H^{\frac{d+3}{2}}(\mathbb{S }^{d}).\] (55)

### Interpolation of \(\mathcal{H}_{0}^{\mathrm{NT}}\) and \(\mathcal{H}_{0}^{\mathrm{RF}}\)

In this subsection, we aims to provide the interpolation relationship between RKHSs associated with \(K_{0}^{\mathrm{NTK}}\) and \(K_{0}^{\mathrm{RFK}}\), on a subdomain of \(\mathbb{S}^{d}\). We remind that if we consider the case on \(\mathbb{S}^{d}\), i.e. \(\mathcal{H}_{0}^{\mathrm{NT}}\) and \(\mathcal{H}_{0}^{\mathrm{RF}}\), the conclusion is direct since they are both dot-product kernels and share the same orthogonal basis \(\{Y_{n,l}\}\) as introduced in (37).

Suppose \(s\geq\frac{d}{2}\) and \(\Omega\) be a subdomain of \(\mathbb{S}^{d}\) with \(C^{\infty}\) smooth boundary. With a little abuse of notation, we define \(H^{s}(\Omega)\) as the RKHS \(H^{s}(\mathbb{S}^{d})\) restricted to \(\Omega\) in the way of Lemma H.3. For an injection \(\varphi:\Omega\to\mathbb{R}^{d}\), we define \(H^{s}(\varphi(\Omega))\circ\varphi\coloneqq\{f\circ\varphi|f\in H^{s}(\varphi( \Omega))\}\) with norm \(\|f\circ\varphi\|=\|f\|_{H^{s}(\varphi(\Omega))}\). Recall that \(\phi_{1}\) is the stereographic projection defined in (52), now let us show the equivalence of \(H^{s}(\mathbb{S}^{d}_{+})\) and \(H^{s}(\phi_{1}(\mathbb{S}^{d}_{+}))\circ\phi_{1}\).

**Lemma C.6** (Equivalence as sets).: _Suppose \(s\geq\frac{d}{2}\). At the aspect of sets, we have \(H^{s}(\mathbb{S}^{d}_{+})=H^{s}(\phi_{1}(\mathbb{S}^{d}_{+}))\circ\phi_{1}\)._

Proof.: For a \(f\in H^{s}(\mathbb{S}^{d}_{+})\), we have an extension \(\|f^{\prime}\|_{H^{s}(\mathbb{S}^{d})}<\infty\) such \(f^{\prime}|_{\mathbb{S}^{d}_{+}}=f\). Thus

\[\left\|(h_{1}f^{\prime})\circ\phi_{1}^{-1}\right\|_{H^{s}(\mathbb{R}^{d})} \leq\left\|f\circ\phi_{1}^{-1}\right\|_{H^{s}(\mathbb{R}^{d})}<\infty\] (56)

which implies \((h_{1}f^{\prime})\circ\phi_{1}^{-1}\in H^{s}(\mathbb{R}^{d})\). Then we have \([(h_{1}f^{\prime})\circ\phi_{1}^{-1}]|_{\phi_{1}(\mathbb{S}^{d}_{+})}\in H^{s} (\phi_{1}(\mathbb{S}^{d}_{+}))\). Since \(f=f^{\prime}|_{\mathbb{S}^{d}_{+}}\) and \(h_{1}|_{\mathbb{S}^{d}_{+}}=1\), we have \(f\circ\phi_{1}^{-1}\in H^{s}(\phi_{1}(\mathbb{S}^{d}_{+}))\).

In the converse direction, we assume \(f\in H^{s}(\phi_{1}(\mathbb{S}^{d}_{+}))\). Then we know there exists \(f^{\prime}\in H^{s}(\mathbb{R}^{d})\) such that \(f^{\prime}|_{\phi_{1}(\mathbb{S}^{d}_{+})}=f\). Now we want to show \(f\circ\phi_{1}\in H^{s}(\mathbb{S}^{d}_{+})\). Define a \(\psi\in C^{\infty}(\mathbb{R}^{d})\) such that \(\psi(\phi_{1}(\mathbb{S}^{d}_{+}))\equiv 1\) and \(\psi((\phi_{1}(U_{1}/U_{2}))^{c})\equiv 0\). According to (54), we have

\[\left\|f\circ\phi_{1}\right\|_{H^{s}(\mathbb{S}^{d}_{+})}\leq\left\|(\psi\cdot f ^{\prime})\circ\phi_{1}\right\|_{H^{s}(\mathbb{S}^{d})}=\left\|(h_{1}\circ\phi _{1}^{-1})\cdot\psi\cdot f^{\prime}\right\|_{H^{s}(\mathbb{R}^{d})}<\infty\] (57)

Thus we finish the proof. 

**Lemma C.7** (Equivalence as space).: _Suppose \(s\geq\frac{d}{2}\). At the aspect of spaces, we have \(H^{s}(\mathbb{S}^{d}_{+})\cong H^{s}(\phi_{1}(\mathbb{S}^{d}_{+}))\circ\phi_{1}\)._

Proof.: By Lemma C.6, we know \(H^{s}(\mathbb{S}^{d}_{+})\cong H^{s}(\phi_{1}(\mathbb{S}^{d}_{+}))\circ\phi_{1}\) as sets. Since \(H^{s}(\mathbb{S}^{d}_{+})\) and \(H^{s}(\phi_{1}(\mathbb{S}^{d}_{+}))\circ\phi_{1}\) are both RKHSs, we can finish the proof by closed graph theorem.

For notational simplicity, denote by \(\mathcal{H}_{1}=H^{s}(\mathbb{S}^{d}_{+})\) and \(\mathcal{H}_{2}=H^{s}(\phi_{1}(\mathbb{S}^{d}_{+}))\circ\phi_{1}\). Define the canonical map \(I:\mathcal{H}_{1}\to\mathcal{H}_{2}\) as \(I:h\mapsto h\). Let \(\{h_{n}\}_{n\in\mathbb{N}}\) be a sequence such that there exists \(h\in\mathcal{H}_{1}\) and \(g\in\mathcal{H}_{2}\) where \(h_{n}\to h\) in \(\mathcal{H}_{1}\) and \(h_{n}=Ih_{n}\to g\) in \(\mathcal{H}_{2}\). It implies that \(h=g\). Therefore, closed graph theorem shows that the linear operator \(I\) is bounded, which means that \(\left\|h\right\|_{\mathcal{H}_{1}}\leq C\left\|h\right\|_{\mathcal{H}_{2}}\) holds for some positive constant \(C\) and any \(h\in\mathcal{H}_{1}\). We can also prove \(\left\|h\right\|_{\mathcal{H}_{2}}\leq C^{\prime}\left\|h\right\|_{\mathcal{H}_{ 1}}\) for any \(h\) in the same way. Consequently, the lemma is proved.

Now we come back to our network case. Let \(S\coloneqq\phi(\mathcal{X})\subset\mathbb{S}_{+}^{d}\) where \(\mathcal{X}\) is the set from which data \(x\) is sampled, and \(\phi\) is used in (42). Since the boundary of \(\mathcal{X}\) is \(C^{\infty}\) smooth, we know that \(S\) is \(C^{\infty}\) smooth. If we combine Lemma C.4, Lemma C.7 and Proposition H.4, then we can directly show the following lemma:

**Lemma C.8**.: _Define \(\mathcal{X}_{1}=\phi_{1}(S)\). For \(K_{0}^{\rm NTK}\) and \(K_{0}^{\rm RFK}\) defined on \(S\), we have the following equivalence:_

\[\mathcal{H}_{0}^{\rm RF}(S) \cong H^{\frac{d+3}{2}}(\mathcal{X}_{1})\circ\phi_{1},\quad\text{and}\] (58) \[\mathcal{H}_{0}^{\rm NT}(S) \cong H^{\frac{d+1}{2}}(\mathcal{X}_{1})\circ\phi_{1}.\]

Now we can obtain the interpolation relationship between \(\mathcal{H}_{0}^{\rm RF}(S)\) and \(\mathcal{H}_{0}^{\rm NT}(S)\).

**Lemma C.9**.: _Suppose \(s\geq 0\). We have_

\[[\mathcal{H}_{0}^{\rm NT}(S)]^{s}\cong[\mathcal{H}_{0}^{\rm RF}(S)]^{\frac{s(d +1)}{2d+3}}\]

Proof.: Define \(\mathcal{X}_{1}=\phi_{1}(S)\). Let \(\sigma\) be the uniform measure on \(\mathbb{S}^{d}\). Recalling (51), we have the interpolation on \(\mathcal{X}_{1}\) with lebesgue measure denoted by \(\mu_{1}\):

\[[H^{\frac{d+3}{2}}(\mathcal{X}_{1})]^{\frac{s(d+1)}{2d+3}}\cong H^{\frac{s(d+ 1)}{2}}(\mathcal{X}_{1})\cong[H^{\frac{d+1}{2}}(\mathcal{X}_{1})]^{s}\] (59)

that is

\[\left(L^{2}(\mathcal{X}_{1},\mu_{1}),H^{\frac{d+3}{2}}(\mathcal{X}_{1})\right) _{\frac{s(d+1)}{d+3},2}\cong\left(L^{2}(\mathcal{X}_{1},\mu_{1}),H^{\frac{d+1 }{2}}(\mathcal{X}_{1})\right)_{s,2}\] (60)

Since \(f\mapsto f\circ\phi_{1}\) is an isometric isomorphism, we have

\[\left(L^{2}(\mathcal{X}_{1},\mu_{1})\circ\phi_{1},H^{\frac{d+3}{2}}(\mathcal{ X}_{1})\circ\phi_{1}\right)_{\frac{s(d+1)}{d+3},2}\cong\left(L^{2}(\mathcal{X}_{1}, \mu_{1})\circ\phi_{1},H^{\frac{d+1}{2}}(\mathcal{X}_{1})\circ\phi_{1}\right)_ {s,2}\] (61)

Recall that \(\mathcal{X}\) is bounded and thus \(\mathcal{X}_{1}=\phi_{1}(\phi(\mathcal{X}))\) is bounded. Therefore, the Jacobian \(J\phi_{1}^{-1}\) satisfies \(c\leq|J\phi_{1}^{-1}|\leq C\) for some constant \(c\) and \(C\). It is easy to verify that \(L^{2}(\mathcal{X}_{1},\mu_{1})\circ\phi_{1}=L^{2}(S,\mu_{1}\circ\phi_{1}) \cong L^{2}(S,\sigma)\). Finally, with Lemma C.8, Lemma H.5 and Lemma H.6, we have

\[[\mathcal{H}_{0}^{\rm RF}(S)]^{\frac{s(d+1)}{d+3}}\cong[\mathcal{H}_{0}^{\rm NT }(S)]^{s}\] (62)

with respect to the uniform measure \(\sigma\) on \(S\). 

### Smoothness of Gaussian process

Lemma C.9 provides the interpolation relationship between \(\mathcal{H}_{0}^{\rm NT}(S)\) and \(\mathcal{H}_{0}^{\rm RF}(S)\). By the kernel transformation relationship of NTK and RFK from \(\mathbb{R}^{d}\) and to \(\mathbb{S}^{d}\) as described in (42), we can also derive the interpolation relationship of \(\mathcal{H}^{\rm NT}\) and \(\mathcal{H}^{\rm RF}\). It will help for us to derive the smoothness of \(f^{\rm GP}\).

**Lemma C.10** (Interpolation of RKHSs).: _Suppose \(s>0\). We have_

\[[\mathcal{H}^{\rm NT}(\mathcal{X})]^{s}\cong[\mathcal{H}^{\rm RF}(\mathcal{X} )]^{\frac{s(d+1)}{d+3}}\]

_with respect to measure \(\mu\) on \(\mathcal{X}\) which has Lebesgue density \(c\leq p(x)\leq C\)._

Proof.: Define a function \(\rho(x)=\|\widetilde{x}\|\) on \(\mathcal{X}\). Define measure \(\nu\) on \(\mathcal{X}\) such that the Radon-Nikodym derivative satisfies \(\frac{{\rm d}\nu}{{\rm d}\mu}=\rho^{2}\). We consider measure \(\nu\circ\phi\) on \(S\) as well as measure \(\mu\) on \(\mathcal{X}\), and then define a map \(I:[\mathcal{H}_{0}^{\rm NT}(S)]^{s}\to[\mathcal{H}^{\rm NT}(\mathcal{X})]^{s}\):

\[I:f\mapsto\rho\cdot(f\circ\phi).\] (63)

Now we prove \(I\) is an isometric isomorphism. We first show that for any eigen pair \((f,\lambda)\) of \((K_{0}^{\rm NTK},S,\nu\circ\phi)\), \((If,\lambda)\) is also an eigen pair of \((K^{\rm NTK},\mathcal{X},\mu)\). Actually, for eigen pair \((f,\lambda)\) we have

\[\int_{S}K_{0}^{\rm NTK}(x,y)f(y)\,{\rm d}(\nu\circ\phi)\,(y)=\lambda f(z).\] (64)We perform a transformation of the integral domain,

\[\begin{split}&\int_{\mathcal{X}}K_{0}^{\mathrm{NTK}}(\phi(x),\phi(y))f (\phi(y))\mathrm{d}\nu(y)=\lambda f(\phi(x))\\ &=\int_{\mathcal{X}}K_{0}^{\mathrm{NTK}}(\phi(x),\phi(y))f(\phi(y) )\rho^{2}(y)\mathrm{d}\mu(y)\end{split}\] (65)

Recalling the transformation between \(K_{0}^{\mathrm{NTK}}\) and \(K^{\mathrm{NTK}}\) in (42), we have

\[\begin{split}&\int_{\mathcal{X}}\rho(x)K_{0}^{\mathrm{NTK}}(\phi(x), \phi(y))f(\phi(y))\rho^{2}(y)\mathrm{d}\mu(y)=\lambda\rho(x)f(\phi(x))\\ &=\int_{\mathcal{X}}K(x,y)f(\phi(y))\rho(y)\mathrm{d}\mu(y)\end{split}\] (66)

These transformations are both reversible. Therefore, through the structure of real interpolation space as described in (6), we can see \(I\) is an isometric isomorphism. In the same way, there exist isometric isomorphism \(I^{\prime}:[\mathcal{H}_{0}^{\mathrm{RF}}(S)]^{\frac{s(d+1)}{d+3}}\to[ \mathcal{H}^{\mathrm{RF}}(\mathcal{X})]^{\frac{s(d+1)}{d+3}}\):

\[\begin{split} I^{\prime}:& f\mapsto\rho\cdot(f\circ \phi).\end{split}\] (67)

Combined the result in Lemma C.9, the Lemma is proved. 

Now we are ready to give the smoothness of Gaussian process \(f^{\mathrm{GP}}\). We remind the reader that \(\mathcal{H}^{\mathrm{NT}}\)and \(\mathcal{H}^{\mathrm{RF}}\) are abbreviations used for denoting \(\mathcal{H}^{\mathrm{NT}}(\mathcal{X})\) and \(\mathcal{H}^{\mathrm{RF}}(\mathcal{X})\), respectively.

Proof of Theorem 4.2.: Let \(t=\frac{s(d+1)}{d+3}\) to simplify the notation. By Lemma C.10, we have

\[[\mathcal{H}^{\mathrm{NT}}]^{s}\cong[\mathcal{H}^{\mathrm{RF}}]^{t}.\] (68)

Recalling the structure of interpolation space, we suppose \([\mathcal{H}^{\mathrm{RF}}]^{t}\) can be written as

\[[\mathcal{H}^{\mathrm{RF}}]^{t}=\left\{\sum_{i\in\mathbb{N}}c_{i}\lambda_{i}^ {\frac{t}{2}}e_{i}\middle|\sum_{i\in\mathbb{N}}c_{i}^{2}<\infty\right\}.\] (69)

Recall that \(f^{\mathrm{GP}}\) represents a random function defined on \((\Omega,\mathcal{F},\mathbf{P})\), where each \(\omega\in\Omega\) corresponds to a path function \(f^{\mathrm{GP}}_{\omega}:\mathcal{X}\to\mathbb{R}\). We can express this in the orthonormal basis as \(f^{\mathrm{GP}}_{\omega}=\sum_{i\in\mathbb{N}}a_{i}(\omega)\lambda_{i}^{\frac {t}{2}}e_{i}\), where

\[a_{i}(\omega)=\langle f^{\mathrm{GP}}_{\omega},\lambda_{i}^{\frac{t}{2}}e_{i} \rangle_{[\mathcal{H}^{\mathrm{RF}}]^{t}}=\lambda_{i}^{-\frac{t}{2}}\int f^{ \mathrm{GP}}_{\omega}e_{i}(x)\mathrm{d}\mu(x).\]

Recall that as defined in Lemma 3.2, \(f^{\mathrm{GP}}\) has the distribution \(\mathcal{GP}(0,K^{\mathrm{RFK}})\). From this, we can acquire the joined distribution for \(a_{i}\). Firstly, let us compute the covariance:

\[\begin{split}\mathrm{Cov}(a_{i},a_{j})&=\mathbf{E}[ a_{i},a_{j}]\\ &=\mathbf{E}\left[\lambda_{i}^{-t/2}\lambda_{j}^{-t/2}\int_{ \mathcal{X}}\int_{\mathcal{X}}f^{\mathrm{GP}}(x)f^{\mathrm{GP}}(y)e_{i}(x)e_ {i}(y)\,\mathrm{d}\mu(x)\mathrm{d}\mu(y)\right]\\ &=\lambda_{i}^{-t/2}\lambda_{j}^{-t/2}\int_{\mathcal{X}}\int_{ \mathcal{X}}\mathbf{E}\left[f^{\mathrm{GP}}(x)f^{\mathrm{GP}}(y)\right]e_{i}( x)e_{i}(y)\,\mathrm{d}\mu(x)\mathrm{d}\mu(y)\\ &=\lambda_{i}^{-t/2}\lambda_{j}^{-t/2}\int_{\mathcal{X}}\int_{ \mathcal{X}}K^{\mathrm{RFK}}(x,y)e_{i}(x)e_{i}(y)\,\mathrm{d}\mu(x)\mathrm{d} \mu(y)\\ &=\lambda_{i}^{-(1-t)/2}\lambda_{j}^{-(1-t)/2}\mathbf{1}_{\{i=j \}}.\end{split}\] (70)

The exchange of integration is accomplished by Fubini's theorem since \(K^{\mathrm{RFK}}\) is a bounded kernel function, and both \(e_{i}\) and \(e_{j}\) are \(L_{2}\) integrable. Moreover, as \(f^{\mathrm{GP}}\) is a Gaussian process, we finally get \(a_{i}\sim N(0,\lambda_{i}^{1-t})\) for \(i\in\mathbb{N}\), and \(a_{i}\), \(a_{j}\) are independent for any \(i\neq j\). Consequently, we can directly derive that

\[\left\lVert f^{\mathrm{GP}}\right\rVert_{[\mathcal{H}_{\mathrm{NT}}]^{s}}^{2}= \sum_{i\in\mathbb{N}}\lambda_{i}^{1-t}Z_{i}^{2},\] (71)

where \(\{Z_{i}\}\) indicates a collection of independent and identically distributed standard Gaussian random variables. Finally, as Lemma C.3 establishes the eigenvalue decay rate as

\[\lambda_{i}\asymp i^{\frac{d+3}{d}},\] (72)

it is direct to prove the theorem.

**Part 1.**  When \(s<\frac{3}{d+1}\), we have \(\frac{d+3}{d}\cdot(1-t)>1\) and thus

\[\mathbf{E}\left\|f^{\mathrm{GP}}\right\|_{[\mathcal{H}_{\mathrm{NT}}]^{s}}^{2} \asymp\sum_{i\in\mathbb{N}}i^{-\frac{d+3}{d}\cdot(1-t)}<+\infty.\] (73)

Consequently we have \(\mathbf{P}\left(\left\|f^{\mathrm{GP}}\right\|_{[\mathcal{H}_{\mathrm{NT}}]^{s }}^{2}<\infty\right)=1\).

**Part 2.**  When \(s\geq\frac{3}{d+1}\), we ascertain that \(\frac{d+3}{d}\cdot(1-t)\leq 1\) and consequently

\[\mathbf{E}\left\|f^{\mathrm{GP}}\right\|_{[\mathcal{H}_{\mathrm{NT}}]^{s}}^{2} \asymp\sum_{i\in\mathbb{N}}i^{-\frac{d+3}{d}\cdot(1-t)}=+\infty.\] (74)

Denote by \(X_{n}=\sum_{i=1}^{n}\lambda_{i}^{1-t}Z_{i}^{2}\). We then obtain

\[\mathbf{E}X_{n}=\sum_{i=1}^{n}\lambda_{i}^{1-t},\quad\mathrm{Var}X_{n}=\sum_{ i=1}^{n}2\lambda_{i}^{1-t}.\] (75)

We can thus derive that

\[\mathbf{P}(X_{n}\leq\frac{\mathbf{E}X_{n}}{2})\leq\mathbf{P}(|X_{n}-\mathbf{ E}X_{n}|\geq\frac{\mathbf{E}X_{n}}{2})\leq\frac{4\mathrm{Var}X_{n}}{\left[ \mathbf{E}X_{n}\right]^{2}}=\frac{8}{\sum_{i=1}^{n}\lambda_{i}^{1-t}}.\] (76)

Given that \(\left\|f^{\mathrm{GP}}\right\|_{[\mathcal{H}_{\mathrm{NT}}]^{s}}^{2}\geq X_{n}\) for any \(n\in\mathbb{N}_{+}\), we have

\[\mathbf{P}(\left\|f^{\mathrm{GP}}\right\|_{[\mathcal{H}_{\mathrm{NT}}]^{s}}^{2} =\infty)=\lim_{M\to\infty}\mathbf{P}(\left\|f^{\mathrm{GP}}\right\|_{[ \mathcal{H}_{\mathrm{NT}}]^{s}}^{2}\geq M)\geq 1-\lim_{n\to\infty} \mathbf{P}(X_{n}\leq\frac{\mathbf{E}X_{n}}{2})=1.\] (77)

This completes the proof. 

## Appendix D Proof of Theorem 4.3

With the findings from Theorem 4.2, Proposition 4.1, and Proposition B.4, the influence of non-zero initialization could be interpreted in terms of a misspecified spectral algorithms problem. To apply Proposition 2.2, it only remains to determine the embedding index of \(\mathcal{H}^{\mathrm{NT}}\). Now, let's proceed to do so.

### Embedding index of \(\mathcal{H}^{\mathrm{NT}}\)

Recall that the Proposition 2.2 requires the embedding index of \(\mathcal{H}^{\mathrm{NT}}\) on \(\mathcal{X}\) under the probability measure \(\mu\). Fortunately, the embedding index of the Sobolev space has been previously established by Zhang et al. [55], which is helpful to simplify our proof.

**Lemma D.1** (Zhang et al. [55] Section 4.2, Embedding index of Sobolev space).: _Suppose \(r>\frac{d}{2}\). For a bounded open set \(\mathcal{X}\subset\mathbb{R}^{d}\) and Lebesgue measure \(\mu\), the embedding index of \(H^{r}(\mathcal{X})\) equals \(\frac{d}{2r}\)._

Since we have established the relationship between \(\mathcal{H}^{\mathrm{NT}}\) and the Sobolev space, we can easily get the embedding index through a similar way used in the proof of Lemma C.10.

**Lemma D.2** (Embedding index of NTK).: _Suppose that the density function \(p(x)\) of probability measure \(\mu\) satisfies the condition \(c\leq p(x)\leq C\), where \(c\) and \(C\) are positive constants. The embedding index of \(\mathcal{H}^{\mathrm{NT}}(\mathcal{X})\) with respect to \(\mu\) is concluded to be \(\frac{d}{d+1}\)._

We omit this proof as it can be carried out in the same manner as Lemma C.10. Here, we provide only the structure. First, the embedding index of \(H^{\frac{d+1}{2}}(S)\) is \(\frac{d}{d+1}(\)Lemma D.1 and Lemma C.7). Second, the embedding index of \(\mathcal{H}^{\mathrm{NT}}_{0}(S)\) is \(\frac{d}{d+1}\) (Lemma C.4). Third, the embedding index of \(\mathcal{H}^{\mathrm{NT}}(\mathcal{X})\) is \(\frac{d}{d+1}\) since \(I:f\mapsto\rho\cdot(f\circ\phi)\) is isometric isomorphism both from \(\mathcal{H}^{\mathrm{NT}}_{0}(S)\) to \(\mathcal{H}^{\mathrm{NT}}(\mathcal{X})\) and from \(L^{\infty}(S,\nu^{\prime}\circ\phi)\) to \(L^{\infty}(\mathcal{X},\mu)\), where measure \(\nu^{\prime}\) is defined on \(\frac{\mathrm{d}\nu^{\prime}}{\mathrm{d}\mu}=\rho\) (an argument similar to that in the proof of Lemma C.10).

### Proof of Theorem 4.3

Proof of Theorem 4.3.: Recall that Proposition 4.1 elucidates the impact of non-zero initialization. Namely, the generalization error of the kernel gradient flow with an initialization of \(f_{0}\) and a regression function \(f^{*}\), is consequently equivalent to that of kernel gradient flow with initialization at \(0\) and a regression function of \(f^{*}-f_{0}\). On the other hand, Proposition B.4 demonstrated the uniform convergence from the network function to the kernel gradient flow predictor as the network width \(m\) tends to infinity. Lemma D.2 verify the embbeding index condition in Proposition 2.2. Thus, we only need to verify the source condition that \(f^{\mathrm{GP}}-f^{*}\) fulfills and to incorporate it with Proposition 2.2 in order to derive the generalization error of the kernel gradient flow.

Now we start the proof. Since the proofs for the cases \(s\geq\frac{3}{d+1}\) and \(0<s<\frac{3}{d+1}\) are exactly the same, we will only provide the proof for the former case here. Through Theorem 4.2, we know for any \(0<r<\frac{3}{d+1}\), it follows that \(\mathbf{E}\big{\|}f^{\mathrm{GP}}\big{\|}_{[\mathcal{H}^{\mathrm{NT}}]^{r}}^{ 2}=\sum\lambda_{i}^{1-r}<\infty\). Let \(C_{t}=\mathbf{E}\big{\|}f^{\mathrm{GP}}\big{\|}_{[\mathcal{H}^{\mathrm{NT}}]^ {r}}\). By the Markov inequality, for any \(\delta^{\prime}\in(0,1)\), we have with probability exceeding \(1-\delta^{\prime}\), that

\[\big{\|}f^{\mathrm{GP}}-f^{*}\big{\|}_{[\mathcal{H}^{\mathrm{NT}}]^{r}}\leq \frac{R+C_{r}}{\delta^{\prime}}.\] (78)

Recall that the eigenvalue decay rate for \(K^{\mathrm{NTK}}\) is \(\frac{d+1}{d}\) as mentioned in Lemma C.2. Therefore, we have for any \(\delta\in(0,1)\) and any \(\varepsilon\in(0,\frac{3}{d+3})\), there exists \(r<\frac{3}{d+1}\) such that \(\frac{r\beta}{r\beta+1}=\frac{3}{d+3}-\varepsilon\) (i.e., \(r=\frac{d^{2}-6d-3d(d+3)\varepsilon}{3(d+1)+\varepsilon(d+1)(d+3)}\)). Denote by \(\widetilde{f}^{*}=f^{*}-f^{\mathrm{GP}}\) and \(\widetilde{f}^{\mathrm{NTK}}_{t}\) be the kernel gradient flow predictor starts from initial value \(0\). Through Proposition 2.2, We thus have

\[\Big{\|}\widetilde{f}^{\mathrm{NTK}}_{t}-\widetilde{f}^{*}\Big{\|}_{L^{2}}^{ 2}\leq\left(\frac{1}{\delta^{\prime}}\ln\frac{6}{\delta}\right)^{2}(R+C_{r})^ {2}C^{\prime}n^{-\frac{3}{d+3}+\varepsilon},\] (79)

holds with probability at least \(1-2\delta^{\prime}\) when \(t\asymp n^{\frac{\beta}{r\beta+1}}\). Through Proposition 4.1, also we have

\[\big{\|}f^{\mathrm{NTK}}_{t}-f^{*}\big{\|}_{L^{2}}^{2}\leq\left(\frac{1}{ \delta^{\prime}}\ln\frac{6}{\delta}\right)^{2}(R+C_{r})^{2}C^{\prime}n^{- \frac{3}{d+3}+\varepsilon},\] (80)

holds with probability at least \(1-2\delta^{\prime}\). Through uniform convergence in Proposition B.4, we have

\[\sup_{t\geq 0}\Big{\|}\big{\|}f^{\mathrm{NN}}_{t}-f^{*}\big{\|}_{L^{2}}-\big{\|}f^ {\mathrm{NTK}}_{t}-f^{*}\big{\|}_{L^{2}}^{2}\Big{|}\leq\left(\frac{1}{\delta^{ \prime}}\ln\frac{6}{\delta}\right)^{2}(R+C_{r})^{2}C^{\prime}n^{-\frac{3}{d+3 }+\varepsilon},\] (81)

with probability at least \(1-\delta^{\prime}\) when \(m\) is large enough. Therefore, with appropriate choice of \(\delta^{\prime}\) and \(C^{\prime}\), we can finish the proof.

## Appendix E Proof of Theorem 4.4

In this section, we establish the generalization error rate lower bound in our problem. We incorporate a result delineated in [40], which systematically studies the learning rate of kernel regression. Prior to this, we take some preparatory work.

We assume \(k\) is a dot-product kernel on \(\mathbb{S}^{d}\) with eigenvalue decay rate \(\beta\), with respect to the uniform measure. We notate the corresponding RKHS as \(\mathcal{H}_{k}\). Then, we can verify that \(\mathcal{H}_{k}\) satisfies to the definition of _regular RKHS_, as detailed in [40]. Subsequently, the main theorem in [40] can be applied under our proposed settings, since \(K_{0}^{\mathrm{NTK}}\) is a dot-product kernel defined on \(\mathbb{S}^{d}\). It engenders the following lemma.

**Lemma E.1** (Generalization error lower bound).: _Assume \(k\) is a dot-product kernel defined on \(\mathbb{S}^{d}\), we have the interpolation space of its RKHS as \([\mathcal{H}_{k}]^{s}=\left\{\sum_{i\in\mathbb{N}}a_{i}\lambda_{i}^{\frac{ \beta}{2}}e_{i}|\sum_{i\in\mathbb{N}}a_{i}^{2}<\infty\right\}\) where \(\{e_{i}\}_{i\in\mathbb{N}}\) is the orthonormal basis of \(L_{2}(\mathbb{S}^{d},\sigma)\) and \(\sigma\) denotes the uniform measure. Decompose the regression function \(f^{*}\) over the series of basis:_

\[f^{*}=\sum_{i\in\mathbb{N}}f_{i}e_{i}.\] (82)_We assume that \(f^{*}\in[\mathcal{H}]^{t}\) holds for any \(t<s\) for a given \(s>0\). Also, we we assume that_

\[\sum_{i:\lambda>\lambda_{i}}|f_{i}|^{2}=\Omega\left(\lambda^{s}\right).\] (83)

_We also assume that the noise term satisfies \(\mathbf{E}[|\epsilon|^{2}|x]=\sigma^{2}\) holds for \(x\in\mathbb{S}^{d}\),a.e. Then, we define the main bias term in generalization error by_

\[\mathcal{R}^{2}(t;f^{*})=\sum_{i\in\mathbb{N}}e^{-2t\lambda_{i}}\lambda_{i}f_{ i}^{2},\] (84)

_and define the variance term by_

\[\mathcal{N}(t)=\sum_{i\in\mathbb{N}}[\lambda_{i}e^{-t\lambda_{i}}]^{2}.\] (85)

_Fix the given input vectors of samples \(X\). Consider the kernel gradient flow process detailed in (8) and let it start from \(0\). For any choice of \(t=t(n)\to\infty\), we have_

\[\mathbf{E}\left[\left\|f_{t}^{\mathrm{GF}}-f^{*}\right\|_{L^{2}}^{2}\middle|X \right]=\Omega_{\mathbf{P}}\left(\mathcal{R}^{2}(t;f^{*})+\frac{1}{n}\mathcal{ N}(t)\right),\] (86)

With the lemma above, now we are ready to prove Theorem 4.4.

Proof of Theorem 4.4.: By Proposition 4.1, we know the initial output function introduce an implicit bias term to the regression function. And thus the original problem is same as to consider a standard kernel gradient flow problem start from initial output zero with regression function \(\tilde{f}^{*}=f^{*}-f^{\mathrm{GP}}\). Recall that \(\mu\) is the uniform measure. On sphere, the RKHSs of dot-product kernels \(K_{0}^{\mathrm{NTK}}\) and \(K_{0}^{\mathrm{RFK}}\) are equivalent to corresponding Sobolev spaces through Lemma C.4. More precisely, suppose that we have chosen an orthonormal basis \(\{e_{i}\}_{i\in\mathbb{N}}\) consisting of spherical harmonic polynomials. Then we have

\[\begin{split}[\mathcal{H}_{0}^{\mathrm{NT}}]^{t}&= \left\{\sum_{i\in\mathbb{N}}a_{i}\omega_{i}^{t/2}e_{i}\Big{|}\sum_{i\in \mathbb{N}}a_{i}^{2}<\infty\right\},\\ [\mathcal{H}_{0}^{\mathrm{RF}}]^{t}&=\left\{\sum_{i \in\mathbb{N}}a_{i}\lambda_{i}^{t/2}e_{i}\Big{|}\sum_{i\in\mathbb{N}}a_{i}^{2} <\infty\right\}\end{split}\] (87)

for any \(t\geq 0\). Through Lemma C.2, we have the eigenvalue decay rate:

\[\omega_{i}\asymp i^{-\frac{d+1}{d}}\quad\text{and}\quad\lambda_{i}\asymp i^{- \frac{d+3}{d}}.\] (88)

We denote by \(\beta_{1}=\frac{d+1}{d}\) and \(\beta_{2}=\frac{d+3}{d}\). Similar to the proof of Theorem 4.2, we write the Kosambi-Karhunen-Loeve expansion of \(\tilde{f}^{*}\):

\[\tilde{f}^{*}=\sum_{i\in\mathbb{N}}\tilde{f}_{i}e_{i}=\sum_{i\in\mathbb{N}}(b _{i}-a_{i})\lambda_{i}^{1/2}e_{i},\] (89)

where

\[a_{i}\overset{\text{i.i.d.}}{\sim}N(0,1),\quad\text{and}\quad f^{*}=\sum_{i \in\mathbb{N}}b_{i}\lambda_{i}^{1/2}e_{i}\in[\mathcal{H}_{0}^{\mathrm{NT}}]^{ s}.\] (90)

Here \(a_{i}\) is a sequence of independent standard Gaussian variables, and \(b_{i}\) represents a sequence derived from the decomposition of \(f^{*}\). With such decomposition, we can verify that (83) holds with probability \(1-\delta^{\prime}\) for any \(\delta^{\prime}\in(0,1)\). Denote by \(g(\lambda)=\sum_{i:\lambda>\omega_{i}}|\widetilde{f}_{i}|^{2}\). Firstly, we have

\[\mathbf{E}[g(\lambda)]=\mathbf{E}\left[\sum_{i:\lambda>\omega_{i}}|\widetilde {f}_{i}|^{2}\right]\asymp\mathbf{E}\left[\sum_{i>\lfloor\lambda^{-\frac{d}{d+ 1}}\rfloor}|\widetilde{f}_{i}|^{2}\right]\gtrsim\lambda^{\frac{3}{d+1}}.\] (91)

We also have the variance

\[\mathrm{Var}\left[g(\lambda)\right]=\mathrm{Var}\left[\sum_{i:\lambda>\omega_ {i}}|\widetilde{f}_{i}|^{2}\right]\asymp\sum_{i:\lambda>\omega_{i}}\lambda_{i }+\sum_{i:\lambda>\omega_{i}}\lambda_{i}b_{i}^{2}\lesssim\lambda^{\frac{3}{d+1}}.\] (92)Where the second term is controlled by the source condition assumption on \(f^{*}\). Therefore, we have

\[\mathbf{P}\left(|g(\lambda)-\mathbf{E}[g(\lambda)]|\geq\mathbf{E}\left[\frac{g( \lambda)}{2}\right]\right)\leq\frac{4\mathrm{Var}[g(\lambda)]}{\left(\mathbf{E} g[(\lambda)]\right)^{2}}=O(\lambda^{\frac{3}{d+1}}).\] (93)

Define event \(A(\lambda)=\{|g(\lambda)-\mathbf{E}[g(\lambda)]|\leq\mathbf{E}[g(\lambda)]/2\}\). For any \(\delta^{\prime}\in(0,1)\), we choose a sequence \(\widetilde{\lambda}_{j}\), such that \(\widetilde{\lambda}_{j}=C^{\prime}j^{-\frac{2(d+1)}{3}}\). Then we have

\[\mathbf{P}\left(\cup_{j\in\mathbb{N}}A\left(\widetilde{\lambda}_{j}\right) \right)\geq 1-\sum_{j\in\mathbb{N}}[C^{\prime}]^{\frac{3}{d+1}}j^{-2}.\] (94)

We can choose appropriate \(C^{\prime}>0\) such that \(\cup_{j\in\mathbb{N}}A(\widetilde{\lambda}_{j})\) holds with probability at least \(1-\delta^{\prime}\), we denote by event \(A\). Conditioned on event \(A\), for any \(\widetilde{\lambda}_{j+1}\leq\lambda\leq\widetilde{\lambda}_{j}\), we have

\[g(\lambda)\geq g(\widetilde{\lambda}_{j+1})\gtrsim\frac{1}{2}(\widetilde{ \lambda}_{j+1})^{\frac{3}{d+1}}\quad\text{and}\quad\frac{\widetilde{\lambda}_{ j+1}}{\widetilde{\lambda}_{j}}=\left(\frac{j}{j+1}\right)^{\frac{2(d+1)}{3}}\] (95)

which shows that

\[g(\lambda)\gtrsim\frac{1}{2}(\widetilde{\lambda}_{j+1})^{\frac{3}{d+1}}\gtrsim \frac{1}{2}\left[\frac{j}{j+1}\right]^{2}\lambda^{\frac{3}{d+1}}.\] (96)

Therefore, we finish the proof of (83).

Then, we turns to the calculation of generalization error lower bound. First, we plug in the decomposition and calculate the bias term \(\mathcal{R}(t;\tilde{f}^{*})\):

\[\mathcal{R}^{2}(t;\tilde{f}^{*})=\sum_{i\in\mathbb{N}}e^{-2t\lambda_{i}} \lambda_{i}(a_{i}^{2}-2b_{i}a_{i}+b_{i}^{2}).\] (97)

Recalling that the eigenvalue decay rate is denoted by \(\beta\), it follows that

\[\mathbf{E}\left[\mathcal{R}^{2}(t;\tilde{f}^{*})\right]\geq\sum_{i\in \mathbb{N}}e^{-2t\lambda_{i}}\lambda_{i}\asymp\sum_{i\in\mathbb{N}}e^{-2ti^{- \beta_{1}}}i^{-\beta_{2}}\asymp t^{\frac{1}{\beta_{1}}-\frac{\beta_{2}}{\beta _{1}}}.\] (98)

Also, the variance of \(\mathcal{R}^{2}(t;\tilde{f}^{*})\) follows that

\[\mathrm{Var}(\mathcal{R}^{2}(t;\tilde{f}^{*}))\lesssim\sum_{i\in\mathbb{N}}e^{ -4t\lambda_{i}}\lambda_{i}^{2}+\sum_{i\in\mathbb{N}}e^{-4t\lambda_{i}}b_{i}^{2 }\lambda_{i}^{2},\] (99)

Here we introduce the denotations:

\[V_{0}\coloneqq\sum_{i\in\mathbb{N}}e^{-4t\lambda_{i}}2\lambda_{i}^{2},\quad \text{and}\quad V_{2}\coloneqq\sum_{i\in\mathbb{N}}e^{-4t\lambda_{i}}b_{i}^{2 }\lambda_{i}^{2}.\] (100)

We then have

\[V_{0}=\sum_{i\in\mathbb{N}}e^{-4t\lambda_{i}}\lambda_{i}^{2}\asymp\sum_{i\in \mathbb{N}}e^{-4ti^{-\beta_{1}}}i^{-2\beta_{2}}\asymp t^{\frac{1}{\beta_{1}}- 2\frac{\beta_{2}}{\beta_{1}}}.\] (101)

As to \(V_{2}\), we first recall that the smoothness of \(f^{*}\) lead to the following inequality:

\[\sum_{i\in\mathbb{N}}b_{i}^{2}i^{-1}<\infty,\] (102)

which implies that

\[\sum_{i\in\mathbb{N}}b_{i}^{4}i^{-2}<\infty.\] (103)

Now we turn to the evaluation of \(V_{2}\):

\[\begin{split} V_{2}&=\sum_{i\in\mathbb{N}}e^{-4t \lambda_{i}}b_{i}^{2}\lambda_{i}^{2}\asymp\sum_{i\in\mathbb{N}}e^{-4ti^{-\beta _{1}}}b_{i}^{2}i^{-2\beta_{2}}=\sum_{i\in\mathbb{N}}e^{-4ti^{-\beta_{1}}}b_{i}^ {2}i^{-1}i^{-2\beta_{2}+1}\\ &\leq\sqrt{\sum_{i\in\mathbb{N}}e^{-8ti^{-2\beta_{1}}}i^{-4\beta_ {2}+2}\sum_{i\in\mathbb{N}}b_{i}^{4}i^{-2}}\lesssim t^{\frac{1}{2\beta_{1}}- 2\frac{\beta_{2}}{\beta_{1}}+\frac{1}{\beta_{1}}}.\end{split}\] (104)It is worth noting that we use Cauchy's inequality to derive the upper bound above. With the control of \(V_{0}\) and \(V_{2}\), we have

\[\mathrm{Var}(\mathcal{R}^{2}(t;\tilde{f}^{*}))\asymp V_{0}+V_{2}\lesssim t^{\frac{ 3}{2\beta_{1}}-2\frac{\beta_{2}}{\beta_{1}}}\] (105)

Consequently, by Chebyshev's inequality, we directly have

\[\mathbf{P}\left(\left|\mathcal{R}^{2}(t;\tilde{f}^{*})-\mathbf{E}\left[ \mathcal{R}^{2}(t;\tilde{f}^{*})\right]\right|\geq\mathbf{E}\left[\mathcal{R} ^{2}(\tilde{f}^{*})\right]/2\right)\leq\frac{4\mathrm{Var}(\mathcal{R}^{2}(t; \tilde{f}^{*}))}{\left(\mathbf{E}\left[\mathcal{R}^{2}(t;\tilde{f}^{*}) \right]\right)^{2}}=O\left(t^{-\frac{1}{2\beta_{1}}}\right).\] (106)

Since \(t=t(n)\rightarrow+\infty\), we have

\[\mathcal{R}^{2}(t;\tilde{f}^{*})=\Omega_{\mathbf{P}}\left(t^{\frac{1}{\beta_ {1}}-\frac{\beta_{2}}{\beta_{1}}}\right).\] (107)

In the same way, we also have the bound of variance term \(\mathcal{N}(t)\).

\[\mathcal{N}(t)\asymp\frac{1}{n}t^{\frac{1}{\beta_{1}}}.\] (108)

Finally, apply Lemma E.1 and Proposition 3.3. We derive that for any \(\delta>0\), as long as \(n\) is large enough and \(m\) is large enough, for any choice of \(t=t(n)\rightarrow\infty\), with probability at least \(1-\delta\) we have

\[\mathbf{E}\left[\|f_{t}^{\mathrm{NN}}-f^{*}\|_{L^{2}}^{2}|X\right]=\Omega\left( \mathcal{R}^{2}+\frac{1}{n}\mathcal{N}\right)=\Omega\left(t^{\frac{1}{\beta_ {1}}-\frac{\beta_{2}}{\beta_{1}}}+\frac{1}{n}t^{\frac{1}{\beta_{1}}}\right)= \Omega\left(n^{-\frac{3}{d+3}}\right).\] (109)

Thus the theorem is proved. 

**Remark E.2**.: In Proposition 3.3, we consider the situation that both input \(X\) and output \(Y\) of samples are fixed, while in the proof above we require that only \(X\) is fixed. However, the conclusion of Proposition 3.3 still holds when \(Y\) of samples is random. This is because the noise term has a finite second moment. Also, the change of domain from \(\mathcal{X}\) to \(\mathbb{S}^{d}\) will not affect the uniform convergence result.

## Appendix F Details in artificial data experiments

Fixing the dimension of data as \(d=5,10\). We draw samples for variable \(x\) from the standard Gaussian distribution \(\mathcal{N}(0,I_{d})\), which are consequently standardized to lie on the surface of the unit hypersphere \(\mathbb{S}^{d}\). The dependent variable \(y\) is formulated as:

\[y=f(x)+\varepsilon,\] (110)

where \(f(x)=\left(\sum_{j=1}^{d}x_{j}\right)^{2}\), \(\varepsilon\sim\mathcal{N}(0,\sigma^{2})\) and \(\sigma=0.2\). The function \(f\) exhibits notable smoothness, since it can be linearly represented in terms of the first few spherical harmonic polynomials on \(\mathbb{S}^{d}\)[9] and the fact that \(K_{0}^{\mathrm{NTK}}\) is a dot-product kernel. We consider fully-connected network with one singular hidden layer, choosing \(m=20*n\) to ensure large enough width. Consistent to previous sections, we choose ReLU function as the non-linear activation and train the network using Gradient Descent for a sufficiently long time. We record the generalization error at each moment and define the moment of minimum generalization error as the final generalization error. This is done to align with the early stopping strategy mentioned in the Theorem 4.3.

## Appendix G Details in real data experiments

In this subsection, we will provide the theoretical basis of the method which approximates the smoothness of the goal function of real dataset. Let \(\mathcal{X}\subset\mathbb{R}^{d}\) be a bounded domain. Given a reproduce kernel \(k(\cdot,\cdot)\) on \(\mathcal{X}\) and a probability measure \(\mu\). Denote the RKHS by \(\mathcal{H}_{k}=\left\{\sum_{i\in\mathbb{N}}a_{i}\lambda_{i}^{\frac{1}{2}}e_{i }\middle|\sum_{i\in\mathbb{N}}a_{i}^{2}<\infty\right\}\). We assume that there is a function \(f:\mathcal{X}\rightarrow\mathbb{R}\) and a probability density \(\mu\) on \(\mathcal{X}\). Suppose that the samples satisfies \(y=f(x)\), then \(f\) has the decomposition:

\[f=\sum_{i\in\mathbb{N}}\theta_{i}e_{i}.\] (111)

[MISSING_PAGE_EMPTY:27]

[MISSING_PAGE_FAIL:28]

Then we have

\[\left\|x\right\|_{(L^{2},\mathcal{H}_{2})_{s,2}}= \int_{0}^{\infty}[t^{-s}K_{2}(t;x)]^{2}\,\frac{dt}{t}\] (115) \[\leq\int_{0}^{\infty}[t^{-s}K_{1}(Ct;x)]^{2}\,\frac{dt}{t}\] \[\leq C^{2s}\int_{0}^{\infty}[(Ct)^{s}K_{1}(Ct;x)]^{2}\,\frac{d(Ct) }{Ct}\] \[=C^{2s}\|x\|_{(L^{2},\mathcal{H}_{1})_{s,2}}\]

Let \(C_{1}=C^{2s}\), we have the canonical injection satisfies \(\left\|(L^{2},\mathcal{H}_{1})_{s,2}\hookrightarrow(L^{2},\mathcal{H}_{2})_{s,2}\right\|_{\text{op}}\leq C_{1}\). Also, since \(\mathcal{H}_{1}\cong\mathcal{H}_{2}\), for any \(x\in\mathcal{H}_{2}\), we have \(\left\|x\right\|_{\mathcal{H}_{1}}\leq c\left\|x\right\|_{\mathcal{H}_{2}}\). We can prove \(\left\|(L^{2},\mathcal{H}_{2})_{s,2}\hookrightarrow(L^{2},\mathcal{H}_{1})_{s,2}\right\|_{\text{op}}\leq C_{2}\) in the same way. Then, we finish the proof. 

**Lemma H.6** (Equivalence of interpolation spaces).: _Suppose \(0<s<1\). Denote \(\mathcal{H}\) be a RKHS and \(\mu,\nu\) be measures on set \(\mathcal{X}\). If we have \(L^{2}(\mathcal{X},\mu)\cong L^{2}(\mathcal{X},\nu)\), then \((L^{2}(\mathcal{X},\mu),\mathcal{H})_{s,2}\cong(L^{2}(\mathcal{X},\nu), \mathcal{H})_{s,2}\)._

Proof.: The proof in accomplished in the same way as Lemma H.5.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and the introduction, we have summarized the background of the NTK theory and the influence of initialization on neural networks under this background. We have also discussed the results, which reflects the contributions of this paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Compared to the other sections, stronger assumptions were used in the lower bound section (data distributed on the sphere). Although this is common in kernel regression, we still reminded in the paper that this is a more strict assumption. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided assumptions in the main body of the paper, and have also given comprehensive theoretical background information and proofs in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In our experiments, our model is simple and uses public datasets. The experimental results are easy to reproduce. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Yes, the datasets we utilized are publicly accessible for everyone. We have comprehensively outlined the experimental parameters and model settings within the experimental section of our paper. Notwithstanding the low complexity of our model, we are more than willing to post our code on GitHub should there be a demand for it. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper has indeed provided all necessary training and testing details for understanding the results. We leveraged the several datasets which are publically accessible to everyone. Furthermore, we have comprehensively outlined all the experimental parameters and model settings. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In the artificial data experiment, we can directly see the generalization error decay rate in the figure. In the real data experiment, we do a least square regression to calculate the smoothness of a function. Guidelines: ** The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, the computational resources needed are minimal, which means the experiments can be easily conducted on nearly any GPU without any additional requirements. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, we conducted original theoretical research, which is in accordance with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: Our work is mainly a theoratical work focusing on learning theory, so this question is not applicable to our paper. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work is mainly a theoratical work focusing on learning theory, so this paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have explicitly referenced the datasets in our paper, as MNIST, CIFAR-10 and Fashion-MNIST. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We did not utilize new assets in our work. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing experiments or research related to human subjects, therefore this question is not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not involve any study participants, so there is no need to discuss potential risks they might face. Therefore, there is also no need for an Institutional Review Board (IRB) approval. Hence, this question is not applicable to our paper.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.