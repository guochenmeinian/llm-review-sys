# Two-Stage Learning to Defer with Multiple Experts

 Anqi Mao

Courant Institute

New York, NY 10012

aqmao@cims.nyu.edu

&Christopher Mohri

Stanford University

Stanford, CA 94305

xmohri@stanford.edu

Mehryar Mohri

Google Research & CIMS

New York, NY 10011

mohri@google.com

&Yutao Zhong

Courant Institute

New York, NY 10012

yutao@cims.nyu.edu

###### Abstract

We study a two-stage scenario for learning to defer with multiple experts, which is crucial in practice for many applications. In this scenario, a predictor is derived in a first stage by training with a common loss function such as cross-entropy. In the second stage, a deferral function is learned to assign the most suitable expert to each input. We design a new family of surrogate loss functions for this scenario both in the score-based and the predictor-rejector settings and prove that they are supported by \(\mathcal{H}\)-consistency bounds, which implies their Bayes-consistency. Moreover, we show that, for a constant cost function, our two-stage surrogate losses are realizable \(\mathcal{H}\)-consistent. While the main focus of this work is a theoretical analysis, we also report the results of several experiments on CIFAR-10 and SVHN datasets.

## 1 Introduction

Large language models (LLMs) have achieved a remarkable performance on diverse tasks across multiple domains, as reported in recent surveys (Wei et al., 2022; Bubeck et al., 2023). However, their practical application faces two critical challenges: the occurrence of _hallucinations_, that is the generation of incorrect or misleading content, and an inefficient inference. Leveraging multiple experts can address both issues. To reduce hallucinations, one can refrain from using the original predictor in uncertain instances and defer to one of the more complex and more accurate experts. To enhance efficiency, one can derive models of different sizes distilled from the original complex model and use one of these more streamlined versions, while deferring to the more complex and less efficient ones for suitable contexts. Both problems require assigning each instance to the most suitable expert. This motivates the problem of _learning to defer in the presence of multiple experts_.

The scenario of _single-stage learning to defer_ has been studied by several publications, starting with the foundational framework introduced by Cortes et al. (2016, 2023) for learning to reject and followed by a series of studies on abstention and deferral (Madras et al., 2018; Raghu et al., 2019; Mozannar and Sontag, 2020; Wilder et al., 2021; Pradier et al., 2021; Keswani et al., 2021; Raman and Yee, 2021; Liu et al., 2022; Verma and Nalisnick, 2022; Charusaie et al., 2022; Cao et al., 2022; Verma et al., 2023; Mao et al., 2023;,b,c; Mozannar et al., 2023). In the single-stage scenario, a predictor and a deferral function are learned simultaneously, with the deferral function determining the best expert assigned to each input instance. However, in practice, a predictor such as an LLM is already available and retraining one in conjunction with a deferral function could be prohibitively costly: depending on its size and the amount of data used, retraining could take several weeks or months. Thus, the single-stage learning to defer scenario and its solutions often do not align with the practical challenges encountered in real-world applications.

Alternative post-hoc methods have been proposed to address the learning to defer problem. Okati et al. (2021) proposed an iterative approach optimizing a predictor and a rejector over multiple epochs. Within each epoch, first the predictor is trained on points where its loss is lower than that of a human expert; second, the rejector is fitted to predict which of the predictor or the human expert has a lower loss. Narasimhaan et al. (2022) suggested a post-hoc correction to the single-stage learning to defer surrogate losses, specifically the cost-sensitive softmax cross-entropy (CSS) surrogate loss in (Mozannar and Sontag, 2020) and the one-versus-all (OvA) surrogate loss in (Verma and Nalisnick, 2022) for cases where they suffer from underfitting. However, as with the single-stage learning to defer solutions, post-hoc approaches do not apply to scenarios where an existing predictor, pre-trained using a standard classification loss function such as cross-entropy, is already available.

Can we derive a principled algorithm for learning to defer with multiple experts in such scenarios? Which surrogate loss should we adopt and what consistency guarantee can we rely on? This paper deals precisely with these questions.

A key criterion for surrogate losses in learning to defer is Bayes-consistency (Zhang, 2004; Bartlett et al., 2006; Steinwart, 2007), that is minimizing the surrogate loss over the family of measurable functions leads to the minimization of the deferral loss. The surrogate losses proposed in (Mozannar and Sontag, 2020; Verma and Nalisnick, 2022) have been shown to be Bayes-consistent for deferral. However, Bayes-consistency is not relevant in learning tasks since the hypothesis set used, for example that of some family of linear functions or neural networks, never includes all measurable functions. Long and Servedio (2013) proposed a notion of realizable \(\mathcal{H}\)-consistency, that is consistency associated with a specific hypothesis set in the realizable scenario. Mozannar et al. (2023) recently showed that existing Bayes-consistent surrogate losses in (Mozannar and Sontag, 2020; Verma and Nalisnick, 2022) are not realizable \(\mathcal{H}\)-consistent for learning with deferral, which can pose significant challenges when learning with a restricted hypothesis set \(\mathcal{H}\), even for simple linear models. Instead, they proposed a new surrogate loss that is realizable \(\mathcal{H}\)-consistent when \(\mathcal{H}\) is closed under scaling. However, they also observed that the loss function of Madras et al. (2018), which is not Bayes-consistent, is actually realizable \(\mathcal{H}\)-consistent. They acknowledged their inability to prove or disprove whether their proposed surrogate loss is Bayes-consistent. Consequently, it has remained an open problem to identify a surrogate loss that is both consistent and realizable-consistent.

In recent work, Verma et al. (2023) proposed the first Bayes-consistent surrogate losses in the scenario of single-stage learning to defer with _multiple experts_(Hemmer et al., 2022; Keswani et al., 2021; Kerrigan et al., 2021; Straitouri et al., 2022; Benz and Rodriguez, 2022). This scenario is more attractive and significant in applications such as large language models, where multiple models are often available for deferral. However, the surrogate losses proposed by the authors do not benefit from realizable \(\mathcal{H}\)-consistency, even in the single-expert setting, since they are a straightforward generalization of those of Mozannar and Sontag (2020) and Verma and Nalisnick (2022).

Bayes-consistency, or even realizable \(\mathcal{H}\)-consistency for a specific hypothesis set \(\mathcal{H}\), is an asymptotic property, and provides no guarantee for approximate minimizers since convergence could be arbitrarily slow. More favorable guarantees, known as \(\mathcal{H}\)_-consistency bounds_, were recently introduced for standard classification settings (Awasthi et al., 2022; Mao et al., 2022; Zhang et al., 2022). These guarantees are upper bounds on the target estimation loss expressed in terms of the surrogate estimation loss. They are stronger and more informative guarantees than Bayes-consistency and \(\mathcal{H}\)-consistency because they are both hypothesis set-specific and non-asymptotic. More recently, Mao et al. (2023) introduced a new family of surrogate losses and algorithms for the general problem of single-stage learning to defer with multiple experts that benefit from strong \(\mathcal{H}\)-consistency bounds.

**Our contributions.** We study a two-stage scenario for learning to defer with multiple experts that is crucial in practice for many applications. In this scenario, a predictor is derived in a first stage by training with a common loss function such as cross-entropy. In the second stage, a deferral function is learned to assign the most suitable expert to each input. We design a new family of surrogate loss functions for this scenario both in the _score-based setting_ (Section 3) and the _predictor-rejector_ setting (Section 4) and prove that they are supported by \(\mathcal{H}\)-consistency bounds, which implies their Bayes-consistency. Moreover, we show that, for a constant cost function, our two-stage surrogate losses are realizable \(\mathcal{H}\)-consistent. While the main focus of this work is a theoretical analysis, we also report the results of several experiments on CIFAR-10 and SVHN datasets (Section 5). We give a comprehensive discussion of related work in Appendix A. We begin by providing some basic definitions and notation (Section 2).

Preliminaries

We consider the standard multi-class classification setting with an input space \(\mathcal{X}\) and a set of \(n\geq 2\) labels \(\mathcal{Y}=[n]\), where we use the notation \([n]\) to denote the set \(\{1,\ldots,n\}\). We study the scenario of _learning to defer with multiple experts_, where the label set \(\mathcal{Y}\) is augmented with \(n_{e}\) additional labels \(\{n+1,\ldots,n+n_{e}\}\) corresponding to \(n_{e}\) pre-defined experts \(h_{1},\ldots,h_{n_{e}}\). In this scenario, the learner has the option of returning a label \(y\in\mathcal{Y}\), which represents the category predicted, or a label \(y=n+j\), \(j\geq 1\), in which case it is _deferring_ to expert \(h_{j}\). This setting is referred to as the _score-based setting_(Mozannar and Sontag, 2020; Cao et al., 2022; Mao et al., 2023), since the deferral corresponds to extra \(n_{e}\) scoring functions. An alternative setting is the _predictor-rejector setting_(Cortes et al., 2016, 2023; Mohri et al., 2023; Mao et al., 2023), where the deferral function is selected from a separate family of functions \(\mathcal{R}\). We introduce that setting and include the corresponding results in Section 4 for completeness.

We denote by \(\overline{\mathcal{Y}}=[n+n_{e}]\) the augmented label set and consider a hypothesis set \(\mathcal{H}\) of functions mapping from \(\mathcal{X}\times\overline{\mathcal{Y}}\) to \(\mathbb{R}\). The prediction associated by \(h\in\mathcal{H}\) to an input \(x\in\mathcal{X}\) is denoted by \(\mathsf{h}(x)\) and defined as the element in \(\overline{\mathcal{Y}}\) with the highest score, \(\mathsf{h}(x)=\operatorname*{argmax}_{y\in[n+n_{e}]}h(x,y)\), with an arbitrary but fixed deterministic strategy for breaking ties. We denote by \(\mathcal{H}_{\mathrm{all}}\) the family of all measurable functions.

The _deferral loss function_\(\mathsf{L}_{\mathrm{def}}\) is defined as follows for any \(h\in\mathcal{H}\) and \((x,y)\in\mathcal{X}\times\mathcal{Y}\):

\[\mathsf{L}_{\mathrm{def}}(h,x,y)=\mathds{1}_{\mathsf{h}(x)*y}\mathds{1}_{ \mathsf{h}(x)\in[n]}+\sum_{j=1}^{n_{e}}c_{j}(x,y)\mathds{1}_{\mathsf{h}(x)=n+j} \tag{1}\]

Thus, the loss incurred coincides with the standard zero-one classification loss when \(\mathsf{h}(x)\), the label predicted, is in \(\mathcal{Y}\). Otherwise, when \(\mathsf{h}(x)\) is equal to \(n+j\), the loss incurred is \(c_{j}(x,y)\), the cost of deferring to expert \(h_{j}\). Let \(\bar{c}_{j}(x,y)=1-c_{j}(x,y)\). We will denote by \(\underline{c}_{j}\geq 0\) and \(\overline{c}_{j}\leq 1\) finite lower and upper bounds on the cost \(\bar{c}_{j}\), that is \(\bar{c}_{j}(x,y)\in[\underline{c}_{j},\overline{c}_{j}]\) for all \((x,y)\in\mathcal{X}\times\mathcal{Y}\). There are many possible choices for these costs. Our analysis for Theorem 1, Corollary 2, Theorem 6 is general and requires no assumption other than their boundedness. One natural choice is to define cost \(c_{j}\) as a function of expert \(h_{j}\)'s inaccuracy, for example \(c_{j}(x,y)=\alpha_{j}\mathds{1}_{\mathsf{h}_{j}(x)*y}+\beta_{j}\), with \(\alpha_{j},\beta_{j}>0\), where \(\mathsf{h}_{j}(x)\) is the prediction made by \(h_{j}\)th for input \(x\). Typically, the hyperparameter \(\alpha_{j}\) has two potential values: zero or one. When \(\alpha_{j}\) is set to one, the first term of the formulation pertains to the inaccuracy of expert expert \(h_{j}\). Conversely, with \(\alpha_{j}\) set to zero, the first term vanishes, focusing solely on the inference cost. Theorems 5 and 7 are analyzed under this assumption. The \(\beta_{j}\) in the second term corresponds to the inference cost incurred by expert \(h_{j}\).

Given a distribution \(\mathcal{D}\) over \(\mathcal{X}\times\mathcal{Y}\), we will denote by \(\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}(h)\) the expected deferral loss of a hypothesis \(h\in\mathcal{H}\), \(\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}(h)=\mathbb{E}_{\{x,y\}\sim\mathcal{D}} [\mathsf{L}_{\mathrm{def}}(h,x,y)]\), and by \(\mathcal{E}^{*}_{\mathsf{L}_{\mathrm{def}}}(\mathcal{H})=\inf_{h\in\mathcal{ H}}\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}(h)\) its infimum or best-in-class expected loss. We will adopt similar definitions for other loss functions.

Given a hypothesis set \(\mathcal{H}\), an _\(\mathcal{H}\)-consistency bound_(Awasthi et al., 2021, 2021, 2022, 2023,

## 3 Two-stage \(\mathcal{H}\)-consistent surrogate loss

In this section, we consider an important _two-stage_ scenario for learning to defer with multiple experts. This is a critical scenario in practice for many applications where a predictor is already available, as a result of training with a loss function \(\ell\) supported by \(\mathcal{H}\)-consistency bounds, such as the logistic loss (first stage). The logistic loss coincides with the cross-entropy loss when a softmax activation is applied to the output of a neural network. The problem then consists of learning a deferral function (second stage) to assign the most suitable expert to each input instance.

We first design a new family of surrogate losses for this _two-stage_ scenario (Section 3.1). Next, we show that our surrogate losses benefit from \(\mathcal{H}\)-consistency bounds (Section 3.2). As a by-product, we prove \(\overline{\mathcal{H}}\)-consistency bounds in standard multi-class classification, where \(\overline{\mathcal{H}}\) denotes hypothesis sets with a fixed scoring function (Section 3.3). These bounds have not been studied before and can be of independent interest in other consistency studies. Moreover, we show that, for a constant cost function, our two-stage surrogate losses are realizable \(\mathcal{H}\)-consistent (Section 3.4).

### General surrogate losses

A hypothesis set \(\mathcal{H}\) of functions mapping from \(\mathcal{X}\times[n+n_{e}]\) to \(\mathbb{R}\) can be decomposed as \(\mathcal{H}=\mathcal{H}_{p}\times\mathcal{H}_{d}\), where \(\mathcal{H}_{p}\) denotes the hypothesis set spanned by the first \(n\) scores, used for prediction, and \(\mathcal{H}_{d}\) the hypothesis set spanned by the final \(n_{e}\) scores, used for deferral. Thus, any \(h\in\mathcal{H}\) can be written as a pair \(h=(h_{p},h_{d})\) with \(h_{p}\in\mathcal{H}_{p}\) and \(h_{d}\in\mathcal{H}_{d}\).

Let \(\ell\) be a surrogate loss for standard multi-class classification with \(n\) classes. We consider the following two-stage scenario: in the first stage, \(h_{p}\) is learned using the surrogate loss \(\ell_{1}\); in the second stage, \(h_{d}\) is learned using a surrogate loss \(\mathsf{L}_{h_{p}}\) that depends on the prediction function \(h_{p}\) learned in the first stage.

To any \(h_{d}\in\mathcal{H}_{d}\), we associate a hypothesis \(\overline{h}_{d}\) defined over \((n_{e}+1)\) classes \(\{0,1,\ldots,n_{e}\}\) by \(\overline{h}_{d}(x,0)=\max_{y\in y}h_{p}(x,y)\), that is the maximal score assigned by \(h_{p}\) to its predicted label, and \(\overline{h}_{d}(x,j)=h_{d}(x,j)\) for \(j\in[n_{e}]\). We can then define our suggested surrogate loss for the second stage as follows:

\[\mathsf{L}_{h_{p}}(h_{d},x,y)=\mathds{1}_{h_{p}(x)=y}\,\ell_{2}( \overline{h}_{d},x,0)+\sum_{j=1}^{n_{e}}\bar{c}_{j}(x,y)\ell_{2}(\overline{h}_ {d},x,j), \tag{3}\]

where \(\ell_{2}(\overline{h}_{d},x,j)\) is a surrogate loss for standard multi-class classification with \((n_{e}+1)\) categories \(\{0,1,\ldots,n_{e}\}\). Intuitively, the indicator term \(\mathds{1}_{h(x)n+j}\) in the deferral loss (1) penalizes \(h_{d}(x,j)\) when it has a small value. Similarly, for a standard surrogate loss \(\ell_{2}(\overline{h}_{d},x,j)\) such as the logistic loss, it penalizes \(\overline{h}_{d}(x,j)\) when it has a small value as well. In Table 2, we present a summary of examples of such second-stage surrogate losses, where \(\ell_{2}\) is selected from common surrogate losses in standard multi-class classification defined in Table 1. A detailed derivation is presented in Appendix B.

From the point of view of the second stage, \(x\mapsto\overline{h}_{d}(x,0)=\max_{y\in y}h_{p}(x,y)\) is a fixed function. We will denote by \(\overline{\mathcal{H}}_{d}\) the family of hypotheses \(\overline{h}_{d}\colon\mathcal{X}\times\{0,1,\ldots,n_{e}\}\to\mathbb{R}\) whose first scoring function, \(\overline{h}_{d}(\cdot,0)\), is fixed and not to be learned in the second stage.

Our formulation bears some similarity with the design of a surrogate loss function for rejectors in (Cortes et al., 2016, 2023) for learning with rejection in binary classification, where the cost is a

\begin{table}
\begin{tabular}{l l} \hline \hline Name & Formulation \\ \hline Sum exponential loss & \(\ell_{\mathrm{exp}}(\overline{h},x,y)=\sum_{y^{\prime}\neq y}e^{\overline{h}( x,y^{\prime})-\overline{h}(x,y)}\). \\ Multinomial logistic loss & \(\ell_{\mathrm{log}}(\overline{h},x,y)=\log\Bigl{(}\sum_{y^{\prime}\in y\cup\{0 \}}e^{\overline{h}(x,y^{\prime})-\overline{h}(x,y)}\Bigr{)}\). \\ Generalized cross-entropy loss & \(\ell_{\mathrm{gce}}(\overline{h},x,y)=\frac{1}{\alpha}\Bigl{[}1-\left[\frac{e^ {\overline{h}(x,y)}}{\sum_{y^{\prime}\in y\cup\{0\}}e^{\overline{h}(x,y^{ \prime})}}\right]^{\alpha}\Bigr{]}\), \(\alpha\in(0,1)\). \\ Mean absolute error loss & \(\ell_{\mathrm{mae}}(\overline{h},x,y)=1-\frac{e^{\overline{h}(x,y)}}{\sum_{y^{ \prime}\in y\cup\{0\}}e^{\overline{h}(x,y^{\prime})}}\). \\ \hline \hline \end{tabular}
\end{table}
Table 1: Common surrogate losses in standard multi-class classification.

constant. However, our surrogate loss is tailored to accommodate a general cost function depending on both \(x\) and \(y\) for deferral, in contrast with a constant one, and it allows for multiple deferral options, as opposed to only one rejection option.

### \(\mathcal{H}\)-consistency bounds for two-stage surrogate losses

In this section, we provide strong guarantees for two-stage surrogate losses, provided that the first-stage loss function \(\ell_{1}\) admits an \(\mathcal{H}_{p}\)-consistency bound, and the second-stage surrogate \(\ell_{2}\) admits an \(\overline{\mathcal{H}}_{d}\)-consistency bound.

**Theorem 1** (\(\mathcal{H}\)-consistency bounds for score-based two-stage surrogates).: _Assume that \(\ell_{1}\) admits an \(\mathcal{H}_{p}\)-consistency bound and \(\ell_{2}\) admits an \(\overline{\mathcal{H}}_{d}\)-consistency bound with respect to the multi-class zero-one classification loss \(\ell_{0-1}\) respectively. Thus, there are non-decreasing concave functions \(\Gamma_{1}\) and \(\Gamma_{2}\) such that, for all \(h_{p}\in\mathcal{H}_{p}\) and \(\overline{h}_{d}\in\overline{\mathcal{H}}_{d}\), we have_

\[\mathcal{E}_{\ell_{0-1}}(h_{p})-\mathcal{E}_{\ell_{0-1}}^{*}( \mathcal{H}_{p})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H}_{p})\leq\Gamma_{1}\big{(} \mathcal{E}_{\ell_{1}}(h_{p})-\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H}_{p})+ \mathcal{M}_{\ell_{1}}(\mathcal{H}_{p})\big{)}\] \[\mathcal{E}_{\ell_{0-1}}(\overline{h}_{d})-\mathcal{E}_{\ell_{0-1 }}^{*}(\overline{\mathcal{H}}_{d})+\mathcal{M}_{\ell_{0-1}}(\overline{\mathcal{ H}}_{d})\leq\Gamma_{2}\big{(}\mathcal{E}_{\ell_{2}}(\overline{h}_{d})- \mathcal{E}_{\ell_{2}}^{*}(\overline{\mathcal{H}}_{d})+\mathcal{M}_{\ell_{2}}( \overline{\mathcal{H}}_{d})\big{)}.\]

_Then, the following holds for all \(h\in\mathcal{H}\):_

\[\mathcal{E}_{\mathrm{L}_{\mathrm{def}}}(h)-\mathcal{E}_{\mathrm{ L}_{\mathrm{def}}}^{*}(\mathcal{H})+\mathcal{M}_{\mathrm{L}_{\mathrm{def}}}( \mathcal{H})\] \[\leq\Gamma_{1}\big{(}\mathcal{E}_{\ell_{1}}(h_{p})-\mathcal{E}_{ \ell_{1}}^{*}(\mathcal{H}_{p})+\mathcal{M}_{\ell_{1}}(\mathcal{H}_{p})\big{)}+ \left(1+\sum_{j=1}^{n_{e}}\overline{c}_{j}\right)\Gamma_{2}\Bigg{(}\frac{ \mathcal{E}_{\mathrm{L}_{h_{p}}}(h_{d})-\mathcal{E}_{\mathrm{L}_{h_{p}}}^{*}( \mathcal{H}_{d})+\mathcal{M}_{\mathrm{L}_{h_{p}}}(\mathcal{H}_{d})}{\sum_{j=1} ^{n_{e}}\overline{c}_{j}}\Bigg{)}.\]

_Furthermore, constant factors \(\left(1+\sum_{j=1}^{n_{e}}\overline{c}_{j}\right)\) and \(\frac{1}{\sum_{j=1}^{n_{e}}\overline{c}_{j}}\) can be removed when \(\Gamma_{2}\) is linear._

The proof is given in Appendix D. It consists of expressing the conditional regret of the deferral loss as the sum of two regrets, first by minimizing \(h_{d}\) for a fixed \(h_{p}\) and then by minimizing \(h_{p}\). Subsequently, we show how each regret can be upper-bounded in terms of the conditional regret of each stage's surrogate loss, leveraging the \(\mathcal{H}_{p}\)-consistency bound of \(\ell_{1}\) and \(\overline{\mathcal{H}}_{d}\)-consistency bound of \(\ell_{2}\) with respect to the zero-one loss. This, in conjunction with the concavity of functions \(\Gamma_{1}\) and \(\Gamma_{2}\), establishes our \(\mathcal{H}\)-consistency bounds.

Thus, the theorem provides a strong guarantee for the two-stage surrogate losses. A specific instance of Theorem 1 holds for the case where \(\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H}_{p})=\mathcal{E}_{\ell_{1}}^{*}( \mathcal{H}_{\mathrm{all}})\) and \(\mathcal{E}_{\mathrm{L}_{h_{p}}}^{*}(\mathcal{H}_{d})=\mathcal{E}_{\mathrm{L} _{h_{p}}}^{*}(\mathcal{H}_{\mathrm{all}})\), ensuring that the Bayes-error coincides with the best-in-class error and, consequently, \(\mathcal{M}_{\ell_{1}}(\mathcal{H}_{p})=\mathcal{M}_{\mathrm{L}_{h_{p}}}( \mathcal{H}_{d})=0\). Given Theorem 1 and the non-negativity property of \(\mathcal{M}_{\mathrm{L}_{\mathrm{def}}}(\mathcal{H})\), we can derive the following corollary.

**Corollary 2**.: _Assume that \(\ell\) satisfies the same assumption as in Theorem 1. Then, for all \(h\in\mathcal{H}\) and any distribution such that \(\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H}_{p})=\mathcal{E}_{\ell_{1}}^{*}( \mathcal{H}_{\mathrm{all}})\) and \(\mathcal{E}_{\mathrm{L}_{h_{p}}}^{*}(\mathcal{H}_{d})=\mathcal{E}_{\mathrm{L} _{h_{p}}}^{*}(\mathcal{H}_{\mathrm{all}})\), we have_

\[\mathcal{E}_{\mathrm{L}_{\mathrm{def}}}(h)-\mathcal{E}_{\mathrm{ L}_{\mathrm{def}}}^{*}(\mathcal{H})\leq\Gamma_{1}\big{(}\mathcal{E}_{\ell_{1}}(h_{p})- \mathcal{E}_{\ell_{1}}^{*}(\mathcal{H}_{p})\big{)}+\left(1+\sum_{j=1}^{n_{e}} \overline{c}_{j}\right)\Gamma_{2}\Bigg{(}\frac{\mathcal{E}_{\mathrm{L}_{h_{p} }}(h_{d})-\mathcal{E}_{\mathrm{L}_{h_{p}}}^{*}(\mathcal{H}_{d})}{\sum_{j=1}^{n_ {e}}\overline{c}_{j}}\Bigg{)},\]

_where the constant factors \(\left(1+\sum_{j=1}^{n_{e}}\overline{c}_{j}\right)\) and \(\frac{1}{\sum_{j=1}^{n_{e}}\overline{c}_{j}}\) can be removed when \(\Gamma_{2}\) is linear._

Corollary 2 implies that when the estimation error of the first-stage surrogate loss, \(\mathcal{E}_{\ell_{1}}(h_{p})-\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H}_{p})\), is reduced to \(\epsilon_{1}\), and the estimation error of the second-stage surrogate loss, \(\mathcal{E}_{\mathrm{L}_{h_{p}}}(h_{d})-\mathcal{E}_{\mathrm{L}_{h_{p}}}^{*}( \mathcal{H}_{d})\), is reduced to \(\epsilon_{2}\), the estimation error of the deferral loss, \(\mathcal{E}_{\mathrm{L}_{\mathrm{def}}}(h)-\mathcal{E}_{\mathrm{L}_{\mathrm{ def}}}^{*}(\mathcal{H})\), is upper-bound by

\[\Gamma_{1}(\epsilon_{1})+\left(1+\sum_{j=1}^{n_{e}}\overline{c}_{j}\right) \Gamma_{2}\Bigg{(}\frac{\epsilon_{2}}{\sum_{j=1}^{n_{e}}\overline{c}_{j}} \Bigg{)}.\]

The common surrogate losses mentioned earlier all satisfy the first-stage requirement; however, it was unclear if they would meet the second-stage criterion since the \(\overline{\mathcal{H}}_{d}\)-consistency bound is for hypothesis sets \(\overline{\mathcal{H}}_{d}\) with a fixed first scoring function. This has not been previously studied in the literature. In the next section, we prove for the first time that common multi-class surrogate losses,

[MISSING_PAGE_FAIL:6]

In their proof, to set an upper bound on the estimation error of the zero-one loss using that of the surrogate loss, they select an auxiliary function \(\overline{h}_{\mu}\) for any hypothesis \(h\). This function is contingent on the distinct scores of \(h\). Subsequently, the authors choose an optimal \(\mu\) to set these bounds. Nevertheless, if any of \(h\)'s scores are fixed, an optimal \(\mu\) does not exist, preventing the establishment of a meaningful bound. Instead, our new proof method overcomes this limitation by choosing \(\overline{h}_{\mu}\) based on the softmax, as the softmax corresponding to the label zero can still vary due to the influence of changes in other scores, even when the scoring function on label zero is fixed.

### Realizable \(\mathcal{H}\)-consistency

Recently, Mozannar et al. (2023) showed that even in the straightforward single-expert setting, existing Bayes-consistent single-stage surrogate losses (Mozannar and Sontag, 2020, Verma and Nalisnick, 2022) are not _realizable \(\mathcal{H}\)-consistent_(Long and Servedio, 2013, Zhang and Agarwal, 2020) for learning with deferral. This can pose significant challenges when learning with a restricted hypothesis set \(\mathcal{H}\), even for simple linear models. Instead, they proposed a new surrogate loss that is realizable \(\mathcal{H}\)-consistent when \(\mathcal{H}\) is _closed under scaling_, meaning that it satisfies the condition \(h\in\mathcal{H}\Rightarrow\tau h\in\mathcal{H}\) for all \(\tau\) in the set of real numbers. However, they stated that they could not prove or disprove whether their proposed surrogate loss is Bayes-consistent. Consequently, it has become crucial to identify a surrogate loss that is both consistent and realizable-consistent, which has remained an open problem.

**Definition 4** (**Realizable \(\mathcal{H}\)-consistency**).: _A surrogate loss \(\mathsf{L}\) is considered a realizable \(\mathcal{H}\)-consistent loss function for the deferral loss \(\mathsf{L}_{\mathrm{def}}\) if, for any distribution that is \(\mathcal{H}\)-realizable, that is, there exists a zero loss solution \(h^{*}\in\mathcal{H}\) with \(\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}(h^{*})=0\), optimizing the surrogate loss results in obtaining the zero-error solution:_

\[\mathcal{E}_{\mathsf{L}}(h_{n})-\mathcal{E}_{\mathsf{L}}^{*}(\mathcal{H}) \xrightarrow{n\rightarrow+\infty}0\implies\mathcal{E}_{\mathsf{L}_{\mathrm{ def}}}(h_{n})-\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}^{*}(\mathcal{H})\xrightarrow{ n\rightarrow+\infty}0.\]

In the following result, we show that our two-stage surrogate losses are realizable \(\mathcal{H}\)-consistent. Combined with their Bayes-consistency properties, which have already been established in Section 3.2, we effectively find surrogate losses that are both Bayes-consistent and realizable consistent in the multi-expert setting, including the single-expert setting as a special case. For simplicity, here, we study the case where \(\ell_{1}=\ell_{2}=\ell_{\mathrm{log}}\), a similar proof holds for other choices of \(\ell_{1}\) and \(\ell_{2}\) defined in Table 1. The proof is included in Appendix F.

**Theorem 5** (**Realizable \(\mathcal{H}\)-consistency for score-based two-stage surrogates**).: _Assume that \(\mathcal{H}\) is closed under scaling and \(c_{j}(x,y)=\beta_{j},\forall(x,y)\in\mathcal{X}\times\mathcal{Y}\). Let \(\ell_{1}\) and \(\ell_{2}\) be the logistic loss. Let \(\hat{h}_{p}\) be the minimizer of \(\mathcal{E}_{\ell_{1}}\) and \(\hat{h}_{d}\) be the minimizer of \(\mathcal{E}_{\mathsf{L}_{\hat{h}_{p}}}\) such that \(\mathcal{E}_{\mathsf{L}_{\hat{h}_{p}}}(\hat{h}_{d})=\min_{h}\mathcal{E}_{ \mathsf{L}_{h_{p}}}(h_{d})\). Then, the following equality holds for any (\(\mathcal{H}\), \(\mathcal{H}\)) -realizable distribution,_

\[\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}(\hat{h})=0,\;\text{where}\;\hat{h}=( \hat{h}_{p},\hat{h}_{d}).\]

Theorem 5 suggests that when the estimation error of the first-stage surrogate loss, \(\mathcal{E}_{\ell_{1}}(h_{p}^{n})-\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H}_{p}) \xrightarrow{n\rightarrow+\infty}0\), and the estimation error of the second-stage surrogate loss, \(\mathcal{E}_{\mathsf{L}_{h_{p}}}(h_{d}^{n})-\mathcal{E}_{\mathsf{L}_{h_{p}}}^{* }(\mathcal{H}_{d})\xrightarrow{n\rightarrow+\infty}0\), the estimation error of the deferral loss, \(\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}(h^{n})-\mathcal{E}_{\mathsf{L}_{ \mathrm{def}}}^{*}(\mathcal{H})\xrightarrow{n\rightarrow+\infty}0\). This result demonstrates that our two-stage surrogate losses are not only Bayes-consistent, but also realizable \(\mathcal{H}\)-consistent when only the inference cost (\(\beta_{j}\)) exists.

## 4 Predictor-rejector setting

The results of the previous sections were all given for the score-based setting. We note that another popular setting in learning with deferral/abstention is the _predictor-rejector setting_(Cortes et al., 2016, 2023), where the deferral corresponds to a separate function \(\mathcal{R}\) instead of extra scores. For completeness, we introduce this setting as well. Here too, we design a family of two-stage surrogate losses benefiting from both (\(\mathcal{H},\mathcal{R}\))-consistency bounds and realizable consistency. For simplicity, we overload the notation as with score-based setting based on the context.

Let \(\mathcal{H}\) be a hypothesis set of prediction functions mapping from \(\mathcal{X}\times\mathcal{Y}\) to \(\mathbb{R}\). The label predicted for \(x\in\mathcal{X}\) using a hypothesis \(h\in\mathcal{H}\) is denoted by \(\mathsf{h}(x)\) and defined as one with the highest score,\(h(x)=\operatorname*{argmax}_{y\in\mathcal{H}}h(x,y)\), with an arbitrary but fixed deterministic strategy for breaking ties. Let \(\mathcal{R}\) be a family of _deforming_ functions mapping from \(\mathcal{X}\) to \(\mathbb{R}^{n_{e}}\), where \(n_{e}\) is the number of experts. A deferral \(r=(r_{1},\ldots,r_{n_{e}})\in\mathcal{R}\) is used to defer the prediction on input \(x\) to the \(j\)th expert \(h_{j}\) if \(r_{j}(x)\leq 0\) and \(r_{j}(x)<\min_{i=1,i\neq j}r_{i}(x)\), in which case a cost \(c_{j}(x,y)=1-\bar{c}_{j}(x,y)\in[1-\bar{c}_{j},1-\underline{c}_{j}]\) is incurred with \(0<\underline{c}_{j}\leq\overline{c}_{j}\leq 1\). A natural choice of the cost is \(c_{j}(x,y)=\alpha_{j}\mathds{1}_{h_{j}(x)*y}+\beta_{j}\), where \(\alpha_{j},\beta_{j}>0\) and \(h_{j}\) is the prediction of the \(j\)th expert. The \(\beta_{j}\) in the second term corresponds to the inference cost incurred by expert \(h_{j}\). Let \(r_{0}=0\) and define \(r(x)=0\) if \(r_{0}(x)<\min_{j\in[n_{e}]}r_{j}(x)\); otherwise, \(r(x)=\operatorname*{argmin}_{j\in[n_{e}]}r_{j}(x)\), with an arbitrary but fixed deterministic strategy for breaking ties. The _learning to defer loss_\(\mathsf{L}_{\mathrm{def}}\) with \(n_{e}\) experts is defined as follows for any \((h,r)\in\mathcal{H}\times\mathcal{R}\) and \((x,y)\in\mathcal{X}\times\mathcal{H}\):

\[\mathsf{L}_{\mathrm{def}}(h,r,x,y)=\mathds{1}_{h(x)*y}\mathds{1}_{r(x)=0}+ \sum_{j=1}^{n_{e}}c_{j}(x,y)\mathds{1}_{r(x)=j}. \tag{4}\]

Given a distribution \(\mathcal{D}\) over \(\mathcal{X}\times\mathcal{Y}\), we will denote by \(\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}(h,r)\) the expected deferral loss of a predictor \(h\in\mathcal{H}\) and a deferral \(r\in\mathcal{R}\), \(\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}(h,r)=\mathbb{E}_{(x,y)\sim\mathcal{D}} [\mathsf{L}_{\mathrm{def}}(h,r,x,y)]\), and by \(\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}^{*}(\mathcal{H},\mathcal{R})=\inf_{h \in\mathcal{H},r\in\mathcal{R}}\mathcal{E}_{\mathsf{L}_{\mathrm{def}}}(h,r)\) its infimum or best-in class expected loss. We will adopt similar definitions for other loss functions. We denote by \(\mathcal{M}_{\mathsf{L}}(\mathcal{H},\mathcal{R})=\mathcal{E}_{\mathsf{L}}^{* }(\mathcal{H},\mathcal{R})-\mathbb{E}_{x}\bigl{[}\inf_{h\in\mathcal{H},r\in \mathcal{R}}\mathbb{E}_{y|x}[\mathsf{L}(h,r,x,y)]\bigr{]}\) the minimizability gap for hypothesis sets (\(\mathcal{H}\),\(\mathcal{R}\)) and a loss function \(\mathsf{L}\).

Let \(\ell_{1}\) be a surrogate loss for standard multi-class classification with \(n\) classes. We consider the following two-stage scenario: in the first stage, a predictor \(h\) is learned using the surrogate loss \(\ell_{1}\); in the second stage, \(r\) is learned using a surrogate loss \(\mathsf{L}_{h}\) that depends on the prediction function \(h\) learned in the first stage.

To any \(r\in\mathcal{R}\), we associate a hypothesis \(\overline{r}\) defined over \((n_{e}+1)\) classes \(\{0,1,\ldots,n_{e}\}\) by \(\overline{r}(x,0)=0\), that is zero scoring function, and \(\overline{r}(x,j)=-r_{j}(x)\) for \(j\in[n_{e}]\). We can then define our suggested surrogate loss for the second stage:

\[\mathsf{L}_{h}(r,x,y)=\mathds{1}_{h(x)*y}\ell_{2}(\overline{r},x,0)+\sum_{j=1 }^{n_{e}}\overline{c}_{j}(x,y)\ell_{2}(\overline{r},x,j). \tag{5}\]

Here, \(\ell_{2}(\overline{r},x,j)\) is a surrogate loss for standard multi-class classification with \((n_{e}+1)\) categories \(\{0,1,\ldots,n_{e}\}\). Intuitively, the indicator term \(\mathds{1}_{r(x)*j}\) in the deferral loss penalizes \(r_{j}(x)\) when it has a large value. However, a standard surrogate loss \(\ell_{2}(\overline{r},x,j)\) such as the logistic loss penalizes \(\overline{r}(x,j)\) when it has a small value. This is why we use a negative sign in the definition of \(\overline{r}\) to maintain consistency between the definitions of \(\mathsf{L}_{h}\) and \(\mathsf{L}_{\mathrm{def}}\). In Table 3, we present a summary of examples of such second-stage surrogate losses, where \(\ell_{2}\) is selected from common surrogate losses in standard multi-class classification defined in Table 1. A detailed derivation is presented in Appendix C.

From the point of view of the second stage, we will denote by \(\overline{\mathcal{R}}\) the family of hypotheses \(\overline{r}\colon\mathcal{X}\times\{0,1,\ldots,n_{e}\}\to\mathbb{R}\) whose first scoring function, \(\overline{r}(\cdot,0)\), is zero function and will not be learned in the second stage. We will provide strong guarantees for two-stage surrogate losses, provided that the first-stage loss function \(\ell_{1}\) admits an \(\mathcal{H}\)-consistency bound, and the second-stage loss function \(\ell_{2}\) admits an \(\overline{\mathcal{R}}\)-consistency bound.

**Theorem 6** ((\(\mathcal{H},\mathcal{R}\))-consistency bounds for predictor-rejector two-stage surrogates).: _Assume that \(\ell_{1}\) admits an \(\mathcal{H}\)-consistency bound and \(\ell_{2}\) admits an \(\overline{\mathcal{R}}\)-consistency bound with respect to the multi-class zero-one classification loss \(\ell_{0-1}\) respectively. Thus, there are non-decreasing concave

\begin{table}
\begin{tabular}{l l} \hline \hline \(\ell_{2}\) & \(\mathsf{L}_{h_{p}}\) \\ \hline \(\ell_{\mathrm{exp}}\) & \(\mathds{1}_{h(x)*y}\sum_{i=1}^{n_{e}}e^{-r_{i}(x)}+\sum_{j=1}^{n_{e}}\bar{c}_ {j}(x,y)\bigl{[}\sum_{i=1,i\neq j}^{n_{e}}e^{r_{j}(x)-r_{i}(x)}+e^{r_{j}(x)} \bigr{]}\) \\ \(\ell_{\mathrm{log}}\) & \(-\mathds{1}_{h(x)*y}\log\biggl{(}\frac{1}{1+\sum_{i=1}^{n_{e}}e^{-r_{i}(x)}} \biggr{)}-\sum_{j=1}^{n_{e}}\bar{c}_{j}(x,y)\log\biggl{(}\frac{e^{-r_{j}(x)}} {1+\sum_{i=1}^{n_{e}}e^{-r_{i}(x)}}\biggr{)}\) \\ \(\ell_{\mathrm{gce}}\) & \(\mathds{1}_{h(x)*y}\frac{1}{\alpha}\biggl{[}1-\biggl{[}\frac{1}{1+\sum_{i=1} ^{n_{e}}e^{-r_{i}(x)}}\biggr{]}^{\alpha}\biggr{]}+\sum_{j=1}^{n_{e}}\bar{c}_ {j}(x,y)\frac{1}{\alpha}\biggl{[}1-\biggl{[}\frac{e^{-r_{j}(x)}}{1+\sum_{i=1} ^{n_{e}}e^{-r_{i}(x)}}\biggr{]}^{\alpha}\biggr{]}\) \\ \(\ell_{\mathrm{mae}}\) & \(\mathds{1}_{h(x)*y}\biggl{[}1-\frac{1}{1+\sum_{i=1}^{n_{e}}e^{-r_{i}(x)}} \biggr{]}+\sum_{j=1}^{n_{e}}\bar{c}_{j}(x,y)\biggl{[}1-\frac{e^{-r_{j}(x)}}{1+ \sum_{i=1}^{n_{e}}e^{-r_{i}(x)}}\biggr{]}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Examples for predictor-rejector second-stage surrogate losses (5).

functions \(\Gamma_{1}\) and \(\Gamma_{2}\) such that, for all \(h\in\mathcal{H}\) and \(\overline{r}\in\mathcal{R}\), we have_

\[\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H })+\mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\Gamma_{1}\big{(}\mathcal{E}_{\ell_ {1}}(h)-\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H})+\mathcal{M}_{\ell_{1}}( \mathcal{H})\] \[\mathcal{E}_{\ell_{0-1}}(\overline{r})-\mathcal{E}_{\ell_{0-1}}^{ *}(\overline{\mathcal{H}})+\mathcal{M}_{\ell_{0-1}}(\overline{\mathcal{H}}) \leq\Gamma_{2}\big{(}\mathcal{E}_{\ell_{2}}(\overline{r})-\mathcal{E}_{\ell_ {2}}^{*}(\overline{\mathcal{H}})+\mathcal{M}_{\ell_{2}}(\overline{\mathcal{H}} )\big{)}.\]

_Then, the following holds for all \(h\in\mathcal{H}\) and \(r\in\mathcal{R}\):_

\[\mathcal{E}_{\mathrm{L}_{\mathrm{def}}}(h,r)-\mathcal{E}_{ \mathrm{L}_{\mathrm{def}}}^{*}(\mathcal{H},\mathcal{R})+\mathcal{M}_{\mathrm{ L}_{\mathrm{def}}}(\mathcal{H},\mathcal{R})\] \[\leq\Gamma_{1}\big{(}\mathcal{E}_{\ell_{1}}(h)-\mathcal{E}_{\ell_ {1}}^{*}(\mathcal{H})+\mathcal{M}_{\ell_{1}}(\mathcal{H})\big{)}+\left(1+ \sum_{j=1}^{n_{\mathrm{s}}}\overline{c}_{j}\right)\Gamma_{2}\bigg{(}\frac{ \mathcal{E}_{\mathrm{L}_{h}}(r)-\mathcal{E}_{\mathrm{L}_{h}}^{*}(\mathcal{R})+ \mathcal{M}_{\mathrm{L}_{h}}(\mathcal{R})}{\sum_{j=1}^{n_{\mathrm{s}}} \overline{c}_{j}}\bigg{)},\]

_where the constant factors \(\left(1+\sum_{j=1}^{n_{\mathrm{s}}}\overline{c}_{j}\right)\) and \(\frac{1}{\sum_{j=1}^{n_{\mathrm{s}}}\overline{c}_{j}}\) can be removed when \(\Gamma_{2}\) is linear._

As with the score-based setting, a specific instance of Theorem 6 holds for the case where \(\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H})=\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H }_{\mathrm{all}})\) and \(\mathcal{E}_{\mathrm{L}_{h}}^{*}(\mathcal{R})=\mathcal{E}_{\mathrm{L}_{h}}^{*} (\mathcal{R}_{\mathrm{all}})\), ensuring that the Bayes-error coincides with the best-in-class error and, consequently, \(\mathcal{M}_{\ell_{1}}(\mathcal{H})=\mathcal{M}_{\mathrm{L}_{h}}(\mathcal{R})=0\). In these cases, when the estimation error of the first-stage surrogate loss, \(\mathcal{E}_{\ell_{1}}(h)-\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H})\), is reduced to \(\epsilon_{1}\), and the estimation error of the second-stage surrogate loss, \(\mathcal{E}_{\mathrm{L}_{h}}(r)-\mathcal{E}_{\mathrm{L}_{h}}^{*}(\mathcal{R})\), is reduced to \(\epsilon_{2}\), the estimation error of the deferral loss, \(\mathcal{E}_{\mathrm{L}_{\mathrm{def}}}(h,r)-\mathcal{E}_{\mathrm{L}_{\mathrm{ def}}}^{*}(\mathcal{H},\mathcal{R})\), is upper bounded by

\[\Gamma_{1}(\epsilon_{1})+\left(1+\sum_{j=1}^{n_{\mathrm{s}}}\overline{c}_{j} \right)\Gamma_{2}\bigg{(}\frac{\epsilon_{2}}{\sum_{j=1}^{n_{\mathrm{s}}} \mathcal{E}_{j}}\bigg{)}.\]

Next, we show that our two-stage surrogate losses are realizable \((\mathcal{H},\mathcal{R})\)-consistent. We say that the distribution is \((\mathcal{H},\mathcal{R})\)_-realizable_, if there exists a zero error solution \((h^{*},r^{*})\in\mathcal{H}\times\mathcal{R}\) with \(\mathcal{E}_{\mathrm{L}_{\mathrm{def}}}(h^{*},r^{*})=0\).

**Theorem 7** (**Realizable \((\mathcal{H},\mathcal{R})\)-consistency for predictor-rejector two-stage surrogates)**.: _Assume that \(\mathcal{H}\) and \(\mathcal{R}\) is closed under scaling and \(c_{j}(x,y)=\beta_{j},\forall(x,y)\in\mathcal{X}\times\mathcal{Y}\). Let \(\ell_{1}\) and \(\ell_{2}\) be the logistic loss. Let \(\hat{h}\) be the minimizer of \(\mathcal{E}_{\ell_{1}}\) and \(\hat{r}\) be the minimizer of \(\mathcal{E}_{\mathrm{L}_{\hat{h}}}\). Then, the following holds for any \((\mathcal{H},\mathcal{R})\) -realizable distribution,_

\[\mathcal{E}_{\mathrm{L}_{\mathrm{def}}}(\hat{h},\hat{r})=0.\]

The proof is included in Appendix H. Theorem 7 suggests that the two-stage surrogate loss is realizable consistent: when the estimation error of the first-stage surrogate loss \(\mathcal{E}_{\ell_{1}}(h_{n})-\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H})\xrightarrow {n\rightarrow\infty}0\), and the estimation error of the second-stage surrogate loss \(\mathcal{E}_{\mathrm{L}_{h}}(r_{n})-\mathcal{E}_{\mathrm{L}_{h}}^{*}(\mathcal{ R})\xrightarrow{n\rightarrow\infty}0\), the estimation error of the deferral loss, \(\mathcal{E}_{\mathrm{L}_{\mathrm{def}}}(h_{n},r_{n})-\mathcal{E}_{\mathrm{L}_{ \mathrm{def}}}^{*}(\mathcal{H},\mathcal{R})\xrightarrow{n\rightarrow\infty}0\). By Theorem 6 and Theorem 7, in the predictor-rejector setting, we also effectively find both Bayes-consistent and realizable consistent surrogate losses with multiple experts when only the inference cost \((\beta_{j})\) exists.

Note that while Sections 3 and 4 both propose new two-stage algorithms based on \(\mathcal{H}\)-consistent surrogate losses, they differ in an important way. Section 3 learns with deferral in a score-based framework, where deferral is associated with extra scores. In contrast, Section 4 learns with deferral in a predictor-rejector setting, where deferral corresponds to a separate function. These represent two distinct learning frameworks that have been studied in the literature. Deriving consistent surrogate losses in the predictor-rejector setting has historically been challenging for traditional single-stage scenarios, leading many to opt for the score-based approach.

We should also highlight that our \(\mathcal{H}\)-consistency bounds in Theorems 1 and 6 can be used to derive finite sample estimation bounds for the minimizer of the surrogate loss over a hypothesis set \(\mathcal{H}\). This is achieved by upper bounding the estimation error of the minimizer of the surrogate loss using standard Rademacher complexity bounds (see [14]).

## 5 Experiments

In this section, we report the results of our experiments on CIFAR-10 [13] and SVHN [13] datasets to test the effectiveness of our proposed algorithms for two-stagelearning to defer with multiple experts. We evaluated the overall accuracy of the learned pairs of predictor and deferral model across different scenarios involving varying the number of experts, where the predictor is pre-learned in the first stage and the deferral is subsequently learned using our proposed surrogate loss. We find that as the number of experts increases, the overall accuracy of the learned pairs also increases, in both scenarios with zero and non-zero base costs. This observation highlights the significance of using a multiple expert framework in our approach and the effectiveness of our surrogate loss within the framework.

We used ResNet architectures (He et al., 2016) for the prediction model, the deferral model and expert models. More precisely, we used ResNet-\(4\) for both the predictor and the deferral. We adopted three expert models: ResNet-10, ResNet-16, ResNet-\(28\) with increasing capacity. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a batch size of \(128\) and weight decay \(1\times 10^{-4}\). Training was run for \(15\) epochs for SVHN and \(50\) epochs for CIFAR-10 with the default learning rate. No data augmentation was used in our experiments. We used our two-stage surrogate loss (3) with the logistic loss \(\ell=\ell_{\log}\) to train the deferral model ResNet-\(4\), with a pre-learned predictor ResNet-\(4\) trained using logistic loss. A check mark indicates the presence of a base cost in the cost function, whereas a cross mark signifies its absence. We first set the cost function to be \(\mathds{1}_{h_{j}(x)=y}\) without a base cost. Next, for the experimental results shown in the last two row of Table 4, we chose base costs \(\beta_{j}\) associated with each expert model as: \(0.1\), \(0.12\), \(0.14\) increasing with model capacity for SVHN and \(0.3\), \(0.32\), \(0.34\) increasing with model capacity for CIFAR-10. A base cost value that is close to the misclassification loss can strike a balance between improving accuracy and maintaining the ratio of deferral. We observed that other neighboring values lead to similar results. Note that the accuracy here refers to the overall accuracy of the learned pairs of predictor and deferral model. It is related to the deferral loss. Specifically, in the absence of the base cost, the accuracy aligns precisely with one minus the expected deferral loss. The results of Table 4 demonstrate the effectiveness of our proposed algorithms for two-stage learning to defer with multiple experts.

To the best of our knowledge, our study pioneers the exploration of a two-stage learning approach for deferral, a framework that is essential in numerous practical applications. Thus, we are unaware of any established baselines within this context.

It is important to underscore the differences between our learning scenario and those presented in (Okati et al., 2021; Narasimhan et al., 2022). While both of them involve two phases, their methodologies are considerably different from ours. Okati et al. (2021) required conditional probabilities paired with loss estimates from the expert--a component not available in our framework, as emphasized by Mozannar et al. (2023). On the other hand, Narasimhan et al. (2022) proposed a post-hoc correction for single-stage learning to defer surrogate losses. This approach, however, is not applicable to a pre-trained predictor from the standard multi-class classification. In contrast, our work focuses on enhancing the pre-trained predictor within the standard framework.

A limitation of our study is that the cost function used within the deferral loss is not fixed, and is typically determined through cross-validation in practice. There exists potential to introduce a principled method for selecting the cost function, which we have reserved for future research.

## 6 Conclusion

We introduced a novel family of surrogate loss functions and algorithms for a crucial two-stage learning to defer approach with multiple experts. We proved that these surrogate losses are supported by \(\mathcal{H}\)-consistency bounds and established their realizable \(\mathcal{H}\)-consistency properties for a constant cost function. This work paves the way for comparing different surrogate losses and cost functions within our framework. Further exploration, both theoretically and empirically, holds the potential to identify optimal choices for these quantities across diverse tasks.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Dataset & Base cost & Base model & Single expert & Two experts & Three experts \\ \hline SVHN & ✗ & 91.12 & 91.85 \(\pm\) 0.01\% & 92.77 \(\pm\) 0.02\% & 93.30 \(\pm\) 0.02\% \\ CIFAR-10 & ✗ & 70.56 & 72.63 \(\pm\) 0.20\% & 75.84 \(\pm\) 0.35\% & 77.68 \(\pm\) 0.07\% \\ SVHN & ✓ & 91.12 & 91.66 \(\pm\) 0.01\% & 92.05 \(\pm\) 0.10\% & 92.19 \(\pm\) 0.03\% \\ CIFAR-10 & ✓ & 70.56 & 71.73 \(\pm\) 0.06\% & 72.31 \(\pm\) 0.31\% & 72.42 \(\pm\) 0.12\% \\ \hline \hline \end{tabular}
\end{table}
Table 4: Accuracy of deferral with multiple experts: mean ± standard deviation over three runs.

## References

* Acar et al. (2020) D. A. E. Acar, A. Gangrade, and V. Saligrama. Budget learning via bracketing. In _International Conference on Artificial Intelligence and Statistics_, pages 4109-4119, 2020.
* Awasthi et al. (2021a) P. Awasthi, N. Frank, A. Mao, M. Mohri, and Y. Zhong. Calibration and consistency of adversarial surrogate losses. In _Advances in Neural Information Processing Systems_, 2021a.
* Awasthi et al. (2021b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. A finer calibration analysis for adversarial robustness. _arXiv preprint arXiv:2105.01550_, 2021b.
* Awasthi et al. (2022a) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Multi-class \(\mathcal{H}\)-consistency bounds. In _Advances in neural information processing systems_, 2022a.
* Awasthi et al. (2022b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. \(\mathcal{H}\)-consistency bounds for surrogate loss minimizers. In _International Conference on Machine Learning_, 2022b.
* Awasthi et al. (2023a) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for adversarial robustness. In _International Conference on Artificial Intelligence and Statistics_, pages 10077-10094, 2023a.
* Awasthi et al. (2023b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. DC-programming for neural network optimizations. _Journal of Global Optimization_, 2023b.
* Bansal et al. (2021) G. Bansal, B. Nushi, E. Kamar, E. Horvitz, and D. S. Weld. Is the most accurate ai the best teammate? optimizing ai for teamwork. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 11405-11414, 2021.
* Bartlett and Wegkamp (2008) P. L. Bartlett and M. H. Wegkamp. Classification with a reject option using a hinge loss. _Journal of Machine Learning Research_, 9(8), 2008.
* Bartlett et al. (2006) P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. _Journal of the American Statistical Association_, 101(473):138-156, 2006.
* Benz and Rodriguez (2022) N. L. C. Benz and M. G. Rodriguez. Counterfactual inference of second opinions. In _Uncertainty in Artificial Intelligence_, pages 453-463. PMLR, 2022.
* Bubeck et al. (2023) S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* Cao et al. (2022) Y. Cao, T. Cai, L. Feng, L. Gu, J. Gu, B. An, G. Niu, and M. Sugiyama. Generalizing consistent multi-class classification with rejection to be compatible with arbitrary losses. In _Advances in neural information processing systems_, 2022.
* Charoenphakdee et al. (2021) N. Charoenphakdee, Z. Cui, Y. Zhang, and M. Sugiyama. Classification with rejection based on cost-sensitive classification. In _International Conference on Machine Learning_, pages 1507-1517, 2021.
* Charusaie et al. (2022) M.-A. Charusaie, H. Mozannar, D. Sontag, and S. Samadi. Sample efficient learning of predictors that complement humans. In _International Conference on Machine Learning_, pages 2972-3005, 2022.
* Chow (1957) C. Chow. An optimum character recognition system using decision function. _IEEE T. C._, 1957.
* Chow (1970) C. Chow. On optimum recognition error and reject tradeoff. _IEEE Transactions on information theory_, 16(1):41-46, 1970.
* Cortes et al. (2016a) C. Cortes, G. DeSalvo, and M. Mohri. Learning with rejection. In _International Conference on Algorithmic Learning Theory_, pages 67-82, 2016a.
* Cortes et al. (2016b) C. Cortes, G. DeSalvo, and M. Mohri. Boosting with abstention. In _Advances in Neural Information Processing Systems_, pages 1660-1668, 2016b.
* Cortes et al. (2023) C. Cortes, G. DeSalvo, and M. Mohri. Theory and algorithms for learning with rejection in binary classification. _Annals of Mathematics and Artificial Intelligence_, to appear, 2023.
* D'Auria et al. (2016)A. De, P. Koley, N. Ganguly, and M. Gomez-Rodriguez. Regression under human assistance. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 2611-2620, 2020.
* El-Yaniv et al. (2010) R. El-Yaniv et al. On the foundations of noise-free selective classification. _Journal of Machine Learning Research_, 11(5), 2010.
* Gangrade et al. (2021) A. Gangrade, A. Kag, and V. Saligrama. Selective classification via one-sided prediction. In _International Conference on Artificial Intelligence and Statistics_, pages 2179-2187, 2021.
* Gao et al. (2021) R. Gao, M. Saar-Tsechansky, M. De-Arteaga, L. Han, M. K. Lee, and M. Lease. Human-ai collaboration with bandit feedback. _arXiv preprint arXiv:2105.10614_, 2021.
* Geifman and El-Yaniv (2017) Y. Geifman and R. El-Yaniv. Selective classification for deep neural networks. In _Advances in neural information processing systems_, 2017.
* Geifman and El-Yaniv (2019) Y. Geifman and R. El-Yaniv. Selectivenet: A deep neural network with an integrated reject option. In _International conference on machine learning_, pages 2151-2159, 2019.
* Grandvalet et al. (2008) Y. Grandvalet, A. Rakotomamonjy, J. Keshet, and S. Canu. Support vector machines with a reject option. In _Advances in neural information processing systems_, 2008.
* He et al. (2016) K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hemmer et al. (2022) P. Hemmer, S. Schellhammer, M. Vossing, J. Jakubik, and G. Satzger. Forming effective human-ai teams: Building machine learning models that complement the capabilities of multiple experts. _arXiv preprint arXiv:2206.07948_, 2022.
* Hemmer et al. (2023) P. Hemmer, L. Thede, M. Vossing, J. Jakubik, and N. Kuhl. Learning to defer with limited expert predictions. _arXiv preprint arXiv:2304.07306_, 2023.
* Herbei and Wegkamp (2005) R. Herbei and M. Wegkamp. Classification with reject option. _Can. J. Stat._, 2005.
* Joshi et al. (2021) S. Joshi, S. Parbhoo, and F. Doshi-Velez. Pre-emptive learning-to-defer for sequential medical decision-making under uncertainty. _arXiv preprint arXiv:2109.06312_, 2021.
* Kalai et al. (2012) A. T. Kalai, V. Kanade, and Y. Mansour. Reliable agnostic learning. _Journal of Computer and System Sciences_, 78(5):1481-1495, 2012.
* Kamar et al. (2012) E. Kamar, S. Hacker, and E. Horvitz. Combining human and machine intelligence in large-scale crowdsourcing. In _AAMAS_, pages 467-474, 2012.
* Kerrigan et al. (2021) G. Kerrigan, P. Smyth, and M. Steyvers. Combining human predictions with model probabilities via confusion matrices and calibration. _Advances in Neural Information Processing Systems_, 34:4421-4434, 2021.
* Keswani et al. (2021) V. Keswani, M. Lease, and K. Kenthapadi. Towards unbiased and accurate deferral to multiple experts. In _Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society_, pages 154-165, 2021.
* Kingma and Ba (2014) D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kleinberg et al. (2018) J. Kleinberg, H. Lakkaraju, J. Leskovec, J. Ludwig, and S. Mullainathan. Human decisions and machine predictions. _The quarterly journal of economics_, 133(1):237-293, 2018.
* Krizhevsky (2009) A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Toronto University, 2009.
* Kuznetsov et al. (2014) V. Kuznetsov, M. Mohri, and U. Syed. Multi-class deep boosting. In _Advances in Neural Information Processing Systems_, pages 2501-2509, 2014.
* Liu et al. (2022) J. Liu, B. Gallego, and S. Barbieri. Incorporating uncertainty in learning to defer algorithms for safe computer-aided diagnosis. _Scientific reports_, 12(1):1762, 2022.
* Liu et al. (2021)P. Long and R. Servedio. Consistency versus realizable H-consistency for multiclass classification. In _International Conference on Machine Learning_, pages 801-809, 2013.
* Madras et al. (2018) D. Madras, E. Creager, T. Pitassi, and R. Zemel. Learning adversarially fair and transferable representations. _arXiv preprint arXiv:1802.06309_, 2018.
* Mao et al. (2023a) A. Mao, M. Mohri, and Y. Zhong. H-consistency bounds: Characterization and extensions. In _Advances in Neural Information Processing Systems_, 2023a.
* Mao et al. (2023b) A. Mao, M. Mohri, and Y. Zhong. Principled approaches for learning to defer with multiple experts. _arXiv preprint arXiv:2310.14774_, 2023b.
* Mao et al. (2023c) A. Mao, M. Mohri, and Y. Zhong. Predictor-rejector multi-class abstention: Theoretical analysis and algorithms. _arXiv preprint arXiv:2310.14772_, 2023c.
* Mao et al. (2023d) A. Mao, M. Mohri, and Y. Zhong. H-consistency bounds for pairwise misranking loss surrogates. In _International conference on Machine learning_, 2023d.
* Mao et al. (2023e) A. Mao, M. Mohri, and Y. Zhong. Ranking with abstention. In _ICML 2023 Workshop The Many Facets of Preference-Based Learning_, 2023e.
* Mao et al. (2023f) A. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for score-based multi-class abstention. _arXiv preprint arXiv:2310.14770_, 2023f.
* Mao et al. (2023g) A. Mao, M. Mohri, and Y. Zhong. Structured prediction with stronger consistency guarantees. In _Advances in Neural Information Processing Systems_, 2023g.
* Mao et al. (2023h) A. Mao, M. Mohri, and Y. Zhong. Cross-entropy loss functions: Theoretical analysis and applications. In _International Conference on Machine Learning_, 2023h.
* Mohri et al. (2023) C. Mohri, D. Andor, E. Choi, M. Collins, A. Mao, and Y. Zhong. Learning to reject with a fixed predictor: Application to decontextualization. _arXiv preprint arXiv:2301.09044_, 2023.
* Mohri et al. (2018) M. Mohri, A. Rostamizadeh, and A. Talwalkar. _Foundations of Machine Learning_. MIT Press, second edition, 2018.
* Mozannar and Sontag (2020) H. Mozannar and D. Sontag. Consistent estimators for learning to defer to an expert. In _International Conference on Machine Learning_, pages 7076-7087, 2020.
* Mozannar et al. (2022) H. Mozannar, A. Satyanarayan, and D. Sontag. Teaching humans when to defer to a classifier via exemplars. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 5323-5331, 2022.
* Mozannar et al. (2023) H. Mozannar, H. Lang, D. Wei, P. Sattigeri, S. Das, and D. Sontag. Who should predict? exact algorithms for learning to defer to humans. In _International Conference on Artificial Intelligence and Statistics_, pages 10520-10545, 2023.
* Narasimhan et al. (2022) H. Narasimhan, W. Jitkrittum, A. K. Menon, A. S. Rawat, and S. Kumar. Post-hoc estimators for learning to defer to an expert. In _Advances in Neural Information Processing Systems_, 2022.
* Narasimhan et al. (2023) H. Narasimhan, A. K. Menon, W. Jitkrittum, and S. Kumar. Learning to reject meets ood detection: Are all abstentions created equal? _arXiv preprint arXiv:2301.12386_, 2023.
* Netzer et al. (2011) Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In _Advances in Neural Information Processing Systems_, 2011.
* Ni et al. (2019) C. Ni, N. Charoenphakdee, J. Honda, and M. Sugiyama. On the calibration of multiclass classification with rejection. In _Advances in Neural Information Processing Systems_, pages 2582-2592, 2019.
* Okati et al. (2021) N. Okati, A. De, and M. Rodriguez. Differentiable learning under triage. _Advances in Neural Information Processing Systems_, 34:9140-9151, 2021.
* Pradier et al. (2021) M. F. Pradier, J. Zazo, S. Parbhoo, R. H. Perlis, M. Zazzi, and F. Doshi-Velez. Preferential mixture-of-experts: Interpretable models that rely on human expertise as much as possible. _AMIA Summits on Translational Science Proceedings_, 2021:525, 2021.
* Pradier et al. (2021)M. Raghu, K. Blumer, G. Corrado, J. Kleinberg, Z. Obermeyer, and S. Mullainathan. The algorithmic automation problem: Prediction, triage, and human effort. _arXiv preprint arXiv:1903.12220_, 2019.
* Raman and Yee (2021) N. Raman and M. Yee. Improving learning-to-defer algorithms through fine-tuning. _arXiv preprint arXiv:2112.10768_, 2021.
* Ramaswamy et al. (2018) H. G. Ramaswamy, A. Tewari, and S. Agarwal. Consistent algorithms for multiclass classification with an abstain option. _Electronic Journal of Statistics_, 12(1):530-554, 2018.
* Steinwart (2007) I. Steinwart. How to compare different loss functions and their risks. _Constructive Approximation_, 26(2):225-287, 2007.
* Straitouri et al. (2021) E. Straitouri, A. Singla, V. B. Meresht, and M. Gomez-Rodriguez. Reinforcement learning under algorithmic triage. _arXiv preprint arXiv:2109.11328_, 2021.
* Straitouri et al. (2022) E. Straitouri, L. Wang, N. Okati, and M. G. Rodriguez. Provably improving expert predictions with conformal prediction. _arXiv preprint arXiv:2201.12006_, 2022.
* Tan et al. (2018) S. Tan, J. Adebayo, K. Inkpen, and E. Kamar. Investigating human+ machine complementarity for recidivism predictions. _arXiv preprint arXiv:1808.09123_, 2018.
* Verma and Nalisnick (2022) R. Verma and E. Nalisnick. Calibrated learning to defer with one-vs-all classifiers. In _International Conference on Machine Learning_, pages 22184-22202, 2022.
* Verma et al. (2023) R. Verma, D. Barrejon, and E. Nalisnick. Learning to defer to multiple experts: Consistent surrogate losses, confidence calibration, and conformal ensembles. In _International Conference on Artificial Intelligence and Statistics_, pages 11415-11434, 2023.
* Wei et al. (2022) J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus. Emergent abilities of large language models. _CoRR_, abs/2206.07682, 2022.
* Wiener and El-Yaniv (2011) Y. Wiener and R. El-Yaniv. Agnostic selective classification. In _Advances in neural information processing systems_, 2011.
* Wilder et al. (2021) B. Wilder, E. Horvitz, and E. Kamar. Learning to complement humans. In _International Joint Conferences on Artificial Intelligence_, pages 1526-1533, 2021.
* Yuan and Wegkamp (2010) M. Yuan and M. Wegkamp. Classification methods with reject option based on convex risk minimization. _Journal of Machine Learning Research_, 11(1), 2010.
* Yuan and Wegkamp (2011) M. Yuan and M. Wegkamp. SVMs with a reject option. In _Bernoulli_, 2011.
* Zhang and Agarwal (2020) M. Zhang and S. Agarwal. Bayes consistency vs. H-consistency: The interplay between surrogate loss functions and the scoring function class. In _Advances in Neural Information Processing Systems_, 2020.
* Zhang (2004) T. Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. _The Annals of Statistics_, 32(1):56-85, 2004.
* Zhao et al. (2021) J. Zhao, M. Agrawal, P. Razavi, and D. Sontag. Directing human attention in event localization for clinical timeline creation. In _Machine Learning for Healthcare Conference_, pages 80-102, 2021.
* Zheng et al. (2023) C. Zheng, G. Wu, F. Bao, Y. Cao, C. Li, and J. Zhu. Revisiting discriminative vs. generative classifiers: Theory and implications. _arXiv preprint arXiv:2302.02334_, 2023.
* Ziyin et al. (2019) L. Ziyin, Z. Wang, P. P. Liang, R. Salakhutdinov, L.-P. Morency, and M. Ueda. Deep gamblers: Learning to abstain with portfolio theory. _arXiv preprint arXiv:1907.00208_, 2019.

###### Contents of Appendix

* A Related work
* B Examples of two-stage score-based surrogate losses
* C Examples of two-stage predictor-rejector surrogate losses
* D Proof of \(\mathcal{H}\)-consistency bounds for score-based two-stage surrogate losses (Theorem 1)
* E Proof of \(\overline{\mathcal{H}}\)-consistency bounds for standard surrogate loss functions (Theorem 3)
* E.1 Multinomial logistic loss
* E.2 Sum exponential loss
* E.3 Generalized cross-entropy loss
* E.4 Mean absolute error loss
* F Proof of realizable consistency for score-based two-stage surrogate losses (Theorem 5)
* G Proof of \((\mathcal{H},\mathcal{R})\)-consistency bounds for predictor-rejector two-stage surrogate losses (Theorem 6)
* H Proof of realizable consistency for predictor-rejector two-stage surrogate losses (Theorem 7)Related work

The scenario of _single-stage learning to defer_ has been extensively explored in previous research. The initial studies focused on the problem of abstention and introduced various approaches such as _confidence-based abstention_[Chow, 1957, 1970, Herbei and Wegkamp, 2005, Bartlett and Wegkamp, 2008, Grandvalet et al., 2008, Yuan and Wegkamp, 2010, 2011, Ramaswamy et al., 2018, Ni et al., 2019], _selective classification_[El-Yaniv et al., 2010, Wiener and El-Yaniv, 2011, Kalai et al., 2012, Geifman and El-Yaniv, 2017, 2019, Ziyin et al., 2019, Acar et al., 2020, Gangrade et al., 2021], a _predictor-rejector_ framework for abstention [Cortes et al., 2016, 2016, Charoenphakdee et al., 2021, Cortes et al., 2023, Mohri et al., 2023, Mao et al., 2023c], and a _score-based setting_ for abstention [Mozannar and Sontag, 2020, Raman and Yee, 2021, Liu et al., 2022, Verma and Nalisnick, 2022, Charusaie et al., 2022, Cao et al., 2022, Mao et al., 2023f, Verma et al., 2023, Mao et al., 2023b, Mozannar et al., 2023].

Another line of research is centered around the joint learning of prediction and deferral functions. Several publications by Madras et al. [2018], Raghu et al. [2019], Wilder et al. [2021], Pradier et al. [2021], Keswani et al. [2021] delve into this topic, considering single-stage learning to defer and its variants. Additionally, the concept of learning to defer has been explored in different scenarios, including combining human and machine predictions, investigating human preferences, regression problems, reinforcement learning, and more [Kamar et al., 2012, Tan et al., 2018, Kleinberg et al., 2018, Bansal et al., 2021, De et al., 2020, Straitouri et al., 2021, Zhao et al., 2021, Joshi et al., 2021, Gao et al., 2021, Mozannar et al., 2022, Hemmer et al., 2023, Narasimhan et al., 2023]. However, in practice, a predictor such as an LLM is already available and retraining one in conjunction with a deferral function could be prohibitively costly: depending on its size and the amount of data used, retraining could take several weeks or months. Thus, the single-stage learning to defer scenario and its solutions often do not align with the practical challenges encountered in real-world applications.

Alternative post-hoc methods have been proposed to address the learning to defer problem. Okati et al. [2021] proposed an iterative approach optimizing a predictor and a rejector over multiple epochs. Within each epoch, first the predictor is trained on points where its loss is lower than that of a human expert; second, the rejector is fitted to predict which of the predictor or the human expert has a lower loss. Narasimhan et al. [2022] suggested a post-hoc correction to the single-stage learning to defer surrogate losses, specifically the cost-sensitive softmax cross-entropy (CSS) surrogate loss in [Mozannar and Sontag, 2020] and the one-versus-all (OvA) surrogate loss in [Verma and Nalisnick, 2022] for cases where they suffer from underfitting. However, as with the single-stage learning to defer solutions, post-hoc approaches do not apply to scenarios where an existing predictor, pre-trained using a standard classification loss function such as cross-entropy, is already available.

A key criterion for surrogate losses in the scenario of learning to defer is Bayes-consistency (also known as consistency) [Zhang, 2004, Bartlett et al., 2006, Steinwart, 2007, Mohri et al., 2018]. This property guarantees that minimizing the surrogate loss over the family of measurable functions leads to the minimization of the deferral loss. The surrogate losses proposed in [Mozannar and Sontag, 2020, Verma and Nalisnick, 2022] have shown to be Bayes-consistent for deferral. However, Bayes-consistency is a property associated with the family of all measurable functions, which of course is considerably broader than the hypothesis sets typically used in learning algorithms, including linear hypothesis sets and the family of neural networks.

Instead, Long and Servedio [2013], Kuznetsov et al. [2014], Zhang and Agarwal [2020] proposed a notion of realizable \(\mathcal{H}\)-consistency, that is consistency associated with a specific hypothesis set in the realizable scenario. Mozannar et al. [2023] recently showed that existing Bayes-consistent surrogate losses in [Mozannar and Sontag, 2020, Verma and Nalisnick, 2022] are not realizable \(\mathcal{H}\)-consistent for learning with deferral, which can pose significant challenges when learning with a restricted hypothesis set \(\mathcal{H}\), even for simple linear models. Instead, they proposed a new surrogate loss that is realizable \(\mathcal{H}\)-consistent when \(\mathcal{H}\) is closed under scaling. However, they also observed that the loss function of Madras et al. [2018], which is not Bayes-consistent, is actually realizable \(\mathcal{H}\)-consistent. They acknowledged their inability to prove or disprove whether their proposed surrogate loss is Bayes-consistent. Consequently, it has remained an open problem to identify a surrogate loss that is both consistent and realizable-consistent.

In recent work, Verma et al. [2023] proposed the first Bayes-consistent surrogate losses in the scenario of learning to defer with _multiple experts_[Hemmer et al., 2022, Keswani et al., 2021, Kerrigan et al.,2021; Straitouri et al., 2022; Benz and Rodriguez, 2022). This scenario is more attractive and significant in applications such as large language models, where multiple models are often available for deferral. However, the surrogate losses proposed by the authors do not benefit from realizable \(\mathcal{H}\)-consistency, even in the single-expert setting, since they are a straightforward generalization of those of Mozannar and Sontag (2020) and Verma and Nalisnick (2022).

In summary, the problem of learning to defer in a single-stage scenario has been extensively studied, but it is often impractical in real-world applications. Post-hoc methods and surrogate losses have been explored, but the challenge remains to find a surrogate loss that is both consistent and realizable-consistent. Recent research has made progress in the scenario of learning to defer with multiple experts but has not achieved realizable \(\mathcal{H}\)-consistency even in a single-expert setting.

## Appendix B Examples of two-stage score-based surrogate losses

**Example:**\(\ell_{2}=\ell_{\mathrm{exp}}\). For \(\ell_{2}(\overline{h}_{d},x,y)=\ell_{\mathrm{exp}}(\overline{h}_{d},x,y)= \sum_{y^{\prime}\neq y}e^{\overline{h}_{d}(x,y^{\prime})-\overline{h}_{d}(x,y)}\), by (3), we have

\[\mathsf{L}_{h_{p}}(h_{d},x,y)\] \[=\mathds{1}_{h_{p}(x)=y}\,\ell_{2}(\overline{h}_{d},x,0)+\sum_{j =1}^{n_{\mathrm{e}}}\bar{c}_{j}(x,y)\ell_{2}(\overline{h}_{d},x,j)\] \[=\mathds{1}_{h_{p}(x)=y}\,\sum_{y^{\prime}\neq 0}e^{\overline{h}_{ d}(x,y^{\prime})-\overline{h}_{d}(x,0)}+\sum_{j=1}^{n_{\mathrm{e}}}\bar{c}_{j}(x,y )\sum_{y^{\prime}\neq y}e^{\overline{h}_{d}(x,y^{\prime})-\overline{h}_{d}(x,j)}\] \[=\mathds{1}_{h_{p}(x)=y}\sum_{i=1}^{n_{\mathrm{e}}}e^{h(x,n+i)- \max_{y\neq y}h(x,y)}+\sum_{j=1}^{n_{\mathrm{e}}}\bar{c}_{j}(x,y)\!\!\left[\sum _{i=1,\uparrow\neq j}^{n_{\mathrm{e}}}e^{h(x,n+i)-h(x,n+j)}+e^{\max_{y\neq y}h( x,y)-h(x,n+j)}\right]\!.\]

**Example:**\(\ell_{2}=\ell_{\mathrm{log}}\). For \(\ell_{2}(\overline{h}_{d},x,y)=\ell_{\mathrm{log}}(\overline{h}_{d},x,y)=\log \!\left(\sum_{y^{\prime}\in\mathcal{Y}\cup\{0\}}e^{\overline{h}_{d}(x,y^{ \prime})-\overline{h}_{d}(x,y)}\right)\), by (3), we have

\[\mathsf{L}_{h_{p}}(h_{d},x,y)\] \[=\mathds{1}_{h_{p}(x)=y}\,\log\!\left(\frac{e^{\max_{y\neq y}h(x,y)}}{e^{\max_{y\neq y}h(x,y)}+\sum_{i=1}^{n_{\mathrm{e}}}e^{h(x,n+i)}}\right) \!-\sum_{j=1}^{n_{\mathrm{e}}}\bar{c}_{j}(x,y)\log\!\left(\frac{e^{h(x,n+j)}}{ e^{\max_{y\neq y}h(x,y)}+\sum_{i=1}^{n_{\mathrm{e}}}e^{h(x,n+i)}}\right)\!.\]

**Example:**\(\ell_{2}=\ell_{\mathrm{gce}}\). For \(\ell_{2}(\overline{h}_{d},x,y)=\ell_{\mathrm{gce}}(\overline{h}_{d},x,y)= \frac{1}{\alpha}\!\left[1-\!\left[\frac{e^{\overline{h}_{d}(x,y)}}{\sum_{y^{ \prime}\neq y\cup\{0\}}e^{\overline{h}_{d}(x,y^{\prime})}}\right]^{\alpha} \right]\!,\alpha\in(0,1)\), by (3), we have

\[\mathsf{L}_{h_{p}}(h_{d},x,y)\] \[=\mathds{1}_{h_{p}(x)=y}\,\frac{1}{\alpha}\!\left[1-\!\left[ \frac{e^{\overline{h}_{d}(x,y)}}{e^{\max_{y\neq y}h(x,y)}+\sum_{i=1}^{n_{ \mathrm{e}}}e^{h(x,n+i)}}\right]^{\alpha}\right]\!+\!\sum_{j=1}^{n_{\mathrm{e}} }\bar{c}_{j}(x,y)\frac{1}{\alpha}\!\left[1-\!\left[\frac{e^{h(x,n+j)}}{e^{ \max_{y\neq y}h(x,y)}+\sum_{i=1}^{n_{\mathrm{e}}}e^{h(x,n+i)}}\right]^{\alpha} \right]\!\]

**Example:**\(\ell_{2}=\ell_{\text{m}\text{e}}\)**.** For \(\ell_{2}(\overline{h}_{d},x,y)=\ell_{\text{m}\text{e}}(\overline{h}_{d},x,y)=1- \frac{e^{\overline{h}_{d}(x,y)}}{\sum_{y^{\prime}\in y\cup\{0\}}e^{\overline{h} _{d}(x,y^{\prime})}}\), by (3), we have

\[\mathsf{L}_{h_{p}}(h_{d},x,y)\] \[=\mathds{1}_{h_{p}(x)=y}\bigg{[}1-\frac{e^{\overline{h}_{d}(x,0) }}{\sum_{y^{\prime}\in y\cup\{0\}}e^{\overline{h}_{d}(x,y^{\prime})}}\bigg{]}+ \sum_{j=1}^{n_{e}}\bar{c}_{j}(x,y)\bigg{(}1-\frac{e^{\overline{h}_{d}(x,j)}}{ \sum_{y^{\prime}\in y\cup\{0\}}e^{\overline{h}_{d}(x,y^{\prime})}}\bigg{)}\] \[=\mathds{1}_{h_{p}(x)=y}\bigg{[}1-\frac{e^{\max_{q\neq y}h(x,y)}} {e^{\max_{q\neq y}h(x,y)}+\sum_{i=1}^{n_{e}}e^{h(x,n+i)}}\bigg{]}+\sum_{j=1}^{ n_{e}}\bar{c}_{j}(x,y)\bigg{[}1-\frac{e^{h(x,n+j)}}{e^{\max_{q\neq y}h(x,y)}+\sum_{i=1} ^{n_{e}}e^{h(x,n+i)}}\bigg{]}.\]

## Appendix C Examples of two-stage predictor-rejector surrogate losses

**Example:**\(\ell_{2}=\ell_{\text{exp}}\)**.** For \(\ell_{2}(\overline{r},x,y)=\ell_{\text{exp}}(\overline{r},x,y)=\sum_{y^{\prime }uy}e^{\overline{r}(x,y^{\prime})-\overline{r}(x,y)}\), by (5), we have

\[\mathsf{L}_{h}(r,x,y)\] \[=\mathds{1}_{h(x)=y}\,\ell_{2}(\overline{r},x,0)+\sum_{j=1}^{n_{ e}}\bar{c}_{j}(x,y)\ell_{2}(\overline{r},x,j)\] \[=\mathds{1}_{h(x)=y}\,\sum_{y^{\prime}\in 0}e^{\overline{r}(x,y^{ \prime})-\overline{r}(x,0)}+\sum_{j=1}^{n_{e}}\bar{c}_{j}(x,y)\sum_{y^{\prime} \neq y}e^{\overline{r}(x,y^{\prime})-\overline{r}(x,j)}\] \[=\mathds{1}_{h(x)=y}\,\sum_{i=1}^{n_{e}}e^{-r_{i}(x)}+\sum_{j=1} ^{n_{e}}\bar{c}_{j}(x,y)\bigg{[}\sum_{i=1,i\neq j}^{n_{e}}e^{r_{j}(x)-r_{i}(x )}+e^{r_{j}(x)}\bigg{]}.\]

**Example:**\(\ell_{2}=\ell_{\text{log}}\)**.** For \(\ell_{2}(\overline{r},x,y)=\ell_{\text{log}}(\overline{r},x,y)=\log\Bigl{(} \sum_{y^{\prime}\in y\cup\{0\}}e^{\overline{r}(x,y^{\prime})-\overline{r}(x, y)}\Bigr{)}\), by (5), we have

\[\mathsf{L}_{h}(r,x,y)\] \[=\mathds{1}_{h(x)=y}\,\ell_{2}(\overline{r},x,0)+\sum_{j=1}^{n_{ e}}\bar{c}_{j}(x,y)\ell_{2}(\overline{r},x,j)\] \[=\mathds{1}_{h(x)=y}\,\log\Biggl{(}\sum_{y^{\prime}\in y\cup\{0 \}}e^{\overline{r}(x,y^{\prime})-\overline{r}(x,0)}\Biggr{)}+\sum_{j=1}^{n_{e} }\bar{c}_{j}(x,y)\log\Biggl{(}\sum_{y^{\prime}\in y\cup\{0\}}e^{\overline{r}( x,y^{\prime})-\overline{r}(x,j)}\Biggr{)}\] \[=-\mathds{1}_{h(x)=y}\log\Biggl{(}\frac{1}{1+\sum_{i=1}^{n_{e}}e ^{-r_{i}(x)}}\Biggr{)}-\sum_{j=1}^{n_{e}}\bar{c}_{j}(x,y)\log\Biggl{(}\frac{e^{ -r_{j}(x)}}{1+\sum_{i=1}^{n_{e}}e^{-r_{i}(x)}}\Biggr{)}.\]

**Example:**\(\ell_{2}=\ell_{\text{g}\text{e}}\)**.** For \(\ell_{2}(\overline{r},x,y)=\ell_{\text{g}\text{e}}(\overline{r},x,y)=\frac{1} {\alpha}\biggl{[}1-\biggl{[}\frac{e^{\overline{r}(x,y)}}{\sum_{y^{\prime}\in y \cup\{0\}}e^{\overline{r}(x,y^{\prime})}}\biggr{]}^{\alpha}\biggr{]},\alpha\in(0,1)\), by (5), we have

\[\mathsf{L}_{h}(r,x,y)\] \[=\mathds{1}_{h(x)=y}\,\ell_{2}(\overline{r},x,0)+\sum_{j=1}^{n_{ e}}\bar{c}_{j}(x,y)\ell_{2}(\overline{r},x,j)\] \[=\mathds{1}_{h(x)=y}\,\frac{1}{\alpha}\biggl{[}1-\biggl{[}\frac{e ^{\overline{r}(x,0)}}{\sum_{y^{\prime}\in y\cup\{0\}}e^{\overline{r}(x,y^{ \prime})}}\biggr{]}^{\alpha}\biggr{]}+\sum_{j=1}^{n_{e}}\bar{c}_{j}(x,y)\frac{1} {\alpha}\biggl{[}1-\biggl{[}\frac{e^{\overline{r}(x,j)}}{\sum_{y^{\prime}\in y \cup\{0\}}e^{\overline{r}(x,y^{\prime})}}\biggr{]}^{\alpha}\biggr{]}\] \[=\mathds{1}_{h(x)=y}\,\frac{1}{\alpha}\biggl{[}1-\biggl{[}\frac{1} {1+\sum_{i=1}^{n_{e}}e^{-r_{i}(x)}}\biggr{]}^{\alpha}\biggr{]}+\sum_{j=1}^{n_{e }}\bar{c}_{j}(x,y)\frac{1}{\alpha}\biggl{[}1-\biggl{[}\frac{e^{-r_{j}(x)}}{1+ \sum_{i=1}^{n_{e}}e^{-r_{i}(x)}}\biggr{]}^{\alpha}\biggr{]}.\]

**Example:**\(\ell_{2}=\ell_{\text{m}ae}\)**.** For \(\ell_{2}(\overline{r},x,y)=\ell_{\text{m}ae}(\overline{r},x,y)=1-\frac{e^{\overline {r}(x,y)}}{\sum_{y^{\prime}4y_{0}(0)}e^{\overline{r}(x,y^{\prime})}}\), by (5), we have

\[\mathsf{L}_{h}(r,x,y)\] \[=\mathds{1}_{\mathsf{h}(x)=y}\ell_{2}(\overline{r},x,0)+\sum_{j=1 }^{n_{e}}\bar{c}_{j}(x,y)\ell_{2}(\overline{r},x,j)\] \[=\mathds{1}_{\mathsf{h}(x)=y}\left(1-\frac{e^{\overline{r}(x,0)}} {\sum_{y^{\prime}\in\mathcal{Y}\cup\{0\}}e^{\overline{r}(x,y^{\prime})}}\right) +\sum_{j=1}^{n_{e}}\bar{c}_{j}(x,y)\!\left(\!1-\frac{e^{\overline{r}(x,j)}}{ \sum_{y^{\prime}\in\mathcal{Y}\cup\{0\}}e^{\overline{r}(x,y^{\prime})}}\right)\] \[=\mathds{1}_{\mathsf{h}(x)=y}\!\left[1-\frac{1}{1+\sum_{i=1}^{n_ {e}}e^{-r_{i}(x)}}\right]+\sum_{j=1}^{n_{e}}\bar{c}_{j}(x,y)\!\left[1-\frac{e^ {-r_{j}(x)}}{1+\sum_{i=1}^{n_{e}}e^{-r_{i}(x)}}\right]\!.\]

Appendix D Proof of \(\mathcal{H}\)(-consistency bounds for score-based two-stage surrogate losses (Theorem 1)

**Theorem 1** (\(\mathcal{H}\)**-consistency bounds for score-based two-stage surrogates)**.: _Assume that \(\ell_{1}\) admits an \(\mathcal{H}_{p}\)-consistency bound and \(\ell_{2}\) admits an \(\overline{\mathcal{H}}_{d}\)-consistency bound with respect to the multi-class zero-one classification loss \(\ell_{0-1}\) respectively. Thus, there are non-decreasing concave functions \(\Gamma_{1}\) and \(\Gamma_{2}\) such that, for all \(h_{p}\in\mathcal{H}_{p}\) and \(\overline{h}_{d}\in\overline{\mathcal{H}}_{d}\), we have_

\[\mathcal{E}_{\ell_{0-1}}(h_{p})-\mathcal{E}_{\ell_{0-1}}^{*}( \mathcal{H}_{p})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H}_{p}) \leq\Gamma_{1}\big{(}\mathcal{E}_{\ell_{1}}(h_{p})-\mathcal{E}_{ \ell_{1}}^{*}(\mathcal{H}_{p})+\mathcal{M}_{\ell_{1}}(\mathcal{H}_{p})\big{)}\] \[\mathcal{E}_{\ell_{0-1}}(\overline{h}_{d})-\mathcal{E}_{\ell_{0-1 }}^{*}(\overline{\mathcal{H}}_{d})+\mathcal{M}_{\ell_{0-1}}(\overline{\mathcal{ H}}_{d}) \leq\Gamma_{2}\big{(}\mathcal{E}_{\ell_{2}}(\overline{h}_{d})-\mathcal{E}_{\ell_{2}}^{*}( \overline{\mathcal{H}}_{d})+\mathcal{M}_{\ell_{2}}(\overline{\mathcal{H}}_{d}) \big{)}.\]

_Then, the following holds for all \(h\in\mathcal{H}\):_

\[\mathcal{E}_{\mathsf{L}_{\mathsf{def}}}(h)-\mathcal{E}_{\mathsf{ L}_{\mathsf{def}}}^{*}(\mathcal{H})+\mathcal{M}_{\mathsf{L}_{\mathsf{def}}}( \mathcal{H})\] \[\leq\Gamma_{1}\big{(}\mathcal{E}_{\ell_{1}}(h_{p})-\mathcal{E}_{ \ell_{1}}^{*}(\mathcal{H}_{p})+\mathcal{M}_{\ell_{1}}(\mathcal{H}_{p})\big{)} +\left(1+\sum_{j=1}^{n_{e}}\overline{c}_{j}\right)\Gamma_{2}\!\left(\!\frac{ \mathcal{E}_{\mathsf{L}_{h_{p}}}(h_{d})-\mathcal{E}_{\mathsf{L}_{h_{p}}}^{*}( \mathcal{H}_{d})+\mathcal{M}_{\mathsf{L}_{h_{p}}}(\mathcal{H}_{d})}{\sum_{j=1 }^{n_{e}}\mathcal{E}_{j}}\!\right)\!.\]

_Furthermore, constant factors \(\left(1+\sum_{j=1}^{n_{e}}\overline{c}_{j}\right)\) and \(\frac{1}{\sum_{j=1}^{n_{e}}\overline{c}_{j}}\) can be removed when \(\Gamma_{2}\) is linear._

Proof.: If \(\mathsf{h}(x)\in[n]\), then \(\mathsf{h}(x)=\mathsf{h}_{p}(x)\). Thus, the learning to defer loss can be expressed as follows:

\[\mathsf{L}_{\mathsf{def}}(h,x,y) =\mathds{1}_{\mathsf{h}(x)\neq y}\mathds{1}_{\mathsf{h}(x)\in[n]} +\sum_{j=1}^{n_{e}}c_{j}(x,y)\mathds{1}_{\mathsf{h}(x)=n+j}\] \[=\mathds{1}_{\mathsf{h}_{p}(x)\neq y}\mathds{1}_{\mathsf{h}(x)\in[ n]}+\sum_{j=1}^{n_{e}}c_{j}(x,y)\mathds{1}_{\

[MISSING_PAGE_FAIL:20]

Therefore, by (6), we obtain

\[\mathcal{E}_{\mathrm{L}_{\mathrm{def}}}(h)-\mathcal{E}_{\mathrm{L}_{ \mathrm{def}}}^{*}(\mathcal{H})+\mathcal{M}_{\mathrm{L}_{\mathrm{def}}}(\mathcal{H })\] \[\leq\begin{cases}\mathbb{E}_{X}\big{[}\Gamma_{2}\big{(}\Delta \mathcal{C}_{\mathrm{L}_{hp},\mathcal{H}_{d}}(h_{d},x)\big{)}\big{]}+\mathbb{E }_{X}\big{[}\Gamma_{1}\big{(}\Delta\mathcal{C}_{\ell_{1},\mathcal{H}_{p}}(h_{p},x)\big{)}\big{]}&\text{when $\Gamma_{2}$ is linear}\\ \big{(}1+\sum_{j=1}^{n_{e}}\overline{c}_{j}\big{)}\,\mathbb{E}_{X}\Big{[}\Gamma _{2}\Big{(}\frac{\Delta\mathcal{C}_{\mathrm{L}_{hp},\mathcal{H}_{d}}(h_{d},x)}{ \sum_{j=1}^{n_{e}}\overline{c}_{j}}\Big{)}\Big{]}+\mathbb{E}_{X}\big{[}\Gamma_ {1}\big{(}\Delta\mathcal{C}_{\ell_{1},\mathcal{H}_{p}}(h_{p},x)\big{)}\big{]} &\text{otherwise}\end{cases}\] \[\leq\begin{cases}\Gamma_{2}\big{(}\mathbb{E}_{X}\Big{[}\Delta \mathcal{C}_{\mathrm{L}_{hp},\mathcal{H}_{d}}(h_{d},x)\big{]}\Big{)}+\Gamma_ {1}\big{(}\mathbb{E}_{X}\big{[}\Delta\mathcal{C}_{\ell_{1},\mathcal{H}_{p}}(h_{ p},x)\big{]}\big{)}&\text{when $\Gamma_{2}$ is linear}\\ \big{(}1+\sum_{j=1}^{n_{e}}\overline{c}_{j}\big{)}\Gamma_{2}\Big{(}\frac{ \overline{c}_{\mathrm{L}_{hp}}^{*}\overline{c}_{j}}{\sum_{j=1}^{n_{e}}\overline {c}_{j}}\,\mathbb{E}_{X}\Big{[}\Delta\mathcal{C}_{\mathrm{L}_{hp},\mathcal{H}_ {d}}(h_{d},x)\big{]}\Big{)}+\Gamma_{1}\big{(}\mathbb{E}_{X}\big{[}\Delta \mathcal{C}_{\ell_{1},\mathcal{H}_{p}}(h_{p},x)\big{]}\big{)}&\text{otherwise} \end{cases}\] \[=\begin{cases}\Gamma_{1}\big{(}\mathcal{E}_{\ell_{1}}(h_{p})- \mathcal{E}_{\ell_{1}}^{*}(\mathcal{H}_{p})+\mathcal{M}_{\ell_{1}}(\mathcal{H }_{p})\big{)}+\Gamma_{2}\Big{(}\mathcal{E}_{\mathrm{L}_{hp}}(h_{d})-\mathcal{E }_{\mathrm{L}_{hp}}^{*}(\mathcal{H}_{d})+\mathcal{M}_{\mathrm{L}_{hp}}( \mathcal{H}_{d})\Big{)}&\text{when $\Gamma_{2}$ is linear}\\ \Gamma_{1}\big{(}\mathcal{E}_{\ell_{1}}(h_{p})-\mathcal{E}_{\ell_{1}}^{*}( \mathcal{H}_{p})+\mathcal{M}_{\ell_{1}}(\mathcal{H}_{p})\big{)}+\big{(}1+\sum _{j=1}^{n_{e}}\overline{c}_{j}\big{)}\Gamma_{2}\Bigg{(}\frac{\mathcal{E}_{ \mathrm{L}_{hp}}(h_{d})-\mathcal{E}_{\mathrm{L}_{hp}}^{*}(\mathcal{H}_{d})+ \mathcal{M}_{h_{hp}}(\mathcal{H}_{d})}{\overline{c}_{j=1}^{n_{e}}\overline{c }_{j}}\Bigg{)}&\text{otherwise},\end{cases}\]

which completes the proof. 

Appendix E Proof of \(\overline{\mathcal{H}}\)-consistency bounds for standard surrogate loss functions (Theorem 3)

Recall that for a hypothesis \(h\colon\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\), we define \(\overline{h}\) as its augmented hypothesis: \(\overline{h}(\cdot,0)=\lambda,\overline{h}(\cdot,1)=h(x,1),\ldots,\overline{h} (\cdot,n)=h(x,n)\) with some constant \(\lambda\in\mathbb{R}\). We define \(\overline{\mathcal{H}}\) as the hypothesis set that consists of all such augmented hypotheses of \(\mathcal{H}\): \(\overline{\mathcal{H}}=\big{\{}\overline{h}:h\in\mathcal{H}\big{\}}\). The prediction associated by \(\overline{h}\in\overline{\mathcal{H}}\) to an input \(x\in\mathcal{X}\) is denoted by \(\overline{h}(x)\) and defined as the element in \(\mathcal{Y}\cup\{0\}\) with the highest score, \(\overline{h}(x)=\operatorname*{argmax}_{y\in\mathcal{Y}\cup\{0\}}h(x,y)\), with an arbitrary but fixed deterministic strategy for breaking ties. For any \(x\in\mathcal{X}\) and label space \(\mathcal{Y}\cup\{0\}\), we will denote, by \(\overline{\mathsf{H}}(x)\) the set of labels generated by hypotheses in \(\overline{\mathcal{H}}\): \(\overline{\mathsf{H}}(x)=\big{\{}\overline{h}(x)\colon h\in\overline{\mathcal{H }}\big{\}}\). By [10, Lemma 3] with label space \(\mathcal{Y}\cup\{0\}\) and a conditional probability vector \(p(x,\cdot)\) on \(\mathcal{Y}\cup\{0\}\), the minimal conditional \(\ell_{0-1}\)-loss and the corresponding calibration gap can be characterized as follows.

**Lemma 8**.: _For any \(x\in\mathcal{X}\), the minimal conditional \(\ell_{0-1}\)-risk and the calibration gap for \(\ell_{0-1}\) can be expressed as follows:_

\[\mathcal{C}_{\ell_{0-1}}^{*}(x) =1-\max_{y\in\overline{\mathsf{H}}(x)}p(x,y)\] \[\Delta\mathcal{C}_{\ell_{0-1}}(h,x) =\max_{y\in\overline{\mathsf{H}}(x)}p(x,y)-p(x,\mathsf{h}(x)).\]

### Multinomial logistic loss

**Theorem 9** (\(\overline{\mathcal{H}}\)**-consistency bound for multinomial logistic loss**).: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any \(\lambda\in\mathbb{R}\), hypothesis \(\overline{h}\in\overline{\mathcal{H}}\) and any distribution,_

\[\mathcal{E}_{\ell_{0-1}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{0-1}}^{*} \big{(}\overline{\mathcal{H}}\big{)}\leq\sqrt{2}\Big{(}\mathcal{E}_{\ell_{ \mathrm{log}}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{\mathrm{log}}}^{*} \big{(}\overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{\mathrm{log}}}\big{(} \overline{\mathcal{H}}\big{)}\Big{)}^{\frac{1}{2}}-\mathcal{M}_{\ell_{0-1}} \big{(}\overline{\mathcal{H}}\big{)}.\]

Proof.: For the multinomial logistic loss \(\ell_{\mathrm{log}}\), the conditional \(\ell_{\mathrm{log}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\ell_{\mathrm{log}}}\big{(}\overline{h},x\big{)}\big{)}=\sum_{y\in \mathcal{Y}\cup\{0\}}p(x,y)\log\!\left(\sum_{y^{\prime}\in\mathcal{Y}\cup\{0\}}e^{ \overline{h}(x,y^{\prime})-\overline{h}(x,y)}\right)=-\sum_{y\in\mathcal{Y}\cup \{0\}}p(x,y)\log(\mathcal{S}(x,y))\]

where we let \(\mathcal{S}(x,y)=\frac{e^{\overline{h}(x,y)}}{\sum_{y^{\prime}\in\mathcal{Y}\cup \{0\}}e^{\overline{h}(x,y^{\prime})}}\in[0,1]\) for any \(y\in\mathcal{Y}\cup\{0\}\) with the constraint that \(\sum_{y\in\mathcal{Y}\cup\{\(\{\overline{h}_{\mu}:\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x,\overline{h}(x)) ]\}\subset\overline{\mathcal{H}}\) such that \(\mathcal{S}_{\mu}(x,\cdot)=\frac{e^{\overline{h}_{\mu}(x,\cdot)}}{\sum_{y^{ \prime}\in\mathcal{V}(0)}e^{\overline{h}_{\mu}(x,y^{\prime})}}\) take the following values:

\[\mathcal{S}_{\mu}(x,y)=\begin{cases}\mathcal{S}(x,y)&\text{if }y\notin\{y_{\max}, \overline{h}(x)\}\\ \mathcal{S}(x,y_{\max})+\mu&\text{if }y=\overline{h}(x)\\ \mathcal{S}(x,\overline{h}(x))-\mu&\text{if }y=y_{\max}.\end{cases}\]

Note that \(\mathcal{S}_{\mu}\) satisfies the constraint:

\[\sum_{y\neq\mathcal{V}(0)}\mathcal{S}_{\mu}(x,y)=\sum_{y\neq\mathcal{V}(0)} \mathcal{S}(x,y)=1,\,\forall\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x, \overline{h}(x))].\]

Let \(\overline{h}\in\overline{\mathcal{H}}\) be a hypothesis such that \(\overline{h}(x)\neq y_{\max}\). By the definition and using the fact that \(\overline{\mathsf{H}}(x)=\mathcal{V}\cup\{0\}\) when \(\mathcal{H}\) is symmetric, we obtain

\[\Delta\mathcal{C}_{\ell_{\log},\overline{\mathcal{H}}}(\overline{ h},x)\] \[=\mathcal{C}_{\ell_{\log}}(\overline{h},x)-\mathcal{C}_{\ell_{ \log}}^{*}\big{(}\overline{\mathcal{H}},x\big{)}\] \[\geq\mathcal{C}_{\ell_{\log}}\big{(}\overline{h},x\big{)}-\inf_{ \mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x,\overline{h}(x))]}\mathcal{C}_{ \ell_{\log}}\big{(}\overline{h}_{\mu},x\big{)}\] \[=\sup_{\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x,\overline{h }(x))]}\bigg{\{}p(x,y_{\max})\big{[}-\log(\mathcal{S}(x,y_{\max}))+\log\big{(} \mathcal{S}(x,\overline{h}(x))-\mu\big{)}\big{]}\] \[\qquad+p(x,\overline{h}(x))\big{[}-\log\big{(}\mathcal{S}(x, \overline{h}(x))\big{)}+\log(\mathcal{S}(x,y_{\max})+\mu\big{)}\big{]}\bigg{\}}\]

Differentiating with respect to \(\mu\) yields the optimum value \(\mu^{*}=\frac{p(x,\overline{h}(x))\mathcal{S}(x,\overline{h}(x))-p(x,y_{\max })\mathcal{S}(x,y_{\max})}{p(x,y_{\max})+p(x,\overline{h}(x))}\). Plugging that value in the inequality gives:

\[\Delta\mathcal{C}_{\ell_{\log},\overline{\mathcal{H}}}\big{(} \overline{h},x\big{)}\geq p(x,y_{\max})\log\frac{[\mathcal{S}(x,\overline{h}(x))+ \mathcal{S}(x,y_{\max})]p(x,y_{\max})}{\mathcal{S}(x,y_{\max})\big{[}p(x,y_{ \max})+p(x,\overline{h}(x))\big{]}}\] \[\qquad+p(x,\overline{h}(x))\log\frac{\big{[}\mathcal{S}(x, \overline{h}(x))+\mathcal{S}(x,y_{\max})\big{]}p(x,\overline{h}(x))}{\mathcal{ S}(x,\overline{h}(x))\big{[}p(x,y_{\max})+p(x,\overline{h}(x))\big{]}}.\]

Differentiating with respect to \(\mathcal{S}\) to show that the minimum is attained for \(\mathcal{S}(x,\overline{h}(x))=\mathcal{S}(x,y_{\max})\), which implies

\[\Delta\mathcal{C}_{\ell_{\log},\overline{\mathcal{H}}}\big{(}\overline{h},x \big{)}\geq p(x,y_{\max})\log\frac{2p(x,y_{\max})}{p(x,y_{\max})+p(x,\overline {h}(x))}+p(x,\overline{h}(x))\log\frac{2p(x,\overline{h}(x))}{p(x,y_{\max})+p( x,\overline{h}(x))}.\]

By Pinsker's inequality, we have, for \(a,b\in[0,1]\), \(a\log\frac{2a}{a+b}\log\frac{2b}{a+b}\geq\frac{(a-b)^{2}}{2(a+b)}\). Using this inequality, we obtain:

\[\Delta\mathcal{C}_{\ell_{\log},\overline{\mathcal{H}}}(\overline{ h},x) \geq\frac{\big{(}p(x,\overline{h}(x))-p(x,y_{\max})\big{)}^{2}}{2\big{(}p(x, \overline{h}(x))+p(x,y_{\max})\big{)}}\] \[\geq\frac{\big{(}p(x,\overline{h}(x))-p(x,y_{\max})\big{)}^{2}}{2} (0\leq p(x,\overline{h}(x))+p(x,y_{\max})\leq 1)\] \[=\frac{1}{2}\big{(}\Delta\mathcal{C}_{\ell_{\log-1},\overline{ \mathcal{H}}}\big{(}\overline{h},x\big{)}\big{)}^{2}. (\text{by Lemma \ref{lem:S1} and \ref{lem:S2}}(x)=\mathcal{V}\cup\{0\})\]

Since the function \(\frac{t^{2}}{2}\) is convex, by Jensen's inequality, we obtain for any hypothesis \(\overline{h}\in\overline{\mathcal{H}}\) and any distribution,

\[\frac{\big{(}\mathbb{E}_{X}\big{[}\Delta\mathcal{C}_{\ell_{\log-1},\overline{ \mathcal{H}}}\big{(}\overline{h},x\big{)}\big{]}\big{)}^{2}}{2}\leq\mathbb{E}_{X} \bigg{[}\frac{\Delta\mathcal{C}_{\ell_{\log},\overline{\mathcal{H}}}\big{(} \overline{h},x\big{)}^{2}}{2}\bigg{]}\leq\mathbb{E}_{X}\bigg{[}\Delta\mathcal{C }_{\ell_{\log},\overline{\mathcal{H}}}\big{(}\overline{h},x\big{)}\bigg{]},\]

which leads to

\[\mathcal{E}_{\ell_{\log-1}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{\log-1} }^{*}\big{(}\overline{\mathcal{H}}\big{)}\leq\sqrt{2}\Big{(}\mathcal{E}_{\ell_ {\log}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{\log}}^{*}\big{(}\overline{ \mathcal{H}}\big{)}+\mathcal{M}_{\ell_{\log}}\big{(}\overline{\mathcal{H}}\big{)} \Big{)}^{\frac{1}{2}}-\mathcal{M}_{\ell_{\log-1}}\big{(}\overline{\mathcal{H}} \big{)}.\]

### Sum exponential loss

**Theorem 10** (\(\overline{\mathcal{H}}\)-consistency bound for sum exponential loss).: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any \(\lambda\in\mathbb{R}\), hypothesis \(\overline{h}\in\overline{\mathcal{H}}\) and any distribution,_

\[\mathcal{E}_{\ell_{0-1}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{0-1}}^{ *}\big{(}\overline{\mathcal{H}}\big{)}\leq\sqrt{2}\big{(}\mathcal{E}_{\ell_{ \mathrm{exp}}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{\mathrm{exp}}}^{*} \big{(}\overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{\mathrm{exp}}}\big{(} \overline{\mathcal{H}}\big{)}\big{)}^{\frac{1}{2}}-\mathcal{M}_{\ell_{0-1}} \big{(}\overline{\mathcal{H}}\big{)}.\]

Proof.: For the sum exponential loss \(\ell_{\mathrm{exp}}\), the conditional \(\ell_{\mathrm{exp}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\ell_{\mathrm{exp}}}\big{(}\overline{h},x\big{)}\big{)}=\sum_{y \in\mathcal{Y}\cup\{0\}}p(x,y)\Bigg{(}\sum_{y^{\prime}\in\mathcal{Y}\cup\{0\}} e^{\overline{h}(x,y^{\prime})-\overline{h}(x,y)}\Bigg{)}-1=\sum_{y\in\mathcal{Y}\cup\{0\}} \frac{p(x,y)}{\mathcal{S}(x,y)}-1\]

where we let \(\mathcal{S}(x,y)=\frac{e^{\overline{h}(x,y)}}{\sum_{y^{\prime}\in\mathcal{Y} \cup\{0\}}e^{\overline{h}(x,y^{\prime})}}\in[0,1]\) for any \(y\in\mathcal{Y}\cup\{0\}\) with the constraint that \(\sum_{y\in\mathcal{Y}\cup\{0\}}\mathcal{S}(x,y)=1\). Let \(y_{\max}=\operatorname*{argmax}_{y\in\mathcal{Y}\cup\{0\}}p(x,y)\), where we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy. For any \(\overline{h}\in\mathcal{H}\) such that \(\overline{h}(x)\neq y_{\max}\) and \(x\in\mathcal{X}\), by the symmetry and completeness of \(\mathcal{H}\), we can always find a family of hypotheses \(\big{\{}\overline{h}_{\mu}\colon\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x,\overline{h}(x))]\big{\}}\subset\overline{\mathcal{H}}\) such that \(\mathcal{S}_{\mu}(x,\cdot)=\frac{e^{\overline{h}_{\mu}(x,\cdot)}}{\sum_{y^{ \prime}\in\mathcal{Y}\cup\{0\}}e^{\overline{h}_{\mu}(x,y^{\prime})}}\) take the following values:

\[\mathcal{S}_{\mu}(x,y)=\begin{cases}\mathcal{S}(x,y)&\text{if }y\notin\big{\{}y_{ \max},\overline{h}(x)\big{\}}\\ \mathcal{S}(x,y_{\max})+\mu&\text{if }y=\overline{h}(x)\\ \mathcal{S}(x,\overline{h}(x))-\mu&\text{if }y=y_{\max}.\end{cases}\]

Note that \(\mathcal{S}_{\mu}\) satisfies the constraint:

\[\sum_{y\in\mathcal{Y}}\mathcal{S}_{\mu}(x,y)=\sum_{y\in\mathcal{Y}}\mathcal{S }(x,y)=1,\,\forall\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x,\overline{h} (x))].\]

Let \(\overline{h}\in\overline{\mathcal{H}}\) be a hypothesis such that \(\overline{h}(x)\neq y_{\max}\). By the definition and using the fact that \(\overline{\mathsf{H}}(x)=\mathcal{Y}\cup\{0\}\) when \(\mathcal{H}\) is symmetric, we obtain

\[\Delta\mathcal{C}_{\ell_{\mathrm{exp}},\overline{\mathcal{H}}} (\overline{h},x)\] \[=\mathcal{C}_{\ell_{\mathrm{exp}}}\big{(}\overline{h},x\big{)}- \mathcal{C}_{\ell_{\mathrm{exp}}}^{*}\big{(}\overline{\mathcal{H}},x\big{)}\] \[=\sup_{\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x,\overline{ h}(x))]}\Bigg{\{}p(x,y_{\max})\Bigg{[}\frac{1}{\mathcal{S}(x,y_{\max})}- \frac{1}{\mathcal{S}(x,\overline{h}(x))-\mu}\Bigg{]}\] \[\qquad+p(x,\overline{h}(x))\Bigg{[}\frac{1}{\mathcal{S}(x, \overline{h}(x))}-\frac{1}{\mathcal{S}(x,y_{\max})+\mu}\Bigg{]}\Bigg{\}}.\]

Differentiating with respect to \(\mu\) yields the optimal value

\[\mu^{*}=\frac{\sqrt{p(x,\overline{h}(x))}\mathcal{S}(x,\overline{h}(x))-\sqrt {p(x,y_{\max})}\mathcal{S}(x,y_{\max})}{\sqrt{p(x,y_{\max})}+\sqrt{p(x, \overline{h}(x))}}.\]

Plugging that value in the inequality gives:

\[\Delta\mathcal{C}_{\ell_{\mathrm{exp}},\overline{\mathcal{H}}}\big{(}\overline{ h},x\big{)}\geq\frac{p(x,y_{\max})}{\mathcal{S}(x,y_{\max})}+\frac{p(x,\overline{h}(x))}{ \mathcal{S}(x,\overline{h}(x))}-\frac{\Big{(}\sqrt{p(x,y_{\max})}+\sqrt{p(x, \overline{h}(x))}\Big{)}^{2}}{\mathcal{S}(x,y_{\max})+\mathcal{S}(x, \overline{h}(x))}.\]Differentiating with respect to \(\mathcal{S}\) to show that the minimum is attained for \(\mathcal{S}(x,\overline{\mathsf{h}}(x))=\mathcal{S}(x,y_{\max})=\frac{1}{2}\), which implies

\[\Delta\mathcal{C}_{\ell_{\mathrm{exp}},\overline{\mathcal{H}}}( \overline{h},x) \geq\left(\sqrt{p(x,y_{\max})}-\sqrt{p(x,\overline{\mathsf{h}}(x))} \right)^{2}\] \[=\frac{\big{(}p(x,\overline{\mathsf{h}}(x))-p(x,y_{\max})\big{)}^ {2}}{\Big{(}\sqrt{p(x,\overline{\mathsf{h}}(x))}+\sqrt{p(x,y_{\max})}\Big{)}^{2 }}.\]

By the concavity of the square-root function, for all \(a,b\in[0,1]\), we have \(\frac{1}{2}\big{(}\sqrt{a}+\sqrt{b}\big{)}\leq\sqrt{\frac{1}{2}(a+b)}\), thus we can write

\[\Delta\mathcal{C}_{\ell_{\mathrm{exp}},\overline{\mathcal{H}}}( \overline{h},x) \geq\frac{\big{(}p(x,\overline{\mathsf{h}}(x))-p(x,y_{\max})\big{)} ^{2}}{2\big{(}p(x,\overline{\mathsf{h}}(x))+p(x,y_{\max})\big{)}}\] \[\geq\frac{\big{(}p(x,\overline{\mathsf{h}}(x))-p(x,y_{\max})\big{)} ^{2}}{2} (p(x,\overline{\mathsf{h}}(x))+p(x,y_{\max})\leq 1)\] \[=\frac{1}{2}\Big{(}\Delta\mathcal{C}_{\ell_{0-1},\overline{ \mathcal{H}}}(\overline{h},x)\Big{)}^{2}.\] (by Lemma 8 and \[\overline{\mathsf{H}}(x)=\mathcal{Y}\cup\{0\}\] )

Since the function \(\frac{t^{2}}{2}\) is convex, by Jensen's inequality, we obtain for any hypothesis \(\overline{h}\in\overline{\mathcal{H}}\) and any distribution,

\[\frac{\Big{(}\mathbb{E}_{X}\Big{[}\Delta\mathcal{C}_{\ell_{0-1},\overline{ \mathcal{H}}}\big{(}\overline{h},x\big{)}\Big{]}\Big{)}^{2}}{2}\leq\underset{X} {\mathbb{E}}\!\!\left[\frac{\Delta\mathcal{C}_{\ell_{\mathrm{exp}},\overline{ \mathcal{H}}}\big{(}\overline{h},x\big{)}^{2}}{2}\right]\leq\underset{X}{ \mathbb{E}}\!\!\left[\Delta\mathcal{C}_{\ell_{\mathrm{exp}},\overline{ \mathcal{H}}}\big{(}\overline{h},x\big{)}\right]\!,\]

which leads to

\[\mathcal{E}_{\ell_{0-1}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{0-1}}^{* }\big{(}\overline{\mathcal{H}}\big{)}\leq\sqrt{2}\big{(}\mathcal{E}_{\ell_{ \mathrm{exp}}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{\mathrm{exp}}}^{* }\big{(}\overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{\mathrm{exp}}}\big{(} \overline{\mathcal{H}}\big{)}\big{)}^{\frac{1}{2}}-\mathcal{M}_{\ell_{0-1}} \big{(}\overline{\mathcal{H}}\big{)}.\]

### Generalized cross-entropy loss

**Theorem 11** (\(\overline{\mathcal{H}}\)**-consistency bound for generalized cross-entropy loss).: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any \(\lambda\in\mathbb{R}\), hypothesis \(\overline{h}\in\overline{\mathcal{H}}\) and any distribution,_

\[\mathcal{E}_{\ell_{0-1}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{0-1}}^{* }\big{(}\overline{\mathcal{H}}\big{)}\leq\sqrt{2(n+1)^{\alpha}}\big{(} \mathcal{E}_{\ell_{\mathrm{gen}}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_ {\mathrm{gen}}}^{*}\big{(}\overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{ \mathrm{gen}}}\big{(}\overline{\mathcal{H}}\big{)}\big{)}^{\frac{1}{2}}- \mathcal{M}_{\ell_{0-1}}\big{(}\overline{\mathcal{H}}\big{)}.\]

Proof.: For the generalized cross-entropy loss \(\ell_{\mathrm{gen}}\), the conditional \(\ell_{\mathrm{gen}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\ell_{\mathrm{gen}}}\big{(}\overline{h},x\big{)}=\sum_{y\in \mathcal{Y}\cup\{0\}}p(x,y)\frac{1}{\alpha}\!\left[1-\left[\frac{e^{\overline{ h}(x,y)}}{\sum_{y^{\prime}\in\mathcal{Y}\cup\{0\}}e^{\overline{h}(x,y^{\prime})}} \right]^{\alpha}\right]=\frac{1}{\alpha}\sum_{y\in\mathcal{Y}\cup\{0\}}p(x,y) (1-\mathcal{S}(x,y)^{\alpha})\]

where we let \(\mathcal{S}(x,y)=\frac{e^{\overline{h}(x,y)}}{\sum_{y^{\prime}\in\mathcal{Y} \cup\{0\}}e^{\overline{h}(x,y^{\prime})}}\in[0,1]\) for any \(y\in\mathcal{Y}\cup\{0\}\) with the constraint that \(\sum_{y\in\mathcal{Y}\cup\{0\}}\mathcal{S}(x,y)=1\). Let \(y_{\max}=\operatorname*{argmax}_{y\in\mathcal{Y}\cup\{0\}}p(x,y)\), where we choose the label with the same deterministic strategy for breaking ties as that of \(\overline{h}(x)\). For any \(\overline{h}\in\mathcal{H}\) such that \(\overline{\mathsf{h}}(x)\neq y_{\max}\) and \(x\in\mathcal{X}\), by the symmetry and completeness of \(\mathcal{H}\), we can always find a family of hypotheses \(\big{\{}\overline{h}_{\mu}:\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x, \overline{\mathsf{h}}(x))]\big{\}}\subset\overline{\mathcal{H}}\) such that \(\mathcal{S}_{\mu}(x,\cdot)=\frac{e^{\overline{h}_{\mu}(x,\cdot)}}{\sum_{y^{ \prime}\in\mathcal{Y}\cup\{0\}}e^{\overline{h}_{\mu}(x,y^{\prime})}}\) take the following values:

\[\mathcal{S}_{\mu}(x,y)=\begin{cases}\mathcal{S}(x,y)&\text{if }y\notin\big{\{}y_{ \max},\overline{\mathsf{h}}(x)\big{\}}\\ \mathcal{S}(x,y_{\max})+\mu&\text{if }y=\overline{\mathsf{h}}(x)\\ \mathcal{S}(x,\overline{\mathsf{h}}(x))-\mu&\text{if }y=y_{\max}.\end{cases}\]Note that \(\mathcal{S}_{\mu}\) satisfies the constraint:

\[\sum_{y\neq\mathcal{Y}\cup\{0\}}\mathcal{S}_{\mu}(x,y)=\sum_{y\neq\mathcal{Y}\cup \{0\}}\mathcal{S}(x,y)=1,\,\forall\,\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S} (x,\overline{\mathsf{h}}(x))].\]

Let \(\overline{h}\in\overline{\mathcal{H}}\) be a hypothesis such that \(\overline{\mathsf{h}}(x)\neq y_{\max}\). By the definition and using the fact that \(\overline{\mathsf{H}}(x)=\mathcal{Y}\cup\{0\}\) when \(\mathcal{H}\) is symmetric, we obtain

\[\Delta\mathcal{C}_{\ell_{\mathrm{gee}},\overline{\mathcal{H}}} \big{(}\overline{h},x\big{)}\] \[=\mathcal{C}_{\ell_{\mathrm{gee}}}\big{(}\overline{h},x\big{)}- \mathcal{C}^{*}_{\ell_{\mathrm{gee}}}\big{(}\overline{\mathcal{H}},x\big{)}\] \[\geq\mathcal{C}_{\ell_{\mathrm{gee}}}\big{(}\overline{h},x\big{)} -\inf_{\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x,\overline{\mathsf{h}}(x) )]}\mathcal{C}_{\ell_{\mathrm{gee}}}\big{(}\overline{h}_{\mu},x\big{)}\] \[=\frac{1}{\alpha}\sup_{\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S }(x,\overline{\mathsf{h}}(x))]}\bigg{\{}p(x,y_{\max})\Big{[}-\mathcal{S}(x,y_ {\max})^{\alpha}+\big{(}\mathcal{S}(x,\overline{\mathsf{h}}(x))-\mu\big{)}^{ \alpha}\Big{]}\] \[\qquad+p(x,\overline{\mathsf{h}}(x))\big{[}-\mathcal{S}(x, \overline{\mathsf{h}}(x))^{\alpha}+\big{(}\mathcal{S}(x,y_{\max})+\mu\big{)}^{ \alpha}\Big{]}\bigg{\}}.\]

Differentiating with respect to \(\mu\) yields the optimal value

\[\mu^{*}=\frac{p(x,\overline{\mathsf{h}}(x))^{\frac{1}{1-\alpha}} \mathcal{S}(x,\overline{\mathsf{h}}(x))-p(x,y_{\max})^{\frac{1}{1-\alpha}} \mathcal{S}(x,y_{\max})}{p(x,y_{\max})^{\frac{1}{1-\alpha}}+p(x,\overline{ \mathsf{h}}(x))^{\frac{1}{1-\alpha}}}.\]

Plugging that value in the inequality gives:

\[\Delta\mathcal{C}_{\ell_{\mathrm{gee}},\overline{\mathcal{H}}} \big{(}\overline{h},x\big{)}\geq\frac{1}{\alpha}\] \[\qquad-\frac{1}{\alpha}p(x,y_{\max})\mathcal{S}(x,y_{\max})^{ \alpha}-\frac{1}{\alpha}p(x,\overline{\mathsf{h}}(x))\mathcal{S}(x,\overline{ \mathsf{h}}(x))^{\alpha}.\]

Differentiating with respect to \(\mathcal{S}\) to show that the minimum is attained for \(\mathcal{S}(x,\overline{\mathsf{h}}(x))=\mathcal{S}(x,y_{\max})=\frac{1}{n+1}\), which implies

\[\Delta\mathcal{C}_{\ell_{\mathrm{gee}},\overline{\mathcal{H}}} \big{(}\overline{h},x\big{)}\geq\frac{1}{\alpha(n+1)^{\alpha}}\bigg{[}2^{ \alpha}\Big{(}p(x,y_{\max})^{\frac{1}{1-\alpha}}+p(x,\overline{\mathsf{h}}(x) )^{\frac{1}{1-\alpha}}\Big{)}^{1-\alpha}-p(x,y_{\max})-p(x,\overline{\mathsf{h }}(x))\bigg{]}.\]

By using the fact that for all \(a,b\in[0,1]\), \(0\leq a+b\leq 1\), we have \(\bigg{(}\frac{a^{\frac{1}{1-\alpha}}+b^{\frac{1}{1-\alpha}}}{2}\bigg{)}^{1- \alpha}-\frac{a+b}{2}\geq\frac{\alpha}{4}(a-b)^{2}\), thus we can write

\[\Delta\mathcal{C}_{\ell_{\mathrm{gee}},\overline{\mathcal{H}}} \big{(}\overline{h},x\big{)} \geq\frac{\big{(}p(x,\overline{\mathsf{h}}(x))-p(x,y_{\max})\big{)} ^{2}}{2(n+1)^{\alpha}}\] \[=\frac{1}{2(n+1)^{\alpha}}\Big{(}\Delta\mathcal{C}_{\ell_{0-1}, \overline{\mathcal{H}}}\big{(}\overline{h},x\big{)}\Big{)}^{2}.\qquad\quad \text{(by Lemma~{}\ref{lem:g_lemma} and~{}\ref{lem:g_lemma}(x)=\mathcal{Y}\cup\{0\}))}\]

Since the function \(\frac{t^{2}}{2(n+1)^{\alpha}}\) is convex, by Jensen's inequality, we obtain for any hypothesis \(\overline{h}\in\overline{\mathcal{H}}\) and any distribution,

\[\frac{\Big{(}\mathbb{E}_{X}\Big{[}\Delta\mathcal{C}_{\ell_{0-1},\overline{ \mathcal{H}}}\big{(}\overline{h},x\big{)}\Big{]}\Big{)}^{2}}{2(n+1)^{\alpha}} \leq\underset{X}{\mathbb{E}}\bigg{[}\frac{\Delta\mathcal{C}_{\ell_{0-1}, \overline{\mathcal{H}}}\big{(}\overline{h},x\big{)}^{2}}{2(n+1)^{\alpha}} \bigg{]}\leq\underset{X}{\mathbb{E}}\Big{[}\Delta\mathcal{C}_{\ell_{\mathrm{gee}}, \overline{\mathcal{H}}}\big{(}\overline{h},x\big{)}\Big{]}\]

which leads to

\[\mathcal{E}_{\ell_{0-1}}\big{(}\overline{h}\big{)}-\mathcal{E}^{*}_{\ell_{0-1}} \big{(}\overline{\mathcal{H}}\big{)}\leq\sqrt{2(n+1)^{\alpha}}\Big{(}\mathcal{E }_{\ell_{\mathrm{gee}}}\big{(}\overline{h}\big{)}-\mathcal{E}^{*}_{\ell_{\mathrm{ gee}}}\big{(}\overline{\mathcal{H}}\big{)}+\mathcal{M}_{\ell_{\mathrm{gee}}} \big{(}\overline{\mathcal{H}}\big{)}\Big{)}^{\frac{1}{2}}-\mathcal{M}_{\ell_{0-1}} \big{(}\overline{\mathcal{H}}\big{)}.\]

### Mean absolute error loss

**Theorem 12** (\(\overline{\mathcal{H}}\)-consistency bound for mean absolute error loss).: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any \(\lambda\in\mathbb{R}\), hypothesis \(\overline{h}\in\overline{\mathcal{H}}\) and any distribution,_

\[\mathcal{E}_{\ell_{0-1}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{0-1}}^{* }\big{(}\overline{\mathcal{H}}\big{)}\leq(n+1)\big{(}\mathcal{E}_{\ell_{\max} }\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{\max}}^{*}\big{(}\overline{ \mathcal{H}}\big{)}+\mathcal{M}_{\ell_{\max}}\big{(}\overline{\mathcal{H}} \big{)}\big{)}-\mathcal{M}_{\ell_{0-1}}\big{(}\overline{\mathcal{H}}\big{)}.\]

Proof.: For the mean absolute error loss \(\ell_{\max}\), the conditional \(\ell_{\max}\)-risk can be expressed as follows:

\[\mathcal{E}_{\ell_{\max}}\big{(}\overline{h},x\big{)}\big{)}=\sum_{y\in \mathcal{Y}\cup\{0\}}p(x,y)\Bigg{(}1-\frac{e^{\overline{h}(x,y)}}{\sum_{y^{ \prime}\in\mathcal{Y}\cup\{0\}}e^{\overline{h}(x,y^{\prime})}}\Bigg{)}=\sum_{y \in\mathcal{Y}\cup\{0\}}p(x,y)\big{(}1-\mathcal{S}(x,y)\big{)}\]

where we let \(\mathcal{S}(x,y)=\frac{e^{\overline{h}(x,y)}}{\sum_{y^{\prime}\in\mathcal{Y} \cup\{0\}}e^{\overline{h}(x,y^{\prime})}}\in[0,1]\) for any \(y\in\mathcal{Y}\cup\{0\}\) with the constraint that \(\sum_{y\in\mathcal{Y}\cup\{0\}}\mathcal{S}(x,y)=1\). Let \(y_{\max}=\operatorname*{argmax}_{y\in\mathcal{Y}\cup\{0\}}p(x,y)\), where we choose the label with the same deterministic strategy for breaking ties as that of \(\overline{h}(x)\). For any \(\overline{h}\in\mathcal{H}\) such that \(\overline{h}(x)\neq y_{\max}\) and \(x\in\mathcal{X}\), by the symmetry and completeness of \(\mathcal{H}\), we can always find a family of hypotheses \(\big{\{}\overline{h}_{\mu}:\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x, \overline{h}(x))]\big{\}}\subset\overline{\mathcal{H}}\) such that \(\mathcal{S}_{\mu}(x,\cdot)=\frac{e^{\overline{h}_{\mu}(x,\cdot)}}{\sum_{y^{ \prime}\neq y_{0}\cup\{0\}}e^{\overline{h}_{\mu}(x,y^{\prime})}}\) take the following values:

\[\mathcal{S}_{\mu}(x,y)=\begin{cases}\mathcal{S}(x,y)&\text{if }y\notin\big{\{}y_{ \max},\overline{h}(x)\big{\}}\\ \mathcal{S}(x,y_{\max})+\mu&\text{if }y=\overline{h}(x)\\ \mathcal{S}(x,\overline{h}(x))-\mu&\text{if }y=y_{\max}.\end{cases}\]

Note that \(\mathcal{S}_{\mu}\) satisfies the constraint:

\[\sum_{y\in\mathcal{Y}\cup\{0\}}\mathcal{S}_{\mu}(x,y)=\sum_{y\neq y\in \mathcal{Y}\cup\{0\}}\mathcal{S}(x,y)=1,\;\forall\mu\in[-\mathcal{S}(x,y_{ \max}),\mathcal{S}(x,\overline{h}(x))].\]

Let \(\overline{h}\in\overline{\mathcal{H}}\) be a hypothesis such that \(\overline{h}(x)\neq y_{\max}\). By the definition and using the fact that \(\overline{\text{H}}(x)=\mathcal{Y}\cup\{0\}\) when \(\mathcal{H}\) is symmetric, we obtain

\[\Delta\mathcal{E}_{\ell_{\max},\overline{\mathcal{H}}}\big{(} \overline{h},x\big{)}\] \[=\mathcal{E}_{\ell_{\max}}\big{(}\overline{h},x\big{)}-\mathcal{ E}_{\ell_{\max}}^{*}\big{(}\overline{\mathcal{H}},x\big{)}\] \[\geq\mathcal{E}_{\ell_{\max}}\big{(}\overline{h},x\big{)}-\inf_{ \mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x,\overline{h}(x))]}\mathcal{E} _{\ell_{\max}}\big{(}\overline{h}_{\mu},x\big{)}\] \[=\sup_{\mu\in[-\mathcal{S}(x,y_{\max}),\mathcal{S}(x,\overline {h}(x))]}\Bigg{\{}p(x,y_{\max})\big{[}-\mathcal{S}(x,y_{\max})+\mathcal{S}(x, \overline{h}(x))-\mu\big{]}\] \[\qquad+p\big{(}x,\overline{h}(x)\big{)}\big{[}-\mathcal{S}(x, \overline{h}(x))+\mathcal{S}(x,y_{\max})+\mu\big{]}\Bigg{\}}.\]

Differentiating with respect to \(\mu\) yields the optimum value \(\mu^{*}=-\mathcal{S}(x,y_{\max})\). Plugging that value in the inequality gives:

\[\Delta\mathcal{E}_{\ell_{\max},\overline{\mathcal{H}}}\big{(}\overline{h},x \big{)}\geq p(x,y_{\max})\mathcal{S}(x,\overline{h}(x))-p(x,\overline{h}(x)) \mathcal{S}(x,\overline{h}(x)).\]

Differentiating with respect to \(\mathcal{S}\) to show that the minimum is attained for \(\mathcal{S}(x,\overline{h}(x))=\frac{1}{n+1}\), which implies

\[\Delta\mathcal{E}_{\ell_{\max},\overline{\mathcal{H}}}\big{(} \overline{h},x\big{)} \geq\frac{1}{n+1}\big{(}p(x,y_{\max})-p(x,\overline{h}(x))\big{)}\] \[=\frac{1}{n+1}\Big{(}\Delta\mathcal{E}_{\ell_{0-1},\overline{ \mathcal{H}}}\big{(}\overline{h},x\big{)}\Big{)}.\qquad\qquad\text{ (by Lemma \ref{lemma:bound} and $\overline{\text{H}}(x)=\mathcal{Y}\cup\{0\}$)}\]

Therefore, we obtain for any hypothesis \(\overline{h}\in\overline{\mathcal{H}}\) and any distribution,

\[\frac{\mathbb{E}_{X}\Big{[}\Delta\mathcal{E}_{\ell_{0-1},\overline{\mathcal{H}}} \big{(}\overline{h},x\big{)}\Big{]}}{n+1}\leq\mathbb{E}_{X}\Big{[}\Delta \mathcal{E}_{\ell_{\max},\overline{\mathcal{H}}}\big{(}\overline{h},x\big{)} \Big{]},\]

which leads to

\[\mathcal{E}_{\ell_{0-1}}\big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{0-1}}^{*} \big{(}\overline{\mathcal{H}}\big{)}\leq(n+1)\big{(}\mathcal{E}_{\ell_{\max}} \big{(}\overline{h}\big{)}-\mathcal{E}_{\ell_{\max}}^{*}\big{(}\overline{\mathcal{H }}\big{)}+\mathcal{M}_{\ell_{\max}}\big{(}\overline{\mathcal{H}}\big{)}\big{)}- \mathcal{M}_{\ell_{0-1}}\big{(}\overline{\mathcal{H}}\big{)}.\]Proof of realizable consistency for score-based two-stage surrogate losses (Theorem 5)

**Theorem 5** (**Realizable \(\mathcal{H}\)-consistency for score-based two-stage surrogates**).: _Assume that \(\mathcal{H}\) is closed under scaling and \(c_{j}(x,y)=\beta_{j},\forall(x,y)\in\mathcal{X}\times\mathcal{Y}\). Let \(\ell_{1}\) and \(\ell_{2}\) be the logistic loss. Let \(\hat{h}_{p}\) be the minimizer of \(\mathcal{E}_{\ell_{1}}\) and \(\hat{h}_{d}\) be the minimizer of \(\mathcal{E}_{\mathcal{L}_{\hat{h}_{p}}}\) such that \(\mathcal{E}_{\mathcal{L}_{\hat{h}_{p}}}(\hat{h}_{d})=\min_{h}\mathcal{E}_{ \mathcal{L}_{h_{p}}}(h_{d})\). Then, the following equality holds for any (\(\mathcal{H}\), \(\mathcal{R}\)) -realizable distribution,_

\[\mathcal{E}_{\mathcal{L}_{\mathrm{def}}}(\hat{h})=0,\text{ where }\hat{h}=( \hat{h}_{p},\hat{h}_{d}).\]

Proof.: First, by definition, it is straightforward to see that for any \(h,x,y\), \(\mathcal{L}_{h_{p}}(h_{d},x,y)\) upper bounds the deferral loss \(\mathcal{L}_{\mathrm{def}}\). Consider a data distribution and costs under which there exists \(h^{*}\in\mathcal{H}\) such that \(\mathcal{E}_{\mathcal{L}_{\mathrm{def}}}(h^{*})=0\).

Let \(\hat{h}_{p}\) be the minimizer of \(\mathcal{E}_{\ell_{1}}\) and \(\hat{h}_{d}\) the minimizer of \(\mathcal{E}_{\mathcal{L}_{\hat{h}_{p}}}\) Then, using the fact that \(\mathcal{L}_{h}\) upper bounds the deferral loss \(\mathcal{L}_{\mathrm{def}}\), we have \(\mathcal{E}_{\mathcal{L}_{\mathrm{def}}}(\hat{h})\leq\mathcal{E}_{\mathcal{L} _{\hat{h}_{p}}}(\hat{h}_{d})\).

Next we analyze two cases. If for a point \(x\), deferral occurs, that is there exists \(j^{*}\in[n_{e}]\), such that \(h^{*}(x)=n+j^{*}\), then we must have \(c_{j^{*}}=0\) for all \(x\) since the data is realizable and \(c_{j^{*}}\) is constant. Therefore, there exists an optimal \(h^{*+}\) deferring all the points to the \(j^{*}\)th expert. Then, by the assumption that \(\mathcal{H}\) is closed under scaling and the Lebesgue dominated convergence theorem, for \(\ell_{2}\) being the logistic loss, \(\mathcal{E}_{\mathcal{L}_{\mathrm{def}}}(\hat{h})\leq\mathcal{E}_{\mathcal{L} _{\hat{h}_{p}}}(\hat{h}_{d})\leq\lim_{\tau\rightarrow+\infty}\mathcal{E}_{ \mathcal{L}_{h^{*}_{p}}}(\tau h^{*+}_{d})=0\), where we used the fact that in the limit of \(\tau\rightarrow+\infty\) the logistic loss term \(\ell_{2}(\overline{h}^{*+}_{d},x,j)\) corresponding to \(j\neq j^{*}\) is zero.

On the other hand, if no deferral occurs for any point, that is \(h^{*}(x)\in[n]\) for any \(x\), then we must have \(\mathds{1}_{h^{*}_{p}(x)*y}=0\) for all \((x,y)\) since the data is realizable. Using the fact that \(\mathcal{H}\) is closed under scaling and that the logistic loss is realizable \(\mathcal{H}\)-consistent in the standard classification, we obtain \(\mathds{1}_{\hat{h}_{p}(x)*y}=0\) for all \((x,y)\). Then, by the assumption that \(\mathcal{H}\) is closed under scaling and the Lebesgue dominated convergence theorem, for \(\ell_{2}\) being the logistic loss, \(\mathcal{E}_{\mathcal{L}_{\mathrm{def}}}(\hat{h})\leq\mathcal{E}_{\mathcal{L} _{\hat{h}_{p}}}(\hat{h}_{d})\leq\lim_{\tau\rightarrow+\infty}\mathcal{E}_{ \mathcal{L}_{h^{*}_{p}}}(\tau h^{*}_{d})=0\), where we used the fact that in the limit of \(\tau\rightarrow+\infty\) the logistic loss term \(\ell_{2}(\overline{h}^{*}_{d},x,j)\) corresponding to \(j\neq 0\) is zero.

Therefore, the optimal solution from minimizing score-based two-stage surrogates leads to a zero error solution of the deferral loss, which proves that the score-based two-stage surrogate loss is realizable consistent. 

Appendix G Proof of \((\mathcal{H},\mathcal{R})\)-consistency bounds for predictor-rejector two-stage surrogate losses (Theorem 6)

**Theorem 6** (\((\mathcal{H},\mathcal{R})\)-consistency bounds for predictor-rejector two-stage surrogates).: _Assume that \(\ell_{1}\) admits an \(\mathcal{H}\)-consistency bound and \(\ell_{2}\) admits an \(\overline{\mathcal{R}}\)-consistency bound with respect to the multi-class zero-one classification loss \(\ell_{0-1}\) respectively. Thus, there are non-decreasing concave functions \(\Gamma_{1}\) and \(\Gamma_{2}\) such that, for all \(h\in\mathcal{H}\) and \(\overline{r}\in\overline{\mathcal{R}}\), we have_

\[\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}^{*}_{\ell_{0-1}}( \mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\Gamma_{1}\big{(} \mathcal{E}_{\ell_{1}}(h)-\mathcal{E}^{*}_{\ell_{1}}(\mathcal{H})+\mathcal{M}_ {\ell_{1}}(\mathcal{H})\big{)}\] \[\mathcal{E}_{\ell_{0-1}}(\overline{r})-\mathcal{E}^{*}_{\ell_{0- 1}}(\overline{\mathcal{R}})+\mathcal{M}_{\ell_{0-1}}(\overline{\mathcal{R}}) \leq\Gamma_{2}\big{(}\mathcal{E}_{\ell_{2}}(\overline{r})-\mathcal{E}^{*}_{ \ell_{2}}(\overline{\mathcal{R}})+\mathcal{M}_{\ell_{2}}(\overline{\mathcal{R}}) \big{)}.\]

_Then, the following holds for all \(h\in\mathcal{H}\) and \(r\in\mathcal{R}\):_

\[\mathcal{E}_{\mathcal{L}_{\mathrm{def}}}(h,r)-\mathcal{E}^{*}_{ \mathcal{L}_{\mathrm{def}}}(\mathcal{H},\mathcal{R})+\mathcal{M}_{\mathcal{L} _{\mathrm{def}}}(\mathcal{H},\mathcal{R})\] \[\leq\Gamma_{1}\big{(}\mathcal{E}_{\ell_{1}}(h)-\mathcal{E}^{*}_{ \ell_{1}}(\mathcal{H})+\mathcal{M}_{\ell_{1}}(\mathcal{H})\big{)}+\left(1+ \sum_{j=1}^{n_{e}}\overline{c}_{j}\right)\Gamma_{2}\bigg{(}\frac{\mathcal{E}_{ \mathcal{L}_{h}}(r)-\mathcal{E}^{*}_{\mathcal{L}_{h}}(\mathcal{R})+\mathcal{M}_ {\mathcal{L}_{h}}(\mathcal{R})}{\sum_{j=1}^{n_{e}}\mathcal{c}_{j}}\bigg{)},\]

_where the constant factors \(\left(1+\sum_{j=1}^{n_{e}}\overline{c}_{j}\right)\) and \(\frac{1}{\sum_{j=1}^{n_{e}}\overline{c}_{j}}\) can be removed when \(\Gamma_{2}\) is linear._

[MISSING_PAGE_FAIL:28]

Therefore, by (7), we obtain

\[\mathcal{E}_{\mathsf{L}_{\mathsf{def}}}(h,r)-\mathcal{E}_{\mathsf{L}_{ \mathsf{def}}}^{*}(\mathcal{H},\mathcal{R})+\mathcal{M}_{\mathsf{L}_{\mathsf{ def}}}(\mathcal{H},\mathcal{R})\] \[\leq\begin{cases}\mathbb{E}_{X}[\Gamma_{2}(\Delta\mathcal{E}_{ \mathsf{L}_{h},\mathcal{R}}(r,x))]+\mathbb{E}_{X}[\Gamma_{1}(\Delta\mathcal{E} _{\ell}(h,x))]&\text{when $\Gamma_{2}$ is linear}\\ \big{(}1+\sum_{j=1}^{n_{n}}\bar{\varepsilon}_{j}\big{)}\,\mathbb{E}_{X}\Big{[} \Gamma_{2}\Big{(}\frac{\Delta\mathcal{E}_{\mathsf{L}_{h},\mathcal{R}}(r,x)}{ \sum_{j=1}^{n_{n}}\bar{\varepsilon}_{j}}\Big{)}\Big{]}+\mathbb{E}_{X}[\Gamma_ {1}(\Delta\mathcal{E}_{\ell}(h,x))]&\text{otherwise}\end{cases}\] \[\leq\begin{cases}\Gamma_{2}\big{(}\mathbb{E}_{X}[\Gamma_{2}(\Delta \mathcal{E}_{\mathsf{L}_{h},\mathcal{R}}(r,x))]\big{)}+\Gamma_{1}(\mathbb{E}_ {X}[\Delta\mathcal{E}_{\ell}(h,x)]\big{)}&\text{when $\Gamma_{2}$ is linear}\\ \big{(}1+\sum_{j=1}^{n_{n}}\bar{\varepsilon}_{j}\big{)}\Gamma_{2}\Big{(}\mathbb{ E}_{X}\Big{[}\frac{\Delta\mathcal{E}_{\mathsf{L}_{h},\mathcal{R}}(r,x)}{ \sum_{j=1}^{n_{n}}\bar{\varepsilon}_{j}}\Big{]}\Big{)}+\Gamma_{1}(\mathbb{E}_{X }[\Delta\mathcal{E}_{\ell}(h,x)])&\text{otherwise}\end{cases}\] \[=\begin{cases}\Gamma_{1}(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell} ^{*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))+\Gamma_{2}\big{(}\mathcal{E }_{\mathsf{L}_{h}}(r)-\mathcal{E}_{\mathsf{L}_{h}}^{*}(\mathcal{R})+\mathcal{M }_{\mathsf{L}_{h}}(\mathcal{R})\big{)}&\text{when $\Gamma_{2}$ is linear}\\ \Gamma_{1}(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H})+\mathcal{M }_{\ell}(\mathcal{H}))+\big{(}1+\sum_{j=1}^{n_{n}}\bar{\varepsilon}_{j}\big{)} \Gamma_{2}\Big{(}\frac{\mathcal{E}_{\mathsf{L}_{h}}(r)-\mathcal{E}_{\mathsf{L }_{h}}^{*}(\mathcal{R})+\mathcal{M}_{\mathsf{L}_{h}}(\mathcal{R})}{\sum_{j=1} ^{n_{n}}\bar{\varepsilon}_{j}}\Big{)}&\text{otherwise},\end{cases}\]

which completes the proof. 

Appendix H Proof of realizable consistency for predictor-rejector two-stage surrogate losses (Theorem 7)

**Theorem 7** (**Realizable \((\mathcal{H},\mathcal{R})\)-consistency for predictor-rejector two-stage surrogates**).: _Assume that \(\mathcal{H}\) and \(\mathcal{R}\) is closed under scaling and \(c_{j}(x,y)=\beta_{j},\forall(x,y)\in\mathcal{X}\times\mathcal{Y}\). Let \(\ell_{1}\) and \(\ell_{2}\) be the logistic loss. Let \(\hat{h}\) be the minimizer of \(\mathcal{E}_{\ell_{1}}\) and \(\hat{r}\) be the minimizer of \(\mathcal{E}_{\mathsf{L}_{\hat{h}}}\). Then, the following holds for any \((\mathcal{H},\mathcal{R})\) -realizable distribution,_

\[\mathcal{E}_{\mathsf{L}_{\mathsf{def}}}(\hat{h},\hat{r})=0.\]

Proof.: First, by definition, it is straightforward to see that for any \(h,r,x,y\), \(\mathsf{L}_{h}(r,x,y)\) upper bounds the deferral loss \(\mathsf{L}_{\mathsf{def}}\). Consider a data distribution and costs under which there exists \(h^{*}\in\mathcal{H}\) and \(r^{*}\in\mathcal{R}\) such that \(\mathcal{E}_{\mathsf{L}_{\mathsf{def}}}(h^{*},r^{*})=0\).

Let \(\hat{h}\) be the minimizer of \(\mathcal{E}_{\ell_{1}}\) and \(\hat{r}\) the minimizer of \(\mathcal{E}_{\mathsf{L}_{\hat{h}}}\) Then, using the fact that \(\mathsf{L}_{h}\) upper bounds the deferral loss \(\mathsf{L}_{\mathsf{def}}\), we have \(\mathcal{E}_{\mathsf{L}_{\mathsf{def}}}(\hat{h},\hat{r})\leq\mathcal{E}_{ \mathsf{L}_{\hat{h}}}(\hat{r})\).

Next we analyze two cases. If for a point \(x\), deferral occurs, that is there exists \(j^{*}\in[n_{e}]\), such that \(r^{*}(x)=j^{*}\), then we must have \(c_{j^{*}}=0\) for all \(x\) since the data is realizable and \(c_{j^{*}}\) is constant. Therefore, there exists an optimal \(r^{**}\) deferring all the points to the \(j^{*}\)th expert. Then, by the assumption that \(\mathcal{R}\) is closed under scaling and the Lebesgue dominated convergence theorem, for \(\ell_{2}\) being the logistic loss, \(\mathcal{E}_{\mathsf{L}_{\mathsf{def}}}(\hat{h},\hat{r})\leq\mathcal{E}_{ \mathsf{L}_{\hat{h}}}(\hat{r})\leq\lim_{\tau\to+\infty}\mathcal{E}_{\mathsf{L}_ {\hat{h}}}(\tau r^{**})=0\), where we used the fact that in the limit of \(\tau\to+\infty\) the logistic loss term \(\ell_{2}(\overline{r}^{**},x,j)\) corresponding to \(j\neq j^{*}\) is zero.

On the other hand, if no deferral occurs for any point, that is \(r^{*}(x)=0\) for any \(x\), then we must have \(\mathds{1}_{h^{*}(x)\neq y}=0\) for all \((x,y)\) since the data is realizable. Using the fact that \(\mathcal{H}\) is closed under scaling and that the logistic loss is realizable \(\mathcal{H}\)-consistent in the standard classification, we obtain \(\mathds{1}_{\hat{h}(x)\neq y}=0\) for all \((x,y)\). Then, by the assumption that \(\mathcal{R}\) is closed under scaling and the Lebesgue dominated convergence theorem, for \(\ell_{2}\) being the logistic loss, \(\mathcal{E}_{\mathsf{L}_{\mathsf{def}}}(\hat{h},\hat{r})\leq\mathcal{E}_{ \mathsf{L}_{\hat{h}}}(\hat{r})\leq\lim_{\tau\to+\infty}\mathcal{E}_{\mathsf{L}_ {\hat{h}}}(\tau r^{*})=0\), where we used the fact that in the limit of \(\tau\to+\infty\) the logistic loss term \(\ell_{2}(\overline{r}^{*},x,j)\) corresponding to \(j\neq 0\) is zero.

Therefore, the optimal solution from minimizing predictor-rejector two-stage surrogates leads to a zero error solution of the deferral loss, which proves that the predictor-rejector two-stage surrogate loss is realizable consistent.