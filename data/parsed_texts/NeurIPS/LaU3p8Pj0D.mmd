# Unifying Causal Representation Learning with the Invariance Principle

Dingling Yao, Dario Rancati, Riccardo Cadei, Marco Fumero, and Francesco Locatello

Institute of Science and Technology Austria

###### Abstract

Causal representation learning aims at recovering latent causal variables from high-dimensional observations to solve causal downstream tasks, such as predicting the effect of new interventions or more robust classification. A plethora of methods have been developed, each tackling carefully crafted problem settings that lead to different types of identifiability. The folklore is that these different settings are important, as they are often linked to different rungs of Pearl's causal hierarchy, although not all neatly fit. Our main contribution is to show that _many existing causal representation learning approaches methodologically align the representation to known data symmetries_. Identification of the variables is guided by equivalence classes across different "data pockets" that are not necessarily causal. This result suggests important implications, allowing us to unify many existing approaches in a single method that can mix and match different assumptions, including non-causal ones, based on the invariances relevant to our application. It also significantly benefits applicability, which we demonstrate by improving treatment effect estimation on real-world high-dimensional ecological data. Overall, this paper clarifies the role of causality assumptions in the discovery of causal variables and shifts the focus to preserving data symmetries.

**Editors:** Marco Fumero, Clementine Domine, Zorah Lahner, Donato Crisostomi, Luca Moschella, Kimberly Stachenfeld

## 1 Introduction

Causal representation learning (CRL) [1] posits that many real-world high-dimensional perceptual data can be described through a simplified latent structure specified by a few interpretable low-dimensional causally-related variables. Many existing approaches in causal representation learning carefully formulate their problem settings to guarantee identifiability [2; 3; 4; 5; 6; 7]. However, some CRL works may not perfectly fit within this causal language framework; for instance, the problem setting of temporal CRL works [8; 9; 10; 11],domain generalization [12; 13; 14] and certain multi-task learning approaches [15; 16] are sometimes framed as informally related to causal representation learning. This has resulted in a variety of methods and findings, some of which rely on assumptions that are not always tailored for practical, real-world applications [17]. This paper contributes a unified rephrasing of many existing nonparametric CRL works through the lens of invariance. We observe that _many existing causal representation approaches share methodological similarities, particularly in aligning the representation with known data symmetries_, while differing primarily in how the invariance principle is invoked. We highlight our contributions as follows:

* We propose a unified rephrasing for existing nonparametric CRL approaches leveraging the invariance principles and prove latent variable identifiability in this general setting (SS 3). We show that 31 existing identification results can be seen as special cases directly implied by our framework (Tab. 2).

* We formalize different definitions of "identifiability" highlighting their connections, and demonstrating how they can be addressed within our framework (App. C.1)
* We analyze the case of partial graph identification, drawing a distinction between the causal assumptions necessary for graph discovery and those required for variable discovery (App. C.2).
* Our framework is broadly applicable across a range of practical settings, with improved results on real-world experimental ecology data using the causal inference benchmark of [17] (SS 4). We demonstrate that existing methods only require a form of distributional invariance for identification, without needing access to interventions (App. F).

## 2 Problem Setting

This section defines our problem setting using standard CRL concepts and assumptions (Formal definitions are deferred to App. B, a comprehensive summary of notations is provided in App. A). While prior works in CRL typically categorize their settings using established causal language (e.g. 'counterfactual,' 'interventional,' or 'observational'), our approach introduces a more general invariance principle that aims to unify diverse problem settings. We introduce the following concepts as mathematical tools to describe our data generating process.

**Definition 2.1** (Invariance property).: Let \(A\subseteq[N]\) be an index subset of the Euclidean space \(\mathbb{R}^{N}\) and let \(\sim_{\iota}\) be an equivalence relationship on \(\mathbb{R}^{|A|}\), with \(A\) of known dimension. Let \(\mathcal{M}:=\mathbb{R}^{|A|}\left/\sim_{\iota}\right.\) be the quotient of \(\mathbb{R}^{|A|}\) under this equivalence relationship; \(\mathcal{M}\) is a topological space equipped with the quotient topology. Let \(\iota:\mathbb{R}^{|A|}\to\mathcal{M}\) be the projection onto the quotient induced by the equivalence relationship \(\sim_{\iota}\). This projection \(\iota\) is termed the _invariant property_ of this equivalence relation. Two vectors \(\mathbf{a},\mathbf{b}\in\mathbb{R}^{|A|}\) are invariant under \(\iota\) if and only if they belong to the same \(\sim_{\iota}\) equivalence class, i.e.: \(\iota(\mathbf{a})=\iota(\mathbf{b})\Leftrightarrow\mathbf{a}\sim_{\iota} \mathbf{b}\). and \(\iota(\mathbf{z}_{A})=\iota(\tilde{\mathbf{z}}_{A})\Leftrightarrow\mathbf{z}_ {A}\sim_{\iota}\tilde{\mathbf{z}}_{A}\).

Extending this definition to the whole latent space \(\mathbb{R}^{N}\), a pair of latents \(\mathbf{z},\tilde{\mathbf{z}}\in\mathbb{R}^{N}\) are _non-trivially invariant on a subset \(A\subseteq[N]\) under the property \(\iota\)_ only if (i) the invariance property \(\iota\) holds on the indices \(A\subseteq[N]\) in the sense that \(\iota(\mathbf{z}_{A})=\iota(\tilde{\mathbf{z}}_{A})\); (ii) for any smooth function \(h_{1},h_{2}:\mathbb{R}^{N}\to\mathbb{R}^{|A|}\), the invariance property between \(\mathbf{z},\tilde{\mathbf{z}}\)_breaks_ under the \(h_{1},h_{2}\) transformations if \(h_{1}\) or \(h_{2}\)_directly_ depends on some other component \(\mathbf{z}_{q}\) with \(q\in[N]\setminus A\). Taking \(h_{1}\) and \(\mathbf{z}\) as an example, we have:

\[\exists q\in[N]\setminus A,\mathbf{z}^{*}\in\mathbb{R}^{N},\quad s.t.\ \frac{\partial h_{1}}{\partial\mathbf{z}_{q}}(\mathbf{z}^{*})\text{ exists and is non zero }\quad\Rightarrow\quad\iota(h_{1}(\mathbf{z}))\neq\iota(h_{2}(\tilde{ \mathbf{z}}))\]

i.e. given that the partial derivative of \(h_{1}\) w.r.t. some latent variable \(\mathbf{z}_{q}\in\mathbf{z}_{[N]\setminus A}\) is non-zero at some point \(\mathbf{z}^{*}\in\mathbb{R}^{N}\), \(h_{1}(\mathbf{z}),h_{2}(\mathbf{z})\) violates invariance principle in the sense that \(\iota(h_{1}(\mathbf{z}))\neq\iota(h_{2}(\tilde{\mathbf{z}}))\).

We denote by \(\mathcal{S}_{\mathbf{z}}:=\{\mathbf{z}^{1},\dots,\mathbf{z}^{K}\}\) the set of latent random vectors with \(\mathbf{z}^{k}\in\mathbb{R}^{N}\) and write its joint distribution as \(P_{\mathcal{S}_{\mathbf{z}}}\). The joint distribution \(P_{\mathcal{S}_{\mathbf{z}}}\) has a probability density \(p_{\mathcal{S}_{\mathbf{z}}}(z^{1},\dots,z^{K})\). Each individual random vector \(\mathbf{z}^{k}\in\mathcal{S}_{\mathbf{z}}\) follows the marginal density \(p_{\mathbf{z}^{k}}\) with the non-degenerate support \(\mathcal{Z}^{k}\subseteq\mathbb{R}^{N}\), whose interior is a non-empty open set of \(\mathbb{R}^{N}\).

**Definition 2.2** (Observable of a set of latent random vectors).: Consider a set of random vectors \(\mathcal{S}_{\mathbf{z}}:=\{\mathbf{z}^{1},\dots,\mathbf{z}^{K}\}\) with \(\mathbf{z}^{k}\in\mathbb{R}^{N}\), the corresponding set of observables \(\mathcal{S}_{\mathbf{x}}:=\{\mathbf{x}^{1},\dots,\mathbf{x}^{K}\}\) is generated by \(\mathcal{S}_{\mathbf{x}}=F(\mathcal{S}_{\mathbf{z}})\), where the map \(F\) defines a push-forward measure \(F_{\#}(P_{\mathcal{S}_{\mathbf{z}}})\) on the image of \(F\) as: \(F_{\#}(P_{\mathcal{S}_{\mathbf{z}}})(x_{1},\dots,x_{K})=P_{\mathcal{S}_{\mathbf{ z}}}(f_{1}^{-1}(x_{1}),\dots,f_{K}^{-1}(x_{K}))\) with the support \(\mathcal{X}:=\text{Im}(F)\subseteq\mathbb{R}^{K\times D}\). Note that \(F\) satisfies the diffeomorphism assumption (Asm. B.1) as each \(f_{k}\) is a diffeomorphism onto its image according to Asm. B.1.

In the following, we denote by \(\mathfrak{I}:=\{\iota_{i}:\mathbb{R}^{|A_{i}|}\to\mathcal{M}_{i}\}\) a finite set of invariance properties with their respective invariant subsets \(A_{i}\subseteq[N]\) and their equivalence relationships \(\sim_{\iota_{i}}\), each inducing as a projection onto its quotient and invariant property \(\iota_{i}\) (Defn. 2.1). For a set of observables \(\mathcal{S}_{\mathbf{x}}:=\{\mathbf{x}^{1},\dots,\mathbf{x}^{K}\}\in\mathcal{X}\) generated from the data generating process described in SS 2, we assume:

**Assumption 2.1**.: For each \(\iota_{i}\in\mathfrak{I}\), there exists a _unique known_ index subset \(V_{i}\subseteq[K]\) with at least two elements (i.e., \(|V_{i}|\geq 2\)) s.t. \(\mathbf{x}_{V_{i}}=F([\mathbf{z}]_{\sim_{\iota_{i}}})\) forms the set of observables generated from an equivalence class \([\mathbf{z}]_{\sim_{\iota_{i}}}:=\{\tilde{\mathbf{z}}\in\mathbb{R}^{N}: \mathbf{z}_{A_{i}}\sim_{\iota_{i}}\tilde{\mathbf{z}}_{A_{i}}\}\), as given by Defn. 2.2. In particular, if \(\mathfrak{I}=\{\iota\}\) consists of a single invariance property \(\iota:\mathbb{R}^{|A|}\to\mathcal{M}\), we have \(\mathcal{S}_{\mathbf{x}}=F([\mathbf{z}]_{\sim_{\iota}})\).

**Remark:** While \(\mathfrak{I}\) does not need to be fully described, which observables should belong to the same equivalence class is known (denoted as \(V_{i}\subseteq[K]\) for the invariance property \(\iota_{i}\in\mathfrak{I}\)). Thisis a standard assumption and is equivalent to knowing e.g., two views are generated from partially overlapped latents [18].

Given a set of observables \(\mathcal{S}_{\mathbf{x}}\in\mathcal{X}\) satisfying Asm. 2.1, we show that we can simultaneously identify multiple invariant latent blocks \(A_{i}\) under a set of weak assumptions. In the best case, if each individual latent component is represented as a single invariant block through individual invariance property \(\iota_{i}\in\mathfrak{I}\), we can learn a fully disentangled representation and further identify the latent causal graph by additional technical assumptions.

## 3 Identifiability Theory via the Invariance Principle

**High-level overview.** This section presents a general theory for latent variable identification that brings together many identifiability results from existing CRL works, including multiview, interventional, temporal, and multitask CRL. Our theory of latent variable identifiability, based on the invariance principle, consists of two key components: (1) ensuring the encoder's sufficiency, thereby obtaining an adequate representation of the original input for the desired task; (2) guaranteeing the learned representation to preserve known data symmetries as invariance properties. The sufficiency is often enforced by minimizing the reconstruction loss [8; 9; 10; 19; 20] in auto-encoder based architecture, maximizing the log likelihood in normalizing flows or maximizing entropy [21; 22; 23; 18] in contrastive-learning based approaches. The invariance property in the learned representations is often enforced by minimizing some equivalence relation-induced pseudometric between a pair of encodings [6; 10; 18; 22] or by some iterative algorithm that provably ensures the invariance property on the output [24; 25]. As a result, all invariant blocks \(A_{i},i\in[n_{\mathfrak{I}}]\) can be identified up to a mixing within the blocks while being disentangled from the rest. This type of identifiability is defined as _block-identifiability_[22] which we restate as follows:

**Definition 3.1** (Block-identifiability [22]).: A subset of latent variable \(\mathbf{z}_{A}:=\{\mathbf{z}_{j}\}_{j\in A}\) with \(A\subseteq[N]\) is block-identified by an encoder \(g:\mathbb{R}^{D}\rightarrow\mathbb{R}^{N}\) on the invariant subset \(A\) if the learned representation \(\hat{\mathbf{z}}_{\hat{A}}:=[g(\mathbf{x})]_{\hat{A}}\) with \(\hat{A}\subseteq[N],|A|=|\hat{A}|\) contains all and only information about the ground truth \(\mathbf{z}_{A}\), i.e. \(\hat{\mathbf{z}}_{\hat{A}}=h(\mathbf{z}_{A})\) for some diffeomorphism \(h:\mathbb{R}^{|A|}\rightarrow\mathbb{R}^{|A|}\).

**Definition 3.2** (Encoders).: The encoders \(G:=\{g_{k}:\mathcal{X}^{k}\rightarrow\mathcal{Z}^{k}\}_{k\in[K]}\) consist of smooth functions mapping from the observational support \(\mathcal{X}^{k}\) to the corresponding latent support \(\mathcal{Z}^{k}\) (SS 2).

**Definition 3.3** (Selection [18]).: A selection \(\oslash\) operates between two vectors \(a\in\{0,1\}^{d}\,,b\in\mathbb{R}^{d}\) s.t. \(a\oslash b:=[b_{j}:a_{j}=1,j\in[d]]\).

**Definition 3.4** (Invariant block selectors).: The invariant block selectors \(\Phi:=\{\phi^{(i,k)}\}_{i\in[n_{\mathfrak{I}}],k\in V_{i}}\) with \(\phi^{(i,k)}\in\{0,1\}^{N}\) perform selection (Defn. 3.3) on the encoded information: for any invariance property \(\iota_{i}\in\mathfrak{I}\), any observable \(\mathbf{x}^{k},k\in V_{i}\) we have the selected representation:

\[\phi^{(i,k)}\oslash\hat{\mathbf{z}}^{k}=\phi^{(i,k)}\oslash g_{k}(\mathbf{x}^{ k})=\left[[g_{k}(\mathbf{x}^{k})]_{j}:\phi^{(i,k)}_{j}=1,j\in[N]\right],\] (3.1)

with \(\left\|\phi^{(i,k)}\right\|_{0}=\|\phi^{(i,k^{\prime})}\|_{0}=|A_{i}|\) for all \(\iota_{i}\in\mathfrak{I},k,k^{\prime}\in V_{i}\).

**Constraint 3.1** (Invariance constraint).: _For any invariance property \(\iota_{i}\in\mathfrak{I},i\in[n_{\mathfrak{I}}]\), the **selected** representations \(\phi^{(i,k)}\oslash g_{k}(\mathbf{x}^{k}),k\in V_{i}\) must be \(\iota_{i}\)-invariant across the observables from the subset \(V_{i}\subseteq[K]\):_

\[\iota_{i}(\phi^{(i,k)}\oslash g_{k}(\mathbf{x}^{k}))=\iota_{i}(\phi^{(i,k^{ \prime})}\oslash g_{k^{\prime}}(\mathbf{x}^{k^{\prime}}))\quad\forall i\in[n_{ \mathfrak{I}}]\ \forall k,k^{\prime}\in V_{i}\] (3.2)

**Constraint 3.2** (Sufficiency constraint).: _For any \(\iota_{i}\in\mathfrak{I},i\in[n_{\mathfrak{I}}]\), the **selected** representation \(\phi^{(i,k)}\oslash g_{k}(\mathbf{x}^{k}),k\in V_{i}\) must preserve all information of the invariant partition \(\mathbf{z}_{A_{i}}\) that we aim to identify, i.e., \(I(\mathbf{z}_{A_{i}},\phi^{(i,k)}\oslash g_{k}(\mathbf{x}^{k}))=H(\mathbf{z}_{ A_{i}})\ \forall i\in[n_{\mathfrak{I}}],k\in V_{i}\)._

**Theorem 3.1** (Identifiability of multiple invariant blocks).: _Consider a set of observables \(\mathcal{S}_{\mathbf{x}}=\{\mathbf{x}^{1},\mathbf{x}^{2},\ldots,\mathbf{x}^{K}\}\) with \(\mathbf{x}^{k}\in\mathcal{X}^{k}\) generated from SS 2 satisfying Asm. 2.1. Let \(G,\Phi\) be the set of smooth encoders (Defn. 3.2) and selectors (Defn. 3.4) that satisfy Constraints 3.1 and 3.2, then the invariant component \(\mathbf{z}^{k}_{A_{i}}\) is block-identified (Defn. 3.1) by \(\phi^{(i,k)}\oslash g_{k}\) for all \(\iota_{i}\in\mathfrak{I},k\in[K]\)._

**What about the variant latents?** Intuitively, the variant latents are generally not identifiable, as the invariance constraint (Constraint 3.1) is applied only to the selected invariant encodings, leaving the variant part without any weak supervision [26]. This result is formalized as follows:

**Proposition 3.2** (General non-identifiability of variant latent variables).: _Consider the setup in Thm. 3.1, let \(A:=\bigcup_{i\in[n\setminus g]}A_{i}\) denote the union of block-identified latent indices and \(A^{c}:=[N]\setminus A\) the complementary set where no \(\iota\)-invariance \(\iota\in\mathfrak{I}\) applies, then the variant latents \(\mathbf{z}_{A^{c}}\) cannot be identified._

Although variant latent variables are generally non-identifiable, they can be identified under certain conditions. The following demonstrates that variant latent variables can be identified under invertible encoders when the variant and invariant partitions are mutually independent.

**Proposition 3.3** (Identifiability of variant latent under independence).: _Consider an optimal encoder \(g\in G^{*}\) and optimal selector \(\phi\in\Phi^{*}\) from Thm. 3.1 that jointly identify an invariant block \(\mathbf{z}_{A}\) (we omit subscriptions \(k,i\) for simplicity), then \(\mathbf{z}_{A^{c}}(A^{c}:=[N]\setminus A)\) can be identified by the complementary encoding partition \((1-\phi)\odot g\) only if: (i) \(g\) is invertible in the sense that \(I(\mathbf{x},g(\mathbf{x}))=H(\mathbf{x})\); (ii) \(\mathbf{z}_{A^{c}}\) is independent on \(\mathbf{z}_{A}\)._

## 4 Experiments

This section demonstrates the real-world applicability of causal representation learning under the invariance principle, evidenced by superior treatment effect estimation performance on the high-dimensional causal inference benchmark [17] using a loss for the domain generalization literature that utilizes the invariance principle [13] (SS 4). Additionally, we provide ablation studies on existing interventional causal representation learning methods [2; 3; 27], showcasing that non-trivial distributional invariance is needed for latent variable identification. This distributional invariance could, but does not have to arise from a valid intervention in the sense of causality (App. F).

**Case Study: ISTAnt** This experiment focuses on ISTAnt [17], a recent real-world ecological benchmark designed for treatment effect estimation. ISTAnt consists of video recordings of ants triplets with occasional grooming behavior. The goal is to extract a per-frame representation for supervised behavior classification (grooming or not) to estimate the Average Treatment Effect of an intervention (exposure to a chemical substance). Further details about this dataset and problem setting is provided in App. G.1

**Experiment settings.** Different videos in ISTAnt are considered different _experiments_ as the experiment settings and treatments vary. We consider hard annotation sampling criteria (more non-annotated than annotated) for both experiments (videos) and positions, as described by [17]. For the training, we adopt a domain generalization objective that utilizes the invariance principle [13], which is restated as follows:

\[\mathcal{R}_{\text{V-REx}}(\mathbf{w}\circ g)=\underbrace{\lambda_{\text{ INV}}\text{Var}(\{\mathcal{R}_{1}(\mathbf{w}\circ g),\ldots,\mathcal{R}_{K}( \mathbf{w}\circ g)\})}_{\text{invariance}}+\underbrace{\sum_{k\in[K]}\mathcal{R}_{k}( \mathbf{w}\circ g)}_{\text{sufficiency}}.\] (4.1)

We vary the strength of the invariant component in eq. 4.1 by setting the invariance regularization multiplier \(\lambda_{\text{INV}}\) from 0 (ERM) to 10 000. We repeat 20 independent runs for each \(\lambda_{\text{INV}}\) to estimate the statistical error. All other implementational details follow [17]. We evaluate the performance with both _balanced accuracy_ and _Treatment Effect Relative Bias_ (TERB). TERB is defined in [17] as the ratio between the bias in the predictions across treatment groups and the true average treatment effect estimated with ground-truth annotations over the whole trial.

**Results.** Fig. 1 depicts the model performance regarding varying invariance regularization strength \(\lambda_{\text{INV}}\). As expected, the balanced accuracy initially increases with the invariance regularization strength \(\lambda_{\text{INV}}\), as our prediction problem benefits from the invariance, until the sufficiency component is not sufficiently balanced with the invariance, and performance decreases. Similarly, the TERB improves positively, weighting the invariance component until a certain threshold. In particular, on average with \(\lambda_{\text{INV}}=100\) the TERB decreases to 20% (from 100% using ERM) with experiment subsampling. In agreement with [17], a naive estimate of the TEB on a small validation set is a reasonable (albeit not perfect) model selection criterion. Although it performs slightly worse than model selection based on ERM loss in the position sampling case, it proves to be more reliable overall. This experiment underscores the advantages of flexibly enforcing known invariances in the data, corroborating our identifiability theory (SS 3).

## 5 Conclusions

In this paper, we take a closer look at the wide range of CRL methods. Interestingly, we find many CRL approaches share methodological similarities in aligning the representation to known data symmetrics. We identified two components involved in identifiability results: preserving information of the data and a set of known invariances (SS 3). Our results help clarify the role of causal assumptions in causal variable identification, shifting the focus from a characterization of specific assumptions for identifiability, which are not necessarily satisfied in real-world scenarios, to a general recipe that allows practitioners to specify known invariances in their problem and learn representations that align with them. We successfully exemplified the real-world applicability of CRL on ecological data, as shown in SS 4. Nevertheless, our paper leaves out certain settings concerning identifiability that may be interesting for future work, such as discrete variables and finite sample guarantees.

## Acknowledgements

We thank Jiaqi Zhang, Francesco Montagna, David Lopez-Paz, Kartik Ahuja, Thomas Kipf, Sara Magliacane, Julius von Kugelgen, Kun Zhang, and Bernhard Scholkopf for extremely helpful discussion. Riccardo Cadei was supported by a Google Research Scholar Award to Francesco Locatello. We acknowledge the Third Bellairs Workshop on Causal Representation Learning held at the Bellairs Research Institute, February 9/16, 2024, and a debate on the difference between interventions and counterfactuals in disentanglement and CRL that took place during Dhanya Sridhar's lecture, which motivated us to significantly broaden the scope of the paper. We thank Dhanya and all participants of the workshop.

## References

* [1] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021.
* [2] Julius von Kugelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Kekic, Elias Bareinboim, David Blei, and Bernhard Scholkopf. Nonparametric identifiability of causal representations from unknown interventions. _Advances in Neural Information Processing Systems_, 36, 2024.
* [3] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning. In _International Conference on Machine Learning_, pages 372-407. PMLR, 2023.
* [4] Johann Brehmer, Pim De Haan, Phillip Lippe, and Taco S Cohen. Weakly supervised causal representation learning. _Advances in Neural Information Processing Systems_, 35:38319-38331, 2022.
* [5] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Scholkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. _Advances in Neural Information Processing Systems_, 36, 2024.
* [6] Jiaqi Zhang, Kristjan Greenewald, Chandler Squires, Akash Srivastava, Karthikeyan Shannugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft

Figure 1: TERB and Balanced Accuracy with standard deviation over 20 different seeds varying the invariance weight \(\lambda_{\text{INV}}\) of V-REx [13] on ISTAnt dataset [17]. With stars, the TERB of the model is selected by different model selection criteria on a small but heterogeneous validation set.

interventions. _Advances in Neural Information Processing Systems_, 36, 2024.
* [7] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, and Ali Tajer. General identifiability and achievability for causal representation learning. In _International Conference on Artificial Intelligence and Statistics_, pages 2314-2322. PMLR, 2024.
* [8] Sebastien Lachapelle, Rodriguez Lopez, Pau, Yash Sharma, Katie E. Everett, Remi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In _First Conference on Causal Learning and Reasoning_, 2022.
* [9] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M Asano, Taco Cohen, and Efstratios Gavves. Causal representation learning for instantaneous and temporal effects in interactive systems. In _The Eleventh International Conference on Learning Representations_, 2022.
* [10] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M Asano, Taco Cohen, and Stratis Gavves. Citris: Causal identifiability from temporal intervened sequences. In _International Conference on Machine Learning_, pages 13557-13603. PMLR, 2022.
* [11] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M Asano, Taco Cohen, and Efstratios Gavves. Biscuit: Causal representation learning from binary interactions. In _Uncertainty in Artificial Intelligence_, pages 1263-1273. PMLR, 2023.
* [12] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _arXiv preprint arXiv:1911.08731_, 2019.
* [13] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In _International conference on machine learning_, pages 5815-5826. PMLR, 2021.
* [14] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization, 2022.
* [15] Sebastien Lachapelle, Tristan Deleu, Divyat Mahajan, Ioannis Mitliagkas, Yoshua Bengio, Simon Lacoste-Julien, and Quentin Bertrand. Synergies between disentanglement and sparsity: Generalization and identifiability in multi-task learning. In _International Conference on Machine Learning_, pages 18171-18206. PMLR, 2023.
* [16] Marco Fumero, Florian Wenzel, Luca Zancato, Alessandro Achille, Emanuele Rodola, Stefano Soatto, Bernhard Scholkopf, and Francesco Locatello. Leveraging sparse and shared feature activations for disentangled representation learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [17] Riccardo Cadei, Lukas Lindorfer, Sylvia Cremer, Cordelia Schmid, and Francesco Locatello. Smoke and mirrors in causal downstream tasks. _Advances in Neural Information Processing Systems_, 37, 2024.
* [18] Dingling Yao, Danru Xu, Sebastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg Martius, Julius von Kugelgen, and Francesco Locatello. Multi-view causal representation learning with partial observability. In _The Twelfth International Conference on Learning Representations_, 2023.
* [19] Francesco Locatello, Ben Poole, Gunnar Raetsch, Bernhard Scholkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 6348-6359. PMLR, 13-18 Jul 2020.
* [20] Kartik Ahuja, Jason S Hartford, and Yoshua Bengio. Weakly supervised representation learning with sparse perturbations. _Advances in Neural Information Processing Systems_, 35:15516-15528, 2022.

* [21] Roland S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. Contrastive learning inverts the data generating process. In _International Conference on Machine Learning_, pages 12979-12990. PMLR, 2021.
* [22] Julius von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. _Advances in neural information processing systems_, 34:16451-16467, 2021.
* [23] Imant Dauhnawer, Alice Bizeul, Emanuele Palumbo, Alexander Marx, and Julia E Vogt. Identifiability results for multimodal contrastive learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* [24] Chandler Squires, Anna Seigal, Salil S. Bhate, and Caroline Uhler. Linear causal disentanglement via interventions. In _International Conference on Machine Learning_, volume 202, pages 32540-32560. PMLR, 2023.
* [25] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, and Ali Tajer. Linear causal representation learning from unknown multi-node interventions. _arXiv preprint arXiv:2406.05937_, 2024.
* [26] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In _international conference on machine learning_, pages 4114-4124. PMLR, 2019.
* [27] Liang Wendong, Armin Kekic, Julius von Kugelgen, Simon Buchholz, Michel Besserve, Luigi Gresele, and Bernhard Scholkopf. Causal component analysis. _Advances in Neural Information Processing Systems_, 36, 2024.
* [28] Judea Pearl. _Causality_. Cambridge university press, 2009.
* [29] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlinear causal discovery with additive noise models. _Advances in neural information processing systems_, 21, 2008.
* [30] Kun Zhang and Aapo Hyvarinen. Distinguishing causes from effects using nonlinear acyclic causal models. In _Causality: Objectives and Assessment_, pages 157-164. PMLR, 2010.
* [31] K Zhang and A Hyvarinen. On the identifiability of the post-nonlinear causal model. In _25th Conference on Uncertainty in Artificial Intelligence (UAI 2009)_, pages 647-655. AUAI Press, 2009.
* [32] P Rubenstein, S Weichwald, S Bongers, J Mooij, D Janzing, M Grosse-Wentrup, and B Scholkopf. Causal consistency of structural equation models. In _33rd Conference on Uncertainty in Artificial Intelligence (UAI 2017)_, pages 808-817. Curran Associates, Inc., 2017.
* [33] Sander Beckers and Joseph Y Halpern. Abstracting causal models. In _Proceedings of the aaai conference on artificial intelligence_, volume 33, pages 2678-2685, 2019.
* [34] Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions. _Journal of the American Statistical Association_, 100(469):322-331, 2005.
* [35] Yuejiang Liu, Alexandre Alahi, Chris Russell, Max Horn, Dominik Zietlow, Bernhard Scholkopf, and Francesco Locatello. Causal triplet: An open challenge for intervention-centric causal representation learning. In _Conference on Causal Learning and Reasoning_, pages 553-573. PMLR, 2023.
* [36] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score-based causal representation learning with interventions. _arXiv preprint arXiv:2301.08230_, 2023.
* [37] Kartik Ahuja, Amin Mansouri, and Yixin Wang. Multi-domain causal representation learning via weak distributional invariances. In _International Conference on Artificial Intelligence and Statistics_, pages 865-873. PMLR, 2024.

* [38] Kun Zhang, Shaoan Xie, Ignavier Ng, and Yujia Zheng. Causal representation learning from multiple distributions: A general setting. _Internatinal Conference on Machine Learning_, 2024.
* [39] Weiran Yao, Guangyi Chen, and Kun Zhang. Temporally disentangled representation learning. _Advances in Neural Information Processing Systems_, 35:26492-26503, 2022.
* [40] Weiran Yao, Yuewen Sun, Alex Ho, Changyin Sun, and Kun Zhang. Learning temporally causal latent processes from general temporal data. _International Conference on Learning Representations_, 2022.
* [41] Sebastien Lachapelle, Pau Rodriguez Lopez, Yash Sharma, Katie Everett, Remi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Nonparametric partial disentanglement via mechanism sparsity: Sparse actions, interventions and sparse temporal dependencies. _arXiv preprint arXiv:2401.04890_, 2024.
* [42] Zijian Li, Ruichu Cai, Zhenhui Yang, Haiqin Huang, Guangyi Chen, Yifan Shen, Zhengming Chen, Xiangchen Song, Zhifeng Hao, and Kun Zhang. When and how: Learning identifiable latent states for nonstationary time series forecasting. _arXiv preprint arXiv:2402.12767_, 2024.
* [43] Zijian Li, Yifan Shen, Kaitao Zheng, Ruichu Cai, Xiangchen Song, Mingming Gong, Zhifeng Hao, Zhengmao Zhu, Guangyi Chen, and Kun Zhang. On the identification of temporally causal representation with instantaneous dependence. _arXiv preprint arXiv:2405.15325_, 2024.
* [44] Thomas Dean and Keiji Kanazawa. A model for reasoning about persistance and causation. In _Computational Intelligence_, page 5, 1989.
* [45] Kevin Patrick Murphy. _Dynamic bayesian networks: representation, inference and learning_. University of California, Berkeley, 2002.
* [46] David A Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. In _International Conference on Learning Representations_, 2021.
* [47] Ilyes Khemakhem, Ricardo Monti, Diederik Kingma, and Aapo Hyvarinen. Ice-beem: Identifiable conditional energy-based deep models based on nonlinear ica. _Advances in Neural Information Processing Systems_, 33:12768-12778, 2020.
* [48] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [49] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _International conference on machine learning_, pages 1530-1538. PMLR, 2015.
* [50] Rich Caruana. Multitask learning. _Machine learning_, 28:41-75, 1997.
* [51] Yu Zhang and Qiang Yang. An overview of multi-task learning. _National Science Review_, 5(1):30-43, 2018.
* [52] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng Dai. Uni-perceiver-moe: Learning sparse generalist models with conditional moes. _Advances in Neural Information Processing Systems_, 35:2664-2678, 2022.
* [53] Jinze Bai, Rui Men, Hao Yang, Xuancheng Ren, Kai Dang, Yichang Zhang, Xiaohuan Zhou, Peng Wang, Sinan Tan, An Yang, et al. Ofasys: A multi-modal multi-task learning system for building generalist models. _arXiv preprint arXiv:2212.04408_, 2022.
* [54] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* [55] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. _Journal of machine learning research_, 17(59):1-35, 2016.
* [56] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization, 2020.
** [57] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 1657-1664, 2013.
* [58] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In _Proceedings of the IEEE international conference on computer vision_, pages 5542-5550, 2017.
* [59] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5018-5027, 2017.
* [60] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In _Proceedings of the European conference on computer vision (ECCV)_, pages 456-473, 2018.
* [61] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1406-1415, 2019.
* [62] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. _Advances in neural information processing systems_, 24, 2011.
* [63] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(4):4396-4415, 2022.
* [64] Yujia Zheng, Ignavier Ng, and Kun Zhang. On the identifiability of nonlinear ica: Sparsity and beyond. _Advances in neural information processing systems_, 35:16411-16422, 2022.
* [65] Danru Xu, Dingling Yao, Sebastien Lachapelle, Perouz Taslakian, Julius von Kugelgen, Francesco Locatello, and Sara Magliacane. A sparsity principle for partially observable causal representation learning. _Forty-first International Conference on Machine Learning_, 2024.
* [66] Bohdan Kivva, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Identifiability of deep generative models without auxiliary information. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 15687-15701. Curran Associates, Inc., 2022.
* [67] James M Robins, Miguel Angel Hernan, and Babette Brumback. Marginal structural models and causal inference in epidemiology. _Epidemiology_, 11(5):550-560, 2000.
* [68] Jonathan M Samet, Francesca Dominici, Frank C Curriero, Ivan Coursac, and Scott L Zeger. Fine particulate air pollution and mortality in 20 us cities, 1987-1994. _New England journal of medicine_, 343(24):1742-1749, 2000.
* [69] Egbert H Van Nes, Marten Scheffer, Victor Brovkin, Timothy M Lenton, Hao Ye, Ethan Deyle, and George Sugihara. Causal feedbacks in climate change. _Nature Climate Change_, 5(5):445-448, 2015.
* [70] Jakob Runge. Modern causal inference approaches to investigate biodiversity-ecosystem functioning relationships. _nature communications_, 14(1):1917, 2023.
* [71] Dingling Yao, Caroline Muller, and Francesco Locatello. Marrying causal representation learning with dynamical systems for science. _Advances in Neural Information Processing Systems_, 37, 2024.
* [72] Jonas Peters, Joris M Mooij, Dominik Janzing, and Bernhard Scholkopf. Causal discovery with continuous additive noise models. _Journal of Machine Learning Research_, 2014.
* [73] Dominik Janzing and Sergio Hernan Garrido Mejia. A phenomenological account for causality in terms of elementary actions. _Journal of Causal Inference_, 12(1):20220076, 2024.
* [74] Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. _Elements of Causal Inference: Foundations and Learning Algorithms_. The MIT Press, 2017. ISBN 0262037319.

* [75] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do different neural networks learn the same representations? _arXiv preprint arXiv:1511.07543_, 2015.
* [76] Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodola. Relative representations enable zero-shot latent space communication. _International Conference on Learning Representations_, 2022.
* [77] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In _International conference on machine learning_, pages 3519-3529. PMLR, 2019.
* [78] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. _arXiv preprint arXiv:2405.07987_, 2024.
* [79] Elliot Creager, Joern-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 2189-2200. PMLR, 18-24 Jul 2021.
* [80] Md Rifat Arefin, Yan Zhang, Aristide Baratin, Francesco Locatello, Irina Rish, Dianbo Liu, and Kenji Kawaguchi. Unsupervised concept discovery mitigates spurious correlations. In _Forty-first International Conference on Machine Learning_, 2024.
* [81] Mohammad Pezeshki, Diane Bouchacourt, Mark Ibrahim, Nicolas Ballas, Pascal Vincent, and David Lopez-Paz. Discovering environments with xrm. In _Forty-first International Conference on Machine Learning_, 2024.
* [82] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. _IEEE Signal Processing Magazine_, 34(4):18-42, 2017.
* [83] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv preprint arXiv:2104.13478_, 2021.
* [84] Taco Cohen and Max Welling. Group equivariant convolutional networks. In _International conference on machine learning_, pages 2990-2999. PMLR, 2016.
* [85] Taco S Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical cnns. _arXiv preprint arXiv:1801.10130_, 2018.
* [86] Marco Fumero, Luca Cosmo, Simone Melzi, and Emanuele Rodola. Learning disentangled representations via product manifold projection. In _International conference on machine learning_, pages 3530-3540. PMLR, 2021.
* [87] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. Towards a definition of disentangled representations. _arXiv preprint arXiv:1812.02230_, 2018.
* [88] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _nature_, 596(7873):583-589, 2021.

## Appendix

### Table of Contents

* A Notation and Terminology
* B Preliminaries
* C Identifiability Theory
* C.1 On the granularity of identification
* C.2 Identifying the causal graph
* D Related Works
* D.1 Multiview Causal Representation Learning
* D.2 Multi-environment Causal Representation Learning
* D.3 Temporal Causal Representation Learning
* D.4 Multi-task Causal Representation Learning
* D.5 Domain Generalization
* D.6 Further Explanations for Tab. 2
* D.7 Notable Cases Not Directly Covered by the Theory
* E Proofs
* E.1 Assumption Justification
* E.2 Proof for Thm. 3.1
* E.3 Proofs for Generalization of Variant Latents
* E.4 Proofs for Granularity of Latent Variable Identification
* F Synthetic Ablation with "Ninterventions"
* G Implementation Details
* G.1 Case Study: ISTAnt
* G.2 Synthetic Ablation with "Ninterventions"
* H Further Discussions and Connections to Other Fields
* H.1 Representational Alignment and Platonic Representation
* H.2 Environment Discovery
* H.3 Connection with Geometric Deep Learning

### Appendix A Notation and Terminology

\([N]\) is used as a shorthand for \(\{1,\ldots,N\}\). We use bold lower-case \(\mathbf{z}\) for random vectors and normal lower-case \(z\) for their realizations. A vector \(\mathbf{z}\) can be indexed either by a single index \(i\in[\dim(\mathbf{z})]\) via \(\mathbf{z}_{i}\) or a index subset \(A\subseteq[\dim(\mathbf{z})]\) with \(\mathbf{z}_{A}:=\{\mathbf{z}_{i}:i\in A\}\). \(P_{\mathbf{z}}\) denotes the probability distribution of the random vector \(\mathbf{z}\) and \(p_{\mathbf{z}}(z)\) denotes the associated probability density function. By default, a "measurable" function is _measurable_ w.r.t. the Borel sigma algebras and defined w.r.t. the Lebesgue measure. A comprehensive list of notation follows:

\[f\] Mixing function \[g\] Smooth encoder \[\mathcal{G}\] Ground truth causal graph \[\mathbf{x}\] Entangled observables \[\mathbf{z}\] Ground truth latent variables \[D\] Dimensionality of observable \[\mathbf{x}\]

\begin{tabular}{l l} \(N\) & Dimensionality of latents \(\mathbf{z}\) \\ \(A\) & Subset of latent indices with invariance properties (\(A\subseteq[N]\)) \\ \(\iota\) & Projector which maps the latents to the space where the invariance property holds \\ \(\sim_{\iota}\) & The latent equivalence relation \\ \(\mathfrak{I}\) & A set of invariance properties \\ \(\mathcal{X}\) & Support of a set of observables \(\mathcal{S}_{\mathbf{x}}\) \\ \(\mathcal{Z}\) & Support of a set of latent vectors \(\mathcal{S}_{\mathbf{z}}\) \\ \(G\) & A set of smooth encoders \\ \(\Phi\) & A set of selectors \\ \(\mathrm{TC}\) & Transitive closure \\ \end{tabular}

## Appendix B Preliminaries

In this subsection, we revisit the common definitions and assumptions in identifiability works from causal representation learning. We begin with the definition of a latent structural causal model:

**Definition B.1** (Latent SCM [2]).: Let \(\mathbf{z}=\{\mathbf{z}_{1},\ldots,\mathbf{z}_{N}\}\) denote a set of causal "endogenous" variables with each \(\mathbf{z}_{i}\) taking values in \(\mathbb{R}\), and let \(\mathbf{u}=\{\mathbf{u}_{1},\ldots,\mathbf{u}_{N}\}\) denotes a set of mutually independent "exogenous" random variables. The latent SCM consists of a set of structural equations

\[\{\mathbf{z}_{i}:=m_{i}(\mathbf{z}_{\mathbf{pa}(i)}),\mathbf{u}_{i}\}_{i=1}^{N},\] (B.1)

where \(\mathbf{z}_{\mathbf{pa}(i)}\) are the causal parents of \(\mathbf{z}_{i}\) and \(m_{i}\) are the deterministic functions that are termed "causal mechanisms". We indicate with \(P_{\mathbf{u}}\) the joint distribution of the exogenous random variables, which due to the independence hypothesis is the product of the probability measures of the individual variables. The associated causal diagram \(\mathcal{G}\) is a directed graph with vertices \(\mathbf{z}\) and edges \(\mathbf{z}_{i}\rightarrow\mathbf{z}_{j}\) iff. \(\mathbf{z}_{i}\in\mathbf{z}_{\mathbf{pa}(j)}\); we assume the graph \(\mathcal{G}\) to be acyclic.

The latent SCM induces a unique distribution \(P_{\mathbf{z}}\) over the endogenous variables \(\mathbf{z}\) as a pushforward of \(P_{\mathbf{u}}\) via eq. (B.1). Its density \(p_{\mathbf{z}}\) follows the causal Markov factorization:

\[p_{\mathbf{z}}(z)=\prod_{i=1}^{N}p_{i}(z_{i}\mid z_{\mathbf{pa}(i)}).\] (B.2)

Instead of directly observing the endogenous and exogenous variables \(\mathbf{z}\) and \(\mathbf{u}\), we only have access to some "entangled" measurements \(\mathbf{x}\) of \(\mathbf{z}\) generated through a nonlinear mixing function:

**Definition B.2** (Mixing function).: A deterministic smooth function \(f:\mathbb{R}^{N}\rightarrow\mathbb{R}^{D}\) mapping the latent vector \(\mathbf{z}\in\mathbb{R}^{N}\) to its observable \(\mathbf{x}\in\mathbb{R}^{D}\), where \(D\geq N\) denotes the dimensionality of the observational space.

**Assumption B.1** (Diffeomorphism).: The mixing function \(f\) is diffeomorphic onto its image, i.e. \(f\) is \(C^{\infty}\), \(f\) is injective and \(f^{-1}|_{\mathcal{I}(f)}:\mathcal{I}(f)\rightarrow\mathbb{R}^{D}\) is also \(C^{\infty}\).

**Remark:** Settings with noisy observations (\(\mathbf{x}=f(\mathbf{z})+\epsilon\), \(\mathbf{z}\perp\epsilon\)) can be easily reduced to our denoised version by applying a standard deconvolution argument as a pre-processing step, as indicated by Buchholz et al. [5], Lachapelle et al. [8].

## Appendix C Identifiability Theory

In addition to the general results for latent variable identification presented in SS 3, we compare in App. C.1 different granularity of latent variable identification and show their transitions through certain assumptions on the causal model or mixing function. Afterward, App. C.2 discusses the identification level of a causal graph depending on the granularity of latent variable identification under certain structural assumptions. Detailed proofs are deferred to App. E.

### On the granularity of identification

Different levels of identification can be achieved depending on the degree of underlying data symmetry. Below, we present three standard identifiability definitions from the CRL literature, each offering stronger identification results than block-identifiability (Defn. 3.1).

**Definition C.1** (Block affine-identifiability).: Let \(\hat{\mathbf{z}}\) be the learned representation, for a subset \(A\subseteq[N]\) it satisfies that:

\[\hat{\mathbf{z}}_{\pi(A)}=D\cdot\mathbf{z}_{A}+\mathbf{b},\] (C.1)

where \(D\in\mathbb{R}^{|A|\times|A|}\) is an invertible matrix, \(\pi(A)\) denotes the index permutation of \(A\), then \(\mathbf{z}_{A}\) is block affine-identified by \(\hat{\mathbf{z}}_{\pi(A)}\).

**Definition C.2** (Element-identifiability).: The learned representation \(\hat{\mathbf{z}}\in\mathbb{R}^{N}\) satisfies that:

\[\hat{\mathbf{z}}=\mathbf{P}_{\pi}\cdot h(\mathbf{z}),\] (C.2)

where \(\mathbf{P}_{\pi}\in\mathbb{R}^{N\times N}\) is a permutation matrix, \(h(\mathbf{z}):=(h_{1}(\mathbf{z}_{1}),\ldots h_{N}(\mathbf{z}_{N}))\in\mathbb{ R}^{N}\) is a is an element-wise diffeomorphism.

**Definition C.3** (Affine-identifiability).: The learned representation \(\hat{\mathbf{z}}\in\mathbb{R}^{N}\) satisfies that:

\[\hat{\mathbf{z}}=\Lambda\cdot\mathbf{P}_{\pi}\cdot\mathbf{z}+\mathbf{b},\] (C.3)

where \(\mathbf{P}_{\pi}\in\mathbb{R}^{N\times N}\) is a permutation matrix, \(\Lambda\in\mathbb{R}^{N\times N}\) is a diagonal matrix with nonzero diagonal entries.

**Remark**: Block affine-identifiability (Defn. C.1) is defined by Ahuja et al. [3], stating that the learned representation \(\hat{\mathbf{z}}\) is related to the ground truth latents \(\mathbf{z}\) through some sparse matrix with zero blocks. Defn. C.2 indicates element-wise identification of latent variables up to individual diffeomorphisms. Element-identifiability for the latent variable identification together with the graph identifiability (Defn. C.4) is defined as \(\sim_{\mathrm{CRL}}\)-identifiability [2, Defn. 2.6], perfect identifiability [7, Defn. 3]. Affine identifiability (Defn. C.3) describes when the ground truth latent variables are identified up to permutation, shift, and linear scaling. In many CRL works, affine identifiability (Defn. C.3) is also termed as follows: perfect identifiability under linear transformation [25, Defn. 1], CD-equivalence [6, Defn. 1], disentanglement [8, Defn. 3].

**Proposition C.1** (Granularity of identification).: _Affine-identifiability (Defn. C.3) implies element-identifiability (Defn. C.2) and block affine-identifiability (Defn. C.1) while element-identifiability and block affine-identifiability implies block-identifiability (Defn. 3.1)._

**Proposition C.2** (Transition between identification levels).: _The transition between different levels of latent variable identification (Fig. 2) can be summarized as follows:_

1. _Element-level identifiability (Defn. C.2 and C.3) can be obtained from block-wise identifiability (Defn._ 3.1 _and_ _C.1) when each individual latent constitutes an invariant block;_
2. _Identifiability up to an affine transformation (Defn._ _C.1 and_ _C.3) can be obtained from general identifiability on arbitrary diffeomorphism (Defn._ _3.1 _and_ _C.2_) by additionally assuming that both the ground truth mixing function and decoder are finite degree polynomials of the same degree._

Figure 2: Relations between different identification classes (Defn. 3.1 and C.1 to C.3). Some CRL works proposed a more fine-grained classification of identifiability concepts with slightly different terminology, which we omit here for readability.

**Discussion.** We note that the granularity of identifiability results is primarily determined by the strength of invariance and parametric assumptions (such as those on mixing functions or causal models) rather than by the specific algorithmic choice. For example, for settings that can achieve element-identifiability [2], affine-identifiability results can be obtained by additionally assuming _finite degree polynomial_ mixing function (proof see App. E). Similarly, one reaches element-identifiability from block-identifiability by enforcing invariance properties on each latent component [18, Thm. 3.8] instead of having only _one_ fat-hand invariant block [22]. Tab. 2 provides an overview of recent identifiability results along with their corresponding invariance and parametric assumptions, illustrating the direct relationship between these assumptions and the level of identifiability they achieve.

### Identifying the causal graph

In addition to latent variable identification, another goal of causal representation learning is to infer the underlying latent dependency, namely the causal graph structure. Hence, we restate the standard definition of graph identifiability in causal representation learning.

**Definition C.4** (Graph-identifiability).: The estimated graph \(\hat{\mathcal{G}}\) is isomorphic to the ground truth \(\mathcal{G}\) through a bijection \(h:V(\mathcal{G})\to V(\hat{\mathcal{G}})\) in the sense that two vertices \(\mathbf{z}_{i},\mathbf{z}_{j}\in V(\mathcal{G})\) are adjacent in \(\mathcal{G}\) if and only if \(h(\mathbf{z}_{i}),h(\mathbf{z}_{j})\in V(\hat{\mathcal{G}})\) are adjacent in \(\hat{\mathcal{G}}\).

We remark that the "faithfulness" assumption [28, Defn. 2.4.1] is a standard assumption in the CRL literature, commonly required for graph discovery. We restate it as follows:

**Assumption C.1** (Faithfulness (or Stability)).: \(P_{\mathbf{z}}\) is a faithful distribution induced by the latent SCM (Defn. B.1) in the sense that \(P_{\mathbf{z}}\) contains no extraneous conditional independence; in other words, the only conditional independence relations satisfied by \(P_{\mathbf{z}}\) are those given by \(\{\mathbf{z}_{i}\perp\mathbf{z}_{\text{nd}(i)}\mid\mathbf{z}_{\text{pa}(i)}\}\) where \(\mathbf{z}_{\text{nd}(i)}\) denotes the non-descends of \(\mathbf{z}_{i}\).

As indicated by Defn. C.4, the preliminary condition of identifying the causal graph is to have an element-wise correspondence between the vertices in the ground truth graph \(\mathcal{G}\) (i.e., the ground truth latents) and the vertices of the estimated graph. Therefore, the following assumes that the learned encoders \(G\) (Defn. 3.2) achieve element-identifiability (Defn. C.2), that is, for each \(\mathbf{z}_{i}\in\mathbf{z}\), we have a differmorphism \(h_{i}:\mathbb{R}\to\mathbb{R}\) such that \(\hat{\mathbf{z}}_{i}=h_{i}(\mathbf{z}_{i})\). However, to identify the graph structure, additional assumptions are needed: either on the source of invariance or on the parametric form of the latent causal model.

**Graph identification via interventions.** Under the element-identifiability (Defn. C.2) of the latent variables \(\mathbf{z}\), the causal graph structure \(\mathcal{G}\) can be identified up to its isomorphism (Defn. C.4), given multi-domain data from _paired perfect_ interventions per-node [2, 7]. Using data generated from _imperfect_ interventions is generally insufficient to identify the direct edges in the causal graph, it can only identify the ancestral relations, i.e., up to the transitive closure of \(\mathcal{G}\)[4, 6]. Unfortunately, even imposing the linear assumption on the latent SCM does not provide a solution [24]. Nevertheless, by adding sparsity assumptions on the causal graph \(\mathcal{G}\) and polynomial assumption on the mixing function \(f\), Zhang et al. [6] has shown isomorphic graph identifiability (Defn. C.4) under _imperfect_ intervention per node. In general, access to the interventions is necessary for graph identification if one is not comfortable making other parametric assumptions about the graph structure. Conveniently, in this setting, the graph identifiability is linked with that of the variables since the latter leverages the invariance induced by the intervention.

**Graph identification via parametric assumptions.** It is well known in causal discovery that the additive noise model [29] is identifiable under certain mild assumptions [30, 31]. In the following, we assume an additive exogenous noise in the latent SCM (Defn. B.1):

**Assumption C.2** (Additive noise).: The endogenous variable \(\mathbf{z}_{i}\in\mathbb{R}\) in the previously defined latent SCM (Defn. B.1) relates to the corresponding exogenous noise variable \(\mathbf{u}_{i}\in\mathbb{R}\) through additivity. Namely, the causal mechanism (eq. (B.1)) can be rewritten as:

\[\{\mathbf{z}_{i}=m_{i}(\mathbf{z}_{\text{pa}(i)})+\mathbf{u}_{i}\}.\] (C.4)

As a generalization of the additive noise model, the post-nonlinear acyclic causal model [30, Sec. 2] allows extra nonlinearity on the top of the additive causal mechanism, providing additional flexibility on the latent model assumption:

**Definition C.5** (Post-nonlinear acyclic causal model).: The following causal mechanism describes a post-nonlinear acyclic causal model:

\[\mathbf{z}_{i}=h_{i}(m_{i}(\mathbf{z}_{\text{pa}(i)})+\mathbf{u}_{i}),\] (C.5)

where \(h_{i}:\mathbb{R}\rightarrow\mathbb{R}\) is a diffeomorphism and \(m_{i}\) is a non-constant function.

Assume the latent variable \(\mathbf{z}_{i}\) is element-wise identified through a bijective mapping \(h_{i}:\mathbb{R}\rightarrow\mathbb{R}\) for all \(i\in[N]\), define the estimated causal parents \(\hat{\mathbf{z}}_{\text{pa}(i)}:=\{h_{j}(\mathbf{z}_{j}):\mathbf{z}_{j}\in \mathbf{z}_{\text{pa}(i)}\}\), then the latent SCM (Defn. B.1) is translated to a post-nonlinear acyclic causal model (Defn. C.5) because

\[\hat{\mathbf{z}}_{i} =h_{i}(\mathbf{z}_{i})=h_{i}(m_{i}(\mathbf{z}_{\text{pa}(i)})+ \mathbf{u}_{i})\] (C.6) \[=h_{i}(m_{i}(\{h_{j}^{-1}(\hat{\mathbf{z}}_{j}):\mathbf{z}_{j}\in \mathbf{z}_{\text{pa}(i)}\})+\mathbf{u}_{i})\] \[=h_{i}(\tilde{m}_{i}(\hat{\mathbf{z}}_{\text{pa}(i)})+\mathbf{u} _{i}),\]

where

\[\tilde{m}_{i}(\hat{\mathbf{z}}_{\text{pa}(i)}):=m_{i}(\{h_{j}^{-1}(\hat{ \mathbf{z}}_{j}):\mathbf{z}_{j}\in\mathbf{z}_{\text{pa}(i)}\}).\]

Thus, the underlying causal graph \(\mathcal{G}\) can be identified up to an isomorphism (Defn. C.4) following the approach given by Zhang and Hyvarinen [31, Sec. 4]

**What happens if variables are identified in blocks?** Consider the case where the latent variables cannot be identified up to element-wise diffeomorphism; instead, one can only obtain a coarse-grained version of the variables (e.g., as a mixing of a block of variables (Defn. 3.1)). Nevertheless, certain causal links between these coarse-grained block variables are of interest. These block variables and their causal relations in between form a "macro" level of the original latent SCM, which is shown to be causally consistent under mild structural assumptions [32, Thm. 11]. In particular, the macro-level model can be obtained from the micro-level model through an _exact transformation_[33, Defn. 3.4] and thus produces the same causal effect as the original micro-level model under the same type of interventions, providing useful knowledge for downstream causal analysis. More formal connections are beyond the scope of this paper. Still, we see this concept of coarse-grained identification on both causal variables and graphs as an interesting avenue for future research.

## Appendix D Related Works

This section reviews related causal representation learning works and frames them as specific instances of our theory (SS 3). These works were originally categorized into various causal representation learning types (multiview, multi-domain, multi-task, and temporal CRL) based on the level of invariance in the data-generating process, leading to varying degrees of identifiability results (App. C.1). While the implementation of individual works may vary, the _methodological principle of aligning representation with known data symmetries_ remain consistent, as shown in SS 3. We begin with revisiting the data-generating process of each category and explain how they can be viewed as specific cases of the proposed invariance framework (SS 2). We then present individual identification algorithms from the CRL literature as particular applications of our theorems based on the implementation choices needed to satisfy the invariance and sufficiency constraints (Constraints 3.1 and 3.2). A more detailed overview of the individual works is provided in Tab. 2.

### Multiview Causal Representation Learning

**High-level overview.** The multiview setting in causal representation learning [18, 23] considers multiple views that are _concurrently_ generated by an overlapping subset of latent variables, and thus having _non-independently_ distributed data. Multiview scenarios are often found in a partially observable setup. For example, multiple devices on a robot measure different modalities, jointly monitoring the environment through these real-time measurements. While each device measures a distinct subset of latent variables, these subsets probably still overlap as they are measuring the same system at the same time. In addition to partial observability, another way to obtain multiple views is to perform an "intervention/perturbation" [4, 19, 20, 22] and collect both pre-action and post-action views on the same sample. This setting is often improperly termed "counterfactual"1 in the CRLliterature, and this type of data is termed "paired data". From another perspective, the paired setting can be cast in the partial observability scenario by considering the same latent before and after an action (mathematically modelled as an intervention) as two separate latent nodes in the causal graph, as shown by von Kugelgen et al. (2022, Fig. 1). Thus, both pre-action and post-action views are partial because neither of them can observe pre-action and post-action latents simultaneously. These works assume that the latents that are not affected by the action remain constant, an assumption that is relaxed in temporal CRL works. See App. D.3 for more discussion in this regard.

**Data generating process.** In the following, we introduce the data-generating process of a multi-view setting in the flavor of the invariance principle as introduced in SS 2. We consider a set of views \(\{\mathbf{x}^{k}\}_{k\in[K]}\) with each view \(\mathbf{x}^{k}\in\mathcal{X}^{k}\) generated from some latents \(\mathbf{z}^{k}\in\mathcal{Z}^{k}\). Let \(S_{k}\subseteq[N]\) be the index set of generating factors for the view \(\mathbf{x}^{k}\), we define \(\mathbf{z}^{k}_{j}=0\) for all \(j\in[N]\setminus S_{k}\) to represent the uninvolved partition of latents. Each entangled view \(\mathbf{x}^{k}\) is generated by a view-specific mixing function \(f_{k}:\mathcal{Z}^{k}\rightarrow\mathcal{X}^{k}\):

\[\mathbf{x}^{k}=f_{k}(\mathbf{z}^{k})\quad\forall k\in[K]\] (D.1)

Define the joint overlapping index set \(A:=\bigcap_{k\in[K]}S_{k}\), and assume \(A\subseteq[N]\) is a non-empty interior of \([N]\). Then the value of the sharing partition \(\mathbf{z}_{A}\) remain _invariant_ for all observables \(\{\mathbf{x}^{k}\}_{k\in[K]}\) on a _sample level_. By considering the joint intersection \(A\), we have _one single_ invariance property \(\iota:\mathbb{R}^{|A|}\rightarrow\mathbb{R}^{|A|}\) in the invariance set \(\mathfrak{I}\); and this invariance property \(\iota\) emerges as the identity map \(\mathrm{id}\) on \(\mathbb{R}^{|A|}\) in the sense that \(\mathrm{id}(\mathbf{z}^{k}_{A})=\mathrm{id}(\mathbf{z}^{k^{\prime}}_{A})\) and thus \(\mathbf{z}^{k}_{A}\sim_{\iota}\mathbf{z}^{k^{\prime}}_{A}\) for all \(k,k^{\prime}\in[K]\). Note that Defn. 2.1 (ii) is satisfied because any transformation \(h_{k}\) that involves other components \(\mathbf{z}_{\mathbf{q}}\) with \(q\notin A\) violates the equity introduced by the identity map. For a subset of observations \(V_{i}\subseteq[K]\) with at least two elements \(|V_{i}|>1\), we define the latent intersection as \(A_{i}:=\bigcap_{k\in V_{i}}\subseteq[N]\), then for each non-empty intersection \(A_{i}\), there is a corresponding invariance property \(\iota_{i}:\mathbb{R}^{|A_{i}|}\rightarrow\mathbb{R}^{|A_{i}|}\) which is the identity map specified on the subspace \(\mathbb{R}^{|A_{i}|}\). By considering all these subsets \(\mathcal{V}:=\{V_{i}\subseteq[K]:|V_{i}|>1,|A_{i}|>0\}\), we obtain a set of invariance properties \(\mathfrak{I}:=\{\iota_{i}:\mathbb{R}^{|A_{i}|}\rightarrow\mathbb{R}^{|A_{i}|}\}\) that satisfy Assm. 2.1.

**Identification algorithms.** Many multiview works [18; 22; 23] employ the \(L_{2}\) loss as a regularizer to enforce _sample-level_ invariance on the invariant partition, cooperated with some sufficiency regularizer to preserve sufficient information about the observables (Constraint 3.2). Aligned with our theory (Thm. 3.1), these works have shown block-identifiability on the invariant partition of the latents across different views. Following the same principle, there are certain variations in the implementations to enforce the invariance principle, e.g. Locatello et al. [19] directly average the learned representations from paired data \(g(\mathbf{x}^{1}),g(\mathbf{x}^{2})\) on the shared coordinates before forwarding them to the decoder; Ahuja et al. [20] enforces \(L_{2}\) alignment up to a learnable sparse perturbation \(\delta\). As each latent component constitutes a single invariant block in the training data, these two works _element-identifies_ (Defn. C.2) the latent variables, as explained by Proposition C.2.

### Multi-environment Causal Representation Learning

**High-level overview.** Multi-environment / interventional CRL considers data generated from multiple environments with respective environment-specific data distributions; hence, the considered data is _independently_ but _non-identically distributed_. In the scope of causal representation learning, multi-environment data is often instantiated through interventions on the latent structured causal model [5; 6; 7; 22; 24; 25; 26]. Recently, several papers attempt to provide a more general identifiability statement where multi-environment data is not necessarily originated from interventions; instead, they can be individual data distributions that preserve certain symmetries, such as marginal invariance or support invariance [37] or sufficient statistical variability [38].

**Data generating process** The following presents the data generating process described in most interventional causal representation learning works. Formally, we consider a set of _non-identically_ distributed data \(\{P_{\mathbf{x}^{k}}\}_{k\in[K]}\) that are collected from multiple environments (indexed by \(k\in[K]\)) with a shared mixing function \(f:\mathbf{x}^{k}=f(\mathbf{z}^{k})\) (Defn. B.2) satisfying Assm. B.1 and a shared latent SCM (Defn. B.1). Let \(k=0\) denote the non-intervened environment and \(\mathcal{I}_{k}\subseteq[N]\) denotes the set of intervened nodes in \(k\)-th environment, the latent distribution \(P_{\mathbf{z}^{k}}\) is associated with the density

\[p_{\mathbf{z}^{k}}(z^{k})=\prod_{j\in\mathcal{I}_{k}}\tilde{p}(z^{k}_{j}\mid z^ {k}_{\text{pa}(j)})\prod_{j\in[N]\setminus\mathcal{I}_{k}}p(z^{k}_{j}\mid z^ {k}_{\text{pa}(j)}),\] (D.2)where we denote by \(p\) the original density and by \(\tilde{p}\) the intervened density. Interventions naturally introduce various distributional invariance that can be utilized for latent variable identification: Under the intervention \(\mathcal{I}_{k}\) in the \(k\)-th environment, we observe that both (1) the marginal distribution of \(\mathbf{z}_{A}\) with \(A:=[N]\setminus\operatorname{TC}(\mathcal{I}_{k})\), with \(\operatorname{TC}\) denoting the transitive closure and (2) the score \([S(\mathbf{z}^{k})]_{A^{\prime}}:=\nabla_{\mathbf{z}_{A}^{k}}\operatorname{ log}\mathbf{p}_{\mathbf{z}^{k}}\) on the subset of latent components \(A^{\prime}:=[N]\setminus\overline{\operatorname{pa}}(\mathcal{I}_{k})\) with \(\overline{\operatorname{pa}}(\mathcal{I}_{k}):=\{j:j\in\mathcal{I}_{k}\cup \operatorname{pa}(\mathcal{I}_{k})\}\) remain _invariant_ across the observational and the \(k\)-th interventional environment. Formally, under intervention \(\mathcal{I}_{k}\), we have

* _Marginal invariance_: \[p_{\mathbf{z}^{0}}(z_{A}^{0})=p_{\mathbf{z}^{k}}(z_{A}^{k})\qquad A:=[N] \setminus\operatorname{TC}(\mathcal{I}_{k});\] (D.3)
* _Score invariance_: \[[S(\mathbf{z}^{0})]_{A^{\prime}}=[S(\mathbf{z}^{k})]_{A^{\prime}}\qquad A^{ \prime}:=[N]\setminus\overline{\operatorname{pa}}(\mathcal{I}_{k}).\] (D.4)

According to our theory Thm. 3.1, we can block-identify both \(\mathbf{z}_{A},\mathbf{z}_{A}^{\prime}\) using these invariance principles (eqs. (D.3) and (D.4)). Since most interventional CRL works assume at least one intervention per node [2, 3, 5, 7, 24, 36], more fine-grained variable identification results, such as element-wise identification (Defn. C.2) or affine-identification (Defn. C.3), can be achieved by combining multiple invariances from these per-node interventions, as we elaborate below.

**Identifiability with one intervention per node.** By applying Thm. 3.1, we demonstrate that latent causal variables \(\mathbf{z}\) can be identified up to element-wise diffeomorphism (Defn. C.2) under single node _imperfect_ intervention per node, given the following assumption.

**Assumption D.1** (Topologically ordered interventional targets).: Specifying Asm. 2.1 in the interventional setting, we assume there are exactly \(N\) environments \(\{k_{1},\dots,k_{N}\}\subseteq[K]\) where each node \(j\in[N]\) undergoes one imperfect intervention in the environment \(k_{j}\in[K]\). The interventional targets \(1\preceq\dots\preceq N\) preserve the topological order, meaning that \(i\preceq j\) only if there is a directed path from node \(i\) to node \(j\) in the underlying causal graph \(\mathcal{G}\).

**Remark:** Asm. D.1 is directly implied by Asm. 2.1 as we need to know which environments fall into the same equivalence class. We believe that identifying the topological order is another subproblem orthogonal to identifying the latent variables, which is often termed "uncoupled/non-aligned problem" [2, 7]. As described by Zhang et al. [6], the topological order of unknown interventional targets can be recovered from single-node imperfect intervention by iteratively identifying the interventions that target the source nodes. This iterative identification process may require additional assumptions on the mixing functions [3, 6, 24, 25, 36] and the latent structured causal model [5, 24], or on the interventions, such as _perfect_ interventions that eliminate parental dependency [7], or the need for two interventions per node [2, 7].

**Corollary D.1**.: _Given \(N\) environments \(\{k_{1},\dots,k_{N}\}\subseteq[K]\) satisfying Asm. D.1, the ground truth latent variables \(\mathbf{z}\) can be identified up to element-wise diffeomorphism (Defn. C.2) by combining both marginal and score invariances (eqs. (D.3) and (D.4)) under our framework (Thm. 3.1)._

Proof.: We consider a coarse-grained version of the underlying causal graph consisting of a block-node \(\mathbf{z}_{[N-1]}\) and the leaf node \(\mathbf{z}_{N}\) with \(\mathbf{z}_{[N-1]}\) causing \(\mathbf{z}_{N}\) (i.e., \(\mathbf{z}_{[N-1]}\to\mathbf{z}_{N}\)). We first select a pair of environments \(V=\{0,k_{N}\}\) consisting of the observational environment and the environment where the leaf node \(\mathbf{z}_{N}\) is intervened upon. According to eq. (D.3), the _marginal invariance_ holds for the partition \(A=[N-1]\), implying identification on \(\mathbf{z}_{[N-1]}\) from Thm. 3.1. At the same time, when considering the set of environments \(V^{\prime}=\{0,k_{1},\dots,k_{N-1}\}\), the leaf node \(N\) is the only component that satisfy _score_ invariance across all environments \(V^{\prime}\), because \(N\) is not the parent of any intervened node (also see [36, Lemma 4]). So here we have another invariant partition \(A^{\prime}=\{N\}\), implying identification on \(\mathbf{z}_{N}\) (Thm. 3.1). By jointly enforcing the marginal and score invariance on \(A\) and \(A^{\prime}\) under a sufficient encoder (Constraint 3.2), we identify both \(\mathbf{z}_{[N-1]}\) as a block and \(\mathbf{z}_{N}\) as a single element. Formally, for the parental block \(\mathbf{z}_{[N-1]}\), we have:

\[\hat{\mathbf{z}}_{[N-1]}^{k}=g_{:N-1}(\mathbf{x}^{k})\qquad\forall k\in\{0,k_{ 1},\dots,k_{N}\}\] (D.5)

where \(g_{:N-1}(\mathbf{x}^{k}):=[g(\mathbf{x}^{k})]_{:N-1}\) relates to the ground truth \(\mathbf{z}_{[N-1]}\) through some diffeomorphism \(h_{[N-1]}:\mathbb{R}^{N-1}\to\mathbb{R}^{N-1}\) (Defn. 3.1). Now, we can remove the leaf node \(N\) as follows: For each environment \(k\in\{0,k_{1},\dots,k_{N-1}\}\), we compute the pushforward of \(P_{\mathbf{x}^{k}}\) using the learned encoder \(g_{N-1}:\mathcal{X}^{k}\to\mathbb{R}^{N-1}\):

\[P_{\mathbf{z}^{k}_{[N-1]}}=g_{\#}(P_{\mathbf{x}^{k}})\]

Note that the estimated representations \(P_{\mathbf{z}^{k}_{[N-1]}}\) can be seen as a new observed data distribution for each environment \(k\) that is generated from the subgraph \(\mathcal{G}_{-N}\) without the leaf node \(N\). Using an iterative argument, we can identify all latent variables element-wise (Defn. C.2), concluding the proof. 

Upon element-wise identification from single-node intervention per node, existing works often provide more fine-grained identifiability results by incorporating other parametric assumptions, either on the mixing functions [3, 6, 36] or the latent causal model [5] or both [24]. This is explained by Proposition C.2, as element-wise identification can be refined to affine-identification (Defn. C.3) given additional parametric assumptions. Note that under this milder setting, the full graph is not identifiable without further assumptions, see [6].

**Identifiability with two interventions per-node** Current literature in interventional CRL targeting the general nonparametric setting [2, 7] typically assumed a pair of _sufficiently different_ perfect interventions per node. Thus, any latent variable \(\mathbf{z}_{j},j\in[N]\), as an interventional target, is _uniquely shared_ by a pair of interventional environment \(k,k^{\prime}\in[K]\), forming an invariant partition \(A_{i}=\{j\}\) constituting of individual latent node \(j\in[N]\). Note that this invariance property on the interventional target induces the following distributional property:

\[[S(\mathbf{z}^{k})-S(\mathbf{z}^{k^{\prime}})]_{j}\neq 0\qquad\text{ only if}\qquad\mathcal{I}_{k}=\mathcal{I}_{k^{\prime}}=\{j\}.\] (D.6)

According to Thm. 3.1, each latent variable can thus be identified separately, giving rise to element-wise identification, as shown by [2, 7].

**Identifiability under multiple distributions.** More recently, Ahuja et al. [37] explains previous interventional identifiability results from a general weak distributional invariance perspective. In a nutshell, a set of variables \(\mathbf{z}_{A}\) can be block-identified if certain invariant distributional properties hold: The invariant partition \(\mathbf{z}_{A}\) can be block-identified (Defn. 3.1) from the rest by utilizing the _marginal distributional invariance_ or _invariance on the support, mean or variance_. Ahuja et al. [37] additionally assume the mixing function to be finite degree polynomial, which leads to block-affine identification (Defn. C.1), whereas we can also consider a general non-parametric setting; they consider _one_ single invariance set, which is a special case of Thm. 3.1 with one joint \(\iota\)-property.

**Identification algorithm.** Instead of iteratively enforcing the invariance constraint across the majority of environments as described in Cor. D.1, most single-node interventional works develop equivalent constraints between pairs of environments to optimize. For example, the marginal invariance (eq. D.3) implies the marginal of the source node is changed _only if_ it is intervened upon, which is utilized by Zhang et al. [6] to identify latent variables and the ancestral relations simultaneously. In practice, Zhang et al. [6] propose a regularized loss that includes Maximum Mean Discrepancy(MMD) between the reconstructed "counterfactual" data distribution and the interventional distribution, enforcing the distributional discrepancy that reveals graphical structure (e.g., detecting the source node). Similarly, by enforcing sparsity on the score change matrix, Varici et al. [36] restricts only score changes from the intervened node and its parents. In the nonparametric case, von Kugelgen et al. [2] optimize for the invariant (aligned) interventional targets through model selection, whereas Varici et al. [7] directly solve the constrained optimization problem formulated using score differences. Considering a more general setup, Ahuja et al. [37] provides various invariance-based regularizers as plug-and-play components for any losses that enforce a sufficient representation (Constraint 3.2).

### Temporal Causal Representation Learning

**High-level overview.** Temporal CRL [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43] focuses on retrieving latent causal structures from time series data, where the latent causal structured is typically modeled as a Dynamic Bayesian Network (DBN) [44, 45]. Existing temporal CRL literature has developed identifiability results under varying sets of assumptions. A common overarching assumption is to require the Dynamic Bayesian Network to be first-order Markovian, allowing only causal links from \(t-1\) to \(t\), eliminating longer dependencies [9, 10, 11, 40]. While many works assume that there is no instantaneous effect, restricting the latent components of \(\mathbf{z}^{t}\) to be mutually dependent [10, 11, 40], some approaches have lifted this assumption and prove identifiability allowing for instantaneous links among the latent components at the same timestep (Lippe et al. [9]).

**Data generating process.** We present the data generating process followed by most temporal causal representation works and explain the underlying latent invariance and data symmetries. Let \(\mathbf{z}^{t}\in\mathbb{R}^{N}\) denotes the latent vector at time \(t\) and \(\mathbf{x}^{t}=f(\mathbf{z}^{t})\in\mathbb{R}^{D}\) the corresponding entangled observable with \(f:\mathbb{R}^{N}\rightarrow\mathbb{R}^{D}\) the shared mixing function (Defn. B.2) satisfying Asm. B.1. The actions \(\mathbf{a}^{t}\) with cardinality \(|\mathbf{a}^{t}|=N\) mostly only target a subset of latent variables while keeping the rest untouched, following its default dynamics [8, 10, 41, 11]. Intuitively, these actions \(\mathbf{a}^{t}\) can be interpreted as a component-wise indicator for each latent variable \(\mathbf{z}^{t}_{j},j\in[N]\) stating whether \(\mathbf{z}_{j}\) follows the default dynamics \(p(\mathbf{z}^{t+1}_{j}\mid\mathbf{z}^{t})\) or the modified dynamics induced by the action \(\mathbf{a}^{t}_{j}\). From this perspective, the non-intervened causal variables at time \(t\) can be considered the invariant partition under our formulation, denoted by \(\mathbf{z}^{t}_{A_{t}}\) with the index set \(A_{t}\) defined as \(A_{t}:=\{j:\mathbf{a}_{j}=0\}\). Note that this invariance can be considered as a generalization of the multiview case because the realizations \(z^{t}_{j},z^{t+1}_{j}\) are not exactly identical (as in the multiview case) but are related via a default transition mechanism \(p(\mathbf{z}^{t+1}_{j}\mid\mathbf{z}^{t})\). To formalize this intuition, we define \(\tilde{\mathbf{z}}^{t}:=\mathbf{z}^{t}\mid\mathbf{a}^{t}\) as the conditional random vector conditioning on the action \(\mathbf{a}^{t}\) at time \(t\). For the non-intervened partition \(A_{t}\subseteq[N]\) that follows the default dynamics, the transition model should be invariant:

\[p(\mathbf{z}^{t}_{A_{t}}\mid\mathbf{z}^{t-1})=p(\tilde{\mathbf{z}}^{t}_{A_{t}} \mid\mathbf{z}^{t-1}),\] (D.7)

which gives rise to a non-trivial distributional invariance property (Defn. 2.1). Note that the invariance partition \(A_{t}\) could vary across different time steps, providing a set of invariance properties \(\mathfrak{I}:=\{t_{t}:\mathbb{R}^{|A_{t}|}\rightarrow\mathcal{M}_{t}\}_{t=1}^ {T}\), indexed by time \(t\). Given by Thm. 3.1, all invariant partitions \(\mathbf{z}^{t}_{A_{t}}\) can be block-identified; furthermore, the complementary variant partition can also be identified under an invertible encoder and mutual independence within \(\mathbf{z}^{t}\) (Proposition 3.3), aligning with the identification results without instantaneous effect [8, 10, 40, 41]. On the other hand, temporal causal variables with instantaneous effects are shown to be identifiable _only if_ "instantaneous parents" (i.e., nodes affecting other nodes instantaneously) are cut by actions [9], reducing to the setting without instantaneous effect where the latent components at \(t\) are mutually independent. Upon invariance, more fine-grained latent variable identification results, such as element-wise identifiability, can be obtained by incorporating additional technical assumptions, such as the sparse mechanism shift [8, 41, 43] and parametric latent causal model [40, 46, 47].

**Identification algorithm.** From a high level, the distributional invariance (eq. (D.7)) indicates full explainability and predictability of \(\mathbf{z}^{t}_{A_{t}}\) from its previous time step \(\mathbf{z}^{t-1}\), regardless of the action \(\mathbf{a}^{t}\). In principle, this invariance principle can be enforced by directly maximizing the information content of the proposed default transition density between the learned representation \(p(\tilde{\mathbf{z}}^{t}_{A_{t}}\mid\tilde{\mathbf{z}}^{t-1})\)[9, 10]. In practice, the invariance regularization is often incorporated together with the predictability of the variant partition conditioning on actions, implemented as a KL divergence between the observational posterior \(q(\hat{\mathbf{z}}^{t}\mid\mathbf{x}^{t})\) and the transitional prior \(p(\hat{\mathbf{z}}^{t}\mid\mathbf{z}^{t-1},\mathbf{a}^{t})\)[8, 11, 39, 40, 41, 38, 42, 43, 44, 39, 40, 41], estimated using variational Bayes [48] or normalizing flow [49]. We additionally show that minimizing this KL-divergence \(D_{\text{KL}}(q(\hat{\mathbf{z}}^{t}\mid\mathbf{x}^{t})\parallel p(\hat{ \mathbf{z}}^{t}\mid\mathbf{z}^{t-1},\mathbf{a}^{t}))\) is equivalent to maximizing the conditional entropy \(p(\hat{\mathbf{z}}^{t}_{A_{t}}\mid\tilde{\mathbf{z}}^{t-1})\) in App. D.

### Multi-task Causal Representation Learning

**High-level overview.** Multi-task causal representation learning aims to identify latent causal variables via external supervision, in this case, the label information of the same instance for various tasks. Previously, multi-task learning [50, 51] has been mostly studied outside the scope of identifiability, mainly focusing on domain adaptation and out-of-distribution generalization. One of the popular ideas that was extensively used in the context of multi-task learning is to leverage interactions between different tasks to construct a generalist model that is capable of solving all classification tasks and potentially better generalizes to unseen tasks [52, 53]. Recently, Lachapelle et al. [15], Fumero et al. [16] systematically studied under which conditions the latent variables can be identified in the multi-task scenario and correspondingly provided identification algorithms.

**Data generating process.** The multi-task causal representation learning considers a _supervised_ setup: Given a latent SCM as defined in Defn. B.1, we generate the observable \(\mathbf{x}\in\mathbb{R}^{D}\) through some mixing function \(f:\mathbb{R}^{N}\rightarrow\mathbb{R}^{D}\) satisfying Asm. B.1. Given a set of task \(\mathcal{T}=\{t_{1},\dots,t_{k}\}\), and let \(\mathbf{y}^{k}\in\mathcal{Y}_{k}\) denote the corresponding task label respect to the task \(t_{k}\). Each task only _directly_depends on a subset of latent variables \(S_{k}\subseteq[N]\), in the sense that the label \(\mathbf{y}^{k}\) can be expressed as a function that contains all and only information about the latent variable \(\mathbf{z}_{S_{k}}\):

\[\mathbf{y}^{k}=r_{k}(\mathbf{z}_{S_{k}}),\] (D.8)

where \(r:\mathbb{R}^{|S_{k}|}\rightarrow\mathcal{Y}_{k}\) is some deterministic function which maps the latent subspace \(\mathbb{R}^{|S_{k}|}\) to the task-specific label space \(\mathcal{Y}_{k}\), which is often assumed to be linear and implemented using a linear readout in practice [15, 16]. For each task \(t_{k},k\in[K]\), we observe the associated data distribution \(P_{\mathbf{x},\mathbf{y}^{k}}\). Consider two different tasks \(t_{k},t_{k^{\prime}}\) with \(k,k^{\prime}\in[K]\), the corresponding data \(\mathbf{x},\mathbf{y}^{k}\) and \(\mathbf{x},\mathbf{y}^{k^{\prime}}\) are _invariant_ in the intersection of task-related features \(\mathbf{z}_{A}\) with \(A=S_{k}\cap S_{k^{\prime}}\). Formally, let \(r_{k}^{-1}(\{\mathbf{y}^{k}\})\) denotes the pre-image of \(\mathbf{y}^{k}\), for which it holds

\[r_{k}^{-1}(\{\mathbf{y}^{k}\})_{A}=r_{k^{\prime}}^{-1}(\{\mathbf{y}^{k^{ \prime}}\})_{A},\] (D.9)

showing alignment on the shared partition of the task-related latents. In the ideal case, each latent component \(j\in[N]\) is _uniquely shared_ by a subset of tasks, all factors of variation can be fully disentangled, which aligns with the theoretical claims by Lachapelle et al. [15], Fumero et al. [16].

**Identification algorithms.** We remark that the _sharing_ mechanism in the context of multi-task learning fundamentally differs from that of multi-view setup, thus resulting in different learning algorithms. Regarding learning, the shared partition of task-related latents is enforced to align up to the linear equivalence class (given a linear readout) instead of sample level \(L_{2}\) alignment. Intuitively, this invariance principle can be interpreted as a soft version of the that in the multiview case. In practice, under the constraint of perfect classification, one employs (1) a sparsity constraint on the linear readout weights to enforce the encoder to allocate the correct task-specific latents and (2) an information-sharing term to encourage reusing latents across various tasks. Equilibrium can be obtained between these two terms only when the shared task-specific latent is element-wise identified (Defn. C.2). Thus, this soft invariance principle is jointly implemented by the sparsity constraint and information sharing regularization [16, Sec. 2.1].

### Domain Generalization

**High-level overview.** Domain generalization aims at _out-of-distribution_ performance. That is, learning an optimal encoder and predictor that performs well at some unseen test domain that preserves the same data symmetries as in the training data. At a high level, domain generalization representation learning [12, 13, 54, 55, 56] considers a similar framework as introduced for interventional CRL, with _independent_ but _non-identically distributed_ data, but additionally incorporated with external supervision and focusing more on model robustness perspective. While interventional CRL aims to identify the true latent factors of variations (up to some transformation), domain generalization learning focuses directly on _out-of-distribution_ prediction, relying on some invariance properties preserved under the distributional shifts. Due to the non-causal objective, new methodologies are motivated and tested on real-world benchmarks (e.g., VLCS [57], PACS [58], Office-Home [59], Terra Incognita [60], DomainNet [61]) and could inspire future real-world applicability of causal representation learning approaches.

**Data generating process.** The problem of domain generalizations is an _extension of supervised learning_ where training data from multiple environments are available Blanchard et al. [62]. An environment is a dataset of i.i.d. observations from a joint distribution \(P_{\mathbf{x}^{k},\mathbf{y}^{k}}\) of the observables \(\mathbf{x}^{k}\in\mathbb{R}^{D}\) and the label \(\mathbf{y}^{k}\in\mathbb{R}\). The label \(\mathbf{y}^{k}\in\mathbb{R}^{m}\) only depends on the invariant latents through a linear regression structural equation model [14, Assmp. 1], described as follows:

\[\mathbf{y}^{k} =\mathbf{w}^{*}\mathbf{z}_{A}^{k}+\epsilon_{k},\,\mathbf{z}_{A}^{ k}\perp\epsilon_{k}\] (D.10) \[\mathbf{x}^{k} =f(\mathbf{z}^{k})\]

where \(\mathbf{w}^{*}\in\mathbb{R}^{D\times m}\) represents the ground truth relationship between the label \(\mathbf{y}^{k}\) and the invariant latents \(\mathbf{z}_{A}^{k}\). \(\epsilon_{k}\) is some white noise with bounded variance and \(f:\mathbb{R}^{N}\rightarrow\mathbb{R}^{D}\) denotes the shared mixing function for all \(k\in[K]\) satisfying Assm. B.1. The set of environment distributions \(\{P_{\mathbf{x}^{k},\mathbf{y}^{k}}\}_{k\in[K]}\) generally differ from each other because of interventions or other distributional shifts such as covariates shift and concept shift. However, as the relationship between the invariant latents and the labels \(\mathbf{w}^{*}\) and the mixing mechanism \(f\) are shared across different environments, the optimal risk remains invariant in the sense that

\[\mathcal{R}_{k}^{*}(\mathbf{w}^{*}\circ f^{-1})=\mathcal{R}_{k^{\prime}}^{*}( \mathbf{w}^{*}\circ f^{-1}),\] (D.11)where \(\mathbf{w}^{*}\) denotes the ground truth relation between the invariant latents \(\mathbf{z}_{A}^{k}\) an the labels \(\mathbf{y}^{k}\) and \(f^{-1}\) is the inverse of the diffeomorphism mixing \(f\)(see eq. (D.10)). Note that this is a non-trivial \(\iota\) property as the labels \(\mathbf{y}^{k}\) only depend on the invariant latents \(\mathbf{z}_{A}^{k}\), thus satisfying Defn. 2.1 (ii).

**Identification algorithm.** Different distributional invariance are enforced by interpolating and extrapolating across various environments. Among the countless contribution to the literature, _mixup_[54] linearly interpolates observations from different environments as a robust data augmentation procedure, Domain-Adversarial Neural Networks [55] support the main learning task discouraging learning domain-discriminant features, Distributionally Robust Optimization (DRO) [12] replaces the vanilla Empirical Risk objective minimizing only with respect to the worst modeled environment, Invariant Risk Minimization [56] combines the Empirical Risk objective with an invariance constraint on the gradient, and Variance Risk Extrapolation [13, V-REx], similar in spirit combines the empirical risk objective with an invariance constraint using the variance among environments. For a more comprehensive review of domain generalization algorithms, see Zhou et al. [63].

### Further Explanations for Tab. 2

**General clarification.** Tab. 2 summarizes all special cases of our invariance framework. For each work, we present their technical assumptions, the type of invariance, the implementation for the invariance and the sufficiency regularizers (to satisfy Constraints 3.1 and 3.2), and the type of identifiability they achieve. Note that this table is by no means exhaustive. Also, we omit some additional results and technical assumptions of individual papers for readability. For each line of work, we provide an additional paragraph elaborating on their practical implementation of the invariance principle.

**(a) Single-node intervention and parametric assumptions.** Many existing CRL works that consider single node intervention per node require additional parametric assumptions, either on the mixing function [6, 36] or the latent causal model [5] or both [24], thus achieving (at least) element-wise identifiability (Defn. C.2). Although some proposed algorithms did not directly focus on solving our invariance-based constrained optimization problem (Thm. 3.1) to achieve identifiability, their theoretical identifiability results can be explained using the invariance principle in our framework, as explained in App. D.2.

**(b) Multi-node intervention and linear mixing.** Recently, [25] extends previous interventional CRL works to unknown multi-node interventions and achieves identifiability under the assumption of a linearly independent intervention signature matrix \(M_{\text{int}}\in\{0,1\}^{N\times K}\) with each column \(k\) represents the intervened node in this environment \(k\). The row-wise linear independence of \(M_{\text{int}}\) implies that each latent variable must have been intervened at least once. Let \(M\in\{0,1\}^{N\times N}\) represent a submatrix of \(M_{\text{int}}\) with _linearly independent_ columns. By performing the change of basis on \(M\) such that only one component is non-zero in each column and projecting the score changes using the corresponding change of basis matrix, the setting becomes similar to the other interventional case (unknown single node intervention per node). This similarity allows it to be intuitively explained using the same distributional invariance principle introduced earlier (App. D.2).

**(c) Paired single-node intervention per node under nonparametric assumptions.** In the nonparametric settings, several works von Kugelgen et al. [2], Varici et al. [7] have shown element-wise latent variable identification under sufficiently different paired perfect intervention per node. By having two sufficiently different interventions per node, one introduces invariance on the interventional target across these paired interventional environments. This invariance property can be enforced using the score differences [7] or algorithmically by performing model selection [2], see App. D.2 for more details.

**(d) Variant latents identification under independence.** While some papers states main identification results on the variant partition, it can be explained by Thm. 3.1 and Proposition 3.3 stating that the variant block can be identified under independence and invertible encoder. For example, Wendong et al. [27, Thm. 4.5] shows block-identifiability on the intervened (variant) latents under [27, Assumption 4.4] of block-wise independence between the invariant and variant blocks.

**(e) Invariance regularizers in temporal CRL.** As explained in App. D.3, instead of directly maximizing the information content of the transition model \(H(\mathbf{z}_{A}^{t}\mid\mathbf{z}^{t-1})\) on the invariant partition \(A\), most temporal CRL minimizes the KL divergence between the observational posterior \(q(\mathbf{z}^{t}\mid\mathbf{x}_{t})\) and the transitional prior \(p(\mathbf{z}^{t}\mid\mathbf{z}^{t-1},\mathbf{a}^{t})\)[8, 11, 39, 41, 46]. In the following, we show that minimizing the KL-divergence \(D_{\text{KL}}(q(\mathbf{z}^{t}\mid\mathbf{x}_{t})\parallel p(\mathbf{z}^{t}\mid \mathbf{z}^{t-1},\mathbf{a}^{t}))\) also maximizes the conditional entropy \(H(\mathbf{z}^{t}_{A}\mid\mathbf{z}^{t-1})\).

First, note that the KL-Divergence can be decomposed given the mutual dependence between the invariant and variant latent partitions:

\[\begin{split}& D_{\text{KL}}(q(\mathbf{z}^{t}\mid\mathbf{x}_{t}) \parallel p(\mathbf{z}^{t}\mid\mathbf{z}^{t-1},\mathbf{a}^{t}))\\ =& D_{\text{KL}}(q(\mathbf{z}^{t}_{A}\mid\mathbf{x}_ {t})\parallel p(\mathbf{z}^{t}_{A}\mid\mathbf{z}^{t-1}))\cdot D_{\text{KL}}(q( \mathbf{z}^{t}_{A^{c}}\mid\mathbf{x}_{t})\parallel p(\mathbf{z}^{t}_{A^{c}} \mid\mathbf{z}^{t-1},\mathbf{a}^{t})),\end{split}\] (D.12)

where \(A^{c}:[N]\setminus A\) denotes the variant latent indices. Since KL-divergence is non-negative, the joint KL-divergence is minimized when both additive terms are minimized. Hence, from now on, we focus on the first term \(D_{\text{KL}}(q(\mathbf{z}^{t}_{A}\mid\mathbf{x}_{t})\parallel p(\mathbf{z}^ {t}_{A}\mid\mathbf{z}^{t-1}))\) where only the invariant partition is involved, which can be rewritten as:

\[\begin{split}& D_{\text{KL}}(q(\mathbf{z}^{t}_{A}\mid\mathbf{x}_ {t})\parallel p(\mathbf{z}^{t}_{A}\mid\mathbf{z}^{t-1}))=\mathbb{E}_{\mathbf{ x}_{t}}\mathbb{E}_{\mathbf{z}^{t}_{A}\mid\mathbf{x}_{t}}\left[\log\frac{q( \mathbf{z}^{t}_{A}\mid\mathbf{x}_{t})}{p(\mathbf{z}^{t}_{A}\mid\mathbf{z}^{t- 1})}\right]\\ =&\mathbb{E}_{\mathbf{x}_{t}}\left[H(q(\mathbf{z}^{ t}_{A}\mid\mathbf{x}_{t}),p(\mathbf{z}^{t}_{A}\mid\mathbf{z}^{t-1}))-H( \mathbf{z}^{t}_{A}\mid\mathbf{z}^{t-1})\right]\\ =&\mathbb{E}_{\mathbf{x}_{t}}\left[H(q(\mathbf{z}^{ t}_{A}\mid\mathbf{x}_{t}),p(\mathbf{z}^{t}_{A}\mid\mathbf{z}^{t-1}))\right]-H( \mathbf{z}^{t}_{A}\mid\mathbf{z}^{t-1}).\end{split}\] (D.13)

Therefore, minimizing \(D_{\text{KL}}(q(\mathbf{z}^{t}_{A}\mid\mathbf{x}_{t})\parallel p(\mathbf{z}^ {t}_{A}\mid\mathbf{z}^{t-1}))\) is equivalent to maximizing \(H(\mathbf{z}^{t}_{A}\mid\mathbf{z}^{t-1})\). Consequently, the commonly used \(D_{\text{KL}}(q(\mathbf{z}^{t}\mid\mathbf{x}_{t})\parallel p(\mathbf{z}^{t} \mid\mathbf{z}^{t-1},\mathbf{a}^{t}))\) in the temporal CRL literature is justified as a valid invariance regularizer, enforcing the transitional invariance (eq. (D.7)).

**(f) Invariance regularizers in domain generalization.** While Sagawa et al. [12] directly optimize for the worst-case risk, a link can be drawn between this objective and the risk invariance: Given a pair of linear head \(\mathbf{w}\) and encoder \(g\) shared across \([K]\) domains, let the order of risks be \(\mathcal{R}^{\pi_{1}}\geq\mathcal{R}^{\pi_{2}}\dots\mathcal{R}^{\pi_{K}}\). Since \(\mathcal{R}^{\pi_{1}}\) is lower bounded by \(\mathcal{R}^{\pi_{2}}\) the minimum of the training objective in Sagawa et al. [12] (\(\max_{k\in[K]}\mathcal{R}^{k}(w,g)\)) is obtained when \(\mathcal{R}^{\pi_{1}}=\mathcal{R}^{\pi_{2}}\). Then we have \(\mathcal{R}^{\pi_{1}}=\mathcal{R}^{\pi_{2}}\geq\dots\geq\mathcal{R}^{\pi_{K}}\), and the next minimum will be obtained when \(\mathcal{R}^{\pi_{1}}=\mathcal{R}^{\pi_{2}}=\mathcal{R}^{\pi_{3}}\), and so on so forth. The optimization procedure stops when the risks are the same across all domains.

[13] minimizes variance between environment risks to enforce the risk invariance, and the we formally show in the following these two are equivalent. Note that the invariance principle for risk alignment can be formulated as:

\[\mathbb{E}_{k,k^{\prime}}\left[\left\|\mathcal{R}_{k}-\mathcal{R}_{k^{\prime}} \right\|_{2}^{2}\right]\] (D.14)

Now we show that minimizing the variance regularizer proposed by Sagawa et al. [12] is equivalent to minimizing the risk alignment term eq. (D.14)

\[\min\text{Var}\left[\mathcal{R}_{k}\right] \equiv\min\mathbb{E}_{k}\left[\left(\mathcal{R}_{k}\right)^{2} \right]-\mathbb{E}_{k}\left[\mathcal{R}_{k}\right]\] \[\equiv\min\mathbb{E}_{k,k^{\prime}}\left[\left(\mathcal{R}_{k}- \mathcal{R}_{k^{\prime}}\right)^{2}\right]\equiv\min\text{ \emph{eq.}}\text{.}\] (D.15)

### Notable Cases Not Directly Covered by the Theory

There are some works are not listed in Tab. 2 that cannot yet be directly explained by our invariance frameworks but are rather loosely connected. One representative line of work [8, 41, 64, 65] relies on the sparsity assumption in the latent dependency to achieve latent variable and graph identification. This assumption is closely related to the _sparse mechanism shift_ hypothesis in causal representation learning [1], stating small distributional changes should not affect all causal variables but only a small subset of these. Note that the sparsity constraint is often formulated as the estimator (either for the graph [15, 41] or of the latents [65]) should be at least sparse as the ground truth one, maximizing the cardinality of the unaffected (invariant) part. Some theoretical results do not rely on multiple data pockets that share certain invariance properties but directly employ specific properties within the observational data, such as independent support [3], or shared cluster membership [47, 66].

Proofs

### Assumption Justification

We justify the Defn. 2.1 (ii) by showing a negative results under violation of the assumption, i.e., trivially invariant latent variables are not identifiable.

**Proposition E.1** (General non-identifiability of trivially invariant latent variables).: _Consider the setup in Thm. 3.1, w.l.o.g we assume \(\mathfrak{I}=\{\iota\}\) and \(\iota\) is trivial in the sense that assumption (ii) in Defn. 2.1 is violated. Then, the corresponding invariant partition \(\mathbf{z}_{A}^{k}\) is not identifiable for any \(k\in[K]\)._

Proof.: We provide a counter example as follows: Define a trivial \(\iota\)-property as "if the first component is greater than zero on \(A=\{1\}\) of some two dimensional latents \(\mathbf{z}\)". Formally,

\[\iota(\mathbf{z}_{1})=\mathbf{1}[\mathbf{z}_{1}>0].\]

Consider a mixing function \(f=id\) and an invertible encoder \(g(\mathbf{x})=g(f(\mathbf{z}))=[\mathbf{z}_{1}+\mathbf{z}_{2},\mathbf{z}_{2}]\) satisfying the sufficiency constraint (Constraint 3.2). Define \(h_{1}=h_{2}=[g\circ f]_{A}\). Then for some realizations \(z,\tilde{z}\) with \(z_{1}+z_{2}>0\) and \(\tilde{z}_{1}+\tilde{z}_{2}>0\) we have \(\iota(h(\mathbf{z}))=\iota(h(\tilde{\mathbf{z}}))\). However, \(h_{1},h_{2}\) can not disentangle \(\mathbf{z}_{1}\), showing non-identifiability for the invariant partition \(\mathbf{z}_{A}\). 

**Link between Defn. 2.1 (ii) and interventional discrepancy.** In the following, we elaborate how Defn. 2.1 (ii) resembles the most common assumption in interventional causal representation learning, the interventional discrepancy [7; 27]. Note that this assumption may termed differently as _sufficient variability_[2; 10], _interventional regularity_[36; 25], but the mathematical formulation remain the same. We begin with restating this assumption:

**Assumption E.1** (Interventional discrepancy [27]).: Given \(k\in[K]\), let \(p_{t_{k}}\) denote the causal mechanism of the intervened variable \(\mathbf{z}_{t_{k}}\) with \(t_{k}\in[N]\). We say a stochastic intervention \(\tilde{p}_{k}\) satisfies interventional discrepancy if

\[\frac{\partial\log p_{t_{k}}}{\partial\mathbf{z}_{t_{k}}}(\mathbf{z}_{t_{k}} \mid\mathbf{z}_{\text{pa}(t_{k})})\neq\frac{\partial\log\tilde{p}_{t_{k}}}{ \partial\mathbf{z}_{t_{k}}}(\mathbf{z}_{t_{k}}\mid\mathbf{z}_{\text{pa}(t_{k} )})\qquad\text{almost everywhere }(a.e.).\]

Proof.: We show that any cases violating the interventional discrepancy assumption also violates Defn. 2.1 (ii) and vice versa. Suppose for a contradiction that there exists \(t_{k}\in[N]\) that is intervened in environment \(k\in[K]\), and there is a non-empty interior \(U\subset\mathbb{R}\) with non-zero measure where the interventional discrepancy is violated, i.e., for all \(z_{t_{k}}\in U\), it holds

\[\frac{\partial\log p_{t_{k}}}{\partial z_{t_{k}}}(\mathbf{z}_{t_{k}}\mid \mathbf{z}_{\text{pa}(t_{k})})=\frac{\partial\log\tilde{p}_{t_{k}}}{\partial z _{t_{k}}}(\mathbf{z}_{t_{k}}\mid\mathbf{z}_{\text{pa}(t_{k})})\] (E.1)

Note that the invariant partition under a single node imperfect intervention yields the complementary set of the transitive closure of \(t_{k}\), i.e., \(A:=[N]\setminus\operatorname{TC}(t_{k})\) because the (joint) marginal distributional invariance holds in the sense that

\[\iota(\mathbf{z}_{A})=p_{\mathbf{z}_{A}}=\tilde{p}_{\mathbf{z}_{A}}.\]

W.l.o.g, we assume \(A=\{1,\dots,t_{k}-1\}\), define a function \(h:\mathbb{R}^{N}\to\mathbb{R}^{|A|}\) with

\[h(\mathbf{z})=[\mathbf{z}_{1},\dots,\mathbf{z}_{t_{k}-2},\mathbf{z}_{t_{k}}]\]

that omits the \(t_{k}-1\) component of \(\mathbf{z}\) but includes the variant component \(t_{k}\). Note that the marginal of \(\mathbf{z}_{t_{k}}\) after intervention remains invariant within \(U\) because

\[p(\mathbf{z}_{t_{k}}) =\int p_{t_{k}}(\mathbf{z}_{t_{k}}\mid\mathbf{z}_{\text{pa}(t_{k })})p(\mathbf{z}_{\text{pa}(t_{k})})d\mathbf{z}_{\text{pa}(t_{k})}\qquad \text{pa}(t_{k})\in A\] \[=\int p_{t_{k}}(\mathbf{z}_{t_{k}}\mid\mathbf{z}_{\text{pa}(t_{k })})\tilde{p}(\mathbf{z}_{\text{pa}(t_{k})})d\mathbf{z}_{\text{pa}(t_{k})}\qquad eq. \text{ (E.1) and both }p_{k},\tilde{p_{k}}\text{ pdfs}\] \[=\int\tilde{p}_{t_{k}}(\mathbf{z}_{t_{k}}\mid\mathbf{z}_{\text{pa} (t_{k})})\tilde{p}(\mathbf{z}_{\text{pa}(t_{k})})d\mathbf{z}_{\text{pa}(t_{k})}\] \[=\tilde{p}(\mathbf{z}_{t_{k}}).\]Therefore, we have \(\iota(h(\mathbf{z}))=\iota(h(\tilde{\mathbf{z}}))\) (with \(\tilde{\mathbf{z}}\) noting the latent vectors under intervention) contradicting Defn. 2.1 (ii). The other direction (violating Defn. 2.1 (ii) implies violating Ass. E.1) can be proved using the same example.

### Proof for Thm. 3.1

Our proof consists of the following steps:

1. We construct the optimal encoders \(G^{*}\) (Defn. 3.2) and selectors \(\Phi^{*}\) (Defn. 3.4) that solves the constrained optimization problem in Thm. 3.1.
2. We show that, for any invariance property \(\iota_{i}\in\mathfrak{I}\) and any observation \(\mathbf{x}^{k}\) in the corresponding \(\iota_{i}\)-equivalent subset \(\mathbf{x}_{V_{i}}\), the selected representation \(\phi^{(i,k)}\osymp g_{k}(\mathbf{x}^{k})\) cannot contain any other information than the invariant partition \(\mathbf{z}^{k}_{A_{i}}\).
3. Lastly, we prove that selected representation \(\phi^{(i,k)}\osymp g_{k}(\mathbf{x}^{k})\) relates to the ground truth invariant partition \(\mathbf{z}^{k}_{A_{i}}\) through a diffeomorphism \(h_{k}:\mathbb{R}^{|A_{i}|}\to\mathbb{R}^{|A_{i}|}\) for all invariance property \(\iota_{i}\in\mathfrak{I}\) and for any observable \(\mathbf{x}^{k}\) from the \(\iota_{i}\)-equivalent subset \(\mathbf{x}_{V_{i}}\); in other words, \(\phi^{(i,k)}\osymp g_{k}(\mathbf{x}^{k})\) block-identifies \(\mathbf{z}^{k}_{A_{i}}\) in the sense of Defn. 3.1.

**Lemma E.1** (Existence of optimal encoders and selectors).: _Consider a set of observables \(\mathcal{S}_{\mathbf{x}}=\{\mathbf{x}^{1},\mathbf{x}^{2},\ldots,\mathbf{x}^{K }\}\in\mathcal{X}\) generated from SS 2 satisfying Ass. 2.1, then there exists optimal encoders \(G\) (Defn. 3.2) and selectors \(\Phi\) (Defn. 3.4) which satisfy both Constraints 3.1 and 3.2._

Proof.: The optimal encoders can be constructed as the set of the inverse of the ground truth mixing functions:

\[G^{*}=\{f_{k}^{-1}\}_{k\in[K]},\] (E.2)

\(f_{k}^{-1}\) is smooth and invertible following Ass. B.1. By definition, for each \(k\in[K]\), we have:

\[f_{k}^{-1}(\mathbf{x}^{k})=\mathbf{z}^{k}\in\mathcal{Z}^{k}.\] (E.3)

Next, we define the optimal selector \(\Phi^{*}=\{\phi^{(i,k)}\}_{i\in[n_{\mathfrak{I}}],k\in[K]}\) such that for all \(i\in n_{\mathfrak{I}},k\in[K]\), it holds

\[\phi^{(i,k)}\osymp\mathbf{z}^{k}=\mathbf{z}^{k}_{A_{i}}.\] (E.4)

Thus, the invariance constraint (Constraint 3.1) is trivially satisfied as given by SS 2. The optimal encoder \(f_{k}^{-1}\) is smooth and invertible following Ass. B.1 so the sufficiency constraint (Constraint 3.2) is also satisfied. Hence, we have shown the existence of the optimum to the constrained optimization problem in Thm. 3.1. 

**Lemma E.2** (Invariant component isolation).: _Consider the same set of observables \(\mathcal{S}_{\mathbf{x}}\) as introduced in Lemma E.1, then for any set of smooth encoders \(G\) (Defn. 3.2), \(\Phi\) (Defn. 3.4) that satisfy the invariance condition (Constraint 3.1), the learned representation \(\phi^{(i,k)}\osymp g_{k}(x_{k})\) can only be dependent on the invariant latent variables \(\mathbf{z}^{k}_{A_{i}}:=\{\mathbf{z}^{k}_{i}:i\in A_{i}\}\), not any non-invariant variables \(\mathbf{z}^{k}_{j}\) with \(j\in A_{i}^{c}:=[N]\setminus A_{i}\)._

Proof.: This proof directly follows the second assumption (ii) in Defn. 2.1. Define

\[h_{k}:=\phi^{(i,k)}\osymp g_{k}\circ f_{k}\quad k\in[K].\] (E.5)

By Constraint 3.1 and the fact that \(f\) and \(g\) are diffeomorphisms, we have

\[\iota(h_{k}(\mathbf{z}^{k}))=\iota(h_{k^{\prime}}(\mathbf{z}^{k^{\prime}})) \quad a.s.\qquad\forall k<k^{\prime}\in[K].\] (E.6)

According to (ii) in Defn. 2.1, \(h_{k},k\in[K]\) cannot directly depends on any other latent component \(\mathbf{z}_{q}\) with \(q\notin A\). Therefore, we have shown that \(h_{k}\) is a function of \(\mathbf{z}^{k}_{A_{i}}\), for all \(k\in[K],\iota_{i}\in\mathfrak{I}\). 

**Theorem 3.1** (Identifiability of multiple invariant blocks).: _Consider a set of observables \(\mathcal{S}_{\mathbf{x}}=\{\mathbf{x}^{1},\mathbf{x}^{2},\ldots,\mathbf{x}^{K }\}\) with \(\mathbf{x}^{k}\in\mathcal{X}^{k}\) generated from SS 2 satisfying Ass. 2.1. Let \(G,\Phi\) be the set of smooth encoders (Defn. 3.2) and selectors (Defn. 3.4) that satisfy Constraints 3.1 and 3.2, then the invariant component \(\mathbf{z}^{k}_{A_{i}}\) is block-identified (Defn. 3.1) by \(\phi^{(i,k)}\osymp g_{k}\) for all \(\iota_{i}\in\mathfrak{I},k\in[K]\)._Proof.: Lem. E.1 verifies that there exists such optimum which satisfies both invariance and sufficiency conditions (Constraints 3.1 and 3.2). Following Lem. E.2, the composition \(\phi^{(i,k)}\oslash g_{k}\) can only encode information related to the invariant latent subset \(A_{i}\) specified by the invariance property \(\iota_{i}\in\mathfrak{I}\) for all \(k\in V_{i}\). As given by Constraint 3.2, all smooth encoders \(g_{k}\in[K]\) contain at least as much information as the ground truth invariant latents \(\mathbf{z}_{A_{i}}\) for \(i\) with \(k\in V_{i}\). Therefore, the selected representation \(\phi^{(i,k)}\oslash g_{k}(\mathbf{x}^{k})\) relates to the ground truth invariant partition \(\mathbf{z}_{A_{i}}\) through some diffeomorphism, i.e., \(\mathbf{z}_{A_{i}}\) is blocked-identified by \(\phi^{(i,k)}\oslash g_{k}(\mathbf{x}^{k})\) for all invariance property \(\iota_{i}\in\mathfrak{I}\) and observable \(k\in V_{i}\),. 

### Proofs for Generalization of Variant Latents

**Proposition 3.2** (General non-identifiability of variant latent variables).: _Consider the setup in Thm. 3.1, let \(A:=\bigcup_{i\in[n_{\mathcal{I}}]}A_{i}\) denote the union of block-identified latent indices and \(A^{c}:=[N]\setminus A\) the complementary set where no \(\iota\)-invariance \(\iota\in\mathfrak{I}\) applies, then the variant latents \(\mathbf{z}_{A^{c}}\) cannot be identified._

Proof.: We provide a simple counter example with two latent variables \(\mathbf{z}=[\mathbf{z}_{1},\mathbf{z}_{2}]\), with the mixing function \(f\) being the identity map \(\mathrm{id}\). W.l.o.g. we assume the invariant partition to be \(A=\{1\}\). According to Thm. 3.1, the invariant latent variable can be identified up to a certain bijection \(h:\mathbb{R}\to\mathbb{R}\). Let \(\hat{\mathbf{z}}\) be the estimated representation:

\[\hat{\mathbf{z}}=[h(\mathbf{z}_{1}),\mathbf{z}_{2}-\mathbf{z}_{1}]\] (E.7)

with the estimated mixing function \(\hat{f}:\mathbb{R}^{2}\to\mathbb{R}^{2}\):

\[\hat{f}(\hat{\mathbf{z}})=[h^{-1}(\hat{\mathbf{z}}_{1}),\hat{\mathbf{z}}_{2}+h ^{-1}(\hat{\mathbf{z}}_{1})],\] (E.8)

then we obtain the same observations \(\hat{f}(\hat{\mathbf{z}})=f(\mathbf{z})\) whereas \(\hat{\mathbf{z}}_{2}\) consists of a mixing of \(\mathbf{z}_{1}\) and \(\mathbf{z}_{2}\), showing the variant latent variable \(\mathbf{z}_{2}\) can not be identified. 

**Proposition 3.3** (Identifiability of variant latent under independence).: _Consider an optimal encoder \(g\in G^{*}\) and optimal selector \(\phi\in\Phi^{*}\) from Thm. 3.1 that jointly identify an invariant block \(\mathbf{z}_{A}\) (we omit subscriptions \(k,i\) for simplicity), then \(\mathbf{z}_{A^{c}}(A^{c}:=[N]\setminus A)\) can be identified by the complementary encoding partition \((1-\phi)\oslash g\) only if: (i) \(g\) is invertible in the sense that \(I(\mathbf{x},g(\mathbf{x}))=H(\mathbf{x})\); (ii) \(\mathbf{z}_{A^{c}}\) is independent on \(\mathbf{z}_{A}\)._

Proof.: Consider the mutual information between the observation \(\mathbf{x}\in\mathcal{S}_{\mathbf{x}}\) and the optimal encoder \(g\in G^{*}\) from Thm. 3.1:

\[\begin{split} I(\mathbf{x};g(\mathbf{x}))&=I\left( \mathbf{x};\phi\oslash g(\mathbf{x}),(1-\phi)\oslash g(\mathbf{x})\right)\\ &=I\left(\mathbf{x};\phi\oslash g(\mathbf{x})\right)+I\left( \mathbf{x};(1-\phi)\oslash g(\mathbf{x})\right).\end{split}\] (E.9)

This decomposition is valid because \(\phi\oslash g(\mathbf{x})\) disentangles \(\mathbf{z}_{A}\) from the rest of the encodings, as given by the definition of block-identifiability Defn. 3.1. Therefore, \(\phi\oslash g(\mathbf{x})\) is independent on \((1-\phi)\oslash g(\mathbf{x})\).

Writing \(\phi\oslash g(\mathbf{x})=h(\mathbf{z}_{A})\) (Thm. 3.1) and \(\mathbf{x}=f(\mathbf{z}_{A},\mathbf{z}_{[N]\setminus A})\) with \(h:\mathbb{R}^{|A|}\to\mathbb{R}^{|A|}\) some bijection and \(f\) the mixing diffeomorphism Defn. B.2, we have:

\[\begin{split} I(\mathbf{x};g(\mathbf{x}))&=I\left( \mathbf{x};\phi\oslash g(\mathbf{x}),(1-\phi)\oslash g(\mathbf{x})\right)\\ &=I\left(\mathbf{x};\phi\oslash g(\mathbf{x})\right)+I\left( \mathbf{x};(1-\phi)\oslash g(\mathbf{x})\right)\\ &=I\left(f(\mathbf{z}_{A},\mathbf{z}_{A^{c}});h(\mathbf{z}_{A}) \right)+I\left(\mathbf{x};(1-\phi)\oslash g(\mathbf{x})\right)\\ &=H(\mathbf{z}_{A})+I\left(\mathbf{x};(1-\phi)\oslash g(\mathbf{ x})\right).\end{split}\] (E.10)

Given by condition (i), we have

\[I(\mathbf{x};g(\mathbf{x}))=H(f(\mathbf{x}))=H(f(\mathbf{z}_{A},\mathbf{z}_{A^ {c}}))=H(\mathbf{z}_{A})+H(\mathbf{z}_{A^{c}}),\] (E.11)

cancelling \(H(\mathbf{z}_{A})\) from both eqs. (E.10) and (E.11), we obtain the following equality:\[I\left(\mathbf{x};(1-\phi)\oslash g(\mathbf{x})\right)=H(\mathbf{z}_{A^{c}}),\] (E.12)

which implies that \((1-\phi)\oslash g(\mathbf{x})=\tilde{h}(\mathbf{z}_{[N]\setminus A})\) for some bijection \(\tilde{h}:\mathbb{R}^{N-|A|}\to\mathbb{R}^{N-|A|}\). That is, the independent complementary block \(\mathbf{z}_{A^{c}}\) is identified by the \((1-\phi)\oslash g(\mathbf{x})\). 

### Proofs for Granularity of Latent Variable Identification

**Proposition C.1** (Granularity of identification).: _Affine-identifiability (Defn. C.3) implies element-identifiability (Defn. C.2) and block affine-identifiability (Defn. C.1) while element-identifiability and block affine-identifiability implies block-identifiability (Defn. 3.1)._

Proof.: The diagonal matrix \(\Lambda\) in eq. (C.3) is invertible and thus also a diffeomorphism \(\phi\) (eq. (C.2)); Diagonal \(\Lambda\) of affine identifiability is a special instance of \(\tilde{\Lambda}\) in eq. (C.1) where all non-diagonal entries are zero. Hence, affine-identifiability implies element-identifiability and block affine-identifiability. On the other hand, block affine-identifiability is block-identifiability with affine bijection \(h\) and element-identifiability defines a special case of block-identifiability where each latent component \(\mathbf{z}_{i}\) is an individual block. 

**Proposition C.2** (Transition between identification levels).: _The transition between different levels of latent variable identification (Fig. 2) can be summarized as follows:_

1. _Element-level identifiability (Defns._ C.2 _and_ C.3_) can be obtained from block-wise identifiability (Defns._ 3.1 _and_ C.1_) when each individual latent constitutes an invariant block;_
2. _Identifiability up to an affine transformation (Defns._ C.1 _and_ C.3_) can be obtained from general identifiability on arbitrary diffeomorphism (Defns._ 3.1 _and_ C.2_) by additionally assuming that both the ground truth mixing function and decoder are finite degree polynomials of the same degree._

Proof.: The proof for (i) is trivial in the sense that identification of block with size one boils down to the identification on the element level. The proof for (ii) is based on Ahuja et al. [3, Thm. 4.4] and Zhang et al. [6, Lem. 1], stating that when both ground truth mixing function and decoder are finite degree polynomials of the same degree, the _invertible_ encoder learns a representation that is affine linear to the ground truth latents, i.e., \(\hat{\mathbf{z}}=\mathbf{L}\cdot\mathbf{z}+\mathbf{b}\) with \(\mathbf{L}\in\mathbb{R}^{N\times N}\).

## Appendix F Synthetic Ablation with "Nin Interventions"

This subsection presents identifiability results under controversial (non-causal) conditions using simulated data. We consider the synthetic setup with full control over the latent space and the data-generating process. We consider a simple graph of three causal variables as \(\mathbf{z}_{1}\to\mathbf{z}_{2}\to\mathbf{z}_{3}\). The corresponding joint density has the form of \(p_{\mathbf{z}}(z_{1},z_{2},z_{3})=p(z_{3}\mid z_{2})p(z_{2}\mid z_{1})p(z_{1})\)

The goal of this experiment is to demonstrate that existing methods for interventional CRL rely primarily on distributional invariance, regardless of whether this invariance arises from a well-defined intervention or some other arbitrary transformation. To illustrate this, we introduce the concept of a "nintervention," which has a similar distributional effect to a regular intervention, maintaining certain conditionals invariant while altering others, but without a causal interpretation.

**Definition F.1** (Ninterventions).: We define a "_nintervention_" on a causal conditional as the process of changing its distribution but cutting all incoming and outgoing edges. Child nodes condition on the old, pre-intervention, random variable. Formally, we consider the latent SCM as defined in Defn. B.1, an _intervention_ on a node \(j\in[N]\) is gives rise to the following conditional factorization \(\tilde{p}_{\mathbf{z}}(z)=\tilde{p}(z_{j})\prod_{i\in[N]\setminus\{j\}}p(z_{i }\mid_{\mathbf{pa}(i)}^{\text{old}})\)

Note that the marginal distribution of all non-nintervened nodes \(P_{\mathbf{z}_{[N]\setminus\bar{z}}}\) remain invariant after intervention. In previous example, we perform a intervention by replacing the conditional density \(p(z_{2}\mid z_{1})\) using a sufficiently different marginal distribution \(p(\tilde{z}_{2})\) that satisfies Defn. 2.1 (ii), which gives rise to the following new factorization: \(\tilde{p}_{\mathbf{z}}(z_{1},z_{2},z_{3})=p(z_{3}\mid z_{2}^{\text{old}}) \tilde{p}(z_{2})p(z_{1}).\) Note that \(\mathbf{z}_{3}\) conditions on the random variable \(\mathbf{z}_{2}\) before intervention, whose realization is denoted as \(z_{2}^{\text{old}}\).

Differing from a causal _intervention_, we cut both the incoming and outgoing links of \(\mathbf{z}_{2}\) and keep the marginal distribution of \(\mathbf{z}_{3}\) the same. Clearly, this is a non-sensical intervention from the causal perspective because we eliminates the causal effect from \(\mathbf{z}_{2}\) to its descendants.

**Experiment settings.** As a proof of concept, we choose a linear Gaussian additive noise model and a nonlinear mixing function implemented as a 3-layer invertible MLP with Leaky-ReLU activation. We average the results over three independently sampled _ninterventional_ densities \(\tilde{p}(z_{2})\) while guaranteeing all _ninterventional_ distributions satisfy Defn. 2.1 (ii). As the marginal distribution of both \(\mathbf{z}_{1},\mathbf{z}_{3}\) remains the same after a _nintervention_, we expect \(\mathbf{z}_{1},\mathbf{z}_{3}\) to be block-identified (Defn. 3.1) according to Thm. 3.1. In practice, we enforce the marginal invariance constraint (Constraint 3.1) by minimizing the MMD loss, as implemented by the interventional CRL works [6, 37] and train an auto-encoder for a sufficient representation (Constraint 3.2). Further details are included in App. G.

**Results.** To validate block-identifiability, we perform Kernel-Ridge Regression between the estimated block \([\hat{\mathbf{z}}_{1},\hat{\mathbf{z}}_{3}]\) and the ground truth latents \(\mathbf{z}_{1},\mathbf{z}_{2},\mathbf{z}_{3}\) respectively. Both \(\mathbf{z}_{1},\mathbf{z}_{3}\) are block-identified, showing a high \(R^{2}\) score of \(0.863\pm 0.031\) and \(0.872\pm 0.035\), respectively. By contrast, the latent variable \(\mathbf{z}_{2}\) is not identified, evidenced by a low \(R^{2}\) of \(0.065\pm 0.017\).

## Appendix G Implementation Details

### Case Study: ISTAnt

**Problem.** Despite the majority of causal representation learning algorithms being designed to enforce the identifiability of some latent factors and tested on controlled synthetic benchmarks, there are a plethora of real-world applications across scientific disciplines requiring representation learning to answer causal questions [67, 68, 69, 70]. Recently, Cadei et al. [17] introduced ISTAnt, the first real-world representation learning benchmark with a real causal downstream task (treatment effect estimation). This benchmark highlights different challenges (sources of biases) that could arise from machine learning pipelines even in the simplest possible setting of a randomized controlled trial. Videos of ants triplets are recorded, and a per-frame representation has to be extracted for supervised behavior classification to estimate the Average Treatment Effect of an intervention (exposure to a chemical substance). Beyond desirable identification result on the latent factors (implying that the causal variables are recovered without bias), no clear algorithm has been proposed yet on minimizing the Treatment Effect Bias (TEB) [17]. One of the challenges highlighted by Cadei et al. [17] is that in practice, there is both covariate and concept shifts due to the effect modification from training on a non-random subset of the RCT because, for example, ecologists do not label individual frames but whole video recordings.

**Solution.** Relying on our framework, we can explicitly aim for low TEB by leveraging _known data symmetries_ from the experimental protocol. In fact, the causal mechanism (\(P(Y^{e}|do(\bm{X}^{e}=\bm{x})\)) stays invariant among the different experiment settings (i.e., individual videos or position of the petri dish). This condition can be easily enforced by existing domain generalization algorithms. For exemplary purposes, we choose Variance Risk Extrapolation [13, V-REx], which directly enforces both the invariance sufficiency constraints (Constraints 3.1 and 3.2) by minimizing the the Empirical Risk together with the risk variance inter-environments.

Figure 4: Examples of high-dimensional observations \(\bm{X}\) with corresponding annotated social behaviour \(Y\) (grooming). Figure and caption adapted from [17, Fig. 2]

Figure 3: Causal Model for generic partially annotated scientific experiment: \(T\) treatment, \(\bm{W}\) experimental settings, \(\bm{X}\) high-dimensional observation, \(Y\) outcome, \(S\) annotation flag. Figure and caption adapted from [17, Fig. 1]

### Synthetic Ablation with "Ninterventions"

The numerical data is generated using a linear Gaussian additive noise model as follows:

\[\begin{split}& p(\mathbf{z}_{1})=\mathcal{N}(\mu_{1},\sigma_{1}^{2}) \\ & p(\mathbf{z}_{2}\mid\mathbf{z}_{1})=\mathcal{N}(\alpha_{1}\cdot \mathbf{z}_{1}+\beta_{1},\sigma_{2}^{2})\\ & p(\mathbf{z}_{3}\mid\mathbf{z}_{2})=\mathcal{N}(\alpha_{2} \cdot\mathbf{z}_{2}+\beta_{2},\sigma_{3}^{2})\\ &\tilde{p}(\mathbf{z}_{2})=\mathcal{N}(\tilde{\mu}_{2},\tilde{ \sigma}_{2}^{2})\end{split}\] (G.1)

We choose \(\mu_{1}=10.5,\sigma_{1}=0.8,\alpha_{1}=0.02,\beta_{1}=0,\sigma_{2}=0.5,\alpha_ {2}=1,\beta_{2}=3,\sigma_{3}=1,\tilde{\sigma}_{2}=0.02\). We sample three independent \(\tilde{\mu}_{2}\) according to a uniform distribution \(\mathrm{Unif}[2,5]\) to validate the consistency of the identification results.

For the training, we employ a simple auto-encoder architecture implementing both encoder and decoder as 3-Layer MLP. We enforce the marginal invariance using the Max Mean Discrepancy loss (MMD) on the first and last component \(\hat{\mathbf{z}}_{1},\hat{\mathbf{z}}_{3}\). Formally, the objective function writes

\[\mathcal{L}(g,\hat{f})=\mathbb{E}_{\mathbf{x},\tilde{\mathbf{x}}}\left[\left\| \hat{f}(g(\mathbf{x}))-\mathbf{x}\right\|_{2}^{2}+\left\|\hat{f}(g(\tilde{ \mathbf{x}}))-\mathbf{x}\right\|_{2}^{2}\right]+\text{MMD}(g(\mathbf{x})_{[1,3 ]},g(\tilde{\mathbf{x}})_{[1,3]}),\]

where \(\mathbf{x},\tilde{\mathbf{x}}\) denote the observational and ninterventional data, respectively.

Further training details are summarized in Tab. 1

## Appendix H Further Discussions and Connections to Other Fields

In this paper, we take a closer look at the wide range of causal representation learning methods. Interestingly, we find that the differences between them may often be more related to "semantics" than to fundamental methodological distinctions. We identified two components involved in identifiability results: preserving information of the data and a set of known invariances. Our results have two immediate implications. First, they provide new insights into the "causal representation learning problem," particularly clarifying the role of causal assumptions. We have shown that while learning the graph requires traditional causal assumptions such as additive noise models or access to interventions, identifying the causal variables may not. This is an important result, as access to causal variables is standalone useful for downstream tasks, e.g., for training robust downstream predictors or even extracting pre-treatment covariates for treatment effect estimation [71], even without knowledge of the full causal graph. Second, we have exemplified how causal representation can lead to successful applications in practice. We moved the goal post from a characterization of specific assumptions that lead to identifiability, which often do not align with real-world data, to a general recipe that allow practitioners to specify known invariances in their problem and learn representations that align with them. In the domain generalization literature, it has been widely observed that invariant training methods often do not consistently outperform empirical risk minimization (ERM). In our experiments, instead, we have demonstrated that the specific invariance enforced by

\begin{table}
\begin{tabular}{l l} \hline
**Parameter** & **Value** \\ \hline Mixing function & 3-layer MLP \\ Encoder & 3-layer MLP \\ Decoder & 3-layer MLP \\ Hidden dim & 128 \\ Activation & Leaky-ReLU \\ Optimizer & Adam \\ Adam: learning rate & 1e-4 \\ Adam: beta1 & 0.9 \\ Adam: beta2 & 0.999 \\ Adam: epsilon & 1e-8 \\ Batch size & 4000 \\ Sample size & 200,000 \\ \# Epochs & 500 \\ \hline \end{tabular}
\end{table}
Table 1: Training setup for synthetic ablations in App. F.

V-REx [13] entails good performance in our causal downstream task (SS 4). Our paper leaves out certain settings concerning identifiability that may be interesting for future work, such as discrete variables and finite samples guarantees.

One question the reader may ask, then, is "_so what is exactly causal in causal representation learning?_". We have shown that the identifiability results in typical causal representation learning are primarily based on invariance assumptions, which do not necessarily pertain to causality. We hope this insight will broaden the applicability of these methods. At the same time, we used causality as a language describing the "parameterization" of the system in terms of latent causal variables with associated known symmetries. Defining the symmetries at the level of these causal variables gives the identified representation a causal meaning, important when incorporating a graph discovery step or some other causal downstream task like treatment effect estimation. Ultimately, our representations and latent causal models can be "true" in the sense of [72] when they allow us to predict "causal effects that one observes in practice". Overall, our view also aligns with "phenomenological" accounts of causality [73], that define causal variables from a set of elementary interventions. In our setting too, the identified latent variables or blocks thereof are directly defined by the invariances at hand. From the methodological perspective, all is needed to learn causal variables is for the symmetries defined over the causal latent variables to entail some statistical footprint across pockets of data. If variables are available, learning the graph has a rich literature [74], with assumptions that are often compatible with learning the variables themselves. Our general characterization of the variable learning problem opens new frontiers for research in representation learning:

### Representational Alignment and Platonic Representation

Several works ([75; 76; 77; 78]) have highlighted the emergence of similar representations in neural models trained independently. In [78] is hypothesized that neural networks, trained with different objectives on various data and modalities, are converging toward a _shared_ statistical model of reality within their representation spaces. To support this hypothesis, they measure the alignment of representations proposing to use a mutual nearest-neighbor metric, which measures the mean intersection of the k-nearest neighbor sets induced by two kernels defined on the two spaces, normalized by k. This metric can be an instance to the distance function in our formulation in Thm. 3.1. Despite not being optimized directly, several models in multiple settings (different objectives, data and modalities) seem to be aligned, hinting at the fact that their individual training objectives may be respecting some unknown symmetries. A precise formalization of the latent causal model and identifiability in the context of foundational models remains open and will be objective for future research.

### Environment Discovery

Domain generalization methods generalize to distributions potentially far away from the training, distribution, via learning representations invariant across distinct environments. However this can be costly as it requires to have label information informing on the partition of the data into environments. Automatic environment discovery ([79; 80; 81]) attempts to solve this problem by learning to recover the environment partition. This is an interesting new frontier for causal representation learning, discovering data symmetries as opposed to only enforcing them. For example, this would correspond to having access to multiple interventional distributions but without knowing which samples belong to the same interventional or observational distribution. Discovering that a data set is a mixture of distributions, each being a different intervention on the same causal model, could help increase applicability of causal representations to large obeservational data sets. We expect this to be particularly relevant to downstream tasks were biases to certain experimental settings are undesirable, as in our case study on treatment effect estimation from high-dimensional recordings of a randomized controlled trial.

### Connection with Geometric Deep Learning

Geometric deep learning (GDL) ([82; 83]) is a well estabilished learning paradigm which involves encoding a geometric understanding of data as an inductive bias in deep learning models, in order to obtain more robust models and improve performance. One fundamental direction for these priors is to encode symmetries and invariances to different types of transformations of the input data, e.g. rotations or group actions ([84; 85]), in representational space. Our work can be fundamentally related with this direction, with the difference that we don't aim to model _explicitly_ the transformations of the input space, but the invariances defined at the latent level. While an initial connection has been developed for disentanglement [86; 87], a precise connection between GDL and causal representation learning remains a open direction. We expect this to benefit the two communities in both directions: (i) by injecting geometric priors in order to craft better CRL algorithms and (ii) by incorporating causality into successful GDL frameworks, which have been fundamentally advancing challenging real-world problems, such as protein folding ([88]).

[MISSING_PAGE_EMPTY:31]

[MISSING_PAGE_FAIL:32]

[MISSING_PAGE_EMPTY:33]

[MISSING_PAGE_FAIL:34]

\begin{tabular}{c c c c c c c c} \hline \hline

\begin{tabular}{c c c c c c c c} \hline \hline
**Work** & \begin{tabular}{c} **Causal** \\ **Model** \\ \end{tabular} & \begin{tabular}{c} **Mixing** \\ **Function** \\ \end{tabular} & **Invariance** & \begin{tabular}{c} **Source of invariance** \\ **ance, Inv. subset.** \(\lambda\) \\ \end{tabular} & **Invariance reg.** & **Sufficiency reg.** & **Identifiability** & **Expl.** \\ \hline Arjovsky & nonparam. & nonparam. & risk & \begin{tabular}{c} invariant relationship between label and invariant \\ features, preserved \\ under covariate \\ \end{tabular} & \(\|\mathbb{V}_{\mathbf{w},\mathbf{w}=1}R^{\mathbf{k}}(\mathbf{w}\circ g)\|^{2}\) & \(\sum_{k\in[K]}R^{\mathbf{k}}(\mathbf{w}\circ g)\) & NA & - \\ \hline Krueger & nonparam. & nonparam. & risk & \begin{tabular}{c} invariant relationship between label and invariant \\ features, preserved \\ shift \\ \end{tabular} & \(\mathbb{V}_{\mathbf{w},\mathbf{w}=1}R^{\mathbf{k}}(\mathbf{w}\circ g)\|^{2}\) & \(\sum_{k\in[K]}R^{\mathbf{k}}(\mathbf{w}\circ g)+\text{Var}(R)\) & NA & (f) \\ \hline Ahuja & nonparam. & nonparam. & risk & 
\begin{tabular}{c} invariant relationship between label and invariant \\ features, preserved \\ shift \\ \end{tabular} & \(\|\mathbb{V}_{\mathbf{w},\mathbf{w}=1}R^{\mathbf{k}}(\mathbf{w}\circ g)\|^{2}\) & \(\sum_{k\in[K]}R^{\mathbf{k}}(\mathbf{w}\circ g)+\text{Var}(R)\) & NA & - \\ \hline \hline \end{tabular}