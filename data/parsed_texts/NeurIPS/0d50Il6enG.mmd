# Non-parametric classification via expand-and-sparsify representation

Kaushik Sinha

School of Computing

Wichita State University

Wichita, KS 67260

kaushik.sinha@wichita.edu

###### Abstract

In _expand-and-sparsify_ (EaS) representation, a data point in \(\mathcal{S}^{d-1}\) is first randomly mapped to higher dimension \(\mathbb{R}^{m}\), where \(m>d\), followed by a sparsification operation where the informative \(k\ll m\) of the \(m\) coordinates are set to one and the rest are set to zero. We propose two algorithms for non-parametric classification using such EaS representation. For our first algorithm, we use _winners-take-all_ operation for the sparsification step and show that the proposed classifier admits the form of a locally weighted average classifier and establish its consistency via Stone's Theorem. Further, assuming that the conditional probability function \(P(y=1|x)=\eta(x)\) is Holder continuous and for optimal choice of \(m\), we show that the convergence rate of this classifier is minimax-optimal. For our second algorithm, we use _empirical \(k\)-thresholding_ operation for the sparsification step, and under the assumption that data lie on a low dimensional manifold of dimension \(d_{0}\ll d\), we show that the convergence rate of this classifier depends only on \(d_{0}\) and is again minimax-optimal. Empirical evaluations performed on real-world datasets corroborate our theoretical results.

## 1 Introduction

Given a training set \(x_{1},\ldots,x_{n}\) of size \(n\) and a test point \(x\) sampled _i.i.d._ from a distribution \(\mu\) over some sample space \(\mathcal{X}\subset\mathbb{R}^{d}\), the basic idea of non-parametric estimation (e.g., density estimation, regression or classification) is to construct a function \(x\mapsto f_{n}(x)=f_{n}(x,x_{1},\ldots,x_{n})\) that approximates the true function \(f\) (which could be a density/regression/classification function as appropriate) as \(n\rightarrow\infty\)Tsybakov (2008). For any \(x\in\mathcal{X}\), \(f_{n}(x)\) typically depends on a small subset of the training set lying within a small neighborhood of (thus close to) \(x\). For example, in case of \(k\)-nearest neighbor, \(f_{n}(x)\) depends on the \(k\) points from the training set that are closest to \(x\); in case of random forest, for each tree constituting the forest, \(f_{n}(x)\) depends on the points from the training set that are routed to the same leaf node to which \(x\) is also routed to; in case of kernel methods, \(f_{n}(x)\) depends on the points from training set defined by an appropriate kernel. In this paper, we propose a new non-parametric classification method where, for any \(x\in\mathcal{S}^{d-1}\), the appropriate neighborhoods are obtained using expand-and sparsify (EaS) representation of \(x\).

On a high level, expand-and-sparsity representation is essentially a transformation from a low-dimensional dense representation of sensory inputs to a much higher-dimensional, sparse representation. Such representation has been found, for instance, in the olfactory system of the fly (Wilson (2013)) and mouse (Stettler and Axel (2009)), the visual system of the cat (Olshausen and Field (2004)), and the electrosensory system of the electric fish (Chacron et al. (2011)). For example, in the olfactory system of Drosophila (Turner et al. (2008), Masse et al. (2009), Wilson (2013), Caron et al. (2013)), the primary sense receptors of the fly are the roughly 2,500 odor receptor neurons(also known as, ORNs), which can be clustered into 50 types, based on their odor responses, leading to a dense, 50-dimensional sensory input vector. In fact, all ORNs of a given type converge on a corresponding glomerulus in the antennal lobe; there are 50 of these in a topographically fixed configuration. This information is then relayed via projection neurons to a collection of roughly 2000 Kenyon cells (KCs) in the mushroom body, with each KC receiving signal from roughly 5-10 glomeruli (Dasgupta and Tosh (2020). The pattern of connectivity between the glomeruli and Kenyon cells appears random (Chacron et al. (2011)). The output of the KCs is integrated by a single anterior paired lateral (APL) neuron which then provides negative feedback causing all but the 5% highest-firing KCs to be suppressed (Lin et al. (2014)). The result is a random sparse high-dimensional representation of the sensory input, that is the basis for subsequent learning. The primary motivation of this paper is to study the benefit of this type of naturally occurring representation in the context of supervised classification.

In our setting, the EaS representation is a mapping from the \(d\)-dimensional unit sphere \(\mathcal{S}^{d-1}\) to \(\{0,1\}^{m},m\gg d\), where a data point \(x\in\mathcal{S}^{d-1}\) is first randomly mapped to higher dimension \(\mathbb{R}^{m}\) using a random projection matrix \(\Theta\), and is followed by a sparsification operation, where the informative \(k\ll m\) of the \(m\) coordinates of the resulting vector \(\Theta x\in\mathbb{R}^{m}\) are set to one and the rest of the coordinates are set to zero. Rows of \(\Theta\) are typically drawn independently from some distribution \(\nu\) over \(\mathcal{S}^{d-1}\) or \(\mathbb{R}^{d}\). Let \(C_{j},j=1,\ldots,m\), be the set of all examples from the input space \(\mathcal{X}\subset\mathcal{S}^{d-1}\) whose \(j^{th}\) coordinate in respective EaS representations are set to 1. We call \(C_{j}\) the _response region_ of \(\theta_{j}\) (the \(j^{th}\) row of \(\Theta\)). Note that for any \(x\in\mathcal{X}\), there are \(k\)_activated_ response regions corresponding to the \(k\) non-zero bits in the EaS representation of \(x\). These \(k\) activated response regions serves as \(k\) neighborhoods of \(x\) for our proposed non-parametric classifier, where, for any \(x\in\mathcal{X}\) and each activated response region \(C_{j}\), we estimate \(\Pr(y=1|C_{j})\) - expected value of \(y\) when \(x\) is restricted to \(C_{j}\) - and take their average to be the estimate of \(\Pr(y=1|x)\). In a toy example, we visually show the EaS representation and the activated response regions \(C_{j}\) of a data point in Fig. 1. Comparing whether this conditional probability estimate exceeds \(1/2\), we predict the class label of \(y\). In particular, we present two algorithms using different sparsification schemes and analyze their theoretical properties.

One may find similarity between our proposed algorithm and a random forest classifier - for any \(x\in\mathcal{X}\), \(k\) activated response regions \(C_{j},x\in C_{j}\), may be interpreted as the leaf nodes (containing \(x\)) of \(k\) decision trees in a random forest. However, unlike random forest, we can not simply increase \(k\) (number of trees) without changing other hyper-parameters (such as \(m\)) hoping to achieving better prediction performance. Therefore, even-though the consistency property of random forest is studied under different settings Biau et al. (2008); Scornet et al. (2015); Scornet (2016); Tang et al. (2018), those ideas can not be applied for our theoretical analysis.

We summarize our contributions below:

* We present an interesting connection between non-parametric estimation and EaS representation, and propose two algorithms for non-parametric classification via EaS representation, and present empirical evaluations on benchmark machine learning datasets.
* For our first algorithm (using \(k\)-WTA sparsification), we establish its universal consistency and prove that it achieves minimax-optimal convergence rate that depends on dimension \(d\)

Figure 1: _Top:_ A point \(x\in\mathbb{R}^{2}\) (coordinate-wise values are different shades of gray) and its projection \(y=\Theta x\in\mathbb{R}^{15}\) (coordinate-wise values are different shades of red). The sparsification step sets the largest 5 values of \(y\) to 1 (black squares) and the rest to zero. _Bottom:_ Activated response regions \(C_{j},x\in C_{j}\), (\(x\) is a red dot), are shown using different colors. The points from the training set that intersects with these activated response regions are shown using black dots.

* When data lie on a low dimensional manifold having dimension \(d_{0}\ll d\), we present a second algorithm (using empirical \(k\)-thresholding sparsification) and prove that its convergence rate is minimax-optimal and depends only on \(d_{0}\).

The rest of the paper is organized as follows. We discuss related work in 2. We present our first algorithm and its theoretical analysis including consistency and convergence rate in section 3. We present our second algorithm and derive its convergence rate in section 4. We present our empirical results in section 5 and conclude discussing limitations and future work in section 6.

## 2 Related work

Non-parametric estimation is an important branch of statistics with a rich history and classical results. Interested readers may refer to Tsybakov (2008); Gyorfi et al. (2002), which provide excellent treatment of this subject. Here we briefly review consistency and convergence rates of well-known non-parametric methods such as, partitioning estimates (histograms), \(k\)-nearest neighbors, kernel methods, and random forests. In the non-parametric literature, it is typical to estimate the regression function \(\eta(x)=\mathbb{E}(y|x)\) from data, and use the resulting estimate \(\eta_{n}\) (using a sample of size \(n\)) to construct plug-in decision function (classification rule). Under mild conditions, such regression estimates and the resulting plug-in classification rules are consistent for histograms, \(k\)-nearest neighbors, and kernel methods Gyorfi et al. (2002); Devroye et al. (1996). Under mild conditions, different variations of random forest methods are also known to be consistent Biau et al. (2008); Scornet et al. (2015); Scornet (2016); Tang et al. (2018). Under the assumption that the regression function \(\eta(x)\) is Lipschitz continuous, the \(L_{2}\) error of the regression estimate of histogram convergences at a rate \(O\left(n^{-1/(d+2)}\right)\) (Theorem 4.3 Gyorfi et al. (2002), the \(L_{2}\) error of the regression estimate of the kernel method convergence at a rate \(O\left(n^{-1/(d+2)}\right)\) (Theorem 5.2 Gyorfi et al. (2002)), and the \(L_{2}\) error of the regression estimate of \(k\)-nearest neighbors method convergences at a rate \(O\left(n^{-2/(d+2)}\right)\) (Theorem 6.2 Gyorfi et al. (2002)). For random forest, Gao and Zhou (2020) established finite-sample rate \(O(n^{-1/(8d+2)})\) on the convergence of pure random forests for classification, which was be improved to be of \(O(n^{-1/(3.87d+2)})\) by considering the midpoint splitting mechanism. They introduced another variant of random forests, which follow Breiman's original random forests but with different mechanisms for splitting dimensions and positions, to get a convergence rate \(O(n^{-1/(d+2)}(\ln n)^{1/(d+2)})\), which reaches the minimax rate, except for a factor \((\ln n)^{1/(d+2)}\).

For EaS representation, when \(\nu\) is the uniform distribution over \(\mathcal{S}^{d-1}\) and the sparsification scheme is a \(k\)-winners-take-all (\(k\)-WTA) operation that sets the \(k\) largest entries of a vector in \(\mathbb{R}^{m}\) to one the rest to zero, Dasgupta and Tosh (2020) proposed a function approximation scheme using such EaS representation that can approximate any Lipschitz continuous function \(f:\mathcal{S}^{d-1}\rightarrow\mathbb{R}\) in the \(L_{\infty}\) norm, where the error decays exponentially slowly with \(d\). Further, they showed when the data lie on a low dimensional submanifold of \(\mathcal{S}^{d-1}\) having dimension \(d_{0}\ll d\), using a different sparsification step, termed as \(k\)-thresholding, any Lipschitz continuous function defined on this manifold can be approximated in the \(L_{\infty}\) norm, where the error decays exponentially slowly with \(d_{0}\). A different EaS representation, where the projection matrix is a sparse binary matrix and the sparsification step is \(k\)-WTA, was proposed in Dasgupta et al. (2017) that effectively hash input data points and such hashing has been shown to provide improved performance in accurately solving the similarity search problems compared to the state-of-the-art locality sensitive hashing (LSH) based techniques (Gionis et al. (1999); Andoni and Indyk (2008); Datar et al. (2004)). Such EaS representation has also been used to summarize data in the form of a bloom filter, termed as fly bloom filter (FBF), and has been successfully used in solving the novelty detection problems in Dasgupta et al. (2018) and classification problem in Sinha and Ram (2021); Ram and Sinha (2021, 2022).

While our work is inspired by the results of Dasgupta and Tosh (2020), there are key differences. First, we explicitly expand upon and apply the idea of Dasgupta and Tosh (2020) to the _supervised_ binary classification setting, and derive the rate at which the error probability of our proposed classifier converges to that of the Bayes optimal classifier. In comparison, the main motivation of Dasgupta and Tosh (2020) was to prove the existence of a weight vector that results in arbitrarily well function approximation in the _unsupervised learning_ setting using EaS representation. Because of this existential nature of their result, the effect of sample size was neither needed nor considered in their result. Second, the \(k\)-thresholding sparsification scheme proposed in Dasgupta and Tosh[2020] for function approximation result assumed that the coordinate-wise thresholds were known. We make no such assumption and explicitly describe how to compute such thresholds from data-resulting in realizable algorithm and derive convergence rate of our proposed classifier that takes care of the uncertainly associated with random natures of these thresholds. Third, while Dasgupta and Tosh [2020] only considers LipSchitz continuous functions, we consider that conditional probability function \(\eta(x)=\Pr(y=1|x)\) to be Holder continuous (a broader function class), and prove that our proposed classifier achieves minimax-optimal convergence rate - whether such optimal convergence rate was achievable in the proposed setting was, unknown prior to our result.

Finally, we note that _random Fourier features_ Rahimi and Recht [2007] are generated using a construction similar to EaS, however the choice of random directions there are chosen using a kernel function that measures a notion of similarity in the input space. EaS representation can also be though of an opposite process of _compressed sensing_ Candes et al. [2006], Donoho [2006], where the goal is to recover a sparse vector given random projections to it, while random projection is used to generate a sparse representation in case of EaS.

## 3 Algorithm 1

```
\(\mathbf{Train}\)EsClassifier\((D_{n},m,k,R)\)  Sample \(\Theta\) with seed \(R\)  Initialize \(w[i]\), \(\mathtt{ct}[i]\gets 0,\ \forall i\in[m]\) for\((x,y)\in S\)do \(\mathtt{eas}\gets h_{1}(x)\) \(w[i]\gets w[i]+y,\ \forall i\in[m]:\mathtt{eas}[i]=1\) \(\mathtt{ct}[i]\leftarrow\mathtt{ct}[i]+1,\ \forall i\in[m]:\mathtt{eas}[i]=1\)  end for \(w[i]\gets w[i]/\mathtt{ct}[i],\ \forall i\in[m]\) return\(\Theta,w\) end for InferEsClassifier\((x,\Theta,k,w)\) \(\mathtt{eas}\gets h_{1}(x)\) return\(\mathbb{I}[(\mathtt{eas}\cdot w)/k\geq\frac{1}{2}]\)  end for
```

**Algorithm 1** Training set \(D_{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\subset\mathcal{S}^{d-1}\times\{0,1\}\), Projection dimensionality \(m\in\mathbb{N}\), \(k\ll m\) non-zeros in the EaS representation, random seed \(R\), and inference with test point \(x\in\mathcal{S}^{d-1}\).

```
\(\mathbf{Train}\)EsClassifier\((D_{n},m,k,R)\)  Sample \(\Theta\) with seed \(R\)  Initialize \(w[i]\), \(\mathtt{ct}[i]\gets 0,\ \forall i\in[m]\) for\((x,y)\in S\)do \(\mathtt{eas}\gets h_{1}(x)\) \(w[i]\gets w[i]+y,\ \forall i\in[m]:\mathtt{eas}[i]=1\) \(\mathtt{ct}[i]\leftarrow\mathtt{ct}[i]+1,\ \forall i\in[m]:\mathtt{eas}[i]=1\)  end for \(w[i]\gets w[i]/\mathtt{ct}[i],\ \forall i\in[m]\) return\(\Theta,w\) end for InferEsClassifier\((x,\Theta,k,w)\) \(\mathtt{eas}\gets h_{1}(x)\) return\(\mathbb{I}[(\mathtt{eas}\cdot w)/k\geq\frac{1}{2}]\)  end for
```

**Algorithm 2** Learning Algorithm 1

For \(x\in\mathcal{S}^{d-1}\), the EaS representation of \(x\), that uses \(k\)-WTA sparsification step, is given by the function \(h_{1}\colon\mathcal{S}^{d-1}\rightarrow\{0,1\}^{m}\) defined as,

\[h_{1}(x)=\Gamma_{k}(\Theta x), \tag{1}\]

where \(\Theta\) is a \(m\times d\) random projection matrix whose rows \(\theta_{1},\ldots,\theta_{m}\) are sampled i.i.d from the uniform distribution over \(\mathcal{S}^{d-1}\) and \(\Gamma_{k}\colon\mathbb{R}^{m}\rightarrow\{0,1\}^{m}\) is the \(k\)-WTA function converting a vector in \(\mathbb{R}^{m}\) to one in \(\{0,1\}^{m}\) by setting the largest \(k\ll m\) elements of \(\Theta x\) to \(1\) and the rest to zero. For any \(j\in[m]\), let \(C_{j}=\{x\in\mathcal{X}:h_{1}(x)[j]=1\}\). We note that the subsets (response regions) \(C_{1},\ldots,C_{m}\) does not form a partition since they can be overlapping. We summarize our first algorithm for binary classification using \(h_{1}\) in Alg. 1. During its learning/training phase, a vector \(w\in[0,1]^{m}\) summarizes the average \(y\) value over \(m\) response regions \(C_{j},j\in[m]\) using the training set. In particular, \(w[j],j\in[m]\) learned during the training phase is precisely given by

\[\hat{\eta}_{j}=\frac{\sum_{i=1}^{n}y_{i}\mathbb{I}[x_{i}\in C_{j}]}{\sum_{i=1} ^{n}\mathbb{I}[x_{i}\in C_{j}]} \tag{2}\]

Using (2),for any \(x\in\mathcal{X}\), we further define

\[\hat{\eta}(x)=\frac{1}{k}\sum_{j:x\in C_{j}}\hat{\eta}_{j} \tag{3}\]During the inference phase, for any test point \(x\), Alg. 1 first computes \((w\cdot h_{1}(x))/k\), which is an average of average \(y\) values over \(k\) response regions \(\{C_{j}:h_{1}(x)[j]=1\}\), which can be interpreted as an estimate of the conditional probability \(\eta(x)\) and is precisely the quantity \(\hat{\eta}(x)\) given in (3). Alg. 1 then makes its prediction based on whether \(\hat{\eta}(x)\) is greater than or equal to \(1/2\). We can rewrite this conditional probability estimate as follows.

\[\hat{\eta}(x) = \frac{w\cdot h_{1}(x)}{k}=\frac{1}{k}\sum_{j:x\in C_{j}}w[j]= \frac{1}{k}\sum_{j:x\in C_{j}}\hat{\eta}_{j}=\frac{1}{k}\sum_{j:x\in C_{j}} \frac{\sum_{i=1}^{n}y_{i}\mathbb{I}[x_{i}\in\mathcal{C}_{j}]}{\sum_{i=1}^{n} \mathbb{I}[x_{i}\in\mathcal{C}_{j}]} \tag{4}\] \[= \sum_{i=1}^{n}\underbrace{\left(\frac{1}{k}\sum_{j:x\in C_{j}} \frac{\mathbb{I}[x_{i}\in\mathcal{C}_{j}]}{\sum_{i=1}^{n}\mathbb{I}[x_{i}\in \mathcal{C}_{j}]}\right)}_{w_{n,i}(x)}y_{i}=\sum_{i=1}^{n}w_{n,i}(x)y_{i}\]

Using viewpoint (4), Alg. 1 can be interpreted as a "plug-in" classifier Devroye et al. (1996) where prediction is based on whether the estimated conditional probability \(\hat{\eta}\) exceeds \(1/2\) or not. In particular, classifier in Alg. 1 can be represented as \(g:\mathcal{X}\rightarrow\{0,1\}\) described as

\[g(x)=\begin{cases}1,&\text{if }\hat{\eta}(x)\geq 1/2,\\ 0,&\text{otherwise}.\end{cases} \tag{5}\]

In comparison, the Bayes optimal classifier \(g_{*}:\mathcal{X}\rightarrow\{0,1\}\) is defined as

\[g_{*}(x)=\begin{cases}1,&\text{if }\eta(x)\geq 1/2,\\ 0,&\text{otherwise}.\end{cases} \tag{6}\]

In equation 4, the weights \(w_{n,i}(x)=w_{n,i}(x,x_{1},\ldots,x_{n})\in\mathbb{R}\) depends on \(x_{1},\ldots,x_{n}\). Next we show that for sufficiently large \(n\) sum of these weights is 1. Therefore, \(\hat{\eta}\) is simply a weighted average estimator of \(\eta\) and is a non-parametric classifier Devroye et al. (1996).

**Lemma 3.1**.: _For any \(x\in\mathcal{X}\), suppose \(n\) is sufficiently large such that \(\{x_{i},\ldots,x_{n}\}\cap C_{j}\neq\emptyset\) for each \(j\) satisfying \(x\in C_{j}\). Then, \(\sum_{i=1}^{n}w_{n,i}(x)=1\)._

Indeed, we show in Lemma D.7 (in the Appendix), that for any \(x\in\mathcal{X}\), whenever \(n\rightarrow\infty\) and \(m^{k}/n\to 0\) as \(n\rightarrow\infty\), \(|\{x_{1},\ldots,x_{n}\}\cap\mathcal{C}_{j}|\rightarrow\infty\) for all \(j\) such that \(x\in C_{j}\).

### Consistency of Algorithm 1

In this section we prove that Alg. 1 is universally consistent. We start with the definition of consistent and universally consistent classification rule Devroye et al. (1996). Let \(D_{n}=\{(x_{1},y_{1}),\ldots,((x_{n},y_{n})\}\) be a training set sampled _i.i.d._ from a certain distribution of \((x,y)\) and let \(g_{n}:\mathcal{X}\rightarrow\{0,1\}\) be a classification rule learned using \(D_{n}\). Then \(g_{n}\) is consistent (or asymptotically Bayes-risk efficient) for a certain distribution of \((x,y)\) if the expected error probability \(\mathbb{E}L_{n}=\Pr\left\{g_{n}(x,D_{n})\neq y\right\}\to L^{*}\) as \(n\rightarrow\infty\), where \(L^{*}\) is the Bayes error probability. A sequence of decision rules is called universally consistent, if it is consistent for any distribution of the pair \((x,y)\). It is well known that a general theorem by Stone Stone (1977) (presented below) provides a recipe for establishing universal consistency of any classification rule of the form

\[g_{n}(x)=\begin{cases}1,&\text{if }\eta_{n}(x)\geq 1/2,\\ 0,&\text{otherwise}.\end{cases} \tag{7}\]

based on an estimate \(\eta_{n}(x)\) of the conditional probability \(\eta(x)\), satisfying \(\eta_{n}(x)=\sum_{i=1}^{n}w_{n,i}(x)y_{i}\) where weights \(w_{n,i}(x)=w_{n,i}(x,x_{1},\ldots,x_{n})\) are non-negative and \(\sum_{i=1}^{n}w_{n,i}(x)=1\).

**Theorem 3.2**.: _[_Stone's Theorem (Theorem 6.3 (Devroye et al., 1996)]__]_ _Assume that for any distribution of \(x\), the weights satisfy the following three conditions: (i) There is a constant \(c\) such that, for every non-negative measurable function \(f\) satisfying \(\mathbb{E}f(x)<\infty\), \(\mathbb{E}\left(\sum_{i=1}^{n}w_{n,i}(x)f(x_{i})\right)\leq c\mathbb{E}f(x)\). (ii) For all \(a>0\), \(\lim_{n\rightarrow\infty}\mathbb{E}\left(\sum_{i=1}^{n}w_{i,n}(x)\mathbb{I}_{ \{\|x_{i}-x\|>a\}}\right)=0\). (iii) \(lim_{n\rightarrow\infty}\mathbb{E}\left(\max_{1\leq i\leq n}w_{n,i}(x)\right)=0\). Then \(g_{n}\) is universally consistent._

[MISSING_PAGE_FAIL:6]

With this definition, we introduce an intermediate quantity

\[\bar{\eta}(x)=\frac{1}{k}\sum_{j:x\in C_{j}}\bar{\eta}_{j} \tag{10}\]

where, \(\bar{\eta}_{j}\) is defined as,

\[\bar{\eta}_{j}=\frac{1}{\mu(C_{j})}\int_{C_{j}}\eta(x)\mu(dx)=\eta(C_{j}) \tag{11}\]

Using triangle inequality, this allows us to write: \(\mathbb{E}_{x,D_{n}}|\hat{\eta}(x)-\eta(x)|=\mathbb{E}_{x,D_{n}}|\hat{\eta}(x)- \bar{\eta}(x)+\bar{\eta}(x)-\eta(X)|\leq\mathbb{E}_{x,D_{n}}|\hat{\eta}(x)-\bar {\eta}(x)|+\mathbb{E}_{x,D_{n}}|\bar{\eta}(x)-\eta(x)|\), and we show how to individually bound each term on the right-hand side of this inequality next.

_Remark 3.7_.: Note that, once \(m\) and \(k\) are fixed, our hypothesis space is the set of all linear models on the \(k\)-sparse \(m\) dimensional binary vectors. The two terms on the right-hand side of the above inequality correspond to _estimation error_ (the error of our proposed classifier with respect to the best hypothesis from the hypothesis space) and _approximation error_ (the error difference between the best hypothesis from the hypothesis space and the target classifier, i.e., Bayes optimal), respectively.

### Bounding \(\mathbb{E}_{x,D_{n}}|\hat{\eta}(x)-\bar{\eta}(x)|\)

Note that there is an inherent randomness in our proposed algorithm associated with the choice of \(\Theta\). In particular, the response regions \(C_{j}\) are random quantities that depend on the choice of \(\Theta\). In this section, we fix \(\Theta\) and conditioned on this, we bound \(\mathbb{E}_{x,D_{n}}|\hat{\eta}(x)-\bar{\eta}(x)|\). This ensures that for any \(\delta>0\), the same bound holds with probability at least \(1-\delta\) over the choice of \(\Theta\).

**Lemma 3.8**.: _Fix any \(\Theta\). Then we have, \(\mathbb{E}_{x,D_{n}}|\hat{\eta}(x)-\bar{\eta}(x)|\leq\sqrt{\frac{m}{kn}}\)._

Sketch of proof:.: From the definition of \(\hat{\eta}(x)\) and \(\bar{\eta}(x)\) given in (3) and (10) and applying Jensen's inequality, crux of the proof is to focus on the expected value of the random quantity \(\frac{1}{k}\sum_{j:x\in C_{j}}\left(\frac{\sum_{i=1}^{n}y_{i}[x_{i}\in C_{j}]}{ \sum_{i=1}^{n}[1[x_{i}\in C_{j}]}-\eta(C_{j})\right)^{2}\). Note that, for \(j\in[m]\), \(\sum_{i=1}^{n}\mathbb{I}[x_{i}\in C_{j}]=n\mu_{n}(C_{j})\) is the number of the points from \(D_{n}\) that fall in \(C_{j}\), where \(\mu_{n}(C_{j})\) is the empirical probability estimate. We bound the quantity of interest in Lemma 3.8 as a sum of two quantities corresponding to the following two cases:

**Case 1: when \(\boldsymbol{\mu_{n}(C_{j})=0}\).** Using the notation \(0/0=0\), the quantity of interest becomes \(\frac{1}{k}\sum_{j:x\in C_{j}}\left(\eta^{2}(C_{j})\mathbb{I}[\mu_{n}(C_{j})=0 ]\right)\). Utilizing the properties of the response regions established in Lemma 3.4, we show in Lemma E.3 that the expected value of the quantity of interest at most \(\frac{m}{nke}\).

**Case 2: when \(\boldsymbol{n\mu_{n}(C_{j})>0}\).** Here the quantity of interest becomes \(\frac{1}{k}\sum_{j:x\in C_{j}}\left(\frac{\sum_{i=1}^{n}(y_{i}-\eta(C_{j})[|x_ {i}\in C_{j}]}{n\mu_{n}(C_{j})})^{2}\mathbb{I}[n\mu_{n}(C_{j})>0]\right.\). Conditioned on \(x,x_{1},\ldots,x_{n}\), we first show in Lemma E.1 that expected value (w.r.t. \(y_{1},\ldots,y_{n}\)) of this quantity becomes at most \(\frac{1}{4k}\sum_{j:x\in C_{j}}\left(\frac{[\eta[n\mu_{n}(C_{j})>0]}{n\mu_{n}( C_{j})}\right)\). Next, conditioned on \(x_{1},\ldots,x_{n}\), and utilizing the properties of the response regions established in Lemma 3.4, we show in Lemma E.2 that expected value (w.r.t. \(x\)) of this quantity is at most \(\frac{1}{4}\sum_{j=1}^{m}\left(\frac{\mu(C_{j})[1|n\mu_{n}(C_{j})>0]}{n\mu_{n} (C_{j})}\right)\). Finally, using standard Binomial bound (Lemma E.4) we show that expected value (w.r.t. \(x_{1},\ldots,x_{n}\)) of this quantity is at most \(\frac{m}{2kn}\).

### Bounding \(\mathbb{E}_{x,D_{n}}|\bar{\eta}(X)-\eta(x)|\)

In order to bound the expected value of \(|\bar{\eta}(x)-\eta(x)|\), we need to impose certain smoothness condition on \(\eta\). We consider a general form of smoothness, known as Holder continuity for \(\eta\).

**Definition 3.9**.: We say that \(\eta:\mathcal{X}\to[0,1]\) is \((L,\beta)\) smooth if for all \(x,x^{\prime}\in\mathcal{X}\), we have \(|\eta(x)-\eta(x^{\prime})|\leq L\|x-x^{\prime}\|^{\beta}\).

Using Holder continuity assumption above, we first show the following:

**Lemma 3.10**.: _Suppose \(\eta\) is \((L,\beta)\) smooth. Then, \(\sup_{x\in\mathcal{S}}|\eta(x)-\bar{\eta}(x)|\leq L\cdot\max_{j\in[m]}(\mathrm{ diam}(C_{j}))^{\beta}\)._

We have already shown in Lemma 3.5 how to bound the diameters of \(C_{j}\). Combining these two results we have 

**Lemma 3.11**.: _Let \(d\geq 3\) and pick any \(0<\delta<1\). Assume \(\eta\) to be \((L,\beta)\) smooth. There is an absolute constant \(c_{0}>0\) such that the following holds. If \(k\geq c_{0}(d\log m+\log(1/\delta))\) then with probability at least \(1-\delta\), \(\mathbb{E}_{x,D_{n}}|\tilde{\eta}(x)-\eta(x)|\leq 8L\left(2k/m\right)^{\frac{\beta} {d-1}}.\)_

Combining Lemma 3.8, 3.11 and 3.6 we are present the main result of this section.

**Theorem 3.12**.: _Let \(D_{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\subset\mathcal{X}\times\{0,1\}\) be the training data and consider the EaS representation given in (1). Let \(d\geq 3\) and pick any \(0<\delta<1\). Assume \(\eta\) to be \((L,\beta)\) smooth. There is an absolute constant \(c_{0}>0\) such that the following holds. If \(k\geq c_{0}(d\log m+\log(1/\delta))\) then with probability at least \(1-\delta\) over the random choice of \(\Theta\),_

\[\Pr(g(x)\neq y)-L^{*}\leq 2\left(\sqrt{\frac{m}{kn}}+8L\left(\frac{2k}{m} \right)^{\frac{\beta}{d-1}}\right)\]

**Corollary 3.13**.: _In Theorem 3.12, set \(m=kn^{\frac{(d-1)}{2\beta+(d-1)}}\). Then, with probability at least \(1-\delta\), over the choice of \(\Theta\),_

\[\Pr(g(x)\neq y)-L^{*}=O\left(n^{-\frac{\beta}{2\beta+(d-1)}}\right).\]

_Remark 3.14_.: Since \(\mathcal{X}\subset\mathcal{S}^{d-1}\), the effective dimension in our setting is \(d^{\prime}=(d-1)\) and the convergence rate of Corollary 3.13 can be rewritten as \(O\left(n^{-\frac{\beta}{2\beta+d^{\prime}}}\right)\) which is minimax-optimal for plug-in classifiers under the assumption that \(\eta\) is \((L,\beta)\)-smooth (Audibert and Tsybakov, 2007).

### Inability to adapt to manifold structure

In Theorem 3.12, we derived the convergence rate of the classifier presented in Alg. 1 by bounding \(\mathbb{E}_{x,D_{n}}|\eta(x)-\hat{\eta}(x)|\), which upper bounds the excess Bayes risk, from above and the resulting convergence rate decays exponentially slowly with the dimension \(d\). We now show that even if the data lie on a low dimensional manifold having dimesnion \(d_{0}\ll d\), there exists a smooth \(\eta\) such that the quantity \(\mathbb{E}_{x,D_{n}}|\eta(x)-\hat{\eta}(x)|\) decreases at a rate no faster than \(n^{-\frac{1}{d+1}}\). To prove claim, we assume data to lie on the following one-dimensional manifold:

\[\mathcal{X}_{1}=\{(x_{1},x_{2},0,0,\ldots,0)\in\mathbb{R}^{d}:x_{1}^{2}+x_{2}^ {2}=1\} \tag{12}\]

We further assume that \(k=\beta=1\) which implies that \(\eta\) is \(L\)-Lipschitz from the definition of \((L,\beta)\)-smoothness. Our lower bound result is as follows.

**Theorem 3.15**.: _For any \(d>3\), let input space \(\mathcal{X}_{1}\) be the one-dimensional sub-manifold of \(\mathbb{R}^{d}\) given in (12). Take \(k=1\) and \(\beta=1\). Suppose the random matrix \(\Theta\) has rows chosen from a distribution that is uniform over \(\mathcal{S}^{d-1}\). Then there exists a \(\frac{1}{2}\)-Lipschitz function \(\eta:\mathcal{X}_{1}\rightarrow[0,1]\) such that the following holds with probability at least \(1/2\) over the choice of \(\Theta\)._

\[\mathbb{E}_{x,D_{n}}|\eta(x)-\hat{\eta}(x)|=\Omega\left(n^{-\frac{1}{d+1}}\right)\]

## 4 Algorithm 2

In this section we present an alternate EaS representation and an associated classification algorithm that adapts to intrinsic dimension \(d_{0}\) when data lie on a low dimensional manifold with dimension \(d_{0}\ll d\). Here, we assume that the rows \(\theta_{i},i\in[m]\) of matrix \(\Theta\) are sampled _i.i.d._ from a multivariate Gaussian distribution \(N(0,1/\sqrt{d}I_{d})\), denote by \(\nu\), where \(I_{d}\) is \(d\times d\) identity matrix. This EaS representation is denoted by \(h_{2}:\mathcal{X}\rightarrow\{0,1\}^{m}\), where for any point \(x\in\mathcal{X}\), the \(j^{th}\) coordinate of \(h_{2}(x)\) is set to \(1\), as given in (14), using a data dependent threshold \(\tau_{n}\). In particular, given a training set of size1\(2n\) sampled _i.i.d._from \((\mu,\eta)\), using the first half of it, namely, \(D^{\prime}_{n}=\{(x^{\prime}_{1},y^{\prime}_{1}),\ldots,(x^{\prime}_{n},y^{ \prime}_{n})\}\), define \(\tau_{n}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) to be:

Footnote 1: Instead of representing the training set to be \(D_{2n}=\{(x_{i},y_{i})\}_{i=1}^{2n}\), for notational convenience, we represent its first half as \(D^{\prime}_{n}=\{(x^{\prime}_{i},y^{\prime}_{i})\}_{i=1}^{n}\) and the second half as \(D_{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\). \(D^{\prime}_{n}\) is only used to compute empirical thresholds in (13) and therefore, we could use unlabeled data sampled _i.i.d._from \(\mu\) for this purpose.

\[\tau_{n}(\theta){=}{\sup}\!\!\left\{\tau\!\!:\frac{1}{n}\sum_{i=1}^{n}\mathbb{I }[\theta\cdot x^{\prime}_{i}\geq\tau]\geq\frac{k}{m}\!\right\} \tag{13}\]\[h_{2}(x)[j]=\mathbb{I}[\theta_{j}\cdot x\geq\tau_{n}(\theta_{j})] \tag{14}\]

We call the sparsification scheme given in (14)_empirical-k-thresholding_. For \(j\in[m]\), the \(j^{th}\) response region \(C_{j}\) is defined as:

\[C_{j}=C(\theta_{j})=\{x\in\mathcal{X}:\theta_{j}\cdot x\geq\tau_{n}(\theta_{j})\} \tag{15}\]

Using the second half of the training data, namely \(D_{n}=\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\}\), average \(y\) values over different \(C_{j}\) are estimated and summarized in vector \(w\) in the same way as in Alg. 1 implying that \(w[j]=\hat{\eta}_{j},\forall j\in[m]\), where \(\hat{\eta}_{j}\) is given in (2). This completes the training phase of our proposed algorithm summarized in Alg. 2.

```
TrainEsClassifier\((D_{n},D^{\prime}_{n},k,t,R)\)  Sample \(\Theta\) with seed \(R\)  Initialize \(w[i],\mathtt{ct}[i]\gets 0,\;\forall i\in[m]\) for\(j\in[m]\)do  Compute \(\tau_{n}(\theta_{j})\) using (13) and \(D^{\prime}_{n}\)  end for for\((x,y)\in D_{n}\)do \(\mathtt{eas}\gets h_{2}(x)\) \(w[i]\gets w[i]+y,\;\forall i\in[m]:\mathtt{eas}[i]=1\) \(\mathtt{ct}[i]\leftarrow\mathtt{ct}[i]+1,\;\forall i\in[m]:\mathtt{eas}[i]=1\)  end for \(w[i]\gets w[i]/\mathtt{ct}[i],\;\forall i\in[m]\) return\(\Theta,w\)  end for InferEsClassifier\((x,\Theta,t,w)\) \(\mathtt{eas}\gets h_{2}^{n}(x)\) \(\Theta_{x}\leftarrow\{\theta_{i}:h_{2}(x)[i]=1\}\) \(\mathtt{eas}\gets\mathtt{eas}\) \(\mathtt{eas}[i]\gets 0,\forall i\in[m]:i\notin A_{t}(x)\) return\(\mathbb{I}[(\mathtt{eas}\cdot w)/t\geq\frac{1}{2}]\)  end for \(h_{2}(x)[j]=\mathbb{I}[\theta_{j}\cdot x\geq\tau_{n}(\theta_{j})]\) (14) \(\mathtt{eas}\leftarrow\mathtt{eas}\) \(\mathtt{eas}[i]\gets 0,\forall i\in[m]:i\notin A_{t}(x)\) return\(\mathbb{I}[(\mathtt{eas}\cdot w)/t\geq\frac{1}{2}]\)  end for \(h_{2}(x)[j]=\mathbb{I}[\theta_{j}\cdot x\geq\tau_{n}(\theta_{j})]\) (15)
```

**Algorithm 2** Training set \(D^{\prime}_{n}=\{(x^{\prime}_{i},y^{\prime}_{i})\}_{i=1}^{n},D_{n}=\{(x_{i},y _{i})\}_{i=1}^{n}\subset\mathcal{X}\times\{0,1\}\), Projection dimensionality \(m\in\mathbb{N}\), integer \(k\ll m\), integer \(t\), random seed \(R\), and inference with test point \(x\in\mathcal{S}^{d-1}\).

The inference phase of Alg. 2 is slightly different from Alg. 1. While \(\mathtt{EaS}\) representation using \(h_{2}\) is not \(k\)-sparse anymore, we show that for large enough sample size, with high probability, it is at least \(k/2\)-sparse and at most \(2k\)-sparse in expectation (see Lemma F.1 in Appendix F). Let \(t\) be an integer passed as an argument to Alg. 2. For any \(x\in\mathcal{X}\), let \(\Theta(x)=\{\theta_{j}:x\in C_{j}\}\) and we define \(A_{t}(x)\) to be the set containing the indices \(j\in[m]\), such that \(\theta_{j}\) is one of the \(t\) closest points to \(x\) from \(\Theta(x)\). To make an inference for any \(x\in\mathcal{X}\), Alg. 2 first computes \((\mathtt{eas}\cdot w)/t\) and makes it prediction based on whether this quantity is greater than \(1/2\). Clearly, \((\mathtt{eas}\cdot w)/t\) is the average of \(w[j]\) for \(j\in A_{t}(x)\) and therefore, the conditional probability estimate \(\hat{\eta}(x)\) of Alg. 2 can be represented as,

\[\hat{\eta}(x)=\frac{1}{t}\sum_{j}\hat{\eta}_{j}\mathbb{I}[j\in A_{t}(x)] \tag{16}\]

**Remark 4.1**.: Note that \(h_{1}\) and \(h_{2}\) are different in a specific way. When the support of data does not cover the whole unit sphere and is concentrated possibly in a small region and \(m\) is large, many of the \(m\) coordinates in \(\mathtt{EaS}\) representation will never be activated (set to \(1\)) for any data point as the corresponding projection direction \(\theta_{j}\) may not be one of the \(k\) closest ones to any data point. Thus, many of the \(\theta_{j}\) will be unused. This problem is the respective response region \(C_{j}\) may not be local to the manifold. For this purpose we need to identify "good" projection directions. Later in Lemma B.4 we show that, for any \(\delta>0\), with probability at least \(1-2\delta\) over the choice of \(\Theta\) and \(D^{\prime}_{n}\), the number of "good" projection directions \(t\) is linear in \(k\).

Due to space limitation, manifold assumptions and other important details of analysis of Alg. 2 are presented in Appendix B and we present the main theoretical results below.

**Theorem 4.2**.: _Let \(D_{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\cup D^{\prime}_{n}=\{(x^{\prime}_{i},y^{ \prime}_{i})\}_{i=1}^{n}\subset\mathcal{X}\times\{0,1\}\) be the training data where the data lies on a low dimensional manifold satisfying manifold assumption presented in section B.1 and suppose the \(\mathtt{EaS}\) representation is given as in (14). Pick any \(0<\delta<1\). Assume \(\eta\) to be \((L,\beta)\) smooth. If \(k\geq c^{\prime}_{d}\ln(m/\delta)\), where \(c^{\prime}_{d}\) is a constant that depend on \(d\), then with probability at least \(1-2\delta\),_

\[\Pr(g(x)\neq y)-L^{*}\leq 2\left(\sqrt{\frac{2m}{\alpha_{d}kn}}+4L\left(\frac {2k}{c_{1}m}\right)^{\frac{\beta}{d_{0}}}\right)\]

_where \(\alpha_{d}\) is a constant that depends on \(d\)._

**Corollary 4.3**.: _In Theorem 4.2, setting \(m=kn^{\frac{d_{0}}{2\beta+d_{0}}}\) ensure that with probability at least \(1-\delta\),_

\[\Pr(g(x)\neq y)-L^{*}=O\left(n^{-\frac{\beta}{2\beta+d_{0}}}\right).\]

_Remark 4.4_.: The convergence rate of Corollary 4.3 depends only on \(d_{0}\) and is minimax-optimal for plug-in classifiers under the assumption that \(\eta\) is \((L,\beta)\)-smooth (Audibert and Tsybakov, 2007).

## 5 Empirical evaluations

We investigate the effectiveness of our proposed method by evaluating it on eight benchmark datasets, details of which are provided in appendix A. We address the following questions:

1. Does performance our proposed classifier improve with increasing \(m\) as suggested by the theory?

2. How does our proposed classifier perform compared to other non-parametric classifiers?

For each dataset, we generate train and test set using scikit-learn'strain_test_split method (\(80:20\) split). We compare our proposed method against two non-parametric classifiers - scikit-learn's implementation of \(k\)-nearest neighbor classifier (\(k\)-NN) and random forest (RF). For \(k\)-NN, we used two values of \(k\): \(k=1\) and \(10\). For RF we use a grid search over the number of estimators (trees) from the set \(\{250,500,750,1000\}\) and perform a \(3\) fold cross validation to choose the final model. We preset our experimental results in Fig. 2 where we plot test accuracy of Alg. 1, \(k\)-NN (for \(k=1\) and \(10\)) and RF by varying expansion factor, where we define expansion factor to be \(m/d\). As per Theorem 3.12, we set \(k\) to be \(d\log m\). As can be seen from Fig. 2, with increasing \(m\) test accuracy of Alg. 1 increases in all eight datasets and becomes comparable to that of \(k\)-NN and RF for large \(m\), thus corroborating our theoretical findings.

## 6 Conclusions, limitations and future work

In this paper, we present an interesting connection between non-parametric estimation and expansion-and-sparsify representation. We presented two non-parametric classification algorithms using EaS representation and proved that both algorithms yield minimax-optimal convergence rates. The convergence rate of the first algorithm depends on the ambient dimension \(d\), while the convergence rate of the second algorithm, under manifold assumption, depends only on the intrinsic dimension \(d_{0}\ll d\). In both algorithms, the projection directions are chosen in a data-independent manner. One limitation of our current work is that, even though the second algorithm adapts to the manifold structure, there is a large constant, possibly depending exponentially on \(d\), involved in bounding the excess Bayes risk, that is hidden under the Big Oh notation. In the future, we plan to investigate various data-dependent projection direction choices for a sparse representation, that would adapt to a manifold structure, and the constant involved in bounding of the excess Bayes risk from above, would be independent of ambient dimension \(d\).

**Acknowledgements:** We thank the anonymous reviewers for their constructive feedback. This work is supported by funding from the "NSF AI Institute for Foundations of Machine Learning (IFML)" (FAIN:2019844).

Figure 2: Empirical evaluation of Alg. 1, \(k\)-NN (for \(k=1\) and \(10\)) and RF on eight datasets Here expansion factor is \(m/d\). An error bar in the form of a shaded graph is provided for Alg. 1 over 10 independent runs.

## References

* Andoni and Indyk (2008) Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. _Commun. ACM_, 51(1):117-122, jan 2008. URL [https://doi.org/10.1145/1327452.1327494](https://doi.org/10.1145/1327452.1327494).
* 633, 2007. doi: 10.1214/009053606000001217. URL [https://doi.org/10.1214/009053606000001217](https://doi.org/10.1214/009053606000001217).
* Biau et al. (2008) Gerard Biau, Luc Devroye, and Gabor Lugosi. Consistency of random forests and other averaging classifiers. _Journal of Machine Learning Research_, 9(66):2015-2033, 2008. URL [http://jmlr.org/papers/v9/biau08a.html](http://jmlr.org/papers/v9/biau08a.html).
* Candes et al. (2006) Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. _Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences_, 59(8):1207-1223, 2006.
* Caron et al. (2013) S. J. Caron, V. Ruta, L. F. Abbott, and R. Axel. Random convergence of olfactory inputs in the Drosophila mushroom body. _Nature_, 497(7447):113-117, May 2013.
* Chacron et al. (2011) Maurice J Chacron, Andre Longtin, and Leonard Maler. Efficient computation via sparse coding in electrosensory neural networks. _Current Opinion in Neurobiology_, 21(5):752-760, 2011.
* Chaudhuri and Dasgupta (2010) Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, _Advances in Neural Information Processing Systems_, volume 23. Curran Associates, Inc., 2010. URL [https://proceedings.neurips.cc/paper_files/paper/2010/file/b534ba68236ba543ae4ab22bd10alid6-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2010/file/b534ba68236ba543ae4ab22bd10alid6-Paper.pdf).
* Dasgupta and Tosh (2020) Sanjoy Dasgupta and Christopher Tosh. Expressivity of expand-and-sparsify representations. _arXiv preprint arXiv:2006.03741_, 2020. URL [https://arxiv.org/pdf/2006.03741.pdf](https://arxiv.org/pdf/2006.03741.pdf).
* Dasgupta et al. (2017) Sanjoy Dasgupta, Charles F Stevens, and Saket Navlakha. A neural algorithm for a fundamental computing problem. _Science_, 358(6364):793-796, 2017.
* Dasgupta et al. (2018) Sanjoy Dasgupta, Timothy C Sheehan, Charles F Stevens, and Saket Navlakha. A neural data structure for novelty detection. _Proceedings of the National Academy of Sciences_, 115(51):13093-13098, 2018.
* Datar et al. (2004) Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In _Proceedings of the Twentieth Annual Symposium on Computational Geometry_, SCG '04, page 253-262. Association for Computing Machinery, 2004. doi: 10.1145/997817.997857. URL [https://doi.org/10.1145/997817.997857](https://doi.org/10.1145/997817.997857).
* Devroye et al. (1996) Luc Devroye, Laszlo Gyorfi, and Gabor Lugosi. _A Probabilistic Theory of Pattern Recognition_, volume 31 of _Stochastic Modelling and Applied Probability_. Springer, 1996. ISBN 978-1-4612-0711-5.
* Donoho (2006) D.L. Donoho. Compressed sensing. _IEEE Transactions on Information Theory_, 52(4):1289-1306, 2006. doi: 10.1109/TIT.2006.871582.
* Gao and Zhou (2020) Wei Gao and Zhi-Hua Zhou. Towards convergence rate analysis of random forests for classification. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 9300-9311, 2020.
* Gionis et al. (1999) Aristides Gionis, Piotr Indyk, and Rajeev Motwani. Similarity search in high dimensions via hashing. VLDB '99, page 518-529. Morgan Kaufmann Publishers Inc., 1999. ISBN 1558606157.
* Gyorfi et al. (2002) Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, and Harro Walk. _A Distribution-Free Theory of Nonparametric Regression_. Springer series in statistics. Springer, 2002. ISBN 978-0-387-95441-7.
* Lin et al. (2014) A. C. Lin, A. M. Bygrave, A. de Calignon, T. Lee, and G. ck. Sparse, decorrelated odor coding in the mushroom body enhances learned odor discrimination. _Nat Neurosci_, 17(4):559-568, Apr 2014.
* Liu et al. (2015)N. Y. Masse, G. C. Turner, and G. S. Jefferis. Olfactory information processing in Drosophila. _Curr Biol_, 19(16):R700-713, Aug 2009.
* Niyogi et al. (2008) Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submanifolds with high confidence from random samples. _Discrete Comput. Geom._, 39(1):419-441, mar 2008. ISSN 0179-5376.
* Olshausen and Field (2004) B. A. Olshausen and D. J. Field. Sparse coding of sensory inputs. _Curr Opin Neurobiol_, 14(4):481-487, Aug 2004.
* Rahimi and Recht (2007) Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, _Advances in Neural Information Processing Systems_, volume 20. Curran Associates, Inc., 2007. URL [https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf).
* Ram and Sinha (2021) Parikshit Ram and Kaushik Sinha. Flynn: Fruit-fly inspired federated nearest neighbor classification. In _International Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with ICML 2021 (FL-ICML'21)_, 2021.
* Ram and Sinha (2022) Parikshit Ram and Kaushik Sinha. Federated nearest neighbor classification with a colony of fruitflies. In _Thirty-Sixth AAAI Conference on Artificial Intelligence_, pages 8036-8044. AAAI Press, 2022.
* Scornet (2016) Erwan Scornet. On the asymptotics of random forests. _J. Multivar. Anal._, 146(C):72-83, apr 2016. ISSN 0047-259X. URL [https://doi.org/10.1016/j.jmva.2015.06.009](https://doi.org/10.1016/j.jmva.2015.06.009).
* Scornet et al. (2015) Erwan Scornet, Gerard Biau, and Jean-Philippe Vert. Consistency of random forests. _The Annals of Statistics_, 43(4):1761-1741, 2015.
* Sinha and Ram (2021) Kaushik Sinha and Parikshit Ram. Fruit-fly inspired neighborhood encoding for classification. In _The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)_, 2021.
* Stettler and Axel (2009) D. D. Stettler and R. Axel. Representations of odor in the piriform cortex. _Neuron_, 63(6):854-864, Sep 2009.
* Stone (1977) Charles J. Stone. Consistent nonparametric regression. _The Annals of Statistics_, 5(4):595-620, 1977.
* Tang et al. (2018) Cheng Tang, Damien Garreau, and Ulrike von Luxburg. When do random forests fail? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 4, 2018.
* Tsybakov (2008) Alexandre B. Tsybakov. _Introduction to Nonparametric Estimation_. Springer Publishing Company, Incorporated, 1st edition, 2008. ISBN 0387790519.
* Turner et al. (2008) G. C. Turner, M. Bazhenov, and G. Laurent. Olfactory representations by Drosophila mushroom body neurons. _J Neurophysiol_, 99(2):734-746, Feb 2008.
* Vanschoren et al. (2013) Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: Networked science in machine learning. _SIGKDD Explorations_, 15(2):49-60, 2013. doi: 10.1145/2641190.2641198. URL [http://doi.acm.org/10.1145/2641190.2641198](http://doi.acm.org/10.1145/2641190.2641198).
* Wilson (2013) R. I. Wilson. Early olfactory processing in Drosophila: mechanisms and principles. _Annu Rev Neurosci_, 36:217-241, Jul 2013.

Dataset details and computing environment

Details of the eight datasets used in our experiments are listed in Table 1. Among the eight datasets, the twomoons is a synthetic dataset generated using make_moons method from sklearn.dataset2 with noise parameter \(0.2\). All the remaining datasets are taken from the OpenML repository3 Vanschoren et al. (2013). Both mnist and fmnist (fashion-mnist for short) are \(10\) class classification problem with \(784\) features. We convert them to binary classification problems by using the label \(3\) and \(5\) for the mnist dataset and label \(2\) (Pullover) and \(4\) (Coat)for fmnist dataset. For efficiency purpose, the feature dimensions for both these datasets are reduced to \(20\) using principal component analysis (PCA). For the remaining six datasets, the task is that of binary classification. For all eight datasets, the features are normalized using StandardScaler option in scikit-learn and are made to be unit norm.

Footnote 2: [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)

Footnote 3: [https://www.openml.org/](https://www.openml.org/)

We run our experiments on a laptop with Intel Xeon W-10855M Processor, 64GB memory and NVDIA Quadro RTX 5000 Mobile GPU (with 16GB memory).

## Appendix B Missing details of convergence rate of Algorithm 2 from section 4

In this section we present missing details of convergence rate analysis of Algorithm 2 from section 4. Proofs of various technical results presented in this section as well as the proof of Theorem 4.2 are deferred to section B.

### Manifold assumption

To analyze convergence rate of Algorithm 2, we proceed in the same was as in section 3.2. However, to take advantage of the fact that data lie on a low-dimensional manifold in our analysis, we make similar manifold assumptions as in Dasgupta and Tosh (2020). The input space \(\mathcal{X}\) is a compact \(d_{0}\)-dimensional Riemannian sub manifold \(M\) of \(\mathbb{R}^{d}\) contained in the unit sphere, that is \(M\subset\mathcal{S}^{d-1}\). \(M\) has nice boundaries and that the distribution on it \(\mu\), is almost uniform: formally, there exists constants \(c_{1},c_{2},c_{3}>0\) such that for all \(x\in M\) and for all \(r\leq r_{0}\) (where \(r_{0}\) is an absolute constant)

\[c_{1}r^{d_{0}}\leq\mu(B_{M}(x,r)<c_{2}r^{d_{0}} \tag{17}\]

\[\text{Vol}(B_{M}(x,r))\geq c_{3}r^{d_{0}} \tag{18}\]

Here \(B_{M}(x,r)=B(x,r)\cap M\).

To effectively analyze the data distribution supported on a manifold, we impose conditions on the curvature by adopting the common requirement that \(M\) has a positive reach \(\rho>0\): that is, every point in an open tubular neighborhood of \(M\) of of radius \(\rho\) has a unique nearest neighbor in \(M\)(Niyogi et al., 2008). For each \(x\in M\), let \(N(x)\) denotes the \((d-d_{0})\) dimensional subspace of normal vectors to the tangent plane at \(x\). For each \(x\in M\), the sets \(\Gamma_{\rho}(x)=\{x+ru\in\mathbb{R}^{d}:u\in N(x),\|u\|=1,0<r<\rho\}\) are disjoint and let \(\tau_{\rho}\) be their union. Let \(\pi_{M}:\tau_{\rho}\to M\) be the projection map that sends any point in \(\tau_{\rho}(x)\) to \(x\), its nearest neighbor in \(M\).

\begin{table}
\begin{tabular}{l|c|c} \hline Dataset Name & \# Samples & \# Features \\ \hline mnist35 & 13454 & 20 \\ fmnist24 & 11200 & 20 \\ segment (v.2) & 2310 & 19 \\ wine (v.7) & 2554 & 11 \\ wind (v.2) & 6574 & 14 \\ puma8NH (v.2) & 8192 & 8 \\ cpu\_small (v.3) & 8192 & 12 \\ twomoons & 5000 & 2 \\ \hline \end{tabular}
\end{table}
Table 1: Dataset statistics 

### Bounding \(\mathbb{E}_{x,D_{n}}|\hat{\eta}(x)-\bar{\eta}(x)|\)

The response regions \(C_{j},j\in[m]\) are now random quantities that depend on the choice of \(\Theta\) as well as \(D^{\prime}_{n}\) used to compute individual thresholds. The quantity \(\hat{\eta}(x)\) given in (16) depends on \(t\). In this section, we fix \(\Theta\) and \(t\) and conditioned on this, we bound \(\mathbb{E}_{x,D_{n},D^{\prime}_{n}}|\hat{\eta}(x)-\bar{\eta}(x)|\). Later in Lemma B.4 we show that, for any \(\delta>0\), with probability at least \(1-2\delta\) over the choice of \(\Theta\) and \(D^{\prime}_{n}\), \(t\) is linear in \(k\).

**Lemma B.1**.: _Fix any \(\Theta\) and \(t\). Then the following holds._

\[\mathbb{E}_{x,D_{n},D^{\prime}_{n}}|\hat{\eta}(x)-\bar{\eta}(x)|\leq\sqrt{ \frac{m}{tn}}.\]

Sketch of proof:.: The proof strategy is similar to Lemma 3.8, but requires careful analysis to accommodate the facts that \(C_{j},j\in[m]\) depend on the choice of \(D^{\prime}_{n}\) and \(\hat{\eta}\) in (16) has an indicator function. Details are provided in Appendix B.

### Bounding \(\mathbb{E}_{x,D_{n}}|\bar{\eta}(x)-\eta(x)|\)

Using identical proof strategy of Lemma 3.10, we have,

**Lemma B.2**.: _Under empirical \(k\)-thresholding scheme, if \(\eta\) is \((L,\beta)\) smooth, then for all \(x\in C_{1}\cup\cdots,\cup C_{m},\)_

\[|\eta(x)-\bar{\eta}(x)|\leq L\max_{j\in[m]}(\operatorname{diam}(C_{j}))^{\beta}\]

Unfortunately, unlike before, all \(C_{j}\) such that \(x\in C_{j}\) may not be local to \(x\) and thus average \(y\) values in some of these \(C_{j}\), may be very different from \(\eta(x)\). To address this, we call a \(\theta\in\mathbb{R}^{d}\)_good_ if it lies in \(\Gamma_{\rho/2}\). Good \(\theta_{j}\)'s are close to the manifold \(M\) and are guaranteed to be activated by a single neighborhood of \(M\). Thus, for large enough \(m\), if the diameters of the \(C_{j}\)'s corresponding to good \(\theta_{j}\)'s are made reasonable small then the average \(y\) values in such \(C_{j}\)'s will be a reasonably close of \(\eta(x)\) for \(x\in C_{j}\). First, for any good \(\theta_{j}\), we bound the diameter of \(C_{j}\) emphasizing that the response region of a good \(\theta_{j}\) is local.

**Lemma B.3**.: _Pick any good \(\theta\in\mathbb{R}^{d}\). Define \(\Delta=\|\theta-\pi_{M}(\theta)\|\) to be the distance from \(\theta\) to its projection on \(M\). Let \(C(\theta)\) be the response region associated with \(\theta\) as defined in (15). Pick any \(0<\delta<1\). Then with probability at least \(1-\delta\), over the choice of \(D^{\prime}_{n}\), the following holds:_

\[B_{M}\left(\pi_{M}(\theta),\sqrt{\frac{\rho-\Delta}{\rho+\Delta}}\left(\frac{ k}{2c_{2}m}\right)^{1/d_{0}}\right)\subset C(\theta)\subset B\left(\pi_{M}( \theta),\sqrt{\frac{\rho+\Delta}{\rho-\Delta}}\left(\frac{2k}{c_{1}m}\right)^ {1/d_{0}}\right)\]

_In particular this implies, \(\operatorname{diam}(C(\theta))\leq 4\left(\frac{2k}{c_{1}m}\right)^{1/d_{0}}\), provided \((2k/(c_{1}m))^{1/d_{0}}<\min(\rho,r_{0})\) and \(n\) satisfies \(n\geq\frac{c_{0}m}{k}\left(\log n+\log\left(\frac{m}{\delta}\right)\right)\), where \(c_{0}>0\) is a universal constant._

Next we show that each \(x\in M\) has number of good \(\theta_{j}\) linear in \(k\) with high probability.

**Lemma B.4**.: _Pick \(\theta_{1},\ldots,\theta_{m}\sim\nu\). There is a constant \(c^{\prime}_{d}\), depending on \(d\), for which the following holds. Pick any \(0<\delta<1\). Set \(k\geq c^{\prime}_{d}\ln(m/\delta)\), then with probability at least \(1-2\delta\) over the choice of \(\theta_{j}\)'s and \(D^{\prime}_{n}\), for every \(x\in M\), there are \(\alpha_{d}k/2\) good \(\theta_{j}\)'s with \(x\in C(\theta_{j})\) where \(c^{\prime}_{d}\) and \(\alpha_{d}\) are constants that depend on dimension \(d\)._

Combining4 Lemma B.3 and B.4, we have

Footnote 4: We note\(k\geq c^{\prime}_{d}\ln(m/delta)\), \((2k/(c_{1}m))^{1/d_{0}}<\min(\rho,r_{0})\) and \(n\geq(c_{0}m/k)(\log n+\log(m/\delta))\). Then with probability at least \(1-2\delta\) over the choice of \(\Theta\) and \(D^{\prime}_{n}\),_

\[\sup_{x\in\mathcal{X}}|\bar{\eta}(x)-\eta(x)|\leq 4L\left(\frac{2k}{c_{1}m} \right)^{\frac{\beta}{d_{0}}}\]

Combining everything leads to the main result presented in Theorem 4.2.

## Appendix C Various proofs from section 3

### Proof of Lemma 3.1

Proof.: Simply interchange the summation. That is,

\[\sum_{i=1}^{n}w_{i,n}(x)=\sum_{i=1}^{n}\left(\frac{1}{k}\sum_{j:x \in C_{j}}\frac{\mathbb{I}[x_{i}\in\mathcal{C}_{j}]}{\sum_{i=1}^{n}\mathbb{I} [x_{i}\in\mathcal{C}_{j}]}\right)=\frac{1}{k}\sum_{j:x\in C_{j}}\left(\frac{ \sum_{i=1}^{n}\mathbb{I}[x_{i}\in\mathcal{C}_{j}]}{\sum_{i=1}^{n}\mathbb{I}[x _{i}\in\mathcal{C}_{j}]}\right)=1\]

## Appendix D Various proofs from section 3.1

In this section we present various technical results that are needed to prove our main theorem (Theorem 3.3). Due to space limitation proofs of some of these technical results are deferred to the supplementary material.

### Proof of Lemma 3.4

Proof.: For part (i), note that if \(x\in C_{i}^{k}\), then by definition, \(\forall j\in\sigma(i),h_{\Theta,k}(x)[j]=1\) which implies \(x\in C_{j}\). Therefore, \(C_{i}^{k}\subseteq\cap_{j\in\sigma(i)}C_{j}\). On the other hand, if \(x\in\cap_{j\in\sigma(i)}C_{j}\) then \(h_{\Theta,k}(x)[j]=1\;\forall j\in\sigma(i)\), which implies \(x\in C_{i}^{k}\). Therefore, \(\cap_{j\in\sigma(i)}C_{j}\subseteq C_{i}^{k}\).

For part (ii), take any \(i,j\in[\binom{m}{k}],i\neq j\), then \(\sigma(i)\neq\sigma(j)\). Therefore, if \(x\in C_{i}^{k}\) then \(x\notin C_{j}^{k}\). Since \(x\) was arbitrary, \(C_{i}^{k}\cap C_{j}^{k}=\emptyset\). Now, for any \(x\in\mathcal{X}\), since \(h_{\Theta,k}(x)\) has exactly \(k\) bits activated (set to 1), there must exist \(l\in[\binom{m}{k}]\) such that \(x\in C_{l}^{k}\). Therefore, \(\cup_{i=1}^{\binom{m}{k}}C_{i}^{k}=\mathcal{X}\).

For part (iii), take any \(x\in C_{j}\). From the definition of \(C_{j}\), \(h_{\Theta,k}(x)[j]=1\) and therefore, \(x\in C_{i}^{k}\) for any \(i\) such that \(j\in\sigma(i)\). From part(ii) since such \(C_{i}^{k}\)s are disjoint and \(x\) was arbitrary, the result follows.

Finally, part (iv) follows immediately from part (ii) and (iii) since \(C_{i}^{k}\)s are disjoint. 

### proof of Lemma 3.5

Proof.: While general idea of this result appeared in Dasgupta and Tosh (2020), we provide a simplified proof. Let \(\nu(r)=\inf_{x\in\mathcal{X}}\nu(B(x,r))\). We first claim that if \(\nu(r)\geq\frac{2}{m}\left(k+c_{0}\left(d\log m+\log\left(\frac{1}{\delta} \right)\right)\right)\) then for every \(j\in[m]\), \(C_{j}\subset B(\theta_{j},r)\). By definition, every ball of radius \(r\), centered at some \(x\in\mathcal{X}\), has \(\nu\)-mass at least \(\nu(r)\). Thus, by Lemma D.1, with probability at least \(1-\delta\), every \(x\in\mathcal{X}\) will have its nearest \(k\)\(\theta_{j}\)'s within a distance \(r\). Therefore, the \(j^{th}\) bit of the EaS representation given in (1) will be activated by points \(x\) within distance \(r\) of \(\theta_{j}\). Therefore, \(\operatorname{diam}(C_{j})\leq 2r\) for all \(j=1,\ldots,m\), where \(r\) satisfies

\[\nu(B(x,r))\geq\frac{2\left(k+c_{0}\left(d\log m+\log\left(\frac{1}{\delta} \right)\right)\right)}{m}\]By Lemma D.2, we can take \(r\) to be the value

\[r =\left(\frac{3\sqrt{d}}{(3/4)^{(d-1)/2}}\cdot\frac{2\left(k+c_{0} \left(d\log m+\log\left(\frac{1}{\delta}\right)\right)\right)}{m}\right)^{1/(d-1)}\] \[=\frac{2}{\sqrt{3}}\left(\frac{6\sqrt{d}\left(k+c_{0}\left(d\log m +\log\left(\frac{1}{\delta}\right)\right)\right)}{m}\right)^{1/(d-1)}.\]

Therefore, we have

\[\mathrm{diam}(C_{j}) \leq 2r\leq\frac{4}{\sqrt{3}}\left(\frac{6\sqrt{d}\left(k+c_{0}\left(d \log m+\log\left(\frac{1}{\delta}\right)\right)\right)}{m}\right)^{1/(d-1)}\] \[\leq 8\left(\frac{\left(k+c_{0}\left(d\log m+\log\left(\frac{1}{\delta }\right)\right)\right)}{m}\right)^{1/(d-1)}\]

where, for the last inequality, we have used the fact that \(d^{1/(2(d-1))}\leq\sqrt{2}\) and for \(d\geq 3\) it holds that \(6^{1/(d-1)}\leq\sqrt{6}\). 

**Lemma D.1** ([16]).: _There is an absolute constant \(c_{0}>0\) for which the following holds. Pick any \(0<\delta<1\). Pick \(\theta_{1},\ldots,\theta_{m}\) independently at random from a distribution \(\mu\) on \(\mathbb{R}^{d}\). Then with probability at least \(1-\delta\), any ball \(B\) in \(\mathbb{R}^{d}\) with_

\[\nu(B)\geq\frac{2}{m}\left(k+c_{0}\left(d\log m+\log\left(\frac{1}{\delta} \right)\right)\right)\]

_contains at least \(k\) of the \(\theta_{i}\)._

**Lemma D.2** (Lemma 12 [17]).: _Suppose \(d\geq 3\), \(r\in(0,1)\), and \(\nu\) is the uniform distribution over \(\mathcal{S}^{d-1}\). Then for any \(x\in\mathcal{S}^{d-1}\),_

\[\nu(B(x,r))\geq\frac{1}{3\sqrt{d}}r^{d-1}\left(1-r^{2}/4\right)^{(d-1)/2}\geq \frac{1}{3\sqrt{d}}\left(3/4\right)^{(d-1)/2}r^{d-1}.\]

### Proof of Theorem 3.3

Proof.: Since the weights \(w_{n,i}\) given in equation 4 are non-negative, in order to satisfy condition (i) of Stone's theorem, we need to show that there exists a positive constant \(c\) such that for any non-negative measurable function \(f\) satisfying \(\mathbb{E}f(x)<\infty\) and for any \(n\), \(\mathbb{E}\left(\sum_{i=1}^{n}w_{n,i}(x)f(x_{i})\right)\leq c\mathbb{E}(f(x))\). Using Lemma D.3, we show that this condition is satisfied for \(c=1\).

Concerning condition (ii) of Stone's theorem, first define \(a_{n}=a_{n}(k_{n},m_{n},\delta_{n})=8\left(\frac{k_{n}+c_{0}\left(d\log m_{n}+ \log\left(\frac{1}{n}\right)\right)}{m_{n}}\right)^{1/(d-1)}\), where \(c_{0}\) is an absolute constant. Then \(\lim_{n\to\infty}\delta_{n}=0\) and using condition (i) and (ii) of this theorem we also have \(\lim_{n\to\infty}a_{n}=0\). Now pick any \(a>0\). We can always find a positive integer \(N\) such that for \(n>N,\) we have \(a>a_{n}\) satisfying \(\sum_{i=1}^{n}w_{i,n}(x)\mathbb{I}_{\{\|x_{i}-x\|>a\}}\leq\sum_{i=1}^{n}w_{i,n }(x)\mathbb{I}_{\{\|x_{i}-x\|>a_{n}\}}\) for all \(x\in\mathcal{X}\). Replacing \(k,m,\delta\) and \(a\) with \(k_{n},\delta_{n}\) and \(a_{n}\) respectively in Lemma D.6, we have

\[\mathbb{E}\left(\sum_{i=1}^{n}w_{i,n}(x)\mathbb{I}[\|x_{i}-x\|>a]\right)\leq \mathbb{E}\left(\sum_{i=1}^{n}w_{i,n}(x)\mathbb{I}[\|x_{i}-x\|>a_{n}]\right) \leq\delta_{n}\]

Taking limit yields, \(\lim_{n\to\infty}\mathbb{E}\left(\sum_{i=1}^{n}w_{i,n}(x)\mathbb{I}[\|x_{i}-x\|> a]\right)=0\).

Concerning condition (iii) of Stone's Theorem, note that

\[\mathbb{E}\left(\max_{1\leq i\leq n}w_{n,i}(x)\right) \leq \mathbb{E}\left(\frac{1}{k}\sum_{j:x\in C_{j}}\frac{1}{\sum_{l=1 }^{n}\mathbb{I}[x_{l}\in C_{j}]}\right)=\mathbb{E}\left(\frac{1}{k}\sum_{i=1 }^{k}\frac{1}{\sum_{l=1}^{n}\mathbb{I}[x_{i}\in C_{j_{i}(x,\Theta)}]}\right) \tag{19}\] \[= \mathbb{E}\left(\frac{1}{k}\sum_{i=1}^{k}\frac{1}{N_{j_{i}(x, \Theta)}}\right)\leq\mathbb{E}\left(\frac{1}{\min_{1\leq i\leq k}\{N_{j_{i}(x,\Theta)}\}}\right)\]where, for \(i=1,\ldots,k\), we have used the notation \(j_{i}(x,\Theta)\) to denote the \(k\) random coordinates of \(h_{\Theta,k}(x)\) that are set to 1 and \(N_{j_{i}(x,\Theta)}\) to denote the number of points from \(x_{1},\ldots,x_{n}\) that fall in \(C_{j_{i}(x,\Theta)}\). Now from equation 19, is it easy to see that \(\lim_{n\to\infty}\mathbb{E}\left(\max_{1\leq i\leq n}w_{n,i}(x)\right)=0\) since using lemma D.7, \(\min_{1\leq i\leq k}\{N_{j_{i}(x,\Theta)}\}\to\infty\) in probability, as \(n\to\infty\). 

### Technical results for satisfying condition (i) of Stone's theorem (Theorem 3.2)

**Lemma D.3**.: _For any non-negative measurable function \(f:\mathcal{X}\to\mathbb{R}_{+}\) satisfying \(\mathbb{E}f(x)<\infty\) and for any \(n\), \(\mathbb{E}\left(\sum_{i=1}^{n}w_{n,i}(x)f(x_{1})\right)\leq\mathbb{E}(f(x))\), where the weights \(w_{n,i}\) are as given in (4)._

Proof.: \[\mathbb{E}\left(\sum_{i=1}^{n}w_{i,n}(x)f(x_{i})\right) = \mathbb{E}\left(\sum_{i=1}^{n}\left(\frac{1}{k}\sum_{j:x\in C_{j }}\frac{\mathbb{I}[x_{i}\in C_{j}]f(x_{i})}{\sum_{l=1}^{n}\mathbb{I}[x_{l}\in C _{j}]}\right)\right)\] \[= \mathbb{E}\left(\frac{1}{k}\sum_{j:x\in C_{j}}\left(\sum_{i=1}^{ n}\frac{\mathbb{I}[x_{i}\in C_{j}]f(x_{i})}{\sum_{l=1}^{n}\mathbb{I}[x_{l}\in C _{j}]}\right)\right)\] \[= \mathbb{E}_{x}\left(\frac{1}{k}\mathbb{E}\left[\sum_{j:x\in C_{j }}\left(\sum_{i=1}^{n}\frac{\mathbb{I}[x_{i}\in C_{j}]f(x_{i})}{\sum_{l=1}^{n} \mathbb{I}[x_{l}\in C_{j}]}\right)\bigg{|}x\right]\right)\] \[= \mathbb{E}_{x}\left(\frac{1}{k}\left[\sum_{j:x\in C_{j}}\mathbb{E }\left(\sum_{i=1}^{n}\frac{\mathbb{I}[x_{i}\in C_{j}]f(x_{i})}{\sum_{l=1}^{n} \mathbb{I}[x_{l}\in C_{j}]}\bigg{|}x\right)\right]\right)\] \[\stackrel{{ a}}{{\leq}} \mathbb{E}_{x}\left(\frac{1}{k}\left[\sum_{j:x\in C_{j}}\frac{1}{ \mu(C_{j})}\int_{C_{j}}f(x_{1})\mu(dx_{1})\right]\right)\] \[= \sum_{i=1}^{\binom{n}{k}}\left(\Pr(x\in C_{i}^{k})\left[\frac{1}{ k}\sum_{j\in\sigma(i)}\frac{1}{\mu(C_{j})}\int_{C_{j}}f(x_{1})\mu(dx_{1})\right]\right)\] \[= \sum_{i=1}^{\binom{n}{k}}\frac{1}{k}\left[\sum_{j\in\sigma(i)} \frac{\Pr(x\in C_{i}^{k})}{\mu(C_{j})}\int_{C_{j}}f(x_{1})\mu(dx_{1})\right]\] \[= \frac{1}{k}\sum_{i=1}^{\binom{n}{k}}\sum_{j\in\sigma(i)}\frac{\mu (C_{i}^{k})}{\mu(C_{j})}\int_{C_{j}}f(x_{1})\mu(dx_{1})\] \[\stackrel{{ b}}{{=}} \frac{1}{k}\sum_{j=1}^{m}\sum_{i:j\in\sigma(i)}\frac{\mu(C_{i}^{k })}{\mu(C_{j})}\int_{C_{j}}f(x_{1})\mu(dx_{1})\stackrel{{ c}}{{=}} \frac{1}{k}\sum_{j=1}^{m}\int_{C_{j}}f(x_{1})\mu(dx_{1})\] \[\stackrel{{ d}}{{=}} \frac{1}{k}\sum_{j=1}^{m}\sum_{i:j\in\sigma(i)}\int_{C_{i}^{k}}f( x_{1})\mu(dx_{1})\stackrel{{ e}}{{=}}\frac{1}{k}\sum_{i}^{\binom{m}{k}}\sum_{j\in\sigma(i)} \left(\int_{C_{i}^{k}}f(x_{1})\mu(dx_{1})\right)\] \[= \frac{1}{k}\sum_{i}^{\binom{m}{k}}k\int_{C_{i}^{k}}f(x_{1})\mu(dx_ {1})=\sum_{i}^{\binom{m}{k}}\int_{C_{i}^{k}}f(x_{1})\mu(dx_{1})\] \[\stackrel{{ f}}{{=}} \int_{\mathbb{S}}f(x_{1})\mu(dx_{1})=\mathbb{E}(f(x_{1}))\stackrel{{ g}}{{=}}\mathbb{E}(f(x))\]

where, inequality \(a\) follows from Lemma D.4. Equality \(b\) follows from the following observation. In the line above inequality \(b\), we are summing \(k\times\binom{m}{k}\) terms. Since \(k\times\binom{m}{k}=m\times\binom{m-1}{k-1}\) and any \(j\in[m]\) can appear in exactly \(\binom{m-1}{k-1}\) different subsets of of size \(k\), equality \(b\) is simply rearranging the terms from the line above by changing the indices appropriately. Equality \(c\) follows from part (iv)of Lemma 3.4. Equality \(d\) follows from part (iii) of Lemma 3.4. Equality \(e\) follows from the following observation. In the line above equality \(e\), we are summing \(m\times\binom{m-1}{k-1}\) terms since any \(j\in[m]\) can appear in exactly \(\binom{m-1}{k-1}\) different subsets of of size \(k\). Again noticing that \(m\times\binom{m-1}{k-1}=k\times\binom{m}{k}\) and each \(\sigma(i)\) has \(k\) terms in it, we are simply rearranging the terms from the line above by changing the indices appropriately. Finally, equality \(f\) follows from part (ii) of Lemma E.4 and equality \(g\) follows from the fact that \(x\) and \(x_{1}\) are i.i.d. 

**Lemma D.4**.: \(\mathbb{E}\left(\sum_{i=1}^{n}\frac{\mathbb{I}[x_{i}\in C_{j}]f(x_{i})}{\sum_ {l=1}^{n}\mathbb{I}[x_{l}\in C_{j}]}\Big{|}x\right)\leq\frac{1}{\mu(C_{j})} \int_{C_{j}}f(x_{1})\mu(dx_{1})\)_._

Proof.: \[\mathbb{E}\left(\sum_{i=1}^{n}\frac{\mathbb{I}[x_{i}\in C_{j}]f(x _{i})}{\sum_{l=1}^{n}\mathbb{I}[x_{l}\in C_{j}]}\Big{|}x\right) = \sum_{i=1}^{n}\mathbb{E}\left(\frac{\mathbb{I}[x_{i}\in C_{j}]f( x_{i})}{\sum_{l=1}^{n}\mathbb{I}[x_{l}\in C_{j}]}\Big{|}x\right)\] \[\stackrel{{ a}}{{=}} \sum_{i=1}^{n}\mathbb{E}\left(\frac{\mathbb{I}[x_{i}\in C_{j}]f( x_{i})}{1+\sum_{l\neq i}\mathbb{I}[x_{l}\in C_{j}]}\Big{|}x\right)\] \[= n\mathbb{E}\left(I_{\{x_{1}\in C_{j}\}}f(x_{1})\frac{1}{1+\sum_{ l=2}^{n}\mathbb{I}[x_{l}\in C_{j}]}\Big{|}x\right)\] \[= n\mathbb{E}_{x_{1}}\left(I_{\{x_{1}\in C_{j}\}}f(x_{1})\mathbb{ E}_{x_{2},\ldots,x_{n}}\left[\frac{1}{1+\sum_{l=2}^{n}\mathbb{I}[x_{l}\in C_{j}] }\Big{|}x,x_{1}\right]\right)\] \[\stackrel{{ b}}{{\leq}} n\mathbb{E}_{x_{1}}\left(\mathbb{I}_{\{x_{1}\in C_{j}\}}f(x_{1}) \frac{1}{n\mu(C_{j})}\right)\] \[= \frac{1}{\mu(C_{j})}\int_{C_{j}}f(x_{1})\mu(dx_{1})\]

where, equality \(a\) follows from that fact, that if \(x_{i}\in C_{j}\), then the term in the parenthesis does not change and if \(x_{i}\notin C_{j}\), then the term within the parenthesis is zero, and inequality \(b\) follows from lemma D.5 since conditioned on \(x\) and \(x_{1}\), the random variable \(\sum_{l=2}^{n}\mathbb{I}_{\{x_{l}\in C_{j}\}}\) is Binomially distributed with parameters \((n-1)\) and \(\mu(C_{j})\). 

**Lemma D.5**.: _(Binomial bound Gyorfi et al. [2002]) Let the random variable \(B(n,p)\) be Binomially distributed with parameters \(n\) and \(p\). Then,_

\[\mathbb{E}\left(\frac{1}{1+B(n,p)}\right)\leq\frac{1}{(n+1)p}\]

### Technical results for satisfying condition (ii) of Stone's theorem (Theorem 3.2)

**Lemma D.6**.: _Pick any \(0<\delta<1\) and let \(a=8\left(\frac{k+c_{0}\big{(}d\log m+\log\big{(}\frac{1}{2}\big{)}}{m}\right) ^{1/(d-1)}\), where \(c_{0}>0\) is an absolute constant defined in Lemma 3.5. Then,_

\[\mathbb{E}\left(\sum_{i=1}^{n}w_{i,n}(x)\mathbb{I}[\|x_{i}-x\|>a]\right)\leq\delta.\]

Proof.: From Lemma 3.5, we have that with probability at least \(1-\delta\), \(\operatorname{diam}(\mathcal{C}_{j})\leq a\) for all \(1\leq j\leq m\). Let us define the random variable:

\[\mathcal{A}_{1}=\left\{\sum_{i=1}^{n}\Biggl{(}\frac{1}{k}\sum_{j:x\in C_{j}} \frac{\mathbb{I}[x_{i}\in C_{j}]}{\sum_{i=1}^{n}\mathbb{I}[x_{i}\in C_{j}]} \Biggr{)}\mathbb{I}[\|x_{i}-x\|>a]\right\}\]

and the event:

\[\mathcal{A}_{2}=\left\{\cup_{j:x\in C_{j}}C_{j}\subset B(x,a)\right\}\]Let \(\mathcal{A}_{2}^{c}\) denotes complement of the event \(\mathcal{A}_{2}\). Then,

\[\mathbb{E}\left(\sum_{i=1}^{n}w_{i,n}(x)\mathbb{I}[\|x_{i}-x\|>a]\right) = \mathbb{E}\left[\sum_{i=1}^{n}\Biggl{(}\frac{1}{k}\sum_{j:x\in C_{ j}}\frac{\mathbb{I}[x_{i}\in C_{j}]}{\sum_{i=1}^{n}\mathbb{I}[x_{i}\in C_{j}]} \Biggr{)}\mathbb{I}_{\{\|x_{i}-x\|>a\}}\right]\] \[= \mathbb{E}\left(\mathcal{A}_{1}|\mathcal{A}_{2}\right)\Pr\left( \mathcal{A}_{2}\right)+\mathbb{E}\left(\mathcal{A}_{1}|\mathcal{A}_{2}^{c} \right)\Pr\left(\mathcal{A}_{2}^{c}\right)\] \[\stackrel{{ a}}{{\leq}} 0\cdot\Pr(\mathcal{A}_{2})+1\cdot\delta=\delta\]

where the inequality follows from the fact that maximum value of \(\mathcal{A}_{1}\) is 1 and conditioned on the event \(\mathcal{A}_{2}\), value of \(\mathcal{A}_{1}\) is 0. 

### Technical results for satisfying condition (iii) of Stone's theorem (Theorem 3.2)

**Lemma D.7**.: _For any \(x\sim\mu\), let \(j_{1}(x,\Theta),\ldots,j_{k}(x,\Theta)\in[m]\) be the \(k\) random coordinates of \(h_{\Theta,k}(x)\) that are set to 1. Let \(N_{j_{1}(x,\Theta)},\ldots,N_{j_{k}(x,\Theta)}\) be the number of data points falling in \(C_{j_{1}(x,\Theta)},\ldots,C_{j_{k}(x,\Theta)}\) respectively. Then \(\min\{N_{j_{1}(x,\Theta)},\ldots,N_{j_{k}(x,\Theta)}\}\rightarrow\infty\) in probability, whenever \(n\rightarrow\infty\) and \(m^{k}/n\to 0\) as \(n\rightarrow\infty\)._

Proof.: For any random \(x\in\mathcal{X}\), let \(C_{i(x,\Theta)}^{k}\) be the random cell of the partition \(\{C_{i}^{k}\}_{i=1}^{\binom{n}{k}}\) which sets \(k\) random coordinates \(j_{1}(x,\Theta),\ldots,j_{k}(x,\Theta)\) of \(h_{\Theta,k}(x)\) to one. Then, \(C_{i(x,\Theta)}^{k}=\cap_{i=1}^{k}C_{j_{i}(x,\Theta)}\). Let \(N_{n}(x,\Theta)=\sum_{i=1}^{n}\mathbb{I}_{\{x_{i}\in C_{i(x,\Theta)}^{k}\}}\) be the number of data points falling in the same cell as \(x\). We first we show that \(N_{n}(x,\Theta)\rightarrow\infty\) in probability. Let \(N_{1},N_{2},\ldots N_{\binom{m}{k}}\) be the number of points of \(x,x_{1},\ldots,x_{n}\) falling in the \(\binom{m}{k}\) respective cells \(C_{1}^{k},\ldots,C_{\binom{m}{k}}^{k}\). Let \(S=\{x,x_{1},\ldots,x_{n}\}\) denote the set of positions of these \(n+1\) points. Since these points are independent and identically distributed, fixing the set \(S\) (but not the order of the points) and \(\Theta\), the conditional probability that \(x\) falls in the \(i^{th}\) cell \(C_{i}^{k}\) is \(N_{i}/(n+1)\). Then for any fixed integer \(t>0\),

\[\Pr(N_{n}(x,\Theta)<t) = \mathbb{E}\left[\Pr\left(N_{n}(x,\Theta)<t|S,\Theta)\right)\right]\] \[= \mathbb{E}\left[\sum_{i:N_{i}<t}\frac{N_{i}}{n+1}\right]\leq(t-1 )\frac{\binom{m}{k}}{n+1}\leq(t-1)\frac{m^{k}}{n}\]

which converges to zero on our assumption on \(n\). Since \(\mathcal{C}_{i(x,\Theta)}^{k}\subseteq\mathcal{C}_{j_{l}(x,\Theta)}\) for \(l=1,\ldots,k\), we have \(N_{n}(x,\Theta)\leq\min\{N_{j_{1}(x,\Theta)},\ldots,N_{j_{k}(x,\Theta)}\}\) and the statement of the Lemma follows.

Various proofs from section 3.2

For ease of exposition, we use the notation \(x_{[1,n]}\) to denote \(x_{1},\ldots,x_{n}\) and \(y_{[1,n]}\) to denote \(y_{1},\ldots,y_{n}\) for various proofs appearing in the section.

### Proof of lemma 3.8

Proof.: Fix any \(\Theta\). This ensures that \(C_{1},\ldots,C_{m}\) are fixed. Now,

\[\mathbb{E}(\hat{\eta}(x)-\bar{\eta}(x))^{2}\] \[=\mathbb{E}_{x_{[1,n]},\mathbb{J}_{[1,n]}}\left[(\hat{\eta}(x)- \bar{\eta}(x))^{2}\right]\] \[=\mathbb{E}_{x_{[1,n]}}\left[\mathbb{E}_{x,y_{[1,n]}}\left[(\hat{ \eta}(x)-\bar{\eta}(x))^{2}|x_{[1,n]})\right]\right]\] \[=\mathbb{E}_{x_{[1,n]}}\left[\mathbb{E}_{x,y_{[1,n]}}\left[\left( \frac{1}{k}\sum_{j:x\in C_{j}}(\hat{\eta}_{j}-\bar{\eta}_{j})\right)^{2}\left| x_{[1,n]}\right]\right]\] \[\stackrel{{ a}}{{\leq}}\mathbb{E}_{x_{[1,n]}}\left[ \mathbb{E}_{x,y_{[1,n]}}\left[\frac{1}{k}\sum_{j:x\in C_{j}}(\hat{\eta}_{j}- \bar{\eta}_{j})^{2}\left|x_{[1,n]}\right]\right]\right.\] \[=\mathbb{E}_{x_{[1,n]}}\left[\mathbb{E}_{x,y_{[1,n]}}\left[\frac{1 }{k}\sum_{j:x\in C_{j}}\left(\frac{\sum_{i=1}^{n}Y_{i}\mathbb{I}[x_{i}\in C_{j} ]}{\sum_{i=1}^{n}\mathbb{I}[x_{i}\in C_{j}]}-\eta(C_{j})\right)^{2}\left|x_{[1, n]}\right]\right]\] \[=\mathbb{E}_{x_{[1,n]}}\left[\mathbb{E}_{x,y_{[1,n]}}\left[\frac{1 }{k}\sum_{j:x\in C_{j}}\left(\frac{\sum_{i=1}^{n}y_{i}\mathbb{I}[x_{i}\in C_{j} ]}{n\mu_{n}(C_{j})}-\eta(C_{j})\right)^{2}\left|x_{[1,n]}\right]\right]\] \[=\mathbb{E}_{x_{[1,n]}}\left[\mathbb{E}_{x,y_{[1,n]}}\left[\frac{1 }{k}\sum_{j:x\in C_{j}}\left(\frac{\sum_{i=1}^{n}(y_{i}-\eta(C_{j}))\mathbb{I} [x_{i}\in C_{j}]}{n\mu_{n}(C_{j})}\right)^{2}\mathbb{I}[n\mu_{n}(C_{j})>0] \Big{|}x_{[1,n]}\right]\right]\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\mathbb{E} _{x_{[1,n]}}\left[\mathbb{E}_{x,y_{[1,n]}}\left[\frac{1}{k}\sum_{j:x\in C_{j}} \eta^{2}(C_{j})\mathbb{I}[\mu_{n}(C_{j})=0]\Big{|}x_{[1,n]}\right]\right]\] \[\stackrel{{ b}}{{\leq}}\mathbb{E}_{x_{[1,n]}}\left[ \mathbb{E}_{x,y_{[1,n]}}\left[\frac{1}{k}\sum_{h_{\mathbf{e},h}(x)[j]=1}\left( \frac{\sum_{i=1}^{n}(y_{i}-\eta(C_{j}))\mathbb{I}[x_{i}\in C_{j}]}{n\mu_{n}(C_ {j})}\right)^{2}\mathbb{I}[n\mu_{n}(C_{j})>0]\Big{|}x_{[1,n]}\right]\right]+ \frac{m}{kne}\] \[=\mathbb{E}_{x_{[1,n]}}\left[\mathbb{E}_{x}\left[\frac{1}{k} \mathbb{E}_{y_{[1,n]}}\left[\sum_{j:x\in C_{j}}\left(\frac{\sum_{i=1}^{n}(y_{i} -\eta(C_{j}))\mathbb{I}[x_{i}\in C_{j}]}{n\mu_{n}(C_{j})}\right)^{2}\mathbb{I }[n\mu_{n}(C_{j})>0]\Big{|}x,x_{[1,n]}\right]\right]\right]+\frac{m}{kne}\] \[\stackrel{{ c}}{{\leq}}\frac{1}{4}\mathbb{E}_{x_{[1,n ]}}\left[\mathbb{E}_{x}\left(\frac{1}{k}\sum_{j:x\in C_{j}}\frac{\mathbb{I}[n \mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})}\Big{|}X_{[1,n]}\right)\right]+\frac{m}{kne}\] \[\stackrel{{ d}}{{\leq}}\frac{1}{4}\mathbb{E}_{x_{[1,n ]}}\left[\frac{1}{k}\sum_{j=1}^{m}\Big{(}\frac{\mu(C_{j})\mathbb{I}[n\mu_{n}(C_ {j})>0]}{n\mu_{n}(C_{j})}\Big{)}\right]+\frac{m}{kne}\] \[=\frac{1}{4k}\sum_{j=1}^{m}\mu(C_{j})\mathbb{E}_{x_{[1,n]}}\left( \frac{\mathbb{I}[n\mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})}\right)+\frac{m}{kne}\] \[\stackrel{{ e}}{{\leq}}\frac{1}{4k}\sum_{j=1}^{m} \frac{2\mu(C_{j})}{(n+1)\mu(C_{j})}+\frac{m}{kne}=\frac{m}{2k(n+1)}+\frac{m}{kne }\leq\frac{m}{2kn}+\frac{m}{kne}\leq\frac{m}{kn}.\]where, inequality \(a\) is due to Jensen's inequality. Inequality \(b\) follows from lemma E.3. Inequality \(c\) follows from lemma E.1 and inequality \(d\) follows from lemma E.2. Finally inequality \(e\) follow from the observation that \(n\mu_{n}(C_{j})\) is Binomially distributed with parameters \(n\) and \(\mu(C_{l})\) and by an application of lemma E.4. The result follows noting that by Jensen's inequality \(\mathbb{E}|\hat{\eta}(x)-\bar{\eta}(x)|\leq\sqrt{\mathbb{E}(\hat{\eta}(x)-\bar{ \eta}(x))^{2}}\). 

**Lemma E.1**.: _Pick \(m\times d\) projection matrix \(\Theta\). Suppose the EaS representation uses (i) a mapping \(\Theta\) and (ii) k-winner-take-all sparsification. Let \(x\) be sampled from \(\mu\) and let \(D_{n}=((x_{1},y_{1}),\ldots,(x_{n},y_{n}))\) is a random training set where \(x_{i}\) is sampled from \(\mu\) and \(y_{i}\) is distributed as \(\eta(x_{i})\) for \(i\in[n]\). Then the following holds._

\[\mathbb{E}_{y_{[1,n]}}\left[\sum_{j:x\in C_{j}}\left(\frac{\sum_{i=1}^{n}(y_{i }-\eta(C_{j}))\mathbb{I}[x_{i}\in C_{j}]}{n\mu_{n}(C_{j})}\right)^{2}\mathbb{I }[n\mu_{n}(C_{j})>0]\bigg{|}x,x_{[1,n]}\right]\leq\frac{1}{4}\sum_{j:x\in C_{j }}\left(\frac{\mathbb{I}[n\mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})}\right)\]

Proof.: Conditioned on \(x\), only \(k\) of the \(m\) coordinates in the EaS representation of \(x\) are non-zero. WLOG, for ease of exposition, assume these \(k\) non-zero coordinate to be \(j_{1},\ldots,j_{k}\in[m]\). Then the number of \(x_{i}\) that falls in any such \(C_{j_{l}}\), where \(l\in[k]\), is \(n\mu_{n}(C_{j_{l}})\). The \(y_{i}\) values corresponding to these \(x_{i}\) points (there are \(n\mu_{n}(C_{j_{l}})\) of them in total) are identically and independently distributed with expectation

\[\mathbb{E}(y_{i}|x_{i}\in C_{j_{l}}) = \Pr(y_{i}=1|x_{i}\in C_{j_{l}})=\frac{1}{\mu(C_{j_{l}})}\int_{C_{ j_{l}}}\Pr(y_{i}=1|x_{i}=x)\mu(dx)\] \[= \frac{1}{\mu(C_{j_{l}})}\int_{C_{j_{l}}}\eta(x)\mu(dx)=\eta(C_{j_{ l}})\]

Therefore, we can write

\[\mathbb{E}_{y_{[1,n]}}\left[\sum_{j:x\in C_{j}}\left(\frac{\sum_{i =1}^{n}(y_{i}-\eta(C_{j}))\mathbb{I}[x_{i}\in C_{j}]}{n\mu_{n}(C_{j})}\right)^ {2}\mathbb{I}[n\mu_{n}(C_{j})>0]\bigg{|}x,x_{[1,n]}\right]\] \[=\sum_{l=1}^{k}\left[\frac{\mathbb{E}_{y_{[1,n]}}\left[\left(\sum_ {i=1}^{n}(y_{i}-\eta(C_{j_{l}}))\mathbb{I}[x_{i}\in C_{j_{l}}]\right)^{2} \mathbb{I}[n\mu_{n}(C_{j_{l}})>0]|x_{[1,n]}\right]}{(n\mu_{n}(C_{j_{l}}))^{2}}\right]\] \[\stackrel{{ a}}{{=}}\sum_{l=1}^{k}\left[\frac{\sum_ {i=1}^{n}\mathbb{E}_{y_{i}}\left[(y_{i}-\eta(C_{j_{l}}))^{2}\mathbb{I}[x_{i} \in C_{j_{l}}]\mathbb{I}[n\mu_{n}(C_{j_{l}})>0]|x_{i}\right]}{(n\mu_{n}(C_{j_ {l}}))^{2}}\right]\] \[\stackrel{{ b}}{{=}}\sum_{l=1}^{k}\left[\frac{\sum_ {i=1}^{n}\eta(C_{j_{l}})(1-\eta(C_{j_{l}}))\mathbb{I}[x_{i}\in C_{j_{l}}] \mathbb{I}[n\mu_{n}(C_{j_{l}})>0]}{(n\mu_{n}(C_{j_{l}}))^{2}}\right]\] \[\stackrel{{ c}}{{\leq}}\frac{1}{4}\sum_{l=1}^{k} \left(\frac{\mathbb{I}[n\mu_{n}(C_{j_{l}})>0]}{n\mu_{n}(C_{j_{l}})}\right)\]

where, equality \(a\) is due to the following observation. For any \(i,j\in[m],i\neq j\) and \(x_{i},x_{j}\in C_{l}\) for some \(l\in[m]\), \(y_{i}\) and \(y_{j}\) are identically and independently distributed with expectation \(\eta(C_{l})\). Therefore, the expectation of the cross product is simply:

\[\mathbb{E}_{y_{i},y_{j}}\left[(y_{i}-\eta(C_{l}))(y_{j}-\eta(C_{l })\right] = \mathbb{E}_{y_{i},y_{j}}\left[y_{i}y_{j}-\eta(C_{l})(y_{i}+y_{j} )+\eta(C_{l})^{2}\right]\] \[= \mathbb{E}y_{i}\mathbb{E}y_{j}-\eta(C_{l})(\mathbb{E}y_{i}+ \mathbb{E}y_{j})+\eta(C_{l})^{2}=0\]

Equality \(b\) follows from variance computation. In particular for any \(Y_{i},i\in[m]\) with \(x_{i}\in C_{l}\) for some \(l\in[m]\), \(\mathbb{E}\left[(y_{i}-\eta(C_{l}))^{2}\right]=\mathbb{E}y_{i}^{2}-2\eta(C_{l}) \mathbb{E}y_{i}+\eta(C_{l})^{2}=\eta(C_{l})-2\eta(C_{l})^{2}+\eta(C_{l})^{2}= \eta(C_{l})(1-\eta(C_{l}))\). Finally, inequality \(c\) follows from the fact that for any \(z\in[0,1]\), the maximum value of \(z(1-z)\) is \(\frac{1}{4}\).

It is easy to observe that the final result is equivalent to \(\frac{1}{4}\sum_{j:x\in C_{j}}\left(\frac{\mathbb{I}[n\mu_{n}(C_{j})>0]}{n\mu _{n}(C_{j})}\right)\).

**Lemma E.2**.: _Pick \(m\times d\) projection matrix \(\Theta\). Suppose the EaS representation uses (i) a mapping \(\Theta\) and (ii) k-winner-take-all sparsification. Let \(x\) be sampled from \(\mu\) and let \(D_{n}=((x_{1},y_{1}),\ldots,(x_{n},y_{n}))\) is a random training set where \(x_{i}\) is sampled from \(\mu\) and \(y_{i}\) is distributed as \(\eta(x_{i})\) for \(i\in[n]\). Then conditioned on \(x_{1},\ldots,x_{n}\), the following holds._

\[\mathbb{E}_{x}\left(\frac{1}{k}\sum_{j:x\in C_{j}}\frac{\mathbb{I}_{\{n\mu_{n}( C_{j})>0\}}}{n\mu_{n}(C_{j})}\right)\leq\frac{1}{k}\sum_{j=1}^{m}\left(\frac{ \mu(C_{j})\mathbb{I}_{\{n\mu_{n}(C_{j})>0\}}}{n\mu_{n}(C_{j})}\right)\]

Proof.: \[\mathbb{E}_{x}\left(\frac{1}{k}\sum_{j:x\in C_{j}}\frac{\mathbb{I} [n\mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})}\right) \stackrel{{ a}}{{=}} \sum_{i=1}^{\binom{m}{k}}\left(\Pr(x\in C_{i}^{k})\left(\frac{1} {k}\sum_{j\in\sigma(i)}\frac{\mathbb{I}[n\mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})} \right)\right)\] \[= \sum_{i=1}^{\binom{m}{k}}\left(\frac{1}{k}\left(\sum_{j\in\sigma( i)}\frac{\mu(C_{i}^{k})\mathbb{I}[n\mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})}\right)\right)\] \[\stackrel{{ b}}{{=}} \frac{1}{k}\sum_{j=1}^{m}\sum_{i:j\in\sigma(i)}\left(\frac{\mu(C_ {i}^{k})\mathbb{I}[n\mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})}\right)\] \[= \frac{1}{k}\sum_{j=1}^{m}\left(\frac{\mathbb{I}[n\mu_{n}(C_{j})>0 ]}{n\mu_{n}(C_{j})}\right)\sum_{i:j\in\sigma(i)}\mu(C_{i}^{k})\] \[\stackrel{{ c}}{{=}} \frac{1}{k}\sum_{j=1}^{m}\left(\frac{\mu(C_{j})\mathbb{I}[n\mu_{n} (C_{j})>0]}{n\mu_{n}(C_{j})}\right)\]

where equality \(a\) follows from part (ii) of lemma 3.4 since \(\{C_{i}^{k}\}_{i=1}^{\binom{m}{k}}\) forms a partition of \(\mathcal{X}\) and the definition of \(\sigma\) in section 3.2. Equality \(b\) follows from the following observation. In the line above inequality \(b\), we are summing \(k\times\binom{m}{k}\) terms. Since \(k\times\binom{m}{k}=m\times\binom{m-1}{k-1}\) and any \(j\in[m]\) can appear in exactly \(\binom{m-1}{k-1}\) different subsets of of size \(k\), equality \(b\) is simply rearranging the terms from the line above by changing the indices appropriately. Equality \(c\) follows from part (iv) of lemma 3.4. 

**Lemma E.3**.: _Pick \(m\times d\) projection matrix \(\Theta\). Suppose the EaS representation uses (i) a mapping \(\Theta\) and (ii) k-winner-take-all sparsification. Let \(x\) be sampled from \(\mu\) and let \(D_{n}=((x_{1},y_{1}),\ldots,(x_{n},y_{n}))\) is a random training set where \(x_{i}\) is sampled from \(\mu\) and \(y_{i}\) is distributed as \(\eta(x_{i})\) for \(i\in[n]\). Then the following holds._

\[\mathbb{E}_{x_{[1,n]}}\left[\mathbb{E}_{x,y_{[1,n]}}\left[\frac{1}{k}\sum_{j: x\in C_{j}}\eta^{2}(C_{j})\mathbb{I}[\mu_{n}(C_{j})=0]\bigg{|}x_{[1,n]} \right]\right]\leq\frac{m}{nke}\]Proof.: \[\mathbb{E}_{x_{[1,n]}}\left[\mathbb{E}_{x,y_{[1,n]}}\left[\frac{1}{k }\sum_{j:X\in C_{j}}\eta^{2}(C_{j})\mathbb{I}[\mu_{n}(C_{j})=0]\bigg{|}x_{[1,n] }\right]\right]\] \[\stackrel{{ a}}{{=}}\mathbb{E}_{x_{[1,n]}}\left[ \mathbb{E}_{x}\left[\frac{1}{k}\sum_{j:x\in C_{j}}\eta^{2}(C_{j})\mathbb{I}[\mu _{n}(C_{j})=0]\bigg{|}x_{[1,n]}\right]\right]\] \[\stackrel{{ b}}{{=}}\mathbb{E}_{x_{[1,n]}}\left[\sum _{i=1}^{\binom{m}{k}}\Pr(x\in C_{i}^{k})\left(\frac{1}{k}\sum_{j\in\sigma(i)} \eta^{2}(C_{j})\mathbb{I}[\mu_{n}(C_{j})=0]\right)\right]\] \[=\mathbb{E}_{x_{[1,n]}}\left[\frac{1}{k}\sum_{i=1}^{\binom{m}{k}} \sum_{j\in\sigma(i)}\mu(C_{i}^{k})\eta^{2}(C_{j})\mathbb{I}[\mu_{n}(C_{j})=0]\right]\] \[\stackrel{{ c}}{{=}}\mathbb{E}_{x_{[1,n]}}\left[ \frac{1}{k}\sum_{j=1}^{m}\eta^{2}(C_{j})\mathbb{I}[\mu_{n}(C_{j})=0]\left(\sum _{i:j\in\sigma(i)}\mu(C_{i}^{k})\right)\right]\] \[=\frac{1}{k}\sum_{j=1}^{m}\eta^{2}(C_{j})\mu(C_{j})\left[\mathbb{ E}_{x_{1},\ldots,x_{n}}\mathbb{I}[\mu_{n}(C_{j})=0]\right]\] \[=\frac{1}{k}\sum_{j=1}^{m}\eta^{2}(C_{j})\mu(C_{j})(1-\mu(C_{j})) ^{n}=\frac{1}{nk}\sum_{j=1}^{m}\eta^{2}(C_{j})n\mu(C_{j})(1-\mu(C_{j}))^{n}\] \[\leq\frac{1}{nk}\sum_{j=1}^{m}\eta^{2}(C_{j})n\mu(C_{j})e^{-n\mu(C _{j})}\] \[\stackrel{{ d}}{{\leq}}\frac{1}{nk}\sum_{j=1}^{m}n \mu(C_{j})e^{-n\mu(C_{j})}\leq\frac{m}{nk}\max_{j}\left\{n\mu(C_{j})e^{-n\mu(C _{j})}\right\}\stackrel{{ e}}{{\leq}}\frac{m}{nke}\]

where equality \(a\) follows from the fact that the quantity with the inner square bracket is unaffected by the \(\mathcal{Y}_{i}\)s. Equality \(b\) follows from part (ii) of lemma 3.4 since \(\{C_{i}^{k}\}_{i=1}^{\binom{m}{k}}\) forms a partition of \(\mathcal{X}\) and the definition of \(\sigma\) in section 3.2. Equality \(c\) follows from the following observation. In the line above inequality \(c\), we are summing \(k\times\binom{m}{k}\) terms. Since \(k\times\binom{m}{k}=m\times\binom{m-1}{k-1}\) and any \(j\in[m]\) can appear in exactly \(\binom{m-1}{k-1}\) different subsets of of size \(k\), equality \(c\) is simply rearranging the terms from the line above by changing the indices appropriately. Inequality \(d\) follows from the fact \(\max_{j}\eta(C_{j})\leq 1\). Finally inequality \(e\) follows from the fact that \(\sup_{z}ze^{-z}=\frac{1}{e}\). 

**Lemma E.4**.: _(Binomial bound Gyorfi et al. [2002]) Let the random variable \(B(n,p)\) be Binomially distributed with parameters \(n\) and \(p\). Then,_

\[\mathbb{E}\left(\frac{1}{B(n,p)}\mathbb{I}[B(n,p)>0]\right)\leq\frac{2}{(n+1)p}\]

### Proof of Lemma 3.10

Proof.: Take any \(x\in\mathcal{X}\). Then

\[|\eta(x)-\bar{\eta}(x)| =\left|\eta(x)-\frac{1}{k}\sum_{j:x\in C_{j}}\bar{\eta}_{j}\right| =\left|\frac{1}{k}\sum_{x\in C_{j}}(\eta(x)-\eta(x^{\prime})\right|\leq\frac{1} {k}\sum_{j:x\in C_{j}}|\eta(x)-\bar{\eta}_{j}|\] \[=\frac{1}{k}\sum_{j:x\in C_{j}}\left|\eta(x)-\frac{1}{\mu(C_{j})} \int_{C_{j}}\eta(x^{\prime})\mu(dx^{\prime})\right|\] \[=\frac{1}{k}\sum_{j:x\in C_{j}}\left|\frac{1}{\mu(C_{j})}\int_{C_ {j}}(\eta(x)-\eta(x^{\prime}))\mu(dx^{\prime})\right|\] \[\leq\frac{1}{k}\sum_{j:x\in C_{j}}\frac{1}{\mu(C_{j})}\int_{C_{j} }|\eta(x)-\eta(x^{\prime})|\mu(dx^{\prime})\] \[\leq\frac{1}{k}\sum_{j:x\in C_{j}}\frac{1}{\mu(C_{j})}\int_{C_{j} }L\|x-x^{\prime}\|^{\beta}\mu(dx^{\prime})\] \[\leq\frac{1}{k}\sum_{j:x\in C_{j}}\frac{L\cdot(\operatorname{diam }(C_{j}))^{\beta}}{\mu(C_{j})}\int_{C_{j}}\mu(dx^{\prime})\leq L\cdot\max_{j \in[m]}(\operatorname{diam}(C_{j}))^{\beta}\]

### Proof of Theorem 3.15

Proof.: Define \(\eta:\mathcal{X}_{1}\rightarrow[0,1]\) to be a triangular function defined below: for \(0<\theta\leq 2\pi\),

\[\eta(\cos\theta,\sin\theta,0,0,\ldots,0)=\begin{cases}\frac{\theta}{\pi},& \text{if }\theta\leq\pi\\ 2-\frac{\theta}{\pi},&\text{if }\theta>\pi\end{cases}\]

Clearly, \(\eta\) is \(\frac{1}{2}\)-Lipschitz and for \(k=1\), \(\bar{\eta}(x)=\sum_{j=1}^{m}\eta(C_{j})\mathbb{I}_{\{x\in C_{j}\}}\). Therefore, using Theorem E.5, with probability at least \(1/2\) over the choice of \(\Theta\),

\[\sup_{x\in\mathcal{X}_{1}}|\eta(x)-\bar{\eta}(x)|\geq c_{d}^{\prime\prime} \cdot\frac{1}{m^{1/(d-1)}\log m}\]

where \(c_{d}^{\prime\prime}\) is an absolute constant depending on \(d\). Taking expectation, with probability at least \(1/2\) over the choice of \(\Theta\), we have, \(\mathbb{E}_{X,D_{n}}|\bar{\eta}(X)-\eta(X)|\geq c_{d}^{\prime\prime}\cdot\frac {1}{m^{1/(d-1)}\log m}\). Next, using Lemma 3.8 with \(k=1\) yields, \(\mathbb{E}_{X,D_{n}}|\hat{\eta}(X)-\bar{\eta}(X)|\leq\sqrt{\frac{m}{n}}\).

Using triangle inequality combining these results,

\[\mathbb{E}_{X,D_{n}}|\eta(X)-\hat{\eta}(X)| = \mathbb{E}_{X,D_{n}}|\eta(X)-\bar{\eta}(X)+\bar{\eta}(X)-\hat{ \eta}(X)| \tag{20}\] \[\geq \mathbb{E}_{X,D_{n}}|\bar{\eta}(X)-\eta(X)|-\mathbb{E}_{X,D_{n}} |\hat{\eta}(X)-\bar{\eta}(X)|\] \[\geq c_{d}^{\prime\prime}\cdot\frac{1}{m^{1/(d-1)}\log m}-\sqrt{ \frac{m}{n}}\]

Ignoring the \(\log\) term we see that for large enough \(n\), the first term on the right hand side of (20) will dominate. In particular, for \(n=\Omega\left(m^{1+\frac{n}{d-1}}\right)\), we get \(\mathbb{E}_{X,D_{n}}|\eta(X)-\hat{\eta}(X)|\geq\Omega\left(m^{-\frac{n}{d-1}}\right)\). To see this (ignoring the log term), if \(\sqrt{\frac{m}{n}}\leq\frac{c_{d}^{\prime\prime}}{2}\cdot\frac{1}{m^{1/(d-1)}}\), then \(\mathbb{E}_{X,D_{n}}|\eta(X)-\hat{\eta}(X)|\geq\frac{c_{d}^{\prime\prime}}{2} \cdot\frac{1}{m^{1/(d-1)}}\). Now,

\[\sqrt{\frac{m}{n}}\leq\frac{c_{d}^{\prime\prime}}{2}\cdot\frac{1}{m^{1/(d-1)} }\Rightarrow n\geq\frac{4}{(c_{d}^{\prime\prime})^{2}}\cdot m^{1+\frac{2}{d-1 }}.\]In particular, setting \(n=\frac{4}{(c_{d}^{\prime})^{2}}m^{1+\frac{2}{d-1}}\), we have

\[\mathbb{E}_{X,D_{n}}|\eta(X)-\hat{\eta}(X)|\geq\frac{c_{d}^{\prime\prime}}{2}m^{- \frac{1}{d-1}}=\left(\frac{c_{d}^{\prime\prime}}{2}\right)^{\frac{d-3}{d-1}}n^{ -\frac{1}{d+1}}.\]

**Theorem E.5**.: _[Theorem 4 of Dasgupta and Tosh (2020)] For any \(d>3\), let input space \(\mathcal{X}_{1}\) be the one-dimensional sub-manifold of \(\mathbb{R}^{d}\) given in (12). Take \(k=1\). Suppose that random matrix \(\Theta\) has rows chosen from the distribution \(\nu\) that is uniform over \(\mathcal{S}^{d-1}\). For any \(0<\lambda<1\), there exists a \(\lambda\)-Lipschitz function \(f:\mathcal{X}_{1}\rightarrow\mathbb{R}\) such that with probability at least \(1/2\) over the choice of \(\Theta\), no matter how the weights \(w_{1},\ldots,w_{m}\) are set, the resulting function \(\hat{f}(x)=\sum_{j=1}^{m}w_{j}\mathds{1}_{\{x\in C_{j}\}}\) has approximation error at least_

\[\sup_{x\in\mathcal{X}_{1}}|\hat{f}(x)-f(x)|\geq c_{d}^{\prime}\cdot\lambda\cdot \frac{1}{m^{1/(d-1)}\log m}\]

_where \(c_{d}^{\prime}\) is some absolute constant depending on \(d\)._

## Appendix F Various proofs from section 4 and Appendix B

### Lemma F.1 and its proof

We first show that while \(\mathtt{EaS}\) representation using \(h_{2}\) is not \(k\)-sparse,for large enough sample size, with high probability, it is at least \(k/2\)-sparse and at most \(2k\)-sparse in expectation. F.

**Lemma F.1**.: _Pick any \(\delta>0\). For \(k,m\) and \(n\), satisfying \(n\geq\frac{\omega m}{k}\left(\log n+\log\left(\frac{m}{\delta}\right)\right)\), where \(c_{0}>0\) is a universal constant, with probability at least \(1-\delta\), over the random choice of training set \(D_{n}^{\prime}\), the following holds for all \(j\in[m]\)._

\[(k/2m)\leq\Pr_{x\sim\mu}(\theta_{j}\cdot x\geq\tau_{n}(\theta_{j}))\leq(2k/m).\]

Proof.: We start with a version of relative generalization error bound, originally due to Vapnik and Chervonenkis, that appeared in Chaudhuri and Dasgupta (2010).

**Theorem F.2**.: _(Theorem 15 of Chaudhuri and Dasgupta (2010)) Let \(\mathcal{G}\) be a class of functions from \(\mathcal{X}\) to \(\{0,1\}\) with VC dimension \(d<\infty\), and \(\mathbb{P}\) a probability distribution on \(\mathcal{X}\). Let \(\mathbb{E}\) be the expectation with respect to \(\mathbb{P}\). Suppose \(n\) points are drawn independently from \(\mathbb{P}\); let \(\mathbb{E}_{n}\) denotes expectation with respect to this sample. Then for any \(\delta>0\), with probability at least \(1-\delta\) the following holds for all \(g\in\mathcal{G}\):_

\[-\min\left(\beta_{n}\sqrt{\mathbb{E}_{n}g},\beta_{n}^{2}+\beta_{n}+\sqrt{ \mathbb{E}g}\right)\leq\mathbb{E}g-\mathbb{E}_{n}g\leq\min\left(\beta_{n}^{2} +\beta_{n}\sqrt{\mathbb{E}_{n}g},\beta_{n}\sqrt{\mathbb{E}g}\right)\]

_where \(\beta_{n}=\sqrt{\frac{4(d\log(2n)+\log(8/\delta))}{n}}\)._

Let \(\theta_{i}\) be the \(i^{th}\) row of the projection matrix \(\Theta\) and let \(\mu^{i}\) be the distribution of \(\theta_{i}\cdot x\) where \(x\) is distribution according to \(\mu\). Then, for any \(a\in\mathbb{R}\),

\[\Pr_{x\sim\mu}(\theta_{i}\cdot x\geq a)=\mu^{i}(\theta_{i}\cdot x\geq a).\]

Suppose \(x_{1}^{\prime},\ldots,x_{n}^{\prime}\) are sampled independently at random from \(\mu\). For any interval \(A_{a}=[a,\infty)\), let

\[\mu_{n}^{i}(A_{a})=\frac{1}{n}\sum_{j=1}^{n}\mathds{1}[\theta_{i}\cdot x_{j}^{ \prime}\geq a].\]

Consider the class of indicator functions over the intervals \([a,\infty)\) for \(a\in\mathbb{R}\). This class has VC dimension 1 and define \(\beta^{2}=\frac{4}{n}\left(\log 2n+\log(\frac{8m}{\delta})\right)\). Suppose \(\mu_{n}^{i}(A_{a})\) satisfies the condition \(\mu_{n}^{i}(A_{a})\geq 4\beta^{2}\).

The bound \(\mu^{i}(A_{a})-\mu^{i}_{n}(A_{a})\leq\beta^{2}+\beta\sqrt{\mu^{i}_{n}(A_{a})}\) from Theorem F.2 yields,

\[\mu^{i}(A_{a})\leq\mu^{i}_{n}(A_{a})+\beta^{2}+\beta\sqrt{\mu^{i}_{n}(A_{a})} \leq\mu^{i}_{n}(A_{a})+\frac{1}{4}\cdot\mu^{i}_{n}(A_{a})+\frac{1}{2}\cdot\cdot \mu^{i}_{n}(A_{a})=\frac{7}{4}\cdot\mu^{i}_{n}(A_{a})\]

Similarly, the bound \(-\beta\sqrt{\mu^{i}_{n}(A_{a})}\leq\mu^{i}(A_{a})-\mu^{i}_{n}(A_{a})\) from Theorem F.2 yields,

\[\mu_{i}(A_{a})\geq\mu^{i}_{n}(A_{a})-\beta\sqrt{\mu^{i}_{n}(A_{a})}\leq\mu^{i} _{n}(A_{a})-\frac{1}{2}\cdot\mu^{i}_{n}(A_{a})=\frac{1}{2}\cdot\mu^{i}_{n}(A_{ a})\]

Combining these two bounds, we can can conclude that for all intervals \(A_{a}\) satisfying \(\mu^{i}_{n}(A_{a})\geq 4\beta^{2}\), with probability at least \(1-\frac{\delta}{m}\) it holds that

\[\frac{1}{2}\cdot\mu^{i}_{n}(A_{a})\leq\mu^{i}(A_{a})\leq\frac{7}{4}\cdot\mu^{i }_{n}(A_{a}).\]

Now if we only consider the intervals \(A_{\tau_{n}(\theta_{j})}\), then from (13) it is clear that \(\mu^{i}_{n}(A_{\tau_{n}(\theta_{j})})\leq\frac{k}{m}+\frac{1}{n}\). This is because for any interval \(\mu^{i}_{n}(\cdot)\) can take \((n+1)\) possible values in steps of \(\frac{1}{n}\), namely, \(0,\frac{1}{n},\frac{2}{n},\ldots,1\). Therefore, from the definition of \(\tau_{n}(\theta_{j})\) in (13), if \(\mu^{i}_{n}(A_{\tau_{n}(\theta_{j})})\neq\frac{k}{m}\), then to ensure that \(\mu^{i}_{n}(A_{\tau_{n}(\theta_{j})})\geq\frac{k}{m}\), its maximum value can be at most \(\frac{k}{m}+\frac{1}{n}\). Therefore, the condition \(\mu^{i}_{n}(A_{\tau_{n}(\theta_{j})})\geq 4\beta^{2}\) implies,

\[\frac{k}{m}+\frac{1}{n} \geq \frac{16}{n}\left(\log 2n+\log\left(\frac{8m}{\delta}\right) \right)=\frac{16}{n}\left(\log 2+\log n+\log 8+\log\left(\frac{m}{\delta} \right)\right)\] \[= \frac{64}{n}+\frac{16}{n}\left(\log n+\log\left(\frac{m}{\delta} \right)\right)\]

or in other words, \(\frac{k}{m}\geq\frac{63}{n}+\frac{16}{n}\left(\log n+\log\left(\frac{m}{\delta }\right)\right)\). Noting that,

\[\frac{7}{4}\cdot\mu^{i}_{n}(A_{\tau_{n}(\theta_{j})})\leq\frac{7k}{4m}+\frac {7}{4n}\leq\frac{7k}{4m}+\frac{7}{4}\cdot\frac{k}{63m}\leq\frac{2k}{m}.\]

Therefore, if \(\frac{k}{m}\geq\frac{\alpha n}{n}\left(\log n+\log\left(\frac{m}{\delta} \right)\right)\) for some universal constant \(c_{0}>0\), then with probability at least \(1-\frac{\delta}{m}\),

\[\frac{k}{2m}\leq\Pr_{x\sim\mu}\left(\theta_{j}\cdot x\geq\tau_{n}(\theta_{j} )\leq\frac{2k}{m}.\right.\]

Taking union bound over \(\theta_{1},\ldots,\theta_{m}\) yields the desired results. 

### Proof of Lemma b.3

Proof.: Pick any good \(\theta\), and let \(x=\pi_{M}(\theta)\) be its projection on \(M\). Since \(\mathcal{X}\) consists of unit vectors, using Lemma F.1 with probability at least \(1-\delta\), the points that lie in \(C(\theta)\) are at least \(k/(2m)\) fraction or at most \(2k/m\) fraction of \(x\)'s (under distribution \(\mu\)) that have highest dot product with \(\theta\) or equivalent to closest to \(\theta\). Thus, \(C(\theta)\) is the set of the form \(B(\theta,r^{\prime})\) where radius \(r^{\prime}\) is so chosen that \(k/(2m)\leq\mu(B(\theta,r^{\prime}))\leq 2k/m\). However, it is not of the form \(B(x,r^{\prime\prime})\)which causes complication. In particular, two questions need to be answered: (i) if a point \(x^{\prime}\in M\) lies within distance \(r<\rho\) of \(x\), how far can it possibly be from \(\theta\), and conversely, (ii) if \(x^{\prime}\in M\) lies within distance \(r<\rho\) of \(\theta\), how far can it possibly be from \(x\)? These two questions are answered in Lemma 6 of Dasgupta and Tosh (2020) showing that,

\[B_{M}(x,r)\subset B\left(\theta,\sqrt{\Delta^{2}+\frac{\rho+\Delta}{\rho}r^{ 2}}\right) \tag{21}\]

\[B_{M}(\theta,r^{\prime})\subset B\left(x,\sqrt{\frac{\rho}{\rho-\Delta}((r^{ \prime})^{2}-\Delta^{2})}\right) \tag{22}\]

For the left-hand containment pick \(r=\sqrt{\frac{\rho-\Delta}{\rho+\Delta}}\left(\frac{k}{2c_{2}m}\right)^{1/d_{0}}\). Further taking,

\[r^{\prime}=\sqrt{\Delta^{2}+\frac{\rho+\Delta}{\rho}r^{2}}\]

[MISSING_PAGE_EMPTY:27]

\(\sum_{i=1}^{m}\mathbb{E}U_{i}\geq\alpha_{d}k\). Let \(E_{2}\) be the event that \(U>\alpha_{d}k/2\) and let \(E_{2}^{c}\) be the complement event. Using Chernoff bound, we have

\[\Pr(E_{2}^{c}|E_{1})=\Pr\left(U\leq\alpha_{d}k/2|E_{1}\right)\leq\Pr\left(U\leq (1/2)\mathbb{E}U|E_{1}\right)\leq e^{-\mathbb{E}U/8}\leq e^{-\alpha_{d}k/8}\]

Bounding the right most quantity above to be at most \(\delta/\hat{M}\), for \(k\) as specified in the lemma statement, for suitable choice of \(c_{d}^{\prime}\), we conclude that \(E_{2}\) holds (conditioned on \(E_{1}\)) with probability at least \(1-\delta\). Therefore, with probability at least \(1-2\delta\), both \(E_{1}\) and \(E_{2}\) hold, implying that for every \(\hat{x}\in\hat{M}\), there are at least \(\alpha_{d}k/2\) good \(\theta_{i}\)'s in \(A(\hat{x},r_{1}/2)\).

Now pick any arbitrary \(x\in M\). There is some \(\hat{x}\in\hat{M}\) with \(\|x-\hat{x}\|\leq r_{1}/2\). Moreover, for any \(\theta_{j}\in A(\hat{x},r_{1}/2)\Rightarrow\theta_{j}\in A(x,r_{1}/2) \Rightarrow x\in C(\theta_{j})\). 

### Proof of Lemma b.1

For ease of exposition, we use the notation \(x_{[1,n]}\) to denote \(x_{1},\ldots,x_{n}\) and \(y_{[1,n]}\) to denote \(y_{1},\ldots,y_{n}\), \(x_{[1,n]}^{\prime}\) to denote \(x_{1}^{\prime},\ldots,x_{n}^{\prime}\) and \(y_{[1,n]}^{\prime}\) to denote \(y_{1}^{\prime},\ldots,y_{n}^{\prime}\) for various proofs appearing in the section. Note number of non-zero entries in the EaS representation given in (14) is variable and we use the notation \(k(x)\) to denote the number of non-zero entries in \(h_{2}(x)\).

[MISSING_PAGE_EMPTY:29]

[MISSING_PAGE_EMPTY:30]

\(\sum_{j:x\in C_{j}}\eta^{2}(C_{j})\mathbb{I}[n\mu_{n}(C_{j})]\mathbb{I}[j\in A_{t} ^{x}]=0\). Inequality \(c\) follows from the fact that \(\eta(C_{j})\leq 1,\forall j\). Inequality \(d\) follows from Lemma F.5. Inequality \(e\) follows since we are adding a non-negative quantity. Finally, inequality \(f\) follows from the fact that \(\sup_{z}ze^{-z}=\frac{1}{e}\). 

**Lemma F.5**.: \[\mathbb{E}_{x}\left[\sum_{j:x\in C_{j}}\mathbb{I}[n\mu_{n}(C_{j})=0]\mathbb{I }[j\in A_{t}(x)]\bigg{|}k(x)=k^{\prime}\right]\leq\sum_{j=1}^{m}\mu(C_{j}) \mathbb{I}[n\mu_{n}(C_{j})=0]\]

Proof.: Conditioned on \(k(x)=k^{\prime}\), \(\mathtt{EaS}\) representation given in (14) has exactly \(k^{\prime}\) non-zero entries. Therefore, using Lemma 3.4,

\[\mathbb{E}_{x}\left[\sum_{j:x\in C_{j}}\mathbb{I}[n\mu_{n}(C_{j}) =0]\mathbb{I}[j\in A_{t}(x)]\bigg{|}k(x)=k^{\prime}\right]\] \[\stackrel{{ a}}{{=}}\sum_{i=1}^{\binom{m}{k^{\prime} }}\Pr\left(x\in C_{i}^{k^{\prime}}\right)\sum_{j:\sigma(i)}\mathbb{I}[n\mu_{n} (C_{j})=0]\mathbb{I}[j\in A_{t}(x)]\] \[=\sum_{i=1}^{\binom{m}{k^{\prime}}}\sum_{j:\sigma(i)}\mu(C_{i}^{ k^{\prime}})\mathbb{I}[n\mu_{n}(C_{j})=0]\mathbb{I}[j\in A_{t}(x)]\] \[\stackrel{{ b}}{{=}}\sum_{j=1}^{m}\sum_{i:j\in\sigma( i)}\mu(C_{i}^{k^{\prime}})\mathbb{I}[n\mu_{n}(C_{j})=0]\mathbb{I}[j\in A_{t}(x)]\] \[=\sum_{j=1}^{m}\mathbb{I}[n\mu_{n}(C_{j})=0]\left(\sum_{i:j\in \sigma(i)}\mu(C_{i}^{k^{\prime}})\mathbb{I}[j\in A_{t}(x)]\right)\] \[\leq\sum_{j=1}^{m}\mathbb{I}[n\mu_{n}(C_{j})=0]\left(\sum_{i:j\in \sigma(i)}\mu(C_{i}^{k^{\prime}})\right)\stackrel{{ c}}{{=}}\sum_{ j=1}^{m}\mu(C_{j})\mathbb{I}[n\mu_{n}(C_{j})=0]\]

where equality \(a\) follows from part (ii) of lemma 3.4 since \(\{C_{i}^{k^{\prime}}\}_{i=1}^{\binom{m}{k^{\prime}}}\) forms a partition of \(\mathcal{X}\) and the definition of \(\sigma\) in section 3.2 with \(k=k^{\prime}\). Equality \(b\) follows from the following observation. In the line above inequality \(b\), we are summing \(k^{\prime}\times\binom{m}{k^{\prime}}\) terms. Since \(k^{\prime}\times\binom{m}{k^{\prime}}=m\times\binom{m-1}{k^{\prime}-1}\) and any \(j\in[m]\) can appear in exactly \(\binom{m-1}{k^{\prime}-1}\) different subsets of of size \(k^{\prime}\), equality \(b\) is simply rearranging the terms from the line above by changing the indices appropriately. Equality \(c\) follows from part (iv) of lemma 3.4. 

**Lemma F.6**.: _Pick \(m\times d\) projection matrix \(\Theta\). Suppose the \(\mathtt{EaS}\) representation uses (i) a mapping \(\Theta\) and (ii) empirical \(k\)-threshold sparsification. Let \(x\) be sampled from \(\mu\) and let \(D_{n}=((x_{1},y_{1}),\ldots,(x_{n},y_{n}))\) is a random training set where \(x_{i}\) is sampled from \(\mu\) and \(y_{i}\) is distributed as \(\eta(x_{i})\) for \(i\in[n]\). Similarly, \(D^{\prime}_{n}=((x^{\prime}_{1},y^{\prime}_{1}),\ldots,(x^{\prime}_{n},y^{ \prime}_{n}))\) be another random training set independent of \(D_{n}\), where \(x^{\prime}_{i}\) is sampled from \(\mu\) and \(y^{\prime}_{i}\) is distributed as \(\eta(x^{\prime}_{i})\) for \(i\in[n]\). Then the following holds._

\[\mathbb{E}_{y_{[1,n]}}\left[\sum_{j:x\in C_{j}}\left(\frac{\sum _{i=1}^{n}(y_{i}-\eta(C_{j}))\mathbb{I}[x_{i}\in C_{j}]}{n\mu_{n}(C_{j})} \right)^{2}\mathbb{I}[n\mu_{n}(C_{j})>0]\mathbb{I}[j\in A_{t}(x)]\bigg{|}x,x_{ [1,n]},x^{\prime}_{[1,n]}\right]\] \[\leq\frac{1}{4}\sum_{j:x\in C_{j}}\left(\frac{\mathbb{I}[n\mu_{ n}(C_{j})>0]\mathbb{I}[j\in A_{t}(x)]}{n\mu_{n}(C_{j})}\right)\]

Proof.: Conditioned on \(x\) and \(x^{\prime}_{[1,n]}\), only \(k(x)=k^{\prime}\) of the \(m\) coordinates in the \(\mathtt{EaS}\) representation of \(x\) are non-zero. If \(k^{\prime}=0\) then the right hand side of the Lemma statement still holds with tghe value being zero since each \(C_{j}\) will be an empty set in that case. WLOG, for ease of exposition,assume that \(k^{\prime}>0\) and these \(k^{\prime}\) non-zero coordinate to be \(j_{1},\ldots,j_{k^{\prime}}\in[m]\). Then the number of \(x_{i}\) that falls in any such \(C_{j_{l}}\), where \(l\in[k^{\prime}]\), is \(n\mu_{n}(C_{j_{l}})\). The \(y_{i}\) values corresponding to these \(x_{i}\) points (there are \(n\mu_{n}(C_{j_{l}})\) of them in total) are identically and independently distributed with expectation

\[\mathbb{E}(y_{i}|x_{i}\in C_{j_{l}}) = \Pr(y_{i}=1|x_{i}\in C_{j_{l}})=\frac{1}{\mu(C_{j_{l}})}\int_{C_{ j_{l}}}\Pr(y_{i}=1|x_{i}=x)\mu(dx)\] \[= \frac{1}{\mu(C_{j_{l}})}\int_{C_{j_{l}}}\eta(x)\mu(dx)=\eta(C_{j_ {l}})\]

Therefore, we can write

\[\mathbb{E}_{y_{[1,n]}}\left[\sum_{j:x\in C_{j}}\left(\frac{\sum_{ i=1}^{n}(y_{i}-\eta(C_{j}))\mathbb{I}[x_{i}\in C_{j}]}{n\mu_{n}(C_{j})} \right)^{2}\mathbb{I}[n\mu_{n}(C_{j})>0]\mathbb{I}[j\in A_{t}(x)]\bigg{|}x,x_ {[1,n]},x^{\prime}_{[1,n]}\right]\] \[=\sum_{l=1}^{k^{\prime}}\left[\frac{\mathbb{E}_{y_{[1,n]}}\left[ (\sum_{i=1}^{n}(y_{i}-\eta(C_{j_{l}}))\mathbb{I}[x_{i}\in C_{j_{l}}])^{2}\, \mathbb{I}[n\mu_{n}(C_{j_{l}})>0]\mathbb{I}[j_{l}\in A_{t}(x)]\bigg{|}x,x_{[1, n]},x^{\prime}_{[1,n]}\right]}{(n\mu_{n}(C_{j_{l}}))^{2}}\right]\] \[\stackrel{{ a}}{{=}}\sum_{l=1}^{k^{\prime}}\left[ \frac{\sum_{i=1}^{n}\mathbb{E}_{y_{i}}\left[(y_{i}-\eta(C_{j_{l}}))^{2}\mathbb{ I}[x_{i}\in C_{j_{l}}]\mathbb{I}[n\mu_{n}(C_{j_{l}})>0]\mathbb{I}[j_{l}\in A_{t}(x )]\bigg{|}x,x_{i},x^{\prime}_{i}\right]}{(n\mu_{n}(C_{j_{l}}))^{2}}\right]\] \[\stackrel{{ b}}{{=}}\sum_{l=1}^{k^{\prime}}\left[ \frac{\sum_{i=1}^{n}\eta(C_{j_{l}})(1-\eta(C_{j_{l}}))\mathbb{I}[x_{i}\in C_{j _{l}}]\mathbb{I}[n\mu_{n}(C_{j_{l}})>0]\mathbb{I}[j_{l}\in A_{t}(x)]}{(n\mu_{n} (C_{j_{l}}))^{2}}\right]\] \[\stackrel{{ c}}{{\leq}}\frac{1}{4}\sum_{l=1}^{k} \left(\frac{\mathbb{I}[n\mu_{n}(C_{j_{l}})>0]\mathbb{I}[j_{l}\in A_{t}(x)]}{n \mu_{n}(C_{j_{l}})}\right)\]

where, equality \(a\) is due to the following observation. For any \(i,j\in[m],i\neq j\) and \(x_{i},x_{j}\in C_{l}\) for some \(l\in[m]\), \(y_{i}\) and \(y_{j}\) are identically and independently distributed with expectation \(\eta(C_{l})\). Therefore, the expectation of the cross product is simply:

\[\mathbb{E}_{y_{i},y_{j}}\left[(y_{i}-\eta(C_{l}))(y_{j}-\eta(C_{l })\right] = \mathbb{E}_{y_{i},y_{j}}\left[y_{i}y_{j}-\eta(C_{l})(y_{i}+y_{j} )+\eta(C_{l})^{2}\right]\] \[= \mathbb{E}y_{i}\mathbb{E}y_{j}-\eta(C_{l})(\mathbb{E}y_{i}+ \mathbb{E}y_{j})+\eta(C_{l})^{2}=0.\]

Equality \(b\) follows from variance computation. In particular for any \(y_{i},i\in[m]\) with \(x_{i}\in C_{l}\) for some \(l\in[m]\),

\[\mathbb{E}\left[(y_{i}-\eta(C_{l}))^{2}\right]=\mathbb{E}y_{i}^{2}-2\eta(C_{l} )\mathbb{E}y_{i}+\eta(C_{l})^{2}=\eta(C_{l})-2\eta(C_{l})^{2}+\eta(C_{l})^{2}= \eta(C_{l})(1-\eta(C_{l})).\]

Finally, inequality \(c\) follows from the fact that for any \(z\in[0,1]\), the maximum value of \(z(1-z)\) is \(\frac{1}{4}\).

It is easy to observe that the final result is equivalent to \(\frac{1}{4}\sum_{j:x\in C_{j}}\left(\frac{\mathbb{I}[n\mu_{n}(C_{j})>0] \mathbb{I}[j\in A_{t}(x)]}{n\mu_{n}(C_{j})}\right)\). 

**Lemma F.7**.: _Pick \(m\times d\) projection matrix \(\Theta\). Suppose the EaS representation uses (i) a mapping \(\Theta\) and (ii) empirical \(k\)-threshold sparsification. Let \(x\) be sampled from \(\mu\) and let \(D_{n}=((x_{1},y_{1}),\ldots,(x_{n},y_{n}))\) is a random training set where \(x_{i}\) is sampled from \(\mu\) and \(y_{i}\) is distributed as \(\eta(x_{i})\) for \(i\in[n]\). Similarly, \(D^{\prime}_{n}=((x^{\prime}_{1},y^{\prime}_{1}),\ldots,(x^{\prime}_{n},y^{ \prime}_{n}))\) be another random training set independent of \(D_{n}\), where \(x^{\prime}_{i}\) is sampled from \(\mu\) and \(y^{\prime}_{i}\) is distributed as \(\eta(x^{\prime}_{i})\) for \(i\in[n]\). Then the following holds._

\[\mathbb{E}_{x_{[1,n]},x^{\prime}_{[1,n]}}\left[\mathbb{E}_{x}\left[\frac{1}{t} \sum_{j:x\in C_{j}}\frac{\mathbb{I}[\mu_{n}(C_{j})>0]\mathbb{I}[j\in A_{t}(x )]}{n\mu_{n}(C_{j})}\bigg{|}x_{[1,n]},x^{\prime}_{[1,n]}\right]\right]\leq \frac{2m}{t(n+1)}\]Proof.: We start by taking the \(1/t\) factor outside.

\[\frac{1}{t}\mathbb{E}_{x_{[1,n]},x^{\prime}_{[1,n]}}\left[\mathbb{E} _{x}\left[\sum_{j:x\in C_{j}}\frac{\mathbb{I}[\mu_{n}(C_{j})>0]\mathbb{I}[j\in A _{t}(x)]}{n\mu_{n}(C_{j})}\bigg{|}x_{[1,n]},x^{\prime}_{[1,n]}\right]\right]\] \[=\frac{1}{t}\mathbb{E}_{x_{[1,n]},x^{\prime}_{[1,n]}}\left[\sum_{k^ {\prime}=0}^{m}\Pr(k(x)=k^{\prime})\mathbb{E}_{x}\left[\sum_{j:x\in C_{j}}\frac{ \mathbb{I}[n\mu_{n}(C_{j})>0]\mathbb{I}[j\in A_{t}(x)]}{n\mu_{n}(C_{j})}\bigg{|} k(x)=k^{\prime}\right]\right]\left|x_{[1,n]},x^{\prime}_{[1,n]}\right]\] \[\stackrel{{ a}}{{=}}\frac{1}{t}\mathbb{E}_{x_{[1,n]},x^{\prime}_{[1,n]}}\left[\sum_{k^{\prime}=1}^{m}\Pr(k(x)=k^{\prime})\mathbb{E} _{x}\left[\sum_{j:x\in C_{j}}\frac{\mathbb{I}[n\mu_{n}(C_{j})>0]\mathbb{I}[j\in A _{t}(x)]}{n\mu_{n}(C_{j})}\bigg{|}k(x)=k^{\prime}\right]\right]\left|x_{[1,n]},x^{\prime}_{[1,n]}\right]\] \[\stackrel{{ b}}{{\leq}}\frac{1}{t}\mathbb{E}_{x_{[1,n]},x^{\prime}_{[1,n]}}\left[\sum_{k^{\prime}=1}^{m}\Pr(k(x)=k^{\prime})\sum_{j=1}^ {m}\frac{\mu(C_{j})\mathbb{I}[n\mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})}\bigg{|}x_{[1,n]},x^{\prime}_{[1,n]}\right]\] \[\stackrel{{ c}}{{\leq}}\frac{1}{t}\mathbb{E}_{x_{[1,n]},x^{\prime}_{[1,n]}}\left[\Pr(k(x)=0)\sum_{j=1}^{m}\frac{\mu(C_{j})\mathbb{I}[n \mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})}+\sum_{k^{\prime}=1}^{m}\Pr(k(x)=k^{\prime}) \sum_{j=1}^{m}\frac{\mu(C_{j})\mathbb{I}[n\mu_{n}(C_{j})=0]}{n\mu_{n}(C_{j})}\right.\] \[\left.\bigg{|}x_{[1,n]},x^{\prime}_{[1,n]}\right]\] \[=\frac{1}{t}\mathbb{E}_{x_{[1,n]},x^{\prime}_{[1,n]}}\left[\sum_{ j=1}^{m}\frac{\mu(C_{j})\mathbb{I}[n\mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})}\bigg{|}x_{[1,n]},x^{\prime}_{[1,n]}\right]\] \[=\frac{1}{t}\mathbb{E}_{x^{\prime}_{[1,n]}}\left[\sum_{j=1}^{m} \mathbb{E}_{x^{\prime}_{[1,n]}}\left[\frac{\mu(C_{j})\mathbb{I}[n\mu_{n}(C_{j})>0 ]}{n\mu_{n}(C_{j})}\bigg{|}x^{\prime}_{[1,n]}\right]\right]\] \[\stackrel{{ d}}{{\leq}}\frac{1}{t}\mathbb{E}_{x^{ \prime}_{[1,n]}}\left[\sum_{j=1}^{m}\frac{2\mu(C_{j})}{(n+1)\mu(C_{j})}\right] =\frac{1}{t}\mathbb{E}_{x^{\prime}_{[1,n]}}\left[\frac{2m}{(n+1)}\right]=\frac{2 m}{t(n+1)}\]

where equality \(a\) follows from the fact that for \(k(x)=0\), number of non-zero entries in the EaS of \(x\) using (14) is zero and \(x\notin C_{j},\forall j\). Therefore, we denote the quantity \(\sum_{j:x\in C_{j}}\frac{\mathbb{I}[n\mu_{n}(C_{j})>0]\mathbb{I}[j\in A_{t}(x) ]}{n\mu_{n}(C_{j})}=0\). Inequality \(b\) follows from Lemma F.8. Inequality \(c\) follows since we are adding a non-negative quantity. Finally, inequality \(d\) follows from the fact that \(n\mu_{n}(C_{j})\) is Binomially distributed with parameters \(n\) and \(\mu(C_{j})\) and by an application of lemma E.4. 

**Lemma F.8**.: \[\mathbb{E}_{x}\left[\sum_{j:x\in C_{j}}\frac{\mathbb{I}[n\mu_{n}(C_{j})>0] \mathbb{I}[j\in A_{t}(x)]}{n\mu_{n}(C_{j})}\bigg{|}k(x)=k^{\prime}\right]\leq \sum_{j=1}^{m}\frac{\mu(C_{j})\mathbb{I}[n\mu_{n}(C_{j})>0]}{n\mu_{n}(C_{j})}\]Proof.: Conditioned on \(k(x)=k^{\prime}\), \(\mathtt{EaS}\) representation given in (14) has exactly \(k^{\prime}\) non-zero entries. Therefore, using Lemma 3.4,

\[\mathbb{E}_{x}\left[\sum_{j:x\in C_{j}}\frac{\mathbb{I}[n\mu_{n}(C_ {j})>0]\mathbb{I}[j\in A_{t}(x)]}{n\mu_{n}(C_{j})}\bigg{|}k(x)=k^{\prime}\right]\] \[\stackrel{{ a}}{{=}}\sum_{i=1}^{n}\Pr\left(x\in C_{i} ^{k^{\prime}}\right)\sum_{j:\sigma(i)}\frac{\mathbb{I}[n\mu_{n}(C_{j})>0] \mathbb{I}[j\in A_{t}(x)]}{n\mu_{n}(C_{j})}\] \[=\sum_{i=1}^{n}\sum_{j:\sigma(i)}\frac{\mu(C_{i}^{k^{\prime}}) \mathbb{I}[n\mu_{n}(C_{j})>0]\mathbb{I}[j\in A_{t}(x)]}{n\mu_{n}(C_{j})}\] \[\stackrel{{ b}}{{=}}\sum_{j=1}^{m}\sum_{i:j\in\sigma (i)}\frac{\mu(C_{i}^{k^{\prime}})\mathbb{I}[n\mu_{n}(C_{j})>0]\mathbb{I}[j\in A _{t}(x)]}{n\mu_{n}(C_{j})}\] \[=\sum_{j=1}^{m}\frac{\mathbb{I}[n\mu_{n}(C_{j})>0]\mathbb{I}[j\in A _{t}(x)]}{n\mu_{n}(C_{j})}\left(\sum_{i:j\in\sigma(i)}\mu(C_{i}^{k^{\prime}})\right)\] \[\leq\sum_{j=1}^{m}\frac{\mathbb{I}[n\mu_{n}(C_{j})>0]}{n\mu_{n}(C _{j})}\left(\sum_{i:j\in\sigma(i)}\mu(C_{i}^{k^{\prime}})\right)\stackrel{{ \varepsilon}}{{=}}\sum_{j=1}^{m}\frac{\mu(C_{j})\mathbb{I}[n\mu_{n}(C_{j})>0]}{ n\mu_{n}(C_{j})}\]

where equality \(a\) follows from part (ii) of lemma 3.4 since \(\{C_{i}^{k^{\prime}}\}_{i=1}^{\binom{m}{k^{\prime}}}\) forms a partition of \(\mathcal{X}\) and the definition of \(\sigma\) in section 3.2 with \(k=k^{\prime}\). Equality \(b\) follows from the following observation. In the line above inequality \(b\), we are summing \(k^{\prime}\times\binom{m}{k^{\prime}}\) terms. Since \(k^{\prime}\times\binom{m}{k^{\prime}}=m\times\binom{m-1}{k^{\prime}-1}\) and any \(j\in[m]\) can appear in exactly \(\binom{m-1}{k^{\prime}-1}\) different subsets of of size \(k^{\prime}\), equality \(b\) is simply rearranging the terms from the line above by changing the indices appropriately. Equality \(c\) follows from part (iv) of lemma 3.4.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See section 3,4,5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: See section 3,4 and Appendix B-F.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: section 5, Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: Data is available from OpenML repository. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bar provided in the form of shaded graph. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Data used from OpenML repository. KNN and Random Forest models are run from Scikit-learn. See section 5. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets**Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.