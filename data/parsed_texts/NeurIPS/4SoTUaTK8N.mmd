# Reversible and irreversible bracket-based dynamics

for deep graph neural networks

Anthony Gruber

Center for Computing Research

Sandia National Laboratories

Albuquerque, NM. USA

adgrube@sandia.gov

K. Lee acknowledges the support from the U.S. National Science Foundation under grant CNS2210137. Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525. This paper describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government. This article has been co-authored by an employee of National Technology & Engineering Solutions of Sandia, LLC under Contract No. DE-NA0003525 with the U.S. Department of Energy (DOE). The employee owns all right, title and interest in and to the article and is solely responsible for its contents. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this article or allow others to do so, for United States Government purposes. The DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan [https://www.energy.gov/downloads/doe-public-access-plan](https://www.energy.gov/downloads/doe-public-access-plan). The work of N. Trask and A. Gruber is supported by the U.S. Department of Energy, Office of Advanced Computing Research under the "Scalable and Efficient Algorithms - Causal Reasoning, Operators and Graphs" (SEA-CROGS) project, the DoE Early Career Research Program, and the John von Neumann fellowship at Sandia.

Kookjin Lee

School of Computing and Augmented Intelligence

Arizona State University

Tempe, AZ. USA

kookjin.lee@asu.edu

Nathaniel Trask

School of Engineering and Applied Science

University of Pennsylvania

Philadelphia, PA. USA

ntrask@seas.upenn.edu

K. Lee acknowledges the support from the U.S. National Science Foundation under grant CNS2210137. Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525. This paper describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government. This article has been co-authored by an employee of National Technology & Engineering Solutions of Sandia, LLC under Contract No. DE-NA0003525 with the U.S. Department of Energy (DOE). The employee owns all right, title and interest in and to the article and is solely responsible for its contents. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this article or allow others to do so, for United States Government purposes. The DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan [https://www.energy.gov/downloads/doe-public-access-plan](https://www.energy.gov/downloads/doe-public-access-plan). The work of N. Trask and A. Gruber is supported by the U.S. Department of Energy, Office of Advanced Computing Research under the "Scalable and Efficient Algorithms - Causal Reasoning, Operators and Graphs" (SEA-CROGS) project, the DoE Early Career Research Program, and the John von Neumann fellowship at Sandia.

###### Abstract

Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth. It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance. Code is available at the Github repository [https://github.com/natrask/BracketGraphs](https://github.com/natrask/BracketGraphs).

Introduction

Graph neural networks (GNNs) have emerged as a powerful learning paradigm able to treat unstructured data and extract "object-relation"/causal relationships while imparting inductive biases which preserve invariances through the underlying graph topology [1; 2; 3; 4]. This framework has proven effective for a wide range of both _graph analytics_ and _data-driven physics modeling_ problems. Despite successes, GNNs have generally struggle to achieve the improved performance with increasing depth typical of other architectures. Well-known pathologies, such as oversmoothing, oversquashing, bottlenecks, and exploding/vanishing gradients yield deep GNNs which are either unstable or lose performance as the number of layers increase [5; 6; 7; 8].

To combat this, a number of works build architectures which mimic physical processes to impart desirable numerical properties. For example, some works claim that posing message passing as either a diffusion process or reversible flow may promote stability or help retain information, respectively. These present opposite ends of a spectrum between irreversible and reversible processes, which either dissipate or retain information. It is unclear, however, what role (ir)reversibility plays [9]. One could argue that dissipation entropically destroys information and could promote oversmoothing, so should be avoided. Alternatively, in dynamical systems theory, dissipation is crucial to realize a low-dimensional attractor, and thus dissipation may play an important role in realizing dimensionality reduction. Moreover, recent work has shown that dissipative phenomena can actually sharpen information as well as smooth it [10], although this is not often noticed in practice since typical empirical tricks (batch norm, etc.) lead to a departure from the governing mathematical theory.

In physics, Poisson brackets and their metriplectic/port-Hamiltonian generalization to dissipative systems provide an abstract framework for studying conservation and entropy production in dynamical systems. In this work, we construct four novel architectures which span the (ir)reversibility spectrum, using geometric brackets as a means of parameterizing dynamics abstractly without empirically assuming a physical model. This relies on an application of the data-driven exterior calculus (DDEC) [11], which allows a reinterpretation of the message-passing and aggregation of graph attention networks [12] as the fluxes and conservation balances of physics simulators [13], providing a simple but powerful framework for mathematical analysis. In this context, we recast graph attention as an inner-product on graph features, inducing graph derivative "building-blocks" which may be used to build geometric brackets. In the process, we generalize classical graph attention [12] to higher-order clique cochains (e.g., labels on edges and loops). The four architectures proposed here scale with identical complexity to classical graph attention networks, and possess desirable properties that have proven elusive in current architectures. On the reversible and irreversible end of the spectrum we have _Hamiltonian_ and _Gradient_ networks. In the middle of the spectrum, _Double Bracket_ and _Metriplectic_ architectures combine both reversibility and irreversibility, dissipating energy to either the environment or an entropic variable, respectively, in a manner consistent with the second law of thermodynamics. We summarize these brackets in Table 1, providing a diagram of their architecture in Figure 1.

**Primary contributions:**

**Theoretical analysis of GAT in terms of exterior calculus.** Using DDEC we establish a unified framework for construction and analysis of message-passing graph attention networks, and provide an extensive introductory primer to the theory in the appendices. In this setting, we show that with our modified attention mechanism, GATs amount to a diffusion process for a special choice of activation and weights.

**Generalized attention mechanism.** Within this framework, we obtain a natural and flexible extension of graph attention from nodal features to higher order cliques (e.g. edge features). We show attention must have a symmetric numerator to be formally structure-preserving, and introduce a novel and flexible graph attention mechanism parameterized in terms of learnable inner products on nodes and edges.

**Novel structure-preserving extensions.** We develop four GNN architectures based upon bracket-based dynamical systems. In the metriplectic case, we obtain the first architecture with linear complexity in the size of the graph while previous works are \(O(N^{3})\).

**Unified evaluation of dissipation.** We use these architectures to systematically evaluate the role of (ir)reversibility in the performance of deep GNNs. We observe best performance for partially dissipative systems, indicating that a combination of both reversibility and irreversibility are important. Pure diffusion is the least performant across all benchmarks. For physics-based problems including optimal control, there is a distinct improvement. All models provide near state-of-the-art performance and marked improvements over black-box GAT/NODE networks.

## 2 Previous works

**Neural ODEs:** Many works use neural networks to fit dynamics of the form \(\dot{x}=f(x,\theta)\) to time series data. Model calibration (e.g., UDE [14]), dictionary-based learning (e.g., SINDy [15]), and neural ordinary differential equations (e.g., NODE [16]) pose a spectrum of inductive biases requiring progressively less domain expertise. Structure-preservation provides a means of obtaining stable training without requiring domain knowledge, ideally achieving the flexibility of NODE with the robustness of UDE/SINDy. The current work learns dynamics on a graph while using a modern NODE library to exploit the improved accuracy of high-order integrators [17; 18; 19].

**Structure-preserving dense networks:** For dense networks, it is relatively straightforward to parameterize reversible dynamics, see for example: Hamiltonian neural networks [20; 21; 22; 23], Hamiltonian generative networks [24], Hamiltonian with Control (SymODEN) [25], Deep Lagrangian networks [26] and Lagrangian neural networks [27]. Structure-preserving extensions to dissipative systems are more challenging, particularly for _metriplectic_ dynamics [28] which require a delicate degeneracy condition to preserve discrete notions of the first and second laws of thermodynamics. For dense networks such constructions are intensive, suffering from \(O(N^{3})\) complexity in the number of features [29; 30; 31]. In the graph setting we avoid this and achieve linear complexity by exploiting exact sequence structure. Alternative dissipative frameworks include Dissipative SymODEN [32] and port-Hamiltonian [33]. We choose to focus on metriplectic parameterizations due to their broad potential impact in data-driven physics modeling, and ability to naturally treat fluctuations in multiscale systems [34].

**Physics-informed vs structure-preserving:** "Physics-informed" learning imposes physics by penalty, adding a regularizer corresponding to a physics residual. The technique is simple to implement and has been successfully applied to solve a range of PDEs [35], discover data-driven models to complement first-principles simulators [36; 37; 38], learn metriplectic dynamics [39], and perform uncertainty quantification [40; 41]. Penalization poses a multiobjective optimization problem, however, with parameters weighting competing objectives inducing pathologies during training, often resulting in physics being imposed to a coarse tolerance and qualitatively poor predictions [42; 43]. In contrast, structure-preserving architectures exactly impose physics by construction via carefully designed networks. Several works have shown that penalty-based approaches suffer in comparison, with structure-preservation providing improved long term stability, extrapolation and physical realizability.

**Structure-preserving graph networks:** Several works use discretizations of specific PDEs to combat oversmoothing or exploding/vanishing gradients, e.g. telegraph equations [44] or various reaction-diffusion systems [45]. Several works develop Hamiltonian flows on graphs [46; 47]. For metriplectic dynamics, [48] poses a penalty based formulation on graphs. We particularly focus on GRAND, which poses graph learning as a diffusive process [49], using a similar exterior calculus framework and interpreting attention as a diffusion coefficient. We show in Appendix A.5 that their

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Formalism** & **Equation** & **Requirements** & **Completeness** & **Character** \\ Hamiltonian & \(\dot{\mathbf{x}}=\{\mathbf{x},E\}\) & \(\mathbf{L}^{*}=-\mathbf{L}\), & complete & conservative \\   Gradient & \(\dot{\mathbf{x}}=-[\mathbf{x},E]\) & \(\mathbf{M}^{*}=\mathbf{M}\) & incomplete & totally dissipative \\ Double Bracket & \(\dot{\mathbf{x}}=\{\mathbf{x},E\}+\{\{\mathbf{x},E\}\}\) & \(\mathbf{L}^{*}=-\mathbf{L}\) & incomplete & partially dissipative \\ Metriplectic & \(\dot{\mathbf{x}}=\{\mathbf{x},E\}+[\mathbf{x},S]\) & \(\mathbf{L}^{*}=-\mathbf{L},\mathbf{M}^{*}=\mathbf{M}\), & complete & partially dissipative \\  & & \(\mathbf{L}\nabla S=\mathbf{M}\nabla E=\mathbf{0}\) & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: The abstract bracket formulations employed in this work. Here \(\mathbf{x}\) represents a state variable, while \(E=E(\mathbf{x}),S=S(\mathbf{x})\) are energy and entropy functions. “Conservative” indicates purely reversible motion, “totally dissipative” indicates purely irreversible motion, and “partially dissipative” indicates motion which either dissipates \(E\) (in the double bracket case) or generates \(S\) (in the metriplectic case).

the governing theory. To account for this, we introduce a _modified attention mechanism_ which retains interpretation as a part of diffusion PDE. In this purely irreversible case, it is of interest whether adherence to the theory provides improved results, or GRAND's success is driven by something other than structure-preservation.

## 3 Theory and fundamentals

Here we introduce the two essential ingredients to our approach: bracket-based dynamical systems for neural differential equations, and the data-driven exterior calculus which enables their construction. A thorough introduction to this material is provided in Appendices A.1, A.3, and A.2.

**Bracket-based dynamics:** Originally introduced as an extension of Hamiltonian/Lagrangian dynamics to include dissipation [50], bracket formulations are used to inform a dynamical system with certain structural properties, e.g., time-reversibility, invariant differential forms, or property preservation. Even without dissipation, bracket formulations may compactly describe dynamics while preserving core mathematical properties, making them ideal for designing neural architectures.

Bracket formulations are usually specified via some combination of reversible brackets \(\{F,G\}=\left\langle\nabla F,\mathbf{L}\nabla G\right\rangle\) and irreversible brackets \([F,G]=\left\langle\nabla F,\mathbf{M}\nabla G\right\rangle,\{\{F,G\}\}=\left \langle\nabla F,\mathbf{L}^{2}\nabla G\right\rangle\) for potentially state-dependent operators \(\mathbf{L}^{*}=-\mathbf{L}\) and \(\mathbf{M}^{*}=\mathbf{M}\). The particular brackets which are used in the present network architectures are summarized in Table 1. Note that complete systems are the dynamical extensions of isolated thermodynamical systems: they conserve energy and produce entropy, with nothing lost to the ambient environment. Conversely, incomplete systems do not account for any lost energy: they only require that it vanish in a prescribed way. The choice of completeness is an application-dependent modeling assumption.

**Exterior calculus:** In the combinatorial Hodge theory [51], an oriented graph \(\mathcal{G}=\{\mathcal{V},\mathcal{E}\}\) carries sets of \(k\)-cliques, denoted \(\mathcal{G}_{k}\), which are collections of ordered subgraphs generated by \(\{k+1\}\) nodes. This induces natural exterior derivative operators \(d_{k}:\Omega_{k}\rightarrow\Omega_{k+1}\), acting on the spaces of functions on \(\mathcal{G}_{k}\), which are the signed incidence matrices between \(k\)-cliques and \((k+1)\)-cliques. An explicit representation of these derivatives is given in Appendix A.1, from which it is easy to check the exact sequence property \(d_{k+1}\circ d_{k}=0\) for any \(k\). This yields a discrete de Rham complex on the graph \(\mathcal{G}\) (Figure 2). Moreover, given a choice of inner product (say, \(\ell^{2}\)) on \(\Omega_{k}\), there is an obvious dual de Rham complex which comes directly from adjointness. In particular, one can define dual derivatives \(d_{k}^{*}:\Omega_{k+1}\rightarrow\Omega_{k}\) via the equality

\[\left\langle d_{k}f,g\right\rangle_{k+1}=\left\langle f,d_{k}^{*}g\right\rangle _{k},\]

from which nontrivial results such as the Hodge decomposition, Poincare inequality, and coercivity/invertibility of the Hodge Laplacian \(\Delta_{k}=d_{k}^{*}d_{k}+d_{k-1}d_{k-1}^{*}\) follow (see e.g. [11]). Using the derivatives \(d_{k},d_{k}^{*}\), it is possible to build compatible discretizations of PDEs on \(\mathcal{G}\) which are guaranteed to preserve exactness properties such as, e.g., \(d_{1}\circ d_{0}=\operatorname{curl}\circ\operatorname{grad}=0\).

The choice of inner product \(\left\langle\cdot,\cdot\right\rangle_{k}\) thus induces a definition of the dual derivatives \(d_{k}^{*}\). In the graph setting [52], one typically selects the \(\ell^{2}\) inner product, obtaining the adjoints of the signed incidence matrices as \(d_{k}^{*}=d_{k}^{T}\). By instead working with the modified inner product \((\mathbf{v},\mathbf{w})=\mathbf{v}^{\intercal}\mathbf{A}_{k}\mathbf{w}\) for a machine-learnable \(\mathbf{A}_{k}\), we obtain \(d_{k}^{*}=\mathbf{A}_{k}^{-1}d_{k}^{\intercal}\mathbf{A}_{k+1}\) (see Appendix A.3). This parameterization

Figure 1: A diagrammatic illustration of the bracket-based architectures introduced in Section 4.

inherits the exact sequence property from the graph topology encoded in \(d_{k}\) while allowing for incorporation of geometric information from data. This leads directly to the following result, which holds for any (potentially feature-dependent) symmetric positive definite matrix \(\mathbf{A}_{k}\).

**Theorem 3.1**.: _The dual derivatives \(d_{k}^{*}:\Omega_{k+1}\to\Omega_{k}\) adjoint to \(d_{k}:\Omega_{k}\to\Omega_{k+1}\) with respect to the learnable inner products \(\mathbf{A}_{k}:\Omega_{k}\to\Omega_{k}\) satisfy an exact sequence property._

Proof.: \[d_{k-1}^{*}d_{k}^{*}=\mathbf{A}_{k-1}^{-1}d_{k-1}^{\intercal}\mathbf{A}_{k} \mathbf{A}_{k}^{-1}d_{k}^{\intercal}\mathbf{A}_{k+1}=\mathbf{A}_{k-1}^{-1} \left(d_{k}d_{k-1}\right)^{\intercal}\mathbf{A}_{k+1}=0.\qed\]

As will be shown in Section 4, by encoding graph attention into the \(\mathbf{A}_{k}\), we may exploit the exact sequence property to obtain symmetric positive definite diffusion operators, as well as conduct the cancellations necessary to enforce degeneracy conditions necessary for metriplectic dynamics.

For a thorough review of DDEC, we direct readers to Appendix A.1 and [11]. For exterior calculus in topological data analysis see [52], and an overview in the context of PDEs see [53; 54].

## 4 Structure-preserving bracket parameterizations

We next summarize properties of the bracket dynamics introduced in Section 3 and displayed in Table 1, postponing details and rigorous discussion to Appendices A.3 and A.6. Letting \(\mathbf{x}=\left(\mathbf{q},\mathbf{p}\right)\) denote node-edge feature pairs, the following operators will be used to generate our brackets.

\[\mathbf{L}=\begin{pmatrix}0&-d_{0}^{*}\\ d_{0}&0\end{pmatrix},\quad\mathbf{G}=\begin{pmatrix}\Delta_{0}&0\\ 0&\Delta_{1}\end{pmatrix}=\begin{pmatrix}d_{0}^{*}d_{0}&0\\ 0&d_{1}^{*}d_{1}+d_{0}d_{0}^{*}\end{pmatrix},\quad\mathbf{M}=\begin{pmatrix}0 &0\\ 0&\mathbf{A}_{1}d_{1}^{*}d_{1}\mathbf{A}_{1}\end{pmatrix}.\]

As mentioned before, the inner products \(\mathbf{A}_{0},\mathbf{A}_{1},\mathbf{A}_{2}\) on \(\Omega_{k}\) which induce the dual derivatives \(d_{0}^{*},d_{1}^{*}\), are chosen in such a way that their combination generalizes a graph attention mechanism. The precise details of this construction are given below, and its relationship to the standard GAT network from [12] is shown in Appendix A.5. Notice that \(\mathbf{L}^{*}=-\mathbf{L}\), while \(\mathbf{G}^{*}=\mathbf{G},\mathbf{M}^{*}=\mathbf{M}\) are positive semi-definite with respect to the block-diagonal inner product \((\cdot,\cdot)\) defined by \(\mathbf{A}=\operatorname{diag}\left(\mathbf{A}_{0},\mathbf{A}_{1}\right)\) (details are provided in Appendix A.6). Therefore, \(\mathbf{L}\) generates purely reversible (Hamiltonian) dynamics and \(\mathbf{G},\mathbf{M}\) generate irreversible (dissipative) ones. Additionally, note that state-dependence in \(\mathbf{L},\mathbf{M},\mathbf{G}\) enters only through the adjoint differential operators, meaning that any structural properties induced by the topology of the graph \(\mathcal{G}\) (such as the exact sequence property mentioned in Theorem 3.1) are automatically preserved.

**Remark 4.1**.: _Strictly speaking, \(\mathbf{L}\) is guaranteed to be a truly Hamiltonian system only when \(d_{0}^{*}\) is state-independent, since it may otherwise fail to satisfy Jacobi's identity. On the other hand, energy conservation is always guaranteed due to the fact that \(\mathbf{L}\) is skew-adjoint._

In addition to the bracket matrices \(\mathbf{L},\mathbf{M},\mathbf{G}\), it is necessary to have access to energy and entropy functions \(E,S\) and their associated functional derivatives with respect to the inner product on \(\Omega_{0}\oplus\Omega_{1}\) defined by \(\mathbf{A}\). For the Hamiltonian, gradient, and double brackets, \(E\) is chosen simply as the "total kinetic energy"

\[E(\mathbf{q},\mathbf{p})=\frac{1}{2}\left(\left|\mathbf{q}\right|^{2}+\left| \mathbf{p}\right|^{2}\right)=\frac{1}{2}\sum_{i\in\mathcal{V}}\left|\mathbf{q }_{i}\right|^{2}+\frac{1}{2}\sum_{\alpha\in\mathcal{E}}\left|\mathbf{p}_{ \alpha}\right|^{2},\]

Figure 2: A commutative diagram illustrating the relationship between the graph derivatives \(d_{k}\), their \(\ell^{2}\) adjoints \(d_{k}^{\intercal}\), and the learnable adjoints \(d_{k}^{*}\). These operators form a _de Rham complex_ due to the exact sequence property \(d_{i+1}\circ d_{i}=d_{i}^{\intercal}\circ d_{i+1}^{*}=d_{i}^{*}\circ d_{i+1}^ {*}=0\). We show that the learnable \(\mathbf{A}_{k}\) may encode attention mechanisms, without impacting the preservation of exact sequence structure.

whose \(\mathbf{A}\)-gradient (computed in Appendix A.6) is just \(\nabla E(\mathbf{q},\mathbf{p})=\left(\mathbf{A}_{0}^{-1}\mathbf{q}\quad\mathbf{A }_{1}^{-1}\mathbf{p}\right)^{\intercal}\). Since the metriplectic bracket uses parameterizations of \(E,S\) which are more involved, discussion of this case is deferred to later in this Section.

**Attention as learnable inner product:** Before describing the dynamics, it remains to discuss how the matrices \(\mathbf{A}_{i}\), \(0\leq i\leq 2\), are computed in practice, and how they relate to the idea of graph attention. Recall that if \(n_{V}>0\) denotes the nodal feature dimension, a graph attention mechanism takes the form \(a(\mathbf{q}_{i},\mathbf{q}_{j})=f\left(\tilde{a}_{ij}\right)/\sum_{j}f\left( \tilde{a}_{ij}\right)\) for some differentiable pre-attention function \(\tilde{a}:n_{V}\times n_{V}\rightarrow\mathbb{R}\) (e.g., for scaled dot product [55]) one typically represents \(a(\mathbf{q}_{i},\mathbf{q}_{j})\) as a softmax, so that \(f=\exp(\mathbf{q})\)). This suggests a decomposition \(a(\mathbf{q}_{i},\mathbf{q}_{j})=\mathbf{A}_{0}^{-1}\mathbf{A}_{1}\) where \(\mathbf{A}_{0}=(a_{0,ii})\) is diagonal on nodes and \(\mathbf{A}_{1}=(a_{1,ij})\) is diagonal on edges,

\[a_{0,ii}=\sum_{j\in\mathcal{N}(i)}f\left(\tilde{a}\left(\mathbf{q}_{i}, \mathbf{q}_{j}\right)\right),\qquad a_{1,ij}=f\left(\tilde{a}\left(\mathbf{q} _{i},\mathbf{q}_{j}\right)\right).\]

Treating the numerator and denominator of the standard attention mechanism separately in \(\mathbf{A}_{0},\mathbf{A}_{1}\) allows for a flexible and theoretically sound incorporation of graph attention directly into the adjoint differential operators on \(\mathcal{G}\). In particular, _if \(\mathbf{A}_{1}\) is symmetric_ with respect to edge-orientation and \(\mathbf{p}\) is an edge feature which is antisymmetric, it follows that

\[\left(d_{0}^{*}\mathbf{p}\right)_{i}=\left(\mathbf{A}_{0}^{-1}d_{0}^{\intercal }\mathbf{A}_{1}\mathbf{p}\right)_{i}=\sum_{j\in\mathcal{N}(i)}a\left(\mathbf{ q}_{i},\mathbf{q}_{j}\right)\mathbf{p}_{ji},\]

which is just graph attention combined with edge aggregation. This makes it possible to give the following informal statement regarding graph attention networks which is explained and proven in Appendix A.5.

**Remark 4.2**.: _The GAT layer from [12] is almost the forward Euler discretization of a metric heat equation._

The "almost" appearing here has to do with the fact that (1) the attentional numerator \(f\left(\tilde{a}(\mathbf{q}_{i},\mathbf{q}_{j})\right)\) is generally asymmetric in \(i,j\), and is therefore symmetrized by the divergence operator \(d_{0}^{\intercal}\), (2) the activation function between layers is not included, and (3) learnable weight matrices \(\mathbf{W}^{k}\) in GAT are set to the identity.

**Remark 4.3**.: _The interpretation of graph attention as a combination of learnable inner products admits a direct generalization to higher-order cliques, which is discussed in Appendix A.4._

**Hamiltonian case:** A purely conservative system is generated by solving \(\dot{\mathbf{x}}=\mathbf{L}(\mathbf{x})\nabla E(\mathbf{x})\), or

\[\begin{pmatrix}\dot{\mathbf{q}}\\ \dot{\mathbf{p}}\end{pmatrix}=\begin{pmatrix}0&-d_{0}^{*}\\ d_{0}&0\end{pmatrix}\begin{pmatrix}\mathbf{A}_{0}^{-1}&0\\ 0&\mathbf{A}_{1}^{-1}\end{pmatrix}\begin{pmatrix}\mathbf{q}\\ \mathbf{p}\end{pmatrix}=\begin{pmatrix}-d_{0}^{*}\mathbf{A}_{1}^{-1}\mathbf{p} \\ d_{0}\mathbf{A}_{0}^{-1}\mathbf{q}\end{pmatrix}.\]

This is a noncanonical Hamiltonian system which generates a purely reversible flow. In particular, it can be shown that

\[\dot{E}(\mathbf{x})=\left(\dot{\mathbf{x}},\nabla E(\mathbf{x})\right)\,= \left(\mathbf{L}(\mathbf{x})\nabla E(\mathbf{x}),\nabla E(\mathbf{x})\right)\, =-\left(\nabla E(\mathbf{x}),\mathbf{L}(\mathbf{x})\nabla E(\mathbf{x}) \right)\,=0,\]

so that energy is conserved due to the skew-adjointness of \(\mathbf{L}\).

**Gradient case:** On the opposite end of the spectrum are generalized gradient flows, which are totally dissipative. Consider solving \(\dot{\mathbf{x}}=-\mathbf{G}(\mathbf{x})\nabla E(\mathbf{x})\), or

\[\begin{pmatrix}\dot{\mathbf{q}}\\ \dot{\mathbf{p}}\end{pmatrix}=-\begin{pmatrix}\Delta_{0}&0\\ 0&\Delta_{1}\end{pmatrix}\begin{pmatrix}\mathbf{A}_{0}^{-1}&0\\ 0&\mathbf{A}_{1}^{-1}\end{pmatrix}\begin{pmatrix}\mathbf{q}\\ \mathbf{p}\end{pmatrix}=-\begin{pmatrix}\Delta_{0}\mathbf{A}_{0}^{-1}\mathbf{q }\\ \Delta_{1}\mathbf{A}_{1}^{-1}\mathbf{p}\end{pmatrix}.\]

This system is a metric diffusion process on nodes and edges separately. Moreover, it corresponds to a generalized gradient flow, since

\[\dot{E}(\mathbf{x})=\left(\dot{\mathbf{x}},\nabla E(\mathbf{x})\right)\,=- \left(\mathbf{G}(\mathbf{x})\nabla E(\mathbf{x}),\nabla E(\mathbf{x})\right)\, =-\left|\nabla E(\mathbf{x})\right|_{\mathbf{G}}^{2}\leq 0,\]

due to the self-adjoint and positive semi-definite nature of \(\mathbf{G}\).

**Remark 4.4**.: _The architecture in GRAND [49] is almost a gradient flow, however the pre-attention mechanism lacks the requisite symmetry to formally induce a valid inner product._

[MISSING_PAGE_FAIL:7]

Since this system is metriplectic when expressed in position-momentum-entropy coordinates (c.f. [56]), it is useful to see if any of the brackets from Section 4 can adequately capture these dynamics without an entropic variable. The results of applying the architectures of Section 4 to reproduce a trajectory of five periods are displayed in Table 2, alongside comparisons with a black-box NODE network and a latent NODE with feature encoder/decoder. While each network is capable of producing a small mean absolute error, it is clear that the metriplectic and Hamiltonian networks produce the most accurate trajectories. It is remarkable both that the Hamiltonian bracket does so well here and that the gradient bracket does so poorly, being that the damped double pendulum system is quite dissipative. On the other hand, it is unlikely to be only the feature encoder/decoder leading to good performance here, as both the NODE and NODE+AE architectures perform worse on this task by about one order of magnitude.

### MuJoCo Dynamics

Next we test the proposed models on more complex physical systems that are generated by the Multi-Joint dynamics with Contact (MuJoCo) physics simulator [57]. We consider the modified versions of Open AI Gym environments [23]: HalfCheetah, Hopper, and Swimmer.

We represent an object in an environment as a fully-connected graph, where a node corresponds to a body part of the object and, thus, the nodal feature \(\mathbf{q}_{i}\) corresponds to a position of a body part or an angle of a joint.3 As the edge features, a pair of nodal velocities \(\mathbf{p}_{\alpha}=(v_{\text{src}(\alpha)},v_{\text{dst}(\alpha)})\) are provided, where \(v_{\text{src}(\alpha)}\) and \(v_{\text{dst}(\alpha)}\) denote velocities of the source and destination nodes connected to the edge.

Footnote 3: Results of an experiment with an alternative embedding (i.e., \(\mathbf{q}_{i}=(q_{i},v_{i})\)) are reported in Appendix B.2.2.

Since the MuJoCo environments contain an actor applying controls, additional control input is accounted for with an additive forcing term which is parameterized by a multi-layer perceptron and introduced into the bracket-based dynamics models. See Appendix B.2 for additional experimental details. The problem therefore consists of finding an optimal control MLP, and we evaluate the improvement which comes from representing the physics surrogate with bracket dynamics over NODE.

All models are trained via minimizing the MSE between the predicted positions \(\tilde{\mathbf{q}}\) and the ground truth positions \(\mathbf{q}\) and are tested on an unseen test set. Table 3 reports the errors of network predictions on the test set measured in the relative \(\ell_{2}\) norm, \(\|\mathbf{q}-\tilde{\mathbf{q}}\|_{2}/\|\mathbf{q}\|_{2}\|\tilde{\mathbf{q}}\| _{2}\). Similar to the double pendulum experiments, all models are able to produce accurate predictions with around or less than \(10\%\) errors. While the gradient bracket makes little to no improvements over NODEs, the Hamiltonian, double, and metriplectic brackets produce more accurate predictions. Interestingly, the Hamiltonian bracket performs the best in this case as well, meaning that any dissipation present is effectively compensated for by the autoencoder which transforms the features.

### Node classification

Moving beyond physics-based examples, it remains to see how bracket-based architectures perform on "black-box" node classification problems. Table 4 and Table 5 present results on common benchmark problems including the citation networks Cora [58], Citeseer [59], and Pubmed [60], as well as the coauthor graph, CoauthorCS [61], and the Amazon co-purchasing graphs, Computer and Photo [62]. For comparison, we report results on a standard GAT [12], a neural graph differential equation

\begin{table}
\begin{tabular}{l l l l} \hline \hline Double pendulum & **MAE \(\mathbf{q}\)** & **MAE \(\mathbf{p}\)** & **Total MAE** \\
**NODE** & \(0.0240\pm 0.015\) & \(0.0299\pm 0.0091\) & \(0.0269\pm 0.012\) \\
**NODE+AE** & \(0.0532\pm 0.029\) & \(0.0671\pm 0.043\) & \(0.0602\pm 0.035\) \\
**Hamiltonian** & \(0.00368\pm 0.0015\) & \(0.00402\pm 0.0015\) & \(0.00369\pm 0.0013\) \\
**Gradient** & \(0.00762\pm 0.0023\) & \(0.0339\pm 0.012\) & \(0.0208\pm 0.0067\) \\
**Double Bracket** & \(0.00584\pm 0.0013\) & \(0.0183\pm 0.0071\) & \(0.0120\pm 0.0037\) \\
**Metriplectic** & \(0.00364\pm 0.00064\) & \(0.00553\pm 0.00029\) & \(0.00459\pm 0.00020\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Mean absolute errors (MAEs) of the network predictions in the damped double pendulum case, reported as avg\(\pm\)stdev over 5 runs.

architecture (GDE) [63], and the nonlinear GRAND architecture (GRAND-nl) from [49] which is closest to ours. Since our experimental setting is similar to that of [49], the numbers reported for GAT, GDE, and GRAND-nl are taken directly from this paper. Note that, despite the similar \(O(N)\) scaling in the metriplectic architecture, the high dimension of the node and edge features on the latter three datasets led to trainable \(E,S\) functions which exhausted the memory on our available machines, and therefore results are not reported for these cases. A full description of experimental details is provided in Appendix B.3.

**Remark 5.1**.: _To highlight the effect of bracket structure on network performance, only minimal modifications are employed during network training. In particular, we do not include any additional regularization, positional encoding, graph rewiring, extraction of connected components, extra terms on the right-hand side, or early stopping. While it is likely that better classification performance could be achieved with some of these modifications included, it becomes very difficult to isolate the effect of structure-preservation. A complete list of tunable hyperparameters is given in Appendix B.3._

The results show different behavior produced by each bracket architecture. It is empirically clear that there is some value in full or partial reversibility, since the Hamiltonian and double bracket architectures both perform better than the corresponding gradient architecture on datasets such as Computer and Photo. Moreover, it appears that the partially reversible double bracket performs the best of the bracket architectures in every case, which is consistent with the idea that both reversible and irreversible dynamics are critical for capturing the behavior of general dynamical systems. Interestingly, the metriplectic bracket performs worse on these tasks by a large margin. We conjecture this architecture may be harder to train for larger problems despite its \(O(N)\) complexity in the graph size, suggesting that more sophisticated training strategies may be required for large problems.

## 6 Conclusion

This work presents a unified theoretical framework for analysis and construction of graph attention networks. The exact sequence property of graph derivatives and coercivity of Hodge Laplacians which follow from the theory allow the construction of four structure-preserving brackets, which we use to evaluate the role of irreversibility in both data-driven physics simulators and graph analytics problems. In all contexts, the pure diffusion bracket performed most poorly, with mixed results between purely reversible and partially dissipative brackets.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Planetoid splits & **CORA** & **CiteSeer** & **PubMed** \\
**GAT** & \(82.8\pm 0.5\) & \(69.5\pm 0.9\) & \(79.0\pm 0.5\) \\
**GDE** & \(83.8\pm 0.5\) & \(72.5\pm 0.5\) & \(79.9\pm 0.3\) \\
**GRAND-nl** & \(83.6\pm 0.5\) & \(70.8\pm 1.1\) & \(79.7\pm 0.3\) \\ \hline
**Hamiltonian** & \(77.2\pm 0.7\) & \(73.0\pm 1.2\) & \(78.5\pm 0.3\) \\
**Gradient** & \(79.9\pm 0.7\) & \(71.8\pm 1.4\) & \(78.6\pm 0.7\) \\
**Double Bracket** & \(82.6\pm 0.9\) & \(74.2\pm 1.4\) & \(79.6\pm 0.6\) \\
**Metriplectic** & \(57.4\pm 1.0\) & \(60.5\pm 1.1\) & \(69.8\pm 0.7\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Test accuracy and standard deviations (averaged over 20 randomly initialized runs) using the original Planetoid train-valid-test splits. Comparisons use the numbers reported in [49].

\begin{table}
\begin{tabular}{l l l l} \hline \hline Dataset & **HalfCheetah** & **Hopper** & **Swimmer** \\
**NODE+AE** & \(0.106\pm 0.0011\) & \(0.0780\pm 0.0021\) & \(0.0297\pm 0.0036\) \\
**Hamiltonian** & \(0.0566\pm 0.013\) & \(0.0279\pm 0.0019\) & \(0.0122\pm 0.00044\) \\
**Gradient** & \(0.105\pm 0.0076\) & \(0.0848\pm 0.0011\) & \(0.0290\pm 0.0011\) \\
**Double Bracket** & \(0.0621\pm 0.0096\) & \(0.0297\pm 0.0048\) & \(0.0128\pm 0.00070\) \\
**Metriplectic** & \(0.105\pm 0.0091\) & \(0.0398\pm 0.0057\) & \(0.0179\pm 0.00059\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Relative error of network predictions for the MuJoCo environment on the test set, reported as avg\(\pm\)stdev over 4 runs.

The linear scaling achieved by the metriplectic brackets has a potential major impact for data-driven physics modeling. Metriplectic systems emerge naturally when coarse-graining multiscale systems. With increasing interest in using ML to construct digital twins, fast data-driven surrogates for complex multi-physics acting over multiple scales will become crucial. In this setting the stability encoded by metriplectic dynamics translates to robust surrogates, with linear complexity suggesting the possibility of scaling up to millions of degrees of freedom.

Limitations:All analysis holds under the assumption of modified attention mechanisms which allow interpretation of GAT networks as diffusion processes; readers should take care that the analysis is for a non-standard attention. Secondly, for all brackets we did not introduce empirical modifications (e.g. regularization, forcing, etc) to optimize performance so that we could study the role of (ir)reversibility in isolation. With this in mind, one may be able to add "tricks" to e.g. obtain a diffusion architecture which outperforms those presented here. Finally, note that the use of a feature autoencoder in the bracket architectures means that structure is enforced in the transformed space. This allows for applicability to more general systems, and can be easily removed when appropriate features are known.

Broader impacts:The work performed here is strictly foundational mathematics and is intended to improve the performance of GNNs in the context of graph analysis and data-driven physics modeling. Subsequent application of the theory may have societal impact, but the current work anticipated to improve the performance of machine learning in graph settings only at a foundational level.

## References

* [1] Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In _Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005._, volume 2, pages 729-734. IEEE, 2005.
* [2] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. _AI open_, 1:57-81, 2020.
* [3] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* [4] William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and applications. _arXiv preprint arXiv:1709.05584_, 2017.
* [5] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 3438-3445, 2020.
* [6] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv preprint arXiv:2104.13478_, 2021.
* [7] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. _Advances in neural information processing systems_, 33:4917-4928, 2020.

\begin{table}
\begin{tabular}{l c c c c c c} \hline Random splits & **CORA** & **CiteSeer** & **PubMed** & **Coauthor CS** & **Computer** & **Photo** \\
**GAT** & \(81.8\pm 1.3\) & \(71.4\pm 1.9\) & \(78.7\pm 2.3\) & \(90.5\pm 0.6\) & \(78.0\pm 19.0\) & \(85.7\pm 20.3\) \\
**GDE** & \(78.7\pm 2.2\) & \(71.8\pm 1.1\) & \(73.9\pm 3.7\) & \(91.6\pm 0.1\) & \(82.9\pm 0.6\) & \(92.4\pm 2.0\) \\
**GRAND-nl** & \(82.3\pm 1.6\) & \(70.9\pm 1.0\) & \(77.5\pm 1.8\) & \(92.4\pm 0.3\) & \(82.4\pm 2.1\) & \(92.4\pm 0.8\) \\ \hline
**Hamiltonian** & \(76.2\pm 2.1\) & \(72.2\pm 1.9\) & \(76.8\pm 1.1\) & \(92.0\pm 0.2\) & \(84.0\pm 1.0\) & \(91.8\pm 0.2\) \\
**Gradient** & \(81.3\pm 1.2\) & \(72.1\pm 1.7\) & \(77.2\pm 2.1\) & \(92.2\pm 0.3\) & \(78.1\pm 1.2\) & \(88.2\pm 0.6\) \\
**Double Bracket** & \(83.0\pm 1.1\) & \(74.2\pm 2.5\) & \(78.2\pm 2.0\) & \(92.5\pm 0.2\) & \(84.8\pm 0.5\) & \(92.4\pm 0.3\) \\
**Metriplectic** & \(59.6\pm 2.0\) & \(63.1\pm 2.4\) & \(69.8\pm 2.1\) & - & - & - \\ \hline \end{tabular}
\end{table}
Table 5: Test accuracy and standard deviations averaged over 20 runs with random 80/10/10 train/val/test splits. Comparisons use the numbers reported in [49].

* [8] Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. _arXiv preprint arXiv:2006.13318_, 2020.
* [9] Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Dissecting the diffusion process in linear graph convolutional networks. _Advances in Neural Information Processing Systems_, 34:5758-5769, 2021.
* [10] Francesco Di Giovanni, James Rowbottom, Benjamin P. Chamberlain, Thomas Markovich, and Michael M. Bronstein. Understanding convolution on graphs via energies, 2023.
* [11] Nathaniel Trask, Andy Huang, and Xiaozhe Hu. Enforcing exact physics in scientific machine learning: a data-driven exterior calculus on graphs. _arXiv preprint arXiv:2012.11799_, 2020.
* [12] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. _arXiv preprint arXiv:1710.10903_, 2017.
* [13] Khemraj Shukla, Mengjia Xu, Nathaniel Trask, and George E Karniadakis. Scalable algorithms for physics-informed neural and graph networks. _Data-Centric Engineering_, 3:e24, 2022.
* [14] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations for scientific machine learning. _arXiv preprint arXiv:2001.04385_, 2020.
* [15] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. _Proceedings of the national academy of sciences_, 113(15):3932-3937, 2016.
* [16] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, pages 6572-6583, 2018.
* [17] Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama, Jinkyoo Park, and Stefano Ermon. Torchdyn: Implicit models and neural numerical methods in pytorch.
* [18] Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In _International Conference on Machine Learning_, pages 10432-10441. PMLR, 2020.
* [19] Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El Ghaoui. Implicit graph neural networks. _Advances in Neural Information Processing Systems_, 33:11984-11995, 2020.
* [20] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [21] Marc Finzi, Ke Alexander Wang, and Andrew G Wilson. Simplifying hamiltonian and lagrangian neural networks via explicit constraints. _Advances in neural information processing systems_, 33:13880-13889, 2020.
* [22] Renyi Chen and Molei Tao. Data-driven prediction of general hamiltonian dynamics via learning exactly-symplectic maps. In _International Conference on Machine Learning_, pages 1717-1727. PMLR, 2021.
* [23] Nate Gruver, Marc Anton Finzi, Samuel Don Stanton, and Andrew Gordon Wilson. Deconstructing the inductive biases of hamiltonian neural networks. In _International Conference on Learning Representations_.
* [24] Peter Toth, Danilo J Rezende, Andrew Jaegle, Sebastien Racaniere, Aleksandar Botev, and Irina Higgins. Hamiltonian generative networks. In _International Conference on Learning Representations_, 2019.
* [25] Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ODE-Net: Learning Hamiltonian dynamics with control. In _International Conference on Learning Representations_.
* [26] Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as model prior for deep learning. In _International Conference on Learning Representations_, 2018.
* [27] Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho. Lagrangian neural networks. In _ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations_, 2020.
* [28] Partha Guha. Metriplectic structure, leibniz dynamics and dissipative systems. _Journal of Mathematical Analysis and Applications_, 326(1):121-136, 2007.

* [29] Kookjin Lee, Nathaniel Trask, and Panos Stinis. Machine learning structure preserving brackets for forecasting irreversible processes. _Advances in Neural Information Processing Systems_, 34:5696-5707, 2021.
* [30] Kookjin Lee, Nathaniel Trask, and Panos Stinis. Structure-preserving sparse identification of nonlinear dynamics for data-driven modeling. In _Mathematical and Scientific Machine Learning_, pages 65-80. PMLR, 2022.
* [31] Zhen Zhang, Yeonjong Shin, and George Em Karniadakis. Gfinns: Generic formalism informed neural networks for deterministic and stochastic dynamical systems. _Philosophical Transactions of the Royal Society A_, 380(2229):20210207, 2022.
* [32] Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Dissipative symoden: Encoding hamiltonian dynamics with dissipation and control into deep learning. In _ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations_.
* [33] Shaan A Desai, Marios Mattheakis, David Sondak, Pavlos Protopapas, and Stephen J Roberts. Port-hamiltonian neural networks for learning explicit time-dependent dynamical systems. _Physical Review E_, 104(3):034312, 2021.
* [34] Miroslav Grmela. Generic guide to the multiscale dynamics and thermodynamics. _Journal of Physics Communications_, 2(3):032001, 2018.
* [35] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational physics_, 378:686-707, 2019.
* [36] Maziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of nonlinear partial differential equations. _Journal of Computational Physics_, 357:125-141, 2018.
* [37] Filippo Masi and Ioannis Stefanou. Multiscale modeling of inelastic materials with thermodynamics-based artificial neural networks (tann). _Computer Methods in Applied Mechanics and Engineering_, 398:115190, 2022.
* [38] Ravi G Patel, Indu Manickam, Nathaniel A Trask, Mitchell A Wood, Myoungkyu Lee, Ignacio Tomas, and Eric C Cyr. Thermodynamically consistent physics-informed neural networks for hyperbolic systems. _Journal of Computational Physics_, 449:110754, 2022.
* [39] Quercus Hernandez, Alberto Badias, David Gonzalez, Francisco Chinesta, and Elias Cueto. Structure-preserving neural networks. _Journal of Computational Physics_, 426:109950, 2021.
* [40] Yibo Yang and Paris Perdikaris. Adversarial uncertainty quantification in physics-informed neural networks. _Journal of Computational Physics_, 394:136-152, 2019.
* [41] Dongkun Zhang, Lu Lu, Ling Guo, and George Em Karniadakis. Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems. _Journal of Computational Physics_, 397:108850, 2019.
* [42] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. _SIAM Journal on Scientific Computing_, 43(5):A3055-A3081, 2021.
* [43] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. _Journal of Computational Physics_, 449:110768, 2022.
* [44] T Konstantin Rusch, Ben Chamberlain, James Rowbottom, Siddhartha Mishra, and Michael Bronstein. Graph-coupled oscillator networks. In _International Conference on Machine Learning_, pages 18888-18909. PMLR, 2022.
* [45] Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. Gread: Graph neural reaction-diffusion equations. _arXiv preprint arXiv:2211.14208_, 2022.
* [46] Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia. Hamiltonian graph networks with ode integrators. _arXiv preprint arXiv:1909.12790_, 2019.
* [47] Suresh Bishnoi, Ravinder Bhattoo, Jayadeva Jayadeva, Sayan Ranu, and NM Anoop Krishnan. Enhancing the inductive biases of graph neural ode for modeling physical systems. In _The Eleventh International Conference on Learning Representations_.

* [48] Quercus Hernandez, Alberto Badias, Francisco Chinesta, and Elias Cueto. Thermodynamics-informed graph neural networks. _IEEE Transactions on Artificial Intelligence_, 1(01):1-1, 2022.
* [49] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele Rossi. Grand: Graph neural diffusion. In _International Conference on Machine Learning_, pages 1407-1418. PMLR, 2021.
* [50] PJ Morrison. Thoughts on brackets and dissipation: old and new. In _Journal of Physics: Conference Series_, volume 169, page 012006. IOP Publishing, 2009.
* [51] Oliver Knill. The dirac operator of a graph, 2013.
* [52] Xiaoye Jiang, Lek-Heng Lim, Yuan Yao, and Yinyu Ye. Statistical ranking and combinatorial hodge theory. _Mathematical Programming_, 127(1):203-244, 2011.
* [53] Pavel B Bochev and James M Hyman. Principles of mimetic discretizations of differential operators. In _Compatible spatial discretizations_, pages 89-119. Springer, 2006.
* [54] Douglas N Arnold. _Finite element exterior calculus_. SIAM, 2018.
* [55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [56] Ignacio Romero. Thermodynamically consistent time-stepping algorithms for non-linear thermomechanical systems. _International Journal for Numerical Methods in Engineering_, 79(6):706-732, 2023/05/14 2009.
* [57] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ international conference on intelligent robots and systems_, pages 5026-5033. IEEE, 2012.
* [58] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. _Information Retrieval_, 3:127-163, 2000.
* [59] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. _AI magazine_, 29(3):93-93, 2008.
* [60] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying for collective classification. In _10th international workshop on mining and learning with graphs_, volume 8, page 1, 2012.
* [61] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls of graph neural network evaluation. _ArXiv_, abs/1811.05868, 2018.
* [62] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In _Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval_, pages 43-52, 2015.
* [63] Michael Poli, Stefano Massaroli, Junyoung Park, Atsushi Yamashita, Hajime Asama, and Jinkyoo Park. Graph neural ordinary differential equations. _arXiv preprint arXiv:1911.07532_, 2019.
* [64] Xiaoye Jiang, Lek-Heng Lim, Yuan Yao, and Yinyu Ye. Statistical ranking and combinatorial hodge theory. _Mathematical Programming_, 127(1):203-244, nov 2010.
* [65] Anthony Bloch, P. S. Krishnaprasad, Jerrold E. Marsden, and Tudor S. Ratiu. The euler-poincare equations and double bracket dissipation. _Communications in Mathematical Physics_, 175(1):1-42, 1996.
* [66] Darryl D Holm, Jerrold E Marsden, and Tudor S Ratiu. The euler-poincare equations and semidirect products with applications to continuum theories. _Advances in Mathematics_, 137(1):1-81, 1998.
* [67] Hans Christian Oettinger. Irreversible dynamics, onsager-casimir symmetry, and an application to turbulence. _Physical Review E_, 90(4):042121, 2014.
* [68] Anthony Gruber, Max Gunzburger, Lili Ju, and Zhu Wang. Energetically consistent model reduction for metriplectic systems. _Computer Methods in Applied Mechanics and Engineering_, 404:115709, 2023.
* [69] Robert J. Renka. A simple explanation of the sobolev gradient method. 2006.
* [70] Chris Yu, Henrik Schumacher, and Keenan Crane. Repulsive curves. _ACM Trans. Graph._, 40(2), may 2021.

* [71] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [72] Joe Chen. Chaos from simplicity: an introduction to the double pendulum. Technical report, University of Canterbury, 2008.
* [73] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [74] Romeo Ortega, Arjan Van Der Schaft, Bernhard Maschke, and Gerardo Escobar. Interconnection and damping assignment passivity-based control of port-controlled hamiltonian systems. _Automatica_, 38(4):585-596, 2002.
* [75] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. _arXiv preprint arXiv:1606.01540_, 2016.
* [76] Lukas Biewald. Experiment tracking with weights and biases, 2020. Software available from wandb.com.

### Glossary of Notation and Symbols

The next list describes several symbols that will be later used within the body of the document

\[\{\{\cdot,\cdot\}\}\] (Irreversible) double bracket on functions with generator \[\mathbf{L}^{2}\] \[\cdot,\cdot]\] Degenerate (irreversible) metric bracket on functions with generator \[\mathbf{M}^{\intercal}=\mathbf{M}\] \[\{\cdot,\cdot\}\] Poisson (reversible) bracket on functions with generator \[\mathbf{L}^{\intercal}=-\mathbf{L}\] \[\mathcal{N}(i),\overline{\mathcal{N}}(i)\] Neighbors of node \[i\in\mathcal{V}\], neighbors of node \[i\in\mathcal{V}\] including \[i\] \[[S]\] Indicator function of the statement \[S\] \[\delta f,\nabla f\] Adjoint of \[df\] with respect to \[\langle\cdot,\cdot\rangle\], adjoint of \[df\] with respect to \[\langle\cdot,\cdot\rangle\] \[\Delta_{k}\] Hodge Laplacian \[d_{k}d_{k}^{*}+d_{k}^{*}d_{k}\] \[\delta_{ij}\] Kronecker delta \[\dot{f}\] Derivative of \[f\] with respect to time \[(\cdot,\cdot)_{k}\] Learnable metric inner product on \[k\] -cliques with matrix representation \[\mathbf{A}_{k}\] \[\langle\cdot,\cdot\rangle_{k}\] Euclidean \[\ell^{2}\] inner product on \[k\] -cliques \[\mathcal{G},\mathcal{V},\mathcal{E}\] Oriented graph, set of nodes, set of edges \[\mathcal{G}_{k},\Omega_{k}\] Set of \[k\] -cliques, vector space of real-valued functions on \[k\] -cliques \[d,d_{k}\] Exterior derivative operator on functions, exterior derivative operator on \[k\] -cliques \[d_{k}^{\intercal},d_{k}^{*}\] Adjoint of \[d_{k}\] with respect to \[\langle\cdot,\cdot\rangle_{k}\], adjoint of \[d_{k}\] with respect to \[\left(\cdot,\cdot\right)_{k}\]

## Appendix A Mathematical foundations

This Appendix provides the following: (1) an introduction to the ideas of graph exterior calculus, A.1, and bracket-based dynamical systems, A.2, necessary for understanding the results in the body, (2) additional explanation regarding adjoints with respect to generic inner products and associated computations, A.3, (3) a mechanism for higher-order attention expressed in terms of learnable inner products, A.4, (4) a discussion of GATs in the context of exterior calculus, A.5, and (5) proofs which are deferred from Section 4, A.6.

### Graph exterior calculus

Here some basic notions from the graph exterior calculus are recalled. More details can be found in, e.g., [11, 51, 64].

As mentioned in Section 3, an oriented graph \(\mathcal{G}=\{\mathcal{V},\mathcal{E}\}\) carries sets of \(k\)_-cliques_, denoted \(\mathcal{G}_{k}\), which are collections of ordered subgraphs generated by \((k+1)\) nodes. For example, the graph in Figure 3 contains six 0-cliques (nodes), six 1-cliques (edges), and one 2-clique. A notion of combinatorial derivative is then given by the _signed incidence matrices_\(d_{k}:\Omega_{k}\rightarrow\Omega_{k+1}\), operating on the space \(\Omega_{k}\) of differentiable functions on \(k\)-cliques, whose entries \(\left(d_{k}\right)_{ij}\) are 1 or -1 if the \(j^{\text{th}}\)\(k\)-clique is

Figure 3: A toy graph with six 0-cliques (nodes), six 1-cliques (edges), and one 2-clique.

incident on the \(i^{\mathrm{th}}\)\((k+1)\)-clique, and zero otherwise. For the example in Figure 3, these are:

\[d_{0}=\begin{pmatrix}-1&1&0&0&0&0\\ 0&-1&1&0&0&0\\ 0&-1&0&1&0&0\\ 0&0&0&-1&1&0\\ 0&1&0&0&-1&0\\ 0&0&0&0&-1&1\end{pmatrix},\qquad d_{1}=(0\quad 0\quad 1\quad 1\quad 1\quad 1 \quad 0)\,.\]

**Remark A.1**.: _While the one-hop neighborhood of node \(i\) in \(\mathcal{G}\), denoted \(\mathcal{N}(i)\), does not include node \(i\) itself, many machine learning algorithms employ the extended neighborhood \(\overline{\mathcal{N}}(i)=\mathcal{N}(i)\cup\{i\}\). Since this is equivalent to considering the one-hop neighborhood of node \(i\) in the self-looped graph \(\overline{\mathcal{G}}\), this modification does not change the analysis of functions on graphs._

It can be shown that the action of these matrices can be conveniently expressed in terms of totally antisymmetric functions \(f\in\Omega_{k}\), via the expression

\[\left(d_{k}f\right)(i_{0},i_{1},...,i_{k+1})=\sum_{j=0}^{k+1}(-1)^{j}f\left(i_ {0},...,\widehat{i_{j}},...,i_{k+1}\right),\]

where \((i_{0},...,i_{k+1})\) denotes a \((k+1)\)-clique of vertices \(v\in\mathcal{V}\). As convenient shorthand, we often write subscripts, e.g., \(\left(d_{k}f\right)_{i_{0}i_{1}...i_{k+1}}\), instead of explicit function arguments. Using \([S]\) to denote the indicator function of the statement \(S\), it is straightforward to check that \(d\circ d=0\),

\[\left(d_{k}d_{k-1}f\right)_{i_{0},...,i_{k+1}} =\sum_{j=0}^{k+1}\left(-1\right)^{j}\left(d_{k-1}f\right)_{i_{0},...,\widehat{i_{j}},...,i_{k+1}}\] \[=\sum_{j=0}^{k+1}\sum_{l=0}^{k+1}[l<j]\left(-1\right)^{j+l}f_{i_ {0}...\widehat{i_{l}}...\widehat{i_{j}}...i_{k+1}}\] \[\qquad+\sum_{j=0}^{k+1}\sum_{l=0}^{k+1}[l>j]\left(-1\right)^{j+l -1}f_{i_{0}...\widehat{i_{j}}...\widehat{i_{l}}...i_{k+1}}\] \[=\sum_{l<j}\left(-1\right)^{j+l}f_{i_{0}...\widehat{i_{l}}... \widehat{i_{j}}...i_{k+1}}\] \[\qquad-\sum_{l<j}\left(-1\right)^{j+l}f_{i_{0}...\widehat{i_{l} }...\widehat{i_{j}}...i_{k+1}}=0,\]

since \((-1)^{j+l-1}=(-1)^{-1}(-1)^{j+l}=(-1)(-1)^{j+l}\) and the final sum follows from swapping the labels \(j,l\). This shows that the \(k\)-cliques on \(\mathcal{G}\) form a _de Rham complex_[53]: a collection of function spaces \(\Omega_{k}\) equipped with mappings \(d_{k}\) satisfying \(\mathrm{Im}\,d_{k-1}\subset\mathrm{Ker}\,d_{k}\) as shown in Figure 4.

When \(K=3\), this is precisely the graph calculus analogue of the de Rham complex on \(\mathbb{R}^{3}\) formed by the Sobolev spaces \(H^{1},H(\mathrm{curl}),H(\mathrm{div}),L^{2}\) which satisfies \(\mathrm{div}\circ\mathrm{curl}=\mathrm{curl}\circ\mathrm{grad}=0\).

While the construction of the graph derivatives and their associated de Rham complex is purely topological, building elliptic differential operators such as the Laplacian relies on a dual de Rham complex, which is specified by an inner product on \(\Omega_{k}\). In the case of \(\ell^{2}\), this leads to dual derivatives which are the matrix transposes of the \(d_{k}\) having the following explicit expression.

**Proposition A.1**.: _The dual derivatives \(d_{k}^{\mathrm{T}}:\Omega_{k+1}\rightarrow\Omega_{k}\) adjoint to \(d_{k}\) through the \(\ell^{2}\) inner product are given by_

\[\left(d_{k}^{\mathrm{T}}f\right)(i_{0},i_{1},...,i_{k})=\frac{1}{k+2}\sum_{i_{ k+1}}\sum_{j=0}^{k+1}f\left(i_{0},...,[i_{j},...,i_{k+1}]\right),\]

_where \([i_{j},...,i_{k+1}]=i_{k+1},i_{j},...,i_{k}\) indicates a cyclic permutation forward by one index._

Figure 4: Illustration of the de Rham complex on \(\mathcal{G}\) induced by the combinatorial derivatives, where \(K>0\) is the maximal clique degree.

Proof.: This is a direct calculation using the representation of \(d_{k}\) in terms of antisymmetric functions. More precisely, let an empty sum \(\Sigma\) denote summation over all unspecified indices. Then, for any \(g\in\Omega_{k}\),

\[\langle d_{k}f,g\rangle =\sum_{i_{0}\dots i_{k+1}\in\mathcal{G}_{k+1}}\left(d_{k}f\right)_ {i_{0}\dots i_{k+1}}g_{i_{0}\dots i_{k+1}}\] \[=\frac{1}{(k+2)!}\sum\left(\sum_{j=0}^{k+1}(-1)^{j}f_{i_{0}\dots i _{j}\dots i_{k+1}}\right)g_{i_{0}\dots i_{k+1}}\] \[=\frac{1}{(k+2)!}\sum f_{i_{0}\dots i_{k}}\left(\sum_{i_{k+1}} \sum_{j=0}^{k+1}(-1)^{j}g_{i_{0}\dots[i_{j}\dots i_{k+1}]}\right)\] \[=\frac{1}{k+2}\sum_{i_{0}i_{1}\dots i_{k}\in\mathcal{G}_{k}}f_{i_ {0}\dots i_{k}}\left(\sum_{i_{k+1}}\sum_{j=0}^{k+1}(-1)^{j}g_{i_{0}\dots[i_{j} \dots i_{k+1}]}\right)\] \[=\sum_{i_{0}i_{1}\dots i_{k}\in\mathcal{G}_{k}}f_{i_{0}\dots i_{k }}\left(d_{k}^{\intercal}g\right)_{i_{0}\dots i_{k}}=\langle f,d_{k}^{\intercal }g\rangle\,,\]

which establishes the result. 

Proposition A.1 is perhaps best illustrated with a concrete example. Consider the graph gradient, defined for edge \(\alpha=(i,j)\) as \(\left(d_{0}f\right)_{\alpha}=\left(d_{0}f\right)_{ij}=f_{j}-f_{i}\). Notice that this object is antisymmetric with respect to edge orientation, and measures the outflow of information from source to target nodes. From this, it is easy to compute the \(\ell^{2}\)-adjoint of \(d_{0}\), known as the graph divergence, via

\[\langle d_{0}f,g\rangle =\sum_{\alpha=(i,j)}\left(f_{j}-f_{i}\right)g_{ij}=\sum_{i}\sum_{ (j>i)\in\mathcal{N}(i)}g_{ij}f_{j}-g_{ij}f_{i}\] \[=\frac{1}{2}\sum_{i}\sum_{j\in\mathcal{N}(i)}f_{i}\left(g_{ji}-g_ {ij}\right)=\langle f,d_{0}^{\intercal}g\rangle\,,\]

where we have re-indexed under the double sum, used that \(i\in\mathcal{N}(j)\) if and only if \(j\in\mathcal{N}(i)\), and used that there are no self-edges in \(\mathcal{E}\). Therefore, it follows that the graph divergence at node \(i\) is given by

\[\left(d_{0}^{\intercal}g\right)_{i}=\sum_{\alpha\ni i}g_{-\alpha}-g_{\alpha}= \frac{1}{2}\sum_{j\in\mathcal{N}(i)}g_{ji}-g_{ij},\]

which reduces to the common form \(\left(d_{0}^{\intercal}g\right)_{i}=-\sum_{j}g_{ij}\) if and only if the edge feature \(g_{ij}\) is antisymmetric.

**Remark A.2**.: _When the inner product on edges \(\mathcal{E}\) is not \(L^{2}\), but defined in terms of a nonnegative, orientation-invariant, and (edge-wise) diagonal weight matrix \(\mathbf{W}=(w_{ij})\), a similar computation shows that the divergence becomes_

\[\left(d_{0}^{*}f\right)_{i}=\frac{1}{2}\sum_{j\in\mathcal{N}(i)}w_{ij}\left(f _{ji}-f_{ij}\right).\]

_The more general case of arbitrary inner products on \(\mathcal{V},\mathcal{E}\) is discussed in section A.3._

The differential operators \(d_{k}^{\intercal}\) induce a dual de Rham complex since \(d_{k-1}^{\intercal}d_{k}^{\intercal}=\left(d_{k}d_{k-1}\right)^{\intercal}=0\), which enables both the construction of Laplace operators on \(k\)-cliques, \(\Delta_{k}=d_{k}^{\intercal}d_{k}+d_{k-1}d_{k-1}^{\intercal}\), as well as the celebrated Hodge decomposition theorem, stated below. For a proof, see, e.g., [11, Theorem 3.3].

**Theorem A.3**.: _(Hodge Decomposition Theorem) The de Rham complexes formed by \(d_{k},d_{k}^{\intercal}\) induce the following direct sum decomposition of the function space \(\Omega_{k}\),_

\[\Omega_{k}=\operatorname{Im}d_{k-1}\oplus\operatorname{Ker}\Delta_{k}\oplus \operatorname{Im}d_{k}^{\intercal}.\]

In the case where the dual derivatives \(d_{k}^{*}\) are adjoint with respect to a learnable inner product which does not depend on graph features, the conclusion of Theorem A.3 continues to hold, leading to an interesting well-posedness result proved in [11] involving nonlinear perturbations of a Hodge-Laplace problem in mixed form.

**Theorem A.4**.: _([11, Theorem 3.6]) Suppose \(\mathbf{f}_{k}\in\Omega_{k}\), and \(g\left(\mathbf{x};\xi\right)\) is a neural network with parameters \(\xi\) which is Lipschitz continuous and satisfies \(g(\mathbf{0})=\mathbf{0}\). Then, the problem_

\[\mathbf{w}_{k-1} =d_{k-1}^{*}\mathbf{u}_{k}+\epsilon g\left(d_{k-1}^{*}\mathbf{u}_ {k};\xi\right),\] \[\mathbf{f}_{k} =d_{k-1}\mathbf{w}_{k-1}+d_{k}^{*}d_{k}\mathbf{u}_{k},\]

_has a unique solution on \(\Omega_{k}/\mathrm{Ker}\,\Delta_{k}\)._

This result shows that initial-value problems involving the Hodge-Laplacian are stable under nonlinear perturbations. Moreover, when \(\Delta_{0}\) is the Hodge Laplacian on nodes, there is a useful connection between \(\Delta_{0}\) and the degree and adjacency matrices of the graph \(\mathcal{G}\). Recall that the degree matrix \(\mathbf{D}=\left(d_{ij}\right)\) is diagonal with entries \(d_{ii}=\sum_{j\in\mathcal{N}\left(i\right)}1\), while the adjacency matrix \(\mathbf{A}=\left(a_{ij}\right)\) satisfies \(a_{ij}=1\) when \(j\in\mathcal{N}\left(i\right)\) and \(a_{ij}=0\) otherwise.

**Proposition A.2**.: _The combinatorial Laplacian on \(\mathcal{V}\), denoted \(\Delta_{0}=d_{0}^{\intercal}d_{0}\), satisfies \(\Delta_{0}=\mathbf{D}-\mathbf{A}\)._

Proof.: Notice that

\[\left(d_{0}^{\intercal}d_{0}\right)_{ij} =\sum_{\alpha\in\mathcal{E}}\left(d_{0}\right)_{\alpha i}\left(d_ {0}\right)_{\alpha j}=\left[i=j\right]\sum_{\alpha\in\mathcal{E}}\left(\left(d _{0}\right)_{\alpha i}\right)^{2}+\left[i\neq j\right]\sum_{\alpha=\left(i,j \right)}\left(d_{0}\right)_{\alpha i}\left(d_{0}\right)_{\alpha j}\] \[=\left[i=j\right]d_{ii}-\left[i\neq j\right]a_{ij}=d_{ij}-a_{ij}= \mathbf{D}-\mathbf{A},\]

where we used that \(\mathbf{D}\) is diagonal, \(\mathbf{A}\) is diagonal-free, and \(\left(d_{0}\right)_{\alpha i}\left(d_{0}\right)_{\alpha j}=-1\) whenever \(\alpha=\left(i,j\right)\) is an edge in \(\mathcal{E}\), since one of \(\left(d_{0}\right)_{\alpha i},\left(d_{0}\right)_{\alpha j}\) is 1 and the other is -1. 

### Bracket-based dynamical systems

Here we mention some additional facts regarding bracket-based dynamical systems. More information can be found in, e.g., [50; 65; 66; 67].

As mentioned before, the goal of bracket formalisms is to extend the Hamiltonian formalism to systems with dissipation. To understand where this originates, consider an action functional \(\mathcal{A}(q)=\int_{a}^{b}L\left(q,\dot{q}\right)dt\) on the space of curves \(q(t)\), defined in terms of a Lagrangian \(L\) on the tangent bundle to some Riemannian manifold. Using \(L_{q},L_{\dot{q}}\) to denote partial derivatives with respect to the subscripted variable, it is straightforward to show that, for any compactly supported variation \(\delta q\) of \(q\), we have

\[d\mathcal{A}(q)\delta q=\int_{a}^{b}dL\left(q,\dot{q}\right)\delta q=\int_{a} ^{b}L_{q}\delta q+L_{\dot{q}}\delta\dot{q}=\int_{a}^{b}\left(L_{q}-\partial_{ t}L_{\dot{q}}\right)\delta q,\]

where the final equality follows from integration-by-parts and the fact that variational and temporal derivatives commute in this setting. It follows that \(\mathcal{A}\) is stationary (i.e., \(d\mathcal{A}=0\)) for all variations only when \(\partial_{t}L_{\dot{q}}=L_{q}\). These are the classical Euler-Lagrange equations which are (under some regularity conditions) transformed to Hamiltonian form via a Legendre transformation,

\[H(q,p)=\sup_{\dot{q}}\left(\left\langle p,\dot{q}\right\rangle-L(q,\dot{q}) \right),\]

which defines the Hamiltonian functional \(\mathcal{H}\) on phase space, and yields the conjugate momentum vector \(p=L_{\dot{q}}\). Substituting \(L=\left\langle p,\dot{q}\right\rangle-H\) into the previously derived Euler-Lagrange equations leads immediately to Hamilton's equations for the state \(\mathbf{x}=\left(q\quad p\right)^{\intercal}\),

\[\dot{\mathbf{x}}=\left(\begin{matrix}\dot{q}\\ \dot{p}\end{matrix}\right)=\left(\begin{matrix}0&1\\ -1&0\end{matrix}\right)\left(\begin{matrix}H_{q}\\ H_{p}\end{matrix}\right)=\mathbf{J}\nabla\mathcal{H},\]

which are an equivalent description of the system in question in terms of the anti-involution \(\mathbf{J}\) and the functional gradient \(\nabla\mathcal{H}\).

An advantage of the Hamiltonian description is its compact bracket-based formulation, \(\dot{\mathbf{x}}=\mathbf{J}\nabla\mathcal{H}=\left\{\mathbf{x},\mathcal{H}\right\}\), which requires only the specification of an antisymmetric Poisson bracket \(\left\{\cdot,\cdot\right\}\) and a Hamiltonian functional \(\mathcal{H}\). Besides admitting a direct generalization to more complex systems such as Korteweg-de Vries or incompressible Euler, where the involved bracket is state-dependent, this formulation makes the energy conservation property of the system obvious. In particular, it follows immediately from the antisymmetry of \(\left\{\cdot,\cdot\right\}\) that

\[\dot{\mathcal{H}}=\left\langle\dot{\mathbf{x}},\nabla\mathcal{H}\right\rangle= \left\{\mathcal{H},\mathcal{H}\right\}=0,\]while it is more difficult to see immediately that the Euler-Lagrange system obeys this same property. The utility and ease-of-use of bracket formulations is what inspired their extension to other systems of interest which do not conserve energy. On the opposite end of this spectrum are the generalized gradient flows, which can be written in terms of a bracket which is purely dissipative. An example of this is heat flow \(\dot{q}=\Delta q:=-\left[q,\mathcal{D}\right]\), which is the \(L^{2}\)-gradient flow of Dirichlet energy \(\mathcal{D}(q)=(1/2)\int_{a}^{b}\left|q^{\prime}\right|^{2}dt\) (c.f. Appendix A.3). In this case, the functional gradient \(\nabla\mathcal{D}=-\partial_{tt}\) is the negative of the usual Laplace operator, so that the positive-definite bracket \(\left[\cdot,\cdot\right]\) is generated by the identity operator \(M=\mathrm{id}\). It is interesting to note that the same system could be expressed using the usual kinetic energy \(\mathcal{E}(q)=(1/2)\int_{a}^{b}\left|q\right|^{2}dt\) instead, provided that the corresponding bracket is generated by \(M=-\Delta\). This is a good illustration of the flexibility afforded by bracket-based dynamical systems.

Since physical systems are not always purely reversible or irreversible, other useful bracket formalisms have been introduced to capture dynamics which are a mix of these two. The double bracket \(\dot{\mathbf{x}}=\{\mathbf{x},E\}+\{\{\mathbf{x},E\}\}=\mathbf{L}\nabla E+ \mathbf{L}^{2}\nabla E\) is a nice extension of the Hamiltonian bracket particularly because it is Casimir preserving, i.e., those quantities which annihilate the Poisson bracket \(\{\cdot,\cdot\}\) also annihilate the double bracket. This allows for the incorporation of dissipative phenomena into idealized Hamiltonian systems without affecting desirable properties such as mass conservation, and has been used to model, e.g., the Landau-Lifschitz dissipative mechanism, as well as a mechanism for fluids where energy decays but entrophy is preserved (see [65] for additional discussion). A complementary but alternative point of view is taken by the metriplectic bracket formalism, which requires that any dissipation generated by the system is accounted for within the system itself through the generation of entropy. In the metriplectic formalism, the equations of motion are \(\dot{\mathbf{x}}=\{\mathbf{x},E\}+[\mathbf{x},S]=\mathbf{L}\nabla E+\mathbf{M}\nabla S\), along with important and nontrivial compatibility conditions \(\mathbf{L}\nabla S=\mathbf{M}\nabla E=\mathbf{0}\), also called degeneracy conditions, which ensure that the reversible and irreversible mechanisms do not cross-contaminate. As shown in the body of the paper, this guarantees that metriplectic systems obey a form of the first and second thermodynamical laws. Practically, the degeneracy conditions enforce a good deal of structure on the operators \(\mathbf{L},\mathbf{M}\) which has been exploited to generate surrogate models [29, 68, 31]. In particular, it can be shown that the reversible and irreversible brackets can be parameterized in terms of a totally antisymmetric order-3 tensor \(\boldsymbol{\xi}=(\xi_{ijk})\) and a partially symmetric order-4 tensor \(\boldsymbol{\zeta}=(\zeta_{ik,jl})\) through the relations (Einstein summation assumed)

\[\begin{split}\{A,B\}&=\xi^{ijk}\,\partial_{i}A\, \partial_{j}B\,\partial_{k}S,\\ [A,B]&=\zeta^{ik,jl}\,\partial_{i}A\,\partial_{k}E \,\partial_{j}B\,\partial_{l}E.\end{split}\]

Moreover, using the symmetries of \(\boldsymbol{\zeta}\), it follows (see [67]) that this tensor decomposes into the product \(\zeta_{ik,jl}=\Lambda_{ik}^{m}D_{mn}\Lambda_{jl}^{n}\) of a symmetric matrix \(\mathbf{D}\) and an order-3 tensor \(\Lambda\) which is skew-symmetric in its lower indices. Thus, by applying symmetry relationships, it is easy to check that \(\{\cdot,S\}=\left[\cdot,E\right]=\mathbf{0}\).

**Remark A.5**.: _In [29], trainable 4- and 3- tensors \(\xi^{ijk}\) and \(\zeta^{ik,jl}\) are constructed to achieve the degeneracy conditions, mandating a costly \(O(N^{3})\) computational complexity. In the current work we overcome this by instead achieving degeneracy through the exact sequence property._

### Adjoints and gradients

Beyond the basic calculus operations discussed in section A.1 which depend only on graph topology, the network architectures discussed in the body also make extensive use of learnable metric information coming from the nodal features. To understand this, it is useful to recall some information about general inner products and the derivative operators that they induce. First, recall that the usual \(\ell^{2}\) inner product on node features \(\mathbf{a},\mathbf{b}\in\mathbb{R}^{|\mathcal{V}|}\), \(\langle\mathbf{a},\mathbf{b}\rangle=\mathbf{a}^{\intercal}\mathbf{b}\), is (in this context) a discretization of the standard \(L^{2}\) inner product \(\int_{\mathcal{V}}ab\,d\mu\) which aggregates information from across the vertex set \(\mathcal{V}\). While this construction is clearly dependent only on the graph structure (i.e., topology), _any_ symmetric positive definite (SPD) matrix \(\mathbf{A}_{0}:\Omega_{0}\rightarrow\Omega_{0}\) also defines an inner product on functions \(\mathbf{a}\in\Omega_{0}\) through the equality

\[\left(\mathbf{a},\mathbf{b}\right)_{0}\coloneqq\langle\mathbf{a},\mathbf{A}_{ 0}\mathbf{b}\rangle=\mathbf{a}^{\intercal}\mathbf{A}_{0}\mathbf{b},\]

which gives a different way of measuring the distance between \(\mathbf{a}\) and \(\mathbf{b}\). The advantage of this construction is that \(\mathbf{A}_{0}\) can be chosen in a way that incorporates geometric information which implicitly regularizes systems obeying a variational principle. This follows from the following intuitive fact: the Taylor series of a function does not change, regardless of the inner product on its domain. For any differentiable function(al) \(E:\Omega_{0}\to\mathbb{R}\), using \(d\) to denote the exterior derivative, this means that the following equality holds

\[dE(\mathbf{a})\mathbf{b}:=\lim_{\varepsilon\to 0}\frac{E(\mathbf{a}+\epsilon \mathbf{b})-E(\mathbf{a})}{\varepsilon}=\left\langle\delta E(\mathbf{a}), \mathbf{b}\right\rangle=\left(\nabla E(\mathbf{a}),\mathbf{b}\right)_{0},\]

where \(\delta E\) denotes the \(\ell^{2}\)-gradient of \(E\) and \(\nabla E\) denotes its \(\mathbf{A}_{0}\)-gradient, i.e., its gradient with respect to the derivative operator induced by the inner product involving \(\mathbf{A}_{0}\). From this, it is clear that \(\delta E=\mathbf{A}_{0}\nabla E\), so that the \(\mathbf{A}_{0}\)-gradient is just an anisotropic rescaling of the \(\ell^{2}\) version. The advantage of working with \(\nabla\) over \(\delta\) in the present case of graph networks is that \(\mathbf{A}_{0}\) can be _learned_ based on the features of the graph. This means that learnable feature information (i.e., graph attention) can be directly incorporated into the differential operators governing our bracket-based dynamical systems by construction.

The prototypical example of where this technique is useful is seen in the gradient flow of Dirichlet energy. Recall that the Dirichlet energy of a differentiable function \(u:\mathbb{R}^{n}\to\mathbb{R}\) is given by \(\mathcal{D}(u)=(1/2)\int\left|\nabla u\right|^{2}d\mu\), where \(\nabla\) now denotes the usual \(\ell^{2}\)-gradient of the function \(u\) on \(\mathbb{R}^{n}\). Using integration-by-parts, it is easy to see that \(d\mathcal{D}(u)v=-\int v\Delta u\) for any test function \(v\) with compact support, implying that the \(L^{2}\)-gradient of \(\mathcal{D}\) is \(-\Delta\) and \(\dot{u}=\Delta u\) is the \(L^{2}\)-gradient flow of Dirichlet energy: the motion which decreases the quantity \(\mathcal{D}(u)\) the fastest _as measured by the \(L^{2}\) norm_. It can be shown that high-frequency modes decay quickly under this flow, while low-frequency information takes much longer to dissipate. On the other hand, we could alternatively run the \(H^{1}\)-gradient flow of \(\mathcal{D}\), which is motion of fastest decrease with respect to the \(H^{1}\) inner product \((u,v)=\int\left\langle\nabla u,\nabla v\right\rangle d\mu\). This motion is prescribed in terms of the \(H^{1}\)-gradient of \(\mathcal{D}\), which by the discussion above with \(\mathbf{A}_{0}=-\Delta\) is easily seen to be the identity. This means that the \(H^{1}\)-gradient flow is given by \(\dot{u}=-u\), which retains the minimizers of the \(L^{2}\)-flow but with quite different intermediate character, since it functions by simultaneously flattening all spatial frequencies. The process of preconditioning a gradient flow by matching derivatives is known as a Sobolev gradient method (c.f. [69]), and these methods often exhibit faster convergence and better numerical behavior than their \(L^{2}\) counterparts [70].

Returning to the graph setting, our learnable matrices \(\mathbf{A}_{k}\) on \(k\)-cliques will lead to inner products \(\left(\cdot,\cdot\right)_{k}\) on functions in \(\Omega_{k}\), and this will induce dual derivatives as described in Appendix A.1. However, in this case we will not have \(d_{0}^{*}=d_{0}^{\intercal}\), but instead the expression given by the following result:

**Proposition A.3**.: _The \(\mathbf{A}_{k}\)-adjoints \(d_{k}^{*}\) to the graph derivative operators \(d_{k}\) are given by \(d_{k}^{*}=\mathbf{A}_{k}^{-1}d_{k}^{\intercal}\mathbf{A}_{k+1}\). Similarly, for any linear operator \(\mathbf{B}:\Omega_{k}\to\Omega_{k}\), the \(\mathbf{A}_{k}\)-adjoint \(\mathbf{B}^{*}=\mathbf{A}_{k}^{-1}\mathbf{B}^{\intercal}\mathbf{A}\)._

Proof.: Let \(\mathbf{q},\mathbf{p}\) denote vectors of \(k\)-clique resp. \((k+1)\)-clique features. It follows that

\[\left(d_{k}\mathbf{q},\mathbf{p}\right)_{k+1}=\left\langle d_{k}\mathbf{q}, \mathbf{A}_{k+1}\mathbf{p}\right\rangle=\left\langle\mathbf{q},d_{k}^{ \intercal}\mathbf{A}_{k+1}\mathbf{p}\right\rangle=\left\langle\mathbf{q}, \mathbf{A}_{k}d_{k}^{\intercal}\mathbf{p}\right\rangle=\left(\mathbf{q},d_{k }^{\intercal}\mathbf{p}\right)_{k}.\]

Therefore, we see that \(d_{k}^{\intercal}\mathbf{A}_{k+1}=\mathbf{A}_{k}d_{k}^{*}\) and hence \(d_{k}^{*}=\mathbf{A}_{k}^{-1}d_{k}^{\intercal}\mathbf{A}_{k+1}\). Similarly, if \(\mathbf{q},\mathbf{q}^{\prime}\) denote vectors of \(k\)-clique features, it follows from the \(\ell^{2}\)-self-adjointness of \(\mathbf{A}_{k}\) that

\[\left(\mathbf{q},\mathbf{B}\mathbf{q}^{\prime}\right)_{k}=\left\langle\mathbf{ q},\mathbf{A}_{k}\mathbf{B}\mathbf{q}^{\prime}\right\rangle=\left\langle \mathbf{B}^{\intercal}\mathbf{A}_{k}\mathbf{q},\mathbf{q}^{\prime}\right\rangle =\left\langle\mathbf{A}_{k}^{-1}\mathbf{B}^{\intercal}\mathbf{A}_{k}\mathbf{q },\mathbf{A}_{k}\mathbf{q}^{\prime}\right\rangle=\left\langle\mathbf{B}^{ \ast}\mathbf{q},\mathbf{q}^{\prime}\right\rangle_{k},\]

establishing that \(\mathbf{B}^{*}=\mathbf{A}_{k}^{-1}\mathbf{B}^{\intercal}\mathbf{A}_{k}\). 

**Remark A.6**.: _It is common in graph theory to encounter the case where \(a_{i}>0\) are nodal weights and \(w_{ij}>0\) are edge weights. These are nothing more than the (diagonal) inner products \(\mathbf{A}_{0},\mathbf{A}_{1}\) in disguise, and so Proposition A.3 immediately yields the familiar formula for the induced divergence_

\[\left(d_{0}^{*}\mathbf{p}\right)_{i}=\frac{1}{a_{i}}\sum_{j:(i,j)\in\mathcal{E }}w_{ij}\left(\mathbf{p}_{ji}-\mathbf{p}_{ij}\right).\]

Note that all of these notions extend to the case of block inner products in the obvious way. For example, if \(\mathbf{q},\mathbf{p}\) are node resp. edge features, it follows that \(\mathbf{A}=\operatorname{diag}\left(\mathbf{A}_{0},\mathbf{A}_{1}\right)\) is an inner product on node-edge feature pairs, and the adjoints of node-edge operators with respect to \(\mathbf{A}\) are computed as according to Proposition A.3.

**Remark A.7**.: _For convenience, this work restricts to diagonal matrices \(\mathbf{A}_{0},\mathbf{A}_{1}\). However, note that a matrix which is diagonal in "edge space" \(\mathcal{G}_{2}\) is generally full in a nodal representation. This is because an (undirected) edge is uniquely specified by the two nodes which it connects, meaning that a purely local quantity on edges is necessarily nonlocal on nodes._

### Higher order attention

As mentioned in the body, when \(f=\exp\) and \(\tilde{a}(\mathbf{q}_{i},\mathbf{q}_{j})=\left(1/d\right)\left\langle\mathbf{W}_{ K}\mathbf{q}_{i},\mathbf{W}_{Q}\mathbf{q}_{j}\right\rangle\), defining the learnable inner products \(\mathbf{A}_{0}=\left(a_{0,ii}\right),\mathbf{A}_{1}=\left(a_{1,ij}\right)\) as

\[a_{0,ii}=\sum_{j\in\mathcal{N}(i)}f\left(\tilde{a}\left(\mathbf{q}_{i},\mathbf{ q}_{j}\right)\right),\qquad a_{1,ij}=f\left(\tilde{a}\left(\mathbf{q}_{i}, \mathbf{q}_{j}\right)\right),\]

recovers scaled dot product attention as \(\mathbf{A}_{0}^{-1}\mathbf{A}_{1}\).

**Remark A.8**.: _Technically, \(\mathbf{A}_{1}\) is an inner product only with respect to a predefined ordering of the edges \(\alpha=\left(i,j\right)\), since we do not require \(\mathbf{A}_{1}\) be orientation-invariant. On the other hand, it is both unnecessary and distracting to enforce symmetry on \(\mathbf{A}_{1}\) in this context, since any necessary symmetrization will be handled automatically by the differential operator \(d_{0}^{*}\)._

Similarly, other common attention mechanisms are produced by modifying the pre-attention function \(\tilde{a}\). While \(\mathbf{A}_{0}^{-1}\mathbf{A}_{1}\) never appears in the brackets of Section 4, letting \(\alpha=\left(i,j\right)\) denote a global edge with endpoints \(i,j\), it is straightforward to calculate the divergence of an antisymmetric edge feature \(\mathbf{p}\) at node \(i\),

\[\left(d_{0}^{*}\mathbf{p}\right)_{i} =\left(\mathbf{A}_{0}^{-1}d_{0}^{\intercal}\mathbf{A}_{1}\mathbf{ p}\right)_{i}=a_{0,ii}^{-1}\sum_{\alpha}\left(d_{0}^{\intercal}\right)_{i \alpha}\left(\mathbf{A}_{1}\mathbf{p}\right)_{\alpha}\] \[=a_{0,ii}^{-1}\sum_{\alpha\ni i}\left(\mathbf{A}_{1}\mathbf{p} \right)_{-\alpha}-\left(\mathbf{A}_{1}\mathbf{p}\right)_{\alpha}=-\sum_{j\in \mathcal{N}(i)}\frac{a_{1,ji}+a_{i,ij}}{a_{0,ii}}\mathbf{p}_{ij}.\]

This shows that \(b(\mathbf{q}_{i},\mathbf{q}_{j})=\left(a_{1,ij}+a_{1,ji}\right)/a_{0,ii}\) appears under the divergence in \(d_{0}^{*}=\mathbf{A}_{0}^{-1}d_{0}^{\intercal}\mathbf{A}_{1}\), which is the usual graph attention up to a symmetrization in \(\mathbf{A}_{1}\).

**Remark A.9**.: _While \(\mathbf{A}_{1}\) is diagonal on global edges \(\alpha=\left(i,j\right)\), it appears sparse nondiagonal in its nodal representation. Similarly, any diagonal extension \(\mathbf{A}_{2}\) to 2-cliques will appear as a sparse 3-tensor \(\mathbf{A}_{2}=\left(a_{2,ijk}\right)\) when specified by its nodes._

This inspires a straightforward extension of graph attention to higher-order cliques. In particular, denote by \(K>0\) the highest degree of clique under consideration, and define \(\mathbf{A}_{K-1}=\left(a_{K-1,ii_{2}\ldots i_{K}}\right)\) by

\[a_{K-1,ii_{2}\ldots i_{K}}=f\left(\mathbf{W}\left(\mathbf{q}_{i_{1}},\mathbf{ q}_{i_{2}},...,\mathbf{q}_{i_{K}}\right)\right),\]

where \(\mathbf{W}\in\mathbb{R}^{\otimes\kappa n_{V}}\) is a learnable \(K\)-tensor. Then, for any \(0\leq k\leq K-2\) define \(\mathbf{A}_{k}=\left(a_{k,ii_{2}\ldots i_{k+1}}\right)\) by

\[a_{k,ii_{1}i_{2}\ldots i_{k+1}}=\sum_{i_{K},\ldots,i_{K-k-1}}a_{K-1,ii_{1}i_{2 }\ldots i_{K}}.\]

This recovers the matrices \(\mathbf{A}_{0},\mathbf{A}_{1}\) from before when \(K=2\), and otherwise extends the same core idea to higher-order cliques. It's attractive that the attention mechanism captured by \(d_{k}^{*}\) remains asymmetric, meaning that the attention of any one node to the others in a \(k\)-clique need not equal the attention of the others to that particular node.

**Remark A.10**.: _A more obvious but less expressive option for higher-order attention is to let_

\[a_{k,i_{1}i_{2}\ldots i_{k+1}}=\frac{a_{K-1,i_{1}i_{2}\ldots i_{K}}}{\sum_{i_{ K},\ldots,i_{K-k-1}}a_{K-1,ii_{1}i_{2}\ldots i_{K}}},\]

_for any \(0\leq k\leq K-2\). However, application of the combinatorial codifferential \(d_{k-1}^{\intercal}\) appearing in \(d_{k-1}^{*}\) will necessarily symmetrize this quantity, so that the asymmetry behind the attention mechanism is lost in this formulation._

To illustrate how this works more concretely, consider the extension \(K=3\) to 2-cliques, and let \(\mathcal{N}(i,j)=\mathcal{N}(i)\cap\mathcal{N}(j)\). We have the tensors \(\mathbf{A}_{2}=\left(a_{2,ijk}\right)\), \(\mathbf{A}_{1}=\left(a_{1,ij}\right)\), and \(\mathbf{A}_{0}=\left(a_{0,i}\right)\) defined by

\[a_{2,ijk}=f\left(\mathbf{W}\left(\mathbf{q}_{i},\mathbf{q}_{j},\mathbf{q}_{k} \right)\right),\quad a_{1,ij}=\sum_{k\in\mathcal{N}(i,j)}a_{2,ijk},\quad a_{0,i}=\sum_{j\in\mathcal{N}(i)}\sum_{k\in\mathcal{N}(i,j)}a_{2,ijk}.\]

This provides a way for (features on) 3-node subgraphs of \(\mathcal{G}\) to attend to each other, and can be similarly built-in to the differential operator \(d_{1}^{*}=\mathbf{A}_{1}^{-1}d_{0}^{\intercal}\mathbf{A}_{2}\).

### Exterior calculus interpretation of GATs

Let \(\mathcal{N}(i)\) denote the one-hop neighborhood of node \(i\), and let \(\overline{\mathcal{N}}(i)=\mathcal{N}(i)\cup\{i\}\). Recall the standard (single-headed) graph attention network (GAT) described in [12], described layer-wise as

\[\mathbf{q}_{i}^{k+1}=\boldsymbol{\sigma}\left(\sum_{j\in\overline{\mathcal{N}} (i)}a\left(\mathbf{q}_{i}^{k},\mathbf{q}_{j}^{k}\right)\mathbf{W}^{k}\mathbf{q }_{j}^{k}\right), \tag{1}\]

where \(\boldsymbol{\sigma}\) is an element-wise nonlinearity, \(\mathbf{W}^{k}\) is a layer-dependent embedding matrix, and \(a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)\) denotes the attention node \(i\) pays to node \(j\). Traditionally, the attention mechanism is computed through

\[a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)=\mathrm{Softmax}_{j}\,\tilde{a} \left(\mathbf{q}_{i},\mathbf{q}_{j}\right)=\frac{e^{\tilde{a}\left(\mathbf{q} _{i},\mathbf{q}_{j}\right)}}{\sigma_{i}},\]

where the pre-attention coefficients \(\tilde{a}\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)\) and nodal weights \(\sigma_{i}\) are defined as

\[\tilde{a}\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)=\mathrm{LeakyReLU}\left( \mathbf{a}^{\intercal}\left(\mathbf{W}^{\intercal}\mathbf{q}_{i}\,||\, \mathbf{W}^{\intercal}\mathbf{q}_{j}\right)\right),\qquad\sigma_{i}=\sum_{j \in\overline{\mathcal{N}}(i)}e^{\tilde{a}\left(\mathbf{q}_{i},\mathbf{q}_{j} \right)}.\]

However, the exponentials in the outer \(\mathrm{Softmax}\) are often replaced with other nonlinear functions, e.g. \(\mathrm{Squareplus}\), and the pre-attention coefficients \(\tilde{a}\) appear as variable (but learnable) functions of the nodal features. First, notice that (1) the attention coefficients \(a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)\) depend on the node features \(\mathbf{q}\) and not simply the topology of the graph, and (2) the attention coefficients are not symmetric, reflecting the fact that the attention paid by node \(i\) to node \(j\) need not equal the attention paid by node \(j\) to node \(i\). A direct consequence of this is that GATs are not purely diffusive under any circumstances, since it was shown in Appendix A.1 that the combinatorial divergence \(d_{0}^{\intercal}\) will antisymmetrize the edge features it acts on. In particular, it is clear that the product \(a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)\left(\mathbf{q}_{i}-\mathbf{q}_{j}\right)\) is asymmetric in \(i,j\) under the standard attention mechanism, since even the pre-attention coefficients \(\tilde{a}\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)\) are not symmetric, meaning that there will be two distinct terms after application of the divergence. More precisely, there is the following subtle result.

**Proposition A.4**.: _Let \(\mathbf{q}\in\mathbb{R}^{|\mathcal{V}|\times n_{V}}\) denote an array of nodal features. The expression_

\[\sum_{j\in\overline{\mathcal{N}}(i)}a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right) \left(\mathbf{q}_{i}-\mathbf{q}_{j}\right),\]

_where \(a=\mathbf{A}_{0}^{-1}\mathbf{A}_{1}\) is not the action of a Laplace operator whenever \(\mathbf{A}_{1}\) is not symmetric._

Proof.: From Appendix A.3, we know that any Laplace operator on nodes is expressible as \(d_{0}^{*}d_{0}=\mathbf{A}_{0}^{-1}d_{0}^{\intercal}\mathbf{A}_{1}d_{0}\) for some positive definite \(\mathbf{A}_{0},\mathbf{A}_{1}\). So, we compute the action of the Laplacian at node \(i\),

\[\left(\Delta_{0}\mathbf{q}\right)_{i} =\left(d_{0}^{*}d_{0}\mathbf{q}\right)_{i}=\left(\mathbf{A}_{0}^ {-1}d_{0}^{\intercal}\mathbf{A}_{1}d_{0}\mathbf{q}\right)_{i}=a_{0,ii}^{-1} \sum_{\alpha}\left(d_{0}^{\intercal}\right)_{i\alpha}\left(\mathbf{A}_{1}d_{0 }\mathbf{q}\right)_{\alpha}\] \[=a_{0,ii}^{-1}\sum_{\alpha\ni i}\left(\mathbf{A}_{1}d_{0}\mathbf{ q}\right)_{-\alpha}-\left(\mathbf{A}_{1}d_{0}\mathbf{q}\right)_{\alpha}=-\frac{1}{2} \sum_{j\in\mathcal{N}(i)}\frac{a_{1,ji}+a_{i,ij}}{a_{0,ii}}\left(\mathbf{q}_{ j}-\mathbf{q}_{i}\right),\] \[=\sum_{j\in\mathcal{N}(i)}a\left(\mathbf{q}_{i},\mathbf{q}_{j} \right)\left(\mathbf{q}_{j}-\mathbf{q}_{i}\right),\]

which shows that \(a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)=\left(1/2\right)\left(a_{1,ji}+a_ {1,ij}\right)/a_{0,ii}\) must have symmetric numerator. 

While this result shows that GATs (and their derivatives, e.g., GRAND) are not purely diffusive, it also shows that it is possible to get close to GAT (at least syntactically) with a learnable diffusion mechanism. In fact, setting \(\boldsymbol{\sigma}=\mathbf{W}^{k}=\mathbf{I}\) in (1) yields precisely a single-step diffusion equation provided that \(a\left(q_{i}^{k},q_{j}^{k}\right)\) is right-stochastic (i.e., \(\sum_{j}a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)1_{j}=1_{i}\)) and built as dictated by Proposition A.4.

**Theorem A.11**.: _The GAT layer (1) is a single-step diffusion equation provided that \(\boldsymbol{\sigma}=\mathbf{W}^{k}=\mathbf{I}\), and the attention mechanism \(a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)=\left(1/2\right)\left(a_{1,ji}+a_ {1,ij}\right)/a_{0,ii}\) is right-stochastic._Proof.: First, notice that the Laplacian with respect to an edge set which contains self-loops is computable via

\[\left(\Delta_{0}\mathbf{q}\right)_{i}=-\sum_{j\in\overline{\mathcal{N}}\left(i \right)}a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)\left(\mathbf{q}_{j}- \mathbf{q}_{i}\right)=\mathbf{q}_{i}-\sum_{j\in\overline{\mathcal{N}}\left(i \right)}a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)\mathbf{q}_{j}.\]

Therefore, taking a single step of heat flow \(\dot{\mathbf{q}}=-\Delta_{0}\mathbf{q}\) with forward Euler discretization and time step \(\tau=1\) is equivalent to

\[\mathbf{q}_{i}^{k+1}=\mathbf{q}_{i}^{k}-\tau\left(\Delta_{0}\mathbf{q}^{k} \right)_{i}=\sum_{j\in\overline{\mathcal{N}}\left(i\right)}a\left(\mathbf{q}_ {i}^{k},\mathbf{q}_{j}^{k}\right)\mathbf{q}_{j}^{k},\]

which is just a modified and non-activated GAT layer with \(\mathbf{W}^{k}=\mathbf{I}\) and attention mechanism \(a\). 

**Remark A.12**.: _Since \(\mathrm{Softmax}\) and its variants are right-stochastic, Theorem A.11 is what establishes equivalence between the non-divergence equation_

\[\dot{\mathbf{q}}_{i}=\sum_{j\in\overline{\mathcal{N}}\left(i\right)}a\left( \mathbf{q}_{i},\mathbf{q}_{j}\right)\left(\mathbf{q}_{j}-\mathbf{q}_{i}\right),\]

_and the standard GAT layer seen in, e.g., [49], when \(a(\mathbf{q}_{i},\mathbf{q}_{j})\) is the usual attention mechanism._

**Remark A.13**.: _In the literature, there is an important (and often overlooked) distinction between the positive graph/Hodge Laplacian \(\Delta_{0}\) and the negative "geometer's Laplacian" \(\Delta\) which is worth noting here. Particularly, we have from integration-by-parts that the gradient \(\nabla=d_{0}\) is \(L^{2}\)-adjoint to minus the divergence \(-\nabla^{\cdot}=d_{0}^{\intercal}\), so that the two Laplace operators \(\Delta_{0}=d_{0}^{\intercal}d_{0}\) and \(\Delta=\nabla\cdot\nabla\) differ by a sign. This is why the same \(\ell^{2}\)-gradient flow of Dirichlet energy can be equivalently expressed as \(\dot{\mathbf{q}}=\Delta\mathbf{q}=-\Delta_{0}\mathbf{q}\), but not by, e.g., \(\dot{\mathbf{q}}=\Delta_{0}\mathbf{q}\)._

This shows that, while they are not equivalent, there is a close relationship between attention and diffusion mechanisms on graphs. The closest analogue to the standard attention expressible in this format is perhaps the choice \(a_{1,ij}=f\left(\tilde{a}\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)\right)\), \(a_{0,ii}=\sum_{j\in\tilde{\mathcal{N}}\left(i\right)}a_{1,ij}\), discussed in Section 4 and Appendix A.4, where \(f\) is any scalar-valued positive function. For example, when \(f(x)=e^{x}\), it follows that

\[\left(\Delta_{0}\mathbf{q}\right)_{i} =-\frac{1}{2}\sum_{j\in\mathcal{N}\left(i\right)}\frac{e^{\tilde{ a}\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)}+e^{\tilde{a}\left(\mathbf{q}_{j}, \mathbf{q}_{i}\right)}}{\sigma_{i}}\left(\mathbf{q}_{j}-\mathbf{q}_{i}\right)\] \[=-\frac{1}{2}\sum_{j\in\mathcal{N}\left(i\right)}\left(a\left( \mathbf{q}_{i},\mathbf{q}_{j}\right)+\frac{e^{\tilde{a}\left(\mathbf{q}_{j}, \mathbf{q}_{i}\right)}}{\sigma_{i}}\right)\left(\mathbf{q}_{j}-\mathbf{q}_{i} \right),\]

which leads to the standard GAT propagation mechanism plus an extra term arising from the fact that the attention \(a\) is not symmetric.

**Remark A.14**.: _Practically, GATs and their variants typically make use of multi-head attention, defined in terms of an attention mechanism which is averaged over some number \(\left|h\right|\) of independent "heads",_

\[a\left(\mathbf{q}_{i},\mathbf{q}_{j}\right)=\frac{1}{\left|h\right|}\sum_{h}a ^{h}\left(\mathbf{q}_{i},\mathbf{q}_{j}\right),\]

_which are distinct only in their learnable parameters. While the results of this section were presented in terms of \(\left|h\right|=1\), the reader can check that multiple attention heads can be used in this framework provided it is the pre-attention \(\tilde{a}\) that is averaged instead._

### Bracket derivations and properties

Here the architectures in the body are derived in greater detail. First, it will be shown that \(\mathbf{L}^{*}=-\mathbf{L}\), \(\mathbf{G}^{*}=\mathbf{G}\), and \(\mathbf{M}^{*}=\mathbf{M}\), as required for structure-preservation.

**Proposition A.5**.: _For \(\mathbf{L},\mathbf{G},\mathbf{M}\) defined in Section 4, we have \(\mathbf{L}^{*}=-\mathbf{L}\), \(\mathbf{G}^{*}=\mathbf{G}\), and \(\mathbf{M}^{*}=\mathbf{M}\)._Proof.: First, denoting \(\mathbf{A}=\operatorname{diag}\left(\mathbf{A}_{0},\mathbf{A}_{1}\right)\), it was shown in section A.3 that \(\mathbf{B}^{*}=\mathbf{A}^{-1}\mathbf{B}^{\intercal}\mathbf{A}\) for any linear operator \(\mathbf{B}\) of appropriate dimensions. So, applying this to \(\mathbf{L}\), it follows that

\[\mathbf{L}^{*} =\begin{pmatrix}\mathbf{A}_{0}^{-1}&\mathbf{0}\\ \mathbf{0}&\mathbf{A}_{1}^{-1}\end{pmatrix}\begin{pmatrix}\mathbf{0}&-d_{0}^{ *}\\ d_{0}&\mathbf{0}\end{pmatrix}^{\intercal}\begin{pmatrix}\mathbf{A}_{0}&\mathbf{0 }\\ \mathbf{0}&\mathbf{A}_{1}\end{pmatrix}\] \[=\begin{pmatrix}\mathbf{0}&\mathbf{A}_{0}^{-1}d_{0}^{\intercal} \mathbf{A}_{1}\\ -\mathbf{A}_{1}^{-1}\left(d_{0}^{*}\right)^{\intercal}\mathbf{A}_{0}&\mathbf{ 0}\end{pmatrix}=\begin{pmatrix}\mathbf{0}&d_{0}^{*}\\ -d_{0}&\mathbf{0}\end{pmatrix}=-\mathbf{L}.\]

Similarly, it follows that

\[\mathbf{G}^{*} =\begin{pmatrix}\mathbf{A}_{0}^{-1}&\mathbf{0}\\ \mathbf{0}&\mathbf{A}_{1}^{-1}\end{pmatrix}\begin{pmatrix}d_{0}^{*}d_{0}& \mathbf{0}\\ \mathbf{0}&d_{1}^{*}d_{1}\end{pmatrix}^{\intercal}\begin{pmatrix}\mathbf{A}_{0} &\mathbf{0}\\ \mathbf{0}&\mathbf{A}_{1}\end{pmatrix}\] \[=\begin{pmatrix}\mathbf{0}&\mathbf{0}\\ \mathbf{0}&d_{1}^{\intercal}\left(d_{1}^{*}\right)^{\intercal}\mathbf{A}_{1}^{ 2}\end{pmatrix}=\begin{pmatrix}\mathbf{0}&\mathbf{0}\\ \mathbf{0}&\mathbf{A}_{1}d_{1}^{*}d_{1}\mathbf{A}_{1}\end{pmatrix}=\mathbf{M},\]

where the second-to-last equality used that \(\mathbf{A}_{1}\mathbf{A}_{1}^{-1}=\mathbf{I}\). 

**Remark A.15**.: _Note that the choice of zero blocks in \(\mathbf{L},\mathbf{G}\) is sufficient but not necessary for these adjointness relationships to hold. For example, one could alternatively choose the diagonal blocks of \(\mathbf{L}\) to contain terms like \(\mathbf{B}-\mathbf{B}^{*}\) for an appropriate message-passing network \(\mathbf{B}\)._

Next, we compute the gradients of energy and entropy with respect to \(\left(\cdot,\cdot\right)\).

**Proposition A.6**.: _The \(\mathbf{A}\)-gradient of the energy_

\[E(\mathbf{q},\mathbf{p})=\frac{1}{2}\left(\left|\mathbf{q}\right|^{2}+\left| \mathbf{p}\right|^{2}\right)=\frac{1}{2}\sum_{i\in\mathcal{V}}\left|\mathbf{q }_{i}\right|^{2}+\frac{1}{2}\sum_{\alpha\in\mathcal{E}}\left|\mathbf{p}_{ \alpha}\right|^{2},\]

_satisfies_

\[\nabla E(\mathbf{q},\mathbf{p})=\begin{pmatrix}\mathbf{A}_{0}^{-1}&\mathbf{0} \\ \mathbf{0}&\mathbf{A}_{1}^{-1}\end{pmatrix}\begin{pmatrix}\mathbf{q}\\ \mathbf{p}\end{pmatrix}=\begin{pmatrix}\mathbf{A}_{0}^{-1}\mathbf{q}\\ \mathbf{A}_{1}^{-1}\mathbf{p}\end{pmatrix}.\]

_Moreover, given the energy and entropy defined as_

\[E(\mathbf{q},\mathbf{p}) =f_{E}\left(s(\mathbf{q})\right)+g_{E}\left(s\left(d_{0}d_{0}^{ \intercal}\mathbf{p}\right)\right),\] \[S(\mathbf{q},\mathbf{p}) =g_{S}\left(s\left(d_{1}^{\intercal}d_{1}\mathbf{p}\right)\right),\]

_where \(f_{E}:\mathbb{R}^{n_{V}}\to\mathbb{R}\) acts on node features, \(g_{E},g_{S}:\mathbb{R}^{n_{E}}\to\mathbb{R}\) act on edge features, and \(s\) denotes sum aggregation over nodes or edges, the \(\mathbf{A}\)-gradients are_

\[\nabla E(\mathbf{q},\mathbf{p})=\begin{pmatrix}\mathbf{A}_{0}^{-1}\mathbf{1} \otimes\nabla f_{E}\left(s(\mathbf{q})\right)\\ \mathbf{A}_{1}^{-1}d_{0}d_{0}^{\intercal}\mathbf{1}\otimes\nabla g_{E}\left(s \left(d_{0}d_{0}^{\intercal}\mathbf{p}\right)\right)\end{pmatrix},\quad\nabla S (\mathbf{q},\mathbf{p})=\begin{pmatrix}\mathbf{0}&\mathbf{0}\\ \mathbf{A}_{1}^{-1}d_{1}^{\intercal}d_{1}\mathbf{1}\otimes\nabla g_{S}\left(s \left(d_{1}^{\intercal}d_{1}\mathbf{p}\right)\right)\end{pmatrix},\]

Proof.: Since the theory of \(\mathbf{A}\)-gradients in section A.3 establishes that \(\nabla E=\mathbf{A}^{-1}\delta E\), it is only necessary to compute the \(L^{2}\)-gradients. First, letting \(\mathbf{x}=(\mathbf{q}\quad\mathbf{p})^{\intercal}\), it follows for the first definition of energy that

showing that \(\delta E(\mathbf{q},\mathbf{p})=(\mathbf{q}\quad\mathbf{p})^{\intercal}\), as desired. Moving to the metriplectic definitions, since each term of \(E,S\) has the same functional form, it suffices to compute the gradient of \(f\left(s\left(\mathbf{B}\mathbf{q}\right)\right)\) for some function \(f:\mathbb{R}^{n_{f}}\to\mathbb{R}\) and matrix \(\mathbf{B}:\mathbb{R}^{|\mathcal{V}|}\to\mathbb{R}^{|\mathcal{V}|}\). To that end, adopting the Einstein summation convention where repeated indices appearing up-and-down in an expression are implicitly summed, if \(1\leq a,b\leq n_{f}\) and \(1\leq i,j\leq|\mathcal{V}|\), we have

\[d\left(s(\mathbf{q})\right)=\sum_{i\in|\mathcal{V}|}d\mathbf{q}_{i}=\sum_{i\in| \mathcal{V}|}\delta_{i}^{j}d\mathbf{q}_{j}=\mathbf{1}^{j}dq_{j}^{a}\mathbf{e} _{a}=(\mathbf{1}\otimes\mathbf{I}):d\mathbf{q}=\nabla\left(s(\mathbf{q}) \right):d\mathbf{q},\]implying that \(\nabla s(\mathbf{q})=\mathbf{1}\otimes\mathbf{I}\). Continuing, it follows that

\[d\left(f\circ s\circ\mathbf{B}\mathbf{q}\right) =f^{\prime}\left(s\left(\mathbf{B}\mathbf{q}\right)\right)_{a}s^{ \prime}\left(\mathbf{B}\mathbf{q}\right)_{i}^{a}B^{ij}dq_{i}^{a}=f^{\prime} \left(s\left(\mathbf{B}\mathbf{q}\right)\right)_{a}\mathbf{e}_{a}\left(B^{ \intercal}\right)^{ij}\mathbf{1}_{j}dq_{i}^{a}\] \[=\left\langle\nabla f\left(s\left(\mathbf{B}\mathbf{q}\right) \right)\otimes\mathbf{B}^{\intercal}\mathbf{1},d\mathbf{q}\right\rangle=\left \langle\nabla\left(f\circ s\circ\mathbf{B}\mathbf{q}\right),d\mathbf{q}\right\rangle,\]

showing that \(\nabla\left(f\circ s\circ\mathbf{B}\right)\) decomposes into an outer product across modalities. Applying this formula to the each term of \(E,S\) then yields the \(L^{2}\)-gradients,

\[\delta E(\mathbf{q},\mathbf{p})=\begin{pmatrix}\mathbf{1}\otimes\nabla f_{E} \left(s(\mathbf{q})\right)\\ d_{0}d_{0}^{\intercal}\mathbf{1}\otimes\nabla g_{E}\left(s\left(d_{0}d_{0}^{ \intercal}\mathbf{p}\right)\right)\end{pmatrix},\quad\delta S(\mathbf{q}, \mathbf{p})=\begin{pmatrix}\mathbf{0}\\ d_{1}^{\intercal}d_{1}\mathbf{1}\otimes\nabla g_{S}\left(s\left(d_{1}^{ \intercal}d_{1}\mathbf{p}\right)\right)\end{pmatrix},\]

from which the desired \(\mathbf{A}\)-gradients follow directly. 

Finally, we can show that the degeneracy conditions for metriplectic structure are satisfied by the network in Section 4.

**Theorem A.16**.: _The degeneracy conditions \(\mathbf{L}\nabla S=\mathbf{M}\nabla E=\mathbf{0}\) are satisfied by the metriplectic bracket in Section A.6._

Proof.: This is a direct calculation using Theorem 3.1 and Proposition A.6. In particular, it follows that

\[\mathbf{L}\nabla S =\begin{pmatrix}\mathbf{0}&-d_{0}^{*}\\ d_{0}&\mathbf{0}\end{pmatrix}\begin{pmatrix}\mathbf{0}\\ \mathbf{A}_{1}^{-1}d_{1}^{\intercal}d_{1}\mathbf{1}\otimes\nabla g_{S}\left(s \left(d_{1}^{\intercal}d_{1}\mathbf{p}\right)\right)\end{pmatrix}\] \[=\begin{pmatrix}-\mathbf{A}_{0}^{-1}\left(d_{1}d_{0}\right)^{ \intercal}d_{1}\mathbf{1}\otimes\nabla g_{S}\left(s\left(d_{1}^{\intercal}d_{ 1}\mathbf{p}\right)\right)\end{pmatrix}=\begin{pmatrix}\mathbf{0}\\ \mathbf{0}\end{pmatrix},\] \[\mathbf{M}\nabla E =\begin{pmatrix}\mathbf{0}&\mathbf{0}\\ \mathbf{0}&\mathbf{A}_{1}d_{1}^{*}d_{1}\mathbf{A}_{1}\end{pmatrix}\begin{pmatrix} \mathbf{A}_{0}^{-1}\mathbf{1}\otimes\nabla f_{E}\left(s(\mathbf{q})\right)\\ \mathbf{A}_{1}^{-1}d_{0}d_{0}^{\intercal}\mathbf{1}\otimes\nabla g_{E}\left(s \left(d_{0}d_{0}^{\intercal}\mathbf{p}\right)\right)\end{pmatrix}\] \[=\begin{pmatrix}\mathbf{0}&\mathbf{0}\\ \mathbf{A}_{1}d_{1}^{*}(d_{1}d_{0})d_{0}^{\intercal}\mathbf{1}\otimes\nabla g_{ E}\left(s\left(d_{0}d_{0}^{\intercal}\mathbf{p}\right)\right)\end{pmatrix}=\begin{pmatrix} \mathbf{0}\\ \mathbf{0}\end{pmatrix},\]

since \(d_{1}d_{0}=0\) as a consequence of the graph calculus. These calculations establish the validity of the energy conservation and entropy generation properties seen previously in the manuscript body. 

**Remark A.17**.: _Clearly, this is not the only possible metriplectic formulation for GNNs. On the other hand, this choice is in some sense maximally general with respect to the chosen operators \(\mathbf{L},\mathbf{G}\), since only constants are in the kernel of \(d_{0}\) (hence there is no reason to include a nodal term in \(S\)), and only elements in the image of \(d_{1}^{\intercal}\) (which do not exist in our setting) are guaranteed to be in the kernel of \(d_{0}^{\intercal}\) for any graph. Therefore, \(\mathbf{M}\) is chosen to be essentially \(\mathbf{G}\) without the \(\Delta_{0}\) term, whose kernel is graph-dependent and hence difficult to design._

## Appendix B Experimental details and more results

This Appendix provides details regarding the experiments in Section 5, as well as any additional information necessary for reproducing them. We implement the proposed algorithms with Python and Pytorch[71] that supports CUDA. The experiments are conducted on systems that are equipped with NVIDIA RTX A 100 and V 100 GPUs. For NODEs capabilities, we use the Torchdiffeq library [16].

### Damped double pendulum

The governing equations for the damped double pendulum can be written in terms of four coupled first-order ODEs for the angles that the two pendula make with the vertical axis \(\theta_{1},\theta_{2}\) and their associated angular momenta \(\omega_{1},\omega_{2}\) (see [72]),

\[\dot{\theta}_{i} =\omega_{i},\qquad 1\leq i\leq 2, \tag{2}\] \[\dot{\omega}_{1} =\frac{m_{2}l_{1}\omega_{1}^{2}\sin\left(2\Delta\theta\right)+2m_{ 2}l_{2}\omega_{2}^{2}\sin\left(\Delta\theta\right)+2gm_{2}\cos\theta_{2}\sin \Delta\theta+2gm_{1}\sin\theta_{1}+\gamma_{1}}{-2l_{1}\left(m_{1}+m_{2}\sin^{2 }\Delta\theta\right)},\] (3) \[\dot{\omega}_{2} =\frac{m_{2}l_{2}\omega_{2}^{2}\sin\left(2\Delta\theta\right)+2 \left(m_{1}+m_{2}\right)l_{1}\omega_{1}^{2}\sin\Delta\theta+2g\left(m_{1}+m_{ 2}\right)\cos\theta_{1}\sin\Delta\theta+\gamma_{2}}{2l_{2}\left(m_{1}+m_{2} \sin^{2}\Delta\theta\right)}, \tag{4}\]where \(m_{1},m_{2},l_{1},l_{2}\) are the masses resp. lengths of the pendula, \(\Delta\theta=\theta_{1}-\theta_{2}\) is the (signed) difference in vertical angle, \(g\) is the acceleration due to gravity, and

\[\gamma_{1} =2k_{1}\dot{\theta}_{1}-2k_{2}\dot{\theta}_{2}\cos\Delta\theta.\] \[\gamma_{2} =2k_{1}\dot{\theta}_{1}\cos\Delta\theta-\frac{2\left(m_{1}+m_{2} \right)}{m_{2}}k_{2}\dot{\theta}_{2},\]

for damping constants \(k_{1},k_{2}\).

Dataset.A trajectory of the damped double pendulum by solving an initial value problem associated with the ODE 2. The initial condition used is (1.0, \(\pi\)/2, 0.0, 0.0), and the parameters are \(m_{1}=m_{2}=1,g=1,l_{1}=1,l_{2}=0.9,k_{1}=k_{2}=0.1\). For time integrator, we use the TorchDiffeq library [16] with Dormand-Prince 5 (DOPRI5) as the numerical solver. The total simulation time is 50 (long enough for significant dissipation to occur), and solution snapshots are collected at 500 evenly-spaced temporal indices.

To simulate the practical case where only positional data for the system is available, the double pendulum solution is integrated to time \(T=50\) (long enough for significant dissipation to occur) and post-processed once the angles and angular momenta are determined from the equations above, yielding the \((x,y)\)-coordinates of each pendulum mass at intervals of \(0.1\)s. This is accomplished using the relationships

\[x_{1} =l_{1}\sin\theta_{1}\] \[y_{1} =-l_{1}\cos\theta_{1}\] \[x_{2} =x_{1}+l_{2}\sin\theta_{2}=l_{1}\sin\theta_{1}+l_{2}\sin\theta_{2}\] \[y_{2} =y_{1}-l_{2}\cos\theta_{2}=-l_{1}\cos\theta_{1}-l_{2}\cos\theta_{2}.\]

The double pendulum is then treated as a fully connected three-node graph with positional coordinates \(\mathbf{q}_{i}=(x_{i},y_{i})\) as nodal features, and relative velocities \(\mathbf{p}_{\alpha}=(d_{0}\mathbf{q})_{\alpha}\) as edge features. Note that the positional coordinates \((x_{0},y_{0})=(0,0)\) of the anchor node are held constant during training. To allow for the necessary flexibility of coordinate changes, each architecture from Section 4 makes use of a message-passing feature encoder before time integration, acting on node features and edge features separately, with corresponding decoder returning the original features after time integration.

To elicit a fair comparison, both the NODE and NODE+AE architectures are chosen to contain comparable numbers of parameters to the bracket architectures (\(\sim 30\)k), and all networks are trained for 100,000 epochs. For each network, the configuration of weights producing the lowest overall error during training is used for prediction.

Hyperparameters.The networks are trained to reconstruct the node/edge features in mean absolute error (MAE) using the Adam optimizer [73]. The NODEs and metriplectic bracket use an initial learning rate of \(10^{-4}\), while the other models use an initial learning rate of \(10^{-3}\). The width of the hidden layers in the message passing encoder/decoder is 64, and the number of hidden features for nodes/edges is 32. The time integrator used is simple forward Euler.

Network architectures.The message passing encoders/decoders are 3-layer MLPs mapping, in the node case, nodal features and their graph derivatives, and in the edge case, edge features and their graph coderivatives, to a hidden representation. For the bracket architectures, the attention mechanism used in the learnable coderivatives is scaled dot product. The metriplectic network uses 2-layer MLPs \(f_{E},g_{E},g_{S}\) with scalar output and hidden width 64. For the basic NODE, node and edge features are concatenated, flattened, and passed through a 4-layer fully connected network of width 128 in each hidden layer, before being reshaped at the end. The NODE+AE architecture uses a 3-layer fully connected network which operates on the concatenated and flattened latent embedding of size \(32*6=192\), with constant width throughout all layers.

### Mujoco

We represent an object as a fully-connected graph, where a node corresponds to a body part of the object and, thus, the nodal feature corresponds to a position of a body part or joint. To learn the dynamics of an object, we again follow the encoder-decoder-type architecture considered in the double-pendulum experiments. First we employ a node-wise linear layer to embed the nodal feature into node-wise hidden representations (i.e., the nodal feature \(\mathbf{q}_{i}\) corresponds to a position of a body part or an angle of a joint.). As an alternative encoding scheme for the nodal feature, in addition to the position or the angle, nodal velocities are considered as additional nodal features, i.e., \(\mathbf{q}_{i}=(q_{i},v_{i})\). The experimental results of the alternative scheme is represented in the following section B.2.2.

The proposed dynamics models also require edge features (e.g., edge velocity), which are not presented in the dataset. Thus, to extract a hidden representation for an edge, we employ a linear layer, which takes velocities of the source and destination nodes of the edge as an input and outputs edge-wise hidden representations, i.e., the edge feature correspond to a pair of nodal velocities \(\mathbf{p}_{\alpha}=(v_{\text{src}(\alpha)},v_{\text{dst}(\alpha)})\), where \(v_{\text{src}(\alpha)}\) and \(v_{\text{dst}(\alpha)}\) denote velocities of the source and destination nodes connected to the edge.

The MuJoCo trajectories are generated in the presence of an actor applying controls. To handle the changes in dynamics due to the control input, we introduce an additive forcing term, parameterized by an MLP, to the dynamics models, which is a similar approach considered in dissipative SymODEN [32]. In dissipative SymODEN, the forcing term is designed to affect only the change of the generalized momenta (also known as the port-Hamiltonian dynamics [74]). As opposed to this approach, our proposed forcing term affects the evolution of both the generalized coordinates that are defined in the latent space. Once the latent states are computed at specified time indices, a node-wise linear decoder is applied to reconstruct the position of body parts of the object. Then the models are trained based on the data matching loss measured in mean-square errors between the reconstructed and the ground-truth positions.

#### b.2.1 Experiment details

We largely follow the experimental settings considered in [23].

Dataset.As elaborated in [23], the standard Open AI Gym [75] environments preprocess observations in ad-hoc ways, e.g., Hopper clips the velocity observations to \([-10,10]^{d}\). Thus, the authors in [23] modified the environments to simply return the position and the velocity \((q,v)\) as the observation and we use the same dataset, which is made publicly available by the authors. The dataset consists of training and test data, which are constructed by randomly splitting the episodes in the replay buffer into training and test data. Training and test data consist of \(\sim\)40K and \(\sim\)300 or \(\sim\) 85 trajectories, respectively. For both training and test data, we include the first 20 measurements (i.e., 19 transitions) in each trajectory.

Hyperparameters.For training, we use the Adam optimizer [73] with the initial learning rate 5e-3 and weight decay 1e-4. With the batch size of 200 trajectories, we train the models for 256 epochs. We also employ a cosine annealing learning rate scheduler with the minimum learning rate 1e-6. For time integrator, we use the Torchdiffeq library with the Euler method.

Network architectures.The encoder and decoder networks are parameterized as a linear layer and the dimension of the hidden representations is set to 80. For attention, the scaled dot-product attention is used with 8 heads and the embedding dimension is set to 16. The MLP for handling the

Figure 5: [Double pendulum] Trajectories of \(\mathbf{q}\) and \(\mathbf{p}\): ground-truth (solid lines) and predictions of the metriplectic bracket model (dashed lines). The evolution of the energy \(E\) and the entropy \(S\) over the simulation time. Note that slight fluctuations appear in \(E\) due to the fact that forward Euler is not a symplectic integrator.

forcing term consists of three fully-connected layers (i.e., input, output layers and one hidden layer with 128 neurons). The MLP used for parameterizing the "black-box" NODEs also consists of three fully-connected layers with 128 neurons in each layer.

#### b.2.2 Additional results

Figure 6 reports the loss trajectories for all considered dynamics models. For the given number of maximum epochs (i.e., 256), the Hamiltonian and double bracket models tend to reach much lower training losses (an order of magnitude smaller) errors than the NODE and Gradient models do. The metriplectic model produces smaller training losses compared to the NODE and gradient models after a certain number of epochs (e.g., 100 epochs for Hopper).

In the next set of experiments, we provide not only positions/angles of body parts as nodal features, but also velocities of the body parts as nodal features (i.e., \(\mathbf{q}_{i}=(q_{i},v_{i})\)). Table 6 reports the relative errors measured in L2-norm; again, the Hamiltonian, double bracket, and metriplectic outperform other dynamics models. In particular, the metriplectic bracket produces the most accurate predictions in the Hopper and Swimmer environments. Figure 7 reports the loss trajectories for all considered models. Similar to the previous experiments with the position as the only nodal feature, the Hamiltonian and Double bracket produces the lower training losses than the NODE and Gradient models do. For the Hopper and Swimmer environments, however, among all considered models, the metriplectic model produces the lowest training MSEs after 256 training epochs.

### Node classification

To facilitate comparison with previous work, we follow the experimental methodology of [61].

\begin{table}
\begin{tabular}{l l l l} \hline \hline Dataset & **HalfCheetah** & **Hopper** & **Swimmer** \\
**NODE+AE** & 0.0848 \(\pm\) 0.0011 & 0.0421 \(\pm\) 0.0041 & 0.0135 \(\pm\) 0.00082 \\
**Hamiltonian** & 0.0403 \(\pm\) 0.0052 & 0.0294 \(\pm\) 0.0028 & 0.0120 \(\pm\) 0.00022 \\
**Gradient** & 0.0846 \(\pm\) 0.00358 & 0.0490 \(\pm\) 0.0013 & 0.0158 \(\pm\) 0.00030 \\
**Double Bracket** & 0.0653 \(\pm\) 0.010 & 0.0274 \(\pm\) 0.00090 & 0.0120 \(\pm\) 0.00060 \\
**Metriplectic** & 0.0757 \(\pm\) 0.0021 & 0.0269 \(\pm\) 0.00035 & 0.0114 \(\pm\) 0.00067 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Relative errors of the network predictions of the MuJoCo environments on the test set, reported as avg\(\pm\)stdev over 4 runs.

Figure 6: [Mujoco] Train MSE over epoch for all considered dynamics models. For the nodal feature, only the position or the angle of the body part/joint is considered.

#### b.3.1 Experiment details

Datasets.We consider the three well-known citation networks, Cora [58], Citeseer [59], and Pubmed [60]; the proposed models are tested on the datasets with the original fixed Planetoid traing/test splits, as well as random train/test splits. In addition, we also consider the coauthor graph, CoauthorCS [61] and the Amazon co-purchasing graphs, Computer and Photo [62]. Table 7 provides some basic statistics about each dataset.

HyperparametersThe bracket architectures employed for this task are identical to those in Section 4 except for that the right-hand side of the Hamiltonian, gradient, and double bracket networks is scaled by a learnable parameter \(\operatorname{Sigmoid}\left(\alpha\right)>0\), and the matrix \(\mathbf{A}_{2}=\mathbf{I}\) is used as the inner product on 3-cliques. It is easy to verify that this does not affect the structural properties or conservation character of the networks. Nodal features \(\mathbf{q}_{i}\) are specified by the datasets, and edge features \(\mathbf{p}_{\alpha}=\left(d_{0}\mathbf{q}\right)_{\alpha}\) are taken as the combinatorial gradient of the nodal features. In order to determine good hyperparameter configurations for each bracket, a Bayesian search is conducted using Weights and Biases [76] for each bracket and each dataset using a random 80/10/10 train/valid/test split with random seed 123. The number of runs per bracket was 500 for CORA, CiteSeer, and PubMed, and 250 for CoauthorCS, Computer, and Photo. The hyperparameter configurations leading to the best validation accuracy are used when carrying out the experiments in Table 4 and Table 5.

Specifically, the hyperparameters that are optimized are as follows: initial learning rate (from 0.0005 to 0.05), number of training epochs (from 25 to 150), method of integration (rk4 or dopri5), integration time (from 1 to 5), latent dimension (from 10 to 150 in increments of 10), pre-attention mechanism \(\tilde{a}\) (see below), positive function \(f\) (either \(\exp\) or \(\operatorname{Squareplus}\)), number of pre-attention heads (from 1 to 15, c.f. Remark A.14), attention embedding dimension (from \(1\times\) to \(15\times\) the number of heads), weight decay rate (from 0 to 0.05), dropout/input dropout rates (from 0 to 0.8), and the MLP activation function for the metriplectic bracket (either \(\operatorname{relu}\), \(\tanh\), or \(\operatorname{squareplus}\)). The pre-attention is chosen

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Dataset & **Cora** & **Citeseer** & **PubMed** & **CoauthorCS** & **Computer** & **Photo** \\ Classes & 7 & 6 & 3 & 15 & 10 & 8 \\ Features & 1433 & 3703 & 500 & 6805 & 767 & 745 \\ Nodes & 2485 & 2120 & 19717 & 18333 & 13381 & 7487 \\ Edges & 5069 & 3679 & 44324 & 81894 & 245778 & 119043 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Dataset statistics.

Figure 7: [Mujoco] Train MSE over epoch for all considered dynamics models. For the nodal feature, along with the position or the angle of the body part/joint, the node velocities are also considered.

from one of four choices, defined as follows:

\[\tilde{a}\left(\mathbf{q}_{i},\mathbf{q}_{j}\right) =\frac{\left(\mathbf{W}_{K}\mathbf{q}_{i}\right)^{\intercal}\mathbf{ W}_{Q}\mathbf{q}_{j}}{d}\] scaled dot product, \[\tilde{a}\left(\mathbf{q}_{i},\mathbf{q}_{j}\right) =\frac{\left(\mathbf{W}_{K}\mathbf{q}_{i}\right)^{\intercal}\mathbf{ W}_{Q}\mathbf{q}_{j}}{\left|\mathbf{W}_{K}\mathbf{q}_{i}\right|\left|\mathbf{W}_{Q} \mathbf{q}_{j}\right|}\] cosine similarity, \[\tilde{a}\left(\mathbf{q}_{i},\mathbf{q}_{j}\right) =\frac{\left(\mathbf{W}_{K}\mathbf{q}_{i}-\overline{\mathbf{W}_{K }\mathbf{q}_{i}}\right)^{\intercal}\left(\mathbf{W}_{Q}\mathbf{q}_{j}- \overline{\mathbf{W}_{Q}\mathbf{q}_{j}}\right)}{\left|\mathbf{W}_{K}\mathbf{q }_{i}-\overline{\mathbf{W}_{K}\mathbf{q}_{i}}\right|\left|\mathbf{W}_{Q} \mathbf{q}_{j}-\overline{\mathbf{W}_{Q}\mathbf{q}_{j}}\right|},\] Pearson correlation \[\tilde{a}\left(\mathbf{q}_{i},\mathbf{q}_{j}\right) =\left(\sigma_{u}\sigma_{x}\right)^{2}\exp\left(-\frac{\left| \mathbf{W}_{K}\mathbf{u}_{i}-\mathbf{W}_{Q}\mathbf{u}_{j}\right|^{2}}{2\ell_{ u}^{2}}\right)\exp\left(-\frac{\left|\mathbf{W}_{K}\mathbf{x}_{i}-\mathbf{W}_{Q} \mathbf{x}_{j}\right|^{2}}{2\ell_{x}^{2}}\right)\!,\!\text{exponential kernel}\]

Network architectures.The architectures used for this experiment follow that of GRAND [49], consisting of the learnable affine encoder/decoder networks \(\phi,\psi\) and learnable bracket-based dynamics in the latent space. However, recall that the bracket-based dynamics require edge features, which are manufactured as \(\mathbf{p}_{\alpha}=\left(d_{0}\mathbf{q}\right)_{\alpha}\). In summary, the inference procedure is as follows:

\[\mathbf{q}(0) =\phi(\mathbf{q}) (\text{nodal feature encoding}),\] \[\mathbf{p}(0) =d_{0}\mathbf{q}(0) (\text{edge feature manufacturing}),\] \[\left(\mathbf{q}(T),\mathbf{p}(T)\right) =\left(\mathbf{q}(0),\mathbf{p}(0)\right)+\int_{0}^{T}\left( \mathbf{\hat{q}},\mathbf{\hat{p}}\right)\mathrm{d}t, \left(\text{latent dynamics}\right)\] \[\tilde{\mathbf{q}} =\psi\left(\mathbf{q}(T)\right), (\text{nodal feature decoding})\] \[\mathbf{y} =c(\tilde{\mathbf{q}}). (\text{class prediction})\]

Training is accomplished using the standard cross entropy

\[H\left(\mathbf{t},\mathbf{y}\right)=\sum_{i=1}^{\left|\mathcal{V}\right|} \mathbf{t}_{i}^{\intercal}\log\mathbf{y}_{i},\]

where \(\mathbf{t}_{i}\) is the one-hot truth vector corresponding to the \(i^{\text{th}}\) node. In the case of the metriplectic network, the networks \(f_{E},g_{E},g_{S}\) are 2-layer MLPs with hidden dimension equal to the latent feature dimension and output dimension 1.

#### b.3.2 Additional depth study

Here we report the results of a depth study on Cora with the Planetoid train/val/test split. Table 9 shows the train/test accuracy of the different bracket architectures under two different increased depth conditions, labeled Task 1 and Task 2, respectively. Task 1 refers to fixing the integration step-size at \(\Delta t=1\) and integrating to a variable final time \(T\), while Task 2 instead fixes the final time \(T\) to the value identified by the hyperparameter search (see Table 8) and instead varies the step-size \(\Delta t\). Notice that both tasks involve repeatedly composing the trained network and hence simulate increasing depth, so that any negative effects of network construction such as oversmoothing, oversquashing, or vanishing/exploding gradients should appear in both cases. For more information, Table 10 provides a runtime comparison corresponding to the depth studies in Table 9.

Observe that every bracket-based architecture exhibits very stable performance in Task 2, where the final time is held fixed while the depth is increased. This suggests that our proposed networks are dynamically stable and effectively mitigate negative effects like oversmoothing which are brought by repeated composition and often seen in more standard GNNs. Interestingly, despite success on Task 2, only the gradient and metriplectic architectures perfectly maintain or improve their performance during the more adversarial Task 1 where the final time is increased with a fixed step-size. This suggests that, without strong diffusion, the advection experienced during conservative dynamics has the potential to radically change label classification over time, as information is moved through the feature domain in a loss-less fashion.

**Remark B.1**.: _It is interesting that the architecture most known for oversmoothing (i.e., gradient) exhibits the most improved classification performance with increasing depth on Task 1. This is perhaps due to the fact that the gradient system decouples over nodes and edges, while the others

[MISSING_PAGE_FAIL:31]