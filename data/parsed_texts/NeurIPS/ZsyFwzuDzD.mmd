# Revisiting, Benchmarking and Understanding Unsupervised Graph Domain Adaptation

Meihan Liu\({}^{1}\), Zhen Zhang\({}^{2}\)1, Jiachen Tang\({}^{1}\), Jiajun Bu\({}^{1}\), Bingsheng He\({}^{2}\), Sheng Zhou\({}^{1}\)

\({}^{1}\)Zhejiang Key Laboratory of Accessible Perception and Intelligent Systems,

College of Computer Science, Zhejiang University \({}^{2}\)National University of Singapore

{lmh_zju,tangjc,bjj,zhousheng_zju}@zju.edu.cn

zhen@nus.edu.sg, hebs@comp.nus.edu.sg

Footnote 1: Corresponding author

###### Abstract

Unsupervised Graph Domain Adaptation (UGDA) involves the transfer of knowledge from a label-rich source graph to an unlabeled target graph under domain discrepancies. Despite the proliferation of methods designed for this emerging task, the lack of standard experimental settings and fair performance comparisons makes it challenging to understand which and when models perform well across different scenarios. To fill this gap, we present the first comprehensive benchmark for unsupervised graph domain adaptation named GDABench, which encompasses 16 algorithms across diverse adaptation tasks. Through extensive experiments, we observe that the performance of current UGDA models varies significantly across different datasets and adaptation scenarios. Specifically, we recognize that when the source and target graphs face significant distribution shifts, it is imperative to formulate strategies to effectively address and mitigate graph structural shifts. We also find that with appropriate neighbourhood aggregation mechanisms, simple GNN variants can even surpass state-of-the-art UGDA baselines. To facilitate reproducibility, we have developed an easy-to-use library PyGDA for training and evaluating existing UGDA methods, providing a standardized platform in this community. Our source codes and datasets can be found at https://github.com/pygda-team/pygda.

## 1 Introduction

The last decade has witnessed significant advancements in Graph Neural Networks (GNNs) with their successful applications spanning various fields [1; 2], including social network analysis [3; 4], protein interaction prediction [5], and traffic flow forecasting [6], etc. However, the presence of distribution shifts [7] and label scarcity in real-world graph data impedes the ability of existing GNN models to adapt to new domains [8]. To addresses this challenge, Unsupervised Graph Domain Adaptation (UGDA) has become an important solution for transferring knowledge from a labeled source graph to an unlabeled target graph. This powerful paradigm has been widely studied, unlocking broader application for graph neural networks.

Despite a wide range of researches of UGDA have been developed [9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26], the understanding of their capabilities and limitations is inadequate due to the following reasons:

* **Inadequate Evaluation of Domain Distribution Discrepancies.** The distribution shifts in node attributes, graph structures, and label proportions between graphs will significantly influence the adaptation performance and result in various adaptation scenarios [26; 27]. However, the typesand magnitudes of distribution discrepancies among different domains have not been thoroughly evaluated and discussed, which makes it challenging to understand the robustness and efficacy of current methods.
* **Lack of Standard, Fair, and Comprehensive Comparisons.** The utilization of distinct datasets, varying data processing methodologies, and divergent data partitioning strategies among existing domain adaptation models results in incomparability across different findings [9; 28; 17]. Furthermore, they are mainly evaluated against limited baselines with constrained scenarios, such as social networks or citation networks, which lack validation of the model's capability in more diverse or complex applications.
* **Limited Investigation on GNN Inherent Transferability.** Despite the advancements made by existing UGDA algorithms, it is still unclear how data shift impose challenges on GNN and how to unleash the transferability power for GNN. Due to the non-IID nature of graph data, the aggregation architectures and aggregation scores affect the underlying distribution of latent representations generated by GNN. When there exists significant structural difference between the source and target domains, such as variations in the degree [29; 30] or differences in subgraph patterns [23; 31], the information aggregation capability of GNN will be directly affected [32; 25; 31]. Thus, understanding the key components that affect adaptation in GNN will be crucial for enhancing GNN's transferabililty, which is still an open problem.

To fill these gaps, we revisit existing UGDA algorithms and conduct a comprehensive benchmark named GDABench. Specifically, GDABench includes 16 state-of-the-art UGDA models and diverse real-world graph datasets covering node attributes, graph structures, and label proportion shifts. Additionally, we also explore the limits of GNN transferability by combining 7 GNN variants with 2 domain alignment and 3 unsupervised graph learning techniques. Our work is the first to provide a rigorous empirical analysis of how various aggregation mechanisms influence alignments in domain adaptation task. Through comprehensive experiments, we observe that: (1) the performance of current UGDA models varies greatly across different datasets and adaptation scenarios; (2) it is crucial to develop tailored strategies to address graph structural shifts, especially when the distribution discrepancies are significant; (3) the GNN's transferability in UGDA heavily relies on two factors: aggregation scope and aggregation architecture, which are influenced by the severity of label shift and the level of graph heterophily, etc; (4) the inherent adaptability of GNNs is largely underestimated by existing methods, which motivates the exploration of a simple yet effective model that fully leverages the core property of GNN. More insights can be found in Section 5.

In summary, our main contributions are as follows:

* We introduce GDABench, the first comprehensive benchmark for unsupervised graph domain adaptation. It includes 16 recent state-of-the-art methods across various real-world datasets with diverse range of adaptation tasks.
* To explore the capability and limitations of exiting UGDA models, we systematically evaluate existing algorithms and investigate the underlying transferability for GNN. With these findings, we reveal a simple yet effective method that can even surpass existing UGDA algorithms.
* We develop an easy-to-use library PyGDA to alleviate the workload of researchers when conducting experiments. Furthermore, users can easily construct their own models or datasets with minimal effort.

The source codes of our benchmark are available at https://github.com/pygda-team/pygda/tree/main/benchmark, which provide unified APIs and adopt consistent data processing as well as data splitting approaches for fair comparisons.

## 2 Preliminaries and Related Work

### Problem Definition

Consider a graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) with \(n\) nodes and \(m\) edges. The node feature matrix, denoted by \(\mathbf{X}=\{x_{v}|v\in\mathcal{V}\}\in\mathbb{R}^{n\times d}\), contains attribute vector for each node, where \(d\) represents the dimensionality of the attributes. We denote adjacency matrix as \(\mathbf{A}\in\mathbb{R}^{n\times n}\), where \(\mathbf{A}_{i,j}=1\) indicates the presence of an edge \(e_{i,j}\in\mathcal{E}\) connecting node \(v_{i}\) and \(v_{j}\), and \(\mathbf{A}_{i,j}=0\) otherwise.

\(\mathbf{Y}\in\mathbb{R}^{N\times C}\) represents the node label matrix, where \(C\) is the category of node labels. We focus on Unsupervised Graph Domain Adaptation (UGDA) on classification tasks. It is a non-trivial task due to the complex domain shift between source and target graphs. Formally, given a labeled source graph \(\mathcal{G}_{S}=(\mathcal{V}_{S},\mathcal{E}_{S},\mathcal{Y}_{S})\) and an unlabeled target graph \(\mathcal{G}_{T}=(\mathcal{V}_{T},\mathcal{E}_{T})\) with the data shift that \(\mathbb{P}_{S}(\mathcal{G})\neq\mathbb{P}_{T}(\mathcal{G})\). The goal is to train a graph neural network model \(h:\mathcal{G}\rightarrow\mathcal{Y}\) that utilizes the labeled source graph \(\mathcal{G}^{s}\) and the unlabeled target graph \(\mathcal{G}^{t}\) in order to make accurate label predictions for the target graph \(\mathcal{G}^{t}\).

### Related Work

**Classical Methods.** There are two classical categories to reduce the domain discrepancy [33; 34]: minimizing pre-defined probability discrepancy metrics [35; 36] and adversarial learning techniques [37; 38; 39; 40]. For the pre-defined probability discrepancy metric minimization models, node representations are initially derived from the encoder, after which domain-invariant representations are acquired by minimizing probability discrepancy distances, such as MMD [41], CMD [36], etc. Rather than directly minimizing domain discrepancies, some approaches integrate the encoder with a domain classifier that predicts the source domain of each representation. For example, DANN [37] facilitates domain invariant learning (DIRL) to distinguish between source and target samples in the latent space. Building upon this framework, WDGRL [42] replaces the domain classifier with a network that learns an approximate Wasserstein distance. These classical methods are designed for CV and NLP tasks, where samples are independently and identically distributed [43]. However, marginal alignment of node representations in non-graph DA research is insufficient for graph-structured data, of which the distribution shift becomes more complex due to the interconnection among different nodes.

**Specialized Methods for Graph Data.** To tackle the unique challenges of knowledge transfer in graphs, several approaches have been proposed [9; 11; 44; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26]. The key idea behind existing work on node-level UGDA is to leverage node representations as intermediaries while minimizing domain shift through adversarial learning. This kind of methods incorporate classical DA methods with deep node embedding by designing a special encoder to learn transferable features. DANE [10] uses shared weight GCNs to get node representations and then handles distribution shift via least square generative adversarial network. ACDNE [11] employs two feature extractors to simultaneously maintain both attributed affinity and topological proximity through a deep network embedding module. Additionally, it integrates a domain classifier to enhance the label-discriminative nature of the node representations. UDAGCN [12] introduces a dual graph convolutional network enhanced by an attention mechanism, allowing it to leverage both local and global consistency for improved graph representation learning. ASN [44] distinguishes between domain-private and domain-shared information while integrating both local and global consistency to effectively capture network topology information. AdaGCN [13] utilizes GCNs to merge network topology with adversarial domain adaptation, effectively enhancing graph convolution processes. CWGCN [22] introduces a two-step correntropy-induced Wasserstein GCN. This method first eliminates noisy nodes from the source graph and subsequently learns the target GCN by extending the Wasserstein distance. SA-GDA [23] introduces a spectral augmentation module aimed at improving node representation learning by integrating spectral information from the target domain into the source domain. DMGNN [21] utilizes a GNN encoder equipped with

Figure 1: This timeline illustrates the diverse UGDA algorithms revisited in this paper. All of them are incorporated into our PyGDA library. More details are shown in Section 2 and Appendix C

dual feature extractors to distinguish between ego-embedding learning and neighbor-embedding learning. Following this, a label propagation node classifier is applied to enhance the accuracy of label predictions. DGDA [24] approaches graph domain adaptation from a generative perspective, breaking down the generation process into three components: semantic latent variables, domain latent variables, and random latent variables.

Despite the progress made by the aforementioned UGDA models, such solutions still struggle to address the complex domain shift present in real-world graph data. The complex distribution shift between graphs usually combine node attribute shift, graph structure shift and label shift. Thus, to deal with the distinct effects of distribution shifts caused by graph structures, StruRW [17] investigates different types of distribution shifts of graph-structured data and reweighs edges in the source graph to reduce the conditional shift of neighborhoods. Based on this work, PairAlign [26] addresses conditional structure shifts by recalibrating the influence of neighboring nodes using edge weights, while also modifying the classification loss through label weights to tackle label shifts. Except for reweighting, JHGDA [18] develops a hierarchical pooling model that extracts meaningful and adaptive structures at various levels. This model simultaneously minimizes both marginal and class conditional distribution shifts across each hierarchical layer. KBL [19] redefines the aggregation mechanism as a process of learning a knowledge-enhanced posterior distribution for target domains, facilitating knowledge transfer by linking informative samples across domains.

Instead of simply using GNN as a node embedding module, some methods devote effort to empirically and theoretically studying the role of GNN within domain adaptation [15; 16; 25]. This allows the UGDA method to be precisely customized based on the property of GNN, thereby enhancing their performance in graph domain adaptation. GRADE [15] introduces graph subtree discrepancy as a metric to measure the graph distribution shift by connecting GNNs with WL subtree kernel [45]. SpecReg [16] finds that the OT-based bound for graph is closely coupled with the Lipschitz constant of GNN and proposes spectral regularization to modulate the Lipschitz constant to restrict the target risk bound. A2GNN [25] further investigates the GNN's underlying generalization capability behind its architecture and finds propagation operation plays a pivotal role in the adaptation procedure. Based on this observation, A2GNN proposes a simple yet effective GNN framework, which stacks more propagation layers on target branch. The timeline of UGDA algorithms covered in GDABench is shown in Figure 1 and more detailed descriptions are provided in Appendix C.

## 3 Datasets

We have carefully selected 5 widely used public datasets that showcase a wide spectrum of distribution shifts across graphs for the node classification task. These include **Airport** which consists of three domains: Brazil (B), Europe (E) and USA (U); **Blog** that includes two domains: Blog1 (B1) and Blog2 (B2); **ArnetMiner** which encompasses three domains: DBLpv7 (D), Citationv1 (C) and ACMv9 (A); **Twitch** that includes six domains: Germany (DE), England (EN), Spain (ES), France (FR), Portugal (PT) and Russia (RU); **MAG** that includes six domains like CN, US, JP, FR, RU, and DE. The selection criteria for these datasets are primarily based on three factors: the complexity of the distribution shift, the scale of the dataset, and the potential for downstream applications. From these datasets, we have included a comprehensive collection, comprising 74 distinct source-target adaptation pairs. Detailed information about each dataset is provided in Table 1, while the statistical methods used to quantify the types of domain shifts exhibited in the dataset are presented in Appendix B. The chosen datasets possess the following characteristics:

* **Wide range of distribution shift.** The graph distribution shifts between source and target domains can largely fall into three categories: feature shift, structure shift and label shift [32; 46; 26]. Our GDABench datasets encompass a diverse range of three distribution shifts across domains in varying degrees. The details are illustrated in Table 1 and more statistics are given in Appendix B. Specifically, the domains within Airport are dominated by structure shift, while domains in Blog, ArnetMiner, Twitch and MAG are affected by all kinds of shifts with different degrees.
* **Different scales with variant spans.** We categorize the size of the dataset by the average number of domain nodes. The small size (S) covers nodes below 5 thousand. The medium size (M) cover nodes below 10 thousand. The large size (L) covers nodes from 10 thousand to hundred thousand. Smaller datasets exhibit smaller differences in domain sizes, and vice versa. In the Blog and Airport, the size difference between the largest and smallest domains is less than 1000 nodes. In the case of ArnetMiner and Twitch, this difference ranges between 1,000 and 10,000. For the MAG, this difference ranges between 10,000 to 100,000. This allows us to understand the impacts of varying domain size on the efficacy of adaptation task.
* **Various downstream application scenarios.** The GDABench datasets encompass multiple application scenarios, including citation relationships (ArnetMiner and MAG), social media interactions (Blog and Twitch) and routine connections (Airport). Specifically, in ArnetMiner and MAG, nodes represent academic papers and edges indicate citation relationship. ArnetMiner groups domains by publisher, while MAG by country. Blog and Twitch capture friendship within blog and gamer networks, respectively. Airport delineates routines connections, where airports serve as nodes connected by flight routes.

## 4 Compared Models

**Specialized UGDA Methods.** This group includes specifically designed algorithms for graph domain adaptation task. We compare 16 models including (1) nine methods incorporating classicial DA methods with deep node embedding: DANE [10], \(\Lambda\)CDNE [11], UDAGCN [12], ASN [44], AdaGCN [13], CWGCN [22], SAGDA [23], DMGNN [21] and DGDA [24]; (2) four methods tailored for graph structure shift: StruRW [17], JHGDA [18], KBL [19] and PairAlign [26]; and (3) three methods based on domain adaptive message passing: GRADE [15], SpecReg [16], and A2GNN [25].

**SimGDA: Vanilla DA with GNN Variants.** To understand the inherent transferability of GNN, we delve into its aggregation process by decoupling it into two key perspectives: _how to aggregate_ and _what to aggregate_. For _how to aggregate_, we consider five types of aggregators, including sum aggregator [47], mean aggregator [48], aggregate with weighted neighbours (GAT) [49] and aggregate with discriminative neighbours (GIN) [50]. For _what to aggregate_, we consider three aspects in terms of the hop-count of neighbours: (1) GNN without neighbours, where graph structure is not considered (degenerating to MLP); (2) GNN with one-hop neighbours; and (3) GNN with multi-hop neighbours. To avoid the over-smooth problem, we also add residual connections to enhance its modeling power [51, 47]. For alignment, we consider two widely used models for domain-invariant feature learning from computer vision: domain distance metric MMD [35] and adversarial learning DANN [37]. Among them, MMD proposes to match the distribution in the latent space through maximum mean discrepancy [41], while DANN introduces an adversarial objective to distinguish source and target samples in the latent space. We use one-layer GCN [47] as a control and create six GNN variants by altering only one module each time. Then, we get 14 models by combining these variants with two vanilla DA methods, abbreviated these 14 models as SimGDA.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline \hline Dataset & Size & Feature Shift & Structure Shift & Label Shift & \# Domains & \# Labels & \# Homo \\ \hline Airport & S & - & \(\bigcirc\) & \(\bigcirc\) & 3 & 4 & 0.52 \\ Blog & S & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & 2 & 6 & 0.40 \\ ArnetMiner & M & \(\bigcirc\) & \(\bigcirc\) & 3 & 5 & 0.83 \\ Twitch & M & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & 6 & 2 & 0.59 \\ MAG & L & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & 6 & 20 & 0.58 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Datasets used in GDABench reflecting a wide range of distribution shifts. ‘-’ indicates no data shift exists. Circles (\(\bigcirc\), \(\bigcirc\) and \(\bigcirc\)) represent the degree of the corresponding shift between domains and Airport does not contain node features. The magnitude of shift is directly proportional to the filling area of the circle. The statistic manners and more details are provided in Appendix B.

Figure 2: The combination process of SimGDA / SimGDA+.

an end-to-end manner: (1) Information Maximization (IM) [52; 53; 20]. Ideally, an accurate prediction for the target domain should exhibit individual certainty while maintaining global diversity. To accomplish this, we minimize the entropy for each individual sample while maximizing the entropy across classes. (2) Graph AutoEncoder (AE) [54; 55; 56]. Graph autoencoders encode nodes into a latent vector space and reconstruct the graph data from the encoded latent space. After obtaining target graph node representations, we employ a distinct decoder to reconstruct target graph structure. (3) Graph Contrastive Learning (CL) [57; 58; 59; 60]: Graph contrastive learning methods maximize mutual information between augmented instances of the same object (e.g., node).

As changing the graph structure may affect the estimation of structure shift, we take random attribute masking to create augmented instances for each target node. Following previous works [58; 60], we utilize a one-layer perceptron (MLP) as projection head to map augmented representations to the shared latent space and then calculate contrastive loss based on normalized temperature-scaled cross entropy loss (NT-Xent) [61]. As a result, we get 42 combined models, which integrates 14 SimGDA variants with 3 unsupervised techniques. We collectively refer to these combined models as **SimGDA+**, and the combination process is shown in Figure 2.

## 5 Experimental Results and Analyses

In this section, we study the experimental results of all the models. We first provide a comprehensive comparison of specialized UGDA methods across five datasets with diverse distribution shifts for node classification task. Following that, we perform a thorough analysis between different GNN variants to understand how data shift imposes challenges on GNNs. Finally, we present the performance of SimGDA variants, which shows the limit of GNNs' transferability. For more information about metrics, hyperparameters, search spaces, and other implementation details, please refer to Appendix D. We further extend our scope to include the graph-level classification task; please refer to the Appendix E for details.

### Overall Comparisons

In this section, we try to elucidate the success of these UGDA algorithms through empirical evaluation across diverse datasets. In Table 2 and Table 3, we take a close look at the models' performance across 5 datasets on partial tasks utilizing Micro-F1 for Blog, Airport, and ArnetMiner, while employing

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{2}{c|}{Airport} & \multicolumn{2}{c|}{Blog} & \multicolumn{2}{c}{ArnetMiner} \\ \cline{2-7}  & E\(\rightarrow\) U & U \(\rightarrow\) E & B1 \(\rightarrow\) B2 & B2 \(\rightarrow\) B1 & C \(\rightarrow\) A & D \(\rightarrow\) A \\ \hline DANE & 31.18 \(\pm_{3.26}\) & 33.75 \(\pm_{0.31}\) & 32.17 \(\pm_{3.20}\) & 32.77 \(\pm_{0.66}\) & 62.87 \(\pm_{1.98}\) & 59.19 \(\pm_{1.66}\) \\ ACDNE & 48.52 \(\pm_{1.17}\) & 45.03 \(\pm_{0.43}\) & 54.30 \(\pm_{1.42}\) & **56.93**\(\pm_{0.56}\) & 72.34 \(\pm_{0.39}\) & 65.37 \(\pm_{3.50}\) \\ UDAGCN & 41.62 \(\pm_{0.58}\) & 33.17 \(\pm_{0.12}\) & 27.58 \(\pm_{0.63}\) & 25.46 \(\pm_{5.96}\) & 70.24 \(\pm_{1.01}\) & 62.49 \(\pm_{1.16}\) \\ ASN & 46.58 \(\pm_{0.21}\) & 40.85 \(\pm_{0.54}\) & 53.91 \(\pm_{0.52}\) & 56.25 \(\pm_{0.52}\) & 71.70 \(\pm_{0.38}\) & 66.15 \(\pm_{1.02}\) \\ AdaGCN & 46.55 \(\pm_{0.42}\) & 49.62 \(\pm_{0.01}\) & 43.06 \(\pm_{3.03}\) & 36.58 \(\pm_{8.94}\) & 67.66 \(\pm_{0.36}\) & 60.47 \(\pm_{0.99}\) \\ DMGNN & 45.85 \(\pm_{1.03}\) & 27.82 \(\pm_{2.95}\) & 46.27 \(\pm_{2.40}\) & 44.96 \(\pm_{1.57}\) & 72.91 \(\pm_{0.44}\) & 70.68 \(\pm_{0.27}\) \\ CWGCN & 44.68 \(\pm_{0.42}\) & 40.69 \(\pm_{0.47}\) & 31.96 \(\pm_{3.47}\) & 33.46 \(\pm_{4.96}\) & 71.65 \(\pm_{0.21}\) & 68.21 \(\pm_{0.09}\) \\ SAGDA & 30.62 \(\pm_{5.57}\) & 35.92 \(\pm_{1.01}\) & 26.51 \(\pm_{11.12}\) & 26.91 \(\pm_{7.26}\) & 65.40 \(\pm_{4.38}\) & 64.60 \(\pm_{0.89}\) \\ DGDA & 43.45 \(\pm_{2.16}\) & 43.78 \(\pm_{2.90}\) & 22.10 \(\pm_{1.45}\) & 21.06 \(\pm_{2.07}\) & 52.20 \(\pm_{4.62}\) & 56.31 \(\pm_{2.01}\) \\ \hline StruRW & 45.94 \(\pm_{0.69}\) & 36.09 \(\pm_{0.01}\) & 40.02 \(\pm_{0.37}\) & 42.10 \(\pm_{1.18}\) & 70.59 \(\pm_{0.15}\) & 64.15 \(\pm_{0.31}\) \\ KBL & 44.54 \(\pm_{0.73}\) & 32.08 \(\pm_{0.20}\) & 35.14 \(\pm_{3.97}\) & 34.90 \(\pm_{2.49}\) & 70.49 \(\pm_{0.26}\) & 63.34 \(\pm_{0.53}\) \\ JHGDA & 36.89 \(\pm_{0.25}\) & 40.85 \(\pm_{1.68}\) & 17.79 \(\pm_{2.12}\) & 23.16 \(\pm_{6.59}\) & 65.53 \(\pm_{0.94}\) & 60.80 \(\pm_{0.35}\) \\ PairAlign & 42.38 \(\pm_{0.77}\) & 36.84 \(\pm_{1.48}\) & 32.17 \(\pm_{10.88}\) & 41.16 \(\pm_{3.02}\) & 58.06 \(\pm_{2.62}\) & 56.68 \(\pm_{0.89}\) \\ \hline GRADE & 49.36 \(\pm_{0.35}\) & 48.45 \(\pm_{1.56}\) & 38.64 \(\pm_{3.73}\) & 44.01 \(\pm_{4.51}\) & 69.16 \(\pm_{0.39}\) & 63.47 \(\pm_{1.10}\) \\ SpecReg & 37.59 \(\pm_{2.55}\) & 28.91 \(\pm_{8.77}\) & 28.27 \(\pm_{4.22}\) & 30.30 \(\pm_{1.35}\) & 68.90 \(\pm_{4.78}\) & 66.30 \(\pm_{4.28}\) \\ A2GNN & 50.64 \(\pm_{1.47}\) & 53.47 \(\pm_{0.24}\) & 22.58 \(\pm_{0.01}\) & 33.04 \(\pm_{4.12}\) & **76.15**\(\pm_{0.06}\) & **74.12**\(\pm_{0.18}\) \\ \hline SimGDA & 55.29 \(\pm_{0.39}\) & 54.39 \(\pm_{0.90}\) & 53.35 \(\pm_{0.69}\) & 43.04 \(\pm_{0.86}\) & 70.80 \(\pm_{0.06}\) & 67.04 \(\pm_{0.15}\) \\ SimGDA+ & **58.11**\(\pm_{0.40}\) & **57.52**\(\pm_{0.38}\) & **57.04**\(\pm_{0.45}\) & 44.17 \(\pm_{0.02}\) & 73.18 \(\pm_{0.38}\) & 71.81 \(\pm_{2.44}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: We compared Micro-F1 of each model on Airport, Blog, and ArnetMiner. Highlighted are the top **first**, \(\mathrm{second}\), and third results. Results for other tasks can be found in Appendix D.

Macro-F1 for MAG and AUROC for Twitch due to their imbalanced labels. For comprehensive experimental results, please refer to Appendix D. Our key findings include:

_Observation 1: When facing significant shifts, it is important to design solutions tailored to mitigate structural discrepancies._ Although several methods that incorporate classical DA approaches with deep node embedding have achieved impressive results on Airport, Blog and ArnetMiner datasets (e.g., ACDNE and DMGNN), they fail to obtain satisfied performance on datasets with significant shifts. These results underscore the limitations of marginal distribution alignment techniques in the presence of significant structural and label shifts in graph data. As shown in Table 3, methods tailored to address graph structure shifts show reasonable improvements over those that incorporate classical DA techniques with deep node embeddings. This suggests that mitigating the impact of graph structure shift on node representation learning under this scenario is crucial.

_Observation 2: Domain-adaptive message passing methods demonstrate superior and robust performance across a wide range of datasets and tasks._ While methods that align marginal feature distributions and those designed for graph structure shifts can address datasets with mild and severe data shifts respectively, strategies specifically developed to leverage GNN properties demonstrate robustness and superior performance across diverse data shifts. As shown in Table 2 and Table 3, methods designed based on the inherent properties of GNN achieves the top-three best performance in 8 tasks out of 12 tasks. This finding suggests that leveraging the structural strengths of GCNs, combined with well-established domain adaptation principles, can result in an effective and efficient approach to addressing the challenges of domain variability in graph datasets. Such strategies represent a promising direction for future research and application in this field.

### Understanding and unlocking the inherent power of GNN

Although many UGDA methods integrate traditional domain adaptation techniques, we observe their performance remains unsatisfactory and can even fall below that of SimGDA. This leads us to an intriguing question: do the intrinsic mechanisms of GNN play a more crucial role in enhancing transferability? To further investigate the question, we take one-layer GCN combined with vanilla DA as a baseline, and compare six variants: Max-Aggr, Mean-Aggr, GAT-Aggr, GIN-Aggr, with-no-neighbor, and with-multi-hop-neighbor; The results are shown in Figure 3. Moreover, we enhance

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{3}{c|}{Twitch} & \multicolumn{3}{c}{MAG} \\ \cline{2-7}  & DE\(\rightarrow\) ES & EN \(\rightarrow\) RU & DE \(\rightarrow\) PT & FR \(\rightarrow\) JP & JP \(\rightarrow\) FR & JP \(\rightarrow\) RU \\ \hline DANE & 56.71 \(\pm_{0.57}\) & 53.47 \(\pm_{0.84}\) & 55.71 \(\pm_{1.70}\) & 16.16 \(\pm_{0.24}\) & 16.71 \(\pm_{1.45}\) & 12.61 \(\pm_{0.19}\) \\ ACDNE & 51.13 \(\pm_{0.34}\) & 50.79 \(\pm_{0.12}\) & 52.47 \(\pm_{1.83}\) & 20.12 \(\pm_{0.37}\) & 18.92 \(\pm_{1.08}\) & 13.91 \(\pm_{0.35}\) \\ UDAGCN & 56.48 \(\pm_{0.18}\) & 53.72 \(\pm_{0.42}\) & 54.22 \(\pm_{1.20}\) & 12.22 \(\pm_{0.31}\) & 11.62 \(\pm_{0.35}\) & 11.17 \(\pm_{0.29}\) \\ ASN & 53.57 \(\pm_{0.72}\) & 50.29 \(\pm_{0.15}\) & 55.03 \(\pm_{0.75}\) & 11.91 \(\pm_{1.45}\) & 12.04 \(\pm_{0.70}\) & 10.79 \(\pm_{0.24}\) \\ AdaGCN & 52.32 \(\pm_{0.76}\) & 51.99 \(\pm_{1.31}\) & 51.09 \(\pm_{0.61}\) & 16.21 \(\pm_{0.47}\) & 14.12 \(\pm_{0.46}\) & 13.05 \(\pm_{0.08}\) \\ DMGNN & 54.11 \(\pm_{0.09}\) & 50.42 \(\pm_{0.03}\) & 53.44 \(\pm_{0.04}\) & 12.01 \(\pm_{0.78}\) & 9.93 \(\pm_{0.40}\) & 10.28 \(\pm_{1.04}\) \\ CWGCN & 57.62 \(\pm_{0.64}\) & 52.90 \(\pm_{0.37}\) & 58.21 \(\pm_{0.57}\) & 11.01 \(\pm_{0.48}\) & 12.37 \(\pm_{0.54}\) & 12.38 \(\pm_{0.25}\) \\ SAGDA & 51.58 \(\pm_{0.09}\) & 51.03 \(\pm_{0.23}\) & 51.96 \(\pm_{0.70}\) & 16.28 \(\pm_{0.51}\) & 3.64 \(\pm_{0.57}\) & 11.40 \(\pm_{0.59}\) \\ DGDA & 54.43 \(\pm_{3.60}\) & 51.68 \(\pm_{1.08}\) & 54.29 \(\pm_{4.28}\) & OOM & OOM & OOM \\ \hline StruRW & 59.60 \(\pm_{0.19}\) & 52.04 \(\pm_{0.36}\) & 58.74 \(\pm_{2.09}\) & 22.10 \(\pm_{0.40}\) & 12.89 \(\pm_{0.85}\) & 12.96 \(\pm_{0.43}\) \\ KBL & 58.33 \(\pm_{0.44}\) & **55.91** \(\pm_{0.16}\) & 51.66 \(\pm_{0.08}\) & 17.60 \(\pm_{0.39}\) & 6.12 \(\pm_{0.14}\) & 14.49 \(\pm_{0.30}\) \\ JHGDA & **62.25** \(\pm_{0.49}\) & 53.75 \(\pm_{0.15}\) & 61.88 \(\pm_{0.48}\) & 20.51 \(\pm_{0.20}\) & 20.46 \(\pm_{0.57}\) & 11.85 \(\pm_{0.37}\) \\ PairAlign & 50.78 \(\pm_{0.22}\) & 51.19 \(\pm_{0.20}\) & 52.03 \(\pm_{0.97}\) & 23.29 \(\pm_{0.49}\) & 23.72 \(\pm_{0.30}\) & 12.34 \(\pm_{0.31}\) \\ \hline GRADE & 58.57 \(\pm_{0.42}\) & 53.55 \(\pm_{0.28}\) & **62.12** \(\pm_{0.17}\) & 11.93 \(\pm_{0.48}\) & 10.95 \(\pm_{0.55}\) & 9.35 \(\pm_{0.25}\) \\ SpecReg & 51.04 \(\pm_{0.33}\) & 50.17 \(\pm_{0.06}\) & 55.91 \(\pm_{0.59}\) & 19.45 \(\pm_{0.54}\) & 20.17 \(\pm_{1.35}\) & 15.82 \(\pm_{0.50}\) \\ A2GNN & 59.41 \(\pm_{0.34}\) & 52.01 \(\pm_{0.32}\) & 61.82 \(\pm_{0.77}\) & **26.20** \(\pm_{0.74}\) & **25.78** \(\pm_{0.25}\) & **16.94** \(\pm_{0.13}\) \\ \hline SimGDA & 61.30 \(\pm_{0.32}\) & 53.11 \(\pm_{0.19}\) & 58.27 \(\pm_{0.16}\) & 18.59 \(\pm_{0.07}\) & 15.16 \(\pm_{0.11}\) & 13.27 \(\pm_{0.04}\) \\ SimGDA+ & 61.53 \(\pm_{0.08}\) & 53.82 \(\pm_{0.08}\) & 61.60 \(\pm_{0.11}\) & 21.94 \(\pm_{0.18}\) & 21.36 \(\pm_{0.09}\) & 15.64 \(\pm_{0.46}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: We evaluated Macro-F1 on MAG and AUROC scores on Twitch. Highlighted are the top **first**, second, and third results. OOM indicates out of memory. Results for other tasks can be found in Appendix D above SimGDA with unsupervised graph learning techniques, referred to as SimGDA+, to explore the limit of GNNs in graph domain adaptation, which is shown in Table 2 and Table 3.

_Observation 3: SimGDA achieves competitive performance compared with UGDA methods._ As shown in Table 2 and Table 3, SimGDA shows comparable results with state-of-the-art approaches across various datasets. Specifically, SimGDA exhibits better performance than structure shift tailored solutions in datasets with minor data shifts, i.e., Airport, Blog and ArmetMinver, and demonstrates advantages surpassing state-of-the-art methods in E \(\rightarrow\) U and U \(\rightarrow\) E. Similar trends are also observed in the Twitch and MAG dataset, where SimGDA outperforms most UGDA methods in six tasks. To further explore the impacts of data shifts on GNNs, we analyze the role of aggregation scope and mechanisms, which tackle diverse shifts through the graph structure.

_Observation 4: The benefit of multi-hop neighbors depends on the degree of label shift and graph heterophily._ Across ArnetMiner, Blog, and Airport datasets, we observed a significant decline in performance for most tasks without neighbours, shown in Figure 3 and Appendix D. In contrast, on the Twitch and MAG datasets with heavy label shift, overlooking neighbor information generally benefits task performance. This suggests that the impact of structural information varies depending on the dataset with diverse degree of label shift.

Besides, we note a performance decline on the Airport and Blog datasets when including multi-hop neighbors. However, on the ArnetMiner, Twitch, and MAG datasets, incorporating multi-hop neighbors leads to performance improvement. The enhancement in performance in these cases can be attributed to the fact that heterophilous graphs exhibit a larger degree of conditional shift, and aggregation process may help to mitigate this situation by providing a more comprehensive view of the node's context.

_Observation 5: A source-unbiased discriminative aggregation mechanism is needed._ As depicted in Figure 3, mean and max aggregators consistently exhibit lower performance across tasks compared to the sum aggregator used in GCN. This can be attributed to their inherent limitations in capturing discriminative structural information [50]. In contrast, the aggregator utilized in GCN can identify and distinguish between different structures by effectively incorporating degree-aware neighbors. Thus, the superiority of the GCN aggregator over mean and max emphasizes the necessity of a discriminative aggregation operator with highly expressive power.

To enhance the expressive power of aggregation operator, we evaluate aggregate mechanism used in GAT and GIN. Despite their stronger expressive capacities, their performance is generally ineffective across datasets such as ArnetMiner, Blog, Airport, and Twitch. This inefficiency can be attributed to the increased risk of model bias towards information from the source domain, stemming from the requirement of more parameters for learning. Consequently, source-biased discriminative aggregation mechanisms deteriorate the model's transfer capability.

_Observation 6: GNNs can serve as a powerful graph domain adaptor._ With the optimal number of neighbor hops and aggregators, we further enhance SimGDA with unsupervised graph learning techniques and obtain SimGDA+. As can be seen in Table 2 and Table 3, SimGDA+ surpasses the

Figure 3: SimGDA: the compared performance of vanilla DA with 6 GNN variants.

performance of most specially-designed methods for graph domain adaptation and even achieves the best performance in certain tasks. This superior performance under various types of shifts showcases the potential of GNNs as powerful domain adaptors. In summary, we contend that GNNs, when designed with appropriate aggregators, careful selection of neighbor hops, and the application of unsupervised graph learning techniques, can serve as effective and reliable graph domain adaptors.

### Do LLMs help mitigate distribution shift in graphs?

Recently, Large Language Models (LLMs) [62] have demonstrated an impressive ability to understand and handle various text-related tasks. When dealing with text-attributed graph, an important question arises: does the distribution shift persist after leveraging LLMs as feature encoders?

To investigate this question, we utilize the prompts from TAPE [63], which allows us to assess the impact of LLM-based features on the model's performance. For datasets, we choose the widely used ogbn-arxiv dataset [64], which contains paper title and abstract text information. Each node represents an arXiv paper, with directed edges indicating citations from one paper to another. The objective is to predict the 40 subject areas of arXiv computer science papers, which are manually assigned by the authors and moderators of arXiv. We split the data into three disjoint domains based on the publication year of the papers, i.e. 1950-2016, 2016-2018, 2018-2020. The statistical details for each domain are shown in the Table 4.

We explored two approaches to enhance the original node attributes:

* LLM enhanced text with word2vec embedding [65], which combines the title, abstract, and LLM-generated predictions and explanations into a single input. This composite text is then fed into word2vec. Then, the node features are obtained by averaging the embeddings of its combined input. We refer to it as arxiv-LLM-w2v.
* LLM enhanced text with BERT embedding [66], which feeds the same composite text into a pretrained DeBERTa. Then, the node features are obtained by sentence embedding. Note that we did not finetune the DeBERTa like TAPE [63], since we focus on unsupervised graph domain adaptation. We refer to it as arxiv-LLM-bert.

First, we use MMD to characterize the degree of feature shift among these three datasets, i.e., ogbn-arxiv, arxiv-LLM-w2v and arxiv-LLM-bert. We consider 3 adaptation tasks, and the results are shown in Table 5. As we can see, when word2vec is used to encode the LLM-enhanced text, the feature shift is reduced compared to the original node features. However, when BERT is used for encoding, the feature shift increases. This indicates that the choice of text encoding method significantly influences the degree of feature shift.

Next, we choose 5 recent graph domain adaptation models to assess the impact of LLM-based features on the model's performance. Each experiment is repeated 3 times, and we report the average Micro-F1 score with standard deviation. As illustrated in Table 6, the performance of most baselines shows significant improvement with the arxiv-LLM-w2v dataset, whereas performance notably declines with the arxiv-LLM-bert dataset compared to the original ogbn-arxiv dataset. These results align with the MMD scores presented in the Table 5, which indicate that arxiv-LLM-bert exhibits a

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline Domains & \(\#\) Nodes & \(\#\) Edges & \(\#\) Homo & \(\#\) Avg Degree \\ \hline
1950-2016 & 69,499 & 237,163 & 0.6945 & 3.41 \\
2016-2018 & 51,241 & 111,754 & 0.6886 & 2.18 \\
2018-2020 & 48,603 & 60,403 & 0.7092 & 1.24 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Statistics of the split ogbn-arxiv dataset.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline Source & Target & ogbn-arxiv & arxiv-LLM-w2v & arxiv-LLM-bert \\ \hline
1950-2016 & 2016-2018 & 0.0405 & 0.0400 \(\downarrow\) & 0.0427 \(\uparrow\) \\
1950-2016 & 2018-2020 & 0.0528 & 0.0535 \(\uparrow\) & 0.0796 \(\uparrow\) \\
2016-2018 & 2018-2020 & 0.0148 & 0.0138 \(\downarrow\) & 0.0149 \(\uparrow\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Feature shift among domains in arxiv dataset by using MMD [41] as metric.

larger distribution shift compared to the other datasets. This emphasizes the necessity of selecting an appropriate text encoder when utilizing LLM-enhanced text for graph domain adaptation. An effective choice of text encoder can greatly impact the performance and mitigate the distribution shifts in text-attributed graph domain adaptation tasks.

## 6 Conclusion

In this paper, we introduce GDABench, the first comprehensive benchmark for unsupervised graph domain adaptation. Our evaluation encompasses 16 well-known models across various real-world datasets exhibiting diverse data distribution shifts. Furthermore, we also designed 6 GNN variants to investigate the inherent transferability of GNNs, enhancing them with 3 unsupervised techniques to explore their potential limits. Our empirical results shows that (1) the performance of current UGDA models varies significantly across different datasets and adaptation scenarios; (2) tailored strategies are essential for addressing and mitigating graph structural shifts, particularly when distribution discrepancies are substantial. (3) the transferability of GNNs in UGDA is heavily dependent on aggregation scope and architecture, influenced by factors such as label shift severity and graph heterophily. We have provided unified APIs and adopted consistent data processing as well as data splitting approaches for fair comparisons. In the future, we plan to extend GDABench to include broader scenarios [67; 68], more cutting-edge models and more complex types of datasets. We hope our benchmark and findings will promote realistic and rigorous evaluations, inspiring new advances in graph domain adaptation.

**Border Impacts and Limitations.** Our benchmark fosters innovation and advances research in graph domain adaptation by providing a standardized evaluation platform, leading to the development of more effective algorithms. This standardization helps researchers compare methods more fairly, driving progress and collaboration within the field. However, benchmark datasets may introduce limitations that could impact the generalization of findings to real-world scenarios. This risk includes the potential for unrealistic performance expectations if the benchmark does not adequately represent the diversity and complexity of real-world data. We plan to enhance GDABench by including more settings such as source-free and open-set scenarios. This expansion will help to cover a wider range of domain adaptation challenges, thereby fostering the development of algorithms that are not only more robust but also versatile enough to navigate the complexities of diverse and dynamic real-world scenarios. This trajectory in research will be pivotal in advancing the capabilities of domain adaptation techniques, ensuring their applicability and efficacy across various domains and evolving data landscapes.

## Acknowledgments and Disclosure of Funding

This work is supported by the National Natural Science Foundation of China (62372408, 62106221), Zhejiang Provincial Natural Science Foundation of China (Grant No: LTGG23F030005), Ningbo Natural Science Foundation (Grant No: 2022J183). The research of Zhen Zhang and Bingsheng He is supported by the National Research Foundation, Singapore under its Industry Alignment Fund - Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline \hline
**Source** & **Target** & **Features** & **UDGCN** & **AdaGCN** & **KBL** & **GRADE** & **A2GNN** \\ \hline \multirow{3}{*}{1950-2016} & \multirow{3}{*}{2016-2018} & opbn-arxiv & 52.42\(\pm\)0.52 & 61.78\(\pm\)0.18 & 50.88\(\pm\)0.24 & 61.85\(\pm\)0.17 & 60.18\(\pm\)0.54 \\  & & arxiv-LLM-w2v & 43.55\(\pm\)0.90 & 66.23\(\pm\)0.14 & 64.90\(\pm\)0.12 & 65.41\(\pm\)0.31 & 63.77\(\pm\)0.36 \\  & & arxiv-LLM-bert & 34.81\(\pm\)0.29 & 35.66\(\pm\)1.20 & 34.64\(\pm\)0.85 & 35.04\(\pm\)0.86 & 39.14\(\pm\)2.00 \\ \hline \multirow{3}{*}{1950-2016} & \multirow{3}{*}{2018-2020} & opbn-arxiv & 48.41\(\pm\)0.64 & 56.74\(\pm\)0.34 & 47.53\(\pm\)1.15 & 57.19\(\pm\)0.26 & 58.89\(\pm\)0.28 \\  & & arxiv-LLM-w2v & 39.56\(\pm\)0.54 & 62.04\(\pm\)0.29 & 60.67\(\pm\)0.29 & 62.04\(\pm\)0.29 & 65.35\(\pm\)0.12 \\  & & arxiv-LLM-bert & 30.18\(\pm\)0.25 & 31.69\(\pm\)0.42 & 30.26\(\pm\)1.09 & 31.09\(\pm\)0.12 & 35.40\(\pm\)1.55 \\ \hline \multirow{3}{*}{2016-2018} & \multirow{3}{*}{2018-2020} & opbn-arxiv & 54.84\(\pm\)0.22 & 62.05\(\pm\)0.04 & 52.99\(\pm\)0.05 & 61.42\(\pm\)0.14 & 59.45\(\pm\)0.28 \\  & & arxiv-LLM-w2v & 47.20\(\pm\)0.99 & 68.42\(\pm\)0.04 & 66.77\(\pm\)0.13 & 66.84\(\pm\)0.16 & 65.83\(\pm\)0.40 \\ \cline{1-1}  & & arxiv-LLM-bert & 39.20\(\pm\)2.53 & 39.93\(\pm\)1.90 & 37.27\(\pm\)2.06 & 34.14\(\pm\)1.54 & 43.62\(\pm\)1.87 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Micro-F1 score of 5 recent graph domain adaptation models with LLM-based features.

[MISSING_PAGE_FAIL:11]

* [20] Boshen Shi, Yongqing Wang, Jiangli Shao, Huawei Shen, Yangyang Li, and Xueqi Cheng. Node classification across networks via category-level domain adaptive network embedding. _Knowledge and Information Systems_, 65(12):5479-5502, 2023.
* [21] X. Shen, Shirui Pan, Kup-Sze Thomas Choi, and Xiaoping Zhou. Domain-adaptive message passing graph neural network. _Neural networks : the official journal of the International Neural Network Society_, 164:439-454, 2023.
* [22] Wei Wang, Gaowei Zhang, Hongyong Han, and Chi Zhang. Correntropy-induced wasserstein gcn: Learning graph embedding via domain adaptation. _IEEE Transactions on Image Processing_, 32:3980-3993, 2023.
* [23] Jinhui Pang, Zixuan Wang, Jiliang Tang, Mingyan Xiao, and Nan Yin. Sa-gda: Spectral augmentation for graph domain adaptation. _Proceedings of the 31st ACM International Conference on Multimedia_, 2023.
* 24, 2021.
* [25] Meihan Liu, Zeyu Fang, Zhen Zhang, Ming Gu, Sheng Zhou, Xin Wang, and Jiajun Bu. Rethinking propagation for unsupervised graph domain adaptation. _ArXiv_, abs/2402.05660, 2024.
* [26] Shikun Liu, Deyu Zou, Han Zhao, and Pan Li. Pairwise alignment improves graph domain adaptation. _ArXiv_, abs/2403.01092, 2024.
* [27] Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J. Gordon. Domain adaptation with conditional distribution matching and generalized label shift. _ArXiv_, abs/2003.04475, 2020.
* [28] Boshen Shi, Yongqing Wang, Fangda Guo, Jiangli Shao, Huawei Shen, and Xueqi Cheng. Opengda: Graph domain adaptation benchmark for cross-network learning. _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, 2023.
* [29] Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu C. Aggarwal, Prasenjit Mitra, and Suhang Wang. Investigating and mitigating degree-related biases in graph convolutional networks. _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, 2020.
* [30] Mingxuan Ju, Tong Zhao, Wenhao Yu, Neil Shah, and Yanfang Ye. Graphpatcher: Mitigating degree bias for graph neural networks via test-time augmentation. _ArXiv_, abs/2310.00800, 2023.
* [31] Zehong Wang, Zheyuan Zhang, Chuxu Zhang, and Yanfang Ye. Tackling negative transfer on graphs. _arXiv preprint arXiv:2402.08907_, 2024.
* [32] Qi Zhu, Yizhu Jiao, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. Explaining and adapting graph conditional shift, 2023.
* [33] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. _TKDE_, 22:1345-1359, 2010.
* [34] Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. _Neurocomputing_, 312:135-153, 2018.
* Volume 37_, ICML'15, page 97-105. JMLR.org, 2015.
* [36] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschlager, and Susanne Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learning. _arXiv preprint arXiv:1702.08811_, 2017.
* [37] Yaroslav Ganin, E. Ustinova, Hana Ajakan, Pascal Germain, H. Larochelle, Francois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. In _JMLR_, 2016.
* [38] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. _Advances in neural information processing systems_, 31, 2018.
* [39] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. _arXiv preprint arXiv:1412.3474_, 2014.

* [40] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. _CVPR_, pages 2962-2971, 2017.
* [41] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alex Smola. A kernel two-sample test. _JMLR_, 13:723-773, 2012.
* [42] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation learning for domain adaptation. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* [43] Dong Wang and Thomas Fang Zheng. Transfer learning for speech and language processing. _2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)_, pages 1225-1237, 2015.
* [44] Xiaowen Zhang, Yuntao Du, Rongbiao Xie, and Chongjun Wang. Adversarial separation network for cross-network node classification. _CIKM_, 2021.
* [45] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. _Journal of Machine Learning Research_, 12(9), 2011.
* [46] Qi Zhu, Yizhu Jiao, Haonan Wang, Natalia Ponomareva, and Bryan Perozzi. Gnn domain adaptation using optimal transport. _arXiv preprint_, 2022.
* [47] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2017.
* [48] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _NIPS_, 2017.
* [49] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio', and Yoshua Bengio. Graph attention networks, 2018.
* [50] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* [51] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [52] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In _International Conference on Machine Learning_, pages 6028-6039. PMLR, 2020.
* [53] Meihan Liu, Zhen Zhang, Ning Ma, Ming Gu, Haishuai Wang, Sheng Zhou, and Jiajun Bu. Structure enhanced prototypical alignment for unsupervised cross-domain node classification. _Neural Networks_, page 106396, 2024.
* [54] Shaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural networks for learning graph representations. In _Proceedings of the AAAI conference on artificial intelligence_, volume 30, 2016.
* [55] Thomas N Kipf and Max Welling. Variational graph auto-encoders. _arXiv preprint arXiv:1611.07308_, 2016.
* [56] Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially regularized graph autoencoder for graph embedding. _arXiv preprint arXiv:1802.04407_, 2018.
* [57] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip. Graph self-supervised learning: A survey. _IEEE transactions on knowledge and data engineering_, 35(6):5879-5900, 2022.
* [58] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. _Advances in neural information processing systems_, 33:5812-5823, 2020.
* [59] Puja Trivedi, Ekdeep Singh Lubana, Yujun Yan, Yaoqing Yang, and Danai Koutra. Augmentations in graph contrastive learning: Current methodological flaws & towards better practices. In _Proceedings of the ACM Web Conference 2022_, pages 1538-1549, 2022.
* [60] Yanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. An empirical study of graph contrastive learning. _arXiv preprint arXiv:2109.01116_, 2021.

* [61] Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In _Proceedings of the 30th International Conference on Neural Information Processing Systems_, NIPS'16, page 1857-1865, Red Hook, NY, USA, 2016. Curran Associates Inc.
* [62] Introducing chatgpt. 2022. OpenAI.
* [63] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and Bryan Hooi. Harnessing explanations: LLM-to-LM interpreter for enhanced text-attributed graph representation learning. In _The Twelfth International Conference on Learning Representations_, 2024.
* [64] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 33:22118-22133, 2020.
* [65] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. _Advances in neural information processing systems_, 26, 2013.
* [66] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. _arXiv preprint arXiv:2006.03654_, 2020.
* [67] Zhen Zhang, Meihan Liu, Anhui Wang, Hongyang Chen, Zhao Li, Jiajun Bu, and Bingsheng He. Collaborate to adapt: Source-free graph domain adaptation via bi-directional adaptation. In _Proceedings of the ACM on Web Conference 2024_, pages 664-675, 2024.
* [68] Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and Dongmei Zhang. Source free graph unsupervised domain adaptation. In _Proceedings of the 17th ACM International Conference on Web Search and Data Mining_, pages 520-528, 2024.
* [69] Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. _CVPR 2011_, pages 1521-1528, 2011.
* [70] Jose G Moreno-Torres, Troy Raeder, Rocio Alaiz-Rodriguez, Nitesh V Chawla, and Francisco Herrera. A unifying view on dataset shift in classification. _Pattern recognition_, 45(1):521-530, 2012.
* [71] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. _Journal of Statistical Planning and Inference_, 90(2):227-244, 2000.
* [72] Jie Tang, Jing Zhang, Limin Yao, Juan-Zi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and mining of academic social networks. In _KDD_, 2008.
* [73] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _NeurIPS_, 2019.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Section 1. 2. Did you describe the limitations of your work? [Yes] See Section 6. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Section 6. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] Our paper strictly conforms to the ethics review guidelines.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] Our paper does not include theoretical results. 2. Did you include complete proofs of all theoretical results? [N/A] Our paper does not include theoretical results.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See URL in Abstract. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See Appendix D. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? See Table 2, Table 3 and Appendix D. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix D.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] See footnotes and references. 2. Did you mention the license of the assets? We use MIT license for our released assets. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] Our codes and datasets are available through URLs. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? See Appendix B. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? We use public datasets that do not contain personally identifiable information or offensive content.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? Our research do not involve human subjects. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? Our research do not involve human subjects. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? Our research do not involve human subjects.

## Appendix A Distribution Shift in Graph-Structured Data

Distribution shift appears when the joint distribution differs between source domain and target domain [7, 69]. Assuming that the relationship between the input and class variables is unchanged, there are two kinds of distribution shift, i.e., covariate shift and label shift (prior probability shift) [70].

### Covariate Shift

Covariate shift [71] refers to changes in the distribution of the input variables, which can be defined formally as follows:

_Definition 1_ (Covariate Shift).: Covariate shift appears when \(\mathbb{P}_{S}(G)\neq\mathbb{P}_{T}(G)\) with the assumption of \(\mathbb{P}_{S}(Y|G)=\mathbb{P}_{T}(Y|G)\), where \(\mathbb{P}_{S}\) and \(\mathbb{P}_{T}\) are the probability distributions of the source and target domains, respectively.

To deal with covariate shift, it is essential to align \(\mathbb{P}_{S}(Y|H)\) and \(\mathbb{P}_{T}(Y|H)\), where \(H\) is the representation after data attributes passing through the encoder. However, in graph-structured data, node representation is not only affected by the data attributes but also graph structure. Thus, covariate shift in graph data can be decoupled as feature shift and structure shift [26].

_Definition 2_ (Feature Shift).: Given the joint distribution of the node attributes and node labels \(\mathbb{P}_{T}(X,Y)\), the feature shift is then defined as \(\mathbb{P}_{S}(X,Y)\neq\mathbb{P}_{T}(X,Y)\) with the assumption of \(\mathbb{P}_{S}(Y|G)=\mathbb{P}_{T}(Y|G)\).

_Definition 3_ (Structure Shift).: Given the joint distribution of the adjacency matrix and node labels \(\mathbb{P}_{T}(A,Y)\), the structure shift is then defined as \(\mathbb{P}_{S}(A,Y)\neq\mathbb{P}_{T}(A,Y)\) with the assumption of \(\mathbb{P}_{S}(Y|G)=\mathbb{P}_{T}(Y|G)\).

### Label Shift

Label shift refers to changes in the distribution of the class variable \(Y\). It also appears with different names in the literature and the definitions have slight differences between them.

_Definition 4_ (Label Shift).: Label shift occurs when the distribution of labels changes across two domains, which is defined as \(\mathbb{P}_{S}(Y)\neq\mathbb{P}_{T}(Y)\) where \(\mathbb{P}_{S}(G|Y)=\mathbb{P}_{T}(G|Y)\).

In all, structure shift is unique to graph data due to the non-IID nature caused by node interconnections. Moreover, the learning of node representations implemented by the GNN will mix the feature shift, structure shift and label shift [32].

## Appendix B Detailed Description of Datasets

In this section, we provide additional details about the datasets used in our benchmark.

### Dataset Description

* **Airport2**: The Airport datasets consist of three separate collections corresponding to Brazil (B), Europe (E), and the USA (U). In these datasets, nodes represent airports and edges denote flight connections between them. The labels categorize airports by activity levels, measured in terms of flights or passenger numbers. Footnote 2: https://github.com/GentleZhu/EGI/tree/main/data
* **Blog3**: Blog1 and Blog2 are disjoint social networks derived from the BlogCatalog dataset. In these networks, nodes correspond to bloggers, and edges reflect friendships among them. The attributes for each node consist of keywords from the blogger's self-description, and each node is assigned a label denoting its group affiliation. Given that both Blog1 and Blog2 originate from the same underlying network, their data distributions are nearly identical.

* **ArnetMiner4**: These datasets comprise paper citation networks sourced from three distinct origins as provided by ArnetMiner [72]: "ACMV9" (A), "CitationV1" (C), and "DBLpV7" (D). Each dataset's nodes symbolize papers, while edges reflect their citation relationships. Specifically, "ACMV9" (A) includes papers from ACM spanning 2000 to 2010, "CitationV1" (C) consists of papers from the Microsoft Academic Graph up to 2008, and "DBLpV7" (D) contains papers from DBLP collected between 2004 and 2008. The aim is to categorize all papers into five specific research areas: Databases, Artificial Intelligence, Computer Vision, Information Security, and Networking. Footnote 4: https://github.com/yuntaodu/ASN/tree/main/data
* **Twitch5**: Twitch gamer networks from six regions--Germany (DE), England (EN), Spain (ES), France (FR), Portugal (PT), and Russia (RU)--comprise nodes representing users and connections that signify friendships among them. Node features include data on users' preferred games, geographical location, and streaming habits, among others. Users within these networks are categorized into two groups based on their use of explicit language. Footnote 5: http://snap.stanford.edu/data/twitch-social-networks.html
* **MAG6**: The MAG dataset, a subset of the Microsoft Academic Graph, is a heterogeneous network featuring four distinct types of entities: papers (736,389 nodes), authors (1,134,649 nodes), institutions (8,740 nodes), and fields of study (59,965 nodes). It includes four varieties of directed relationships linking pairs of entity types: an author's affiliation with an institution, an author's authorship of a paper, paper citations, and papers' association with fields of study. Each paper node is enriched with a 128-dimensional word2vec feature vector, while the other entities lack input node features. The primary task within this dataset involves predicting the publication venue (conference or journal) for each paper, leveraging information about its content, cited references, authors, and the affiliations of these authors. Following PairAlign [26], we split the original dataset into six countries. Footnote 6: https://zenodo.org/records/10681285

### Shift Statistics of Datasets

According to dataset statistics, shown in Table 7 and Figure 4, we measure the degree of domain shift exhibited in the datasets for each tasks using statistical methods. We use MMD [41], CSS [26], Kullback-Leibler Divergence to characterize the degree of feature shift, structure shift and label shift. The results of each tasks is shown in Table 13. We take the average results of all tasks as the shift statistics for the datasets, shown in Table 8. The 74 tasks compiled by the five carefully selected datasets can cover all combinations of domain shift scenarios.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline \hline Dataset & \# Domains & \# Nodes & \# Edges & \# Homo & \# Avg Degree & \# Fact Dims & \# Labels \\ \hline \multirow{3}{*}{Airpot} & USA (U) & 1,190 & 27,198 & 0.6978 & 22.86 & \multirow{3}{*}{241} & \multirow{3}{*}{4} \\  & BRAZIL (B) & 131 & 2,148 & 0.4683 & 16.40 & & \\  & EUROPE (E) & 399 & 11,990 & 0.4048 & 30.05 & & \\ \hline \multirow{3}{*}{Blog} & Blog1 (B1) & 2,300 & 66,942 & 0.3991 & 29.11 & \multirow{3}{*}{8,189} & \multirow{3}{*}{6} \\  & Blog2 (B2) & 2,896 & 107,672 & 0.4002 & 37.18 & & \\ \hline \multirow{3}{*}{ArnetMiner} & DBLP7 (D) & 5,484 & 16,234 & 0.8198 & 2.96 & \multirow{3}{*}{6,775} & \multirow{3}{*}{5} \\  & ACMV9 (A) & 9,360 & 31,112 & 0.7998 & 3.32 & & \\  & CitationV1 (C) & 8,935 & 30,196 & 0.8598 & 3.38 & & \\ \hline \multirow{3}{*}{Twitch} & England (EN) & 7,126 & 35,324 & 0.5560 & 4.96 & \multirow{3}{*}{2} \\  & Germany (DE) & 9,498 & 153,138 & 0.6322 & 16.14 & & \\  & France (FR) & 6,549 & 112,666 & 0.5595 & 17.20 & & \\  & Russia (RU) & 4,385 & 37,304 & 0.6176 & 8.51 & & \\  & Spain (ES) & 4,648 & 59,382 & 0.5800 & 12.78 & & \\  & Portugal (PT) & 1,912 & 31,299 & 0.5708 & 16.40 & & \\ \hline \multirow{3}{*}{MAG} & China (CN) & 101,952 & 285,991 & 0.5307 & 2.81 & \multirow{3}{*}{128} & \multirow{3}{*}{20} \\  & Germany (DE) & 43,032 & 127,704 & 0.5311 & 2.97 & & \\ \cline{1-1}  & France (FR) & 29,262 & 79,182 & 0.5732 & 2.71 & & \\ \cline{1-1}  & Japan (HP) & 37,498 & 91,412 & 0.5645 & 2.44 & & \\ \cline{1-1}  & Russia (RU) & 32,833 & 68,294 & 0.7682 & 2.08 & & \\ \cline{1-1}  & USA (US) & 132,558 & 702,482 & 0.5174 & 5.30 & & \\ \hline \hline \end{tabular}
\end{table}
Table 7: Dataset Statistics.

* **Feature shift determined**: Tasks ES \(\rightarrow\) PT, PT\(\rightarrow\)ES, EN\(\rightarrow\)DE, ED\(\rightarrow\)EN, FR\(\rightarrow\)ES, FR\(\rightarrow\)PT, ES\(\rightarrow\)FR, PT\(\rightarrow\)FR, RU\(\rightarrow\)ES, ES\(\rightarrow\)RU, RU\(\rightarrow\)PT, PT\(\rightarrow\)RU, RU\(\rightarrow\)FR and FR\(\rightarrow\)RU in Twitch. Tasks JP\(\rightarrow\)US, US\(\rightarrow\)JP, JP\(\rightarrow\)CN and CN\(\rightarrow\)JP in MAG.
* **Structure shift determined**: Tasks E\(\rightarrow\)B, B\(\rightarrow\)E, U\(\rightarrow\)B, B\(\rightarrow\)U, U\(\rightarrow\)E and E\(\rightarrow\)U in Airport. Tasks GP\(\rightarrow\)DE and US\(\rightarrow\)DE in MAG. Tasks FR\(\rightarrow\)DE, RU\(\rightarrow\)EN, RU\(\rightarrow\)DE, ES\(\rightarrow\)DE, EN\(\rightarrow\)RU, DE\(\rightarrow\)RU, DE\(\rightarrow\)ES and DE\(\rightarrow\)PT in Twitch.
* **Label shift determined**: Task FR\(\rightarrow\)DE in MAG.
* **Determined by both feature and structure shift**: Tasks D\(\rightarrow\)A, D\(\rightarrow\)C, A\(\rightarrow\)D and C\(\rightarrow\)D in ArmetMiner. Tasks FR\(\rightarrow\)EN, EN\(\rightarrow\)FR, PT\(\rightarrow\)EN, EN\(\rightarrow\)PT, DE\(\rightarrow\)FR, FR\(\rightarrow\)DE, PT\(\rightarrow\)DE and EN\(\rightarrow\)ES in Twitch. Tasks JP\(\rightarrow\)FR, RU\(\rightarrow\)PT, RU\(\rightarrow\)CN and DE\(\rightarrow\)JP in MAG.
* **Determined by both feature and label shift**: Tasks EN\(\rightarrow\)US, US\(\rightarrow\)EN in MAG.
* **Determined by both structure and label shift**: Tasks DE\(\rightarrow\)US, FR\(\rightarrow\)US, US\(\rightarrow\)FR, FR\(\rightarrow\)RU in MAG.
* **All shifts effects**: Tasks B1\(\rightarrow\)B2 and B2\(\rightarrow\)B1 in Blog. Tasks A\(\rightarrow\)C and C\(\rightarrow\)A in ArmetMiner. Tasks DE\(\rightarrow\)FR, CN\(\rightarrow\)FR, JP\(\rightarrow\)RU, RU\(\rightarrow\)FR, CN\(\rightarrow\)RU, FR\(\rightarrow\)JP, RU\(\rightarrow\)DE, CN\(\rightarrow\)DE, RU\(\rightarrow\)US, DE\(\rightarrow\)CN, FR\(\rightarrow\)CN, DE\(\rightarrow\)RU and US\(\rightarrow\)RU in MAG.

## Appendix C GDA Baselines

MLP, GCN [47], GAT [49], and GIN [50] are classical GNN models. We directly adopt the implementation from Pytorch Geometric. The publicly available implementations of baselines can be found at the following URLs:

* **DANE**[10] uses shared weight GCNs to get node representations and then handles distribution shift via least square generative adversarial network. The source code is available at https://github.com/Jerry2398/DANE-Simple-implementation.

Figure 4: Label distribution of GDABench datasets.

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline \hline Dataset & Size & Feature Shift & Structure Shift & Label Shift & Domain Num \\ \hline Blog & S & 0.0132 & 0.0802 & 0.2532 & 2 \\ Airport & S & 0 & 0.2769 & 0.0351 & 3 \\ ArnetMiner & M & 0.0241 & 0.2074 & 1.1519 & 3 \\ Twitch & M & 0.0468 & 0.3264 & 8.6949 & 6 \\ MAG & L & 0.0499 & 0.3960 & 25.7725 & 6 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Domain shifts statistics of GDABench datasets.

* **ACDNE**[11] utilizes two feature extractors to jointly preserve attributed affinity and topological proximities as deep network embedding module and incorporates a domain classifier to make node representations label-discriminative. The source code is available at https://github.com/shenxiaacom/ACDNE.
* **UDAGCN**[12] develops a dual graph convolutional network with attention mechanism to jointly exploit local and global consistency for effective graph representation learning. The source code is available at https://github.com/GRAND-Lab/UDAGCN.
* **ASN**[44] separates domain-private and domain-shared information and combines local and global consistency to capture network topology information. The source code is available at https://github.com/yuntaodu/ASN.
* **AdaGCN**[13] leverages GCN to integrate network topology and combines adversarial domain adaptation with graph convolution. The source code is available at https://github.com/daiquanyu/AdaGCN_TKDE.
* **StruRW**[17] investigates different types of distribution shifts of graph-structured data and reweights edges in the source graph to reduces the conditional shift of neighborhoods. The source code is available at https://github.com/Graph-COM/StruRW.
* **GRADE**[15] introduces graph subtree discrepancy as a metric to measure the graph distribution shift by connecting GNNs with WL subtree kernel [45]. The source code is available at https://github.com/jwu4sml/GRADE.
* **SpecReg**[16] finds the OT-based bound for graph is closely coupled with the Lipschitz constant of GNN and proposes spectral regularization to modulate the Lipschitz constant to restrict the target risk bound. The source code is available at https://github.com/Shen-Lab/GDA-SpecReg.
* **A2GNN**[25] further investigates the GNN's underlying generalization capability behind its architecture and finds propagation operation plays a pivotal role. Based on this observation, A2GNN proposes a simple yet effective GNN which stacks more propagation layers on target branch. The source code is available at https://github.com/Meihan-Liu/24AAAI-A2GNN.
* **JHGDA**[18] designs a hierarchical pooling model to extract meaningful and adaptive hierarchical structures and jointly minimizes marginal and class conditional distribution shifts on each hierarchical level. The source code is available at https://github.com/Skyorca/JHGDA.
* **KBL**[19] redefines the aggregate mechanism as learning a knowledge-enhanced posterior distribution for target domains, which learns the scope of knowledge transfer by connecting knowledgeable samples between domains. The source code is available at https://github.com/wendongbi/Bridged-GNN.
* **DMGNN**[21] employes a GNN encoder with dual feature extractors to separate ego-embedding learning from neighbor-embedding learning and then a label propagation node classifier is employed

Figure 5: The compared performance of vanilla DA with 6 GNN variants.

to refine label prediction. The source code is available at https://github.com/shenxiaocam/DM_GNN.
* **CWGCN**[22] puts forward a two-step correntropy-induced Wasserstein GCN, which first suppresses the noisy nodes in the source graph and then learns the target GCN based on extending the Wasserstein distance. The source code is available at https://github.com/CocoLab-2022/CW-GCN.
* **SAGDA**[23] proposes a spectral augmentation module to enhance the node representation learning, which combines the target domain spectral information within the source domain. Since the authors did not release the source code, we try our best to reproduce their results.
* **DGDA**[24] addresses graph domain adaptation in a generative view, which disentangles the generation process into the semantic latent variables, the domain latent variables, and the random latent variables. The source code is available at https://github.com/rynewu224/GraphDA.
* **PairAlign**[26] not only uses edge weights to recalibrate the influence among neighboring nodes to handle conditional structure shift but also adjusts the classification loss with label weights to handle label shift. The source code is available at https://github.com/Graph-COM/Pair-Align.

## Appendix D Other Information in GDABench

We implement our GDABench library in PyTorch [73] and provide an infrastructure to run all the experiments to generate corresponding results. We have stored all models and logged all hyperparameters to facilitate reproducibility. Our framework can be easily extended to include new algorithms.

### Metrics

Following previous works [44, 12], we present the experiment performance on target domain. We select Area Under the Receiver Operating Characteristic Curve (AUROC) for Twitch, Micro-F1 for Airport, Blog and ArnetMiner and Macro-F1 for MAG.

* **AUROC** measures how well a model can distinguish between positive and negative classes by looking at the area under the ROC curve. This curve shows the true positive rate versus the false positive rate at various thresholds. An AUROC score of 1 means perfect distinction, while a score of 0.5 indicates the model does no better than guessing randomly.
* **Macro-F1** calculates the F1 score for each category independently and then taking the average of these scores. This method treats all categories equally, regardless of their frequency in the dataset.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{3}{c|}{Airport} & \multicolumn{3}{c}{ArnetMiner} \\ \cline{2-9}  & B \(\to\) E & B \(\to\) U & E \(\to\) B & U \(\to\) B & A \(\to\) C & A \(\to\) D & C \(\to\) D & D \(\to\) C \\ \hline DANE & 33.00 & 41.23 & 41.98 & 39.44 & 64.40 & 62.52 & 66.13 & 71.30 \\ ACDNE & 46.45 & 56.30 & 55.73 & 64.12 & 79.07 & 74.27 & 75.47 & 79.06 \\ UDAGCN & 43.78 & 35.49 & 45.29 & 37.91 & 78.21 & 72.98 & 76.14 & 72.15 \\ ASN & 53.05 & 46.58 & 62.34 & 49.36 & 78.68 & 72.02 & 75.57 & 77.58 \\ AdaGCN & 50.63 & 43.47 & 60.56 & 61.32 & 73.87 & 66.91 & 72.56 & 71.20 \\ DMGNN & 33.92 & 29.92 & 35.37 & 34.10 & 81.59 & 76.62 & 76.77 & 80.65 \\ CWGCN & 46.37 & 46.58 & 58.78 & 44.27 & 80.00 & 74.29 & 76.23 & 76.95 \\ SAGDA & 35.51 & 37.76 & 47.33 & 48.35 & 77.5 & 70.56 & 74.03 & 59.49 \\ DGDA & 49.71 & 33.56 & 44.02 & 49.36 & 64.48 & 57.85 & 63.29 & 57.98 \\ \hline StruRW & 56.06 & 43.36 & 65.65 & 61.32 & 77.24 & 67.51 & 74.37 & 73.96 \\ KBL & 45.28 & 45.52 & 51.40 & 33.84 & 77.71 & 69.16 & 74.48 & 74.62 \\ JHGDA & 48.87 & 40.59 & 65.14 & 43.51 & 73.74 & 69.13 & 71.71 & 71.59 \\ PairAlign & 39.93 & 42.18 & 51.91 & 54.96 & 68.29 & 61.80 & 62.89 & 63.28 \\ \hline GRADE & 52.88 & 49.22 & 75.83 & 49.62 & 74.09 & 69.18 & 72.57 & 73.12 \\ SpecReg & 48.87 & 44.20 & 63.36 & 40.97 & 80.81 & 73.16 & 74.60 & 71.96 \\ A2GNN & 53.13 & 54.54 & 62.34 & 59.29 & 82.64 & 77.43 & 78.13 & 81.54 \\ \hline SimGDA & 55.64 & 53.11 & 60.31 & 62.60 & 79.91 & 75.16 & 75.95 & 77.31 \\ SimGDA+ & 58.40 & 57.56 & 72.14 & 67.18 & 82.97 & 76.60 & 77.50 & 82.09 \\ \hline \hline \end{tabular}
\end{table}
Table 9: We evaluated the Micro-F1 score on Airport and ArnetMiner.

It is particularly useful when you want to understand the model's performance across smaller or less frequent categories, ensuring that performance on rare categories has as much weight as performance on more common ones.
* **Micro-F1** computes the average F1 score. This is achieved by summing up the true positives, false positives, and false negatives of the model across all categories and then calculating the F1 score using these totals. As a result, Micro-F1 gives a higher weight to the performance on more frequent categories, making it a useful metric when you're interested in understanding how the model performs on the majority of cases or the overall dataset.

### Additional Experimental Details

* **Hardware Specifications.** The experiments were conducted on a Linux server equipped with an Intel(R) Xeon(R) Platinum 8163 CPU operating at 2.50GHz, running Ubuntu 18.04.5 LTS. For GPU resources, we utilized a single NVIDIA Tesla V100 graphics card with 32GB of memory. The Python libraries employed for implementing our experiments include Python 3.8, PyTorch 1.13.1, PyTorch Geometric 2.4.0, PyTorch Sparse 0.6.15, and PyTorch Scatter 2.1.0.
* **Hyperparameter Settings.** To control the effect of hyperparameter selection and ensure fairness, we standardize the evaluation process with hyperparameter tuning. We utilize grid search to form the predefined search space for each models. We use all the source nodes and target nodes for model training. The experiments are repeated three times, and we report the mean performance. Table 14 provides a comprehensive list of all hyperparameters used in our grid search.
* **More Experimental Results.** In accordance with Table 2 and 3, we provide the performance for all tasks of each model in Table 9, 11 and 12. In accordance with Figure 3, we provide the compared performance of vanilla DA with 6 GNN variants in Figure 5.
* **Exploration of Hyperparameter Impact.** We investigate how various hyperparameters in common modules influence the performance of different UGDA methods on ArnetMiner dataset (task D \(\rightarrow\) A). We focus on two key aspects: the number of GNN layers and the representation dimensions. Results are shown in Figure 6.
* **Running Time and Memory Consumption.** We also demonstrate the running time and memory consumption of each model on S/M/L datasets respectively. For time consumption, we evaluate the

Figure 6: The impact of node representation dimension and the number of layers on ArnetMiner dataset (D\(\rightarrow\)A). We classify all baselines into three groups: DA incorporated node embedding methods (Group 1), structure shift directed alignment (Group 2) and domain adaptive message passing (Group 3). The first row illustrates the impact of node representation dimension, while the second row presents the effect of the number of layers.

efficiency of baselines by measuring the time it takes to converge. As shown in Figure 7, we can observe that some algorithms (e.g. A2GNN) can achieve relatively good performance with less complexity.

### The PyGDA Library

PyGDA is a Python library for Graph Domain Adaptation built upon PyTorch and PyG to easily train graph domain adaptation models in a sklearn style. PyGDA includes 15+ graph domain adaptation models. See examples with PyGDA below!

Graph Domain Adaptation Using PyGDA with 5 Lines of Code

from pygda.models import A2GNN

_# choose a graph domain adaptation model_ model = A2GNN(in_dim=num_features, hid_dim=args.nhid, num_classes=num_classes, device=args.device)

_# train the model_ model.fit(source_data, target_data)

_# evaluate the performance_ logits, labels = model.predict(target_data)

PyGDA is featured for:

* Consistent APIs and comprehensive documentation.
* Cover 15+ graph domain adaptation models.
* Scalable architecture that efficiently handles large graph datasets through mini-batching and sampling techniques.
* Seamlessly integrated data processing with PyG, ensuring full compatibility with PyG data structures.

## Appendix E Experiments on Graph Classification

To expand our research scope, we take graph-level shifts into consideration and add a pooling layer to evaluate capabilities of baselines in graph-level domain adaptation. We employ three TUdatasets: Proteins, Mutagenicity, and Frankenstein, partitioning each dataset into 2 equally sized disjoint groups based on density shifts. Detailed statistics are shown in Table 10.

The results are detailed in Table 15 and Table 16. Among the methods, GRADE and A2GNN are domain adaptive message passing methods and the remaining are DA incorporated node embedding methods. Key observations are as follows:

_DA incorporated node embedding methods shows task-inconsistency across node and graph-level tasks._ For example, DANE performs averagely in node-level tasks, but its performance improves significantly in graph-level tasks. This disparity highlights a challenge in predicting the performance of unsupervised graph domain adaptation (UGDA) models in real-world applications. The inconsistency suggests that models optimized for node-level tasks may not generalize well to graph-level tasks and

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline Dataset & \(\#\) Nodes & \(\#\) Edges & \(\#\) Feature & \(\#\) Class & Num of graphs \\ \hline Proteins & 39.06 & 72.82 & 4 & 2 & 1,113 \\ Mutagenicity & 30.32 & 30.77 & 14 & 2 & 4,337 \\ Frankenstein & 16.90 & 17.88 & 780 & 2 & 4,337 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Statistics of graph-level datasets in GDABench.

Figure 7: Running time and memory consumption of baselines.

[MISSING_PAGE_EMPTY:24]

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline Dataset & Source & Target & Feature Shift & Structure Shift & Label Shift \\ \hline \multirow{3}{*}{Blog} & Blog1 & Blog2 & 0.0140 & 0.0802 & 0.253 \\  & Blog2 & Blog1 & 0.0137 & 0.0802 & 0.258 \\ \hline \multirow{4}{*}{Airport} & USA & BRAZIL & 0.0514 & 0.2331 & 0.065 \\  & USA & EUROPE & 0.0913 & 0.3983 & 0.005 \\  & BRAZIL & USA & 0.0523 & 0.2331 & 0.066 \\  & RAZIL & EUROPE & 0.0549 & 0.1993 & 0.035 \\  & EUROPE & USA & 0.1000 & 0.3983 & 0.005 \\  & EUROPE & BRAZIL & 0.0582 & 0.1993 & 0.034 \\ \hline \multirow{4}{*}{AmetMiner} & DBLP\(\gamma\)7 & ACM\(\gamma\)9 & 0.0312 & 0.2327 & 0.997 \\  & DBLP\(\gamma\)7 & Citation\(\gamma\)1 & 0.0245 & 0.1965 & 1.643 \\  & ACM\(\gamma\)9 & DBLP\(\gamma\)7 & 0.0305 & 0.2327 & 1.062 \\  & ACM\(\gamma\)9 & Citation\(\gamma\)1 & 0.0163 & 0.1931 & 0.780 \\  & Citation\(\gamma\)1 & DBLP\(\gamma\)7 & 0.0244 & 0.1965 & 1.624 \\  & Citation\(\gamma\)1 & ACM\(\gamma\)9 & 0.0166 & 0.1931 & 0.805 \\ \hline \multirow{4}{*}{Twitch} & EN & DE & 0.0493 & 0.1486 & 0.715 \\  & EN & FR & 0.0440 & 0.3148 & 6.449 \\  & EN & RU & 0.0368 & 0.5960 & 20.578 \\  & EN & ES & 0.0530 & 0.3836 & 13.883 \\  & EN & PT & 0.0790 & 0.3374 & 8.330 \\  & DE & EN & 0.0478 & 0.1486 & 0.707 \\  & DE & FR & 0.0408 & 0.4635 & 11.403 \\  & DE & RU & 0.0387 & 0.7446 & 28.985 \\  & DE & ES & 0.0283 & 0.5233 & 20.866 \\  & DE & PT & 0.0391 & 0.4860 & 13.871 \\  & FR & EN & 0.0463 & 0.3148 & 6.315 \\  & FR & DE & 0.0333 & 0.4635 & 11.302 \\  & FR & RU & 0.0503 & 0.2811 & 3.754 \\  & FR & ES & 0.0432 & 0.0688 & 1.335 \\  & FR & PT & 0.0733 & 0.0226 & 0.115 \\  & RU & EN & 0.0369 & 0.5960 & 18.693 \\  & RU & DE & 0.0355 & 0.7446 & 26.658 \\  & RU & FR & 0.0542 & 0.2811 & 3.479 \\  & RU & ES & 0.0426 & 0.2124 & 0.562 \\  & RU & PT & 0.0551 & 0.2586 & 2.363 \\  & ES & EN & 0.0525 & 0.3386 & 13.080 \\  & ES & DE & 0.0282 & 0.5323 & 19.901 \\  & ES & FR & 0.0406 & 0.0688 & 1.284 \\  & ES & RU & 0.0460 & 0.2124 & 0.583 \\  & ES & PT & 0.0320 & 0.0462 & 0.640 \\  & PT & EN & 0.0776 & 0.3374 & 8.080 \\  & PT & DE & 0.0407 & 0.4860 & 13.620 \\  & PT & FR & 0.0713 & 0.0226 & 0.114 \\  & RU & 0.0554 & 0.2586 & 2.526 \\  & PT & ES & 0.0311 & 0.0462 & 0.660 \\ \hline \multirow{4}{*}{CN} & DE & 0.0750 & 0.3608 & 33.807 \\  & CN & FR & 0.0773 & 0.3902 & 26.427 \\  & CP & 0.0451 & 0.2775 & 16.382 \\  & CN & RU & 0.0779 & 0.5454 & 30.058 \\  & US & 0.0781 & 0.2858 & 28.992 \\  & DE & CN & 0.0727 & 0.3608 & 46.271 \\  & DE & FR & 0.0213 & 0.2041 & 2.316 \\  & DE & IP & 0.0464 & 0.3278 & 22.811 \\  & DE & RU & 0.0419 & 0.4778 & 48.632 \\  & DE & US & 0.0179 & 0.3561 & 16.266 \\  & FR & CN & 0.0702 & 0.3902 & 46.780 \\  & FR & DE & 0.0196 & 0.2041 & 2.241 \\  & FR & JP & 0.0508 & 0.3815 & 30.343 \\  & FR & RU & 0.0382 & 0.5091 & 49.558 \\  & FR & US & 0.0187 & 0.4210 & 23.644 \\  & JP & CN & 0.0486 & 0.2775 & 14.352 \\  & JP & DE & 0.0391 & 0.3278 & 12.240 \\  & JP & PR & 0.0467 & 0.3815 & 13.597 \\  & JP & RU & 0.0513 & 0.4968 & 27.544 \\  & JP & US & 0.0540 & 0.2893 & 8.235 \\  & RU & CN & 0.0776 & 0.5454 & 18.701 \\  & RU & DE & 0.0442 & 0.4778 & 31.345 \\  & RU & FR & 0.0416 & 0.5091 & 28.260 \\  & RU & IP & 0.0524 & 0.4968 & 18.269 \\  & RU & US & 0.0517 & 0.6171 & 35.979 \\  & US & CN & 0.0832 & 0.2858 & 35.273 \\  & US & DE & 0.0206 & 0.3561 & 14.702 \\  & US & FR & 0.0197 & 0.4210 & 19.245 \\  & JP & 0.0431 & 0.2893 & 10.104 \\  & US & RU & 0.0567 & 0.6171 & 60.803 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Domain shifts statistics of each task.

[MISSING_PAGE_FAIL:27]

vice versa. Consequently, this variability complicates the task of assessing how well these models will perform when deployed in diverse and complex real-world scenarios where both node-level and graph-level information may be critical.

_Domain adaptive message passing methods demonstrate superior and consistency performance across a wide range of datasets and tasks._ As shown in Table 3, 9, 16 and 15, methods designed based on the inherent properties of GNN achieves the top-three best performance in 8 tasks out of 12 node-level tasks and top-two best performance in 5 tasks out of 6 graph-level tasks. This observation verified our findings that establishing domain adaptation principles by leveraging inherent properties of GNN can result in an effective and efficient approach to addressing the challenges of domain variability in graph datasets.

To summarize, our observations underscore the importance of leveraging the intrinsic properties of GNNs to devise effective domain adaptation strategies, which not only enhances performance but also ensures consistency in real-world applications.

## Appendix F Discussion

### How these findings generalize to real-world scenarios

Our benchmark includes a range of datasets with varying characteristics to capture different aspects of graph domain adaptation. This diversity aims to provide a broad perspective on the applicability of our methods. In real-world scenarios, applying graph adaptation methods effectively involves several key considerations: Firstly, it is imperative to develop tailored strategies specifically designed to address the structural shifts observed in graphs. For example, if a graph is dynamic and changing overtime, it is crucial to accord greater attention to its evolving structure. Secondly, recognizing the importance of the aggregation scope and aggregation architecture in GNNs' transferability within unsupervised graph domain adaptation (UGDA) are crucial. In real-world graphs, noise is inevitable, hence, strategically selecting effective neighbors not only improve performance but also avoid noise.

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{2}{c|}{Proteins} & \multicolumn{2}{c|}{Mutagenicity} & \multicolumn{2}{c}{Frankenstein} \\ \cline{2-7}  & P1 \(\rightarrow\) P2 & P2 \(\rightarrow\) P1 & M1 \(\rightarrow\) M2 & M2 \(\rightarrow\) M1 & F1 \(\rightarrow\) F2 & F2 \(\rightarrow\) F1 \\ \hline DANE & **59.14**\(\pm_{3.06}\) & 56.30 \(\pm_{6.09}\) & 67.11 \(\pm_{0.17}\) & **76.50**\(\pm_{0.35}\) & 52.24 \(\pm_{1.02}\) & 54.94 \(\pm_{2.13}\) \\ UDAGCN & 53.15 \(\pm_{2.74}\) & 50.19 \(\pm_{1.20}\) & 56.71 \(\pm_{0.61}\) & 63.35 \(\pm_{0.56}\) & 50.06 \(\pm_{0.64}\) & 52.32 \(\pm_{1.40}\) \\ AdaGCN & 49.33 \(\pm_{1.62}\) & 57.99 \(\pm_{2.82}\) & 58.00 \(\pm_{0.10}\) & 35.97 \(\pm_{0.10}\) & 55.99 \(\pm_{0.94}\) & 51.76 \(\pm_{4.43}\) \\ CWGCN & 40.57 \(\pm_{3.13}\) & 42.75 \(\pm_{6.01}\) & 39.00 \(\pm_{4.96}\) & 37.32 \(\pm_{1.66}\) & 39.46 \(\pm_{0.22}\) & 51.68 \(\pm_{0.67}\) \\ SAGDA & 46.65 \(\pm_{6.14}\) & 33.42 \(\pm_{1.01}\) & 56.26 \(\pm_{3.74}\) & 54.95 \(\pm_{8.22}\) & 36.89 \(\pm_{4.93}\) & 38.03 \(\pm_{6.81}\) \\ \hline GRADE & 32.23 \(\pm_{0.86}\) & 50.52 \(\pm_{1.77}\) & **68.98**\(\pm_{0.21}\) & 76.32 \(\pm_{0.26}\) & **56.93**\(\pm_{1.80}\) & **54.98**\(\pm_{2.62}\) \\ A2GNN & 47.71 \(\pm_{3.22}\) & **58.85**\(\pm_{1.16}\) & 55.42 \(\pm_{0.10}\) & 50.17 \(\pm_{1.59}\) & 46.97 \(\pm_{0.96}\) & 43.33 \(\pm_{1.87}\) \\ \hline \hline \end{tabular}
\end{table}
Table 16: To evaluate the baselines on graph-level shifts, we compared the Macro-F1 scores of each model on the Proteins, Mutagenicity, and Frankenstein datasets. The best results are highlighted in **bold**, and the second-best results are underlined.

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{2}{c|}{Proteins} & \multicolumn{2}{c|}{Mutagenicity} & \multicolumn{2}{c}{Frankenstein} \\ \cline{2-7}  & P1 \(\rightarrow\) P2 & P2 \(\rightarrow\) P1 & M1 \(\rightarrow\) M2 & M2 \(\rightarrow\) M1 & F1 \(\rightarrow\) F2 & F2 \(\rightarrow\) F1 \\ \hline DANE & **60.14**\(\pm_{3.58}\) & 75.66 \(\pm_{0.98}\) & 67.25 \(\pm_{0.14}\) & **76.92**\(\pm_{0.35}\) & 54.77 \(\pm_{0.53}\) & 56.96 \(\pm_{2.89}\) \\ UDAGCN & 53.50 \(\pm_{2.42}\) & 73.14 \(\pm_{4.29}\) & 58.11 \(\pm_{0.58}\) & 65.34 \(\pm_{0.55}\) & 52.48 \(\pm_{0.32}\) & 52.37 \(\pm_{1.38}\) \\ AdaGCN & 52.60 \(\pm_{0.78}\) & **78.12**\(\pm_{0.37}\) & 58.89 \(\pm_{0.06}\) & 56.18 \(\pm_{0.02}\) & 56.28 \(\pm_{0.75}\) & 53.01 \(\pm_{3.63}\) \\ CWGCN & 50.45 \(\pm_{8.81}\) & 44.84 \(\pm_{8.20}\) & 55.60 \(\pm_{1.27}\) & 56.72 \(\pm_{0.67}\) & 49.76 \(\pm_{0.27}\) & 51.92 \(\pm_{0.71}\) \\ SAGDA & 53.14 \(\pm_{4.80}\) & 46.22 \(\pm_{2.99}\) & 57.06 \(\pm_{3.54}\) & 56.00 \(\pm_{8.85}\) & 50.35 \(\pm_{0.26}\) & 51.01 \(\pm_{8.37}\) \\ \hline GRADE & 43.93 \(\pm_{0.31}\) & 76.80 \(\pm_{0.29}\) & **69.00**\(\pm_{0.22}\) & 76.57 \(\pm_{0.31}\) & **57.54**\(\pm_{1.09}\) & **58.39**\(\pm_{4.57}\) \\ A2GNN & 51.70 \(\pm_{1.54}\) & 69.65 \(\pm_{4.21}\) & 56.83 \(\pm_{0.19}\) & 58.88 \(\pm_{1.23}\) & 50.43 \(\pm_{0.69}\) & 48.99 \(\pm_{3.97}\) \\ \hline \hline \end{tabular}
\end{table}
Table 15: To evaluate the baselines on graph-level shifts, we compared the Micro-F1 scores of each model on the Proteins, Mutagenicity, and Frankenstein datasets. The best results are highlighted in **bold**, and the second-best results are underlined.

Thirdly, by leveraging the properties of GNNs that make them inherently adaptable to changes in graph structure and data distribution, we can develop simple yet highly effective models.

### A broader discussion on DA problem and other related UGDA scenarios

UDA vs UGDA.Unsupervised domain adaptation (UDA) entails transferring knowledge from a labeled source domain to an unlabeled target domain. A prevalent strategy in domain adaptation is to reduce domain discrepancies while learning domain-invariant representations, a method that has seen considerable success in the fields of computer vision and natural language processing. However, these techniques typically operate under the assumption that inputs are independently and identically distributed (IID), making them unsuitable for tasks involving non-IID data, such as node classification in graph-structured datasets.

Ugda vs muti-domain Ugda.Muti-domain UGDA extends the concept of domain adaptation to situations where there are multiple source domains and a single target domain. This approach aims to learn a model that can generalize well across multiple source domains, and then adapt it to perform well on the target domain. Compared to standard UGDA, multi-domain UGDA can enhance generalization by leveraging the diversity of multiple source domains. However, it may require more complex models and additional computational resources.

Ugda vs source-free Ugda.Source-free UGDA advances domain adaptation by tackling the challenge of adapting models without access to labeled data from the source domains. This setting is more challenging as it involves learning to transfer knowledge without explicit supervision. Source-free UGDA methods often employ techniques such as self-training or consistency regularization to adapt the model to the target domain. Compared to UGDA, source-free UGDA may be more sensitive to domain shift and require careful selection of adaptation techniques.