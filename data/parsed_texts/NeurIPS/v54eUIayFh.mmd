# UniControl: A Unified Diffusion Model for

Controllable Visual Generation In the Wild

 Can Qin

Shu Zhang

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Ning Yu

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Yihao Feng

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Xinyi Yang

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Yingbo Zhou

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Huan Wang

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Juan Carlos Niebles

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Caiming Xiong

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Silvio Savarese

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Stefano Ermon

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Yun Fu

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Ran Xu

Salesforce AI Research, \({}^{\star}\)Northeastern University, \({}^{\ddagger}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

###### Abstract

Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions, we augment pretrained text-to-image diffusion models and introduce a task-aware HyperNet to modulate the diffusion models, enabling the adaptation to different C2I tasks simultaneously. Trained on nine unique C2I tasks, UniControl demonstrates impressive zero-shot generation abilities with unseen visual conditions. Experimental results show that UniControl often surpasses the performance of single-task-controlled methods of comparable model sizes. This control versatility positions UniControl as a significant advancement in the realm of controllable visual generation. 1

Footnote 1: Code: https://github.com/salesforce/UniControl

## 1 Introduction

Generative foundation models are revolutionizing the ways that humans and AI interact in natural language processing (NLP) [1, 2, 3, 4, 5, 6], computer vision (CV) [7, 8, 9, 10], audio processing (AP) [11, 12], and robotic controls [13, 14, 15], to name a few. In NLP, generative foundation models such as InstructGPT or GPT-4, achieve excellent performance on a wide range of tasks, _e.g.,_ question answering, summarization, text generation, or machine translation within a _single-unified_ model. Such multi-tasking ability is one of the most appealing characteristics of generative foundation models. Furthermore, generative foundation models can also perform zero-shot or few-shot learning on unseen tasks [3, 16, 17].

For generative models in vision domains [9, 18, 19, 20], such multi-tasking ability is less clear. Stable Diffusion Model (SDM) [9] has established itself as the major cornerstone for text-conditioned image generation. However, while text descriptions provide a very flexible way to control the generated images, their ability to provide pixel-level precision for spatial, structural, or geometric controls is
Figure 1: UniControl is trained with multiple tasks with a unified model, and it further demonstrates promising capability in zero-shot tasks generalization with visual example results shown above.

language and various visual conditions. Naturally, UniControl can perform multi-tasking and can encode visual conditions from different tasks into a universal representation space, seeking a common representation structure among tasks. The unified design of UniControl allows us to enjoy the advantages of improved training and inference efficiency, as well as enhanced controllable generation. On the one hand, the model size of UniControl does not significantly increase as the number of tasks scales up. On the other hand, UniControl derives advantages from the inherent connections between different visual conditions [_e.g._, 23, 24, 25]. These relationships, such as depth and segmentation mapping, leverage shared geometric information to enhance the controllable generation quality.

The unified controllable generation ability of UniControl relies on two novel designed modules, a _mixture of expert (MOE)-style adapter_ and a _task-aware HyperNet_[26, 27]. The MOE-style adapter can learn necessary low-level feature maps from various visual conditions, allowing UniControl to capture unique information from different visual conditions. The task-aware HyperNet, which takes the task instruction as natural language prompt inputs, and outputs a task-aware embedding. The output embeddings can be incorporated to modulate ControlNet [21] for task-aware visual condition controls, where each task corresponds to a particular format of visual condition. As a result, the task-aware HyperNet allows UniControl to learn meta-knowledge across various tasks, and obtain abilities to generalize to unseen tasks. As Tab. 1, UniControl has significantly compressed the model size compared with its direct baseline, _i.e._, Multi-ControlNet, by unifying nine tasks into **ONE** model.

To obtain multi-tasking and zero-shot learning abilities, we pre-train UniControl on nine distinct tasks across five categories: **1)**_edges_ (Canny, HED, User Sketch); **2)**_region-wise maps_ (Segmentation Maps, Bounding Boxes); **3)**_skeletons_ (Human Pose Skeletons); **4)**_geometric maps_ Depth, Surface Normal); **5)**_editing_ (Image Outpainting). We build **MultiGen-20M** dataset, comprising over 20 million high-quality triplets of original images, language prompts, and visual conditions for all the tasks. Then UniControl is trained for over 5,000 GPU hours on NVIDIA A100-40G hardware that is comparable with the overall training cost of different ControlNets. Moreover, UniControl exhibits a remarkable capacity for zero-shot adaptation to new tasks, highlighting its potential for deployment in real-world applications. Our contributions are summarized below:

\(\bullet\) We present UniControl, a unified model capable of handling various visual conditions for the controllable visual generation.

\(\bullet\) We collect a new dataset for multi-condition visual generation with more than 20 million image-text-condition triplets over nine distinct tasks across five categories.

\(\bullet\) We conduct extensive experiments to demonstrate that the unified model UniControl outperforms each single-task controlled image generation, thanks to learning the intrinsic relationships between different visual conditions.

\(\bullet\) UniControl shows the ability to adapt to unseen tasks in a zero-shot manner, highlighting its versatility and potential for widespread adoption in the wild.

## 2 Related Works

**Diffusion-based Generative Models.** Diffusion models were initially introduced in [28] that yield favorable outcomes for generating images [18, 21]. Improvements have been made through various training and sampling techniques such as score-based diffusion [29, 30], Denoising Diffusion Probabilistic Model (DDPM) [31], and Denoising Diffusion Implicit Model (DDIM) [32], When training U-Net denoisers [33] with high-resolution images, researchers involve speed-up techniques including pyramids [34], multiple stages [20], or latent representations [9]. In particular, UniControl leverages Stable Diffusion Models (SDM) [9] as the base model to perform multi-tasking.

**Text-to-Image Diffusion.** Diffusion models emerge to set up a cutting-edge performance in text-to-image generation tasks [20, 19], by cross-attending U-Net denoiser in diffusion generators with CLIP [22] or T5-pretrained [2] text embeddings. GLIDE [35] is another example of a text-guided diffusion model that supports image generation and editing. UniControl and closely related

\begin{table}
\begin{tabular}{c|c c c c|c} \hline \hline  & **Stable Diffusion** & **ControlNet** & **MoE-Adapter** & **TaskHyperNet** & **Total** \\ \hline
**UniControl** & 1065.7M & 361M & 0.06M & 12.7M & **1.44B** \\
**Multi-ControlNet** & 1065.7M & 361M \(\times\) 9 & - & - & 4.32B \\ \hline \hline \end{tabular}
\end{table}
Table 1: Architecture and Model Size (\(\#\)Params): UniControl _vs._ Multi-ControlNetControlNet [21] are both built upon previous works on diffusion-based text-to-image generation [9]. [36] introduces the compositional conditions to guide visual generation.

**Image-to-Image Translation.** Image-to-image (I2I) translation task was initially proposed in Pix2Pix [37], focusing on learning a mapping between images in different domains. Recently, diffusion-based approaches [38; 39; 21] set up the new state of the art results. Recent diffusion-based image editing methods show outstanding performances without requiring paired data, _e.g.,_ SDEdit [40], prompt-to-prompt [41], Edict [42]. Other image editing examples include various diffusion bridges and flows [43; 44; 45; 46; 47], classifier guidance [30] based methods for colorization, super-resolution [34], inpainting [48], and _etc_. ControlNet [21] takes both visual and text conditions and achieves new state-of-the-art controllable image generation. Our proposed UniControl unifies various visual conditions of ControlNet, and is capable of performing zero-shot learning on newly unseen tasks. Concurrently, Prompt Diffusion [49] introduces visual prompt [50] from image inpainting to controllable diffusion models, which requires two additional image pairs as the in-context example for both training and inference. By contrast, UniControl takes only a single visual condition while still capable of both multi-tasking and zero-shot learning.

## 3 UniControl

In this section, we describe the training and the model design of our unified controllable diffusion model **UniControl**. Specifically, we first provide the problem setup and training objectives in Sec. 3.1, and then show the novel network design of UniControl in Sec. 3.2. Finally, we explain how to perform zero-shot image generation with the trained UniControl in Sec. 3.3.

### Training Setup

Different from the previous generative models such as Stable Diffusion Models (SDM) [9] or ControlNet [21], where the image generation conditions are _single_ language prompt, or _single_ type of visual condition such as _canny_, UniControl is required to take a wide range of visual conditions from different tasks, as well as the language prompt.

To achieve this, we reformulate the training conditions and target pairs for UniControl. Specifically, suppose we have a dataset consisting of \(K\) tasks : \(\mathcal{D}:=\{\mathcal{D}_{1}\cup\cdots\cup\mathcal{D}_{K}\}\), and for each task training set \(\mathcal{D}_{k}\), denote the training pairs by \(([c_{\mathrm{text}},c_{\mathrm{task}}],\mathcal{I}_{c},\bm{x})\), with \(c_{\mathrm{task}}\) being the task instruction that indicates the task type, \(c_{\mathrm{text}}\) being the language prompt describing the target image, \(\mathcal{I}_{c}\) being the visual conditions, and \(\bm{x}\) being the target image. With the additional task instruction, UniControl can differentiate visual conditions from different tasks. A concrete training example pair is the following:

where the task is to _translate the canny edge to real images following language prompt_. With the induced training pairs \((\bm{x},[c_{\mathrm{task}},c_{\mathrm{text}}],\mathcal{I}_{c})\), we define the training loss for task \(k\) following LDM [9]:

\[\ell^{k}(\theta):=\mathbb{E}_{z,e,t,c_{\mathrm{task}},c_{\mathrm{text}}, \mathcal{I}_{c}}\left[\|\varepsilon-\varepsilon_{\theta}(z_{t},t,c_{\mathrm{ task}},c_{\mathrm{text}},\mathcal{I}_{c})\|_{2}^{2}\right]\,,\text{with}\ ([c_{\mathrm{task}},c_{\mathrm{text}}],\mathcal{I}_{c},\bm{x})\sim \mathcal{D}_{k}\,,\]

where \(t\) represents the time step, \(z_{t}\) is the noise-corrupted latent tensor at time step \(t\), \(z_{0}=E(\bm{x})\), and \(\theta\) is the trainable parameters of UniControl. We also apply classifier-free guidance [51] to randomly drop 30% text prompts to enhance the controllability of input visual conditions. We train UniControl uninformly on the \(K\) tasks. To be more specific, we first randomly select a task \(k\) and sample a mini-match from \(\mathcal{D}_{k}\), and optimize \(\theta\) with the calculated loss \(\ell^{k}(\theta)\).

### Model Design

Since our unified model UniControl needs to achieve superior performance on a set of diverse tasks, it is necessary to ensure the network design enjoys the following properties: **1)** The model can overcome the misalignment of low-level features from different tasks; **2)** The model can learn meta-knowledge across tasks, and adapt to each task effectively.

The first property can ensure that UniControl can learn necessary and unique information from all tasks. For instance, if UniControl takes the segmentation map as the visual condition, the model might ignore the 3D information. As a result, the feature map learned may not be suitable for the task that takes the depth map images as visual condition. The second property would allow the model to learn the shared knowledge across tasks, as well as the differences among them.

We introduce two novel designed modules, _MOE-style adapter_ and _task-aware HyperNet_, that allows UniControl enjoys the above two properties. An overview of the model design for UniControl is in Fig. 2. We describe the detailed designs of these modules below.

MOE-Style Adapter.Inspired by the design of Mixture-of-Experts (MOEs) [52], we devise a group of convolution modules to serve as the adapter for UniControl to capture features of various low-level visual conditions. Precisely, the designed adapter module can be expressed as

\[\mathcal{F}_{\mathrm{Adapter}}(\mathcal{I}_{c}^{k}):=\sum_{i=1}^{K}\mathds{1} (i==k)\cdot\mathcal{F}_{\mathrm{Cov1}}^{(i)}\circ\mathcal{F}_{\mathrm{Cov2}}^ {(i)}(\mathcal{I}_{c}^{k})\,,\]

where \(\mathds{1}(\cdot)\) is the indicator function, \(\mathcal{I}_{c}^{k}\) is the conditioned image from task \(k\), and \(\mathcal{F}_{\mathrm{Cov1}}^{(i)},\mathcal{F}_{\mathrm{Cov2}}^{(i)}\) are the convolution layers of the \(i\)-th module of the adapter. We remove the weights of the original MOEs since our designed adapter is required to differentiate various visual conditions. Meanwhile, naive MOE modules can not explicitly distinguish different visual conditions when the weights are learnable. Moreover, such task-specific MOE adapters facilitate the zero-shot tasks with explicit retrieval of the adapters of highly related pre-training tasks. Besides, the number of parameters for each convolution module is approximately 70K, which is computationally efficient.

Figure 2: This figure shows our proposed UniControl method. To accommodate diverse tasks, we’ve designed a Mixture of Experts (MOE) Adapter, containing roughly \(\sim\)70K \(\#\)params for each task, and a Task-aware HyperNet (\(\sim\)12M \(\#\)params) to modulate \(N\) (i.e., 7) zero-conv layers. This structure allows for multi-task functionality within a singular model, significantly reducing the model size compared to an equivalent stack of single-task models, each with around 1.4B \(\#\)params.

Task-Aware HyperNet.The task-aware HyperNet modulates the zero-convolution modules of ControlNet [21] with the task instruction condition \(c_{\mathrm{task}}\). As shown in Figure 2, our hyperNet first projects the task instruction \(c_{\mathrm{task}}\) into task embedding with the help of CLIPText encoder. Then similar in spirit of style modulation in StyleGAN2 [53], we inject the task embedding into the trainable copy of ControlNet, by multiplying the task embedding to each zero-conv layer. In specific, the length of the embedding is the same as the number of input channels of the zero-conv layer, and each element scalar in the embedding is multiplied to the convolution kernel per input channel. We also show that our newly designed task-aware HyperNet can also efficiently learn from training instances and task supervision following a similar analysis as in ControlNet [21].

### Task Generalization Ability

With the comprehensive pretraining on the MultiGen-20M dataset, UniControl exhibits zero-shot capabilities on tasks that were not encountered during its training, suggesting that Unicontrol possesses the ability to transcend in-domain distributions for broader generalization. We demonstrate the zero-shot ability of UniControl in the following two scenarios:

Hybrid Tasks Generalization.As shown in the left side of Fig. 3, We consider two different visual conditions as the input of UniControl, a hybrid combination of segmentation maps and human skeletons, and augment specific keywords "background" and "foreground" into the text prompts. Besides, we rewrite the hybrid task instruction as a blend of instructions of the combined two tasks such as "segmentation map and human skeleton to image".

Zero-Shot New Tasks Generalization.As shown in the right side of Fig. 3, UniControl needs to generate controllable images on a newly unseen visual condition. To achieve this, estimating the task weights based on the relationship between unseen and seen pre-trained tasks is essential. The task weights can be estimated by either manual assignment or calculating the similarity score of task instructions in the embedding space. The example result in Fig. 5 (d) is generated by our manually assigned MOE weights as "depth: 0.6, seg: 0.3, canny: 0.1" for colorization. The MOE-style adapter can be linearly assembled with the estimated task weights to extract shallow features from the newly unseen visual condition.

## 4 Experiments

We empirically evaluate the effectiveness and robustness of UniControl. We conduct a series of comprehensive experiments across various conditions and tasks, utilizing diverse datasets to challenge the model's adaptability and versatility. Experimental setup, methodologies, and results analysis are provided in the subsequent sections.

### Experiment Setup

Implementation.The UniControl is illustrated as Fig. 2 with Stable Diffusion, ControlNet, MOE Adapter, and Task-aware HyperNet consisting \(\sim\)1.5B parameters. MOE Adapter consists of parallel convolutional modules, each of which corresponds to one task. The task-aware HyperNet inputs the CLIP text embedding [22] of task instructions and outputs the task embeddings to modulate the weights of zero-conv kernels. We implement our model upon the ControlNet. We take the AdamW [54] as the optimizer based on PyTorch Lightning [55]. The learning rate is assigned as 1\(\times 10^{-5}\). Our full-version UniControl model is trained on 16 Nvidia-A100 GPUs with the batch size of 4, requiring \(\sim\) 5, 000 GPU hours. We have also applied Safety-Checker as safeguards of results.

Figure 3: Illustration of MOE’s behaviors under zero-shot scenarios. The left part shows the capacity of the MOE to generalize to hybrid task conditions, achieved through the integration of outputs from two pertinent convolution layers. The right part illustrates the ability of the MOE-style adapter to generalize to unseen tasks, facilitated by the aggregation of pre-trained tasks using estimated weights.

**Data Collection.** Since the training set of ControlNet is currently unavailable, we initiate our own data collection process from scratch and name it as **MultiGen-20M**. We use a subset of Laion-Aesthetics-V2 [56] with aesthetics ratings over six, excluding low-resolution images smaller than 512. This yields approximately 2.8 million image-text pairs. Subsequently, we process this dataset for nine distinct tasks across five categories (edges, regions, skeletons, geometric maps, real images):

* **Canny (2.8M)**: Utilize the Canny edge detector [57] with randomized thresholds.
* **HED (2.8M)**: Deploy the Holistically-nested edge detection [58] for robust boundary determination.
* **Depth (2.8M)**: Employ the Midas [59] for monocular depth estimation.
* **Normal (2.8M)**: Use the depth estimation results from the depth task to estimate scene or object surface normals.
* **Segmentation (2.8M)**: Implement the Uniformer [60] model, pre-trained on the ADE20K [61] dataset, to generate segmentation maps across 150 classes.
* **Object Bounding Box (874K)**: Utilize YOLO V4 [62] pre-trained on the COCO [63] dataset for bounding box labelling across 80 object classes.
* **Human Skeleton (1.3M)**: Employ the pre-trained Openpose [64] model to generate human skeleton labels from source images.
* **Image Outpainting (2.8M)**: Create boundary masks for source images with random masking percentages from 20% to 80%.

Further processings are carried out on HED maps using Gaussian filtering and binary thresholding to simulate user sketching. Overall, we amass over 20 million image-prompt-condition triplets. Task instructions were naturally derived from the respective conditions, with each task corresponding to a specific instruction, such as "canny edge to image" for the Canny task. We maintain a one-to-one correspondence between tasks and instructions without introducing variance to ensure stability during training. We have additionally collected a testing dataset for evaluation with 100-300 image-condition-prompt triplets for each task. The source data is collected from Laion and COCO. **We will open-source our training and testing data to contribute to the community.**

**Benchmark Models.** The most straightforward comparison for UniControl comes from task-specific ControlNet models. Six tasks overlap with those presented in ControlNet, so their official models are chosen as baselines for these tasks. For fair comparison, we re-implement the ControlNet model (single task) using our collected data. Our unified multi-task UniControl is compared against these

Figure 4: Visual comparison between official or re-implemented task-specific ControlNet and our proposed model. The example data is collected from our testing set sampled from COCO and Laion.

task-aware models for each task. We apply default sampler as DDIM [32] with guidance weight 9 and steps 50. All single-task models used for comparison are trained by 100K iterations and our multi-task model is trained around 900K with similar iterations for each task to ensure fairness. The efficiency and compact design of our proposed model are evident in its construction. The total size of UniControl is around 1.5B \(\#\)params and a single task ControlNet+SDM takes 1.4B. In order to achieve the same nine-task functionality, a single-task strategy would require the ensemble of a SDM with nine task-specific ControlNet models, amounting to approximately 4.3B \(\#\)params in total.

### Visual Comparison

We visually compare different tasks (Canny, HED, Depth, Normal, Segmentation, Openpose, Bounding Box, and Outpainting) in Fig. 4. Our method consistently outperforms the baseline ControlNet model. This superiority is in terms of both visual quality and alignment with conditions or prompts.

For the Canny task, the results generated by our model exhibit a higher degree of detail preservation and visual consistency. The outputs of UniControl maintain a faithful reproduction of the edge information (_i.e.,_ round table) compared to ControlNet. In the HED task, our model effectively captures the robust boundaries, leading to visually appealing images with clear and sharp edge transitions, whereas ControlNet results appear to be non-factual. Moreover, our model demonstrate a more subtle understanding of 3D geometrical guidance of depth maps and surface normals than ControlNet. The depth map conditions produce visibly more accurate outputs. In the Normal task, our model faithfully reproduces the normal surface information (_i.e.,_ ski pole), leading to more realistic

Figure 5: (a)-(b): Example results of UniControl over hybrid (unseen combination) conditions with key words “background” and ”foreground” attached in prompts. (c)-(e): Example results of UniControl on three unseen tasks (deblurring, colorization, inpainting).

Figure 6: User study between our method and official ControlNet checkpoints on six tasks. Our method outperforms ControlNet on all tasks.

and visually superior outputs. During the Segmentation, Openpose, and Object Bounding Box tasks, the produced images generated by our model are better aligned with the given conditions than that by ControlNet, ensuring a higher fidelity to the input prompts. For example, the re-implemented ControlNet-BBox misunderstands "a woman near a statue", whereas our outputs exhibit a high degree of accuracy and detail. In the Outpainting task, our model demonstrates its superiority by generating reasonable images with smooth transitions and natural-looking textures. It outperforms the ControlNet model, which produces less coherent results - "a bear missing one leg". This visual comparison underscores the strength and versatility of our approach across a diverse set of tasks.

### Quantitative Evaluation

User Study.We compare the performance of our method with both the released ControlNet model and the re-implemented single-task ControlNet on our training set. As shown in Fig. 6, our approach consistently outperforms the alternatives in all cases. In the HED-to-image generation task, our method significantly surpasses ControlNet. This superiority is even more pronounced in the depth and normal surface to image generation tasks, where users overwhelmingly favor our method, demonstrating its ability to handle complex geometric interpretations. When compared to the re-implemented single-task model, Fig. 7 reveals that our approach maintains a smaller advantage, yet it still demonstrates its benefits by effectively discerning image regions to guide content generation. Even in the challenging outpainting task, our model outperforms the baseline, highlighting its robustness and capacity to generalize.

Image Perceptual Metric.We evaluate the distance between our output and the ground truth image. As we aim to obtain a structural similar image to the ground truth image, we adopt the perceptual metric in [65], where a lower value indicates more similar images. As shown in Tab. 2, UniControl outperforms ControlNet on five tasks, and obtains the same image distance to ControlNet on Segmentation.

Frechet Inception Distance (FID).We've further conducted quantitative analysis with FID [66] to include more classic single-task-controlled methods such as GLIGEN [67] and T2I-adapter [68]. With a collection of over 2,000 test samples sourced from Laion and COCO, we've assessed a wide range of tasks covering edges (Canny, HED), regions (Seg), skeletons (Pose), and geometric maps (Depth, Normal). The Tab. 3 demonstrates that our UniControl consistently surpasses the baseline methods across the majority of tasks. Notably, UniControl achieves this while maintaining a more compact and efficient architecture than its counterparts.

Ablation Study.We've conducted an ablation study, specifically focusing on the MoE-Style Adapter and TaskHyperNet in Tab. 4 with FID scores reported as the previous part. It is noticeable that the full-version UniControl (MoE-Style Adapter + TaskHyperNet) significantly outperforms the ablations which demonstrates the superiority of proposed MoE-Style Adapter and TaskHyperNet.

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline  & **Canny \(\downarrow\)** & **HED \(\downarrow\)** & **Normal \(\downarrow\)** & **Depth \(\downarrow\)** & **Pose \(\downarrow\)** & **Segmentation \(\downarrow\)** \\ \hline
**UniControl** & **0.546** & **0.466** & **0.623** & **0.654** & **0.741** & **0.693** \\
**ControlNet** & 0.577 & 0.582 & 0.778 & 0.700 & 0.747 & **0.693** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Image Perceptual Distance

Figure 7: User study between our multi-task model (Ours-multi) and single task model (Ours-single) on eight tasks. Our method outperforms baselines on most of tasks, and achieves big performance gains on tasks of seg-to-image and outpainting-to-image. Moreover, the p-value of voting Ours-multi in all cases is computed as **0.0028** that is statistically significant according to the criteria of \(<\)0.05.

### Zero-shot Generalization

We further showcase the surprising capabilities of our method to undertake the zero-shot challenge of hybrid conditions combination and unseen tasks generalization.

**Hybrid Tasks Combination.** This involves generating results from two distinct conditions simultaneously. Our model's zero-shot ability is tested with combinations such as depth and human skeleton or segmentation map and human skeleton. The results are shown in Fig. 5 (a)-(b). When the background is conditioned on a depth map, the model effectively portrays the intricate 3D structure of the scene, while maintaining the skeletal structure of the human subject. Similarly, when the model is presented with a combination of a segmentation map and human skeleton, the output skillfully retains the structural details of the subject, while adhering to the segmentation boundaries. These examples illustrate our model's adaptability and robustness, highlighting its ability to handle complex hybrid tasks without any prior explicit training.

**Unseen Tasks Generalization.** To evaluate the zero-shot ability to generalize to unseen tasks such as gray image colorization, image deblurring, and image inpainting, we conduct the case analysis in Fig. 5 (c)-(e). The model skillfully handles the unseen tasks, producing compelling results. This capability is deeply rooted in the shared attributes and implicit correlations among pre-training and new tasks, allowing our model to adapt seamlessly. For instance, the colorization task leverages the model's understanding of image structures from the segmentation task and depth estimation task, while deblurring and inpainting tasks benefit from the model's familiarity with edge detection and outpainting ones.

## 5 Conclusion and Discussion

We introduce UniControl, a novel unified model for incorporating a wide range of conditions into the generation process of diffusion models. UniControl has been designed to be adaptable to various tasks through the employment of two key components: a Mixture-of-Experts (MOE) style adapter and a task-aware HyperNet. The experimental results have showcased the model's robust performance and adaptability across different tasks and conditions, demonstrating its potential for handling complex text-to-image generation tasks.

**Limitation and Broader Impact.** While UniControl demonstrates impressive performance, it still inherits the limitation of diffusion-based image generation models. Specifically, it is limited by our training data, which is obtained from a subset of the Laion-Aesthetics datasets. We observe that there is a data bias in this dataset. Although we have performed keywords and image based data filtering methods, we are aware that the model may generate biased or low-fidelity output. Our model is also limited when high-quality human output is desired. UniControl could be improved if better open-source datasets are available to block the creation of biased, toxic, sexualized, or other harmful content. We hope our work can motivate researchers to develop visual generative foundation models.

\begin{table}
\begin{tabular}{c c|c c c c c c|c} \hline \hline
**MoE-Adapter** & **TaskHyperNet** & **Canny \(\downarrow\)** & **HED \(\downarrow\)** & **Depth \(\downarrow\)** & **Normal \(\downarrow\)** & **Seg \(\downarrow\)** & **Pose \(\downarrow\)** & **Avg \(\downarrow\)** \\ \hline \(\bigstar\) & \(\bigstar\) & 27.2 & 29.0 & 27.6 & 28.8 & 29.1 & 30.2 & 28.7 \\ ✓ & \(\bigstar\) & 24.5 & 26.1 & 23.7 & 24.8 & 26.9 & 28.3 & 25.7 \\ ✓ & ✓ & **22.9** & **23.6** & **21.3** & **23.4** & **25.5** & **27.4** & **24.0** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation Study (FID)

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline  & **Canny \(\downarrow\)** & **HED \(\downarrow\)** & **Depth \(\downarrow\)** & **Normal \(\downarrow\)** & **Seg \(\downarrow\)** & **Pose \(\downarrow\)** \\ \hline
**GLIGEN**[67] & 24.9 & 27.8 & 25.8 & 27.7 & - & - \\
**T2I-Adapter**[68] & 23.6 & - & 25.4 & - & 27.1 & 28.9 \\
**ControlNet**[21] & **22.7** & 25.1 & 25.5 & 28.4 & 26.7 & 28.8 \\
**UnControl** & 22.9 & **23.6** & **21.3** & **23.4** & **25.5** & **27.4** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative Comparison (FID)

## References

* [1]A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. Won Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [2] OpenAI. Gpt-4 technical report, 2023.
* [3] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* [4] Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, and Jiahui Yu. Cobit: A contrastive bi-directional image-text generation model. _arXiv preprint arXiv:2303.13455_, 2023.
* [5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, pages 10684-10695, 2022.
* [6] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.
* [7] Bo Li, Dongseong Hwang, Zhouyuan Huo, Junwen Bai, Guru Prakash, Tara N Sainath, Khe Chai Sim, Yu Zhang, Wei Han, Trevor Strohman, et al. Efficient domain adaptation for speech foundation models. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE, 2023.
* [8] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating speech, music, sound, and talking head. _arXiv preprint arXiv:2304.12995_, 2023.
* [9] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. _arXiv preprint arXiv:2204.01691_, 2022.
* [10] Yilun Dai, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. _arXiv preprint arXiv:2302.00111_, 2023.
* [11] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics: Design principles and model abilities. Technical Report MSR-TR-2023-8, Microsoft, February 2023. URL https://www.microsoft.com/en-us/research/publication/chatgpt-for-robotics-design-principles-and-model-abilities/.

* [16] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022.
* [17] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2022.
* [18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _NeurIPS_, 34:8780-8794, 2021.
* [19] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. _arXiv preprint arXiv:2205.11487_, 2022.
* [20] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [21] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.
* [22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. _arXiv preprint arXiv:2103.00020_, 2021.
* [23] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3712-3722, 2018.
* [24] Golnaz Ghiasi, Barret Zoph, Ekin D Cubuk, Quoc V Le, and Tsung-Yi Lin. Multi-task self-training for learning general representations. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8856-8865, 2021.
* [25] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1871-1880, 2019.
* [26] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In _International Conference on Learning Representations_, 2017.
* [27] Johannes Von Oswald, Christian Henning, Benjamin F Grewe, and Joao Sacramento. Continual learning with hypernetworks. _arXiv preprint arXiv:1906.00695_, 2019.
* [28] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, pages 2256-2265. PMLR, 2015.
* [29] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in Neural Information Processing Systems_, 32, 2019.
* [30] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _NeurIPS_, 33:6840-6851, 2020.
* [32] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv:2010.02502_, October 2020. URL https://arxiv.org/abs/2010.02502.
* [33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _MICCAI_, pages 234-241. Springer, 2015.

* [34] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _J. Mach. Learn. Res._, 23(47):1-33, 2022.
* [35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.
* [36] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In _European Conference on Computer Vision_, pages 423-439. Springer, 2022.
* [37] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In _CVPR_, 2017.
* [38] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In _ACM SIGGRAPH 2022 Conference Proceedings_, pages 1-10, 2022.
* [39] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen. Pretraining is all you need for image-to-image translation. _arXiv preprint arXiv:2205.12952_, 2022.
* [40] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2021.
* [41] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.
* [42] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. _arXiv preprint arXiv:2211.12446_, 2022.
* [43] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for image-to-image translation. In _The Eleventh International Conference on Learning Representations_, 2022.
* [44] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. _arXiv preprint arXiv:2210.02747_, 2022.
* [45] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. _arXiv preprint arXiv:2209.03003_, 2022.
* [46] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: Understanding and extending diffusion generative models. _arXiv preprint arXiv:2208.14699_, 2022.
* [47] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar. I\({}^{2}\)sb: Image-to-image schrodinger bridge. _arXiv preprint arXiv:2302.05872_, 2023.
* [48] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repairt: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2022.
* [49] Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang Wang, and Mingyuan Zhou. In-context learning unlocked for diffusion models. _arXiv preprint arXiv:2305.01115_, 2023. URL https://arxiv.org/abs/2305.01115.
* [50] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. _Advances in Neural Information Processing Systems_, 35:25005-25017, 2022.
* [51] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.

* [52] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. _arXiv preprint arXiv:1701.06538_, 2017.
* [53] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8110-8119, 2020.
* [54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [55] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _NeurIPS_. NeurIPS, 2019.
* [56] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laino-5b: An open large-scale dataset for training next generation image-text models. _arXiv preprint arXiv:2111.02114_, 2021.
* [57] John Canny. A computational approach to edge detection. _IEEE Transactions on pattern analysis and machine intelligence_, pages 679-698, 1986.
* [58] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In _Proceedings of the IEEE international conference on computer vision_, pages 1395-1403, 2015.
* [59] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. _IEEE transactions on pattern analysis and machine intelligence_, 44(3):1623-1637, 2020.
* [60] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. _arXiv preprint arXiv:2201.09450_, 2022.
* [61] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. _International Journal of Computer Vision_, 127:302-321, 2019.
* [62] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. _arXiv preprint arXiv:2004.10934_, 2020.
* [63] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* [64] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7291-7299, 2017.
* [65] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2018.
* [66] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _NeurIPS_, 30, 2017.
* [67] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. _arXiv:2301.07093_, 2023.

* [68] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.

**Appendix**

## Appendix A Details of Implementation

### MOE-Style Adapter

The MOE adapter is implemented as a set of parallel ConvNets composed of three consecutive convolution and non-linear activation layers. The entire model is comprised of nine individual MOE adapters, each of which consumes 70K parameters. Task keys are designated to each adapter, ensuring that they align with the corresponding visual conditions. Once the MOE adapter processes the input, the remaining model parameters become shared across all tasks. This architecture facilitates task adaptability while promoting parameter efficiency.

### Task-aware HyperNet

The task-aware hypernet is applied to modulate the parameters of zero-conv layers in the ControlNet. Since the ControlNet can be considered as the hypernet of Stable Diffusion (fixed copy). Our idea can be concluded as the **control over control** or **meta-control** to let the task-aware hypernet learn the universe representation that is generalizable across different tasks. To implement it, we firstly map the task keys to instruction with a mapping function as: {"hed": "hed edge to image", "canny": "canny edge to image", "seg": "segmentation map to image", "depth": "depth map to image", "normal": "normal surface map to image", "pose": "human pose skeleton to image", "hedsketch": "sketch to image", "bbox": "bounding box to image", "outpainting": "image outpainting". Then, such instructions will be projected as text embeddings with the help of a language model (we adopt CLIPText in our implementation). The Task-aware HyperNet takes these task instruction embeddings, and projects them into different shapes to match the size of different zero-conv kernels, which will be modulated by these task embeddings accordingly. We would fix the parameters of task-aware hyperNet in the later stage of model training to ensure the stability of dynamics.

### Data Collection

We have collected a large amount of training set (MultiGen-20M) including over 20M condition-image-prompt triplets across nine different tasks. We firstly download 3/4 of Laion-Aesthetics-V2 with score over six and filter out low-resolution (<512) images. As a result, 2.8M images are selected as source images. Then we apply the visual condition extractors as described in the main paper to collect Canny, HED, Sketch, Depth, Normal Surface, Seg Map, Object Bounding Box, Human Skeleton and Outpainting.

## Appendix B Numerical Analysis of Task-Aware Modulated ControlNet

We show that our proposed task-aware modulated ControlNet preserves the properties of the original ControlNet structure. Specifically, we show **1)** The new task-aware modulated ControlNet preserves the zero-initialization property of ControlNet; **2)** The parameters of the task-aware modulated Controlnet can be updated once we start to train the model.

Denote the input feature map by \(\bm{x}\), the frozen SD Block in Fig. 2 by \(\mathcal{F}_{\mathrm{SD}}\), the extra condition by \(c\), two zero convolution operators by \(\mathcal{Z}^{1}_{\theta_{1}}(\cdot)\) and \(\mathcal{Z}^{2}_{\theta_{2}}(\cdot)\), the trainable copy of SD Block by \(\mathcal{G}^{\mathrm{SD}}_{\theta_{s}}(\cdot)\), the task instruction by \(c_{\mathrm{task}}\), and the task-aware hyperNet by \(\mathcal{H}_{\theta_{\mathcal{H}}}(\cdot)\). Then the output of the new task-aware modulated Controlnet can be expressed as

\[\bm{y}_{c}=\mathcal{F}_{\mathrm{SD}}(\bm{x})+\mathcal{Z}^{1}_{\theta_{1}}( \mathcal{G}^{\mathrm{SD}}_{\theta_{s}}(\bm{x}+\mathcal{Z}^{2}_{\theta_{2}}(c) \cdot\mathcal{H}_{\theta_{\mathcal{H}}}(c_{\mathrm{task}})))\cdot\mathcal{H}_{ \theta_{\mathcal{H}}}(c_{\mathrm{task}})\,.\] (1)

**Property of Zero Initialization.** Similar to ControlNet [21], the weights and biases of the convolution layers are initialized as zeros. As a result, we have \(\mathcal{Z}^{1}_{\theta_{1}}(\cdot)\equiv 0\) and \(\bm{y}_{c}=\mathcal{F}_{\mathrm{SD}}(\bm{x})\), regardless of the initialization of \(\mathcal{H}_{\theta_{\mathcal{H}}}(\cdot)\).

Gradient Analysis.We analyze the gradient of the modulated part

\[\nabla_{\theta}\left(Z^{1}_{\theta_{1}}(I)\cdot\mathcal{H}_{\theta_{\mathcal{H}} }(c_{\mathrm{task}})\right)=\mathcal{H}_{\theta_{\mathcal{H}}}(c_{\mathrm{task }})\cdot\nabla_{\theta}Z^{1}_{\theta_{1}}(I)+Z^{1}_{\theta_{1}}(I)\cdot\nabla_{ \theta_{\mathcal{H}}}\mathcal{H}_{\theta_{\mathcal{H}}}(c_{\mathrm{task}})\,,\] (2)

where \(I\) is the input of the zero convolution layer.

When we start to train the network, the first part of the RHS of (2) follows similar analysis of ControlNet [21] since \(\mathcal{H}_{\theta_{\mathcal{H}}}(c_{\mathrm{task}})\) is constant when we analyze the gradient \(\nabla_{\theta}Z^{1}_{\theta_{1}}(I)\). Since the parameters of \(\mathcal{H}_{\theta_{\mathcal{H}}}(c_{\mathrm{task}})\) are not initialized to zero, it is known that \(\mathcal{H}_{\theta_{\mathcal{H}}}(c_{\mathrm{task}})\neq 0\). So the gradient dynamic follows the analysis of ControlNet. Therefore, we conclude that \(Z^{1}_{\theta_{1}}(I)\neq 0\) after the first gradient update, and that the network can start to learn and update the following standard dynamics of stochastic gradient descent.

As for the second part of the RHS of (2), \(Z^{1}_{\theta_{1}}(I)\equiv 0\) before the first gradient update, so the gradient is zero for \(\theta_{\mathcal{H}}\). However, after the first gradient update of \(\theta_{1}\), we know \(Z^{1}_{\theta_{1}}(I)\neq 0\), and \(\theta_{\mathcal{H}}\) can be updated with non-zero gradients.

To conclude, the new task-aware Modulated ControlNet can still be efficiently updated and learned even if the convolution layers are initialized to zero.

## Appendix C Zero-shot-task Results and Analysis

We show more zero-shot-task results in this section, where the tasks have not been trained on. In Fig. 13, we show zero-shot deblurring results guided by the keywords. Our deblurred images can successfully recover the fine-grained details of the images without training on such data. We note that some details are still missing, _e.g.,_ the details in the painting in the first row are still not clear enough. In Fig. 14, we illustrate two zero-shot image colorization results. We believe that most parts of the generated images are acceptable, though the clothes of the second woman do not look the same to the input blurred image. In Fig. 15, we observe impressive zero-shot inpainting results. In the first row, the duck that is inputted in the text has been successfully generated in the inpainted image. The second row obtains acceptable results as well, though the faces do not look perfect. The overall zero-shot quality of UniControl is remarkable.

While inpainting and outpainting might appear related, they are fundamentally distinct. Inpainting heavily leverages the contextual information from unmasked regions, necessitating a precise match. Conversely, outpainting has more freedom, with the generative model prioritizing prompts to envision new content. As shown in Fig. 8, directly using outpainting model for inpainting tasks can be challenging since the model tends to leave a sharp change over the mask boundaries. Our pretrained UniControl, thanks to intensive training across multiple tasks, has learned edge and region-to-image mappings, which assists in preserving contextual information.

Our model also demonstrates a promising capacity to generalize under scribble conditions, showing parallels to the ControlNet's ability, even though UniControl hasn't been directly trained using scribble data. Fig. 9 provides results illustrating the scribble-to-image generation.

## Appendix D Details of User Study

In the evaluation steps, we use Amazon Mechanical Turk (Mturk) 2 to perform user study. Specifically, we ask three Mturk master workers to select the best output result for each input condition. As shown in Fig. 10, we provide instructions on guidelines to select the best generated image. The annotators are provided the condition map and the text that describes the image, and are required to select the better output between the two generated images. Considering that images can both in good or bad qualities, we provide the tie option as well. We use the majority vote to determine the result of each image, which means that an image is considered as a better image if two or more annotators vote for it. We use 294 images for the tasks of Canny, HED, Surface Normal, Depth, Segmentation, User Sketch, and Outpainting. We adopt 100 images for the task of Human Skeleton and 187 images for the task of Bounding Box. In summary, we totally obtain 7,035 voting results for all nine tasks. 2/3 of source images in testing set are collected from MSCOCO with the remaining 1/3 from Laion. And it includes a very diverse range of topics including indoor scene, outdoor scene, oil painting, portrait, pencil sketch, animation, cartoon, etc.

Footnote 2: https://www.mturk.comFigure 8: Visual comparison of Ours-single-outpainting and UniControl on the inpainting task. The single outpainting model cannot well address the zero-shot inpainting task whereas UniControl demonstrates promising capacity.

Figure 9: Visual comparison of ControlNet-Scribble and UniControl on the scribble data. ControlNet-Scribble is trained by the scribble data which, however, are unseen for UniControl.

[MISSING_PAGE_FAIL:19]

Figure 12: Failure Cases: distorted body (row one); blurred faces (row two); incorrect creation (row three).

Figure 13: More zero-shot-task deblurring results.

Figure 14: More zero-shot-task gray-to-RGB colorization results.

Figure 15: More zero-shot-task image in-painting results. The in-painting MOE adapter weights are directly inherited from outpainting.

Figure 16: Canny to Image Generation

Figure 17: HED to Image Generation

Figure 18: Depth to Image Generation

Figure 19: Surface Normal to Image Generation

Figure 20: Human Pose Skeleton to Image Generation

Figure 21: Bounding Box (by YOLO-V4-MSCOCO) to Image Generation

Figure 22: Segmentation Map (by Uniformer-ADE20K) to Image Generation

Figure 23: Image Outpainting

Figure 24: User Sketch to Image Generation