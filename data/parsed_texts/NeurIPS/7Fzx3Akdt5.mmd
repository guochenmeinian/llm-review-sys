# Harnessing Multiple Correlated Networks

for Exact Community Recovery

 Miklos Z. Racz

Northwestern University

Evanston, IL 60208

miklos.racz@northwestern.edu

&Jifan Zhang

Northwestern University

Evanston, IL 60208

jifanzhang2026@u.northwestern.edu

###### Abstract

We study the problem of learning latent community structure from multiple correlated networks, focusing on edge-correlated stochastic block models with two balanced communities. Recent work of Gaudio, Racz, and Sridhar (COLT 2022) determined the precise information-theoretic threshold for exact community recovery using two correlated graphs; in particular, this showcased the subtle interplay between community recovery and graph matching. Here we study the natural setting of more than two graphs. The main challenge lies in understanding how to aggregate information across several graphs when none of the pairwise latent vertex correspondences can be exactly recovered. Our main result derives the precise information-theoretic threshold for exact community recovery using any constant number of correlated graphs, answering a question of Gaudio, Racz, and Sridhar (COLT 2022). In particular, for every \(K\geq 3\) we uncover and characterize a region of the parameter space where exact community recovery is possible using \(K\) correlated graphs, even though (1) this is information-theoretically impossible using any \(K-1\) of them and (2) none of the latent matchings can be exactly recovered.

## 1 Introduction

Finding communities in networks--that is, groups of nodes that are similar--is one of the fundamental problems in machine learning. This task is crucially important for understanding the underlying structure and function of networks across diverse applications, including sociology and biology [23]. The increasing availability of network data sets offers the intriguing possibility of improving community recovery algorithms by synthesizing information across correlated networks. However, in many settings the graphs are not aligned--which may happen for a variety of reasons, including anonymization, missing or erroneous data, or simply the alignment being unknown--which presents a challenge. Thus graph matching--the task of recovering the latent vertex alignment between graphs--plays a central role in efforts to integrate data across networks. Our work follows an exciting recent line of work at the intersection of community recovery and graph matching.

Recently, Racz and Sridhar [41] initiated the study of community recovery in correlated stochastic block models (SBMs), focusing on the simplest setting of two correlated graphs with two balanced communities. They determined the information-theoretic limits for exact graph matching, which has applications for community recovery. In particular, they uncovered a region of the parameter space where exact community recovery is possible using two correlated graphs even though it is information-theoretically impossible to do so using just a single graph. Subsequently, Gaudio, Racz, and Sridhar [22] determined the information-theoretic limits for exact community recovery from two correlated SBMs. This required going beyond exact graph matching and understanding the subtle interplay between community recovery and graph matching.

Gaudio, Racz, and Sridhar [22] posed the question of understanding what happens in the case of more than two graphs, which arises naturally in all the motivating examples. For instance, people participate in numerous overlapping yet complementary social networks, and only by combining these can we fully understand and make inferences about society. Similarly, synthesizing information across protein-protein interaction networks from several related species can aid in inferring protein functions [44]. The main challenge lies in understanding how to optimally pass information across three or more graphs.

Our main contribution fully answers this open question by Gaudio, Racz, and Sridhar [22]. Specifically, we precisely characterize the information-theoretic threshold for exact community recovery given \(K\) correlated SBMs, for any constant \(K\). This result highlights an intricate phase diagram and quantifies the value of each additional correlated graph for the task of community recovery. In particular, for every \(K\geq 3\) we uncover and characterize a region of the parameter space where exact community recovery is possible using \(K\) correlated graphs, even though (1) this is impossible using any \(K-1\) of them and (2) none of the latent matchings can be exactly recovered. See Section 3 and Theorems 1 and 2 for details.

Along the way, we also precisely characterize the information-theoretic threshold for exact graph matching given \(K\) correlated SBMs, for any constant \(K\). In particular, we uncover and characterize a region of the parameter space where the latent matching between two correlated SBMs cannot be exactly recovered given just the two graphs, but it can be exactly recovered given \(K>3\) correlated SBMs. See Section 3 and Theorems 3 and 4 for details.

To prove our results, we study the so-called \(k\)-core matching between all pairs of graphs. Recent works have shown the \(k\)-core matching to be a flexible and successful tool in a variety of settings for two correlated graphs [13; 22; 43]. Our main technical contribution is to extend this analysis to more than two graphs. The main difficulty lies in understanding the size of _intersections_ of "bad sets" for \(k\)-core matchings for different pairs of graphs. We refer to Section 4 for details.

## 2 Models and questions

**The stochastic block model (SBM).** The SBM is the most common probabilistic generative model for networks with latent community structure. First introduced by Holland, Laskey, and Leinhardt [25], it has garnered considerable attention and research. In particular, it can be employed as a natural testbed for evaluating and assessing clustering algorithms on average-case networks [1]. The SBM notably displays sharp information-theoretic phase transitions for various inference tasks, offering a detailed understanding of when community information can be extracted from network data. The phase transition thresholds were conjectured by Decelle et al. [15] and were proved rigorously in several papers [33; 34; 35; 36; 2; 3]. We refer to the survey [1] for a detailed overview of the SBM.

In this paper, we focus on the simplest setting, a SBM with two symmetric communities. Let \(n\) be a positive integer and let \(p,q\in[0,1]\) be parameters representing probabilities. We construct a graph \(G\sim\mathrm{SBM}(n,p,q)\) as follows. The graph \(G\) has \(n\) vertices, labeled by \([n]:=\{1,2,3,\ldots,n\}\). Each vertex \(i\) is assigned a community label \(\sigma^{*}(i)\) from the set \(\{+1,-1\}\); these are drawn i.i.d. uniformly at random across \(i\in[n]\). Let \(\bm{\sigma}^{*}:=\{\sigma^{*}(i)\}_{i=1}^{n}\) denote the community label vector. The vertices are thus categorized into two communities: \(V^{+}:=\{i\in[n]:\sigma^{*}(i)=+1\}\) and \(V^{-}:=\{i\in[n]:\sigma^{*}(i)=-1\}\). Given the community labels \(\bm{\sigma}^{\bullet}\), the edges of \(G\) are drawn independently between pairs of distinct vertices. If \(\sigma^{*}(i)=\sigma^{*}(j)\), then the edge \((i,j)\) is in \(G\) with probability \(p\); otherwise, it is in \(G\) with probability \(q\).

**Community recovery.** In the community recovery task, an algorithm takes as input the graph \(G\) (without knowing \(\bm{\sigma}^{\star}\)) and outputs an estimated community labeling \(\widehat{\bm{\sigma}}\). Define the _overlap_ between the estimated labeling and the ground truth as follows:

\[\mathsf{ov}(\bm{\sigma}^{\star},\widehat{\bm{\sigma}}):=\frac{1}{n}\left| \sum_{i=1}^{n}\sigma^{*}(i)\widehat{\sigma}(i)\right|.\]

The overlap measures how well the true community labels and the estimated labels of the algorithm match. Note that \(\mathsf{ov}(\bm{\sigma}^{\star},\widehat{\bm{\sigma}})\in[0,1]\), where the larger the value is, the better performance the algorithm has. In particular, the algorithm succeeds in exactly recovering the partition into two communities (i.e., \(\bm{\sigma}^{\star}=\widehat{\bm{\sigma}}\) or \(\bm{\sigma}^{\star}=-\widehat{\bm{\sigma}}\)) if and only if \(\mathsf{ov}(\bm{\sigma}^{\star},\widehat{\bm{\sigma}})=1\). Our focus in this paper is achieving this goal, known as exact community recovery.

It is well-known that exact community recovery is most challenging and interesting in the logarithmic average degree regime [1]. Accordingly, we focus on this regime: in most of the paper we assume that \(p=a\frac{\log n}{n}\) and \(q=b\frac{\log n}{n}\) for some constants \(a,b>0\). In this regime there is a sharp information-theoretic threshold for exact community recovery [2, 35, 3]. Let \(\mathrm{D}_{+}(a,b):=(\sqrt{a}-\sqrt{b})^{2}/2\) denote the so-called _Chernoff-Hellinger divergence_. Then the information-theoretic threshold is given by

\[\mathrm{D}_{+}(a,b)=1.\] (2.1)

In other words, if \(\mathrm{D}_{+}(a,b)>1\), then exact recovery is possible (and, in fact, efficiently). That is, there is a (polynomial-time) algorithm which outputs an estimator \(\widehat{\bm{\sigma}}\) with the guarantee that \(\lim_{n\to\infty}\mathbb{P}(\mathsf{ov}(\bm{\sigma}^{\star},\widehat{\bm{ \sigma}})=1)=1\). On the other hand, if \(\mathrm{D}_{+}(a,b)<1\) then exact recovery is impossible: for any estimator \(\widetilde{\bm{\sigma}}\), we have that \(\lim_{n\to\infty}\mathbb{P}(\mathsf{ov}(\bm{\sigma}^{\star},\widetilde{\bm{ \sigma}})=1)=0\).

**Correlated SBMs.** The objective of our work is to understand how the sharp threshold for exact community recovery varies when the input data involves multiple correlated graphs. To do so, we first define a natural model of multiple correlated SBMs [29, 39, 28] (and see further discussion in Section 6 about alternative models).

We construct \((G_{1},\dots,G_{K})\sim\mathrm{CSBM}(n,p,q,s)\) as follows, where the additional parameter \(s\in[0,1]\) reflects the degree of correlation between the graphs (and the number of graphs \(K\) is dropped from the notation for ease of readability). First, generate a parent graph \(G_{0}\sim\mathrm{SBM}(n,p,q)\) with community labels \(\bm{\sigma}^{\star}\). Subsequently, given \(G_{0}\), construct \(G^{\prime}_{1},G^{\prime}_{2},\dots,G^{\prime}_{K}\) by independent subsampling. Specifically, each edge of \(G_{0}\) is included in \(G^{\prime}_{i}\) with probability \(s\), independently of everything else, and non-edges of \(G_{0}\) remain non-edges in \(G^{\prime}_{i}\). The graphs \(G^{\prime}_{i}\) inherit both the vertex labels and the community labels from the parent graph \(G_{0}\). Finally, let \(\pi^{\star}_{12},\dots,\pi^{\star}_{1K}\) be i.i.d. uniformly random permutations of \([n]\) and let \(\bm{\pi}^{\star}:=(\pi^{\star}_{12},\dots,\pi^{\star}_{1K})\). Define \(G_{1}:=G^{\prime}_{1}\) and, for all \(i\in\{2,\dots,K\}\), define \(G_{i}:=\pi^{\star}_{1i}(G^{\prime}_{i})\). In other words, for every \(i>1\) and \(j\in[n]\), vertex \(j\) in \(G^{\prime}_{i}\) is relabeled to \(\pi^{\star}_{1i}(j)\) in \(G_{i}\). This last relabeling step mirrors the real-world observation that vertex labels are often unaligned across graphs. This construction is shown in Figure 1.

An important property of the model is that marginally each graph \(G_{i}\) is an SBM. Since the subsampling probability is \(s\), we have that \(G_{i}\sim\mathrm{SBM}(n,ps,qs)\). Thus, it follows from (2.1) that, in the logarithmic average degree regime where \(p=a\frac{\log n}{n}\) and \(q=b\frac{\log n}{n}\), the communities can be exactly recovered from \(G_{1}\) alone precisely when \(s\mathrm{D}_{+}(a,b)=D_{+}(sa,sb)>1\).

The key question in our work is how to improve the threshold by incorporating more information as \(K\), the number of correlated SBMs, increases. This question was initiated by Racz and Sridhar [41] and then solved by Gaudio, Racz, and Sridhar [22] when \(K=2\).

An essential observation is that, to go beyond the threshold, one needs to combine information from the \(K\) graphs \(G_{1},\dots,G_{K}\) through graph matching. Then one can exactly recover the community labels using the combined information, even in regimes where it is information-theoretically impossible to exactly recover \(\bm{\sigma}^{\star}\) given up to \(K-1\) graphs.

To be more specific, if \(\bm{\pi}^{\star}\) were known, then one can reconstruct \(G^{\prime}_{j}\) from \(G_{j}\) and then combine the graphs \(G^{\prime}_{1},\dots,G^{\prime}_{K}\) to obtain the union graph \(H^{*}\), defined as follows: the edge \((i,j)\) is included in

Figure 1: Schematic showing the construction of multiple correlated SBMs (see text for details).

\(H^{*}\) if and only if \((i,j)\) is included in at least one of \(G_{1}^{\prime},\ldots,G_{K}^{\prime}\). Note that \(H^{*}\) is also an \(\mathrm{SBM}\); specifically, \(H^{*}\sim\mathrm{SBM}\left(n,\left(1-(1-s)^{K}\right)p,\left(1-(1-s)^{K}\right)q\right)\), so (2.1) directly implies that the communities can be exactly recovered from the union graph \(H^{*}\) if \(\left(1-(1-s)^{K}\right)\mathrm{D}_{+}(a,b)>1\).

**Graph matching.** In real applications, the permutations \(\boldsymbol{\pi^{*}}=(\pi_{12}^{*},\ldots,\pi_{1K}^{*})\) are often not known. The arguments above highlight the importance of an intermediate task, known as graph matching: how can one recover the latent permutations \(\boldsymbol{\pi^{*}}\) given the graphs \((G_{1},G_{2},\ldots,G_{K})\)? While here we regard graph matching as an important intermediate step, it is of great significance in its own right, with applications in social network privacy [40], machine learning [10], and more. For two correlated SBMs, this problem was resolved by Racz and Sridhar [41], who proved that the information-theoretic threshold for (pairwise) exact graph matching is \(s^{2}(a+b)/2=1\). Note that this is also the connectivity threshold for the intersection graph of \(G_{1}\) and \(G_{2}^{\prime}\) (see [41]). Denote

\[\mathrm{T}_{\mathrm{c}}(a,b):=\frac{a+b}{2}.\] (2.2)

With this notation, the (pairwise) exact graph matching threshold is given by \(s^{2}\mathrm{T}_{\mathrm{c}}(a,b)=1\). This directly implies (by a union bound) that if \(s^{2}\mathrm{T}_{\mathrm{c}}(a,b)>1\), then \(\boldsymbol{\pi^{*}}\) can be exactly recovered given \((G_{1},G_{2},\ldots,G_{K})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n }{n},s)\), for any constant \(K\). By the discussion above, this also gives a sufficient condition for exact community recovery given \((G_{1},G_{2},\ldots,G_{K})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n }{n},s)\):

\[s^{2}\mathrm{T}_{\mathrm{c}}(a,b)>1\qquad\text{ and }\qquad\left(1-(1-s)^{K} \right)\mathrm{D}_{+}(a,b)>1.\] (2.3)

We will generalize the exact graph matching result of Racz and Sridhar [41] and show (see Theorem 3 below) that \(\boldsymbol{\pi^{*}}\) can be exactly recovered given \((G_{1},G_{2},\ldots,G_{K})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n }{n},s)\) if

\[s\left(1-(1-s)^{K-1}\right)\mathrm{T}_{\mathrm{c}}(a,b)>1,\] (2.4)

which (for \(K>2\)) is weaker than the condition \(s^{2}\mathrm{T}_{\mathrm{c}}(a,b)>1\) implied by [41]. (Moreover, in Theorem 4 we show that the condition in (2.4) is tight for exact recovery of \(\boldsymbol{\pi^{*}}\).) Thus, by the discussion above, this gives a sufficient condition for exact community recovery given \((G_{1},G_{2},\ldots,G_{K})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n }{n},s)\):

\[s\left(1-(1-s)^{K-1}\right)\mathrm{T}_{\mathrm{c}}(a,b)>1\qquad\text{and }\qquad\left(1-(1-s)^{K}\right)\mathrm{D}_{+}(a,b)>1.\] (2.5)

**The interplay between community recovery and graph matching.** The condition (2.5) is, however, not tight. To attain the sharp threshold for exact community recovery given \(K\) graphs, we need to answer the following question: does there exist a parameter regime where exact community recovery is possible for \(K\) graphs, even though (1) exact graph matching is impossible, and (2) exact community recovery is impossible using only \(K-1\) graphs?

For \(K=2\) graphs, Gaudio, Racz, and Sridhar [22] proved that the sharp threshold for exact community recovery given \(K\) correlated SBMs is given by

\[s^{2}\mathrm{T}_{\mathrm{c}}(a,b)+s(1-s)\mathrm{D}_{+}(a,b)>1\qquad\text{ and }\qquad\left(1-(1-s)^{2}\right)\mathrm{D}_{+}(a,b)>1.\] (2.6)

The condition \(\left(1-(1-s)^{2}\right)\mathrm{D}_{+}(a,b)>1\) is necessary due to the work [41]. The first condition in (2.6) demonstrates the interplay between community recovery and graph matching. To be more specific, the first term \(s^{2}\mathrm{T}_{\mathrm{c}}(a,b)\) is the threshold for exact graph matching given \((G_{1},G_{2})\), while the second term \(s(1-s)\mathrm{D}_{+}(a,b)\) comes from community recovery.

Our main contribution generalizes this result, determining the exact community recovery threshold for \(K\geq 3\) graphs. If \(\left(1-(1-s)^{K}\right)\mathrm{D}_{+}(a,b)>1\), then the sharp threshold is given by

\[s\left(1-(1-s)^{K-1}\right)\mathrm{T}_{\mathrm{c}}(a,b)+s(1-s)^{K-1}\mathrm{ D}_{+}(a,b)>1.\] (2.7)

The condition (2.7) also clearly exhibits the interplay between community recovery and graph matching. The first term comes from graph matching, while the second term comes from community recovery, as in the case of \(K=2\). We refer to Section 3 and Theorems 1 and 2 for details.

Despite the apparent similarity in results, when \(K\geq 3\) the situation differs significantly from that of two graphs. The primary challenge lies in the existence of multiple methods for matching \(K\geq 3\) graphs. When \(K=2\), there is only a single matching that needs to be recovered from \(G_{1}\) and \(G_{2}\). In contrast, with three or more graphs, the graphs can be matched pairwise, or to some anchor graph, or potentially in many other ways. Integrating information across different matchings requires substantial additional effort. We present the formal results in the next section.

Results

Our main contributions are to determine the precise information-theoretic thresholds for exact community recovery and for exact graph matching given \(K\) correlated SBMs.

### Threshold for exact community recovery

We first describe the precise information-theoretic threshold for exact community recovery, starting with the positive direction.

**Theorem 1** (Exact community recovery from \(K\) correlated SBMs).: _Fix constants \(a,b>0\) and \(s\in[0,1]\), and let \((G_{1},G_{2},\ldots,G_{K})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n}{ n},s)\). Suppose that the following two conditions both hold:_

\[\left(1-(1-s)^{K}\right)\mathrm{D}_{+}(a,b)>1\] (3.1)

_and_

\[s\left(1-(1-s)^{K-1}\right)\mathrm{T}_{\mathrm{c}}(a,b)+s(1-s)^{K-1}\mathrm{D }_{+}(a,b)>1.\] (3.2)

_Then exact community recovery is possible. That is, there is an estimator \(\widehat{\bm{\sigma}}=\widehat{\bm{\sigma}}(G_{1},G_{2},\ldots,G_{K})\) such that \(\lim\limits_{n\to\infty}\mathbb{P}\left(\mathsf{ov}\left(\widehat{\bm{\sigma}},\bm{\sigma}^{\star}\right)=1\right)=1\)._

Combined with Theorem 2 below (which shows that Theorem 1 is tight), this result precisely answers an open problem of Gaudio, Racz, and Sridhar [22]. The condition (3.1) is required for exact community recovery for \(K\) graphs by [41]. We now focus on the condition (3.2). In the prior work [22], it is proved that the threshold for exact community recovery for two graphs is given by (2.6). The primary contribution of Theorem 1 is to go beyond this threshold as the number of graphs \(K\) increases. In particular, this showcases that there exists a regime where (1) it is impossible to exactly recover \(\bm{\sigma}^{\star}\) from \((G_{1},G_{2},\ldots,G_{K-1})\) alone and (2) any exact graph matching is impossible, yet one can perform exact recovery of \(\bm{\sigma}^{\star}\) given \((G_{1},G_{2},\ldots,G_{K})\). This requires developing novel algorithms that integrate information from \((G_{1},G_{2},\ldots,G_{K})\) delicately and incorporate multiple graph matchings carefully.

Here we first provide a detailed discussion of the algorithms for three graphs \((G_{1},G_{2},G_{3})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n}{n},s)\), which is the simplest case with intriguing new phenomena and challenges as mentioned. This avoids complicated notations (which arise for general \(K\)) for easier understanding. The new techniques used for combining multiple matchings and integrating information with three graphs are subsequently generalized to \(K>3\) correlated SBMs. The high level idea of the algorithm for exact community recovery when \(K=3\) consists of five steps (in the following discussion we assume \(a>b\); when \(a<b\), change majority to minority everywhere):

1. Obtain an almost exact community labeling of \(G_{1}\).
2. Obtain three pairwise partial almost exact graph matchings \(\widehat{\mu}_{12}\), \(\widehat{\mu}_{13}\), and \(\widehat{\mu}_{23}\) between graph pairs \((G_{1},G_{2})\), \((G_{1},G_{3})\), and \((G_{2},G_{3})\), respectively.
3. For vertices in \(G_{1}\) that are part of at least two matchings, refine the almost exact community labeling in Step 1 via majority vote in the (union) graph consisting of edges that appear at least once in \((G_{1},G_{2},G_{3})\).
4. For vertices in \(G_{1}\) that are part of only \(\widehat{\mu}_{12}\) (resp. \(\widehat{\mu}_{13}\)), label them via majority vote of their neighbors' labels in the graph consisting of edges that appear only in \(G_{1}\) and not in \(G_{2}\) (resp. only in \(G_{1}\) and not in \(G_{3}\)).
5. For vertices in \(G_{1}\) that are not part of any of the three matchings or only part of \(\widehat{\mu}_{23}\), label them via majority vote of their neighbors' labels in \(G_{1}\).

Each step in the algorithm involves abundant technical details. See Section 4 for a detailed overview of the algorithms and proofs. Note that the threshold (3.2) captures the interplay between community recovery and graph matching, which we now discuss in more detail.

The first term in (3.2), which is \(s\left(1-(1-s)^{2}\right)\mathrm{T}_{\mathrm{c}}(a,b)\) for \(K=3\), comes from graph matching. In [22] it is shown that for one matching, say \(\widehat{\mu}_{12}\), the best possible almost exact graph matching makes \(n^{1-s^{2}\mathrm{T}_{\mathrm{c}}(a,b)+o(1)}\) errors. Here, we show that it is possible to obtain almost exact matchings \(\widehat{\mu}_{12}\) and \(\widehat{\mu}_{13}\) (namely, these will be \(k\)-core matchings; see Section 4 for details) such that the size of the _intersection_ of the two error sets is \(n^{1-s(1-(1-s)^{2})\mathrm{T}_{\mathrm{c}}(a,b)+o(1)}\), which is a smaller power of \(n\). This quantifies how synthesizing information across graph matchings can reduce errors and this exponent is precisely what shows up in the first term in (3.2). This observation is important and relevant for Steps 4 and 5 in the algorithm.

On the other hand, the second term in (3.2), which is \(s(1-s)^{2}\mathrm{D}_{+}(a,b)\) for \(K=3\), comes from community recovery. In fact, this term arises from the majority votes in Step 5, where we use only edges in \(G_{1}\). Note that the nodes that are unmatched by \(\widehat{\mu}_{12}\) and \(\widehat{\mu}_{13}\) are, roughly speaking, the isolated nodes in the intersection graphs of \(G_{1}\) and \(G_{2}\), and \(G_{1}\) and \(G_{3}\), respectively. Thus, while we use all edges in \(G_{1}\) in this step, the relevant edges are not present in \(G_{2}\) nor in \(G_{3}\), giving the "effective" factor of \(s(1-s)^{2}\). By (2.1), the exact community recovery threshold for \(\mathrm{SBM}(n,s(1-s)^{2}a\frac{\log n}{n},s(1-s)^{2}b\frac{\log n}{n})\) is \(s(1-s)^{2}\mathrm{D}_{+}(a,b)=1\), giving the second term in (3.2). The following impossibility result shows the tightness of Theorem 1.

**Theorem 2** (Impossibility of exact community recovery).: _Fix constants \(a,b>0\) and \(s\in[0,1]\), and let \((G_{1},G_{2},\ldots,G_{K})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n} {n},s)\). Suppose that either_

\[\left(1-(1-s)^{K}\right)\mathrm{D}_{+}(a,b)<1\] (3.3)

_or_

\[s\left(1-(1-s)^{K-1}\right)\mathrm{T}_{\mathrm{c}}(a,b)+s(1-s)^{K-1}\mathrm{D} _{+}(a,b)<1.\] (3.4)

_Then exact community recovery is impossible. That is, for any estimator \(\widetilde{\bm{\sigma}}=\widetilde{\bm{\sigma}}(G_{1},G_{2},\ldots,G_{K})\), we have that \(\lim\limits_{n\to\infty}\mathbb{P}\left(\mathsf{ov}\left(\widetilde{\bm{\sigma }},\bm{\sigma^{\star}}\right)=1\right)=0\)._

Impossibility of exact community recovery given \(K\) graphs under the condition (3.3) is proved in [41]. Hence, Theorem 2 focuses on proving impossibility for exact community recovery given \(K\) graphs under the condition (3.4). In particular, condition (3.4) reveals a parameter regime where exact community recovery from \((G_{1},G_{2},\ldots,G_{K})\) is impossible, yet, if \(\bm{\pi}^{*}\) were known, then exact community recovery would be possible based on the (correctly matched) union graph.

Theorems 1 and 2 combined give the tight threshold for exact community recovery for general \(K\) correlated SBMs, see (2.7). Fig. 2 exhibits phase diagrams illustrating the results for three graphs.

### Threshold for exact graph matching

The techniques that we develop in order to prove Theorems 1 and 2 also allow us to solve the question of exact graph matching, that is, exactly recovering \(\bm{\pi}^{*}=(\pi_{12}^{*},\ldots,\pi_{1K}^{*})\) from \((G_{1},G_{2},\ldots,G_{K})\). In the context of correlated SBMs and community recovery, exact graph matching can be thought of as an intermediate step towards exact community recovery. However, more generally, graph matching is a fundamental inference problem in its own right; see Section 5 for discussion of related work. We start with the positive direction in the following theorem.

**Theorem 3** (Exact graph matching from \(K\) correlated SBMs).: _Fix constants \(a,b>0\) and \(s\in[0,1]\), and let \((G_{1},G_{2},\ldots,G_{K})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n }{n},s)\). Suppose that_

\[s\left(1-(1-s)^{K-1}\right)\mathrm{T}_{\mathrm{c}}(a,b)>1.\] (3.5)

_Then exact graph matching is possible. That is, there exists an estimator \(\widehat{\bm{\pi}}=\widehat{\bm{\pi}}(G_{1},G_{2},\ldots,G_{K})=(\widehat{\pi} _{12},\ldots,\widehat{\pi}_{1K})\) such that \(\lim_{n\to\infty}\mathbb{P}\left(\widehat{\bm{\pi}}(G_{1},G_{2},\ldots,G_{K}) =\bm{\pi}^{*}\right)=1\)._

Since the condition in (3.5) is weaker than \(s^{2}\mathrm{T}_{\mathrm{c}}(a,b)>1\) (which is the threshold for exact graph matching for \(K=2\), as shown in [41]), Theorem 3 implies that there exists a parameter regime where \(\widehat{\pi}_{12}\) cannot be exactly recovered from \((G_{1},G_{2})\), but \(\widehat{\bm{\pi}}\) can be exactly recovered from \((G_{1},G_{2},\ldots,G_{K})\). In other words, it is necessary to combine information across all graphs in order to recover \(\widehat{\bm{\pi}}\) (and even just to recover \(\widehat{\pi}_{12}\)).

The estimator \(\widehat{\bm{\pi}}\) in Theorem 3 is based on pairwise \(k\)-core matchings (see Sec. 4 for further details). Roughly speaking, for each pairwise \(k\)-core matching the number of unmatched vertices is \(n^{1-s^{2}\mathrm{T}_{\mathrm{c}}(a,b)+o(1)}\); however, we shall show that the number of vertices which cannot be matched through some combination of pairwise \(k\)-core matchings is \(n^{1-s\left(1-(1-s)^{K-1}\right)\mathrm{T}_{\mathrm{c}}(a,b)+o(1)}\), which is of smaller order. So, when \(s\left(1-(1-s)^{K-1}\right)\mathrm{T}_{\mathrm{c}}(a,b)>1\), then exact graph matching is possible. The following impossibility result shows the tightness of Theorem 3.

**Theorem 4** (Impossibility of exact graph matching from \(K\) correlated SBMs).: _Fix constants \(a,b>0\) and \(s\in[0,1]\), and let \((G_{1},G_{2},\ldots,G_{K})\sim\mathrm{CSBM}(n,\frac{a\log n}{n},\frac{b\log n}{n},s)\). Suppose that_

\[s\left(1-(1-s)^{K-1}\right)\mathrm{T_{c}}(a,b)<1.\] (3.6)

_Then exact graph matching is impossible. That is, for any estimator \(\widetilde{\boldsymbol{\pi}}=\widetilde{\boldsymbol{\pi}}(G_{1},G_{2},\ldots,G _{K})=(\widetilde{\pi}_{12},\ldots\widetilde{\pi}_{1K})\) we have that \(\lim_{n\to\infty}\mathbb{P}\left(\widetilde{\boldsymbol{\pi}}(G_{1},G_{2}, \ldots,G_{K})=\boldsymbol{\pi}^{*}\right)=0\)._

Theorems 3 and 4 combined give the tight threshold for exact graph matching for general \(K\) correlated SBMs, see (2.4). We note that, in independent and concurrent work [5], Ameen and Hajek derived the threshold for exact graph matching from \(K\) correlated Erdos-Renyi random graphs; in other words, they proved Theorems 3 and 4 in the special case of \(a=b\).

Comparing Theorems 3 and 4 with Theorems 1 and 2, note that there exists a parameter regime where exact community recovery is possible even though exact graph matching is impossible.

## 4 Overview of algorithms and proofs

In this section we elaborate on the technical details of the community recovery algorithm, for which high-level ideas were presented in Section 3. We focus our discussion on the setting of \(K=3\) graphs, which already captures the main technical challenges; we highlight these and explain how

Figure 2: Phase diagram for exact community recovery for three graphs with fixed \(s\), and \(a\in[0,40]\), \(b\in[0,40]\) on the axes. _Green region_: exact community recovery is possible from \(G_{1}\) alone; _Cyan region_: exact community recovery is impossible from \(G_{1}\) alone, but exact graph matching of \(G_{1}\) and \(G_{2}\) is possible, and subsequently exact community recovery is possible from \((G_{1},G_{2})\); _Dark Blue region_: exact community recovery is impossible from \(G_{1}\) alone, exact graph matching is also impossible from \((G_{1},G_{2})\), yet exact community recovery is possible from \((G_{1},G_{2})\); _Pink region_: exact community recovery is impossible from \((G_{1},G_{2})\) (even though it would be possible if \(\pi_{12}^{*}\) were known), yet exact community recovery is possible from \((G_{1},G_{2},G_{3})\); _Violet region_: exact community recovery is impossible from \((G_{1},G_{2},G_{3})\) (even though it would be possible from \((G_{1},G_{2})\) if \(\pi_{12}^{*}\) were known); _Light Green region_: exact community recovery is impossible from \((G_{1},G_{2})\), but exact graph matching of graph pairs is possible, and subsequently exact community recovery is possible from \((G_{1},G_{2},G_{3})\); _Grey region_: exact community recovery is impossible from \((G_{1},G_{2})\), exact graph matching is also impossible from \((G_{1},G_{2})\), but exact graph matching is possible from \((G_{1},G_{2},G_{3})\), and subsequently exact community recovery is possible from \((G_{1},G_{2},G_{3})\); _Yellow region_: exact community recovery is impossible from \((G_{1},G_{2})\), exact graph matching is impossible from \((G_{1},G_{2},G_{3})\), yet exact community recovery is possible from \((G_{1},G_{2},G_{3})\); _Orange region_: exact community recovery is impossible from \((G_{1},G_{2},G_{3})\) (even though it would be possible from \((G_{1},G_{2},G_{3})\) if \(\boldsymbol{\pi}^{*}\) were known); _Red region_: exact community recovery is impossible from \((G_{1},G_{2},G_{3})\) (even if \(\boldsymbol{\pi}^{*}\) is known). The principal finding of this paper is the characterization of the Pink, Violet, Orange, Yellow, Grey, and Light Green regions.

we overcome them. We subsequently explain the generalization from \(3\) graphs to \(K\) graphs. The overview of the impossibility proof is discussed as well.

\(k\)**-core matching.** We now define a \(k\)-core matching [13, 22, 43], which is used for almost exact graph matching in Step 2. Given a pair of graphs \((G,H)\) with vertex set \([n]\), for any permutation \(\pi\), we have the corresponding intersection graph \(G\wedge_{\pi}H\), where \((i,j)\) is an edge in the intersection graph if and only if \((i,j)\) is an edge in \(G\) and \((\pi(i),\pi(j))\) is an edge in \(H\). The \(k\)-core estimator explores all possible permutations \(\pi\) of \([n]\) to seek a permutation \(\widehat{\pi}\) that maximizes the size of the \(k\)-core of the intersection graph \(G\wedge_{\pi}H\); recall that the \(k\)-core of a graph is the maximal induced subgraph for which all vertices have degree at least \(k\). The output of the \(k\)-core estimator is then a partial matching \(\widehat{\mu}\), which is the restriction of \(\widehat{\pi}\) to the vertex set of the \(k\)-core in \(G\wedge_{\widehat{\pi}}H\).

One significant advantage of using \(k\)-core matchings is a certain optimality property in terms of performance. Specifically, if \((G_{1},G_{2})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n}{n},s)\), then the \(k\)-core estimator between \(G_{1}\) and \(G_{2}\) fails to match at most \(n^{1-s^{2}\mathrm{T}_{c}(a,b)+o(1)}\) vertices, which is the same order as the number of singletons of \(G_{1}\wedge_{\pi_{12}^{*}}G_{2}\) and any graph matching algorithm would fail to match these singletons [11, 41, 22]. Another significant benefit of utilizing \(k\)-core matchings is the correctness of the \(k\)-core estimator for correlated SBMs, as discussed in [22]. The \(k\)-core estimator might not be able to match all vertices under the parameter regime that we are interested in; however, every vertex that it does match is matched correctly with high probability.

**Community recovery subroutines.** The high-level summary of the algorithm is as follows. Since exact community recovery might be impossible in \(G_{1}\) alone, we first obtain an initial estimate which gives almost exact community recovery in \(G_{1}\), as described in Step 1. In Step 2, we use pairwise partial \(k\)-core matchings with \(k=13\) to obtain \(\widehat{\boldsymbol{\mu}}:=\{\widehat{\mu}_{12},\widehat{\mu}_{13}, \widehat{\mu}_{23}\}\) (see Fig. 2(b)), which we will use to combine information across \((G_{1},G_{2},G_{3})\) to recover communities. Note that each partial matching \(\widehat{\mu}_{ij}\) only matches a subset of the vertices, denoted as \(M_{ij}\); we denote the set of vertices not matched by \(\widehat{\mu}_{ij}\) by \(F_{ij}:=[n]\setminus M_{ij}\). Subsequently, we split the vertices into two categories: "good" vertices and "bad" vertices, where "good" vertices are part of at least two matchings and "bad" vertices are part of at most one matching (see Fig. 2(a)). We conduct several majority votes among "good" and "bad" vertices to do the clean-up after the graph matching phase, where each subroutine is meticulously executed to disentangle the intricate dependencies among \((G_{1},G_{2},G_{3})\) and \(\widehat{\boldsymbol{\mu}}\).

**Exact community recovery for the "good" vertices.** The major distinction between being "good" and being "bad" is that "good" vertices can combine information from all three graphs via their union graph (which is denser), whereas "bad" vertices cannot. Suppose that vertex \(i\) is part of \(\widehat{\mu}_{12}\) and \(\widehat{\mu}_{13}\) (i.e., \(i\in M_{12}\cap M_{13}\)). We can then identify the union graph \(G_{1}\vee_{\widehat{\mu}_{12}}G_{2}\vee_{\widehat{\mu}_{13}}G_{3}\), which consists of edges \((i,j)\) such that \((i,j)\) is an edge in \(G_{1}\) or \((\widehat{\mu}_{12}(i),\widehat{\mu}_{12}(j))\) is an edge in \(G_{2}\) or \((\widehat{\mu}_{13}(i),\widehat{\mu}_{13}(j))\) is an edge in \(G_{3}\), and \(i\) is part of this union graph. Similarly, if \(i\in M_{12}\cap M_{23}\), then \(i\) is part of the union graph \(G_{1}\vee_{\widehat{\mu}_{12}}G_{2}\vee_{\widehat{\mu}_{23}\circ\widehat{\mu}_ {12}}G_{3}\) that also integrates information from all three graphs. On the union graph, we can refine the almost exact community labeling by reclassifying "good" vertices based on a majority vote among the labels of their neighbors that are also "good", and this reclassification will be correct as long as the condition (3.1) holds.

There are many underlying technical challenges and roadblocks in the theoretical analysis. The key difficulty arises from the structure of the union graph. It is statistically guaranteed that in \(G_{1}\vee_{\pi_{12}^{*}}G_{2}\vee_{\pi_{13}^{*}}G_{3}\), all vertices have a community label which is the same as the majority community among their neighbors [35]. However, whether this is also the case for \(G_{1}\vee_{\widehat{\mu}_{12}}G_{2}\vee_{\widehat{\mu}_{13}}G_{3}\) is unclear, since the latter graph is only defined on the "good" vertices \(M_{12}\cap M_{13}\). One would like to demonstrate that the removal of "bad" vertices does not significantly affect the majority community among neighbors of "good" vertices. Prior work [22] addressed a similar problem for two graphs by employing a technique known as Luczak expansion [27] to \(F_{12}\) to ensure that the vertices inside the expanded set \(\overline{F_{12}}\) are only weakly connected to the vertices outside of the expanded set \([n]\setminus\overline{F_{12}}\). Unfortunately, this method is no longer applicable for correlated SBMs with three or more graphs. Even though the size of the expanded set \(\overline{F_{12}}\) is orderwise equal to the size of \(F_{12}\), the size of the intersection of the expanded sets \(\overline{F_{12}}\cap\overline{F_{13}}\) might not be orderwise equal to the size of \(F_{12}\cap F_{13}\), which directly leads to the failure of the algorithm working down to the information-theoretic threshold. To overcome this challenge, we consider the graph \(G\{[n]\setminus v\}\) to decouple the dependence of \(v\) being connected to a vertex \(w\) and \(w\) being part of the \(k\)-core. Applying the Luczak expansion on such a graph for any given \(v\), and through a union bound, we prove that unmatched vertices are contained in the set of vertices whose degree is smaller than a constant, with high probability. Thisallows us to quantify the size of \(F_{12}\cap F_{13}\) and meanwhile directly ensure that "good" vertices within the \(k\)-core are only weakly connected with "bad" vertices.

Another hurdle needed to overcome, as stated in [22], concerns the almost exact community recovery in Step 1 which is subsequently used for majority votes. Therefore, it is of great importance to guarantee that the incorrectly-classified vertices are not well-connected and do not have a great impact on majority votes. Consequently, we utilize an algorithm originally developed by Mossel, Neeman, and Sly [35] which allows us to manage the geometry of the misclassified vertices and demonstrate that the vertices classified incorrectly are indeed only weakly connected.

**Exact community recovery for the "bad" vertices.** The remaining step is to label the "bad" vertices. The "bad" vertices can be further classified into three categories (see Fig. 3a): vertices in \(F_{12}\cap F_{13}\), which are only matched by \(\widehat{\mu}_{23}\) or are not matched by any of the three matchings; vertices in \(F_{13}\cap F_{23}\setminus F_{12}\), which are only matched by \(\widehat{\mu}_{12}\); and vertices in \(F_{12}\cap F_{23}\setminus F_{13}\), which are only matched by \(\widehat{\mu}_{13}\).

Consider the vertices in \(F_{12}\cap F_{13}\) (the other cases are similar). First of all, as discussed above, we show that \(|F_{12}\cap F_{13}|=n^{1-s(1-(1-s)^{s})\mathrm{T}_{c}(a,b)+o(1)}\) with high probability. Consider the graph \(G_{1}\setminus_{\widehat{\pi}_{12}}G_{2}\setminus_{\widehat{\pi}_{13}}G_{3}\), which consists of the edges \((i,j)\) in \(G_{1}\) such that \((\widehat{\pi}_{12}(i),\widehat{\pi}_{12}(j))\) and \((\widehat{\pi}_{13}(i),\widehat{\pi}_{13}(j))\) are not edges in \(G_{2}\) and \(G_{3}\), respectively. Due to the approximate independence of \(F_{12}\cap F_{13}\) and \(G_{1}\setminus_{\widehat{\pi}_{12}}G_{2}\setminus_{\widehat{\pi}_{13}}G_{3}\), for a vertex \(i\in F_{12}\cap F_{13}\) we can calculate the probability of the failure of the majority vote in the graph \(G_{1}\setminus_{\widehat{\pi}_{12}}G_{2}\setminus_{\widehat{\pi}_{13}}G_{3}\) in a relatively straightforward manner, giving \(n^{-s(1-s)^{2}\mathrm{D}_{+}(a,b)+o(1)}\). The factor \(s(1-s)^{2}\) arises from the fact that the edges in this graph are subsampled in \(G_{1}\) and are not subsampled in \(G_{2}\) and \(G_{3}\). Now since a vertex in \(F_{12}\cap F_{13}\) can have at most \(12\) edges outside of \(F_{12}\) in \(G_{1}\setminus_{\widehat{\pi}_{12}}G_{2}\), and also at most \(12\) edges outside of \(F_{13}\) in \(G_{1}\setminus_{\widehat{\pi}_{13}}G_{3}\), the majority vote for \(i\in F_{12}\cap F_{13}\) essentially does not change whether it is performed in \(G_{1}\) or in \(G_{1}\setminus_{\widehat{\pi}_{12}}G_{2}\setminus_{\widehat{\pi}_{13}}G_{3}\). Putting all this together, the probability that the majority vote fails is at most:

\[\mathbb{P}(\text{exists a vertex }i\in F_{12}\cap F_{13}\text{ such that the majority vote fails})\] \[=|F_{12}\cap F_{13}|\times\mathbb{P}(\text{majority vote fails for a vertex})=n^{1-s(1-(1-s)^{2})\mathrm{T}_{c}(a,b)-s(1-s)^{2}\mathrm{D}_{+}(a,b)+o(1)}.\]

Thus, if (3.2) with \(K=3\) holds, then majority vote will correctly classify all vertices in \(F_{12}\cap F_{13}\).

**Generalization to \(K\) graphs.** For \(K\) graphs, we have \(\binom{K}{2}\) pairwise matchings to consider (see Fig. 3b). We again categorize the vertices as "good" and "bad". The "good" vertices can integrate information across all \(K\) graphs through the pairwise partial \(k\)-core matchings \(\{\widehat{\mu}_{ij}:i,j\in[K],i\neq j\}\), while "bad" vertices cannot. To illustrate this concept more vividly, for any vertex \(v\), consider a new "metagraph" \(\mathcal{MG}_{v}\) on \(K\) nodes, defined as follows: there is an edge between \(i\) and \(j\) in \(\mathcal{MG}_{v}\) if and only if \(v\) can be matched through \(\widehat{\mu}_{ij}\) (see Fig. 4). If the metagraph \(\mathcal{MG}_{v}\) is connected, then there exists a path that can connect all of its \(K\) nodes. Equivalently, there exists a set of matchings that allows us to combine information across all \(K\) graphs. Subsequently, we quantify the number of "bad" vertices to be \(n^{1-s(1-(1-s)^{K-1})\mathrm{T}_{c}(a,b)+o(1)}\). The remaining analysis for \(K\) graphs can be derived by generalizing the analysis for three graphs.

**Impossibility proof.** As discussed in Section 3, we focus on the proof of (3.4) for impossibility. We compute the maximum a posterior (MAP) estimator for the communities in \(G_{1}\). We show that, even with significant additional information provided, including all the correct community labels in \(G_{2}\), the true matchings \(\pi^{*}_{ij}\) for \(i,j\in\{2,3,\ldots,K\}\), and most of the true matching \(\pi^{*}_{12}\) except for singletons in the graph \(G_{1}\setminus_{\pi^{*}_{12}}(G_{2}\vee\ldots\lor G_{K})\), the MAP estimator fails to exactly recovery communities with probability bounded away from 0 if (3.4) holds. The proof is adapted from the MAP analysis in [22]. The difference is that here we are considering \(K\) graphs \(G_{1},G_{2},\ldots,G_{K}\) with different additional information provided for the MAP estimator. Given that the MAP estimator is ineffective under this regime, all other estimators also fail.

**Exact graph matching threshold.** The proof of the exact graph matching threshold is implicitly present in the proof of the exact community recovery threshold. Essentially, since we show that the number of "bad" vertices is \(n^{1-s(1-(1-s)^{K-1})\mathrm{T}_{c}(a,b)+o(1)}\), the condition (3.5) implies that there are no "bad" vertices with high probability. Since all vertices are "good", and "good" vertices can integrate information across all \(K\) graphs, the latent matchings can be recovered exactly. For the impossibility result we analyze the MAP estimator and show that, even with significant additional information, including the true matchings \(\{\pi^{*}_{ij}:i,j\in\{2,3,\ldots,K\}\}\), it fails if (3.6) holds.

Related work

Our work generalizes--and solves an open question raised by--the work of Gaudio, Racz, and Sridhar [22]. Just as [22], our work lies at the interface of the literatures on community recovery and graph matching1--two fundamental learning problems--which we briefly summarize here.

Footnote 1: We note that graph matching has both positive and negative societal impacts. In particular, it is well known that graph matching algorithms can be used to de-anonymize social networks, showing that anonymity is different, in general, from privacy [38]. At the same time, studying fundamental limits can aid in determining precise conditions when anonymity can indeed guarantee privacy, and when additional safeguards are necessary.

**Community recovery in SBMs.** A huge research literature exists on learning latent community structures in networks, and this topic is especially well understood for the SBM [25; 15; 34; 35; 33; 36; 2; 3; 7]. Specifically, we highlight the work of [2; 35], which identify the precise threshold for exact community recovery for SBMs with two balanced communities. Our algorithm builds upon their analysis, taking particular care about dealing with the dependencies arising from the multiple inexact partial matchings between \(K\) correlated graphs.

**Graph matching: correlated Erdos-Renyi random graphs.** The past decade has seen a plethora of research on average-case graph matching, focusing on correlated Erdos-Renyi random graphs [40]. The information-theoretic thresholds for recovering the latent vertex correspondence \(\pi^{*}\) have been established for exact recovery [11; 46; 12], almost exact recovery [13], and weak recovery [20; 21; 24; 46; 16]. In parallel, a line of work has focused on algorithmic advances [37; 6; 18; 19; 30; 31; 32], culminating in recent breakthroughs that developed efficient graph matching algorithms in the constant noise setting [31; 32]. We particularly highlight the work of Cullina, Kiyavash, Mittal, and Poor [13], who introduced \(k\)-core matchings and showed their utility for partial matching of correlated Erdos-Renyi random graphs. Subsequent work has shown the power of \(k\)-core matchings as a flexible and successful tool for graph matching [22; 43; 4]. Our work both significantly builds upon these works, as well as further develops this machinery, which may be of independent interest. We also note the independent and concurrent work of Ameen and Hajek which determined the exact graph matching threshold for \(K\) correlated Erdos-Renyi random graphs [5].

**Graph matching: beyond correlated Erdos-Renyi random graphs.** Motivated by real-world networks, a growing line of recent work studies graph matching beyond Erdos-Renyi graphs [8; 26; 9; 39; 42; 49; 41; 22; 45; 43; 17; 48; 47], including for correlated SBMs [29; 39; 28; 41; 22; 48; 47]. The works that are most relevant to ours are [41; 22], which have been discussed extensively above.

## 6 Discussion and Future Work

Our main contribution highlights the power of integrative data analysis for community recovery, yet many open questions still remain.

**Efficient algorithms.** Theorem 1 characterizes when exact community recovery is information-theoretically possible from \(K\) correlated SBMs. Is this possible _efficiently_ (i.e., in time polynomial in \(n\))? The bottleneck in the algorithm that we use to prove Theorem 1, which makes it inefficient, is the \(k\)-core matching step; the other steps are efficient. Recent breakthrough results have developed efficient graph matching algorithms for correlated Erdos-Renyi random graphs [31; 32], which promisingly suggest that an efficient algorithm for exact community recovery may indeed exist in this regime. We refer to [22] for further discussion on this point.

**General block models.** We focused here on the simplest case of SBMs with two balanced communities. It would be interesting to extend these results to general block models with multiple communities. This is understood well in the single graph setting [1] and recent work has also characterized the threshold for exact graph matching for two correlated SBMs with \(k\) symmetric communities [47].

**Alternative constructions of correlated graphs.** An exciting research direction is to study different constructions of correlated graph models. For general \(K\), there are many ways that \(K\) graphs can be correlated. In particular, the following is a natural alternative construction of multiple correlated SBMs. First, generate \(G_{0}\sim\mathrm{SBM}(n,p,q)\). Then, independently generate \(H_{i}\sim\mathrm{SBM}(n,p^{\prime},q^{\prime})\) for \(i\in[K]\). Construct \(G^{\prime}_{i}:=G_{0}\lor H_{i}\), and finally generate \(G_{i}\) through an independent random permutation of the vertex indices in \(G^{\prime}_{i}\). This construction is equivalent to the one we studied in this paper for \(K=2\) and it is different when \(K\geq 3\), and investigating it is interesting and valuable.

## Acknowledgements

We thank Taha Ameen, Julia Gaudio, Elchanan Mossel, and Anirudh Sridhar for helpful discussions. We also thank anonymous reviewers for constructive feedback.

## References

* Abbe [2018] E. Abbe. Community Detection and Stochastic Block Models: Recent Developments. _Journal of Machine Learning Research_, 18(177):1-86, 2018.
* Abbe et al. [2016] E. Abbe, A. S. Bandeira, and G. Hall. Exact Recovery in the Stochastic Block Model. _IEEE Transactions on Information Theory_, 62(1):471-487, 2016.
* Abbe and Sandon [2015] E. Abbe and C. Sandon. Community detection in general stochastic block models: Fundamental limits and efficient algorithms for recovery. In _2015 IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 670-688. IEEE, 2015.
* Ameen and Hajek [2023] T. Ameen and B. Hajek. Robust Graph Matching when Nodes are Corrupt. Preprint available at https://arxiv.org/abs/2310.18543, 2023.
* Ameen and Hajek [2024] T. Ameen and B. Hajek. Exact Random Graph Matching with Multiple Graphs. Preprint available at https://arxiv.org/abs/2405.12293, 2024.
* Barak et al. [2019] B. Barak, C.-N. Chou, Z. Lei, T. Schramm, and Y. Sheng. (Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 32, pages 9190-9198, 2019.
* Bordenave et al. [2015] C. Bordenave, M. Lelarge, and L. Massoulie. Non-backtracking Spectrum of Random Graphs: Community Detection and Non-regular Ramanujan Graphs. In _Proceedings of the 2015 IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 1347-1357. IEEE, 2015.
* Bringmann et al. [2014] K. Bringmann, T. Friedrich, and A. Krohmer. De-anonymization of Heterogeneous Random Graphs in Quasilinear Time. In _Proceedings of the 22nd Annual European Symposium on Algorithms (ESA)_, pages 197-208, 2014.
* Chiasserini et al. [2016] C.-F. Chiasserini, M. Garetto, and E. Leonardi. Social Network De-Anonymization Under Scale-Free User Relations. _IEEE/ACM Transactions on Networking_, 24(6):3756-3769, 2016.
* Cour et al. [2006] T. Cour, P. Srinivasan, and J. Shi. Balanced Graph Matching. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 19, 2006.
* Cullina and Kiyavash [2016] D. Cullina and N. Kiyavash. Improved Achievability and Converse Bounds for Erdos-Renyi Graph Matching. In _ACM SIGMETRICS_, volume 44, pages 63-72, 2016.
* Cullina and Kiyavash [2018] D. Cullina and N. Kiyavash. Exact alignment recovery for correlated Erdos-Renyi graphs. Preprint available at https://arxiv.org/abs/1711.06783, 2018.
* Cullina et al. [2020] D. Cullina, N. Kiyavash, P. Mittal, and H. V. Poor. Partial Recovery of Erdos-Renyi Graph Alignment via \(k\)-Core Alignment. In _ACM SIGMETRICS Performance Evaluation Review_, volume 48, pages 99-100. ACM, 2020.
* Cullina et al. [2016] D. Cullina, K. Singhal, N. Kiyavash, and P. Mittal. On the Simultaneous Preservation of Privacy and Community Structure in Anonymized Networks. Preprint available at https://arxiv.org/abs/1603.08028, 2016.
* Decelle et al. [2011] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborova. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. _Physical Review E_, 84(6):066106, 2011.
* Ding and Du [2023] J. Ding and H. Du. Matching recovery threshold for correlated random graphs. _The Annals of Statistics_, 51(4):1718-1743, 2023.

* [17] J. Ding, Y. Fei, and Y. Wang. Efficiently matching random inhomogeneous graphs via degree profiles. Preprint available at https://arxiv.org/abs/2310.10441, 2023.
* [18] J. Ding, Z. Ma, Y. Wu, and J. Xu. Efficient random graph matching via degree profiles. _Probability Theory and Related Fields_, 179(1):29-115, 2021.
* [19] Z. Fan, C. Mao, Y. Wu, and J. Xu. Spectral Graph Matching and Regularized Quadratic Relaxations: Algorithm and Theory. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, volume 119 of _Proceedings of Machine Learning Research (PMLR)_, pages 2985-2995, 2020.
* [20] L. Ganassali and L. Massoulie. From tree matching to sparse graph alignment. In _Proceedings of the 33rd Conference on Learning Theory (COLT)_, volume 125 of _Proceedings of Machine Learning Research (PMLR)_, pages 1633-1665, 2020.
* [21] L. Ganassali, L. Massoulie, and M. Lelarge. Impossibility of Partial Recovery in the Graph Alignment Problem. In _Proceedings of the 34th Conference on Learning Theory (COLT)_, volume 134 of _Proceedings of Machine Learning Research (PMLR)_, pages 2080-2102, 2021.
* [22] J. Gaudio, M. Z. Racz, and A. Sridhar. Exact Community Recovery in Correlated Stochastic Block Models. In _Proceedings of the 35th Conference on Learning Theory (COLT)_, volume 178 of _Proceedings of Machine Learning Research (PMLR)_, pages 2183-2241, 2022.
* [23] M. Girvan and M. E. J. Newman. Community structure in social and biological networks. _Proceedings of the National Academy of Sciences_, 99(12):7821-7826, 2002.
* [24] G. Hall and L. Massoulie. Partial recovery in the graph alignment problem. _Operations Research_, 71(1):259-272, 2023.
* [25] P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. _Social Networks_, 5(2):109-137, 1983.
* [26] N. Korula and S. Lattanzi. An efficient reconciliation algorithm for social networks. _Proceedings of the VLDB Endowment_, 7(5):377-388, 2014.
* [27] T. Luczak. Size and connectivity of the \(k\)-core of a random graph. _Discrete Mathematics_, 91(1):61-68, 1991.
* [28] V. Lyzinski. Information Recovery in Shuffled Graphs via Graph Matching. _IEEE Transactions on Information Theory_, 64(5):3254-3273, 2018.
* [29] V. Lyzinski, D. L. Sussman, D. E. Fishkind, H. Pao, L. Chen, J. T. Vogelstein, Y. Park, and C. E. Priebe. Spectral clustering for divide-and-conquer graph matching. _Parallel Computing_, 47:70-87, 2015.
* [30] C. Mao, M. Rudelson, and K. Tikhomirov. Random Graph Matching with Improved Noise Robustness. In _Proceedings of the 34th Conference on Learning Theory (COLT)_, volume 134 of _Proceedings of Machine Learning Research (PMLR)_, pages 3296-3329, 2021.
* [31] C. Mao, M. Rudelson, and K. Tikhomirov. Exact matching of random graphs with constant correlation. _Probability Theory and Related Fields_, 186:327-389, 2023.
* [32] C. Mao, Y. Wu, J. Xu, and S. H. Yu. Random Graph Matching at Otter's Threshold via Counting Chandeliers. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing (STOC)_, pages 1345-1356, 2023.
* [33] L. Massoulie. Community detection thresholds and the weak Ramanujan property. In _Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC)_, pages 694-703. ACM, 2014.
* [34] E. Mossel, J. Neeman, and A. Sly. Reconstruction and estimation in the planted partition model. _Probability Theory and Related Fields_, 162:431-461, 2015.
* 24, 2016.
* [36] E. Mossel, J. Neeman, and A. Sly. A proof of the block model threshold conjecture. _Combinatorica_, 38(3):665-708, 2018.
* [37] E. Mossel and J. Xu. Seeded graph matching via large neighborhood statistics. _Random Structures & Algorithms_, 57(3):570-611, 2020.
* [38] A. Narayanan and V. Shmatikov. De-anonymizing Social Networks. In _Proceedings of the 30th IEEE Symposium on Security and Privacy_, pages 173-187. IEEE Computer Society, 2009.
* [39] E. Onaran, S. Garg, and E. Erkip. Optimal de-anonymization in random graphs with community structure. In _2016 50th Asilomar Conference on Signals, Systems and Computers_, pages 709-713. IEEE, 2016.
* [40] P. Pedarsani and M. Grossglauser. On the privacy of anonymized networks. In _Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)_, pages 1235-1243, 2011.
* [41] M. Z. Racz and A. Sridhar. Correlated Stochastic Block Models: Exact Graph Matching with Applications to Recovering Communities. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, pages 22259-22273, 2021.
* [42] M. Z. Racz and A. Sridhar. Correlated randomly growing graphs. _The Annals of Applied Probability_, 32(2):1058-1111, 2022.
* [43] M. Z. Racz and A. Sridhar. Matching Correlated Inhomogeneous Random Graphs using the \(k\)-core Estimator. In _2023 IEEE International Symposium on Information Theory (ISIT)_, pages 2499-2504, 2023.
* [44] R. Singh, J. Xu, and B. Berger. Global alignment of multiple protein interaction networks with application to functional orthology detection. _Proceedings of the National Academy of Sciences_, 105(35):12763-12768, 2008.
* [45] H. Wang, Y. Wu, J. Xu, and I. Yolou. Random Graph Matching in Geometric Models: the Case of Complete Graphs. In _Proceedings of the 35th Conference on Learning Theory (COLT)_, volume 178 of _Proceedings of Machine Learning Research (PMLR)_, pages 3441-3488, 2022.
* [46] Y. Wu, J. Xu, and S. H. Yu. Settling the Sharp Reconstruction Thresholds of Random Graph Matching. _IEEE Transactions on Information Theory_, 68(8):5391-5417, 2022.
* [47] J. Yang and H. W. Chung. Graph Matching in Correlated Stochastic Block Models for Improved Graph Clustering. In _Proceedings of the 2023 59th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 1-8. IEEE, 2023.
* [48] J. Yang, D. Shin, and H. W. Chung. Efficient Algorithms for Exact Graph Matching on Correlated Stochastic Block Models with Constant Correlation. In _Proceedings of the 40th International Conference on Machine Learning (ICML)_, volume 202 of _Proceedings of Machine Learning Research (PMLR)_, pages 39416-39452, 2023.
* [49] L. Yu, J. Xu, and X. Lin. The Power of \(D\)-hops in Matching Power-Law Graphs. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 5(2):1-43, 2021.

Organization

The rest of the paper is structured as follows. First, we elaborate on the recovery algorithm for three graphs in Section C. Section D includes some useful preliminary propositions, including some nice properties of almost exact community recovery on \(G_{1}\). Section E discusses the \(k\)-core estimator. After these preparations, we are ready to prove the main theorems in the paper.

Section F proves Theorem 1 for three graphs, where we first validate the accuracy of the community labels for "good" vertices and then classify the remaining "bad" vertices. Section G presents the proof of the impossibility result (Theorem 2) for three graphs. Section H discusses the recovery algorithm for \(K\) graphs and provides a general proof for \(K\) graphs, with additional arguments on how to identify "good" and "bad" vertices. Section I discusses the proof of the impossibility result (Theorem 2) for \(K\) graphs. Section J contains the proof of the threshold for exact graph matching given \(K\) graphs, that is, the proofs of Theorems 3 and 4.

## Appendix B Notation

We introduce here some notation that will be used in the rest of the paper. In most of the paper we focus on the setting of \(K=3\) graphs: \((G_{1},G_{2},G_{3})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n}{n},s)\), and this is the setting that we consider here as well.

Let \(V:=[n]=\{1,2,3,...,n\}\) denote the vertex set of the parent graph \(G_{0}\), and let \(V^{+}:=\{i\in[n]:\sigma^{*}(i)=+1\}\) and \(V^{-}:=\{i\in[n]:\sigma^{*}(i)=-1\}\) denote the sets of vertices in the two communities. Let \(\binom{[n]}{2}:=\{(i,j):i,j\in[n],i\neq j\}\) denote the set of all unordered vertex pairs. Given a community labeling \(\sigma\in\{+1,-1\}^{n}\), we define the set of intra-community vertex pairs as \(\mathcal{E}^{+}(\sigma):=\{(i,j)\in\binom{[n]}{2}:\sigma(i)=\sigma(j)\}\) and the set of inter-community vertex pairs as \(\mathcal{E}^{+}(\sigma):=\{(i,j)\in\binom{[n]}{2}:\sigma(i)=-\sigma(j)\}\). Note that \(\mathcal{E}^{+}(\sigma)\) and \(\mathcal{E}^{-}(\sigma)\) form a partition of \(\binom{[n]}{2}\).

Let \(A\), \(B\), and \(C\) denote the adjacency matrices of \(G_{1}\), \(G_{2}\), and \(G_{3}\), respectively. Let \(B^{\prime}\) and \(C^{\prime}\) denote the adjacency matrices of \(G^{\prime}_{2}\) and \(G^{\prime}_{3}\), respectively. Note that, by construction, we have for all \((i,j)\in\binom{[n]}{2}\) that \(B^{\prime}_{i,j}=B_{\pi_{12}^{\prime}(i),\pi_{12}^{\prime}(j)}\) and \(C^{\prime}_{i,j}=C_{\pi_{13}^{\prime}(i),\pi_{13}^{\prime}(j)}\). Observe that we have the following probabilities for every \((i,j)\in\binom{[n]}{2}\). If \(a,b,c\in\{0,1\}^{3}\) and \(a+b+c>0\), then

\[\mathbb{P}\left(\left(A_{ij},B^{\prime}_{ij},C^{\prime}_{ij}\right)=(a,b,c) \right)=\begin{cases}s^{a+b+c}(1-s)^{3-a-b-c}p&\text{if }\sigma^{*}(i)=\sigma^{*}(j),\\ s^{a+b+c}(1-s)^{3-a-b-c}q&\text{if }\sigma^{*}(i)\neq\sigma^{*}(j).\end{cases}\]

Furthermore, we have that

\[\mathbb{P}\left(\left(A_{ij},B^{\prime}_{ij},C^{\prime}_{ij}\right)=(0,0,0) \right)=\begin{cases}1-p+(1-s)^{3}p&\text{if }\sigma^{*}(i)=\sigma^{*}(j),\\ 1-q+(1-s)^{3}q&\text{if }\sigma^{*}(i)\neq\sigma^{*}(j).\end{cases}\]

## Appendix C The recovery algorithm for three graphs

Our recovery algorithm is based on discovering a matching between subsets of two graphs.

**Definition C.1**.: _Let \(G_{i}\) and \(G_{j}\) be two graphs in vertex set \([n]\) with adjacency matrix \(A,B\), respectively. The pair \((M_{ij},\mu_{ij})\) is a matching between \(G_{i}\) and \(G_{j}\) if_

* \(M_{ij}\in[n]\)_,_
* \(\mu_{ij}:M_{ij}\to[n]\)_,_
* \(\mu_{ij}\) _is injective._

Given a matching \((M_{ij},\mu_{ij})\), here are some related notations. Define \(G_{i}\vee_{\mu_{ij}}G_{j}\) to be the union graph, whose vertex set is \(M\), whose vertex index is the same as the vertex index of \(G_{i}\) and whose edge set is \(\{\{\ell,m\}:\ell,m\in M_{ij},A_{\ell m}+B_{\mu_{ij}(\ell),\mu_{ij}(m)}\geq 1\}\). In other words, the edges are those that appear in either \(G_{i}\) or \(G_{j}\). Conversely, \(G_{i}\wedge_{\mu_{ij}}G_{j}\) represents intersection graph, whose vertex set is \(M_{ij}\), whose vertex index is the same as the vertex index of \(G_{i}\) and whose edge set is \(\{\{\ell,m\}:\ell,m\in M_{ij},A_{\ell m}=B_{\mu_{ij}(\ell),\mu_{ij}(m)}=1\}\). In other words, the edges are those that appear in both \(G_{i}\) and \(G_{j}\). Define \(G_{i}\setminus_{\mu_{ij}}G_{j}\) to be the graph \(G_{i}\) minus \(G_{j}\), whose vertex set is \(M_{ij}\), whose vertex index is the same as the vertex index of \(G_{i}\) and the edges are those only appear on \(G_{i}\) and not appear in \(G_{j}\).

**Definition C.2**.: _Let \(G_{i}\) and \(G_{j},G_{k}\) be three graphs on vertex set \([n]\) with adjacency matrix \(A,B,C\), respectively. The pair \((M_{ij},\mu_{ij})\) is a matching between \(G_{i}\) and \(G_{j}\), while the pair \((M_{jk},\mu_{jk})\) is a matching between \(G_{j}\) and \(G_{k}\). Denote \(\mu_{jk}\circ\mu_{ij}\) as the composition matching between \(G_{i}\) and \(G_{k}\), defined on the vertex set \(M_{ij}\cap M_{jk}\)._

For three graphs, we can define the additional notations in the same manner as in Definition C.1 and the core concepts remain consistent. \(G_{i}\vee_{\mu_{ij}}G_{j}\vee_{\mu_{jk}\circ\mu_{ij}}G_{k}\) represents the union graph of \(G_{i},G_{j},G_{k}\), whose vertex set is \(M=M_{ij}\cap M_{jk}\), whose vertex index is the same as the vertex index of \(G_{i}\) and whose edge set is \(\{\{\ell,m\}:\ell,m\in M,A_{\ell m}+B_{\mu_{ij}(\ell),\mu_{ij}(m)}+C_{\mu_{jk} \circ\mu_{ij}(\ell),\mu_{jk}\circ\mu_{ij}(m)}\geq 1\}\). In other words, the edge set are the edges that appears in at least one graph out of \(G_{i},G_{j},G_{k}\). Similarly, \(G_{i}\wedge_{\mu_{ij}}G_{j}\wedge_{\mu_{jk}}G_{k}\) represents the intersection graph, the edge set is \(\{\{\ell,m\}:\ell,m\in M;A_{\ell m}=B_{\mu_{ij}(\ell),\mu_{ij}(m)}=C_{\mu_{jk} \circ\mu_{ij}(\ell),\mu_{jk}\circ\mu_{ij}(m)}=1\}\). Define \(G_{i}\vee_{\mu_{ij}}G_{j}\setminus_{\mu_{jk}}G_{k}\) be the graph whose edge set is those edges that appear in either \(G_{i}\) or \(G_{j}\) and not appear in \(G_{k}\). Similarly, we can define \(G_{i}\wedge_{\mu_{ij}}G_{i}\setminus_{\mu_{jk}}G_{k}\), \(G_{i}\setminus_{\mu_{ij}}(G_{i}\vee_{\mu_{jk}}G_{k})\), and \(G_{i}\setminus_{\mu_{ij}}(G_{i}\wedge_{\mu_{jk}}G_{k})\) as well. Note that all the definitions above are defined on vertex set \(M\) and use vertex index in \(G_{i}\).

Introduce \(d_{\min}(G):=\min_{i\in[n]}d(i)\), where \(d(i)\) is the degree of vertex \(i\).

**Definition C.3**.: _A matching \((M_{ij},\mu_{ij})\) is a \(k\)-core matching of \((G_{i},G_{j})\) if \(d_{\min}(G_{i}\wedge_{\mu_{ij}}G_{j})\geq k\). A matching \((M_{ij},\mu_{ij})\) is called a maximal \(k\)-core matching if it involves the greatest number of vertices, among all \(k\)-core matchings._

**Input:** Pair of graphs \(G_{i},G_{j}\) on \(n\) vertices, \(k\in[n]\).

**Output:** A matching \((\widehat{M}_{ij},\widehat{\mu}_{ij})\) of \(G_{i}\) and \(G_{j}\).

```
1: Enumerating all possible matchings, find the maximal \(k\)-core matching \((\widehat{M}_{ij},\widehat{\mu}_{ij})\) of \(G_{i}\) and \(G_{j}\). ```

**Algorithm 1**\(k\)-core matching

Let \((M_{ij},\mu_{ij})\) be the matching found by Algorithm 1 with \(k=13\). \(M_{ij}\) coincides with the maximal \(k\)-core of \(G_{i}\wedge_{\pi^{*}_{ij}}G_{j}\), denote it as \(M^{*}_{ij}\) while \(\mu_{ij}\) coincides with the true permutation \(\pi^{*}_{ij}\), with high probability (Lemma E.5).

The \(k\)-core matching is symmetric, i.e. \(\mu_{ij}(M_{ij})=M_{ji}\). Note that by Definition C.1, \(M_{ij}\) uses the vertex index of \(G_{i}\) while \(M_{ji}\) uses the vertex index of \(G_{j}\), they are equivalent and exchangeable through the 1-1 mapping. Now define \(F_{ij}:=[n]\setminus M_{ij}\) be the set of vertices which are excluded from the matching. Note that \(F_{ij}\) use the vertice index same as \(G_{i}\). We define \(F^{*}_{ij}:=[n]\setminus M^{*}_{ij}\) be the set of vertices which are outside the maximal \(k\)-core of \(G_{i}\wedge_{\pi^{*}_{ij}}G_{j}\).

As briefly discussed in Section 4, we start with leveraging the "good" vertices in order to find the correct communities. The "good" vertices are those which are part of at least two matchings out of three partial matchings \(\mu_{12},\mu_{13},\mu_{23}\). The details are shown in Algorithm 2.

Figure 3: Schematic landscape of partial matchings over three graphs.

The remaining step is to label the "bad" vertices which cannot utilize the combined information from three graphs. Hence, we classify the "bad" vertices according to the majority of neighborhood restricted to the corresponding "good" vertices. The detailed descriptions are shown Algorithm 3.

```
0: Three graphs \(G_{1},G_{2},G_{3}\) on \(n\) vertices and three \(13\)-core matching \((M_{12},\mu_{12})\), \((M_{13},\mu_{13})\), and \((M_{23},\mu_{23})\), parameters \(a,b,s\), a label on the "good" vertices \(\widehat{\sigma}\).
0: A labeling of \([n]\) given by \(\widehat{\sigma}\).
1: For \(i\in F_{12}\cap F_{13}\), set \(\widehat{\sigma}(i)\in\{-1,1\}\) according to the neighborhood majority (resp., minority) of \(\widehat{\sigma}(i)\) with respect to the graph \(G_{1}(M_{12}\cap M_{13}\cup\{i\})\) if \(a>b\) (resp., \(a<b\)).
2: For \(i\in F_{23}\cap F_{13}\setminus F_{12}\), set \(\widehat{\sigma}(i)\in\{-1,1\}\) according to the neighborhood majority (resp., minority) of \(\widehat{\sigma}(i)\) with respect to the graph \(G_{1}\setminus_{\mu_{12}}G_{2}(M_{12}\cap M_{13}\cup\{i\})\) if \(a>b\) (resp., \(a<b\)).
3: For \(i\in F_{12}\cap F_{23}\setminus F_{13}\), set \(\widehat{\sigma}(i)\in\{-1,1\}\) according to the neighborhood majority (resp., minority) of \(\widehat{\sigma}(i)\) with respect to the graph \(G_{1}\setminus_{\mu_{12}}G_{2}\setminus_{\mu_{13}}G_{3})\{M_{13}\cap M_{12}\}\) if \(a>b\) (resp., \(a<b\)).
4: Return \(\widehat{\sigma}:(M_{13}\cap M_{32})\cup(M_{12}\cap M_{13})\cup(M_{23}\cap M_{1 2})\rightarrow\{-1,1\}\). ```

**Algorithm 3** Labeling the bad vertices

**Input:** Three graphs \(G_{1},G_{2},G_{3}\) on \(n\) vertices and three \(13\)-core matching \((M_{12},\mu_{12})\),

(\(M_{13},\mu_{13}\)), and \((M_{23},\mu_{23})\), parameters \(a,b,s\), a label on the "good" vertices \(\widehat{\sigma}\).

**Output:** A labeling of \([n]\) given by \(\widehat{\sigma}\).

```
1: For \(i\in F_{12}\cap F_{13}\), set \(\widehat{\sigma}(i)\in\{-1,1\}\) according to the neighborhood majority (resp., minority) of \(\widehat{\sigma}(i)\) with respect to the graph \(G_{1}(M_{12}\cap M_{13}\cup\{i\})\) if \(a>b\). (resp., \(a<b\))
2: For \(i\in F_{23}\cap F_{13}\setminus F_{12}\), set \(\widehat{\sigma}(i)\in\{-1,1\}\) according to the neighborhood majority (resp., minority) of \(\widehat{\sigma}(i)\) with respect to the graph \(G_{1}\setminus_{\mu_{12}}G_{2}(M_{12}\cap M_{13}\cup\{i\})\) if \(a>b\) (resp., \(a<b\)).
3: For \(i\in F_{12}\cap F_{23}\setminus F_{13}\), set \(\widehat{\sigma}(i)\in\{-1,1\}\) according to the neighborhood majority (resp., minority) of \(\widehat{\sigma}(i)\) with respect to the graph \(G_{1}\setminus_{\mu_{13}}G_{3}(M_{12}\cap M_{13}\cup\{i\})\) if \(a>b\) (resp., \(a<b\)).
4: Return \(\widehat{\sigma}:[n]\rightarrow\{-1,1\}\). ```

**Algorithm 4** Full Community Recovery

The complete exact recovery algorithm is exhibited in Algorithm 4. First, the \(13\)-core matchings are preformed. Next, the "good" vertices are labeled according to the union graph. Finally, the "bad" vertices are labeled according to neighborhood labels in \(G_{1}\) or \(G_{1}\setminus_{\mu_{12}}G_{2}\) or \(G_{1}\setminus_{\mu_{13}}G_{3}\).

```
0: Three graphs \((G_{1},G_{2},G_{3})\) on \(n\) vertices, \(k=13\), and \(\epsilon>0\).
0: A labeling of \([n]\) given by \(\widehat{\sigma}\).
1: Apply Algorithm 1 on input \((G_{i},G_{j},k)\), obtaining a matching \((\widehat{M}_{ij},\widehat{\mu}_{ij}),i\neq j\in\{1,2,3\}\). Denote \(\widehat{M}:=(\widehat{M}_{13}\cap\widehat{M}_{32})\cup(\widehat{M}_{12} \cap\widehat{M}_{13})\cup(\widehat{M}_{23}\cap\widehat{M}_{12})\).
2: Apply Algorithm 2 on input \((G_{1},G_{2},G_{3},\widehat{M}_{12},\widehat{M}_{23},\widehat{M}_{13}, \widehat{\mu}_{13},\widehat{\mu}_{12},\widehat{\mu}_{23})\), obtaining a labeling \(\widehat{\sigma}:\widehat{M}\rightarrow\{-1,1\}\).
3: Apply Algorithm 3 on input \((G_{1},G_{2},G_{3},\widehat{M}_{12},\widehat{M}_{23},\widehat{M}_{13}, \widehat{\mu}_{13},\widehat{\mu}_{12},\widehat{\mu}_{23},\widehat{\sigma})\), obtaining a labeling \(\widehat{\sigma}:[n]\rightarrow\{-1,1\}\).
4: Return \(\widehat{\sigma}:[n]\rightarrow\{-1,1\}\). ```

**Algorithm 5** Full Community Recovery

## Appendix D Preliminaries

Here we provide some useful preliminary propositions.

### Binomial Probabilities

**Lemma D.1**.: _Suppose that \(a\geq b\). Let \(Y\sim\operatorname{Bin}(m^{+},a\log(n)/n)\) and \(Z\sim\operatorname{Bin}(m^{-},b\log(n)/n)\) be independent. If \(m^{+}=(1+o(1))n/2,m^{-}=(1+o(1))n/2\), then for any \(\epsilon>0\),_

\[\mathbb{P}(Y-Z\leq\epsilon\log n)\leq n^{-\mathrm{D}_{+}(a,b)+\epsilon\log(a/ b)/2+o(1)}.\]

Proof.: Proved by [22, Lemma 3.3]. 

### A useful construction of three correlated stochastic block models

In this section, we elaborate on an alternative method for constructing three correlated SBMs, which emphasizes the independent regions of \(G_{1},G_{2}\) and \(G_{3}\). we detail the construction for three graphs to maintain reasonable and manageable notation throughout our discussion. The extension of these ideas to the general case of \(K\) graphs follows a similar structure where the key steps and arguments can be directly applied. This construction is analogous to the construction from [22, Section 3.2], generalizing the case from two graphs to three graphs.

Firstly, we construct a random partition \(\{\mathcal{E}_{ijk},i,j,k,\in\{0,1\}\}\) of \({[n]\choose 2}\). Independently, for each pair \(\{i,j\}\in{[n]\choose 2}\), we let \(\{i,j\}\in\{\mathcal{E}_{ijk}\}\) with a probability of \((1-s)^{3-i-j-k}s^{i+j+k}\). Subsequently, for each pair \(\{i,j\}\in{[n]\choose 2}\), an edge is constructed between \(i\) and \(j\) with probability \(p\) if the two vertices are in the same community, and with probability \(q\) if they are in different communities. Graph \(G_{1}\) is constructed using the edges from \(\cup_{j,k\in\{0,1\},i=1}\mathcal{E}_{ijk}\), while the graph \(G^{\prime}_{2}\) is constructed using edges from \(\cup_{i,k\in\{0,1\},j=1}\mathcal{E}_{ijk}\). Graph \(G_{2}\) is then generated from \(G^{\prime}_{2}\) and \(\pi^{*}_{12}\) by relabeling the vertices of \(G^{\prime}_{2}\) according to \(\pi^{*}_{12}\). Similarly, the graph \(G^{\prime}_{3}\) is constructed using edges from \(\cup_{i,j\in\{0,1\},k=1}\mathcal{E}_{ijk}\) and \(G_{3}\) is obtained from \(G^{\prime}_{3}\) and \(\pi^{*}_{13}\) by relabeling the vertices of \(G^{\prime}_{3}\) according to \(\pi^{*}_{13}\). This construction offers an alternative method for generating multiple correlated SBMs and emphasizes regions of independence between the multiple graphs. The following lemma D.2 describes the idea formally.

**Lemma D.2**.: _The random partition construction of correlated SBMs in Section D.2 is equivalent to the original construction shown in Figure 1. Moreover, conditioned on \(\boldsymbol{\pi}^{\star}:=(\pi^{*}_{12},\pi^{*}_{13},\pi^{*}_{23})\), \(\sigma^{\star}\), and \(\boldsymbol{\mathcal{E}}:=\{\mathcal{E}_{ijk},i,j,k\in\{0,1\}\}\), the graphs that are comprised of edges in disjoint \(\mathcal{E}_{ijk}\) are mutually independent._

Proof.: Firstly we show that the distribution of \((A_{i,j},B_{\pi^{*}_{12}(i),\pi^{*}_{12}(j)},C_{\pi^{*}_{13}(i),\pi^{*}_{13}( j)})\) is the same under two constructions. Then by the indepence of vertex pairs, the equivalence follows. In the first construction,

If \(a+b+c>0\):

\[\mathbb{P}((A_{ij},B_{\pi^{*}_{12}(i)\pi^{*}_{12}(j)},C_{\pi^{*}_{13}(i)\pi^{* }_{13}(j)})=(a,b,c)|\boldsymbol{\pi}^{\star},\sigma^{*})\]

\[=\left\{\begin{array}{ll}s^{a+b+c}(1-s)^{3-a-b-c}p&\mbox{if }\sigma^{*}(i)= \sigma^{*}(j),\\ s^{a+b+c}(1-s)^{3-a-b-c}q&\mbox{if }\sigma^{*}(i)\neq\sigma^{*}(j).\end{array}\right.\]

If \(a+b+c=0\):

\[\mathbb{P}((A_{ij},B_{\pi^{*}_{12}(i)\pi^{*}_{12}(j)},C_{\pi^{*}_{13}(i)\pi^{* }_{13}(j)})=(0,0,0)|\boldsymbol{\pi}^{\star},\sigma^{*})\]

\[=\left\{\begin{array}{ll}1-p+(1-s)^{3}p&\mbox{if }\sigma^{*}(i)=\sigma^{*}(j),\\ 1-q+(1-s)^{3}q&\mbox{if }\sigma^{*}(i)\neq\sigma^{*}(j).\end{array}\right.\]

Under the second construction, if \(\sigma^{*}(i)=\sigma^{*}(j)\):

\[\mathbb{P}((A_{ij},B_{\pi^{*}_{12}(i)\pi^{*}_{12}(j)},C_{\pi^{*}_{13}(i)\pi^{* }_{13}(j)})=(a,b,c)|\boldsymbol{\pi}^{\star},\sigma^{*})=\mathbb{P}(\{i,j\} \in\mathcal{E}_{abc})\times p\]

\[=\left\{\begin{array}{ll}s^{a+b+c}(1-s)^{3-a-b-c}p&\mbox{if }a+b+c>0,\\ 1-p+(1-s)^{3}p&\mbox{if }a+b+c=0.\end{array}\right.\]

If \(\sigma^{*}(i)\neq\sigma^{*}(j)\), the joint distribution is the same only with \(p\) replaced by \(q\). We can see that the joint distribution under two constructions is the same. To prove the second part of the lemma, note that conditioned on \(\boldsymbol{\pi}^{*},\sigma^{*}\), and the random partition \(\{\mathcal{E}_{ijk},i,j,k\in\{0,1\}\}\), the edges in \(\mathcal{E}_{ijk}\) form independently. Hence the graphs that are comprised of edges in disjoint \(\mathcal{E}_{ijk}\) are mutually independent.

**Definition D.3**.: _Define the constant:_

\[s_{abc}:=\left\{\begin{array}{ll}s^{3}&(a,b,c)=(1,1,1),\\ s^{2}(1-s)&(a,b,c)\in\{(0,1,1),(1,0,1),(1,1,0)\},\\ s(1-s)^{2}&(a,b,c)\in\{(0,0,1),(1,0,0),(0,1,0)\},\\ {(1-s)}^{3}&(a,b,c)=(0,0,0).\end{array}\right.\]

_The event \(\mathcal{F}\) holds if and only if_

\[n/2-n^{3/4}\leq|V^{+}|,|V^{-}|\leq n/2+n^{3/4},\]

_and the following conditions hold for all \(a,b,c\in\{0,1\},i\in[n]\):_

\[s_{abc}(|V^{\sigma^{*}(i)}|-n^{3/4})\leq|\{j:j\in\mathcal{E}_{ abc}\cap\mathcal{E}^{+}(\sigma^{*}(i))\}|\leq s_{abc}(|V^{\sigma^{*}(i)}|+n^{3/4}),\] \[s_{abc}(|V^{-\sigma^{*}(i)}|-n^{3/4})\leq|\{j:j\in\mathcal{E}_{ abc}\cap\mathcal{E}^{-}(\sigma^{*}(i))\}|\leq s_{abc}(|V^{-\sigma^{*}(i)}|+n^{3/4}).\]

**Lemma D.4**.: _Define \(s_{m}:=\min_{a,b,c\in\{0,1\}}s_{abc}\). We have \(\mathbb{P}(\mathcal{F}^{c})\leq 100n\exp(-\frac{s_{m}^{2}\sqrt{n}}{2})\)._

Proof.: Denote \(\mathcal{G}\) holds if and only if \(n/2-n^{3/4}\leq|V^{+}|,|V^{-}|\leq n/2+n^{3/4}\). The event \(\mathcal{G}\) is proved in Lemma 3.8 in [22]. \(\mathbb{P}(\mathcal{G}^{c})\leq 4e^{-\sqrt{n}}\).

Then, look at the remaining condition of event \(\mathcal{F}\). Fix \(i\in[n]\), condition on \(\sigma_{1}^{*},\pi_{12}^{*},\pi_{13}^{*}\). Note that

\[k_{abc}^{+}(i):=|\{j:j\in\mathcal{E}_{abc}\cap\mathcal{E}^{+}( \sigma_{1}^{*}(i))\}|\sim\mathrm{Bin}(|V^{\sigma^{*}(i)}|-1,s_{abc}).\]

By Hoeffding inequality we have

\[\mathbb{P}(|k_{abc}^{+}(i)-s_{abc}(|V^{\sigma^{*}(i)}|-1)|\geq \frac{s_{abc}n^{3/4}}{2}|\pi_{12}^{*},\pi_{13}^{*},\sigma_{1}^{*})1( \mathcal{G})\] \[\leq 2\exp(-\frac{s_{abc}^{2}n^{3/2}}{2|V^{\sigma^{*}(i)}|})1( \mathcal{G})\leq 2\exp(-(1-o(1))s_{abc}^{2}\sqrt{n}).\]

Then by a union bound,

\[\mathbb{P}(\exists i\in[n]:|k_{abc}^{+}(i)-s_{abc}|V^{\sigma^{*}( i)}|\|\geq s_{abc}n^{3/4})\] \[\leq\sum_{i=1}^{n}\mathbb{P}(|k_{abc}^{+}(i)-s_{abc}(|V^{\sigma^{ *}(i)}|-1)|\geq\frac{s_{abc}n^{3/4}}{2})\] \[\leq 2n\exp(-(1-o(1))s_{abc}^{2}\sqrt{n})+4\exp(-\sqrt{n})\leq 6 n\exp(-(1-o(1))s_{m}^{2}\sqrt{n}).\]

Similarly, we can define \(k_{abc}^{-}(i)\) and through an identical proof we have

\[\mathbb{P}(\exists i\in[n]:|k_{abc}^{-}(i)-s_{abc}|V^{-\sigma^{*}(i)}|\|\geq s _{abc}n^{3/4})\leq 6n\exp(-(1-o(1))s_{m}^{2}\sqrt{n}).\]

The conclusion then follows by a union bound. 

### Almost exact recovery in a single SBM

**Lemma D.5**.: _The algorithm (Algorithm 1, [22]) correctly classifies all vertices in \([n]\setminus I_{\epsilon}(G)\), where \(I_{\epsilon}(G):=\{v\in[n]:\mathbf{ma}_{jG}(v)\leq\epsilon\log n\text{ or }N(v)>100\max\{1,a,b\}\log n\}\) if \(a>b\)._

Proof.: Directly proved by [22, Lemma 5.1], adapted from [35, Proposition 4.3]. 

**Lemma D.6**.: _Consider a \(\mathrm{SBM}(n,\alpha\log n/n,\beta\log n/n)\), denote \(\gamma=\max(\alpha,\beta)\). Then for every \(\sigma^{*}\) we have_

\[\mathbb{P}(\forall i\in[n],|N(i)|\leq 100\max(1,\gamma)\log n|\sigma^{*})\geq 1 -n^{-99}.\]

Proof.: Directly proved by [22, Lemma 5.2], based on arguments of [35].

**Lemma D.7**.: _With the assumption of \(D_{+}(a,b)<99\), \(E(|I_{\epsilon}(G)|)\leq 3n^{1-D_{+}(a,b)+\epsilon}|\log(a/b)|\)._

Proof.: Proved by [22, Lemma 5.3]. 

**Lemma D.8**.: _If \(0<\epsilon<\frac{\mathrm{D}_{+}(a,b)}{2\log(a/b)}\), then_

\[\mathbb{P}(\forall i\in[n],|N(i)\cap I_{\epsilon}(G)|\leq 2\lceil\mathrm{D}_{+} (a,b)^{-1}\rceil|\sigma^{*})=1-o(1).\]

Proof.: Proved by [22, Lemma 5.4]. 

## Appendix E Analysis of the \(k\)-core estimator

In this section, we prove two important properties of \(k\)-core estimator. Lemma E.4 describes that all vertices have weak connections with those vertices who are not part of the \(k\)-core, in the logarithmic regime that we are interested in. Lemma E.1 argues that all the vertices with degree larger than a given constant will be part of the \(k\)-core.

**Lemma E.1**.: _Fix \(a,b>0\). Consider the graph \(G\sim\mathrm{SBM}(n,\frac{a\log n}{n},\frac{b\log n}{n})\). For any integer \(m\) satisfying \(m>\frac{2}{a+b}\), all vertices whose degree is greater than \(m+k\) are part of the \(k\)-core with high probability._

Proof.: For a given \(m>\frac{2}{a+b}\), we would like to prove that any vertex \(v\) with degree greater than \(k+m\) will not be part of the \(k\)-core with probability \(o(n^{-1})\). The lemma then follows by a union bound.

**Isolating vertex \(v\) for independence.**

For a fixed \(v\in[n]\), consider the graph \(\widetilde{G}:=G\{[n]\setminus v\}\). Now we look at the \(k\)-core of \(\widetilde{G}\), denote it by \(C_{k}(\widetilde{G})\). Since the \(\deg_{G}(v)>k+2/(a+b)\), we can suppose that \(\deg_{G}(v)=m+k,m>2/(a+b)\). If the vertex \(v\) is not part of the \(k\)-core of \(G\), it must has more than \(m\) neighbors who are \(\notin C_{k}(\widetilde{G})\). Note that the event \(w\notin C_{k}(\widetilde{G})\) is independent of the event \(w\in N_{G}(v)\), while the latter event is stochastically dominated by a binomial distribution with probability \(\nu\log n/n,\nu=\max(a,b)\). Hence, by the tower rule,

\[\mathbb{P}(v\text{ is not part of $k$ - core in }G) \leq\mathbb{P}(|\{w\in N_{G}(v):w\notin C_{k}(\widetilde{G})\}|>m)\] \[\leq\mathbb{E}[1_{\mathrm{Bin}(|\{w:w\notin C_{k}(\widetilde{G}) \}|,\nu\log n/n)>m}].\] (E.2)

The size of \(\{w:w\notin C_{k}(\widetilde{G})\}\) can not be directly quantified. Hence, we would like to find a set \(\overline{U}\) based on \(\widetilde{G}\) such that \(\{w\in[n]\setminus v:w\notin C_{k}(\widetilde{G})\}\subset\overline{U}\), where we can bound the size of \(\overline{U}\). Now we denote \(\mu=|\overline{U}|\nu\log n/n\), then we have that

\[\mathbb{E}[1_{\mathrm{Bin}(|\{w:w\notin C_{k}(\widetilde{G})\}|,\nu\log n/n)>m }]\leq\mathbb{E}[1_{\mathrm{Bin}(|\overline{U}|,\nu\log n/n)>m}]\leq\mathbb{E }[\min_{t>0}\exp(\mu(e^{t}-1)-tm)].\] (E.3)

To construct \(\overline{U}\), the idea is motivated by Luczak expansion in [27]. We consider a modified version of expanding the set in our setting.

**Quantify the set \(U\).**

Define \(U\) to be the set of vertices with degree at most \(T\) in the graph \(\widetilde{G}\). The choice of \(T\) would be specified later. Denote \(\mathcal{H}:=\{n/2-n^{3/4}\leq|V^{+}|,|V^{-}|\leq n/2+n^{3/4}\}\). By Lemma D.4,\(\mathbb{P}(\mathcal{H}^{c})=o(1/n)\).

\[\mathbb{E}(|U|) \leq\mathbb{E}(|U|1_{\mathcal{H}})+\mathbb{P}(\mathcal{H}^{c})n= \mathbb{E}(|U|1_{\mathcal{H}})+o(1)\] \[\leq n\sum_{i=0}^{T}\sum_{j=0}^{i}\binom{(1+o(1)\frac{n}{2})}{j} \binom{(1+o(1)\frac{n}{2})}{i-j}p^{j}(1-p)^{(1-o(1))\frac{n}{2}-j}q^{i-j}(1-q )^{(1-o(1))\frac{n}{2}-i+j}\] \[\leq 2n(1-\frac{a\log n}{n})^{(1-o(1))\frac{n}{2}}(1-\frac{b\log n }{n})^{(1-o(1))\frac{n}{2}}\sum_{i=0}^{T}\sum_{j=0}^{i}\left(\frac{(1+o(1))n}{ 2}\right)^{i}(\frac{\nu\log n}{n})^{i}\] \[\leq 2n^{1-\frac{a+2}{2}+o(1)}\sum_{i=0}^{T}(i+1)\left(\frac{(1+o( 1))\nu\log n}{2}\right)^{i}\] \[\leq 2(T+1)^{2}(\frac{\nu\log n}{2})^{T}n^{1-\frac{a+b}{2}+o(1)} =n^{1-\frac{a+b}{2}+o(1)}.\]

Consider the situation when \(1-\frac{a+b}{2}>0\). Now we claim: for any constant \(W\), \(\mathbb{E}[|U|^{W}]\leq n^{W-W\frac{a+b}{2}+o(1)}\). Suppose for \(W-1\), it is true, then for \(W\):

\[\mathbb{E}[|U|^{W}] =\sum_{i_{1},...,i_{W}}\mathbb{P}(i_{1}\in U,\ldots,i_{W}\in U)\] \[=\sum_{i_{1}\neq i_{2}...\neq i_{W}}\mathbb{P}(i_{1}\in U,\ldots, i_{W}\in U)\] \[\quad+\sum_{i_{1}\neq i_{2}...\neq i_{W-1}}\mathbb{P}(i_{1}\in U,\ldots,i_{W-1}\in U)+\ldots+\sum_{i_{1}\in[n]}\mathbb{P}(i_{1}\in U)\] \[\leq\sum_{i_{1}\neq i_{2}...\neq i_{W}}\mathbb{P}(i_{1}\in U, \ldots,i_{W}\in U)+\mathbb{E}(|U|^{W-1})\] \[\leq\sum_{i_{1}\neq i_{2}...\neq i_{W}}\mathbb{P}(i_{1}\in U, \ldots,i_{W}\in U)+\mathbb{E}[|U|^{W-1}].\]

The remaining thing is to show that \(\sum_{i_{1}\neq i_{2}...\neq i_{W}}\mathbb{P}(i_{1}\in U,\ldots,i_{W}\in U) \leq n^{W-W\frac{a+b}{2}+o(1)}\). We have

\[\sum_{i_{1}\neq i_{2}...\neq i_{W}}\mathbb{P}(i_{1}\in U,\ldots, i_{W}\in U)\] \[\leq\sum_{i_{1}\neq i_{2}...\neq i_{W}}\mathbb{P}(\cap_{j=1}^{W} \{i_{j}\text{ has at most T neighbours in }[n]\setminus i_{1},...,i_{W},v\})\] \[=\sum_{i_{1}\neq i_{2}...\neq i_{W}}\mathbb{P}(\{i_{1}\text{ has at most T neighbours in }[n]\setminus i_{1},...,i_{W},v\})^{W}\] \[\leq\mathbb{E}[|U|]^{W}\leq n^{W-W\frac{a+b}{2}+o(1)}.\]

The first inequality is because that if \(i_{1}\in U\) in \(\widetilde{G}\), then \(i_{1}\) has at most \(T\) neighbours in \([n]\setminus v\), then it implies that \(\{i_{1}\text{ has at most }T\text{ neighbours in }[n]\setminus i_{1},...,i_{W},v\}\). The second inequality is due to the independence of the events. By induction, the claim follows. By choosing appropriate \(m^{\prime}\)-th moment method of \(|U|\), we can select a \(\epsilon\) such that \(\mathbb{P}(|U|\geq n^{1-\epsilon})\leq n^{-m^{\prime}(a+b)/2+m^{\prime}\epsilon+ o(1)}=o(n^{-m^{\frac{a+b}{2}}})\). Denote \(\mathcal{D}_{0}\) as \(|U|\leq n^{1-\epsilon}\), then \(\mathbb{P}(\mathcal{D}_{0}^{c})=o(n^{-m\frac{a+b}{2}})\).

**The possibility of the existence of a well-connected small subgraph.**

Now we would like to bound the probability of the existence of a well-connected small subgraph. Define the event

\[\mathcal{D}:=\{\text{there exists }S\in[n]\text{ such that }|S|<n^{1-\epsilon}\text{ and }G\{S\}\text{ has at least }N|S|\text{ edges }\}.\]

We would like to bound \(\mathbb{P}(\mathcal{D})=o(n^{-m(a+b)/2})\). Let \(S\) be a \(\kappa\)-vertex subset of \([n]\). Let \(X_{S}\) be the indicator variable that is 1 if the subgraph induced by \(S\) has at least \(N|S|\) edges. Denote\(\nu=\max(a,b)\), then we have

\[\mathbb{E}[\sum_{S\in[n],|S|=\kappa}X_{S}] \leq\sum_{S\in[n],|S|=\kappa}\binom{\kappa}{2}\choose N\kappa}( \frac{\nu\log n}{n})^{N\kappa}\leq\sum_{S\in[n],|S|=\kappa}(\frac{\kappa\nu \log n}{Nn})^{N\kappa}\] \[\leq\binom{n}{\kappa}(\frac{\kappa e\nu\log n}{Nn})^{N\kappa}\leq ((\frac{e^{1+1/N}\nu\log n}{N})^{N}(\frac{\kappa}{n})^{N-1})^{\kappa}.\]

The second and the third inequality is because \(\binom{n}{k}\leq(\frac{en}{k})^{k}\). Under the assumption \(|S|<n^{1-\epsilon}\),

\[(\frac{e^{1+1/N}\nu\log n}{N})^{N}(\frac{\kappa}{n})^{N-1}<n^{-\epsilon(N-1)+o( 1)}.\]

Hence for \(n\) sufficiently large we have:

\[\mathbb{E}[\sum_{S\in[n],|S|<n^{1-\epsilon}}X_{S}]\leq\sum_{\kappa=1}^{n^{1- \epsilon}}(n^{-\epsilon(N-1)+o(1)})^{\kappa}\leq n^{-\epsilon(N-1)+o(1)}.\]

Then by Markov's inequality, \(\mathbb{P}(\mathcal{D})\leq n^{-\epsilon(N-1)+o(1)}\). If we want to bound \(\mathbb{P}(\mathcal{D})=o(n^{-m(a+b)/2})\), set \(N>\frac{(a+b)m}{2\epsilon}+1\). Hence,

\[\mathbb{P}(\mathcal{D})\leq\mathbb{E}[\sum_{S\in[n],|S|<n^{1-\epsilon}}X_{S}] \leq n^{-\epsilon(N-1)+o(1)}<n^{-m(a+b)/2}.\]

#### Identify the expansion set of \(U\).

Now we do the following expansion on \(U\), the expansion process is adapted from Luczak expansion first introduced in [27].

1. Define \(U_{0}:=U\).
2. Given \(U_{t}\), define \(U_{t+1}^{1}\) to be the set of those vertices outside \(U_{t}\) which have at least \(N^{\prime}\) neighbors in \(U_{t}\). If \(U_{t+1}^{1}\) is non-empty, set \(U_{t+1}=U_{t}\cup\{u\}\), where \(u\) is the first vertex in \(U_{t+1}^{1}\). Otherwise, stop the expansion with the set \(U_{t}\).

Suppose the expansion ends at the step \(h\), hence we have an increasing sequence \(\{U_{s}\}_{s=0}^{h}\). Denote \(\overline{U}:=U_{h}\) to be the set after expansion.

Now claim that on the event \(\mathcal{D}^{c}\cap\mathcal{D}_{0}\), we can choose \(N_{1},N^{\prime}>0,\) such that \(|\overline{U}|\leq N_{1}|U|\). Suppose that \(|\overline{U}|>N_{1}|U|\), then there exists \(\ell>0\) s.t. \(|U_{\ell}|=N_{1}|U|\). On event \(\mathcal{D}_{0}\), there exists \(\epsilon>0,|U|\leq n^{1-\epsilon}\), hence \(|U_{\ell}|=N_{1}|U|\leq n^{1-\epsilon+o(1)}\). Denote \(e_{l}\) as the number of edges in \(\widetilde{G}\{U_{\ell}\}\). Each step in the expanding process, at least \(N^{\prime}\) edges are added into the graph, hence \(e_{\ell}\geq N^{\prime}\ell\geq N^{\prime}(|U_{\ell}|-|U|)=(N^{\prime}-\frac{N ^{\prime}}{N^{\prime}})|U_{\ell}|\). We can choose \(N^{\prime},N_{1}\) such that \((N^{\prime}-\frac{N^{\prime}}{N_{1}})\geq N\). However on the event \(\mathcal{D}^{c}\), the set \(|U_{\ell}|<n^{1-\epsilon}\) cannot have at least \(N|U_{\ell}|\) edges, which is a contradiction. Therefore, on the event \(\mathcal{D}^{c}\cap\mathcal{D}_{0},|\overline{U}|\leq N_{1}|U|\). Subsequently,

\[\mathbb{E}[|\overline{U}|^{m}]\leq N_{1}^{m}E[|U|^{m}1_{\mathcal{D}^{c}\cap \mathcal{D}_{0}}]+n^{m}\mathbb{P}(\mathcal{D})+n^{m}\mathbb{P}(\mathcal{D}_{0})\]

#### Bound the probability of \(v\) being in the \(k\)-core.

Note that \(\widetilde{G}\{[n]\setminus\overline{U}\}\) has minimum degree at least \(T-N^{\prime}\). If a vertex \(i\in[n]\setminus\overline{U}\), then \(i\notin U\), it follows that \(i\) has degree at least \(T\) in \(\widetilde{G}\). However \(i\) can have at most \(N^{\prime}\) neighbor in \(\overline{U}\) by construction of expansion process, so \(i\) must have at least \(T-N^{\prime}\) neighbors in \([n]\setminus\overline{U}\). We can set \(T=k+N^{\prime}\), then If \(i\in[n]\setminus\overline{U}\), \(i\) is part of \(k\)-core in \(\widetilde{G}\).

Since the \(\deg(v)\geq m+k\), it must has at least \(m\) neighbors who are not part of \(k\)-core in \(\widetilde{G}\).

Follow the equation (E.2), (E.3), denote \(\mu=|\overline{U}|\nu\log n/n\), then we have

\[\mathbb{E}[1_{\operatorname{Bin}(|\overline{U}|,\nu\log n/n)>m}] \leq\mathbb{E}[\min_{t>0}\exp(\mu(e^{t}-1)-tm)]\leq\mathbb{E}[e\mu ^{m}]+o(n^{-1})\] \[=\frac{e\nu^{m}\log n^{m}}{n^{m}}\mathbb{E}[|\overline{U}|^{m}]+o( n^{-1})\leq n^{-(a+b)m/2+o(1)}+o(n^{-1})\] \[=o(n^{-1}).\]

The second inequality follows by setting \(t=\log(1/\mu)\). This is valid since \(\mathbb{P}(|U|>n^{1-c})\leq\frac{\mathbb{E}[|U|^{W}]}{n^{(1-c)W}}\leq n^{-((a+b )/2-c)W+o(1)}\). We can select \(0<c<\frac{a+b}{2},W>0\) such that \(\mathbb{P}(|U|>n^{1-c})=o(n^{-1})\). On the event \(|U|\leq n^{1-c}\), \(\mu=o(1),1/\mu>1\). The third inequality follows by \(\mathbb{E}[|\overline{U}|^{m}]\leq n^{m-m\frac{a+b}{2}+o(1)}\) and the last equality is due to \(m>\frac{2}{a+b}\).

If \(1-(a+b)/2\leq 0\), we can directly set \(m=1\). Similarly, we can prove \(\mathbb{E}[|\overline{U}|]\leq n^{1-\frac{a+b}{2}+o(1)}\), then through a similar calculation, \(\mathbb{P}(\) v is not part of \(k\)-core in \(G\}|)=o(n^{-1})\).

Hence, by a union bound, we can say that all vertices with degree larger than \(m+k\) will be part of \(k\)-core with high probability. 

**Lemma E.4**.: _Fix \(a,b,\varepsilon>0\). Let \(G\sim\operatorname{SBM}(n,\frac{a\log n}{n},\frac{b\log n}{n})\). Then, w.h.p., all vertices have at most \(\varepsilon\log n\) neighbors who are not part of the \(k\)-core._

Proof.: For \(v\in[n]\), consider the graph \(\widetilde{G}:=G\{[n]\backslash v\}\). Following the same arguments in Lemma E.1, we can obtain \(\overline{U}\). We have

\[\mathbb{P}(|\{w\in N_{G}(v):w\text{ is not part of $k$-core in $G$}\}|>\epsilon\log n)\] \[\leq \mathbb{P}(|\{w\in N_{G}(v):w\notin C_{k}(\widetilde{G})\}|> \epsilon\log n)\] \[\leq \mathbb{P}(|\{w\in N_{G}(v):w\notin C_{k}(\widetilde{G})\}|>m) \leq o(n^{-1}).\]

Based on the proof of Lemma E.1, we can show the lemma follows immediately when \(n\) is sufficiently large. Hence, all vertices have at most \(\epsilon\log n\) neighbors who are not part of the \(k\)-core with probability \(1-o(1)\). 

**Lemma E.5**.: _Fix constants \(a,b>0,s\in[0,1]\). Let \((G_{1},G_{2})\sim\operatorname{CSBM}(n,a\frac{\log n}{n},b\frac{\log n}{n},s)\). Let \(M^{*}\) be the set of vertices of the 13-core in the graph \(G_{1}\wedge_{\pi^{*}}G_{2}\). \(\pi^{*}\) is the permutation of vertices from \(G_{1}\) to \(G_{2}\). Let \((M_{1},\mu_{1})\) be the output of the \(k\)-core match of \((G_{1},G_{2}),k=13\). Then \(\mathbb{P}((M_{1},\mu_{1})=(M^{*},\pi^{*}))=1-o(1)\)._

**Remark:** We can replace \((M_{k},\mu_{k})\) by \((M_{k}^{*},\pi_{k}^{*}\{M^{*}\})\) in any analysis.

Proof.: Proved by [22, Lemma 4.8]. 

**Lemma E.6**.: _Let \(G\sim\operatorname{SBM}(n,a\frac{\log n}{n},b\frac{\log n}{n})\) for fixed \(a,b>0\). Fix \(k\geq 1\). With probability \(1-o(1)\), we have that \(|F|\leq n^{1-\operatorname{T_{c}}(a,b)+o(1)}\)._

Proof.: Proved by [22, Lemma 4.13]. 

**Lemma E.7**.: _Suppose \(G_{1},G_{2},G_{3}\) are independently subsampled with probability \(s\) from a parent graph \(G\sim\operatorname{SBM}(n,a\log n/n,b\log n/n)\) for \(a,b>0\). Let \(F_{ij}^{*}\) be the set of vertices outside the \(k\)-core of \(G_{i}\wedge_{\pi_{ij}}G_{j}\)(taking vertex index in i) with \(k=13\). Prove that with probability \(1-o(1)\), \(|F_{ij}^{*}\cap F_{jk}^{*}|\leq n^{1-(2s^{2}-s^{3})\operatorname{T_{c}}(a,b)+ \delta}\), for any \(\delta>0\)._

Proof.: \(|F_{12}^{*}\cap F_{23}^{*}|=|F_{21}^{*}\cap F_{23}^{*}|=|F_{12}^{*}\cap F_{13 }^{*}|\), by symmetricity of \(G_{1},G_{2},G_{3}\).

Define \(U_{ij}\) to be the set of vertices with degree at most \(m+k\) in the intersection graph \(G_{i}\wedge_{\pi_{ij}}G_{j}\) which marginally follows \(\operatorname{SBM}(n,\frac{as^{2}\log n}{n},\frac{bs^{2}\log n}{n})\), and \(m\) is an integer satisfying \(m>\frac{2}{s^{2}(a+b)}\). Then by Lemma E.1, w.h.p., \(F_{ij}^{*}\subset U_{ij}\). Hence \(|F_{12}^{*}\cap F_{23}^{*}|\leq|U_{12}\cap U_{23}|\) with high probability.

It thus remains to bound \(|U_{12}\cap U_{13}|\). Firstly, we bound the expectation of \(|U_{12}\cap U_{13}|\):

\[\mathbb{E}[|U_{12}\cap U_{13}|]=\sum_{v=1}^{n}\mathbb{E}[\mathbf{1}_{v\in U_{12 }\cap U_{13}}]=n\mathbb{E}[\mathbf{1}_{v\in U_{12}}\mathbf{1}_{v\in U_{13}}].\]

Let \(D_{1}\) denote the degree of vertex \(v\) in the graph \(G_{1}\). Let \(X_{a}\sim\mathrm{Bin}((1+o(1))n/2,sa\log n/n)\), and \(X_{b}\sim\mathrm{Bin}((1+o(1))n/2,sb\log n/n)\). On the event \(\mathcal{F},D_{1}\overset{d}{=}X_{a}+X_{b}\), where \(\mathcal{F}\) is defined in Definition D.3, \(X_{a},X_{b}\) are independent. Note that by Lemma D.4, \(\mathbb{P}(\mathcal{F}^{c})=o(\frac{1}{n^{2}})\). We have

\[\mathbb{E}[\mathbf{1}_{v\in U_{12}}\mathbf{1}_{v\in U_{13}}] =\mathbb{E}[\mathbb{E}[\mathbf{1}_{v\in U_{12}}\mathbf{1}_{v\in U _{13}}|D_{1}]]=\mathbb{E}\left[\binom{m+k}{i=0}\binom{D_{1}}{i}s^{i}(1-s)^{D_ {1}-i}\right]^{2}\] \[=\sum_{i=0}^{m+k}\sum_{j=0}^{m+k}C(i,j)\mathbb{E}[D_{1}^{i+j}(1-s) ^{2D_{1}}].\] (E.8)

Here \(C(i,j)\) is a constant related to \(i,j\). Now look at \(\mathbb{E}[D_{1}^{L}(1-s)^{2D_{1}}]\). In our regime, \(L\leq 2(m+k)\) are constant. Hence:

\[\mathbb{E}[D_{1}^{L}(1-s)^{2D_{1}}\mathbf{1}_{\mathcal{F}}] =\mathbb{E}[(X_{a}+X_{b})^{L}(1-s)^{2X_{a}}(1-s)^{2X_{b}}\mathbf{ 1}_{\mathcal{F}}]\] \[=\sum_{t=0}^{K}C_{t}\mathbb{E}[X_{a}^{t}(1-s)^{2X_{a}}\mathbf{1} _{\mathcal{F}}]\mathbb{E}[X_{b}^{L-t}(1-s)^{2X_{a}}\mathbf{1}_{\mathcal{F}}].\] (E.9)

Here \(C_{t}\) is constant related to \(t\), the second equality is due to the independence of \(X_{a},X_{b}\). Now look at \(\mathbb{E}[X_{a}^{t}(1-s)^{2X_{a}}\mathbf{1}_{\mathcal{F}}]\).

\[\mathbb{E}[X_{a}^{t}(1-s)^{2X_{a}}\mathbf{1}_{\mathcal{F}}] \leq\mathbb{E}[X_{a}^{t}(1-s)^{2X_{a}}]\] \[=\sum_{\ell=0}^{(1+o(1))n/2}\ell^{\ell}(1-s)^{2\ell}(\frac{sa \log n}{n})^{\ell}(1-\frac{sa\log n}{n})^{(1+o(1))n/2-\ell}\] \[=\sum_{\ell=0}^{(\log n)^{3}}\ell^{\ell}(\frac{(1-s)^{2}sa\log n }{n})^{\ell}(1-\frac{sa\log n}{n})^{(1+o(1))n/2-\ell}\] \[\quad+\sum_{\ell=(\log n)^{3}+1}^{(1+o(1))n/2}\ell^{\ell}(\frac{( 1-s)^{2}sa\log n}{n})^{\ell}(1-\frac{sa\log n}{n})^{(1+o(1))n/2-\ell}.\]

We can bound the first part:

\[\sum_{\ell=0}^{(\log n)^{3}}\ell^{\ell}(\frac{(1-s)^{2}sa\log n }{n})^{\ell}(1-\frac{sa\log n}{n})^{(1+o(1))n/2-\ell}\] \[\leq (\log n)^{3t}\sum_{\ell=0}^{(\log n)^{3}}(\frac{(1-s)^{2}sa\log n }{n})^{\ell}(1-\frac{sa\log n}{n})^{(1+o(1))n/2-\ell}\] \[\leq (\log n)^{3t}\sum_{\ell=0}^{(1+o(1))n/2-\ell}(\frac{(1-s)^{2}sa \log n}{n})^{\ell}(1-\frac{sa\log n}{n})^{(1+o(1))n/2-\ell}\] \[= (\log n)^{3t}(1-(1-(1-s)^{2})\frac{sa\log n}{n})^{(1+o(1))n/2} \leq n^{-(1-(1-s)^{2})sa/2+o(1)}\]

Then we can bound the second part, note that

\[\sum_{\ell=(\log n)^{3}+1}^{(1+o(1))n/2}\ell^{\ell}(\frac{(1-s)^{ 2}sa\log n}{n})^{\ell}(1-\frac{sa\log n}{n})^{(1+o(1))n/2-\ell}\] \[=\sum_{\ell=(\log n)^{3}+1}^{(1+o(1))n/2}\ell^{\ell}(1-s)^{2\ell} \mathbb{P}(X_{a}=\ell)\leq(\log n)^{3t}(1-s)^{2(\log n)^{3}}\mathbb{P}(X_{a}>( \log n)^{3})\] \[\leq n^{2(\log n)^{2}\log(1-s)+o(1)}=o(n^{-(1-(1-s)^{2})sa/2+o(1)}).\]The first inequality is true because \(\ell^{t}(1-s)^{2\ell}\) decreases when \(\ell>(\log n)^{3}\) for sufficiently large \(n\). The last equality is true because \((\log n)^{2}\log(1-s)<-(1-(1-s)^{2})sa/2\) for sufficiently large \(n\).

Hence, by summing up the two parts, \(\mathbb{E}[X_{t}^{t}(1-s)^{2X_{a}}]\leq n^{-s(1-(1-s)^{2})a/2+o(1)}\), similarly we can proof \(\mathbb{E}[X_{b}^{L-t}(1-s)^{2X_{b}}]\leq n^{-s(1-(1-s)^{2})b/2+o(1)}\). By (E.8) and (E.9),

\[\mathbb{E}[\mathbf{1}_{v\in U_{12}}\mathbf{1}_{v\in U_{13}}]\leq\mathbb{P}( \mathcal{F})^{c}+n^{-s(1-(1-s)^{2})\mathrm{T}_{c}(a,b)+o(1)}=n^{-s(1-(1-s)^{2 })\mathrm{T}_{c}(a,b)+o(1)}+o(\frac{1}{n^{2}}).\]

By Markov's inequality we have

\[\mathbb{P}(|U_{12}\cap U_{13}|\geq\log n\mathbb{E}[|U_{12}\cap U_{13}|])\leq \frac{1}{\log n}=o(1).\]

Hence we have \(|F_{12}^{*}\cap F_{23}^{*}|\leq|U_{12}\cap U_{13}|\leq\log n\mathbb{E}[|U_{12 }\cap U_{13}|]\leq n^{1-s(1-(1-s)^{2})\mathrm{T}_{c}(a,b)+\delta}\), for any \(\delta>0\), with probability \(1-o(1)\). 

## Appendix F Proof of Theorem 1 for three graphs

Exact recovery in \([n]\setminus(F_{12}\cup F_{23})\cup[n]\setminus(F_{12}\cup F_{13})\cup[n] \setminus(F_{13}\cup F_{23})\)

**Definition F.1**.: _For a vertex \(i\) in G, define the quantity majority of \(i\):_

\[\mathsf{maj}_{G}(i):=|N_{G}(i)\cap V^{\sigma^{*}(i)}|-|N_{G}(i)\cap V^{-\sigma ^{*}(i)}|.\]

By Lemma D.1, we can directly deduce that if \((1-(1-s)^{3})D_{+}(a,b)>1+\epsilon|\log(a/b)|\), then for all \(i\in[n]\) we have that \(\mathsf{maj}_{G_{1}\lor_{v_{12}}G_{2}\lor_{v_{13}}G_{3}}(i)\geq\epsilon\log n\) with probability \(1-o(1)\).

### Exact recovery in \([n]\setminus(F_{12}\cup F_{23})\)

Now suppose that \(i\in M_{12}\cap M_{23}\). Look at \(\widetilde{G}:=(G_{1}\vee_{\mu_{12}}G_{2}\vee_{\mu_{23}o\mu_{12}}G_{3})([n] \setminus\{F_{12}\cup F_{23}\})\).

**Lemma F.2**.: _Suppose that \((1-(1-s)^{3})D_{+}(a,b)>1+2\epsilon|\log(a/b)|\). Then with probability \(1-o(1)\), all vertices in \((M_{12}\cap M_{23})\) have an \(\epsilon\log n\) majority in \(\widetilde{G}\{M_{12}\cap M_{23}\}\)._

Proof.: Denote \(F_{ij}^{*}\) the set of vertices outside the 13-core of \(G_{i}\wedge_{\pi_{ij}^{*}}G_{j}\). In light of Lemma E.5 and its remark, we can replace \(\mu_{ij}\) with \(\pi_{ij}^{*},F_{ij}\) with \(F_{ij}^{*}\) in Lemma F.2. Where we define

\[G^{*}:=G_{1}\vee_{\pi_{12}^{*}}G_{2}\vee_{\pi_{13}^{*}}G_{3},\]

\[H:=G^{*}\{M_{12}^{*}\cap M_{23}^{*}\}.\]

To bound the neighborhood majority in \(H\), for \(i\in M_{12}^{*}\cap M_{23}^{*}\) note that:

\[\mathsf{maj}_{H}(i) =\sigma^{*}(i)\sum_{j\in N_{H}(i)}\sigma^{*}(j)\leq\mathsf{maj}_{ G^{*}}(i)+|N_{G^{*}}(i)\cap\{F_{12}^{*}\cup F_{23}^{*}\}|,\] \[\mathsf{maj}_{H}(i) =\sigma^{*}(i)\sum_{j\in N_{H}(i)}\sigma^{*}(j)\geq\mathsf{maj}_{ G^{*}}(i)-|N_{G^{*}}(i)\cap\{F_{12}^{*}\cup F_{23}^{*}\}|.\]

To sum up, we have

\[|\mathsf{maj}_{H}(i)-\mathsf{maj}_{G^{*}}(i)|\leq|N_{G^{*}}(i)\cap\{F_{12}^{* }\cup F_{23}^{*}\}|.\] (F.3)

Note that \(\mathsf{maj}_{G^{*}}(i)>2\epsilon\log n,i\in[n]\) with probability \(1-o(1)\), given that \((1-(1-s)^{3})D_{+}(a,b)>1+2\epsilon|\log(a/b)|\) by Lemma D.1. Now we prove that the right hand side of (F.3) can be bounded by \(\epsilon\log n\). Look at \(|N_{G^{*}}(i)\cap\{F_{12}^{*}\cup F_{23}^{*}\}|\),

\[|N_{G^{*}}(i)\cap\{F_{12}^{*}\cup F_{23}^{*}\}| \leq|N_{G_{1}\wedge G_{2}}(i)\cap(F_{12}^{*})|+|N_{G_{2}\wedge G_{3 }}(\pi_{12}^{*}(i))\cap(F_{23}^{*})|\] \[+|N_{G^{*}\setminus(G_{1}\wedge G_{2})}(i)\cap(F_{12}^{*})|+|N_{G ^{*}\setminus(G_{2}\wedge G_{3})}(i)\cap(F_{23}^{*})|.\]

First, look at \(|N_{G_{1}\wedge G_{2}}(i)\cap F_{12}^{*}|\), by Lemma E.4, w.h.p.,

\[|N_{G_{1}\wedge G_{2}}(i)\cap F_{12}^{*}|<\epsilon\log n/8.\]Similarly, we have w.h.p.

\[|N_{G_{2}\wedge G_{3}}(\pi_{12}^{*}(i)\cap F_{23}^{*})|<\epsilon\log n/8.\]

What is left is to bound \(|N_{G^{*}\setminus(G_{2}\wedge G_{3})}(i)\cap(F_{23}^{*})|,|N_{G^{*}\setminus(G_ {1}\wedge G_{2})}(i)\cap(F_{12}^{*})|\).

Note that conditioned on \(\pi_{12}^{*},\pi_{13}^{*},\pi_{23}^{*},\sigma^{*},\boldsymbol{\mathcal{E}}:= \{\xi_{ijk},i,j,k\in\{0,1\}\}\), the graph \(G^{*}\setminus(G_{1}\wedge G_{2})\) is independent of \(F_{12}^{*}\) by Lemma D.2, since \(F_{12}^{*}\) depends only on \(G_{1}\wedge G_{2}\). Thus we can stochastically dominate \(|N_{G^{*}\setminus(G_{1}\wedge G_{2})}(i)\cap F_{12}^{*}|\) by a Poisson random variable \(\mathbf{X}\) with mean

\[\lambda_{n}:=\nu\frac{\log n}{n}|\{j\in F_{12}^{*}:\{i,j\}\in\mathcal{E}_{100 }\cup\mathcal{E}_{101}\cup\mathcal{E}_{001}\cup\mathcal{E}_{010}\cup \mathcal{E}_{011}\}|\leq\nu\frac{\log n}{n}|F_{12}^{*}|,\nu:=max(a,b).\]

For a fixed \(\delta>0\), define an event \(\mathcal{Z}:=\{|F_{12}^{*}|\leq n^{1-s^{2}\mathrm{T}_{\mathrm{c}}(a,b)+\delta }\}\). On \(\mathcal{Z}\), we have that \(\lambda_{n}\leq n^{-s^{2}\mathrm{T}_{\mathrm{c}}(a,b)+\delta+o(1)}\). Hence, for any positive integer m:

\[\mathbb{P}(\{|N_{G^{*}\setminus(G_{1}\wedge G_{2})}(i)\cap(F_{12 }^{*})|\geq m\}\cap\mathcal{Z})\leq\mathbb{P}(\{X\geq m\}\cap\mathcal{Z})= \mathbb{E}[\mathbb{P}(X\geq m|F_{12}^{*},\boldsymbol{\mathcal{E}},\sigma^{*}, \boldsymbol{\pi^{*}})\mathbf{1}_{\mathcal{Z}}]\\ \leq\mathbb{E}[(\inf_{\theta>0}e^{-\theta m+\lambda_{n}(e^{ \theta}-1)})\mathbf{1}_{\mathcal{Z}}]\leq\mathbb{E}[e\lambda_{n}^{m}\mathbf{1 }_{\mathcal{Z}}]\leq n^{-m(s^{2}\mathrm{T}_{\mathrm{c}}(a,b)-\delta-o(1))}.\]

Above, the equality on the second line is due to the tower rule and since \(\mathcal{Z}\) is measurable with respect to \(|F_{12}^{*}|\), the inequality on the third line is due to a Chernoff bound; the inequality on the fourth line follows from setting \(\theta=\log(1/\lambda_{n})\) (which is valid since \(\lambda_{n}=o(1)\) if \(\mathcal{Z}\) holds). The final inequality uses the upper bound for \(\lambda_{n}\) on \(\mathcal{Z}\). Taking a union bound, we have

\[\mathbb{P}(\{\exists i\in[n],|N_{G^{*}\setminus(G_{2}\wedge G_{1})}(i)\cap F_ {12}^{*}|\geq m\}\cap\mathcal{Z})\leq n^{1-m(s^{2}\mathrm{T}_{\mathrm{c}}(a,b )-\delta-o(1))}.\]

Here if we take \(m>(s^{2}\mathrm{T}_{\mathrm{c}}(a,b))^{-1}\) and \(\delta<s^{2}\mathrm{T}_{\mathrm{c}}(a,b)-m^{-1}\), the probability turns to \(o(1)\). Thus, we can set \(m=\lceil(s^{2}\mathrm{T}_{\mathrm{c}}(a,b))^{-1}\rceil+1\). In light of Lemma E.6, \(|F_{12}^{*}|\leq n^{1-s^{2}\mathrm{T}_{\mathrm{c}}(a,b)+\delta},\delta>0\) w.h.p. Hence, the event \(\mathcal{Z}\) happens with probability \(1-o(1)\). Hence we have

\[\mathbb{P}(\{\forall i\in[n],N_{G^{*}\setminus(G_{2}\wedge G_{1})}(i)\cap F_ {12}^{*}|\leq\lceil(s^{2}\mathrm{T}_{\mathrm{c}}(a,b))^{-1}\rceil\})=1-o(1).\]

By an identical proof, we have that

\[\mathbb{P}(\{\forall i\in[n],N_{G^{*}\setminus(G_{2}\wedge G_{3})}(i)\cap F_ {23}^{*}|\leq\lceil(s^{2}\mathrm{T}_{\mathrm{c}}(a,b))^{-1}\rceil\})=1-o(1).\]

Hence we have, with probability \(1-o(1)\), for \(i\in M_{12}^{*}\cap M_{23}^{*}\),

\[|\mathsf{maj}_{H}(i)-\mathsf{maj}_{G_{1}\vee_{i12}^{*}G_{2}\vee_{i13}^{*}G_{ 3}}(i)|<\epsilon\log n,\]

and hence with probability \(1-o(1)\),

\[\mathsf{maj}_{H}(i)>\epsilon\log n.\]

Then by Lemma E.5, we can replace \(H\) with \(\widetilde{G}\), \(F_{ij}^{*}\) with \(F_{ij}\), the lemma follows. 

Next, prove that each vertex in \(G_{2}\vee_{\pi_{23}^{*}}G_{3}\setminus_{\pi_{12}^{*}}G_{1}\) has a small number of neighbors in \(\pi_{12}^{*}(I_{\epsilon}(G_{1}))\)

**Lemma F.4**.: _If \(0<\epsilon\leq\frac{s\mathrm{D}_{+}(a,b)}{4\left\lvert\log(a/b)\right\rvert}\), then_

\[\mathbb{P}(\forall i\in[n],|N_{G_{2}\vee_{\pi_{23}^{*}}G_{3}\setminus_{\pi_{12 }^{*}}G_{1}}(i)\cap\pi_{12}^{*}(I_{\epsilon}(G_{1}))|\leq 2\lceil(s \mathrm{D}_{+}(a,b))^{-1}\rceil)=1-o(1).\]

Proof.: Since \(I_{\epsilon}(G_{1})\) depends on \(G_{1}\) alone, it follows that \(I_{\epsilon}(G_{1})\) and \(G_{2}\vee_{\pi_{23}^{*}}G_{3}\setminus_{\pi_{12}^{*}}G_{1}\) are conditionally independent given \(\boldsymbol{\pi^{*}},\sigma^{*},\boldsymbol{\mathcal{E}}\). Hence we can stochastically dominate \(|N_{G_{2}\vee_{\pi_{23}^{*}}G_{3}\setminus_{\pi_{12}^{*}}G_{1}}(i)\cap\pi_{12} ^{*}(I_{\epsilon}(G_{1}))|\) by a Poisson random variable \(\mathbf{X}\) with mean \(\lambda_{n}\) given by

\[\lambda_{n}:=\nu\log n/n|\{j\in I_{\epsilon}(G_{1}):\{i,j\}\in\mathcal{E}_{01 1}\cup\mathcal{E}_{010}\cup\mathcal{E}_{001}\}|\leq\nu\log n/n|I_{\epsilon}(G _{1})|.\]

Next, define the event \(\mathcal{Z}:=\{|I_{\epsilon}(G_{1})|\leq n^{1-s\mathrm{D}_{+}(a,b)+2\epsilon| \log(a/b)|}\}\).

Notice that \(P(\mathcal{Z})=1-o(1)\) by Lemma D.7 and Markov's inequality, provided \(s\mathrm{D}_{+}(a,b)<99\). Following identical arguments as the proof of Lemma F.2, we arrive at

\[\mathbb{P}(\exists i\in[n],|N_{G_{2}\vee_{\pi_{23}^{*}}G_{3}\setminus_{\pi_{12}^ {*}}G_{1}}(i)\cap\pi_{12}^{*}(I_{\epsilon}(G_{1}))|\geq m)=o(1),\]

when \(m>\lceil(s\mathrm{D}_{+}(a,b)-2\epsilon|\log a/b|)^{-1}\rceil\). If \(\epsilon\leq\frac{s\mathrm{D}_{+}(a,b)}{4\left\lvert\log(a/b)\right\rvert}\), it suffices to set \(m=2\lceil(s\mathrm{D}_{+}(a,b))^{-1}\rceil+1\)

**Lemma F.5**.: _Suppose that \(a,b,\epsilon>0\) satisfy the following conditions:_

\[(1-(1-s)^{3})\mathrm{D}_{+}(a,b)>1+2\epsilon|\log a/b|,\qquad 0<\epsilon\leq \frac{s\mathrm{D}_{+}(a,b)}{4|\log a/b|}.\]

_With high probability, the algorithm correctly labels all vertices in \(\{i\in[n]\setminus(F_{12}^{*}\cup F_{23}^{*})\}\)._

Proof.: Compare the neighborhood majority in \(H\) corresponding to \(\widehat{\sigma}_{1}\) with the true majority in \(H\), where \(H\) is defined in Lemma F.2:

\[|\sigma^{*}(i)\sum_{j\in N_{H}(i)}(\widehat{\sigma}_{1}(j)-\sigma ^{*}(j))| \leq|N_{H}(i)\cap I_{\epsilon}(G_{1})|\leq|N_{G^{*}}(i)\cap I_{ \epsilon}(G_{1})|\] \[\leq|N_{G_{2}\vee_{\epsilon_{23}^{*}}G_{3}\setminus\star_{12}^{ *}G_{1}}(i)\cap\pi_{12}^{*}(I_{\epsilon}(G_{1}))|+|N_{G_{1}}(i)\cap I_{ \epsilon}(G_{1})|\] \[\leq 2\lceil\mathrm{D}_{+}(a,b)^{-1}\rceil+2\lceil(s \mathrm{D}_{+}(a,b))^{-1}\rceil\leq\epsilon\log n/2.\]

The first inequality uses Lemma D.5 that the set of errors are contained in \(I_{\epsilon}(G_{1})\). The last inequality is due to Lemma D.8, F.4. Notice that \(\mathsf{maj}_{H}(i)\geq\epsilon\log n\) for \(i\in[n]\setminus(F_{12}^{*}\cup F_{23}^{*})\). Hence, \(\sigma^{*}(i)\sum_{j\in N_{H}(i)}\widehat{\sigma}_{1}(j)\geq\mathsf{maj}_{H} (i)-|\sigma^{*}(i)\sum_{j\in N_{H}(i)}(\widehat{\sigma}_{1}(j)-\sigma^{*}(j)) |\geq\epsilon\log n/2>0\), which implies that the sign of neighborhood majorities are equal to the truth community label for any \(i\in[n]\setminus(F_{12}^{*}\cup F_{23}^{*})\), with probability \(1-o(1)\). Then we can convert \(H\) to \(\widetilde{G}\{[n]\setminus(F_{12}\cup F_{23})\}\), the vertices in \([n]\setminus(F_{12}\cup F_{23})\) are correctly labeled with probability \(1-o(1)\).

Using an identical proof, we can argue that the algorithm correctly label all vertices in \(M_{13}\cap M_{32}\) and \(M_{12}\cap M_{13}\). 

Exact recovery in \([n]\setminus\{(M_{13}\cap M_{32})\cup(M_{12}\cap M_{13})\cup(M_{23}\cap M_{12 })\}\)

Define \(M=(M_{13}\cap M_{32})\cup(M_{12}\cap M_{13})\cup(M_{23}\cap M_{12})\). Denote \(F_{b}=(F_{12}\cap F_{13})\cup(F_{12}\cap F_{23}\setminus F_{13})\cup(F_{13} \cap F_{23}\setminus F_{12})\), note that \(F_{b}=[n]\setminus M\).

**Lemma F.6**.: _Suppose that \(a,b,\epsilon>0\) satisfy the following conditions:_

\[(1-(1-s)^{3})\mathrm{D}_{+}(a,b)>1+2\epsilon|\log a/b|,\qquad 0< \epsilon\leq\frac{s\mathrm{D}_{+}(a,b)}{4|\log a/b|},\] \[s(1-(1-s)^{2})\mathrm{T}_{\mathrm{c}}(a,b)+s(1-s)^{2}\mathrm{D}_ {+}(a,b)>1.\]

_With high probability, the algorithm correctly labels all vertices that are in \(F_{b}\)._

Proof.: For \(i\in F_{b}\), define \(H_{i}:=(G_{1}\setminus_{\pi_{13}^{*}}G_{3}\setminus_{\pi_{12}^{*}}G_{2})\{(M _{12}\cap M_{13})\cup\{i\}\}\). Let \(E_{i}\) be the event that i has a majority of at most \(\epsilon^{\prime}\log n\) in the graph \(H_{i}\). Let \(\widehat{\sigma}\) be the labeling after the step. For brevity, define a "nice" event based on the previous results. Define the event \(\mathcal{H}\), which holds if and only if:

* \(F_{ij}=F_{ij}^{*}\);
* \(\widehat{H_{i}}=H_{i}\);
* \(\widehat{\sigma}(i)=\sigma^{*}(i)\) for all \(i\in M_{12}\cap M_{13}\);
* The event \(\mathcal{F}\) holds;
* \(|F_{b}|\leq n^{1-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+\delta}\).

By Lemmas E.5, E.7, D.4, F.5, the event \(\mathcal{H}\) holds with probability \(1-o(1)\). Furthermore, define \(E_{i}^{*}:=\mathsf{maj}_{H_{i}}(i)\leq\epsilon^{\prime}\log n\), we have:

\[\mathbb{P}(\cup_{i\in[n]}(\{i\in F_{b}\}\cap E_{i})) \leq\mathbb{P}((\cup_{i\in[n]}(\{i\in F_{b}^{*}\}\cap E_{i}^{*})) \cap\mathcal{H})+\mathbb{P}(\mathcal{H}^{c})\] \[\leq\sum_{i=1}^{n}\mathbb{P}(\{i\in F_{b}^{*}\}\cap E_{i}^{*} \cap\{F_{b}^{*}\leq n^{1-(2s^{2}-3s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+\delta} \}\cap\mathcal{F})+o(1).\] (F.7)By the tower rule, rewrite the term in the right hand side as:

\[\mathbb{E}[\mathbb{P}(E_{i}^{*}|\pi^{*},\sigma^{*},\bm{\mathcal{E}},F_{b}^{*}) \mathbf{1}_{i\in F_{b}^{*}}\mathbf{1}_{\{|F_{b}^{*}|\leq n^{1-(2s^{2}-s^{3}) \mathrm{T}_{\mathrm{c}}(a,b)+\delta}\}\cap\mathcal{F}}].\] (F.8)

Now look at \(\mathbb{P}(E_{i}^{*}|\pi^{*},\sigma^{*},\bm{\mathcal{E}},F_{b}^{*})\). Conditional on \(\bm{\mathcal{E}},\sigma^{*},\pi^{*}\), \(\mathsf{maj}_{H_{i}}(i):=\frac{d}{Z}-Z\), where \(Y,Z\) are independent with:

\[Y\sim Bin(|j\in M_{12}^{*}\cup M_{13}^{*}:\{i,j\}\in\mathcal{E}_{100}\cap \mathcal{E}^{+}(\sigma^{*})|,a\log n/n),\]

\[Z\sim Bin(|j\in M_{12}^{*}\cup M_{13}^{*}:\{i,j\}\in\mathcal{E}_{100}\cap \mathcal{E}^{-}(\sigma^{*})|,b\log n/n).\]

By Definition D.3 of the event \(\mathcal{F}\), we know that \(|j\in M_{12}^{*}\cup M_{13}^{*}:\{i,j\}\in\mathcal{E}_{100}\cap\mathcal{E}^{- }(\sigma^{*})|=(1-o(1))s(1-s)^{2}n/2\) and \(|j\in M_{12}^{*}\cup M_{13}^{*}:\{i,j\}\in\mathcal{E}_{100}\cap\mathcal{E}^{+ }(\sigma^{*})|=(1-o(1))s(1-s)^{2}n/2\).

Lemma D.1 implies

\[\mathbb{P}(E_{i}^{*}|\pi^{*},\sigma^{*},\bm{\mathcal{E}},F_{b}^{*})\mathbf{1} _{i\in F_{b}^{*}}\mathbf{1}_{\{|F_{b}^{*}|\leq n^{1-(2s^{2}-s^{3})\mathrm{T}_{ \mathrm{c}}(a,b)+\delta}\}\cap\mathcal{F}}\leq n^{-s(1-s)^{2}\mathrm{D}_{+}(a, b)+\epsilon^{\prime}\log(a/b)/2+o(1)}.\]

Follow (F.8) and take a union bound, we have

\[\sum_{i=1}^{n}\mathbb{P}(\{i\in F_{b}^{*}\}\cap E_{i}^{*}\cap\{F _{b}^{*}\leq n^{1-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+\delta}\}\cap \mathcal{F})+o(1)\] \[\leq n^{-s(1-s)^{2}\mathrm{D}_{+}(a,b)+\epsilon^{\prime}\log(a/b)/2+o (1)}\mathbb{E}[|F_{b}^{*}|\mathbf{1}_{F_{b}^{*}\leq n^{1-(2s^{2}-s^{3}) \mathrm{T}_{\mathrm{c}}(a,b)+\delta}}]\] \[\leq n^{1-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)-s(1-s)^{2} \mathrm{D}_{+}(a,b)+\epsilon^{\prime}\log(a/b)/2+\delta+o(1)}.\]

Under the condition \((2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+s(1-s)^{2}\mathrm{D}_{+}(a,b)>1\), we can choose \(\epsilon^{\prime},\delta\) small enough so that the right hand side is \(o(1)\). \(\mathsf{maj}_{H_{i}}(i)>\epsilon^{\prime}\log n\) for \(i\in F_{b}^{*}\), by Lemma E.5, \(\mathsf{maj}_{\widetilde{H_{i}}}(i)>\epsilon^{\prime}\log n\) for \(i\in F_{b}\).

Suppose that \(i\in F_{13}\cap F_{12}\), i has at most 12 neighbors in the graph \((G_{1}\wedge_{\pi_{12}^{*}}G_{2})\{(M_{12}\cap M_{13})\cup\{i\}\}\), and in the graph \((G_{1}\wedge_{\pi_{13}^{*}}G_{3})\{(M_{12}\cap M_{13})\cup\{i\}\}\). Therefore, i has an at least \((\epsilon^{\prime}\log n-24)\) majority in \(G_{1}\{(M_{12}\cap M_{13})\cup\{i\}\}\), with high probability. Then, Algorithm 3 correctly label all vertices in \(F_{13}\cap F_{12}\).

Suppose that \(i\in F_{12}\cap F_{23}\setminus F_{13}\), i has at most 12 neighbors in the graph \((G_{1}\wedge_{\pi_{12}^{*}}G_{2})\{(M_{12}\cap M_{13})\cup\{i\}\}\) Therefore, i has an at least \((\epsilon^{\prime}\log n-12)\) majority in \(G_{1}\setminus_{\pi_{13}^{*}}G_{3}\{(M_{12}\cap M_{13})\cup\{i\}\}\), with high probability. Algorithm 3 correctly label all vertices in \(F_{13}\cap F_{23}\setminus F_{12}\). 

## Appendix G Proof of impossibility for three graphs

In this section we prove that Theorem 2 when the exact community recovery is impossible. The impossibility under the condition \((1-(1-s)^{3})\mathrm{D}_{+}(a,b)<1\) has been proved in [41]. Hence we focus on proving impossibility when

\[(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+s(1-s)^{2}\mathrm{D}_{+}(a,b)<1.\] (G.1)

To prove it, we study the MAP (maximum a posterior) estimator for the communities in \(G_{1}\). Even with the additional information provided, including all the correct community labels in \(G_{2}\), the true matching \(\pi_{23}^{*}\) and most of the true matching \(\pi_{12}^{*}\), the MAP estimator fails to exactly recovery communities with probability bounded away form 0 if the condition (G.1) holds. The proof is adapted from the MAP analysis in [22]. The difference is that we are considering three correlated SBM \(G_{1},G_{2},G_{3}\). Since we know the true matching \(\pi_{23}^{*}\), we can consider \(H:=G_{2}\vee_{\pi_{23}^{*}}G_{3}\sim\mathrm{SBM}(n,(1-(1-s)^{2})a\log n/n,(1-(1-s )^{2})b\log n/n)\). Denote \(R_{ij}\) the singleton in \(G_{i}\wedge G_{j}\). Then \(R=R_{12}\wedge R_{13}\) is the singleton set in \(G_{1}\wedge H\).

### Notation

Here we review and introduce some notations in brief.

\(\sigma_{i}^{*}:=\) the ground truth community labels in \(G_{i},i=1,2,3\),

\(V_{i}^{+}:=\{j\in[n]:\sigma_{i}^{*}(j)=+1\},V_{i}^{-}:=\{j\in[n]:\sigma_{i}^{*}(j )=-1\},\)

\(\sigma_{2}^{i}(\pi_{12}^{*}(i))=\sigma_{1}^{*}(i),\)

here we have \(V_{2}^{+}=\pi_{12}^{*}(V_{1}^{+})\).

### The MAP estimator

First define the singleton set of a permutation \(\pi\) with respect to the adjacency matrices \(A\), \(B\), and \(C\) to be:

\[R(\pi,A,B,C):=\{i\in[n]:\forall j\in[n],A_{i,j}D_{\pi(i),\pi(j)}=0\},\]

where \(D_{ij}:=\max\{B_{ij},C_{\pi_{23}^{*}(i)\pi_{23}^{*}(j)}\}\). For brevity, write \(R_{\pi}:=R(\pi,A,B,C)\).

**Definition G.2**.: _Define the set \(S(\pi,A,B,C)\) as followings:_

1. \(i\in R(\pi,A,B,C)\)_;_
2. \(i\) _is a singleton in_ \(G_{1}\{R_{\pi}\}\)_;_
3. _If_ \(j\in N_{1}(i)\)_,_ \(\pi(j)\notin N_{H}(\pi(R_{\pi}))\)_._

Where \(A,B,C\) is the adjacency matrix of \(G_{1},G_{2},G_{3}\) respectively and \(D\) is the adjacency matrix in \(H=G_{2}\vee_{\pi_{23}^{*}}G_{3}\). Note that \(D\) is the adjacency matrix of \(H\), so \(N_{H}(\pi(R_{\pi}))=\{i\in[n]:\exists k\in\pi(R_{\pi}),D_{i,k}=1\}\).

Define \(\bar{R}_{\pi}:=R_{\pi}\cup\pi^{-1}(N_{H}(\pi(R_{\pi})))\). The condition 2 and 3 in Definition G.2 can be replaced by \(A_{i,j}=0\) for all \(j\in\bar{R}_{\pi}\). Write \(R^{*}=R(\pi_{12}^{*},A,B,C)\), \(S^{*}=S(\pi_{12}^{*},A,B,C)\), and \(\bar{R}^{*}=\bar{R}_{\pi_{12}^{*}}\) for brevity. We study the MAP estimate provided the additional knowledge \(\sigma_{2}^{*}\), \(\pi_{23}^{*}\), and \(\pi_{12}^{*}\{[n]\setminus S^{*}\}\).

**Theorem 5**.: _Let \(A,B,C,\sigma_{2}^{*},\pi_{23}^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\},S^{*}\) be given. For \(i\in[n]\setminus S^{*},\widehat{\sigma}_{MAP}(i)=\sigma_{2}^{*}(\pi_{12}^{*}(i))\). For vertices in \(S^{*}\), the MAP estimator depends on whether \(a,b\) is larger: 1. If \(a>b\), then the MAP estimator assigns the label +1 to the vertices corresponding to the largest \(|S^{*}\cap V_{1}^{+}|\) values in the collection \(\{\mathsf{maj}(i)\}_{i\in S^{*}}\) and assigns the label -1 to the remaining vertices in \(S^{*}\). 2. If \(a<b\), then the MAP estimator assigns the label +1 to the vertices corresponding to the smallest \(|S^{*}\cap V_{1}^{+}|\) values in the collection \(\{\mathsf{maj}(i)\}_{i\in S^{*}}\) and assigns the label -1 to the remaining vertices in \(S^{*}\)._

Then the following corollary prove the potential failure of the MAP estimator.

**Corollary G.3**.: _If \(a>b\), there exists \(i\in S^{*}\cap V_{1}^{+},j\in S^{*}\cap V^{-}\) such that \(\mathsf{maj}(i)<\mathsf{maj}(j)\), then the MAP estimator fails. Similarly, if \(a<b\), there exists \(i\in S^{*}\cap V_{1}^{+},j\in S^{*}\cap V^{-}\) such that \(\mathsf{maj}(i)>\mathsf{maj}(j)\), then the MAP estimator fails._

Proof.: Suppose \(a>b\). If the MAP estimator classifies \(i\) as \(+1\) correctly. By Theorem 5 the MAP estimator classifies \(j\) as \(+1\) which is wrong. The argument for the case \(a<b\) is similar. 

### The analysis of the failure of MAP estimator

**Definition G.4**.: _The event \(G_{\delta}\) holds if and only if_

\[n^{1-(2s^{2}-s^{3})T_{c}(a,b)-\delta}\leq|R^{*}\cap V_{1}^{+}|,|R^{*}\cap V_{1} ^{-}|,|\bar{R}^{*}\cap V_{1}^{+}|,|\bar{R}^{*}\cap V_{1}^{+}|\leq n^{1-(2s^{2} -s^{3})T_{c}(a,b)+\delta}.\]

**Lemma G.5**.: _For any fixed \(\delta>0\), \(\mathbb{P}(G_{\delta})=1-o(1)\)._

The proof of this lemma is straightforward but tedious and we defer it to Section G.6.

Now define the variable \(W_{i}\):

\[W_{i}:=\begin{cases}1(i\in S^{*},\mathsf{maj}(i)<0),&i\in R^{*}\cap V_{1}^{+},\\ 1(i\in S^{*},\mathsf{maj}(i)>0),&i\in R^{*}\cap V_{1}^{-}.\end{cases}\] (G.6)Denote \(\mathcal{I}\) be the sigma algebra induced by the random variables

\[D=\max(B,C),\pi_{12}^{*},\sigma_{1}^{*},R^{*},\{\mathcal{E}_{abc}:a,b,c\in\{0,1\}\}.\]

Note that \(\bar{R}^{*},\bar{R}^{*}\cap V_{1}^{+},\bar{R}^{*}\cap V_{1}^{-}\) are \(\mathcal{I}\) measurable.

Now \(\Gamma\)d like to show that \(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}>0,\sum_{i\in R^{*}\cap V_{1}^{-}}W_{i}>0\) with high probability, then it follows that \(\exists i\in S^{*}\cap V_{1}^{+},j\in S^{*}\cap V_{1}^{-}\) such that \(\mathsf{maj}(i)<0<\mathsf{maj}(j)\). By Corollary G.3, the MAP estimator fails. Use the first and second method to analyze \(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i},\sum_{i\in R^{*}\cap V_{1}^{-}}W_{i}\):

**Lemma G.7**.: _Fix \(\delta>0\) and denote \(\theta:=1-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)-s(1-s)^{2}\mathrm{D}_{+ }(a,b)\). We have_

\[\mathbb{E}[\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}|\mathcal{I}]1(\mathcal{F}\cap \mathcal{G}_{\delta}) \geq(1-n^{-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+2\delta})n ^{\theta-\delta-o(1)}1(\mathcal{F}\cap\mathcal{G}_{\delta})\]

_and_

\[\mathbb{E}[\sum_{i\in R^{*}\cap V_{1}^{-}}W_{i}|\mathcal{I}]1(\mathcal{F}\cap \mathcal{G}_{\delta}) \geq(1-n^{-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+2\delta})n ^{\theta-\delta-o(1)}1(\mathcal{F}\cap\mathcal{G}_{\delta}).\]

**Lemma G.8**.: _Fix \(\delta>0\) and denote \(\theta:=1-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)-s(1-s)^{2}\mathrm{D}_{+ }(a,b)\)_

\[\mathrm{Var}(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}|\mathcal{I})1(\mathcal{F} \cap\mathcal{G}_{\delta})\leq n^{2\theta-3\delta}1(\mathcal{F}\cap\mathcal{G} _{\delta})\]

_and_

\[\mathrm{Var}(\sum_{i\in R^{*}\cap V_{1}^{-}}W_{i}|\mathcal{I})1(\mathcal{F} \cap\mathcal{G}_{\delta})\leq n^{2\theta-3\delta}1(\mathcal{F}\cap\mathcal{G} _{\delta}).\]

The proofs of these two lemmas are deferred to Section G.7. Using the lemmas above we can now prove Theorem 5.

Proof of Theorem 2 when \(K=3\).: Firstly, show that \(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}>0\) with high probability. Use the second moment method, we obtain

\[\mathbb{P}(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}>0|\mathcal{I}) \geq\frac{\mathbb{E}[\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}| \mathcal{I}]^{2}}{\mathbb{E}[(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i})^{2}| \mathcal{I}]}\] \[=\frac{\mathbb{E}[\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}|\mathcal{ I}]^{2}}{\mathbb{E}[\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}|\mathcal{I}]^{2}+ \mathrm{Var}(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}|\mathcal{I})}\] \[\geq 1-\frac{\mathrm{Var}(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}| \mathcal{I})}{\mathbb{E}[\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}|\mathcal{I}]^{ 2}}.\]

Hence for unconditional probability, let \(\delta\) small enough, \(\delta<\min((2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)/8,\theta/4)\), then

\[\mathbb{P}(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}>0) \geq\mathbb{P}(\{\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}>0\}\cap \mathcal{F}\cap\mathcal{G}_{\delta})=\mathbb{E}[\mathbb{P}(\sum_{i\in R^{*} \cap V_{1}^{+}}W_{i}>0|\mathcal{I})1(\mathcal{F}\cap\mathcal{G}_{\delta})]\] \[\geq\mathbb{E}\left[\left(1-\frac{\mathrm{Var}(\sum_{i\in R^{*} \cap V_{1}^{+}}W_{i}|\mathcal{I})}{\mathbb{E}[\sum_{i\in R^{*}\cap V_{1}^{+}}W_ {i}|\mathcal{I}]^{2}}\right)1(\mathcal{F}\cap\mathcal{G}_{\delta})\right]\] \[\geq\left((1-o(1))n^{2\theta-3\delta-2(\theta-\delta)+o(1)} \right)\mathbb{P}(\mathcal{F}\cap\mathcal{G}_{\delta})=1-o(1).\]

The inequality on the last line is by Lemma G.7, G.8. The equality in the last line is by Lemma G.5, D.4. The proof for \(\mathbb{P}(\sum_{i\in R^{*}\cap V_{1}^{-}}W_{i}>0)=1-o(1)\) is identical. Hence, in light of Corollary G.3, the MAP estimator fails with probability \(1-o(1)\)

### Analysis of \(S_{\pi}\)

In this section we introduce the set \(\mathcal{A}_{\pi}\) and some properties of \(S_{\pi}\).

First, define the set

\[\mathcal{A}(S^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\}):=\{\pi\in S_{n}:S_{\pi}=S^ {*},\pi([n]\setminus S_{\pi})=\pi_{12}^{*}([n]\setminus S^{*})\}.\]

For brevity, sometimes write \(\mathcal{A}^{*}\).

**Lemma G.9**.: _For any \(\pi\in\mathcal{A}^{*},A_{i,j}B_{\pi(i),\pi(j)}C_{\pi_{23}^{*}(\pi(i)),\pi_{23}^ {*}(\pi(j))}=A_{i,j}B_{\pi_{12}^{*}(i)\pi_{12}^{*}(j)}C_{\pi_{13}^{*}(i),\pi_{1 3}^{*}(j)}\). Moreover, if \(i\in S^{*}\) or \(j\in S^{*}\),\(A_{i,j}B_{\pi(i),\pi(j)}C_{\pi_{23}^{*}(\pi(i)),\pi_{23}^{*}(\pi(j))}=A_{i,j}B_{ \pi_{12}^{*}(i)\pi_{12}^{*}(j)}C_{\pi_{13}^{*}(i),\pi_{13}^{*}(j)}=0,A_{i,j}C_{ \pi_{23}^{*}(\pi(i)),\pi_{23}^{*}(\pi(j))}=A_{i,j}C_{\pi_{13}^{*}(i),\pi_{13}^{ *}(j)}=0.\)_

Proof.: If \(i,j\in[n]\setminus S^{*}\), then \(\pi(i)=\pi_{12}^{*}(i)\) and \(\pi(j)=\pi_{12}^{*}(j)\), hence \(A_{i,j}B_{\pi(i),\pi(j)}C_{\pi_{23}^{*}(\pi(i)),\pi_{23}^{*}(\pi(j))}=A_{i,j}B_ {\pi_{12}^{*}(i)\pi_{12}^{*}(j)}C_{\pi_{13}^{*}(i),\pi_{13}^{*}(j)}\). If \(i\in S^{*}\) or \(j\in S^{*}\), then by Definition G.2, \(i\in R^{*}\) or \(j\in R^{*}\), hence \(A_{i,j}B_{\pi(i),\pi(j)}C_{\pi_{23}^{*}(\pi(i)),\pi_{23}^{*}(\pi(j))}=A_{i,j}B_ {\pi_{12}^{*}(i)\pi_{12}^{*}(j)}C_{\pi_{13}^{*}(i),\pi_{13}^{*}(j)}=0\), \(A_{i,j}C_{\pi_{23}^{*}(\pi(i)),\pi_{23}^{*}(\pi(j))}=A_{i,j}C_{\pi_{13}^{*}(i ),\pi_{13}^{*}(j)}=0\), and \(A_{i,j}B_{\pi_{1}(i),\pi(j)}=A_{i,j}B_{\pi_{12}^{*}(i)\pi_{12}^{*}(j)}=0\). 

**Definition G.10**.: _Let \(\rho\) be a permutation of \(S^{*}\) The permutation \(P_{\pi,\rho}\) is given by_

\[P_{\pi,\rho}:=\begin{cases}\pi(i)&i\in[n]\setminus S^{*},\\ \pi(\rho(i))&i\in S^{*}.\end{cases}\]

**Lemma G.11**.: _Let \(\rho\) be a permutation of \(S^{*}\). Then \(P_{\pi,\rho}\in\mathcal{A}^{*}\)._

Proof.: The proof is identical to [22, Lemma 8.11]. We only need to change \(B\) to \(B^{\prime}=\max(B,C)\) in the argument. 

A useful corollary of Lemma G.11 is that the elements of \(\mathcal{A}^{*}\) can be described by permutation of S.

**Corollary G.12**.: _We have the following representation_

\[\mathcal{A}^{*}=\{P_{\pi^{*},\rho}:\rho\text{ is a permutation of }S^{*}\}.\]

### Deriving the MAP estimator, proof of Theorem 5

#### g.5.1 The posterior distribution of \(\pi_{12}^{*}\)

First we define

\[\mu^{+}(\pi)_{abc} :=\sum_{(\pi(i),\pi(j))\in\mathcal{E}^{+}(\sigma_{2}^{*})}1((A_{i, j},B_{\pi(i),\pi(j)},C_{\pi_{23}^{*}(\pi(i)),\pi_{23}^{*}(\pi(j))})=(a,b,c)),a,b,c \in\{0,1\},\] \[\mu^{-}(\pi)_{abc} :=\sum_{(\pi(i),\pi(j))\in\mathcal{E}^{-}(\sigma_{2}^{*})}1((A_{i,j},B_{\pi(i),\pi(j)},C_{\pi_{23}^{*}(\pi(i)),\pi_{23}^{*}(\pi(j))})=(a,b,c)),a,b,c\in\{0,1\},\] \[\nu^{+}(\pi) :=\sum_{(\pi(i),\pi(j))\in\mathcal{E}^{+}(\sigma_{2}^{*})}A_{i,j},\] \[\nu^{-}(\pi) :=\sum_{(\pi(i),\pi(j))\in\mathcal{E}^{-}(\sigma_{2}^{*})}A_{i,j}.\]

With these definitions, we can derive an exact expression for the posterior distribution of \(\pi_{12}^{*}\) given \(A,B,C,\sigma_{2}^{*}\).

**Lemma G.13**.: _Let \(\pi\in S_{n}\). There's a constant \(D_{1}=D_{1}(A,B,C,\sigma_{2}^{*},\pi_{23}^{*})\) such that_

\[\mathbb{P}(\pi_{12}^{*}=\pi|A,B,C,\sigma_{2}^{*},\pi_{23}^{*})\] \[= D_{1}\left(\frac{p_{111}p_{000}}{p_{011}p_{100}}\right)^{\mu^{+}( \pi)_{111}}\left(\frac{p_{100}}{p_{000}}\right)^{\nu^{+}(\pi)}\left(\frac{p_{000 }p_{110}}{p_{100}p_{010}}\right)^{\mu^{+}(\pi)_{110}}\left(\frac{p_{000}p_{101 }}{p_{001}p_{100}}\right)^{\mu^{+}(\pi)_{101}}\] \[\times \left(\frac{q_{111}q_{000}}{q_{011}q_{100}}\right)^{\mu^{-}(\pi)_ {111}}\left(\frac{q_{100}}{q_{000}}\right)^{\nu^{-}(\pi)}\left(\frac{q_{000}q_{1 10}}{q_{100}q_{010}}\right)^{\mu^{-}(\pi)_{110}}\left(\frac{q_{000}q_{101}}{q_ {001}q_{100}}\right)^{\mu^{-}(\pi)_{101}}.\]Proof.: The proof is adapted from [22, Lemma 8.13]. By Bayes Rule,

\[\mathbb{P}(\pi_{12}^{*}=\pi|A,B,C,\sigma_{2}^{*},\pi_{23}^{*})=\frac{\mathbb{P}( A,B,C|\sigma_{2}^{*},\pi_{12}^{*}=\pi,\pi_{23}^{*})\mathbb{P}(\pi_{12}^{*}=\pi| \sigma_{2}^{*},\pi_{23}^{*})}{\mathbb{P}(A,B,C|\sigma_{2}^{*},\pi_{23}^{*})}.\]

In the construction of the multiple Correlated \(\mathrm{SBM}\), the permutation \(\pi_{12}^{*}\) is chosen independently of everything else, including the community labeling \(\sigma_{2}^{*}\) and the permutation \(\pi_{23}^{*}\). Hence we can rewrite

\[\mathbb{P}(\pi_{12}^{*}=\pi|A,B,C,\sigma_{2}^{*},\pi_{23}^{*})=d_{1}(A,B,C, \sigma_{2}^{*},\pi_{23}^{*})\mathbb{P}(A,B,C|\sigma_{2}^{*},\pi_{23}^{*},\pi_{ 12}^{*}=\pi),\]

where \(d_{1}(A,B,C,\sigma_{2}^{*})=(n|\mathbb{P}(A,B,C|\sigma_{2}^{*},\pi_{23}^{*})) ^{-1}\). Look at \(\mathbb{P}(A,B,C|\sigma_{2}^{*},\pi^{*}=\pi,\pi_{23}^{*})\), note that the edge formation process in \(G_{1},G_{2}\) and \(G_{3}\) is mutually independent across all vertex pairs given \(\sigma_{2}^{*},\pi_{12}^{*},\pi_{23}^{*}\). Hence we have

\[\mathbb{P}(A,B,C|\sigma_{2}^{*},\pi^{*}=\pi,\pi_{23}^{*})=\prod_{ijk\in\{0,1 \}}p_{ijk}^{\mu^{+}(\pi)_{ijk}}q_{ijk}^{\mu^{-}(\pi)_{ijk}}.\]

In particular, the sums \(\sum_{(\pi(i),\pi(j))\in\mathcal{E}^{+}(\sigma_{2}^{*})}B_{\pi(i),\pi(j)}C_{ \pi_{23}^{*}\pi(i),\pi_{23}^{*}\pi(j)}\), \(\sum_{(\pi(i),\pi(j))\in\mathcal{E}^{+}(\sigma_{2}^{*})}B_{\pi(i),\pi(j)}\), and \(\sum_{(\pi(i),\pi(j))\in\mathcal{E}^{+}(\sigma_{2}^{*})}C_{\pi_{23}^{*}\pi(i),\pi_{23}^{*}\pi(j)},|\mathcal{E}^{+}(\sigma_{2}^{*})|\) are measurable with respect to \(B,C,\sigma_{2}^{*},\pi_{23}^{*}\). Hence we do not care the relevant value and use \(\Lambda\) to represent. Now, for simple notations we write \(\sum ABC\) to represent \(\sum_{(\pi(i),\pi(j))\in\mathcal{E}^{+}(\sigma_{2}^{*})}A_{i,j}B_{\pi(i),\pi( j)}C_{\pi_{23}^{*}(\pi(i)),\pi_{23}^{*}(\pi(j))}\), \(\sum AB\) to represent \(\sum_{(\pi(i),\pi(j))\in\mathcal{E}^{+}(\sigma_{2}^{*})}A_{i,j}B_{\pi(i),\pi( j)}\), and \(\sum AC\) to represent \(\sum_{(\pi(i),\pi(j))\in\mathcal{E}^{+}(\sigma_{2}^{*})}A_{i,j}C_{\pi_{23}^{*}( \pi(i)),\pi_{23}^{*}(\pi(j))}\). Then, we can write

\[\mu^{+}(\pi)_{011}= \sum(1-A)BC=\sum BC-\mu^{+}(\pi)_{111}=\Lambda-\mu^{+}(\pi)_{111},\] \[\mu^{+}(\pi)_{010}= \sum(1-A)B(1-C)=\Lambda-\mu^{+}(\pi)_{110},\] \[\mu^{+}(\pi)_{001}= \sum(1-A)(1-B)C=\Lambda-\mu^{+}(\pi)_{101},\] \[\mu^{+}(\pi)_{000}= \sum(1-A)(1-B)(1-C)=\Lambda-\mu^{+}(\pi)_{100}+\mu^{+}(\pi)_{101}+ \mu^{+}(\pi)_{110}+\mu^{+}(\pi)_{111},\] \[\mu^{+}(\pi)_{100}= \sum A(1-B)(1-C)=\Lambda-\mu^{+}(\pi)_{111}-\mu^{+}(\pi)_{101}- \mu^{+}(\pi)_{110}+\nu^{+}(\pi).\]

Hence we can write

\[\prod_{ijk\in\{0,1\}}p_{ijk}^{\mu^{+}(\pi)_{ijk}}=d_{2}^{+}\left(\frac{p_{111 }p_{000}}{p_{011}p_{100}}\right)^{\mu^{+}(\pi)_{111}}\left(\frac{p_{100}}{p_{ 000}}\right)^{\nu^{+}(\pi)}\left(\frac{p_{000}p_{110}}{p_{100}p_{010}}\right)^{ \mu^{+}(\pi)_{110}}\left(\frac{p_{000}p_{101}}{p_{001}p_{100}}\right)^{\mu^{+}( \pi)_{101}}.\]

Here \(d_{2}^{+}\) is some constant given the information \(B,C,\sigma_{2}^{*},\pi_{23}^{*}\) Replicating the arguments for \(\mu^{-}(\pi)_{abc}\), we have that

\[\prod_{ijk\in\{0,1\}}q_{ijk}^{\mu^{-}(\pi)_{ijk}}=d_{2}^{-}\left(\frac{q_{111} q_{000}}{q_{011}q_{100}}\right)^{\mu^{-}(\pi)_{111}}\left(\frac{q_{100}}{q_{000}} \right)^{\nu^{-}(\pi)}\left(\frac{q_{000}q_{110}}{q_{100}q_{010}}\right)^{\mu^{- }(\pi)_{110}}\left(\frac{q_{000}q_{101}}{q_{001}q_{100}}\right)^{\mu^{-}(\pi)_{ 101}}.\]

Combining the two equations we prove the statement of lemma with \(D_{1}=d_{1}d_{2}^{+}d_{2}^{-}\). 

**Lemma G.14**.: _There is a constant \(D_{2}=D_{2}(A,B,C,\sigma_{2}^{*},S^{*},\pi_{23}^{*},\pi_{12}^{*}\{[n]\setminus S ^{*}\})\) such that_

\[\mathbb{P}(\pi_{12}^{*}=\pi|A,B,C,\sigma_{2}^{*},\pi_{23}^{*},S^{*},\pi_{12}^{*}\{[n ]\setminus S^{*}\})=D_{2}(\sqrt{\frac{p_{100}q_{000}}{p_{000}q_{100}}})^{\nu^{+} \pi-\nu^{-}(\pi)}1(\pi\in\mathcal{A}^{*}).\]

Proof.: By Bayes Rule we have that

\[\mathbb{P}(\pi_{12}^{*}=\pi|A,B,C,\sigma_{2}^{*},\pi_{23}^{*},S^{*}, \pi_{12}^{*}\{[n]\setminus S^{*}\})\] \[= \frac{\mathbb{P}(\pi_{12}^{*}=\pi|A,B,C,\sigma_{2}^{*},\pi_{23}^{*} )\mathbb{P}(S^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\}|\pi_{12}^{*}=\pi,A,B,C, \sigma_{2}^{*},\pi_{23}^{*})}{\mathbb{P}(S^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\}|A, B,C,\sigma_{2}^{*},\pi_{23}^{*})}\] \[= \frac{\mathbb{P}(\pi_{12}^{*}=\pi|A,B,C,\sigma_{2}^{*},\pi_{23}^{*}) }{\mathbb{P}(S^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\}|A,B,C,\sigma_{2}^{*},\pi_{2 3}^{*})}1(\pi\in\mathcal{A}^{*}).\]The probability in the denominator is a function of \(A,B,C,S^{*},\pi^{*}\{[n]\setminus S^{*}\},\pi^{*}_{23},\sigma^{*}_{2}\). Furthermore, by Lemma G.9, \(\mu^{+}(\pi)_{111},\mu^{+}(\pi)_{110},\mu^{+}(\pi)_{101},\mu^{-}(\pi)_{111},\mu^ {-}(\pi)_{110},\mu^{-}(\pi)_{101}\) are constant over \(\pi\in\mathcal{A}^{*}\). By Lemma G.13, we can write

\[\mathbb{P}(\pi^{*}_{12}=\pi|A,B,C,\sigma^{*}_{2},\pi^{*}_{23},S^{*},\pi^{*}_{12 }\{[n]\setminus S^{*}\})=d_{2}\left(\frac{p_{100}}{p_{000}}\right)^{\nu^{+}( \pi)}\left(\frac{q_{100}}{q_{000}}\right)^{\nu^{-}(\pi)}1(\pi\in\mathcal{A}^{*}),\]

where

\[d_{2} =\frac{D_{1}}{\mathbb{P}(S^{*},\pi^{*}_{12}\{[n]\setminus S^{*} \}|A,B,C,\sigma^{*}_{2},\pi^{*}_{23})}\left(\frac{q_{111}q_{000}}{q_{011}q_{100 }}\right)^{\mu^{-}(\pi^{*}_{12})_{111}}\left(\frac{q_{000}q_{110}}{q_{100}q_{0 10}}\right)^{\mu^{-}(\pi^{*}_{12})_{110}}\] \[\times\left(\frac{q_{000}q_{101}}{q_{001}q_{100}}\right)^{\mu^{-} (\pi^{*}_{12})_{101}}\left(\frac{p_{111}p_{000}}{p_{011}p_{100}}\right)^{\mu^{ +}(\pi)_{111}}\left(\frac{p_{000}p_{110}}{p_{100}p_{100}}\right)^{\mu^{+}(\pi) _{110}}\left(\frac{p_{000}p_{101}}{p_{001}p_{100}}\right)^{\mu^{+}(\pi)_{101}}.\]

\(D_{1}\) is the same constant in Lemma G.13, \(d_{2}\) is a constant given \(A,B,C,\sigma^{*}_{2},S^{*},\pi^{*}_{23},\pi^{*}_{12}\{[n]\setminus S^{*}\}\). To further simplifies the posterior distribution,

\[\mathbb{P}(\pi^{*}_{12}=\pi|A,B,C,\sigma^{*}_{2},\pi^{*}_{23},S^{* },\pi^{*}_{12}\{[n]\setminus S^{*}\})=d_{2}\left(\frac{p_{100}}{p_{000}} \right)^{\nu^{+}(\pi)}\left(\frac{q_{100}}{q_{000}}\right)^{\nu^{-}(\pi)}1(\pi \in\mathcal{A}^{*})\] \[=d_{2}(\sqrt{\frac{p_{100}q_{100}}{p_{000}q_{100}}})^{\nu^{-}(\pi )+\nu^{+}(\pi)}(\sqrt{\frac{p_{100}q_{100}}{p_{000}q_{100}}})^{\nu^{+}(\pi)- \nu^{-}(\pi)}1(\pi\in\mathcal{A}^{*}).\]

Note that \(\nu^{-}(\pi)+\nu^{+}(\pi)=\sum_{(i,j)\in\binom{[n]}{2}}A_{i,j}\) that only depends on \(A\). The results follows then

\[\mathbb{P}(\pi^{*}_{12}=\pi|A,B,C,\sigma^{*}_{2},\pi^{*}_{23},S^{*},\pi^{*}_{1 2}\{[n]\setminus S^{*}\})=D_{2}(\sqrt{\frac{p_{100}q_{100}}{p_{000}q_{100}}}) ^{\nu^{+}(\pi)-\nu^{-}(\pi)}1(\pi\in\mathcal{A}^{*}),\]

where \(D_{2}=d_{2}(\sqrt{\frac{p_{100}q_{100}}{p_{000}q_{100}}})^{\sum_{(i,j)\in\binom{ [n]}{2}}A_{i,j}}\). 

#### g.5.2 The posterior distribution of \(\sigma^{*}\)

Now we can study the posterior distribution of the community labeling \(\sigma^{*}\). For a community partition \(X=(X^{+},X^{-})\) of \([n]\) in \(G_{1}\), define the set

\[B(X):=\{\pi\in\mathcal{A}^{*}:\pi(X^{+})=V^{+}_{2},\pi(X^{-})=V^{-}_{2}\}.\]

In particular, if \(\sigma_{X}\) denotes the community memberships associated with \(X\), the following must hold:

* \(\sigma_{X}(i)=\sigma^{*}_{2}(\pi^{*}_{12}(i))\) for \(i\in[n]\setminus S^{*}\);
* \(|S^{*}\cap X^{+}|=|S^{*}\cap V^{+}_{1}|,|S^{*}\cap X^{-}|=|S^{*}\cap V^{-}_{1}|\).

The first condition must hold since we know the true vertex correspondence and the true community labels outside of the \(S^{*}\). The second condition must hold since the number of vertices of each community in \(S^{*}\) can be deduced by examining the community labels of \(\pi^{*}(S^{*})\) with respect to \(\sigma^{*}_{2}\).

**Lemma G.15**.: _If \(|B(X)|\) is not empty, then \(|B(X)|=|S^{*}\cap X^{-}|!|S^{*}\cap X^{+}|!\)._

Proof.: The proof is almost identical to [22, Lemma 8.15]. Suppose that \(\pi_{0},\pi_{1}\in B(X)\), by Corollary G.12, there exists \(\rho\) such that \(\pi_{1}=P_{\pi_{0},\rho}\). Claim: if \(i\in S^{*}\cap X^{+},\rho(i)\in S^{*}\cap X^{+}\), if \(i\in S^{*}\cap X^{-},\rho(i)\in S^{*}\cap X^{-}\). If \(\exists i\in S^{*}\cap X^{+},\rho(i)\in S^{*}\cap X^{-}\), then \(\sigma^{*}_{2}(\pi_{0}(i))=1,\sigma^{*}_{2}(\pi_{1}(i))=\sigma^{*}_{2}(\pi_{0}( \rho(i)))=-1\). This violates the definition of \(B(X)\). The claim is proved. Hence we can decomposition \(\rho\) into two disjoint permutations \(\rho^{+}\), \(\rho^{-}\).\(\rho^{+}\) is a permutation of \(S^{*}\cap X^{+}\) while \(\rho^{-}\) is a permutation of \(S^{*}\cap X^{-}\). Hence \(|B(X)|=(\#\) of choices of \(\rho^{+})\times(\#\) of choices of \(\rho^{-})=|S^{*}\cap X^{+}||S^{*}\cap X^{-}|!=|S^{*}\cap V^{+}_{1}||!|S^{*} \cap V^{-}|!\). 

Then we look at \(\nu^{+}(\pi)-\nu^{-}(\pi)\).

**Lemma G.16**.: _For all \(\pi\in B(X),\) we have that \(\nu^{+}(\pi)-\nu^{-}(\pi)=D_{3}+\sum_{i\in S^{*}}\mathsf{maj}(i)\sigma_{X}(i)\) where \(D_{3}\) is a constant depending on \(A,B,C,\sigma^{*}_{2},\pi^{*}_{23},S^{*},\pi^{*}_{12}\{[n]\setminus S^{*}\}\) but not on \(X\)._Proof.: Note that \(\sigma_{X}(i)\sigma_{X}(j)=1\) if \((\pi(i),\pi(j))\in\mathcal{E}^{+}(\sigma_{2}^{*})\), \(\sigma_{X}(i)\sigma_{X}(j)=-1\) if \((\pi(i),\pi(j))\in\mathcal{E}^{-}(\sigma_{2}^{*})\). We have

\[\nu^{+}(\pi)-\nu^{-}(\pi)=\sum_{(i,j)\in\{[n]\setminus S^{*}\}} \sigma_{X}(i)\sigma_{X}(j)A_{i,j}\] \[=\sum_{(i,j)\in\{[n]\setminus S^{*}\}}\sigma_{X}(i)\sigma_{X}(j)A _{i,j}+\sum_{i,j\in S^{*}}\sigma_{X}(i)\sigma_{X}(j)A_{i,j}+\sum_{i\in S^{*},j\in\{[n]\setminus S^{*}\}}\sigma_{X}(i)\sigma_{X}(j)A_{i,j}.\]

We should note that if \((i,j)\in\{[n]\setminus S^{*}\}\), then \(\sigma_{X}(i)\sigma_{X}(j)A_{i,j}=\sigma_{2}^{*}(\pi_{12}(i))\sigma_{2}^{*}( \pi_{12}(j))A_{i,j}=\sigma_{2}^{*}(\pi_{12}^{*}(i))\sigma_{2}^{*}(\pi_{12}^{* }(j))A_{i,j}\). Denote

\[D_{3}:=\sum_{(i,j)\in\{[n]\setminus S^{*}\}}\sigma_{X}(i)\sigma_{X}(j)A_{i,j}.\]

Clearly \(D_{3}\) depends only on \(A,\sigma_{2}^{*},S^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\}\). If \(i,j\in S^{*}\), \(A_{i,j}=0\) by Definition G.2.

\[\sum_{i\in S^{*},j\in\{[n]\setminus S^{*}\}}\sigma_{X}(i)\sigma_{ X}(j)A_{i,j} =\sum_{i\in S^{*},j\in\{[n]\setminus S^{*}\}}\sigma_{X}(i)\sigma_{ 1}^{*}(j)A_{i,j}\] \[=\sum_{i\in S^{*}}\sigma_{X}(i)\sum_{j\in\{[n]\setminus S^{*}\} }\sigma_{1}^{*}(j)A_{i,j}\] \[=\sum_{i\in S^{*}}\sigma_{X}(i)\mathsf{maj}(i).\]

The last equality is because if \(i\in S^{*}\),\(\mathsf{maj}(i)=\sum_{j\in[n]}A_{i,j}\sigma_{1}^{*}(j)=\sum_{j\in\{[n] \setminus S^{*}\}}A_{i,j}\sigma_{1}^{*}(j)\), since \(A_{i,j}=0\) if \(i,j\in S^{*}\). Then the statement follows,

\[\nu^{+}(\pi)-\nu^{-}(\pi)=D_{3}+\sum_{i\in S^{*}}\sigma_{X}(i)\mathsf{maj}(i).\qed\]

**Lemma G.17**.: _If \(B(X)\) is nonempty, then_

\[\mathbb{P}((V_{1}^{+},V_{2}^{+})=(X^{+},X^{-})|A,B,C,\sigma_{2}^ {*},\pi_{23}^{*},S^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\})\] \[=D_{4}\left(\sqrt{\frac{p_{100}q_{100}}{p_{000}q_{000}}}\right)^{ \sum_{i\in S^{*}}\sigma_{X}(i)\mathsf{maj}(i)},\]

_where \(D_{4}\) is a constant depending on \(A,B,C,\sigma_{2}^{*},\pi_{23}^{*},S^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\}\), but not on \(X\)._

Proof.: We have that

\[\mathbb{P}((V_{1}^{+},V_{2}^{+})=(X^{+},X^{-})|A,B,C,\sigma_{2}^ {*},\pi_{23}^{*},S^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\})\] \[= \sum_{\pi\in B(X)}\mathbb{P}(\pi_{12}^{*}=\pi|A,B,C,\sigma_{2}^{ *},\pi_{23}^{*},S^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\})\] \[= \sum_{\pi\in B(X)}D_{2}\left(\sqrt{\frac{p_{100}q_{000}}{p_{000}q _{100}}}\right)^{\nu^{+}(\pi)-\nu^{-}(\pi)}\] \[= D_{2}\left(\sqrt{\frac{p_{100}q_{000}}{p_{000}q_{100}}}\right)^ {D_{3}}\sum_{\pi\in B(X)}\left(\sqrt{\frac{p_{100}q_{000}}{p_{000}q_{100}}} \right)^{\sum_{i\in S^{*}}\sigma_{X}(i)\mathsf{maj}(i)}\] \[= D_{2}\left(\sqrt{\frac{p_{100}q_{000}}{p_{000}q_{100}}}\right)^ {D_{3}}|B(X)|\left(\sqrt{\frac{p_{100}q_{000}}{p_{000}q_{100}}}\right)^{\sum_{i \in S^{*}}\sigma_{X}(i)\mathsf{maj}(i)}\] \[= D_{4}\left(\sqrt{\frac{p_{100}q_{000}}{p_{000}q_{100}}}\right)^ {\sum_{i\in S^{*}}\sigma_{X}(i)\mathsf{maj}(i)},\]

where \(D_{4}=D_{2}\left(\sqrt{\frac{p_{100}q_{000}}{p_{000}q_{100}}}\right)^{D_{3}}|S^ {*}\cap V_{1}^{+}||S^{*}\cap V_{1}^{-}|\). The statement follows.

Proof of Theorem 5.: Given \(A,B,C,\sigma_{2}^{*},\pi_{23}^{*},S^{*},\pi_{12}^{*}[n]\setminus S^{*}\), it's obvious that \(\widehat{\sigma}_{MAP}(i)=\sigma_{2}^{*}(\pi_{12}^{*}(i))\) for \(i\in[n]\setminus S^{*}\). For vertices in \(S^{*}\), note that

\[\frac{p_{100}q_{000}}{p_{000}q_{100}}=\frac{s(1-s)^{2}p(1-(1-(1-s)^{3})q)}{s(1- s)^{2}q(1-(1-(1-s)^{3})q)}=(1+o(1))\frac{p}{q}=(1+o(1))\frac{a}{b}.\]

Thus, by Lemma G.17, if \(a>b\), the MAP estimator maximizes \(\sum_{i\in S^{*}}\sigma_{X}(i)\mathsf{maj}(i)\) while the MAP estimator minimizes \(\sum_{i\in S^{*}}\sigma_{X}(i)\mathsf{maj}(i)\) if \(a<b\). Suppose \(a>b\), while satisfying the condition \(|S^{*}\cap X^{+}|=|S^{*}\cap V^{+}|\), \(|S^{*}\cap X^{-}|=|S^{*}\cap V^{-}|\), the maximum of \(\sum_{i\in S^{*}}\sigma_{X}(i)\mathsf{maj}(i)\) is obtained by setting \(\sigma_{X}(i)=+1\) to the vertices \(i\in S^{*}\) corresponding to the largest \(|S^{*}\cap V_{1}^{+}|\) values in the collection \(\{\mathsf{maj}(i)\}_{i\in S^{*}}\) and assigns the label -1 to the remaining vertices in \(S^{*}\). The proof is the same suppose \(a<b\). Then Theorem 5 follows. 

### Proof of Lemma g.5

Denote \(E_{i}\) as the event that \(i\) is a singleton in \(G_{1}\wedge_{\pi_{12}^{*}}H\), in other words that \(i\in R^{*}\). We assume that the communities are approximately balanced. More precisely, we assume that the event \(\mathcal{G}=\{n/2-n^{3/4}\leq|V^{+}|,|V^{-}|\leq n/2+n^{3/4}\}\) holds. By Lemma D.4, we have that \(\mathbb{P}(\mathcal{G})=1-o(1)\).

Conditioning on \(\sigma_{1}^{*}\), if \(i\in V_{1}^{+}\), then we have

\[\mathbb{P}(E_{i}|\sigma_{1}^{*})1(\mathcal{G})=(1-s(1-(1-s)^{2})a \log n/n)^{|V_{1}^{+}|-1}(1-s(1-(1-s)^{2})b\log n/n)^{|V_{1}^{-}|}1(\mathcal{ G})\] \[\leq \exp(-s(2s-s^{2})\log n/n(a(|V_{1}^{+}|-1)+b|V_{1}^{-}|))1( \mathcal{G})\] \[= \exp(-(1-o(1))(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)\log n)1 (\mathcal{G})=n^{-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+o(1)}1(\mathcal{ G}).\]

The first inequality uses the fact \(1-x\leq e^{-x}\). Hence

\[\mathbb{E}[|R^{*}\cap V_{1}^{+}||\sigma_{1}^{*}]1(\mathcal{G}) =\sum_{i\in V^{+}}\mathbb{P}(E_{i}|\sigma_{1}^{*})1(\mathcal{G}) \leq|V_{1}^{+}|n^{-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+o(1)}1( \mathcal{G})\] \[\leq n^{1-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+o(1)}.\]

By Markov's inequality,

\[\mathbb{P}(|R^{*}\cap V_{1}^{+}|\geq n^{1-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c }}(a,b)+\delta}|\sigma_{1}^{*})1(\mathcal{G})\leq n^{-\delta+o(1)}=o(1).\]

Hence

\[\mathbb{P}(|R^{*}\cap V_{1}^{+}|\geq n^{1-(2s^{2}-s^{3})\mathrm{T }_{\mathrm{c}}(a,b)+\delta})\\ \leq\mathbb{P}(\mathcal{G}^{c})+\mathbb{E}[\mathbb{P}(|R^{*}\cap V _{1}^{+}|\geq n^{1-(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+\delta}|\sigma_{ 1}^{*})1(\mathcal{G})]=o(1).\]

Now we derive a lower bound for \(|R^{*}\cap V_{1}^{+}|\). For \(\epsilon=\epsilon_{n}\) sufficiently small:

\[(\log(\mathbb{P}(E_{i}|\sigma_{1}^{*})))1(\mathcal{G}) =((|V_{1}^{+}|-1)\log(1-s(1-(1-s)^{2})a\log n/n)\] \[\quad+|V_{1}^{-}|\log(1-s(1-(1-s)^{2})b\log n/n))1(\mathcal{G})\] \[\geq-(1+\epsilon)(2s^{2}-s^{3})\log n/n(a|V_{1}^{+}|+b|V_{1}^{-}|) 1(\mathcal{G})\] \[=(1-o(1))(1+\epsilon)(2s^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)( \log n)1(\mathcal{G}).\]

The first inequality uses that \(\log(1-x)\geq-(1+\epsilon)x\) provided \(0<x<\epsilon/(1+\epsilon)\). Setting \(\epsilon=n^{-0.5}\), we have that

\[\mathbb{E}[|R^{*}\cap V_{1}^{+}||\sigma_{1}^{*}]1(\mathcal{G})=\sum_{i\in V^{+} }\mathbb{P}(E_{i}|\sigma_{1}^{*})1(\mathcal{G})\geq n^{1-(2s^{2}-s^{3})\mathrm{ T}_{\mathrm{c}}(a,b)-o(1)}1(\mathcal{G}).\]

Then we bound the variance of \(|R^{*}\cap V_{1}^{+}|\). For \(i,j\in V_{1}^{+},i\neq j\):

\[\mathrm{Cov}(1(E_{i}),1(E_{j})|\sigma_{1}^{*})=\mathbb{P}(E_{i}E_ {j}|\sigma_{1}^{*})-\mathbb{P}(E_{i}|\sigma_{1}^{*})^{2}\\ =(1-s(2s-s^{2})a\log n/n)^{2|V_{1}^{+}|-3}(1-s(2s-s^{2})b\log n/n) ^{2|V_{1}^{-}|}(1-(1-s(2s-s^{2})a\log n/n)).\]On event \(\mathcal{G}\), \(2|V_{1}^{+}|-3=(1+o(1))n\), \(2|V_{1}^{-}|=(1+o(1))n\). Hence using the equality \(1-x\leq e^{-x}\), we have that

\[\mathrm{Cov}(1(E_{i}),1(E_{j})|\sigma_{1}^{*})1(\mathcal{G}) \leq s(2s-s^{2})a\log n/n\exp(-(1-o(1))s(2s-s^{2})(a+b)\log n)1( \mathcal{G})\] \[=n^{-1-(2s^{2}-s^{3})(a+b)+o(1)}1(\mathcal{G}).\]

Then

\[\mathrm{Var}(|R^{*}\cap V_{1}^{+}||\sigma_{1}^{*})1(\mathcal{G})\] \[= \mathrm{Var}(\sum_{i\in V_{1}^{+}}1(E_{i})|\sigma_{1}^{*})1( \mathcal{G})=\sum_{i,j\in V_{1}^{+}}\mathrm{Cov}(1(E_{i}),1(E_{j})|\sigma_{1}^ {*})1(\mathcal{G})\] \[\leq \sum_{i\in V_{1}^{+}}\mathbb{P}(E_{i}|\sigma_{1}^{*})1(\mathcal{G })+\sum_{i\neq j\in V_{1}^{+}}\mathrm{Cov}(1(E_{i}),1(E_{j})|\sigma_{1}^{*})1 (\mathcal{G})\] \[\leq (n^{1-(2s^{2}-s^{3})(a+b)+o(1)}+n^{1-(2s^{2}-s^{3})(a+b)/2+o(1)} )1(\mathcal{G})=n^{1-(2s^{2}-s^{3})(a+b)/2+o(1)}1(\mathcal{G}).\]

By the Paley-Zygmund inequality,

\[\mathbb{P}(|R^{*}\cap V_{1}^{+}|\geq n^{1-(2s^{2}-s^{3})\mathrm{ T}_{c}(a,b)-\delta}|\sigma_{1}^{*})1(\mathcal{G})\] \[\geq (1-n^{-\delta+o(1)})^{2}\frac{\mathbb{E}[|R^{*}\cap V_{1}^{+}|| \sigma_{1}^{*}]^{2}}{\mathbb{E}[|R^{*}\cap V_{1}^{+}|^{2}|\sigma_{1}^{*}]}1( \mathcal{G})\] \[\geq (1-n^{-\delta+o(1)})^{2}(1-\frac{\mathrm{Var}[|R^{*}\cap V_{1}^{+ }||\sigma_{1}^{*}]}{\mathbb{E}[|R^{*}\cap V_{1}^{+}||\sigma_{1}^{*}]^{2}})1( \mathcal{G})\] \[\geq (1-n^{-\delta+o(1)})^{2}(1-n^{-(1-(2s^{2}-s^{3})\mathrm{T}_{c}(a, b))+o(1)})1(\mathcal{G})=(1-o(1))1(\mathcal{G}).\]

Together with \(\mathbb{P}(\mathcal{G})=1-o(1)\), we have

\[\mathbb{P}(n^{1-(2s^{2}-s^{3})\mathrm{T}_{c}(a,b)-\delta}\leq|R^{*}\cap V_{1} ^{+}|\leq n^{1-(2s^{2}-s^{3})\mathrm{T}_{c}(a,b)+\delta})=1-o(1).\]

The proof for \(|R^{*}\cap V_{1}^{-}|\) is the same. Now we study \(|\bar{R}^{*}\cap V_{1}^{+}|\). Since \(R^{*}\subset\bar{R}^{*}\),we have the lower bound \(\mathbb{P}(|\bar{R}^{*}\cap V_{1}^{+}|\geq|R^{*}\cap V_{1}^{+}|\geq n^{1-(2s^{ 2}-s^{3})\mathrm{T}_{c}(a,b)-\delta})=1-o(1)\). For the upper bound,

\[|\bar{R}^{*}\cap V_{1}^{+}|\leq|\bar{R}^{*}|\leq\sum_{i\in R^{*}}(1+|N_{H}( \pi_{12}^{*}(i))|)\leq|R^{*}|(1+\max_{j\in[n]}N_{H}(j)).\]

By Lemma D.6, since \(H\sim\mathrm{SBM}(n,(1-(1-s)^{2})a\log n/n,(1-(1-s)^{2})b\log n/n)\), we have that \(\max_{j\in[n]}N_{H}(j)\leq 100\max a,b(1-(1-s)^{2})\log n\) with probability \(1-o(1)\). Hence we have that with high probability

\[|\bar{R}^{*}\cap V_{1}^{+}| \leq(1+100\max a,b(1-(1-s)^{2})\log n)|R^{*}|\] \[\leq 2(1+100\max a,b(1-(1-s)^{2})\log n)n^{1-(2s^{2}-s^{3}) \mathrm{T}_{c}(a,b)+\delta}\] \[\leq n^{1-(2s^{2}-s^{3})\mathrm{T}_{c}(a,b)+\delta}.\]

### Proof of Lemma G.7, g.8

Firstly, we use some useful conditional independence properties given the sigma algebra \(\mathcal{I}\).

**Lemma G.18**.: _Let \(i\in R^{*}\), conditioned on \(\mathcal{I},\{A_{ij}\,:\,\{i,j\}\,\in\mathcal{E}_{100}\}\) is a collection of mutually independent random variables where_

\[A_{ij}\sim\begin{cases}\mathrm{Ber}(a\log n/n)&\sigma_{1}^{*}(i)=\sigma_{1}^{*}( j),\\ \mathrm{Ber}(b\log n/n)&\sigma_{1}^{*}(i)=-\sigma_{1}^{*}(j).\end{cases}\]

_The random variables \(1(i\in S^{*})\) and \(\sum_{j\in[n]\setminus\bar{R}^{*}}A_{i,j}\sigma_{1}^{*}(j)\) are conditionally independent given \(\mathcal{I}\)._

Proof.: Note that the sets \(R^{*},\bar{R}^{*}\) only depend on \(\pi_{12}^{*},H,G_{1}\,\wedge_{\pi_{12}^{*}}H\). \(G_{1}\,\backslash_{\pi_{12}^{*}}H\) is comprised of edges in \(\mathcal{E}_{100}\), the graph \(H\) is comprised of edges in \(\cup_{j+k>0}\mathcal{E}_{ijk}\), the graph \(G_{1}\,\wedge_{\pi_{12}^{*}}H\) is comprised of edges in \(\cup_{j+k>0}\mathcal{E}_{ijk}\). Thus by lemma D.2, \(G_{1}\,\backslash_{\pi_{12}^{*}}H\) is conditionally independent of given \(\bm{\pi}^{*},\sigma_{1}^{*}\) and the partition \(\bm{\mathcal{E}}\). In particular, \(\{A_{i,j}:\{i,j\}\in\mathcal{E}_{100}\}\) is conditionally independent of \(\mathcal{I}\).

Note that \(1(i\in S^{*})=1(i\in R^{*})1(A_{i,j}=0,\forall j\in\bar{R}^{*})\). Hence \(1(i\in S^{*})\) is measurable with respect to the sigma algebra generated by \(\mathcal{I}\) and the collection \(C_{1}:=\{A_{i,j}:j\in\bar{R}^{*},\{i,j\}\in\mathcal{E}_{100}\}\). On the other hand, since \(\bar{R}^{*}\) is \(\mathcal{I}-\) measurable, \(\sum_{j\in[n]\setminus\bar{R}^{*}}A_{i,j}\sigma_{1}^{*}(j)\) is measurable with respect to the sigma algebra generated by \(\mathcal{I}\) and the collection \(C_{2}:=\{A_{i,j}:j\in[n]\setminus\bar{R}^{*},\{i,j\}\in\mathcal{E}_{100}\}\). \(C_{1}\cap C_{2}=\emptyset\), the independence of \(A_{i,j}\) implies that the two random variables are conditionally independent given \(\mathcal{I}\). 

**Lemma G.19**.: _For any \(\delta>0\), \(i\in R^{*}\),it holds for sufficiently large \(n\) that_

\[\mathbb{P}(i\in S^{*}|\mathcal{I})1(\mathcal{G}_{\delta})\geq(1-n^{-(2s^{2}-s ^{3})\mathrm{T}_{\mathrm{c}}(a,b)+2\delta})1(\mathcal{G}_{\delta}).\]

Proof.: For \(i\in R^{*}\), define the following random sets that are \(\mathcal{I}-\) measurable:

\[C^{+}(i): =\{j\in\bar{R}^{*}:\{i,j\}\in\mathcal{E}_{100}\cap\mathcal{E}^{+} (\sigma_{1}^{*})\},\] \[C^{-}(i): =\{j\in\bar{R}^{*}:\{i,j\}\in\mathcal{E}_{100}\cap\mathcal{E}^{- }(\sigma_{1}^{*})\}.\]

Note that \(i\in S^{*}\) if and only if \(i\in R^{*}\) and \(A_{i,j}=0\), \(\forall j\in C^{+}(i)\cup C^{-}(i)\) conditioned on \(\mathcal{I}\). Hence by Lemma G.18:

\[\mathbb{P}(i\in S^{*}|\mathcal{I}) =\mathbb{P}(A_{i,j}=0,\forall j\in C^{+}(i)\cup C^{-}(i)| \mathcal{I})=(1-a\log n/n)^{|C^{+}(i)|}(1-b\log n/n)^{|C_{i}(i)|}\] \[\geq(1-a|C^{+}(i)|\log n/n)(1-b|C^{-}(i)|\log n/n)\] \[\geq 1-(a|C^{+}(i)|+b|C^{-}(i)|)\log n/n.\]

The first inequality is because of Bernoulli's inequality. Note that on event \(G_{\delta},|C^{+}(i)|,|C^{-}(i)|\leq|\bar{R}^{*}|\leq 2n^{1-(2s^{2}-s^{3}) \mathrm{T}_{\mathrm{c}}(a,b)+\delta}\). Thus

\[\mathbb{P}(i\in S^{*}|\mathcal{I})1(\mathcal{G}_{\delta})\geq(1-2(a+b)n^{-(2s ^{2}-s^{3})\mathrm{T}_{\mathrm{c}}(a,b)+\delta}\log n)1(\mathcal{G}_{\delta}).\]

Since \(2(a+b)\log n\leq n^{\delta}\), the lemma follows. 

Now, define the random variable

\[X_{i}:=\begin{cases}\mathbb{P}(\sum_{j\in[n]\setminus\bar{R}^{*}}A_{i,j} \sigma_{1}^{*}(j)<0|\mathcal{I})&i\in R^{*}\cap V_{1}^{+},\\ \mathbb{P}(\sum_{j\in[n]\setminus\bar{R}^{*}}A_{i,j}\sigma_{1}^{*}(j)>0| \mathcal{I})&i\in R^{*}\cap V_{1}^{-}.\end{cases}\]

Then we will study \(X_{i}\) on the event \(\mathcal{F}\cap G_{\delta}\).

**Lemma G.20**.: _For \(i\in R^{*}\),_

\[X_{i}1(\mathcal{F}\cap G_{\delta})=n^{-s(1-s)^{2}\mathrm{D}_{+}(a,b)+o(1)}1( \mathcal{F}\cap G_{\delta}).\]

Proof.: Suppose \(i\in R^{*}\cap V_{1}^{+}\). Note that \(\bar{R}^{*},\sigma_{1}^{*}\) are \(\mathcal{I}-\) measurable. By Lemma G.18:

\[\sum_{j\in[n]\setminus\bar{R}^{*}}A_{i,j}\sigma_{1}^{*}(j)\stackrel{{ d}}{{=}}Y-Z\]

\[Y\sim\mathrm{Bin}(|\{j\in[n]\setminus\bar{R}^{*}:\{i,j\}\in\mathcal{E}_{100} \cap\mathcal{E}^{+}(\sigma_{1}^{*})|,a\log n/n),\]

\[Z\sim\mathrm{Bin}(|\{j\in[n]\setminus\bar{R}^{*}:\{i,j\}\in\mathcal{E}_{100} \cap\mathcal{E}^{-}(\sigma_{1}^{*})|,b\log n/n).\]

Where \(Y,Z\) are independent. For brevity, suppose \(Y\sim\mathrm{Bin}(y,p)\), \(Z\sim\mathrm{Bin}(z,q)\). On the event \(\mathcal{F}\cap G_{\delta}\), the upper bound for \(y\):

\[y\leq|\{j\in[n]:\{i,j\}\in\mathcal{E}_{100}\cap\mathcal{E}^{+}(\sigma_{1}^{*}) |\leq s(1-s)^{2}(n/2+2n^{3/4})=(1+o(1))s(1-s)^{2}n/2.\]

The lower bound for \(y\):

\[y \geq|\{j\in[n]:\{i,j\}\in\mathcal{E}_{100}\cap\mathcal{E}^{+}( \sigma_{1}^{*})|-|\bar{R}^{*}|\] \[\geq s(1-s)^{2}(n/2-2n^{3/4})-n^{1-(2s^{2}-s^{3})\mathrm{T}_{ \mathrm{c}}(a,b)+\delta}=(1-o(1))s(1-s)^{2}n/2.\]
Proof of Lemma g.8.: Suppose \(i\neq j\in R^{*}\cap V_{1}^{+}\), we have

\[\mathbb{E}[W_{i}W_{j}|\mathcal{I}] =\mathbb{P}(i,j\in S^{*},\sum_{k\in[n]\setminus\bar{R}^{*}}A_{i,k} \sigma_{1}^{*}(k)<0,\sum_{k\in[n]\setminus\bar{R}^{*}}A_{j,k}\sigma_{1}^{*}(k)< 0|\mathcal{I})\] \[\leq\mathbb{P}(\sum_{k\in[n]\setminus\bar{R}^{*}}A_{i,k}\sigma_{1} ^{*}(k)<0,\sum_{k\in[n]\setminus\bar{R}^{*}}A_{j,k}\sigma_{1}^{*}(k)<0| \mathcal{I})\] \[=\mathbb{P}(\sum_{k\in[n]\setminus\bar{R}^{*}}A_{i,k}\sigma_{1}^{ *}(k)<0|\mathcal{I})\mathbb{P}(\sum_{k\in[n]\setminus\bar{R}^{*}}A_{j,k} \sigma_{1}^{*}(k)<0|\mathcal{I})=X_{i}X_{j}.\]

The second inequality is because by Lemma G.18, \(\{A_{i,k}\}_{k\in[n]\setminus\bar{R}^{*}}\) and \(\{A_{j,k}\}_{k\in[n]\setminus\bar{R}^{*}}\) are conditionally independent given \(\mathcal{I}\). Consider the case \(i=j\),

\[\mathbb{E}[W_{i}^{2}|\mathcal{I}]=\mathbb{P}(i\in S^{*},\sum_{j\in[n]\setminus \bar{R}^{*}}A_{i,j}\sigma_{1}^{*}(j)<0|\mathcal{I})\leq X_{i}.\]

Summing over \(i\in R^{*}\cap V_{1}^{+}\):

\[\mathbb{E}[(\sum_{i\in R^{*}\cap V^{+}}W_{i})^{2}|\mathcal{I}] =\sum_{i\in R^{*}\cap V^{+}}\mathbb{E}[W_{i}|\mathcal{I}]+\sum_{i \neq j\in R^{*}\cap V^{+}}\mathbb{E}[W_{i}W_{j}|\mathcal{I}]\] \[\leq\sum_{i\in R^{*}\cap V^{+}}X_{i}+\sum_{i\neq j\in R^{*}\cap V ^{+}}X_{i}X_{j}\leq\sum_{i\in R^{*}\cap V^{+}}X_{i}+(\sum_{i\in R^{*}\cap V^{+ }}X_{i})^{2}.\]Note that

\[\mathbb{E}[\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}|\mathcal{I}]^{2}1( \mathcal{F}\cap\mathcal{G}_{\delta}) =(\sum_{i\in R^{*}\cap V_{1}^{+}}\mathbb{P}(i\in S^{*},\sum_{j\in[n ]\setminus\bar{R}^{*}}A_{i,j}\sigma_{1}^{*}(j)<0|\mathcal{I}))^{2}1(\mathcal{F }\cap\mathcal{G}_{\delta})\] \[\geq(1-n^{-(2s^{2}-s^{3})\mathrm{T}_{c}(a,b)+2\delta})^{2}(\sum_ {i\in R^{*}\cap V_{1}^{+}}X_{i})^{2}1(\mathcal{F}\cap\mathcal{G}_{\delta}).\]

Hence we have

\[\mathrm{Var}(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}|\mathcal{I})1( \mathcal{F}\cap\mathcal{G}_{\delta})\] \[= \left(\mathbb{E}[(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i})^{2}| \mathcal{I}]-\mathbb{E}[\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}|\mathcal{I}]^{2} \right)1(\mathcal{F}\cap\mathcal{G}_{\delta})\] \[\leq \left(\sum_{i\in R^{*}\cap V_{1}^{+}}X_{i}+(\sum_{i\in R^{*}\cap V _{1}^{+}}X_{i})^{2}\left(1-(1-n^{-(2s^{2}-s^{3})\mathrm{T}_{c}(a,b)+2\delta})^ {2}\right)\right)1(\mathcal{F}\cap\mathcal{G}_{\delta})\] \[\leq \left(\sum_{i\in R^{*}\cap V_{1}^{+}}X_{i}+2(\sum_{i\in R^{*} \cap V_{1}^{+}}X_{i})^{2}n^{-(2s^{2}-s^{3})\mathrm{T}_{c}(a,b)+2\delta}\right) 1(\mathcal{F}\cap\mathcal{G}_{\delta})\] \[\leq \left(|R^{*}\cap V_{1}^{+}|n^{-s(1-s)^{2}\mathrm{D}_{+}(a,b)+o(1) }+2\left(|R^{*}\cap V_{1}^{+}|n^{-s(1-s)^{2}\mathrm{D}_{+}(a,b)+o(1)}\right)^{ 2}n^{-(2s^{2}-s^{3})\mathrm{T}_{c}(a,b)+2\delta}\right)1(\mathcal{F}\cap G_{ \delta})\] \[\leq (n^{\theta+\delta+o(1)}+2n^{2\theta+2\delta-(2s^{2}-s^{3}) \mathrm{T}_{c}(a,b)+2\delta+o(1)})1(\mathcal{F}\cap G_{\delta}).\]

The second inequality uses the fact that \(1-(1-n^{-x})^{2}\leq 2n^{-x}\). The third inequality exists by Lemma G.5, G.20. Now, for further simplying the upper bound, note that if \(\delta\) is small enough (specifically, \(\delta<\min((2s^{2}-s^{3})\mathrm{T}_{c}(a,b)/8,\theta/4)\)), then \(\theta+\delta+o(1)<2\theta-3\delta\), and \(2\theta+4\delta-(2s^{2}-s^{3})\mathrm{T}_{c}(a,b)+o(1)<2\theta-3\delta\). Hence

\[\mathrm{Var}(\sum_{i\in R^{*}\cap V_{1}^{+}}W_{i}|\mathcal{I})1(\mathcal{F} \cap\mathcal{G}_{\delta})\leq n^{2\theta-3\delta}1(\mathcal{F}\cap\mathcal{G }_{\delta}).\]

The proof for \(\mathrm{Var}(\sum_{i\in R^{*}\cap V_{1}^{-}}W_{i}|\mathcal{I})1(\mathcal{F} \cap\mathcal{G}_{\delta})\) is the same. 

## Appendix H Proof of Theorem 1 for \(K\) graphs

### Categorization of vertices

We start with the definition of categorizing vertices as "good" and "bad" vertices. To begin with, we define a metagraph for each vertex. Then we categorize each vertex as "good" and "bad" according to the connectivity of the metagraph for the vertex.

**Definition H.1**.: _Given \((G_{1},\ldots,G_{K})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n}{n},s)\), and \(\binom{K}{2}\) partial \(k\)-core matchings \(\widehat{\bm{\mu}}:=\{\widehat{\mu}_{ij}:i\neq j\in[K]\}\). For any vertex \(v\), define the following graph matching metagraph for the vertex \(v\), denoted it as \(\mathcal{MG}_{v}\). \(\mathcal{E}(\mathcal{MG}_{v})\) denotes the edge set in the graph \(\mathcal{MG}_{v}\). There are \(K\) nodes in \(\mathcal{MG}_{v}\), where node \(i\) represents the graph \(G_{i}\), and an edge exists between \((i,j)\), that is, \((i,j)\in\mathcal{E}(\mathcal{MG}_{v})\) if and only if vertex \(v\) can be matched in the partial matching \(\widehat{\mu}_{ij}\) between \(G_{i}\) and \(G_{j}\)._

Note that \(\mathcal{MG}_{v}\) is an undirected graph. This is because of an inherent symmetry in the definition of a \(k\)-core matching, which looks at the \(k\)-core of the intersection graph of the two matched graphs. Thus for \(k\)-core matchings, a vertex is matched by \(\widehat{\mu}_{ij}\) if and only if it is matched by \(\widehat{\mu}_{ji}\). This property does not necessarily hold for other graph matching algorithms.

**Definition H.2**.: _Given \((G_{1},\ldots,G_{K})\sim\mathrm{CSBM}(n,a\frac{\log n}{n},b\frac{\log n}{n},s)\), and \(\binom{K}{2}\) partial \(k\)-core matchings \(\widehat{\bm{\mu}}:=\{\widehat{\mu}_{ij}:i\neq j\in[K]\}\), we define a vertex \(v\) to be "good" if and only if \(\mathcal{MG}_{v}\) is connected. Conversely, a vertex \(v\) is "bad" if and only if \(\mathcal{MG}_{v}\) is disconnected._For a "bad" vertex \(v\), since \(\mathcal{MG}_{v}\) is disconnected, the metagraph has at least two disjointed components. Hence, there must exist two sets \(\Gamma_{g}(v),\Gamma_{b}(v)\) satisfying \(\Gamma_{g}(v)\cap\Gamma_{b}(v)=\emptyset,\Gamma_{g}(v)\cup\Gamma_{g}(b)=[K]\) and for any \(i\in\Gamma_{g}(v),j\in\Gamma_{b}(v)\), \((i,j)\) is not an edge in \(\mathcal{MG}_{v}\). In other words, \(v\) cannot be matched for any matching between the graph \(G_{i},i\in\Gamma_{g}(v)\) and the graph \(G_{j},j\in\Gamma_{b}(v)\). Heuristically, the definition implies that "bad" vertices cannot utilize the combined information for all \(K\) graphs.

Otherwise,the "good" vertices can utilize the combined information for all \(K\) graphs, as shown in Lemma H.3.

**Lemma H.3**.: _For a "good" vertex \(v\) defined in Definition H.2, for any two node i,j (represents two graphs \(G_{i}\), \(G_{j}\)), there exists a path \(i:=\ell_{0}-\ell_{1}-\ell_{2}\ldots-\ell_{d}:=j\) such that \(v\) can be matched for \(\widetilde{\mu}_{\ell_{m}\ell_{m+1}},m\in\{0,1,...,k-1\}\). Define \(\widehat{\pi}_{ij}(v):=\widehat{\mu}_{\ell_{d-1}\ell_{d}}\circ\widetilde{\mu }_{\ell_{d-2}\ell_{d-1}}\circ\ldots\circ\widehat{\mu}_{\ell_{1}\ell_{2}}\circ \widetilde{\mu}_{\ell_{0}\ell_{1}}(v)\)._

Proof.: For a "good" vertex \(v\), since \(\mathcal{MG}_{v}\) is connected, for any two nodes \(i,j\in[K]\), there exists a path \(\psi_{ij}(v):=\{\ell_{0}-\ell_{1}-\ldots-\ell_{d-1}-\ell_{d},\ell_{0}=i,\ell_ {d}=j,(\ell_{m},\ell_{m+1})\in\mathcal{E}(\mathcal{MG}_{v})\}\). We can use the path \(\psi_{ij}(v)\) to define the \(\widehat{\pi}_{ij}(v)\) 

**Remark:** Note that such a path is not unique. However, the choice of path does not matter whp. By Lemma E.5, for \(k\)-core partial matching, \(\widehat{\mu}_{ij}=\pi^{*}_{ij},i,j\in[K]\), with high probability. Hence, for two different paths with endpoints being \(i,j\in[K]\), denoted by \(\psi^{1}_{ij},\psi^{2}_{ij}\), we can define \(\widehat{\pi}^{1}_{ij},\widehat{\pi}^{2}_{ij}\) based on the two paths \(\psi^{1}_{ij},\psi^{2}_{ij}\) separately. Then, with high probability \(\widehat{\pi}^{1}_{ij}=\pi^{*}_{ij}=\widehat{\pi}^{2}_{ij}\).

By Lemma H.3 and its remark, we can have the union graph of \(K\) graphs \((G_{1}\lor G_{2}\lor G_{3}\vee\ldots\lor G_{K})\) for the "good" vertices, using the matchings \(\widehat{\boldsymbol{n}}:=(\widehat{\pi}_{12},\widehat{\pi}_{13},\ldots, \widehat{\pi}_{1K})\) where \(\widehat{\pi}_{1K}\) is defined in Lemma H.3. If there are multiple paths existing, pick the shortest one, and break ties in lexicographic order.

### Exact community recovery algorithm for \(K\) graphs

The key steps of the exact community recovery algorithm for \(K\) graphs are essentially the same as the algorithm for three graphs. Based on a given almost exact community recovery \(\widehat{\sigma}_{1}\) and \(\binom{K}{2}\)\(k\)-core matchings pairwise, we divide the vertices into two categories:"good" and "bad" vertices according to Definition H.1, H.2. Then refine the community label according to the majority votes for the "good" and "bad" vertices sequentially, to obtain the exact community recovery label under the given conditions. The full algorithm for exact community recovery is given in Algorithm 5:

### Analysis of the \(k\)-core estimator

Here we quantify the size of the"bad" vertices. Suppose that for vertex \(v\), \(\Gamma_{g}=\{G_{1},G_{2},\ldots,G_{L}\}\) and \(\Gamma_{b}=\{G_{L+1},G_{L+2},\ldots,G_{K}\}\), here \(1<L<K-1\), \(v\) cannot be matched for all the matchings between one graph from \(\Gamma_{a}\) and another graph from \(\Gamma_{b}\). Apparently, \(v\) cannot be matched for \(L(K-L)\) matchings. Heuristically, as the number of mathings that cannot be matched for vertices increases, the size of such vertices decreases, since every matching would match \((1-o(1))n\) vertices and only a very small fraction of vertices that cannot be matched. The following lemma demonstrated the claim.

Figure 4: Schematic showing the meta graph \(\mathcal{MG}_{v}\) when \(K=5\).

**Input:**\(K\) graphs \((G_{1},G_{2},\ldots,G_{K})\) on \(n\) vertices, \(k=13\), and \(\epsilon>0\).

**Output:** A labeling of \([n]\) given by \(\widehat{\sigma}\).

```
1:Apply [35, Algorithm 1] to the graph \(G_{1}\) and parameters \((sa,sb,\epsilon)\), obtaining a label \(\widehat{\sigma_{1}}\).
2:Apply Algorithm 1 on input \((G_{i},G_{j},k)\), obtaining a matching \((\widehat{M}_{ij},\widehat{\mu}_{ij}),i\neq j\in\{1,2,..,K\}\).
3:For "good" vertices \(v\), look at the set \(\Psi:=\{\widehat{\mu}_{ij},i,j\in[K]:(i,j)\in\mathcal{E}(\mathcal{MG}_{v})\}\), by Lemma H.3, we can define the \(\widehat{\bm{\pi}}:=(\widehat{\pi}_{12},\widehat{\pi}_{13},\ldots,\widehat{ \pi}_{1K})\) to obtain the union graph of \(K\) graphs based on the matchings from \(\Psi\), denote it as \((G_{1}\lor G_{2}\ldots\lor G_{K})_{\Psi}\). Denote \(M:=\cap M_{ij}\), where \((i,j)\) satisfying \(\widehat{\mu}_{ij}\in\mathcal{MG}_{v}\). Set \(\widehat{\sigma}(v)\in\{-1,1\}\) according to the neighborhood majority (resp., minority) of \(\widehat{\sigma_{1}}(v)\) with respect to the graph \((G_{1}\lor G_{2}\ldots\lor G_{K})_{\Psi}\{M\}\) if \(a>b\) (resp., \(a<b\)).
4:For "bad" vertices \(v\), denote \(\phi:=\{j\in[K]:(1,j)\in\mathcal{E}(\mathcal{MG}_{v})\}\). Denote \(M:=\cap_{i\in[K]}M_{1i}\), set \(\widehat{\sigma}(v)\in\{-1,1\}\) according to the neighborhood majority (resp., minority) of \(\widehat{\sigma}(v)\) with respect to the graph \(G_{1}\setminus_{j\notin\phi}G_{j}(M\cup\{v\})\) if \(a>b\) (resp., \(a<b\)).
5:Return \(\widehat{\sigma}:[n]\to\{-1,1\}\). ```

**Algorithm 5** Community Recovery for \(K\) graphs

**Lemma H.4**.: _Suppose that \(G_{1},G_{2},\ldots,G_{K}\) are independently subsampled with probability \(s\) from a parent graph \(G\sim\mathrm{SBM}(n,a\log n/n,b\log n/n)\) for \(a,b>0\). Let \(F^{*}_{ij}\) be the set of vertices outside the \(k\)-core of \(G_{i}\land_{\pi_{ij}}G_{j}\) with \(k=13\). For \(1\leq L\leq K-1\) and every \(\delta>0\), with probability \(1-o(1)\) we have that \(|\cap_{1\leq i\leq L,L+1\leq j\leq K}F^{*}_{ij}|\leq n^{1-s(1-(1-s)^{K-1}) \mathrm{T}_{\mathrm{c}}(a,b)+\delta}\)._

Proof.: Define \(U_{ij}\) to be the set of vertices with degree at most \(m+k\) in \(G_{i}\land_{\pi_{ij}}G_{j}\), where \(m>\frac{2}{(a+b)^{2}}\), as defined in Lemma E.1. Then by Lemma E.1, w.h.p. we have \(F^{*}_{ij}\subset U_{ij}\) and \(|\cap_{1\leq i\leq L,L+1\leq j\leq K}F^{*}_{ij}|\leq|\cap_{1\leq i\leq L,L+1 \leq j\leq K}U_{ij}|\). Now we look at the expectation:

\[\mathbb{E}\left[|\cap_{1\leq i\leq L,L+1\leq j\leq K}U_{ij}|\right]=n\mathbb{E }\left[\prod_{i=1}^{L}\prod_{j=L+1}^{K}\mathbf{1}_{v\in U_{ij}}\right].\]

Let \(D\) denote the degree of vertex \(v\) in the graph \(G_{1}\lor G_{3}\ldots\lor G_{L}\).

\[\mathbb{E}\left[\prod_{i=1}^{L}\prod_{j=L+1}^{K}\mathbf{1}_{v\in U _{ij}}\right] =\mathbb{E}\left[\mathbb{E}\left[\prod_{j=L+1}^{K}\prod_{i=1}^{L} \mathbf{1}_{v\in U_{ij}}\left|D\right|\right]\right]\] \[\leq\mathbb{E}\left[\left(\sum_{i=0}^{(m+k)L}\binom{D}{i}s^{i}(1- s)^{D-i}\right)^{K-L}\right]\]

The first equality is by the tower rule. The second inequality is due to two observations. Firstly, \(\{\prod_{i=1}^{L}\mathbf{1}_{v\in U_{ij}}\}\subseteq\{\deg(v)\leq L(m+k)\) in the graph \((G_{1}\lor G_{3}\ldots\lor G_{L})\wedge G_{j},L+1\leq j\leq K\}\). To be more detailed, if the degree of \(v\) is at most \(m+k\) in the graph \(G_{i}\wedge G_{j},1\leq i\leq L\), then the degree of \(v\) is at most \(L(m+k)\) in the graph \((G_{1}\lor G_{3}\ldots\lor G_{L})\wedge G_{j}\). The second observation is, given \(D\), for \(j_{1}\neq j_{2}\), the events \(\{\deg(v)\leq L(m+k)\) in the graph \((G_{1}\lor G_{3}\ldots\lor G_{L})\wedge G_{j_{1}}\},\{\deg(v)\leq L(m+k)\) in the graph \((G_{1}\lor G_{3}\ldots\lor G_{L})\wedge G_{j_{2}}\}\) are independent.

Similar to the proof of Lemma E.7, let \(X_{a}\sim\mathrm{Bin}((1+o(1))n/2,(1-(1-s)^{L})a\log n/n)\), and \(X_{b}\sim\mathrm{Bin}((1+o(1))n/2,(1-(1-s)^{L})b\log n/n)\). On the event \(\mathcal{F},D_{1}\overset{d}{=}X_{a}+X_{b}\), where \(\mathcal{F}\) is defined in Definition D.3, \(X_{a},X_{b}\) are independent. Note that by Lemma D.4, \(\mathbb{P}(\mathcal{F}^{c})=o(\frac{1}{n^{2}})\). We have

\[\mathbb{E}\left[\left(\sum_{i=0}^{m+k}\binom{D}{i}s^{i}(1-s)^{D-i }\right)^{K-L}\right]\\ =\sum_{i_{1},i_{2},\ldots,i_{K-L}=0}^{(m+k)L}C(i_{1},i_{2},\ldots, i_{K-L})\mathbb{E}[D^{\sum_{j=0}^{K-L}i_{j}}(1-s)^{(K-L)D}].\] (H.5)Here \(C(i_{1},i_{2},\ldots,i_{K-L})\) is a constant given \(i_{1},i_{2},\ldots,i_{K-L}\). Now look at \(\mathbb{E}[D^{N}(1-s)^{(K-L)D}]\). In our regime, \(N\leq(m+k)(K-L)L\) are constant. Hence:

\[\mathbb{E}[D^{N}(1-s)^{(K-L)D}\mathbf{1}_{\mathcal{F}}] =\mathbb{E}[(X_{a}+X_{b})^{N}(1-s)^{(K-L)X_{a}}(1-s)^{(K-L)X_{b}} \mathbf{1}_{\mathcal{F}}]\] \[=\sum_{t=0}^{N}C_{t}\mathbb{E}[X_{a}^{t}(1-s)^{(K-L)X_{a}}\mathbf{ 1}_{\mathcal{F}}]\mathbb{E}[X_{b}^{N-t}(1-s)^{(K-L)X_{a}}\mathbf{1}_{\mathcal{ F}}].\]

Here \(C_{t}\) is constant related to \(t\), the second equality is due to the independence of \(X_{a},X_{b}\). Now look at \(\mathbb{E}[X_{a}^{t}(1-s)^{(K-L)X_{a}}\mathbf{1}_{\mathcal{F}}]\).

\[\mathbb{E}[X_{a}^{t}(1-s)^{(K-L)X_{a}}\mathbf{1}_{\mathcal{F}}] \leq\mathbb{E}[X_{a}^{t}(1-s)^{(K-L)X_{a}}]\] \[=\sum_{\ell=0}^{(1+o(1))n/2}\ell^{t}(1-s)^{(K-L)\ell}(\frac{(1-(1 -s)^{L})a\log n}{n})^{\ell}(1-\frac{(1-(1-s)^{L})a\log n}{n})^{(1+o(1))n/2-\ell}\] \[=\sum_{\ell=0}^{(\log n)^{3}}\ell^{t}(\frac{(1-s)^{K-L}(1-(1-s)^{ L})a\log n}{n})^{\ell}(1-\frac{(1-(1-s)^{L})a\log n}{n})^{(1+o(1))n/2-\ell}\] \[+\sum_{\ell=(\log n)^{3}+1}^{(1+o(1))n/2}\ell^{t}(\frac{(1-s)^{K- L}(1-(1-s)^{L})a\log n}{n})^{\ell}(1-\frac{(1-(1-s)^{L})a\log n}{n})^{(1+o(1))n/2- \ell}.\]

Similarly, we can bound the first part:

\[\sum_{\ell=0}^{(\log n)^{3}}\ell^{t}(\frac{(1-s)^{K-L}(1-(1-s)^{L })a\log n}{n})^{\ell}(1-\frac{(1-(1-s)^{L})a\log n}{n})^{(1+o(1))n/2-\ell}\] \[\leq (\log n)^{3t}\sum_{\ell=0}^{(\log n)^{3}}(\frac{(1-s)^{K-L}(1-(1- s)^{L})a\log n}{n})^{\ell}(1-\frac{(1-(1-s)^{L})a\log n}{n})^{(1+o(1))n/2-\ell}\] \[\leq (\log n)^{3t}\sum_{\ell=0}^{(1+o(1))n/2}(\frac{(1-s)^{K-L}(1-(1-s )^{L})a\log n}{n})^{\ell}(1-\frac{(1-(1-s)^{L})a\log n}{n})^{(1+o(1))n/2-\ell}\] \[= (\log n)^{3t}(1-(1-(1-s)^{K-L})(1-(1-s)^{L})\frac{sa\log n}{n})^{( 1+o(1))n/2}\] \[\leq n^{-(1-(1-s)^{K-L})(1-(1-s)^{L})a/2+o(1)}.\]

Then we can bound the second part, using the similar arguments in Lemma E.7, we can show that

\[\sum_{\ell=(\log n)^{3}+1}^{(1+o(1))n/2}\ell^{t}(\frac{(1-s)^{K-L} (1-(1-s)^{L})a\log n}{n})^{\ell}(1-\frac{(1-(1-s)^{L})a\log n}{n})^{(1+o(1))n/2-\ell}\] \[=o(n^{-(1-(1-s)^{K-L})(1-(1-s)^{L})a/2+o(1)}).\]

Hence, by summing up the two parts, \(\mathbb{E}[X_{a}^{t}(1-s)^{(K-L)X_{a}}]\leq n^{-(1-(1-s)^{K-L})(1-(1-s)^{L})a/2 +o(1)}\). This is also true for \(X_{b}\). Then, \(\mathbb{E}[D^{N}(1-s)^{(K-L)D}]\leq n^{-(1-(1-s)^{K-L})(1-(1-s)^{L})\mathrm{T} _{c}(a,b)+o(1)}\).

For \(f(x)=(1-s)^{x}+(1-s)^{K-x},x\in[1,K-1]\), the function \(f(x)\) obtains its maximum at \(x=1,K-1\). Hence \(n^{((1-s)^{K-L}-1)((1-(1-(1-s)^{L})^{L})\mathrm{T}_{c}(a,b)+o(1)}\leq n^{-s(1-(1 -s)^{K-1})\mathrm{T}_{c}(a,b)+o(1)}\). Similar to Lemma E.7, by Markov inequality, the lemma follows immediately. 

**Lemma H.6**.: _The size of "bad" vertices defined in Definition H.2 can be upper bounded by \(n^{-s(1-(1-s)^{K-1})\mathrm{T}_{\mathrm{c}}(a,b)+o(1)}\)._

Proof.: By definition H.2, the meta graph \(\mathcal{MG}_{v}\) for"bad" vertex \(v\) are disconnected. Hence, there exists at least two components of \(\bar{\mathcal{MG}}_{v}\), where there are no edges between the nodes from two components. 

With all the preparations for "bad" vertices and "good" vertices, we are now ready to prove Theorem 1 for general \(K\) graphs.

### Exact recovery for "good" vertices

By Lemma D.1, we can directly deduce that if \((1-(1-s)^{K})D_{+}(a,b)>1+\epsilon|\log(a/b)|\), then for all \(i\in[n]\) we have that \(\mathsf{maj}_{G_{1}\lor G_{2}\ldots\lor G_{K}}(i)\geq\epsilon\log n\) w.h.p.

Now for a "good"vertex \(i\), there exists a corresponding matching set \(\Psi\) that \(i\) can be matched for all the matchings in \(\Psi\) and there is an union graph \(\widetilde{G}:=(G_{1}\lor G_{2}\vee\ldots\lor G_{3})\) that can be derived from the matchings in \(\Psi\). Let \(M\) be the set of vertices that can be matched for all matchings in \(\Psi\).

**Lemma H.7**.: _Suppose that \((1-(1-s)^{3})D_{+}(a,b)>1+2\epsilon|\log(a/b)|\). Then with probability \(1-o(1)\), all the "good" vertices in \(M\) have an \(\epsilon\log n\) majority in \(\widetilde{G}\{M\}\)._

Proof.: Denote \(F^{*}_{ij}\) the set of vertices outside the 13-core of \(G_{i}\wedge_{\pi^{*}_{ij}}G_{j}\). In light of Lemma E.5 and its remark, we can replace \(\mu_{ij}\) with \(\pi^{*}_{ij}\), \(F_{ij}\) with \(F^{*}_{ij}\),\(M\) with \(M^{*}\) in Lemma F.2. Where we define

\[G^{*}:=G_{1}\vee_{\pi^{*}_{12}}G_{2}\vee\ldots\vee_{\pi^{*}_{1K}}G_{K},\qquad H :=G^{*}\{M^{*}\}.\]

To bound the neighborhood majority in \(H\), for \(i\in M^{*}\) we have:

\[\mathsf{maj}_{H}(i) =\sigma^{*}(i)\sum_{j\in N_{H}(i)}\sigma^{*}(j)\leq\mathsf{maj}_{ G^{*}}(i)+\sum_{\tilde{\mu}_{j\in\Psi}}|N_{G^{*}}(i)\cap F^{*}_{j\ell}|,\] \[\mathsf{maj}_{H}(i) =\sigma^{*}(i)\sum_{j\in N_{H}(i)}\sigma^{*}(j)\geq\mathsf{maj}_{ G^{*}}(i)-\sum_{\tilde{\mu}_{j\in\ell}\Psi}|N_{G^{*}}(i)\cap F^{*}_{j\ell}|.\]

To sum up, we have

\[|\mathsf{maj}_{H}(i)-\mathsf{maj}_{G^{*}}(i)|\leq\sum_{\tilde{\mu}_{j\ell} \notin\Psi}|N_{G^{*}}(i)\cap F^{*}_{j\ell}|.\] (H.8)

Note that \(\mathsf{maj}_{G^{*}}(i)>2\epsilon\log n\), \(i\in[n]\) with probability \(1-o(1)\), given that \((1-(1-s)^{K})D_{+}(a,b)>1+2\epsilon|\log(a/b)|\) by Lemma D.1. Now we would like to prove that the right hand side of (H.8) can be bounded by \(\epsilon\log n\).

Note that \(|N_{G^{*}}(i)\cap F^{*}_{j\ell}|\leq|N_{G_{j}\wedge G_{\ell}}(i)\cap(F^{*}_{ j\ell})|+|N_{G^{*}\setminus(G_{j}\wedge G_{\ell})}(i)\cap(F^{*}_{j\ell})|\).

First, look at \(|N_{G_{j}\wedge G_{\ell}}(i)\cap(F^{*}_{j\ell})|\), by Lemma E.4, w.h.p.,

\[|N_{G_{j}\wedge G_{\ell}}(i)\cap F^{*}_{j\ell}|<\epsilon\log n/2K^{2}.\]

The remaining thing is to bound \(|N_{G^{*}\setminus(G_{j}\wedge G_{\ell})}(i)\cap(F^{*}_{j\ell})|\). Note that conditioned on \(\boldsymbol{\pi}^{*},\sigma^{*},\boldsymbol{\mathcal{E}}:=\{\mathcal{E}_{i_ {1}i_{2}\ldots i_{K}},i_{1},i_{2},\ldots,i_{K}\in\{0,1\}\}\), the graph \(G^{*}\setminus(G_{j}\wedge G_{\ell})\) is independent of \(F^{*}_{j\ell}\) by Lemma D.2, since \(F^{*}_{j\ell}\) depends only on \(G_{j}\wedge G_{\ell}\). Thus we can stochastically dominate \(|N_{G^{*}\setminus(G_{j}\wedge G_{\ell})}(i)\cap F^{*}_{j\ell}|\) by a Poisson random variable X with mean

\[\lambda_{n}:=\nu\frac{\log n}{n}|\{j\in F^{*}_{j\ell}:\{i,j\}\in G^{*}\setminus (G_{j}\wedge G_{\ell})|\leq\nu\frac{\log n}{n}|F^{*}_{j\ell}|,\nu:=max(a,b).\]

For a fixed \(\delta>0\), define an event \(\mathcal{Z}:=\{|F^{*}_{j\ell}|\leq n^{1-s^{2}\mathrm{T}_{\mathrm{c}}(a,b)+ \delta}\}\). On \(\mathcal{Z}\), \(\lambda_{n}\leq n^{-s^{2}\mathrm{T}_{\mathrm{c}}(a,b)+\delta+o(1)}\). Hence, for any positive integer \(m\):

\[\mathbb{P}(\{|N_{G^{*}\setminus(G_{j}\wedge G_{\ell})}(i)\cap(F^{ *}_{j\ell})|\geq m\}\cap\mathcal{Z}\leq\mathbb{P}(\{X\geq m\}\cap\mathcal{Z})= \mathbb{E}[\mathbb{P}(X\geq m|F^{*}_{j\ell},\boldsymbol{\mathcal{E}},\sigma^{* },\boldsymbol{\pi}^{*})\mathbf{1}_{\mathcal{Z}}]\\ \leq\mathbb{E}[(\inf_{\theta>0}e^{-\theta m+\lambda_{n}(e^{\theta} -1)})\mathbf{1}_{\mathcal{Z}}]\leq\mathbb{E}[\varepsilon\lambda_{n}^{m}\mathbf{1 }_{\mathcal{Z}}]\leq n^{-m(s^{2}\mathrm{T}_{\mathrm{c}}(a,b)-\delta-o(1))}.\]

Above, the equality on the second line is due to the tower rule and since \(\mathcal{Z}\) is measurable with respect to \(|F^{*}_{j\ell}|\), the inequality on the third line is due to a Chernoff bound; the inequality on the fourth line follows from setting \(\theta=\log(1/\lambda_{n})\) (which is valid since \(\lambda_{n}=o(1)\) if \(\mathcal{Z}\) holds). The final inequality uses the upper bound for \(\lambda_{n}\) on \(\mathcal{Z}\). Taking a union bound, we have

\[\mathbb{P}(\{\exists i\in[n],|N_{G^{*}\setminus(G_{j}\wedge G_{\ell})}(i)\cap F^ {*}_{j\ell}|\geq m\}\cap\mathcal{Z})\leq n^{1-m(s^{2}\mathrm{T}_{\mathrm{c}}(a,b )-\delta-o(1))}.\]

Here if we take \(m>(s^{2}\mathrm{T}_{\mathrm{c}}(a,b))^{-1}\) and \(\delta<s^{2}\mathrm{T}_{\mathrm{c}}(a,b)-m^{-1}\), the probability turns to \(o(1)\). Thus, we can set \(m=\lceil(s^{2}\mathrm{T}_{\mathrm{c}}(a,b))^{-1}\rceil+1\). In light of Lemma E.6, \(|F^{*}_{j\ell}|\leq n^{1-s^{2}\mathrm{T}_{\mathrm{c}}(a,b)+\delta},\delta>0\) w.h.p. Hence, the event \(\mathcal{Z}\) happens with probability \(1-o(1)\). Hence we have

\[\mathbb{P}(\{\forall i\in[n],N_{G^{*}\setminus(G_{j}\wedge G_{\ell})}(i)\cap F^ {*}_{j\ell}|\leq\lceil(s^{2}\mathrm{T}_{\mathrm{c}}(a,b))^{-1}\rceil\})=1-o(1).\]Hence we have, with probability \(1-o(1)\), for \(i\in M^{*}\)

\[|\mathsf{maj}_{H}(i)-\mathsf{maj}_{G^{*}}(i)|<\sum_{\widehat{\mu}_{j}\notin \Psi}(\epsilon\log n/2K^{2}+\lceil(s^{2}\mathrm{T}_{\mathrm{c}}(a,b))^{-1} \rceil)<\epsilon\log n,\]

and hence with probability \(1-o(1)\),

\[\mathsf{maj}_{H}(i)>\epsilon\log n.\]

Then by Lemma E.5, we can replace \(H\) with \(\widetilde{G}\), \(F_{ij}^{*}\) with \(F_{ij}\), the lemma follows. 

Next, prove that each vertex in \(G^{*}\setminus_{\pi_{12}^{*}}G_{1}\) has a small number of neighbors in \(I_{\epsilon}(G_{1})\).

**Lemma H.9**.: _If \(0<\epsilon\leq\frac{s\mathrm{D}_{+}(a,b)}{4\lfloor\log(a/b)\rfloor}\), then_

\[\mathbb{P}(\forall i\in[n],|N_{G^{*}\setminus_{\pi_{12}^{*}}G_{1}}(i)\cap I_{ \epsilon}(G_{1})|\leq 2\lceil(s\mathrm{D}_{+}(a,b))^{-1}\rceil)=1-o(1).\]

Proof.: Since \(I_{\epsilon}(G_{1})\) depends on \(G_{1}\) alone, it follows that \(I_{\epsilon}(G_{1})\) and \(G^{*}\setminus_{\pi_{12}^{*}}G_{1}\) are conditionally independent given \(\boldsymbol{\pi}^{*},\sigma^{*},\boldsymbol{\mathcal{E}}\). Hence we can stochastically dominate \(|N_{G^{*}\setminus_{\pi_{12}^{*}}G_{1}}(i)\cap I_{\epsilon}(G_{1})|\) by a Poisson random variable X with mean \(\lambda_{n}\) given by

\[\lambda_{n}:=\nu\log n/n|\{j\in I_{\epsilon}(G_{1}):\{i,j\}\in G^{*}\setminus G _{1}\}|\leq\nu\log n/n|I_{\epsilon}(G_{1})|.\]

Next, define the event \(\mathcal{Z}:=\{|I_{\epsilon}(G_{1})|\leq n^{1-s\mathrm{D}_{+}(a,b)+2\epsilon \lfloor\log(a/b)\rfloor}\}\).

Notice that \(P(\mathcal{Z})=1-o(1)\) by Lemma D.7 and Markov's inequality, provided \(s\mathrm{D}_{+}(a,b)<99\). Following identical arguments as the proof of Lemma H.7, we arrive at

\[\mathbb{P}(\exists i\in[n],|N_{G^{*}\setminus_{\pi_{12}^{*}}G_{1}}(i)\cap I_{ \epsilon}(G_{1})|\geq m)=o(1)\]

when \(m>\lceil(s\mathrm{D}_{+}(a,b)-2\epsilon\lfloor\log a/b\rfloor)^{-1}\rceil\). If \(\epsilon\leq\frac{s\mathrm{D}_{+}(a,b)}{4\lfloor\log(a/b)\rfloor}\), then it suffices to set \(m=2\lceil(s\mathrm{D}_{+}(a,b))^{-1}\rceil+1\). 

**Lemma H.10**.: _Suppose that \(a,b,\epsilon>0\) satisfy the following conditions:_

\[(1-(1-s)^{K})\mathrm{D}_{+}(a,b)>1+2\epsilon|\log a/b|,0<\epsilon\leq\frac{s \mathrm{D}_{+}(a,b)}{4\lfloor\log a/b\rfloor}.\]

_With high probability, the algorithm correctly labels all vertices in \(\{i\in[n]\setminus M^{*}\}\)._

Proof.: Compare the neighborhood majority in \(H\) corresponding to \(\widehat{\sigma_{1}}\) with the true majority in \(H\), where \(H\) is defined in Lemma F.2:

\[|\sigma^{*}(i)\sum_{j\in N_{H}(i)}(\widehat{\sigma_{1}}(j)- \sigma^{*}(j))|\leq|N_{H}(i)\cap I_{\epsilon})|\leq|N_{G^{*}}(i)\cap I_{ \epsilon}(G_{1})|\] \[\leq|N_{G^{*}\setminus G_{1}}(i)\cap I_{\epsilon}(G_{1})|+|N_{G_ {1}}(i)\cap I_{\epsilon}(G_{1})|\leq 2\lceil\mathrm{D}_{+}(a,b)^{-1} \rceil+2\lceil(s\mathrm{D}_{+}(a,b))^{-1}\rceil\leq\epsilon\log n/2.\]

The first inequality uses Lemma D.5 that the set of errors are contained in \(I_{\epsilon}(G_{1})\). The last inequality is due to Lemma D.8, H.9. Notice that \(\mathsf{maj}_{H}(i)\geq\epsilon\log n\) for \(i\in M^{*}\). Hence, \(\sigma^{*}(i)\sum_{j\in N_{H}(i)}\widehat{\sigma}_{1}(j)\geq\mathsf{maj}_{H} (i)-|\sigma^{*}(i)\sum_{j\in N_{H}(i)}(\widehat{\sigma_{1}}(j)-\sigma^{*}(j))| \geq\epsilon\log n/2>0\), which implies that the sign of neighborhood majorities are equal to the truth community label for any \(i\in M^{*}\), with probability \(1-o(1)\). Then we can convert \(H\) to \(\widetilde{G}\{M\}\), the vertices in \(M\) are correctly labeled with probability \(1-o(1)\). 

Using an identical proof, we can argue that the algorithm correctly labels all "good" vertices with probability \(1-o(1)\).

### Exact recovery for "bad" vertices

**Lemma H.11**.: _Suppose that \(a,b,\epsilon>0\) satisfy the following conditions:_

\[(1-(1-s)^{K})\mathrm{D}_{+}(a,b)>1+2\epsilon|\log a/b|,\;0< \epsilon\leq\frac{s\mathrm{D}_{+}(a,b)}{4|\log a/b|},\] \[s(1-(1-s)^{K-1})\mathrm{T}_{\mathrm{c}}(a,b)+s(1-s)^{K-1}\mathrm{D}_{+}( a,b)>1.\]

_With high probability, the algorithm correctly labels all "bad" vertices._

Proof.: For vertex \(i\) that are "bad", denote \(\psi:=\{j\in[K]:i\text{ cannot be matched through }\widehat{\mu}_{1j},j\neq 1\}\). Denote \(F_{b}\) as the vertex set of all the "bad" vertices that have the same \(\psi\) with vertex \(i\). Denote \(M^{*}:=\cap_{i\in[K]}M^{*}_{1i}\): define \(H_{i}:=(G_{1}\setminus_{\pi^{*}_{12}}G_{2}\setminus_{\pi^{*}_{13}}G_{3} \ldots\setminus_{\pi^{*}_{1K}}^{*}G_{K})\{M\cup\{i\}\}\). Let \(E_{i}\) be the event that i has a majority of at most \(\epsilon^{\prime}\log n\) in the graph \(H_{i}\). Let \(\widehat{\sigma}\) be the labeling after the step. For brevity, define a "nice" event based on the previous results. Define the event \(\mathcal{H}\), which holds if and only if:

* \(F_{ij}=F^{*}_{ij}\);
* \(\widehat{\sigma}(i)=\sigma^{*}(i)\) for all \(i\in M^{*}\);
* The event \(\mathcal{F}\) holds;
* \(|F_{b}|\leq n^{1-s(1-(1-s)^{K-1})\mathrm{T}_{\mathrm{c}}(a,b)+\delta}\).

By Lemma E.5, H.4, D.4, H.10, the event \(\mathcal{H}\) holds with probability \(1-o(1)\). Furthermore, define \(E^{*}_{i}:=\mathsf{maj}_{H_{i}}(i)\leq\epsilon^{\prime}\log n\), we have that

\[\mathbb{P}(\cup_{i\in[n]}(\{i\in F_{b}\}\cap E_{i}))\\ \leq\mathbb{P}((\cup_{i\in[n]}(\{i\in F^{*}_{b}\}\cap E^{*}_{i}) )\cap\mathcal{H})+\mathbb{P}(\mathcal{H}^{c})\\ \leq\sum_{i=1}^{n}\mathbb{P}(\{i\in F^{*}_{b}\}\cap E^{*}_{i} \cap\{F^{*}_{b}\leq n^{1-s(1-(1-s)^{K-1})\mathrm{T}_{\mathrm{c}}(a,b)+\delta} \}\cap\mathcal{F})+o(1).\]

By the tower rule, rewrite the term in the right hand side as:

\[\mathbb{E}\left[\mathbb{P}\left(E^{*}_{i}\mid\pi^{*},\sigma^{*},\bm{\mathcal{ E}},F^{*}_{b}\right)\mathbf{1}_{i\in F^{*}_{b}}\mathbf{1}_{\{|F^{*}_{b}|\leq n ^{1-s(1-(1-s)^{K-1})\mathrm{T}_{\mathrm{c}}(a,b)+\delta}\}\cap\mathcal{F}} \right].\] (H.12)

Now look at \(\mathbb{P}\left(E^{*}_{i}\mid\pi^{*},\sigma^{*},\bm{\mathcal{E}},F^{*}_{b}\right)\). Conditional on \(\bm{\mathcal{E}},\sigma^{*},\pi^{*}\), \(\mathsf{maj}_{H_{i}}(i):=\stackrel{{ d}}{{=}}Y-Z\), where \(Y,Z\) are independent with:

\[Y\sim\mathrm{Bin}(|j\in M^{*}:\{i,j\}\in\mathcal{E}_{100...0} \cap\mathcal{E}^{+}(\sigma^{*})|,a\log n/n),\] \[Z\sim\mathrm{Bin}(|j\in M^{*}:\{i,j\}\in\mathcal{E}_{100...0} \cap\mathcal{E}^{-}(\sigma^{*})|,b\log n/n).\]

By the Definition D.3 of the event \(\mathcal{F}\), we know that \(|j\in M^{*}:\{i,j\}\in\mathcal{E}_{100...0}\cap\mathcal{E}^{-}(\sigma^{*})|=(1- o(1))s(1-s)^{K-1}n/2\) and \(|j\in M^{*}:\{i,j\}\in\mathcal{E}_{100...0}\cap\mathcal{E}^{+}(\sigma^{*})|=(1- o(1))s(1-s)^{K-1}n/2\).

Lemma D.1 implies that

\[\mathbb{P}(E^{*}_{i}|\pi^{*},\sigma^{*},\bm{\mathcal{E}},F^{*}_{ b})\mathbf{1}_{i\in F^{*}_{b}}\mathbf{1}_{\{|F^{*}_{b}|\leq n^{1-s(1-(1-s)^{K-1}) \mathrm{T}_{\mathrm{c}}(a,b)+\delta}\}\cap\mathcal{F}}\\ \leq n^{-s(1-s)^{K-1}\mathrm{D}_{+}(a,b)+\epsilon^{\prime}\log( a/b)/2+o(1)}.\]

Follow (H.12) and take a union bound, we have that

\[\sum_{i=1}^{n}\mathbb{P}(\{i\in F^{*}_{b}\}\cap E^{*}_{i}\cap\{F^ {*}_{b}\leq n^{1-s(1-(1-s)^{K-1})\mathrm{T}_{\mathrm{c}}(a,b)+\delta}\}\cap \mathcal{F})+o(1)\] \[\leq n^{-s(1-s)^{K-1}\mathrm{D}_{+}(a,b)+\epsilon^{\prime}\log(a/b)/2+o (1)}\mathbb{E}[|F^{*}_{b}|\mathbf{1}_{F^{*}_{b}\leq n^{1-s(1-(1-s)^{K-1}) \mathrm{T}_{\mathrm{c}}(a,b)-s(1-s)^{K-1}}}]\] \[\leq n^{1-s(1-(1-s)^{K-1})\mathrm{T}_{\mathrm{c}}(a,b)-s(1-s)^{K-1} \mathrm{D}_{+}(a,b)+\epsilon^{\prime}\log(a/b)/2+\delta}.\]Under the condition \(s(1-(1-s)^{K-1})\mathrm{T}_{\mathrm{c}}(a,b)+s(1-s)^{K-1}\mathrm{D}_{+}(a,b)>1\), we can choose \(\epsilon^{\prime},\delta\) small enough so that the right hand side is \(o(1)\). \(\mathsf{maj}_{H_{i}}(i)>\epsilon^{\prime}\log n\) for \(i\in F_{b}^{*}\), by Lemma E.5, \(\mathsf{maj}_{\widehat{H_{i}}}(i)>\epsilon^{\prime}\log n\) for \(i\in F_{b}\).

Note that \(i\) cannot be matched for all \(\widehat{\mu}_{1i},i\in\psi\). Hence \(i\) has at most 12 neighbors in the graph \((G_{1}\wedge_{\pi_{1i}^{*}}G_{i})\). Therefore for any \(i\in F_{b}\) has at least \(\epsilon^{\prime}\log n-12|\psi|\) majority in \(G_{1}\setminus_{\widehat{\mu}_{1j},j\notin\psi}G_{j}\{M\cup\{i\}\}\) with high probability. Hence, we can correctly label all vertices in \(F_{b}\) with high probability.

Use the same arguments for all types of "bad" vertices, we can correctly label all "bad" vertices. 

## Appendix I Proof of impossibility for \(K\) graphs

We study the MAP (maximum a posterior) estimator for the communities in \(G_{1}\). Even with the additional information provided, including all the correct community labels in \(G_{2}\), the true matching \(\pi_{23}^{*},\pi_{24}^{*},\ldots,\pi_{2K}^{*}\) and most of the true matching \(\pi_{12}^{*}\), the MAP estimator fails to exactly recovery communities with probability bounded away form 0 if the condition (G.1) holds. The proof can be derived by generalizing proof of impossibility for three graphs. The only difference is that we are considering \(K\) correlated \(\mathrm{SBM}\ G_{1},G_{2},\ldots,G_{K}\). Since we know the true matching \(\pi_{i,j}^{*},i,j\in\{2,3,\ldots,K\}\), we can consider \(H:=G_{2}\lor G_{3}\ldots\lor G_{K}\sim\mathrm{SBM}(n,(1-(1-s)^{K-1})a\log n/n, (1-(1-s)^{K-1})b\log n/n)\). Denote \(R_{ij}\) the singleton in \(G_{i}\wedge G_{j}\). Then \(R=R_{12}\wedge R_{13}\ldots\wedge R_{1K}\) is the singleton set in \(G_{1}\wedge H\). The proof follows the same arguments with more involved notation, and hence we omit the details. Here we point out the differences of the proof for \(K\) graphs.

Define \(R_{\pi}:=R(\pi,A,B^{2},...,B^{K}):=\{i\in[n]:\forall j\in[n],A_{i,j}D_{\pi(i) \pi(j)}=0,D=\max(B^{2},....,B^{K})\}\), here \(B^{i}\) is the adjacent matrix of \(G_{k}\). Similar to Definition G.2, we can define \(S_{\pi}=S(\pi,A,B^{2},..,B^{K})\). Let \(G_{\delta}\) be the event that the following inequalities all hold:

\[n^{1-s(1-(1-s)^{K-1})T_{\mathrm{c}}(a,b)-\delta} \leq|R^{*}\cap V_{1}^{+}|,|R^{*}\cap V_{1}^{-}|,|\bar{R}^{*}\cap V _{1}^{+}|,|\bar{R}^{*}\cap V_{1}^{+}|\] \[\leq n^{1-s(1-(1-s)^{K-1})T_{\mathrm{c}}(a,b)+\delta}.\]

We can prove similar versions of Lemma G.7 and G.8, with \((2s^{2}-s^{3})\) replaced by \(s(1-(1-s)^{K-1})\) and \(s(1-s)^{2}\) replaced by \(s(1-s)^{K-1}\). We can have same versions of Lemma G.9, Corollary G.12. We can similarly define \(\mu^{+}(\pi)_{i_{1}i_{2}...i_{K}}\) and \(\mu^{-}(\pi)_{i_{1}i_{2}...i_{K}}\) for all \(i_{1},\ldots,i_{K}\in\{0,1\}\), and \(\nu^{+}(\pi)\) and \(\nu^{-}(\pi)\). When deriving the posterior distribution of \(\pi_{12}^{*}\), similar to Lemma G.14, the information of \(A,B^{2},\ldots,B^{K},\sigma_{2}^{*},S^{*},\pi_{12}^{*}\{[n]\setminus S^{*}\}, \pi^{*}:=\{\pi_{ij}^{*},i,j\in\{2,3,\ldots,K\}\}\) are given. Note that for \(\pi\in\mathcal{A}^{*}\), we have that \(\mu^{+}(\pi)_{i_{1}i_{2}...i_{K}}\) and \(\mu^{-}(\pi)_{i_{1}i_{2}...i_{K}}\) are constant for all \(i_{2},\ldots,i_{K}\in\{0,1\}\) except for \(\mu^{+}(\pi)_{10...0}\) and \(\mu^{-}(\pi)_{10...0}\). We can derive an analogue of Lemma G.14 with \(p_{100}\) replaced by \(p_{100...0}\), \(p_{000}\) replaced by \(p_{000...0}\) and similar for \(q\). Then we have analogous versions of Lemmas G.15, G.16, and G.17, with \(p_{100}\) replaced by \(p_{100...0}\), \(p_{000}\) replaced by \(p_{000...0}\), and similar for \(q\). Note that \(\frac{p_{100...0}q_{000...0}}{p_{000...0}q_{100...0}}=(1+o(1))\frac{a}{b}\). The impossibility proof for Theorem 2 follows.

## Appendix J Proofs for exact graph matching

### Exact graph matching for \(K\) graphs

Proof.: Through the \(13\)-core matching in Algorithm 1, we obtain \(\widetilde{\pi}:=\{\widehat{\mu}_{ij},i,j\in[K]\}\), where \(\widehat{\mu}_{ij}\) is the \(13\)-core matching between the graph \(G_{i}\) and \(G_{j}\).

Recall Definition H.2, we can directly infer that the "good" vertices are those which can be matched for \(K\) graphs through a path across \(13\)-core estimators \(\widetilde{\pi}\). We can define a new estimator \(\widehat{\pi}\) for those "good" vertices using the combination of \(13\)-core estimator through the path that connects all \(K\) graphs. The path is defined as in Lemma H.3. For any "good" vertex, such path exists and we can define the estimator \(\widehat{\pi}\) for that vertex.

Note that, by Lemma E.5, with high probability \(\widehat{\mu}_{ij}=\pi_{ij}^{*}\). Hence if for all the matched vertices, they will be matched correctly. If the number of "bad" vertices approaches zero, it indicates that all vertices are correctly matched. Consequently, exact graph matching can be achieved through the 13-core matching algorithm. By Lemma H.4, H.6, we can quantify the size of "bad" vertices: for every \(\delta>0\) we have that \(|\cap_{1\leq i\leq L,L+1\leq j\leq K}F_{ij}^{*}|\leq n^{1-s(1-(1-s)^{K-1}) \mathrm{T}_{\mathrm{c}}(a,b)+\delta}\). When \(1-s(1-(1-s)^{K-1})\mathrm{T}_{\mathrm{c}}(a,b)+\delta\), we can compute the size of "bad" vertices.

\((1-s)^{K-1})\mathrm{T}_{c}(a,b)>1\), that is, when the condition (3.5) holds, the number of "bad" vertices goes to zero when \(n\) goes to infinity, with high probability. Thus all vertices can be correctly matched and exact graph matching for \(K\) graphs is possible with high probability. 

### Impossibility of exact graph matching for \(K\) graphs

Proof.: Now we consider the graph matching problem with additional information provided, including the true correspondences \(\pi_{2}^{*},\ldots,\pi_{s}^{*}\) and the community label \(\sigma^{*}\). Then we obtain the union graph \(H:=G_{2}\vee_{\pi_{2}^{*}}G_{3}\vee\ldots\vee_{\pi_{2K}^{*}}G_{K}\). We now prove the impossibility by contradictory. Suppose that there exists an estimator \(\widehat{\pi}\) which can exactly match \(G_{1},G_{2},...,G_{K}\), note that \(H\sim\mathrm{SBM}(n,\frac{(1-(1-s)^{K-1})a\log n}{n},\frac{(1-(1-s)^{K-1})b \log n}{n})\). One key point, is that, we can subsample \(H_{2}^{\prime},H_{3}^{\prime},...,H_{K}^{\prime}\) from \(H\). To be more specific, consider the following parameter: \(r_{i_{1},i_{2},...,i_{K}}=\frac{\sum_{j=1}^{K^{-1}}i_{j}(1-s)^{K-\sum_{j=1}^{ K}i_{j}}}{1-(1-s)^{K-1}}\) where \(i_{1},i_{2},\ldots,i_{K}\in\{0,1\}\) and \(\sum_{j=1}^{K}i_{j}>0\). Here \(\sum r_{i_{1},i_{2},...,i_{K}}=1\). Then for any vertex pair \((i,j)\):

1. If \((i,j)\) is not an edge in \(H\), then \((i,j)\) is not an edge in \(H_{2}^{\prime},H_{3}^{\prime},\ldots,H_{K}^{\prime}\).
2. If \((i,j)\) is an edge in \(H\), with probability \(r_{i_{1},i_{2},...,i_{K}},(i,j)\) is an edge in the graphs \(\{H_{i_{j}}^{\prime}\}\) where \(i_{j}=1\) in \(r_{i_{1},i_{2},...,i_{K}}\) and \((i,j)\) is not an edge in the graphs \(\{H_{i_{j}}^{\prime}\}\) where \(i_{j}=0\) in \(r_{i_{1},i_{2},...,i_{K}}\).

Following the subsampling described as above, we can simulate \(H_{2}^{\prime},\ldots,H_{K}^{\prime}\) from \(H\). Note that by construction, \((G_{1},G_{2}^{\prime},G_{3}^{\prime},\ldots,G_{K}^{\prime})\) has the same distribution as \((G_{1},H_{2}^{\prime},\ldots,H_{K}^{\prime})\). Then after independent permutations, we can obtain \((H_{3},H_{4},\ldots,H_{K})\) by relabeling the vertex index in \((H_{3}^{\prime},H_{4}^{\prime},\ldots,H_{K}^{\prime})\). Note that \(H_{2}=H_{2}^{\prime}\). Then \((G_{1},G_{2},\ldots,G_{K})\) has the same distribution as \((G_{1},H_{2},H_{3},\ldots,H_{K})\). Since the estimator \(\widehat{\pi}\) can exactly match \((G_{1},G_{2},\ldots,G_{K})\) with high probability, it can also exactly match \((G_{1},H_{2},H_{3},\ldots,H_{k})\) with high probability. Naturally, it can exactly match vertices in \(G_{1}\) and \(H_{2}\), since \(H\) and \(H_{2}\) share the same vertex index then we can have an estimator that exactly match \(G_{1}\) and \(H\) given \(G_{1}\) and \(H\), where \(G_{1},H\) are correlated SBMs, independently subsampling from the parent graph \(G\) with probability \(s_{1}=s\) for \(G_{1}\) and \(s_{2}=1-(1-s)^{K-1}\) for \(H\). However, [14, Theorem 1] proves that suppose \((G_{1},G_{2})\sim\mathrm{CSBM}(n,\frac{a\log n}{n},\frac{b\log n}{n},s_{1},s _{2})\) subsampling from \(G\sim\mathrm{SBM}(n,\frac{a\log n}{n},\frac{b\log n}{n})\) with probability \(s_{1}\) and \(s_{2}\), respectively, if \(s_{1}s_{2}\mathrm{T}_{c}(a,b)<1\), then exact graph matching between \(G_{1}\) and \(G_{2}\) is impossible. Directly applying [14, Theorem 1] we have that exact graph matching between \(G_{1}\) and \(H\) is impossible if \(s(1-(1-s)^{K-1})<1\). This is a contradiction, and hence exact graph matching for \(G_{1},\ldots,G_{K}\) is impossible.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: As stated in the abstract and introduction, the main contribution of our paper is to derive the precise information-theoretic threshold for exact community recovery given \(K\) correlated stochastic block models, for any constant \(K\geq 3\). This is exactly the content of Theorem 1 and Theorem 2 in Section 3 (as discussed in detail in Sections 2 and 3). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss limitations, and possible future work that may address these, in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We clearly define the model and questions that we study in Section 2, and we fully state our main theoretical results, including all assumptions, in Theorem 1, Theorem 2, Theorem 3, and Theorem 4. For all theorems, we provide a brief overview of the proof in Section 4, and we give the full proof in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The current work is primarily theoretical. Nonetheless, broader societal impacts, both positive and negative, are discussed in Section 1 and Section 5. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not involve any data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Relevant prior work on models (all theoretical) is cited and discussed in detail throughout the paper, following the norms of the research literature. No code or data is used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with Human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.