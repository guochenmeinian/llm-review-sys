# DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios

Junchao Wu\({}^{1}\) Runzhe Zhan\({}^{1}\) Derek F. Wong\({}^{1}\)1 Shu Yang\({}^{1}\)

Xinyi Yang\({}^{1}\) Yulin Yuan\({}^{2}\) Lidia S. Chao\({}^{1}\)

\({}^{1}\)NLP\({}^{2}\)CT Lab, Department of Computer and Information Science, University of Macau

\({}^{2}\)Department of Chinese Language and Literature, University of Macau

nlp2ct.(junchao,runzhe,shuyang,xinyi)@gmail.com

{derekfw,yulinyuan,lidiasc}@um.edu.mo

Corresponding author

###### Abstract

Detecting text generated by large language models (LLMs) is of great recent interest. With zero-shot methods like DetectGPT, detection capabilities have reached impressive levels. However, the reliability of existing detectors in real-world applications remains underexplored. In this study, we present a new benchmark, _DetectRL_, highlighting that even state-of-the-art (SOTA) detection techniques still underperformed in this task. We collected human-written datasets from domains where LLMs are particularly prone to misuse. Using popular LLMs, we generated data that better aligns with real-world applications. Unlike previous studies, we employed heuristic rules to create adversarial LLM-generated text, simulating various prompts usages, human revisions like word substitutions, and writing noises like spelling mistakes. Our development of _DetectRL_ reveals the strengths and limitations of current SOTA detectors. More importantly, we analyzed the potential impact of writing styles, model types, attack methods, the text lengths, and real-world human writing factors on different types of detectors. We believe _DetectRL_ could serve as an effective benchmark for assessing detectors in real-world scenarios, evolving with advanced attack methods, thus providing more stressful evaluation to drive the development of more efficient detectors2.

Footnote 2: Data and code are publicly available at: https://github.com/NLP2CT/DetectRL

## 1 Introduction

Detecting text generated by LLMs is a challenging task. It is often more difficult for humans than for detection techniques to identify LLM-generated text, as humans typically underperform detection methods designed for this purpose [1]. Recently, the implications of LLM-generated content have come into focus, highlighting their significant societal and academic impacts and associated risks [2; 3]. The main concerns stem from the hallucinations and misuse of LLMs [4], leading to issues such as plagiarism [5], the spread of fake news [6], and challenges to educators and human scholarship in AI-assisted academic writing [7]. Previous and current popular detection benchmarks, such as TuringBench [8], MGTBench [9], MULTITuDE [10], MAGE [11] and M4 [12], have primarily focused on evaluating detectors' performance across various domains, generative models, and languages by constructing idealized test data. However, they have overlooked the assessment of detectors' capabilities in more common scenarios encountered in practical applications [4], such as various prompt usages, human revisions, and writing noises, as shown in Table 1.

In this paper, we study the following questions: **(1) How do SOTA LLM-generated text detectors perform in real-world application scenarios? (2) What real-world factors influence detectorperformance, and to what extent?** We investigate these questions by introducing _DetectRL_, a novel benchmark for LLM-generated text detection. We achieve this by crafting challenges that are commonly encountered in real-world scenarios. These challenges simulate various prompts usages of human, human revisions of text such as word substitutions, and adversarial writing noises, including spelling mistakes. To enhance these simulations, we incorporate well-designed attack methods like prompt-based attacks, paraphrasing, adversarial perturbations, and data mixing. We selected data from domains where LLMs are frequently used and prone to abuse, such as academic writing, news writing, creative writing, and social media, to serve as samples of human-written text. To create LLM-generated texts that closely resemble real-world application scenarios, we employed powerful and widely used LLMs, including GPT-3.5-turbo [13], PaLM-2-bison [14], Claude-instant [15], and Llama-2-70b [16]. Furthermore, to ensure a wider diversity of text length, we filtered out shorter texts and applied a varying length augmentation method. This approach significantly broadened the range of text lengths available for detection, enhancing the practical value of the task. We balanced sample distributions across domains, LLMs, and attack types in all test scenarios to enhance diversity, thereby creating more challenging evaluations. These distribution variances are common in real-world scenarios but are often overlooked in ideal test environments where current detectors are developed.

The construction of this benchmark was highly effective in achieving our goals. **The experimental results present a significant challenge to existing detection methods.** Current detectors, particularly those employing zero-shot techniques, often struggle with accurately identifying LLM-generated texts. For example, adversarial perturbation attacks reduce the performance of all zero-shot detectors by an average of 39.28% AUROC. In contrast, supervised detectors have demonstrated robust detection capabilities in various domains, generative models, and attacks settings.

Through our benchmark analysis, **we highlight the strong relationship between various factors and detector performance**. Key elements that undermine the robustness and generalization of detectors include the informal style of domain data, distinct statistical patterns of LLMs, and adversarial perturbation attacks. Our findings indicate that shorter training data is beneficial for building robust detectors, while longer test data improves detector performance. Additionally, when human-written text undergoes attacks, the impact on detector performance is minimal, and performance may even improve after perturbation. This underscores the potential for adversarial perturbations to enhance current detection capabilities. Furthermore, our proposed framework aims to support the long-term development of attack methods against detectors. This will enable the creation of more challenging benchmarks that reflect real-world usages and evaluate the effectiveness of detection methods.

## 2 _DetectRL_

Previous datasets were mainly constructed by directly collecting human-written texts and those generated by LLMs using the same questions or prompt prefixes. This approach assumes an ideal detection environment and overlooks critical design considerations such as application domains, generative models, potential attacks, and text lengths. We improve the current dataset construction approach to better align with real-world detection scenarios. In this section, we introduce _DetectRL_, a new benchmark designed to facilitate such assessments, with its overall framework shown in Figure 1.

\begin{table}
\begin{tabular}{l|c c|c c c c c c c c} \hline \hline
**Benchmark \(\downarrow\) Eval \(\rightarrow\)** & Multi & Multi & Various & Human & Writing & Data & Detector & Training & Test & Real World \\  & Domains & LLMs & Prompts & Revision & Noises & Mixing & Generalization & Length & Length & Human Writing \\ \hline TuringBench [8] & ✓ & ✓ & - & - & - & - & - & - & - & - \\ MGTBench [9] & ✓ & ✓ & - & \(\bigcirc\) & \(\bigcirc\) & - & \(\bigtriangleup\) & - & \(\bigtriangleup\) & - \\ MULTIPhDe [10] & ✓ & ✓ & - & - & - & - & \(\bigtriangleup\) & - & - & - \\ M4 [12] & ✓ & ✓ & ✓ & - & - & - & ✓ & - & - & - \\ MAGE [11] & ✓ & ✓ & - & \(\bigcirc\) & - & - & ✓ & - & - & - \\ \hline _DetectRL_ (**Ours**) & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with existing benchmarks. ✓: benchmark evaluates this scenario. \(\bigtriangleup\): has studies, not in evaluation. \(\bigcirc\): similar scenario exist, but not fully aligns with real-world applications.

### Framework

Data sources_DetectRL_ is a comprehensive benchmark consisting of academic abstracts from the arXiv Archive,3 covering the years 2002 to 2017. It also includes news articles from the XSum dataset [17], creative stories from Writing Prompts [18], and social reviews from Yelp Reviews [19]. The texts generated by LLMs within these domains are considered to pose higher risk of misleading content when misused, which underscores the importance of effective detection strategies. We extracted 2,800 samples per dataset as human-written texts. To avoid the potential contamination from text generated by LLMs, all selected data was released prior to the advent of ChatGPT.

Footnote 3: https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data

ModelsBased on the collected human-written texts, we selected several LLMs that widely used in real-world, including GPT-3.5-turbo [13], PaLM-2-bison [14], Claude-instant [15], and Llama-2-70b [16], to perform text generation tasks. These models are mostly black-box and require substantial computational resources, making white-box detection methods challenging. We obtain text samples generated by these LLMs through interactive sessions with each model. For more details on the LLMs and text generation settings, please refer to Appendix D.

Data generationWe employed various attack methods to simulate complex real-world detection scenarios. Following the classifications from the studies by [4] and [20], we categorized our attack methods into prompt attacks, paraphrase attacks, and perturbation attacks. Additionally, we treated data mixing as a separate scenario in our study. Please see Appendix D.4 for implementation details.

_Prompt attacks_ are intended to use carefully designed prompts to guide LLMs in generating text that closely mimics human writing style. Our employed prompt attacks include Few-shot Prompting [21] and ICO Prompting, which is part of SICO Prompting [22].

_Paraphrase attacks_ have been extensively studied in recent research on LLM-generated text detection [23], focusing on rewriting text while maintaining its original meaning. Alongside using the DIPPER-paraphraser [23], we also employed Back-translation via Google Translate4 and Polishing using LLMs, which are two paraphrasing methods commonly utilized in everyday scenarios.

Footnote 4: https://translate.google.com/

_Perturbation attacks_ mainly involve introducing adversarial perturbations on text directly generated by LLMs. These attacks can effectively simulate common writing errors, character or word substitutions

Figure 1: The overall framework of _DetectRL_. Human-written samples are collected from high-risk and abuse-prone domains. We employ widely-used and powerful LLMs to create LLM-generated samples. All samples undergo well-designed attacks to simulate real-world scenarios and a varying length augmentation method is applied to enhance the benchmark’s diversity. _DetectRL_ consists of four distinct tasks to evaluate the detectors’ comprehensive detection abilities and robustness.

or other adversarial noises in real-world applications. We utilized DeepWordBug [24] for Character-level Perturbations, TextFooler [25] for Word-level Perturbations, and TextBugger [24] for Sentence-level Perturbations, all implemented using TextAttack [26].

_Data Mixing_ involves two primary approaches: Multi-LLM Mixing and LLM-Centered Mixing. In Multi-LLM Mixing, we create LLM-generated samples by sampling and combining sentences from multiple LLMs. On the other hand, LLM-Centered Mixing involves substituting one-fourth of an LLM-generated text with randomly selected human-written content. Despite this substitution, the text remains labeled as LLM-generated, since the majority originates from the LLM.

Data augmentationWe enhance the diversity of samples of different lengths through data augmentation, primarily by splitting texts at the sentence level. This approach creates multiple versions of each text sample with varying lengths. Based on the distribution of text lengths, we then categorize these sample into intervals of 20 words each (up to 360 words, since longer texts are rare). Within each interval, we uniformly sample 900 examples to comprehensively assess the detector's performance.

### Task definition

Based on the meticulously curated dataset, we manifest the _DetectRL_ framework into four distinct tasks for LLM-generated text detectors assessment, described as follows:

Task 1: In-domain robustness assessment: multi-domain, multi-LLM, and multi-attack assessment.This task aims to evaluate the foundational performance of detectors in different domains, generators, and attack strategies, focusing specifically on their in-domain robustness in various real-world scenarios. We use the average performance score as the assessment metric.

Task 2: Generalization assessment.This task assesses the generalization of detectors from three perspectives: domain, LLM, and attack, to determine their effectiveness in diverse scenarios. Unlike Task 1, this task emphasizes the detector's ability to handle out-of-distribution samples. For example, we evaluate the performance of detectors trained on texts from one domain when applied to texts from different domains to determine their generalization score across domains. The same approach is used to assess generalization across different LLMs and attack strategies.

\begin{table}
\begin{tabular}{l|l|l|l|l|l} \hline \hline \multirow{2}{*}{**Task**} & \multirow{2}{*}{**Setting**} & \multirow{2}{*}{**Sub Setting**} & \multicolumn{2}{c|}{**Training**} & \multicolumn{1}{c}{\multirow{2}{*}{**Test**}} \\  & & & **Supervised** & \multicolumn{1}{c|}{**Zero-Shot**} & \multicolumn{1}{c|}{**Test**} \\ \hline \multirow{6}{*}{**Task**} & Multi- & Accidents & 25,990 & 2,008 & 2,008 \\  & Domain & News & 25,992 & 2,008 & 2,008 \\  & Domain & Greenhill & 25,985 & 2,008 & 2,008 \\  & & Social Media & 25,984 & 2,008 & 2,008 \\ \cline{2-6}  & \multirow{2}{*}{Multi-} & GPT-3-attribute & 25,987 & 2,008 & 2,008 \\  & & Clanda-3-metric & 25,987 & 2,008 & 2,008 \\  & & Clanda-3-metric & 25,987 & 2,008 & 2,008 \\  & & Clanda-3-metric & 25,987 & 2,008 & 2,008 \\ \cline{2-6}  & \multirow{2}{*}{Multi-} & Direct & 20,384 & 2,016 & 2,016 \\  & & Pompe & 31,568 & 2,012 & 2,023 \\  & & Progressive & 42,767 & 2,016 & 2,016 \\  & & Portability & 42,784 & 2,016 & 2,016 \\  & & Data Mining & 40,148 & 2,008 & 2,008 \\ \hline \multirow{6}{*}{**Task**} & Domain & News & 25,992 & 2,008 & 6,024 \\  & Generalization & Creating & 25,985 & 2,008 & 6,024 \\  & & Social Media & 25,984 & 2,008 & 6,024 \\ \cline{2-6}  & \multirow{2}{*}{LLM} & GPT-3-attribute & 25,987 & 2,008 & 6,024 \\  & & Clanda-3-metric & 25,990 & 2,008 & 6,024 \\  & & Clanda-3-metric & 25,987 & 2,008 & 6,024 \\  & & Clanda-2-3-metric & 25,987 & 2,008 & 6,024 \\  & & Llamo-2-3-70b & 25,987 & 2,008 & 6,024 \\  & & Direct & 20,384 & 2,016 & 6,048 \\  & & Claudio-3-metric & 25,784 & 2,016 & 6,048 \\  & & Promote & 31,568 & 2,012 & 6,046 \\  & & Portuguese & 42,767 & 2,016 & 6,048 \\  & & Personalization & 42,784 & 2,016 & 6,048 \\  & & Data Mining & 40,118 & 2,008 & 6,024 \\ \hline \multirow{2}{*}{**Task**} & Varying & Training-Time & 16,200 & 16,200 & 900 \\  & Text Length & Text-Time & 900 & 900 & 16,200 \\ \hline \multirow{6}{*}{**Task**} & Direct & 20,384 & 2,016 & 2,016 \\  & Human & Progressive & 42,767 & 2,016 & 2,016 \\ \cline{1-1}  & Writing & Portability & 42,784 & 2,016 & 2,016 \\ \cline{1-1}  & Data Mining & 42,798 & 2,012 & 2,012 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Benchmark statistics.

Task 3: Varying text length assessment.This task evaluates the impact of text length on the performance of detectors, considering both training-time and test-time phase. In the training-time phase, detectors are trained on samples of different length intervals and then tested on samples from the pivotal interval. In the test-time phase, the detector trained on samples from the pivotal interval is evaluated with samples of varying lengths. This approach provides a comprehensive understanding of how text length influences detection capabilities.

Task 4: Real-world human writing assessment.This task evaluates how real-world human writing factors impact the performance of detectors. In this innovative assessment, we simulate and replicate these factors like word substitutions and spelling errors by applying attacks on human-written texts, highlighting the challenges they pose to detectors in real-world scenarios.

### Benchmark statistics

The statistics for the collected data are presented in Appendix Table 9. This dataset comprises 100,800 human-written samples, including 11,200 raw samples and 89,600 samples modified via attack manipulations. Additionally, it contains 134,400 samples generated by LLMs, categorized as follows: 11,200 samples generated with direct prompt, 22,400 with prompt attacks, 33,600 with paraphrase attacks, 33,600 with perturbation attacks, and 22,400 with data mixing. To evaluate detectors performance, we designed the _DetectRL_ benchmark by carefully extracting relevant subsets of data to align with the task design. The selected samples ensure a balance across domains, LLMs, and attack types. The training data was specifically tailored for both supervised and zero-shot detectors, and performance was evaluated using common test sets. Detailed statistics for each task and the analysis of the textual features of _DetectRL_ samples are presented in Figure 2. For a more detailed analysis, please refer to Appendix D.6.

### Evaluation metrics

We employ AUROC and \(F_{1}\) Score as the main evaluation metrics. AUROC is widely used for assessing zero-shot detection methods [27] because it considers the True Positive Rate (TPR) and False Positive Rate (FPR) across different classification thresholds. This makes AUROC particularly useful for evaluating detector performance at different thresholds. The \(F_{1}\) Score provides a comprehensive evaluation of detector capabilities by balancing the Precision and Recall. Additionally, we provide detailed Precision and Recall scores in Appendix F for further reference, with a specific focus on Recall to highlight the detectors' effectiveness in identifying LLM-generated text.

## 3 Experiments and discussion

In this section, we organize our experiments and discussions from five distinct perspectives: **(1) Benchmarking the cutting-edge detectors**: We evaluate the current SOTA detectors against our benchmark to identify ongoing challenges. **(2) Robustness analysis**: We analyze the factors contributing to robustness issues across various domains, LLMs, and attack scenarios. **(3) Assessing generalization**: We investigate how well detectors perform on data distribution they were not specifically trained on, highlighting their out-of-distribution robustness. **(4) Length discrimination**: We examine the detectors' ability to differentiate between texts of varying lengths and discuss the impact of training on such texts. **(5) Real-world human writing scenarios**: We analyze the effects of real-world post-processing and mistake in human-written texts, discussing their implications to provide more nuanced and valuable insights.

### Benchmarking detectors

DetectorsWe employed a variety of SOTA detectors to assess the difficulty of _DetectRL_. Given that LLMs in real-world scenarios are often black-box and inaccessible, we exclude watermarking methods from our evaluation. Our evaluation encompasses prominent zero-shot techniques and supervised fine-tuned classifiers, including Log-Likelihood [28], Entropy [29], Rank [30], Log-Rank [30], LRR [31], NPR [31], DetectGPT [27], Fast-DetectGPT [32], Revise-Detect. [33], DNA-GPT [34], Binoculars [35], RoBERTa Classifier (RoB [36]), and XLM-RoBERTa Classifier5 (X-RoB [37]).

For the white-box zero-shot detection method, we employ the GPT-Neo-2.7B [38] as the scoring model, in line with the methodology proposed in Fast-DetectGPT [32], to detect the text generated by black-box LLMs. For the black-box zero-shot detection method like Revise-Detect. [33] and DNA-GPT [34], we use GPT-do-Mini [39] to perform operations such as text revision and text continuation. For supervised detectors, all classifiers are trained using the same parameters. For detailed training parameters settings, please refer to Table 12 and AppendixE.2.

Main resultsWe assessed the performance of existing detectors on _DetectRL_, as shown in Table 3. Higher average scores indicate greater utility of the detector. These results highlight the challenges posed by our benchmark and explain why current SOTA detectors have not been widely adopted. The leaderboard results demonstrate that supervised detectors consistently outperform zero-shot detectors, demonstrating greater effectiveness and robustness. Among the zero-shot methods, Binoculars ranked highest but scored only 79.61%. The second-best is Revise-Detect., scoring 64.13%, followed by Log-Rank, LRR, Log-Likelihood, DNA-GPT, and Fast-DetectGPT. Additionally, our analysis highlights the unreliability of advanced detectors such as DetectGPT and NPR in real-world applications.

Significant ChallengesOur benchmarks reveal significant challenges in the current LLM-generated text detection research. We found that incorporating a mix distribution of domains, LLMs, and attack types increases the testing pressure of zero-shot methods. For example, in the multi-LLM setting, the average AUROC of all zero-shot detectors is only 58.61%. This is because data from each LLM spans various domains and attack methods, leading to substantial distribution differences even within data from the same LLM. These variations are often overlooked in ideal testing environments, making it difficult for zero-shot detectors developed based on them to work effectively. Specifically, zero-shot detectors struggle against powerful LLMs, achieving an average AUROC of only 77.67% on texts generated via direct prompting, with only Binoculars surpassing a 90% AUROC. The performance of these detectors declines markedly under well-designed attacks that simulate real-world scenarios, with average decreases of 1.97% in prompt attacks, 15.67% in paraphrase attacks, 38.43% in perturbation attacks, and 18.17% in data mixing scenarios. In contrast, supervised methods demonstrate impressive effectiveness, achieving an average AUROC of 99.40% on data generated through direct prompting and maintaining robustness against well-designed attacks.

Unexpectedly, recent advancements in LLM-generated text detection, such as DetectGPT [27], NPR [31], Fast-DetectGPT [32], and DNA-GPT [34], did not perform as expected on our benchmark. Their performance was even weaker than some traditional zero-shot baselines. Analysis across various domains and LLMs revealed a general lack of robustness as a potential underlying issue. For instance, DetectGPT's performance was notably low, with only 22.15% AUROC in academic writing (ArXiv) and 12.21% AUROC in news writing (Xsum), though it achieved 58.95% AUROC in creative writing and 44.43% AUROC in social media (Yelp Review). A similar trend was observed with the

\begin{table}
\begin{tabular}{l|c c c c c|c c c c c c|c c c} \hline \hline \multicolumn{11}{c}{**Leaderboard: LLM-Generated Text Detector in Real-World Scenarios**} \\ \hline
**Tasks Settings \(\rightarrow\)** & \multicolumn{3}{c|}{**Multi-**} & \multicolumn{3}{c|}{**Multi-**} & \multicolumn{3}{c|}{**Generalization**} & \multicolumn{3}{c|}{**Time**} & \multicolumn{3}{c|}{**Human**} \\  & **Domain** & **LLM** & **Attack** & **Domain** & **LLM** & **Attack** & **Train** & **Test** & **Writing** & **Writing** & **Avg.** \\
**Detectors \(\downarrow\)** & AUROC & \(F_{i}\) & AUROC & \(F_{i}\) & **AUROC** & \(F_{i}\) & \(F_{i}\) & \(F_{i}\) & \(F_{i}\) & \(F_{i}\) & \(F_{i}\) & AUROC & \(F_{i}\) & \\ \hline
**Rob-Base** & 99.98 & 99.75 & 99.93 & 99.58 & 99.56 & 97.66 & 83.00 & 91.81 & 92.37 & 79.99 & 44.00 & 97.34 & 94.31 & 90.02 \\
**Rob-Large** & 99.78 & 98.87 & 95.16 & 90.03 & 99.87 & 99.03 & 77.20 & 82.85 & 83.96 & 86.08 & 85.23 & 96.68 & 94.63 & 91.49 \\
**X-Rob-Base** & 99.92 & 99.34 & 99.14 & 98.17 & 98.94 & 96.07 & 75.97 & 92.73 & 90.58 & 84.25 & 73.83 & 93.43 & 90.29 & 91.71 \\
**X-Rob-Large** & 99.01 & 97.44 & 97.40 & 93.47 & 99.37 & 99.75 & 77.16 & 85.89 & 73.74 & 86.35 & 79.83 & 97.21 & 94.43 & 90.55 \\
**Binoculars** & 83.95 & 78.25 & 83.30 & 74.83 & 85.05 & 78.53 & 77.47 & 74.10 & 74.70 & 73.82 & 74.34 & 90.68 & 85.98 & 90.61 \\
**Revise-Detect.** & 67.24 & 60.82 & 66.36 & 53.72 & 70.89 & 57.24 & 54.50 & 53.28 & 50.63 & 65.71 & 67.96 & 83.29 & 82.16 & 62.43 \\
**Log-Rank** & 64.43 & 57.53 & 63.75 & 54.18 & 68.52 & 55.15 & 55.10 & 52.78 & 57.44 & 59.84 & 86.35 & 86.35 & 62.43 \\
**LRR** & 65.47 & 55.45 & 64.93 & 53.01 & 68.53 & 57.99 & 54.61 & 52.73 & 57.41 & 57.09 & 58.15 & 85.99 & 80.56 & 62.46 \\
**Log-Likelihood** & 63.71 & 56.36 & 62.97 & 53.13 & 67.97 & 54.38 & 53.37 & 51.77 & 50.73 & 57.92 & 59.28 & 88.84 & 83.75 & 61.83 \\
**DNA-GPT** & 64.92 & 58.53 & 64.36 & 51.09 & 68.36 & 53.36 & 51.57 & 47.09 & 41.98 & 57.63 & 62.43 & 87.80 & 87.27 & 60.70 \\
**Fast-DetectGPT** & 58.52 & 48.07 & 59.58 & 46.55 & 60.70 & 50.63 & 48.35 & 36.56 & 49.47 & 61.31 & 55.08 & 76.03 & 68.47 & **55.33** \\
**Rank** & 51.34 & 44.97 & 50.33 & 42.06 & 57.08 & 48.83 & 42.61 & 41.49 & 38.84 & 41.67 & 46.65 & 83.86 & 80.00 & **51.52** \\
**NPR** & 48.37 & 41.41 & 47.27 & 40.04 & 53.49 & 45.22 & 38.58 & 38.83 & 36.10 & 37.60 & 42.17 & 80.03 & 75.98 & 48.08 \\
**DetectGPT** & 34.43 & 21.52 & 34.93 & 14.80 & 36.19 & 19.15 & 11.54 & 13.11 & 11.84 & 35.78 & 34.69 & 60.86 & 48.76 & 29.05 \\
**Entropy** & 46.02 & 27.40 & 46.97 & 34.25 & 43.75 & 24.69 & 25.06 & 31.07 & 16.53 & 13.38 & 15.99 & 22.39 & 16.60 & 28.01 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The overall leaderboard for LLM-generated text detectors in real-world scenarios ranks detectors based on their robustness and generalization across various domains, LLMs, and attack scenarios. It also considers the impact of text length in training-time and test-time phase, as well as performance against real-world human writing factors.

best zero-shot detector, Binoculars, which performed more than 10% lower in academic writing and news writing compared to other domains. Additionally, Binoculars showed significantly reduced effectiveness on text generated by Claude, achieving only 55.15% AUROC, while presenting 88.14%,

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c} \hline \hline
**Metrics \(\rightarrow\)** & \multicolumn{2}{c}{AUROC} & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) \\ \hline \multicolumn{11}{c}{**Multi-Domain**} \\ \hline
**Domain Settings \(\rightarrow\)** & - & - & ArXiv & \multicolumn{2}{c}{XSum} & \multicolumn{2}{c}{Writing} & \multicolumn{2}{c}{Review} & \multicolumn{2}{c}{Avg.} \\ \hline
**Log-Likelihood** & - & 65.35 & 57.55 & 45.68 & 41.32 & 68.00 & 59.38 & 75.84 & 67.22 & 63.7122 & 56.367 \\
**Entropy** & **-** & 48.39 & 29.71 & 67.84 & 57.23 & 39.06 & 20.55 & 28.82 & 02.14 & 46.0253 & 27.4066 \\
**Rank** & **-** & 57.17 & 54.62 & 36.87 & 22.47 & 56.26 & 50.90 & 55.08 & 51.90 & 51.3409 & 44.97 \\
**Log-Rank** & **-** & 67.01 & 60.09 & 46.74 & 42.60 & 67.58 & 57.57 & 76.40 & 69.88 & 64.43 & 57.5378 \\
**LRR** & **-** & 70.54 & 61.34 & 50.09 & 38.38 & 64.65 & 53.09 & 76.61 & 68.99 & 65.47 & 55.4570 \\
**NPR** & **-** & 53.85 & 49.65 & 34.59 & 18.31 & 54.96 & 52.30 & 50.09 & 45.39 & 48.387 & 41.416 \\
**DetectGPT** & **-** & 22.15 & 60.00 & 12.21 & 00.00 & 89.59 & 50.83 & 44.43 & 35.25 & 34.434 & 21.502 \\
**DNA-GPT** & - & 67.41 & 58.30 & 64.22 & 45.09 & 69.04 & 85.28 & 78.17 & 69.28 & 69.71 & 57.723 \\
**Revie-Detect.** & **-** & 70.40 & 37.51 & 50.34 & 46.07 & 73.24 & 64.29 & 75.01 & 68.71 & 67.2475 & 54.1465 \\
**Binoculars** & **-** & 84.03 & 76.77 & 77.39 & 72.18 & 93.48 & 79.73 & 90.00 & 84.32 & 86.95 & 78.75 \\
**Fast-DetectGPT** & **-** & 43.69 & 24.46 & 39.12 & 28.39 & 74.21 & 67.84 & 77.02 & 71.62 & 58.03 & 48.08 \\ Avg. & **-** & 59.09 & 46.36 & 47.37 & 37.45 & 65.34 & 55.88 & 66.11 & 57.70 & 59.68 & 49.39 \\ \hline
**Rob-Base** & **-** & 100.0 & 100.0 & 99.99 & 99.85 & 99.99 & 99.65 & 99.97 & 99.50 & 99.99 & 99.75 \\
**Rob-Large** & **-** & 99.99 & 99.90 & 99.85 & 98.95 & 99.54 & 97.73 & 99.76 & 98.90 & 99.54 & 98.87 \\
**X-Rob-Base** & **-** & 100.00 & 100.0 & 99.97 & 99.55 & 99.84 & 98.76 & 99.88 & 90.05 & 99.92 & 99.59 \\
**X-Rob-Large** & **-** & 99.98 & 99.85 & 99.84 & 98.95 & 99.85 & 98.31 & 96.40 & 92.66 & 99.23 & 97.19 \\
**Avg.** & **-** & 99.99 & 99.93 & 99.91 & 99.92 & 99.80 & 98.61 & 99.00 & 97.52 & 99.67 & 98.85 \\ \hline \multicolumn{11}{c}{**Multi-LLM**} \\ \hline
**LLM Settings \(\rightarrow\)** & - & - & GPT-3.5 & \multicolumn{2}{c}{Claude} & \multicolumn{2}{c}{Pal-M-2} & \multicolumn{2}{c}{Llama-2} & \multicolumn{2}{c}{Avg.} \\ \hline
**Log-Likelihood** & - & 62.89 & 57.80 & 43.32 & 28.10 & 70.03 & 60.73 & 75.65 & 65.90 & 62.97 & 53.13 \\
**Entropy** & **-** & 46.84 & 23.29 & 52.25 & 30.42 & 45.34 & 16.56 & 43.48 & 66.75 & 46.97 & 34.25 \\
**Rank** & **-** & 52.19 & 49.32 & 41.68 & 22.78 & 50.40 & 41.74 & 57.05 & 54.40 & 50.33 & 42.06 \\
**Log-Rank** & **-** & 62.84 & 56.87 & 43.23 & 30.12 & 70.89 & 63.09 & 77.97 & 66.66 & 63.75 & 54.18 \\
**LRR** & **-** & 61.61 & 52.12 & 43.30 & 18.91 & 71.17 & 65.51 & 83.65 & 75.51 & 64.93 & 53.01 \\
**NPR** & **-** & 50.29 & 43.81 & 41.64 & 32.91 & 46.44 & 34.77 & 52.53 & 48.68 & 47.27 & 40.04 \\
**DetectGPT** & **-** & 43.46 & 26.27 & 38.36 & 12.56 & 26.72 & 00.00 & 36.71 & 20.40 & 34.93 & 14.80 \\ DNA-GPT & **-** & 61.87 & 55.04 & 44.88 & 25.67 & 71.48 & 60.77 & 75.22 & 62.89 & 64.36 & 51.09 \\
**Revie-Detect.** & **-** & 70.10 & 62.72 & 49.87 & 27.28 & 69.84 & 59.03 & 75.65 & 65.87 & 66.36 & 53.72 \\
**Binoculars** & **-** & 88.14 & 82.50 & 55.15 & 39.35 & 93.30 & 88.20 & 96.64 & 92.30 & 33.30 & 75.58 \\
**Fast-DetectGPT** & **-** & 65.56 & 59.55 & 30.10 & 00.00 & 65.97 & 57.58 & 76.79 & 69.08 & 59.58 & 46.55 \\ Avg. & **-** & 60.52 & 51.75 & 43.84 & 24.37 & 61.80 & 49.81 & 68.30 & 62.98 & 58.61 & 47.12 \\ \hline
**Rob-Base** & **-** & 99.97 & 99.70 & 99.98 & 99.80 & 99.94 & 99.40 & 99.84 & 99.45 & 99.93 & 99.59 \\
**Rob-Large** & **-** & 99.77 & 98.86 & 96.23 & 92.48 & 97.93 & 92.64 & 86.72 & 76.17 & 95.66 & 90.54 \\
**X-Rob-Base** & **-** & 99.88 & 99.45 & 98.26 & 97.48 & 98.77 & 97.19 & 99.69 & 98.57 & 99.15 & 98.17 \\ X-Rob-Large & **-** & 99.55 & 97.56 & 91.67 & 84.24 & 98.73 & 94.43 & 99.

93.30%, and 96.64% AUROC on text generated by GPT-3.5, PaLM-2, and Llama-2, respectively. These findings suggest that the performance differences of detectors across different domains and LLMs become significantly more pronounced when subjected to well-designed attacks.

### In-domain Robustness

Effectiveness of zero-shot detectors varies with the stylistic nature of domain data.As shown in Table 4, our results indicate that texts with a more formal style present greater challenges for detection. Detectors generally perform better with informal data, such as that from social media, but their effectiveness decreases markedly in more formal settings like news writing. Interestingly, this decrease in performance is even more pronounced in advanced detectors like Fast-DetectGPT [32]. Despite this variability, supervised classifiers demonstrate consistent reliability in detection across various domains. This finding aligns with insights from [40], emphasizing the robustness of supervised classifiers in diverse textual environments.

Differences in statistical patterns of LLMs pose significant challenges to detectors.As illustrated in Table 4, our experiments reveal a notable phenomenon: nearly all zero-shot LLM-generated text detectors exhibit a significant decline in performance when processing texts generated by Claude. This suggests that the effectiveness of detectors is influenced by the type of generative model used to generate the text to be detected, and their performance can deteriorate with varying statistical patterns. We hypothesize that these differences arise from variations in data, architecture, and training methods of the models, though verifying this is difficult due to the opaque nature of black-box models. Moreover, supervised detectors are more affected by the type of generative model than by the domain, particularly in models with larger sizes. For example, Rob-Large achieved an AUROC of only 86.72% and an \(F_{1}\) Score of only 76.17% on texts generated by Llama-2, while X-Rob-Large achieved an AUROC of only 91.67% and an \(F_{1}\) Score of only 82.24% on texts generated by Claude.

Adversarial perturbation attacks represent a significant threat to zero-shot detectors.As shown in Table 4, our findings indicate that the adversarial perturbation attacks drastically reduce the effectiveness of zero-shot detectors, reducing their performance to an average AUROC of 38.43%, which is less than half compared to their performance under paraphrase attacks. Additionally, data mixing presents a new challenging scenario, resulting in performance levels similar to paraphrase attacks, with detectors achieving an average AUROC of 59.50%. While prompt attacks, such as few-shot prompting, can generate higher-quality text more aligned with human preferences, their impact on zero-shot detectors is minimal. However, enhancing LLM-generated texts through human-written prompts, such as those used for polishing, continues to pose challenges for detectors (see Appendix F.1), decreasing their effectiveness by an average of 8.97% AUROC. This finding suggests that prompt-based methods remain a viable means of compromising detector performance. In contrast, supervised detectors consistently maintain robust performance across various attack types, demonstrating their potential for practical applications.

### Generalization of detectors

In real-world applications, there is a significant demand for detectors that can effectively adapt to various types of text. In this paper, we further investigate this requirement, specifically focusing on the relationship between the distribution of training and test data for these detectors. We assessed the generalization of three representative detectors: LRR [31], Fast-DetectGPT [32], and the RoB-Base Classifier [36]. We discussed their generalization from three perspectives: domain, LLM, and attack. Notably, we observed phenomena that align with the findings discussed in Section 3.2.

As shown in Table 5, our experimental results indicate that detectors trained on less formal stylistic domain data, such as creative writing and social media, exhibit stronger generalization. Their comprehensive performance is around 10% AUROC better than detectors trained on more formal stylistic domain data, such as academic writing and news writing. The variations in statistical patterns of generative models significantly impact the generalization of detectors. Detectors trained on texts generated by models with similar statistical patterns, such as GPT-3.5, PaLM-2, and Llama-2, generally perform well with each other. However, they struggle with texts generated by Claude. As discussed in Section 3.2, data with perturbation attacks poses the greatest challenge for generalization. Taking LRR as an example, the average AUROC for detectors trained on data with direct prompts,

[MISSING_PAGE_EMPTY:9]

shot detectors' performance and test text length. In contrast, supervised methods showed a rapid performance increase up to the pivotal length interval, followed by a slight decline.

### Impact of real-world human writing scenarios

We explored a critical question in real-world detection: How do human-driven factors impact detector performance? To investigate this, we simulated various modifications to human-written texts. We introduced paraphrase attacks to mimic text revisions and incorporated spelling errors through perturbation attacks. Moreover, we mixed LLM-generated sentences with human-written content to simulate AI-assisted writing scenarios. Experimental results, as shown in Table 6, indicate that attacks on human-written texts yield markedly different outcomes compared to those on LLM-generated texts. Specifically, paraphrasing attacks on human-written texts effectively confused zero-shot detectors, reducing the AUROC by an average of 7.95%. In contrast, data mixing had a minimal impact on zero-shot detectors' performance, with only a slight decline of 3.71% in AUROC. This contrasts sharply with the significant 18.17% decline in AUROC when human-written texts were mixed with LLM-generated texts. The resilience of human-written texts to such mixing may be attributed to their inherent complexity, making it difficult for zero-shot detectors to identify the inclusion of LLM-generated content. Interestingly, perturbation attacks on human-written texts appeared to enhance the discernment capabilities of zero-shot detectors, resulting in an average increase of 10.22% in AUROC. Similar trends were observed with supervised detectors. This suggests that human-written texts may inherently contain more adversarial features [41], which are utilized by detectors for identification. Such perturbations can further emphasize these distinctions, leading to improved performance.

## 4 Conclusion

In this paper, we introduce _DetectRL_, a novel benchmark designed to evaluate the detection capabilities of detectors against LLM-generated text. _DetectRL_ compiles texts from human sources in high-risk and abuse-prone domains, utilizes popular and powerful LLMs, employs well-designed attack techniques, and constructs datasets encompassing a diverse range of text lengths. This benchmark aims to assess the usability of detectors in scenarios that closely resemble real-world applications. Our experimental findings reveal the primary reasons why existing detectors for LLM-generated texts struggle in practical applications. Additionally, we engage in an in-depth discussion of the potential factors influencing detector performance, offering valuable insights into current detection research. Furthermore, _DetectRL_ provides a data curation framework to facilitate the future development of LLM-generated text detection technologies. This framework supports the rapid creation of an evolving, comprehensive, and adversarial benchmark, enabling continuous adaptation and improvement of detectors in the ongoing cat-and-mouse game of LLM-generated text detection.

\begin{table}
\begin{tabular}{l|c c c c c c c c c} \hline \hline
**Settings \(\rightarrow\)** & \multicolumn{2}{c}{**Direct**} & \multicolumn{2}{c}{**Paraphrase Attack**} & \multicolumn{2}{c}{**Perturbation Attack**} & \multicolumn{2}{c}{**Data Mixing**} & \multicolumn{2}{c}{**Avg.**} \\
**Detectors \(\downarrow\)** & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) \\ \hline \multicolumn{11}{c}{**Zero-shot Detectors**} \\ \hline
**Log-Likelihood** & 89.25 & 82.09 & 76.77 & 74.28 & 99.53 & 97.76 & 88.40 & 80.88 & 88.48 & 83.75 \\
**Entropy** & 26.47 & 00.00 & 27.15 & 00.00 & 03.37 & 00.00 & 32.58 & 66.40 & 22.39 & 16.60 \\
**Rank** & 83.50 & 76.27 & 72.14 & 74.13 & 99.63 & 98.13 & 80.17 & 71.48 & 83.86 & 80.00 \\
**Log-Rank** & 89.25 & 81.45 & 76.78 & 75.17 & 99.49 & 97.57 & 88.32 & 81.23 & 88.46 & 83.85 \\
**LRR** & 85.83 & 77.40 & 76.05 & 74.46 & 98.09 & 94.78 & 83.99 & 75.60 & 85.99 & 80.56 \\
**NFR** & 77.98 & 71.61 & 69.82 & 70.60 & 98.35 & 95.51 & 73.97 & 66.22 & 80.03 & 75.98 \\
**DetectGPT** & 52.84 & 40.90 & 68.45 & 73.45 & 87.95 & 79.74 & 34.20 & 00.98 & 60.86 & 48.76 \\
**DNA-GPT** & 88.01 & 80.78 & 77.19 & 75.95 & 98.81 & 95.83 & 87.40 & 76.55 & 87.85 & 82.27 \\
**Rewise-Detect** & 86.88 & 79.61 & 65.39 & 73.65 & 98.96 & 95.48 & 85.52 & 77.37 & 84.18 & 81.52 \\
**Binoculars** & 94.75 & 88.10 & 80.00 & 74.76 & 92.66 & 94.87 & 93.80 & 88.32 & 91.70 & 86.51 \\
**Fast-DetectGPT** & 79.56 & 72.45 & 77.18 & 70.13 & 84.43 & 74.45 & 65.23 & 60.53 & 76.60 & 69.39 \\ Avg. & 27.60 & 68.24 & 69.72 & 66.96 & 87.89 & 84.01 & 73.96 & 67.72 & 77.30 & 71.91 \\ \hline \multicolumn{11}{c}{**Supervised Detectors**} \\ \hline
**Rob-Base** & 99.77 & 98.10 & 89.82 & 80.98 & 99.99 & 99.65 & 99.81 & 98.51 & 97.34 & 94.31 \\
**Rob-Large** & 99.77 & 98.95 & 87.01 & 80.42 & 99.99 & 99.95 & 99.95 & 99.20 & 96.68 & 94.63 \\
**X-Rob-Base** & 98.36 & 96.20 & 81.93 & 75.06 & 99.96 & 99.30 & 93.47 & 90.62 & 93.43 & 90.29 \\
**X-Rob-Large** & 99.79 & 98.31 & 89.07 & 80.32 & 99.99 & 99.90 & 98.92 & 99.20 & 97.21 & 94.43 \\
**Avg.** & 99.42 & 97.89 & 86.95 & 76.19 & 99.98 & 99.70 & 98.26 & 96.88 & 96.16 & 93.41 \\ \hline \hline \end{tabular}
\end{table}
Table 6: The performance of detectors in real-world human writing assessment. The shades of blue and red illustrate the performance differences between the zero-shot and the supervised detectors, respectively. The underlined values represent the best performance.

## Acknowledgments

This work was supported in part by the Science and Technology Development Fund, Macau SAR (Grant No. FDCT/060/2022/AFJ, the mainland China collaboration project, National Natural Science Foundation of China Grant No. 62261160648), the Research Program of Guangdong Province (Grant No. 2220004002576, EF2023-00090-FST), the Science and Technology Development Fund, Macau SAR (Grant No. FDCT/0070/2022/AMJ, the mainland China collaboration project, China Strategic Scientific and Technological Innovation Cooperation Project Grant No. 2022YFE0204900), the Multi-year Research Grant from the University of Macau (Grant No. MYRG-GRG2023-00006-FST-UMDF, MYRG-GRG2024-00165-FST), and the Tencent AI Lab Rhino-Bird Gift Fund (Grant No. EF2023-00151-FST).

## References

* [1]D. Ippolito, D. Duckworth, C. Callison-Burch, and D. Eck (2020) Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 1808-1822. External Links: Link Cited by: SS1.
* [2]J. P. Cardenuto, J. Yang, R. Padilha, R. Wan, D. Moreira, H. Li, S. Wang, F. A. Andalo, S. Marcel, and A. Rocha (2023) The age of synthetic realities: challenges and opportunities. CoRRabs/2306.11503. External Links: Link, 2306.11503 Cited by: SS1.
* [3]P. Yu, J. Chen, X. Feng, and Z. Xia (2023) CHEAT: A large-scale dataset for detecting chatgpt-written abstracts. CoRRabs/2304.12008. External Links: Link, 2304.12008 Cited by: SS1.
* [4]J. Wu, S. Yang, R. Zhan, Y. Yuan, D. F. Wong, and L. S. Chao (2023) A survey on llm-generated text detection: Necessity, methods, and future directions. CoRRabs/2310.14724. External Links: Link, 2310.14724 Cited by: SS1.
* 4 May 2023, pp. 3637-3647. External Links: Link, 2002.0236 Cited by: SS1.
* [6]A. Pagnoni, M. Graciarena, and Y. Tsvetkov (2022) Threat scenarios and best practices to detect neural fake news. In Proceedings of the 2022 ACM SIGKDD International Conference on Knowledge and Data Mining, pp. 115-120. External Links: Link, Document Cited by: SS1.
* [7]C. Stokel-Walker (2022) Ai bot chatgpt writes smart essays--should professors worry?[published online ahead of print december 9, 2022]. Nature News. External Links: Link, 2002.02202 Cited by: SS1.
* [8]A. Uchendu, Z. Ma, T. Le, R. Zhang, and D. Lee (2021) TURINGBENCH: A benchmark environment for turing test in the age of neural text generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 2001-2016. External Links: Link, 2001.0018 Cited by: SS1.
* [9]X. He, X. Shen, Z. Chen, M. Backes, and Y. Zhang (2023) Mgtbench: benchmarking machine-generated text detection. CoRRabs/2303.14822. External Links: Link, 2303.14822 Cited by: SS1.
* [10]D. Macko, R. Moro, A. Uchendu, J. Samuel Lucas, M. Yamashita, M. Pikuliak, I. Srba, T. Le, D. Lee, J. Simko, and M. Bielikova (2023) Multitude: large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 9960-9987. External Links: Link, 2303.14822 Cited by: SS1.
* [11]D. Mecko, R. Moro, A. Uchendu, J. Samuel Lucas, M. Yamashita, M. Pikuliak, I. Srba, T. Le, D. Lee, J. Simko, and M. Bielikova (2023) Multitude: large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 9960-9987. External Links: Link, 2303.1482 Cited by: SS1.
* 4 May 2023, pp. 3637-3647. External Links: Link, 2303.1482 Cited by: SS1.
* 4 May 2023, pp. 3637-3647. External Links: Link, 2303.1482 Cited by: SS1.
* 4 May 2023, pp. 3637-3647. External Links: Link, 2303.1482 Cited by: SS1.
* 4 May 2023, pp. 3637-3647. External Links: Link, 2303.1482 Cited by: SS1.
* 4 May 2023, pp. 3637-3647. External Links: Link, 2303.1482 Cited by: SS1.
* [17]C. Stokel-Walker (2022) A bot chatgpt writes smart essays--should professors worry?[published online ahead of print december 9, 2022]. Nature News. External Links: Link, 2022.0222 Cited by: SS1.
* [18]A. Uchendu, Z. Ma, T. Le, R. Zhang, and D. Lee (2021) TURINGBENCH: A benchmark environment for turing test in the age of neural text generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 2001-2016. External Links: Link, 2001.0018 Cited by: SS1.
* [19]X. He, X. Shen, Z. Chen, M. Backes, and Y. Zhang (2023) Mgtbench: benchmarking machine-generated text detection. CoRRabs/2303.14822. External Links: Link, 2303.14822 Cited by: SS1.
* [20]D. Macko, R. Moro, A. Uchendu, J. Samuel Lucas, M. Yamashita, M. Pikuliak, I. Srba, T. Le, D. Lee, J. Simko, and M. Bielikova (2023) Multitude: large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 9960-9987. External Links: Link, 2303.1482 Cited by: SS1.
* [21]D. Macko, R. Moro, A. Uchendu, J. Samuel Lucas, M. Yamashita, M. Pikuliak, I. Srba, T. Le, D. Lee, J. Simko, and M. Bielikova (2023) Multitude: large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 9960-9987. External Links: Link, 2303.1482 Cited by: SS1.
* [22]D. Macko, R. Moro, A. Uchendu, J. Samuel Lucas, M. Yamashita, M. Pikuliak, I. Srba, T. Le, D. Lee, J. Simko, and M. Bielikova (2023) Multitude: large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 9960-99987. External Links: Link, 2303.1482 Cited by: SS1.
* [23]D. Macko, R. Moro, A. Uchendu, J. Samuel Lucas, M. Yamashita, M. Pikuliak, I. Srba, T. Le, D. Lee, J. Simko, and M. Bielikova (2023) Multitude: large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 9960-9987. External Links: Link, 2303.1482 Cited by: SS1.
* [24]D. Macko, R. Moro, A. Uchendu, J. Samuel Lucas, M. Yamashita, M. Pikuliak, I. Srba, T. Le, D. Lee, J. Simko, and M. Bielikova (2023) Multitude: large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 9960-9987. External Links: Link, 2303.1482 Cited by: SS1.
* [25]D. Macko, R. Moro, A. Uchendu, J. Samuel Lucas, M. Yamashita, M. Pikuliak, I. Srba, T. Le, D. Lee, J. Simko, and M. Bielikova (2023) Multitude: large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 9960-9987. External Links: Link, 2303.1482 Cited by: SS1.
* [26]D. Macko, R. Moro, A. Uchendu, J. Samuel Lucas, M. Yamashita, M. Pikuliak, I. Srba, T. Le, D. Lee, J. Simko, and M. Bielikova (2023) Multitude: large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 9960-9987. External Links: Link, 2303.1482 Cited by: SS1.

* [11] Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Zhilin Wang, Longyue Wang, Linyi Yang, Shuming Shi, and Yue Zhang. MAGE: machine-generated text detection in the wild. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024_, pages 36-53, 2024.
* Volume 1: Long Papers, St. Julian's, Malta, March 17-22, 2024_, pages 1369-1407, 2024.
* [13] OpenAI Blog. Introducing chatgpt. https://openai.com/index/chatgpt/, 2023.
* [14] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Akanksha Chowdhery, Clement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Diaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. Palm 2 technical report. _CoRR_, abs/2305.10403, 2023.
* [15] Anthropic Blog. Releasing claude instant 1.2. https://www.anthropic.com/news/releasing-claude-instant-1-2, 2023.
* [16] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288, 2023.
* November 4, 2018_, pages 1797-1807. Association for Computational Linguistics, 2018.
* [18] Angela Fan, Mike Lewis, and Yann N. Dauphin. Hierarchical neural story generation. In Iryna Gurevych and Yusuke Miyao, editors, _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018_, Volume 1: Long Papers, pages 889-898. Association for Computational Linguistics, 2018.
* [19] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 649-657, 2015.

* Yang et al. [2023] Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda R. Petzold, William Yang Wang, and Wei Cheng. A survey on detection of l1ms-generated content. _CoRR_, abs/2310.15654, 2023.
* Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Lu et al. [2023] Ning Lu, Shengcai Liu, Rui He, Qi Wang, and Ke Tang. Large language models can be guided to evade ai-generated text detection. _CoRR_, abs/2305.10847, 2023.
* Krishna et al. [2023] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. _CoRR_, abs/2303.13408, 2023.
* Gao et al. [2018] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. In _2018 IEEE Security and Privacy Workshops, SP Workshops 2018, San Francisco, CA, USA, May 24, 2018_, pages 50-56. IEEE Computer Society, 2018.
* Jin et al. [2020] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is BERT really robust? A strong baseline for natural language attack on text classification and entailment. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8018-8025. AAAI Press, 2020.
* Demos, Online, November 16-20, 2020_, pages 119-126. Association for Computational Linguistics, 2020.
* Mitchell et al. [2023] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 24950-24962. PMLR, 2023.
* Solaiman et al. [2019] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jasmine Wang. Release strategies and the social impacts of language models. _CoRR_, abs/1908.09203, 2019.
* Lavergne et al. [2008] Thomas Lavergne, Tanguy Urvoy, and Francois Yvon. Detecting fake content with relative entropy scoring. In Benno Stein, Efstathios Stamatatos, and Moshe Koppel, editors, _Proceedings of the ECAI'08 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse, Patras, Greece, July 22, 2008_, volume 377 of _CEUR Workshop Proceedings_. CEUR-WS.org, 2008.
* August 2, 2019, Volume 3: System Demonstrations, pages 111-116. Association for Computational Linguistics, 2019.
* [31] Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. DetectlIm: Leveraging log rank information for zero-shot detection of machine-generated text. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023_, pages 12395-12412. Association for Computational Linguistics, 2023.
* [32] Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. _CoRR_, abs/2310.05130, 2023.
* [33] Biru Zhu, Lifan Yuan, Ganqu Cui, Yangyi Chen, Chong Fu, Bingxiang He, Yangdong Deng, Zhiyuan Liu, Maosong Sun, and Ming Gu. Beat llms at their own game: Zero-shot llm-generated text detection via querying chatgpt. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023_, pages 7470-7483, 2023.
* [34] Xianjun Yang, Wei Cheng, Yue Wu, Linda Ruth Petzold, William Yang Wang, and Haifeng Chen. DNA-GPT: divergent n-gram analysis for training-free detection of gpt-generated text. In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_, 2024.
* [35] Abhimanyu Hans, Avi Schwarzschild, Valeria Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Spotting llms with binoculars: Zero-shot detection of machine-generated text. In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_, 2024.
* [36] Sungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik Cho, Jiyoon Han, Jangwon Park, Chisung Song, Junseong Kim, Yongsook Song, Taehwan Oh, Joohong Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong, Inkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo Kim, Myeonghwa Lee, Seongbo Jang, Seungwon Do, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee, Kyumin Park, Jamin Shin, Seonghyun Kim, Lucy Park, Alice Oh, Jungwoo Ha, and Kyunghyun Cho. Klue: Korean language understanding evaluation, 2021.
* [37] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, pages 8440-8451. Association for Computational Linguistics, 2020.
* [38] Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021.
* [39] OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. _OpenAI blog_, 2024.
* [40] Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang, Shuming Shi, and Yue Zhang. Deepfake text detection in the wild. _CoRR_, abs/2305.13242, 2023.
* [41] Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xuebo Liu, Lidia S. Chao, and Min Zhang. Who wrote this? the key to zero-shot llm-generated text detection is gecscore. _CoRR_, abs/2405.04286, 2024.
* [42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, ZhengYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
* 16, 2023_, 2023.
* [44] Runzhe Zhan, Xinyi Yang, Derek F. Wong, Lidia S. Chao, and Yue Zhang. Prefix text as a yarn: Eliciting non-english alignment in foundation language model. In _Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024_, pages 12131-12145, 2024.
* [45] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. _CoRR_, abs/2303.18223, 2023.
* [46] Tao Fang, Shu Yang, Kaixin Lan, Derek F. Wong, Jinpeng Hu, Lidia S. Chao, and Yue Zhang. Is chatgpt a highly fluent grammatical error correction system? A comprehensive evaluation. _CoRR_, abs/2304.01746, 2023.
* [47] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. _CoRR_, abs/2301.07597, 2023.
* [48] Soichiro Murakami, Sho Hoshino, and Peinan Zhang. Natural language generation for advertising: A survey. _CoRR_, abs/2306.12719, 2023.
* [49] Yuta Yanagi, Ryohei Orihara, Yuichi Sei, Yasuyuki Tahara, and Akihiko Ohsuga. Fake news detection with generated comments for news articles. In _2020 IEEE 24th International Conference on Intelligent Engineering Systems (INES)_, pages 85-90. IEEE, 2020.
* 25, 2022_, pages 841-852. ACM, 2022.
* or at least it used to be: Educational opportunities and challenges of AI code generation. In Maureen Doyle, Ben Stephenson, Brian Dorn, Leen-Kiat Soh, and Lina Battestilli, editors, _Proceedings of the 54th ACM Technical Symposium on Computer Science Education, Volume 1, SIGCSE 2023, Toronto, ON, Canada, March 15-18, 2023_, pages 500-506. ACM, 2023.
* [52] Teo Susnjak. Chatgpt: The end of online exam integrity? _CoRR_, abs/2212.09292, 2022.
* [53] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language model with integrated external knowledge bases. _CoRR_, abs/2306.16092, 2023.
* [54] Marco Guevara, Shan Chen, Spencer Thomas, Tafadzwa L. Chaunzwa, Idalid Franco, Benjamin H. Kann, Shalini Moningi, Jack M. Qian, Madeleine Goldstein, Susan Harper, Hugo J. W. L. Aerts, Paul J. Catalano, Guergana K. Savova, Raymond H. Mak, and Danielle S. Bitterman. Large language models to identify social determinants of health in electronic health records. _npj Digit. Medicine_, 7(1), 2024.
* [55] Kaixin Lan, Tao Fang, Derek F. Wong, Yabo Xu, Lidia S. Chao, and Cecilia G. Zhao. FOCUS: forging originality through contrastive use in self-plagiarism for language models. In _Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024_, pages 14432-14447, 2024.

* [56] Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G. Baraniuk. Self-consuming generative models go MAD. _CoRR_, abs/2307.01850, 2023.
* [57] Martin Majovsky, Martin Cerny, Matej Kasal, Martin Komarc, and David Netuka. Artificial intelligence can generate fraudulent but authentic-looking scientific medical articles: Pandora's box has been opened. _Journal of Medical Internet Research_, 25:e46924, 2023.
* [58] Chris Stokel-Walker and Richard Van Noorden. What chatgpt and generative ai mean for science. _Nature_, 614(7947):214-216, 2023.
* [59] Sebastian Porsdam Mann, Brian D Earp, Sven Nyholm, John Danaher, Nikolaj Moller, Hilary Bowman-Smart, Joshua Hatherley, Julian Koplin, Monika Plozza, Daniel Rodger, et al. Generative ai entails a credit-blame asymmetry. _Nature Machine Intelligence_, pages 1-4, 2023.
* [60] Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, and Amrit Singh Bedi. Towards possibilities & impossibilities of ai-generated text detection: A survey. _CoRR_, abs/2310.15264, 2023.
* [61] Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang, and Cho-Jui Hsieh. Red teaming language model detectors with language models. _CoRR_, abs/2305.19713, 2023.
* [62] Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi. Tweepfake: about detecting deepfake tweets. _CoRR_, abs/2008.00036, 2020.
* [63] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 9051-9062, 2019.
* [64] Yikang Liu, Ziyin Zhang, Wangyang Zhang, Shisen Yue, Xiaojing Zhao, Xinyuan Cheng, Yiwen Zhang, and Hai Hu. Argugpt: evaluating, understanding and identifying argumentative essays generated by GPT models. _CoRR_, abs/2304.07666, 2023.
* [65] Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, and Preslav Nakov. M4: multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection. _CoRR_, abs/2305.14902, 2023.
* [66] OpenAI. Introducing chatgpt, 2022.
* [67] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanulayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. _CoRR_, abs/2204.02311, 2022.
* [68] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with BERT. In _8th International Conference on Learning Representations, ICLR_ 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.
* [69] Rudolph Flesch. A new readability yardstick. _Journal of applied psychology_, 32(3):221, 1948.

* [70] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020.
* [71] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.

Related work

### LLM-generated text and risk

With the expansion of model size [42] and the development of efficient preference alignment methods [43; 44], LLMs have emerged with powerful capabilities for text understanding and generation [45; 46]. The text produced by the current LLMs closely resembles the quality of human-written text, particularly in terms of coherence, fluency, and grammatical accuracy, making it difficult for humans to distinguish between the two [47]. The release of ChatGPT has propelled human society into the era of LLMs, with these models finding widespread application across various aspects of daily life, such as generating advertising copy [48], writing news articles [49], storytelling [50], and coding [51]. They are also significantly influencing various fields and industries, including education [52], law [53], and medicine [54], gaining broad acceptance among people.

However, the use of LLMs has raised several concerns. Recent research by [4] highlights significant challenges and potential risks associated with LLM-generated text from five perspectives: regulatory oversight related to artificial intelligence and copyright [55], erosion of user trust in internet content, homogenization of generated text that could impede LLM progress [56], challenges posed to education and academia by LLM misuse [57], and the formation of information echo chambers in society.

### LLM-generated text detection

Given the potential misuse of LLMs, it is crucial to develop detectors that can effectively identify LLM-generated text. These detectors can help minimize the threats posed by misuse, thereby promoting the trustworthy AI applications in the era of LLMs [58; 59]. Existing LLM-generated text detection technologies [4; 60] mainly includes watermarking technology, statistics-based methods, neural-based detectors and human-assisted methods. Despite the impressive progress in LLM-generated text detection task, [23; 61] point out that these detectors become unreliable when under real-world scenarios and well-designed attacks. Building more effective and robust detectors remains a significant challenge.

### Detection benchmark

Previous work has already dedicated significant effort to the construction of benchmarks for LLM-generated text detection, mainly encompassing early deepfake research such as the TweepFake dataset [62] and the GROVER Dataset [63], as well as prior work on detecting LLM-generated texts like the GPT-2 Output Dataset6 and TuringBench [8]. HC3 [47] is one of the recent impressive datasets, containing ChatGPT-generated text data in both English and Chinese, covering multi-domain and multi-lingual evaluation. Other benchmarks, such as MGTBench [9], ArguGPT [64], and the MAGE [11], also consider texts generated by various LLMs. M4 [65] is a comprehensive dataset recently released, covering multi-domain, multi-lingual, and multi-generator evaluation scenarios.

Footnote 6: https://github.com/openai/gpt-2-output-dataset

However, these benchmarks still mainly focused on ideal detection settings, such as using some open-source language models with limited performance and simple text generation settings, while they lack simulations and explorations of real-world application scenarios, which has been explicitly highlighted in [4]. Our work aims to bridge this gap by offering a benchmark for detecting LLM-generated texts in a form more adapted to real-world scenarios, primarily including high-risk and abuse-prone domain, the use of more powerful and commonly employed LLMs, well-designed attack methods, varied text lengths for training and testing, and factors related to real-world human writing.

## Appendix B Limitations

Considering the rapid innovation within the NLP community, we acknowledge that our benchmark's temporal relevance could be a potential limitation. This is due to the fast-paced development of LLMs and the emergence of new application scenarios and challenges. From the perspective of LLM development, new LLMs are being created at an astonishing rate and will continue to impact existing detectors, while our benchmark only examines the detectors' ability to discriminate against the advanced and popular LLMs currently available. Regarding application scenarios and challenges,newly designed attack methods, the requirement for increasingly fine-grained detection, and the ever-expanding demands of application domains place progressively higher demands on existing detectors. Our benchmark setup only examines the current major demands and does not encompass the full spectrum of challenges, including those that may arise in the future.

Nonetheless, we open-source our benchmark framework and encourage researchers to build upon it. By using our framework, researchers can quickly create more applicable and demand-specific test data to evaluate detector performance, ensuring the benchmark remains relevant as the field evolves.

## Appendix C Ethics statement

We developed _DetectRL_ by collecting publicly available human-written texts in high-risk and abuse-pron domains, generating similar texts using advanced and popular LLMs, designing and applying various attack methods for data augmentation. The release of _DetectRL_ aims to advance research on detecting LLM-generated texts, enhancing their robustness and applicability of detectors in real-world scenarios. However, while promoting this research, we have also considered the potential for misuse. By making our dataset construction framework publicly available, there's a possibility that our well-designed attack methodologies could be used to develop defenses that might undermine existing detection systems.

Despite this risk, we believe that our work will significantly contribute to the development of more robust and applicable detectors for LLM-generated text. These detectors can be continuously improved and employed to enhance LLM-generated text applications in the era of LLMs, all while participating in the ongoing cat-and-mouse game with evolving attack methods.

Additionally, although we have manually reviewed most of the data, there remains a risk that the data may still contain personally identifiable information or offensive content. Therefore, please ensure that our data is used solely for academic purposes and exercise caution.

## Appendix D Data collection

### Human-written datasets

The human-written texts we utilized were sourced from domains where real-world applications of LLMs present higher risks. We selected Arxiv Abstracts to represent academic writing, Xsum for news writing, Writing Prompts for creative writing, and Yelp Reviews for social media interactions. The specific details of these datasets are as follows:

ArXiv AbstractsThe ArxivPapers dataset7 is an unlabelled collection of over 104K papers related to machine learning published on arXiv.org between 2007 and 2020. The dataset includes around 94K papers (with available LaTeX source code) organized into a structured format comprising titles, abstracts, sections, paragraphs, and references.

Footnote 7: https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data

XSumThe Extreme Summarization dataset serves as a benchmark for evaluating abstractive single-document summarization systems. This collection includes 226,711 news articles sourced from BBC reports between 2010 and 2017, covering a diverse range of topics such as news, politics, sports, weather, business, technology, science, health, family, education, entertainment, and the arts [17].

Writing PromptsThe Writing Prompts dataset is a dataset focused on the art of story generation, comprising 300,000 human-written stories, each paired with a unique writing prompt from an online community. This extensive collection is designed to support hierarchical story generation, a process that starts with creating a story premise and evolves into a complete narrative [18].

Yelp ReviewsThe Yelp Reviews Polarity dataset originates from the Yelp Dataset Challenge 2015,8 featuring reviews posted on Yelp. Refined by [19] for text classification research, the dataset categorizes reviews with 1 and 2 stars as negative (class 1) and those with 3 and 4 stars as positive (class 2), providing a balanced approach to sentiment analysis. It includes a total of 560,000 training samples and 38,000 testing samples.

### Generative models and hyper-parameters

The generative models we selected are powerful LLMs commonly used in daily life. Table 7 lists the model paths or API services of these LLMs. The temperature for all models is set to the default parameter of 1, promoting the generation of creative and unpredictable text. The specific details of these LLMs are as follows:

GPT-3.5-turboGPT-3.5-turbo [66], developed by OpenAI, is a variant of the Generative Pretrained Transformer (GPT) model, specifically tailored for generating human-like text based on the input it receives. This model has been trained on a diverse range of internet text, enabling it to understand and produce responses across a vast array of topics and styles.

PaLM-2-bisonPaLM-2-bison represents the latest advancement in Google's LLMs technology, building upon the foundation of PaLM [67]. This model showcases exceptional capabilities in advanced reasoning tasks such as code interpretation and mathematical problem-solving, classification and question-answering, adept translation, and multilingual communication, as well as in generating natural language with improved proficiency over previous models.

Claude-instantClaude-instant [15] represents a significant leap forward in the realm of AI assistants, developed from Anthropic's rigorous research into crafting AI systems that are helpful, honest, and harmless. Designed to accommodate a wide array of use cases, Claude excels in summarization, search functionalities, creative and collaborative writing, question answering, and coding, among other tasks.

Llama-2-70bLlama-2-70b is a SOTA generative open-source LLMs developed by Meta, part of the broader Llama 2 collection [42]. This model outperforms numerous open-source chat models in benchmark evaluations and equates to the leading closed-source models like ChatGPT and PaLM in terms of helpfulness and safety.

### Data generation settings

All text generation tasks were conducted through chat with LLMs. Specifically, for academic writing abstracts, we provided the article's title to the LLMs and asked them to generate an abstract based on the title; for news articles, we provided the summary of the article and asked the LLMs to generate the complete news article based on the summary; for creative writing, we provided writing prompts to the LLMs and requested that they engage in creative storytelling based on these prompts. Social media was the simplest task, as the LLMs would continue writing based on the first sentence of the social commentary text. Below, we provide the generation instructions for texts in different domains:

#### d.3.1 Academic writing

[ {'role': 'user', 'content': 'Given the academic article title, write an academic article abstract with <sentences num> sentences:\n academic article title: <prefix> \n academic article abstract:'}, ]

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Generative Model** & **Model Path / API Service** & **Hyper-parameters** \\ \hline GPT-3.5-turbo & OpenAI/gpt-3.5-turbo & temperature=1 \\ PaLM-2-bison & Google/chat-bison@002 & temperature=1 \\ Claude-instant & Anthropic/claude-instant-1.2 & temperature=1 \\ Llama-2-70b & meta-llama/Llama-2-70b-chat-hf & temperature=1 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Details of the generative models that is used to produce LLM-generated text.

The <sentences num> refers to the sentences length corresponding to the human-written sample, and <prefix> is the specific article title. For example, it could be like "Calculation of prompt diphoton production cross sections at Tevatron and LHC energies", and the response is supposed to write an academic abstract based on the sentences length and article title of the human-written sample.

#### d.3.2 News writing

```
[{'role':'user','content':'Giventhenewsarticlesummary,writeanewsarticlewith<sentencesnum>sentences:\nnewsarticlesummary:<prefix>\nnewsarticle:'},] ```

2: Direct prompt for news writing

The <sentences num> refers to the sentences length corresponding to the human-written sample, and <prefix> is the specific news article summary. For example, it could be like "A former Lincolnshire Police officer carried out a series of sex attacks on boys, a jury at Lincoln Crown Court was told.", and the response is supposed to write a news article based on the sentences length and the news article summary of the human-written sample.

#### d.3.3 Creative writing

```
[{'role':'user','content':'Giventhewritingprompt,writeastorywith<sentencesnum>sentences:\nwritingprompt:<prefix>\nstory:'},] ```

3: Direct prompt for creative writing

The <sentences num> refers to the sentences length corresponding to the human-written sample, and <prefix> is the specific writing prompt. For example, it could be like "Through Iron And Flame", and the response is supposed to write a story based on the sentences length and the writing prompt of the human-written sample.

#### d.3.4 Social media

```
[{'role':'user','content':'Giventhereview'sfirstsentence,pleasehelptocontinuethereviewwith<sentencesnum>sentences:\nreview'sfirstsentence:<prefix>\ncontinuedreview:'},] ```

4: Direct prompt for social media

The <sentences num> refers to the sentences length corresponding to the human-written sample, and <prefix> is the specific writing prompt. For example, it could be like "I don't know what Dr. Goldberg was like before moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office.", and the response is supposed to write the continued review based on the sentences length and the first sentence of the human-written sample.

### Data attacks settings

#### d.4.1 Prompt attacks

Prompt attacks are designed to use carefully crafted prompts to guide LLMs to generate text that aligns more closely with human writing styles. The Prompt Attacks we use include Few-Shot Prompt [21] and ICO Prompt (part of SICO Prompt) [22]. Few-Shot Prompting involves presenting LLMs with a few human-written examples to enhance alignment with human writing styles. The SICO Prompt introduces a novel approach called Substitution-based In-Context Example Optimization (SICO), which automatically constructs prompts to evade detection, as proposed by [22]. It operates through a two-stage prompting process. We specifically use the ICO (In-Context Example Optimization) aspect of SICO, excluding the substitution process to prevent text perturbations. We provide examples of Few-Shot Prompting and ICO Prompting for academic writing tasks as fellow:

```
[{'role':'user','content':'<incontentlearningexamples>\nGiventheacademicarticletitle,writeanacademicarticlewith<{sentencesnum>sentences:\nacademicarticletitle:<prefix>:\nacademicarticle:'},] ] ```

5: Few-Shot Prompt

The <In-content learning examples> refer to contextual examples retrieved for LLMs to learn from. We set the number of examples to three, using the BM25 retrieval algorithm. Each example includes an academic article title and a corresponding article pair. The <sentences num> refers to the sentences length of the corresponding human-written sample, the <prefix> is the specific article title, and the task is to write an academic article abstract based on the sentence length and article title of the human-written sample.

```
[{'role':'user','content':'HerearethewritingsfromAIandhuman:\n<in-contentlearningexamples>\nCompareandgivethekeydistinctfeature(specificallyvocabulary,sentencestructure)ofhuman\'swritings(donotuseexamples):'},{'role':'bot','content':'<step1response>'},{'role':'user','content':'Basedonthedescription,giventheacademicarticletitle,writeanacademicarticlewith<sentencesnum>sentencesinhumanstylewritings:\nacademicarticletitle:<prefix>\nhuman:'},] ] ```

6: ICO Prompt

Similar to Few-Shot Prompt, the <in - content learning examples > in the ICO Prompt refer to context examples retrieved for the LLM to learn from. We set the number of examples to 3, using the BM25 retrieval algorithm. Each example consists of text generated by an LLM and text written by a human. <step1 response> refers to the answer from the first round of questioning, where the model extracts key distinct features of human writings. The <sentences num> refers to the word length of the corresponding human writing sample, and <prefix> is the specific article title.

#### d.4.2 Paraphrase attacks

Paraphrase attacks involve rewriting text to preserve its original meaning. We utilize various techniques, including the DIPPER paraphrasing tool [23], Back-translation, and Polishing with LLMs. The Discourse Paraphraser (DIPPER), as described by [23], is an advanced 11-billion parameter

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Attacks Typts** & **Sub Types** & **Methods** \\ \hline
**Direct Prompt** & Direct Prompt & Prompt \\
**Prompt Attacks** & Few-Shot Prompt & Prompt \\  & ICO Prompt & Prompt \\  & DIPPER Paraphrase & DIPPER Paraphraser \\
**Paraphrase Attacks** & Polish Using LLMs & Prompt \\  & Back Translation & Google Translation API \\  & Character-Level Perturbation & TextFooler \\
**Perturbation Attacks** & Word-Level Perturbation & DeepBugWord \\  & Sentence-Level Perturbation & TextBugger \\
**Data Mixing** & Multi-LLMs Mixing & Sentence Mixing \\  & LLM-Centered Mixing & Sentence Mixing \\ \hline \hline \end{tabular}
\end{table}
Table 8: Data attacks settings.

model designed for generating paraphrases by considering context and managing lexical diversity and content order. Inspired by real-world applications, we implement machine translation for paraphrasing through back-translation. Specifically, we use the Google Translate API9 to translate each LLM-generated sample from English to Chinese and then back to English. To ensure that the text maintains good semantic consistency before and after Back-translation, we use BERTScore [68], an automatic evaluation metric that assesses translation quality from a semantic perspective. Polishing with LLMs is a widely used paraphrasing method in the era of LLMs, typically initiated via prompts. Below, we provide an example of polishing with LLMs for academic writing tasks:

Footnote 9: https://translate.google.com/

```
[{'role':'user','content':'Giventheacademicarticleabstract,

polishthewritingtomeethereviewstyle,improvethespelling,

grammar,clarity,concisionandoverallreadability:\nacademicarticle abstract:<prefix>'},

] ```

7: PolishPrompt

The <prefix> is the specific article abstract, and the response is supposed to polish the provided academic article abstract.

#### d.4.3 Perturbation attacks

Perturbation attacks primarily focus on adversarial perturbations on the text directly generated by LLMs, effectively simulating post-processing of LLM-generated text by humans and common writing errorslike spelling mistakes in real life. Our approach employs adversarial perturbation methods include TextFooler [25], DeepWordBug [24], and TextBugger [24], which correspond to word-level, character-level, and sentence-level adversarial perturbations, respectively. DeepWordBug [24] is a black-box perturbation method that can efficiently generate character-level text perturbations with the goal of minimizing the edit distance of the perturbation. TextFooler [25] is a text-based adversarial method that uses synonyms to replace words in a sentence that are vulnerable to attacks while maintaining good grammatical correctness and semantic coherence. TextBugger [24] creates adversarial texts suitable for real-world applications, ensuring that the adversarial samples remain visually and semantically consistent with the originals, and considers both character and word-level perturbations. All perturbation attacks are implemented using the TextAttacks [26] framework.

\begin{table}
\begin{tabular}{l l|c|c|c|c|c|c|c|c|c|c|c} \hline \hline Domains & Channel & Direct & Prompt Attacks & \multicolumn{3}{c|}{Parglephrase Attacks} & \multicolumn{3}{c|}{Perturbation Attacks} & \multicolumn{3}{c|}{Data Mining} & \multicolumn{1}{c}{Total} \\ \cline{2-13} \multicolumn{1}{c}{} & \multicolumn{1}{c|}{\#Datasets} & \multicolumn{1}{c|}{FP} & \multicolumn{1}{c|}{IP} & \multicolumn{1}{c|}{DP} & \multicolumn{1}{c|}{PP} & \multicolumn{1}{c|}{BP} & \multicolumn{1}{c|}{CP} & \multicolumn{1}{c|}{WP} & \multicolumn{1}{c|}{SP} & \multicolumn{1}{c|}{MM} & \multicolumn{1}{c|}{LM} & \\ \hline \hline Axiiv Abstracts & Human & 2,800 & - & - & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & - & 2,800 & 25,200 \\  & GPT.3-5-turbo & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & Claude-instator & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & PLM-2-bicon & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & Llama-2-700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\ \hline XSum & Human & 2,800 & - & - & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & 25,200 \\  & GPT.3-5-turbo & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & Claude-instator & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & PLM-2-bicon & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & Llama-2-700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\ \hline Writing Promptes & Human & 2,800 & - & - & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & - & 2,800 & 25,200 \\  & GPT.3-5-turbo & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & Claude-instator & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & PLM-2-bicon & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & Llama-2-700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\ \hline Yelp Reviews & Human & 2,800 & - & - & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & 2,800 & 25,200 \\  & GPT.3-5-turbo & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & Claude-instator & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & PLM-2-bicon & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\  & Llama-2-700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 700 & 8,400 \\ \hline Total & - & 22,400 & 11,200 & 11,200 & 22,400 & 22,400 & 22,400 & 22,400 & 22,400 & 11,200 & 22,400 & **235,200** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Datasets Statistics. **FP** stands for Few-Shot Prompt, **IP** stands for ICO Prompt; **DP** represents DIPPER paraphrase, **PP** represents Polishing with LLMs, **BP** represents Back-translation paraphrase; **CP** stands for Character-Level perturbation, **WP** stands for Word-Level perturbation, **SP** stands for Sentence-Level perturbation; **MM** represents Multi-LLMs Mixing, **LM** represents LLM-Centered Mixing.

#### d.4.4 Data mixing

Data mixing is a common real-world scenario. Our data mixing methods include a mixing of texts generated by various LLMs (Multi-LLMs Mixing) and texts centered around LLM-generated text with a mixing of human-written text (LLM-Centered Mixing). The Multi-LLMs Mixing refers to a single text composed of sentences from different generative models. LLM-Centered Mixing involve replacing one quarter of the sentences in an LLM-generated text with human-written text at random. To facilitate this, we ensured that both human-written and LLM-generated texts contained at least four sentences during collection, providing a solid foundation for our data mixing process.

For Multi-LLMs Mixing, we sample and recombine sentences from texts generated by four different LLMs, aligning with the length of human-written texts to create a new sample. Similarly, for LLM-Centered Mixing, one-quarter of the sentences in the LLM-generated text are randomly replaced with sentences from the corresponding human-written text. This approach presents a more challenging scenario, as the data-mixed samples often lack coherent semantics.

### Datasets statistics

The statistics for the curated datasets are presented in Table 9. The datasets include 100,800 human-written samples, consisting of 11,200 raw samples and 89,600 that have undergone attack manipulations. Additionally, there are 134,400 samples generated by LLMs, categorized as follows: 11,200 with direct prompt, 22,400 with prompt attacks, 33,600 with paraphrase attacks, 33,600 with perturbation attacks, and 22,400 involving data mixing.

### Textual features analysis

In this section, we analyze the textual features of _DetectRL_ samples to provide additional potentially valuable insights.

Text lengthWe performed a statistical analysis of text length distribution in _DetectRL_, as shown in Figure 4. Compared to academic writing and social media texts, news writing and creative writing exhibit notably longer average lengths. The distributions for texts generated by Claude-instant, PaLM-2-bison, and Llama-2-70b are similar, whereas GPT-3.5-turbo tends to produce longer texts. Additionally, we observed that samples subjected to attack manipulation show almost no significant difference in length, except in the data mixing setup.

Figure 4: Text length distribution of _DetectRL_.

Figure 5: \(N\)-gram distribution of _DetectRL_.

\(N\)-gramsWe performed statistical analysis of the \(n\)-gram distribution in _DetectRL_, focusing on unigrams, bigrams, and trigrams. The results are presented in Figure 5. Among the four domains, creative writing exhibits the greatest variety of unigrams, bigrams, and trigrams, indicating a higher \(n\)-gram diversity. In contrast, academic writing shows the lowest diversity. Among the different LLMs, GPT-3.5-turbo demonstrates the most extensive vocabulary usage, followed by Claude-instant, Llama-2-70b, and PaLM-2-bison, in order of decreasing \(n\)-gram richness. Additionally, samples with perturbation attack show the highest \(n\)-gram diversity due to the substitution of characters and vocabulary. Notably, in samples involving data mixing, \(n\)-gram richness is significantly lower, approximately half that of other sample types.

ReadabilityWe carried out a statistical analysis of the readability distribution within _DetectRL_, primarily by calculating the Flesch Reading Ease Score (FRES) for each sample. The FRES [69] assesses reading difficulty by considering word length and sentence length. The formula used to calculate this score is as follows:

\[\text{FRES}=206.835-1.015\times\left(\frac{\text{Total Words}}{ \text{Total Sentences}}\right)\] (1) \[-84.6\times\left(\frac{\text{Total Syllabes}}{\text{Total Words}}\right)\]

Scores range from 0 to 100, with higher scores indicating better readability. The results revealed significant differences in text readability across various domains. Among all categories, creative writing texts exhibit the highest readability, followed by social media and news writing texts, while academic writing texts are the least readable. When comparing texts generated by different LLMs, we observed that the texts produced by Claude-instant, PaLM-2-bison, and Llama-2-70b show a high degree of consistency in readability. However, texts generated by GPT-3.5-turbo show a noticeable readability gap compared to the others. Similarly, texts generated from direct prompts, prompt attacks, paraphrase attacks, and perturbation attacks display a comparable distribution of readability scores. Yet, samples processed through data mixing show lower average readability, likely due to the inclusion of human-written texts.

Lexical diversityWe conducted a statistical analysis of the Lexical Diversity Score (LDS) within _DetectRL_, using the following formula to calculate this feature:

Figure 6: Readability distribution of _DetectRL_.

Figure 7: Lexical diversity distribution of _DetectRL_.

[MISSING_PAGE_FAIL:26]

We use Youden's J statistic to determine the optimal threshold for the detectors. This approach achieves the best balance between the TPR and the FPR, thereby maximizing the overall correct classification rate.

Log-Likelihood [30]A simple zero-shot method employs a language model to calculate the log-probability for each token within a text. A higher average log-likelihood indicates a greater likelihood that the text is generated by an LLM.

Entropy [29]A zero-shot method relies on entropy to assess the randomness of text in order to identify text generated by LLMs. Human-written text typically shows more unpredictable variations. Consequently, text with lower entropy is more likely to have been produced by an LLM.

Rank [30]A zero-shot method assigns a rank score to each token based on the previous context. By calculating the average score, a higher average rank score suggests a greater likelihood that the text is generated by an LLM.

Log-Rank [30]An enhanced version of the Rank-based method. It uses a language model to calculate the logarithmic rank score of each word in the text. By calculating the average score, a higher average log-rank score suggests a greater likelihood that the text is generated by an LLM.

Lrr [31]The Log-Likelihood Log-Rank Ratio (LRR), an enhanced zero-shot method that effectively integrates Log-Likelihood and Log-Rank. Text with a higher LRR is more likely to be generated by an LLM.

Npr [31]The Normalized Perturbed Log-Rank (NPR) is a zero-shot method that identifies differences by comparing the Log-Rank scores of perturbed human-written text with those generated by LLMs. Text with a higher NPR is more likely to be generated by an LLM.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline
**Settings \(\rightarrow\)** & \multicolumn{4}{c}{**DIPPER Paraphrase**} & \multicolumn{4}{c}{**Polish using LLMs**} & \multicolumn{4}{c}{**Back Translation**} \\
**Detectors \(\downarrow\)** & Pre & Rec & \(F_{1}\) & AUROC & Pre & Rec & \(F_{1}\) & AUROC & Pre & Rec & \(F_{1}\) & AUROC \\ \hline \multicolumn{11}{c}{**Zero-shot Detectors**} \\ \hline
**Log-Likelihood** & 85.60 & 80.85 & 83.16 & 91.30 & 77.73 & 64.78 & 70.67 & 78.61 & 100.0 & 00.29 & 00.59 & 22.03 \\
**Entropy** & 00.00 & 00.00 & 00.00 & 32.42 & 50.02 & 100.66 & 66.68 & 32.66 & 74.36 & 72.81 & 73.58 & 79.77 \\
**Rank** & 77.91 & 73.51 & 75.65 & 82.91 & 69.43 & 63.09 & 66.11 & 72.38 & 100.0 & 0.29 & 00.59 & 24.77 \\
**Log-Rank** & 85.18 & 82.14 & 83.63 & 91.47 & 77.91 & 62.30 & 69.23 & 77.40 & 85.71 & 00.59 & 01.18 & 23.52 \\
**LRR** & 86.21 & 77.57 & 81.67 & 88.94 & 76.98 & 51.09 & 61.41 & 70.33 & 62.96 & 00.55 & 09.36 & 31.15 \\
**NPR** & 75.54 & 68.57 & 70.37 & 77.20 & 66.70 & 59.02 & 62.63 & 68.37 & 75.00 & 00.29 & 00.59 & 24.47 \\
**DetectGT** & 66.23 & 20.70 & 31.54 & 45.90 & 72.72 & 17.94 & 31.17 & 47.83 & 00.00 & 00.00 & 00.00 & 03.81 \\
**DNA-GPT** & 82.78 & 76.78 & 79.67 & 88.33 & 73.26 & 66.67 & 69.48 & 77.33 & 100.0 & 01.68 & 03.31 & 28.77 \\
**Rewise-Detect** & 85.68 & 89.08 & 87.35 & 94.03 & 81.11 & 69.44 & 78.42 & 84.17 & 00.00 & 00.00 & 00.00 & 20.41 \\
**Binoculars** & 95.68 & 88.99 & 91.73 & 96.58 & 95.68 & 70.43 & 81.14 & 85.04 & 65.08 & 40.27 & 49.90 & 58.22 \\ Fast-Detect(GPT) & 72.92 & 63.59 & 67.93 & 75.06 & 78.42 & 37.50 & 50.73 & 61.68 & 00.00 & 00.00 & 00.00 & 17.77 \\ Avg. & 73.97 & 63.28 & 68.42 & 85.85 & 64.81 & 60.32 & 64.00 & 68.70 & 60.32 & 10.99 & 12.64 & 30.42 \\ \hline \multicolumn{11}{c}{**Supervised Detectors**} \\ \hline
**Rob-Base** & 99.00 & 99.00 & 99.00 & 99.90 & 99.30 & 98.90 & 99.10 & 99.95 & 100.0 & 99.40 & 99.70 & 99.97 \\
**Rob-Large** & 99.70 & 99.40 & 99.55 & 99.91 & 98.62 & 99.50 & 99.06 & 99.89 & 99.90 & 98.98 & 99.85 & 99.99 \\
**X-Rob-Base** & 97.34 & 98.31 & 97.82 & 99.56 & 94.15 & 97.51 & 95.80 & 98.69 & 97.42 & 97.91 & 97.57 & 99.13 \\
**X-Rob-Large** & 98.52 & 99.60 & 99.06 & 99.77 & 98.33 & 99.30 & 98.81 & 99.93 & 100.0 & 99.50 & 99.75 & 99.75 \\
**Avg.** & 98.64 & 99.08 & 98.86 & 99.78 & 97.60 & 98.80 & 98.19 & 99.61 & 99.61 & 99.28 & 99.15 & 99.22 & 99.71 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Parameters for supervised detectors training.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Parameters** & **Settings** \\ \hline Learning Rate & 1e-6 \\ Batch Size & 8 \\ Epochs & 3 \\ Seed & 2023 \\ GPU Envs & NVIDIA GeForce RTX 3090 24GB \\ \hline \hline \end{tabular}
\end{table}
Table 12: Parameters for supervised detectors training.

DetectGPT [27]A zero-shot method for detection using probabilistic curvature. It utilizes random perturbations of paragraphs from a general pre-trained language model and discriminates LLM-generated text through the statistical curvature threshold of log probabilities.

DNA-GPT [34]A zero-shot detection method that utilizes _N_-gram analysis or probability divergence in a white-box setting to compare the differences between the truncated original text and the text completed by a language model. A higher score suggests a greater likelihood that the text was generated by an LLM.

Revise-Detect. [33]A zero-shot black-box method based on the intuition that ChatGPT makes fewer edits to text generated by LLMs compared to human-written text. If the similarity between the text and its ChatGPT-revised version is higher, the text is more likely to be generated by an LLM.

Binoculars [35]A zero-shot detection method employs a pair of LLMs to calculate the ratio of perplexity to cross-perplexity. This evaluates how one model reacts to the next token predictions of another model. A lower score suggests that the text is more likely generated by an LLM.

Fast-DetectGPT [32]An optimized zero-shot detector that replaces the perturbation step of DetectGPT with a more efficient sampling step. We chose the optimal settings reported by the authors, using GPT-Neo-2.7b as the scoring model and GPT-J-6b [71] as the reference model.

RoBERTa Classifier [36]A popular and competitive detector method. Recognize LLM generated text by fine-tuning the RoBERTa classifier on large amounts of labeled text.

XLM-RoBERTa Classifier [37]A multi-lingual version of RoBERTa. We use XLM-RoBERTa-Base and XLM-RoBERTa-Large to build detectors to explore the potential of multilingual supervised methods.

## Appendix F Additional experiment results

### Detailed robustness analysis against different types of attacks.

In this section, we will further discuss the performance of detectors against various specific attack methods. Our study on prompt attack, including Few-Shot Prompt and ICO Prompt, revealed that these methods have minimal impact on detector performance. Both zero-shot and supervised detectors showed only a 1-2% decrease in average AUROC performance. This indicates that efforts to guide models to mimic human writing through such instructions may not effectively evade detection.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline
**Settings \(\rightarrow\)** & \multicolumn{4}{c}{**Char-Level Perturbation**} & \multicolumn{4}{c}{**Word-Level Perturbation**} & \multicolumn{4}{c}{**Sentence-Level Perturbation**} \\
**Detectors \(\downarrow\)** & Pre & Rec & \(F_{1}\) & AUROC & Pre & Rec & \(F_{1}\) & AUROC & Pre & Rec & \(F_{1}\) & AUROC \\ \hline \multicolumn{11}{c}{**Zero-shot Detectors**} \\ \hline
**Log-Likelihood** & 60.00 & 00.29 & 00.59 & 28.33 & 80.00 & 00.97 & 01.57 & 39.35 & 75.00 & 00.59 & 01.18 & 39.25 \\
**Entropy** & 73.65 & 70.73 & 72.16 & 77.12 & 60.30 & 69.94 & 64.76 & 62.70 & 61.51 & 72.61 & 66.60 & 65.28 \\
**Rank** & 00.00 & 00.00 & 00.00 & 00.68 & 00.00 & 00.00 & 00.43 & 80.00 & 00.00 & 00.00 & 10.42 \\
**Log-Rank** & 66.66 & 00.59 & 01.17 & 28.96 & 78.57 & 01.09 & 02.15 & 42.79 & 72.22 & 00.79 & 01.57 & 41.49 \\
**LRR** & 71.42 & 01.98 & 03.86 & 33.43 & 63.67 & 29.96 & 41.28 & 53.79 & 72.59 & 14.98 & 24.83 & 49.40 \\
**NPR** & 00.00 & 00.00 & 00.00 & 00.82 & 00.00 & 00.00 & 00.29 & 00.00 & 00.00 & 00.00 & 00.840 \\
**DetectGPT** & 00.00 & 00.00 & 00.00 & 16.83 & 00.00 & 00.00 & 00.00 & 16.27 & 00.00 & 00.00 & 00.00 & 22.90 \\
**DNA-GPT** & 88.88 & 01.58 & 03.11 & 35.44 & 94.44 & 61.68 & 33.31 & 41.35 & 81.81 & 01.78 & 03.49 & 45.54 \\
**Revis-Detect** & 00.00 & 00.00 & 00.41 & 48.42 & 64.78 & 48.45 & 65.72 & 67.80 & 00.00 & 00.00 & 00.34 & 54.51 \\
**Binoculars** & 83.98 & 61.40 & 70.94 & 75.65 & 82.99 & 57.14 & 67.68 & 73.10 & 88.30 & 62.20 & 72.99 & 79.56 \\ Fast-DetectGPT & 73.11 & 42.36 & 53.64 & 61.99 & 63.63 & 00.69 & 01.37 & 31.69 & 73.19 & 46.32 & 56.74 & 66.49 \\ Avg. & 47.06 & 16.26 & 18.67 & 38.00 & 53.68 & 20.55 & 22.42 & 39.65 & 47.73 & 18.11 & 20.67 & 42.11 \\ \hline \multicolumn{11}{c}{**Supervised Detectors**} \\ \hline
**Rob-Base** & 99.70 & 99.80 & 99.75 & 99.99 & 98.31 & 98.51 & 98.41 & 97.99 & 100.0 & 99.30 & 99.65 & 99.99 \\
**Rob-Large** & 99.90 & 99.80 & 99.95 & 99.96 & 99.50 & 99.90 & 99.70 & 99.98 & 100.00 & 99.40 & 99.70 & 99.97 \\
**X-Rob-Base** & 99.90 & 99.50 & 99.70 & 99.97 & 97.98 & 96.52 & 97.25 & 97.27 & 99.89 & 98.51 & 99.20 & 99.92 \\
**X-Rob-Large** & 99.90 & 99.80 & 99.85 & 99.99 & 99.95 & 99.90 & 99.70 & 99.99 & 99.89 & 99.90 & 99.45 & 99.60 \\ Avg. & 99.85 & 99.73 & 99.79 & 99.98 & 98.82 & 98.71 & 98.77 & 99.26 & 99.94 & 99.05 & 99.50 & 99.89 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Performance of perturbation attacks.

[MISSING_PAGE_FAIL:29]

Figure 10: Generalization in multi-attack.

Figure 9: Generalization in multi-LLM.

\begin{table}
\begin{tabular}{l|l} \hline \hline Setting & Text \\ \hline \multicolumn{3}{c}{**Human-Written Text**} \\ \hline Human & The standard C*-algebraic version of the algebra of canonical commutation relations, the Weyl algebra, frequently causes difficulties in applications since it neither admits the formulation of physically interesting dynamic laws nor does it incorporate pertinent physical observables such as (bounded functions of) the Hamiltonian.... \\ \hline Polish Using LLM & The standard C*-algebraic version of the **Weyl algebra**, **which describes** canonical commutation relations, **often poses challenges** in applications. **It hinders** the formulation of physically **meaningful dynamical laws and the **incorporation of relevant** physical observables, such as bounded functions of the Hamiltonian.... \\ \hline Back Translation & Standard C*algebra version of the **specification exchange** algebra, **Weldid numbers often cause** difficulties in applications, **because** it neither **recognizes interesting dynamic expressions**, nor **relevant** physical observable objects, such as **Hamilton, Hamilton (Limited).... \\ \hline DIPPER Paraphrase & **Here we present a new C*-algebra of the canonical commutation relations which does not suffer from these problems. It is based on the resolvents of the canonical operators and their algebraic relations. The resulting C*-algebra, the resolvent algebra, is shown to have many desirable analytic properties and the regularity structure of its representations is surprisingly simple....** \\ \hline Character Perturbation & The standard C*-algebrace version of the algebra of canonical commutation relations, the Weyl algebra, frequently causes difficulties in applications since it neither admits the formulation of physically **outstanding dynamicallaws** nor does it incorporate pertinent physical observables such as (bounded functions of) the Hamiltonian.... \\ \hline Word Perturbation & The standard C*-algebraic version of the algebra of canonical commutation relations, the Weyl **math**, frequently causes **hardship** in applications since it neither admits the formulation of physically **outstanding dynamicallaws** nor does it incorporate **thereto** physical observables such as (bounded functions of) the Hamiltonian.... \\ \hline Sentence Perturbation & The standard C*-algebraic version of the algebra of canonical commutation relations, the Weyl **algera**, frequently causes difficulties in applications since it neither admits the **devising** of physically interesting dynamical laws nor does it incorporate pertinent physical observables such as (bounded functions of) the Hamiltonian.... \\ \hline \hline \end{tabular}
\end{table}
Table 14: Academic writing samples written by **human** in _DetectRL_. We use blue to mark human-written text, green to mark the modified parts in human-written text after paraphrase attacks, and orange to mark the modified parts in human-written text after perturbation attacks.

\begin{table}
\begin{tabular}{l|l} \hline \hline Setting & Text \\ \hline \multicolumn{3}{c}{**LLM-Generated Text**} \\ \hline Direct Prompt & The resolvent algebra is a new mathematical structure that provides a powerful framework for studying quantum systems. It is based on the idea of using the resolvent operator, which is the inverse of the energy operator, as the basic building block for constructing the algebra. \\ \hline Few-Shot Prompt & The resolvent algebra is a new approach to canonical quantum systems. It is a *-algebra of operators that is generated by the resolvent of the Hamiltonian and the identity operator. The resolvent algebra is a natural setting for the study of quantum systems, as it provides a unified framework for describing both the classical and quantum aspects of a system. \\ \hline ICO Prompt & In this paper, we introduce a new algebraic structure, called the resolvent algebra, which provides a unified framework for studying a wide class of canonical quantum systems. The resolvent algebra is a *-algebra with involution, which is generated by the resolvents of the system’s Hamiltonian. \\ \hline Polish Using LLM & The resolvent algebra is a novel mathematical structure offering a robust framework for studying quantum systems. **This framework** is rooted in the concept of using the resolvent operator, the inverse of the energy operator, as the fundamental building block for constructing the algebra. \\ \hline Back Translation & **Analysis algebra is a new mathematical structure that provides a powerful framework for studying the quantum system. It is based on the use of decomposition operators (the countdowner counts) as the basic construction block of the construction algebra. \\ \hline DIPPER Paraphrase & The resolvent algebra is a *-algebra, which means that **it has a natural notion of multiplication** and involution. It also has a natural topology, which makes it possible to study the structure of the algebra in a rigorous way. It is based on the idea of using the resolvent operator, which is the inverse of the energy operator, as the basic building block for constructing the algebra. \\ \hline Character Perturbation & The resolvent algebra is a new mathematical structure that provides a powerful framework for studying quantum systems. It is based on idea of using the resolvent operator, which is the **inversG** of the energy operator, as the basic **building** block for constructing the algebra. \\ \hline Word Perturbation & The resolvent algebra is a new mathematical structure that provides a powerful framework for **investigation** quantum systems. He is based on the **thought** of using the resolvent operator, which is the inverse of the energy operator, as the basic building block for constructing the algebra. \\ \hline Sentence Perturbation & The resolvent algebra is a new **mathematical** structure that provides a powerful framework for studying quantum systems. It is based on the idea of using the resolvent operator, which is the inverse of the energy **operandi**, as the basic building block for constructing the algebra. \\ \hline \hline \end{tabular}
\end{table}
Table 15: Academic writing samples generated by **PaLM-2-bison** in _DetectRL_. We use pink to mark LLM-generated text, red to mark text generated by the selected prompt, green to mark the modified parts in LLM-generated text after paraphrase attacks, and orange to mark the modified parts in LLM-generated text after perturbation attacks.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Abstract and section 1. 2. Did you describe the limitations of your work? See Appendix B. 3. Did you discuss any potential negative societal impacts of your work? See Appendix C. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See Appendix D for data collection details and Appendix E.2 for detectors settings. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Our random seeds are only used for sampling data and will not affect the fairness of the experimental results. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? See Table E.2.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? We borrow or extend some code from Fast-DetectGPT [27], and we acknowledge this and cite the relevant works in our experimental setup. 2. Did you mention the license of the assets? The datasets used are completely open source and public to the research community. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? All data and code will be completely open source and no permission is required. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? See Appendix C.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]