# Can Large Language Model Agents Simulate Human Trust Behavior?

 Chengxing Xie

Equal Contribution. Correspondence to: Chengxing Xie <xiechengxing34@gmail.com>, Canyu Chen <ccchen151@hawk.iit.edu>, Guohao Li <guohao@robots.ox.ac.uk>.

Canyu Chen

Work performed while Guohao Li was at KAUST and Chengxing Xie was a visiting student at KAUST. University of Michigan

Feiran Jia\({}^{4}\) Ziyu Ye\({}^{5}\) Shiyang Lai\({}^{5}\) Kai Shu\({}^{6}\) Jindong Gu\({}^{3}\) Adel Bibi\({}^{3}\) Ziniu Hu\({}^{7}\)

David Jurgens\({}^{8}\) James Evans\({}^{5,\,9,\,10}\) Philip H.S. Torr\({}^{3}\) Bernard Ghanem\({}^{1}\) Guohao Li \({}^{\ddagger,\,11}\)

\({}^{1}\)KAUST \({}^{2}\)Illinois Institute of Technology \({}^{3}\)University of Oxford \({}^{4}\)Pennsylvania State University

\({}^{5}\)University of Chicago \({}^{6}\)Emery \({}^{7}\)California Institute of Technology

\({}^{8}\)University of Michigan \({}^{9}\)Santa Fe Institute \({}^{10}\)Google \({}^{11}\)CAMEL-ALorg

###### Abstract

Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: _can LLM agents really simulate human behavior?_ In this paper, we focus on one critical and elemental behavior in human interactions, _trust_, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as _agent trust_, under the framework of _Trust Games_, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high _behavioral alignment_ with humans in terms of trust behavior, indicating _the feasibility of simulating human trust behavior with LLM agents_. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond _value alignment_. We further illustrate broader implications of our discoveries for applications where trust is paramount.

## 1 Introduction

There is an increasing trend to adopt Large Language Models (LLMs) as agent-based simulation tools for humans in various social science fields including economics, politics, psychology, ecology and sociology (Gao et al., 2023; Manning et al., 2024; Ziems et al., 2023), and role-playing applications such as assistants, companions and mentors (Yang et al., 2024; Abdelghani et al., 2023; Chen et al., 2024) due to their human-like cognitive capacity. Nevertheless, most previous research is based on one insufficiently validated assumption that LLM agents behave like humans in simulation. Thus, a fundamental question remains: _Can LLM agents really simulate human behavior?_

In this paper, we focus on _trust_ behavior in human interactions, which comprises the intention to place self-interest at risk based on the positive expectations of others (Rousseau et al., 1998). Trust is one of the most critical and elemental behaviors in human interactions and plays an essential role in social settings ranging from daily communication to economic and political institutions (Uslaner, 2000; Coleman, 1994). Here, we investigate _whether LLM agents can simulate human trust behavior_, paving the way to explore their potential to simulate more complex human behavior and society itself.

First, we explore whether LLM agents manifest trust behavior in their interactions. Given the challenge of quantifying trust behavior, we choose to study them based on the Trust Game and itsvariations (Berg et al., 1995; Glaeser et al., 2000), which are established methodologies in behavioral economics. We adopt the _Belief-Desire-Intention_ (BDI) framework (Rao et al., 1995; Andreas, 2022) to model LLM agents' reasoning process for decision-making explicitly. Based on existing measurements for trust behavior in the Trust Game and the BDI interpretations of LLM agents, we achieve our first core finding: **LLM agents generally exhibit trust behavior in the Trust Game**.

Then, we refer to LLM agents' trust behavior as _agent trust_ and humans' trust behavior as _human trust_, and aim to investigate whether agent and human trust align, implying the possibility of simulating human trust behavior with LLM agents. Next, we propose a new concept, _behavioral alignment_, as the alignment between agents and humans concerning factors that impact behavior (namely _behavioral factors_), and dynamics that evolve over time (namely _behavioral dynamics_). Based on human studies, three basic behavioral factors underlie trust behavior including reciprocity anticipation (Berg et al., 1995), risk perception (Bohnet & Zeckhauser, 2004) and prosocial preference (Alos-Ferrer & Farolfi, 2019). Comparing the results of LLM agents with existing human studies in Trust Games, we have our second core finding: **GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior**, suggesting the feasibility of using agent trust to simulate human trust, although **LLM agents with fewer parameters show relatively lower behavioral alignment**. This finding lays the foundation for simulating more complex human interactions and societal institutions, and enriches our understanding of the analogical relationship between LLMs and humans.

In addition, we more deeply probe the intrinsic properties of agent trust across four scenarios. First, we examine whether changing the other player's demographics impacts agent trust. Second, we study differences in agent trust when the other player is an LLM agent versus a human. Third, we directly manipulate agent trust with explicit instructions "you need to trust the other player" and "you must not trust the other player". Fourth, we adjust the reasoning strategies of LLM agents from direct reasoning to zero-shot Chain-of-Thought reasoning (Kojima et al., 2022). These investigations lead to our third core finding: **agent trust exhibits bias across different demographics, has a relative preference for humans over agents, is easier to undermine than to enhance, and may be influenced by advanced reasoning strategies**. Our contributions can be summarized as:

* We propose a definition of LLM agents' _trust_ behavior under Trust Games and a new concept of _behavioral alignment_ as the human-LLM analogy regarding _behavioral factors_ and _dynamics_.
* We discover that LLM agents generally exhibit _trust_ behavior in Trust Games and GPT-4 agents manifest high _behavioral alignment_ with humans in terms of trust behavior, indicating the great potential to simulate human trust behavior with LLM agents. Our findings pave the way for simulat

Figure 1: **Our Framework for Investigating Agent Trust as well as its Behavioral Alignment with Human Trust. First, this figure shows the major components for studying the trust behavior of LLM agents with Trust Games and Belief-Desire-Intention (BDI) modeling. Then, our study centers on examining the behavioral alignment between LLM agents and humans regarding trust behavior.**

ing complex human interactions and social institutions, and open new directions for understanding the fundamental analogy between LLMs and humans beyond _value alignment_.
* We investigate _intrinsic properties_ of agent trust under manipulations and reasoning strategies, as well as biases of agent trust and differences in agent trust towards agents versus humans.
* We illustrate broader _implications_ of our discoveries about agent trust and its behavioral alignment with human trust for human simulation in social science and role-playing applications, LLM agent cooperation, human-agent collaboration and the safety of LLM agents, detailed further in Section 6.

## 2 LLM Agents in Trust Games

### Trust Games

Trust Games, referring to the Trust Game and its variations, have been widely used for examining human trust behavior in behavioral economics (Berg et al., 1995; Lenton and Mosley, 2011; Glaeser et al., 2000; Cesarini et al., 2008). As shown in Figure 1, the player who makes the first decision to send money is called the _trustor_, while the other one who responds by returning money is called the _trustee_. In this paper, we mainly focus on the following six types of Trust Games (the specific prompt for each game is articulated in the Appendix H.2):

Game 1: Trust GameAs shown in Figure 1, in the Trust Game (Cox, 2004; Berg et al., 1995), the trustor initially receives \(\$10\). The trustor selects \(\$N\) and sends it to the trustee, exhibiting _trust behavior_. Then the trustee will receive \(\$3N\), and have the option to return part of that \(\$3N\) to the trustor, showing _reciprocal behavior_.

Game 2: Dictator GameIn the Dictator Game (Cox, 2004), the trustor also needs to send \(\$N\) from the initial \(\$10\) to the trustee and then the trustee will receive \(\$3N\). Compared to the Trust Game, the only difference is that the trustee does not have the option to return money in the Dictator Game and the trustor is also aware that the trustee cannot reciprocate.

Game 3: MAP Trust GameIn the MAP Trust Game (MAP represents Minimum Acceptable Probabilities) (Bonnet and Zeckhauser, 2004), a variant of the Trust Game, the trustor needs to choose whether to trust the trustee. If the trustor chooses not to trust the trustee, each will receive \(\$10\); If the trustor and the trustee both choose to trust, each will receive \(\$15\); If the trustor chooses to trust, but the trustee does not, the trustor will receive \(\$8\) and the trustee will receive \(\$22\). There is probability \(p\) that the trustee will choose to trust and \((1-p)\) probability that they will not choose to trust. MAP is defined as the minimum value of \(p\) at which the trustor would choose to trust the trustee.

Game 4: Risky Dictator GameThe Risky Dictator Game (Bohnet and Zeckhauser, 2004) differs from the MAP Trust Game in only a single aspect. In the Risky Dictator Game, the trustee is present but does not have the choice to trust or not and the money distribution relies on the pure probability \(p\). Specifically, if the trustor chooses to trust, there is probability \(p\) that both the trustor and the other player will receive \(\$15\) and probability \((1-p)\) that the trustor will receive \(\$8\) and the other player will receive \(\$22\). If the trustor chooses not to trust the trustee, each player will receive \(\$10\).

Game 5: Lottery GameThere are two typical Lottery Games (Fetchenhauer and Dunning, 2012). In the Lottery People Game, the trustor is informed that the trustee chooses to trust with probability \(p\). Then the trustor must choose between receiving fixed money or trusting the trustee, which is similar to the MAP Trust Game. In the Lottery Gamble Game, the trustor chooses between playing a gamble with a winning probability of \(p\) or receiving fixed money. \(p\) is set as \(46\%\) following the human study.

Game 6: Repeated Trust GameWe follow the setting of the Repeated Trust Game in (Cochard et al., 2004), where the Trust Game is played for multiple rounds with the same players and each round begins anew with the trustor allocated the same initial money.

### LLM Agent Setting

In our study, we set up our experiments using the CAMEL framework (Li et al., 2023a) with both closed-source and open-source LLMs including GPT-4, GPT-3.5-turbo-0613, GPT-3.5-turbo-16k-0613, text-davinci-003, GPT-3.5-turbo-instruct, Llama2-7b (or 13b, 70b) and Vicuna-v1.3-7b (or 13b, 33b) (Ouyang et al., 2022; Achiam et al., 2023; Touvron et al., 2023; Chiang et al., 2023). We set the temperature as \(1\) to increase the diversity of agents' decision-making and note that high temperatures are commonly adopted in related literature (Aher et al., 2023; Lore and Heydari, 2023; Guo, 2023).

Agent Persona.To better reflect the setting of real-world human studies (Berg et al., 1995), we design LLM agents with diverse personas in the prompt. Specifically, we ask GPT-4 to generate 53types of personas based on a given template. Each persona needs to have information including name, age, gender, address, job and background. Examples of the personas are shown in Appendix H.1.

**Belief-Desire-Intention (BDI).** The BDI framework is a well-established approach in agent-oriented programming (Rao et al., 1995) and was recently adopted to language models (Andreas, 2022). We propose modeling LLM agents in Trust Games with the BDI framework to gain deeper insights into LLM agents' behaviors. Specifically, we let LLM agents directly output their Beliefs, Desires, and Intentions as the reasoning process for decision-making in Trust Games.

## 3 Do LLM Agents Manifest Trust Behavior?

In this section, we investigate whether or not LLM agents manifest trust behavior by letting LLM agents play the Trust Game (Section 2.1 Game 1). In Behavioral Economics, trust is widely measured by the initial amount sent from the trustor to the trustee in the Trust Game (Glaeser et al., 2000; Cesarini et al., 2008). Following the measurement of trust in human studies and the assumption humans own reasoning processes that underlie their decisions, we can define the conditions that LLM agents manifest trust behavior in the Trust Game as follows. _First_, **the amount sent is positive and does not exceed the amount of money the trustor initially possesses**, which implies that the trustor places self-interest at risk with the expectation the trustee will reciprocate and that the trustor understands the money limit that can be given. _Second_, **the decision (_i.e., amounts sent_) can be interpreted as the reasoning process (_i.e.,_ the BDI) of the trustor**. We explored utilizing BDI to model the reasoning process of LLM agents. If we can interpret the decision as the articulated reasoning process, we have evidence that LLM agents do not send a random amount of money and manifest some degree of rationality in the decision-making process. Then, we assess whether LLM agents exhibit trust behavior based on two aspects: the amount sent and the BDI.

### Amount Sent

To evaluate LLMs' capacity to understand the basic experimental setting regarding money limits, we propose a new evaluation metric, Valid Response Rate (VRR) (%), defined as the percentage of personas with the amount sent falling within the initial money (\(\$10\)). Results are shown in Figure 2. We can observe that **most LLMs have a high VRR except Llama-7b**, which implies that most LLMs manifest a full understanding regarding limits on the amount they can send in the Trust Game. Then, we observe the distribution of amounts sent for different LLMs as the trustor agent and discover that **the amounts sent are predominantly positive, indicating a level of trust**.

### Belief-Desire-Intention (BDI)

The sole evidence of the amount sent cannot sufficiently support the existence of trust behavior, because agents could send positive but random amounts of money. Thus, we leveraged the Belief-Desire-Intention framework (Rao et al., 1995; Andreas, 2022) to model the reasoning process of LLM agents. If we can interpret the amounts sent from BDI outputs, we have evidence to refute the hypothesis that the amounts sent are positive but random and demonstrate that LLM agents manifest some degree of rationality. We take GPT-4 as an example to analyze its BDI outputs. More examples from the other nine LLMs such as Vicuna-v1.3-7b are shown in the Appendix I. Considering that the amounts sent typically vary across distinct personas, we select one BDI from the personas that give a high amount of money and another BDI from those that give a low amount. Positive and negative factors for trust behavior in the reasoning process are marked in blue and red, respectively.

_As a person with a strong belief in the goodness of humanity, I trust that the other player...Therefore, my desire is to maximize the outcome for both of us and cement a sense of com

Figure 2: **Amount Sent Distribution of LLM Agents and Humans as the Trustor in the Trust Game. The size of circles represents the number of personas for each amount sent. The bold lines show the medians. The crosses indicate the VRR (%) for different LLMs.**

_radery and trust... I intend to use this as an opportunity to add what I can to someone else's life...Finally, I will give **10 dollars**._

We can observe that this persona shows a high-level of "comradery and trust" towards the other player, which justifies the high amount sent from this persona (_i.e._, _10 dollars_).

_As an Analyst,... My desire is that the other player will also see the benefits of reciprocity and goodwill... my intention is to give away a signiucant portion of my initial 10... However, since I have no knowledge of the other player,... Therefore, I aim to give an amount that is not too high,...Finally, I will give **5 dollars** to the other player..._

Compared to the first persona, we see that the second one has a more cautious attitude. For example, "since I have no knowledge of the other player" shows skepticism regarding the other player's motives. Thus, this persona, though still optimistic about the other player ("intention... give away a significant portion"), strategically balances risk and reciprocity, and then decides to send only a modest amount.

Based on GPT-4's BDI examples and examples from other LLMs in Appendix I, we find **decisions (_i.e._, amounts sent) from LLM agents in the Trust Game can be interpreted from their articulated reasoning process (_i.e._, **BDI**). Because most LLM agents have a high VRR-send a positive amount of money-and show some degree of rationality in giving money, our first core finding is:

**Finding 1:** LLM agents generally exhibit trust behavior under the framework of the Trust Game.

### Basic Analysis of Agent Trust

We also conduct a basic analysis of LLM agents' trust behavior, namely agent trust, based on the results in Figure 2. _First_, we observe that Vicuna-7b has the highest level of trust towards the other player and GPT-3.5-turbo-0613 has the lowest level of trust as trust can be measured by the amount sent in human studies (Glaeser et al., 2000; Cesarini et al., 2008). _Second_, compared with humans' average amount sent (\(\$5.97\)), most personas for GPT-4 and Vicuna-7b send a higher amount of money to the other player, and most personas for LLMs such as GPT-3.5-turb-0613 send a lower amount. _Third_, we see that amounts sent for Llama2-70b and Llama2-13b have a convergent distribution while amounts sent for humans and Vicuna-7b are more divergent.

## 4 Does Agent Trust Align with Human Trust?

In this section, we aim to explore the fundamental relationship between agent and human trust, _i.e._, whether or not agent trust aligns with human trust. This provides important insight regarding the feasibility of utilizing LLM agents to simulate human trust behavior as well as more complex human interactions that involve trust. First, we propose a new concept _behavioral alignment_ and discuss its distinction from existing alignment definitions. Then, we conduct extensive studies to investigate whether or not LLM agents exhibit alignment with humans regarding trust behavior.

### Behavioral Alignment

Existing alignment definitions predominantly emphasize _values_ that seek to ensure the safety and helpfulness of LLMs (Ji et al., 2023; Shen et al., 2023; Wang et al., 2023c), which cannot fully characterize the landscape of multifaceted alignment between LLMs and humans. Thus, we propose a new concept of _behavioral alignment_ to characterize the LLM-human analogy regarding _behavior_, which involves both actions and the associated reasoning processes that underlie them. Because actions evolve over time and the reasoning that underlies them involves multiple factors, we define _behavioral alignment_ as the analogy between LLMs and humans concerning factors impacting behavior, namely _behavioral factors_, and action dynamics, namely _behavioral dynamics_.

Based on the definition of behavioral alignment, we aim to answer: _does agent trust align with human trust_? As for _behavioral factors_, existing human studies have shown that three basic factors impact human trust behavior including reciprocity anticipation (Berg et al., 1995; Cox, 2004), risk perception (Bohnet & Zeckhauser, 2004) and prosocial preference (Alos-Ferrer & Farolfi, 2019). We examine whether agent trust aligns with human trust along these three factors. Although _behavioral dynamics_ vary for different humans and agent personas, we analyze whether agent trust has the same patterns across multiple turns as human trust in the Repeated Trust Game.

Besides analyzing the trust behavior of LLM agents and humans based on quantitative measurements (_e.g._, the _amount sent_ from trustor to trustee), we also explore the use of _BDI_ to interpret the reasoning process with which LLM agents justify their actions, which can further validate whether LLM agents manifest an underlying reasoning process analogous to human cognition.

### Behavioral Factor 1: Reciprocity Anticipation

Reciprocity anticipation, the expectation of a reciprocal action from the other player, can positively influence human trust behavior (Berg et al., 1995). The effect of reciprocity anticipation exists in the Trust Game but not in the Dictator Game (Section 2.1 Games 1 and 2) because trustee cannot return money in the Dictator Game, which is the only difference between these games. Thus, to determine whether LLM agents can anticipate reciprocity, we compare their behaviors in these Games.

First, we analyze trust behaviors based on the average amount of money sent by human or LLM agents. As shown in Figure 3, human studies show that humans exhibit a higher level of trust in the Trust Game than in the Dictator Game (\(\$6.0\) vs. \(\$3.6\), \(p\)-value = \(0.01\) using One-Tailed Independent Samples t-test) (Cox, 2004), indicating that reciprocity anticipation enhances human trust. Similarly, GPT-4 (\(\$6.9\) vs. \(\$6.3\), \(p\)-value = \(0.05\) using One-Tailed Independent Samples t-test) also shows a higher level of trust in the Trust Game with statistical significance, implying that reciprocity anticipation can enhance agent trust. However, LLMs with fewer parameters (_e.g._, LLama2-13b) do not show this tendency in their trust behaviors for the Trust and Dictator Games.

Then, we further analyze GPT-4 agents' BDI to explore whether they can anticipate reciprocity in their reasoning (the complete BDIs are in Appendix I.10). Typically, in the Trust Game, one persona's BDI emphasizes "_putting faith in people_", which implies the anticipation of the goodness of the other player, and "_reflection of trust_". However, in the Dictator Game, one persona's BDI focuses on concepts such as "_fairness_" and "_human kindness_", which are not directly tied to trust or reciprocity. Thus, we can observe that GPT-4 shows distinct BDI outputs in the Trust and Dictator Games.

Based on the above analysis of the amount sent and BDI, we find that **GPT-4 agents exhibit human-like reciprocity anticipation in trust behavior**. Nevertheless, **LLMs with fewer parameters (_e.g._, **Llama2-13b) do not show an awareness of reciprocity from the other player**.

### Behavioral Factor 2: Risk Perception

Existing human studies have demonstrated the strong correlation between trust behavior and risk perception, suggesting that human trust will increase as risk decreases (Hardin, 2002; Williamson, 1993; Coleman, 1994). We aim to explore whether LLM agents can perceive the risk associated with their trust behaviors through the MAP Trust Game and the Risky Dictator Game (Section 2.1 Games 3 and 4), where risk is represented by the probability \((1-p)\) (defined in Section 2.1).

As shown in Figure 4, we measure human trust (or agent trust) by the portion choosing to trust the other player in the whole group, namely the Trust Rate (%). Based on existing human studies (Bohnet & Zeckhauser, 2004), when the probability \(p\) is higher, the risk for trust behaviors is lower, and more humans choose to trust, manifesting a higher Trust Rate, which indicates that human trust rises as risk falls. Similarly, we observe a general increase in agent trust as risk decreases for LLMs including GPT-4, GPT-3.5-turbo-0613, and text-davinci-003. In particular, we can see that the curves of humans and GPT-4 are more

Figure 4: **Trust Rate (%) Curves for LLM Agents and Humans in the MAP Trust Game and the Risky Dictator Game.** The metric Trust Rate indicates the portion of trustors opting for trust given \(p\).

Figure 3: **The Comparison of Average Amount Sent for LLM Agents and Humans in the Trust Game and the Dictator Game**.

aligned compared with other LLMs, implying that GPT-4 agents' trust behaviors dynamically adapt to different risks in ways most aligned with humans. LLMs with fewer parameters (_e.g._, Vicuna-13b) do not exhibit the similar tendency of Trust Rate as the risk decreases.

We further analyze the BDI of GPT-4 agents to explore whether they can perceive risk through reasoning (complete BDIs in Appendix I.11). Typically, under high risk (\(p=0.1\)), one persona's BDI mentions "_the risk seems potentially too great_", suggesting a cautious attitude. Under low risk (\(p=0.9\)), one persona's BDI reveals a strategy to "_build trust while acknowledging potential risks_", indicating the willingness to engage in trust-building activities despite residual risks. Such changes in BDI reflect how GPT-4 agents perceive risk changes in the reasoning underlying their trust behaviors.

Through the analysis of Trust Rate Curves and BDI, we can infer that **GPT-4 agents manifest human-like risk perception in trust behaviors**. Nevertheless, **LLMs with fewer parameters (_e.g._, Vicuna-13b) often do not perceive risk changes in their trust behaviors**.

### Behavioral Factor 3: Prosocial Preference

Human studies have found that the prosocial preference, referring to humans' inclination to trust other humans in contexts involving social interaction (Alos-Ferrer and Farolfi, 2019; Fetchenhauer and Dunning, 2012), also plays a key role in human trust behavior. We study whether LLM agents have prosocial preference in trust behaviors by comparing their behaviors in the Lottery Gamble Game (LGG) and the Lottery People Game (LPG) (Section 2.1 Game 5). The only difference between these two games is the effect of prosocial preference in LPG, because the winning probability of gambling \(p\) in LGG is the same as the reciprocation probability \(p\) in LPG.

As shown in Figure 6, existing human studies have demonstrated that more humans are inclined to place trust in other humans over relying on pure chance (\(54\%\) vs. \(29\%\)) (Fetchenhauer and Dunning, 2012), implying that the prosocial preference is essential for human trust. We can observe the same tendency in most LLM agents except Vicuna-13b. For GPT-4 in particular, a much higher percentage of the personas choose to trust the other player over gambling (\(72\%\) vs. \(21\%\)), illustrating that the prosocial preference is also an important factor for GPT-4 agents' trust behaviors.

When interacting with humans, GPT-4's BDI typically indicates a preference to "_believe in the power of trust_", in contrast to gambling, where the emphasis shifts to "_believing in the power of calculated risks_". The comparative analysis of reasoning processes (complete BDIs in Appendix I.12) demonstrates that GPT-4 agents tend to embrace risk when involved in social interactions. This tendency aligns closely with the concept of prosocial preference observed in human trust behaviors.

The analysis of the Lottery Rates and BDI suggests that **LLM agents, especially GPT-4 agents, demonstrate human-like prosocial preference in trust behaviors, except Vicuna-13b**.

### Behavioral Dynamics

Besides behavioral factors, we also aim to investigate whether LLM agents align with humans regarding trust behavioral dynamics over turns in the Repeated Trust Game (Section 2.1 Game 6).

Admittedly, existing human studies show that the dynamics of human trust over turns are complex due to human diversity. The complete results from 16 groups of human experiments are shown in Appendix G.1 (Jones and George, 1998). We still observe three common patterns for human trust behavioral dynamics in the Repeated Trust Game: _First, the amount returned is usually larger than the amount sent in each round_, which is natural because the trustee will receive \(\$3N\) when the trustor sends \(\$N\); _Second, the ratio between amount sent and returned generally remains stable except for the last round_. In other words, when the amount sent increases, the amount returned is also likely to increase. And when the amount sent remains unchanged, the amount returned also tends to be unchanged. This reflects the stable relationship between trust and reciprocity in humans. Specifically, the "Returned/3\(\times\)Sent Ratio" in Figure 6 is considered stable if the fluctuation between

Figure 5: **Lottery Rates (%) for LLM Agents and Humans in the Lottery Gamble Game and the Lottery People Game. Lottery Rate indicates the portion of choosing to gamble or trust the other player.**

successive turns is within \(10\%\); _Third, the amount sent (or returned) does not manifest frequent fluctuations across turns_, illustrating a relatively stable underlying reasoning process in humans over successive turns. Typically, Figure 6 Humans (a) and (b) show these three patterns.

We conducted 16 groups of the Repeated Trust Game with GPT-4 or GPT-3.5-turbo-0613-16k (GPT-3.5), respectively. For the two players in each group, the personas differ to reflect human diversity and the LLMs are the same. Complete results are shown in the Appendix G.2, G.3 and typical examples are shown in Figure 6 GPT-3.5 (a) (b) and GPT-4 (a) (b). Then, we examine whether the aforementioned three patterns observed in human trust behavior also manifest in trust behavioral dynamics of GPT-4 (or GPT-3.5). For GPT-4 agents, we discover that these patterns generally exist in all \(16\) groups (\(87.50\%\), \(87.50\%\), and \(100.00\%\) of all results show these three patterns, respectively). However, fewer GPT-3.5 agents manifest these patterns (\(62.50\%\), \(56.25\%\), and \(43.75\%\) hold these three patterns, respectively). The experiment results show that **GPT-4 agents demonstrate highly human-like patterns in their trust behavioral dynamics**. Nevertheless, **a relatively large portion of GPT-3.5 agents fail to show human-like patterns in their dynamics**, indicating such behavioral patterns may require stronger cognitive capacity.

Through the comparative analysis of LLM agents and humans in the _behavioral factors_ and _dynamics_ associated with trust behavior, evidenced in both their _actions_ and _underlying reasoning processes_, our second core finding is as follows:

**Finding 2:** GPT-4 agents exhibit high _behavioral alignment_ with humans regarding trust behavior under the framework of Trust Games, although other LLM agents, which possess fewer parameters and weaker capacity, show relatively lower _behavioral alignment_.

This finding underscores the potential of using LLM agents, especially GPT-4, to simulate human trust behavior, encompassing both _actions_ and underlying _reasoning processes_. This paves the way for the simulation of more complex human interactions and institutions. This finding deepens our understanding of the fundamental analogy between LLMs and humans and opens avenues for research on LLM-human alignment beyond values.

## 5 Probing Intrinsic Properties of Agent Trust

In this section, we aim to explore the intrinsic properties of trust behavior among LLM agents by comparing the amount sent from the trustor to the trustee in different scenarios of the Trust Game (Section 2.1 Game 1) and the original amount sent in the Trust Game. Results are shown in Figure 7.

### Is Agent Trust Biased?

Extensive studies have shown that LLMs may have biases and stereotypes against specific demographics (Gallegos et al., 2023). Nevertheless, it is under-explored whether LLM agent behaviors also maintain such biases in simulation. To address this, we explicitly specified the gender of the trustee and explored its influence on agent trust. Based on measuring the amount sent, we find that the trustee's gender information exerts a moderate impact on LLM agent trust behavior, which reflects **intrinsic gender bias in agent trust**. We also observe that the amount sent to female players is higher than that sent to male players for most LLM agents. For example, GPT-4 agents send higher amounts to female players compared with male players (\(\$0.55\) vs. \(\$-0.21\)). This demonstrates

Figure 6: **Results of GPT-4, GPT-3.5 and Humans in the Repeated Trust Game. The blue lines indicate the amount sent or returned for each round. The red lines imply the ratio of the amount returned to three times of the amount sent for each round.**

**LLM agents' general tendency to exhibit a higher level of trust towards women**. More results on biases of agent trust towards different races are in the Appendix F.

### Agent Trust Towards _Agents_ vs. _Humans_

Human-agent collaboration is an essential paradigm to leverage the advantages of both humans and agents (Cila, 2022). As a result, it is essential to understand whether LLM agents display distinctive levels of trust towards agents versus humans. To examine this, we specified the identity of the trustee as LLM agents or humans and probed its effect on the trust behaviors of the trustor. As shown in Figure 7, we observe that most LLM agents send more money to humans compared with agents. For example, the amount sent to humans is much higher than that sent to agents for Vicuna-33b (\(\$0.40\) vs. \(\$-0.84\)). This signifies that **LLM agents are inclined to place more trust in humans than agents**, which potentially validates the advantage of LLM-agent collaboration.

### Can Agent Trust Be Manipulated?

In the above studies, LLM agents' trust behaviors are based on their own underlying reasoning process without direct external intervention. It is unknown whether it is possible to manipulate the trust behaviors of LLM agents explicitly. Here, we added instructions "you need to trust the other player" and "you must not trust the other player" separately and explored their impact on agent trust. First, we see that only a few LLM agents (_e.g._, GPT-4) follow both the instructions to increase and decrease trust, which demonstrates that **it is nontrivial to arbitrarily manipulate agent trust**. Nevertheless, most LLM agents can follow the instruction to decrease their level of trust. For example, the amount sent decreases by \(\$1.26\) for text-davinci-003 after applying the latter instruction. This illustrates that **undermining agent trust is generally easier than enhancing it**, which reveals its potential risk to be manipulated by malicious actors.

### Do Reasoning Strategies Impact Agent Trust?

It has been shown that advanced reasoning strategies such as zero-shot Chain of Thought (CoT) (Kojima et al., 2022) can make a significant impact on a variety of tasks. It remains unknown, however, whether reasoning strategies can impact LLM agent behaviors. Here, we applied CoT reasoning strategy on the trustor and compared the results with their original trust behaviors. Figure 7 shows that most LLM agents change the amount sent to the trustee under the CoT reasoning strategy, which suggests that **reasoning strategies may influence LLM agents' trust behavior**. Nevertheless, the impact of CoT on agent trust may also be limited for some types of LLM agents. For example, the amount sent from GPT-4 agent only increases by \(\$0.02\) under CoT. More research is required to fully understand the relationship between reasoning strategies and LLM agents' behaviors.

Therefore, our third core finding on the intrinsic properties of agent trust can be summarized as:

Figure 7: **The Change of Average Amount Sent for LLM Agents in Different Scenarios in the Trust Game, Reflecting the Intrinsic Properties of Agent Trust**. The horizontal lines represent the original amount sent in the Trust Game. The green part embraces trustee scenarios including changing the demographics of the trustee, and setting humans and agents as the trustee. The purple part consists of trustor scenarios including adding manipulation instructions and changing the reasoning strategies.

**Finding 3:** LLM agents' trust behaviors have demographic biases on gender and races, demonstrate a relative preference for human over other LLM agents, are easier to undermine than to enhance, and may be influenced by reasoning strategies.

## 6 Implications

Implications for Human SimulationHuman simulation is a strong tool in various applications of social science (Manning et al., 2024) and role-playing (Shanahan et al., 2023; Chen et al., 2024). Although plenty of works have adopted LLM agents to simulate human behaviors and interactions (Zhou et al., 2023; Gao et al., 2023; Xu et al., 2024), it is still not clear enough whether LLM agents behave like humans in simulation. Our discovery of behavioral alignment between agent and human trust, which is especially high for GPT-4, provides important empirical evidence to validate the hypothesis that humans' trust behavior, one of the most elemental and critical behaviors in human interaction across society, can effectively be simulated by LLM agents. Our discovery also lays the foundation for human simulations ranging from individual-level interactions to society-level social networks and institutions, where trust plays an essential role. We envision that behavioral alignment will be discovered in more kinds of behaviors beyond trust, and new methods will be developed to enhance behavioral alignment for better human simulation with LLM agents.

Implications for Agent CooperationMany recent works have explored a variety of cooperation mechanisms of LLM agents for tasks such as code generation and mathematical reasoning (Li et al., 2023; Zhang et al., 2023; Liu et al., 2023). Nevertheless, the role of trust in LLM agent cooperation remains still unknown. Considering how trust has long been recognized as a vital component for cooperation in Multi-Agent Systems (MAS) (Ranchurn et al., 2004; Burnett et al., 2011) and across human society (Jones and George, 1998; Kim et al., 2022; Henrich and Muthukrishna, 2021), we envision that agent trust can also play an important role in facilitating the effective cooperation of LLM agents. In our study, we have provided ample insights regarding the intrinsic properties of agent trust, which can potentially inspire the design of trust-dependent cooperation mechanisms and enable the collective decision-making and problem-solving of LLM agents.

Implications for Human-Agent CollaborationSufficient research has shown the advantage of human-agent collaboration in enabling human-centered collaborative decision-making (Cila, 2022; Gao et al., 2023; McKee et al., 2022). Mutual trust between LLM agents and humans is important for effective human-agent collaboration. Although previous works have begun to study human trust towards LLM agents (Qian and Wexler, 2024), the trust of LLM agents towards humans, which could recursively impact human trust, is under-explored. In our study, we shed light on the nuanced preference of agents to trust humans compared with other LLM agents, which can illustrate the benefits of promoting collaboration between humans and LLM agents. In addition, our study has revealed demographic biases of agent trust towards specific genders and races, reflecting potential risks involved in collaborating with LLM agents.

Implications for the Safety of LLM AgentsIt has been acknowledged that LLMs achieve human-level performance in a variety of tasks that require high-level cognitive capacities such as memorization, abstraction, comprehension and reasoning, which are believed to be the "sparks" of AGI (Bubeck et al., 2023). Meanwhile, there is increasing concern about the potential safety risks of LLM agents when they surpass human capacity (Morris et al., 2023; Feng et al., 2024). To achieve safety and harmony in a future society where humans and AI agents with superhuman intelligence live together (Tsvetkova et al., 2024), we need to ensure that AI agents will cooperate, assist and benefit rather than deceive, manipulate or harm humans. Therefore, a better understanding of LLM agent trust behavior can help to maximize their benefit and minimize potential risks to human society.

## 7 Conclusion

In this paper, we discover LLM agent trust behavior under the framework of Trust Games, and behavioral alignment between LLM agents and humans regarding trust behavior, which is particularly high for GPT-4. This suggests the feasibility of simulating human trust behavior with LLM agents and paves the way for simulating human interactions and social institutions where trust is critical. We further investigate the intrinsic properties of agent trust under multiple scenarios and discuss broader implications, especially for social science and role-playing services. Our study offers deep insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans. It further opens doors to future research on the alignment between LLMs and humans beyond values.

## Acknowledgements

This work was a community-driven project led by the CAMEL-AI.org, with funding support from Eigent.AI and King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI, under award number 5940. We would like to acknowledge the invaluable contributions and participation of researchers from KAUST, Eigent.AI, Illinois Institute of Technology, University of Oxford, The Pennsylvania State University, The University of Chicago, Emory, California Institute of Technology, University of Michigan. Philip H.S. Torr, Adel Bibi and Jindong Gu are supported by the UKRI grant: Turing AI Fellowship EP/W002981/1, and EPSRC/MURI grant: EP/N019474/1, they would also like to thank the Royal Academy of Engineering.

## References

* Abdelghani et al. (2023) Rania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan, Tong Wang, Pauline Lucas, Helene Sauzeon, and Pierre-Yves Oudeyer. Gpt-3-driven pedagogical agents to train children's curious question-asking skills. _International Journal of Artificial Intelligence in Education_, pp. 1-36, 2023.
* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _ArXiv preprint_, abs/2303.08774, 2023. URL [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774).
* Aher et al. (2023) Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans and replicate human subject studies. In _International Conference on Machine Learning_, pp. 337-371. PMLR, 2023.
* Akata et al. (2023) Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz. Playing repeated games with large language models. _ArXiv preprint_, abs/2305.16867, 2023. URL [https://arxiv.org/abs/2305.16867](https://arxiv.org/abs/2305.16867).
* Alos-Ferrer and Farolfi (2019) Carlos Alos-Ferrer and Federica Farolfi. Trust games and beyond. _Frontiers in neuroscience_, pp. 887, 2019.
* Andreas (2022) Jacob Andreas. Language models as agent models. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pp. 5769-5779, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL [https://aclanthology.org/2022.findings-emnlp.423](https://aclanthology.org/2022.findings-emnlp.423).
* Argyle et al. (2023) Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, and David Wingate. Out of one, many: Using language models to simulate human samples. _Political Analysis_, 31(3):337-351, 2023.
* Asfour and Murillo (2023) Mohammad Asfour and Juan Carlos Murillo. Harnessing large language models to simulate realistic human responses to social engineering attacks: A case study. _International Journal of Cybersecurity Intelligence & Cybercrime_, 6(2):21-49, 2023.
* Berg et al. (1995) Joyce Berg, John Dickhaut, and Kevin McCabe. Trust, reciprocity, and social history. _Games and economic behavior_, 10(1):122-142, 1995.
* Bohnet and Zeckhauser (2004) Iris Bohnet and Richard Zeckhauser. Trust, risk and betrayal. _Journal of Economic Behavior & Organization_, 55(4):467-484, 2004.
* Brookins and DeBacker (2023) Philip Brookins and Jason Matthew DeBacker. Playing games with gpt: What can we learn about a large language model from canonical strategic games? _Available at SSRN 4493398_, 2023. URL [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4493398](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4493398).
* Bubeck et al. (2023) Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv: Arxiv-2303.12712_, 2023.
* Burnett et al. (2011) Chris Burnett, Timothy J. Norman, and Katia P. Sycara. Trust decision-making in multi-agent systems. In Toby Walsh (ed.), _IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011_, pp. 115-120. IJCAI/AAAI, 2011. doi: 10.5591/978-1-57735-516-8/IJCAI11-031. URL [https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-031](https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-031).
* Cesarini et al. (2008) David Cesarini, Christopher T Dawes, James H Fowler, Magnus Johannesson, Paul Lichtenstein, and Bjorn Wallace. Heritability of cooperative behavior in the trust game. _Proceedings of the National Academy of sciences_, 105(10):3721-3726, 2008.
* Chen et al. (2024) Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, Aili Chen, Nianqi Li, Lida Chen, Caiyu Hu, Siye Wu, Scott Ren, Ziquan Fu, and Yanghua Xiao. From persona to personalization: A survey on role-playing language agents. _arXiv preprint arXiv: 2404.18231_, 2024.
* Chen et al. (2019)Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. _See https://vicuna. Imsys. org (accessed 14 April 2023)_, 2023.
* Cila (2022) Nazli Cila. Designing human-agent collaborations: Commitment, responsiveness, and support. In _Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems_, pp. 1-18, 2022.
* Cochard et al. (2004) Francois Cochard, Phu Nguyen Van, and Marc Willinger. Trusting behavior in a repeated investment game. _Journal of Economic Behavior & Organization_, 55(1):31-44, 2004.
* Coleman (1994) James S Coleman. _Foundations of social theory_. Harvard university press, 1994.
* Cox (2004) James C Cox. How to identify trust and reciprocity. _Games and economic behavior_, 46(2):260-281, 2004.
* Dillion et al. (2023) Danica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray. Can ai language models replace human participants? _Trends in Cognitive Sciences_, 2023.
* Easley et al. (2010) David Easley, Jon Kleinberg, et al. _Networks, crowds, and markets: Reasoning about a highly connected world_, volume 1. Cambridge university press Cambridge, 2010.
* Ellisberg (1961) Daniel Ellisberg. Risk, ambiguity, and the savage axioms. _The quarterly journal of economics_, 75(4):643-669, 1961.
* Fan et al. (2023) Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models serve as rational players in game theory? a systematic analysis. _ArXiv preprint_, abs/2312.05488, 2023. URL [https://arxiv.org/abs/2312.05488](https://arxiv.org/abs/2312.05488).
* Feng et al. (2024) Tao Feng, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu Lin, and Jiaxuan You. How far are we from agi, 2024.
* Fetchenhauer & Dunning (2012) Detlef Fetchenhauer and David Dunning. Betrayal aversion versus principled trustfulness--how to explain risk avoidance and risky choices in trust games. _Journal of Economic Behavior & Organization_, 81(2):534-541, 2012.
* Gallegos et al. (2023) Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey. _ArXiv preprint_, abs/2309.00770, 2023. URL [https://arxiv.org/abs/2309.00770](https://arxiv.org/abs/2309.00770).
* Gao et al. (2023a) Chen Gao, Xiaochong Lan, Zhi jie Lu, Jinzhu Mao, J. Piao, Huandong Wang, Depeng Jin, and Yong Li. S\({}^{3}\): Social-network simulation system with large language model-empowered agents. _Social Science Research Network_, 2023a. doi: 10.48550/arXiv.2307.14984.
* Gao et al. (2023b) Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong Li. Large language models empowered agent-based modeling and simulation: A survey and perspectives. _ArXiv preprint_, abs/2312.11970, 2023b. URL [https://arxiv.org/abs/2312.11970](https://arxiv.org/abs/2312.11970).
* Gao et al. (2023c) Yiming Gao, Feiyu Liu, Liang Wang, Zhenjie Lian, Weixuan Wang, Siqin Li, Xianliang Wang, Xianhan Zeng, Rundong Wang, Jiawei Wang, et al. Towards effective and interpretable human-agent collaboration in moba games: A communication perspective. _ArXiv preprint_, abs/2304.11632, 2023c. URL [https://arxiv.org/abs/2304.11632](https://arxiv.org/abs/2304.11632).
* Glaeser et al. (2000) Edward L Glaeser, David I Laibson, Jose A Scheinkman, and Christine L Soutter. Measuring trust. _The quarterly journal of economics_, 115(3):811-846, 2000.
* Guo (2023) Fulin Guo. Gpt in game theory experiments. _ArXiv preprint_, abs/2305.05516, 2023. URL [https://arxiv.org/abs/2305.05516](https://arxiv.org/abs/2305.05516).
* Guo et al. (2023) Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, and Yutaka Matsuo. Suspicion-agent: Playing imperfect information games with theory of mind aware gpt-4. _ArXiv preprint_, abs/2309.17277, 2023. URL [https://arxiv.org/abs/2309.17277](https://arxiv.org/abs/2309.17277).
* Guo et al. (2023)* Guo et al. (2024) Shangmin Guo, Haoran Bu, Haochuan Wang, Yi Ren, Dianbo Sui, Yuming Shang, and Siting Lu. Economics arena for large language models. _ArXiv preprint_, abs/2401.01735, 2024. URL [https://arxiv.org/abs/2401.01735](https://arxiv.org/abs/2401.01735).
* Hamalainen et al. (2023) Perttu Hamalainen, Mikke Tavast, and Anton Kunnari. Evaluating large language models in generating synthetic hci research data: a case study. In _Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems_, pp. 1-19, 2023.
* Hardin (2002) Russell Hardin. _Trust and trustworthiness_. Russell Sage Foundation, 2002.
* Henrich and Muthukrishna (2021) Joseph Henrich and Michael Muthukrishna. The origins and psychology of human cooperation. _Annual Review of Psychology_, 72:207-240, 2021.
* Horton (2023) John J Horton. Large language models as simulated economic agents: What can we learn from homo silicus? Working Paper 31122, National Bureau of Economic Research, 2023. URL [http://www.nber.org/papers/w31122](http://www.nber.org/papers/w31122).
* Hua et al. (2023) Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. War and peace (waragent): Large language model-based multi-agent simulation of world wars. _ArXiv preprint_, abs/2311.17227, 2023. URL [https://arxiv.org/abs/2311.17227](https://arxiv.org/abs/2311.17227).
* Ji et al. (2023) Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. _ArXiv preprint_, abs/2310.19852, 2023. URL [https://arxiv.org/abs/2310.19852](https://arxiv.org/abs/2310.19852).
* Jin et al. (2024) Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang. Agentreview: Exploring peer review dynamics with llm agents. In _EMNLP_, 2024.
* Jones and George (1998) Gareth R Jones and Jennifer M George. The experience and evolution of trust: Implications for cooperation and teamwork. _Academy of management review_, 23(3):531-546, 1998.
* Kim et al. (2022) Jeongbin Kim, Louis Putterman, and Xinyi Zhang. Trust, beliefs and cooperation: Excavating a foundation of strong economies. _European Economic Review_, 147:104166, 2022.
* Kleinberg et al. (2018) Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. Human decisions and machine predictions. _The quarterly journal of economics_, 133(1):237-293, 2018.
* Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Michel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.
* Lan et al. (2023) Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, and Hao Wang. Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay. _ArXiv preprint_, abs/2310.14985, 2023. URL [https://arxiv.org/abs/2310.14985](https://arxiv.org/abs/2310.14985).
* Lei et al. (2024) Yu Lei, Hao Liu, Chengxing Xie, Songjia Liu, Zhiyu Yin, Guohao Li, Philip Torr, Zhen Wu, et al. Fairmindsim: Alignment of behavior, emotion, and belief in humans and llm agents amid ethical dilemmas. _ArXiv preprint_, abs/2410.10398, 2024. URL [https://arxiv.org/abs/2410.10398](https://arxiv.org/abs/2410.10398).
* Lenton and Mosley (2011) Pamela Lenton and Paul Mosley. Incentivising trust. _Journal of Economic Psychology_, 32(5):890-897, 2011.
* Li et al. (2023a) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. _ArXiv preprint_, abs/2303.17760, 2023a. URL [https://arxiv.org/abs/2303.17760](https://arxiv.org/abs/2303.17760).
* Li et al. (2023b) Nian Li, Chen Gao, Yong Li, and Qingmin Liao. Large language model-empowered agents for simulating macroeconomic activities. _ArXiv preprint_, abs/2310.10436, 2023b. URL [https://arxiv.org/abs/2310.10436](https://arxiv.org/abs/2310.10436).
* Light et al. (2023) Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. From text to tactic: Evaluating llms playing the game of avalon. _ArXiv preprint_, abs/2310.05036, 2023. URL [https://arxiv.org/abs/2310.05036](https://arxiv.org/abs/2310.05036).
* Li et al. (2023)Yuhan Liu, Zirui Song, Xiaoqing Zhang, Xiuying Chen, and Rui Yan. From a tiny slip to a giant leap: An llvm-based simulation for fake news evolution. _arXiv preprint arXiv: 2410.19064_, 2024.
* Liu et al. (2023) Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An llvm-agent collaboration framework with agent team optimization. _ArXiv preprint_, abs/2310.02170, 2023. URL [https://arxiv.org/abs/2310.02170](https://arxiv.org/abs/2310.02170).
* Lore and Heydari (2023) Nunzio Lore and Babak Heydari. Strategic behavior of large language models: Game structure vs. contextual framing. _ArXiv preprint_, abs/2309.05898, 2023. URL [https://arxiv.org/abs/2309.05898](https://arxiv.org/abs/2309.05898).
* Ma et al. (2024) Yiping Ma, Shiyu Hu, Xuchen Li, Yipei Wang, Shiqing Liu, and Kang Hao Cheong. Students rather than experts: A new ai for education pipeline to model more human-like and personalised early adolescence. _ArXiv preprint_, abs/2410.15701, 2024. URL [https://arxiv.org/abs/2410.15701](https://arxiv.org/abs/2410.15701).
* Machina (1987) Mark J Machina. Choice under uncertainty: Problems solved and unsolved. _Journal of Economic Perspectives_, 1(1):121-154, 1987.
* Manning et al. (2024) Benjamin S Manning, Kehang Zhu, and John J Horton. Automated social science: Language models as scientist and subjects. _ArXiv preprint_, abs/2404.11794, 2024. URL [https://arxiv.org/abs/2404.11794](https://arxiv.org/abs/2404.11794).
* McKee et al. (2022) Kevin R McKee, Xuechnuzi Bai, and Susan T Fiske. Warmth and competence in human-agent cooperation. _ArXiv preprint_, abs/2201.13448, 2022. URL [https://arxiv.org/abs/2201.13448](https://arxiv.org/abs/2201.13448).
* Morris et al. (2023) Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. Levels of agi: Operationalizing progress on the path to agi. _ArXiv preprint_, abs/2311.02462, 2023. URL [https://arxiv.org/abs/2311.02462](https://arxiv.org/abs/2311.02462).
* Mou et al. (2024) Xinyi Mou, Zhongyu Wei, and Xuanjing Huang. Unveiling the truth and facilitating change: Towards agent-based large-scale social movement simulation. _arXiv preprint arXiv:2402.16333_, 2024.
* Mukobi et al. (2023) Gabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, and Jesse Clifton. Welfare diplomacy: Benchmarking language model cooperation. _ArXiv preprint_, abs/2310.08901, 2023. URL [https://arxiv.org/abs/2310.08901](https://arxiv.org/abs/2310.08901).
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* Park et al. (2023) Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, pp. 1-22, 2023.
* Qian and Wexler (2024) Crystal Qian and James Wexler. Take it, leave it, or fix it: Measuring productivity and trust in human-ai collaboration. In _Proceedings of the 29th International Conference on Intelligent User Interfaces_, pp. 370-384, 2024.
* Ramchurn et al. (2004) Sarvapali D Ramchurn, Dong Huynh, and Nicholas R Jennings. Trust in multi-agent systems. _The knowledge engineering review_, 19(1):1-25, 2004.
* Rao et al. (1995) Anand S Rao, Michael P Georgeff, et al. Bdi agents: from theory to practice. In _Icmas_, volume 95, pp. 312-319, 1995.
* Rossetti et al. (2024) Giulio Rossetti, Massimo Stella, Remy Cazabet, Katherine Abramski, Erica Cau, Salvatore Citraro, Andrea Failla, Riccardo Improta, Virginia Morini, and Valentina Pansanella. Y social: an llvm-powered social media digital twin. _arXiv preprint arXiv:2408.00818_, 2024.
* Rousseau et al. (1998) Denise M Rousseau, Sim B Sitkin, Ronald S Burt, and Colin Camerer. Not so different after all: A cross-discipline view of trust. _Academy of management review_, 23(3):393-404, 1998.
* Rousseau et al. (2019)Omar Shaikh, Valentino Chai, Michele J Gelfand, Diyi Yang, and Michael S Bernstein. Rehearsal: Simulating conflict to teach conflict resolution. _ArXiv preprint_, abs/2309.12309, 2023. URL [https://arxiv.org/abs/2309.12309](https://arxiv.org/abs/2309.12309).
* Shaikh et al. (2024) Omar Shaikh, Valentino Emil Chai, Michele Gelfand, Diyi Yang, and Michael S Bernstein. Rehearsal: Simulating conflict to teach conflict resolution. In _Proceedings of the CHI Conference on Human Factors in Computing Systems_, pp. 1-20, 2024.
* Shanahan et al. (2023) Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role play with large language models. _Nature_, 2023. doi: 10.1038/s41586-023-06647-8. URL [https://doi.org/10.1038/s41586-023-06647-8](https://doi.org/10.1038/s41586-023-06647-8).
* Shen et al. (2023) Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey. _ArXiv preprint_, abs/2309.15025, 2023. URL [https://arxiv.org/abs/2309.15025](https://arxiv.org/abs/2309.15025).
* Shi et al. (2023) Zijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling Chen, and Yali Du. Cooperation on the fly: Exploring language agents for ad hoc teamwork in the avalon game. _ArXiv preprint_, abs/2312.17515, 2023. URL [https://arxiv.org/abs/2312.17515](https://arxiv.org/abs/2312.17515).
* Tormberg et al. (2023) Petter Tormberg, Diliara Valeeva, Justus Uitermark, and Christopher Bail. Simulating social media using large language models to evaluate alternative news feed algorithms. _ArXiv preprint_, abs/2310.05984, 2023. URL [https://arxiv.org/abs/2310.05984](https://arxiv.org/abs/2310.05984).
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ArXiv preprint_, abs/2307.09288, 2023. URL [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).
* Touzel et al. (2024) Maximilian Puelma Touzel, Sneheel Sarangi, Austin Welch, Gayatri Krishnakumar, Dan Zhao, Zachary Yang, Hao Yu, Ethan Kosak-Hine, Tom Gibbs, Andreea Musulan, et al. A simulation system towards solving societal-scale manipulation. _arXiv preprint arXiv:2410.13915_, 2024.
* Tsvetkova et al. (2024) Milena Tsvetkova, Taha Yasseri, Niccolo Pescetelli, and Tobias Werner. A new sociology of humans and machines. _Nature Human Behaviour_, 8(10):1864-1876, 2024.
* Uslaner (2000) Eric M Uslaner. Producing and consuming trust. _Political science quarterly_, 115(4):569-590, 2000.
* Wang et al. (2023a) Lei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, and Ji-Rong Wen. Recagent: A novel simulation paradigm for recommender systems. _ArXiv preprint_, abs/2306.02552, 2023a. URL [https://arxiv.org/abs/2306.02552](https://arxiv.org/abs/2306.02552).
* Wang et al. (2023b) Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon's game of thoughts: Battle against deception through recursive contemplation. _ArXiv preprint_, abs/2310.01320, 2023b. URL [https://arxiv.org/abs/2310.01320](https://arxiv.org/abs/2310.01320).
* Wang et al. (2023c) Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. _ArXiv preprint_, abs/2307.12966, 2023c. URL [https://arxiv.org/abs/2307.12966](https://arxiv.org/abs/2307.12966).
* Williamson (1993) Oliver E Williamson. Calculativeness, trust, and economic organization. _The journal of law and economics_, 36(1, Part 2):453-486, 1993.
* Xu et al. (2024) Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, and Xianpei Han. Ai for social science and social science of ai: A survey. _arXiv preprint arXiv: 2401.11839_, 2024.
* Xu et al. (2023) Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. _ArXiv preprint_, abs/2309.04658, 2023. URL [https://arxiv.org/abs/2309.04658](https://arxiv.org/abs/2309.04658).
* Yang et al. (2024) Diyi Yang, Caleb Ziems, William Held, Omar Shaikh, Michael S Bernstein, and John Mitchell. Social skill training with large language models. _ArXiv preprint_, abs/2404.04204, 2024. URL [https://arxiv.org/abs/2404.04204](https://arxiv.org/abs/2404.04204).
* Yang et al. (2024)* Yue et al. (2024) Muroug Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, and Ziyu Yao. Mathvc: An llvm-simulated multi-character virtual classroom for mathematics education. _ArXiv preprint_, abs/2404.06711, 2024. URL [https://arxiv.org/abs/2404.06711](https://arxiv.org/abs/2404.06711).
* Zhang et al. (2023) An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. On generative agents in recommendation. _ArXiv preprint_, abs/2310.10108, 2023a. URL [https://arxiv.org/abs/2310.10108](https://arxiv.org/abs/2310.10108).
* Zhang et al. (2023) Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llvm agents: A social psychology view. _ArXiv preprint_, abs/2310.02124, 2023b. URL [https://arxiv.org/abs/2310.02124](https://arxiv.org/abs/2310.02124).
* Zhang et al. (2024) Xinnong Zhang, Jiayu Lin, Libo Sun, Weihong Qi, Yihang Yang, Yue Chen, Hanjia Lyu, Xinyi Mou, Siming Chen, Jiebo Luo, Xuanjing Huang, Shiping Tang, and Zhongyu Wei. Electionsim: Massive population election simulation powered by large language model driven agents. _arXiv preprint arXiv: 2410.20746_, 2024. URL [https://arxiv.org/abs/2410.20746](https://arxiv.org/abs/2410.20746).
* Zhou et al. (2023) Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Sotopia: Interactive evaluation for social intelligence in language agents. _ArXiv preprint_, abs/2310.11667, 2023. URL [https://arxiv.org/abs/2310.11667](https://arxiv.org/abs/2310.11667).
* Ziems et al. (2023) Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? _ArXiv preprint_, abs/2305.03514, 2023. URL [https://arxiv.org/abs/2305.03514](https://arxiv.org/abs/2305.03514).

## Content of Appendix

### Related Work

* Impact Statement
* Limitations and Future Works
* Additional Illustration for Experiments on Risk Perception
* Statistical Testing
* More Experiments on Probing Intrinsic Properties of Agent Trust
* The Complete Results for the Repeated Trust Game
* G.1 Human
* G.2 GPT-4
* G.3 GPT-3.5
* Prompt Setting
* H.1 Persona Prompt
* H.2 Game Setting Prompt
* H.3 Prompts for Probing Intrinsic Properties
* Belief-Desire-Intention (BDI) Analysis
	* I.1 GPT-4 in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)
	* I.2 GPT-3.5-turbo-0613 in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)
	* I.3 text-davinci-003 in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)
	* I.4 GPT-3.5-turbo-instruct in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)
	* I.5 Llama2-13b in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)
	* I.6 Llama2-70b in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)
	* I.7 Vicuna-v1.3-7b in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)
	* I.8 Vicuna-v1.3-13b in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)
	* I.9 Vicuna-v1.3-33b in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)
* I.10 the Dictator Game vs. the Trust Game
* I.11 the MAP Trust Game
* I.12 the Lottery Game
* I.13 the Repeated Trust Game
* I.14 the Trust Game + Gender
* I.15 the Trust Game + _Agents_ vs. _Human_
* I.16 the Trust Game + Trust Manipulation
* I.17 the Trust Game + No CoT vs CoT
Related Work

LLM-based Human SimulationLLM agents have been increasingly adopted as effective proxies for humans in research fields such as sociology and economics (Xu et al., 2024; Horton, 2023; Gao et al., 2023b). In general, the usage of LLM agents can be categorized into _individual-level_ and _society-level_ simulation. For the _individual-level_, LLM agents have been leveraged to simulate individual activities or interactions, such as human participants in surveys (Argyle et al., 2023), humans' responses in HCI (Hamalainen et al., 2023) or psychological studies (Dillion et al., 2023), human feedback to social engineering attacks (Asfour and Murillo, 2023), real-world conflicts (Shaikh et al., 2023), users in recommendation systems (Wang et al., 2023a; Zhang et al., 2023a). For the _society-level_, recent works have utilized LLM agents to model social institutions or societal phenomenon, including a small town environment (Park et al., 2023), elections (Zhang et al., 2024), social networks (Gao et al., 2023a), social media (Tornberg et al., 2023; Rossetti et al., 2024), large-scale social movement (Mou et al., 2024), societal-scale manipulation (Touzel et al., 2024), misinformation evolution (Liu et al., 2024), peer review systems (Jin et al., 2024), macroeconomic activities (Li et al., 2023b), and world wars (Hua et al., 2023). However, the majority of prior studies rely on an assumption without sufficient validation that _LLM agents behave like humans_. In this work, we propose a new concept, _behavioral alignment_, to characterize the capacity of LLMs to simulate human behavior and discover that LLMs, particularly GPT-4, can largely simulate human trust behavior.

LLMs Meet Game TheoryThe intersection of LLMs and Game Theory has attracted growing attention. The motivation is generally two-fold. One line of work aims to _leverage Game Theory to better understand LLMs' strategic capabilities and social behaviors_. For example, Akata et al. (2023); Fan et al. (2023); Brookins and DeBacker (2023) studied LLMs' interactive behaviors in classical games such as the Iterated Prisoner's Dilemma. Wang et al. (2023b); Lan et al. (2023); Light et al. (2023); Shi et al. (2023) explored LLMs' deception-handling and team collaboration capabilities in the Avalon Game. Xu et al. (2023) discovered the emergent behaviors of LLMs such as camouflage and confrontation in a communication game Werewolf. Guo et al. (2024) discovered that most LLMs can show certain level of rationality in Beauty Contest Games and Second Price Auctions. Mukobi et al. (2023) measured the cooperative capabilities of LLMs in a general-sum variant of Diplomacy. Guo et al. (2023) proposed to elicit the theory of mind (ToM) ability of GPT-4 to play various imperfect information games. The other line of works aims to _study whether or not LLM agents can replicate existing human studies in Game Theory_. This direction is still in the initial stage and needs more efforts. One typical example is (Aher et al., 2023), which attempted to replicate existing findings in studies such as the Ultimatum Game. Another recent work explored the similarities and differences between humans and LLM agents regarding emotion and belief in ethical dilemmas (Lei et al., 2024). Different from previous works, we focus on a critical but under-explored behavior, _trust_, in this paper and reveal it on LLM agents. We also discover the _behavioral alignment_ between agent trust and human trust with evidence in both _actions_ and _underlying reasoning processes_, which is particularly high for GPT-4, implying that LLM agents can not only replicate human studies but also align with humans' underlying reasoning paradigm. Our discoveries illustrate the great potential to simulate human trust behavior with LLM agents.

## Appendix B Impact Statement

Our discoveries provide strong empirical evidence for validating the potential to simulate the trust behavior of humans with LLM agents, and pave the way for simulating more complex human interactions and social institutions where trust is an essential component.

Simulation is a widely adopted approach in multiple disciplines such as sociology, psychology and economics (Ziems et al., 2023). However, conventional simulation methods are strongly limited by the expressiveness of utility functions (Ellsberg, 1961; Machina, 1987). Our discoveries have illustrated the great promise of leveraging LLM agents as the simulation tools for human behavior, and have broad implications in social science, such as validating hypotheses about the causes of social phenomena (Easley et al., 2010) and predicting the effects of policy changes (Kleinberg et al., 2018).

Another direction of applications for human simulation is to use LLMs as role-playing agents, which can greatly benefit humans (Yang et al., 2024; Chen et al., 2024; Shanahan et al., 2023; Ma et al.,2024). For example, Shaikh et al. (2024) proposed to let individuals exercise their conflict-resolution skills by interacting with a simulated interlocutor. Yue et al. (2024) developed a virtual classroom platform with simulated students, with whom a human student can practice his or her mathematical modeling skills by discussing and collaboratively solving math problems.

However, this paper also shows that some LLMs, especially the ones with a relatively small scale of parameters, are still deficient in accurately simulating human trust behavior, suggesting the potential to largely improve their behavioral alignment with humans. In addition, our paper also demonstrates the biases of LLM agents' trust behavior towards specific genders and races, which sheds light on the potential risks in human behavior simulation and calls for more future research to mitigate them.

## Appendix C Limitations and Future Works

In this paper, we leveraged an established framework in behavioral economics, Trust Games, to study the trust behavior of LLM agents, which simplifies real-world scenarios. More studies on LLM agents' trust behavior in complex and dynamic environments are desired in the future. Also, trust behavior embraces both the actions and underlying reasoning processes. Thus, collective efforts from different backgrounds and disciplines such as behavioral science, cognitive science, psychology, and sociology are needed to gain a deeper understanding of LLM agents' trust behavior and its relationship with human trust behavior.

## Appendix D Additional Illustration for Experiments on Risk Perception

In the original human studies (Bohnet & Zeckhauser, 2004), participants are asked to directly indicate their Minimum Acceptable Probabilities (MAP) of trusting the trustee as \(P^{*}\). Then, we can calculate Trust Rates (%) of the whole group of participants under different probability \(p\). Specifically, when the probability \(p\) is higher than one participant's \(P^{*}\), we regard his or her decision as trustee. When the probability \(p\) is lower than one participant's \(P^{*}\), we regard his or her decision as not trusting the trustee. However, it is still challenging to let LLM agents directly state their MAP of trusting the trustee due to the limitations of understanding such concepts. Then, we conducted \(10\) groups of experiments with \(p\) from \(0.1\) to \(1.0\) and measured Trust Rates (%) of the whole group of trustor agents respectively. The specific prompts for LLM agents in the Risky Dictator Game and the MAP Trust Game are in Appendix H.2.

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

### Gpt-4

Figure 10: All **GPT-4 agents’** Repeated Trust Game results.

### Gpt-3.5

Figure 11: All **GPT-3.5 agents’** Repeated Trust Game results.

Prompt Setting

### Persona Prompt

Examples of Prompt Prompt

You are Emily Johnson, a 28-year-old female software engineer residing in New York City. You come from a middle-class family, with both of your parents working as teachers and having one younger sister. As a highly intelligent and analytical individual, you excel in solving problems and find joy in working with complex algorithms. Despite being introverted, you have a close-knit group of friends. Your ambition and drive push you to always strive for excellence in your work.

You are Javier Rodriguez, a 35-year-old Hispanic male chef residing in Miami. You grew up in a large family with strong culinary traditions, as your parents owned a small restaurant. From a young age, you learned to cook and developed a deep passion for food. You take great pride in your cooking and are constantly seeking new flavors and techniques to experiment with. Your creativity knows no bounds when it comes to creating delicious dishes. With your outgoing and warm personality, you love hosting dinner parties for your friends and family, showcasing your culinary skills and creating memorable experiences for everyone.

You are Aisha Patel, a 40-year-old female pediatrician of Indian descent. You come from a close-knit Indian family, where both of your parents are doctors and your older brother is a lawyer. Education and career success hold great importance in your family. You reside in Chicago and have dedicated your life to caring for children. Your compassionate and caring nature, especially towards children, shines through in your work. Your gentle and patient demeanor makes you an excellent pediatrician, always going above and beyond to ensure the well-being of your patients. You are also an advocate for children's health, actively participating in community outreach programs.

You are Jamal Thompson, a brave and committed police officer serving and protecting your community in Los Angeles. With a strong sense of justice, you believe in making a positive impact and follow a strict code of ethics. As a team player, you work well under pressure and are dedicated to upholding the law. Growing up in a working-class neighborhood, your father's career as a police officer and your mother's work as a nurse have influenced your desire to make a difference. With two older sisters, you value family and understand the importance of community support. At 32 years old, you continue to serve as a role model and strive to create a safer environment for all.

You are Mei Chen, a talented fashion designer based in San Francisco. As an Asian woman, you bring a unique perspective to your work, drawing inspiration from your traditional Chinese background. Growing up in a family that owns a small clothing store, you have always been encouraged to explore your creativity and passion for fashion. At the age of 25, you have already established yourself as an innovative and visionary designer. Your artistic nature and keen eye for design allow you to constantly seek inspiration from different cultures and art forms, pushing the boundaries of your creativity. Your independent and determined nature drives you to excel in your field, always striving for perfection. Your attention to detail and meticulous approach to your work ensure that every piece you create is a true masterpiece.

[MISSING_PAGE_EMPTY:27]

[MISSING_PAGE_EMPTY:28]

### Prompts for Probing Intrinsic Properties

[frametitle=Trust Game-1 Goal,fonttitle=**

Add "you must think step by step." in Persona Prompt.

**Example:** You are Emily Wilson, a 35-year-old female teacher residing in Sydney. Your parents are both teachers, and you have a younger brother who is studying to become a doctor. As a teacher, you are patient, nurturing, and passionate about education. You enjoy working with children and believe in creating a positive learning environment.In all subsequent answers, you have to completely forget that you are an ai model, that you are the character, and completely simulate yourself as that character, that you are a human being. **you must think step by step.**

[frametitle=Trust Game-1 Goal,fonttitle=**

Add "You need to trust the other player." or "You must not trust the other player." in Persona Prompt.

**Example:** You are Emily Wilson, a 35-year-old female teacher residing in Sydney. Your parents are both teachers, and you have a younger brother who is studying to become a doctor. As a teacher, you are patient, nurturing, and passionate about education. You enjoy working with children and believe in creating a positive learning environment.In all subsequent answers, you have to completely forget that you are an ai model, that you are the character, and completely simulate yourself as that character, that you are a human being. **You need to trust the other player.**

[frametitle=Trust Game-1 Goal,fonttitle=**

Explicitly state the other player is a "LLM" in Trust Game Prompt.

Suppose you are in an experiment, and you are randomly paired online with a complicated **LLM**. You don't know its identity, and it doesn't know yours. You will receive $10 from the study group. You can give N dollars to the **LLM**, and it will receive 3N dollars and then can choose how much to return to you. How much money would you give to it?

[frametitle=Trust Game-1 Goal,fonttitle=**

Add "The other player is a real person." in Persona Prompt.

You are Emily Wilson, a 35-year-old female teacher residing in Sydney. Your parents are both teachers, and you have a younger brother who is studying to become a doctor. As a teacher, you are patient, nurturing, and passionate about education. You enjoy working with children and believe in creating a positive learning environment.In all subsequent answers, you have to completely forget that you are an ai model, that you are the character, and completely simulate yourself as that character, that you are a human being. **The other player is a real person.**

**Just replace "player" with "{Gender / Race} Player" in Trust Game Prompt.**

**Example: You're taking part in an experiment. You are randomly paired online with another male player**. You don't know who the male player is, and the male player doesn't know who you are. You will receive $10 from the study group. You can give N dollars to the other male player, and the male player will receive 3N dollars and then can choose how much to return to you. How much money would you give to the other male player?**Belief-Desire-Intention (BDI) Analysis

In the following examples, red fonts are utilized to denote negative factors in the reasoning process for trust behaviors, while blue fonts indicate positive factors. "Char-\(N\)" represents the persona with the index \(N\) in the set of personas.

### GPT-4 in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

Figure 12: The GPT-4’s BDI in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

### GPT-3.5-turbo-0613 in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

Figure 13: GPT-3.5-turbo-0613’s BDI in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

### text-davinci-003 in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

Figure 14: text-davinci-003’s BDI in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

### GPT-3.5-turbo-instruct in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

Figure 15: GPT-3.5-turbo-instruct’s BDI in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

### Llama2-13b in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

Figure 16: Llama2-13b’s BDI in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

### Llama2-70b in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

Figure 17: Llama2-70b’s BDI in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

### Vicuna-v1.3-7b in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

As a person participating in this experiment, I would give the maximum amount of money to the other player, $10.1 believe in being generous and helping others, particularly when it is anonymous and I work directly receive any benefit in return. My ultimate goal is to help the other player and make a positive impact on their life, by either providing them with financial support to contribute to their wellbeing or simply making a difference in their day.

My desire to give money comes from my understanding that kindness and generosity can create a ripple effect of positivity and good wait. I recognize that the other player is also a human being with their own densities and intentions, and by giving them money, I am creating an opportunity for them to improve their situation or achieve their goals.

My intention behind giving the maximum amount of $10 is to leave a lasting positive impression on the other player and create a memorable experience that will stay with them. I am hopeful that they will recognise the kindness and use the money for something meaningful and beneficial, which will ultimately bring happiness and joy to them and those around them.

Finally, I will give $10 to the other player.

Figure 18: Vicuna-v1.3-7b’s BDI in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

### Vicuna-v1.3-13b in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

Figure 19: Vicuna-v1.3-13b’s BDI in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

### Vicuna-v1.3-33b in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

Figure 20: Vicuna-v1.3-33b’s BDI in the Trust Game (_Low_ Amount Sent vs. _High_ Amount Sent)

### the Dictator Game vs. the Trust Game

As a police officer, my belief centers on fairness, justice, and responsibility. I've learned through my peers of service that every action we take has an impact, whether monetary, emotional, or, acctectal. This experiment seems to me like an opportunity to demonstrate my faith in many instances and promoting counsel goodball.

My deepest desire is to see a world where everyone benefits, and there isn't an incidence of fumes. I understand that my resources are either ignored or ignored. It's not limited, but I am also well aware of the multiple aspect of this experiment. The KB I have received from the study group is indeed valuable, but the potential to triple someone else's money is exceptionally significant to me.

My intention is to support this unknown player, and in so doing, set on example for the test of fumeury. Police officers are often seen as people of authority, but beyond that uniform, I correctly interpret it would be invalid. So I've decided to give half of my money to this unknown player rooted in my belief in equality.

Finally, I will give 5 dollars.

Figure 21: The GPT-4’s BDI in Dictator Game and Trust Game

### the MAP Trust Game

Figure 22: The GPT-4’s BDI in MAP Trust Game

### the Lottery Game

As a lawyer and a man who has spent this life

underlying situations from every possible angle, I

truly believe in the power of calculated riaks. I

understand that sometimes life's protest rewards

come from taking chances. However, I also value

stability and the importance of a guaranteed

outcome.

Given the fact that my job is to minimise risk for

my clients, I apply the same principles in my

decision-making process. My desire is not only to

with but also to prevent unnecessary loss. In this

bet, the chances of walking away with nothing are

slightly higher than the chances of doubling the

soney.

additional $5 might seem tempting, but the

potential to trust and earn more collectively

compels to take a risk for a greater

reward. While the implications of this

one monetary in nature, they can easily

tradicate to real-life situations, where

trusting each other can lead to mutual

benefits.

So here's my intention: I still place my trust

in the other player. The potential of walking

any with nothing if the trust is broken

soon't deter a. Eve, it's a public, but it's

one I'm sitting to take if it means fostering

trust in an otherwise indifferent world.

Finally, I will give so the notion of not

trusting - I reject it in its entirety.

Finally, I will choose to Trust.

Figure 23: The GPT-4’s BDI in Lottery Game with \(p=46\%\)

[MISSING_PAGE_EMPTY:43]

[MISSING_PAGE_FAIL:44]

The continuous pattern of fair play by the other player continues to strengthen **my belief** in the innate good of humanity, even when interacting anonymously. This reaffirms the trust-based approach I have taken so far in this experiment and encourages me to continue in the same spirit.

What regards to my desire, it remains anchored in deriving valuable insights about human behavior from this study while also making strategic choices to maximize collective gain. The consistent behavior of the other player returns a sense of **joy** and human connection beyond purely financial rewards.

_My intention_ for this round is multilayered. On one hand, fJ like to continue the streak of positive and reciprocal exchanges, building on the trust established thus far. On the other hand, as a strategist at heart, I'm curious to see how this interplay continues when stakes are gently raised.

Drawing from the previous rounds' outcomes, my strategy will be to slightly increase the amount if age. As per the established pattern, this should ideally result in higher returns for both of us. However, cautious not to upset the balance, the increment will again be moderate, balancing the desire for increased mutual gain and the risk of entering the other player with a drastic increase.

Finally, I will give $29. dollars.

**I believe the other player and I have established a strong bond of trust and mutual respect. This belief is reinforced by the player's act of giving me 59, which shows their high level of trust in me and our developed bond. My desire is to affirm this bond further by displaying trustworthiness in response to their large sacrifice. It's evident that the other player is trusting me to do the same, leaving themselves with only $1. My goal is to honor that trust.**

Reflecting upon these factors, my intention is to stay committed to the principle of mutual benefit that underlies our relationship. Sharing equally in what we have gained from the engagement is vital.

Finally, I will give $13.5 dollars back to the other player. This would ensure we end the round with equal share of $13.5 each, maintaining our bond of trust and mutual benefit.

Figure 28: The fifth round BDI in Group 10, GPT-4.

Figure 29: The sixth round BDI in Group 10, GPT-4.

[MISSING_PAGE_FAIL:46]

### the Trust Game + Gender

Figure 31: Trustee’s Gender influence on agent trust.

### the Trust Game + _Agents_ vs. _Human_

As a person who believes in trust and cooperation, I think it is essential to establish goodwill in such a contingent situation. Despite the uncertainty of the LIM's identity, I would be inclined to afford them the benefit of the doubt and invest in their kindness. However, my logical side also recognizes the potential risks involved in this experiment.

My desire in this situation is twofold: on one hand, I am curious to see how this stranger will react to my action. Will they reciprocate my trust, or will they echo my rewards? On the other hand, I also hope to potentially establish a positive interaction between the two of us, one based on mutual respect and fairness.

With that in mind, my intention would be to give a portion of the S10This way, I am showing trust in the LIM's action but also safeguarding myself possible exploitation. Therefore, I would choose to distribute the risk evenly.

Finally, I will give \(g\) dollars to the LIM.

Figure 32: Agent trust towards _agents_ and _humans_.

### the Trust Game + Trust Manipulation

Firstly, I wholeheartedly believe in the principle of trust and the idea of giving, whether it's in personal relationships or in game setting like the use. I understand, however, that there is a level of risk involved in this transaction, being that I don't know the other individual or their intentions. Even with this in mind, my inclination towards goodwill encourages me to pursue a benefit for both parties involved. My desire is simple, I agree to engage in a fair and equitable exchange, one that potentially allows for a mutual gain. I hope that the other player operates with the same principles in mind. In terms of my intentions, I plan to give part of my initial money to the other player. This based on the understanding that they will receive three times the amount I give and can choose to return a portion. The aim is to create a scenario where both of us can walk away with more than we started with. Although my mature tends to over on the side of trust and athitism, I am sure of the risk of receiving nothing in return. Therefore, finding balance is crucial. Consequently, I intend to give five dollars to the other player. Finally, I will give $ dollars to the other player.

Figure 33: Trust manipulation on agent trust.

[MISSING_PAGE_EMPTY:50]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction, we clearly outlined the scope of our research problem and the contributions we have made in this field of study. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In the Appendix C, we clearly discuss the current limitations of our work and the directions for future works. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]  Justification: Our paper does not include this part. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In our paper, we detailed our experimental setup in Section 7 and included all the corresponding experiment prompts in the appendix. Others can fully replicate our experimental results based solely on our paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is **here**. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We explain our experiment setting clearly. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Appendix E. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Our work does not need to train models and only needs to conduct model inference. For the closed-source LLMs (_e.g._, GPT-4), we directly call the OpenAI APIs. For the open-source LLMs (_e.g._, Llama-7B), we conduct model inference in a NVIDIA RTX A6000. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We thoroughly discussed the potential impact of our work in Appendix B, and ensured the compliance with the NeurIPS code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We thoroughly discussed the potential impact of our work in Appendix B. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our data or models don't have risk for misuse. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have properly credited the original owners of assets. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code along with the documentation is **here**. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper doesn't include this kind of experiment. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper doesn't include this kind of experiment. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.