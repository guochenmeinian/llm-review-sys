# A Framework for Bilevel Optimization

on Riemannian Manifolds

 Andi Han\({}^{1}\)  Bamdev Mishra\({}^{2}\)  Pratik Jawanpuria\({}^{2}\)  Akiko Takeda\({}^{1}\)

\({}^{1}\)RIKEN AIP \({}^{2}\)Microsoft, India \({}^{3}\)University of Tokyo

andi.han@riken.jp

{bamdevm, pratik.jawanpuria}@microsoft.com.

takeda@mist.i.u-tokyo.ac.jp

###### Abstract

Bilevel optimization has gained prominence in various applications. In this study, we introduce a framework for solving bilevel optimization problems, where the variables in both the lower and upper levels are constrained on Riemannian manifolds. We present several hypergradient estimation strategies on manifolds and analyze their estimation errors. Furthermore, we provide comprehensive convergence and complexity analyses for the proposed hypergradient descent algorithm on manifolds. We also extend our framework to encompass stochastic bilevel optimization and incorporate the use of general retraction. The efficacy of the proposed framework is demonstrated through several applications.

## 1 Introduction

Bilevel optimization is a hierarchical optimization problem where the upper-level problem depends on the solution of the lower-level, i.e.,

\[\min_{x\in\mathbb{R}^{d_{x}}}F(x)=f(x,y^{*}(x)),\qquad\text{ s.t. }y^{*}(x)=\operatorname*{arg \,min}_{y\in\mathbb{R}^{d_{y}}}g(x,y).\]

Applications involving bilevel optimization include meta learning [16], hyperparameter optimization [18], and neural architecture search (NAS) [53], to name a few. The lower-level problem is usually assumed to be strongly convex.

Common strategies for solving such problem can be classified into two categories: single-level reformulation [29, 60] and approximate hypergradient descent [19, 40]. The former aims to reformulate the bilevel optimization problem into a single-level one using the optimality conditions of the lower-level problem as constraints. However, this may impose a large number of constraints for machine learning applications. The latter scheme directly solves the bilevel problem through iteratively updating the lower and upper-level parameters and, hence, is usually more efficient. Nevertheless, existing works have mostly focused on unconstrained bilevel optimization [19, 32, 40, 11, 52, 45, 14].

In this work, we study bilevel optimization problems where \(x\) and \(y\) are on Riemannian manifolds \(\mathcal{M}_{x}\) and \(\mathcal{M}_{y}\), respectively. We focus on the setup where the lower-level function \(g(x,y)\) is geodesic strongly convex (a generalized notion of convexity on manifolds, defined in Section 2) in \(y\). This ensures the lower-level problem has a unique solution \(y^{*}(x)\) given \(x\). The upper-level function \(f\) can be nonconvex on \(\mathcal{M}_{x}\times\mathcal{M}_{y}\). Because the unconstrained bilevel optimization is a special case of our formulation on manifolds, such a formulation includes a wider class of applications. Examples of Riemannian bilevel optimization include Riemannian meta learning [64] and NAS over SPD networks [62]. Moreover, there has been a surge of interest of min-max optimization over Riemannian manifolds [37, 41, 73, 27, 25, 67, 35], which also gets subsumed in the framework of bilevel optimization with \(g=-f\).

**Contributions.****(i)** We derive intrinsic Riemannian hypergradient via the implicit function theorem and propose four strategies for estimating the hypergradient, i.e., through Hessian inverse, conjugate gradient, truncated Neumann series, and automatic differentiation. We then provide hypergradient estimation error bounds for all the proposed strategies. **(ii)** We introduce the Riemannian hypergradient descent algorithm to solve bilevel optimization problems on manifolds and provide convergence guarantees. We also generalize the framework to the stochastic setting and to allow the use of retraction. **(iii)** The efficacy of the proposed modeling is shown on several problem instances including hyper-representation over SPD matrices, Riemannian meta learning, and unsupervised domain adaptation. The proofs, extensions, and experimental details are deferred to the appendix sections.

**Related works in unconstrained setting.** Unconstrained bilevel optimization where the lower-level problem is strongly convex has been widely studied [19; 32; 40; 11; 52; 45; 14]. A crucial ingredient is the notion of hypergradient in bilevel optimization problems and its computation. There exist strategies for approximating the hypergradient, e.g., using conjugate gradient [40], Neumann series [19], iterative differentiation [21], and Nystrom method [31]. While bilevel optimization with constraints is relatively unexplored, a few works exists that impose constraints only for the upper level problem [32; 10]. Recently, linearly lower-level constrained bilevel optimization has been explored in [65; 68], where a projected gradient method is employed for the lower-level problem.

**Related works on manifolds.** There has been limited work on bilevel optimization problems on manifolds. [7] studies semivectorial bilevel optimization on Riemannian manifolds where the upper-level is a scalar optimization problem while the lower-level is a multiobjective problem under greatest coalition. [50; 49] reformulate bilevel problems on manifolds into a single-level problem based on the KKT conditions on manifolds. However, for all those works, it is unclear whether there exists an algorithm that efficiently solves the problem in large-scale settings. In contrast, we aim to provide a general framework for solving bilevel optimization on Riemannian manifolds. [47] is a contemporary work that also proposes gradient-based algorithms for bilevel optimization on Riemannian manifolds. The main differences of our work with respect to [47] are as follows: (1) We provide an analysis for various hypergradient estimators while [47] focuses on conjugate gradient for deterministic setting and Neumann series for stochastic setting; (2) We provide an analysis for retraction which is more computationally efficient than exponential map and parallel transport employed in [47]; and (3) We explore the utility of Riemannian bilevel optimization in various machine learning applications, which is not the case with [47].

## 2 Preliminaries and notations

A Riemannian manifold \(\mathcal{M}\) is a smooth manifold equipped with a smooth inner product structure (a Riemannian metric) \(\langle\cdot,\cdot\rangle_{p}:T_{z}\mathcal{M}\times T_{z}\mathcal{M}\to \mathbb{R}\) for any \(z\in\mathcal{M}\) and its tangent space \(T_{z}\mathcal{M}\). The induced norm is thus \(\|u\|_{z}=\sqrt{\langle u,u\rangle_{z}}\) for any \(u\in T_{z}\mathcal{M}\). A geodesic \(c:[0,1]\to\mathcal{M}\) generalizes the line segment in the Euclidean space as the locally shortest path on manifolds. The exponential map on a manifold is defined as \(\mathrm{Exp}_{z}(u)=c(1)\) for a geodesic \(c\) that satisfies \(c(0)=z,c^{\prime}(0)=u\). In a totally normal neighbourhood \(\mathcal{U}\) where exponential map has a smooth inverse, the Riemannian distance \(d(x,y)=\|\mathrm{Exp}_{x}^{-1}(y)\|_{x}=\|\mathrm{Exp}_{y}^{-1}(x)\|_{y}\). The parallel transport operation \(\Gamma_{z_{1}}^{z_{2}}:T_{z_{1}}\mathcal{M}\to T_{z_{2}}\mathcal{M}\) is a linear map which preserves the inner product, i.e., \(\langle u,v\rangle_{z_{1}}=\langle\Gamma_{z_{1}}^{z_{2}}u,\Gamma_{z_{1}}^{z_{ 1}}v\rangle_{z_{2}}\), \(\forall u,v\in T_{z_{1}}\mathcal{M}\). The (Cartesian) product of Riemannian manifolds \(\mathcal{M}_{x}\times\mathcal{M}_{y}\) is also a Riemannian manifold.

For a differentiable function \(f:\mathcal{M}\to\mathbb{R}\), the Riemannian gradient \(\mathcal{G}f(z)\in T_{z}\mathcal{M}\) is the tangent vector that satisfies \(\langle\mathcal{G}f(z),u\rangle_{z}=\mathrm{D}f(z)[u]\) for all \(u\in T_{z}\mathcal{M}\). Here \(\mathrm{D}\) is the differential operator and \(\mathrm{D}f(z)[u]\) represents the directional derivative of \(f\) at \(z\) along \(u\). For a twice differentiable function \(f\), Riemannian Hessian \(\mathcal{H}f(z)\) is defined as the covariant derivative of Riemannian gradient.

Geodesic convexity extends the convexity notion in the Euclidean space to Riemannian manifolds. A geodesic convex set \(\mathcal{Z}\subseteq\mathcal{M}\) is where any two points can be joined by a geodesic. A function \(f:\mathcal{M}\to\mathbb{R}\) is said to be geodesic (strongly) convex if for all geodesics \(c:[0,1]\to\mathcal{Z}\), \(f(c(t))\) is (strongly) convex in \(t\in[0,1]\). If the function is smooth, then \(f\) is called \(\mu\)-geodesic strongly convex if and only if \(f(\mathrm{Exp}_{z}(tu))\geq f(z)+t\langle\mathcal{G}f(z),u\rangle_{z}+t^{2} \frac{\mu}{2}\|u\|_{z}^{2}\),\(\forall t\in[0,1]\). An equivalent second-order characterization is \(\mathcal{H}(z)\succeq\mu\mathrm{id}\), where we denote \(\mathrm{id}\) as the identity operator.

For a bifunction \(\phi:\mathcal{M}_{x}\times\mathcal{M}_{y}\to\mathbb{R}\), we denote \(\mathcal{G}_{x}\phi(x,y),\mathcal{G}_{y}\phi(x,y)\) as the Riemannian (partial) gradient and \(\mathcal{H}_{x}\phi(x,y),\mathcal{H}_{y}\phi(x,y)\) as the Riemannian Hessian. The Riemannian cross-derivativesare linear operators \(\mathcal{G}^{2}_{xy}\phi(x,y):T_{y}\mathcal{M}_{y}\to T_{x}\mathcal{M}_{x}, \mathcal{G}^{2}_{yx}\phi(x,y):T_{x}\mathcal{M}_{x}\to T_{y}\mathcal{M}_{y}\) defined as \(\mathcal{G}^{2}_{xy}\phi(x,y)[v]=\mathrm{D}_{y}\mathcal{G}_{x}\phi(x,y)[v]\) for any \(v\in T_{y}\mathcal{M}_{y}\) (with \(\mathrm{D}\) representing the differential operator) and similarly for \(\mathcal{G}^{2}_{yx}\phi(x,y)\). For a linear operator \(T:T_{x}\mathcal{M}_{x}\to T_{y}\mathcal{M}_{y}\), the adjoint operator, denoted as \(T^{\dagger}\) is defined with respect to the Riemannian metric, i.e., \(\langle T[u],v\rangle_{y}=\langle T^{\dagger}[v],u\rangle_{x}\) for any \(u\in T_{x}\mathcal{M}_{x},v\in T_{y}\mathcal{M}_{y}\). The operator norm of \(T\) is defined as \(\|T\|_{y}\coloneqq\sup_{u\in T_{x}\mathcal{M}_{x}:\|u\|_{x}=1}\|T[u]\|_{y}\).

## 3 Proposed Riemannian hypergradient algorithm

In this work, we consider the constrained bilevel optimization problem

\[\min_{x\in\mathcal{M}_{x}}F(x)\coloneqq f(x,y^{*}(x)),\qquad\text{ s.t. }y^{*}(x)=\operatorname*{arg\,min}_{y\in\mathcal{M}_{y}}g(x,y),\] (1)

where \(\mathcal{M}_{x},\mathcal{M}_{y}\) are two Riemannian manifolds and \(f,g:\mathcal{M}_{x}\times\mathcal{M}_{y}\to\mathbb{R}\) are real-valued jointly smooth functions. We focus on the setting where the lower-level function \(g(x,y)\) is geodesic strongly convex. This ensures the lower-level problem has a unique solution \(y^{*}(x)\) for a given \(x\). The upper-level function \(f\) can be nonconvex on \(\mathcal{M}_{x}\times\mathcal{M}_{y}\).

We propose to minimize \(F(x)\) directly within the Riemannian optimization framework. To this end, we need the notion of the Riemannian gradient of \(F(x)\coloneqq f(x,y^{*}(x))\), which we call the Riemannian hypergradient.

**Proposition 1**.: _The differential of \(y^{*}(x)\) and the Riemannian hypergradient of \(F(x)\) are given by_

\[\begin{array}{rl}\mathrm{D}y^{*}(x)&=-\mathcal{H}_{y}^{-1}g(x,y^{*}(x)) \circ\mathcal{G}^{2}_{yx}g(x,y^{*}(x))\\ \mathcal{G}F(x)&=\mathcal{G}_{x}f(x,y^{*}(x))-\mathcal{G}^{2}_{xy}g(x,y^{*}(x ))[\mathcal{H}_{y}^{-1}g(x,y^{*}(x))[\mathcal{G}_{y}f(x,y^{*}(x))]].\end{array}\] (2)

The above proposition crucially relies on the implicit function theorem on manifolds [25] and requires the invertibility of the Hessian of the lower level function \(f\) with respect to \(y\). This is guaranteed in our setup as \(f\) is geodesic strongly convex in \(y\). Hence, there exists a unique differentiable function \(y^{*}(x)\) that maps \(x\) to the lower-level solution. We show the Riemannian hypergradient descent (RHGD) algorithm for (1) in Algorithm 1.

```
1:Initialize \(x_{0}\in\mathcal{M}_{x},y_{0}\in\mathcal{M}_{y}\).
2:for\(k=0,...,K-1\)do
3:\(y_{k}^{0}=y_{k}\).
4:for\(s=0,...,S-1\)do
5:\(y_{k}^{s+1}=\mathrm{Exp}_{y_{k}^{s}}(-\eta_{y}\,\mathcal{G}_{y}g(x_{k},y_{k}^ {s}))\).
6:endfor
7:Set \(y_{k+1}=y_{k}^{S}\).
8:Compute approximated hypergradient \(\widehat{\mathcal{G}}F(x_{k})\).
9:Update \(x_{k+1}=\mathrm{Exp}_{x_{k}}(-\eta_{z}\widehat{\mathcal{G}}F(x_{k}))\).
10:endfor ```

**Algorithm 1** Riemannian hypergradient descent (RHGD)

We highlight that Step 8 of Algorithm 1 approximates the Riemannian hypergradient. In the rest of the section, we discuss various computationally efficient ways to estimate the Riemannian hypergradient and discuss the corresponding theoretical guarantees for RHGD. The error of hypergradient approximation comes from the inaccuracies of \(y_{k+1}\) to \(y^{*}(x_{k})\) and also from the Hessian inverse.

### Hypergradient estimation

When the inverse Hessian of the lower-level problem can be computed efficiently, we can estimate the hypergradient directly by evaluating the Hessian inverse (**HINV**) at \(y_{k+1}\), i.e., \(\widehat{\mathcal{G}}_{\mathrm{hinv}}F(x_{k})=\mathcal{G}_{x}f(x_{k},y_{k+1}) --\mathcal{G}^{2}_{xy}g(x_{k},y_{k+1})\big{[}\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1 })[\mathcal{G}_{y}f(x_{k},y_{k+1})]\big{]}\). However, computing the inverse Hessian is computationally expensive in many scenarios. We now discuss three practical strategies for estimating the Riemannian hypergradient when \(y_{k+1}\) is given.

**Conjugate gradient approach (CG).** When evaluating the Hessian inverse is difficult, we can solve the linear system \(\mathcal{H}_{y}g(x_{k},y_{k+1})[u]=\mathcal{G}_{y}f(x_{k},y_{k+1})\) for some \(u\in T_{y_{k+1}}\mathcal{M}_{y}\). To this end, we employ the tangent space conjugate gradient algorithm (Appendix F, Algorithm 3) that solves the linear system on the tangent space \(T_{y_{k+1}}\mathcal{M}_{y}\) with only access to Hessian-vector products, i.e., \(\widehat{\mathcal{G}}_{\mathrm{eg}}F(x_{k})=\mathcal{G}_{x}f(x_{k},y_{k+1})- \mathcal{G}_{xy}^{2}g(x_{k},y_{k+1})[\hat{v}_{k}^{T}]\), where \(\hat{v}_{k}^{T}\) is computed as a solution to \(\mathcal{H}_{y}g(x_{k},y_{k+1})[\hat{v}_{k}^{T}]=\mathcal{G}_{y}f(x_{k},y_{k+1})\), where \(T\) is the number of iterations of the tangent space conjugate gradient algorithm.

**Truncated Neumann series approach (NC).** The Neumann series states for an invertible operator \(H\) such that \(\|H\|\leq 1\), its inverse \(H^{-1}=\sum_{i=0}^{\infty}(\mathrm{id}-H)^{i}\), where \(\mathrm{id}\) is the identity operator. An alternative approach to estimate the Hessian inverse is to use a truncated Neumann series, which leads to the following approximated hypergradient, \(\widehat{\mathcal{G}}_{\mathrm{ns}}F(x_{k})=\mathcal{G}_{x}f(x_{k},y_{k+1})- \mathcal{G}_{xy}^{2}g(x_{k},y_{k+1})[\gamma\sum_{i=0}^{T-1}(\mathrm{id}- \gamma\mathcal{H}_{y}g(x_{k},y_{k+1}))^{i}[\mathcal{G}_{y}f(x_{k},y_{k+1})]]\), where \(\gamma\) is chosen such that \((\mathrm{id}-\gamma\mathcal{H}_{y}g(x_{k},y_{k+1}))\succ 0\). \(\gamma\) can be set as \(\gamma=\frac{1}{\mathcal{L}}\), where the gradient operator is \(L\)-Lipschitz (discussed later in Definition 1). Empirically, we observe that this approach is faster than the conjugate gradient approach. However, it requires estimating \(T\) and \(L\) beforehand.

**Automatic differentiation approach (AD).** Another hypergradient estimation strategy follows the idea of iterative differentiation by backpropagation. After running several iterations of gradient update to obtain \(y_{k+1}\) (which is a function of \(x_{k}\)), we can use automatic differentiation to compute directly the Riemannian gradient of \(f(x_{k},y_{k+1}(x_{k}))\) with respect to \(x_{k}\). We can compute the Riemannian hypergradient from the differential in the direction of arbitrary \(u\in T_{x_{k}}\mathcal{M}_{x}\) using basic chain rules.

### Theoretical analysis

This section provides theoretical analysis for the proposed hypergradient estimators as well as the Riemannian hypergradient descent. First, we require the notion of Lipschitzness of functions and operators defined on Riemannian manifolds. Below, we introduce the definition in terms of bi-functions and bi-operators and state the assumptions that are required for the analysis.

**Definition 1** (Lipschitzness).: (1) For a bifunction \(f:\mathcal{M}_{x}\times\mathcal{M}_{y}\to\mathbb{R}\), we say \(f\) has \(L\) Lipschitz Riemannian gradient in \(\mathcal{U}_{x}\times\mathcal{U}_{y}\subseteq\mathcal{M}_{x}\times\mathcal{M}_ {y}\) if it satisfies for any \(x,x_{1},x_{2}\in\mathcal{U}_{x},y,y_{1},y_{2}\in\mathcal{U}_{y}\), \(\|\nabla_{y_{1}}^{y_{2}}\mathcal{G}_{y}f(x,y_{1})-\mathcal{G}_{y}f(x,y_{2})\| _{y_{2}}\leq Ld(y_{1},y_{2})\), \(\|\mathcal{G}_{x}f(x,y_{1})-\mathcal{G}_{x}f(x,y_{2})\|_{x}\leq Ld(y_{1},y_{2})\), \(\|\Gamma_{x_{1}}^{x_{2}}\mathcal{G}_{x}f(x_{1},y)-\mathcal{G}_{x}f(x,y)\|_{x _{2}}\leq Ld(x_{1},x_{2})\) and \(\|\mathcal{G}_{y}f(x_{1},y)-\mathcal{G}_{y}f(x,y)\|_{y}\leq Ld(x_{1},x_{2})\).

(2) For an operator \(\mathcal{G}(x,y):T_{y}\mathcal{M}_{y}\to T_{x}\mathcal{M}_{x}\), we say \(\mathcal{G}(x,y)\) is \(\rho\)-Lipschitz if it satisfies, \(\|\Gamma_{x_{1}}^{x_{2}}\mathcal{G}(x_{1},y)-\mathcal{G}(x_{2},y)\|_{x_{2}} \leq\rho\,d(x_{1},x_{2})\) and \(\|\mathcal{G}(x,y_{1})-\mathcal{G}(x,y_{2})\Gamma_{y_{1}}^{y_{2}}\|_{x}\leq \rho\,d(y_{1},y_{2})\).

(3) For an operator \(\mathcal{H}(x,y):T_{y}\mathcal{M}_{y}\to T_{y}\mathcal{M}_{y}\), we say \(\mathcal{H}(x,y)\) is \(\rho\)-Lipschitz if it satisfies, \(\|\Gamma_{y_{1}}^{y_{2}}\mathcal{H}(x,y_{1})\Gamma_{y_{2}}^{y_{1}}-\mathcal{H }(x,y_{2})\|_{y_{2}}\leq\rho\,d(y_{1},y_{2})\) and \(\|\mathcal{H}(x_{1},y)-\mathcal{H}(x_{2},y)\|_{y}\leq\rho\,d(x_{1},x_{2})\).

It is worth mentioning that Definition 1 implies the joint Lipschitzness over the product manifold \(\mathcal{M}_{x}\times\mathcal{M}_{y}\), which is verified in Appendix C.2. Due to the possible nonconvexity for the upper level problem, the optimality is measured in terms of the Riemannian gradient norm of \(F(x)\).

**Definition 2** (\(\epsilon\)-stationary point).: We call \(x\in\mathcal{M}_{x}\) an \(\epsilon\)-stationary point of bilevel optimization (1) if it satisfies \(\|\mathcal{G}F(x)\|_{x}^{2}\leq\epsilon\).

**Assumption 1**.: All the iterates in the lower level problem are bounded in a compact subset that contains the optimal solution, i.e., there exists a constants \(D_{k}>0\), for all \(k\) such that \(d(y_{k}^{*},y^{*}(x_{k}))\leq D_{k}\) for all \(s\). Such a neighbourhood has unique geodesic. We take \(\bar{D}\coloneqq\max_{k}\{D_{1},...,D_{k}\}\).

**Assumption 2**.: Function \(f(x,y)\) has bounded Riemannian gradients, i.e., \(\|\mathcal{G}_{y}f(x,y)\|_{y}\leq M\), \(\|\mathcal{G}_{x}f(x,y)\|_{x}\leq M\) for all \((x,y)\in\mathcal{U}\) and the Riemannian gradients are \(L\)-Lipschitz in \(\mathcal{U}\).

**Assumption 3**.: Function \(g(x,y)\) is \(\mu\)-geodesic strongly convex in \(y\in\mathcal{U}_{y}\) for any \(x\in\mathcal{U}_{x}\) and has \(L\) Lipschitz Riemannian gradient \(\mathcal{G}_{x}g(x,y),\mathcal{G}_{y}g(x,y)\) in \(\mathcal{U}\). Further, the Riemannian Hessian \(\mathcal{H}_{y}g(x,y)\), cross derivatives \(\mathcal{G}_{xy}^{2}g(x,y)\), \(\mathcal{G}_{yx}^{2}g(x,y)\) are \(\rho\)-Lipschitz in \(\mathcal{U}\).

Assumption 1 is standard in Riemannian optimization literature by properly bounding the domain of variables, which allows to express Riemannian distance in terms of (inverse) Exponential map. Also, the boundedness of the domain implies the bound on curvature, as is required for analyzing convergence for geodesic strongly convex lower-level problems [41; 71]. Assumptions 2 and 3 are common regularity conditions imposed on \(f\) and \(g\) in the bilevel optimization literature. This translates into the smoothness of the function \(F\) and \(\mathrm{D}y^{*}(x)\) (discussed in Appendix C.3).

We first bound the estimation error of the proposed schemes of approximated hypergradient as follows. For the hypergradient computed by automatic differentiation, we highlight that due to the presence of exponential map in the chain of differentiation, it is non-trivial to explicitly express \(\mathrm{D}_{x_{k}}y_{k}^{S}\). Here, we adopt the property of exponential map (which is locally linear) in the ambient space [1], i.e., \(\mathrm{Exp}_{x}(u)=x+u+O(\|u\|_{x}^{2})\). This requires the use of tangent space projection of \(\xi\) in the ambient space as \(\mathcal{P}_{x}(\xi)\), which is solved for the \(v\) such that \(\langle v,\xi\rangle_{x}=\langle u,\xi\rangle\) for any \(\xi\in T_{x}\mathcal{M}\).

For notation simplicity, we denote \(\kappa_{l}\coloneqq\frac{L}{\mu}\) and \(\kappa_{\rho}\coloneqq\frac{\rho}{\mu}\). For analysis, we consider \(\kappa_{\rho}=\Theta(\kappa_{l})\).

**Lemma 1** (Hypergradient approximation error bound).: _Under Assumptions 1, 2, 3, we can bound the error for approximated hypergradient as_

1. _Hinv:_ \(\|\widehat{\mathcal{G}}_{\mathrm{hinv}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}\leq (L+\kappa_{\rho}M+\kappa_{l}L+\kappa_{l}\kappa_{\rho}M)d\big{(}y^{*}(x_{k}),y_ {k+1}\big{)}\)_._
2. _CG:_ \(\|\widehat{\mathcal{G}}_{\mathrm{cg}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}} \leq\big{(}L+\kappa_{\rho}M+L\big{(}1+2\sqrt{\kappa_{l}}\big{)}\big{(}\kappa_ {l}+\frac{M\kappa_{\rho}}{\mu}\big{)}\big{)}d(y^{*}(x_{k}),y_{k+1})+2L\sqrt{ \kappa_{l}}\big{(}\frac{\sqrt{\kappa_{l}}-1}{\sqrt{\kappa_{l}}+1}\big{)}^{T} \|\hat{v}_{0}^{0}-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}}\)_, where_ \(v_{k}^{*}=\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))[\mathcal{G}_{y}f(x_{k},y^{* }(x_{k}))]\)_._
3. _NS:_ \(\|\widehat{\mathcal{G}}_{\mathrm{ns}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}} \leq(L+\kappa_{l}L+\kappa_{\rho}M+\kappa_{l}\kappa_{\rho}M)d(y^{*}(x_{k}),y_{k +1})+\kappa_{l}M(1-\gamma\mu)^{T}\)_._
4. _AD: Suppose further there exist_ \(C_{1},C_{2},C_{3}>0\) _such that_ \(\|\mathrm{D}_{x_{k}}y_{k}^{s}\|_{y_{k}^{s}}^{\mathrm{c}}\leq C_{1}\)_,_ \(\|\Gamma_{x}^{\mathrm{F}}\mathcal{P}_{x}v-v\|_{y}\leq C_{2}d(x,y)\|v\|_{y}\) _and_ \(\mathrm{D}_{x}\mathrm{Exp}_{x}(u)=\mathcal{P}_{\mathrm{Exp}_{x}(u)}\big{(} \mathrm{id}+\mathrm{D}_{x}u\big{)}+\mathcal{E}\) _where_ \(\|\mathcal{E}\|_{\mathrm{Exp}_{x}(u)}\leq C_{3}\|\mathrm{D}_{x}u\|_{x}\|u\|_{x}\) _for any_ \(x,y\in\mathcal{U}\) _and_ \(v\in T_{y}M_{y}\)_,_ \(u\in T_{x}\mathcal{M}_{x}\)_. Then,_ \[\|\widehat{\mathcal{G}}_{\mathrm{ad}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}} \leq\big{(}\frac{2M\widetilde{C}}{\mu-\eta_{y}\zeta L^{2}}+L(1+\kappa_{l}) \big{)}(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{\frac{S-1}{2}}d(y_{k},y^{*}(x_ {k}))+M\kappa_{l}(1-\eta_{y}\mu)^{S}\)_, where_ \(\widetilde{C}\coloneqq(\kappa_{l}+1)\rho+(C_{2}+\eta_{y}C_{3})L\big{(}(1-\eta_ {y}\mu)C_{1}+\eta_{y}L\big{)}\)_._

From Lemma 1, it is evident that the exact Hessian inverse exhibits the tightest bound, which is followed by conjugate gradient (CG) and truncated Neumann series (NS). Automatic differentiation (AD) presents the worst upper bound on the error due to the introduction of curvature constant \(\zeta\), resulting in \((1-\Theta(\frac{\mu^{2}}{L^{2}\zeta}))^{S}=(1-\Theta(\frac{1}{\kappa_{l}^{2} \zeta}))^{S}\) for the trailing term, which could be much larger than \((1-\gamma\mu)^{T}=(1-\Theta(\frac{1}{\kappa_{l}}))^{T}\) for NS and \(\big{(}\frac{\sqrt{\kappa_{l}}-1}{\sqrt{\kappa_{l}}+1}\big{)}^{T}=(1-\Theta( \frac{1}{\sqrt{\kappa_{l}}}))^{T}\) for CG. Further, the error critically relies on the number of inner iterations \(S\) compared with \(T\) for CG and NS, and the constants \(C_{1},C_{2},C_{3}\) can be large for manifolds with high curvature. We now present the main convergence result with the four proposed hypergradient estimation strategies.

**Theorem 1**.: _Denote \(\Delta_{0}\coloneqq F(x_{0})+d^{2}(y_{0},y^{*}(x_{0}))\) and \(L_{F}\coloneqq\big{(}\frac{L}{\mu}+1\big{)}\big{(}L+\frac{\tau M}{\mu}+\frac{ \rho LM}{\mu^{2}}+\frac{L^{2}}{\mu}\big{)}=O(\kappa_{l}^{3})\). Under Assumptions 1, 2, 3, we have the following bounds on the hypergradient norm obtained by Algorithm 1._

* _Hinv:_ _Let_ \(\eta_{x}=\frac{1}{20L_{F}}\) _and_ \(S\geq\widetilde{\Theta}(\kappa_{l}^{2}\zeta)\)_. We have_ \(\min_{k=0,\ldots,K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq 80L_{F}\Delta_{0}/K\)_._

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Methods & \(G_{f}\) & \(G_{g}\) & \(JV_{g}\) & \(HV_{g}\) \\ \hline HGD-CG [40] & \(O(\kappa_{l}^{3}\epsilon^{-1})\) & \(\widetilde{O}(\kappa_{l}^{4}\epsilon^{-1})\) & \(O(\kappa_{l}^{3}\epsilon^{-1})\) & \(\widetilde{O}(\kappa_{l}^{3.5}\epsilon^{-1})\) \\ -AD [40] & \(O(\kappa_{l}^{3}\epsilon^{-1})\) & \(\widetilde{O}(\kappa_{l}^{4}\epsilon^{-1})\) & \(\widetilde{O}(\kappa_{l}^{4}\epsilon^{-1})\) & \(\widetilde{O}(\kappa_{l}^{4}\epsilon^{-1})\) \\ SHGD-NS [40, 11] & \(O(\kappa_{l}^{5}\epsilon^{-2})\) & \(\widetilde{O}(\kappa_{l}^{3}\epsilon^{-2})\) & \(O(\kappa_{l}^{5}\epsilon^{-2})\) & \(\widetilde{O}(\kappa_{l}^{6}\epsilon^{-2})\) \\ \hline RHGD-HINV & \(O(\kappa_{l}^{3}\epsilon^{-1})\) & \(\widetilde{O}(\kappa_{l}^{5}\zeta\epsilon^{-1})\) & \(O(\kappa_{l}^{3}\epsilon^{-1})\) & NA \\ -CG & \(O(\kappa_{l}^{4}\epsilon^{-1})\) & \(\widetilde{O}(\kappa_{l}^{6}\zeta\epsilon^{-1})\) & \(O(\kappa_{l}^{4}\epsilon^{-1})\) & \(\widetilde{O}(\kappa_{l}^{4}\epsilon^{-1})\) \\ -NS & \(O(\kappa_{l}^{3}\epsilon^{-1})\) & \(\widetilde{O}(\kappa_{l}^{5}\zeta\epsilon^{-1})\) & \(O(\kappa_{l}^{3}\epsilon^{-1})\) & \(\widetilde{O}(\kappa_{l}^{4}\epsilon^{-1})\) \\ -AD & \(O(\kappa_{l}^{3}\epsilon^{-1}

* _CG: Let \(\Lambda\coloneqq C_{v}^{2}+\kappa_{l}^{2}(\frac{5M^{2}C_{v}^{2}D^{2}}{\mu}+1)\), where \(C_{v}\coloneqq\frac{M\kappa_{\rho}}{\mu}+\frac{M\kappa_{\rho}\kappa_{l}}{\mu}+ \kappa_{l}^{2}+\kappa_{l}\). Choosing \(\eta_{x}=\frac{1}{24\lambda}\), \(S\geq\widetilde{\Theta}(\kappa_{l}^{2}\zeta)\), and \(T_{\rm cg}\geq\widetilde{\Theta}(\sqrt{\kappa_{l}})\), we have \(\min_{k=0,...,K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{96\Lambda}{K} \big{(}\Delta_{0}+\|v_{0}^{*}\|_{p^{*}(x_{0})}^{2}\big{)}\)._
* _NS: Choosing \(\eta_{x}=\frac{1}{20L_{F}},S\geq\widetilde{\Theta}(\kappa_{l}^{2}\zeta)\), and \(T_{\rm ns}\geq\widetilde{\Theta}(\kappa\log(\frac{1}{\epsilon}))\) for an arbitrary \(\epsilon>0\), we have \(\min_{k=0,...,K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{80L_{F}}{K} \Delta_{0}+\frac{\epsilon}{2}\)._
* _AD: Choosing \(\eta_{x}=\frac{1}{20L_{F}}\) and \(S\geq\widetilde{\Theta}(\kappa_{l}^{2}\zeta\log(\frac{1}{\epsilon}))\) for an arbitrary \(\epsilon>0\), we have \(\min_{k=0,...,K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{80L_{F}}{K} \Delta_{0}+\frac{\epsilon}{2}\)._

**Complexity analysis.** Based on the convergence guarantees in Theorem 1, we have analyzed (in Corollary 1), the computational complexity of the proposed algorithm with four different hypergradient estimation strategies in reaching the \(\epsilon\)-stationary point. The results are summarized in Table 1. For reference, we also provide the computational cost of Euclidean algorithms which solve bilevel Euclidean optimization problem [40]. We notice that except for CG, the gradient complexity for \(f\) (i.e., \(G_{f}\)) matches the Euclidean version. For conjugate gradient, the complexity is higher by \(O(\kappa_{l})\), which is due to the additional distortion from the use of vector transport when tracking the error of conjugate gradient at each epoch. In terms of gradient complexity for \(g\) (i.e., \(G_{g}\)), all deterministic methods require a higher complexity by at least \(\widetilde{O}(\kappa_{l}\zeta)\) compared to the Euclidean baselines. This is because of the curvature distortion when analyzing the convergence for geodesic strongly convex functions. Similar comparisons can be also made with respect to the computations of cross-derivatives and Hessian vector products.

### Extension to stochastic bilevel optimization

```
1:Initialize \(x_{0}\in\mathcal{M}_{x},y_{0}\in\mathcal{M}_{y}\).
2:for\(k=0,...,K-1\)do
3:\(y_{k}^{0}=y_{k}\).
4:for\(s=0,...,S-1\)do
5: Sample a batch \(\mathcal{B}_{1}\).
6:\(y_{k}^{s+1}=\mathrm{Exp}_{y_{k}^{s}}(-\eta_{y}\,\mathcal{G}_{y\mathcal{B}_{1 }}(x_{k},y_{k}^{s}))\).
7:endfor
8: Set \(y_{k+1}=y_{k}^{S}\).
9: Sample batches \(\mathcal{B}_{2},\mathcal{B}_{3},\mathcal{B}_{4}\).
10: Compute \(\widehat{\mathcal{G}}F(x_{k})\).
11: Update \(x_{k+1}=\mathrm{Exp}_{x_{k}}(-\eta_{x}\widehat{\mathcal{G}}F(x_{k}))\).
12:endfor ```

**Algorithm 2** Riemannian stochastic bilevel optimization with Hessian inverse.

In this section, we consider the bilevel optimization problem (1) in the stochastic setting, where \(f(x,y^{*}(x))\coloneqq\frac{1}{n}\sum_{i=1}^{n}f_{i}(x,y^{*}(x))\) and \(g(x,y)\coloneqq\frac{1}{m}\sum_{i=1}^{m}g_{i}(x,y)\). The algorithm for solving the stochastic bilevel optimization problem is in Algorithm 2, where we sample \(\mathcal{B}_{1},\mathcal{B}_{2},\mathcal{B}_{3},\mathcal{B}_{4}\) afresh every iteration. The batch index is omitted for clarity. The batches are sampled uniformly at random with replacement such that the mini-batch gradient is an unbiased estimate of the full gradient. Here, we denote \(f_{\mathcal{B}}(x,y)\coloneqq\frac{1}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}f_{i} (x,y)\) and similarly for \(g\). We let \([n]\coloneqq\{1,\ldots,n\}\).

In Step 10 of Algorithm 2, we can employ any hypergradient estimator proposed in Section 3.1. In this work, we only show convergence under the Hessian inverse approximation of hypergradient, i.e., \(\widehat{\mathcal{G}}F(x_{k})=\mathcal{G}_{x}f_{\mathcal{B}_{2}}(x_{k},y_{k+1 })-\mathcal{G}_{xy}^{2}g_{\mathcal{B}_{3}}(x_{k},y_{k+1})[\mathcal{H}_{y}^{-1}g_ {\mathcal{B}_{4}}(x_{k},y_{k+1})][\mathcal{G}_{y}f_{\mathcal{B}_{2}}(x_{k},y_{ k+1})]]\). Similar analysis can be followed for other approximation strategies. The theoretical guarantees are in Theorem 2, where we require Assumption 4, which is common in existing works for analyzing stochastic algorithms on Riemannian manifolds [42, 23, 22].

**Assumption 4**.: Under stochastic setting, Assumption 1 holds and Assumptions 2, 3 are satisfied for component functions \(f_{i}(x,y),g_{j}(x,y)\), for all \(i\in[n],j\in[m]\). Further, stochastic gradient, Hessian, and cross derivatives are unbiased estimates.

**Theorem 2**.: _Under Assumption 4, consider Algorithm 2. Suppose we choose \(\eta_{x}=\frac{1}{20L_{F}},S\geq\widetilde{\Theta}(\kappa_{l}^{2}\zeta)\), and \(|\mathcal{B}_{1}|,|\mathcal{B}_{2}|,|\mathcal{B}_{3}|,|\mathcal{B}_{4}|\geq \Theta(\kappa_{l}^{2}\epsilon^{-1})\) for an arbitrary \(\epsilon>0\). Then we have \(\min_{k=0,...,K-1}\mathbb{E}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{80L_{F} \Delta_{0}}{K}+\frac{\epsilon}{2}\) and the gradient complexity to reach \(\epsilon\)-stationary solution is \(G_{f}=O(\kappa_{l}^{5}\epsilon^{-2}),G_{g}=\widetilde{O}(\kappa_{l}^{3}\zeta \epsilon^{-2}),JV_{g}=O(\kappa_{l}^{5}\epsilon^{-2})\)._

In Table 1, we compare our attained complexities with that of stocBiO [40], which makes use of a truncated Neumann series. With exact Hessian inverse, we can match the \(G_{f}\) and \(JV_{g}\) complexitieswith stocBio. For the \(G_{g}\) complexity, the additional curvature constant is inevitable from the convergence analysis for geodesic strongly convex functions. Nevertheless we observe the same order dependency on \(\kappa_{l}\). This is mainly due to the analysis where we choose a smaller stepsize \(\eta_{y}=\Theta(\frac{\mu}{L^{2}})\) compared to \(\Theta(\frac{2}{L+\mu})\) in [40]. The larger stepsize, despite increasing the convergence rate, also increases the variance under stochastic setting. We believe an order of \(\Theta(\kappa_{l})\) lower can be established for stocBio, following our analysis.

### Extension to retraction

Our analysis till now has been limited to the use of the exponential map. However, the retraction mapping is often preferred over the exponential map due to its lower computational cost. Here, we show that use of retraction in our algorithms also leads to similar convergence guarantees.

**Assumption 5**.: There exist constants \(\widetilde{c}\geq 1,c_{R}\geq 0\) such that \(d^{2}(x,y)\leq\widetilde{c}\|u\|_{x}^{2}\) and \(\|\mathrm{Exp}_{x}^{-1}(y)-u\|_{x}\leq c_{R}\|u\|^{2}\), for any \(x,y=\mathrm{Retr}_{x}(u)\in\mathcal{U}\).

Assumption 5 is standard (e.g. in [42; 23]) in bounding the error between exponential map and retraction given that retraction is a first-order approximation to the exponential map.

**Theorem 3**.: _Suppose Assumptions 1, 2, 3 and 5 hold and let \(\widetilde{L}_{F}=4\kappa_{l}c_{R}M+5\widetilde{c}L_{F}\). Then consider Algorithm 1 with exponential map replaced with general retraction. We can obtain the following bounds._

* _HINV: Let_ \(\eta_{x}=\Theta(1/\widetilde{L}_{F}),S\geq\widetilde{\Theta}(\kappa_{l}^{2} \zeta)\)_. Then_ \(\min_{k=0,\ldots,K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq 16\widetilde{L}_{F} \Delta_{0}/K\)_._
* _CG: Let_ \(\eta_{x}=\Theta(1/\widetilde{\Lambda}),S\geq\widetilde{\Theta}(\kappa_{l}^{2} \zeta),T_{\mathrm{cg}}\geq\widetilde{\Theta}(\sqrt{\kappa_{l}})\)_, where_ \(\widetilde{\Lambda}=C_{v}^{2}\bar{c}+\kappa_{l}^{2}(\frac{5M^{2}C_{a}^{2} \bar{D}^{2}}{\mu}+\bar{c})\)_. Then_ \(\min_{k=0,\ldots,K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{9\bar{c} \bar{\Lambda}}{K}(\Delta_{0}+\|v_{0}^{*}\|_{y^{*}(x_{0})}^{2})\)_._
* _NS: Let_ \(\eta_{x}=\Theta(1/\widetilde{L}_{F}),S\geq\widetilde{\Theta}(\kappa_{l}^{2} \zeta)\)_. Then for an arbitrary_ \(\epsilon>0\)_,_ \(T_{\mathrm{ns}}\geq\widetilde{\Theta}(\kappa_{l}\log(1/\epsilon))\)_, we have_ \(\min_{k=0,\ldots,K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{16\widetilde {L}_{F}}{K}\Delta_{0}+\frac{\epsilon}{2}\)_._
* _AD: Let_ \(\eta_{x}=\Theta(1/\widetilde{L}_{F})\)_,_ \(S\geq\widetilde{\Theta}(\kappa_{l}^{2}\zeta\log(1/\epsilon))\)_. Then for an arbitrary_ \(\epsilon>0\)_, we have_ \(\min_{k=0,\ldots,K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{16\widetilde {L}_{F}}{K}\Delta_{0}+\frac{\epsilon}{2}\)_._

Theorem 3 demonstrates that employing a general retraction preserves the same order of convergence and complexity as the exponential map in Theorem 1. This is due to the fact that \(\widetilde{L}_{F}=\Theta(L_{F})\) and \(\widetilde{\Lambda}=\Theta(\Lambda)\), where \(L_{F}\) and \(\Lambda\) are as defined in Theorem 1. In addition, when exponential map is used, Theorem 3 recovers the results in Theorem 1 as \(c_{R}=0\) and \(\bar{c}=1\).

## 4 Experiments

This section explores various applications of bilevel optimization problems over manifolds. All the experiments are implemented based on Geoopt [44] and the codes are available at https://github.com/andyjm3/rhgd.

### Synthetic problem

We consider the following bilevel optimization problem on the Stiefel manifold \(\mathrm{St}(d,r)=\{\mathbf{W}\in\mathbb{R}^{d\times r}:\mathbf{W}^{\top} \mathbf{W}=\mathbf{I}_{r}\}\) and SPD manifold \(\mathbb{S}_{++}^{d}=\{\mathbf{M}\in\mathbb{R}^{d\times d}:\mathbf{M}\succ 0\}\) (in Appendix A):

\[\max_{\mathbf{W}\in\mathrm{St}(d,r)}\mathrm{tr}(\mathbf{M}^{*}\mathbf{X}^{ \top}\mathbf{Y}\mathbf{W}^{\top}),\quad\text{ s.t. }\mathbf{M}^{*}=\operatorname*{arg\,min}_{\mathbf{M}\in\mathbb{S}_{++}^{d}} \ \langle\mathbf{M},\mathbf{X}^{\top}\mathbf{X}\rangle+\langle\mathbf{M}^{-1}, \mathbf{W}\mathbf{Y}^{\top}\mathbf{Y}\mathbf{W}^{\top}+\nu\mathbf{I}\rangle,\]

where \(\mathbf{X}\in\mathbb{R}^{n\times d},\mathbf{Y}\in\mathbb{R}^{n\times r}\), with \(n\geq d\geq r\), are given matrices and \(\nu>0\) is the regularization parameter. The above is a synthetically constructed problem that aims to maximize the similarity between \(\mathbf{X}\) and \(\mathbf{Y}\) in different feature dimensions. We align \(\mathbf{X}\) and \(\mathbf{Y}\) to the same dimension via \(\mathbf{W}\in\mathrm{St}(d,r)\) and also learn an appropriate geometric metric \(\mathbf{M}\in\mathbb{S}_{++}^{d}\) in the lower-level problem [69]. The geodesic convexity of the lower-level problem and the Hessian inverse expression are discussed in Appendix H.1.

**Results.** We generate random data matrices \(\mathbf{X},\mathbf{Y}\) with \(n=100,d=50\), and \(r=20\). We set \(\nu=0.01\) and fix \(\eta_{x}=\eta_{y}=0.5\). We compare the three proposed strategies for approximating the hypergradient where we select \(\gamma=1.0\) and \(T_{\mathrm{ns}}=50\) for Neumann series (NS) and set maximum iterations \(T_{\mathrm{cg}}\) for conjugate gradient (CG) to be \(50\) and break once the residual reaches a tolerance of \(10^{-10}\). We set the number of outer iterations (epochs) \(K\) to be \(200\). Figure 1 compares RHGD with different approximation strategies implemented with \(S=20\) or \(50\) number of inner iterations.

### Hyper-representation over SPD manifolds

Hyper-representation [54; 61] aims to solve a regression/classification task while searching for the best representation of the data. It can be formulated as a bilevel optimization problem, where the lower-level optimizes the regression/classification parameters while the upper-level searches for the optimal embedding of the inputs. Suppose we are given a set of SPD matrices, \(\mathcal{D}=\{\mathbf{A}_{i}\}_{i=1}^{n}\) where \(\mathbf{A}_{i}\in\mathbb{S}_{++}^{d}\) and the task is to learn a low-dimensional embedding of \(\mathbf{A}_{i}\) while remaining close to their semantics labels. In particular, we partition the set into a training set \(\mathcal{D}_{\mathrm{tr}}\) and validation set \(\mathcal{D}_{\mathrm{val}}\).

**Shallow hyper-representation for regression.** We consider a shallow learning paradigm over \(\mathcal{D}\) through the regression task. The representation is parameterized with \(\mathbf{W}^{\top}\mathbf{A}_{i}\mathbf{W}\) for \(\mathbf{W}\in\mathrm{St}(d,r)\). The requirement of orthogonality on \(\mathbf{W}\) follows [38; 30; 33] that ensures the learned representations are SPD. The learned representation is then transformed to a Euclidean space for performing regression, namely through a matrix logarithm (that acts as a bijective map between the space of SPD matrices and symmetric matrices) and a vectorization operation \(\mathrm{vec}(\cdot)\) that extract the upper-triangular part of the symmetric matrix. The bilevel optimization problem is

\[\begin{array}{l}\min\limits_{\mathbf{W}\in\mathrm{St}(d,r)}\sum\limits_{i \in\mathcal{D}_{\mathrm{val}}}\frac{(\mathrm{vec}(\mathrm{logm}(\mathbf{W}^{ \top}\mathbf{A}_{i}\mathbf{W}))\boldsymbol{\beta}^{*}-\mathbf{y}_{i})^{2}}{2| \mathcal{D}_{\mathrm{val}}|},\\ \text{s.t. }\boldsymbol{\beta}^{*}=\underset{\boldsymbol{\beta}\in\mathbb{R}^{r(r+1 )/2}}{\arg\min}\sum\limits_{i\in\mathcal{D}_{\mathrm{tr}}}\frac{(\mathrm{vec }(\mathrm{logm}(\mathbf{W}^{\top}\mathbf{A}_{i}\mathbf{W}))\boldsymbol{\beta} -\mathbf{y}_{i})^{2}}{2|\mathcal{D}_{\mathrm{tr}}|}+\frac{\lambda}{2}\| \boldsymbol{\beta}\|^{2}.\end{array}\]

The regularization \(\lambda>0\) ensures the lower-level problem is strongly convex. The upper-level problem is on the validation set while the lower-level problem is on the training set. We generate random \(\mathbf{W},\mathbf{A}_{i}\) and \(\boldsymbol{\beta}\) and construct \(\mathbf{y}\) with \(y_{i}=\mathrm{vec}(\mathrm{logm}(\mathbf{W}^{\top}\mathbf{A}_{i}\mathbf{W})) \boldsymbol{\beta}+\epsilon_{i}\), where \(\epsilon_{i}\sim\mathcal{N}(0,1)\). We generate \(200\ \mathbf{A}_{i}\) with \(|\mathcal{D}_{\mathrm{val}}|=100\) and \(|\mathcal{D}_{\mathrm{tr}}|=100\). In Figure 1(a), we show the loss on validation set (the upper loss) in terms of number of outer iterations. We compare both the deterministic (RHGD) and stochastic (RSHGD) versions of Riemannian hypergradient descent. We again observe that the best performance is attained by either the ground-truth Hessian inverse or the conjugate gradient. NS requires carefully selecting the hyperparameters \(\gamma,T\), which pose difficulties in real applications. For the stochastic versions, all the methods perform similarly.

**Deep hyper-representation for classification.** We now explore a 2-layer SPD network [38] for classifying ETH-80 image set [46]. The dataset consists of 8 classes, each with 10 objects. Each object is represented by an image set consisting of images taken from different viewing angles. Here, we represent each image set by taking the covariance matrix of the images in the same set after resizing them into \(10\times 10\). This results in 80 SPD matrices \(\mathbf{A}_{i}\) of size \(100\times 100\) for classification. Let \(\Phi(\mathbf{A}_{i})=\mathrm{vec}(\mathrm{logm}(\mathbf{W}_{2}^{\top}\mathrm{ ReEig}(\mathbf{W}_{1}^{\top}\mathbf{A}_{i}\mathbf{W}_{1})\mathbf{W}_{2}))\) be the output of the 2 layer network where \(\mathrm{ReEig}(\mathbf{A})=\mathbf{U}\max\{\epsilon\mathbf{I},\boldsymbol{ \Sigma}\}\mathbf{U}^{\top}\) is the eigenvalue rectifying activation with the eigenvectors \(\mathbf{U}\) and

Figure 1: Figures (a) & (b) show the plot of objective of the upper-level problem (Upper Objective) for different strategies. HINV and CG strategies have fastest convergence, followed by NS and AD. The corresponding estimation errors are shown in (c). Figure (d) specifically shows the robustness of approximation error obtained by NS across different \(\gamma\) and \(T\) values.

eigenvalues \(\mathbf{\Sigma}\) of \(\mathbf{A}\). We consider the same bilevel optimization as above except the least-squares loss function becomes the cross-entropy loss. Here we sample \(5\) samples from each class to form the training set and the rest as the validation set. We set \(d_{1}=20,d_{2}=5\), and fix learning rate to be \(0.1\) for both lower and upper problems. Figures 1(b) and 1(c) show the good performance on the validation accuracy (upper-level loss).

### Riemannian meta learning

Meta learning [16; 34] allows adaptation of models to new tasks with minimal amount of additional data and training, by distilling past learning experiences. A recent work [64] considers meta learning with orthogonality constraint. In particular, the upper-level optimization searches for the base parameters shared by all tasks while the lower level optimizes over the task-specific parameters to ensure generalization ability. Let \(P_{\mathcal{T}}\) denote the distribution of meta tasks and for each training epoch, we sample \(m\) tasks \(\mathcal{D}^{\ell}\sim P_{\mathcal{T}},\ell=1,...,m\). Each task is composed of a support and query set denoted by \(\mathcal{D}^{\ell}_{\mathrm{s}},\mathcal{D}^{\ell}_{\mathrm{q}}\), and the task is to learn a set of base parameters \(\Theta\) such that the model can quickly adapt to the query set from the support set by adjusting only a few parameters \(w\). For each task, the task-specific parameter \(w^{*}_{\ell}\) is learned from the support set, which is used to update the base parameters by minimizing the loss over the query set. In standard settings, \(w_{\ell}\) corresponds to the final linear layer of a neural network [39; 40]. Here, we adopt the setup with \(w_{\ell}\) to be the last layer parameters in the Euclidean space while enforcing \(\Theta\) on the Stiefel manifold. The problem of Riemannian meta-learning is \(\min_{\Theta\in\mathbb{St}}\frac{1}{m}\sum_{\ell=1}^{m}\mathcal{L}(\Theta,w^ {*}_{\ell};\mathcal{D}^{\ell}_{\mathrm{q}})\) s.t. \(w^{*}_{\ell}=\arg\min_{w_{\ell}}\frac{1}{m}\sum_{\ell=1}^{m}\mathcal{L}( \Theta,w_{\ell};\mathcal{D}^{\ell}_{\mathrm{s}})+\mathcal{R}(w_{\ell})\), where \(\mathcal{D}^{\ell}_{\mathrm{s}}\), \(\mathcal{D}^{\ell}_{\mathrm{q}}\) are the support and query sets for task \(\ell\) and \(\mathcal{R}(\cdot)\) is a regularizer that ensures strong convexity of the lower-level problem.

**Results.** We consider 5-ways 5-shots meta learning over the MiniImageNet dataset [59] where the backbone network is a 4-block CNN with the kernel of the first 2 layers constrained to be orthogonal in terms of the output channel (following [48]). The kernel size is \(3\times 3\) and we consider \(16\) output channels with a padding of \(1\). Each convolutional block consists of a convolutional layer, followed by a ReLU activation, a max-pooling and a batch normalization layer. \(\Theta\), thus, has the dimension \((16*3*3)\times 16=144\times 16\), which is constrained to the Stiefel manifold.

In Figure 1(d), we plot the test accuracy averaged for over 200 tasks. We compare RHGD with an extrinsic update baseline PHGD, which projects the update from the Euclidean space to the Stiefel manifold at every iteration. We observe the RHGD converges faster compared to the extrinsic update PHGD, thereby showing the benefit of the Riemannian modeling.

### Unsupervised domain adaptation

Given two marginals \(\bm{\mu}\in\mathbb{R}^{n},\bm{\nu}\in\mathbb{R}^{m}\) with equal total mass, i.e., \(\bm{\mu}^{\top}\mathbf{1}_{n}=\bm{\nu}^{\top}\mathbf{1}_{m}=1\) where we assume unit mass without loss of generality. Let

[MISSING_PAGE_EMPTY:10]

## References

* [1] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. _Optimization algorithms on matrix manifolds_. Princeton University Press, 2008.
* [2] Jan Harold Alcantara and Akiko Takeda. Theoretical smoothing frameworks for general nonsmooth bilevel problems. Technical report, arXiv preprint arXiv:2401.17852, 2024.
* [3] Foivos Alimisis, Antonio Orvieto, Gary Becigneul, and Aurelien Lucchi. A continuous-time perspective for modeling acceleration in Riemannian optimization. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2020.
* [4] Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Geometric means in a novel vector space structure on symmetric positive-definite matrices. _SIAM Journal on Matrix Analysis and Applications_, 29(1):328-347, 2007.
* [5] Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations. In _the AAAI Conference on Artificial Intelligence (AAAI)_, 2018.
* [6] Rajendra Bhatia. Positive definite matrices. In _Positive Definite Matrices_. Princeton university press, 2009.
* [7] Henri Bonnel, Leonard Todjihounde, and Constantin Udriste. Semivectorial bilevel optimization on Riemannian manifolds. _Journal of Optimization Theory and Applications_, 167:464-486, 2015.
* [8] Nicolas Boumal. _An introduction to optimization on smooth manifolds_. Cambridge University Press, 2023.
* [9] Lesi Chen, Jing Xu, and Jingzhao Zhang. On finding small hyper-gradients in bilevel optimization: Hardness results and improved analysis. In _Annual Conference on Learning Theory (COLT)_, 2024.
* [10] Tianyi Chen, Yuejiao Sun, Quan Xiao, and Wotao Yin. A single-timescale method for stochastic bilevel optimization. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2022.
* [11] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [12] Nicolas Courty, Remi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 39(9):1853-1865, 2017.
* [13] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2013.
* [14] Mathieu Dagreou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [15] Ahmed Douik and Babak Hassibi. Manifold optimization over the set of doubly stochastic matrices: A second-order geometry. _IEEE Transactions on Signal Processing_, 67(22):5761-5774, 2019.
* [16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International Conference on Machine Learning (ICML)_, 2017.
* [17] Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z Alaya, Aurelie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, et al. POT: Python optimal transport. _Journal of Machine Learning Research_, 22(1):3571-3578, 2021.

* [18] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In _International Conference on Machine Learning (ICML)_, 2018.
* [19] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. Technical report, arXiv preprint arXiv:1802.02246, 2018.
* [20] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2012.
* [21] Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration complexity of hypergradient computation. In _International Conference on Machine Learning (ICML)_, 2020.
* [22] Andi Han and Junbin Gao. Riemannian stochastic recursive momentum method for non-convex optimization. In _International Joint Conference on Artificial Intelligence (IJCAI)_, 2021.
* [23] Andi Han and Junbin Gao. Improved variance reduction methods for Riemannian non-convex optimization. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(11):7610-7623, 2022.
* [24] Andi Han, Bamdev Mishra, Pratik Jawanpuria, and Junbin Gao. Learning with symmetric positive definite matrices via generalized Bures-Wasserstein geometry. In _International Conference on Geometric Science of Information (GSI)_, 2023.
* [25] Andi Han, Bamdev Mishra, Pratik Jawanpuria, and Junbin Gao. Nonconvex-nonconcave min-max optimization on Riemannian manifolds. _Transactions on Machine Learning Research_, 2023.
* [26] Andi Han, Bamdev Mishra, Pratik Jawanpuria, and Junbin Gao. Riemannian accelerated gradient methods via extrapolation. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2023.
* [27] Andi Han, Bamdev Mishra, Pratik Jawanpuria, Pawan Kumar, and Junbin Gao. Riemannian Hamiltonian methods for min-max optimization on manifolds. _SIAM Journal on Optimization_, 33(3):1797-1827, 2023.
* [28] Andi Han, Bamdev Mishra, Pratik Kumar Jawanpuria, and Junbin Gao. On Riemannian optimization over positive definite matrices with the Bures-Wasserstein geometry. _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [29] Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel programming. _SIAM Journal on Scientific and Statistical Computing_, 13(5):1194-1217, 1992.
* [30] Mehrtash Harandi, Mathieu Salzmann, and Richard Hartley. Dimensionality reduction on SPD manifolds: The emergence of geometry-aware methods. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 40(1):48-62, 2017.
* [31] Ryuichiro Hataya and Makoto Yamada. Nystrom method for accurate and scalable implicit differentiation. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2023.
* [32] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic. _SIAM Journal on Optimization_, 33(1):147-180, 2023.
* [33] Inbal Horev, Florian Yger, and Masashi Sugiyama. Geometry-aware principal component analysis for symmetric positive definite matrices. In _Asian Conference on Machine Learning (ACML)_, 2016.
* [34] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(9):5149-5169, 2021.

* [35] Zihao Hu, Guanghui Wang, Xi Wang, Andre Wibisono, Jacob D Abernethy, and Molei Tao. Extragradient type methods for Riemannian variational inequality problems. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2024.
* [36] Feihu Huang and Shangqian Gao. Riemannian gradient methods for stochastic composition problems. _Neural Networks_, 153:224-234, 2022.
* [37] Feihu Huang and Shangqian Gao. Gradient descent ascent for minimax problems on Riemannian manifolds. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* [38] Zhiwu Huang and Luc Van Gool. A Riemannian network for spd matrix learning. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2017.
* [39] Kaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. Convergence of meta-learning with task-specific adaptation over partial parameters. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [40] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In _International Conference on Machine Learning (ICML)_, 2021.
* [41] Michael Jordan, Tianyi Lin, and Emmanouil-Vasileios Vlatakis-Gkaragkounis. First-order algorithms for min-max optimization in geodesic metric spaces. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [42] Hiroyuki Kasai, Hiroyuki Sato, and Bamdev Mishra. Riemannian stochastic recursive gradient algorithm. In _International Conference on Machine Learning (ICML)_, 2018.
* [43] Philip A Knight. The Sinkhorn-Knopp algorithm: convergence and applications. _SIAM Journal on Matrix Analysis and Applications_, 30(1):261-275, 2008.
* [44] Max Kochurov, Rasul Karimov, and Serge Kozlukov. Geoopt: Riemannian optimization in PyTorch. Technical report, arXiv preprint arXiv:2005.02819, 2020.
* [45] Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D Nowak. A fully first-order method for stochastic bilevel optimization. In _International Conference on Machine Learning (ICML)_, 2023.
* [46] Bastian Leibe and Bernt Schiele. Analyzing appearance and contour based methods for object categorization. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2003.
* [47] Jiaxiang Li and Shiqian Ma. Riemannian bilevel optimization. Technical report, arXiv preprint arXiv:2402.02019, 2024.
* [48] Jun Li, Fuxin Li, and Sinisa Todorovic. Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform. In _International Conference on Learning Representations (ICLR)_, 2019.
* [49] Jiagen Liao and Zhongping Wan. Inexact restoration methods for semivectorial bilevel programming problem on Riemannian manifolds. _Axioms_, 11(12):696, 2022.
* [50] Jiagen Liao and Zhongping Wan. On the Karush-Kuhn-Tucker reformulation of the bilevel optimization problems on Riemannian manifolds. _Filomat_, 36(11):3609-3624, 2022.
* [51] Zhenhua Lin. Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition. _SIAM Journal on Matrix Analysis and Applications_, 40(4):1353-1370, 2019.
* [52] Bo Liu, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu. Bome! bilevel optimization made easy: A simple first-order approach. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [53] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In _International Conference on Learning Representations (ICLR)_, 2018.

* [54] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2020.
* [55] Luigi Malago, Luigi Montrucchio, and Giovanni Pistone. Wasserstein Riemannian geometry of Gaussian densities. _Information Geometry_, 1:137-179, 2018.
* [56] David Martinez-Rubio, Christophe Roux, Christopher Criscitiello, and Sebastian Pokutta. Accelerated methods for Riemannian min-max optimization ensuring bounded geometric penalties. Technical report, arXiv preprint arXiv:2305.16186, 2023.
* [57] Bamdev Mishra, NTV Satyadev, Hiroyuki Kasai, and Pratik Jawanpuria. Manifold optimization for non-linear optimal transport problems. Technical report, arXiv preprint arXiv:2103.00902, 2021.
* [58] Gabriel Peyre and Marco Cuturi. Computational optimal transport: With applications to data science. _Foundations and Trends in Machine Learning_, 11(5-6):355-607, 2019.
* [59] Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised few-shot classification. In _International Conference on Learning Representations (ICLR)_, 2018.
* [60] Chenggen Shi, Jie Lu, and Guangquan Zhang. An extended Kuhn-Tucker approach for linear bilevel programming. _Applied Mathematics and Computation_, 162(1):51-63, 2005.
* [61] Daouda Sow, Kaiyi Ji, and Yingbin Liang. On the convergence theory for Hessian-free bilevel algorithms. _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [62] Rhea Sukthanker, Zhiwu Huang, Suryansh Kumar, Erik Endsjo Goron, Yan Wu, and Luc Van Gool. Neural architecture search of spd manifold networks. In _International Joint Conference on Artificial Intelligence (IJCAI)_, 2021.
* [63] Yue Sun, Nicolas Flammarion, and Maryam Fazel. Escaping from saddle points on Riemannian manifolds. _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [64] Hadi Tabealhojeh, Peyman Adibi, Hossein Karshenas, Soumava Kumar Roy, and Mehrtash Harandi. RMAML: Riemannian meta-learning with orthogonality constraints. _Pattern Recognition_, 140:109563, 2023.
* [65] Ioannis Tsaknakis, Prashant Khanduri, and Mingyi Hong. An implicit gradient-type method for linearly constrained bilevel problems. In _International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 2022.
* [66] Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. _Mathematical Programming_, 161:419-449, 2017.
* [67] Xi Wang, Deming Yuan, Yiguang Hong, Zihao Hu, Lei Wang, and Guodong Shi. Riemannian optimistic algorithms. Technical report, arXiv preprint arXiv:2308.16004, 2023.
* [68] Quan Xiao, Han Shen, Wotao Yin, and Tianyi Chen. Alternating projected sgd for equality-constrained bilevel optimization. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2023.
* [69] Pourya Zadeh, Reshad Hosseini, and Suvrit Sra. Geometric mean metric learning. In _International Conference on Machine Learning (ICML)_, 2016.
* [70] Dewei Zhang and Sam Davanloo Tajbakhsh. Riemannian stochastic gradient method for nested composition optimization. Technical report, arXiv preprint arXiv:2207.09350, 2022.
* [71] Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian SVRG: Fast stochastic optimization on Riemannian manifolds. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2016.

* [72] Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In _Conference on Learning Theory (COLT)_, 2016.
* [73] Peiyuan Zhang, Jingzhao Zhang, and Suvrit Sra. Sion's minimax theorem in geodesic metric spaces and a Riemannian extragradient algorithm. _SIAM Journal on Optimization_, 33(4):2885-2908, 2023.

## Appendix A Riemannian geometries of considered manifolds

**Symmetric positive definite (SPD) manifold.** The SPD manifold of size \(d\) is denoted as \(\mathbb{S}^{d}_{++}\coloneqq\{\mathbf{X}\in\mathbb{R}^{d\times d}:\mathbf{X}^{ \top}=\mathbf{X},\mathbf{X}\succ 0\}\) and the commonly considered Riemannian metric is the affine-invariant metric \(\langle\mathbf{U},\mathbf{V}\rangle_{\mathbf{X}}=\mathrm{tr}(\mathbf{X}^{-1} \mathbf{U}\mathbf{X}^{-1}\mathbf{V})\)[6], for \(\mathbf{U},\mathbf{V}\in T_{\mathbf{X}}\mathbb{S}^{d}_{++}\). Other Riemannian metrics, such as (generalized) Bures-Wasserstein [55; 24], log-Euclidean [4] and log-Cholesky [51] metrics can also be considered. The exponential map is given by \(\mathrm{Exp}_{\mathbf{X}}(\mathbf{U})=\mathbf{X}\mathrm{expm}(\mathbf{X}^{-1 }\mathbf{U})\) where \(\mathrm{expm}(\cdot)\) denotes the principal matrix exponential. The corresponding logarithm map is given by \(\log_{\mathbf{X}}(\mathbf{Y})=\mathbf{X}\mathrm{logm}(\mathbf{X}^{-1}\mathbf{ Y})\). Its Riemannian gradient of a real-valued function \(f\) is derived as \(\mathrm{grad}f(\mathbf{X})=\mathbf{X}\nabla f(\mathbf{X})\mathbf{X}\) and the Riemannian Hessian is \(\mathrm{Hess}f(\mathbf{X})[\mathbf{U}]=\mathrm{Dgrad}f(\mathbf{X})[\mathbf{U}] -\{\mathbf{U}\mathbf{X}^{-1}\mathrm{grad}f(\mathbf{X})\}_{\mathrm{S}}=\mathbf{ X}\nabla^{2}f(\mathbf{X})[\mathbf{U}]\mathbf{X}+\{\mathbf{U}\nabla f( \mathbf{X})\mathbf{X}\}_{\mathrm{S}}\) where we use \(\{\mathbf{A}\}_{\mathrm{S}}\coloneqq(\mathbf{A}+\mathbf{A}^{\top})/2\).

**Stiefel manifold.** The Stiefel manifold is the set of orthonormal matrices, i.e., \(\mathrm{St}(d,r)\coloneqq\{\mathbf{X}\in\mathbb{R}^{d\times r}:\mathbf{X}^{ \top}\mathbf{X}=\mathbf{I}\}\). A common Riemannian metric is the Euclidean inner product. We consider the QR-based retraction in the experiment, which is \(\mathrm{Ret}_{\mathbf{X}}(\mathbf{U})=\mathrm{qf}(\mathbf{X}+\mathbf{U})\) where \(\mathrm{qf}(\cdot)\) extracts the Q-factor from the QR decomposition. Let the orthogonal projection to the tangent space be denoted as \(\mathrm{P}_{\mathbf{X}}(\mathbf{U})=\mathbf{U}-\mathbf{X}\{\mathbf{X}^{\top} \mathbf{U}\}_{\mathrm{S}}\). Then, the Riemannian gradient and Riemannian Hessian are given by \(\mathrm{grad}f(\mathbf{X})=\mathrm{P}_{\mathbf{X}}(\nabla f(\mathbf{X}))\) and \(\mathrm{Hess}f(\mathbf{X})[\mathbf{U}]=\mathrm{P}_{\mathbf{X}}(\nabla^{2}f( \mathbf{X})[\mathbf{U}]-\mathbf{U}\{\mathbf{X}^{\top}\nabla f(\mathbf{X})\}_ {\mathrm{S}})\).

**Doubly stochastic manifold.** The doubly stochastic manifold (or coupling manifold) between two discrete probability measures \(\mu,\nu\) with marginals \(\mathbf{a}\in\mathbb{R}^{m},\mathbf{b}\in\mathbb{R}^{n}\) is the set \(\Pi(\mu,\nu)=\{\mathbf{\Gamma}\in\mathbb{R}^{m\times n}:\Gamma_{ij}>0, \mathbf{\Gamma}\mathbf{1}_{n}=\mathbf{a},\mathbf{\Gamma}^{\top}\mathbf{1}_{m} =\mathbf{b}\}\). It can be equipped with the Fisher information metric, defined as \(\langle\mathbf{U},\mathbf{V}\rangle_{\mathbf{\Gamma}}=\sum_{i,j}(U_{ij}V_{ij} )/\Gamma_{ij}\) for any \(\mathbf{U},\mathbf{V}\in T_{\mathbf{\Gamma}}\Pi(\mu,\nu)\). The retraction is given by \(\mathrm{Ret}_{\mathbf{\Gamma}}(\mathbf{U})=\mathrm{Sinkhorn}(\mathbf{\Gamma} \odot\exp(\mathbf{U}\circ\mathbf{\Gamma}))\) where \(\exp,\odot,\odot\) are elementwise exponential, product, and division operations. \(\mathrm{Sinkhorn}(\cdot)\) represents the Sinkhorn-Knopp iterations for balancing a matrix [43].

## Appendix B Important Lemmas

**Proposition 2** ([8]).: _In a totally normal neighbourhood \(\mathcal{U}\subseteq\mathcal{M}\), a function \(f:\mathcal{U}\to\mathbb{R}\) is \(\mu\)-geodesic strongly convex, then it satisfies for all \(x,y\in\mathcal{U}\)_

\[f(y)\geq f(x)+\langle\mathrm{grad}f(x),\mathrm{Exp}_{x}^{-1}(y)\rangle_{x}+ \frac{\mu}{2}d^{2}(x,y).\]

_If a function \(f\) has \(L\)-Lipschitz Riemannian gradient, then it satisfies for all \(x,y\in\mathcal{U}\)_

\[f(y)\leq f(x)+\langle\mathrm{grad}f(x),\mathrm{Exp}_{x}^{-1}(y)\rangle_{x}+ \frac{L}{2}d^{2}(x,y).\]

**Lemma 2** ([63; 26]).: _There exists a constant \(C_{0}>0\) such that for any \(y_{1},y_{2},y_{3}\in\mathcal{U}_{\text{p}}\), \(u\in T_{y_{1}}\mathcal{M}_{\text{y}}\), \(\|\Gamma_{y_{2}}^{y_{1}}\Gamma_{y_{1}}^{y_{2}}u-\Gamma_{y_{1}}^{y_{3}}u\|\leq C _{0}d(y_{1},y_{2})d(y_{2},y_{3})\|u\|_{y_{1}}\)_

**Lemma 3** (Trigonometric distance bound [72; 71; 28]).: _Let \(x_{a},x_{b},x_{c}\in\mathcal{U}\subseteq\mathcal{M}\) and denote \(a=d(x_{b},x_{c})\), \(b=d(x_{a},x_{c})\) and \(c=d(x_{a},x_{b})\) as the geodesic side lengths. Then,_

\[a^{2}\leq\zeta b^{2}+c^{2}-2\langle\mathrm{Exp}_{x_{a}}^{-1}(x_{b}),\mathrm{ Exp}_{x_{a}}^{-1}(x_{c})\rangle_{x_{a}}\]

_where \(\zeta=\frac{\sqrt{|\kappa^{-}|\bar{D}}}{\tanh(\sqrt{|\kappa^{-}|D})}\) if \(\kappa^{-}<0\) and \(\zeta=1\) if \(\kappa^{-}\geq 0\). Here, \(\bar{D}\) denotes the diameter of \(\mathcal{U}\) and \(\kappa^{-}\) denotes the lower bound of the sectional curvature of \(\mathcal{U}\)._

## Appendix C Proofs for Section 3.1

### Proof of Proposition 1

Proof of Proposition 1.: By the first-order optimality condition, \(y^{*}(x)\) satisfies \(\mathcal{G}_{y}g(x,y^{*}(x))=0\in T_{y^{*}(x)}\mathcal{M}_{y}\). Based on Theorem 5 in [25], taking the (implicit) derivative of the equality with respect to \(x\) yields \(\mathcal{G}_{yx}^{2}g(x,y^{*}(x))[u]+\mathcal{H}_{y}g(x,y^{*}(x))[\mathrm{D}y^{* }(x)[u]]=0\) for any \(u\in T_{x}\mathcal{M}_{x}\). This gives\(\mathrm{D}y^{*}(x)=-\mathcal{H}_{y}^{-1}g(x,y^{*}(x))\circ\mathcal{G}_{yz}^{2}g(x,y ^{*}(x))\). Notice that \(\mathrm{D}y^{*}(x):T_{x}\mathcal{M}_{x}\to T_{y^{*}(x)}\mathcal{M}_{y}(x)\), its adjoint operator \((\mathrm{D}y^{*}(x))^{\dagger}\) is derived as follows. For any \(u\in T_{x}\mathcal{M}_{x}\), \(v\in T_{y^{*}(x)}\mathcal{M}_{y}\)

\[\langle(\mathrm{D}y^{*}(x))^{\dagger}[v],u\rangle_{x}=\langle \mathrm{D}y^{*}(x)[u],v\rangle_{y^{*}(x)} =-\big{\langle}\big{(}\mathcal{H}_{y}^{-1}g(x,y^{*}(x))\circ \mathcal{G}_{yz}^{2}g(x,y^{*}(x))\big{)}[u],v\rangle_{y^{*}(x)}\] \[=-\big{\langle}\mathcal{G}_{yz}^{2}g(x,y^{*}(x))[u],\mathcal{H}_{ y}^{-1}g(x,y^{*}(x))[v]\big{\rangle}_{y^{*}(x)}\] \[=-\big{\langle}\big{(}\mathcal{G}_{xy}^{2}g(x,y^{*}(x))\circ \mathcal{H}_{y}^{-1}g(x,y^{*}(x))\big{)}[v],u\big{\rangle}_{x}\]

where the first equality uses the definition of adjoint operator and the third equality is due to Proposition D.2 in [27] that \(\mathcal{G}_{xy}^{2}g\) and \(\mathcal{G}_{yx}^{2}g\) are adjoint operators. By identification, we have \((\mathrm{D}y^{*}(x))^{\dagger}=-\mathcal{G}_{xy}^{2}g(x,y^{*}(x))\circ \mathcal{H}_{y}^{-1}g(x,y^{*}(x))\).

Finally by the chain rule, we obtain (from the definition of Riemannian gradient), for any \(u\in T_{x}\mathcal{M}_{x}\)

\[\langle\mathcal{G}F(x),u\rangle_{x} =\langle\mathcal{G}_{x}f(x,y^{*}(x)),u\rangle_{x}+\mathrm{D}_{y}f( x,y^{*}(x))[\mathrm{D}y^{*}(x)[u]]\] \[=\langle\mathcal{G}_{x}f(x,y^{*}(x)),u\rangle_{x}+\langle \mathcal{G}_{y}f(x,y^{*}(x)),\mathrm{D}y^{*}(x)[u]\rangle_{y}\] \[=\langle\mathcal{G}_{x}f(x,y^{*}(x)),u\rangle_{x}+\langle(\mathrm{ D}y^{*}(x))^{\dagger}\mathcal{G}_{y}f(x,y^{*}(x)),u\rangle_{x}\] \[=\langle\mathcal{G}_{x}f(x,y^{*}(x))-\mathcal{G}_{xy}^{2}g(x,y^{* }(x))[\mathcal{H}_{y}^{-1}g(x,y^{*}(x))[\mathcal{G}_{y}f(x,y^{*}(x))]],u\rangle _{x}.\]

By identification the proof is complete. 

### On Lipschitzness of gradients

**Proposition 3**.: _If a bifunction \(f(x,y)\) has L-Lipschitz Riemannian gradient, then it satisfies \(\|\mathcal{G}f(z_{1})-\Gamma_{z_{2}}^{z_{1}}\mathcal{G}f(z_{2})\|_{z_{1}}\leq 2 Ld(z_{ 1},z_{2})\), where we let \(z=(x,y)\). If an operator \(\mathcal{G}(x,y):T_{y}\mathcal{M}_{y}\to T_{x}\mathcal{M}_{x}\) is \(\rho\)-Lipschitz, then it satisfies \(\|\mathcal{G}(z_{1})-\Gamma_{z_{2}}^{x_{1}}\mathcal{G}(z_{2})\Gamma_{y_{1}}^{y_ {2}}\|_{x_{1}}\leq\rho\,d(z_{1},z_{2})\). If an operator \(\mathcal{H}(x,y):T_{y}\mathcal{M}_{y}\to T_{x}\mathcal{M}_{x}\) is \(\rho\)-Lipschitz, then it satisfies \(\|\mathcal{H}(z_{1})-\Gamma_{y_{2}}^{y_{1}}\mathcal{H}(z_{2})\Gamma_{y_{1}}^{y _{2}}\|_{y_{1}}\leq\rho\,d(z_{1},z_{2})\)._

Proof of Proposition 3.: From the definition of Riemannian gradient of product manifold we have

\[\|\mathcal{G}f(z_{1})-\Gamma_{z_{2}}^{x_{1}}\mathcal{G}f(z_{2})\| _{z_{1}} =\|\mathcal{G}_{x}f(x_{1},y_{1})-\Gamma_{x_{2}}^{x_{1}} \mathcal{G}_{x}f(x_{2},y_{2})\|_{x_{1}}+\|\mathcal{G}_{y}f(x_{1},y_{1})-\Gamma _{y_{2}}^{y_{1}}\mathcal{G}_{y}f(x_{2},y_{2})\|_{y_{1}}\] \[\leq\|\mathcal{G}_{x}f(x_{1},y_{1})-\mathcal{G}_{x}f(x_{1},y_{2}) \|_{x_{1}}+\|\mathcal{G}_{x}f(x_{1},y_{2})-\Gamma_{x_{2}}^{x_{1}}\mathcal{G}_{x }f(x_{2},y_{2})\|_{x_{1}}\] \[\quad+\|\mathcal{G}_{y}f(x_{1},y_{1})-\mathcal{G}_{y}f(x_{2},y_{1 })\|_{y_{1}}+\|\mathcal{G}_{y}f(x_{2},y_{1})-\Gamma_{y_{2}}^{y_{1}}f(x_{2},y_{2 })\|_{y_{1}}\] \[\leq Ld(y_{1},y_{2})+Ld(x_{1},x_{2})+Ld(x_{1},x_{2})+Ld(y_{1},y_{2 })=2Ld(z_{1},z_{2})\]

where we use triangle inequality of Riemannian norm.

Similarly, for the other two claims, we verify

\[\|\mathcal{G}(z_{1})-\Gamma_{x_{2}}^{x_{1}}\mathcal{G}(z_{2})\Gamma_{y_{1}}^{y _{2}}\|_{z_{1}} =\|\mathcal{G}(x_{1},y_{1})-\mathcal{G}(x_{1},y_{2})\Gamma_{y_{1}}^{y _{2}}\|_{x_{1}}+\|\mathcal{G}(x_{1},y_{2})\Gamma_{y_{1}}^{y_{2}}-\Gamma_{x_{2}}^ {x_{1}}\mathcal{G}(x_{2},y_{2})\Gamma_{y_{1}}^{y_{2}}\|_{x_{1}}\] \[\leq\rho d(y_{1},y_{2})+\rho d(x_{1},x_{2})=\rho d(z_{1},z_{2}).\]

The same arguments also hold for \(\mathcal{H}(x,y)\) and hence the proof is omitted. 

### Boundedness of ingredients

**Lemma 4**.: _Under Assumptions 1, 2, 3, we can show_

* \(\|\mathcal{G}_{yx}^{2}g(x,y)\|_{y}=\|\mathcal{G}_{xy}^{2}g(x,y)\|_{x}\leq L\) _holds for any_ \((x,y)\in\mathcal{U}_{x}\times\mathcal{U}_{y}\)_._
* \(\|\mathrm{D}y^{*}(x)\|_{y^{*}(x)}\leq\kappa_{l}\) _and_ \(\|\mathrm{D}y^{*}(x_{1})-\Gamma_{y^{*}(x_{2})}^{y^{*}(x_{1})}\mathrm{D}y^{*}(x_{2}) \Gamma_{x_{1}}^{x_{2}}\|_{y^{*}(x_{1})}\leq L_{y}d(x_{1},x_{2})\)_, for any_ \(x,x_{1},x_{2}\in\mathcal{U}_{x}\)_, where we let_ \(L_{y}:=\kappa_{l}^{2}\kappa_{\rho}+2\kappa_{l}\kappa_{\rho}+\kappa_{\rho}\)_._
* \(d(y^{*}(x_{1}),y^{*}(x_{2}))\leq\kappa_{l}d(x_{1},x_{2})\)_, for any_ \(x,x_{1},x_{2}\in\mathcal{U}_{x}\)__
* _For any_ \(x,x_{1},x_{2}\in\mathcal{U}_{x},y,y_{1},y_{2}\in\mathcal{U}_{y}\)__ \[\|\Gamma_{y_{1}}^{y_{2}}\mathcal{H}_{y}^{-1}g(x,y_{1})\Gamma_{y_{2}}^{y_{1}}- \mathcal{H}_{y}^{-1}g(x,y_{2})\|_{y_{2}}\leq\frac{\kappa_{\rho}}{\mu}d(y_{1},y_ {2}),\] \[\|\mathcal{H}^{-1}g(x_{1},y)-\mathcal{H}^{-1}g(x_{2},y)\|_{y}\leq \frac{\kappa_{\rho}}{\mu}d(x_{1},x_{2}).\]_4.5 Let \(L_{F}\coloneqq(\kappa_{l}+1)\big{(}L+\kappa_{\rho}M+\kappa_{\rho}\kappa_{l}M+\kappa _{l}L\big{)}\). Then for any \(x_{1},x_{2}\in\mathcal{U}_{x}\), \(\|\Gamma_{x_{1}}^{x_{2}}\mathcal{G}F(x_{1})-\mathcal{G}F(x_{2})\|_{x_{2}}\leq L _{F}d(x_{1},x_{2})\)._

Proof of Lemma 4.: (4.1) First we have for any \(v\in T_{y}\mathcal{M}_{y}\)

\[\|\mathcal{G}_{xy}^{2}g(x,y)[v]\|_{x}=\|\mathrm{D}_{y}\mathcal{G}_{x}g(x,y)[v ]\|_{x}\leq\lim_{t\to 0}\frac{\|\mathcal{G}_{x}g\big{(}x,\mathrm{Exp}_{y}(tv) \big{)}-\mathcal{G}_{x}g\big{(}x,y\big{)}\|_{x}}{|t|}\leq\lim_{t\to 0}\frac{L\|tv \|_{y}}{|t|}=L\|v\|_{y},\]

where we use the fact that \(d(\mathrm{Exp}_{y}(\xi),y)=\|\xi\|_{y}\). The operator norm is the same between \(\mathcal{G}_{xy}^{2}g(x,y)\) and \(\mathcal{G}_{yx}^{2}g(x,y)\) is due to the adjointness. This proves the first claim.

(4.2) We first verify \(\mathrm{D}y^{*}(x)\) can be bounded as

\[\|\mathrm{D}y^{*}(x)\|_{y^{*}(x)}=\|\mathcal{H}_{y}^{-1}g(x,y^{*}(x))\|_{y^{* }(x)}\|\mathcal{G}_{yx}^{2}g(x,y^{*}(x))\|_{y^{*}(x)}\leq\frac{L}{\mu},\]

and \(\mathrm{D}y^{*}(x)\) is also Lipschitz as

\[\|\mathrm{D}y^{*}(x_{1})-\Gamma_{y^{*}(x_{2})}^{y^{*}(x_{1})} \mathrm{D}y^{*}(x_{2})\Gamma_{x_{1}}^{x_{2}}\|_{y^{*}(x_{1})}\] \[\leq\|\mathcal{H}_{y}^{-1}g(x_{1},y^{*}(x_{1}))-\Gamma_{y^{*}(x_{ 2})}^{y^{*}(x_{1})}\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{2}))\Gamma_{y^{*}(x_{1} )}^{y^{*}(x_{2})}\|_{y^{*}(x_{1})}\|\mathcal{G}_{yx}^{2}g(x_{1},y^{*}(x_{1}))\| _{y^{*}(x_{1})}\] \[\quad+\|\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{2}))\|_{x_{2}}\| \Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\mathcal{G}_{yx}^{2}g(x_{1},y^{*}(x_{1}))- \mathcal{G}_{yx}^{2}g(x_{2},y^{*}(x_{2}))\Gamma_{x_{1}}^{x_{2}}\|_{y^{*}(x_{2 })}\] \[\leq L\|\mathcal{H}_{y}^{-1}g(x_{1},y^{*}(x_{1}))-\mathcal{H}_{y} ^{-1}g(x_{2},y^{*}(x_{1}))\|_{y^{*}(x_{1})}+L\|\mathcal{H}_{y}^{-1}g(x_{2},y^{ *}(x_{1}))-\Gamma_{y^{*}(x_{2})}^{y^{*}(x_{1})}\mathcal{H}_{y}^{-1}g(x_{2},y^{ *}(x_{2}))\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{1})}\|_{y^{*}(x_{1})}\] \[\quad+\frac{1}{\mu}\|\mathcal{G}_{yx}^{2}g(x_{1},y^{*}(x_{1}))- \mathcal{G}_{yx}^{2}g(x_{2},y^{*}(x_{1}))\Gamma_{x_{1}}^{x_{2}}\|_{y^{*}(x_{1} )}+\frac{1}{\mu}\|\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\mathcal{G}_{yx}^{2}g(x_{ 2},y^{*}(x_{1}))-\mathcal{G}_{yx}^{2}g(x_{2},y^{*}(x_{2}))\|_{y^{*}(x_{1})}\] \[\leq\frac{L\rho}{\mu^{2}}d(x_{1},x_{2})+\frac{L\rho}{\mu^{2}}d(y^{ *}(x_{1}),y^{*}(x_{2}))+\frac{\rho}{\mu}d(x_{1},x_{2})+\frac{\rho}{\mu}d(y^{*} (x_{1}),y^{*}(x_{2}))\] \[\leq\big{(}\frac{L^{2}\rho}{\mu^{3}}+\frac{2L\rho}{\mu^{2}}+\frac{ \rho}{\mu}\big{)}d(x_{1},x_{2}).\]

(4.3) Now suppose we let \(c:[0,1]\to\mathcal{M}_{y}\), defined as \(c(t)\coloneqq y^{*}(\gamma(t))\) where \(\gamma:[0,1]\to\mathcal{M}_{x}\) is a geodesic that connects \(x_{1},x_{2}\), i.e., \(\gamma(0)=x_{1},\gamma(1)=x_{2}\). Then

\[d(y^{*}(x_{1}),y^{*}(x_{2}))=\int_{0}^{1}\|c^{\prime}(t)\|_{c(t)}dt=\int_{0}^{1 }\|\mathrm{D}y^{*}(\gamma(t))[\gamma^{\prime}(t)]\|_{c(t)}dt\leq\frac{L}{\mu} \int_{0}^{1}\|\gamma^{\prime}(t)\|_{\gamma(t)}dt=\frac{L}{\mu}d(x_{1},x_{2}),\]

where we use the fact that the manifold is complete.

(4.4) For the second claim, we first notice for any (invertible) linear operators \(A,B\), \(A^{-1}-B^{-1}=A^{-1}(B-A)B^{-1}\) and thus \(\|A^{-1}-B^{-1}\|\leq\|A^{-1}\|\|A-B\|\|B^{-1}\|\) for some well-defined norm \(\|\cdot\|\). Here substituting \(A=\Gamma_{y_{1}}^{y_{2}}\mathcal{H}_{y}g(x,y_{1})\Gamma_{y_{2}}^{y_{1}}\), \(B=\mathcal{H}_{y}g(x,y_{2})\), we have

\[\|\Gamma_{y_{1}}^{y_{2}}\mathcal{H}_{y}^{-1}g(x,y_{1})\Gamma_{y_{2 }}^{y_{1}}-\mathcal{H}_{y}^{-1}g(x,y_{2})\|_{y_{2}}\] \[=\|\mathcal{H}_{y}^{-1}g(x,y_{1})\|_{y_{2}}\|\Gamma_{y_{1}}^{y_{2}} \mathcal{H}_{y}g(x,y_{1})\Gamma_{y_{2}}^{y_{1}}-\mathcal{H}_{y}g(x,y_{2})\|_{y_{2 }}\|\mathcal{H}_{y}^{-1}g(x,y_{2})\|_{y_{2}}\] \[\leq\frac{\rho}{\mu^{2}}d(y_{1},y_{2}),\]

where we notice \((\Gamma_{y_{1}}^{y_{2}}\mathcal{H}_{y}(x,y_{1})\Gamma_{y_{2}}^{y_{1}})^{-1}= \Gamma_{y_{1}}^{y_{2}}\mathcal{H}_{y}(x,y_{1})^{-1}\Gamma_{y_{2}}^{y_{1}}\) and use the isometry property of parallel transport. The same argument applies for \(\|\mathcal{H}^{-1}g(x_{1},y)-\mathcal{H}^{-1}g(x_{2},y)\|_{y}\).

(4.5) we have

\[\|\Gamma_{x_{1}}^{x_{2}}\mathcal{G}F(x_{1})-\mathcal{G}F(x_{2})\|_{x_{2}}\] \[\leq\|\Gamma_{x_{1}}^{x_{2}}\mathcal{G}_{x}f(x_{1},y^{*}(x_{1}))- \mathcal{G}_{x}f(x_{2},y^{*}(x_{2}))\|_{x_{2}}\] \[+\|\Gamma_{x_{1}}^{x_{2}}\mathcal{G}_{xy}^{2}g(x_{1},y^{*}(x_{1}))- \mathcal{G}_{xy}^{2}g(x_{2},y^{*}(x_{2}))\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\|_{x _{2}}\|\mathcal{H}_{y}^{-1}g(x_{1},y^{*}(x_{1}))\|_{y^{*}(x_{1})}\|\mathcal{G}_{ y}f(x_{1},y^{*}(x_{1}))\|_{y^{*}(x_{1})}\] \[+\|\mathcal{G}_{xy}^{2}g(x_{2},y^{*}(x_{2}))\|_{x_{2}}\|\Gamma_{y^{* }(x_{1})}^{y^{*}(x_{2})From Assumption 2, 3 and Lemma 4, we can obtain

\[\|\Gamma_{x_{1}}^{x_{2}}\mathcal{G}_{x}f(x_{1},y^{*}(x_{1}))- \mathcal{G}_{x}f(x_{2},y^{*}(x_{2}))\|_{x_{2}}\] \[\leq\|\mathcal{G}_{x}f(x_{1},y^{*}(x_{1}))-\mathcal{G}_{x}f(x_{1}, y^{*}(x_{2}))\|_{x_{1}}+\|\Gamma_{x_{1}}^{x_{2}}\mathcal{G}_{x}f(x_{1},y^{*}(x_{2}) )-\mathcal{G}_{x}f(x_{2},y^{*}(x_{2}))\|_{x_{2}}\] \[\leq Ld(y^{*}(x_{1}),y^{*}(x_{2}))+Ld(x_{1},x_{2})=\big{(}\frac{L^ {2}}{\mu}+L\big{)}d(x_{1},x_{2}).\] \[\|\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\mathcal{G}_{y}f(x_{1},y^{*} (x_{1}))-\mathcal{G}_{y}f(x_{2},y^{*}(x_{2}))\|_{y^{*}(x_{2})}\] \[\leq\|\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\mathcal{G}_{y}f(x_{1}, y^{*}(x_{1}))-\mathcal{G}_{y}f(x_{1},y^{*}(x_{2}))\|_{y^{*}(x_{2})}+\|\mathcal{G}_{y}f (x_{1},y^{*}(x_{2}))-\mathcal{G}_{y}f(x_{2},y^{*}(x_{2}))\|_{y^{*}(x_{2})}\] \[\leq\big{(}\frac{L^{2}}{\mu}+L\big{)}d(x_{1},x_{2}).\]

Similarly, we have

\[\|\Gamma_{x_{1}}^{x_{2}}\mathcal{G}_{xy}^{2}g(x_{1},y^{*}(x_{1}))- \mathcal{G}_{xy}^{2}g(x_{2},y^{*}(x_{2}))\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})} \|_{x_{2}}\] \[\leq\|\mathcal{G}_{xy}^{2}g(x_{1},y^{*}(x_{1}))-\mathcal{G}_{xy}^ {2}g(x_{1},y^{*}(x_{2}))\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\|_{x_{1}}+\| \Gamma_{x_{1}}^{x_{2}}\mathcal{G}_{xy}^{2}g(x_{1},y^{*}(x_{2}))-\mathcal{G}_{ xy}^{2}g(x_{2},y^{*}(x_{2}))\|_{x_{2}}\] \[\leq(\frac{\rho L}{\mu}+\rho)d(x_{1},x_{2})\]

and

\[\|\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\mathcal{H}_{y}^{-1}g(x_{1},y^{*}(x_{1}))-\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{2}))\Gamma_{y^{*}(x_{1})}^ {y^{*}(x_{2})}\|_{y^{*}(x_{2})}\] \[\leq\|\mathcal{H}_{y}^{-1}g(x_{1},y^{*}(x_{1}))-\mathcal{H}_{y}^ {-1}g(x_{2},y^{*}(x_{1}))\|_{y^{*}(x_{1})}+\|\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{ 2})}\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{1}))\Gamma_{y^{*}(x_{2})}^{y^{*}(x_{ 1})}-\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{2}))\|_{y^{*}(x_{2})}\] \[\leq\big{(}\frac{\rho L}{\mu^{2}}+\frac{\rho L}{\mu^{3}}\big{)}d (x_{1},x_{2}).\]

Combining all the results together, we can show

\[\|\Gamma_{x_{1}}^{x_{2}}\mathcal{G}F(x_{1})-\mathcal{G}F(x_{2})\| _{x_{2}} \leq\Big{(}\frac{L^{2}}{\mu}+L+(\frac{\rho L}{\mu}+\rho)\frac{M}{ \mu}+LM\big{(}\frac{\rho}{\mu^{2}}+\frac{\rho L}{\mu^{3}}\big{)}+\frac{L}{\mu }\big{(}\frac{L^{2}}{\mu}+L\big{)}\Big{)}d(x_{1},x_{2})\] \[=\big{(}\frac{L}{\mu}+1\big{)}\big{(}L+\frac{\rho M}{\mu}+\frac{ \rho LM}{\mu^{2}}+\frac{L^{2}}{\mu}\big{)}d(x_{1},x_{2}),\]

which completes the proof. 

### On strong convexity of the lower-level problem

**Lemma 5** (Convergence under strong convexity).: _Under Assumptions 1, 2, 3, suppose \(\eta_{y}<\frac{\mu}{L^{2}\zeta}\), where \(\zeta\geq 1\) is a curvature constant defined in Lemma 3, then we have \(d^{2}(y_{k}^{s+1},y^{*}(x_{k}))\leq(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)d^{2} (y_{k}^{s},y^{*}(x_{k})).\)_

Proof of Lemma 5.: We apply the trigonometric distance bound from Lemma 3 to obtain

\[d^{2}(y_{k}^{s+1},y^{*}(x_{k})) \leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\eta_{y}^{2}\zeta\|\mathcal{G}_ {y}g(x_{k},y_{k}^{s})\|_{y_{k}^{s}}^{2}+2\eta_{y}\langle\mathcal{G}_{y}g(x_{k},y _{k}^{s}),\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\rangle_{y_{k}^{s}}\] \[\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\eta_{y}^{2}\zeta\|\mathcal{G}_ {y}g(x_{k},y_{k}^{s})\|_{y_{k}^{s}}^{2}\] \[\quad+2\eta_{y}\big{(}g(x_{k},y^{*}(x_{k}))-g(x_{k},y_{k}^{s})- \frac{\mu}{2}d^{2}(y_{k}^{s},y^{*}(x_{k}))\big{)}\] \[\leq(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)d^{2}(y_{k}^{s},y^{*}(x _{k})),\]

where the second inequality is due to geodesic strong convexity and the third inequality is due to \(\|\mathcal{G}_{y}g(x_{k},y_{k}^{s})\|_{y_{k}^{s}}^{2}=\|\mathcal{G}_{y}g(x_{k},y _{k}^{s})-\Gamma_{y^{*}(x_{k})}^{y_{k}^{s}}\mathcal{G}_{y}g(x_{k},y^{s}(x_{k}))\|_ {y_{k}^{s}}^{2}\leq L^{2}d^{2}(y_{k}^{s},y^{*}(x_{k}))\) and the fact that \(y^{*}(x_{k})\) is optimal. Here, we require \(\eta_{y}<\frac{\mu}{L^{2}\zeta}\) in order for \(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu<1\).

### Proof of Lemma 1

Proof of Lemma 1.: _Hessian inverse_: for the Hessian inverse approximation, we let

\[\overline{\mathcal{G}}f(x,y)=\mathcal{G}_{x}f(x,y)-\mathcal{G}_{xy}^{2}g(x,y) \big{[}\mathcal{H}_{y}^{-1}g(x,y)[\mathcal{G}_{y}f(x,y)]\big{]}.\]

It can be seen that \(\mathcal{G}F(x_{k})=\overline{\mathcal{G}}f(x_{k},y^{*}(x_{k}))\) and \(\widehat{\mathcal{G}}_{\mathrm{hinv}}F(x_{k})=\overline{\mathcal{G}}f(x_{k}, y_{k+1})\). Then for any \(x\in\mathcal{U}_{x},y_{1},y_{2}\in\mathcal{U}_{y}\), we have

\[\|\overline{\mathcal{G}}f(x,y_{1})-\overline{\mathcal{G}}f(x,y_{2 })\|_{x}\] \[\leq\|\mathcal{G}_{x}f(x,y_{1})-\mathcal{G}_{x}f(x,y_{2})\|_{x}+ \|\mathcal{G}_{xy}^{2}g(x,y_{1})-\mathcal{G}_{xy}^{2}g(x,y_{2})\Gamma_{y_{1}}^ {y_{2}}\|_{x}\|\mathcal{H}_{y}^{-1}g(x,y_{1})\|_{y_{1}}\|\mathcal{G}_{y}f(x,y_ {1})\|_{y}\] \[\quad+\|\mathcal{G}_{xy}^{2}g(x,y_{2})\|_{x}\|\Gamma_{y_{1}}^{y_{ 2}}\mathcal{H}_{y}^{-1}g(x,y_{1})[\mathcal{G}_{y}f(x,y_{1})]-\mathcal{H}^{-1}g (x,y_{2})[\mathcal{G}_{y}f(x,y_{2})]\|_{y_{2}}\] \[\leq\big{(}L+\frac{\rho M}{\mu}\big{)}d(y_{1},y_{2})+L\|\Gamma_{y _{1}}^{y_{2}}\mathcal{H}_{y}^{-1}g(x,y_{1})-\mathcal{H}_{y}^{-1}g(x,y_{2}) \Gamma_{y_{1}}^{y_{2}}\|_{y_{2}}\|\mathcal{G}_{y}f(x,y_{1})\|_{y}\] \[\quad+L\|\mathcal{H}_{y}^{-1}g(x,y_{2})\|_{y}\|\Gamma_{y_{1}}^{y_{ 2}}\mathcal{G}_{y}f(x,y_{1})-\mathcal{G}_{y}f(x,y_{2})\|_{y_{2}}\] \[\leq\big{(}L+\frac{\rho M+L^{2}}{\mu}+\frac{LM\rho}{\mu^{2}} \big{)}d(y_{1},y_{2}),\]

where we use Assumption 2, 3 and Lemma 4.

_Conjugate gradient_: we let \(v_{k}^{*}=\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))[\mathcal{G}_{y}f(x_{k},y^{ *}(x_{k}))]\in T_{y^{*}(x_{k})}\mathcal{M}_{y}\) and let \(\hat{v}_{k}^{*}=\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})[\mathcal{G}_{y}f(x_{k},y_ {k+1})]\in T_{y_{k+1}}\mathcal{M}_{y}\). We first bound

\[\|\widehat{\mathcal{G}}_{\mathrm{cg}}F(x_{k})-\mathcal{G}F(x_{k}) \|_{x_{k}}\] \[\leq\|\mathcal{G}_{x}f(x_{k},y_{k+1})-\mathcal{G}_{x}f(x_{k},y^{ *}(x_{k}))\|_{x_{k}}+\|\mathcal{G}_{xy}^{2}g(x_{k},y_{k+1})\|_{x_{k}}\|\hat{v} _{k}^{T}-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}}\] \[\quad+\|\mathcal{G}_{xy}^{2}g(x_{k},y^{*}(x_{k}))-\mathcal{G}_{xy }^{2}g(x_{k},y_{k+1})\Gamma_{y^{*}(x_{k})}^{y_{k+1}}\|_{x_{k}}\|v_{k}^{*}\|_{y^ {*}(x_{k})}\] \[\leq Ld(y^{*}(x_{k}),y_{k+1})+L\|\hat{v}_{k}^{T}-\Gamma_{y^{*}(x_ {k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}}+\rho\,d(y^{*}(x_{k}),y_{k+1})\|v_{k}^{*}\|_ {y^{*}(x_{k})}\] \[\leq\big{(}L+\kappa_{\rho}M\big{)}d(y^{*}(x_{k}),y_{k+1})+L\|\hat {v}_{k}^{T}-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}},\]

where \(\|v_{k}^{*}\|_{y^{*}(x_{k})}\leq M/\mu\). From standard convergence result eq. 6.19 in [8], we have

\[\|\hat{v}_{k}^{T}-\hat{v}_{k}^{*}\|_{y_{k+1}}^{2}\leq 4\kappa_{l}\Big{(} \frac{\sqrt{\kappa_{l}}-1}{\sqrt{\kappa_{l}}+1}\Big{)}^{2T}\|\hat{v}_{k}^{0}- \hat{v}_{k}^{*}\|_{y_{k+1}}^{2}.\]

This leads to

\[\|\hat{v}_{k}^{T}-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+ 1}}\] (4) \[\leq\|\hat{v}_{k}^{T}-\hat{v}_{k}^{*}\|_{y_{k+1}}+\|\Gamma_{y^{*} (x_{k})}^{y_{k+1}}v_{k}^{*}-\hat{v}_{k}^{*}\|_{y_{k+1}}\] \[\leq 2\sqrt{\kappa_{l}}\Big{(}\frac{\sqrt{\kappa_{l}}-1}{\sqrt{ \kappa_{l}}+1}\Big{)}^{T}\|\hat{v}_{k}^{0}-\hat{v}_{k}^{*}\|_{y_{k+1}}+\| \Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}-\hat{v}_{k}^{*}\|_{y_{k+1}}\] \[\leq 2\sqrt{\kappa_{l}}\Big{(}\frac{\sqrt{\kappa_{l}}-1}{\sqrt{ \kappa_{l}}+1}\Big{)}^{T}\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x_{k})}^{y_{k}}\|_{y_{ k+1}}+\Big{(}1+2\sqrt{\kappa_{l}}\Big{(}\frac{\sqrt{\kappa_{l}}-1}{\sqrt{ \kappa_{l}}+1}\Big{)}^{T}\Big{)}\|\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}- \hat{v}_{k}^{*}\|_{y_{k+1}}\] \[\leq 2\sqrt{\kappa_{l}}\Big{(}\frac{\sqrt{\kappa_{l}}-1}{\sqrt{ \kappa_{l}}+1}\Big{)}^{T}\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x_{k})}^{y_{k}}v_{k}^{*} \|_{y_{k+1}}+\Big{(}1+2\sqrt{\kappa_{l}}\Big{)}\big{(}\kappa_{l}+\frac{M \kappa_{\rho}}{\mu}\big{)}d\big{(}y^{*}(x_{k}),y_{k+1}\big{)},\] (5)

where in the last inequality, we use the definition of \(v_{k}^{*}\) and \(\hat{v}_{k}^{*}\) and the Lipschitzness assumptions. Combining the results yield the desired result.

_Neumann series_: let \(\widehat{\mathcal{H}}_{k}(y)\coloneqq\gamma\sum_{i=0}^{T-1}(\mathrm{id}- \gamma\mathcal{H}_{y}g(x_{k},y))^{i}\). Then we can bound

\[\|\widehat{\mathcal{G}}_{\mathrm{ns}}F(x_{k})-\mathcal{G}F(x_{k}) \|_{x_{k}}\] \[\leq L\,d(y^{*}(x_{k}),y_{k+1})\] \[+\|\mathcal{G}_{xy}^{2}g(x_{k},y_{k+1})\|_{x_{k}}\|\widehat{ \mathcal{H}}_{k}(y_{k+1})-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}\mathcal{H}_{y}^{-1}g(x_{k },y^{*}(x_{k}))\Gamma_{y_{k+1}}^{y^{*}(x_{k})}\|_{y_{k+1}}\|\mathcal{G}_{y}f(x_{k },y_{k+1})\|_{y_{k+1}}\] \[+\|\mathcal{G}_{xy}^{2}g(x_{k},y_{k+1})\|_{x_{k}}\|\mathcal{H}_{y }^{-1}g(x_{k},y^{*}(x_{k}))\|_{y^{*}(x_{k})}\|\Gamma_{y_{k+1}}^{y^{*}(x_\[\leq(L+\kappa_{l}L+\kappa_{\rho}M)d(y^{*}(x_{k}),y_{k+1})+LM\|\widehat{ \mathcal{H}}_{k}(y_{k+1})-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}\mathcal{H}_{y}^{-1}g(x _{k},y^{*}(x_{k}))\Gamma_{y_{k+1}}^{y^{*}(x_{k})}\|_{y_{k+1}}.\]

We now bound

\[\|\widehat{\mathcal{H}}_{k}(y_{k+1})-\Gamma_{y^{*}(x_{k})}^{y_{k+ 1}}\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))\Gamma_{y_{k+1}}^{y^{*}(x_{k})}\|_{ y_{k+1}}\] \[\leq\|\widehat{\mathcal{H}}_{k}(y_{k+1})-\mathcal{H}_{y}^{-1}g(x _{k},y_{k+1})\|_{y_{k+1}}+\|\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})-\Gamma_{y^{*} (x_{k})}^{y_{k+1}}\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))\Gamma_{y_{k+1}}^{y ^{*}(x_{k})}\|_{y_{k+1}}\] \[\leq\|\gamma\sum_{i=T}^{\infty}(\mathrm{id}-\gamma\mathcal{H}_{y} g(x_{k},y_{k+1}))^{i}\|_{y_{k+1}}+\frac{\kappa_{\rho}}{\mu}d(y^{*}(x_{k}),y_{k+1})\] \[\leq\frac{(1-\gamma\mu)^{T}}{\mu}+\frac{\kappa_{\rho}}{\mu}d(y^{ *}(x_{k}),y_{k+1}),\]

where we use the lower bound on \(\mathcal{H}_{y}g(x_{k},y_{k+1})\). Substituting the results back the bound yields the desired result.

_Automatic differentiation_: Given \(y_{k}^{s+1}=\mathrm{Exp}_{y_{k}^{s}}(-\eta_{y}\mathcal{G}_{y}g(x_{k},y_{k}^{s}))\), we can show its differential is

\[\mathrm{D}_{x_{k}}y_{k}^{s+1} =\mathcal{P}_{y_{k}^{s+1}}\big{(}\mathrm{D}_{x_{k}}y_{k}^{s}-\eta _{y}\mathcal{G}_{yx}^{2}g(x_{k},y_{k}^{s})-\eta_{y}\mathcal{H}_{y}g(x_{k},y_{k }^{s})\mathrm{D}_{x_{k}}y_{k}^{s}\big{)}+\mathcal{E}_{k}^{s}\] \[=\mathcal{P}_{y_{k}^{s+1}}\big{(}(\mathrm{id}-\eta_{y}\mathcal{H }_{y}g(x_{k},y_{k}^{s}))\mathrm{D}_{x_{k}}y_{k}^{s}-\eta_{y}\mathcal{G}_{yx}^{ 2}g(x_{k},y_{k}^{s})\big{)}+\mathcal{E}_{k}^{s}\]

where

\[\|\mathcal{E}_{k}^{s}\|_{y_{k}^{s+1}} \leq C_{3}\|(\mathrm{id}-\eta_{y}\mathcal{H}_{y}g(x_{k},y_{k}^{ s}))\mathrm{D}_{x_{k}}y_{k}^{s}-\eta_{y}\mathcal{G}_{yx}^{2}g(x_{k},y_{k}^{s}) \|_{y_{k}^{s}}\|\mathcal{G}_{y}f(x_{k},y_{k}^{s})\|_{y_{k}^{s}}\] \[\leq\eta_{y}^{2}C_{3}\big{(}(1-\eta_{y}\mu)C_{1}+\eta_{y}L\big{)} \|\mathcal{G}_{y}f(x_{k},y_{k}^{s})\|_{y_{k}^{s}}.\]

In addition, we notice \(\widehat{\mathcal{G}}_{\mathrm{ad}}F(x_{k})=\mathcal{G}_{x}f(x_{k},y_{k}^{S})+ (\mathrm{D}_{x_{k}}y_{k}^{S})^{\dagger}[\mathcal{G}_{y}f(x_{k},y_{k}^{S})]\) and we can bound

\[\|\widehat{\mathcal{G}}_{\mathrm{ad}}F(x_{k})-\mathcal{G}F(x_{k}) \|_{x_{k}}\] \[\leq\|\mathcal{G}_{x}f(x_{k},y_{k}^{S})-\mathcal{G}_{x}f(x_{k},y ^{*}(x_{k}))\|_{x_{k}}+\|(\mathrm{D}_{x_{k}}y_{k}^{S})^{\dagger}-(\mathrm{D}_{ x_{k}}y^{*}(x_{k}))^{\dagger}\Gamma_{y_{k}^{S}}^{y^{*}(x_{k})}\|_{x_{k}}\|\mathcal{G}_{y}f(x_{k},y_{k}^{S})\|_{y_{k}^{S}}\] \[\quad+\|\mathrm{D}_{x_{k}}y^{*}(x_{k})\|_{y^{*}(x_{k})}\|\Gamma_{ y_{k}^{S}}^{y^{*}(x_{k})}\mathcal{G}_{y}f(x_{k},y_{k}^{S})-\mathcal{G}_{y}f(x_{k},y ^{*}(x_{k}))\|_{y_{k}^{s}}.\]

In addition, we notice \(\widehat{\mathcal{G}}_{\mathrm{ad}}F(x_{k})=\mathcal{G}_{x}f(x_{k},y_{k}^{S})+ (\mathrm{D}_{x_{k}}y_{k}^{S})^{\dagger}[\mathcal{G}_{y}f(x_{k},y_{k}^{S})]\) and we can bound

\[\|\widehat{\mathcal{G}}_{\mathrm{ad}}F(x_{k})-\mathcal{G}F(x_{k}) \|_{x_{k}}\] \[\leq\|\mathcal{G}_{x}f(x_{k},y_{k}^{S})-\mathcal{G}_{x}f(x_{k},y ^{*}(x_{k}))\|_{x_{k}}+\|(\mathrm{D}_{x_{k}}y_{k}^{S})^{\dagger}-(\mathrm{D}_{ x_{k}}y^{*}(x_{k}))^{\dagger}\Gamma_{y_{k}^{S}}^{y^{*}(x_{k})}\|_{x_{k}}\|\mathcal{G}_{y}f(x_{k},y_{k}^{S})\|_{y_{k}^{S}}\] \[\quad+\|\mathrm{D}_{x_{k}}y^{*}(x_{k})\|_{y^{*}(x_{k})}\|\Gamma_{ y_{k}^{S}}^{y^{*}(x_{k})}\mathcal{G}_{y}f(x_{k},y_{k}^{S})-\mathcal{G}_{y}f(x_{k},y ^{*}(x_{k}))\|_{y_{k}^{s}}.\]

In addition, we notice \(\widehat{\mathcal{G}}_{\mathrm{ad}}F(x_{k})=\mathcal{G}_{x}f(x_{k},y_{k}^{S})+ (\mathrm{D}_{x_{k}}y_{k}^{S})^{\dagger}[\mathcal{G}_{y}f(x_{k},y_{k}^{S})]\) and we can bound

\[\|\widehat{\mathcal{G}}_{\mathrm{ad}}F(x_{k})-\mathcal{G}F(x_{k}) \|_{x_{k}}\] \[\leq\|\mathcal{G}_{x}f(x_{k},y_{k}^{S})-\mathcal{G}_{x}f(x_{k},y^{ *}(x_{k}))\|_{x_{k}}+\|(\mathrm{D}_{x_{k}}y_{k}^{S})^{\dagger}-(\mathrm{D}_{ x_{k}}y^{*}(x_{k}))^{\dagger}\Gamma_{y_{k}^{S}}^{y^{*}(x_{k})}\|_{x_{k}}\|\mathcal{G}_{y}f(x_{k},y_{k}^{S}) \|_{y_{k}^{S}}\] \[\quad+\|\mathrm{D}_{x_{k}}y^{*}(x_{k})\|_{y^{*}(x_{k})}\|\Gamma_{ y_{k}^{S}}^{y^{*}(x_{k})}\mathcal{G}_{y}f(x_{k},y_{k}^{S})-\mathcal{G}_{y}f(x_{k},y ^{*}(x_{k}))\|_{y_{k}^{s}}.\]

In addition, we notice \(\widehat{\mathcal{G}}_{\mathrm{ad}}F(x_{k})=\mathcal{G}_{x}f(x_{k},y_{k}^{S})+( \mathrm{D}_{x_{k}}y_{k}^{S})^{\dagger}[\mathcal{G}_{y}f(x_{k},y_{k}^{S})]\) and we can bound

\[\|\widehat{\mathcal{G}}_{\mathrm{ad}}F(x_{k})-\mathcal{G}F(x_{k}) \|_{x_{k}}\] \[\leq\|\mathcal{G}_{x}f(x_{k},y_{k}^{S})-\mathcal{G}_{x}f(x_{k},y^{ *}(x_{k}))\|_{x_{k}}+\|(\mathrm{D}_{x_{k}}y_{k}^{S})^{\dagger}-(\mathrm{D}_{ x_{k}}y^{*}(x_{k}))^{\dagger}\Gamma_{y_{k}^{S}}^{y^{*}(x_{k})}\|_{x_{k}}

\[\leq(L+L\kappa_{l})(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{\frac{S}{2 }}d(y^{*}(x_{k}),y_{k}^{0})+M\|\Gamma_{y^{*}(x_{k})}^{y_{k}^{S}}\mathrm{D}_{x_{k }}y^{*}(x_{k})-\mathrm{D}_{x_{k}}y_{k}^{S}\|_{y_{k}^{S}}\] \[\leq\Big{(}\frac{2M\widetilde{C}}{\mu-\eta_{y}\zeta L^{2}}+L(1+ \kappa_{l})\Big{)}(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{\frac{S-1}{2}}d(y_{k },y^{*}(x_{k}))+M\kappa_{l}(1-\eta_{y}\mu)^{S},\]

where we use the fact that \(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu\leq 1\). 

### Proof of Theorem 1

Proof of Theorem 1.: By smoothness of \(F(x)\) (Lemma 4.5), we have

\[F(x_{k+1})-F(x_{k}) \leq-\eta_{x}\langle\mathcal{G}F(x_{k}),\widehat{\mathcal{G}}F(x_ {k})\rangle_{x_{k}}+\frac{\eta_{x}^{2}L_{F}}{2}\|\widehat{\mathcal{G}}F(x_{k} )\|_{x_{k}}^{2}\] \[\leq-\big{(}\frac{\eta_{x}}{2}-\eta_{x}^{2}L_{F}\big{)}\| \mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\big{(}\frac{\eta_{x}}{2}+\eta_{x}^{2}L_{F} \big{)}\|\mathcal{G}F(x_{k})-\widehat{\mathcal{G}}F(x_{k})\|_{x_{k}}^{2}.\] (7)

Now we consider the different hypergradient estimator separately.

_1. Hessian inverse_: Let \(C_{\mathrm{hinv}}\coloneqq L+\kappa_{\rho}M+\kappa_{l}L+\kappa_{l}\kappa_{\rho }M\).

\[\|\mathcal{G}F(x_{k})-\widehat{\mathcal{G}}_{\mathrm{hinv}}F(x_{k})\|_{x_{k}}^ {2}\leq C_{\mathrm{hinv}}^{2}d^{2}(y^{*}(x_{k}),y_{k+1})\leq C_{\mathrm{hinv}} ^{2}(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y^{*}(x_{k}),y_{k}),\] (8)where we notice \(y_{k+1}=y_{k}^{S}\) and apply Lemma 5. Furthermore,

\[d^{2}(y_{k},y^{*}(x_{k}))\] \[\leq 2d^{2}(y_{k-1}^{S},y^{*}(x_{k-1}))+2d^{2}(y^{*}(x_{k}),y^{*}(x_ {k-1}))\] \[\leq 2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y^{*}(x_{k-1 }),y_{k-1})+2\eta_{x}^{2}\kappa_{l}^{2}\|\widehat{\mathcal{G}}_{\mathrm{hinv}}F (x_{k-1})\|_{x_{k}}^{2}\] \[\leq 2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y^{*}(x_{k-1 }),y_{k-1})+4\eta_{x}^{2}\kappa_{l}^{2}\|\widehat{\mathcal{G}}_{\mathrm{hinv}}F (x_{k-1})-\mathcal{G}F(x_{k-1})\|_{x_{k-1}}^{2}\] \[\quad+4\eta_{x}^{2}\kappa_{l}^{2}\|\mathcal{G}F(x_{k-1})\|_{x_{k- 1}}^{2}\] \[\leq 2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y^{*}(x_{k-1 }),y_{k-1})+4\eta_{x}^{2}\kappa_{l}^{2}C_{\mathrm{hinv}}^{2}(1+\eta_{y}^{2} \zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})\] \[\quad+4\eta_{x}^{2}\kappa_{l}^{2}\|\mathcal{G}F(x_{k-1})\|_{x_{k- 1}}^{2}\] \[=2(1+2\eta_{x}^{2}\kappa_{l}^{2}C_{\mathrm{hinv}}^{2})(1+\eta_{y} ^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})+4\eta_{x}^{2} \kappa_{l}^{2}\|\mathcal{G}F(x_{k-1})\|_{x_{k-1}}^{2}\] (9)

where we apply Lemma 5 and 4.3 in the second inequality.

Construct a Lyapunov function \(R_{k}\coloneqq F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))\). Then,

\[R_{k+1}-R_{k} =F(x_{k+1})-F(x_{k})+\big{(}d^{2}(y_{k+1},y^{*}(x_{k+1}))-d^{2}(y _{k},y^{*}(x_{k}))\big{)}\] \[\leq-\big{(}\frac{\eta_{x}}{2}-\eta_{x}^{2}L_{F}\big{)}\| \mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\big{(}\frac{\eta_{x}}{2}+\eta_{x}^{2}L_{F} \big{)}\|\mathcal{G}F(x_{k})-\widehat{\mathcal{G}}_{\mathrm{hinv}}F(x_{k})\|_ {x_{k}}^{2}\] \[\quad+\Big{(}\big{(}(2+4\eta_{x}^{2}\kappa_{l}^{2}C_{\mathrm{hinv }}^{2})(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}-1\big{)}d^{2}(y^{*}(x_{k}),y _{k})+4\eta_{x}^{2}\kappa_{l}^{2}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\Big{)}\] \[\leq-(\frac{\eta_{x}}{2}-\eta_{x}^{2}L_{F}-4\eta_{x}^{2}\kappa_{l }^{2})\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\] \[\quad+\Big{(}\big{(}2+C_{\mathrm{hinv}}^{2}(\frac{\eta_{x}}{2}+ \eta_{x}^{2}L_{F})+4\eta_{x}^{2}\kappa_{l}^{2}C_{\mathrm{hinv}}^{2}\big{)}(1+ \eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}-1\Big{)}d^{2}(y^{*}(x_{k}),y_{k})\] \[\leq-(\frac{\eta_{x}}{2}-5\eta_{x}^{2}L_{F})\|\mathcal{G}F(x_{k}) \|_{x_{k}}^{2}\] \[\quad+\Big{(}\big{(}2+C_{\mathrm{hinv}}^{2}(\frac{\eta_{x}}{2}+ 5\eta_{x}^{2}L_{F})\big{)}(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}-1\Big{)} d^{2}(y^{*}(x_{k}),y_{k})\]

where we combine (7) and (9) in the first inequality and use \(\kappa_{l}^{2}\leq L_{F}\) in the third inequality. Now setting \(\eta_{x}=\frac{1}{20L_{F}}\), we can simplify the inequality as

\[R_{k+1}-R_{k} \leq-\frac{1}{80L_{F}}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\Big{(} (2+\frac{3C_{\mathrm{hinv}}^{2}}{80L_{F}})(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y} \mu)^{S}-1\Big{)}d^{2}(y^{*}(x_{k}),y_{k})\] \[\leq-\frac{1}{80L_{F}}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\]

where we choose \(S\geq\log(\frac{80L_{F}}{160L_{F}+3C_{\mathrm{hinv}}^{2}})/\log(1+\eta_{y}^{2} \zeta L^{2}-\eta_{y}\mu)=\widetilde{\Theta}(\kappa_{l}^{2}\zeta)\) for the last inequality.

Summing over \(k=0,...K-1\) yields

\[\frac{1}{K}\sum_{k=0}^{K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{80L_{F}( R_{0}-R_{K})}{K}\leq\frac{80L_{F}\Delta_{0}}{K},\]

which suggests \(\min_{k=0,...,K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{80L_{F}\Delta_{0}}{K}\).

_2. Conjugate gradient:_ Let \(C_{\mathrm{cg}}\coloneqq L+\kappa_{\rho}M+L\big{(}1+2\sqrt{\kappa_{l}}\big{)} \big{(}\kappa_{l}+\frac{M\kappa_{\rho}}{\mu}\big{)}\). Then we can show

\[\|\mathcal{G}F(x_{k})-\widehat{\mathcal{G}}_{\mathrm{cg}}F(x_{k})\|_ {x_{k}}^{2}\] \[\leq 2C_{\mathrm{cg}}^{2}(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S }d^{2}(y^{*}(x_{k}),y_{k})+8L^{2}\kappa_{l}\Big{(}\frac{\sqrt{\kappa_{l}}-1}{ \sqrt{\kappa_{l}}+1}\Big{)}^{2T}\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x_{k})}^{y_{k+1 }}v_{k}^{*}\|_{y_{k+1}}^{2},\] (10)

where it follows from Lemma 1 and Lemma 5. Then following similar analysis as in Hessian inverse case

\[d^{2}(y_{k+1},y^{*}(x_{k+1}))\leq 2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y ^{*}(x_{k}),y_{k})+4\eta_{x}^{2}\kappa_{l}^{2}\|\widehat{\mathcal{G}}_{\mathrm{ cg}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\]\[+4\eta_{x}^{2}\kappa_{l}^{2}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\] \[\leq(2+8\eta_{x}^{2}\kappa_{l}^{2}C_{\rm cg}^{2})(1+\eta_{y}^{2} \zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y^{*}(x_{k}),y_{k})\] \[\quad+32\eta_{x}^{2}\kappa_{l}^{3}L^{2}(\frac{\sqrt{\kappa_{l}}-1 }{\sqrt{\kappa_{l}}+1})^{2T}\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v _{k}^{*}\|_{y_{k+1}}^{2}+4\eta_{x}^{2}\kappa_{l}^{2}\|\mathcal{G}F(x_{k})\|_{x _{k}}^{2}.\] (11)

Further, noticing \(\hat{v}_{k}^{0}=\Gamma_{y_{k}}^{y_{k+1}}\hat{v}_{k-1}^{T}\), we bound \(\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}}=\|\hat{v }_{k-1}^{T}-\Gamma_{y_{k+1}}^{y_{k}}\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\| _{y_{k}}\) as

\[\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k +1}}\] \[\leq\|\hat{v}_{k-1}^{T}-\Gamma_{y^{*}(x_{k})}^{y_{k}}v_{k}^{*}\|_ {y_{k}}+\frac{MC_{0}\bar{D}}{\mu}d(y_{k+1},y^{*}(x_{k}))\] \[\leq\|\hat{v}_{k-1}^{T}-\Gamma_{y^{*}(x_{k-1})}^{y_{k}}v_{k-1}^{*} \|_{y_{k}}+\left\|v_{k}^{*}-\Gamma_{y^{*}(x_{k})}^{y^{*}(x_{k})}v_{k-1}^{*} \right\|_{y_{k}}+\frac{MC_{0}\bar{D}}{\mu}d(y_{k+1},y^{*}(x_{k}))\] \[\leq\|\hat{v}_{k-1}^{T}-\Gamma_{y^{*}(x_{k-1})}^{y_{k}}v_{k-1}^{*} \|_{y_{k}}+\left\|v_{k}^{*}-\Gamma_{y^{*}(x_{k-1})}^{y^{*}(x_{k-1})}v_{k-1}^{* }\right\|_{y_{k}}+\frac{MC_{0}\bar{D}}{\mu}\big{(}d(y_{k},y^{*}(x_{k}))+d(y_{k +1},y^{*}(x_{k}))\big{)}\] \[\leq 2\sqrt{\kappa_{l}}\big{(}\frac{\sqrt{\kappa_{l}}-1}{\sqrt{ \kappa_{l}}+1}\big{)}^{T}\|\hat{v}_{k-1}^{0}-\Gamma_{y^{*}(x_{k-1})}^{y_{k}}v_ {k-1}^{*}\|_{y_{k}}+(1+\sqrt{\kappa_{l}})(\kappa_{l}+\frac{M\kappa_{\rho}}{ \mu})d(y^{*}(x_{k-1}),y_{k})\] \[\quad+\left\|v_{k}^{*}-\Gamma_{y^{*}(x_{k-1})}^{y^{*}(x_{k})}v_{k -1}^{*}\right\|_{y_{k}}+\frac{2MC_{0}\bar{D}}{\mu}d(y_{k},y^{*}(x_{k}))\] \[\leq 2\sqrt{\kappa_{l}}\big{(}\frac{\sqrt{\kappa_{l}}-1}{\sqrt{ \kappa_{l}}+1}\big{)}^{T}\|\hat{v}_{k-1}^{0}-\Gamma_{y^{*}(x_{k-1})}^{y_{k}}v_ {k-1}^{*}\|_{y_{k}}+2\sqrt{\kappa_{l}}(\kappa_{l}+\frac{M\kappa_{\rho}}{\mu})( 1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{\frac{S}{2}}d(y^{*}(x_{k-1}),y_{k-1})\] \[\quad+\left\|v_{k}^{*}-\Gamma_{y^{*}(x_{k-1})}^{y^{*}(x_{k})}v_{k -1}^{*}\right\|_{y_{k}}+\frac{2MC_{0}\bar{D}}{\mu}d(y_{k},y^{*}(x_{k}))\] (12)

where we use Lemma 2 in the first and third inequalities. The second last inequality follows from (5) and \(d(y_{k+1},y^{*}(x_{k}))\leq d(y_{k},y^{*}(x_{k}))\). The last inequality follows from Lemma 5 and \(\kappa_{l}\geq 1\). Now we bound

\[\|v_{k}^{*}-\Gamma_{y^{*}(x_{k-1})}^{y^{*}(x_{k})}v_{k-1}^{*}\|_{y ^{*}(x_{k})}\] \[=\big{\|}\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))\big{[}\mathcal{ G}_{y}f(x_{k},y^{*}(x_{k}))\big{]}-\Gamma_{y^{*}(x_{k-1})}^{y^{*}(x_{k})} \mathcal{H}_{y}^{-1}g(x_{k-1},y^{*}(x_{k-1}))[\mathcal{G}_{y}f(x_{k-1},y^{*}(x_ {k-1}))]\big{\|}_{y^{*}(x_{k})}\] \[\leq M\|\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))-\Gamma_{y^{*}( x_{k-1})}^{y^{*}(x_{k})}\mathcal{H}_{y}^{-1}g(x_{k-1},y^{*}(x_{k-1}))\Gamma_{y^{*}(x_{k})} ^{y^{*}(x_{k-1})}\|_{y^{*}(x_{k})}\] \[\quad+\frac{1}{\mu}\|\Gamma_{y^{*}(x_{k})}^{y^{*}(x_{k-1})} \mathcal{G}_{y}f(x_{k},y^{*}(x_{k}))-\mathcal{G}_{y}f(x_{k-1},y^{*}(x_{k-1}))\|_{ y^{*}(x_{k-1})}\] \[\leq M\|\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))-\mathcal{H}_{y}^{-1 }g(x_{k-1},y^{*}(x_{k}))\|_{y^{*}(x_{k})}\] \[\quad+M\|\mathcal{H}_{y}^{-1}g(x_{k-1},y^{*}(x_{k}))-\Gamma_{y^{*} (x_{k-1})}^{y^{*}(x_{k})}\mathcal{H}_{y}^{-1}g(x_{k-1},y^{*}(x_{k-1}))\Gamma_{y^ {*}(x_{k})}^{y^{*}(x_{k-1})}\|_{y^{*}(x_{k})}\] \[\quad+\frac{1}{\mu}\|\Gamma_{y^{*}(x_{k})}^{y^{*}(x_{k-1})} \mathcal{G}_{y}f(x_{k},y^{*}(x_{k}))-\mathcal{G}_{y}f(x_{k},y^{*}(x_{k-1}))\|_{y^ {*}(x_{k-1})}\] \[\quad+\frac{1}{\mu}\|\mathcal{G}_{y}f(x_{k},y^{*}(x_{k-1}))- \mathcal{G}_{y}f(x_{k-1},y^{*}(x_{k-1}))\|_{y^{*}(x_{k-1})}\] \[\leq\frac{M\kappa_{\rho}}{\mu}d(x_{k},x_{k-1})+\frac{M\kappa_{ \rho}}{\mu}\kappa_{l}d(x_{k},x_{k-1})+\frac{L}{\mu}\kappa_{l}d(x_{k},x_{k-1})+ \frac{L}{\mu}d(x_{k},x_{k-1})\] \[=\eta_{x}C_{v}\|\widehat{\mathcal{G}}_{\rm cg}F(x_{k-1})- \mathcal{G}F(x_{k-1})\|_{x_{k-1}}+\eta_{x}C_{v}\|\mathcal{G}F(x_{k-1})\|_{x_{k-1}}\] (13)

where we let \(C_{v}\coloneqq\frac{M\kappa_{\rho}}{\mu}+\frac{M\kappa_{\rho}\kappa_{l}}{\mu}+ \kappa_{l}^{2}+\kappa_{l}\). Combining (13) and (12), we obtain

\[\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}}^{2}\] \[\leq 20\kappa_{l}\big{(}\frac{\sqrt{\kappa_{l}}-1}{\sqrt{\kappa_{l} }+1}\big{)}^{2T}\|\hat{v}_{k-1}^{0}

[MISSING_PAGE_EMPTY:25]

Finally, telescoping the inequality, we obtain

\[\frac{1}{K}\sum_{k=0}^{K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}\leq\frac{96\Lambda R_{ 0}}{K}=\frac{96\Lambda}{K}\Big{(}F(x_{k})+d^{2}(y_{0},y^{*}(x_{0}))+\|v_{0}^{*} \|_{y^{*}(x_{0})}^{2}\Big{)},\]

where we use the fact that \(\hat{v}_{k}^{0}=0\) and the isometry property of parallel transport.

_3. Truncated Neumann series_: Let \(C_{\rm ns}:=L+\kappa_{l}L+\kappa_{\rho}M+\kappa_{l}\kappa_{\rho}M\). Here we notice that \(C_{\rm ns}=C_{\rm hinv}\). Then by Lemma 1, we see

\[\|\widehat{\mathcal{G}}_{\rm ns}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq 2 C_{\rm ns}^{2}(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y^{*}(x_{k}),y _{k})+2\kappa_{l}^{2}M^{2}(1-\gamma\mu)^{2T}.\] (15)

Similar in the previous analysis,

\[d^{2}(y_{k+1},y^{*}(x_{k+1})) \leq 2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y^{*}(x_{k} ),y_{k})+4\eta_{x}^{2}\kappa_{l}^{2}\|\widehat{\mathcal{G}}_{\rm ns}F(x_{k})- \mathcal{G}F(x_{k})\|_{x_{k}}^{2}\] \[\quad+4\eta_{x}^{2}\kappa_{l}^{2}\|\mathcal{G}F(x_{k})\|_{x_{k}}^ {2}\] (16)

Let the Lyapunov function be \(R_{k}:=F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))\). Then

\[R_{k+1}-R_{k}\] \[\leq-\big{(}\frac{\eta_{x}}{2}-\eta_{x}^{2}L_{F}-4\eta_{x}^{2} \kappa_{l}^{2}\big{)}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\big{(}\frac{\eta_{x} }{2}+\eta_{x}^{2}L_{F}+4\eta_{x}^{2}\kappa_{l}^{2}\big{)}\|\mathcal{G}F(x_{k} )-\widehat{\mathcal{G}}_{\rm ns}F(x_{k})\|_{x_{k}}^{2}\] \[\quad+\big{(}2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}-1\big{)} d^{2}(y_{k},y^{*}(x_{k}))\] \[\leq-\frac{1}{80L_{F}}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\big{(} \big{(}2+\frac{3C_{\rm ns}^{2}}{40L_{F}}\big{)}(1+\eta_{y}^{2}\zeta L^{2}- \eta_{y}\mu)^{S}-1\big{)}d^{2}(y_{k},y^{*}(x_{k}))\] \[\quad+\frac{3}{40L_{F}}\kappa_{l}^{2}M^{2}(1-\gamma\mu)^{2T}\]

where we set \(\eta_{x}=\frac{1}{20L_{F}}\) and apply (15) in the second inequality.

Now setting \(S\geq\log(\frac{40L_{F}}{80L_{F}+3C_{\rm ns}^{2}})/\log(1+\eta_{y}^{2}\zeta L ^{2}-\eta_{y}\mu)=\widetilde{\Theta}(\kappa_{l}^{2}\zeta)\) and telescoping the results yields

\[\frac{1}{K}\sum_{k=0}^{K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{80L_{F} R_{0}}{K}+6\kappa_{l}^{2}M^{2}(1-\gamma\mu)^{2T}\leq\frac{80L_{F}R_{0}}{K}+ \frac{\epsilon}{2}\]

where we set \(T\geq-\frac{1}{2}\log(\frac{12\kappa_{l}^{2}M^{2}}{\epsilon})/\log(1-\gamma\mu )=\widetilde{\Theta}(\kappa\log(\frac{1}{\epsilon}))\).

_4. Automatic differentiation_: Let \(C_{\rm ad}:=\frac{2M\widetilde{C}}{\mu-\eta_{y}\zeta L^{2}}+L(1+\kappa_{l})\). Then

\[\|\mathcal{G}F(x_{k})-\widehat{\mathcal{G}}_{\rm ad}F(x_{k})\|_{x_{k}}^{2}\leq 2 C_{\rm ad}^{2}(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S-1}d^{2}(y_{k},y^{*}(x _{k}))+2M^{2}\kappa_{l}^{2}(1-\eta_{y}\mu)^{2S}\]

and similarly

\[d^{2}(y_{k+1},y^{*}(x_{k+1})) \leq 2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y^{*}(x_{k} ),y_{k})+4\eta_{x}^{2}\kappa_{l}^{2}\|\widehat{\mathcal{G}}_{\rm ad}F(x_{k})- \mathcal{G}F(x_{k})\|_{x_{k}}^{2}\] \[\quad+4\eta_{x}^{2}\kappa_{l}^{2}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\]

Let the Lyapunov function be \(R_{k}:=F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))\). Then

\[R_{k+1}-R_{k} \leq-\big{(}\frac{\eta_{x}}{2}-\eta_{x}^{2}L_{F}-4\eta_{x}^{2} \kappa_{l}^{2}\big{)}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\big{(}\frac{\eta_{x} }{2}+\eta_{x}^{2}L_{F}+4\eta_{x}^{2}\kappa_{l}^{2}\big{)}\|\mathcal{G}F(x_{k} )-\widehat{\mathcal{G}}_{\rm ad}F(x_{k})\|_{x_{k}}^{2}\] \[\quad+\Big{(}\big{(}2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S-1}-1\big{)}d^{2}(y_{k},y^{*}(x_{k})) \Big{)}\] \[\leq-\frac{1}{80L_{F}}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\Big{(} \big{(}(2+\frac{3C_{\rm ad}^{2}}{40L_{F}})(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y} \mu)^{S-1}-1\big{)}d^{2}(y_{k},y^{*}(x_{k}))\] \[\quad+\frac{3}{40L_{F}}M^{2}\kappa_{l}^{2}(1-\eta_{y}\mu)^{2S}\] \[\leq-\frac{1}{80L_{F}}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\frac{3} {40L_{F}}M^{2}\kappa_{l}^{2}(1-\eta_{y}\mu)^{2S}\]where we set \(\eta_{x}=\frac{1}{20L_{F}}\) and choose \(S\geq\log\frac{40L_{F}}{80L_{F}+3C_{ad}^{2}}/\log(1+\eta_{y}^{2}\zeta L^{2}-\eta_ {y}\mu)+1=\widetilde{\Theta}(\kappa_{l}^{2}\zeta)\). Telescoping the result gives

\[\frac{1}{K}\sum_{k=0}^{K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{80L_{F} R_{0}}{K}+6M^{2}\kappa_{l}^{2}(1-\eta_{y}\mu)^{2S}\leq\frac{80L_{F}R_{0}}{K}+ \frac{\epsilon}{2},\]

by choosing \(S\geq\frac{1}{2}\log(\frac{\epsilon}{12M^{2}\kappa_{l}^{2}})\log(1-\eta_{y} \mu)=\widetilde{\Theta}(\kappa_{l}^{2}\zeta\log(\frac{1}{\epsilon}))\). Hence we set \(S\geq\widetilde{\Theta}(\kappa_{l}^{2}\zeta\log(\frac{1}{\epsilon}))\) for both conditions to hold. 

### Proof of Corollary 1

The computational cost of gradient and Hessian for each method for approximating the hypergradient are as follows.

**Corollary 1**.: _The complexities of reaching an \(\epsilon\)-stationary solution are_

* _Hessian inverse:_ \(G_{f}=O(\kappa_{l}^{3}\epsilon^{-1})\)_,_ \(G_{g}=\widetilde{O}(\kappa_{l}^{5}\zeta\epsilon^{-1})\)_,_ \(JV_{g}=O(\kappa_{l}^{3}\epsilon^{-1})\)_,_ \(HV_{g}=NA\)_._
* _Conjugate gradient:_ \(G_{f}=O(\kappa_{l}^{4}\epsilon^{-1})\)_,_ \(G_{g}=\widetilde{O}(\kappa_{l}^{6}\zeta\epsilon^{-1})\)_,_ \(JV_{g}=O(\kappa_{l}^{4}\epsilon^{-1})\)_,_ \(HV_{g}=\widetilde{O}(\kappa_{l}^{4.5}\epsilon^{-1})\)_._
* _Truncated Neumann series:_ \(G_{f}=O(\kappa_{l}^{3}\epsilon^{-1}),G_{g}=\widetilde{O}(\kappa_{l}^{5}\zeta \epsilon^{-1})\)_,_ \(JV_{g}=O(\kappa_{l}^{3}\epsilon^{-1})\)_,_ \(HV_{g}=\widetilde{O}(\kappa_{l}^{4}\epsilon^{-1}\log(\epsilon^{-1}))\)_._
* _Automatic differentiation:_ \(G_{f}=O(\kappa_{l}^{3}\epsilon^{-1}),G_{g}=\widetilde{O}(\kappa_{l}^{5}\zeta \epsilon^{-1}\log(\epsilon^{-1})),JV_{g}=\widetilde{O}(\kappa_{l}^{5}\zeta \epsilon^{-1}\log(\epsilon^{-1}))\)_,_ \(HV_{g}=\widetilde{O}(\kappa_{l}^{5}\zeta\epsilon^{-1}\log(\epsilon^{-1}))\)_._

Proof of Corollary 1.: From the convergence established in Theorem 1, we see the iterations in order to reach \(\epsilon\)-stationary solution are given by

* (Hessian inverse) \(K=O(L_{F}\epsilon^{-1})=O(\kappa_{l}^{3}\epsilon^{-1})\), \(S=\widetilde{O}(\kappa_{l}^{2}\zeta)\).
* (Conjugate gradient) \(K=O(\Lambda\epsilon^{-1})=O(\kappa_{l}^{4}\epsilon^{-1})\), \(S=\widetilde{O}(\kappa_{l}^{2}\zeta),T=\widetilde{O}(\sqrt{\kappa_{l}})\).
* (Truncated Neumann series) \(K=O(\kappa_{l}^{3}\epsilon^{-1})\), \(S=\widetilde{O}(\kappa_{l}^{2}\zeta),T=\widetilde{O}(\kappa_{l}\log(\epsilon^{ -1}))\).
* (Automatic differentiation) \(K=O(\kappa_{l}^{3}\epsilon^{-1})\), \(S=\widetilde{O}\big{(}\kappa_{l}^{2}\zeta\log(\epsilon^{-1})\big{)}\).

Then based on Algorithm 1, the gradient complexities are \(G_{f}=2K\) and \(G_{g}=KS\) and cross-derivative and Hessian product complexities are \(JV_{g}=K,HV_{g}=KT\) for CG and NS and \(JV_{g}=KS\), \(HV_{g}=KS\) for AD (which we approximate based on the analysis in Lemma 1). We notice here for the Hessian inverse, because we do not compute Hessian vector product, we write NA for Hessian vector product based on the Neumann series. This completes the proof. 

## Appendix D Proofs for Section 3.3

We first show Lemma 4 holds for each \(f_{i}(x,y),g_{i}(x,y)\). Further, the variance of the estimate can be bounded as follows. We here use \([\cdot]\) to denote all possible derivatives, including \(x,y,xy,yx\).

**Lemma 6**.: _Under Assumption 4, we have for any \(x,y\in\mathcal{U}\), (1) \(\mathbb{E}\|\mathcal{G}_{[\cdot]}f_{i}(x,y)-\mathcal{G}_{[\cdot]}f(x,y)\|_{[ \cdot]}^{2}\leq M^{2}\). (2) \(\mathbb{E}\|\mathcal{G}_{[\cdot]}^{2}g_{i}(x,y)-\mathcal{G}_{[\cdot]}^{2}g(x,y )\|_{[\cdot]}^{2}\leq L^{2}\). (3) \(\mathbb{E}\|\mathcal{H}_{y}^{-1}g_{i}(x,y)-\mathcal{H}_{y}^{-1}g(x,y)\|_{y}^{2} \leq\mu^{-2}\)._

For notation, denote the filtration \(\mathcal{F}_{k}\coloneqq\{y_{0},x_{0},y_{1},x_{1},...,x_{k},y_{k+1}\}\) and here we let \(\mathbb{E}_{k}\coloneqq\mathbb{E}[\cdot|\mathcal{F}_{k}]\). With a slight abuse of notation, we further consider \(\mathcal{F}_{k}^{s}\coloneqq\{y_{0},x_{0},y_{1},x_{1},...,y_{k},y_{k}^{1},..., y_{k}^{s}\}\) and correspondingly let \(\mathbb{E}_{k}^{s}\coloneqq\mathbb{E}[\cdot|\mathcal{F}_{k}^{s}]\).

**Lemma 7** (Convergence under strong convexity and stochastic setting).: _Under stochastic setting and under the Assumption that \(g\) is geodesic strongly convex, we can show \(\mathbb{E}_{k}^{s}d^{2}(y_{k}^{s+1},y^{*}(x_{k}))\leq(1+\eta_{y}^{2}\zeta L^{2}- \eta_{y}\mu)d^{2}(y_{k}^{s},y^{*}(x_{k}))+\frac{\eta_{y}^{2}\zeta M^{2}}{| \mathcal{B}_{1}|}\) and \(\mathbb{E}_{k-1}d^{2}(y_{k+1},y^{*}(x_{k}))\leq(1+\eta_{y}^{2}\zeta L^{2}- \eta_{y}\mu)^{S}d^{2}(y_{k},y^{*}(x_{k}))+\frac{\eta_{y}\zeta M^{2}}{\mu-\eta_ {y}\zeta L^{2}}\frac{1}{|\mathcal{B}_{1}|}\)._

**Lemma 8**.: _Under Assumption 4, we can bound \(\mathbb{E}_{k}\|\widehat{\mathcal{G}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}^{2} \leq\frac{4M^{2}+16M^{2}\kappa_{k}^{2}}{|\mathcal{B}_{2}|}+\frac{8M^{2}\kappa _{k}^{2}}{|\mathcal{B}_{3}|}+\frac{16M^{2}\kappa_{k}^{2}}{|\mathcal{B}_{4}|}+ 2C_{\mathrm{hinv}}^{2}d^{2}(y_{k+1},y^{*}(x_{k}))\)._

### Proofs for the lemmas

Proof of Lemma 6.: Here we only prove one and the rest follows exactly. Due to the unbiasedness of the stochastic estimate, we have

\[\mathbb{E}\|\mathcal{G}_{x}f_{i}(x,y)-\mathcal{G}f(x,y)\|_{x}^{2}=\mathbb{E} \|\mathcal{G}_{x}f_{i}(x,y)\|_{x}^{2}-\|\mathcal{G}_{x}f(x,y)\|_{x}^{2}\leq \mathbb{E}\|\mathcal{G}_{x}f_{i}(x,y)\|_{x}^{2}\leq M^{2}\]

where we use Assumption 4. 

Proof of Lemma 7.: Similarly from the proof of Lemma 5, we take expectation over \(\mathcal{F}_{k}^{s}\)

\[\mathbb{E}_{k}^{s}d^{2}(y_{k}^{s+1},y^{*}(x_{k}))\] \[\leq\mathbb{E}_{k}^{s}\big{[}d^{2}(y_{k}^{s},y^{*}(x_{k}))+\eta_{ y}^{2}\mathbb{E}_{k}^{s}\|\mathcal{G}_{y}g_{\mathcal{B}_{1}}(x_{k},y_{k}^{s}) \|_{y_{k}^{s}}^{2}+2\eta_{y}\langle\mathcal{G}_{y}g_{\mathcal{B}_{1}}(x_{k},y_ {k}^{s}),\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\rangle_{y_{k}^{s}}\big{]}\] \[\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\eta_{y}^{2}\langle\mathbb{E}_ {k}^{s}\|\mathcal{G}_{y}g_{\mathcal{B}_{1}}(x_{k},y_{k}^{s})-\mathcal{G}_{y}g (x_{k},y_{k}^{s})\|_{y_{k}^{s}}^{2}+\eta_{y}^{2}\zeta\|\mathcal{G}_{y}g(x_{k}, y_{k}^{s})\|_{y_{k}^{s}}^{2}\] \[\quad+2\eta_{y}\langle\mathcal{G}_{y}g(x_{k},y_{k}^{s}),\mathrm{ Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\rangle_{y_{k}^{s}}\] \[\leq(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)d^{2}(y_{k}^{s},y^{*}(x _{k}))+\eta_{y}^{2}\zeta\mathbb{E}_{k}^{s}\frac{1}{|\mathcal{B}_{1}|^{2}}\sum _{i\in\mathcal{B}_{1}}\mathbb{E}_{k}^{s}\|\mathcal{G}_{y}g_{i}(x_{k},y_{k}^{s })-\mathcal{G}_{y}g(x_{k},y_{k}^{s})\|_{y_{k}^{s}}^{2}\] \[\leq(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)d^{2}(y_{k}^{s},y^{*}( x_{k}))+\frac{\eta_{y}^{2}\zeta M^{2}}{|\mathcal{B}_{1}|},\]

where we use the strong convexity and the fact that \(\mathbb{E}\|\mathcal{G}_{y}g_{\mathcal{B}_{1}}(x,y)-\mathcal{G}_{y}g(x,y)\|_{y }^{2}=\frac{1}{|\mathcal{B}_{1}|^{2}}\mathbb{E}\|\sum_{i\in\mathcal{B}_{1}}( \mathcal{G}_{y}g_{i}(x,y)-\mathcal{G}_{y}g(x,y))\|_{y}^{2}=\frac{1}{|\mathcal{B }_{1}|^{2}}\sum_{i\in\mathcal{B}_{1}}\mathbb{E}\|\mathcal{G}_{y}g_{i}(x,y)- \mathcal{G}_{y}g(x,y)\|_{y}^{2}\) in the third inequality and Lemma 6 in the last inequality. Further, we telescope the inequality and taking the expectation \(\mathbb{E}_{k-1}\) gets

\[\mathbb{E}_{k-1}d^{2}(y_{k}^{S},y^{*}(x_{k})) \leq(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y_{k},y^{*}( x_{k}))+\frac{\eta_{y}^{2}\zeta M^{2}}{|\mathcal{B}_{1}|}\sum_{s=0}^{S-1}(1+\eta_{y}^{2} \zeta L^{2}-\eta_{y}\mu)^{s}\] \[\leq(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}d^{2}(y_{k},y^{*}( x_{k}))+\frac{\eta_{y}\zeta M^{2}}{\mu-\eta_{y}\zeta L^{2}}\frac{1}{|\mathcal{B}_{1}|},\]

where we use the fact that \(\sum_{s=0}^{S-1}\theta^{s}\leq\frac{1}{1-\theta}\) for \(0<\theta<1\). 

Proof of Lemma 8.: Recall that \(\mathcal{G}F(x_{k})=\mathcal{G}_{x}f(x,y^{*}(x))-\mathcal{G}_{xy}^{2}g(x,y^{*} (x))\big{[}\mathcal{H}_{y}^{-1}g(x,y^{*}(x))\big{]}\mathcal{G}_{y}f(x,y^{*}(x) )\big{]}\). Then

\[\mathbb{E}_{k}\|\widehat{\mathcal{G}}F(x_{k})-\mathcal{G}F(x_{k})\| _{x_{k}}^{2}\] \[\leq 2\mathbb{E}_{k}\|\widehat{\mathcal{G}}F(x_{k})-\widehat{ \mathcal{G}}_{\mathrm{hinv}}F(x_{k})\|_{x_{k}}^{2}+2\|\widehat{\mathcal{G}}_{ \mathrm{hinv}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\] \[\leq 2\mathbb{E}_{k}\|\widehat{\mathcal{G}}F(x_{k})-\widehat{ \mathcal{G}}_{\mathrm{hinv}}F(x_{k})\|_{x_{k}}^{2}+2C_{\mathrm{hinv}}^{2}d^{2}(y_{k+ 1},y^{*}(x_{k}))\] (17)

where the second inequality uses Lemma 1. Now we bound the first term \(\mathbb{E}_{k}\|\widehat{\mathcal{G}}F(x_{k})-\widehat{\mathcal{G}}_{\mathrm{hinv}}F( x_{k})\|_{x_{k}}^{2}\) as follows.

First we bound

\[\mathbb{E}_{k}\|\mathcal{H}_{y}^{-1}g_{\mathcal{B}_{4}}(x_{k},y_{k+1})[\mathcal{G}_ {y}f_{\mathcal{B}_{2}}(x_{k},y_{k+1})]-\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})[ \mathcal{G}_{y}f(x_{k},y_{k+1})]\|_{y_{k+1}}^{2}\]\[\leq 2\mathbb{E}_{k}\|\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})- \mathcal{G}_{y}f(x_{k},y_{k+1})-\mathcal{G}_{y}f(x_{k},y_{k+1})\|_{y_{k+1}}^{2}\] \[\quad+2\mathbb{E}_{k}\|(\mathcal{H}_{y}^{-1}g_{\mathcal{B}_{4}}(x_ {k},y_{k+1})-\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1}))[\mathcal{G}_{y}f_{\mathcal{ B}_{2}}(x_{k},y_{k+1})]\|_{y_{k+1}}^{2}\] \[\leq 2\|\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})\|_{y_{k+1}}^{2} \mathbb{E}_{k}\|\mathcal{G}_{y}f_{\mathcal{B}_{2}}(x_{k},y_{k+1})-\mathcal{G}_ {y}f(x_{k},y_{k+1})\|_{y_{k+1}}^{2}\] \[\quad+2\mathbb{E}_{k}\|\mathcal{H}_{y}^{-1}g_{\mathcal{B}_{4}}(x_ {k},y_{k+1})-\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})\|_{y_{k+1}}^{2}\mathbb{E}_{k }\|\mathcal{G}_{y}f_{\mathcal{B}_{2}}(x_{k},y_{k+1})\|_{y_{k+1}}^{2}\] \[\leq\frac{2M^{2}}{\mu^{2}}\big{(}\frac{1}{|\mathcal{B}_{2}|}+ \frac{1}{|\mathcal{B}_{4}|}\big{)}\]

where we notice that \(\|\mathcal{G}_{y}f_{\mathcal{B}_{2}}(x_{k},y_{k+1})\|_{y_{k+1}}\leq\frac{1}{| \mathcal{B}_{2}|}\sum_{i\in\mathcal{B}_{2}}\|\mathcal{G}_{y}f_{i}(x_{k},y_{k+1 })\|_{y_{k+1}}\leq M\).

Hence, we can bound

\[\mathbb{E}_{k}\|\widehat{\mathcal{G}}F(x_{k})-\widehat{\mathcal{G }}_{\rm{hinv}}F(x_{k})\|_{x_{k}}^{2}\] \[\leq 2\mathbb{E}_{k}\|\mathcal{G}_{x}f_{\mathcal{B}_{2}}(x_{k},y_{k +1})-\mathcal{G}_{x}f(x_{k},y_{k+1})\|_{x_{k}}^{2}+\frac{8M^{2}}{\mu^{2}} \big{(}\frac{1}{|\mathcal{B}_{2}|}+\frac{1}{|\mathcal{B}_{4}|}\big{)}\mathbb{ E}_{k}\|\mathcal{G}_{xy}^{2}g_{\mathcal{B}_{3}}(x_{k},y_{k+1})\|_{x_{k}}^{2}\] \[\quad+4\mathbb{E}_{k}\|\mathcal{G}_{xy}^{2}g(x_{k},y_{k+1})- \mathcal{G}_{xy}^{2}g_{\mathcal{B}_{3}}(x_{k},y_{k+1})\|_{x_{k}}^{2}\|\mathcal{ H}_{y}^{-1}g(x_{k},y_{k+1})|\mathcal{G}_{y}f(x_{k},y_{k+1})\|_{y_{k+1}}^{2}\] \[\leq\frac{2M^{2}}{|\mathcal{B}_{2}|}+8M^{2}\kappa_{l}^{2}\big{(} \frac{1}{|\mathcal{B}_{2}|}+\frac{1}{|\mathcal{B}_{4}|}\big{)}+\frac{4M^{2} \kappa_{l}^{2}}{|\mathcal{B}_{3}|},\] (18)

where we use Lemma 6 in the last inequality. Combining (18) with (17) yields the desired result. 

### Proof of Theorem 2

Proof of Theorem 2.: From the smoothness of \(F(x)\) (i.e., (7)) and taking full expectation we obtain,

\[\mathbb{E}[F(x_{k+1})-F(x_{k})]\leq-\big{(}\frac{\eta_{x}}{2}- \eta_{x}^{2}L_{F}\big{)}\mathbb{E}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\big{(} \frac{\eta_{x}}{2}+\eta_{x}^{2}L_{F}\big{)}\mathbb{E}\|\mathcal{G}F(x_{k})- \widehat{\mathcal{G}}F(x_{k})\|_{x_{k}}^{2}.\]

Further, we can bound

\[\mathbb{E}d^{2}(y_{k+1},y^{*}(x_{k+1}))\] \[\leq 2\mathbb{E}d^{2}(y_{k+1},y^{*}(x_{k}))+4\eta_{x}^{2}\kappa_{l }^{2}\mathbb{E}\|\widehat{\mathcal{G}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}^{ 2}+4\eta_{x}^{2}\kappa_{l}^{2}\mathbb{E}\|\mathcal{G}F(x_{k})\|_{x_{k}}\] \[\leq 2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}\mathbb{E}d^{2}(y_ {k},y^{*}(x_{k}))+\frac{2\eta_{y}\zeta M^{2}}{\mu-\eta_{y}\zeta L^{2}}\frac{1} {|\mathcal{B}_{1}|}+4\eta_{x}^{2}\kappa_{l}^{2}\mathbb{E}\|\mathcal{G}F(x_{k} )\|_{x_{k}}\] \[\quad+4\eta_{x}^{2}\kappa_{l}^{2}\mathbb{E}\|\widehat{\mathcal{G} }F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\]

where we use Lemma 7 and 8 in the second inequality.

Next, we construct a Lyapunov function as \(R_{k}\coloneqq F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))\). Then

\[\mathbb{E}[R_{k+1}-R_{k}]\] \[\leq\mathbb{E}[F(x_{k+1})-F(x_{k})]+\mathbb{E}[d^{2}(y_{k+1},y^{* }(x_{k+1})-d^{2}(y_{k},y^{*}(x_{k}))]\] \[\leq-\big{(}\frac{\eta_{x}}{2}-\eta_{x}^{2}L_{F}-4\eta_{x}^{2} \kappa_{l}^{2}\big{)}\mathbb{E}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\big{(} \frac{\eta_{x}}{2}+\eta_{x}^{2}L_{F}+4\eta_{x}^{2}\kappa_{l}^{2}\big{)}\mathbb{ E}\|\mathcal{G}F(x_{k})-\widehat{\mathcal{G}}F(x_{k})\|_{x_{k}}^{2}\] \[\quad+\Big{(}\big{(}2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}- 1\big{)}\mathbb{E}d^{2}(y_{k},y^{*}(x_{k}))+\frac{2\eta_{y}\zeta M^{2}}{\mu- \eta_{y}\zeta L^{2}}\frac{1}{|\mathcal{B}_{1}|}\Big{)}\] \[=-\frac{1}{80L_{F}}\mathbb{E}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+ \frac{3}{80L_{F}}\mathbb{E}[\mathbb{E}_{k}\|\mathcal{G}F(x_{k})-\widehat{ \mathcal{G}}F(x_{k})\|_{x_{k}}^{2}]\] \[\quad+\Big{(}\big{(}2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}- 1\big{)}\mathbb{E}d^{2}(y_{k},y^{*}(x_{k}))+\frac{2\eta_{y}\zeta M^{2}}{\mu- \eta_{y}\zeta L^{2}}\frac{1}{|\mathcal{B}_{1}|}\Big{)}\] \[\leq-\frac{1}{80L_{F}}\mathbb{E}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+ \frac{3}{80L_{F}}\Big{(}\frac{4M^{2}+16M^{2}\kappa_{l}^{2}}{|\mathcal{B}_{2}|}+ \frac{8M^{2}\kappa_{l}^{2}}{|\mathcal{B}_{3}|}+\frac{16M^{2}\kappa_{l}^{2}}{| \mathcal{B}_{4}|}\Big{)}+\frac{3C_{\rm{hinv}}^{2}}{40L_{F}}\mathbb{E}d^{2}(y_{k +1},y^{*}(x_{k}))\] \[\quad+\Big{(}\big{(}2(1+\eta_{y}^{2}\zeta L^{2}-\eta_{y}\mu)^{S}- 1\big{)}\mathbb{E}d^{2}(y_{k},y^{*}(x_{k}))+\frac{2\eta_{y}\zeta M^{2}}{\mu- \eta_{y}\zeta L^{2}}\frac{1}{|\mathcal{B}_{1}|}\Big{)}\] \[\leq-\frac{1}{80L_{F}}\mathbb{E}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+ \Big{(}(2+\frac{3C_{\rm{hinv}}^{2}}{40L_{F}})(1+\eta_{y}^{2}\zeta L^{\[+\frac{3}{80L_{F}}\Big{(}\frac{4M^{2}+16M^{2}\kappa_{l}^{2}}{|\mathcal{ B}_{2}|}+\frac{8M^{2}\kappa_{l}^{2}}{|\mathcal{B}_{3}|}+\frac{16M^{2}\kappa_{l}^{2}}{ |\mathcal{B}_{4}|}\Big{)}+\big{(}\frac{3C_{\rm{hinv}}^{2}}{40L_{F}}+2\big{)} \frac{\eta_{y}\zeta M^{2}}{\mu-\eta_{y}\zeta L^{2}}\frac{1}{|\mathcal{B}_{1}|}\] \[\leq-\frac{1}{80L_{F}}\mathbb{E}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2 }+\frac{3}{80L_{F}}\Big{(}\frac{4M^{2}+16M^{2}\kappa_{l}^{2}}{|\mathcal{B}_{2} |}+\frac{8M^{2}\kappa_{l}^{2}}{|\mathcal{B}_{3}|}+\frac{16M^{2}\kappa_{l}^{2}} {|\mathcal{B}_{4}|}\Big{)}+\big{(}\frac{3C_{\rm{hinv}}^{2}}{40L_{F}}+2\big{)} \frac{\eta_{y}\zeta M^{2}}{\mu-\eta_{y}\zeta L^{2}}\frac{1}{|\mathcal{B}_{1}|}\]

where we choose \(\eta_{x}=\frac{1}{20L_{F}}\) in the first equality and \(S\geq\log(\frac{40L_{F}}{80L_{F}+3C_{\rm{hinv}}})/\log(1+\eta_{y}^{2}\zeta L^{ 2}-\eta_{y}\mu)=\widetilde{\Theta}(\kappa_{l}^{2}\zeta)\) for the last inequality. Telescoping the result gives

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\|\mathcal{G}F(x_{k})\|_{x_{k }}^{2} \leq\frac{80L_{F}R_{0}}{K}+\Big{(}\frac{12M^{2}+48M^{2}\kappa_{l}^{ 2}}{|\mathcal{B}_{2}|}+\frac{24M^{2}\kappa_{l}^{2}}{|\mathcal{B}_{3}|}+\frac{ 48M^{2}\kappa_{l}^{2}}{|\mathcal{B}_{4}|}\Big{)}\] \[\quad+(6C_{\rm{hinv}}^{2}+160L_{F})\frac{\eta_{y}\zeta M^{2}}{\mu -\eta_{y}\zeta L^{2}}\frac{1}{|\mathcal{B}_{1}|}\] \[\leq\frac{80L_{F}R_{0}}{K}+\frac{\epsilon}{2},\]

where the last inequality follows from the choice that \(|\mathcal{B}_{1}|\geq(24C_{\rm{hinv}}^{2}+640L_{F})\frac{8\eta_{y}\zeta M^{2} }{\mu-\eta_{y}\zeta L^{2}}/\epsilon=\Theta(\kappa_{l}^{4}/\epsilon)\), \(|\mathcal{B}_{2}|\geq\frac{144M^{2}+576M^{2}\kappa_{l}^{2}}{\epsilon}=\Theta( \kappa_{l}^{2}/\epsilon)\), \(|\mathcal{B}_{3}|\geq\frac{288M^{2}\kappa_{l}^{2}}{\epsilon}=\Theta(\kappa_{l} ^{2}/\epsilon)\), \(|\mathcal{B}_{4}|\geq\frac{576M^{2}\kappa_{l}^{2}}{\epsilon}=\Theta(\kappa_{l} ^{2}/\epsilon)\) in the last inequality.

In order to reach \(\epsilon\)-stationary solution, we require \(K=O(\kappa_{l}^{3}\epsilon^{-1})\) and thus the (stochastic) gradient complexity for \(f\) is \(G_{f}=2K|\mathcal{B}_{2}|=O(\kappa_{l}^{5}\epsilon^{-2})\) and for \(g\) is \(G_{g}=KS|\mathcal{B}_{1}|=\widetilde{O}(\kappa_{l}^{9}\zeta\epsilon^{-2})\). The complexity for cross-derivative is \(K|\mathcal{B}_{3}|=O(\kappa_{l}^{5}\epsilon^{-2})\). 

## Appendix E Proofs for Section 3.4

Proof of Theorem 3.: We first give a complete proof for the Hessian inverse estimator as follows. For the other estimators, we only provide a proof sketch.

Proof for HIN.(1) First, we derive the convergence under strong convexity using retraction. By the trigonometric distance bound

\[d^{2}(y_{k}^{s+1},y^{s}(x_{k}))\] \[\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\zeta d^{2}(y_{k}^{s},y_{k}^{s +1})-2\langle\mathrm{Exp}_{y_{k}^{s}}^{-1}y_{k}^{s+1},\mathrm{Exp}_{y_{k}^{s}} ^{-1}y^{*}(x_{k})\rangle_{y_{k}^{s}}\] \[\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\eta_{y}^{2}\zeta\|\mathcal{G }_{y}g(x_{k},y_{k}^{s})\|_{y_{k}^{s}}^{2}-2\langle\mathrm{Exp}_{y_{k}^{s}}^{-1}y _{k}^{s+1}-\mathrm{Retr}_{y_{k}^{s}}^{-1}y_{k}^{s+1},\mathrm{Exp}_{y_{k}^{s}} ^{-1}y^{*}(x_{k})\rangle_{y_{k}^{s}}\] \[\quad+2\eta_{y}\langle\mathcal{G}_{y}g(x_{k},y_{k}^{s}),\mathrm{Exp }_{y_{k}^{s}}^{-1}y^{*}(x_{k})\rangle_{y_{k}^{s}}\] \[\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\eta_{y}^{2}\zeta\|\mathcal{G }_{y}g(x_{k},y_{k}^{s})\|_{y_{k}^{s}}^{2}+2\eta_{y}\langle\mathcal{G}_{y}g(x_{k },y_{k}^{s}),\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\rangle_{y_{k}^{s}}\] \[\quad+2\bar{D}\|\mathrm{Exp}_{y_{k}^{s}}^{-1}y_{k}^{s+1}-\mathrm{ Retr}_{y_{k}^{s}}^{-1}y_{k}^{s+1}\|_{y_{k}^{s}}\] \[\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\eta_{y}^{2}(\zeta\bar{c}+2 \bar{D}c_{R})\|\mathcal{G}_{y}g(x_{k},y_{k}^{s})\|_{y_{k}^{s}}^{2}+2\eta_{y} \langle\mathcal{G}_{y}g(x_{k},y_{k}^{s}),\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{ k})\rangle_{y_{k}^{s}}\] \[\leq\big{(}1+\eta_{y}^{2}(\zeta\bar{c}+2\bar{D}c_{R})L^{2}-\mu\eta _{y}\big{)}d^{2}(y_{k}^{s},y^{*}(x_{k})\big{)}\]

where we use Assumption 5 in the second inequality and fourth inequality. We require \(\eta_{y}<\frac{\mu}{(\zeta\bar{c}+2\bar{D}c_{R})L^{2}}\) in order to achieve linear convergence. For simplicity, we let \(\tau=\mu\eta_{y}-\eta_{y}^{2}(\zeta\bar{c}+2\bar{D}c_{R})\). This leads to \(d^{2}(y_{k}^{s+1},y^{*}(x_{k}))\leq(1-\tau)d^{2}(y_{k}^{s},y^{*}(x_{k}))\).

(2) Next, we notice the bound on hypergradient approximation error still holds as \(\|\widehat{\mathcal{G}}_{\rm{hinv}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}\leq C_{ \rm{hinv}}d\big{(}y^{*}(x_{k}),y_{k+1}\big{)}\), where \(C_{\rm{hinv}}=L+\kappa_{\rho}M+\kappa_{l}L+\kappa_{l}\kappa_{\rho}M\). Further, by \(L\)-smoothness,

\[F(x_{k+1})-F(x_{k})\] \[\leq\langle\mathcal{G}F(x_{k}),\mathrm{Exp}_{x_{k}}^{-1}x_{k+1} \rangle_{x_{k}}+\frac{L_{F}}{2}d^{2}(x_{k},x_{k+1})\]\[\leq\langle\mathcal{G}F(x_{k}),\mathrm{Exp}_{x_{k}}^{-1}x_{k+1}- \mathrm{Retr}_{x_{k}}^{-1}x_{k+1}\rangle_{x_{k}}-\eta_{x}\langle\mathcal{G}F(x_{k }),\widehat{\mathcal{G}}F(x_{k})\rangle_{x_{k}}+\frac{\bar{c}L_{F}\eta_{x}^{2}}{ 2}\|\widehat{\mathcal{G}}F(x_{k})\|_{x_{k}}^{2}\] \[\leq\big{(}2\kappa_{l}Mc_{R}+\frac{\bar{c}L_{F}}{2}\big{)}\eta_{x} ^{2}\|\widehat{\mathcal{G}}F(x_{k})\|_{x_{k}}^{2}-\eta_{x}\langle\mathcal{G}F( x_{k}),\widehat{\mathcal{G}}F(x_{k})\rangle_{x_{k}}\] \[\leq(4\kappa_{l}Mc_{R}+\bar{c}L_{F})\eta_{x}^{2}\|\widehat{ \mathcal{G}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+(4\kappa_{l}Mc_{R}+ \bar{c}L_{F})\eta_{x}^{2}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\] \[\quad+\frac{\eta_{x}}{2}\|\mathcal{G}F(x_{k})-\widehat{\mathcal{G }}F(x_{k})\|_{x_{k}}^{2}-\frac{\eta_{x}}{2}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\] \[=-\Big{(}\frac{\eta_{x}}{2}-(4\kappa_{l}Mc_{R}+\bar{c}L_{F})\eta_ {x}^{2}\Big{)}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\Big{(}\frac{\eta_{x}}{2}+(4 \kappa_{l}Mc_{R}+\bar{c}L_{F})\eta_{x}^{2}\Big{)}\|\mathcal{G}F(x_{k})-\widehat {\mathcal{G}}F(x_{k})\|_{x_{k}}^{2}.\]

where in the third inequality, we bound \(\|\mathcal{G}F(x_{k})\|_{x_{k}}\leq M+\frac{L}{\mu}M\leq\frac{2LM}{\mu}\).

(3) Then we can bound

\[d^{2}(y_{k},y^{*}(x_{k})) \leq 2d^{2}(y_{k-1}^{S},y^{*}(x_{k-1}))+2d^{2}(y^{*}(x_{k}),y^{*}(x_ {k-1}))\] \[\leq 2(1-\tau)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})+2\eta_{x}^{2}\kappa _{l}^{2}\bar{c}\|\widehat{\mathcal{G}}_{\mathrm{hinv}}F(x_{k-1})\|_{x_{k}}^{2}\] \[\leq 2(1+2\eta_{x}^{2}\kappa_{l}^{2}C_{\mathrm{hinv}}^{2}\bar{c})( 1-\tau)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})+4\eta_{x}^{2}\kappa_{l}^{2}\bar{c}\| \mathcal{G}F(x_{k-1})\|_{x_{k-1}}^{2},\]

where the last inequality follows similarly as (9).

Let a Lyapunov function be \(R_{k}\coloneqq F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))\). Then

\[R_{k+1}-R_{k}\] \[\leq-\Big{(}\frac{\eta_{x}}{2}-(4\kappa_{l}Mc_{R}+\bar{c}L_{F}) \eta_{x}^{2}-4\eta_{x}^{2}\kappa_{l}^{2}\bar{c}\Big{)}\|\mathcal{G}F(x_{k})\|_ {x_{k}}^{2}\] \[\quad+\Big{(}\big{(}2+C_{\mathrm{hinv}}^{2}(\frac{\eta_{x}}{2}+(4 \kappa_{l}Mc_{R}+\bar{c}L_{F})\eta_{x}^{2})+4\eta_{x}^{2}\kappa_{l}^{2}\bar{c} \big{)}(1-\tau)^{S}-1\Big{)}d^{2}(y^{*}(x_{k}),y_{k})\] \[\leq-\Big{(}\frac{\eta_{x}}{2}-(4\kappa_{l}Mc_{R}+\bar{c}L_{F}) \eta_{x}^{2}-4\eta_{x}^{2}\kappa_{l}^{2}\bar{c}\Big{)}\|\mathcal{G}F(x_{k})\|_ {x_{k}}^{2}\] \[\quad+\Big{(}\big{(}2+C_{\mathrm{hinv}}^{2}(\frac{\eta_{x}}{2}+(4 \kappa_{l}Mc_{R}+\bar{c}L_{F})\eta_{x}^{2})+4\eta_{x}^{2}\kappa_{l}^{2}\bar{c} \big{)}(1-\tau)^{S}-1\Big{)}d^{2}(y^{*}(x_{k}),y_{k})\] \[\leq-\Big{(}\frac{\eta_{x}}{2}-\tilde{L}_{F}\eta_{x}^{2}\Big{)}\| \mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\Big{(}\big{(}2+C_{\mathrm{hinv}}^{2}(\frac {\eta_{x}}{2}+\tilde{L}_{F}\eta_{x}^{2})\big{)}(1-\tau)^{S}-1\Big{)}d^{2}(y^{*} (x_{k}),y_{k})\] \[\leq-\frac{1}{16\tilde{L}_{F}}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\]

where we use \(\kappa_{l}^{2}\bar{c}\leq L_{F}\bar{c}\) and let \(\tilde{L}_{F}\coloneqq 4\kappa_{l}c_{R}M+5\bar{c}L_{F}\) in the second last inequality, and we choose \(\eta_{x}=\frac{1}{4L_{F}}\), \(S\geq\log\big{(}\frac{16L_{F}}{32L_{F}+3C_{\mathrm{hinv}}^{2}}\big{)}/\log(1- \tau)=\widetilde{\mathcal{G}}(\kappa_{l}^{2}\zeta)\), in the last inequality. Then telescoping the results yields Finally, we sum over \(k=0,...,K-1\), which leads to

\[\frac{1}{K}\sum_{k=0}^{K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq \frac{16\tilde{L}_{F}R_{0}}{K}.\]

Thus in order to achieve \(\epsilon\)-stationary solution, we require \(K=O(\tilde{L}_{F}\epsilon^{-1})=O(\kappa_{l}^{3}\epsilon^{-1})\) and hence the order of gradient and second-order complexities remain unchanged.

Extensions to other estimators.To extend the proof to other hypergradient estimators, we first notice that the convergence of inner iterations for solving lower-level problems is agnostic to the choice of hypergradient estimators, i.e.,

\[d^{2}\big{(}y_{k}^{s+1},y^{*}(x_{k})\big{)}\leq(1-\tau)d^{2}\big{(}y_{k}^{s},y^{*} (x_{k})\big{)},\quad\tau=\mu\eta_{y}-\eta_{y}^{2}(\zeta\bar{c}+2Dc_{R}).\]

(1) _Hypergradient approximation error._ For hypergradient estimator based on Hessian inverse, conjugate gradient, truncated Neumann series, the hypegradient approximation error remains the same as in Lemma 1, given no retraction is involved in the computation. That is,

* **CG**: \(\|\widehat{\mathcal{G}}_{\mathrm{cg}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}\leq C _{\mathrm{cg}}d(y^{*}(x_{k}),y_{k+1})+2L\sqrt{\kappa_{l}}\big{(}\frac{\sqrt{ \kappa_{l}}-1}{\sqrt{\kappa_{l}}+1}\big{)}^{T}\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x _{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}}\), where \(C_{\mathrm{cg}}=L+\kappa_{\rho}M+L(1+2\sqrt{\kappa_{l}})(\kappa_{l}+\frac{M \kappa_{\rho}}{\mu})\).

* **NS**: \(\|\widehat{\mathcal{G}}_{\text{ns}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}\leq C_{ \text{ns}}d(y^{*}(x_{k}),y_{k+1})+\kappa_{l}M(1-\gamma\mu)^{T}\), where \(C_{\text{ns}}=C_{\text{hinv}}\).

For hypergradient based on automatic differentiation (**AD**), we first show that there exists a constant \(C_{4}\) (that depends on \(C_{3},\bar{c},c_{R}\)) such that \(\mathrm{D}_{x}\mathrm{R}\mathrm{r}_{x}(u)=\mathcal{P}_{\mathrm{R}\mathrm{r} _{x}(u)}(\mathrm{id}+\mathrm{D}_{x}u)+\mathcal{E}\). with \(\|\mathcal{E}\|_{\mathrm{R}\mathrm{r}_{x}(u)}\leq C_{4}\|\mathrm{D}_{x}u\|_{ x}\|u\|_{x}\). Such a result can be derived by bounding the difference between retraction and exponential map. Then we follow the analysis for Lemma 1 as follows. Given \(y_{k}^{*+1}=\mathrm{R}\mathrm{e}\mathrm{r}_{y_{k}^{*}}(-\eta_{y}\mathcal{G}_{ y}g(x_{k},y_{k}^{*}))\), we have

\[\mathrm{D}_{x_{k}}y_{k}^{*+1}=\mathcal{P}_{y_{k}^{*+1}}\big{(}(\mathrm{id}- \eta_{y}\mathcal{H}_{y}g(x_{k},y_{k}^{*}))\mathrm{D}_{x_{k}}y_{k}^{*}-\eta_{y} \mathcal{G}_{yx}^{2}g(x_{k},y_{k}^{*})\big{)}+\mathcal{E}_{k}^{*}\]

where \(\|\mathcal{E}_{k}^{*}\|_{y_{k}^{*+1}}\leq\eta_{y}^{2}C_{4}\big{(}(1-\eta_{y} \mu)C_{1}+\eta_{y}L\big{)}\|\widehat{\mathcal{G}}_{y}f(x_{k},y_{k}^{*})\|_{y_{ k}^{*}}\). The rest of the proof follows exactly from Lemma 1, where we replace the convergence of inner iteration with the updated rate. This gives

\[\|\widehat{\mathcal{G}}_{\text{ad}}F(x_{k})-\mathcal{G}F(x_{k})\| _{x_{k}}\] \[\leq\Big{(}\frac{2M\widetilde{C}^{\prime}}{\mu-\eta_{y}(\bar{c} \bar{c}+2Dc_{R})}+L(1+\kappa_{l})\Big{)}(1-\tau)^{\frac{S-1}{2}}d(y_{k},y^{*}( x_{k}))+M\kappa_{l}(1-\eta_{y}\mu)^{S},\]

where \(\widetilde{C}^{\prime}\coloneqq(\kappa_{l}+1)\rho+(C_{2}+\eta_{y}C_{4})L\big{(} (1-\eta_{y}\mu)C_{1}+\eta_{y}L\big{)}\)

In summary

* **AD**: \(\|\widehat{\mathcal{G}}_{\text{ad}}F(x_{k})-\mathcal{G}F(x_{k})\|_{x_{k}}\leq C _{\text{ad}}(1-\tau)^{\frac{S-1}{2}}d(y_{k},y^{*}(x_{k}))+M\kappa_{l}(1-\eta_{ y}\mu)^{S}\), where \(C_{\text{ad}}\coloneqq\frac{2M\widetilde{C}^{\prime}}{\mu-\eta_{y}(\bar{c}\bar{c}+2Dc_{R })}+L(1+\kappa_{l})\).

(2) _Objective decrement._ This part is also the same across all hypergradient estimators, i.e., \[F(x_{k+1})-F(x_{k})\] \[\leq-\Big{(}\frac{\eta_{x}}{2}-(4\kappa_{l}Mc_{R}+\bar{c}L_{F}) \eta_{x}^{2}\Big{)}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\Big{(}\frac{\eta_{x}}{ 2}+(4\kappa_{l}Mc_{R}+\bar{c}L_{F})\eta_{x}^{2}\Big{)}\|\mathcal{G}F(x_{k})- \widehat{\mathcal{G}}F(x_{k})\|_{x_{k}}^{2}\]

(3) _Lyapunov function decrement._ The definition of Lyapunov function depends on the choice of hypergradient estimator.

For **CG**, we define \(R_{k}\coloneqq F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))+\|\hat{v}_{k}^{0}-\Gamma_{y^ {*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}}^{2}\). Then following similar analysis, we first bound

\[d^{2}(y_{k+1},y^{*}(x_{k+1})) \leq(2+8\eta_{x}^{2}\kappa_{l}^{2}C_{\text{cg}}^{2})(1-\tau)^{S}d ^{2}(y^{*}(x_{k}),y_{k})\] \[\quad+32\eta_{x}^{2}\kappa_{l}^{3}L^{2}\big{(}\frac{\sqrt{\kappa_ {l}}-1}{\sqrt{\kappa_{l}}+1}\big{)}^{2T}\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x_{k} )}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}}^{2}+4\eta_{x}^{2}\kappa_{l}^{2}\bar{c}\| \mathcal{G}F(x_{k})\|_{x_{k}}^{2}.\] (19)

and

\[\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k +1}}\] \[\leq 2\sqrt{\kappa_{l}}\big{(}\frac{\sqrt{\kappa_{l}}-1}{\sqrt{ \kappa_{l}}+1}\big{)}^{T}\|\hat{v}_{k-1}^{0}-\Gamma_{y^{*}(x_{k-1})}^{y_{k}}v_{ k-1}^{*}\|_{y_{k}}+2\sqrt{\kappa_{l}}(\kappa_{l}+\frac{M\kappa_{\rho}}{\mu})(1- \tau)^{\frac{S}{2}}d(y^{*}(x_{k-1}),y_{k-1})\] \[\quad+\big{\|}v_{k}^{*}-\Gamma_{y^{*}(x_{k-1})}^{y^{*}(x_{k})}v_{ k-1}^{*}\big{\|}_{y_{k}}+\frac{2MC_{0}D}{\mu}d(y_{k},y^{*}(x_{k}))\]

Then similarly,

\[\|v_{k}^{*}-\Gamma_{y^{*}(x_{k-1})}^{y^{*}(x_{k})}v_{k-1}^{*}\|_{ y_{k+1}}\] \[\leq\eta_{x}\bar{c}C_{v}\|\widehat{\mathcal{G}}_{\text{cg}}F(x_{k-1 })-\mathcal{G}F(x_{k-1})\|_{x_{k-1}}+\eta_{x}\bar{c}C_{v}\|\mathcal{G}F(x_{k-1}) \|_{x_{k-1}}\]

where in the second inequality, we use the bound between retraction and exponential map as well as triangle inequality. Then combining the above two results, gives

\[\|\hat{v}_{k}^{0}-\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k +1}}^{2}\]\[\leq 20\kappa_{l}(\frac{\sqrt{\kappa_{l}}-1}{\sqrt{\kappa_{l}}+1})^{2T} \|\hat{v}_{k-1}^{0}-\Gamma_{y^{*}(x_{k-1})}^{y_{k}}v_{k-1}^{*}\|_{y_{k}}^{2}+20 \kappa_{l}(\kappa_{l}+\frac{M\kappa_{\rho}}{\mu})^{2}(1-\tau)^{S}d(y^{*}(x_{k-1 }),y_{k-1})\] \[\quad+5\bar{c}\eta_{x}^{2}C_{v}^{2}\|\widehat{\mathcal{G}}_{\rm cg }F(x_{k-1})-\mathcal{G}F(x_{k-1})\|_{x_{k-1}}^{2}+5\bar{c}\eta_{x}^{2}C_{v}^{2 }\|\mathcal{G}F(x_{k-1})\|_{x_{k-1}}^{2}+\frac{5M^{2}C_{0}^{2}D^{2}}{\mu^{2}}d ^{2}(y_{k},y^{*}(x_{k})).\]

Then we can show

\[R_{k+1}-R_{k}\] \[\leq -\big{(}\frac{\eta_{x}}{2}-\eta_{x}^{2}\widetilde{\Lambda}\big{)} \|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}+\big{(}\frac{\eta_{x}}{2}+\eta_{x}^{2} \widetilde{\Lambda}\big{)}\|\mathcal{G}F(x_{k})-\widehat{\mathcal{G}}_{\rm cg }F(x_{k})\|_{x_{k}}^{2}\] \[\quad+\Big{(}\big{(}32\eta_{x}^{2}\kappa_{l}^{3}L^{2}(\frac{5M^ {2}C_{0}^{2}D^{2}}{\mu^{2}}+1)+20\kappa_{l}\big{)}\big{(}\frac{\sqrt{\kappa_{l }}-1}{\sqrt{\kappa_{l}}+1}\big{)}^{2T}-1\Big{)}\|\hat{v}_{k}^{0}-\Gamma_{y^{*} (x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}}^{2}\] \[\leq -\frac{1}{96\widetilde{\Lambda}}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\]

where we let \(\widetilde{\Lambda}\coloneqq C_{v}^{2}\bar{c}+\kappa_{l}^{2}(\frac{5M^{2}C_{0 }^{2}D^{2}}{\mu}+\bar{c})\) and without loss of generality \(\tilde{L}_{F}\leq\widetilde{\Lambda}\). The second inequality is by (19). The last inequality is by appropriately choosing \(S,T\), which is on the same order as the exponential map case. Then telescoping the result yields

\[\frac{1}{K}\sum_{k=0}^{K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{96 \widetilde{\Lambda}R_{0}}{K}.\]

For **NS**, we define \(R_{k}=F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))\) and derive

\[d^{2}(y_{k+1},y^{*}(x_{k+1}))\leq 2(1-\tau)^{S}d^{2}(y^{*}(x_{k}),y_{k})+4\eta_{x} ^{2}\kappa_{l}^{2}\bar{c}\|\widehat{\mathcal{G}}_{\rm ns}F(x_{k})-\mathcal{G} F(x_{k})\|_{x_{k}}^{2}+4\eta_{x}^{2}\kappa_{l}^{2}\bar{c}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\]

Then we can follow exactly the same proof as HINV that

\[\frac{1}{K}\sum_{k=0}^{K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{16 \tilde{L}_{F}R_{0}}{K}+\frac{\epsilon}{2}\]

by appropriately choosing \(S,T\) and \(\eta_{x}\).

For **AD**, we define \(R_{k}\coloneqq F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))\). Then following the same analysis except for the choice of \(S\), we can show,

\[\frac{1}{K}\sum_{k=0}^{K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{16 \tilde{L}_{F}R_{0}}{K}+\frac{\epsilon}{2}\]

Thus the proof is now complete. 

## Appendix F Tangent space conjugate gradient

In Algorithm 3, we show the tangent space conjugate gradient algorithm for solving the linear system \(\mathcal{H}[v]=\mathcal{G}\). Similar to [40], we set the initialization to be the transported output of \(\hat{v}_{k-1}^{T}\) from last iteration, where \(\hat{v}_{-1}^{T}=0\), which is beneficial for convergence analysis. For practical purposes, we notice setting \(v_{0}=0\) provides sufficient accurate solution without the expensive parallel transport operation.

```
1:Set \(r_{0}=\mathcal{G}\in T_{x}\mathcal{M},p_{0}=r_{0}\).
2:for\(t=0,...,T-1\)do
3: Compute \(\bar{r}_{t+1}=\mathcal{H}[v_{t}]\).
4:\(\alpha_{t+1}=\frac{\|r_{t}\|_{x}^{2}}{\langle p_{t},\mathcal{H}[p_{t}]\rangle_ {x}}\).
5:\(v_{t+1}=r_{t}+\alpha_{t+1}p_{t}\).
6:\(r_{t+1}=r_{t}-\alpha_{t+1}\mathcal{H}[p_{t}]\).
7:\(\beta_{t+1}=\frac{\|r_{t+1}\|_{x}^{2}}{\|r_{t}\|_{x}^{2}}\).
8:\(p_{t+1}=r_{t+1}+\beta_{t+1}p_{t}\).
9:endfor
10:Output: \(v_{T}\) ```

**Algorithm 4** Riemannian bilevel solver for min-max optimization

## Appendix G Extensions: on Riemannian mix-max and compositional optimization

The bilevel optimization considered in the paper (1) generalizes the two other widely studied problems, namely the min-max optimization and compositional optimization.

### Min-max optimization on Riemannian manifolds

Riemannian min-max problems have gained increasing interest over the recent years [37, 41, 27, 73, 67, 25, 56, 35], which takes the form of

\[\min_{x\in\mathcal{M}_{x}}\max_{y\in\mathcal{M}_{y}}f(x,y),\]

and can be seen as a special case of bilevel optimization problem (1) where \(g(x,y)=-f(x,y)\). Because the problem is nonconvex in \(x\), the order of minimization and maximization matters [73, 25]. Nevertheless, under the assumption where \(f\) is geodesic strongly convex in \(y\), the optimal solution \(x^{*}\) satisfies \(\mathcal{G}F(x^{*})=0\), where \(\mathcal{G}F(x)=\mathcal{G}_{x}f(x,y^{*}(x))\) due to \(\mathcal{G}_{y}f(x,y^{*}(x))=\mathcal{G}_{y}g(x,y^{*}(x))=0\). Thus Algorithm 1 reduces to alternating gradient descent ascent over Riemannian manifolds, as outlined in Algorithm 4.

Here we adapt the convergence analysis to the min-max optimization setting. Given we no longer require second-order derivatives, we restate assumptions for functions \(f\), \(g\) below.

**Assumption 6**.: (1) Assumption 1 holds. (2) Function \(f(x,y)\), \(g(x,y)\) have \(L\)-Lipschitz Riemannian gradients. (3) Further, \(g(x,y)\) is \(\mu\)-geodesic strongly convex in \(y\).

Under the min-max setup and Assumption 6, we see \(\mathcal{G}F(x)=\mathcal{G}_{x}f(x,y^{*}(x))\) and thus the Lipschitz constant can be derived as \(L_{F}=(\kappa_{l}+1)L=\Theta(\kappa_{l})\). Further we can directly apply Theorem 1 for the Hessian inverse with \(C_{\mathrm{hinv}}=L\), which leads to the following convergence result.

**Theorem 4**.: _Under Assumption 6, choosing \(S\geq\widetilde{\Theta}(\kappa_{l}^{2}\zeta)\), \(\eta_{x}=\frac{1}{20L_{F}}\), we have \(\min_{k=0,...,K-1}\|\mathcal{G}F(x_{k})\|_{x_{k}}^{2}\leq\frac{80(\kappa_{l}+ 1)L\Delta_{0}}{K}\) and to reach \(\epsilon\)-stationary solution, we require gradient complexities as \(G_{f}=O(\kappa_{l}\epsilon^{-1})\) and \(G_{g}=\widetilde{O}(\kappa_{l}^{3}\zeta\epsilon^{-1})\)._

### Compositional optimization on Riemannian manifolds

Compositional problems on Riemannian manifolds have been considered in [36, 70], which requires to solve

\[\min_{x\in\mathcal{M}_{x}}\psi(\phi(x)),\] (20)

where \(\psi:\mathcal{M}_{y}\to\mathbb{R}\) and \(\phi:\mathcal{M}_{x}\to\mathcal{M}_{y}\). It is worth noting that in both works [36, 70], the inner function \(\phi:\mathcal{M}_{x}\to\mathbb{R}^{d}\) is vector-valued. In contrast, we consider a general manifold-valued function \(\phi\). Because the function \(\phi\) can be potentially complex and may be stochastic, we follow [11] to reformulate (20) into a bilevel optimization problem by letting

\[f(x,y)\coloneqq\psi(y^{*}(x)),\ \text{s.t.}\ y^{*}(x)=\operatorname*{arg\,min}_ {y\in\mathcal{M}_{y}}\{g(x,y)\coloneqq\frac{1}{2}d^{2}(\phi(x),y)\}.\]

As long as the squared Riemannian distance is geodesic strongly convex, the reformulation is equivalent to the original problem (20). As formally stated in Lemma 9, this is satisfied for non-positively curved space, like Euclidean space, hyperbolic manifold, SPD manifold with affine invariant metric. For positively curved space, the strong convexity is guaranteed when restricting the domain relative to the curvature.

**Lemma 9**.: _Let \(\mathcal{U}\subseteq\mathcal{M}\) has sectional curvature lower and upper bounded by \(\kappa^{-}\) and \(\kappa^{+}\) respectively. Further \(\mathcal{U}\) has diameter upper bounded by \(\bar{D}\), which satisfies \(\bar{D}<\frac{\pi}{\sqrt{\kappa^{+}}}\) if \(\kappa^{+}>0\). Then let \(\delta=1\) when \(\kappa^{+}\leq 0\) and \(\delta=\frac{\sqrt{\kappa^{+}}\bar{D}}{\tan(\sqrt{\kappa^{+}}\bar{D})}\) when \(\kappa^{+}>0\) and consider \(\zeta\) be the same curvature constant as in Lemma 3. Then function \(\mathcal{H}_{y}g(x,y)\) has Riemannian Hessian bounded within \([\delta,\zeta]\) in spectrum._

Proof of Lemma 9.: The proof follows from Lemma 2 in [3]. Consider an arbitrary curve \(\gamma:[0,1]\to\mathcal{M}\), and let \(f(x)=\frac{1}{2}d^{2}(x,p)\), for some \(p\in\mathcal{M}\). From [3], we know that \(\mathcal{H}f(\gamma(t))[\gamma^{\prime}(t)]=-\boldsymbol{\nabla}_{\gamma^{ \prime}(t)}\mathrm{Exp}_{\gamma^{\prime}(t)}^{-1}(p)\) and under the conditions, \(\delta\|\gamma^{\prime}(t)\|_{\gamma(t)}^{2}\leq\langle\boldsymbol{\nabla}_{ \gamma^{\prime}(t)}\mathrm{Exp}_{\gamma(t)}^{-1}(p),-\gamma^{\prime}(t) \rangle_{\gamma(t)}\leq\zeta\|\gamma^{\prime}(t)\|_{\gamma(t)}^{2}\), where we denote \(\boldsymbol{\nabla}\) as the covariant derivative. This immediately leads to

\[\delta\|\gamma^{\prime}(t)\|_{\gamma(t)}^{2}\leq\langle\gamma^{\prime}(t), \mathcal{H}f(\gamma(t))[\gamma^{\prime}(t)]\rangle_{\gamma(t)}\leq\zeta\| \gamma^{\prime}(t)\|_{\gamma(t)}^{2}\]

which completes the proof. 

Thus, for positively curved manifold, if \(\bar{D}<\frac{\pi}{2\sqrt{\kappa^{+}}}\), we have \(\delta>0\), which ensures geodesic strong convexity of the inner problem. As shown in Lemma 12 in [3], \(\mathcal{G}_{y}d^{2}(\phi(x),y)=2\mathrm{Exp}_{y}^{-1}\phi(x)\) and the Riemannian gradient descent on \(y\) lead to

\[y_{k}^{s+1}=\mathrm{Exp}_{y_{k}^{s}}\big{(}-\eta_{y}\mathrm{Exp}_{y_{k}^{s}}^{ -1}\phi(x_{k})\big{)},\]

which suggests \(y_{k}^{s+1}\) lies on a geodesic that connects \(y_{k}^{s}\) and \(\phi(x_{k})\). When \(S=1\) and when the lower-level function \(g\) is vector-valued, the algorithm recovers the deterministic version of SCGD [66].

However, unlike in the Euclidean space, the Riemannian Hessian does not simplify to the identity operator, but rather the covariant derivative of inverse exponential map and the cross derivatives \(\mathcal{G}_{xy}^{2}g(x,y)\neq-(\mathrm{D}\phi(x))^{\dagger}\).

**Assumption 7**.: (1) Assumption 1 holds and further \(\bar{D}<\frac{\pi}{2\sqrt{\kappa^{+}}}\) if \(\kappa^{+}>0\). (2) Function \(f(x,y)\) has Riemannian gradients that are bounded by \(M\) and are \(L\)-Lipschitz. (3) Function \(g\) has \(\rho\)-Lipschitz Riemannian Hessian and cross derivatives.

We notice that for function \(g\) we only require second-order derivatives to be Lipschitz because the first-order Lipschitzness can be inferred from Lemma 9.

**Theorem 5**.: _Under Assumption 7, Theorem 1 holds with \(L=\zeta,\mu=\delta\)._

To prove the convergence, we only need to show Lemma 4 holds. It can be readily proved from Lemma 9 and Assumption 7 that Lemma 4 holds with \(L=\zeta,\mu=\delta\). Hence the convergence follows directly.

Experimental details

### Synthetic problem

We first verify the lower-level problem is geodesic strongly convex.

**Proposition 4**.: _For any \(\mathbf{A},\mathbf{B}\succ 0\), function \(f(\mathbf{M})=\langle\mathbf{M},\mathbf{A}\rangle+\langle\mathbf{M}^{-1}, \mathbf{B}\rangle\) is \(\mu\)-geodesic strongly convex in \(\mathcal{U}\subset\mathbb{S}_{++}^{d}\) with \(\mu=\lambda_{a,-}\lambda_{-}+\frac{\lambda_{b,-}\lambda_{-}}{\lambda_{+}^{2}}\), where \(\lambda_{a,-},\lambda_{b,-}\) are the minimum eigenvalue of \(\mathbf{A},\mathbf{B}\) and \(\lambda_{\pm}\) are the bounds for maximum and minimum eigenvalues for \(\mathbf{M}\in\mathcal{U}\)._

_The inverse of Riemannian Hessian of function \(f(\mathbf{M})=\langle\mathbf{M},\mathbf{A}\rangle+\langle\mathbf{M}^{-1}, \mathbf{B}\rangle\) is derived as, for any symmetric \(\mathbf{U}\), \(\mathcal{H}^{-1}f(\mathbf{M})[\mathbf{U}]=\mathbf{M}^{1/2}\mathbf{G}\mathbf{M} ^{1/2}\) where \(\mathbf{G}\) is the solution to the Lyapunov equation \(\mathbf{G}(\mathbf{M}^{1/2}\mathbf{A}\mathbf{M}^{1/2}+\mathbf{M}^{-1/2} \mathbf{B}\mathbf{M}^{-1/2})+(\mathbf{M}^{1/2}\mathbf{A}\mathbf{M}^{1/2}+ \mathbf{M}^{-1/2}\mathbf{B}\mathbf{M}^{-1/2})\mathbf{G}=\mathbf{M}^{-1/2} \mathbf{U}\mathbf{M}^{-1/2}\)._

Proof of Proposition 4.: We first derive the Euclidean gradient and Hessian as

\[\nabla f(\mathbf{M})=\mathbf{A}-\mathbf{M}^{-1}\mathbf{B}\mathbf{M}^{-1}, \quad\nabla^{2}f(\mathbf{M})[\mathbf{U}]=\mathbf{M}^{-1}\mathbf{U}\mathbf{M} ^{-1}\mathbf{B}\mathbf{M}^{-1}+\mathbf{M}^{-1}\mathbf{B}\mathbf{M}^{-1} \mathbf{U}\mathbf{M}^{-1}\]

for any \(\mathbf{U}=\mathbf{U}^{\top}\). The Riemannian gradient and Hessian are derived as

\[\mathcal{G}f(\mathbf{M}) =\mathbf{M}\mathbf{A}\mathbf{M}-\mathbf{B}\] \[\mathcal{H}f(\mathbf{M})[\mathbf{U}] =\mathbf{U}\mathbf{M}^{-1}\mathbf{B}+\mathbf{B}\mathbf{M}^{-1} \mathbf{U}+\{\mathbf{U}\nabla f(\mathbf{M})\mathbf{M}\}_{\mathrm{S}}\] \[=\mathbf{U}\mathbf{M}^{-1}\mathbf{B}+\mathbf{B}\mathbf{M}^{-1} \mathbf{U}+\frac{1}{2}\big{(}\mathbf{U}\mathbf{A}\mathbf{M}-\mathbf{U}\mathbf{ M}^{-1}\mathbf{B}+\mathbf{M}\mathbf{A}\mathbf{U}-\mathbf{B}\mathbf{M}^{-1} \mathbf{U}\big{)}\] \[=\frac{1}{2}\big{(}\mathbf{U}\mathbf{A}\mathbf{M}+\mathbf{M} \mathbf{A}\mathbf{U}+\mathbf{U}\mathbf{M}^{-1}\mathbf{B}+\mathbf{B}\mathbf{M }^{-1}\mathbf{U}\big{)}\]

where we let \(\{\mathbf{A}\}_{\mathrm{S}}=(\mathbf{A}+\mathbf{A}^{\top})/2\). To show the function is geodesic strongly convex, it suffices to show \(\mathcal{H}f(\mathbf{M})\) is positive definite, which is to show \(\langle\mathcal{H}f(\bar{\mathbf{M}})[\mathbf{U}],\mathbf{U}\rangle_{\mathbf{ M}}\geq\mu\|\mathbf{U}\|_{\mathbf{M}}^{2}>0\) for any \(\mathbf{U}=\mathbf{U}^{\top}\). To this end, we vectorize the Riemannian Hessian in terms of \(\mathbf{U}\) as \(\mathrm{vec}(2\mathcal{H}f(\mathbf{M})[\mathbf{U}])=(\mathbf{M}\mathbf{A} \otimes\mathbf{I}+\mathbf{I}\otimes\mathbf{M}\mathbf{A}+\mathbf{B}\mathbf{M }^{-1}\otimes\mathbf{I}+\mathbf{I}\otimes\mathbf{B}\mathbf{M}^{-1})\mathrm{ vec}(\mathbf{U})\), where \(\otimes\) denotes the Kronecker product. Then, we have

\[\langle\mathcal{H}f(\mathbf{M})[\mathbf{U}],\mathbf{U}\rangle_{ \mathbf{M}}\] \[=\mathrm{tr}(\mathbf{M}^{-1}\mathbf{U}\mathbf{M}^{-1}\mathcal{H}f (\mathbf{M})[\mathbf{U}])\] \[=\frac{1}{2}\mathrm{vec}(\mathbf{U})^{\top}(\mathbf{M}^{-1} \otimes\mathbf{M}^{-1})(\mathbf{M}\mathbf{A}\otimes\mathbf{I}+\mathbf{I} \otimes\mathbf{M}\mathbf{A}+\mathbf{B}\mathbf{M}^{-1}\otimes\mathbf{I}+ \mathbf{I}\otimes\mathbf{B}\mathbf{M}^{-1})\mathrm{vec}(\mathbf{U})\] \[=\frac{1}{2}\mathrm{vec}(\mathbf{U})^{\top}(\mathbf{A}\otimes \mathbf{M}^{-1}+\mathbf{M}^{-1}\otimes\mathbf{A}+\mathbf{M}^{-1}\mathbf{B} \mathbf{M}^{-1}\otimes\mathbf{M}^{-1}+\mathbf{M}^{-1}\otimes\mathbf{M}^{-1} \mathbf{B}\mathbf{M}^{-1})\mathrm{vec}(\mathbf{U})\] \[\geq(\lambda_{a,-}\lambda_{m,-}+\frac{\lambda_{b,-}\lambda_{m,-}} {\lambda_{m,+}^{2}})\mathrm{vec}(\mathbf{U})^{\top}(\mathbf{M}^{\top}\otimes \mathbf{M}^{-1})\mathrm{vec}(\mathbf{U})=\mu\|\mathbf{U}\|_{\mathbf{M}}^{2},\]

where we let \(\lambda_{a,\pm}\) be the maximum/minimum eigenvalues of \(\mathbf{A}\) and similarly for \(\lambda_{b,\pm},\lambda_{m,\pm}\).

The Hessian inverse can be derived subsequently. This completes the proof. 

### Computational time for each hypergradient estimator

This section report the average runtime in seconds (over 10 runs) for single evaluation of hypergradient using four different strategies (for Hypergradient estimation) for the synthetic problem (Section 4.1, Figure 1). The hyper-parameters are set to be the same as the main experiment. We see in general automatic differentiation (AD) is the most efficient strategy. Nevertheless, according to Figure 1(a), it is less accurate compared to other strategies.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline
**HINV** & **CG** & **NS** & **AD** \\ \hline
0.0154 & 0.1037 & 0.1030 & 0.0053 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of runtime for single computation of hypergradient.

### Hyperparameter selection

The selection of hyper-parameters is performed to reflect the best performance. The stepsize is selected from the range [1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1] and for Neumann series is selected from [0.1, 0.5, 1.0, 1.5, 2.0] and number of inner iterations is selected from [5, 10, 30, 50, 100]. Figure 1(d) shows the sensitivity of hypergradient error as we vary and the number of inner iterations.

### Computational Complexity

This section lists out the computational complexity for each task considered in the experiment section. In Table 4, we present an estimate of the per-iteration complexity of computing the gradient, Hessian/Jacobian-vector products. We highlight that we only provide estimates of the complexities given that there may not exist closed form expressions for the gradient and second-order derivatives.

Here, \(n_{v},n_{t}\) denote the size of validation set and training set respectively. For meta learning, \(m\) denotes the number tasks and \(n\) denotes the number of samples for each task. For domain adaptation, \(m,n\) denote the number of samples for two domains, \(s\) denotes the number of Sinkhorn iterations.

## Appendix I Experiment Configurations

All the experiments are conducted on a single NVIDIA RTX 4060 GPU. All datasets used in the paper are publicly available, which are properly cited in the main paper. We include detailed setups for the experiments in the main paper as well as documented in code (provided as supplementary material).

## Appendix J Broader Impact

This paper proposes new algorithms and are of theoretical in nature. We do not foresee any immediate negative societal impact that we feel obliged to report.

\begin{table}
\begin{tabular}{c|c|c|c} \hline  & **Hyper-rep (shallow)** & **Meta learning** & **Domain adaptation** \\ \hline \(x\) size & \(d\times r\) & \(d\times r\) & \(m\times n\) \\ \hline \(y\) size & \(r(r+1)/2\) & \(d\times r\) & \(d\times d\) \\ \hline \(G_{f}\) & \(O(n_{v}d^{2}r+n_{v}r^{3})\) & \(O(mnd^{2}r)\) & \(O(smn)\) \\ \hline \(G_{g}\) & \(O(n_{t}r^{4})\) & \(O(mnd^{2}r)\) & \(O(d^{3}+md^{2}+nd^{2})\) \\ \hline \(JV_{g}\) & \(O(n_{t}(r^{4}+d^{2}r))\) & \(O(mnd^{2}r)\) & \(O(d^{3}+md^{2}+nd^{2}+smn)\) \\ \hline \(HV_{g}\) & \(O(n_{t}r^{4})\) & \(O(mndr^{2})\) & \(O(d^{3}+md^{2}+nd^{2})\) \\ \hline \end{tabular}
\end{table}
Table 4: Per-iteration complexity estimate for each task

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Please refer to Section 3 for theoretical developments and Section 4 for empirical studies. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We include discussions of limitations in the conclusion section, i.e., Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We have included all the assumptions and complete proof in the paper. Please refer to Section 3 and Section C, D, E.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided the implementation details for reproducing the experiments, along with the code in the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have provided the code and data as supplementary material to ensure reproducibility. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have included the details in the experiment section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We have reported standard deviation in Figure 2(b) and (c). For Figure 1, the results barely change as we vary the random seed. For Riemannian meta learning, it becomes time costly to report standard deviation. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have included the detailed configuration in Appendix I. We report the time of execution in Appendix H.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have included a broader impact section, i.e., Section J. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not release data or models that have high risks for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: In the experiment section, this paper has cited the papers when using exisiting assets, inluding code and data. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not introduce new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.