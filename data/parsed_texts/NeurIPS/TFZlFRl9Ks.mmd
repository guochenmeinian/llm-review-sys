# CAT3D: Create Anything in 3D

with Multi-View Diffusion Models

 Ruiqi Gao\({}^{1}\)* Aleksander Holyinski\({}^{1}\)* Philipp Henzler\({}^{2}\) Arthur Brussee\({}^{1}\)

**Ricardo Martin-Brualla\({}^{2}\) Pratul Srinivasan\({}^{1}\) Jonathan T. Barron\({}^{1}\) Ben Poole\({}^{1}\)* \({}^{1}\)Google DeepMind \({}^{2}\)Google Research *equal contribution**

###### Abstract

Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene. We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model. Given any number of input images and a set of target novel viewpoints, our model generates highly consistent novel views of a scene. These generated views can be used as input to robust 3D reconstruction techniques to produce 3D representations that can be rendered from any viewpoint in real-time. CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation.

## 1 Introduction

The demand for 3D content is higher than ever, since it is essential for enabling real-time interactivity for games, visual effects, and wearable mixed reality devices. Despite the high demand, high-quality 3D content remains relatively scarce. Unlike 2D images and videos which can be easily captured with consumer photography devices, creating 3D content requires complex specialized tools and a substantial investment of time and effort.

Figure 1: CAT3D enables 3D scene creation from any number of generated or real images.

Fortunately, recent advancements in photogrammetry techniques have greatly improved the accessibility of 3D asset creation from 2D images. Methods such as NeRF [1], Instant-NGP [2], and Gaussian Splatting [3] allow anyone to create 3D content by taking photos of a real scene and optimizing a representation of that scene's underlying 3D geometry and appearance. The resulting 3D representation can then be rendered from any viewpoint, similar to traditional 3D assets. Unfortunately, creating detailed scenes still requires a labor-intensive process of capturing hundreds to thousands of photos. Captures with insufficient coverage of the scene can lead to an ill-posed optimization problem, which often results in incorrect geometry and appearance and, consequently, implausible imagery when rendering the recovered 3D model from novel viewpoints.

Reducing this requirement from dense multi-view captures to less exhaustive inputs, such as a single image or text, would enable more accessible 3D content creation. Prior work has developed specialized solutions for different input settings, such as geometry regularization techniques targeted for sparse-view reconstruction [4; 5], feed-forward models trained to create 3D objects from single images [6], or the use of image-conditioned [7] or text-conditioned [8] generative priors in the optimization process--but each of these specialized methods comes with associated limitations in quality, efficiency, and generality.

In this paper, we instead focus on the fundamental problem that limits the use of established 3D reconstruction methods in the observation-limited setting: an insufficient number of supervising views. Rather than devising specialized solutions for different input regimes [8; 9; 7], a shared solution is to instead simply _create_ more observations--collapsing the less-constrained, under-determined 3D creation problems to the fully-constrained, fully-observed 3D reconstruction setting. This way, we reformulate a difficult ill-posed _reconstruction_ problem as a _generation_ problem: given any number of input images, generate a collection of consistent novel observations of the 3D scene. Recent video generative models show promise in addressing this challenge, as they demonstrate the capability to synthesize video clips featuring plausible 3D structure [10; 11; 12; 13; 14; 15]. However, these models are often expensive to sample from, challenging to control, and limited to smooth and short camera trajectories.

Our system, **CAT3D**, instead accomplishes this through a multi-view diffusion model trained _specifically_ for novel-view synthesis. Given any number of input views and any specified novel viewpoints, our model generates multiple 3D-consistent images through an efficient parallel sampling strategy. These generated images are subsequently fed through a robust 3D reconstruction pipeline to produce a 3D representation that can be rendered at interactive rates from any viewpoint. We show that our model is capable of producing photorealistic results of arbitrary objects or scenes from any number of captured or synthesized input views in as little as one minute. We evaluate our work across various input settings, ranging from sparse multi-view captures to a single captured image, and even just a text prompt (by using a text-to-image model to generate an input image from that prompt). CAT3D outperforms prior works for measurable tasks (such as the multi-view capture case) on multiple benchmarks, and is an order of magnitude faster than previous state-of-the-art. For tasks where empirical performance is difficult to measure (such as text-to-3D and single image to 3D), CAT3D compares favorably with prior work in all settings.

## 2 Related Work

Creating entire 3D scenes from limited observations requires 3D generation, _e.g._, creating content in unseen regions, and our work builds on the ever-growing research area of 3D generative models [16]. Due to the relative scarcity of 3D datasets, much research in 3D generation is centered on transferring knowledge learned by 2D image-space priors, as 2D data is abundant. Our diffusion model is built on the recent development of video and multi-view diffusion models that produce highly consistent novel views. We show that pairing these models with 3D reconstruction, similar to [17; 18], enables efficient and high quality 3D creation. Below we discuss how our work is related to several areas of prior work.

2D priors.Given limited information such as text, pretrained text-to-image models can provide a strong generative prior for text-to-3D generation. However, distilling the knowledge present in these image-based priors into a coherent 3D model currently requires an iterative distillation approach. DreamFusion [8] introduced Score Distillation Sampling (SDS) to synthesize 3D objects (as NeRFs) from text prompts. Research in this space has aimed to improve distillation strategies [19; 20; 21; 22;Figure 2: **Qualitative results (renders)**: CAT3D can create high-quality 3D objects or scenes from a number of input modalities: an input image generated by a text-to-image model (_top row_), a single captured real image (_middle row_), and multiple captured real images (_bottom row_).

23], swap in other 3D representations [24; 25; 26; 27; 28], and amortize the optimization process [29]. Using text-based priors for single-image-to-3D has also shown promise [30; 31; 32], but requires a complex balancing of the image observation with additional constraints. Incorporating priors such as monocular depth models or inpainting models has been useful for creating 3D scenes, but tends to result in poor global geometry [33; 34; 35; 36].

2D priors with camera conditioning.While text-to-image models excel at generating visually appealing images, they lack precise control over the pose of images, and thus require a time-consuming 3D distillation process to encourage the 3D model to conform to the 2D prior. To overcome this limitation, several approaches train or fine-tune generative models with explicit image and pose conditioning [7; 37; 38; 39; 40; 41; 42]. These models provide stronger priors for what an object or scene should look like given text and/or input image(s), but they also model all output views _independently_. In cases where there is little uncertainty in what should appear at novel views, reasoning about generated views independently is sufficient for efficient 3D reconstruction [43]. But when there is some uncertainty exists, these top-performing methods still require expensive 3D distillation to resolve the inconsistencies between different novel views.

Multi-view priors.Modeling the correlations between multiple views provides a much stronger prior for what 3D content is consistent with partial observations. Methods like MVDream [44], ImageDream [9], Zero123++ [45], ConsistNet [46], SyncDreamer [47] and ViewDiff [48] fine-tune text-to-image models to generate multiple views simultaneously. CAT3D is similar in architecture to ImageDream, where the multi-view dependency is captured by an architecture resembling video diffusion models with 3D self-attention. Given this stronger prior, these papers also demonstrate higher quality and more efficient 3D extraction.

Video priors.Video diffusion models have demonstrated an astonishing capability of generating realistic videos [49; 50; 10; 12; 15; 13; 51], and are thought to implicitly reason about 3D. However, it remains challenging to use off-the-shelf video diffusion models for 3D generation for a number of reasons. Current models lack exact camera controls, limiting generation to clips with only smooth and short camera trajectories, and struggle to generate videos with only camera motion but no scene dynamics. Several works have proposed to resolve these challenges by fine-tuning video diffusion models for camera-controled or multi-view generation. For example, AnimateDiff [52] LoRA fine-tuned a video diffusion model with fixed types of camera motions, and MotionCtrl [53] conditioned the model on arbitrary specified camera trajectories. ViVid-1-to-3 [54] combines a novel view synthesis model and a video diffusion model for generating smooth trajectories. SVD-MV [55], IM-3D [17] and SV3D [55] further explored leveraging camera-controlled or multi-view video diffusion models for 3D generation. However, their camera trajectories are limited to orbital ones surrounding the center content. These approaches mainly focus on 3D object generation, and do not work for 3D scenes, few-view 3D reconstruction, or objects in context (objects that have not been masked or otherwise separated from the image's background).

Feed-forward methods.Another line of research is to learn feed-forward models that take a few views as input, and output 3D representations directly, without an optimization process per instance [6; 56; 57; 58; 18; 59; 60]. These methods can produce 3D representations efficiently (within a few seconds), but the quality is often worse than approaches built on image-space priors.

## 3 Method

CAT3D is a two-step approach for 3D creation: first, we generate a large number of novel views consistent with one or more input views using a multi-view diffusion model, and second, we run a robust 3D reconstruction pipeline on the generated views (see Figure 3). Below we describe our multi-view diffusion model (Section 3.1), our method for generating a large set of nearly consistent novel views from it (Section 3.2), and how these generated views are used in a 3D reconstruction pipeline (Section 3.3).

### Multi-View Diffusion Model

We train a multi-view diffusion model that takes a single or multiple views of a 3D scene as input and generates multiple output images given their camera poses (where "a view" is a paired image and its camera pose). Specifically, given \(M\) conditional views containing \(M\) images \(\mathbf{I}^{\mathrm{cond}}\) and their corresponding camera parameters \(\mathbf{p}^{\mathrm{cond}}\), the model learns to capture the joint distribution of \(N\) target images \(\mathbf{I}^{\mathrm{tgt}}\) assuming their \(N\) target camera parameters \(\mathbf{p}^{\mathrm{tgt}}\) are also given:

\[p\big{(}\mathbf{I}^{\mathrm{tgt}}|\mathbf{I}^{\mathrm{cond}},\mathbf{p}^{ \mathrm{cond}},\mathbf{p}^{\mathrm{tgt}}\big{)}\,.\] (1)

Model architecture.Our model architecture is similar to video latent diffusion models (LDMs) [49; 11], but with camera pose embeddings for each image instead of time embeddings. Given a set of conditional and target images, the model encodes every individual image into a latent representation through an image variational auto-encoder [61]. Then, a diffusion model is trained to estimate the joint distribution of the latent representations given conditioning signals. We initialize the model from an LDM trained for text-to-image generation similar to [62] trained on web-scale image data, with an input image resolution of \(512\times 512\times 3\) and latents with shape \(64\times 64\times 8\). As is often done in video diffusion models [50; 10; 11], the main backbone of our model remains the pretrained 2D diffusion model but with additional layers connecting the latents of multiple input images. As in [44], we use 3D self-attention (2D in space and 1D across images) instead of simple 1D self-attention across images. We directly inflate the existing 2D self-attention layers after every 2D residual block of the original LDM to connect latents with 3D self-attention layers while inheriting the parameters from the pre-trained model, introducing minimal amount of extra model parameters. We found that conditioning on input views through 3D self-attention layers removed the need for PixelNeRF [63] and CLIP image embeddings [64] used by the prior state-of-the-art model on few-view reconstruction, ReconFusion [7]. We use FlashAttention [65; 66] for fast training and sampling, and fine-tune all the weights of the latent diffusion model. Similar to prior work [10; 67], we found it important to shift the noise schedule towards high noise levels as we move from the pre-trained image diffusion model to our multi-view diffusion model that captures data of higher dimensionality. Concretely, following logic similar to [67], we shift the log signal-to-noise ratio by \(\log(N)\) towards the high signal-to-noise ratio region, where \(N\) is the number of target views. Similar shifts have been adopted empirically

Figure 3: **Illustration of the method. Given one to many views, CAT3D creates a 3D representation of the scene in as little as one minute. CAT3D has two stages: (1) generate a large set of synthetic views with a multi-view latent diffusion model conditioned on the input views and target poses; (2) run a robust 3D reconstruction pipeline on the observed and generated views. This decoupling of the generative prior and 3D reconstruction process results in efficiency improvements and reduced methodological complexity relative to prior work [7; 8; 42], while also improving visual quality.**by [11; 10]. For training, latents of target images are noise perturbed while latents of conditional images are kept as clean, and the diffusion loss is defined only on target images. A binary mask is concatenated to the latents along the channel dimension, to denote conditioning vs. target images. To deal with multiple 3D generation settings, we train a single versatile model that can model a total of 8 conditioning and target views (\(N+M=8\)), and randomly select the number of conditional views \(M\) to be \(1\) or \(3\) during training, corresponding to \(7\) and \(5\) target views respectively. The noise schedule is shifted based on \(5\) target views. See Appendix B for more model details.

Camera conditioning.To condition on the camera pose, we use a camera ray representation ("raymap") that is the same height and width as the latent representations [38; 68] and encodes the ray origin and direction at each spatial location. The rays are computed relative to the camera pose of the first conditional image, so our pose representation is invariant to rigid transformations of 3D world coordinates. Raymaps for each image are concatenated channel-wise onto the latents for the corresponding image.

### Generating Novel Views

Given a set of input views, our goal is to generate a large set of consistent views to fully cover the scene and enable accurate 3D reconstruction. To do this, we need to decide on the set of camera poses to sample, and we need to design a sampling strategy that can use a multi-view diffusion model trained on a small number of views to generate a much larger set of consistent views.

Camera trajectories.Compared to 3D object reconstruction where orbital camera trajectories can be effective, a challenge of 3D scene reconstruction is that the views required to fully cover a scene can be complex and depend on the scene content. We empirically found that designing reasonable camera trajectories for different types of scenes is crucial to achieve compelling few-view 3D reconstruction. The camera paths must be sufficiently thorough and dense to fully-constrain the reconstruction problem, but also must not pass through objects in the scene or view scene content from unusual angles. In summary, we explore four types of camera paths based on the characteristic of a scene: (1) orbital paths of different scales and heights around the center scene, (2) forward facing circle paths of different scales and offsets, (3) spline paths of different offsets, and (4) spiral trajectories along a cylindrical path, moving into and out of the scene. See Appendix C for more details.

Generating a large set of synthetic views.A challenge in applying our multi-view diffusion model to novel view synthesis is that it was trained with a small and finite set of input and output views -- just 8 in total. To increase the total number of output views, we cluster the target viewpoints into smaller groups and generate each group independently given the conditioning views. We group target views with close camera positions, as these views are typically the most dependent. For single-image conditioning, we adopt an autoregressive sampling strategy, where we first generate a set of 7 _anchor views_ that cover the scene (similar to [42], and chosen using the greedy initialization from [69]), and then generate the remaining groups of views in parallel given the observed and anchor views. This allows us to efficiently generate a large set of synthetic views while still preserving both long-range consistency between anchor views and local similarity between nearby views. For the single-image setting, we generate 80 views, while for the few-view setting we use 480-960 views. See Appendix C for details.

Conditioning larger sets of input views and non-square images.To expand the number of views we can condition on, we choose the nearest \(M\) views as the conditioning set, as in [7]. We experimented with simply increasing the sequence length of the multi-view diffusion architecture during sampling, but found that the nearest view conditioning and grouped sampling strategy performed better. To handle wide aspect ratio images, we combine square samples from square-cropped input views with wide samples cropped from input views padded to be square.

### Robust 3D reconstruction

Our multi-view diffusion model generates a set of high-quality synthetic views that are reasonably consistent with each other. However, the generated views are generally not perfectly 3D consistent. Indeed, generating perfectly 3D consistent images remains a very challenging problem even for current state-of-the-art video diffusion models [70]. Since 3D reconstruction methods have been designed to take photographs (which are by definition perfectly consistent) as input, we modify the standard NeRF training procedure to improve its robustness to inconsistent input views.

We build upon Zip-NeRF [71], whose training procedure minimizes the sum of a photometric reconstruction loss, a distortion loss, an interlevel loss, and a normalized L2 weight regularizer. We additionally include a perceptual loss (LPIPS [72]) between the rendered image and the input image. Compared to the photometric reconstruction loss, LPIPS emphasizes high-level semantic similarity between the rendered and observed images, while ignoring potential inconsistencies in low-level high-frequency details. Since generated views closer to the observed views tend to have less uncertainty and are therefore more consistent, we weight the losses for generated views based on the distance to the nearest observed view. This weighting is uniform at the beginning of the training, and is gradually annealed to a weighting function that more strongly penalizes reconstruction losses for views closer to one of the observed views. See Appendix D for additional details.

## 4 Experiments

We trained the multi-view diffusion model at the core of CAT3D on four datasets with camera pose annotations: Obiayverse [73], CO3D [74], RealEstate10k [75] and MVImgNet [76]. We then evaluated CAT3D on the few-view reconstruction task (Section 4.1) and the single image to 3D task (Section 4.2), demonstrating qualitative and quantitative improvements over prior work. The design choices that led to CAT3D are ablated and discussed further in Section 4.3.

### Few-View 3D Reconstruction

We first evaluate CAT3D on five real-world benchmark datasets for few-view 3D reconstruction. Among those, CO3D [74] and RealEstate10K [75] are in-distribution datasets whose training splits were part of our training set (we use their test splits for evaluation), whereas DTU [77], LLFF [78] and the mip-NeRF 360 dataset [79] are out-of-distribution datasets that were not part of the training dataset. We tested CAT3D on the \(3\), \(6\) and \(9\) view reconstruction tasks, with the same train and eval splits as [7]. In Table 1, we compare to the state-of-the-art for dense-view NeRF reconstruction with no learned priors (Zip-NeRF [71]) and methods that heavily leverage generative priors such as ZeroNVS [42]

Figure 4: **Qualitative comparison, few-view reconstruction (renders).** A comparison of rendered reconstructions on scenes from mip-NeRF 36 (_top row_) and CO3D (_bottom row_), given \(3\) input captured views. Compared to ReconFusion [7], CAT3D better aligns with ground-truth in seen regions, while hallucinating plausible content in unseen regions. See supplemental website for additional comparisons.

[MISSING_PAGE_FAIL:8]

in-domain and out-of-domain datasets) and few-view 3D reconstruction performance. We also compare important design choices for 3D reconstruction. Results from our ablation study are reported in Table 3 and Figure 6 in Appendix A and summarized below. Overall, we found that video diffusion architectures, with 3D self-attention (spatiotemporal) and raymap embeddings of camera pose, produce consistent enough views to recover 3D representations when combined with robust reconstruction losses.

**Image and pose.** Previous work [7] used PixelNerf [63] feature-map conditioning for multiple input views. We found that replacing PixelNeRF with attention-based conditioning in a conditional video diffusion architecture using a per-image embedding of the camera pose results in improved samples and 3D reconstructions, while also reducing model complexity and the number of parameters. We found that embedding the camera pose as a low-dimensional vector (as in [37]) works well for in-domain samples, but generalizes poorly compared to raymap conditioning (see Section 3.1).

**Increasing the number of views.** We found that jointly modeling multiple output views (_i.e._, 5 or 7 views instead of 1) improves sample metrics -- even metrics that evaluate the quality of each output image _independently_. Jointly modeling multiple outputs creates more consistent views that result in an improved 3D reconstruction as well.

\begin{table}
\begin{tabular}{l c c} \hline \hline Model & Time (min) & CLIP (Image) \\ \hline ImageDream [9] & 120 & 83.77 \(\pm\) 5.2 \\ One2345++ [84] & 0.75 & 83.78 \(\pm\) 6.4 \\ IM-3D (NeRF) [17] & 40 & 87.37 \(\pm\) 5.4 \\ IM-3D [17] & 3 & **91.40**\(\pm\) 5.5 \\ CAT3D (ours) & 1 & 88.54 \(\pm\) 8.6 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluating image-to-3D quality with CLIP image scores on examples from [9] (numbers reproduced from [17]). CAT3D produces competitive results to object-centric baselines while also working on whole scenes. (Note: shown images are 3D renders, and time for CAT3D was evaluated on 16 A100 GPUs.)

Figure 5: **3D creation from single input images**. Renderings of 3D models from CAT3D (_middle row_) are higher quality than baselines (_bottom row_) for scenes, and competitive for objects. Note that scale ambiguity amplifies the differences in renderings between methods. See supplemental website for additional comparisons.

**Attention layers.** We found that 3D self-attention (spatiotemporal) is crucial, as it yields improved performance relative to factorized 2D self-attention (spatial-only) and 1D self-attention (temporal-only). While models with 3D self-attention in the finest feature maps (\(64\times 64\)) result in the highest fidelity images, they incur a significant computational overhead for training and sampling for relative small gain in fidelity. We therefore decided to use 3D self-attention only in feature maps of size \(32\times 32\) and smaller.

**Multi-view diffusion model training.** Initializing from a pre-trained text-to-image latent diffusion model improved performance on out-of-domain examples. We experimented with fine-tuning the multi-view diffusion model to multiple variants specialized for specific numbers of inputs and outputs views, but found that a single model jointly trained on 8 frames with either 1 or 3 conditioning views was sufficient to enable accurate single image and few-view 3D reconstruction.

**3D reconstruction.** LPIPS loss is crucial for achieving high-quality texture and geometry, aligning with findings in [17; 7]. On Mip-NeRF 360, increasing the number of generated views from 80 (single elliptical orbit) to 720 (nine orbits) improved central object geometry but sometimes introduced background blur, probably due to inconsistencies in generated content.

## 5 Discussion

We present CAT3D, a unified approach for 3D content creation from any number of input images. CAT3D leverages a multi-view diffusion model for generating highly consistent novel views of a 3D scene, which are then input into a 3D multi-view reconstruction pipeline. CAT3D decouples the generative prior from 3D extraction, leading to efficient, simple, and high-quality 3D generation.

Although CAT3D produces compelling results and outperforms prior works on multiple tasks, it has limitations. Because our training datasets have roughly constant camera intrinsics for views of the same scene, the trained model cannot handle test cases well where input views are captured by multiple cameras with different intrinsics. The generation quality of CAT3D relies on the expressivity of the base text-to-image model, and it performs worse in the cases where scene content is out of distribution for the base model (e.g. human faces, since the base model was trained on limited human data). The number of output views supported by our multi-view diffusion model is still relatively small, so when we generate a large set of samples from our model, not all views may be 3D consistent with each other (see Supplementary website). Finally, CAT3D uses manually-constructed camera trajectories that cover the scene thoroughly (see Appendix C), which may be difficult to design for large-scale open-ended 3D environments.

There are a few directions worth exploring in future work to improve CAT3D. The multi-view diffusion model may benefit from being initialized from a pre-trained video diffusion model, as observed by [10; 17]. The consistency of samples could be further improved by extending the number of conditioning and target views handled by the model. Automatically determining the camera trajectories required for different scenes could increase the flexibility of the system.

**Acknowledgements** We would like to thank Daniel Watson, Rundi Wu, Jason Y. Zhang, Richard Tucker, Jason Baldridge, Michael Niemeyer, Rick Szeliski, Dana Roth, Jordi Pont-Tuset, Andeep Torr, Irina Blok, Doug Eck, and Henna Nandwani for their valuable contributions to this work. We also extend our gratitude to Shlomi Fruchter, Kevin Murphy, Mohammad Babaeizadeh, Han Zhang and Amir Hertz for training the base text-to-image latent diffusion model.

## References

* [1] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. _ECCV_, 2020.
* [2] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _SIGGRAPH_, 2022.
* [3] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. _SIGGRAPH_, 2023.

* [4] Jiawei Yang, Marco Pavone, and Yue Wang. FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization. _CVPR_, 2023.
* [5] Nagabhushan Somraj, Adithyan Karanayil, and Rajiv Soundararajan. SimpleNeRF: Regularizing Sparse Input Neural Radiance Fields with Simpler Solutions. _SIGGRAPH Asia_, 2023.
* [6] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large Reconstruction Model for Single Image to 3D. _arXiv:2311.04400_, 2023.
* [7] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, and Aleksander Holynski. Reconfusion: 3d reconstruction with diffusion priors, 2023.
* [8] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion. _ICLR_, 2022.
* [9] Peng Wang and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation. _arXiv:2312.02201_, 2023.
* [10] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. _arXiv:2311.15127_, 2023.
* [11] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. _CVPR_, 2023.
* [12] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning. _arXiv:2311.10709_, 2023.
* [13] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space-time diffusion model for video generation. _arXiv_, 2024.
* [14] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models, 2023.
* [15] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024.
* [16] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T Barron, Amit H Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, et al. State of the art on diffusion models for visual computing. _arXiv:2310.07204_, 2023.
* [17] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos Kokkinos. IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation, 2024.
* [18] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model. _arXiv:2311.06214_, 2023.
* [19] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, and Lei Zhang. DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation. _arXiv_, 2023.
* [20] Peihao Wang, Zhiwen Fan, Dejia Xu, Dilin Wang, Sreyas Mohan, Forrest Iandola, Rakesh Ranjan, Yilei Li, Qiang Liu, Zhangyang Wang, et al. SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity. _arXiv_, 2023.
* [21] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin. Collaborative score distillation for consistent visual editing. _NeurIPS_, 36, 2024.

* [22] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. _NeurIPS_, 2023.
* [23] Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 19740-19750, 2023.
* [24] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. _ICCV_, 2023.
* [25] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-Resolution Text-to-3D Content Creation. _CVPR_, 2023.
* [26] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. _arXiv:2309.16653_, 2023.
* [27] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. _arXiv:2310.08529_, 2023.
* [28] Dave Epstein, Ben Poole, Ben Mildenhall, Alexei A Efros, and Aleksander Holynski. Disentangled 3d scene generation with layout learning. _arXiv preprint arXiv:2402.16936_, 2024.
* [29] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. ATT3D: Amortized Text-to-3D Object Synthesis. _ICCV_, 2023.
* [30] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from a single image. _CVPR_, 2023.
* [31] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors. _arXiv:2306.17843_, 2023.
* [32] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior. _ICCV_, 2023.
* [33] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Niessner. Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models. _ICCV_, 2023.
* [34] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David J Fleet. Monocular depth estimation using diffusion models. _arXiv:2302.14816_, 2023.
* [35] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. WonderJourney: Going from Anywhere to Everywhere. _arXiv:2312.03884_, 2023.
* [36] Ethan Weber, Aleksander Holynski, Varun Jampani, Saurabh Saxena, Noah Snavely, Abhishek Kar, and Angjoo Kanazawa. Nerfiller: Completing scenes via generative 3d inpainting. _arXiv preprint arXiv:2312.04560_, 2023.
* [37] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-Shot One Image to 3D Object. _arXiv_, 2023.
* [38] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. _arXiv:2210.04628_, 2022.
* [39] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Ben Mildenhall, Nataniel Ruiz, Shiran Zada, Kfir Aherman, Michael Rubenstein, Jonathan Barron, Yuanzhen Li, and Varun Jampani. DreamBooth3D: Subject-Driven Text-to-3D Generation. _ICCV_, 2023.

* [40] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion. _ICML_, 2023.
* [41] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. GeNVS: Generative novel view synthesis with 3D-aware diffusion models. _arXiv_, 2023.
* [42] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Image. _CVPR_, 2024.
* [43] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization. _arXiv_, 2023.
* [44] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. MVDream: Multi-view Diffusion for 3D Generation. _arXiv_, 2023.
* [45] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base model, 2023.
* [46] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion. _arXiv:2310.10343_, 2023.
* [47] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. SyncDreamer: Generating Multiview-consistent Images from a Single-view Image. _arXiv_, 2023.
* [48] Lukas Hollein, Aljaz Bozic, Norman Muller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollhofer, and Matthias Niessner. Viewdiff: 3d-consistent image generation with text-to-image models, 2024.
* [49] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv:2204.03458_, 2022.
* [50] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv:2210.02303_, 2022.
* [51] Siddhant Jain, Daniel Watson, Eric Tabellion, Aleksander Holyinski, Ben Poole, and Janne Kontkanen. Video interpolation with diffusion models. _arXiv preprint arXiv:2404.01203_, 2024.
* [52] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv preprint arXiv:2307.04725_, 2023.
* [53] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. _arXiv preprint arXiv:2312.03641_, 2023.
* [54] Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, Shweta Mahajan, and Kwang Moo Yi. ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models. _arXiv:2312.01305_, 2023.
* [55] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion, 2024.
* [56] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 3DGen: Triplane Latent Diffusion for Textured Mesh Generation. _arXiv:2303.05371_, 2023.
* [57] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data. _ICCV_, 2023.

* [58] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang. DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model, 2023.
* [59] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. _arXiv:2312.13150_, 2023.
* [60] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting. _arXiv:2404.19702_, 2024.
* [61] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv:1312.6114_, 2013.
* [62] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. _CVPR_, 2022.
* [63] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural Radiance Fields from One or Few Images. _CVPR_, 2021.
* [64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. _ICML_, 2021.
* [65] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. _NeurIPS_, 35, 2022.
* [66] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. _arXiv:2307.08691_, 2023.
* [67] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images. _ICML_, 2023.
* [68] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. _CVPR_, 2022.
* [69] David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In _ACM-SIAM Symposium on Discrete Algorithms_, 2007.
* [70] Ayush Sarkar, Hanlin Mai, Amitabh Mahapatra, Svetlana Lazebnik, David A Forsyth, and Anand Bhattad. Shadows Don't Lie and Lines Can't Bend! Generative Models don't know Projective Geometry... for now. _arXiv:2311.17138_, 2023.
* [71] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. _ICCV_, 2023.
* [72] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. _CVPR_, 2018.
* [73] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Obigavares: A universe of annotated 3d objects. _CVPR_, 2023.
* [74] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction. _ICCV_, 2021.
* [75] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. _SIGGRAPH_, 2018.
* [76] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. MVImgNet: A Large-scale Dataset of Multi-view Images. _CVPR_, 2023.

* [77] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanaes. Large scale multi-view stereopsis evaluation. _CVPR_, 2014.
* [78] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. _SIGGRAPH_, 2019.
* [79] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. _CVPR_, 2022.
* [80] Jaidey Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ramamoorthi. RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion, 2024.
* [81] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior. _arXiv_, 2023.
* [82] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers, 2023.
* [83] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from a single image, 2024.
* [84] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. _arXiv:2311.07885_, 2023.
* [85] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* [86] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.

Ablations

Here we conduct ablation studies over several important decisions that led to CAT3D. We consider several multi-view diffusion model variants, and evaluated them on novel-view synthesis with held-out validation sets (\(4\)k samples) from the training datasets (_in-domain samples_) and from the mip-NeRF 360 dataset (_out-of-domain samples_), as well as 3-view reconstruction on the mip-NeRF 360 dataset (_out-of-domain renders_). Unless otherwise specified, models in this section are trained with \(3\) input views and \(5\) output views, evaluated after \(120\)k optimization iterations. Quantitative results are summarized in Table 3. Then we discuss important 3D reconstruction design choices, with qualitative comparison in Figure 6.

Number of target views.We start from the setting where the model takes 3 conditional views as input and generates a single target view, identical to the ReconFusion baseline [7]. Results show that our multi-view architecture is favored when dealing with multiple input views, compared to the PixelNeRF used by [7]. We also show that going from a single target view to \(5\) target views results in a significant improvement on both novel-view synthesis and few-view reconstruction.

Camera conditioning.As mentioned in Section 3.1, we compared two camera parameterizations, one is an 8-dimensional encoding vector fed through cross-attention layers that contains relative position, relative rotation quaternion, and absolute focal length. The other is camera rays fed through channel-wise concatenation with the input latents. We observe that the latter performs better across all metrics.

Attention layers.An important design choice is the type and number of self-attention layers used when connecting multiple views. As shown in Table 3, it is critical to use 3D self-attention instead of temporal 1D self-attention. However, 3D attention is expensive; 3D attention at the largest feature maps (\(64\times 64\)) is of \(32\)k sequence length and this incurs a significant computation overhead during training and sampling with a marginal performance gain, especially for out-of-domain samples and renderings. We therefore chose to use 3D attention only for feature maps of size \(32\times 32\) and smaller.

Multi-view diffusion model training.We compared the settings of training the multi-view diffusion models from scratch with initializing from a pre-trained text-to-image latent diffusion model. The latter performs better, especially for out-of-domain cases. We further trained the model for more iterations and observed a consistent performance gain up until \(1\)M iterations. Then we fine-tuned the model for handling both cases of 1 conditional + 7 target views and 3 conditional + 5 target views (_jointly_), for another \(0.4\)M iterations. We found this joint finetuning leads to better in-domain novel-view synthesis results with \(3\) conditional views, and out-of-domain results that are on-par with the previous model.

3D reconstruction.We found perceptual distance (LPIPS) loss is crucial in recovering high-quality texture and geometry, similar to [17, 7]. We also compared the use of 80 views along one orbital path with 720 views along nine variably scaled orbital paths. In the Mip-NeRF 360 setting, increasing the number of views helps better regularize the geometry of central objects, but sometimes leads to blurrier textures in the background, due to inconsistencies in generated content.

## Appendix B Details of Multi-View Diffusion Model

We initialize the multi-view diffusion model from a latent diffusion model (LDM) trained for text-to-image generation similar to [62] trained on web scale image datasets. See Figure 7 for a visualization of the model architecture. We modify the LDM to take multi-view images as input by inflating the 2D self-attention after every 2D residual blocks to 3D self-attention [44]. Our model adds minimal additional parameters to the backbone model: just a few additional convolution channels at the input layer to handle conditioning information. We drop the text embedding from the original model. Our latent diffusion model has \(850\)M parameters, smaller than existing approaches built on video diffusion models such as IM-3D [17] (4.3B) and SV3D [55] (1.5B).

We fine-tune the full latent diffusion model for 1.4M iterations with a batch size of \(128\) and a learning rate of \(5\times 10^{-5}\). The first 1M iterations are trained with the setting of 1 conditional view and 7target views, while the rest \(0.4\)M iterations are trained with an equal mixture of 1 condition + 7 target views and 3 conditional + 5 target views. Our model was trained for \(16\) days on 128 TPU-v4 chips. Following [7] we draw training samples with equal probability from the four training datasets. We enable classifier-free guidance (CFG) [85] by randomly dropping the conditional images and camera poses with a probability of \(0.1\) during training.

## Appendix C Details of Generating Novel Views

We use DDIM [86] with \(50\) sampling steps and CFG guidance weight \(3\) for generating novel views. It takes 5 seconds to generate \(80\) views on 16 A100 GPUs. As mentioned in Section 3.2, selecting camera trajectories that fully cover the 3D scene is important for high-quality 3D generation results. See Figure 8 for an illustration of the camera trajectories we use. For the single image-to-3D setting we use two different types of camera trajectories, each containing \(80\) views:

* A spiral around a cylinder-like trajectory that moves into and out of the scene.
* An orbit trajectory for images with a central object.

For few-view reconstruction, we create different trajectories based on the characteristics of different datasets:

* RealEstate10K: we create a spline path fitted from the input views, and shift the trajectories along the \(xz\)-plane by certain offsets, resulting in 800 views.
* LLFF and DTU: we create a forward-facing circle path fitted from all views in the training set, scale it, and shift along the \(z\)-axis by certain offsets, resulting in 960 and 480 views respectively.
* CO3D: we create a spline path fitted from the input views, and scale the trajectories by multiple factors, resulting in 640 views.
* Mip-NeRF 360: we create a elliptical path fitted from all views in the training set, scale it, and shift along the \(z\)-axis by certain offsets, resulting in 720 views.

For generating anchor views in the single-image setting, we used the model with 1 conditional input and 7 target outputs. For generating the full set of views (both in the single-image anchored setting,

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**In-domain**} & \multicolumn{3}{c}{**Out-of-domain**} \\ \cline{2-10}  & \multicolumn{3}{c}{**diffusion samples**} & \multicolumn{3}{c}{**diffusion samples**} & \multicolumn{3}{c}{**NeRF renderings**} \\ \cline{2-10}
**Setting** & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS \\ \hline
**Baseline** & & & & & & & & & \\ ReconFusion [7] & — & — & — & 14.01 & 0.265 & 0.568 & 15.49 & 0.358 & 0.585 \\ \hline
**\# target views** & & & & & & & & & \\
3 cond 1 tgt & 18.85 & 0.638 & 0.359 & 14.12 & 0.262 & 0.553 & 16.17 & 0.360 & 0.546 \\
3 cond 5 tgt & **21.66** & **0.733** & **0.277** & **14.63** & **0.278** & **0.515** & **16.29** & **0.368** & **0.530** \\ \hline
**Camera conditioning** & & & & & & & & & \\ Low-dim vector & 21.17 & 0.710 & 0.304 & 14.19 & 0.266 & 0.530 & 15.97 & 0.359 & 0.544 \\ Raymap & **21.66** & **0.733** & **0.277** & **14.63** & **0.278** & **0.515** & **16.29** & **0.368** & **0.530** \\ \hline
**Attention layers** & & & & & & & & & \\ Temporal attention & 18.62 & 0.653 & 0.362 & 13.41 & 0.250 & 0.582 & 15.03 & 0.330 & 0.595 \\
3D attention until \(16\times 16\) & 21.41 & 0.730 & 0.281 & 14.23 & 0.274 & 0.530 & 16.21 & 0.364 & 0.541 \\
3D attention until \(32\times 32\) & 21.66 & 0.733 & 0.277 & 14.63 & **0.278** & 0.515 & 16.29 & **0.368** & 0.530 \\
3D attention until \(61\times 61\) (full) & **22.83** & **0.783** & **0.235** & **14.64** & 0.274 & **0.509** & **16.35** & 0.367 & **0.528** \\ \hline
**Model training** & & & & & & & & & \\ From scratch, 3 cond 5 tgt & 21.16 & 0.722 & 0.282 & 13.88 & 0.255 & 0.546 & 15.68 & 0.348 & 0.557 \\ From pretrained, 3 cond 5 tgt & 21.66 & 0.733 & 0.277 & 14.63 & 0.278 & 0.515 & 16.29 & 0.368 & 0.530 \\ From pretrained, 3 cond 5 tgt, 1M iters & 22.49 & 0.757 & 0.256 & **15.19** & **0.303** & **0.482** & 16.58 & **0.384** & **0.509** \\ From pretrained, jointly, 1.4M iters & **22.96** & **0.777** & **0.235** & 15.15 & 0.294 & 0.488 & **16.62** & 0.377 & 0.515 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Ablation study of multi-view diffusion models.** A comparison of model variants and parameters settings across _in-domain_ sequences (a mixture of Objavverse, Co3D, RealEstate10k and MVImgNet — all sequences in the corresponding eval sets of our training data) and _out-of-domain_ sequences (examples from the mip-NeRF 360 dataset). We evaluate the quality of _samples_ from the diffusion model, as well as _renderings_ from the subsequently optimized NeRF.

as well as in the multi-view setting), we used the model with 3 conditional inputs and groups of 5 target outputs, selected by their indices in the camera trajectories.

Anchor selection.For single-image conditioning, we first select a set of target viewpoints as _anchors_ to ground the scene content. To select a set of anchor views that are spread throughout the scene and provide good coverage, we use the initialization strategy from [69]. This method greedily selects the camera whose position is furthest away from the already selected views. We found this to work well on a variety of trajectory-like view sets as well as random views that have been spread throughout a scene.

Dealing with non-square images.While the multi-view latent diffusion model we trained only supports \(512\times 512\) square images, we found the model still performed well when padding non-square images to square. However, this method often reduces resolution, so we also run our model on a square-cropped version of the inputs, and then compose the square-cropped outputs with the edges from the padded outputs to create a different aspect ratio image.

## Appendix D Details of 3D Reconstruction

For the Zip-NeRF [71] baseline, we follow [7] to make a few modifications of the hyperparameters (See Appendix D in [7]) that better suit the few-view reconstruction setting. We use a smaller view dependence network with width 32 and depth 1, and a smaller number of training iterations of 1000, which helps avoid overfitting and substantially speeds up the training and rendering. Our synthetic view sampling and 3D reconstruction process is run on \(16\) A100 GPUs. For few-view reconstruction,

Figure 6: **Qualitative comparison of 3D reconstruction design choices. Rendered images (left) and depth maps (right) of a Mip-NeRF 360 scene under different settings: (a) 720 generated views along multiple orbital paths, (b) 80 generated views on a single orbital path, and (c) 720 views, without the perceptual (LPIPS) loss.**

we sample \(128\times 128\) patches of rays and train with a global batch size of \(1\)M that takes \(4\) minutes to train. For single image to 3D, we use \(32\times 32\) patches of rays and a global batch size of \(65\)k that takes \(55\) seconds to train. Learning rate is logarithmically decayed from \(0.04\) to \(10^{-3}\). The weight of the perceptual loss (LPIPS) is set to \(0.25\) for single image to 3D and few-view reconstruction on RealState10K, LLFF and DTU datasets, and to \(1.0\) for few-view reconstruction on CO3D an MipNeRF-360 daateats.

Distance based weighting.We design a weighting schedule that upweights views closer to captured views in the later stage of training to improve details. Specifically, the weighting is given by a Gaussian kernel: \(w\propto\exp\bigl{(}-bs^{2}\bigr{)}\), where \(s\) is the distance to the closest captured view and \(b\) is a scaling factor. For few-view reconstruction, \(b\) is linearly annealed from \(0\) to \(15\). We also anneal the weighting of generated views globally to further emphasize the importance of captured views.

Figure 7: **Illustration of the network.** CAT3D builds on a latent text-to-image diffusion model. The input images of size \(512\) are \(8\times\) downsampled to latents of size \(64\times 64\), which are concatenated with the relative camera raymap and a binary mask that indicates whether or not the image has been observed. A 2D U-Net with temporal connections is utilized for building the latent diffusion model. After each residual block with resolution \(\leq 32\times 32\), we inflate the original 2D self-attention (spatial) of the text-to-image model to be 3D self-attention (spatiotemporal). We remove the text embedding conditioning of the original model.

Figure 8: **Camera trajectories for generating novel views.** Within each panel, left shows the side view and right shows the top view of the trajectories, colored by indices of views. (a)-(b): two types of trajectories used by single image to 3D. Observed view is highlighted in red, while anchor views are highlighted in orange. (c)-(g): trajectories used by 3D reconstruction. 3 input views are highlighted in red.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims around performance, runtime, and comparison to existing methods are described in the experiments. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our method for certain categories, generalizing to different camera intrinsics, generating large sets of consistent views, and requirement of manually constructed camera trajectories in the discussion section. We also describe limitations in the experiments section, e.g. image-to-3D for segmented objects having room for improvement. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: No theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We aimed to provide the full details necessary to reproduce these results, including architecture details and a diagram, description of the training datasets and procedures, evaluation protocols for 3D, and several detailed appendices. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We have not open sourced the code used in this work, but the datasets we used are all publicly available (Re10K, CO3D, MVImgNet, Obiayerse). Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide details on the diffusion model architecture in Appendix B, including learning rate and optimization hyperparameters, data mixtures, CFG masking probability, and more. The dataset splits were used and shared in previous work. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: For few-view reconstruction metrics, we follow prior work and do not report error bars. For single-image CLIP metrics we use standard deviation over prompts, as in prior work. Evaluation of 3D generative models is challenging, and better metrics are an active area of research. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: For 3D reconstruction, Appendix D describes the compute requirements and iterations. For diffusion model training, the compute requirements are specified in Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We believe we have followed the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No]Justification: We currently discuss mostly the positive impacts of this work, but can add more acknowledgement of the potential for harm caused by 3D generative models if the reviewers believe it is useful. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: We do not plan to release these models and have not planned safeguards around the release. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the papers that created the datasets we used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We produce 3D assets in the form of videos and 3D models that have been linked to in the supplementary material. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Paper does not involve crowdsourcing nor research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.