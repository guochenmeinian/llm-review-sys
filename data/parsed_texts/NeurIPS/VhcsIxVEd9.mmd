# DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions

Haochen Wang\({}^{1,3}\)  Junsong Fan\({}^{1,4}\)  Yuxi Wang\({}^{1,4}\)  Kaiyou Song\({}^{2}\)

**Tong Wang\({}^{2}\)  Zhaoxiang Zhang\({}^{1,3,4}\)**

\({}^{1}\)Center for Research on Intelligent Perception and Computing (CRIPAC),

State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS),

Institute of Automation, Chinese Academy of Sciences (CASIA)

\({}^{2}\)Megvii Technology \({}^{3}\)University of Chinese Academy of Sciences (UCAS)

\({}^{4}\)Centre for Artificial Intelligence and Robotics, HKISI_CAS

{wanghaochen2022, junsong.fan, zhaoxiang.zhang}@ia.ac.cn

yuxiwang93@gmail.com {songkaiyou, wangtong}@megvii.com

Corresponding author.

###### Abstract

As it is empirically observed that Vision Transformers (ViTs) are quite insensitive to the order of input tokens, the need for an appropriate self-supervised pretext task that enhances the location awareness of ViTs is becoming evident. To address this, we present DropPos, a novel pretext task designed to _reconstruct **Dropped Positions**_. The formulation of DropPos is simple: we first drop a large random subset of positional embeddings and then the model _classifies the actual position_ for each non-overlapping patch among all possible positions _solely_ based on their visual appearance. To avoid trivial solutions, we increase the difficulty of this task by keeping only a _subset_ of patches visible. Additionally, considering there may be different patches with similar visual appearances, we propose position smoothing and attentive reconstruction strategies to relax this classification problem, since it is _not_ necessary to reconstruct their _exact_ positions in these cases. Empirical evaluations of DropPos show strong capabilities. DropPos outperforms supervised pre-training and achieves competitive results compared with state-of-the-art self-supervised alternatives on a wide range of downstream benchmarks. This suggests that explicitly encouraging spatial reasoning abilities, as DropPos does, indeed contributes to the improved location awareness of ViTs. The code is publicly available at https://github.com/Haochen-Wang409/DropPos.

## 1 Introduction

Learning extensible visual representations without any human annotations, known as self-supervised learning (SSL), has become a research hotspot in the field of computer vision [7, 9, 26, 27, 29, 47, 59, 62] since it enables efficient transfer to various downstream benchmarks. To achieve this goal, researchers carefully design visual pretext tasks to produce appropriate supervision using _only_ images [3, 18, 46, 52, 69, 70]. Two popular approaches among these are contrastive learning (CL) [62] and masked image modeling (MIM) [3], both of which have demonstrated promising scaling behavior of vision models, particularly Vision Transformers (ViTs) [21]. Despite the success of these methods, ViTs are found to be relatively _insensitive_ to the order of input tokens [13, 67, 43], leading to the hypothesis that they tend to model the relationship between a set of _unordered_ input tokens. Therefore, a natural question arises: beyond the current CL and MIM paradigms, _whether a pretext task that explicitly enhances the positional awareness of ViTs can further improve their representation learning abilities?_To answer this question, we begin by revisiting the forward procedure of ViTs. A sequence of positional embeddings (PEs) [51] is added to patch embeddings to preserve position information. Intuitively, simply _discarding_ these PEs and requesting the model to reconstruct the position for each patch naturally becomes a qualified location-aware pretext task. By forcing the model _only_ to utilize visual appearance for position reconstruction, the model is supposed to learn the shape relationships and layouts of image contents, thereby improving the ability to encode spatial visual content. However, despite the simplicity and intuitiveness of this idea, position-related methods such as [18, 44] have fallen far behind. We identify several difficulties in designing this type of paradigm:

_(i)_ Discarding all PEs brings _discrepancies between pre-training and fine-tuning_ because the model has _never_ been exposed to any PEs during pre-training. _(ii)_ Strengths of ViTs in modeling long-range dependencies may cause them to solve the task superficially, leading to trivial solutions that _fail to learn highly semantic representations by solving this simple task. _(iii)_ Patches with similar visual appearances may result in confusing reconstruction targets, making it crucial to _decide which position to reconstruct precisely._

Driven by this analysis, we present a novel, straightforward, and highly effective pretext task for self-supervised visual representation learning: _reconstructing Dropped Positions_ (DropPos). DropPos tackles the above issues systematically: _(i)_ To address discrepancies, simply _dropping_ a large subset of PEs instead of discarding all of them works effectively. _(ii)_ To avoid trivial solutions, we only keep a _subset_ of patches visible during pre-training, forcing ViTs to reconstruct the position of each visible patch with only _partial_ inputs. _(iii)_ To prevent ViTs from being overwhelmed by this particular task, we propose to relax this problem using position smoothing and attentive reconstruction strategies.

DropPos can be easily formulated into a simple patch-wise classification problem. Here is how it works: First, input patches are randomly masked and a _large_ subset of PEs corresponding to these visible patches is dropped. Then, ViTs are tasked with classifying the positions of visible patches among all possible candidates, relying on _partial_ observations and a few anchors (_i.e._, patches with unmasked PEs). Fig. 1 illustrates two benefits of DropPos: _(i)_ requiring less prior knowledge of data augmentation techniques compared to CL, and _(ii)_ eliminating the need for a careful selection of target representation [39] and mask strategy [54, 8, 56] as MIM does.

We conducted extensive experiments to evaluate the performance of DropPos in comparison to state-of-the-art alternatives across various downstream tasks. With only 800 epochs of pre-training, DropPos achieves 84.2% top-1 accuracy on ImageNet-1K [48] validation set using ViT-B/16 [21], outperforming MAE [28] pre-trained with 1600 epochs by +0.6%. Moreover, on COCO [38] object detection/segmentation, and ADE20k [71] semantic segmentation benchmarks, which requires more abilities of spatial reasoning, DropPos surpasses MAE [28]_consistently_.

## 2 Related work

**Self-supervised learning.** The key challenge of self-supervised learning is the design of pretext tasks [18, 28, 32, 42, 44, 45, 50, 54, 69], how to produce appropriate supervision signals using _only_ images. Among them, contrastive learning [5, 9, 26, 29, 58, 60, 62, 66] has been popular recently, which seeks to maximize agreement between different views of the same images. Nevertheless, the

Figure 1: **Comparison between different pretext tasks.****(a)** Contrastive learning aims to maximize the agreement between different views of one image. **(b)** Masked image modeling predicts specific contents of masked patches. **(c)** DropPos reconstructs positions _solely_ based on visual appearance.

performance of contrastive-related methods highly depends on carefully-designed data augmentation techniques [5, 6, 11, 12, 72]. Another mainstream is masked image modeling [1, 3, 10, 28, 54, 56], which involves predicting _specific contents_ masked patches [2, 3, 19, 20, 28, 54, 64, 72]. Unlike these methods, DropPos offers a novel alternative for self-supervised ViTs: reconstructing dropped positions based on _only_ visual clues. In this way, the model is urged to learn the spatial arrangement of local image patches, contributing to better localization abilities, which is important for spatial reasoning recognition tasks, _e.g._, segmentation [57, 58, 22, 53] and video understanding [17, 25].

**Position prediction in self-supervised learning.** Doersch _et al._[18] first split an image into 3\(\times\)3 grids and train a network to predict the _relative position_ of paired patches from the same image. This problem is formulated as an 8-way classification. Noroozi and Favaro [44] extended this approach to solve "jigsaw puzzles" by urging the model to _predict the order_ of _shuffled_ non-overlapping crops. These approaches were originally developed using ConvNets [33] and thus lacked the ability to learn long-range dependencies, which is crucial for position prediction. Very recently, Zhai _et al._[67] revisited this type of pretext task in the scope of Transformers [51] by pre-training ViTs [21] to predict patch positions _solely_ based on their visual appearance, while discarding positional embeddings. However, these pre-trained models have _never_ been exposed to positional embeddings, leading to discrepancies between pre-training and fine-tuning, and thus their performances are significantly inferior to state-of-the-art alternatives [6, 13, 28, 54]. Sameni _et al._[49] come up with an auxiliary position-related objective and combine it with the popular contrastive learning paradigm. Caron _et al._[4] extended this idea by predicting the relative location of a _query_ (_i.e._, local crop) to the corresponding _reference_ (_i.e._, global crop). However, [4]_only_ examined the effectiveness on segmentation benchmarks [15, 24, 41, 71]. In contrast, DropPos incorporates a novel dropped position reconstruction objective and thus achieves competitive performances on a variety of downstream tasks, including classfication [48], detection [38], and segmentation [38, 71].

**Positional embeddings in Vision Transformers.** In the standard forward procedure of ViTs [21], learnable positional embeddings (PEs) are added with patch embeddings. Improving PEs and introducing more inductive bias into ViTs has become an active research area [23]. This is because some research shows that ViTs are surprisingly robust against permutations of the order of input patches. For instance, Chen _et al._[13] demonstrate that the model performs well in classification even with _no_ position embedding, suggesting that ViTs have _not_ fully leveraged the positional information. Additionally, Naseer _et al._[43] shows that ViTs are much _less susceptible_ to patch shuffling perturbations than ConvNets. Therefore, we propose DropPos, a novel self-supervised pretext task that explicitly makes ViTs location-aware and promotes the emergence of spatial features.

## 3 Method

**Preliminary: Vision Transformers.** Given an image \(\mathbf{I}\in\mathbb{R}^{H\times W\times C}\), it is first reshaped into a sequence of patches \(\mathbf{I}_{p}\in\mathbb{R}^{N\times(P^{2}C)}\), where \((H,W)\) indicates the spatial resolution, \(C\) is the number of channels, \(P\) is the patch size, and \(N=HW/P^{2}\) is the number of patches. A linear projection is then applied to \(\mathbf{I}_{p}\), mapping it to \(D\) dimensions to get patch embeddings \(\mathbf{x}\in\mathbb{R}^{N\times D}\). Also, a \(\llbracket\texttt{CLS}\rrbracket\) token \(\mathbf{x}_{\text{cls}}\in\mathbb{R}^{D}\) is used to aggregate the information. Position embeddings \(\mathbf{p}\in\mathbb{R}^{(N+1)\times D}\) are added to the patch embeddings to retain positional information. Then, \(\mathbf{z}=[\mathbf{x}_{\text{cls}};\mathbf{x}]\oplus\mathbf{p}\) is the input of transformer blocks [51] which consists of a stack of multi-head self-attention mechanisms, where \(\oplus\) denotes element-wise plus. In particular, \(\mathbf{p}_{0}\) denotes the position embeddings of the \(\llbracket\texttt{CLS}\rrbracket\) token, which will never be dropped.

### DropPos

Fig. 2 illustrates our proposed DropPos. We first randomly generate a binary patch mask \(\mathbf{M}\in\{0,1\}^{N}\), where 1 means visible and 0 means masked, respectively. Note that the \(\llbracket\texttt{CLS}\rrbracket\) token, _i.e._, \(\mathbf{z}_{0}\), is _always_ visible in our setting, and thus the length of \(\mathbf{M}\) is \(N\) instead of \(N+1\) for simplicity. Given a pre-defined mask ratio \(\gamma\in(0,1)\), \(\mathbf{M}\) is supposed to subject to

\[\sum_{i=0}^{N-1}\mathbf{M}_{i}=(1-\gamma)N,\] (1)

which means there are \(\gamma N\) masked patches in total. It is worth noticing that we use patch masks to increase the difficulty of the pretext task, which is quite different from building a pretext task to recover input images in MIM [3, 28]. Then, we denote \(\mathbf{x}_{\mathrm{vis}}=\mathtt{gather}(\mathbf{x},\mathbf{M})\in\mathbb{R}^{(1- \gamma)N\times D}\) to be the visible tokens, and position embeddings (PEs) for these unmasked tokens are randomly dropped, where \(\mathtt{gather}(\cdot,\cdot)\) means only those unmasked patches (_i.e._, \(\mathbf{M}_{i}=1\)) are reserved. To address possible discrepancies, we aim to _drop_ a subset of PEs of these visible patches instead of simply discarding all of them. Thus, we randomly generate a binary positional mask \(\mathbf{M}_{\mathrm{pos}}\in\{0,1\}^{(1-\gamma)N}\). Given a pre-defined position mask ratio \(\gamma_{\mathrm{pos}}\in(0,1)\), \(\mathbf{M}_{\mathrm{pos}}\) is supposed to subject to

\[\sum_{i=0}^{(1-\gamma)N-1}\mathbf{M}_{\mathrm{pos}}^{i}=(1-\gamma_{\mathrm{ pos}})(1-\gamma)N,\] (2)

which indicates that the model is supposed to reconstruct \(\gamma_{\mathrm{pos}}(1-\gamma)N\) dropped positions based on remaining \((1-\gamma_{\mathrm{pos}})(1-\gamma)N\) anchor patches and the [CLS] token. PEs for visible patches \(\mathbf{p}_{\mathrm{vis}}=\mathtt{gather}(\mathbf{p}_{1:N},\mathbf{M})\) are gathered first. Then, we aim to obtain the final PEs \(\mathbf{p}^{\prime}\in\mathbb{R}^{(1-\gamma)N\times D}\) by replacing those dropped positions with a learnable [MASK] token:

\[\mathbf{p}_{i}^{\prime}=\begin{cases}\mathbf{p}_{0},&\text{if }i=0,\\ \mathbf{p}_{\mathrm{vis}}^{i-1},&\text{if }\mathbf{M}_{\mathrm{pos}}^{i-1}=1,\\ \mathbf{p}_{\mathrm{mask}},&\text{otherwise},\end{cases}\] (3)

where \(i=0,1,\ldots,(1-\gamma)N\) is the patch index, and \(\mathbf{p}_{\mathrm{mask}}\in\mathbb{R}^{D}\) is the [MASK] token. Finally, the input of the Transformer encoder is \(\mathbf{z}^{\prime}=[\mathbf{x}_{\mathrm{cls}};\mathbf{x}_{\mathrm{vis}}] \oplus\mathbf{p}^{\prime}\in\mathbb{R}^{((1-\gamma)N+1]\times D}\).

**Implementation.** Pre-training ViTs using our DropPos can be implemented efficiently and requires few specialized operations. The process involves three stages prior to forwarding features to the encoder. First, we randomly mask a subset of embedded patch tokens. Following MAE [28], the list of tokens is shuffled randomly, and the last portion of the list is removed. Next, positional embeddings of those unmasked patches are randomly dropped using the _same_ masking operation. Then, positional mask tokens are introduced to obtain the final positional embeddings, which are added to the patch tokens finally and encoded by ViTs followed by a lightweight decoder. A simple classification objective is applied to those unmasked patches introduced as follows.

```
#x:embeddedpatchtokens
#gamma_pos:patch/positionmaskratio
#mask_token:learnable[MASK]token
#patchmasking,[CLS]tokeniskeptvisible
#mask_vis_ =masking(x,gamma)
#x_vis_ = gather(x,ids_vis)
#positiondroppingforallpatches
#positiondrops_embed-#gather(pos_embed,ids_vis)
#ids_reps_ = masking(pos_embed,gamma_pos)
#embed=gather(pos_embed,ids_keep)
#concatwithmasktokens
#concatwithmask tokens
#pos_embed-#cat([pos_embed,mask_token])
#retstore
#edge_embed-#gather(pos_embed,ids_res)
#addtopatchtokensforforwardpass returnx_vis+pos_embed ```

**Algorithm 1** Pseudo-Code of DropPos.

Figure 2: **Illustration of DropPos. We first mask a large random subset of input images. Then, positional embeddings of visible patches are randomly dropped and [MASK] tokens are introduced. A lightweight projector is adopted to _reconstruct_ those dropped positions. A simple patch-wise classification objective is adopted and \(\mathtt{gray}\) position targets do not contribute to training.**

### Pre-training DropPos

DropPos is pre-trained by reconstructing dropped positions. Concretely, as illustrated in Fig. 2 and presented in Sec. 3.1, an ViT [21]\(f_{\theta}\) parameterized by \(\theta\), takes \(\mathbf{z}^{\prime}\) as input. A lightweight decoder \(g_{\psi}\) parameterized by \(\psi\) then projects \(f_{\theta}(\mathbf{z}^{\prime})\) to patch-wise position predicitons \(\mathbf{o}=g_{\psi}(f_{\theta}(\mathbf{z}^{\prime}))\in\mathbb{R}^{(1-\gamma) N\times N}\). We omit the [CLS] token here for simplicity. For each _visible_ patch \(i\in\{0,1,\dots,(1-\gamma)N-1\}\), \(\mathtt{softmax}(\mathbf{o}_{i})\in\mathbb{R}^{N}\) represents the probability density function of the predicted positions. Next, we have to produce appropriate ground truths, _i.e._, the actual position for each visible patch. To this end, we gather positions via \(\mathbf{y}=\mathtt{gather}([0,1,\dots,N-1],\mathbf{M})\in\mathbb{R}^{(1-\gamma )N}\), where vector \([0,1,\dots,N-1]\) is naturally the actual position for all patches before masking (w/o the [CLS] token). Overall, the objective is simply the vanilla cross-entropy loss given true positions \(\mathbf{y}_{ij}\):

\[\mathcal{L}=-\sum_{i=0}^{(1-\gamma)N-1}\sum_{j=0}^{N-1}(1-\mathbf{M}_{\mathrm{ pos}}^{i})\cdot\mathtt{one\_hot}(\mathbf{y}_{ij})\cdot\log\left[\frac{\exp( \mathbf{o}_{ij})}{\sum_{k=0}^{(1-\gamma)N-1}\exp(\mathbf{o}_{ik})}\right].\] (4)

_Only_ patches with dropped positions, _i.e._, \(\mathtt{orange\_patches}\) in Fig. 2, contribute to this objective, which is reflected by the term \((1-\mathbf{M}_{\mathrm{pos}}^{i})\) in Eq. (4). Next, several techniques are proposed to prevent ViTs from being overwhelmed by this particular task, making them pay more attention to model spatial dependencies instead of simply reconstructing positions.

**Position smoothing.** Considering that different categories (_i.e._, positions) are _not_ completely independent under this setting, the classification problem in Eq. (4) is relaxed when the model has predictions _close_ to the actual position. Specifically, a weight matrix \(\mathbf{w}\in(0,1)^{N\times N}\) is defined to measure similarities between different patches:

\[w(i,j)=\exp\left(-\frac{\mathrm{dist}(i,j)}{\sigma^{2}}\right),\] (5)

where \(\mathrm{dist}(i,j)\) means the Euclidean distance between patch \(i\) and \(j\), and \(\sigma\) is a hyper-parameter. The matrix then should be normalized by \(w^{*}(i,j)=w(i,j)/\left(\sum_{k=0}^{N-1}w(i,k)\right)\). Here, \(\mathbf{w}_{i}^{*}\in\mathbb{R}^{N}\) indicates the smoothed ground truth when the actual position is \(i\). Overall, the smoothed objective is:

\[\mathcal{L}_{\mathrm{smooth}}=-\sum_{i=0}^{(1-\gamma)N-1}\sum_{j=0}^{N-1}(1- \mathbf{M}_{\mathrm{pos}}^{i})\cdot w^{*}(\mathbf{y}_{ij},j)\cdot\log\left[ \frac{\exp(\mathbf{o}_{ij})}{\sum_{k=0}^{(1-\gamma)N-1}\exp(\mathbf{o}_{ik})} \right].\] (6)

Furthermore, we relax the problem in Eq. (6) by setting a large \(\sigma\) at the early stage, and we gradually _decay_ the value of \(\sigma\) to produce a challenging pretext task. A simple linearly decay is performed:

\[\sigma_{t}=\frac{t}{T}(\sigma_{T}-\sigma_{0})+\sigma_{0},\] (7)

where \(t\) and \(T\) denote the iteration and the number of total steps, respectively. \(\sigma_{0}\) and \(\sigma_{T}\) are the initial value and final value of \(\sigma\), respectively. We set \(\sigma_{0}=1\) and \(\sigma_{T}=0\) by default.

**Attentive reconstruction.** Since there may be different patches that share similar visual appearance (_e.g._, two blue patches that represent the sky), it is _not_ necessary to reconstruct their _exact_ positions. Simply swapping these patches still maintains reasonable visual coherence. Therefore, we leverage the feature similarities between the [CLS] token and patch tokens to be an extra weight of Eq. (6):

\[\mathcal{L}_{\mathrm{smooth}}^{\mathrm{attn}}=-\sum_{i=0}^{(1-\gamma)N-1} \sum_{j=0}^{N-1}(1-\mathbf{M}_{\mathrm{pos}}^{i})\cdot A_{\mathbf{y}_{ij}} \cdot w^{*}(\mathbf{y}_{ij},j)\cdot\log\left[\frac{\exp(\mathbf{o}_{ij})}{ \sum_{k=0}^{(1-\gamma)N-1}\exp(\mathbf{o}_{ik})}\right],\] (8)

where \(\mathbf{A}\in\mathbb{R}^{N}\) denotes the similarity matrix. Concretely, \(A_{i}\) means the affinity between the [CLS] token and patch token \(i\). Let \(\mathbf{f}_{\mathrm{cls}}\in\mathbb{R}^{D}\) and \(\mathbf{f}\in\mathbb{R}^{N\times D}\) be the features output by the encoder \(f_{\theta}\), and thus the affinity \(A_{i}\) is computed by

\[\mathbf{A}_{i}=\frac{\exp(\cos(\mathbf{f}_{\mathrm{cls}},\mathbf{f}_{i})/\tau) }{\sum_{j=0}^{N-1}\exp(\cos(\mathbf{f}_{\mathrm{cls}},\mathbf{f}_{j})/\tau)},\] (9)

where \(\tau\) indicates the temperature parameter and is set to 0.1 by default.

## 4 Experiments

**Pre-training.** We perform self-supervised pre-training on the ImageNet-1K [48] training set with a resolution of 224\(\times\)224. By default, we take ViT-B/16 [72] as the backbone and perform 200 epochs of pre-training. The decoder is a stack of Transformer [51] and has a depth of 2 and a width of 512. Patch mask ratio \(\gamma\) and position mask ratio \(\gamma_{\mathrm{pos}}\) are both 0.75 by default. Our implementation is based on HPM [54]. Details can be found in _Supplementary Material_.

**Evaluation.** We perform supervised training to evaluate our DropPos with end-to-end fine-tuning on ImageNet-1K [48] for classification. By default, 100 epochs of fine-tuning are performed following common practices [28, 54] for ViT-B/16 [21]. We report top-1 validation accuracy of a single 224\(\times\)224 crop. As for COCO [38], following previous methods [28, 54], we take Mask R-CNN [30] with FPN [37] as the detector. We perform end-to-end fine-tuning on COCO [38] for 1\(\times\) schedule with 1024\(\times\)1024 resolution. We report AP\({}^{\mathrm{b}}\) for object detection and AP\({}^{\mathrm{m}}\) for instance segmentation, respectively. Our implementation is based on detectron2 [61] and ViTDet [36]. For ADE20k [71], following previous methods [28, 54], we take UperNet [63] as the decoder and perform end-to-end fine-tuning on ADE20k [71] with 512\(\times\)512 resolution for 80k iterations. We take mIoU [24] as the main evaluation metric. Our implementation is based on mmsegmentation [14].

### Ablation studies

In Tabs. 1 to 4, we take the ViT-B/16 [21] pre-trained with 200 epochs on ImageNet-1K [48] as the backbone. We highlight the default settings of our DropPos. Concretely, if not specified, the default setting is \(\gamma=0.75\), \(\gamma_{\mathrm{pos}}=0.75\), \(\sigma_{0}=1\), \(\sigma_{T}=0\), and \(\tau=0.1\). By default, 100 epochs of fine-tuning on ImageNet-1K [48], 1\(\times\) schedule fine-tuning on COCO [38], and 80k iterations fine-tuning on ADE20k [71] are performed.

To evaluate the performance on the pretext task, _i.e._, reconstructing dropped positions, we also report the _averaged_ top-1 accuracy of position predictions on the ImageNet-1K _validation_ set in Tabs. 1 to 4. We vary \(\gamma\in\{0,0.25,0.5,0.75\}\) and \(\gamma_{\mathrm{pos}}\in\{0.25,0.5,0.75,0.95\}\) when measuring the position prediction accuracy, and average the position accuracy among 16 different cases.

**Main properties.** Illustrated by Tabs. 1 to 4, we find sufficient evidence to support the three difficulties claimed in Sec. 1. _(i)_ In Tab. 1, _ViTs fail to learn highly semantic representations by simply solving the position reconstruction task_, because the performances of \(\gamma=0\) significantly lag behind. _(ii)_ _Discrepancies between pre-training and fine-tuning_ are revealed in Tab. 2 by setting

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{\(\gamma\)} & \multicolumn{2}{c}{ImageNet-1K} & \multicolumn{2}{c}{COCO detection} & \multicolumn{2}{c}{COCO segmentation} & \multicolumn{2}{c}{ADE20k} \\ \cline{2-11}  & fine-tune & position & \(\mathbf{AP}^{\mathrm{b}}_{50}\) & \(\mathrm{AP}^{\mathrm{b}}_{55}\) & \(\mathbf{AP}^{\mathrm{m}}_{55}\) & \(\mathbf{AP}^{\mathrm{m}}_{50}\) & \(\mathrm{AP}^{\mathrm{m}}_{55}\) & \(\mathbf{m}_{55}\) & **mIoU** & aAcc \\ \hline
0.00 & 81.94 & 59.79 & **34.10** & 52.80 & 37.17 & **31.24** & 50.27 & 33.03 & **31.66** & 75.86 \\
0.25 & 82.02 & 79.62 & 37.05 & 56.23 & 40.18 & **33.60** & 53.51 & 35.84 & **32.80** & 76.92 \\
0.50 & 82.78 & 87.27 & **38.45** & 57.83 & 41.87 & **34.88** & 55.14 & 37.50 & **36.33** & 78.25 \\
0.75 & **82.96** & 87.83 & **42.14** & 61.99 & 46.38 & **37.93** & 59.23 & 40.76 & **40.68** & 80.14 \\
0.90 & 82.81 & 65.12 & **41.06** & 60.92 & 44.57 & **37.04** & 58.23 & 39.62 & **40.30** & 80.09 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Ablation study of patch mask ratio \(\gamma\). Note that “\(\gamma=0\)” means that the whole image is visible. We report the _averaged_ top-1 accuracy of position predictions.**

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline \multirow{2}{*}{\(\gamma_{\mathrm{pos}}\)} & \multicolumn{2}{c}{ImageNet-1K} & \multicolumn{2}{c}{COCO detection} & \multicolumn{2}{c}{COCO segmentation} & \multicolumn{2}{c}{ADE20k} \\ \cline{2-11}  & fine-tune & position & \(\mathbf{AP}^{\mathrm{b}}_{50}\) & \(\mathrm{AP}^{\mathrm{b}}_{55}\) & \(\mathbf{AP}^{\mathrm{m}}_{55}\) & \(\mathbf{AP}^{\mathrm{m}}_{50}\) & \(\mathrm{AP}^{\mathrm{m}}_{55}\) & **mIoU** & aAcc \\ \hline
0.25 & 82.71 & 65.19 & **37.98** & 57.65 & 41.39 & **34.61** & 54.79 & 36.93 & **36.15** & 78.16 \\
0.50 & 82.86 & 86.70 & **39.23** & 58.77 & 42.99 & **35.60** & 56.23 & 37.89 & **37.82** & 78.60 \\
0.75 & **82.96** & 87.83 & **42.14** & 61.99 & 46.38 & **37.93** & 59.23 & 40.76 & **40.68** & 80.14 \\
1.00 & 82.66 & 19.44 & **41.48** & 61.50 & 45.18 & **37.41** & 58.81 & 39.92 & **39.98** & 80.19 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Ablation study of position patch mask ratio \(\gamma_{\mathrm{pos}}\). Note that “\(\gamma_{\mathrm{pos}}=1\)” means that we do not provide any reference patch, _i.e._, all visible patches are randomly shuffled. We report the _averaged_ top-1 accuracy of position predictions.**\(\gamma_{\mathrm{pos}}=1\), where we observe performance deterioration. _(iii) Deciding which position to reconstruct precisely is crucial_ because by setting \(\sigma=0\) in Tab. 3 and \(\tau=\infty\) in Tab. 4, ViTs have better position predictions task but perform worse on downstream tasks.

**Performance on the position reconstruction task.** In general, a _positive correlation_ is observed between the accuracy of the downstream task and the averaged accuracy of predicted positions, indicating that the averaged accuracy of position predictions is a reliable indicator of how well the model is trained. However, there are some exceptions, _i.e._, \(\sigma=0\) in Tab. 3 and \(\tau=\infty\) in Tab. 4, where the model reconstructs more precise positions but performs worse on downstream benchmarks, indicating that _deciding which position to reconstruct precisely is crucial_.

Please refer to _Supplementary Material_ for the detailed performance of the position reconstruction task, where we provide more evidence to support the three difficulties proposed in Sec. 1.

**Patch mask ratio \(\gamma\).** We vary the patch mask ratio \(\gamma\) from 0 to 0.75 in Tab. 1. When \(\gamma=0\), the entire image is visible during pre-training, resulting in nearly _perfect_ position predictions and making the task too simple for self-supervised learning (see _Supplementary Material_ for details). However, the _averaged_ accuracy of position predictions is quite low since the accuracy deteriorates quickly as we enlarge \(\gamma\) during evaluation. Adopting a larger \(\gamma\) for pre-training leads to better performance, especially on detection and segmentation benchmarks. Therefore, we can conclude that _patch masking is essential in DropPos to prevent trivial solutions_. However, deterioration is observed when \(\gamma=0.9\). This is because the model may be under-fitted using this extremely difficult pretext task.

**Position mask ratio \(\gamma_{\mathrm{pos}}\).** We study the effectiveness of the position mask ratio \(\gamma_{\mathrm{pos}}\) in Tab. 2. DropPos appears to be more _robust_ against different \(\gamma_{\mathrm{pos}}\) than \(\gamma\). However, an appropriate \(\gamma_{\mathrm{pos}}\) is still necessary. A small \(\gamma_{\mathrm{pos}}\) leads to trivial solutions even if we have a large \(\gamma\), while an extremely large \(\gamma_{\mathrm{pos}}\), _e.g._, \(\gamma_{\mathrm{pos}}=1\), results in _discrepencies between pre-training and fine-tuning_.

**Position smoothing.** By setting different values of \(\sigma_{0}\) and \(\sigma_{T}\), we study the effectiveness of position smoothing. By setting \(\sigma=0\), we remove the position smoothing strategy, and the model tends to be overwhelmed by this position reconstruction task, leading to _high accuracy in position prediction but poor performance on downstream tasks_. However, when \(\sigma\) is too large, the smoothed positions become noisy, contributing to _poor_ performance on _both_ downstream tasks and position reconstruction. Additionally, equipped with a linear decay schedule of \(\sigma\), DropPos is gradually guided, bringing improvements on _both_ downstream tasks and position reconstruction.

**Attentive reconstruction.** Attentive reconstruction is studied in Tab. 4 by setting different values of temperature \(\tau\), where \(\tau=\infty\) means no attentive reconstruction. Without attentive reconstruction, DropPos is able to obtain better position predictions but performs worse on downstream tasks. This is because we do _not_ have to reconstruct the _exact_ positions when _different patches share similar visual appearances_. This phenomenon can be mitigated by setting an appropriate \(\tau\). A large \(\tau\) leads

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{\(\tau\)} & \multicolumn{2}{c}{ImageNet-1K} & \multicolumn{4}{c}{COCO detection} & \multicolumn{4}{c}{COCO segmentation} & \multicolumn{2}{c}{ADE20k} \\ \cline{2-11}  & fine-tune & position & \(\mathbf{AP^{b}}\) & \(\mathrm{AP^{b}_{\mathrm{SO}}}\) & \(\mathrm{AP^{b}_{\mathrm{S}}}\) & \(\mathbf{AP^{m}_{\mathrm{S}}}\) & \(\mathrm{AP^{m}_{\mathrm{S}}}\) & \(\mathrm{AP^{m}_{\mathrm{S}}}\) & mIoU & aAcc \\ \hline \(\infty\) & 82.84 & 88.66 & **40.56** & 60.48 & 44.15 & **36.78** & 57.64 & 39.46 & **38.38** & 79.36 \\
0.1 & **82.96** & 87.83 & **42.14** & 61.99 & 46.38 & **37.93** & 59.23 & 40.76 & **40.68** & 80.14 \\
0.5 & 82.86 & 87.78 & **40.73** & 60.33 & 44.77 & **36.71** & 57.72 & 39.38 & **38.80** & 79.62 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Ablation study of attentive reconstruction. “\(\tau=\infty\)” indicates no attentive reconstruction. We report the _averaged_ top-1 accuracy of position predictions.**

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{\(\sigma\)} & \multicolumn{2}{c}{ImageNet-1K} & \multicolumn{4}{c}{COCO detection} & \multicolumn{4}{c}{COCO segmentation} & \multicolumn{2}{c}{ADE20k} \\ \cline{2-11}  & fine-tune & position & \(\mathbf{AP^{b}_{\mathrm{SO}}}\) & \(\mathrm{AP^{b}_{\mathrm{S}}}\) & \(\mathrm{AP^{b}_{\mathrm{S}}}\) & \(\mathbf{AP^{m}_{\mathrm{S}}}\) & \(\mathrm{AP^{m}_{\mathrm{S}}}\) & \(\mathrm{AP^{m}_{\mathrm{S}}}\) & mIoU & aAcc \\ \hline
0 & 82.85 & 88.81 & **40.32** & 59.89 & 43.90 & **36.45** & 57.09 & 38.88 & **38.32** & 79.22 \\
1 & 82.91 & 87.13 & **40.19** & 59.76 & 43.81 & **36.30** & 57.06 & 38.87 & **38.60** & 79.17 \\
2 & 82.79 & 69.48 & **40.16** & 59.94 & 43.92 & **36.32** & 57.32 & 38.88 & **36.69** & 79.30 \\
1 \(\rightarrow\) 0 & **82.96** & 87.83 & **42.14** & 611.99 & 46.38 & **37.93** & 59.23 & 40.76 & **40.68** & 80.14 \\
2 \(\rightarrow\) 0 & 82.88 & 87.65 & **40.46** & 60.03 & 44.21 & **36.53** & 57.20 & 39.30 & **38.79** & 79.43 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Ablation study of \(\sigma\). “\(\sigma=0\)” means no position smoothing. “\(\rightarrow\)” denotes a linear decay schedule is performed on \(\sigma\). We report the _averaged_ top-1 accuracy of position predictions.**to noisy position ground truths, leading to _poor_ performance on _both_ downstream tasks and position reconstruction, which is the same as a large \(\sigma\).

### Comparisons with previous results

We compare our proposed DropPos with the supervised pre-training baseline and a wide range of self-supervised methods, including _(i)_ contrastive learning methods [6, 13], _(ii)_ masked image modeling methods [34, 3, 54, 56, 64], and _(iii)_ their combinations [19, 72]. Effective pre-training epoch is used for fair comparison following [54, 72] since it accounts for the _actual_ trained images/views. The detailed definition can be found in _Supplementary Material_. All methods are pre-trained with the same resolution, _i.e._, 224\(\times\)224 on ImageNet-1K [48].

**ImageNet-1K classification.** We compare our DropPos with state-of-the-art alternatives on the ImageNet-1K [48] classification benchmark in Tab. 5. Notably, with only 200 epochs pre-training,

\begin{table}
\begin{tabular}{l l c c c c c c c c} \hline \hline \multirow{2}{*}{method} & \multirow{2}{*}{venue} & \multirow{2}{*}{eff. ep.} & \multicolumn{3}{c}{COCO detection} & \multicolumn{3}{c}{COCO segmentation} & \multicolumn{3}{c}{ADE20k} \\ \cline{4-10}  & & & AP\({}^{\text{b}}_{\text{S}}\) & AP\({}^{\text{b}}_{\text{S}}\) & AP\({}^{\text{b}}_{\text{S}}\) & AP\({}^{\text{m}}_{\text{S}}\) & AP\({}^{\text{m}}_{\text{S}}\) & mIoU & aAcc \\ \hline MAE\({}^{\ddagger}\)[28] & [CVPR’22] & 200 & 40.1 & 60.5 & 44.1 & **36.4** & 57.8 & 39.3 & 40.5 & 80.1 \\ DropPos & [Ours] & 200 & **42.1** & 62.0 & 46.4 & **37.9** & 59.2 & 40.8 & **40.7** & 80.1 \\ \hline MoCo v3\({}^{\ddagger}\)[13] & [ICCV’21] & 600 & 43.7 & 65.7 & 47.7 & **39.1** & 62.0 & 41.8 & **44.7** & 81.5 \\ MAE\({}^{\ddagger}\)[28] & [CVPR’22] & 1600 & 47.3 & 68.2 & 52.5 & **42.4** & 65.3 & 45.6 & **47.0** & 82.7 \\ BootMAE\({}^{\ddagger}\)[19] & [ECCV’22] & 800 & 47.3 & 67.9 & 52.1 & **42.3** & 65.0 & 45.8 & **47.3** & 83.0 \\ SemMAE\({}^{\ddagger}\)[34] & [NeuIPS’22] & 800 & 45.6 & 66.2 & 55.2 & **40.9** & 63.3 & 44.4 & **44.9** & 82.0 \\ LocalMIA\({}^{\ddagger}\)[56] & [CVPR’23] & 1600 & 47.4 & 67.7 & 52.2 & **42.2** & 64.8 & 45.5 & **47.1** & 83.1 \\ DropPos & [Ours] & 800 & **47.7** & 68.3 & 52.8 & **42.6** & 65.3 & 46.2 & **47.8** & 82.8 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Comparison with previous methods on downstream tasks.** All methods take the ViT-B/16 [21] as the backbone and utilize Mask R-CNN [30] on COCO [38] object detection and instance segmentation, and UperNet [63] on ADE20k [71] semantic segmentation, respectively. \(\ddagger\) means the result is borrowed from [28]. \(\dagger\) indicates our implementation, including pre-training and supervised fine-tuning, while \(\sharp\) represents we reproduce fine-tuning using the official pre-trained backbone. We perform 1\(\times\) schedule of fine-tuning on COCO using ViTDet [36], and 80k iterations of fine-tuning on ADE20k using mmsegmentation [14].

\begin{table}
\begin{tabular}{l l c c} \hline \hline method & venue & eff. ep. & ViT-B & ViT-L \\ \hline supervised & & - & 80.9\({}^{\dagger}\) & 82.6\({}^{\ddagger}\) \\ MAE [28] & [CVPR’22] & 200 & 82.2\({}^{\ddagger}\) & 83.3\({}^{\ddagger}\) \\ DropPos & [Ours] & 200 & **83.0** & **83.7** \\ \hline \multicolumn{4}{l}{_Contrastive Learning_} \\ DINO\({}^{\ddagger}\)[6] & [ICCV’21] & 1600 & 82.8 & - \\ MoCo v3\({}^{\ddagger}\)[13] & [ICCV’21] & 600 & 83.2 & 84.1 \\ \hline \multicolumn{4}{l}{_Masked Image Modeling_} \\ BEI\({}^{\ddagger}\)[3] & [ICLR’22] & 800 & 83.2 & 85.2 \\ MAE\({}^{\ddagger}\)[28] & [CVPR’22] & 1600 & 83.6 & **85.9** \\ SimMIM [64] & [CVPR’22] & 800 & 83.8 & - \\ SemMAE [34] & [NeuIPS’22] & 800 & 83.4 & - \\ LocalMIM [56] & [CVPR’23] & 1600 & 84.0 & - \\ HPM [54] & [CVPR’23] & 800 & **84.2** & 85.8 \\ \hline \multicolumn{4}{l}{_Masked Image Modeling + Contrastive Learning_} \\ iBOT [72] & [ICLR’22] & 1600 & 84.0 & - \\ BootMAE [19] & [ECCV’22] & 800 & **84.2** & **85.9** \\ \hline \multicolumn{4}{l}{_Position Reconstructing_} \\ DropPos & [Ours] & 800 & **84.2** & 85.8 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Comparison with previous methods on ImageNet-1K classification.** All methods are evaluated by fine-tuning. The resolution of images is fixed to 224\(\times\)224. \(\dagger\) means our implementation. \(\ddagger\) means the result is borrowed from [28].

DropPos achieves 83.0% and 83.7% using ViT-B/16 and ViT-L/16 as the backbone, surpassing the supervised baseline by +1.1%, and MAE [28] by +0.8% and +0.4% respectively. This empirical evidence demonstrates that _enhancing the location awareness of ViTs indeed brings better visual representations_. Furthermore, with 800 epochs of pre-training, DropPos manages to achieve competitive results compared with the state-of-the-art. Specifically, it achieves 84.2% and 85.8% using ViT-B/16 and ViT-L/16, respectively. Strikingly, DropPos reaches competitive results with BootMAE [19], which combines contrastive learning and masked image modeling.

**COCO object detection and segmentation.** We fine-tune Mask R-CNN [30] on COCO [38] with 1\(\times\) schedule, _i.e._, 12 epochs, using the configuration of ViTDet [36]. We take ViT-B/16 [21] as the backbone for all entries in Tab. 6. We regard AP\({}^{\text{b}}\) and AP\({}^{\text{m}}\) as the main metric for object detection and instance segmentation, respectively. For further comparison, we additionally report AP\({}^{\text{b}}_{50}\) and AP\({}^{\text{b}}_{75}\) for object detection, and AP\({}^{\text{m}}_{50}\) and AP\({}^{\text{m}}_{75}\) for instance segmentation. With only 200 epochs of pre-training, DropPos achieves 42.1% AP\({}^{\text{b}}\) and 37.9% AP\({}^{\text{m}}\), outperforming MAE [28] by +2.0% and +1.5%, respectively. Note that these improvements appear to be more significant than those on the classification benchmark shown in Tab. 5, indicating that _DropPos indeed contributes to better spatial reasoning abilities of ViTs_. With 800 epochs of pre-training, DropPos achieves 47.7% AP\({}^{\text{b}}\) and 42.6% AP\({}^{\text{m}}\), surpassing MAE [28] by +0.4% and +0.2%, respectively.

**ADE20k semantic segmentation.** We fine-tune UperNet [63] on ADE20k [71] with 80k iterations. We take ViT-B/16 [21] as the backbone for all entries in Tab. 6 and search for the optimal learning rate for each entry. We regard mIoU as the main metric for semantic segmentation. We additionally report aAcc for further comparison. With only 200 epochs of pre-training, DropPos achieves 40.7% mIoU, outperforming MAE [28] by +0.2%. With 800 epochs of pre-training, DropPos achieves 47.8% mIoU, surpassing MAE [28] by +0.8%.

**Qualitative results.** We load DropPos with 200 epochs of pre-training and provide qualitative results on this position reconstruction task in Fig. 3. DropPos manages to reconstruct the exact positions of most patches _even in extreme circumstances_, _i.e._, \(\gamma=0.75\) and \(\gamma_{\text{pos}}=0.95\). This suggests that DropPos, as a self-supervised pretext task, _indeed makes ViTs location-aware_.

### Analysis

To explore whether the improved position sensitivity results in better feature representation and benefits to downstream tasks or not, we propose a metric to evaluate the position sensitivity.

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multirow{2}{*}{method} & \multirow{2}{*}{venue} & Position prediction & ImageNet-1K \\  & & Pre-trained & Fine-tuned & Top-1 \\ \hline MoCo v3 [13] & [ICCV’21] & 43.2 & 78.0 \(\uparrow 34.8\) & 83.2 \\ MAE [28] & [CVPR’22] & 63.1 & 82.5 \(\uparrow 19.4\) & 83.6 \\ DropPos & [ours] & **77.3** & **88.0**\(\uparrow 10.7\) & **84.2** \\ \hline \hline \end{tabular}
\end{table}
Table 7: An in-depth analysis about **whether the improved position sensitivity benefits downstream tasks or not**. We freeze the backbone and train an extra linear patch classification head. 75% of position embeddings are randomly masked during linear probing. We can conclude that _image classification indeed needs better location awareness_ and thus designing a position prediction pretext task is crucial and worth studying.

Figure 3: **Qualitative results of position reconstruction.** We evaluate position predictions with _different_\(\gamma\) but fix \(\gamma_{\text{pos}}=0.95\). Black patches are _masked_ during inference. The positions of those white patches are wrongly predicted, while the remaining patches are predicted correctly. For each tuple, we show results under **(a)**\(\gamma=0\), **(b)**\(\gamma=0.25\), **(c)**\(\gamma=0.5\), **(d)**\(\gamma=0.75\), and **(e)** the original image. DropPos manages to reconstruct _precise_ positions.

Specifically, we freeze the backbone and train an extra linear position prediction head using the vanilla cross-entropy loss. Top-1 accuracies of position predictions _before and after_ fine-tuning are reported in Tab. 7, and 75% of position embeddings are randomly masked during training. Higher values mean the model is better at modeling the position relationship. The top-1 accuracy on the ImageNet-1K [48] validation set after fine-tuning is also reported.

As shown in Tab. 7, the backbone performs better in position prediction after fine-tuning, indicating that _image classification indeed needs strong abilities in modeling spatial relationships_. It means that _better position sensitivity corresponds to better performances on downstream tasks_. This evidence suggests that our motivation, _i.e._, enhancing the location awareness of ViTs, is reasonable, and the topic is worth studying. By designing a position prediction pretext task, the backbone pre-trained by DropPos has better position modeling abilities, performing better on a variety of downstream tasks.

## 5 Conclusion

In this paper, we present DropPos, a novel, simple, and effective self-supervised pretext task that enhances the location awareness of ViTs. By masking patch tokens first and dropping positional embeddings next, ViTs are requesting to classify the position of each visible patch among all candidates based on partial visual appearance and a few anchor patches. In this way, we manage to avoid _(i) discrepancies_ between pre-training and fine-tuning, _(ii) trivial solutions_, and _(iii)_ reconstructing precise positions when _unnecessary_, resulting in improved spatial reasoning and understanding abilities of ViTs. Experiments across various benchmarks demonstrate the efficacy of DropPos, where DropPos _consistently_ achieves competitive results compared with previous methods.

**Discussion.** Due to limited computational resources, we do not use DropPos to pre-train larger ViTs, _e.g._, ViT-H/14. Despite these limitations, a growing need to design a pretext task that effectively uncovers the representation learning capabilities of vision models is becoming evident. We hope our DropPos will inspire future work. Additionally, how to enhance location awareness of vision models for spatial reasoning tasks in a supervised manner is also valuable to study.

## Acknowledgements

This work was supported in part by the Major Project for New Generation of AI (No. 2018AAA0100400), the National Natural Science Foundation of China (No. 61836014, No. U21B2042, No. 62072457, No. 62006231), and the InnoHK program.

## References

* [1] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. _arXiv preprint arXiv:2301.08243_, 2023.
* [2] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. In _International Conference on Machine Learning (ICML)_, pages 1298-1312. PMLR, 2022.
* [3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In _International Conference on Learning Representations (ICLR)_, 2022.
* [4] Mathilde Caron, Neil Houlsby, and Cordelia Schmid. Location-aware self-supervised transformers. _arXiv preprint arXiv:2212.02400_, 2022.
* [5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:9912-9924, 2020.
* [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9650-9660, 2021.
* [7] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi Chen, Jing Shi, Shuang Xu, and Bo Xu. Vlp: A survey on vision-language pre-training. _Machine Intelligence Research_, 20(1):38-56, 2023.
* [8] Haijian Chen, Wendong Zhang, Yunbo Wang, and Xiaokang Yang. Improving masked autoencoders by learning where to mask. _arXiv preprint arXiv:2303.06583_, 2023.

* [9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International Conference on Machine Learning (ICML)_, pages 1597-1607, 2020.
* [10] Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. _arXiv preprint arXiv:2202.03026_, 2022.
* [11] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. _arXiv preprint arXiv:2003.04297_, 2020.
* [12] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 15750-15758, 2021.
* [13] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9640-9649, 2021.
* [14] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https://github.com/open-mmlab/mmsegmentation, 2020.
* [15] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3213-3223, 2016.
* [16] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)_, pages 702-703, 2020.
* [17] Jiaxin Deng, Dong Shen, Haojie Pan, Xiangyu Wu, Ximan Liu, Gaofeng Meng, Fan Yang, Size Li, Ruiji Fu, and Zhongyuan Wang. A unified model for video understanding and knowledge embedding with heterogeneous knowledge graph dataset. In _Proceedings of the ACM International Conference on Multimedia Retrieval (ICMR)_, 2023.
* [18] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 1422-1430, 2015.
* [19] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Bootstrapped masked autoencoders for vision bert pretraining. In _European Conference on Computer Vision (ECCV)_, pages 247-264. Springer, 2022.
* [20] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Peco: Perceptual codebook for bert pre-training of vision transformers. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2023.
* [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations (ICLR)_, 2021.
* [22] Ye Du, Yujun Shen, Haochen Wang, Jingjing Fei, Wei Li, Liwei Wu, Rui Zhao, Zehua Fu, and Qingjie Liu. Learning from future: A novel self-training framework for semantic segmentation. _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [23] Philipp Dufter, Martin Schmitt, and Hinrich Schutze. Position information in transformers: An overview. _Computational Linguistics_, 48(3):733-763, 2022.
* [24] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. _International Journal of Computer Vision (IJCV)_, 88:303-308, 2009.
* [25] Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal learners. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:35946-35958, 2022.
* [26] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:21271-21284, 2020.
* [27] Yuxian Gu, Jiaxin Wen, Hao Sun, Yi Song, Pei Ke, Chujie Zheng, Zheng Zhang, Jianzhu Yao, Lei Liu, Xiaoyan Zhu, et al. Eva2. 0: Investigating open-domain chinese dialogue systems with large-scale pre-training. _Machine Intelligence Research_, 20(2):207-219, 2023.

* [28] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 16000-16009, 2022.
* [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9729-9738, 2020.
* [30] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 2961-2969, 2017.
* [31] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In _European Conference on Computer Vision (ECCV)_, 2016.
* [32] Dahun Kim, Donghyeon Cho, Donggeun Yoo, and In So Kweon. Learning image representations by completing damaged jigsaw puzzles. In _IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 793-802, 2018.
* [33] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [34] Gang Li, Heliang Zheng, Daqing Liu, Chaoyue Wang, Bing Su, and Changwen Zheng. Semmae: Semantic-guided masking for learning masked autoencoders. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [35] Xiang Li, Wenhai Wang, Lingfeng Yang, and Jian Yang. Uniform masking: Enabling mae pre-training for pyramid-based vision transformers with locality. _arXiv preprint arXiv:2205.10063_, 2022.
* [36] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In _European Conference on Computer Vision (ECCV)_, pages 280-296. Springer, 2022.
* [37] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2117-2125, 2017.
* [38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European Conference on Computer Vision (ECCV)_, pages 740-755. Springer, 2014.
* [39] Xingbin Liu, Jinghao Zhou, Tao Kong, Xianming Lin, and Rongrong Ji. Exploring target representations for masked autoencoders. _arXiv preprint arXiv:2209.03917_, 2022.
* [40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.
* [41] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 891-898, 2014.
* [42] T Nathan Mundhenk, Daniel Ho, and Barry Y Chen. Improvements to context based self-supervised learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9339-9348, 2018.
* [43] Muhammad Muzammal Naseer, Kanchana Ranasinghe, Salman H Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. _Advances in Neural Information Processing Systems (NeurIPS)_, 34:23296-23308, 2021.
* [44] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In _European Conference on Computer Vision (ECCV)_, pages 69-84, 2016.
* [45] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [46] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2536-2544, 2016.
* [47] Yixuan Qiu, Feng Lin, Weitong Chen, and Miao Xu. Pre-training in medical data: A survey. _Machine Intelligence Research_, pages 1-33, 2023.
* [48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International Journal of Computer Vision (IJCV)_, 115:211-252, 2015.

* [49] Sepehr Sameni, Simon Jenni, and Paolo Favaro. Representation learning by detecting incorrect location embeddings. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2023.
* [50] Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Deeppermnet: Visual permutation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3949-3957, 2017.
* [51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems (NeurIPS)_, 30, 2017.
* [52] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In _International Conference on Machine Learning (ICML)_, pages 1096-1103, 2008.
* [53] Haochen Wang, Yujun Shen, Jingjing Fei, Wei Li, Liwei Wu, Yuxi Wang, and Zhaoxiang Zhang. Pulling target to source: A new perspective on domain adaptive semantic segmentation. _arXiv preprint arXiv:2305.13752_, 2023.
* [54] Haochen Wang, Kaiyou Song, Junsong Fan, Yuxi Wang, Jin Xie, and Zhaoxiang Zhang. Hard patches mining for masked image modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10375-10385, 2023.
* [55] Haochen Wang, Yuchao Wang, Yujun Shen, Junsong Fan, Yuxi Wang, and Zhaoxiang Zhang. Using unreliable pseudo-labels for label-efficient semantic segmentation. _arXiv preprint arXiv:2306.02314_, 2023.
* [56] Haoqing Wang, Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhi-Hong Deng, and Kai Han. Masked image modeling with local multi-scale reconstruction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [57] Yuchao Wang, Jingjing Fei, Haochen Wang, Wei Li, Liwei Wu, Rui Zhao, and Yujun Shen. Balancing logit variation for long-tail semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 19561-19573, 2023.
* [58] Yuchao Wang, Haochen Wang, Yujun Shen, Jingjing Fei, Wei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, and Xinyi Le. Semi-supervised semantic segmentation using unreliable pseudo-labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4248-4257, 2022.
* [59] Ji-Rong Wen, Zi Huang, and Hanwang Zhang. Editorial for special issue on large-scale pre-training: Data, models, and fine-tuning. _Machine Intelligence Research_, 20(2):145-146, 2023.
* [60] Xin Wen, Bingchen Zhao, Anlin Zheng, Xiangyu Zhang, and Xiaojuan Qi. Self-supervised visual representation learning with semantic grouping. _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [61] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.
* [62] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3733-3742, 2018.
* [63] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _European Conference on Computer Vision (ECCV)_, pages 418-434, 2018.
* [64]Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9653-9663, 2022.
* [65] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 6023-6032, 2019.
* [66] Sukmin Yun, Hankook Lee, Jaehyung Kim, and Jinwoo Shin. Patch-level representation learning for self-supervised vision transformers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8354-8363, 2022.
* [67] Shuangfei Zhai, Navdeep Jaitly, Jason Ramapuram, Dan Busbridge, Tatiana Likhomanenko, Joseph Y Cheng, Walter Talbot, Chen Huang, Hanlin Goh, and Joshua M Susskind. Position prediction as an effective pretraining strategy. In _International Conference on Machine Learning (ICML)_, pages 26010-26027, 2022.
* [68] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. 2018.
** [69] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In _European Conference on Computer Vision (ECCV)_, pages 649-666, 2016.
* [70] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1058-1067, 2017.
* [71] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20K dataset. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 633-641, 2017.
* [72] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image bert pre-training with online tokenizer. In _International Conference on Learning Representations (ICLR)_, 2022.

## Supplementary Material

In this supplementary material, we first provide mode implementation details for reproducibility in Sec. A. Next, we provide more experiments in Sec. B. Finally, in Sec. C, we evaluate the performance of the position reconstruction task using pre-trained models under different settings, and we provide more evidence to support the proposed three difficulties in Sec. 1.

## Appendix A Implementation details

**ViT architecture.** We follow the standard vanilla ViT [21] architecture used in MAE [28] as the backbone, which is a stack of Transformer blocks [51]. Following MAE [28], we use the fixed 2D sine-cosine positional embeddings during pre-training. For the downstream classification task, we use features globally averaged from the encoder output for both end-to-end fine-tuning.

**Effective training epochs.** Following iBOT [72], we take the effective training epochs as the metric of the training schedule, due to extra computation costs brought by the multi-crop [5] augmentation, which is a widely used technique for contrastive methods. Specifically, the effective training epochs are defined as the actual pre-training epochs multiplied with a scaling factor \(r\). For instance, DINO [6] is trained with 2 global 224\(\times\)224 crops and 10 local 96\(\times\)96 crops, and thus \(r=2+(96/224)^{2}\times 10\approx 4\). More details and examples can be found in [72].

### ImageNet classification

For all experiments in this paper, we take ImageNet-1K [48], which contains 1.3M images for 1K categories, as the pre-trained dataset. By default, we take ViT-B/16 [21] as the backbone and it is pre-trained 200 epochs followed by 100 epochs of end-to-end fine-tuning. Implementation details can be found in the following table. Most of the configurations are borrowed from MAE [28]. The linear learning rate scaling rule is adopted: \(lr=lr_{\rm base}\times{\rm batch\_size}\)\(/\)\(256\). For supervised training from scratch, we simply follow the fine-tuning setting without another tuning. For ViT-B/16, pre-training and fine-tuning are conducted with 64 and 32 2080Ti GPUs, respectively. For ViT-L/16, pre-training and fine-tuning are conducted with 32 and 16 Tesla V100 GPUs, respectively.

\begin{tabular}{l|l|l} config & pre-training & fine-tuning \\ \hline optimizer & AdamW & AdamW \\ base learning rate & 1.5e-4 & 1e-3 \\ weight decay & 0.05 & 0.05 \\ momentum & \(\beta_{1}\), \(\beta_{2}\) = 0.9, 0.95 & \(\beta_{1}\), \(\beta_{2}\) = 0.9, 0.999 \\ layer-wise lr decay & 1.0 & 0.8 \\ batch size & 4096 & 1024 \\ learning rate schedule & cosine decay & cosine decay \\ warmup epochs & 10 (ViT-B/16), 40 (ViT-L/16) & 5 \\ training epochs & 200 & 100 (ViT-B/16), 50 (ViT-L/16) \\ augmentation & RandomResizedCrop & RandAug (9, 0.5) [16] \\ label smoothing & - & 0.1 \\ mixup [68] & - & 0.8 \\ cutmix [65] & - & 1.0 \\ drop path [31] & - & 0.1 \\ \end{tabular}

### COCO object detection and segmentation

We take Mask R-CNN [30] with FPN [37] as the object detector. Following [28] and [54], to obtain pyramid feature maps for matching the requirements of FPN [37], whose feature maps are all with a stride of 16, we equally divide the backbone into 4 subsets, each consisting of a last global-window block and several local-window blocks otherwise, and then apply convolutions to get the intermediate feature maps at different scales (stride 4, 8, 16, or 32).

We perform end-to-end fine-tuning on COCO [38] for 1\(\times\) schedule with 1024\(\times\)1024 resolution, where 88,750 iterations of training with a batch size of 16 are performed. We simply follow the configuration of ViTDet [36], where the learning rate is 3e-4 and decays at the 78,889-th and 85,463-th iteration by a factor of 10. Experiments are conducted on 8 Tesla V100 GPUs.

### ADE20k semantic segmentation

We take UperNet [63] as the segmentation decoder following the code of [3, 14, 54]. Fine-tuning on ADE20k [71] for 80k iterations is performed. Specifically, each iteration consists of 16 images with 512\(\times\)512 resolution. The AdamW optimizer is adopted with an initial learning rate of 7e-4 and a weight decay of 0.05 with ViT-B. We apply a polynomial learning rate schedule with the first warmup of 1500 iterations following common practice [3, 54, 14]. When fine-tuning using backbones pre-trained with different methods, we search for the optimal learning rate or simply follow their official implementation for a fair comparison. Specifically, the learning rate is 1e-4 for [13, 28, 34], 4e-4 for [19, 56], respectively. All experiments are conducted on 8 Tesla V100 GPUs.

## Appendix B More Experiments

**The initialization of the positional encoding.** DropPos uses fixed 2D sin-cos position embeddings by default. We ablate the initialization of position embeddings in Tab. S1 and it demonstrates that fixed sin-cos position embeddings achieve the best performance.

**DropPos with Swin Transformers [40].** To verify the scaling property and the generalization of DropPos, we provide experiments when DropPos is equipped with the Swin Transformer. We follow the implementation of UM-MAE [35] and pre-train a Swin-Tiny [40] from scratch using DropPos. All models are pre-trained with 200 epochs and fine-tuned with 100 epochs, following the configuration of UM-MAE [35]. From Tab. S2, we can conclude that DropPos still works on Swin Transformers [40], and thus enhancing the location awareness of vision transformers is still worth studying.

## Appendix C Performance of position reconstruction

In this section, we evaluate the performance of the position reconstruction task using pre-trained models under different settings. Specifically, we vary \(\gamma\in\{0,0.25,0.5,0.75\}\) and \(\gamma_{\mathrm{pos}}\in\{0.25,0.5,0.75,0.95\}\) when measuring the position prediction accuracy. We report performance under different evaluation settings as well as the _averaged_ accuracy among 16 different cases. From Tabs. S3 to S5, we find evidence to support the three difficulties for designing an appropriate position-related pretext task introduced in Sec. 1: _(i)_ discrepancies between pre-training and fine-tuning, _(ii)_ failing to learn highly semantic representations by solving this simple position reconstruction task, and _(iii)_ difficult to decide which patch positions to reconstruct precisely.

We study the effectiveness of different values of \(\gamma\) during pre-training in Tab. S3. Interestingly, we find evidence for _failing to learn highly semantic representations by solving this simple position reconstruction task._ As illustrated by Tab. S3a, the pre-trained model performs _extremely well_ when we set \(\gamma=0\) for evaluation but fails to keep this trend when we enlarge \(\gamma\). This indicates that giventhe strength of ViTs in modeling long-range dependencies, they have easily solved this task in a superficial way, and thus pre-training with \(\gamma=0\) becomes trivial for ViTs. To this end, an appropriate \(\gamma\) is necessary to increase the difficulty of the pretext task and avoid trivial solutions.

We study the effectiveness of different values of \(\gamma_{\text{pos}}\) during pre-training in Tab. S4, and we find evidence for _discrepancies between pre-training and fine-tuning_. As shown by Tab. S4, the model fails to reconstruct accurate positions given some visible anchors. This is because the model has _never_ been exposed to any positional embeddings (PEs) during pre-training. Therefore, providing some anchors is necessary to address discrepancies. Also, it may help the model focus on modeling _relative_ relationships instead of simply reconstructing absolute positions.

We study the effectiveness of different values of \(\gamma_{\mathrm{pos}}\) during pre-training in Tab. S4, and we find evidence for _hard to decide which patch positions to reconstruct precisely._ As shown by Tabs. S5a and S5f, the model achieves higher position prediction accuracy but performs worse on downstream tasks (please refer to Tabs. 3 and 4 for downstream performances). Therefore, to prevent being overwhelmed by this particular position reconstruction task, techniques for relaxing the patch-wise classification problem become necessary, _i.e._, position smoothing, and attentive reconstruction.