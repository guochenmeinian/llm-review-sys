# Boosting Semi-Supervised Few-Shot Object Detection with SoftER Teacher

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Few-shot object detection (FSOD) is an emerging problem aimed at detecting novel concepts from few exemplars. Existing approaches to FSOD assume abundant base labels to adapt to novel objects. This paper studies the task of _semi-supervised FSOD_ by considering a realistic scenario in which both base and novel labels are simultaneously scarce. We explore the utility of unlabeled data and discover its remarkable ability to boost semi-supervised FSOD by way of region proposals. Motivated by this finding, we introduce SoftER Teacher, a robust detector combining pseudo-labeling with representation learning on region proposals, to harness unlabeled data for improved FSOD without relying on abundant labels. Extensive experiments show that SoftER Teacher surpasses the novel performance of a strong supervised detector using only 10% of required base labels, without experiencing catastrophic forgetting observed in prior approaches. Our work also sheds light on a potential relationship between semi-supervised and few-shot detection suggesting that a stronger semi-supervised detector leads to a more effective few-shot detector.

## 1 Introduction

Modern object detection systems have enjoyed tremendous progress in recent years, with many successful applications across diverse industries. Their success can be mainly attributed to the availability of large-scale, well-annotated datasets such as the MS-COCO benchmark [28]. However, the demand for more powerful and accurate detection models requires considerable investments in the hand-labeling of massive amounts of data, which are costly to scale. Thus, there is a growing trend in the community to shift toward a more _label-efficient_ paradigm, one that can enable detection models to do more with less hand-labeled data. Such emerging research directions include self-supervised representation learning from unlabeled data [2; 7; 39], multi-modal pre-training from Web-labeled data [25; 32], semi-supervised detection (SSOD) [30; 48; 52], and few-shot detection (FSOD) [10; 11; 47], all of which have shown great promise in alleviating the dependency on large amounts of instance-level class annotations and bounding boxes.

Figure 1: The evaluation of _generalized_ FSOD is characterized by the trade-off between novel accuracy and base forgetting. We leverage unlabeled data to optimize for semi-supervised FSOD on both classes. Our approach significantly expands base class AP (\(39.3\to 44.4\)) while exhibiting less than 7% in base degradation (_vs._ 16% for LVC [21]). SoftER Teacher is the best model on the Overall AP metric, leading the next best Retentive R-CNN [10] by \(+2.0\) AP.

This paper focuses on the intersection of SSOD and FSOD, which are essentially two sides of the same coin in the context of label-efficient detection. On one side, SSOD investigates the detection problem with a _small fraction of images_ containing ground truth labels. On the other side, FSOD addresses the objective of adapting a _base_ detector to learn _novel_ concepts from _few instance-level_ annotations. Existing approaches to FSOD assume abundant base classes to train the base detector. However, such assumption is not ideal in practical scenarios where labels may be limited for both base and novel classes, giving rise to the research question: _can we do more on FSOD with the available unlabeled data without additional hand-labeling?_

We answer this question by introducing the unique task of _semi-supervised few-shot detection_, in which we explore the utility of unlabeled data for improving detection with label scarcity for both base and novel classes. Inspired by recent advances in SSOD and FSOD, our approach is two-fold: (1) we leverage unlabeled data to improve detection with a small fraction of labeled images; and (2) we generalize the resulting semi-supervised detector into a label-efficient few-shot detector by way of transfer learning. Our chief motivation is to not necessarily depend on an abundance of base classes for robust few-shot detection, which increases the versatility of our approach in realistic applications.

Moreover, our approach to semi-supervised FSOD adapts a base detector to learn novel concepts with _reduced performance degradation to base classes_, a desirable result missing in most prior approaches. Figure 1 illustrates that while recent work [11, 21, 36] achieve impressive detection on novel categories, _they all ignore the importance of preserving base class accuracy_. For _generalized_ FSOD [10], the goal is to expand the learned vocabulary of the base detector with novel concepts. As such, base and novel class performances are equally important, since samples at test time may contain instances of both objects. Therefore, the more realistic evaluation metric for FSOD is not only novel AP, but the combined base and novel AP, for which our approach establishes a new state of the art.

We measure the utility of unlabeled data within our integrated semi-supervised few-shot framework, and discover an interesting empirical finding connecting the effectiveness of unlabeled data to semi-supervised FSOD by way of region proposals. Without bells and whistles, by simply adding unlabeled data to a supervised detector, we show a marked improvement on both base and novel class performances while also mitigating catastrophic base forgetting [31].

**Summary of Main Contributions.** First, we introduce SoftER Teacher, a simple yet effective and versatile detector, to combine the strengths of pseudo-labeling with representation learning on unlabeled images. SoftER Teacher enhances the quality of region proposals to substantially boost semi-supervised FSOD. Our empirical analysis on the relationship between unlabeled data and region proposals extends earlier results on proposal evaluation beyond supervised detection [17, 46].

Second, Figure 2 illustrates a potential relationship suggesting that a strong semi-supervised detector is also a label-efficient few-shot detector, an interesting and non-trivial empirical observation linking the two disparate domains. On the task of semi-supervised FSOD, our SoftER Teacher model exceeds the novel class performance of a strong supervised detector [10] using only 10% of required base

Figure 2: We present the Label-Efficient Detection framework to harness supplementary unlabeled data for generalized semi-supervised few-shot detection. At the core of the framework is our proposed SoftER Teacher with Entropy Regression for improved semi-supervised base representation learning (**upper right**). Extensive comparative experiments show that SoftER Teacher is also a more label-efficient few-shot detector (**lower right**).

labels, while exhibiting less than 9% in base forgetting. When trained on 100% of labeled base classes with supplementary unlabeled data, SoftER Teacher sets a new standard on semi-supervised few-shot performance using varying amounts of bounding box annotations.

Third, we establish the Label-Efficient Detection benchmark to quantify the utility of unlabeled data for generalized semi-supervised FSOD. We hope that our benchmark serves as a strong baseline, and a blueprint, to inspire future research toward this new problem setting in the community.

## 2 Related Work

Semi-Supervised Detection.Recent approaches to SSOD can be summarized into consistency-based and pseudo-labeling categories. The leading consistency method is Humble Teacher [44], which trains a pair of detectors on both labeled and unlabeled data in the student-teacher framework [16; 45]. Humble Teacher learns representations from unlabeled data by enforcing consistency on predicted _soft labels_ from region proposals. Humble Teacher was inspired by CSD [18], which was the first approach to leverage consistency regularization [40] for SSOD, but utilizes strong data augmentation to deliver robust performance.

The state of the art on SSOD, however, belongs to a family of pseudo-labeling methods, which trains a pair of detectors on pseudo labels along with (limited) human labels. One such method is Soft Teacher [52] which vastly improves upon its previous counterparts STAC [42] and Unbiased Teacher [29] by enabling end-to-end pseudo-labeling on unlabeled images. More recently, Consistent Teacher [48] advances the performance envelope by reducing the inconsistency of pseudo targets. In both consistency-based and pseudo-labeling methods, the teacher model is an exponential moving average (EMA) of its student counterpart and is used to predict soft or pseudo labels on unlabeled data. The main difference between the two is how the surrogate labels are used to generate unsupervised targets to be jointly trained with the supervised objective.

We extend the strong performance of Soft Teacher by incorporating a new module for Entropy Regression to learn additional representations from unlabeled images by way of region proposals. Our model, aptly named _SoftER Teacher_, combines the attractive benefits of pseudo-labeling with supplementary proposal learning to establish a stronger baseline for SSOD.

Few-Shot Detection.Existing methods on FSOD can also be grouped into two categories: meta learning and transfer learning. Early work on meta learning introduced meta models to acquire class-level knowledge for adapting a base detector to novel concepts. Meta learners are jointly trained and fine-tuned with the base detectors to perform tasks like feature re-weighting, such as FSRW [19] and Meta R-CNN [53], or category-specific weight prediction (MetaDet) [49] using few exemplars of support images and ground truth bounding box annotations for the target objects.

Recent work on transfer learning for FSOD found that fine-tuning the last layer of the pre-trained base detector (_i.e._, the box classifier and regressor) on a balanced subset of base and novel classes, while freezing the rest of the detector, can significantly improve detection accuracy. The simple yet effective two-stage fine-tuning approach TFA [47] outperformed all prior meta learning methods on both base and novel detection metrics. Retentive R-CNN [10] extends TFA by introducing the Bias-Balanced RPN and Re-Detector modules to achieve strong novel performance without sacrificing base accuracy, a desideratum of FSOD. Other transfer learning methods, such as FSCE [43], DeFRCN [36], FADI [3], and DCFS [11], address the shortcomings of the box classifier to boost FSOD. While these methods obtain impressive performance on novel categories, they suffer from considerable base class forgetting, making them sub-optimal in real-world applications requiring efficient and accurate detection on test samples containing instances of both classes.

We show that both base and novel class performances can be further improved when unlabeled images are incorporated into the two-stage fine-tuning procedure without catastrophic base forgetting, which lead to a new standard for semi-supervised FSOD.

Semi-Supervised Few-Shot Detection.There have been few attempts at leveraging unlabeled data to improve FSOD, but to our knowledge none directly addressed the task of optimizing for semi-supervised few-shot detection, in which setting both base and novel labels are simultaneously scarce. LVC [21] mines novel targets from the _same base training dataset_ via pseudo-labeling to boost novel class detection, but comes at the cost of base performance. UniT [22] obtains impressive results on any-shot detection, but assumes abundant image-level labels for the base and novel targets. AndSSFOD [51] performs semi-supervised FSOD within an episodic meta training and \(N\)-way \(k\)-shot evaluation framework [20] while also requiring abundant base classes. Our approach is fundamentally different in that we do not strictly depend on abundant base labels, but make effective utilization of _external unlabeled data_ for robust semi-supervised FSOD with reduced base degradation.

## 3 Approach

We propose to combine the available (limited) labeled examples with supplementary unlabeled images to boost semi-supervised FSOD. We begin with the fully supervised scenario in which we have access to a set of labeled image-target pairs \((x_{l},y_{l})\in\mathcal{D}_{\text{sup}}\). The supervised FSOD setting [10; 19; 47] assumes a base dataset \(C_{\text{base}}\in\mathcal{D}_{\text{sup}}\) with abundant instance-level annotations and a novel dataset \(C_{\text{novel}}\in\mathcal{D}_{\text{sup}}\) with only a few \(k\) (_e.g._, \(k\in\{1,5,10\}\)) labeled instances, or "shots", per category. The goal of FSOD is to expand the base detector by adapting it to learn new target concepts such that the resulting detector is optimized for accuracy on a test set comprising both classes in \(C_{\text{base}}\cup C_{\text{novel}}\).

To maintain parity with existing work, we adopt the simple yet effective two-stage transfer learning approach [10; 47] currently leading the FSOD literature, which comprises an initial stage of base class pre-training followed by a second stage of novel category fine-tuning. We consider the modern Faster R-CNN (FRCN) [37] system as our supervised base detector, which consists of a ResNet [15]_backbone_ and a feature pyramid network (FPN) [27]_neck_ for feature extraction, a region proposal network (RPN), an RoIAlign [14] operation for mapping proposals to region-of-interest (RoI) features, and a fully-connected _head_ for RoI classification and regression. Let FRCN be represented by \(f_{\theta}\), a stochastic function parametrized by a set of learnable weights \(\theta\). Formally, the base pre-training step is subjected to the standard supervised objective, over a mini-batch of labeled examples \(b_{l}\), given by:

\[\mathcal{L}_{\text{sup}}=\frac{1}{|b_{l}|}\sum_{i\in b_{l}}\mathcal{L}_{\text{ cls}}^{\text{roi}}(f_{\theta}(x_{i}),y_{i})+\mathcal{L}_{\text{reg}}^{\text{ roi}}(f_{\theta}(x_{i}),y_{i}).\] (1)

Here, \(f_{\theta}(x_{i})\) denotes a forward pass on the \(i\)-th image to produce box classification and localization predictions from class-agnostic proposals, \(y_{i}\) is the \(i\)-th ground truth annotation containing box labels and coordinates, and \(\left(\mathcal{L}_{\text{cls}}^{\text{roi}},\mathcal{L}_{\text{reg}}^{\text{ roi}}\right)\) are the cross-entropy and \(L_{1}\) losses, respectively, for the RoI head. Henceforth for simplicity, we develop our approach only on the RoI head and omit the presentation on the classification and regression losses of the RPN, which remain constant without changes, to predict and localize the "objectness" of region proposals.

### What Makes for Effective FSOD?

We revisit this question from the perspective of maximizing representation learning while minimizing base forgetting. In two-stage detectors [14; 37], the quality of region proposals is a strong predictor of supervised detection performance [17; 46], since they focus the detector head on candidate RoIs. This is especially true for FSOD approaches based on transfer learning, in which the established procedure is to freeze the RPN during few-shot fine-tuning. Intuitively, if we can incorporate methods and/or data to boost representation learning by way of the RPN, then the detector should have a higher chance of discovering novel categories to improve few-shot performance.

We conduct extensive experiments on the COCO dataset to verify our intuition. We split the dataset into disjoint 60 base and 20 novel categories and pre-train three variants of the FRCN detector on the base classes: (i) FRCN-Base, (ii) FRCN-Base augmented with COCO unlabeled2017 images leveraging the Soft Teacher formulation, and (iii) FRCN-Full using both base and novel classes to represent the upper-bound performance. We also experiment with CRPN-Base, a method specially designed to improve proposal quality and detection performance using a two-stage Cascade RPN [46].

Figure 2(a) quantifies the "class-agnosticism" of various RPNs, using the standard metric AR@300 proposals, for varying fractions of base labels. Surprisingly, unlabeled data has the remarkable ability to boost proposal recall on novel-only categories, even in the extremely low 1% label limit. Somewhat unsurprising is the ability of CRPN-Base to propose novel objects competitive with FRCN-Base\(+\)Unlabeled when more base labels are available. Consistent with previous findings [10; 21], Figures 2(b) and 2(c) show that the vanilla supervised FRCN-Base has a strong tendency to reject novel objects as background, due to the lack of annotations, resulting in the worst recall on novel classes.

As alluded in Section 1, the contribution of unlabeled data to FSOD goes beyond improving base and novel detection performances; unlabeled data can also help mitigate catastrophic base forgetting.

We find analogous effectiveness of FRCN-Base\(~{}+~{}\)Unlabeled on the combined Overall AR0300 metric, for both base \(+\) novel objects, suggesting the RPN, when trained with unlabeled data, has the ability to retain base proposals and help combat base degradation during few-shot fine-tuning.

Discussion.This paper rethinks a different and more versatile way to improve the RPN for FSOD while avoiding catastrophic forgetting. The previous LVC approach proposed to unfreeze the RPN during fine-tuning to obtain large performance gains on novel categories, but comes at the cost of significant base degradation (up to 19%). Similarly, FSCE [43] proposed to unfreeze the RPN while also doubling the number of proposals to encourage novel foreground detection during fine-tuning. However, this method increases the detection overhead and remains unclear whether it helps mitigate base forgetting. We illustrate that simply adding unlabeled data to the base detector leads to a compelling boost in proposal quality, without the need for any _ad hoc_ modifications to the RPN.

We attribute this unique benefit of our approach to the potential base-novel object interactions found in abundant images. When learning with unlabeled data, the base detector can obtain semantically similar cues of novel objects to inform the RPN on foreground detection. Sun _et al_. [43] showed that visually analogous objects have high cosine similarity scores (_e.g_., \(\text{sim}(\texttt{cov},\texttt{horse})=0.39\)). With 1% of labels, these base-novel interactions are limited, resulting in a recall of \(22.7\%\). Given a sizable unlabeled dataset, the base detector improves its representations to yield a gain of \(+12.3\) points.

### Semi-Supervised Base Pre-Training

Motivated by the promising utility of unlabeled data, we now relax the strict assumption on having abundant base classes for FSOD and introduce a new and more general setting of having a small fraction of base labels given abundant unlabeled images. We revisit the task of semi-supervised base pre-training by formulating an unsupervised loss computed on an unlabeled dataset \(\mathcal{D}_{\text{unsup}}\) to be jointly trained with the supervised loss on \(\mathcal{D}_{\text{sup}}\). We consider the following canonical optimization objective widely adopted as part of the framework for semi-supervised learning [1, 23, 33]:

\[\min_{\theta}\mathcal{L}_{\text{sup}}(\mathcal{D}_{\text{sup}},\theta)+ \lambda\mathcal{L}_{\text{unsup}}(\mathcal{D}_{\text{unsup}},\theta),\] (2)

where \(\lambda>0\) is a hyper-parameter controlling the contribution of the unsupervised component. Next, we describe the unsupervised criterion on \(\mathcal{D}_{\text{unsup}}\) to make FRCN into a semi-supervised detector.

**Soft Teacher.** We adopt Soft Teacher [52] as the baseline SSOD formulation for its simplicity but strong performance. Soft Teacher trains FRCN in a student-teacher fashion on both labeled and unlabeled data. The student is trained on labeled examples in the standard supervised manner per Eq. (1). For unlabeled images, the teacher is treated as a fixed detector to generate thousands of box candidates, most of which are eliminated for redundancy with non-maximum suppression. Additionally, box candidates are thresholded for foreground objects and go through an iterative jittering-refinement procedure to reduce localization variance, resulting in a set of high-quality pseudo boxes to be jointly trained with ground truth annotations.

As is common practice [41, 45], the teacher's parameters \(\bar{\theta}\) are updated from the student's via \(\bar{\theta}=\text{EMA}(\theta)\) at each training step. Integral to the success of Soft Teacher is a student-teacher data augmentation strategy inspired by STAC [42]. The student trains on unlabeled images subjected to complex random perturbations, akin to RandAugment [6], including affine transforms. Separately,

Figure 3: We analyze the effectiveness of the RPN as a function of base labels. **(a)** Unlabeled data provides a convincing boost in proposal quality, closing the gap between the Base and Full detectors, which should lead to better discovery of novel categories during fine-tuning. **(b-c)** In low-label regimes, unlabeled data can help produce diverse proposals (green boxes) on novel unseen objects {boat, bus, car, dog}, whereas the vanilla supervised FRCN-Base fails to capture comparable foreground objects (red boxes). Best viewed digitally.

the teacher receives weakly augmented images with simple random resizing and flipping. This multi-stream augmentation design allows the teacher to generate reliable unsupervised targets on easy images to guide the student's learning on difficult images for better generalization.

At the time of box classification and regression in the RoI head, we have a set of unlabeled images along with teacher-generated pseudo labels \((x_{u},\hat{y}_{u})\in\mathcal{D}_{\text{unsup}}\). The unsupervised loss for Soft Teacher on a mini-batch of unlabeled images \(b_{u}\) is defined as:

\[\mathcal{L}_{\text{unsup}}^{\text{soft}}=\frac{1}{|b_{u}|}\sum_{i\in b_{u}} \mathcal{L}_{\text{cls}}^{\text{roi}}(f_{\theta}(x_{i}),\hat{y}_{i})+\mathcal{ L}_{\text{reg}}^{\text{roi}}(f_{\theta}(x_{i}),\hat{y}_{i}).\] (3)

**SoftER Teacher.**  The design of Soft Teacher employs class confidence thresholding and box jittering to select high-quality pseudo-label candidates for unsupervised classification and regression. However, it uses an aggressive threshold of 0.9, resulting in a trade-off between low recall and high precision at 33% and 89%, respectively [52]. We observe that low recall can result in poor detection performance on small and ambiguous objects [26], especially in low-label regimes where the teacher has insufficient confidence about its predicted pseudo labels. We aim to extend Soft Teacher and improve its detection recall by learning additional representations from abundant region proposals.

Given a set of proposals \(p\) generated by the student's RPN on a batch of unlabeled images, we apply the student-teacher data augmentation pipeline described above to obtain \((p_{s},p_{t})\), denoting transformed student and teacher proposals, which are related to each other by a transformation matrix \(M\). We then forward pass Faster R-CNN twice, as the student \(f_{\theta}\) and teacher \(f_{\hat{\theta}}\), to obtain two sets of RoI outputs for predicted box classification logits \((z_{s},z_{t})\) and localization coordinates \((r_{s},r_{t})\). Let \(g_{c}\) be the softmax function over the channel dimension \(c\). We define an auxiliary unsupervised criterion for proposal box similarity based on a cross-entropy measure \(H(z_{s},z_{t})\):

\[\begin{split}\mathcal{L}_{\text{cls}}^{\text{ent}}=\frac{1}{ \sum_{i}w_{i}}\sum_{i\in p}w_{i}\cdot H(z_{is},z_{it}),\\ \text{in which}\quad H(z_{is},z_{it})=-\frac{1}{C}\sum_{c\in C}g_ {c}(z_{it})\log g_{c}(z_{is}).\end{split}\] (4)

Here, \(g_{c}\) outputs a distribution over \(C\) classes and \(w_{i}\) is the Boolean weight for the predicted positive (foreground) class: \(w_{i}=1\) if \(\text{argmax}(z_{it})\neq background\), else \(w_{i}=0\).

Similarly for proposal box regression, we constrain the predicted box coordinates \((r_{s},r_{t})\) to be close. Since there are complex geometric distortions between the two, we first map teacher proposal coordinates \(r_{t}\) to the student space using the transformation \(M\). Then, we align the proposal boxes via the intersection-over-union (IoU) criterion:

\[\mathcal{L}_{\text{reg}}^{\text{iou}}=1-\frac{1}{|p|}\sum_{i\in p}w_{i}\cdot \text{IoU}(r_{is},M(r_{it})),\] (5)

where we treat the IoU metric as a loss [38] to quantify the discrepancy between student and teacher proposal coordinates. Note that both the cross-entropy and IoU losses, Eqs. (4) and (5), are computed only on predicted foreground classes.

Recall that we have two different transformation pipelines operating on each proposal, so we have two augmented views of each proposal. Figure 4 illustrates that by enforcing these randomly

Figure 4: Visualization of student-teacher proposals with confidence scores \(\geq 0.99\). As illustrated by the arrow, a pair of student-teacher proposals is related by a transformation matrix \(M\), which is used to align proposals between student and teacher images for enforcing box classification similarity and localization consistency.

augmented views, and their box coordinates, to be _similar_, we enable the student to tap into abundant region proposals to learn diverse feature representations across a spectrum of scale, color, and geometric perturbations. Our formulation draws inspiration from recent research on self-supervised representation learning with multi-augmented views [5; 13]. Note the cross-entropy similarity between the student and teacher predictions, Eq. (4), can be interpreted as a form of entropy regularization [12], which has been proven to work well in various semi-supervised classification scenarios [33; 34]. The overall optimization objective at the RoI head for our SoftER Teacher model is computed as:

\[\mathcal{L}_{\text{total}}^{\text{softer}}=\mathcal{L}_{\text{sup}}+\alpha \mathcal{L}_{\text{unsup}}^{\text{soft}}+\beta\left(\mathcal{L}_{\text{cls}}^{ \text{ent}}+\mathcal{L}_{\text{reg}}^{\text{iou}}\right)\,,\] (6)

where we set \(\alpha=\frac{|b_{u}|}{|b_{t}|}\) following Soft Teacher and find \(\beta=2\alpha\) works well across all experiments.

Discussion.Our approach to proposal learning is unique and different from Humble Teacher [44] in two major ways: (1) we use diverse data transformations, including geometric distortions, to enforce proposal similarity, whereas Humble Teacher considered only simple random flipping and color transforms; and (2) we adopt the IoU metric for box localization consistency over \(L_{2}\) distance, which has been shown to produce inferior box regression [38]. Further, Humble Teacher acknowledged that matching student-teacher proposals under complex affine transforms is not a trivial task and "could lead to undesirably complicated details." We solve this complicated task by tracking the affine matrix \(M\) in our entropy regression module, to address a key weakness of Soft Teacher by boosting object recall, thereby enabling SoftER Teacher to demonstrate superior learning with unlabeled data over Humble Teacher and Soft Teacher. Please refer to Appendix B.3 for detailed comparative results.

### Semi-Supervised Few-Shot Fine-Tuning

We propose a simple two-step approach to harness unlabeled data for semi-supervised few-shot fine-tuning. First, we initialize the few-shot detector, \(f_{\hat{g}}^{\prime}\leftarrow f_{\hat{g}}\), with parameters copied from the base _teacher_ detector pre-trained with unlabeled data per Eq. (6). And second, we further train the RoI head of \(f_{\hat{g}}^{\prime}\) on novel classes using the available few-shot and unlabeled examples while freezing the base backbone, FPN, and RPN components. Then, we fine-tune the few-shot detector on a balanced training set of \(k\) shots per class containing both base and novel instances. We only update the RoI box classifier while freezing all other components, including the box regressor, since it is the main source of error [11; 43]. To our knowledge, _we are the first to incorporate external unlabeled data with few-shot fine-tuning_, which provides a compelling boost to novel performance while enjoying substantial gains in base detection without catastrophic forgetting. We present detailed ablation studies in Appendix A to validate our approach and design choices.

## 4 Experiments

Datasets.Consistent with the current literature on FSOD, we evaluate our approach on the challenging PASCAL VOC [9] and MS-COCO 2017 [28] detection benchmarks. For VOC, we use the combined VOC07+12 trainval splits as the labeled training set and evaluate performance on the VOC2007 test set. For COCO, we utilize the train2017 split as labeled data and test on val2017. We also leverage COCO-20, the subset of COCO data having the same 20 class names as VOC, and COCO unlabeled2017 as the sources of supplementary unlabeled data.

Performance Metrics.Following established evaluation protocol, we assess detection performance using AP\({}_{50}\) for the average precision at overlap threshold 0.5 and AP\({}_{50:95}\) for the mean average precision computed over a range of 10 overlap thresholds between 0.5 and 0.95. We also report _average recall AR_ to complement AP for assessing object coverage, which has previously been used to evaluate detection performance of small objects [24; 26].

Implementation Details.For most experiments, we adopt the ResNet-101 backbone pre-trained on ImageNet 1K [8] for a direct comparison with existing work. For some experiments, we also employ ResNet-50 to demonstrate parameter-efficient learning with SoftER Teacher. We implement our models in MMDetection [4] and PyTorch [35]. Complete details are given in Appendix C.

### SoftER Teacher is a Parameter- and Label-Efficient Few-Shot Detector

We conduct our few-shot experiments on the same VOC and COCO samples provided by the TFA benchmark [47]. The VOC dataset is randomly partitioned into 15 base and 5 novel classes, in which 

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

## References

* [1] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with Pseudo-Ensembles. In _NeurIPS_, 2014.
* [2] Amir Bar, Xin Wang, Vadim Kantorov, Colorado J. Reed, Roei Herzig, Gal Chechik, Anna Rohrbach, Trevor Darrell, and Amir Globerson. DETReg: Unsupervised Pretraining With Region Priors for Object Detection. In _CVPR_, 2022.
* [3] Yuhang Cao, Jiaqi Wang, Ying Jin, Tong Wu, Kai Chen, Ziwei Liu, and Dahua Lin. Few-Shot Object Detection via Association and Discrimination. In _NeurIPS_, 2021.
* [4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, et al. MMDetection: Open MMLab Detection Toolbox and Benchmark. https://arxiv.org/abs/1906.07155, 2019.
* [5] Xinlei Chen and Kaiming He. Exploring Simple Siamese Representation Learning. In _CVPR_, 2021.
* [6] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. RandAugment: Practical Automated Data Augmentation with a Reduced Search Space. In _NeurIPS_, 2020.
* [7] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. UPDETR: Unsupervised Pre-Training for Object Detection with Transformers. In _CVPR_, 2021.
* [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In _CVPR_, pages 248-255, 2009.
* [9] Mark Everingham, Luc Van Gool, Christopher K.I. Williams, John Winn, and Andrew Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. _IJCV_, 88(2):303-338, 2010.
* [10] Zhibo Fan, Yuchen Ma, Zeming Li, and Jian Sun. Generalized Few-Shot Object Detection without Forgetting. In _CVPR_, 2021.
* [11] Bin-Bin Gao, Xiaochen Chen, Zhongyi Huang, Congchong Nie, Jun Liu, Jinxiang Lai, Guannan Jiang, Xi Wang, and Chengjie Wang. Decoupling Classifier for Boosting Few-Shot Object Detection and Instance Segmentation. In _NeurIPS_, 2022.
* [12] Yves Grandvalet and Yoshua Bengio. Semi-Supervised Learning by Entropy Minimization. In _NeurIPS_, 2004.
* [13] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, et al. Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning. In _NeurIPS_, 2020.
* [14] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In _ICCV_, 2017.
* [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In _CVPR_, 2016.
* [16] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the Knowledge in a Neural Network. In _NeurIPS Deep Learning and Representation Learning Workshop_, 2015.
* [17] Jan Hosang, Rodrigo Benenson, Piotr Dollar, and Bernt Schiele. What Makes for Effective Detection Proposals? _IEEE TPAMI_, 38(4):814-830, 2016.
* [18] Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. Consistency-Based Semi-Supervised Learning for Object Detection. In _NeurIPS_, 2019.
* [19] Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-Shot Object Detection via Feature Reweighting. In _ICCV_, 2019.
* [20] Leonid Karlinsky, Joseph Shtok, Sivan Harary, Eli Schwartz, Amit Aides, Rogerio Feris, Raja Giryes, and Alex M. Bronstein. RepMet: Representative-Based Metric Learning for Classification and Few-Shot Object Detection. In _CVPR_, 2019.
* [21] Prannay Kaul, Weidi Xie, and Andrew Zisserman. Label, Verify, Correct: A Simple Few Shot Object Detection Method. In _CVPR_, 2022.
* [22] Siddhesh Khandelwal, Raghav Goyal, and Leonid Sigal. UniT: Unified Knowledge Transfer for Any-shot Object Detection and Segmentation. In _CVPR_, 2021.
* [23] Samuli Laine and Timo Aila. Temporal Ensembling for Semi-Supervised Learning. In _ICLR_, 2017.
* [24] Jianan Li, Xiaodan Liang, Yunchao Wei, Tingfa Xu, Jiashi Feng, and Shuicheng Yan. Perceptual Generative Adversarial Networks for Small Object Detection. In _CVPR_, 2017.
* [25] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-Training Paradigm. In _ICLR_, 2022.
* [26] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, and Jian Sun. DetNet: A Backbone Network for Object Detection. In _ECCV_, 2018.
* [27] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature Pyramid Networks for Object Detection. In _CVPR_, 2017.
* [28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In _ECCV_, 2014.
* [29] Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, and Peter Vajda. Unbiased Teacher for Semi-Supervised Object Detection. In _ICLR_, 2021.

* [30] Yen-Cheng Liu, Chih-Yao Ma, and Zsolt Kira. Unbiased Teacher v2: Semi-Supervised Object Detection for Anchor-Free and Anchor-Based Detectors. In _CVPR_, 2022.
* [31] David Lopez-Paz and Marc'Aurelio Ranzato. Gradient Episodic Memory for Continual Learning. In _NeurIPS_, 2017.
* [32] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, et al. Simple Open-Vocabulary Object Detection with Vision Transformers. In _ECCV_, 2022.
* [33] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning. _IEEE TPAMI_, 41:1979-1993, 2017.
* [34] Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, and Ian J. Goodfellow. Realistic Evaluation of Deep Semi-Supervised Learning Algorithms. In _NeurIPS_, 2018.
* [35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In _NeurIPS_, pages 8024-8035. Curran Associates, Inc., 2019.
* [36] Limeng Qiao, Yuxuan Zhao, Zhiyuan Li, Xi Qiu, Jianan Wu, and Chi Zhang. DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection. In _ICCV_, 2021.
* [37] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In _NeurIPS_, 2015.
* [38] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression. In _CVPR_, 2019.
* [39] Byungseok Roh, Wuhyun Shin, Ildoo Kim, and Sungwoong Kim. Spatially Consistent Representation Learning. In _CVPR_, 2021.
* [40] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with Stochastic Perturbations for Deep Semi-Supervised Learning. In _NeurIPS_, 2016.
* [41] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying Semi-Supervised Learning with Consistency and Confidence. In _NeurIPS_, 2020.
* [42] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A Simple Semi-Supervised Learning Framework for Object Detection. https://arxiv.org/abs/2005.04757, 2020.
* [43] Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, and Chi Zhang. FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding. In _CVPR_, 2021.
* [44] Yihe Tang, Weifeng Chen, Yijun Luo, and Yuting Zhang. Humble Teachers Teach Better Students for Semi-Supervised Object Detection. In _CVPR_, 2021.
* [45] Antti Tarvainen and Harri Valpola. Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results. In _NeurIPS_, 2017.
* [46] Thang Vu, Hyunjun Jang, Trung X. Pham, and Chang D. Yoo. Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution. In _NeurIPS_, 2019.
* [47] Xin Wang, Thomas E. Huang, Trevor Darrell, Joseph E. Gonzalez, and Fisher Yu. Frustratingly Simple Few-Shot Object Detection. In _ICML_, 2020.
* [48] Xinjiang Wang, Xingyi Yang, Shilong Zhang, Yijiang Li, Litong Feng, Shijie Fang, Chengqi Lyu, Kai Chen, and Wayne Zhang. Consistent-Teacher: Towards Reducing Inconsistent Pseudo-Targets in Semi-Supervised Object Detection. In _CVPR_, 2023.
* [49] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Meta-Learning to Detect Rare Objects. In _ICCV_, 2019.
* [50] Jiaxi Wu, Songtao Liu, Di Huang, and Yunhong Wang. Multi-Scale Positive Sample Refinement for Few-Shot Object Detection. In _ECCV_, 2020.
* [51] Wuti Xiong, Yawen Cui, and Li Liu. Semi-Supervised Few-Shot Object Detection with a Teacher-Student Network. In _BMVC_, 2021.
* [52] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-End Semi-Supervised Object Detection with Soft Teacher. In _ICCV_, 2021.
* [53] Xiaopeng Yan, Ziliang Chen, Anni Xu, Xiaoxi Wang, Xiaodan Liang, and Liang Lin. Meta R-CNN : Towards General Solver for Instance-Level Few-Shot Learning. In _ICCV_, 2019.