# Model-free Posterior Sampling via Learning Rate Randomization

Daniil Tiapkin\({}^{1,2}\) Denis Belomestny\({}^{3,2}\) Daniele Calandriello\({}^{4}\) Eric Moulines\({}^{1,5}\)

**Remi Munos\({}^{4}\) Alexey Naumov\({}^{2}\) Pierre Perrault\({}^{6}\) Michal Valko\({}^{4}\) Pierre Menard\({}^{7}\)**

\({}^{1}\)CMAP, Ecole Polytechnique \({}^{2}\)HSE University \({}^{3}\)Duisburg-Essen University

\({}^{4}\)Google DeepMind \({}^{5}\)Mohamed Bin Zayed University of AI, UAE \({}^{6}\)IDEMIA \({}^{7}\)ENS Lyon

{daniil.tiapkin,eric.moulines}@polytechnique.edu

denis.belomestny@uni-due.de

{dcalandriello,munos,valkom}@google.com anaumov@hse.ru

pierre.perrault@outlook.com pierre.menard@ens-lyon.fr

###### Abstract

In this paper, we introduce Randomized Q-learning (RandQL), a novel randomized model-free algorithm for regret minimization in episodic Markov Decision Processes (MDPs). To the best of our knowledge, RandQL is the first tractable model-free posterior sampling-based algorithm. We analyze the performance of RandQL in both tabular and non-tabular metric space settings. In tabular MDPs, RandQL achieves a regret bound of order \(\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})\), where \(H\) is the planning horizon, \(S\) is the number of states, \(A\) is the number of actions, and \(T\) is the number of episodes. For a metric state-action space, RandQL enjoys a regret bound of order \(\widetilde{\mathcal{O}}(H^{5/2}T^{(d_{z}+1)/(d_{z}+2)})\), where \(d_{z}\) denotes the zooming dimension. Notably, RandQL achieves optimistic exploration _without using bonuses_, relying instead on a novel idea of _learning rate randomization_. Our empirical study shows that RandQL outperforms existing approaches on baseline exploration environments.

## 1 Introduction

In reinforcement learning (RL, Sutton and Barto 1998), an agent learns to interact with an unknown environment by acting, observing the next state, and receiving a reward. The agent's goal is to maximize the sum of the collected rewards. To achieve this, the agent can choose to use model-based or model-free algorithms. In model-based algorithms, the agent builds a model of the environment by inferring the reward function and the transition kernel that produces the next state. The agent then plans in this model to find the optimal policy. In contrast, model-free algorithms directly learn the optimal policy, which is the mapping of a state to an optimal action, or equivalently, the optimal Q-values, which are the mapping of a state-action pair to the expected return of an optimal policy starting by taking the given action at the given state.

Although empirical evidence suggests that model-based algorithms are more sample efficient than model-free algorithms (Deisenroth and Rasmussen, 2011; Schulman et al., 2015); model-free approaches offer several advantages. These include smaller time and space complexity, the absence of a need to learn an explicit model, and often simpler algorithms. As a result, most of the recent breakthroughs in deep RL, such as those reported by Mnih et al. (2013); Schulman et al. (2015, 2017); Haarnoja et al. (2018), have been based on model-free algorithms, with a few notable exceptions, such as Schrittwieser et al. (2020); Hessel et al. (2021). Many of these model-free algorithms (Mnih et al., 2013; Van Hasselt et al., 2016; Lillicrap et al., 2016) are rooted in the well-known Q-learning algorithm of Watkins and Dayan (1992). Q-learning is an off-policy learning technique where the agent follows a behavioral policy while simultaneously incrementally learning the optimal Q-valuesby combining asynchronous dynamic programming and stochastic approximation. Until recently, little was known about the sample complexity of Q-learning in the setting where the agent has no access to a simulator allowing to sample an arbitrary state-action pair. In this work, we consider such challenging setting where the environment is modelled by an episodic Markov Decision Process (MDP) of horizon \(H\). After \(T\) episodes, the performance of an agent is measured through regret which is the difference between the cumulative reward the agent could have obtained by acting optimally and what the agent really obtained during the interaction with the MDP.

This framework poses the famous exploration-exploitation dilemma where the agent must balance the need to try new state-action pairs to learn an optimal policy against exploiting the current observations to collect the rewards. One effective approach to resolving this dilemma is to adopt the principle of optimism in the face of uncertainty. In finite MDPs, this principle has been successfully implemented in the model-based algorithm using bonuses (Jaksch et al., 2010; Azar et al., 2017; Fruit et al., 2018; Dann et al., 2017; Zanette and Brunskill, 2019). Specifically, the upper confidence bounds (UCBs) on the optimal Q-value are built by adding bonuses and then used for planning. Building on this approach, Jin et al. (2018) proposed the OptQL algorithm, which applies a similar bonus-based technique to Q-learning, achieving efficient exploration. Recently, Zhang et al. (2020) introduced a simple modification to OptQL that achieves optimal sample complexity, making it competitive with model-based algorithms.

Another class of methods for optimistic exploration is Bayesian-based approaches. An iconic example among this class is the posterior sampling for reinforcement learning (PSRL,Strens 2000, Osband et al. 2013) algorithm. This model-based algorithm maintains a _surrogate Bayesian model_ of the MDP, for instance, a Dirichlet posterior on the transition probability distribution if the rewards are known. At each episode, a new MDP is sampled (i.e., a transition probability for each state-action pair) according to the posterior distribution of the Bayesian model. Then, the agent plans in this sampled MDP and uses the resulting policy to interact with the environment. Notably, an optimistic variant of PSRL, named optimistic posterior sampling for reinforcement learning (OPSRL, Agrawal and Jia, 2017; Tiapkin et al., 2022a) also enjoys an optimal sample complexity (Tiapkin et al., 2022a). The random least square value iteration (RLSVI, Osband et al. (2013)) is another well-known model-based algorithm that leverages a Bayesian-based technique for exploration. Precisely, RLSVI directly sets a Gaussian prior on the optimal Q-values and then updates the associated posterior trough value iteration in a model (Osband et al., 2013; Russo, 2019). A close variant of RLSVI proposed by Xiong et al. (2022), using a more sophisticated prior/posterior couple, is also proven to be near-optimal.

It is noteworthy that Bayesian-based exploration techniques have shown superior empirical performance compared to bonus-based exploration, at least in the tabular setting (Osband et al., 2013; Osband and Van Roy, 2017). Furthermore, these techniques have also been successfully applied to the deep RL setting (Osband et al., 2016; Azizzadenesheli et al., 2018; Fortunato et al., 2018; Li et al., 2022; Sasso et al., 2023). Finally, Bayesian methods allow for the incorporation of apriori information into exploration (e.g. by giving more weight to important states). However, most of the theoretical studies on Bayesian-based exploration have focused on model-based algorithms, raising the natural question of whether the PSRL approach can be extended to a provably efficient model-free algorithm that matches the good empirical performance of its model-based counterparts. Recently, Dann et al. (2021) proposed a model-free posterior sampling algorithm for structured MDPs, however, it is not computationally tractable. Therefore, a provably tractable model-free posterior sampling algorithm has remained a challenge.

In this paper, we aim to resolve this challenge. We propose the randomized Q-learning (RandQL) algorithm that achieves exploration without bonuses, relying instead on a novel idea of learning rate randomization. RandQL is a tractable model-free algorithm that updates an ensemble of Q-values via Q-learning with Beta distributed step-sizes. If tuned appropriately, the noise introduced by the random learning rates is similar to the one obtained by sampling from the posterior of the PSRL algorithm. Thus, one can see the ensemble of Q-values as posterior samples from the same induced posterior on the optimal Q-values as in PSRL. Then, RandQL chooses among these samples in the same optimistic fashion as OPSRL. We prove that for tabular MDPs, a staged version (Zhang et al., 2020) of RandQL, named Staged-RandQL enjoys the same regret bound as the OptQL algorithm, that is, \(\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})\) where \(S\) is the number of states and \(A\) the number of actions. Furthermore, we extend Staged-RandQL beyond the tabular setting into the Net-Staged-RandQL algorithm to deal with metric state-action spaces (Domingues et al., 2021; Sinclair et al., 2019). Net-Staged-RandQL operates similarly to Staged-RandQL but over a fixed discretization of the state-action space and uses a specific prior tuning to handle the effect of discretization. We prove that Net-Staged-RandQL enjoys a regret bound of order \(\widetilde{\mathcal{O}}(H^{5/2}T^{(d_{c}+1)/(d_{c}+2)})\), where \(d_{c}\) denotes the covering dimension. This rate is of the same order as the one of Adaptive-QL by Sinclair et al. (2019, 2023), an adaptation of OptQL to metric state-action space and has a better dependence on the budget \(T\) than one of the model-based kernel algorithms such that Kernel-UCBVI by Domingues et al. (2021c). We also explain how to adapt Net-Staged-RandQL and its analysis to work with an adaptive discretization as by Sinclair et al. (2019, 2023). Finally, we provide preliminary experiments to illustrate the good performance of RandQL against several baselines in finite and continuous environments.

We highlight our main contributions:

* The RandQL algorithm, a new tractable (provably efficient) model-free Q-learning adaptation of the PSRL algorithm that explores through randomization of the learning rates.
* A regret bound of order \(\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})\) for a staged version of the RandQL algorithm in finite MDPs where \(S\) is the number of states and \(A\) the number of actions, \(H\) the horizon and \(T\) the budget.
* A regret bound of order \(\widetilde{\mathcal{O}}(H^{5/2}T^{(d_{c}+1)/(d_{c}+2)})\) for an adaptation of RandQL to metric spaces where \(d_{c}\) denotes the covering dimension.
* Adaptive version of metric space extension of RandQL algorithm that achieves a regret bound of order \(\widetilde{\mathcal{O}}(H^{5/2}T^{(d_{z}+1)/(d_{z}+2)})\), where \(d_{z}\) is a _zooming_ dimension.
* Experiments in finite and continuous MDPs that show that RandQL is competitive with model-based and model-free baselines while keeping a low time-complexity.

## 2 Setting

We consider an episodic MDP \(\big{(}\mathcal{S},\mathcal{A},H,\{p_{h}\}_{h\in[H]},\{r_{h}\}_{h\in[H]}\big{)}\), where \(\mathcal{S}\) is the set of states, \(\mathcal{A}\) is the set of actions, \(H\) is the number of steps in one episode, \(p_{h}(s^{\prime}|s,a)\) is the probability transition from state \(s\) to state \(s^{\prime}\) upon taking action \(a\) at step \(h\), and \(r_{h}(s,a)\in[0,1]\) is the bounded deterministic reward received after taking the action \(a\) in state \(s\) at step \(h\). Note that we consider the general case of rewards and transition functions that are possibly non-stationary, i.e., that are allowed to depend on the decision step \(h\) in the episode.

Policy & value functionsA _deterministic_ policy \(\pi\) is a collection of functions \(\pi_{h}:\mathcal{S}\rightarrow\mathcal{A}\) for all \(h\in[H]\), where every \(\pi_{h}\) maps each state to a _single_ action. The value functions of \(\pi\), denoted by \(V^{\pi}_{h}\), as well as the optimal value functions, denoted by \(V^{\pi}_{h}\) are given by the Bellman and the optimal Bellman equations,

\[Q^{\pi}_{h}(s,a) =r_{h}(s,a)+p_{h}V^{\pi}_{h+1}(s,a) V^{\pi}_{h}(s) =\pi_{h}Q^{\pi}_{h}(s)\] \[Q^{\pi}_{h}(s,a) =r_{h}(s,a)+p_{h}V^{\star}_{h+1}(s,a) V^{\star}_{h}(s) =\max_{a}Q^{\star}_{h}(s,a),\]

where by definition, \(V^{\star}_{H+1}\triangleq V^{\pi}_{H+1}\triangleq 0\). Furthermore, \(p_{h}f>(s,a)\triangleq\mathbb{E}_{s^{\prime}\sim p_{h}(\cdot\mid s,a)}[f(s^{ \prime})]\) denotes the expectation operator with respect to the transition probabilities \(p_{h}\) and \(\pi_{h}g(s)\triangleq g(s,\pi_{h}(s))\) denotes the composition with the policy \(\pi\) at step \(h\).

Learning problemThe agent, to which the transitions are _unknown_ (the rewards are assumed to be known1 for simplicity), interacts with the environment during \(T\) episodes of length \(H\), with a _fixed_ initial state \(s_{1}\).2 Before each episode \(t\) the agent selects a policy \(\pi^{t}\) based only on the past observed transitions up to episode \(t-1\). At each step \(h\in[H]\) in episode \(t\), the agent observes a state \(s^{t}_{h}\in\mathcal{S}\), takes an action \(\pi^{t}_{h}(s^{t}_{h})=a^{t}_{h}\in\mathcal{A}\) and makes a transition to a new state \(s^{t}_{h+1}\) according to the probability distribution \(p_{h}(s^{t}_{h},a^{t}_{h})\) and receives a deterministic reward \(r_{h}(s^{t}_{h},a^{t}_{h})\).

Footnote 1: Our work can be extended without too much difficulty to the case of random rewards.

RegretThe quality of an agent is measured through its regret, that is the difference between what it could obtain (in expectation) by acting optimally and what it really gets,

\[\mathfrak{R}^{T}\triangleq\sum_{t=1}^{T}V^{\star}_{1}(s_{1})-V^{\pi^{t}}_{1}(s _{1})\,.\]Additional notationFor \(N\in\mathbb{N}_{++}\), we define the set \([N]\triangleq\{1,\ldots,N\}\). We denote the uniform distribution over this set by \(\mathrm{Unif}[N]\). We define the beta distribution with parameters \(\alpha,\beta\) as \(\mathrm{Beta}(\alpha,\beta)\). Appendix A references all the notation used.

## 3 Randomized Q-learning for Tabular Environments

In this section we assume that the state space \(\mathcal{S}\) is finite of size \(S\) as well as the action space \(\mathcal{A}\) of size \(A\). We first provide some intuitions for RandQL algorithm.

### Concept

The main idea of RandQL is to perform the usual Q-learning updates but instead of adding bonuses to the targets as OptQL to drive exploration, RandQL injects noise into the updates of the Q-values through _noisy learning rates_. Precisely, for \(J\in\mathbb{N}\), we maintain an ensemble of size \(J\) of Q-values3\((\overline{Q}^{n,j})_{j\in[J]}\) updated with random independent Beta-distributed step-sizes \((w_{n,j})_{j\in[J]}\) where \(w_{n,j}\sim\mathrm{Beta}(H,n)\). Then, policy Q-values \(\overline{Q}^{n}\) are obtained by taking the maximum among the Q-values of the ensemble

Footnote 3: We index the quantities by \(n\) in this section where \(n\) is the number of times the state-action pair \((s,a)\) is visited. In particular this is different from the global time \(t\) since, in our setting, all the state- action pair are not visited at each episode. See Section 3.2 and Appendix B precise notations.

\[\overline{Q}^{n+1,j}_{h}(s,a) =(1-w_{n,j})\overline{Q}^{n,j}_{h}(s,a)+w_{n,j}[r_{h}(s,a)+ \overline{V}^{n}_{h+1}(s^{n}_{h+1})]\] \[\overline{Q}^{n+1}_{h}(s,a) =\max_{j\in[J]}\overline{Q}^{n+1,j}_{h}(s,a),\quad\overline{V}^{ n+1}_{h}(s)=\max_{a\in\mathcal{A}}\overline{Q}^{n+1}_{h}(s,a),\]

where \(s^{n}_{h+1}\) stands for the next (in time) state after \(n\)-th visitation of \((s,a)\) at step \(h\).

Note that the policy Q-values \(\overline{Q}^{n}\) are designed to be upper confidence bound on the optimal Q-values. The policy used to interact with the environment is greedy with respect to the policy Q-values \(\pi^{n}_{h}(s)\in\arg\max_{a}\overline{Q}^{n}_{h}(s,a)\). We provide a formal description of RandQL in Appendix B.

Connection with OptQL We observe that the learning rates of RandQL are in expectation of the same order \(\mathbb{E}[w_{n,j}]=H/(n+H)\) as the ones used by the OptQL algorithm. Thus, we can view our randomized Q-learning as a noisy version of the OptQL algorithm that doesn't use bonuses.

Connection with PSRL If we unfold the recursive formula above we can express the Q-values \(\overline{Q}^{n+1,j}\) as a weighted sum

\[\overline{Q}^{n+1,j}_{h}(s,a)=W^{0}_{n,j}\overline{Q}^{1,j}_{h}(s,a)+\sum_{k =1}^{n}W^{k}_{n,j}[r_{h}(s,a)+\overline{V}^{k}_{h+1}(s^{k}_{h+1})],\]

where we define \(W^{0}_{n,j}=\prod_{\ell=0}^{n-1}(1-w_{\ell,j})\) and \(W^{k}_{n,j}=w_{k-1,j}\prod_{\ell=k}^{n-1}(1-w_{\ell,j})\).

To compare, we can unfold the corresponding formula for PSRL algorithm using the aggregation properties of the Dirichlet distribution (see e.g. Section 4 of Tiapkin et al. (2022) or Appendix C)

\[\overline{Q}^{n+1}_{h}(s,a)=\widetilde{W}^{0}_{n}\overline{Q}^{1}_{h}(s,a)+ \sum_{k=1}^{n}\widetilde{W}^{k}_{n}[r_{h}(s,a)+\overline{V}^{n+1}_{h+1}(s^{k }_{h+1})], \tag{1}\]

where weights \((\widetilde{W}^{0}_{n},\ldots,\widetilde{W}^{n}_{n})\) follows Dirichlet distribution \(\mathrm{Dir}(n_{0},1,\ldots,1)\) and \(n_{0}\) is a weight for the prior distribution. In particular, one can represent these weights as partial products of other weights \(w_{n}\sim\mathrm{Beta}(1,n+n_{0})\). If we use (1) to construct a model-free algorithm, this would require recomputing the targets \(r_{h}(s,a)+\overline{V}^{n+1}(s^{k}_{h+1})\) in each iteration. To make algorithm more efficient and model-free, we approximate \(\overline{V}^{n+1}\) by \(\overline{V}^{k}\), and, as a result, obtain RandQL algorithm with weight distribution \(w_{n,j}\sim\mathrm{Beta}(1,n+n_{0})\).

Note that in expectation this algorithm is equivalent to OptQL with the uniform step-sizes which are known to be sub-optimal due to a high bias (see discussion in Section 3 of (Jin et al., 2018)). There are two known ways to overcome this sub-optimality for Q-learning: to introduce more aggressive learning rates \(w_{n,j}\sim\mathrm{Beta}(H,n+n_{0})\) leading to RandQL algorithm, or to use stage-dependent framework by Bai et al. (2019); Zhang et al. (2020) resulting in Staged-RandQL algorithm.

The aforementioned transition from PSRL to RandQL is similar to the transition from UCBVI(Azar et al., 2017) to Q-learning. To make UCBVI model-free, one has to to keep old targets in Q-values. This, however, introduces a bias that could be eliminated either by more aggressive step-size (Jin et al., 2018) or by splitting on stages (Bai et al., 2019). Our algorithms (RandQL and Staged-RandQL) perform the similar tricks for PSRL and thus could be viewed as model-free versions of it. Additionally, RandQL shares some similarities with the OPSRL algorithm (Agrawal and Jia, 2017; Tiapkin et al., 2022) in the way of introducing optimism (taking maximum over \(J\) independent ensembles of Q-values). Let us also mention a close connection to the theory of Dirichlet processes in the proof of optimism for the case of metric spaces (see Remark 1 in Appendix E.4).

PriorAs remarked above, in expectation, RandQL has a learning rate of the same order as OptQL. In particular, it implies that the first \((1-1/H)\) fraction of the the target will be forgotten exponentially fast in the estimation of the Q-values, see Jin et al. (2018); Menard et al. (2021). Thus we need to re-inject prior targets, as explained in Appendix B, in order to not forget too quickly the prior and thus replicate the same exploration mechanism as in the PSRL algorithm.

### Algorithm

In this section, following Bai et al. (2019); Zhang et al. (2020), we present the Staged-RandQL algorithm a scheduled version of RandQL that is simpler to analyse. The main idea is that instead of using a carefully tuned learning rate to keep only the last \(1/H\) fraction of the targets we split the learning of the Q-values in stages of exponentially increasing size with growth rate of order \(1+1/H\). At a given stage, the estimate of the Q-value relies only on the targets within this stage and resets at the beginning of the next stage. Notice that the two procedures are almost equivalent. A detail description of Staged-RandQL is provided in Algorithm 1.

**Counts and stages** Let \(n^{t}_{h}(s,a)\triangleq\sum_{i=1}^{t-1}\mathds{1}\{(s^{i}_{h},a^{i}_{h})=(s, a)\}\) be the number of visits of state-action pair \((s,a)\) at step \(h\) before episode \(t\). We say that a triple \((s,a,h)\) belongs to the \(k\)-th stage at the beginning of episode \(t\) if \(n^{t}_{h}(s,a)\in[\sum_{i=0}^{k-1}e_{i},\sum_{i=0}^{k}e_{i})\). Here \(e_{k}=\lfloor(1+1/H)^{k}\cdot H\rfloor\) is the length of the stage \(k\geq 0\) and, by convention, \(e_{-1}=0\). Let \(\widetilde{n}^{t}_{h}(s,a)\triangleq n^{t}_{h}(s,a)-\sum_{i=0}^{k-1}e_{i}\) be the number of visits of state-action pair \((s,a)\) at step \(h\) during the current stage \(k\).

**Temporary Q-values** At the beginning of a stage, let say time \(t\), we initialize \(J\)_temporary_ Q-values as \(\widetilde{Q}^{t,j}_{h}(s,a)=r_{h}(s,a)+r_{0}(H-h-1)\) for \(j\in[J]\) and \(r_{0}\) some pseudo-reward. Then as long as \((s^{t}_{h},a^{t}_{h},h)\) remains within a stage we update recursively the _temporary_ Q-values

\[\widetilde{Q}^{t+1,j}_{h}(s,a)=\begin{cases}(1-w_{j,\widetilde{n}})\widetilde {Q}^{t,j}_{h}(s,a)+w_{j,\widetilde{n}}[r_{h}(s,a)+\overline{V}^{t}_{h+1}(s^{ t}_{h+1})],&(s,a)=(s^{t}_{h},a^{t}_{h})\\ \widetilde{Q}^{t,j}_{h}(s,a)&\text{otherwise},\end{cases}\]

where \(\widetilde{n}=\widetilde{n}^{t}_{h}(s,a)\) is the number of visits, \(w_{j,\widetilde{n}}\) is a sequence of i.i.d. random variables \(w_{j,\widetilde{n}}\sim\mathrm{Beta}(1/\kappa,(\widetilde{n}+n_{0})/\kappa)\) with \(\kappa>0\) being some posterior inflation coefficient and \(n_{0}\) a number of pseudo-transitions.

**Policy Q-values** Next we define the policy Q-values that is updated at the end of a stage. Let say for state-action pair \((s,a)\) at step \(h\) an stage ends at time \(t\). This policy Q-values is then given by the maximum of temporary Q-values \(\overline{Q}^{t+1}_{h}=\max_{j\in[J]}\widetilde{Q}^{t+1,j}_{h}(s,a)\). Then the policy Q-values is constant within a stage. The value used to defined the targets is \(\overline{V}^{t+1}_{h}(s)=\max_{a\in\mathcal{A}}\overline{Q}^{t+1}_{h}(s,a)\). The policy used to interact with the environment is greedy with respect to the policy Q-values \(\pi^{t+1}_{h}(s)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\overline{Q}^{t+1 }_{h}(s,a)\) (we break ties arbitrarily).

### Regret bound

We fix \(\delta\in(0,1)\) and the number of posterior samples \(J\triangleq\lceil c_{J}\cdot\log(2SAHT/\delta)\rceil\), where \(c_{J}=1/\log(2/(1+\Phi(1)))\) and \(\Phi(\cdot)\) is the cumulative distribution function (CDF) of a normal distribution. Note that \(J\) has a logarithmic dependence on \(S,A,H,T,\) and \(1/\delta\).

We now state the regret bound of Staged-RandQL with a full proof in Appendix D.

```
1:Input: inflation coefficient \(\kappa\), \(J\) ensemble size, number of prior transitions \(n_{0}\), prior reward \(r_{0}\).
2:Initialize:\(\overline{V}_{h}(s)=\overline{Q}_{h}(s,a)=\widetilde{Q}_{h}^{j}(s,a)=r(s,a)+r_{0}(H-h -1)\), initialize counters \(\widetilde{n}_{h}(s,a)=0\) for \(j,h,s,a\in[J]\times[H]\times\mathcal{S}\times\mathcal{A}\) and stage \(q_{h}(s,a)=0\).
3:for\(t\in[T]\)do
4:for\(h\in[H]\)do
5: Play \(a_{h}\in\arg\max_{a}\overline{Q}_{h}(s_{h},a)\).
6: Observe reward and next state \(s_{h+1}\sim p_{h}(s_{h},a_{h})\).
7: Sample learning rates \(w_{j}\sim\mathrm{Beta}(1/\kappa,(\widetilde{n}+n_{0})/\kappa)\) for \(\widetilde{n}=\widetilde{n}_{h}(s_{h},a_{h})\).
8: Update temporary \(Q\)-values for all \(j\in[J]\) \[\widetilde{Q}_{h}^{j}(s_{h},a_{h}):=(1-w_{j})\widetilde{Q}_{h}^{j}(s_{h},a_{h })+w_{j}\big{(}r_{h}(s_{h},a_{h})+\overline{V}_{h+1}(s_{h+1})\big{)}\,.\]
9: Update counter \(\widetilde{n}_{h}(s_{h},a_{h}):=\widetilde{n}_{h}(s_{h},a_{h})+1\)
10:if\(\widetilde{n}_{h}(s_{h},a_{h})=\lfloor(1+1/H)^{q}H\rfloor\) for \(q=q_{h}(s_{h},a_{h})\) being the current stage then
11: Update policy \(Q\)-values \(\overline{Q}_{h}(s_{h},a_{h}):=\max_{j\in[J]}\widetilde{Q}_{h}^{j}(s_{h},a_{h})\).
12: Update value function \(\overline{V}_{h}(s_{h}):=\max_{a\in\mathcal{A}}\overline{Q}_{h}(s_{h},a)\)
13: Reset temporary \(Q\)-values \(\widetilde{Q}_{h}^{j}(s_{h},a_{h}):=r_{h}(s_{h},a_{h})+r_{0}(H-h-1)\).
14: Reset counter \(\widetilde{n}_{h}(s_{h},a_{h}):=0\) and change stage \(q_{h}(s_{h},a_{h}):=q_{h}(s_{h},a_{h})+1\).
15:endif
16:endfor
17:endfor
```

**Algorithm 1** Tabular Staged-RandQL

**Theorem 1**.: _Consider a parameter \(\delta\in(0,1)\). Let \(\kappa\triangleq 2(\log(8SAH/\delta)+3\log(e\pi(2T+1)))\), \(n_{0}\triangleq\lceil\kappa(c_{0}+\log_{17/16}(T))\rceil\), \(r_{0}\triangleq 2\), where \(c_{0}\) is an absolute constant defined in (5); see Appendix D.3. Then for Staged-RandQL, with probability at least \(1-\delta\),_

\[\mathfrak{R}^{T}=\widetilde{\mathcal{O}}\Big{(}\sqrt{H^{5}SAT}+H^{3}SA\Big{)}.\]

DiscussionThe regret bound of Theorem 1 coincides (up to a logarithmic factor) with the bound of the OptQL algorithm with Hoeffding-type bonuses from Jin et al. (2018). Up to a \(H\) factor, our regret matches the information-theoretic lower bound \(\Omega(\sqrt{H^{3}SAT})\)(Jin et al., 2018; Domingues et al., 2021). This bound could be achieved (up to logarithmic terms) in model-free algorithms by using Bernstein-type bonuses and variance reduction (Zhang et al., 2020). We keep these refinements for future research as the main focus of our paper is on the novel randomization technique and its use to construct computationally tractable model-free algorithms.

Computational complexityStaged-RandQL is a model-free algorithm, and thus gets the \(\widetilde{\mathcal{O}}(HSA)\) space complexity as OptQL, recall that we set \(J=\widetilde{\mathcal{O}}(1)\). The per-episode time-complexity is also similar and of order \(\widetilde{\mathcal{O}}(H)\).

## 4 Randomized Q-learning for Metric Spaces

In this section we present a way to extend RandQL to general state-action spaces. We start from the simplest approach with predefined \(\varepsilon\)-net type discretization of the state-action space \(\mathcal{S}\times\mathcal{A}\)(see Song and Sun, 2019), and then discuss an adaptive version of the algorithm, similar to one presented by Sinclair et al. (2019).

### Assumptions

To pose the first assumption, we start from a general definition of covering numbers.

**Definition 1** (Covering number and covering dimension).: Let \((M,\rho)\) be a metric space. A set \(\mathcal{M}\) of open balls of radius \(\varepsilon\) is called an \(\varepsilon\)-cover of \(M\) if \(M\subseteq\bigcup_{B\in\mathcal{M}}B\). The cardinality of the minimal \(\varepsilon\)-cover is called covering number \(N_{\varepsilon}\) of \((M,\rho)\). We denote the corresponding minimal \(\varepsilon\)-covering by \(\mathcal{N}_{\varepsilon}\). A metric space \((M,\rho)\) has a covering dimension \(d_{c}\) if \(\forall\varepsilon>0:N_{\varepsilon}\leq C_{N}\varepsilon^{-d_{c}}\), where \(C_{N}\) is a constant.

The last definition extends the definition of dimension beyond vector spaces. For example, is case of \(M=[0,1]^{d}\) the covering dimension of \(M\) is equal to \(d\). For more details and examples see e.g. Vershynin (2018, Section 4.2).

Next we are ready to introduce the first assumption.

**Assumption 1** (Metric Assumption).: Spaces \(\mathcal{S}\) and \(\mathcal{A}\) are separable compact metric spaces with the corresponding metrics \(\rho_{\mathcal{S}}\) and \(\rho_{\mathcal{A}}\). The joint space \(\mathcal{S}\times\mathcal{A}\) endowed with a product metric \(\rho\) that satisfies \(\rho((s,a),(s^{\prime},a^{\prime}))\leq\rho_{\mathcal{S}}(s,s^{\prime})+\rho_{ \mathcal{A}}(a,a^{\prime})\). Moreover, the diameter of \(\mathcal{S}\times\mathcal{A}\) is bounded by \(d_{\max}\), and \(\mathcal{S}\times\mathcal{A}\) has covering dimension \(d_{c}\) with a constant \(C_{N}\).

This assumption is, for example, satisfied for the finite state and action spaces endowed with discrete metrics \(\rho_{\mathcal{S}}(s,s^{\prime})=\mathds{1}\{s\neq s^{\prime}\},\rho_{\mathcal{ A}}(a,a^{\prime})=\mathds{1}\{a\neq a^{\prime}\}\) with \(d_{c}=0,\,C_{N}=SA\) and \(S\) and \(A\) being the cardinalities of the state and action spaces respectively. The above assumption also holds in the case \(\mathcal{S}\subseteq[0,1]^{d_{\mathcal{S}}}\) and \(\mathcal{A}\subseteq[0,1]^{d_{\mathcal{A}}}\) with \(d_{c}=d_{\mathcal{S}}+d_{\mathcal{A}}\).

The next two assumptions describe the regularity conditions of transition kernel and rewards.

**Assumption 2** (Reparametrization Assumption).: The Markov transition kernel could be represented as an iterated random function. In other words, there exists a measurable space \((\Xi,\mathcal{F}_{\Xi})\) and a measurable function \(F_{h}\colon(\mathcal{S}\times\mathcal{A})\times\Xi\to\mathcal{S}\), such that \(s_{h+1}\sim p_{h}(s_{h},a_{h})\iff s_{h+1}=F_{h}(s_{h},a_{h},\xi_{h})\) for a sequence of independent random variables \(\{\xi_{h}\}_{h\in[H]}\).

This assumption is naturally satisfied for a large family of probabilistic model, see Kingma and Welling (2014). Moreover, it has been utilized by the RL community both in theory (Ye and Zhou, 2015) and practice (Heess et al., 2015; Liu et al., 2018). Essentially, this assumption holds for Markov transition kernels over a separable metric space, see Theorem 1.3.6 by Douc et al. (2018). However, the function \(F_{h}\) could be ill-behaved. To avoid this behaviour, we need the following assumption.

**Assumption 3** (Lipschitz Assumption).: The function \(F_{h}(\cdot,\xi_{h})\) is \(L_{F}\)-Lipschitz in the first argument for almost every value of \(\xi_{h}\). Additionally, the reward function \(r_{h}\colon\mathcal{S}\times\mathcal{A}\to[0,1]\) is \(L_{r}\)-Lipschitz.

This assumption is commonly used in studies of the Markov processes corresponding to iterated random functions, see Diaconis and Freedman (1999); Ghosh and Marecek (2022). Moreover, this assumption holds for many cases of interest. As main example, it trivially holds in tabular and Lipschitz continuous deterministic MDPs (Ni et al., 2019). Notably, this observation demonstrates that Assumption 3 does not necessitate Lipschitz continuity of the transition kernels in total variation distance, since deterministic Lipschitz MDPs are not continuous in that sense. Additionally, incorporation of an additive noise to deterministic Lipschitz MDPs will lead to Assumption 3 with \(L_{F}=1\).

Furthermore, it is possible to show that Assumption 3 implies other assumptions stated in the literature. For example, it implies that the transition kernel is Lipschitz continuous in \(1\)-Wasserstein metric, and that \(Q^{\star}\) and \(V^{\star}\) are both Lipschitz continuous.

**Lemma 1**.: _Let Assumption 1,2,3 hold. Then the transition kernels \(p_{h}(s,a)\) are \(L_{F}\)-Lipschitz continuous in \(1\)-Wasserstein distance_

\[\mathcal{W}_{1}(p_{h}(s,a),p_{h}(s^{\prime},a^{\prime}))\leq L_{F}\cdot\rho((s,a),(s^{\prime},a^{\prime})),\]

_where \(1\)-Wasserstein distance between two probability measures on the metric space \((M,\rho)\) is defined as \(\mathcal{W}_{1}(\nu,\eta)=\sup_{f\text{ is }1-\text{Lipschitz}}\int_{M}f\mathrm{d}\nu -\int_{M}f\mathrm{d}\eta\)._

**Lemma 2**.: _Let Assumption 1,2,3 hold. Then \(Q^{\star}_{h}\) and \(V^{\star}_{h}\) are Lipschitz continuous with Lipschitz constant \(L_{V,h}\leq\sum_{h^{\prime}=h}^{H}L_{F}^{h^{\prime}-h}L_{r}\)._

The proof of these lemmas is postponed to Appendix E. For a more detailed exposition on 1-Wasserstein distance we refer to the book by Peyre and Cuturi (2019). The first assumption was studied by Domingues et al. (2021); Sinclair et al. (2023) in the setting of model-based algorithms in metric spaces. We are not aware of any natural examples of MDPs with a compact state-action space where the transition kernels are Lipschitz in \(\mathcal{W}_{1}\) but fail to satisfy Assumption 3.

### Algorithms

In this section, following Song and Sun (2019), we present Net-Staged-RandQL algorithm that combines a simple non-adaptive discretization and an idea of stages by Bai et al. (2019); Zhang et al. (2020).

We assume that we have an access to all Lipschitz constants \(L_{r},L_{F},L_{V}\triangleq L_{V,1}\). Additionally, we have access to the oracle that computes \(\varepsilon\)-cover \(\mathcal{N}_{\varepsilon}\) of the space \(\mathcal{S}\times\mathcal{A}\) for any predefined \(\varepsilon>0\)4.

Footnote 4: Remark that the simple greedy algorithm can generate \(\varepsilon\)-cover of size \(N_{\varepsilon/2}\), that will not affect the asymptotic behavior of our regret bounds, see Song and Sun (2019).

**Counts and stages** Let \(n^{t}_{h}(B)\triangleq\sum_{i=1}^{t-1}\mathds{1}\{(s^{t}_{h},a^{i}_{h})\in B\}\) be the number of visits of the ball \(B\in\mathcal{N}_{\varepsilon}\) at step \(h\) before episode \(t\). Let \(e_{k}=[(1+1/H)^{k}\cdot H]\) be length of the stage \(k\geq 0\) and, by convention, \(e_{-1}=0\). We say that \((B,h)\) belongs to the \(k\)-th stage at the beginning of episode \(t\) if \(n^{t}_{h}(B)\in[\sum_{i=0}^{k-1}e_{i},\sum_{i=0}^{k}e_{i})\). Let \(\widetilde{n}^{t}_{h}(B)\triangleq n^{t}_{h}(s,a)-\sum_{i=0}^{k-1}e_{i}\) be the number of visits of the ball \(B\) at step \(h\) during the current stage \(k\).

**Temporary Q-values** At the beginning of a stage, let say time \(t\), we initialize \(J\)_temporary_ Q-values as \(\widetilde{Q}^{t,j}_{h}(B)=r_{0}H\) for \(j\in[J]\) and \(r_{0}\) some pseudo-reward. Then within a stage \(k\) we update recursively the _temporary_ Q-values

\[\widetilde{Q}^{t+1,j}_{h}(B)=\begin{cases}(1-w_{j,\widetilde{n}})\widetilde{Q }^{t,j}_{h}(B)+w_{j,\widetilde{n}}[r_{h}(s^{t}_{h},a^{t}_{h})+\overline{V}^{t} _{h+1}(s^{t}_{h+1})],&(s,a)=(s^{t}_{h},a^{t}_{h})\\ \widetilde{Q}^{1,j}_{h}(B)&\text{otherwise,}\end{cases}\]

where \(\widetilde{n}=\widetilde{n}^{t}_{h}(B)\) is the number of visits, \(w_{j,\widetilde{n}}\) is a sequence of i.i.d random variables \(w_{j,\widetilde{n}}\sim\mathrm{Beta}(1/\kappa,(\widetilde{n}+n_{0}(k))/\kappa)\) with \(\kappa>0\) some posterior inflation coefficient and \(n_{0}(k)\) a number of pseudo-transitions. The important difference between tabular and metric settings is the dependence on the pseudo-count \(n_{0}(k)\) on \(k\) in the latter case, since here the prior is used to eliminate the approximation error.

**Policy Q-values** Next, we define the policy Q-values that are updated at the end of a stage. Let us fix a ball \(B\) at step \(h\) and suppose that the currents stage ends at time \(t\). Then the policy Q-values are given by the maximum of the temporary Q-values \(\overline{Q}^{t+1}_{h}(B)=\max_{j\in[J]}\widetilde{Q}^{t+1,j}_{h}(B)\). The policy Q-values are constant within a stage. The value used to define the targets is computed on-flight using the formula \(\overline{V}^{t}_{h}(s)=\max_{a\in\mathcal{A}}\overline{Q}^{t}_{h}(\psi_{ \varepsilon}(s,a))\), where \(\psi_{\varepsilon}\colon\mathcal{S}\times\mathcal{A}\to\mathcal{N}_{\varepsilon}\) is a quantization map, that assigns each state-action pair \((s,a)\) to a ball \(B\ni(s,a)\). The policy used to interact with the environment is greedy with respect to the policy Q-values and also computed on-flight \(\pi^{t}_{h}(s)\in\arg\max_{a\in\mathcal{A}}\overline{Q}^{t}_{h}(\psi_{ \varepsilon}(s,a))\) (we break ties arbitrarily).

A detail description of Net-Staged-RandQL is provided in Algorithm 4 in Appendix E.2.

Footnote 4: Remark that the simple greedy algorithm can generate \(\varepsilon\)-cover of size \(N_{\varepsilon/2}\), that will not affect the asymptotic behavior of our regret bounds, see Song and Sun (2019).

### Regret Bound

We fix \(\delta\in(0,1),\) the discretization level \(\varepsilon>0\) and the number of posterior samples

\[J\triangleq\lceil\tilde{c}_{J}\cdot(\log(2C_{N}HT/\delta)+d_{c}\log(1/ \varepsilon))\rceil,\]

where \(\tilde{c}_{J}=1/\log(4/(3+\Phi(1)))\) and \(\Phi(\cdot)\) is the cumulative distribution function (CDF) of a normal distribution. Note that \(J\) has a logarithmic dependence on \(H,T,1/\varepsilon\) and \(1/\delta\). For the regret-optimal discretization level \(\varepsilon=T^{-1/(d_{c}+2)}\), the number \(J\) is almost independent of \(d_{c}\). Let us note that the role of prior in metric spaces is much higher than in the tabular setting. Another important difference is dependence of the prior count on the stage index. In particular, we have

\[n_{0}(k)=\bigg{\lceil}\widetilde{n}_{0}+\kappa+\frac{\varepsilon L}{H-1} \cdot(e_{k}+\widetilde{n}_{0}+\kappa)\bigg{\rceil},\qquad\widetilde{n}_{0}=(c _{0}+1+\log_{17/16}(T))\cdot\kappa\]

where \(c_{0}\) is an absolute constant defined in (5) ( see Appendix D.3), \(\kappa\) is the posterior inflation coefficient and \(L=L_{r}+(1+L_{F})L_{V}\) is a constant. We now state the regret bound of Net-Staged-RandQL with a full proof being postponed to Appendix E.

**Theorem 2**.: _Suppose that \(N_{\varepsilon}\leq C_{N}\varepsilon^{-d_{c}}\) for all \(\varepsilon>0\) and some constant \(C_{N}>0\). Consider a parameter \(\delta\in(0,1)\) and take an optimal level of discretization \(\varepsilon=T^{-1/(d_{c}+2)}\). Let \(\kappa\triangleq 2(\log(8HC_{N}/\delta)+d_{c}\log(1/\varepsilon)+3\log(\varepsilon \pi(2T+1))),\)\(r_{0}\triangleq 2\). Then it holds for Net-Staged-RandQL with probability at least \(1-\delta\),_

\[\mathfrak{R}^{T}=\widetilde{\mathcal{O}}\bigg{(}H^{5/2}C_{N}^{1/2}T^{\frac{d_{c }+1}{d_{c}+2}}+H^{3}C_{N}T^{\frac{d_{c}}{d_{c}+2}}+LT^{\frac{d_{c}+1}{d_{c}+2 }}\bigg{)}.\]

We can restore the regret bound in the tabular setting by letting \(d_{c}=0\) and \(C_{N}=SA\), where \(S\) is the cardinality of the state-space, and \(A\) is the cardinality of the action-space.

DiscussionFrom the point of view of instance-independent bounds, our algorithm achieves the same result as Net-QL (Song and Sun, 2019) and Adaptive-QL (Sinclair et al., 2019), that matches the lower bound \(\Omega(HT^{\frac{d+1}{d_{c}+2}})\) by Sinclair et al. (2023) in dependence on budget \(T\) and covering dimension \(d_{c}\). Notably, as discussed by Sinclair et al. (2023), the model-based algorithm such as Kernel-UCBVI(Domingues et al., 2021) does not achieves optimal dependence in \(T\) due to hardness of the transition estimation problem.

Computational complexityFor a fixed level of discretization \(\varepsilon\), our algorithm has a space complexity of order \(\widetilde{\mathcal{O}}(H\mathcal{N}_{\varepsilon})\). Assuming that the computation of a quantization map \(\psi_{\varepsilon}\) has \(\widetilde{\mathcal{O}}(1)\) time complexity, we achieve a per-episode time complexity of \(\widetilde{\mathcal{O}}(HA)\) for a finite action space and \(\mathcal{O}(HN_{\varepsilon})\) for an infinite action space in the worst case due to computation of \(\arg\max_{a\in\mathcal{A}}\overline{Q}_{h}(\psi_{\varepsilon}(s,a))\). However, this can be improved to \(\widetilde{\mathcal{O}}(H)\) if we consider adaptive discretization (Sinclair et al., 2019).

Adaptive discretizationAdditionally, we propose a way to combine RandQL with adaptive discretization by Cao and Krishnamurthy (2020), Sinclair et al. (2023). This combination results in two algorithms: Adaptive-RandQL and Adaptive-Staged-RandQL. The second one could achieve the instance-dependent regret bound that scales with a _zooming_ dimension, the instance-dependent measure of dimension. We will follow Sinclair et al. (2023) in the exposition of the required notation.

**Definition 2**.: For any \((s,a)\in\mathcal{S}\times\mathcal{A}\), the stage-dependent sub-optimality gap is defined as \(\mathrm{gap}_{h}(s,a)=V^{*}_{h}(s)-Q^{*}_{h}(s,a)\).

This quantity is widely used in the theoretical instance-dependent analysis of reinforcement learning and contextual bandit algorithms.

**Definition 3**.: The near-optimal set of \(\mathcal{S}\times\mathcal{A}\) for a given value \(\varepsilon\) defined as \(Z^{\varepsilon}_{h}=\{(s,a)\in\mathcal{S}\times\mathcal{A}\mid\mathrm{gap}_{h }(s,a)\leq(H+1)\varepsilon\}\).

The main insight of this definition is that essentially we are interested in a detailed discretization of the near-optimal set \(Z^{\varepsilon}_{h}\) for small \(\varepsilon\), whereas all other state-action pairs could be discretized in a more rough manner. Interestingly enough, \(Z^{\varepsilon}_{h}\) could be a lower dimensional manifold, leading to the following definition.

**Definition 4**.: The step-\(h\) zooming dimension \(d_{z,h}\) with a constant \(C_{N,h}\) and a scaling factor \(\rho>0\) is given by

\[d_{z,h}=\inf\bigl{\{}d>0:\forall\varepsilon>0\;N_{\varepsilon}(Z^{\rho\, \varepsilon}_{h})\leq C_{N,h}\varepsilon^{-d}\bigr{\}}.\]

Under some additional structural assumptions on \(Q^{*}_{h}\), it is possible to show that the zooming dimension could be significantly smaller than the covering dimension, see, e.g., Lemma 2.8 in Sinclair et al. (2023). However, at the same time, it has been shown that \(d_{z,h}\geq d_{\mathcal{S}}-1\), where \(d_{\mathcal{S}}\) is a covering dimension of the state space. Thus, the zooming dimension allows adaptation to a rich action space but not a rich state space.

Given this definition, it is possible to define define an adaptive algorithm Adaptive-Staged-RandQL that attains the following regret guarantees

**Theorem 3**.: _Consider a parameter \(\delta\in(0,1)\). For a value \(\kappa\) that depends on \(T,d_{c}\) ad \(\delta\), for Adaptive-Staged-RandQL the following holds with probability at least \(1-\delta\),_

\[\mathfrak{R}^{T}=\widetilde{\mathcal{O}}\biggl{(}H^{3}+H^{3/2}\sum_{h=1}^{H} T^{\frac{d_{z,h}+1}{d_{z,h}+2}}\biggr{)},\]

_where \(d_{z,h}\) is the step-\(h\) zooming dimension and we ignore all multiplicative factors in the covering dimension \(d_{c}\)\(\log(C_{N})\), and Lipschitz constants._

We refer to Appendix F to a formal statement and a proof.

## 5 Experiments

In this section we present the experiments we conducted for tabular environments using rlberry library (Domingues et al., 2021). We also provide experiments in non-tabular environment in Appendix I.

EnvironmentWe use a grid-world environment with \(100\) states \((i,j)\in[10]\times[10]\) and \(4\) actions (left, right, up and down). The horizon is set to \(H=50\). When taking an action, the agent moves in the corresponding direction with probability \(1-\varepsilon\), and moves to a neighbor state at random with probability \(\varepsilon=0.2\). The agent starts at position \((1,1)\). The reward equals to \(1\) at the state \((10,10)\) and is zero elsewhere.

Variations of randomized Q-learningFor the tabular experiment we use the RandQL algorithm, described in Appendix B as it is the version of randomized Q-learning that is the closest to the baseline OptQL. Note that, we compare the different versions of randomized Q-learning in Appendix B.

BaselinesWe compare RandQL algorithm to the following baselines: (i) OptQL[Jin et al., 2018] (ii) UCBVI[Azar et al., 2017] (iii) Greedy-UCBVI, a version of UCBVI using real-time dynamic programming [Efroni et al., 2019] (iv) PSRL [Osband et al., 2013] and (v) RLSVI[Russo, 2019]. For the hyper-parameters used for these baselines refer to Appendix I.

ResultsFigure 1 shows the result of the experiments. Overall, we see that RandQL outperforms OptQL algorithm on tabular environment, but still degrades in comparison to model-based approaches, that is usual for model-free algorithms in tabular environments. Indeed, using a model and backward induction allows new information to be more quickly propagated. But as counterpart, RandQL has a better time-complexity and space-complexity than model-based algorithm, see Table 2 in Appendix I.

## 6 Conclusion

This paper introduced the RandQL algorithm, a new model-free algorithm that achieves exploration without bonuses. It utilizes a novel idea of learning rate randomization, resulting in provable sample efficiency with regret of order \(\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})\) in the tabular case. We also extend RandQL to the case of metric state-action space by using proper discretization techniques. The proposed algorithms inherit the good empirical performance of model-based Bayesian algorithm such that PSRL while keeping the small space and time complexity of model-free algorithm. Our result rises following interesting open questions for a further research.

Optimal rate for RandQLWe conjecture that RandQL could get optimal regret in the tabular setting if coupled with variance reductions techniques as used by Zhang et al. [2020]. However, obtaining such improvements is not straightforward due to the intricate statistical dependencies involved in the analysis of RandQL.

Beyond one-step learningWe observe a large gap in the experiments between Q-learning type algorithm that do one-step planning and e.g. UCBVI algorithm that does full planning or Greedy-UCBVI that does one-step planning with full back-up (expectation under transition of the model) for all actions. Therefore, it would interesting to study also algorithms that range between these two extremes [Efroni et al., 2018, 2019].

## Acknowledgments

The work of D. Tiapkin, A. Naumov, and D. Belomestny were supported by the grant for research centers in the field of AI provided by the Analytical Center for the Government of the Russian Federation (ACRF) in accordance with the agreement on the provision of subsidies (identifier of the agreement 000000D730321P5Q0002) and the agreement with HSE University No. 70-2021-00139. E. Moulines received support from the grant ANR-19-CHIA-002 SCAI and parts of his work has been done under the auspices of Lagrange Center for maths and computing. P. Menard acknowledges the Chaire SeqALO (ANR-20-CHIA-0020-01). This research was supported in part through computational resources of HPC facilities at HSE University.

Figure 1: Regret curves of RandQL and baselines in a grid-world environment for \(H=50\) and transition noise \(\varepsilon=0.2\). The average is over 4 seeds.

## References

* Agrawal and Goyal (2013) Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In Carlos M. Carvalho and Pradeep Ravikumar, editors, _Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics_, volume 31 of _Proceedings of Machine Learning Research_, pages 99-107, Scottsdale, Arizona, USA, 29 Apr-01 May 2013. PMLR. URL [https://proceedings.mlr.press/v31/agrawal13a.html](https://proceedings.mlr.press/v31/agrawal13a.html).
* Agrawal and Jia (2017) Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL [https://proceedings.neurips.cc/paper/2017/file/3621f1454cacf995530eea53652ddf8fb-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3621f1454cacf995530eea53652ddf8fb-Paper.pdf).
* Alfers and Dinges (1984) Duncan Alfers and Hermann Dinges. A normal approximation for beta and gamma tail probabilities. _Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebiete_, 65:399-420, 1984. URL [https://link.springer.com/content/pdf/10.1007/BF00533744.pdf](https://link.springer.com/content/pdf/10.1007/BF00533744.pdf).
* Azar et al. (2017) Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, 2017. URL [https://arxiv.org/pdf/1703.05449.pdf](https://arxiv.org/pdf/1703.05449.pdf).
* Azizzadenesheli et al. (2018) Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration through bayesian deep q-networks. In _2018 Information Theory and Applications Workshop, ITA 2018, San Diego, CA, USA, February 11-16, 2018_, pages 1-9. IEEE, 2018. doi: 10.1109/ITA.2018.8503252. URL [https://doi.org/10.1109/ITA.2018.8503252](https://doi.org/10.1109/ITA.2018.8503252).
* Bai et al. (2019) Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient q-learning with low switching cost, 2019. URL [https://arxiv.org/abs/1905.12849](https://arxiv.org/abs/1905.12849).
* Cao and Krishnamurthy (2020) Tongyi Cao and Akshay Krishnamurthy. Provably adaptive reinforcement learning in metric spaces. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 9736-9744. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/6ef1173b096aa200158bfbc8af3ae8e3-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/6ef1173b096aa200158bfbc8af3ae8e3-Paper.pdf).
* Dann et al. (2017) Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. In _Neural Information Processing Systems_, 2017. URL [https://arxiv.org/pdf/1703.07710.pdf](https://arxiv.org/pdf/1703.07710.pdf).
* Dann et al. (2021) Christoph Dann, Mehryar Mohri, Tong Zhang, and Julian Zimmert. A provably efficient model-free posterior sampling method for episodic reinforcement learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 12040-12051. Curran Associates, Inc., 2021. URL [https://proceedings.neurips.cc/paper_files/paper/2021/file/649d45bf179296e31731adfdd4df25588-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/649d45bf179296e31731adfdd4df25588-Paper.pdf).
* Deisenroth and Rasmussen (2011) Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In _Proceedings of the 28th International Conference on International Conference on Machine Learning_, ICML'11, page 465-472, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195.
* Diaconis and Freedman (1999) Persi Diaconis and David Freedman. Iterated random functions. _SIAM Review_, 41(1):45-76, 1999. doi: 10.1137/S0036144598338446. URL [https://doi.org/10.1137/S0036144598338446](https://doi.org/10.1137/S0036144598338446).
* A Reinforcement Learning Library for Research and Education, 10 2021a. URL [https://github.com/rlberry-py/rlberry](https://github.com/rlberry-py/rlberry).
* Domingues et al. (2021b) Omar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited. In Vitaly Feldman, Katrina Ligett, and Sivan Sabato, editors, _Proceedings of the 32nd International Conference on Algorithmic Learning Theory_, volume 132 of _Proceedings of Machine Learning Research_, pages 578-598. PMLR, 16-19 Mar 2021b. URL [https://proceedings.mlr.press/v132/domingues21a.html](https://proceedings.mlr.press/v132/domingues21a.html).
* Dann et al. (2021c)Omar Darwiche Domingues, Pierre Menard, Matteo Pirotta, Emilie Kaufmann, and Michal Valko. Kernel-based reinforcement learning: A finite-time analysis. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 2783-2792. PMLR, 18-24 Jul 2021c. URL [https://proceedings.mlr.press/v139/domingues21a.html](https://proceedings.mlr.press/v139/domingues21a.html).
* Douc et al. (2018) Randal Douc, Eric Moulines, Pierre Priouret, and Philippe Soulier. _Markov chains_. Springer, 2018.
* Efroni et al. (2018) Yonathan Efroni, Gal Dalal, Bruno Scherrer, and Shie Mannor. Multiple-step greedy policies in approximate and online reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018. URL [https://proceedings.neurips.cc/paper_files/paper/2018/file/3f998e713a6e02287c374fd26835d87e-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2018/file/3f998e713a6e02287c374fd26835d87e-Paper.pdf).
* Efroni et al. (2019) Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor. Tight regret bounds for model-based reinforcement learning with greedy policies. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/25caef3a545a1fff2ff4055484f0e758-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/25caef3a545a1fff2ff4055484f0e758-Paper.pdf).
* Ferguson (1973) Thomas S Ferguson. A bayesian analysis of some nonparametric problems. _The annals of statistics_, pages 209-230, 1973.
* Fiechter (1994) Claude-Nicolas Fiechter. Efficient reinforcement learning. In _Conference on Learning Theory_, 1994. URL [http://citeseerx.ist.psu.edu/viewdoc/download](http://citeseerx.ist.psu.edu/viewdoc/download);jsessionid=7F5F8FD1A7ED07356410DDD5B384FE7doi=10.1.1.49.8652&rep=rep1&type=pdf.
* Fortunato et al. (2018) Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alexander Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. Noisy networks for exploration. In _Proceedings of the International Conference on Representation Learning (ICLR 2018)_, Vancouver (Canada), 2018.
* Fruit et al. (2018) Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient bias-span-constrained exploration-exploitation in reinforcement learning. In _International Conference on Machine Learning_, pages 1578-1586. PMLR, 2018.
* Garivier et al. (2018) Aurelien Garivier, Hedi Hadiji, Pierre Menard, and Gilles Stoltz. Kl-ucb-switch: optimal regret bounds for stochastic bandits from both a distribution-dependent and a distribution-free viewpoints. _arXiv preprint arXiv:1805.05071_, 2018.
* Ghosal and Van der Vaart (2017) Subhashis Ghosal and Aad Van der Vaart. _Fundamentals of nonparametric Bayesian inference_, volume 44. Cambridge University Press, 2017.
* Ghosh and Marecek (2022) Ramen Ghosh and Jakub Marecek. Iterated function systems: A comprehensive survey, 2022.
* Guo et al. (2007) Senlin Guo, Feng Qi, and Hari Srivastava. Necessary and sufficient conditions for two classes of functions to be logarithmically completely monotonic. _Integral Transforms and Special Functions_, 18:819-826, 11 2007. doi: 10.1080/10652460701528933.
* Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* Heess et al. (2015) Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper_files/paper/2015/file/148510031349642de5ca0c544f31b2ef-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/148510031349642de5ca0c544f31b2ef-Paper.pdf).
* Hessel et al. (2021) Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent Sifre, Theo-phane Weber, David Silver, and Hado van Hasselt. Muesli: Combining improvements in policy optimization. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 4214-4226. PMLR, 2021. URL [http://proceedings.mlr.press/v139/hessel21a.html](http://proceedings.mlr.press/v139/hessel21a.html).
* Hessel et al. (2018)Junya Honda and Akimichi Takemura. An asymptotically optimal bandit algorithm for bounded support models. In Adam Tauman Kalai and Mehryar Mohri, editors, _COLT_, pages 67-79. Omnipress, 2010. ISBN 978-0-9822529-2-5. URL [http://dblp.uni-trier.de/db/conf/colt/colt2010.html#RondaT10](http://dblp.uni-trier.de/db/conf/colt/colt2010.html#RondaT10).
* Jaksch et al. (2010) Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_, 99:1563-1600, 2010. URL [http://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf](http://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf).
* Jin et al. (2018) Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is Q-learning provably efficient? In _Neural Information Processing Systems_, 2018. URL [https://arxiv.org/pdf/1807.03765.pdf](https://arxiv.org/pdf/1807.03765.pdf).
* Kingma and Welling (2014) Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun, editors, _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings_, 2014. URL [http://arxiv.org/abs/1312.6114](http://arxiv.org/abs/1312.6114).
* Li et al. (2022) Ziniu Li, Yingru Li, Yushun Zhang, Tong Zhang, and Zhi-Quan Luo. Hyperdqn: A randomized exploration method for deep reinforcement learning. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022._ OpenReview.net, 2022. URL [https://openreview.net/forum?id=X0nrKAXu7g-](https://openreview.net/forum?id=X0nrKAXu7g-).
* Lillicrap et al. (2016) Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua Bengio and Yann LeCun, editors, _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings_, 2016. URL [http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971).
* Liu et al. (2018) Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, and Qiang Liu. Action-dependent control variates for policy optimization via stein identity. In _ICLR 2018 Conference_, February 2018. URL [https://www.microsoft.com/en-us/research/publication/action-dependent-control-variates-policy-optimization-via-stein-identity/](https://www.microsoft.com/en-us/research/publication/action-dependent-control-variates-policy-optimization-via-stein-identity/).
* Menard et al. (2021) Pierre Menard, Omar Darwiche Domingues, Xuedong Shang, and Michal Valko. Ucb momentum q-learning: Correcting the bias without forgetting. In _International Conference on Machine Learning_, pages 7609-7618. PMLR, 2021.
* Mnih et al. (2013) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. In _NIPS Deep Learning Workshop_. 2013.
* Ni et al. (2019) Chengzhuo Ni, Lin F Yang, and Mengdi Wang. Learning to control in metric space with optimal regret. In _2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 726-733. IEEE, 2019.
* Osband and Van Roy (2015) Ian Osband and Benjamin Van Roy. Bootstrapped thompson sampling and deep exploration. _CoRR_, abs/1507.00300, 2015. URL [http://arxiv.org/abs/1507.00300](http://arxiv.org/abs/1507.00300).
* Osband and Van Roy (2017) Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning? In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 2701-2710. PMLR, 06-11 Aug 2017. URL [https://proceedings.mlr.press/v70/osband17a.html](https://proceedings.mlr.press/v70/osband17a.html).
* Osband et al. (2013) Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. _Advances in Neural Information Processing Systems_, 26, 2013.
* Osband et al. (2016) Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016. URL [https://proceedings.neurips.cc/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf](https://proceedings.neurips.cc/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf).
* Osband et al. (2017)Gabriel Peyre and Marco Cuturi. Computational optimal transport: With applications to data science. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019. ISSN 1935-8237. doi: 10.1561/2200000073. URL [http://dx.doi.org/10.1561/22000000073](http://dx.doi.org/10.1561/22000000073).
* 1706, 1994. doi: 10.1214/aop/1176988477. URL [https://doi.org/10.1214/aop/1176988477](https://doi.org/10.1214/aop/1176988477).
* Russo (2019) Daniel Russo. Worst-case regret bounds for exploration via randomized value functions. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/451ae86722d26a608c2e174b2b2773fi-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/451ae86722d26a608c2e174b2b2773fi-Paper.pdf).
* Sasso et al. (2023) Remo Sasso, Michelangelo Conserva, and Paulo E. Rauber. Posterior sampling for deep reinforcement learning. _CoRR_, abs/2305.00477, 2023. doi: 10.48550/arXiv.2305.00477. URL [https://doi.org/10.48550/arXiv.2305.00477](https://doi.org/10.48550/arXiv.2305.00477).
* Schrittwieser et al. (2020) Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.
* Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _CoRR_, abs/1707.06347, 2017. URL [http://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17](http://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17).
* Simchowitz and Jamieson (2019) Max Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular mdps. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/10a5ab2db37feedfdeaab192ead4ac0e-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/10a5ab2db37feedfdeaab192ead4ac0e-Paper.pdf).
* Sinclair et al. (2019) Sean R. Sinclair, Siddhartha Banerjee, and Christina Lee Yu. Adaptive discretization for episodic reinforcement learning in metric spaces. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 3(3):1-44, dec 2019. doi: 10.1145/3366703. URL [https://doi.org/10.1145%2P3366703](https://doi.org/10.1145%2P3366703).
* Sinclair et al. (2023) Sean R. Sinclair, Siddhartha Banerjee, and Christina Lee Yu. Adaptive discretization in online reinforcement learning. _Operations Research_, 71(5):1636-1652, 2023. doi: 10.1287/opre.2022.2396. URL [https://doi.org/10.1287/opre.2022.2396](https://doi.org/10.1287/opre.2022.2396).
* Skorski (2023) Maciej Skorski. Bernstein-type bounds for beta distribution. _Modern Stochastics: Theory and Applications_, 10(2):211-228, 2023. ISSN 2351-6046. doi: 10.15559/23-VMSTA223.
* Song and Sun (2019) Zhao Song and Wen Sun. Efficient model-free reinforcement learning in metric spaces, 2019.
* Strens (2000) Malcolm J. A. Strens. A bayesian framework for reinforcement learning. In _Proceedings of the Seventeenth International Conference on Machine Learning_, ICML '00, page 943-950, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1558607072.
* Sutton and Barto (1998) R. Sutton and A. Barto. _Reinforcement Learning: an Introduction_. MIT press, 1998.
* Tiapkin et al. (2020) Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Remi Munos, Alexey Naumov, Mark Rowland, Michal Valko, and Pierre Menard. Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 10737-10751. Curran Associates, Inc., 2022a. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/45e15bae91a6f213d45e203b8a29be48-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/45e15bae91a6f213d45e203b8a29be48-Paper-Conference.pdf).
* Sutskever et al. (2017)Daniil Tiapkin, Denis Belomestny, Eric Moulines, Alexey Naumov, Sergey Samsonov, Yunhao Tang, Michal Valko, and Pierre Menard. From Dirichlet to rubin: Optimistic exploration in RL without bonuses. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 21380-21431. PMLR, 17-23 Jul 2022b. URL [https://proceedings.mlr.press/v162/tiapkin22a.html](https://proceedings.mlr.press/v162/tiapkin22a.html).
* Van Hasselt et al. (2016) Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In _Proceedings of the AAAI conference on artificial intelligence_, volume 30, 2016.
* Vershynin (2018) Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Watkins and Dayan (1992) Chris J. Watkins and Peter Dayan. Q-learning. _Machine Learning_, 8(3-4):279-292, 1992. URL [https://link.springer.com/content/pdf/10.1007/BF00992698.pdf](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf).
* Wong (1998) Tzu-Tsung Wong. Generalized dirichlet distribution in bayesian analysis. _Applied Mathematics and Computation_, 97(2):165-181, 1998. ISSN 0096-3003. doi: [https://doi.org/10.1016/S0096-3003](https://doi.org/10.1016/S0096-3003)(97)10140-0. URL [https://www.sciencedirect.com/science/article/pii/S0096300397101400](https://www.sciencedirect.com/science/article/pii/S0096300397101400).
* Xiong et al. (2022) Zhihan Xiong, Ruoqi Shen, Qiwen Cui, Maryam Fazel, and Simon S Du. Near-optimal randomized exploration for tabular markov decision processes. _Advances in Neural Information Processing Systems_, 35:6358-6371, 2022.
* Ye and Zhou (2015) Fan Ye and Enlu Zhou. Information relaxation and dual formulation of controlled markov diffusions. _IEEE Transactions on Automatic Control_, 60(10):2676-2691, 2015.
* Zanette and Brunskill (2019) Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In _International Conference on Machine Learning_, 2019. URL [https://arxiv.org/pdf/1901.00210.pdf](https://arxiv.org/pdf/1901.00210.pdf).
* Zhang et al. (2020) Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via reference-advantage decomposition. _arXiv preprint arXiv:2004.10019_, 2020. ISSN 23318422. URL [https://arxiv.org/pdf/2004.10019.pdf](https://arxiv.org/pdf/2004.10019.pdf).

## Appendix

### Table of Contents

* A Notation
* B Description of RandQL
* B.1 RandQL algorithm
* B.2 Sampled-RandQL algorithm
* C Weight Distribution in RandQL
* D Proofs for Tabular algorithm
* D.1 Algorithm
* D.2 Concentration
* D.3 Optimism
* D.4 Regret Bound
* E Proofs for Metric algorithm
* E.1 Assumptions
* E.2 Algorithm
* E.3 Concentration
* E.4 Optimism
* E.5 Regret Bounds
* F Adaptive RandQL
* F.1 Additional Notation
* F.2 Algorithm
* F.3 Regret Bound
* G Deviation and Anti-Concentration Inequalities
* G.1 Deviation inequality for \(\mathcal{K}_{\text{inf}}\)
* G.2 Anti-concentration Inequality for Dirichlet Weighted Sums
* G.3 Rosenthal-type inequality
* H Technical Lemmas
* I Experimental details
	* I.1 Tabular experiments
	* I.2 Non-tabular experiments

## Appendix A Notation

Let \((\mathsf{X},\mathcal{X})\) be a measurable space and \(\mathcal{P}(\mathsf{X})\) be the set of all probability measures on this space. For \(p\in\mathcal{P}(\mathsf{X})\) we denote by \(\mathbb{E}_{p}\) the expectation w.r.t. \(p\). For random variable \(\xi:\mathsf{X}\to\mathbb{R}\) notation \(\xi\sim p\) means \(\mathrm{Law}(\xi)=p\). We also write \(\mathbb{E}_{\xi\sim p}\) instead of \(\mathbb{E}_{p}\). For independent (resp. i.i.d.) random variables \(\xi_{\ell}\stackrel{{\mathrm{ind}}}{{\sim}}p_{\ell}\) (resp. \(\xi_{\ell}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}p\)), \(\ell=1,\ldots,d\), we will write \(\mathbb{E}_{\xi_{\ell}\stackrel{{\mathrm{ind}}}{{\sim}}p_{\ell}}\) (resp. \(\mathbb{E}_{\xi_{\ell}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}p}\)), to denote expectation w.r.t. product measure on \((\mathsf{X}^{d},\mathcal{X}^{\otimes d})\). For any \(x\in\mathsf{X}\) we denote \(\delta_{x}\) a Dirac measure supported at point \(x\).

\begin{table}
\begin{tabular}{l|l} \hline \hline \multicolumn{1}{c}{**Notation**} & \multicolumn{1}{c}{**Meaning**} \\ \hline \(\mathcal{S}\) & state space of size \(S\) \\ \(\mathcal{A}\) & action space of size \(A\) \\ \(H\) & length of one episode \\ \(T\) & number of episodes \\ \(\frac{J}{r_{h}}(s,a)\) & reward \\ \(p_{h}(s^{\prime}|s,a)\) & probability transition \\ \(Q_{\ell}^{\pi}(s,a)\) & Q-function of a given policy \(\pi\) at step \(h\) \\ \(V_{\ell}^{\pi}(s)\) & V-function of a given policy \(\pi\) at step \(h\) \\ \(Q_{\ell}^{\pi}(s,a)\) & optimal Q-function at step \(h\) \\ \(V_{h_{h}^{*}}(s)\) & optimal V-function at step \(h\) \\ \(\mathfrak{R}^{T}\) & regret \\ \hline \(n_{0}\) and \(n_{0}(k)\) & number of pseudo-transitions \\ \(s_{0}\) & optimistic pseudo-state \\ \(r_{0}\) & pseudo-reward \\ \(\frac{\kappa}{s_{h}^{t}}\) & posterior inflation parameter \\ \hline \(s_{h}^{t}\) & state that was visited at \(h\) step during \(t\) episode \\ \(a_{h}\) & action that was picked at \(h\) step during \(t\) episode \\ \(B_{h}^{t}\) & a ball that contains a pair \((s_{h}^{t},a_{h}^{t})\) \\ \(n_{h}^{t}(s,a)\) & number of visits of state-action at the beginning of episode \(t\) \\ \(n_{h}^{t}(s,a)=\sum_{k=1}^{t-1}\mathds{1}\big{\{}(s_{h}^{k},a_{h}^{k})=(s,a) \big{\}}\) \\ \(\frac{n_{h}^{t}(B)}{\varepsilon_{k}}\) & number of \(k\)-th stage \(e_{k}=\lfloor(1+1/H)^{k}H\rfloor\) for \(k\geq 0\) and \(e_{-1}=0\) \\ \(k_{h}^{t}(s,a)\) & index of stage previous to time \(t\) at step \(h\) and state-action pair \((s,a)\): \\ \(k_{h}^{t}(s,a)=\max\{k:n_{h}^{t}(s,a)\geq\sum_{i=0}^{k}e_{i}\}\) \\ \(\widetilde{n}_{h}^{t}(s,a)\) & number of visits of state-action during the current stage: \\ \(\widetilde{n}_{h}^{t}(s,a)=n_{h}^{t}(s,a)-\sum_{i=0}^{k_{h}^{t}(s,a)}e_{i}\) \\ \(\widetilde{n}_{h}^{t}(B)\) & number of visits of a ball \(B\) during the current stage: \\ \hline \(\overline{V}_{h}^{t}(s)\) & upper approximation of the optimal V-value \\ \(\overline{Q}_{h}^{t}(s,a)\) & upper approximation of the optimal Q-value \\ \(\overline{Q}_{h}^{t}(B)\) & upper approximation of the optimal Q-value for all \((s,a)\in B\) \\ \(\widetilde{Q}_{h}^{t,j}(s,a)\) & temporary estimate of the optimal Q-value \\ \(\widetilde{Q}_{h}^{t,j}(B)\) & temporary estimate of the optimal Q-value for all \((s,a)\in B\) \\ \(w_{n,j}\) & random learning rates \\ \hline \(\rho_{\mathcal{S}},\rho_{A},\rho\) & metrics on \(\mathcal{S},\mathcal{A}\) and \(\mathcal{S}\times\mathcal{A}\) correspondingly \\ \(\mathcal{N}_{\varepsilon}\) & minimal \(\varepsilon\)-cover if \(\mathcal{S}\times\mathcal{A}\) of size \(N_{\varepsilon}\) \\ \(d_{c}\) & covering dimension of space \(\mathcal{S}\times\mathcal{A}\): \(\forall\varepsilon>0:N_{\varepsilon}\leq C_{N}\varepsilon^{-d_{c}}\) \\ \(d_{\max}\) & diameter of \(\mathcal{S}\times\mathcal{A}\) \\ \(F_{h}(s,a,\xi_{h})\) & reparametrization function \(s_{h+1}\sim p_{h}(s,a)\iff s_{h+1}=F_{h}(s,a,\xi_{h})\) \\ \(L_{r},L_{F}\) & Lipschitz constants of rewards and reparametrization function \\ \(L_{V}\) & Lipschitz constants of \(Q_{h}^{*}\) and \(V_{h}^{*}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Table of notation use throughout the paper for the tabular settingFor any \(p,q\in\mathcal{P}(\mathsf{X})\) the Kullback-Leibler divergence \(\mathrm{KL}(p,q)\) is given by

\[\mathrm{KL}(p,q)\triangleq\begin{cases}\mathbb{E}_{p}\Big{[}\log\frac{\mathrm{d}p }{\mathrm{d}q}\Big{]},&p\ll q,\\ +\infty,&\text{otherwise}.\end{cases}\]

For any \(p\in\mathcal{P}(\mathsf{X})\) and \(f:\mathsf{X}\to\mathbb{R}\), \(pf=\mathbb{E}_{p}[f]\). In particular, for any \(p\in\Delta_{d}\) and \(f:\{0,\ldots,d\}\to\mathbb{R}\), \(pf=\sum_{\ell=0}^{d}f(\ell)p(\ell)\). Define \(\mathrm{Var}_{p}(f)=\mathbb{E}_{s^{\prime}\sim p}\big{[}(f(s^{\prime})-pf)^{2} \big{]}=p[f^{2}]-(pf)^{2}\). For any \((s,a)\in\mathcal{S}\), transition kernel \(p(s,a)\in\mathcal{P}(\mathcal{S})\) and \(f\colon\mathcal{S}\to\mathbb{R}\) define \(pf(s,a)=\mathbb{E}_{p(s,a)}[f]\) and \(\mathrm{Var}_{p}[f](s,a)=\mathrm{Var}_{p(s,a)}[f]\).

Let \((\mathsf{X},\rho)\) be a metric space, then the 1-Wasserstein distance between \(p,q\in\mathcal{P}(\mathsf{X})\) is defined as \(\mathcal{W}_{1}(p,q)=\sup_{f\,\text{is 1-Lipschitz}}\mathbb{E}_{p}[f]- \mathbb{E}_{q}[f]\).

We write \(f(S,A,H,T)=\mathcal{O}(g(S,A,H,T,\delta))\) if there exist \(S_{0},A_{0},H_{0},T_{0},\delta_{0}\) and constant \(C_{f,g}\) such that for any \(S\geq S_{0},A\geq A_{0},H\geq H_{0},T\geq T_{0},\delta<\delta_{0},f(S,A,H,T, \delta)\leq C_{f,g}\cdot g(S,A,H,T,\delta)\). We write \(f(S,A,H,T,\delta)=\tilde{\mathcal{O}}(g(S,A,H,T,\delta))\) if \(C_{f,g}\) in the previous definition is poly-logarithmic in \(S,A,H,T,1/\delta\).

For \(\alpha,\beta>0\), we define \(\mathrm{Beta}(\alpha,\beta)\) as a beta distribution with parameters \(\alpha,\beta\). For set \(\mathsf{X}\) such that \(|\mathsf{X}|<\infty\) define \(\mathrm{Unif}(\mathsf{X})\) as a uniform distribution over this set. In particular, \(\mathrm{Unif}[N]\) is a uniform distribution over a set \([N]\).

For a measure \(p\in\mathcal{P}([0,b])\) supported on a segment \([0,b]\) (equipped with a Borel \(\sigma\)-algebra) and a number \(\mu\in[0,b]\) we define

\[\mathcal{K}_{\text{inf}}(p,\mu)\triangleq\inf\{\mathrm{KL}(p,q):q\in\mathcal{P }([0,b]),p\ll q,\mathbb{E}_{X\sim q}[X]\geq\mu\}\,.\]

As the Kullback-Leibler divergence this quantity admits a variational formula by Lemma 18 of Garivier et al. (2018) up to rescaling for any \(u\in(0,b)\)

\[\mathcal{K}_{\text{inf}}(p,\mu)=\max_{\lambda\in[0,1/(b-\mu)]}\mathbb{E}_{X \sim p}[\log(1-\lambda(X-\mu))]\,.\]

## Appendix B Description of RandQL

In this appendix we describe RandQL and Sampled-RandQL algorithms.

### RandQL algorithm

We recall that \(n^{t}_{h}(s,a)=\sum_{i=1}^{t-1}\mathds{1}\{(s^{i}_{h},a^{i}_{h})=(s,a)\}\) is the number of visits of state-action pair \((s,a)\) at step \(h\) before episode \(t\).

We start by initializing the ensemble of Q-values, the policy Q-values, and values to an optimistic value \(\widetilde{Q}^{t,j}_{h}(s,a)=\overline{Q}^{1}_{h}(s,a)=\overline{V}^{1}_{h}(s,a)=r_{h}(s,a)+r_{0}(H-h)\) for all \((j,h,s,a)\in[J]\times[H]\times\mathcal{S}\times\mathcal{A}\) and \(r_{0}>0\) some pseudo-rewards.

At episode \(t\) we update the ensemble of Q-values as follows, denoting by \(n=n^{t}_{h}(s,a)\) the count, \(w_{j,n}\sim\mathrm{Beta}(H,n)\) the independent learning rates,

\[\widetilde{Q}^{t+1,j}_{h}(s,a)=\begin{cases}(1-w_{j,n})\widetilde{Q}^{t,j}_{h} (s,a)+w_{j,n}\hat{Q}^{t,j}_{h}(s,a),&(s,a)=(s^{t}_{h},a^{t}_{h})\\ \widetilde{Q}^{t,j}_{h}(s,a)&\text{otherwise},\end{cases}\]

where we defined the target \(\hat{Q}^{t,j}_{h}(s,a)\) as a mixture between the usual target and some prior target with mixture coefficient \(\hat{w}_{n,j}\sim\mathrm{Beta}(n,n_{0})\) and \(n_{0}\) the number of prior samples,

\[\hat{Q}^{t,j}_{h}(s,a)=\hat{w}_{j,n}[r_{h}(s,a)+\overline{V}^{t}_{h+1}(s^{t}_{ h+1})]+(1-\hat{w}_{j,n})[r_{h}(s,a)+r_{0}(H-h-1)]\,.\]

It is important to note that in our approach, we need to re-inject prior targets to avoid forgetting their effects too quickly due to the aggressive learning rate. Indeed, the exponential decay of the prior effect can hurt exploration. We observe that the ensemble Q-value only averages uniformly over the last \(1/H\) fraction of the targets, as the expected value of the learning rate is \(\mathbb{E}[w_{j,n}]=H/(n+H)\). Since \(\mathbb{E}[1-\hat{w}_{j,n}]=n_{0}(n+n_{0})\) the weight put on the prior sample in expectation, when we unfold the definition of \(\widetilde{Q}_{h}^{t+1,j}\), is of order \(H/n\cdot n/H\cdot n_{0}/(n+n_{0})=n_{0}/(n+n_{0})\), which is consistent with the usual prior forgetting in Bayesian learning. In Staged-RandQL, we avoid forgetting the prior too quickly by resetting the temporary Q-value to a prior value at the beginning of each stage.

The policy Q-values are obtained by taking the maximum among the ensemble of Q-values

\[\overline{Q}_{h}^{t+1}(s,a)=\max_{j\in[J]}\widetilde{Q}_{h}^{t+1,j}(s,a)\,.\]

The policy is then greedy with respect to the policy Q-values \(\pi_{h}^{t+1}(s)\in\arg\max_{a\in\mathcal{A}}\overline{Q}_{h}^{t+1}(s,a)\) and the value is \(\overline{V}_{h}^{t+1}(s)=\max_{a\in\mathcal{A}}\overline{Q}_{h}^{t+1}(s,a)\). The complete RandQL procedure is detailed in Algorithm 2.

```
1:Input:\(J\) ensemble size, number of prior transitions \(n_{0}\), prior reward \(r_{0}\).
2:Initialize:\(\overline{V}_{h}(s)=\overline{Q}_{h}(s,a)=\widetilde{Q}_{h}^{j}(s,a)=r(s,a) +r_{0}(H-h)\), initialize counters \(n_{h}(s,a)=0\) for \(h,s,a\in[H]\times\mathcal{S}\times\mathcal{A}\).
3:for\(t\in[T]\)do
4:for\(h\in[H]\)do
5: Play \(a_{h}\in\arg\max_{a}\overline{Q}_{h}(s_{h},a)\).
6: Observe reward and next state \(s_{h+1}\sim p_{h}(s_{h},a_{h})\).
7: Sample \(\hat{w}_{j}\sim\mathrm{Beta}(n,n_{0})\) for \(n=n_{h}(s_{h},a_{h})\).
8: Build targets for all \(j\in[J]\) \[\hat{Q}_{h}^{j}=\hat{w}_{j}[r_{h}(s_{h},a_{h})+\overline{V}_{h+1}(s_{h+1})]+(1 -\hat{w}_{j})[r_{h}(s_{h},a_{h})+r_{0}(H-h)]\,.\]
9: Sample learning rates \(w_{j}\sim\mathrm{Beta}(H,n)\).
10: Update ensemble \(Q\)-functions for all \(j\in[J]\) \[\widetilde{Q}_{h}^{j}(s_{h},a_{h}):=(1-w_{j})\widetilde{Q}_{h}^{j}(s_{h},a_{h} )+w_{j}\hat{Q}_{h}^{j}\,.\]
11: Update policy \(Q\)-function \(\overline{Q}_{h}(s_{h},a_{h}):=\max_{j\in[J]}\widetilde{Q}_{h}^{j}(s_{h},a_{h})\).
12: Update value function \(\overline{V}_{h}(s_{h}):=\max_{a\in\mathcal{A}}\overline{Q}_{h}(s_{h},a)\,.\)
13:endfor
14:endfor
```

**Algorithm 2** RandQL

### Sampled-RandQL algorithm

To create an algorithm that is more similar to PSRL, it is possible to select a Q-value at random from the ensemble of Q-values, rather than using the maximum Q-value

\[\overline{Q}_{h}^{t}(s,a)=\widetilde{Q}_{h}^{t,j_{t}}(s,a)\qquad\text{with }j_{t} \sim\mathrm{Unif}[J].\]

In this case we also need to update each Q-value in the ensemble with its corresponding target, see Osband and Van Roy (2015),

\[\hat{Q}_{h}^{t,j}(s,a)=\hat{w}_{j,n}[r_{h}(s,a)+\widetilde{V}_{h+1}^{t,j}(s_{ h+1}^{t})]+(1-\hat{w}_{j,n})[r_{h}(s,a)+r_{0}(H-h-1)]\]

where \(\widetilde{V}_{h}^{t,j}(s)=\max_{a\in\mathcal{A}}\widetilde{Q}_{h}^{t,j}(s,a)\). We name this new procedure Sampled-RandQL and detail it in Algorithm 3.

## Appendix C Weight Distribution in RandQL

In this section we study the joint distribution of weights over all targets in RandQL algorithm, described in details in Appendix B. To do it, we describe a very useful distribution, defined by Wong [1998].

**Definition 5**.: We say that a random vector \((X_{1},\ldots,X_{n},X_{n+1})\) has a _generalized Dirichlet distribution_\(\mathrm{GDir}(\alpha_{1},\ldots,\alpha_{n};\beta_{1},\ldots,\beta_{n})\) if \(X_{n+1}=1-(X_{1}+\ldots+X_{n})\) and \((X_{1},\ldots,X_{n})\) it has the following density over the simplex \(\{x_{1},\ldots,x_{n}:x_{1}+\ldots+x_{n}\leq 1\}\),

\[p(x)=\prod_{i=1}^{n}\frac{1}{B(\alpha_{i},\beta_{i})}x_{i}^{\alpha_{i}-1}(1-x_ {1}-\ldots-x_{i})^{\gamma_{i}}\]

for \(x_{1}+\ldots+x_{n}\leq 1,x_{j}\geq 0\) for \(j=1,\ldots,n\), and \(\gamma_{j}=\beta_{j}-\alpha_{j+1}-\beta_{j+1}\) for \(j=1,\ldots,n-1\) and \(\gamma_{n}=\beta_{n}-1\). If we set \(x_{n+1}=1-(x_{1}+\ldots+x_{n})\) then we obtain a homogeneous formula

\[p(x)=\prod_{i=1}^{n}\frac{1}{B(\alpha_{i},\beta_{i})}x_{i}^{\alpha_{i}-1}\Bigg{(} \sum_{j=i+1}^{n+1}x_{j}\Bigg{)}^{\gamma_{i}}\]

Alternative characterization of generalized Dirichlet distribution could be given using independent beta-distributed random variables \(Z_{1},\ldots,Z_{n}\) with \(Z_{i}\sim\mathrm{Beta}(\alpha_{i},\beta_{i})\) as follows

\[X_{1} =Z_{1},\] \[X_{j} =Z_{j}(1-X_{1}-\ldots-X_{j-1})=Z_{j}\prod_{i=1}^{j-1}(1-Z_{i}) \quad\text{for }j=2,3,\ldots,n\] \[X_{n+1} =1-X_{1}-\ldots-X_{n}=\prod_{i=1}^{n}(1-Z_{i})\]

Therefore, for RandQL algorithm without prior re-injection we have the following formula

\[\widetilde{Q}_{h}^{t,j}(s,a)=\sum_{i=0}^{n_{h}^{t}(s,a)}W_{j,n}^{i}\bigg{(}r_ {h}(s_{h}^{\ell^{i}},a_{h}^{\ell^{i}})+\overline{V}_{h+1}^{\ell^{i}}(s_{h+1}^ {\ell^{i}})\bigg{)},\]for \(n=n^{t}_{h}(s,a)\) and weights are defined as follows

\[W^{0}_{j,n}=\prod_{q=0}^{n-1}(1-w_{j,q}),\quad W^{i}_{j,n}=w_{j,i-1}\cdot\prod_{q =i}^{n-1}(1-w_{j,q}),\;i\geq 1.\]

And, moreover, we have that this vector of weights has the _generalized Dirichlet distribution_

\[(W^{n}_{n,j},W^{n-1}_{n,j},\ldots,W^{1}_{n,j},W^{0}_{n,j})\sim\mathrm{GDir}(H,H,\ldots,H;n+n_{0},\ldots,n_{0}+1,n_{0}).\]

That is, weights generated by the RandQL procedure is an inverted generalized Dirichlet random vector, that induces additional similarities with a usual posterior sampling approaches. Notably, that for \(H=1\) we recover exactly usual Dirichlet distribution, as in the setting of Staged-RandQL.

In the setting of the analysis, the main feature of this distribution is asymmetry in attitude to the order of components. In particular, the expectation of the prior weight \(W^{0}_{n,j}\) is \(\prod_{i=1}^{n}\Bigl{(}1-\frac{H}{i+H}\Bigr{)}\sim n^{-H}\) that leads to too rapid forgetting of the prior information.

Proofs for Tabular algorithm

### Algorithm

In this section we describe in detail the tabular algorithms and the ways we will analyze them. We also provide some notations that will be used in the sequel.

Let \(n_{h}^{t}(s,a)\) be the number of visits of \((s,a,h)\) (i.e., of the state-action pair \((s,a)\) at step \(h\)) at the beginning of episode \(t\): \(n_{h}^{t}(s,a)=\sum_{i=1}^{t-1}\mathds{1}\{(s_{h}^{i},a_{h}^{i})=(s,a)\}\). In particular, \(n_{h}^{T+1}(s,a)\) is the number of visits of \((s,a,h)\) after all episodes.

Let \(e_{k}=\lfloor(1+1/H)^{k}\cdot H\rfloor\) be the length of each stage for any \(k\geq 0\) and, by convention, \(e_{-1}=0\). We will say that at the beginning of episode \(t\) a triple \((s,a,h)\) is in \(k\)-th stage if \(n_{h}^{t}(s,a)\in[\sum_{i=0}^{k-1}e_{i},\sum_{i=0}^{k}e_{i})\).

Let \(\widetilde{n}_{h}^{t}(s,a)\) be the number of visits of state-action pair during the current stage at the beginning of episode \(t\). Formally, it holds \(\widetilde{n}_{h}^{t}(s,a)=n_{h}^{t}(s,a)-\sum_{i=0}^{k-1}e_{i}\), where \(k\) is the index of current stage.

Let \(\kappa>0\) be the posterior inflation coefficient, \(n_{0}\) be the number of prior transitions, and \(J\) be the number of temporary \(Q\)-functions. Let \(\widetilde{Q}_{h}^{t,j}\) be the \(j\)-th _temporary_ Q-function and \(\overline{Q}_{h}^{t}\) be the _policy_ Q-function at the beginning of episode \(t\). We initialize them as follows

\[\overline{Q}_{h}^{1}(s,a)=r_{h}(s,a)+r_{0}(H-h-1),\quad\widetilde{Q}_{h}^{1,j }(s,a)=r_{h}(s,a)+r_{0}(H-h-1),\]

We can treat this initialization as a setting prior over \(n_{0}\) pseudo-transitions to artificial state \(s_{0}\) with \(r_{0}>1\) reward for each interaction.

For each transition we perform the following update of temporary Q-functions

\[\widetilde{Q}_{h}^{t+1/2,j}(s,a)=\begin{cases}(1-w_{j,\widetilde{n}}^{k}) \cdot\widetilde{Q}_{h}^{t,j}(s,a)+w_{j,\widetilde{n}}^{k}[r_{h}(s,a)+\overline {V}_{h+1}^{t}(s_{h+1}^{t})],&(s,a)=(s_{h}^{t},a_{h}^{t})\\ \widetilde{Q}_{h}^{t,j}(s,a)&\text{otherwise},\end{cases} \tag{2}\]

where \(\widetilde{n}=\widetilde{n}_{h}^{t}(s,a)\) is the number of visits of \((s,a,h)\) during the current stage at the beginning of episode \(t\), \(k\) is the index of the current stage, and \(w_{j,\widetilde{n}}^{k}\) is a sequence of independent beta-distribution random variables \(w_{j,\widetilde{n}}^{k}\sim\operatorname{Beta}(1/\kappa,(\widetilde{n}+n_{0})/\kappa)\). Here we slightly abuse the notation by dropping the dependence of weights \(w_{j,\widetilde{n}}^{k}\) on the triple \((h,s,a)\) in order to simplify the exposition. In the case that the explicit dependence is required, we will call these weights as \(w_{j,\widetilde{n}}^{k,h}(s,a)\).

Next we define the stage update as follows

\[\overline{Q}_{h}^{t+1}(s,a) =\begin{cases}\max_{j\in[J]}\widetilde{Q}_{h}^{t+1/2,j}(s,a)& \widetilde{n}_{h}^{t}(s,a)=\lfloor(1+1/H)^{k}H\rfloor\\ \overline{Q}_{h}^{t}(s,a)&\text{otherwise}\end{cases}\] \[\widetilde{Q}_{h}^{t+1,j}(s,a) =\begin{cases}r_{h}(s,a)+r_{0}(H-h+1)&\widetilde{n}_{h}^{t}(s,a) =\lfloor(1+1/H)^{k}H\rfloor\\ \widetilde{Q}_{h}^{t+1/2,j}(s,a)&\text{otherwise}\end{cases}\] \[\overline{V}_{h}^{t+1}(s) =\max_{a\in\mathcal{A}}\overline{Q}_{h}^{t+1}(s,a)\] \[\pi_{h}^{t+1}(s) \in\operatorname*{arg\,max}_{a\in\mathcal{A}}\overline{Q}_{h}^{ t+1}(s,a),\]

where \(k\) is the current stage. In other words, we update \(\overline{Q}^{t+1}\) with temporary values of \(\widetilde{Q}^{t+1/2,j}\), and then, if the change of stage is triggered, reinitialize \(\widetilde{Q}_{h}^{t+1,j}(s,a)\) for all \(j\). For episode \(t\) we will call \(k_{h}^{t}(s,a)\) the index of stage where \(\overline{Q}_{h}^{t}(s,a)\) was updated (and \(k_{h}^{t}(s,a)=-1\) if there was no update). For all \(t\) we define \(\tau_{h}^{t}(s,a)\leq t\) as an episode when the stage update happens. In other words, for any \(t\) the following holds

\[\overline{Q}_{h}^{t+1}(s,a)=\max_{j\in[J]}\widetilde{Q}_{h}^{\tau_{h}^{t}(s,a) +1/2,j}(s,a),\]

where \(\tau_{h}^{t}(s,a)=0\) and \(e_{k}=0\) if there was no updates. To simplify the notation we will omit dependence on \((s,a,h)\) where it is deducible from the context.

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_EMPTY:24]

where we drop dependence on state-action pairs everywhere where it is deducible from the context.

Consider a sequence \(\ell^{1}<\ldots<\ell^{n}\) be an excursion of the state-action pair \((s,a)\) at the step \(h\). Each \(\ell^{i}\) is a stopping time w.r.t \(\mathcal{F}_{t,h}\), so we can consider a stopped filtration (with a shift by 1 in indices) \(\widetilde{\mathcal{F}}_{i-1}=\mathcal{F}_{\ell^{i},h}\). In other words, this filtration at time-stamp \(i-1\) contains all the information that is available just before generation of random weights for \(i\)-th update of temporary Q-functions inside the last stage. We notice that under this definition we have

\[\mathbb{E}[\overline{V}_{h+1}^{\ell_{i}}(s_{h+1}^{\ell_{i}})| \widetilde{\mathcal{F}}_{i-1}] =\overline{V}_{h+1}^{\ell_{i}}(s_{h+1}^{\ell_{i}}),\] \[\mathbb{E}[W_{j,n,k}^{i}|\widetilde{\mathcal{F}}_{i-1}] =\mathbb{E}\Bigg{[}w_{j,i-1}^{k}\prod_{\ell=i}^{n-1}(1-w_{j,\ell} ^{k})|\mathcal{F}_{i-1}\Bigg{]}=\mathbb{E}[W_{j,n,k}^{i}],\]

Next, we notice that the joint vector of weights follows the Dirichlet distribution, applying aggregation property and extending the filtration backward by adding fake transitions we can extend sum to \(n+n_{0}\) summands defining \(s_{h+1}^{\ell^{-i}}=s_{0}\)

\[\sum_{i=0}^{n}\bigl{(}W_{j,n,k}^{i}-\mathbb{E}[W_{j,n,k}^{i}]\bigr{)}\overline {V}_{h+1}^{\ell^{i}}(s_{h+1}^{\ell^{i}})=\sum_{q=-n_{0}+1}^{n}\Bigl{(} \widetilde{W}_{q}-\mathbb{E}[\widetilde{W}_{q}]\Bigr{)}\overline{V}_{h+1}^{ \ell^{q}}(s_{h+1}^{\ell^{q}}).\]

Finally, we notice that marginals of Dirichlet random vector follow Beta distribution, therefore by Proposition 7 and union bound we conclude \(\mathbb{P}[\mathcal{E}^{B}(\delta)]\geq 1-\delta/8\).

To show that \(\mathbb{P}(\mathcal{E}^{\mathrm{conc}}(\delta))>1-\delta/8\), it is enough to apply Hoeffding inequality for a fixed number of samples \(e_{k}\) used in empirical mean, and then use union bound of all possible values of \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\) and \(e_{k}\in[T]\).

Next, define the following sequence

\[Z_{t,h}\triangleq(1+1/H)^{H-h}\Bigl{(}[V_{h+1}^{\star}-V_{h+1}^{\pi^{t}}](s_{h +1}^{t})-p_{h}[V_{h+1}^{\star}-V_{h+1}^{\pi^{t}}](s_{h}^{t},a_{h}^{t})\Bigr{)},\quad t\in[T],h\in[H],\]

It is easy to see that these sequences form a martingale-difference w.r.t filtration \(\mathcal{F}_{t,h}=\sigma\bigl{\{}\{(s_{h^{\prime}}^{\ell},a_{h^{\prime}}^{\ell },\pi^{\ell}),\ell<t,h^{\prime}\in[H]\}\cup\{(s_{h^{\prime}}^{t},a_{h^{\prime} }^{t},\pi^{t}),h^{\prime}\leq h\}\bigr{\}}\). Moreover, \(|Z_{t,h}|\leq 2e_{0}H\) for all \(t\in[T]\) and \(h\in[H]\). Hence, the Azuma-Hoeffding inequality implies

\[\mathbb{P}\Bigl{(}\Bigl{|}\sum_{t=1}^{T}\sum_{h=1}^{H}Z_{t,h}\Bigr{|}>2e_{0}H \sqrt{2tH\cdot\beta(\delta)}\Bigr{)}\leq 2\exp(-\beta(\delta))=\delta/8,\]

therefore \(\mathbb{P}[\mathcal{E}(\delta)]\geq 1-\delta/8\). 

### Optimism

In this section we prove that our estimate of \(Q\)-function \(\overline{Q}_{h}^{\,t}(s,a)\) is optimistic, that is the event

\[\mathcal{E}_{\mathrm{opt}}\triangleq\Bigl{\{}\forall t\in[T],h\in[H],(s,a)\in \mathcal{S}\times\mathcal{A}:\overline{Q}_{h}^{t}(s,a)\geq Q_{h}^{\star}(s,a) \Bigr{\}}. \tag{4}\]

holds with high probability on the event \(\mathcal{E}^{\star}(\delta)\).

Define constants

\[c_{0}\triangleq\frac{8}{\pi}\Biggl{(}\frac{4}{\sqrt{\log(17/16)}}+8+\frac{49 \cdot 4\sqrt{6}}{9}\Biggr{)}^{2}+1. \tag{5}\]

and

\[c_{J}\triangleq\frac{1}{\log\Bigl{(}\frac{2}{1+\Phi(1)}\Bigr{)}}, \tag{6}\]

where \(\Phi(\cdot)\) is a CDF of a normal distribution.

[MISSING_PAGE_EMPTY:26]

**Proposition 3** (Optimism).: Assume that \(J=\lceil c_{J}\cdot\log(2SAHT/\delta)\rceil\), \(\kappa=2\beta^{*}(\delta,T)\), \(r_{0}=2\), and \(n_{0}=\lceil(c_{0}+1+\log_{17/16}(T))\cdot\kappa\rceil\), where \(c_{0}\) is defined in (5) and \(c_{J}\) is defined in (6). Then \(\mathbb{P}(\mathcal{E}^{\mathrm{opt}}\mid\mathcal{E}^{\star}(\delta))\geq 1- \delta/2\).

### Regret Bound

Let us define the main event \(\mathcal{G}^{\prime}(\delta)=\mathcal{G}(\delta)\cap\mathcal{E}^{\mathrm{opt}}\). On this event we have the following corollary that connects RandQL with OptQL with Hoeffding bonuses.

Define the following quantity

\[\beta^{\max}(\delta)=\max\bigl{\{}\kappa,n_{0}/\kappa,\beta^{B}(\delta),\beta^{ \mathrm{conc}}(\delta),\beta(\delta),\log(T+n_{0})\bigr{\}}=\mathcal{O}(\log( SATH/\delta)).\]

**Corollary 1**.: _Assume conditions of Proposition 3 hold. Let \(t\in[T],h\in[H],(s,a)\in\mathcal{S}\times\mathcal{A}\). Define \(k=k_{h}^{t}(s,a)\) and let \(\ell^{1}<\ldots<\ell^{\epsilon_{k}}\) be a excursions of \((s,a,h)\) until the previous stage. Then on the event \(\mathcal{G}^{\prime}(\delta)\) the following bound holds for \(k\geq 0\)_

\[0\leq\overline{Q}_{h}^{t}(s,a)-Q_{h}^{\star}(s,a)\leq\frac{1}{n}\sum_{i=1}^{n }[\overline{V}_{h+1}^{\ell^{i}}(s_{h+1}^{\ell^{i}})-V_{h+1}^{\star}(s_{h+1}^{ \ell^{i}})]+\mathcal{B}_{h}^{t}(k),\]

_where_

\[\mathcal{B}_{h}^{t}(k)=61\mathrm{e}^{2}\frac{r_{0}H(\beta^{\max}(\delta))}{ \sqrt{e_{k}}}+1201\mathrm{e}\frac{r_{0}H(\beta^{\max}(\delta))^{4}}{e_{k}}.\]

Proof.: The lower bound follows from the definition of the event \(\mathcal{E}^{\mathrm{opt}}\). For the upper bound we first apply the decomposition for \(\overline{Q}_{h}^{t}(s,a)\) and the definition of event \(\mathcal{E}^{B}(\delta)\) from Lemma 4

\[\overline{Q}_{h}^{t}(s,a) =r_{h}(s,a)+\max_{j\in[J]}\Biggl{\{}\sum_{i=0}^{e_{k}}W_{j,e_{k} }^{i}\overline{V}_{h+1}^{\ell^{i}}(s_{h+1}^{\ell^{i}})\Biggr{\}}\] \[\leq r_{h}(s,a)+\frac{1}{e_{k}+n_{0}}\sum_{i=1}^{e_{k}}\overline{ V}_{h+1}^{\ell^{i}}(s_{h+1}^{\ell^{i}})+\frac{n_{0}\kappa\cdot r_{0}H}{e_{k}+n_{0 }}+60\mathrm{e}^{2}\sqrt{\frac{r_{0}^{2}H^{2}\kappa\beta^{B}(\delta)}{e_{k}+n_ {0}}}\] \[+1200\mathrm{e}\frac{r_{0}H\kappa\log(e_{k}+n_{0})(\beta^{B}( \delta))^{2}}{e_{k}+n_{0}}.\]

Then, by Bellman equations,

\[\overline{Q}_{h}^{t}(s,a)-Q_{h}^{\star}(s,a) \leq\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}\biggl{[}\overline{V}_{h+1} ^{\ell^{i}}-V_{h+1}^{\star}\biggr{]}\bigl{(}s_{h+1}^{\ell^{i}})+\frac{1}{e_{k} }\sum_{i=1}^{e_{k}}\Bigl{[}V_{h+1}^{\star}(s_{h+1}^{\ell^{i}})-p_{h}V_{h+1}^{ \star}(s,a)\Bigr{]}\] \[+(1200\mathrm{e}+1)\frac{r_{0}H(\beta^{\max}(\delta))^{4}}{e_{k} +n_{0}}+60\mathrm{e}^{2}\cdot\frac{r_{0}H\beta^{\max}(\delta)}{\sqrt{e_{k}+n_ {0}}}\]

By the definition of event \(\mathcal{E}^{\mathrm{conc}}(\delta)\) we conclude the statement. 

Let us define \(\delta_{h}^{t}=\overline{V}_{h}^{t}(s_{h}^{t})-V_{h}^{\pi^{t}}(s_{h}^{t})\) and \(\zeta_{h}^{t}=\overline{V}_{h}^{t}(s_{h}^{t})-V_{h}^{\star}(s_{h}^{t})\).

**Lemma 5**.: _Assume conditions of Proposition 3 hold. Then on event \(\mathcal{G}^{\prime}(\delta)=\mathcal{G}(\delta)\cap\mathcal{E}^{\mathrm{opt}}\), where \(\mathcal{G}(\delta)\) is defined in Lemma 4, the following upper bound on regret holds_

\[\mathfrak{R}^{T}\leq\mathrm{e}H\sum_{t=1}^{T}\sum_{h=1}^{H}\mathds{1}\{k_{h}^{ t}(s_{h}^{t},a_{h}^{t})=-1\}+\sum_{t=1}^{T}\sum_{h=1}^{H}(1+1/H)^{H-h}\xi_{h}^{t}+ \mathrm{e}\sum_{t=1}^{T}\sum_{h=1}^{H}\mathcal{B}_{h}^{t},\]

_where \(\xi_{h}^{t}=p_{h}[V_{h+1}^{\star}-V_{h+1}^{\star}](s_{h}^{t},a_{h}^{t})-[V_{h+1 }^{\star}-V_{h+1}^{\star^{t}}](s_{h+1}^{t})\) and \(\mathcal{B}_{h}^{t}=\mathcal{B}_{h}^{t}(s_{h}^{t},a_{h}^{t})\cdot\mathds{1}\{k _{h}^{t}(s_{h}^{t},a_{h}^{t})\geq 0\}\) for \(\mathcal{B}_{h}^{t}\) defined in Corollary 1._

Proof.: We notice that on the event \(\mathcal{E}^{\mathrm{opt}}\) the following upper bound holds

\[\mathfrak{R}^{T}\leq\sum_{t=1}^{T}\delta_{1}^{t}. \tag{8}\]Next we analyze \(\delta_{h}^{t}\). By the choice of \(a_{h}^{t}=\arg\max_{a\in\mathcal{A}}\overline{Q}_{h}^{t}(s_{h}^{t},a)\), Corollary 1, and Bellman equations, we have

\[\delta_{h}^{t} =\overline{V}_{h}^{t}(s_{h}^{t})-V_{h}^{\pi^{t}}(s_{h}^{t})= \overline{Q}_{h}^{t}(s_{h}^{t},a_{h}^{t})-Q_{h}^{\pi^{t}}(s_{h}^{t},a_{h}^{t})\] \[=\overline{Q}_{h}^{t}(s_{h}^{t},a_{h}^{t})-Q_{h}^{*}(s_{h}^{t},a_{ h}^{t})+Q_{h}^{*}(s_{h}^{t},a_{h}^{t})-Q_{h}^{\pi^{t}}(s_{h}^{t},a_{h}^{t})\] \[\leq H\mathds{1}\{N_{h}^{t}=0\}+\mathds{1}\{N_{h}^{t}>0\}\Bigg{(} \frac{1}{N_{h}^{t}}\sum_{i=1}^{N_{h}^{t}}\zeta_{h+1}^{t^{i}_{t,h}}+\mathcal{B} _{h}^{t}(s_{h}^{t},a_{h}^{t})+p_{h}[V_{h+1}^{*}-V_{h+1}^{\pi^{t}}](s_{h}^{t},a_ {h}^{t})\Bigg{)}.\]

where \(k_{h}^{t}=k_{h}^{t}(s_{h}^{t},a_{h}^{t})\), \(N_{h}^{t}=e_{k_{h}^{t}}\), \(\ell_{i,h}^{t}\) is episode of the \(i\)-th visitation of the state-action pair \((s_{h}^{t},a_{h}^{t})\) during the stage \(k_{h}^{t}\), and additionally by the convention \(0/0=0\). Let \(\xi_{h}^{t}=p_{h}[V_{h+1}^{\star}-V_{h+1}^{\pi^{t}}](s_{h}^{t},a_{h}^{t})-[V_{ h+1}^{\star}-V_{h+1}^{\pi^{t}}](s_{h+1}^{t})\) be a martingale-difference sequence, and \(\mathcal{B}_{h}^{t}=\mathcal{B}_{h}^{t}(s_{h}^{t},a_{h}^{t})\mathds{1}\{N_{h} ^{t}>0\}\) then

\[\delta_{h}^{t}\leq H\mathds{1}\{N_{h}^{t}=0\}+\frac{\mathds{1}\{N_{h}^{t}>0\}} {N_{h}^{t}}\sum_{i=1}^{N_{h}^{t}}\zeta_{h+1}^{t^{i}_{,h}}-\zeta_{h+1}^{t}+ \delta_{h+1}^{t}+\xi_{h}^{t}+\mathcal{B}_{h}^{t}.\]

and, as a result

\[\sum_{t=1}^{T}\delta_{h}^{t} \leq H\sum_{t=1}^{T}\mathds{1}\{N_{h}^{t}=0\}+\sum_{t=1}^{T}\frac {\mathds{1}\{N_{h}^{t}>0\}}{N_{h}^{t}}\sum_{i=1}^{N_{h}^{t}}\zeta_{h+1}^{t_{,h}}\] \[-\sum_{t=1}^{T}\zeta_{h+1}^{t}+\sum_{t=1}^{T}\delta_{h+1}^{t}+ \sum_{t=1}^{T}\xi_{h}^{t}+\sum_{t=1}^{T}\mathcal{B}_{h}^{t}.\]

Next we have to analyze the second term, following the approach by Zhang et al. (2020),

\[\sum_{t=1}^{T}\frac{\mathds{1}\{N_{h}^{t}>0\}}{N_{h}^{t}}\sum_{i= 1}^{N_{h}^{t}}\zeta_{h+1}^{t_{,h}} =\sum_{q=1}^{T}\sum_{t=1}^{T}\frac{\mathds{1}\{N_{h}^{t}>0\}}{N_ {h}^{t}}\sum_{i=1}^{N_{h}^{t}}\zeta_{h+1}^{t_{,h}}\mathds{1}\{\ell_{t,h}^{i}=q\}\] \[=\sum_{q=1}^{T}\zeta_{h+1}^{q}\cdot\sum_{t=1}^{T}\frac{\mathds{1} \{k_{h}^{t}\geq 0\}}{N_{h}^{t}}\sum_{i=1}^{N_{h}^{t}}\mathds{1}\{\ell_{t,h}^{i}=q\}.\]

Notice that \(\sum_{i=1}^{N_{h}^{t}}\mathds{1}\{\ell_{t,h}^{i}=q\}\leq 1\) since all visitations are increasing in \(i\), and, moreover, it turns to equality if and only if \((s_{h}^{q},a_{h}^{q})=(s_{h}^{t},a_{h}^{t})\) and this visitation happens in stage \(k_{h}^{t}\), where \(k_{h}^{t}\) is equal to the stage of episode \(q\) with respect to \((s_{h}^{q},a_{h}^{q},h)\). Since the sum is over all the next episodes with respect to stage of \(q\), we have that the number of non-zero elements in the sum over \(t\) is bounded by \((1+1/H)N_{h}^{T}\). Thus

\[\sum_{q=1}^{T}\zeta_{h+1}^{q}\cdot\sum_{t=1}^{T}\frac{\mathds{1}\{k_{h}^{t}\geq 0 \}}{N_{h}^{t}}\sum_{i=1}^{N_{h}^{t}}\mathds{1}\{\ell_{t,h}^{i}=q\}\leq\left(1+ \frac{1}{H}\right)\sum_{q=1}^{T}\zeta_{h+1}^{q}.\]

After a simple algebraic manipulations and using the fact that \(\zeta_{h}^{t}\leq\delta_{h}^{t}\),

\[\sum_{t=1}^{T}\delta_{h}^{t} \leq H\sum_{t=1}^{T}\mathds{1}\{N_{h}^{t}=0\}+\sum_{t=1}^{T}(1+1/ H)\zeta_{h+1}^{t}-\sum_{t=1}^{T}\zeta_{h+1}^{t}+\sum_{t=1}^{T}\delta_{h+1}^{t}+ \sum_{t=1}^{T}\mathcal{B}_{h}^{t}\] \[\leq H\sum_{t=1}^{T}\mathds{1}\{N_{h}^{t}=0\}+\left(1+\frac{1}{H} \right)\sum_{t=1}^{T}\delta_{h+1}^{t}+\sum_{t=1}^{T}\xi_{h}^{t}+\sum_{t=1}^{T} \mathcal{B}_{h}^{t}.\]

By rolling out the upper bound on regret (8) and using inequality \((1+1/H)^{H-h}\leq\mathrm{e}\) we have

\[\mathfrak{R}^{T}\leq\mathrm{e}H\sum_{t=1}^{T}\sum_{h=1}^{H}\mathds{1}\{N_{h}^{t}= 0\}+\sum_{t=1}^{T}\sum_{h=1}^{H}(1+1/H)^{H-h}\xi_{h}^{t}+\mathrm{e}\sum_{t=1}^{ T}\sum_{h=1}^{H}\mathcal{B}_{h}^{t}.\]Proof of Theorem 1.: First, we notice that the event \(\mathcal{G}^{\prime}(\delta)\) defined in Lemma 5, holds with probability at least \(1-\delta\) by Lemma 4 and Proposition 3. Thus, we may assume that \(\mathcal{G}^{\prime}(\delta)\) holds.

We start from the decomposition given by Lemma 5

\[\mathfrak{R}^{T}\leq\mathrm{e}H\sum_{t=1}^{T}\sum_{h=1}^{H}\mathds{1}\{k_{h}^{t }(s_{h}^{t},a_{h}^{t})=-1\}+\sum_{t=1}^{T}\sum_{h=1}^{H}(1+1/H)^{H-h}\xi_{h}^{ t}+\mathrm{e}\sum_{t=1}^{T}\sum_{h=1}^{H}\mathcal{B}_{h}^{t}.\]

The first term is upper bounded by \(\mathrm{e}SAH^{3}\), since there is no more than \(H\) visits of each state-action-step triple before the update for the first stage. The second term is bounded by \(\widetilde{\mathcal{O}}(\sqrt{H^{3}T})\) by a definition of the event \(\mathcal{E}(\delta)\) in Lemma 4. To upper bound the last term we have to analyze the following sum

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\frac{\mathds{1}\{e_{k_{h}^{t}(s_{h}^{t},a_{h}^{t} )}>0\}}{\sqrt{e_{k_{h}^{t}(s_{h}^{t},a_{h}^{t})}}}\leq\sum_{(s,a,h)\in\mathcal{ S}\times\mathcal{A}\times[H]}\sum_{k=0}^{k_{h}^{T+1}(s,a)}\frac{e_{k+1}}{\sqrt{e_{k}}},\]

where

\[e_{k}=\left\lfloor\left(1+\frac{1}{H}\right)^{k}H\right\rfloor\Rightarrow\frac {e_{k+1}}{\sqrt{e_{k}}}\leq 2\sqrt{e_{k}},\]

therefore by Cauchy inequality

\[\sum_{k=0}^{k_{h}^{T+1}(s,a)}\frac{e_{k+1}}{\sqrt{e_{k}}}\leq 2\sum_{k=0}^{k_{h}^{ T+1}(s,a)}\sqrt{e_{k}}\leq 2\sqrt{k_{h}^{T+1}(s,a)}\sqrt{\sum_{k=0}^{k_{h}^{T+1}(s,a)}e_{k}} \leq 2\sqrt{\frac{\log(T)}{\log(1+1/H)}}\sqrt{n_{h}^{T+1}(s,a)},\]

where we used the definition of the previous stage \(k_{h}^{T+1}(s,a)\)

\[N_{h}^{T+1}(s,a)\geq\sum_{k=0}^{k_{h}^{T+1}(s,a)}e^{k},\]

thus by Cauchy inequality and inequality \(\log(1+1/H)\geq 1/(4H)\) for \(H\geq 1\)

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\frac{\mathds{1}\{e_{k_{h}^{t}(s_{h}^ {t},a_{h}^{t})>0\}}}{\sqrt{e_{k_{h}^{t}(s_{h}^{t},a_{h}^{t})}}} \leq 2\sqrt{H\log(T)}\sum_{(s,a,h)\in\mathcal{S}\times\mathcal{A} \times[H]}\sqrt{N_{h}^{T+1}(s,a)+1}\] \[\leq 4\sqrt{SAH^{2}\log(T)}\sqrt{(s,a,h)}(N_{h}^{T+1}(s,a)+1)\] \[\leq 4\sqrt{SAH^{3}T\log(T)}+4SAH^{2}\log(T).\]

Using this upper bound, we have

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\mathcal{B}_{h}^{t}=\widetilde{\mathcal{O}}\Bigg{(} H\sum_{t=1}^{T}\sum_{h=1}^{H}\frac{\mathds{1}\{e_{k_{h}^{t}(s_{h}^{t},a_{h}^{t} )}>0\}}{\sqrt{e_{k_{h}^{t}(s_{h}^{t},a_{h}^{t})}}}\Bigg{)}=\widetilde{\mathcal{ O}}\Big{(}\sqrt{H^{5}SAT}+SAH^{3}\Big{)}.\]

Combining this upper bound with the previous ones, we conclude the statement.

Proofs for Metric algorithm

### Assumptions

In this section we proof Lemma 2 and Lemma 1.

Proof of Lemma 1.: By the dual formula for 1-Wasserstein distance (see e.g. Section 6 of Peyre and Cuturi (2019)) we have

\[\mathcal{W}_{1}(p_{h}(s,a),p_{h}(s^{\prime},a^{\prime}))=\sup_{f\text{ is }1 -\text{Lipchitz}}\{p_{h}f(s,a)-p_{h}f(s^{\prime},a^{\prime})\}.\]

By Assumption 2 we have

\[p_{h}f(s,a)-p_{h}f(s^{\prime},a^{\prime})=\mathbb{E}_{\xi_{h}}[f(F_{h}(s,a, \xi_{h}))-f(F_{h}(s^{\prime},a^{\prime},\xi_{h}))]\leq L_{F}\rho((s,a),(s^{ \prime},a^{\prime})).\]

Proof of Lemma 2.: Let us proceed by a backward induction over \(h\). For \(h=H+1\) we have \(Q^{\star}_{H+1}(s,a)=V^{\star}_{H+1}(s)=0\), therefore they are \(0\)-Lipchitz.

Next we assume that have for any \(h^{\prime}>h\) the statement of Lemma 2 holds. Then by Bellman equations

\[|Q^{\star}_{h}(s,a)-Q^{\star}_{h}(s^{\prime},a^{\prime})|\leq|r_{h}(s,a)+r_{h} (s^{\prime},a^{\prime})|+|p_{h}V^{\star}_{h+1}(s,a)-p_{h}V^{\star}_{h+1}(s^{ \prime},a^{\prime})|.\]

By Assumption 2 we can represent the action of the transition kernel as follows

\[p_{h}V^{\star}_{h+1}(s,a)-p_{h}V^{\star}_{h+1}(s^{\prime},a^{\prime})=\mathbb{ E}_{\xi_{h}}\big{[}V^{\star}_{h+1}(F_{h}(s,a,\xi_{h}))-V^{\star}_{h+1}(F_{h}(s^{ \prime},a^{\prime},\xi_{h})\big{]}.\]

Since by induction hypothesis \(V^{\star}_{h+1}\) is \(\sum_{h^{\prime}=h+1}^{H}L_{F}^{h^{\prime}-h}L_{r}\)-Lipschitz and \(F_{h}(\cdot,\xi_{h})\) is \(L_{F}\)-Lipschitz, therefore

\[|Q^{\star}_{h}(s,a)-Q^{\star}_{h}(s^{\prime},a^{\prime})| \leq\Bigg{(}L_{r}+L_{F}\cdot\sum_{h^{\prime}=h+1}^{H}L_{F}^{h^{ \prime}-h}L_{r}\Bigg{)}\rho((s,a),(s^{\prime},a^{\prime}))\] \[\leq\Bigg{(}\sum_{h^{\prime}=h}^{H}L_{F}^{h^{\prime}-h}L_{r} \Bigg{)}\rho((s,a),(s^{\prime},a^{\prime}))\]

To show that \(V^{\star}_{h}\) is also Lipchitz, we have that there is some action \(a^{\star}\) equal to \(\pi^{\star}(s)\) or \(\pi^{\star}(s^{\prime})\), such that

\[|V^{\star}_{h}(s)-V^{\star}_{h}(s^{\prime})|\leq|Q^{\star}_{h}(s,a^{\star})-Q^ {\star}_{h}(s^{\prime},a^{\star})|\leq L_{V,h}\cdot\rho((s,a^{\star}),(s^{ \prime},a^{\star}))\leq L_{V,h}\cdot\rho_{\mathcal{S}}(s,s^{\prime}),\]

where in the end we used the sub-additivity assumption on metric over joint space (see Assumption 1). 

### Algorithm

Next we describe a simple non-adaptive version of our algorithm that works with metric spaces. We assume that for any \(\varepsilon>0\) we can compute a minimal \(\varepsilon\)-cover of state-action space \(\mathcal{N}_{\varepsilon}\).5

Footnote 5: Remark that the greedy algorithm can easily generate \(\varepsilon\)-cover of size \(N_{\varepsilon/2}\), that will not affect the asymptotic behavior of regret bounds, see Song and Sun (2019).

Next we will use the same notation but with state-action pairs replaces with balls from a fixed cover \(\mathcal{N}_{\varepsilon}\). To unify the notation, we define \(\psi_{\varepsilon}\colon\mathcal{S}\times\mathcal{A}\to\mathcal{N}_{\varepsilon}\) that maps any point \((s,a)\) to any ball from \(\varepsilon\)-cover that contains it.

For any \(t,h\) we define \(B^{t}_{h}=\psi_{\varepsilon}(s^{t}_{h},a^{t}_{h})\). Next, let \(n^{t}_{h}(B)\) be a number of visits of ball \(B\) before the episode \(t\): \(n^{t}_{h}(B)=\sum_{k=1}^{t-1}\mathds{1}\{B^{k}_{h}=B\}\).

Let \(e_{k}=\lfloor(1+1/H)^{k}\cdot H\rfloor\) be length of each stage for any \(k\geq 0\) and, by convention, \(e_{-1}=0\). We will call that in the beginning of episode \(t\) a pair \((B,h)\) is in \(k\)-th stage if \(n^{t}_{h}(B)\in[\sum_{i=0}^{k-1}e_{i},\sum_{i=0}^{k}e_{i})\).

Let \(\widetilde{n}_{h}^{t}(B)\) be a number of visits of state-action pair during the current stage in the beginning of episode \(t\). Formally, \(\widetilde{n}_{h}^{t}(B)=n_{h}^{t}(B)-\sum_{i=0}^{k-1}e_{i}\), where \(k\) is an index of current stage.

Define \(\kappa>0\) be a posterior inflation coefficient, \(n_{0}\) is a number of pseudo-transitions, and \(J\) as a number of temporary \(Q\)-functions. Let \(\widetilde{Q}_{h}^{t,j}\) be a \(j\)-th _temporary_ Q-value and \(\overline{Q}_{h}^{t}\) be a _policy_ Q-value at the beginning of episode \(t\), defined over the \(\varepsilon\)-cover. We initialize them as follows

\[\overline{Q}_{h}^{1}(B)=r_{0}H,\quad\widetilde{Q}_{h}^{1,j}(s,a)=r_{0}H.\]

Additionally, we define to the value function as follows

\[\overline{V}_{h}^{t}(s)=\max_{a\in\mathcal{A}}\overline{Q}_{h}^{t}(\psi_{ \varepsilon}(s,a)).\]

Notice that we cannot precomute it as in the tabular setting, however, it is possible to use its values in lazy fashion.

For each transition we preform the following update of temporary Q-values over balls \(B\in\mathcal{N}_{\varepsilon}\)

\[\widetilde{Q}_{h}^{t+1/2,j}(B)=\begin{cases}(1-w_{j,\widetilde{n}})\cdot \widetilde{Q}_{h}^{t,j}(B)+w_{j,\widetilde{n}}[r_{h}(s_{h}^{t},a_{h}^{t})+ \overline{V}_{h+1}^{t}(s_{h+1}^{t})],&B=B_{h}^{t}\\ \widetilde{Q}_{h}^{t,j}(B)&\text{otherwise},\end{cases}\]

where \(\widetilde{n}=\widetilde{n}_{h}^{t}(B)\) is the number of visits of \((B,h)\) in the beginning of episode \(t\), and \(w_{j,\widetilde{n}}\) is a sequence of independent beta-distribution random variables \(w_{j,\widetilde{n}}\sim\operatorname{Beta}(1/\kappa,(\widetilde{n}+n_{0})/\kappa)\).

Next we define the stage update as follows

\[\overline{Q}_{h}^{t+1}(B) =\begin{cases}\max_{j\in[J]}\widetilde{Q}_{h}^{t+1/2,j}(B)& \widetilde{n}_{h}^{t}(B)=\lfloor(1+1/H)^{k}H\rfloor\\ \overline{Q}_{h}^{t}(B)&\text{otherwise}\end{cases}\] \[\widetilde{Q}_{h}^{t+1,j}(B) =\begin{cases}r_{0}H&n_{h}^{t}(B)\in\widetilde{n}_{h}^{t}(B)= \lfloor(1+1/H)^{k}H\rfloor\\ \widetilde{Q}_{h}^{t+1/2,j}(B)&\text{otherwise}\end{cases}\] \[\overline{V}_{h}^{t+1}(s) =\min\{r_{0}(H-h),\max_{a\in\mathcal{A}}\overline{Q}_{h}^{t+1}( \psi_{\varepsilon}(s,a))\};\] \[\pi_{h}^{t+1}(s) \in\operatorname*{arg\,max}_{a\in\mathcal{A}}\overline{Q}_{h}^{t +1}(\psi_{\varepsilon}(s,a)),\]where \(k\) is the current stage. A detailed description of the algorithm is presented in Algorithm 4.

For episode \(t\) we will call \(k^{t}_{h}(B)\) the index of stage where \(\overline{Q}^{t}_{h}(B)\) were updated (and \(k^{t}_{h}(B)=-1\) if there was no update). For all \(t\) we define \(\tau^{t}_{h}(B)\leq t\) as the episode when the stage update happens. In other words, for any \(t\) the following holds

\[\overline{Q}^{t+1}_{h}(B)=\max_{j\in[J]}\widetilde{Q}^{\tau^{t}_{h}(B)+1/2,j}_ {h}(B),\]

where \(\tau^{t}_{h}(B)=0\) and \(e_{k}=0\) if there was no updates. To simplify the notation we will omit dependence on \((s,a,h)\) where it is deducible from the context.

We notice that in this case we use \(e_{k}\) samples to compute \(\widetilde{Q}^{\tau^{t}_{h}(B)+1/2,j}\) for \(k=k^{t}_{h}(s,a)\). For this \(k\) we define \(\ell^{i}_{k,h}(s,a)\) as the time of \(i\)-th visit of state-action pair \((s,a)\) during \(k\)-th stage. Then we have the following decomposition

\[\widetilde{Q}^{\tau^{t}+1/2,j}_{h}(B)=\sum_{i=0}^{e_{k}}W^{i}_{j,e_{k}}\bigg{(} r_{h}(s^{\ell^{i}}_{h},a^{\ell^{i}}_{h})+\overline{V}^{\ell^{i}_{h+1}}(s^{\ell^ {i}}_{h+1})\bigg{)}, \tag{9}\]

where we drop dependence on \(k\) and \((B,h)\) in \(\ell^{i}\) to simplify notations, using the convention \(r_{h}(s^{\ell^{0}}_{h},a^{\ell^{0}}_{h})=r_{0}\), \(\overline{V}^{\ell^{0}}_{h+1}(s^{\ell^{0}}_{h+1})=r_{0}(H-1)\) and the following aggregated weights

\[W^{0}_{j,n}=\prod_{q=0}^{n-1}(1-w_{j,q}),\quad W^{i}_{j,n}=w_{j,i-1}\cdot\prod _{q=i}^{n-1}(1-w_{j,q}),\;i\geq 1.\]

### Concentration

Let \(\beta^{*}\colon(0,1)\times\mathbb{N}\times(0,d_{\max})\to\mathbb{R}_{+}\) and \(\beta^{B},\beta^{\mathrm{conc}},\beta\colon(0,1)\times(0,d_{\max})\to\mathbb{ R}_{+}\) be some function defined later on in Lemma 6. We define the following favorable events

\[\mathcal{E}^{*}(\delta,\varepsilon)\triangleq\left\{\forall t\in \mathbb{N},\forall h\in[H],\forall B\in\mathcal{N}_{\varepsilon},k=k^{t}_{h} (B),(s,a)=\mathrm{center}(B):\right.\] \[\left.\mathcal{K}_{\text{inf}}\Bigg{(}\frac{1}{e_{k}}\sum_{i=1}^{e _{k}}\delta_{V^{*}_{h+1}(F_{h}(s,a,\xi^{\ell^{i}}_{h+1}))},p_{h}V^{*}_{h+1}(s, a)\Bigg{)}\leq\frac{\beta^{*}(\delta,e_{k},\varepsilon)}{e_{k}}\right\},\] \[\mathcal{E}^{B}(\delta,\varepsilon)\triangleq\left\{\forall t\in [T],\forall h\in[H],\forall B\in\mathcal{N}_{\varepsilon},\forall j\in[J],k=k^ {t}_{h}(B):\right.\] \[\left.\left|\sum_{i=0}^{e_{k}}\big{(}W^{i}_{j,e_{k},k}-\mathbb{E} [W^{i}_{j,e_{k},k}]\big{)}\bigg{(}r_{h}(s^{\ell^{i}}_{h},a^{\ell^{i}}_{h})+ \overline{V}^{\ell^{i}}_{h+1}(s^{\ell^{i}}_{h+1})\bigg{)}\right|\right.\] \[\left.\leq 60\mathrm{e}^{2}\sqrt{\frac{r_{0}^{2}H^{2}\kappa\beta^ {B}(\delta,\varepsilon)}{e_{k}+n_{0}(k)}}+1200\mathrm{e}\frac{r_{0}H\kappa \log(e_{k}+n_{0}(k))(\beta^{B}(\delta,\varepsilon))^{2}}{e_{k}+n_{0}(k)}\right\},\] \[\mathcal{E}^{\mathrm{conc}}(\delta,\varepsilon)\triangleq\left\{ \forall t\in[T],\forall h\in[H],\forall B\in\mathcal{N}_{\varepsilon},k=k^ {t}_{h}(B):\right.\] \[\left.\left|\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}V^{*}_{h+1}(s^{\ell^{ i}_{h,h}(B)}_{h+1})-p_{h}V^{*}_{h+1}(s^{\ell^{i}_{h,h}(B)}_{h},a^{\ell^{i}_{h,h}(B)} )\right|\leq\sqrt{\frac{2r_{0}^{2}H^{2}\beta^{\mathrm{conc}}(\delta, \varepsilon)}{e_{k}}}\right\}\] \[\mathcal{E}(\delta)\triangleq\Bigg{\{}\sum_{t=1}^{T}\sum_{h=1}^{ H}(1+1/H)^{H-h}\big{|}p_{h}[V^{*}_{h+1}-V^{*\tau}_{h+1}](s^{t}_{h},a^{t}_{h})-[V^{*}_{h+1}- V^{*\tau}_{h+1}](s^{t}_{h+1})\big{|}\right.\] \[\left.\leq 2er_{0}H\sqrt{2HT\beta(\delta)}.\right\}.\]We also introduce the intersection of these events, \(\mathcal{G}(\delta)\triangleq\mathcal{E}^{*}(\delta)\cap\mathcal{E}^{B}(\delta) \cap\mathcal{E}^{\mathrm{conc}}(\delta)\cap\mathcal{E}(\delta)\). We prove that for the right choice of the functions \(\beta^{*}\), \(\beta^{\mathrm{KL}}\), \(\beta^{\mathrm{conc}},\beta,\beta^{\mathrm{Var}}\) the above events hold with high probability.

**Lemma 6**.: _For any \(\delta\in(0,1)\) and \(\varepsilon\in(0,d_{\max})\) and for the following choices of functions \(\beta,\)_

\[\beta^{*}(\delta,n,\varepsilon) \triangleq\log(8H/\delta)+\log(N_{\varepsilon})+3\log(\mathrm{e} \pi(2n+1))\,,\] \[\beta^{B}(\delta,\varepsilon) \triangleq\log(8H/\delta)+\log(N_{\varepsilon})+\log(TJ)\,,\] \[\beta^{\mathrm{conc}}(\delta,\varepsilon) \triangleq\log(8H/\delta)+\log(N_{\varepsilon})+\log(2T),\] \[\beta(\delta) \triangleq\log(16/\delta),\]

_it holds that_

\[\mathbb{P}[\mathcal{E}^{*}(\delta,\varepsilon)]\geq 1-\delta/8, \mathbb{P}[\mathcal{E}^{B}(\delta,\varepsilon)]\geq 1-\delta/8,\] \[\mathbb{P}[\mathcal{E}^{\mathrm{conc}}(\delta,\varepsilon)]\geq 1 -\delta/8, \mathbb{P}[\mathcal{E}(\delta)]\geq 1-\delta/8.\]

_In particular, \(\mathbb{P}[\mathcal{G}(\delta)]\geq 1-\delta/2\)._

Proof.: Let us describe the changes from the similar statement in Lemma 4.

Regarding event \(\mathcal{E}^{*}(\delta,\varepsilon)\), for any fixed ball \(B\) we have exactly the same structure of the problem thanks to Assumption 2 and a sequence of i.i.d. random variables \(\xi_{h}^{\ell^{i}}\). Thus, Theorem 4 combined with a union bound over \(B\in\mathcal{N}_{\varepsilon}\) and \(H\in[H]\) concludes \(\mathbb{P}(\mathcal{E}^{*}(\delta,\varepsilon))\geq 1-\delta/8\).

The proof for the event \(\mathcal{E}^{B}(\delta,\varepsilon)\) remains the almost the same, with two differences: the predictable weights slightly changed but the upper bound for them remain the same, and we have take a union bound not over all state-action pairs \((s,a)\in\mathcal{S}\times\mathcal{A}\) but all over balls \(B\in\mathcal{N}_{\varepsilon}\).

To show that \(\mathbb{P}(\mathcal{E}^{\mathrm{conc}}(\delta,\varepsilon))\geq 1-\delta/8\), let us fix \(B\in\mathcal{N}_{\varepsilon},h\in[H]\) and \(e_{k}\in[T]\). Then we can define a filtration \(\mathcal{F}_{t,h}=\sigma\big{\{}\{(s_{h^{\prime}}^{t},a_{h^{\prime}}^{t},\pi^ {\ell}),\ell<t,h^{\prime}\in[H]\}\cup\{(s_{h^{\prime}}^{t},a_{h^{\prime}}^{t}, \pi^{t}),h^{\prime}\leq h\}\big{\}}\) and, since \(\ell_{k,h}^{i}(B)\) are stopping times for all \(i=1,\ldots,e_{k}\), we can define the stopped filtration \(\widetilde{\mathcal{F}_{i}}=\mathcal{F}_{\ell^{i},h}\). Then we notice that \(X_{i}=V_{h+1}^{*}(s_{h+1}^{\ell_{k,h}^{i}(B)})-p_{h}V_{h+1}^{*}(s_{h}^{\ell_{ k,h}^{i}(B)},a_{h}^{\ell_{k,h}^{i}(B)})\) forms a martingale-difference sequence with respect to \(\widetilde{\mathcal{F}_{i}}\). Thus, by Azuma-Hoeffding inequality and a union bound we have \(\mathbb{P}(\mathcal{E}^{\mathrm{conc}}(\delta,\varepsilon))\geq 1-\delta/8\).

The proof of \(\mathbb{P}(\mathcal{E}(\delta))\geq 1-\delta/8\) remains exactly the same as in Lemma 4. 

### Optimism

In this section we prove that our estimate of \(Q\)-function \(\overline{Q}_{h}^{\,t}(s,a)\) is optimistic that is the event

\[\mathcal{E}_{\mathrm{opt}}(\varepsilon)\triangleq\Big{\{}\forall t\in[T],h\in[ H],(s,a)\in\mathcal{S}\times\mathcal{A}:\overline{Q}_{h}^{\,t}(\psi_{ \varepsilon}(s,a))\geq Q_{h}^{\star}(s,a)\Big{\}}. \tag{10}\]

holds with high probability on the event \(\mathcal{E}^{\star}(\delta,\varepsilon)\).

Define constants

\[c_{0}\triangleq\frac{8}{\pi}\Bigg{(}\frac{4}{\sqrt{\log(17/16)}}+8+\frac{49 \cdot 4\sqrt{6}}{9}\Bigg{)}^{2}+1. \tag{11}\]

and slightly another constant

\[\tilde{c}_{J}\triangleq\frac{1}{\log\!\left(\frac{4}{3+\Phi(1)}\right)}, \tag{12}\]

where \(\Phi(\cdot)\) is a CDF of a normal distribution.

**Proposition 4**.: Define a constant \(L=L_{r}+L_{V}(1+L_{F})\). Assume that \(J=\lceil\tilde{c}_{J}\cdot(\log(2HT/\delta)+\log(N_{\varepsilon})\rceil\), \(\kappa=2\beta^{\star}(\delta,T,\varepsilon)\), \(r_{0}=2\), and a prior count \(n_{0}(k)=\lceil\widetilde{n}_{0}+\kappa+\frac{\frac{\varepsilon L}{H-1}}{H-1} \cdot(e_{k}+\widetilde{n}_{0}+\kappa)\rceil\) dependent on the stage \(k\), where \(\widetilde{n}_{0}=(c_{0}+1+\log_{17/16}(T))\cdot\kappa\).

Then on event \(\mathcal{E}^{\star}(\delta,\varepsilon)\) the following event

\[\mathcal{E}_{\mathrm{anticonc}}\triangleq\left\{\forall t\in[T]\; \forall h\in[H]\;\forall B\in\mathcal{N}_{\varepsilon}:\text{for}\;k=k_{h}^{t}(B ),(s,a)=\mathrm{center}(B):\right.\] \[\left.\max_{j\in[J]}\biggl{\{}W^{0}_{j,e_{k},k}r_{0}(H-1)+\sum_{i= 1}^{e_{k}}W^{i}_{j,e_{k},k}V^{\star}_{h+1}(F_{h}(s,a,\xi^{\ell^{i}}_{h})) \biggr{\}}\geq p_{h}V^{\star}_{h+1}(s,a)+L\varepsilon\right\}\]

holds with probability at least \(1-\delta/2\).

**Remark 1**.: We notice that the obtained result is connected to the theory of Dirichlet processes.

First, let us define the Dirichlet process, following Ferguson [1973]. The stochastic process \(G\), indexed by elements \(B\) of \(\mathsf{X}\), is a Dirichlet Process with parameter \(\nu\) (\(G\sim\mathrm{DP}(\nu)\)) if

\[G(B_{1}),\ldots,G(B_{d})\sim\mathrm{Dir}(\nu(B_{1}),\ldots,\nu(B_{d})),\]

for any measurable partition \((B_{1},\ldots,B_{d})\) of \(\mathsf{X}\).

Let \(\widehat{P}_{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{Z_{i}}\) be an empirical measure of an i.i.d. sample \(Z_{1},\ldots,Z_{n}\sim P\). Let \(\nu\) be a finite (not necessarily probability) measure on \(\mathsf{X}\) and \(\widetilde{P}_{n}\sim\mathrm{DP}(\nu+n\widehat{P}_{n})\). Then we have the following representation for the expectations of a function \(f\colon\mathsf{X}\to\mathbb{R}\) over a sampled measure \(\widetilde{P}_{n}\) (see Theorem 14.37 of Ghosal and Van der Vaart [2017] with \(\sigma=0\) for a proof)

\[\widetilde{P}_{n}f=V_{n}\cdot Qf+(1-V_{n})\sum_{i=1}^{n}W_{i}f(Z_{i}),\]

where \(V_{n}\sim\mathrm{Beta}(|\nu|,n)\), \(Q\sim\mathrm{DP}(\nu)\), and a vector \((W_{1},\ldots,W_{n})\) follows uniform Dirichlet distribution \(\mathrm{Dir}(1,\ldots,1)\). If we take \(\nu=n_{0}\cdot\delta_{Z_{0}}\) for some \(Z_{0}\in\mathsf{X}\) such that \(f(Z_{0})=r_{0}(H-1)^{\circ}\), then by a stick-breaking process representation of the Dirichlet distribution we have

\[\widetilde{P}_{n}f=\widetilde{W}_{0}r_{0}(H-1)+\sum_{i=1}^{n}\widetilde{W}_{1} f(Z_{i}),\quad(\widetilde{W}_{0},\ldots,\widetilde{W}_{1})\sim\mathrm{Dir}(n_{0},1,\ldots,1).\]

By taking an appropriate \(\mathsf{X}\) and \(f\) we have that Proposition 4 could be interpret as a deriving a lower bound on the probability of \(\mathbb{P}[\widetilde{P}_{n}f\geq Pf+\varepsilon L\mid\{Z_{i}\}_{i=1}^{n}]\).

Proof.: First for all, let us fix \(t\in[T],h\in[H]\) and \(B\in\mathcal{N}_{\varepsilon}\) and, consequently, \(k=k_{h}^{t}(B)\). Also, let fix \(j\in[J]\). To simplify the notation in the sequel, define \(X_{0}=r_{0}(H-1)\) and \(X_{i}=V^{\star}_{h+1}(F_{h}(s,a,\xi^{\ell^{i}}_{h}))\) for \(i>0\). Notice that \(X_{i}\) for \(i>0\) is a sequence of i.i.d. random variables supported on \([0,H-h-1]\).

By Lemma 3 we have \((W^{0}_{j,e_{k},k},\ldots,W^{e_{k}}_{j,e_{k},k})\sim\mathrm{Dir}(n_{0}(k)/ \kappa,1/\kappa,\ldots,1/\kappa)\). Then we use the aggregation property of Dirichlet distribution: there is a vector \((\widetilde{W}_{j}^{-1},\ldots,\widetilde{W}_{j}^{e_{k}})\sim\mathrm{Dir}((n_ {0}(k)-\widetilde{n}_{0})/\kappa,\widetilde{n}_{0}/\kappa,1/\kappa,\ldots,1/ \kappa)\) such that

\[\sum_{i=0}^{e_{k}}W^{i}_{j,e_{k},k}X_{i}=\widetilde{W}_{j}^{-1}X_{0}+\sum_{i=0 }^{e_{k}}\widetilde{W}_{j}^{i}X_{i}.\]

Next we are going to represent the Dirichlet random vector \(\widetilde{W}\) by a stick breaking process (or, equivalently, represent via the generalized Dirichlet distribution)

\[\widetilde{W}_{j}^{-1} =\xi_{j} \xi_{j}\sim\mathrm{Beta}((n_{0}(k)-\widetilde{n}_{0})/\kappa,(e_{ k}+\widetilde{n}_{0})/\kappa),\] \[(\widetilde{W}_{j}^{0},\ldots,\widetilde{W}_{j}^{e_{k}}) =(1-\xi_{j})\cdot(\widetilde{W}_{j}^{0},\ldots,\widetilde{W}_{ j}^{e_{k}}), \widetilde{W}_{j}\sim\mathrm{Dir}(\widetilde{n}_{0}/\kappa,1/\kappa,\ldots,1/ \kappa),\]where \(\xi_{j}\) and \(\widehat{W}_{j}\) are independent. Therefore, we have the final decomposition

\[\sum_{i=0}^{e_{k}}W_{j,e_{k},k}^{i}X_{i}-p_{h}V_{h+1}^{\star}(s,a)- \varepsilon L =\underbrace{\xi_{j}\big{(}r_{0}(H-1)-p_{h}V_{h+1}^{\star}(s,a) \big{)}-\varepsilon L}_{T_{\rm approx}}\] \[+(1-\xi_{j})\cdot\underbrace{\left(\sum_{i=0}^{e_{k}}\widehat{W}_ {j}^{i}X_{i}-p_{h}V_{h+1}^{\star}(s,a)\right)}_{T_{\rm stoch}}.\]

By independence of \(\xi_{j}\) and \(\widehat{W}_{j}\) we have

\[\mathbb{P}\!\left[\sum_{i=0}^{e_{k}}W_{j,e_{k},k}^{i}X_{i}\geq p_{h}V_{h+1}^{ \star}(s,a)+\varepsilon L|\{X_{i}\}_{i=1}^{e_{k}}\right]\geq\mathbb{P}\!\left[ T_{\rm approx}\geq 0\right]\cdot\mathbb{P}\!\left[T_{\rm stoch}\geq 0\right]\!.\]

We split our problem to lower bound the two separate probabilities.

Approximation errorTo deal with approximation error, we first of all notice that \(p_{h}V_{h+1}^{\star}(s,a)\leq H-1\), therefore we have

\[\mathbb{P}\!\left[T_{\rm approx}\geq 0\right]=\mathbb{P}\!\left[\xi_{j}\geq \frac{\varepsilon L}{H-1}\right]\!.\]

Next we assume that \(\varepsilon<(H-1)/L\), since \(\xi_{j}\sim\operatorname{Beta}((n_{0}(k)-\widetilde{n}_{0})/\kappa,(e_{k}+ \widetilde{n}_{0})/\kappa)\), we may apply Alfs and Dinges (1984, Theorem 1.2")

\[\mathbb{P}\!\left[T_{\rm approx}\geq 0\right]\geq\Phi\Big{(}\!-\!\operatorname {sign}(p-\mu)\cdot\sqrt{2\overline{\alpha}\operatorname{kl}(p,\mu)}\Big{)},\]

where \(p=(n_{0}(k)-\widetilde{n}_{0}-\kappa)/(e_{k}+\widetilde{n}_{0}-\kappa)\) and \(\mu=\varepsilon L/(H-1)\). Since \(n_{0}(k)=\lceil\widetilde{n}_{0}+\kappa+\frac{\varepsilon L}{H-1}\cdot(e_{k} +\widetilde{n}_{0}+\kappa)\rceil\), we have \(\mathbb{P}\!\left[T_{\rm approx}\geq 0\right]\geq 1/2\).

Stochastic errorSince \(X_{0}=r_{0}(H-1)\) is an upper bound on \(V\)-function, and we have that the weight of the first atom \(\alpha_{0}\triangleq\widetilde{n}_{0}/\kappa-1=c_{0}+\log_{17/16}(T)-1\) for \(c_{0}\) defined in (11).

Define a measure \(\overline{\nu}_{e_{k}}=\frac{\widetilde{n}_{0}-\kappa}{e_{k}+\widetilde{n}_{0 }-\kappa}\delta_{X_{0}}+\sum_{i=1}^{e_{k}}\frac{1}{e_{k}+n_{0}-1}\delta_{X_{i}}\). Since \(p_{h}V_{h+1}^{\star}(s,a)\leq H-h-1\), we can apply Lemma 10 with a fixed \(\varepsilon=1/2\) conditioned on independent random variables \(X_{i}\)

\[\mathbb{P}\!\left[\sum_{i=0}^{e_{k}}\widehat{W}_{j}^{i}X_{i}\geq p _{h}V_{h+1}^{\star}(s,a)\mid\{X_{i}\}_{i=1}^{e_{k}}\right]\] \[\geq\frac{1}{2}\Bigg{(}1-\Phi\Bigg{(}\sqrt{\frac{2(e_{k}+n_{0}- \kappa)\,\mathcal{K}_{\text{inf}}\big{(}\overline{\nu}_{e_{k}},p_{h}V_{h+1}^{ \star}(s,a)\big{)}}{\kappa}}\Bigg{)}\Bigg{)},\]

where \(\Phi\) is a CDF of a normal distribution. By Lemma 12 and the event \(\mathcal{E}^{\star}(\delta,\varepsilon)\)

\[(e_{k}+n_{0}-\kappa)\,\mathcal{K}_{\text{inf}}\big{(}\overline{\nu}_{e_{k}},p _{h}V_{h+1}^{\star}(s,a)\big{)}\leq e_{k}\,\mathcal{K}_{\text{inf}}\big{(} \overline{\nu}_{e_{k}},p_{h}V_{h+1}^{\star}(s,a)\big{)}\leq\beta^{\star}( \delta,T,\varepsilon),\]

where \(\widehat{\nu}_{e_{k}}=\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}\delta_{V_{h+1}^{\star} (F(s,a,\xi_{h+1}^{i}))}\), and, as a corollary

\[\mathbb{P}\!\left[\sum_{i=0}^{e_{k}}\widehat{W}_{j}^{i}X_{i}\geq p_{h}V_{h+1} ^{\star}(s,a)\mid\mathcal{E}^{\star}(\delta,\varepsilon),\{X_{i}\}_{i=1}^{e_{ k}}\right]\geq\frac{1}{2}\Bigg{(}1-\Phi\Bigg{(}\sqrt{\frac{2\beta^{\star}(\delta,T, \varepsilon)}{\kappa}}\Bigg{)}\Bigg{)}.\]

By taking \(\kappa=2\beta^{\star}(\delta,T,\varepsilon)\) we have a constant probability of being optimistic for stochastic error

\[\mathbb{P}\!\left[T_{\rm stoch}\geq 0\mid\mathcal{E}^{\star}(\delta,\varepsilon) \right]\geq\frac{1-\Phi(1)}{2}.\]

Overall, combining two lower bound for approximation and stochastic terms, we have

\[\mathbb{P}\!\left[\sum_{i=0}^{e_{k}}W_{j,e_{k},k}^{i}X_{i}\geq p_{h}V_{h+1}^{ \star}(s,a)+\varepsilon L|\mathcal{E}^{\star}(\delta,\varepsilon)\right]\geq \frac{1-\Phi(1)}{4}=\gamma.\]Next, using a choice \(J=\lceil(\log(2HT/\delta)+\log(N_{\varepsilon}))/\log(1/(1-\gamma))\rceil=\lceil \tilde{c}_{J}\cdot(\log(2HT/\delta)+\log(N_{\varepsilon}))\rceil\)

\[\mathbb{P}\!\left[\max_{j\in[J]}\!\left\{\sum_{i=0}^{\varepsilon_{k}}W_{j,e_{k},k}^{i}X_{i}\right\}\geq p_{h}V_{h+1}^{\star}(s,a)+\varepsilon L|\mathcal{E}^{ \star}(\delta,\varepsilon)\right]\geq 1-(1-\gamma)^{J}\geq 1-\frac{\delta}{2N_{ \varepsilon}HT}.\]

By a union bound we conclude the statement. 

Next we provide a connection between \(\mathcal{E}^{\mathrm{anticonc}}\) and \(\mathcal{E}^{\mathrm{opt}}\).

**Proposition 5**.: It holds \(\mathcal{E}^{\mathrm{opt}}\subseteq\mathcal{E}^{\mathrm{anticonc}}\).

Proof.: We proceed by a backward induction over \(h\). Base of induction \(h=H+1\) is trivial. Fix state-action pair \((s,a)\) and let us call \((s^{\prime},a^{\prime})\) a center of the ball \(\psi_{\varepsilon}(s,a)\) that is the ball where \((s,a)\) contains.

Next by the update formula for \(\overline{Q}_{h}^{t}\), and Bellman equations

\[\overline{Q}_{h}^{t}(\psi_{\varepsilon}(s,a))-Q_{h}^{\star}(s,a) =\max_{j\in[J]}\!\left\{\sum_{i=0}^{n}W_{j,n}^{i}[r_{h}(s_{h}^{ \epsilon^{i}},a_{h}^{\epsilon^{i}})-r_{h}(s^{\prime},a^{\prime})]\right.\] \[\left.+\sum_{i=0}^{n}W_{j,n}^{i}\overline{V}_{h+1}^{\epsilon^{i}} (s_{h+1}^{\epsilon^{i}})-p_{h}V_{h+1}^{\star}(s^{\prime},a^{\prime})\right\}\! +\![Q_{h}^{\star}(s,a)-Q_{h}^{\star}(s^{\prime},a^{\prime})],\]

where \(n=e_{k_{h}^{\epsilon^{i}}(B)}\) and we drop dependence on \(k,t,h,s,a\) in \(\ell^{i}\). By induction hypothesis we have \(\overline{V}_{h+1}^{\ell^{i}}(s^{\prime})\geq\overline{Q}_{h+1}^{\ell^{i}}( \psi_{\varepsilon}(s^{\prime},\pi^{\star}(s^{\prime})))\geq Q_{h+1}^{\star}(s^ {\prime},\pi^{\star}(s^{\prime}))=V_{h+1}^{\star}(s^{\prime})\) for any \(i\), thus combining it with Lipchitz continuity of reward function and \(Q^{\star}\), and the value of \(r_{h}(s^{\ell^{0}},a^{\ell^{0}})=r_{0}>r_{h}(s,a)\),

\[\overline{Q}_{h}^{t}(\psi_{\varepsilon}(s,a))-Q_{h}^{\star}(s,a)\geq \max_{j\in[J]}\!\left\{W_{j,n}^{0}r_{0}(H-1)+\sum_{i=1}^{n}W_{j,n }^{i}V_{h+1}^{\star}(F_{h}(s_{h}^{\ell^{i}},a_{h}^{\ell^{i}},\xi_{h}^{\ell^{i} }))\right\}\] \[-p_{h}V_{h+1}^{\star}(s^{\prime},a^{\prime})-(L_{r}+L_{V})\varepsilon.\]

Next we apply Lipschitz continuity of \(F_{h}\) and \(V_{h+1}^{\star}\) and obtain

\[\overline{Q}_{h}^{t}(\psi_{\varepsilon}(s,a))-Q_{h}^{\star}(s,a)\geq \max_{j\in[J]}\!\left\{W_{j,n}^{0}r_{0}(H-1)+\sum_{i=1}^{n}W_{j,n }^{i}V_{h+1}^{\star}(F_{h}(s^{\prime},a^{\prime},\xi_{h}^{\ell^{i}}))\right\}\] \[-p_{h}V_{h+1}^{\star}(s^{\prime},a^{\prime})-(L_{r}+L_{V}(1+L_{F} ))\varepsilon.\]

By the definition of event \(\mathcal{E}^{\mathrm{anticonc}}\) we conclude the statement. 

**Proposition 6** (Optimism).: Define a constant \(L=L_{r}+L_{V}(1+L_{F})\). Assume that \(J=\lceil\tilde{c}_{J}\cdot(\log(2HT/\delta)+\log(N_{\varepsilon})\rceil\), \(\kappa=2\beta^{\star}(\delta,T,\varepsilon)\), \(r_{0}=2\), and a prior count \(n_{0}(k)=\lceil\widetilde{n}_{0}+\kappa+\frac{\varepsilon L}{H-1}\cdot(e_{k}+ \widetilde{n}_{0}+\kappa)\rceil\) dependent on the stage \(k\), where \(\widetilde{n}_{0}=(c_{0}+1+\log_{17/16}(2e_{k}))\cdot\kappa\), \(c_{0}\) is defined in (11), \(\tilde{c}_{J}\) is defined in (12). Then \(\mathbb{P}(\mathcal{E}^{\mathrm{opt}}\mid\mathcal{E}^{\star}(\delta, \varepsilon))\geq 1-\delta/2\).

### Regret Bounds

As in the tabular setting, we first connect our algorithm to the algorithm by Song and Sun (2019), using the following corollary. Define an event \(\mathcal{G}^{\prime}(\delta,\varepsilon)=\mathcal{G}(\delta,\varepsilon)\cap \mathcal{E}^{\mathrm{opt}}\).

Let us define the logarithmic term as follows

\[\beta^{\max}(\delta,\varepsilon)=\max\{\kappa,\widetilde{n}_{0}/\kappa,\beta^{B }(\delta,\varepsilon),\beta(\delta,\varepsilon),\beta^{\mathrm{conc}}( \delta,\varepsilon)\}\]

that has dependence of order \(\mathcal{O}(\log(TH/\delta)+\log N_{\varepsilon})\).

**Corollary 2**.: _Fix \(\varepsilon\in(0,L_{V}/H)\) and assume conditions of Proposition 6. Let \(t\in[T],h\in[H],B\in\mathcal{N}_{\varepsilon}\). Define \(k=k_{h}^{t}(B)\) and let \(\ell^{1}<\ldots<\ell^{e_{k}}\) be a excursions of \((B,h)\) till the end of the previous stage. Then on the event \(\mathcal{G}^{\prime}(\delta)\) the following bound holds for \(k\geq 0\) and any \((s,a)\in B\)_

\[0\leq\overline{Q}_{h}^{t}(B)-Q_{h}^{\star}(s,a)\leq\frac{1}{e_{k}}\sum_{i=1}^{ e_{k}}[\overline{V}_{h+1}^{\ell^{i}}(s_{h+1}^{\ell^{i}})-V_{h+1}^{\star}(s_{h+1}^{ \ell^{i}})]+\mathcal{B}_{h}^{t}(k),\]_where_

\[\mathcal{B}^{t}_{h}(k)=121\mathrm{e}^{2}\cdot\sqrt{\frac{H^{2}(\beta^{\max}(\delta, \varepsilon))^{2}}{e_{k}}}+2401\mathrm{e}\cdot\frac{H(\beta^{\max}(\delta, \varepsilon))^{4}}{e_{k}}+3(L_{r}+(1+L_{F})L_{V})\varepsilon.\]

Proof.: The lower bound follows from the definition of the event \(\mathcal{E}^{\mathrm{opt}}\). For the upper bound we first apply the decomposition for \(\overline{Q}^{t}_{h}(s,a)\) and the definition of event \(\mathcal{E}^{B}(\delta,\varepsilon)\) from Lemma 6

\[\overline{Q}^{t}_{h}(B) =\max_{j\in[J]}\biggl{\{}\sum_{i=0}^{e_{k}}W^{i}_{j,n}\biggl{(}r_ {h}(s^{\ell^{i}}_{h},a^{\ell^{i}}_{h})+\overline{V}^{\ell^{i}}_{h+1}(s^{\ell^{ i}}_{h+1})\biggr{)}\biggr{\}}\] \[\leq\frac{1}{e_{k}+n_{0}(k)}\sum_{i=1}^{e_{k}}\biggl{(}r_{h}(s^{ \ell^{i}}_{h},a^{\ell^{i}}_{h})+\overline{V}^{\ell^{i}}_{h+1}(s^{\ell^{i}}_{h+ 1})\biggr{)}+\frac{n_{0}(k)\cdot 2H}{e_{k}+n_{0}(k)}\] \[+120\mathrm{e}^{2}\sqrt{\frac{H^{2}\kappa\beta^{B}(\delta, \varepsilon)}{e_{k}+n_{0}(k)}}+2400\mathrm{e}\frac{H\kappa\log(n+n_{0}(k))( \beta^{B}(\delta,\varepsilon))^{2}}{e_{k}+n_{0}(k)}.\]

Additionally, by Bellman equations

\[Q^{\star}_{h}(s,a) =\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}Q^{\star}_{h}(s^{\ell^{i}}_{h},a^{\ell^{i}}_{h})+\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}\Bigl{(}Q^{\star}_{h}(s,a) -Q^{\star}_{h}(s^{\ell^{i}}_{h},a^{\ell^{i}}_{h})\Bigr{)}\] \[\geq\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}\Bigl{(}r_{h}(s^{\ell^{i}}_{ h},a^{\ell^{i}}_{h})+p_{h}V^{\star}_{h+1}(s^{\ell^{i}}_{h},a^{\ell^{i}}_{h}) \Bigr{)}-2\varepsilon L_{V}.\]

Combining and using the fact that \(n_{0}(k)\leq\frac{Le}{H-1}\cdot(e_{k}+n_{0}(k))+\widetilde{n}_{0}+\kappa\) for \(L=L_{r}+(1+L_{F})L_{V}\)

\[\overline{Q}^{t}_{h}(s,a)-Q^{\star}_{h}(s,a) \leq\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}\biggl{[}\overline{V}^{\ell^ {i}}_{h+1}-V^{\star}_{h+1}\biggr{]}(s^{\ell^{i}}_{h+1}+\frac{1}{e_{k}}\sum_{i =1}^{e_{k}}\Bigl{[}V^{\star}_{h+1}(s^{\ell^{i}}_{h+1})-p_{h}V^{\star}_{h+1}(s^ {\ell^{i}}_{h},a^{\ell^{i}}_{h})\Bigr{]}\] \[+120\mathrm{e}^{2}\cdot\sqrt{\frac{H^{2}(\beta^{\max}(\delta, \varepsilon))^{2}}{e_{k}}}+(2400\mathrm{e}+2)\frac{H(\beta^{\max}(\delta, \varepsilon))^{4}}{e_{k}}+3L\varepsilon.\]

Finally, the applications of event \(\mathcal{E}^{\mathrm{conc}}(\delta,\varepsilon)\) concludes the statement. 

Let us define \(\delta^{t}_{h}=\overline{V}^{t}_{h}(s^{t}_{h})-V^{\pi^{t}}_{h}(s^{t}_{h})\) and \(\zeta^{t}_{h}=\overline{V}^{t}_{h}(s^{t}_{h})-V^{\star}_{h}(s^{t}_{h})\).

**Lemma 7**.: _Assume conditions of Proposition 6. Then on event \(\mathcal{G}^{\prime}(\delta,\varepsilon)=\mathcal{G}(\delta,\varepsilon) \cap\mathcal{E}^{\mathrm{opt}}\), where \(\mathcal{G}(\delta,\varepsilon)\) is defined in Lemma 6, the following upper bound on regret holds_

\[\mathfrak{R}^{T}\leq 2\mathrm{e}H\sum_{t=1}^{T}\sum_{h=1}^{H}\mathds{1}\{N^{t}_ {h}=0\}+\sum_{t=1}^{t}\sum_{h=1}^{H}(1+1/H)^{H-h}\xi^{t}_{h}+\mathrm{e}\sum_{t =1}^{T}\sum_{h=1}^{H}\mathcal{B}^{t}_{h}.\]

_where \(\xi^{t}_{h}=p_{h}[V^{\star}_{h+1}-V^{\pi^{t}}_{h+1}](s^{t}_{h},a^{t}_{h})-[V^{ \star}_{h+1}-V^{\pi^{t}}_{h+1}](s^{t}_{h+1})\) and \(\mathcal{B}^{t}_{h}=\mathcal{B}^{t}_{h}(k^{t}_{h}(s^{t}_{h},a^{t}_{h}))\cdot \mathds{1}\{k^{t}_{h}(s^{t}_{h},a^{t}_{h})\geq 0\}\) for \(\mathcal{B}^{t}_{h}\) defined in Corollary 2._

Proof.: As in the tabular setting, we notice that on the event \(\mathcal{E}^{\mathrm{opt}}\) we can upper bound the regret in terms of \(\delta^{t}_{1}\).

\[\mathfrak{R}^{T}\leq\sum_{t=1}^{T}\delta^{t}_{1}. \tag{13}\]

Next we analyze \(\delta^{t}_{h}\). Since \(a^{t}_{h}=\arg\max_{a\in\mathcal{A}}\overline{Q}^{t}_{h}(\psi_{\varepsilon}(s ^{t}_{h},a))\), we can use Corollary 2 and Bellman equations in the following way

\[\delta^{t}_{h} =\overline{V}^{t}_{h}(s^{t}_{h})-V^{\pi^{t}}_{h}(s^{t}_{h})= \overline{Q}^{t}_{h}(B^{t}_{h})-Q^{\pi^{t}}_{h}(s^{t}_{h},a^{t}_{h})\] \[=\overline{Q}^{t}_{h}(B^{t}_{h})-Q^{\star}_{h}(s^{t}_{h},a^{t}_{ h})+Q^{\star}_{h}(s^{t}_{h},a^{t}_{h})-Q^{\pi^{t}}_{h}(s^{t}_{h},a^{t}_{h})\] \[\leq r_{0}H\mathds{1}\{N^{t}_{h}=0\}+\mathds{1}\{N^{t}_{h}>0\} \Biggl{(}\frac{1}{N^{t}_{h}}\sum_{i=1}^{N^{t}_{h}}\zeta^{t^{t,}_{h}}_{h+1}+ \mathcal{B}^{t}_{h}(k^{t}_{h})+p_{h}[V^{\star}_{h+1}-V^{\pi^{t}}_{h+1}](s^{t}_{h },a^{t}_{h})\Biggr{)}.\]where \(k_{h}^{t}=k_{h}^{t}(B_{h}^{t})\), \(N_{h}^{t}=e_{k_{h}^{t}}\), \(\ell_{i,h}^{t}\) is an \(i\)-th visitation of the ball \(B_{h}^{t}\) during an stage \(k_{h}^{t}\), and additionally by a convention \(0/0=0\).

Define \(\xi_{h}^{t}=p_{h}[V_{h+1}^{\star}-V_{h+1}^{\pi^{t}}](s_{h}^{t},a_{h}^{t})-[V_{h +1}^{\star}-V_{h+1}^{\pi^{t}}](s_{h+1}^{t})\) a martingale-difference sequence, and \(\mathcal{B}_{h}^{t}=\mathcal{B}_{h}^{t}(k_{h}^{t})\mathds{1}\{N_{h}^{t}>0\}\) then

\[\delta_{h}^{t}\leq r_{0}H\mathds{1}\{N_{h}^{t}=0\}+\frac{\mathds{1}\{N_{h}^{t} >0\}}{N_{h}^{t}}\sum_{i=1}^{N_{h}^{t}}\zeta_{h+1}^{\ell_{i,h}^{t}}-\zeta_{h+1}^ {t}+\delta_{h+1}^{t}+\xi_{h}^{t}+\mathcal{B}_{h}^{t}.\]

and, as a result

\[\sum_{t=1}^{T}\delta_{h}^{t} \leq r_{0}H\sum_{t=1}^{T}\mathds{1}\{N_{h}^{t}=0\}+\sum_{t=1}^{T} \frac{\mathds{1}\{N_{h}^{t}>0\}}{N_{h}^{t}}\sum_{i=1}^{N_{h}^{t}}\zeta_{h+1}^{ \ell_{i,h}^{t}}\] \[\quad-\sum_{t=1}^{T}\zeta_{h+1}^{t}+\sum_{t=1}^{T}\delta_{h+1}^{t} +\sum_{t=1}^{T}\xi_{h}^{t}+\sum_{t=1}^{T}\mathcal{B}_{h}^{t}.\]

For the second term we may repeat arguments as in the proof of Lemma 5 and obtain

\[\sum_{q=1}^{T}\zeta_{h+1}^{q}\cdot\sum_{t=1}^{T}\frac{\mathds{1}\{k_{h}^{t}\geq 0 \}}{N_{h}^{t}}\sum_{i=1}^{N_{h}^{t}}\mathds{1}\{\ell_{t,h}^{t}=q\}\leq\left(1+ \frac{1}{H}\right)\sum_{q=1}^{T}\zeta_{h+1}^{q}.\]

After a simple algebraic manipulations and using the fact that \(\zeta_{h}^{t}\leq\delta_{h}^{t}\)

\[\sum_{t=1}^{T}\delta_{h}^{t} \leq H\sum_{t=1}^{T}\mathds{1}\{N_{h}^{t}=0\}+\sum_{t=1}^{T}(1+1/ H)\zeta_{h+1}^{t}-\sum_{t=1}^{T}\zeta_{h+1}^{t}+\sum_{t=1}^{T}\delta_{h+1}^{t} +\sum_{t=1}^{T}\mathcal{B}_{h}^{t}\] \[\leq H\sum_{t=1}^{T}\mathds{1}\{N_{h}^{t}=0\}+\left(1+\frac{1}{H} \right)\sum_{t=1}^{T}\delta_{h+1}^{t}+\sum_{t=1}^{T}\xi_{h}^{t}+\sum_{t=1}^{T} \mathcal{B}_{h}^{t}.\]

By rolling out the upper bound on regret (13) we have

\[\mathfrak{R}^{T}\leq 2\mathrm{e}H\sum_{t=1}^{T}\sum_{h=1}^{H}\mathds{1}\{N_{h}^ {t}=0\}+\sum_{t=1}^{t}\sum_{h=1}^{H}(1+1/H)^{H-h}\xi_{h}^{t}+\mathrm{e}\sum_{t =1}^{T}\sum_{h=1}^{H}\mathcal{B}_{h}^{t}.\]

Proof of Theorem 2.: First, we notice that the event \(\mathcal{G}^{\prime}(\delta,\varepsilon)\) defined in Lemma 7, holds with probability at least \(1-\delta\) by Lemma 6 and Proposition 6. Thus, we may assume that \(\mathcal{G}^{\prime}(\delta,\varepsilon)\) holds for \(\varepsilon>0\) that we will specify later.

By Lemma 7

\[\mathfrak{R}^{T}\leq 2\mathrm{e}H\sum_{t=1}^{T}\sum_{h=1}^{H}\mathds{1}\{k_{h}^ {t}=-1\}+\sum_{t=1}^{t}\sum_{h=1}^{H}(1+1/H)^{H-h}\xi_{h}^{t}+\mathrm{e}\sum_{t =1}^{T}\sum_{h=1}^{H}\mathcal{B}_{h}^{t}.\]

The first term is upper bounded by \(2\mathrm{e}H^{3}\cdot N_{\varepsilon}\), since there is no more than \(H\) visits of each ball in \(\varepsilon\)-net before the update for the first stage. The second term is bounded by \(\mathcal{O}(\sqrt{H^{3}T\beta^{\max}(\delta,\varepsilon)})\) by a definition of the event \(\mathcal{E}(\delta)\) in Lemma 6.

To analyze the last term, consider the following sum

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\frac{\mathds{1}\{e_{k_{h}^{t}(B_{h}^{t})}>0\}}{ \sqrt{e_{k_{h}^{t}(B_{h}^{t})}}}\leq\sum_{(B,h)\in\mathcal{N}_{\varepsilon} \times[H]}\sum_{k=0}^{k_{h}^{T}(B)}\frac{e_{k+1}}{\sqrt{e_{k}}},\]where

\[e_{k}=\Bigg{\lfloor}\bigg{(}1+\frac{1}{H}\bigg{)}^{k}H\Bigg{\rfloor}\Rightarrow\frac{ e_{k+1}}{\sqrt{e_{k}}}\leq 2\sqrt{H}\bigg{(}1+\frac{1}{H}\bigg{)}^{k/2},\]

therefore

\[\sum_{h=0}^{k_{h}^{T}(B)}\frac{e_{k+1}}{\sqrt{e_{k}}}\leq 4\sqrt{H}\frac{(1+1/H) ^{(k_{h}^{T}(B)+1)/2}}{\sqrt{1+1/H}-1}=4H\sqrt{e^{k_{h}^{T}(B)+1}}. \tag{14}\]

Notice that

\[N_{h}^{T+1}(B)\geq\sum_{k=0}^{k_{h}^{T}(B)}e^{k}=H(e^{k_{h}^{T}(B)+1}-1) \Rightarrow e^{k_{h}^{T}(B)+1}\leq\frac{N_{h}^{T+1}(B)+1}{H}\]

thus from the Cauchy-Schwarz inequality

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\frac{\mathds{1}\{e_{k_{h}^{t}(B_{h}^ {t})>0}\}}{\sqrt{e_{k_{h}^{t}(B_{h}^{t})}}} \leq 4\sqrt{H}\sum_{(B,h)\in\mathcal{N}_{\epsilon}\times[H]}\sqrt{ N_{h}^{T+1}(B)+1}\] \[\leq 4\sqrt{SAH^{2}}\sqrt{\sum_{(B,h)}(N_{h}^{T+1}(B)+1)}\leq 4 \sqrt{H^{3}T\cdot N_{\epsilon}}+4N_{\epsilon}H^{2}.\]

By the similar arguments we have

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\frac{\mathds{1}\{e_{k_{h}^{t}(B_{h}^ {t})}>0\}}{e_{k_{h}^{t}(B_{h}^{t})}}\leq\mathcal{O}(HN_{\epsilon}\log(T)).\]

Using this upper bound, we have for \(L=L_{r}+(1+L_{F})L_{V}\)

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\mathcal{B}_{h}^{t} =\mathcal{O}\Bigg{(}H\beta^{\max}(\delta,\varepsilon)\sum_{t=1}^{ T}\sum_{h=1}^{H}\frac{\mathds{1}\{e_{k_{h}^{t}(s_{h}^{t},a_{h}^{t})}>0\}}{ \sqrt{e_{k_{h}^{t}(s_{h}^{t},a_{h}^{t})}}}\Bigg{)}\] \[+\mathcal{O}\Bigg{(}H(\beta^{\max}(\delta,\varepsilon))^{4}\sum_{ t=1}^{T}\sum_{h=1}^{H}\frac{\mathds{1}\{e_{k_{h}^{t}(s_{h}^{t},a_{h}^{t})}>0\}}{ \sqrt{e_{k_{h}^{t}(s_{h}^{t},a_{h}^{t})}}}\Bigg{)}\] \[+\mathcal{O}(LTH\varepsilon)\] \[\leq\mathcal{O}\Big{(}\sqrt{H^{5}TN_{\epsilon}\cdot(\beta^{\max} (\delta,\varepsilon))^{2}}+H^{3}N_{\epsilon}(\beta^{\max}(\delta,\varepsilon) )^{4}+LTH\varepsilon\Big{)}.\]

Overall, for any fixed \(\varepsilon>0\) we have

\[\mathfrak{R}^{T}\leq\mathcal{O}\Big{(}\sqrt{H^{5}TN_{\epsilon}\cdot(\beta^{ \max}(\delta,\varepsilon))^{2}}+H^{3}N_{\epsilon}(\beta^{\max}(\delta, \varepsilon))^{4}+LTH\varepsilon+\sqrt{H^{3}T}\Big{)}.\]

Next we finally use that \(\mathcal{S}\times\mathcal{A}\) have covering dimension \(d_{c}\) that means\(N_{\varepsilon}\leq C_{N}\cdot\varepsilon^{-d_{c}}\), thus our regret bound transforms as follows

\[\mathfrak{R}^{T} \leq\mathcal{O}\bigg{(}\sqrt{H^{5}TC_{N}\varepsilon^{-d_{c}}\cdot (\log(TC_{N}H/\delta)+d_{c}\log(1/\varepsilon))^{2}}\] \[\quad+H^{3}C_{N}\varepsilon^{-d_{c}}(\log(TC_{N}H/\delta)+d_{c} \log(1/\varepsilon))^{4}+LTH\varepsilon\bigg{)}.\]

By taking \(\varepsilon=T^{-1/(d_{c}+2)}\) we conclude the statement Adaptive RandQL

In this section we describe how to improve the dependence in our algorithm from covering dimension to zooming dimension, and describe all required notation.

### Additional Notation

In this section we introduce an additional notation that is needed for introducing an adaptive version of RandQL algorithm for metric spaces.

Hierarchical partitionNext, we define all required notation to describe an adaptive partition, as Sinclair et al. (2019, 2023). Finally, we define the following general framework of hierarchical partition. Instead of balls, we will use a more general notion of regions that will induce a better structure from the computational point of view. We recall for any compact set \(A\subseteq\mathcal{S}\times\mathcal{A}\) we call \(\operatorname{diam}(A)=\max_{x,y\in A}\rho(x,y)\).

**Definition 6**.: A hierarchical partition of \(\mathcal{S}\times\mathcal{A}\) of a depth \(d>0\) is a collection of regions \(\mathcal{P}_{d}\) and their centers such that

* Each region \(B\in\mathcal{P}_{d}\) is of the form \(\mathcal{S}(B)\times\mathcal{A}(B)\), where \(\mathcal{S}(B)\subseteq\mathcal{S},\mathcal{A}(B)\subseteq\mathcal{A}\);
* \(\mathcal{P}_{d}\) is a cover of \(\mathcal{S}\times\mathcal{A}\): \(\bigcup_{B\in\mathcal{P}_{d}}B=\mathcal{S}\times\mathcal{A}\);
* For every \(B\in\mathcal{P}_{d}\), we have \(\operatorname{diam}(B)\leq d_{\max}\cdot 2^{-d}\);
* Let \(B_{1},B_{2}\in\mathcal{P}_{d}\). If \(B_{1}\neq B_{2}\) then \(\rho(\operatorname{center}(B_{1}),\operatorname{center}(B_{2}))\geq d_{\max} \cdot 2^{-d}\);
* For any \(B\in\mathcal{P}_{d}\), there exists a unique \(A\in\mathcal{P}_{d-1}\) (called the parent of \(B\)) such that \(B\subseteq A\).

and, for \(d=0\) we define it as \(\mathcal{P}_{0}=\{\mathcal{S}\times\mathcal{A}\}\).

We call the tree generated by the structure of \(\mathcal{T}=\{\mathcal{P}_{d}\}_{d\geq 0}\) a tree of this hierarchical partition. The main example of this partition is the dyadic partition of \(\mathcal{S}\times\mathcal{A}\) in the case of \(\mathcal{S}=[0,1]^{d_{\mathcal{A}}}\), \(\mathcal{A}=[0,1]^{d_{\mathcal{A}}}\) and the metric induced by the infinity norm \(\rho((s,a),(s^{\prime},a^{\prime}))=\max\{\|s-s^{\prime}\|_{\infty},\|a-a^{ \prime}\|_{\infty}\}\). For examples we refer to (Sinclair et al., 2023).

### Algorithm

In this section we describe two algorithms: Adaptive-RandQL which is an adaptive metric counterpart of RandQL, and Adaptive-Staged-RandQL which is an adaptive metric counterpart of Staged-RandQL. First, we start from the notation and algorithmic parts that will be common for both algorithms.

Algorithms maintain an adaptive partition \(\mathcal{P}_{h}^{t}\) of \(\mathcal{S}\times\mathcal{A}\), that is a sub-tree of an (infinite) tree of the hierarchical partition \(\mathcal{T}=\{\mathcal{P}_{d}\}_{d\geq 0}\). We initialize \(\mathcal{P}_{h}^{1}=\{\mathcal{P}_{0}\}\), and then we refine the tree \(\mathcal{P}_{h}^{t}\) be adding new nodes that corresponding to nodes of \(\mathcal{T}\). The leaf nodes of \(\mathcal{P}_{h}^{t}\) represent the active balls, and for \(B\in\mathcal{P}_{h}^{t}\) the set of its inactive parent balls is defined as \(\{B^{\prime}\in\mathcal{P}_{h}^{t}\ |\ B\subset B^{\prime}\}\). For any \(B\in\mathcal{P}_{h}^{t}\) we define \(d(B)\) as a depth of \(B\) in the tree under a convention \(d(\mathcal{S}\times\mathcal{A})=0\).

Additionally, we need to define so-called _selection rule_ and _splitting rule_. For any state \(s\in\mathcal{S}\) we define the set of all relevant balls as \(\mathcal{R}_{h}^{t}(s)=\{\text{ active }b\in\mathcal{P}_{h}^{t}\ |\ (s,a)\in B \text{ for some }a\in\mathcal{A}\}\). Then for the current state \(s_{h}^{t}\) we define the current ball as \(B_{h}^{t}=\operatorname*{arg\,max}_{B\in\mathcal{R}_{h}^{t}(s_{h}^{t})} \overline{Q}_{h}^{t}(B)\) and the corresponding action as \(a_{h}^{t}\). To define the splitting rule we maintain the counters \(n_{h}^{t}(B)\) for all \(B\in\mathcal{P}_{h}^{t}\) as a number of visits of a node \(B\) and all its parent nodes. Then we will perform splitting of the current ball \(B_{h}^{t}\) if \(\sqrt{d_{\max}^{2}/n_{h}^{t}(B_{h}^{t})}\leq\operatorname{diam}(B_{h}^{t})\). During splitting, we extend \(\mathcal{P}_{h}^{t+1}\) by its child nodes in the hierarchical partition tree \(\mathcal{T}\). For more details we refer to (Sinclair et al., 2023), up to small changes in notation. In particular, their constant \(\tilde{C}\) is equal to \(d_{\max}\) in our setting to make the construction exactly the same for both Adaptive-RandQL and Adaptive-Staged-RandQL algorithms.

Adaptive-RandQL This algorithm is an adaptive metric version of RandQL algorithm. We recall that for \(B\in\mathcal{P}^{t}_{h}\) we define \(n^{t}_{h}(B)=\sum_{i=1}^{t-1}\mathds{1}\{(B^{i}_{h})\text{ is a parent of }B\}\) is the number of visits of the ball \(B\) and its parent balls at step \(h\) before episode \(t\). We start by initializing the ensemble of Q-values, the policy Q-values, and values to an optimistic value \(\widetilde{Q}^{t,j}_{h}(B)=\overline{Q}^{1}_{h}(B)=\overline{V}^{1}_{h}(B)=r_{ 0}H\) for all \((j,h)\in[J]\times[H]\) and the unique ball in the partition \(B=\mathcal{S}\times\mathcal{A}\) and \(r_{0}>0\) some pseudo-rewards.

At episode \(t\) we update the ensemble of Q-values as follows, denoting by \(n=n^{t}_{h}(B)\) the count, \(w_{j,n}\sim\operatorname{Beta}(H,n)\) the independent learning rates,

\[\widetilde{Q}^{t+1,j}_{h}(B)=\begin{cases}(1-w_{j,n})\widetilde{Q}^{t,j}_{h}(B )+w_{j,n}\hat{Q}^{t,j}_{h}(s^{t}_{h},a^{t}_{h}),&B=B^{t}_{h}\\ \widetilde{Q}^{t,j}_{h}(B)&\text{otherwise},\end{cases}\]

where we defined the target \(\hat{Q}^{t,j}_{h}(s^{t}_{h},a^{t}_{h})\) as a mixture between the usual target and some prior target with mixture coefficient \(\hat{w}_{n,j}\sim\operatorname{Beta}(n,n_{0})\) and \(n_{0}\) the number of prior samples,

\[\hat{Q}^{t,j}_{h}(s^{t}_{h},a^{t}_{h})=\hat{w}_{j,n}[r_{h}(s^{t}_{h},a^{t}_{h}) +\overline{V}^{t}_{h+1}(s^{t}_{h+1})]+(1-\hat{w}_{j,n})r_{0}H\,.\]

For a discussion on prior re-injection we refer to Appendix B. The value function is computed on-flight by the rule \(\overline{V}^{t}_{h}(s)=\max_{B\in\mathcal{R}^{t}_{h}}\overline{Q}^{t}_{h}(B)\).

The policy Q-values are obtained by taking the maximum among the ensemble of Q-values

\[\overline{Q}^{t+1}_{h}(B)=\max_{j\in[J]}\widetilde{Q}^{t+1,j}_{h}(B)\,.\]

The policy is then greedy with respect to the policy Q-values and selection rule \((s,\pi^{t+1}_{h}(s))=\operatorname{center}(B)\), where \(B=\operatorname*{arg\,max}_{B\in\mathcal{R}^{t+1}_{h}}\overline{Q}^{t+1}_{h}(B)\). After the update of Q-values, algorithm verifies the splitting rule. If the splitting rule is triggered, then all new balls are defined by counter and Q-values of its parent. We notice that all Q-values could be efficiently computed on the nodes of the adaptive partition. The complete and detailed description is presented in Algorithm 6.

Adaptive-Staged-RandQL The notation for this algorithm is very close to Net-Staged-RandQL and we describe only differences between them. The main difference is a way to compute value \(\overline{V}^{t}_{h}(s)=\max_{B\in\mathcal{R}^{t}_{h}(s)}\overline{Q}^{t}_{h} (B)\) and policy \((s,\pi^{t}_{h}(s))=\operatorname{center}(B)\) for \(\arg\max_{B\in\mathcal{R}^{t}_{h}(s)}\overline{Q}^{t}_{h}(B)\). Additionally, all counters including temporary will move to the child nodes after splitting, as it performed in Adaptive-RandQL. The detailed description is presented in Algorithm 6.

```
1:Input: inflation coefficient \(\kappa\), \(J\) ensemble size, number of prior transitions \(n_{0}(k)\), prior reward \(r_{0}\).
2:Initialize:\(\overline{Q}_{h}(B)=\widetilde{Q}^{j}_{h}(B)=r_{0}H,\) initialize counters \(\widetilde{n}_{h}(B)=0\) for \(j,h,B\in[J]\times[H]\times\mathcal{N}_{e}\), stage \(q_{h}(B)=0\).
3:for\(t\in[T]\)do
4:for\(h\in[H]\)do
5: Compute \(B_{h}=\arg\max_{B\in\mathcal{R}^{t}_{h}(s_{h})}\overline{Q}_{h}(B)\) and play \(a_{h}\) for \((s_{h},a_{h})=\text{center}(B_{h})\);
6: Observe reward and next state \(s_{h+1}\sim p_{h}(s_{h},a_{h})\).
7: Sample learning rates \(w_{j}\sim\text{Beta}(1/\kappa,(\widetilde{n}+n_{0}(q_{h}(B_{h}))/\kappa)\) for \(\widetilde{n}=\widetilde{n}_{h}(B_{h})\).
8: Compute value \(\overline{V}_{h+1}(s_{h+1})=\max_{B\in\mathcal{R}^{t}_{h}(s_{h+1})}\overline{Q }_{h+1}(B)\).
9: Update temporary \(Q\)-values for all \(j\in[\tilde{J}]\) \[\widetilde{Q}^{j}_{h}(B):=(1-w_{j})\widetilde{Q}^{j}_{h}(B)+w_{j}\big{(}r_{h} (s_{h},a_{h})+\overline{V}_{h+1}(s_{h+1})\big{)}\,.\]
10: Update counters \(\widetilde{n}_{h}(B_{h}):=\widetilde{n}_{h}(B_{h})+1\) and \(n_{h}(B_{h}):=n_{h}(B_{h})+1\).
11:if\(\widetilde{n}_{h}(B_{h})=\lfloor(1+1/H)^{q}H\rfloor\) for \(q=q_{h}(B_{h})\) is the current stage then
12: Update policy \(Q\)-values \(\overline{Q}_{h}(B_{h}):=\max_{j\in[J]}\widetilde{Q}^{j}_{h}(B_{h})\).
13: Reset temporary \(Q\)-values \(\widetilde{Q}^{j}_{h}(B_{h}):=r_{0}H\).
14: Reset counter \(\widetilde{n}_{h}(B_{h}):=0\) and change stage \(k_{h}(B_{h}):=k_{h}(B_{h})+1\).
15:endif
16: If \(\sqrt{d_{\max}^{2}/n_{h}(B_{h})}\leq\text{diam}(B_{h})\), then refine partition \(B_{h}\) (see Sinclair et al. (2023)).
17:endfor
18:endfor
```

**Algorithm 6**Adaptive-Staged-RandQL

### Regret Bound

In this section we state the regret bounds for Adaptive-Staged-RandQL and derive a proof. The given proof shares a lot of similarities with the proof of Net-Staged-RandQL in the first half and to the proof of Adaptive-QL by Sinclair et al. (2023) in the second half.

We fix \(\delta\in(0,1)\), and the number of posterior samples

\[J\triangleq\lceil\tilde{c}_{J}\cdot(\log(2C_{N}HT/\delta)+d_{c}\log_{2}(8T/d_ {\max}))\rceil, \tag{15}\]

where \(\tilde{c}_{J}=1/\log(4/(3+\Phi(1)))\) and \(\Phi(\cdot)\) is the cumulative distribution function (CDF) of a normal distribution.

Additionally we select

\[n_{0}(k)=\bigg{\lceil}\widetilde{n}_{0}+\kappa+\frac{L\cdot d_{\max}}{H-1} \cdot\frac{e_{k}+\widetilde{n}_{0}+\kappa}{\sqrt{He_{k}-k-H^{2}}}\bigg{\rceil},\quad\widetilde{n}_{0}=(c_{0}+1+\log_{17/16}(T))\cdot\kappa\]

where \(c_{0}\) is an absolute constant defined in (5) (see Appendix D.3), \(\kappa\) is the posterior inflation coefficient and \(L=L_{r}+(1+L_{F})L_{V}\) is a constant. Next we restate the regret bound result for Adaptive-Staged-RandQL algorithm.

**Theorem** (Restatement of Theorem 3).: _Consider a parameter \(\delta\in(0,1)\). Let \(\kappa\triangleq 2(\log(8HC_{N}/\delta)+d_{c}\log_{2}(8T/d_{\max})+3\log(\mathrm{e} \pi(2T+1)))\), \(r_{0}\triangleq 2\). Then it holds for Adaptive-Staged-RandQL, with probability at least \(1-\delta\),_

\[\mathfrak{R}^{T}=\widetilde{\mathcal{O}}\bigg{(}LH^{3/2}\sum_{h=1}^{H}T^{\frac{ d_{z,h}+1}{d_{z,h}+2}}\bigg{)},\]

_where \(d_{z,h}\) is the step-\(h\) zooming dimension and we ignore all multiplicative factors in the covering dimension \(d_{c}\)._Proof.: We divide the proof to four main parts, a little bit different proof of Staged-RandQL and Net-Staged-RandQL since we also need to apply clipping techniques.

Concentration eventsWe can define (almost) the same set of events as in Appendix E.3, where union bound over balls is taken over all the hierarchical partition tree up to depth \(D\) that we define as \(\mathcal{T}_{D}\).

\[\mathcal{E}^{\star}(\delta) \triangleq\left\{\forall t\in\mathbb{N},\forall h\in[H],\forall B \in\mathcal{T}_{D},k=k_{h}^{t}(B),(s,a)=\mathrm{center}(B):\right.\] \[\left.\mathcal{K}_{\mathrm{inf}}\!\left(\frac{1}{e_{k}}\sum_{i=1} ^{e_{k}}\delta_{V_{h+1}^{\star}(F_{h}(s,a,\xi_{h+1}^{\ell^{i}}))},p_{h}V_{h+1} ^{\star}(s,a)\right)\leq\frac{\beta^{\star}(\delta,e_{k},\varepsilon)}{e_{k}} \right\},\] \[\mathcal{E}^{B}(\delta,T) \triangleq\left\{\forall t\in[T],\forall h\in[H],\forall B\in \mathcal{T}_{D},\forall j\in[J],k=k_{h}^{t}(B):\right.\] \[\left.\left|\sum_{i=0}^{e_{k}}\!\left(W_{j,e_{k},k}^{i}-\mathbb{E} [W_{j,e_{k},k}^{i}]\right)\!\left(r_{h}(s_{h}^{\ell^{i}},a_{h}^{\ell^{i}})+ \overline{V}_{h+1}^{\ell^{i}}(s_{h+1}^{\ell^{i}})\right)\right|\right.\] \[\left.\leq 60\mathrm{e}^{2}\sqrt{\frac{r_{0}^{2}H^{2}\kappa\beta^ {B}(\delta,\varepsilon)}{e_{k}+n_{0}(k)}}+1200\mathrm{e}\frac{r_{0}H\kappa\log (e_{k}+n_{0}(k))(\beta^{B}(\delta,\varepsilon))^{2}}{e_{k}+n_{0}(k)}\right\},\] \[\mathcal{E}^{\mathrm{conc}}(\delta,T) \triangleq\left\{\forall t\in[T],\forall h\in[H],\forall B\in \mathcal{T}_{D},k=k_{h}^{t}(B):\right.\] \[\left.\left|\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}V_{h+1}^{\star}(s_{h+ 1}^{\ell^{i}_{k,h}(B)})-p_{h}V_{h+1}^{\star}(s_{h}^{\ell^{i}_{k,h}(B)},a_{h}^{ \ell^{i}_{k,h}(B)})\right|\leq\sqrt{\frac{2r_{0}^{2}H^{2}\beta^{\mathrm{conc}}( \delta,\varepsilon)}{e_{k}}}\right\}\!,\] \[\mathcal{E}(\delta) \triangleq\left\{\,\sum_{t=1}^{T}\sum_{h=1}^{H}(1+3/H)^{H-h}\big{|} p_{h}[V_{h+1}^{\star}-V_{h+1}^{\pi_{t}}](s_{h}^{t},a_{h}^{t})-[V_{h+1}^{\star}-V_{h+1} ^{\pi_{t}}](s_{h+1}^{t})\big{|}\right.\] \[\left.\leq 2\mathrm{e}^{3}r_{0}H\sqrt{2HT\beta(\delta)}.\right.\]

To apply the union bound argument, we have to bound the size of \(\mathcal{T}_{D}\). First, we notice that relation between centers of balls in each layer \(\mathcal{P}_{d}\) implies that there at least \(|\mathcal{P}_{d}|\) non-intersecting balls of radius \(d_{\max}\cdot 2^{-d-2}\). Thus, the size of this sub-tree could be bounded as

\[|\mathcal{T}_{D}|\leq\sum_{d=0}^{D}N_{d_{\max}2^{-d-2}}\leq C_{N}\sum_{d=0}^{D }\!\left(2^{d+2}/d_{\max}\right)^{d_{c}}\leq(8/d_{\max})^{d_{c}}C_{N}\cdot 2^{d_{c} \cdot D}.\]

using the relation between covering and packing numbers, see e.g. Lemma 4.2.8 by Vershynin (2018). The only undefined quantity here is \(D\), that can be upper-bounded given budget \(T\). To do it, we apply Lemma B.2 by Sinclair et al. (2023) for any \(B\in\mathcal{P}_{h}^{t}\)

\[\left(\frac{d_{\max}}{2\cdot\mathrm{diam}(B)}\right)^{2}\leq n_{h}^{t}(B)\leq \left(\frac{d_{\max}}{\mathrm{diam}(B)}\right)^{2}. \tag{16}\]

Our goal is to find a value \(D\) such that \(\mathcal{P}_{h}^{T+1}\subseteq\mathcal{T}_{D}\) for any MDPs and correct interactions. To do it, we notice that it is equivalent to show that \(\mathrm{diam}(B)\geq d_{\max}2^{-D}\), that could be guaranteed since

\[\mathrm{diam}(B)\geq\frac{d_{\max}}{2\sqrt{n_{h}^{T+1}(B)}}\geq\frac{d_{\max}} {2T},\]

which implies that \(D=1+\log_{2}(T)\) is enough. Finally, since for the value of interest

\[\log|\mathcal{T}_{D}|\leq d_{c}\log_{2}(T)+\log C_{N}+d_{c}\log(8/d_{\max}),\]

we can define the \(\beta\)-functions as follows follows\[\beta^{*}(\delta) \triangleq\log(8C_{N}H/\delta)+d_{c}\log_{2}(8T/d_{\max})+3\log( \mathrm{e}\pi(2n+1))\,,\] \[\beta^{B}(\delta,T) \triangleq\log(8C_{N}H/\delta)+d_{c}\log_{2}(8T/d_{\max})+\log(TJ)\,,\] \[\beta^{\mathrm{conc}}(\delta,T) \triangleq\log(8C_{N}H/\delta)+d_{c}\log_{2}(8T/d_{\max})+\log(2T),\] \[\beta(\delta) \triangleq\log(16C_{N}H/\delta)+d_{c}\log_{2}(8T/d_{\max}),\]

and following line-by-line the proof of Lemma 6, for an event \(\mathcal{G}(\delta)=\mathcal{E}^{*}(\delta)\cap\mathcal{E}^{B}(\delta,T)\cap \mathcal{E}^{\mathrm{conc}}(\delta,T)\cap\mathcal{E}(\delta)\) we have \(\mathbb{P}(\mathcal{G}(\delta))\geq 1-\delta/2\).

OptimismNext, we state the required analog of Proposition 4. We can show that with probability at least \(1-\delta/2\) on the event \(\mathcal{E}^{*}(\delta)\) the following event

\[\mathcal{E}_{\mathrm{anticonc}}\triangleq\left\{\forall t\in[T]\;\forall h\in [H]\;\forall B\in\mathcal{T}_{D}:\text{for}\;k=k_{h}^{t}(B),(s,a)=\mathrm{ center}(B):\right.\]

\[\max_{j\in[J]}\biggl{\{}W_{j,e_{k},k}^{0}r_{0}(H-1)+\sum_{i=1}^{e_{k}}W_{j,e_{ k},k}^{i}V_{h+1}^{*}(F_{h}(s,a,\xi_{h}^{\ell^{i}}))\biggr{\}}\geq p_{h}V_{h+1}^{ \star}(s,a)+L\cdot\mathrm{diam}(B_{h}^{t})\biggr{\}}\]

under the choice \(J=\lceil\tilde{c}_{J}\cdot(\log(2HT/\delta)+\log(|\mathcal{T}_{D}|))\rceil\), \(\kappa=2\beta^{\star}(\delta,T)\), \(r_{0}=2\), and a prior count

\[n_{0}(k)=\lceil\widetilde{n}_{0}+\kappa+\frac{L\cdot d_{\max}}{H-1}\cdot\frac{e _{k}+\widetilde{n}_{0}+\kappa}{\sqrt{He_{k}-k-H^{2}}}\rceil\]

dependent on the stage \(k\), where \(\widetilde{n}_{0}=(c_{0}+1+\log_{17/16}(T))\cdot\kappa\), \(L=L_{r}+L_{V}(1+L_{F})\). In particular, the proof exactly the same as the proof of Proposition 4 for \(\varepsilon\) dependent on \(k\).

At the same time, it is possible to show that \(\mathcal{E}_{\mathrm{anticonc}}\) implies

\[\mathcal{E}_{\mathrm{opt}}\triangleq\Bigl{\{}\forall t\in[T],h\in[H],\forall B \in\mathcal{P}_{h}^{t},\forall(s,a)\in B:\overline{Q}_{h}^{t}(B)\geq Q_{h}^{ \star}(s,a)\Bigr{\}}. \tag{17}\]

Indeed, in the proof of Proposition 5 we actively uses the bound \(\rho((s_{h}^{\ell^{i}},a_{h}^{\ell^{i}}),(s,a))\leq\varepsilon\). In the adaptive setting, we have to, at first, use an upper bound \(\rho((s_{h}^{\ell^{i}},a_{h}^{\ell^{i}}),(s,a))\leq\mathrm{diam}(B_{h}^{\ell^ {i}})\) by a construction \(B\subseteq B_{h}^{\ell^{i}}\), and then apply Lemma B.2 by Sinclair et al. [2023] by defining an upper bound

\[\mathrm{diam}(B_{h}^{\ell^{i}})\leq\frac{d_{\max}}{\sqrt{n_{h}^{\ell^{i}_{h}}( B_{h}^{\ell^{i}})}}\leq\frac{d_{\max}}{\sqrt{\sum_{i=0}^{k-1}e_{i}}}\leq\frac{d_{ \max}}{\sqrt{H\sum_{i=0}^{k-1}(1+1/H)^{i}-k}}\leq\frac{d_{\max}}{\sqrt{He_{k}-k -H^{2}}}\]

for \(k=k_{h}^{t}(B)\) for a particular ball \(B\in\mathcal{P}_{h}^{t}\) in the case \(He_{k}-k-H^{2}\geq 0\).

By combining event \(\mathcal{E}_{\mathrm{opt}}\) and the event \(\mathcal{E}^{B}(\delta)\) we can prove the same statement as Corollary 2.

Let \(t\in[T],h\in[H],B\in\mathcal{P}_{h}^{t}\). Define \(k=k_{h}^{t}(B)\) and let \(\ell^{1}<\ldots<\ell^{e_{k}}\) be a excursions of \((B,h)\) till the end of the previous stage. Then on the event \(\mathcal{G}^{\prime}(\delta)=\mathcal{G}(\delta)\cap\mathcal{E}_{\mathrm{opt}}\) the following bound holds for \(k\geq 0\) and for any \((s,a)\in B\)

\[0\leq\overline{Q}_{h}^{t}(B)-Q_{h}^{\star}(s,a)\leq H\mathds{1}\{ He_{k}/2\leq k+H^{2}\}+\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}[\overline{V}_{h+1}^{\ell^{i}} (s_{h+1}^{\ell^{i}})-V_{h+1}^{*}(s_{h+1}^{\ell^{i}})]+\mathcal{B}_{h}^{t}, \tag{18}\]

where

\[\mathcal{B}_{h}^{t}=121\mathrm{e}^{2}\cdot\sqrt{\frac{H^{2}(\beta^{\max}( \delta,T))^{2}}{e_{k}}}+2401\mathrm{e}\cdot\frac{H(\beta^{\max}(\delta,T))^{4}} {e_{k}}+\frac{5L\cdot d_{\max}}{\sqrt{He_{k}}} \tag{19}\]

where \(k=k_{h}^{t}(B_{h}^{t})\) and \(\beta^{\max}(\delta,T)=\max\{\beta^{\star}(\delta,T),\beta^{B}(\delta),\beta^{ \mathrm{conc}}(\delta),\beta(\delta)\}\). Also we can express this bound in terms of a diameter of \(B_{h}^{t}\) as follows

\[\mathrm{diam}(B_{h}^{t}) \geq\frac{d_{\max}}{2\sqrt{n_{h}^{t}(B_{h}^{t})}}\geq\frac{d_{ \max}}{2\sqrt{\sum_{i=0}^{k}e_{i}}}\geq\frac{d_{\max}}{2\sqrt{H\sum_{i=0}^{k}(1 +1/H)^{i}}}\geq\frac{d_{\max}}{2\sqrt{H}}\] \[\geq\frac{d_{\max}}{2\sqrt{H^{2}(1+1/H)^{k+1}}}\geq\frac{d_{\max} }{2\sqrt{2He_{k}}},\]thus

\[\frac{1}{\sqrt{He_{k}}}\leq\frac{3\text{diam}(B_{h}^{t})}{d_{\max}},\]

and we have

\[\begin{split}\mathcal{B}_{h}^{t}&\leq 7566\text{e}^{2}H^{ 3/2}(\beta^{\max}(\delta,T))^{4}\text{diam}(B_{h}^{t})/d_{\max}+15L\text{diam }(B_{h}^{t})\\ &\leq\rho(H,\delta,L)\cdot\text{diam}(B_{h}^{t}),\end{split} \tag{20}\]

where we define \(\rho(H,\delta,L)\triangleq 7566\text{e}^{2}H^{3/2}(\beta^{\max}(\delta,T))^{4}/d_{ \max}+15L\).

As a additional corollary, we have for all \(t\in[T],h\in[H]\)

\[\overline{V}_{h}^{t}(s)=\max_{B\in\mathcal{R}_{h}^{t}(s)}\overline{Q}_{h}^{t} (B)=\overline{Q}_{h}^{t}(B^{\star})\geq Q_{h}^{\star}(s,\pi^{\star}(s))=V_{h}^ {\star}(s), \tag{21}\]

where \(B^{\star}\) is a ball that contains a pair \((s,\pi^{\star}(s))\).

This upper and lower bound have the similar structure as Lemma D.2 by Sinclair et al. (2023) and the rest of the proof directly follows (Sinclair et al., 2023).

Clipping techniquesNext we introduce the required clipping techniques developed by Simchowitz and Jamieson (2019), Cao and Krishnamurthy (2020). Definition 2 introduces the quantity \(\text{gap}_{h}(s,a)=V_{h}^{\star}(s)-Q_{h}^{\star}(s,a)\), and for any compact set \(B\subseteq\mathcal{S}\times\mathcal{A}\) we define \(\text{gap}_{h}(B)=\min_{(s,a)\in B}\text{gap}_{h}(s,a)\). Finally, we define clipping operator for any \(\mu,\nu\in\mathbb{R}\)

\[\text{clip}(\mu|\nu)=\mu\mathds{1}\{\mu\leq\nu\}. \tag{22}\]

In particular, this operator satisfies the following important property

**Lemma 8** (Lemma E.2. of Sinclair et al. (2023)).: _Suppose that \(\text{gap}_{h}(B)\leq\psi\leq\mu_{1}+\mu_{2}\) for any \(\psi,\mu_{1},\mu_{2}\). Then_

\[\psi\leq\text{clip}\bigg{[}\mu_{1}\bigg{|}\frac{\text{gap}_{h}(B)}{H+1}\bigg{]} +\bigg{(}1+\frac{1}{H}\bigg{)}\mu_{2}\]

Now we apply this lemma to our update rules, producing a result similar to Lemma E.3 of Sinclair et al. (2023). We notice that

\[\begin{split}\text{gap}_{h}(B_{h}^{t})&\leq\text{ gap}_{h}(s_{h}^{t},a_{h}^{t})=V_{h}^{\star}(s_{h}^{t})-Q_{h}^{\star}(s_{h}^{t},a_ {h}^{t})\\ &\leq\overline{V}_{h}^{t}(s_{h}^{t})-Q_{h}^{\star}(s_{h}^{t},a_{h }^{t})=\overline{Q}_{h}^{t}(B_{h}^{t})-Q_{h}^{\star}(s_{h}^{t},a_{h}^{t}). \end{split}\]

Thus, denoting \(\psi=\overline{Q}_{h}^{t}(B_{h}^{t})-Q_{h}^{\star}(s_{h}^{t},a_{h}^{t})\) and, by (18),

\[\mu_{1}=H\mathds{1}\{He_{k_{h}^{t}}/2>k_{h}^{t}+H^{2}\}+\mathcal{B}_{h}^{t}, \quad\mu_{2}=\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}[\overline{V}_{h+1}^{t^{i}}(s_{ h+1}^{t^{i}})-V_{h+1}^{\star}(s_{h+1}^{t^{i}})]\]

we apply Lemma 8 and obtain

\[\begin{split}\overline{V}_{h}^{t}(s_{h}^{t})-Q_{h}^{\star}(s_{h}^ {t},a_{h}^{t})&\leq\text{clip}\bigg{[}H\mathds{1}\{He_{k_{h}^{t }}/2\leq k_{h}^{t}+H^{2}\}+\mathcal{B}_{h}^{t}|\frac{\text{gap}_{h}(B_{h}^{t}) }{H+1}\bigg{]}\\ &+\bigg{(}1+\frac{1}{H}\bigg{)}\frac{1}{e_{k}}\sum_{i=1}^{e_{k}}[ \overline{V}_{h+1}^{t^{i}}(s_{h+1}^{t^{i}})-V_{h+1}^{\star}(s_{h+1}^{t^{i}})] \end{split} \tag{23}\]

for \(k_{h}^{t}=k_{h}^{t}(B_{h}^{t})\) and \(\mathcal{B}_{h}^{t}\) defined in (19).

Regret decompositionThe rest of the analysis we preform conditionally on event \(\mathcal{G}^{\prime}(\delta)=\mathcal{G}(\delta)\cap\mathcal{E}_{\mathrm{opt}}\) that holds with probability at least \(1-\delta\).

By defining \(\delta_{h}^{t}=\overline{V}_{h}^{t}(s_{h}^{t})-V^{\pi^{t}}(s_{h}^{t})\) and \(\zeta_{h}^{t}=\overline{V}_{h}^{t}(s_{h}^{t})-V_{h}^{\star}(s_{h}^{t})\) we have

\[\mathfrak{R}^{T}=\sum_{t=1}^{T}V_{1}^{\star}(s_{1}^{t})-V_{1}^{\pi^{t}}(s_{1}^ {t})\leq\sum_{t=1}^{T}\delta_{1}^{t},\]and, at the same time, by Bellman equations

\[\delta_{h}^{t} =\overline{V}_{h}^{t}(s_{h}^{t})-Q^{\pi_{h}^{t}}(s_{h}^{t},a_{h}^{t} )=\overline{V}_{h}^{t}(s_{h}^{t})-Q_{h}^{*}(s_{h}^{t},a_{h}^{t})+Q_{h}^{*}(s_{h} ^{t},a_{h}^{t})-Q^{\pi^{t}}(s_{h}^{t},a_{h}^{t})\] \[=\overline{V}_{h}^{t}(s_{h}^{t})-Q_{h}^{*}(s_{h}^{t},a_{h}^{t})+V_ {h+1}^{*}(s_{h+1}^{t})-V_{h}^{\pi^{t}}(s_{h+1}^{t})+\xi_{h}^{t}\] \[=\overline{V}_{h}^{t}(s_{h}^{t})-Q_{h}^{*}(s_{h}^{t},a_{h}^{t})+ \delta_{h+1}^{t}-\zeta_{h+1}^{t}+\xi_{h}^{t},\]

where \(\xi_{h}^{t}=p_{h}[V_{h+1}^{*}-V_{h+1}^{\pi^{t}}](s_{h}^{t},a_{h}^{t})-[V_{h+1}^ {*}-V_{h+1}^{\pi^{t}}](s_{h+1}^{t})\) is a martingale-difference sequence. By (23) we have

\[\sum_{t=1}^{T}\delta_{h}^{t} =\sum_{t=1}^{T}\overline{V}_{h}^{t}(s_{h}^{t})-Q_{h}^{*}(s_{h}^{t},a_{h}^{t})+\delta_{h+1}^{t}-\zeta_{h+1}^{t}+\xi_{h}^{t}\] \[\leq\left(1+\frac{1}{H}\right)\sum_{t=1}^{T}\frac{1}{e_{k_{h}^{t} }}\sum_{i=1}^{e_{k_{h}^{t}}^{t}}\zeta_{h+1}^{t_{h}^{t}}+\sum_{t=1}^{T}\delta_{ h+1}^{t}-\sum_{t=1}^{T}\zeta_{h+1}^{t}+\sum_{t=1}^{T}\xi_{h}^{t}\] \[+\sum_{t=1}^{T}\text{clip}\bigg{[}H\mathds{1}\{He_{k_{h}^{t}}/2>k _{h}^{t}+H^{2}\}+\mathcal{B}_{h}^{t}(k_{h}^{t})\big{|}\frac{\text{gap}_{h}(B_ {h}^{t})}{H+1}\bigg{]}\]

where \(k_{h}^{t}=k_{h}^{t}(B_{h}^{t})\). Repeating argument of Lemma 5 and Zhang et al. (2020)

\[\left(1+\frac{1}{H}\right)\sum_{t=1}^{T}\frac{1}{e_{k_{h}^{t}}}\sum_{i=1}^{e_ {k_{h}}}\zeta_{h+1}^{t_{h}^{t}}\leq\left(1+\frac{1}{H}\right)^{2}\sum_{t=1}^{ T}\zeta_{h+1}^{t}\leq\left(1+\frac{3}{H}\right)\sum_{t=1}^{T}\zeta_{h+1}^{t}.\]

Using an upper bound \(\zeta_{h}^{t}\leq\delta_{h}^{t}\) we have for any \(h\geq 1\)

\[\sum_{t=1}^{T}\delta_{h}^{t} \leq\left(1+\frac{3}{H}\right)\sum_{t=1}^{T}\delta_{h+1}^{t}+\sum _{t=1}^{T}\xi_{h}^{t}\] \[+\sum_{t=1}^{T}\text{clip}\bigg{[}H\mathds{1}\{He_{k_{h}^{t}}/2 \leq k_{h}^{t}+H^{2}\}+\mathcal{B}_{h}^{t}\big{|}\frac{\text{gap}_{h}(B_{h}^{ t})}{H+1}\bigg{]},\]

and, rolling out starting with \(h=1\) we have the following regret decomposition

\[\mathfrak{R}^{T} \leq\mathrm{e}^{3}\sum_{t=1}^{T}\sum_{h=1}^{H}H\mathds{1}\{He_{k _{h}^{t}}/2\leq k_{h}^{t}+H^{2}\} =(\mathbf{A})\] \[+\mathrm{e}^{3}\sum_{t=1}^{T}\sum_{h=1}^{H}\text{clip}\bigg{[} \mathcal{B}_{h}^{t}\big{|}\frac{\text{gap}_{h}(B_{h}^{t})}{H+1}\bigg{]} =(\mathbf{B})\] \[+\sum_{t=1}^{T}\sum_{h=1}^{H}(1+3/H)^{H-h}\xi_{h}^{t}. =(\mathbf{C})\]

Term \((\mathbf{A})\)For this term we notice that for any fixed \(h\) the following event

\[He_{k_{h}^{t}}\leq 2(k_{h}^{t}+H^{2})\iff H\lfloor H(1+1/H)^{k_{h}^{t}}\rfloor \leq 2(k_{h}^{t}+H^{2}),\]

that is guaranteed if

\[(1+1/H)^{k_{h}^{t}}\leq 2T+3\iff k_{h}^{t}\log(1+1/H)\leq\log(2T/H^{2}+3).\]

Thus, indicator can be equal to \(1\) no more than \(H\log(2T+3)\) times for any \(t\in[T]\). As a result,

\[(\mathbf{A})\leq\mathrm{e}^{2}H^{3}\log(2T+3).\]Term \((\mathbf{B})\)Let us rewrite this term using a definition of clipping operator and use the definition of near-optimal set (see Definition 3)

\[(\mathbf{B})=\mathrm{e}^{3}\sum_{t=1}^{T}\sum_{h=1}^{H}\mathcal{B}_{h}^{t}\mathds{1 }\big{\{}(H+1)\mathcal{B}_{h}^{t}\geq\mathrm{gap}_{h}(B_{h}^{t})\big{\}}\leq \mathrm{e}^{3}\sum_{t=1}^{T}\sum_{h=1}^{H}\mathcal{B}_{h}^{t}\mathds{1}\{ \mathrm{center}(B_{h}^{t})\in Z_{h}^{\mathcal{B}_{h}^{t}}\}.\]

Next we consider the summation for a fixed \(h\). Here we follow Theorem F.3 by Sinclair et al. (2023) and obtain

\[\sum_{t=1}^{T}\mathcal{B}_{h}^{t}\mathds{1}\{\mathrm{center}(B_{h}^{t})\in Z_ {h}^{\mathcal{B}_{h}^{t}}\}=\sum_{r}\sum_{B:\mathrm{diam}(B)=r}\sum_{t:B_{h}^ {t}=B}\mathcal{B}_{h}^{t}\mathds{1}\{\mathrm{center}(B)\in Z_{h}^{\mathcal{B} _{h}^{t}}\},\]

where we applied an additional rescaling by a function \(\rho\) defined in (20).

Next we fix a constant \(r_{0}>0\) and break a summation into two parts: \(r\geq r_{0}\) and \(r\leq r_{0}\).

1. Case \(r\leq r_{0}\). In this situation we have can apply (20) \[\sum_{r\leq r_{0}}\sum_{B:\mathrm{diam}(B)=r}\sum_{t:B_{h}^{t}=B} \mathcal{B}_{h}^{t}\mathds{1}\{\mathrm{center}(B)\in Z_{h}^{\mathcal{B}_{h}^{ t}}\}\] \[=\mathcal{O}(Tr_{0}\rho(H,\delta,L)).\]
2. Case \(r\geq r_{0}\). In this situation we also apply (20) under the indicator function \[\sum_{r\geq r_{0}}\sum_{B:\mathrm{diam}(B)=r}\sum_{t:B_{h}^{t}=B} \mathcal{B}_{h}^{t}\mathds{1}\{\mathrm{center}(B)\in Z_{h}^{\mathcal{B}_{h}^{ t}}\}\] \[\leq\sum_{r\geq r_{0}}\sum_{B:\mathrm{diam}(B)=r:\rho(H,\delta,L) }\mathds{1}\{\mathrm{center}(B)\in Z_{h}^{\rho(H,\delta,L)\cdot r}\}\sum_{t:B_ {h}^{t}=B}\mathcal{B}_{h}^{t}.\] To upper bound the last sum we repeat the argument of (14) and apply (16), using the fact that \(\mathrm{diam}(B)=r\cdot\rho(H,\delta,L)\) \[\sum_{t:B_{h}^{t}=B}\frac{1}{\sqrt{e_{k}}} \leq\sum_{k=0}^{k_{T}^{T}(B)}\frac{e_{k+1}}{\sqrt{e_{k}}}\leq 4H \sqrt{e^{k_{h}^{T}(B)+1}}\] \[\leq 4\sqrt{H(n_{h}^{T+1}(B)+1)}\leq 4\sqrt{2H}\cdot\frac{d_{ \max}}{\mathrm{diam}(B)}=\frac{\sqrt{32H}\cdot d_{\max}}{r}.\] As a result, we have by (19) \[\sum_{t:B_{h}^{t}=B}\mathcal{B}_{h}^{t}\leq\frac{\sqrt{32H}\cdot d_{\max}}{r} \cdot\Big{(}2522\mathrm{e}^{2}H(\beta^{\max}(\delta,T))^{4}+5Ld_{\max}/\sqrt{ H}\Big{)}\] and \[\sum_{r\geq r_{0}}\sum_{B:\mathrm{diam}(B)=r}\sum_{t:B_{h}^{t}=B} \mathcal{B}_{h}^{t}\mathds{1}\{\mathrm{center}(B)\in Z_{h}^{\mathcal{B}_{h}^{ t}}\}\] \[=\mathcal{O}\Bigg{(}\sum_{r\geq r_{0}}N_{r}(Z_{h}^{\rho(H,\delta,L)\cdot r})\cdot\frac{H^{3/2}d_{\max}(\beta^{\max}(\delta,T))^{4}+Ld_{\max}^{2 }}{r}\Bigg{)}.\] Finally, by an arbitrary choice of \(r_{0}\) and a definition of zooming dimension with a scaling \(\rho=\rho(H,\delta,L)\) (Definition 4) \[(\mathbf{B})=\mathcal{O}\Bigg{(}(H^{3/2}d_{\max}(\beta^{\max}(\delta,T))^{4}+ Ld_{\max}^{2})\cdot\sum_{h=1}^{H}\inf_{\bar{r}_{0}}\Bigg{\{}Tr_{0}+\sum_{r\geq r _{0}}\frac{C_{N,h}}{\bar{r}^{d_{s,h}+1}}\Bigg{\}}\Bigg{)}.\]Term \((\mathbf{C})\)For this term we just apply definition of the main event \(\mathcal{G}(\delta)\supseteq\mathcal{E}(\delta)\) and obtain

\[(\mathbf{C})=\mathcal{O}\Big{(}\sqrt{H^{3}T\beta^{\max}(\delta,T)}\Big{)}.\]

Final regret boundFirst, we notice that \(\beta^{\max(\delta,T)}=\widetilde{\mathcal{O}}(d_{c})\), therefore we have

\[\mathfrak{R}^{T}=\widetilde{\mathcal{O}}\Bigg{(}H^{3}d_{c}+(H^{3/2}d_{c}^{4}+L )\sum_{h=1}^{H}\inf_{r_{0}>0}\Bigg{\{}Tr_{0}+\sum_{r\geq r_{0}}\frac{C_{N,h}}{ r^{d_{s,h}+1}}\Bigg{\}}+\sqrt{H^{3}Td_{c}}\Bigg{)}.\]

Taking \(r_{0}=K^{-d_{s,h}+1/2}\) for each \(h\) and summing the geometric series we conclude the statement.

Deviation and Anti-Concentration Inequalities

### Deviation inequality for \(\mathcal{K}_{\text{inf}}\)

For a measure \(\nu\in\mathcal{P}([0,b])\) supported on a segment \([0,b]\) (equipped with a Borel \(\sigma\)-algebra) and a number \(\mu\in[0,b]\) we recall the definition of the minimum Kullback-Leibler divergence

\[\mathcal{K}_{\text{inf}}(\nu,\mu)\triangleq\inf\{\mathrm{KL}(\nu,\eta):\eta\in \mathcal{P}([0,b]),\nu\ll\eta,\mathbb{E}_{X\sim\eta}[X]\geq\mu\}\,.\]

As the Kullback-Leibler divergence this quantity admits a variational formula.

**Lemma 9** (Lemma 18 by Garivier et al., 2018).: _For all \(\nu\in\mathcal{P}([0,b])\), \(u\in[0,b)\),_

\[\mathcal{K}_{\text{inf}}(\nu,u)=\max_{\lambda\in[0,1]}\mathbb{E}_{X\sim\nu} \bigg{[}\log\bigg{(}1-\lambda\frac{X-u}{b-u}\bigg{)}\bigg{]}\,,\]

_moreover if we denote by \(\lambda^{\star}\) the value at which the above maximum is reached, then_

\[\mathbb{E}_{X\sim\nu}\bigg{[}\frac{1}{1-\lambda^{\star}\frac{X-u}{b-u}}\bigg{]} \leq 1\,.\]

**Remark 2**.: Contrary to Garivier et al. (2018) we allow that \(u=0\) but in this case Lemma 9 is trivially true, indeed

\[\mathcal{K}_{\text{inf}}(\nu,0)=0=\max_{\lambda\in[0,1]}\mathbb{E}_{X\sim\nu} \bigg{[}\log\bigg{(}1-\lambda\frac{X}{b}\bigg{)}\bigg{]}\,.\]

Let \((X_{t})_{t\in\mathbb{N}}\). be i.i.d. samples from a measure \(\nu\) supported on \([0,b]\). We denote by \(\widehat{\nu}_{n}\in\mathcal{P}([0,b])\) the empirical measure \(\widehat{\nu}_{n}=\sum_{i=1}^{n}\delta_{X_{i}}\), where \(\delta_{X_{i}}\) is a Dirac measure on \(X_{i}\in[0,b]\).

We are now ready to state the deviation inequality for the \(\mathcal{K}_{\text{inf}}\) by Tiapkin et al. (2022) which is a self-normalized version of Proposition 13 by Garivier et al. (2018). Notice that this inequality is stated in terms of slightly less general definition of \(\mathcal{K}_{\text{inf}}\), however, the proof remains completely the same.

**Theorem 4**.: _For all \(\nu\in\mathcal{P}([0,b])\) and for all \(\delta\in[0,1]\),_

\[\mathbb{P}\big{(}\exists n\in\mathbb{N}^{\star},\,n\,\mathcal{K}_{\text{inf}}( \widehat{\nu}_{n},\mathbb{E}_{X\sim\nu}[X])>\log(1/\delta)+3\log(e\pi(1+2n)) \big{)}\leq\delta.\]

### Anti-concentration Inequality for Dirichlet Weighted Sums

In this section we state anti-concentration inequality by Tiapkin et al. (2022) in terms of slightly different definition of \(\mathcal{K}_{\text{inf}}\).

\[c_{0}(\varepsilon)=\Bigg{(}\frac{4}{\sqrt{\log(17/16)}}+8+\frac{49\cdot 4 \sqrt{6}}{9}\Bigg{)}^{2}\frac{2}{\pi\cdot\varepsilon^{2}}+\log_{17/16}\bigg{(} \frac{5}{32\cdot\varepsilon^{2}}\bigg{)}. \tag{24}\]

**Theorem 5** (Lower bound).: _For any \(\alpha=(\alpha_{0}+1,\alpha_{1},\ldots,\alpha_{m})\in\mathbb{R}_{++}^{m+1}\) define \(\overline{p}\in\Delta_{m}\) such that \(\overline{p}(\ell)=\alpha_{\ell}/\overline{\alpha},\ell=0,\ldots,m\), where \(\overline{\alpha}=\sum_{j=0}^{m}\alpha_{j}\). Let \(\varepsilon\in(0,1)\). Assume that \(\alpha_{0}\geq c_{0}(\varepsilon)+\log_{17/16}(\overline{\alpha})\) for \(c_{0}(\varepsilon)\) defined in (24), and \(\overline{\alpha}\geq 2\alpha_{0}\). Then for any \(f\colon\{0,\ldots,m\}\to[0,b_{0}]\) such that \(f(0)=b_{0}\), \(f(j)\leq b<b_{0}/2,j\in\{1,\ldots,m\}\) and \(\mu\in(\overline{p}f,b_{0})\)_

\[\mathbb{P}_{w\sim\mathrm{Dir}(\alpha)}[wf\geq\mu]\geq(1-\varepsilon)\mathbb{P }_{g\sim\mathcal{N}(0,1)}\Bigg{[}g\geq\sqrt{2\overline{\alpha}\,\mathcal{K}_ {\text{inf}}\bigg{(}\sum_{i=0}^{m}\overline{p}(i)\cdot\delta_{f(i)},\mu\bigg{)} }\Bigg{]}.\]

Next we formulate a simple corollary of Theorem 5, that slightly relaxes assumptions of this theorem under assumption \(\mu<b\leq b_{0}/2\).

**Lemma 10**.: _For any \(\alpha=(\alpha_{0}+1,\alpha_{1},\ldots,\alpha_{m})\in\mathbb{R}^{m+1}_{++}\) define \(\overline{p}\in\Delta_{m}\) such that \(\overline{p}(\ell)=\alpha_{\ell}/\overline{\alpha},\ell=0,\ldots,m\), where \(\overline{\alpha}=\sum_{j=0}^{m}\alpha_{j}\). Also define a measure \(\bar{\nu}=\sum_{i=0}^{m}\overline{p}(i)\cdot\delta_{f(i)}\)._

_Let \(\varepsilon\in(0,1)\). Assume that \(\alpha_{0}\geq c_{0}(\varepsilon)+\log_{17/16}(2(\overline{\alpha}-\alpha_{0}))\) for \(c_{0}(\varepsilon)\) defined in (24). The for any \(f\colon\{0,\ldots,m\}\to[0,b_{0}]\) such that \(f(0)=b_{0},f(j)\leq b\leq b_{0}/2,j\in[m]\), and any \(\mu\in(0,b)\)_

\[\mathbb{P}_{w\sim\operatorname{Dir}(\alpha)}[wf\geq\mu]\geq(1-\varepsilon) \mathbb{P}_{g\sim\mathcal{N}(0,1)}\bigg{[}g\geq\sqrt{2\overline{\alpha}\, \mathcal{K}_{\text{inf}}(\bar{\nu},\mu)}\bigg{]}.\]

Proof.: Assume that assumption \(\overline{\alpha}\geq 2\alpha_{0}\) holds.

Then we show that the Theorem 5 also holds for \(\mu\leq\overline{p}f\). First, we notice that for any \(\gamma>0\)

\[\mathbb{P}_{w\sim\operatorname{Dir}(\alpha)}[wf\geq\mu]\geq\mathbb{P}_{w\sim \operatorname{Dir}(\alpha)}[wf\geq\overline{p}f+\gamma]\geq(1-\varepsilon) \mathbb{P}_{g\sim\mathcal{N}(0,1)}\Big{[}g\geq\sqrt{2\overline{\alpha}\, \mathcal{K}_{\text{inf}}(\bar{\nu},\overline{p}f+\gamma)}\Big{]}.\]

By continuity of \(\mathcal{K}_{\text{inf}}\) in its second argument (see Theorem 7 by Honda and Takemura (2010)) we can tend \(\gamma\) to zero, and then use an equality \(\mathcal{K}_{\text{inf}}(\bar{\nu},\overline{p}f)=\mathcal{K}_{\text{inf}}( \bar{\nu},\mu)=0\).

Next, assume \(\overline{\alpha}\leq 2\alpha_{0}\). In this case we have \(\overline{p}f\geq b\), thus for any \(0\leq\mu\leq b\)

\[\mathbb{P}_{w\sim\operatorname{Dir}(\alpha)}[wf\geq\mu]\geq\mathbb{P}_{\xi \sim\operatorname{Beta}(\alpha_{0}+1,\overline{\alpha}-\alpha_{0})}[b_{0}\xi \geq\mu]\geq\mathbb{P}_{\xi\sim\operatorname{Beta}(\alpha_{0}+1,\overline{ \alpha}-\alpha_{0})}\bigg{[}\xi\geq\frac{1}{2}\bigg{]},\]

where we first apply a lower bound \(f(j)\geq 0\) for all \(j>0\) and \(f(0)=b_{0}\), and second apply a bound \(\mu\leq b_{0}/2\). Here we may apply the result of Alferes and Dinges (1984, Theorem 1.2") and obtain the following lower bound

\[\mathbb{P}_{w\sim\operatorname{Dir}(\alpha)}[wf\geq\mu]\geq\Phi\Big{(}- \operatorname{sign}(\alpha_{0}/\overline{\alpha}-1/2)\cdot\sqrt{2\overline{ \alpha}\operatorname{kl}(\alpha_{0}/\overline{\alpha},1/2)}\Big{)}\geq(1- \varepsilon)\mathbb{P}_{g\sim\mathcal{N}(0,1)}[g\geq 0]\]

where we used \(\alpha_{0}/\overline{\alpha}>1/2\). 

### Rosenthal-type inequality

In this section we state Rosenthal-type inequality for martingale differences by (Pinelis, 1994, Theorem 4.1). The exact constants could be derived from the proof.

**Theorem 6**.: _Let \(X_{1},\ldots,X_{n}\) be a martingale-difference sequence adapted to a filtration \(\{\mathcal{F}_{i}\}_{i=1,\ldots,n}\): \(\mathbb{E}[X_{i}|\mathcal{F}_{i}]=0\). Define \(\mathcal{V}_{i}=\mathbb{E}[X_{i}^{2}|\mathcal{F}_{i-1}]\). Then for any \(p\geq 2\) the following holds_

\[\mathbb{E}^{1/p}\Bigg{[}\Big{|}\sum_{i=1}^{n}X_{i}\Big{|}^{p}\Bigg{]}\leq C_{1} p^{1/2}\mathbb{E}^{1/p}\Bigg{[}\Big{|}\sum_{i=1}^{n}\mathcal{V}_{i}\Big{|}^{p/2} \Bigg{]}+2C_{2}p\mathbb{E}^{1/p}\Big{[}\underset{i\in[n]}{\max}|X_{i}|^{p} \Bigg{]},\]

_where \(C_{1}=60\mathrm{e},C_{2}=60\)._

Additionally, we need some additional lemma to use this inequality in our setting.

**Definition 7**.: A random variable \(X\) is called _sub-exponential_ with parameters \((\sigma^{2},b)\) if the following tail condition holds for any \(t>0\)

\[\mathbb{P}[|X-\mathbb{E}[X]|\geq t]\leq 2\exp\biggl{(}-\frac{t^{2}}{2\sigma^{2}+2 bt}\biggr{)}.\]

By Theorem 1 of Skorski (2023) we have for any \(\xi\in B(\alpha,\beta)\) with \(\beta\geq\alpha\) and any \(t>0\)

\[\mathbb{P}[|\xi-\mathbb{E}[\xi]|\geq t]\leq 2\exp\biggl{(}-\frac{t^{2}}{2(v+ct/3)} \biggr{)},\]

where

\[v=\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}\leq\frac{\alpha}{( \alpha+\beta)^{2}},\quad c=\frac{2(\beta-\alpha)}{(\alpha+\beta)(\alpha+ \beta+2)}\leq\frac{2}{\alpha+\beta},\]

so \(\xi\) is \((\alpha/(\alpha+\beta)^{2},2/(3(\alpha+\beta)))\) sub-exponential.

**Lemma 11**.: _Let \(X_{1},\ldots,X_{n}\) be a sequence of centred \((\sigma^{2},b)\) sub-exponential random variables, not necessarily independent. Then for any \(p\geq 2\)_

\[\mathbb{E}\biggl{[}\max_{\ell\in[n]}\lvert X_{\ell}\rvert^{p}\biggr{]}\leq\max \{\sqrt{8\sigma^{2}\log n},8b\log n\}^{p}+\mathrm{e}(2\sigma)^{p}p^{p/2}+2 \mathrm{e}(8b)^{p}p^{p}.\]

Proof.: By Fubini theorem we have for any \(\eta\geq 0\): \(\mathbb{E}[\eta^{p}]=p\int_{0}^{\infty}u^{p-1}\mathbb{P}[\eta\geq u]\mathrm{d}u\), thus for any \(a>0\) the following holds

\[\mathbb{E}\biggl{[}\max_{\ell\in[n]}\lvert X_{\ell}\rvert^{p}\biggr{]} =p\int_{0}^{\infty}u^{p-1}\mathbb{P}\biggl{[}\max_{\ell\in[n]} \lvert X_{\ell}-\mathbb{E}[X_{\ell}]\rvert\geq u\biggr{]}\mathrm{d}u\] \[\leq a^{p}+p\int_{a}^{\infty}u^{p-1}\mathbb{P}[\exists\ell\in[n]: \lvert X_{\ell}\rvert\geq u]\mathrm{d}u\] \[\leq a^{p}+2p\int_{a}^{\infty}u^{p-1}n\exp\biggl{(}-\frac{u^{2}}{ 2(\sigma^{2}+bu)}\biggr{)}\mathrm{d}u.\]

By selecting \(a=\max\{\sqrt{8\sigma^{2}\log n},8b\log n\}\) we have

\[n\exp\biggl{(}-\frac{u^{2}}{2(\sigma^{2}+bu)}\biggr{)}\leq\exp \biggl{(}-\frac{u^{2}}{4(\sigma^{2}+bu)}\biggr{)}\leq\exp\biggl{(}-\frac{u^{2} }{8\sigma^{2}}\biggr{)}+\exp\biggl{(}-\frac{u}{8b}\biggr{)}\]

for any \(u\geq a\), thus

\[\mathbb{E}\biggl{[}\max_{\ell\in[n]}\lvert X_{\ell}\rvert^{p} \biggr{]} \leq\max\{\sqrt{8\sigma^{2}\log n},8b\log n\}^{p}\] \[+2p\int_{a}^{\infty}u^{p-1}\exp\biggl{(}-\frac{u^{2}}{8\sigma^{2} }\biggr{)}\mathrm{d}u+2p\int_{a}^{\infty}u^{p-1}\exp\biggl{(}-\frac{u}{8b} \biggr{)}\mathrm{d}u\] \[\leq\max\{\sqrt{8\sigma^{2}\log n},8b\log n\}^{p}+p(2\sqrt{2} \sigma)^{p}\Gamma(p/2)+2p(8b)^{p}\Gamma(p).\]

By the bounds on Gamma-function we have

\[p\Gamma(p/2)=\Gamma(p/2+1)\leq(p+1)^{(p+1)/2}2^{-(p+1)/2}\mathrm{e}^{1-p/2} \leq\mathrm{e}p^{p/2}2^{-p/2}\]

and \(p\Gamma(p)=\Gamma(p+1)\leq(p+1/2)^{p+1/2}\mathrm{e}^{1-p}\leq\mathrm{e}p^{p}\) (see Guo et al. [2007]), thus

\[\mathbb{E}\biggl{[}\max_{\ell\in[n]}\lvert X_{\ell}\rvert^{p}\biggr{]}\leq\max \{\sqrt{8\sigma^{2}\log n},8b\log n\}^{p}+\mathrm{e}(2\sigma)^{p}p^{p/2}+2 \mathrm{e}(8b)^{p}p^{p}.\]

**Proposition 7**.: Let \(W_{1},\ldots,W_{n}\) be a sequence of Beta-distributed random variables \(W_{i}\sim\mathrm{Beta}(1/\kappa,(n-1)/\kappa)\) for \(\kappa>0\). Let \(\{\mathcal{F}_{i}\}_{i\in[n]}\) be a filtration such that \(W_{i}\) is independent from \(\mathcal{F}_{i-1}:\mathbb{E}[W_{i}\lvert\mathcal{F}_{i-1}]=\mathbb{E}[W_{i}]\), and \(X_{1},\ldots,X_{n}\) be a sequence of bounded predictable random variables: \(\mathbb{E}[X_{i}\lvert\mathcal{F}_{i-1}\rvert=X_{i},\lvert X_{i}\rvert\leq B\).

Then with probability at least \(1-\delta\) the following holds

\[\left\lvert\sum_{i=1}^{n}W_{i}X_{i}-\frac{1}{n}\sum_{i=1}^{n}X_{i}\right\rvert \leq 60\mathrm{e}^{2}B\sqrt{\frac{\kappa\log(1/\delta)}{n}}+1200\mathrm{e}B \frac{\kappa\log(n)\log^{2}(1/\delta)}{n}\]

Proof.: First we notice that \(Z_{i}=(W_{i}-\mathbb{E}[W_{i}])\cdot X_{i}\) forms a martingale-difference sequence: \(\mathbb{E}[Z_{i}\lvert\mathcal{F}_{i-1}\rvert=0\). Therefore, we can apply Theorem 6

\[\mathbb{E}^{1/p}\Biggl{[}\left\lvert\sum_{i=1}^{n}Z_{i}\right\rvert^{p}\Biggr{]} \leq 60\mathrm{e}\sqrt{p}\cdot\mathbb{E}^{1/p}\Biggl{[}\left\lvert\sum_{i=1}^{n} \mathcal{V}_{i}\right\rvert^{p/2}\Biggr{]}+120p\cdot\mathbb{E}^{1/p}\biggl{[} \max_{i\in[n]}\lvert Z_{i}\rvert^{p}\biggr{]},\]

where \(\mathcal{V}_{i}=\mathbb{E}[Z_{i}^{2}\lvert\mathcal{F}_{i-1}\rvert=X_{i}^{2} \mathrm{Var}(W_{i})\). We can easily upper bound the variance of Beta-distributed random variable and obtain

\[\mathbb{E}^{1/p}\Biggl{[}\left\lvert\sum_{i=1}^{n}\mathcal{V}_{i}\right\rvert^{ p/2}\Biggr{]}\leq\mathbb{E}^{1/p}\Biggl{[}\left\lvert\sum_{i=1}^{n}\frac{\kappa X_{i}^{2}}{n^{2}} \right\rvert^{p/2}\Biggr{]}\leq\sqrt{\frac{\kappa B^{2}}{n}}.\]For the second term we apply Lemma 11 since \(W_{i}\) are \((\kappa/n^{2},2\kappa/(3n))\)-sub-exponential

\[\mathbb{E}^{1/p}\Big{[}\max_{i\in[n]}|Z_{i}|^{p}\Big{]} \leq B\Bigg{(}\max\Bigg{\{}\sqrt{\frac{8\kappa\log n}{n^{2}}},\frac {16\kappa\log n}{3n}\Bigg{\}}+\mathrm{e}^{1/p}\sqrt{\frac{\kappa}{n^{2}}}\sqrt{ p}+(2\mathrm{e})^{1/p}\frac{16\kappa}{3n}\cdot p\Bigg{)}\] \[\leq 20B\kappa\cdot p\cdot\frac{\log n}{n}.\]

Therefore we have

\[\mathbb{E}^{1/p}\Bigg{[}\Big{|}\sum_{i=1}^{n}Z_{i}\Big{|}^{p}\Bigg{]}\leq 60 \mathrm{e}\cdot p^{1/2}\sqrt{\frac{\kappa B^{2}}{n}}+1200\cdot p^{2}\frac{B \kappa\cdot\log n}{n}.\]

Next we turn from moments to tails. By Markov inequality with \(p=\log(1/\delta)\)

\[\mathbb{P}\Bigg{[}\Bigg{|}\sum_{i=1}^{n}Z_{i}\Bigg{|}\geq t \Bigg{]} \leq\Bigg{(}\frac{\mathbb{E}^{1/p}\big{[}\sum_{i=1}^{n}Z_{i}|^{p }\big{]}}{t}\Bigg{)}^{p}\] \[\leq\Bigg{(}\frac{60\mathrm{e}B\sqrt{\kappa\log(1/\delta)/n}+1200 \log^{2}(1/\delta)B\kappa\log(n)/n}{t}\Bigg{)}^{\log(1/\delta)}.\]

Taking \(t=60\mathrm{e}^{2}B\sqrt{\frac{\kappa\log(1/\delta)}{n}}+1200\mathrm{e}B\frac{ \kappa\log(n)\log^{2}(1/\delta)}{n}\) we conclude the statement. 

## Appendix H Technical Lemmas

**Lemma 12**.: _Let \(\nu\in\mathcal{P}([0,b])\) be a probability measure over the segment \([0,b]\) and let \(\bar{\nu}=(1-\alpha)\delta_{b_{0}}+\alpha\cdot\nu\) be a mixture between \(\nu\) and a Dirac measure on \(b_{0}>b\). Then for any \(\mu\in(0,b)\)_

\[\mathcal{K}_{\text{inf}}(\bar{\nu},\mu)\leq(1-\alpha)\,\mathcal{K}_{\text{inf }}(\nu,\mu).\]

Proof.: By a variational formula for \(\mathcal{K}_{\text{inf}}\) (see Lemma 9)

\[\mathcal{K}_{\text{inf}}(\bar{\nu},\mu)=\max_{\lambda\in[0,1/(b_{0}-\mu)]} \mathbb{E}_{X\sim\bar{\nu}}[\log(1-\lambda(X-\mu))].\]

Since \(\bar{\nu}\) is a mixture, we have for any \(\lambda\in[0,1/(b_{0}-\mu)]\)

\[\mathbb{E}_{X\sim\bar{\nu}}[\log(1-\lambda(X-\mu))]=(1-\alpha)\mathbb{E}_{X \sim\bar{\nu}}[\log(1-\lambda(X-\mu))]+\alpha\log(1-\lambda(b_{0}-\mu)).\]

Notice that \(\max_{\lambda>0}\log(1-\lambda(b_{0}-\mu))=0\). Thus, maximizing each term separately over \(\lambda\), we have

\[\mathcal{K}_{\text{inf}}(\bar{\nu},\mu) \leq(1-\alpha)\max_{\lambda\in[0,1/(b_{0}-\mu)]}\mathbb{E}_{X \sim\bar{\nu}}[\log(1-\lambda(X-\mu))]\] \[\leq(1-\alpha)\max_{\lambda\in[0,1/(b-\mu)]}\mathbb{E}_{X\sim \bar{\nu}}[\log(1-\lambda(X-\mu))]=(1-\alpha)\,\mathcal{K}_{\text{inf}}(\nu, \mu).\]

## Appendix I Experimental details

In this section we detail the experiments we conducted for tabular and non-tabular environments. For all experiments we used 2 CPUs (Intel Xeon CPU \(2.20\)GHz), and no GPU was used. Each experiment took approximately one hour.

### Tabular experiments

In our initial experiment, we investigated a simple grid-world environment.

#### Environments

For tabular experiments we use two environments.

The first one is a grid-world environment with \(100\) states \((i,j)\in[10]\times[10]\) and \(4\) actions (left, right, up and down). The horizon is set to \(H=50\). When taking an action, the agent moves in the corresponding direction with probability \(1-\varepsilon\), and moves to a neighbor state at random with probability \(\varepsilon=0.2\). The agent starts at position \((1,1)\). The reward equals to \(1\) at the state \((10,10)\) and is zero elsewhere.

The second one is a chain environment described by Osband et al. (2016) with \(L=15\) states and \(2\) actions (left or right). The horizon is equal to \(30\), the probability of moving into wrong direction is equal to \(0.1\). The agent starts in the leftmost state with reward \(0.05\), also the largest reward is equal to \(1\) is the rightmost state.

#### Variations of randomized Q-learning

First we compare the different variations of randomized Q-learning on grid-world environment. Precisely we consider:

* RandQL a randomized version of OptQL, detailed in Appendix B.
* Staged-RandQL a staged version of RandQL, described in Section 3.2.
* Sampled-RandQL a version of RandQL which samples one Q-value function in the ensemble to act, described in Appendix B.

For these algorithms we used the same parameters: posterior inflation \(\kappa=1.0\), \(n_{0}=1/S\) prior sample (same as PSRL, see below), ensemble size \(J=10\). We use a similar ensemble size as the one used for the experiments with OPSRL by Tiapkin et al. (2022). For Staged-RandQL we use stage of sizes \(\big{(}(1+1/H)^{k}\big{)}_{k\geq 1}\) without the \(H\) factor, in order to have several epochs per state-action pair even for few episodes.

The comparison is presented in Figure 2. We observe that RandQL and Sampled-RandQL behave similarly with slightly better performance for Sampled-RandQL. This is coherent with the experiment on the comparison between OPSRL and PSRL Tiapkin et al. (2022) where the optimistic version performs worst than the fully randomized algorithm. We also note that even with the aggressive stage schedule, Staged-RandQL needs more episode to converge. We conclude that despite that stage simplifies the analysis, it artificially slows down the learning in practice.

To ease the comparison with the baselines, for the rest of the experiments we only use RandQL because of its similarity with OptQL.

#### Baselines

We compare RandQL algorithm to the following baselines:

* OptQL (Jin et al., 2018) a model-free optimistic Q-learning.

Figure 2: Regret curves of RandQL, Staged-RandQL and Sampled-RandQL on a grid-world environment with \(100\) states and \(4\) actions for \(H=50\) an transitions noise \(0.2\). We show average over 4 seeds.

* UCBVI [Azar et al., 2017] a model-based optimistic dynamic programming.
* Greedy-UCBVI [Efroni et al., 2019] optimistic real-time dynamic programming.
* PSRL [Osband et al., 2013] model-based posterior sampling.
* RLSVI [Russo, 2019] model-based randomized dynamic programming.

The selection of parameters can have a significant impact on the empirical regrets of an algorithm. For example, adjusting the multiplicative constants in the bonus of UCBVI or the scale of the noise in RLSVI can result in vastly different regrets. To ensure a fair comparison between algorithms, we have made the following parameter choices:

* For bonus-based algorithm, UCBVI, OptQL we use simplified bonuses from an idealized Hoeffding inequality of the form \[\beta_{h}^{t}(s,a)\triangleq\min\!\left(\sqrt{\frac{1}{n_{h}^{t}(s,a)}}+\frac {H-h+1}{n_{h}^{t}(s,a)},H-h+1\right).\] (25) As explained by Menard et al. [2021], this bonus does not necessarily result in a true upper-confidence bound on the optimal Q-value. However, it is a valid upper-confidence bound for \(n_{h}^{t}(s,a)=0\) which is important in order to discover new state-action pairs.
* For RLSVI we use the variance of Gaussian noise equal to simplified Hoeffding bonuses described above in (25).
* For PSRL, we use a Dirichlet prior on the transition probability distribution with parameter \((1/S,\ldots,1/S)\) and for the rewards a Beta prior with parameter \((1,1)\). Note that since the reward \(r\) is not necessarily in \(\{0,1\}\) we just sample a new randomized reward \(r^{\prime}\sim\mathrm{Ber}(r)\) accordingly to a Bernoulli distribution of parameter \(r\), to update the posterior, see Agrawal and Goyal [2013].

ResultsFigure 3 shows the result of the experiments. Overall, we see that RandQL outperforms OptQL algorithm on tabular environment, but still degrades in comparison to model-based approaches, that is usual for model-free algorithms in tabular environments. Indeed, as explained by Menard et al. [2021], using a model and backward induction allows new information to be more quickly propagated. For example UCBVI needs only one episode to propagate information about the last step \(h=H\) to the first step \(h=1\) whereas OptQL or RandQL need at least \(H\) episodes. But as counterpart, RandQL has a better time-complexity and space- complexity than model based algorithm, see Table 2.

### Non-tabular experiments

The second experiment was performed on a set of two dimensional continuous environments [Domingues et al., 2021a] with levels of increasing exploration difficulty.

Figure 3: Regret curves of RandQL and several baselines in (left) a grid-world environment with \(100\) states and \(4\) actions for \(H=50\) an transitions noise \(0.2\), and (right) in a chain environment of length \(L=15\), \(2\) actions for \(H=30\) with transition noise \(0.1\): smaller is better. We show average and error bars over 4 seeds.

EnvironmentWe use a ball environment with the 2-dimensional unit Euclidean ball as state-space \(\mathcal{S}=\{s\in\mathbb{R}^{2},\|s\|_{2}\leq 1\}\) and of horizon \(H=30\). The action space is a list of 2-dimensional vectors \(\mathcal{A}=\{[0.0,0.0],[-0.05,0.0],[0.05,0.0],[0.0,0.05],[0.0,-0.05]\}\) that can be associated with the action of staying at the same place, moving left, right, up or down. Given a state \(s_{h}\) and an action \(a_{h}\) the next state is

\[s_{h+1}=\mathrm{proj}_{\mathcal{S}}(s_{h}+a_{h}+\sigma z_{h})\]

where \(z_{h}\sim\mathcal{N}([0,0],I_{2})\) is some independent Gaussian noise with zero mean and identity covariance matrix and \(\mathrm{proj}_{B}\) is the euclidean projection on the unit ball \(\mathcal{S}\). The initial position \(s_{1}=\sigma_{1}z_{1}\) with \(z_{1}\sim\mathcal{N}([0,0],I_{2})\) and \(\sigma_{1}=0.001\), is sampled at random from a Gaussian distribution. The reward function independent of the action and the step

\[r_{h}(s,a)=\max(0,1-\|s-s^{\prime}\|/c)\]

where \(s^{\prime}=[0.5,0.5]\in\mathcal{S}\) is the reward center and \(c>0\) is some smoothness parameter. We distinguish \(3\) levels by increasing exploration difficulty:

* Level \(1\), dense reward and small noise. The smoothness parameter is \(c=0.5\cdot\sqrt{2}\approx 0.71\) and the transition standard deviation is \(\sigma=0.01\).
* Level \(2\), sparse reward and small noise. The smoothness parameter is \(c=0.2\) and the transition standard deviation is \(\sigma=0.01\).
* Level \(3\), sparse reward and large noise. The smoothness parameter is \(c=0.2\) and the transition standard deviation is \(\sigma=0.025\).

RandQL algorithmAmong the different versions of RandQL for continuous state-action space, see Section 4, we pick the Adaptive-RandQL algorithm, described in Appendix F, as it is the closest version to the Adaptive-QL algorithm. It combines the RandQL algorithm and adaptive discretization. For Adaptive-RandQL we used an ensemble of size \(J=10\approx\log(T)\), \(\kappa=10\approx\log(T)\) and a prior number of samples of \(n_{0}=0.33\). Note that we increased the number of prior samples in comparison to the tabular case as explained in Section 4.

BaselinesWe compare Adaptive-RandQL algorithm to the following baselines:

* Adaptive-QL[Sinclair et al., 2019, 2023], an adaptation of OptQL algorithm to continuous state-space thanks to adaptive discretization;
* Kernel-UCBVI[Domingues et al., 2021c], a kernel-based version of the UCBVI algorithm;
* DQN[Mnih et al., 2013], a deep RL algorithm;
* BootDQN[Osband and Van Roy, 2015], a deep RL algorithm with an additional exploration given by bootstraping several Q-networks;

For Adaptive-QL and Kernel-UCBVI baselines we employ the same simplified bonuses (25) used for the tabular experiments. For Kernel-UCBVI we used Gaussian kernel of bandwidth \(0.025\) and the representative states technique, with \(300\) representative states, described by Domingues et al. [2021c].

For DQN and BootDQN we use as network a 2-layer multilayer perceptron (MLP) with hidden layer size equals to \(64\). For exploration, DQN utilizes \(\varepsilon\)-greedy exploration with coefficient annealing from \(1.0\) to \(0.1\) during the first \(10,000\) steps. For BootDQN we use ensemble of \(10\) heads and do not use \(\varepsilon\)-greedy exploration.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Algorithm** & **Time-complexity (per episode)** & **Space complexity** \\ \hline UCBVI[Avar et al., 2017] & \(\widetilde{\mathcal{O}}(HS^{2}A)\) & \(\widetilde{\mathcal{O}}(HS^{2}A)\) \\ RLSVI[Russo, 2019] & & \\ Greedy-UCBVI[Efoui et al., 2019] & \(\widetilde{\mathcal{O}}(HSA)\) & \\ OptQL[Jin et al., 2018] & \(\widetilde{\mathcal{O}}(H)\) & \(\widetilde{\mathcal{O}}(HSA)\) \\ \hline RandQL (this paper) & \(\widetilde{\mathcal{O}}(H)\) & \(\widetilde{\mathcal{O}}(HSA)\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Time- and space- complexity of several tabular algorithms.

ResultsFigure 4 shows the results of non-tabular experiments. Overall, we see that Adaptive-RandQL outperforms Adaptive-QL in all environments, especially in the sparse reward setting. However, we see that model-based algorithm is much more sample efficient than model-free algorithm, as it was shown by Domingues et al. (2021). This is connected to low dimension of the presented environment, where the difference in theoretical regret bounds is not so large. However, this performance come at the price of 3-times larger time complexity, see Table 3.

Regarding the comparison to neural-network based algorithms, we see that approaches based on adaptive discretization always outperforms DQN and BootDQN on an environment with non-sparse rewards. We connect this phenomenon to the fact that neural network algorithms are solving two problems at the same time: exploration and optimization, whereas discretization-based approaches solve only exploration problem.

In the setup of sparse rewards it turns out that neural network-based approaches are competitive with Adaptive-QL and Adaptive-RandQL. Notably, DQN shows itself as the worst one, whereas Adaptive-RandQL and BootDQN show similar performance, additionally justifying exploration effect of ensemble learning and randomized exploration.

Figure 4: Cumulative rewards (higher is better) of Adaptive-RandQL and several baselines in ball environments with increasing exploration difficulty: Upper Left displays Level 1, Upper Right shows Level 2, Down shows Level 3. We show average and error bars over 4 seeds.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Algorithm** & **Episode time (second)** \\ \hline Adaptive-RandQL & \(5.780e^{-02}\) \\ Adaptive-QL & \(4.213e^{-02}\) \\ Kernel-UCBVI & \(1.523e^{-01}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average time of one episode in second (averaged over \(20000\) episodes).