# D-CIPHER: Discovery of Closed-form Partial Differential Equations

 Krzysztof Kacprzyk

University of Cambridge

kk751@cam.ac.uk

&Zhaozhi Qian

University of Cambridge

zq224@cam.ac.uk

&Mihaela van der Schaar

University of Cambridge,

The Alan Turing Institute

mv472@cam.ac.uk

###### Abstract

Closed-form differential equations, including partial differential equations and higher-order ordinary differential equations, are one of the most important tools used by scientists to model and better understand natural phenomena. Discovering these equations directly from data is challenging because it requires modeling relationships between various derivatives that are not observed in the data (_equation-data mismatch_) and it involves searching across a huge space of possible equations. Current approaches make strong assumptions about the form of the equation and thus fail to discover many well-known phenomena. Moreover, many of them resolve the equation-data mismatch by estimating the derivatives, which makes them inadequate for noisy and infrequent observations. To this end, we propose D-CIPHER, which is robust to measurement artifacts and can uncover a new and very general class of differential equations. We further design a novel optimization procedure, CoLLie, to help D-CIPHER search through this class efficiently. Finally, we demonstrate empirically that it can discover many well-known equations that are beyond the capabilities of current methods.

## 1 Introduction

Scientists have been using mathematical equations to describe the world for centuries. In particular, closed-form differential equations turned out to be one of the best tools to model physical phenomena. A differential equation describes a relationship between a quantity and its derivatives (rates of change); it is called closed-form if this relationship is described by a mathematical expression consisting of a finite number of variables, constants, arithmetic operations, and some well-known functions (e.g., exponent, logarithm, trigonometric functions)1. Closed-form differential equations provide a general description of reality in a concise representation that is amenable to closer inspection by scientists. This renders them transparent and interpretable to human experts.

Footnote 1: Detailed discussion in Appendix A.2

Discoveries of these equations required a thorough knowledge of the theory, strong mathematical skills, substantial creativity, and good intuition. The goal of this work is to discover closed-form differential equations directly from data thus accelerating the process of scientific discovery.

**Challenges in discovering differential equations from data**

* **Partial and higher-order derivatives.** Many algorithms [10, 42] can only identify Ordinary Differential Equations (ODEs) which evolve only with respect to one variable (usually time). In contrast, many natural phenomena are described by equations involving many variables (e.g., spatial coordinates) called Partial Differential Equations (PDEs). Many equations also involve higher-order derivatives.

* **Derivatives not observed.** Discovering differential equations from data is challenging because the derivatives are usually not observed in the dataset (_equation-data mismatch_[42]). This makes verifying a candidate equation a non-trivial task. Most of the methods try to resolve this issue by estimating the derivatives [10; 47]. However, derivative estimation is difficult, especially when the data is sampled infrequently or with high noise [42; 35]. For an illustrative study, see Appendix F.
* **Strong assumptions and constrained search space.** The majority of algorithms for identifying differential equations make many assumptions about the form of the equation. In particular, they make the _evolution assumption_ (defined and explained later) and assume that the equation can be represented as a linear combination of prespecified functions and differential operators [10; 35]. However, many well-known equations, such as a forced harmonic oscillator or an inhomogeneous wave equation, cannot be represented in that way.

Currently, a few algorithms tackle only some of these challenges. In particular, Weak SINDy [35] is able to discover PDEs without estimating the derivative by utilizing a variational approach. However, the form of the equation is constrained to be in a form amenable for a sparse regression algorithm. D-CODE [42], on the other hand, uses a variational approach in conjunction with a symbolic regression algorithm to discover closed-form ODEs. However, it cannot handle higher-order derivatives or multiple independent variables, so it cannot be used to discover closed-form PDEs. The algorithms that do not require the evolution assumption appeared in [33] and [23] but they require derivative estimation and only consider equations represented as linear combinations of prespecified functions.

**Contributions.** In this work, we develop the **D**iscovery of **C**losed-form **P**artial and **H**igher-order Differential **E**quations in a **R**obust Way framework (D-CIPHER) that does not estimate the derivatives, requires fewer assumptions, has a broader search space than previous techniques, and works for both higher-order ODEs and PDEs. Our contributions are as follows:

* We examine the landscape of different types of PDEs from the discovery perspective. In particular, we introduce new notions such as _evolution form_, _evolution assumption_, _derivative-bound_ part, and _derivative-free_ part. We use them to describe what kinds of PDEs can be discovered with current methods and to motivate our new class of differential equations. (Section 3)
* We propose a new general class of PDEs (_Variational-Ready_ PDEs) that admit the variational formulation (and thus allows to circumvent the derivative estimation). We also prove a theorem that motivates a novel objective function. (Section 5)
* We use the novel objective function to develop D-CIPHER, a new algorithm that searches over the _Variational-Ready_ PDEs. (Section 6)

In addition to the main contributions above, we also develop a new optimization procedure (CoLLie) to help D-CIPHER search through this space efficiently. (Section 7)

## 2 Preliminaries

In this section, we provide background information about PDEs and their variational formulation.

**Notation and definitions.** We denote the set \(\{1,2,\ldots,n\}\) as \([n]\) and the set of non-negative integers as \(\mathbb{N}_{0}\). Throughout this paper we let \(M,N,K\in\mathbb{N}\) be some natural numbers and let \(\Omega\subset\mathbb{R}^{M}\) be an open set inside \(\mathbb{R}^{M}\). A comprehensive table with all symbols used in this work can be found in Appendix A together with some definitions restated more formally.

**Going beyond ODEs.** The simplest differential equations are ordinary differential equations that describe quantities that evolve with respect to only one independent variable, usually time. Most methods assume that the ODE is explicit and can be represented as a system of first-order ODEs:

\[\dot{u}_{j}(t)=f_{j}(t,\bm{u}(t))\] (1)

where \(\dot{u}_{j}\) represents the derivative of \(u_{j}\). Then the discovery problem is reduced to deciding the order of the derivative (usually first or second) and the discovery of \(f_{j}\).

For PDEs, it is not enough to talk about _the_ derivative, as we can take derivatives with respect to different variables. We denote the _mixed derivative_ as \(\partial^{\bm{\alpha}}\), where \(\bm{\alpha}\in\mathbb{N}_{0}^{M}\) is called a multi-index, and define it as \(\partial^{\bm{\alpha}}=\partial_{1}^{\alpha_{1}}\partial_{2}^{\alpha_{2}} \ldots\partial_{M}^{\alpha_{M}}\). Each \(\partial_{i}^{\alpha_{i}}=\partial^{\alpha_{i}}/\partial x_{i}^{\alpha_{i}}\) is a \(\alpha_{i}^{\text{th}}\)-order partial derivative with respect to \(x_{i}\) (the \(i^{\text{th}}\) independent variable)2. We define the order of \(\bm{\alpha}\) as \(|\bm{\alpha}|=\sum_{i=1}^{M}\alpha_{i}\). We call \(\partial^{\bm{\alpha}}\)_non-trivial_ if \(|\bm{\alpha}|>0\).

A PDE of order \(K\) is any equation of the form

\[f(\bm{x},\bm{u}(\bm{x}),\partial^{[K]}\bm{u}(\bm{x}))=0\ \forall\bm{x}\in\Omega\] (2)

where \(\partial^{[K]}\bm{u}\) are all non-trivial mixed derivatives of all \(u_{j}\) (\(j\in[N]\)) up to the \(K^{\text{th}}\) order. We call a PDE _closed-form_ if \(f\) is closed-form.

**Variational formulation** (VF) of PDEs is a way to describe PDEs without referring to their derivatives [19]. It works as follows: we take a differential equation, we multiply it by a special _testing function_, and integrate. Finally, we perform integration by parts to move the derivatives from the dependent variable \(u\) onto the testing functions. E.g., for a homogeneous heat equation,

\[\partial_{t}u-\theta\partial_{x}^{2}u=0\iff\int_{\mathbb{R}^{2}}(\partial_{t} u-\theta\partial_{x}^{2}u)\phi\ dtdx=0\ \forall\phi\iff\int_{\mathbb{R}^{2}}-u\partial_{t}\phi+\theta u\partial_{x}^{ 2}\phi\ dtdx=0\ \forall\phi\] (3)

For more details, see Appendix A and B. By not depending on the derivatives, methods that utilize VF are more robust to noise than their derivative-estimating counterparts [42, 35, 36].

## 3 Relaxing assumptions while staying robust to noise

Below, we introduce the _evolution assumption_ (EA) and the _linear combination_ (LC) assumption made by the current discovery methods.

**Evolution Assumption.** Although there is no generally accepted notion of an explicit PDE (as is the case for ODEs), we define an _evolution form_ of a PDE to be an equation of the form

\[\partial^{\bm{\alpha}}u_{j}(\bm{x})=f(\bm{x},\bm{u}(\bm{x}),\partial^{[K]/ \bm{\alpha}}\bm{u}(\bm{x}))\ \forall\bm{x}\in\Omega\] (4)

where \(\partial^{[K]/\bm{\alpha}}\) is \(\partial^{[K]}\) with \(\partial^{\bm{\alpha}}\) omitted, \(\bm{\alpha}\) is a known multi-index and \(j\in[N]\). Note that if \(M=1\) and \(|\bm{\alpha}|=K\) then Equation 4 becomes exactly the definition of an explicit ODE.

In fact, many algorithms for PDE discovery assume a particular evolution form [35]. We call it an _evolution assumption_ (EA). However, this assumption requires the knowledge of \(\bm{\alpha}\) and \(j\) which might not be trivial. Usually, \(\partial^{\bm{\alpha}}\) is assumed to be the first derivative with respect to time (\(\partial_{t}\)) [47] but it is not the case for many well-known PDEs such as the wave equation or Gauss's law.

**Linear combinations.** Current PDE discovery algorithms [47, 35, 12] consider PDEs that are linear in parameters. That means the PDE can be represented as a linear combination of functions, i.e.,

\[\sum_{p=1}^{P}\theta_{p}f_{p}(\bm{x},\bm{u}(\bm{x}),\partial^{[K]}\bm{u}(\bm {x}))=0\ \forall\bm{x}\in\Omega\] (5)

where \(\theta_{p}\in\mathbb{R}\) for \(p\in[P]\) are the only constants that are optimized. As there are lot of expressions that cannot be put in that form, these algorithms fail to discover more complex equations. In particular, for an unknown \(\theta\in\mathbb{R}\) functions such as \(\sin(\theta x_{i})\), \(e^{\theta x_{i}}\) or \(\frac{1}{x_{i}+\theta}\) cannot be learned by these algorithms.

**How to relax these assumptions and still allow for variational formulation?** Current methods that utilize VF either assume that the PDE is in an LC form or they only work for explicit first-order ODEs. Moreover, all of them also make the evolution assumption. Relaxing LC is not trivial because not all PDEs admit VF. As in Equation 3, the PDE has to be a sum of terms for which the integration by parts can be performed. Our crucial observation is that for any term that does not contain any derivatives (and thus does not need to be integrated by parts) _no additional constraints_ need to be put in place. Due to the significance of these terms, we propose the following characterization of a PDE.

**Derivative-bound part and derivative-free part.** Any PDE can be expressed in the form

\[f(\bm{x},\bm{u}(\bm{x}),\partial^{[K]}\bm{u}(\bm{x}))-g(\bm{x},\bm{u}(\bm{x}) )=0\ \forall\bm{x}\in\Omega\] (6)

where we collect all the terms with the derivatives into \(f(\bm{x},\bm{u}(\bm{x}),\partial^{[K]}\bm{u}(\bm{x}))\) and all terms without the derivatives into \(g(\bm{x},\bm{u}(\bm{x}))\). We call \(f\) the _derivative-bound_ part and \(g\) the _derivative-free_ part (denoted also \(\partial\)-_bound_ and \(\partial\)_-free_). \(\partial\)-free part can be evaluated directly given \(\bm{u}\), whereas the \(\partial\)-bound part requires access to the derivatives. Note that for first-order ODEs, \(f\) is trivial and equal to \(\dot{u}_{j}\).

**Constraints on the derivative-bound part.** Although VF does not require any constraints on the \(\partial\)-free part, we still need to put some constraints on the \(\partial\)-bound part for the integration by parts to work. This is what we do in Section 5, where we aim to define currently the broadest form of PDEs that admit the variational formulation using the above characterization.

**Optimization challenge.** D-CIPHER does _not_ need the evolution assumption and it can even discover some PDEs that cannot be put into the evolution form. Moreover, unlike previous methods, D-CIPHER is _not_ limited to PDEs that can be represented as a linear combination of functions (we describe the exact form we assume in Section 6). This makes the optimization problem much harder as we search over _all_ closed-form functions \(g\) and for each candidate, we try to find the best counterpart \(f\) among the allowed expressions. This is very different from the previous approaches, which either do not need to find \(f\) as they work only for first-order ODEs [42] or they constrain equally both the \(\partial\)-bound part and \(\partial\)-free part to be a linear combination of some pre-specified functions [35] (for more details, see Table 10, and Table 11 in Appendix F). One way we address this challenge is by developing a new optimization procedure (Section 7).

## 4 Related works

**Symbolic Regression.** The goal of symbolic regression is to find a closed-form expression that best models the given dataset both in terms of accuracy and simplicity. In contrast with the conventional regression analysis which optimizes the parameters of a pre-specified model, symbolic regression aims at discovering both the general structure and the parameters of the model. Most of the existing work focuses on developing optimization algorithms. Genetic Programming [26] has been widely used for that task [49]. A different strategy has been employed in AI Feynman [54; 55] that uses neural networks to reduce the search space by identifying simplifying properties like symmetry or separability. Optimization methods based on pre-trained neural networks [4; 20], reinforcement learning [40], and Meijer-G functions [1; 13] have also been proposed.

**Data-driven discovery of closed-form differential equations.** Data-driven discovery of physical laws is an established area of machine learning [6; 49]. The pioneering work in that area was SINDy [10] that constrained the space of equations to linear combinations of functions from a predefined library and used sparse regression to discover explicit ODEs. It was later extended to include implicit ODEs [33; 23] and PDEs [47; 48]. Various other extensions were proposed by improving the derivative estimation and the training procedure [45; 61], adding additional selection criteria [32] and learning the library using genetic programming [34; 12; 62]. A different approach is taken by [29] (an extension of [30]) which uses convolutional and symbolic neural networks. It is important to note that _all of these methods still assume the PDE to be a linear combination_ as discussed in Section 3 (Equation 5) which significantly limits their search space. Some other developments are based on Gaussian processes [44; 43] but they require the exact form of the PDE and only optimize the parameters.

**Variational approach.** Recently, the variational approach has been used as a viable alternative to derivative estimation. However, they have only been used for differential equations in a linear combination form [35; 36; 46] or closed-form first-order ODEs [42]. Extending the variational approach to closed-form PDEs is not trivial as PDEs are much more complex than ODEs and not all closed-form PDEs admit the variational formulation. In fact, the approaches that learn the library mentioned in the previous paragraph can produce exactly such terms which prohibits the use of variational formulation. To address these challenges we use the new notions defined in Section 3 to define a new and general class of PDEs in Section 5 that admit the variational formulation.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Method & PDEs & No \(\partial\) estimation & No evolution assumption & Any closed-form \(\partial\)-free part \\ \hline SINDy [10] & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ SINDy-implicit [33] & \(\times\) & \(\times\) & \(\checkmark\) & \(\times\) \\ PDE-FIND [47] & \(\checkmark\) & \(\times\) & \(\times\) & \(\times\) \\ PDE-Net 2.0 [29] & \(\checkmark\) & \(\times\) & \(\times\) & \(\times\) \\ WSINDy [35; 46] & \(\checkmark\) & \(\checkmark\) & \(\times\) & \(\times\) \\ D-CODE [42] & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ D-CIPHER & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Columns correspond to challenges outlined in Section 1 and answer the following questions: Can it discover PDEs? Does it avoid derivative estimation? Is the evolution assumption unnecessary (Equation 4)? Can it discover any closed-form \(\partial\)-free part (Equation 6)?Variational-Ready PDEs

In this section, we propose a new and very general class of PDEs, the _Variational-Ready_ PDEs (VR-PDEs), which can be characterized _without_ referring to the derivative. The VR-PDEs allow arbitrary \(\partial\)-free part but make some minor restrictions on the \(\partial\)-bound part. These restrictions allow one to use the variational formulation of PDEs to circumvent derivative estimation entirely. Despite the minor restriction, VR-PDEs contain many well-known PDEs, including all linear PDEs, Maxwell's equations, and Navier-Stokes equations (additional examples provided in Appendix B).

To define the new class of PDEs, we need the following definition.

**Definition 1** (Extended derivative and differential operator).: Let \(\bm{\alpha}\in\mathbb{N}_{0}^{M}\), \(|\bm{\alpha}|\leq K\), be a multi-index. Let \(h:\mathbb{R}^{M+N}\to\mathbb{R}\) and \(a:\mathbb{R}^{M}\to\mathbb{R}\) be smooth functions. An _extended derivative_\(\mathcal{E}\), denoted \((\bm{\alpha},a,h)\), maps a vector field \(\bm{u}:\mathbb{R}^{M}\to\mathbb{R}^{N}\) to a function \(\mathcal{E}[\bm{u}]:\mathbb{R}^{M}\to\mathbb{R}\) defined as:

\[\mathcal{E}[\bm{u}](\bm{x})=a(\bm{x})\partial^{\bm{\alpha}}[h(\bm{x},\bm{u}( \bm{x}))]\] (7)

\(\mathcal{E}\) is called _closed-form_ if \(a\) and \(h\) are closed-form. We call \(\mathcal{E}\)_non-degenerate_ if \(|\bm{\alpha}|>0\).

Now, let \(\left(\mathcal{E}_{p}\right)_{p\in[P]}\) be a finite sequence of non-degenerate extended derivatives. The extended differential operator, denoted as \(\mathcal{E}_{[P]}\) is an operator defined as:

\[\mathcal{E}_{[P]}[\bm{u}](\bm{x})=\sum_{p=1}^{P}\mathcal{E}_{p}[\bm{u}](\bm{x})\] (8)

_Remark_.: Any linear operator \(L=\sum_{\bm{\alpha}\in\mathcal{A}}a_{\bm{\alpha}}\partial^{\bm{\alpha}}\) acting on \(u_{j}\) is an extended differential operator.

**Definition 2** (Variational-Ready PDE).: Let \(\mathcal{E}_{[P]}\) be an extended differential operator, and let \(g:\mathbb{R}^{M+N}\to\mathbb{R}\) be a continuous function. We denote a _Variational-Ready_ PDE (VR-PDE) by a pair \(\left(\mathcal{E}_{[P]},g\right)\) and define it as:

\[\mathcal{E}_{[P]}[\bm{u}](\bm{x})-g(\bm{x},\bm{u}(\bm{x}))=0\ \forall\bm{x}\in\Omega\] (9)

We extend the standard variational formulation of PDEs (Proposition 1 in Appendix B) from linear PDEs to all VR-PDEs. The following definition is useful in further discussion.

**Definition 3**.: Consider a field \(\bm{u}:\Omega\to\mathbb{R}^{N}\), and an extended derivative \(\mathcal{E}=(\bm{\alpha},a,h)\). Let \(\phi:\Omega\to\mathbb{R}\) be a _testing function_ (\(\mathcal{C}^{K}\) function 3 with compact support). We define the functional

Footnote 3: We say \(u:\mathbb{R}^{M}\to\mathbb{R}\) is in \(\mathcal{C}^{K}\) if \(\partial^{\bm{\alpha}}u\) exists and is continuous for all \(|\bm{\alpha}|\leq K\).

\[\mathcal{F}(\mathcal{E},\bm{u},\phi)=\int_{\Omega}h(\bm{x},\bm{u}(\bm{x}))(-1 )^{|\bm{\alpha}|}\partial^{\bm{\alpha}}[a(\bm{x})\phi(\bm{x})]d\bm{x}\]

We can now use this functional to formulate variational characterization of VR-PDEs.

**Theorem 1**.: \(\bm{u}:\Omega\to\mathbb{R}^{N}\)_, where \(u_{j}\in\mathcal{C}^{K}\), is a solution to a VR-PDE in Equation 9 if and only if_

\[\sum_{p=1}^{P}\mathcal{F}(\mathcal{E}_{p},\bm{u},\phi)-\int_{\Omega}\left[g( \bm{x},\bm{u}(\bm{x}))\phi(\bm{x})\right]d\bm{x}=0\] (10)

_for all testing functions \(\phi:\Omega\to\mathbb{R}\)._

Proof.: Appendix B. 

This theorem motivates the _variational loss function_ as we expect the left-hand side of Equation 10 to be closer to 0 the closer the canditate PDE is to the true one. To calculate how well a set of vector fields \(\mathcal{D}=\{\bm{u}^{(d)}\}_{d=1}^{D}\) matches a VR-PDE \(\left(\mathcal{E}_{[P]},g\right)\) we propose the following loss function.

\[\mathcal{L}\left(\mathcal{E}_{[P]},g\right)=\sum_{d=1}^{D}\sum_{s=1}^{S}\left( \sum_{p=1}^{P}\mathcal{F}(\mathcal{E}_{p},\bm{u}^{(d)},\phi_{s})-\int_{\Omega} g(\bm{x},\bm{u}^{(d)}(\bm{x}))\phi_{s}(\bm{x})d\bm{x}\right)^{2}\] (11)

where \(\{\phi_{s}\}_{s=1}^{S}\) is a set of predefined testing functions.

This novel loss function makes it possible to evaluate to what extent any VR-PDE matches the observed data. This loss can be used as an optimization objective in any algorithm that searches over some subspace of closed-form VR-PDEs. We propose D-CIPHER in Section 6 as an example of such an algorithm.

D-Cipher

In this section, we formulate the problem of PDE discovery and then we introduce a novel algorithm (D-CIPHER) to solve it. The diagram and pseudocode are presented in Figure 1 and in Appendix C.

**Problem formulation** We are given a dataset of _observed fields_\(\mathcal{D}=\{\boldsymbol{v}^{(d)}\}_{d=1}^{D}\) with a finite _sampling grid_\(\mathcal{G}\subset\Omega\). Each \(\boldsymbol{v}^{(d)}(\boldsymbol{x})\) is a noisy measurement, i.e., \(\boldsymbol{v}^{(d)}:\mathcal{G}\rightarrow\mathbb{R}^{N}\) is defined as

\[v_{j}^{(d)}(\boldsymbol{x})=u_{j}^{(d)}(\boldsymbol{x})+\epsilon_{j}^{(d)}( \boldsymbol{x})\;\forall\boldsymbol{x}\in\mathcal{G}\;\forall j\in[N]\] (12)

where \(\epsilon_{j}^{(d)}(\boldsymbol{x})\) is a realization of a zero-mean random variable (noise), each \(u_{j}^{(d)}:\Omega\rightarrow\mathbb{R}\) is a \(\mathcal{C}^{K}\) function, and every _true field_\(\boldsymbol{u}^{(d)}\) is governed by the same closed-form PDE \(f\). The task is to infer the closed-form PDE, \(f\), from the dataset \(\mathcal{D}=\{\boldsymbol{v}^{(d)}\}_{d=1}^{D}\) and the sampling grid \(\mathcal{G}\). We assume that \(f\) is inside the class of closed-form VR-PDEs (Section 5) and its \(\partial\)-bound part is inside a subspace of extended differential operators spanned by a user-specified dictionary (see Step 1 below).

We propose an algorithm that consists of three steps. In the first step, we define the subspace of closed-form VR-PDEs we want to search over to reflect our knowledge of the problem. In the second step, we reconstruct the fields from noisy measurements. In the last step, we solve an optimization problem using a modified symbolic regression algorithm. For more details, check Appendix C.

**Step 1: Choose the form and incorporate prior knowledge.** A human expert should encode their prior knowledge of the problem into a dictionary of non-degenerate extended derivatives \(\mathcal{Q}\) = \(\{\hat{\mathcal{E}}_{p}\}_{p\in[P]}\). We use this dictionary to search over a finite-dimensional subspace of closed-form operators spanned by this set. In other words, we assume that the VR-PDE is of the form:

\[\sum_{p=1}^{P}\beta_{p}\hat{\mathcal{E}}_{p}[\boldsymbol{u}](\boldsymbol{x})- g(\boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))=0\;\forall\boldsymbol{x}\in\Omega\] (13)

where \(\boldsymbol{\beta}\in\mathbb{R}^{P}\), \(g\) is _any_ closed-form function of \(M+N\) variables, and \(\hat{\mathcal{E}}_{p}=(\boldsymbol{\alpha}_{p},a_{p},h_{p})\).

For instance, a dictionary might include only the partial derivatives up to a certain order. For a 1+1 second-order equation that means \(\mathcal{Q}=\{\partial_{t},\partial_{x},\partial_{tx},\partial_{t}^{2}, \partial_{x}^{2}\}\). That is already enough to discover heat and wave equations with any closed-form source. If, for instance, the user suspects the presence of the advection term \(uu_{x}\) (as in the Burgers' equation), the term \(\partial_{x}(u^{2})\) can be included in the library.

It's important to note that we do _not_ assume any particular form of \(g\) apart from being closed-form.

**Step 2: Estimate the fields.** As the dataset \(\mathcal{D}\) consists of noisy and infrequently sampled fields, we first need to estimate the "true" fields \(\boldsymbol{\hat{u}}^{(d)}\) from \(\boldsymbol{v}^{(d)}\). Any choice of reconstruction algorithm can be used and the user should choose it according to the problem setting and their domain knowledge.

**Step 3: Optimize.** We minimize the loss function in Equation 11 for the estimated fields \(\{\boldsymbol{\hat{u}}^{(d)}\}_{d=1}^{D}\) among all PDEs of the form in Equation 13. We solve the following optimization problem:

\[\min_{g}\min_{||\boldsymbol{\beta}||_{1}=1}\sum_{d=1}^{D}\sum_{s=1}^{S}\left( \sum_{p=1}^{P}\mathcal{F}(\beta_{p}\hat{\mathcal{E}}_{p},\boldsymbol{\hat{u}} ^{(d)},\phi_{s})-\int_{\Omega}g(\boldsymbol{x},\boldsymbol{\hat{u}}^{(d)}( \boldsymbol{x}))\phi_{s}(\boldsymbol{x})d\boldsymbol{x}\right)^{2}\] (14)

As we want to discover both \(g\) and \(\boldsymbol{\beta}\) we cannot use the standard penalties on \(\boldsymbol{\beta}\) such as the \(\lambda||\boldsymbol{\beta}||_{2}\) or \(\lambda||\boldsymbol{\beta}||_{1}\), as the loss would be minimized by \(g=0\) and \(\boldsymbol{\beta}=\boldsymbol{0}\). Therefore we put the constraint \(||\boldsymbol{\beta}||_{1}=1\). We choose the L1 norm to encourage sparsity in the coordinates of the vector \(\boldsymbol{\beta}\).

The inner minimization in Equation 14 can be rewritten as a constrained least-squares problem.

\[\min_{||\boldsymbol{\beta}||_{1}=1}\sum_{(d,s)\in[D]\times[S]}\left(\boldsymbol {\beta}\cdot\boldsymbol{z}^{(d,s)}-w^{(d,s)}\right)^{2}\] (15)

where \(\hat{\mathcal{E}}_{p}=(\boldsymbol{\alpha}_{p},a_{p},h_{p})\) and \(\boldsymbol{z}^{(d,s)}\in\mathbb{R}^{P}\), \(w^{(d,s)}\in\mathbb{R}\) are defined as

\[\begin{split} z_{p}^{(d,s)}&=\int_{\Omega}h_{p}( \boldsymbol{x},\boldsymbol{\hat{u}}^{(d)}(\boldsymbol{x}))(-1)^{|\boldsymbol{ \alpha}_{p}|}\partial^{\boldsymbol{\alpha}_{p}}(a_{p}(\boldsymbol{x})\phi_{s} (\boldsymbol{x}))d\boldsymbol{x}\\ w^{(d,s)}&=\int_{\Omega}g(\boldsymbol{x},\boldsymbol{ \hat{u}}^{(d)}(\boldsymbol{x}))\phi_{s}(\boldsymbol{x})d\boldsymbol{x}\end{split}\] (16)We show the full derivation in Appendix C. \(\bm{z}^{(d,s)}\) can be precomputed at the beginning of the algorithm without estimating the derivatives of the reconstructed fields. They can be easily calculated if the derivatives of the testing functions \(\phi_{s}\) and the derivatives of \(a_{p}\) can be analytically computed.

As the optimization problem in Equation 15 has to be solved many times for different closed-form expressions \(g\), it poses some unique challenges. As standard approaches are not sufficiently fast, we design a new heuristic algorithm to solve this problem, CoLLie, and describe it in the next section.

## 7 CoLLie

The problem in Equation 15 from the previous section can be formulated as follows. Given matrix \(\bm{A}\in\mathbb{R}^{m\times n}\) and vector \(\bm{b}\in\mathbb{R}^{m}\), find a vector \(\bm{z}\in\mathbb{R}^{n}\) that minimizes \(||\bm{A}\bm{z}-\bm{b}||_{2}^{2}\) such that \(||\bm{z}||_{1}=1\). The task is challenging as the unit L1 sphere is not convex. A method that guarantees an optimal solution is based on an observation that the \((n-1)\)-dimensional L1 sphere consists of \(2^{n}\)\((n-1)\)-simplices (which are convex). Minimizing \(||\bm{A}\bm{z}-\bm{b}||_{2}^{2}\) on a simplex is a quadratic program [8] with many available solvers [2, 51, 3]. However, that means that the computation time scales _exponentially_ with the number of dimensions. This is prohibitively long for the inner optimization of our algorithm. Therefore, we design a heuristic algorithm CoLLie (**C**onstrained **L**1 Norm **Le**ast Squares) that finds an approximate solution but is significantly faster (Figure 2). We observe that this optimization problem is related to the one encountered in LASSO. Denote \(\bm{z_{0}}\) the solution that minimizes \(||\bm{A}\bm{z}-\bm{b}||_{2}^{2}\) (no constraints). If \(||\bm{z_{0}}||\geq 1\), the problem is equivalent to finding \(\lambda\) (in the Lagrangian form of LASSO, Equation 47) such that the LASSO solution has the norm 1. Least Angle Regression (LARS) [17] is a popular algorithm used to minimize the LASSO objective that computes complete solution paths. These paths show how the coefficients of the solution change as \(\lambda\) moves from \(0\) to \(\lambda_{max}\) (from no constraints to effectively imposing the solution to be \(\bm{0}\)). See Figure 6 in Appendix D.2. CoLLie uses these solution paths to calculate the exact solution to the optimization problem. The case \(0<||\bm{z}_{0}||<1\) is harder as it corresponds to \(\lambda<0\). CoLLie addresses this challenge by extending the solution paths generated by LARS beyond \(\lambda=0\) for \(\lambda<0\). We assume that the paths continue to be piecewise linear and keep their slope (Figure 6 in Appendix

Figure 1: This diagram describes how the algorithm works. After the optimization procedure is finished, we get the best found closed-form function \(g\) and use CoLLie to find the best vector \(\bm{\beta}\). The found equation has the form \(\sum_{p=1}^{P}\beta_{p}\hat{\mathcal{E}}_{p}[\bm{u}](\bm{x})-g(\bm{x},\bm{u}( \bm{x}))=0\)

D.2). CoLLie then uses these assumptions to efficiently find an approximate solution. We provide a detailed description of CoLLie in Appendix D.

## 8 Experiments

We perform a series of synthetic experiments to show how well D-CIPHER is able to discover some well-known differential equations4 (Table 2). First, we demonstrate that D-CIPHER performs better than current methods when discovering PDEs in a linear combination form (Section 8.1). Then we demonstrate it can discover PDEs with a closed-form \(\partial\)-free part that _cannot_ be expressed as a linear combination and thus are beyond the capabilities of current methods (Section 8.2). We contrast D-CIPHER with its ablated version where the derivatives are estimated and the standard MSE loss is used instead of the variational loss (details in Appendix E.1). For additional information about the experiments (e.g., implementation details, data generation, experimental settings) see Appendix E.

Footnote 4: All experiment code can be found at https://github.com/krzysztof-kacprzyk/D-CIPHER

**Evaluation metrics.** To establish how well a discovered PDE matches the ground truth, we evaluate its \(\partial\)-free and \(\partial\)-bound parts separately. For the \(\partial\)-free part, we assign a binary variable indicating whether the correct functional form of the equation was recovered (please check Appendix E.8 for details). For the \(\partial\)-bound part, we measure the RMSE between the found coefficients of \(\bm{\beta}\) and the target ones. We report the averages and standard deviations for both parts. We call the averages respectively **Success Probability** and **Average RMSE**.

**Implementation.** We use B-Splines [15] as the testing functions and we estimate the fields in Step 2 of D-CIPHER with a Gaussian Process [60]. The outer optimization in Step 3 is performed using a modified genetic programming algorithm [26] and the inner optimization by CoLLie (Section 7). We also show additional experiments with different estimation algorithms in Appendix F.

### Discovering Linear Combinations: comparison with other methods

We compare D-CIPHER against two variants of PDE-FIND [47] and WSINDy [46] (as implemented in PySINDy library [24, 16]) with optimization performed by Stepwise Sparse Regression [7] or

\begin{table}
\begin{tabular}{l l l l} \hline \hline Name & Equation & LC & VR \\ \hline Homogeneous heat equation & \(\partial_{t}u-\theta_{1}\partial_{x}^{2}u=0\) & ✓ & ✓ \\ Burger’s equation & \(\partial_{t}u+u\partial_{x}u-\theta_{1}\partial_{x}^{2}u=0\) & ✓ & ✓ \\ Kuramoto-Sivashinsky equation & \(\partial_{t}u+\partial_{x}^{2}u+\partial_{x}^{2}u+u\partial_{x}u=0\) & ✓ & ✓ \\ Forced and damped harmonic oscillator & \(\partial_{t}^{2}+2\theta_{1}\theta_{2}\partial_{t}u+\theta_{2}^{2}u=\theta_{3} \sin(\theta_{4}t)\) & ✓ \\ SLM model (Appendix F.1) & \(\partial_{t}u+\partial_{x}u=-2e^{\theta_{1}x}u\) & ✓ & ✓ \\ Inhomogeneous heat equation & \(\partial_{t}u-\theta_{1}\partial_{x}^{2}u=\theta_{2}e^{\theta_{3}t}\) & ✓ & ✓ \\ Inhomogeneous wave equation & \(\partial_{t}^{2}u-\theta_{1}\partial_{x}^{2}u=\theta_{2}e^{t}\sin(\theta_{3}t)\) & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 2: Equations used in the experiments. “LC” column specifies if the equation can be represented as a linear combination (Equation 5). “VR” column specifies if the PDE is Variational-Ready

Figure 2: We compare CoLLie with an algorithm that uses CVXOPT [2] to solve each of the convex subproblems. We report the relative error between the loss obtained by CoLLie and the minimum loss achieved by CVXOPT. Panels **B** and **C** show the averages and the distributions of relative errors. The average relative error is below 0.005 and the bulk of the distribution is below \(10^{-7}\). At the same time CoLLie is orders of magnitude faster (Panel **A**).

Forward Regression Orthogonal Least-Squares [5]. We note that D-CIPHER is specifically designed to discover PDEs that are beyond the capabilities of current methods, i.e., where the derivative-free part can be any closed-form expression. Current methods are usually tested on equations where the derivative-free part is trivial (identically equal to 0). Even though these algorithms are specialized to discover these simpler kinds of equations, D-CIPHER performs better than (or equally well as) PDE-FIND and WSINDy, regardless of the optimization algorithm, when tested on Burgers' equation the homogeneous heat equation, and Kuramoto-Sivashinsky equation (Figure 3). This demonstrates gain from both the variational loss and the new optimization routine.

### Discovering equations beyond current methods

**Forced and damped harmonic oscillator.** As the oscillator is described by a second-order ODE, it cannot be discovered by D-CODE [42]. D-CIPHER discovers the correct functional form of the \(\partial\)-free part and achieves a low RMSE for the coefficients of \(\boldsymbol{\beta}\) in most of the experimental settings. The performance is higher than or comparable to the ablated version of D-CIPHER, thus demonstrating gain from using the variational approach. We present the results in Figure 4.

**Inhomogeneous heat equation.** D-CIPHER is able to discover the correct equation even in settings with very high noise. It performs better than the ablated version, thus showing the importance of the variational objective. The result are presented in Table 3.

Figure 4: Success probability of discovering the correct \(\partial\)-free part of the equation and the average RMSE between the recovered \(\partial\)-bound part and the target one across different experimental settings. We compare D-CIPHER against its ablated version (Abl. D-CIPHER).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c}{Success probability} & \multicolumn{3}{c}{Average RMSE} \\  & \(\sigma_{R}=0.05\) & \(0.1\) & \(0.2\) & \(\sigma_{R}=0.05\) & \(0.1\) & \(0.2\) \\ \hline D-CIPHER & 0.64 (.07) & 0.42 (.07) & 0.12 (.05) & 0.15 (.009) & 0.21 (.007) & 0.24 (.005) \\ Ablated D-CIPHER & 0.46 (.07) & 0.20 (.06) & 0.04 (.03) & 0.18 (.009) & 0.24 (.008) & 0.27 (.007) \\ \hline \hline \end{tabular}
\end{table}
Table 3: We report the success probability of discovering the \(\partial\)-free part and the Average RMSE of the \(\partial\)-bound part for the inhomogeneous heat equation. Standard deviations shown in brackets.

Figure 3: Simulation results for the Burgers’ equation, homogeneous heat equation, and Kuramoto–Sivashinsky equation. We report the Average RMSE of the \(\partial\)-bound part of the equation. Note that some of the benchmarks overlap

**Inhomogeneous wave equation.** This equation does not have the standard evolution form, as it does not involve the \(\partial_{t}\) term. Thus, even without the source term, most of the current methods cannot be applied directly to discover this equation. In Figure 5 we show the absolute difference between the true field and the fields computed from the sources discovered by D-CIPHER and its ablated version across different measurement settings. D-CIPHER finds the correct functional form with coefficients not far from the ground truth. The ablated version fails to discover the correct functional form and the found \(\partial\)-free part does not reproduce the correct behavior of the equation.

## 9 Discussion

**Applications.** As D-CIPHER can potentially discover any closed-form \(\partial\)-free part, it is especially useful when this part of the PDE captures an essential component of the phenomenon. We demonstrate it by finding the heat and vibration sources as well as the driving force of an oscillator. Beyond the spatio-temporal physical equations, D-CIPHER might prove useful in discovering population models structured by age, size, and spatial position [59, 58], age-dependent epidemiological models [21], and predator-prey models with age-structure [41]. All these equations are VR-PDEs where the \(\partial\)-free parts are crucial elements of the equations signifying the rates of mortality, infection, recovery, or growth. We believe that discovering closed-form equations for these systems would prove invaluable in understanding their behavior.

**Limitations and open challenges.** D-CIPHER may fail in some scenarios, either due to _challenging experimental settings_ or a _challenging underlying PDE_. Challenging experimental settings might include unobserved variables, high measurement noise, infrequent sampling, and inadequate domain (e.g., small time horizon). Challenging PDE forms might include a PDE outside of the VR-PDE class or a \(\partial\)-free part with a complex expression that is difficult to find. We note that we address some of these challenges by utilizing a variational approach, defining VR-PDEs to be a very general class of equations, and designing CoLLie, enabling a thorough search across closed-form expressions.

**Ethics Statement.** We want to emphasize that D-CIPHER was designed to facilitate the process of scientific discovery by extracting closed-form PDEs from data. It is not intended to or capable of replacing human experts in the modeling process. No human-derived data was used.

## Acknowledgments and disclosure of funding

This work is supported by Cancer Research UK and Roche. We want to thank Samuel Holt, Jonathan Crabbe, and anonymous reviewers for their useful comments and feedback on earlier versions of this work. We are grateful to Professor Yuanzhang Xiao for insightful discussions about the optimization algorithms.

Figure 5: We solve the inhomogeneous wave equation for the \(\partial\)-free parts found by the D-CIPHER and its ablated version Abl. D-CIPHER. We show the absolute difference between the computed fields and the true field generated by \(\partial\)-free part \(2\times e^{t}\sin(3t)\).

## References

* [1] Ahmed M Alaa and Mihaela van der Schaar. Demystifying black-box models with symbolic metamodels. _Advances in Neural Information Processing Systems_, 32:11304-11314, 2019.
* [2] MS Andersen, J Dahl, and L Vandenberghe. CVXOPT: A Python package for convex optimization, Version 1.1. 6, 2013, 2013.
* [3] MOSEK ApS. The MOSEK optimization toolbox for MATLAB manual. Version 9.0., 2019.
* [4] L. Biggio, T. Bendinelli*, A. Neitz, A. Lucchi, and G. Parascandolo. Neural Symbolic Regression that Scales. In _38th International Conference on Machine Learning_, July 2021.
* [5] Stephen A. Billings. _Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains_. John Wiley & Sons, September 2013.
* [6] J. Bongard and H. Lipson. Automated reverse engineering of nonlinear dynamical systems. _Proceedings of the National Academy of Sciences_, 104(24):9943-9948, June 2007.
* [7] Lorenzo Boninsegna, Feliks Nuske, and Cecilia Clementi. Sparse learning of stochastic dynamical equations. _The Journal of Chemical Physics_, 148(24):241723, June 2018.
* [8] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* [9] Richard P Brent. _Algorithms for minimization without derivatives_. Courier Corporation, 2013.
* [10] Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. _Proceedings of the National Academy of Sciences_, 113(15):3932-3937, April 2016.
* [11] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. _Advances in neural information processing systems_, 31, 2018.
* [12] Yuntian Chen, Yingtao Luo, Qiang Liu, Hao Xu, and Dongxiao Zhang. Any equation is a forest: Symbolic genetic algorithm for discovering open-form partial differential equations (SGA-PDE). _arXiv:2106.11927 [cs.NE]_, 2021.
* [13] Jonathan Crabbe, Yao Zhang, William Zame, and Mihaela van der Schaar. Learning outside the Black-Box: The pursuit of interpretable models. _arXiv:2011.08596 [cs]_, November 2020. arXiv: 2011.08596.
* [14] D G Crighton. Model Equations of Nonlinear Acoustics. _Annual Review of Fluid Mechanics_, 11(1):11-33, 1979.
* [15] Carl De Boor. _A practical guide to splines_, volume 27. springer-verlag New York, 1978.
* [16] Brian de Silva, Kathleen Champion, Markus Quade, Jean-Christophe Loiseau, J. Kutz, and Steven Brunton. Pysindy: A python package for the sparse identification of nonlinear dynamical systems from data. _Journal of Open Source Software_, 5(49):2104, 2020.
* [17] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least Angle Regression. _The Annals of Statistics_, 32(2):93, 2004.
* [18] Lev D Elsgolc. _Calculus of variations_. Courier Corporation, 2012.
* [19] F. G. (Friedrich Gerard) Friedlander. _Introduction to the theory of distributions / F.G. Friedlander_. Cambridge University Press, Cambridge, 1982.
* [20] Samuel Holt, Zhaozhi Qian, and Mihaela van der Schaar. Deep generative symbolic regression. In _The Eleventh International Conference on Learning Representations_, 2022.
* [21] Frank Hoppensteadt. An Age Dependent Epidemic Model. _Journal of the Franklin Institute_, 297(5):325-333, May 1974.

* [22] James M. Hyman and Basil Nicolaenko. The Kuramoto-Sivashinsky equation: A bridge between PDE'S and dynamical systems. _Physica D: Nonlinear Phenomena_, 18(1):113-126, January 1986.
* [23] Kadierdan Kaheman, J. Nathan Kutz, and Steven L. Brunton. SINDy-PI: a robust algorithm for parallel implicit sparse identification of nonlinear dynamics. _Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 476(2242):20200279, October 2020. Publisher: Royal Society.
* [24] Alan A. Kaptanoglu, Brian M. de Silva, Urban Fasel, Kadierdan Kaheman, Andy J. Goldschmidt, Jared Callaham, Charles B. Delahunt, Zachary G. Nicolaou, Kathleen Champion, Jean-Christophe Loiseau, J. Nathan Kutz, and Steven L. Brunton. Pysindy: A comprehensive python package for robust sparse system identification. _Journal of Open Source Software_, 7(69):3994, 2022.
* [25] Doyo Kereyu and Genanew Gofe. Convergence Rates of Finite Difference Schemes for the Diffusion Equation with Neumann Boundary Conditions. _American Journal of Computational and Applied Mathematics_, page 11, 2016.
* [26] John R. Koza. _Genetic programming: on the programming of computers by means of natural selection_. Complex adaptive systems. MIT Press, Cambridge, Mass, 1992.
* [27] Yoshiki Kuramoto. Instability and Turbulence of Wavefronts in Reaction-Diffusion Systems. _Progress of Theoretical Physics_, 63(6):1885-1903, June 1980.
* [28] Haitao Liu, Yew-Soon Ong, Xiaobo Shen, and Jianfei Cai. When gaussian process meets big data: A review of scalable GPs. _IEEE Transactions on Neural Networks and Learning Systems_, 31(11):4405-4423, 2020.
* [29] Zichao Long, Yiping Lu, and Bin Dong. PDE-Net 2.0: Learning PDEs from data with a numeric-symbolic hybrid deep network. _Journal of Computational Physics_, 399:108925, December 2019.
* [30] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. PDE-Net: Learning PDEs from Data. In _Proceedings of the 35th International Conference on Machine Learning_, pages 3208-3216. PMLR, July 2018. ISSN: 2640-3498.
* [31] Lynn H. Loomis. _Advanced calculus / Lynn H. Loomis and Shlomo Sternberg_. Addison-Wesley series in mathematics. Addison-Wesley Pub. Co., Reading, Mass. ; London, 1968. Publication Title: Advanced calculus.
* [32] N. M. Mangan, J. N. Kutz, S. L. Brunton, and J. L. Proctor. Model selection for dynamical systems via sparse regression and information criteria. _Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 473(2204):20170009, August 2017.
* [33] Niall M. Mangan, Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Inferring Biological Networks by Sparse Identification of Nonlinear Dynamics. _IEEE Transactions on Molecular, Biological and Multi-Scale Communications_, 2(1):52-63, June 2016. Conference Name: IEEE Transactions on Molecular, Biological and Multi-Scale Communications.
* ICCS 2019_, Lecture Notes in Computer Science, pages 635-641, Cham, 2019. Springer International Publishing.
* [35] Daniel A. Messenger and David M. Bortz. Weak SINDy for partial differential equations. _Journal of Computational Physics_, 443:110525, October 2021.
* [36] Daniel A. Messenger and David M. Bortz. Weak SINDy: Galerkin-Based Data-Driven Model Selection. _Multiscale Modeling & Simulation_, 19(3):1474-1497, January 2021.

* [37] Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondrej Certik, Sergey B Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K Moore, Sartaj Singh, and others. SymPy: symbolic computing in Python. _PeerJ Computer Science_, 3:e103, 2017. Publisher: PeerJ Inc.
* [38] A. R. Mitchell and D. F. Griffiths. _The finite difference method in partial differential equations_. Wiley, Chichester [Eng.] ; New York, 1980.
* [39] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* [40] Brenden K Petersen, Mikel Landajuela Larma, T Nathan Mundhenk, Claudio P Santiago, Soo K Kim, and Joanne T Kim. Deep Symbolic Regression: Recovering Mathematical Expressions From Data via Risk-seeking Policy Gradients. _ICLR 2021_, 2021.
* [41] J. Promrak, G. C. Wake, and C. Rattanakul. Predator-prey Model with Age Structure. _The ANZIAM Journal_, 59(2):155-166, October 2017. Publisher: Cambridge University Press.
* [42] Zhaozhi Qian, Krzysztof Kacprzyk, and Mihaela van der Schaar. D-CODE: Discovering Closed-form ODEs from Observed Trajectories. _The Tenth International Conference on Learning Representations_, 2022.
* [43] Maziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of nonlinear partial differential equations. _Journal of Computational Physics_, 357:125-141, March 2018.
* [44] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Numerical Gaussian Processes for Time-Dependent and Nonlinear Partial Differential Equations. _SIAM Journal on Scientific Computing_, 40(1):A172-A198, January 2018.
* [45] Chengping Rao, Pu Ren, Yang Liu, and Hao Sun. Discovering Nonlinear PDEs from Scarce Data with Physics-Encoded Learning. _ICLR 2022_, 2022.
* [46] Patrick A. K. Reinbold, Daniel R. Gurevich, and Roman O. Grigoriev. Using noisy or incomplete data to discover models of spatiotemporal dynamics. _Physical Review E_, 101(1):010203, January 2020.
* [47] Samuel H. Rudy, Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Data-driven discovery of partial differential equations. _Science Advances_, 3(4):e1602614, April 2017.
* [48] Hayden Schaeffer. Learning partial differential equations via data discovery and sparse optimization. _Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 473(2197):20160446, January 2017.
* [49] Michael Schmidt and Hod Lipson. Distilling Free-Form Natural Laws from Experimental Data. _Science_, 324(5923):81-85, April 2009.
* [50] Michael Spivak. _Calculus On Manifolds: a Modern Approach To Classical Theorems Of Advanced Calculus_. CRC Press, 2018. OCLC: 1029237047.
* [51] B. Stellato, G. Banjac, P. Goulart, A. Bemporad, and S. Boyd. OSQP: an operator splitting solver for quadratic programs. _Mathematical Programming Computation_, 12(4):637-672, 2020.
* [52] Trevor Stephens. gplearn: Genetic programming in python, with a scikit-learn inspired and compatible api, 2022.
* [53] Robert Tibshirani. Regression Shrinkage and Selection via the Lasso. _Journal of the Royal Statistical Society. Series B (Methodological)_, 58(1):267-288, 1996. Publisher: [Royal Statistical Society, Wiley].
* [54] Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, and Max Tegmark. AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. _34th Conference on Neural Information Processing Systems (NeurIPS 2020)_, 2020.

* [55] Silviu-Marian Udrescu and Max Tegmark. AI Feynman: A physics-inspired method for symbolic regression. _Science Advances_, 6(16):eaay2631, April 2020.
* [56] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020.
* [57] Sachin Wani and Sarita Thakar. Crank-Nicolson Type Method for Burgers Equation. _International Journal of Applied Physics and Mathematics_, 3:324-328, January 2013.
* [58] G. F. Webb. Population Models Structured by Age, Size, and Spatial Position. In J. M. Morel, F. Takens, B. Teissier, Pierre Magal, and Shigui Ruan, editors, _Structured Population Models in Biology and Epidemiology_, volume 1936, pages 1-49. Springer Berlin Heidelberg, Berlin, Heidelberg, 2008. Series Title: Lecture Notes in Mathematics.
* [59] Glenn F. Webb. _Theory of Nonlinear Age-Dependent Population Dynamics_. CRC Press, January 1985.
* [60] Christopher K Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA, 2006.
* [61] Hao Xu. DL-PDE: Deep-Learning Based Data-Driven Discovery of Partial Differential Equations from Discrete and Noisy Data. _Communications in Computational Physics_, 29(3):698-728, June 2021.
* [62] Hao Xu, Haibin Chang, and Dongxiao Zhang. DLGA-PDE: Discovery of PDEs with incomplete candidate library via combination of deep learning and genetic algorithm. _Journal of Computational Physics_, 418:109584, October 2020.

## Table of supplementary materials

1. Appendix A: notation and definitions
2. Appendix B variational formulation for linear PDEs and the proof of Theorem 1
3. Appendix C: details of the D-CIPHER framework, including pseudocode
4. Appendix D: details of the CoLLie algorithm
5. Appendix E: details of experiments and the implementation
6. Appendix F: additional experiments and discussion

## Appendix A Notation and definitions

### Notation

### Definitions

In this section, we collect the definitions of some of the important terms used in the paper for easy reference.

**Definition 4** (Closed-form expressions and functions).: A _closed-form_ expression is a mathematical expression that consists of a finite number of variables, constants, arithmetic operations, and certain well-known functions (e.g., logarithm, trigonometric functions). A function \(f\) is called _closed-form_ if it can be represented by a closed-form expression. E.g., \(f(x,y)=x^{2}\log(y)+\sin(3z)\).

\begin{table}
\begin{tabular}{l l} \hline \hline Symbol & Meaning \\ \hline \([n]\) & a set of numbers \(\{1,\ldots,n\}\) \\ \(\mathbb{N}\) & a set of natural numbers, i.e., \(\{1,2,3,\ldots,\}\) \\ \(\mathbb{N}_{0}\) & a set of non-negative integers, i.e., \(\{0,1,2,3,\ldots\}\) \\ \(M\) & the dimension of the domain of a vector field \\ \(N\) & the dimension of the codomain of a vector field \\ \(K\) & denotes the smoothness of functions or the maximum order of derivatives \\ \(D\) & the size of the dataset of observed fields \\ \(S\) & the number of testing functions \\ \(\Omega\) & an open set in \(\mathbb{R}^{M}\) \\ \(\hat{u}(t)\) & the derivative of \(u\) at \(t\) \\ \(\bm{\alpha}\) & a multi-index, an element of \(\mathbb{N}_{0}^{M}\) \\ \(|\bm{\alpha}|\) & the order of \(\bm{\alpha}\), \(|\bm{\alpha}|=\sum_{i}^{M}\alpha_{i}\) \\ \(\partial_{i}^{\alpha_{i}}\) & \(\alpha_{i}^{\text{th}}\)-order partial derivative with respect to the \(i^{\text{th}}\) variable \\ \(\partial^{\bm{\alpha}}\) & \(\partial_{1}^{\alpha_{1}}\partial_{2}^{\alpha_{2}}\ldots\partial_{M}^{\alpha_ {M}}\) \\ \(\mathcal{C}^{K}\) & a set of functions with continuous partial derivatives \(\partial^{\bm{\alpha}}\) for all \(|\bm{\alpha}|\leq K\) \\ \(\mathcal{E}\) & an extended derivative, Definition 1 \\ \(\mathcal{E}_{[P]}\) & an extended differential operator, Definition 1 \\ \(\mathcal{F}\) & the functional used in the variational loss, Definition 3 \\ \(\mathcal{L}(\mathcal{E}_{[P]},g)\) & the variational loss, Equation 11 \\ \(\mathcal{G}\) & a sampling grid, Definition 10 \\ \(\bm{u}\) & a true field, Definition 9 \\ \(\bm{v}\) & an observed field, Definition 10 \\ \(\mathcal{D}\) & a dataset of observed trajectories \\ \(\epsilon\) & the noise \\ \(\mathcal{Q}\) & a dictionary of non-degenerate extended derivatives \\ \(\bm{\beta}\) & a vector describing the \(\partial\)-bound part of the VR-PDE \\ \(\sigma_{R}\) & a noise ratio \\ \(\Delta t\) & a sampling interval \\ \hline \hline \end{tabular}
\end{table}
Table 4: Symbols used in this work_Remark_.: In practice, we do not want to consider any finite expression. Any symbolic regression algorithm penalizes expressions that are too long putting a soft constraint on the number of elements used. That is why deep neural networks are not considered closed-form even if they satisfy the conditions in Definition 4.

**Definition 5** (Multi-index).: An \(n\)-dimensional _multi-index_\(\bm{\alpha}\) is an \(n\)-tuple

\[\bm{\alpha}=(\alpha_{1},\alpha_{2},\ldots,\alpha_{n})\]

where \(\forall i\in[n]\:\alpha_{i}\in\mathbb{N}_{0}\). Thus \(\bm{\alpha}\in\mathbb{N}_{0}^{n}\). We define the order of \(\bm{\alpha}\) as \(|\bm{\alpha}|=\sum_{i=1}^{n}\alpha_{i}\).

**Definition 6**.: For any \(n\)-dimensional multi-index \(\bm{\alpha}\) we define a mixed derivative

\[\partial^{\bm{\alpha}}=\partial_{1}^{\alpha_{1}}\partial_{2}^{\alpha_{2}} \ldots\partial_{n}^{\alpha_{n}}\]

where \(\partial_{i}^{\alpha_{i}}=\partial^{\alpha_{i}}/\partial x_{i}^{\alpha_{i}}\) is a \(\alpha_{i}^{\text{th}}\)-order partial derivative with respect to \(x_{i}\) (the \(i^{\text{th}}\) independent variable). We call \(\partial^{\bm{\alpha}}\)_non-trivial_ if \(|\bm{\alpha}|>0\). We denote the list of all non-trivial partial derivatives of \(u\) up to order \(K\) as \(\partial^{[K]}u\).

**Definition 7** (Closed-form Partial Differential Equation).: Let \(f\) be a closed-form real smooth function. We say that a _vector field_\(\bm{u}:\Omega\to\mathbb{R}^{N}\) is _governed_ by a \(K^{\text{th}}\)-order closed-form PDE described by \(f\) if

\[f(\bm{x},\bm{u}(\bm{x}),\partial^{[K]}\bm{u}(\bm{x}))=0\;\forall\bm{x}\in\Omega\] (17)

where \(\partial^{[K]}\bm{u}\) are all non-trivial mixed derivatives of all \(u_{j}\) (\(j\in[N]\)) up to the \(K^{\text{th}}\) order.

**Definition 8** (Testing function).: Support of a function \(\phi:\Omega\to\mathbb{R}\) is defined as

\[supp\;\phi=\overline{\{\bm{x}\in\Omega:\phi(\bm{x})\neq 0\}}\]

where \(\overline{\mathcal{B}}\) is the topological _closure_ of \(\mathcal{B}\) in \(\Omega\).

\(\phi\) is called a _testing function_ if it is a \(\mathcal{C}^{K}\) function with _compact_ support.

**Definition 9** (True Field).: We define a _true field_ on \(\Omega\) as a vector valued function \(\bm{u}:\Omega\to\mathbb{R}^{N}\) where each \(u_{j}:\Omega\to\mathbb{R}\) is a \(\mathcal{C}^{K}\) function.

**Definition 10** (Observed field and sampling grid).: We define a _sampling grid_\(\mathcal{G}\) to be a finite subset of \(\Omega\). Let \(\bm{u}:\Omega\to\mathbb{R}^{N}\) be a true field on \(\Omega\). An _observed field sampled from_\(\bm{u}\) on a grid \(\mathcal{G}\) is a function \(\bm{v}:\mathcal{G}\to\mathbb{R}^{N}\) of the form

\[v_{j}(\bm{x})=u_{j}(\bm{x})+\epsilon_{j}(\bm{x})\;\forall\bm{x}\in\mathcal{G} \;\forall j\in[N]\]

where \(\epsilon_{j}(\bm{x})\) corresponds to _noise_, a realisation of a zero-mean random variable.

**Definition 11** (\(L1\) sphere).: Let \(n\in\mathbb{N}\). We define \(n\)-dimensional \(L1\) sphere to be a subset of \(\mathbb{R}^{n+1}\) defined as:

\[\{\bm{x}\in\mathbb{R}^{n+1}\;|\;||x||_{1}=1\}\subset\mathbb{R}^{n+1}\] (18)

**Definition 12** (Standard simplex).: Let \(n\in\mathbb{N}\). We define standard \(n\)-simplex to be a subset of \(\mathbb{R}^{n+1}\) defined as:

\[\{\bm{x}\in\mathbb{R}^{n+1}\;|\;\sum_{i=1}^{n+1}x_{i}=1\wedge x_{i}\geq 0\; \forall i\in[n+1]\}\subset\mathbb{R}^{n+1}\] (19)

## Appendix B Variational-Ready PDEs

### Variational Formulation of PDEs

In this section, we provide the standard variational formulation of PDEs for linear PDEs [19].

**Definition 13** (Linear differential operator).: Let \(\mathcal{A}\) be a finite set of multi-indices. A linear differential operator \(L\) is defined as

\[L=\sum_{\bm{\alpha}\in\mathcal{A}}a_{\bm{\alpha}}\partial^{\bm{\alpha}}\]

where \(a_{\bm{\alpha}}\in\mathcal{C}^{K}\) is a non-zero sufficiently smooth function of dependent variables. If \(\max_{\bm{\alpha}\in\mathcal{A}}|\bm{\alpha}|=n\) then we call \(L\) an \(n^{\text{th}}\)-order linear differential operator. If all \(a_{\bm{\alpha}}\) are constants we say that \(L\) has _constant coefficients_.

The _adjoint_ of \(L\), denoted \(L^{\dagger}\), is a linear differential operator defined as

\[L^{\dagger}u(\bm{x})=\sum_{\bm{\alpha}\in\mathcal{A}}(-1)^{|\bm{\alpha}|} \partial^{\bm{\alpha}}(a_{\bm{\alpha}}(\bm{x})u(\bm{x}))\] (20)

**Proposition 1** (Variational Formulation of PDEs for linear PDEs).: _Let \(K\in\mathbb{N}\). Consider a scalar field \(u:\Omega\to\mathbb{R}\), such that \(u\in\mathcal{C}^{K}\), a \(K^{\text{th}}\)-order linear differential operator \(L\), and a continuous function \(g:\Omega\to\mathbb{R}\). Let \(\phi:\Omega\to\mathbb{R}\) be a testing function. Then \(u\) satisfies a linear PDE_

\[L[u(\bm{x})]-g(\bm{x})=0\;\forall\bm{x}\in\Omega\] (21)

_if and only if_

\[\int_{\Omega}\left[u(\bm{x})L^{\dagger}\phi(\bm{x})-g(\bm{x})\phi(\bm{x}) \right]d\bm{x}=0\] (22)

_for all testing functions \(\phi:\Omega\to\mathbb{R}\)._

_Note that the integrals are always well-defined as \(\phi\) has a compact support._

### Theorem

Before we prove the Theorem 1, we need the following lemma, which is a particular formulation of the Fundamental lemma of calculus of variations [18]. We also need a generalized version of the divergence theorem [31].

**Lemma** (Fundamental lemma of calculus of variations).: _Let \(K\in\mathbb{N}\), \(\Omega\) be an open set in \(\mathbb{R}^{M}\), and \(u:\Omega\to\mathbb{R}\) be a continuous function. Then \(u\) is equal to \(0\) on the whole \(\Omega\) if and only if \(\int_{\Omega}u(\bm{x})\phi(\bm{x})d\bm{x}=0\) for all \(\mathcal{C}^{K}\) functions \(\phi:\Omega\to\mathbb{R}\) with compact support._

Proof.: If \(u\) is identically \(0\) on \(\Omega\) then all the integrals are trivially equal to \(0\).

We now prove the converse.

Let as assume for contradiction that there exists a point \(\bm{x}_{0}\in\Omega\) such that \(u(\bm{x}_{0})\neq 0\). Without loss of generality we assume \(u(\bm{x}_{0})=\epsilon>0\). As \(u\) is continuous there exists an open ball around \(\bm{x}_{0}\) of radius \(\delta\), denoted \(B^{\delta}_{\bm{x}_{0}}=\{\bm{x}\in\Omega\;|\;||\bm{x}-\bm{x}_{0}||_{2}<\delta\}\), such that \(\forall\bm{x}\in B^{\delta}_{\bm{x}_{0}}\;u(\bm{x})>\epsilon/2>0\).

Now let \(\phi\) be a \(\mathcal{C}^{K}\) function that is positive on \(B^{\delta}_{\bm{x}_{0}}\) and \(0\) elsewhere. Such a function can always be created by appropriately shifting and scaling \(\phi(\bm{x})=e^{-1/(1-||\bm{x}||_{2}^{2})}\cdot\mathbf{1}_{\{||\bm{x}||_{2}<1\}}\). Its support is a closed ball \(\bar{B}^{\delta}_{\bm{x}_{0}}=\{\bm{x}\in\Omega\;|\;||\bm{x}-\bm{x}_{0}||_{2} \leq\delta\}\) which is compact. Then

\[\int_{\Omega}u(\bm{x})\phi(\bm{x})d\bm{x}=\int_{B^{\delta}_{\bm{x}_{0}}}u(\bm {x})\phi(\bm{x})d\bm{x}>0\] (23)

as both \(u(\bm{x})\) and \(\phi(\bm{x})\) are positive on \(B^{\delta}_{\bm{x}_{0}}\). Thus we found a continuous function \(\phi\) with compact support such that:

\[\int_{\Omega}u(\bm{x})\phi(\bm{x})d\bm{x}\neq 0\] (24)

Therefore \(u\) is identically \(0\) on \(\Omega\). 

To make this section self-contained, we provide the statement of the generalized divergence theorem [31].

**Theorem 2** (Divergence theorem).: _Let \(\Omega\) be an open set in \(\mathbb{R}^{M}\) and let \(f,g\) be continuous on \(\bar{\Omega}=\Omega\cup\partial\Omega\) and continuously differentiable on \(\Omega\). Then_

\[\int_{\Omega}\partial^{1}_{i}[f(\bm{x})]g(\bm{x})d\bm{x}=-\int_{\Omega}f(\bm{x })\partial^{1}_{i}[g(\bm{x})]d\bm{x}+\int_{\partial\Omega}\nu_{i}f(\bm{x})g( \bm{x})d\bm{x}\] (25)

_where \(\bm{\nu}\) is a normal unit vector to the boundary \(\partial\Omega\)._

In a 1-dimensional setting, the statement of the theorem reduces to the integration by parts.

We can now prove Theorem 1.

Proof.: Let us denote \(\mathcal{E}_{p}=(\bm{\alpha}_{p},a_{p},h_{p})\). Then the PDE in Equation 9 can be written as:

\[\sum_{p=1}^{P}a_{p}(\bm{x})\partial^{\bm{\alpha}_{p}}[h_{p}(\bm{x},\bm{u}(\bm{x }))]-g(\bm{x},\bm{u}(\bm{x}))=0\;\forall\bm{x}\in\Omega\] (26)The LHS is continuous as all \(a_{p}\) and \(h_{p}\) are smooth, \(g\) is continuous, \(u\in\mathcal{C}^{K}\), and \(|\boldsymbol{\alpha}_{p}|\leq K\;\forall p\in[P]\). Thus we can use the fundamental lemma of calculus of variations to say that the Equation 26 is true if and only if

\[\int_{\Omega}\left[\sum_{p=1}^{P}a_{p}(\boldsymbol{x})\partial^{\boldsymbol{ \alpha}_{p}}[h_{p}(\boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))]-g( \boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))\right]\phi(x)d\boldsymbol{x}=0\] (27)

for all testing functions \(\phi\). We transform the LHS of Equation 27 using linearity to:

\[\sum_{p=1}^{P}\int_{\Omega}a_{p}(\boldsymbol{x})\partial^{\boldsymbol{\alpha}_ {p}}[h_{p}(\boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))]\phi(x)-\int_{ \Omega}g(\boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))\phi(\boldsymbol{x})d \boldsymbol{x}\] (28)

Let us now focus on

\[\int_{\Omega}a_{p}(\boldsymbol{x})\partial^{\boldsymbol{\alpha}_{p}}[h_{p}( \boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))]\phi(x)\] (29)

and let us denote \(\boldsymbol{\alpha}_{p}=(\alpha_{p1},\ldots,\alpha_{pM})\). Then \(\partial^{\boldsymbol{\alpha}_{p}}=\partial_{1}^{\alpha_{p1}}\ldots\partial_{ M}^{\alpha_{pM}}\) and the expression can be written as

\[\int_{\Omega}\partial_{1}^{\alpha_{p1}}\ldots\partial_{M}^{\alpha_{pM}}[h_{p}( \boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))]a_{p}(\boldsymbol{x})\phi(x)d \boldsymbol{x}\] (30)

Let us denote the support of \(\phi\) as \(\mathcal{B}\). As \(\phi\) is equal to zero outside of its support, we can write the expression as

\[\int_{\mathcal{B}}\partial_{1}^{\alpha_{p1}}\ldots\partial_{M}^{\alpha_{pM}}[ h_{p}(\boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))]a_{p}(\boldsymbol{x})\phi(x)d \boldsymbol{x}\] (31)

Without loss of generality, let us assume that \(\alpha_{p1}>0\). By the divergence theorem, this can be rewritten as

\[-\int_{\mathcal{B}}\partial_{1}^{\alpha_{p1}-1}\ldots\partial_{M}^{\alpha_{pM }}[h_{p}(\boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))]\partial_{1}^{1}[a_{ p}(\boldsymbol{x})\phi(x)]d\boldsymbol{x}\] (32)

because the integral over the boundary is equal to 0

\[\int_{\partial\mathcal{B}}\nu_{1}\partial_{1}^{\alpha_{p1}-1}\ldots\partial_{ M}^{\alpha_{pM}}[h_{p}(\boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))]a_{p}( \boldsymbol{x})\phi(x)d\boldsymbol{x}=0\] (33)

as \(\phi\) has a compact support (and thus vanishes on the boundary). We can perform this operation \(\alpha_{p1}\) times to shift the whole derivative \(\partial_{1}^{\alpha_{p1}}\) to the second part of the equation and obtain

\[(-1)^{\alpha_{p1}}\int_{\mathcal{B}}\partial_{2}^{\alpha_{p2}}\ldots\partial_ {M}^{\alpha_{pM}}[h_{p}(\boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))] \partial_{1}^{\alpha_{p1}}[a_{p}(\boldsymbol{x})\phi(x)]d\boldsymbol{x}\] (34)

Then we repeat this for other derivatives and we end up with the following expression:

\[(-1)^{\alpha_{p1}}\ldots\cdot(-1)^{\alpha_{pM}}\int_{\mathcal{B}}h_{p}( \boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))\partial_{1}^{\alpha_{p1}}\ldots \partial_{M}^{\alpha_{pM}}[a_{p}(\boldsymbol{x})\phi(x)]d\boldsymbol{x}\] (35)

As the integrand is zero outside of \(\mathcal{B}\), this can be rewritten as:

\[(-1)^{|\boldsymbol{\alpha}_{p}|}\int_{\Omega}h_{p}(\boldsymbol{x},\boldsymbol{ u}(\boldsymbol{x}))\partial^{\boldsymbol{\alpha}_{p}}[a_{p}(\boldsymbol{x}) \phi(x)]d\boldsymbol{x}\] (36)

or more compactly, using the functional defined in Definition 3, as:

\[\mathcal{F}(\mathcal{E}_{p},\boldsymbol{u},\phi)\] (37)

Therefore Equation 28 can be written as:

\[\sum_{p=1}^{P}\mathcal{F}(\mathcal{E}_{p},\boldsymbol{u},\phi)-\int_{\Omega} \left[g(\boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))\phi(\boldsymbol{x}) \right]d\boldsymbol{x}\] (38)

Thus, we proved that Equation 26 is true if and only if

\[\sum_{p=1}^{P}\mathcal{F}(\mathcal{E}_{p},\boldsymbol{u},\phi)-\int_{\Omega} \left[g(\boldsymbol{x},\boldsymbol{u}(\boldsymbol{x}))\phi(\boldsymbol{x}) \right]d\boldsymbol{x}=0\] (39)

for all testing functions \(\phi\).

### Examples

The examples of VR-PDEs can be found in Table 5.

## Appendix C D-Cipher

### Rewrite the inner optimization as a constrained least squares

Let us rewrite the objective in Equation 14.

\[\sum_{d=1}^{D}\sum_{s=1}^{S}\left(\sum_{p=1}^{P}\mathcal{F}(\beta_{p}\hat{ \boldsymbol{c}}_{p},\hat{\boldsymbol{u}}^{(d)},\phi_{s})-\int_{\Omega}g( \boldsymbol{x},\hat{\boldsymbol{u}}^{(d)}(\boldsymbol{x}))\phi_{s}( \boldsymbol{x})d\boldsymbol{x}\right)^{2}\] (40)

First, let us observe that

\[\mathcal{F}(\beta_{p}\hat{\boldsymbol{c}}_{p},\hat{\boldsymbol{u}}^{(d)}, \phi_{s})=\int_{\Omega}h_{p}(\boldsymbol{x},\hat{\boldsymbol{u}}^{(d)}( \boldsymbol{x}))(-1)^{|\boldsymbol{\alpha}_{p}|}\partial^{\boldsymbol{\alpha}_ {p}}[\beta_{p}a_{p}(\boldsymbol{x})\phi_{s}(\boldsymbol{x})]d\boldsymbol{x}= \beta_{p}z_{p}^{(d,s)}\] (41)

if we let \(z_{p}^{(d,s)}\in\mathbb{R}\) be defined as

\[z_{p}^{(d,s)}=\int_{\Omega}h_{p}(\boldsymbol{x},\hat{\boldsymbol{u}}^{(d)}( \boldsymbol{x}))(-1)^{|\boldsymbol{\alpha}_{p}|}\partial^{\boldsymbol{\alpha} _{p}}(a_{p}(\boldsymbol{x})\phi_{s}(\boldsymbol{x}))d\boldsymbol{x}\] (42)

Moreover, if we define \(w^{(d,s)}\in\mathbb{R}\) as

\[w^{(d,s)}=\int_{\Omega}g(\boldsymbol{x},\hat{\boldsymbol{u}}^{(d)}( \boldsymbol{x}))\phi_{s}(\boldsymbol{x})d\boldsymbol{x}\] (43)

we can rewrite expression 40 as

\[\sum_{d=1}^{D}\sum_{s=1}^{S}\left(\sum_{p=1}^{P}\beta_{p}z_{p}^{(d,s)}-w^{(d,s )}\right)\] (44)

Now, the sum over \(p\) can be written as a dot product between \(\boldsymbol{z}^{(d,s)}\in\mathbb{R}^{P}\) and \(\boldsymbol{\beta}\in\mathbb{R}^{P}\). We can also combine the sums over \(d\) and \(s\). We obtain

\[\sum_{(d,s)\in[D]\times[S]}\left(\boldsymbol{\beta}\cdot\boldsymbol{z}^{(d,s) }-w^{(d,s)}\right)^{2}\] (45)

which is exactly the same as the objective in Equation 15.

### Pseudocode

The pseudocode of D-CIPHER is presented in Algorithm 1.

\begin{table}
\begin{tabular}{l l l} \hline \hline Name & Equation & Linear & VR \\ \hline Damped wave eq. with a source & \(u_{tt}+\rho u_{t}-\kappa\nabla^{2}u=g(\boldsymbol{x})\) & ✓ & ✓ \\ Gauss law & \(\nabla\cdot\boldsymbol{E}=\nicefrac{{\rho}}{{\epsilon_{0}}}\) & ✓ & ✓ \\ Burger’s equation & \(u_{t}+uu_{x}-\nu u_{xx}=0\) & ✗ & ✓ \\ Navier-Stokes equations & \(\boldsymbol{u}_{t}+(\boldsymbol{u}\cdot\nabla)\boldsymbol{u}-\nu\nabla^{2} \boldsymbol{u}=-\nicefrac{{1}}{{\rho}}\nabla p+\boldsymbol{g}\) & ✗ & ✓ \\ Korteweg-De Vries equation & \(u_{t}+u_{xxx}-6uu_{x}=0\) & ✗ & ✓ \\ Kuramoto-Sivashinsky equation & \(u_{t}+u_{xx}+u_{xxxx}+uu_{x}=0\) & ✗ & ✓ \\ Fisher’s equation & \(u_{t}-\kappa u_{xx}=ru(1-u)\) & ✗ & ✓ \\ Liouville’s equation & \(u_{xx}+u_{yy}=\kappa e^{\rho u}\) & ✗ & ✓ \\ Porous medium equation & \(u_{t}-\nabla^{2}(u^{\kappa})=0\) & ✗ & ✓ \\ Sine-Gordon equation & \(u_{tt}-u_{xx}=-\sin(u)\) & ✗ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 5: Examples of equations which are Variational-Ready```
0: Observed fields \(\mathcal{D}=\{\bm{v}^{(d)}\}_{d=1}^{D}\), grid \(\mathcal{G}\)
0: Symbolic regression optimization algorithm \(\mathcal{O}\)
0: Smoothing algorithm \(\mathcal{S}\)
0: Testing functions \(\{\phi_{s}\}_{s=1}^{S}\)
0: Dictionary \(\mathcal{Q}=\{\hat{\mathcal{E}}_{p}\}_{p=1}^{P}\), \(\hat{\mathcal{E}}_{p}=(\bm{\alpha}_{p},a_{p},h_{p})\)\(\triangleright\) Step 1
0: Target PDE \(\hat{\bm{u}}^{(d)}=\mathcal{S}(\bm{v}^{(d)})\;\forall d\in[D]\)\(\triangleright\) Step 2  initialize matrix \(\bm{Z}\in\mathbb{R}^{D\times S}\times\mathbb{R}^{P}\) \(Z_{p}^{(d,s)}\leftarrow\int_{\Omega}h_{p}(\bm{x},\hat{\bm{u}}^{(d)}(\bm{x}))(- 1)^{|\bm{\alpha}_{p}|}\partial^{\bm{\alpha}_{p}}(a_{p}(\bm{x})\phi_{s}(\bm{x}) )d\bm{x}\) procedureLoss\((g)\)  initialize vector \(\bm{w}\in\mathbb{R}^{D\times S}\) \(w^{(d,s)}\leftarrow\int_{\Omega}g(\bm{x},\hat{\bm{u}}^{(d)}(\bm{x}))\phi_{s}( \bm{x})d\bm{x}\) \(\bm{\beta}\leftarrow\textsc{CoLLie}(\bm{Z},\bm{w})\)\(\triangleright\) Section 7 \(L=||\bm{Z}\bm{\beta}-\bm{w}||_{2}^{2}\) return\(L\) endprocedure\(g=\mathcal{O}(\textsc{Loss})\)\(\triangleright\) Step 3  initialize vector \(\bm{w}\in\mathbb{R}^{D\times S}\) \(w^{(d,s)}\leftarrow\int_{\Omega}g(\bm{x},\hat{\bm{u}}^{(d)}(\bm{x}))\phi_{s}( \bm{x})d\bm{x}\) \(\bm{\beta}\leftarrow\textsc{CoLLie}(\bm{Z},\bm{w})\)\(\triangleright\) Section 7 return\(\sum_{p=1}^{P}\beta_{p}\hat{\mathcal{E}}_{p}[\bm{u}](\bm{x})-g(\bm{x},\bm{u}(\bm{x}) )=0\) ```

**Algorithm 1** D-CIPHER

### Testing functions

Testing functions should satisfy the following conditions:

1. Be sufficiently smooth (at least \(\mathcal{C}^{K}\) for a \(K^{\text{th}}\) order PDE)
2. Compact support
3. Derivatives can be computed analytically
4. Orthonormal

Conditions 1 and 2 follow directly from Definition 3. Condition 3 is necessary because we do not want to estimate the derivatives of the testing functions. Condition 4 follows from the result obtained by [42] that suggests that these functions should be a subset of an orthonormal basis of L2 space.

We use B-Splines [15] as the testing functions in our experiments because we can control their smoothness and the derivatives are easy to compute. We scale and shift them appropriately so that they are orthonormal.

Other testing functions are possible and examining them constitutes an interesting research direction. In particular, piecewise polynomials as defined by [35] or various wavelets. Ideally, we would like to choose wavelets that form an orthonormal basis for the L2 space, such as

* smooth (\(\mathcal{C}^{\infty}\)) but not compact
* smooth (\(\mathcal{C}^{\infty}\)) but not compact (better rate of decay than Shannon)
* smooth (\(\mathcal{C}^{K}\) for a specified \(K\)) and compact but they do not have a closed-form expression.

Another interesting avenue of research would be to adapt the testing functions to the input data.

CoLLie

### Lagrangian

The problem that CoLLie is supposed to solve is a constrained least-squares optimization defined as:

\[\text{minimize }||\bm{A}\bm{z}-\bm{b}||_{2}^{2}\] (46) \[\text{subject to }||\bm{z}||_{1}-1=0\]

where \(\bm{A}\in\mathbb{R}^{m\times n}\) has a full column rank, \(\bm{b}\in\mathbb{R}^{m}\), and \(\bm{z}\in\mathbb{R}^{n}\) for some \(m,n\in\mathbb{N}\).

We consider the _Lagrangian_\(L:\mathbb{R}^{n}\times\mathbb{R}\to\mathbb{R}\) associated with this problem [8] defined as

\[L(\bm{z},\lambda)=||\bm{A}\bm{z}-\bm{b}||_{2}^{2}+\lambda(||\bm{z}||_{1}-1)\] (47)

Now let us define \(\hat{\bm{z}}:\mathbb{R}\to\mathbb{R}^{n}\) as

\[\hat{\bm{z}}(\lambda)=\operatorname*{arg\,min}_{\bm{z}\in\mathbb{R}^{n}}L( \bm{z},\lambda)=\operatorname*{arg\,min}_{\bm{z}\in\mathbb{R}^{n}}||\bm{A} \bm{z}-\bm{b}||_{2}^{2}+\lambda||\bm{z}||_{1}\] (48)

The goal of our algorithm is to find \(\lambda^{*}\in\mathbb{R}\) such that \(||\hat{\bm{z}}(\lambda^{*})||_{1}=1\). Let us define a function \(q:\mathbb{R}\to\mathbb{R}\) as

\[q(\lambda)=||\hat{\bm{z}}(\lambda)||_{1}\] (49)

The goal can be phrased as finding \(\lambda^{*}\in\mathbb{R}\) such that \(q(\lambda^{*})=1\).

Let us note that \(\hat{\bm{z}}(0)\) is just a solution to the ordinary least squares (OLS) problem with no constraints and its norm is \(q(0)\).

### Extending LARS

#### Case 1. \(q(0)\geq 1\).

If we assume that \(\hat{\bm{z}}\) is continuous then \(q\) is also continuous. From the continuity and the fact that \(\lim_{\lambda\to+\infty}q(\lambda)=0\) and \(q(0)\geq 1\) we infer that there exists a \(\lambda\geq 0\) such that \(q(\lambda)=1\). Moreover, for \(\lambda\geq 0\) the problem in Equation 48 is the same as in LASSO [53]. Therefore we just need to perform LASSO for different \(\lambda\) and choose the one that gives the solution with L1 norm equal to 1.

To do it in practice we use Least Angle Regression (LARS) [17], a popular algorithm used to minimize the LASSO objective. It generates complete solution paths, i.e., a function \(\bm{c}:\mathbb{R}_{+}\to\mathbb{R}^{n}\) defined as

\[\bm{c}(\lambda)=\operatorname*{arg\,min}_{\bm{z}\in\mathbb{R}^{n}}||\bm{A}\bm {z}-\bm{b}||_{2}^{2}+\lambda||\bm{z}||_{1}\] (50)

which is equivalent to \(\hat{\bm{z}}\) for \(\lambda\geq 0\). An illustration of LARS solution paths can be seen in Figure 6. Each line corresponds to a function \(c_{i}\) which describes the coefficient for the \(i^{\text{th}}\) covariate. The paths are defined from some \(\lambda_{0}\) where all \(c_{i}(\lambda_{0})=0\) to \(\lambda=0\) where \(\bm{c}(0)=\hat{\bm{z}}(0)\). In other words, the solution paths cover the whole range of constraints from the strictest, effectively imposing the L1 norm of \(\bm{z}\) to be \(0\), up to no constraints, solving the OLS problem.

The solution paths from the LARS algorithm are piecewise linear and the outputs are the values of the coefficients for points \((\lambda_{0}>\ldots>\lambda_{n}=0)\) where the slopes change. We calculate the norm at each of these points, \(||\bm{c}(\lambda_{i})||_{1}\), and find \(j\in[n]\) such that \(||\bm{c}(\lambda_{j-1})||_{1}<1\leq||\bm{c}(\lambda_{j})||_{1}\). As each \(\bm{c}_{i}\) is a linear function on \([\lambda_{j},\lambda_{j-1}]\) and we know both \(\bm{c}(\lambda_{j-1})\) and \(\bm{c}(\lambda_{j})\), we can effectively search for \(\lambda\in[\lambda_{j},\lambda_{j-1}]\) such that \(||\bm{c}(\lambda)||_{1}=1\). The search can be performed by any root-finding algorithm. We use Brent's method [9].

#### Case 2. \(0<q(0)<1\).

This is much more difficult as it corresponds to solving the problem in Equation 48 for \(\lambda<0\). The solutions given by the LARS algorithm are too small. In fact, the solution with the biggest norm is \(\bm{c}(0)=\hat{\bm{z}}(0)\), the OLS solution, with norm exactly \(q(0)<1\).

To address this challenge, we propose the following heuristic. We _extend_ the solution paths generated by LARS beyond \(\lambda=0\) for \(\lambda<0\). We assume that the paths will continue to be piecewise linear and that they will keep the slope they have in the last interval \([\lambda_{n}=0,\lambda_{n-1}]\). Let us denote this slope as

\[\Delta c_{i}=\frac{c_{i}(0)-c_{i}(\lambda_{n-1})}{0-\lambda_{n-1}}\] (51)This is graphically represented in Figure 6. Formally, these extended paths, \(\bm{\bar{c}}:\mathbb{R}\rightarrow\mathbb{R}\) are defined as:

\[\bar{c}_{i}(\lambda)=\begin{cases}c_{i}(\lambda),&\lambda\geq 0\\ c_{i}(0)+\lambda\Delta c_{i},&\lambda<0\end{cases}\] (52)

Now, we want to find \(\lambda<0\) such that \(||\bm{\bar{c}}(\lambda)||_{1}=1\). To achieve this in practice, we first make the following observations.

For any \(\lambda<0\) we say that \(\bar{c}_{i}(\lambda)\) is on the _right side_ if \(\bar{c}_{i}(\lambda)\Delta c_{i}\leq 0\) and we say that \(\bar{c}_{i}(\lambda)\) is on the _wrong side_ if \(\bar{c}_{i}(\lambda)\Delta c_{i}>0\). In other words, being on the wrong side just means that the path is yet to cross the x-axis if we keep decreasing \(\lambda\). We can easily find \(\lambda^{\prime}\) such that for all \(\lambda<\lambda^{\prime}\) all \(\bar{c}_{i}(\lambda)\) are on the right side (none of the paths will ever cross the x-axis).

\[\lambda^{\prime}=\min\left\{\frac{0-c_{i}(0)}{\Delta c_{i}}\mid i\in[n]\wedge c _{i}(0)\Delta c_{i}>0\right\}\] (53)

If \(||\bm{\bar{c}}(\lambda^{\prime})||_{1}\geq 1\) we just need to search the interval \([\lambda^{\prime},0]\) for \(\lambda\) such that \(||\bm{\bar{c}}(\lambda^{\prime})||_{1}=1\).

If \(||\bm{\bar{c}}(\lambda^{\prime})||_{1}<1\) then we need to search \(\lambda<\lambda^{\prime}\). However, by definition, for all \(\lambda<\lambda^{\prime}\), all \(c_{i}(\lambda)\) are on the right side. That means \(||\bm{\bar{c}}(\lambda)||_{1}\) as a function of \(\lambda\) is just a linear function on the interval \((-\infty,\lambda^{\prime})\). To see that, let us observe that

\[||\bm{\bar{c}}(\lambda)||_{1}=\sum_{i=1}^{n}|\bar{c}_{i}(\lambda)|=\sum_{i=1}^ {n}sign(\bar{c}_{i}(\lambda))\bar{c}_{i}(\lambda)\] (54)

Additionally, for \(\lambda<\lambda^{\prime}\) all \(c_{i}(\lambda)\) are on the right side, so we have \(sign(\bar{c}_{i}(\lambda))=-sign(\Delta c_{i})\). We can rewrite \(||\bm{\bar{c}}(\lambda)||_{1}\) as:

\[||\bm{\bar{c}}(\lambda)||_{1} =\sum_{i=1}^{n}\left(-sign(\Delta c_{i})(c_{i}(0)+\lambda\Delta c _{i})\right)\] (55) \[=-\sum_{i=1}^{n}sign(\Delta c_{i})c_{i}(0)-\left(\sum_{i=1}^{n} sign(\Delta c_{i})\Delta c_{i}\right)\lambda\] \[=-\sum_{i=1}^{n}sign(\Delta c_{i})c_{i}(0)-\left(\sum_{i=1}^{n}| \Delta c_{i}|\right)\lambda\]

Therefore the solution can be found using the following equation

\[\lambda^{*}=\lambda^{\prime}+\frac{1-||\bm{\bar{c}}(\lambda^{\prime})||_{1}}{ -\sum_{i=1}^{n}|\Delta c_{i}|}\] (56)

**Case 3.**\(q(0)=0\). In that case, we just return a precomputed solution to the problem

\[\begin{array}{l}\text{minimize }||\bm{A}\bm{z}||_{2}^{2}\\ \text{subject to }||\bm{z}||_{1}-1=0\end{array}\] (57)

which we compute by subdividing the problem into \(2^{n}\) quadratic programs and solving each of them separately using CVXOPT algorithm [2] as described in Section 7.

### Comparison

We perform a comparison between CoLLie and an algorithm based on CVXOPT [2] as described in Section \(7\). CoLLie uses different procedures depending on the problem, thus for a fair comparison we generate an equal number of tests falling under Case 1 and Case 2 (two main cases) as described in D.2. To achieve that, we generate a random \(m\times n\) matrix \(\bm{A}\) where each entry is sampled from the standard normal distribution. We use \(m=1000\) and \(n\) ranging from \(2\) to \(7\). We then generate a vector \(\bm{\hat{z}}\in\mathbb{R}^{n}\) such that each entry is sampled from a uniform distribution on \([-0.5,0.5]\). Then \(\bm{\hat{z}}\) is normalized to have L1 norm equal to \(1\). We sample a number \(l\) from a uniform distribution on \([-1,3]\) and multiply \(\bm{\hat{z}}\) by \(l\) to obtain \(\bm{z}^{\prime}=l\bm{\hat{z}}\). By this procedure, we are guaranteed that cases \(||\bm{z}^{\prime}||_{1}<1\)and \(||\bm{z}^{\prime}||_{1}\geq 1\) are equally likely. We then let \(\bm{b}=\bm{A}\bm{z}^{\prime}\). The task is then to find \(\bm{z}\) with L1 norm equal to \(1\) that minimizes \(||\bm{A}\bm{z}-\bm{b}||_{2}^{2}\). As \(q(0)=||\bm{z}^{\prime}||_{1}=l\), Case 1 and Case 2 are equally likely.

We perform 1000 experiments for each \(n\). As losses for optimal solutions can be on widely different scales, we report the relative error between the loss obtained by CoLLie and the loss obtained by the algorithm based on CVXOPT, which seems to always find the optimal solution.

The time is measured on a single computer with an Intel Core i5-6500 CPU (4 cores) and 16GB of RAM.

## Appendix E Experiments

### Ablated D-CIPHER

The ablated version uses the standard MSE loss with estimated derivatives and thus solves the following optimization problem:

\[\min_{g}\min_{||\bm{\beta}||_{1}}\sum_{d=1}^{D}\sum_{\bm{x}\in\mathcal{G}} \left(\sum_{p=1}^{P}\beta_{p}\hat{\mathcal{E}}_{p}[\bm{v}^{(d)}](\bm{x})-g( \bm{x},\bm{v}^{(d)}(\bm{x}))\right)^{2}\] (58)

where \(\bm{v}^{(d)}\) is the observed field, \(\mathcal{G}\) is the sampling grid, and \(\hat{\mathcal{E}}_{p}[\bm{v}^{(d)}](\bm{x})\) requires derivative estimation. The ablated version uses the same symbolic regression algorithm to search over closed-form \(g\) and CoLLie for the inner optimization.

### Implementation

**Step 1.** For the homogeneous heat equation and Burgers' equation we use dictionary \(\mathcal{Q}=\{u,\partial_{t}u,\partial_{x}u,\partial_{x}(u^{2}),\partial_{x}^ {2}u,\partial_{x}^{2}(u^{2})\}\). For Kuramoto-Sivashinsky equation we use dictionary \(\mathcal{Q}=\{u,\partial_{t}u,\partial_{x}u,\partial_{x}(u^{2}),\partial_{x}^ {2}u,\partial_{x}^{2}(u^{2}),\partial_{x}^{3}u,\partial_{x}^{3}(u^{2}), \partial_{x}^{4}u,\partial_{x}^{4}(u^{2})\}\). For the damped and forced harmonic oscillator we use the dictionary \(\{\partial_{t},\partial_{t}^{2}\}\), and for the wave and heat equations, we use \(\{\partial_{t},\partial_{x},\partial_{t}^{2},\partial_{t}\partial_{x},\partial _{x}^{2}\}\).

**Step 2.** Field estimation is performed using the Gaussian Process Regression from the Python library scikit-learn [39]. The kernel is chosen to be the RBF kernel [60] with an added White kernel to account for noise. The observed field is initially standardized by subtracting the mean and dividing by the standard deviation. Then the GaussianProcessRegressor is fitted to the data. The estimated fields are generated by predicting the values of a trained Gaussian Process on a full integration grid

Figure 6: Panel **A** shows and example of solution paths calculated by the LARS algorithm. Panel **B** shows their extended versions as defined in Case 2 in D.2. The x-axis is reversed, so \(\lambda\) decreases as it moves to the right.

and then scaling back to their original range (by multiplying by the standard deviation and adding the mean).

**Step 3.** The search over the closed-form expression is performed using the symbolic regression library gplearn [52]. We use a custom fitness function that solves the inner optimization problem in Equation 14. This inner optimization is performed by CoLLie (Section 7). The integration is performed using Riemann sums.

**Ablated version of D-CIPHER.** The derivative estimation is performed by first fitting a Gaussian process (in the same way as in Step 2) and then using the finite difference to estimate the derivative in one of the coordinates for all points in the sampling grid. To obtain higher-order derivatives, a Gaussian process is fitted again and the derivative is once again calculated using the finite difference (possibly in a different direction than the first time).

### Hyperparameters

**Gaussian process regression.** The kernel parameters of the Gaussian Process are automatically adjusted during training. The default bounds of the length scale of the RBF kernel and the noise level of the White kernel are used, i.e., \((1e-5,1e5)\).

**GPlearn.** We do not perform parameter tuning for the gplearn library and use the same parameters as in D-CODE [42] except for the parsimony coefficient and the number of generations.

The number of generations is chosen to be 30 for the damped and forced harmonic oscillator and 20 for the inhomogeneous heat and wave equations.

Please check [52] for the detailed description of these parameters.

We modify the implementation of the parsimony coefficient. The standard implementation adds to the loss the length of the equation multiplied by the parsimony coefficient. In our implementation, we increase the loss by the parsimony coefficient. This modification is performed because for different experiments we record the loss on widely different scales. To prevent tuning this parameter for every experimental setting we introduce a penalty that can work on different scales. The parsimony coefficient is chosen manually by performing experiments for a few values. The value used in the experiments is \(0.05\).

The set of allowed mathematical operations is: \(\{+,-,\times,\div,\sin,\exp,\log\}\)

We want to emphasize that we use the same configuration of gplearn in D-CIPHER and its ablated version.

**Integration and number of testing functions.** For the damped and forced harmonic oscillator we use \(10\) testing functions and the integration step \(0.01\). For the inhomogeneous heat and wave equations we use \(100\) testing functions and integrate on a grid with steps \(\delta t=0.01\) and \(\delta x=0.01\).

**Derivative estimation in the ablated version of D-CIPHER.** The Gaussian process is configured the same way as described above. The interval used in the finite difference method to estimate the derivative was chosen to be: \(10^{-3}\).

\begin{table}
\begin{tabular}{l l} \hline \hline Hyperparameter & Value \\ \hline population size & 15000 \\ tournament size & 20 \\ p crossover & 0.6903 \\ p subtree mutation & 0.1330 \\ p hoist mutation & 0.0361 \\ p point mutation & 0.0905 \\ generations & 20 and 30 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyperparameters used in gplearn

### Choice of equations

Equations used in Section 8.1 are canonical equations from physics that often appear in other works about PDE discovery [47]. The homogeneous heat equation is a second-order PDE that models how heat diffuses through a region. It contains the dissipative term \(\partial_{x}^{2}u\). Burgers' equation is second-order PDE used, for instance, in fluid mechanics or nonlinear acoustics [14]. It contains the advection term \(u\partial_{x}u\) and the diffusion term that prevents shock formation. Kuramoto-Sivashinsky equation is a fourth-order PDE used in modelling reaction-diffusion systems [27] and is known for its chaotic behavior [22].

In Section 8.2, we chose equations of physical significance that have an interesting \(\partial\)-free part that is not a linear combination (as discussed in Section 3). That makes them impossible to discover by the current methods. A forced and damped harmonic oscillator is a second-order ODE. Although D-CODE [42] can discover any closed-form first-order ODE, it cannot be used to discover second-order ODEs. Thus there is currently no algorithm capable of discovering this equation. Inhomogeneous heat and wave equations are second-order PDEs where the \(\partial\)-free part is a source (of heat and wave respectively). Moreover, the wave equation does not have the standard evolution form, as it does not involve the \(\partial_{t}\) term. Thus even without the source term, most of the current methods cannot be applied directly to discover this equation.

### Data generation

**Homogeneous heat equation.** The fields were generated by solving the equation \(\partial_{t}u-\theta_{1}\partial_{x}^{2}u=0\) (\(\theta_{1}=0.25\)) with Neumann boundary conditions \(\partial_{x}u(t,0)=\partial_{x}u(t,X)=0\) and an initial condition \(u(0,x)=u_{0}(x)\), where \(u_{0}\) is randomly sampled from a Gaussian process. The equation is solved using the implicit BTCS scheme [25] with steps \(\delta t=0.001\) and \(\delta x=0.001\). The observed field is generated by sampling \((t,x)\in[0,T]\times[0,X]\), evaluating the true field \(u(t,x)\) and adding Gaussian noise. \(T=2\) and \(X=2\) are used in the experiments.

**Burger's equation.** The fields are computed by solving \(\partial_{t}u+u\partial_{x}u-\theta_{1}\partial_{x}^{2}U=0\) (\(\theta_{1}=0.2\)) with an initial condition \(u(0,x)=u_{0}(x)\), where \(u_{0}\) is randomly sampled from a Gaussian process, and with Dirichlet boundary conditions \(u(t,0)=u_{0}(0)\), \(u(t,X)=u_{0}(X)\). The equation is solved the using Crank-Nicolson scheme [57] with steps \(\delta t=0.002\) and \(\delta x=0.002\). The observed field is generated by sampling \((t,x)\in[0,T]\times[0,X]\), evaluating the field \(u(t,x)\) and adding Gaussian noise. \(T=2\) and \(X=2\) are used in the experiments.

**Kuramoto-Sivashinsky equation.** The solution is the same as the one used in [47]. The observed field is generated by sampling \((t,x)\in[0,T]\times[0,X]\), evaluating the field \(u(t,x)\) and adding Gaussian noise. \(T=100\) and \(X=100\) are used in the experiments.

**Damped and forced harmonic oscillator.** The true fields are created by analytically solving the equation \(\partial_{t}^{2}u(t)+2\theta_{1}\theta_{2}\partial_{t}u(t)+\theta_{2}^{2}u(t) =\theta_{3}\sin(\theta_{4}t)\), where \(\theta_{1}=0.5,\theta_{2}=4.0,\theta_{3}=5.0,\theta_{4}=3.0\), with random initial conditions for \(u(0)\) and \(\partial_{t}u(0)\). The observed fields are then created by sampling \(t\in[0,T]\), evaluating \(u(t)\), and adding Gaussian noise. \(T=2\) was used in the experiments.

**Inhomogeneous heat equation.** The true fields are computed by solving \(\partial_{t}u(t,x)-\theta_{1}\partial_{x}^{2}u(t,x)=\theta_{2}e^{\theta_{3}t}\), where \(\theta_{1}=0.25,\theta_{2}=1.25,\theta_{3}=1.8\), with Neumann boundary conditions \(\partial_{x}u(t,0)=\partial_{x}u(t,X)=0\) and an initial condition \(u(0,x)=u_{0}(x)\), where \(u_{0}\) is randomly sampled from a Gaussian process. The equation is solved using the implicit BTCS scheme [25] with steps \(\delta t=0.001\) and \(\delta x=0.001\). The observed field is generated by sampling \((t,x)\in[0,T]\times[0,X]\), evaluating the true field \(u(t,x)\) and adding Gaussian noise. \(T=2\) and \(X=2\) are used in the experiments.

**Inhomogeneous wave equation.** The true fields are computed by solving \(\partial_{t}^{2}u(t,x)-\theta_{1}\partial_{x}^{2}u(t,x)=\theta_{2}e^{t}\sin( \theta_{3}t)\), where \(\theta_{1}=1.0,\theta_{2}=2.0,\theta_{3}=3.0\), with Dirichlet boundary conditions \(u(t,0)=u_{0}(0)\), \(u(t,X)=u_{0}(X)\), where \(u_{0}\) is randomly sampled from a Gaussian process and specifies the initial condition \(u(0,x)=u_{0}(x)\). The equation is solved using the Implicit Difference Method [38] with steps \(\delta t=0.001\) and \(\delta x=0.001\). The observed field was generated by sampling \((t,x)\in[0,T]\times[0,X]\), evaluating the true field \(u(t,x)\) and adding Gaussian noise. \(T=2\) and \(X=2\) were used in the experiments.

### Experimental settings

**Homogeneous heat equation**

Noise ratio (\(\sigma_{R}\)): \(0.001\), \(0.01\), \(0.1\)

Number of samples (\(D\)): \(10\)

Domain (\(\Omega\)): \([0,2]\times[0.2]\)

Grid (\(\mathcal{G}\)): \(\{0,0.07,\ldots,2\}\times\{0,0.07,\ldots,2\}\)

**Burgers' equation**

Noise ratio (\(\sigma_{R}\)): \(0.001\), \(0.01\), \(0.1\)

Number of samples (\(D\)): \(10\)

Domain (\(\Omega\)): \([0,2]\times[0.2]\)

Grid (\(\mathcal{G}\)): \(\{0,0.1,\ldots,2\}\times\{0,0.1,\ldots,2\}\)

**Kuramoto-Sivashinsky equation**

Noise ratio (\(\sigma_{R}\)): \(0.001\), \(0.01\), \(0.1\)

Number of samples (\(D\)): \(1\)

Domain (\(\Omega\)): \([0,100]\times[0.100]\)

Grid (\(\mathcal{G}\)): \(\{0,1.67,\ldots,100\}\times\{0,1.67,\ldots,100\}\)

**Damped and forced harmonic oscillator**

Default values are in bold.

Noise ratio (\(\sigma_{R}\)): \(0.001\), \(0.005\), \(\mathbf{0.01}\), \(0.1\), \(0.2\), \(0.5\)

Number of samples (\(D\)): \(1\), \(2\), \(5\), \(\mathbf{10}\), \(15\)

Domain (\(\Omega\)): \([\mathbf{0,2}]\)

Grid (\(\mathcal{G}\)): \(\{0,0.08,\ldots,2\}\), \(\{\mathbf{0,0.1,\ldots,2\}\), \(\{0,0.13,\ldots,2\}\), \(\{0,0.2,\ldots,2\}\), \(\{0,0.4,\ldots,2\}\)

**Inhomogeneous heat equation**

Noise ratio (\(\sigma_{R}\)): \(0.05\), \(0.1\), \(0.2\)

Number of samples (\(D\)): \(10\)

Domain (\(\Omega\)): \([0,2]\times[0.2]\)

Grid (\(\mathcal{G}\)): \(\{0,0.07,\ldots,2\}\times\{0,0.07,\ldots,2\}\)

**Inhomogeneous wave equation**

Noise ratio (\(\sigma_{R}\)): \(0.001\), \(0.01\), \(0.015\)

Number of samples (\(D\)): \(10\)

Domain (\(\Omega\)): \([0,2]\times[0.2]\)

Grid (\(\mathcal{G}\)): \(\{0,0.07,\ldots,2\}\times\{0,0.07,\ldots,2\}\)

### Benchmarks

We use the implementation of PDE-FIND and WSINDy from the PySINDY library [24, 16].

An important hyperparameter in PDE-FIND and WSINDy is the library \(\Theta\) used. We impose that \(\mathcal{Q}\) and \(\Theta\cup\{\partial_{t}u\}\) have the same number of elements. Moreover, solutions given by PDE-FIND and WSINDy are scaled to have the L1 norm equal to 1. Both of these measures are undertaken to ensure that RMSE error is comparable between the algorithms.

For the homogeneous heat equation and Burgers' equation we use a library

\(\Theta=\{u,\partial_{x},\partial_{x}^{2}u,u\partial_{x}u,u\partial_{x}^{2}u\}\).

For the Kuramoto-Sivashinsky equation, we use a library

\[\Theta=\{u,\partial_{x},\partial_{x}^{2}u,\partial_{x}^{3}u,\partial_{x}^{4}u,u \partial_{x}u,u\partial_{x}^{2}u,u\partial_{x}^{3},u\partial_{x}^{4}\}.\]

In the experiments, we have not optimized for the derivative-free part as it is identically equal to 0 in all equations.

### Correct functional form

To measure success probability we need to establish whether two closed-form functions match. The previous approach [42] considered their _functional forms_, i.e., expressions where all numeric constants are replaced by placeholders. By this measure, functions \(\sin(3x)\) and \(\sin(3.5x)\) match as they have the same functional form \(\sin(Cx)\), where \(C\) is a placeholder.

However, this definition is quite restrictive because functions \(\sin(3x)\), \(\sin(3x)+0.001\), \(1.001\sin(3x)\), and \(\sin(3x+0.001)\) all have different functional forms.

We consider it an open challenge to design a good metric that would meaningfully reflect whether the correct equation is discovered. We propose the following.

For a target function \(f\), we consider its _augmented form_\(\tilde{f}\), defined as \(\tilde{f}(x)=C_{1}f(C_{3}x+C_{4})+C_{2}\), where all \(C_{i}\) are placeholders. Then all numeric constants are turned into placeholders as well. In the end, we combine the constants. For instance, \(C_{1}+C_{2}\) becomes just \(C_{3}\).

As an example, let us consider a function \(f(x)=1.3e^{2x}\). The augmented functional form is created in the following way:

1. Augment: \(C_{1}\times 1.3e^{2\times(C_{3}\times x+C_{4})}+C_{2}\)
2. Replace: \(C_{1}\times C_{5}e^{C_{6}\times(C_{3}\times x+C_{4})}+C_{2}\)
3. Combine: \(C_{1}e^{C_{3}x+C_{4}}+C_{2}\)

We perform this procedure for the target function. We can now take the standard functional form of the candidate function and check whether it matches the augmented functional form of the target function, taking into account that some of the constants might not be present in the candidate expression.

To aid in this procedure, we use a Python library for symbolic mathematics, SymPy [37].

### Computation time

The average computation time for a single experiment with the damped and forced harmonic oscillator is 281 seconds with a standard error of 4.5 seconds. The average computation time for a single experiment with an inhomogeneous heat equation is 68 minutes with a standard error of 38 seconds. This time is measured on a single computer with an Intel Core i5-6500 CPU (4 cores) and 16GB of RAM.

The experiments are run simultaneously on 5 computers like the one described above. The total time for all experiments (all seeds, all equations, all experimental settings, and both versions of D-CIPHER) is 65 hours.

### Licenses

The licenses of the software used in this work are presented in Table 7

## Appendix F Additional experiments and discussion

### Sharpe-Lotka-McKendrick model

We test D-CIPHER on a population model called the Sharpe-Lotka-McKendrick model [59]. The model is described by the following equation

\[\partial_{t}u(t,a)+\partial_{a}u(t,a)+m(a)u(t,a)=0\] (59)where \(m(a)\) is age-specific mortality rate. We choose \(m(a)=2e^{\theta a}\). We set \(\theta=1.5\) in the experiments.

We note that the derivative-free part of the target PDE cannot be expressed as a linear combination of functions from a finite dictionary if the parameters are not known a priori. D-CIPHER is uniquely positioned among other discovery algorithms as the only technique that can recover any mortality rate that can be represented as a closed-form expression. We show a comparison between D-CIPHER and the Ablated D-CIPHER in Table 8

### Discovering systems

Discovering systems of PDEs is a much harder problem than discovering a single PDE. One of the issues is the fact that we would call _indeterminism_. It follows from the following fact. If a vector field \(\bm{u}\) is a solution to two differential equations \(f_{1}\) and \(f_{2}\), i.e.,

\[\begin{split} f_{1}(\bm{x},\bm{u}^{(d)}(\bm{x}),\partial^{[K]} \bm{u}^{(d)}(\bm{x}))&=0\ \forall\bm{x}\in\Omega\\ f_{2}(\bm{x},\bm{u}^{(d)}(\bm{x}),\partial^{[K]}\bm{u}^{(d)}( \bm{x}))&=0\ \forall\bm{x}\in\Omega\end{split}\] (60)

then it is also a solution to any linear combination of these equations, i.e.,

\[\lambda_{1}\times f_{1}(\bm{x},\bm{u}^{(d)}(\bm{x}),\partial^{[K]}\bm{u}^{(d)} (\bm{x}))+\lambda_{2}\times f_{2}(\bm{x},\bm{u}^{(d)}(\bm{x}),\partial^{[K]} \bm{u}^{(d)}(\bm{x}))=0\ \forall\bm{x}\in\Omega\] (61)

for any \(\lambda_{1},\lambda_{2}\in\mathbb{R}\). Moreover, equations can sometimes be differentiated to yield more equations.

Let us take as an example the Cauchy-Riemann equations defined as:

\[\partial_{x}u_{1}-\partial_{y}u_{2}=0\partial_{x}u_{2}+\partial_{y}u_{1}=0\] (62)

Let us assume that we have a true vector field \((u_{1},u_{2})\) that satisfies both equations. Then the following equations are also satisfied

\[\begin{split}\partial_{x}u_{1}-\partial_{y}u_{2}+\partial_{x}u_{ 2}+\partial_{y}u_{1}&=0\\ \partial_{x}u_{1}-\partial_{y}u_{2}-\partial_{x}u_{2}-\partial_{y} u_{1}&=0\end{split}\] (63)

We can also differentiate the Cauchy-Riemann equations to arrive at the Laplacian equations for \(u_{1}\) and \(u_{2}\).

\[\begin{split}\partial_{x}^{2}u_{1}+\partial_{y}^{2}u_{1}=0\\ \partial_{x}^{2}u_{2}+\partial_{y}^{2}u_{2}=0\end{split}\] (64)

\begin{table}
\begin{tabular}{l l l} \hline \hline Software & License \\ \hline gplearn & BSD 3-Clause ”New” or ”Revised” License \\ cvxopt & GNU General Public License \\ cvxpy & Apache License \\ sympy & New BSD License \\ scikit-learn & BSD 3-Clause ”New” or ”Revised” License \\ numpy & liberal BSD license \\ pandas & BSD 3-Clause ”New” or ”Revised” License \\ scipy & liberal BSD license \\ python & Zero-Clause BSD license \\ pysindy & MIT License \\ \hline \hline \end{tabular}
\end{table}
Table 7: Software used and their licenses

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Success Probability} & \multicolumn{2}{c}{Average RMSE} \\  & \(\sigma_{R}=0.001\) & \(0.01\) & \(\sigma_{R}=0.001\) & \(0.01\) \\ \hline D-CIPHER & 0.6 (0.15) & 0.5 (0.16) & 0.007 (0.0008) & 0.008 (0.0011) \\ Abl. D-CIPHER & 0.2 (0.13) & 0.2 (0.13) & 0.017 (0.0009) & 0.017 (0.0008) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Simulation results for the Sharpe-Lotka-McKendrick model. We report the success probability of discovering the \(\partial\)-free part and the Average RMSE of the \(\partial\)-bound part. Standard deviations are shown in bracketsWe could also combine the first-order equations with second-order equations or consider even higher-order derivatives. Although all these equations are compatible with our vector field \((u_{1},u_{2})\), not all of them are equally desirable to discover. That is why we believe that in any algorithm for discovering systems of differential equations substantial expert knowledge or inductive biases have to be encoded to guide the algorithm into the right equations.

Current methods do not consider systems of equations or consider a system of equations of a very particular form. In the latter case, each equation models a derivative with respect to time of a different scalar field. The system is assumed to look like this:

\[\begin{split}&\partial_{t}u_{1}=f_{1}(\bm{x},\bm{u}(\bm{x}), \partial^{[K]}\bm{u}(\bm{x}))\\ &\partial_{t}u_{2}=f_{2}(\bm{x},\bm{u}(\bm{x}),\partial^{[K]}\bm {u}(\bm{x}))\\ &\cdots\\ &\partial_{t}u_{L}=f_{L}(\bm{x},\bm{u}(\bm{x}),\partial^{[K]}\bm {u}(\bm{x}))\end{split}\] (65)

In addition, the LHS of these equations is often assumed to only contain spatial derivatives.

D-CIPHER can be used to discover some systems of differential equations if enough prior knowledge is provided in the choice of the dictionary \(\mathcal{Q}\). Moreover, the discovered equations are not required to have a particular evolution form as is the case in current approaches.

Firstly, we note that D-CODE [42] has been shown to discover a system of equations that looks like Equations 65 when all equations are first-order ODEs. As D-CIPHER reduces to D-CODE when applied to first-order ODEs, it is also capable of discovering such a system.

We demonstrate that D-CIPHER is able to discover both Cauchy-Riemann equations if we use two different dictionaries. Each of the dictionaries yields a different equation. Additionally, it is a well-known fact that if a vector field satisfies Cauchy-Riemann equations then the constituent scalar fields are harmonic, i.e., they satisfy Laplace's equation. Based on the same dataset we are also able to discover both Laplace's equations given another set of two different dictionaries. We note that Laplace's equation does not contain \(\partial_{x}\) term, so most of the current methods cannot be directly applied to discover this equation. The results are presented in Figure 7. The dictionaries used to discover each of the equations are the following.

For \(\partial_{x}u_{1}-\partial_{y}u_{2}=0\) we use \(\mathcal{Q}_{1}=\{\partial_{x}u_{1},\partial_{y}u_{2},\partial_{x}^{2}u_{1}, \partial_{y}u_{2}\}\)

For \(\partial_{x}u_{2}+\partial_{y}u_{1}=0\) we use \(\mathcal{Q}_{2}=\{\partial_{x}u_{2},\partial_{y}u_{1},\partial_{x}^{2}u_{2}, \partial_{y}^{2}u_{1}\}\)

For \(\partial_{x}^{2}u_{1}+\partial_{y}^{2}u_{1}=0\) we use \(\mathcal{Q}_{3}=\{\partial_{x}u_{1},\partial_{y}u_{1},\partial_{x}^{2}u_{1}, \partial_{y}^{2}u_{1}\}\)

For \(\partial_{x}^{2}u_{2}+\partial_{y}^{2}u_{2}=0\) we use \(\mathcal{Q}_{4}=\{\partial_{x}u_{2},\partial_{y}u_{2},\partial_{x}^{2}u_{2}, \partial_{y}^{2}u_{2}\}\)

In the experiments, we have not optimized for the derivative-free part as it is identically equal to 0 in all equations.

### Increasing the size of the dictionary \(\mathcal{Q}\)

We test D-CIPHER on the Sharpe-Lotka-McKendrick model (Equation 59) with different dictionaries. We start with a small dictionary \(\mathcal{Q}_{1}=\{\partial_{t}u,\partial_{a}u\}\), and we create every new dictionary from the previous one by adding one more extended derivative. The final dictionary contains 10 elements, \(\mathcal{Q}_{10}=\{\partial_{t}u,\partial_{a}u,\partial_{a}^{2}u,\partial_{t}^ {2}u,\partial_{t}\partial_{a}u,\partial_{a}(u^{2}),\partial_{a}^{2}(u^{2}), \partial_{t}(u^{2}),\partial_{t}^{2}(u^{2}),\partial_{t}(u^{3}),\partial_{a} (u^{3})\}\). The Average RMSE of the \(\partial\)-bound part is shown in Figure 8. We do not observe any increase in average error. Note that in these experiments, we just focus on the \(\partial\)-bound part and do not optimize the \(\partial\)-free part. The relationship between the computation time and the size of the dictionary is represented in Figure 9.

### Computational complexity

We want to emphasize that PDE discovery is not a time-critical application (usually this process is performed manually by scientists) and we believe D-CIPHER's computation time is acceptable for such a task. In this section, we describe which parts of the algorithms are most computationally intensive.

Computation in D-CIPHER is performed in Step 2 and Step 3.

**Step 2.** Computational complexity of Step 2 depends on the choice of the smoothing algorithm. We want to emphasize that the user can use any smoothing algorithm based on their domain knowledge and experience, including spline regression, LOWESS, and Kalman filters. Gaussian Process has time complexity \(\mathcal{O}(n^{3})\) where \(n\) is the number of data points in a grid \(\mathcal{G}\). D-CIPHER is specifically designed to work for sparse and noisy data, so we have not encountered major computational issues while performing Gaussian process regression. We also note that significant progress has been made in adapting Gaussian processes for datasets with many data points [28].

**Step 3.** D-CIPHER consists of two optimization loops. The outer optimization is performed by a symbolic regression algorithm (in our case genetic programming). The inner optimization is performed by CoLLie (Section 7). Searching through a space of closed-form expression requires testing many candidate equations. D-CIPHER is designed to work with many different algorithms for symbolic regression and it is advised to choose an algorithm that can search through this space most efficiently.

Figure 8: Simulation results for Sharpe-Lotka-McKendrick model. We report the Average RMSE of the \(\partial\)-bound part for different sizes of the dictionary \(\mathcal{Q}\)

Figure 7: Simulation results for both Cauchy-Riemann equations and two Laplace’s equations. We report the Average RMSE of the \(\partial\)-bound part in different noise settings

CoLLie was specifically designed to solve the optimization problem as quickly as possible with a minor accuracy trade-off (see Figure 2). It is based on LARS which has time complexity \(\mathcal{O}(mn^{2})\)[17] where \(m\) is the number of samples and \(n\) is the number of features. In our case, \(n=P\) is usually small as it corresponds to the size of the dictionary and \(m=SD\). In our experiments, \(S\) was set up to 100 and \(D\) up to 10. Overall LARS is performed very quickly. The additional steps in CoLLie require only a few arithmetical operations (linear in \(n\)) and a possible root searching of a single variable function that is efficiently implemented using Brentq algorithm [9].

Other important parts of the algorithm are the numerical integrations. One such integration is performed at the beginning of the algorithm to compute the matrix \(\bm{Z}\) (see Algorithm 1). It does not contribute much to the computation time as it is performed only once. The other integration is performed for each candidate equation to compute vector \(\bm{w}\). Also, substantial time is spent on computing the values of the candidate function \(g\) used in the integration. Fortunately, both of these operations can be implemented as vectorized operations which are designed to run very efficiently on modern hardware. We also discourage overly long equations for \(g\) (check the discussion in Appendix E.3) to limit the number of operations performed.

### Challenges of derivative estimation

One of the advantages of D-CIPHER compared to other methods is the use of the variational formulation of PDEs that allows it to circumvent derivative estimation. This is important as derivative estimation is challenging, especially in noisy settings with infrequent sampling. The problem becomes more pronounced the higher the order of the derivative. To demonstrate these issues, we perform a series of synthetic experiments.

**Qualitative study.** First we qualitatively show how challenging the task of derivative estimation is. We generate an observed trajectory for the damped and forced harmonic oscillator. Then we estimate this trajectory using both Guassian Process regression and Spline regression. As shown in Figure 10 (Panel **A**), the estimated trajectories are very close to the true trajectory. Then we estimate the first derivative (Panel **B**) and the second derivative (Panel **C**). We show the standard finite difference methods as well as derivative estimation techniques using Spline regression and Gaussian Process regression. In both cases we see that the estimated derivatives do not match the ground truth (calculated analytically) as closely as in Panel **A**. Moreover the mismatch for the second derivative seems to be bigger than for the first derivative.

**Quantitative study.** We investigate this relation quantitatively for the damped and forced harmonic oscillator and the wave equation. For the oscillator we generate an observed trajectory and then we estimate its derivatives, up to the fourth order using both finite difference and Gaussian Processes. We then compare the derivatives with the analytically calculated ground truths and measure root

Figure 9: Computation time of the experiments in Appendix F.3 The plot shows how the size of the dictionary \(\mathcal{Q}\) influences the computation time

mean squared error. The results are shown in Figure 11 (Panel **A**). We can see that the error increases the higher the order of the derivative. For the wave equation, we perform a similar experiment but this time we estimate different mixed derivatives. We consider any mixed derivative \(\partial_{t}^{i}\partial_{x}^{j}\), where \(i,j\in\{0,1,2\}\). We demonstrate the results in Figure 11 (Panel **B**). We observe that the error increases the higher the order of the derivative \((i+j)\).

### Challenges of PDE discovery and how we address them

PDE discovery is a very difficult task with many challenges. In Table 9 we summarize some of them and describe how our work addresses them.

### Significance of the new notions

In this section, we want to justify that the new notions we introduce (evolution assumption, linear combination form, derivative-bound part, derivative-fee part, Variational-Ready PDEs) are important theoretical contributions that help us understand the landscape of different PDEs from the machine learning perspective.

The definitions we introduce let us characterize different classes of PDEs. These new notions complement the standard recognized PDE classes such as semi-linear, quasilinear, hyperbolic, etc. These standard classes were introduced predominantly to characterize the solving techniques or the properties of the solutions, whereas the notions we introduce relate to the difficulty of discovering such equations from data.

Figure 11: Panel **A** demonstrates the error between the estimated derivative and the ground truth increases with the order of the derivative (performed for the damped and forced harmonic oscillator). Panel **B** shows that the same happens for the wave equation. The \((i.j)\) entry of the heatmap should be interpreted as the RMSE between the estimated derivative \(\partial_{t}^{i}\partial_{x}^{j}\) and the ground truth.

Figure 10: Panel **A** shows that estimation of the true trajectory can be performed successfully by both Spline regression (Spline) and Gaussian Process regression (GP). Panel **B** shows the estimated first derivative and Panel **C** shows the estimated second derivative. We observe that the higher the order the less accurate is the estimate.

**Significance of linear combination form and the evolution assumption.** In Table 10 we demonstrate how the presence of the two assumptions, the linear combination form (LC) and the evolution assumption (EA), influences the optimization problem. We see that with both assumptions, the problem is relatively straightforward and reduces to sparse linear regression. With only one of the assumptions present the problem becomes more difficult. With neither of these assumptions, the problem becomes very difficult and requires some other assumptions. D-CIPHER does not make either of these assumptions but it assumes the PDE to be of the form described by Equation 13.

**Significance of \(\partial\)-bound part, \(\partial\)-free part and Variational-Ready PDEs.** The difficulty of derivative estimation has been one of the main challenges of PDE discovery. The variational formulation allows to circumvent derivative estimation and thus is more robust to noisy data. Previously, the variational formulation has been applied only to a subset of equations in a linear combination form and with the evolution assumption. We observed that any restrictions that the variational formulation might put on the equation come from the terms containing the derivatives. Thus we define the derivative-bound part and the derivative-free part of the PDE due to their significance for the variational formulation. That allows us to define Variational-Ready PDEs as currently the broadest class of PDEs that admit the variational formulation. We believe it is an important contribution as methods requiring derivative estimation underperform in settings with high noise. This definition outlines the current limits of any method that circumvents derivative estimation in that way.

### Comparison with D-CODE

To clarify our contributions, we compare D-CIPHER to D-CODE [42] which we think is closest in spirit to the algorithm we developed as both of them allow any closed-form derivative-free part and

\begin{table}
\begin{tabular}{l l l l} \hline \hline LC & EA & Equation form & Optimization problem & Examples \\ \hline Yes & Yes & \(\partial_{t}u_{j}=\sum_{p=1}^{P}\theta_{p}f_{p}(x,\bm{u}(\bm{x}),\partial^{[K] }\bm{u}(\bm{x}))\) & Relatively easy. Can be formulated as finding sparse solution to linear least squares (similar to ridge regression) & [10, 47] \\ Yes & No & \(\sum_{p=1}^{P}\theta_{p}f_{p}(x,\bm{u}(\bm{x}),\partial^{[K]}\bm{u}(\bm{x}))=0\) & Medium difficulty. Can be formulated as \(P\) separate linear least squares problems as above & [23] \\ No & Yes & \(\partial_{t}u_{j}=g(\bm{x},\bm{u}(x))\) & Medium difficulty. Find \(g\) using symbolic regression & [42] \\ No & No & \(f(\bm{x},\bm{u}(\bm{x}),\partial^{[K]}\bm{u}(\bm{x}))=0\) & Very difficult. Requires other assumptions (in this work, Equation 13) & D-CIPHER \\ \hline \hline \end{tabular}
\end{table}
Table 10: This table demonstrates how the presence of the two assumptions, the linear combination form (LC) and the evolution assumption (EA), influences the optimization problem.

\begin{table}
\begin{tabular}{l l} \hline \hline Challenge & Novel contribution (how we address the challenge) \\ \hline Noisy measurements & We use the variational loss function (Equation 11) \\ Large diversity of PDEs & We allow any closed-form \(\partial\)-free part and require no evolution assumption (Section 3 \\ Learning compact equations & We penalize long \(\partial\)-free parts and use L1 normalization for the \(\partial\)-bound part (Section 6, Appendix E.3 \\ Efficient search & We develop a quick algorithm, CoLLie, for the inner optimization (Section 7 \\ Chaotic systems & We do _not_ estimate the initial conditions [11] or perform forward time stepping [29] which are computationally unstable for chaotic systems \\ \hline \hline \end{tabular}
\end{table}
Table 9: Some challenges of PDE discovery and how we address themuse variational formulation to circumvent derivative estimation. We also note how the new notions we introduce help us compare the two works.

Algorithm presented in D-CODE [42] can discover any first-order explicit closed-form ODE, i.e., an equation of the form.

\[\partial_{t}u_{j}(t)=g(\bm{u}(t),t)\]

where \(g\) is a closed-form function. It uses the variational formulation of ODEs to circumvent derivative estimation.

D-CODE [42] can be considered a special case of D-CIPHER. We can recover it from D-CIPHER by choosing the dictionary to contain only one element, i.e., \(\mathcal{Q}=\partial_{t}u_{j}\).

As the derivative-bound part is fixed, every ODE of that form admits the variational formulation. This is not true for PDEs as there are derivative-bound parts that might prohibit the variational formulation. Thus D-CIPHER required careful consideration of the appropriate class of equations to search over.

As D-CIPHER needs to find both the \(\partial\)-free part (function \(g\)) and the \(\partial\)-bound part, the optimization problem is much more complicated. That is why we restrict the derivative-bound part of the PDE to be spanned by terms from the pre-specified dictionary and develop an efficient optimization algorithm, CoLLie. We emphasize that, as is the case for [42], we do not put any constraints on the derivative-free part of the PDE, apart from it being closed-form.

### Error bounds

While we would like to have error bounds for the discovered PDE, we note that the problem we solve is significantly more difficult than the one considered in other works. The space of PDEs we consider is much more complex than the space of PDEs in a linear combination form. In other works (e.g., [47], [35]), the PDE is basically a vector in \(\mathbb{R}^{P}\) and the discovery task is mostly reduced to finding a sparse enough vector that approximately solves a certain linear equation. Of course, there is a lot of literature that aids in establishing error bounds in such problem settings. However, D-CIPHER searches over a space \(\mathbb{R}^{P}\times CFE(M+N)\), where \(CFE(M+N)\) is a space of closed-form expressions in \(M+N\) variables. This space is combinatorial in the functional form and continuous in real constants. This makes it very challenging to derive any error bounds.

Figure 12: Comparison of different estimation algorithms that can be used in D-CIPHER. GP - Gaussian Process regression, Nearest - Nearest point interpolation, Linear - Linear interpolation, Cubic - Cubic interpolation.

\begin{table}
\begin{tabular}{l l l} \hline \hline Property & D-CODE [42] & D-CIPHER \\ \hline Applicable to PDEs & No & Yes \\ Higher-order derivatives & No (only first-order) & Yes \\ Requires evolution assumption & Yes & No \\ Derivative-bound part & Fixed: \(\partial_{t}u_{j}\) & Learned: \(\sum_{p=1}^{P}\beta_{p}\hat{\mathcal{E}}_{p}[\bm{u}]\) \\ Derivative-free part & Any closed-form function & Any closed-form function \\ Derivative estimation & No & No \\ \hline \hline \end{tabular}
\end{table}
Table 11: Comparison between D-CIPHER and D-CODE [42]

### Comparison between different estimation algorithms

Step 2 of D-CIPHER requires estimating the fields. We emphasize that any choice of reconstruction algorithm can be used, and it should be chosen based on the application and domain knowledge. In our experiments, D-CIPHER is implemented using Gaussian Process regression [60]. In this section, we investigate other common interpolation algorithms. We implement D-CIPHER with different estimation algorithms. In particular, we compare Gaussian Process (GP) against: Nearest point interpolation (Nearest), Linear interpolation, and Cubic interpolation (Cubic). The implementation details of these three algorithms can be found in the scipy [56] documentation. The results are presented in Figure 12. We see that the estimation algorithms that produce smoother functions (GP, Cubic) tend to give better results.

### How comprehensive does the dictionary \(\mathcal{Q}\) need to be?

Table 12 shows 17 different differential equations and what terms they need in a dictionary. All of them can be discovered with a dictionary of size of 10, \(\mathcal{Q}=\{\partial_{t}u,\partial_{x}u,\partial_{t}^{2}u,\partial_{t}^{2}u,\partial_{t}^{2}u,\partial_{t}\partial_{x}^{2},\partial_{x}^{3}u,\partial_{x }^{4}u,\partial_{x}(u^{2}),\partial_{x}^{2}(u^{2})\}\), and 9 of those equations can be described with a dictionary containing just 4 terms, \(\mathcal{Q}=\{\partial_{t}u,\partial_{x}u,\partial_{t}^{2}u,\partial_{x}^{2}u\}\). It is thus likely that such small dictionaries are sufficient to discover most of the well-known equations. Many differential equations have similar derivative-bound parts and differ by derivative-free parts. Being able to discover any closed-form derivative-free part is what makes D-CIPHER stand out.

### How to take advantage of the observed derivatives?

D-CIPHER can make use of the observed derivatives (if they are available) by adapting the dictionary. Consider a setting with a dictionary \(\mathcal{Q}=\{\partial_{t}u,\partial_{x}u,\partial_{t}\partial_{x}u,\partial_ {t}^{2}u,\partial_{x}^{2}u\}\). If we happen to have the measurements of \(\partial_{t}u\) then we can introduce a new variable \(v=\partial_{t}u\) and change the dictionary to \(\mathcal{Q}=\{v,\partial_{x}u,\partial_{x}v,\partial_{t}v,\partial_{x}^{2}u\}\). Note, we have performed experiments where the dictionary contains more than one dependent variable in Appendix F.2. With observed derivatives, we can also enlarge the space of Variational-Ready PDEs by allowing \(g\) (\(\partial\)-free part) to depend on \(v\) as well.

### Impact on real-world problems

D-CIPHER is especially useful in discovering governing equations for systems with more than one independent variable. For instance, spatiotemporal data or temporal data structured by age or size.

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline Equation & \(\partial_{t}u\) & \(\partial_{x}u\) & \(\partial_{t}\partial_{x}u\) & \(\partial_{t}^{2}u\) & \(\partial_{x}^{2}u\) & \(\partial_{t}\partial_{x}^{2}u\) & \(\partial_{x}^{3}u\) & \(\partial_{x}^{4}u\) & \(\partial_{x}(u^{2})\) & \(\partial_{x}^{2}(u^{2})\) \\ \hline Heat equation & ✓ & & & & ✓ & & & & & \\ Wave equation & & & & ✓ & & & & & \\ Burger’s equation & ✓ & & & & ✓ & & & & ✓ & \\ SLM model & ✓ & ✓ & & & & & & & \\ Damped harmonic oscillator & ✓ & & & ✓ & & & & & & \\ Kuramoto-Sivashinsky equation & ✓ & & & & ✓ & & & ✓ & ✓ & \\ Benjamin-Bona-Mahony equation & ✓ & ✓ & & & & ✓ & & & ✓ & \\ Boussinesq equation & & & & ✓ & ✓ & & & ✓ & & ✓ \\ Chafee-Infante equation & ✓ & ✓ & & & ✓ & & & & \\ Damped wave equation & ✓ & & & ✓ & ✓ & & & & & \\ Fisher’s equation & ✓ & ✓ & & & & & & & \\ Hunter-Saxon equation & & & & ✓ & & & & & & ✓ \\ Klein-Gordon equation & & & & & ✓ & & & & & \\ Korteweg-De Vries equation & ✓ & & & & & & ✓ & & & \\ Liouville’s equation & & & & ✓ & ✓ & & & & \\ Sine-Gordon equation & & & & ✓ & ✓ & & & & & \\ Sinh-Gordon equation & & & & ✓ & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 12: This table lists various Variational-Ready PDEs (some of which are discussed in the paper) and shows which extended derivative terms they require. All of them can be discovered with a dictionary of size of 10, and 9 of them can be described with a dictionary containing just 4 terms. It is thus likely that such small dictionaries are sufficient to discover most of the well-known equations.

In particular, we envision D-CIPHER to be useful in modeling spatiotemporal physical systems, population models, and epidemiological models.

**Spatiotemporal physical systems.** D-CIPHER may prove useful in discovering equations governing the oceans or the atmosphere. For instance, some places actively add or remove CO2 from the atmosphere. These "sources" and "sinks" are likely to be described by a \(\partial\)-free part which D-CIPHER is specially equipped to discover. Similarly, with the ocean temperature where \(\partial\)-free part can describe a heat source. Another area of application can be modeling seismic waves across the earth's crust. Here the \(\partial\)-free part can describe the vibration source (e.g., an earthquake).

**Population models.** Population models can be used in agriculture to determine the harvest or for pest control to predict their impact on the crop. They have also been used in environmental conservation to model the population of endangered species. Population models have also been used in modeling the growth of cells to better understand tumor growth. Moreover, understanding the evolution of a population pyramid for a specific country may prove invaluable in ensuring its economic stability. As in all these scenarios, the rates of growth and mortality are likely to be described by \(\partial\)-free part, D-CIPHER is uniquely positioned to discover such equations as an aid to human experts.

**Epidemiological models.** Epidemiological models are crucial during a pandemic for better planning and interventions. For many diseases, the rates of mortality and infection are age-dependent. Thus, modeling the spread of disease using PDEs (rather than ODEs) might provide superior results.

### D-CIPHER in practice

Below we discuss the things to consider while using D-CIPHER.

**The order of the differential equation.** One of the first considerations should be to choose the order of the differential equation \(K\). For many dynamical systems, \(K=2\) is sufficient unless we expect very complicated behavior. Then, considering \(K=3\) or even \(K=4\) may be warranted. Note, that we show that D-CIPHER can discover a fourth-order PDE (Kuramoto-Sivashinsky equation) in Section 8.

**Homogeneous equations.** Before searching through the whole space of closed-form \(g\) (derivative-free parts), we can consider whether the equation we want to discover may be homogeneous. These experiments on the restricted search space can provide quick insights before searching through all closed-form derivative-free parts.

**Terms in a dictionary.** For a given order of a differential equation \(K\), it is a good idea to include all standard differential operators up to order \(K\) acting on all the variables. For instance, for \(K=2\) and \(M=1+1\) we could choose \(\mathcal{Q}=\{\partial_{t}u,\partial_{x}u,\partial_{t}\partial_{x}u,\partial_ {t}^{2}u,\partial_{x}^{2}u\}\). That allows to cover all linear PDEs with constant coefficients up to that order. To allow for non-linear PDEs we can include a term like \(\partial_{t}(u^{2})=2u\partial_{t}u\) that often describes advection (as in Burger's equation).

**Dictionary steering when dealing with many dependent variables.** When we deal with a system of PDEs rather than a single PDE choosing a dictionary is increasingly important. As we explain in Appendix F.2, discovering whole systems of PDEs is very challenging and D-CIPHER is not designed to do so out of the box. However, we show how that can be done in certain situations. We can steer what kind of equations are discovered by choosing the terms in the dictionary.

**Estimation algorithm.** Estimation algorithms make different assumptions on the data-generating process and should be chosen based on domain expertise. As we show in Appendix F.10, algorithms that produce smoother functions, such as Gaussian Process regression and cubic spline interpolation, tend to have good results. We can consider the advantages and disadvantages of these methods. For instance, Gaussian Process regression works very well for smooth signals. However, it is computationally intensive and might not perform well if the signal is not smooth enough (it has abrupt changes). Spline interpolation, on the other hand, is faster and more appropriate for less smooth signals, but it might introduce certain unwanted artifacts because of using cubic polynomials to interpolate the data.