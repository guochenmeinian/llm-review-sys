# Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning

 Shengyi Huang\({}^{1,2}\)1 &Quentin Gallouedec\({}^{1,3}\)1 &Florian Felten\({}^{4}\) &Antonin Raffin\({}^{5}\)

**Rousslan Fernand Julien Dossa\({}^{6}\)** &Yanxiao Zhao\({}^{7,8}\) &Ryan Sullivan\({}^{9}\) &Viktor Makoviychuk\({}^{10}\)

**Denys Makoviichuk\({}^{11}\)** &Mohamad H. Danesh\({}^{12}\) &Cyril Roumegous\({}^{13}\) &Jiayi Weng

**Chufan Chen\({}^{14}\)** &Md Masudur Rahman\({}^{15}\) &Joao G. M. Araujo\({}^{16}\) &Guorui Quan\({}^{17}\)

**Daniel C.H. Tan\({}^{18,19}\)** &Timo Klein\({}^{20,21}\) &Rujikorn Charakorn\({}^{22}\) &Mark Towers\({}^{23}\)

**Yann Berthelot\({}^{24,25}\)** &Kinal Mehta\({}^{26}\) &Dipam Chakraborty\({}^{27}\) &Arjun KG

**Valentin Charraut\({}^{28}\)** &Chang Ye\({}^{29}\) &Zichen Liu\({}^{30}\) &Lucas N. Alegre\({}^{31}\) &Alexander Nikulin\({}^{32}\)

**Xiao Hu\({}^{33}\)** &Tianlin Liu\({}^{34}\) &Jongwook Choi\({}^{35}\) &Brent Yi\({}^{36}\)**

Footnote 1: Equal contributions

###### Abstract

In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark (ORLB), a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. ORLB is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. It covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, ORLB comes with a command-line interface (CLI) for easy fetching and generating figures to present the results. In this document, we include two case studies to demonstrate the usefulness of ORLB in practice. To the best of our knowledge, ORLB is the first RL benchmark of its kind, and the authors hope that it will improve and facilitate the work of researchers in the field.

## 1 Introduction

Reinforcement Learning (RL) research is based on comparing new methods to baselines to assess progress (Patterson et al., 2023). This process requires the availability of the data associated with these baselines (Raffin et al., 2021) or, alternatively, the ability to replicate them and generate the data oneself (Raffin, 2020). In addition, reproducible results allow the methods to be compared with new benchmarks and to identify the areas in which the methods excel and those in which they are likely to fail, thus providing avenues for future research.

In practice, the RL research community faces complex challenges in comparing new methods with reference data. The unavailability of reference data requires researchers to reproduce experiments, which is difficult due to insufficient source code documentation and evolving software dependencies.

Implementation details, as highlighted in past research, can significantly impact results (Henderson et al., 2018; Huang et al., 2022). Moreover, limited computing resources play a crucial role, hindering the reproduction process and affecting researchers without substantial access.

The lack of standardized metrics and benchmarks across studies not only impedes comparison but also results in a substantial waste of time and resources. To address these issues, the RL community must establish rigorous reproducibility standards, ensuring replicability and comparability across studies. Transparent sharing of data, code, and experimental details, along with the adoption of consistent metrics and benchmarks, would collectively enhance the evaluation and progression of RL research, ultimately accelerating advancements in the field.

ORLB presents a rich collection of tracked RL experiments and aims to set a new standard by providing a diverse training dataset. This initiative prioritizes the use of existing data over re-running baselines, emphasizing reproducibility and transparency. Our contributions are:

* **Extensive dataset:** Offers a large, diverse collection of tracked RL experiments.
* **Standardization:** Establishes a new norm by encouraging reliance on existing data, reducing the need for re-running baselines.
* **Comprehensive metrics:** Includes diverse tracked metrics for method-specific and system evaluation, in addition to episodic return.
* **Reproducibility:** Emphasizes clear instructions and fixed dependencies, ensuring easy experiment replication.
* **Resource for research:** Serves as a valuable and collaborative resource for RL research.
* **Facilitating exploration:** Enables reliable exploration and assessment of new and exisiting RL methods.

## 2 Comprehensive overview of ORLB: content, methodology, tools, and applications

This section provides a detailed exploration of the contents of ORLB, including its diverse set of libraries and environments, and the metrics it contains. We also look at the practical aspects of using ORLB, highlighting its ability to ensure accurate reproducibility and facilitate the creation of data visualizations thanks to its CLI.

### Content

ORLB data is stored and shared with Weights and Biases (Biewald, 2020). The data is contained in a common entity named openrlbenchmark. Runs are divided into several _projects_. A project can correspond to a library, but it can also correspond to a set of more specific runs, such as envpool-cleanrl in which we find CleanRL runs (Huang et al., 2022) launched with the EnvPool

Figure 1: Example of learning curves obtained with Open RL Benchmark. These compare the episodic returns obtained by different implementations of PPO and DQN on three Atari games.

implementation of environments (Weng et al., 2022). A project can also correspond to a reference implementation, such as TD3 (project sfujim-TD3) or Phasic Policy Gradient (Cobbe et al., 2021) (project phasic-policy-gradient). ORLB also includes reports, which are interactive documents designed to enhance the visualization of selected representations. These reports provide a more user-friendly format for practitioners to share, discuss, and analyze experimental results, even across different projects. Figure 2 shows a preview of one such report.

At the time of writing, ORLB contains nearly 25,000 runs, for a total of 72,000 hours (more than 8 years) of tracking. In the following paragraphs, we present the libraries and environments for which runs are available in ORLB, as well as the metrics tracked.

LibrariesORLB contains runs for several reference RL libraries. These libraries are: abcdRL (Zhao, 2022), Acme (Hoffman et al., 2020), Cleanba (Huang et al., 2023), CleanRL (Huang et al., 2022), jaxrl (Kostrikov, 2021), moolib (Mella et al., 2022), MORL-Baselines (Felten et al., 2023), OpenAI Baselines (Dhariwal et al., 2017), rlgames (Makoviichuk and Makoviychuk, 2021) Stable Baselines3 (Raffin et al., 2021; Raffin, 2020) Stable Baselines Jax (Raffin et al., 2021) and TorchBeast (Kuttler et al., 2019).

EnvironmentsThe runs contained in ORLB cover a wide range of classic environments. They include Atari (Bellemare et al., 2013; Machado et al., 2018), Classic control (Brockman et al., 2016), Box2d (Brockman et al., 2016) and MuJoCo (Todorov et al., 2012) as part of either Gym (Brockman et al., 2016) or Gymnasium (Towers et al., 2023) or EnvPool (Weng et al., 2022). They also include Bullet (Coumans and Bai, 2016), Procgen Benchmark (Cobbe et al., 2020), Fetch environments (Plappert et al., 2018), PandaGym (Gallouedec et al., 2021), highway-env (Leurent, 2018), Minigrid (Chevalier-Boisvert et al., 2023) and MO-Gymnasium (Alegre et al., 2022).

Tracked metricsMetrics are recorded throughout the learning process, consistently linked with a global step indicating the number of interactions with the environment, and an absolute time, which allows to compute the duration of a run. We categorize these metrics into four distinct groups:

* **Training-related metrics:** These are general metrics related to RL learning. This category contains, for example, the average returns obtained, the episode length or the number of collected samples per second.

Figure 2: An example of a report on the Weights and Biases platform, dealing with the contribution of QDagger (Agarwal et al., 2022), and using data from ORLB. The URL to access the report is https://wandb.ai/openrlbenchmark/openrlbenchmark/reports/Atari-CleanRL-s-Qdaggerger–\(\_\)VmlldzoONTG10DY5.

* **Method-specific metrics:** These are losses and measures of key internal values of the methods. For PPO, for example, this category includes the value loss, the policy loss, the entropy or the approximate KL divergence.
* **Evolving configuration parameters:** These are configuration values that change during the learning process. This category includes, for example, the learning rate when there is decay, or the exploration rate (\(\epsilon\)) in the Deep Q-Network (DQN) (Mnih et al., 2013).
* **System metrics:** These are metrics related to system components. These could be GPU memory usage, its power consumption, its temperature, system and process memory usage, CPU usage or even network traffic.

The specific metrics available may vary from one library to another. In addition, even where the metrics are technically similar, the terminology or key used to record them may vary from one library to another. Users are advised to consult the documentation specific to each library for precise information on these measures.

### Everything you need for perfect repeatability

Reproducing experimental results in computational research, as discussed in Section 4.3, is often challenging due to evolving codebases, incomplete hyperparameter listings, version discrepancies, and compatibility issues. Our approach aims to enhance reproducibility by ensuring users can exactly replicate benchmark results. Each experiment includes a complete configuration with all hyperparameters, frozen versions of dependencies, and the exact command, including the necessary random seed, for systematic reproducibility. As a example, CleanRL (Huang et al., 2022b) introduces a unique utility that streamlines the process of experiment replication (see Figure 3). This tool produces the command lines to set up a Python environment with the necessary dependencies, download the run file, and the precise command required for the experiment reproduction. Such an approach to reproduction facilitates research and makes it possible to study in depth unusual phenomena, or cases of rupture2, in learning processes, which are generally ignored in the results presented, either because they are deliberately left out or because they are erased by the averaging process.

Footnote 2: Exemplified in https://github.com/DLR-RM/rl-baselines3-zoo/issues/427

### The CLI for generating figures in one command line

ORLB offers convenient access to raw data from RL libraries on standard environments. It includes a feature for easily extracting and visualizing data in a paper-friendly format, streamlining the process of filtering and extracting relevant runs and metrics for research papers through a single command. The CLI is a powerful tool for generating most metrics-related figures for RL research and notably, all figures in this document were generated using the CLI. The data in ORLB can also be accessed by custom scripts, as detailed in Appendix A.2. Specifically, the CLI integrated into ORLB provides users with the flexibility to:

Figure 3: CleanRL’s module reproduce allows the user to generate, from an ORLB run reference, the exact command suite for an identical reproduction of the run.

* Specify algorithms' implementations (from which library) along with their corresponding git commit or tag;
* Choose target environments for analysis;
* Define the metrics of interest;
* Opt for the additional generation of metrics and plots using RLiable (Agarwal et al., 2021).

Concrete example usage of the CLI and resulting plots are available in Appendix A.1.

## 3 ORLB in action: an insight into case studies

ORLB offers a powerful tool for researchers to evaluate and compare different RL algorithms. In this section, we will explore two case studies that showcase its benefits. First, we propose to investigate the effect of using TD(\(\lambda\)) for value estimation in PPO (Schulman et al., 2017) versus using Monte Carlo (MC). This simple study illustrates the use of ORLB through a classic research question. Moreover, to the best of our knowledge, this question has never been studied in the literature. We then show how ORLB is used to demonstrate the speedup and variance reduction of a new IMPALA implementation proposed by Huang et al. (2023). By using ORLB, we can save time and resources while ensuring consistent and reproducible comparisons. These case studies highlight the role of the benchmark in providing insights that can advance the field of RL research.

### Easily assess the contribution of TD(\(\lambda\)) for value estimation in PPO

In the first case study, we show how ORLB can be used to easily compare the performance of different methods for estimating the value function in PPO (Schulman et al., 2017), one of the many implementation details of this algorithm (Huang et al., 2022a). Specifically, we compare the commonly used Temporal Difference (TD)(\(\lambda\)) estimate to the Monte-Carlo (MC) estimate.

PPO typically employs Generalized Advantage Estimation (GAE) (Schulman et al., 2016) to update the actor. The advantage estimate is expressed as follows:

\[A_{t}^{\mathrm{GAE}(\gamma,\lambda)}=\sum_{l=0}^{N-1}(\gamma\lambda)^{l}\delta _{t+l}^{V}\] (1)

where \(\lambda\in[0,1]\) adjusts the bias-variance tradeoff and \(\delta_{t+l}^{V}=R_{t+l}+\gamma\hat{V}(S_{t+l+1})-\hat{V}(S_{t+l})\). The target return for critic optimization is estimated with TD(\(\lambda\)) as follows:

\[G_{t}^{\lambda}=(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_{t:t+n}\] (2)

where \(G_{t:t+n}=\sum_{k=0}^{n-1}\gamma^{k}R_{t+k+1}+\gamma^{n}V(S_{t+n})\) is the \(n\)-steps return. In practice, the target return for updating the critic is computed from the GAE value, by adding the minibatch return, a detail usually overlooked by practitioners (Huang et al., 2022a, point 5). While previous studies (Patterson et al., 2023) have shown the joint benefit of GAE and over MC estimates for actor and critic, we focus on the value function alone. To isolate the influence of the value function estimation, we vary the method used for the value function and keep GAE for advantage estimation.

The first step is to identify the reference runs in ORLB. Since PPO is a well-known baseline, there are many runs available; we decided to use those from Stable Baselines3 for this example. We then retrieve the exact source code and command used to generate the runs - thanks to the pinned dependencies that come with them - and make the necessary changes to the source code. For each selected environment, we start three learning runs using the same command as the one we retrieved. The runs are saved in a dedicated project3. For fast and user-friendly rendering of the results, we 

[MISSING_PAGE_FAIL:6]

the IMPALA results. However, these shared results are limited to the paper's presented curves, which provide a smoothed measure of episodic return as a function of interaction steps on a specific set of Atari tasks. It is worth noting that these tasks are not an exact match for the widely recognized Atari 57, and the raw data used to generate these curves is unavailable.

Recognizing the lack of raw data for existing IMPALA implementations, the authors reproduced the experiments, tracked the runs and integrated them into ORLB. As a reminder, these logged data include not only the return curves, but also the system configurations and temporal data, which are crucial to support the Cleanba authors' optimization claim. Comparable experiments have been run, tracked and shared on ORLB with the proposed Cleanba implementation.

Using ORLB CLI, the authors generated several figures. In Figure 6, taken from (Huang et al., 2023), the authors show that the results in terms of sample efficiency compare favorably with the baselines, and that for the same system configuration, convergence was temporally faster with the proposed implementation, thus proving claims (1) and (2). Figure 7 demonstrates that Cleanba variants maintain consistent learning curves across different hardware configurations. Conversely, moolib's IMPALA shows marked variability in similar settings, despite identical hyperparameters, confirming the authors' third claim.

## 4 Current practices in RL: data reporting, sharing and reproducibility

Many new methods have emerged in recent years, with some becoming standard baselines, but current practices in the field make it challenging to interpret, compare, and replicate study results. In this section, we highlight the inconsistent presentation of results, focusing on learning curves as an example. This inconsistency can hinder interpretation and lead to incorrect conclusions. We also note the insufficient availability of learning data, despite some positive efforts, and examine challenges related to method reproducibility.

Figure 6: Median human-normalized scores with 95% stratified bootstrap CIs of Cleanba (Huang et al., 2023) variants compared with moolib (Mella et al., 2022) and monobeast (Küttler et al., 2019). The experiments were conducted on 57 Atari games (Bellemare et al., 2013). The data used to generate the figure comes from ORLB, and the figure was generated with a single command from ORLB’s CLI. Figure from (Huang et al., 2023).

Figure 7: Aggregated normalized human scores with stratified 95% bootstrap CIs, showing that unlike moolib (Mella et al., 2022), Cleanba (Huang et al., 2023) variants have more predictable learning curves (using the same hyperparameters) across different hardware configurations. Figure from (Huang et al., 2023).

### Analyzing learning curve practices

Plotting learning curves is a common way to show an agent's performance over learning. We closely examine the components of learning curves and the choices made by key publications. We find a lack of uniformity, with presentation choices rarely explained and sometimes not explicitly stated.

AxisTypically, the \(y\) axis measures either the return acquired during data collection or evaluation. Some older papers, like (Schulman et al., 2015; Mnih et al., 2016; Schulman et al., 2017), fail to specify the metric, using the vague term _learning curve_. The first approach sums the rewards collected during agent rollout (Dabney et al., 2018; Burda et al., 2019). The second approach suspends training, averaging the agent's return over episodes, deactivating exploration elements (Fujimoto et al., 2018; Haarnoja et al., 2018; Hessel et al., 2018; Janner et al., 2019; Badia et al., 2020; Ecoffet et al., 2021; Chen et al., 2021). This method is prevalent and provides a more precise evaluation. Regarding the \(x\) axis, while older baselines (Schulman et al., 2015; Mnih et al., 2016) use policy updates and learning epochs, the norm is to use interaction counts with the environment. In Atari environments, it is often the number of frames, adjusting for frame skipping to match human interaction frequency.

Shaded areaData variability is typically shown with a shaded area, but its definition varies across studies. Commonly, it represents the standard deviation (Chen et al., 2021; Janner et al., 2019) and less commonly half the standard deviation (Fujimoto et al., 2018). Haarnoja et al. (2018) uses a min-max representation to include outliers, covering the entire observed range. This method offers a comprehensive view but amplifies outliers' impact with more runs. Ecoffet et al. (2021) adopts a probabilistic approach, showing a 95% bootstrap confidence interval around the mean, ensuring statistical confidence. Unfortunately, Schulman et al. (2015, 2017); Mnih et al. (2016); Dabney et al. (2018); Badia et al. (2020) omit statistical details or even the shaded area, introducing uncertainty in data variability interpretation, as seen in (Hessel et al., 2018).

Normalization and aggregationPerformance aggregation assesses method results across various tasks and domains, indicating their generality and robustness. Outside the Atari context, aggregation practices are uncommon due to the lack of a universal normalization standard. Without a widely accepted normalization strategy, scores are typically not aggregated, or if they are, it relies on a min-max approach lacking absolute significance and unsuitable for comparisons. Early Atari research did not use normalization or aggregate results (Mnih et al., 2013). There has been a shift towards normalizing against human performance, though this has weaknesses and may not reflect true agent mastery (Toromanoff et al., 2019). Aggregation methods vary: the mean is common but influenced by outliers, leading some studies to prefer the more robust median, as in (Hessel et al., 2018). Many papers now report both mean and median results (Dabney et al., 2018; Hafner et al., 2023; Badia et al., 2020). Recent approaches, like using the Interquartile Mean (IQR), provide a more accurate performance representation across diverse games (Lee et al., 2022), as suggested by Agarwal et al. (2021).

### Spectrum of data sharing practices

While the mentioned studies often have reference implementations (see Section 4.3), the sharing of training data typically extends only to the curves presented in their articles. This necessitates reliance on libraries that replicate these methods, offering benchmarks with varying levels of completeness. Several widely-used libraries in the field provide high-level summaries or graphical representations without including raw data (e.g., Tensorforce (Kuhnle et al., 2017), Garage (garage contributors, 2019), ACME (Hoffman et al., 2020), MushroomRL (D'Eramo et al., 2021), ChainerRL (Fujita et al., 2021), and TorchRL (Bou et al., 2023)). Spinning Up (Achiam, 2018) offers partial data accessibility, providing benchmark curves but withholding raw data. TF-Agent (Guadarrama et al., 2018) is slightly better, offering experiment tracking with links to TensorBoard.dev, though its future is uncertain due to service closure. Tianshou (Weng et al., 2022) provides individual run reward data for Atari and average rewards for MuJoCo, with more detailed MuJoCo data available via a GoogleDrive link, but it is not widely promoted. RLLib (Liang et al., 2018) maintains an intermediate stance in data sharing, hosting run data in a dedicated repository. However, this data is specific to select experiments and often presented in non-standard, undocumented formats, complicating its use. Leading effective data-sharing platforms include Dopamine (Castro et al., 2018) and Sample Factory (Petrenko et al., 2020). Dopamine consistently provides accessible raw evaluation data for various seeds and visualizations, along with trained agents on Google Cloud. Sample Factory offers comprehensive data via Weights and Biases (Biewald, 2020) and a selection of pre-trained agents on the Hugging Face Hub, enhancing reproducibility and collaborative research efforts.

### Review on reproducibility

The literature shows variations in these practices. Some older publications like (Schulman et al., 2015, 2017; Bellemare et al., 2013; Mnih et al., 2016; Hessel et al., 2018) and even recent ones like (Reed et al., 2022) lack a codebase but provide detailed descriptions for replication6. However, challenges arise because certain hyperparameters, important but often unreported, can significantly affect performance (Andrychowicz et al., 2020). In addition, implementation choices have proven to be critical (Henderson et al., 2018; Huang et al., 2023, 2022a; Engstrom et al., 2020), complicating the distinction between implementation-based improvements and methodological advances.

Footnote 6: This section uses the taxonomy introduced by Lynnerup et al. (2019): _repeatability_ means accurately duplicating an experiment with source code and random seed availability, _reproducibility_ involves redoing an experiment using an existing codebase, and _replicability_ aims to achieve similar results independently through algorithm implementation.

Recognizing these challenges, the RL community is advocating for higher standards. NeurIPS, for instance, has been requesting a reproduction checklist since 2019 (Pineau et al., 2021). Recent efforts focus on systematic sharing of source code to promote reproducibility. However, codebases are often left unmaintained post-publication (with rare exceptions (Fujimoto et al., 2018)), creating complexity for users dealing with various dependencies and unsolved issues. To address these challenges, libraries have aggregated multiple baseline implementations (see Section 2.1), aiming to match reported paper performance. However, long-term sustainability remains a concern. While these libraries enhance reproducibility, in-depth repeatability is still rare.

## 5 Discussion and conclusion

Reproducing results in RL research is often difficult due to limited access to data and code, as well as the impact of minor implementation variations on performance. Researchers typically rely on imprecise comparisons with paper figures, making the reproduction process time-consuming and challenging. To address these issues, we introduce ORLB, a large collection of tracked experiments spanning various algorithms, libraries and benchmarks. ORLB records all relevant metrics and data points, offering detailed resources for precise reproduction. This tool facilitates access to comprehensive datasets, simplifies the extraction of valuable information, enables metric comparisons, and provides a CLI for easier data access and visualization. As a dynamic resource, ORLB is regularly updated by both its maintainers and the user community, gradually improving the reliability of the available results.

Despite its strengths, ORLB faces challenges in user-friendliness that need to be addressed. Inconsistencies between libraries in evaluation strategies and terminology can make it difficult for users. Scaling community engagement becomes a challenge with more members, libraries, and runs. The lack of Git-like version tracking for runs adds to these limitations.

ORLB is a major step forward in addressing the needs of RL research. It offers a comprehensive, accessible, and collaborative experiment database, enabling precise comparisons and analysis. It improves data access and promotes a deeper understanding of algorithmic performance. While challenges remain, ORLB has the potential to raise the standard of RL research.

## Affiliations

\({}^{1}\)Hugging Face

\({}^{2}\)Drexel University

\({}^{3}\)Univ. Lyon, Centrale Lyon, CNRS, INSA Lyon, UCBL, LIRIS, UMR 5205

\({}^{4}\)SnT, University of Luxembourg

\({}^{5}\)German Aerospace Center (DLR) RMC, Wessling, Germany

\({}^{6}\)Graduate School of System Informatics, Kobe University, Hyogo, Japan

\({}^{7}\)School of Computer Science and Technology, University of Chinese Academy of Sciences

\({}^{8}\)Chengdu Institute of Computer Applications, Chinese Academy of Sciences

\({}^{9}\)University of Maryland, College Park

\({}^{10}\)NVIDIA

\({}^{11}\)Snap Inc.

\({}^{12}\)School of Computer Science, McGill University

\({}^{13}\)Polytech Montellier DO

\({}^{14}\)Zhejiang University

\({}^{15}\)Department of Computer Science, Purdue University

\({}^{16}\)Work done while at Cohere

\({}^{17}\)Chinese University of Hong Kong, Shenzhen

\({}^{18}\)University College London

\({}^{19}\)Agency for Science, Technology and Research

\({}^{20}\)Faculty of Computer Science, University of Vienna, Vienna, Austria

\({}^{21}\)UniVie Doctoral School Computer Science, University of Vienna

\({}^{22}\)Vidyasirimedhi Institute of Science and Technology (VISTEC)

\({}^{23}\)University of Southampton

\({}^{24}\)Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIS4AL

\({}^{25}\)Saint-Gobain Research Paris

\({}^{26}\)International Institute of Information Technology, Hyderabad, India

\({}^{27}\)Alcrowd SA

\({}^{28}\)Valeo Driving Assistance Research

\({}^{29}\)New York University

\({}^{30}\)Sea AI Lab

\({}^{31}\)Institute of Informatics, Federal University of Rio Grande do Sul

\({}^{32}\)AIRI

\({}^{33}\)Department of Automation, Tsinghua University

\({}^{34}\)University of Basel

\({}^{35}\)University of Michigan

\({}^{36}\)UC Berkley

## Acknowledgments

This work has been supported by a highly committed RL community. We have listed all the contributors to date, and would like to thank all future contributors and users in advance.

This work was granted access to the HPC resources of IDRIS under the allocation 2022- [AD011012172R1] made by GENCI. The MORL-Baselines experiments have been conducted on the HPCs of the University of Luxembourg, and of the Vrije Universiteit Brussel. This work was partly supported by the National Key Research and Development Program of China (2023YFB3308601), Science and Technology Service Network Initiative (KFJ-STS-QYZD-2021-21-001), the Talents by Sichuan provincial Party Committee Organization Department, and Chengdu - Chinese Academy of Sciences Science and Technology Cooperation Fund Project (Major Scientific and Technological Innovation Projects). Some experiments are conducted at Stability AI and Hugging Face's cluster.

## References

* Achiam [2018] Joshua Achiam. Spinning Up in Deep Reinforcement Learning. https://github.com/openai/spinningup, 2018. URL https://github.com/openai/spinningup.
* Agarwal et al. [2021] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Bellemare. Deep Reinforcement Learning at the Edge of the Statistical Precipice. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pp. 29304-29320, 2021.
* December 9, 2022_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/balc5356d9164bb64c446a4b690226b0-Abstract-Conference.html.
* Alegre et al. [2022] Lucas N. Alegre, Florian Felten, El-Ghazali Talbi, Gregoire Danoy, Ann Nowe, Ana L. C. Bazzan, and Bruno C. da Silva. MO-Gym: A Library of Multi-Objective Reinforcement Learning Environments. In _Proceedings of the 34th Benelux Conference on Artificial Intelligence BNAIC/Benelearn 2022_, 2022.
* Andrychowicz et al. [2020] Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem. What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study. _arXiv preprint arXiv:2006.05990_, 2020.
* Badia et al. [2020a] Adria Puigdomenech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the Atari Human Benchmark. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pp. 507-517. PMLR, 2020a. URL http://proceedings.mlr.press/v119/badia20a.html.
* Badia et al. [2020b] Adria Puigdomenech Badia, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, and Charles Blundell. Never Give Up: Learning Directed Exploration Strategies. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020b. URL https://openreview.net/forum?id=Sye57xStvB.
* Bellemare et al. [2013] Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An Evaluation Platform for General Agents. _Journal of Artificial Intelligence Research_, 47:253-279, 2013. doi: 10.1613/JAIR.3912. URL https://doi.org/10.1613/jair.3912.
* Biewald [2020] Lukas Biewald. Experiment Tracking with Weights and Biases, 2020. URL https://www.wandb.com/. Software available from wandb.com.
* Bou et al. [2023] Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang, Gianni De Fabritiis, and Vincent Moens. TorchRL: A Data-Driven Decision-Making Library for Pytorch. _arXiv preprint arXiv:2306.00577_, 2023.
* Brockman et al. [2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. _arXiv preprint arXiv:1606.01540_, 2016.
* Burda et al. [2019] Yuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network distillation. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=H11JJnR5Ym.
* Burda et al. [2020]Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare. Dopamine: A Research Framework for Deep Reinforcement Learning. _arXiv preprint arXiv:1812.06110_, 2018.
* Chen et al. [2021] Xinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross. Randomized Ensembled Double Q-Learning: Learning Fast Without a Model. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL https://openreview.net/forum?id=AY8zfZm0tDd.
* Chevalier-Boisvert et al. [2023] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks. _arXiv preprint arXiv:2306.13831_, 2023.
* Cobbe et al. [2020] Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging Procedural Generation to Benchmark Reinforcement Learning. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pp. 2048-2056. PMLR, 2020. URL http://proceedings.mlr.press/v119/cobbe20a.html.
* Cobbe et al. [2021] Karl Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic Policy Gradient. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pp. 2020-2027. PMLR, 2021. URL http://proceedings.mlr.press/v139/cobbe21a.html.
* Coumans and Bai [2016] Erwin Coumans and Yunfei Bai. PyBullet, a Python Module for Physics Simulation for Games, Robotics and Machine Learning. 2016.
* Dabney et al. [2018] Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit Quantile Networks for Distributional Reinforcement Learning. In Jennifer G. Dy and Andreas Krause (eds.), _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pp. 1104-1113. PMLR, 2018. URL http://proceedings.mlr.press/v80/dabney18a.html.
* D'Eramo et al. [2021] Carlo D'Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. MushroomRL: Simplifying Reinforcement Learning Research. _Journal of Machine Learning Research_, 22(131):1-5, 2021. URL http://jmlr.org/papers/v22/18-056.html.
* Dhariwal et al. [2017] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OpenAI Baselines. https://github.com/openai/baselines, 2017. URL https://github.com/openai/baselines.
* Ecoffet et al. [2021] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. First Return, Then Explore. _Nature_, 590(7847):580-586, 2021. doi: 10.1038/S41586-020-03157-9. URL https://doi.org/10.1038/s41586-020-03157-9.
* Engstrom et al. [2020] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janos, Larry Rudolph, and Aleksander Madry. Implementation Matters in Deep RL: A Case Study on PPO and TRPO. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL https://openreview.net/forum?id=r1etN1rtPB.
* Espeholt et al. [2020] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. In Jennifer G. Dy and Andreas Krause (eds.), _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pp. 1406-1415. PMLR, 2018. URL http://proceedings.mlr.press/v80/espeholt18a.html.
* Felten et al. [2023] Florian Felten, Lucas Nunes Alegre, Ann Nowe, Ana L. C. Bazzan, El Ghazali Talbi, Gregoire Danoy, and Bruno Castro da Silva. A Toolkit for Reliable Benchmarking and Research in Multi-Objective Reinforcement Learning. In _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 3, NeurIPS Datasets and Benchmarks 2023_, 2023. URL https://openreview.net/forum?id=jfwRLudQyj.
* Fujimoto et al. [2018] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing Function Approximation Error in Actor-Critic Methods. In Jennifer G. Dy and Andreas Krause (eds.), _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pp. 1582-1591. PMLR, 2018. URL http://proceedings.mlr.press/v80/fujimoto18a.html.
* Fujita et al. [2021] Yasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, and Takahiro Ishikawa. ChainerRL: A Deep Reinforcement Learning Library. _Journal of Machine Learning Research_, 22(77):1-14, 2021. URL http://jmlr.org/papers/v22/20-376.html.
* Gallouedec et al. [2021] Quentin Gallouedec, Nicolas Cazin, Emmanuel Dellandrea, and Liming Chen. panda-gym: Open-Source Goal-Conditioned Environments for Robotic Learning. _4th Robot Learning Workshop: Self-Supervised and Lifelong Learning at NeurIPS_, 2021.
* [25] The garage contributors. Garage: A toolkit for reproducible reinforcement learning research. https://github.com/rlworkgroup/garage, 2019.
* Guadarrama et al. [2018] Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman, Ke Wang, Ekaterina Gonina, Neal Wu, Efi Kokiopoulou, Luciano Sbaiz, Jamie Smith, Gabor Bartok, Jesse Berent, Chris Harris, Vincent Vanhoucke, and Eugene Brevdo. TF-Agents: A library for Reinforcement Learning in TensorFlow. https://github.com/tensorflow/agents, 2018. URL https://github.com/tensorflow/agents.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In Jennifer G. Dy and Andreas Krause (eds.), _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pp. 1856-1865. PMLR, 2018. URL http://proceedings.mlr.press/v80/haarnoja18b.html.
* Hafner et al. [2023] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering Diverse Domains through World Models. _arXiv preprint arXiv:2301.04104_, 2023.
* Henderson et al. [2018] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep Reinforcement Learning That Matters. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018_, pp. 3207-3214. AAAI Press, 2018. doi: 10.1609/AAAI.V32I1.11694. URL https://doi.org/10.1609/aaai.v32i1.11694.
* Hessel et al. [2018] Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining Improvements in Deep Reinforcement Learning. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018_, pp. 3215-3222. AAAI Press, 2018. doi: 10.1609/AAAI.V32I1.11796. URL https://doi.org/10.1609/aaai.v32i1.11796.
* Hessel et al. [2018]Matthew W. Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev, Danila Sinopalnikov, Piotr Stanczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, Leonard Hussenot, Robert Dadashi, Gabriel Dulac-Arnold, Manu Orsini, Alexis Jacq, Johan Ferret, Nino Vieillard, Seyed Kamyar Seyed Ghasemipour, Sertan Girgin, Olivier Pietquin, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Abe Friesen, Ruba Haroun, Alex Novikov, Sergio Gomez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A Research Framework for Distributed Reinforcement Learning. _arXiv preprint arXiv:2006.00979_, 2020.
* Huang et al. [2022a] Shengyi Huang, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and Weixun Wang. The 37 Implementation Details of Proximal Policy Optimization. In _ICLR Blog Track_, 2022a. URL https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/. https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/.
* Huang et al. [2022b] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and Joao G.M. Araujo. CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms. _Journal of Machine Learning Research_, 23(274):1-18, 2022b. URL http://jmlr.org/papers/v23/21-1342.html.
* Huang et al. [2023] Shengyi Huang, Jiayi Weng, Rujikorn Charakorn, Min Lin, Zhongwen Xu, and Santiago Ontanon. Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform, 2023.
* Janner et al. [2019] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to Trust Your Model: Model-Based Policy Optimization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pp. 12498-12509, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/5faf461eff3099671ad63c6f3f094f7f-Abstract.html.
* Kostrikov [2021] Ilya Kostrikov. JAXRL: Implementations of Reinforcement Learning algorithms in JAX. https://github.com/ikostrikov/jaxrl, Oct 2021. URL https://github.com/ikostrikov/jaxrl.
* Kuhnle et al. [2017] Alexander Kuhnle, Michael Schaarschmidt, and Kai Fricke. Tensorforce: a TensorFlow library for applied reinforcement learning. https://github.com/tensorforce/tensorforce, 2017. URL https://github.com/tensorforce/tensorforce.
* Kuttler et al. [2019] Heinrich Kuttler, Nantas Nardelli, Thibaut Lavril, Marco Selvatici, Viswanath Sivakumar, Tim Rocktaschel, and Edward Grefenstette. TorchBeast: A PyTorch Platform for Distributed RL. _arXiv preprint arXiv:1910.03552_, 2019.
* December 9, 2022_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/b2cac94f82928a85055987d9f4d4753f-Abstract-Conference.html.
* Leutner [2018] Edouard Leutner. An Environment for Autonomous Driving Decision-Making. https://github.com/eleuvent/highway-env, 2018. URL https://github.com/eleuvent/highway-env.
* Liang et al. [2019] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for Distributed Reinforcement Learning. In Jennifer G. Dy and Andreas Krause (eds.), _Proceedings of the 35th InternationalConference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pp. 3059-3068. PMLR, 2018. URL http://proceedings.mlr.press/v80/liang18b.html.
* November 1, 2019, Proceedings_, volume 100 of _Proceedings of Machine Learning Research_, pp. 466-489. PMLR, 2019. URL http://proceedings.mlr.press/v100/lynnerup20a.html.
* Machado et al. [2018] Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and Michael Bowling. Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents. _Journal of Artificial Intelligence Research_, 61:523-562, 2018. doi: 10.1613/JAIR.5699. URL https://doi.org/10.1613/jair.5699.
* Makoviichuk and Makoviychuk [2021] Denys Makoviichuk and Viktor Makoviychuk. rl-games: A High-performance Framework for Reinforcement Learning. https://github.com/Denys88/rl_games, May 2021. URL https://github.com/Denys88/rl_games.
* Mella et al. [2022] Vegard Mella, Eric Hambro, Danielle Rothermel, and Heinrich Kuttler. moolib: A Platform for Distributed RL. _GitHub repository_, 2022. URL https://github.com/facebookresearch/moolib.
* Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing Atari with Deep Reinforcement Learning. _arXiv preprint arXiv:1312.5602_, 2013.
* Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), _Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016_, volume 48 of _JMLR Workshop and Conference Proceedings_, pp. 1928-1937. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/mniha16.html.
* Patterson et al. [2023] Andrew Patterson, Samuel Neumann, Martha White, and Adam White. Empirical Design in Reinforcement Learning. _arXiv preprint arXiv:2304.01315_, 2023.
* Petrenko et al. [2020] Aleksei Petrenko, Zhehui Huang, Tushar Kumar, Gaurav S. Sukhatme, and Vladlen Koltun. Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pp. 7652-7662. PMLR, 2020. URL http://proceedings.mlr.press/v119/petrenko20a.html.
* Pineau et al. [2021] Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Lariviere, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Hugo Larochelle. Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program). _Journal of Machine Learning Research_, 22:164:1-164:20, 2021. URL http://jmlr.org/papers/v22/20-303.html.
* Plappert et al. [2018] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research. _arXiv preprint arXiv:1802.09464_, 2018.
* Raffin [2020] Antonin Raffin. RL Baselines3 Zoo. https://github.com/DLR-RM/rl-baselines3-zoo, 2020.

Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-Baselines3: Reliable Reinforcement Learning Implementations. _Journal of Machine Learning Research_, 22(268):1-8, 2021.
* Reed et al. [2022] Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A Generalist Agent. _Transactions on Machine Learning Research_, 2022, 2022. URL https://openreview.net/forum?id=1ikKOkHjvj.
* Schulman et al. [2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust Region Policy Optimization. In Francis R. Bach and David M. Blei (eds.), _Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015_, volume 37 of _JMLR Workshop and Conference Proceedings_, pp. 1889-1897. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/schulman15.html.
* Schulman et al. [2016] John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-Dimensional Continuous Control Using Generalized Advantage Estimation. In Yoshua Bengio and Yann LeCun (eds.), _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings_, 2016. URL http://arxiv.org/abs/1506.02438.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Todorov et al. [2012] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A Physics Engine for Model-Based Control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012_, pp. 5026-5033. IEEE, 2012.
* Toromanoff et al. [2019] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. Is Deep Reinforcement Learning Really Superhuman on Atari? _arXiv preprint arXiv:1908.04683_, 2019.
* Towers et al. [2023] Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu, Manuel Goulao, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierre, Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium, March 2023. URL https://zenodo.org/record/8127025.
* Weng et al. [2022a] Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: A Highly Modularized Deep Reinforcement Learning Library. _Journal of Machine Learning Research_, 23(267):1-6, 2022a. URL http://jmlr.org/papers/v23/21-1127.html.
* Weng et al. [2022b] Jiayi Weng, Min Lin, Shengyi Huang, Bo Liu, Denys Makoviichuk, Viktor Makoviychuk, Zichen Liu, Yufan Song, Ting Luo, Yukun Jiang, Zhongwen Xu, and Shuicheng Yan. EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine. In _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 2, NeurIPS Datasets and Benchmarks 2022_, 2022b. URL http://papers.nips.cc/paper_files/paper/2022/hash/8caaf08e49ddbad6694fae067442ee21-Abstract-Datasets_and_Benchmarks.html.
* Zhao [2022] Yanxiao Zhao. abcdRL: Modular Single-file Reinforcement Learning Algorithms Library. https://github.com/sdpkjc/abcdrl, December 2022. URL https://github.com/sdpkjc/abcdrl.

## Checklist

1. For all authors...

* Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]
* Did you describe the limitations of your work? [Yes] see Section 5.
* Did you discuss any potential negative societal impacts of your work? [No]
* Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
* If you are including theoretical results...
* Did you state the full set of assumptions of all theoretical results? [N/A]
* Did you include complete proofs of all theoretical results? [N/A]
* If you ran experiments (e.g. for benchmarks)...
* Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The paper deals specifically with new ways of sharing experimental results to improve reproducibility.
* Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]
* Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes]
* Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
* If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
* If your work uses existing assets, did you cite the creators? [Yes]
* Did you mention the license of the assets? [Yes]
* Did you include any new assets either in the supplemental material or as a URL? [Yes] Each experiment on ORLB is carefully linked to the source code needed to produce it.
* Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] Data is collected by proactive contributors
* Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
* If you used crowdsourcing or conducted research with human subjects...
* Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]
* Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
* Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]
Plotting results guidelines

### Using the CLI

This section gives notable additional examples of usage of the provided CLI. A more comprehensive set of examples and manual is available in the README page of the project.

#### a.1.1 Plotting episodic return from various libraries

First, we showcase the most basic usage of the CLI, that is comparing two different implementations of the same algorithm based on learning curve of episodic return. For example, Figure 8 and 9 compare CleanRL's TD3 implementation against the original TD3, both in terms of sample efficiency and time. The command used to generate this plot is listed below.

In the above command, wpn denotes the project name, typically the learning library name. This allows to fetch results of implementations from different projects. Moreover, it is possible to specify which metric to compare, in this case charts/episodic_return. Also, the CLI provides the possibility to select a given algorithm and apply a different name in the plot, e.g. we rename TD3 to _Official TD3_ and td3_continuous_action_jax to _Clean RL TD3_. Finally, we can also select a set of environments through the --end-ids option.

Figure 8: Comparing CleanRL’s TD3 against the original TD3 implementation (sample efficiency).

Figure 9: Comparing CleanRL’s TD3 against the original TD3 implementation (time).

[MISSING_PAGE_FAIL:19]

Figure 12: Clean RL PPO vs. OpenAI Baselines PPO, sample efficiency (RLiable).

Figure 13: Clean RL PPO vs. OpenAI Baselines PPO, walltime efficiency (RLiable).

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_FAIL:23]

Figure 16: Comparison between the original PPO and the PPO with MC value estimates in various MuJoCo and Box2D environments. Plots represent the evolution of the episodic return as a function of the number of interactions with the environment, and shaded areas represent the standard deviation.

Figure 17: Comparison between the original PPO and the PPO with MC value estimates in various MuJoCo and Box2D environments. Plots represent the evolution of the episodic return as a function of the number of interactions with the environment, and shaded areas represent the standard deviation.

[MISSING_PAGE_FAIL:26]

Figure 20: Sample efficiency curves for algorithms on the MuJoCo Benchmark (Todorov et al., 2012). This graph presents the mean episodic return for algorithms implemented using Stable Baselines 3 (Raffin et al., 2021), averaged across a minimum of 10 runs (refer to ORLB for specific run counts). Data points are subsampled to 10,000 and interpolated for clarity. The curves are smoothed using a rolling average with a window size of 100. The shaded regions around each curve indicate the standard deviation.