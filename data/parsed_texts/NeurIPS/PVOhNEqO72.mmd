**Mapping the intermolecular interaction universe through self-supervised learning on molecular crystals**

**Anonymous Author(s)**

Affiliation

Address

email

#### Abstract

Molecular interactions fundamentally influence all aspects of chemistry and biology. Prevailing machine learning approaches emphasize the modeling of molecules in isolation or at best provide limited modeling of molecular interactions, typically restricted to protein-ligand and protein-protein interactions. Here, we present how to use molecular crystals to define the MolInteractDB dataset that contains valuable biochemical knowledge, which can be captured by large self-supervised pre-trained models. MolInteractDB incorporates 344,858 molecular crystal structure entries from the Cambridge Structural Database. We formulate entries in the MolInteractDB dataset as radial patches of flexible size and at varying positions in the crystal to represent intermolecular interactions across crystal structures. We characterize a variety of interactions highlighted across 6 million patches. Leveraging MolInteractDB, we develop InteractNN, a self-supervised SE(3)-equivariant 3D message passing network. We show that InteractNN captures the latent knowledge of chemical elements as well as intermolecular interaction types at a scale not directly accessible to human scientists. To demonstrate its potential, we fine-tuned InteractNN to predict the binding affinity between proteins and ligands, producing results comparable with state-of-the-art models.

## 1 Introduction

Intermolecular interactions between molecules play a central role in understanding and predicting chemical phenomena [7; 17; 27; 65]. In drug discovery, intermolecular interactions between the ligand and target are key factors for the selectivity and specificity of the drug [52; 68; 3; 46; 34; 19; 71]. While these interactions are important for chemists, the exploration of intermolecular interactions in machine learning is limited. Many state-of-the-art models in molecular property prediction train on molecular datasets featuring molecules in isolation, for PCQM4Mv2 [39], QM9 [43], and ZINC [18]. In contrast, ML models for molecular interactions are restricted to protein-ligand and protein-protein interaction (PPI) structures, leaving the broader field of intermolecular interactions largely untouched. Given the fundamental role of intermolecular interactions, it is important to consider a broader variety of these interactions to improve the generalizability of ML models.

An experimental data modality that captures intermolecular interactions is a crystal structure [40], which records the 3D coordinates of the atoms in the crystal. In molecular crystals, molecules are bound together by intermolecular interactions in an infinitely repeating lattice. To represent this periodic structure, crystal structures are expressed as a unit cell--the smallest repeating unit of the crystal. There are large datasets of crystal structures including the Cambridge Structural Database[13] (CSD) featuring 1,222,711 entries of which 344,858 are of organic molecular crystals that satisfied our search criteria. Unlike the discrete molecules found in many molecular property datasets, representing unit cells to capture intermolecular interactions for ML presents unique challenges.

**Present work.** We present MolInteractDB, a dataset created from the CSD that captures intermolecular interactions from unit cells of entries in the CSD. Each entry in the MolInteractDB is a radial patch which includes the 3D coordinates and atomic identities of intermolecular interacting molecular fragments. A key contribution of MolInteractDB is the expansion of intermolecular examples available for ML models. By leveraging the CSD, we extend beyond molecular interactions limited to protein-ligand and protein-protein complexes, and present more intermolecular interactions that are chemically relevant for ML. In addition to dataset creation, we also developed the InteractNN model which is trained with self-supervised objectives to learn an informative latent space of patches. Probing the latent representation space reveals its ability to learn chemical types of interactions, and elemental differences. Finally, we show that InteractNN can be fine-tuned to predict binding affinity of protein-ligand interactions and achieves comparable results to state-of-the-art.

## 2 Related work

**Machine learning for molecules.** In the field of molecular machine learning, there are various studies ranging from property prediction to generation. Molecules can be represented either as 1D strings, such as SMILES [61] and SELFIES [25], and are typically trained using language models [72; 60]. Alternatively, 2D and 3D molecular structures can be represented as graphs [44; 67; 69; 11; 48; 64; 32] and trained using graph neural networks (GNNs). These models can predict molecular properties [29; 41; 32] and help design new molecules [34; 42; 70; 28; 21; 35; 62].

**Geometric deep learning for molecular prediction and design.** Molecules can adopt multiple 3D configurations, known as conformers, which are not represented in 1D or 2D forms. Additionally, 3D geometric information significantly influences the properties and functions of a molecule. Consequently, several geometric deep learning models incorporate 3D coordinates for molecular property prediction [47; 9; 31; 58]. Given the scarcity of labeled 3D molecular data, self-supervised formulations for pre-training on 3D molecular structures have been developed. Notable models include GraphMVP [30], GNS-TAT [66], and 3D InfoMax [51]. Among them, GNS-TAT [66] demonstrates that pre-training by denoising 3D structures towards equilibrium can enhance performance in downstream tasks. Subsequently, these models are fine-tuned on smaller 3D molecular datasets with labeled molecular properties. Progress has also been made in constructing equivariant models. These ensure that when certain symmetry operations or transformations are applied to the input, equivalent transformations are reflected in the output. This is crucial for maintaining the consistency of output predictions with SE(3)-symmetry operations, which include translations and rotations [45; 14].

**Machine learning for molecular interactions.** Molecular interactions underpin virtually all processes within living organisms. Several models have been developed to predict molecular interactions, including binding affinity prediction [37; 63; 38; 26; 37], binding site prediction [36; 24; 20; 22], and PPI prediction [7; 54; 8]. The field of molecular interactions is expanding with emerging areas of interest such as the design of molecular glues to stabilize PPIs [4] and the modulation of PPIs to target the undruggable proteome [33]. However, the scarcity of data, primarily due to the challenges in capturing 3D molecular data of interacting biological compounds, has curtailed the widespread application of ML in these nascent fields. Recognizing the need to understand intermolecular interactions across stages of drug design and development and across therapeutic modalities, we harness large datasets of molecular crystals to advance the modeling of intermolecular interactions.

## 3 Creating MolInteractDB dataset

In this section we outline how we curate the CSD to capture examples of intermolecular interactions from molecular crystals. We start by defining intermolecular patches (Sec. 3.1) and proceed by outlining the curation of MolInteractDB (Sec. 3.2).

### Overview of Cambridge Structural Database (CSD)

The CSD [13] contains all known crystal structures of small-molecule organic and metal-organic crystal structures. These structures are experimentally determined with X-ray or neutron crystallography. As of 2023 there are over 1.25 million crystal structures in the CSD, of which under half of these structures are classified as organic. A review from Taylor and Wood highlights the contributions of the CSD in researching molecular geometries, interactions, and assemblies [55].

**Curating molecular cystals from the CSD.** Querying and accessing of crystal structure data was done with the CSD Python API. Each entry of the CSD describes a crystal structure stored as a unit cell, the smallest component that represents the repeating crystal structure, and the metadata including publications associated with the entry, experimental details, and the chemical formula. Additionally, the CSD computationally assigns bonds and bond types between atoms to every entry. An example of data available for an entry in the CSD is shown in Figure 1. We filtered CSD v2022.3.0 for all entries that satisfied all of the following criteria: organic, not polymeric, has 3D coordinates, no disorder, no errors, no metals, had only one SMILES string describing the crystal entry (in other words, each crystal is comprised of only one chemical compound). This filtered the CSD dataset from 1,222,711 entries to 344,858.

### Creating intermolecular patches in MolInteractDB using molecular crystals

To represent intermolecular interactions we define intermolecular patches as entries in MolInteractDB. Each radial patch is centered between two molecules to capture the geometric orientations of non-bonding interactions between two molecules. This approach captures diverse types of intermolecular interactions, including hydrogen bonding, Van der Waals interactions, aromatic interactions. Here we do not directly model the periodic unit cell, as we focus on recording intermolecular interactions. Radial patches have been shown to be useful in related fields of modeling protein surfaces [7; 54; 8; 53; 2], where patches are defined on the surface of a protein to reduce large protein surfaces to a fingerprint. Our approach differs to the use of patches for modeling of protein surfaces--which only feature one molecule--instead our patches capture interactions between molecules.

**Definition 3.1** (**Intermolecular Patch**).: An intermolecular patch \(G^{(ij)}\) is a graph with geometric 3D coordinate attributes that is comprised of molecular fragments of intermolecularly interacting molecules \(i\) and \(j\), here denoted as \(M^{(i)}\) and \(M^{(j)}\), respectively. Intermolecular interactions are all non-bonding interactions between \(M^{(i)}\) and \(M^{(j)}\); this includes hydrogen bonding, dipole-dipole interactions, Van der Waals interactions, and aromatic-aromatic interactions. Molecular fragments in the patch are all atoms in the molecules that are within a radius \(r\) from the weighted center, \(c^{(ij)}=1/(2|M^{(i)}||M^{(j)}|)\left(|M^{(j)}|\sum_{k\in M^{(i)}}\mathbf{p}_{ k}+|M^{(i)}|\sum_{k\in M^{(j)}}\mathbf{p}_{k}\right)\), where \(\mathbf{p}_{k}\) is the atomic coordinates of the molecules. The nodes and edges of the \(G^{(ij)}=(V_{G^{(ij)}},E_{G^{(ij)}})\) are:

* **Nodes:**\(V_{G^{(ij)}}=\left(V^{(i)},V^{(j)}\right)\), where \(V^{(i)},V^{(j)}\) are atoms in \(M^{(i)}\) and \(M^{(j)}\) that are within radius \(r\) to the center \(c^{(ij)}\). We denote arbitrary nodes in \(V_{G^{(ij)}}\) with \(a\) and \(b\).
* **Edges:**\(E_{G^{(ij)}}=\left(E^{(ij)},E^{(i)},E^{(j)}\right)\) are comprised of:
* **Intermolecular edges:**\(E^{(ij)}\) connect atoms in \(V^{(i)}\) with atoms in \(V^{(j)}\) that are positioned within distance \(d_{\mathrm{inter}}\) of each other.
* **Intramolecular edges:**\(E^{(i)}\) are edges between atoms in \(V^{(i)}\), and \(E^{(j)}\) are edges between nodes in \(V^{(j)}\) that are within distance \(d_{\mathrm{intra}}\).

We also refer to neighbouring nodes \(b\in\mathcal{N}_{a}^{(t)}\) of node \(a\) in a patch, where \(t\in\{\mathrm{inter},\mathrm{intra}\}\) are the intermolecular and intramolecular edge neighbours. For intermolecular neighbours, we refer to the

Figure 1: Illustrative example of the CSD entry for ABOSAN.

edges \(E^{(ij)}\). For intramolecular neighbours, if \(a\in V^{(i)}\) we refer to the edges \(E^{(i)}\), otherwise if \(a\in V^{(j)}\) we refer to the edges \(E^{(j)}\).

The MolInteractDB dataset, \(\mathcal{D}=\{G^{(i_{k}j_{k})}\mid k=1,\ldots,N\}\), is comprised of patches \(G^{(ij)}\) constructed from CSD entries. For our purposes of learning intermolecular interactions we sampled many patches to represent all examples of intermolecular interactions in a unit cell (Figure 2). Given an entry of the CSD, we iterate through each unique, valid conformer \(M^{(i)}\) in the unit cell using the CSD Python API. For each conformer \(M^{(i)}\) we iterate through all neighbouring peripheral conformers \(M^{(j)}\) of this molecule given by the unit cell that are within \(d_{\mathrm{inter}}\) to an atom in \(M^{(i)}\). A patch \(G^{(ij)}\) is constructed from all atoms in \(M^{(i)}\) and \(M^{(j)}\) that are within radius \(r\) to the weighted center \(c^{(ij)}\). This extraction of patches from a unit cell will yield some patches that are equal up to permutation.

We set \(r=8\) A \(d_{\mathrm{inter}}=4\) A  and \(d_{\mathrm{intra}}=2\) A. After iterating through all 344,858 CSD entries that satisfied our CSD filters, this constructs 6,059,368 patches in \(\mathcal{D}\). The choice of radius, intermolecular and intramolecular edge distance cutoff for the patch will influence the number of patches created. A radius \(r\) that is too small would break basic chemical motifs, which would lead to insufficient chemical context for interactions in the patch. Intramolecular edge cutoffs \(d_{\mathrm{intra}}\) that are too short would also disregard longer chemical bonds, and intermolecular edge cutoffs \(d_{\mathrm{inter}}\) that are too short would limit the number of patch examples. Our choice of cutoffs aim to provide sufficient chemical context. We summarise statistics of the patches in MolInteractDB in Table 1.

## 4 InteractNN model and its compelling use cases

Next we outline the InteractNN model that uses MolInteractDB for self-supervised pretraining. We provide details for how we probe the learned latent space of the InteractNN to explore the space of chemical interactions (Sec. 4.1) and show how InteractNN can be fine-tuned for protein-ligand binding prediction (Sec. 4.2).

\begin{table}
\begin{tabular}{l r r r r|r r} \hline \hline \multicolumn{4}{c|}{**Graph features**} & \multicolumn{4}{c}{**Chemical features**} \\ \hline Feature & Mean & SD & Min & Max & Element & \% Distribution \\ \hline \# Nodes & 67.8 & 24.3 & 4 & 424 & Carbon & 44.6 \\ \# Intermolecular edges & 34.1 & 34.5 & 1 & 8,547 & Hydrogen & 42.4 \\ \# Intramolecular edges & 89.5 & 36.6 & 2 & 2,859 & Oxygen & 6.1 \\ \multicolumn{4}{l}{Intermolecular node degree} & 1.3 & 0.2 & 0.0 & 22.9 & Nitrogen & 3.9 \\ \multicolumn{4}{l}{Intramolecular node degree} & 0.5 & 0.4 & 0.1 & 11.45 & Fluorine & 0.8 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Properties of 6,059,368 patches in MolInteractDB with \(r=8\) Å, \(d_{\mathrm{inter}}=4\) Å, and \(d_{\mathrm{intra}}=2\) Å.

Figure 2: Intermolecular molecular patches from MolInteractDB. CSD entry ABIGAV is shown.

### Overview of InteractNN model

InteractNN uses a SE(3)-equivariant 3D message passing network on intermolecular patches to learn representations that are informative of the intermolecular interaction between molecules.

**Problem (Self-Supervised Pre-Training For intermolecular Patches).**_Given is an unlabeled pre-training dataset of intermolecular patches, \(\mathcal{D}=\{G^{(i_{k}j_{k})}\mid k=1,\dots,N\}\), and a target dataset of labeled intermolecular patches \(\mathcal{S}=\{(G^{(i_{k}j_{k})}_{\text{target}},y_{k})\mid k=1,\dots,M\}\), where \(M<<N\). Our goal is to pre-train a model \(\mathcal{F}\) on \(\mathcal{D}\) such that it generates representations \(\mathbf{z}_{k}=\mathcal{F}(G^{(i_{k}j_{k})})\) for every intermolecular patch \(G^{(i_{k}j_{k})}\) that are chemically informative, and \(\mathcal{F}\) can also be fine-tuned on \(\mathcal{S}\) to predict \(y_{k}\) for every \(G^{(i_{k}j_{k})}_{\text{target}}\)._

**Atom-level representation learning.** Here we outline the SE(3)-equivariant 3D message passing network for InteractNN on the nodes of the intermolecular patch \(G^{(i,j)}\). Several rotational equivariant neural networks have been introduced for modeling molecules [49, 23, 31, 1]. We build on the E(3)-equivariant neural network layers presented by Tensor-Field Networks implemented in e3nn [10] and DiffDock [3]. Message passing for the intermolecular edges and intramolecular edges are done separately, but the message passing framework for the two edge types is the same.

The feature vectors \(\mathbf{h}_{a}\) of nodes \(a\) in \(G^{(i,j)}\) are geometric objects that comprise a direct sum of irreducible representations of the O(3) symmetry group. The feature vectors \(\mathbf{h}_{a}^{(\lambda,p)}\) are indexed with \(\lambda,p\), where \(\lambda=0,1,2,\dots\) is a non-negative integer denoting the rotation order and \(p\in\{\mathrm{o},\mathrm{e}\}\) indicates odd or even parity, which together index the irreducible representations (irreps) of O(3). There are also multiple features in \(\mathbf{h}_{a}\) which have the same irrep. In our model, we set \(\lambda_{\mathrm{max}}=1\) for \(\mathbf{h}_{a}\), and we denote the number of scalar (\(0\mathrm{e}\)) and pseudoscalar (\(0\mathrm{o}\)) irrep features in \(\mathbf{h}_{a}\) with \(\mathrm{ns}\), and the number of vector (\(1\mathrm{o}\)) and pseudovector (\(1\mathrm{e}\)) irrep features in \(\mathbf{h}_{a}\) with \(\mathrm{nv}\).

First, the element type of node \(a\) is embedded with a normal distribution and trainable weights to a vector with feature configuration \(\mathrm{ns}\times 0\mathrm{e}\). The edge length between the coordinates of node \(a\) and neighbouring node \(b\) is also embedded with Gaussian smearing to a vector comprised of \(\mathrm{ns}\times 0\mathrm{e}\), then the Gaussian embedding vector is passed through a 2-layer MLP projector to output a feature vector \(\mathbf{e}_{ab}\) with feature configuration \(\mathrm{ns}\times 0\mathrm{e}\).

There are \(L\) layers of message passing between nodes. At each layer \(l\), the node updates for node \(a\) in the intermolecular patch \(G^{(i,j)}\) are given by:

\[\mathbf{h}_{a}\leftarrow\underset{t\in\{\text{inter},\text{intra}\}}{\oplus }\mathrm{BN}^{(t)}\left(\frac{1}{\left|\mathcal{N}_{a}^{(t)}\right|}{\sum_{b \in\mathcal{N}_{a}^{(t)}}}Y^{(\lambda)}\left(\hat{r}_{ab}\right)\otimes_{ \psi_{ab}}\mathbf{h}_{b}\right)\text{ with }\psi_{ab}=\Psi^{(t)}\left(\mathbf{e}_{ab}, \mathbf{h}_{a}^{0\mathrm{e}},\mathbf{h}_{b}^{0\mathrm{e}}\right),\] (1)

where node \(b\) are the neighbours of node \(a\) in \(G^{(i,j)}\) given by intermolecular or intramolecular edges denoted with \(t\). The message is computed with tensor products between the spherical harmonic projection with rotation order \(\lambda=2\) of the unit bond direction vector, \(Y^{(\lambda)}\left(\hat{r}_{ab}\right)\), and the irreps of the feature vector of the neighbour \(\mathbf{h}_{b}\). This is a weighted tensor product and the weights are given by a 2-layer MLP, \(\Psi^{(t)}\), based on the \(0\mathrm{e}\) features of the nodes \(\mathbf{h}_{a}\) and \(\mathbf{h}_{b}\) and the edge features \(\mathbf{e}_{ab}\). After each layer \(l\) of message passing, \(\mathbf{h}_{a}\) is filtered down to irreps with \(\lambda_{\mathrm{max}}=1\). After \(L\) layers the final irreps configuration of \(\mathbf{h}_{a}\) is \(\mathrm{ns}\times 0\mathrm{e}+\mathrm{nv}\times 1\mathrm{o}+\mathrm{nv}\times 1 \mathrm{e}+\mathrm{ns}\times 0\mathrm{o}\) and the embedding of node \(a\), \(\mathbf{h}_{a}\) is a \(d_{\text{node}}\)-dimension vector.

**Intermolecular patch-level representation learning.** For a patch-level embedding of the nodes a convolution is done between all nodes in a molecule and the unweighted center \(c^{(i)}\) of the nodes \(V^{(i)}\). This is repeated for nodes \(V^{(j)}\) in the patch \(G^{(ij)}\). The edge distance from node \(a\) to \(c^{(i)}\) is also embedded with Gaussian smearing and passsed through a 2-layer MLP projector to output a feature vector \(\mathbf{e}_{ac^{(i)}}\) with feature configuration \(\mathrm{ns}\times 0\mathrm{e}\) as:

\[\mathbf{h}_{c^{(i)}}=\mathrm{BN}\left(\frac{1}{\left|V^{(i)}\right|}\sum_{a \in V^{(i)}}Y^{(\lambda)}\left(\hat{r}_{a,c^{(i)}}\right)\otimes_{\gamma_{ac^ {(i)}}}\mathbf{h}_{a}\right)\text{ with }\gamma_{ac^{(i)}}=\Gamma\left(\mathbf{e}_{ac^{(i)}}, \mathbf{h}_{a}^{0\mathrm{e}}\right).\] (2)

This is a weighted tensor product and the weights are given by a 2-layer MLP projector, \(\Gamma\), based on the \(0\mathrm{e}\) features of the nodes \(\mathbf{h}_{a}\) and the edge features \(\mathbf{e}_{ac^{(i)}}\). The embedding of the intermolecularpatch \(G^{(i_{k}j_{k})}\) is given by \(\mathbf{z}_{k}=[h_{c^{(i_{k})}}^{0}||h_{c^{(i_{k})}_{i}}^{0}]\), the concatenation of the scalars from embedding molecule \(i_{k}\) and \(j_{k}\), which is a \(d_{\text{patch}}\)-dimension vector.

**Self-supervised training with denoising.** Node-level denoising as an objective function has been useful for pre-training on 3D coordinate molecular datasets from DFT generated molecules to prevent over-smoothing of GNNs [12], and it has proven that it is related to learning a force field of per-atom forces [66; 6]. In addition, denoising is linked to score-matching which has also been popular in training generative models [16; 3]. Thus, this motivates the application of denoising as an objective for self-supervised training on MolInteractDB.

Given a patch \(G^{(i,j)}\in\mathcal{D}\), \(\tilde{G}^{(i,j)}\) is a perturbed patch created by adding i.i.d. Gaussian noise to the atomic positions, \(\mathbf{p}_{a}\) of each node \(a\in V_{\tilde{G}^{(i,j)}}\). That is, for each node \(a\in V_{\tilde{G}^{(i,j)}}\) the atomic position \(\mathbf{p}_{a}^{\prime}=\mathbf{p}_{a}+\boldsymbol{\delta}_{a}\), where \(\boldsymbol{\epsilon}_{a}\sim\mathcal{N}\left(0,\sigma I_{3}\right),\sigma=0.5\) and \(\boldsymbol{\delta}_{a}=\min\left(\boldsymbol{\epsilon}_{a},\mathbf{1}\right)\). The objective is to predict \(\{\boldsymbol{\delta}_{1},\ldots,\boldsymbol{\delta}_{|V_{\tilde{G}^{(i,j)}} |}\}\) given \(\tilde{G}^{(i,j)}\). The model \(\mathcal{F}_{\text{denoise}}\) is trained to minimise the loss \(\mathcal{L}\):

\[\mathcal{L}=\sqrt{\frac{1}{N}\sum_{G^{(i,j)}\in\mathcal{D}}\|\mathcal{F}_{ \text{denoise}}(\tilde{G}^{(i,j)})-(\boldsymbol{\delta}_{1},\ldots,\boldsymbol {\delta}_{|V_{\tilde{G}^{(i,j)}}|})\|^{2}}\] (3)

We add a denoising layer to \(\mathcal{F}\) for \(\mathcal{F}_{\text{denoise}}\) to predict the noise applied for each node from \(\tilde{G}^{(i,j)}\). This final layer of the message passing on \(\tilde{G}^{(i,j)}\) takes as input the node-level embeddings \(\mathbf{h}_{a}\) and is the same message passing framework as outlined in Eq. (1). However, the output irreps are restricted to \(1\times 10+1\times 1\mathrm{e}\). To convert this to 3D coordinates, the \(1\times 1\mathrm{o}\) and \(1\times 1\mathrm{e}\) are summed element-wise to produce a vector in \(\mathbb{R}^{3}\) and the prediction is clamped to the maximum noise applied which is 1 A.

### Implementation and use cases

**Implementation.** The model was trained with the denoising objective with a batch size of 64 on MolInteractDB for 48 hours on 48GB RTX 8000 GPUs. The model hyper-parameters were set to \(\mathrm{ns}=32,\,\mathrm{nv}=16,\,L=6\), and \(lr=1\times 10^{-3}\).

**Probing latent representation space.** We investigate whether the self-supervised training of the model resulted in a chemically meaningful latent space by characterizing the patch-level and node-level embedding spaces of MolInteractDB. To determine the chemical labels of patches, we convert the molecular fragments within a patch into RDKit molecules and sourced labels from RDKit. Nodes are labeled based on their atomic elements and further categorized by examining the elements they were bonded to, as well as the bond types.

**Modeling protein-ligand binding.** In this use case, we use the PDBbind v2020 dataset [59], which is a curated subset of the Protein DataBank (PDB) with the structure of bound ligands to proteins, and the associated binding affinity. The task is: given the protein-ligand structure, predict the binding affinity. We use the pocket-ligand substructures of the protein-ligand structure given by PDBbind, where amino acids in the pocket are all amino acids with any atom within 6 A to the ligand. Given the pocket-ligand, we construct a graph with the same features as a patch where the two molecules in the patch are the pocket and the ligand, and intermolecular edges are defined as edges between the pocket and the ligand. Note that we do not restrict the size of the pocket-ligand patch to a radial cutoff. The pocket-ligand graph is passed through the pre-trained InteractNN which gives a patch-level embedding that is passed through a 3-layer MLP predictor to output a binding affinity prediction clamped to between 0 and 15. The InteractNN is fully fine-tuned on the pocket-ligand structures to minimize the root mean squared error between predicted and experimental binding affinity.

## 5 Experiments

### Use case: Probing the latent space of chemical interactions

**Setup.** Given the pre-trained InteractNN we embedded all the nodes and patches and visualized a 2D UMAP for each set of nodes and patches. For patches we label the types of intermolecular interactions at the interface between the two molecules in the patch. If any of the intermolecular interactions are between two atoms in an aromatic system, the patch is labeled as aromatic. Otherwise, if any of the intermolecular interactions are between two atoms where one is a hydrogen bond donor and another is a hydrogen bond acceptor, the patch is labeled as hydrogen bonding. Other interactions, such as dipole-dipole, and Van der Waals interactions are not labeled. For node embeddings, we labelled each node with the atomic element. For the most common elements, carbon and hydrogen, we explored with further granularity by considering the elements the nodes are bonded to and bond types. To test the statistical significance of chemical clusters we use the Kolmogorov-Smirnov (KS) test to compare randomly sampled pairwise distances of \(d\)-dimensional embeddings compared to pairwise distances sampled within \(d\)-dimensional embeddings of the same chemical label.

**Results.** The InteractNN learns an overall embedding space for patches as well as nodes in every patch and we find that embeddings are meaningfully localized based on various chemical properties. In Figure 3, we use a 2D UMAP to visualize the embedding of 300,000 randomly sampled patches from MolInteractDB. Labeling of the UMAP with the chemical type of intermolecular interaction as aromatic groups interacting with aromatic groups, or hydrogen bond donor and hydrogen bond acceptor shows InteractNN learns a chemically enriched latent space in a self-supervised manner. We also see statistically significant differences with \(p\)-value < 0.001 for the pairwise distance of embeddings labeled as hydrogen or aromatic against all patch-level embeddings.

Visualization of the embedding space of 300,000 sampled nodes of patches from MolInteractDB in Figure 4 highlights that InteractNN has learnt differences in atomic environments in a self-supervised manner. In Figure 4a, we see in the embedding space that the InteractNN has differentiated between the elements. Isolating the most common elements, hydrogen, and carbon, pairwise distance of node embeddings within these elements are statistically significantly different to pairwise distances of all node embeddings (\(p\)-value < 0.001). We also show that the embeddings of carbon and hydrogen nodes can be stratified further by the bonding environment. Remarkably, without any prior knowledge of bond types, Figure 4e shows that InteractNN embeds the aromatic carbons in a separate region to the aliphatic carbons (single bonded carbons).

### Use case: Protein-ligand binding affinity prediction

A sequence-based split of 60% from Atom3D [56] is used to train and test the model. We compare our protein-ligand binding affinity prediction with state-of-the-art models trained and tested under the same dataset split. Performance is determined by minimizing the root mean squared error between predicted and actual binding affinity, and by maximizing Pearson and Spearman correlation coefficients between the predicted and actual binding affinity. Results in Table 2 show that the performance of InteractNN is comparable to state-of-the-art models across all metrics. We also show that the absence of pre-training for InteractNN results in a decay in performance.

Figure 3: 2D UMAP plots of InteractNN embeddings of (**a**) 300,000 randomly sampled patches from MolInteractDB. Each dot is a patch and they are labeled by the type of intermolecular interactions present in the patch. (**b**) 300,000 randomly sampled nodes from patches from MolInteractDB. Each dot is a node and they are labeled by element of the node.

## 6 Conclusion

Intermolecular interactions are essential to chemical properties and diverse functions of biological systems. In this work, we introduce a MolInteractDB dataset that leverages large molecular crystal databases to extract examples of intermolecular interactions between molecular fragments in the form of intermolecular patches. We explore the diversity of this dataset and train a InteractNN model on MolInteractDB in a self-supervised manner. We show that the learned latent space of InteractNN is informative for capturing nuances between hydrogen bonding and aromatic interactions. The model can also distinguish between chemical elements. Finally, we fine-tune the model for protein-ligand binding affinity prediction and achieve results comparable to state-of-the-art models. In the future, we will adapt InteractNN for fine-tuning on other molecular interaction tasks, including protein-protein interactions, and explore the model's ability for few-shot prompting and zero-shot learning.

## References

* (1) Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Method** & **RMSE**\(\downarrow\) & **Pearson**\(\uparrow\) & **Spearman**\(\uparrow\) \\ \hline Atom3D [56] & 1.408 & 0.743 & 0.743 \\ ProfTrans [5] & 1.641 & 0.595 & 0.588 \\ MaSIF [7] & 1.426 & 0.709 & 0.701 \\ IEConv [15] & 1.473 & 0.667 & 0.675 \\ Holoprot [50] & 1.365 & 0.749 & 0.742 \\ ProNet [57] & **1.343** & **0.765** & **0.761** \\ \hline InteractNN & 1.355 & 0.748 & 0.746 \\ InteractNN no pre-training & 1.415 & 0.719 & 0.717 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on protein-ligand binding affinity task with 60% sequence identity split. The top two results are highlighted as **1st** and 2nd. We report the benchmark metrics provided by ProNet [57].

Figure 4: 2D UMAP plots of InteractNN embeddings of 300,000 randomly sampled nodes from patches in MolInteractDB. (**a**) Highlighting the hydrogen nodes against other elements. (**b**) Each dot is a hydrogen node and they are labeled by the element that hydrogen is bonded to. (**c**) Highlighting the carbon nodes against other elements. (**d**) Each dot is a carbon node and they are labeled by the type of bond, and element that carbon is bonded to. (**e**) Pairwise distance of node embeddings with a given label. All distributions are statistically significantly different from each other (two-sided non-parametric KS test; \(p\)-value < 0.001).

neural networks for data-efficient and accurate interatomic potentials. _Nature communications_, 13(1):2453, 2022.
* Budowski-Tal et al. [2018] Inbal Budowski-Tal, Rachel Kolodny, and Yael Mandel-Gutfreund. A novel geometry-based approach to infer protein interface similarity. _Scientific reports_, 8(1):8192, 2018.
* Corso et al. [2023] Gabriele Corso, Hannes Stark, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking. _ICLR_, 2023.
* Dong et al. [2021] Guoqiang Dong, Yu Ding, Shipeng He, and Chunquan Sheng. Molecular glues for targeted protein degradation: from serendipity to rational discovery. _Journal of medicinal chemistry_, 64(15):10606-10620, 2021.
* Elnaggar et al. [2021] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Toward understanding the language of life through self-supervised learning. _IEEE transactions on pattern analysis and machine intelligence_, 44(10):7112-7127, 2021.
* Feng et al. [2023] Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, and Wei-Ying Ma. Fractional denoising for 3d molecular pre-training. In _International Conference on Machine Learning_, pages 9938-9961. PMLR, 2023.
* Gainza et al. [2020] Pablo Gainza, Freyr Sverrisson, Frederico Monti, Emanuele Rodola, D Boscaini, MM Bronstein, and BE Correia. Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. _Nature Methods_, 17(2):184-192, 2020.
* Gainza et al. [2023] Pablo Gainza, Sarah Wehrle, Alexandra Van Hall-Beauvais, Anthony Marchand, Andreas Scheck, Zander Harteveld, Stephen Buckley, Dongchun Ni, Shuguang Tan, Freyr Sverrisson, et al. De novo design of protein interactions with learned surface fingerprints. _Nature_, pages 1-9, 2023.
* Gasteiger et al. [2020] Johannes Gasteiger, Janek Gross, and Stephan Gunnemann. Directional message passing for molecular graphs. In _ICLR '20_, 2020.
* Geiger and Smidt [2022] Mario Geiger and Tess Smidt. e3nn: Euclidean neural networks. _arXiv preprint arXiv:2207.09453_, 2022.
* Gilmer et al. [2017] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International Conference on Machine Learning_, pages 1263-1272. PMLR, 2017.
* Godwin et al. [2021] Jonathan Godwin, Michael Schaarschmidt, Alexander Gaunt, Alvaro Sanchez-Gonzalez, Yulia Rubanova, Petar Velickovic, James Kirkpatrick, and Peter Battaglia. Simple gnn regularisation for 3d molecular property prediction & beyond. _arXiv preprint arXiv:2106.07971_, 2021.
* Groom et al. [2016] Colin R Groom, Ian J Bruno, Matthew P Lightfoot, and Suzanna C Ward. The cambridge structural database. _Acta Crystallographica Section B: Structural Science, Crystal Engineering and Materials_, 72(2):171-179, 2016.
* Han et al. [2022] Jiaqi Han, Yu Rong, Tingyang Xu, and Wenbing Huang. Geometrically equivariant graph neural networks: A survey. _arXiv preprint arXiv:2202.07230_, 2022.
* Hermosilla et al. [2020] Pedro Hermosilla, Marco Schafer, Matej Lang, Gloria Fackelmann, Pere Pau Vazquez, Barbora Kozlikova, Michael Krone, Tobias Ritschel, and Timo Ropinski. Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures. _arXiv preprint arXiv:2007.06252_, 2020.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _arXiv preprint arXiv:2006.11239_, 2020.

* Huang et al. [2020] Kexin Huang, Tianfan Fu, Lucas M Glass, Marinka Zitnik, Cao Xiao, and Jimeng Sun. Deep-Purpose: A deep learning library for drug-target interaction prediction. _Bioinformatics_, 2020.
* Irwin et al. [2012] John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. ZINC: a free tool to discover chemistry for biology. _Journal of chemical information and modeling_, 52(7):1757-1768, 2012.
* Isert et al. [2023] Clemens Isert, Kenneth Atz, and Gisbert Schneider. Structure-based drug design with geometric deep learning. _Current Opinion in Structural Biology_, 79:102548, 2023.
* Jimenez et al. [2017] Jose Jimenez, Stefan Doerr, Gerard Martinez-Rosell, Alexander S Rose, and Gianni De Fabritiis. Deepsite: protein-binding site predictor using 3d-convolutional neural networks. _Bioinformatics_, 33(19):3036-3042, 2017.
* Jin et al. [2018] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. _ICML_, 2018.
* Kandel et al. [2021] Jeevan Kandel, Hilal Tayara, and Kil To Chong. Puresnet: prediction of protein-ligand binding sites using deep residual neural network. _Journal of cheminformatics_, 13(1):1-14, 2021.
* Klicpera et al. [2021] Johannes Klicpera, Florian Becker, and Stephan Gunnemann. GemNet: Universal directional graph neural networks for molecules. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2021.
* Krapp et al. [2023] Lucien F Krapp, Luciano A Abriata, Fabio Cortes Rodriguez, and Matteo Dal Peraro. Pesto: parameter-free geometric deep learning for accurate prediction of protein binding interfaces. _Nature Communications_, 14(1):2175, 2023.
* Krenn et al. [2020] Mario Krenn, Florian Hase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation. _Machine Learning: Science and Technology_, 1(4):045024, 2020.
* Li et al. [2021] Shuangli Li, Jingbo Zhou, Tong Xu, Liang Huang, Fan Wang, Haoyi Xiong, Weili Huang, Dejing Dou, and Hui Xiong. Structure-aware interactive graph neural networks for the prediction of protein-ligand binding affinity. KDD '21, New York, NY, USA, 2021. Association for Computing Machinery.
* Lim et al. [2019] Jaechang Lim, Seongok Ryu, Kyubyong Park, Yo Joong Choe, Jiyeon Ham, and Woo Youn Kim. Predicting drug-target interaction using a novel graph neural network with 3d structure-embedded graph representation. _Journal of chemical information and modeling_, 59(9):3981-3988, 2019.
* Liu et al. [2022] Meng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, and Shuiwang Ji. Generating 3d molecules for target protein binding. In _International Conference on Machine Learning_, 2022.
* Liu et al. [2021] Shengchao Liu, Meng Qu, Zuobai Zhang, Huiyu Cai, and Jian Tang. Multi-task learning with domain knowledge for molecular property prediction. In _NeurIPS 2021 AI for Science Workshop_, 2021.
* Liu et al. [2021] Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pre-training molecular graph representation with 3D geometry. _International Conference on Learning Representations_, 2021.
* Liu et al. [2021] Yi Liu, Limei Wang, Meng Liu, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3d graph networks. _arXiv preprint arXiv:2102.05013_, 2021.
* Lu et al. [2019] Chengqiang Lu, Qi Liu, Chao Wang, Zhenya Huang, Peize Lin, and Lixin He. Molecular property prediction: A multilevel quantum interactions modeling perspective. In _AAAI_, volume 33, pages 1052-1060, 2019.

* [33] Haiying Lu, Qiaodan Zhou, Jun He, Zhongliang Jiang, Cheng Peng, Rongsheng Tong, and Jianyou Shi. Recent advances in the development of protein-protein interactions modulators: mechanisms and clinical trials. _Signal transduction and targeted therapy_, 5(1):213, 2020.
* [34] Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. A 3D generative model for structure-based drug design. In _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021.
* [35] Youzhi Luo, Keqiang Yan, and Shuiwang Ji. GraphDF: A discrete flow model for molecular graph generation. _Proceedings of the 38th International Conference on Machine Learning, ICML_, 139:7192-7203, 2021.
* [36] Artur Meller, Michael D Ward, Jonathan H Borowsky, Jeffrey M Lotthammer, Meghana Kshirsagar, Felipe Oviedo, Juan Lavista Ferres, and Gregory Bowman. Predicting the locations of cryptic pockets from single protein structures using the pocketminer graph neural network. _Nature Communications_, 2023.
* [37] Marc A Moesser, Dominik Klein, Fergus Boyles, Charlotte M Deane, Andrew Baxter, and Garrett M Morris. Protein-ligand interaction graphs: Learning from ligand-shaped 3d interaction graphs to improve binding affinity prediction. _bioRxiv_, pages 2022-03, 2022.
* [38] Seokhyun Moon, Wonho Zhung, Soojung Yang, Jaechang Lim, and Woo Youn Kim. Pignet: a physics-informed deep learning model toward generalized drug-target interaction predictions. _Chemical Science_, 13(13):3661-3673, 2022.
* [39] Maho Nakata and Tomomi Shimazaki. Pubchemqc project: a large-scale first-principles electronic structure database for data-driven chemistry. _Journal of chemical information and modeling_, 57(6):1300-1308, 2017.
* [40] Michael O'Keeffe and Bruce G Hyde. _Crystal structures_. Courier Dover Publications, 2020.
* [41] Andre F. Oliveira, Juarez L. F. Da Silva, and Marcos G. Quiles. Molecular property prediction and molecular design using a supervised grammar variational autoencoder. _Journal of Chemical Information and Modeling_, 0(0):null, 0. PMID: 35174705.
* [42] Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. In _International Conference on Machine Learning_, pages 17644-17655. PMLR, 2022.
* [43] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. _Scientific data_, 1(1):1-7, 2014.
* [44] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. In _Advances in Neural Information Processing Systems, NeurIPS_, 2020.
* [45] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. _Proceedings of the 38th International Conference on Machine Learning, ICML_, 139:9323-9332, 2021.
* [46] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Lio, Carla Gomes, Max Welling, Michael Bronstein, and Bruno Correia. Structure-based drug design with equivariant diffusion models. _arXiv preprint arXiv:2210.13695_, 2022.
* [47] Kristof Schutt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Muller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. _NeurIPS_, 30, 2017.

* [48] Kristof T Schutt, Farhad Arbabzadah, Stefan Chmiela, Klaus R Muller, and Alexandre Tkatchenko. Quantum-chemical insights from deep tensor neural networks. _Nature communications_, 8(1):1-8, 2017.
* [49] Kristof T Schutt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Muller. Schnet-a deep learning architecture for molecules and materials. _The Journal of Chemical Physics_, 148(24):241722, 2018.
* [50] Vignesh Ram Somnath, Charlotte Bunne, and Andreas Krause. Multi-scale representation learning on proteins. _Advances in Neural Information Processing Systems_, 34:25244-25255, 2021.
* [51] Hannes Stark, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Gunnemann, and Pietro Lio. 3d infomax improves gnns for molecular property prediction. In _International Conference on Machine Learning_, pages 20479-20502. PMLR, 2022.
* [52] Hannes Stark, Octavian-Eugen Ganea, Lagnajit Pattanaik, Regina Barzilay, and Tommi Jaakkola. EquiBind: Geometric deep learning for drug binding structure prediction. _arXiv preprint arXiv:2202.05146_, 2022.
* [53] Vitalii Stebliankin, Azam Shirali, Prabin Baral, Jimeng Shi, Prem Chapagain, Kalai Mathee, and Giri Narasimhan. Evaluating protein binding interfaces with transformer networks. _Nature Machine Intelligence_, pages 1-12, 2023.
* [54] Freyr Sverrisson, Jean Feydy, Bruno E Correia, and Michael M Bronstein. Fast end-to-end learning on protein surfaces. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15272-15281, 2021.
* [55] Robin Taylor and Peter A Wood. A million crystal structures: The whole is greater than the sum of its parts. _Chemical reviews_, 119(16):9427-9477, 2019.
* [56] Raphael John Lamarre Townsend, Martin Vogele, Patricia Adriana Suriana, Alexander Derry, Alexander Powers, Yianni Laloudakis, Sidhika Balachandar, Bowen Jing, Brandon M Anderson, Stephan Eismann, et al. Atom3d: Tasks on molecules in three dimensions. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)_, 2021.
* [57] Limei Wang, Haoran Liu, Yi Liu, Jerry Kurtin, and Shuiwang Ji. Learning protein representations via complete 3d graph networks. _arXiv preprint arXiv:2207.12600_, 2022.
* [58] Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. Comenet: Towards complete and efficient message passing for 3d molecular graphs. _arXiv preprint arXiv:2206.08515_, 2022.
* [59] Renxiao Wang, Xueliang Fang, Yipin Lu, and Shaomeng Wang. The pdbbind database: Collection of binding affinities for protein- ligand complexes with known three-dimensional structures. _Journal of medicinal chemistry_, 47(12):2977-2980, 2004.
* [60] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. SMILES-BERT: large scale unsupervised pre-training for molecular property prediction. In _Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics_, pages 429-436, 2019.
* [61] David Weininger. SMILES, a chemical language and information system. 1. introduction to methodology and encoding rules. _Journal of chemical information and computer sciences_, 28(1):31-36, 1988.
* [62] Minkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael Gomez-Bombarelli, and Jian Tang. An end-to-end framework for molecular conformation generation via bilevel programming. 2021.

* Yan et al. [2023] Jiaixian Yan, Zhaofeng Ye, Ziyi Yang, Chengqiang Lu, Shengyu Zhang, Qi Liu, and Jiezhong Qiu. Multi-task bioassay pre-training for protein-ligand binding affinity prediction. _arXiv preprint arXiv:2306.04886_, 2023.
* Yang et al. [2019] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molecular representations for property prediction. _Journal of chemical information and modeling_, 59(8):3370-3388, 2019.
* Yang et al. [2023] Ziduo Yang, Weihe Zhong, Qiujie Lv, Tiejun Dong, and Calvin Yu-Chian Chen. Geometric interaction graph neural network for predicting protein-ligand binding affinities from 3d structures (gign). _The Journal of Physical Chemistry Letters_, 14(8):2020-2033, 2023.
* Zaidi et al. [2022] Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-Gonzalez, Peter Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via denoising for molecular property prediction. In _The Eleventh International Conference on Learning Representations_, 2022.
* Zhang et al. [2020] Shichang Zhang, Ziniu Hu, Arjun Subramonian, and Yizhou Sun. Motif-driven contrastive learning of graph representations. _arXiv preprint arXiv:2012.12533_, 2020.
* Zhang et al. [2023] Yangtian Zhang, Huiyu Cai, Chence Shi, Bozitao Zhong, and Jian Tang. E3bind: An end-to-end equivariant network for protein-ligand docking. _ICLR_, 2023.
* Zhang et al. [2021] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph self-supervised learning for molecular property prediction. _Advances in Neural Information Processing Systems_, 34:15870-15882, 2021.
* Zhang et al. [2023] Zaixi Zhang, Yaosen Min, Shuxin Zheng, and Qi Liu. Molecule generation for target protein binding with structural motifs. In _The Eleventh International Conference on Learning Representations_, 2023.
* Zhang et al. [2023] Zaixi Zhang, Jiaxian Yan, Qi Liu, and Enhong Che. A systematic survey in geometric deep learning for structure-based drug design. _arXiv preprint arXiv:2306.11768_, 2023.
* Zhao et al. [2023] Wenyu Zhao, Dong Zhou, Buqing Cao, Kai Zhang, and Jinjun Chen. Adversarial modality alignment network for cross-modal molecule retrieval. _IEEE Transactions on Artificial Intelligence_, 2023.