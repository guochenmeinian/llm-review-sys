# Towards Generic Semi-Supervised Framework for

Volumetric Medical Image Segmentation

 Haonan Wang\({}^{1}\), Xiaomeng Li\({}^{1,2}\)

\({}^{1}\)The Hong Kong University of Science and Technology

\({}^{2}\)HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute, Futian, Shenzhen

hwanggr@connect.ust.hk, eexmli@ust.hk

Corresponding author.

###### Abstract

Volume-wise labeling in 3D medical images is a time-consuming task that requires expertise. As a result, there is growing interest in using semi-supervised learning (SSL) techniques to train models with limited labeled data. However, the challenges and practical applications extend beyond SSL to settings such as unsupervised domain adaptation (UDA) and semi-supervised domain generalization (SemiDG). This work aims to develop a generic SSL framework that can handle all three settings. We identify two main obstacles to achieving this goal in the existing SSL framework: 1) the weakness of capturing distribution-invariant features; and 2) the tendency for unlabeled data to be overwhelmed by labeled data, leading to over-fitting to the labeled data during training. To address these issues, we propose an **Aggregating & Decoupling** framework. The aggregating part consists of a Diffusion encoder that constructs a _common knowledge set_ by extracting distribution-invariant features from aggregated information from multiple distributions/domains. The decoupling part consists of three decoders that decouple the training process with labeled and unlabeled data, thus avoiding over-fitting to labeled data, specific domains and classes. We evaluate our proposed framework on four benchmark datasets for SSL, Class-imbalanced SSL, UDA and SemiDG. The results showcase notable improvements compared to state-of-the-art methods across all four settings, indicating the potential of our framework to tackle more challenging SSL scenarios. Code and models are available at: https://github.com/xmed-lab/GenericSSL.

## 1 Introduction

Labeling volumetric medical images requires expertise and is a time-consuming process. Therefore, the use of semi-supervised learning (SSL) is highly desirable for training models with limited labeled data. Various SSL techniques [1, 2, 3, 4, 5, 6, 7] have been proposed, particularly in the field of semi-supervised volumetric medical image segmentation (SSVMIS), to leverage both labeled and unlabeled data. However, current SSVMIS methods [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18] assume that the labeled and unlabeled data are from the same domain, implying they share the same distribution. In practice, medical images are often collected from different clinical centers using various scanners, resulting in significant domain shifts. These shifts arise due to differences in patient populations, scanners, and scan acquisition settings. As a consequence, these SSVMIS methods have limitations in real-world application scenarios and frequently encounter overfitting issues, leading to suboptimal results.

To address this limitation, researchers have increasingly focused on Unsupervised Domain Adaptation (UDA) techniques. These techniques leverage both labeled (source domain) and unlabeled data (targetdomain) for training, but the data originate from different domains. Furthermore, Semi-supervised Domain Generalization (SemiDG), a more stringent scenario, has garnered significant interest. SemiDG utilizes labeled and unlabeled data from multiple domains during training and is evaluated on an unseen domain. Currently, methods for these three scenarios are optimized separately, and there is no existing approach that addresses all three scenarios within a unified framework. However, given that all training stages involve labeled and unlabeled data, it is intuitive to explore a **generic SSL-based framework** that can handle all settings and eliminate the need for complex task-specific designs. Therefore, this paper aims to develop a generic framework that can handle existing challenges in real-world scenarios, including:

* _Scenario 1: SSL (Figure 1(a)): The sample data used for both training and testing are from the same domain, representing the standard SSL setting._
* _Scenario 2: UDA (Figure 1(b)): The sampled data originate from two domains, with the labels of the target domain being inaccessible, representing the UDA setting._
* _Scenario 3: SemiDG (Figure 1(c)): The sampled data encompasses multiple domains, with only a limited number of them being labeled, representing the SemiDG setting._

Potential similarities can be found and summarized as follows: (1) in the training stage, both labeled data and unlabeled data are used; (2) in the scenario of the real-world application domain, whether the distribution shifts in SSL or the domain shifts in UDA and SemiDG can all be regarded as _sampling bias_, _i.e._, the main difference is how we sample the data in Figure 1.

Now we wonder whether the existing SSVMIS methods are powerful enough to handle this general task. Experimental results show that the existing SSL methods do not work well on UDA and SemiDG settings, as shown in Table 3 & 4, and vice versa (Table 2). _One of the main obstacles lies in the severe over-fitting of these models, which is caused by the dominance of labeled data during training_. Specifically, the state-of-the-art SSVMIS methods are mainly based on two frameworks: (1) Teacher-student framework [1], where a student model is first trained with the labeled data, and a teacher model obtained from the EMA of the student model generates the pseudo label to re-train the student model with labeled data, see Figure 2(a); (2) CPS (Cross Pseudo Supervision) [3] framework, which leverages the consistency between two perturbed models and the pseudo label generated by one of the networks will be used to train the other network, see Figure 2(b). The predicting modules in these two main frameworks are trained with both labeled and unlabeled data; however, the labeled data, with precise ground truths as supervision, converges more rapidly compared with the unlabeled data. **Thus, the training process will easily get overwhelmed by the supervised training task**, as shown in Figure 3. Another challenge lies in the fact that existing SSVMIS methods fail to address the issue of distribution shifts, let alone domain shifts, resulting in a limitation in capturing features that are invariant to changes in distribution.

Figure 1: From (a), (b) to (c): generalizing SSL to UDA and SemiDG settings by sampling more diverse data to form the training and testing sets.

Based on the similarities and the main weaknesses of the mainstream SSVMIS methods, we argue that a generic framework is possible if we can solve the over-fitting issue and design a powerful methods to capture the distribution-invariant features. To tackle the above issues and design a generic SSVMIS methods for the real-world application scenarios, this work proposes a novel Aggregating & Decoupling (A&D) framework. Specifically, A&D consists of an Aggregating stage and a Decoupling stage. In the Aggregating stage, based on the recent success of the diffusion model [19; 20], we propose a Diff-VNet to aggregate the multi-domain features into one shared encoder to construct a _common knowledge set_ to improve the capacity of capturing the distribution-invariant features. To solve the over-fitting issue, in the Decoupling stage, we decouple the decoding process to (1) a labeled data training flow which mainly updates a Diff-VNet decoder and a difficulty-aware V-Net decoder to generate high-quality pseudo labels; (2) an unlabeled data training flow which mainly updates another vanilla V-Net decoder with the supervision of the pseudo labels. The denoising process of the Diff-VNet decoder provides the domain-unbiased pseudo labels while the difficulty-aware V-Net decoder class-unbiased pseudo labels. We also propose a re-parameterizing & smoothing combination strategy to further improve the quality of the pseudo labels.

The key contributions of our work can be summarized as follows: (1) we unify the SSL, Class Imbalanced SSL, UDA, and SemiDG for volumetric medical image segmentation with one generic framework; (2) we state the over-fitting issues of the current SSL methods and propose to solve it by an efficient data augmentation strategy and decoupling the decoders for labeled data and unlabeled, respectively; (3) we introduce the Diffusion V-Net to learn the underlying feature distribution from different domains to generalize the SSL methods to more realistic application scenarios; (4) The proposed Aggregating & Decoupling framework achieves state-of-the-art on representative datasets of SSL, class-imbalance SSL, UDA, and SemiDG tasks. Notably, our method achieves significant improvements on the Synapse dataset (12.3 in Dice) and the MMWHS dataset in the MR to CT setting (8.5 in Dice). Extensive ablation studies are conducted to validate the effectiveness of the proposed methods.

## 2 Related Work

### Semi-supervised Segmentation & the Class Imbalance Issue

Semi-supervised segmentation aims to explore tremendous unlabeled data with supervision from limited labeled data. Recently, self-training-based methods [3; 4; 21] have become the mainstreamof this domain. Approaches with consistency regularization strategies [22; 3; 21] achieved good performance by encouraging high similarity between the predictions of two perturbed networks for the same input image, which highly improved the generalization ability. In the medical image domain, the data limitation issue is more natural and serious. Existing approaches [23; 24; 10; 14; 13; 17; 16; 25] to combat the limited data have achieved great success but are bottlenecked by the application scenarios and cannot handle more challenging but practical settings such as UDA and SemiDG.

Class Imbalance IssueClass imbalance issue is a significant problem to extend the existing SSL-based methods to more practical setting, since medical datasets have some classes with notably higher instances in training samples than others. In natural image domain, different means are proposed to solve this issue, including leveraging unlabeled data [26; 27; 28; 29; 30], re-balancing data distributions in loss [31; 30; 32], debiased learning [6; 5]_etc._ In medical image domain, this issue is more severe but only few work [15; 33; 25] noticed this problem. Incorporating the class-imbalance awareness is crucial for the generalization of the SSL methods.

### Unsupervised Domain Adaptation & Semi-supervised Domain Generalization

Domain adaptation (DA) aims to solve the domain shifts by jointly training the model with source domain data and target domain data. Unsupervised Domain Adaptation (UDA) [34; 35; 36; 37] is the most challenging and practical setting among all the DA setting, since no target labels are required. In this context, UDA is becoming increasingly important in the medical image segmentation field, and as a result, a myriad of UDA approaches have been developed for cross-domain medical image segmentation by leveraging: generative adversarial-based methods [38; 39; 40; 41; 42; 43; 44; 45; 46], semi-supervised learning techniques [47; 48; 49], and self-training as well as contrastive learning techniques [50; 51], _etc_. Though with promising adaptation results, these methods highly rely on the unlabeled target domain information, which hinders the generalizability.

Domain generalization (DG) is a more strict setting, where the difference with DA is that the model does not use any information from the target domain. Unlike unsupervised domain adaptation, semi-supervised domain generalization (SemiDG) does not assume access to labeled data from the target domains. Existing SemiDG methods [52; 53] leverage various unusual strategies to solve the domain shifts, _e.g._, meta-learning [52], Fourier Transformation [54], compositionality [55]_etc._, which are not general and have unsatisfactory performance on the tasks such as UDA and SSL.

Compared to these prior works, our work is the first to unify SSL, Imbalanced SSL, UDA, and SemiDG settings. This extension not only amplifies the scope and versatility of SSL-based frameworks in medical image segmentation but also stands in stark contrast to earlier approaches that remained confined to singular domains such as SSL or UDA.

### Diffusion Model

Denoising diffusion models [56; 19; 57; 58] have shown significant success in various generative tasks [59; 60; 61; 62; 63], due to the powerful ability of modeling the underlying distribution of the data, conceptually having a greater capacity to handle challenging tasks. Noticing this property, there has been a rise in interest to incorporate them into segmentation tasks, including both natural image segmentation [64; 65; 66], and medical image segmentation [67; 68; 20] Given the notable achievements of diffusion models in these respective domains, leveraging such models to develop generation-based perceptual models would prove to be a highly promising avenue to push the boundaries of perceptual tasks to newer heights.

## 3 Method

### Overview of the Aggregating & Decoupling Framework

In this section, we will introduce our **Aggregating & Decoupling (A&D)** framework, as shown in Figure 4, which consists of an Aggregating stage and a Decoupling stage. The training pipeline is illustrated in Algorithm 1.

The **Aggregating stage** aims to construct a _common knowledge set_ across domains based on the idea that all data share common underlying high-level knowledge, such as texture information.

By aggregating information from multiple domains and jointly trained, the encoder can capture the underlying domain-invariant features. To achieve this, we introduce a powerful yet efficient Sampling-based Volumetric Data Augmentation (SVDA) strategy to enlarge the distribution diversity and leverage the diffusion model to capture the invariant features of the diversified data.

In the decoding of the existing SSL methods, the decoders are simultaneously trained with both labeled and unlabeled data, which leads to coupling and over-fitting issues and further hinder the extending to the general SSL. The **Decoupling stage** aims to solve these issues by decoupling the labeled and unlabeled data training flows. Concretely, for the labeled data flow, (1) a diffusion decoder is mainly used to guide the diffusion encoder to learn the distribution-invariant representations through the diffusion backward process, and thus produces the _domain-unbiased pseudo labels_; (2) a vanilla V-Net decoder with the proposed difficulty-aware re-weighting strategy is mainly to avoid the model over-fit to the easy and majority classes, and thus produces the _class-unbiased pseudo labels_. Then, for the unlabeled data flow, the domain- and class-unbiased pseudo labels are ensembled through a proposed Reparameterize & Smooth (RS) strategy to generate high quality pseudo labels. Finally, the pseudo labels are used to supervise the training of an additional V-Net decoder for prediction only.

### Aggregating Stage

Assume that the entire dataset comprises of \(N_{L}\) labeled samples \(\{(x_{i}^{l},y_{i})\}_{i=1}^{N_{L}}\) and \(N_{U}\) unlabeled samples \(\{x_{i}^{u}\}_{i=1}^{N_{U}}\), where \(x_{i}\in\mathbb{R}^{D\times H\times W}\) is the input volume and \(y_{i}\in\mathbb{R}^{K\times D\times H\times W}\) is the ground-truth annotation with \(K\) classes. The goal of the aggregating stage is to augment the data with SVDA and encode the labeled \((x^{l},y)\) and unlabeled data \(x^{u}\) to high-level distribution-invariant features for denoising labeled data flow \(h^{l;\xi}\) difficulty-aware labeled data flow \(h^{l;\psi}\), and unlabeled data flow \(h^{u}\).

Sampling-based Volumetric Data Augmentation (SVDA)Instead of the time-consuming traditional data augmentation used in [69] which cascaded all the augmentations, SVDA build upon an augmentation set and \(N_{aug}\) operations are randomly sampled to apply to both the labeled and unlabeled data. The augmentation set consists of 3D spatial-wise transforms (random crop, random rotation, random scaling) and voxel-wise transforms (Gaussian blur, brightness, contrast, gamma). \(N_{aug}\) is empirically set to 3.

Figure 4: Overview of the proposed **Aggregating & Decoupling** framework. Blue and orange regions denote the training process with labeled data and unlabeled data, respectively. We separate the training of the decoders using labeled data and unlabeled data, and only use the decoder trained with unlabeled data for prediction.

**Diffusion for Capturing Invariant Features** We follow Diff-UNet [20] to use diffusion model for perception but modify it to a V-Net version and remove the additional image encoder. Given the labeled volume data \(x^{l}\in\mathbb{R}^{D\times W\times H}\) and its label \(y\in\mathbb{R}^{D\times W\times H}\), we first convert the label to the one-hot format \(y_{0}\in\mathbb{R}^{K\times D\times W\times H}\) and add successive \(t\) step noise \(\epsilon\) to obtain the noisy label \(y_{t}\in\mathbb{R}^{K\times D\times W\times H}\), which is the diffusion forward process:

\[y_{t}=\sqrt{\bar{\alpha}_{t}}y_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon,\epsilon \in\mathcal{N}(0,1)\] (1)

Then, the noisy label is concatenated with the image \(x^{l}\) as the input of the Diff-VNet. Concretely, the high-level features are different for different data flows. For the denoising flow, _i.e._, \(D(x^{l};\xi)\) as decoder, the diffusion encoder takes concatenation \(\mathtt{concat}([y_{t},x^{l}])\) and time step \(t\) as input to generate the time-step-embedded multi-scale features \(h_{i}^{l;\xi}\in\mathbb{R}^{i\times F\times\frac{H}{2}\times\frac{W}{2^{l}} \times\frac{H}{2^{l}}}\) where \(i\) is the stage and \(F\) is the basic feature size. \(h_{i}^{l;\xi}\) are further used by \(D(x^{l};\xi)\) to predict the clear label \(y_{0}\). For the difficulty-aware training flow and the unlabeled data flow, _i.e._, \(D(x^{l};\psi)\) and \(D(x^{u};\theta)\) as decoders, the encoder only takes \(x_{l}\) and \(x_{u}\) as input to obtain the multi-scale features \(h_{i}^{l;\psi}\), \(h_{i}^{u}\), respectively. Note that \(h_{i}^{l;\psi}\), \(h_{i}^{u}\) are with same shapes with \(h_{i}^{l;\xi}\).

### Decoupling Stage

The decoupling stage consists of four steps: supervised denoising training to generate domain-unbiased pseudo labels, supervised difficulty-aware training to generate class-unbiased pseudo labels, pseudo labeling to ensemble the two pseudo labels and unsupervised training to get the final predictor.

Supervised Denoising Training with the Diffusion Decoder \(D(x^{l};\xi)\)Taking \(h_{i}^{l;\xi}\) as inputs, \(D(x^{l};\xi)\) decodes the features to predict the clear label \(y_{0}\) as domain-unbiased pseudo label. The objective function is defined as follow:

\[\mathcal{L}_{deno}=\frac{1}{N_{L}}\sum_{i=0}^{N_{L}}\mathcal{L}_{DiceCE}(p_{i} ^{l;\xi},y_{i})\] (2)

where \(\mathcal{L}_{DiceCE}(x,y)=\frac{1}{2}[\mathcal{L}_{CE}(x,y)+\mathcal{L}_{Dice} (x,y)]\) is the combined dice and cross entropy loss.

Supervised Difficulty-aware Training with \(D(x^{l};\psi)\)To alleviate the common class imbalance issue in SSVMIS, we design a Difficulty-aware Re-weighting Strategy (DRS) to force the model to focus on the most difficult classes (_i.e._ the classes learned slower and with worse performances). The difficulty is modeled in two ways with the probability map \(p^{l;\xi}\) produced by diffusion decoder \(D(x^{l};\xi)\): learning speed and performance. We use Population Stability Index [70] to measure thelearning speed of each class after the \(e^{th}\) iteration:

\[du_{k,e}=\sum_{e-\tau}^{e}\min(\triangle,0)\text{ln}(\frac{\lambda_{k,e}}{ \lambda_{k,e-1}}),\quad dl_{k,e}=\sum_{e-\tau}^{e}\max(\triangle,0)\text{ln}( \frac{\lambda_{k,e}}{\lambda_{k,e-1}})\] (3)

where \(\lambda_{k}\) denotes the Dice score of \(p^{l;\xi}\) of \(k^{th}\) class in \(e^{th}\) iteration and \(\triangle=\lambda_{k,e}-\lambda_{k,e-1}\). \(du_{k,e}\) and \(dl_{k,e}\) denote classes not learned and learned after \(e^{th}\) iteration. \(\tau\) is the number accumulation iterations and set to 50 empirically. Then, we define the difficulty of \(k^{th}\) class after \(e^{th}\) iteration as:

\[d_{k,e}=\frac{du_{k,e}}{dl_{k,e}}\] (4)

where the classes learned faster have smaller \(d_{k,e}\), the corresponding weights in the loss function will be smaller to slow down the learn speed. We also accumulate \(1-\lambda_{k,e}\) for \(\tau\) iterations to obtain the reversed dice weight \(w_{\lambda_{k,e}}\) and weight \(d_{k,e}\). In this case, classes with lower dice scores will have larger weights in the loss function, which forces the model to pay more attention to these classes. The overall difficulty-aware weight of \(k^{th}\) class is defined as:

\[w_{k}^{diff}=w_{\lambda_{k,e}}\cdot(d_{k,e})^{\alpha}\] (5)

where \(\alpha\) is empirically set to \(\frac{1}{5}\) in the experiments to alleviate outliers. The objective function of the supervised difficulty-aware training is defined as follow:

\[\mathcal{L}_{diff}=\frac{1}{N_{L}}\frac{1}{K}\sum_{i=0}^{N_{L}}\sum_{k=0}^{K}w _{k}^{diff}\mathcal{L}_{DiceCE}(p_{i,k}^{l;\psi},y_{i,k})\] (6)

Pseudo Labeling with Reparameterize & Smooth (RS) StrategyThe domain-unbiased \(p^{u;\xi}\) probability map is generated by iterating the diffusion model (\(E(x^{l},x^{u};\xi)\)+\(D(x^{l};\xi)\)) \(t\) times with the Denoising Diffusion Implicit Models (DDIM) method [57]. The class-unbiased \(p^{u;\psi}\) probability map can be obtained by \(D(x^{l};\psi)\) with stopped gradient forward pass. We ensemble \(p^{u;\xi}\) and \(p^{u;\psi}\) to generate high-quality pseudo labels. However, when combining these two maps, we found that the denoised probability map \(p^{u;\psi}\) is too sparse, _i.e._, with very high confidence of each class. This property is benefit for the fully-supervised tasks, but in this situation, it will suppress \(p^{u;\psi}\) and is not robust to noise and error. Thus, we re-parameterize \(p^{u;\psi}\) with the Gumbel-Softmax to add some randomness and using Gaussian blur kernel to remove the noise brought by this operation. The final pseudo label is:

\[y^{\xi,\psi}=\texttt{argmax}(\texttt{Gumbel-Softmax}(p^{u;\xi})+\texttt{ Softmax}(p^{u;\psi}))\] (7)

Unsupervised Training with \(D(x^{u};\theta)\)Finally, we can use the pseudo label \(y^{\xi,\psi}\) to train \(D(x^{u};\theta)\) in an unsupervised manner. The objective function of the unsupervised training is defined as:

\[\mathcal{L}_{u}=\frac{1}{N_{U}}\sum_{i=0}^{N_{U}}\mathcal{L}_{DiceCE}(p_{i}^{u; \theta},y^{\xi,\psi})\] (8)

To better leverage the domain- and class-unbiased features, we also transmit with knowledge distillation strategy: \(\theta=w_{ema}\times\theta+(1-w_{ema})\times(\xi+\psi)/2,w_{ema}=0.99\). The overall training function of the A&G framework is:

\[\mathcal{L}=\mathcal{L}_{deno}+\mathcal{L}_{diff}+\mu\mathcal{L}_{u}\] (9)

where \(\mu\) is empirically set as \(10\) and follow [15] to use the epoch-dependent Gaussian ramp-up strategy to gradually enlarge the ratio of unsupervised loss. In the inference stage, only the diffusion encoder \(E(x^{l},x^{u};\xi)\) and \(D(x^{u};\theta)\) are used to generate the predictions.

## 4 Experiments

### Datasets and Implementation Details

We evaluate our proposed A&D framework on four datasets for four tasks, _i.e._, LASeg dataset [72] for SSL, Synapse dataset [73] for class imbalanced SSL, MMWHS dataset [74] for UDA, and M&Ms

[MISSING_PAGE_FAIL:8]

significant improvements over the existing state-of-the-art approach on the Synapse dataset with 20% labeled data and the MMWHS dataset in the MR to CT setting, demonstrating a substantial increase of 12.3 in Dice score for the Synapse dataset and 8.5 for the MMWHS dataset.

### Analyses

Architecture AnalysisAs shown in Figure 5, we test different architectures for the framework, the corresponding results are in Table 5. When using three separate networks (a)(b), the performance drop significantly, the reason is that this structure can not learn the domain-invariant features. Using pure V-Net (c) or diffusion-based networks (d) also leads to inferior results, especially for the pure diffusion model, it is hard to train due to the limited labeled data, high-quality pseudo labels are hard to obtain, which further hinder the training of the unsupervised branch.

Ablation on the componentsWe analyze the effectiveness of different components in our method. According to the results in Table 6, on MMWHS MR to CT setting (UDA), when removing the diffusion model, performance decreases the most, which indicated the importance ability of the diffusion model for capturing the distribution-invariant features. The result of removing the SVDA also indicates that when the data is not diverse enough, the diffusion model cannot capture effective underlying features. On Synapse dataset for IBSSL, the results are slightly different with those in the UDA setting, the DRS plays more important role than the Diffusion. In 5% M&Ms dataset for SemiDG, the results are quite aligned with results in the UDA setting.

Hyper-parameter SelectionWe evaluate the performance of our method under different time step \(t\) of the diffusion model and the number of sampled augmentations, as shown in Table 7.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Arch. & a & b & c & d & e \\ \hline Dice & 76.14 & 50.53 & 82.19 & 45.63 & 90.14 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation study of the components in our framework on 20% labeled Synapse setting (IBSSL), MMWHS MR to CT setting (UDA) and 2% labeled 2% labeled M&Ms setting (SemiDG).

\begin{table}
\begin{tabular}{l c c c c c||c c c c|c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c||}{2\% Labeled data} & \multicolumn{4}{c}{5\% Labeled data} \\  & Domain A & Domain B & Domain C & Domain D & Average & Domain A & Domain B & Domain C & Domain D & Average \\ \hline nnUNet [69] & 52.87 & 64.63 & 72.97 & 73.27 & 65.94 & 65.30 & 79.73 & 78.06 & 81.25 & 76.09 \\ SDNet+Aug [53] & 54.48 & 67.81 & 76.46 & 74.35 & 68.28 & 71.21 & 77.31 & 81.40 & 79.95 & 77.47 \\ LDDG [78] & 59.47 & 56.16 & 68.21 & 68.56 & 63.16 & 66.22 & 69.49 & 73.40 & 75.66 & 71.29 \\ SAML [79] & 56.31 & 56.32 & 75.70 & 69.94 & 64.57 & 67.11 & 76.35 & 77.43 & 78.64 & 74.88 \\ BCP [76]\({}^{*}\) & 71.57 & 76.20 & 76.87 & 77.94 & 75.65 & 73.66 & 79.04 & 77.01 & 78.49 & 77.05 \\ DGNet [52] & 66.01 & 72.72 & 77.54 & 75.14 & 72.85 & 72.40 & 80.30 & 82.51 & 83.77 & 79.75 \\ vMFNet [55] & 73.13 & 77.01 & **81.57** & 82.02 & 78.43 & 77.06 & 82.29 & **84.01** & **85.13** & 82.12 \\ \(\mathbf{\textbf{A\&D}}\)**(ours)** & **79.62** & **82.26** & 80.03 & **83.31** & **81.31** & **81.71** & **85.44** & 82.18 & 83.9 & **83.31** \\ \hline \hline \multicolumn{10}{l}{\({}^{*}\) SOTA method on semi-supervised segmentation.} \\ \end{tabular}
\end{table}
Table 4: Results on two settings of M&Ms dataset for **SemiDG** task.

Figure 5: Ablation study on different architectures. (e) is the final framework

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Methods & A\&D & w/o SVDA & w/o Diffusion & w/o DRS & w/o RS \\ \hline IBSSL & 60.9 & 55.3 & 56.7 & 52.0 & 58.7 \\ UDA & 90.1 & 84.6 & 79.2 & 85.8 & 87.0 \\ SemDG & 80.6 & 77.9 & 74.9 & 78.2 & 78.6 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation study of the noise step \(t\) and the number of sampled augmentations on MMWHS CT to MR setting.

VisualizationWe also present the visualization results to further analyze our method. As shown in Figure 6, for the UDA task, our framework can generate smoother volumetric objects. Our framework also has detect minority classes, as can be seen in Appendix, which indicates the effectiveness of incorporating the class-imbalance awareness with the proposed difficulty-aware re-weighing strategy.

LimitationsThe diffusion process introduces additional training costs; however, the inference efficiency remains unaffected as only the decoder trained using unlabeled data is utilized during inference. Moreover, the failure cases are mainly in the M&Ms dataset for SemiDG setting. Specifically, our method usually fails on the first and the last slices along the depth axis. Due to the restricted depth dimension (less than 10), the 2D slices and the corresponding masks vary significantly. In such a case, it is hard for our volumetric framework to capture depth-wise information for the first or last slice with only one neighboring slice as a reference, and thus leads to false positive results.

## 5 Conclusion

In this paper, we propose a generic framework for semi-supervised learning in volumetric medical image segmentation, called Aggregating & Decoupling. This framework addresses four related settings, namely SSL, class imbalanced SSL, UDA, and SemiDG. Specifically, the aggregating part of our framework utilizes a Diffusion encoder to construct a "common knowledge set" by extracting distribution-invariant features from aggregated information across multiple distributions/domains. On the other hand, the decoupling part involves three decoders that facilitate the training process by decoupling labeled and unlabeled data, thus mitigating overfitting to labeled data, specific domains, and classes. Experimental results validate the effectiveness of our proposed method under four settings.

The significance of this work lies in its ability to encourage semi-supervised medical image segmentation methods to address more complex real-world application scenarios, rather than just developing frameworks in ideal experimental environments. Furthermore, we have consolidated all four settings within a single codebase, enabling the execution of any task using a single bash file by merely adjusting the arguments. We believe that this consolidated codebase will be convenient for further research and beneficial for the community.

## Acknowledgement

We thank the anonymous NeurIPS reviewers for providing us with valuable feedback that greatly improved the quality of this paper! This work was supported in part by the Hong Kong Innovation and Technology Fund under Project ITS/030/21 and in part by Foshan HKUST Projects under Grants FSUST21-HKUST10E and FSUST21- HKUST11E and in part by the Project of Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone (HZQB-KCZYB-2020083).

Figure 6: Visualization of the UDA task in 2D and 3D views.

## References

* [1]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [2]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [3]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems33 (2), pp.. Cited by: SS1.
* [4]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [5]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [6]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [7]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [8]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [9]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [10]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [11]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [12]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [13]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [14]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [15]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [16]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [17]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [18]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [19]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [20]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [21]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [22]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [23]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [24]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [25]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [26]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [27]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [28]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [29]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [30]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [31]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [32]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [33]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [34]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [35]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [36]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [37]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [38]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [39]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [40]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [41]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [42]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [43]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [44]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [45]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [46]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [47]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [48]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [49]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [50]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [51]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [52]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [53]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [54]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [55]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [56]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [57]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [58]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [59]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems30. Cited by: SS1.
* [60]A. Tarvainen and H. Valpola (2017) Mean teachers are better role models: weight-averaged consistency targets improve semi-* [30] Z. Lai, C. Wang, S.-c. Cheung, and C.-N. Chuah, "Sar: Self-adaptive refinement on pseudo labels for multiclass-imbalanced semi-supervised learning," in _CVPR_, pp. 4091-4100, 2022.
* [31] Y. Hong, S. Han, K. Choi, S. Seo, B. Kim, and B. Chang, "Disentangling label distribution for long-tailed visual recognition," in _CVPR_, pp. 6626-6636, 2021.
* [32] S. Yu, J. Guo, R. Zhang, Y. Fan, Z. Wang, and X. Cheng, "A re-balancing strategy for class-imbalanced classification based on instance difficulty," in _CVPR_, pp. 70-79, 2022.
* [33] H. Basak, S. Ghosal, and R. Sarkar, "Addressing class imbalance in semi-supervised image segmentation: A study on cardiac mri," in _MICCAI_, pp. 224-233, 2022.
* [34] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, "Unpaired image-to-image translation using cycle-consistent adversarial networks," in _Proceedings of the IEEE international conference on computer vision_, pp. 2223-2232, 2017.
* [35] Y. Huo, Z. Xu, H. Moon, S. Bao, A. Assad, T. K. Moyo, M. R. Savona, R. G. Abramson, and B. A. Landman, "Synseg-net: Synthetic segmentation without target modality ground truth," _IEEE transactions on medical imaging_, vol. 38, no. 4, pp. 1016-1025, 2018.
* [36] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros, and T. Darrell, "Cycada: Cycle-consistent adversarial domain adaptation," in _International conference on machine learning_, pp. 1989-1998, Pmlr, 2018.
* [37] Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker, "Learning to adapt structured output space for semantic segmentation," in _CVPR_, pp. 7472-7481, 2018.
* [38] Q. Dou, C. Ouyang, C. Chen, H. Chen, B. Glocker, X. Zhuang, and P.-A. Heng, "Pnp-adanet: Plug-and-play adversarial domain adaptation network at unpaired cross-modality cardiac segmentation," _IEEE Access_, vol. 7, pp. 99065-99076, 2019.
* [39] C. Chen, Q. Dou, H. Chen, J. Qin, and P.-A. Heng, "Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation," in _Proceedings of the AAAI conference on artificial intelligence_, vol. 33, pp. 865-872, 2019.
* [40] M. Bateson, H. Everade, J. Dolz, H. Lombaert, and I. Ben Ayed, "Source-relaxed domain adaptation for image segmentation," in _MICCAI_, pp. 490-499, Springer, 2020.
* [41] C. Chen, Q. Dou, H. Chen, J. Qin, and P. A. Heng, "Unsupervised bidirectional cross-modality adaptation via deeply synergistic image and feature alignment for medical image segmentation," _IEEE transactions on medical imaging_, vol. 39, no. 7, pp. 2494-2505, 2020.
* [42] D. Zou, Q. Zhu, and P. Yan, "Unsupervised domain adaptation with dual-scheme fusion network for medical image segmentation," in _IJCAI_, pp. 3291-3298, 2020.
* [43] X. Han, L. Qi, Q. Yu, Z. Zhou, Y. Zheng, Y. Shi, and Y. Gao, "Deep symmetric adaptation network for cross-modality medical image segmentation," _IEEE transactions on medical imaging_, vol. 41, no. 1, pp. 121-132, 2021.
* [44] K. Yao, Z. Su, K. Huang, X. Yang, J. Sun, A. Hussain, and F. Coenen, "A novel 3d unsupervised domain adaptation framework for cross-modality medical image segmentation," _IEEE Journal of Biomedical and Health Informatics_, vol. 26, no. 10, pp. 4976-4986, 2022.
* [45] X. Bian, X. Luo, C. Wang, W. Liu, and X. Lin, "Dda-net: Unsupervised cross-modality medical image segmentation via dual domain adaptation," _Computer Methods and Programs in Biomedicine_, vol. 213, p. 106531, 2022.
* [46] H. Shin, H. Kim, S. Kim, Y. Jun, T. Eo, and D. Hwang, "Cosmos: Cross-modality unsupervised domain adaptation for 3d medical image segmentation based on target-aware domain translation and iterative self-training," _arXiv preprint arXiv:2203.16557_, 2022.
* [47] C. S. Perone, P. Ballester, R. C. Barros, and J. Cohen-Adad, "Unsupervised domain adaptation for medical imaging segmentation with self-ensembling," _NeuroImage_, vol. 194, pp. 1-11, 2019.
* [48] Y. Xia, D. Yang, Z. Yu, F. Liu, J. Cai, L. Yu, Z. Zhu, D. Xu, A. Yuille, and H. Roth, "Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation," _Medical image analysis_, vol. 65, p. 101766, 2020.
* [49] X. Liu, F. Xing, N. Shusharina, R. Lim, C.-C. Jay Kuo, G. El Fakhri, and J. Woo, "Act: Semi-supervised domain-adaptive medical image segmentation with asymmetric co-training," in _MICCAI_, pp. 66-76, Springer, 2022.
* [50] L. Liu, Z. Zhang, S. Li, K. Ma, and Y. Zheng, "S-cuda: Self-cleansing unsupervised domain adaptation for medical image segmentation," _Medical Image Analysis_, vol. 74, p. 102214, 2021.
* [51] M. Gu, S. Vesal, M. Thies, Z. Pan, F. Wagner, M. Rusu, A. Maier, and R. Kosti, "Conftuda: Contrastive few-shot unsupervised domain adaptation for medical image segmentation," _arXiv preprint arXiv:2206.03888_, 2022.
* [52] X. Liu, S. Thermos, A. O'Neil, and S. A. Tsaftaris, "Semi-supervised meta-learning with disentanglement for domain-generalised medical image segmentation," in _MICCAI_, pp. 307-317, Springer, 2021.
* [53] X. Liu, S. Thermos, A. Chartsias, A. O'Neil, and S. A. Tsaftaris, "Disentangled representations for domain-generalized cardiac segmentation," in _Statistical Atlases and Computational Models of the Heart. M&Ms and EMIDEC Challenges: 11th International Workshop, STACOM 2020, Held in conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Revised Selected Papers 11_, pp. 187-195, Springer, 2021.
* [54] H. Yao, X. Hu, and X. Li, "Enhancing pseudo label quality for semi-supervised domain-generalized medical image segmentation," in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 36,pp. 3099-3107, 2022.
* [55] X. Liu, S. Thermos, P. Sanchez, A. Q. O'Neil, and S. A. Tsaftaris, "vmfnet: Compositionality meets domain-generalised segmentation," in _MICCAI_, pp. 704-714, Springer, 2022.
* [56] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, "Deep unsupervised learning using nonequilibrium thermodynamics," in _International Conference on Machine Learning_, pp. 2256-2265, PMLR, 2015.
* [57] J. Song, C. Meng, and S. Ermon, "Denoising diffusion implicit models," _arXiv preprint arXiv:2010.02502_, 2020.
* [58] A. Q. Nichol and P. Dhariwal, "Improved denoising diffusion probabilistic models," in _International Conference on Machine Learning_, pp. 8162-8171, PMLR, 2021.
* [59] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen, "Glide: Towards photorealistic image generation and editing with text-guided diffusion models," _arXiv preprint arXiv:2112.10741_, 2021.
* [60] P. Dhariwal and A. Nichol, "Diffusion models beat gans on image synthesis," _Advances in Neural Information Processing Systems_, vol. 34, pp. 8780-8794, 2021.
* [61] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi, "Image super-resolution via iterative refinement," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [62] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, "Video diffusion models," _arXiv preprint arXiv:2204.03458_, 2022.
* [63] B. L. Trippe, J. Yim, D. Tischer, T. Broderick, D. Baker, R. Barzilay, and T. Jaakkola, "Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem," _arXiv preprint arXiv:2206.04119_, 2022.
* [64] T. Amit, T. Shaharbany, E. Nachmani, and L. Wolf, "Segdiff: Image segmentation with diffusion probabilistic models," _arXiv preprint arXiv:2112.00390_, 2021.
* [65] T. Chen, L. Li, S. Saxena, G. Hinton, and D. J. Fleet, "A generalist framework for panoptic segmentation of images and videos," _arXiv preprint arXiv:2210.06366_, 2022.
* [66] Y. Ji, Z. Chen, E. Xie, L. Hong, X. Liu, Z. Liu, T. Lu, Z. Li, and P. Luo, "Ddp: Diffusion model for dense visual prediction," _arXiv preprint arXiv:2303.17559_, 2023.
* [67] J. Wu, H. Fang, Y. Zhang, Y. Yang, and Y. Xu, "Medsegdiff: Medical image segmentation with diffusion probabilistic model," _arXiv preprint arXiv:2211.00611_, 2022.
* [68] J. Wolleb, R. Sandkuhler, F. Bieder, P. Valmaggia, and P. C. Cattin, "Diffusion models for implicit image segmentation ensembles," in _International Conference on Medical Imaging with Deep Learning_, pp. 1336-1348, PMLR, 2022.
* [69] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein, "nnu-net: a self-configuring method for deep learning-based biomedical image segmentation," _Nature methods_, vol. 18, no. 2, pp. 203-211, 2021.
* [70] H. Jeffreys, "An invariant form for the prior probability in estimation problems," _Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences_, vol. 186, no. 1007, pp. 453-461, 1946.
* [71] L.-Z. Guo and Y.-F. Li, "Class-imbalanced semi-supervised learning with adaptive thresholding," in _ICML_, pp. 8082-8094, PMLR, 2022.
* [72] Z. Xiong, Q. Xia, Z. Hu, N. Huang, C. Bian, Y. Zheng, S. Vesal, N. Ravikumar, A. Maier, X. Yang, _et al._, "A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging," _Medical image analysis_, vol. 67, p. 101832, 2021.
* [73] B. Landman, Z. Xu, J. Igelsias, M. Styner, T. Langerak, and A. Klein, "2015 miccai multi-atlas labeling beyond the cranial vault-workshop and challenge," 2015.
* [74] X. Zhuang and J. Shen, "Multi-scale patch and multi-modality atases for whole heart segmentation of mri," _Medical image analysis_, vol. 31, pp. 77-87, 2016.
* [75] V. M. Campello, P. Okontra, C. Izquierdo, C. Martin-Isla, A. Sojoudi, P. M. Full, K. Maier-Hein, Y. Zhang, Z. He, J. Ma, _et al._, "Multi-centre, multi-vendor and multi-disease cardiac segmentation: the mkms challenge," _IEEE Transactions on Medical Imaging_, vol. 40, no. 12, pp. 3543-3554, 2021.
* [76] Y. Bai, D. Chen, Q. Li, W. Shen, and Y. Wang, "Bidirectional copy-paste for semi-supervised medical image segmentation," in _CVPR_, pp. 11514-11524, 2023.
* [77] M. Jafari, S. Francis, J. M. Garibaldi, and X. Chen, "Lmisa: A lightweight multi-modality image segmentation network via domain adaptation using gradient magnitude and shape constraint," _Medical Image Analysis_, vol. 81, p. 102536, 2022.
* [78] H. Li, Y. Wang, R. Wan, S. Wang, T.-Q. Li, and A. Kot, "Domain generalization for medical imaging classification with linear-dependency regularization," _Advances in Neural Information Processing Systems_, vol. 33, pp. 3118-3129, 2020.
* [79] Q. Liu, Q. Dou, and P.-A. Heng, "Shape-aware meta-learning for generalizing prostate mri segmentation to unseen domains," in _MICCAI_, pp. 475-485, Springer, 2020.

## Appendix A Appendix

### More Details of Datasets and Implementation

The hyper-parameters for different datasets are shown in Table 8.

The details of the datasets and the pre-processing operations are as follows.

LASeg Dataset for SSLThe Atrial Segmentation Challenge (LASeg) dataset [72] provides 100 3D gadolinium-enhanced MR imaging scans (GE-MRIs) and LA segmentation masks for training and validation. Following previous work [8; 17], we split the 100 scans into 80 for training and 20 for evaluation. We use the processed datasets from [8] where all the scans were cropped centering at the heart region for better comparison of the segmentation performance of different methods and normalized as zero mean and unit variance. In the training stage, SS-Net [17] and BCP [76] use test set for validation to select the best model, which is unreasonable. We use labeled data instead and achieve better performances.

Synapse Dataset for Class Imbalanced SSLThe Synapse [73] dataset has 13 foreground classes, including spleen (Sp), right kidney (RK), left kidney (LK), gallbladder (Ga), esophagus (Es), liver(Li), stomach(St), aorta (Ao), inferior vena cava (IVC), portal & splenic veins (PSV), pancreas (Pa), right adrenal gland (RAG), left adrenal gland (LAG) with one background and 30 axial contrast-enhanced abdominal CT scans. We randomly split them as 20,4 and 6 scans for training, validation, and testing, respectively. Following DHC [25], we run the experiments 3 times with different random seeds.

MMWHS Dataset for UDAMulti-Modality Whole Heart Segmentation Challenge 2017 dataset (MMWHS) [74] is a cardiac segmentation dataset including two modality images (MR and CT). Each modality contains 20 volumes collected from different sites, and no pair relationship exists between modalities. Following the previous work [41], we choose four classes of cardiac structures. They are the ascending aorta (AA), the left atrium blood cavity (LAC), the left ventricle blood cavity (LVC), and the myocardium of the left ventricle (MYO). For the pre-processing, follow [41], (1) all the scans

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Datasets & patch size & learning rate & batch size & feature size \(F\) \\ \hline LASeg & \(112\times 112\times 80\) & 1e-2 & 4 & 32 \\ Synapse & \(64\times 128\times 128\) & 3e-2 & 4 & 32 \\ MMWHS & \(128\times 128\times 128\) & 5e-3 & 2 & 32 \\ M\&Ms & \(32\times 128\times 128\) & 1e-2 & 4 & 32 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Hyper-parameters for different datasets.

Figure 7: Visualization of the class imbalance SSL task in 2D and 3D views. The yellow arrows denote minority classes detected.

were cropped centering at the heart region, with four cardiac substructures selected for segmentation; (2) for each 3D cropped image top 2% of its intensity histogram was cut off for alleviating artifacts; (3) each 3D image was then normalized to [0, 1]. To make a fair comparison, we keep the test set the same with prior arts [38; 41; 44].

M&Ms Dataset for SemiDGThe multi-center, multi-vendor & multi-disease cardiac image segmentation (M&Ms) dataset [75] contains 320 subjects, which are scanned at six clinical centers in three different countries by using four different magnetic resonance scanner vendors, i.e., Siemens, Philips, GE, and Canon. We consider the subjects scanned from different vendors are from different domains (95 in domain A, 125 in domain B, 50 in domain C, and another 50 in domain D). We use each three of them as the source domain for training and the rest as the unseen domain for testing. For the pre-processing, (1) all the scans were cropped with four cardiac substructures selected for segmentation; (2) for each 3D cropped image top and bottom 0.5% of its intensity histogram was cut off for alleviating artifacts; (3) each 3D image was then normalized to [0, 1]. Since the data has very few slices on the z-axis (less than 16), the previous work used 2D-based solutions. In this work, since we aim to design a generic framework for volumetric medical image segmentation, we stacked the z-axis to 32 to meet the minor requirement for our encoder with four down-sampling layers. This dataset can also be considered as the extreme case of 3D segmentation tasks.

### More Analyses

Comparison of Computational CostsWe compared the parameters and the inference time of our method with the most SOTA methods in different settings: SS-Net [17] on LASeg dataset for SSL, CLD [15] on Synapse dataset for IBSSL, and vMFNet [55] as well as EPL [54] on M&Ms dataset for SemiDG, as shown in Table 9. From the table we can observe that our method has the fewest parameters except for vMFNet. Although vMFNet has only 20 M parameters, their performances (73.88% and 72.3%) on SSL and UDA tasks are significantly lower than ours (89.40% and 90.0%).

Figure 8: Visualization of the RS process of the foreground class on LASeg dataset. The probability map \(p^{u;\psi}\) of the difficulty-aware decoder may have low confidence in the inner region (red box), whereas the probability map \(p^{u;\xi}\) of the diffusion decoder may have inaccurate boundaries but with very high confidence (red arrows).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Methods & Param.(M) & Inference time (s/iter) & SSL & UDA \\ \hline SS-Net [17] & 75.672 & 21.20 & 88.55 & 78.2 \\ CLD [15] & 75.554 & 39.58 & 85.37 & 75.4 \\ vMFNet [55] & 20.423 & 4.89 & 73.88 & 72.3 \\ EPL [54] & 80.939 & 5.52 & 76.49 & 71.9 \\ A\&D (ours) & 57.894 & 3.69 & 90.31 & 90.1 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Comparison of computational costs of state-of-the-art methods on different settings with the performances in terms of Dice score.

As for the inference time, we can observe that our A&D is the fastest. This can be attributed to our effective aggregating and decoupling strategies, enabling our method to exclusively utilize the unlabeled branch for inference.

Visualization of the Re-parameterize & Smooth (RS)As shown in Figure 8, the output probability map \(p^{u;\xi}\) of diffusion with DDIM \(D(\xi)\) is with very high confidence with its prediction, however, the results are not stable since the unlabeled data is _unseen_ during the training process of the diffusion decoder, especially for some problematic classes (MYO of MMWHS, Figure 9) with ambiguous boundaries and noise. Thus, if we sum it with the map \(p^{u;\psi}\) generated by the V-Net decoder \(D(\psi)\), the error regions (e.g., upper right corner) with high confidence will surpass some correct regions of \(p^{u;\psi}\) with lower confidence and further harm the quality of the final pseudo label. Moreover, in some cases, the two output probability maps have complementary properties (Figure 8), indicating the effectiveness of ensembling them for the high-quality pseudo labels.

Ablation on the Effectiveness of Decoupling the Labeled and Unlabeled Data Training Flows Based on our final framework, we add an additional training process with labeled data on the decoder \(D(x^{u};\theta)\) trained with unlabeled data to verify the effectiveness of the decoupling idea. Compared with the final A&D framework, when adding an additional labeled data training branch, the performance in terms of Dice drops from 90.03% to 86.94% on the MR to CT setting of the MMWHS dataset. The result indicates that when the predictor is trained with labeled and unlabeled data, it may get over-fitted to the easier labeled data flow, which verifies the effectiveness of the key idea of our decoupling stage.

Figure 9: Visualization of the RS process of the myocardium of the left ventricle (MYO) class which is the class with worst performance on MR to CT setting of MMWHS dataset. In this case, the probability map \(p^{u;\xi}\) of the diffusion decoder contains more error regions due to the ambiguous boundaries and noise but also with very high confidence.