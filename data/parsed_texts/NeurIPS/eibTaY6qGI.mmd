Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis

 Victor Letzelter\({}^{1,2}\)

Mathieu Fontaine\({}^{2}\)

Mickael Chen\({}^{1}\)

Patrick Perez\({}^{1}\)

Slim Essid\({}^{2}\)

Gael Richard\({}^{2}\)

\({}^{1}\) valeo.ai, Paris, France

\({}^{2}\) LTCI, Telecom Paris, Institut Polytechnique de Paris, France

###### Abstract

We introduce Resilient Multiple Choice Learning (rMCL), an extension of the MCL approach for conditional distribution estimation in regression settings where multiple targets may be sampled for each training input. Multiple Choice Learning is a simple framework to tackle multimodal density estimation, using the Winner-Takes-All (WTA) loss for a set of hypotheses. In regression settings, the existing MCL variants focus on merging the hypotheses, thereby eventually sacrificing the diversity of the predictions. In contrast, our method relies on a novel learned scoring scheme underpinned by a mathematical framework based on Voronoi tessellations of the output space, from which we can derive a probabilistic interpretation. After empirically validating rMCL with experiments on synthetic data, we further assess its merits on the sound source localization task, demonstrating its practical usefulness and the relevance of its interpretation.

## 1 Introduction

Machine learning models are commonly trained to produce, for any given input, a single prediction. In most cases, for instance, when minimizing the empirical risk with quadratic loss, this prediction can be interpreted as the conditional output expectation given the input. However, there are many tasks for which the conditional output distribution can be multimodal, either by the nature of the task or due to various sources of uncertainty. Temporal tracking and forecasting, for instance, are problems of this type [5, 32, 44]. In such cases, the mean might fall in a low-density region of the conditional probability function, and it would be beneficial to predict multiple hypotheses instead [34, 20]. In this context, Multiple Choice Learning (MCL) has emerged as a simple pragmatic solution [17, 29]. Thanks to a network architecture with multiple heads, MCL models can produce multiple hypotheses, one per head. During supervised training, the gradients are only computed for the head that provides the best prediction given the current input sample. This Winner-Takes-All (WTA) training scheme allows each head to specialize in a region of the output space. Accordingly, Rupprecht et al. [34] have proposed a probabilistic interpretation of MCL based on Voronoi tessellations.

On that account, MCL suffers from two significant issues: _hypotheses collapse_ and _overconfidence_. _Hypotheses collapse_ occurs during training when a head takes the lead, the others being almost never selected under WTA and thus not updated. This leads to a situation where most heads are not trained effectively and produce meaningless outputs during inference. _Overconfidence_ can also be observed at inference time when looking at MCL under the lens of its probabilistic interpretation. Hypotheses that correspond to rare events tend to be over-represented, thus not reflecting the true distribution of outputs well. Multiple variants of MCL have been developed to alleviate these issues, in particular for classification or segmentation tasks [29; 27; 42; 13].

In this paper, we are interested in MCL in the context of regression settings, which has been mostly overlooked. More specifically, we propose rMCL, for _resilient Multiple Choice Learning_, a MCL variant with a learned scoring scheme to mitigate overconfidence. Unlike prior work, rMCL is designed to handle more general settings where multiple targets may be sampled for each input during training. Moreover, we propose a probabilistic interpretation of rMCL, and show that rMCL's scoring fixes overconfidence by casting multi-choice regression as conditional distribution estimation. While introduced in the context of the original WTA loss, the proposed scoring approach is also compatible with other variants of WTA. We validate our claims with experiments on synthetic data, and we further evaluate the performance of rMCL on the sound source localization problem. The accompanying code with a rMCL implementation is made available.1

Footnote 1: https://github.com/Victorletzelter/code-rMCL

## 2 Related Work

**Uncertainty estimation.** The estimation of machine learning model's uncertainty can be studied from different perspectives depending on the type of ambiguity being considered. The uncertainty can concern either the model (_epistemic_) or the data (_aleatoric_) [7; 12]. Kendall and Gal [22] showed that those types of uncertainty can be combined in deep learning settings, placing both a distribution on the weights and the outputs of the model. Epistemic uncertainty estimation may, for instance, be achieved by variational approximation of the weights posterior [31; 14]. Independent ensembles (IE) of neural networks [18; 11] is another widespread technique for epistemic uncertainty approximation. Several neural networks, trained from different random initializations, provide samples for approximating the weights posterior distribution [26].

**Multiple Choice learning.** Originally introduced by Guzman-Rivera et al. [17] and adapted to deep learning settings by Lee et al. [28; 29], MCL is a framework in which multiple output heads propose different possible predictions. Since many datasets contain only a single output realization for each input, the heads are trained with a Winner-Takes-All scheme where only the head that made the best prediction is updated. It can be understood as a specific case of dependent ensembles [8; 46; 4; 47] where the members interact with each other during training. Alternatively, [34] have shown that MCL can be adapted successfully for multi-label classification [23].

The Winner-Takes-All training scheme, however, can cause two main issues known as _hypotheses collapse_ and _overconfidence_[34; 6; 10; 19; 10; 27; 42]. Most related previous works tackle either or both of those questions. Hypotheses collapse occurs during training when some heads receive little or no updates due to bad initial values under the WTA loss and other hypotheses taking the lead. To fix this issue, Rupprecht et al. [34] proposed a relaxed Winner-Takes-All (RWTA, or \(\varepsilon\)-WTA) loss that allows the update of non-winner heads, albeit with gradients scaled by a small constant \(\varepsilon\). While this approach ensures that every head receives gradients updates, it also tends to level the different heads, which counters the initial goal of WTA and thus needs careful tuning. Finally, one can leverage the evolving Winner-Takes-All loss [32], with the top-\(n\) heads getting updated (top-\(n\)-WTA) instead of only the best one. The authors validate that their approach, with a scheduled decreasing value for \(n\), achieves improved conditioning of the outputs by reducing inconsistent hypotheses in the context of future frames prediction.

On the other hand, overconfidence is an issue that can be observed in inference when evaluating a model as a density estimator. In this case, one would want the different outputs to be distributed across the different modes of the true conditional probability of the outputs. Empirical observations [27; 42] show that this is not usually the case, and rare events tend to be overly represented, rendering such a model inadequate for integration within real-world decision-making systems. In this context, Lee et al. [27] proposed _Confident Multiple Choice Learning_ (CMCL) for solving overconfidence in a classification setup. Additionally to the WTA loss, CMCL is trained by maximizing the entropy of the class distributions predicted by the non-selected hypotheses. The classifier's final prediction is based on a simple average of the discrete class distributions predicted by each of the heads. Although designed for tackling the overconfidence problem, CMCL reduces the diversity of the hypotheses. On that account, _Versatile Multiple Choice Learning_[42] (vMCL) proposed to address the issue by leveraging a choice network aiming at predicting a score for each hypothesis head. This may be seen as a variant of Mixture-of-Experts approaches [33], in which the choice network is supervised with explicit targets. The final prediction in vMCL is derived by weighting the class distributions from each hypothesis by their respective scores. These works focus on classification tasks and aggregate the hypotheses, thereby losing individual information from each head. In contrast, we address regression tasks and demonstrate how to benefit from the diversity of predictions _without_ aggregation.

We propose to revisit the vMCL approach for regression, single-target or multi-target, and to extend accordingly the mathematical interpretation of the WTA proposed by Rupprecht et al. [34].

## 3 rMCL regression and its probabilistic interpretation

After reviewing MCL, this section introduces resilient Multiple Choice Learning, dubbed rMCL. This proposed variant handles multi-target settings thanks to its scoring scheme. We then provide a distribution learning interpretation of rMCL that is relevant to regression tasks, thus completing the MCL probabilistic interpretations of prior works [9; 34].

### Fixing the overconfidence issue in Multiple Choice Learning

Let \(\mathcal{X}\subseteq\mathbb{R}^{d}\) and \(\mathcal{Y}\subseteq\mathbb{R}^{q}\) be the input and output spaces in a supervised learning setting, such that the training set \(\mathscr{D}\) is composed of samples \((\bm{x}_{s},\bm{y}_{s})\) from an underlying joint distribution \(p(\bm{x},\bm{y})\) over \(\mathcal{X}\times\mathcal{Y}\).

Multiple choice learning was proposed to address tasks with ambiguous outputs, _i.e._, for which the ground-truth distribution \(p(\bm{y}\,|\,\bm{x})\) is multimodal [17; 29; 10]. Adapted by Lee et al. [29] for deep-learning settings in _Stochastic Multiple Choice Learning_ (sMCL), it leverages several models \(f_{\theta}\triangleq(f_{\theta}^{1},\ldots,f_{\theta}^{K})\in\mathcal{F}( \mathcal{X},\mathcal{Y}^{K})\), referred to as \(K\)_hypotheses_, trained using the Winner-takes-all (or Oracle) loss. It consists, given an underlying loss function \(\ell\) and for each sample \((\bm{x}_{s},\bm{y}_{s})\) in the current batch, of first the computation of

\[\mathcal{L}(f_{\theta}(\bm{x}_{s}),\bm{y}_{s})\triangleq\min_{k\in[1,K]}\ell \left(f_{\theta}^{k}\left(\bm{x}_{s}\right),\bm{y}_{s}\right)\] (1)

after each forward pass, followed by backpropagation on the _winner_ hypothesis, that is, the minimizing one. Assuming now that a set \(\bm{Y}_{s}\) of targets is available in the training set for input \(\bm{x}_{s}\), Firman et al. [10] have shown that (1) can be generalized by updating the best hypothesis per target using the meta-loss

\[\mathcal{L}\left(f_{\theta}\left(\bm{x}_{s}\right),\bm{Y}_{s}\right)=\sum_{\bm {y}\in\bm{Y}_{s}}\sum_{k=1}^{K}\bm{1}\left(\bm{y}\in\mathcal{Y}^{k}\left(\bm{ x}_{s}\right)\right)\ell\left(f_{\theta}^{k}\left(\bm{x}_{s}\right),\bm{y} \right),\] (2)

where

\[\mathcal{Y}^{k}(\bm{x})\triangleq\left\{\bm{y}\in\mathcal{Y},\ \ell\left(f_{ \theta}^{k}(\bm{x}),\bm{y}\right)<\ell\left(f_{\theta}^{r}(\bm{x}),\bm{y} \right),\ \forall r\neq k\right\}.\] (3)

This loss can, however, lead to poor conditioning of the output set of predictions (_hypotheses collapse_), with only one or a few hypotheses being exclusively selected for backpropagation. Furthermore, sMCL is subject to the _overconfidence_ problem. Following Theorem 1 of [34], a necessary condition for minimizing the risk

\[\int_{\mathcal{X}}\sum_{k=1}^{K}\int_{\mathcal{Y}^{k}(\bm{x})}\ell\left(f_{ \theta}^{k}(\bm{x}),\bm{y}\right)p(\bm{x},\bm{y})\mathrm{d}\bm{y}\mathrm{d}\bm {x},\] (4)

is that each prediction \(f_{\theta}^{k}(\bm{x})\) coincides with the centroid of \(\mathcal{Y}^{k}(\bm{x})\) with respect to \(p(\bm{y}\,|\,\bm{x})\). We say that the components \(\{\mathcal{Y}^{k}(\bm{x})\}_{k}\) form a _centroidal_ Voronoi tessellation [9]. If \(\ell\) is for instance the \(\ell_{2}\)-loss, this condition means that for each non-zero probability cell \(k\), \(f_{\theta}^{k}(\bm{x})\) amounts to the conditional mean of \(p(\bm{y}\,|\,\bm{x})\) within the Voronoi cell \(\mathcal{Y}^{k}(\bm{x})\). However, this theorem tells us nothing about the predictions in very low probability zones; in such regions, the inference-time predictions \(f_{\theta}^{k}(\bm{x})\) will be meaningless. During inference, the sMCL indeed faces a limitation: it cannot solely rely on the predicted hypotheses to identify Voronoi cells in the output space with low probability mass (see Figure 1). This observation leads us to propose _hypothesis-scoring_ heads \(\bm{\gamma}_{\theta}^{1},\ldots,\bm{\gamma}_{\theta}^{K}\in\mathcal{F}( \mathcal{X},[0,1])\). Those aim to predict, for unseen input \(\bm{x}\), the probability \(\mathbb{P}(Y_{\bm{x}}\in\mathcal{Y}^{k}(\bm{x}))\) where \(Y_{\bm{x}}\sim p(\bm{y}\,|\,\bm{x})\) with the aim of mitigating this overconfidence issue.

While MCL variants have often been proposed for classification tasks, we are, to the best of our knowledge, the first to propose to solve this issue for multi-target regression, interpreting the problem as a multimodal conditional distribution estimation.

### Resilient Multiple Choice Learning

We consider hereafter a problem of estimation of a multimodal conditional distribution denoted \(p(\boldsymbol{y}\,|\,\boldsymbol{x})\) for each input \(\boldsymbol{x}\in\mathcal{X}\). In a real-world setting, only one or several samples (the _targets_) drawn from \(p(\boldsymbol{y}\,|\,\boldsymbol{x})\) are usually accessible. Sound source localization is a concrete instance of such multimodal prediction for whose \(p(\boldsymbol{y}\,|\,\boldsymbol{x}_{s})\) represents the sound source position for an input audio clip \(\boldsymbol{x}_{s}\in\mathcal{X}\) at a given time \(t\).

In this last example, each target sample in \(\boldsymbol{Y}_{s}\) may represent the location of a _mode_ of the ground-truth multimodal distribution. For such multi-output regression tasks, the MCL training of a randomly initialized multi-hypotheses model with scoring functions, \((f^{1}_{\theta},\ldots,f^{K}_{\theta},\boldsymbol{\gamma}^{1}_{\theta},\ldots,\boldsymbol{\gamma}^{K}_{\theta})\) can be adapted as follows.

For each training sample \((\boldsymbol{x}_{s},\boldsymbol{Y}_{s})\), let

\[\mathcal{K}_{+}(\boldsymbol{x}_{s})\triangleq\bigg{\{}k^{+}\in\llbracket 1,K \rrbracket:\exists\boldsymbol{y}\in\boldsymbol{Y}_{s},k^{+}\in\operatorname*{ argmin}_{k}\,\ell(f^{k}_{\theta}(\boldsymbol{x}_{s}),\boldsymbol{y})\bigg{\}}\] (5)

and \(\mathcal{K}_{-}(\boldsymbol{x}_{s})\triangleq\llbracket 1,K\rrbracket- \mathcal{K}_{+}(\boldsymbol{x}_{s})\) be the set of _positive_ (or _winner_) and _negative_ hypotheses respectively. It is then possible to combine the multi-target WTA loss \(\mathcal{L}\) in (2) with a hypothesis scoring loss

\[\mathcal{L}_{\text{scoring}}(\theta)\triangleq-\Big{(}\sum_{k^{+}\in \mathcal{K}_{+}(\boldsymbol{x}_{s})}\log\boldsymbol{\gamma}^{k^{+}}_{\theta} \left(\boldsymbol{x}_{s}\right)+\sum_{k^{-}\in\mathcal{K}_{-}(\boldsymbol{x} _{s})}\log\left(1-\boldsymbol{\gamma}^{k^{-}}_{\theta}\left(\boldsymbol{x}_{s} \right)\right)\Big{)},\] (6)

in a compound loss \(\mathcal{L}+\beta\mathcal{L}_{\text{scoring}}\). This novel approach differs from previous MCL variants [42] in its ability to predict multimodal distributions in regression settings and by the introduction of separated scoring branches that are updated based on loss (6), such that the target for the scoring branch \(k\) is the probability that hypothesis \(k\) is among the winners for that sample.

A resource-efficient implementation of rMCL is achieved by deriving the hypotheses and score heads from a shared representation, with distinct parameters at the final stages of the architecture (_e.g._, through independent fully connected layers). Designed as such, it is also possible to reduce memory cost by updating only a fraction of score heads associated with the negative hypotheses at each training step, typically with distinct samples \(k^{-}\sim\mathcal{U}\left(\mathcal{K}_{-}(\boldsymbol{x}_{s})\right)\). This trick could also alleviate the imbalanced binary classification task that the scoring heads face with a large number of hypotheses \(|\mathcal{K}_{-}(\boldsymbol{x}_{s})|\gg|\mathcal{K}_{+}(\boldsymbol{x}_{s})|\) (typically, the number of target samples at disposal is small relative to the number of hypotheses \(K\)). The variants of the WTA (_e.g._, top-\(n\)-WTA, \(\varepsilon\)-WTA, see Sec. 2) are also compatible with rMCL. Inference with the proposed rMCL model is outlined in Algorithm 1.

As highlighted above, the hypothesis output can be interpreted, in the context of risk minimization, as the conditional mean of the Voronoi cell \(\mathcal{Y}^{k}(\boldsymbol{x})\) it defines, providing that the cell has non-zero probability. Furthermore, given (6), the output of the score head \(\boldsymbol{\gamma}^{k}_{\theta}(\boldsymbol{x})\) can be interpreted as an approximation of the probability of a sample from \(p(\boldsymbol{y}\,|\,\boldsymbol{x})\) to belong to this cell.

``` Input: Unlabelled input \(\boldsymbol{x}\in\mathcal{X}\). Trained hypotheses and score heads \(\big{(}f^{1}_{\theta},\ldots,f^{K}_{\theta},\boldsymbol{\gamma}^{1}_{\theta}, \ldots,\boldsymbol{\gamma}^{K}_{\theta}\big{)}\in\mathcal{F}(\mathcal{X}, \mathcal{Y})^{K}\times\mathcal{F}(\mathcal{X},[0,1])^{K}\). Output: Prediction of the output conditional distribution \(p(\boldsymbol{y}\,|\,\boldsymbol{x})\).
1: Perform a forward pass by computing \(f^{1}_{\theta}(\boldsymbol{x}),\ldots,f^{K}_{\theta}(\boldsymbol{x}), \boldsymbol{\gamma}^{1}_{\theta}(\boldsymbol{x}),\ldots,\boldsymbol{\gamma}^ {K}_{\theta}(\boldsymbol{x})\).
2: Construct the associated Voronoi components \(\mathcal{Y}^{k}(\boldsymbol{x})\) (3) with \(\mathcal{Y}=\cup_{k=1}^{K}\overline{\mathcal{Y}^{k}(\boldsymbol{x})}\).
3: Normalize the predicted scores \(\boldsymbol{\gamma}^{k}_{\theta}(\boldsymbol{x})\leftarrow\nicefrac{{*}}{{ \sigma}}(\nicefrac{{\boldsymbol{x}}}{{\boldsymbol{x}}})/\sum_{k=1}^{K} \nicefrac{{*}}{{\gamma}^{k}_{\theta}(\boldsymbol{x})}\).
4: If \(\boldsymbol{Y}_{\boldsymbol{x}}\sim p(\boldsymbol{y}\,|\,\boldsymbol{x})\), then for \(k\) such that \(\boldsymbol{\gamma}^{k}_{\theta}(\boldsymbol{x})>0\), interpret the predictions as estimations of

\[\bigg{\{}\begin{array}{l}\boldsymbol{\gamma}^{k}_{\theta}(\boldsymbol{x})= \mathbb{P}(Y_{\boldsymbol{x}}\in\mathcal{Y}^{k}(\boldsymbol{x}))\\ f^{k}_{\theta}(\boldsymbol{x})=\mathbb{E}[Y_{\boldsymbol{x}}\,|\,Y_{ \boldsymbol{x}}\in\mathcal{Y}^{k}(\boldsymbol{x})]\end{array}\] (7)

[MISSING_PAGE_FAIL:5]

We compared the behavior of our proposed approach rMCL against sMCL and independent ensembles (IE) using a 3-layer perceptron backbone with 256 hidden units and 20 output hypotheses, and \(\ell_{2}\) as the underlying distance. For rMCL, we used scoring weight \(\beta=1\). In addition to the full-fledged rMCL, we assess a variant denoted 'rMCL\({}^{*}\)' where a single negative hypothesis is uniformly selected during training. For independent ensembles, we trained with independent random initialization 20 single-hypothesis models, the predictions of which being combined to be comparable to multi-hypothesis models. The training processes were executed until the convergence of the training loss. We report the checkpoints for which the validation loss was the lowest. All networks were optimized using Adam [24]. We show in Figure 1 (top right) test predictions of the classical sMCL compared to those of the rMCL model. In visualizing the predictions where the ground-truth distribution is known, we observe empirically that the _centroidal_ tessellation property, \(\mathbb{E}[Y_{\bm{x}}^{k}]=f_{\theta}^{k}(\bm{x})\), is verified with good approximation (see red points Figure 1, bottom right). We otherwise notice the _overconfidence_ problem of the sMCL model in low-density zones when the output distribution is multimodal by looking at the output prediction, _e.g._, for \(x=0.1\) and \(x=0.9\) in Figure 1 where the sMCL predictions fail in low-density zones. In contrast, rMCL solves this issue, assigning to each Voronoi cell a score that approximates the probability mass of the ground-truth distribution in this zone (see Figure 1, bottom). Furthermore, the independent ensembles (triangles) suffer from a collapse issue around the conditional mean of the ground-truth distribution. This behavior is expected as the minimizer of the continuous formulation of the risk (4) on the whole output space is the conditional expectation of the output given the input.

From a quantitative perspective, as demonstrated in Fig. 1, we can evaluate the effectiveness of rMCL in addressing the _overconfidence_ issue compared to the classical sMCL. To ensure a fair comparison with sMCL settings, we have opted for using the Earth Mover's Distance (EMD) metric, which measures the discrepancy between the ground truth and predicted distributions. Predicted distributions are considered mixtures of Dirac deltas as in (12) for the metric computation, with sMCL predictions assigning uniform mode weights. As depicted in Fig. 1, rMCL outperforms sMCL when the target distribution is multimodal, _i.e._, for extreme values of \(x\). However, in the unimodal scenario, the performances of rMCL and sMCL are fairly similar. Furthermore, the 'rMCL\({}^{*}\)' variant (depicted by crosses) that is lighter regarding memory consumption exhibits performance comparable to the standard rMCL. Additional assessments, which are detailed in Appendix B.2, have demonstrated the robustness of the rMCL when corrupting the training dataset with outliers in the output space.

Figure 1: **Comparisons on a 2D regression toy problem**. Comparing sMCL with standard WTA (‘sMCL’, pink diamonds), proposed rMCL (blue circles), a variant of rMCL with a single negative hypothesis chosen uniformly at training (‘rMCL\({}^{*}\)’, purple crosses), and Independent Ensemble (‘IE’, yellow triangles). (_Left_) Test EMD for different inputs \(x\); (_Right_) Inference at \(x=0.1,0.6,0.9\) and Voronoi tessellations generated by the hypotheses. Green points are samples from the ground-truth distribution at the corresponding input. Cells conditional mean (red points) matches with the predicted hypotheses. The score predicted in each cell, displayed as color saturation of the blue circles, as per scale on the right, approximates the corresponding proportion of points. At \(x=0.1\) and \(0.9\), we can observe how rMCL tackles well the overconfidence issue in the low-density zones.

Experiments with audio data

### Sound source localization as a conditional distribution estimation problem

Sound source localization (SSL) is the task of predicting the successive angular positions of audio sources from a multichannel input audio clip \(\bm{x}\in\mathcal{X}\)[15]. This problem can be cast either as a regression problem (estimating the continuous positions of the sources, _e.g._, [37]) or a classification one (segmenting the output space into zones in which we wish to detect a source, _e.g._, [2]). While not requiring the number of sources to be known in advance, the classification approach suffers from the lack of balanced data and limited spatial resolution. On the other hand, the regression approach enables off-grid localization but typically suffers from the source permutation problem [40]. Assuming that the maximum number of overlapping sources \(M\) is known, a solution for handling this multi-target regression problem is to predict, at each time frame \(t\) depending on the chosen output resolution, a vector accounting for the sources' activity \(\bm{a}_{t}\in\{0,1\}^{M}\), as well as azimuth and elevation angles \(\bm{\phi}_{t}\in\mathbb{R}^{M}\) and \(\bm{\vartheta}_{t}\in\mathbb{R}^{M}\). A model can be trained with a permutation invariant training (PIT) approach [1, 37, 45] using an optimization criterion of the form

\[\mathcal{L}(\theta)=\sum_{t}\min_{\sigma\in\mathcal{S}_{M}}\ell_{\mathrm{CE}} \left(\sigma(\hat{\bm{a}}_{t}),\bm{a}_{t}\right)+\ell_{g}\left((\sigma(\hat{ \bm{\phi}}_{t}),\sigma(\hat{\bm{\vartheta}}_{t})),(\bm{\phi}_{t},\bm{\vartheta }_{t})\right),\] (13)

where \(\ell_{\mathrm{CE}}\) and \(\ell_{g}\) correspond respectively to a cross-entropy term and a geometrical loss, the latter being computed only for active sources indexes. \(\mathcal{S}_{M}\) is the set of permutations of \(M\) elements and the notation \(\sigma(\bm{z})\) stands for the \(M\)-dim vector \(\bm{z}\) with its components permuted according to \(\sigma\). In the following, we will denote \(\bm{Y}_{t}\) the set of source positions at time \(t\).

With the distribution learning mindset, this task can be seen as an attempt to estimate, at each time step, the sound source position distribution \(p(\bm{y}\,|\,\bm{x})\), which can be viewed as a Dirac mixture, \(p(\bm{y}\,|\,\bm{x})\propto\sum_{\bm{y}_{i}\in\bm{Y}_{t}}\delta_{\bm{y}_{i}}( \bm{y})\), if we suppose that the targets are point-wise, with another Dirac mixture representing the predicted active modes at predicted positions. Therefore, a natural way to evaluate such SSL regression models is to solve the linear assignment problem, _e.g._, using Hungarian matching with spherical distance as an underlying metric. To handle more general distributions, we propose to generalize the metric used to the Earth Mover's Distance (see Sec. 4.2).

The rMCL framework is well suited to SSL as it allows one to benefit from both the advantages of the regression and classification viewpoints in the same spirit as [41]. There is no need for prior knowledge of the number of sources, and it avoids challenges related to imbalanced spatial positions and the source permutation problem of (13). This method comes at a low computational cost regarding added parameters when opting for a low-level representation shared by the hypotheses and scores heads (see Sec. 3.2). Furthermore, it allows for producing a heat map of the sound sources' positions with a probabilistic prediction, which could otherwise account for their spatial dispersion depending on the chosen law \(\pi_{k}\) selected in the Voronoi cells. Modeling the sound sources as point sources, a delta law will be selected following the formulation of Proposition 1.

### Experimental setup

**Datasets.** We conducted our experiments on increasingly complex SSL datasets originally introduced by Adavanne et al. [1]: i) ANSYN, derived from DCASE 2016 Sound Event Detection challenge with spatially localized events in anechoic conditions and ii) RESYN, a variant of ANSYN in reverberant conditions. Each dataset is divided into three sub-datasets depending on the maximum number of overlapping events (1, 2, or 3, denoted as D1, D2, and D3). These sub-datasets are further divided into three splits (including one for testing). Each training split contains 300 recordings (30 s), of which 60 are reserved for validation. Preprocessing is detailed in Appendix A.2. For each experiment, we used all training splits from D1, D2, and D3. Evaluation was then conducted for each test set based on the overlapping levels. Moreover, we present results from additional SSL datasets, including REAL[1] and DCASE19[3], in Appendix B.3.

**Metrics.** To assess the performance of the different methods, we employed the following metrics:

* The _'Oracle' error_ (\(\downarrow\)): \(\mathcal{O}(\bm{x}_{n},\bm{Y}_{n})=\frac{1}{|\bm{Y}_{n}|}\sum_{\bm{y}_{m}\in \bm{Y}_{n}}\min_{k\in\llbracket 1,K\rrbracket}d\left(f_{\theta}^{k}\left(\bm{x}_{ n}\right),\bm{y}_{m}\right)\).
* The _Earth Mover's Distance_ (EMD \(\downarrow\)): also known as the 1st Wasserstein distance, it is a distance measure between two probability distributions. In this context, these are the predicted distribution, \(\hat{p}(\bm{y}\,|\,\bm{x}_{n})=\sum_{k=1}^{K}\gamma_{\theta}^{k}(\bm{x}_{n})\delta_{ \theta}^{k}(\bm{x}_{n})(\bm{y})\), and the ground-truth one, \(p(\bm{y}\,|\,\bm{x}_{n})=\frac{1}{|\bm{Y}_{n}|}\sum_{\bm{y}_{m}\in\bm{Y}_{n}} \delta_{\bm{y}_{m}}(\bm{y})\). The EMD solves the optimization problem \(W_{1}\left(p(\cdot|\bm{x}_{n}),\hat{p}(\cdot|\bm{x}_{n})\right)=\min_{\bm{y} \in\bm{y}}\sum_{k=1}^{K}\sum_{\bm{y}_{m}\in\bm{Y}_{n}}\psi_{k,m}d(f_{\theta}^{ k}(\bm{x}_{n}),\bm{y}_{m}),\) where \(\bm{\psi}=\left(\psi(f_{\theta}^{k}(\bm{x}_{n}),\bm{y}_{m})\right)_{k,m}\) is a transport plan and \(\Psi\) is the set of all valid transport plans [21].

While the oracle error assesses the quality of the _best_ hypothesis predicted for each target, the EMD metric looks at _all_ hypotheses. It provides insight into the overall consistency of the predicted distribution. The EMD also penalizes the overconfidence issue of the WTA with sMCL as described in Sec. 3.4. In order to fit the directional nature of the sound source localization task, these metrics are computed in a spherical space equipped with distance \(d(\hat{\bm{y}},\bm{y})=\arccos[\sin(\hat{\vartheta})\sin(\vartheta)+\cos( \hat{\vartheta})\cos(\vartheta)\cos(\phi-\hat{\phi})]\), where \((\hat{\phi},\hat{\vartheta})\) and \((\phi,\vartheta)\) are source positions \(\hat{\bm{y}}\) and \(\bm{y}\), respectively, expressed as azimuth and elevation angles.

Neural network backboneWe employed the CRNN architecture of SeldNet [1] as a backbone, which we modified to enable multi-hypothesis predictions. This adaptation involves adjusting the output format and duplicating the last fully-connected layers for hypothesis and scoring heads. As only the prediction heads are adjusted, the number of parameters added to the architecture is negligible. Refer to Appendix A.2 for additional architecture and training details.

BaselinesWe compared the proposed rMCL with several baselines, each utilizing the same feature-extractor backbone as previously described. The baselines include a Permutation Invariant Training variant ('PIT variant') proposed in [37] for SSL, the conventional WTA setup ('WTA, 5 hyp.') and its single hypothesis variant ('WTA, 1 hyp.'), its \(\varepsilon\)-relaxed version with \(\varepsilon=0.5\) ('\(\varepsilon\)-WTA, 5 hyp.') and its top-\(n\) variant with \(n=3\) ('top-\(n\)-WTA, 5 hyp.'), as well as independent ensembles ('IE'). To ensure a fair comparison, we used the same number of 5 hypotheses for the WTAs, sufficient to cover the maximum of 3 sound sources in the dataset (refer to Sec. 4.5 and the sensitivity study in Table 4). For the single hypothesis WTA, the head is updated by only considering the best target, as this fares better than using one update per target. IE was constructed from five such single hypothesis WTA models trained independently with random initialization.

### Comparative evaluation of rMCL in SSL

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Dataset: **RESYN** & EMD D1 & EMD D2 & EMD D3 & Oracle D1 & Oracle D2 & Oracle D3 \\ \hline PIT variant & 10.53 \(\pm\) 0.90 & 25.09 \(\pm\) 2.14 & 35.05 \(\pm\) 1.98 & 4.89 \(\pm\) 0.55 & 15.92 \(\pm\) 1.36 & 24.95 \(\pm\) 1.62 \\ IE (5 members) & **8.24 \(\pm\) 1.02** & 26.65 \(\pm\) 2.49 & 38.7 \(\pm\) 2.62 & **3.77 \(\pm\) 0.61** & 20.95 \(\pm\) 2.27 & 32.85 \(\pm\) 2.48 \\ WTA, 1 hyp. & 8.32 \(\pm\) 1.28 & 29.26 \(\pm\) 2.85 & 43.25 \(\pm\) 2.94 & 8.32 \(\pm\) 1.28 & 29.26 \(\pm\) 2.85 & 43.25 \(\pm\) 2.94 \\ WTA, 5 hyp. & 57.88 \(\pm\) 1.71 & 51.74 \(\pm\) 1.36 & 47.38 \(\pm\) 1.26 & 5.81 \(\pm\) 0.58 & **9.46 \(\pm\) 0.71** & **13.33 \(\pm\) 0.69** \\ top-\(n\)-WTA, 5 hyp. & 42.74 \(\pm\) 2.86 & 37.25 \(\pm\) 1.88 & 36.48 \(\pm\) 1.40 & 6.21 \(\pm\) 0.79 & 11.02 \(\pm\) 1.00 & 17.32 \(\pm\) 1.11 \\ \(\varepsilon\)-WTA, 5 hyp. & 8.84 \(\pm\) 1.09 & 27.3 \(\pm\) 2.52 & 38.43 \(\pm\) 2.42 & 6.48 \(\pm\) 0.95 & 20.54 \(\pm\) 2.33 & 30.18 \(\pm\) 2.15 \\ rMCL, 5 hyp. & 12.14 \(\pm\) 1.12 & **24.45 \(\pm\) 1.91** & **32.28 \(\pm\) 1.85** & 5.74 \(\pm\) 0.66 & 10.5 \(\pm\) 0.87 & 14.6 \(\pm\) 0.87 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Source localization in reverberant conditions. Results on RESYN dataset, with same table layout as in Table 1.**

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Dataset: **ANSYN** & EMD D1 & EMD D2 & EMD D3 & Oracle D1 & Oracle D2 & Oracle D3 \\ \hline PIT variant & 6.22 \(\pm\) 0.80 & 14.65 \(\pm\) 1.22 & 23.41 \(\pm\) 1.39 & 3.58 \(\pm\) 0.46 & 10.58 \(\pm\) 0.91 & 18.10 \(\pm\) 1.04 \\ IE (5 members) & 4.05 \(\pm\) 0.48 & 21.64 \(\pm\) 2.29 & 34.34 \(\pm\) 2.37 & **1.24 \(\pm\) 0.24** & 16.91 \(\pm\) 2.05 & 28.82 \(\pm\) 2.25 \\ WTA, 1 hyp. & **3.97 \(\pm\) 0.55** & 24.69 \(\pm\) 2.72 & 39.66 \(\pm\) 2.67 & 3.97 \(\pm\) 0.55 & 24.69 \(\pm\) 2.72 & 39.66 \(\pm\) 2.67 \\ WTA, 5 hyp. & 48.22 \(\pm\) 1.78 & 44.41 \(\pm\) 1.25 & 41.83 \(\pm\) 0.96 & 3.56 \(\pm\) 0.39 & **6.53 \(\pm\) 0.44** & **10.44 \(\pm\) 0.58** \\ top-\(n\)-WTA, 5 hyp. & 5.14 \(\pm\) 0.80 & 18.09 \(\pm\) 1.32 & 25.12 \(\pm\) 1.30 & 3.33 \(\pm\) 0.46 & 7.48 \(\pm\) 0.79 & 13.54 \(\pm\) 0.96 \\ \(\varepsilon\)-WTA, 5 hyp. & 5.11 \(\pm\) 0.66 & 19.20 \(\pm\) 1.78 & 28.39 \(\pm\) 1.68 & 3.62 \(\pm\) 0.57 & 10.86 \(\pm\) 1.26 & 17.44 \(\pm\) 1.22 \\ rMCL, 5 hyp. & 7.04 \(\pm\) 0.58 & **13.87 \(\pm\) 0.99** & **20.76 \(\pm\) 1.04** & 3.85 \(\pm\) 0.46 & 7.16 \(\pm\) 0.67 & 11.29 \(\pm\) 0.78 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Source localization in anechoic conditions. Average scores (\(\pm\) standard deviation) on ANSYN dataset for PIT, IE, various WTA-based methods, and proposed rMCL. The ‘EMD’ evaluates all the hypotheses jointly, while the ‘Oracle’ error only looks at the best hypotheses. D1 corresponds to the single-target setting, while D2 and D3 consider up to 2 and 3 targets, respectively.**We present in Tables 1 and 2 the results of rMCL and baselines in the anechoic (ANSYN) and reverberant (RESYN) conditions, respectively. The consistency of the results should be noted. For each model, the metrics tend to improve as the number of sources decreases and, under similar conditions, results are marginally lower for more challenging datasets. Unsurprisingly, the single hypothesis approach exhibits strong performances in the unimodal cases (D1), but performs significantly worse in the multimodal cases (D2 and D3). Ensembling (IE) improves its performance but still displays the lack of diversity already exposed in Section 3.4. As for the 5-hypothesis WTA, in its original form, it is ill-suited for multi-target regression due to the overconfidence issue: despite a strong oracle metric, the EMD results are very poor. On the other hand, rMCL surpasses its competitors and shows the best EMD in every multimodal setting. It also consistently obtains the second-best oracle error, only slightly above WTA, while not suffering from overconfidence.

We also present results for REAL and DCASE19 in Appendix B.3, that display similar trends. We did not observe the collapse issue (see Section 3.1) in our settings, neither with WTA nor with the proposed rMCL model. We believe it is solved in practice by the variability of the data samples in the stochastic optimization during training [19]; refer to Appendix B.1 for further discussions.

### Combining rMCL with other WTA variants

As noted in Section 3.2, rMCL's improvements are, in theory, orthogonal to that of other WTA variants. In this section, we combine the rMCL with top-\(n\)-WTA and \(\varepsilon\)-WTA approaches into respectively top-\(n\)-rMCL and \(\varepsilon\)-rMCL. The results are shown in Tables 3. We note that while top-\(n\) rMCL does not provide significant improvement to rMCL, \(\varepsilon\)-rMCL, on the other hand improves EMD scores at the expense of increased oracle error. We observe similar effects when this method is applied to WTA, indicating that there are indeed some additive effects of \(\varepsilon\)-WTA and rMCL. Also note that with regards to Tables 1 and 2, all the proposed variants still get better EMD than all competitors under multimodal conditions, showing the robustness of our method.

### Effect of the number of hypotheses

The impact of varying the number of hypotheses on the performance of the rMCL model is presented in Table 4 and Figure B.4. First and foremost, we notice the anticipated trend of the oracle metric improving as the number of hypotheses increases. Concerning the EMD metric, the single hypothesis model, which avoids errors from negative hypotheses, is most effective in handling unimodal

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Dataset: **ANSYN** & EMD D1 & EMD D2 & EMD D3 & Oracle D1 & Oracle D2 & Oracle D3 \\ \hline PTT variant & 6.22 \(\pm\) 0.80 & 14.65 \(\pm\) 1.22 & 23.41 \(\pm\) 1.39 & 3.58 \(\pm\) 0.46 & 10.58 \(\pm\) 0.91 & 18.10 \(\pm\) 1.04 \\ WTA, 1 hyp. & **3.97 \(\pm\) 0.55** & 24.69 \(\pm\) 2.72 & 39.66 \(\pm\) 2.67 & 3.97 \(\pm\) 0.55 & 24.69 \(\pm\) 2.72 & 39.66 \(\pm\) 2.67 \\ rMCL, 3 hyp. & 9.89 \(\pm\) 0.95 & 14.37 \(\pm\) 0.91 & 20.96 \(\pm\) 1.03 & 5.65 \(\pm\) 0.73 & 8.6 \(\pm\) 0.63 & 13.42 \(\pm\) 0.86 \\ rMCL, 5 hyp. & 7.04 \(\pm\) 0.58 & **13.87 \(\pm\) 0.99** & **20.76 \(\pm\) 1.04** & 3.85 \(\pm\) 0.46 & 7.16 \(\pm\) 0.67 & 11.29 \(\pm\) 0.78 \\ rMCL, 10 hyp. & 9.14 \(\pm\) 0.76 & 15.2 \(\pm\) 0.84 & 21.28 \(\pm\) 0.96 & 2.94 \(\pm\) 0.35 & 4.76 \(\pm\) 0.39 & 7.54 \(\pm\) 0.50 \\ rMCL, 20 hyp. & 9.13 \(\pm\) 0.71 & 16.04 \(\pm\) 0.84 & 22.55 \(\pm\) 0.87 & **2.06 \(\pm\) 0.22** & **3.61 \(\pm\) 0.30** & **5.83 \(\pm\) 0.37** \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Sensitivity analysis**. Effect of the number of hypotheses on the performance of rMCL.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Dataset: **ANSYN** & EMD D1 & EMD D2 & EMD D3 & Oracle D1 & Oracle D2 & Oracle D3 \\ \hline rMCL, 5 hyp. & 7.04 \(\pm\) 0.58 & 13.87 \(\pm\) 0.99 & 20.76 \(\pm\) 1.04 & 3.85 \(\pm\) 0.46 & **7.16 \(\pm\) 0.67** & **11.29 \(\pm\) 0.78** \\ top-\(n\)-rMCL, 5 hyp. & **5.46 \(\pm\) 0.62** & 13.88 \(\pm\) 1.06 & 21.45 \(\pm\) 1.10 & 4.2 \(\pm\) 0.55 & 8.04 \(\pm\) 0.74 & 13.72 \(\pm\) 0.89 \\ \(\varepsilon\)-rMCL, 5 hyp. & 5.89 \(\pm\) 0.69 & **12.13 \(\pm\) 0.98** & **19.95 \(\pm\) 1.16** & **3.6 \(\pm\) 0.54** & 8.76 \(\pm\) 0.89 & 14.47 \(\pm\) 1.02 \\ \hline \hline Dataset: **RESYN** & EMD D1 & EMD D2 & EMD D3 & Oracle D1 & Oracle D2 & Oracle D3 \\ \hline rMCL, 5 hyp. & 12.14 \(\pm\) 1.12 & 24.45 \(\pm\) 1.91 & **32.28 \(\pm\) 1.85** & **5.74 \(\pm\) 0.66** & **10.5 \(\pm\) 0.87** & **14.6 \(\pm\) 0.87** \\ top-\(n\)-rMCL, 5 hyp. & 9.3 \(\pm\) 1.15 & 23.81 \(\pm\) 2.22 & 33.33 \(\pm\) 2.06 & 6.99 \(\pm\) 0.85 & 13.43 \(\pm\) 1.40 & 19.72 \(\pm\) 1.29 \\ \(\varepsilon\)-rMCL, 5 hyp. & **8.64 \(\pm\) 1.03** & **22.82 \(\pm\) 2.12** & 32.47 \(\pm\) 1.97 & 6.08 \(\pm\) 0.91 & 18.39 \(\pm\) 2.07 & 26.92 \(\pm\) 1.94 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Combining proposed rMCL with other WTA variants**. Average scores (\(\pm\) standard deviation) for source localization in anechoic (top) and reverberant (bottom) conditions.

distributions. In the case of the EMD metric applied to multimodal distributions, we observe that multiple hypothesis models improve the results, but excessively increasing the number of hypotheses may marginally degrade performances. A further study could examine this phenomenon in detail, which may be related to the expressive power of the hypothesis heads or the accumulation of errors in score heads predictions.

## 5 Discussion and limitations

In the audio experiments, the performance of rMCL was found to be affected by the number of hypotheses to tune depending on the complexity of the dataset. Moreover, as with the other variants of WTA, fixing the overconfidence issue with rMCL slightly degrades the performance of the best hypothesis (oracle error) for a reason that is yet to be determined. Otherwise, while \(\varepsilon\)-WTA behaves as expected with rMCL, trading off the quality of the best hypotheses for overall performance, top-\(n\)-WTA does not exhibit the same behavior. This discrepancy warrants further investigation.

The probabilistic interpretation of rMCL, as presented in Algorithm 1 and in Sec. 3.3, states that the different hypotheses would be organized in an optimal partition of the output space, forming a Voronoi tessellation. Ideally, each hypothesis would capture a region of the distribution, and the scores representing how likely this zone would activate in a given context. This interpretation remains theoretically valid whenever train and test examples are sampled from the same joint distribution \(p(\bm{x},\bm{y})\). However, in realistic settings, the hypotheses might not adhere to meaningful regions, but this could be controlled and evaluated, provided that an external validation dataset is available. In future work, we intend to use calibration techniques [16, 39] to identify and alleviate these issues.

To sum up, this paper proposes a new Multiple Choice Learning variant, suitable for tackling the MCL overconfidence problem in regression settings. Our method is based on a learned scoring scheme that handles situations where a set of targets is available for each input. Furthermore, we propose a probabilistic interpretation of the model, and we illustrate its relevance with an evaluation on synthetic data. Its practical usefulness is also demonstrated in the context of point-wise sound source localization. Further work could include a specific study about the specialization of the predictors, and the validation of the proposed algorithm in increasingly complex real-world datasets.

## Acknowledgments

This work was funded by the French Association for Technological Research (ANRT CIFRE contract 2022-1854). We would like to thank the reviewers for their valuable feedback.

## References

* [1] Sharath Adavanne, Archontis Politis, Joonas Nikunen, and Tuomas Virtanen. Sound event localization and detection of overlapping sources using convolutional recurrent neural networks. _IEEE Journal of Selected Topics in Signal Processing_, 13(1):34-48, 2018.
* [2] Sharath Adavanne, Archontis Politis, and Tuomas Virtanen. Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network. In _2018 26th European Signal Processing Conference (EUSIPCO)_, pages 1462-1466. IEEE, 2018.
* [3] Sharath Adavanne, Archontis Politis, and Tuomas Virtanen. A multi-room reverberant dataset for sound event localization and detection. _arXiv preprint arXiv:1905.08546_, 2019.
* [4] Kazi Md Rokibul Alam, Nazmul Siddique, and Hojjat Adeli. A dynamic ensemble learning algorithm for neural networks. _Neural Computing and Applications_, 32:8675-8690, 2020.
* [5] Samuel S Blackman. Multiple hypothesis tracking for multiple target tracking. _IEEE Aerospace and Electronic Systems Magazine_, 19(1):5-18, 2004.
* [6] Mike Brodie, Chris Tensmeyer, Wes Ackerman, and Tony Martinez. Alpha model domination in multiple choice learning. In _2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)_, pages 879-884. IEEE, 2018.

* [7] Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? _Structural safety_, 31(2):105-112, 2009.
* [8] Thomas G Dietterich. Ensemble methods in machine learning. In _International workshop on multiple classifier systems_, pages 1-15. Springer, 2000.
* [9] Qiang Du, Vance Faber, and Max Gunzburger. Centroidal voronoi tessellations: Applications and algorithms. _SIAM review_, 41(4):637-676, 1999.
* [10] Michael Firman, Neill DF Campbell, Lourdes Agapito, and Gabriel J Brostow. Diversenet: When one right answer is not enough. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5598-5607, 2018.
* [11] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective. _arXiv preprint arXiv:1912.02757_, 2019.
* [12] Yarin Gal et al. Uncertainty in deep learning. 2016.
* [13] Nuno Cruz Garcia, Sarah Adel Bargal, Vitaly Ablavsky, Pietro Morerio, Vittorio Murino, and Stan Sclaroff. Distillation multiple choice learning for multimodal action recognition. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 2755-2764, 2021.
* [14] Alex Graves. Practical variational inference for neural networks. _Advances in neural information processing systems_, 24, 2011.
* [15] Pierre-Amaury Grumiaux, Srdan Kitic, Laurent Girin, and Alexandre Guerin. A survey of sound source localization with deep learning methods. _The Journal of the Acoustical Society of America_, 152(1):107-151, 2022.
* [16] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330. PMLR, 2017.
* [17] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple choice learning: Learning to produce multiple structured outputs. _Advances in neural information processing systems_, 25, 2012.
* [18] Lars Kai Hansen and Peter Salamon. Neural network ensembles. _IEEE transactions on pattern analysis and machine intelligence_, 12(10):993-1001, 1990.
* [19] Eddy Ilg, Ozgun Cicek, Silvio Galesso, Aaron Klein, Osama Makansi, Frank Hutter, and Thomas Brox. Uncertainty estimates and multi-hypotheses networks for optical flow. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 652-667, 2018.
* [20] Ehsan Imani and Martha White. Improving regression performance with distributional losses. In _International conference on machine learning_, pages 2157-2166. PMLR, 2018.
* [21] Leonid V Kantorovich. On the translocation of masses. In _Dokl. Akad. Nauk. USSR (NS)_, volume 37, pages 199-201, 1942.
* [22] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? _Advances in neural information processing systems_, 30, 2017.
* [23] Youngwook Kim, Jae Myung Kim, Zeynep Akata, and Jungwoo Lee. Large loss matters in weakly supervised multi-label classification. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14156-14165, 2022.
* [24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [25] Harold W Kuhn. The hungarian method for the assignment problem. _Naval research logistics quarterly_, 2(1-2):83-97, 1955.

* [26] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.
* [27] Kimin Lee, Changho Hwang, KyoungSoo Park, and Jinwoo Shin. Confident multiple choice learning. In _International Conference on Machine Learning_, pages 2014-2023. PMLR, 2017.
* [28] Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Why m heads are better than one: Training a diverse ensemble of deep networks. _arXiv preprint arXiv:1511.06314_, 2015.
* [29] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael Cogswell, Viresh Ranjan, David Crandall, and Dhruv Batra. Stochastic multiple choice learning for training diverse deep ensembles. _Advances in Neural Information Processing Systems_, 29, 2016.
* [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2018.
* [31] David JC MacKay. A practical bayesian framework for backpropagation networks. _Neural computation_, 4(3):448-472, 1992.
* [32] Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox. Overcoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7144-7153, 2019.
* [33] Saeed Masoudnia and Reza Ebrahimpour. Mixture of experts: a literature survey. _The Artificial Intelligence Review_, 42(2):275, 2014.
* [34] Christian Rupprecht, Iro Laina, Robert DiPietro, Maximilian Baust, Federico Tombari, Nassir Navab, and Gregory D Hager. Learning in an uncertain world: Representing ambiguity through multiple hypotheses. In _Proceedings of the IEEE international conference on computer vision_, pages 3591-3600, 2017.
* [35] Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. A dataset and taxonomy for urban sound research. In _Proceedings of the 22nd ACM international conference on Multimedia_, pages 1041-1044, 2014.
* [36] Christopher Schymura, Benedikt Bonninghoff, Tsubasa Ochiai, Marc Delcroix, Keisuke Kinoshita, Tomohiro Nakatani, Shoko Araki, and Dorothea Kolossa. PILOT: Introducing transformers for probabilistic sound event localization. In _Interspeech 2021_, pages 2117-2121. ISCA, 2021.
* [37] Christopher Schymura, Tsubasa Ochiai, Marc Delcroix, Keisuke Kinoshita, Tomohiro Nakatani, Shoko Araki, and Dorothea Kolossa. Exploiting attention-based sequence-to-sequence architectures for sound event localization. In _2020 28th European Signal Processing Conference (EUSIPCO)_, pages 231-235. IEEE, 2021.
* [38] John Shore and Rodney Johnson. Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy. _IEEE Transactions on information theory_, 26(1):26-37, 1980.
* [39] Hao Song, Tom Diethe, Meelis Kull, and Peter Flach. Distribution calibration for regression. In _International Conference on Machine Learning_, pages 5897-5906. PMLR, 2019.
* [40] Aswin Shanmugam Subramanian, Chao Weng, Shinji Watanabe, Meng Yu, and Dong Yu. Deep learning based multi-source localization with source splitting and its effectiveness in multi-talker speech recognition. _Computer Speech & Language_, 75:101360, 2022.
* [41] Harshavardhan Sundar, Weiran Wang, Ming Sun, and Chao Wang. Raw waveform based end-to-end deep convolutional network for spatial localization of multiple acoustic sources. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 4642-4646. IEEE, 2020.

* [42] Kai Tian, Yi Xu, Shuigeng Zhou, and Jihong Guan. Versatile multiple choice learning and its application to vision computing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6349-6357, 2019.
* [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [44] Zehan Wang, Sihong Zhou, Yuyao Huang, and Wei Tian. Dsmcl: Dual-level stochastic multiple choice learning for multi-modal trajectory prediction. In _2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)_, pages 1-6. IEEE, 2020.
* [45] Dong Yu, Morten Kolbaek, Zheng-Hua Tan, and Jesper Jensen. Permutation invariant training of deep models for speaker-independent multi-talker speech separation. In _2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 241-245. IEEE, 2017.
* [46] Zhi-Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neural networks: many could be better than all. _Artificial intelligence_, 137(1-2):239-263, 2002.
* [47] Xiaoyan Zhu, Jiaxuan Li, Jingtao Ren, Jiayin Wang, and Guangtao Wang. Dynamic ensemble learning for multi-label classification. _Information Sciences_, 623:94-111, 2023.