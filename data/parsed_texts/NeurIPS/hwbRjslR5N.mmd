## Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models

**Nitzan Bitton-Guetta\({}^{1}\)** **Aviv Slobodkin\({}^{2}\)** **Aviya Maimon\({}^{2}\)** **Eliya Habba\({}^{3}\)**

**Royi Rassin\({}^{2}\)** **Yonatan Bitton\({}^{4}\)** **Idan Szpektor\({}^{4}\)** **Amir Globerson\({}^{4,5}\)** **Yuval Elovici\({}^{1}\)**

\({}^{1}\)Ben Gurion University \({}^{2}\)Bar-Ilan University

\({}^{3}\)The Hebrew University of Jerusalem \({}^{4}\)Google Research \({}^{5}\)Tel Aviv University

nitzangu@post.bgu.ac.il; yonatanbitton@google.com

https://visual-riddles.github.io/

## Abstract

Imagine observing someone scratching their arm; to understand why, additional context would be necessary. However, spotting a mosquito nearby would immediately offer a likely explanation for the person's discomfort, thereby alleviating the need for further information. This example illustrates how subtle visual cues can challenge our cognitive skills and demonstrates the complexity of interpreting visual scenarios. To study these skills, we present Visual Riddles, a benchmark aimed to test vision and language models on visual riddles requiring commonsense and world knowledge. The benchmark comprises 400 visual riddles, each featuring a unique image created by a variety of text-to-image models, question, ground-truth answer, textual hint, and attribution. Human evaluation reveals that existing models lag significantly behind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading with 40% accuracy. Our benchmark comes with automatic evaluation tasks to make assessment scalable. These findings underscore the potential of Visual Riddles as a valuable resource for enhancing vision and language models' capabilities in interpreting complex visual scenarios.

Figure 1: Introducing Visual Riddles, designed to test models on their ability to use commonsense, world knowledge, hints, attributions, and factuality in interpreting complex visual cues. This resource aims to enhance models capability to handle nuanced and factual visual scenarios.

Introduction

Humans intuitively utilize commonsense reasoning to interpret complex elements in visual scenes, a skill that current vision and language models frequently lack. For instance, the simple act of a person scratching their arm gains added context when a mosquito is present on a nearby night-stand, as depicted in Fig. 1. While humans easily recognize such contextual nuances, existing image-understanding models struggle to integrate visual cues with world knowledge stemming from cultural aspects, life-experiences, and physical or social knowledge [1, 2, 3, 4]. This gap has spurred the development of various benchmarks like VisIT-Bench [5] and Encyclopedic VQA [6], which rely on pre-existing images to formulate challenging questions. While effective, this approach risks using images seen during pre-training of large vision-language models and restricts the scope of scenarios to those already captured, potentially limiting creativity and challenge variety.

To address these shortcoming, we introduce _Visual Riddles_, a benchmark comprising 400 visual riddles, each featuring a question and a synthetic image generated specifically for the challenge. The process of creating a riddle involves designing a scenario with embedded clues that appear as natural elements, then translating this scenario into both an image and a question (SS3). For example, as shown in the top of Fig. 1, a seemingly inconspicuous mosquito becomes a key clue to explain the person's discomfort. This method, going from scenario to image, and not the other way around, not only enhances creative liberty but also broadens the range of everyday situations explored, challenging both humans and models in their interpretative and commonsense reasoning abilities. The benchmark also incorporates textual hints and attributions (Fig. 1) to direct focus to clues and provide external sources of verification for answers, thus expanding the challenge's scope and depth.

Our benchmark's main task (SS4) involves solving riddles in an _open-ended_ visual question answering (VQA) format, which takes as input an image and a question, and expects a free-text answer. This setup evaluates the ability to detect subtle visual cues and apply commonsense reasoning or world knowledge to formulate answers. Additionally, we investigate the impact of including hints and attributions in the input to enhance comprehension.

Yet, as in every QA benchmark, evaluation is a key challenge in this setting. To facilitate scalable research, we introduce two more tasks. The first is a multiple-choice version of the main task, including for the hint- and attribution-assisted variants, allowing for easy accuracy-based automatic scoring. The second task assesses the ability of models to determine the accuracy of open-ended responses in two settings: _reference-free_, where models evaluate responses based solely on the image and the question, and _reference-based_, where the correct answer is also given. This task suggests auto-raters to evaluate our riddles, aiming to advance research on such automatic evaluation methods.

Experimental results (SS5) reveal a significant gap in performance between humans and state-of-the-art vision language models, with the top-performing model, Gemini-Pro-1.5 [7], only achieving 40% accuracy versus humans' 82%. Surprisingly, the multiple-choice format proved nearly as challenging, yielding only slightly better results than the open-ended task; however, performance showed a significant improvement when auxiliary data, such as hints and attributions, was provided. Automated tests, both reference-free and reference-based, show that Gemini-Pro-1.5, the top auto-rater, matches human judgment 87% of the time in reference-based evaluations, thereby proposing a suitable method for automatically evaluating open-ended answers. We also find that some models fail to respond to visual clues accurately when tasked with evaluating answers' validity (SS6.1). Finally, attempts to reproduce the benchmark's images, rich in nuanced visual clues, using the same prompts and various text-to-image models (SS6.2), result in a mere 15% success rate, showcasing the unique challenges visual riddles present to current generative models.

Overall, the findings suggest that the Visual Riddles poses a significant challenge even to state-of-the-art vision-and-language models, emphasizing the critical need for further developments in commonsense reasoning and world knowledge integration to improve model performance on complex visual riddles. To support future research and model evaluation, we make the Visual Riddles dataset, code, and leaderboard publicly available at https://visual-riddles.github.io/.

## 2 Related Work

Our research is closely linked to commonsense reasoning in multimodal models and the evaluation of factuality in both language-only and multimodal settings, with our benchmark uniquely focusing on fine-grained image understanding that combines commonsense and world knowledge, making direct comparisons challenging.

Commonsense reasoning in multimodal models.Recent progress in vision-language models including models like BLIP2 [8], Flamingo [9], LLaVA [10], GPT4 [11], and Gemini-Pro [12]. These developments have sparked interest in commonsense reasoning [2, 13], leading to complex visual reasoning challenges within the vision-and-language domain. These include specialized tests for understanding associations and analogies [14, 15], interpreting unusual images in WHOOPS! [16], visual abductive reasoning tasks (e.g., Sherlock; [17]), multi-modal humor understanding tasks [3], and world-instructed image-editing tasks requiring commonsense reasoning (e.g., EditWorld; [18]). Notable benchmarks include VisIT-Bench [5] where the authors took existing images and generated challenging questions about them, OK-VQA [19] where questions require world knowledge, and others [20, 21]. However, Visual Riddles distinguishes itself by allowing annotators more creative freedom to devise challenging scenarios and generate corresponding images and questions, rather than restricting them exclusively to what appears in natural images. This also ensures the images have not been previously seen during pretraining.

Factuality evaluation.Evaluating factuality has been a focal point in language-only models, particularly for tasks where outputs rely on grounding texts like summarization or machine translation. This domain utilizes reference-based metrics (e.g., Rouge [22], BLEU [23], Bertscore [24], COMET [25]) that compare outputs to a reference, alongside reference-free metrics (e.g., SummaC [26], \(Q^{2}\)[27], TRUE [28], FiC [29]) that evaluate outputs against the input texts. Parallel advancements in vision-language tasks have introduced metrics such as Clipscore [30], AVIBench [31], and Tifa [32], with benchmarks like MileBench [33], SeeTRUE [34], BLINK [35], and Vibe-Eval [36] enhancing the

Figure 2: Overview of the Visual Riddles tasks: (1) Main Task: Solve open-ended questions. (2) Utilizing Hints: Use textual aids to identify key visual clues in riddles. (3) Employing Attributions: Apply web-sourced attributions to improve world-knowledge. (4) Multiple Choice: Select the correct answer to the riddle from five options. (5) Automatic Evaluation: Evaluate open-ended answers in two scenarios— Reference-Free, assessing the correctness of a candidate answer (CA) based only on the visual riddle, and Reference-Based, comparing CAs to the ground truth answer (GTA).

rigor of factuality assessments. Our dataset, Visual Riddles, extends these challenges into the realm of visual commonsense, requiring both deep world knowledge and nuanced commonsense reasoning to interpret complex visual cues. This requirement marks a significant step beyond traditional benchmarks, like Encyclopedic VQA [6], which predominantly tests factual knowledge against curated text sources. By demanding high sensitivity to visual subtleties alongside robust commonsense analysis, Visual Riddles offers a uniquely stringent test of multimodal model capabilities.

## 3 Data Collection

The Visual Riddles Challenge uses visual clues in images to test common-sense reasoning in vision-and-language models. The goal is to develop a dataset with images that represent ambiguous and culturally rich scenarios. To solve the riddles, models must detect subtle visual clues and engage in reasoning that integrates commonsense and world knowledge. The Visual Riddles benchmark was hand-curated by seven designers, experienced in generating images with text-to-image models. The design process was guided by comprehensive instructions to create visual riddles that were complex enough to challenge models yet solvable by humans. Each riddle consisted of a synthetic image paired with a question and a corresponding ground truth answer (see examples in Fig. 1). To generate high-quality images, the designers had access to advanced text-to-image models, including Midjourney, Ideogram, Canva, Gemini [12], SDXL [37], DALL-E 3 [38], and Stable-Diffusion [39]. The generated images included subtle clues crucial for solving the riddles, and designers were encouraged to embed cultural nuances and their own world knowledge into the riddles. The images were not only photo-realistic but also contained all necessary elements to lead to the correct answer. Designers provided additional hints to the visual clues that guided where to look in the image when trying to solve the riddle (top example in Fig. 1) and, for riddles requiring world knowledge, included attributions to relevant sources (bottom example in Fig. 1). After creation, each riddle was peer-reviewed by at least three other designers to ensure the hint's clarity and the riddle's solvability. Upon approval, the designer drafted a detailed answer that logically explained the solution based on the visual clues. Further elaboration on the image generation process and the instructions for riddle creation are in A.2. The Visual Riddles benchmark is licensed under Apache License 2.0.

Commonsense and World Knowledge Categorization and Difficulty AnalysisFinally, we categorize the instances based on the type of commonsense reasoning and knowledge required, including safety-related knowledge, biological knowledge, cultural knowledge, and many more (see A.3). Each riddle is assigned to one of 16 distinct categories, labeled with the single category that best fits it. Additionally, each riddle is assigned a Difficulty Level Index, which quantifies its complexity, ranging from simple, straightforward clues to obscure ones requiring specialized knowledge.

## 4 Visual Riddles Benchmark

This study introduces three vision-and-language tasks within the Visual Riddles benchmark to evaluate model capabilities. The tasks are: solving open-ended visual riddles, choosing the correct answer from multiple options, and conducting automated evaluations on open-ended riddles. These tasks may incorporate auxiliary information such as hints or attributions to enhance assessment accuracy.

Open-ended VQAIn this task, models are evaluated on their ability to solve visual riddles comprising of an image and a question, requiring not only correct solutions but also detailed explanations. Questions may be binary, requiring a 'yes' or 'no' answer with justification, or _open-ended_, inviting freely-formulated responses to queries like 'why', 'where', or 'how'. These questions require both locating visual clues in the images and integrating these clues with commonsense reasoning and world knowledge to formulate coherent and correct answers. For example, in Fig. 2.1, a good answer to the question _"What is probably the gender of the cat?"_ should reference both the _visual clue_ (the cat's fur color) and the _relevant world knowledge_ (Calico cats are predominantly female).

Multiple-choice VQAWe propose this task as an alternative to the open-ended VQA, which can be evaluated automatically using a simple accuracy metric. It involves selecting the correct answer from five options for a visual riddle comprising an image and a question, as shown in Fig. 2.4. Details on how these candidate answers were collected are provided in SS5.3.

Open-ended VQA Automatic EvaluationIn this task, models are being assessed for their ability to evaluate the accuracy of open-ended responses to visual riddles. As outlined in Fig. 2.5, it has two categories: **(a) Reference-Free**, where the model assesses a candidate answer based only on the image and question; and **(b) Reference-Based**, where it also considers the ground-truth answer to evaluate the candidate's response. This framework aims to identify the best auto-rater for open-ended tasks, supporting automated leaderboard creation. Designed for scalable evaluation of open-ended responses, this task uses vision and language models to judge both Reference-Free and Reference-Based responses. The Reference-Free baseline is included despite available ground truths, as it sometimes outperforms the Reference-Based approach [5], enhancing our insights into model performance across different contexts.

Auxiliary InformationWe utilized auxiliary information within tasks of our benchmark to assess its impact on models performances. Specifically, we incorporate textual hints and source attributions to enhance the model's ability to solve visual riddles by providing targeted contextual cues. Textual hints are concise directives that focus the model's attention on specific visual clues within the image. For instance, a hint such as _"Look at the colors of the fur"_ in the calico cat example (see Fig. 2.2) effectively directs the model's focus towards the type of the cat. Additionally, attributions provide essential world knowledge through a webpage URL from which we extract all the text content, offering the models the necessary information to solve the visual riddle. For example, providing a text stating that calico cats are _"almost exclusively female"_ (see Fig. 2.3) assists models in more effectively inferring a cat's gender from its fur color.

## 5 Experiments

We evaluate several state-of-the-art publicly accessible vision-and-language models on each of the tasks in Visual Riddles, which we overview in SS4. Then, we describe the experimental setup and results of each of the benchmark tasks, including open-ended VQA (SS5.2), multiple-choice VQA (SS5.3) and open-ended VQA automatic evaluation (SS5.4). For riddles evaluated with auxiliary data, which may have lengthy prompts, we only test models capable of accommodating such lengths.

### Models

We evaluate LLaVA-1.5-7B [10], LLaVA-1.6-34B [40], InstructBLIP-7B [41], GPT4-turbo-preview [11], Gemini-Pro-Vision [12], and Gemini-Pro-1.5 [7]. We used the most up-to-date versions of each model, as of May 2024. After submission, we included four additional models: GPT4o [42], Claude 3.5 Sonnet [43], Owen-VL-Max [44], and Molmo-7B [45], all updated to the end of October 2024. Each experiment required prompts of varying token lengths; notably, the prompts for attribution tasks were particularly lengthy, as they also included the attributing texts. Models selection is

Figure 3: Amazon Mechanical Turk interface for selecting answers to open-ended riddles. Annotators are presented with an image, a question and several candidate answers, including both human-curated and model-generated predictions, and are tasked with identifying the correct responses.

therefore based on their capacity to accommodate the full prompt required for each task, ensuring that inputs are processed fully without truncation. For further details, see A.4.

### Open-ended VQA

We start our investigation by assessing the performance of humans and models on the primary _open-ended_ VQA task, which requires solving riddles based solely on the accompanying images. To that end, we collect human responses to the riddles through crowdsourcing, as well as prompting several vision-language models, and evaluate all responses using human judgement.

Human AnswersWe utilized Amazon Mechanical Turk to collect human open-ended answers for the benchmark's riddles. Qualification test ensured ten reliable workers, annotated each of the 400 riddles (three workers per riddle). During annotation, we instructed annotators to not only answer the question but also provide justifications. Annotators were also encouraged to utilize tools like Google Lens and different search engines to research clues and ensure accurate identification of objects. For example, in Fig. 2.4, annotators which are unfamiliar with the Kakapo bird were advised to use Google Lens to identify it. For more details on the annotation process, including full guidelines and UI screenshot examples, see A.5.

Model AnswersWe extract open-ended answers from the models in SS5.1 (latest configurations). Two baselines are evaluated: large vision and language models (LVLM) generating answers from image-question prompts, and Caption\(\rightarrow\) LLM. For the latter, we extract image captions using Gemini-Pro-1.5 and humans (pre-collected), and generate answers for the riddles from caption-question prompts using the best-performing LVLM model. Further details are available in A.4.

Human Rating of ResponsesWe assess the correctness of human and model answers using three annotators in a multiple-choice selection format. They select responses that match the ground truth, without any hallucinations, based on provided images, questions, and ground-truth answers. The final rating is determined by a majority vote. Annotator agreement, measured by Krippendorff's alpha [46], reached 79%. An example of human annotation from Visual Riddles is shown in Fig. 3. Further details on annotation guidelines and results can be found in A.6.

ResultsThe results, displayed in Table 1, show that models face significant challenges solving the riddles in the Visual Riddles benchmark. In the LVLM category, Gemini-Pro-1.5 is the best performing model with a score of 40%, followed by GPT-4 at 34% and Gemini-Pro-Vision at 30%, while other models perform below 15%. Human performance, by contrast, reaches 82%, underscoring that **these riddles remain a significant challenge for current models**. In the \(Caption\to LLM\) category the best model, Human (Oracle)\(\rightarrow\)Gemini-Pro-1.5, shows a 10% gain over the LVLM, yet it still falls short of human performance. This indicates a recognition gap, with Gemini-Pro-1.5 (23%) providing inadequate captions for images requiring subtle clues, unlike human-generated captions (50%). Further, this shortcoming persists as Gemini-Pro-1.5, even with ground truth captions, improves only to 50%, lagging behind the human rate of 82%. Further results on various categories and difficulty index levels are available in A.3.

\begin{table}
\begin{tabular}{l l c} \hline \multicolumn{3}{c}{} & \multicolumn{1}{c}{**\% Human Rating \(\uparrow\)**} \\ \hline  & Gemini Pro 1.5 & **40** \\  & Gemini Pro Vision & 30 \\ LVLM & GPT4 & 34 \\  & LLaVA-1.6-34B & 15 \\  & LLaVA-1.5-7B & 13 \\  & InstructBlip & 13 \\ \hline Caption\(\rightarrow\) LLM & Gemini Pro 1.5 \(\rightarrow\) Gemini Pro 1.5 & 23 \\  & Human (Oracle) \(\rightarrow\) Gemini Pro 1.5 & **50** \\ \hline  & Humans & **82** \\ \hline \end{tabular}
\end{table}
Table 1: Human ratings for large vision and language models, caption generation to large language models pipelines, and human evaluators on 400 riddles in the Visual Riddles benchmark.

Caption Quality and Model GapsTo understand the impact of caption quality on model performance in the Caption\(\rightarrow\)LLM setup, we analyzed 200 riddles (50% of the dataset), comparing human-generated (ground truth) captions with model-generated ones. Human captions, provided by riddle creators to generate the images, typically contained the key elements needed to solve the riddles. For example, Fig. 4 shows a riddle involving a 1000-piece Dora the Explorer jigsaw puzzle. The model-generated caption failed to mention crucial details such as the number of pieces, small size, or complexity, making the question "Should I buy this for my toddler?" unanswerable. Our findings show that 97% of human-generated captions included the necessary information, while only 57% of model-generated captions did. This discrepancy highlights that models often miss important details when captioning, limiting their ability to answer riddles effectively. Even when provided with human-generated Oracle captions, models still struggled with reasoning, achieving only a 50% success rate compared to 23% with model-generated captions, as presented in Table 1. This indicates a 27% "visual recognition gap", where models could improve by better recognizing visual cues, and a 32% "reasoning gap", showing the models' difficulty in reasoning even with perfect visual input. These findings underscore the importance of both accurate captioning and improved reasoning capabilities to close the gap between model and human performance.

Tool Usage and Model GapsWe analyzed all data with attribution, comprising 164 riddles (41%) where external tools, such as Google Search or Lens, could potentially aid in solving. The remaining 59% did not necessitate these tools. Model performance was assessed across both categories, revealing a slight difference: models reached 21% accuracy on riddles where external tools might have been useful and 26% on those not requiring such assistance. These findings suggest that the main difficulties for models lie in reasoning and interpreting visual clues rather than depending on external knowledge.

### Multiple-choice VQA

We next evaluate the performance of large vision and language models on the multiple-choice VQA task of the Visual Riddles benchmark, shifting from a generative to a classification task. Each riddle includes an image and a question, along with five answer choices: the correct one from the designer, three incorrect options derived from the human-judgment evaluations of model or human responses to the open-ended riddles (SS5.2), and a "cannot determine" option for ambiguous cases. For the 12% instances where fewer incorrect responses were available (i.e., when most responses by humans and models were correct), we used GPT-4 to generate additional distractors, using two in-context examples of visual riddles with incorrect answers. Finally, accuracy is calculated as the proportion of riddles correctly solved by the models, with random guessing yielding a baseline success rate of 20%.

Figure 4: Comparison of model-generated and human-generated captions that were used in the Caption\(\rightarrow\)LLM setup. ’X’ marks captions where critical details are missing in the model-generated version, while ’V’ marks captions where these details are present.

ResultsThe results in Table 2, indicate that this multiple-choice version is comparably challenging to the open-ended visual riddles, with GPT-4, Gemini-Pro-Vision and Gemini-Pro-1.5 showing the highest accuracies of 45%, 41% and 38%, respectively. Overall, model performance on this task slightly exceeds that on the open-ended task as assessed by human evaluators. Excluding instances where models selected the "cannot determine" option enhances these figures, elevating GPT-4 to 52% and Gemini-Pro-1.5 to 48%. This trend, detailed further in A.7, reveals that models often default to "cannot determine" in the absence of sufficient information. This tendency is mitigated by providing hints and attributions, making GPT-4's accuracy significantly increase to 69% and 82%, respectively. Post-submission results further highlight this trend, with GPT4o leading the models at 55% accuracy overall and reaching 83% with hints.

### Open-ended VQA Automatic Evaluation

The automatic evaluation experiments are designed to assess the capability of vision and language models to accurately judge the correctness of open-ended answers to visual riddles. This evaluation is critical for identifying the most effective approach for automated evaluation, supporting scalable future work on our benchmark, and ensuring integration with the leaderboard intended for widespread use by the research community.

Comparing Auto-Raters to Human RatingsWe evaluate the models in two scenarios: _reference-free_ and _reference-based_, in the first scenario, models independently assess the correctness of an answer based solely on the image and its associated question, and in the second, models are additionally provided with the ground-truth answer along with the candidate answer, challenging them to validate the provided answer against the correct one. Each auto-rater candidate was evaluated on a subset of two responses provided by other models and humans, excluding its own responses from this evaluation. This subset consists of balanced responses, including one correct and one incorrect. If one of these options was not available, two responses from the same category were selected. We tested the candidate auto-rater models on the visual riddles annotated in SS5.2 to determine which model's performance most closely aligns with human judgments. This alignment is crucial for selecting models suitable for evaluative roles in automated systems.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & **\% Accuracy**\(\uparrow\) & **\% Cannot** & **\% Accuracy w/o** & **+ Hint**\(\uparrow\) & **+ Attribution**\(\uparrow\) \\  & & **Determine** & **Cannot Determine** & & \\ \hline Gemini Pro 1.5 & 38 & 20 & 48 & 66 & 72 \\ Gemini Pro Vision & 41 & 3 & 42 & 62 & - \\ GPT4 & **45** & 12 & 52 & **69** & **82** \\ LLaVA-1.6-34B & 24 & 8 & 26 & 30 & - \\ LLaVA-1.5-7B & 17 & 0 & 17 & 29 & - \\ \hline Claude 3.5 Sonnet & 46 & 4 & 48 & 45 & - \\ GPT4o & **55** & 17 & 67 & **83** & - \\ Qwen-VL-Max & 35 & 3 & 37 & 53 & - \\ Molmo-7B & 34 & 1 & 35 & 42 & - \\ \hline \hline \end{tabular}
\end{table}
Table 2: Three types of multiple choice evaluations: overall accuracy, accuracy excluding instances where the model selected “cannot determine”, and accuracy with auxiliary data (hints or attributions).

\begin{table}
\begin{tabular}{l l c} \hline \hline  & **Judge** & 
\begin{tabular}{c} **Accuracy of Judge Prediction** \\ **Compared to Human Rating**\(\uparrow\) \\ \end{tabular} \\ \hline Reference-Based & Gemini Pro 1.5 & **87** \\  & Gemini Pro Vision & 75 \\  & GPT4 & 86 \\  & LLaVA -1.6-34b & 76 \\  & LLaVA -1.5-7b & 70 \\ \hline Reference-Free & Gemini Pro 1.5 & **52** \\  & Gemini Pro Vision & 38 \\  & GPT4 & 51 \\  & LLaVA -1.6-34b & 43 \\  & LLaVA -1.5-7b & 35 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Candidate auto-raters performances compared to human rating.

ResultsTable 3 presents the accuracy of the models under two scenarios, with Gemini-Pro-1.5 achieving the highest performance--52% in the reference-free context and 87% in the reference-based context, as measured against human ratings. These results suggest that the reference-free scenario may be less suitable for auto-evaluation, as evidenced by the top score of 52%, indicating a moderate correlation with human judgment. However, the reference-based scenario shows a stronger correlation, with the top two models achieving 87% and 86%. These findings indicate the need for an appropriate judge to assess open-ended answers.

Utilizing the Optimal Auto-Rater to Evaluate Visual RiddlesHaving established Gemini-Pro-1.5 as the best auto-rater, we utilize it in a reference-based setting to assess all models on the three open-ended settings detailed in SS5.2, i.e., the main task, and its hint- and attribution-assisted variants. Table 4 presents the auto-rating accuracy of various models on the open-ended task, with Gemini-Pro-1.5 as the evaluator. Gemini-Pro-1.5 achieves the highest performance, scoring 53% (40% with human rating) on open-ended questions, 62% with hints, and 29% with attributions. These results suggest that models perform better when provided with hints but not with attributions. Hints improve model performance by directing attention to visual clues, but they do not notably enhance reasoning capabilities. Conversely, when attributions are provided, models struggle to filter through irrelevant details, indicating significant challenges in solving open-ended visual riddles compared to human levels, even with auxiliary information. This highlights ongoing difficulties in improving models' visual reasoning capabilities and bridging the gap between human and machine understanding in visual interpretation tasks.

## 6 Analysis

This section explores two key aspects of the Visual Riddles challenge: assessing models' use of visual cues through comparisons between original and modified images, and examining generative models' efficacy in replicating images aligned with our riddle prompts for an automatic generation pipeline.

### Assessing Visual Aspects

of Riddles: Ablation Study with Modified Images

To assess models' preference for specific answers, we selected 72 images from our benchmark and created modified versions, altering visual cues to invalidate ground truth answers. This study explores whether models base their answers solely on text or consider visual clues. We conducted two rounds of testing: one with original images and one with modified versions. In both instances, we presented the model with the question, the original ground truth answer, and asked,"Is the answer correct?" (Fig. 5). Post-image modification, known ground truth answers became incorrect, make the evaluation to a binary assessment. For each model, we calculated the percentage of correctness rates on each type of image and the gap. A high gap indicates that the model identified the answer as correct for

\begin{table}
\begin{tabular}{l c c c c} \hline  & \multicolumn{2}{c}{**Visual Riddles**} & \multicolumn{2}{c}{**Hints**} & \multicolumn{2}{c}{**Attribution**} \\ \cline{2-5}  & **\% Human** & **\% Auto-Rater** & **\% Auto-Rater** & **\% Auto-Rater** \\  & **Rating**\(\uparrow\) & **Rating**\(\uparrow\) & **Rating**\(\uparrow\) & **Rating**\(\uparrow\) \\ \hline Gemini Pro 1.5 & **40** & **53** & **62** & **29** \\ Gemini Pro Vision & 30 & 34 & 47 & - \\ GPT4 & 34 & 38 & 61 & 25 \\ LLaVA -1.6-34b & 15 & 21 & 16 & - \\ LLaVA -1.5-7b & 13 & 19 & 30 & - \\ Instruct4Blip & 13 & 20 & 28 & - \\ \hline Claude 3.5 Sonnet & - & 39 & - & - \\ GPT4o & - & 50 & - & - \\ Qwen-VL-Max & - & 26 & - & - \\ Molmo-7B & - & 36 & - & - \\ \hline Humans & **82** & **78** & - & - \\ \hline \end{tabular}
\end{table}
Table 4: Automatic evaluation of open-ended answers by Gemini-Pro-1.5.

\begin{table}
\begin{tabular}{l c c c} \hline  & **Original** & **Modified** & **Gap\(\uparrow\)** \\  & **Images\(\uparrow\)** & **Images\(\downarrow\)** & **Gap\(\uparrow\)** \\ \hline Gemini Pro 1.5 & 74 & 14 & **60** \\ Gemini Pro Vision & 86 & 51 & 35 \\ GPT4 & 68 & 15 & 53 \\ Lava-1.6-34b & 93 & 53 & 40 \\ Lava-1.5-7b & 54 & 38 & 16 \\ \hline \end{tabular}
\end{table}
Table 5: Percentage of correctness rates of ground truth answers for original and altered images. The ‘Gap’ column quantifies the reduction in performance due to image modifications, illustrating the challenge of context changes for model accuracy.

the original image and incorrect for modified one. Table 5 reveals models' preference for specific answers despite changes in critical image elements and their struggle to identify correct answers, especially with modified images. Gemini-Pro-1.5 excels with a 60% gap, while others below 53%.

### The Visual Riddles Prompt Set: A Challenge for Text-to-Image Models

This section shifts our focus from evaluating VLMs and LLMs to showcasing the unique challenge posed by the Visual Riddles prompts to text-to-image models. Designing images for the Visual Riddles dataset, with their specific and often subtle visual clues, proved surprisingly difficult. To quantify this difficulty, we attempted to reproduce 100 visual riddles using five popular open-source diffusion models: SDXL [47], SDXL-Turbo [48], LCM-SDXL [49], SD-1.4, and SD-2.1 [50]. Each model generated 100 images based on these prompts, amounting to 500 images in total. However, only 61 (12%) successfully matched the prompts. The most successful model was SDXL-Turbo, achieving a 15% success rate. A complete breakdown of the models' performance is available in A.8. Fig. 6 showcasing several models struggling to capture the nuances embedded in the prompt. This low overall success rate underscores the unique challenge presented by the Visual Riddles prompts, establishing them as a valuable resource for evaluating and advancing text-to-image generation, especially for tasks that demand high precision and the ability to render subtle visual features.

## 7 Conclusions and Limitations

Our analysis of the Visual Riddles Challenge shows that state-of-the-art vision-language models face difficulties in interpreting complex visual scenarios that require commonsense reasoning. With an average success rate of 40%, models significantly lag behind human performance, which stands at 82%. This performance gap underscores the ongoing challenge of bridging human and machine understanding in complex visual interpretation tasks. Although Visual Riddles is smaller than other benchmarks, it follows a "quality over quantity" approach, seen in recent benchmarks[51; 14; 16; 5; 52; 53], which emphasize high-quality challenges. Our dataset's size allows for meticulous handcuration to ensure diverse commonsense challenges, focusing on evaluation over training. While we utilized a variety of generative tools to reduce potential biases, this reliance may introduce inherent limitations, and despite efforts to exclude offensive content, some individuals may still find certain riddles inappropriate. Future work could explore automated dataset generation, creating a dedicated training set, and validating Visual Riddles as a robust test set. Generating multiple images for each prompt, coupled with repeated evaluations, would also allow more robust assessments of models' visual reasoning capabilities.

Figure 5: Modified images ablation study: a demonstration of the process where the model evaluates an answer’s validity using two scenarios: one with the original image and another with a modified image that alters the visual clue, affecting the correctness of the original ground truth answer.

Figure 6: Reproducing visual riddles: all models fail to capture the nuance in the description.

## References

* [1] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. _arXiv preprint arXiv:1904.09728_, 2019.
* [2] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019_, pages 6720-6731. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00688. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Zellers_From_Recognition_to_Cognition_Visual_Commonsense_Reasoning_CVPR_2019_paper.html.
* [3] Jack Hessel, Ana Marasovic, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor" understanding" benchmarks from the new yorker caption contest. _arXiv preprint arXiv:2209.06293_, 2022.
* [4] Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. Commonsenseqa 2.0: Exposing the limits of ai through gamification. _ArXiv preprint_, abs/2201.05320, 2022. URL https://arxiv.org/abs/2201.05320.
* [5] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schmidt. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use, 2023.
* [6] Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, Andre Araujo, and Vittorio Ferrari. Encyclopedic vqa: Visual questions about detailed properties of fine-grained categories, 2023.
* [7] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _arXiv preprint arXiv:2403.05530_, 2024.
* [8] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* [9] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _arXiv preprint arXiv:2204.14198_, 2022.
* [10] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. URL https://arxiv.org/abs/2304.08485.
* [11] OpenAI. Gpt-4 technical report, 2024.
* [12] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [13] Ramakrishna Vedantam, Xiao Lin, Tanmay Batra, C. Lawrence Zitnick, and Devi Parikh. Learning common sense through visual abstraction. In _2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015_, pages 2542-2550. IEEE Computer Society, 2015. doi: 10.1109/ICCV.2015.292. URL https://doi.org/10.1109/ICCV.2015.292.
* [14] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. WinoGAViL: Gamified association benchmark to challenge vision-and-language models. _arXiv preprint arXiv:2207.12576_, 2022.
* [15] Yonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, and Gabriel Stanovsky. VASR: Visual analogies of situation recognition. _arXiv preprint arXiv:2212.04542_, 2022.

* Bitton-Guetta et al. [2023] Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2616-2627, 2023.
* Hessel et al. [2022] Jack Hessel, Jena D Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko, and Yejin Choi. The abduction of sherlock holmes: A dataset for visual abductive reasoning. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVI_, pages 558-575. Springer, 2022.
* Yang et al. [2024] Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. Editworld: Simulating world dynamics for instruction-following image editing, 2024.
* Marino et al. [2019] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge, 2019. URL https://arxiv.org/abs/1906.00067.
* Jain et al. [2021] Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh Ramakrishnan, and Soumen Chakrabarti. Select, substitute, search: A new benchmark for knowledge-augmented visual question answering. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR '21. ACM, July 2021. doi: 10.1145/3404835.3463259. URL http://dx.doi.org/10.1145/3404835.3463259.
* Schwenk et al. [2022] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge, 2022. URL https://arxiv.org/abs/2206.01718.
* Lin [2004] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81, 2004.
* Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002.
* Zhang et al. [2019] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. _arXiv preprint arXiv:1904.09675_, 2019.
* Rei et al. [2020] Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. Comet: A neural framework for mt evaluation. _arXiv preprint arXiv:2009.09025_, 2020.
* Laban et al. [2022] Philippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A Hearst. Summac: Re-visiting nli-based models for inconsistency detection in summarization. _Transactions of the Association for Computational Linguistics_, 10:163-177, 2022.
* Honovich et al. [2021] Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. \(q^{2}\): Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7856-7870, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.619. URL https://aclanthology.org/2021.emnlp-main.619.
* Honovich et al. [2022] Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Re-evaluating factual consistency evaluation. _arXiv preprint arXiv:2204.04991_, 2022.
* Slobodkin et al. [2024] Aviv Slobodkin, Ori Shapira, Ran Levy, and Ido Dagan. Multi-review fusion-in-context. _arXiv preprint arXiv:2403.15351_, 2024.
* Hessel et al. [2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. _arXiv preprint arXiv:2104.08718_, 2021.

* [31] Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, and Kaipeng Zhang. Avibench: Towards evaluating the robustness of large vision-language model on adversarial visual-instructions. _arXiv preprint arXiv:2403.09346_, 2024.
* [32] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 20406-20417, 2023.
* [33] Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. _arXiv preprint arXiv:2404.18532_, 2024.
* [34] Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, and Idan Szpektor. What you see is what you read? improving text-image alignment evaluation. _Advances in Neural Information Processing Systems_, 36, 2024.
* [35] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. _arXiv preprint arXiv:2404.12390_, 2024.
* [36] Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, et al. Vibe-eval: A hard evaluation suite for measuring progress of multimodal language models. _arXiv preprint arXiv:2405.02287_, 2024.
* [37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.
* [38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.
* [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [40] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github.io/blog/2024-01-30-llava-next/.
* [41] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
* [42] Open AI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/. Accessed: 2024-10-29.
* [43] Anthropic. Claude 3.5 sonnet model. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. Accessed: 2024-10-29.
* [44] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. _arXiv preprint arXiv:2308.12966_, 2023.
* [45] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. _arXiv preprint arXiv:2409.17146_, 2024.
* [46] Klaus Krippendorff. _Content Analysis: An Introduction to Its Methodology_. Sage Publications, Beverly Hills, CA: Sage, 2004.

* [47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.
* [48] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation, 2024.
* [49] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module. _arXiv preprint arXiv:2311.05556_, 2023.
* [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.
* [51] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5238-5248, 2022.
* [52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024.
* [53] Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, and Nanyun Peng. Contextual: Evaluating context-sensitive text-rich visual reasoning in large multimodal models. _arXiv preprint arXiv:2401.13311_, 2024.

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section 3. Additionally, the license file will be included in the supplemental material.
* Did you include the license to the code and datasets? [Yes] See Section 3. Additionally, the license file will be included in the supplemental material.
* Did you include the license to the code and datasets? [Yes] See Section 3. Additionally, the license file will be included in the supplemental material.

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We introduce a novel benchmark of visual riddles designed to challenge vision and language models. We demonstrate its significant level of difficulty and emphasize its importance within the paper. 2. Did you describe the limitations of your work? [Yes] See Section 7 3. Did you discuss any potential negative societal impacts of your work? [Yes] We discuss potential impacts in Appendix B 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? All data and code necessary to reproduce the experiment will be provided in the supplemental material. Additionally, it will be available on https://visual-riddles.github.io/ in the future. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Our paper does not include any training. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? Expanded upon in Appendix A.1
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [N/A] We solely utilize existing models for evaluation purposes. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] Our work presents a benchmark, including images, questions, answers, and any additional data, along with the code, which will initially be included in the supplemental material and later made available on our website at https://visual-riddles.github.io/. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] It is presented in Appendix C 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Section 75. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See Section 5.2, Section 5.2, Appendix A.5 and Appendix A.6 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] See Appendix A.5 and Appendix A.6Appendix

The subsequent sections offer essential insights into reproducibility, encompassing detailed explanations and examples regarding model evaluation, analysis, and data collection.

For access to the complete Visual Riddles dataset, visit https://huggingface.co/datasets/visual-riddles/visual-riddles. Comprehensive information about the dataset fields and loading instructions is provided therein.

### Reproduciblity and Resources

We executed all evaluation code on a single Colab notebook, accessible on our website https://visual-riddles.github.io/. For models using APIs like Gemini and GPT-4, we employed CPU resources. However, for models like Llava and InstructBLIP, which required GPU resources, we utilized a single A100 GPU. We utilized a paid plan for models with APIs, enabling us to generate 400 predictions for each task within a timeframe of less than 30 minutes, equivalent to the runtime of Llava and InstructBLIP models used locally with GPUs. Table 6 lists the models specified in the APIs, and how they were used.

Auxiliary Data: AttributionsFor each visual riddle containing an attribution field, we extracted the textual content from the webpage URL using Selenium WebDriver (https://www.selenium.dev/). Consequently, prompts containing attributions may be lengthy.

### Image Generation Guidelines

The Visual Riddles benchmark was created by seven designers including four women and three men, most of whom are authors of this paper, all from the same country, experienced in generating images using text-to-image models. The annotators were instructed to create images that integrate information with world knowledge and common sense to answer a given textual question. The answer must be grounded in the data presented in the image, making it impossible to respond accurately without comprehending the image and identifying the embedded clues. To generate high-quality images, our designers are given access to advanced text-to-image models, including Midjourney1, Ideogram2, Canva3, DALL-E 3[38], and Stable-Diffusion [39]. Each image must be sufficiently challenging such that at least one of the evaluation models (e.g., Gemini-Pro-1.5 [7], GPT4-turbo-preview [11] and LLaVA-1.6-34B [40]) fails to correctly answer the question based on the provided image. In addition, the designers provided not only the correct answer to the question but also a hint to the image that should guide where to look in the image when trying to solve the riddle (Example in Fig. 1)

\begin{table}
\begin{tabular}{l l l l} \hline
**API** & **Model Name** & **Specified** & **Type of** & **Tasks Used** \\  & & & **Model Used** & \\ \hline GPT4 & gpt-4-vision-preview & VLM & Open-Ended, Multiple-Choice (Distractors + Evaluation), Auto-Eval Judge & \\ Gemini-Pro-1.5 & gemini-1.5-pro-latest & VLM & Open-Ended, Multiple-Choice, Auto-Eval Judge, Auto-Rater & \\ Gemini-Pro-1.5 & gemini-1.5-pro-latest & LLM & Caption \(\rightarrow\) LLM on Open-Ended & \\ Gemini-Pro-Vision & gemini-pro-vision & VLM & Open-Ended, Multiple-Choice, Auto-Eval Judge \\ \hline \end{tabular}
\end{table}
Table 6: APIs model specification 

### Call for AI image designers!

We are collecting human designers to assist with the generation of a general world knowledge and common-sense challenge data set.

The generation task involves the creation of unique instances each consisting of an image, a question, and an answer. These instances should test the limits of AFs understanding of general world knowledge by presenting scenarios that require multi-step reasoning based on visual cues. The main goal is to craft questions that an AI model cannot easily answer just by analyzing the image and the text, thereby pushing the boundaries of current AI capabilities.

The images should be in a **jpg** format 1024x1024 size, the textual file should contain the question, the answer, the relevant image file name for each instance, and the name of the models that you were used to generate each of your images. This file should be in a **csv** format:

\begin{tabular}{l|l|l|l|l|l|l|} Question & Answer & Image & Image & Generative & Hint & Attribution \\  & & file name & caption & Model name & & \\ \end{tabular}

**Examples for generative models:**

https://deogaram.ai/lpg/1. https://xocenshot.google.com/3330/F.Liu2F.FuhPZ.

https://stabledt/tusi.mweb.com/google.vainette. https://www.canwa.com/ai-image-generator/.

https://www.midjourney.com/home. https://geemil.google.com/qgp

**Image Creation**

_Rastism and AI Generation_: Each image should be crafted to look as realistic as possible, utilizing advanced AI-based image generation tools such as Dal-E. Miquerney, or Stable Diffusion etc. The chosen imagery should directly relate to the question and answer, serving as a visual foundation for the challenge.

_Accuracy Verification_: After generating an image, ensure that it accurately represents the intended scenario or concept (the concept must be a real-world one, don't use ideas that do not happen in reality). The image should visually encapsulate all the elements necessary to lead to the question and, by extension, the answer.

**Crafting Questions**

_Challenge Through Indirect Connection_: Questions should be designed to challenge AI models by requiring two-step logical reasoning (two-hop) based on the information depicted in the image. For instance, an image showing leafless trees and lush greenery suggests a winter scene, which implies colder temperatures, leading to questions about appropriate atire for the conditions.

_Specifically and Relevance_: Each question must directly relate to the visual and conceptual content of the image, prompting the responder to make logical connections between what is seen and the broader implications or facts of the world.

**"additional note" : verify that each question cannot be answered without the relevant image.**

**Formulating Answers**

The answers can be short and free-style, please follow these:

_Detail-Oriented Responses_: Answers should be comprehensive, detailing all aspects of the image relevant to the question. This includes describing the visible elements that lead to the understanding necessary to address the question accurately.

_Completeness and Advisory_: Beyond just stating facts, answers should also provide a conclusive statement or recommendation based on the question posed. The response should leave no ambiguity about the appropriate course of action or the logical conclusion to be drawn from the image-question context.

**PAY ATTENTON: the answer should refer to both the visual clue in the image, and the commonsense/world knowledge that is required for the solution.**

**Hint:a textual hint that directs the solver (human or model) for the visual clue. cannot be the clue itself.**

**Attribution: for questions that their answer requires a "reliable source" like answers that talk about rules/countries/biological conditions/cultural rules/etc, there is a need to provide a link for a websource that confirms your answer (does not have to be only wikipedia).**

**Figure 7: Guidelines for human designers to create a visual riddle — Part 1. This section includes instructions on the visual riddle creation process including requirements for captions, generative models, hints, and attribution.**

**Attribution: for questions that their answer requires a “reliable source" like answers that talk about rules/countries/biological conditions/cultural rules/etc, there is a need to provide a link for a websource that confirms your answer (does not have to be only wikipedia).**

[MISSING_PAGE_FAIL:19]

there is no significant correlation between the difficulty categories for humans and models, with a correlation coefficient of -0.12. For difficulty levels, while human performance consistently surpasses models, both show a decline as difficulty increases, with a strong correlation of 0.94 (see top and middle parts of Fig. 9). To further illustrate, examples of images corresponding to different difficulty levels are provided in Fig. 10 (bottom).

Figure 9: A heatmap depicting how models (Top) and human annotators (Middle) fail to solve visual riddles in the Visual Riddles dataset, visualizing by the color intensity of each cell. The x-axis represents different categories of riddles, while the y-axis shows the difficulty index levels. Red cells highlight particularly challenging areas, with vertical red bands indicating categories that consistently pose difficulties and horizontal red bands confirming the appropriateness of the difficulty index levels assigned. This visualization underscores the diverse range of commonsense knowledge needed to effectively tackle the visual-riddles. Below, there is an example of images related to some of our categories.

### Prompts for Different Models

There are several tasks with different prompts:

Multiple-Choice VQA:A specific prompt is used for generating distractors when there are insufficient incorrect answers using GPT-4. Additionally, for multiple-choice VQA, we evaluate the models' performance across three settings: (1) given only the image, the question, and five possible answers (with only one correct answer), (2) given the image, the question, the possible answers, and a hint,

Figure 10: Aggregated results for model and human performance across categories (top) and difficulty levels (middle), along with visual riddle examples illustrating varying difficulty levels (bottom). The top section shows the average open-ended VQA accuracy for models and humans by category, with a correlation of -0.12 between their performances. The middle section presents the average open-ended VQA accuracy for models and humans by difficulty level, showing a correlation of 0.94.

and (3) given the image, the question, the possible answers, and an attribution. The prompts structure is outlined in Table 9.

VQA Automatic Evaluation:To find the best judge (auto-rater), we evaluate the models in two scenarios - _reference-free_ and _reference-based_. The prompts structure is outlined in Table 10.

Auto-Rater for Open-Ended VQA and Ablation Study with Modified Images:Using the best Auto-Rater we evaluate all models automatic ratings on the Open-Ended VQA task. We also use the same prompt in our analysis we perform ablation study to explores whether models base their answers solely on text or consider visual clues. The prompts structure is outlined in Table 10.

### Open-Ended VQA: Annotators Solve Visual Riddles Guidelines

In order to evaluate how well humans are capable in solving the visual riddles questions, a human response were collected using Amazon Mechanical Turk platform www.mturk.com. To gather human responses for the benchmark's riddles, we used the Amazon Mechanical Turk platform, paying annotators $18 per hour. We contacted workers with a proven track record in similar tasks and invited them to a qualification round that began with a review of task guidelines and included solving five riddles of varying difficulty. Following an assessment of their responses and providing personalized feedback, only those demonstrating a strong understanding of the task qualified. Of 14 candidates, 10 proceeded to the actual annotation, where each of the 400 riddles was solved by three annotators. In the guidelines, annotators were presented with five examples. For each example, they were initially shown the visual riddle and subsequently given the solution and the process for solving the question. We recommended that annotators first attempt to solve the riddles on their own before reviewing our provided answers.

Two examples of the guidelines are in Fig. 11 and Fig. 12. In the first, answering the question required only common-sense and counting capabilities of objects in the image while in the second example, world knowledge about cultural principles is needed therefore, the workers were expected to search the different items, understand the clues given in the image and only then answer the question.

\begin{table}
\begin{tabular}{c c} \hline
**Category** & **\% Images** \\ \hline Cultural Principles & 19.50 \\ Social Norms & 15.75 \\ Safety and Survival & 8.25 \\ Food-related & 7.50 \\ Object Counting & 7.00 \\ Health & 6.00 \\ Traffic & 5.00 \\ Animal Behavior & 5.00 \\ Geographic Knowledge & 4.50 \\ Entertainment & 4.00 \\ Architectural & 3.75 \\ Temporal Principles & 3.00 \\ Environmental & 3.00 \\ Religion Principles & 3.00 \\ Physical Principles & 2.50 \\ Biological Principles & 2.25 \\ \hline \end{tabular}
\end{table}
Table 7: Distribution of Categories Across Images

\begin{table}
\begin{tabular}{c c} \hline
**Difficulty Level** & **\% Images** \\ \hline
0 & 11.10 \\
1 & 33.25 \\
2 & 38.25 \\
3 & 17.50 \\ \hline \end{tabular}
\end{table}
Table 8: Distribution of Categories Across ImagesIn Fig. 13 there is an example of the UI page for annotation of solving a visual riddle.

### Open-Ended VQA: Humans and Models Answers Annotation Evaluation Guidelines

In order to evaluate how well models and humans answered the visual riddle, we used MTurk platform, paying 185 per hour. We contacted workers and invited them to a qualification round that began with a review of task guidelines and included solving five riddles of varying difficulty. Following an assessment of their responses and providing personalized feedback, only those with a strong understanding of the task moved on to the actual annotation phase. Of 9 candidates, 6 proceeded to the actual annotation phase. To ensure greater cultural diversity in the benchmark, all AMT annotators were selected from countries different from those of the visual riddles creators.

\begin{table}
\begin{tabular}{l l} \hline
**Task** & **Prompt** \\ \hline Reference-Free, & “Answer with only Yes OR No. Given the image \\ Ablation Study with & and the question, is the candidate answer correct? \\ Modified Images & \n Question: \(<question>\)\n Candidate Answer: \(<candidate\_answer>\)\n” \\ \hline Reference-Based, & “Answer with only Yes OR No. Given the image, the question and the ground-truth answer, is the candidate answer correct? \\ Ended VQA & \(<question>\)\n Ground-Truth Answer:\(<ground\_truth\_answer>\)\n Candidate Answer: \(<candidate\_answer>\)\n” \\ \hline \end{tabular}
\end{table}
Table 10: Prompts for different models for Automatic Evaluation

\begin{table}
\begin{tabular}{l l} \hline
**Task** & **Prompt** \\ \hline Creating Distractor & “Here is a question regarding the image, and a ground-truth answer. \(\n\) Question:\(<question>\)\n Ground-Truth Answer \(<ground\_truth\_answer>\)\n Please generate \(<num\_of\_incorrect\_distractors>\) wrong answers (that are kind-of similar to the ground-truth answer, and in a similar length) to the question based on the image, in the format of: \n n YOUR FIRST ANSWER@@@YOUR SECOND ANSWER@@@@@...” \\ \hline Clean & “This is a multiple-choice question concerning the image. Out of the options labeled (1)-(5), only one is correct. Please provide your answer as a single digit that corresponds to the correct option. For instance, if the correct answer is (3), you should respond with 3. \n n Question: \(<question>\)\n Candidate answers: \(\n\) (1)\(<candidate1>\)\n (2)\(<candidate2>\)...” \\ \hline + Hint & “This is a multiple-choice question concerning the image. Out of the options labeled (1)-(5), only one is correct. Please provide your answer as a single digit that corresponds to the correct option. For instance, if the correct answer is (3), you should respond with 3. \n n Question: \(<question>\)\n Hint: \(<Hint>\)\n Candidate answers: \(\n\) (1)\(<candidate1>\)\n (2)\(<candidate2>\)...” \\ \hline + Attribution & “This is a multiple-choice question concerning the image. Out of the options labeled (1)-(5), only one is correct. Please provide your answer as a single digit that corresponds to the correct option. For instance, if the correct answer is (3), you should respond with 3. \n n Question: \(<question>\)\n Hint: \(<Hint>\)\n Candidate answers: \(\n\) (1)\(<candidate1>\)\n (2)\(<candidate2>\)...” \\ \hline + & \multicolumn{1}{c}{} \\ \hline + & \multicolumn{1During annotation, In the LVLM and the Caption \(\rightarrow\) LLM cases, each annotator was provided with the image, the question, and the correct answer, along with the candidate answers from six different models and three humans (the human answers were obtained as described in SS5.2). In the same way (image, question and ground truth answer), we annotated the candidate answers for human (oracle) and Gemini-Pro-1.5 captions from the best model as described in SS5.2.

In the guidelines, annotators were presented with five examples. For each example, they were initially shown the image, the question, the gold-answer, and the candidate answers. Afterwards, they could see which answers were correct and the reasoning behind the correctness or incorrectness of each candidate answer. Marking a candidate answer as correct required that it not only accurately answered the question based on the given answer but also contained no hallucinations.

Figure 11: Example of the guidelines for MTurk annotators for solving a visual riddle as an open ended question. (1) Question: Solving visual riddles composed of images and open-ended questions. (2) Answer: the answer to the question. (3) Explanation & Notes: explanation of why this is the correct answer given the question and the image.

An example of the guidelines as well as a UI page is in Fig 14. Using these annotations as mentioned in SS5.3, we compose the Multiple-Choice VQA prompts by sampling three answers annotated as incorrect. If there are fewer than three, we generate additional incorrect answers. We then include the ground-truth answer provided by the riddle designer, along with the "cannot determine" option.

Figure 12: Example of the guidelines for MTurk annotators for solving a visual riddle as an open ended question. (1) Question: Solving visual riddles composed of images and open-ended questions. (2) Answer: the answer to the question. (3) Explanation & Notes: explanation of why this is the correct answer given the question and the image. (4) Attribution: for this image a world knowledge is required therefore - a search in google is helpful in getting the data.

### Multiple Choice VQA Analysis: Model Hesitation

As mentioned in SS5.3, a notable occurrence was the frequent selection of the "cannot determine" option by some models. Further analysis excluding these instances revealed improvements in comparison to the results with this option (see Table 2), with GPT-4 achieving 52% accuracy instead of 45%, and Gemini-Pro-1.5 reaching 48% accuracy instead of 38%, respectively. These results suggest that **some models hesitate to answer certain questions, opting for the "cannot determine" option when lacking sufficient information to decide.** Additionally, as shown in Table 11, providing models with auxiliary information via hints and attributions reduces their likelihood of selecting the "cannot

Figure 14: Example of guidelines for MTurk annotators evaluating different answers to the visual riddle. (1) Annotator UI page displaying the visual riddle, the correct answer, and the candidate answers. (2) Solution: Identification of which candidate answers correctly solve the visual riddle. (3) Explanation & Notes: Detailed reasoning for the correctness or incorrectness of each candidate answer (only explanations for three candidates are shown due to space constraints).

Figure 13: Example of MTurk UI page for answering the question given an image and a questiondetermine" option. For example, Gemini-Pro-1.5 achieves an accuracy rate of 74% with a hint, while GPT-4 achieves an accuracy rate of 84% with an attribution.

### The Visual Riddles Prompt Set: A Challenge for Text-to-Image Models

n this section, we present the complete breakdown of the models' performance in generating images that fit the visual-riddles prompts. Table 13 presents the full results, indicating that models struggle to generate images that include delicate hints hidden within them, with the best-generating model, SDXL-Turbo [48], creating only 15% of images that match the given prompt.

## Appendix B Discussion any potential negative societal impacts Visual Riddles

While Visual Riddles offers a unique platform for enhancing visual reasoning and commonsense understanding, it also presents potential challenges that merit careful consideration. One concern is the inadvertent reinforcement of biases. Despite rigorous efforts to design inclusive and culturally neutral visual riddles, the possibility remains that some content might unintentionally reflect or amplify societal stereotypes. Moreover, the complexity of certain riddles could disadvantage users with specific cognitive or sensory impairments, thereby limiting their participation and representation in the research facilitated by this benchmark.

\begin{table}
\begin{tabular}{l c} \multicolumn{2}{c}{**\% Generation**} \\  & **Success**\(\uparrow\) \\ \hline SD-1.4 & 7 \\ SD-2.1 & 12 \\ SDXL-LCM & 12 \\ SDXL & 14 \\ SDXL-Turbo & **15** \\ \hline \end{tabular}
\end{table}
Table 13: Model success rates

\begin{table}
\begin{tabular}{l c c c} \hline  & **\% Cannot** & **+ Hint \% Accuracy** & **+ Attribution** \% Accuracy** \\  & **Cannot Determine** & **w/o Cannot Determine** & **w/o Cannot Determine** \\ \hline Gemini Pro 1.5 & 48 & **74** & 79 \\ Gemini-Pro-Vision & 42 & 64 & \\ GPT4 & **52** & 71 & **84** \\ LLAVA-1.6-34B & 26 & 34 & \\ LLAVA-1.5-7B & 17 & 29 & \\ \hline Claude 3.5 Sonnet & 48 & 45 & \\ GPT4o & **67** & **87** & \\ Qwen-VL-Max & 37 & 53 & \\ Molmo-7B & 35 & 43 & \\ \hline \end{tabular}
\end{table}
Table 12: Accuracies Excluding “Cannot Determine” Answer

\begin{table}
\begin{tabular}{l c c c} \hline  & **\% Cannot** & **+ Hint** & **+ Attribution** \\  & **Determine** & **\% Cannot Determine** & **\% Cannot Determine** \\ \hline Gemini Pro 1.5 & **20** & **12** & **9** \\ Gemini-Pro-Vision & 3 & 2 & \\ GPT4 & 12 & 3 & 3 \\ LLaVA-1.6-34B & 8 & 10 & \\ LLaVA-1.5-7B & 0 & 0 & \\ \hline Claude 3.5 Sonnet & 4 & 1 & \\ GPT4o & 17 & 4 & \\ Qwen-VL-Max & 3 & 1 & \\ Molmo-7B & 1 & 1 & \\ \hline \end{tabular}
\end{table}
Table 11: Percentage of “Cannot Determine” AnswersAnother limitation is the reliance on automated evaluation methods. Such methods may not fully capture the depth of human reasoning and interpretation, potentially affecting the robustness and transparency of the evaluations. This might inadvertently prioritize certain types of reasoning over others, skewing the development and assessment of AI systems.

To address these issues, we are committed to a continuous review process involving diverse stakeholders to help identify and mitigate any biases or exclusions. This process will include refining the riddles and improving the evaluation methods to ensure they are as inclusive and representative as possible. Additionally, ongoing adjustments will be made to the evaluation protocols to enhance their ability to assess nuanced and complex responses, thereby ensuring that the benchmark remains a fair and effective tool for advancing AI research in an ethically responsible manner.

## Appendix C Designer Consent

We acknowledge and extend our gratitude to all designers who contributed to the benchmark. The credit for each visual riddle is included as part of our dataset. All designers have consented to contribute their creations to this research.